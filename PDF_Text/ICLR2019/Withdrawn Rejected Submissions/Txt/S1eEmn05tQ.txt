Under review as a conference paper at ICLR 2019
Uncertainty in Multitask Transfer Learning
Anonymous authors
Paper under double-blind review
Ab stract
Using variational Bayes with neural networks, we develop an algorithm capable
of accumulating knowledge into a prior from multiple different tasks. This results
in a rich prior capable of few-shot learning on new tasks. The posterior can go
beyond the mean field approximation and yields good uncertainty on the performed
experiments. Analysis on toy tasks show that it can learn from significantly different
tasks while finding similarities among them. Experiments on Mini-Imagenet reach
state of the art with 74.5% accuracy on 5 shot learning. Finally, we provide two
new benchmarks, each showing a failure mode of existing meta learning algorithms
such as MAML and prototypical Networks.
1	Introduction
Recently, significant progress has been made to scale Bayesian neural networks to large tasks and
to provide better approximations of the posterior distribution (Blundell et al., 2015). Recent works
extend fully factorized posterior distributions to more general families (Louizos and Welling, 2017;
Krueger et al., 2017; Sun et al., 2017). It is also possible to sample from the posterior distribution
trough mini-batch updates (Mandt et al., 2017; Zhang et al., 2017).
However, for neural networks, the prior is often chosen for convenience. This may become a
problem when the number of observations is insufficient to overcome the choice of the prior. In
this regime, the prior must express our current knowledge on the task and, most importantly, our
lack of knowledge on it. In addition to that, a good approximation of the posterior under the small
sample size regime is required, including the ability to model multiple modes. This is indeed the
case for Bayesian optimization (Snoek et al., 2012), Bayesian active learning (Gal et al., 2017),
continual learning (Kirkpatrick et al., 2017), safe reinforcement learning (Berkenkamp et al., 2017),
exploration-exploitation trade-off in reinforcement learning (Houthooft et al., 2016). Gaussian
processes (Rasmussen, 2004) have historically been used for these applications, but an RBF kernel
constitute a prior that is unsuited for many tasks. More recent tools such as deep Gaussian processes
(Damianou and Lawrence, 2013) show great potential and yet their scalability whilst learning from
multiple tasks needs to be improved.
Our contributions are as follow:
1.	We provide a simple and scalable procedure to learn an expressive prior and posterior over
models from multiple tasks.
2.	We reach state of the art performances on mini-imagenet.
3.	We propose two new benchmarks, each exposing a failure mode of popular meta learning
algorithms. In contrast, our method perform well on these benchmarks.
•	MAML (Finn et al., 2017) does not perform well on a collection of sinus tasks when
the frequency varies.
•	Prototypical Network (Snell et al., 2017)’s performance decrease considerably when
the diversity of tasks increases.
Outline: We first describe the proposed approach in Section 2. In Section 3, we extend to three
level of hierarchies and obtain a model more suited for classification. Section 4 review related
methods and outline the key differences. Finally, In Section 5, we conduct experiments on three
different benchmarks to gain insight in the behavior of our algorithm.
1
Under review as a conference paper at ICLR 2019
2	Learning a Deep Prior
By leveraging the variational Bayesian approach, we show how we can learn a prior over models with
neural networks. We start our analysis with the goal of learning a prior p(w∣α) over the weights W of
neural networks across multiple tasks. We then provide a reduction of the Evidence Lower BOund
(ELBO) showing that it is not necessary to explicitly model a distribution in the very high dimension
of the weight space of neural networks. Instead the algorithm learns a subspace suitable for expressing
model uncertainty within the distributions of tasks considered in the multi-task environment. This
simplification results in a scalable algorithm which we refer to as deep prior.
2.1	Hierarchical Bayes
To learn a probability distribution p(w∣α) over the weights W of a network parameterized by α,
we use a hierarchical Bayes approach across N tasks, with hyper-prior p(α). Each task has its
own parameters Wj, with W = {Wj}jN=1. Using all datasets D = {Sj}jN=1, we have the following
posterior:1
P(W ,a|D) = PmID) γ P(Wj |a, Sj)
j
H p(D∣W)p(W∣α)p(α)
=	P(yij |xij,Wj)p(wj ∣α)p(α),
ji
The term P(yij |xij , Wj ) corresponds to the likelihood of sample i of task j given a model parame-
terized by Wj e.g. the probability of class yij from the softmax of a neural network parameterized
by Wj with input Xij. For the posterior p(α∣D), We assume that the large amount of data available
across multiple tasks will be enough to overcome a generic prior P(α), such as an isotropic Nor-
mal distribution. Hence, we consider a point estimate of the posterior p(α∣D) using maximum a
posteriori2.
We can now focus on the remaining term: P(Wj ∣α). Since Wj is potentially high dimensional with
intricate correlations among the different dimensions, we cannot use a simple Gaussian distribution.
Following inspiration from generative models such as GANs (Goodfellow et al., 2014) and VAE
(Kingma and Welling, 2013), we use an auxiliary variable Z 〜N(0, Idz) and a deterministic function
projecting the noise Z to the space of W i.e. W = hα(z). Marginalizing z, we have: p(W∣α)=
z P(z)P(W|z, α)dz = z P(z)δhα(z)-wdz, where δ is the Dirac delta function. Unfortunately,
directly marginalizing Z is intractable for general hα . To overcome this issue, we add Z to the joint
inference and marginalize it at inference time. Considering the point estimation ofα, the full posterior
is factorized as follows:
QN=IP(Wj, zj|a,Sj)	⑴
=QN=I P(Wj lzj, α, Sj )P(zj|a, Sj)
H QjN=1 P(Wj|Zj, α)P(Zj) Qin=j 1 P(yij |xij, Wj),
where P(yij |xij , Wj ) is the conventional likelihood function of a neural network with weight matrices
generated from the function hα i.e.: Wj = hα(Zj). Similar architecture has been used in Krueger
et al. (2017) and Louizos and Welling (2017), but we will soon show that it can be reduced to a
simpler architecture in the context of multi-task learning. The other terms are defined as follows:
P(Zj) = N(0, I)	(2)
P(zj ,Wj |a) = P(zj )δhα(z3 )-Wj	⑶
P(Zj ,Wj lα,Sj ) = P(zj |a, Sj )δhα(Zj )-wj	⑷
The task will consist of jointly learning a function hα common to all tasks and a posterior distribution
P(zj∙∣α, Sj) for each task. At inference time, predictions are performed by marginalizing Z i.e.:
P(y|x, D) =	E	P(y|x, hα(Zj)).
zj ^P(z3 lα,Sj )
1p(xij) cancelled with itself from the denominator since it does not depend on wj nor α. This would have
been different for a generative approach.
2This can be done through simply minimizing the cross entropy of a neural network with L2 regularization.
2
Under review as a conference paper at ICLR 2019
2.2	Reduction of the Evidence Lower Bound.
In the previous section, we described the different components for expressing the posterior distribution
of Equation 4. While all these components are tractable, the normalization factor is still intractable.
To address this issue, we follow the Variational Bayes approach (Blundell et al., 2015).
Conditioning on α, we saw in Equation 1 that the posterior factorizes independently for all tasks.
This reduces the joint Evidence Lower BOund (ELBO) to a sum of individual ELBO for each task.
Given a family of distributions qθj(zj |Sj, α), parameterized by {θj}jN=1 and α, the Evidence Lower
Bound for task j is:
ln p(Sj) ≥
nj
q(Zj,WE∣Sj,α) H ln p(yijM,wj ) - KLj
nj
E	ln p(yij|xij, hα(zj)) - KLj
qθj(zj |Sj ,α) i=1
ELBOj,
where,
KLj = KL[q(zj,wjISj,α) Il P(Zj,wjIa)]
= E	E in qθj(zj∖sj，a) δhα(Zj)-Wj
qθj(zj lSj Q)q(WjIzj ,α)	p(zj|a)	δhα(zj )-wj
= E in qθj(zj ISj,a)
qθj(zj |Sj ,α)	P(ZjIa)
= KL qθj(z j ISj, α)	p(zjIα)
(5)
(6)
Notice that after simplification3, KLj is no longer over the space of wj but only over the space Zj.
Namely, the posterior distribution is factored into two components, one that is task specific and one
that is task agnostic and can be shared with the prior. This amounts to finding a low dimensional
manifold in the parameter space where the different tasks can be distinguished. Then, the posterior
p(ZjISj, a) only has to model which of the possible tasks are likely, given observations Sj instead of
modeling the high dimensional p(wj ISj, a).
But, most importantly, any explicit reference to w has now vanished from both Equation 5 and
Equation 6. This simplification has an important positive impact on the scalability of the proposed
approach. Since we no longer need to explicitly calculate the KL on the space of w, we can simplify
the likelihood function to p(yij Ixij, Zj, a), which can be a deep network parameterized by a, taking
both xij and Zj as inputs. This contrasts with the previous formulation, where hα(Zj ) produces all
the weights of a network, yielding an extremely high dimensional representation and slow training.
2.3	Posterior Distribution
For modeling qθj(zj |Sj, a), We can use N(μj∙, σj-), where μj∙ and σj∙ can be learned individually
for each task. This, however limits the posterior family to express a single mode. For more flexibility,
we also explore the usage of more expressive posterior, such as Inverse Autoregressive Flow (IAF)
(Kingma et al., 2016) or Neural Autoregressive Flow (Huang et al., 2018). This gives a flexible
tool for learning a rich variety of multivariate distributions. In principle, we can use a different IAF
for each task, but for memory and computational reasons, we use a single IAF for all tasks and we
condition4 on an additional task specific context cj .
Note that with IAF, we cannot evaluate qθj(Zj ISj, a) for any values of Z efficiently, only for these
which we just sampled, but this is sufficient for estimating the KL term with a Monte-Carlo approxi-
3We can justify the cancellation of the Dirac delta functions by instead considering a Gaussian with finite
variance, . For all > 0, the cancellation is valid, so letting → 0, we recover the result.
4We follow the architecture proposed in Kingma et al. (2016).
3
Under review as a conference paper at ICLR 2019
mation i.e.:
nmc
KLj ≈ ——Eln qθj (Zji)ISj,α) - lnN (Zji)10,1),
nmc i=1
where Zji)〜qθj(zj |Sj ,α). It is common to approximate KLj with a single sample and let the
mini-batch average the noise incurred on the gradient. We experimented with nmc = 10, but this did
not significantly improve the rate of convergence.
2.4	Training Procedure
In order to compute the loss proposed in Equation 5, we would need to evaluate every sample of
every task. To accelerate the training, we use a Monte-Carlo approximation as is commonly done
through the mini-batch procedure. First we replace summations with expectations:
N	nj
ELBO = X ZjEqjXln p(yij |xij , zj ) - KLj
j=1 j qj i=1
= E N nj E E ln p(yij |xij , zj ) - KLj	(7)
j〜UN	∖	Zj ~qj i~Unj	I
Now it suffices to approximate the gradient with nmb samples across all tasks. Thus, we simply
concatenate all datasets into a meta-dataset and added j as an extra field. Then, we sample uniformly5
nmb times with replacement from the meta-dataset. Notice the term nj appearing in front of the
likelihood in Equation 7, this indicates that individually for each task it finds the appropriate trade-off
between the prior and the observations. Refer to Algorithm 1 for more details on the procedure.
1:	for i in 1 .. nmb :
2:	sample x, y and j uniformly from the meta dataset
3:	Zj,ln q(Zj) = IAFa(μj∙, σj, Cj)
4:	KLj ≈ lnq(Zj) - lnN(Zj |0, Idz)
5:	Li = nj ln p(y |x, Zj , α) + KLj
Algorithm 1: Calculating the loss for a mini-batch
3	Extending to 3 Level of Hierarchies
Deep prior gives rise to a very flexible way to transfer knowledge from multiple tasks. However,
there is still an important assumption at the heart of deep prior (and other VAE-based approach such
as Edwards and Storkey (2016)): the task information must be encoded in a low dimensional variable
Z . In Section 5, we show that it is appropriate for regression, but for image classification, it is not the
most natural assumption. Hence, we propose to extend to a third level of hierarchy by introducing
a latent classifier on the obtained representation. This provides a simple way to enhance existing
algorithm such as Prototypical Networks (Proto Net) (Snell et al., 2017).
In Equation 5, for a given6 task j, we decomposed the likelihood p(S|z) into Qin=1 p(yi|xi, z) by
assuming that the neural network is directly predicting p(yi|xi, z). Here, we introduce a latent
variable v to make the prediction p(yi|xi, v). This can be, for example, a Gaussian linear regression
on the representation φα(x, Z) produced by the neural network. The general form now factorizes as
follow: p(S|z) = E i p(yi|v, xi)p(xi), which is commonly called the marginal likelihood.
V 〜p(v | z)
To compute ELBOj in 5 and update the parameters α, the only requirement is to be able to compute
the marginal likelihood p(S|z). There are closed form solutions for, e.g., linear regression with
Gaussian prior, but our aim is to compare with algorithms such as Prototypical Networks on a
5We also explored a sampling scheme that always make sure to have at least k samples from the same task.
The aim was to reduce gradient variance on task specific parameters but, we did not observed any benefits.
6We removed j from equations to alleviate the notation.
4
Under review as a conference paper at ICLR 2019
classification benchmark. Alternatively, we can factor the marginal likelihood as follow p(S|z) =
in=1 p(yi|xi, S0..i-1, z). If a well calibrated task uncertainty is not required, one can also use a
leave-one-out procedure Qin=1 p(yi|xi, S \ {xi, yi}, z). Both of these factorizations correspond to
training n times the latent classifier on a subset of the training set and evaluating on a sample left
out. We refer the reader to Rasmussen (2004, Chapter 5) for a discussion on the difference between
leave-one-out cross-validation and marginal likelihood.
For a practical algorithm, we propose a closed form solution for leave-one-out in prototypical
networks. In its standard form, the prototypical network produces a prototype ck by averaging all
representations Yi = φα(xi, Z) of class k i.e. Ck = ∣~K∣ Pii∈κ Yi, where K = {i : yi = k}. Then,
predictions are made using p(y = k|x, α, Z) H exp (- ∣∣Ck - Yik2).
Theorem 1. Let ck-i ∀k be the prototypes computed without example xi, yi in the training set. Then,
(IKK-1 kck - γik2,
∣ck - Yi∣2,
ifyi = k
otherwise
(8)
We defer the proof to supplementary materials. Hence, we only need to compute prototypes once
and rescale the Euclidean distance when comparing with a sample that was used for computing the
current prototype. This gives an efficient algorithm with the same complexity as the original one and
a good proxy for the marginal likelihood.
4	Related Work
Hierarchical Bayes algorithms for multitask learning has a long history (DaUme III, 2009; Wan et al.,
2012; Bakker and Heskes, 2003). However most of the literature focuses on simple statistical models
and does not consider transferring on new tasks.
More recently, Edwards and Storkey (2016) and Bouchacourt et al. (2017) explore hierarchical
Bayesian inference with neural networks and evaluate on new tasks. Both papers use a two-level
Hierarchical VAE for modeling the observations. While similar, our approach differs in a few different
ways. We use a discriminative approach and focus on model uncertainty. We show that we can obtain
a posterior on Z without having to explicitly encode Sj . We also explore the usage of more complex
posterior family such as IAF. these differences make our algorithm simpler to implement, and easier
to scale to larger datasets. Other works consider neural networks with latent variables (Tang and
Salakhutdinov, 2013; Depeweg et al., 2017; Turner et al., 2018) but does not explore the ability to
learn across multiple tasks.
Some recent works on meta-learning are also targeting transfer learning from multiple tasks. Model-
Agnostic Meta-Learning (MAML) (Finn et al., 2017) finds a shared parameter θ such that for a given
task, one gradient step on θ using the training set will yield a model with good predictions on the
test set. Then, a meta-gradient update is performed from the test error through the one gradient
step in the training set, to update θ. This yields a simple and scalable procedure which learns to
generalize. Recently Grant et al. (2018) considers a Bayesian version of MAML. Additionally, (Ravi
and Larochelle, 2016) also consider a meta-learning approach where an encoding network reads the
training set and generates the parameters of a model, which is trained to perform well on the test set.
Finally, some recent interest in few-shot learning give rise to various algorithms capable of transferring
from multiple tasks. Many of these approaches (Vinyals et al., 2016; Snell et al., 2017) find a
representation where a simple algorithm can produce a classifier from a small training set. Bauer
et al. (2017) use a neural network pre-trained on a standard multi-class dataset to obtain a good
representation and use classes statistics to transfer prior knowledge to new classes.
5	Experimental Results
Through experiments, we want to answer i) Can deep prior learn a meaningful prior on tasks? ii) Can
it compete against state of the art on a strong benchmark? iii) In which situations does deep prior and
other approaches fail?
5
Under review as a conference paper at ICLR 2019
5.1	Regression on one dimensional Harmonic signals
To gain a good insight into the behavior of the prior and posterior, we choose a collection of one
dimensional regression tasks. We also want to test the ability of the method to learn the task and not
just match the observed points. For this, we will use periodic functions and test the ability of the
regressor to extrapolate outside of its domain.
Specifically, each dataset consists of (x, y) pairs (noisily) sampled from a sum of two sine waves with
different phase and amplitude and a frequency ratio of 2: f (x) = aι sin(ω∙x+b1)+a2 sin(2∙ω∙x+b2),
where y 〜N(f (x), σ22). We construct a meta-training set of 5000 tasks, sampling ω 〜U(5, 7),
(b1,b2)〜U(0, 2∏)2 and (a1,a2)〜N(0,1)2 independently for each task. To evaluate the ability
to extrapolate outside of the task’s domain, we make sure that each task has a different domain.
Specifically, X values are sampled according to N(μχ, 1), where μχ is sample from the meta-domain
U (-4, 4). The number of training samples ranges from 4 to 50 for each task and, evaluation is
performed on 100 samples from tasks never seen during training.
Model Once z is sampled from IAF, we simply concatenate it with x and use 12 densely connected
layers of 128 neurons with residual connections between every other layer. The final layer linearly
projects to 2 outputs μy and s, where S is used to produce a heteroskedastic noise, σy = Sigmoid(s) ∙
0.1 + 0.001. Finally, We use p(y∣x, Z) = N(μy(x, z), σy(x, z)2) to express the likelihood of the
training set. To help gradient flow, we use ReLU activation functions and Layer Normalization7 (Ba
et al., 2016).
Results Figure 1a depicts examples of tasks with 1, 2, 8, and 64 samples. The true underlying
function is in blue while 10 samples from the posterior distributions are faded in the background.
The thickness of the line represent 2 standard deviations. The first plot has only one single data point
and mostly represents samples from the prior, passing near this observed point. Interestingly, all
samples are close to some parametrization of Equation 5.1. Next with only 2 points, the posterior is
starting to predict curves highly correlated with the true function. However, note that the uncertainty
is over optimistic and that the posterior failed to fully represent all possible harmonics fitting these
two points. We discuss this issue more in depth in supplementary materials. Next, with 8 points, it
managed to mostly capture the task, with reasonable uncertainty. Finally, with 64 points the model is
certain of the task.
To add a strong baseline, we experimented with MAML (Finn et al., 2017). After exploring a variety
of values for hyper-parameter and architecture design we couldn’t make it work for our two harmonics
meta-task. We thus reduced the meta-task to a single harmonic and reduced the base frequency range
by a factor of two. With these simplifications, we managed to make it converge, but the results are
far behind that of deep prior even in this simplified setup. Figure 1b shows some form of adaptation
with 16 samples per task but the result is jittery and the extrapolation capacity is very limited. these
results were obtained with a densely connected network of 8 hidden layers of 64 units8, with residual
connections every other layer. The training is performed with two gradient steps and the evaluation
with 5 steps. To make sure our implementation is valid, we first replicated their regression result with
a fixed frequency as reported in (Finn et al., 2017).
Finally, to provide a stronger baseline, we remove the KL regularizer of deep prior and reduced
the posterior qθj(zj |Sj ,α) to a deterministic distribution centered on μj. The mean square error is
reported in Figure 2 for an increasing dataset size. This highlights how the uncertainty provided by
deep prior yields a systematic improvement.
5.2	Mini-Imagenet Experiment
Vinyals et al. (2016) proposed to use a subset of Imagenet to generate a benchmark for few-shot
learning. Each task is generated by sampling 5 classes uniformly and 5 training samples per class, the
remaining images from the 5 classes are used as query images to compute accuracy. The number of
unique classes sums to 100, each having 600 examples of84 × 84 images. To perform meta-validation
and meta-test on unseen tasks (and classes), we isolate 16 and 20 classes respectively from the original
7Layer norm only marginally helped.
8We also experimented with various other architectures.
6
Under review as a conference paper at ICLR 2019
(a) Deep Prior
Figure 1: Preview of a few tasks (blue line) with increasing amount of training samples (red dots).
Samples from the posterior distribution are shown in semi-transparent colors. The width of each
samples is two standard deviations (provided by the predicted heteroskedastic noise).
(b) MAML
increasing dataset size
model
without kl
with kl
N samples
Figure 2: left: Mean Square Error on increasing dataset size. The baseline corresponds to the same
model without the KL regularizer. Each value is averaged over 100 tasks and 10 different restart.
right: 4 sample tasks from the Synbols dataset. Each row is a class and each column is a sample
from the classes. In the 2 left tasks, the symbol have to be predicted while in the two right tasks, the
font has to be predicted.
set of 100, leaving 64 classes for the training tasks. This follows the procedure suggested in Ravi and
Larochelle (2016).
The training procedure proposed in Section 2 requires training on a fixed set of tasks. We found that
1000 tasks yields enough diversity and that over 9000 tasks, the embeddings are not being visited
often enough over the course of the training. To increase diversity during training, the 5 × 5 training
and test sets are re-sampled every time from a fixed train-test split of the given task9 .
We first experimented with the vanilla version of deep prior (2). In this formulation, we use a ResNet
(He et al., 2016) network, where we inserted FILM layers (Perez et al., 2017; de Vries et al., 2017)
between each residual block to condition on the task. Then, after flattening the output of the final
convolution layer and reducing to 64 hidden units, we apply a 64 × 5 matrix generated from a
transformation of z . Finally, predictions are made through a softmax layer. We found this architecture
to be slow to train as the generated last layer is noisy for a long time and prevent the rest of the
9If the train and test split is not fixed for a given task, one could leak the test information through the task
embeddings across different resampling of the task.
7
Under review as a conference paper at ICLR 2019
Accuracy	
Matching Networks (VinyalS et al., 2016)	60.0 %
Meta-Learner (Ravi and Larochelle, 2016)	60.6 %
MAML (finn et al., 2017)	63.2%
Prototypical Networks (Snell et al., 2017)	68.2 %
SNAIL (mishra et al., 2018)	68.9 %
Discriminative k-shot (Bauer et al., 2017)	73.9 %
adaResNet (MUnkhdalai et al., 2018)	71.9 %
Deep Prior (Ours)	62.7 %
Deep Prior + Proto Net (Ours)	74.5 %
Table 1: Average classification accuracy on
5-shot Mini-Imagenet benchmark.
	5-way, 5-shot Mini-Imagenet	4-way, 4-shot Synbols
Proto Net (ours)	68.6 ± 0.5%^^	69.6 ± 0.8%
+ ResNet(12)	72.4 ± 1.0%	76.8 ± 0.4%
+ Conditioning	72.3 ± 0.6%	80.1 ± 0.9%
+ Leave-One-Out	73.9 ± 0.4%	82.7 ± 0.2%
+ KL	74.5 ± 0.5%	83.5 ± 0.4%
Table 2: Ablation Study of our model. Accuracy is
shown with 90% confidence interval over bootstrap
of the validation set.
network to learn. Nevertheless, we obtained 62.6% accuracy on Mini-Imagenet, on par with many
strong baselines.
To enhance the model, we combine task conditioning with prototypical networks as proposed in
Section 3. This approach alleviates the need to generate the final layer of the network, thus accelerating
training and increasing generalization performances. While we no longer have a well calibrated
task uncertainty, the KL term still acts as an effective regularizer and prevents overfitting on small
datasets10. With this improvement, we are now the new state of the art with 74.5% (Table 1). In
Table 2, we perform an ablation study to highlight the contributions of the different components of
the model. In sum, a deeper network with residual connections yields major improvements. Also,
task conditioning does not yield improvement if the leave-one-out procedure is not used. Finally, the
KL regularizer is the final touch to obtain state of the art.
5.3	Heterogeneous Collection of Tasks
In Section 5.2, we saw that conditioning helps, but only yields a minor improvement. This is due to
the fact that Mini-Imagenet is a very homogeneous collection of tasks where a single representation
is sufficient to obtain good results. To support this claim, we provide a new benchmark11 of synthetic
symbols which we refer to as Synbols. Images are generated using various font family on different
alphabets (Latin, Greek, Cyrillic, Chinese) and background noise (Figure 2, right). For each task
we have to predict either a subset of 4 font families or 4 symbols with only 4 examples. Predicting
either fonts or symbols with two separate Prototypical Networks, yields 84.2% and 92.3% accuracy
respectively, with an average of 88.3%. However, blending the two collections of tasks in a single
benchmark, brings prototypical network down to 76.8%. Now, conditioning on the task with deep
prior brings back the accuracy to 83.5%. While there is still room for improvement, this supports the
claim that a single representation will only work on homogeneous collection of tasks and that task
conditioning helps learning a family of representations suitable for heterogeneous benchmarks.
6	Conclusion
Using a variational Bayes framework, we developed a scalable algorithm for hierarchical Bayesian
learning of neural networks, called deep prior. This algorithm is capable of transferring information
from tasks that are potentially remarkably different. Results on the Harmonics dataset shows that
the learned manifold across tasks exhibits the properties of a meaningful prior. Finally, we found
that MAML, while very general, will have a hard time adapting when tasks are too different. Also,
we found that algorithms based on a single image representation only works well when all tasks can
succeed with a very similar set of features. Together, these findings allowed us to reach the state of
the art on Mini-Imagenet.
References
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
10We had to cross validate the weight of the kl term and obtained our best results using values around 0.1
11Code and dataset will be provided.
8
Under review as a conference paper at ICLR 2019
B.	Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. Journal of
Machine Learning Research, 4(May):83-99, 2003.
M. Bauer, M. Rojas-Carulla, J. B. Swiatkowski, B. Scholkopf, and R. E. Turner. Discriminative
k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326, 2017.
F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learning
with stability guarantees. In Advances in Neural Information Processing Systems, pages 908-919,
2017.
C.	Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks.
arXiv preprint arXiv:1505.05424, 2015.
D.	Bouchacourt, R. Tomioka, and S. Nowozin. Multi-level variational autoencoder: Learning
disentangled representations from grouped observations. arXiv preprint arXiv:1705.08841, 2017.
A. Damianou and N. Lawrence. Deep gaussian processes. In Artificial Intelligence and Statistics,
pages 207-215, 2013.
H. Daume III. Bayesian multitask learning with latent hierarchies. In Proceedings ofthe Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence, pages 135-142. AUAI Press, 2009.
H. de Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin, and A. Courville. Modulating early visual
processing by language. In Advances in Neural Information Processing Systems, pages 6597-6607,
2017.
S. Depeweg, J. M. Herndndez-Lobato, F. Doshi-Velez, and S. Udluft. Uncertainty decomposition in
bayesian neural networks with latent variables. arXiv preprint arXiv:1706.08495, 2017.
H.	Edwards and A. Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185, 2016.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In Proc. International Conference on Machine Learning, pages 1126-1135, 2017.
Y. Gal, R. Islam, and Z. Ghahramani. Deep bayesian active learning with image data. arXiv preprint
arXiv:1703.02910, 2017.
I.	Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pages 2672-2680, 2014.
E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting gradient-based meta-learning as
hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational
information maximizing exploration. In Advances in Neural Information Processing Systems,
pages 1109-1117, 2016.
C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville. Neural autoregressive flows. arXiv preprint
arXiv:1804.00779, 2018.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
D. P. Kingma, T. Salimans, and M. Welling. Improving variational inference with inverse autoregres-
sive flow. arXiv preprint arXiv:1606.04934, 2016.
J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan,
T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.
Proceedings of the National Academy of Sciences, 114(13):3521-3526, 2017.
9
Under review as a conference paper at ICLR 2019
D. Krueger, C.-W. Huang, R. Islam, R. Turner, A. Lacoste, and A. Courville. Bayesian hypernetworks.
arXiv preprint arXiv:1710.04759, 2017.
C. Louizos and M. Welling. Multiplicative normalizing flows for variational bayesian neural networks.
arXiv preprint arXiv:1703.01961, 2017.
S.	Mandt, M. D. Hoffman, and D. M. Blei. Stochastic gradient descent as approximate bayesian
inference. The Journal ofMachine Learning Research, 18(1):4873-4907, 2017.
N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In ICLR,
2018.
T.	Munkhdalai, X. Yuan, S. Mehri, and A. Trischler. Rapid adaptation with conditionally shifted
neurons. In ICML, 2018.
E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general
conditioning layer. arXiv preprint arXiv:1709.07871, 2017.
C. E. Rasmussen. Gaussian processes in machine learning. In Advanced lectures on machine learning,
pages 63-71. Springer, 2004.
S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. 2016.
J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks for few-shot learning. In Advances in
Neural Information Processing Systems, pages 4080-4090, 2017.
J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning
algorithms. In Advances in neural information processing systems, pages 2951-2959, 2012.
S. Sun, C. Chen, and L. Carin. Learning structured weight uncertainty in bayesian neural networks.
In Artificial Intelligence and Statistics, pages 1283-1292, 2017.
Y. Tang and R. R. Salakhutdinov. Learning stochastic feedforward neural networks. In Advances in
Neural Information Processing Systems, pages 530-538, 2013.
R. Turner, T. Bui, Y. Li, and N. Cuong. Variational continual learning. 2018.
O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems, pages 3630-3638. 2016.
J. Wan, Z. Zhang, J. Yan, T. Li, B. D. Rao, S. Fang, S. Kim, S. L. Risacher, A. J. Saykin, and
L. Shen. Sparse bayesian multi-task learning for predicting cognitive outcomes from neuroimaging
measures in alzheimer’s disease. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on, pages 940-947. IEEE, 2012.
G. Zhang, S. Sun, D. Duvenaud, and R. Grosse. Noisy natural gradient as variational inference. arXiv
preprint arXiv:1712.02390, 2017.
7 Appendix
7.1	Proof of Leave-One-Out
Theorem 1. Let ck-i ∀k be the prototypes computed without example xi, yi in the training set. Then,
IKKI Ilck - φα(Xi) k2, if Iyi = k	(9)
kck - φα(xi)k2,	otherwise
Ick-i - φα(xi)I2 =
10
Under review as a conference paper at ICLR 2019
Proof. Let γi = φα(xi), n = |K| and assume yi = k then,
Yi - c-i = Yi - n11	E	Yj
j∈K∧j6=i
Yi - n11 I ∑	Yj + Yi - Yi) n-1 n--i
j∈K∧j6=i
γi 1 +
—
nnι	n fij
j∈K
n--i (Yi - Ck).
When yi 6= k, the result is trivially γi - ck-i = γi - ck.
(10)
(11)
(12)
(13)
□
7.2	Limitations of IAF
Figure 3: top: True function in the original space with 2 observed data points. middle: True posterior
distribution, where the orange dot corresponds to the location of the true underlying function. bottom:
Samples from IAF’s learned posterior.
When experimenting with the Harmonics toy dataset in Section 5.1, we observed issues with re-
peatability, most likely due to local minima. We decided to investigate further on the multimodality
of posterior distributions with small sample size and the capacity of IAF to model them. For this
purpose we simplified the problem to a single sine function and removed the burden of learning the
prior. The likelihood of the observations is defined as follows:
f (x) = sin(5(ω ∙ x + b)); y 〜N(f (x), σ2),
where σy = 0.1 is given and p(ω) = p(b) = N(0, 1). Only the frequency ω and the bias b are
unknown12, yielding a bi-dimensional problem that is easy to visualize and quick to train. We use a
dataset of 2 points at x = 1.5 and x = 3 and the corresponding posterior distribution is depicted in
Figure 3-middle, with an orange point at the location of the true underlying function. Some samples
from the posterior distribution can be observed in Figure 3-top.
We observe a high amount of multi-modality on the posterior distribution (Figure 3-middle). Some
of the modes are just the mirror of another mode and correspond to the same functions e.g. b + 2π
or -f ; b + π. But most of the time they correspond to different functions and modeling them
is crucial for some application. The number of modes varies a lot with the choice of observed
dataset, ranging from a few to several dozens. Now, the question is: "How many of those modes
can IAF model?". Unfortunately, Figure 3-bottom reveals poor capability for this particular case.
After carefully adjusting the hyperparameters13 of IAF, exploring different initialization schemes and
running multiple restarts, we rarely capture more than two modes (sometimes 4). Moreover, it will
not be able to fully separate the two modes. There is systematically a thin path of density connecting
each modes as a chain. With longer training, the path becomes thinner but never vanishes and the
magnitude stays significant.
12We scale ω and b by a factor of 5 so that the range of interesting values fits well in the interval (-1, 1). This
makes it more approachable by IAF.
1312 layers with 64 hidden units MADE network for each layer, learned with Adam at a learning rate of
0.0002.
11