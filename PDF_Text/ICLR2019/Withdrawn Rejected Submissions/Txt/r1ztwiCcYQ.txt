Under review as a conference paper at ICLR 2019
Variational SGD: dropout, generalization and
CRITICAL POINT AT THE END OF CONVEXITY
Anonymous authors
Paper under double-blind review
Ab stract
The goal of the paper is to propose an algorithm for learning the most generaliz-
able solution from given training data. It is shown that Bayesian approach leads
to a solution that dependent on statistics of training data and not on particular
samples. The solution is stable under perturbations of training data because it is
defined by an integral contribution of multiple maxima of the likelihood and not
by a single global maximum. Specifically, the Bayesian probability distribution
of parameters (weights) of a probabilistic model given by a neural network is es-
timated via recurrent variational approximations. Derived recurrent update rules
correspond to SGD-type rules for finding a minimum ofan effective loss that is an
average of an original negative log-likelihood over the Gaussian distributions of
weights, which makes it a function of means and variances. The effective loss is
convex for large variances and non-convex in the limit of small variances. Among
stationary solutions of the update rules there are trivial solutions with zero vari-
ances at local minima of the original loss and a single non-trivial solution with
finite variances that is a critical point at the end of convexity of the effective loss
in the mean-variance space. At the critical point both first- and second-order gra-
dients of the effective loss w.r.t. means are zero. The empirical study confirms that
the critical point represents the most generalizable solution. While the location of
the critical point in the weight space depends on specifics of the used probabilistic
model some properties at the critical point are universal and model independent.
1 Introduction
Finding a generalizable solution is a critical problem for any machine learning task. The ultimate
goal of learning from the available ground truths is to make a good prediction for new data. The
Bayesian method is a very powerful approach that gives a probabilistic measure of the ability of a
proposed model to predict by estimating how well the model predicts known data.
The accuracy of the predictions depends on how the found solution is able to overcome a sampling
bias to avoid overfitting for given particular samples of training data.
Specifically, in Bayesian method predictions of labels y for an input x are made by using a proba-
bilistic model, for certainty a neural network, which defines a function parametrized by weights w
that allows computing probabilities P (y|x, w) for each weight point. Each weight point contributes
to predicted probabilities of labels P rob(y|x) in accordance with probability distribution of weights.
The distribution of weights is learned from a known training data set {xn, yn; n = 1..N} and its
prior probability distribution P0 (w) in the following way:
NN
P rob(y|x) =	P (y|x, w)P0(w) Y P(yn|xn,w)/	P0(w) Y P (yn|xn, w)	(1)
w	n=1	w	n=1
Here the predicted probability P rob(y|x) is an average of the model probability P (y|x, w) at a
weight w over the learned weight distribution. To make predictions we are only interested in a
method that allows to find averages in eq.(1) and not absolute values of the integrals. According
to mean value theorem (Cauchy (1813), also in Encyclopedia of Mathematics Hazewinkel (1994))
values of the averages can be represented by a single point, which in our case means that there
1
Under review as a conference paper at ICLR 2019
is a single point in the weight space w0 that represents a result of computing the integrals, so
P rob(y|x) = P (y|x, w0). That point w0 is a solution of the training of the neural network.
A standard approach to get the solution is a maximum likelihood method that finds a maximum
of the integrand. However, there are some cases when the maximum likelihood fails to represent
main contribution to the integral by weights. Consider this example: if log-likelihood for N data
samples has a maximum at some weight point w1, then in general its first derivative by weights is
zero, second derivative is negative and proportional to N, so corresponding Gaussian integral by the
weights is proportional to N -d/2 , where d is number of weights. This will change if there is a flat
maximum, which has not only first but also second and third derivatives equal to zero. In this case
the integral is proportional to N-d/4. For large number of samples the flat maximum makes the
most significant contribution to the integral by weights:
I1
P1N
w
e-NC2w2
(X PN × NTl/,I2
P2N
w
e-NC4w4
X P2N × N-d/4,
and I2/I1 χ (P2/Pi)n X Nd/4. For a typical case when the number of weights d 〜N and average
sample probabilities at maxima are comparable O(P1)〜O(P2) the integral around flat maximum
I2 is always bigger than the integral around narrow maximum I1 , unless P2 is zero.
While in general a likelihood has a number of regular local maxima and no flat maximum the effect
of integration over multiple frequent local maxima can result in an effective flat maximum that
defines a solution.
We argue that any local or global maximum of likelihood gives a wrong solution that is not gen-
eralizable and so makes inaccurate predictions, because the locations for the global maximum and
local maxima depend on specific samples in the training data and any modification of it by adding
or removing samples will change the solution (Zhang et al., 2017). Instead we will show that there
is another solution that more associated with properties of the distribution of training data and less
with particular samples.
The purpose of this paper is to show that the effective flat maximum always exists for specific pa-
rameters of prior weight distribution P0(w) (regularization parameters) and corresponding solution
is the most generalizable solution that can be found in training. We show that the solution is a critical
point in an effective loss that represents the result of integration over the weights.
In the next sections we derive the algorithm for the optimizer for finding the critical point solution
and analyze properties of the solutions. The empirical study is outside of the scope of the paper and
will be presented separately.
For simplicity we use same notations for a vector of weights and its components, as well as corre-
sponding parameters of distributions of weights because all weight components are independent in
our consideration and it is clear from context when it is a vector or its component.
2 Method for computing the integrals
We use a recurrent approach for estimating the integrals. First, we represent a probability of each
training sample as a product of factors close to one, P (y|x, w) = (1 + 1/T lnP(y|x, w))T, where
free parameter T 1 is a number of epochs:
Z P0(w) Y P (yn∣Xn,w)= Z P0(w) Y(1 + Tln P 3n∣Xn,w))
w	n=1	w	n=1	T
then model each factor as a product of Gaussian distributions Q(w∣μ, σ) one for each component of
weight vector.
For each iteration a running prior distribution of weights is updated by absorbing a single factor to
produce a new prior
/	r	X	(w-μ)2
(	1	∖	e 2σ2
Qt(w) I 1 + T lnP(y∣x,w) I → Qt+ι(w), WithQ0(w) = P0(w)andQ(w∣μ,σ) = √__2.
2
Under review as a conference paper at ICLR 2019
Specifically, we do the following:
First, let’s enumerate all factors for all data samples
N	1	T N×T	1
U(1 + T ln P (yn|xn,w) ) = Π ( 1 + T ln P(yt|xt,W)
n=1	t=1
Then, under the integral by weights we use an identical re-writing for a product of a prior Qt (w)
and one of the factors and a new prior Qt+1 (w) and normalization factor Nt
ZwQt(W) (1+Tln P(yt|xt, W))(C=Nt Zw [qr≡)] Qt+i(w)(...),
where normalization factor
Nt = / Qt(W) (l + TlnP(yt∣χt,w)
and distribution
Rt(W)
Qt(W)(1 + T lnP(yt∣xt,w))
Nt
Finally, we make an approximation by replacing ratio of distributions Rt (W)/Qt+ (W) by its mean
for distribution Qt+1 which is equal to 1. Then the iterations are repeated until all factors from
probabilities of data are replaced by a single final Gaussian distribution and some normalization
factor.
To minimize the introduced error on each iteration we select the means and variances of the new
prior Qt+1 to minimize variance of the ratio Rt/Qt+1 . The variance is equal to
VarQi 1 [Rt/Qt+1 ] = Z [ RWW、] Q5(w) - 1= ERJ Rt(w)、
Qt+1 [ t/Qt+1 ] Jw [Qt+ι(W)J Qt+1( )	Rt [Qt+ι(W)
-1.
The lower bound of the variance is expressible via KL divergence (Kullback & Leibler (1951)) of
Rt and Qt+1
VarQt+1 [Rt/Qt+ ] ≥ exp
Zw
Rt (W) log
Rt(W)
Qt+1(W)
- 1 = exp (DKL (Rt kQt+ )) - 1.
Finding the minimum of KL divergence is equivalent to minimizing the lower bound of the variance
of the ratio of Rt/Qt+1 which leads to equations
∂∂
∂μt+ι k L Q(w|μt，σt)
1 + TlnP(y∣x,W)) lnQ(W∣μt+ι,σt+ι)
0.
Then solving the above equations gives the update rules for means and variances of each weight
component
μt+ι = μt +
σ2 d
NT ∂μt
ln P (W)
,μt,σt
11
--------
2	2
σt2+1	σt2
1	∂2
ΝT∂μ2
ln P (W)
/ μt,σt
(2)
where averages are defined as hA(W)iμ,σ = JW Q(W∣μ, σ)A(W) and averages of gradients by
weights are equal to gradients of averages by means(岛A(w))μ 仃=∂μ ^A(w))μ 仃.
Another useful identity allows to replace second gradient of (μ, σ)-average w.r.t mean on first gra-
dient by variance
∂	1 ∂2
∂σ2 hA(w)i”,σ = 2 ∂μ2 hA(w)i”,σ.
(3)
3
Under review as a conference paper at ICLR 2019
For a full batch the log of probability of data in eqs.(2) ln P(w) = PnN lnP(yn|xn, w), while for a
minibatch the sum goes over the size of the minibatch.
In eqs.(2) We used rescaled variances σ2 → σ2/N so all gradients are normalized per data sample.
With the rescaled variances the prior distribution of weights is a product over all weight dimensions
d
d
Po(w∣μo,σo) = Nd/2 Y
i=1
--N (Wi-μi,o)2
e	2σ2,0
、J2*,。
The index t enumerates iterations over all minibatches in all epochs. Then for a minibatch size one
the total number of iterations equals to a number of epochs T times number of data samples N in a
training set.
By recursively applying the update rules in eqs.(2) over N samples and T epochs We obtain the
approximation of Bayesian integrals that alloWs to compute averages
ZN	1 tmax
PO(W) Y P (yn∖χn,w)(...) ≈ J Qtmax(W)(...) × exP (T X J Qt(W)In Pt(W)).
It is important to emphasize that the averages are defined by distribution Qtmax (W) after a finite
number of iterations tmax , Which for the minibatch one is equal to a product N × T and not by
distribution at the infinite number of iterations Q∞(W). Number of epochs T controls the accuracy
of the factorized representation and in practical computing any T larger than 10 or 100 is good
enough.
The prediction probability in eq.(1) is defined by weight w。that is a mean μtmax of distribution
Qt
max (w)
Prob(y∖χ) ≈ P(y∖x, μtmax).
That mean μtmax is a final point of iterations in eqs.(2). The trajectory in mean-variance space is
defined by starting point (μo, σ0), the mean and variance of a prior distribution of weights in eq.(1)
which are regularization parameters: Pb(W) = Q(w∖μo, σo).
3	Results
Before going into detailed analysis of eqs.(2) let’s formulate the results in the form of the following
statements.
The update rules above are solving SGD-type optimization problem for an effective loss given
by Gaussian average L(μ, σ) = 一 Jw Q(W∣μ, σ) Pn lnP(yn\xn, W) for each mean-variance point
(μ,σ).
The following statements have been proved:
1.	The effective loss L(μ, σ) is convex for large variances σ2. In particular, it is true for any neural
network with ReLU activations and L1, L2 or cross entropy loss functions. For a convex effective
loss its second-order gradient by mean is positive.
2.	The effective loss is converging to the original loss in the limit of small variances where it is
generally non-convex (excluding completely trivial linear cases).
3.	For each mean there is a critical variance σc2 that separates convex effective loss from non-convex.
At the critical variance second-order gradient of effective loss by mean is zero.
4.	There are trivial stationary solutions of the update rules that correspond to zero variances and zero
gradients of loss w.r.t. weights. These solutions are unstable when training set is modified because
they correspond to narrow minima that are changing drastically as data change (Zhang et al., 2017).
Because second-order gradients by weights are large and positive, changes in loss are second-order
by weights and changes in solutions are at least linear and often result in jumps to a new location.
4
Under review as a conference paper at ICLR 2019
5.	There is a non-trivial stationary solution at critical point at the end of convexity where both first-
order and second-order gradients of effective loss are zero. That solution is much less sensitive to
changes in data. It is responding by cubic changes in loss and quadratic changes in solutions and due
to convexity there is no jumps. For that reason the critical point solution is the most generalizable
solution as well as most stable against adversarial perturbations (Goodfellow et al., 2014).
6.	Trajectories in mean-variance space that follow from update rules show universal behavior in
vicinity of the critical point. Analysis shows that not all trajectories are converging to the critical
point. Typical trajectory that starts in convex area moves toward the critical point until it may cross
to non-convex area then it moves away from critical point and finally ends as a trivial solution in a
local minimum.
7.	Approximating Gaussian averaging by sampling in weight space results in dropout method also
known as ”fast dropout” (Wang & Manning, 2013) and ”dropconnect” (Wan et al., 2013). The update
rules completely define the dropout rate for each weight component. With averaging via dropout the
critical point and the convexity of effective loss exist only in an unreachable limit, nevertheless it
allows to find a solution that is close to the critical point.
4	Analysis of a simple two-minimum model
To understand better an effect of averaging let’s consider a simple polynomial case where loss l(w)
as a function of weights has two symmetrical minima at points (-a, +a):
l(w) = (w2 - a2)2.
Then the effective loss is Gaussian average of the loss above L(μ, σ) = (l(w)iμ,σ, specifically
— — (w-μ*	2	2	2
L(μ,σ) =	/	2 (w2 - a2)2=(μ2—a2)2+12σ2 (μ^+~ι r).	(4)
w	2πσ2	2	4	6
And the first and second gradients of the effective loss are
∂μ = 4μ (μ2 — a2 + 3σ2) ,	IdL = 忤2 + 4 " — a2) ∙
(5)
When variance is large, σ2 > a2∕3, second gradient of the effective loss L w.r.t. μ is positive and
there is only one minimum at μ = 0. When variance is small σ2 < a2∕3, the effective loss has a
maximum at μ = 0 and two minima at μ = ±ʌ/a2 - 3σ2.
When σ2 = a2/3 there is a critical point at μ = 0 where both first and second gradients of the
effective loss w.r.t. μ are zero: ∂μ = 0, ∂∂μ2 = 0.
By using the update rules from eqs.(2) where average log of probability is equal to the negative
effective loss L = —hln Pi with first and second gradients defined above in eqs.(5) we can consider
trajectories in the mean-variance space:
_	σ2 ∂L(μt,σt)	11	1 ∂2L(μt,σt)
μt+1	μt	T	∂μt	,	σ2+ι	σ2 T	dμ2
We can see that the critical point is a saddle point in the mean-variance space. Any trajectory that is
missing a critical point after an infinite number of iterations ends in a local minimum ofan effective
loss with zero variance σ2, which is an original local minimum.
There are only two trajectories starting with μ = 0 from convex area (3σ2 > a2) and non-convex
area (3σ2 < a2 ) that after a large enough number of iterations will always converge to the critical
point.
However, for a finite number of iterations there is an area of starting points (μo,σ0) that defines
trajectories with end points arbitrary close to a critical point.
5
Under review as a conference paper at ICLR 2019
5 Analysis of a general case
Let’s consider a multilayered neural network where predictions y are computed via ReLU activations
and loss function l(y) is a convex function of predictions. For each weight w predictions y are
piecewise functions of the weight. Then second gradient of loss function by weight is
∂2l(y(w)) = ∂2l(y) (∂y(w) A 2 + ∂l(y) ∂2y(w)
∂w2	∂y2 ( ∂w ) ∂y ∂w2
There are two terms in second gradient of loss by weight: first is always positive due to convexity of
l(y) and second contains second gradient of predictions. Average of the first term is always positive.
Average of the second term could be positive or negative.
Due to piecewise weight dependency second gradient of predictions ∂2 y∕∂w2 is singular and not
zero, however for any network with finite number of layers predictions y are linear functions of the
weight when the value of the weight goes to infinity. For that reason second gradient of predictions
for large values of the weight is zero and second gradient of loss by weight is positive for large
weight values.
If variance of Gaussian distribution of the weight is very large then averages are defined by contri-
butions of large weight values and then average of the second gradient of loss by weight is positive.
∂ 2l(w)	∂ 2L(μ,σ)
if σ →∞,	hki",σ > 0 and ~∂^ > 0.
That proves the convexity of the effective loss for large variances.
On other side when the variance goes to zero the effective loss converges to original loss which
generally non-convex. Because an effective loss is a continuous function of the variance there exists
a critical value of the variance where second gradient of the effective loss may have zero at some
mean. That means the mean-variance plane could be divided on two convex and non-convex areas.
These areas are separated by the line on mean-variance plane where second gradient of the effective
loss w.r.t. means is zero.
The general properties of the critical point are universal due to the identity in eq.(3). Let’s consider
an expansion of the effective loss L(μ, σ) in a vicinity of a point μc, σ0 where both first gradients of
L by mean and variance are zero:
L ≈ a2 (μ - μc)2 + a3 (μ - μc)3 + b2 (σ2 - σc2)2 + c(μ - μc)(σ2 - σc2)+ . . . ,
so
∂L
∂μ
2a2 (μ - μc)+ 3a3 (μ - μc)2 + c(σ2 - σc2),
∂L
∂σ2 = 2b2(σ - σc) + c(μ - μc)
Then using the identity in equation (3) we can see that
1 ∂2L
2 ∂μ2
a2 + 3a3(μ - μc)
and a2 = 0
because zero-order term by (μ 一 μC) in 皓 is zero. Also it gives C = 3a3.
Due to the identity (3) the universal structure of the effective loss at the critical point is as follows
L ≈ a3 (μ - μc)3 + b2 (σ2 - σc2 )2 + 3a3 (μ - μc)(σ2 - σc2)+ . . .
Essentially, the condition (3) makes a regular minimum of the effective loss to be a critical point.
6
Under review as a conference paper at ICLR 2019
At critical point both first and second gradients by means of the effective loss are zero. At critical
point the effective loss is a flat function of the means. That makes the solution stable when training
data set is modified unlike any maximum log-likelihood solutions that changes when data changes.
For that reason the critical point solution is most generalizable.
6	Generalization error for a simple two-minimum model
Any available data set is a collection of samples from an unknown true distribution. The empirical
loss is defined as loss per sample for sampled data for a given weight point. The expected loss is
average over unknown true distribution of data at a given weight point.
In a two-minimum model loss for a data point is l(w) = (w2 - a2)2, where a is a sample specific
parameter. The empirical loss per sample for m samples is
m	mm
Lemp(W) = 一 ^X(W2 - an)2=w4—2w2—^X an +— ^X an,
m	n	m nm n
n=1	n=1	n=1
while the expected loss per data point for the same weight point is
Lexp (W) = E (W2 - a2)2 = W4 - 2W2E a2 + E a4 .
The generalization error is a difference between an expected loss and empirical loss for the same
solution W:
GErr(W) = Lexp(W) - Le
mp(W).
For a given weight point the expectation of the generalization error is zero: E [GErr(W)]
E [Lexp(W)] - [L
emp (W)] = 0.
However, we only can find solution for a minimum of the empirical loss.
dLemp (W)
∂w
0, Wemp
m
一 X an.
mn
n=1
Due to sampling a minimum loss solution for the empirical loss depends on specific samples. For
that reason the expectation of the generalization error for a minimum of the empirical loss solution
is not zero and is equal to
E [GErr(Wemp] = 2E
m
一X an
mn
n=1
2
2EE [(a2 - E [a2] )2].
(6)
The expectation of the generalization error is positive which reflects overfitting and goes to zero
when number of samples m goes to infinity.
Now, let’s compute generalization error for the effective loss (4) for m data samples. The solution
for empirical effective loss are found from equations
dLemp(μ, σ) = 0 dLemp(μ,σ)
dμ	, dσ
There are trivial solutions with zero variance σ0 = 0,μ0 = ml Pm=I On. These solutions have the
same expected generalization error as in eq.(6): E [GErreff (μo, σ0)] = 2E [(a2 一 E [a2] )2] /m.
And there is a critical point solution with non-zero variance: μc = 0, 3σ2 = ml Pm=I a2n . That so-
lution gives the expected generalization error that is 3 times smaller than the expected generalization
error for trivial solutions:
7
Under review as a conference paper at ICLR 2019
2
E [GErreff (μc, σj]=嬴E [(a2 - E [a2] )2] /3 = E [GErreff (μo,σ°)] /3.
This example supports the claim of the paper that the critical point solution is more generalizable
than a trivial minimum loss solution.
7	Conclusions
In the paper we consider a learning of a predictive model from training data by approximately
computing Bayesian integral over weights - the parameters of the model.
By using recurrent variational approximations with Gaussian weight distributions we are able to find
a solution - a single point in weight space that represents an effect of averaging over distribution of
weights in the Bayesian integrals.
We show that this approach leads to SGD-type optimization problem for an effective loss in mean-
variance space. For each mean-variance point the effective loss is defined by average of the log-
likelihood over Gaussian distribution at same mean-variance point. Due to averaging the effective
loss and its gradients of any order are continuous function of means even for ReLU based neural
networks.
The recurrent update rules define trajectories in mean-variance space. Starting points of the trajec-
tories are defined by regularization parameters, which are parameters of the Gaussian weight prior
in Bayesian integrals.
It is shown that there are two types of stationary solutions of the update rules. First solution type
corresponds to local minima of the original loss or maxima of the log-likelihood. Second solution
type is a critical point in mean-variance space that is a result of the integration over multiple maxima
of the log-likelihood.
At the critical point both first and second gradient of the effective loss are zero. That leads to stability
of the solution against perturbations of the training data set due to addition or removal data samples
or via creation of adversarial examples.
References
A.L. Cauchy. J. Ecole Polytechnique, 9:87-98,1813.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In eprint arXiv:1412.6572 [stat.ML]. 2014.
Michiel Hazewinkel. Cauchy theorem. Springer Science+Business Media B.V. / Kluwer Academic
Publishers, 2001 edition, 1994.
S. Kullback and R. A. Leibler. On information and sufficiency. Ann. Math. Statist., 22(1):79-86,
1951.
L. Wan, M.D. Zeiler, S. Zhang, Y. LeCun, and R. Fergus. Regularization of neural networks using
dropconnect. In ICML 2013, June 2013.
Sida Wang and Christopher Manning. Fast dropout training. In Proceedings of the 30th International
Conference on Machine Learning (ICML-13), pp. 118-126, 2013.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. In Proceedings of the International Conference on Learning Represen-
tations (ICLR), 2017.
8