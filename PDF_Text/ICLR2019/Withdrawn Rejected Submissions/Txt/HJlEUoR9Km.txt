Under review as a conference paper at ICLR 2019
Improved resistance of neural networks to
adversarial images through generative pre-
TRAINING
Anonymous authors
Paper under double-blind review
Ab stract
We train a feed forward neural network with increased robustness against adver-
sarial attacks compared to conventional training approaches. This is achieved
using a novel pre-trained building block based on a mean field description of a
Boltzmann machine. On the MNIST dataset the method achieves strong adversar-
ial resistance without data augmentation or adversarial training. We show that the
increased adversarial resistance is correlated with the generative performance of
the underlying Boltzmann machine.
1	Introduction
In an adversarial attack on a classification system one seeks inputs that lie very close (according
to some norm or measure) to a regular input, but give a completely different classification result.
Adversarial attacks on machine learning systems have been studied for over a decade, starting from
the study of attacks on spam filtering systems (for a historical review see Biggio and Roli (2018)).
Adversarial attacks on deep learning image recognition models have first been discussed by Szegedy
et al. (2013). In addition to highlighting security implications adversarial images have acquired a
new meaning in this context, presenting us with a fundamental conundrum: How can deep learning
systems succesfully generalise and at the same time be extremely vulnerable to minute changes in
the input? The topic of adversarial images in neural nets has been studied intensely with attacks
and defences leapfrogging each other (for an overview see Kurakin et al. (2018)). All common
neural network models suffer from susceptibility to adversarial attacks. It has even been shown by
Moosavi-Dezfooli et al. (2017) that adversarial attack patterns can be transferred between different
models, suggesting some common underlying mechanism. Despite the intense study of the subject
a solution remains elusive, with Jetley et al. (2018) suggesting that classification performance and
vulnerability to adversarial attacks are correlated, while Gilmer et al. (2018) present evidence for
the opposite in a simple model. The difficulty in finding neural networks robust against adversarial
attacks suggest that the standard discriminative neural network training procedure and the resulting
networks’ vulnerability are linked. Making use of generative methods (e.g variational autoencoders,
introduced in this context by Gu and Rigazio (2014)) has resulted in creating the most adversarially
robust classifications but with added computational cost at inference time (Schott et al. (2018)).
Another state of the art approach to increasing adversarial resistance applies denoising to image
patches to achieve strong robustness (Moosavi-Dezfooli et al. (2018)).
In this article we concentrate on one common generative model, Boltzmann machines, and the im-
pact of its use on adversarial resistance. We examine the adversarial resistance on the standard
MNIST dataset. Boltzmann machines are a stochastic model with binary units (see e.g. Hinton
(2007)). A number of “hidden” units form a model for the data distribution over the “visible” units.
In a classification scenario one would fix the visible units and use the probability distribution over
the hidden units for further processing. In a restricted Boltzmann machine the hidden units are in-
dependent and therefore the distribution over the hidden units in a classification scenario becomes
a product distribution fully characterised by the average values of the individual hidden units. Pre-
training using restricted Boltzmann machines has been used in early neural networks (Hinton et al.
(2006)) but was abandoned as it became clear that good training can be achieved without it. Here we
follow a slightly different path. We will use a Boltzmann machine to model image patches only. We
do the modelling in a hierarchical fashion: train one Boltzmann machine for small image patches,
1
Under review as a conference paper at ICLR 2019
then add more hidden units to model the joint distribution over larger patches, similar to the stacked
RBM approach of Norouzi et al. (2009). This forces us to depart from a simple restricted Boltzmann
machine model and therefore we cannot obtain the hidden probability distribution as easily as for a
restricted Boltzmann machine. Extended mean field methods developed for the description of spin
glasses in physics (ThoUless et al. (1977); Georges and Yedidia (1991); Kuhn and Helias (2017))
have been applied SUcceSfully to train a restricted Boltzmann machine by Gabrie et al. (2015) and
BUdzianowski (2016). We Use these methods to arrive at a description of the hidden Units in terms
of their average valUes, where now we replace the single neUral network layer resUlting from the
restricted Boltzmann machine with a many-layer iteration derived from the mean field description.
This pre-trained bUilding block can be Used as a fixed inpUt layer in a neUral network trained in the
UsUal discriminatory fashion.
In sUmmary the contribUtions of this article are as follows:
•	We constrUct a new neUral network bUilding block, based on the mean field description of a
Boltzmann machine. The resUlt is a deep feed forward neUral network with shared weights
and a strUctUre derived from the energy fUnction of the Underlying Boltzmann machine.
•	We Use this bUilding block in a neUral network to achieve state of the art resistance to
adversarial attacks on MNIST and show a correlation between the loss fUnction of the
Boltzmann machine (i.e. it’s generative capability) and the adversarial resistance of the
resUlting neUral network.
•	We show that the fUnction mapping visible Unit inpUts to hidden Unit oUtpUts implemented
by the new bUilding block is strongly dependent on the inpUt. A ball of random distortions
aroUnd a training example gets mapped to a smaller oUtpUt volUme compared to the same
mapping starting from a random inpUt.
2	B oltzmann Machines as Feature Extractors
Let Us review the the mean field description ofa Boltzmann machine (BUdzianowski (2016); Gabrie
et al. (2015)). Boltzmann machines are stochastic, energy based models derived from the statistical
physics of interacting spins. Boltzmann machines are a generative model, the generated probabil-
ity distribUtion being the thermal eqUilibriUm probability distribUtion over all possible spin com-
binations s = (s1, s2, . . . , sN) with sn = ±1, as determined by the energy fUnction throUgh its
parameters θ = (b, J)
E(θ, S) = S ∙ J ∙ S + b ∙ s,
so that the probability of realising the configUration s is proportional to the Boltzmann factor
p(θ, S) Z e-E(θ,s).
The spins are divided into two sets, the visible Units v and the hidden Units h. When training a
Boltzmann machine we want to generate a target probability distribUtion p0 (v) over the visible Units,
as defined by example configUrations vj(0), j = 1, . . . , nJ. This can be achieved by adjUsting the
parameters of the energy fUnction to minimise the relative entropy between the marginal distribUtion
over the visible Units
p(v, θ) = Trh p(v, h, θ).
and the target distribUtion:
nJ
D(p0||p) = X p0 (vj(0)) logp0(vj(0)) - p0 (vj(0)) log p(θ, vj(0)).	(1)
j=1
The gradient of the relative entropy with respect to the parameters of the energy fUnction θ can be
expressed in terms of expectation valUes of spins
dD(po∣∣p) = XJ PO(Vj))
dbk	j=1 p(θ, vj0))
hskiC (j) - hskiF ,
(2)
2
Under review as a conference paper at ICLR 2019
and spin-spin correlators
dD(po∖∖p) _XXJ P0(vj))/	∖	/	∖	小
~~kl^Γ = j=1 pθjɪ hsk Ila(L(SkI)F ,	⑶
where h. . .iF denotes the expectation value of the model without constraints (free) and h. . .iC(j) the
expectation value if the visible units are fixed to example vj (clamped). This gradient can be used
in a first order optimisation method to arrive at an optimal parameter configuration θ0 . While the
expression for the gradient looks simple, evaluating the probabilities and expectation values is not,
and some approximation scheme needs to be used. Two schemes are usually used, one based on
sampling (Hinton and E. (2002)), the other one based on a mean field approximation. Here we will
concentrate on the mean field approximation.
The central quantity in mean field theory is the free energy. We can obtain the free energy of our
model as an expansion in the order of the coupling J as derived by a number of authors (Georges
and Yedidia (1991); Kuhn and Helias (2017)).
F = X Fn
n
To lowest order the free energy is given by the entropy of independent spins.
N
F0 = X
i=1
where mi is the magnetisation of spin i. Up to second order we obtain
F1=	Jijmimj +	bimi,
1 - mi	1 - mi	1 + mi	1 + mi
~τ~ log [―^ +[-^ iog
F2 = E Jij(I- m2) (1 - m2),
hiji
where the notation hiji denotes summation over unequal indices. For orders up to four see ap-
pendix A. From the free energy we can derive an equation to obtain the mean field values mi in the
approximation up to order N
mi = tanh
dFn
dmi
(4)
with
In particular, the first two orders are
Ri() = bi + 2 X	Jij mj
j
and
Ri(2) = -4mi X Ji2j(1 - mj2).
j
Higher orders can be found in the appendix A. In the free case, the transcendental equation, Eq. 4
can be solved by iteration, starting from zero or random magnetisations, until the magnetisations
converge. In the clamped case we keep the visible magnetisations fixed, while iterating over the
hidden units. This results in two sets of distinct magnetisations: the free magnetisations mF and the
clamped magnetisations mC(j), where example j is fixed on the visible units. We also get a set of
corresponding free energies FF and FC(j). The correlation functions can be obtained from the free
energies
hSkSl iF/Cj )
2 dJkl
3
Under review as a conference paper at ICLR 2019
Figure 1: An example of an unrolling of the second order mean field iteration into a 5 layer neural
network. The weights in all layers except the first one are shared, resulting in a deep network with
a small number of parameters. The structure of the network is fully determined by the mean field
equations derived from the energy function of Boltzmann machine.
and the expectation values are given directly by
hsk iF/C(j) = mF/C(j),k.
These expectation values can be inserted in the expressions for the gradient, Eqs. 2 and 3, and used
to minimise the relative entropy Eq. 1. Probabilities can be estimated from the free energies
p(θ, vj(0)) = eFC(j) -FF .
The training of a Boltzmann machine emphasises the main task of the machine as generating ex-
amples from the target distribution, but one can look at the machine from a different point of view:
using the hidden units for constructing a model of the example data. The probability distribution over
the hidden units if an example is fixed on the visible units then tells us something about the model
required to describe the example. In a restricted Boltzmann machine (RBM) the probability distri-
bution is a product distribution over the hidden units and features, but one can imagine introducing
couplings between hidden units to account for correlations between individual features to arrive at
a better/simpler description of the data. In the remaining article we will discuss machines that have
bipartite connectivity between visible and hidden units and full connectivity between hidden units.
During inference we want to obtain the hidden magnetisations given the magnetisations of the visible
units. We split the magnetisations into visible and hidden parts m = (mv, mh). Similarly we split
the magnetic fields b = (0, bh) and the couplings
J=	0 W
J =	W Jh
where W now describes the bipartite connectivity between visible and hidden units and Jh the
coupling between hidden units. Keeping terms to second order the mean field equations become
mh = tanh [2W ∙ mo + bh + 2J% ∙ m% - 4m%Jh ∙(1 - mQ].
(5)
Solving the mean field equation Eq. 5 requires iteration, e.g. starting from the initial magnetisations
mh = 0. The iteration can be unrolled into an r layer neural network, where r is greater than the
number of iterations necessary for convergence (see Fig. 1). We will refer to this neural network as
the mean field Boltzmann network.
3 Training the mean field B oltzmann network
The Boltzmann machine underlying the mean field Boltzmann network is trained on data derived
from the standard MNIST training set. We binarize the MNIST images with a threshold (50/255)
and create two datasets. The first dataset contains 4 x 4 patches cut from the binarised images, the
second contains 8 x 8 patches. We start by training a Boltzmann machine for the 4 x 4 patches.
Of all 216 possible patches, 679 patches contain over 98% of all occurences in the dataset. We
use these 679 to train a mean field Boltzmann machine with 32 hidden units (see Fig. 2). Four of
4
Under review as a conference paper at ICLR 2019
③ hidden
visible
C∖
32
Oooo
Oooo
Oooo
Oooo
口口
H□□HBŋΠΠQI
B□αsG□nnα
□□BE3BEEE
□E□E□BΠHB
a□□∣Eαu□BB
an□αn□ESE]
aHBnQSB□B
Figure 2: Illustration of the structure of the Boltzmann machine and the training procedure. a) shows
the Boltzmann machine used in the first training step consisting of 16 visible units and 32 hidden
units. The line between the visible and hidden units represents bipartite connectivity between the
units in the two sets. The loop on top of the hidden units indicates full connectivity between hidden
units. 4 x 4 patches from the example data are used to generatively train the machine. b) shows
the structure of the Boltzmann machine used in the second training step, consisting of 64 visible
units and 256 hidden units. The trained small machines are assigned a 4 x 4 sub-patch in the 8 x 8
patches. Each of the 32 hidden units in the small machines gains bipartite connectivity to a further
fully connected 128 hidden units and the full machine is trained according to the standard Boltzmann
machine training procedure.

these trained small machines are then combined with 128 hidden units to model the 8 x 8 patches.
The dataset of 8 x 8 patches contains approximately 600000 examples. For the calculation of the
gradient we use batches of 10000 examples. To calculate the free expectation values we use the
mean field expansion up to fourth order, for the clamped expectation values it suffices to go to
second order. The magnetisations in the free case are calculated using 100 mean-field iterations, for
the clamped magnetisation we use 20 iterations. In the late stage of the training the value for the
free partition function is underestimated. This can lead to the sum of individual sample probabilities
exceeding one. We adjust the free partition function in calculation of the relative entropy and the
gradient so that the probabilities within one batch are normalised to one. Similar results to the staged
training can be obtained by training a Boltzmann machine of identical connectivity directly on the
8x8 patches.
4	Adversarially robust neural networks
The trained mean field Boltzmann network constitutes a new neural network building block that
takes in 8 x 8 patches and outputs a vector of 128 hidden magnetisations. This building block
has identical input/output dimensions to a standard convolutional layer with 8 x 8 filter size and
128 filters. We can use this building block as an input layer in an identical fashion to a standard
convolutional layer, sweeping it over the two input image dimensions. Starting from the 28 x 28
MNIST images, this results in an intermediate representation of 128 x 21 x 21 dimensions. On
top of this representation we build a standard three layer convolutional neural network with ReLU
activations. We add 3 convolution layers (kernel size 3 x 3, stride 2) with 128 filters, 64 filters
and 32 filters, respectively, followed by a fully connected layer and a softmax function). The input
5
Under review as a conference paper at ICLR 2019
Figure 3: Dependence of adversarial resistance and test error on Boltzmann machine pre-training for
a neural network with a mean field Boltzmann network input layer. The blue plot shows the mean
square error (for image pixels in the range [0, 1]) required to create an adversarial image averaged
over 1000 random samples from the MNIST test set as a function of the relative entropy D achieved
in Boltzmann machine pre-training. Lower relative entropy at the pre-training stage results in higher
resistance to adversarial attacks. The blue lines indicate the adversarial resistance for LeNet and a
pre-trained first RBM layer. Attached to the datapoints are examples of adversarial images at a given
relative entropy. The orange line shows the test error (in %) of a mean field Boltzmann network over
the whole clean MNIST test set, with the straight orange lines showing the reference test error of
LeNet and a network with RBM pre training.
layer parameters are kept fixed during training and the remaining layers are trained in a standard
discriminative fashion.
Let us establish the connection between better generative pre-training and increased adversarial
resistance. We initialize the mean field Boltzmann network with parameters from various stages of
the generative pre-training. As a measure for the generative training process we can use the relative
entropy of the generated and target probability distribution, Eq. 1. To assess adversarial resistance
we use an iterative gradient based (white box) method with L2 constraints and record the mean
squared error for the first image iteration that is misclassified. We average the mean squared error
for 1000 randomly chosen images from the MNIST test set. We also calculate the test error for
the whole clean MNIST test set for a given neural network. The results are presented in Fig. 3.
We see that a smaller relative entropy in the pre-training phase correlates with a higher adversarial
resistance. For comparison we apply the same procedure to a standard LeNet network as well as a
neural network where the mean field Boltzmann network is based on an RBM with 64 visible and
128 hidden units trained in the same mean field approximation. Both LeNet and RBM pre-trained
network do not show the strong adversarial resistance observed in the neural network using the mean
field Boltzmann network as input layer. On the other hand we see that the classification error of the
robust neural network on the clean test set, while above the LeNet and RBM error, is independent of
the pre-training. We find no evidence for a strong correlation of classification error and adversarial
resistance in our model.
One reason for the apparent robustness of neural networks are obfuscated gradients (Athalye et al.
(2018)). Therefore we also evaluate the robust network using a gradient free (black box) method:
the Boundary Method (Brendel et al. (2017)), implemented in Foolbox. For evaluation we use the
network that showed the highest adversarial resistance to the iterated gradient method. The Bound-
6
Under review as a conference paper at ICLR 2019
Table 1: Comparison of our method with the ABS model. The entries in the table show the average
L2 norm and the robustness at a threshold of = 1.5. Our method is tested over 1000 randomly
selected images from the MNIST test set. Histograms of the L2 norms of the different attacks can be
found in appendix B. WB and BB indicates white box and black box attack methods, respectively.
For the compound attack we select the lowest L2 distance out of all four methods.
model	Gradient Iterative (WB)	Boundary Method (BB)	Transfer Attack (BB)	Gaussian Noise Attack (BB)	Compound Attack
mean field Boltzmann network	3.9/97%	3.46/85 %	5.6 / 96 %	10.6/98%	2.73 / 82%
-Analysis by- Synthesis	3.1/87%	2.6/83 %	4.6 / 94 %	10.9/98 %	2.3/80%
ary Method, using 1200 iterations, spherical_step=0.1 and otherwise default parameters achieves an
average L2 distance of
We have added two more attacks to our evaluation. The first attack is a transfer attack using a
gradient based method on LeNet to obtain a perturbation. The amplitude of the perturbation is
then increased until the network under test misclassifies the image. The second attack is a random
Gaussian noise attack, where the variance of the added noise is increased until the network under test
misclassifies the image. Using all four attacks we can construct a compound attack as suggested by
Schott et al. (2018) by considering the attack with the smallest L2 norm for each image. Combining
all four attacks reduces the adversarial robustness below the robustness of any single attack.
Another reason for the apparent robustness of our model could be the very deep (20 layers) pre-
processing stage leading to vanishing gradients. To investigate this further we remove the last, fully
connected layer in our neural network and train a second, shallower network with the 32 x 6 x 6
dimensional activation data. The distilled network consists of 5 convolution layers: (8 x 8, stride 1,
256 filters), (1 x 1, stride 1, 256 filters), (3 x 3, stride 2, 128 filters, padding 1), (3 x 3, stride 2, 128
filters, padding 1), (3 x 3, stride 1, 32 filters, padding 1), with ReLU activations. Putting back the
fully connected layer from the full network we can measure the adversarial resistance in the same
way as earlier. We obtain an average L2 distance of 3.35 and an error rate over the clean dataset of
1.35%, compared to an average L2 distance of 3.9 for the original network. Although the distilled
network is showing a loss of robustness, the average L2 distance is still more than twice than the
average L2 distance for LeNet. This shows that some robustness can be transferred from the network
structure used for learning to a relatively simple network and suggests that the robustness is not an
artefact of a too deep network structure.
A comparison of our results with currently the most adversarially robust method, the Analysis By
Synthesis (ABS) model of Schott et al. (2018), is shown in Tab. 1. For results from other methods
see table 1 in Schott et al. (2018). Our model achieves comparable robustness results.
5	Properties of Mean Field B oltzmann Networks
To understand the emerging adversarial robustness as a function of improved generative capability
we examine the response of the mean field Boltzmann network around different input vectors. For
one set of input vectors we will consider sample patterns from the training set, for the other set we
select random binary patterns. We then add uniform random noise of amplitude to the input vectors
and evaluate the different outputs of the network. The measure ∆r∕t, presented in Fig. 4, is the L?
norm of the average difference of clean and noisy output of a set of 1000 random and training set
patterns respectively with 10000 random noise patterns for each input pattern. Training set patterns
are much more resistant to noise than random patterns and the resistance increases as the Boltzmann
machine training progresses, particularly for small amplitude noise.
7
Under review as a conference paper at ICLR 2019
10
8
6 4
clean pattern noisy pattern
Figure 4: Response of a mean field Boltzmann network to noise at the input for different sets of
input patterns. The inset shows how much adding random noise (uniformly distributed) of maxi-
mum amplitude to the input pattern changes the output pattern. The main plot shows the ratio
of the differences for the random and training set patterns as a function of training progress of the
Boltzmann machine as measured by the relative entropy.
norm
Δ
6	Discussion
We have shown that we can construct a feed forward neural network that shows strong adversar-
ial resistance. This is achieved by incorporating a generatively pre-trained building block derived
from the mean field description of a Boltzmann machine, which we called the mean field Boltzmann
network. The resulting adversarial resistance strongly correlates with the effectiveness of the gener-
ative pre-training. We believe that the increased adversarial resistance can be traced to how the mean
field Boltzmann network rejects noise around training example patches. The noise rejection does
not happen for randomly selected examples. The rejection behaviour is an indication of the local
non-linear behaviour of the mean field Boltzmann network. Compare this with the properties of a
random matrix as feature extractor, where the restricted isometry property tells us that distances are
preserved in the mapping with high probability. Too weak non-linearity of neural networks has been
suggested as one reason for the existence of adversarial images and the increased non-linearity in our
model correlates with increased adversarial resistance. On the level of individual image patches we
can still find noise patterns that result in a large response when added to a training example, but they
are much less likely. Take this together with the convolutional way that the mean field Boltzmann
network is deployed. To get a large adversarial response from a given image region all overlapping
image patches have to have a large response to the particular adversarial pattern. With the proba-
bility of large response supressed for one pattern, it becomes even smaller for all the overlapping
pattern, resulting in strong adversarial resistance for the full neural network.
Our results, while encouraging, leave a number of questions open. First we have to ask ourselves
if the success is due to the simplicity of the MNIST dataset. Will we be able to find typical image
elements and model them with a Boltzmann machine for real image data? Second, while the ad-
versarial images we find show a large deviation from the original image most do not resemble other
image classes (for example images see appendix C). The algorithm still confidently places them in
a particular class. We would prefer a behaviour where images unrecognisable by a human would
result in a low confidence of the prediction.
References
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th Inter-
8
Under review as a conference paper at ICLR 2019
national Conference on Machine Learning, ICML2018, Stockholmsmdssan, Stockholm, Sweden,
July 10-15, 2018, pages 274-283, 2018.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine
learning. Pattern Recognition, 84:317-331, 2018. doi: 10.1016/j.patcog.2018.07.023.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. CoRR, abs/1712.04248, 2017.
PaWeI Budzianowski. Training Restricted Boltzmann Machines Using High-TemPeratUre ExPan-
sions. Master’s thesis, University of Cambridge, 2016.
Marylou Gabri6, Eric W. Tramel, and Florent Krzakala. Training restricted boltzmann machines via
the thouless-anderson-Palmer free energy. CoRR, abs/1506.02914, 2015.
A Georges and J S Yedidia. How to exPand around mean-field theory using high-temPerature ex-
Pansions. J. Phys. A. Math. Gen., 24(9):2173-2192, 1991. ISSN 0305-4470. doi: 10.1088/
0305-4470/24/9/024.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Watten-
berg, and Ian J. Goodfellow. Adversarial sPheres. CoRR, abs/1801.02774, 2018.
Shixiang Gu and Luca Rigazio. Towards deeP neural network architectures robust to adversarial
examPles. CoRR, abs/1412.5068, 2014.
Geoffrey Hinton. Boltzmann machine. Scholarpedia, 2(5):1668, 2007. ISSN 1941-6016. doi:
10.4249/scholarPedia.1668.
Geoffrey E. Hinton and Geoffrey E. Training Products of ExPerts by Minimizing Contrastive
Divergence. Neural Comput., 14(8):1771-1800, 2002. ISSN 0899-7667. doi: 10.1162/
089976602760128018.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A Fast Learning Algorithm for DeeP
Belief Nets. Neural Comput., 18(7):1527-1554, 2006. ISSN 0899-7667. doi: 10.1162/neco.
2006.18.7.1527.
Saumya Jetley, Nicholas A. Lord, and PhiliP H. S. Torr. With friends like these, who needs adver-
saries? CoRR, abs/1807.04200, 2018.
Tobias Kuhn and Moritz Helias. Expansion of the effective action around non-Gaussian theories.
2017. ISSN 1751-8113. doi: 10.1088/1751-8121/aad52e.
Alexey Kurakin, Ian J. Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang,
Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren,
Alan L. Yuille, Sangxia Huang, Yao Zhao, Yuzhe Zhao, Zhonglin Han, Junjiajia Long, Yerkebu-
lan Berdibekov, Takuya Akiba, Seiya Tokui, and Motoki Abe. Adversarial attacks and defences
competition. CoRR, abs/1804.00097, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 86-94, 2017. doi: 10.1109/CVPR.2017.
17.
Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, and Oncel Tuzel. Divide, denoise, and de-
fend against adversarial attacks. CoRR, abs/1802.06806, 2018.
Mohammad Norouzi, Mani Ranjbar, and Greg Mori. Stacks of convolutional restricted boltzmann
machines for shift-invariant feature learning. In 2009 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA,
pages 2735-2742, 2009. doi: 10.1109/CVPRW.2009.5206577.
L. Schott, J. Rauber, W. Brendel, and M. Bethge. Towards the first adversarially robust neural
network model on mnist. 2018.
9
Under review as a conference paper at ICLR 2019
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of ’Solvable model of a spin glass’.
Philos. Mag., 35(3):593-601, 1977. ISSN 0031-8086. doi: 10.1080/14786437708235992.
A Higher order terms in the mean field expansion
The third and fourth order term in the free energy becomes
F3 = X : Jij Jjk Jki (1 - mi) (1 - mj) (1 - mk)
hijki
+ 8 X Jj mi (1- m2) mj (1- m2
hiji
F4 = 2 ^X JijJjkJklJli (1 - m2) (1 - mj) (1 - mk) (1 - m2) +
hijkli
+ 16 E JjJik Jkjmi (1 - mj) mj (ι - mj) (1 - mk)
hij ki
-2 X Jj(I- m2)(1 - m2)(1 + 3m2 + 3mj- 15m2mj)
hiji
The third and fourth order terms for the mean field iteration are
R(3) = -8 X JijJjk Jkimi (1 - m2) (1 - mk)
hjki
+3 X Jj(I- 3m2) mj-(1 - m2),
R(4) = -16 ^X JijJjkJklJlimi (1 - m2) (1 - mk) (1 - m2) +
hjkli
- 32	Jk2j Jij Jkimimj 1 - mj2 mk 1 - m2k
hjki
+ 32 X JjJkjJki (1 - 3m2) mj (1 - m2) (1 - mk)
hjki
-136 X Jj mi (1 - m2) [1 - 3m2+3m2 (5m2 - 3)]
j
10
Under review as a conference paper at ICLR 2019
B HISTOGRAMS OF L2 DISTANCES
gradient based attack on MFBM
Boundary Method attack on MFBM
706050403020100
4	6	8	10	12	14
LZnorm
gradient based attack on LeNet
≡: ∣L
O 2	4	6	8	10	12 U
706050403020100
Sμln8
Figure 5: Histograms of the L2 distances of adversarial images after various attacks for a set of 1000
images randomly selected from the MNIST training set. MFBM denotes the mean field Boltzmann
machine approach developed in this article. The vertical line indicates the cut-off = 1.5 used in
Table 1.
11
Under review as a conference paper at ICLR 2019
C Example adversarial images
Adversarial: 6	Difference
Original: 1	Adversarial: 7	Difference
Difference
Difference
Dififerenre
Adversarial: 2	Difference
Original: 6
Adversarial: 2	Difference
Differenre
Difference
Figure 6: The first images of each class in the randomly selected images from the MNIST test set,
their adversarial images obtained by the Boundary Method and the difference to the original image.
The adversarial images are labeled by their misclassified labels. For some images the adversarial
image clearly tends towards a human recognizable digit of another class (see 4, 5 and 9).
D	Mean field training vs. exact solution
For a very small Boltzmann machine we can examine how the mean field training procedure used
in the main part of the article compares to the exact solution. We select a Boltzmann machine of 16
units, which we divide into 5 visible units and 11 hidden units. We choose the connectivity to have
bipartite connections between visible and hidden units and full connectivity between the hidden
units. To examine the training behaviour we choose two example states out of 32 possible input
states, (1, -1, -1, -1, 1) and (-1, -1, 1, 1, -1), to be realised with equal probability. For the training we
use the mean-field derived relative entropy and its gradient from section 2. First, we can examine
how the relative entropies from the mean field approach and the exact calculation compare during
the training procedure. The results are presented in Fig. 7a. For most of the training the mean field
derived relative entropy tracks the exact entropy very closely, only to be underestimated towards the
end of the training. This is due to the underestimation of the partition function / free energy, resulting
in the overestimation of probabilities. The probabilities are presented in Figs. 7b, 7c and their ratio
in Fig. 7d. We see that although the probabilities are overestimated, their ratio is reproduced almost
exactly.
12
Under review as a conference paper at ICLR 2019
(a)
(b)
(c)
(d)
Figure 7: Comparison of an exact solution to the Boltzmann machine and the fourth order mean
field method used in this article for different stages in the Boltzmann machine training. (a) Mean
field derived relative entropy as a function of the exact relative entropy. The relative entropy does
not reach zero during training. (b) Probability of the first example as a function of exact relative
entropy. Red shows the mean field solution, blue the exact solution. (c) Probability of the second
example as a function of exact relative entropy. (d) Ratio of the probabilities of first and second
example.
13
Under review as a conference paper at ICLR 2019
E
Figure 8: Response ofa small mean field Boltzmann network to noise at the input for different sets of
input patterns. The plot shows how much adding random noise (uniformly distributed) of maximum
amplitude to the input pattern changes the output pattern. The red line shows the response averaged
over the two training patterns, the blue line shows the response averaged over the remaining 30
patterns.
E
Figure 9: Response of a small restricted mean field Boltzmann network to noise at the input for dif-
ferent sets of input patterns. The plot shows how much adding random noise (uniformly distributed)
of maximum amplitude to the input pattern changes the output pattern. The red line shows the
response averaged over the two training patterns, the blue line shows the response averaged over the
remaining 30 patterns.
We can now consider the noise rejection property for the small machine, using the same procedure
as in the main article. For this small machine we can calculate the averages over the input states for
all possible combination. To evaluate the noise rejection we used 1000 random noise patterns for
each example. The results are shown in Fig. 8 and show again the same noise rejection behaviour
for the example states. In contrast, if we use a simple RBM connected machine of the same size,
we don’t see any noise rejection behaviour (see Fig. 9).
14