Under review as a conference paper at ICLR 2019
Metropolis-Hastings view on variational
INFERENCE AND ADVERSARIAL TRAINING
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we propose to view the acceptance rate of the Metropolis-Hastings
algorithm as a universal objective for learning to sample from target distribution
-given either as a set of samples or in the form of unnormalized density. This
point of view unifies the goals of such approaches as Markov Chain Monte Carlo
(MCMC), Generative Adversarial Networks (GANs), variational inference. To re-
veal the connection we derive the lower bound on the acceptance rate and treat it
as the objective for learning explicit and implicit samplers. The form of the lower
bound allows for doubly stochastic gradient optimization in case the target distri-
bution factorizes (i.e. over data points). We empirically validate our approach on
Bayesian inference for neural networks and generative models for images.
1	Introduction
Bayesian framework and deep learning have become more and more interrelated during recent years.
Recently Bayesian deep neural networks were used for estimating uncertainty (Gal & Ghahramani,
2016), ensembling (Gal & Ghahramani, 2016) and model compression (Molchanov et al., 2017). On
the other hand, deep neural networks may be used to improve approximate inference in Bayesian
models (Kingma & Welling, 2014).
Learning modern Bayesian neural networks requires inference in the spaces with dimension up to
several million by conditioning the weights of DNN on hundreds of thousands of objects. For such
applications, one has to perform the approximate inference - predominantly by either sampling from
the posterior with Markov Chain Monte Carlo (MCMC) methods or approximating the posterior
with variational inference (VI) methods.
MCMC methods provide the unbiased (in the limit) estimate but require careful hyperparameter
tuning especially for big datasets and high dimensional problems. The large dataset problem has
been addressed for different MCMC algorithms: stochastic gradient Langevin dynamics (Welling &
Teh, 2011), stochastic gradient Hamiltonian Monte Carlo (Chen et al., 2014), minibatch Metropolis-
Hastings algorithms (Korattikara et al., 2014; Chen et al., 2016). One way to address the problem
of high dimension is the design ofa proposal distribution. For example, for the Metropolis-Hastings
(MH) algorithm there exists a theoretical guideline for scaling the variance of a Gaussian proposal
(Roberts et al., 1997; 2001). More complex proposal designs include adaptive updates of the pro-
posal distribution during iterations of the MH algorithm (Holden et al., 2009; Giordani & Kohn,
2010). Another way to adapt the MH algorithm for high dimensions is combination of adaptive di-
rection sampling and the multiple-try Metropolis algorithm as proposed in (Liu et al., 2000). Thor-
ough overview of different extensions of the MH algorithm is presented in (Martino, 2018).
Variational inference is extremely scalable but provides a biased estimate of the target distribution.
Using the doubly stochastic procedure (TitSiaS & Lazaro-Gredilla, 2014; Hoffman et al., 2013) VI
can be applied to extremely large datasets and high dimensional spaces, such as a space of neural
network weights (Kingma et al., 2015; Gal & Ghahramani, 2015; 2016). The bias introduced by
variational approximation can be mitigated by using flexible approximations (Rezende & Mohamed,
2015) and resampling (Grover et al., 2018).
Generative Adversarial Networks (Goodfellow et al., 2014) (GANs) is a different approach to
learn samplers. Under the framework of adversarial training different optimization problems could
be solved efficiently (Arjovsky et al., 2017; Nowozin et al., 2016). The shared goal of ”learning to
1
Under review as a conference paper at ICLR 2019
sample” inspired the connection of GANs with VI (Mescheder et al., 2017) and MCMC (Song et al.,
2017).
In this paper, we propose a novel perspective on learning to sample from a target distribution by
optimizing parameters of either explicit or implicit probabilistic model. Our objective is inspired by
the view on the acceptance rate of the Metropolis-Hastings algorithm as a quality measure of the
sampler. We derive a lower bound on the acceptance rate and maximize it with respect to parameters
of the sampler, treating the sampler as a proposal distribution in the Metropolis-Hastings scheme.
We consider two possible forms of the target distribution: unnormalized density (density-based set-
ting) and a set of samples (sample-based setting). Each of these settings reveals a unifying property
of the proposed perspective and the derived lower bound. In the density-based setting, the lower
bound is the sum of forward and reverse KL-divergences between the true posterior and its approx-
imation, connecting our approach to VI. In the sample-based setting, the lower bound admit a form
of an adversarial game between the sampler and a discriminator, connecting our approach to GANs.
The closest work to ours is of Song et al. (2017). In contrast to their paper our approach (1) is
free from hyperparameters; (2) is able to optimize the acceptance rate directly; (3) avoids minimax
problem in the density based setting.
Our main contributions are as follows:
1.	We introduce a novel perspective on learning to sample from the target distribution by
treating the acceptance rate in the Metropolis-Hastings algorithm as a measure of sampler
quality.
2.	We derive the lower bound on the acceptance rate allowing for doubly stochastic optimiza-
tion of the proposal distribution in case when the target distribution factorizes (i.e. over
data points).
3.	For sample-based and density-based forms of target distribution we show the connection of
the proposed algorithm to variational inference and GANs.
The rest of the paper is organized as follows. In Section 2 we introduce the lower bound on the AR.
Special forms of target distribution are addressed in Section 3. We validate our approach on the prob-
lems of approximate Bayesian inference in the space of high dimensional neural network weights
and generative modeling in the space of images in Section 4. We discuss results and directions of
the future work in Section 5.
2	Acceptance Rate for the Metropolis-Hastings Algorithm
2.1	Preliminaries
In MH algorithm we need to sample from target distribution p(x) while we are only able to sample
from proposal distribution q(x0 | x). One step of the MH algorithm can be described as follows.
1.	sample proposal point χ0 〜q(χ01 x), given previously accepted point X
(x0, if P(X )q(XJIx ) > u, U 〜Uniform[0,1]
2.	accept ,	p(x)q(x0 | x)	,	,
x, otherwise
If the proposal distribution q(x0 | x) does not depend on x, i.e. q(x0 | x) = q(x0), the algorithm is
called independent MH algorithm.
The quality of the proposal distribution is measured by acceptance rate and mixing time. Mixing
time defines the speed of convergence of the Markov chain to the stationary distribution. The accep-
tance rate of the MH algorithm is defined as
AR = Eξ min{1,ξ} = / dxdx0p(x)q(x01 x) min (1, Px ,q'x ∣x ? ∣,
p(x)q(x0 |x)
(1)
where
ξ = P((X [q(X0||X ) , X 〜P(X), χ0 〜q(XI x).
p(x)q(x0 | x)
(2)
2
Under review as a conference paper at ICLR 2019
In case of independent proposal distribution we show that the acceptance rate defines a semimetric
in distribution space between p and q (see Appendix A.2).
2.2	Optimizing the lower bound on acceptance rate
Although, we can maximize the acceptance rate of the MH algorithm (Eq. 1) directly w.r.t. pa-
rameters φ of the proposal distribution qφ(x0 | x), we propose to maximize the lower bound on the
acceptance rate. As our experiments show (see Section 4) the optimization of the lower bound com-
pares favorably to the direct optimization of the acceptance rate. To introduce this lower bound we
first express the acceptance rate in terms of total variation distance.
Theorem 1 For random variable ξ = PXq(Xo∣X), X 〜p(x),x0 〜q(x01 x)
Eξmin{1,ξ} = 1 - 2Eξ∣ξ - 1| = 1 - TVfp(x0)q(x | x0) p(x)q(x01 x)),	(3)
where TV is the total variation distance.
The proof of Theorem 1 can be found in Appendix A.1. This reinterpretation in terms of total
variation allows us to lower bound the acceptance rate through the Pinsker’s inequality
AR ≥ 1 -
j1 ∙ KL(P(X0)q(x |XO)
p(X)q (X0 | X) .
(4)
The maximization of this lower bound can be equivalently formulated as the following optimization
problem
min KL p(X0)qφ(X | X0)
φ
p(X)qφ(X0 | X) .
(5)
In the following sections, we show the benefits of this optimization problem in two different settings
— when the target distribution is given in a form of unnormalized density and as a set of samples. In
Appendix C.5 and C.1 we provide the empirical evidence that maximization of the proposed lower
bound results in the maximization of the acceptance rate.
3	Optimization of Proposal Distribution
From now on we consider only optimization problem Eq. 5 but the proposed algorithms can be also
used for the direct optimization of the acceptance rate (Eq. 1).
To estimate the loss function (Eq. 5) we need to evaluate the density ratio. In the density-based
setting unnormalized density of the target distribution is given, so we suggest to use explicit proposal
distribution to compute the density ratio explicitly. In the sample-based setting, however, we cannot
compute the density ratio, so we propose to approximate it via adversarial training (Goodfellow
et al., 2014). The brief summary of constraints for both settings is shown in Table 1.
Table 1: Constraints for two settings of learning sampling algorithms
Setting	Target distribution	Proposal distribution	Density Ratio
Density-based	given P(X) H P(X)	explicit model q(X0)	explicit
Sample-based	set of samples X 〜P(X)	implicit model q(X0) implicit model q(X0∣X)	learned discriminator
The following subsections describe the algorithms in detail.
3
Under review as a conference paper at ICLR 2019
3.1	Density-based Setting
In the density-based setting, we assume the proposal to be an explicit probabilistic model, i.e. the
model that we can sample from and evaluate its density at any point up to the normalization constant.
We also assume that the proposal is reparameterisable (Kingma & Welling, 2014; Rezende et al.,
2014; Gal, 2016).
During the optimization of the acceptance rate we might face a situation when proposal collapses
to the delta-function. This problem usually arises when we use Markov chain proposal, for ex-
ample, qφ(x0 | x) = N (x0 | x, σ). For such proposal we can obtain arbitrary high acceptance rate,
making the σ small enough. However, this is not the case for the independent proposal distribution
qφ (x0 | x) = qφ(x0). In Appendix B.1 we provide more details and intuition on this property of
acceptance rate maximization. We also provide empirical evidence in Section 4 that collapsing to
the delta-function does not happen for the independent proposal.
In this paper, we consider two types of explicit proposals: simple parametric family (Section 4.2)
and normalizing flows (Rezende & Mohamed, 2015; Dinh et al., 2016) (Section 4.1). Rich family
of normalizing flows allows to learn expressive proposal and evaluate its density in any point of
target distribution space. Moreover, an invertible model (such as normalizing flow) is a natural
choice for the independent proposal due to its ergodicity. Indeed, choosing the arbitrary point in the
target distribution space, we can obtain the corresponding point in the latent space using the inverse
function. Since every point in the latent space has positive density, then every point in the target
space also has positive density.
Considering qφ (x0) as the proposal, the objective of optimization problem 5 takes the form
L(p, qφ) = KL p(x0)qφ(x)p(x)qφ(x0) = E
log P(X)qφ(xO)
‘χx二窝)p(χ0)qφ(x).
(6)
Explicit form of the proposal qφ(χ0) and the target p(χ) distributions allows us to obtain density
ratios q≠(x)∕q≠(x0) and p(x0)∕p(x) for any points x,x0. But to estimate the loss in Eq. 6 We also
need to obtain samples from the target distribution X 〜 p(x) during training. For this purpose,
We use the current proposal qφ and run the independent MH algorithm. After obtaining samples
from the target distribution it is possible to perform optimization step by taking stochastic gradients
W.r.t. φ. Pseudo-code for the obtained procedure is shoWn in Algorithm 1.
Algorithm 1 Optimization of proposal distribution in density-based case
Require: explicit probabilistic model qφ(x0)
Require: density of target distribution P(X) a p(x)
while φ not converged do
SamPle {xk}K=1 〜qφ(x0)
sample {xk }3ι 〜p(x) using independent MH with current proposal qφ
L(p, qφ) ` κ1 PK=IIog P(Xk)qφ(Xk)	. approximate loss with finite number of samples
φ J φ 一 αVφL(p, qφ)	. perform gradient descent step
end while
return optimal parameters φ
Algorithm 1 could also be employed for the direct optimization of the acceptance rate (Eq. 1). Now
we apply this algorithm for Bayesian inference problem and show that during optimization of the
lower bound we can use minibatches of data, while it is not the case for direct optimization of the
acceptance rate. We consider Bayesian inference problem for discriminative model on dataset D =
{(xi , yi )}iN=1 , where xi is the feature vector of ith object and yi is its label. For the discriminative
model we know likelihood P(yi | xi, θ) and prior distributionP(θ). In order to obtain predictions for
some object xi , we need to evaluate the predictive distribution
P(yi | xi) = Ep(θ | D)P(yi | xi, θ).	(7)
To obtain samples from posterior distribution P(θ | D) we suggest to learn proposal distribution
qφ(θ) and perform independent MH algorithm. Thus the objective 6 can be rewritten as
L P(θ | D), qφ(θ)	=KL P(θ0 | D)qφ(θ)
P(θ | D)qφ(θ0)
(8)
4
Under review as a conference paper at ICLR 2019
Note that due to the usage of independent proposal, the minimized KL-divergence splits up into the
sum of two KL-divergences.
KL p(θ0 | D)qφ(θ)
p(θ | D)qφ(θ0)
KL qφ(θ)
p(θ |D)
+KL p(θ0 | D)
(9)
Minimization of the first KL-divergence corresponds to the variational inference procedure.
KL (qφ(θ) p(θ |D)) = -Eθ〜qφ(θ) XX logp(yi | ")+ KL(qφ(θ)kp(θ)) + logP(D)	(10)
The second KL-divergence has the only term that depends on φ. Thus we obtain the following
optimization problem
N
min - Eθ〜qφ(θ) ElOgp(y |χi,θ) + KL(qφ(θ)kp(θ)) - Eθ〜p(θ∣d) logqφ(θ) .	(11)
φ	i=1
The first summand here contains the sum over all objects in dataset D. We follow doubly stochastic
variational inference and suggest to perform unbiased estimation of the gradient in problem 11 using
only minibatches of data. Moreover, we can use recently proposed techniques (Korattikara et al.,
2014; Chen et al., 2016) that perform the independent MH algorithm using only minibatches of data.
Combination of these two techniques allows us to use only minibatches of data during iterations of
algorithm 1. In the case of the direct optimization of the acceptance rate, straightforward usage of
minibatches results in biased gradients. Indeed, for the direct optimization of the acceptance rate
(Eq. 1) we have the product over the all training data inside min function.
3.2	Sample-based Setting
In the sample-based setting, we assume the proposal to be an implicit probabilistic model, i.e. the
model that we can only sample from. As in the density-based setting, we assume that we are able to
perform the reparameterization trick for the proposal.
In this subsection we consider only Markov chain proposal qφ(x0 | x), but everything can be applied
to independent proposal qφ(x0) by simple substitution qφ(x0 | x) with qφ(x0). From now we will
assume our proposal distribution to be a neural network that takes x as its input and outputs x0 .
Considering proposal distribution parameterized by a neural network allows us to easily exclude
delta-function from the space of solutions. We avoid learning the identity mapping by using neural
networks with the bottleneck and noisy layers. For the detailed description of the architectures see
Appendix C.8.
The set of samples from the true distribution X 〜p(χ) allows for the Monte Carlo estimation of
the loss
L(p, qφ) = E
x0
l	P(X)qφ(X0 1 X)
X 〜P(X) . °g P(XDqφ(x | x0).
〜qφ(x |χ)
(12)
To compute the density ratio P((X)北((：∣lx) We suggest to use well-known technique of density ra-
tio estimation via training discriminator network. Denoting discriminator output as D(X, X0), we
suggest the following optimization problem for the discriminator.
min - E χ 〜P(X)	log D(x,x0)- E X 〜P(X) log(1 - D(x0,x))	(13)
x0 〜qφ(X0 | x)	X0 〜qφ (x0 | X)
Speaking informally, such discriminator takes two images as input and tries to figure out which
image is sampled from true distribution and which one is generated by the one step of proposal
distribution. It is easy to show that optimal discriminator in problem 13 will be
D(x,x0)=，、/3" X) 一，、.
P(X)qφ(X0 | X) + P(X0)qφ(X | X0)
(14)
5
Under review as a conference paper at ICLR 2019
Note that for optimal discriminator we have D(x, x0) = 1 - D(x0, x). In practice, we have no
optimal discriminator and these values can differ significantly. Thus, we have four ways for density
ratio estimation that may differ significantly.
p(x)qφ(x01 x)	D(x,x0)	1 — D(x0,x)	1 — D(x0,x)	D(x,x0)	(^)
p(x0)qφ(x | x0)	1 - D(x, x0)	D(x0, x)	1 - D(x, x0)	D(x0, x)
To avoid the ambiguity we suggest to use the discriminator of a special structure. Let D(x, x0) be
a convolutional neural network with scalar output. Then the output of discriminator D(x, x0) is
defined as follows.
/六/ r∖ \
D(x, χθ) = ~~ exp(DEx)L--------------	(16)
exp(D(x, x0)) + exp(D(x0, x))
In other words, such discriminator can be described as the following procedure. For single neural
network D(∙, ∙) we evaluate two outputs D(x, x0) and D(x0, x). Then we take softmax operation for
these values. Summing up all the steps, we obtain algorithm 2.
Algorithm 2 Optimization of proposal distribution in sample-based case
Require: implicit probabilistic model qφ(x0 | x)
Require: large set of samples X 〜p(x)
for n iterations do
sample {xk}K=ι 〜X
SamPle {xk}K=ι 〜qφ(x1x)
train discriminator D by optimizing 13
L(p, qφ) ≈ ~K PK=Ilog IDD(X,jχ0)	. approximate loss with finite number of samples
φ J φ — αVφL(p, qφ)	. perform gradient descent step
end for
return parameters φ
Algorithm 2 could also be employed for direct optimization of the acceptance rate (Eq. 1). But, in
Appendix B.2 we provide an intuition for this setting that the direct optimization of the acceptance
rate may struggle from vanishing gradients.
4	Experiments
In this section, we provide experiments for both density-based and sample-based settings, showing
the proposed procedure is applicable to high dimensional target distributions. Code for reproducing
all of the experiments will be published with the camera-ready version of the paper.
4.1	Synthetic distributions
To demonstrate performance of our approach we reproduce the experiment from (Song et al., 2017).
For target distributions we use synthetic 2d distributions (see Appendix C.3 for densities): ring (a
ring-shaped density), mog2 (a mixture of 2 Gaussians), mog6 (a mixture of 6 Gaussians), ring5 (a
mixture of 5 distinct rings). We measure performance of learned samplers using Effective Sample
Size (see Appendix C.4 for formulation). Since the unnormalized densities of target distributions are
given, we can learn proposals as suggested in the density-based setting (Section 3.1).
To learn the independent proposal we use RealNVP model (Dinh et al., 2016) (see details in Ap-
pendix C.2) and compare the performance of proposals after optimization of different objectives:
the acceptance rate (AR), our lower bound on the acceptance rate (ARLB), evidence lower bound
that corresponds to the variational inference (VI). We also compare the performance of obtained
independent proposals with the performance of Markov chain proposals: A-NICE-MC (Song et al.,
2017), Hamiltonian Monte Carlo (HMC).
In Tables 2, 3 we see that our approach has comparable performance with A-NICE-MC (Song et al.,
2017). However, comparison between A-NICE-MC and learning independent proposal is not the
main subject of interest, since A-NICE-MC learns Markov chain proposal. On the one hand, Markov
6
Under review as a conference paper at ICLR 2019
chain proposal uses more information while generating a new sample, hence can learn more expres-
sive stationary distribution, on the other hand, usage of previous sample increase autocorrelation
between samples and reduces ESS. Thus, the main point of interest is the comparison of two inde-
pendent proposals: one is learned by maximization of the acceptance rate (or its lower bound), and
the second is learned by variational inference procedure, i.e. maximization of evidence lower bound.
In Table 2 we see that both maximization of the acceptance rate and its lower bound outperform vari-
ational inference for all target distributions. Moreover, in Fig. 1 we show that variational inference
fails to cover all the modes of mog6 in contrast to proposals learned via maximization of accep-
tance rate or its lower bound. Densities of learned proposals and histograms for all distributions are
presented in Appendix C.6.
Table 2: Performance of learned proposals as measured by Effective Sample Size (see Appendix C.4
for formulation). Higher is better (1000 maximum). All the numbers are rounded to integers. See
description of the compared methods in the text.
Target	A-NICE-MC	HMC	AR (ours)	ARLB (ours)	VI
ring	1000	1000	851	850	702
mog2	355	1	786	604	297
mog6	320	1	311	367	12
ring5	156	1	336	249	170
0-ι
6CIT
βoj
Table 3: Performance of learned proposals as measured by Effective Sample Size per second. All
the numbers are rounded to integers. See description of the compared methods in the text.
Target	A-NICE-MC	AR (ours)	ARLB (ours)	VI
ring	706417	^^275164	275488	227253
mog2	231253	267491	205552	101074
mog6	143877	94938	112034	3663
ring5	85186	95314	70635	48224
ACCeptanCe Rate
ACCePtanCe Rate LOWer BOUnd
VaHatIOnaIlnferenCe
■■
300
150
Figure 1: 2d histrogram of samples
from the MH algorithm with differ-
ent proposals. From left to right pro-
posals are learned by: variational in-
ference, the acceptance rate maxi-
mization, the acceptance rate lower
bound maximization.
20
¢0
ao
¢0
ao
ao

”T
O

«
o
∞
«
o
∞
«

O
4.2	Density-based Setting
In density-based setting, we consider Bayesian inference problem for the weights of a neural net-
work. In our experiments we consider approximation of predictive distribution (Eq. 7) as our main
goal. To estimate the goodness of the approximation we measure negative log-likelihood and accu-
racy on the test set.
In subsection 3.1 we show that lower bound on acceptance rate can be optimized more efficiently
than acceptance rate due to the usage of minibatches. But other questions arise.
1.	Does the proposed objective in Eq. 11 allow for better estimation of predictive distribution
compared to the variational inference?
2.	Does the application of the MH correction to the learned proposal distribution allow for
better estimation of the predictive distribution (Eq. 7) than estimation via raw samples
from the proposal?
To answer these questions we consider reduced LeNet-5 architecture (see Appendix C.7) for clas-
sification task on 20k images from MNIST dataset (for test data we use all of the MNIST test
7
Under review as a conference paper at ICLR 2019
set). Even after architecture reduction we still face a challenging task of learning a complex dis-
tribution in 8550-dimensional space. For the proposal distribution we use fully-factorized gaussian
qφ(θ) = Qd=ι N(θj | μj,σj and standard normal distribution for prior p(θ) = Q；=i N(θj 10,1).
For variational inference, we train the model using different initialization and pick the model accord-
ing to the best ELBO. For our procedure, we do the same and choose the model by the maximum
value of the acceptance rate lower bound. In Algorithm 1 we propose to sample from the posterior
distribution using the independent MH and the current proposal. It turns out in practice that it is
better to use the currently learned proposal qφ(θ) = N(θ | μ, σ) as the initial state for random-walk
MH algorithm. That is, We start with the mean μ as an initial point, and then use random-walk
proposal q(θ0 | θ) = N(θ0 | θ, σ) with the variances σ of current independent proposal. This should
be considered as a heuristic that improves the approximation of the loss function.
The optimization of the acceptance rate lower bound results in the better estimation of predictive
distribution than the variational inference (see Fig. 2). Optimization of acceptance rate for the same
number of epochs results in nearly 30% accuracy on the test set. That is why we do not report results
for this procedure in Fig. 2.
Figure 2: Negative log-likelihood (left) and accuracy (right) on test set of MNIST dataset for varia-
tional inference (blue lines) and the optimization of the acceptance rate lower bound (orange lines).
In both procedures we apply the independent MH algorithm to estimate the predictive distribution.
Figure 3: Test negative log-likelihood for two approximations of the predictive distribution based on
samples: from proposal distribution nllq and after MH correction nllMH . Left figure corresponds
to the optimization of the acceptance rate lower bound, right figure corresponds to the variational
inference.
To answer the second question we estimate predictive distribution in two ways. The first way is to
perform 100 accept/reject steps of the independent MH algorithm with the learned proposal qφ(θ)
8
Under review as a conference paper at ICLR 2019
after each epoch, i.e. perform MH correction of the samples from the proposal. The second way
is to take the same number of samples from qφ (θ) without MH correction. For both estimations of
predictive distribution, we evaluate negative log-likelihood on the test set and compare them.
The MH correction of the learned proposal improves the estimation of predictive distribution for the
variational inference (right plot of Fig. 3) but does not do so for the optimization of the acceptance
rate lower bound (left plot of Fig. 3). This fact may be considered as an implicit evidence that our
procedure learns the proposal distribution with higher acceptance rate.
4.3	Sample-based Setting
In the sample-based setting, we estimate density ratio using a discriminator. Hence we do not use the
minibatching property (see subsection 3.1) of the obtained lower bound, and optimization problems
for the acceptance rate and for the lower bound have the same efficiency in terms of using data.
That is why our main goal in this setting is to compare the optimization of the acceptance rate and
the optimization of the lower bound. Also, in this setting, we have Markov chain proposal that is
interesting to compare with the independent proposal. Summing up, we formulate the following
questions:
1.	Does the optimization of the lower bound has any benefits compared to the direct optimiza-
tion of the acceptance rate?
2.	Do we have mixing issue while learning Markov chain proposal in practice?
3.	Could we improve the visual quality of samples by applying the MH correction to the
learned proposal?
We use DCGAN architecture for the proposal and discriminator (see Appendix C.8) and apply our
algorithm to MNIST dataset. We consider two optimization problems: direct optimization of the
acceptance rate and its lower bound. We also consider two ways to obtain samples from the approx-
imation of the target distribution — use raw samples from the learned proposal, or perform the MH
algorithm, where we use the learned discriminator for density ratio estimation.
In case of the independent proposal, we show that the MH correction at evaluation step allows to
improve visual quality of samples — figures 4(a) and 4(b) for the direct optimization of acceptance
rate, figures 4(c) and 4(d) for the optimization of its lower bound. Note that in Algorithm 2 we do
not apply the independent MH algorithm during training. Potentially, one can use the MH algorithm
considering any generative model as a proposal distribution and learning a discriminator for density
ratio estimation. Also, for this proposal, we demonstrate the negligible difference in visual quality of
samples obtained by the direct optimization of acceptance rate (see Fig. 4(a)) and by the optimization
of the lower bound (see Fig. 4(c)).
10
2。
6。
70
136 7770s7√4
4 I 34√？。v477
3ifg 夕 4/4 77，
gi 9 产 σ-l3 勺Vcr
O oð V ? ∖64 4 7 /
3q774 73〃 q。
— 76 7573,3gl
。3夕r~3 〃夕,〃』
q47⅛ly6 9 33,
o4e2 77/Q 夕 4
7-rJ 9/√78 9
7 S 76 q O 3 3 3 G
ΛVΛV91-√∙‹-zβ λ173
44£？7«&c/s
G si-7/ q∕.5g 7Γ1
q/1 — 4/，q7q
OYO夕/ 4 / /7〃
978 75工 qOqs
3⅛>ob∕q∕0ΓΓf
7 J / √ / 5 76 4 b
(a)	(b)	(c)	(d)
Figure 4: Samples from the learned independent proposal obtained via optimization: of acceptance
rate (4(a), 4(b)) and its lower bound (4(c), 4(d)). In Fig. 4(b), 4(d) we show raw samples from
the learned proposal. In Fig. 4(a), 4(c) we show the samples after applying the independent MH
correction to the samples, using the learned discriminator for density ratio estimation.
In the case of the Markov chain proposal, we show that the direct optimization of acceptance rate
results in slow mixing (see Fig. 5(a)) — most of the time the proposal generates samples from one
of the modes (digits) and rarely switches to another mode. When we perform the optimization of
the lower bound the proposal switches between modes frequently (see Fig. 5(b)).
9
Under review as a conference paper at ICLR 2019
55GQ 33y4//
55SG 3 3?///»
5ss 733 7夕？f
55g □√ 3 >? σ/ / / /
/
?
VaOQJ^/夕夕ʃ/
0077 ,夕 ʃ 620夕
Qa∙κr∕∕fFg2
3268/2 彳？夕ɪ
彳1/，夕〃 ςbr/ /
? ʃ95¥夕,“m2
7 - V ^/71α/7
夕 3 夕 41 7/tf ZA/
。。夕6/夕90么7
夕q 4斗,Λ/ / / 7
Figure 5: SampleS from the chain
obtained via the MH algorithm with
the learned propoSal and the learned
diScriminator for denSity ratio eSti-
mation. Fig. 5(a) correSpondS to the
direct optimization of the acceptance
rate. Fig. 5(b) — to optimization of
the lower bound on acceptance rate.
SampleS in the chain are obtained
one by one from left to right from
top to bottom.
(b)
(a)
/N /夕yv>N rN /
了7，Xr∕7/夕，
yΛaa∕7777y
F / / h 夕 <i087δo 夕
/a，,əʌoʃ77/
7Nq,2夕£^S夕
/;。/7&，2«2
7* Z 8 夕3 & 7/ 夕 ʃ
g√77∕gg∕sa
y3o00405y?
404 78O/045
—54 335 夕 54。S
265今4gOo45
⅜5054 57054
49,7503005
55544 7O0S5
少//54U53〃53
8745 9 3455 F-
131004 0 553 Jz
Figure 6: Samples from the pro-
posal distribution and conditioned
on the digit in the red box. The pro-
posal was optimized according to the
lower bound on the acceptance rate.
Note that we obtain different distri-
butions of the samples because of
conditioning of our proposal.
5


To show that the learned proposal distribution has the Markov property rather than being totally
independent, we show samples from the proposal conditioned on two different points in the dataset
(see Fig. 6). The difference in samples from two these distributions (Fig. 6(a), 6(a)) reflects the
dependence on the conditioning.
Additionally, in Appendix C.9 we present samples from the chain after 10000 accepted images and
also samples from the chain that was initialized with noise.
5 Discussion and future work
This paper proposes to use the acceptance rate of the MH algorithm as the universal objective for
learning to sample from some target distribution. We also propose the lower bound on the acceptance
rate that should be preferred over the direct maximization of the acceptance rate in many cases. The
proposed approach provides many ways of improvement by the combination with techniques from
the recent developments in the field of MCMC, GANs, variational inference. For example
•	The proposed loss function can be combined with the loss function from (Levy et al., 2017),
thus allowing to learn the Markov chain proposal in the density-based setting.
•	We can use stochastic Hamiltonian Monte Carlo (Chen et al., 2014) for the loss estimation
in Algorithm 1.
•	In sample-based setting one can use more advanced techniques of density ratio estimation.
Application of the MH algorithm to improve the quality of generative models also requires exhaus-
tive further exploration and rigorous treatment.
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. WaSSerStein gan. arXiv preprint
arXiv:1701.07875, 2017.
Haoyu Chen, Daniel Seita, Xinlei Pan, and John Canny. An efficient minibatch acceptance teSt for
metropoliS-haStingS. arXiv preprint arXiv:1610.06848, 2016.
10
Under review as a conference paper at ICLR 2019
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International Conference on Machine Learning, pp. 1683-1691, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Yarin Gal. Uncertainty in deep learning. PhD thesis, University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approx-
imate variational inference. arXiv preprint arXiv:1506.02158, 2015.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Paolo Giordani and Robert Kohn. Adaptive independent metropolis-hastings by fast estimation of
mixtures of normals. Journal of Computational and Graphical Statistics, 19(2):243-259, 2010.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Aditya Grover, Ramki Gummadi, Miguel Lazaro-Gredilla, Dale Schuurmans, and Stefano Ermon.
Variational rejection sampling. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceed-
ings of the Twenty-First International Conference on Artificial Intelligence and Statistics, vol-
ume 84 of Proceedings of Machine Learning Research, pp. 823-832, Playa Blanca, Lanzarote,
Canary Islands, 09-11 Apr 2018. PMLR. URL http://proceedings.mlr.press/v84/
grover18a.html.
Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths
in hamiltonian monte carlo. Journal of Machine Learning Research, 15(1):1593-1623, 2014.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.
Lars Holden, Ragnar Hauge, Marit Holden, et al. Adaptive independent metropolis-hastings. The
Annals of Applied Probability, 19(1):395-413, 2009.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparame-
terization trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
Anoop Korattikara, Yutian Chen, and Max Welling. Austerity in mcmc land: Cutting the metropolis-
hastings budget. In International Conference on Machine Learning, pp. 181-189, 2014.
Daniel Levy, Matthew D Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte
carlo with neural networks. arXiv preprint arXiv:1711.09268, 2017.
Jun S Liu, Faming Liang, and Wing Hung Wong. The multiple-try method and local optimization
in metropolis sampling. Journal of the American Statistical Association, 95(449):121-134, 2000.
Luca Martino. A review of multiple try mcmc algorithms for signal processing. Digital Signal
Processing, 2018.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722,
2017.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. arXiv preprint arXiv:1701.05369, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
11
Under review as a conference paper at ICLR 2019
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, 2014.
Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal scaling
of random walk metropolis algorithms. The annals of applied probability, 7(1):110-120, 1997.
Gareth O Roberts, Jeffrey S Rosenthal, et al. Optimal scaling for various metropolis-hastings algo-
rithms. Statistical science, 16(4):351-367, 2001.
Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. In
Advances in Neural Information Processing Systems, pp. 5140-5150, 2017.
Michalis Titsias and MigUeI Lazaro-Gredilla. Doubly stochastic variational bayes for non-conjugate
inference. In International Conference on Machine Learning, pp. 1971-1979, 2014.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681-688,
2011.
12
Under review as a conference paper at ICLR 2019
A Acceptance rate of the MH algorithm
A.1 Proof of Theorem 1
Remind that We have random variables ξ = P(X))；((：0|：)),x 〜p(x),x0 〜 q(x01 x) and U 〜
Uniform[0, 1], and want to prove the following equalities.
Eξ min{1, ξ} = P{ξ > u} = 1 - 1 Eξ∣ξ - 1|
Equality Eξ min{1, ξ} = P{ξ > u} is obvious.
Eξ min{1, ξ} =	pξ (x) min{1, x}dx =	pξ (x)dx +	pξ (x)xdx
P{ξ>u}=	dxpξ (x)	[0 ≤ u ≤ 1]du =	pξ (x)dx +	pξ (x)xdx
(17)
(18)
(19)
Equality P{ξ > u} = 1 - 1 Eξ∣ξ - 1| can be proofed as follows.
P{ξ
> u} = du	pξ (x)dx =	(1 - Fξ (u))du
0u	0
1-
upξ (u)du
1 - Fξ (1) +	upξ (u)du,
0
(20)
(21)
where Fξ (u) is CDF of random variable ξ. Note that Fξ(0) = 0 since ξ ∈ (0, +∞]. Eq. 21 can be
rewritten in two ways.
1-Fξ(1)+/ upξ(u)du =1 + / (U — 1)pξ(u)du = 1 — / |u — 1∣pξ(u)du
To rewrite Eq. 21 in the second way we note that Eξ = 1.
1-Fξ(1)+/ upξ(u)du = /	pξ(u)du+1- /	upξ(u)du = 1—/	|u—1∣pξ(u)du
01	1	1
Summing equations 22 and 23 results in the following formula
P{ξ>u} = 1 - 2Eξ∣ξ - 1|.
Using the form of ξ we can rewrite the acceptance rate as
1 — ^Eξ∣ξ — 1| = 1 — TV(p(x0)q(x | x0) p(x)q(x0 | x)).
(22)
(23)
(24)
(25)
A.2 Acceptance rate of independent MH defines semimetric in distribution
SPACE
In independent case we have ξ
P(X)qq(x)，x 〜 p(x),x0 〜 q(x0) and We want to prove that
Eξ∣ξ — 1| is semimetric (or pseudo-metric) in space of distributions. For this appendix, we denote
D(p,q) = Eξ∣ξ — 1|. The first two axioms for metric obviously holds
1.	D(p, q) = 0 Q⇒ P = q
2.	D(p, q) = D(q, p)
There is an example when triangle inequality does not hold. For distributions p =
Uniform[0, 2/3], q = Uniform[1/3, 1], s = Uniform[0, 1]
D(p, S) + D(q, s) = 4 < 2 = D(p, q).	(26)
13
Under review as a conference paper at ICLR 2019
But weaker inequality can be proved.
D(p, s) + D(q, s)
|p(x)s(y)
- p(y)s(x)|dydx +
|q (x)s(y)
- q(y)s(x)|dydx
| p(x)s(y)q(z) - p(y)s(x)q(z) | + | q(x)s(y)p(z) - q(y)s(x)p(z) |
、--V--} 、--V--}	、--V--} 、--V--}
ab	cd
D(p, s) + D(q, s)
|q(x)s(z)p(y)
dxdydz
|p(z)s(y)q(x) - p(y)s(z)q(x)|dxdydz+
- q(z)s(x)p(y)|dxdydz ≥
q(x)s(y)p(z) - p(y)s(x)q(z)
J X--{--} X--{--}
cb
dxdydz
D(p, s) + D(q, s)
|p(z)s(x)q(y) - p(x)s(z)q(y)|dxdydz+
+	|q(y)s(z)p(x) - q(z)s(y)p(x)|dxdydz ≥
q(y)s(x)p(z) - p(x)s(y)q(z)
J X--V--' X--V--'
da
dxdydz
(27)
(28)
(29)
(30)
(31)
(32)
/
+
Summing up equations 28, 30 and 32 we obtain
3(D(p, s) + D(q, s)) ≥ dxdydz |a - b| + |c - d| + |c - b| + |d - a| ≥ 2 dxdydz|d - b| =
(33)
2 dxdydzs(x)q(y)p(z) - q(z)p(y) = 2D(p, q)
(34)
2
D(p, S) + D(q, S) ≥ 3D(p, q)	(35)
B Optimization of proposal distribution
B.1	On collapsing to the delta-function
Firstly, let’s consider the case of gaussian random-walk proposal q(x0 | x) = N(x0 | x, σ). The
optimization problem for the acceptance rate takes the form
AR =	dxdx0P(X)N(x01 x, σ) min < 1, P x > → max .
p(x)	σ
(36)
It is easy to see that we can obtain acceptance rate arbitrarly close to 1, taking σ small enough.
In the case of the independent proposal, we don’t have the collapsing to the delta-function problem.
In our work, it is important to show non-collapsing during optimization of the lower bound, but the
same hold for the direct optimization of the acceptance rate. To provide such intuition we consider
one-dimensional case where we have some target distribution P(x) and independent proposal q(x) =
N(x | μ,σ). Choosing σ small enough, We approximate sampling with the independent MH as
sampling on some finite support X ∈ [μ - a,μ + a]. For this support, we approximate the target
distribution with the uniform distribution (see Fig. 7).
For such approximation, optimization of lower bound takes the form
min KL(P(x)kq(x)) + KL(q(x)kP(x))	(37)
q
min KL(Uniform[-a, a]kN (x | 0, σ, -a, a)) + KL(N (x | 0, σ, -a, a)kUniform[-a, a])	(38)
σ
14
Under review as a conference paper at ICLR 2019
Figure 7: In this figure we show schematic view of approximation of of target distribution with
uniform distribution. Red bounding box is made bigger for better comprehension.
Here N(x | 0, σ, -a, a) is truncated normal distribution. The first KL-divergence can be written as
follows.
KL (Uniform [-a, a] kN (X | 0,σ, -a, a))=———/ dx log N (X | 0,σ, -a, a) — log 2a = (39)
2a -a
1
—
2a
1 2a3
-	2a log(σZ) - a log 2π - 2-2 —
-	log 2a
(40)
log σ + log Z + ʌr + 1 log 2π - log 2a
6σ2	2
(41)
Here Z is normalization constant of truncated log normal distribution and Z = Φ(a∕σ) - Φ(-a∕σ),
where Φ(x) is CDF of standard normal distribution. The second KL-divergence is
KL(N (X | 0, , -a, a)kUniform[-a, a]) =
-	1 log(2πe) - log σ - log Z + √2a-Z exp ( -+ log 2a
(42)
(43)
Summing up two KL-divergencies and taking derivative w.r.t. σ we obtain
∂
—— KL (Uniform [ - a, a] kN (X | 0,σ, -a, a)) + KL(N (X | 0,σ, -a, a)kUniform[-a, a]) =
∂-
(44)
a2	a3	a2	a	a2
-3-3+ √2π-4Z exp(-2-2J+ √2π exp(—而)
1
-2Z
1	-2a	a2
-Z2 -2√2πexp (-2σ2 力=
(45)
1	a3	a2	a2	a2	2a	a2
a [-3-3+ √2π-2Zexp (-取)①-1 + √2π-Zexp (-2σ2 川
(46)
To show that the derivative of the lower bound w.r.t. is negative, we need to prove that the
following inequality holds for positive X.
-L3+——-=------X---------exp(-X2/2) fx2-1+——-=--------------exp(-x2∕2)) < 0
3	√2π(Φ(x) — Φ(-x))	√2π(Φ(x) — Φ(-x))
X>0
Defining φ(X)
equality 47 as
,2	(47)
fX e-t /2dt and noting that 2Φ(x) = √2π(Φ(x) - Φ(-x)) We can rewrite in-
1	-x2/2 2	2Xe-x2/2	2X
而e	(X	Φ(x)) < 三
X>0
(48)
15
Under review as a conference paper at ICLR 2019
By the fundamental theorem of calculus, we have
xe
-x2/2
Z x e-t2/2(1 - t2)dt
0
Hence,
x
φ(X)-Xe-x"Z e-"2dt ≥ 产/“
x t2dt
e-x
3
2/2 x3
3
Or equivalently,
φ(x) ≥ e-x2/2x3 + 3x
Using this inequality twice, we obtain
e-x2∕2 V 3
φ(X)	X(X2 + 3)
and
2	Xe-x /2	2	3	X2(2 + X2)
X - 1 + 1(XΓ ≤ X - 1 + X2+3 = χ2 + 3
Thus, the target inequality can be verified by the verification of
3x(2 + x2) 2x
(x2 +3)2 ≤ T.
(49)
(50)
(51)
(52)
(53)
(54)
Thus, we show that partial derivative of our lower bound w.r.t. σ is negative. Using that knowledge
we can improve our loss by taking a bigger value of σ. Hence, such proposal does not collapse to
delta-function.
B.2	Intuition for better gradients in sample-based setting
In this section, we provide an intuition for sample-based setting that the loss function for lower
bound has better gradients than the loss function for acceptance rate. Firstly, we remind that in the
sample-based setting we use a discriminator for density ratio estimation.
DmxO)=，、，p(x)q(Xr) 一，、
p(X)q(X0 | X) + p(X0)q(X | X0)
(55)
For this purpose we use the discriminator of special structure
/六 /	/ ∖ ∖	1
D(x, x0) = ~~ exp(D(X，X))~------------=------------------1--------------T (56)
exp(D(X, X0)) + exp(D(X0, X))	1+exp -(De(X,X0) -De(X0,X))
We denote d(X, X0) = D(X, X0) - D(X0, X) and consider the case when the discriminator can easily
distinguish fake pairs from valid pairs. So D(x, x0) is close to 1 and d(X, x0)》0 for X 〜P(X)
and X0 〜q(X | x). To evaluate gradients We consider Monte Carlo estimations of each loss and take
gradients w.r.t. X0 in order to obtain gradients for parameters of proposal distribution. We do not
introduce the reparameterization trick to simplify the notation but assume it to be performed. For
the optimization of the acceptance rate We have
dXdX0p(X)q(X0|X)
p(X0)q(X | X0)
---------------1 ’
p(X)q(X0 | X)
p(X0)q(X | X0)
—；~:—；——；—— — 1
p(X)q (X0 | X)
p(X0)q(X | X0)
—:~：—:—；—— — 1
p(X)q (X0 | X)
1 — D(x, X0)
D(X, X0)
(57)
(58)

∂Lar	1	∂D(x,x0)
∂x0	D2(x,x0) ∂x0
0 ∂d(X, X0)
exp(-d(x,X)) -^x-
(59)
16
Under review as a conference paper at ICLR 2019
While for the optimization of the lower bound we have
0	0	p(x)q(x0 | x)	p(x)q(x0 |x)
J dX(dX p(x)q(x | X)Iog (p(χ0)q(χ | χ0)) ' log (p(χ0)q(χ | χ0))
p(x0)q(x | x0)	1 - D(x, x0)
LB = - og (P(X)q(x0∣ x) ) ≈- og ( D(X,X0))
∂Llb	1	∂D(x,x0)	∂d(X,X0)
∂x0	(1 — D(x, x0 ))D(x, x0)	∂x0	∂x0
Now we compare Eq. 59 and Eq. 62. We see that in case of strong discriminator we have vanishing
gradients in Eq. 59 due to exp(-d(x, x0)), while it is not the case for Eq. 62.
C	Experiments
C.1 Toy problem
(60)
(61)
(62)
Figure 8: Level-plots in parameter space for the toy problem. Left: level-plot for the acceptance rate
of the MH algorithm. Right: level-plot for the lower bound of the acceptance rate.
-0.25
-β.αo
—0.50
--0.75
--1.50
1.75

This experiment shows that it is possible to optimize the acceptance rate, optimizing its lower bound.
For the target distribution We consider bimodal Gaussian p(x) = 0.5 ∙ N(x | — 2,0.5) + 0.5 ∙
N(x | 2,0.7), for the independent proposal we consider unimodal gaussian q(x) = N(x | μ, σ). We
perform stochastic gradient optimization using Algorithm 1 from the same initialization for both
objectives (Fig. 8) and obtain approximately the same local maximums.
C.2 Architecture of the RealNVP proposal
For the proposal distribution we use similar architecture to the NICE proposal. The RealNVP model
(Dinh et al., 2016) use the same strategy for evaluating the Jacobian as the NICE model does. Each
coupling layer define the following function. Given a D dimensional input x and d < D, the output
y is evaluated by the formula
y1:d = x1:d ,
yd+1:D = xd+1:D	exp(s(x1:d)) + t(x1:d),
where the functions s, t can be arbitrary complex, since the structure of the functions doesn’t influ-
ence the computation of the Jacobian.
For our proposal we use 4 coupling layers with s and t consist of two fully-connected layers with
hidden dimension of 256.
C.3 Synthetic distributions densities
For synthetic distributions we consider the same distributions as in Song et al. (2017).
17
Under review as a conference paper at ICLR 2019
The analytic form of p(x) for ring is:
p(x) H exp(-U(x)),
U(x)
0.32
(63)
The analytic form of p(x) for mog2 is:
p(x) = 2 N (x∣μι,σι) + 1N (x∣μ2,σ2)	(64)
where μι = [5, 0], μ2 = [—5, 0], σι = σ2 = [0.5,0.5].
The analytic form of p(x) for mog6 is:
16
p(x)=- EN (x∣μi ,σi)	(65)
i=1
where μi = [sin i3∏, cos i3∏] and σ% = [0.5,0.5].
The analytic form of p(x) for ring5 is:
p(x) H exp(-U (x)), U(x) = min(u1, u2, u3, u4, u5)	(66)
where Ui = ('x2 + X2 — i)2∕0.04.
C.4 Effective Sample Size formulation
For the effective sample size formulation we follow Song et al. (2017).
Assume a target distribution p(x), and a Markov chain Monte Carlo (MCMC) sampler that produces
a set ofN correlated samples {xi}1N from some distribution q({xi}1N) such that q(xi) = p(xi). Sup-
pose we are estimating the mean of p(x) through sampling; we assume that increasing the number
of samples will reduce the variance of that estimate.
Let V = Varq [PiN=1 xi/N] be the variance of the mean estimate through the MCMC sam-
ples. The effective sample size (ESS) of {xi}1N, which we denote as M = ESS({xi}1N), is
the number of independent samples from p(x) needed in order to achieve the same variance, i.e.
Varp [PjM=1 xj /M] = V . A practical algorithm to compute the ESS given {xi}1N is provided by:
ESS({xi}1N)
N
1 +2 PN=II(I- N )ρs
(67)
where ρs denotes the autocorrelation under q of x at lag s. We compute the following empirical
estimate PS for ρs:
ρs
1
σ2 (N — S)
N
):(Xn - μ)(xn-s
=S+1
-μ)
(68)
where μ and σ are the empirical mean and variance obtained by an independent sampler.
Due to the noise in large lags s, we adopt the approach of Hoffman & Gelman (2014) where we
truncate the sum over the autocorrelations when the autocorrelation goes below 0.05.
C.5 Optimization of the lower bound
In this section we provide the empirical evidence that maximization of the proposed lower bound on
the acceptance rate (ARLB) results in maximization of the acceptance rate (AR). For that purpose
we evaluate ARLB and AR at each iteration during the optimization of ARLB. After training we
evaluate correlation coefficient between ARLB and logarithm of AR. The curves are shown in Fig.
9. Correlation coefficients for different distributions are: -0.914 (ring), -0.905 (mog2), -0.956
(mog6), -0.982 (ring5).
18
Under review as a conference paper at ICLR 2019
3.β
2.5
Ring, COrrelatiOn=-0.914


train loss
acceptance rate
—train loss
acceptance rate
Mog2, correlation=-0.905^

iteration
iteration
(a)
Mog6, Correlation=-0.956
tram loss
acceptance rate
5.0
4.5
4.0
(b)
(°:W 6-
train loss
acceptance rate
iteration
(c)
Figure 9: plots for the acceptance rate and the acceptance rate lower bound evaluated at every
iteration during the optimization of the acceptance rate lower bound. Correlation coefficient is
evaluated between the logarithm of the acceptance rate and the acceptance rate lower bound.
-2.5
iteration
(d)
19
Under review as a conference paper at ICLR 2019
C.6 Learned proposals
In this section we provide levelplots of learned proposals densities (see Fig. 10). We also provide
2d histrograms of samples from the MH algorithm using the corresponding proposals (see Fig. 11).
FaI2
0.09
0.06
0.03
L 0.8
Varlatlonal Inference
Figure 10: levelplots of learned proposal densities. For each distribution from left to right proposals
are learned by: variational inference, the acceptance rate maximization, the acceptance rate lower
bound maximization.
Varlatlonal Inference
Acceptance Rate
Acceptance Rate Lower Bound
DHDDHDK
，	“	∙	3	・♦	∙	“	∙	3	・♦	∙ a ∙	3	・♦	'
Figure 11: 2d histrograms of samples from the MH algorithm with different proposals. For each
distribution from left to right proposals are learned by: variational inference, the acceptance rate
maximization, the acceptance rate lower bound maximization.
C.7 Architecture of the reduced LeNet-5
class LeNet5(BayesNet):
def __init__(self):
super(LeNet5, self).__init__()
self.num_classes = 10
self.conv1 = layers.ConvFFG(1, 10, 5, padding=0)
self.relu1 = nn.ReLU(True)
self.pool1 = nn.MaxPool2d(2, padding=0)
self.conv2 = layers.ConvFFG(10, 20, 5, padding=0)
self.relu2 = nn.ReLU(True)
self.pool2 = nn.MaxPool2d(2, padding=0)
self.flatten = layers.ViewLayer([20*4*4])
self.dense1 = layers.LinearFFG(20*4*4, 10)
self.relu3 = nn.ReLU()
self.dense2 = layers.LinearFFG(10, 10)
20
Under review as a conference paper at ICLR 2019
C.8 Architectures of neural networks in sample-based setting
In sample-based setting we use usual DCGAN architecture for independent proposal distribution
class Generator(layers.ModuleWrapper):
def __init__(self):
super(Generator, self).__init__()
self.fc = nn.Linear(100, 128*8*8)
self.unflatten = layers.ViewLayer([128, 8, 8])
self.in1 = nn.InstanceNorm2d(128)
self.us1 = nn.ConvTranspose2d(128, 128, 2, 2)
self.conv1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
self.in2 = nn.InstanceNorm2d(128, 0.8)
self.lrelu1 = nn.LeakyReLU(0.2, inplace=True)
self.us2 = nn.ConvTranspose2d(128, 128, 2, 2)
self.conv2 = nn.Conv2d(128, 64, 3, stride=1, padding=1)
self.in3 = nn.InstanceNorm2d(64, 0.8)
self.lrelu2 = nn.LeakyReLU(0.2, inplace=True)
self.conv3 = nn.Conv2d(64, 1, 3, stride=1, padding=1)
self.tanh = nn.Tanh()
And a little be modified acrhitecture for Markov chain proposal distribution
class Generator(layers.ModuleWrapper):
def __init__(self):
super(Generator, self).__init__()
self.d_conv1 = nn.Conv2d(1, 16, 5, stride=2, padding=2)
self.d_lrelu1 = nn.LeakyReLU(0.2, inplace=True)
self.d_do1 = nn.Dropout2d(0.5)
self.d_conv2 = nn.Conv2d(16, 4, 5, stride=2, padding=2)
self.d_in2 = nn.InstanceNorm2d(4, 0.8)
self.d_lrelu2 = nn.LeakyReLU(0.2, inplace=True)
self.d_do2 = nn.Dropout2d(0.5)
self.b_view = layers.ViewLayer([4*8*8])
self.b_fc = nn.Linear(4*8*8, 256)
self.b_lrelu = nn.LeakyReLU(0.2, inplace=True)
self.b_fc = nn.Linear(256, 128 * 8 * 8)
self.b_do = layers.AdditiveNoise(0.5)
self.e_unflatten = layers.ViewLayer([128, 8, 8])
self.e_in1 = nn.InstanceNorm2d(128, 0.8)
self.e_us1 = nn.ConvTranspose2d(128, 128, 2, 2)
self.e_conv1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
self.e_in2 = nn.InstanceNorm2d(128, 0.8)
self.e_lrelu1 = nn.LeakyReLU(0.2, inplace=True)
self.e_us2 = nn.ConvTranspose2d(128, 128, 2, 2)
self.e_conv2 = nn.Conv2d(128, 64, 3, stride=1, padding=1)
self.e_in3 = nn.InstanceNorm2d(64, 0.8)
self.e_lrelu2 = nn.LeakyReLU(0.2, inplace=True)
self.e_conv3 = nn.Conv2d(64, 1, 3, stride=1, padding=1)
self.e_tanh = nn.Tanh()
For both proposals we use the proposed discriminator with the following architecture.
class Discriminator(nn.Module):
def __init__(self):
super(Discriminator, self).__init__()
self.conv1 = nn.Conv2d(2, 16, 3, 2, 1)
21
Under review as a conference paper at ICLR 2019
self.lrelu1 = nn.LeakyReLU(0.2, inplace=True)
self.conv2 = nn.Conv2d(16, 32, 3, 2, 1)
self.lrelu2 = nn.LeakyReLU(0.2, inplace=True)
self.in2 = nn.InstanceNorm2d(32, 0.8)
self.conv3 = nn.Conv2d(32, 64, 3, 2, 1)
self.lrelu3 = nn.LeakyReLU(0.2, inplace=True)
self.in3 = nn.InstanceNorm2d(64, 0.8)
self.conv4 = nn.Conv2d(64, 128, 3, 2, 1)
self.lrelu4 = nn.LeakyReLU(0.2, inplace=True)
self.in4 = nn.InstanceNorm2d(128, 0.8)
self.flatten = layers.ViewLayer([128*2*2])
self.fc = nn.Linear(128*2*2, 1)
def forward(self, x, y):
xy = torch.cat([x, y], dim=1)
for module in self.children():
xy = module(xy)
yx = torch.cat([y, x], dim=1)
for module in self.children():
yx = module(yx)
return F.softmax(torch.cat([xy, yx], dim=1), dim=1)
C.9 Additional figures for Markov chain proposals in sample-based setting
In this section, we show additional figures for Markov chain proposals. In Fig. 12 we show samples
from the chain that was initialized by the noise. In Fig. 13 we show samples from the chain after
10000 accepted samples.
α
w
to
30
40
sα
βa
TO
80
M
G 5“ ð ð 3 O/ ⅛ 1
c∖r2J Z46 夕 7qs
√>c6？7 ʒ 4∙ √ ʃ-7q
5¾%q3 7〃θ2q
q3QS6575a Λr
qQ 33 〃 0τ*，Zq
O33o0∙τyc∖5
6G¾07O54IV
Sq3ass45 94
LrS6g4 37Ig
OUr7x233∂7
0<7Γ∙ 7 A 2¾3∂7
OE，勺yc 333乙ɔ
O C Γs772¾33~/
。,「'7，OoS331
@ " γ∙77833∙5γλ
GU f 7 78832 3
CsrGπ-7783z23
ss u773 £2 a ʒ
so N 7soo^33
01234SS7S9	0	1234567a	9
(a)
(b)
Figure 12: Samples from the chain initialized with noise. To obtain samples we use the MH algo-
rithm with the learned proposal and the learned discriminator for density ratio estimation. In Fig.
5(a) we use proposal and discriminator that are learned during optimization of acceptance rate. In
Fig. 5(b) we use proposal and discriminator that are learned during the optimization of the accep-
tance rate lower bound. Samples in the chain are obtained one by one from left to right from top to
bottom starting with noise (first image in the figure).
22
Under review as a conference paper at ICLR 2019
012345S7S9
(a)
qlg 72 夕 CTJJT 7/
5 2¼7 ,/ 8 7 J /
21”/ /"/N/Γ
92/<3夕夕/3,2
O477jy7,二r
06∕∕∕∕pq7?
35,7Ck 7 bq “夕
50/2y2225-。
“s∕÷? ʃ7317
56* 7夕/2252
012345S7S9
(b)
Figure 13: Samples from the chain after 10000 accepted samples. To obtain samples we use the
MH algorithm with the learned proposal and the learned discriminator for density ratio estimation.
In Fig. 5(a) we use proposal and discriminator that are learned during optimization of acceptance
rate. In Fig. 5(b) we use proposal and discriminator that are learned during the optimization of the
acceptance rate lower bound. Samples in chain are obtained one by one from left to right from top
to bottom.
23