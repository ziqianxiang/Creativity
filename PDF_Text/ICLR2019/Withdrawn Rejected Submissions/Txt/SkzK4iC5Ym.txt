Under review as a conference paper at ICLR 2019
Diminishing Batch Normalization
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a generalization of the BN algorithm, diminishing batch
normalization (DBN), where we update the BN parameters in a diminishing mov-
ing average way. Batch normalization (BN) is very effective in accelerating the
convergence of a neural network training phase that it has become a common
practice. Our proposed DBN algorithm remains the overall structure of the origi-
nal BN algorithm while introduces a weighted averaging update to some trainable
parameters. We provide an analysis of the convergence of the DBN algorithm that
converges to a stationary point with respect to trainable parameters. Our analysis
can be easily generalized for original BN algorithm by setting some parameters
to constant. To the best knowledge of authors, this analysis is the first of its kind
for convergence with Batch Normalization introduced. We analyze a two-layer
model with arbitrary activation function. The primary challenge of the analysis is
the fact that some parameters are updated by gradient while others are not. The
convergence analysis applies to any activation function that satisfies our common
assumptions. For the analysis, we also show the sufficient and necessary con-
ditions for the stepsizes and diminishing weights to ensure the convergence. In
the numerical experiments, we use more complex models with more layers and
ReLU activation. We observe that DBN outperforms the original BN algorithm
on Imagenet, MNIST, NI and CIFAR-10 datasets with reasonable complex FNN
and CNN models.
1	Introduction
Deep neural networks (DNN) have shown unprecedented success in various applications such as
object detection. However, it still takes a long time to train a DNN until it converges. Ioffe &
Szegedy identified a critical problem involved in training deep networks, internal covariate shift, and
then proposed batch normalization (BN) to decrease this phenomenon. BN addresses this problem
by normalizing the distribution of every hidden layer’s input. In order to do so, it calculates the pre-
activation mean and standard deviation using mini-batch statistics at each iteration of training and
uses these estimates to normalize the input to the next layer. The output of a layer is normalized by
using the batch statistics, and two new trainable parameters per neuron are introduced that capture
the inverse operation. It is now a standard practice Bottou et al. (2016); He et al. (2016). While this
approach leads to a significant performance jump, to the best of our knowledge, there is no known
theoretical guarantee for the convergence of an algorithm with BN. The difficulty of analyzing the
convergence of the BN algorithm comes from the fact that not all of the BN parameters are updated
by gradients. Thus, it invalidates most of the classical studies of convergence for gradient methods.
In this paper, we propose a generalization of the BN algorithm, diminishing batch normalization
(DBN), where we update the BN parameters in a diminishing moving average way. It essentially
means that the BN layer adjusts its output according to all past mini-batches instead of only the
current one. It helps to reduce the problem of the original BN that the output of a BN layer on a
particular training pattern depends on the other patterns in the current mini-batch, which is pointed
out by Bottou et al.. By setting the layer parameter we introduce into DBN to a specific value, we
recover the original BN algorithm.
We give a convergence analysis of the algorithm with a two-layer batch-normalized neural net-
work and diminishing stepsizes. We assume two layers (the generalization to multiple layers can
be made by using the same approach but substantially complicating the notation) and an arbitrary
loss function. The convergence analysis applies to any activation function that follows our common
1
Under review as a conference paper at ICLR 2019
assumption. The main result shows that under diminishing stepsizes on gradient updates and up-
dates on mini-batch statistics, and standard Lipschitz conditions on loss functions DBN converges
to a stationary point. As already pointed out the primary challenge is the fact that some trainable
parameters are updated by gradient while others are updated by a minor recalculation.
Contributions. The main contribution of this paper is in providing a general convergence guar-
antee for DBN. Specifically, we make the following contributions.
•	In section 4, we show the sufficient and necessary conditions for the stepsizes and dimin-
ishing weights to ensure the convergence of BN parameters.
•	We show that the algorithm converges to a stationary point under a general nonconvex
objective function.
This paper is organized as follows. In Section 2, we review the related works and the development
of the BN algorithm. We formally state our model and algorithm in Section 3. We present our main
results in Sections 4. In Section 5, we numerically show that the DBN algorithm outperforms the
original BN algorithm. Proofs for main steps are collected in the Appendix.
2	Literature Review
Before the introduction of BN, it has long been known in the deep learning community that input
whitening and decorrelation help to speed UP the training process. In fact, Orr & Muller show that
preprocessing the data by subtracting the mean, normalizing the variance, and decorrelating the input
has various beneficial effects for back-propagation. Krizhevsky et al. propose a method called local
response normalization which is inspired by computational neuroscience and acts as a form of lateral
inhibition, i.e., the capacity of an excited neuron to reduce the activity of its neighbors. GUlcehre
& Bengio propose a standardization layer that bears significant resemblance to batch normalization,
except that the two methods are motivated by very different goals and perform different tasks.
Inspired by BN, several new works are taking BN as a basis for further improvements. Layer normal-
ization Ba et al. (2016) is much like the BN except that it uses all of the summed inputs to compute
the mean and variance instead of the mini-batch statistics. Besides, unlike BN, layer normaliza-
tion performs precisely the same computation at training and test times. Normalization propagation
that Arpit et al. uses data-independent estimations for the mean and standard deviation in every
layer to reduce the internal covariate shift and make the estimation more accurate for the validation
phase. Weight normalization also removes the dependencies between the examples in a minibatch
so that it can be applied to recurrent models, reinforcement learning or generative models Salimans
& Kingma (2016). Cooijmans et al. propose a new way to apply batch normalization to RNN and
LSTM models.
Given all these flavors, the original BN method is the most popular technique and for this reason our
choice of the analysis. To the best of our knowledge, we are not aware of any prior analysis of BN.
BN has the gradient and non-gradient updates. Thus, nonconvex convergence results do not im-
mediately transfer. Our analysis explicitly considers the workings of BN. However, nonconvex
convergence proofs are relevant since some small portions of our analysis rely on known proofs and
approaches.
Neural nets are not convex, even if the loss function is convex. For classical convergence results
with a nonconvex objective function and diminishing learning rate, we refer to survey papers Bert-
sekas (2011); Bertsekas & Tsitsiklis (2000); Bottou et al. (2016). Bertsekas & Tsitsiklis provide
a convergence result with the deterministic gradient with errors. Bottou et al. provide a conver-
gence result with the stochastic gradient. The classic analyses showing the norm of gradients of
the objective function going to zero date back to Grippo (1994); Polyak & Tsypkin (1973); Polyak
(1987). For strongly convex objective functions with a diminishing learning rate, we learn the classic
convergence results from Bottou et al..
2
Under review as a conference paper at ICLR 2019
3	Model and Algorithm
The optimization problem for a network is an objective function consisting of a large number of
component functions, that reads:
N
min f(θ, λ) = X fi(Xi : θ, λ),
i=1
subject to θ ∈ P, λ ∈ Q,
(1)
where fi : Rn1 × Rn2 → R, i = 1, ..., N, are real-valued functions for any data record Xi. Index i
associates with data record Xi and target response yi (hidden behind the dependency of f on i) in the
training set. Parameters θ include the common parameters updated by gradients directly associated
with the loss function, i.e., behind the part that we have a parametric model, while BN parameters
λ are introduced by the BN algorithm and not updated by gradient methods but by the mini-batch
statistics. We define that the derivative of fi is always taken with respect to θ:
▽fi(Xi : θ,λ) := Vθfi(Xi : θ,λ).
(2)
The deep network we analyze has 2 fully-connected layers with D1 neurons each. The techniques
presented can be extended to more layers with additional notation. Each hidden layer computes
y = a(Wu) with activation function a(∙) and U is the input vector of the layer. We do not need to
include an intercept term since the BN algorithm automatically adjusts for it. BN is applied to the
output of the first hidden layer.
Figure 1: The structure of our batch-normalized network model in the analysis.
We next describe the computation in each layer to show how we obtain the output of the network.
The notations introduced here is used in the analysis. Figure 1 shows the full structure of the net-
work. The input data is vector X, which is one of {Xi}N=ι. Vector λ = ((μj ):，(σ7- )%J is the
set of all BN parameters and vector θ = W1, W2, (βj(1))jD=1, (γj(1))jD=1 is the set of all trainable
parameters which are updated by gradients.
Matrices W1 , W2 are the actual model parameters and β, γ are introduced by BN. The value of jth
neuron of the first hidden layer is
zj1)(X : θ) = a(Wι,j,∙X),
(3)
where Wι,j,∙ denotes the weights of the linear transformations for the jth neuron.
The j th entry of batch-normalized output of the first layer is
yj(1)(X: θ, λ) = γj(1)
ZjI)(X: θ—μj! β⑴
σj+ eB	+βj ,
3
Under review as a conference paper at ICLR 2019
where βj1) and γj1) are trainable parameters updated by gradient and μj and σ7- are batch normal-
ization parameters for zj1). Trainable parameter μj is the mini-batch mean of zj1) and trainable
parameter σj is the mini-batch sample deviation of zj(1). Constant B keeps the denominator from
zero. The output of jth entry of the output layer is:
zj2)(X ： θ) = a (W2,j,∙ γj1) (zji)(X + ：]! + βj1)#!	(4)
The objective function for the ith sample is
fi(Xi : θ,λ) = LiQZ(2 (Xi : θ,λ)) j ,	(5)
where li(∙) is the loss function associated with the target response yi. For sample i, We have the
following complete expression for the objective function:
fi(Xi ： θ,λ)=ii 卜X W2,k,j H a(W；：+；- μ)+βj]J.	(6)
Function fi(Xi : θ, λ) is nonconvex with respect to θ and λ.
3.1	Algorithm
Algorithm 1 shows the algorithm studied herein. There are two deviations from the standard BN
algorithm, one of them actually being a generalization. We use the full gradient instead of the
more popular stochastic gradient (SG) method. It essentially means that each batch contains the
entire training set instead of a randomly chosen subset of the training set. An analysis of SG is
potential future research. Although the primary motivation for full gradient update is to reduce the
burdensome in showing the convergence, the full gradient method is similar to SG in the sense that
both of them go through the entire training set, while full gradient goes through it deterministically
and the SG goes through it in expectation. Therefore, it is reasonable to speculate that the SG method
has similar convergence property as the full algorithm studied herein.
Algorithm 1 DBN: Diminishing Batch-Normalized Network Update Algorithm
1:	Initialize θ ∈ Rni and λ ∈ Rn2
2:	for iteration m=1,2,... do
3:	θ(m+1) ：= θ(m) - n(m)P=1 Vfi(Xi: θ(m), λ(m))
4:	for j=1,...,D1 do
5:	μjm+1) ：= N PNIzjI)(Xi : θ(m+1))________________________
6:	σjm+1) :={N PL (zj12(Xi : θ(m+1)) - μ(m+1))2
7:	λ(m+1) := a(m+1) ((μjm+1))j=1, (σjm+1))j=1) +(1 — α(m+1))λ(m)
The second difference is that we update the BN parameters (θ, λ) by their moving averages with
respect to diminishing α(m). The original BN algorithm can be recovered by setting α(m) = 1
for every m. After introducing diminishing α(m), λ(m) and hence the output of the BN layer is
determined by the history of all past data records, instead of those solely in the last batch. Thus,
the output of the BN layer becomes more general that better reflects the distribution of the entire
dataset. We use two strategies to decide the values of α(m). One is to use a constant smaller than 1
for all m, and the other one is to decay the α(m) gradually, such as α(m) = 1/m.
In our numerical experiment, we show that Algorithm 1 outperforms the original BN algorithm,
where both are based on SG and non-linear activation functions with many layers FNN and CNN
models.
4
Under review as a conference paper at ICLR 2019
(7)
(8)
(9)
4	General Case
The main purpose of our work is to show that Algorithm 1 converges. In the general case, we focus
on the nonconvex objective function.
4.1	Assumptions
Here are the assumptions we used for the convergence analysis.
Assumption 1 (Lipschitz continuity on θ and λ). For every i we have
.. ʌ ... — .. ~ ʌ.. ~ ʌ
kVfi(X : θ,λ) - Vfi(X : θ, λ)k2 ≤ Lkθ - θk2, ∀θ,θ, λ, X.
..	~ ʌ ...
kVwι,j,∙fi(X : θ,λ) -Vwι,j,∙fi(X : θ,λ)k2
—..~ ʌ	..	~ ʌ - -
≤ LkWι,j,∙ - Wι,j,∙ k2,∀λ, θ,θ,X,j ∈{1,…，Dι}.
.. ~. ʌ . .. — .. ~ ʌ ..
kVfi(X : θ,λ) - Vfi(X : θ, ^)k2 ≤ Lkλ - ^k2,
∀θ, ʌ, ʌ, X,j ∈ {1,…，Dl}.
Noted that the Lipschitz constants associated with each of the above inequalities are not necessarily
the same. Here L is an upper bound for these LiPschitz constants for simplicity.
Assumption 2 (bounded parameters). Sets P and Q are compact set, where θ ∈ P and λ ∈ Q.
Thus, there exists a constant M that weights W and parameters λ are bounded element-wise by this
constant M.
kW1k	M and kW2k	M and kλk M.
This also implies that the updated θ, λ in Algorithm 1 remain in P and Q, respectively.
Assumption 3 (diminishing update on θ). The stepsizes of θ update satisfy
∞∞
X η(m)
= ∞ and X(η(m))2 < ∞.	(10)
m=1	m=1
This is a common assumption for diminishing stepsizes in optimization problems.
Assumption 4 (Lipschitz continuity of li(∙)). Assume the lossfunctions li(∙) for every i is continu-
ously differentiable. It implies that there exists M such that
kli(X)- li(y)k ≤ Mkx - yk,∀x,y.
Assumption 5 (existence ofa stationary point). There exists a stationary point (θ*,λ*) such that
kV∕(θ*,λ*)k=0.
We note that all these are standard assumptions in convergence proofs. We also stress that Assump-
tion 4 does not directly imply 1. Since we assume that P and Q are compact, then Assumptions 1,
4 and 5 hold for many standard loss function such as softmax and MSE.
Assumption 6 (Lipschitz at activation function). The activationfunction a(∙) is Lipschitz with con-
stant k:
|a(x)| ≤ kkxk	(11)
Since for all activation function there is a(0) = 0, the condition is equivalent to |a(x) - a(0)| ≤
kkx - 0k. We note that this assumption works for many popular choices of activation functions,
such as ReLU and LeakyReLu.
5
Under review as a conference paper at ICLR 2019
4.2 Convergence Analysis
We first have the following lemma specifying sufficient conditions for λ to converge. Proofs for
main steps are given in the Appendix.
Theorem 7 Under Assumptions 1, 2, 3 and 6, if {α(m) } satisfies
∞	∞m
Xα(m)<∞and XXα(m)η(n) <∞,
m=1	m=1 n=1
then sequence {λ(m)} converges to λ.
We give a discussion of the above conditions for α(m) and η(m) at the end of this section. With the
help of Theorem 7, we can show the following convergence result.
Lemma 8 Under Assumptions 4, 5 and the assumptions of Theorem 7, when
∞∞i	∞∞
X X X α(i)η(n) < ∞ and X X α(n) < ∞,	(12)
m=1 i=m n=1	m=1 n=m
we have
M
limsup X η(m)kVfr(θ(m),λ)k2 < ∞.	(13)
M→∞ m=1
This result is similar to the classical convergence rate analysis for the non-convex objective function
with diminishing stepsizes, which can be found in Bottou et al. (2016).
Lemma 9 Under the assumptions of Lemma 8, we have
liminfkV/(e(m)0)k2 = 0.	(14)
m→∞
This theorem states that for the full gradient method with diminishing stepsizes the gradient norms
cannot stay bounded away from zero. The following result characterizes more precisely the conver-
gence property of Algorithm 1.
Lemma 10 Under the assumptions stated in Lemma 8, we have
lim ||V/(e(m),λ)k2 =0.
m→∞
Our main result is listed next.
Theorem 11 Under the assumptions stated in Lemma 8, we have
lim ∣∣Vf7(θ(m),λ(m))k2 =0.
m→∞
(15)
(16)
We cannot show that {θ(m)}'s converges (standard convergence proofs are also unable to show such
a stronger statement). For this reason, Theorem 11 does not immediately follow from Lemma 10
together with Theorem 7. The statement of Theorem 11 would easily follow from Lemma 10 if the
convergence of {θ(m)} is established and the gradient being continuous.
Considering the cases η(m) = O(焉)and α(m) = O(煮).We show in the Appendix that the set
of sufficient and necessary conditions to satisfy the assumptions of Theorem 7 are h > 1 and k ≥ 1.
The set of sufficient and necessary conditions to satisfy the assumptions of Lemma 8 are h > 2
and k ≥ 1. For example, We can pick η(m) = O(*) and α(m) = O(m⅛πτ) to achieve the above
convergence result in Theorem 11.
6
Under review as a conference paper at ICLR 2019
5 Computational Experiments
We conduct the computational experiments with Theano and Lasagne on a Linux server with a
Nvidia Titan-X GPU. We use MNIST LeCun et al. (1998), CIFAR-10 Krizhevsky & Hinton (2009)
and Network Intrusion (NI) kdd (1999) datasets to compare the performance between DBN and the
original BN algorithm. For the MNIST dataset, we use a four-layer fully connected FNN (784 ×
300 × 300 × 10) with the ReLU activation function and for the NI dataset, we use a four-layer fully
connected FNN (784 × 50 × 50 × 10) with the ReLU activation function. For the CIFAR-10 dataset,
we use a reasonably complex CNN network that has a structure of (Conv-Conv-MaxPool-Dropout-
Conv-Conv-MaxPool-Dropout-FC-Dropout-FC), where all four convolution layers and the first fully
connected layers are batch normalized. We use the softmax loss function and l2 regularization with
for all three models. All the trainable parameters are randomly initialized before training. For all 3
datasets, we use the standard epoch/minibatch setting with the minibatch size of 100, i.e., we do not
compute the full gradient and the statistics are over the minibatch. We use AdaGrad Duchi, John and
Hazan, Elad and Singer (2011) to update the learning rates η(m) for trainable parameters, starting
from 0.01.
We use two different strategies to decide the values of α(m) in DBN: constant values of α(m) and
diminishing α(m) where α(m) = 1/m and α(m) = 1/m2. We test the choices of constant α(m) ∈
{1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001, 0}.
Figure 2: Comparison of predicted accuracy on test datasets for different choices of α(m). From left to right
are FNN on MNIST, FNNonNIandCNNon CIFAR-10.
(a)	(b)	(c)
Figure 3: Comparison of predicted accuracy on test datasets for the most efficient choices of α(m). From left
to right are FNN on MNIST, FNNonNIandCNNon CIFAR-10.
7
Under review as a conference paper at ICLR 2019
Figure 4: Comparison of the convergence of the loss function value on the validation set for different choices
ofα(m). From left to right are FNN on MNIST, FNNonNIandCNNon CIFAR-10.
We test all the choices of α(m) with the performances presented in Figure 2. Figure 2 shows that all
the non-zero choices of α(m) converge properly. The algorithms converge without much difference
even when α(m) in DBN is very small, e.g., 1/m2. However, if we select α(m) = 0, the algorithm
is erratic. Besides, we observe that all the non-zero choices of α(m) converge at a similar rate. The
fact that DBN keeps the batch normalization layer stable with a very small α(m) suggests that the
BN parameters do not have to be depended on the latest minibatch, i.e., the original BN.
We compare a selected set of the most efficient choices of α(m) in Figures 3 and 4. They show that
DBN with α(m) < 1 is more stable than the original BN algorithm. The variances with respect to
epochs of the DBN algorithm are smaller than those of the original BN algorithms in each figure.
Table 1: Best results for different choices of α(m) on each dataset, showing the top three with a heat map.
Model	Test Error		
	MNIST	NI	CIFAR-10
α(m) = 1	2.70%	7.69%	17.31%
α(m) = 0.75	1.91%	7.37%	17.03%
α(m) = 0.5	1.84%	7.46%	17.11%
α(m) = 0.25	1.91%	7.24%	17.00%
α(m) = 0.1	1.90%	7.36%	17.10%
α(m) = 0.01	1.94%	7.47%	16.82%
α(m) = 0.001	1.95%	7.43%	16.28%
α(m) = 1/m	2.10%	7.45%	17.26%
α(m) = 1/m2	2.00%	7.59%	17.23%
α(m) = 0	24.27%	26.09%	79.34%
Table 1 shows the best result obtained from each choice of α(m). Most importantly, it suggests that
the choices of α(m) = 1/m and 1/m2 perform better than the original BN algorithm. Besides,
all the constant less-than-one choices of α(m) perform better than the original BN, showing the
importance of considering the mini-batch history for the update of the BN parameters. The BN
algorithm in each figure converges to similar error rates on test datasets with different choices of
α(m) except for the α(m) = 0 case. Among all the models we tested, α(m) = 0.25 is the only one
that performs top 3 for all three datasets, thus the most robust choice.
To summarize, our numerical experiments show that the DBN algorithm outperforms the original
BN algorithm on the MNIST, NI and CIFAT-10 datasets with typical deep FNN and CNN models.
Future Directions. On the analytical side, we believe an extension to more than 2 layers is
doable with significant augmentations of the notation. A stochastic gradient version is likely to
be much more challenging to analyze. A second open question concerns more general activation
functions. It would be interesting to analyze other activation functions, such as Sigmoid, that do not
apply to our current assumptions.
8
Under review as a conference paper at ICLR 2019
References
KDD Cup 1999 Data, 1999. URL http://www.kdd.org/kdd-cup/view/
kdd-cup-1999/Data.
Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, and Venu Govindaraju. Normalization Propa-
gation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. In
International Conference on Machine Learning, volume 48, pp. 11, 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv preprint
arXiv:1607.06450, 2016.
Dimitri P. Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimiza-
tion: A Survey. OPtimizationfor Machine Learning, 2010(3):1-38, 2011.
Dimitri P. Bertsekas and John N. Tsitsiklis. Gradient Convergence in Gradient Methods with Errors.
SIAM Journal on OPtimization, 10:627-642, 2000.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization Methods for Large-Scale Machine
Learning. arXiv PrePrint arXiv:1606.04838, 2016.
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron Courville. Recurrent Batch Normaliza-
tion. arXiv PrePrint arXiv:1603.09025, 2016.
Yoram Duchi, John and Hazan, Elad and Singer. Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
L. Grippo. A Class of Unconstrained Minimization Methods for Neural Network Training. OPti-
mization Methods and Software, 4(2):135-150, 1994.
CagIar GUICehre and Yoshua Bengio. Knowledge Matters: Importance of Prior Information for
Optimization. Journal of Machine Learning Research, 17(8):1-32, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In ComPuter Vision and Pattern Recognition, pp. 770-778, dec 2016.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training
by Reducing Internal Covariate Shift. In International Conference on Machine Learning, pp.
448-456, 2015.
Alex Krizhevsky and Geoffrey E. Hinton. Learning MultiPle Layers of Features from Tiny Images.
PhD thesis, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet Classification with Deep Convo-
lutional Neural Networks. In Advances in neural information Processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, and Yoshua Bengio. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Genevieve B. Orr and Klaus-Robert Muller. Neural Networks: Tricks of the Trade. Springer, New
York, 2003.
B. T. Polyak. Introduction to optimization. Translations series in mathematics and engineering.
OPtimization Software, 1987.
B. T. Polyak and Y. Z. Tsypkin. Pseudogradient Adaption and Training Algorithms. Automation
and Remote Control, 34:45-67, 1973.
Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to
Accelerate Training of Deep Neural Networks. In Advances in Neural Information Processing
Systems, pp. 901-901, 2016.
9
Under review as a conference paper at ICLR 2019
6 Appendix: Proofs
6.1	Preliminary Results
The following proofs are shortened to corporate with AAAI submission page limit.
Proposition 12 There exists a constant M such that, for any θ and fixed λ, we have
Wf(θ,λ)k2 ≤ M.	(17)
Proof. By Assumption 5, we know there exists (θ*,λ*) SuChthatk V∕(θ*,λ*)∣∣2 = °∙ Then we have
Wf(θ,λ)∣∣2
= ∣Nf(θ,λ)∣∣2 - ∣N∕(θ,λ*)∣∣2 + ∣N∕(θ,λ*)∣∣2 -∣Nf(θ*,λ*)∣∣2
≤∣Nf(θ,λ) - vfr(θ,λ* )k2 + kvf7(θ,λ*) - vf7(θ*,λ*)k2
N
≤ XkVfi(Xi : θ,λ) - Vfi(Xi : θ,λ*)∣∣2	(18)
i=1
N
+ XkVfi(Xi: θ,λ*) -Vfi(Xi: θ*,λ*)k2
i=1
≤NL(kλ - λ*∣∣2 + kθ - θ*∣∣2),
where the last inequality is by Assumption 1. We then have
kvf(θ, λ)k2 ≤ N2L2(kλ - λ*∣∣2 + kθ - θ*k2)2 ≤ M,	(19)
beCause sets P and Q are CompaCt by Assumption 2.
Proposition 13 We have
1
fi(X : θ,λ) ≤ fi(X : θ,λ) + Vfi(X : θ,λ)T(θ - θ) + -L∣∣θ - θ∣∣2,∀θ,θ,X.	(20)
Proof. This is a known result of the LipsChitz-Continuous Condition that Can be found in Bottou et al. (2016).
We have this result together with Assumption 1.
6.2	Proof of Theorem 7
Lemma 14 When Pm∞=1 α(m) < ∞ and Pm∞=1 Pnm=1 α(m)η(n) < ∞,
μ Jm)
(m)	j
μ ∙	:= 7----7rτττ--⅛ττ~~；--------Is a Cauchy series.
叱 (1 - a(I))(I - a⑵)...(--α(m))	J
Proof. By Algorithm 1, we have
(21)
α(m)
We define α(m) := (I- α(% (I- α(2))…(I- α(m) ) and AWIm := WIm)- WIm-I) . After dividing
equation 21 by (1 - α(1))(1 - α(2))...(1 - α(m)), we obtain
1N
Mm)= α(m) N X a(w1mXi) + μ(m-1).	(22)
i=1
Then we have
Nm
同m) -μmτ)∣ ≤ a(m)|k|N XX∆W歌Xil
i=1 n=1
Nm	N
α(m)∣k∣NXX η(n)XVwij,• fι(Xι : θ(n),λ(n)) ∙ Xi
i=1 n=1	l=1
α(m)∣k∣ N X X (η(n)∣(X VWIjfl (Xi： θ(n),λ(n))! ∙ Xi
i=1 n=1	l=1
10
Under review as a conference paper at ICLR 2019
Nm	N
≤ α(m)|k|NXX η(n)kXvwι,j,∙fι(Xι ： θ(n),λ(n))k ∙∣g	(23)
i=1 n=1	l=1
Nm
≤ α(m)∣k∣ X X俨(L ∙ (kw(j,∙ - W];k2 + kλ(n) - λ*,∙k2) ∙∣∣Xik2)	(24)
i=1 n=1
mN
≤ α(m) X (η(n)) |k| X (2LMkXik2)	(25)
n=1	i=1
m
≤ α(m) X 俨 MLM.	(26)
n=1
Equation equation 6.2 is due to W1(,mi,)j = Pnm=1 ∆W1(,ni,)j .
Therefore,
qm
∣μjp) - μjq)l ≤ MLM ∙ X X α(m)η(n).	(27)
m=p n=1
It remains to show that
∞
X α(m) < ∞,	(28)
m=1
∞m
XX α(m)η(n) < ∞,	(29)
m=1 n=1
implies the convergence of {μ(m)}. By (28), We have Π∞=1(1 一 α(m)) > 0, since ln(Π∞=1(1 一 α(m))) >
Pm∞=1 -α(m) > -∞.
It is also easy to shoW that there exists C and Mc such that for all m ≥ Mc, We have
(1 - α(1))(1 - α(2)) ... (1 - α(m)) ≥ C.	(30)
Therefore, lim (1 - α(1))(1 - α(2)) . . . (1 - α(m)) ≥ C.
m→∞
Thus the folloWing holds:
α(m) ≤ -1 α(m)	(31)
C
and
qm	qm
XXa(m)η(n) ≤ C1 XXOmQ	(32)
m=p n=1	m=p n=1
From equation 29 and equation 32 it follows that the sequence {μjm)} is a Cauchy series.	□
Lemma 15 Since {μjm)} is a Cauchy series, {μ(m)} is a Cauchy series.
Proof. Weknow thatμjm) = μ(m)(1 — α(1))...(1 — α(m)). Since lim μjm) → μj and lim (1 — α(1))...(1 —
m→∞	m→∞
α(m)) → C, we have lim μjm) → μj ∙ C. Thus μjm) is a Cauchy series.
m→∞
Lemma 16 If Pm∞=1 α(m) < ∞ and Pm∞=1 Pnm=1 α(m)η(n) < ∞, {σj(m) } is a Cauchy series.
Proof. We define σ(m) := σjm)(1 — α(1))...(1 — α(m)). Then we have
1 N	2
∣σjm+1) - σjm)∣ = α(m) t N X (。”樱Xi)-"
i=1
11
Under review as a conference paper at ICLR 2019
Mm) |k|
α	√N t
N
X
i=1
—
2
(33)
k
Since {μjm)} is convergent, there exists ci, c2 and Ni SUCh that for any m > Ni, 一∞ < ci < μjm) < c2 <
∞.FOranyC∈ {ci-,c},we have
∣σjm+1) - σjm)∣ ≤
α(m) M- ∙,
α √N ∖
N
X
i=1
-。丫
(34)
≤ α(m)
|k|
√N ∖
N
X|
i=1
| + ∣c∣!
(35)
≤ α(m)
|k|
√N ∖
Nm	2
X X η(n) (2NLMkXik2)+ |C|
i=1	n=1
(36)
Inequality equation 35 is by the following fact:
α(m)∣k∣ ∙
α(m) -IkL ∙ a
√N	∖
N ∙
m
X 俨 + |C|	.
n=1
m2
,m X η(n) + |C|
n=i
(37)
(38)
nu
X(ai - c)2 ≤ ut
n
X(|ai| + |c|)2,
i=1
(39)
where b and ai for every i are arbitrary real scalars. Besides, equation 39 is due to -2aic ≤
max{-2|ai|c, 2|ai|c}.
Inequality equation 36 follow from the square function being increasing for nonnegative numbers. Besides
these facts, equation 36 is also by the same techniques we used in equation 23-equation 25 where we bound the
derivatives with the Lipschitz continuity in the following inequality:
N
kX Vwij,"ι(Xι : θ(n),λ(n))k ≤ 2NLM.
l=1
(40)
τ	1 ■ .	，• cr∙ι	11 ，• ，ii	11，	•，	■ ι ι	ι n'ʃ	r∏ι i`
Inequality equation 37 is by collecting the bounded terms into a single bound Ml,m. Therefore,
q-1
同q) - σjp)∣ ≤ X a(m)|k| ∙
m=p
m
,m X 俨 + M .
n=1
(41)
Using the similar methods in deriving equation 28 and equation 29, it can be seen that a set of sufficient
conditions ensuring the convergence for {σjm)} is: Pm=I α(m) < ∞ and P∞=ι Pmi=I α(m)η(n) < ∞.
Therefore, the convergence conditions for {σjm)} are the same as for {μjm)}.
It is clear that these lemmas establish the proof of Theorem 7.
6.3 Consequences of Theorem 7
Proposition 17 Under the assumptions of Theorem 7, we have ∣λ(m) 一 λ∣∞ ≤ am,, where
∞i	∞
am =MiXXα(i)η(j)+M2Xα(i).
i=m j=i	i=m
(42)
Mi and M2 are constants.
≤
∖
k
k
12
Under review as a conference paper at ICLR 2019
Proof. For the upper bound of σjm), by equation 38, we have
q—1	/	m	∖
同q) - σjP) I ≤ X &(m) |k| 必l,m X 产 + |C| .
m=p	∖	n = 1	/
Wedefine j= (1-"..71-α(u))... .Therefore,
∞	i	i	∖
应一σj7≤ Xa(i)| k| MLMX产 + |C|
i=m	∖	j = 1	)
≤J∣ X a(i) (MLM X / + 川.
i=m	∖	j=1	)
(43)
The first inequality comes by substituting P by m and by taking lim as q → ∞ in equation 41. The second
inequality comes from equation 30. We then obtain,
…j≤Wm'- σ片 I+ (1- 0(1))σj(1- α(m))
≤
σ(m) - σ∞) I + —^~σ—L------------------^-σj—L—
j j I + (1 - α(I))…(1 - α(m))	(1 - α(I))…(1 - α(U))…
~(m)	~(∞) I	-	(1 - a(m+1))…(I - α(U))…-1
σ -%	|+ σ (1-α(1))...(1-α(u))...
σjm - σj∞)∣ + C |1 - (1 - a(m+1))…(1 - α(U))…|
(44)
≤
σ(m)
∞
-σ片 I + * X W)
n = m + 1
The second inequality is by (1 — 0(I))…(1 — a(m)) < 1, the third inequality is by equation 30 and the last
inequality can be easily seen by induction. By equation 44, we obtain
_	∞
|%j - σ浮 | = Iim 同M)- %m)| ≤ |%j - %m)| + 篝 X a(n).
J	M→∞J	J	J	C /J
n=m+1
Therefore, we have
，、	zf.	∞	..
E-6m)*|%- σ? + 胃 X On)
n = m + 1
∞	/	i	∖	_	∞
≤ X a(i)|k|- (MLM X/ + |C|	+ 卷 X。⑶
i=m	∖	j=1	i	i=m+1
∞ ι	/	i	∖	_	∞
≤ X Ca(i)|k|- M,m X 产 + |C| + 号 X a(i)
i=m	∖	j=1	J	i=m+1
(45)
(46)
--- .-.
< MLM|k|
≤	C-
∞ i
XX。⑶产
i=m j = 1
+
华)X α(i)
/ i=m
The first inequality is by equation 45, the second inequality is by equation 41, the third inequality is by equa-
tion 31 and the fourth inequality is by adding the nonnegative term σj- a(m) to the right-hand side.
C
For the upper bound of μjm), we have
i〃『-防 i ≤ i产-μ∞) i + i (1- α(1))μ(1- α(m)) - μ(T .	(47)
Let us define Am := I 小m) 一小⑹1and Bm :=  -------小、"j-------rʒ- — μ(∞) . Recall from Theorem 7
I	I	(1 — α(I))...(1 — a(m))
that {μjm)} is a Cauchy series, by equation 27,同P) — μjq) | ≤ MLM ' Pm=P Pm=I o(m)n(n). Therefore,
the first term in equation 47 is bounded by
∞ i
Mm)- j ≤ MLM ∙ X X a(i)n(n) < ∞.	(48)
i=m n = 1
13
Under review as a conference paper at ICLR 2019
For the second term in equation 47, recall that C := (1 - α(1))...(1 - α(u))  Then we have
∞
___________μ_______________λ(∞) ≤ 肩 X ɑ(i)
(1 - α(i))...(1 - α(m))____μ ≤ μ iJ+ Ia
C ∙
where the inequality can be easily seen by in-
duction. Therefore, the second term in equation 47 is bounded by
∞
≤ C X a(i).	(49)
i=m+1
∞
________μj____________ιl(∞)
(1 - a(1))…(1 - a(m))_μ
From these we obtain
∞i
∣μ(m) - μj∣≤ MLM X X a⑺俨 + C X a(i).	(50)
i=m n=1	i=m+1
The first inequality is by equation 47 and the second inequality is by equation 48 and equation 49. Combining
equation 46 and equation 50, we have that
∞i	∞
∣λ(m) - λ∣∞ ≤ Mi XX a⑶ ηj + M2 X a(i),
i=m j=1	i=m
where M1 and M2 are constants defined as M1
max( σj+CkllCl ,C).
MLL,M |k|
max(————,Ml,m) and M2
C,
Proposition 18 Under the assumptions of Theorem 7,
-V∕(θ(m), λ)τ ∙ ^f(θ(m), λ(m)) ≤ -kv,f(θ(m) ,λ)k2 + LM √n2am,
where am is defined in Proposition 17.
Proof. For simplicity of the proof, let us define x(m) := VjF(θ(m),λ),	y(m) := VjT(θ(m),λ(m)). Wehave
|x(m) - y(m)∣∞ ≤ L√n2kλ(m) - λk∞ ≤ L√n2am,	(51)
where √n2 is the dimension of λ. The second inequality is by Assumption 1 and the fourth inequality is by
Proposition 17. Inequality equation 51 implies that for all m and i, We have Ixim) — y(m) | ≤ L√n2am.
It remains to show
-X y(m)x(m) ≤-X xim)2 + LM √n2am, ∀i,m.	(52)
ii
This is established by the following four cases.
1)If xim) ≥ 0,x(m) - y(m) ≥ 0, then x(m) ≤ L√n2am + y(m). Thus -x(m)y(m) ≤ -xim)2 + LM√n2am
by Proposition 12.
2) If xim) ≥ 0,xim) - y(m) ≤ 0, then xim) ≤ y(m), x(m)2 ≤ xim) ∙ y(m) and -X(T)yimm ≤ -x(m)2.
3)If xim) < 0,xim) - y(m) ≥ 0, then xim) ≥ y(m), x(m)2 ≤ xim) ∙ y(m) and -xim)y(m) ≤ -x(m)2.
4) If x(m) < 0, xim) - y(m) ≤ 0, then y(m) - xim) ≤ J^√n2am, y(m)xim) - x(m)2 ≥ L^2amx(m) and
-y(m)xim) ≤ -xim)2 — L√n2amx(m) ≤ -xim)2 + LM√n2am. The last inequality is by Proposition 12.
All these four cases yield equation 52.
Proposition 19 Under the assumptions of Theorem 7, we have
f(θ(m+1),λ) ≤f(θ(m,λ) - η(m)kv∕(θ(m),λ)k2
+ η(m)LM√n2am + 2(η(m))2 ∙ nlm,
where M is a constant and am is defined in Proposition 17.
(53)
Proof. By Proposition 13,
1
fi(Xi : θ,λ) ≤ fi(Xi : θ,λ) + Vfi(Xi : θ, λ)T(θ - θ) + 2Lkθ - θ∣∣2.
Therefore, we can sum it over the entire training set from i = 1 to N to obtain
N
f(θ, λ) ≤ f(θ, λ) + Vf(θ, λ)T (θ - θ) + 万 L∣∣θ - θ∣∣2.	(54)
14
Under review as a conference paper at ICLR 2019
In Algorithm 1, we define the update of θ in the following full gradient way:
N
θ(m+1) ：= θ(m) - η(m) ∙X ∙v∕i(Xi: θ(m),λ(m)),
i=1
(55)
which implies
θ(m+1) - θ(m = -η(m) ∙ ^f(θ(m),λ(m)).
(56)
By equation 56 We have θ 一 θ = θ(m+1) 一 θ(m) = -η(m)V,f(θ(m),λ(m)). We now substitute θ := θ(m+1),
θ := θ(m) and λ := λ into equation 54 to obtain
≤f(θg,λ) - η(m)v,f(θ(m),λ)τVf(θ(m), λ(m)) + (η(m))2 ∙ NLLM
≤∕(θ(m), λ) - η(m) kv∕(θ(m) ,λ)k2 + η(m) L M √n2am
+ 1(η(m))2 ∙ NLM.
(57)
The first inequality is by plugging equation 56 into equation 54, the second inequality comes from Proposition
12 and the third inequality comes from Proposition 18.
6.4	Proof of Theorem 11
Here we show Theorem 11 as the consequence of Theorem 7 and Lemmas 8, 9 and 10.
6.4.1	Proof of Lemma 8
Here we show Lemma 8 as the consequence of Lemmas 20, 21 and 22.
Lemma 20 Pm∞=1 Pi∞=m Pin=1 α(i)η(n) < ∞ and Pm∞=1 Pn∞=m α(n) < ∞ is a set of sufficient condition
to ensure
∞
X ⑸-σjm)1 < ∞, ∀j.
m=1
(58)
Proof. By plugging equation 45 and equation 43 into equation 58, we have the following for all j :
∞
∞
∞
X∣σj - σjm)∣≤ £	∣σj - σjm)∣ + ∣ X α(n)
m=1
.-. ---
≤ |k|，MLM
∞∞
m=1
i
α(i)	η(j) +
m=1 i=m	j =1
σj + lk||Cl
C
m+1
∞∞
| X X α(n)
m=1 n=m+1
(59)

C
It is easy to see that the the following conditions are sufficient for right-hand side of equation 59 to be finite:
∞∞i
m=1	i=m	n=1
Therefore, we obtain
α(i)η(n) < ∞ and Pm∞=1 Pn∞=m α(n) < ∞.
∞
X ⑸-σ” < ∞, ∀j.
m=1
Lemma 21 Under Assumption 4,
∞∞i	∞	∞
XXX α(i)η(n) < ∞ and XX α(n) < ∞
m=1 i=m n=1	m=1 n=m
is a set of sufficient conditions to ensure
M
limsup £ ∣f(θ(m),λ(m)) - ,f(θ(m),λ)∣ < ∞.
M→∞ m=1
Proof. By Assumption 4, we have
D
∣∣li(x) - li(y)k ≤ Mkx — y∣∣≤ M £|xi — yi].
(60)
i=1
15
Under review as a conference paper at ICLR 2019
By the definition of fi(∙), we then have
∞
X ∣∕(θ(M,λS)) - f(θ(m),λ)
m = 1
∞ N
≤ X XI Qi(Xi ： θ(m),λ(m)) - ii(Xi ： θ(m),λ))I
m = 1 i=1
∞ D N
≤M XXX
m = 1 j = 1 i=1
α(WIm)Xi) -μm	α(WIm)Xi)-防
----7-：---------------------
σjm + CB	σj + eB
∞ D NN	_ _	(m)
≤M3 XX(X IkIIWS XiI -⅛j-
m = 1 j = 1 ∖i=1	B
-	/Jm)
μj	μj
-------------7--:-----
σj + CB	σ(m) + CB
(61)
(62)
(63)
(64)
+ N
The first inequality is by the Cauchy-Schwarz inequality, and the second one is by equation 60. To show the
finiteness of equation 64, we only need to show the following two statements:
and
∞ N
XX IkIIWj Xi I
m = 1 i=1
μj
σj + Cb
σj- σjm)
cB
〃jm)
σjm) + CB
< ∞,∀j
(65)
(66)
∞
X
m = 1
—
< ∞, ∀j.
Proof of equation 65: For all j we have
XX IkIIWjXiI
m=1 i=1
σj- σ(m)
cB
∞
N
∞	..
≤ ^X IkINDMmax∣∣Xi∣∣ —鬲-σ(m)
m=1	i	CB
(67)
= IkINDM max∣∣Xi∣∣
i
m=1
(m)
σ'j
The inequality comes from ∣W(m)Xi∣ ≤ DMIlXilI2, where D is the dimension of Xi and M is the element-
wise upper bound for W(m) in Assumption 2.
Finally, we invoke Lemma 14 to assert that P∞=11 σ7∙ — σjm) ∣ is finite.
Proof of equation 66: For all j we have
∞
X
m = 1
∞
≤X
m = 1
μ
σj + Cb
μ
σj + Cb
〃jm)
σjm) + CB
〃jm)
σj + Cb
∞
+X
m=1
〃jm)
σj + CB
Am)
σ(m) + CB
(68)
The first term in equation 68 is finite since {μjm)} is a Cauchy series. For the second term, we know that there
exists a constant M such that for all m ≥ M, μjm) ≤ μ + 1. This is also by the fact that {μjm)} is a Cauchy
series and it converges to μ. Therefore, the second term in equation 68 becomes
M-1
X
m=1
M-1
≤X
m=1
σj + Cb
〃jm)
σj + Cb
(m)
μj
σ(m) + CB
(m)
μj
(m)
σj	+ CB
∞
+X
m = M
“jm)
σj + Cb
j
σjm) + CB
∞
+ X (μ +1)
m=M
σj + Cb
1
(m)
σj	+ CB
(69)
〃浮
—
—
—
—
—
—
1
—
Noted that function f (σ)
1
is Lipschitz continuous since its gradient is bounded by ɪ. Therefore
CB
σ + CB
we can choose ɪ as the Lipschitz constant for f (σ). We then have the following inequality:
CB
16
Under review as a conference paper at ICLR 2019
1	1
---------------7-:------
σj+ 3	σjm) + CB
≤ 2r∣σj - σjm)l∙
eB
(70)
Plugging equation 70 into equation 69, we obtain
M-1	(m)	(m)
X _^j______μj
m=ισ+eB . wm)+£b
M-1	(M)	(M)
≤ X二-------上—
m=ι σ + eB	σjm) + eB
∞
+ X (a + 1)
m = M
11
--
σj + eB	σ(m + Cb
+ X中员-。浮|,
m=M	B
(71)
where the first term is finite by the fact that M is a finite constant. We have shown the condition for the second
term to be finite in Lemma 20. Therefore,
∞
X
m = 1
-	/Jm)
μj	μj
--
σj + eB	σjm) + Cb
< ∞, ∀j.
By equation 65 and equation 66, we have that the right-hand side of equation 64 is finite. It means that the
left-hand side of equation 64 is finite. Thus,
∞
X b"x(m))- /"a I < ∞.	□
m = 1
Lemma 22 If
∞	∞ i	∞	∞
XXX
a(i)n(n) < ∞ and XX
a(n) < ∞,
m = 1 i=m n = 1	m=1 n = m
then
M
lim sup ^X n(m)g/(e(m)力||2 < ∞.
M →∞ m = 1
Proof. For simplicity of the proof, we define
M
T(M) ：= X n(m)wf0m)*||2,
m = 1
O(m) := /(θ(m+1),λ(m+1)) - f (θ(m,λ(m),
∆1m+1) ：= /(θ(m+1),λ(m+1)) - /(θ(m+1),λ ),
△2m) ：= /(θ(m+1),λ)- /(θ(m),λ),
where λ is the converged value of λ in Theorem 7. Therefore,
O(m) = ∆1m+1) + ∆1m) + ∆2m) ≤ ∣∆1m+1)∣ + ∣∆1m)∣ + ∆2m.	(72)
By Proposition 19,
△2m) ≤ -n(m)|N/0m),λ)k2 + n(m)LMgm + |(n(m))2 ∙ nlm.	(73)
We sum the inequality equation 72 from 1 to K with respect to m and plug equation 73 into it to obtain
K	K	K	K
X O(m) ≤ X ∣∆1m+1)∣ + X ∣∆1m)∣- X{n(m)w/0m),刈2}
m=1	m=1	m=1	m=1
+ X n(m)LM√n2αm + X {2 Wm))2NLM}
m=1	m=1
KK
=X ∣∆1m+1)∣ + X ∣∆1m)∣- T(K)
m=1	m=1
KK
+ L2 √n ∙ X 产am + X {IWm))2NLM}.
m=1	m=1
(74)
17
Under review as a conference paper at ICLR 2019
From this, we have:
limsup T(K) ≤ limsup -1(f(θ(K) ,λ(K)) - f7(θ(1), λ(1)))
K→∞	K→∞ c1
1K
+ limsup - X (∣∆1m+1)∣ + ∣∆1m)|)
K→∞ c1
m=1
K
+ limsup L2√n2 X η(m)am
K→∞
m=1
Ilj NLK V÷ (m)2
+ limsup----) η .
K→∞	2c1
m=1
(75)
Next we show that each of the four terms in the right-hand side of equation 75 is finite, respectively. For the
first term,
limsup -1(∕(θ(K),λ(K)) - f(θ(1),λ(1))) < ∞	(76)
K→∞ c1
is by the fact that the parameters {θ, λ} are in compact sets, which implies that the image of fi(∙) is in a
bounded set.
For the second term, we showed its finiteness in Lemma 21.
For the third term, by equation 42, we have
K
lim sup	η(m)am
K→∞
m=1
K	∞i	∞
lim sup X η(m) K1 XX
α(i)η(j) +K2 X α(i)
K→∞ m=1	i=m j=1	i=m
(77)
K	∞i	K	∞
=K1 lim sup X η(m) X X α(i)η(j) +K2limsupXη(m) Xα(i).
K→∞	K→∞
m=1	i=m j=1	m=1	i=m
The right-hand side of equation 77 is finite because
∞	∞i	∞	∞i
X η(m)	X X α(i)η(j)	<X X X α(i)η(j)	<∞
m=1	i=m j=1	m=1	i=m j=1
(78)
and
∞	∞	∞∞
X η(m) X α(i) < XX α(i) < ∞.
m=1	i=m	m=1 i=m
(79)
The second inequalities in equation 78 and equation 79 come from the stated assumptions of this lemma.
For the fourth term,
lim sup
K→∞
NLM
2c
K
X η(m)2
m=1
<∞
holds, because we have Pm∞=1 (η(m))2	<
P∞=ιη(m)IN∕(θ(m),X)k2 < ∞^holds.
(80)
∞ in Assumption 3. Therefore, T(∞)	=
In Lemmas 20, 21 and 22, We show that {σ(m)} and {μ(m)} are Cauchy series, hence Lemma 8 holds.
6.4.2	Proof of Lemma 9
This proof is similar to the the proof by Bertsekas & Tsitsiklis (2000).
Proof. By Theorem 8, we have
M
lim sup X η(m)kv,f(θ(m),λ)k2 < ∞.	(81)
M→∞ m=1
If there exists a e > 0 and an integer m such that
kv∕(θ(m),X)∣∣2 ≥

18
Under review as a conference paper at ICLR 2019
for all m ≥ m, we would have
MM
liminf X η(m) kV,f(θ(m), λ) k2 ≥ liminf e2 X η(m) = ∞	(82)
M→∞	M→∞
m=m	m=m
which contradicts equation 81. Therefore, liminfkV,f(θ(m), λ)k2 = 0.	□
m→∞
6.4.3	Proof of Lemma 10
Lemma 23 Let Yt , W, t and Zt be three sequences such that Wt is nonnegative for all t. Assume that
Yt+1 ≤ Yt - Wt + Zt,	t = 0, 1, ...,	(83)
and that the series PtT=0 Zt converges as T → ∞. Then either Yt → ∞ or else Yt converges to a finite value
and Pt∞=0 Wt < ∞.
This lemma has been proven by Bertsekas & Tsitsiklis (2000).
Lemma 24 When
∞∞i	∞∞
XXX
α(i)η(n) < ∞ and XX
α(n) < ∞,
m=1 i=m n=1	m=1 n=m
it follows that f (θ(m), λ) converge to a finite value.
Proof. By Proposition 19, we have
f(θ(m+1),λ) ≤∕(θ(m),λ) - η(m)kv∕(θ(m),λ)k2
+ η(m)LM√n2am + 2(η(m))2 ∙ nlm.
Let Y(m) := f7(θ(m),λ), W(m) := η(m)kvf(θ(m),λ)k2 andZ(m) := η(m)LM√2am + 1(η(m))2∙NLM.
By equation 10 and equation 77- equation 79, it is easy to see that PmM=0 Z(m) converges as M → ∞.
Therefore, by Lemma 23, Y(m) converges to a finite value. The infinite case can not occur in our setting due
to Assumptions 1 and 2.	□
(84)
(85)
Lemma 25 If
Pm∞=1 Pi∞=m Pin=1 α(i)η(n) <∞ and	Pm∞=1 Pn∞=m α(n) <∞,
then lim kV∕(θ(m),刈∣2 = 0.
m→∞
Proof. To show that lim ∣∣V∕(θ(m), λ)∣∣2 = 0, assume the contrary; that is,
m→∞
limsup∣∣Vf(θ(m), X)k2 > 0.
m→∞
Then there exists an e > 0 such that ∣∣V∕(θ(m), X)k < e/2 for infinitely many m and also ∣∣V∕(θ(m), X)k > e
for infinitely many m. Therefore, there is an infinite subset of integers M, such that for each m ∈ M, there
exists an integer q (m) > m such that
kV∕(θ(m),λ)k <e/2,
kV∕(θ(i(m)),λ)k >e,
(86)
e/2 ≤ kV∕(θ(i),λ)k ≤ e,
if m < i < q(m).
From ∣∣V∕(θ(m+1), λ)k — ∣∣V∕(θ(m), X)k ≤ L η(m)kV∕(θ(m), λ(m))k, it follows that for all m ∈ M that are
sufficiently large so that Lη(m) < e/4, We have
e/4 ≤ kVf(θ(m),λ(m))k.	(87)
Otherwise the condition e/2 ≤ ∣∣V∕(θ(m+1), X)k would be violated. Without loss of generality, We assume
that the above relations as well as equation 57 hold for all m ∈ M. With the above observations, we have for
all m ∈ M,
19
Under review as a conference paper at ICLR 2019
I ≤ w∕(θq(m),刈-w∕(θ(m),刈 ≤ Lkθq(m) -θ(m)k
q(m)-1
≤ L X η(i)(kvfr(θ(i),λ)k + gf(θ(i),λ(i)) -v,f(θ(i),λ)k)
i=m
q(m)-1	q(m)-1	∞ j	(88)
=Le X η(i) + L2√n2Mι X η(i) XX a(j)η(k)
i=m	i=m	j=m k=1
q(m)-1	∞
+ L2√n2M2 X η(i) X α(j)
i=m	j=m
The first inequality is by equation 86 and the third one is by the Lipschitz condition assumption. The seventh
one is by equation 51. By equation 12, we have for all m ∈ M,
q(m)-1	∞ j	∞ ∞ j
X η(i)XXα(j)η(k) <XXXα(j)η(k) <∞	(89)
i=m	j=m k=1	i=1 j=i k=1
and
q(m)-1	∞	∞ ∞
X η(i)Xα(j)<XXα(j)<∞.	(90)
i=m	j=m	i=1 j=i
It is easy to see that for any sequence {αi} with Pi∞=1 αi < ∞, if follows that lim inf Pi∞=M αi
i=1	M→∞	i=M
fore, lim inf Piq=(mm)-1 η(i) Pj∞=m Pjk=1 α(j)η(k) = 0 and lim inf Piq=(mm)-1 η(i) Pj∞=m α(j) =
m→∞	i=m	j=m	k=1	m→∞	i=m	j=m
it follows that
q(m)-1
lim inf X η(i)
m→∞
i=m
= 0. There-
0. From this
(91)
≥ 21L.
By equation 51 and equation 87, if we pick m ∈ M such that L√n2am ≤ ∣, we have ∣∣Vf(θ(m),X)k ≥ ∣.
Using equation 57, we observe that
f7(θq(m),λ)
≤ f(θr*,λ -Ci (I)2qX η(i) + 2 ∙ NLM q(X (η(i))2, ∀m ∈ M,
i=m	i=m
(92)
where the second inequality is by equation 87. By Lemma 24, /(θq(m), λ) and /(θ(m), λ) converge to the
same finite value. Using this convergence result and the assumption Pr∞=0(η(r))2 < ∞, this relation implies
that
lim sup Piq=(rr)-1 η(i) = 0 and contradicts equation 91.
r→∞,r∈M
By Lemmas 23, 24 and 25, we show that Theorem 11 holds.
6.5	Discussions of conditions for stepsizes
Here we discuss the actual conditions for η(r) and α(r)
8. We only consider the cases η(m) =工 and α(m)
η(m) = O(m1k) and α(m) = O(煮).™
to satisfy the assumptions of Theorem 7 and Lemma
= m1h , but the same analysis applies to the cases
6.6	Assumptions of Theorem 7
For the assumptions of Theorem 7, the first condition Pr∞=1 α(r) < ∞ requires h > 1. Besides, the second
condition
∞r	∞	∞
X X ɑ(m)η(n) ≈ h⅛ X 俨 nh-1 = h-1 X nɪr < ∞	约
-	n	-n
r=1 n=1	n=1	n=1
requires k + h > 2. The approximation comes from the fact that for every p >1, we have
∞
k=n
k-p
≈	∞ k-pdx
k=n
∞
1 1-p
-----X
1	1
P — 1 np-i
(94)
1 - p
20
Under review as a conference paper at ICLR 2019
Since k ≥ 1 due to Assumption 3, we conclude that k + h > 2. Therefore, the conditions for η(m) and α(m)
to satisfy the assumptions of Theorem 7 are h > 1 and k ≥ 1.
6.7 Assumptions of Lemma 8
For the assumptions of Theorem 7, the first condition
	∞∞ X X α(n) m=1 n=m	∞1 ≈ X 1 mh mh-1 m=1	<∞	(95)
requires h > 2. Besides, the second condition is ∞∞ i X X X α(i) m=1 i=m n=1	∞∞ η(n) = X X m=1 i=m	i α(i) X η(n) n=1	∞∞ ≤CXXα(i)<∞. m=1 i=m	(96)
The inequality holds because for any p > 1, we have
nn
kX=1k-p ≈ k=1 k-pdk
-ɪ k1-p
1 - P
-ɪ(l - n1-p) ≤ C
p - 11	' 一
(97)
Therefore, the conditions for η(m) and α(m) to satisfy the assumptions of Lemma 8 are h > 2 and k ≥ 1.
21