Under review as a conference paper at ICLR 2019
Dimension-Free Bounds for Low-Precision
Training
Anonymous authors
Paper under double-blind review
Ab stract
Low-precision training is a promising way of decreasing the time and energy cost
of training machine learning models. Previous work has analyzed low-precision
training algorithms, such as low-precision stochastic gradient descent, and derived
theoretical bounds on their convergence rates. These bounds tend to depend on the
dimension of the model d in that the number of bits needed to achieve a particular
error bound increases as d increases. This is undesirable because a motivating
application for low-precision training is large-scale models, such as deep learning,
where d can be huge. In this paper, we prove dimension-independent bounds for
low-precision training algorithms that use fixed-point arithmetic, which lets us
better understand what affects the convergence of these algorithms as parameters
scale. Our methods also generalize naturally to let us prove new convergence
bounds on low-precision training with other quantization schemes, such as low-
precision floating-point computation and logarithmic quantization.
1 Introduction
As machine learning models continue to scale to target larger problems on bigger data, the task
of training these models quickly and efficiently becomes an ever-more-important problem. One
promising technique for doing this is low-precision computation, which replaces the 32-bit or 64-bit
floating point numbers that are usually used in ML computations with smaller numbers, often 8-bit
or 16-bit fixed point numbers. Low-precision computation is a broadly applicable technique that has
received a lot of attention, especially for deep learning, and specialized hardware accelerators have
been developed to support it (Jouppi et al., 2017; Burger, 2017; Caulfield et al., 2017).
A major application for low-precision computation is the training of ML models using empirical
risk minimization. This training is usually done using stochastic gradient descent (SGD), and most
research in low-precision training has focused on low-precision versions of SGD. While most of
this work is empirical (WU et al., 2018; Das et al., 2018; ZhU et al., 2016; Koster et al., 2017;
Lee et al., 2017; Hubara et al., 2016; Rastegari et al., 2016; Zhou et al., 2016; Gupta et al., 2015;
CoUrbariaUx et al., 2014; 2015; De Sa et al., 2017), significant research has also been done in the
theoretical analysis of low-precision training. This theoretical work has sUcceeded in proving boUnds
on the convergence rate of low-precision SGD and related low-precision methods in varioUs settings,
inclUding for convex (De Sa et al., 2018; Zhang et al., 2017) and non-convex objectives (De Sa et al.,
2015; Li et al., 2017; Alistarh et al., 2017). One common characteristic of these resUlts is that the
boUnds tend to depend on the dimension d of the model being learned (eqUivalently, d is the nUmber
of parameters). For example, (Li et al., 2017) gives the convergence boUnd
E [f(wT) - f(w*)] ≤
(1 +log(T + 1))σ21aχ + σmaχδ√d
2μτ	+	2
(1)
where the objective f is strongly convex with parameter μ, low-precision SGD outputs WT after T
iterations, w* is the true global minimizer of the objective, σ2mχ is an upper bound on the second
moment of the stochastic gradient samples E[kf(w)∣∣2] ≤ σ2mχ, and δ is the quantization step, the
difference between adjacent numbers in the low-precision format. Notice that, as T → ∞, this
bound shows convergence down to a level of error that increases with the dimension d. Equivalent-
ly, in order to achieve the same level of error as d increases, we would need to use more bits of
quantization to make δ smaller. Similar dimension-dependent results, where either the error or the
number of bits needed increases with d, can also be seen in other work on low-precision training
algorithms (Alistarh et al., 2017; Zhang et al., 2017; De Sa et al., 2018). This dependence on d is
unsatisfying because the motivation for low-precision training is to tackle large-scale problems on
big data, where d can range up to 108 or more for commonly used models (Simonyan and Zisserman,
1
Under review as a conference paper at ICLR 2019
Table 1: Summary of our dimension-free results compared with prior work. The values report
the number of bits needed, according to the theoretical bound, for the LP-SGD (Li et al., 2017)
algorithm to achieve an expected objective gap (f (W) - f (w*)) of E when We let step size ɑ → 0,
epoch length T → ∞. Here we let R denote the radius of the range of numbers representable in the
low-precision format and assume ∣∣w*∣∣2 = Θ(R). The rest of the parameters can be found in the
assumptions to be introduced later.
NUMBER OF BITS NEEDED FOR E [f (W) - f (w*)] ≤ €
PRIOR DIMENSION-DEPENDENT BOUND	log2 θ(R<Jmax√d∕ε)
OUR DIMENSION-FREE BOUND	log2 θ(Rσι∕ε)
DIMENSION-FREE WITH LOGARITHMIC QUANTIZATION log2 θ((Rσ∕ε) ∙ log(1 + σι/σ))
2014). For example, to compensate for a factor of d = 108 in (1), we could add bits to decrease the
quantization step δ by a factor of √d, but this would require adding log2(i0* 2 3 4) ≈ 13 bits, which is
significant compared to the 8 or 16 bits that are commonly used in low-precision training.
In this paper, we address this problem by proving dimension-free bounds on the convergence of
LP-SGD Li et al. (2017). Our main technique for doing so is a tight dimension-independent bound
on the expected quantization error of the low-precision stochastic gradients in terms of the 'ι-norm.
Our results are summarized in Table 1, and we make the following contributions:
• We describe conditions under which we can prove a dimension-free bound on the conver-
gence of SGD with fixed-point, quantized iterates on strongly convex problems.
• We study non-linear quantization schemes, in which the representable low-precision num-
bers are distributed non-uniformly. We prove dimension-free convergence bounds for SGD
using logarithmic quantization (Lee et al., 2017), and we show that using logarithmic quan-
tization can reduce the number of bits needed for LP-SPG to provably converge.
• We study quantization using low-precision floating-point numbers, and we present theoret-
ical analyis that suggests how to assign a given number of bits to exponent and mantissa to
optimize the accuracy of training algorithms. We validate our results experimentally.
2 Related Work
Motivated by the practical implications of faster machine learning, much work has been done on
low-precision training. This work can be roughly divided into two groups. The first focuses on
training deep models with low-precision weights, to be later used for faster inference. For some
applications, methods of this type have achieved good results with very low-precision models: for
example, binarized (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016) and ternary
networks (Zhu et al., 2016) have been observed tobe effective (although as is usual for deep learning
they lack theoretical convergence results). However, these approaches are still typically trained with
full-precision iterates: the goal is faster inference, not faster training (although faster training is
often achieved as a bonus side-effect).
A second line of work on low-precision training, which is applied to both DNN training and non-
deep-learning tasks, focuses on making various aspects of SGD low-precision, while still trying to
solve the same optimization problem as the full-precision version. The most common way to do this
is to make the iterates of SGD (the Wt in the SGD update step wt+1 = Wt — αRft(wt)) stored
and computed in low-precision arithmetic (Courbariaux et al., 2014; Gupta et al., 2015; De Sa et al.,
2018; 2015; Li et al., 2017). This is the setting we will focus on most in this paper, because it has
substantial theoretical prior work which exhibits the dimension-dependence we set out to study (Li
et al., 2017; Zhang et al., 2017; Alistarh et al., 2017; De Sa et al., 2018). The only paper we
found with a bound that was not dimension-dependent was De Sa et al. (2015), but in that paper
the authors required that the gradient samples be 1-sparse (have only one nonzero entry), which is
not a realistic assumption for most ML training tasks. In addition to quantizing the iterates, other
work has studied quantizing the training set (Zhang et al., 2017) and numbers used to communicate
among parallel workers (Alistarh et al., 2017). We expect that our results on dimension-free bounds
will be complementary with these existing theoretical approaches, and we hope that they can help
to explain the success of the exciting empirical work in this area.
3 Dimension-Free Bounds for SGD
In this section, we analyze the performance of stochastic gradient descent (SGD) using low-precision
training. Though there are numerous variants of this algorithm, SGD remains the de facto algorithm
2
Under review as a conference paper at ICLR 2019
used most for machine learning. We will start by describing SGD and how it can be made low-
precision. Suppose we are trying to solve the problem
1n
minimize: f (W) = — ɪ2 fi(w)	over： W ∈ Rd.	(2)
i=1
SGD solves this problem iteratively by repeatedly running the update step
Wt+1 = Wt - αVfit (Wt)	(3)
where α is the step size1 or learning rate, and it is the index of a component function chosen ran-
domly and uniformly at each iteration from {1, . . . , n}. To make this algorithm low-precision, we
quantize the iterates (the vectors Wt) and store them in a low-precision format. The standard format
to use lets us represent numbers in a set
dom(δ, b) = {-δ ∙ 2b-1,…，-δ, 0,δ, ∙∙∙ ,δ ∙ (2b-1 - 1)}
with δ > 0 being the quantization gap, the distance between adjacent representable numbers, and
b ∈ N being the number of bits we use (De Sa et al., 2018). Usually, δ is a power of 2, and this
scheme is called fixed-point arithmetic. It is straightforward to encode numbers in this set as b-bit
signed integers, by just multiplying or dividing by δ to convert to or from the encoded format—
and we can even do many arithmetic computations on these numbers directly as integers. This
is sometimes called linear quantization because the representable points are distributed uniformly
throughout their range. However, as the gradient samples will produce numbers outside this set dur-
ing iteration, we need some way to map these numbers to the set of numbers that we can represent.
The standard way to do this is with a quantization function Q(x) : R → dom(δ, b). While many
quantization functions have been proposed, the one typically used in theoretical analysis (which we
will continue to use here) is randomized rounding. Randomized rounding, also known as unbiased
rounding or stochastic rounding, rounds up or down at random such that E [Q(x)] = x whenever x
is within the range of representable numbers (i.e. when -δ ∙ 2b-1 ≤ X ≤ δ ∙ (2b-1 — 1)). When X
is outside that range, we quantize it to the closest representable point. When we apply Q(δ,b) to a
vector argument, it quantizes each of its components independently.
Using this quantization function, we can write the update step for low-precision SGD (LP-SGD),
which is a simple quantization of (3),
Wt+1 = Q(Wt- aVfit (Wt))	(4)
As mentioned before, one common feature of prior bounds on the convergence of LP-SGD is that
they depend on the number of dimensions d, whereas bounds on full precision SGD under the same
conditions do not do so. This difference is due to the fact that, when we use a quantization function
Q to quantize a number w, it increases its variance by E [(Q(w) - w)2] ≤ δ2∕4. Observe that this
inequality is tight since it holds as an equality when W is in the middle of two quantization points,
e.g. W = δ∕2, as illustrated in Figure 1(a). When quantizing a vector W ∈ Rd, the squared error can
be increased by	d	2
EhkQ(W)- w∣∣2] = XE [(Q(Wk) - Wk)2] ≤ 丁,	⑸
k=1
and this bound is again tight. This variance inequality is the source of the d term in analyses of
LP-SGD, and the tightness of the bound leads to the natural belief that the d term is inherent, and
that low-precision results are inevitably dimension-dependent.
However, we propose that if we can instead bound the variance in (5) with some properties of
the problem itself that do not change as d changes, we can achieve a result that is dimension-
independent. One way to do this is to look at the variance graphically. Figure 3 plots the quantization
error as a function ofW along with the bound in (5). Notice that the squared error looks like a series
of parabolas, and the bound in (5) is tight at the top of those parabolas, but loose elsewhere. Instead,
suppose we want to do the opposite and produce a bound that is tight when the error is zero (at points
in dom(δ,b)). To do this, we observe that E [(Q(w) — w)2] ≤ δ∣W - z| for any Z ∈ dom(δ,b). This
bound is also tight when z is adjacent to W, and we plot it in Figure 3 as well. The natural vector
analog of this is
E [kQ(w)	-	wk2i	≤ Pd=I	δ∣Wk - Zk |	= δ ∣∣w -	zkι,	∀z	∈ dom(δ, b)d	(6)
1Usually in SGD the step size is decreased over time, but here for simplicity we consider a constant learning
rate schedule.
3
Under review as a conference paper at ICLR 2019
d",6 sso_
0.200-
0.175-
0.150-
0.125-
0.100-
0.075 -
0.050 -
0.025 -
0.000 -
Convergence of Low-Precision SGD
——d = 128,fp
——d = 1024f fp
一 d = 8192f fp
d = 128fb = 8
——d = 1024fb = 8
——d = 8192fb = 8
d = 128,b = 6
d = 1024, b = 6
d = 8192,b = 6
O 200000 400000 600000 800000 1000000
iterations
(b) The convergence of training loss gap f (w) -
f (w*) as we train low-precision SGD, showing the
effect of different model sizes and precision.
Spunog Pu"ωucπ≡π>
Variance and Bounds for Linear Quantization
....actual variance
....δ2∕4 bound
.... our /ɪ-norm bound
-2.0 -1.5 -1.0 -0.5 0.0	0.5	1.0	1.5	2.0
quantized value
(a) The squared quantization error
E (Q(w) - w)2 and two possible tight
upper bounds in the one-dimensional case.
Figure 1: (a) Quantization error bounds; (b) Convergence of full-precision (fp) SGD and LP-SGD.
where ∣∣∙kι denotes the L1 norm. This is a dimension-independent bound We can use to replace (5)
to bound the convergence of LP-SGD and other algorithms. However, this replacement is nontrivial
as our bound is now non-constant: it depends on w, which is a variable updated each iteration. Also,
in order to bound this new L1 norm term, we will need some new assumptions about the problem.
Next, we will state these assumptions, along with the standard assumptions used in the analysis of
SGD for convex objectives, and then we will use them to present our dimension-free bound on the
convergence of SGD.
Assumption 1. All the loss functions f are differentiable, and their gradients are L-Lipschitz Con-
tinuous in the sense of 2-norm, that is,
∀i ∈{1, 2,…，n}, ∀x,y ∈ Rd, kVfi3-Vfi(y)∣∣2 ≤ L ∣x -y∣∣?
Assumption 2. All the gradients of the loss functions fi are L1 -Lipschitz continuous in the sense of
1-norm to 2-norm, that is,
∀i ∈{1, 2,…，n}, ∀x,y ∈ Rd, ∣Vfi(x) -Vfi(y)kι ≤ Li ∣x - y∣2
These two assumptions are simply expressing of Lipschitz continuity in different norms. Assump-
tion 1 is a standard assumption in the analysis of SGD on convex objectives, and has been applied
in the low-precision case as well in prior work (De Sa et al., 2018). Assumption 2 is analogous to 1,
except we are bounding the L1 norm instead of the L2 norm. This holds naturally (with a reasonable
value of L1 ) for many problems, in particular problems for which the gradient samples are sparse.
Assumption 3. The gradient ofthe total loss function f is μ-strongly Convexfor some μ > 0:
∀w, v, f (w) - f (v) - 22 l∣w - v∣2 ≥ (W -V)TVf (v)
Assumption 3 is a standard assumption that bounds the curvature of the loss function f, and is
satisfied for many classes of convex objectives. For example, any convex loss with L2 regularization
will always be strongly convex. When an objective is strongly convex and Lipschitz continuous, it is
standard to say it has condition number K = L∕μ, and here We extend this to say it has L1 condition
number Ki = Li/μ.
Assumption 4. The
gradient of each loss function is bounded by some constant σ near the optimal
point in the sense of li and l2 norm, that is,
E IlVfi(W*)|口 ≤ σ2,	E h∣∣Vfi(w*)∣∣J ≤ σι
This assumption constrains the gradient for each loss function at the optimal point. We know
Vf (w*) = n Ei Vfi(W*) = 0, so it is intuitive that each Vfi(W*) can be bounded by some
value. Therefore this is a natural assumption to make and it has been used in a lot of other work in
this area. Note that this assumption only needs to hold under the expectation over all fi . With these
assumptions, we proved the following theorem for low-precision SGD:
Theorem 1. Suppose that we run LP-SGD on an objective that satisfies Assumptions 1-4, and with
step size α < 1∕(2κ2μ). After T LP-SGD update steps (4), select WT uniformly at random from
{wo, Wi,..., WT-ι}. Then, the expected objective gap of WT is bounded by
E [f(WT) -f(W*)] ≤ 2-1T i∣wo - w*ii2 + ασ + δσ1 + δ-41μ
4
Under review as a conference paper at ICLR 2019
This theorem shows a bound of the expected distance between the result we get at K-th iteration
and the optimal value. By choosing an appropriate step size we can achieve convergence at a 1/T
rate, while the limit we converge to is only dependent on dimension-free factors. Meanwhile, as
mentioned in the first section, previous work gives a dimension-dependent bound (1) for the prob-
lem, which also converges at a 1/T rate.2 Therefore our result guarantees a dimension-independent
convergence limit without weakening the convergence rate.
It is important to note that, because the dimension-dependent bound in (5) was tight, We should not
expect our new result to improve upon the previous theory in all cases. In the worst case, κι = √d∙ K
and similarly σι = √d ∙ σ; this follows from the fact that for vectors in Rd, the norms are related by
the inequality ∣∣χkι ≤ √d ∙ ∣∣χ∣∣2∙ Substituting this into our result produces a dimension-dependent
bound again. This illustrates the importance of introducing the new parameters κ1 and σ1 and
requiring that they be bounded; if we could not express our bound in terms of these parameters, the
best we could do here is recover a dimension-dependent bound.
Experiments Next, we validate our theoretical results experimentally. To do this, we analyzed
how the size of the noise floor of convergence of SGD and LP-SGD varies as the dimension is
changed for a class of synthetic problems. Importantly, we needed to pick a class of problems for
which the parameters L, Li, μ, σ, and σι, did not change as we changed the dimension d. To do this,
we chose a class of synthetic linear regression models with loss components sampled independently
and identically as	]
where X is a sparse vector sampled to have S nonzero entries each of which is sampled uniformly
from {-1,1}, and y is sampled from N(XT w* ,β2) for some variance parameter β. Importantly, the
nonzero entries of X were chosen non-uniformly such that Pr[Xi = 0] = Pi for some probabilities Pi
which decrease as i increases; this lets us ensure that μ remains constant as d is increased. For sim-
plicity, we sampled a fresh loss component of this form at each SGD iteration, which is sometimes
called the online setting. It is straightforward to derive that for this problem
μ = pd	L = s Li = s√s	σ2 = β2s	σι = ʌ/ 2s∕πσ.
We set α = 0.01, β = 0.2, P1 = 0.9, Pd = 0.001, and s = 16, we chose each entry ofw* uniformly
from [-1/2, 1/2], and we set δ such that the low-precision numbers would range from -1 to 1.
Figure 1(b) shows the convergence of SGD and LP-SGD as the dimension d is changed, for both
8-bit and 6-bit quantization. Notice that while changing d has an effect on the initial convergence
rate for both SGD and LP-SGD, it has no effect on the noise ball size, the eventual loss gap that the
algorithm converges to. Figure 2(a) measures this noise ball size more explicitly as the dimension
is changed: it reports the loss gap averaged across the second half of the iterates. Notice that as
the dimension d is changed, the average loss gap is almost unchanged, even for very low-precision
methods for which the precision does significantly affect the size of the noise ball. This validates
our dimension-free bounds, and shows that they can describe the actual dependence on d in at least
one case.
Figure 2(b) validates our results in the opposite way: it looks at how this gap changes as our new
parameters σι and Li change while d, μ, and σ are kept fixed. To do this, we fixed d = 1024 and
changed S across a range, setting β = 0.8∕√s, which keeps σ2 constant as S is changed: this has the
effect of changing σi (and, as a side effect, Li and L). We can see from figure 2(b) that changing σi
in this way has a much greater effect on LP-SGD than it does on SGD. This validates our theoretical
results, and suggests that σi and Li can effectively determine the effect of low-precision compute
on SGD.
4 Non-linear Quantization
Up till now, most theoretical work in the area of low-precision machine learning has been on linear
quantization, where the distance between adjacent quantization points is a constant value δ. Another
option is non-linear quantization (NLQ), in which we quantize to a set of points that are non-
uniformly distributed. This approach has been shown to be effective for accelerating deep learning
2Previous work (1) used a decaying step size while ours uses a constant step size to achieve a better result.
5
Under review as a conference paper at ICLR 2019
Dimension vs. Noise Ball Size of Low-Precision SGD
(a) The size of the noise ball is not significantly
affected by model size d.
Oi vs. Noise Ball Size of Low-Precision SGD
Figure 2: Plots of the asymptotic loss gap from Figure 1(b) as a function of model size d and σ1.
(b) The size of the noise ball does depend on σ1,
especially when precision is low.
in some settings (Lee et al., 2017). In general, we can quantize to a set of points
D = {-qn, ∙ ∙ ∙ , -q1,q0, q1, ,一,qn-1},
and, just like with linear quantization, we can still use a quantization function Q(w) with randomized
rounding that rounds up or down to a number in D in such a way that E [Q(w)] = w for w ∈
[-qn, qn-1]. When we consider the quantization variance here, the natural dimension-dependent
bound would be
EhkQ(W)- wk2i ≤ d max(qi - qi-i)2.
This is still a tight bound since it holds with equality for a number in the middle of two most distant
quantization points. However, when applied in the analysis of LP-SGD, this bound induces poor
performance and often under-represents the actual result.
Here we discuss a specific NLQ method and use it to introduce a tight bound on the quantization
variance. This method has been previously studied as logarithmic quantization or μ-law quantiza-
tion, and is defined recursively by
q0 = 0,
qi+1 - qi = δ + ζqi
(7)
where δ > 0 and ζ > 0 are fixed parameters. Note that this includes linear quantization as a special
case by setting ζ = 0. It turns out that we can prove a tight dimension-independent bound on the
quantization variance of this scheme. First, we introduce the following definition.
Definition 1. An unbiased quantization function Q satisfies the dimension-free variance bound with
parameters δ, ζ, and η if for all w ∈ [-qn, qn-1] and all z ∈ D,
EhkQ(W)- wk2i ≤ δ kw - zkι + Z kzk2 ∙ kw - zk2 + η kw -zk2 .
We can prove that our logarithmic quantization scheme satisfies this bound.
Lemma 1. The logarithmic quantization scheme (7) satisfies the dimension-free variance bound
with parameters δ, Z, and η = 4(；十/ < 4.
Notice that this bound becomes identical to the linear quantization bound (6) when ζ = 0, so this
result is a strict generalization of our results from the linear quantization case. With this setup, we
can apply NLQ to the low-precision training algorithms we have studied earlier in this paper.
Theorem 2. Suppose that we run LP-SGD on an objective that satisfies Assumptions 1-4, and using
a quantization Scheme that satisfies the dimension-free variance bound. If Z < K, then
E[(f(Wτ) - f(w*))] ≤
∣∣W0 - W*k2 ] (1 + η)ασ2 + δσι + Zσ ∣∣w*∣∣2 ∣ (δL1 + ZL ∣∣w*∣∣2 + Zσ)2
—2ατ —十	2	+	ιμ
This theorem is consistent with Theorem 1 in that, if we set Z = η = 0, which makes logarithmic
quantization linear, they would have an identical result. If we fix the representable range R (the
largest-magnitude values representable in the low-precision format) and choose our quantization
6
Under review as a conference paper at ICLR 2019
Variance and Bounds for Non-Lmear Quantization
SPUnog PUe 8cπΞπ>
O 200	400	600	800 1000 1200 1400
quantized value
Puno8 PUe"uueμe>
Variance and Bound for de-normal FPQ
O 50	100	150	200	250
quantized value
(a) variance and bound for μ-law quantization	(b) variance and bound for FPQ
Figure 3:	A figure showing the actual quantization variance E kQ(w) - wk22 and the tight upper
bound that we introduced in one dimension. Similarly to 1(a) we plot this bound when taking the
minimum over all possible z.
parameters optimally, we get the result that the number of bits we need to achieve objective gap is
log2 O ((Rσ∕ε) ∙ log(1+ σι/σ)). This bound is notable because even in the worst case where we do
not have a bound on σ1 and must use σ1 ≤ √d∙σ,this bound gives us log2 O ((Rσ/ε) ∙log (1+√d)).
That is, while a dimension-dependent factor still remains, it is now “hidden” within a log term. This
greatly decreases the effect of the dimension, and suggests that NLQ may be a promising technique
to use for low-precision training at scale. Also note that, although this bound holds only when
we set Z < 1 = L, which to some extent limits the acceleration of the strides in logarithmic
quantization, the bound L is independent of σ and σι, thus this effect of “pushing ” σι into a log
term is independent of the setting of ζ.
Floating point. Next, we look at another type of non-linear quantization that is of great practical
use: floating-point quantization (FPQ). Here, the quantization points are simply floating-point num-
bers with some fixed number of exponential bits be and mantissa bits bm . Floating-point numbers
are represented in the form
( — I)Signbit ∙ 2exponent-bias ∙(1..？.？ ... .5. )	(8)
where “exponent” is a be-bit unsigned number, the mi are the bm bits of the mantissa, and “bias” is
a term that sets the range of the representable numbers by determining the range of the exponent. In
standard floating point numbers, the exponent ranges from [-2be-1 + 2, 2be-1 - 1], which corre-
sponds to a bias of 2be-1 - 1. To make our results more general, we also consider non-standard bias
by defining a scaling factor s = 2-(bias-standard bias); the standard bias setting corresponds to s = 1.
We also consider the case of denormal floating point numbers, which tries to address underflow by
replacing the 1 in (8) with a 0 for the smallest exponent value. Under these conditions, we can prove
that floating-point quantization satisfies the bound in Definition 1.
Lemma 2. The FPQ scheme using randomized rounding satisfies the dimension-free variance bound
with parameters δnormal , ζ, and η for normal FPQ and δdenormal , ζ, and η for denormal FPQ where
δ normal = 226匕
δdenormal
8s
22be +bm
Z = 2 bm , η = 4(Z+1).
This bound can be immediately combined with Theorem 2 to produce dimension-independent
bounds on the convergence rate of low-precision floating-point SGD. Ifwe are given a fixed number
of total bits b = be + bm , we can minimize this upper bound on the objective gap to try to predict
the best way to allocate our bits between the exponent and the mantissa. Unfortunately, there is no
analytical expression for this optimal choice ofbe. To give a sense of the asymptotic behavior of this
optimal allocation, we present upper and lower bounds on it.
Theorem 3. When using FPQ without denormal numbers, given b total bits, the optimal number of
exponential bits be such that the asymptotic upper bound on the objective gap given by Theorem 2
is minimized is in the interval between:
h	(2(In 2)s%、I 9d	h ( 2(In 2)sLi、_|_ 9^i
log2	I2 log2	σ∣∣w*∣∣2	+ 2b	and	log2	I2 log2	(Lkw*|丁0)	+2b].
7
Under review as a conference paper at ICLR 2019
SGD noise ball size under FPQ with 16 bits
65432 1 O 1 2
Ooooooo - -
Iiiiiiioo
1 1
Bz=eq əs-ou
2	3	4	5	6	7	8
number of exponential bits
BZ-S=eq asδu
=eq Ss-Ou -OU-UEOVf
5 4
O O
1 1
(a) Training noise ball size of SGD using 16 bits	(b) Training noise ball size of SGD using 16 bits
normal FPQ on synthetic data set	normal FPQ on MNIST
Figure 4:	Plots of noise ball size vs. be when running SGD with 16 bits FPQ on synthetic data set
and MNIST. Note the use of two y-axes in Figure 4(b) to make the series fit in one figure.
Theorem 4. When using denormal FPQ, given b total bits, the optimal number of exponential bits
be such that the asymptotic upper bound on the objective gap, as T → ∞ and α → 0, given by
Theorem 2 is minimized is in the interval between:
l hi 2 W (eσkw*k2M	d 1	hi	2 W (e(Lkw*k2+LM
log2 [1 - ιn2 W	8sσι	and	log2 [1 - ιn2 W(	8sLτ~
where e denotes the base of the natural logarithm and W stands for the Lambert W function. In
cases where neither of these two values exists, the noise ball size increases as be, thus be = 2 would
be the optimal setting, which is equivalent to linear quantization.
These theorems give us an idea of where the optimal setting of be lies such that the theoretical
asymptotic error is minimized. When using normal FPQ, this optimal assignment of be is O(log(b)),
and for denormal FPQ the result is independent of b. This suggests that once the total number of bits
grows past a threshold, we should assign most of or all the extra bits to the mantissa.
Experiments For FPQ, we ran experiments on two different data sets. First, we ran LP-SGD on
the same synthetic data set that we used for linear regression. Here we used normal FPQ with 20
bits in total, and we get the result in Figure 4(a). In this diagram, we plotted the empirical noise
ball size, its theoretical upper bound, and the optimal interval for be as Theorem 3 predicts. As the
figure shows, our theorem accurately predicts the optimal setting of exponential bits, which is 5 in
this case, to minimize both the theoretical upper bound and the actual empirical result of the noise
ball size, despite the theoretical upper bound being loose.
Second, we ran LP-SGD on the MNIST dataset (Deng, 2012). To set up the experiment, we nor-
malized the MNIST data to be in [0, i] by dividing by 255, then subtracted out the mean for each
features. We ran multiclass logistic regression using an L2 regularization constant of i0-4 and a
step size of α = i0-4, running for 500 total epochs (passes through the dataset) to be sure we con-
verged. For this task, our (measured) problem parameters were L = 37.4i, L1 = 685.27, σ = 2.38,
σ1 = 29.ii, and d = 784. In Figure 4(b), we plotted the observed loss gap, averaged across the
last ten epochs, for LP-SGD using various 16-bit floating point formats. We also plot our theoretical
bound on the loss gap, and the predicted optimal number of exponential bits to use based on that
bound. Our results show that even though our bound is very loose for this task, it still predicts the
right number of bits to use with reasonable accuracy. This experiment also validates the use of IEEE
standard half-precision floating-point numbers, which have 5 exponential bits, for this sort of task.
5 Conclusion
In this paper, we present dimension-independent bounds on the convergence of SGD when applied
to low-precision training. We point out the conditions under which such bounds hold. We further
extend our results to non-linear methods of quantization: logarithmic quantization and floating point
quantization. We analyze the performance of SGD under logarithmic quantization and demonstrate
that NLQ is a promising method for reducing the number of bits required in low-precision train-
ing. We also presented ways in which our theory could be used to suggest how to allocate bits
between exponent and mantissa when FPQ is used. We hope that our work will encourage further
investigation of non-linear quantization techniques.
8
Under review as a conference paper at ICLR 2019
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems, pages 1707-1718, 2017.
Doug Burger. Microsoft unveils project brainwave for real-time ai. Microsoft Research, Microsoft,
22, 2017.
Adrian M Caulfield, Eric S Chung, Andrew Putnam, Hari Angepat, Daniel Firestone, Jeremy Fow-
ers, Michael Haselman, Stephen Heil, Matt Humphrey, Puneet Kaur, et al. Configurable clouds.
IEEE Micro, 37(3):52-61, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with
low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pages 3123-3131, 2015.
Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha,
Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas,
et al. Mixed precision training of convolutional neural networks using integer operations. arXiv
preprint arXiv:1802.00930, 2018.
Christopher De Sa, Matthew Feldman, Christopher Re, and KUnle Olukotun. Understanding and
optimizing asynchronous low-precision stochastic gradient descent. In Proceedings of the 44th
Annual International Symposium on Computer Architecture, pages 561-574. ACM, 2017.
Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger, Kun-
le Olukotun, and Christopher Re. High-accuracy low-precision training. arXiv preprint arX-
iv:1803.03383, 2018.
Christopher M De Sa, Ce Zhang, Kunle Olukotun, and Christopher Re. Taming the wild: A unified
analysis of hogwild-style algorithms. In Advances in neural information processing systems,
pages 2674-2682, 2015.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
Signal Processing Magazine, 29(6):141-142, 2012.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International Conference on Machine Learning, pages 1737-
1746, 2015.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. In Advances in neural information processing systems, pages 4107-4115, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In International Conference on Neural Information Processing Systems, pages 315-
323, 2013.
Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,
Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analy-
sis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on
Computer Architecture, pages 1-12. ACM, 2017.
Urs Koster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal, William Constable, Oguz
Elibol, Scott Gray, Stewart Hall, Luke Hornof, et al. Flexpoint: An adaptive numerical format
for efficient training of deep neural networks. In Advances in Neural Information Processing
Systems, pages 1742-1752, 2017.
Edward H Lee, Daisuke Miyashita, Elaina Chai, Boris Murmann, and S Simon Wong. Lognet:
Energy-efficient neural networks using logarithmic computation. In Acoustics, Speech and Signal
Processing (ICASSP), 2017 IEEE International Conference on, pages 5900-5904. IEEE, 2017.
9
Under review as a conference paper at ICLR 2019
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training
quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems,
pages 5813-5823,2017.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pages 525-542. Springer, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep
neural networks. arXiv preprint arXiv:1802.04680, 2018.
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. Zipml: Training linear
models with end-to-end low precision, and a little bit of deep learning. In International Confer-
ence on Machine Learning, pages 4035-4043, 2017.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arX-
iv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
10
Under review as a conference paper at ICLR 2019
A Algorithm
In our work, we presented dimension-free bounds on the performance of low-precision SGD, here
we present the algorithm in detail.
Algorithm 1 LP-SGD: Low-Precision Stochastic Gradient Descent
given: n loss functions fi, number of epochs T , step size α, and initial iterate w0.
given: low-precision quantization function Q.
for t = 0 to T - 1 do
sample it uniformly from {1, 2, ∙…，n},
quantize wt+ι — Q (Wt - aVf⅛ (Wt))
end for
return WT
B Proof for results in Table 1
As mentioned in the caption of Table 1, here only we consider the convergence limit, that is, we
assume α → 0, T → ∞, and we compute the minimum number of bits b we would require in
order for the limit to be less than some small positive ε. Meanwhile, we denote the radius of the
representable range by R and We assume R = [∣w* |卜 without loss of generality, as this is the worst
case for all our bounds that depend on ∣∣w* |卜.Then in linear quantization, we have:
q2b-1-1 = δ ∙ (2b-1 - 1) ≥ R
and in non-linear quantization, we need:
q2b-1-1 = Z ((1 + Z)"-1T)- 1) ≥ R	(9)
In the following proof we’ll take the equality for these two inequalities.
B.1	LP-SGD in previous work
In previous work Li et al. (2017), we have
f (WT) - f (W* ) ≤
(1 + log(T + 1))G2 + Gδ√d
2μT
2
m2 ax is an upper bound on the
here we re-denote G as σmax for concordance with our result. Here σ
second moment of the stochastic gradient samples E [∣∣f(w)∣∣2] ≤ σ2mχ. Substitute δ with 2b-R_1
and set the limit (as α → 0 and T → ∞) to be ≤ ε, and notice that 2b-1 - 1 > 2b-2, then we have:
σmaxRvZd	八 f 、
2(2b-i- 1) = O (ε) ⇒
σ ^λ R. ( σmaxRVd
b ≤ i0g21 —ε—
+ 1 = log2 O (σmaxR8
B.2	LP-SGD in our work
In Theorem 1, we know that
口⑺ r * *∖ι/ 1 H	*∣∣2 l ασ2+ δσι l δ2κ2μ
E [f(W)- f(w )] ≤ 2ατ kwo- W ∣∣2 + ——2——+ ~^τ~
Set the limit (as α → 0 and T → ∞) to be ≤ ε, then we need:
等=O (ε), δ2Kμ = O (ε)
Then for sufficiently small ε, more explicitly, ε that satisfies 悬O(ε) < 1, setting
δ=O σ^
11
Under review as a conference paper at ICLR 2019
will satisfy the requirements, and we will get
R
2b-1 - 1
⇒ b = log2 O
σιR
This is the expression that we wanted. Notice that even if we did not invoke small ε in the above
big-O analysis, we can set
δ = O min
Then our number of bits would look like
b = log2 O max
which shows explicitly that we have replaced the dimension factor with parameters of the loss func-
tions.
B.3	LP-SGD in our work using NLQ
In Theorem 2, We know that, if Z < ɪ, then
E[(f(W)-f(w*))] ≤ 2αT kwo- w*k2 +
(1+ η)ασ2 + δσι + Zσ ∣∣w*∣∣2 ∣ (δL1 + ZL ∣∣w*∣∣2 + Zσ)2
2
4μ
Set the limit (as α → 0 and T → ∞)to be ≤ ε and replace ∣∣w* |卜 with R; then we get
δσι + CσR	(δLι + ZLR + Cσ)2 _ o / x
2	+	4μ	=	(ε).
So, in addition to our requirement that Z ≤ κ-1, it suffices to have
δσι = O (ε), ZσR = O (ε),	δ2L1 = O (ε),	Z2(LR + σ)2 = O (ε).
μ	μ
If we set
O(ε)
σι
Z— O^
Z σR
δ
then all our other requirements will be satisfied for sufficiently small ε. Specifically, we need ε to
be small enough that
σRO (ε) ≤ 1,	σ⅛O (ε) ≤ 1,
(LR + σ)2
σ2R2μ
O(ε)≤1.
As is standard in big-O analysis, we assume that ε is small enough that these requirements are
satisfied, in which case our assignment of δ and Z, combined with the results of Theorem 2, is
sufficient to ensure an objective gap of ε. Next, starting from (9), the number of bits we need for
non-linear quantization must satisfy
(1 + Z)(2b-1-1)- 1 ≥ ZR
δ
which happens only when
(2b-1 - 1) log(1 + Z) ≥ log 1 +
Since we know that 0 ≤ Z < 1, it follows that log(1 + Z) ≥ Z/2. So in order for the above to be
true, it suffices to have
(2b-1 - 1) ∙ 2 ≥ log (1 + ZR).
Since 2b-1 - 1 > 2b-2, it follows that it suffices to have
2b ∙ 8 ≥ log
1+
12
Under review as a conference paper at ICLR 2019
And this will be true if
b = log2 O (1 log(1 +
Finally, using our assignment of δ and ζ gives us
b= log2 O
(P log (1 + σ1)).
This is the expression that we wanted. Notice that even if we did not invoke small ε in the above
big-O analysis, We would still get a rate in which all of our '1-dependent terms are inside the double-
logarithm, because none of the requirements above that constrain Z are '1-dependent. To be explicit,
to do this we would set δ and ζ to be
ζ=O
min
ε	√εμ	ι
σR, LR + σ，κ
Then our number of bits would look like
b = log2 O
LR⅛σ ,κ)∙log(ι +
√εμ	) ∖
which shows explicitly that any '1 -dependent terms are inside the double logarithm.
C Proof for theorems
Before we prove the main theorems presented in the paper, we will prove the following lemmas that
will be useful later, as well as the lemmas we presented before.
The proof of lemma 1 can be extracted from the proof of lemma 5 that we will show later.
Proof of Lemma 2. Here we consider the positive case first, then symmetrically the negative case
also holds. First, for normal FPQ, the set of quantization points are:
D = {0} ∪ {s ∙(1 + x-) ∙ 2y | X = 0,1,…
nm - 1, y = - ne + 2, ∙∙∙ ,n2e - 1
and we set the parameters for the nonlinear quantization bound to be:
δ = S ∙ 2-n2e +2 = 4s Z = ɪ „
'=	=(√2)ne , Z = nm , η
1
4(1 + ζ)	4nm(nm + 1)
For any w within representable range, we can assume it is in [qi, qi+1), then
E [[Q(w) - w]2]	=	qi+1W ∙	(w	-	qi)2	+ —— ∙	(qi+1 -	w)2
qi+1 - qi	qi+1 - qi
= (w - qi)(qi+1 - w)
So now we only need to prove that
∀v ∈ D, (w — qi)(qi+ι 一 W) ≤ δ ∙ |w - v| + Z ∙ |v| ∙ |w — v| + η ∙ |w — v|2
First, we consider a special case where qi = 0. In this case, qi+ι = S ∙ 1 ∙ 2-ne+2 = δ. If V = 0, it
is obvious that
LHS = (w - qi)(qi+1 - w) = w(δ - w) ≤ δw ≤ RHS
and similarly for v = δ,
LHS = (w - qi)(qi+1 - w) = w(δ - w) ≤ δ(δ - w) ≤ RHS
and for v > δ,
RHS ≥ δ(v-w) ≥ δ(δ-w) ≥ w(δ-w) = LHS
13
Under review as a conference paper at ICLR 2019
Next, We consider the case where q% = 0. In this case, We can assume q% = S ∙ (1 + ɪ ) ∙ 2y, then
nm
qi+1 - qi = S ∙ 2y ≤ nmqi = ζqi.
If v ≥ qi+1, denote y = qi+1 - w, then
LHS =(W - qi)(qi+ι - W) = y ∙ (qi+ι - qi - y) = y ∙ (Zqi - y)
RHS ≥ Z ∙ qi+ι ∙ (qi+ι - W) = Zqi+ιy ≥ Zqiy ≥ LHS
Secondly, if 0 ≤ v ≤ qi , denote y = W - qi , then
LHS = (w - qi)(qi+ι - W) = y ∙ (qi+ι - qi - y) = y ∙ (Zqi - y)
RHS = δ ∙ (w — v) + Z ∙ V ∙ (w — v) + η ∙ (w — v)2
=—(Z — η) ∙ v2 + (-δ + ZW — 2ηw) ∙ V + δw — δ2
observe that Z - η > 0, so the right hand side is a concave function of v, thus it achieves minimum
at either v = 0 or v = qi . At v = qi :
RHS = δy + Zqiy + ηy2 ≥ Zqiy ≥ LHS
and at v = 0, since qi+1 ≤ (1 + Z)qi and qi ≤ w,
RHS — LHS = δ ∙ w + Z • 0 ∙ W + η ∙ w2 — (w — qi)(qi+ι — w)
= (1 + η)w2 + (δ - qi - qi+1)w + qi qi+1
≥	(1 +	η)w2	-	(qi + qi+ι) ∙ W + qiqi+ι
=	(1 +	η)w2	-	[(2 + Z)qiw + (qi+1 - (1 +	Z)qi)w] +	[(1	+	Z)qi2	+ (qi+1 - (1 + Z)qi)qi]
=(1+	η)w2	-	(2 + Z)qi ∙ W + (I + Z)q2 +	(qi+1	- (I +	Z)qi)(qi - w)
≥	(I +	η)w2	-	(2 + Z)qi ∙ W + (I + Z)q2
which is a positive parabola. Recall that η = ^+ɪj = 淤斗)-1, thus the determinant is (2 +
Z)2qi2 -4(1 +η)(1 +Z)qi2 = 0, therefore RHS - LHS ≥ 0.
Now we extend this conclusion to the case where v ≤ 0. In this case,
RHS = δ ∙ (w — v) + Z ∙ (—v) ∙ (w — v) + η ∙ (w — v)2
since w, Z, δ, η are all positive, this is apparently a decreasing function of v, thus it achieves mini-
mum at v = 0, which is what we have already proven.
So far, we’ve proven the lemma in the case of w ≥ 0, v ≥ 0 and w ≥ 0, v ≤ 0, and symmetrically
it holds for w ≤ 0, v ≤ 0 and w ≤ 0, v ≥ 0, which indicates that we can extend D to be a set
containing both positive and negative numbers.
In the de-normal FPQ case, the set of quantization points are:
D = S s ∙ x— ∙ 2-等+3 | X = 0,1, ∙∙∙ ,nm - 1，
nm
∪ [s ∙ (1+---) ∙ 2y | x = 0, 1,…,nm - 1, y = - ^7e + 3,…，"Te - 1
nm	2	2
and we set the parameters for the nonlinear quantization bound to be:
「	1	ne+3	8	Sne	ι 1
δ =S ∙ nm ∙2 2	= c ∙ We, z = nm, η
1
4(1 + ζ)	4nm(nm + 1)
The proof for this case follows the exact same structure as the normal FPQ case.
□
Lemma 3. Under condition of linear quantization when using low-precision representation (δ, b),
for any w, v ∈ Rd where Q(δ,b) (w) = w,
EhlIQ(δ,b)(w + v) - w*∣∣2i ≤ k(w + v) - w*k2+ δkvkι.
where Q is the linear quantization function.
14
Under review as a conference paper at ICLR 2019
Proof of Lemma 3. (This proof follows the same structure as the proof for lemma 1 in (De Sa et al.,
2018)) First, observe that this lemma holds if it holds for each dimension, so we only need to prove
that for any w, v ∈ R where Q(δ,b) (w) = w, i.e. w ∈ dom(δ, b),
E [(Q(δ,b)(w + V) — w*)2] ≤ (W + V - w*)2 + δ∣v∣
then we can sum up all the dimensions to get the result.
Now we consider the problem in two situations. First, if w + V is within the range representable by
(δ, b), then E Q(δ,b)(w + V) = w + V. In this case,
E [(Q(δ,b)(W + V)- w*)2]
=E [[(Q(δ,b)(w + v) - (w + V)) - ((w + v) - W*)]2]
=E [[Q(δ,b)(W + V) - (w + v)]2 - 2[Q(δ,b)(w + V) - (w + V)H(W + V) - w*]]
+ [(w + V) - w*]2
= E [[Q(δ,b)(W +V) - (W + V)]2] - 2[(W +V) - (W + V)][(W + V) - W*]
+ [(W + V) - W*]2
= [(W+V) - W*]2 +E [[Q(δ,b)(W + V) - (W + V)]2]
Since (W + V) is within representable range, E [Q(δ,b)(W + V) - (W + V)]2 is equivalent to
E [[Q(δ,∞) (V) +W - (W+V)]2], which equals E [[Q(δ,∞) (V) - V]2] since Q(δ,b)(W) = W.
Now We only need to prove that E [[Q(6,∞)(v) — v]2] ≤ δ∣V∣. Observe that this trivially holds for
V = 0, and is symmetrical for positive and negative V. Without loss of generality we assume V > 0,
let z be the rounded-down quantization ofV, then we have z ≥ 0. Then Q(δ,b)(V) will round to z+ δ
(the rounded-up quantization of v) with probability v-z, and it will round to Z with probability
z+δ-v. This quantization is unbiased because
E Q(δ,∞)(W)
V - z	z + δ - V	Vz - z2 +Vδ - zδ	z2 + zδ - Vz
—(Z+δ)+-^z=—δ—+—δ—
V.
Thus, its variance will be
E [(Q(δ,∞)(V) -V)2] = v--z (Z+δ -V)2+z+δ_v(Z -V)2
=(V -Z)(Z+δ -V)(Z+： -V+v-z
δδ
= δ(V - Z) - (V - Z)2
≤ δ(V - Z) ≤ δV.
therefore
E [(Q(δ,b)(W + v) - W*)2] ≤ (w + V - W*)2 + δ∣v∣
In the other case, when W+V is on the exterior of the representable region, the quantization function
Q(δ,b) just maps it to the nearest representable value. Since W* is in the interior of the representable
region, this operation will make W + V closer to W*. Thus,
(Q(δ,b) (W + V) - W*)2 ≤ (W+V - W*)2,
and so it will certainly be the case that
E [(Q(δ,b)(w + v) - W*)2] ≤ (w + V - W*)2 + δ∣v∣.
Now that we’ve proven the inequality for one dimension, we can sum up all d dimensions and get
E hQ(δ,b)(W	+	V)	- W*22i	≤	k(W+V) - W*k22	+ δkVk1 .
□
For completeness, we also re-state the proof of following lemma, which was presented as equation
(8) in (Johnson and Zhang, 2013), and here we present the proof for this lemma used in (De Sa et al.,
2018).
15
Under review as a conference paper at ICLR 2019
Lemma 4. Under the standard condition of Lipschitz continuity, ifi is sampled uniformly at random
from {1, . . . , N}, then for any w,
EhkVfi(W)- Vfi(w*)k2] ≤ 2L (f(w)- f(w*)).
Proof of Lemma 4. For any i, define
gi(w) = fi(w) - fi(w*) — (W — W*)TVfi(W)
Clearly, if i is sampled randomly as in the lemma statement, E [gi(w)] = f (w). But also, w* must
be the minimizer of gi, so for any W
gi(W*) ≤ min gi (W - ηVgi (W))
η
≤ min (gi(w) - η I∣vgi(w)k2 + η2L kvgi(w)k2)
=gi(W) — 21L kvgi(w)k2 .
where the second inequality follows from the Lipschitz continuity property. Re-writing this in terms
of fi and averaging over all the i now proves the lemma statement.	□
Lemma 5. Under the condition of logarithmic quantization, for any W, v ∈ Rd where v ∈ Dd,
E hkQ(W) — W*k22i ≤ kW — W*k22 + δkW —vk1 + ζ kvk2 kW — vk2 +ηkW —vk22
where Q is the non-linear quantization function.
Note that the proof this lemma naturally extends to lemma 1, thus we omitted the proof for lemma 1
and just present the proof for lemma 5.
Proof of Lemma 5. Here we only consider the positive case first, where
D = {qo,qι,…，qn-ι}
with [0, qn-1] being the representable range of D. As for the negative case, we will show later that
it holds symmetrically.
Observe that this lemma holds if it holds for each dimension, so we only need to prove that for any
W, v ∈ R where v ∈ D,
E [[Q(w) — w*]2] ≤ |w - w*∣2 + δ ∙ |w — v| + Z ∙ |v| ∙ |w — v| + η ∙ |w — v|2
then we can sum up all the dimensions and use Cauchy-Schwarz inequality to get the result.
Now we consider the problem in two situations.
First, if W is outside the representable range, the quantization function Q just maps it to the nearest
representable value. Since W* is in the interior of the representable range, this operation will make
W closer to W*. Thus,
[Q(W) — W*]2 ≤ (W — W*)2,
and so it will certainly be the case that
E [[Q(w) — w*]2] ≤ |w - w*∣2 + δ ∙ |w — v| + Z ∙ |v| ∙ |w — v| + η ∙ |w — v|2
Second, ifW is within the representable range, then E [Q(W)] = W. In this case,
E [[Q(w) — w*]2]
= E [[(Q(w) — w) — (w — w*)]2]
= E [[Q(w) — w]2 — 2[Q(w) — w](w — w*)] + (w — w*)2
= E [[Q(w) — w]2] — 2(w — w)(w — w*) + (w — w*)2
= (w — w*)2 + E [[Q(w) — w]2]
16
Under review as a conference paper at ICLR 2019
Since w is within representable range, we can assume it is in [qi, qi+1), then
E [[Q(w) - w]2] = qi+1"-W ∙ (w - qi)2 + —— ∙ (qi+1 - w)2
qi+1 - qi	qi+1 - qi
= (w - qi)(qi+1 - w)
So now we only need to prove that
(W - qi)(qi+ι - W) ≤ δ ∙ |w - v| + Z ∙∣v∣∙∣w - v| + η ∙ |w - v|2
Note that v ∈ D, so it is either v ≥ qi+1 or v ≤ qi .
Firstly, if v ≥ qi+1 , denote y = qi+1 - W, then
LHS =(W - qi)(qi+ι - W) = y ∙ (qi+ι - qi - y) = y ∙ (δ + Zqi - y)
RHS = δ	∙	(v — w)	+ Z ∙ V ∙ (v —	w) + η ∙ (v — w)2
≥ δ	∙	(qi+ι -	w) + Z ∙ qi+1	∙ (qi+1 — w) + η ∙	(qi+1	— w)2
2
= δy + Zqi+1y + ηy2
≥ δy + Zqiy - y2 = LHS
Secondly, if 0 ≤ v ≤ qi , denote y = w - qi , then
LHS =	(w - qi)(qi+ι -	w) = y ∙ (qi+1 - qi - y) =	y ∙ (δ +	Zqi	- y)
RHS =	δ ∙ (w — v) + Z ∙	v ∙ (w — v) + η ∙ (w — v)2
=—(Z — η) ∙ v2 + (—δ + ZW — 2ηw) ∙ V + δw — δ2
observe that Z - η > 0, so the right hand side is a concave function of v, thus it achieves minimum
at either v = 0 or v = qi . At v = qi :
RHS = δy + Zqiy + ηy2 ≥ δy + Zqiy - y2 = LHS
and at v = 0:
RHS — LHS =	δ ∙	w + Z ∙ 0 ∙	W	+ η ∙ w2 — (w — qi)(qi+ι — w)
=	(1	+ η)w2 +	(δ	- qi - qi+1 )w + qiqi+1
=	(1	+ η )w2 -	(2	+ Z )qi • w + qiqi+ι
≥	(I	+ η)w2 -	(2	+ Z)qi • W + (I + Z)q2
which is a positive parabola. Recall that η = 品+ij = 给露-1, thus the determinant is (2 +
Z)2qi2 -4(1 +η)(1 +Z)qi2 = 0, therefore RHS - LHS ≥ 0.
Now we extend this conclusion to the case where v ≤ 0. In this case,
RHS = δ • (w - v) + Z • (-v) • (w - v) + η • (w - v)2
since w, Z, δ, η are all positive, this is apparently a decreasing function of v, thus it achieves mini-
mum at v = 0, which is what we have already proven.
So far, we’ve proven the lemma in the case of w ≥ 0, v ≥ 0 and w ≥ 0, v ≤ 0, and symmetrically
it holds for w ≤ 0, v ≤ 0 and w ≤ 0, v ≥ 0, which indicates that we can extend D to be a set
containing both positive and negative numbers, and we can reset D to be
D = {-qn, • • • , -q1,q0, q1, • • • ,qn-1}
where
q0 = 0, qi+1 - qi = δ + Zqi
□
Now we have proven all the lemmas we need. Next, we make some small modifications to the
assumptions (weakening them) so that our theorems are shown in a more general sense. For as-
sumption 2, we change it to:
17
Under review as a conference paper at ICLR 2019
Assumption 5. All the gradients of the loss functions fi are L1 -Lipschitz continuous in the sense of
1-norm to p-norm, that is,
∀i ∈ {1, 2,…n}, ∀x,y, ∣∣Vfi(χ) - Vfi(y)kι ≤ LI ||x - y||p
While in the body of the paper and in our experiments we choose p = 2 for simplicity, here we
are going to prove that a generalization of Theorem 1 holds for all real numbers p. We also need a
similar generalization of Assumption 3.
Assumption 6. The average of the loss functions f = n Ei f is μι- strongly Convex near the
optimal point in the sense of p-norm, that is,
∀w, μ ||w - w*||p ≤ f (w) - f(w*)
with p being any real number.
This assumption is essentially the same as the assumption for strong convexity that we stated before,
since in practice We would choose P = 2 and then μι and μ would be the same. But here We are
actually presenting our result in a stronger sense in that we can choose any real number p and the
proof goes the same.
Now we are ready to prove the theorems. Note that the result of the following proof contains μι
since we are proving a more general version of our theorems; substituting them with μ will lead to
the same result that we stated before.
Proof of Theorem 1. In low-precision SGD, we have:
ut+1 = wt - αVft (wt), wt+1 = Q(ut+1 )
by lemma 3, we know that
E h∣wt+ι-w*∣∣2] = E IQ(Wt-aVft(wt)) - w[j]
≤ E M- αVft(wt) - w*∣U + δE [卜Vft(Wt)∣∣ J
=EhkWt- w*∣2] - 2αE h(wt - w*)TVft(Wt)]
+ α2E ∣∣Vft(wt)∣∣2] + αδE[∣∣Vft(wt)∣∣J
≤ EhkWt- w*k2] - 2αE h(f(Wt)- f(w*)) + 2 ∣* - w*∣∣2]
+ α2E ∣∣Vfi(Wt)∣∣2] + αδE[∣∣Vft(Wt)∣∣J
=(1 - αμ)E h∣Wt - W*k2] + α2E ∣∣Vft(Wt)∣1] + αδE [∣∣Vft(Wt)∣∣J
-2αE[(f(Wt)- f(W*))]
where the second inequality holds due to the strongly convexity assumption. According to the
assumptions we had, we have:
E [∣Vfi(W)k2] = E[∣∣Vfi(W)-Vfi(W*)+ Vfi(W*)∣∣2]
=EhkVfi(W)- Vfi(W*)k2 + 2(Vfi(W) - Vfi(W*))TVfi(W*) + ∣∣Vfi(W*)k2]
=EhkVfi(W)- Vfi(W*)k2 + kVfi(W*)k2]
≤ L2 ∙ E [∣w - w*∣∣2] + σ2
E[kVfi(W)kJ = E [kVfi (W)-Vfi(W*) + Vfi(W*)kj
≤ E [kVfi(W)-Vfi(W*)kι + kVfi(w*)kJ
≤ Li ∙ E [∣w - w*k2] + σι
18
Under review as a conference paper at ICLR 2019
where the last inequality holds due to assumption 2 where we let p = 2. Applying this result to the
previous formula and we will have:
Ehkwt+ι - w*k2] ≤ (I- αμ)E hkwt - w*k2i + α2E ∣∣v∙ft(wt)∣∣2] + αδEhKft(Wt)Ii
— 2αE [(f (wt) - f(w*))]
≤ (1 — αμ + α2L2)E [kwt — w* ||2] + αδL1E [kwt — w* |图
—2αE [(f (wt) — f (w*))] + α2σ2 + αδσι
Here we introduce a positive constant C that we’ll set later, and by basic inequality we get
α2δ2L2	α2δ2L2
αδLιE[∣∣wt- w*k2]	≤	CE[∣∣wt- w*k2]2	+	≤	CE	Ilwt-	w*∣∣2	+
4C	4C
thus
E kwt+1 - w*k22
≤ (1 - αμ + α2L2 + C)E [∣∣wt - w*∣∣2]
-2αE[(f(wt)-f(w*))]
+ α σ + αδσ1 +
α2δ2L2
4C
one setting C to be αμ 一 α2L2, We will have:
2αE [(f(wt) - f (w*))] ≤ E h∣wt - w*∣∣2i -E h∣wt+ι - w*∣∣2] +α2σ2 +αδσι +。[⑶
L	」 L	」	4( α μ α L )
since we can set α to be small enough such that αL2 ≤ 2, then the result will become:
2αE [(f (wt) - f (w*))] ≤ E kwt - w*k22
-E [kwt+ι - w*∣∣2] + α2σ2 + αδσι + α:；1
now we sum up this inequality from t = 0 to t = T - 1 and divide by 2αT , then we get:
T-1
X E [(f(wt) - f(w*))] ≤
t=o
1
T
≤
kw0 - w*k2- EblwT - w*k2]	ασ2 + δσ1 δ2L2
20τ	+ ―2 — + 而
kwo — w*k2 ɑσ2 + δσι δ2κ1 μ
2αT	+^^ + 丁
and since we sample w uniformly from (wo,wι,…,wτ-ι),we get
E [(f(W) - f(w*))] ≤ 2αT kwo - w*k2+ασ + δσ1+δκμ
□
Proof of Theorem 2 . In low-precision SGD, we have:
..	_ V7 _?/一.、	八/一	、
ut+ι = wt - αVft(Wt), wt+1 = Q(Ut+1)
19
Under review as a conference paper at ICLR 2019
by lemma 5, we know that
E h∣wt+ι - w*∣2] = E ]∣∣Q(wt - aVft(wt)) - w*(
E
≤
Wt - αVf (Wt) - w*∣L
+SEhI 卜 Vft(Wt) i i ι]+Z EhkWtk2l 卜Vf (Wt)∣口
+ ηE ∣∣αVf(Wt)
2,
2
≤
E [∣∣Wt - W*k2] — 2αE [(Wt — w*)tVS(Wt)]
〜
2
+ α2E IlVf(Wt)
2
+ αδE h I I Vft(Wt) I IJ + ZE h(IlWt — w*I∣2 + I∣w*k2) ∙α llVft(Wt)∣L] + ηα2E	llVft(Wt)
2
2
k2] + α2E IlVf(Wt)
≤
E [kWt- w*∣∣2] - 2αE [(/(wt) - f(w*)) + 2 ∣wt - w*
〜
2
2
+ αδE h l l Vft(wt) l l J + αZE [(∣∣wt -
(1 - α川)E [∣∣wt - w*∣2] +(1 + η)&2]
w*∣∣2 + ∣∣w*
E	∣∣Vft(wt)
回 IlVft(Wt)∣∣J
2
+ η02E	∣∣Vf(wt)
I - 2αE [(f(Wt)- f(w*))]
〜
2
2
+ aZ E h(IWt- w*k2 + kw*k2)IlVft(Wt)∣□ + αδE h||Vft(Wt)llJ
where the third inequality holds due to the strongly convexity assumption. According to the assump-
tions we had, we have:
E hkVfi(w)k2] = E[kVfi(W)-Vfi(w*)+ Vfi(w*)k2]
=E [kVfi(w) - Vfi(w*)k2 + 2(Vfi(w) - Vfi(w*))TVfi(w*) + kVfi(w*)k2]
=EhkVfi(w) - Vfi(w*)k2 + kVfi(w*)k2]
≤ L2 ∙ E h∣w - w*k2] + σ2
kVfi(w)k2 = kVfi(w) - Vfi(w*) + Vfi(w*)k2
≤ kVfi(w) -Vfi(w*)k2 + kVfi(w*)k2
≤ L ∙∣w - w*∣2 + σ
kVfi(w)kι = kVfi(w) - Vfi(w*) + Vfi(w*)kι
≤ kVfi(W)-Vfi(w*)kι + kVfi(w*)kι
≤ L1 ∙ ∣∣w - w*∣2 + σι
where the last inequality holds due to assumption 2 where we let p = 2. Apply this result to the
previous formula, denote η0 = 1+ η, and then we will have:
EhkWt+1- w*k2 ]
≤ (1 - α4)E h∣wt - w*k2] + η0α2E (Vft(Wt)∣J - 2αE [(f (Wt)-/ (w*))]
+ α< Eh(∣∣wt - w*∣∣2 + llw*k2) Il Vft(Wt)∣□ + αδEhIIVft (Wt)(J
≤ (1 — αμ + η0ɑ2L2)E [∣∣Wt — w*∣2] + αδL1E [∣Wt — w*∣2] + η0α2σ2 + αδσι
+ aZE[(IlWt- w*k2 + kw*k2)(L ∙ kw ― w*k2 + σ)] ― 2αE[(f(Wt)- f(W*))]
=(1 — αΜ + αζ"L + η0α2L2)E [∣∣wt — w*∣2] — 2αE [(f(Wt)- f (w*))]
+ (αδL1 + a《L ∣∣w*∣2 + α^σ)E [∣Wt — w*[卜]+ η0α2σ2 + αδσι + αζ,σ ∣∣w*∣2
20
Under review as a conference paper at ICLR 2019
Here we introduce a positive constant C that we’ll set later, and by basic inequality we get
(αδL1 + αZL ∣∣w*∣∣2 + αZσ)E [∣∣wt — w*k2]
≤ CE [kwt - w*k2]2 +
≤ CE [kwt - w*k2] + α
(αδL1 + αZL ∣∣w*∣∣2 + aZσ)2
4C
2(δL1 + ZL ∣∣w*∣∣2 + Cσ)2
4C
thus
Ehk
wt+1 - w
'*k2] ≤ (I- αμ + αCL + η'α2L2 + C)EhkWt- w*k2i -2αE[(f(Wt)- f(w*))]
+ η0α2σ2 + αδσι + αZσ ∣∣w* |卜 +
ɑ2(δL1 + ZL ∣∣w*∣∣2 + Zσ)2
4C
one setting C to be αμ - a《L - η0α2L2, we will have:
2αE [(f(Wt) - f(W*))] ≤ E hkWt - W*k22i -E hkWt+1 -W*
+ η0α2σ2 + αδσι + αZσ ∣∣w* |卜 +
k22
ɑ2(δL1 + ZL ∣∣w*∣∣2 + Zσ)2
4(αμ — a¢L — η0α2L2)
since We can set α to be small enough such that αμ - a《L - η0α2L2 ≥ 21 αμ, then the result will
become:
2αE [(f(wt) - f(w*))] ≤ E [kwt - w*∣∣2] - E [∣∣wt+1 - w*
+ η0α2σ2 + αδσι + αZσ ∣∣w*∣∣2 +
k22
α(δL1 + ZL kw*∣∣2 + 4σ)2
2μ
now we sum up this inequality from t = 0 to t = T - 1 and divide by 2αT , then we get:
1 T-1
T ∑E [(f (Wt) - f(w*))]
t=o
kwo - w*k2 - EhkWT - w*k2i	η0ασ2
≤	20T	+
+ δσι + 4σ ∣∣w*∣∣2 + (6L1 + ZL ∣∣w*∣∣2 + 8)2
2
4μ
/ ∣∣Wo - W*k2	η0ασ2 + δσ1 + Zσ ∣∣w*∣∣2
≤ —2αT - +	2	+
(6L1 + ZL kw*∣∣2 + 力)2
4μ
and since We sample W uniformly from (wo,wι, ∙∙∙ ,wτ-ι),we get
1	η0ασ2
E [(f (W) - f (w*))] ≤ 2αT kwo - w*k2 + η-
+ δσ1 + 4σ ∣∣w*∣∣2 ] (6L1 + ZL ∣∣w*∣∣2 + 力)2
2
4μ
□
Proof of Theorem 3. In the normal FPQ case, the set of quantization points are:
D = {0} ∪ ( s • ( 1+-) ∙ 2y | X = 0, 1, …，nm - 1, y = - ^τe + 2,…，Te - 1
nm	2	2
then the parameters for the nonlinear quantization bound can be computed as:
δ = S • 2-等+2 = -4S^, Z =工,η
(√2)ne,	nm
1
4(1 + ζ)	4nm(nm + 1)
For NLQ-SGD, the noise ball size according to theorem 2 is:
δσ1 + ζσ kw*k2 + (δLι + ZL kw*k2 + S)2
4μ
2
21
Under review as a conference paper at ICLR 2019
Denote this as 1A + 看B2. When b is large, δ, Z, η are small, then the dominating term for the noise
ball is
1	11	n
A = δσ1 + ζσ kw k2 =4sσι	ne + σ kw ∣∣2 一 =4sσι	+ + σ kw ∣∣2 —
2	nm	2	C
let the derivative over ne to be 0 and we get:
∂A	1	1
∂ne = -2(ln2)sσι (√2γne+σ kw k2 c = 0,
ne	2(ln2)sσ1C
σ kw*k2
ne = 2 log2
(2(ln2)sσ1C∖
(σ kw*k2 J
be=log2 (2b+2log2 12⅛2⅞
And when b is small, δ, ζ, η are large and the dominating term for the noise ball is
1	11	n
B = δL1+ζL kw k2+ζσ = 4sL1 / 6∖ne +(L kw k2+σ) 一 =4sLι	+(L kw ∣∣2+σ) C
2	nm	2	C
let the derivative of ne to be 0 and we get:
∂B	1
∂ne = -2(ln2)sLι ^√2yne
+ (L kw*k2 + σ)~1 = 0,
C
2(ln2)sL1C
L kw*k2 + σ
ne = 2 log2
p(ln2)sL1Cλ
(L kw*k2 + σ)
be = log2
2b + 2 log2
2(ln2)sLι
L kw*k2 + σ
For b such that neither the terms dominates the result, we know the noise ball size is:
1A + 1B2 = δσ1 + Cσ kw*k2 + (6L1 + ZL kw*∣∣2 + ζσ产
2	4μ —	2	4μ
then the derivative of ne is:
2(1A +ɪB2)=1 ^A + B∂B
∂n ∖2	4μ	)	2 ∂ne	2μ ∂ne
and since both InA and ∂B are increasing functions and We know that:
∂A	∂B
——	=0	——
,
e n =2loσ ( 2(ln2)sσiC )	One n =2loe ( 2(ln2)sL1 C }
ne = 2 log2 I σ∣∣w* ∣∣2	)	ne = 2 log21 Lkw* k2+σ )
then we know the solution of ∣d- (A A + -1-t B2) = 0 is in the interval between 2 log2 (嘈2)4。
dne ∖ 2	4μ J	σ ∖ σkw k 2
and 2loo∙(以由2%%。ʌ
and 2log2 ( Lkw*k2+σ J∙
Proof of Theorem 4. In the denormal FPQ case, the set of quantization points are:
D
S s ∙ — ∙ 2-ne+3 | X = 0,1,…,nm - 1、
nm
∪ IS ∙ (1+-) ∙ 2y | x = 0, 1,…,nm - 1, y = - U + 3,…，U - 1
nm	2	2
then the parameters for the nonlinear quantization bound is:
1 ne +3	8 Sne	1	1
δ =S ∙ nm	∙2 2	= c	∙ We,	ζ =	nm,	η
4(1+ ζ)	4nm (nm + 1)
1
22
Under review as a conference paper at ICLR 2019
For NLQ-SGD, the noise ball size according to theorem 2 is:
δσι + Zσ ∣∣w*∣∣2 + (δL1 + ZL ∣∣w*∣∣2 + Zσ)2
2
4μ
Denote this as ɪA + 4μB2. When b is large, δ, Z, η are small and the dominating term for the noise
ball is
A = δσι + ζσ ∣∣w*k2 = 8C1	nene + σ ∣∣w*k2 —
C 2	nm
let the derivative over ne to be 0 and we get:
8sσ1	ne	. I， *∣∣
丁 (√2T + σ kw k2
ne
C
∂A
∂ne
8sσ1 1 — (ln √2)ne
C	(√2)ne
+ σ Ilw*k2 ∙1 = 0
C
denote V(x) = X ∙ ex, and Lambert W function W(y) = V-1(y),y ≥ - e. then we need
∂A
∂ne
8sσ1 1 — (ln √2)ne
C	(√2)ne
+ σ kw*k2 尸=—j^1 V(1 - (In √2)ne) + σ kw*k2 }
C eC	C
0
thus we have:
ne = 1 -1⅛ w(y
,be = lθg2 [1 - ɪ Wfeσ⅛
2 ln 2	8s61
And when b is small, δ, Z, η are large and the dominating term for the noise ball is
B = δL1 + ZL kw*∣2 + Zσ = *1 ∙/ne + (L I∣w*k2 + σ)ne
let the derivative of ne to be 0 and we get:
∂B
∂ne
8sL1 1 — (ln √2)ne
C	(√2)ne
+ (L kw*k2 + σ)⅛ =当V(1 -(ln√2n) + (L ∣∣w*k2 + σ)⅛=0
C eC	C
thus we have:
ne = 1 - -2- W
e ln2
e(L kw*k2 + 6
8sL1
be = 2log2 1 - 1n2 W
e(L kw*k2 + 6
8sL1
For b such that neither the terms dominates the result, we know the noise ball size is:
1A + ɪ B2 二
2	4μ
then the derivative of ne is:
∂
∂ne
δσ1 + ζσ kw*k2 + (δLi + ZL kw*k2 + ζσ产
4μ
2A+
1 ∂A
----+
2 ∂n
B ∂B
2μ ∂n
2
and since both ∂nA and 需 are increasing functions and we know that:
e
∂A
∂ne
ne = 1- in22 W
∂B
=0,	K-
eσkw*k2、	dne
8sσ1 J
ne =1-τn22 W
e(Lkw*∣∣2+σ)
8sLl
0
then we know the solution of 就(1A +
0 is in the interval between 1 -
ɪ W e e≠*3 and 1 - ɪ W
ln 2	8sσ1	ln 2
e(Lkw*k2 + 6
8sL1
□
23