Under review as a conference paper at ICLR 2019
Towards More Theoretically- Grounded Parti-
cle Optimization Sampling for Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Many deep-learning based methods such as Bayesian deep learning (DL) and deep
reinforcement learning (RL) have heavily relied on the ability of a model being able
to efficiently explore via Bayesian sampling. Particle-optimization sampling (POS)
is a recently developed technique to generate high-quality samples from a target dis-
tribution by iteratively updating a set of interactive particles, with a representative
algorithm the Stein variational gradient descent (SVGD). Though obtaining signifi-
cant empirical success, the non-asymptotic convergence behavior of SVGD remains
unknown. In this paper, we generalize POS to a stochasticity setting by injecting
random noise in particle updates, called stochastic particle-optimization sampling
(SPOS). Notably, for the first time, we develop non-asymptotic convergence theory
for the SPOS framework, characterizing convergence of a sample approximation
w.r.t. the number of particles and iterations under both convex- and noncovex-
energy-function settings. Interestingly, we provide theoretical understanding of a
pitfall of SVGD that can be avoided in the proposed SPOS framework, i.e., particles
tend to collapse to a local mode in SVGD under some particular conditions. Our
theory is based on the analysis of nonlinear stochastic differential equations, which
serves as an extension and a complementary development to the asymptotic con-
vergence theory for SVGD such as (Liu, 2017). With such theoretical guarantees,
SPOS can be safely and effectively applied on both Bayesian DL and deep RL
tasks. Extensive results demonstrate the effectiveness of our proposed framework.
1	Introduction
Recent development of deep-learning techniques have required the ability ofan algorithm to efficiently
explore some particular space (e.g., parameter space) via Bayesian sampling, due to high model
complexity of modern deep models. Meanwhile, recent years have seen the development of scalable
sampling methods such as stochastic gradient MCMC (SG-MCMC) (Welling & Teh, 2011; Chen
et al., 2014; Ding et al., 2014; Chen et al., 2015) and Stein variational gradient descent (SVGD) (Liu
& Wang, 2016) to facilitate big-data analysis. SG-MCMC is a family of scalable Bayesian sampling
algorithms relying on It6 diffusions, linear stochastic differential equations (SDE) with appropriately
designed coefficients such that the corresponding stationary distributions match a target distribution.
One potential issue is that samples might be highly correlated partially due to the nature of Markov
chains, leading to low sample efficiency, an undesirable property of SG-MCMC.
SVGD, on the other hand, belongs to the family of particle-based sampling methods, which optimize
a set of interacting particles to minimize some distance metric between the target distribution and
the distribution induced by the particles. By optimizing the distance measure, one maintains an
optimal set of particles at each time. Recent development of SVGD has shown that the underlying
mathematical principle is based on a family of nonlinear SDEs, in the sense that coefficients of the
SDE depend on the current density of the particles. Though achieving numerous practical successes
(Liu & Wang, 2016; Feng et al., 2017; Liu et al., 2017; Haarnoja et al., 2017; Liu & Zhu, 2018),
little theory has been developed to understand the convergence property of the algorithm. A recent
theoretical development has interpreted SVGD as a special type of gradient flow in the space of
probability measures, and developed theory to disclose its asymptotic convergence behavior (Liu,
2017).
1
Under review as a conference paper at ICLR 2019
Recently, Chen et al. (2018) unified SG-MCMC and SVGD by proposing a particle-optimization
sampling framework that interprets both as Wasserstein gradient flows (WGFs). Generally speaking, a
WGF is a partial differential equation (PDE) defined on the space of probability measures, describing
the evolution of a density function over time. In (Chen et al., 2018), the authors define a WGF by
combining the corresponding Fokker-Planck equations for both SG-MCMC and SVGD, and solve
it with deterministic particle approximations. However, due to the diffusion nature, deterministic-
particle approximation leads to a hard-to-control error, challenging for theoretical analysis.
Based on the unified framework in (Chen et al., 2018), we propose to solve WGFs with stochastic
particle approximation, leading to stochastic particle-optimization sampling (SPOS). The idea is
instead of solving the WGF with an uncontrollable deterministic approximation for a diffusion term,
we solve it stochastically by injecting random noise to the particle-update equations. Remarkably,
for the first time, we develop nonasymptotic convergence theory for the family of SPOS algorithms,
considering both convex- and nonconvex-energy functions. Different from existing theory for SG-
MCMC based algorithms (Teh et al., 2016; Vollmer et al., 2016; Chen et al., 2015; Raginsky et al.,
2017; Zhang et al., 2017; Xu et al., 2018), our development relies on the theory of nonlinear SDEs,
which is more involved and less explored in literature. Particularly, we adopt tools from granular
media equations (Malrieu, 2003; Cattiaux et al., 2008) to develop nonasymptotic error bounds in
terms of the 1-Wasserstein distance. Please refer to Section M in the Supplementary Material (SM)
for detailed distinctions of our work to existing work. Within our theoretical framework, we provide a
formal theoretical understanding of a pitfall of SVGD, i.e., particles tend to collapse to one point under
some particular conditions; whereas this can be avoided in the proposed SPOS framework due to the
injected random noise. Our theory is general enough to be applied for various deep-learning tasks,
including Bayesian deep learning and Bayesian exploration in reinforcement learning. Our extensive
experimental results well suggest advantages of our framework compared to existing methods.
2	Preliminaries
This section introduces necessary preliminaries, along with notations used in this paper. For the sake
of clarity, through out the paper, we use bold letters to denote variables in continuous-time diffusions
and model definitions, e.g., θt in (1) defined below (indexed by “time” t). By contrast, normal unbold
letters are used to denote parameters in algorithms (discrete solutions of continuous-time diffusions),
e.g., θk(i) in (3) below (indexed by “iteration” k). For conciseness, all the proofs as well as some extra
experimental results are presented in the SM. Discussion on the complexity of our algorithm is also
included in Section L of the SM.
2.1	Stochastic gradient MCMC
In Bayesian sampling, We aim to generate random samples from a posterior distribution p(θ∣X) 8
p(X∣θ)p(θ), where θ ∈ Rd represents the model parameter with a prior distribution p(θ), and X，
{xq}NN=ι represents the observed data with likelihoodp(X∣θ) = QqP(Xq ∣θ). Define the potential en-
ergy as: U(θ) , - logp(X∣θ) - logp(θ) = - Pq=I (logP(Xq ∣θ) + N logp(θ))，Pq=I Uq(θ).
SG-MCMC algorithms belong to diffusion-based sampling methods, where a continuous-time diffu-
sion process is designed such that its stationary distribution matches the target posterior distribution.
The diffusion process is driven by a specific stochastic differential equation (SDE). For example, in
stochastic gradient Langevin dynamic (SGLD), the SDE endows the following form:
dθt = -β-1F(θt)dt + √2∕βdWt ,
(1)
where F(θ)，VθU(θ) = PN=I VθUq(θ)，PN=I Fq(θ); t is the time index; β > 0 is the
temperature parameter; and Wt ∈ Rd is a d-dimensional Brownian motion. More instances of SDEs
corresponding to other SG-MCMC algorithms can be defined by specifying different forms of F and
potentially other diffusion coefficients. We focus on SGLD and (1) in this paper, and refer interested
readers to (Ma et al., 2015) for more detailed description of general SG-MCMC algorithms. Denote
the probability density function of θt in (1) as νt, and a ∙ b，a> b for two vectors a and b. It is
known that νt is characterized by the following Fokker-Planck (FP) equation (Risken, 1989):
∂tνt = Vθ ∙ (β-1νtF(θ) + β-1VθVt).
(2)
where the stationary distribution ν∞ equals to our target distribution p(θ∣X) according to Chiang
& HWang (1987). SGLD generates samples fromp(θ∣X) by numerically solving the SDE (1). For
2
Under review as a conference paper at ICLR 2019
scalability, it replaces F (θk) in each iteration with an unbiased evaluation by randomly sampling
a subset of X, i.e. F(θk) is approximated by: Gk，BN Pq∈z^ Fq(θk), where Zk is a random
subset of [1, 2, ∙∙∙ , N] with size Bk in each iteration. Based on the above settings, SGLD uses the
EUler method with stepsize hk to numerically solve (1) and obtains the update equation: θk+ι =
θk - β-1Gkhk + P2β~1 hkξk, ξk 〜N(0,1).
2.2	Stein variational gradient descent
Different from SG-MCMC, SVGD is a deterministic particle-optimization algorithm that is able to
generate samples from a target distribution. In the algorithm, a set of particles interact with each
other, driving them to high density locations in the parameter space while keeping them far away
from each other with repulsive force. The update equations of the particles follow the fastest descent
direction of the KL-divergence between current empirical distribution of the particles and the target
distribution, on an RKHS induced by a kernel function κ(∙, ∙) (Liu & Wang, 2016). Formally, Liu &
Wang (2016) derived the following updating rules for the particles {θk(i) }iM=1 at the k-th iteration with
StePSiZe hk and Gki) , B Pq∈ik Fq (θki)):
M
θ(i) — θ(i) + k X hκ(θj) θ(i))G(i) + V r)κ(θj) θ(i))i ∀i	(3)
θk+1 = Uk + M 乙[κ(θk ,θk )Gk + vθ(j) κ(θk ,θk )_| , ∀i	(3)
j=1
where the first term in the bracket encourages particles to locate on high density modes, and the
second term serves as repulsive force that pushes away different particles. Different from SG-MCMC,
only particles at the current iteration, {θk(i)}iM=1, are used to approximate the target distribution.
2.3	Particle-optimization based sampling methods
SG-MCMC and SVGD, though look closely related, behave very differently in terms of algorithms,
e.g., stochastic and noninteractive versus deterministic and interactive particle updates. Recently,
Chen et al. (2018) proposed a deterministic particle-optimization framework that unifies both SG-
MCMC and SVGD. Specifically, the authors viewed both SG-MCMC and SVGD as Wasserstein
gradient flows (WGFs) on the space of probabilistic measures, and derived several deterministic
particle-optimization techniques for particle evolutions, like what SVGD does. For SG-MCMC, the
FP equation (2) for SGLD is a special type of WGFs. Together with an interpretation of SVGD as a
special case of the Vlasov equation in nonlinear PDE literature, Chen et al. (2018) proposed a general
form of PDE to characterize the evolution of the density for the model parameter θ, denoted as νt at
time t with ν∞ matching our target (posterior) distribution, i.e.,
∂tνt = Vθ ∙ (Vtβ-1F(θ) + Vt (K * νt(θ)) + β-1VθVt) ,	(4)
where K is a function controlling the interaction of particles in the PDE system. For example, in
SVGD, Chen et al. (2018) showed that K and K * Vt(θ) endow the following forms:
K(θ, θ0) , F(θ0)κ(θ0, θ) - Vθ0 κ(θ0, θ) and K
Vt (θ) ,
K(θ, θ0)Vt(θ0)dθ0
(5)
*
where κ(∙, ∙) is a kernel function such as the RBF kernel. In the following, we introduce a new unary
function K(θ) = exp(-kθk-), thus κ(θ, θ0) can be rewritten as κ(θ, θ0) = K(θ 一 θ0). Hence, (4)
with K defined in (5) can be equivalently rewritten as:
∂tVt = Vθ ∙ (VteTF(θ) + Vt (EY〜VtK(θ — Y)F(Y) 一 VK * %(θ)) + β-1VθVt) ,	(6)
where Y is a random sample from Vt but independent of θ. Importantly,
Proposition 1 (Chen et al. (2018)) The stationary distribution of (6) equals to our target distribu-
tion, which means v∞(Θ) = p(θ∣X).
Chen et al. (2018) proposed to solve (4) numerically with deterministic particle-optimization algo-
rithms such as the blob method. Specifically, the continuous density Vt is approximated by a set of
M particles {θ(i)}M=ι that evolve over time t, i.e. Vt ≈ MPiM=1δθ(i)(θ), where δθ(i)(θ) = 1 if
θ = θt(i) and 0 otherwise. Note VθVt in (4) is no longer a valid definition when adopting particle
approximation for Vt. Consequently, VθVt needs nontrivial approximations, e.g., by discrete gradient
flows or blob methods proposed in (Chen et al., 2018). We omit the details here for simplicity.
3
Under review as a conference paper at ICLR 2019
3	Stochastic Particle- Optimization Sampling Algorithms
The deterministic particle-approximation methods proposed by Chen et al. (2018) to approximately
solve the WGF problem (4) introduce approximation errors for Vθ Vt that are hard to control an-
alytically. To overcome this problem, we propose to solve (4) StochaStically to replace the VθVt
term with a Brownian motion. Specifically, first note that the term β-1Vθ ∙ VθVt is contributed
from Brownian motion, i.e., solving the SDE, dθt = √2β-1dWt, is equivalent to solving the
corresponding FP equation: ∂νt = β-1Vθ ∙ Vθνt. Consequently, We decompose RHS of (4) into
two parts: Fi，Vθ ∙ (VteTF (θt) + (K* Vt)Vt) and F2，β-1 Vθ ∙ Vθ νt Our idea is to solve Fi
deterministically under a PDE setting, and solve F2 stochastically based on its corresponding SDE.
When adopting particle approximation for the density Vt , both solutions of Fi and F2 are represented
in terms of particles {θt(i) }. Thus we can combine the solutions from the two parts directly to
approximate the original exact solution of (4). Similar to the results of SVGD in Section 3.3 in
(Liu, 2017), we first formally show in Theorem 2 that when approximating Vt with particles, i.e.,
Vt ≈ Mm PMi δθ(i) (θ), the PDE can be transformed into a system of deterministic differential
equations with interacting particles.
Theorem 2 When approximating Vt in (4) with particleS {θt(i)}i , the PDE ∂tVt = Fi reduceS to the
following SyStem of differential equationS deScribing evolutionS of the particleS over time: ∀i
1M	1M
dθ(i) = -β-1F(θ(i))dt — M X K(θ(i) — θj)F(θ j))dt + M X VK成-θj))dt	(7)
j=1	j=1
On the other hand, by solving ∂tVt = F2 stochastically with its equivalent SDE counterpart, we
arrive at the following differential equation system, describing evolutions of the particles {θt(i) } over
time t: ∀i
1 M	1 M	∖	____
dθ(i) = -β-1F(θ(i)) — M X K(θ(i) — θ j))F(θj + M X VK(θ(i) — θ j)) dt + p2β-τdW(i)
j=1	j=1
Our intuition is that if
the particle evolution
(8) can be solved ex-
actly, the solution of
(6) Vt will be well-
approximated by the
particles {θ(i)}M=ι. In
our theory, we show
this intuition is actu-
Figure 1: Comparison of SPOS (left) and SVGD (right) on a multi-mode
distribution. The circles with different colors are the resulting 100 particles,
which are able to spread over all modes for SPOS.
ally true. In practice, however, solving (8) is typically infeasible, and thus numerical methods
are adopted. Furthermore, in the case of big data, following SG-MCMC, F(θk(i)) is typically replaced
by a stochastic version Gki)，BN Pq1∈ιk Fq(°ki)) evaluated with a minibatch of data of size Bk for
computational feasibility. Based on the Euler method (Chen et al., 2015) with a stepsize hk, (8) leads
to the following updates for the particles at the k-th iteration
θk+i = θki)-hkβ-1Gki) - M XK(θki) - θkj))Gkj)
h M	________
+ M X VK(θki) - θj) + p2β-1hkξki) 总i 〜N(0, I) ∀i.	(9)
j=i
We called the algorithm with particle update equations (9) stochastic particle-optimization sampling
(described in Algorithm 3), in the sense that particles are optimized stochastically with extra random
Gaussian noise. Intuitively, the added Gaussian noise would enhance the ability of the algorithm to
jump out of local modes, leading to better ergodic properties compared to standard SVGD. This serves
as one of our motivations to generalize SVGD to SPOS. To illustrate the advantage of introducing the
noise term, we compare SPOS and SVGD on sampling a difficult multi-mode distribution, with the
density function given in Section A of the SM. The particles are initialized on a local mode close to
zero. Figure 1 plots the final locations of the particles along with the true density, which shows that
particles in SPOS are able to reach different modes, while they are all trapped at one mode in SVGD.
This pitfall of SVGD will be studied formally in Section 4.4.
4
Under review as a conference paper at ICLR 2019
4 Non-Asymptotic Convergence Analysis: the Convex Case
In this section, we develop non-
asymptotic convergence theory
for the proposed SPOS when the
energy function U (θ) is convex.
The nonconvex case is discussed
in Section 5. We prove non-
asymptotic convergence rates for
SPOS algorithm under the 1-
Wasserstein metric W1, a special
Algorithm 1 Stochastic Particle-Optimization Sampling
Input: Initial particles {θ0(i) }iM=1 with θ0(i) ∈ Rd, step size hk,
batch size Bk
1:	for iteration k= 0,1,...,T do
2:	Update θk(i+) 1 with (9) for ∀i.
3:	end for
Output: {θ(i)}Mι
case of p-Wasserstein metric de-
fined as Wp(μ,ν) = (inf ζ∈r(μ,ν) RRd × Rd ∣∣Xμ - XV kpdZ(Xμ,Xν)) p, where Γ(μ, ν) is the set of joint
distributions on Rd X Rd with marginal distribution μ and ν. Note that SPOS reduces to SVGD
when β → ∞, thus our theory also sheds light on the convergence behavior of SVGD, where
non-asymptotic theory is currently missing, despite the asymptotic theory developed recently (Liu,
2017; Lu et al., 2018). It is worth noting that part of our proofs are generalization of techniques for
analyzing granular media equations in (Malrieu, 2003; Cattiaux et al., 2008).
4.1	Basic setup and assumptions
Due to the exchangeability of the particle system {θt(i)}iM=1 in (8), if we initialize all the particles θt(i)
with the same distribution ρ0, they would endow the same distribution for each time t. We denote
the distribution of each θt(i) as ρt. Similar arguments hold for the particle system {θk(i)}iM=1 in (9),
(i)
and thus We denote the distribution of each θk ' as μk. To this end, our analysis aims at bounding
Wi (μτ, ν∞) since ν∞ equals to our target distribution p(θ∣X) according to Proposition 1. Before
proceeding to our theoretical results, we first present the following basic assumptions.
Assumption 1 Assume F and K satisfy the following conditions:
•	There exist positive mF and mK, such that hF (θ) - F(θ0), θ - θ0i ≥ mF kθ - θ0k2 and
NK(θ) — VK(θ0), θ — θ0i≤ -mκkθ — θ0k2.
•	F is bounded by HF and LF -Lipschitz continuous i.e., kF (θ)k ≤ HF and kF(θ1) -
F(θ2)k ≤LFkθ1-θ2k;.
•	K is LK -Lipschitz continuous; VK is bounded by HyK and LyK -Lipschitz continuous
•	F(0) = 0 and K is an even function, i.e., K(-θ) = K (θ).
Note the first bullet indicates U to be a convex function and K to be a concave function. For an RBF
kernel, the later could be achieved by setting the bandwidth large enough and only considering the
concave region for simplicity. This assumption is used for revealing some undesired property of
SVGD developed below. We do not need such an assumption when analyzing under a nonconvex
energy function U in Section 5. Then "F (0) = 0" in the second bullet is reasonable, as F in our
setting corresponds to an unnormalized log-posterior, which can be shifted such that F(0) = 0 for a
specific problem. Since we often care about bounded space in practice, we can realize the third bullet
due to the continuity of K and VK.
The high-level idea of bounding Wι(μτ, ν∞) in this section is to decompose it as follows:
W1(μτ,ν∞) ≤ W1(μτ, ρPT-01 hk) + WI(PPT-01 hk,νPT-01 hk) + WI(VPT-01 hk,ν∞). (IO)
4.2	B ounds with stochastic particle approximation
WefirstlyboundW1(ρPkT=-01hk,νPkT=-01hk)andW1(νPkT=-01hk,ν∞)
with the following theorems.
Theorem 3 Under Assumption 1 and let ρ0 = ν0, there exist some positive constants c1 and c2
independent of (M, t) and satisfying c2 < β-1 such that
W1(ρt, νt) ≤ c1(β-1 - c2)-1M-1/2,	∀t.
(11)
5
Under review as a conference paper at ICLR 2019
Remark 1 According to Theorem 3, we can bound the W1 (ρPT -1 h , νPT -1 h ) term as
Wι(ρPτ-i hfc,νPτ-ι hfc) ≤ √^(j-i C). Furthermore, by letting t → ∞, we have Wι(ρ∞,ν∞) ≤
√M(β-ι-c), which is an important intermediate result to prove the following theorem.
Theorem 4 Under Assumption 1, the following holds: W1(νt, ν∞) ≤ c3e-2λ1t, where λ1 =
β-1mF - 3HF LK - 2LF and c3 is some positive constant independent of (M, t). Hence, the
W1(νPkT=0 hk, ν∞) term in (10) can be bounded as:
W1(νPT-1 hk, ν∞) ≤ c3 exp -2λ1(X hk)	.	(12)
=	k=0
To ensure W1(νPT-1 h , ν∞) to decrease over time, one needs to choose β small enough such that
λ1 > 0. This also sheds light on a failure case of SVGD (where β → ∞) discussed in Section 4.4.
4.3	B ounds with a numerical solution
To bound the Wι(μτ, PPT-1 联)term, We adopt techniques from (Raginsky et al., 2017; XU et al.,
2018) on analyzing the beha=viors of SGLD, and derive the following results for our SPOS algorithm:
Theorem 5 Under Assumptions 1, for a fixed step size hk = h that is small enough, the correspond-
ing Wι(μτ, PTh) is bounded as
Wι(μτ, PTh) ≤ c4Md2β-3(c5β2B-1 + C6h)1T1 h1	(13)
where B is the fixed size of the minibatch in each iteration and (c4, c5, c6) are some positive constants
independent of (M, T, h).
The dependence of T in the bound above makes the bound relatively loose. Fortunately, We can
improve the bound to make it independent of T by considering a decreasing-stepsize SPOS algorithm,
stated in Theorem 6.
Theorem 6 Under Assumptions 1, fora decreasing step size hk = h0/(k + 1), and let the minibatch
size in each iteration k be Bk = Bo + [log(k + 1)]100/99 ,the corresponding Wι(μτ ,ρPτ-ι h)
k=0 k
term is bounded, for some β small enough, as
Wι(μτ,PPT-1hk) ≤ c4β-3Md2 (c7h3 + C8β3ho∕Bo + C9h0β2)1/2 ,	(14)
where B0 is the initial minibatch size, and (c4, c7, c8, c9) are some positive constants independent of
(M,T,h0).
Note Bk increases at a very loW speed, e.g., only by 15 after 105 iterations, thus it Would not
affect algorithm efficiency. According to Theorem 6, Wι(μτ, PPT-1 hɛ) would approach zero when
1/2	k=0
ho M → 0. By directly combining results from Theorem 3-6, one can easily bound the target
Wi (μτ, ν∞). Detailed statements are given in Theorem 15-16 in Section H of the SM.
4.4	A Pitfall of SVGD
Based on the above analysis, we now formally show a pitfall of SVGD, i.e., particles in SVGD tend
to collapse to a local mode under some particular conditions. Inspired by the work on analyzing the
granular media equations by Malrieu (2003); Cattiaux et al. (2008), we measure this by calculating
the expected distance between particles, called expected particle distance (EPD). Firstly, we bound
the EPD for the proposed SPOS algorithm in Theorem 7.
Theorem 7 Under Assumption 1, further assuming every {θt(i)} with the same initial probability
law ρo andΓ，Eθ〜p。#〜ρ° [∣∣θ — θ0∣∣2] < ∞. Choose a β such that λ = mF + mκ 一 HFLK > 0.
Then the EPD OfSPOS is bounded as: EPD，,PMj E∣∣θ(i) — θ(j)k2 ≤ Cιe-2λt + 4qdM,
where Ci = M(M — 1)Γ — 4pβ- M.
Remark 2 There are two interesting cases: i) When C1 > 0, the EPD would decrease to the bound
4，dβ-1M∕λ along time t. This represents the phenomenon ofan attraction force between particles;
6
Under review as a conference paper at ICLR 2019
ii) When C1 < 0, the EPD would increase to the same bound, which represents the phenomenon of a
repulsive force between particles, e.g., when particles are initialized with the same value (Γ = 0),
they would be pushed away from each other until the EPD increases to the aforementioned bound.
Intuitively, the EPD for SVGD can be obtained by taking the β → ∞ limit. Corollary 8 formally
characterizes the particle-degeneracy phenomenon of SVGD, which has been empirically studied in
(Zhuo et al., 2018).
Corollary 8 Under the same conditions of Theorem 7, the EPD in SVGD is bounded as: EPD，
√PM- kθ(i) — θ(j)k2 ≤ Coe-2λt, where Co = JPMk θ0i) - θ0j)k2 and λ = mκ - HF LK.
Remark 3 We would like to emphasize two points: 1) In the case of λ ≥ 0, Corollary 8 indicates
that particles in SVGD would collapse to a point when t → ∞. In practice, we usually find that
particles are trapped in a local mode instead of collapsing. This is due to two reasons: i) numerical
errors inject noise into the particles; ii) some particles are out of the concave region of K stated
in Assumption 1 in SVGD, which is required for the theory to hold. All these make the empirical
EPD not exactly the same as the true particle distance. 2) Corollary 8 also applies when the energy
function is nonconvex. Our proof in the SM considers the nonconvex case as well. Consequently, this
serves as a strong theoretical motivation to apply SPOS instead of SVGD in deep learning.
5 Non-Asymptotic Convergence Analysis: the Nonconvex Case
Since the non-convex case is much more complicated than the convex case, we reply on dif-
ferent assumptions and adopt another distance metric, denoted as B, to characterize the con-
vergence behavior of SPOS under the non-convex case. Specifically, B(μ,ν) is defined as
B(μ, V)，∣Eθ〜μ[f (θ)] - Eθ〜ν[f (θ)]∣ for a known Lf -continuous function f satisfying AssUmP-
tion 2 below. Note such metric has also been adopted in (Vollmer et al., 2016; Chen et al., 2015). Our
analysis considers (T, M, hk) as variables in B. In addition, we use {θki)}M=ι to denote the particles
when full gradients are adopted in (9). The distribution of the particles is denoted as μk.
Our high-level idea of bounding B(μT, ν∞) is to decompose it as follows:
~ , , ~ , . ~ , . . ~ , . . ~ , ....
B(μτ,V∞) ≤ B(μτ,μτ) + B(μτ,μ∞) + B(μ∞,ρ∞) + B(ρ∞,ν∞)	(15)
Our second idea is to concatenate the particles at each time into a single vector representation, i.e.
defining the new parameter at time t as Θt，[θ(1), ∙∙∙ , θ[M)] ∈ RMd. Consequently, the nonlinear
SDE system (8) can be turned into a linear SDE ,which means Θt is driven by the following linear
SDE:
dΘt = -Fθ (Θt)dt + P2β-1dW(Md) ,	(16)
where	Fθ(Θt)	，	[β-1 F(θ(1))	-	M PM VK(θ(1)	-	θ(j))	+ M £*	K(θ(1)	-
θj))F(θ(j)),…,β-1F(θ(M)) - M PM VK(θ(M) -θ(j)) + M PM K(θ(M) -θ(j))F(θ(j))]
is a vector function RMd → RMd, and Wt(M d) is Brownian motion of dimension Md. Similarly,
we can define Θk，帆1),…，θkM)] ∈ RMd for the full-gradient case. Hence, it can be seen that
through such a decomposition in (15), the bound related to a nonlinear SDE system (8) reduces to that
of a linear SDE. The second term B(μτ, μ∞) reflexes the geometric ergodicity of a linear dynamic
system with a numerical method. It is known that even if a dynamic system has an exponential
convergence rate to its equilibrium, its corresponding numerical method might not. Our bound for
B(μτ, μ∞) is essentially a specification of the result of Mattingly et al. (2002), which has also been
applied by XU et al. (2018). The third term B(μ∞,ρ∞) reflects the numerical error of a linear SDE,
which has been studied in related literature such as (Chen et al., 2015). To this end, we adopt standard
assumptions used in the analysis of SDEs (Vollmer et al., 2016; Chen et al., 2015), rephrased in
Assumption 2.
Assumption 2 (Assumption in (Vollmer et al., 2016; Chen et al., 2015)) For the linear SDE (16)
and a Lipschitz function f, let ψ be the solution functional of the Poisson equation: Gψ(Θk) =
7
Under review as a conference paper at ICLR 2019
MM PMIf (θf)) — Eθ〜p(θ∣D)[f (θ)], where G denotes the infinite generator of the linear SDE
(16). Assume ψ and its up to 4th-order derivatives, Dkψ, are bounded by a function V, i.e.,
kDkψk ≤ Hk Vpk for k = (0, 1, 2, 3, 4), Hk, pk > 0. Furthermore, the expectation of V on
{Θt} is bounded: supl EVp(Θt) < ∞, and V is smooth such that sups∈(0,1) Vp (sΘ + (1 - s) Θ0) ≤
H (Vp (Θ) + Vp (Θ0)), ∀Θ, Θ0, p ≤ max{2pk} for H > 0.	,
Assumption 3 i) F, K and VK are LF, LK and Lyk Lipschitz; ii) F satisfies the dissipative
property, i.e., hF(θ), θi ≥ m kθk2 — bfor some m, b > 0; iii) Remark 1 applies to the nonconvex
setting, i.e. SupkfkLip≤ι ∣Eθ〜μ∞[f(θ)] - Eθ"∞[f(θ)]∣ = Wι(ρ∞,ν∞) = O(M-1/2).
Remark 4 Assumption 2 is necessary to control the gap between a numerical solution and the exact
solution of an SDE. Specifically, it IS used to bound the B(μ∞, ρ∞) term and the B(μτ, μτ) term
above. Purely relying on the dissipative assumption in Assumption 3 as in non-convex optimization
with SG-MCMC (Raginsky et al., 2017; Xu et al., 2018) would induce a bound increasing linearly
w.r.t. time t. Thus it is not suitable for our goal. Finally, iii) in Assumption 3 is a mild condition and
reasonable because we expect particles to be able to approximate all distributions equally well in
the asymptotic limit of t → ∞ by ergodicity due to the injected noise. How to remove/replace this
assumption is an interesting future work.
Based on the assumptions above, the bounds for B(μτ, μ∞) and B(μ∞, ρ∞) are summarized below.
Theorem 9 Under Assumption 2-3, if we set the stepsize hk = h, we can have the following results:
B(μτ,μ∞) ≤ Cqσ-Mdd/Q + ςemθh)exp (_2m8ThσMd/ log(ς))	(17)
B(μ∞,p∞) ≤ C3h∕β,	(18)
where ς = 2Lθ (Mbe + m®β + Md)∕mθ, Lθ = √2β-1Lf + l0, m® = β-1m — m0, and
(σ, C2, C3, l0, m0) are some positive constants independent of (T, M, h) and σ ∈ (0, 1)
Remark 5 It is seen that in order to make the B(μτ, μ∞) term asymptotically decrease to zero, the
number of running iteration T should increase at a rate faster enough to compensate the effect of
increasing M. We believe there is room for improving this bound, which is an interesting future work.
Next we bound the B(μτ, μτ) term related to stochastic gradients. By adapting results from linear
SDE (XU et al., 2018), B(μτ, μτ) can be bounded with Theorem 10.
Theorem 10 Under Assumptions 2-3, ifwe set Bk = B and hk = h, B(μτ, μτ) is bounded as
B(μτ, μτ) ≤ C5Th(LθΓ0 + MC4)((6 + 2Γ0)β∕(BM))1/2 .
where Γ0 = 2(1 + 1∕mΘ)(Mb + 2M2C42 + Md∕β) and , (C4, C5) is some positive constant
independent of (T, M, h)
Finally, by combining the results from Theorem 9, 10 and iii) in Assumption 3, we arrive at a bound
for our target B(μT, ν∞), summarized in Theorem 11.
Theorem 11 Under Assumptions 2-3, there exist some positive constants (C2, C3, C4, C5 , C6) such
that: B(μτ, ν∞) ≤ C2ςσ-Md/2(1 + ςemθh) exp (—2m&ThσMd/log(ς))
+ C3h∕β + C5Th(LθΓ0 + MC4) ((6 + 2Γ0)β∕(BM))1/2 + C6MT/2 ,
where σ, ς and Γ0 are the same as those in Theorem 9-10.
6	Experiments
In this section, we illustrate the effectiveness of our proposed Bayesian sampling framework on
several deep-learning models, including Bayesian learning of deep neural network and Bayesian
exploration in deep reinforcement learning. We start with a simple illustrative example.
6.1	B ounds illustration with a simple Gaussian example
8
Under review as a conference paper at ICLR 2019
We follow Chen et al. (2015) and consider
a standard Gaussian model where Xi 〜
N(θ, 1),θ 〜N(0,1). 1000 data samples
{xi } are generated, and every minibatch in
the stochastic gradient is of size 10. The
test function is defined as f (θ) , θ2, with
explicit expression for the posterior average.
To evaluate the expectations in the bias and
MSE, we average over 200 runs with random
initializations. The estimation errors are plot-
ted in Figure 2. It is observed that at the be-
ginning, the errors for the ones with less par-
ticles decrease faster than those with more
particles. This is reflected in the bounds in Figure 2: The estimation errors with different particles.
Theorem 15, which are dominated by the bound in Theorem 5 (indicating larger M results in larger
errors at the beginning). When more running time/iterations are given, the increase of error in
Theorem 5 by increasing M essentially cancels out the exponentially-decay term in Theorem 4.
According to Theorem 3, the error would eventually decrease with increasing number of particles.
6.2 Bayesian neural networks for regression
We conduct experiments for Bayesian learning of DNNs, where we Bayesian DNNs are used to model weight uncer- tainty of neural networks, an important topic that has been well explored (Herndndez- Lobato & Adams, 2015; Blun- dell et al., 2015; Li et al., 2016; Louizos & Welling, 2016). We assign simple isotropic Gaussian priors to	Table 1: Averaged predictions with standard deviations in terms of RMSE and log-likelihood on test sets.				
	Dataset	Test RMSE SVGD	SPOS		Test Log likelihood SVGD	SPOS	
	Boston_Housing Concrete Energy Kin8nm (0.4) Naval (0.4) CCPP Winequality Yacht (0.4) Protein YearPredict	2.961 ± 0.109 6.157 ± 0.082 1.291 ± 0.029 0.075 ± 0.001 0.004 ± 0.000 4.127 ± 0.027 0.604 ± 0.007 1.597 ± 0.099 4.392 ± 0.015 8.684 ± NA	2.829 ± 0.126 5.071 ± 0.1495 0.752 ± 0.0285 0.079 ± 0.001 0.004 ± 0.000 3.939 ± 0.0495 0.598 ± 0.014 0.84 ± 0.0865 4.254 ± 0.005 8.681 ± NA	-2.591 ± 0.029 -3.247 ± 0.01 -1.534 ± 0.026 1.138 ± 0.004 4.032 ± 0.008 -2.843 ± 0.006 -0.926 ± 0.009 -1.818 ± 0.06 -2.905 ± 0.010 -3.580 ± NA	-2.532 ± 0.082 -3.062 ± 0.037 -1.158 ± 0.073 1.092 ± 0.013 4.145 ± 0.02 -2.794 ± 0.025 -0.911 ± 0.041 -1.446 ± 0.121 -2.876 ± 0.009 -3.576 ± NA
					
the weights, and perform posterior sampling with different methods. For all methods, we use a
RBF kernel K(θ, θ0) = exp(-∣∣θ - θ0∣∣2∕η2), with the bandwidth set to η = med2/ log M. Here
med is the median of the pairwise distance between particles. We use a single-layer BNN for regres-
sion tasks. Following Li et al. (2015), 10 UCI public datasets are considered: 100 hidden units for 2
large datasets (Protein and YearPredict), and 50 hidden units for the other 8 small datasets. Following
Zhang et al. (2018b), we repeat the experiments 20 times with batchsize 100 for all datasets except
for Protein and YearPredict, which we repeat 5 times and once with batchsize 1000. The datasets are
randomly split into 90% training and 10% testing. For a fair comparison, we use the same split of
data (train, val and test) for SVGD and SPOS. The test results are reported on the best model on the
validation set. We adopt the root mean squared error (RMSE) and test log-likelihood as the evaluation
criteria. The experimental results are shown in Table 1, from where we can see the proposed SPOS
outperforms other methods, achieving state-of-the-art results.
6.3	Bayesian exploration in deep reinforcement learning
it is well-accepted that RL performance directly measures how well the uncertainty is learned ,
reflected by the exploration stage. As a result, we apply our method for RL. Following Liu et al.
(2017); Zhang et al. (2018a), we define policies in RL with Bayesian neural networks. This naturally
introduces uncertainty into action selections under a specific state-action pair, rendering Bayesian
explorations to make policy learning more effective.
Specifically, denote the policy as πθ(a | s) parameterized by θ with prior distribution p(θ), where a
represent the action variable, and s the state variable. According to Liu et al. (2017), learning the
optimal policy corresponds to calculating the following posterior distribution for θ:
q(θ) (X exp (1 J(θ)) p(θ),	(19)
9
Under review as a conference paper at ICLR 2019
OOOA
105 -
PJBMSa 8βυ∙laΛ4
Cartpde Swing Up (AzC)
2∞	4∞	6∞	800	1000
Episodes
PJBMSa 8βυ∙laΛ4
Double Pendulum ∣A2C)
SVPG
SPOS-PG
0	20	40	60	80	100
Episodes
300
250
200
150
100
,50
-50
-100
Cartpole Swing Up
——SVPG
—SPOS-PG
0	200	400	6∞	800	1000
Episodes
PJBMSa ωCTa⅛><
-1200
0	200	400	6∞	800	10∞
Episodes
Figure 3: Policy learning with Bayesian exploration in policy-gradient methods on six scenarios with
SVPG and SPOS-PG.
where J(θ) denotes the expected cumulative reward under the policy with parameter θ and α a
hyperparameter. Consequently, θ could be updated by drawing samples from (19) with the proposed
SPOS. We denote this method as SPOS-PG. In addition, when drawing samples with SVGD, the
resulting algorithm is called Stein variational policy gradient (SVPG) (Liu et al., 2017). Note
in implementation, the term J(θ) can be approximated with REINFORCE (Williams, 1992) or
advantage actor critic (Schulman et al., 2015), which we will investigate in our experiments.
We follow the same setting as in (Liu et al., 2017) except using simpler policy-network architectures
as Houthooft et al. (2016). We conduct experiments on three classical continuous control tasks
are considered: Cartpole Swing-Up, Double Pendulum, and Cartpole. Specifically, the policy is
parameterized as a two-layer (25-10 hidden units) neural network with tanh as the activation function.
The maximal length of horizon is set to 500. We use a sample size of 10000 for policy gradient
estimation, and M = 16, α = 10. For the simplest task, Cartpole, all agents are trained for 100
episodes. For the other two complex tasks, all agents are trained up to 1000 episodes. The average
reward versus number of episodes are plotted in Figure 3. It is observed that our proposed SPOS-PG
obtains much larger average rewards as well as smaller variance compared to SVPG, though the
convergence behaviors are similar in the simplest task Carpole.
7	Conclusion
Motivated by the need of effective and efficient Bayesian sampling techniques in modern deep
learning, we propose a probability approach for particle-optimization-based sampling that unifies
SG-MCMC and SVGD. Notably, for the first time, by analyzing the corresponding nonlinear SDE,
we develop non-asymptotic convergence theory for the proposed SPOS framework, a missing yet
important theoretical result since the development of SVGD. Within our theoretical framework, a
pitfall of SVGD, which has been studied empirically (Zhuo et al., 2018), is formally analyzed. Our
theory also indicates the convergence of SPOS to the true posterior distribution in the asymptotic limit
of infinite particles and iterations under appropriate conditions. Our theory is of great practice value,
as for the first time it provides nonasymptotic theoretical guarantees for the recently proposed particle-
optimization-based algorithms such as the SVGD, whose advantages have also been extensively
evaluated by experiments on Bayesian learning of DNNs and Bayesian exploration of DRL. There
are a number of interesting future works. For example, one might explore more recently developed
techniques such as (Cheng et al., 2018) to improve the convergence bound; one can also adopt
the SPOS framework for non-convex optimization like what SG-MCMC is used for, and develop
corresponding theory to study the convergence property of the algorithm to the global optimum.
10
Under review as a conference paper at ICLR 2019
References
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks.
In ICML, 2015.
F. Bolley and C. Villani. Weighted Csiszdr-KUllback-Pinsker inequalities and applications to trans-
portation inequalities. Annales de la Facultedes sciences de Toulouse : Mathematiques, 14(6):
331-352, 2005.
J. A. Carrillo, K. Craig, and F. S. Patacchini. A blob method for diffusion. (arXiv:1709.09195), 2017.
P. Cattiaux, A. Guillin, and F. Malrieu. Probabilistic approach for granular media equations in the
non-uniformly convex case. Probability Theory and Related Fields, 140(1-2):19-40, 2008.
C. Chen, N. Ding, and L. Carin. On the convergence of stochastic gradient MCMC algorithms with
high-order integrators. In Neural Information Processing Systems (NIPS), 2015.
C. Chen, R. Zhang, W. Wang, B. Li, and L. Chen. A unified particle-optimization framework for
scalable Bayesian sampling. In UAI, 2018.
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In International
Conference on Machine Learning (ICML), 2014.
X. Cheng, N. S. Chatterji, Y. Abbasi-Yadkori, P. L. Bartlett, and M. I. Jordan. Sharp convergence
rates for Langevin dynamics in the nonconvex setting. In arXiv:1805.01648, 2018.
Tzuu-Shuh Chiang and Chii-Ruey Hwang. Diffusion for global optimization in rn. SIAM J. Control
Optim., 25(3):737-753, May 1987. ISSN 0363-0129. doi: 10.1137/0325042. URL http:
//dx.doi.org/10.1137/0325042.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using
stochastic gradient thermostats. In Neural Information Processing Systems (NIPS), 2014.
A. Durmus, A. Eberle, A. Guillin, and R. Zimmer. An Elementary Approach To Uniform In Time
Propagation Of Chaos. ArXiv e-prints, May 2018.
A. Durmus, A. Eberle, A. Guillin, and R. Zimmer. An elementary approach to uniform in time
propagation of chaos. In arXiv:1805.11387, 2018.
Y. Feng, D. Wang, and Q. Liu. Learning to draw samples with amortized stein variational gradient
descent. In UAI, 2017.
C. R. Givens and R. M. Shortt. A class of wasserstein metrics for probability distributions. Michigan
Math. J., 31, 1984.
T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based
policies. In ICML, 2017.
J. M. Herndndez-Lobato and R. P. Adams. Probabilistic backpropagation for scalable learning of
Bayesian neural networks. In ICML, 2015.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME:
Variational information maximizing exploration. In NIPS, 2016.
C. Li, C. Chen, D. Carlson, and L. Carin. Preconditioned stochastic gradient Langevin dynamics for
deep neural networks. In AAAI, 2016.
Y. Li, J. Herndndez-Lobato, and R. E. Turner. Stochastic expectation propagation. In NIPS, 2015.
C. Liu and J. Zhu. Riemannian Stein variational gradient descent for Bayesian inference. In AAAI,
2018.
Q. Liu. Stein variational gradient descent as gradient flow. In NIPS, 2017.
Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. In Neural Information Processing Systems (NIPS), 2016.
11
Under review as a conference paper at ICLR 2019
Y. Liu, P. Ramachandran, Q. Liu, and J. Peng. Stein variational policy gradient. In UAI, 2017.
C. Louizos and M. Welling. Structured and efficient variational deep learning with matrix Gaussian
posteriors. In ICML, 2016.
J. Lu, Y. Lu, and J. Nolen. Scaling limit of the Stein variational gradient descent part I: the mean
field regime. In arXiv:1805.04035, 2018.
Y. A. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient MCMC. In NIPS, 2015.
F. Malrieu. Convergence to equilibrium granular media equations and their euler schemes. The
AnnnalsofAPPUedProbabilily,13(2):540-560, 2003.
J. C. Mattingly, A. M. Stuartb, and D. J. Higham. Ergodicity for SDEs and approximations: locally
Lipschitz vector fields and degenerate noise. Stochastic Processes and their APPlications, 101(2):
185-232, 2002.
M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin
dynamics: a nonasymptotic analysis. In COLT, 2017.
D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. In ICML, 2015.
H. Risken. The Fokker-Planck equation. Springer-Verlag, New York, 1989.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv PrePrint arXiv:1506.02438,
2015.
Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and fluctuations for stochastic gradient
Langevin dynamics. JMLR, 17(1):193-225, 2016.
C. Villani. OPtimal transPort: old and new. Springer Science & Business Media, 2008.
S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (exploration of the (Non-)asymptotic bias and variance
of stochastic gradient Langevin dynamics. JMLR, 1:1-48, 2016.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML,
2011.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 1992.
P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of Langevin dynamics based algorithms for
nonconvex optimization. In NIPS, 2018.
R. Zhang, C. Chen, C. Li, and L. Carin. Policy optimization as wasserstein gradient flows. In ICML,
2018a.
R. Zhang, C. Li, C. Chen, and L. Carin. Learning structural weight uncertainty for sequential
decision-making. In AISTATS, 2018b.
Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient Langevin dynamics.
In COLT, 2017.
J. Zhuo, C. Liu, J. Shi, J. Zhu, N. Chen, and B. Zhang. Message passing stein variational gradient
descent. In ICML, 2018.
12
Under review as a conference paper at ICLR 2019
A Density Function of the Multi-Mode Distribution in Section 3
The negative log-density function of the multi-mode distribution in Section 3 is defined as:
U(θ)，e3θ2-3 P1=Ici sin(1 πi(θ+4)),
where c = (-0.47, -0.83, -0.71, -0.02, 0.24, 0.01, 0.27, -0.37, 0.87, -0.37) is a vector, ci is the
i-th element of c.
B Gronwall Lemma
The Gronwall Lemma plays an important role in parts of our proofs, which is stated in Lemma 12.
Lemma 12 (Gronwall Lemma) LetI denotes an interval of the form [a, +∞) for some a ∈ R. If
v(t), defined on I, is differentiable in I and satisfies the following inequality:
v0(t) ≤ β(t)v(t) ,
where β(t) is a real-value continuous function defined on I. Then v(t) can be bounded as:
v (t) ≤ v
(a) exp(
a
β(s)ds)
C Proof of Theorem 2
To prove Theorem 2, we rely on the definition of generalized derivative in Definition 1.
Definition 1 (Generalized Derivative) Let g and φ be locally integrable functions on an open set
Ω ⊂ Rd, that is, Lebesgue integrable on any closed bounded set F ⊂ ω. Then φ is the generalized
derivative of g with respect to θ on Ω, written as φ = ∂θ g, if for any infinitely-differentiablefunction
U with compact support in Ω, we have
j
Jω
g(θ)∂θu(θ)dθ
-
Jω
φ(θ)u(θ)dθ .
Proof The proof relies on further expansions on the definition of generalized derivative on specific
functions. Specifically, let the function g in Definition 1 be in a form of g , Gf for two functions G
and f. The generalized derivative of (Gf)(v, t), written as ∂θ (Gf)(θ, t), satisfies
∂θ (Gf)(θ, t)u(θ)dθ
Gf(θ, t)∂θu(θ)dθ
(20)
for all differentiable function u(∙).
In Theorem 2, we want to prove a particle representation of the following PDE:
∂tμt = Fi = -Vθ ∙ (μtF(θt) + (K * μt)μt)，-∂θ(Gf )(θ, t)
⇒
/ ∂tμtu(θ)dθ
∂θ(Gf)(θ,t)u(θ)dθ,
where f (θ,t) = μt. Consequently, We have
∂tf(θ, t)u(θ)dθ
∂θ(Gf)(θ, t)u(θ)dθ
(21)
By applying (20) in (21), we have
∂tf(θ, t)u(θ)dθ
∂θ(Gf)(θ,t)u(θ)dθ
Gf(θ, t)∂θu(θ)dθ .
13
Under review as a conference paper at ICLR 2019
Since fdθ，μ(dθ, t), We have
J ∂tμ(dθ,t)u(θ) = J G ∙ μ(dθ,t)∂θu(θ)
⇒^ J μ(dθ, t)u(θ) = J G ∙ μ(dθ,t)∂θu(θ)
⇒dtEμ(t) [u(θ)] = Eμ(t) [G ∙ dθU(O)] .
(22)
In particle approximation, We have μ(t)= 吉 PM=I 6(e(i))(。). For each particle, let u(θ) = θ, (22)
reduces to the folloWing equation:
dθt(i) = G(θt(i))dt .
This completes the proof.
D Proof of Theorem 3
Note that one challenge in our analysis compared With the analysis for linear SDEs, such as those for
SG-MCMC Vollmer et al. (2016); Chen et al. (2015), is hoW to bound the gap betWeen the original
nonlinear PDE (4) and the reduced nonlinear SDE (8). Based on the techniques on analyzing granular
media equations in Malrieu (2003); Cattiaux et al. (2008); Durmus et al. (2018), We introduce a
nonlinear SDE as an element in-betWeen (6) and (8) like :
ʃdθt = -β-1F(θt) - EY〜VtK(θt- Y)F(Y) + VK * νt(θt) + P2β-1dWt
[L(θt) = νtdθ
(23)
where L(θt) denotes the probability law of θt, Wt ∈ Rd is a d-dimensional Brownian motion and Y
is a random variable independent of θt andjust used here for the sake of clarity. In order to match the
SDE system (8) of the particles {θt(i)}iM=1 , we duplicate (23) M times, each endowing with an exact
solution θ(i) indexed by i. The distribution of each particles {^^(i)}M=1 is Vt and the corresponding
WF) can be set exactly the same as the Wf) in (8):
[dθ(i) = -β-1F(θ(i)) - EY…tK(θ(i) - Yi)Fx) + VK * Vt(Ai)) + P2β-1 dWF
[L(θ(i)) = νtdθ
(24)
where Yi is a random variable independent of θ(i) and just used here for the convenience of the proof.
Proof [Proof of Theorem 3] Firstly we have
1M
d (θ(i) - θ(i)) = — β-1 (F (θ(i)) — F (θ(i))) dt + 而 X [VK (θ(i) - θj) - VK * Vt (国 iR dt
j
1M
-IM X (F (θj))W (θ(i) - θj - Eγ~t F (Y )W 就i - Y)) dt
j
⇒d
(25)
where Aj(t) = -β-1 (F(θ(i)) - F(θ(i))) ∙ (θ(i) -欧))
Bij (t) =(VK(θ(i) - θj)) - VK(θ(i) - θj))) ∙(
—
14
Under review as a conference paper at ICLR 2019
Cij⑴=(vκ(θ(i) - θ(j)) - VK * νt(θ(iR ∙ (θ(i) -欧))
Fij (t) = -(F(θ(j))K(θ(i) - θ(j)) - F(θ(j))K(θ(i) - θ(j))) ∙ W- θ(i))
Gij (t) = -(F(θ(j))K(θ(i) - θ(j)) - F(θ(j))K(国i) - θ(j))) ∙ (θ(i) - θ(i))
Hij⑻=-(F间j))K(θ(i) - θ(j)) - Eγ"tF(Yj)K(θ(i) - Yj)) ∙ (θ(i) - θ(i))
For the Aij (t) term, according to the i) in Assumption 1 for F, we have
X Aij(t) = - Xβ-1 (f(θ(i)) - F(θ(i))) ∙ (θ(i) - θ(i))
≤-β-1mF M Xw- θ(i)∣∣2
i
For the Bij (t) term, applying the concave condition for K and the oddness of vK in Assumption 1,
we have
M
X Bij⑻=X (vκ(θ(i) - θ(j)) - VK(θ(i) -砂))) ∙ (θ(i) -欧))
ij	ij
M
=2 X (vκ(θ(i) - θ(j)) - vκ(θ(i) - θ(j))) ∙ (θ(i) - θ(i) - (θ(j) - θ(j)))
ij
M2
≤-2mκ X∣∣θ(i)- θ(i)-(θ(j)- θ(j))∣∣ ≤ 0
ij
For the Cij(t) term, we have
/
E
\
X (vκ(θ(i)- θ(j))-VK * νt(θ(i)))
j
八1/2
XE(vK(θ(i)- θ(j))-vκ * νt(θ(i))) J
(3)	y—
≤ HyK √2M
where the (1) is obtained by applying the Cauchy-Schwarz inequality and (2) by the fact that
E (K(θ(i) - θ(j)) - K * νt(θ(i))) =0. We Can tune the bandwidth of the RBF kernel to make
kvKk ≤ HyK. Hence (3) is obtain by the boundedness of vK(θ).
Similarly, since K ≤ 1, we have the following result for Hij(t) term,
E	Hij (t)
j
—
2	1/2
≤E
θ(j)) - Eγ~,F(Yj)K(θ(i)- Yj)
E
2∖1/2
θ(j)) - Eγ∙»F(Yj)K(θ(i)- Yj))
15
Under review as a conference paper at ICLR 2019
≤HF√2M E
For the Fij(t) and Gij(t) terms, we have:
X Fij⑴=-X(F(θ(j))K(θ(i) - θ(j)) - F(θt(j))K(θ(i) — θ(j") ∙ (θ(i) - θ衿
ij	ij
≤ X LFlWj)- 叫 W- 叫
ij
≤ 2LfM X∣∣θ(i)- θ(i)∣∣2 .
i
X Gij⑴=X (F(θ(j))K(θ(i) - θ(j)) - F(θ(j))K(θ(i) - θ(j))) ∙ W- θ(i))
ij	ij
≤ HF LK X ∣∣θ(i) - θ(i) - (θ(j) - θ(j) )∣∣ ∣∣θ(i) - θ(i)∣∣
ij
≤ 3HfLkM XW- θ(i)∣∣2
i
We denote γi(t)，E ∣∣θ(i) 一 θ(i) ∣∣ . DUe to the exchangeability of the particles, γi(t) are the same
for all the particles, denoted as γ(t). Then according to (25), we have
Y0(t) ≤-2λιγ(t)+ HPK^√mHF6 √Y(t).
where λ1 = β-1mF - 3HF LK - 2LF
⇒ (√Y(t) - L(HVK + HF"72— )0 ≤ -λι(√Y(t) - L(HVK + HF”陋—)
√M(β-1 - 3HfLK - 2Lf)	—	√M(β-1 - 3HfLK - 2Lf)
Note that θ(i) and θ(i) are initialized with the same initial distribution μo = ν0. In the definition of
θ(i), there is no restriction on how the initial value is set. As a result, We can set θ0i) to be identical
to 80i), leading to γ(0) = 0. Then according to the Gronwall Lemma, we have
P^ ≤	(HVK+HF "√2
√M(β-1 - 3Hf LK - 2Lf )
Hence, there exist some positive constant (c1, c2) such that:
(1)
W1 (ρt, νt) ≤ W2 (ρt, νt)
≤)rE W-叫2 ≤) E-I-C2)	(26)
where (1) holds due to the relationship between W1 and W2 metric Givens & Shortt (1984), (2) due
to the definition of W2 and (3) due to the result from the previous proof.
16
Under review as a conference paper at ICLR 2019
E	Proof of Theorem 4
Proof [Proof of Theorem 4] Firstly, what we aim at is W1(νt, ν∞) ≤ c3 exp (-2λ1t) in this theorem.
According to the relationship between W1 and W2 metric Givens & Shortt (1984), once we bound
W2(νt, ν∞) as W2(νt, ν∞) ≤ c3 exp (-2λ1t), we will finish our proof.
Next, look at the equation (8):
If we set the initial distribution of each particle to be ν0, which means ρ0 = L(θ0(i)) = ν0, we will
derive M particles denoted as {θt(,i1)}iM=1. We denote the distribution of each θt(,i1) at t as ρt,1.
If we set the initial distribution of each particle to be ν∞, which means ρ0 = L(θ0(i)) = ν∞, we will
derive M particles denoted as {θt(,i2)}iM=1. We denote the distribution of each θt(,i2) at t as ρt,2.
Since we need to bound W2 (νt , ν∞), we make the following decomposition:
W2(νt,ν∞) ≤ W2(νt,ρt,1) + W2(ρt,1,ρt,2) + W2(ρt,2,ν∞) .	(27)
Note that ρ0,1 = ν0 and ρ0,2 = ν∞. Then, according to (26), we have
c1	c1
W2(νt,pt1) ≤ √M(β-1 - C2) and W2(ρt,2,ν∞) ≤ √M(β-1-C2)
Now we need to focus on the term W2(ρt,1, ρt,2). Since W2(ρt,1, ρt,2) ≤ E
r(t), we will derive a bound for E
—
in the following. We have
ξij (t) =(VK就1 - Θ(jI))- vκ(θ(i2 - θg))) ∙(咽-咽)
ξj(t) = -(F(θ(j1)K(θ(i1	- θ(jι1)	- F(θtj2)K(θ(i1	- θ(jι1))	.⑻1	-咽)
ξj(t) = -(F(θ(j21)K(θ(i)	- θ(jI))- F(θj1)K(咽-咽)[•⑻1	-咽]
For the ξi1j (t) terms, according to the i) in Assumption 1 for F, we have
X ξi ⑴=-X β-1 (f(瘠)-F(眼)•侬1 -咽)
ij	ij
≤ -β-1mF M X θt(,i11 - θt(,i212 .
i
17
Under review as a conference paper at ICLR 2019
For the ξj (t) term, applying the concave condition for K and the oddness of VK in Assumption 1,
we have
M
X ξ2j (t) = X (VK(θ(i) - θ(jI))- VKa2 -咽))∙⑻) -啊
ij	ij
M
=2X (vK(θ(i) - θ(jI))- vK(θ(i2 -喝))∙ (θ(i) -咽-(θ(jI)-喝))
ij
M2
≤-2mκ XIWi)-咽-(θ(jI)-吗)|| ≤ 0 .
For the ξi3j (t) terms, after applying the LF -Lipschitz property for F and K ≤ 1, we have
Xξj(t) = X - (F(θ(jI))K(θ(i) - θ(jI))- F(%K(θ(i) -唱))∙ (θ(i) - %)
ij	ij
≤ X LF IIIθt(,j1) - θt(,j2)III IIIθt(,i1) - θt(,i2)III
ij
≤ 2LF M X IIIθt(,i1) - θt(,i2)III2 .
i
For the ξi4j (t) terms :
X ξ4j (t)=- x(f (喝)K(θ(i) - θ(jI))- F (咽)K(θ(i2 -喝))・侬) -黑)
ij	ij
≤ HF LK X IIIθt(,i1) - θt(,i2) - (θt(,j1) - θt(,j2))III IIIθt(,i1) - θt(,i2)III
ij
≤3HFLKMXIIIθt(,i1)-θt(,i2)III2 .
i
Now we have
r0 (t) ≤ -2(β -1 mF - 3HF LK - 2LF )r(t) .
According to the Gronwall lemma,
r(t) ≤ r(0)e-2λ1t,
where λ1 = β-1mF - 3HF LW - 2LF .
Consequently, there exists some positive constant c3 such that
W2(ρt,1 , ρt,2) ≤ c3e-2λ1t
Then we have
W2(νt, ν∞) ≤ c3e-2λ1t +---J=-c1---------1----—c1----------
√M(β-1 - c2)	√M(β-1 - c2)
However, it worth noting that νt is the solution of (6) which has nothing to do with the number of
particles, M. Then let M → ∞, we can derive that W2 (νt, ν∞) ≤ c3e-2λ1t. Now we finish our
proof.
18
Under review as a conference paper at ICLR 2019
F	Proof of Theorem 5
To bound the Wι(μτ, PPT-1 限)term, note the original SDE driving the particles {θ(i)} in (8) is
a nonlinear SDE, which is=hard to deal with. Fortunately, (8) can be turned into a linear SDE by
concatenating the particles at each time into a single vector representation, i.e., by defining the new
parameter at time t as Θt，[θ(1),…，θ(M)] ∈ RMd. Consequently, Θt is driven by the following
linear SDE:
dΘt = -Fθ (Θt)dt + P2β-1dW(Md) ,	(28)
where	F® (Θt)	，	[β-1 F(θ(1))	-	M PM VK(θ(1)	-	θ(j)) +	M PM=	K(θ(1)	-
θ"F(θ”,…,β-1F(θ(M)) - M P= VK(θ(M) -θ(j)) + M P= K(θ(M) -θ(j))F(θ(j))]
is a vector function RMd → RMd, and Wt(M d) is Brownian motion of dimension Md.
Now we define the %® @)，[β-1Fq(θ(1)) - MN P* VK(θ(1) - θ(j)) + M PM K(θ(1) -
θ(j))Fq (θ(j)),…，β-1Fq (θ(M)) - MN P= VK(θ(M) - θj) + 吉 PM K(θ(M) -
θt(j ))Fq(θt(j))]. We can verify that FΘ(Θt) = PqN=1 F(q)Θ(Θt).
Then we define Θk，[θk1),…，θkM)] and Gθfc，BN Pq∈Ifc F(q)θ(Θk). We can verify that the
following result holds:
Θk+ι = Θk - β-1Gθk hk + √2β-1hkΞk ,	(29)
where Ξk 〜N(0, lMd×Md). Now we reach the conclusion that Θk of (29) is accutually the
numerical solution of the SDE (28) via stochastic gradients.
We denote the distribution of Θk as μ? and the distribution of Θt as p®. Before proceeding to our
theoretical results, we need to present the following Lemmas which is very important in our proof.
Lemma 13 Wι(μk,Pt) ≤ √MW√μ?,p®)
Proof [Proof of Lemma 13] Let us recall the definition of W1 metric and its Kantorovich-Rubinstein
duality Villani (2008), i.e. Wι(μ, V)，SupkgkQ≤ι ∣Eθ〜μ[g(θ)] 一 Eθ〜ν[g(θ)]∣. We can prove the
fact that if g(θ) : Rd → R is a Lg-Lipschitz function in Rd, the g® (Θ), defined as g® (Θ) =
√= PM g(θ(i)), is a Lg-Lipschitz function in RMd, where Θ，[θ⑴，…，θ(M)]:
MM
kg® (Θι) -g®(Θ2)k ≤√m X kg(θ(i)) -g(θ(i))k ≤ √ΜM X kθ(i) - θ2i)k
≤
LgkΘ1-Θ2k
Then we have:
1 M	(1)	1	1 M
M XWki)〜μjg(θki))] - Eθ"ρt[g(θ(i) )]| =) √M √M X(%)〜μjg(θki))] - %j[g(θ(i))])
=√M lEΘk 〜μJgΘ (θk )]- E®t 〜Pt [g® (Θt)]∣
The (1) holds since Eg(i)〜*Jg(θk1))]= … =Ej(m)〜*Jg(θkM))] for all the particles θki) and
Eθ(i)〜ρt[g(θ(1))] =∙ ∙ ∙ = Eθ(M)〜ρ [g(θt(M))] for all the particles θt(i). Then according to the
19
Under review as a conference paper at ICLR 2019
definition of W1 metric, we derive that
1 M
Wι(μk,ρt) = sup M X ∣%)〜μkEθki))l - Eθ(i)^ρt[g(θ(i))]∣ =
kgklip≤1	i=1
^7z≡ sup lEΘk 〜μJgΘ (θk)] - EΘt 〜ρt[gΘ (θt)]1
M kgklip≤1
≤√MW1 (μθ ,ρθ)
^7z≡ sup	lEΘk 〜μJgΘ (θk )] - EΘt 〜ρt[gΘ (θt)]1
M kgΘklip≤1
Lemma 14 Assuming F (0) = 0. If F in (9) is Lipschitz with constant LF, and satisfies the
dissipative property that hF(θ), θi ≥ mF kθk2 - b. Then FΘ in (28) is Lipschitz-continuous with
constant √2β-1Lp + l0 and satisfies〈F® (Θ), Θ∖ ≥ (β-1mp - m0) ∣∣Θk2 一 β-1Mb, where l0
and m0 are some positive constants.
Proof [Proof of Lemma 14]
uM	uM
∣FΘ(Θ1)-FΘ(Θ2)∣ =ut* X ∣ωi1 + ωi2 + ωi3∣2 ≤ tuX(∣ωi1∣ +∣ωi2∣ +∣ωi3∣)2
where
∣ωi1∣ =∣β-1F(θ1(i))-β-1F(θ2(i))∣
≤β-1LF∣θ1(i) -θ2(i)∣
MM
k"2k = k/XK(θ(i) - θ(j))F(θ(j)) - XK(θ(i) - θ2j))F(θ(j)))k
jj
1M	M
≤ k M(X K(θ(i) - θ(j))F(θ(j)) - X K(θ(i) - θ2j))F(θ(j)))k
jj
MM
+ k M(X K虏-θ2j))F (θ(j)) - X k (θ2i) - θ2j))F (θ2j)))k
jj
≤ (2LKHF+LF)kθ1(i) -θ2(i)k
MM
kω3k = k- M(X VK(θ(i) - θ(j)) - X VK(θ(i) - θ2j)))k
jj
M
≤ LMK(X kθ(i) - θ(i)-(θ(j)- θ(j))k)
j
1M
≤ LPK(kθ(i) - θ(i)k + M X kθ(j) - θ(j)k)
j
It is easy to verify that there exits some positive constant l0 such that
kFΘ(Θ1) -FΘ(Θ2)k
uM
tuX kωi1 + ωi2 + ωi3 k
≤t
MM
X2(β-1LF+2LKHF+LF+LPK)2kθ1(i) - θ2(i)k2 + 2 X kθ1(j) -θ2(j)k2
20
Under review as a conference paper at ICLR 2019
≤ ,2(β-1 LF + 2LkHF + LF + L弋K)2 + 2
≤ (√20-1Lf +10)
M
X kθ1(i) - θ2(i)k2
i

(√2β-1LF + 10)kΘι - Θ2k
Next, we have
hFΘ(Θ), Θi
MM	M
=X I β-1F(θ⑴)θ⑴ + ɪ X K(θ⑴-θ(j))F(θ(j))θ⑴——X VK(θ⑴-θ(jj)θ(i)
ij	j
Notice :
MM
X β-1F (θ(i))θ(i) ≥ β-1mFXkθ(i)k2 - β-1β-1Mb
ii
= β-1mF kΘk2 - β-1Mb
Since we have assumed F(0) = 0, we have:
M	M	MM
X Mm X K(θ(i) - θ(j))F (θ(j))θ(i) ≥ — Mm XX LF kθ(i)kkθj)k
i	j	ij
M
≥ -2LFXkθ(i)k2 = -2LF kΘk2
i=1
Since VK is an odd function, we have:
MM	MM
X MM X VK(θ⑴- θ(j))θ(i) ≥ - X MM X Lvkkθ(i) — θ(j) kkθ(i) k
ij	ij
M
≥ -3LvK X kθ(i)k2 = -3LvKkΘk2
i
As a result, we can derive the following result:
hFΘ(Θ),Θi ≥ (β-1m-2LF -3LvK)kΘk2 -β-1Mb
Now it is ready to prove Theorem 5. It worth noting that after assuming F(0) = 0, the first bullet in
Assumption 1 recovers the dissipative assumption as hF(θ), θi ≥ mF kθk2.
Proof Next we use Lemma C.5 in Xu et al. (2018) to verify that FΘ satisfies the assumptions in
Raginsky et al. (2017) by setting δ = B with a0 a positive constant and B the size of the random set
I. Let μΘ ：= L(Θk) and ρ? := L(Θt). Now we can borrow the result of Lemma 3.6 in Raginsky
et al. (2017). The relative entropy Dkl(μ? ∣∣ρfh) satisfies:
Dkl(μθ∣Pθh) ≤ (Aoβa0 + Aιh)kh
B
with
A0 = (2(ɪ + IO)2 (a2 + 2(1 ∨ τpι^^ɪ----0)(2a2 + ~7Γ)} + a2)
β	β -1 mF - m0	β
Ai =6( ^β^ + 10)2(βAo + Md)
21
Under review as a conference paper at ICLR 2019
and a1 , a2 are some positive constants. When the β is small enough, there exist some positive
constants a3 , a4 such that
Md	Md
A0 ≤ a3 羽，A1 ≤ a4 ~βi
Similar to the proof of Lemma 14, it is easy to verify that there exists some positive constant a5
such that hFΘ(Θ1) - FΘ(Θ2), Θ1 - Θ2i ≥ (β-1mF - a5)kΘ1 - Θ2k2. Notice, when β is small
enough, (28) satisfies the conditions of Proposition 4.2 in Cattiaux et al. (2008). Hence, there exits
so:me positive constant C such that TW^ι(μk , Pkh) ≤ CyjDKL(μθkρΘh).
According to Corollary 4 and Lemma 8 in Bolley & Villani (2005), we can derive an explicit
expression for C :
C ≤ a6β-1Md,
when β is small enough and a6 is some positive constant.
Applying the Lemma 13, we have
Wι(μk,Pkh) ≤
√= W1(μΘ,Pkh) ≤ a6Md3β-3(a3a0β2B-1 + a4h)2k1 h1
Let k = T and we can finish the poof.
G Proof of Theorem 6
Proof Our proof is based on the proof of Lemma 3.6 in Raginsky et al. (2017) with some modifica-
tions. Firstly, adopt the same notations in the Section F and we get the following update:
Θk+1 = Θk - β-1Gθk hk + √2β-1hkΞk ,	(30)
where Ξk 〜N(0, lMd×Md) and hk = kh⅛. We assume E(G^fc) = F® (Θk), ∀Θ ∈ RMd , which
is a general assumption due to the way that we choose the minibatch Ik. We need to define q(t),
which will be used in the following proof:
k-1	k
q(t) = {k ∈ R| Xhi ≤ t < X hi} .
Furthermore, we define Pi-=10 hi , 0 and Pi0=0 hi , h0 here just used for the convenience of
statement in the following.
Now we focus on the following continuous-time interpolation of Θk:
Θ(t)=Θo- Zt GΘ(s)
0
HtI hi)) ds+r zo W(Md)
whereI(S) ≡ Ik for t ∈ 叵鼠；h工° h), G%(Θ)，晶 £©(() F(q)®(Θ) and B(S) is
the size of the minibatch I(s). And for each k, Θ(Pk-c1 hi) and Θk have the same probability law
ρΘ. Since Θ(t) is not a Markov process, We define the following It6 process which has the same
one-time marginals as Θ(t)
Λ(t) = Θ° - /t Gs (Λ(s)) ds + βZ[ WsMd)
where Gt(X) := E
q(t)-1
GΘ(t) ∣Θ( X hi) I ∣Θ(t) = X
22
Under review as a conference paper at ICLR 2019
Let PtΛ := L (Λ(s) : 0 ≤ s ≤ t) and PtΘ := L (Θ(s) : 0 ≤ s ≤ t). According to the proof of
lemma 3.6 in Raginsky et al. (2017), we can derive a similar result for the relative entropy of PtΛ and
PtΘ:
Dkl(PΛ k PΘ )= - / d PΛ log ∣Pλ
β ft
4 J0
EkFθNsx- Gs (Λ(s))k2ds
β「
4 J0
EkFθ(Θ(s))- Gs (Θ(s))k2ds
The last line follows that L(Θ(s)) = L(Λ(s)), ∀s.
In the following proof, we will let t = Pik=-01 hi for some k ∈ R. Now we can use the martingale
property of It6 integral to derive:
Pk-1	Pk-1
DKL(PΛi=0hikPΘi=0hi)
β k-1	Pij=0 hi
=β∑	EkFθ(Θ(s))-Gs (Θ(s))k2ds
4 j=0 JPj-I hi
β k-1	Pij=0 hi	q(s)-1
≤ 2∑ 一	EkFθ(Θ(s))- Fθ(Θ( E hi))k2ds
j j=0 JPj=O hi	i=0
β k-1	Pij=0 hi	q(s)-1	q(s)-1
+ 2∑ E1	EkFθ(Θ( E hi)) - GΘ(s)	Θ( E hi)	k2ds
2 j=0 Pij=0 hi	i=0	i=0
2	k-1	Pij 0hi	q(s)-1
≤ VX jhi Ekθ(s) -θ( X hi)k2ds
β k-1	Pij=0 hi	q(s)-1	q(s)-1
+ 2∑ E1	EkFθ(Θ( E hi)) - GΘ(s)	Θ( E hi)	k2ds
j=0 Pij=0 hi	i=0	i=0
(31)
(32)
For the first part (31), we consider some s ∈ [Pij=-01 hi, Pij=0 hi). The following equation holds:
j-1
θ(S)- θ(E hi)
i=0
j-1
- (s - X hi)GΘj + p2∕β(WsMd)- WPMdI h)
j	i=0 hi
i=0
j-1	j-1
-(s - X hi)GIΘj + (s - X hi)(FΘ(Θj) - GIΘj)
i=0	i=0
+ P2∕β(wsMd)- WPMdI %)
i=0 hi
Thus, we can use Lemma 3.1 and 3.2 in Raginsky et al. (2017), and Lemma C.5 in Xu et al. (2018) to
get the following result:
j-1
EkΘ(s)- 旦(£ hi)k2
i=0
≤3jW EGj k2+ 3jW EkFθ (Θ) - Gθj k2+ βhM)
≤12jh⅛0≤m≤x-ι(LFθ 园叼2 + bi) + H
where b1 is some positive constant.
23
Under review as a conference paper at ICLR 2019
Consequently, the first part above (31) can be bounded as:
βLFθ X 产=。hi
2	j=0 JPi=O hi
q(s)-i
EkΘ(s) - Θ( X hi)k2ds
i=0
≤ βLFθ X
≤ 2 乙
j=0
12
1∖q max
(j + 1)3 0≤j ≤K -1
(LF® Ekθ∙ k2+bi)+jMdr
≤π2βL2FΘh030≤mj≤aKx-1(L2FΘEkΘjk2 +b1) +
π2L2F h02M d
where the last line follows from the fact that
k-i	1	k-i	1
j10 (jw ≤ j10 (jw
< X 1	= ∏2
≤ j=0 (j + 1)2 =E
According to Lemma C.5 in Xu et al. (2018), the second part (32) can be bounded as follows:
2
β X /Pi=。hi
2 j=0 JPi=。hi
q(s)-1
q(s)-1
EkFθ(Θ( E hi)) -G% Θ( E hi)	k2ds
i=0
i=0
X 2j⅛ EkFθ (θ ) - GΘ- k2
≤βh0 max
0≤j≤k-1
≤βh0 max
0≤j≤k-1
4	k-1
瓦+ X
(4	k-1
瓦+ X
(j + 1)(Bo + log 19090 (j + 1))
(j + 1)log 崂(j + 1)
4
4
≤(b2 + vr)βh0 max (LFaEkθjk2 + bI),
B0	0≤j≤k-1
where the last line follows from the fact that when r > 1,
k-1
X
j=1
(j+1)logr(j+1)
∞
≤X
j=1
(j + 1)logr (j + 1) ≤
4log1-r 2
r-1
4
4
Denote μ? ：= L(Θk) and ρΘ ：= L(Θt). DUe to the data-Processing inequality for the relative
entropy, we have
DklGkPPk-ι hi) ≤ DKL(PPk=。1 hi k PPk=。1 hi)
≤π2βL2FΘh03 max (L2FΘEkΘjk2+b1)+
0≤j≤k-1
π2L2F h02M d
2
+ (b2 + TΓ)βh0 maχ (lFθ Ek θj k2 + bi)
B0	0≤j≤k-1	Θ
≤ (π2βLFθ h03 + b2βh0 + ~ΡΓβh0) max (LFθEkθj k2 + b1) +
Θ	B0	0≤j≤k-i	Θ
π2L2F h02M d
2
Lemma 3.2 in Raginsky et al. (2017) has Provided a uniform bound to max0≤j≤k-i(L2F EkΘj k2 +
bi). Hence We can tell that DKL(PPi=。hi k PPi=0 hi) would not increase w.r.t. k. This is a nice
property that the fixed-step-size SPOS does not endow. Since LFa = √2β-iLF + l0, it is easy to
24
Under review as a conference paper at ICLR 2019
verify that when β is small enough, there exists some positive constants b3 , b4 , b5 and b6 such that:
Dkl(μθ 1虔富 hi)
4βh0	π2L2F h02M d
≤(π βLFθ ho + b2βh0 + -^~) n.max 1(LFθEkθj k2 + b1) + ----⅜-------
Θ	B0	0≤j≤K-1	Θ	2
≤(b3h0 + T0 + b5h0β 2) Md.
B0	β4
Similar to the proof of Theorem 5, We can bound the Wι(μθIlPPk-I hι ) term with Corollary 4,
Lemma 8 in Bolley & Villani (2005) and Proposition 4.2 in Cattiaux et=al. (2008). Specifically, when
β is small enough, there exist some positive constant a6 such that:
Wι(μθIlPPk-I hi) ≤ a6(MdNDKL用 1虔二1h)
≤ a6β-3M3 d22 (b3h3 + "匕 h0 + b5h2β2)1 .
B0
According to Lemma 13, we have
Wl(μk,Pkh) ≤ √MW1(μθ ∣∣ρPk-θι hi ) = a6β-3Md 2 (b3 h3 +，:口" + b5h0β2 ) 2
Let k = T and we can finish the proof.
H Detailed Statements of Nonasymptotic Convergence under the
Convex Case
We give detailed statements of non-asymptotic convergence of SPOS under the convex case, which can
be proved by directly combining results from Theorem 3-6, thus they are omitted for simplicity. We
consider a fixed-stepsize and a decreasing-stepsize cases in Theorem 15 and Theorem 16, respectively.
Theorem 15	(Fixed Stepsize) Under Assumption 1, if we set hk = h0 and Bk = B0, we can bound
the Wι(μτ, ν∞) as
W1(μτ, ν∞) ≤~Γ=~~~----- + c3 eχp {-2 (β 1 mF - 3HFLK - 2LF) Th}
M(β-1 - c2 )
+ c3Md 3 β-3(c4β2B-1 + c5h) 2 T 2 h 1 .
where (cι,c2, c3, c4, c5,c6, β) are some positive constants such that β > c2 and mF > 3HFLK 一
2LF.
Theorem 16	(Decreasing Stepsize) Under Assumption 1, if we set hk = h0/(k + 1) and Bk
B0 + [log(k + 1)]100/99, we can bound the Wι(μτ, ν∞) as
Wι(μτ, ν∞) ≤
c1
√M(β-1 一 c2)
+ c3 exp ɔ —2 (β-1mF — 3HfLK - 2Lf) (〉： hk)]
k=0
+ c3β-3Md2 (c6h3 + c7β h0 + C8h2β2)2 .
B0
where (c1,c2,c3, c6, c7, c8, β) are some positive constants such that β > c2 and mF > 3HfLK 一
2LF, as
25
Under review as a conference paper at ICLR 2019
I Proof of Theorem 7 and Corollary 8
Proof [Proof of Theorem 7]
d θt(i) -θt(j) = -β-1 F (θt(i)) - F (θt(j)) dt
1M
+ M X [vk(θ(i) - θ(q)) - VK(θ(j) - θ(q))] dt
q
1M
- IM X (F(θ(q))K碎-θ(q)) - F(θ(q))K阳-θ(q)))) dt
q
+ r2(dW(i) - dW(j))
⇒d (E Xw-叫]
MM
+E X M XhVK(θ(i)
M
-E2Xβ-1	F(θt(i))-F(θt(j))	θt(i)	-θt(j)	dt
ij
dt
ij	q
MM
-EX M X (F(θ(q))K(θ(i) - θ(q)) - F(θ(q))K(θ(j) - θ(q)))) (θ(i)
dt
ij
—
—
—
q
M Py
+ E2 X X(dW(i) - dW(j)) (θ(i) - θ(j))
ij
≤ -2β-1mFE X θt(i) -θt(j)2dt - 2mKE X θt(i) - θt(j)2 dt
ij	ij
M2
+ 2HF LK E X θt(i) - θt(j)	dt
ij
+ 2∏ (EX(dWt(i) - dWtj)))
M
EXθt(i) - θt(j)
ij
1/2
2
Denote z(t)= E PiMj θt(i) - θt(j) . We have
z(t)0 ≤ -(2β-1mF + 2mκ — 2HfLK)z(t) + 4M J^^z(t)
(33)
We can finish our proof by applying Gronwall Lemma on (33).
Proof [Proof for Corollary 8]
dθt(i)-θt(j) =
1M
+ M X [vκ(θ(i) - θ(q)) - VK(θ(j) - θ(q))] dt
q
1M
-M X (F(θ(q))K(θ(i) - θ(q)) - F(θ(q))K(θ(j) - θ(q))))dt
q
26
Under review as a conference paper at ICLR 2019
⇒d (EXW-研∣)=
MM
+ E X mm X [vκ(θ(i) - θ(q)) - VK(θj) - θ(q))]⑻)-θj dt
ij	q
MM
-EX M X (F(θ(q))K(θ(i) - θ(q)) - F(θ(q))K(θ(j) - θ(q)))) (θ(i) - θj dt
ij	q
M2	M2
≤ -2mK E X θt(i) - θt(j)	dt+2HFLKEXθt(i) -θt(j) dt
ij	ij
Denote z(t)= E PiMj θt(i) - θt(j) . We have
z(t)0 ≤ -(2mK - 2HF LK)z(t)	(34)
We can finish our proof by applying Gronwall Lemma on (34).	■
J	Proof of Theorem 9
Proof [Proof of Theorem 9] Our conclusion for B(μτ, μ∞) is essentially a specification of the result
in Mattingly et al. (2002), which has also been applied in Xu et al. (2018).
Specifically, we rely on the following lemma, which is essentially Theorem 7.3 in Mattingly et al.
(2002) and Lemma C.3 in Xu et al. (2018), considering the SDE (16):
dΘt = -Fθ (Θt)dt + P2β-1dW(Md)
As mentioned in Section 5, we firstly denote the distribution of Θt as ρtΘ . Then we define the
Θk，[θp),…，θkM)] ∈ RMd, which is actually the numerical solution of (16) using full gradient
with Euler method. And we denote the distribution of Θk as μΘ.
Lemma 17 Let FΘ be Lipschitz-continuous with constant LΘ, and satisfy the dissipative property
that hFθ (Θ), Θi ≥ mθ ∣∣Θ∣∣2 - b®. Define Vθ (Θ) = Co + Lθ/2∣∣Θk2. The Euler method for
(16) has a unique invariant measure μ∞, and for all testfunction f® Such that f® | ≤ V® (Θ), we
have
Md
∣E[fθ(Θk))] - Eθ∞〜^θ[f(Θ∞)]∣ ≤ Cκρ-Md∖l + Kemh)exp ---l®g(KP	),
where P ∈ (0,1), C > 0 are positive constants, and K = 2L® (b® β + m® β + Md)/m6.
Now we adopt the f® : RMd → R defined as f® (Θ)=吉 PM f (θ⑴)，where f : Rd → R is
a Lf -Lipschitz function and satisfies our Assumption 2 and Θ，[θ⑴，…，θ(M)]. Similar to the
proof of Lemma 12, we can find that f® : RMd is a Lf/√M-Lipschitz function. Furthermore,
according to the Lemma 14, it is easily check that F® is L®-Lipschitz where L® = √2β-1LF + l0.
Hence, when the β is small enough, we have Lf /√M ≤ 2β-1LL+ + l0. Then we can set the Co
large enough to force f® to satisfy the condition in Lemma 17 that |f® | ≤ V® (Θ). Then, according
to the exchangeability of the particle system {θki)} and Lemma 14, we can bound the B(Rt, μ∞) as
B(μτ,μ∞) ≤ |E[f® (Θt))] - Eθ∞〜μ∞ [f (Θ∞)]∣
≤ C2ςσ-Md/2 (1 + ςemθh) exp (-2m®ThσMd/ log(ς))
27
Under review as a conference paper at ICLR 2019
where ς = 2Lθ (Mbe + mθβ + Md)∕mθ, Lθ = V^eTLF + l0,mΘ = β-1 m - m0, and
(σ, C2, l0, m0) are some positive constants independent of (T, M, h) and σ ∈ (0, 1).
To prove the bound for B(μ∞, ρ∞) Since Θk = (θf1,…,θ((M)) can be considered as a solution
to the SDE (16), standard results from linear FP equation can be applied. Specifically, for the
B(μ∞, ρ∞) term, we rely on the following lemma adapted from Lemma C.4 In Xu et al. (2018)Chen
et al. (2015), and is essentially the work of Chen et al. (2015) when taking K → ∞.
Lemma 18 Under the same assumption as in Lemma 17, for the Lipschitz-continuous function
fθ (Θ) = -M PM f (θ(i)) mentioned above, the following bound is satisfied for some positive
constant C:
he
[f(θ∞)] ≤C β+K)
1 K-1
K E E[fΘ (θk )] - EΘ∞〜ρ∞
k=1
The uniqueness of invariant measure of the Euler method from Lemma 17 implies the numerical
solution Θk to be ergodic. Then similar to the proof of Lemma 4.2 in Xu et al. (2018), we consider
the case where K → ∞. Take average over the {Θ(}K==01, we have
1	K-1
Eθg〜^θ [fθ(Θ∞)] = Jim - E E[fθ(Θk)]
∞ "∞	K→∞ K
k=1
Now according to the exchangeability of the particle system {θ(i)} and {θ(i)}, we can bound the
B(μ∞, ρ∞) as :
B(μ∞,ρ∞) ≤ ∣Eθ∞〜^Θ[fθ(Θ∞)] - Eθ∞〜ρ∞[fθ(Θ∞)]∣ ≤ Che
where C3 are some positive constant.
K Proof of Theorem 10
Proof [Proof of Theorem 10] Adopting the same notation used in the proof of the Theorem 5, we
define Θ(，[θ(1),…，θ(M)] and Gfk，BN Pq∈ιfc F(q)θ(Θk). We denote the distribution of Θ(
as μkΘ .
Θk+1 = Θk - β-1Gfk hk + P2β-1hkΞk ,
We firstly give a bound to the W2(μΘ, μΘ) (the definition of the ^^Θ has been mention in the last
section). According to the proof of the Lemma 4.4 in Xu et al. (2018)
W2(μΘ,μΘ) ≤ kh(LθΓ0 + MC4) ((6 + 2Γ0)β∕B)1/2
where Γ0 = 2(1 + 1∕mΘ)(M b + 2M2C42 + Md∕e) and C4 is some positive constant independent
of (T, M, h). Note the fact that Wι(μf ,μ(θ) ≤ Wz(μ?,μ(θ) and Wι(μk,μk) ≤ √MWi(μ(θ,μ(θ)
(see the proof of Lemma 13, similar result holds here), We get
Wi(μτ,μτ) ≤ Th(LθΓ0 + MC4) ((6 + 2Γ0)β∕(BM))1/2.
Let us compare the definition of Wi(μ, ν) and B (μ, ν)
Wi(μ,ν) , sup ∣Eθ〜μ[g(θ)] - Eθ〜V[g(θ)]∣
kgklip≤i
B(μ,ν) , ∣Eθ〜”[f(θ)]- Eθ”[f(θ)]∣
and we can derive the result that B(μτ, μκ) ≤ Lf Wi(μτ, μτ). Now we finish our proof. ■
28
Under review as a conference paper at ICLR 2019
L Discussion on the complexity of our method
The complexity of an algorithm mainly refers to its time complexity (corresponding to the number
of iterations in our method i.e. T) and space complexity (corresponding to the number of particles
used in our method i.e. M). Hence the complexity of our method can be well explored with our
work, since our non-asymptotic convergence theory was developed w.r.t. the number of particles
i.e. M and iterations i.e. T. Their relationship (tradeoff) was even discussed further in Experiment
6.1. Moreover, comparing (9) with (3) , one can easily find that our space complexity is exactly the
same as SVGD and our computational time in each iteration is almost the same as SVGD with an
extra addition operation. However, it worth noting that our method have much better performance in
practice and no "pitfall" verified by both our theory and our experiments. .
M Comparison with Related Work
Firstly, our proposed framework SPOS is different from the recently proposed particle-optimization
sampling framework (Chen et al., 2018), in the sense that we solve the nonlinear PDE (6) stochasti-
cally. For example they deterministically solve the equation in (6) ∂νt = β-1Vθ ∙Vθ Vt approximately
using blob method adopted from (Carrillo et al., 2017).
Secondly, our method is also distinguishable to existing work on granular media equations such as
(Durmus et al., 2018). Their work about the Granular media equations focuses on the following PDE:
∂tνt = Vθ ∙ (VteTF(θ) + Vt (VK * %(θ)) + β-1Vθ%) ,	(35)
whereas our framework focuses on the following one:
∂tνt = Vθ ∙ (VteTF(θ) + Vt (EY〜VtK(θ - Y)F(Y) - VK * νt(θ)) + β-1VθVt) .	(36)
The extra term Vt (EY〜VtK(θ 一 Y)F(Y)) in our framework makes the analysis much more chal-
lenging. The main differences are summarized below:
•	Formulations are different. The extra term EY〜*tK(θ 一 Y)F(Y) cannot be combined with
the F(θ) term in their (35). This is because function F(θ) itself is a function independent
of t; while Eγ〜*tK(θ 一 Y)F(Y) depends on both θ and t. This makes our problem much
more difficult.
•	Assumptions are different. For example, the analysis on granular media equations in
(Cattiaux et al., 2008) requires that F satisfies a special condition C(A, α), which is a strong
condition impractical to be satisfied in our case; And Durmus et al. (2018) adopts different
assumptions from ours with a different goal.
•	For the Euler integrator, Durmus et al. (2018) does not consider an Euler solution. Further-
more, our sampling method needs "'stochastic gradient" i.e. Gk，BN Pq∈z^ Fq(θki)) in
(9) for computational feasibility, which is quite different from the former work on particle-
SDE such as (Malrieu, 2003; Cattiaux et al., 2008). Few of the former work on particle-SDE
considered the stochastic gradient issue.
To sum up, the main purpose of our paper is to provide a non-asymptotic analysis of our method
instead of improving the former work on a certain type of PDE. This is also the reason why we said
that part of our proof techniques are based on those for analyzing granular media equations.
N	Extra Experiments
N.1 Toy Experiments
We compare the proposed SPOS with other popular methods such as SVGD and standard SGLD
on four mutil-mode toy examples. We aim to sample from four unnormalized 2D densities
p(z)/exp{U(z)}, with the functional form provided in Rezende & Mohamed (2015). We opti-
mize/sample 50 and 2000 particles to approximate the target distributions. The results are illustrated
in Figure 4 and Figure 5, respectively.
29
Under review as a conference paper at ICLR 2019
Figure 4: Illustration of different algorithms on toy distributions. Dots are the final particles; the
Figure 5: Illustration of different algorithms on toy distributions. Dots are the final particles; the blue
regions represent densities estimated by the particles. Each column is a distribution case. First row:
ground true densities; Second row: standard SGLD; Third row: SVGD; Fourth row: SPOS.
N.1.1 Bayesian Neural Networks for MNIST classification
We perform the classification tasks on the standard MNIST dataset. A two-layer MLP 784-X-X-10
with ReLU activation function is used, with X being the number of hidden units for each layer. The
training epoch is set to 100. The test errors are reported in Table 2. Surprisingly, the proposed
SPOS outperforms other algorithms such as SVGD at a significant level, though it is just a simple
modification of SVGD by adding in random Gaussian noise. This is partly due to the fact that our
SPOS algorithm can jump out of local modes efficiently, as explained in Section 4.4.
30
Under review as a conference paper at ICLR 2019
Table 2: Classification error of FNN on MNIST.
Method	Test Error	
	400-400	800-800
SPOS	1.32%-	1.24%
SVGD	1.56%	1.47%
SGLD	1.64%-	1.41%
RMSprop	1.59%	1.43%
RMSspectral	1.65%	1.56%
SGD	1.72%	1.47%
BPB, Gaussian	1.82%-	1.99%
SGD, dropout	1.51%	1.33%
31