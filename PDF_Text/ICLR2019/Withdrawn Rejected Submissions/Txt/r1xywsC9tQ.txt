Under review as a conference paper at ICLR 2019
Mapping the hyponymy relation of wordnet
onto vector Spaces
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we investigate mapping the hyponymy relation of WordNet to
feature vectors. We aim to model lexical knowledge in such a way that it can
be used as input in generic machine-learning models, such as phrase entailment
predictors. We propose two models. The first one leverages an existing mapping
of words to feature vectors (fastText), and attempts to classify such vectors as
within or outside of each class. The second model is fully supervised, using solely
WordNet as a ground truth. It maps each concept to an interval or a disjunction
thereof. On the first model, we approach, but not quite attain state of the art
performance. The second model can achieve near-perfect accuracy.
1	Introduction
Distributional encoding of word meanings from the large corpora (Mikolov et al., 2013; 2018; Pen-
nington et al., 2014) have been found to be useful for a number of NLP tasks. These approaches
are based on a probabilistic language model by Bengio et al. (2003) of word sequences, where each
word w is represented as a feature vector f(w) (a compact representation of a word, as a vector of
floating point values).
This means that one learns word representations (vectors) and probabilities of word sequences at the
same time.
While the major goal of distributional approaches is to identify distributional patterns of words and
word sequences, they have even found use in tasks that require modeling more fine-grained relations
between words than co-occurrence in word sequences. Folklore has it that simple manipulations
of distributional word embedding vectors is inadequate for problems involving detection of other
kinds of relations between words rather than their co-occurrences. In particular, distributional word
embeddings are not easy to map onto ontological relations and vice-versa. We consider in this
paper the hyponymy relation, also called the is-a relation, which is one of the most fundamental
ontological relations.
Possible sources of ground truth for hyponymy are WordNet Fellbaum (1998), FrameNet Baker
et al. (1998), and JeuxDeMots1 (Lafourcade & Joubert, 2008). These resources have been de-
signed to include various kinds of lexical relations between words, phrases, etc. However these
resources have a fundamentally symbolic representation, which can not be readily used as input to
neural NLP models. Several authors have proposed to encode hyponymy relations in feature vectors
(Vilnis & McCallum, 2014; Vendrov et al., 2015; Athiwaratkun & Wilson, 2018; Nickel & Kiela,
2017). However, there does not seem to be a common consensus on the underlying properties of
such encodings. In this paper, we aim to fill this gap and clearly characterize the properties that
such an embedding should have. We additionally propose two baseline models approaching these
properties: a simple mapping of fastText embeddings to the WordNet hyponymy relation, and
a (fully supervised) encoding of this relation in feature vectors.
1Unlike WordNet and FrameNet, which are developed by a teams of linguist with rigorous guidelines
and strategy, JeuxDeMots is a resource complied by collecting the untrained users’ judgments about the
relationships between words, phrases etc.)
1
Under review as a conference paper at ICLR 2019
2	Goals
We want to model hyponymy relation (ground truth) given by WordNet — hereafter referred to
as hyponymy. In this section we make this goal precise and formal. Hyponymy can in general
relate common noun phrases, verb phrases or any predicative phrase, but hereafter we abstract from
all this and simply write “word” for this underlying set. In this paper, we write (⊆) for the reflexive
transitive closure of the hyponymy relation (ground truth), and (⊆M) for relation predicted by a
model M.2 Ideally, we want the model to be sound and complete with respect to the ground truth.
However, a machine-learned model will typically only approach those properties to a certain level,
so the usual relaxations are made:
Property 1 (Partial soundness) A model M is partially sound with precision α iff., for a proportion
α of the pairs of words w, w0 such that w ⊆M w0 holds, w ⊆ w0 holds as well.
Property 2 (Partial completeness) A model M is partially complete with recall α iff., for a propor-
tion α of the pairs of words w, w0 such that w ⊆ w0 holds, then w ⊆M w0 holds as well.
These properties do not constrain the way the relation (⊆M) is generated from a feature space.
However, a satisfying way to generate the inclusion relation is by associating a subspace of the
vector space to each predicate, and leverage the inclusion from the feature space. Concretely, the
mapping of words to subspaces is done by a function P such that, given a word w and a feature
vector x, P (w, x) indicates if the word w applies to a situation described by feature vector x. We
will refer to P as a classifier. The inclusion model is then fully characterized by P, so we can denote
it as such ((⊆P)).
Property 3 (space-inclusion compatibility) There exists P : (W ord × V ec) → [0, 1] such that
(w0 ⊆P W) ^⇒ (∀x.P(w, x) ≤ P(w0, x))
A consequence of such a model is that the relation (⊆P) is necessarily reflexive and transitive
(because subspace inclusion is such) — the model does not have to learn this. Again, the above
property will apply only to ideal situations: it needs to be relaxed in some machine-learning contexts.
To this effect, we can define the measure of the subspace of situations which satisfies a predicate
p : V ec → [0, 1] as follows:
measure(p)
p(x)dx
Vec
(Note that this is well-defined only if p is a measurable function over the measurable space of
feature vectors.) We leave implicit the density of the vector space in this definition. Following this
definition, a predicate p is included in a predicate q iff.
measure(p ∧ q) = JVecp(x)q(x)dx =]
measure(p)	Vec p(x)dx
However, now, we can define a relaxed inclusion relation, corresponding to a proportion of ρ of p
included in q:
Property 4 (relaxed space-inclusion compatibility) There exists P : Word → V ec → [0, 1] and
ρ ∈ [0, 1] such that
(w0 ⊆p W) ^⇒
JVec P(w0, X)P(w, x)dx〉
Rvec P(w, x)dx	ρ P
In the following, we call ρ the relaxation factor.
2We note right away that, on its own, the popular metric of cosine similarity (or indeed any metric) is
incapable of modeling hyponymy, because it is an asymmetric relation. That is to say, we may know that the
embedding of “animal” is close to that of “bird”, but from that property we have no idea if we should conclude
that “a bird is an animal” or rather that “an animal is a bird”.
2
Under review as a conference paper at ICLR 2019
3	MAPPING WORDNET OVER fastTEXT
Our first model of hyponymy works by leveraging a general-purpose, unsupervised method of
generating word vectors. We use fastText Mikolov et al. (2018) as a modern representative of word-
vector embeddings. Precisely, we use pre-trained word embeddings available on the fastText web-
page, trained on Wikipedia 2017 and the UMBC webbase corpus and the statmt.org news dataset
(16B tokens). We call FTDom the set of words in these pre-trained embeddings.
A stepping stone towards modeling the inclusion relation correctly is modeling correctly each pred-
icate individually. That is, we want to learn a separation between fastText embeddings of words that
belong to a given class (according to WORDNET) from the words that do not. We let each word w
in fastText represent a situation corresponding to its word embedding f (w). Formally, we aim to
find P such that
Property 5 P(w, f (w0)) = 1 ^⇒ w0 ⊆ W
for every word w and w0 found both in WORDNET and in the pre-trained embeddings. If the above
property is always satisfied, the model is sound and complete, and satisfies Property 3.
Because many classes have few representative elements relative to the number of dimensions of the
fastText embeddings, we limit ourselves to a linear model for P, to limit the possibility of overdoing.
That is, for any word w, P(w) is entirely determined by a bias b(w) and a vector θ(w) (with 300
dimensions):
P(w, x) = δ(θ(w) ∙ x + b(w) > 0)
where δ(true) = 1 and δ(false) = 0.
We learn θ(w) and b(w) by using logistic regression, independently for each WORDNET word w.
The set of all positive examples for w is {f (w0) | w0 ∈ FTDom, w0 ⊆ w}, while the set of negative
examples is {f (w0) | w0 ∈ FTDom, w0 6⊆ w}. We use 90% of positive examples for training
(reserving 10% for testing) and we use the same amount of negatives.
We train and test for all the predicates with at least 10 positive examples. We then test Property 5.
On the 10% of positive examples reserved for testing, we find that 89.4% (std. dev. 14.6 points) are
identified correctly. On 1000 randomly selected negative examples, we find that 89.7% are correctly
classified (std dev. 5.9 points). The result for positives may look high, but because the number of
true negative cases is typically much higher than that of true positives (often by a factor of 100),
this means that the recall and precision are in fact very low for this task. That is, the classifier can
often identify correctly a random situation, but this is a relatively easy task. Consider for example
the predicate for “bird”. If we test random negative entities (“democracy”, “paper”, “hour”, etc.),
then we may get more than 97% accuracy. However, if we pick our samples in a direct subclass,
such as (non-bird) animals, we typically get only 75% accuracy. That is to say, 25% of animals are
incorrectly classified as birds.
To get a better intuition for this result, we show a Principal Component Analysis (PCA) on animals,
separating bird from non-birds. It shows mixing of the two classes. This mixture can be explained
by the presence of many uncommon words in the database (e.g. types of birds that are only known
to ornithologists). One might argue that we should not take such words into account. But this would
severely limit the number of examples: there would be few classes where logistic regression would
make sense.
However, we are not ready to admit defeat yet. Indeed, we are ultimately not interested in Property 5,
but rather in properties 1 and 2, which we address in the next subsection.
3.1	Inclusion of subspaces
A strict interpretation of Property 3 would dictate to check if the subspaces defined in the previous
section are included in each other or not. However, there are several problems with this approach. To
begin, hyperplanes defined by θ and b will (stochastically) always intersect therefore one must take
into account the actual density of the fastText embeddings. One possible approximation would be
that they are within a ball of certain radius around the origin. However, this assumption is incorrect:
modeling the density is hard problem in itself. In fact, the density of word vectors is so low (due to
3
Under review as a conference paper at ICLR 2019
-0.50
Figure 1: PCA representation of animals. Birds are highlighted in orange.
Figure 2: Results of inclusion tests. On the left-hand-side, we show the distribution of correctly
identified inclusion relations in function of ρ. On the right-hand-side, we show the distribution of
(incorrectly) identified inclusion relations in function of ρ.
the high dimensionality of the space) that the question may not make sense. Therefore, we refrain
from making any conclusion on the inclusion relation of the euclidean subspaces, and fall back to a
more experimental approach.
Thus we will test the suitability of the learned P (w) by testing whether elements of its subclasses
are contained in the superclass. That is, we define the following quantity
Q(w0, w) = average{P (w0, x) | x ∈ FTDom, P(w, f (x))}
which is the proportion of elements of w0 that are found in w. This value corresponds to the relax-
ation parameter ρ in Property 4.
If w0 ⊆ w holds, then we want Q(w0 , w0) to be close to 1, and close to 0 otherwise. We plot
(figure 2) the distribution of Q(w, w0) for all pairs w0 ⊆ w, and a random selection of pairs such that
w0 6⊆ w. The negative pairs are generated by taking all pairs (w0, w) such that w0 ⊆ w, and generate
two pairs (w1, w) and (w0, w2), by picking w1 and w2 at random, such that neither of the generated
pairs is in the hyponymy relation. We see that most of the density is concentrated at the extrema.
Thus, the exact choice of ρ has little influence on accuracy for the model. For ρ = 0.5, the recall is
88.8%. The ratio of false positives to the total number of negative test cases is 85.7%. However, we
4
Under review as a conference paper at ICLR 2019
Figure 3: Two trees underlying the same dag. Notes are labeled with their depth-first index on the
left and their associated interval on the right. Removed edges are drawn as a dotted line.
have a very large number of negatives cases (the square of the number of classes, about 7 billions).
Because of this, we get about 1 billion false positives, and the precision is only 0.07%. Regardless,
the results are comparable with state-of-the art models (section 5).
4	WordNet predicates as disjunction of intervals
In this section we propose a baseline, fully supervised model model for hyponymy.
The key observation is that most of the hyponymy relation fits in a tree. Indeed, out of 82115
nouns, 7726 have no hypernym, 72967 have a single hypernym, 1422 have two hypernyms or more.
In fact, by removing only 1461 direct edges, we obtain a tree. The number of edges removed in the
transitive closure of the relation varies, depending on which exact edges are removed, but a typical
number is 10% of the edges. In other words, when removing edges in such a way, one lowers the
recall to about 90%, but the precision remains 100%. Indeed, no pair is added to the hyponymy
relation. This tree can then be mapped to one-dimensional intervals, by assigning a position to each
of the nodes, according to their index in depth-first order (ix(w) below). Then, each node is assigned
an interval corresponding to the minimum and the maximum position assigned to their leaves. A
possible DAG and a corresponding assignment of intervals is shown in Fig. 3. The corresponding
definition of predicates is the following:
P(w, x) = x ≥ lo(w) ∧ x ≤ hi(w)
lo(w) = min{ixT (w0) | w0 ⊆T w}
hi(w) = max{ixT (w0) | w0 ⊆T w}
where (⊆T ) is the reflexive-transitive closure of the T tree relation (included in HYPONYMY). The
pair of numbers (lo(w), lo(w)) fully characterizes P (w). In other words, the above model is fully
sound (precision=1), and has a recall of about 0.9. Additionally, Property 3 is verified.
Because it is fully sound, a model like the above one can always be combined with another model to
improve its recall with no impact on precision — including itself. Such a self-combination is useful
if one does another choice of removed edges. Thous, each word is characterized by an n-dimensional
co-product (disjoint sum) of intervals.
w ⊆M w0 =4	(loi (w0) ≥ loi (w) ∧ hii (w0) ≤ hii (w))
i
loi (w) = min{ixTi (w0) | w0 ⊆Ti w}
hii (w) = max{ixTi (w0) | w0 ⊆Ti w}
By increasing n, one can increase the recall to obtain a near perfect model. Table 4b shows typical
recall results for various values of n. We underline that Property 3 is not verified: the co-product of
intervals do not form subspaces.
5 Related Work
Many authors have considered modeling hyponymy. However, in many cases, this task was not the
main point of their work, and we feel that the evaluation of the task has often been partially lacking.
5
Under review as a conference paper at ICLR 2019
Here, we go back to several of those and attempt to shed a new light on existing results, based on
the properties presented in section 2.
5.1	Precision and recall for hyponymy models
Several authors, including Athiwaratkun & Wilson (2018); Vendrov et al. (2015); Vilnis et al. (2018),
have proposed Feature-vector embeddings of WordNet. Among them, several have tested their
embedding on the following task: they feed their model with the transitive closure of hyponymy,
but withhold 4000 edges. They then test how many of those edges can be recovered by their model.
They also test how many of 4000 random negative edges are correctly classified. They report the
average of those numbers. We reproduce here their results for this task in Table 4a. As we see it,
there are two issues with this task. First, it mainly accounts for recall, mostly ignoring precision. As
we have explained in section 3.1, this can be a significant problem for WordNet, which is sparse.
Second, because WordNet is the only input, it is questionable if any edge should be withheld at
all (beyond those in the transitive closure of generating edges). We believe that, in this case, the
gold standard to achieve is precisely the transitive closure. Indeed, because the graph presentation
of WordNet is nearly a tree, most of the time, the effect of removing an edge will be to detach a
subtree. But, without any other source of information, this subtree could in principle be re-attached
to any node and still be a reasonable ontology, from a purely formal perspective. Thus we did not
withhold any edge when training our second model on this task (the first one uses no edge at all). In
turn, the numbers reported in Table 4a should not be taken too strictly.
Authors, system	Result	n	recall
Vendrov et al. (2015), order-embeddings	90.6	1	0.91766
Athiwaratkun & Wilson (2018), DOE (KL)	92.3	2	0.96863
ViIniS et al.(2018), Box + CPD	92.3	5	0.99288
us, fast Text with LR and P = 0.5	87.2	10	0.99973
us, single interval (tree-model) us, interval disjunctions, n = 5	94.5 99.6	(b) Typical recalls for multi-dimensional in-	
(a) Authors, systems and respective results on the task of detection of hy- terval model. (Preci-
ponymy in WordNet	sion is always 1.)
Figure 4: Tables
5.2	Predicates as subspace and the geometry of the feature vector space
The task of modeling hyponymy is often used as an oblique way to associate subspaces to nouns
(Property 3). Property 3 is an instance of the notion of order-embedding proposed by Vendrov
et al. (2015), where we take subspace-inclusion as the underlying order. Vendrov et al. (2015) uses
another concrete order: the intersection of (reverse) order of several scalar spaces.
A difficulty of using subspace inclusion is that, when the underlying space is sparse and high-
dimensional, like section 3, it is difficult to meaningfully assign a continuous density to the feature
space. One way to tackle the issue involves using lower-dimensional hyperbolic vector spaces,
instead of an Euclidean one. This is proposed by (Nickel & Kiela, 2017; 2018). Nickel & Kiela
(2017) makes use of the Poincare ball model, which has been found to be useful for hosting vectorial
embeddings of complex networks (Krioukov et al., 2010), and especially tree-like structures. One
more model that Nickel & Kiela (2018) propose is Lorenz’s hyperbolic space. Yet their aim is not
to classify the feature space.
Another model was proposed by Vendrov et al. (2015), referred as a lattice model by Vilnis et al.
(2018), a term which we adopt here. In this model, every vector has non-negative coordinates. The
HYPONYMY relation is modeled as follows: X entails Y iff. for each coordinate i, xi ≤ yi , where
(x1, . . . , xn) and (y1, . . . , yn) are the n-dimensional vectors representing X and Y , respectively.
So, each word w is associated a vector θ(w), and P(w, x) = δ (Vi xi ≤ θ(w)i).
6
Under review as a conference paper at ICLR 2019
5.3	Equipping vector space models with probabilistic interpretations
Even though we did not list this as one of our goals, one can generalize properties 3 and 4, to go
beyond inclusion, and interpret the ratio of subspace measures as a probability:
Property 6 (Measures correspond to probabilities)
Vec P (w0, x)P (w, x)dx
' P rob(w0 |w)
Athiwaratkun & Wilson (2018) provide an approach to detect the hyponymy relation between
words. They make use of (multivariate) Gaussian densities to represent words, which was first pro-
posed by Vilnis & McCallum (2014). The idea to use Gaussian densities for representing words was
also used also by Vendrov et al. (2015) and Vulic et al. (2016). However, unlike (Vilnis & McCal-
lum, 2014; Vendrov et al., 2015; Vulic et al., 2016), but similar to what we do in our interval-based
model, Athiwaratkun & Wilson advocate supervised training to build such representations for the
hyponymy relation. Their approach outperforms those of Vendrov et al. (2015) and Vulic et al.
(2016). Even though the model is based on usage of probabilistic distributions, the probabilistic na-
ture of such representations is mainly used for modeling the uncertainty of a meaning. Plainly, they
do not classify the vector space: they have no P function and Property 3 is not considered. Instead,
to model inclusion, which is needed for hyponymy detection, Athiwaratkun & Wilson (2018)
make use of divergence measures, on the basis of which they define probabilistic encapsulation of
two densities.
A probabilistic setting of entailment is proposed by Hockenmaier & Lai (2017), based on earlier
work by Young et al. (2014); Lai & Hockenmaier (2014). The probabilistic meaning of a phrase is
a random variable that encodes the ability of that phrase to describe a randomly selected image and
they propose to embed phrases in a vector space (thus precisely fitting Property 6). Their phrase em-
beddings are inspired by the lattice model of Vendrov et al. (2015). For a phrase X , with the vector
embedding x = (x1, . . . , xn), the denotational probability is defined as p(x) = exp (- Pin=1 xi).
The joint probability of two phrases represented by vectors x and y is the probability of their join
vector z, defined as zi = maxxi, yi for every 1 ≤ i ≤ n. As Hockenmaier & Lai (2017) note, this
approach has some imperfections. Namely, for the joint probability, it holds that:
p(x, y) = p(z) ≥ exp - Xxi + yi = p(x)p(y)
Because p(x, y) = p(x|y)p(y) = p(y|x)p(x), one can infer that p(x|y) ≥ p(x) and p(x|y) ≥ p(x),
that is to say that, any two phrases (words) are positively correlated.
To overcome this problem, Vilnis et al. (2018) propose instead, box embeddings within a unit hy-
percube [0, 1]n, where a box stands for a hyperrectangle whose sides are parallel to coordinate axes.
Each word is thus represented by a box. Vilnis et al. (2018) define then the meet and join of two
boxes: their meet is an intersecting box (if any, otherwise the empty box ⊥); the join in the minimum
(in size) box that incorporates both boxes. They assume that the distribution of the underlying vector
space is uniform, and so the probability of a word is defined as the volume of the corresponding box
(p(⊥) is set to be 0) — thus precisely fitting Property 6. As Vilnis et al. (2018) show, boxes as
probabilistic random variables can have any correlation between -1 and 1, and thus avoids the afore-
mentioned problems of the previous approach. In addition, Vilnis et al. (2018) induce probabilistic
reasoning over WORDNET concepts: a node x (of a graph representation of WORDNET) is assigned
a probability p(x) = Nx/N where dxis a number of descendant nodes ofx and N is the total number
of nodes in the graph. The joint probability of two nodes x and y is defined as p(x, y) = N (x, y)/L
where N (x, y) is the total number of co-occurrences of x and y as the ancestors of the same leaf;
and L is the total number of leaves. (This is very close to the probabilities assigned by our simple
1-dimensional interval model.) Having available p(x, y), p(x), and p(y), one computes p(x|y), and
p(y|x). This is further used by Vilnis et al. (2018) to augment the training data with “soft edges”
(if p(x|y) > p(y|x) and p(x|y) is sufficiently high, Vilnis et al. (2018) argue that they can be used
in order to prune graphs without adding cycles). The data from WordNet augmented in this way
is used for training by Vilnis et al. (2018) to detect hyponymy. Their performance in terms of
accuracy is the same as of Athiwaratkun & Wilson (2018)’s model (see Table 4a).
7
Under review as a conference paper at ICLR 2019
6 Discussion, Future Work, and Conclusion
We found that defining the problem of representing hyponymy in a feature vector is not easy. Dif-
ficulties include 1. the sparseness of data, 2. whether one wants to base inclusion on an underlying
(possibly relaxed) inclusion in the space of vectors, and 3. determining what one should generalize.
Our investigation of WORDNET over fastText demonstrates that WORDNET classes are not cleanly
linearly separated in fastText, but they are sufficiently well separated to give a useful recall for an
approximate inclusion property. Despite this, and because the negative cases vastly outnumber the
positive cases, the rate of false negatives is still too high to give any reasonable precision. One could
try to use more complex models, but the sparsity of the data would make such models extremely
sensitive to overfitting.
Our second model takes a wholly different approach: we construct intervals directly from the hy-
ponymy relation. The main advantage of this method is its simplicity and high-accuracy. Even with
a single dimension it rivals other models. A possible disadvantage is that the multi-dimensional
version of this model requires disjunctions to be performed. Such operations are not necessarily
available in models which need to make use of the hyponymy relation. At this stage, we make no
attempt to match the size of intervals to the probability of a word. We aim to address this issue in
future work.
Finally, one could see our study as a criticism of WordNet as a natural representative of hy-
ponymy. Because it is almost structured like a tree, one can suspect that it in fact misses many
hyponymy relations. This would also explain why our simple fastText-based model predicts more
relations than present in WordNet. One could think of using other resources, such as JeuxDe-
Mots. Our preliminary investigations suggest that these seem to suffer from similar flaws — we
leave complete analysis to further work.
References
Ben Athiwaratkun and Andrew Gordon Wilson. On modeling hierarchical data via probabilistic
order embeddings. In International Conference on Learning Representations, 2018.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The berkeley framenet project. In Pro-
ceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics - Volume 1, ACL ,98, pp. 86-90, StroUds-
burg, PA, USA, 1998. Association for Computational Linguistics.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
langUage model. JOURNAL OF MACHINE LEARNING RESEARCH, 3:1137-1155, 2003.
Christiane Fellbaum (ed.). WordNet: An Electronic Lexical Database. Language, Speech, and
Communication. MIT Press, Cambridge, MA, 1998. ISBN 978-0-262-06197-1.
Julia Hockenmaier and Alice Lai. Learning to predict denotational probabilities for modeling en-
tailment. In Proceedings of the 15th Conference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 1: Long
Papers, pp. 721-730, 2017.
Dmitri V. Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marian BogUna.
Hyperbolic geometry of complex networks. Physical review. E, Statistical, nonlinear, and soft
matter physics, 823 Pt 2:036106, 2010.
Mathieu Lafourcade and Alain Joubert. JeuxDeMots : un prototype ludique pour l,emergence de
relations entre termes. In JADT08: Journees internationales d'Analyse Statistiques des Donnees
Textuelles, pp. 657-666, France, 2008.
Alice Lai and Julia Hockenmaier. Illinois-lh: A denotational and distributional approach to
semantics. In Proceedings of the 8th International Workshop on Semantic Evaluation, Se-
mEval@COLING 2014, Dublin, Ireland, August 23-24, 2014., pp. 329-334, 2014.
8
Under review as a conference paper at ICLR 2019
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Proceedings of the 26th International
Conference on Neural Information Processing Systems - Volume 2, NIPS’13, pp. 3111-3119,
2013.
Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Ad-
vances in pre-training distributed word representations. In Proceedings of the International Con-
ference on Language Resources and Evaluation (LREC 2018), 2018.
Maximillian Nickel and DoUWe Kiela. Poincare embeddings for learning hierarchical representa-
tions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6338-6347. CUr-
ran Associates, Inc., 2017.
Maximillian Nickel and DoUWe Kiela. Learning continUoUs hierarchies in the Lorentz model of
hyperbolic geometry. In Jennifer Dy and Andreas KraUse (eds.), Proceedings of the 35th In-
ternational Conference on Machine Learning, volUme 80 of Proceedings of Machine Learning
ReSearch, pp. 3779-3788, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for Word
representation. In EMNLP, volume 14, pp. 1532-1543, 2014.
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. CoRR, abs/1511.06361, 2015.
Luke Vilnis and Andrew McCallum. Word representations via gaussian embedding. CoRR,
abs/1412.6623, 2014.
Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic embedding of knowl-
edge graphs with box lattice measures. In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1: Long Papers), pp. 263-272. Association for
Computational Linguistics, 2018.
Ivan Vulic, Daniela Gerz, Douwe Kiela, Felix Hill, and Anna Korhonen. Hyperlex: A large-scale
evaluation of graded lexical entailment. CoRR, abs/1608.02117, 2016.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual
denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:
67-78, 2014.
9