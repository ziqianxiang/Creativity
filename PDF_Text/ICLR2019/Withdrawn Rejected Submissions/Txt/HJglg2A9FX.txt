Under review as a conference paper at ICLR 2019
Iteratively Learning from the Best
Anonymous authors
Paper under double-blind review
Ab stract
We study a simple generic framework to address the issue of bad training data;
both bad labels in supervised problems, and bad samples in unsupervised ones.
Our approach starts by fitting a model to the whole training dataset, but then iter-
atively improves it by alternating between (a) revisiting the training data to select
samples with lowest current loss, and (b) re-training the model on only these se-
lected samples. It can be applied to any existing model training setting which
provides a loss measure for samples, and a way to refit on new ones. We show the
merit of this approach in both theory and practice. We first prove statistical con-
sistency, and linear convergence to the ground truth and global optimum, for two
simpler model settings: mixed linear regression, and gaussian mixture models.
We then demonstrate its success empirically in (a) saving the accuracy of exist-
ing deep image classifiers when there are errors in the labels of training images,
and (b) improving the quality of samples generated by existing DC-GAN models,
when it is given training data that contains a fraction of the images from a different
and unintended dataset. The experimental results show significant improvement
over the baseline methods that ignore the existence of bad labels/samples.
1 Introduction
This paper is motivated by the problem that training large machine learning models requires well-
curated training sets with lots of samples. This is an issue in both supervised and unsupervised
settings. For supervised problems (e.g. multi-label classification) we need very many properly la-
beled examples; generating these requires a lot of human effort. Errors in labels can significantly
affect the accuracy of naive training. For example, a faulty CIFAR-10 dataset with 20% of auto-
mobile images mis-labeled as “airplane” (and so on for the other classes) leads to the accuracy of
a neural architecture like WRN of Zagoruyko & Komodakis (2016) to go from over 90% to about
70%. Even in unsupervised problems, where there are no labels, spurious samples are a nuisance.
Consider for example the task of training a generative adversarial network (GAN) Goodfellow et al.
(2014) to produce realistic images of faces. While this has been shown to work well using a dataset
of face images 1, like Celeb-A Liu et al. (2015), it degrades quickly if the training dataset is corrupted
with non-face images. Our experiments illustrate these issues.
Main idea We study a simple framework that addresses the problem of spurious data in both
supervised and unsupervised settings. Note that if we knew which samples were clean, we could
learn a model; conversely, if we had a good model, we could find the clean samples. But we do not
have either; instead we have a chicken-and-egg problem: finding the best subset of samples needs a
model, but finding a model needs a subset of samples. In this paper, we take the natural approach to
this problem by (0) fitting an initial model on all samples, and then alternating between (1) finding
the best subset of samples given a model, and (2) fitting a model given a subset. 2 Our approach is
most related to the works of Bhatia et al. (2015) and Vainsencher et al. (2017), and we contrast to
these in the related work section.
1https://github.com/carpedm20/DCGAN-tensorflow
2Our framework sounds initially similar to EM-style algorithms like k-means. Note however that EM needs
to postulate a model for all the data points, while we search over a subset and do not worry about the loss on
corrupted points. We are alternating between a simple search over subsets and a fitting problem on only the
selected subset; this is not an instance of EM.
1
Under review as a conference paper at ICLR 2019
Our approach can be applied to any existing model that provides (a) confidence scores for samples,
and (b) a way to train the model for a set of samples. Indeed in our experiments we use existing
models and training code, but with iterative (re)selections of samples as above. In supervised set-
tings, any model that maps features to responses typically associates a confidence to every sample,
e.g., in linear regression, given a model θ, the confidence in a sample (xi, yi) increases as its squared
error (yi - xi>θ)2 decreases. For a neural network classifier, on the other hand, the confidence can
be measured by the cross entropy loss between the output probability distribution and the true dis-
tribution. In unsupervised settings, the confidence of a sample is given by its “likelihood” under
the model. If we are lucky, e.g., in Gaussian mixture models, this likelihood is explicit and can be
evaluated. However, if there is no explicit likelihood, we need to rely on a surrogate. E.g., in our
GAN setting below, we rely on the output of the discriminator as a confidence measure. We also
note an important caveat: this paper is not focused on (and may not be effective for) the settings of
adversarial examples, or of gross corruptions of features. Rather, our focus is to address the issue of
bad labeling or curation of data, as motivated above.
Our Results and Paper Outline
We show that our framework (outlined in Section 3) can address the spurious sample problem, both
theoretically and empirically and for both supervised and unsupervised problems:
(1)	Mixed linear regression (Section 4) and Gaussian mixture models (Section 5): For each
of these mixture problem settings, under the standard statistical assumptions, we establish both
statistical consistency and linear convergence of our iteratively updated model parameters. This
holds true from any initial point; the convergence is to the mixture component that is closest to the
initial point.
(2)	Neural network classifiers (Section 6) : We investigate image classification in CIFAR-100,
CIFAR-10 and MNIST datasets (each with a different and appropriate existing neural architecture)
in the setting where there are systematic errors in the labels - i.e. every bad example in one label
class comes from the same other label class. We show that our framework can significantly improve
the classification accuracy over the baseline method that ignores the existence of bad samples.
(3)	Deep generative models (Section 7): We show both quantitatively and qualitatively the im-
provement of using our algorithm for image generation when the dataset consists of two types of
images. Specifically, investigate training a GAN to (a) generate face images as in the “intended”
Celeb-A dataset, but when the given training data has a fraction of the samples being the “spurious”
CIFAR-10 images, and (b) generate intended MNIST digits when training data contains a fraction of
samples from the spurious Fashion-MNIST dataset. A crucial innovation here is that the output of
the discriminator network in the GAN can be used as a confidence metric for the training samples.
In Section 8, we discuss the positives and negatives of our results, underlying intuition and also
some future directions. All of our experiments were done on clearly identified publicly available
neural network architectures, datasets, and training software. We also clearly describe our error and
noise effects; we expect them to be easily reproducible.
2 Related Work
Alternating minimization for robustness Two works are most related to ours. Bhatia et al. (2015)
proposes an iterative hard thresholding algorithm for adversarial noise in linear regression responses;
they establish statistical guarantees and linear convergence under this harder noise model. We focus
on multiple mixed linear regression, a different setting that results in an easier noise model, and
we provide convergence guarantee for recovering any single mixture component starting from a
local region. In addition we also prove these for Gaussian mixture models, which is not studied in
their work. Vainsencher et al. (2017) proposes what is essentially a soft version of our method, and
proves local convergence and generalization. However they do not have any initialization, and hence
no global or consistency guarantees. Neither of these empirically explore the overall approach for
more complex / neural network models.
2
Under review as a conference paper at ICLR 2019
Robust regression Recent work on robust regression consider strong robustness in terms of both
the inputs and the outputs and provide guarantees for constant corruption ratio Diakonikolas et al.
(2018); Prasad et al. (2018); Klivans et al. (2018). Chen et al. (2013); Balakrishnan et al. (2017a);
Liu et al. (2018) study the high dimensional setting under this regime. These algorithms usually re-
quire much more computation, compared with methods for dealing with noisy outputs, e.g., Bhatia
et al. (2015). Another type of robustness considers heteroscedastic noise for every sample. From
the learning perspective, Anava & Mannor (2016); Chaudhuri et al. (2017) both require strong con-
straints on the noise model which largely reduces the degree of freedom. Anava & Mannor (2016)
considers the online learning setting, while Chaudhuri et al. (2017) considers the active learning
setting.
Mixed linear regression Alternating minimization type algorithms are used for mixed linear re-
gression with convergence guarantee Yi et al. (2014). Chen et al. (2014) provides a convex formula-
tion for the problem. Balakrishnan et al. (2017b) shows expectation-maximization (EM) algorithm is
provably effective for a set of tasks including the basic setting of mixed linear regression and Gaus-
sian mixture model. Sedghi et al. (2016) extends to the multiple component setting, where provable
tensor decomposition methods are used. More recently, Li & Liang (2018) gives a more general
result for learning mixtures of linear regression. On the other hand, Ray et al. (2018) studies the
problem of finding a single component in mixed linear regression problem using side information.
Noisy labels Classification tasks with noisy labels are also of wide interest. Frenay et al. (2014)
gives an overview of the related methods. Theoretical guarantee for noisy binary classification has
been studied under different settings Scott et al. (2013); Natarajan et al. (2013); Menon et al. (2016).
More recently, noisy label problems have been studied for deep neural network based models. Reed
et al. (2014) and Malach & Shalev-Shwartz (2017) develop the idea of bootstrapping and query-by-
committee into neural network based models. On the other hand, Khetan et al. (2018) and Zhang &
Sabuncu (2018) provide new losses for training under the noise. Sukhbaatar & Fergus (2014) adds a
noise layer into the training process, while Ren et al. (2018b) provides a meta-algorithm for learning
the weights of all samples by referencing to a clean subset of validation data during training. Ren
et al. (2018a) consider building robust classifiers for the positive-unlabeled classification problem
using l0 norm penalization.
Others Bora et al. (2018) studies the problem of noisy images for GANs. In their setting, all
images are of low quality under some measurements, while in our problem, we consider the image
dataset consists of (possibly more than) two types of images, which is different. There are also
studies that consider noisy samples more generally. The classical RANSAC method Fischler &
Bolles (1981) provides a paradigm for fitting a model to experimental data, and can be potentially
used for the noisy data setting. However, the algorithm requires multiple times of random sampling
to find a valid consensus score, which is conceptually different from our idea of doing iterative
filtering. EM algorithm Moon (1996) can also be used for detecting noisy samples, e.g., in the
setting of mixture models Balakrishnan et al. (2017b). However, our algorithm does not need to
know the likelihood for the bad samples for the iterative updates.
3	Iteratively Learning from the Best ( ILFB )
We now describe the setup and algorithm, in a unified way that covers both supervised and unsu-
pervised settings. We will then specialize to the many specific problems we study. We are provided
with samples si, ∙∙∙ ,sn, using which we want to train a model parameterized by θ ∈ Rd. We are
also provided with a loss function; let fθ(s) denote the loss ofa sample s when the parameters are
θ. Let τ ∈ [0, 1] be the fraction of data we want to fit. With this setup, the idea of learning from the
best can be viewed as trying to solve
min min	fθ (si).
θ S:|S|=Tn 乙 '
:	=τn i∈S
We do this by alternating between finding S and finding θ, as described below in Alg. 1.
3
Under review as a conference paper at ICLR 2019
Algorithm 11LFB Update
1:	input: samples {si}in=1, fraction τ, max. iteration number T
2:	(optional) initialize: set t J 0 and initially fit all samples θo J argminθ Pi∈[n] fθ(Si),
3:	while t < T do
4:	calculate current losses fθt (si) for all samples si, i ∈ [n]
5:	find new best set of samples
St J arg min	fθt (si)
S：|S|=Tn
i∈S
by sorting them according to losses fθt (si) and choosing the ones with smallest losses.
6:	fit a model for this new set
θt+1 J arg min	fθ(si)
i∈St
7:	t J t + 1
8:	return: θT
Comments Steps 4, 5 and 6 above can be done approximately, when exact solutions are unavail-
able. For example, in our experiments involving deep neural networks (both the classifiers and the
generative models), for step 6 we do not fit a model exactly, but rather do training using stochas-
tic gradient descent. Also, for our experiments on training GANs on the best subset, we do not
have an explicit loss; we instead use the loss at the output of the discriminator as a surrogate (more
details on that below). Similarly, the initialization step above can possibly be replaced with some
problem-setting dependent alternative, if one is available.
In the following, we describe how this directly specializes to each of our four settings.
4	ILFB for Mixed Linear Regression
We first describe ILFB as specialized to the case when the θ represents the parameters of a linear
regression, and then describe the statistical setting of mixed linear regression where we provide
rigorous statistical guarantees on its performance.
Algorithm The samples are si = (xi, yi) with features xi ∈ Rd and responses yi ∈ R, and
θ ∈ Rd are the parameters of the linear model. The loss is fθ (x, y) = (y - x>θ)2. The algorithm
thus initializes θ0 by doing an ordinary least squares (OLS) on all the samples, and then alternates
between finding the set of τn samples with the lowest squared error for the current θ, and then
finding the new θ by doing an OLS on this set.
Statistical Setting In the standard and widely studied mixed linear regression problem there are
n samples (xi, yi), with features Xi 〜N(0, Id) being standard gaussians in d dimensions. The full
set of samples S = [n] can be splitted into m sets S = ∪j∈[m] Sj, each corresponding to samples
that generate from one mixture component, |Sj | = Tjn. The response variables yi are given by:
yi = xi>θ(j) + i, for i ∈ Sj	(1)
where G 〜N(0, σ2) is the additive noise, and θ(j)s are assumed to be unit norm vectors. We
analyze our algorithm in the fresh sample setting, where iteration t finds θt+1 from θt using a
new set of samples. This too is standard in the statistical analysis of mixed linear regression, and
corresponds to the case of using mini-batches of samples to learn a model.
We now prove two results: the first shows that the iterates converge linearly to some θ(j) once θt
is closer enough to θ(j ) than any of the other components, and the second shows that the simple
initialization step ensures a θ0 that is close enough to the dominant component θ(j ) under mild
assumptions.
4
Under review as a conference paper at ICLR 2019
Theorem 1 (local linear convergence). For the mixed linear regression setting as above, we run
ILFB as shown in Algorithm 1. Suppose for some j ∈ [m], the iterate θt and τ satisfy:
(C1) τ ≤ 0.99τj?;
(C2) I∣θt - θ(j)∣∣2 ≤ C(T)minι∈[m]∖{j} ∣∣θt - θ(l)k2 - P - C2(τ)σ, where C(T)=
min{ c1-3τ ,1}.
Then, the next iterate θt+1 of the algorithm satisfies
Iθt+1 - θ(j)I2 ≤ c1Iθt - θ(j)I2 + (c1 + c2)σ
with high Probability (i.e., with probability 1 — n-c0), provided the number ofsamples n ≥ CdCoTd,
where c1 ∈ (0, 1), c, c0, c2, c3 are constants.
Theorem 1 establishes a linear dependence of the sample complexity on the dimension d. The
required sample size increases as T gets smaller. For T large enough such that C(T) = 1, the
condition on θt does not depend on σ . However, notice that for smaller T, the condition of θt
tolerates smaller noise. This is because, even if θt is very close to θ(j), when the noise and the
density of samples from other mixture components are both high, the number of samples from other
components selected by the current θt would still be quite large, and the update will not converge to
θ(j).
The proof for the theorem is in Appendix C.1, and synthetic experiments verifying the performance
of ILFB are in Appendix C.2. In Appendix C.2, we show that our guarantee for the algorithm is
tight, in the sense that there always exist a σ dependency even if we take infinite samples. In the
noiselss setting, our theorem guarantees local linear convergence with exact recovery.
5 ILFB for Gaussian Mixture Model
In this section, we describe ILFB applied to the setting of Gaussian mixture model, and provide
statistical guarantees on its performance.
Algorithm The samples si = xi are points in Rd, θ ∈ Rd is the mean we want to estimate, and
fθ(x) is proportional to the negative log-likelihood, i.e., fθ(x) = Ix - θI22. The algorithm thus
initializes the mean by finding the average of all n points, and then iteratively finds the set of Tn
points closest to the current mean, and updating the mean to be the average of this set.
Statistical setting Given a dataset D = {xi | i ∈ S = {1, .…,n}}, where S can be SPlitted into
disjoint sets S = ∪j∈[m]Sj, |Sj | = Tj?n. The samples are generated following:
Xi ~ N (θj) ,σ2I) , for i ∈ Sj.	(2)
We now Prove two results: the first shows that iterates converge linearly under the fresh samPle set-
ting in a local region, while the second shows with some additional conditions, that the initialization
of ILFB falls into the local region of the dominant comPonent.
Theorem 2 (local linear convergence). We run Alg. 1 on the given dataset D generated following
(2). Suppose for some j ∈ [m], the iterate θt and parameter T in Algorithm 1 satisfy:
(C1) T ≤ 0.99Tj?;
(C2) kθt - θ(j)k2 ≤ 0.9minl∈[m]∖{j} kθt - θ(l)k2 - c3σ ʌ/lθg 2(1-τ) l{τ< i }
then, with high probability (i.e., with probability 1 - n-c0),
Iθt+1 - θ(j)∣2 ≤ cι∣θt - θ(j)∣2 + C2√dσ,
where c1 ∈ (0, 1), c0, c2, c3 are constants.
5
Under review as a conference paper at ICLR 2019
Table 1: Classification with systematic label error: Performance for MNIST, CIFAR-10, CIFAR-
100 datasets as the ratio of good samples varies from 60% to 90%. Here Baseline : Naive training
using all the samples; ILFB : Our iterative update algorithm; Oracle : Training with all good
samples. We see significant improvement of ILFB over Baseline for all the settings.
dataset	MNIST			CIFAR-10			CIFAR-100		
# good # total	Baseline	ILFB	Oracle	Baseline	ILFB	Oracle	Baseline	ILFB	Oracle
60%	70.26	90.00	94.30	64.63	75.16	91.26	58.28	65.25	80.39
70%	85.95	92.09	94.50	71.05	86.09	92.25	66.75	73.83	81.65
80%	92.62	94.30	94.61	78.84	89.14	92.85	72.68	78.70	82.50
90%	93.88	94.41	94.92	84.12	91.03	93.47	78.12	81.10	83.41
Here, the 0.9 factor in the assumption is chosen for convenience of the analysis. Essentially, what
We need is this value being less than one. For τ < 1, we require an additional separation term that
depends on σ. This term essentially implies a √log kσ separation distance (consider all mixture
components have weights 1/k), which is the lower bound for computationally efficient algorithm to
learn a Gaussian mixture model according to Regev & Vijayaraghavan (2017). Intuitively, given the
required initialization condition, the samples selected by the initialization parameter contain more
than half of the good samples, and we can show that the algorithm is guaranteed to converge. Notice
that there is no dependency between n and d. In the extreme case where σ = 0, we only need n to
be a constant value: from any good initialization point, the algorithm will converge to θ(j) in one
step.
The proof for the theorem is in Appendix D.1, and synthetic experiments verifying the performance
of ILFB are in Appendix D.2. Again, in Appendix D.2, we show that the our guarantee of the
algorithm is tight.
6	ILFB for Deep Image Classification with S ystematic Training
Label Error
Experimental setting We consider systematic errors in labels, i.e. the setting where all the sam-
ples that actually come from a class “a” are given the same bad label “b”, a setting that is less benign
than one with more randomness in the errors. We investigated the ability of our framework to ac-
count for these errors for the following dataset - neural architecture pairs:
(a)	MNIST LeCun et al. (1998) dataset with a standard 2-layer CNN 3
(b)	CIFAR-10 Krizhevsky & Hinton (2009) with a 16-layer WideResNet 4 Zagoruyko & Komodakis
(2016)
(c)	CIFAR-100 5 with a 28-layer WideResNet 6
In each of these cases, the neural network has excellent performance when there are no label errors
in training; this degrades significantly as the fraction of spurious samples increases.
Algorithm In each case, the algorithm involves first training the NN on all the samples, and then
iteratively alternating between (a) picking the fraction τ of samples in each label class with the
lowest cross-entropy loss under the current θ, and (b) retraining the model from scratch with these
picked samples, to get a new θ . Training was done on MXNet. Mapping back to the development
in Section 3, here θ represents the weights of the neural network, and fθ (s) is the standard cross-
entropy loss used in training.
Further details When training on all the samples, we run simple stochastic gradient descent
algorithm with initial learning rate 0.5/0.1/0.1 and batch size 1000/256/64 for MNIST/CIFAR-
10/CIFAR-100, respectively. The learning rate is divided by 5 at the 50-th epoch and each ex-
3follow the implementation in https://mxnet.incubator.apache.org/tutorials/python/mnist.html
4cifar_wideresnet16_10 from the online model zoo: https://gluon-cv.mxnet.io/api/model_zoo.html
5We use the coarse labels as the classification target
6WideResNet: depth 28, width factor 10, dropout rate 0.3
6
Under review as a conference paper at ICLR 2019
Table 2: Generative models from mixed training data: A quantitative measure of the efficacy of
our approach is to find how many of the good training samples the final discriminator can identify;
this is shown here for the three different “good”/“bad” dataset pairs. For each pair, the fraction
of “good” samples is 90%, 80% or 70%. The table depicts the ratio of the good samples in the
training data that are recovered by the discriminator when it is run on the training samples. The
higher this fraction, the more effective the generator. For MNIST-Fashion and CelebA-CIFAR10,
our approach shows significant improvements with iteration count. For CIFAR10-CelebA dataset,
the error is extremely simple to be corrected, likely because faces are easier to discriminate against
when compared to natural images.
MNIST(good)-Fashion(bad) CIFAR10(good)-CelebA(bad) CelebA(good)-CIFAR10(bad)
orig	90%	80%	70%	90%	80%	70%	90%	80%	70%
ILFB iter-1	91.90%	76.84%	77.77%	100.0%	99.99%	98.67%	97.12%	81.34%	75.57%
ILFB iter-2	96.05%	91.95%	79.12%	100.0%	99.85%	99.10%	97.33%	88.11%	76.45%
ILFB iter-3	99.15%	96.14%	85.66%	100.0%	99.91%	99.69%	97.43%	89.48%	86.63%
ILFB iter-4	100.0%	99.67%	91.51%	100.0%	99.96%	99.75%	97.53%	92.89%	82.15%
ILFB iter-5	100.0%	100.0%	97.00%	100.0%	99.99%	99.95%	98.14%	92.94%	94.02%
periment runs for 80 epochs. For each iteration of update using our ILFB algorithm, since less
samples are used at each epoch, we adjust the total epoch number accordingly, so that the training
for every iteration uses the same number of SGD updates. We set τ to be 5% less than the true ratio
of “good” samples.
Results Table 1 shows the results for the baseline, oracle and our methods. Please see the caption
thereof. We observe significant improvement over the baseline under all experiments. In Appendix
B, we provide a comparison of using different initialization methods, where ILFB performs better
than its counterpart with random initialization. In Appendix E, we provide more experimental results
illustrating that ILFB is not sensitive to mis-specified τ , the improvement of this iterative procedure
compared with a single iteration, the advantage over other outlier removal options, and its capability
of handling random label noise.
7	ILFB for Deep Generative Models with Mixed Training Data
Experimental setting We consider training a GAN - specifically, the DC-GAN architecture Rad-
ford et al. (2015) - to generate images similar to those from a good dataset, but when the training
data given to it contains some fraction of the samples from a different bad dataset. All images are
unlabeled, and we do not know which training sample comes from which dataset. We investigated
the efficacy of our approach in three such settings:
(a)	When the good dataset is the Celeb-A face images, and the bad dataset is CIFAR-10.
(b)	When the good dataset is the CIFAR-10 face images, and the bad dataset is Celeb-A.
(c)	When the good dataset is MNIST digits, and the bad is Fashion-MNIST Xiao et al. (2017).
For each of these, we consider different fractions of bad samples in the training data, evaluate their
effect on standard GAN training, and then the efficacy of our approach as we execute it for upto 5
iterations.
Algorithm Recall that training a GAN consists of updating the weights of both a generator net-
work and a discriminator network; our model θ is the parameters of both of these networks. Unlike
the classification seeing, we now need to develop a loss fθ (s) - and unlike in the simple Gaussian
mixture model setting we do not have explicit access to a likelihood function for the generative
model. Our crucial innovation is to use the loss7 at the output of the discriminator - the same one
used to train the discriminator - as the fθ (s). The algorithm starts by training on all samples, and
then alternates between picking training samples with the smallest discriminator loss, and retraining
7Notice that for different GAN architectures, the loss function for training the discriminator varies, however,
we can always find a surrogate loss by modifying the original loss function, and use the loss of the discriminator
for real images as the surrogate loss for ILFB .
7
Under review as a conference paper at ICLR 2019
(a) baseline
(b) 1st iteration
10 0 23∕Bi37
e5gG3z7l
s∕8λ6gŋz
。，1/10 q3 夕<
3〃<?。臼133
√ΓOr05c*ħ<py
3 7 7Q qΛo /
O ,6H3q0。
(c)	3rd iteration
(d)	5th iteration
Figure 1: Qualitative performance of ILFB for GANs: We apply ILFB to a dataset of 80% MNIST
“good” images + 20% Fashion-MNIST “bad”. The panels show the fake images from 32 randomly
chosen (and then fixed) latent vectors, as ILFB iterations update the GAN weights. Baseline is the
standard training of fitting to all samples. We can see that the baseline generates both digit and
fashion images, but by the 5th iteration it hones in on digit images.
Figure 2: Qualitative performance of ILFB for GANs: Given training data consisting of 70%
CelebA “good” images + 30% CIFAR-10 “bad” images, the four panels above each show the per-
formance after iterations of our ILFB algorithm. First we choose 18 random vectors in latent pace,
and fix them for all iterations. In each iteration, we retrain the GAN on corresponding selected sam-
ples to get new weighs, which are then used to generate the 18 fake images. Baseline refers to the
standard training where we fit to all the samples (this is also our initialization). Visually, we can see
that the generator is able to improve its generation quality and only generate face-like images after
the 5th iteration.
the model on these picked samples. Here training means updating both the generator and discrimi-
nator network weights, which is done via SGD. Again, we set τ to be 5% less than the true ratio of
“good” samples.
8
Under review as a conference paper at ICLR 2019
7OAltBMO0
n960R2偷 7
工3/夕W,O 3
7/7丹
(b) 1st iteration
(c) 3rd iteration
(d) 5th iteration
(a) baseline
Figure 3: Illustrative failure case: This figure shows that when the fraction of “bad” samples is too
large, ILFB cannot clean them out. The setting is exactly the same as in Figure 1, but now with 60%
MNIST “good” images + 40% Fashion-MNIST “bad” images. We can see that now the 5th iteration
still retains the fake fashion images. Please see the discussion section for some intuition on this.
Results We evaluate the effect of bad training samples, and the improvement by our approach,
in two ways: quantitatively in Table 2 and qualitatively in Figures 1 and 2. Figure 3 illustrates a
failure case, when the bad set is too big, which we feel gives insight into what is going on. Please
see the respective captions. The failure example happens for MNIST-Fashion dataset when ratio of
MNIST images is 60%. In fact, for every iteration, ILFB selects all images from FashionMNIST
(which counts for 0.4n) and 0.15n MNIST images (the least number of MNIST images the algorithm
can select). See Fig. 3 for qualitative evaluation of the results. In Appendix E, we provide more
experimental results illustrating that ILFB is not sensitive to mis-specified τ and the advantage over
other outlier removal options.
8	Discussion
We demonstrated, both theoretically for simpler settings, and empirically for more challenging neu-
ral network ones, the merit of iteratively fitting to the best samples - in both supervised and UnsUPer-
vised problems. The underlying intuition is that ILFB can focus in on the “good” samples, provided
it is properly initailized; and when the nUmber of bad samples is significant bUt not overwhelming,
initial fitting on all the data is good enoUgh to get started. We now add several discUssions on the
merits and otherwise of oUr resUlts:
(1)	One way to view the effect of ILFB in the GAN setting: a discriminator is likely least certain
on the smallest modes of a distribUtion, and oUr process of dropping training samples based on this
resUlts in the elimination of smaller modes, which correspond to modes from the bad samples. This
is illUstrated by the failUre case in Fig. 3, where the presence of a high fraction of ”bad” fashion-
MNIST samples makes their modes comparable to those of the ”good” MNIST.
(2)	Retaining samples best fit by the cUrrent model is inherently local, in the sense that samples with
large errors are ignored. Finding a good initialization to start with is thUs important; withoUt this we
may have bad performance. When the size of the good samples is comparatively large, an initial fit
on all the samples is good enoUgh; otherwise, we may Use other alternatives to initialize the model,
e.g., fit the model on a smaller dataset with clean data.
(3)	The ILFB algorithm is simple and efficient enoUgh to be applied to most modern machine learn-
ing tasks. When there exists extra compUtation power or knowledge on the specific dataset, it is
certainly possible to improve based on ILFB . However, we believe ILFB is simple and general
enoUgh, and may serve as a strong resUlt when more complicated oUtlier detection or noisy label
algorithms are proposed.
9
Under review as a conference paper at ICLR 2019
References
Oren Anava and Shie Mannor. Heteroscedastic sequences: beyond gaussianity. In International
Conference on Machine Learning, pp. 755-763, 2016.
Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally efficient robust
sparse estimation in high dimensions. In Conference on Learning Theory, pp. 169-212, 2017a.
Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em
algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120,
2017b.
Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding. In
Advances in Neural Information Processing Systems, pp. 721-729, 2015.
Ashish Bora, Eric Price, and Alexandros G. Dimakis. AmbientGAN: Generative models from lossy
measurements. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=Hy7fDog0b.
Stephane Boucheron, Maud Thomas, et al. Concentration inequalities for order statistics. Electronic
Communications in Probability, 17, 2012.
Kamalika Chaudhuri, Prateek Jain, and Nagarajan Natarajan. Active heteroscedastic regression. In
International Conference on Machine Learning, pp. 694-702, 2017.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial
corruption. In International Conference on Machine Learning, pp. 774-782, 2013.
Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression
with two components: Minimax optimal rates. In Conference on Learning Theory, pp. 560-604,
2014.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stew-
art. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815,
2018.
Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting
with applications to image analysis and automated cartography. Communications of the ACM, 24
(6):381-395, 1981.
Beno^t Frenay, Ata Kaban, et al. A comprehensive introduction to label noise. In ESANN, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ashish Khetan, Zachary C. Lipton, and Anima Anandkumar. Learning from noisy singly-
labeled data. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=H1sUHgb0Z.
Adam R. Klivans, Pravesh K. Kothari, and Raghu Meka. Efficient algorithms for outlier-robust
regression. In Conference on Learning Theory, pp. 1420-1430, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yuanzhi Li and Yingyu Liang. Learning mixtures of linear regressions with nearly optimal com-
plexity. In Conference on Learning Theory, pp. 1125-1144, 2018.
Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. High dimensional robust sparse
regression. arXiv preprint arXiv:1805.11643, 2018.
10
Under review as a conference paper at ICLR 2019
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Eran Malach and Shai Shalev-Shwartz. Decoupling” when to update” from” how to update”. In
Advances in Neural Information Processing Systems, pp. 961-971, 2017.
Aditya Krishna Menon, Brendan Van Rooyen, and Nagarajan Natarajan. Learning from binary
labels with instance-dependent corruption. arXiv preprint arXiv:1605.00751, 2016.
Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6):
47-60, 1996.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust esti-
mation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Avik Ray, Joe Neeman, Sujay Sanghavi, and Sanjay Shakkottai. The search problem in mixture
models. Journal of Machine Learning Research, 18(206):1-61, 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.
Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated gaussians. In
Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pp. 85-96.
IEEE, 2017.
Ke Ren, Haichuan Yang, Yu Zhao, Wu Chen, Mingshan Xue, Hongyu Miao, Shuai Huang, and
Ji Liu. A robust auc maximization framework with simultaneous outlier detection and feature
selection for positive-unlabeled classification. IEEE transactions on neural networks and learning
systems, 2018a.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, StockholmSmaSsan, Stockholm, Sweden, July 10-15, 2018, pp. 4331T340, 2018b.
URL http://proceedings.mlr.press/v80/ren18a.html.
Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise:
Consistency and maximal denoising. In Conference On Learning Theory, pp. 489-511, 2013.
Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning
mixtures of generalized linear models. In Artificial Intelligence and Statistics, pp. 1223-1231,
2016.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2(3):4, 2014.
Daniel Vainsencher, Shie Mannor, and Huan Xu. Ignoring is a bliss: Learning with large noise
through reweighting-minimization. In Conference on Learning Theory, pp. 1849-1881, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear
regression. In International Conference on Machine Learning, pp. 613-621, 2014.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.
Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. arXiv preprint arXiv:1805.07836, 2018.
11
Under review as a conference paper at ICLR 2019
A Effect of mis-specification
A. 1 Mis-specification for a clean dataset
We test the performance of ILFB under mis-specified τ when the dataset is clean, i.e., there is only
a single component. We choose random θ? ∈ Rd with unit norm, the mis-specified τ is set as 0.95,
dimension d = 100, and sample size n ∈ {500, 1000, 1500}, T = 30. As shown in Fig. 4, as σ gets
larger, the gap between ILFB and OLS estimator (in terms of `2 distance) increases linearly.
noise level
Figure 4: Effect of mis-specification for linear
regression model
Figure 5: Effect of mis-specification for mixed
linear regression
We further show the performance of ILFB as sample size increases. In Fig. 6, the asymptotic
performance under different noise levels is shown. For the clean dataset, ILFB is observed to give
consistent estimate. We observe similar performance in the Gaussian setting as well.
asymptotic behavior	asymptotic behavior	asymptotic behavior
Scrntt=-W-
-W*	HH*
number Ofsamples
I
-W*	HH*
number Ofsamples
number Ofsamples

Figure 6: Consistency of ILFB linear regression under different noise levels (σ = 0.1, 1, 10 from
left to right).
A.2 Mis-specification for a noisy dataset
In this part, we test the performance when using different τ values for a noisy dataset. Again, we
present the result for the mixed linear regression setting. We run ILFB for 10 iterations and the
results are based on an average of 50 random runs. In Fig. 5, we see that the performance is not
sensitive to the mis-specified τ in a large region. Interestingly, ILFB still performs well when τ is
slightly larger than the true ratio of the component we are interested in.
B	Random Initialization for Classification
We study the effect of initialization for the classification problem using MNIST (5%) dataset. In
Table 3, we present the accuracy after 5-th iteration of training for both random initialization and
the original ILFB methods. Even with random initialization, the performance is consistently better
than the baseline, however, the improvement of accuracy is smaller when compared to the original
ILFB , which uses all samples for training as initialization.
12
Under review as a conference paper at ICLR 2019
#good
#total
60%	70%	80%	90%
Baseline	70.26
Random Initialization + ILFB 82.05
ILFB	90.00
85.95
91.48
92.09
92.62
93.80
94.30
93.88
93.79
94.41
Table 3: MNIST-5% dataset with systematic label error using ILFB with different initialization
methods. The accuracy at the 5-th iteration is shown for both initialization methods. Results are
averaged over 5 runs.
C Linear Regression Model
Since we focus on recovering the parameters of one mixture component from the mixed linear
regression model, we refer to the targeted component as the ‘good’ component, and the rest are
‘bad’ components. More specifically, the full set of samples S = [n] can be splitted into a set of
“good” samples Sgood, and m sets of “bad” samples ∪j∈[m] Sbjad. The response variables yi are
given by:
xi>θ? + i, ifi ∈ Sgood
yi=	xi>θ(j) + i,	ifi∈Sbjad
where Ei 〜N(0, σ2) is the additive noise, and θ? as well as allthe θ(j) are assumed to be unit norm
vectors.
C.1 Proof for update steps in linear regression model
For simplicity of the notation, we present the proof by assuming yi for bad samples have the form
yi = ni + ei, where ni corresponds to xi>θ(j), such that i ∈ Sbjad.
Let θt be the current learned parameter. θt+1 is learned by using one iteration in Algorithm 1.
Namely, we first select a subset St of size τn with the smallest loss (yi - xi>θt)2, then re-calculate
the MLE estimator only using those samples. Denote Wt as the diagonal matrix whose diagonal
entry Wii equals 1 when the i-th sample is in set St, otherwise equals 0. Then,
Θt+1 = (X>WtX)-1 X>Wty,
(3)
where we used the fact that Wt2 = Wt . Notice that W? is the ground truth for Wt (accordingly,
define S? for the ground truth of St). For clearness of the presentation, we ignore the subscript t
when there is no ambiguation. Then, according to the definition of y and reorganizing the terms, (3)
can be expanded as
θt+1 - θ? = (X>WX)-1 X>W (W*Xθ* + (I - W?) n + e) - θ?
=(X>WX)-1 (X>WW*Xθ* + X>Wn - X>WW*n - X>WXθ? + X>We)
=(X>WX)-1 X> (WW? - W)(Xθ? - n - e) + (X>WX)-1 X>WW*e
where as we addressed at the beginning of this section, for vector n, given i ∈ Sbad, ni = xi>θ(j)
for some j ∈ [m]. Therefore,
kθt+1 - θ*∣∣2
=(X>WX)-1 X> (WW? - W)(Xθ? - Xθt + Xθt - n - e) + (X> WX)T X>WW*e
≤∣∣(X>WX)T( ∙ ∣∣X> (WW? - W)(Xθ? - n - e)∣∣2 + ∣∣(X>WX)-1 X>WW*e(
z
|
|
z
z
(4)
We bound the three terms on the right hand side of (4) separately.
13
Under review as a conference paper at ICLR 2019
Helpful lemmas
Lemma 3 (Sub-Gamma property for the q-quantile). Let n ≥ 3, let X(i)≥ ∙∙∙ ≥ X(n)be the order
statistics of the absolute value of standard Gaussian samples. qn is an integer for some constant
q ∈ (0，1)∙ Let Vn = qn k,g2 ∙ Forall 0 ≤ λ < 2√Vn，
logE heλ(x(qn)-E[X(qn)])i ≤ —-vnλ 「、	(5)
2(1-2λ √!)
This shows the sub-gamma property of X(qn), and as a result, for all t > 0, and constant 2 < q < 1,
E X(qn)
P X(qn) -
log n ≤ n-c1
(6)
(7)
This result is generalized from the result in Boucheron et al. (2012).
Lemma 4. Suppose we have two Gaussian distributions D1 = N(0, ∆2), D2 = N(0, 1)∙ We have
fn i∙i∙d∙ SamPIeSfrom Di and (1 一 f )n i∙i∙d∙ samplesfrom D2∙ Denote the set ofthe top fn samples
with smallest abstract values as Sfn, where f ≤ f ∙ Then, with high probability, for ∆ ≤ 1, at most
(C max {△ (1 — f) n √log n, log n}) samples in Sfn are from D2 ∙
Proof of Lemma 4∙ First, let us consider the probability of a random variable from D2 has smaller
abstract value than a random variable from D1 . This would give us a rough idea about the occupancy
of the samples from D2 in the top set. Denote the two variables from D1 , D2 as u1 , u2 respectively.
Then, with basic calculations, we have
P [|u2| ≤ |u1|]
∞P[∣uι∣≥ s]
0
2	-〜
I_e 2 ds
√2∏
s2
∞ —上 2 上，
≤	2e 2∆2	,_e 2 ds
~ Jo	√2∏
=2 √d+∆2.
This calculation implies that if given same number of samples from D1 and D2, it is reasonable
to expect that only around 2∆ fraction of the top set samples are from D2, when △ is small. In
the original setting where We have fn samples from Di and (1 — f n from D2, We should expect
around 2∆ 1-f fn = 2∆(1 — 7) samples in the top set are from D2.
Now, we aim for a rigorous proof of the result with good concentration. Our idea is to first show
concentration result for the maximum value from distribution Di, and then, we show the high prob-
ability upper bound for the number of samples from D2 that are less than the maximum value.
Step I - the sample with maximum abstract value from Di. We know that for random normal
i.i.d. Gaussian variables xi , i ∈ [n],
_	, ,	t~~ ------ t2
P max |xi| ≥ ʌ/2 log 2n + t ≤ 2e ɪ.
i∈[n]
Therefore, for f n samples from Di, with high probability, the maximum abstract value is in the
order of O(√log n△).
Step II. On the other hand, for a random u ~ D2, we know that for small positive values δ =
c√log n△, P [∣u21 ≤ δ] ≤ ∏∏δ gives a tight upper bound. Let Mδ,i be the event sample Ui from
14
Under review as a conference paper at ICLR 2019
D2 has abstract value less than δ, and a Bernoulli random variable mi,δ that is the indicator of event
Mδ,i holds or not. Then,
E
(1-f)n
mi,δ
i=1
≤ Wδ (1 - f) n.
For independent random variable XiS, i ∈ [n] that lie in interval [0,1], with X = Ei Xi and μ
E [X], Chernoff’s inequality tells us
μγ2
P [X ≥ (1+ Y) μ] ≤ e-F, ∀γ ∈ [0,1]
P [X ≥ (1+ Y) μ] ≤ e-μγ, ∀Y > 1
As a consequence, for mi,δs, we have
(I-f )n	w
P E mi,δ ≥ (1 + Y)V-δ (1 - f) n
i=1	π
∀Y ∈ [0, 1]
(1-f)n
P	Xmi,δ
i=1
≥ (1 + Y) y 2δ (1 - f) n ≤
e-
^3
, ∀Y > 1.
For the first case, where Y ∈ [0, 1], we can setY
/ log n
CV δ(1-f)n
to get high probability guarantee. The
constraint on Y requires δn > c log n for some fixed c. On the contrary, when this is violated, i.e.,
whenδ is much smaller, then, by the Chernoff bound for the case Y > 1, we can set Y
C log n
δ(1-f )n .
Combining Step I and Step II. To summarize, for some fixed constant c, with high probability:
•	For ∆ > CVzlogn, at most 2cδ (1 - f) n = c√logn∆ (1 - f) n samples in Sfn are from
D2.
•	For ∆ ≤ c*g n, at most (1 + Y) δ (1 - f) n = C log n samples in Sfn are from D?.
Lemma 5. Assume τ < τ? ( τ /τ? = Cτ < 1 is a constant), with high probability, we have:
σmin (X>WX) ≥C0∣St ∩ S?| + X Cj |St ∩ Sj | ≥ CTn	(8)
j∈[m]
where co, ∙∙∙ , Cm, c are constants.
Proof of Lemma 5. We focus on finding concentration results for the smallest eigenvalue of
X>WX. Rewrite X>WX as
X>WX =E XiX>1 [li,t ∈ argτn- min(lι,t,…，ln,t)]
i
=E XiX> 1 [li,t ∈ argτn- min(lι,t,…，ln,t)] 1 [i ∈ S?]
i
+ X Xxixi>1 [li,t ∈ argτn- min(lι,t,…，ln,t)] 1 [i ∈ Sj]	(9)
j∈[m] i
where 1[∙] is the indicator function. Note that the values {li,t}n=ι are defined as:
li,t = (yi - xJθt).
More specifically, consider the model setting, we have:
li,t = [xi>(θ? -θt)+i]2, i ∈ S?
li,t = (x>(θj - θt) + q)2 , i ∈ Sj,j ∈ [m]
15
Under review as a conference paper at ICLR 2019
For all i ∈ S?, li,ts have the same distribution as δμ, where μ is a χ2(1) random variable, and
δ is the magnitude δ = kθ? - θtk22 + σ2. The bad samples i ∈ [n]\S?, for i ∈ Sbjad have the
same distribution as (σ2 + ∣∣θ(j) - θtk2)μ. Also, the assumption in θt assumes that the magnitude
of the good distribution is smaller. Applying the results in Lemma 3 and Lemma 4, assuming the
condition in Theorem 1 holds, for given θt , θ?, there exists a constant c1 such that the separation
value for deciding top-τ n or not is lower bounded by cδ with high probability. For a fixed c0, we
consider the following quantity:
Σ(co) = E	XiX>	|	|x[	(θ?	-	θt)	+ q| ≤	co J∣θ*	-	θtk2 + σ2
By the geometry of the constraint, we have that
σmin (Σ (c)) ≥
/c s2 ɪ e-s2 ds
J-C	√2∏
，/、	_c2
1 — 2ψ(c) — Ce 2
In other words, the lower bound we have for the smallest eigenvalue of the expectation of (9) mono-
tonically increases with c, and can be substituted with another constant 2c0 (notice that previously,
we have a lower bound on the separation value). Therefore, considering the sub-exponential prop-
erty, we get the concentration result based on the above expected value:
σmin
(X Xix>1 [li,t ∈ arg Tn- min(lι,t, ∙
,in,t)]i[i ∈ S?]	≥ co |St ∩S?l,
Similarly, since ∣θj - θt∣ ≤ 3, we have a lower bound O(|St ∩ Sj |) for i ∈ St ∩ Sj as well, as long
as the set |St ∩ Sj | ≥ c log2 n. In any case, we have the minimum eigenvalue is lower bounded by
cτn.
T2 in (4)	T2 can be bounded as follows:
2
T2 =	X (x>θ? - ni - ei) Xi
i∈St∖S?	2
=(Xθ? - n - e)>(W - WW*)XX>(W - WW*)(Xθ* - n - e)
≤σ ((W - WW*)XX>(W - WW?)) (Xθ? - n - e)>(W - WW*)(Xθ* - n - e)
=σ ((W - WW*)XX>(W - WW?)) X X (x>θ* - x>θ(j) - ek)2
j∈[m] k∈St∩S(j) ∖S?
≤σ ((W - WW*)XX>(W - WW?)) X X	(x>(θ? - θt))2 + (x>R- θ(j)) - efc
j∈[m] k∈St∩S(j) ∖S?
≤σ ((W - WW*)XX>(W - WW?))2 ∣θt - θ*∣2
+ cσ ((W - WW*)XX>(W - WW?)) ∣St∖S?| (∣θt - θ?∣2 + σ2)
≤c∣St∖S?|2 (kθt-θ*k2+ σ2)
for |St\S?| ≥ c log2 n (when |St\S?| < c log2 n, the final convergence result still holds true). As a
consequence,
T2 ≤c∣St∖S*∣(kθt- θ?k2 + σ).	(10)
T3 in (4) For this part, we can reuse the result we proved for T1 . Also, notice that with high
probability:
X>WW?e22 = e>W?WXX>WW?e≤ cd log nnσ2	(11)
16
Under review as a conference paper at ICLR 2019
Putting things together Combining Lemma 5, (10), and (11), we have:
kθt+1 - θ*∣∣2
c0∣St∖S*∣(∣∣θt- θ*∣∣2 + σ)	c√ndlogn
≤--------------------------1-----------σ
τn	τn
≤c3kθt - θ*∣∣2 + (C3 + c4)σ.
We require n ≥ Cdcogd. Also, according to Lemma 4, given ∣∣θt - θ?∣∣2 ≤ c3cɪ--r mi□j∈[m] ∣∣θt -
θj∣∣2 - 41 一 (C-cT) σ (for c3 ∈ (0,1)), then with high probability, |St\S?| ≤ c3τn. Notice that
for small τ, the noise should not be too large. Otherwise, even if θt is very close to θ?, because of
the noise and the high density of bad samples, |St\S?| would still be quite large, and the update will
not converge.
C.2 Simulations
C.2.1 ILFB on mixed linear regression
d=100, n=1000
0.50	0.55	0.60	0.65	0.70	0.75	0.θ0
percent of good samples
d=100, n=1000
0	2	4	6 B 10	12	14
# rounds

Figure 7: Left: `2 loss of recovered parameter to the true parameter for (a) MLE: naive MLE/OLS
estimator; (b) Oracle: MLE estimator for the subset of good samples; (c) Super-Oracle: MLE
estimator for the whole set of samples given the correct output for bad samples; (d) R-Init+ILFB:
our algorithm with random initialization; (e)M-Init+ILFB: our algorithm with MLE as initialization.
σ = 0.2, systematic error setting. Right: Speed of convergence under systematic error setting,
number of good samples is set as 600. M-Init: ILFB using MLE initialization; R-Init: ILFB using
random initialization.
We verify the performance of ILFB on mixed linear regression model via synthetic experiments.
When constructing the dataset, we consider a special case where all bad samples are generated from
a single βj . The parameters for both the good and bad components are generated randomly on a unit
sphere. First, in Fig. 7 (left), we test the performance when τ? varies in the interval [0.5, 0.8] with
σ = 0.2. We compare:
(1)	MLE: All samples are treated as good samples to find the best parameter;
(2)	Oracle: learn from the T?n good samples given oracle access;
(3)	Super-Oracle: learn from n samples given oracle access;
(4)	R-Init+ILFB : ILFB with random initialization;
(5)	M-Init+ILFB : ILFB using MLE as initialization.
The parameter recovery (y-axis) is measured by ∣θalg - θ? ∣2 . We can see that ILFB with MLE as
initialization performs better than the random initialization counterpart, which shows the effective-
ness of taking good initailizations. Next, we show experimentally the speed of convergence in Fig. 7
(right) with multiple levels of measurement noise (in the small noise regime). The plots show linear
convergence of each update step, which matches the result in Theorem 1.
17
Under review as a conference paper at ICLR 2019
C.2.2 Asymptotic performance
Next, we check the performance of ILFB asymptotically. Our theory implies that the final recovery
will get to a noise ball centered at the true parameter, but does not ensure the algorithm goes to the
true parameter, even when sample size goes to infinity. In this section, we show experimentally that
this noise ball guarantee is indeed tight, i.e., one should not expect ILFB to give exact recovery even
in the infinite sample case.
Our experimental setting is as follows: we consider mixed linear regression with two components,
where 60% of the samples comes from the interested component, and all xis follow isotropic normal
Gaussian distribution. The parameters for the two components are set as unit norm, and orthogonal
to each other, with d = 10. We run ILFB for 15 iterations with τ set as 5% less than that of the true
ratio. Number of samples varies from 100 to 51200. Standard deviation is calculated based on 100
runs for each experiment.
Fig. 8 provides the comparison between ILFB and oracle performance under different noise regime,
i.e., σ = 0.1, 1, 10. We observe the performance of ILFB is close to the oracle in the small noise
regime, while when the noise is comparable to the signal, there exists a gap between ILFB and
the oracle, as the result of ILFB falls within the noise ball and stops decreasing, while the oracle
will go to the exact solution asymptotically. Similar performance appears in the large noise regime,
however, for n ≤ 51200, the gap between the two seems not too far apart, which is due to the
dominance of the noise.
asymptotic behavior	asymptotic behavior	asymptotic behavior
»Ht*	HH»	««**	(H∙*
number Cfsamples
♦	MW*	HH»	««•*	*«M*
number Dfsamples
»	i«H»	1H∙*	1H∙*	««•*	BH∙*
number Ofsamples
Figure 8: Performance of ILFB for mixed linear regression under different noise levels (σ =
0.1, 1, 10 from left to right). For small noise (left plot), the performance of ILFB decreases along
with the oracle performance. When the noise gets larger (middle plot), we observe a gap between
ILFB and the oracle when sample size n increases. When the noise gets dominate the signal (right
plot), the performance of ILFB and the oracle are not far apart for n ≤ 51200, but the gap will get
larger asymptotically. Notice that for all experiments, the standard deviation of the true signal is 1.
C.3 Initialization for linear regression model
Theorem 6 (initialization). Given the model described in (1), for some j ∈ [m], define djm :
minι∈[m]∖{j} kθ(j) -θ(l)∣∣2, if Pι∈[m]∖{j} ∣∣θ(j) — θ(l)k2∣Sl| ≤ Kdm holdsforsome K. We have:
1.1
kθ	θ0k2 ≤ 丁m
with high probability, where θo = Θols, n ≥ C max{；：叱,mκ Jog(mκ)}.
(dm )	(dm )
The assumption in Theorem 6 requires the total sum of distances of the parameters for samples from
other components should not be too large. For a fixed djm , if the distance for some θ(l) becomes
larger, it may become harder for the OLS estimator to be in the local region close to θ(j). When
the condition holds for some large κ, e.g., κ > 2.2(1 + ⅛-3τ)and σ <
C1c3τdm
2√(1-τ )2-c2C2τ2
when
C(τ) < 1 (or for some large τ, κ > 2.2), following Theorem 1, the algorithm converges to a local
O(σ) ball around θ(j).
18
Under review as a conference paper at ICLR 2019
Proof. We write out the OLS estimator as follows:
θOLS = (X>X)-1 X> y
=(X>X)-1 X> (W?Xθ? + (I - W?) n + e)
=(X>X)-1 X> (Xθ? - (I - W*)Xθ? + (I - W?)n + e)
=θ? - (X>X)-1 X> (I - W?)(Xθ? - n) + (X>X)-1 X>e
Therefore, with high probability,
kθOLS - θ? k2
≤
m
X X (X>X)-1XiX>(θ? -θ⑺
j=1 i∈Sbjad
+ l(X>X)T x>e∣∣2
2
≤ Pj∈[m] kθ? - θ⑶ k2 ∙ ISbadI + cpm(1-τynlogn + p∕(d^+^c√dTogn)(n+^c√nIogn)
-	n — Zn log n	n — Zn log n
κ
for n ≥ Cmax{Kd2σ2, mκ dog(mK)}, where in(12), We use the assumption Pj∈[m] ∣∣θ? 一
θ(j)k2∣SbadI≤ n minj∈m] kθ?- θ(j) ∣2.
D Gaussian Mixture Model
Similar to Section C, we refer to the targeted component as the ‘good’ component and the rest are
‘bad’ components. More speifically, S can be splitted into disjoint sets S = Sgood ∪Sbad ∪…∪Sb‰,
the samples follows:
Xi 〜N (θ*,σ2I) ,	ifi ∈ Sgood
Xi 〜N (θ"σ2I) , if i ∈ Sbad
D.1 Proof for update steps in Gaussian mixture model
Similar to the proof for linear regression setting, we use St to denote the set of selected samples
according to parameter θt . Then,
θt+ι = ɪ X Xi.	(13)
τn
i∈St
Assume S? ⊂ Sgood to be the set of good samples that are closest to θ?, and IS?I = IStI = τn. In
other words, S? is selected by growing a ball centered at θ?, until τn of the good samples fall into
this ball, and we select them to be the set S?. Accordingly, let
θ=- X Xi.	(14)
τn
i∈S?
With high probability, ∣^ 一 θ*∣2 ≤ C√√nn = o(σ√d). For i ∈ Sbad ∩ St, the norm of Xi 一 θt
is small. More specifically,
∣Xi - θt∣∣2 ≤ IlXl- θt∣∣2 ,∀i ∈ Sbad ∩ St,∀l ∈ S?\St.
19
Under review as a conference paper at ICLR 2019
For any good sample with index l, the value kxl - θt k2 ≤ kxl - θ? k2 + kθ? - θt k2 by triangle
inequality, where the square of 1 ∣∣xι - θ？∣∣2 follows χ2(d) distribution. Denote Bd(q) be the q-
quantile value of χ2(d) distribution, with high probability, the q-quantile of 1 ∣∣xι - θ*∣2 for all
good samples is bounded by 2y∕Bd(q). Next, We have
θt+1
-θ?=，
τn
xi -	xi
i∈St∖S?	i∈S*∖St	.
+ (θ — θ?)
which is based on the definition in (13) and (14). Then,
kθt+1 - θ*∣∣2
τn
E	(Xi- θt+θt)	- E	(Xi-	θ?+θ?)	+	(θ - θ?)
1
≤——
τn
i∈St ∖S?
/
i∈S? ∖St
|S?\St| ∣θ? - θt∣2 +	(Xi - θt)
j∈[m] i∈Sbjad∩St
Xi-θ?
i∈S? ∖St
≤τ1n (|S?\Stlkθ? - θt∣2 + |St\S?l (kθt - θ*∣2 + 2σʌ/Bd(JSτ∩S?1)) +
(15)
j +**l∣2
2	(16)
X Xi-θ?	) +版-θ*l∣2
i∈S*∖St	2)
(17)
1
2
+
∖
2
(15)	makes an expansion for each term: the norm ofXi - θt for i ∈ St\S? should be small because
of the selection rule, while the norm ofXi - θ? for i ∈ S?\St should be small because they are good
samples. Bd (**|) = O(d). Inequality (16) splits (15) into 4 terms. The second term in (16) is
bounded based on triangle inequality and the fact that if a sequence is element-wise larger than the
other sequence, then the order statistics is also larger. For the second last term in (17), since the set
S? is selected by spanning a ball centered at θ?, the distance to the center from any sample in set S?
is bounded by cσ√d for some constant c. Therefore,
kθt+ι - θ*∣2 ≤ 2≡t∣ kθ? - θt∣2 + 4σ√d + C等σ√d.	(18)
τ n	τ n	τ n
In order for (18) to converge, We expect |S?\St| < τn∕2. For τ? ≥ 2, given ∣∣θt - θ*∣2 ≤
0.9minj∈[m] ∣∣θt - θj∣2, there exists ci ∈ (0,1), such that |S?\St| ≤ C1 τn with high probability
(i.e., more than half of samples in St are from the correct component). As a consequence,
I∣θt+1 - θ*k ≤ Ci ∣∣θt - θ*k + C2σ√d.	(19)
For τ < 2, in order to guarantee |S?\St| < τn∕2, we not only require ∣∣θt - θ?∣∣2 ≤
0.9 minj∈[m] ∣θt - θj∣2. This is because, the samples generated from θ? may be a small fraction of
the whole dataset (less than half), and when θt is slighly closer to θ? , it will still collect more bad
samples because of the higher density. Intuitively, when σ is large enough, for any θt ∈ Rd, |S?\St|
can be larger than τ n/2. Observe that the median of squared distance from the good samples to θt
(denoted as medgood) concentrates at the sample mean, which concentrates at the expected mean,
which is ∣∣θt - θ? 12 + σ2d. On the other hand, we require the number of bad samples whose squared
distance to θt is less than medgood is less than τn∕2, which corresponds to the 2.-T)-quantile value
to be greater than medgood . Therefore, a valid θt should satisfy:
P [σ2n>n + 2a) < ∣θt - θ*∣2 -∣θt - *12 + σ2d] ≤ 2(i⅛)
where n 〜N(0, Id) is a normal Gaussian vector. By sub-exponential inequality, and reorganizing
the terms, a sufficient condition we require is ∣θt - θ? ∣2 ≤ minj∈[m]
∣θt- θj∣2 - C,log 与T)σ.
This condition essentially implies a √log kσ separation distance, which is the lower bound for
computationally efficient algorithm to learn a Gaussian mixture model according to Regev & Vi-
jayaraghavan (2017).
20
Under review as a conference paper at ICLR 2019
D.2 S imulation results
In this part, we provide synthetic experimental results for the Gaussian mixture model. Most of the
settings are similar to the settings for mixed linear regression in Section C.2. For clearness, we will
re-state some of the definitions/settings.
D.2.1 ILFB on Gaussian mixture model
We verify the performance of ILFB for Gaussian mixture model through synthetic experiments.
In Gaussian mean estimation, we are interested in recovering the mean of the dominant mixture
component. The bad samples are generated from another Gaussian distribution, and the distance of
the two centers is set as 1 for convenience. We test the performance when τ? varies in the interval
[0.5, 0.8] with σ = 0.15. We compare:
(1)	MLE: All samples are treated as good samples to find the best parameter;
(2)	Oracle: learn from the T?n good samples given oracle access;
(3)	Super-Oracle: learn from n samples given oracle access;
(4)	R-Init+ILFB : ILFB with random initialization;
(5)	M-Init+ILFB : ILFB using MLE as initialization.
According to Fig. 9 (left), ILFB performs close to the oracle, and is much better than the naive
method and ILFB with random initialization. In Fig. 9 (right), we see linear convergence at the first
few rounds until reaching a static point.
0.50	0.55	0.60	0.65	0.70	0.75
percent of good samples
Figure 9: Left: Performance of different algorithms for Gaussian mixture model, measured by the
`2 loss of the recovered parameter to the true parameter, σ = 0.15. Right: Convergence speed of
ILFB under different noise levels, using both MLE initialization and random initialization.
d=100, n=1000
O 1
O -
1 O
.IEe-Ied paj>ou」SSo-
0	2	4	6 B 10	12	14	16
# rounds
D.2.2 Asymptotic performance
Next, we check the asymptotic behavior of ILFB for Gaussian mixture model under different noise
levels. As we stated in our theorem, ILFB will converge to the noise ball centered at the true
parameter, which does not guarantee consistency. In fact, our experiments verifies that the result is
tight.
Our experimental setting is as follows: we consider mixture of two Gaussian distributions, where
60% of the data comes from the interested component. The distance between two centers is 1. We
test the performance under different sample sizes, varies from 100 to 51200. The standard deviation
of each result is calculated based on 100 random runs.
In Fig. 10 (middle), there exists a gap between ILFB and the oracle, i.e., the oracle goes asymptoti-
cally to the true parameter as n gets larger, while the performance of ILFB does not decrease with n
after some threshold. The same problem exists when the noise becomes larger, as shown in Fig. 10,
however, for n ≤ 51200, the results for both ILFB and the oracle are dominated by the noise. In the
small noise regime, both ILFB and the oracle perform well.
21
Under review as a conference paper at ICLR 2019
We also want to point oUt that this pheonema of non-exact recovery when the noise is large is not
a Problem introdUced by oUr algorithm. In fact, for a series of Work on learning GaUssian mixtUre
models (estimating the means given identity covariance matrix), a separation betWeen every Pair
of mixtUre comPonents is reqUired in order to learn in Polynomial time (notice that this Polynomial
time is UsUally mUch larger comPared to oUr algorithm Which is near-linear). For Fig. 10 (middle
and right), this seParation reqUirement is clearly not satisfied.
asymptotic behavior	asymptotic behavior	asymptotic behavior
I
∙,juij-p-
-W*	HH*
number Ofsamples
∙,juij-p-
-W*	HH*
number Ofsamples
B=HJβ-p-
««**
(*«♦
>HH	IHW
number Ofsamples

Figure 10: Performance of ILFB for Gaussian mixture model under different noise levels (σ =
0.1, 1, 10 from left to right). For small noise (left plot), the performance of ILFB decreases along
with the oracle performance. When the noise gets larger (middle plot), we observe a gap between
ILFB and the oracle when sample size n increases. When the noise gets dominate the signal (right
plot), the performance of ILFB and the oracle are not far apart for n ≤ 51200, but the gap will get
larger asymptotically.
D.3 Initialization for Gaussian mixture model
We present a performance guarantee of the simple initialization step, which takes the average over
all the samples. More specifically, we show that the initialization will be closer to the dominant
component under mild assumptions.
Theorem 7 (initialization). Given a dataset D generated following (2), for a certain j, define
dm :二 minι∈[m]∖{j} ∣∣θ(j) 一 θ(l)∣∣2. Consider taking the overall mean as the initialization θo, then,
if ∣∣pι∈[m]∖{j}(θ(j) -θ(l))lSll∣∣2 ≤ αndm, and n ≥ (*ασ(dj产,We have kθo 一 θ(j)k2 ≤
0.9minι∈[m]∖{j} ∣∣θ0 — θ(l)∣∣2 With high probability.
Proof. AssUme ∣∣Pj∈m](θ? — θ(j))∣Sbad∣∣∣	≤ αnmin7-∈[m] ∣∣θ? — θ(j)1. Considertheinitial-
ization
1n
θo =-y^xi.
n i=1
We have with high probability,
kθo-θ*∣2=1
n
n
X xi — θ?
i=1
11∣∣ x …+x
2	∣ i∈Sgood
i∈Sbad
xi-θl
1
≤ 一
n
X (θ? - θ(j))∣Sbad ∣ + cσ√d
j∈[m]	∣2
≤ 1 αn min
n	j∈[m]
θ? — θ(j)
≤2 min ∣∣θ? — θ(j) ∣∣
19 j∈[m] ∣	∣2
if n ≥(磊 -a：2d2 .This WillimPly ∣∣θo — θ*∣∣2 ≤ 0.9min7∙∈m] ∣∣θ0 —。⑶上.
22
Under review as a conference paper at ICLR 2019
E More experimental results
Table 4: MNIST classification: comparison with other choices
dataset					MNIST			
τ? =	# good # total	Baseline	ILFB	Centroid	1-Step	∆τ = 10%	∆τ = 15%	∆τ = 20%
60%		70.26	90.00	74.52	78.48	88.24	86.48	84.49
70%		85.95	92.09	88.47	89.31	90.83	89.19	88.10
80%		92.62	94.30	92.79	92.78	92.78	90.17	88.63
90%		93.88	94.41	94.32	93.94	92.88	91.21	89.90
Table 5: MNIST GAN: comparison with other choices
dataset					MNIST			
τ? =	# good # total	Baseline	ILFB	Centroid	1-Step	∆τ = 10%	∆τ = 15%	∆τ = 20%
70%		70	97.00	61.46	77.77	83.33	78.06	83.59
80%		80	100.00	77.46	76.84	98.80	99.56	97.77
90%		90	100.00	89.57	91.90	98.85	99.01	98.04
Table 6: MNIST classification with random error.
dataset					MNIST			
τ? =	# good # total	Baseline	ILFB	Centroid	1-Step	∆τ = 10%	∆τ = 15%	∆τ = 20%
30%		82.74	87.88	82.72	85.17	85.30	75.14	61.52
50%		90.57	91.80	91.00	91.48	90.43	87.37	78.21
70%		93.04	92.40	93.47	92.55	91.14	89.39	87.90
90%		93.98	93.93	94.14	93.58	92.32	90.61	89.95
Experimental settings: In this section, we present additional experimental results, in order to ver-
ify the performance of ILFB under different parameter settings, and compare with other algorithms.
More specifically, we present the results using the following methods/algorithms:
•	Baseline: naive trainig using all the samples;
•	ILFB : our proposed iterative learning algorithm with 5 iterations, using a mis-specified τ
which is 5% less than the true value;
•	Centroid: using the centroid of the input data to filter out outliers. For classification task,
we calculate the centroids for the samples with the same label/class and filter each class
separately;
•	1-Step: ILFB algorithm with a single iteration;
•	∆τ = τ? - τ ∈ {10%, 15%, 20%}: ILFB under different mis-specified τ value,
under the following three problem settings:
•	Classification with systematic error;
•	MNIST generation with Fashion-MNIST images ;
•	Classification with random label error.
For classification tasks (Table 4 and Table 6), we show the best accuracy on the validation set. For
the generation task (Table 5), we present the ratio of true MNIST samples selected by each method.
For the baseline method, since the DC-GAN is trained using all samples, the reported value is exactly
the τ? .
23
Under review as a conference paper at ICLR 2019
Results: Table 4 shows the performance under systematic error, for τ? varies from 60% to 90%.
ILFB not only performs better than the baseline, but also outperform the centroid method by a large
margin, especially when the dataset is noisy (τ? is small). By comparing ILFB with its 1-step
counterpart, we see the benefit brought by doing the iterative updating is also significant. Table 5
shows the performance of generation quality under different noise levels. We observe that centroid
method does not work, which may due to the fact that all MNIST and Fashion-MNIST images are
hard to be distinguished as two clusters in the pixel space. Notice that there are in fact 20 clusters (10
from MNIST, and 10 from Fashion-MNIST), and we are interested in 10 of them. ILFB works well
since it automatically learns a clustering rule when generating on the noisy dataset. For example, for
τ? = 80%, even with a mis-specified τ = 60%, ILFB is capable of ignoring almost all bad samples.
Again, we also observe significant improvement of ILFB over its 1-step counterpart. In Table 6, we
show the performance of ILFB in the random error setting. Again, ILFB performs much better in
the extremely noisy setting.
We also have results showing that ILFB works well for generation when the corrupted samples are
pure Gaussian noise. However, we do not think it is a practical assumption, and the result is not
presented here.
24