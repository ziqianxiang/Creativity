Under review as a conference paper at ICLR 2019
seGEN: Sample-Ensemble Genetic Evolutionary Net-
work Model
Anonymous authors
Paper under double-blind review
Abstract
Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success
in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical
feature representations of the observational data. Meanwhile, due to its severe disadvantages in data
consumption, computational resources, parameter tuning costs and the lack of result explainability, deep
learning has also suffered from lots of criticism. In this paper, we will introduce a new representation
learning model, namely “Sample-Ensemble Genetic Evolutionary Network” (seGEN), which can serve
as an alternative approach to deep learning models. Instead of building one single deep model, based on a
set of sampled sub-instances, seGEN adopts a genetic-evolutionary learning strategy to build a group of
unit models generations by generations. The unit models incorporated in seGEN can be either traditional
machine learning models or the recent deep learning models with a much “narrower” and “shallower”
architecture. The learning results of each instance at the final generation will be effectively combined
from each unit model via diffusive propagation and ensemble learning strategies. From the computational
perspective, seGEN requires far less data, fewer computational resources and parameter tuning efforts,
but has sound theoretic interpretability of the learning process and results. Extensive experiments have
been done on several different real-world benchmark datasets, and the experimental results obtained by
seGEN have demonstrated its advantages over the state-of-the-art representation learning models.
1	Introduction
In recent years, deep learning, a rebranding of deep neural network research works, has achieved a remarkable success.
The essence of deep learning is to compute the hierarchical feature representations of the observational data Goodfellow
et al. (2016); LeCun et al. (2015). With multiple hidden layers, the deep learning models have the capacity to capture very
good projections from the input data space to the objective output space, whose outstanding performance has been widely
illustrated in various applications, including speech and audio processing Deng et al. (2013); Hinton et al. (2012), language
modeling and processing Arisoy et al. (2012); Mnih & Hinton (2009), information retrieval Hill (2012); Salakhutdinov
& Hinton (2009), objective recognition and computer vision LeCun et al. (2015), as well as multimodal and multi-task
learning Weston et al. (2010; 2011). By this context so far, various kinds of deep learning models have been proposed
already, including deep belief network Hinton et al. (2006), deep Boltzmann machine Salakhutdinov & Hinton (2009),
deep neural network Jaeger (2002); Krizhevsky et al. (2012) and deep autoencoder model Vincent et al. (2010).
Meanwhile, deep learning models also suffer from several serious criticism due to their several severe disadvantages Zhou
& Feng (2017a). Generally, learning and training deep learning models usually demands (1) a large amount of training
data, (2) large and powerful computational facilities, (3) heavy parameter tuning costs, but lacks (4) theoretic explanation of
the learning process and results. These disadvantages greatly hinder the application of deep learning models in many areas
which cannot meet the requirements or requests a clear interpretability of the learning performance. Due to these reasons,
by this context so far, deep learning research and application works are mostly carried out within/via the collaboration
with several big technical companies, but the models proposed by them (involving hundreds of hidden layers, billions of
parameters, and using a large cluster with thousands of server nodes Dean et al. (2012)) can hardly be applied in other
real-world applications.
In this paper, we propose a brand new model, namely seGEN (Sample-Ensemble Genetic Evolutionary Network), which
can work as an alternative approach to the deep learning models. Instead of building one single model with a deep archi-
tecture, seGEN adopts a genetic-evolutionary learning strategy to train a group of unit models generations by generations.
Here, the unit models can be either traditional machine learning models or deep learning models with a much “narrower”
and “shallower” structure. Each unit model will be trained with a batch of training instances sampled form the dataset.
By selecting the good unit models from each generation (according to their performance on a validation set), seGEN will
evolve itself and create the next generation of unit modes with probabilistic genetic crossover and mutation, where the
selection and crossover probabilities are highly dependent on their performance fitness evaluation. Finally, the learning
results of the data instances will be effectively combined from each unit model via diffusive propagation and ensemble
learning strategies. These terms and techniques mentioned here will be explained in great detail in Section 4. Compared
with the existing deep learning models, seGEN have several great advantages, and we will illustrate them from both the
bionics perspective and the computational perspective as follows.
1
Under review as a conference paper at ICLR 2019
From the bionics perspective, seGEN effectively models the evolution of creatures from generations to generations, where
the creatures suitable for the environment will have a larger chance to survive and generate the offsprings. Meanwhile, the
offsprings inheriting good genes from its parents will be likely to adapt to the environment as well. In the seGEN model,
each unit network model in generations can be treated as an independent creature, which will receive a different subsets
of training instances and learn its own model variables. For the unit models suitable for the environment (i.e., achieving
a good performance on a validation set), they will have a larger chance to generate their child models. The parent model
achieving better performance will also have a greater chance to pass their variables to the child model.
From the computational perspective, seGEN requires far less data and resources, and also has a sound theoretic explanation
of the learning process and results. The unit models in each generation of seGEN are of a much simpler architecture,
learning of which can be accomplished with much less training data, less computational resources and less hyper-parameter
tuning efforts. In addition, the training dataset pool, model hyper-parameters are shared by the unit models, and the
increase of generation size (i.e., unit model number in each generation) or generation number (i.e., how many generation
rounds will be needed) will not increase the learning resources consumption. The relatively “narrower” and “shallower”
structure of unit models will also significantly enhance the interpretability of the unit models training process as well as
the learning results, especially if the unit models are the traditional non-deep learning models. Furthermore, the sound
theoretical foundations of genetic algorithm and ensemble learning will also help explain the information inheritance
through generations and result ensemble in seGEN.
In this paper, we will use network embedding problem Wang et al. (2016); Chang et al. (2015); Perozzi et al. (2014)
(applying autoencoder as the unit model) as an example to illustrate the seGEN model. Meanwhile, applications of
seGEN on other data categories (e.g., images and raw feature inputs) with CNN and MLP as the unit model will also be
provided in Section 5.3. The following parts of this paper are organized as follows. The problem formulation is provided
in Section 3. Model seGEN will be introduced in Section 4, whose performance will be evaluated in Section 5. Finally,
Section 2 introduces the related works and we conclude this paper in Section 6.
2	Related Work
Deep Learning Research and Applications: The essence of deep learning is to compute hierarchical features or repre-
sentations of the observational data Goodfellow et al. (2016); LeCun et al. (2015). With the surge of deep learning research
and applications in recent years, lots of research works have appeared to apply the deep learning methods, like deep be-
lief network Hinton et al. (2006), deep Boltzmann machine Salakhutdinov & Hinton (2009), Deep neural network Jaeger
(2002); Krizhevsky et al. (2012) and Deep autoencoder model Vincent et al. (2010), in various applications, like speech and
audio processing Deng et al. (2013); Hinton et al. (2012), language modeling and processing Arisoy et al. (2012); Mnih
& Hinton (2009), information retrieval Hill (2012); Salakhutdinov & Hinton (2009), objective recognition and computer
vision LeCun et al. (2015), as well as multimodal and multi-task learning Weston et al. (2010; 2011).
Network Embedding: Network embedding has become a very hot research problem recently, which can project a graph-
structured data to the feature vector representations. In graphs, the relation can be treated as a translation of the entities,
and many translation based embedding models have been proposed, like TransE Bordes et al. (2013), TransH Wang et al.
(2014) and TransR Lin et al. (2015). In recent years, many network embedding works based on random walk model and
deep learning models have been introduced, like Deepwalk Perozzi et al. (2014), LINE Tang et al. (2015), node2vec Grover
& Leskovec (2016), HNE Chang et al. (2015) and DNE Wang et al. (2016). Perozzi et al. extends the word2vec model
Mikolov et al. (2013) to the network scenario and introduce the Deepwalk algorithm Perozzi et al. (2014). Tang et al. Tang
et al. (2015) propose to embed the networks with LINE algorithm, which can preserve both the local and global network
structures. Grover et al. Grover & Leskovec (2016) introduce a flexible notion of a node’s network neighborhood and
design a biased random walk procedure to sample the neighbors. Chang et al. Chang et al. (2015) learn the embedding of
networks involving text and image information. Chen et al. Chen & Sun (2016) introduce a task guided embedding model
to learn the representations for the author identification problem.
3	Problem Formulation
In this section, we will provide the definitions of several important terminologies, based on which we will define the
network representation learning problem.
3.1	Terminology Definition
The seGEN model will be illustrated based on the network representation learning problem in this paper, where the input
is usually a large-sized network structured dataset.
DEFINITION 1 (Network Data): Formally, a network structured dataset can be represented as a graph G = (V, E), where
V denotes the node set and E contains the set of links among the nodes.
In the real-world applications, lots of data can be modeled as networks. For instance, online social media can be represented
as a network involving users as the nodes and social connections as the links; e-commerce website can be denoted as a
network with customer and products as the nodes, and purchase relation as the links; academic bibliographical data can be
modeled as a network containing papers, authors as the nodes, and write/cite relationships as the links. Given a large-sized
input network data G = (V, E), a group of sub-networks can be extracted from it, which can be formally represented as a
sub-network set of G.
2
Under review as a conference paper at ICLR 2019
Figure 1: The seGEN Framework.
Definition 2 (Sub-network Set): Based on a certain sampling strategy, we can represent the set of sampled sub-
networks from network G as set G = {g1,g2, ∙∙∙ , gm} of size m. Here, gi ∈ G denotes a sub-network of G, and it
can be represented as gi = (Vgi , Egi), where Vgi ⊆ V, Egi ⊆ E and G 6= gi.
In Section 4, we will introduce several different sampling strategies, which will be applied to obtained several different
sub-network pools for unit model building and validation.
Learning Results
Output
Result
Step 1: Network Sampling Step 2: SUb-NetWork Representation Learning Step 3: Result Ensemble
3.2	Problem Formulation
Problem Statement: Based on the input network data G = (V, E), the network representation learning problem aims at
learning a mapping f : V → Rd to project each node from the network to a low-dimensional feature space. There usually
exist some requirements on mapping f (∙), which should preserve the original network structure, i.e., closer nodes should
have close representations; while disconnected nodes have different representations on the other hand.
4	Proposed Methods
In this section, we will introduce the proposed framework seGEN in detail. As shown in Figure 1, the proposed framework
involves three steps: (1) network sampling, (2) sub-network representation learning, and (3) result ensemble. Given the
large-scale input network data, framework seGEN will sample a set of sub-networks, which will be used as the input to
the genetic evolutionary network model for representation learning. Based on the learned results for the sub-networks,
framework seGEN will combine them together to obtain the final output result. In the following parts, we will introduce
these three steps in great detail respectively.
4.1	Network Sampling
In framework seGEN, instead of handling the input large-scale network data directly, we propose to sample a subset
(of set size s) of small-sized sub-networks (of a pre-specified sub-network size k) instead and learn the representation
feature vectors of nodes based on the sub-networks. To ensure the learned representations can effectively represent the
characteristics of nodes, we need to ensure the sampled sub-networks share similar properties as the original large-sized
input network. As shown in Figure 1, 5 different types of network sampling strategies (indicated in 5 different colors) are
adopted in this paper, and each strategy will lead to a group of small-sized sub-networks, which can capture both the local
and global structures of the original network.
4.1.1	BFS based Network Sampling
Based on the input network G = (V, E), Breadth-First-Search (BFS) based network sampling strategy randomly picks a
seed node from set V and performs BFS to expend to the unreached nodes. Formally, the neighbors of node v ∈ V can
be denoted as set Γ(v; 1) = {u|u ∈ V ∧ (u, v) ∈ E}. After picking v, the sampling strategy will continue to randomly
add k - 1 nodes from set Γ(v; 1), if ∣Γ(v; 1)| ≥ k - 1; otherwise, the sampling strategy will go to the 2-hop neighbors of
V (i.e., Γ(v; 2) = {u∣∃w ∈ V, (u, W) ∈ E ∧ (w, V) ∈ E ∧ (u, V) ∈ E}) and so forth until the remaining k — 1 nodes are
selected. In the case when the size of connected component that v involves in is smaller than k, the strategy will further
pick another seed node to do BFS from that node to finish the sampling of k nodes. These sampled k nodes together with
the edges among them will form a sampled sub-network g, and all the p sampled sub-networks will form the sub-network
pool GBFS (parameter p denotes the pool size).
4.1.2	DFS based Network Sampling
Depth-First-Search (DFS) based network sampling strategy works in a very similar way as the BFS based strategy, but it
adopts DFS to expand to the unreached nodes instead. Similar to the BFS method, in the case when the node connected
component has size less than k, DFS sampling strategy will also continue to pick another node as the seed node to continue
the sampling process. The sampled nodes together with the links among them will form the sub-networks to be involved
in the final sampled sub-network pool GDFS (of size p).
3
Under review as a conference paper at ICLR 2019
A remark to be added here: the sub-networks sampled via BFS can mainly capture the local network structure of nodes
(i.e., the neighborhood), and in many of the cases they are star structured diagrams with the picked seed node at the
center surrounded by its neighbors. Meanwhile, the sub-networks sampled with DFS are slightly different, which involve
“deeper” network connection patterns. In the extreme case, the sub-networks sampled via DFS can be a path from the seed
nodes to a node which is (k - 1)-hop away.
4.1.3	HS based Network Sampling
To balance between those extreme cases aforementioned, we introduce a Hybrid-Search (HS) based network sampling
strategy by combining BFS and DFS. HS randomly picks seed nodes from the network, and reaches other nodes based on
either BFS or DFS strategies with probabilities p and (1 - p) respectively. For instance, in the sampling process, HS first
picks node v ∈ V as the seed node, and samples a random node u ∈ Γ(v; 1). To determine the next node to sample, HS
will “toss a coin” with p probability to sample nodes from Γ(v; 1) \ {u} (i.e., BFS) and 1 - p probability to sample nodes
from Γ(u; 1) \ {v} (i.e., DFS). Such a process continues until k nodes are selected, and the sampled nodes together with
the links among them will form the sub-network. We can represent all the sampled sub-networks by the HS based network
sampling strategy as pool GHS .
These three network sampling strategies are mainly based on the connections among the nodes, and nodes in the sampled
sub-networks are mostly connected. However, in the real-world networks, the connections among nodes are usually very
sparse, and most of the node pairs are not connected. In the following part, we will introduce two other sampling strategies
to handle such a case.
4.1.4	Biased Node Sampling
Instead of sampling sub-networks via the connections among them, the node sampling strategy picks the nodes at random
from the network. Based on node sampling, the final sampled sub-network may not necessarily be connected and can
involve many isolated nodes. Furthermore, uniform sampling of nodes will also deteriorate the network properties, since
it treats all the nodes equally and fails to consider their differences. In this paper, we propose to adopt the biased node
sampling strategy, where the nodes with more connections (i.e., larger degrees) will have larger probabilities to be sampled.
Based on the connections among the nodes, We can represent the degree of node V ∈ V as d(u) = ∣Γ(u; 1)|, and the
probabilities for U to be sampled can be denoted as P(U) = d2∣U). Instead of focusing on the local structures of the
netWork, the sub-netWorks sampled With the biased node sampling strategy can capture more “global” structures of the
input netWork. Formally, all the sub-netWorks sampled via this strategy can be represented as pool GNS.
4.1.5	Biased Edge Sampling
Another “global” sub-netWork sampling strategy is the edge based sampling strategy, Which samples the edges instead of
nodes. Here, uniform sampling of edges Will be reduced to biased node selection, Where high-degree nodes Will have a
larger probability to be involved in the sub-netWork. In this paper, We propose to adopt a biased edge sampling strategy
instead. For each edge (u, V) ∈ E, the probability for it to be sampled is actually proportional to d(U2+dd(V). The sam-
pled edges together With the incident nodes Will form a sub-netWork, and all the sampled sub-netWorks With biased edge
sampling strategy can be denoted as pool GES .
These tWo netWork sampling strategies can select the sub-structures of the input netWork from a global perspective, Which
can effectively capture the sparsity property of the input netWork. In the experiments to be introduced in Section 5, We Will
evaluate these different sampling strategies in detail.
4.2	GEN Model
In this part, We Will focus on introducing the Genetic Evolutionary NetWork (GEN) model, Which accepts each sub-netWork
pool as the input and learns the representation feature vectors of nodes as the output. We Will use G to represent the sampled
pool set, Which can be GBFS, GDFS , GHS , GNS or GES respectively.
4.2.1	Unit Model Population Initialization
In the GEN model, there exist multiple generations of unit models, Where the earlier generations Will evolve and generate
the later generations. Each generation Will also involve a group of unit models, namely the unit model population. Formally,
the initial generation of the unit models (i.e., the 1st generation) can be represented as set M1 = {Mf, M21,…，Mm } (of
size m), Where Mi1 is a base unit model to be introduced in the folloWing subsection. Formally, the variables involved in
each unit model, e.g., Mi1, can be denoted as vector θi1, Which covers the Weight and bias terms in the model (Which Will
be treated as the model genes in the evolution to be introduced later). In the initialization step, the variables of each unit
model are assigned With a random value generated from the standard normal distribution.
4.2.2	Unit Model Description
In this paper, We Will take netWork representation learning as an example, and propose to adopt the correlated autoencoder
as the base model. We Want to clarify again that the seGEN frameWork is a general frameWork, and it Works Well for
different types of data as Well as different base models. For some other tasks or other learning settings, many other existing
models, e.g., CNN and MLP to be introduced in Section 5.3, can be adopted as the base model as Well.
Autoencoder is an unsupervised neural netWork model, Which projects data instances from the original feature space to
a loWer-dimensional feature space via a series of non-linear mappings. Autoencoder model involves tWo steps: encoder
4
Under review as a conference paper at ICLR 2019
and decoder. The encoder part projects the original feature vectors to the objective feature space, while the decoder step
recovers the latent feature representations to a reconstructed feature space.
Based on each sampled sub-network g ∈ T, where g = (Vg , Eg), we can represent
the sub-network structure as an adjacency matrix Ag = {0,1}lVgl×lVg|, where
Ag(i, j) = 1 iff (vi, vj) ∈ Eg. Formally, for each node vi ∈ Vg, we can represent its
raw feature as Xi = Ag (i,:). Let y 1, y2,…，yO be the corresponding latent feature
representation of Xi at hidden layers 1, 2,…，o in the encoder step. The encoding
result in the objective feature space can be denoted as zi ∈ Rd of dimension d. In
the decoder step, the input will be the latent feature vector zi , and the final output
Zi
y1
yι
will be the reconstructed vector Xi (of the same dimension as Xi). The latent feature
vectors at each hidden layers can be represented as yoo, yO-1,…,y1. As shown in Figure 2: Autoencoder Model.
the architecture in Figure 2, the relationships among these variables can be represented with the following equations:
{Encoder:	( Decoder:
y1 = σ(W1Xi + b1),	I yo = σ(W o+1Zi + bo+1),
yk = σ(Wkyk-1 + bk), Vk ∈{2,…，o}, ( yk-1 = σ(Wky + bk), Vk ∈{2,…，o},
Zi = σ(Wo+1yO + bo+1).	∖ Xi = σ(W 1y1 + b1).
The objective of traditional autoencoder model is to minimize the loss between the original feature vector Xi and the
reconstructed feature vector Xi of data instances. Meanwhile, for the network representation learning task, the learning
task of nodes in the sub-networks are not independent but highly correlated. For the connected nodes, they should have
closer representation feature vectors in the latent feature space; while for those which are isolated, their latent representation
feature vectors should be far away instead. What’s more, since the input feature vectors are extremely sparse (lots of the
entries are 0s), simply feeding them to the model may lead to some trivial solutions, like 0 vector for both Zi and the
decoded vector Xi. Therefore, we propose to extend the Autoencoder model to the correlated scenario for networks, and
define the objective of the correlated autoencoder model as follows:
Le(g) = X k(Xi-Xi) Θ bik2+ α X Si,jkzi-Zjk2+ β ∙ X (IlWiiIF + ∣∣WillF),
vi∈Vg	vi,vj ∈Vg,vi 6=vj	i=1
where sij = +1, if g(i, j) = 1, and α, β are the weights of the correlation and regularization terms respectively.
-1,	if Ag(i, j) = 0.
Entries in weight vector bi have value 1 except the entries corresponding to non-zero element in Xi, which will be assigned
with value Y (γ > 1) to preserve these non-zero entries in the reconstructed vector Xi.
4.2.3	Generation Model Learning Setting
Instead of fitting each unit model with all the sub-networks in the pool G, in GEN, a set of sub-network training
batches 71, T2,…，Tm will be sampled for each unit model respectively in the learning process, where |Ti| = b, Vi ∈
{1,2,…，m} are of the pre-defined batch size b. These batches may share common sub-networks as well, i.e., T ∩ Tj-
may not necessary be 0. In the GEN model, the unit models learning process for each generation involves two steps: (1)
generating the batches Ti from the pool set G for each unit model Mi1 ∈ M1 , and (2) learning the variables of the unit
model Mi1 based on sub-networks in batch Ti . Considering that the unit models have a much smaller number of hidden
layers, the learning time cost of each unit model will be much less than the deeper models on larger-sized networks. In
Section 5, we will provide a more detailed analysis about the running time cost and space cost of seGEN.
4.2.4	Unit Model Fitness Evaluation and Selection
The unit models in the generation set M1 can have different performance, due to (1) different initial variable values, and (2)
different training batches in the learning process. In framework seGEN, instead of applying “deep” models with multiple
hidden layers, we propose to “deepen” the models in another way: “evolve the unit model into ‘deeper’ generations”. A
genetic algorithm style method is adopted here for evolving the unit models, in which the well-trained unit models will
have a higher chance to survive and evolve to the next generation. To pick the well-trained unit models, we need to evaluate
their performance, which is done with the validation set V sampled from the pool. For each unit model Mk1 ∈ M1 , based
on the sub-networks in set V, we can represent the introduced loss of the model as
Lc(Mk1;V)=X	X si,j llZ1k,i - Z1k,jll22,
g∈V vi,vj ∈Vg ,vi 6=vj
where Z1k,i and Z1k,j denote the learned latent representation feature vectors of nodes vi , vj in the sampled sub-network g
and si,j is defined based on g in the same way as introduced before.
The probability for each unit model to be picked as the parent model for the crossover and mutation operations can be
represented as
exp-L(Mk;V)
P^^^	I exD-L(MiIM .
Mi1 ∈M1 exp	i
5
Under review as a conference paper at ICLR 2019
In the real-world applications, a normalization of the loss terms among these unit models is necessary. For the unit model
introducing a smaller loss, it will have a larger chance to be selected as the parent unit model. Considering that the
crossover is usually done based a pair of parent models, we can represent the pairs of parent models selected from set M1
as P1 = {(Mi1, Mj1)k}k∈{i,2, ∙ ,m}, based on which we will be able to generate the next generation of unit models, i.e.,
M2.
4.2.5	Unit Model Crossover and Mutation
For the kth pair of parent unit model (Mi1, Mj1)k ∈ P1, we can denote their genes as their variables θi1, θj1 respectively
(since the differences among the unit models mainly lie in their variables), which are actually their chromosomes for
crossover and mutation.
Crossover: In this paper, we propose to adopt the uniform crossover to get the chromosomes (i.e., the variables) of their
child model. Considering that the parent models Mi1 and Mj1 can actually achieve different performance on the validation
set V, in the crossover, the unit model achieving better performance should have a larger chance to pass its chromosomes
to the child model.
Formally, the chromosome inheritance probability for parent model Mi1 can be represented as
p(Mi1) =
exp-L(Mi1;V)
exp-L(MIM +exp-L(M1;V)
Meanwhile, the chromosome inheritance probability for model Mj1 can be denoted as p(Mj1) = 1 - p(Mi1).
In the uniform crossover method, based on parent model pair (Mi1, Mj1)k ∈ P1, we can represent the obtained child model
chromosome vector as θk ∈ Rlθ1 | (the superscript denotes the 2nd generation and ∣θ11 denotes the variable length), which is
generated from the chromosome vectors θi1 and θj1 of the parent models. Meanwhile, the crossover choice at each position
of the chromosomes vector can be represented as a vector C ∈ {i,j}lθ1l. The entries in vector C are randomly selected
from values in {i,j} with a probability p(Mi1) to pick value i and a probability p(Mj1) to pick value j respectively. The
lth entry of vector θk2 before mutation can be represented as
θ2(l) = l(c(l) = i) ∙ θ1(l) + l(c(l)=j) ∙ θj(l),
where indicator function 1(∙) returns value 1 if the condition is True; otherwise, it returns value 0.
Mutation: The variables in the chromosome vector θ2(l) ∈ Rlθ1l are all real values, and some of them can be altered,
which is also called mutation in traditional genetic algorithm. Mutation happens rarely, and the chromosome mutation
probability is γ in the GEN model. Formally, we can represent the mutation indicator vector as m ∈ {0, 1}d, and the lth
entry of vector θk2 after mutation can be represented as
θ2(l) = 1 (m(l) = 0) ∙ θk(l) + 1 (c(l) = 1) ∙ rand(0, 1),
where rand(0, 1) denotes a random value selected from range [0, 1]. Formally, the chromosome vector θk2 defines a new
unit model with knowledge inherited form the parent models, which can be denoted as Mk2. Based on the parent model set
P1, we can represent all the newly generated models as M2 = {Mk2}(M1,M1)k∈P1 , which will form the 2nd generation of
unit models.
4.3	Result Ensemble
Based on the models introduced in the previous subsection, in this part, we will introduce the hierarchical result ensemble
method, which involves two steps: (1) local ensemble of results for the sub-networks on each sampling strategies, and (2)
global ensemble of results obtained across different sampling strategies.
4.3.1	Local Ensemble
Based on the sub-network pool G obtained via the sampling strategies introduced before, we have learned the Kth genera-
tion of the GEN model MK (or M for simplicity), which contains m unit models. In this part, we will introduce how to
fuse the learned representations from each sub-networks with the unit models. Formally, given a sub-network g ∈ G with
node set Vg , by applying unit model Mj ∈ M to g, we can represent the learned representation for node vq ∈ Vg as vector
zj,q, where q denotes the unique node index in the original complete network G before sampling. For the nodes vp ∈/ Vg,
we can denote its representation vector zj,p = null, which denotes a dummy vector of length d. Formally, we will be able
represent the learned representation feature vector for node vq as
zq =	zj,q,
(1)
g∈G,Mj ∈M,
where operator t denotes the concatenation operation of feature vectors.
Considering that in the network sampling step, not all nodes will be selected in sub-networks. For the nodes vp ∈/ Vg, ∀g ∈
G, we will not be able to learn its representation feature vector (or its representation will be filled with a list of dummy
6
Under review as a conference paper at ICLR 2019
empty vector). Formally, we can represent these non-appearing nodes as set Vn = V \ Sg∈G Vg. In this paper, to compute
the representation for these nodes, we propose to propagate the learned representation from their neighborhoods to them
instead. Formally, given node vp ∈ Vn and its neighbor set Γ(vp) = {vo|vo ∈ V ∧ (u, vp) ∈ E}, if there exists node in
Γ(vp) with non-empty representation feature vector, we can represent the propagated representation for vp as
Zp = N X I(Vo ∈ Vn) ∙ Zo,	⑵
vo ∈Γ(vp)
where N = Pvo仃凡)I(Vo ∈ Vn). In the case that Γ(vp) ⊂ Vn, random padding will be applied to get the representation
vector Zp for node vp .
4.3.2	Global Ensemble
Generally, these different network sampling strategies introduced at the beginning in Section 4.1 captures different lo-
cal/global structures of the network, which will all be useful for the node representation learning. In the global result
ensemble step, we propose to group these features together as the output.
Formally, based on the BFS, DFS, HS, biased node and biased edge sampling strategies, to differentiate their learned
representations for nodes (e.g., Vq ∈ V), we can denoted their representation feature vectors as ZqBFS , ZqDFS , ZqHS, ZqNS and
ZqES respectively. In the case that node Vq has never appeared in any sub-networks in any of the sampling strategies, its
corresponding feature vector can be denoted as a dummy vector filled with 0s. In the global ensemble step, we propose to
linearly sum the feature vectors to get the fuses representation Zq as follows:
Zq = X	Wi ∙ Zq.
i∈{BFS,DFS,HS,NS,ES}
Learning of the weight parameters wBFS , wDFS , wHS , wNS and wES is feasible with the complete network structure, but it
may introduce extra time costs and greatly degrade the efficiency seGEN. In this paper, we will simply assign them with
equal value, i.e., Zq is an average of zBfs, zDfs, zHs, ZNS and ZES learned with different sampling strategies.
4.4	Model Analysis
In this section, we will analyze the proposed model seGEN regarding its performance, running time and space cost, which
will also illustrate the advantages of seGEN compared with the other existing deep learning models.
4.4	. 1 Performance Analysis
Model seGEN, in a certain sense, can also be called a “deep” model. Instead of stacking multiple hidden layers inside one
single model like existing deep learning models, seGEN is deep since the unit models in the successive generations are
generated by a namely “evolutionary layer” which performs the validation, selection, crossover, and mutation operations
connecting these generations. Between the generations, these “evolutionary operations” mainly work on the unit model
variables, which allows the immigration of learned knowledge from generation to generation. In addition, via these gener-
ations, the last generation in seGEN can also capture the overall patterns of the dataset. Since the unit models in different
generations are built with different sampled training batches, as more generations are involved, the dataset will be samples
thoroughly for learning seGEN. There have been lots of research works done on analyzing the convergence, performance
bounds of genetic algorithms Rudolph (1994), which can provide the theoretic foundations for seGEN.
Due to the difference in parent model selection, crossover, mutation operations and different sampled training batches,
the unit models in the generations of seGEN may perform quite differently. In the last step, seGEN will effectively
combine the learning results from the multiple unit models together. With the diverse results combined from these different
learning models, seGEN is able to achieve better performance than each of the unit models, which have been effectively
demonstrated in Zhou et al. (2002).
4.4.2	Space and Time Complexity Analysis
According the the model descriptions provided in Section 4, we summarize the key parameters used in seGEN as follows,
which will help analyze its space and time complexity.
•	Sampling: Original data size: n. Sub-instance size: n0 . Pool size: p.
•	Learning: Generation number: K . Population size: m. Feature vector size: d. Training/Validation batch size: b.
Here, we will use network structured data as an example to analyze the space and time complexity of the seGEN model.
Space Complexity: Given a large-scale network with n nodes, the space cost required for storing the whole network in a
matrix representation is O(n2). Meanwhile, via network sampling, we can obtain a pool of sub-networks, and the space
required for storing these sub-networks takes O p(n0)2 . Generally, in application of SEGEN, n0 can take very small
number, e.g., 50, andP can take value P = c ∙ nn (C is a constant) so as to cover all the nodes in the network. In such a case,
the space cost of SEGEN will be linear to n, O(cn0n), which is much smaller than O(n2).
Time Complexity: Depending on the specific unit models used in composing SEGEN, we can represent the introduced
time complexity of learn one unit model with the original network with n nodes as O(f(n)), where f(n) is usually a high-
order function. Meanwhile, for learning SEGEN on the sampled sub-networks with n0 nodes, all the introduced time cost
7
Under review as a conference paper at ICLR 2019
Table 1: Representation Learning Experiment Results Comparison on Foursquare Network Dataset.
Network		AUC	Prec@500			Community		Density		Silhouette		
Recovery	1	5	10	1	5	10 I	Detection	5	25	50	5	25	50
SEGEN(PS2)	0.909 (2)	0.909 (2) 0.909 (2)	0.872 (2)	0.642 (3)	0.530 (3) I	SEGEN(PS3)	0.875 (2)	0.550 (2)	0.792 (3)	0.353 (2)	0.206 (2)	0.208 (3)
SEGEN(PS1)	0.817 (6)	0.819 (6) 0.818 (6)	0.772 (5)	0.400 (4)	0.266 (4) I	SEGEN(PS1)	0.792 (6)	0.477 (4)	0.742 (4)	0.317 (4)	0.188 (3)	0.156 (5)
SEGEN-HS(PS2)	0.935 (1)	0.936 (1) 0.936 (1)	0.852 (4)	0.388 (5)	0.000 (-) I	SEGEN-HS(PS3)	0.812 (5)	0.385 (11)	0.705 (5)	0.252 (10)	0.056 (6)	0.166 (4)
SEGEN-BFS(PS2)	0.860 (4)	0.859 (4) 0.858 (4)	0.428 (10)	0.000 (-)	0.000 (-) I	SEGEN-BFS(PS3)	0.746 (7)	0.425 (8)	0.587 (6)	0.206 (11)	0.022 (10)	0.108 (6)
SEGEN-DFS(PS2)	0.881 (3) 0.882 (3) 0.881 (3)		0.965 (1)	0.814 (2)	0.648 (2) I	SEGEN-DFS(PS3)	0.860 (4)	0.532 (3)	0.436 (11)	0.280 (9)	0.017 (11)	-0.006 (11)
SEGEN-NS(PS2)	0.801 (7)	0.797 (7) 0.797 (7) 0.256 (11)		0.002 (10) 0.002 (9) I		SEGEN-NS(PS3)	0.871 (3)	0.425 (8)	0.824 (2)	0.327 (3)	0.060 (5)	0.294 (2)
SEGEN-ES(PS2)	0.820 (5)	0.822 (5) 0.822 (5)	0.872 (2)	0.872 (1)	0.872 (1)∣	SEGEN-ES(PS3)	0.948 (1)	0.933 (1)	0.924 (1)	0.482 (1)	0.429 (1)	0.407 (1)
LINE	0.536 (9) 0.537 (9) 0.537 (9)		0.712 (6)	0.268 (9)	0.172 (7) I	LINE	0.695 (8)	0.443 (6)	0.478 (8)	0.311 (5)	0.046 (8)	0.082 (8)
DeepWalk	0.536 (9) 0.537 (9) 0.537 (9)		0.686 (9)	0.308 (7)	0.184 (6) I	DeepWalk	0.695 (8)	0.449 (5)	0.485 (7)	0.311 (5)	0.042 (9)	0.082 (8)
node2vec	0.538 (8) 0.540 (8) 0.539 (8)		0.692 (8)	0.299 (8)	0.162 (8) I	node2vec	0.691 (11) 0.419 (10)		0.469 (9)	0.297 8	0.066 (4)	0.070 (10)
HPE	0.536 (9) 0.537 (9) 0.537 (9)		0.708 (7)	0.354 (6)	0.188 (5) I	HPE	0.695 (8)	0.431 (7)	0.465 (10)	0.311 (5)	0.051 (7)	0.089 (7)
will be O (Km(b ∙ f (n0) + d ∙ n0)), where term d ∙ n0 (an approximation to variable size) represents the cost introduced in
the unit model crossover and mutation about the model variables. Here, by assigning b with a fixed value b = C ∙/,the
time complexity of SEGEN will be reduced to O (Kmcfn) ∙ n + Kmdn), which is linear to n.
4.4.3	Advantages Over Deep Learning Models
Compared with existing deep learning models based on the whole dataset, the advantages of seGEN are summarized
below:
•	Less Data for Unit Model Learning: For each unit model, which are of a “shallow” and “narrow” structure
(shallow: less or even no hidden layers, narrow: based on sampled sub-instances with a much smaller size),
which needs far less variables and less data for learning each unit model.
•	Less Computational Resources: Each unit model is of a much simpler structure, learning process of which con-
sumes far less computational resources in both time and space costs.
•	Less Parameter Tuning: SEGEN can accept both deep (in a simpler version) and shallow learning models as
the unit model, and the hyper-parameters can also be shared among the unit models, which will lead to far less
hyper-parameters to tune in the learning process.
•	Sound Theoretic Explanation: The unit learning model, genetic algorithm and ensemble learning (aforemen-
tioned) can all provide the theoretic foundation for seGEN, which will lead to sound theoretic explanation of
both the learning result and the seGEN model itself.
5	Experiments
To test the effectiveness of the proposed model, extensive experiments will be done on several real-world network structured
datasets, including social networks, images and raw feature representation datasets. In this section, we will first introduce
the detailed experimental settings, covering experimental setups, comparison methods, evaluation tasks and metrics for the
social network representation learning task. After that, we will show its convergence analysis, parameter analysis and the
main experimental results of seGEN on the social network datasets. Finally, we will provide the experiments seGEN
based on the image and raw feature representation datasets involving CNN and MLP as the unit models respectively.
5.1	Social Network Dataset Experimental Settings
5.1.1	Experimental Setup
The network datasets used in the experiments are crawled from two different online social networks, Twitter and
Foursquare, respectively. The Twitter network dataset involves 5, 120 users and 130, 576 social connections among the user
nodes. Meanwhile, the Foursquare network dataset contains 5, 392 users together with the 55, 926 social links connecting
them. According to the descriptions of seGEN, based on the complete input network datasets, a set of sub-networks are
randomly sampled with network sampling strategies introduced in this paper, where the sub-network size is denoted as n0,
and the pool size is controlled by p. Based on the training/validation batches sampled sub-network pool, K generations of
unit models will be built in SEGEN, where each generation involves m unit models (convergence analysis regarding pa-
rameter K is available in Section 7.1.1). Finally, the learning results at the ending generation will be effectively combined
to generate the ensemble output. For the nodes which have never been sampled in any sub-networks, their representations
can be learned with the diffusive propagation from their neighbor nodes introduced in this paper. The learned results by
seGEN will be evaluated with two application tasks, i.e., network recovery and community detection respectively. The
detailed parameters sensitivity analysis is also available in Section 7.1.2.
5.1.2	Comparison Methods
The network representation learning comparison models used in this paper are listed as follows
8
Under review as a conference paper at ICLR 2019
Table 2: Representation Learning Experiment Results Comparison on Twitter Network Dataset.
Network	AUC			Prec@500		Community		Density		Silhouette		
Recovery	1	5	10	1	5	10 I	Detection	5	25	50	5	25	50
SEGEN(PS4)	0.879 (1)	0.881 (1)	0.881 (1)	0.914 (3)	0.638 (3) 0.370 (3)∣	SEGEN(PS5)	0.980 (2)	0.845 (3)	0.770 (3)	0.566 (4)	0.353 (3)	0.341 (2)
SEGEN(PS1)	0.814 (4)	0.813 (4)	0.814 (4)	0.606 (4) 0.194 (4) 0.102 (4)∣		SEGEN(PS1)	0.786 (7)	0.751 (4)	0.753 (4)	0.481 (10) 0.328 (4)		0.318 (4)
SEGEN-HS(PS4)	0.862 (2)	0.863 (2)	0.863 (2)	0.594 (5)	0.000 (-) 0.000 (-) I	SEGEN-HS(PS5)	0.967 (6)	0.171 (11) 0.116 (11)		0.549 (5)	-0.021 (11) -0.032 (10)	
SEGEN-BFS(PS4)	0.846 (3)	0.846 (3)	0.846 (3)	0.570 (6)	0.000 (-) 0.000 (-) I	SEGEN-BFS(PS5)	0.973 (4)	0.973 (2)	0.886 (2)	0.622 (2)	0.517 (2)	0.330 (3)
segen-dfs(ps4) 0.503 (10) 0.508 (10) 0.507 (10) 0.268 (10)					0.000 (-) 0.000 (-) I	SEGEN-DFS(PS5)	0.994 (1)	0.338 (9) 0.143 (10)		0.532 (7)	0.237 (6)	-0.023 (9)
SEGEN-NS(PS4)	0.794 (5)	0.794 (5)	0.795 (5)	0.962 (2) 0.824 (2) 0.688 (2)∣		SEGEN-NS(PS5)	0.979 (3)	0.981 (1)	0.965 (1)	0.597 (3)	0.525 (1)	0.461 (1)
SEGEN-ES(PS4)	0.758 (6)	0.760 (6)	0.760 (6)	1.000 (1)	1.000 (1) 1.000 (1)∣	SEGEN-ES(PS5)	0.970 (5)	0.525 (8)	0.482 (8)	0.693 (1)	0.284 (5)	-0.059 (11)
LINE	0.254 (11) 0.254 (11) 0.253 (11) 0.106 (11)				0.018 (7 ) 0.006 (7)∣	LINE	0.524 (11) 0.324 (10)		0.251 (9)	0.465 (11) -0.012 (9)		-0.012 (7)
DeepWalk	0.533 (9)	0.531 (9)	0.532 (9)	0.524 (9) 0.146 (6 ) 0.070 (6)∣		DeepWalk	0.545 (10)	0.542 (7)	0.503 (7)	0.492 (9)	0.173 (8)	0.150 (6)
node2vec	0.704 (7)	0.703 (7)	0.704 (7)	0.528 (8)	0.012 (8) 0.000 (-) I	node2vec	0.697 (8)	0.693 (5)	0.694 (5)	0.530 (8)	-0.020 (10) -0.015 (8)	
HPE	0.593 (8)	0.595 (8)	0.594 (8)	0.534 (7)	0.186 (5 ) 0.094 (5) I	HPE	0.579 (9)	0.579 (6)	0.579 (6)	0.544 (6)	0.208 (7)	0.187 (5)
•	SEGEN: Model SEGEN proposed in this paper is based on the genetic algorithm and ensemble learning, which
effectively combines the learned sub-network representation feature vectors from the unit models to generate the
feature vectors of the whole network.
•	LINE: The LINE model is a scalable network embedding model proposed in Tang et al. (2015), which optimizes
an objective function that preserves both the local and global network structures. LINE uses a edge-sampling
algorithm to addresses the limitation of the classical stochastic gradient descent.
•	DEEPWALK: The DEEPWALK model Perozzi et al. (2014) extends the word2vec model Mikolov et al. (2013) to
the network embedding scenario. DeepWalk uses local information obtained from truncated random walks to
learn latent representations.
•	NODE2VEC: The NODE2VEC model Grover & Leskovec (2016) introduces a flexible notion of a node’s net-
work neighborhood and design a biased random walk procedure to sample the neighbors for node representation
learning.
•	HPE: The HPE model Chen et al. (2016) is originally proposed for learning user preference in recommendation
problems, which can effectively project the information from heterogeneous networks to a low-dimensional space.
5.1.3	Evaluation Tasks and Metrics
The network representation learning results can hardly be evaluated directly, whose evaluations are usually based on certain
application tasks. In this paper, we propose to use application tasks, network recovery and clustering, to evaluate the learned
representation features from the comparison methods. Furthermore, the network recovery results are evaluated by metrics,
like AUC and Precision@500. Meanwhile the clustering results are evaluated by Density and Silhouette. Without specific
remarks, the default parameter setting for seGEN in the experiments will be Parameter Setting 1 (PS1): sub-network size:
10, pool size: 200, batch size: 10, generation unit model number: 10, generation number: 30.
5.2	Social Network Dataset Experimental Results
The model training convergence analysis, and detailed analysis about the pool sampling and model learning parameters
is available in the Appendix in Section 7.1. Besides these analysis results, we also provide the performance analysis of
seGEN and baseline methods in Tables 1-2, where the parameter settings are specified next to the method name. We
provide the rank of method performance among all the methods, which are denoted by the numbers in blue font, and the
top 5 results are in a bolded font. As shown in the Tables, we have the network recovery and community detection results on
the left and right sections respectively. For the network recovery task, we change the ratio of negative links compared with
positive links with values {1, 5, 10}, which are evaluated by the metrics AUC and Prec@500. For the community detection
task, we change the number of clusters with values {5, 25, 50}, and the results are evaluated by the metrics Density and
Silhouette.
Besides PS1 introduced at the beginning of Section 5.1, we have 4 other parameter settings selected based on the parameter
analysis introduced before. PS2 for network recovery on Foursquare: sub-network size 50, pool size 600, batch size 5,
generation size 50. PS3 for community detection on Foursquare: sub-network size 25, pool size 300, batch size 35,
generation size 5. PS4 for network recovery on Twitter: sub-network size 50, pool size 700, batch size 10, generation size
5. PS5 for community detection on Twitter: sub-network size 45, pool size 500, batch size 50, generation size 5.
According to the results shown in Table 1, method seGEN with PS2 can obtain very good performance for both the
network recovery task and the community detection task. For instance, for the network recovery task, method seGEN
with PS2 achieves 0.909 AUC score, which ranks the second and only lose to seGEN-HS with PS2; meanwhile, seGEN
with PS2 also achieves the second highest Prec@500 score (i.e., 0.872 for np-ratio = 1) and the third highest Prec@500
score (i.e., 0.642 and 0.530 for np-ratios 5 and 10) among the comparison methods. On the other hand, for the community
detection task, seGEN with PS3 can generally rank the second/third among the comparison methods for both density and
9
Under review as a conference paper at ICLR 2019
Table 3: Experiments on MNIST Dataset.
Comparison Methods	Accuracy Rate%
SEGEN (CNN)	99.37
LeNet-5	99.05 LeCun etal.(1998)
gcForest	99.26 Zhou&Feng (2017b)
Deep Belief Net	98.75 Hinton et al. (2006)
Random Forest	96.8Zhou&Feng (2017b)
SVM (rbf)	98.60 DeCoste & Scholkopf (2002)
Table 4: Experiments on Other Datasets.
Comparison Methods	Accuracy Rate % on Datasets		
	YEAST	ADULT	LETTER
SEGEN (MLP)	63.70	87.05	96.90
MLP	62.05	85.03	-96.70-
gcForest	63.45	86.40	-97.40-
Random Forest	60.44	85.63	-96.28-
SVM (rbf)	40.76	76.41-	-97.06-
kNN (k=3)	48.80	76.00	95.23
silhouette evaluation metrics. For instance, with the cluster number is 5, the density obtained by SEGEN ranks the second
among the methods, which loses to seGEN-LS only. Similar results can be observed for the Twitter network as shown in
Figure 2.
By comparing seGEN with seGEN merely based on HS, BFS, DFS, NS, LS, we observe that the variants based on
one certain type of sampling strategies can obtain relatively biased performance, i.e., good performance for the network
recovery task but bad performance for the community detection task or the reverse. For instance, as shown in Figure 1,
methods seGEN with HS, BFS, DFS performs very good for the network recovery task, but its performance for the
community detection ranks even after LINE, HPE and DeepWalk. On the other hand, seGEN with NS and LS is shown
to perform well for the community detection task instead in Figure 1, those performance ranks around 7 for the network
recovery task. For the Twitter network, similar biased results can be observed but the results are not identically the same.
Model seGEN combining these different sampling strategies together achieves relatively balanced and stable performance
for different tasks. Compared with the baseline methods LINE, HPE, DeepWalk and node2vec, model seGEN can
obtain much better performance, which also demonstrate the effectiveness of seGEN as an alternative approach for deep
learning models on network representation learning.
5.3	Experiments on Other Datasets and Unit Models
Besides the extended autoencoder model and the social network datasets, we have also tested the effectiveness of seGEN
on other datasets and with other unit models. In Table 3, we show the experimental results of seGEN and other baseline
methods on the MNIST hand-written image datasets. The dataset contains 60, 000 training instances and 10, 000 testing
instances, where each instance is a 28 × 28 image with labels denoting their corresponding numbers. Convolutional Neural
Network (CNN) is used as the unit model in seGEN, which involves 2 convolutional layers, 2 max-pooling layers, and
two fully connection layers (with a 0.2 dropout rate). ReLU is used as the activation function in CNN, and we adopt Adam
as the optimization algorithm. Here, the images are of a small size and no sampling is performed, while the learning results
of the best unit model in the ending generation (based on a validation batch) will be outputted as the final results. In the
experiments, seGEN (CNN) is compared with several classic methods (e.g., LeNet-5, SVM, Random Forest, Deep Belief
Net) and state-of-the-art method (gcForest). According to the results, seGEN (CNN) can outperform the baseline methods
with great advantages. The Accuracy rate obtained by SEGEN is 99.37%, which is much higher than the other comparison
methods.
Meanwhile, in Table 4, we provide the learning results on three other benchmark datasets, including YEAST1, ADULT2
and LETTER3. These three datasets are in the traditional feature representations. Multi-Layer Perceptron (MLP) is used
as the unit model in seGEN for these three datasets. We cannot find one unified architecture of MLP, which works for
all these three datasets. In the experiments, for the YEAST dataset, the MLP involves 1 input layer, 2 hidden layers and 1
output layers, whose neuron numbers are 8-64-16-10; for the ADULT, the MLP architecture contains the neurons 14-70-
50-2; for the LETTER dataset, the used MLP has 3 hidden layers with neurons 16-64-48-32-26 at each layer respectively.
The Adam optimization algorithm with 0.001 learning rate is used to train the MLP model. For the ensemble strategy in
these experiments, the best unit model is selected to generate the final prediction output. According to the results, compared
with the baseline methods, seGEN (MLP) can also perform very well with MLP on the raw feature representation datasets
with great advantages, especially the YEAST and ADULT datasets. As to the LETTER dataset, seGEN (MLP) only loses
to gcForest, but can outperform the other methods consistently.
6	Conclusion
In this paper, we have introduced an alternative approach to deep learning models, namely seGEN. Significantly different
from the existing deep learning models, seGEN builds a group of unit models generations by generations, instead of
building one single model with extremely deep architectures. The choice of unit models covered in seGEN can be either
traditional machine learning models or the latest deep learning models with a “smaller” and “narrower” architecture.
seGEN has great advantages over deep learning models, since it requires much less training data, computational resources,
parameter tuning efforts but provides more information about its learning and result integration process. The effectiveness
of efficiency of seGEN have been well demonstrated with the extensive experiments done on the real-world network
structured datasets.
1https://archive.ics.uci.edu/ml/datasets/Yeast
2https://archive.ics.uci.edu/ml/datasets/adult
3https://archive.ics.uci.edu/ml/datasets/letter+recognition
10
Under review as a conference paper at ICLR 2019
References
E. Arisoy, T. Sainath, B. Kingsbury, and B. Ramabhadran. Deep neural network language models. In WLM, 2012.
A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for modeling multi-
relational data. In NIPS. 2013.
S.	Chang, W. Han, J. Tang, G. Qi, C. Aggarwal, and T. Huang. Heterogeneous network embedding via deep architectures.
In KDD, 2015.
C. Chen, M. Tsai, Y. Lin, and Y. Yang. Query-based music recommendations via preference embedding. In RecSys, 2016.
T.	Chen and Y. Sun. Task-guided and path-augmented heterogeneous network embedding for author identification. CoRR,
abs/1612.02814, 2016.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng.
Large scale distributed deep networks. In NIPS, 2012.
D. Decoste and B. Scholkopf. Training invariant support vector machines. Mach. Learn., 2002.
L. Deng, G. Hinton, and B. Kingsbury. New types of deep neural network learning for speech recognition and related
applications: An overview. In ICASSP, 2013.
I.	Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.
org.
A. Grover and J. Leskovec. Node2vec: Scalable feature learning for networks. In KDD, 2016.
S. Hill. Elite and upper-class families. In Families: A Social Class Perspective. 2012.
G.	Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Comput., 2006.
G.	Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kings-
bury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 2012.
H.	Jaeger. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the “echo state network”
approach. Technical report, Fraunhofer Institute for Autonomous Intelligent Systems (AIS), 2002.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS,
2012.
Y	. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of
the IEEE, 1998.
Y	. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521, 2015. doi: 10.1038/nature14539. http://dx.doi.
org/10.1038/nature14539.
Y	. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning entity and relation embeddings for knowledge graph completion. In
AAAI, 2015.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their
compositionality. In NIPS, 2013.
A.	Mnih and G. Hinton. A scalable hierarchical distributed language model. In NIPS. 2009.
B.	Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014.
G. Rudolph. Convergence analysis of canonical genetic algorithms. IEEE Transactions on Neural Networks, 1994.
R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 2009.
J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW,
2015.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising autoencoders: Learning useful repre-
sentations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 2010.
D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In KDD, 2016.
Z. Wang, J. Zhang, J. Feng, and Z. Chen. Knowledge graph embedding by translating on hyperplanes. In AAAI, 2014.
11
Under review as a conference paper at ICLR 2019
J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to rank with joint word-image embeddings.
Journal of Machine Learning, 2010.
J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI, 2011.
Z. Zhou and J. Feng. Deep forest: Towards an alternative to deep neural networks. In IJCAI, 2017a.
Z. Zhou and J. Feng. Deep forest: Towards an alternative to deep neural networks. In IJCAI, 2017b.
Z. Zhou, J. Wu, and W. Tang. Ensembling neural networks: Many could be better than all. Artif. Intell., 2002.
12
Under review as a conference paper at ICLR 2019
7 Appendix
7.1	Social Network Dataset Experimental Analysis
In this part, we will provide experimental analysis about the convergence and parameters of seGEN, including the sub-
network size, the pool size, batch size and generation size respectively.
7.1.1	Convergence Analysis
Generation Number
(a) Foursquare: Convergence
Generation Number
(b) Twitter: Convergence
Figure 3: Convergence Analysis on Foursquare and Twitter.
The learning process of seGEN involves multiple generations. Before showing the experimental results, we will analyze
how many generations will be required for achieving stable results. In Figure 3, we provide the introduced loss by the
seGEN on both Foursquare and Twitter networks, where the x axis denotes the generations and y axis represents the sum
of introduced Lc loss on the validation set based on all these 5 different sampling strategies. According to the results,
model SEGEN can converge within less 30 generations for the network representation learning on both Foursquare and
Twitter, which will be used as the max-generation number throughout the following experiments.
7.1.2 Pool Sampling and Model Learning Parameter Analysis
(a) Foursquare:
0.85
• 0∙85 u
■■ ■
/h:第
/ 1000
/ 800
z⅛铲
200
AUC
-0.01
-0.01
--0.01 ⅛
-0.02 §
--0.02 £
-0.02 ω
-0.02
-0.03
-0.03
(d) Foursquare:
∖⅞⅞¾s^
•InI—，
/ 1000
/ 800
Z 600V
400 ʃ
200 N
(b) Foursquare:
OOg
皤搬期精
(c) Foursquare: Density
PreC@500
(f) TWitter
0.21
0.20
0.18 ⅛
0.17 ⅛
0.15 g
0.14 £
0.13 ω
0.11
0.10
0.08
∖∕n∕¾∕a
rf∕¾½¾
0.79
0.74
0.69 g
0.64 tn
0.59 ®
0.54 ⅛
0.49 H
0.45
0.40
0.35
1000
800
600"
400 6*
200 Z
:PreC@500
—/ 1000
/ 800
/ 600
400 3
200 qθ°
Silhouette
io
20
gNeMe
30 40
AkSi%： 50
0.75
0.72
0.69 a
0.66 ⅛
0.63 c
0.60 A
0.57
0.54
0.51
0.48
1000
800
200 √5
(g) Twitter: Density
(h) Twitter: Silhouette
Figure 4: Sampling Parameter Analysis on Foursquare and Twitter.
1000
800
600步
400 ʃ
200 Z
-


In Figure 4, we show the sensitivity analysis about the network sampling parameters, i.e., sub-network size and the
pool size, evaluated by AUC, Prec@500, Density and Silhouette respectively, where Figures 4(a)-4(d) are about the
13
Under review as a ConferenCe paper at ICLR 2019
Foursquare and Figures 4(e)-4(h) are about the Twitter network. The sub-network size parameter Changes with values
in {5,10,15,…，50} and pool size changes with values in range {100, 200,…，1000}.
ACCording to the plots, for the Foursquare network, larger sub-network size and larger pool size will lead to better per-
formanCe in the network reCovery task; meanwhile, smaller sub-network size will aChiver better performanCe for the
Community deteCtion task. For instanCe, SEGEN Can aChieve the best performanCe with sub-network size 50 and pool size
600 for the network reCovery task; and SEGEN obtain the best performanCe with sub-network size 25 and pool size 300
for the Community deteCtion. For the Twitter network, the performanCe of seGEN is relatively stable for the parameters
analyzed, whiCh has some fluCtuations for Certain parameter values. ACCording to the results, the optimal sub-network and
pool sizes parameter values for the network reCovery task are 50 and 700 for the network reCovery task; meanwhile, for the
Community deteCtion task, the optimal parameter values are 45 and 500 respeCtively.
0.82
0.82
0.82
0.82
0.82
0.82
0.82
0.82
0.82
0.81
(a) Foursquare: AUC
0.82
0.81
P 0.81
S 0.80
® 0.79
g 0.79
o 0.78
0.77
0.76
0.76
30 2。1°
50 4。
¾⅛0 30
y 5。
(b) Foursquare: PreC@500
(C) Foursquare: Density

I巡
0.19》
解
0.18
(d) Foursquare:
,-0.01
-0.01
-0.02 φ
-0.02 a
-0.02 g
-0.02 o
-0.02 ≡
-0.02 S
-0.02
-0.02
Silhouette
0.82
0.82
0.82
0.82
≥ 0.82
C 0.82
0.81
0.81
0.81
0.81
(e) Twitter: AUC
0.82
0.79
0.77
0.74
0.71
(f) TWitter
------ 30 20 10
50	50 40 BatChaZe
:PreC@500
(g) Twitter:
0.49
0.49
0.49
Density
0.48 ⅛
0.48 c
0.48 M
0.48
0.47
0.47
0.47
(h) TWitter: Silhouette



Figure 5: BatCh and Generation Size Parameter Analysis on Foursquare and Twitter.
In Figure 5, we provide the parameter sensitivity analysis about the batCh size and generation size (i.e., the number of unit
models in eaCh generation) on Foursquare and Twitter. We Change the generation size and batCh size both with values in
{5,10,15, ∙∙∙ , 50}, and compute the AUC, PreC@500, Density and Silhouette scores obtained by seGEN.
ACCording Figures 5(a)-5(d), batCh size has no signifiCant impaCt on the performanCe of seGEN, and the generation size
may affect seGEN greatly, especially for the Prec@500 metric (the AUC obtained by seGEN changes within range [0.81,
0.82] with actually minor fluctuation in terms of the values). The selected optimal parameter values selected for network
recovery are 50 and 5 for generation and bath sizes. Meanwhile, for the community detection, SEGEN performs the best
with smaller generation and batch size, whose optimal values are 5 and 35 respectively. For the Twitter network, the impact
of the batch size and generation size is different from that on Foursquare: smaller generation size lead to better performance
for seGEN evaluated by Prec@500. The fluctuation in terms of AUC is also minor in terms of the values, and the optimal
values of the generation size and batch size parameters for the network recovery task are 5 and 10 respectively. For the
community detection task on Twitter, we select generation size 5 and batch size 40 as the optimal value.
14