Under review as a conference paper at ICLR 2019
RELWALK - A Latent Variable Model Approach
to Knowledge Graph Embedding
Anonymous authors
paper under double-blind review
Ab stract
Knowledge Graph Embedding (KGE) is the task of jointly learning entity and rela-
tion embeddings for a given knowledge graph. Existing methods for learning KGEs
can be seen as a two-stage process where (a) entities and relations in the knowl-
edge graph are represented using some linear algebraic structures (embeddings),
and (b) a scoring function is defined that evaluates the strength of a relation that
holds between two entities using the corresponding relation and entity embeddings.
Unfortunately, prior proposals for the scoring functions in the first step have been
heuristically motivated, and it is unclear as to how the scoring functions in KGEs
relate to the generation process of the underlying knowledge graph. To address
this issue, we propose a generative account of the KGE learning task. Specifically,
given a knowledge graph represented by a set of relational triples (h, R, t), where
the semantic relation R holds between the two entities h (head) and t (tail), we
extend the random walk model (Arora et al., 2016a) of word embeddings to KGE.
We derive a theoretical relationship between the joint probability p(h, R, t) and the
embeddings of h, R and t. Moreover, we show that marginal loss minimisation,
a popular objective used by much prior work in KGE, follows naturally from
the log-likelihood ratio maximisation under the probabilities estimated from the
KGEs according to our theoretical relationship. We propose a learning objective
motivated by the theoretical analysis to learn KGEs from a given knowledge graph.
The KGEs learnt by our proposed method obtain state-of-the-art performance on
FB15K237 and WN18RR benchmark datasets, providing empirical evidence in
support of the theory.
1	Introduction
Knowledge graphs such as Freebase (Bollacker et al., 2008) organise information in the form of
graphs, where entities are represented by vertices in the graph and the relation between two entities is
represented by the edge that connects the corresponding two vertices. By embedding entities and
relations that exist in a knowledge graph in some (possibly lower-dimensional and latent) space
we can infer previously unseen relations between entities, thereby expanding a given knowledge
graph (Nickel et al., 2016; Yang et al., 2015; Lin et al., 2015; Nickel et al., 2011; Trouillon et al.,
2016; Wang et al., 2017; Bordes et al., 2011).
Existing KGE methods can be seen as involving two main steps. First, given a knowledge graph
represented by a set of relational triples (h, R, t), where a semantic relation R holds between a head
entity h and a tail entity t, entities and relations are represented using some mathematical structures
such as vectors, matrices or tensors. Second, a scoring function is proposed that evaluates the
relational strength of a triple (h, R, t) and entity and relation embeddings that optimise the defined
scoring function are learnt using some optimisation method. Table 1 shows some of the scoring
functions proposed in prior work in KGE learning.
Despite the wide applications of entity and relation embeddings created via KGE methods, the
existing scoring functions are motivated heuristically to capture some geometric requirements of
the embedding space. For example, TransE (Bordes et al., 2011) assumes that the entity and
relation embeddings co-exist in the same (possibly lower dimensional) vector space and translating
(shifting) the head entity embedding by the relation embedding must make it closer to the tail entity
embedding, whereas ComplEx (Trouillon et al., 2016) models the asymmetry in relations using
1
Under review as a conference paper at ICLR 2019
Model	Score function f (h, R, t)	Relation parameters
Unstructured (Bordes et al., 2011)	Ilh - tk'l∕2	none
Structured embeddings (Bordes et al., 2011)	kR1h - R2tk'ι,2	R1,R2 ∈ Rd×d
TransE (Bordes et al., 2011)	I|h + R - tk'l∕2	R ∈ Rd
DistMult (Yang et al., 2015)	hh, R, ti	R ∈ Rd
RESCAL (Nickel et al., 2011)	h>Rt	Rd×d
ComplEx (Trouillon et al., 2016)	hh, R, ti	R∈ Cd
Table 1: Score functions proposed in selected prior work on KGE. Entity embeddings h, t ∈ Rd
are vectors in all models, except in ComplEx where h, t ∈ Cd. Here, X'“ denotes either '1 or '2
norm of the vector x. In ComplEx, X is the elementwise complex conjugate, and(•,•,•)denotes the
component-wise multi-linear inner-product.
the component-wise multi-linear inner-product among entity and relation embeddings. Relational
triples extracted from a given knowledge graph are used as positive training instances, whereas
pseudo-negative (Bordes et al., 2011) instances are automatically generated by randomly corrupting
positive instances. Finally, KGE are learnt such that the prediction loss computed over the positive
and negative instances is minimised.
Despite the good empirical performances of the existing KGE methods, theoretical understanding of
KGE methods is comparatively under developed. For example, it is not clear how the heuristically
defined KGE objectives relate to the generative process of a knowledge graph. In this paper, we
attempt to fill this void by providing a theoretical analysis of KGE. Specifically, in section 2, we
propose a generative process where we explain the formation of a relation R between two entities
h and t using the corresponding relation and entity embeddings. Following this generative story,
we derive a relationship between the probability of R holding between h and t, p(h, t | R), and
the embeddings of R, h and t. Interestingly, the derived relationship is not covered by any of the
previously proposed heuristically-motivated scoring functions, providing the first-ever KGE method
with a provable generative explanation.
Next, in section 3, we show that the margin loss, which has been popularly used as a training objective
in prior work on KGE, naturally arises as the log-likelihood ratio computed from p(h, t | R). Based
on this result, we derive a training objective that we subsequently optimise for learning KGEs that
satisfy our theoretical relationship. Using standard benchmark datasets proposed in prior work on
KGE learning, we evaluate the learnt KGEs on a link prediction task and a triple classification task.
Experimental results show that the learnt KGEs obtain state-of-the-art performance on FB15K237
and WN18RR benchmarks, thereby providing empirical evidence to support the theoretical analysis.
2	Relational Walk
Let us consider a knowledge graph D where the knowledge is represented by relational triples
(h, R, t) ∈ D. Here, R is a relational predicate of two arguments, where h (head) and t (tail) entities
respectively filling the first and second arguments. We assume relations to be asymmetric in general.
In other words, if (h, R, t) ∈ D then it does not necessarily follow that (t, R, h) ∈ D. The goal
of KGE is to learn embeddings (representations) for the relations and entities in the knowledge
graph such that the entities that participate in similar relations are embedded closely to each other
in the entity embedding space, while at the same time relations that hold between similar entities
are embedded closely to each other in the relational embedding space. We call the learnt entity
and relation embeddings collectively as KGEs. Following prior work on KGE (Bordes et al., 2011;
Trouillon et al., 2016; Yang et al., 2015), we assume that entities and relations are embedded in the
same vector space, allowing us to perform linear algebraic operations using the embeddings in the
same vector space.
Let us consider a random walk characterised by a time-dependent knowledge vector ck , where k is
the current time step. The knowledge vector represents the knowledge we have about a particular
group of entities and relations that express some facts about the world. For example, the knowledge
that we have about people that are employed by companies can be expressed using entities of classes
2
Under review as a conference paper at ICLR 2019
such as people and organisation, using relations such as CEO-of, employed-at, works-for, etc. We
assume that entities h and t are represented by time-independent d-dimensional vectors, respectively
h, t ∈ Rd.
We assume the task of generating a relational triple (h, R, t) in a given knowledge graph to be a
two-step process as described next. First, given the current knowledge vector at time k, c = ck and
the relation R, we assume that the probability of an entity h satisfying the first argument of R to be
given by (1).
p(h | R, c) = ɪ exp (h>Rιc) .	(1)
Zc
Here, R1 ∈ Rd×d is a relation-specific orthogonal matrix that evaluates the appropriateness of h for
the first argument of R. For example, if R is the CEO-of relation, we would require a person as
the first argument and a company as the second argument of R. However, note that the role of R1
extends beyond simply checking the types of the entities that can fill the first argument of a relation.
For our example above, not all people are CEOs and R1 evaluates the likelihood of a person to be
selected as the first argument of the CEO-of relation. Zc is a normalisation coefficient such that
Ph∈V p(h | R, c) = 1, where the vocabulary V is the set of all entities in the knowledge graph.1
After generating h, the state of our random walker changes to c0 = ck+1 , and we next generate the
second argument of R with the probability given by (2).
p(t | R, c') = ɪ exp (t>R2d).	⑵
Zc0
Here, R2 ∈ Rd×d is a relation-specific orthogonal matrix that evaluates the appropriateness of t as the
second argument of R. Zc0 is a normalisation coefficient such that Pt∈V p(t | R, c) = 1. Following
our previous example of the CEO-of relation, R2 evaluates the likelihood of an organisation to
be a company with a CEO position. Importantly, R1 and R2 are representations of the relation R
and independent of the entities. Therefore, we consider (R1 and R2) to collectively represent the
embedding of R. Orthogonality of R1 , R2 is a requirement for the mathematical proof and also act as
a regularisation constraint to prevent overfitting by restricting the relational embedding space. We
first perform our mathematical analysis for relational embeddings represented by orthogonal matrices
and discuss later how this requirement can be relaxed.
We assume a slow random walk where the knowledge vectors do not change significantly between
consecutive time steps (ck ≈ ck+1 ). More specifically, we assume that kck - ck+1 k ≤ 2 for some
small 2 > 0. This is a realistic assumption for generating the two entity arguments in the same
relational triple because, if the knowledge vectors were significantly different in the two generation
steps, then it is likely that the corresponding relations are also different, which would not be coherent
with the above-described generative process. Moreover, we assume that the knowledge vectors are
distributed uniformly in the unit sphere and denote the distribution of knowledge vectors by C .
To learn KGEs, we must estimate the probability that h and t satisfy the relation R, p(h, t | R), which
can be obtained by taking the expectation of p(h, t | R, c, C0) w.r.t. c, C 〜C given by (3).
p(h, t | R) = Ec,c0 [p(h, t | R, c, c0)]	(3)
= Ec,c0 [p(h | R, c)p(t | R, c0)]	(4)
exp (h>Rιc) exp (t>R2c)
Zc	Zc0
(5)
Here, partition functions are given by Zc = Ph∈V Pc∈C exp (h>Rιc) and Zc0 =
Pt∈V Pc0∈C exp t>R2c0 . (4) follows from our two-step generative process where the gener-
ation of h and t in each step is independent given the relation and the corresponding knowledge
vectors.
Computing the expectation in (5) is generally difficult because of the two partition functions Zc and
Zc0 . However, Lemma 1 shows that the partition functions are narrowly distributed around a constant
value for all c (or c0) values with high probability.
1We can consider different vocabularies for the entities that can fill the first argument and second argument
of relations in a knowledge graph. However, for simplicity, we use a common vocabulary here.
3
Under review as a conference paper at ICLR 2019
Lemma 1 (Concentration Lemma). If the entity embedding vectors satisfy the Bayesian prior
V = Sv, where V is from the spherical Gaussian distribution, and S is a scalar random variable,
which is always bounded by a constant κ, then the entire ensemble of entity embeddings satisfies that
Pr [(1 - z)Z ≤ Zc ≤ (1 + z)Z] ≥ 1 - δ,	(6)
C〜C
for Ez = O(1∕√n), and δ = exp(-Ω(log2 n)), where n ≥ d is the number of words and Zc is the
partition function for c given by c∈V exp h>R1c .
proof: To prove the concentration lemma, we show that the mean Eh [Zc] of Zc is concentrated
around a constant for all knowledge vectors c and its variance is bounded. Recall that
Zc = X exp (h>R1c).
h∈V
(7)
If P is an orthogonal matrix and x is a vector, then kP>xk22 = (P>x)>(P>x) = x>PP>x = kxk22,
because P>P = I. Therefore, from (7) and the orthogonality of the relational embeddings, we see
that R1c is a simple rotation of c and does not alter the length of c. We represent h = Shh, where
Sh = khk and h is a unit vector (i.e. khk2 = 1) distributed on the spherical Gaussian with zero mean
and unit covariance matrix Id ∈ Rd×d . Let S be a random variable that has the same distribution
as Sh. Moreover, let us assume that S is upper bounded by a constant κ such that S ≤ κ. From the
assumption of the knowledge vector c, it is on the unit sphere as well, which is then rotated by R1.
We can write the partition function using the inner-product between two vectors h and R1c, Zc =
Ph∈V exp h>(R1c) . Arora et al. (2016a) showed that (Lemma 2.1 in their paper) the expectation
of a partition function of this form can be approximated as follows:
Ec[Zc] = nEc[exp (h>R1c)]
≥ nEc[1 + h>R1c] = n.
(8)
(9)
where n = |V | is the number of entities in the vocabulary. (8) follows from the expectation of a sum
and the independence of h and R1 from c. The inequality of (9) is obtained by applying the Taylor
expansion of the exponential series and the final equality is due to the symmetry of the spherical
Gaussian. From the law of total expectation, we can write
Ec[Zc] = nEc[exp (h>Rιc)] = nEsh [EχΜ [exp (h>Rιc) | Sh]] .	(10)
where, x = h>R1c. Note that conditioned on Sh, h is a Gaussian random variable with variance
σ2 = S2h. Therefore, conditioned on Sh, x is a random variable with variance σ2 = σh2. Using this
distribution, we can evaluate Ex|sh exp h>R1c as follows:
Eχ∣Sh [exp (h>Rιc) | Shi = / √1~^ exP
f 1
=	/ exp
Jx √2πσ2
=exp(σ2∕2).
exp(x)dx
—
(χ-σ22)2+。2力”,
Therefore, it follows that
Ec[Zc] = nEsh [exp(σ2 /2)] = nEsh [exp(S2h/2)] = nexp(S2/2),
(11)
(12)
(13)
(14)
where S is the variance of the `2 norms of the entity embeddings. Because the set of entities is given
and fixed, both n and σ are constants, proving that E[Zc] does not depend on c.
Next, we calculate the variance Vc [Zc] as follows:
Vc[Zc] = XVc[exp h>R1c]
h
≤ nEc hexp h>R1ci
= nEsh hEx|sh hexp 2h>R1t | shii .	(15)
4
Under review as a conference paper at ICLR 2019
Because 2h>R1t is a Gaussian random variable with variance 4σ2 = 4s2h from a similar calculation
as in (11) we obtain,
Eχ∣sh [exp (2h>Rιt) | s%] =exp(2σ2).	(16)
By substituting (16) in (15) we have that
Vc[Zc] ≤ nEsh [exp (2σ2)] = nE§h [exp(2s2)] ≤ Λn	(17)
for Λ = exp(8κ2) a constant bounding S ≤ K as stated.	□
From above, we have bounded both the mean and variance of the partition function by constants that
are independent of the knowledge vector. Note that neither exp h>R1c nor exp t>R2c0 are sub-
Gaussian nor sub-exponential. Therefore, standard concentration bounds derived for sub-Gaussian or
sub-exponential random variables cannot be used in our analysis. However, the argument given in
Appendix A.1 in Arora et al. (2016b) for a partition function with bounded mean and variance can be
directly applied to Zc in our case, which completes the proof of the concentration lemma. □
From the symmetry between h and t, Lemma 1 also applies for the partition function Pt∈V t>R2c0 .
Under the conditions required to satisfy Lemma 1, the following main theorem of this paper holds:
Theorem 1. Suppose that the entity embeddings satisfy (1). Then, we have
logp(h, t | R)= kR1>h +dR2""2 — 2log Z ± e.	(18)
_ , ____ ≈ , , .
for E = O(1∕√n) + O(1∕d), where
Z = Zc = Zc0.	(19)
The complete proof of Theorem 1 is given in Appendix A. Below we briefly sketch the main steps.
Proof sketch: Let F be the event that both c and c0 are within (1 ± Ez)Z. Then, from Lemma 1 and
the union bound, event F happens with probability at least 1 - 2 exp(-Ω(log2 n)). The R.H.S. of
(5) can be split into two parts T1 and T2 according to whether F happens or not.
p(h, t | R) = Ec,c0 、		exp (h>Rιc) exp (h>R2c0) -ZC	Z- IF	+ Ec,c0 J	X		exp (h>R1c) exp (h>R2c0) -Z	Zc- IF Σ	 	i
{z	sz =T1	=T2			
(20)
T1 can be approximated as given by (21).
Ti = 1 ±黑”Ec,c0 [exp (h>Rιc) exp (t>R2c0)]	(21)
Z2
On the other hand, T2 can be shown to be a constant, independent of d, given by (22).
∣T2∣ = exp(-Ω(log1∙8 n))	(22)
The vocabulary size n of real-world knowledge graphs is typically over 105, for which T2 becomes
negligibly small. Therefore, it suffices to consider only T1. Because of the slowness of the random
walk we have c ≈ c0
Using the law of total expectation we can write T1 as follows:
Ti = 1 ±O2(ez)Ec [exp (h>Rιc) Ec,∣c [exp (t>R2C0)]]
Z2
=1 ±ΘO(ez) Ec [exp (h>Rιc) A(c)]	(23)
where A(c) := Ec0|c exp t>R2c0 . Doing some further evaluations we show that
A(C) = (1 ± 62)exp (t>R2c)	(24)
Plugging (50) back in (23) provides the claim of the theorem.	□
The relationship given by (18) indicates that head and tail entity embeddings are first transformed
respectively by Ri> and R2>, and the squared `2 norm of the sum of the transformed vectors is
proportional to the probability p(h, t | R).
5
Under review as a conference paper at ICLR 2019
3 Learning Knowledge Graph Embeddings
In this section, we derive a training objective from Theorem 1 that we can then optimise to learn KGE.
The goal is to empirically validate the theoretical result by evaluating the learnt KGEs. Knowledge
graphs represent information about relations between two entities in the form of relational triples.
The joint probability p(h, R, t) given by Theorem 1 is useful for determining whether a relation R
exists between two given entities h and t. For example, if we know that with a high probability
that R holds between h and t, then we can append (h, R, t) to the knowledge graph. The task of
expanding knowledge graphs by predicting missing links between entities or relations is known as
the link prediction problem (Trouillon et al., 2016). In particular, if we can automatically append
such previously unknown knowledge to the knowledge graph, we can expand the knowledge graph
and address the knowledge acquisition bottleneck.
To derive a criteria for determining whether a link must be predicted among entities and relations, let
us consider a relational triple (h, R, t) ∈ D that exists in a given knowledge graph D. We call such
relational triples as positive triples because from the assumption it is known that R holds between h
and t. On the other hand, consider a negative relational triple (h0, R, t0) ∈ D formed by, for example,
randomly perturbing a positive triple. A popular technique for generating such (pseudo) negative
triples is to replace h or t with a randomly selected different instance of the same entity type. As an
alternative for random perturbation, Cai and Wang (2018) proposed a method for generating negative
instances using adversarial learning. Here, we are not concerned about the actual method used for
generating the negative triples but assume a set of negative triples, DD, generated using some method,
to be given.
Given a positive triple (h, R,t) ∈ D and a negative triple (h0, R, t0) ∈ DD, We would like to learn
KGEs such that a higher probability is assigned to (h, R, t) than that assigned to (h0, R, t0). We can
formalise this requirement using the likelihood ratio given by (25).
p(h, R,t) ≥
p(h0,R,t0) - η
(25)
Here, η > 1 is a threshold that determines how higher we would like to set the probabilities for the
positive triples compares to that of the negative triples.
By taking the logarithm of both sides in (25) we obtain
log p(h, R, t) - logp(h0, R, t0) ≥ logη
logη + logp(h0, R, t0) - log p(h, R,t) ≥ 0	(26)
If a positive triple (h, R, t) is correctly assigned a higher probability than a negative triplep(h0, R, t0),
then the left hand side of (26) will be negative, indicating that there is no loss incurred during this
classification task. Therefore, we can re-write (26) to obtain the marginal loss Bordes et al. (2013;
2011), L(D, D), a popular choice as a learning objective in prior work in KGE, as shown in (27).
L(D, D)=	X max (0, log η + logp(h0, R, t0) - log p(h, R, t))
(h,R,t)∈D
(h0,R,t0 )∈D
=max (0, 2d log η + ∣∣Rι>h0 + R2>t0k2 - ∣∣Rι>h + R2>t∣∣2)	(27)
We can assume 2d log η to be the margin for the constraint violation.
Theorem 1 requires R1 and R2 to be orthogonal. To reflect this requirement, we add two `2
regularisation terms ∣R1>R1 - I∣22 and ∣R2>R2 - I∣22 respectively with regularisation coefficients
λ1 and λ2 to the objective function given by (27). In our experiments, we compute the gradients
(27) w.r.t. each of the parameters h, t, R1 and R2 and use stochastic gradient descent (SGD) for
optimisation. This approach can be easily extended to learn from multiple negative triples as shown
in Appendix B.
4	Related Work
At a high-level of abstraction, KGE methods can be seen as differing in their design choices for the
following two main problems: (a) how to represent entities and relations, and (b) how to model the
6
Under review as a conference paper at ICLR 2019
interaction between two entities and a relation that holds between them. Next, we briefly discuss
prior proposals to those two problems (refer (Wang et al., 2017; Nickel et al., 2015; Nguyen, 2017)
for an extended survey on KGE).
A popular choice for representing entities is to use vectors, whereas relations have been represented by
vectors, matrices or tensors. For example, TransE (Bordes et al., 2011), TransH (Wang et al., 2014),
TransD (Ji et al., 2015), TransG (Xiao et al., 2016), TransR (Lin et al., 2015), lppTransD (Yoon et al.,
2016), DistMult (Yang et al., 2015), HolE (Nickel et al., 2016) and ComplEx (Trouillon et al., 2016)
represent relations by vectors, whereas Structured Embeddings (Bordes et al., 2011), TranSparse (Ji
et al., 2016), STransE (Nguyen et al., 2016), RESCAL (Nickel et al., 2011) use matrices and Neural
Tensor Network (NTN) (Socher et al., 2013) uses 3D tensors. ComplEx (Trouillon et al., 2016)
introduced complex vectors for KGEs to capture the asymmetry in semantic relations. (Ding et al.,
2018) obtained state-of-the-art performance for KGE by imposing non-negativity and entailment
constraints to ComplEx.
Given entity and relation embeddings, a scoring function is defined that evaluates the strength of
a relation R between two entities h and t in a triple (h, R, t). The scoring functions that encode
various intuitions have been proposed such as the `1 or `2 norms of the vector formed by a translation
of the head entity embedding by the relation embedding over the target embedding, or by first
performing a projection from the entity embedding space to the relation embedding space (Yoon
et al., 2016) As an alternative to using vector norms as scoring functions, DistMult and ComplEx use
the component-wise multi-linear dot product.
Once a scoring function is defined, KGEs are learnt that assign better scores to relational triples in
existing knowledge graphs (positive triples) over triples where the relation does not hold (negative
triples) by minimising a loss function such as the logistic loss (RESCAL, DistMult, ComplEx) or
marginal loss (TransE, TransH, TransD, TransD). Because knowledge graphs record only positive
triples, a popular method to generate pseudo negative triples is to perturb a positive instance by
replacing its head or tail entity by an entity selected uniformly at random from the vocabulary of
the entities. However, uniformly sampled negative triples are likely to be obvious examples that do
not provide much information to the learning process and can be detected by simply checking for
the type of the entities in a triple. Cai and Wang (2018) proposed an adversarial learning approach
where a generator assigns a probability to each relation triple and negative instances are sampled
according to this probability distribution to train a discriminator that discriminates between positive
and negative instances. (Xiao et al., 2016) proposed TransG, a generative model based on the Chinese
restaurant process, to model multiple relations that exist between a pair of entities. However, their
relation embeddings are designed to satisfy vector translation similar to TransE.
As an alternative to directly learning embeddings from a graph, several methods (Grover and Leskovec,
2016; Perozzi et al., 2014; Ristoski et al., 2018) have considered the vertices visited during truncated
random walks over the graph as pseudo sentences, and have applied popular word embedding learning
algorithms such as skip-gram with negative sampling or continuous bag-of-words model (Mikolov
et al., 2013) to learn vertex embeddings. However, pseudo sentences generated this way are syntacti-
cally very different from sentences in natural languages.
On the other hand, our work extends the random walk analysis by Arora et al. (2016a) that derives
a useful connection between the joint co-occurrence probability of two words and the `2 norm of
the sum of the corresponding word embeddings. Specifically, they proposed a latent variable model
where the words in a corpus are generated by a probabilistic model parametrised by a time-dependent
discourse vector that performs a random walk. However, unlike in our work, they do not consider the
relations between two co-occurring words in a corpus. Bollegala et al. (2018) extended the model
proposed by Arora et al. (2016a) to capture co-occurrences involving more than two words. They
defined the co-occurrence of k unique words in a given context as a k-way co-occurrence, where Arora
et al. (2016a)’s result could be seen as a special case corersponding to k = 2. Moreover, Bollegala
et al. (2018) showed that it is possible to learn word embeddings that capture some types of semantic
relations such as antonymy and collocation using 3-way co-occurrences more accurately than using
2-way co-occurrences. However, their model does not explicitly consider the relations between
words/entities and uses only a corpus for learning the word embeddings.
7
Under review as a conference paper at ICLR 2019
Table 2: Triple classification.
Accuracy
Method	WN11	FB13
SE	53.0	75.2
TransE	75.9	81.5
TransR	85.9	82.5
TransG	87.4	87.3
NTN	70.4	87.1
RelWalk	75.48	87.5
Table 3: Link prediction. Results marked with [?] are taken from Dettmers et al. (2017), [•] from
Nguyen et al. (2017), [/] from and Cai and Wang (2018). All other results for the baselines are taken
from their original papers.
Method	FB15K237					WN18RR				
	MRR	MR	H@1	H@3	H@10	MRR	MR	H@1	H@3	H@10
TransE∙	0.294	347	-	-	0.465	0.226	3384	-	-	0.50
TransD/	0.28	-	-	-	0.453	-	-	-	-	0.43
DistMult?	0.241	254	0.155	0.263	0.419	0.43	5110	0.39	0.44	0.49
ComplEx?	0.247	339	0.158	0.275	0.428	0.44	5261	0.41	0.46	0.51
ConvE	0.316	246	0.239	0.35	0.491	0.46	5277	0.39	0.43	0.48
RelWalk	0.329	105	0.243	0.354	0.502	0.451	3232	0.42	0.47	0.51
5	Empirical validation
To empirically evaluate the theoretical result stated in Theorem 1, we learn KGEs (denoted by
RelWalk) by minimising the marginal loss objective derived in section 3. We use the FB15k237,
FB13 (subsets of Freebase) and WN11, WN18RR (subsets of WordNet) datasets, which are standard
benchmarks for KGE. We use the standard training, validation and test splits as detailed in Table 4.
We generate negative triples by replacing a head or a tail entity in a positive triple by a randomly
selected different entity and learn KGEs. We train the model until convergence or at most 1000
epochs over the training data where each epoch is divided into 100 mini-batches. The best model
is selected by early stopping based on the performance of the learnt embeddings on the validation
set (evaluated after each 20 epochs). The training details and hyperparameter settings are detailed in
Appendix C. RelWalk is implemented in the open-source toolkit OpenKE (Han et al., 2018).2
We conduct two evaluation tasks: link prediction (predict the missing head or tail entity in a given
triple (h, R, ?) or (?, R, t)) (Bordes et al., 2011) and triple classification (predict whether a relation R
holds between h and t in a given triple (h, R, t)) (Socher et al., 2013). We evaluate the performance
in the link prediction task using mean reciprocal rank (MRR), mean rank (MR (the average of the
rank assigned to the original head or tail entity in a corrupted triple) and hits at ranks 1, 3 and 10
(H@1,3,10), whereas in the triple classification task we use accuracy (percentage of the correctly
classified test triples). We only report scores under the filtered setting Bordes et al. (2013), which
removes all triples appeared in training, validating and testing sets from candidate triples before
obtaining the rank of the ground truth triple. In link prediction, we consider all entities that appear in
the corresponding argument in the entire knowledge graph as candidates.
In Tables 2 and 3 we compare the KGEs learnt by RelWalk against prior work using the published
results. For link prediction, RelWalk reports SoTA on both WN18RR and FB15K237 in all evaluation
measures, except against ConvE in WN18RR measured by MRR. WN18RR excludes triples from
WN18 that are simply inverted between train and test partitions (Toutanova and Chen, 2015; Dettmers
et al., 2017). RelWalk’s consistently good performance on both versions of this dataset shows that it is
considering the global structure in the knowledge graph when learning KGEs. For triple classification,
RelWalk reports the best performance on FB13, whereas TransG reports the best performance on
2To facilitate the double blind policy, the source code for RelWalk will be released upon paper acceptance
8
Under review as a conference paper at ICLR 2019
WN11. Considering that both TransG and RelWalk are generative models, it would be interesting to
further investigate generative approaches for KGE in the future. Overall, the experimental results
support our theoretical claim and emphasise the importance of theoretically motivating the scoring
function design process.
6	Conclusion
We proposed RelWalk, a generative model of KGE and derived a theoretical relationship between
the probability of a triple and entity, relation embeddings. We then proposed a learning objective
based on the theoretical relationship we derived. Experimental results on a link prediction and a triple
classification tasks show that RelWalk obtains strong performances in multiple benchmark datasets.
References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable
model approach to pmi-based word embeddings. Transactions of Association for Computational
Linguistics, 4:385-399, 2016a.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: A latent
variable model approach to word embeddings. arXiv, 2016b.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabora-
tively created graph database for structuring human knowledge. In Proc. of SIGMOD, pages 1247 -
1250, 2008.
Danushka Bollegala, Yuichi Yoshida, and Ken-ichi Kawarabayashi. Using k-way Co-occurrences for
Learning Word Embeddings. In Proc. of AAAI, 2018.
Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embed-
dings of knowledge bases. In Proc. of AAAI, 2011.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durdn, Jason Weston, and Oksana Yakhenko.
Translating embeddings for modeling multi-relational data. In Proc. of NIPS, 2013.
Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings.
In Proc. of NAACL, pages 1470-1480, 2018.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D
Knowledge Graph Embeddings, 2017. URL http://arxiv.org/abs/1707.01476.
Boyang Ding, Quan Wang, Bin Wang, and Li Guo. Improving knowledge graph embedding using
simple constraints. In Proc. of ACL, pages 110-121, 2018.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proc. of
KDD, 2016.
Xu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. Openke: An
open toolkit for knowledge embedding. In Proc. of EMNLP, 2018.
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via
dynamic mapping matrix. In Proc. of ACL, pages 687-696, 2015.
Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Knowledge graph completion with adaptive sparse
transfer matrix. In Proc. of AAAI, pages 985-991, 2016.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation
embeddings for knowledge graph completion. In Proc. of AAAI, pages 2181-2187, 2015.
Tomas Mikolov, Kai Chen, and Jeffrey Dean. Efficient estimation of word representation in vector
space. In Proc. of International Conference on Learning Representations, 2013.
9
Under review as a conference paper at ICLR 2019
Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. A novel embedding
model for knowledge base completion based on convolutional neural network. arXiv preprint
arXiv:1712.02121, 2017.
Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowledge
base completion. 03 2017. URL https://arxiv.org/abs/1703.08098.
Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson. Stranse: a novel embedding model of
entities and relationships in knowledge bases. In Proc. ofNAACL-HLT,, pages 460-466, 2016.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning
on multi-relational data. In Proc. of ICML, pages 809-816, 2011.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33, 2015.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge
graphs. In Proc. of AAAI, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, pages 701-710, New York, NY, USA, 2014. ACM. ISBN
978-1-4503-2956-9. doi: 10.1145/2623330.2623732. URL http://doi.acm.org/10.1145/
2623330.2623732.
Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko Paulheim. Rdf2vec:
Rdf graph embeddings and their applications. Semantic Web, (Preprint):1-32, 2018.
Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural
tensor networks for knowledge base completion. In Proc. of NIPS, 2013.
Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text
inference. In Proc. of 3rd Workshop on Continuous Vector Space Models and their Compositionality,
pages 57-66, 2015.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex
embeddings for simple link prediction. In Proc. of ICML, 2016. URL http://arxiv.org/abs/
1606.06357.
Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge graph embedding: A survey of approaches and
applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724-2743, Dec
2017. ISSN 1041-4347. doi: 10.1109/TKDE.2017.2754499.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In Proc. of AAAI, pages 1112 - 1119, 2014.
Han Xiao, Minlie Huang, and Xiaoyan Zhu. Transg : A generative model for knowledge graph
embedding. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 2316-2325, Berlin, Germany, August 2016. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/P16-1219.
Bishan Yang, Wen-tau Yih, Xiadong He, Jianfeng Gao, and Li Deng. Embedding entities and relations
for learning and inference in knowledge bases. In ICLR, 2015.
Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, and Se-Young Park. A translation-based knowledge
graph embedding preserving logical property of relations. In Proc. of NAACL, pages 907-916,
2016.
10
Under review as a conference paper at ICLR 2019
Appendix
A Proof of Theorem 1
Let us consider the probabilistic event that (1 - z)Z ≤ Zc ≤ (1 + z)Z to be Fc and (1 - z)Z ≤
Zc0 ≤ (1 + z)Z to be Fc0. From Lemma 1 we have Prc [Fc] ≥ 1 - δ. Then from the union bound
we have,
Pr[Fc ∨ Pc，] ≤ Pr[Fζ] + Pr[Fζo ]
=1-Pr[Fc]+1-Pr[Fc0]
= 2δ.	(28)
Moreover, let F be the probabilistic event that both Fc and Fc0 being True. Then from Pr[F ] =
1 - Pr[Fc ∨ Fco] We have, Pr[F] ≥ 1 - 2δ. We can decompose the expectation in the R.H.S. in (5)
into two terms T1 and T2 depending on whether respectively F is True or False as follows:
exp h>R1c exp (h>R2c0)	-I	Γexp h>R1c exp (h>R2c0)	"I
p(h,t | r) = EC,C0	y	1	1F + Ec,c0	1	1F
Zc	Zc0	Zc	Zc0
、------------------{z-------------------} 、------------------V------------------}
=T1	=T2
(29)
Here, 1F and 1户 are indicator functions given by:
and
01
if F is True,
otherWise,
if F is True,
otherWise.
(30)
(31)
Let us first shoW that T2 is negligibly small.
For tWo real integrable functions ψ1 (x) and ψ2 (x) in [a, b], the Cauchy-SchWarz’s inequality states
that
ψ1 (x)ψ2 (x)dx
a
≤ Z b[ψ1(x)]2dxZ b[ψ2(x)]2dx.
aa
(32)
Applying (32) to T2 in (29) We have:
(Ec,c' Z Z , exp (h>R1c) exp (t>R2c0) 1f])
≤ (Ede，exp (h>R1 c)2 1f )(
=(EC [7^2 exp (h>R1c) Ec0IC [1F]
Ec,c0 ]z2 exp (t>R2c0) 1F )
)(Ec0 ]z_ exp (t>R2c0) Ec∣c0 [1F]
(33)
Note that Zc ≥ 1 because Zc is the sum of positive numbers and if h>R1c ≥ 0 for at least one
of the h ∈ V , then the total sum Will be greater than 1. Therefore, by dropping Zc term from the
denominator We can further increase the first term in (33) as given by (34).
Ec]Z2 exp (h>Rιc)2 Ec0∣c [1f] ≤ ECheXP (h>Rιc)2 Ec，|c [1f]]	(34)
Let us split the expectation on the R.H.S. of (34) into tWo cases depending on Whether h>R1c > 0
or otherWise, indicated respectively by 1(h>R1c>0) and 1(h>R1c≤0).
ECheXP (h>Rιc)2 Ec0∣c [1f]]
=Ec [exp (h>Rιc)21(h>Ric>o)Ecθ∣c [1f]] + Ec [exp (h>Rιc)21(h>Ric≤o)Ec0∣c [1f]] (35)
11
Under review as a conference paper at ICLR 2019
The second term of (35) is upper bounded by
Ec,c0 [1f] ≤ exp (-Ω(log2 n))
(36)
The first term of (35) can be bounded as follows:
吼卜XP (h>Rιc)2 1(h>Ric>o)Ecθ∣c [1f]] ≤ 吼卜xp(αh>Rιc)21(h>Rι.。库,^ [1百]]
≤ Ec hexp(αh>R1c)2Ec0∣c [1f]]	(37)
where α > 1. Therefore, it is sufficient to bound EC kxp(αh>R1c)2Eco∣c [1户]]when ∣∣h∣∣ =
Ω(√d).
Let us denote by Z the random variable 2h>Rιc. Moreover, let r(z) = Ec0∣z [1f], which is a function
of z between [0, 1]. We wish to upper bound Ec[exp(z)r(z)]. The worst-case r(z) can be quantified
using a continuous version of Abel’s inequality (proved as Lemma A.4 in Arora et al. (2016b)), we
can upper bound Ec [exp(z)r(z)] as follows:
Ec [exp(z)r(z)] ≤ E exp(z)1[t,+∞] (z)	(38)
where t satisfies that EJ1[t,+∞](z)] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(-Ω(log2 n)). Here, 1[t,+∞](z)
is a function that takes the value 1 when z ≥ t and zero elsewhere. Then, we claim Prc [z ≥ t] ≤
exp(-Ω(log2 n)) implies that t ≥ Ω(log.9 n).
If C was distributed as N(0, dI), this would be a simple tail bound. However, as C is distributed
uniformly on the sphere, this requires special care, and the claim follows by applying the tail bound
for the spherical distribution given by Lemma A.1 in (Arora et al., 2016a) instead. Finally, applying
Corollary A.3 in (Arora et al., 2016a), we have:
E[exp(z)r(z)] ≤ E[exp(z)1[t,+∞](z)] = exp(-Ω(log1∙8 n))	(39)
From a similar argument as above we can obtain the same bound for C0 as well. Therefore, T2 in (29)
can be upper bounded as follows:
Ec,c0
7 ɪ exp (h>Rιc) exp (t>R2c0) 1f
Zc Zc0
12
Zl exp (h>R1c) Ec0∣c [1f]
Zr exp (t>R∣c0)2 眺d [1f]
1/2
≤ exp(-Ω(log1.8 n))
(40)
Because n = |V|, the size of the entity vocabulary, is large (ca. n > 105) in most knowledge graphs,
we can ignore the T2 term in (29). Combining this with (29) we obtain an upper bound for p(h, t | R)
given by (41).
p(h, t | R) ≤ (1 + z)2
= (1 + z)2
22
1 Zlz
Ec,c0 [exp (h>Rιc) exp (t>R2c0) 1f] + |D| exp(-Ω(log .8 n))
Ec,c0 [exp (h>R1c) exp (t>R2c0)] + δo	(41)
where |D| is the number of relational tuples (h, R, t) in the KB D and δo = |D| exp(-Ω(log1.8 n)) ≤
exp(-Ω(log1.8 n)) by the fact that Z ≤ exp(2κ)n = O(n), where K is the upper bound on h>R1c
and t>R2c0, which is regarded as a constant.
On the other hand, we can lower bound p(h, t | R) as given by (42).
p(h, t | R)
222
1 - zlZl-Z
222
)))
zzz
---
(1 (1 (1
≥≥≥
Ec,c0 exp h>R1c exp t>R2c0 1F
Ec,c0 [exp (h>Rιc) exp (t>R2c0)] — |D| exp(-Ω(log1.8 n))
Ec,c0 [exp (h>Rιc) exp (t>R2c0)] — δo	(42)
12
Under review as a conference paper at ICLR 2019
Taking the logarithm ofboth sides, from (41) and (42), the multiplicative error translates to an additive
error given by (43).
logp(h,t | R) = log (Ec,c∕ [exp (hτR1c) exp (tτR2C0)] ± 3-2log Z + 2log(1 ± 品)
=log (Ec	[exp	(hτR1c)	E”	[exp (tτR2c0)]] ± δ0) - 2log Z	+ 2log(1 ± 品)
=log (Ec	[exp	(hτR1c)	A(c)	± δ0]) - 2log Z + 2log(1 ± 品)	(43)
where A(c) := Ec∕∣c [exp (tτR2c0)].
We assumed that c and c0 are on the unit sphere and R1 and R2 to be orthogonal matrices. Therefore,
Ric and R2c0 are also on the unit sphere. Moreover, if we let the upper bound of the '2 norm of the
entity embeddings to be K√d, then we have ∣∣h∣∣ ≤ KVd and ∣∣t∣∣ ≤ K√d. Therefore, we have
{R1 h, c0 - ci≤ MllIlC - Cll ≤ κ0√d∣∣c - Cll	(44)
Then we can lower bound A(c) as follows:
A(c) = exp (tτR2c) Ec∕∣c [exp (tτRz(cz — c))]
≤ exp (tτR2c) Ec[c [exp(EVdkc -c∣∣)]
≤ (1 + e2)exp (tτ R2c)
For some ⑦ > 0. The last inequality holds because
Ecd kXP ^κz√d∣cz — cl)] = J exp (κz√d∣∣cz - Ck) P(C0∣c)dc0
=exp(κ0
X-----
^{z
≥1
X.
exp(∣c — ClI)P(ClC)dc0
_ - /
{z
≥1
1 + ∈2
(45)
(46)
To obtain a lower bound on A(c) from the first-order Taylor approximation of exp(x) ≥ 1 + x we
observe that
Ecd kXP (K0√d∣c0 - c∣)] + Ecd 卜XP (-κ 0√d∣c0 - c∣∣)] ≥ 2.	(47)
Therefore, from our model assumptions we have
Ec∣cjexp (-κ0√dkc0 - c∣)] ≥ 1 -⑦	(48)
Hence,
A(c) = exp (tτR2c) Eco∣c [exp (tτR2(c0 — c))]
≥ exp (tτR2c) Ec0∣c [exp (-κz√d∣∣cz — c∣∣)]
≥ (1 - e2)exp (tτR2c)	(49)
Therefore, from (46) and (49) we have
A(c) = (1 ± s)exp (tτR2c)	(50)
Plugging A(c) back in (43) we obtain
logp(h,t ∣ R) = log (Ec [exp (hτR1c) A(c) ± δ°]) - 2logZ + 2log(1 ± 品)
=log (Ec [exp (hτR1c) (1 ± 0)exp (tτR2c) ± δ°]) - 2log Z + 2log(1 ± Cz)
(51)
=log (Ec [exp (hτR1c) exp (tτR2c) ± δ°]) - 2logZ + 2log(1 ± Cz) + log(1 ± ⑹
=log (Ec [exp (hτR1c + tτR2c) ± δ°]) - 2logZ + 2log(1 ± Cz) + log(1 ± C2)
=log (Ec [exp (RITh + R2Tt)TC ± δ°]) - 2logZ + 2log(1 ± Cz) + log(1 ± C2)
13
Under review as a conference paper at ICLR 2019
Note that c has a uniform distribution over the unit sphere. In this case, from Lemma A.5 in (Arora
et al., 2016b), (52) holds approximately.
EclexP (Rι>h + R2>t) >c] 二(1 ± €3)exp
kR1>h + R2>tk2
2d
(52)
where €3 = O(1/d). Plugging (52) in (51) we have that
logp(h,t | R)= kR1>h +R2>tk2 + O(€z) + O(€2) + O(€3)+ O(δ0) - 2logZ (53)
where δ0 = δo ∙ (Ec [exp ((Rjh + R2>t)>c)]) 1 = exp(-Ω(log1∙8 n)). Therefore, δ0 can be
ignored. Note that €3 = O(1∕d) and Cz = O(1∕√n) by assumption. Therefore, we obtain that
logp(h,t | R = kR1>h +dR2"k2 + O(€z) + O(C2) + O(1∕d) - 2logZ (54)
□
B Learning with multiple negative triples
In this section, we show how the margin loss-based learning objective derived in section 3 can be
extended to learn from more than one negative triples per each positive triple. This formulation
leads to rank-based loss objective used in prior work on KGE. Considering that negative triples are
generated via random perturbation, it is important to consider multiple negative triples during training
to better estimate the classification boundary.
Let us consider that we are given a positive triple, (h, R, t) and a set of K negative triples
{(h0k, R, t0k)}kK=1. We would like our model to assign a probability, p(h, t | R), to the positive
triple that is higher than that assigned to any of the negative triples. This requirement can be written
as (55).
p(h, t|R) ≥ k=m1,a..x.,K p(h0k, t0k | R)	(55)
We could further require the ratio between the probability of the positive triple and maximum
probability over all negative triples to be greater than a threshold η ≥ 1 to make the requirement of
(55) to be tighter.
p(h,t |R)	≥n
k=m,aXκ PM,tk | r) - η
(56)
By taking the logarithm of (56) we obtain
log p(h, t | R) - log k=m1,a..x.,Kp(h0k,t0k | R) ≥ log(η)	(57)
Therefore, we can define the margin loss for a misclassification as follows:
L (h, R, t), {(h0k, R,t0k)}kK=1 = max 0, log k=m1,a..x.,Kp(h0k, t0k | R) + log(η) - log p(h, t | R)	(58)
However, from the monotonicity of the logarithm we have ∀x1, x2 > 0, if log(x1) ≥ log(x2)
then x1 ≥ x2. Therefore, the logarithm of the maximum can be replaced by the maximum of the
logarithms in (58) as shown in (59).
L ((h,R,t),	{(hk ,R,tk)}K=J = max 仅,fc=ma.Xκ	log (p(hk ,tk	[	R))	+ log(η)	- log p(h,t |	R))	(59)
By substituting (18) for the probabilities in (59) we obtain the rank-based loss given by (60).
L (h, R, t), {(h0k, R, t0k)}kK=1) = max 0, 2d log(η) + k=m1,a..x.,K kR1>h0k + R2>t0kk22 - kR1>h + R2>tk22)
(60)
In practice, we can use p(h0k, t0k | R) to select the negative triple with the highest probability for
training with the positive triple.
14
Under review as a conference paper at ICLR 2019
Table 4: StatiSticS of the datasets
Dataset	Relations	Entities	Train	Test	Validation
FB15K	1,345	14,951	483,142	59,071	50,000
FB15K237	237	14,541	272,115	17,535	20,466
WN18	18	40,943	141,442	5,000	5,000
WN18RR	11	40,943	86,835	3,134	3,034
WN11	11	38,588	112,581	10,544	2,609
FB13	13	75,043	316,232	23,733	5,908
C Training details
The statistics of the benchmark datasets are show in Table 4.
We selected the initial learning rate (α) for SGD in {0.01, 0.001}, the regularisation coefficients
(λ1, λ2) for the orthogonality constraints of relation matrices in {0, 1, 10, 100}. The number of
randomly generated negative triples nneg for each positive example is varied in {1, 10, 20, 50, 100}
and d ∈ {50, 100}. Optimal hyperparameter settings were: λ1 = λ2 = 10, nneg = 100 for all the
datasets, α = 0.001 for FB15K, FB15K237 and FB13, α = 0.01 for WN18, WN18RR and WN11.
For FB15K237 and WN18RR d = 100 was the best, whereas for all other datasets d = 50 performed
best.
15