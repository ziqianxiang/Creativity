Under review as a conference paper at ICLR 2019
LIPSCHITZ REGULARIZED DEEP NEURAL NETWORKS
GENERALIZE
Anonymous authors
Paper under double-blind review
AB STRACT
We show that if the usual training loss is augmented by a Lipschitz regularization
term, then the networks generalize. We prove generalization by first establishing
a stronger convergence result, along with a rate of convergence. A second result
resolves a question posed in Zhang et al. (2016): how can a model distinguish
between the case of clean labels, and randomized labels? Our answer is that Lip-
schitz regularization using the Lipschitz constant of the clean data makes this dis-
tinction. In this case, the model learns a different function which we hypothesize
correctly fails to learn the dirty labels.
1	INTRODUCTION
While deep neural networks networks (DNNs) give more accurate predictions than other machine
learning methods (LeCun et al., 2015), they lack some of the performance guarantees of these other
methods. One step towards performance guarantees for DNNs is a proof of generalization with a
rate. In this paper, we present such a result, for Lipschitz regularized DNNs. In fact, we prove a
stronger convergence result from which generalization follows.
We also consider the following problem, inspired by (Zhang et al., 2016).
Problem 1.1. [Learning from dirty data] Suppose we are given a labelled data set, which has Lips-
chitz constant Lip(D) = O(1) (see (3) below). Consider making copies of 10 percent of the data,
adding a vector of norm E to the perturbed data points, and changing the label of the perturbed points.
Call the new, dirty, data set D. The dirty data has Lip(D) = O(1∕e). However, if We compute the
histogram of the pairwise Lipschitz constants, the distribution of the values on the right hand side of
(3), are mostly below Lip(D) with a small fraction of the values being O(1∕e), since the duplicated
images are E apart but with different labels. Thus we can solve (1) with L° estimate using the preva-
lent smaller values, which is an accurate estimate of the clean data Lipschitz constant. The solution
of (1) using such a value is illustrated on the right of Figure 1. Compare to the Tychonoff regularized
solution on the right of Figure 2. We hypothesis that on dirty data the solution of (1) replaces the
thin tall spikes with short fat spikes leading to better approximation of the original clean data.
In Figure 1 we illustrate the solution of (1) (with L0 = 0), using synthetic one dimensional data.
In this case, the labels {-1, 0, 1} are embedded naturally into Y = R, and λ = 0.1. Notice that
the solution matches the labels exactly on a subset of the data. In the second part of the figure, we
show a solution with dirty labels which introduce a large Lipschitz constant, in this case, the solution
reduces the Lipschitz constant, thereby correcting the errors.
Learning from dirty labels is studied in §2.4. We show that the model learns a different function
than the dirty label function. We conjecture, based on synthetic examples, that it learns a better
approximation to the clean labels.
We begin by establishing notation. Consider the classification problem to fix ideas, although our
restuls apply to other problems as well.
Definition 1.2. Let Dn = x1, . . . , xn be a sequence of i.i.d. random variables sampled from the
probability distribution ρ. The data xi are in X = [0, 1]d. Consider the classification problem
with D labels, and represent the labels by vertices of the probability simplex, Y ⊂ RD . Write
yi = u0 (xi ) for the map from data to labels.
1
Under review as a conference paper at ICLR 2019
Write u(x; w) for the map from the input to data to the last layer of the network.1 Augment the
training loss with Lipschitz regularization
min
u:X →Y
Jn[u]
1n
—i(U 1(U(xi； w),yi) + λmax(Lip(u) — L°, 0)
n i=1
(1)
The first term in (1) is the usual average training loss. The second term in (1) the Lipschitz regular-
ization term: the excess Lipschitz constant of the map u, compared to the constant L0.
In order to apply the generalization theorem, we need to take L0 ≥ Lip(u0), the Lipschitz constant
of the data on the whole data manifold. In practice, Lip(u0) can be estimated by the Lipschitz
constant of the empirical data. The definition of the Lipschitz constants for functions and data, as
well as the implementation details are presented in §1.3 below.
Figure 1: Synthetic labelled data and Lipschitz regularized solution u. Left: The solution value
matches the labels exactly on a large portion of the data set. Right: dirtly labels: 10% of the data is
incorrect; the regularized solution corrects the errors.
Our analysis will apply to the problem (1) which is convex inu, and does not depend explicitly
on the weights, w. Of course, once u	is restricted to a fixed neural network architecture, the cor-
responding minimization problem becomes non-convex in the weights. Our analysis can avoid the
dependence on the weights because we make the assumption that there are enough parameters so
thatu can exactly fit the training data. The assumption is justified by Zhang et al. (2016). As we
send n → ∞ for convergence, we require that the network also grow, in order to continue to satisfy
this assumption. Our results apply to other non-parametric methods in this regime.
1.1	RELATED WORK AND APPLICATIONS
Generalization bounds have been obtained previously via VC dimension analysis of neural net-
works (Bartlett, 1997). The generalization rates have factors of the form Ak for a k-layer neural net-
work with bounds IIwWl ≤ A for all weight vectors Wi in the network. Such bounds are only applica-
ble for low-complexity networks. Other works have considered connections between generalization
and stability (Bousquet & Elisseeff, 2002; Xu & Mannor, 2012). More recently, (Bartlett et al.,
2017) proposed the Lipschitz constant of the network as a candidate measure for the Rademacher
complexity, which is a measure of generalization (Shalev-Shwartz & Ben-David, 2014, Chapter 26).
Also, Cranko et al. (2018) showed that Lipschitz regularization can be viewed as a special case of
distributional robustness. Unlike other recent contributions such as (Hardt et al., 2015), our analysis
does not depend on the training method. In fact, our analysis has more in common with inverse
problems in image processing, such as Total Variation denoising and inpainting (Bertalmio et al.,
2000; Rudin et al., 1992). For further discussion, see Appendix C.
1We apologize for not using the standard notation f for the last layer!
2
Under review as a conference paper at ICLR 2019
The estimate of Lip(u; X ) provided by (4) can be quite different from the the Tychonoff gradient
regularization (Drucker & Le Cun, 1992),
l1l E Nxu(Xi)F
|I| i∈I
since (4) corresponds to a maximum of the values of the norms, and the previous equation corre-
sponds to the mean-squared values. In fact, recent work on semi-supervised learning suggests that
higher p-norms of the gradient are needed for generalization when the data manifold is not well ap-
proximated by the data (El Alaoui et al., 2016; Calder, 2017; Kyng et al., 2015; Slepcev & Thorpe,
2017). In Figure 2 we compare to the problems in Figure 1 using Tychonoff regularization. The Ty-
chonoff regularization is less effective at correcting errors. The effect is more pronounced in higher
dimensions.
Figure 2: Synthetic labelled data and Tychonoff regularized solution u. Left: The solution value
matches the labels exactly on a large portion of the data set. Right dirty labels: 10% of the data
is incorrect; the regularized solution is not as effective at correcting errors. The effect is more
pronounced in higher dimensions.
1.2	RELATED WORK ON LIPSCHITZ REGULARIZATION
An upper bound for the Lipschitz constant of the model is given by the norm of the product of the
weight matrices (Szegedy et al., 2013, Section 4.3). Let w = (w1, . . . , wJ) be the weight matrices
for each layer. Then
Lip(u; X) ≤ ∏J=ι∣∣wi∣∣.	(2)
Regularization of the network using methods based on (2) has been implemented recently in (Gouk
et al., 2018) and (Yoshida & Miyato, 2017). Because the upper bound in (2) does not take into
account the coefficients in weight matrices which are zero due to the activation functions, the gap
in the inequality can be off by factors of many orders of magnitude for deep networks (Finlay &
Oberman, 2018).
Implementing (4) can be accomplished using backpropagation in the x variable on each label, which
can become costly for D large. Special architectures could also be used to implement Lipschitz
regularization, for example, on a restricted architecture, Liao et al. (2018) renormalized the weight
matrices of each layer to be norm 1.
Lipschitz regularization may help with adversarial examples (Szegedy et al., 2013) (Goodfellow
et al., 2014) which poses a problem for model reliability (Goodfellow et al., 2018). Since the Lips-
Chitz constant Li of the loss, I, controls the norm of a perturbation
∣∣l(u(χi + Ev)) - l(u(Xi))∣∣γ ≤ ELiIlvllX
3
Under review as a conference paper at ICLR 2019
maps with smaller Lipschitz constants may be more robust to adversarial examples. Finlay & Ober-
man (2018) implemented Lipschitz regularization of the loss, and achieved better robustness against
adversarial examples, compared to adversarial training (Goodfellow et al., 2014) alone.
Lipschitz regularization may also improve stability of GANs. 1-Lipschitz networks are also impor-
tant for Wasserstein-GANs (Arjovsky et al., 2017) (Arjovsky & Bottou, 2017). In (Wei et al., 2018)
the gradient penalty away from norm 1 is implemented, augmented by a penalty around perturbed
points, with the goal of improved stability. Spectral regularization for GANs was implemented in
(Miyato et al., 2018).
1.3	LIPSCHITZ CONSTANTS AND IMPLEMENTATION
Definition 1.3 (Lipschitz constants of functions and data). Choose norms ∣∣ ∙ ∣∣γ, and ∣∣ ∙ ∣∣χ on X
and Y , respectively. The Lipschitz constant (in these norms) of a function u : X0 ⊂ X → Y is
given by
Lip(u; X0)
sup
x1 ,x2 ∈X0
Ilu(XI) - U(X2)Il Y
∣∣χι - χ2∣IX
When X0 is all of X, we write Lip(u; X) = Lip(u). The Lipschitz constant of the data is given by
Lip(u0 ; Dn) = max
x1,x2 ∈Dn
IIu0(x1) - u0(x2)∣∣γ
∣∣χχ - ∙X2∣IX
(3)
Finlay & Oberman (2018) implement Lipschitz regularization as follows. The basis for the imple-
mentation of the Lipschitz constant is Rademacher’s Theorem (Evans, 2018, §3.1), which states
that if a function g(x) is Lipschitz continuous then it is differentiable almost everywhere and
Lip(g) = ImaXxlIVg(X)II.
Restricting to a mini-batch, we obtain the following method for estimating the Lipschitz constant.
Let u(X; w ) be a Lipschitz continuous function. Then
max IlVxuE； W)II ≤ Lip(u; X)	(4)
i∈I
For vector valued functions, the appropriate matrix norm must be used, see §B.
2 LIPSCHITZ REGULARIZATION AND CONVERGENCE
2.1	LIMITING PROBLEM
The variational problem (1) admits Lipschitz continuous minimizers, but in general the minimizers
are not unique. When L0 = Lip(u0), it is clear that u0, is a solution of (1): both the loss term
and the regularization term are zero when applied to u0 . In addition, any L0-Lipschitz extension of
u0|Dn is also a minimizer of (1), so solutions are not unique.
Let un be any solution of the Lipschitz regularized variational problem (1). We study the limit of
un as n → ∞. Since the empirical probability measures ρn converge to the data distribution ρ, the
continuum variational problem corresponding to (1) is
min
u:X →Y
J [u] ≡ L[u; ρ] + λ max(Lip(u) - L0 , 0),
(5)
where in (5) we have introduced the following notation.
Definition 2.1. Given the loss function, I, a map u : X → Y, and a probability measure, μ,
supported on X, define
L[u,μ] = Ex
〜μ[l(u(x),u0(x))]=
X
I (u (x), uo (x ))dμ (x)
to be the expectation of the loss with respect to the measure. In particular, the generalization loss of
the map u : X → Y is given by L[u, ρ]. Write L[u, Dn] := L[u, ρn] for the average loss on the data
set Dn, where Pn := 1 E δxi is the empirical measure corresponding to Dn.
Remark 2.2. Generalization is defined in (Goodfellow et al., 2016, Section 5.2) as the expected
value of the loss function on a new input sampled from the data distribution. As defined, the full
generalization error includes the training data, but it is of measure zero, so removing it does not
change the value.
4
Under review as a conference paper at ICLR 2019
2.2	LOSS FUNCTION ASSUMPTIONS
We introduce the following assumption on the loss function.
Assumption 2.3 (Loss function). The function I : Y × Y → R is a loss function if it satisfies (i)
I ≥ 0, (ii) I(y1,y2) = 0 if and only if yι = y2, and (iii) I is strictly convex in y>
Example 2.4 (RD with L loss). Set Y = RD, and let each label be a basis vector. Set I(y1,y2)=
∣∣yι — y2112 to be the L2 loss.
Example 2.5 (Classification). In classification, the output of the network is a probability vector on
the labels. Thus Y = ∆D, the D-dimensional probability simplex, and each label is mapped to a
basis vector. The cross-entropy loss IKL (y, Z) = - ED=I Zi log(yi∕zi). For labels, IKL (y, ek)=
— log(yk ).
Example 2.6 (Regularized cross-entropy). In the classification setting, it is often the case that the
softmax function
ezj
softmax(z)j = -ŋ--------	(6)
kD=1 ezk
is combined with the cross-entropy loss. In this paper, we regard softmax as the last layer of the
DNN, so we assume the output u(x) of the network lies in the probability simplex. If the output, Z,
of the second to last layer of the DNN, which is the input to softmax in (6), lies in a compact set,
i.e., |Zj | ≤ C for all i and some C > 0, then softmax(Z)j ≥ e-2C, and so the range of softmax lies
in the set
A ：= {y ∈	RD	:	yi	≥	e-2C	and yι +---+ yD	=	1},
which is strictly interior to the probability simplex. Restricted to A, the cross-entropy loss IKL is
strongly convex and Lipschitz continuous, which is required in Theorems 2.12 and 2.11 below.
In our analysis, it is slightly more convenient to define the regularized cross entropy loss with pa-
rameter E > 0
lKL(y,z) = - E (Zi + E)Iog (yi+^).
i=1	∖zi+ J
For classification problems, where Z = ek, we have IKL(y, ek) = -(1 + E) log((yk + e)∕(1 + e)),
which is Lipschitz and strongly convex for any 0 ≤ yi ≤ 1 within the probability simplex. Thus,
the regularized cross entropy IKL satisfies the strong convexity and Lipschitz regularity required by
Theorems 2.12 and 2.11 on the whole probability simplex.
2.3	GENERALIZATION RESULT
Here, we show that solutions of the random variational problem (1) converge to solutions of (5).
We make the standard manifold assumption (Chapelle et al., 2006), and assume the data distribution
ρ is a probability density supported on a compact, smooth, m-dimensional manifold M embedded
in X = [0, 1]d, where m ≪ d. We denote the probability density again by ρ : M → [0, ∞).
Hence, the data Dn is a sequence x1 , . . . , xn of i.i.d. random variables on M with probability
density ρ. Associated with the random sample we have the closet point projection map σn : X →
{x1 , . . . , xn } ⊂ X that satisfies
∣∣x — σn(x)∣∣χ = min ∕x — XilIX}
1≤i≤n
for all x ∈ X. We recall that W 1,∞ (X; Y) is the space of Lipschitz mappings from X to Y.
Throughout this section, C, c > 0 denote positive constants depending only on M, and we assume
C ≥ 1 and 0 < c < 1. We follow the analysis tradition of allowing the particular values of C and c
to change from line to line.
We establish that that minimizers of (5) are unique on M in Theorem A.1, which follows from the
strict convexity of the loss restricted to the data manifold M. See also Figure 3 which shows how
the solutions need not be unique off the data manifold.
Our first result is in the case where Lip[u0] ≤ L0, and so the Lipschitz regularizer is not fully active.
This corresponds to the case of clean labels. We state our result in generality, for approximate
minimizers of (1), and specialize to the case Lip[u0] ≤ L0 in Remark 2.8.
5
Under review as a conference paper at ICLR 2019
Theorem 2.7 (Convergence result). Assume inf x∈M ρ(x) > 0. For any t > 0, with probability at
least 1 - C t-1 n-(ct-1) every sequence un ∈ W 1,∞ (X; Y ) with zero empirical loss L[u0, ρn] = 0
satisfies
Ilu0 - UnllL∞(M; Y) ≤ C(L0 + LipIUnD
t log(n)
n
1/m
Remark 2.8. If Un ∈ W1,∞(X; Y ) is any sequence of minimizers of (1) and LipIU0] ≤ L0, then
J IUn] ≤ J IU0] = 0. Thus, LipIUn] ≤ L0 and Theorem 2.7 applies to the sequence Un, yielding
Ilu0 - UnllL∞(M;Y) ≤ CL0
t log(n)
n
1/m
(
)
(
)
It is important to note that Theorem 2.7 does not requires un to be minimizers of (1)—we just
require zero empirical loss, which is often achieved in practice (Zhang et al., 2016). This allows for
approximation errors in solving (1) on the whole domain X , due to the restriction that u must be
expressed via a Deep Neural Network.
As an immediate corollary, we can prove that the generalization loss converges to zero, and so we
obtain generalization.
Corollary 2.9. Assume that for some q ≥ 1 the loss I satisfies
i(y,yo) ≤ CIIy - y0IIY for ail yo,y ∈ Y.
(7)
Then under the assumptions of Theorem 2.7
LIun,ρ] ≤ C(L0 + LipIun])q
holds with probability at least 1 - C t-1 n-(ct-1).
(
t log(n)
n
q/m
)
Proof. By (7), we can bound the generalization loss as follows
L[un, p] = I l(un(x),u0 (X))P(X) dVθl(x) ≤ Cllun - u0∣∣L∞(M∙γ )∙
M;
The proof is completed by invoking Theorem 2.7.
□
We now turn to the proof of Theorem 2.7, which requires a bound on the distance between the
closest point projection σn and the identity. The result is standard in probability, and we include it
for completeness in Lemma 2.10 proved in §A.1. We refer the interested reader to (Penrose et al.,
2003) for more details.
Lemma 2.10. Suppose that infM P > 0. Then for any t > 0
(t log(n) Y/m
IlId - σn∣∣L∞(M'X) ≤ C I ---n--- I
with probability at least 1 - C t-1 n-(ct-1).
We now give the proof of Theorem 2.7.
Proof of Theorem 2.7. Since LIun , Pn] = 0 we have u0 (Xi) = un (Xi ) for all 1 ≤ i ≤ n. Thus for
any X ∈ X we have
Il u0 (x) — un (x) ∣∣Y = Il u0 (x) — u0 (σn (x)) + u0 (。η(x)) — un (。九(x) ) + un (。九(x)) — un (x) ∣∣ Y
≤ ∣∣uθ(x) — u0 (σn(x))∣∣γ + ∣∣un(σn(x)) — un(X)IIY
≤ (Lo + Lip[un])^x — σn(x)∣∣χ.
Therefore, we deduce
Iluo - un∣∣L∞(M;Y) ≤ (L0 + Lip[un])∣∣Id — σn∣∣L∞(M=X)∙
The proof is completed by invoking Lemma 2.10.	口
6
Under review as a conference paper at ICLR 2019
Figure 3: On the data manifold there is only one minimizer. Off the data manifold, there can be
multiple minimizers.
2.4	CONVERGENCE FOR DIRTY LABELS
We now consider the setting of Problem 1.1, illustrated in Figure 1 right. We assume that we only
have access to a “dirty” label function, which corresponds to an additive error of the form
u0 = uclean + ue
where uclean is the label function, and ue : X → Y is some error function, which is assumed to be
zero with high probability. Assume that the error vector e has a much larger Lipschitz constant than
the labels, so that Lip(u0) ≫ Lip(uclean).
We wish to fit the clean labels, while not fitting the errors, having access only to u0 . The labels
correspond to the subset of the data which generate the low Lipschitz constant Lclean , while the
errors correspond to pairs of labels that generate a high Lipschitz constant. Thus Lclean can easily
be estimated from the distribution of the pairwise Lipschitz constants of the data. With the goal in
mind, we set L0 = Lclean in (1). The Lipschitz regularizer is active in (1), which can lead to the
solution succeeding in avoiding the dirty labels, as in Figure 1 right.
Our main results (Theorems 2.12 and 2.11) show that minimizers of Jn converge to minimizers of
J almost surely as the number of training points n tends to ∞. It is beyond the scope of this work
to estimate to what extent the errors are corrected, however we do know that the solution cannot fit
u0 due to the value of the Lipschitz constant, which is already an improvement over the case λ = 0.
The proofs for this section can be found in Section A.2.
Theorem 2.11.	Suppose that I: Y X Y → R is Lipschitz and strongly convex and let L = Lip(U0).
Thenfor any t > 0, with probability at least 1 — 2t-m+2 n-(Ct-I) all minimizing sequences un of
(1) and all minimizers u* of (5) satisfy
2. M
Ilun- u* IlYPdVol(x) ≤ CL
1
t log(n) ∖ m+2
n
The next result drops the assumption of strong convexity of the loss.
Theorem 2.12.	Suppose that infM P > 0, I : Y X Y → R is Lipschitz, and let u* ∈ W 1,∞(X; Y)
be any minimizer of (5). Then with probability one
un -→ u* uniformly on M as n → ∞,	(8)
where un is any sequence of minimizers of (1). Furthermore, every uniformly convergent subse-
quence of un converges on X to a minimizer of (5).
Remark 2.13. In Theorem 2.12 and Theorem 2.11, the sequence un does not, in general, converge
on the whole domain X . The important point is that the sequence converges on the data manifold
M, and solves the variational problem (5) off of the manifold, which ensures that the output of the
DNN is stable with respect to the input. See Figure 3.
7
Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Gunnar Aronsson, Michael Crandall, and Petri Juutinen. A tour of the theory of absolutely mini-
mizing functions. Bulletin of the American mathematical society, 41(4):439-505, 2004.
Gilles Aubert and Pierre Kornprobst. Mathematical problems in image processing: partial differen-
tial equations and the calculus of variations, volume 147. Springer Science & Business Media,
2006.
Peter L Bartlett. For valid generalization the size of the weights is more important than the size of
the network. In Advances in neural information processing systems, pp. 134-140, 1997.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting.
In Proceedings of the 27th annual conference on Computer graphics and interactive techniques,
pp. 417-424. ACM Press/Addison-Wesley Publishing Co., 2000.
StePhane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonaSymp-
totic theory of independence. Oxford university press, 2013.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of machine learning
research, 2(Mar):499-526, 2002.
Andrea Braides. Gamma-convergence for Beginners, volume 22. Clarendon Press, 2002.
Jeff Calder. Consistency of lipschitz learning with infinite unlabeled data and finite labeled data.
arXiv preprint arXiv:1710.10364, 2017.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning. MIT, 2006.
Zac Cranko, Simon Kornblith, Zhan Shi, and Richard Nock. Lipschitz networks and distributional
robustness. arXiv preprint arXiv:1809.01129, 2018.
Bernard Dacorogna. Direct methods in the calculus of variations, volume 78. Springer Science &
Business Media, 2007.
Harris Drucker and Yann Le Cun. Improving generalization performance using double backpropa-
gation. IEEE Transactions on Neural Networks, 3(6):991-997, 1992.
Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J Wainwright, and Michael I Jordan.
Asymptotic behavior of∖elLp-based laplacian regularization in semi-supervised learning. In Con-
ference on Learning Theory, pp. 879-906, 2016.
Christopher Elion and Luminita A Vese. An image decomposition model using the total variation
and the infinity laplacian. In Computational Imaging V, volume 6498, pp. 64980W. International
Society for Optics and Photonics, 2007.
Lawrence C. Evans. Partial differential equations, volume 19 of Graduate Studies in Mathematics.
American Mathematical Society, 1998. ISBN 0-8218-0772-2.
Lawrence Craig Evans. Measure theory and fine properties of functions. Routledge, 2018.
Chris Finlay and Adam M Oberman. Improved robustness to adversarial examples using lipschitz
regularization of the loss, 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
8
Under review as a conference paper at ICLR 2019
Ian Goodfellow, Patrick McDaniel, and Nicolas Papernot. Making machine learning robust against
adversarial inputs. Communications of the ACM, 61(7):56-66, June 2018. ISSN 00010782.
doi: 10.1145/3134599. URL http://dl.acm.org/citation.cfm?doid=3234519.
3134599.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Laurence Guillot and Carole Le Guyader. Extrapolation of vector fields using the infinity laplacian
and with applications to image segmentation. In Xue-Cheng Tai, Knut M0rken, Marius Lysaker,
and Knut-Andreas Lie (eds.), Scale Space and Variational Methods in Computer Vision, pp. 87-
99, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN 978-3-642-02256-2.
LaSzlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of
nonparametric regression. Springer Science & Business Media, 2006.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Roger A Horn, Roger A Horn, and Charles R Johnson. Matrix analysis. Cambridge university press,
1990.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Rasmus Kyng, Anup Rao, Sushant Sachdeva, and Daniel A Spielman. Algorithms for lipschitz
learning on graphs. In Conference on Learning Theory, pp. 1190-1223, 2015.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising
linear relationship predicts test performance in deep networks. arXiv preprint arXiv:1807.09659,
2018.
Edward James McShane. Extension of range of functions. Bulletin of the American Mathematical
Society, 40(12):837-842, 1934.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Mathew Penrose et al. Random geometric graphs. Number 5. Oxford university press, 2003.
Thomas Pock, Daniel Cremers, Horst Bischof, and Antonin Chambolle. Global solutions of varia-
tional models with convex regularization. SIAM Journal on Imaging Sciences, 3(4):1122-1145,
2010.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: nonlinear phenomena, 60(1-4):259-268, 1992.
Walter Rudin. Principles of mathematical analysis. McGraw-hill New York, 1976.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014. doi: 10.1017/CBO9781107298019.
Dejan Slepcev and Matthew Thorpe. Analysis of p-laplacian regularization in semi-supervised learn-
ing. arXiv preprint arXiv:1707.06213, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Michel Talagrand. The generic chaining: upper and lower bounds of stochastic processes. Springer
Science & Business Media, 2006.
9
Under review as a conference paper at ICLR 2019
AN Tikhonov and V Ya Arsenin. Solutions of Ill-Posed Problems. Winston and Sons, New York,
1977.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training
of wasserstein gans: A consistency term and its dual effect. arXiv preprint arXiv:1803.01541,
2018.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv:1611.03530, 2016.
A PROOFS
A.1 PROOFS FOR CLEAN LABELS
In this section we provide the proof of results stated in §2.3.
Theorem A.1. Suppose the loss function satisfies Assumption 2.3. If u, v ∈ W1,∞(X; Y ) are two
minimizers of (5) and infM ρ > 0 then u = v on M.
Proof. Let w = (u + v)/2. Then
J[w]
=J l( 2u + 1 v, uo) P dVol(x) + λ max (Lip (2 u + 1 V) , 0)
≤	[1 l(u,uo) + 11 (v,uo)] ρ dVol(x) + λ max (ɪ Lip(U) + 1 Lip(V), 0)
M
≤	[11 (u, uo) + 21 (v,uo)] ρ dVol(x) + λ [ɪ max (Lip (U) ,0) + 1 max (Lip (v), 0)]
M
1J [u] + 2 J [v] = min J [u].
Therefore, w is a minimizer ofJ and so we have equality above, which yields
11 l(u,uo) + 11 (v,U0)] ρ dVol(x) =	I (ɪu + 1 v,u0) P dVol(x).
MM
Since I is strictly convex in its first argument, it follows that U = V on M.
□
Proof of Lemma 2.10 of § 2.3. There exists EM such that for any 0 < E ≤ ∈m, We can cover M
with N geodesic balls B1 ,B2,..., BN of radius J where N ≤ Ce-m and C depends only on M
(GyOrfi et al., 2006). Let Zi denote the number of random variables X1,...,Xn falling in Bi. Then
Zi 〜B(n,pi), where Pi = IB P(X) dVol(x). Since P ≥ θ > 0 and Vol(Bi) ≥ CEm we have
Pi ≥ cem. Let An denote the event that at least one Bi is empty (i.e., Zi = 0 for some i). Then by
the union bound we deduce
N
P(An) ≤ E P (Zi = 0)
i=1
≤ CE-d(1 - CEm)n
=C exp (n log(1 - CEm) - log(em))
≤ C exp (-cnem - log(em)).
Choose 0 < E ≤ EM in the form nem = t log(n) with t ≤ n6M / log(n). Then
P(An) ≤ Ct-1 exp (-(Ct - 1) log(n)) .
10
Under review as a conference paper at ICLR 2019
In the event that An does not occur, then each Bi has at least one point, and so |x - σn(x) | ≤ CE
for all x ∈ M. Therefore
(t log(n)、1/m
IIId - σn∣∣L∞(M;X) ≤ C E = C I --n--- I
with probability at least 1 — Ct-1 exp (—(ct — 1) log(n)). Since ∣∣Id — σn∣L∞(M^) ≤ C∖Γd, the
result holds for t ≥ n∈M/ log(n), albeit with a larger constant C.	□
A.2 PROOFS FOR DIRTY LABELS
Here, we give the proofs of results from Section 2.4.
Definition A.2. We say that I is strongly convex with parameter θ > 0 if
l(ty1 + (I - t)y2, y0) + 2t(I - t)ly1 - y2∣lY ≤ tl(y1,y0) + (I - t)l(y2,y0)	(9)
for all y0 , y1 , y2 ∈ Y and 0 ≤ t ≤ 1.
We note that when I is twice differentiable, this notion of strong convexity is equivalent to assuming
▽ji I ≥ θI. The definition in equation (9) is useful for non-smooth functions, such as the Lipschitz
semi-norm present in J [u].
We give a proposition useful in the proof of Lemma A.4.
Proposition A.3. If I is strongly convex with parameter θ > 0 then
J [tu1 + (1 - t)u2 ] +
2t(i-1) /
M
∣∣u1 — u,2∣∣YPdVol(x) ≤ tJ[u1] + (1 — t)J[u2]
for all u1 , u2 ∈ W 1,∞ (X; Y ) and 0 ≤ t ≤ 1.
Proof. We compute
J [tu1 + (1 - t)u2]
/ l(tu1
+ (1 - t)u2, u0)P dV ol(x) + λmax (Lip(tu1 + (1 - t)u2), 0)
θ
≤ tJ[u1] + (I - t)J[u2] - 2t(I -
t)
M
∣∣u1 — u2∣∣Y ρ dVol(x),
which completes the proof.
Before proving Theorem 2.11, we require a preliminary lemma.
Lemma A.4. If u* ∈ W 1,∞ (X; Y) is a minimizer of ⑸ and U ∈ W 1,∞(X; Y) then
∣∣u — u*IIYρdVol(x) ≤ J[u] — J[u*].
□
2 L
Proof. We use Proposition A.3 with u1
θ
J [tu +(1 — t)u] + -t(1 — t)
Since J [tu* + (1 — t)u] ≥ J[u*]
J[u*] + θt(1 — t) / /
2M
*
u*
u* and u2 = u to obtain
-UIlYPdVol(x) ≤ tJ[u*] + (1 — t)J[u].
—UIlYρdVol(x) ≤ tJ[u*] + (1 — t)J[u],
L U
and so
θ
2t力
Setting t = 1 completes the proof.
*
u*
—UIlYρdVol(x) ≤ J[u] — J[u*].
□
11
Under review as a conference paper at ICLR 2019
The proof of Theorem 2.12 requires a preliminary Lemma. Let HL(X； Y) denote the collection of
L-Lipschitz functions w : X → Y.
Lemma A.5. Suppose that infM P > 0, and dim(M) = m. Then for any t > 0
sup
w∈Hl(X;Y)
1 n
n E W(Xi)-
'i=1
w ρ dVol(x)
≤ CL
t log(n)
n
(10)
/
M
(
)m⅛2
holds with probability at least 1 — 2t- m+2 n-(ct-1).
The estimate (10) is called a discrepancy result (Talagrand, 2006; Gyorfi et al., 2006), and is a
uniform version of concentration inequalities.
A key tool in the proof of Lemma A.5 is Bernstein,s inequality (Boucheron et al., 2013), which we
recall now for the reader,s convenience. For Xi,..., Xn i.i.d. with variance σ2 = E[(Xi -E[Xi])2],
if |Xi| ≤ M almost surely for all i then Bernstein,s inequality states that for any E > 0
P (In E Xi- E[Xi] > e) ≤ 2exp (-2σ2 +；Me/3) .
Proof of Lemma A.5. We note that it is sufficient to prove the result for w ∈ HL(X; Y) with
JM wρ dVol(x) = 0. In this case, we have W(X) = 0 for some x ∈ M, and so IlWllL∞(X: Y) ≤ CL.
We first give the proof for M = X = [0,1]m. We partition X into hypercubes B1,...,Bn of
side length h > 0, where N = h-m. Let Zj denote the number of x1,...,xn falling in Bj. Then
Zj is a Binomial random variable with parameters n and Pj = JB P dx ≥ Chm. By the Bernstein
inequality we have for each j that
P (l1 ZLLj
P dx > E) ≤ 2 exp (-cnh-me?)
(11)
provided 0 < E ≤ hm. Therefore, we deduce
1 n	1 N
n E w(xi) ≤ i ∑ Zj
i=1
j=1
max W
Bj
N
≤ max w
M Bj
N
≤ (min w
Bj
j=1
p P dx + CLh-m E
+ CLh) / ρ dx + CLh-nιe
N
≤ ΣJ J wρdx + CLh-m(hm+1 + E)
=W wρ dx + CL(h + h-me)
X
holds with probability at least 1-2h-m exp (-cnh-mt2) for any 0 < E ≤ hm. Choosing E = hm+1
we have that
n
1n
w ɪ2 w(xi) — J wρ dx ≤ CLh
i=1
holds for all u ∈ HL(X; Y) with probability at least 1 — 2h-m exp (-cnhm+2), provided h ≤ 1.
By selecting nhm+2 = t log(n)
sup
WGHL(X：Y)
n EW(Xi)工
wρ dV ol(x) ≤ CL
t log(n)
)m⅛2
(
n
12
Under review as a conference paper at ICLR 2019
holds with probability at least 1 - 2t- m+2 n-(Ct-I) for t ≤ n/ log(n). Since We have
IlwIlL∞(X;Y) ≤ CL, the estimate
n E w(xi)- M
sup
w∈HL (X;Y)
wρ dV ol(x) ≤ CL,
trivially holds, and hence we can allow t > n/ log(n) as well.
We sketch here how to prove the result on the manifold M. We cover M with k geodesic balls
of radius E > 0, denoted Bm(xi, e),..., BM(Xk, e), and let φι,..., Wk be a partition of unity
subordinate to this open covering of M. For E > 0 sufficiently small, the Riemannian exponential
map exp, : B(0, E) ⊂ TxM → M is a 祖ffeomorphism between the ball B(0,r) ⊂ TxM and
the geodesic ball Bm (x, E) ⊂ M, where TxM = Rm. Furthermore, the Jacobian of expx at
v ∈ B(0, r) ⊂ TxM, denoted by Jx (v), satisfies (by the Rauch Comparison Theorem)
(1 + C|v|2)-1 ≤ Jx(V) ≤ 1 + C|v|2.
Therefore, we can run the argument above on the ball B(0, r) ⊂ Rm in the tangent space, lift
the result to the geodesic ball Bm (xi, E) via the Riemannian exponential map expx, and apply the
bound
1n
一 ^2w(xi) — J wP dVol (x)
k 1n
∑ n A φj(Xi)W(Xi)-L
φjwρ dV ol(X)
≤
to complete the proof.
□
Remark A.6. The exponent 1/(m + 2) is not optimal, but affords a very simple proof. It is possible
to prove a similar result with the optimal exponent 1/m in dimension m ≥ 3, but the proof is
significantly more involved. We refer the reader to (Talagrand, 2006) for details.
Remark A.7. The proof of Theorem 2.12 shows that (1) Γ-converges to (5) almost surely as n → ∞
in the L∞ (X; Y ) topology. Γ-convergence is a notion of convergence for functionals that ensures
minimizers along a sequence of functionals converge to a minimizer of the Γ-limit. While we do
not use the language of Γ-convergence here, the ideas are present in the proof of Theorem 2.12. We
refer to (Braides, 2002) for details on Γ-convergence.
Proof of Theorem 2.12. By Lemma A.5 the event that
lim sup	|L[w, Pn] - L[w, P]| = 0
n→∞ w∈Hl(X;Y)	'	'
(12)
for all Lipschitz constants L > 0 has probability one. For the rest of the proof we restrict ourselves
to this event.
Let Un ∈ W 1,∞(X; Y) be a sequence of minimizers of (1), and let u* ∈ W 1,∞(X; Y) be any
minimizer of (5). Then since
λ(Lip(un) - L0) ≤ Jn [un] ≤ Jn [u0] = λ(Lip(u0) - L0)
We have Lip(Un) ≤ Lip(U0) =： L for all n. By the Arzela-Ascoli Theorem (Rudin, 1976) there
exists a subsequence unj and a function u ∈ W1,∞ (X; Y) such that unj → u uniformly as nj →
∞. Note we also have Lip(U) ≤ lim infj→∞ Lip(Unj ). Since
∣L[Un, Pn] — L[U,P]∣ ≤ ∣L[Un,Pn] - L[u, Pn] | + |L[u, Pn] - L[%p]∣
≤ Cllun - UIlL∞(M;Y) + SUP	|L[w, Pn] - LW P] |
w∈HL (X;Y)
it follows from (12) that L[un,, Pnj] → L[u, p] as j → ∞. It also follows from (12) that Jn [u*] →
J[u*] as n → ∞. Therefore
J[U*] = lim Jn[U*]
n→∞
≥ lim inf Jn [Un]
n→∞
= lim inf L[Un , Pn] + λ max(Lip(Un ) - L0 , 0)
n→∞
= lim L[Un, Pn] + lim inf λ max(Lip(Un) - L0, 0)
n→∞	n→∞
≥ L[U, P] + λ max(Lip(U) - L0 , 0) = J [U].
13
Under review as a conference paper at ICLR 2019
* on M, and so Unj → u* uniformly on
Therefore, U is a minimizer of J. By Theorem A.1, U = U
M as j → ∞.
Now, suppose that (8) does not hold. Then there exists a subsequence Unj and δ > 0 such that
max |Unj (x) - U* (x)| > δ
x∈M	j
for all j ≥ 1. However, we can apply the argument above to extract a further subsequence of Unj
that converges uniformly on M to u*, which is a contradiction. This completes the proof. □
Proof of Theorem 2.11. Let L = Lip(u0). By Lemma A.5
sup
w∈Hl(X;Y)
∣L[w,Pn] - L[w, ρ]∣ ≤ CL (
t log(n)
1
)m+2
(13)
n
holds with probability at least 1 — 2t- m+2 n-(Ct-I) for any t > 0. Let Us assume for the rest of the
proof that (13) holds.
As in the proof of Theorem 2.12, we have Lip(Un) ≤ L and Lip(U*) ≤ L, and so
| Jn [U*] — J [u*]| , | Jn[Un] - J [Un]| ≤ CL (^onn)) '"" .
Therefore
1
J [Un] — J [U*] = Jn[Un] — J [U*] + J [Un] — Jn[”n] ≤ CL (^^(2))“'+.
By Lemma A.4 we deduce
1
2 L UUn - U" YP dVol(x) ≤ CL (tlonn)) m+ ,
which completes the proof.
□
B INDUCED MATRIX NORMS
In some cases, we can take advantage of explicit formulas for matrix norms, which makes the esti-
mates in (2) an explicit function of the weights. Define the induced matrix norm by
IMl
p,q =SUP 一
X	Uxup
Then the following matrix norms formulas hold (see (Horn et al., 1990, Chapter 5.6.4))
∣∣M ∣∣∞,∞ = maxɪ2 ∣mj	UM l∣1,1 =max£ ImijI
ji
∣∣M∣∣1,∞ = max ImijI, IIM∣∣2,∞ = max ;旌 mj
C VARIATIONAL PROBLEMS IN IMAGE PROCESSING AND LIPSCHITZ
EXTENSIONS
The variational problem (1) can be interpreted as a relaxation of the Lipschitz Extension problem.
min Lip[u]
u:X—Y
subject to u(x) = u0 (x) for x ∈ D
(LE)
for D ⊂ X . The problem (LE) has more that one solution. Two classical results giving explicit so-
lutions in one dimension go back to Kirzbaum and to McShane (McShane, 1934). However solving
14
Under review as a conference paper at ICLR 2019
Figure 4: Comparison of different regularization methods. Lipschitz regularization preserves most
of the labels (Figure 1). Tychonoff regularization smooths the solution (left). Total Variation regu-
larization shifts the label values towards the mean (right).
(LE) is not practical for large scale problems. There has be extensive work on the Lipschitz Exten-
sion problem, see, (Johnson & Lindenstrauss, 1984), for example. More recently, optimal Lipschitz
extensions have been studied, with connections to Partial Differential Equations, see (Aronsson
et al., 2004). We can interpret (1) as a relaxed version of (LE), where λ-1 is a parameter which
replaces the unknown Lagrange multiplier for the constraint.
Variational problems are fundamental tools in mathematical approaches to image processing (Aubert
& Kornprobst, 2006) and inverse problems more generally. Without regularization inverse problems
can be ill-posed. The general form of the problem is
J [u] = L[u; uo] + λR[Vu]	(14)
which combines a loss or fidelity functional, L[u, u0], which depends on the values of u and the
reference image u°, and a regularization functional, R[Vu], which depends on the gradient, Vu.
The parameter λ determines the relative strength of the two terms which emphasize fidelity versus
regularization.
Example C.1. For example, a typical fidelity term is the standard least-squares L[u,u0] = ∣∣u —
U0^l2(d). The regularization 111Vu(X)IIlL2(D)corresponds to the classical TyChonoV regularization
(Tikhonov & Arsenin, 1977), R[Vu] = ∣∣ ∣Vu(x) ∣ ∣∣l∕d) is the Total Variation regularization model
of Rudin, Osher and Fatemi (Rudin et al., 1992).
Lipschitz regularization in not nearly as common. It appears in image processing in (Pock et al.,
2010, §4.4) (Elion & Vese, 2007) and (Guillot & Le Guyader, 2009). Variational problems of the
form (14) can be studied by the direct method in the calculus of variations (Dacorogna, 2007).
The problem (14) can be discretized to obtain a finite dimensional convex convex optimization
problem. The variational problem can also be studied by finding the first variation, which is a Partial
Differential Equation (Evans, 1998), which can then be solved numerically. Both approaches are
discussed in (Aubert & Kornprobst, 2006).
In Figure 4 we compare different regularization terms, in one dimension. The difference between
the regularizers is more extreme in higher dimensions.
15