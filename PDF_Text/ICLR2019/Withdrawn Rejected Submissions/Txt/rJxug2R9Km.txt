Under review as a conference paper at ICLR 2019
Meta-Learning for
Contextual Bandit Exploration
Anonymous authors
Paper under double-blind review
Ab stract
We describe MRL玄E, a meta-learning algorithm for learning a good exploration
policy in the interactive contextual bandit setting. Here, an algorithm must take
actions based on contexts, and learn based only on a reward signal from the action
taken, thereby generating an exploration/exploitation trade-off. MfiLfiE addresses
this trade-off by learning a good exploration strategy based on offline synthetic
tasks, on which it can simulate the contextual bandit setting. Based on these
simulations, Mêlfie uses an imitation learning strategy to learn a good exploration
policy that can then be applied to true contextual bandit tasks at test time. We
compare Mêlfie to seven strong baseline contextual bandit algorithms on a set of
three hundred real-world datasets, on which it outperforms alternatives in most
settings, especially when differences in rewards are large. Finally, we demonstrate
the importance of having a rich feature representation for learning how to explore.
1 Introduction
In a contextual bandit problem, an agent attempts to optimize its behavior over a sequence of rounds
based on limited feedback (Kaelbling, 1994; Auer, 2003; Langford & Zhang, 2008). In each round,
the agent chooses an action based on a context (features) for that round, and observes a reward for
that action but no others (§ 2). Contextual bandit problems arise in many real-world settings like
online recommendations and personalized medicine. As in reinforcement learning, the agent must
learn to balance exploitation (taking actions that, based on past experience, it believes will lead to
high instantaneous reward) and exploration (trying actions that it knows less about).
In this paper, we present a meta-learning approach to automatically learn a good exploration mech-
anism from data. To achieve this, we use supervised learning data sets on which we can simulate
contextual bandit tasks. Based on these simulations, our algorithm, Mêlfie (MEta LEarner for
Exploration)1, learns a good heuristic exploration strategy that should ideally generalize to future
contextual bandit problems. Mêlfie contrasts with more classical approaches to exploration (like
-greedy or LinUCB; see §4), in which exploration strategies are constructed by hand and by expert
algorithm designer. These approaches often achieve provably good exploration strategies in the worst
case, but are potentially overly pessimistic and are sometimes computationally intractable.
At training time (§2.3), Mêlfie simulates many contextual bandit problems from fully labeled data.
Using this data, in each round, Mêlfie is able to counterfactually simulate what would happen under
all possible action choices. We can then use this information to compute regret estimates for each
action, which can be optimized using the AggreVaTe imitation learning algorithm (Ross & Bagnell,
2014). Our imitation learning strategy mirrors that of the meta-learning approach of Bachman et al.
(2017) in the active learning setting. We present a simplified, stylized analysis of the behavior of
Mêlfie to ensure that our cost function encourages good behavior (§ 2.4). Empirically, we use
Mêlfie to train an exploration policy on only synthetic datasets and evaluate the resulting bandit
performance across three hundred (simulated) contextual bandit tasks (§3.2), comparing to a number
of alternative exploration algorithms, and showing the efficacy of our approach (§3.4).
1 Code release: the code is available online https://www.dropbox.com/sh/dc3v8po5cbu8zaw/
AACu1f_4c4wIZxD1e7W0KVZ0a?dl=0
1
Under review as a conference paper at ICLR 2019
2 Meta-Learning for Contextual Bandits
Contextual bandits is a model of interaction in which an agent chooses actions (based on contexts) and
receives immediate rewards for that action alone. For example, in a simplified news personalization
setting, at each time step t, a user arrives and the system must choose a news article to display to
them. Each possible news article corresponds to an action a, and the user corresponds to a context xt .
After the system chooses an article at to display, it can observe, for instance, the amount of time that
the user spends reading that article, which it can use as a reward rt(at). The goal of the system is to
choose articles to display that maximize the cumulative sum of rewards, but it has to do this without
ever being able to know what the reward would have been had it shown a different article a0t .
Formally, we largely follow the setup and notation of Agarwal et al. (2014). Let X be an input space
of contexts (users) and [K] = {1, . . . , K} be a finite action space (articles). We consider the statistical
setting in which there exists a fixed but unknown distribution D over pairs (x, r) ∈ X×[0, 1]K, where
r is a vector of rewards (for convenience, we assume all rewards are bounded in [0, 1]). In this setting,
the world operates iteratively over rounds t = 1, 2,   At each round t:
1.	The world draws (Xt, rt)〜D and reveals context xt.
2.	The agent (randomly) chooses action at ∈ [K] based on xt, and observes reward rt (at).
The goal of an algorithm is to maximize the cumulative sum of rewards over time. Typically the
primary quantity considered is the average regret of a sequence of actions a1, . . . , aT to the behavior
of the best possible function in a prespecified class F :
1T
Reg(ai, …，aτ) = max T E bt(f (Xt))- rt(at)
f∈ T t=1
An agent is call no-regret if its average regret is zero in the limit of large T.
(1)
2.1	Policy Optimization over Fixed Histories
To produce a good agent for interacting with the world, we assume access to a function class F
and to an oracle policy optimizer for that function class. For example, F may be a set of single
layer neural networks mapping user features (e.g., IP, browser, etc.) X ∈ X to predicted rewards
for actions (articles) a ∈ [K], where K is the total number of actions. Formally, the observable
record of interaction resulting from round t is the tuple (Xt, at, rt(at), pt(at)) ∈ X×[K]×[0, 1]×[0, 1],
where pt (at) is the probability that the agent chose action at, and the full history of interaction is
ht = h(Xi, ai, ri(ai), pi(ai))iit=1. The oracle policy optimizer, POLOPT, takes as input a history of
user interactions with the news recommendation system and outputs an f ∈ F with low expected
regret.
A standard example of a policy optimizer is to combine inverse propensity scaling (IPS) with a
regression algorithm (Dudik et al., 2011). Here, given a history h, each tuple (X, a, r,p) in that
history is mapped to a multiple-output regression example. The input for this regression example
is the same X; the output is a vector of K costs, all of which are zero except the ath component,
which takes value r/p. For example, if the agent chose to show to user X article 3, made that decision
with 80% probability, and received a reward of 0.6, then the corresponding output vector would
be h0, 0, 0.75, 0, . . . , 0i. This mapping is done for all tuples in the history, and then a supervised
learning algorithm on the function class F is used to produce a low-regret regressor f. This is the
function returned by the policy optimizer.
IPS has this nice property that it is an unbiased estimator; unfortunately, it tends to have large variance
especially when some probabilities p are small. In addition to IPS, there are several standard policy
optimizers that mostly attempt to reduce variance while remaining unbiased: the direct method
(which estimates the reward function from given data and uses this estimate in place of actual reward),
the double-robust estimator, and multitask regression. In our experiments, we use the direct method
because we found it best on average, but in principle any could be used.
2.2	TEST TIME BEHAVIOR OF MRLEE
In order to have an effective approach to the contextual bandit problem, one must be able to both
optimize a policy based on historic data and make decisions about how to explore. After all, in order
2
Under review as a conference paper at ICLR 2019
for the example news recommendation system to learn whether a particular user is interested in news
articles on some topic is to try showing such articles to see how the user responds (or to generalize
from related articles or users). The exploration/exploitation dilemma is fundamentally about long-
term payoffs: is it worth trying something potentially suboptimal now in order to learn how to behave
better in the future? A particularly simple and effective form of exploration is -greedy: given a
function f output by POLOPT, act according to f (x) with probability (1 - ) and act uniformly at
random with probability . Intuitively, one would hope to improve on such a strategy by taking more
(any!) information into account; for instance, basing the probability of exploration on f’s uncertainty.
Our goal in this paper is to learn how to explore from experience. The training procedure for MELfiE
will use offline supervised learning problems to learn an exploration policy π, which takes two inputs:
a function f ∈ F and a context x, and outputs an action. In our example, f will be the output of the
policy optimizer on all historic data, and x will be the current user. This is used to produce an agent
which interacts with the world, maintaining an initially empty history buffer h, as:
1.	The world draws (Xt, rt)〜D and reveals context xt.
2.	The agent computes ft J POLOPT(h) and a greedy action Gt = ∏(ft, xt).
3.	The agent plays at = at with probability (1 一 μ), and at uniformly at random otherwise.
4.	The agent observes rt(at) and appends (xt, at, rt(at), pt) to the history h,
where Pt = μ∕K if at = at； andPt = 1 - μ + μ∕K if at = <⅛.
Here, ft is the function optimized on the historical data, and π uses it and xt to choose an action.
Intuitively, π might choose to use the prediction ft(xt) most of the time, unless ft is quite uncertain
on this example, in which case π might choose to return the second (or third) most likely action
according to ft. The agent then performs a small amount of additional μ-greedy-style exploration:
most of the time it acts according to π but occasionally it explores some more. In practice (§3), we
find that setting μ = 0 is optimal in aggregate, but non-zero μ is necessary for our theory (§2.4).
2.3	Training Mêlfie by Imitation Learning
The meta-learning challenge is: how do we learn a good exploration policy π ? We assume we have
access to fully labeled data on which we can train π; this data must include context/reward pairs, but
where the reward for all actions is known. This is a weak assumption: in practice, we use purely
synthetic data as this training data; one could alternatively use any fully labeled classification dataset
(this is inspired by Beygelzimer & Langford (2009)). Under this assumption about the data, it is
natural to think of n,s behavior as a sequential decision making problem in a simulated setting, for
which a natural class of learning algorithms to consider are imitation learning algorithms (DaUme
et al., 2009; Ross et al., 2011; Ross & Bagnell, 2014; Chang et al., 2015).2 Informally, at training
time, Mêlfie will treat one of these synthetic datasets as if it were a contextual bandit dataset. At
each time step t, it will compute ft by running POLOPT on the historical data, and then ask: for each
action, what would the long time reward look like if I were to take this action. Because the training
data for MELfiE is fully labeled, this can be evaluated for each possible action, and a policy π can be
learned to maximize these rewards.
Importantly, we wish to train π using one set of tasks (for which we have fully supervised data on
which to run simulations) and apply it to wholly different tasks (for which we only have bandit
feedback). To achieve this, we allow π to depend representationally on ft in arbitrary ways: for
instance, it might use features that capture ft ’s uncertainty on the current example (see § 3.1 for
details). We additionally allow π to depend in a task-independent manner on the history (for instance,
which actions have not yet been tried): it can use features of the actions, rewards and probabilities in
the history but not depend directly on the contexts x. This is to ensure that π only learns to explore
and not also to solve the underlying task-dependent classification problem.
More formally, in imitation learning, we assume training-time access to an expert, π? , whose
behavior we wish to learn to imitate at test-time. From this, we can define an optimal reference
policy π? , which effectively “cheats” at training time by looking at the true labels. The learning
problem is then to estimate π to have as similar behavior to π? as possible, but without access to
those labels. Suppose we wish to learn an exploration policy π for a contextual bandit problem
2In other work on meta-learning, such problems are often cast as full reinforcement-learning problems. We
opt for imitation learning instead because it is computationally attractive and effective when a simulator exists.
3
Under review as a conference paper at ICLR 2019
Algorithm 1 MRLEE (supervised training sets {Sm}, hypothesis class F, exploration rate μ = 0.1,
number of validation examples NVal = 30), feature extractor Φ
i： for round n = 1, 2,...,N do
2:	initialize meta-dataset D = {} and choose dataset S at random from {Sm}
3:	partition and permute S randomly into train Tr and validation Val where |Val| = NVal
4:	set history h0 = {}
5:	for round t = 1, 2, . . . , |Tr| do
6:	let (xt, rt) = Trt
7:	for each action a = 1, . . . , K do
8：	optimize ft,。= POLOPT(F, ht-ι ㊉(xt, a, rt(a), 1-(K-1)μ)) on augmented history
9：	roll-out: estimate p。，the value of a, using rt(a) and a roll-out policy πout
10:	end for
11：	compute ft = POLOPT(F, ht-1)
12：	aggregate D — D ㊉(Φ(ft, xt, ht-ι, Val), hpι,..., PKi)
13：	roll-in: at 〜 K 1k + (1 - μ)∏n-ι(ft, Xt) with probability pt, 1 is an indicator function
14： append history h — ht7㊉(xt,at,rt(at),Pt)
15： end for
16：	update πn = LEARN(D)
17： end for
18： return {πn}nN=1
with K actions. We assume access to M supervised learning datasets S1, . . . , SM, where each
Sm = {(x1, r1), . . . , (xNm, rNm)} of size Nm, where each xn is from a (possibly different) input
space Xm and the reward vectors are all in [0, 1]K.We wish to learn an exploration policy π with
maximal reward： ergo, π should imitate a π? that always chooses its action optimally.
We additionally allow π to depend on a very small amount of fully labeled data from the task at hand,
which we use to allow π to calibrate ft ’s predictions.3
The imitation learning algorithm we use is AggreVaTe (Ross & Bagnell, 2014) (closely related to
DAgger (Ross et al., 2011)), and is instantiated for the contextual bandits meta-learning problem in
Alg 1. MRLEE operates in an iterative fashion, starting with an arbitrary π and improving it through
interaction with an expert. Over N rounds, MRLEE selects random training sets and simulates the
test-time behavior on that training set. The core functionality is to generate a number of states (ft, xt)
on which to train π, and to use the supervised data to estimate the value of every action from those
states. Mêlée achieves this by sampling a random supervised training set and setting aside some
validation data from it (line 3). It then simulates a contextual bandit problem on this training data;
at each time step t, it tries all actions and “pretends” like they were appended to the current history
(line 8) on which it trains a new policy and evaluates it’s roll-out value (line 9, described below).
This yields, for each t, a new training example for ∏, which is added to n,s training set (line 12); the
features for this example are features of the classifier based on true history (line 11) (and possibly
statistics of the history itself), with a label that gives, for each action, the corresponding value of that
action (the PaS computed in line 9). MRLEE then must commit to a roll-in action to actually take; it
chooses this according to a roll-in policy (line 13), described below.
The two key questions are： how to choose roll-in actions and how to evaluate roll-out values.
Roll-in actions. The distribution over states visited by MRLEE depends on the actions taken, and in
general it is good to have that distribution match what is seen at test time as closely as possible. This
distribution is determined by a roll-in policy (line 13), controlled in MRLEE by exploration parameter
μ ∈ [0,1/K]. As μ → 1/K, the roll-in policy approaches a uniform random policy; as μ → 0, the
roll-in policy becomes deterministic. When the roll-in policy does not explore, it acts according to
π(ft, .).
Roll-out values. The ideal value to assign to an action (from the perspective of the imitation
learning procedure) is that total reward (or advantage) that would be achieved in the long run if we
3Because π needs to learn to be task independent, we found that if fts were uncalibrated, it was very difficult
for π to generalize well to unseen tasks. In our experiments we use only 30 fully labeled examples, but alternative
approaches to calibrating ft that do not require this data would be ideal.
4
Under review as a conference paper at ICLR 2019
took this action and then behaved according to our final learned policy. Unfortunately, during training,
we do not yet know the final learned policy. Thus, a surrogate roll-out policy πout
is used instead.
A convenient, and often computationally efficient alternative, is to evaluate the value assuming all
future actions were taken by the expert (Langford & Zadrozny, 2005; DaUme et al., 2009; Ross &
Bagnell, 2014). In our setting, at any time step t, the expert has access to the fully supervised reward
vector rt for the context xt . When estimating the roll-out value for an action a, the expert will return
the true reward value for this action rt(a) and we use this as our estimate for the roll-out value.
2.4 Theoretical Guarantees
MRLEE is an instantiation of AGGREVATE (Ross & Bagnell, 2014) to meta-learning for contextual
bandits. As such, it inherits the guarantees provided by AggreVaTe; for example:
Theorem 1 (Thm 2.1 of Ross & Bagnell (2014), adapted) After N rounds in the parameter-free
setting, if a LEARN (line 16) is no-regret algorithm, then as N → ∞, with probability 1, it holds that
J (∏) ≤ J (π?) + 2T√K^ class, where J (∙) is the reward of the exploration policy, π is the average
policy returned, and ^ dass is the average regression regret for each πn, accurately predicting ρ.
This says that if we can achieve low regret at the problem of learning π on the training data it observes
(“D” in MRLEE), then this translates into low regret in the contextual-bandit setting.
Furthermore, we provide a stylized analysis for the test-time behavior of Mêlée. In particular,
we analyze Mêlée’s test-time behavior in a special case: when the underlying learning algorithm
is Banditron. Banditron is a variant of the multiclass Perceptron that operates under bandit
feedback. Details of this analysis (and proofs, which directly follow the original Banditron
analysis) are given in Appendix A; here we state the main result. Let γt = Pr[rt(π(ft, xt) =
1)∣xt] - Pr[rt(ft(xt)) = 1|xt] be the edge of ∏(ft,.) over f, and let Γ = TT PT=IE 1+KYt be an
overall measure of the edge. (For instance: if π does nothing, then all γt = 0 and Γ = 1.)
Theorem 2 Assume that for the sequence of examples, (x1, r1), (x2, r2), . . . , (xT , rT ), we have, for
all t, ||xt|| ≤ 1. Let W ? be any matrix, let L be the cumulative hinge-loss of W ?, let μ be a uniform
exploration probability, and let D = 2 ||W ? ||2F be the complexity of W?. Assume that Eγt ≥ 0 for
all t (that π never decreases the probability of a “correct” action). Then the number of mistakes M
made by MRLEE with BANDITRON as POLOPT satisfies:
EM ≤ L + KμT + 3 max {DΓ/μ, PDTKΓμ} + PDLr/μ	(2)
where the expectation is taken with respect to the randomness of the algorithm.
This result is highly stylized and the assumption that Eγt ≥ 0 is overly strong. It does, however, help
us understand the behavior of Mêlée, qualitatively: First, the quantity that matters in Theorem 2,
Etγt is (in the 0/1 loss case) exactly what MRLEE is optimizing: the expected improvement for
choosing an action against ft’s recommendation. Second, the benefit of using π within BANDITRON
is a local benefit: because π is trained with expert rollouts, as discussed in § 2.4, the primary
improvement in the analysis is to ensure that π does a better job predicting (in a single step) than
ft does. An obvious open question is whether it is possible to base the analysis on the regret of π
(rather than its error) and whether it is possible to extend beyond the simple Banditron setting.
3	Experimental Setup and Results
Our experimental setup operates as follows: Using a collection of synthetically generated classification
problem, we train an exploration policy π using MRLEE (Alg 1). This exploration policy learns to
explore on the basis of calibrated probabilistic predictions from f together with a predefined set of
exploration features (§3.1). Once π is learned and fixed, we follow the test-time behavior described
in §2.2 on a set of 300 “simulated” contextual bandit problems, derived from standard classification
tasks. In all cases, the underlying classifier f is a linear model trained with a policy optimizer that
runs stochastic gradient descent under the hood.
We seek to answer two questions experimentally: (1) How does Mêlée compare empirically to
alternative (hand-crafted) exploration strategies? (2) How important are the additional features used
by the meta-learner in comparison to using calibrated probability predictions from f as features?
5
Under review as a conference paper at ICLR 2019
3.1	Training Details for the Exploration Policy
Exploration Features. In our experiments, the exploration policy is trained based on features Φ
(Alg 1, line 12). These features are allowed to depend on the current classifier ft , and on any part of
the history except the inputs xt in order to maintain task independence. We additionally ensure that
its features are independent of the dimensionality of the inputs, so that π can generalize to datasets
of arbitrary dimensions. The specific features we use are listed below; these are largely inspired by
Konyushkova et al. (2017) but adapted and augmented to our setting. The features of ft that we use
are: a) predicted probability p(at |ft , xt); b) entropy of the predicted probability distribution; c) a
one-hot encoding for the predicted action ft(xt). The features of ht-1 that we use are: a) current
time step t; b) normalized counts for all previous actions predicted so far; c) average observed rewards
for each action; d) empirical variance of the observed rewards for each action in the history. In our
experiments, we found that it is essential to calibrate the predicted probabilities of the classifier ft .
We use a very small held-out dataset, of size 30, to achieve this. We use Platt’s scaling (Platt, 1999;
Lin et al., 2007) method to calibrate the predicted probabilities. Platt’s scaling works by fitting a
logistic regression model to the classifier’s predicted scores.
Training Datasets. In our experiments, we follow Konyushkova et al. (2017) (and also Peters et al.
(2014), in a different setting) and train the exploration policy π only on synthetic data. This is possible
because the exploration policy π never makes use of x explicitly and instead only accesses it through
ft’s behavior on it. We generate datasets with uniformly distributed class conditional distributions.
The datasets are always two-dimensional. Details are in Appendix B.
Implementation Details. Our implementation is based on scikit-learn (Pedregosa et al., 2011). We
fix the training time exploration parameter μ to 0.1. We train the exploration policy π on 82 synthetic
datasets each of size 3000 with uniform class conditional distributions, a total of 246k samples
(Appendix B). We train π using a linear classifier Breiman (2001) and set the hyper-parameters
for the learning rate, and data scaling methods using three-fold cross-validation on the whole meta-
training dataset. For the classifier class F, we use a linear model trained with stochastic gradient
descent. We standardize all features to zero mean and unit variance, or scale the features to lie
between zero and one. To select between the two scaling methods, and tune the classifier’s learning
rate, we use three-fold cross-validation on a small fully supervised training set of size 30 samples.
The same set is used to calibrate the predicted probabilities of ft .
3.2	Evaluation Tasks and Metrics
Following Bietti et al. (2018), we use a collection of 300 binary classification datasets from openml.
org for evaluation; the precise list and download instructions is in Appendix C. These datasets cover
a variety of different domains including text & image processing, medical diagnosis, and sensory data.
We convert multi-class classification datasets into cost-sensitive classification problems by using a 0/1
encoding.Given these fully supervised cost-sensitive multi-class datasets, we simulate the contextual
bandit setting by only revealing the reward for the selected actions. For evaluation, we use progressive
validation (Blum et al., 1999), which is exactly computing the reward of the algorithm. Specifically,
to evaluate the performance of an exploration algorithm A on a dataset S of size n, we compute the
progressive validation return G(A) as the average reward UP to n: G(A) = n Pn=I rt(at), where
at is the action chosen by the algorithm A and rt is the true reward vector.
Because our evaluation is over 300 datasets, we report aggregate results in two forms. The simpler
one is Win/Loss Statistics: We compare two exploration methods on a given dataset by counting
the number of statistically significant wins and losses. An exploration algorithm A wins over
another algorithm B if the progressive validation return G(A) is statistically significantly larger than
B’s return G(B) at the 0.01 level using a paired sample t-test. We additionally report cumulative
distributions of rewards for each algorithm. In particular, for a given relative reward value (x ∈
[0, 1]), the corresponding CDF value for a given algorithm is the fraction of datasets on which
this algorithm achieved reward at least x. We compute relative reward by Min-Max normalization.
Min-Max normalization linearly transforms reward y to X = m-mnn, where min & max are the
minimum & maximum rewards among all exploration algorithms.
6
Under review as a conference paper at ICLR 2019
ι.o
1.0
W°∙8
C
S 0.6
N
ro 0.4
E
J
o
N 0.2
0.8
S
σ>
'I 0.6
0.4
0.2
■LEE wins on 147 datasets
0.0
0.0
0.2	0.4	0.6	0.8	1.0
0.0 ,
0.0
Relative Reward (Higher is Better)
MELEE, μ: 0.0
ε-decreasing wins on 124 datasets
0.2	0.4	0.6	0.8	1
Reward for MELEE, ε∙. 0.0
-ɛ-deereasing, ε0: 0.1	EG ɛ-greedy
£
・・・ ɛ-greedyf £： 0.0	LinUCB	τ-firstf ε: 0.02
Cover, Bag Size: 16, ψ: 0.1
Figure 1: Comparison of algorithms on 300 classification problems. (Left) Comparison of all
exploration algorithms using the empirical cumulative distribution function of the relative progressive
validation return G (upper-right is optimal). The curves for -decreasing & -greedy coincide.
(Middle) Comparison of Mel⅛e to the second best performing exploration algorithm (6-decreasing),
every data point represents one of the 300 datasets, x-axis shows the reward of G(MELEe), y-axis
show the reward of G(6-decreasing), and red dots represent statistically significant runs. (Right) A
representative learning curve on dataset #1144.
3.3	Baseline Exploration Algorithms
Our experiments aim to determine how Mêlée compares to other standard exploration strategies. In
particular, we compare to:
6-greedy: With probability 6, explore uniformly at random; with probability 1 - 6 act greedily
according to ft (Sutton, 1996). Experimentally, we found 6 = 0 optimal on average, consistent
with the results of Bietti et al. (2018).
6-decreasing: selects a random action with probabilities 6i, where 6i = 60/t, 60 ∈]0, 1] and t is the
index of the current round. In our experiments we set 60 = 0.1. (Sutton & Barto, 1998)
Exponentiated Gradient 6-greedy: maintains a set of candidate values for 6-greedy exploration.
At each iteration, it runs a sampling procedure to select a new 6 from a finite set of candidates.
The probabilities associated with the candidates are initialized uniformly and updated with the
Exponentiated Gradient (EG) algorithm. Following Li et al. (2010b), we use the candidate set
{ei = 0.05×i + 0.01, i = 1, •一 , 10} for e.
LinUCB: Maintains confidence bounds for reward payoffs and selects actions with the highest
confidence bound. It is impractical to run “as is” due to high-dimensional matrix inversions. We
use diagonal approximation to the covariance when the dimensions exceeds 150. (Li et al., 2010a)
τ -first: Explore uniformly on the first τ fraction of the data; after that, act greedily.
Cover: Maintains a uniform distribution over a fixed number of policies. The policies are used to
approximate a covering distribution over policies that are good for both exploration and exploita-
tion (Agarwal et al., 2014).
Cover Non-Uniform: similar to Cover, but reduces the level of exploration of Cover to be more
competitive with the Greedy method. Cover-Nu doesn’t add extra exploration beyond the actions
chose by the covering policies (Bietti et al., 2018).
In all cases, we select the best hyperparameters for each exploration algorithm following Bietti et al.
(2018). These hyperparameters are: the choice of e in e-greedy, τ in τ -first, the number of bags, and
the tolerance ψ for Cover and Cover-NU. We set e = 0.0, τ = 0.02, bag size = 16, and ψ = 0.1.
3.4	Experimental Results
The overall results are shown in Figure 1. In the left-most figure, we see the CDFs for the different
algorithms. To help read this, note that at x = 1.0, we see that MELEE has a relative reward at least 1.0
on more than 40% of datasets, while e-decreasing and e-greedy achieve this on about 30% of datasets.
7
Under review as a conference paper at ICLR 2019
We find that the two strongest baselines are -decreasing and -greedy (better when reward differences
are small, toward the left of the graph). The two curves for -decreasing and -greedy coincide. This
happens because the exploration probability 0 for -decreasing decays rapidly approaching zero
With a rate of t, where t is the index of the current round. Melee outperforms the baselines in the
“large reward” regimes (right of graph) but underperforms -decreasing and -greedy in low reward
regimes (left of graph). In Figure 2, we show statistically-significant win/loss differences for each
of the algorithms. Mêlée is the only algorithm that always wins more than it loses against other
algorithms.
To understand more directly how MeLeE compares to -
decreasing, in the middle figure of Figure 1, we show a
scatter plot of rewards achieved by Mêlée (x-axis) and
-decreasing (y-axis) on each of the 300 datasets, with statis-
tically significant differences highlighted in red and insignif-
icant differences in blue. Points below the diagonal line
correspond to better performance by Mêlée (147 datasets)
and points above to -decreasing (124 datasets). The remain-
ing 29 had no significant difference.
In the right-most graph in Figure 1, we show a representa-
tive example of learning curves for the various algorithms.
Here, we see that as more data becomes available, all the ap-
proaches improve (except τ -first, which has ceased to learn
after 2% of the data).
160
80
0
-160
Figure 2: Win statistics: each (row, col-
umn) entry shows the number of times the
row algorithm won against the column, mi-
nus the number of losses.
Finally, we consider the effect that the additional features
have on Mêlée’s performance. In particular, we consider
a version of Mêlée with all features (this is the version
used in all other experiments) with an ablated version that
only has access to the (calibrated) probabilities of each action from the underlying classifier f . The
comparison is shown as a scatter plot in Figure 3. Here, we can see that the full feature set does
provide lift over just the calibrated probabilities, with a win-minus-loss improvement of 24.
4	Related Work and Discussion
The field of meta-learning is based on the idea of replacing
hand-engineered learning heuristics with heuristics learned
from data. One of the most relevant settings for meta-learning
to ours is active learning, in which one aims to learn a decision
function to decide which examples, from a pool of unlabeled ex-
amples, should be labeled. Past approaches to meta-learning for
active learning include reinforcement learning-based strategies
(Woodward & Finn, 2017; Fang et al., 2017), imitation learning-
based strategies (Bachman et al., 2017), and batch supervised
learning-based strategies (Konyushkova et al., 2017). Similar
approaches have been used to learn heuristics for optimization
(Li & Malik, 2016; Andrychowicz et al., 2016), multiarm (non-
contextual) bandits Maes et al. (2012), and neural architecture
search (Zoph & Le, 2016), recently mostly based on (deep)
reinforcement learning. While meta-learning for contextual
bandits is prima facie most similar to meta-learning for active
learning, there is a fundamental difference that makes it sig-
nificantly more challenging: in active learning, the goal is to
select as few examples as you can to learn, so by definition the
horizon is short; in contextual bandits, learning to explore is
fundamentally a long-horizon problem, because what matters
is not immediate reward but long term learning.
Reward for MELEE with All Features
Figure 3: Comparison of training
Mêlée with all the features (§ 3.1, y-
axis) vs training using only the cali-
brated prediction probabilities (x-axis).
Mêlée gets an additional leverage
when using all the features.
In reinforcement learning, Gupta et al. (2018) investigated the task of meta-learning an exploration
strategy for a distribution of related tasks by learning a latent exploration space. Similarly, Xu et al.
8
Under review as a conference paper at ICLR 2019
(2018) proposed a teacher-student approach for learning to do exploration in off-policy reinforcement
learning. While these approaches are effective if the distribution of tasks is very similar and the
state space is shared among different tasks, they fail to generalize when the tasks are different. Our
approach targets an easier problem than exploration in full reinforcement learning environments, and
can generalize well across a wide range of different tasks with completely unrelated features spaces.
There has also been a substantial amount of work on constructing “good” exploration policies, in
problems of varying complexity: traditional bandit settings (Karnin & Anava, 2016), contextual
bandits (FeraUd et al., 2016) and reinforcement learning (Osband et al., 2016). In both bandit
settings, most of this work has focused on the learning theory aspect of exploration: what exploration
distributions guarantee that learning will succeed (with high probability)? MfiLfiE, lacks such
gUarantees: in particUlar, if the data distribUtion of the observed learning contexts (φ(ft)) in some
test problem differs substantially from that on which Mfilfie was trained, we can say nothing about
the quality of the learned exploration. Nevertheless, despite fairly substantial distribution mismatch
(synthetic → real-world), MfiLfiE works well in practice, and our stylized theory (§2.4) suggests that
there may be an interesting avenue for developing strong theoretical results for contextual bandit
learning with learned exploration policies, and perhaps other meta-learning problems.
References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In In Proceedings of the 31st
International Conference on Machine Learning (ICML-14, pp. 1638-1646, 2014.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in
Neural Information Processing Systems, pp. 3981-3989, 2016.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine
Learning Research, 3:397-422, 2003.
Philip Bachman, Alessandro Sordoni, and Adam Trischler. Learning algorithms for active learning.
In ICML, 2017.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In Proceedings
of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
129-138. ACM, 2009.
Alberto Bietti, Alekh Agarwal, and John Langford. A Contextual Bandit Bake-off. working paper or
preprint, May 2018. URL https://hal.inria.fr/hal-01708310.
Avrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for k-fold and
progressive cross-validation. In Proceedings of the twelfth annual conference on Computational
learning theory, pp. 203-208. ACM, 1999.
Leo Breiman. Random forests. Mach. Learn., 45(1):5-32, October 2001. ISSN 0885-6125. doi:
10.1023/A:1010933404324. URL https://doi.org/10.1023/A:1010933404324.
Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, III, and John Langford.
Learning to search better than your teacher. In Proceedings of the 32Nd International Conference
on International Conference on Machine Learning - Volume 37, ICML, pp. 2058-2066. JMLR.org,
2015.
Hal Daume, John Langford, and Daniel Marcu. Search-based structured prediction. Machine
Learning, 75(3):297-325, Jun 2009. ISSN 1573-0565. doi: 10.1007/s10994-009-5106-x.
Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and
Tong Zhang. Efficient optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369,
2011.
Meng Fang, Yuan Li, and Trevor Cohn. Learning how to active learn: A deep reinforcement learning
approach. In EMNLP, 2017.
9
Under review as a conference paper at ICLR 2019
Raphael FeraUd, Robin Allesiardo, Tanguy Urvoy, and Fabrice Clerot. Random forest for the
contextual bandit problem. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the
19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings
of Machine Learning Research, pp. 93-101, Cadiz, Spain, 09-11 May 2016. PMLR. URL
http://proceedings.mlr.press/v51/feraud16.html.
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-
reinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245,
2018.
Leslie Pack Kaelbling. Associative reinforcement learning: Functions ink-dnf. Machine Learning,
15(3):279-298, 1994.
Sham M. Kakade, Shai Shalev-Shwart, and Ambuj Tewari. Efficient bandit algorithms for online
multiclass prediction. In ICML, 2008.
Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with opti-
mal sequences.	In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 199-
207. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6341-multi-armed-bandits-competing-with-optimal- sequences.pdf.
Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In
Advances in Neural Information Processing Systems, 2017.
John Langford and Bianca Zadrozny. Relating reinforcement learning performance to classification
performance. In Proceedings of the 22nd international conference on Machine learning, pp.
473-480. ACM, 2005.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information Processing Systems 20, pp. 817-824. Curran
Associates, Inc., 2008.
Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th International Conference
on World Wide Web, WWW ’10, pp. 661-670, New York, NY, USA, 2010a. ACM. ISBN 978-
1-60558-799-8. doi: 10.1145/1772690.1772758. URL http://doi.acm.org/10.1145/
1772690.1772758.
Wei Li, Xuerui Wang, Ruofei Zhang, Ying Cui, Jianchang Mao, and Rong Jin. Exploitation and
exploration in a performance based contextual advertising system. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’10, pp.
27-36, New York, NY, USA, 2010b. ACM.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. A note on platt’s probabilistic outputs for support
vector machines. Machine Learning, 68(3):267-276, Oct 2007. ISSN 1573-0565. doi: 10.1007/
s10994-007-5018-6. URLhttps://doi.org/10.1007/s10994-007-5018-6.
Francis Maes, Louis Wehenkel, and Damien Ernst. Meta-learning of exploration/exploitation strate-
gies: The multi-armed bandit case. In International Conference on Agents and Artificial Intelli-
gence, pp. 100-115. Springer, 2012.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep explo-
ration via bootstrapped dqn. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
4026-4034. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6501-deep-exploration-via-bootstrapped-dqn.pdf.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
10
Under review as a conference paper at ICLR 2019
Jonas Peters, Joris M Mooij, Dominik Janzing, and Bernhard Scholkopf. Causal discovery with
continuous additive noise models. The Journal of Machine Learning Research, 15(1):2009-2053,
2014.
John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In ADVANCES IN LARGE MARGIN CLASSIFIERS, pp. 61-74. MIT Press,
1999.
Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret
learning. arXiv preprint arXiv:1406.5979, 2014.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference
on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research,
pp. 627-635, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR.
Richard S Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse
coding. In Advances in neural information processing systems, pp. 1038-1044, 1996.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Mark Woodward and Chelsea Finn. Active one-shot learning. arXiv preprint arXiv:1702.06559,
2017.
Tianbing Xu, Qiang Liu, Liang Zhao, Wei Xu, and Jian Peng. Learning to explore with meta-policy
gradient. arXiv preprint arXiv:1803.05044, 2018.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
11
Under review as a conference paper at ICLR 2019
Supplementary Material For:
Meta-Learning for Contextual Bandit Exploration
A Stylized test-time analysis for Banditron: Details
The BANDITRONMRLEE algorithm is specified in Alg 2. The is exactly the same as the typical test
time behavior, except it uses a BANDITRON-type strategy for learning the underlying classifier f in
the place of POLOPT. POLICYELIMINATIONMETA takes as arguments: π (the learned exploration
policy) and μ ∈ (0,1∕(2K)) an added uniform exploration parameter. The Banditron learns a
linear multi-class classifier parameterized by a weight matrix of size K×D, where D is the input
dimensionality. The Banditron assumes a pure multi-class setting in which the reward for one
(“correct”) action is 1 and the reward for all other actions is zero.
At each round t, a prediction at is made according to f (summarized by Wt). We then define an
exploration distribution that “most of the time” acts according to π(ft, .), but smooths each action
with μ probability. The chosen action at is sampled from this distribution and a binary reward is
observed. The weights of the Banditron are updated according to the Banditron update rule
using Ut.
Algorithm 2 BANDITRONMRLEE (g, μ)
1： initialize W1 = 0 ∈ Rk×d
2: for rounds t = 1 . . . T : do
3：	observe xt ∈ RD
4： compute ^t = ft(xt) = argmaxk∈κ (Wtxt)k
5：	define Qμ(a) = μ +(1 — Kμ)1[a = π(Wt, xt)]
6： sample at 〜Qμ
7：	observe reward rt(at) ∈ {0, 1}
8： define Ut ∈ Rk×d as:
Ua,∙= Xt (1[rt(atQ=iαi[at=a] - 1[at = a])
9：	update Wt+1 = Wt + Ut
10： end for
The only difference between BANDITRONMRLEE and the original BANDITRON is the introduction of
π in the sampling distribution. The original algorithm achieves the following mistake bound shown
below, which depends on the notion of multi-class hinge-loss. In particular, the hinge-loss of W
on (x, r) is '(W, (x, r)) = max。=。？ max {0,1 — (Wx)a? +(Wx)。}, where a? is the a for which
r(a) = 1. The overall hinge-loss L is the sum of ` over the sequence of examples.
Theorem 3 (Thm. 1 and Corr. 2 of Kakade et al. (2008)) Assume that for the sequence of exam-
ples, (x1, r1), (x2, r2), . . . , (xT, rT), we have, for all t, ||xt|| ≤ 1. Let W? be any matrix, let L be
the cumulative hinge-loss of W?, and let D = 2 ||W?||2F be the complexity of W?. The number of
mistakes M made by the BANDITRON satisfies
EM ≤ L + KμT + 3 max { —, PDTKμ} + PDL∕μ	(3)
where the expectation is taken with respect to the randomness of the algorithm. Furthermore, in a
low noise setting (there exists W ? with fixed complexity d and loss L ≤ O( ,DKT)), then by setting
μ = PD/(TK), we obtain EM ≤ O(√KDT).
We can prove an analogous result for BanditronMêlée. The key quantity that will control how
much π improves the execution of BANDITRONMRLEE is how much π improves on ft when ft
is wrong. In particular, let γt = Pr[rt(π(ft,xt) = 1)|xt] - Pr[rt(ft(xt)) = 1|xt] be the edge of
π(ft, .) over f, and let Γ = T PT=ι E *KYt be an overall measure of the edge. (If ∏ does nothing,
then all γt = 0 and Γ = 1.) Given this quantity, we can prove the following Theorem 2.
Proof: [sketch] The proof is a small modification of the original proof of Theorem 3. The only
change is that in the original proof, the following bound is used： Et||Ut∣∣2∕∣∣xt∣∣2 = 1 + 1∕μ ≤ 2∕μ.
12
Under review as a conference paper at ICLR 2019
一 _ .. -ɪ,,o..C	-	-,	2Et一	一	一	一
We use, instead: Et∣∣Ut∣∣2∕∣∣χt∣∣2 ≤ 1 + Etμ⅛ ≤ ——-1+γt-. The rest of the proof goes through
identically.
B Details of Synthetic Datasets
We generate datasets with uniformly distributed class conditional distributions. We generate 2D
datasets by first sampling a random variable representing the Bayes classification error. The Bayes
error is sampled uniformly from the interval 0.0 to 0.5. Next, we generate a balanced dataset where
the data for each class lies within a unit rectangle and sampled uniformly. We overlap the sampling
rectangular regions to generate a dataset with the desired Bayes error selected in the first step.
C List of Datasets
The datasets we used can be accessed at https://www.openml.org/d/<id>. The list of
(<id>, size) pairs below shows the (<id> for the datasets we used and the dataset size in number of
examples:
(46,100) (716, 100) (726, 100) (754, 100) (762, 100) (768, 100) (775, 100) (783, 100) (789, 100)
(808, 100) (812, 100) (828, 100) (829, 100) (850, 100) (865, 100) (868, 100) (875, 100) (876, 100)
(878, 100) (916, 100) (922, 100) (932, 100) (1473, 100) (965, 101) (1064, 101) (956, 106) (1061,
107) (771, 108) (736, 111) (448, 120) (782, 120) (1455, 120) (1059, 121) (1441, 123) (714, 125)
(867, 130) (924, 130) (1075, 130) (1141, 130) (885, 131) (444, 132) (921, 132) (974, 132) (719, 137)
(1013, 138) (1151, 138) (784, 140) (1045, 145) (1066, 145) (1125, 146) (902, 147) (1006, 148) (969,
150) (955, 151) (1026, 155) (745, 159) (756, 159) (1085, 159) (1054, 161) (748, 163) (747, 167)
(973, 178) (463, 180) (801, 185) (1164, 185) (788, 186) (1154, 187) (941, 189) (1131, 193) (753,
194) (1012, 194) (1155, 195) (1488, 195) (446, 200) (721, 200) (1124, 201) (1132, 203) (40, 208)
(733, 209) (796, 209) (996, 214) (1005, 214) (895, 222) (1412, 226) (820, 235) (851, 240) (464, 250)
(730, 250) (732, 250) (744, 250) (746, 250) (763, 250) (769, 250) (773, 250) (776, 250) (793, 250)
(794, 250) (830, 250) (832, 250) (834, 250) (863, 250) (873, 250) (877, 250) (911, 250) (918, 250)
(933, 250) (935, 250) (1136, 250) (778, 252) (1442, 253) (1449, 253) (1159, 259) (450, 264) (811,
264) (336, 267) (1152, 267) (53, 270) (1073, 274) (1156, 275) (880, 284) (1121, 294) (43, 306) (818,
310) (915, 315) (1157, 321) (1162, 322) (925, 323) (1140, 324) (1144, 329) (1011, 336) (1147, 337)
(1133, 347) (337, 349) (59, 351) (1135, 355) (1143, 363) (1048, 369) (860, 380) (1129, 384) (1163,
386) (900, 400) (906, 400) (907, 400) (908, 400) (909, 400) (1025, 400) (1071, 403) (1123, 405)
(1160, 410) (1126, 412) (1122, 413) (1127, 421) (764, 450) (1065, 458) (1149, 458) (1498, 462)
(724, 468) (814, 468) (1148, 468) (1150, 470) (765, 475) (767, 475) (1153, 484) (742, 500) (749,
500) (750, 500) (766, 500) (779, 500) (792, 500) (805, 500) (824, 500) (838, 500) (855, 500) (869,
500) (870, 500) (879, 500) (884, 500) (886, 500) (888, 500) (896, 500) (920, 500) (926, 500) (936,
500) (937, 500) (943, 500) (987, 500) (1470, 500) (825, 506) (853, 506) (872, 506) (717, 508) (1063,
522) (954, 531) (1467, 540) (1165, 542) (1137, 546) (335, 554) (333, 556) (947, 559) (949, 559)
(950, 559) (951, 559) (826, 576) (1004, 600) (334, 601) (1158, 604) (770, 625) (997, 625) (1145,
630) (1443, 661) (774, 662) (795, 662) (827, 662) (931, 662) (292, 690) (1451, 705) (1464, 748)
(37, 768) (1014, 797) (970, 841) (994, 846) (841, 950) (50, 958) (1016, 990) (31, 1000) (715, 1000)
(718, 1000) (723, 1000) (740, 1000) (743, 1000) (751, 1000) (797, 1000) (799, 1000) (806, 1000)
(813, 1000) (837, 1000) (845, 1000) (849, 1000) (866, 1000) (903, 1000) (904, 1000) (910, 1000)
(912, 1000) (913, 1000) (917, 1000) (741, 1024) (1444, 1043) (1453, 1077) (1068, 1109) (934, 1156)
(1049, 1458) (1454, 1458) (983, 1473) (1128, 1545) (1130, 1545) (1138, 1545) (1139, 1545) (1142,
1545) (1146, 1545) (1161, 1545) (1166, 1545) (1050, 1563) (991, 1728) (962, 2000) (971, 2000)
(978, 2000) (995, 2000) (1020, 2000) (1022, 2000) (914, 2001) (1067, 2109) (772, 2178) (948, 2178)
(958, 2310) (312, 2407) (1487, 2534) (737, 3107) (953, 3190) (3, 3196) (1038, 3468) (871, 3848)
(728, 4052) (720, 4177) (1043, 4562) (44, 4601) (979, 5000) (1460, 5300) (1489, 5404) (1021, 5473)
(1069, 5589) (980, 5620) (847, 6574) (1116, 6598) (803, 7129) (1496, 7400) (725, 8192) (735, 8192)
(752, 8192) (761, 8192) (807, 8192)
13