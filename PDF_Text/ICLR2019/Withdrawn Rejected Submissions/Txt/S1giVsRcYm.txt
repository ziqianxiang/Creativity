Under review as a conference paper at ICLR 2019
Count-Based Exploration with
the Successor Representation
Anonymous authors
Paper under double-blind review
Ab stract
The problem of exploration in reinforcement learning is well-understood in the tab-
ular case and many sample-efficient algorithms are known. Nevertheless, it is often
unclear how the algorithms in the tabular setting can be extended to tasks with large
state-spaces where generalization is required. Recent promising developments
generally depend on problem-specific density models or handcrafted features. In
this paper we introduce a simple approach for exploration that allows us to develop
theoretically justified algorithms in the tabular case but that also give us intuitions
for new algorithms applicable to settings where function approximation is required.
Our approach and its underlying theory is based on the substochastic successor
representation, a concept we develop here. While the traditional successor repre-
sentation is a representation that defines state generalization by the similarity of
successor states, the substochastic successor representation is also able to implicitly
count the number of times each state (or feature) has been observed. This extension
connects two until now disjoint areas of research. We show in traditional tabular
domains (RiverSwim and SixArms) that our algorithm empirically performs as
well as other sample-efficient algorithms. We then describe a deep reinforcement
learning algorithm inspired by these ideas and show that it matches the performance
of recent pseudo-count-based methods in hard exploration Atari 2600 games.
1	Introduction
Reinforcement learning (RL) tackles sequential decision making problems by formulating them as
tasks where an agent must learn how to act optimally through trial and error interactions with the
environment. The goal in these problems is to maximize the sum of the numerical reward signal
observed at each time step. Because the actions taken by the agent influence not just the immediate
reward but also the states and associated rewards in the future, sequential decision making problems
require agents to deal with the trade-off between immediate and delayed rewards. Here we focus on
the problem of exploration in RL, which aims to reduce the number of samples (i.e., interactions) an
agent needs in order to learn to perform well in these tasks when the environment is initially unknown.
The sample efficiency of RL algorithms is largely dependent on how agents select exploratory actions.
In order to learn the proper balance between immediate and delayed rewards agents need to navigate
through the state space to learn about the outcome of different transitions. The number of samples an
agent requires is related to how quickly it is able to explore the state-space. Surprisingly, the most
common approach is to select exploratory actions uniformly at random, even in high-profile success
stories of RL (e.g., Tesauro, 1995; Mnih et al., 2015). Nevertheless, random exploration often fails in
environments with sparse rewards, that is, environments where the agent observes a reward signal of
value zero for the majority of states.1
In model-based approaches agents explicitly learn a model of the dynamics of the environment which
they use to plan future actions. In this setting the problem of exploration is well understood. When
all states can be enumerated and uniquely identified (tabular case), we have algorithms with proven
sample complexity bounds on the maximum number of suboptimal actions an agent selects before
1 When we refer to environments with sparse rewards we do so for brevity and ease of presentation. Actually,
any sequential decision making problem has dense rewards. In the RL formulation a reward signal is observed
at every time step. By environments with sparse rewards we mean environments where the vast majority of
transitions lead to reward signals with the same value.
1
Under review as a conference paper at ICLR 2019
converging to an -optimal policy (e.g., Brafman & Tennenholtz, 2002; Kearns & Singh, 2002; Strehl
& Littman, 2008). However, these approaches are not easily extended to large environments where it
is intractable to enumerate all of the states. When using function approximation, the concept of state
visitation is not helpful and learning useful models is by itself quite challenging.
Due to the difficulties in learning good models in large domains, model-free methods are much
more popular. Instead of building an explicit model of the environment, they estimate state values
directly from transition samples (state, action, reward, next state). Unfortunately, this approach makes
systematic exploration much more challenging. Nevertheless, because model-free methods make
up the majority of approaches scalable to large domains, practitioners often ignore the exploration
challenges these methods pose and accept the high sample complexity of random exploration. Reward
bonuses that promote exploration are one alternative to random walks (e.g., Bellemare et al., 2016;
Martin et al., 2017), but none such proposed solutions are widely adopted in the field.
In this paper we introduce an algorithm for exploration based on the successor representation (SR).
The SR, originally introduced by Dayan (1993), is a representation that generalizes between states
using the similarity between their successors, i.e., the similarity between the states that follow the
current state given the environment’s dynamics and the agent’s policy. The SR is defined for any
problem, it can be learned through temporal-difference learning (Sutton, 1988) and, as we discuss
below, it can also be seen as implicitly estimating the transition dynamics of the environment. Our
approach is inspired by the substochastic successor representation (SSR), a concept we introduce
here. The SSR is defined so that it implicitly counts state visitation, allowing us to use it to encourage
exploration. This idea connects representation learning and exploration, two otherwise disjoint areas
of research. The SSR allows us to derive an exploration bonus that when applied to model-based RL
generates algorithms that perform as well as theoretically sample-efficient algorithms. Importantly,
the intuition developed with the SSR assists us in the design of a model-free deep RL algorithm
that achieves performance similar to pseudo-count-based methods in hard exploration Atari 2600
games (Bellemare et al., 2016; Ostrovski et al., 2017).
2	Preliminaries
We consider an agent interacting with its environment in a sequential manner. Starting from a state
S0 ∈ S, at each step the agent takes an action At ∈ A, to which the environment responds with a state
St+1 ∈ S according to a transition probability function p(s0|s, a) = Pr(St+1 = s0|St = s, At = a),
and with a reward signal Rt+1 ∈ R, where r(s, a) indicates the expected reward for a transition from
state s under action a, that is, r(s, a) =. E[Rt|St = s, At = a].
The value of a state s when following a policy π, vπ (s), is defined to be the expected sum of
discounted rewards from that state:
vπ(s) =. Eπ PkT=t+1 γk-t-1RkSt = s , with γ being the
discount factor. When the transition probability function p and the reward function r are known, we
can compute vπ(s) recursively by solving the system of equations below (Bellman, 1957):
Vn(S) = Ean⑷SHr(S,a) + Y Es0 P(SlS,a)vn(SO)].
This equation can also be written in matrix form with vπ , r ∈ R|S| andPn ∈ RlSl×lSl:
vπ = r+ γPπ vπ = (I - γPπ)-1r,	(1)
where Pn is the state to state transition probability function induced by π, that is, Pn(S, S0) =
Pa n(a|S)P(SlS,a).
Traditional model-based algorithms for RL work by learning estimates of the matrix Pn and of the vec-
ʌ
tor r and using them to estimate Vn, for example by solving Equation 1. We use P∏ and r to denote
empirical estimates of Pn and r. Formally,
n(S, S0)
n(S)
r(S)=
C(s,s0)
n(S)
(2)
where r(i) denotes the i-th entry in the vector r, n(S, s0) is the number of times the transition S → s0
was observed, n(S) = Ps0∈S n(S, S0), and C(S, S0) is the sum of the rewards associated with the
n(S, S0) transitions (we drop the action in the discussion to simplify notation).
2
Under review as a conference paper at ICLR 2019
Alternatively, in model-free RL, instead of estimating Pπ and r we estimate vπ (s) directly from
samples. We often use temporal-difference (TD) learning (Sutton, 1988) to update our estimates of
v∏(s), V(∙), online:
V(St) J V(St) + α [Rt+ι + γV(St+ι) - V(St)],	(3)
where α is the step-size parameter. Generalization is required in problems with large state spaces,
where it is unfeasible to learn an individual value for each state. We do so by parametrizing V(S) with
a set of weights θ. We write, given the weights θ, V(s; θ) ≈ Vn (S) and ^(s, a; θ) ≈ q∏ (s, a), where
qπ(s, a) = r(s, a) + γ Ps0 p(s0|s, a)Vπ(s0). Model-free methods have performed well in problems
with large state spaces, mainly due to the use of neural networks as function approximators (e.g.,
Mnih et al., 2015).
Our algorithm is based on the successor representation (SR; Dayan, 1993). The successor representa-
tion, with respect to a policy π, Ψπ, is defined as
∞
Ψπ(S,S0) = Eπ,p XγtI{St = S0}S0 = S ,
t=0
where we assume the sum is convergent with I denoting the indicator function. Dayan (1993) has
shown that this expectation can be estimated from samples through TD learning. It also corresponds
to the Neumann series of γP :
∞
Ψπ=Xγt(Pπ)t =(I-γPπ)-1.	(4)
t=0
Notice that the SR is part of the solution when computing a value function: vπ = Ψπr (Equation 1).
We use Ψ∏ to denote the SR computed through Pn, the approximation of P∏.
The definition of the SR can also be extended to features. Successor features generalize the SR to the
function approximation setting (Barreto et al., 2017). We use the definition for the uncontrolled case
in this paper. Importantly, the successor features can also be learned with TD learning.
Definition 2.1 (Successor Features). For a given 0 ≤ γ < 1, policy π, and for a feature representation
φ(S) ∈ Rd, the successor features for a state S are:
∞
ψn(S) = En,p Xγtφ(St)S0 = S .
t=0
Alternatively, in matrix form, Ψn = Pt∞=0 γt(Pn)tΦ = (I - γPn)-1Φ. Notice that this definition
reduces to the SR in the tabular case, where Φ = I.
3	The Substochastic Successor Representation
In this section we introduce the concept of the substochastic successor representation (SSR). The SSR
is derived from an empirical transition matrix similar to Equation 2, but where each state incorporates
a small (1/(n(S) + 1)) probability of terminating at that state, rather than transiting to a next state.
As we will show, we can recover the visit counts n(S) through algebraic manipulation on the SSR.
While computing the SSR is usually impractical, we use it as inspiration in the design of a new
deep reinforcement learning algorithm for exploration (Section 4). In a nutshell, we view the SSR
as approximating the process of learning the SR from an uninformative initialization (i.e., the zero
vector), and using a stochastic update rule. While this approximation is relatively coarse, we believe
it gives qualitative justification to our use of the learned SRto guide exploration. To further this claim,
we demonstrate that using the SSR in synthetic, tabular settings yields comparable performance to
that of theoretically-derived exploration algorithms.
Definition 3.1 (Substochastic Successor Representation). Let Pn denote the substochastic matrix
induced by the environment's dynamics and by the policy π such that Pn (s0∣s) = ：(：)；). For a given
∙-v
0 ≤ γ < 1, the substochastic successor representation, Ψn, is defined as:
∞
Ψn = X YtPnt = (I-YPn )-1 ∙
t=0
3
Under review as a conference paper at ICLR 2019
The theorem below formalizes the idea that the `1 norm of the SSR implicitly counts state visitation.
Theorem 1. Let n(s) denote the number of times state s has been visited and let χ(s) = (1 + γ) -
∙-v	∙-v
∣∣Ψ∏ (s) || ι, where Ψ∏ is the substochastic SR as in Definition 3.1. For a given 0 ≤ γ < 1,
2
」____________L ≤ χ(s) ≤ Y
n(s) + 1	1 - γ - X')- n(s) + 1
ʌ ~ 人
Proof of Theorem 1. Let Pπ be the empirical transition matrix. We first rewrite Pπ in terms of Pπ:
∙-v
Pn (s, s0)
n(s, s0)
n(s) + 1
n(s) n(s, s0)
n(s) + 1 n(s)
n(s)
n(s) + 1
Pn (
s, s0) =
1 - n(s⅛ι )Pn(s,SO)
The expression above can also be written in matrix form: Pn = (I 一 N)P∏, where N ∈ RlSl×lSl
denotes the diagonal matrix of augmented inverse counts. Expanding Ψ∏ we have:
γ∞
2
Ψ n = tζ (YPn )t = I + YPn +	(YPn )t = I + γP∏ + YPK Ψ n ∙
The top eigenvector of a stochastic matrix is the all-ones vector, e (Meyn & Tweedie, 2012), and it
~ ʌ
corresponds to the eigenvalue 1. Using this fact and the definition of Pn with respect to Pn we have:
(I + Y(I - N)Pn)e + Y2Pn ψne
(I + Y )e — YN e + Y 22 2Ψ n e.
(5)
22
We can now bound the term Y2Pn Ψne using the fact that e is also the top eigenvector of the
successor representation and has eigenvalue ɪ-1Y (Machado et al., 2018b):
2
0 ≤ 寸Pn Ψne ≤ ：j---e.
1-Y
Plugging (5) into the definition of X we have (notice that Ψ(s)e = ∣∣Ψ(s)∣∣ι):
χ(s) = (1 + Y)e — (1 + Y)e + YNe — YlPn2Ψne = YNe — YlPn2Ψne ≤ YNe.
When we also use the other bound on the quadratic term we conclude that, for any state s,
2
二-----------Y- ≤ X(S) ≤ . y
n(s) + 1	1 — γ -	- n(s) + 1
□
In other words, the SSR, obtained after a slight change to the SR, can be used to recover state visitation
counts. The intuition behind this result is that the phantom transition, represented by the +1 in the
denominator of the SSR, serves as a proxy for the uncertainty about that state by underestimating the
∙-v
SR. This is due to the fact that	s0 Pn (S, S0) gets closer to 1 each time state S is visited.
This result can now be used to convert the SSR into a reward function in the tabular case. We do so by
using the SSR to define an exploration bonus, rint, such that the reward being maximized by the agent
becomes r(S, a) + βrint(S), where β is a scaling parameter. Since we want to incentivize agents to
∙-v
visit the least visited states as quickly as possible, we can trivially define rint = 一||Wn (s)∣∣ι, where
we penalize the agent by visiting the states that lead to commonly visited states. Notice that the shift
(1 + Y) in X(S) has no effect as an exploration bonus because it is the same across all states.
4
Under review as a conference paper at ICLR 2019
Table 1: Comparison between our algorithm, termed ESSR, and R-Max, E3, and MBIE. The
numbers reported for R-Max, E3, and MBIE are an estimate obtained from the histograms presented
by Strehl & Littman (2008). The performance of our algorithm is the average over 100 runs. A 95%
confidence interval is reported between parentheses.
RrVERSWIM
SiXARMs
E3	R-MAX	MBIE
3,000,000	3,000,000	3,250,000
T,800；000"	…2,800,000 ―	-9,250,000"
ESSR
3,088,924^^(± 57,584)^^
7,327,222……(± 1,189,460)
∙-v
Evaluating -∣∣Ψ∏(s)∣∣i as an exploration bonus
We evaluated the effectiveness of the proposed exploration bonus in a standard model-based algorithm.
in our implementation the agent updates its transition probability model and reward model through
Equation 2 and its ssR estimate as in Definition 3.1 (the pseudo-code of this algorithm is available in
the Appendix), which is then used for the exploration bonus rint. We used the domains Riverswim and
sixArms (strehl & Littman, 2008) to assess the performance of this algorithm.2 These are traditional
domains in the PAC-MDP literature (Kakade, 2003) and are often used to evaluate provably sample-
efficient algorithms. Details about these environments are also available in the Appendix. We used the
same protocol used by strehl & Littman (2008). our results are available in Table 1. it is interesting
to see that our algorithm performs as well as R-Max (Brafman & Tennenholtz, 2002) and E3 (Kearns
& singh, 2002) on Riverswim and it clearly outperforms these algorithms on sixArms.
4 Counting feature activations with the sR
in large environments, where enumerating all states is not an option, directly using the ssR as
described in the previous section is not viable. Learning the ssR becomes even more challenging
when the representation, φ(∙),is also being learned and so is non-stationary. In this section We design
an algorithm for the function approximation setting inspired by the results from the previous section.
since explicitly estimating the transition probability function is not an option, we learn the sR directly
using TD learning. in order to capture the ssR we rely on TD’s tendency to underestimate values
when the estimates are pessimistically initialized, just as the ssR underestimates the true successor
representation; with larger underestimates for states (and similarly features) that are rarely observed.
This is mainly due to the fact that when the sR is being learned with TD learning, because a reward of
1 is observed at each time step, there is no variance in the target and the predictions slowly approach
the true value of the sR. When pessimistically initialized, the predictions approach the target from
below. in this sense, what defines how far a prediction is from its final target is indeed how many
times it has been updated in a given state. Finally, recent work (Kulkarni et al., 2016; Machado et al.,
2018b) have shown successor features can be learned jointly with the feature representation itself.
These ideas are combined together to create our algorithm.
The neural network we used to learn the agent’s value function while also learning the feature
representation and the successor representation is depicted in Figure 1. The layers used to compute
the state-action value function, q(St, ∙), are structured as in DQN (Mnih et al., 2015), but with different
numbers of parameters (i..e, filter sizes, stride, and number of nodes). This was done to match oh
et al.’s (2015) architecture, which is known to succeed in the auxiliary task we define below. From
here on, we will call the part of our architecture that predicts q(St, ∙) DQNe. It is trained to minimize
LTD
E[((1 - T)δ(s, a) + τδMc(s, a))2],
2our algorithm maximizes the discounted return (γ = 0.95). We used policy iteration where the policy
evaluation step is terminated when the estimates of the value function change by less than 0.01. in Riverswim β
was set to 100 and in sixArms β was set to 1000. These values were obtained after evaluating the algorithm
for β ∈ {1, 10, 100, 200, 1000, 2000}. The code used to generate these results is available at: https:
//github.com/ommitted/blind.
5
Under review as a conference paper at ICLR 2019
DeconV
64,6x6
pad 2,2
Deconv
64,6x6
pad 2,2
Deconv
1,6x6
pad O,0
stride 2 ∣^^. stride 2 l∖ ∖ strid
84 X 84
£+i
Figure 1: Neural network architecture used by our algorithm when learning to Play Atari 2600 games.
where δ(s, a) and δMC(s, a) are defined as
δ(s, a) = Rt + βrint(s; θ-) + γ max q(s0, a0; θ-) - q(s, a; θ),
a0
∞
δMC(s,a) = Xγt r(St, At) + βrint(St; θ-) - q(s, a; θ).
This loss is known as the mixed Monte-Carlo return (MMC) and it has been used in the Past by the
algorithms that achieved succesful exPloration in deeP reinforcement learning (Bellemare et al., 2016;
Ostrovski et al., 2017). The distinction between θ and θ- is standard in the field, with θ- denoting
the Parameters of the target network, which is uPdated less often for stability PurPoses (Mnih et al.,
2015). As before, we use rint to denote the exPloration bonus obtained from the successor features of
the internal representation, φ(∙), which will be defined below. Moreover, to ensure all features are
in the same range, We normalize the feature vector so that ∣∣Φ(∙)∣∣2 = 1. In Figure 1 We highlight
the layer in which we normalize its output with the symbol φ . Notice that the features are always
non-negative due to the use of ReLU gates.
The successor features are computed by the two bottom layers of the network, which minimize the loss
LSR = E∏,p [(φ(St; θ-) + γψ(St+ι; θ-) - ψ(St; θ))[.
Zero is a fixed point for the SR. This is particularly concerning in settings with sparse rewards. The
agent might learn to set φ(∙) = ~ to achieve zero loss. We address this problem by not propagating
VLsr to φ(∙) (this is depicted in Figure 1 as an open circle stopping the gradient), and by creating an
auxiliary task (Jaderberg et al., 2017) to encourage a representation to be learned before a non-zero
reward is observed. As Machado et al. (2018b), we use the auxiliary task of predicting the next
observation, learned through the architecture proposed by Oh et al. (2015), which is depicted as the top
layers in Figure 1. The loss We minimize for this last part of the network is LReCOnS = (St+1 一 St+ι).
The overall loss minimized by the network is L = wTD LTD + wSRLSR + wRecons LRecons.
The last step in describing our algorithm is to define rint(St; θ-), the intrinsic reward we use to
encourage exploration. We choose the exploration bonus to be the inverse of the '2-norm of the
vector of successor features of the current state, that is,
1
rint(St; θ-)
IIΨ(St; θ-)I∣2,
where ψ(St; θ-) denotes the successor features of state St parametrized by θ-. The exploration
bonus comes from the same intuition presented in the previous section, but instead of penalizing
the agent with the norm of the SR we make rint(St; θ-) into a bonus (we observed in preliminary
experiments not discussed here that DQN performs better when dealing with positive rewards).
Moreover, instead of using the 'ι-norm we use the '2-norm of the SR since our features have unit
length in '2 (whereas the successor probabilities in the tabular-case have unit length in '1).
Finally, we initialize our network the same way Oh et al. (2015) does. We use Xavier initializa-
tion (Glorot & Bengio, 2010) in all layers except the fully connected layers around the element-wise
multiplication denoted by 0, which are initialized uniformly with values between -0.1 and 0.1.
6
Under review as a conference paper at ICLR 2019
Table 2: Performance of the proposed algorithm, DQNeMMC+SR, compared to various agents on the
“hard exploration” subset of Atari 2600 games. The DQN results reported are from Machado et al.
(2018a) while the DQNMMC+CTS and DQNMMC+PixelCNN results were extracted from the learning
curves available in Ostrovski et al.’s (2017) work. DQNeMMC denotes another baseline used in the com-
parison. When available, standard deviations are reported between parentheses. See text for details.
		DQN			DQNMMC+CTS	DQNMMC+PixelCNN	DQNMMC		DQNMMC+SR	
FREEWAY	32.4	(0.3)	292	29.4	29.5	(0.1)	29.5	(0.1)
'"GravITar		…118.5	(22.0)		199.8			275.4		''1078.3''''	(254.1)	.…430.3…	(109.4)
'"MONt.'REv."^'		0.0	■■■■(0.0)……		2941.9			1671.7			0.0 .…	(0.0)……	…1778.6…	.(903.6)…
…PRiVATEEYE …	―1447.4	(2,567.9)		32.8			14386.0			…113.4 ....	(42.3)'"'	……99.1…	■(1.8)……
"Solaris		…783.4	(55.3)……		1147.1			2279.4		―2244.6…	(378.8)	…2155.7…	(398.3)
…VENTURE			4.4	■■■■(5.4)……		0.0			856.2		. i220.i .…	(51.0)…	…1241.8…	'(23'6.0)"'
5	Empirical evaluation of exploration in deep RL
We evaluated our algorithm on the Arcade Learning Environment (Bellemare et al., 2013). Following
Bellemare et al.’s (2016) taxonomy, we evaluated our algorithm in the Atari 2600 games with sparse
rewards that pose hard exploration problems. They are: Freeway, Gravitar, Montezuma’ s
Revenge, Private Eye, Solaris, and Venture.3
We followed the evaluation protocol proposed by Machado et al. (2018a). We used Montezuma’ s
Revenge to tune our parameters (training set). The reported results are the average over 10 seeds
after 100 million frames. We evaluated our agents in the stochastic setting (sticky actions, ς = 0.25)
using a frame skip of 5 with the full action set (|A| = 18). The agent learns from raw pixels, that is,
it uses the game screen as input.
Our results were obtained with the algorithm described in Section 4. We set β = 0.025 after a
rough sweep over values in the game MONTEZUMA’ S REVENGE. We annealed in DQN’s -greedy
exploration over the first million steps, starting at 1.0 and stopping at 0.1 as done by Bellemare et al.
(2016). We trained the network with RMSprop with a step-size of 0.00025, an value of 0.01, and a
decay of 0.95, which are the standard parameters for training DQN (Mnih et al., 2015). The discount
factor, γ, is set to 0.99 and wTD = 1, wSR = 1000, wRecons = 0.001. The weights wTD, wSR, and wRecons
were set so that the loss functions would be roughly the same scale. All other parameters are the
same as those used by Mnih et al. (2015).
Table 2 summarizes the results after 100 million frames. The performance of other algorithms
is also provided for reference. Notice we are reporting learning performance for all algorithms
instead of the maximum scores achieved by the algorithm. We use the superscript MMC to distinguish
between the algorithms that use MMC from those that do not. When comparing our algorithm,
DQNeMMC+SR, to DQN we can see how much our approach improves over the most traditional
baseline. By comparing our algorithm’s performance to DQNMMC+CTS (Bellemare et al., 2016) and
DQNMMC+PixelCNN (Ostrovski et al., 2017) we compare our algorithm to established baselines for
exploration. As highlighted in Section 4, the parameters of the network we used are different from
those used in the traditional DQN network, so we also compared the performance of our algorithm
to the performance of the same network our algorithm uses but without the additional modules
(next state prediction and successor representation) by setting wSR = wRecons = 0 and without the
intrinsic reward bonus by setting β = 0.0. The column labeled DQNeMMC contains the results for this
baseline. This comparison allows us to explicitly quantify the improvement provided by the proposed
exploration bonus. The learning curves of these algorithms, their performance after different amounts
of experience, and additional results analyzing, for example, the impact of the introduced auxiliary
task, are available in the Appendix.
We can clearly see that our algorithm achieves scores much higher than those achieved by DQN, which
struggles in games that pose hard exploration problems. Moreover, by comparing DQNeMMC+SR to
DQNeMMC we can see that the provided exploration bonus has abig impact in the game MONTEZUMA’ S
Revenge, which is probably known as the hardest game among those we used in our evaluation.
Interestingly, the change in architecture and the use of MMC leads to a big improvement in games
such as Gravitar and Venture, which we cannot fully explain. However, notice that the change
3The code used to generate these results is available at: https://github.com/ommitted/blind.
7
Under review as a conference paper at ICLR 2019
in architecture does not have any effect in Montezuma’ s Revenge. The proposed exploration
bonus seems to be essential in this game. Finally, we also compared our algorithm to DQNMMC+CTS
and DQNMMC+PixelCNN. We can observe that, on average, it performs as well as these algorithms,
but instead of requiring a density model it requires the SR, which is already defined for every problem
since it is a component of the value function estimates, as discussed in Section 2.
6	Related work
There are multiple algorithms in the tabular, model-based case with guarantees about their per-
formance (e.g., Brafman & Tennenholtz, 2002; Kearns & Singh, 2002; Strehl & Littman, 2008;
Osband et al., 2016). RiverSwim and SixArms are domains traditionally used when evaluating these
algorithms. In this paper we have given evidence that our algorithm performs as well as some of these
algorithms with theoretical guarantees. Among these algorithms, R-Max seems the closest approach
to ours. As with R-Max, the algorithm we presented in Section 3 augments the state-space with
an imaginary state and encourages the agent to visit that state, implicitly reducing the algorithm’s
uncertainty in the state-space. However, R-Max deletes the transition to this imaginary state once a
state has been visited a given number of times. Ours lets the probability of visiting this imaginary state
vanish with additional visitations. Moreover, notice that it is not clear how to apply these traditional
algorithms such as R-Max and E3 to large domains where function approximation is required.
Conversely, there are not many model-free approaches with proven sample-complexity bounds (e.g.,
Strehl et al., 2006), but there are multiple model-free algorithms for exploration that actually work
in large domains (e.g., Stadie et al., 2015; Bellemare et al., 2016; Ostrovski et al., 2017; Plappert
et al., 2018). Among these algorithms, the use of pseudo-counts through density models is the closest
to ours (Bellemare et al., 2016; Ostrovski et al., 2017). Inspired by those papers we used the mixed
Monte-Carlo return as a target in the update rule. In Section 5 we have shown that our algorithm
performs generally as well as these approaches without requiring a density model. Importantly,
Martin et al. (2017) had already shown that counting activations of fixed, handcrafted features in
Atari 2600 games leads to good exploration behavior. Nevertheless, by using the SSR we are not only
counting learned features but we are also implicitly capturing the induced transition dynamics.
Finally, the SR has already been used in the context of exploration. However, it was used to help the
agent learn how to act in a higher level of abstraction in order to navigate through the state space
faster (Machado et al., 2017; 2018b). Such an approach has led to promising results in the tabular
case but only anecdotal evidence about its scalability has been provided when the idea was applied to
large domains such as Atari 2600 games. Importantly, the work developed by Machado et al. (2018b),
Kulkarni et al. (2016) and Oh et al. (2015) are the main motivation for the neural network architecture
presented here. Oh et al. (2015) have shown how one can predict the next screen given the current
observation and action (our auxiliary task), while Machado et al. (2018b) and Kulkarni et al. (2016)
have proposed different architectures for learning the successor representation from raw pixels.
7	Conclusion
RL algorithms tend to have high sample complexity, which often prevents them from being used in
the real-world. Poor exploration strategies is one of the main reasons for this high sample-complexity.
Despite all of its shortcomings, uniform random exploration is, to date, the most commonly used
approach for exploration. This is mainly due to the fact that most approaches for tackling the
exploration problem still rely on domain-specific knowledge (e.g., density models, handcrafted
features), or on having an agent learn a perfect model of the environment. In this paper we introduced
a general method for exploration in RL that implicitly counts state (or feature) visitation in order to
guide the exploration process. It is compatible to representation learning and the idea can also be
adapted to be applied to large domains.
This result opens up multiple possibilities for future work. Based on the results presented in Section 3,
for example, we conjecture that the substochastic successor representation can be actually used to
generate algorithms with PAC-MDP bounds. Investigating to what extent different auxiliary tasks
impact the algorithm’s performance, and whether simpler tasks such as predicting feature activations
or parts of the input (Jaderberg et al., 2017) are effective is also worth studying. Finally, it might
be interesting to further investigate the connection between representation learning and exploration,
since it is also known that better representations can lead to faster exploration (Jiang et al., 2017).
8
Under review as a conference paper at ICLR 2019
References
Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom SchaUL David Silver, and Hado
van Hasselt. Successor Features for Transfer in Reinforcement Learning. In Advances in Neural
Information Processing Systems (NIPS),pp. 4058-4068, 2017.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning
Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence
Research, 47:253-279, 2013.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom SchauL David Saxton, and Remi
Munos. Unifying Count-Based Exploration and Intrinsic Motivation. In Advances in Neural
Information Processing Systems (NIPS), pp. 1471-1479, 2016.
Richard E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.
Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A General Polynomial Time Algorithm for
Near-Optimal Reinforcement Learning. Journal of Machine Learning Research, 3:213-231, 2002.
Peter Dayan. Improving Generalization for Temporal Difference Learning: The Successor Represen-
tation. Neural Computation, 5(4):613-624, 1993.
Xavier Glorot and Yoshua Bengio. Understanding the Difficulty of Training Deep Feedforward
Neural Networks. In Proceedings of the International Conference on Artificial Intelligence and
Statistics (AISTATS), pp. 249-256, 2010.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks. In
Proceedings of the International Conference on Learning Representations (ICLR), 2017.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Con-
textual Decision Processes with Low Bellman Rank are PAC-Learnable. In Proceedings of the
International Conference on Machine Learning (ICML), pp. 1704-1713, 2017.
Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computa-
tional Neuroscience Unit, University College London, 2003.
Michael J. Kearns and Satinder P. Singh. Near-Optimal Reinforcement Learning in Polynomial Time.
Machine Learning, 49(2-3):209-232, 2002.
Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep Successor
Reinforcement Learning. CoRR, abs/1606.02396, 2016.
Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. A Laplacian Framework for Option
Discovery in Reinforcement Learning. In Proceedings of the International Conference on Machine
Learning (ICML), pp. 2295-2304, 2017.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open
Problems for General Agents. Journal of Artificial Intelligence Research, 61:523-562, 2018a.
Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray
Campbell. Eigenoption Discovery through the Deep Successor Representation. In Proceedings of
the International Conference on Learning Representations (ICLR), 2018b.
Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-Based Exploration
in Feature Space for Reinforcement Learning. In Proceedings of the International Joint Conference
on Artificial Intelligence (IJCAI), pp. 2471-2478, 2017.
Sean P Meyn and Richard L Tweedie. Markov Chains and Stochastic Stability. 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning.
Nature, 518:529-533, 2015.
9
Under review as a conference paper at ICLR 2019
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder P. Singh. Action-
Conditional Video Prediction using Deep Networks in Atari Games. In Advances in Neural
Information Processing Systems (NIPS),pp. 2863-2871, 2015.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and Exploration via Randomized
Value Functions. In Proceedings of the International Conference on Machine Learning (ICML),
pp. 2377-2386, 2016.
Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. Count-Based ExPlo-
ration with Neural Density Models. In Proceedings of the International Conference on Machine
Learning (ICML), pp. 2721-2730, 2017.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter Space Noise for Exploration.
In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
Bradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing Exploration in Reinforcement
Learning With Deep Predictive Models. CoRR, abs/1507.00814, 2015.
Alexander L. Strehl and Michael L. Littman. An Analysis of Model-based Interval Estimation for
Markov Decision Processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC
Model-Free Reinforcement Learning. In Proceedings of the International Conference on Machine
Learning (ICML), pp. 881-888, 2006.
Richard S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine Learning,
3:9-44, 1988.
Gerald Tesauro. Temporal Difference Learning and TD-Gammon. Communications of the ACM, 38
(3):58-68, 1995.
10
Under review as a conference paper at ICLR 2019
Supplemental Material
Count-Based Exploration with the Successor Representation
This supplementary material contains details omitted from the main text due to space constraints.
The list of contents is below:
•	Pseudo-code of the model-based algorithm discussed in Section 3;
•	Description of RiverSwim and SixArms, the tabular domains we used in our evaluation;
•	Learning curves of DQNe and DQNeMMC+SR and their performance after different amounts
of experience in the Atari 2600 games used for evaluation;
•	Results of additional experiments designed to evaluate the role of the auxiliary task in the
results reported in the paper for ESSR.
Exploration through the Substochastic Successor Representation
In the main paper we described our algorithm as a standard model-based algorithm where the agent
updates its transition probability model and reward model through Equation 2 and its SSR estimate as
in Definition 3.1. The pseudo-code with details about the implementation is presented in Algorithm 1.
Algorithm 1 Exploration through the Substochastic Successor Representation (ESSR)
n(s, s0) J 0
t(s, a, S) J- 1
r(s, a) J 0
ʌ
PGa) J 1/|S|
P(S, s0) J 0
π J random over
∀s, s0 ∈ S
∀s, s0 ∈ S, ∀a ∈ A
∀s ∈ S, ∀a ∈ A
∀s ∈ S, ∀a ∈ A
∀s, s0 ∈ S
A
while episode is not over do
Observe s ∈ S, take action a ∈ A selected according to π(s), and observe a reward R and a
next state s0 ∈ S
n(s, s0) J n(s, s0) + 1
t(s, a, s0) J t(s, a, s0) + 1
n(s) J Px0,b t(s, b, x0)
n(s, a) J	x0 t(s, a, x0)
^(Q C C J (t(S,a,sO)-2)xr(S,a,sO)+R
r (s,a,s ) 弋	t(s,a,s0)-1
for each state x0 ∈ S do
Pgax)J ⅛⅛i
P(s,χ0) J 器言•
end for
∙-v
∙-v
Ψ J (I — YP)
∙-v
rint J -Ψe
-1
ʌ
∏ J PolicyIteration(P, r + e厂面)
end while
11
Under review as a conference paper at ICLR 2019
Description of RiverSwim and SixArms
The two domains we used as testbed to evaluate the proposed model-based algorithm with the
exploration bonus generated by the substochastic successor representation are shown in Figure 2.
These domains are the same used by Strehl & Littman (2008). For SixArms, the agent starts in state 0.
For RiverSwim, the agent starts in either state 1 or 2 with equal probability.
<’1,0.7, 0)	<1,0.6, 0)	<1,0.6,	0)	<1,0.6,	0)	<1,0.6,	0)	<1, 0.3, 10000)
<'0, 1,0)	<0, 1,0)	<0, 1, 0)	<0, 1,0)	<0, 1,0)
<1, 0.1,0)	<1,0.1,0)	<1,0.1,0)	<1,0.1, 0)	<1,0.1,0)
(4, 1, 1660)∣
4 )<0-2, 1,。)
⑶ 0.05, 0)'
(0-4,1,0),
(5, 1, 6000)
(5,1,0)
∖(0:3, 1,01
∙!(≡
(2, 1,300)
(0-3, 1,50)
(5, 1,50)
(b) SixArmS
(3, 1, 800)
'(4, 1, 0)
(UU^
∣(1, 1, 133)
(2-5,1： 0)
(a) RiverSWim

Figure 2:	Domains used as testbed in the tabular case. The tuples in each transition should be read as
haction id, probability, rewardi. See text for details.
Evaluation the impact of the auxiliary task in ESSR
The algorithm we introduced in the paper, ESSR, relies on a network that estimates the state-action
value function, the successor representation, and the next observation to be seen given the agent’s
current observation and action. While the results depicted in Table 2 allow us to clearly see the benefit
of using an exploration bonus derived from the successor representation, they do not inform us about
the impact of the auxiliary task in the results. The experiments in this section aim at addressing this
issue. We focus on Montezumas Revenge because it is the game where the problem of exploration is
maximized, with most algorithms not being able to do anything without an exploration bonus.
The first question we asked was whether the auxiliary task was necessary in our algorithm. We evalu-
ated this by dropping the reconstruction module from the network to test whether the initial random
noise generated by the successor representation is enough to drive representation learning. It is not.
When dropping the auxiliary task, the average performance of this baseline over 4 seeds in Mon-
TEZUMA’ S REVENGE after 100 million frames was 100.0 points (σ2 = 200.0; min: 0.0, max: 400.0).
As comparison, our algorithm obtains 1778.6 points (σ2 = 903.6, min: 400.0, max: 2500.0). These
results suggest that auxiliary tasks seem to be necessary for our method to perform well.
We also evaluated whether the auxiliary task was sufficient to generate the results we observed. To do
so we dropped the SR module and set β = 0.0 to evaluate whether our exploration bonus was actually
improving the agent’s performance or whether the auxiliary task was doing it. The exploration bonus
seems to be essential in our algorithm. When dropping the exploration bonus and the successor
representation module, the average performance of this baseline over 4 seeds in Montezuma’ s
REVENGE after 100 million frames was 398.5 points (σ2 = 230.1; min: 0.0, max: 400.0). Again,
clearly, the auxiliary task is not a sufficient condition for the performance we report.
The reported results use the same parameters as those reported in the main paper. Learning curves for
each individual run are depicted in Figure 3.
Figure 3:	Evaluation of the sufficiency and necessity of the auxiliary task in DQNeMMC+SR. The
learning curves are smoothed with a running average computed using a window of size 100.
12
Under review as a conference paper at ICLR 2019
Additional results for DQNeMMC+SR and DQNeMMC in the Atari 2600 games
As recommended by Machado et al. (2018a), we report the performance of DQNeMMC+SR and
DQNeMMC after different amounts of experience (10, 50, and 100 million frames) in Tables 3 and 4.
Finally, Figure 4 depicts the learning curves obtained with the evaluated algorithms in each game.
Lighter lines represent individual runs while the solid lines encode the average over the multiple runs.
Game	10M frames		50M frames		100M frames	
FREEWAY	-249-	">5)^^	29y	"TO^^	295-	
GRAVitar		■ 244.1"	(23.8)	■ 326.4 "	(53.0)	""430.3"	(109.4)
MONT.REVENGE		2.6"	U2)…	…563.8-	(465.7)"	"1778.6"	(903.6)"
Private Eye		……99.2"	U2).…	……98.5"	(3.3)…	……99.1…	(1.8)……
SoLARis		"1547.5"	(410.9)	"2Ο36.3"	(339.0)	-21557	(398.3)
Venture		……26.2"	..(22T)…	■ 942.0 ■	(423.8)"	"1241.8"	(236.0)"
Table 3: Results obtained with DQNeMMC+SR after different amounts of experience.
Game	10M frames		50M frames		100M frames	
FREEWAY	-257^^	∏X5)^^	29.6-	^W)^^	29Γ"	^>1-
GRAVitar		'229.9"	"(31.3)"	''559.3"	(75.9)-	"1078.3 "	(254.1)"
MONT. ReVEnge		0.0 一	70.0)…		0.0 一	(0.0)…		0.0"	(0.0)……
PriVAte Eye		""216.7"	(219.5)	■ 109.1"	(44.1)"	"113.4"	■(42.3)…
SoLARiS		"2230.0"	"(322.3)"	"218T.5"	(292.9)	"2244.6"	(378.8)
Venture		……63.8"	"(31.3)"	"794.1"	(151.9)"	"1220.1"	一(51.0)…
Table 4: Results obtained with DQNeMMC after different amounts of experience.
Gravitar
OOM 10M 20M 30M MM e<JM βOM 70M 80M 9 OM IoOM
Num. Frames
Venture
Figure 4:	DQNeMMC+SR and DQNeMMC learning curves in the Atari 2600 games used as testbed. The
curves are smoothed with a running average computed using a window of size 100.
13