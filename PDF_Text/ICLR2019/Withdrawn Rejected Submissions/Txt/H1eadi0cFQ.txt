Under review as a conference paper at ICLR 2019
Escaping Flat Areas via Function-Preserving
Structural Network Modifications
Anonymous authors
Paper under double-blind review
Ab stract
Hierarchically embedding smaller networks in larger networks, e.g. by increasing
the number of hidden units, has been studied since the 1990s. The main interest
was in understanding possible redundancies in the parameterization, as well as in
studying how such embeddings affect critical points. We take these results as a point
of departure to devise a novel strategy for escaping from flat regions of the error
surface and to address the slow-down of gradient-based methods experienced in
plateaus of saddle points. The idea is to expand the dimensionality of a network in
a way that guarantees the existence of new escape directions. We call this operation
the opening of a tunnel. One may then continue with the larger network either
temporarily, i.e. closing the tunnel later, or permanently, i.e. iteratively growing the
network, whenever needed. We develop our method for fully-connected as well as
convolutional layers. Moreover, we present a practical version of our algorithm that
requires no network structure modification and can be deployed as plug-and-play
into any current deep learning framework. Experimentally, our method shows
significant speed-ups.
1	Introduction
Training deep neural networks involves the minimization of a non-convex, (piecewise) smooth
error function defined over a high-dimensional space of parameters. Such objectives are most often
optimized by stochastic gradient descent or variants thereof Duchi et al. (2011); Zeiler (2012);
Kingma & Ba (2014). One of the main difficulties occurring in this minimization problem is caused
by the proliferation of saddle points, often surrounded by flat areas, which may substantially slow
down learning Dauphin et al. (2014). Escaping plateaus around saddle points is of critical importance
not only to accelerate learning, but also because they can be mistaken for local minima, leading to
poor solutions if learning were to be stopped too early. Recent approaches to analyze and deal with
this challenge are often based on adding noise, e.g. Jin et al. (2017).
Here, we pursue a different philosophy. Inspired by the seminal paper of Fukumizu & Amari (2000),
we propose to expand a network by creating an additional hidden unit or filter, whenever optimization
is slowing down too much. This unit is obtained by a simple cloning process of a feature or feature
map, combined with a fast to execute optimization that scales outbound weights. The latter explicitly
aims at opening-up an escape direction by maximizing the gradient norm in the expanded network.
In particular, we exploit this ability to devise a method to escape saddle points, bad local minima and
general flat areas in neural network optimization, which we call Tunnels. After taking a small number
of update steps in the expanded network, we can decide to project back to the original network
architecture. The hope is that the flat area or bad local minimum will have been left behind.
Our contributions are as follows:
•	We extend the network embedding idea of Fukumizu & Amari (2000) to deep networks and
CNNs.
•	We show that in contrast to the case of a three-layer perceptron, the newly constructed
network has a non-zero gradient, even if the original network did not.
•	We derive a closed-form solution for cloning a unit or feature map to maximize the norm of
this gradient.
1
Under review as a conference paper at ICLR 2019
•	We develop a practical version of our algorithm that requires no network structure modifica-
tion and can be deployed as plug-and-play into any current deep learning framework.
•	We show experimentally that our method escapes effectively from flat areas during training.
In Section 2, we provide background, motivation, and explain in detail how to open the tunnel by
solving an interposed optimization problem. We discuss the fully connected case as well as the
practically relevant case of convolutional layers. In Section 3, we describe how to close the tunnel, i.e.
how to project back the expanded network onto one of the original dimensionality. We also propose a
practical version of our idea that does not require an explicit modification of the network structure,
based on a well-chosen reorganization of the weights. Finally, experiments and related work are
presented in Sections 4 and 5 respectively.
2	Tunnel Opening
2.1	Reducible Networks
Feedforward neural networks can be defined in terms of a directed acylic graph (U, E) of computa-
tional units. Concretely, we assume that each unit u ∈ U is equiped with an activiation function σu
and a bias parameter bu ∈ R. Moreover, each edge (u, v) ∈ E has an associated weight wuv ∈ R,
where we implicitly set wuv = 0 for (u, v) 6∈ E.
We can interpret a network computationally by assigning values xu to source or input units and
propagating these values through the DAG using local computations with ridge function xv =
σv(bv + Pu wuvxu) all the way to the sink or output units. Ignoring values at intermediate, hidden
nodes, a network thus realizes a transfer function F(∙; {w, b}) : Rn → Rm, where n and m denote
the number of input and output nodes, respectively. We call two networks input-output equivalent, if
they define the same transfer function.
It is a fundamental question whether the parameterization {w, b} → F(∙; {w, b}) is one-to-one and,
if not, what the sources of non-uniqueness are. As investigated in the classical work of Sussmann
(1992), one source of non-uniqueness can be tied to specific properties of the parameters, which make
the network reducible to a smaller network. There are precisely three such conditions on hidden units,
which do not depend on special properties of the activation function1:
Inconsequential unit u0	∀v : wu0v = 0	(1a)
Constant unit u0	xu0 = const	(1b)
Redundant units u0 , u	xu0 = ±xu	(1c)
(1a) says that xu0 is ignored downstream as all outbound weights are zero. Note that this means, that
the inbound weights can be chosen arbitrarily, in particular we may transform wu0v 7→ 0 (∀v), thus
converting u0 into an isolated node, without altering the network’s transfer function. (1b) implies
that each child v of u0 can effectively be disconnected from u0 via the parameter transformation
(wu0v, bv) 7→ (0, bv + wu0vxu0), rendering u0 inconsequential. Finally, (1c) permits the invariant
parameter transformation (wu0v, wuv) 7→ (0, wuv ± wu0v), which reconnects all children of u0 with
u instead, again rendering u0 inconsequential.
A network is reducible, if at least one of its hidden nodes is reducible, otherwise it is irreducible. As
shown rigorously in Sussmann (1992) for three-layer perceptrons with a single output, irreducible
networks are minimal, i.e. they are not input-output equivalent to a network with fewer hidden units,
and the remaining unidentifiability can be tied to a simple symmetry group (e.g. permutations and
sign changes of hidden units).
2.2	Network Expansion
We are interested in embedding smaller networks into larger ones by adding a single hidden node,
such that the transfer functions remains invariant. If the smaller network is irreducible, then by the
above result, the new unit will have to be reducible. This tells us that such extensions can only be
obtained through reversing the operations in (1).
1Further redundancies may be introduced by specifics of σu .
2
UlIderreVieW as a COnferenCe PaPer at ICLR 2019
TabIerParameter transformationSfOr IIet—
WOrk extension.
一 CASE 一
SMALL - EXTENDED NET
3	g	2
		N
Il	Il o厉 o厉	I Il Il O O	I Il Il o o
⅛ 、 T T T T ψ › ψ g ɪr疑	%飞 ⅛ 、 m H 京O	%飞 ⅛	、 Il	m O石
ASSUme that WehaVe an error function ε ..号
TabIe--Consequences OfnetWOrk extension
on gradient
一 CASE 一 SMALL 一 EXTENDED NET
	3
广 Il o	治 S Il o
广 m 更 Q g Q ɪ 2 m 石 I	m 更 Q g Q S H W m 石
→Finducing a risk 力(F)UEs^x))L
defined empirically in PmCtiC∙We COlISider the Set OfCritiCal Po5'ts Of a IIetWOrk Via the COndition
v{f b}力(^∙~7-)- HFukumizu 济 Amari (2000) have investigated how the CritieaI POts Of
a network change after adding a reducible UniL One Ofthe main results being that a global minimum
Ofthe SmIer network Can induce affine SUbSPaCeS Oflocminima and SaddIe POmtSin the extended
network，ThiS anaIySiS OllIy holds for the SPeCiaICaSe Of a SiIIe downstream (LOUtPUt) unit.
IIl generaly embedding a Sma=er IIetWOrk in anpuoutput equivalent larger Olle may introduce
directions Of IIOl!—vanishing error derivativeeven at CritiCaI pointCritiCal points Of a SmalIer
IIetWOrk are IIOt guaranteed to remain CritiCaI POts after extensILThiSiS the motivation Of Why
OIle Can POtentiany Create escape tumιels from regions Ilear PoOr CritiCal POts by SUCh operation
SPeCifiCanyyWe have the OPtiOnS to add a node U; (A) With POPiNQS OUtbOUnd WeightOr (B) With
non—zero inbound weightS (C) byOning it from an existing Unit U and re—weightmg its OUtbOUnd
WeightS ∙ Forma=y=his CaIl be realized by ParametertranSfOnnatnas descbed2Table L
Let US first State What CaIl be Said about the PartialderiVatiVeafter a IleW Ullit has been added∙In
a DAG OIle CaIl COmPUte aau}ure for Hxed inputs in three steps-S COmPUtiIlg all activations 七七
Via forward PrOPagation (ii) COmPUtmg deltas≡ as/a七 Via backpropagation from Sink nodes to
SOUrCe IlOdeS and (Ui)a IOCaI application Ofthe ChaiIl ruie
(2)
ThiS result--!! the SitUation described in TabIe 2 for the activation OfmboUlld (I) S OUtboUlld (π)
Weight
IntUitiVely speakby activating the OUtbOUlld Weightthe unit is no IOlIgerinCOlISeqUentiaI and
the inbound WeightS mattery whereas by activating the input WeightWhat WaS a COIIStant (and thus
useless) node befortumna SeleCtiVe and POtentiIy USefUI feature for downstream COmPUtatn
ItiS not CIear how to best initialize the new WeightOne OPtiOn is to ProCeed in the SPirit Of methods
SUCh as BreimaIl (1993) and more broadIy boosting algorithms FriedmaIl et al，(2000)； BUehlmaml
(2006L WhiCh iterativy Ht a residulosamping1 S SOme Of the existing parameterHOWeVeL
this SeemS (i) COmPutatnly UnattraCtiVe On PreSent—day COmPUterand (ii) PUrSUeS the PhiIOSOPhy
OfinCremental IearnWhereaS OUr ObjeCtiVe is IIOttO greedily grow the IIetWOrk unit by UiliL but
mthertemporalIy OPeIl an escape tuɪmeL
2∙3 NoDE CLoNlNG
-∙rsL we IOOk at a motivat5'IIal example Of an existing IIOde U With two ChMeIl IIOdeS Ff-The
backpropagatIIfOnnUyields , U <⅛ + b-where δR H R 合 UC ?ζQ m J 2τhis SPHtS the
effect that U has On the errorVia the COmPUtationPathSleading through C a=d g respectively The
Partiderivative Of an inbound Weight from Some Parent七 Of U in the originnetwork is then SimPIy
am
FPU
(3)
R m 毋 expresses rhe fasrhar a ParamereriS UnCOnSrra5'ed∙
Under review as a conference paper at ICLR 2019
Let us define the shortcuts api := E[δui x0u xp]. At a critical point it holds that ap1 + ap2 = 0,
i.e. ap := ap1 = -ap2. If we clone u into u0 (case (C) in Table 1), then
E
∂E
∂wpu
(λ1 - λ2)ap = -E
一 ∂E 一
βWpu0 _
(4)
In particular, we can see that by choosing λ1 = λ2 we remain at a critical point and that the
contributions to the squared gradient norm are maximal for λ1 = 1 - λ2 ∈ {0, 1}. This holds
irrespective of the value of aw and hence irrespective of the chosen upstream unit p. Note that we
could consider increasing weights even further by choosing λ1 > 1 and λ2 < 0 (or vice versa).
However this would artificially increase the gradient norm by making outbound weights of u and u0
arbitrarily large.
In the general case, where u has in-degree K and out-degree L, we can use the same quantities api as
above, which we can summarize in a matrix A = (api) ∈ RK×L. We can now define an objective
quantifying the contribution of the two modified units u and u0 to the squared gradient norm obtained
after cloning:
dWpu ]+ e
H(λ) = E
p
一 ∂E 一
βWpu0 _
which can also be written as
H(λ) =λTATAλ+(1-λ)TATA(1-λ).
(5)
(6)
Note that at a critical point, We have that the columns of A add UP to the zero vector Ei a∙i = 0, in
which case the above quantity simplifies as H(λ) = 2λT AT Aλ.
At a critical point, this is a convex maximization problem Which achieves its minimal value of 0 at
any λ ∈ R1. Maximizing H over the unit hyper-cube Will in many cases degenerate to an integer
problem, Which may not be fast to solve. Also, We generally prefer to not make too drastic changes
to the netWork, When opening the tunnel. Hence, We pursue the strategy of taking some λ0 ∈ R1 as a
natural starting point and considering λt = λ0 + tλ*, where λ* is a unit length principal eigenvector
of A>A. This is equivalent to maximizing H over the sphere of radius t, centered at λ0. We choose
λ0 = 11 as it minimizes H over R1 in the general case, i.e. at a non-critical point. Note that
H(λt) = (tρ)2 , where ρ is the largest eigenvalue.
The tunnel opening we propose in order to escape saddles therefore consists in cloning an existing
unit u into a new unit u0 , while assigning them the outbound weights of u respectively rescaled by λt
and 1 - λt, where t > 0 is a hyperparameter controlling the norm of the newly obtained gradient.
2.4 Filter cloning
A particular instance of neural networks used in practice includes convolutional neural networks.
Since such networks may be very deep and are often parametrized by millions of parameters, their
optimization can be challenging. In order to implement our proposed method for these networks, one
needs to adapt the convex optimization problem of Eq. (6) and find the correponding matrix A.
We consider here cloning a filter of a convolutional layer followed by a non-linearity and either a
convolutional or fully-connected layer. We derive computations in the first case, the second being
similar and presented in appendix A.
Convolution between single-channel image x and filter K is defined by
[x?K]i,j =	[K]rs [x]r+i,s+j,	(7)
r,s
where Hij denotes the pixel at the ith row and jth column.
A convolutional layer with pointwise non-linearity computes the following activation:
[xv]ij = σv (bv +	[xu ? Kuv]ij),	(8)
u
4
Under review as a conference paper at ICLR 2019
where u is the channel index. By simple application of the chain rule, we have
∂E	∂ E	∂[Xv ]rs
d [Kpu]ij	V,- d[xv]rs d[Kpu]ij
By a well-chosen chain rule we now have
d [xv]rs _、、 d[xv]rs	d[xu]i0 +r,j0 + s _ ∖ ' ∣^ K 1 r 0 ^∣ d [xu]i0+r,j0+s
∂ [Kpu]ij = J ∂ [Xu ]i0+r,j0 +s ∂[Kp^ij =红[Uv ]i0j0[xv ]rs ∂[Kpu]ij
i ,j	i ,j
(9)
(10)
Like before, let Us define the shortcuts [δv]『§ := ∂E/∂[xv]rs, and
a(pij),(vi0j0) := E
[Kuv]i0j0 X [xv]rs[δv ]rs d [⅛+]jj+s
(11)
Let us summarize these in a matrix A = (a(pij),(vi0j0)) where (pij) indexes a row and (vi0j0) a
column. Now, cloning a filter U into u0 would lead to the substitutions [Kuv]i，j，J λ(vi0j0)[Kuv]i0j0
and [Ku0v]i0j0 J (1 一 Xgojo))[Kuv]i0j0 for some scaling vector λ, yielding:
E
一 ∂E 一
.∂ [Kpu]ij_
(Aλ)pij and E
一 ∂ E 一
∂ [Kpu0 ]ij_
(A(1 - λ))pij .
(12)
The contribution of the two modified filters u and u0 to the squared gradient norm obtained after
cloning is again given by
H(λ) =λTATAλ+(1-λ)TATA(1-λ),
and the same analysis holds.
(13)
3	Tunnel closing
3.1	Averaging inbound, summing outbound weights
One can simply go back to a network of the original size by deleting the newly added unit (resp. filter)
u0, replacing the inbound weights wpu of the previously cloned unit u by the average (wpu + wpu0)/2
and its outbound weights wuv by the sum wuv + wu0v. Note that closing the tunnel in this way just
after opening it will leave the original network unchanged, but we aim to perform a few gradient steps
between opening and closing, in which case this heuristic does not guarantee function preservation.
3.2	Practical closing-opening via weight reorganization
Instead of opening the tunnel by adding a new unit (resp. filter), and then closing it later, we propose
a weight reorganization bypassing the network structure modification. The idea is the following:
select two units u1 and u2 for closing, and a unit u to clone. Start by closing u2 on u1 by averaging:
wpu1 7→ (wpu1 + wpu2 )/2,	wu1 v 7→ wu1v + wu2v	(14)
and then open by cloning u using the spot ‘left free’ by u2:
wpu2 7→ wpu ,	wu2v 7→ (1 一 λv )wuv ,	wuv 7→ λv wuv	(15)
for all v, p.
This method has the benefit of requiring no structural modification, making tunnels easier to use in
practice.
4	Experiments
In the following experiments, we train a variety of CNNs on standard image classification datasets.
We then test variations of our method for escaping plateaus in optimization. In stochastic settings,
we repeat all experiments ten times and plot mean and one standard deviation over the repetitions.
Detailed experimental setup information as well as full length runs can be found in the Appendix.
5
Under review as a conference paper at ICLR 2019
4.1	Adding Filters to a single hidden convolutional layer
In order to demonstrate the effectiveness of our method for escaping plateaus, we start out on a small
toy CNN containing a single hidden convolutional layer with 8 filters and a sigmoid nonlinearity,
followed by a fully connected softmax layer. The network is trained on the MNIST dataset. Due to
the small size of our network, we are able to employ Newton’s method to reliably find flat areas in the
optimization landscape. Once a flat area has been found, we use SGD to train the network for the rest
of the training.
At step 50, we perform tunnel opening and add 2 filters to the hidden layer. The same 2 filters are
removed again when we perform tunnel closing at step 300. To stay in accordance to our theory,
we restrict ourselves to the most basic building blocks of neural networks. This means that we use
a straightforward 2D convolution, followed by a sigmoid nonlinearity. Notably, we do not employ
tricks such as batch normalization, pooling or residual connections.
Results are shown in Figure 1. Note the immediate drop in loss when we add the filters. Even after
we close the tunnel and remove the filters again, the model continues to remain unstuck, therefore we
conclude that we have successfully escaped the flat area. Also note the comparison between adding
filters using the proposed λt versus adding filters in a random fashion (using a randomly sampled
λ with equal magnitude as λt), which provides no improvement. Further, we observe empirically
that we can find and get stuck in flat areas with equal ease even when using networks of initially
higher capacity, i.e. as if the tunnel had been open from the beginning of training. This suggests
that the occurrence of flat areas is not merely a problem of network capacity and that the targeted
manipulation of network structure can be used to escape such flat areas with success.
4.2	MEASURING THE IMPACT OF λt
Having developed a theory that includes as a hyperparameter the choice of t to control the norm of
the gradient after tunnel opening, we investigated the influence of this parameter in a setting that
allows us to eliminate stochasticity by computing gradients over the entire dataset. We used full batch
gradient descent to optimize the setup from Section 4.1 to convergence (negligible gradient norm,
no change in loss). We then applied tunnel opening using different choices of t to compute λt and
measured the resulting norm of the full batch gradient.
Results can be seen in Figure 1. Clearly, the choice of t has a direct influence on the resulting gradient
norm. Note the quadratic relationship between t and the gradient norm as well as the additive effect
of the number of filters added, both as predicted by our theory. We also measured the total loss and
could confirm that it remains unchanged after tunnel opening, irrespective of the choice of t, again as
our theory predicts.
Figure 1: Left: Training loss of a single-hidden-layer CNN on MNIST. Blue: Original network stuck
in a flat area. Green: We add two filters at step 50 using our tunnel opening technique and we remove
them again at step 300. Orange: We add and remove the same two filters, but instead of our gradient
maximization technique, we add the filters in a random fashion. Right: Resulting full gradient norm
after tunnel opening in a fully optimized single-hidden-layer CNN on MNIST against the choice of
hyperparameter t. Each curve refers to a different number of tunnels opened (i.e. filters added).
6
Under review as a conference paper at ICLR 2019
4.3	Practical weight reorganization for a single hidden convolutional layer
Figure 2 shows the practical closing-opening version of our algorithm presented in Section 3.2, with
the same initial setup as in Section 4.1. As before, we use Newton’s method to find an appropriate
flat area and run SGD afterwards. At step 50, we perform weight reorganization on 2 filters in
the convolutional layer, which means we first use tunnel closing to merge 4 filters into 2, then we
perform tunnel opening to add back the 2 filters we lost in the process. We compare to two baselines:
Perturbed gradient descent (Jin et al., 2017) and NEON (Xu & Yang, 2017), both also after step 50.
As can be seen, the targeted weight reorganization method not only escapes from the flat area, but
also vastly outperforms both baselines as well as random weight reorganization (again, of equal
magnitude) despite not changing the network architecture, but merely modifying already existing
weights. This is further evidence that the success of our method can be attributed to the particular
form of weight manipulation and not simply to an increase in network capacity.
4.4	Tunnel opening in a deep neural network
Lastly, we want to show that it is also possible to apply tunnel opening to just part of a larger and
deeper network and still benefit its optimization. In addition, we would like to demonstrate the
applicability of our method in more realistic settings, thus we drop a few of our initial restrictions.
We train a CNN with 5 hidden convolutional layers of 64 filters each on the ImageNet dataset, using
max pooling, batch normalization and ReLU nonlinearities. Note that we apply tunnel opening only
to the last two convolutional layers, such that batch normalization and pooling, though applied in the
network, are not interfering with our derived closed-form solution.
Most importantly, we no longer use Newton’s method in order to find a saddle point. Instead, we
restart optimization from different random initializations until we find an initialization that robustly
gets stuck in a flat area. Further, to make our experiments closer to practical use, we no longer use
plain SGD, but train the network using Adam Kingma & Ba (2014). We perform tunnel opening at
step 5000, where we add a single filter to each of the last two hidden layers. After opening, we clear
the Adam and BatchNorm accumulation buffers and decrease the learning rate for the first 20 steps in
order to minimize unwanted warm-restart effects (this is equally done in the control runs).
Results are displayed in Figure 2. As can be seen, the random tunnel opening does not manage to
escape the plateau, whereas the proposed method does so successfully and faster than the baseline
methods. This shows that the method can be applied even in individual parts of deep networks in
order to benefit the optimization of the entire network.
Figure 2: Left Training loss of a single-hidden-layer CNN on MNIST. Blue: Original Network
stuck in a flat area. Orange: Random weight reorganization at step 50. Purple: Proposed weight
reorganization at step 50. Shaded areas represent one standard deviation around the mean of 10
repeated experiments. Right: Training loss of a five-layer CNN on ImageNet. Blue: Original
Network stuck in a flat area. Orange: Random tunnel opening at step 5000. Purple: Proposed tunnel
opening at step 5000. Shaded areas represent one standard deviation around the mean of 10 repeated
experiments. Baselines: Green: Perturbed GD (Jin et al., 2017). Red: NEON (Xu & Yang, 2017).
iteration
7
Under review as a conference paper at ICLR 2019
5	Comparison to related work
Our ideas are inspired by the work of Fukumizu & Amari (2000), which proposes different ways
to embed a single-hidden-layer neural network into a bigger one while preserving its output values
at any point. They further prove that such networks can always be added a hidden unit such that
each critical point of the smaller network would induce a myriad of critical points in the augmented
network. This increase in the quantity of critical points for bigger networks was rigorously analyzed
in the specific case of linear models with quadratic loss in Kawaguchi (2016), where the author proves
in particular that linear models are more prone to possessing degenerate saddle points with depth, and
generalizes his study to deep non-linear models under some independence assumption; Dauphin et al.
(2014) argue that such a proliferation of saddle points, often surrounded by flat areas, also called
plateaus, constitutes one of the main obstacles to training deep neural networks with gradient-based
algorithms, because of the incurred slow-down.
Different solutions have been proposed in order to cope with the saddle-point problem. In the case
of strict saddles, i.e. critical points where the Hessian has at least one negative eigenvalue, it has
been shown that the presence of noise in SGD would enable it to escape from these strict saddles in a
polynomial number of iterations Ge et al. (2015), while injecting some supplementary noise could
improve the escaping rate Jin et al. (2017); Xu & Yang (2017). Second order methods were also
designed specifically to escape from saddles Dauphin et al. (2014); Reddi et al. (2017); globalized
(or regularized) Newton methods such as Trust Region Conn et al. (2000) and cubic regularizations
Nesterov & Polyak (2006), as well as recent stochastic variants Kohler & Lucchi (2017), escape strict
saddles by computing second-order information, and Anandkumar & Ge (2016) propose a method
using third order derivatives to escape from degenerate saddles that are indistinguishable from local
minima with their first and second order derivatives. Lastly, Bengio et al. (2006); Bach (2017) show
that the optimization of neural networks can be posed as a convex optimization problem if the number
of hidden units is variable, thereby avoiding the problem entirely.
Although escaping from strict saddles seems very important, Sankar & Balasubramanian (2017)
show that gradient-based algorithms on deep networks tend to converge towards saddles with high
degeneracy. On the other hand, Chaudhari & Soatto (2017) argue that SGD run on deep neural
networks converges to limit cycles where the loss is constant but the gradient norm is non-zero.
Previous work has also investigated changing the neural network’s architecture while preserving its
functional mapping: Chen et al. (2015); Cai et al. (2018); Lu et al. (2018) provide methods for adding
hidden units or layers to trained neural networks. However, this is done in the context of transfer
learning and architecture search and none provides explicit control over the resulting gradient of the
expanded network.
To the best of our knowledge, our approach significantly differs from all existing work in the literature,
in that without adding any supplementary noise or making use of higher order information, we propose
to escape saddles by increasing the size of the network in a way that provenly gives a non-zero
gradient, while preserving the network’s output for any input. Further notice that our method has the
‘a priori’ potential to escape from bad local minima as well, since a tunnel can transport the network’s
weights to a completely different position in parameter space.
Conclusion and future work
Flat areas surrounding saddle points are believed to be a major problem in the optimization of deep
networks via gradient-based algorithms, as they can slow-down learning or be mistaken for local
minima. Developing methods to escape these saddles is therefore of crucial importance. We propose
a new and original technique to escape plateaus, that can either be used by adaptively modifying the
size of the network during training, or simply by reorganizing its weights if we are to avoid structural
modifications. We provided theoretical guarantees of creating a new maximal gradient-descent
direction while preserving the loss by opening a tunnel, and empirical evidence that both the structural
modification and the weight reorganization versions of tunnels can efficiently escape saddles during
training. Potential future work includes extending the technique to allow for batch-normalization and
pooling operators following the layer on which tunneling is performed.
8
Under review as a conference paper at ICLR 2019
References
Animashree Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points
in non-convex optimization. In Conference on Learning Theory, pp. 81-102, 2016.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research, 18(19):1-53, 2017.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pp. 123-130, 2006.
Leo Breiman. Hinging hyperplanes for regression, classification, and function approximation. IEEE
Transactions on Information Theory, 39(3):999-1013, 1993.
Peter Buehlmann. Boosting for high-dimensional linear models. The Annals of Statistics, pp. 559-583,
2006.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation
for efficient architecture search. arXiv preprint arXiv:1806.02639, 2018.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. arXiv preprint arXiv:1710.11029, 2017.
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge
transfer. arXiv preprint arXiv:1511.05641, 2015.
Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust region methods. SIAM, 2000.
Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, KyUnghyUn Cho, Surya Ganguli, and YoshUa
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. CoRR abs/1406.2572, pp. 2933-2941, 2014.
John C DUchi, Elad Hazan, and Yoram Singer. Adaptive SUbgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine Learning Research (), 12:2121-2159, 2011.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression: a statistical
view of boosting (with discUssion and a rejoinder by the aUthors). The annals of statistics, 28(2):
337-407, 2000.
Kenji FUkUmizU and ShUn-ichi Amari. Local minima and plateaUs in hierarchical strUctUres of
mUltilayer perceptrons. Neural networks, 13(3):317-327, 2000.
Rong Ge, FUrong HUang, Chi Jin, and Yang YUan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Kenji KawagUchi. Deep learning withoUt poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jonas Moritz Kohler and AUrelien LUcchi. SUb-sampled cUbic regUlarization for non-convex opti-
mization. In International Conference on Machine Learning, 2017.
JUn LU, Wei Ma, and Boi Faltings. Compnet: NeUral networks growing via the compact network
morphism. arXiv preprint arXiv:1804.10316, 2018.
YUrii Nesterov and Boris T Polyak. CUbic regUlarization of newton method and its global performance.
Mathematical Programming, 108(1):177-205, 2006.
Sashank J Reddi, Manzil Zaheer, SUvrit Sra, Barnabas Poczos, Francis Bach, RUslan SalakhUtdi-
nov, and Alexander J Smola. A generic approach for escaping saddle points. arXiv preprint
arXiv:1709.01434, 2017.
9
Under review as a conference paper at ICLR 2019
Adepu Ravi Sankar and Vineeth N Balasubramanian. Are saddles good enough for deep learning?
arXiv preprint arXiv:1706.02052, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Hector J Sussmann. Uniqueness of the weights for minimal feedforward nets with a given input-output
map. Neural networks, 5(4):589-593, 1992.
Yi Xu and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost
linear time. arXiv preprint arXiv:1711.01944, 2017.
Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv.org, December 2012.
10
Under review as a conference paper at ICLR 2019
A Filter cloning followed by a fully-connected layer
We consider here cloning a filter of a convolutional layer followed by a non-linearity and then a
fully-connected layer. Derivations are similar as in Section 2.4.
A fully connected layer with pointwise non-linearity, applied just after a convolutional layer
([xu]ab)u,a,b (where u denotes the channel and a, b the pixels) with pointwise non-linearity, computes
the following scalar activation:
xv = σv (bv +	[xu]ab [wuv]ab). uab	(16)
,, By simple application of the chain rule, we have	
∂E	∂E	∂xv	
d[Kpu] j	V dxv d[Kpu] j	(17)
By a chain rule again, we obtain	
dxv	X、	dxv	d[xu]i0j0	X λγ	O	o d[xu]i0,j0 ∂[KPU]j = ij0 ∂[χuo]i0jo ∂[KpU]j = jwu]ij0xv∂[KpU]j.	(18)
Like before, let Us define the shortcuts δv := ∂E/∂xv, and
a(pij),(vi0j0) := E
[wuv]i0j0xvδv
∂[Xu]i0j0
∂ [Kpu]ij.
(19)
Let us summarize these in a matrix A = (a(pij),(vi0j0)) where (pij) indexes a row and (vi0j0) a
column. Now, cloning a filter U into U would lead to the substitutions [wuv]i,j, J λ(vi,jo) [wuv]i，jo
and [wu0v]i0jo J (1 一 λ(vi0j0))[wuv]i0j0 for some scaling vector λ, yielding:
∂E	∂E
E ∂K~i- =(AX)Pj and E ∂fK^~i- =(A(I - λ))pij.	(20)
∂ [Kpu]ij	∂ [Kpu0 ]ij
The contribution of the two modified filters u and u0 to the squared gradient norm obtained after
cloning is again given by
H(X) = XT AT Aλ + (1 一 λ)T AT A(1 一 λ),
(21)
and the same analysis holds.
B Experimental Setup
All our experiments are implemented in Tensorflow3 and are run on a single consumer GPU, the
memory size and processing speed of which speed are currently the bottleneck of the proposed
method. When performing tunnel opening / weight reorganization, we accumulate the required
gradients during a pass over the entire dataset in order to stay consistent with the theory. We have
experimented with two approximations to this: First, one can compute the required gradients from
only a partial pass through the dataset and second, one can use moving averages during the iterations
before tunnel opening to accumulate an approximation to the required gradients. Since we are dealing
with flat areas, it is reasonable to assume that these moving averages over recent iterates would
give a good approximation to the quantities at the tunnel opening point. For our experiments, both
approximations perform as well as performing a full dataset pass up to a certain degree of stochasticity
and may be viable options in practice. However, even with stochastic approximations, the size of the
networks that the model can be applied to is limited by computational power. Further work will go
into developing computationally efficient implementations of our method.
We preprocess the used image datasets by subtracting the pixel means calculated over the entire
dataset. During training, we randomize the order in which we proceed to the dataset after each epoch
and we use a minibatch size of 256.
3Code to use our method will be made available upon publication.
11
Under review as a conference paper at ICLR 2019
Hyperparameters were chosen using grid search over the range of viable parameters and kept constant
during comparisons within the same setup. Shared parameters, such as learning rate, were determined
in a baseline run of the network (not stuck in flat areas), which also provided a baseline target loss to
be reached. The choice of when to perform tunnel opening was done to select a point when sufficient
steps without change of loss had been performed in order to ensure that the algorithm is stuck in the
flat area. We have experimented with changing the opening point to later in the plateau and observed
that the effect is the same: Optimization escapes the flat area after opening, irrespective of when
exactly the opening happens.
As a technical detail, when performing our algorithm, we use a standard implementation of SVD for
solving the required eigenvalue problem.
In the following, we describe the detailed setup for our individual experiments.
B.1	Adding Filters to a single hidden convolutional layer
We build a simple network with a single hidden convolutional layer with a sigmoid nonlinearity,
followed by a fully connected softmax layer. The convolutional layer contains 8 filters of size 5 × 5
in order to keep the network size small. We train the network on the MNIST dataset using SGD and a
learning rate of 1.0. For finding the initial flat area we use Newton’s Method with the full empirical
Hessian inverse and a step size of 0.1 for 20 steps. When opening the tunnel, we add 2 filters to the
initial 8, which we remove again when closing the tunnel. To keep our network structure simple, we
do not employ tricks such as batch normalization, pooling or residual connections.
B.2	Practical weight reorganization for a single hidden convolutional layer
The setup for this experiment is equivalent to the first one, except that instead of adding and removing
filters, we perform our suggested practical weight reorganization.
B.3	Tunnel opening in a deep neural network
For the last experiment, we change our setup to a more realistic scenario. Our network consists of 5
hidden convolutional layer, each with 64 filters and ReLU nonlinearities. This is followed by a fully
connected softmax layer with 1000 output units. We equip the first 3 layers with batch normalization
and max pooling of stride 2. We train on a scaled version of the ImageNet dataset (64 × 64 pixels)
using Adam Kingma & Ba (2014) with a learning rate of 0.00002. We find a flat area by exploring
different random initializations until we get stuck in a large flat area. We then re-use this initialization
for all experiments. However, a degree of stochasticity is provided by random shuffling of the dataset.
This shows that first, the flat area is a property of the network, not of the exact data order and second,
we get stuck (and are able to escape) in a reproducible manner. When performing opening, we add
a single filter to each of the two last hidden layers. Immediately after opening, we clear Adam’s
accumulation buffers and reduce the learning rate by a factor of 10 for 20 steps as a warmup procedure
for Adam. This is done both for the runs where we perform opening as well as during the control run.
Still, there remains a small increase in the loss function after we apply our algorithm. Empirically, we
observe that this increase vanishes when we use SGD after tunnel opening instead of Adam, which
is evidence that the increase is due to the warm-restart effect of Adam adapting to the new network
architecture.
C More experimental results
C.1 Symmetry breaking
We investigated the behavior of the network after applying our method with special regard to how
the symmetry between the original filter Ku and the copied filter Ku0 is broken. We measured the
distance kKu - Ku0 k2 experimentally for various settings of λt. According to our theory, using our
proposed λt , this distance should increase very rapidly after tunnel opening, because the flat area
is escaped quickly, while any other setting of λt should lag behind significantly. In the special case
where λt is a vector entirely composed of 2, the two units should be exactly the same and symmetry
should never be broken.
12
Under review as a conference paper at ICLR 2019
Results can be seen in Figure 3. As predicted by our theory, using our proposed λt escapes the flat
area quickly and the two initially equal filters diverge rapidly during optimization. Compare this to
the random (uniformly sampled) λt , which escapes only much later and more slowly, and thereby the
symmetry between the filters is broken more slowly. Note that the iteration scale for this experiment
is longer than the other plots, in order to show the final convergence of the filters to a common area
of distance from each other, so the initial speed difference is significant. Further note that any fixed
λt to a vector of all same values will break symmetry slightly slower than random λt , with the clear
excaption of λt = 21, which never breaks symmetry and the distance remains at zero. This confirms
our theoretical analysis.
Z--R— nV- əuue:ls-P Z-I
Figure 3: Distance between new filter u0 and filter u from which it was copied during tunnel opening
in MNIST experiment. Note that symmetry is broken faster using our proposed method, which is
due to the network escaping the flat area more quickly. Uniformly sampled λt , as well as fixed
λt escape more slowly and therefore break symmetry more slowly. Note that in the special case
Xt = 21, symmetry is never broken. Shaded areas are one standard deviation from the mean over 10
experiment repetitions.
C.2 Wall clock times
In Table 3, we report the run times of our full experimental runs (as displayed in Figure 4). As can
be seen, the overhead from the tunnel opening procedure is a one-time insignificant addition when
compared to the variability of compute times.
C.3 Classification accuracies
Though our work investigates purely the escape from plateaus in training set loss, we measured the
final classification accuracies of our obtained classifiers. Table 4 displays these. Note that we did not
find a difference between networks that had been optimized using our method and networks that had
not, except that the final accuracy is reached faster.
C.4 Tunnel opening at local minima
Since our theory is applicable at any point where the gradient vanishes, we could apply it not only in
flat areas, but also in local optima (e.g. see Section 4.2). We investigated this using various pretrained
architectures on the ImageNet dataset, including our own 5-layer CNN, but also variants of VGG
(Simonyan & Zisserman, 2014).
13
Under review as a conference paper at ICLR 2019
Dataset	Method	Wall clock time mean	(standard deviation)
MNIST Control	1420	(218)
Random	1317	(370)
Proposed	1318	(284)
Noisy SGD	1390	(236)
NEON	1367	(251)
ImageNet Control	31222	(2218)
Random	32812	(2429)
Proposed	31361	(2636)
Noisy SGD	31964	(2927)
NEON	32518	(2737)
Table 3: Wall clock time (mean and standard deviation) of our experimental runs in seconds.
Dataset	Layers	Filters per layer	Test set accuracy
MNIST	1	8	0.98
ImageNet	5	64	0.28
Table 4: Classification accuracies of classifiers used in our experiments.
In accordance with our theory, we observed that tunnel opening leads to an increase in gradient
norm, followed by a decrease in training loss, but we were unable to significantly increase the test set
classification accuracies of the provided classifiers. More work remains to be done on the importance
of switching between distinct local minima and their generalization properties.
C.5 Full experiment runs
Figure 4 provides plots for the full runs of the experiments listed in the main section. Note that all
optimization methods converge to the same loss (and the same test set accuracy).
14
Under review as a conference paper at ICLR 2019
Figure 4: Full experiment runs. Top Left: 1-layer MNIST CNN opening and closing. Top Right:
1-layer MNIST CNN weight reorganization. Bottom: 5-layer ImageNet CNN tunnel opening.
15