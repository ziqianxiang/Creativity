Under review as a conference paper at ICLR 2019
A Better Baseline for Second Order Gradient
Estimation in Stochastic Computation Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Motivated by the need for higher order gradients in multi-agent reinforcement
learning and meta-learning, this paper studies the construction of baselines for
second order Monte Carlo gradient estimators in order to reduce the sample vari-
ance. Following the construction of a stochastic computation graph (SCG), the
Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct es-
timates of arbitrary order gradients through differentiation. However, a baseline
term that serves as a control variate for reducing variance is currently provided
only for first order gradient estimation, limiting the utility of higher-order gra-
dient estimates. To improve the sample efficiency of DiCE, we propose a new
baseline term for higher order gradient estimation. This term may be easily in-
cluded in the objective, and produces unbiased variance-reduced estimators under
(automatic) differentiation, without affecting the estimate of the objective itself
or of the first order gradient. Importantly it reuses the same baseline function
(e.g. the state-value function in reinforcement learning) already used for the first
order baseline. We provide theoretical analysis and numerical evaluations of our
baseline term, which demonstrate that it can dramatically reduce the variance of
second order gradient estimators produced by DiCE. This computational tool can
be easily used to estimate second order gradients with unprecedented efficiency
wherever automatic differentiation is utilised, and has the potential to unlock ap-
plications of higher order gradients in reinforcement learning and meta-learning.
1	Introduction
Problems that have intractable stochasticity often give rise to objectives that are not directly differ-
entiable. In reinforcement learning, for example, the expected return involves an expectation over
the stochasticity induced by both the policy and the environment dynamics. As a result, the gradient
of this objective with respect to the policy parameters cannot be directly calculated. However, we
may construct Monte Carlo estimates of the gradient from samples, and then apply gradient-based
optimisation methods. In such problems, the score function trick (Fu, 2006) may be used to easily
define estimates of the first order gradients of the stochastic objective. However, to calculate these
gradient estimates in practice, we need to leverage the powerful toolbox of automatic differentia-
tion. To this end, Schulman et al. (2015) introduce the surrogate loss (SL) within the formalism
of stochastic computation graphs (SCG). Under single differentiation the SL produces an unbiased
gradient estimator for the first order gradient of the primary objective.
Estimating higher order gradients of these stochastic objectives is important in many settings. One
example is in multi-agent learning. In Learning with Opponent-Learning Awareness (LOLA) (Fo-
erster et al., 2018a), the expected return of one agent is differentiated through the learning step of
another agent. Another use-case is the combination of meta-learning with gradient-based meth-
ods applied to reinforcement learning (Finn et al., 2017; Al-Shedivat et al., 2017), as the objective
function is repeatedly differentiated through a hierarchy of learning processes. Lastly, higher order
gradients may be used directly for some optimisation algorithms, such as the quasi-Newton algo-
rithm (Wright & Nocedal, 1999).
Despite the success of higher order gradients in these applications, accurate and efficient estimation
of higher order gradients is challenging. This is due to the complexity of constructing the cor-
responding estimators, as well as their extremely high variance. It is therefore critical to develop
1
Under review as a conference paper at ICLR 2019
methods that produce low variance estimates of higher order gradients and are easy to use in practice
through automatic differentiation. One way to reduce variance of stochastic estimators is by intro-
ducing control variates (Paisley et al., 2012; Tucker et al., 2017; Grathwohl et al., 2017). The key
contribution of this work is the design of a variance reduction technique for higher order gradient
estimators based on the control variate framework and amenable to automatic differentiation.
Given an objective function with random variables, the naive approach is to derive the corresponding
higher order gradient estimators analytically. Al-Shedivat et al. (2017) use the score function esti-
mator repeatedly to obtain proper estimators for higher orders in a meta-learning setting. Foerster
et al. (2018a) combine this approach with Taylor expansion to generate higher order gradient estima-
tors in a multi-agent setting. However, this approach is difficult to generalise to arbitrary objective
functions, as well as being error-prone and incompatible with automatic differentiation.
Schulman et al. (2015) argue that the gradient estimate produced by differentiating an SL objective
may be treated as an objective itself, and a new SL can be built to estimate higher order gradients.
Unfortunately, as shown by Foerster et al. (2018b), this approach can produce incorrect higher order
gradient estimates because the cost terms are treated as fixed samples in the SL, and so it does not
maintain all necessary dependencies in the SCG after differentiation. Foerster et al. (2018b) pro-
pose the Infinitely Differentiable Monte-Carlo Estimator (DiCE) to estimate higher order gradients
correctly while maintaining the ease of use by leveraging automatic differentiation.
DiCE uses two methods to reduce the variance of its first order gradient estimates. First, it respects
the causal dependence of costs on stochastic events that can influence them, and second, it imple-
ments a simple baseline method. However, the higher order gradient estimates generated by DiCE
still have high variance, and their quality relies on a large number of samples.
In this paper, we design a novel baseline term for second-order gradient estimation based on DiCE.
This term can be easily included in the original objective without changing the estimate of the ob-
jective itself or of the first order gradients. Nonetheless, appropriate control variates for the higher
order gradients are produced automatically upon repeated differentiation of the modified objective.
In this baseline term, we include an additional dependence which properly captures the causal re-
lationship between stochastic nodes in an SCG. Using the DiCE approach, we are able to preserve
these dependencies through differentiation. We prove that adding our baseline term to the DiCE
objective will not affect the expected estimate of second order gradients. We explicitly derive the
second order gradients of the DiCE objective with our baseline term in order to demonstrate how it
introduces control variates for higher order gradient estimates. Additionally, we conduct a series of
numerical evaluations to verify the correctness of our baseline term and show its effectiveness for
variance reduction. We believe that our baseline term is an important missing component to make
DiCE an extremely powerful and valuable tool to access higher order gradients in reinforcement
learning and meta-learning.
2	Background
Suppose X is a random variable distributed as X 〜 p(x; θ), and f (x) is a deterministic function of
x. We assume that f (x) is independent of θ so that Vnf = 0 for n ∈ {1,2,... }. Suppose We have
the objective function L(θ) = Ex [f(X)] and we need to compute the gradients of the expectation,
VθEx[f(x)], in order to use gradient-based optimization methods. An unbiased gradient estimator
g is a random variable such that E[g(f)] = VθE[f(x)]. The score function estimator (Fu, 2006) is
an unbiased estimator given by
Ex [f(x)Vθ logp(x; θ)] = Vθ Ex [f(x)].	(1)
The control variates method is a variance reduction technique for improving the efficiency of Monte
Carlo estimators. A control variate is a function c(x) Whose value can be easily obtained, and Whose
expectation E[c(x)] is knoWn. Using the control variate, We can construct a neW random variable
z(f) = g(f) - c(x) + Ex [c(x)].	(2)
The expectation of the random variable z(f ) is
E[z(f)]=Ex[g(f)]-Ex[c(x)]+Ex[c(x)]=Ex[f(x)].
2
Under review as a conference paper at ICLR 2019
Consequently, if g(f) is an unbiased estimator of the gradient, so is z(f). However, if the random
variables g(f) and z(f) are positively correlated, the variance of z(f) can be lower than that of g(f)
(Grathwohl et al., 2017).
2.1	Stochastic Computation Graphs
A stochastic computation graph (SCG) (Schulman et al., 2015) is a directed and acyclic graph that
consists of three types of nodes:
1.	Input nodes Θ: an input node θ ∈ Θ contains parameters set externally. We may be
interested in the dependence of an objective function on these nodes, and will attempt to
differentiate the objective with respect to their parameters.
2.	Deterministic nodes D: a deterministic node d ∈ D is a deterministic function of its parent
nodes.
3.	Stochastic nodes S : a stochastic node w ∈ S is a random variable whose distribution is
conditioned on its parent nodes.
Additionally, cost nodes C can be added into the formalism of an SCG without loss of generality. A
cost node c ∈ C is a determinstic function of its parent nodes that produces a scalar value. The set
of cost nodes C are those associated with an objective function L = E[Pc∈C c]. In an SCG, (v, w)
is a directed edge that connects node v and non-input node w, where v is a parent node of w . The
notation V Y W means there is a path from node V to node w, i.e., node V influences node w.
2.2	DICE
In order to estimate higher order gradients correctly, Foerster et al. (2018b) propose the Infinitely
Differentiable Monte-Carlo Estimator (DiCE) within the formalism of SCGs. DiCE uses the magic
box (ZIaSa novel operator. The input to □ isa set of stochastic nodes W, and □ is designed to have
the following properties:
1.	0(W) - 1,
2.	Vθ□(W)=0(W) Pw∈w Vθ logp(w; θ).
Here means “evaluates to”, which is different from “equals to”, =, i.e., full equality including
equality of all derivatives. In the context of a computation graph, — denotes a forward pass eval-
uation. In contrast, the second property describes the behaviour of □ under differentiation. The
right hand side of this equality can in turn be evaluated to estimate gradients as described below,
or differentiated further. To achieve these properties, Foerster et al. (2018b) show that □ can be
straightforwardly implemented as follows:
0(W) = exp(τ - ⊥(τ)),
where τ = Pw∈W Vθ log p(w; θ) and ⊥ is a ‘stop-grad’ operator that sets the derivative to zero,
Vχ⊥(χ) = 0, as it is commonly available in auto-differentiation libraries. This implementation will
give us the required properties of the magic box operator □ in practice.
For a node W in an SCG, we use Sw to denote the set of stochastic nodes that influence the node W
and are influenced by θ, i.e., Sw = {s∣s ∈ S, s Y w,θ Y s}. Using □, the DiCE objective (Foerster
et al., 2018b) is defined as
LO = X 口 (Sc)c.	⑶
c∈C
Under repeated differentiation the DiCE objective generates arbitrary order gradient estimators (Fo-
erster et al., 2018b, Theorem 1): E[V∕LT — V/L, for n ∈ {0,1, 2,... }.
2.3	Variance Reduction with DiCE
L口 by itself already implements a simple form of variance reduction by respecting causality. In
gradient estimates, each cost node c is multiplied by the sum of gradients of log-probabilities of
3
Under review as a conference paper at ICLR 2019
only upstream nodes Sc that can influence c. This reduces variance compared to using the log joint
probability of all stochastic nodes, which would still create an unbiased gradient estimate.
However, Foerster et al. (2018b) offer additional variance reduction for first-order gradient estima-
tion by including a baseline
B(I) = X (I- 口 ({w}))bw,	(4)
w∈S
where bw 1 2 is a function of the set NONINFLUENCED(W) = {v∣w 幺 v}, i.e. the set of nodes
that does not influence w. Here bw may be chosen to reduce variance, and a common choice for
bw is the average cost-to-go, i.e. E[Rw |NONINFLUENCED(w))]. Here Rw = Pc∈C c, where
Cw = {c∣c ∈ C,w Y c}, i.e., the set of cost nodes that depend on node w.2 In order to maintain
unbiased gradient estimates, the baseline factor bw should be a function that is independent of the
stochastic node w. Greensmith et al. (2004) provide an overview of variance reduction techniques
for gradient estimators, including the use of this type of baseline. The baseline term Bg) can be
added to the DiCE objective to obtain
Lbl = L∏ + Br.	(5)
Note that % evaluates to the same value as LE because (1 一 0(W)) - 0.
3	Method
First, consider explicitly the effect of the traditional baseline on the first order gradient estimates.
The estimates without and with baseline are as follows (derivations given in Appendix A.1):
VθL0 - X RwVθ logp(w; θ),
w∈S
VθLb1 — X (Rw 一 bw)Vθ logp(w; θ).	(6)
w∈S
For each stochastic node w ∈ S, the term bwVθ log p(w; θ) works as a control variate to reduce the
variance of the term RwVθ log p(w; θ). To ensure the appropriate correlations, bw may be a function
trained to estimate E[Rw |NONINFLUENCED(w))]. As a result, VθLb is a first order gradient
estimator with lower variance than VθL0. Additionally, VθLb is unbiasedbecause E[VθBg)] — 0
(see Appendix A.1).
Note that in (6) we have omitted a term, Pc∈C Vθc, that arises when the cost nodes depend directly
on θ. In most use-cases this term will not appear, as the costs are sampled from an unparameterised
process, such as the unkown environment in reinforcement learning. This straight-through contri-
bution to the gradient estimate is also typically much lower variance than the contribution estimated
using the score function trick. Due to these considerations, we assume that the costs are independent
of θ in the remainder of this work, although the remaining terms for both first and second order are
derived in the appendix.
Next, We consider second-order gradient estimation using the objective Lb. The second order
gradient of the DiCE objective can be evaluated as follows (derivations given in Appendix A.2):
V2Lπ - X Rw Vp察 +2 X Vθ logp(w; θ)[ X RvVθ logP(V θ),
p^	p(w; θ)
w∈S	' w	w∈S	Lv∈S,wγv	」
V2 Lbl - X (Rw 一 bw) Vpw3 +2 X Vθ log p(w; θ)[	X Rv Vθ log P(V θ)l.⑺
亡 乙」	p(w; θ)	乙」	乙」
w∈S	w∈S	v∈S,wγv
Ib(NONINFLUENCED(W)) in (Schulman et al., 2015)
2We use the Rw notation to correspond with a return for readers familiar with reinforcement learning, a key
use case. The cost notation for nodes is kept to maintain consistency with Schulman et al. (2015).
4
Under review as a conference paper at ICLR 2019
The baseline objective function Lb still implements a partial variance reduction in the second order
gradient estimates, by providing a control variate for the first term in (7). However, the Rv in the
second term are not paired with suitable control variates. As a result, the variance of this term could
be extremely high. In fact, due to the nested summations over the high-variance Rv , this term can
dominate the variance of total gradient estimate. We explore this empirically in section 4, where
We observe that B(1) is of little use for reducing the overall variance of the second order gradient
estimates.
3.1	A Second Order Baseline
To substantially reduce variance for the second order gradient estimator, we propose a new baseline:
B(2) = X (1- 口({w}))∙ (1- 口(Sw))bw
w∈S0
(8)
Here S0 = {w∣w ∈ S, Sw = 0, θ Y w}, i.e., the set of stochastic nodes that depend on θ and at least
one other stochastic node. Note that B(2) - 0 because (1 一 □({w})) - 0 and (1 一 £3(Sw)) - 0,
and bw is the same as that used in B(1). The new DiCE objective function becomes:
Lg = L 口 + B(I)-B(2).	(9)
Since B(1) — 0 and B(2) — 0, L口，Lb, and Lb2 all evaluate to the same estimate of the origi-
nal objective. Further, all derivatives of our modified objective L窗 are unbiased estimators of the
derivatives of the original objective, that now contain suitable control variates for variance reduction.
We now show how this baseline term indeed leads to a lower variance while still being an unbiased
estimate for the higher order derivatives of our objective.
3.2	Bias and Variance Analysis
In the DiCE objective L口，for each cost node C ∈ C, the corresponding (Z)(Sc) reflects the depen-
dency of c on all stochastic nodes which influence it (and depend on θ). In contrast, the baseline
term B(1) only includes □({w}), considering each stochastic node W separately. This simple ap-
proach results in variance reduction for first order gradients, as shown in (6). However, the failure
to capture the dependence of stochastic nodes on each other in the simple baseline prevents it from
reducing the variance of the cross terms that arise in second order derivatives (the final term in (7)).
To capture these relationships properly, we include (Z)(Sw) in the definition of B(2), i.e., an addi-
tional dependence on the stochastic nodes that influence w. Use of the □ operator ensures that these
dependencies are preserved through differentiation.
To verify that our proposed baseline indeed captures these dependencies appropriately, we now
consider its impact on the gradient estimates. The first and second order gradients of the baseline
term B(2) can be evaluated as follows (derivations given in Appendix A.2):
Vθ B(2) - 0,
W)B(2) - 2 X Vθ logp(w; θ)[	X bv Vθ logp(v; θ)].	(10)
w∈S	v∈S,wYv
The first order gradient estimates remain unchanged: as V)B(2) — 0, V)LM evaluates to the same
value as V)Lb. The second order gradient estimate of our full objective, V2L^, is as follows:
V)Lb2 - X (Rw - bw) Vpw^ + 2 X V) log p(w; θ)[ X (RV - bv )V)log p(v; θ).
回J	z—p	p(w; θ)	z—z	z—z
w∈S	w∈S	v∈S,wYv
(11)
Control variates have been introduced for the terms in the second part of (11), when using our new
baseline term B(2). Rw and bw are positively correlated by design, as they should be for variance
5
Under review as a conference paper at ICLR 2019
reduction of the first order gradients. As a result, the estimator R%L^ could have significantly lower
variance compared with V2 Lb and ▽%L臼,as We verify empirically in Section 4.
Furthermore, we verify that our baseline does not change the expected estimate of second order
derivatives.
Theorem 1. E [V2Lb2] - V E [L].
Proof. First, we can prove that E[V2B(1)] - 0 and E[VθB(2)] - 0 (See Appendix). Since V%L口
is an unbiased estimator of V2 E [L] , i.e., V2Lξ — V2 E [L] , then:
E [VθLb2] = E [v2lπ] + E [v2b(I)] - E [v2b剂-V2 E [l].
□
Thus, V2L0 is an unbiased second order gradient estimator of the original objective E [L].
3.3	Reinforcement Learning
We now consider the particular case of reinforcement learning. Given a policy π, we can generate
an episode of horizon T :
τ = (s0, a0, r0, . . . , sT, aT, rT).
The discounted return at time step t is the discounted sum of future rewards, Rt(τ) = PkT=t γk-trt,
where γ ∈ [0, 1] is a discount factor. When the reinforcement learning problem is formalised as an
SCG, the cost nodes are the discounted rewards and the objective function is L = E[PtT=0 γtrt].
The corresponding DiCE objective function is:
T
LG = X 0(ato≤t) ∙ Ytrt,	(12)
t=0
where at0≤t is the set of all previous actions at time step t, i.e., at0≤t = {a0, a1 , . . . , at}. Clearly,
these are the stochastic nodes that influence the reward at time t. We choose the baseline b(st) to
be a function of state st; it must be independent of the action at. In particular, we choose b(st) =
γtV(st), where V(St) is an estimate of the state value function Vπ(St) = E[Rt∣st]. First order
variance reduction may now be achieved with the baseline term:
T
B(I)= X(1- 口(at))b(st).	(13)
t=0
To reduce the variance of the second order gradient estimators, we can use our novel baseline term:
T
B(2)= ^X(1 - M(Ot)) ∙(1 - s(at0<t ))b(st).	(14)
t=1
These baseline terms can be added to our original objective. As in the general case, the correspond-
ing DiCE objectives with baselines are Lb = Ln + B(1) and L2 = Lm + B(1) 一 B(2).
In □(ato<t), we need to have strict inequality t0 < t, which captures the causality from all previous
actions. The agent is able to look backward at its past actions but excludes its current action. Since
there is no previous action at t = 0, the summation runs from t = 1 to t = T . It is essential to
exclude the current action at at time step t for variance reduction because the baseline b(St) must be
independent of the action at to remain unbiased. Note that this does not leave a term in the gradient
estimate without a control variate: the “diagonal” term corresponding to only the action at t = 0 is
already addressed by the second derivative of B(1) and appears in the first term in (11).
6
Under review as a conference paper at ICLR 2019
4	Experiments
First, We numerically verify that DiCE with our new baseline term Lb can generate correct esti-
mators of the Hessians in an SCG using a set of randomly initialised fixed policies in the iterated
prisoner’s dilemma.
In this setting, two agents play the game of the prisoner’s dilemma iteratively. At each round, there
are two possible actions for each agent, which are Cooperate (C) and Defect (D). As a result, there
are four possible outcomes, CC, CD, DC, and DD at each round, which are the observation at the
next time step. The payoff matrix of this game is given in Figure 1.
Multi-agent DiCE. The objective function for
agent i is L = EPT=O γtr] The per-agent DiCE
objective Lh is a simple extension of (12), replacing
rt by the per-agent reward rti , and at by the joint ac-
tion atj∈{1,2} . For correct higher order gradients it
is essential to consider the dependence of the reward
on the actions of both agents in this way. We will re-
quire per-agent baseline factors bi (st ) to then form
per-agent baseline terms BFI) and Bi,⑵ in analogy
with (13, 14). Again, the single-agent action at each
timestep is replaced by the joint action.
C
-1
-1
-3
0
D
0
-3
-2
-2
Figure 1: The payoff matrix of prisoner’s
dilemma. Numbers in a cell correspond
to the utilities of the player with the same
colour.
C
D
Using DiCE, the dependencies between the returns
and parameters of the two agents are accounted for, and first and second order gradients can be
estimated efficiently using automatic differentiation. We will test and compare the performance of
L^,i and Lb2τ in second-order gradient estimation.
Foerster et al. (2018a) derive the value function of IPD analytically, which we use as ground truth
to verify the correctness of our estimator. Figure 2(b) shows that we can obtain correct second-
order gradient estimates using our baseline term. Comparing to Figure 2(a), our novel baseline
term dramatically improves the estimation of second order gradients. In the original DiCE paper
(Foerster et al., 2018b), a sample size of 100k is required to obtain second order gradient estimates
with correlation coefficient 0.97. After formulating our baseline term, the required sample size is
reduced to 1k, a reduction by two orders of magnitude. Figure 3 shows the correlation coefficients of
the exact Hessian and the estimated Hessian using different sample sizes. These results demonstrate
that our baseline term is important for estimating second order gradients accurately and efficiently
when using DiCE.
：：l ∏N	_I_
O	25	50	75 IOO
Hessian dimension
(a)	Flattened Hessian (green) of L2,1. The correla-
tion coefficient is 0.29.
-2
■30	25	50	75	100
Hessian dimension
(b)	Flattened Hessian (blue) of Lb2,1. The correla-
tion coefficient is 0.97.
Figure 2: Flattened true (Red) and estimated Hessian of agent 1 for the iterated prisoner’s dilemma.
The sample size is 1000.
7
Under review as a conference paper at ICLR 2019
LOLA-DiCE. In LOLA-DiCE (Foerster et al., 2018b) agents differentiate through the learning
step of other agents, using the DiCE objective to calculate higher order derivatives:
T
L1(θ1, θ2)LOLA = Eπθ1,πθ2+∆θ2(θ1,θ2)[	γtrt1],
where δθ2∕i,^ = α2Vθ2 E∏θ1 ,∏θ2 [PT=0 γtr2] and α2 is a step size. Training details are pro-
vided in Appendix A.3.
8 6 4 2
SeieiS
UoQB-3」」OU
“"O	5000	10000
Sample size
Figure 3: The correlation coefficients of the ex-
act Hessian and the estimated Hessian generated
from the multi-agent DiCE objective function
with (orange) and without (blue) the second or-
der baseline term B(2),i. The error bar shows
the standard deviation.
-1.2
-1.4-
-1.6-
u∙lns∙l dφβ∙Lφdφ62φ><
-1.8
0	200	400	600
Iterations
Figure 4: The performance of the LOLA-DiCE
algorithm on the IPD with (blue) and without
(red) the new second order baseline. Shading
indicates the error of the mean.
Figure 4 shows the performance of LOLA-DiCE with and without our second order baseline. We
find that without the second order baseline agents fail to learn, resulting in an average per-step
return of around -1.6, close to that of a random policy which achieves -1.5. Using a two times
smaller batch size (32 vs 64), performance is comparable to what was achieved in the original work.
However, our results using the first order baseline are much worse than what is reported in the
original work, albeit at a smaller batch size. In communication with the authors we established
that those results were produced by making the rewards at each timestep zero-mean within each
batch, rather than relying on the first order baseline. In settings where the value function is mostly
independent of the state, which happens to be the case in the IPD with a large γ, this simple trick can
produce variance reduction similar to what we achieve with our second order baseline. However,
this ad-hoc normalisation would fail in a setting with sparser rewards or, in general, in settings where
the value function strongly depends on the current state. 5
5 Conclusion
Recent progress in multi-agent reinforcement learning and meta-learning has lead to a variety of
approaches that employ second order gradient estimators. While these are easy to construct through
the recently introduced DiCE objective (Foerster et al., 2018b), the high variance of second order
gradient estimators has prevented their widespread application in practice. By reusing the DiCE
formalism, we introduce a baseline for second order gradient estimators in stochastic computation
graphs. Similar to DiCE, this baseline is automatically constructed from user-defined objectives us-
ing automatic differentiation frameworks, making it straightforward to use in practice. Our baseline
does not change the expected value of any derivatives. We demonstrate empirically that our new
baseline dramatically improves second order gradient estimation in a multi-agent task, reducing the
required sample size by two orders of magnitude. We believe that low-variance second order gradi-
ent estimators will unlock a large variety of reinforcement learning and meta-learning applications
in the future. Furthermore, we would like to extend the approach to deal with settings where the
costs depend directly on the parameters. Lastly, we are interested in extending our framework to a
baseline-generating term for any-order gradient estimators.
8
Under review as a conference paper at ICLR 2019
References
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel.
Continuous adaptation via meta-learning in nonstationary and competitive environments. CoRR,
abs/1710.03641, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
ICML2017, pp. 1126-1135, 2017.
Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 122-130. International Founda-
tion for Autonomous Agents and Multiagent Systems, 2018a.
Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktaschel, Eric Xing, and Shimon
Whiteson. DiCE: The infinitely differentiable Monte Carlo estimator. In Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1524-1533, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018b. PMLR. URL
http://proceedings.mlr.press/v80/foerster18a.html.
Michael C Fu. Gradient estimation. Handbooks in operations research and management science,
13:575-616, 2006.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David K. Duvenaud. Backpropa-
gation through the void: Optimizing control variates for black-box gradient estimation. CoRR,
abs/1711.00123, 2017.
Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research, 5:1471-1530, 2004.
John Paisley, David Blei, and Michael Jordan. Variational bayesian inference with stochastic search.
arXiv preprint arXiv:1206.6430, 2012.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems 28: An-
nual Conference on Neural Information Processing Systems, pp. 3528-3536, 2015.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, pp. 2627-2636, 2017.
Stephen Wright and Jorge Nocedal. Numerical optimization. Springer Science, 35(67-68):7, 1999.
9
Under review as a conference paper at ICLR 2019
A Appendix
A.1 First Order Gradients
DiCE Objectice.
VθLπ = X Vθ (□ (Sc) ∙ C)= X C ∙ Vθ口(Sc) + X H(Sc) ∙ VθC
=X c Q(Sc) X Vθ logp(w; θ)) + X 0(Sc) ∙ Vθc
c∈C	w∈Sc	c∈C
1(wγc)C ∙Vθ logp(w; θ) +	VθC
w∈S c∈C	c∈C
=X(X C) ∙ Vθ logp(w; θ) + X VθC
w∈S c∈Cw	c∈C
= X RwVθ logp(w; θ) + X VθC.
w∈S	c∈C
When the cost nodes does not depend on θ directly, we have VθC = 0. Thus,
VθL0 - X RwVθ logp(w; θ).
w∈S
Baseline Terms. For the first baseline term,
VθB(I) = — X bwVθ□({w})
w∈S
=—X bw0({w})Vθ logp(w; θ)
w∈S
—	bwVθ log p(w; θ).
w∈S
We can consider a single term in (15),
E[bw Vθ logp(w; θ)] = bw Xp(w; θ) RVp(w；θ)
w	p(w; θ )
= bwVθ	p(w; θ)
w
= bwVθ1 = 0.
According to the linearity of expectations, we have
E[VθBg)] - 0.
For the second baseline term,
VθB(2) = X bw h — (1 — □(Sw))Vθ0({w}) — (1 — 0({w}))VθC)(Sw)i
w∈S0
- 0.
Obviously, E[VθB(2)] - 0.
(15)
A.2 Second Order Gradients
DiCE Objective.
v2l 口 = X v2(0 (Sc) ∙c)
c∈C
=XC• V犯(Sc) + X 2vθc ∙ vθ□ (Sc) + X d(Sc) ∙ V2C .
l∈C-V--} l∈C-V---} l∈C-V-}
ABC
10
Under review as a conference paper at ICLR 2019
Next, we can evaluate terms A, B , and C,
A = X C C(Sc) [( X vθ logP(w； θ)) + X V2 logp(w; θ)i
c∈C	w∈Sc	w∈Sc
Vθ logp(w; θ) ∙ Vθ logp(w; θ) + X V2 logθ p(w; θ)]
w∈Sc
1 Xc[ X (Vθlogp(wθ))2 + 2 X X
c∈C	W∈Sc	W∈Sc V∈Sc,wYv
+2 Xc X X
Vθ logp(w; θ) ∙ Vθ logp(v; θ)],
c∈C	w∈Sc v∈Sc ,wYv
^{z
A2
}
A1 = ΣΣ1(wYc)c(Vθ log p(w; θ))2 + Vθ2 log p(w; θ)
w∈S c∈C
E ( E c) ∙ [(Vθ logp(w; θ))2 + V2 logp(w; θ)]
w∈S c∈Cw
Rw
w∈S
V2p(w; θ)
p(w; θ)
A2 =ΣΣ Σ IvYc ∙ Vθ logp(w; θ) ∙Vθ logp(v; θ)
c∈C w∈S v∈S,wYv
=X X (X c)∙Vθ logp(w; θ) ∙Vθ logp(v; θ)
w∈S v∈S,wYv c∈Cv
= X Vθ log p(w; θ)	X RvVθ logp(v; θ) ,
w∈S	v∈S,wYv
B = X 2Vθc ∙ 口(Sc) [ X Vθ logp(w; θ)]
c∈C	w∈Sc
1 X2Vθch X Vθ log p(w; θ)i,
c∈C	w∈Sc
C = E欧Sc) ∙V2c 1 £v2c
c∈C	c∈C
As a result, we have
V2Lπ 1 X Rw Vp等 + 2 X Vθ log p(w; θ)[ X Rv Vθ log p(v; θ)
p^	p(w; θ)
w∈S	w∈S	v∈S,wYv
+2XVθch X Vθ log p(w; θ)i +XVθ2c.
When the cost nodes does not depend on θ directly, we have Vθc = 0 and V2θc = 0. Thus,
V2Lπ 1 X Rw Vp察 +2 X Vθ log p(w; θ)[ X	Rv Vθ log P(V θ)
p^	p(w; θ)
w∈S	w∈S	v∈S,wYv
11
Under review as a conference paper at ICLR 2019
Baseline Terms. For the first baseline term,
V2B(1) = - X bw V20 ({w})
w∈S
—X bw□(at) [(Vθ logp(w; θ))2 + V2 logp(w; θ)]
w∈S
- X bw (Vθ logp(w; θ))2 + Vθ2 log p(w; θ)
w∈S
X b h (Vθp(w; θ))2 + Vθp(w; θ)
w[ p(w; θ)2	p(w; θ)
X b vΘP(W; θ
W∈sW P(W； θ) .
(Vθp(w; θ))2
p(w; θ)2
(16)
We can consider a single term in (16),
E[bw ：] = bw XP(w； θ) vP≡Γ
= bwV2θ X P(W； θ)
w
= bwVθ2 1 = 0.
According to the linearity of expectations, we have
E[V2B(1)] - 0.
For the second baseline term,
V2B(2) = X bw h - V20({w}) ∙ (1 - 0(Sw)) -(1- 0({w})) ∙ V2口(Sw)
w∈S0
+ 2fZ)({w})Vθ logp(w; θ) ∙ (Z)(SW) X Vθ logp(v; θ)]
v∈Sw
- 2 X bwVθ logP(W; θ)h X Vθ logP(v; θ)i
w∈S0	v∈Sw
=2
1(vYw)bw Vθ logp(w; θ) ∙ Vθ logp(v; θ)
v∈S w∈S0
= 2X X	bwVθ logP(W; θ)Vθ logP(v; θ)
V∈S w∈Sv ,vYw
=2 X Vθ logp(v; θ)[ X	Vθ logp(w; θ) ∙ bw].	(17)
v∈S	w∈Sv ,vYw
Next, we consider the expectation of a single term in (17)
E Vθ logP(v; θ)	bwVθ log P(W; θ) = E E Vθ log P(v; θ)	bwVθ log P(W; θ)v
= E Vθ logP(v; θ)	X	bw E V log P(W; θ)v
w∈Sv,vYw
=0
because E [Vθ log P(w; θ)∣v] = 0 where V Y w. According to the linearity of expectations, We can
obtain that E[V2B口] - 0.
12
Under review as a conference paper at ICLR 2019
A.3 Architecture and Hyperparameters.
We use a tabular policy and value function, initialised from a normal distribution with unit variance
and zero mean. The discount, γ = 0.96, and the episodes are truncated after 150 steps. Our
experiments use a batch size of 32, a learning rate of 0.05 for the policy, and a look-ahead step size,
α = 0.3. To allow for proper variance reduction we train two value functions, one for the inner
loop and one for the outer loop, which are pre-trained over 200 training steps. To ensure that the
value function closely tracks the value under the changing policy we carry out 10 training steps of
the value functions with a learning rate of 0.1 for each policy update.
13