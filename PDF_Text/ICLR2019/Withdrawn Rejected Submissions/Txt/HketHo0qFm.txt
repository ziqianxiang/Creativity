Under review as a conference paper at ICLR 2019
Hybrid Policies Using Inverse Rewards for Re-
inforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
This paper puts forward a broad-spectrum improvement for reinforcement learn-
ing algorithms, which combines the policies using original rewards and inverse
(negative) rewards. The policies using inverse rewards are competitive with the
original policies, and help the original policies correct their mis-actions. We have
proved the convergence of the inverse policies. The experiments for some games
in OpenAI gym show that the hybrid polices based on deep Q-learning, double
Q-learning, and on-policy actor-critic obtain the rewards up to 63.8%, 97.8%, and
54.7% more than the original algorithms. The improved polices are more stable
than the original policies as well.
1 Introduction
1.1	Motivation
Reinforcement learning, as one of the most crucial branch in machine learning area, has now been
broadly studied and improved thus become more and more powerful and accurate since DeepMind in-
troduced deep Q-learning Mnih et al. (2013). Recently, a couple of successors of deep Q-learning have
been developed and applied on many areas van Hasselt et al. (2016) Wang et al. (2016). Meanwhile,
the classical actor-critic algorithms Konda & Tsitsiklis (2000) played ideas from SARSA Rummery
& Niranjan (1994) and deep Q-learning as well.
In a traditional mathematical setting, suppose that we want to approximate the target stationary value
function Q*(s, a) with a fixed basis vi, so that the result function,
Q(s, a; θ) =	θivi(s, a),	(1)
i
minimizes the loss (Q*(s, a) - Q(s, a; θ))2.
Then the gradient descent optimization algorithm will help us finding the minimum of the above by
repeatedly using the following updating formula,
θt+ι = θt + α(Q* - Q(st, at； θt))V%Q(st, at； θt).	(2)
In Q-learning, one of the main challenge is that the target stationary value function Q*(s, a) in
equation 2 cannot be computed directly. To resolve this problem people needs to estimate Q*(s,a) in
equation 2. For example, in Q learning the following estimation function is used,
Q*(st,at) ≈ YtQ = Rt + YmaχQ(st+ι,b; θt).	(3)
YQ is also called target for the Q* value. With the different target network parameter θ0, deep
Q-learning uses,
Q*(st, at) ≈ YtDQN = Rt + γmaxQ(st+1, b； θt0).
b∈A
(4)
It is well understood that deep Q-learning has intrinsic overestimations Thrun & Schwartz (1993) and
the neural network introduces more significant error in the computation of the iterations van Hasselt
et al. (2016). Although double Q-learning van Hasselt et al. (2016) and its successors claimed some
improvements, the effectiveness is not good enough.
We confirmed that the estimation error is not neglectable and will occasionally leads to fluctuation.
For example, we tried deep Q-learning, double Q-learning, and on-policy actor-critic with neural
network for CartPole Barto et al. (1983) in OpenAI gym ope (a). (See figure 1) .
1
Under review as a conference paper at ICLR 2019
500 1000 1500 2000 2500 3000 3500 4000	0	500 1000 1500 2000 2500 3000 3500 4000	0	500 1000 1500 2000 2500 3000 3500 4000
(a) DeeP Q-learning for CartPole. (b) Double Q-learning for CartPole	(C) Actor-critic for CartPole
Figure 1: RL algorithms for CartPole. x-axis is the iterations of episodes and y-axis is the rewards of the
corresPonding ePisode. The goal of the game is to keeP the Pole standing on the cart as long as Possible. The
maximum number of stePs , i.e., the sum of the rewards, of each ePisode is 200.
Recall that the environment of CartPole returns 1 reward Point for each steP until the Pole is down or
the cart is out of range. The Player needs to Push the cart to the left or the right to keeP the Pole not
falling down and the cart in the range as long as Possible, i.e., getting more reward Points.
The exPeriment shows the unstability of these reinforcement learning algorithms. In figure 1a, the
Policy is degraded significantly and is quite unstable even after it continuously gets 200 Points. So do
double Q-learning (Figure 1b) and actor-critic (figure 1c). All exPeriments in figure 1 show that the
Policies degrade dramatically even after they reach the goal for a while. We cannot see the trend of
the convergence in 4000 ePisodes. The unstability means that more training may not result in better
Policies. Even worse, the users of these algorithms may be afraid of more training and do not know
when to stoP learning.
Based on these observation, we believe that it will be helPful to find a metric to test whether the
estimated Q* (YQ or YDQN) we used in Q-learning is too "risky". For such purpose, we introduced
inverse policy to measure the badness of a state and uses it to fix YQ , YDQN, or other targets.
We believe that a good state should not be too bad in the measure of inverse policy which measures
the badness of a state. In addition to use the optimistic estimation YQ or other targets, we also think
pessimistically and use inverse policy to find and fixes those "risky" estimation in equation 1.
Once the above tuition has been established, we successfully developed our hybrid policy and showed
that the overall performance and stability of RL algorithms can be improved dramatically. Also we
believe that this motivation can be further developed, see further works in section 5.
1.2	Our Approaches
In the traditional reinforcement learning, the agent gets rewards from the environment and uses the
rewards as the feedback in the algorithm. For a particular state, a traditional reinforcement learning
algorithm tries finding the action that leads to the maximum rewards in the future.
Let us change to the opposite perspective. What if we find the worst action for a particular state?
For CartPole, if we find the worst action, the other action must be the best. Hence, the alternative
approach is to learn the "worst" actions in this case. For more complicated action spaces, without
loss of generality, we can find out both the maximum and the minimum values because any value in
Q(s, a; θt ) is known in value function-based algorithms. If we learn the poor actions for greater Q
values, the minimum Q value approximately corresponds to the best action in the original environment.
The straightforward approach is using inverse rewards, i.e. negative rewards, to learn the poor
actions in the reinforcement learning algorithm. This is our basic idea called inverse policy, and the
corresponding Q value is,
Qt-+1(st,at) = Qt-(st, at) + α[-Rt + γmaxQt-(st+1,b) - Qt-(st, at)]	(5)
b∈A
We will discuss the details in section 2.1 and show that inverse policies are competitive with the
original algorithms in section 3. However, they are just the alternative approaches and are not
obviously better than the original reinforcement learning algorithms.
Combining the original reinforcement algorithm and its inverse policy, our next approach is to
leverage them to correct the mis-actions for each other. In this algorithm, we have two parameterized
value functions: Q(s, a; θ+) for the original Q values and Q(s, a; θ-) for the inverse Q values. We
2
Under review as a conference paper at ICLR 2019
use the inverse policy to help the original policy to choose the potential next steps, and vice versa on
the inverse policy side.
Eventually, we have the following hybrid policy. The policy chooses the action with the maximum
QH value at each step.
QH(st, at) = C(Q+(st, at), Q-(st, at)) ≈ C(Q(st, at； θ+), Q(st, at； θ-)),	(6)
where Q+ is the Q value for the original policy, Q- is the Q value for the inverse policy and C(q1, q2)
is a simple function that merges two values into one.
Note that the above improvement using hybrid policy is used for off-policy algorithms. For on-policy
algorithms like SARSA and on-policy actor-critic, the similar improvement is simpler. We use Q+
and Q- functions to approximate the Q value along the same trajectories. Then similar QH is used
to merge the values in order to make final decision. We will discuss this issue in section 2.3 and take
on-policy actor-critic as the example.
1.3	Contributions
We make the following contributions in this paper.
1.	To the best of our knowledge, we are the first one who use inverse rewards for reinforcement
learning and prove the convergence of the inverse policies. The policies with the inverse
rewards are competitive with those policies with the original rewards.
2.	We merge the original policies and the inverse policies as the hybrid policies. In the hybrid
policies, the inverse policies effectively correct many mis-actions generated by the original
policies, and vice versa. (See section 3.2)
3.	Both the inverse policy and the hybrid policy are broad-spectrum for both on-policy and
off-policy reinforcement learning algorithms. We have applied them on deep Q-learning,
double Q-learning, and on-policy actor-critic. They are also potentially applicable for the
policy gradient-based algorithms and other cutting-edge algorithms.
4.	Our evaluation shows that the hybrid policies are much better than the original policies. In
the experiments, the hybrid polices gain up to 63.8%, 97.8%, and 54.7% rewards more than
the original deep Q-learning, double Q-learning and on-policy actor-critic for some OpenAI
gym environments, respectively. Moreover, our improvements are much more stable.
2 Inverse Policy and Hybrid Policy
2.1	Inverse Policy
The basic idea of inverse policy in section 1.2 transforms the reward r to -r and uses -r for learning.
Recall equation 5. In practice, we use Q(s, a； θ-) to approach Q-(s, a):
Qt+ι(st,at) ≈ Q(st,at； θt+ι)
(7)
= Q(st, at； θt ) + α [-Rt + γ maxQ(st+1, b； θt ) - Q(st, at； θt )].
b∈A
And the updating strategy is,
θt+ι = θ- + α-(YQ- - Q(st, at； θ-))Vθ-Q(st, at； θ-)	(8)
YtQ- = -Rt + γ- maxQ(st+1, b； θt-).	(9)
b∈A
Like the original deep Q-learning algorithm Melo, the updating strategy in equation 8 and 9 converges
to Q- (proved in Appendix A). It follows that the optimal inverse policy can be obtained as:
∀s ∈ S, π-*(s) = argmax(-Q-*(s, a)) = arg min Q-*(s, a).	(10)
a∈A	a∈A
Note that the inverse policy is a broad-spectrum method for many reinforcement learning algorithms.
For example, we can use the following equations for double Q-learning,
YtDoUbbeQ- = -Rt + Y-Q(St+ι, argmax Q(st+ι,b; θ-); θ-)	(11)
b∈A
θt+ι = θ- + α-(YDoMeQ- - Q(st, at； θ-))Vθ- Q(st, at； θ-).	(12)
3
Under review as a conference paper at ICLR 2019
Similarly, the inverse policy for on-policy algorithms is much simpler as follows.
Θ-+1 = θ- + α-(-Rt + Y-Q(St+1,at+1; θ-) - Q(st, at； θ-))J- Q(st, at； θ-).	(13)
2.2	Hybrid Policy
We continue from the inverse policy to introduce the hybrid policy, which combines the original
and inverse policy to get a better one. Recall that one major problem of deep Q-learning is the
policy-degradation caused by the wrongly-estimated Q values in the learning process. The traditional
Q-learning is based on an assumption that the target Q value, e.g. Y Q as shown in equation 3, is a
good approximation of Q*, by following the operator max to find the best action in the next state
in the current Q function. However, in some cases sub-optimal actions are selected and result in
mis-estimated Q values. This kind of error is intrinsic and has been discussed in the work of Thrun &
Schwartz (1993)van Hasselt (2011).
In this paper, we suggest that the combination of the original deep Q-learning policy and the inverse
policy leads to a better one, i.e. selects better actions than each single one of the former both, since
some wrongly-selected actions of one Q function can be corrected by the other one.
Following this feature, the general idea of hybrid deep Q-learning algorithm is to combine the original
and the inverse Q values into hybrid Q value, which can help select the optimal actions for both the
learning and the policy.
The novel algorithm includes two sets of parameterized Q value functions, denoted as	Q(θ+)	(the
original Q function) and Q(θ- ) (the inverse Q function). For each function,	we compute	its	hybrid Q
value QH, which is influenced by its counterpart Q function,
Q+H(s, a) = C(Q(s, a； θ+), Q(s, a； θ-)),	(14)
Q-H(s, a) = C(Q(s, a； θ-), Q(s, a； θ+)).	(15)
The operator C(q1, q2) 1 is a simple function, which uses the value of q2 to adjust the value of q1 .
For both Q+ and Q- functions, QH is applied to determine the optimal action, both in the sampling
and in the updating rule. During the sampling and exploration, Q+ and Q- pick their actions by
maximizing QH values, as the following policies,
∀s ∈ S, π+H (s) = arg max Q+H (s, a), π-H (s) = arg max Q-H (s, a).	(16)
a∈A	a∈A
Meanwhile, in the updating rule of deep Q-learning, QH is applied to select the optimal action. The
computation of target Q value, YtQ, should be re-written for both Q functions,
YQ+ = Rt + Y+Q(st+1, arg max Q+H(st+ι,b); θ+),
b∈A
YtQ- = -Rt + γ- Q(st+1,argmaxQt-H(st+1,b)； θt- ),
b∈A
(17)
while the gradient descent method used in Q value approximation as the traditional deep Q-learning
remains unchanged. Note that QH influences only the choice of optimal action, not the Q value itself.
In equation 16 and 17, Q+ function chooses the optimal action depending on not only Q(s, a； θ+ )
but also Q(s, a； θ- ). It means that the inverse policy is helping the original policy to choose potential
next steps. Symmetrically, the original policy also helps correcting the inverse policy.
Eventually, we have the following hybrid policy πH, which is the overall policy of our agent. In our
algorithm, πH is the same as π+H .
πH(s) ' π+H (s) = arg max C (Q(s, a； θ+), Q(s, a； θ- )).	(18)
a∈A
An optimal policy ∏H* can be obtained upon the convergence of Q+ and Q- , which is,
∀s ∈ S, πH*(x) = arg max C(Q+*, Q-*).	(19)
a∈A
During the learning process, Q+ and Q- are equally important as they both contribute to the hybrid
Q value (QH). In practice, both functions are trained simultaneously, with separately sampling and
learning process ,so that each one improves itself and helps the other at the same time.
1In our experiment, the combination operator C is defined as C = λq1 + (1 - λ)(-q2), where λ is an
arbitrary interpolation factor. Note that C(q1, q2) 6= C(q2, q1).
4
Under review as a conference paper at ICLR 2019
The hybrid Q-learning algorithm can be easily generalized in other value-function-based algorithms.
In double Q-learning, the QH value can be computed to improve the original algorithm by using the
following equations,
YtDoubleQ+ = Rt + Y+Q(st+1, argmax C (Q(st+1, a； θ+), Q(st+ι,a; θ-)); θ+0),	(20)
a∈A
YtDmMeQ- = -Rt + Y-Q(St+1, arg max C(Q(St+ι,a; θ- ),Q(st+ι,a; θ+)); θ-).	(21)
a∈A
As we discussed in section 1, the hybrid policy for on-policy algorithms is simpler. We implement
the following on-policy QH using the original Q(S, a； θ+) and Q(S, a； θ-) in equation 13,
QH(S, a) = C(Q(S, a； θ+), Q(S, a； θ-)).	(22)
2.3	Practice on On-Policy Actor-Critic Algorithms
We manage to port our algorithm of hybrid policy to the on-policy actor-critic (AC) algorithm,
which is widely used in the current reinforcement learning cases. There exist several variants of AC
algorithm, such as DDPG David Silver (2014), A3C Volodymyr Mnih (2016) and DPPO Schulman
et al. (2017). We share our experience on the original on-policy AC algorithm, and the practice should
be reproducible on other AC-based algorithms.
AC uses value-function-based critic to learn the values of the current policy (i.e. Qπ) and guides
the actor’s policy update. The idea of hybrid policy is used to build a more stable and accurate
value function for the critic. We choose on-policy AC to show that our approach is also effective
for on-policy algorithms. Like in deep Q-learning, an inverse-value function is created, denoted as
the inverse critic. This critic takes reversed rewards and is trained simultaneously with the native
critic. Then the two critics evaluate the behavior of the actor together. We define the target value to
be approximated by both native and inverse critics as,
θQ+ι = θQ+ — α+(Rt + γ+Qπ(st+1,at+1; θQ+) - Qn(st, at； θQ+ ))V©q+ Qn(st, at； θQ+),
θQ-ι = θQ- - α-(-Rt + γ-Qπ(st+1,at+1; θQ-) - Qn(st, at； θQ-))V0q- Qn(St, at; θQ-),
θ∏+ι = θ∏ — βVθ∏ log∏(at∣st; θ∏)C(Qn(st, at； θQ+), Qn(st, at； θQ-)).
(23)
where β is the actor’s learning rate, θQ+ and θQ- are the parameters of the original and inverse critics,
and θn is the parameter of the actor. We presents the experiment results based on this approach in
Section 3 and show that our approach can learn the value of Qn more efficiently and stably.
3 Evaluation
3.1	Overview
We use CartPole ope (a), Mountain Car ope (c), and Pendulum ope (d) in OpenAI gym v0.9.6 ope (b)
without any modification. CartPole is run with 4000 episodes and 200 maximum steps per episode.
Mountain Car is run with 2000 episodes and 1000 maximum steps. 2 Pendulum is run with 1000
episodes and 2000 maximum steps. We ran the experiments on an Intel x86 machine on Ubuntu Linux
16.4 with TensorFlow 1.6 ten; Abadi et al. (2016) as our RL platform. Each algorithm is implemented
as a neural network, which consists of one fully-connected layer with 128 hidden-nodes. For each
game, we test the original method of deep Q-learning, double Q-learning, and on-policy actor-critic 3
as our baseline firstly. Then we apply inverse policy and hybrid policy on these algorithms.
Note that we use the off-line training in this paper. We train the policy for an entire episode and use
the trained policy to test the benchmark for an entire episode. The police is unchanged during the
2In Mountain Car, we use |nextState[0] - currentState[0]| as the reward for training because the original
deep Q-learning and double Q-learning obtain very poor results using the original reward generated by the
environment, which is meaningless. Nevertheless, we use the original rewards of Mountain Car in figure 2 and
table 2 to make the results easy to understand.
3Because deep Q-learning and double Q-learning cannot process the continuous action space, we discretize
the continuous action space into 5 discrete actions for Pendulum. We still use continuous action space in
actor-critic algorithm for Pendulum.
5
Under review as a conference paper at ICLR 2019
Table 1: The algorithm parameters.
	Cart Pole			Mountain Car			Pendulum		
	α	Y	C (qi ,q2)	α	Y	C(qi ,q2)	α	Y	C (qι,q2)
	 Deep Q+	0.002	0.9	C ∣λ=0.52	0.001	0.9	C ∣λ=0.52	0.001	0.9	C ∣λ=0.5 2
Deep Q-	0.002	0.7		0.001	0.9		0.00005	0.9	
Double Q+	0.002	0.9	C ∣λ=0.52	0.001	0.9	C ∣λ=0.52	0.001	0.9	C ∣λ=0.5 2
Double Q-	0.002	0.9		0.001	0.9		0.00005	0.9	
AC(Critic+)1	0.001	0.9	C ∣λ=0.52	0.005	0.9	C ∣λ=0.52	0.001	0.9	C ∣λ=0.5 2
AC(Critic-)1	0.0005	0.7		0.005	0.9		0.001	0.9	
+ The parameters for the original policy.
- The parameters for the inverse policy.
1	For the actor, α is 0.00002(CartPole), 0.0005(Mountain Car) and 0.0001(Pendulum), respectively.
2	The combination function C(q1, q2) = λq1 + (1 - λ)(-q2).
testing episode. In comparison, many other works employ on-line training which trains and tests
the policy in the same episode. We choose the off-line training because it is more lightweight and
efficient in practice, and easier to expose the unstability of the policies. The performance and the
stability of the off-line training is obviously worse than that of the on-line training.
Some hyper-parameters that are set in the experiments include: (1) α, as the learning rate for the
gradient descent method, (2) γ, as the discount factor in the accumulation of Q values, (3) λ, as the
factor used in the combination operator C(q1, q2), which controls the merging of Q values. For each
experiment, these parameters are separately adjusted to achieve the best outcome in table 1.
(h) Pendulum + double Q-learning
(g) Pendulum + deep Q-learning
(i) Pendulum + actor-critic
Figure 2: The results of Q-learning, double Q-learning and on-policy actor-critic algorithms, on games of
CartPole, Mountain Car and Pendulum. The comparison of algorithms in three variants: the traditional, the
inverse policy and the hybrid policy. The x-axis is the iterations of episodes and y-axis is the 100-step average
rewards around the corresponding episode. In game Mountain Car and Pendulum, we use log scaled y-axis,
which is computed as y0 = -log(-y).
Figure 2	demonstrates the combined results of the different policies, for each algorithm and each
game. We compute the average reward of the 100 episodes around each episode, so that we can
6
Under review as a conference paper at ICLR 2019
Table 2: The mean and standard deviation of rewards for the final on-line performance.
Algorithm	Cart Pole		Mountain Car		Pendulum	
	mean	std dev.	mean	std dev.	mean	std dev.
Deep Q Original	122.71	54.94	-151.02	26.59	-990.08	3238.70
Deep Q Inverse	194.87	26.33	-134.78	17.44t	-5647.43	4160.85
Deep Q Hybrid	198.58∣∕52.2%*	16.33t	-127.19t∕15.8%*	31.86	-358.79V63.8%*	695.38t
Double Q Original	100.75	55.78	-153.94	24.84	-1537.95	4076.06
Double Q Inverse	181.44	52.48	-138.25	18.9#	-5821.21	4889.33
Double Q Hybrid	199.35t∕97.8%*	4.07t	-116.6∣∕24.1%*	23.70	-649.91158.1%*	1450.11 *
Actor-Critic Original	169.19	36.54	-128.22	32.64	-3696.94	4442.36
Actor-Critic Inverse	148.42	28.74	-282.98	99.14	-1246.66t	532.72*
Actor-Critic Hybrid	188.59∣∕11.9%*	26.0#	-120.0t∕6.4%*	27.09t	-1677.30∕54.7%*	751.33
↑ the best result (i.e. higher mean value and lower standard deviation) in the comparable experiments.
I the improvement rate of hybrid policy compared with the original algorithm.
(a) Deep Q-learning
(b) Double Q-learning
(c) Actor-critic
Figure 3:	The correction of policies on Mountain Car experiment, with deep Q-learning, double Q-learning and
actor-critic. x-axis and y-axis are the the velocity and the position of the car, respectively.
observe the change of expected reward along the training. Note that in the experiment of Mountain
Car and Pendulum, the result range is transformed to log scale for ease of review. The detailed figures
showing the rewards at each episode can be found at Appendix B.
In CartPole as figure 2a and figure 2b, the inverse policies are better than the original policies, and
learn faster. This is because the game has only two actions, and the failures give valuable information
to agent, which make it favorable to the inverse policy. Meanwhile, as expected, in both cases the
hybrid policies manage to surpass the inverse ones with more steady increase in the end. The inverse
policy in figure 2c for actor-critic is less impressive, which matches the original in general, with a
better stability. Still, the hybrid policy is the best.
We can get the similar conclusion for Mountain Car from figure 2d 2e 2f, where the hybrid policy
outperforms the others at end of training. Though for actor-critic in figure 2f the hybrid policy is only
slightly better than the original in the end, it learns the suitable policy much earlier than the others.
For Pendulum (Fig 2g2h 2i), we deliberately set a large amount of training steps to observe the policy
degradation and stability. We can see the obvious degradation for all three algorithms and for the
inverse policy in double Q-learning. In comparison, the hybrid policy can effectively mitigate the
trend of policy degradation in a long and intensive training process. Note that for actor-critic, the
hybrid policy works well on continuous action space, which shows the generalization of our approach.
In table 2, we evaluate the performance by computing the mean and standard deviation with respect
to the final on-line performance van Hasselt (2011), i.e. the episode scores obtained in the last 10%
of the whole training runs. The mean shows the score expectation by the concluded policy, and the
standard deviation measures the stability. With no surprise, in almost all cases the hybrid policy gives
the best average reward. In most cases, hybrid policy is also the most stable policy, except in a few
cases inverse policy behaves more steadily. Meanwhile, either one of the two variant policies is better
than their original peer in almost all cases.
3.2 Discussions and insights
Some insights can be obtained by studying the performance of inverse policy. From our experience,
inverse policy learns directly from the bad experience of agent, which makes it quite effective when
useful information is only received at the failure of the game. Note that such failure-sensitive problem
7
Under review as a conference paper at ICLR 2019
is quite common in sparse-reward problems. As in CartPole, inverse agent can learn effective policy
much more quickly, based on the massive failure experience from the beginning episodes. Therefore,
in failure-sensitive problems, inverse policy can be used to boost the policy in short time.
We found that, for inverse policies, exploration is quite crucial. Since the inverse agent picks the bad
move that leads to a fast failure, it is easy to be stuck at local optima. The way we choose to address
this problem by adding large random noise to the selection of inverse agent, such as a high random
factor in the inverse Q-learning algorithm.
We also studied how the policy of the original is influenced by the Q- function. Mountain Car
includes two continuous state dimensions: the position ([-1.2,0.6]) and the velocity ([-0.07, 0.07])
of the car. We record all the states and chosen actions in the final 10% of training. Note that these
actions are actually chosen by the hybrid policy. At each step, we also record the actions that Q+
policy thought as optimal. Then we show the states where Q+ policy were corrected by the Q-
function in figure 3. For actor-critic, we record the actions where the original critic and inverse critic
have different evaluation on the chosen action (with respect to which action has the max value).
In both figures of figure 3a and figure 3b, most corrected actions gather at the states of low or medium
car velocity [-0.03,0.03]. In these states the agent is more frequently trained. According to the study
of Q-learning overestimation Thrun & Schwartz (1993)van Hasselt (2011), biased noise is cumulative,
which worsens with more samples. Plus, since the following rewards at these states are close, the
action value gap is relatively narrow. So sub-optimal actions value is more likely to surpass the
true optimal action Bellemare et al. (2016). By correcting these mis-actions, Q- policy effectively
improve the agent’s behavior. From the result of actor-critic in figure 3c, for on-policy Q functions,
the mis-estimation of Q values are more like unbiased noise, since the corrected actions scatters over
states. This result matches the analysis in van Hasselt (2011).
4	Related Works
Many studies are performed to solve the problem of sub-optimal policy and the policy-degradation.
All value-function-based algorithms may benefit from the mitigation of mis-estimated Q values.
Some algorithms, such as double Q-learning van Hasselt et al. (2016) and dueling Q-learning Wang
et al. (2016), proposed novel Q-learning architecture to obtain better approximation of Q* and more
stable learning process. In the work of Bellemare et al. (2016), this problem is addressed in a more
fundamental way. The paper proposed a new consistent Bellman operator in the Q-learning update
rule, which is claimed to be able to increase the gap of action values in every state, while maintaining
the optimal policy. It is shown that by modifying the Bellman operator, a more stable policy can be
achieved. In our work, the same insight is given.
The definition and usage of reward is another key point. Several studies focus on the idea of leveraging
different forms of reward. Starting from the Horde architecture Sutton et al. (2011), the concept
of pseudo-reward function is proposed to represent any useful feature-based signals. Based on
those rewards, separate general value functions (GVF) are trained in separate agents. Eventually,
multiple agents are combined together to solve complex problems. This idea continues in later
works of UNREAL Jaderberg et al. (2016) and HRA Van Seijen et al. (2017). In these studies, one
complex task is studied by breaking it down into several reward functions, or by associating it with
axillary tasks aside. The difference between the works above and ours is that, we have shown that
an impressive improvement can be achieved by just leveraging the inverse reward, instead of using
additional reward functions and agents.
5	Conclusion
The inverse policies and hybrid policies in this paper improve the original reinforcement learning
algorithms significantly. We obtained 63.8%, 97.8%, and 54.7% more rewards than the original
deep Q-learning, double Q-learning, and on-policy actor-critic algorithms do for some OpenAI gym
environments. More importantly, our new policies are much more stable in the experiments.
We will apply our new algorithms on more complicated environments. Although our improvements
may be easily applied on the algorithms based on policy gradient intuitively, we have to write the
theory for them and run some solid tests in the near future.
8
Under review as a conference paper at ICLR 2019
References
https://github.com/openai/gym/wiki/CartPole-v0, a.
https://gym.openai.com/, b.
https://github.com/openai/gym/wiki/MountainCar-v0, c.
https://github.com/openai/gym/wiki/Pendulum-v0, d.
https://www.tensorflow.org/.
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete
Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale
machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementa-
tion (OSD116), pp. 265-283, 2016. URL https://www.usenix.org/system/files/
conference/osdi16/osdi16-abadi.pdf.
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics,
pp. 834-846, 1983.
Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip S Thomas, and Remi Munos. Increasing
the action gap: New operators for reinforcement learning. In AAAI, pp. 1476-1483, 2016.
Nicolas Heess Thomas Degris Daan Wierstra Martin Riedmiller David Silver, Guy Lever. Determin-
istic policy gradient algorithms. Proceedings of the 31 st International Conference on Machine
Learning, 2014.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. SIAM Journal on Control and Optimization,
pp. 1008-1014, 2000.
Francisco S. Melo. Convergence of q-learning: A simple proof. http://users.isr.ist.utl.
pt/~mtjspaan/readingGroup/ProofQlearning.pdf.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep
Learning Workshop, 2013.
G. A. Rummery and M. Niranjan. On-line q-learning using connectionist systems. Technical report,
1994.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and
Multiagent Systems-Volume 2, pp. 761-768. International Foundation for Autonomous Agents and
Multiagent Systems, 2011.
Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement
learning. In Proceedings of The Fourth Connectionist Models Summer School, 1993.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16:
1476-1483, 2016.
9
Under review as a conference paper at ICLR 2019
Hado Philip van Hasselt. Insights in reinforcement learning. 2011.
Harm Van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey
Tsang. Hybrid reward architecture for reinforcement learning. In Advances in Neural Information
Processing Systems,pp. 5392-5402, 2017.
Mehdi Mirza Alex Graves Tim Harley Timothy P. Lillicrap1 David Silver Koray Kavukcuoglu
Volodymyr Mnih, Adri鱼 PUigdomenech Badia. Asynchronous methods for deep reinforcement
learning. Proceedings of the 33 rd International Conference on Machine Learning, 2016.
Ziyu Wang, Nando de Freitas, and Marc Lanctot. Dueling network architectures for deep reinforce-
ment learning. International Conference on Machine Learning (ICML), 2016.
10