Under review as a conference paper at ICLR 2019
An Energy-Based Framework
for Arbitrary Label Noise Correction
Anonymous authors
Paper under double-blind review
Ab stract
We propose an energy-based framework for correcting mislabelled training exam-
ples in the context of binary classification. While existing work addresses random
and class-dependent label noise, we focus on feature dependent label noise, which
is ubiquitous in real-world data and difficult to model. Two elements distinguish
our approach from others: 1) instead of relying on the original feature space, we
employ an autoencoder to learn a discriminative representation and 2) we intro-
duce an energy-based formalism for the label correction problem. We prove that a
discriminative representation can be learned by training a generative model using a
loss function comprised of the difference of energies corresponding to each class.
The learned energy value for each training instance is compared to the original
training labels and contradictions between energy assignment and training label
are used to correct labels. We validate our method across eight datasets, spanning
synthetic and realistic settings, and demonstrate the technique’s state-of-the-art
label correction performance. Furthermore, we derive analytical expressions to
show the effect of label noise on the gradients of empirical risk.
1	Introduction
Machine learning algorithms depend on reliable training labels or, when applicable, sufficiently
robust learning models in order to produce generalizable predictions (ZhU & Wu, 2004; Frenay &
Verleysen, 2014). Many empirical datasets suffer from training label corruption, which can result
from annotation error, human bias, or a noisy process for generating labels (Smyth, 1996; Brodley
& Friedl, 1999). In practice, it can be costly to obtain noise-free labels; hence, research on reducing
the effects of label noise on learning has received considerable attention.
Most work, however, focuses on label noise that is independent of the input features (e.g., random
label noise or class conditional random noise). The introduction of feature dependency significantly
complicates mathematical analysis (Natarajan et al., 2013; Liu & Tao, 2014; Northcutt et al., 2017).
Although work in this domain has yielded impressive results and improved generalization (Reb-
bapragada & Brodley, 2007a; Natarajan et al., 2013; Patrini et al., 2017; Rolnick et al., 2017; Ren
et al., 2018), the simplicity of these noise assumptions fails to capture crucial mislabelling processes
that arise in practice. In this work, we propose a semi-supervised framework for correcting the
effects of feature dependent label noise on supervised learning algorithms.
Label noise processes are categorized into three types, as illustrated in Figure 1. Type I refers to
a noise model where any label in the training data is incorrect with probability γ. In the case of
type II noise, the probability of label corruption is conditioned on the class, such that we have
different noise rate γ0 and γ1 for each class. Finally, type III noise models describe how label noise
depends explicitly on input features. Feature dependent label noise or equivalently type III noise
is a more realistic type of label noise that is ubiquitous in empirical datasets (Lachenbruch, 1974;
Schafer & Graham, 2002; Frenay & Verleysen, 2014). As an example of type In noise, consider the
diagnostic labels of Alzheimer’s disease. In this setting, the probability of label noise depends on
both age and sex (i.e., younger male patients are harder to diagnose) (Khachaturian, 1985; Murray
et al., 2016). Type III noise also includes the prevalence of unreliable labels for training instances in
low density regions of feature space (Denoeux, 1995; 1997; 2000) or near classification boundaries
(Lachenbruch, 1966; 1974; Chhikara & McKeon, 1984; Cohen, 1997; Beigman & Klebanov, 2009;
Beigman Klebanov & Beigman, 2009; Kolcz & Cormack, 2009).
1
Under review as a conference paper at ICLR 2019
Figure 1: Different categories of label noise and their statistical dependencies, as depicted by the
red arrow. In type I noise, all instances are equally likely to be mislabelled base on some probability
Y ∈ [0,1 ]. In the case of type II, this probability is different for each class: γ0 ∈ [0,1 ] and
γ1 ∈ [0,1 ]. Type In label noise is the most realistic model and yet the least studied. In this case the
probability of an error is a function of the input features: i.e. Perror 〜f (x).
In this paper, we present a practical framework for correcting label noise that extends beyond type
I and type II noise. We train an energy-based generative model on a small subset of the training
data with reliable labels, either manually annotated or automatically learned. This model is used to
identify and correct mislabelled examples based on the whether the assigned energy ofa given point
is compatible with its original label. We empirically demonstrate our framework’s ability to handle
input dependent label noise on both simulated and real datasets.
2	Related Work
The full body of work on the problem of label noise is too extensive to review here; however, we
direct interested readers to the detailed review by Frenay & Verleysen (2014). In this section, We
highlight key contributions in the label noise literature. We then briefly discuss existing theoretical
work.
2.1	Label Noise Correction
Existing work in label noise correction is designed for Type I and Type II noise and falls into three
categories: relabeling, learning procedures and loss functions. We expand on each area below.
Relabeling: Earlier approaches to label noise focus on relabeling the noisy training set. Brodley &
Friedl (1999) use the output of an ensemble of classifiers to identify mislabeled training examples.
Sun et al. (2007) identify mislabeled instances based on the entropy of class probabilities outputted
by a Bayesian classifier. These approaches, while robust to simpler label noise models, do not
account for Type III noise.
Learning Procedures: Methods modifying the learning procedure include the perceptron algorithm
with margin (PAM) Frenay & Verleysen (2014). Crammer & Lee (2010) use a velocity-based learn-
ing procedure to learn the weight vector distribution, termed gaussian herding (NHERD). Sukhbaatar
et al. (2014) introduce a noise layer to a neural network architecture to learn the noise function, while
Rebbapragada & Brodley (2007b) weight each example by class confidence in the training proce-
dure. These methods commonly suffer from overfitting to the noise and again, primarily focus on
Type I and Type II noise.
Loss Functions: Long & Servedio (2010) have shown classification algorithms that optimize a
convex potential over a linear class are not robust to random label noise. This has led to work which
aims to modify convex loss functions in order to make them noise-tolerant in the presence of type
I and type II noise. Ghosh et al. (2015) derived sufficient conditions for classification losses that
render them robust to random noise; namely, the components of a loss (where each component is
defined over a given class) must sum to a constant value. Rooyen et al. (2015) proposed a convex
loss that avoids the negative result of Long & Servedio (2010) for type I noise by virtue of being
negatively unbounded. Natarajan et al. (2013) have shown that risk minimization over the corrupted
data is consistent with risk minimization over clean data provided that the standard convex loss (e.g.,
binary cross entropy) is modified appropriately to produce an unbiased loss estimator (ULE). Each
of these loss functions is presented for and validated on Type I and Type II noise.
2
Under review as a conference paper at ICLR 2019
2.2	Existing Theoretical Guarantees
(Bylander, 1994; Blum et al., 1998; Blum & Mitchell, 1998) offer guarantees for hypothesis gener-
alization in the face of Type I and Type II noise. Angluin & Laird (1988); Bylander (1997; 1998);
Servedio (1999) present guarantees for type III noise, where the probability of error depends on the
distance to the margin. We introduce a gradient-based interpretation of label noise to these existing
results.
3	B inary Classification in the Presence of Label Noise
3.1	Problem Setup and Notation
Let P denote the true distribution from which n i.i.d. training examples
(x1, y1), (x2, y2), . . . , (xn, yn) have been drawn, where (xi, yi) ∈ Rd × {0, 1}. The clean
and corrupted training datasets are denoted as
T = {(xi,yi) for i = 1, 2,...,n} & T= {(xi.yi) for i = 1, 2,...,n},	(1)
respectively, where due to some label noise process yi → y%. Hence, We have access to the noisy
data T during training instead of the clean data T. It can be assumed that the corrupted samples
(xi,yi) are drawn from a noisy distribution P.
In supervised binary classification, we aim to learn a discriminator f : Rd → R that minimizes
the risk with respect to a given loss function1. Formally, we want to minimize the empirical risk
is R[',f, T] = mm Pm=I ' (f (xi, θ),yi), where the m, ', θ denote the size of the mini-batch size,
loss function, and the learnable model parameters respectively. Gradient based learning algorithms
compute Vθ(R [', f, T]) and update model parameters. For example, as in mini-batch gradient
descent,
Θt+1 = θt- ηVθ (R[',f,T]),	⑵
where η is the learning rate. In practice, we may not have access to the clean data T. Instead
we must learn the discriminator using the noisy data T. Thus, the second term in Eq. 2 becomes
Vθ(R[', f, T]), which denotes the risk with respect to the noisy training data.
3.2	Label Noise Effect on Risk Minimization
We can describe the effect of label noise on learning by expressing the noisy gradient Vθ (R[',f, Tl)
in terms of the true gradient Vθ(R[', f, T]) and a noise term. In the case of type I noise where any
label yi can be flipped to 1 - yi with probability γ ∈ [0, 1/2], we have
E(x,y)〜P [VθR[',川=(1- 2γ) Etrue + 2γ Era∏d,	⑶
where EtrUe ≡ E(X y)〜P [VθR[', T]] is the true (i.e. noise-free) gradient, and Erand is the completely
noisy term obtained by replacing all targets yi with a random value of either 0 or 1. For type II
noise, we have an equation analogous to Eq. 3, where Y is replaced by the average γ = (γo + γι)/2
noise rate and we have an additional term reflecting the noise imbalance between each class when
γ0 6= γ1. For more information on this and derivations of these equations, see Appendix A.
From Eq. 3 we deduce that for small γ the true gradients dominate the dynamics of searching θ
space for optimal parameters. As γ grows, the true gradients are scaled down by the factor (1 - 2γ),
which means that the contribution of Etrue diminishes and the random perturbation term Erand starts
to dominate as Y grows. A similar conclusion can be reached for type II noise (replacing Y with Y)
where we have an additional term modifying the true gradients based on the class imbalance (see
A.2).
The effect of Erand can be negated by increasing the amount of training data proportionately with Y
as discussed by Rolnick et al. (2017): i.e. as the probability of a random error occurring increases,
1In practice the loss function ` is usually classification-calibrated to minimize the 0-1 loss given that the
sample size is sufficiently large (Bartlett et al., 2006).
3
Under review as a conference paper at ICLR 2019
Figure 2: Plots showing the path traversed in θ space by a supervised model as learnable parameters
are updated via a gradient based optimizer. The gold line shows the path followed by a noise-free
model and the blue lines show the modified paths due to label noise. In the case of random label
noise (left plot), the perturbations Erand to the true gradient direction are curbed by an increase in
training set size. In contrast, for input dependent noise (right plot), supplying the model with more
data perpetuates the label bias.
the model needs more data to make progress in the direction of the true gradient. We can neutralize
the effects (i.e. suppress Erand) of random noise (type I) or class conditional random noise (type II)
by increasing the number of training examples and by utilizing a robust loss function when possible
(Natarajan et al., 2013).
In contrast, Type III noise is substantially more difficult to address because the available training
data T can have optimal discriminators that are significantly different from the true discriminator
corresponding to the clean data T. Separating Type III noise into the true term and the perturbation
term necessitates restrictive assumptions on the noise model (See Appendix A. In Figure 2, we show
this by visualizing SGD over the true training set (gold lines) and the noise training set (blue lines).
As shown in Figure 2, increasing the size of the training set is effective in address Type II noise.
This strategy is not only ineffective for addressing type III noise, but they can also lead to our model
learning the biased discriminator; thus, hindering any possibility of generalization.
4	An Energy-based Framework for Bias Correction
Type III noise is particularly challenging because noise can depend on the input features in an
arbitrarily complex way. Modeling this requires significant information about the noise process,
which can be difficult to obtain in practice. In order to deal with this challenge we will avoid
training directly on the mislabelled instances. Instead, we start our training procedure by assuming
there exists a small subset of T that is noise-free (e.g. 1% of the data is sufficient the MNIST
dataset). Such a set can be obtained via a manual labelling process or by using domain knowledge
to identify the appropriate data points2.
Our objective is to correct mislabelled training instances such that the corrected data leads to the
improved generalization of a trained model. We will demonstrate in Section 5 that our framework
extends beyond Type I and Type II noise to Type III noise, in realistic settings. In particular, we
demonstrate the method’s strength in correcting algorithmically assigned labels.
The proposed correction framework consists of three steps:
1.	Obtain known labels: There are two ways of identifying instances that have correct labels.
The first approach is to use domain knowledge - frequently, one has access to a subset of
clean labels and their instances. We can use this subset as our clean data for step 2. If
this set is unknown or too small, then we can inference which examples are likely to be
correctly labeled, as is done in Ding et al. (2018). We report results on both approaches in
Section 5.
2This assumption can be relaxed in situations where we can learn which labels are likely to be clean. E.g.
see Ding et al. (2018)
4
Under review as a conference paper at ICLR 2019
2.	Train semi-supervised model: The framework uses the filtered data from the previous step
to train a semi-supervised algorithm that learns to classify instances based on similarity
between features. In contrast with supervised learning, where the goal is to learn p(y|x),
we want to learn which features are most compatible with a given label: i.e. p(x|y). Thus,
we train a generative model (e.g. energy-based autoencoder) to use feature similarity (as
quantified by an energy function) to identify which instances belong to the specific class.
3.	Identify and correct mislabelled instances: The features of the full training data (with mis-
labeled targets) are injected into the previously trained semi-supervised model resulting in
each instance being assigned an energy value based on the output of the model. The en-
ergy corresponding to each instance serves as a proxy for class assignment (i.e. low energy
corresponds to y = 0 whereas high energy corresponds to y = 1). Contradictions between
energy assignment and training labels are used to correct the training data such that all
labels are compatible with their assigned energy.
We first motivate the design decisions involved in this framework. Namely, we explore our choice
of contrastive divergence as a loss function and empirically justify our use of an autoencoder. In the
subsequent section, we elaborate on our implementation of the framework.
4.1 Framework Motivation
Given a small, clean dataset T ⊂ T, we aim to train a model which can be used to determine
labels on the remaining, noisily labelled dataset. One obvious approach is to train a binary classi-
fier directly and use it to correct labels. However, as discussed by Berthelot et al. (2017), binary
classification provides a relatively weak training signal which largely ignores the intricacies of the
input feature distribution. See Fig. 3 where we compare the class separation achieved by a few stan-
dard binary classifiers with our proposed approach. Alternatively, one could train class conditional
models. However, such models are trained without knowledge of the primary task, differentiating
classes. Further, traditional unsupervised training regimes typically use a maximum likelihood for-
mulation (i.e. forward KL divergence) which is prone to distributing probability mass broadly (the
so-called “mean-seeking behaviour” (Murphy, 2012)) and further limit the discriminative ability of
the learned model.
Instead, we prioritize learning to discriminate the underlying classes based on aspects of the input
feature distributions. To do this, we propose to use a contrastive divergence (Carreira-Perpinan &
Hinton, 2005) training loss:
CD (θ) = KL [p0 (x) kq (x; θ)] - KL [p1 (x) kq (x; θ)] .	(4)
where p0(x) and p1 (x) denote the target probability distributions conditioned on class 0 and 1
respectively and q(x; θ) denotes the learned model. Thus, training aims to produce a model q (x; θ)
which assigns high probability to samples from class 0, while giving low probability to samples from
class 1. In theorem 1, we show that a finite sample approximation of the optimization objective in
Eq. 4 can be computed as
L ⑹=|Tj X E(XO; θ) - |TI X E(XI; θ),
1	0| xo∈T0	1	1| x1∈T1
(5)
where Ty, for y ∈ {0, 1}, contains instances from class y and Ty ⊆ T, ITyI denotes the
number of elements in Ty and E(X; θ) is the energy of the model q (X; θ), i.e., q (X; θ) =
exp (-E(X; θ)) /Z(θ) where Z(θ) is the normalizing partition function. Analogously, we can
define Eq. 5 over each mini-batch. This contrastive loss have been applied successfully in adver-
sarial training, resulting in faster and improved stability of training, enhanced generator quality, and
improved generator diversity (Zhao et al., 2016; Berthelot et al., 2017). As described in section 2.2
of (LeCun et al., 2006), the negative log-likelihood function, E(X; θ) = - log q (X; θ), constitute
a valid energy which has been used in numerous energy-based models [e.g. (Bengio et al., 2003;
Zhao et al., 2016; Berthelot et al., 2017)].
Theorem 1. The contrastive divergence CD (θ) in Equation 4 has the finite approximation L (θ) up
to some constant k which is independent of θ.
Proof.
Using the definition of KL divergence, we expand Equation 4:
I
x
CD(θ)=-
x
p0 (X) logq (X; θ)dX +
p1 (X0) log q (X0; θ)dX0 + k
(6)
5
Under review as a conference paper at ICLR 2019
(a) Logistic Regression
(b) SVM
(c) 1-Layer NN
(d) Autoencoder
(e) Clean Data
(f) Noisy Data
(g) AE Corrected Data
Figure 3: In the first row, we visualize the separation achieved by classifiers trained on a training
set with noisy labels. From left to right, we show the results of logistic regression, an SVM, a 1
layer feed-forward neural network and the proposed technique. In the second row, we visualize
the training data. From left to right, we show the clean data, the noisy data and the autoencoder-
corrected data. We observe that standard binary classifiers are unable to achieve satisfactory class
separation. The 1 hidden layer NN classifier accomplishes significant separation but it incorrectly
identifies mislabelled instances. The proposed model is able to identify mislabelled instances based
on the energy distribution of the training data.
where the two entropy terms x p0 (x) log p0 (x)dx and x p1 (x) log p1 (x)dx have been grouped
in the constant k as they do not depend on the optimization parameters θ . Assume the Gibbs form
for the model distribution,
e-E (x；θ)
q (x； θ) =	,	⑺
Z(θ)
where E(x; θ) is the energy and Z(θ) = Jx e-E(x;。)dx is the partition function. Then, Equation 6
becomes
CD (θ)	=	p0 (x) E(x; θ)dx - p1 (x0) E(x; θ)dx0	(8) x	x0 -	p0 (x) dx Z(θ) +	p1 (x) dx Z(θ) + k,	(9)
where the partition function terms conveniently cancel out due to the normalization of p0 and p1 .
Therefore,
CD (θ) = Ep0(x) [E (x)] -Ep1(x)[E(x)]+k	(10)
and we conclude that CD (θ) has the finite approximation L (θ) as defined in Equation 5, up to a
constant k.
4.2 Framework Implementation: Contrastive Autoencoder
Using the clean subset T , we implement step 2 of our framework by using an energy-based au-
toencoder (analogous to Zhao et al. (2016)). In order to ensure stable training and to prevent
the second term of Equation 5 from diverging, we balance our mini-batch to have an equal num-
ber of positive and negative class samples. After training is complete, we process the original
training data T using the trained autoencoder. Specifically, we compute the energy using the re-
sulting outputs: E (x; θ) = - logq (x; θ), which reduces to the reconstruction loss correspond-
ing to the autoencoder (Goodfellow et al., 2016). For example, if the underlying distribution is
assumed to be Gaussian then, We have E (x; θ) = ∣∣x - X(θ)k2, where X(θ) is the output of
6
Under review as a conference paper at ICLR 2019
the autoencoder3. Whereas, if the underlying distribution is assumed to be Bernoulli, we obtain
E (x; θ) = — [x log X(θ) + (1 - x)log(1 - X(θ))].
To execute our label correction protocol, we impose the following two conditions. (i) if E(x; θ) >
E1 - a for x ∈ T0, then change label of x. Here we denote T0 as the originals negative samples, E1
as the mean energy over the set of clean positive samples T1, and a is a tuneable hyperparameter
that determines how aggressively we want to change 0 → 1. Analogously, we impose condition (ii)
if E(x; θ) < Eo — b for X ∈ Tl, then change label of x. The rationale is to modify samples where
the original label y contradicts the energy assignment. When one is not able to tune the threshold
hyperparameters a and b, an empirical heuristic is to set them equal to the standard deviation about
the means Ei and Eo respectively. See Appendix B details and pseudo-code.
5	Experiments
We conduct experiments with simulated data and real-world data where three different type III noise
models are studied: (i) Linear noise (PerTOr 〜ɑx”: the probability of an error occurring depends
linearly on a feature (Table 1). (ii) Quadratic noise (perror 〜ɑx2): the probability of an error
occurring depends on the square of a feature. We control the amount of noise added with α and
we select xi if it has sufficient variance to make the noise model distinct from random noise, i.e. 0
variance implies random noise (Table 2). (iii) Boundary noise: the probability of label error depends
on the distance from the class boundary, which is determined using the noise-free data. We report the
average class-weighted F1-score along with the standard error over 10 runs with random train-test
(80:20) splits (Table 3).
UCI benchmark datasets: We train logistic regression on the corrected dataset resulting from the
proposed algorithm. If a small clean dataset is not initially provided, then our results are labelled
as AE (learned), otherwise they are labelled at AE (known) - see step 1 in Sec. 4. We compare to
noise robust algorithms: NHERD Crammer & Lee (2010), PAM Frenay & Verleysen (20l4), and
ULE Natarajan et al. (2013). Additionally, we report upper and lower bounds by training logistic
regression on the noisy data (LR-N) and on the clean data (LR-C). All algorithms are tested on noise-
free data. Even without access to any clean labels (i.e., we learn which labels are likely clean), our
framework generally outperforms the other algorithms in the presence of linear noise and quadratic
noise. In general, learning which labels are noisy following the method in Ding et al. (2018) is
not possible for arbitrary label noise processes: e.g., when the optimal discriminators of T are very
different than the optimal discriminators of T. For boundary noise, we show that using a small clean
dataset T enables our methods to learn in the presence of boundary noise, where the other methods
generally breakdown (Table 3). For a fair evaluation, the benchmark algorithms (which don’t have
access to clean data) should be compared to AE (learned).
MNIST: Next, we compare our method with another state-of-the-art noise robust algorithm that
relies on clean data, i.e., the loss weighting scheme of Ren et al. (2018). Both methods are given
the same percentage of clean data for each noise setting. The task is to classify the distinguish “3”
(class 0) vs. other digits (class 1). The noise process is input dependent as it changes 4, 5, 6 to class
0 depending on a specified noise rate (α). We demonstrate (Table 4) that our method generalizes
better in the presence of type III noise, even when starting with only 1% of the data that is clean.
Arrhythmia We use the MIT-BIH Arrhythmia dataset Goldberger et al. (2000) to evaluate the pro-
posed method’s ability to correct algorithmically assigned labels. Algorithmically-assigned labels
(AALs) are prevalent in domains with abundant unlabeled data and high labelling costs. Common
applications include web page annotation, but the value of this approach extends to automatic anno-
tation of images and natural language. We employ the data preprocessing procedure described by
MOndejar-GUerra et al. (2019), where each arrhythmia is described by 59 features.
We divide the dataset into three parts: T1, T2 and T3. T1 and T2 serve as training sets and T3 is
the test set. We train a classifier on T1 and subsequently use that classifier to generate AALs for
T2. Thus, the predictions of the trained classifier become y and the original expert labels remain
y . As demonstrated in Table 1, the proposed technique achieves a higher F1 score than competing
3More precisely, E (x; θ) = ∣∣x — X(θ)k2 up to an irrelevant scaling factor that We can drop for simplicity.
7
Under review as a conference paper at ICLR 2019
Dataset	Noise Parameters (col, α)	LR-N	NHERD	PAM	ULE	AE (learned)	AE (known)	LR-C
Lin-Sep	1, 1.2	0.86 ± 0.03	0.91 ± 0.01	0.76 ± 0.01	0.90 ± 0.02	0.94 ± 0.01	0.95 ± 0.01	0.96 ± 0.01
Diabetes	5, 1.0	0.60 ± 0.03	0.50 ± 0.01	0.48 ± 0.03	0.60 ± 0.03	0.64 ± 0.01	0.70 ± 0.02	0.72 ± 0.02
German	1 , 1.2	0.67 ± 0.03	0.72 ± 0.01	0.49 ± 0.03	0.63 ± 0.01	0.67 ± 0.03	0.73 ± 0.01	0.76 ± 0.02
Image	1 , 0.7	0.61 ± 0.03	0.63 ± 0.01	0.54 ± 0.01	0.61 ± 0.02	0.67 ± 0.01	0.77 ± 0.01	0.77 ± 0.01
Twonorm	1 , 1.2	0.68 ± 0.04	0.50 ± 0.02	0.43 ± 0.03	0.82 ± 0.04	0.88 ± 0.01	0.91 ± 0.02	0.98 ± 0.01
Breast Cancer	5, 1.0	0.66 ± 0.02	0.67 ± 0.02	0.52 ± 0.03	0.60 ± 0.03	0.60 ± 0.02	0.71 ± 0.02	0.70 ± 0.01
Arrhythmia	-		0.79 ± 0.01	0.83 ± 0.01	0.81 ± 0.02	0.66 ± 0.04	0.85 ± 0.02	0.85 ± .01	0.85 ± 0.02
Table 1: Noise model: probability of an error occurring depends linearly on an input feature. We
report the class weighted mean f1 score on the noise-free test set along with the standard error.
Dataset	Noise Parameters (col, α)	LR-N	NHERD	PAM	ULE	AE (learned)	AE (known)	LR-C
Lin-Sep	1, 1.2	0.40 ± 0.01	0.53 ± 0.04	0.62 ± 0.03	0.61 ± 0.02	0.93 ± 0.01	0.96 ± 0.01	0.96 ± 0.01
Diabetes	5, 1.2	0.62 ± 0.01	0.59 ± 0.01	0.67 ± 0.01	0.71 ± 0.01	0.66 ± 0.03	0.69 ± 0.01	0.72 ± 0.02
German	1, 1.2	0.60 ± 0.02	0.72 ± 0.01	0.60 ± 0.01	0.67 ± 0.01	0.68 ± 0.03	0.73 ± 0.03	0.76 ± 0.02
Image	1, 0.7	0.55 ± 0.02	0.64 ± 0.01	0.57 ± 0.01	0.60 ± 0.03	0.74 ± 0.01	0.77 ± 0.01	0.77 ± 0.01
Twonorm	1, 1.2	0.90 ± 0.03	0.55 ± 0.02	0.86 ± 0.01	0.94 ± 0.01	0.96 ± 0.01	0.97 ± 0.01	0.98 ± 0.01
Breast Cancer	5, 1.0	0.60 ± 0.02	0.60 ± 0.02	0.63 ± 0.02	0.63 ± 0.03	0.65 ± 0.03	0.66 ± 0.01	0.70 ± 0.01
Table 2: Noise model: probability of an error occurring depends quadratically on an input feature.
We report the class weighted mean f1 score on the noise-free test set along with the standard error.
Dataset	Noise Parameters (α)	LR-N	NHERD	PAM	ULE	AE (known)	LR-C
Lin-Sep	0.7	0.39 ± 0.01	0.41 ± 0.01	0.53 ± 0.02	0.85 ± 0.01	0.94 ± 0.01	0.96 ± 0.01
Diabetes	0.7	0.56 ± 0.01	0.53 ± 0.03	0.50 ± 0.02	0.55 ± 0.02	0.68 ± 0.02	0.72 ± 0.02
German	0.7	0.57 ± 0.01	0.70 ± 0.01	0.47 ± 0.01	0.60 ± 0.01	0.72 ± 0.01	0.76 ± 0.02
Image	0.7	0.43 ± 0.01	0.61 ± 0.01	0.45 ± 0.02	0.43 ± 0.01	0.74 ± 0.03	0.77 ± 0.01
Twonorm	0.5	0.52 ± 0.03	0.51 ± 0.03	0.52 ± 0.03	0.51 ± 0.04	0.88 ± 0.03	0.98 ± 0.01
Breast Cancer	0.7	0.62 ± 0.02	0.62 ± 0.02	0.46 ± 0.02	0.63 ± 0.02	0.66 ± 0.04	0.70 ± 0.01
Table 3: Noise model: probability of an error occurring depends linearly on the distance from class
boundary (which is defined by the clean dataset before noise is synthetically introduced). We report
the class weighted mean f1 score on the noise-free test set along with the standard error.
% Clean data	Noise Parameter (α)	AE(known)	Ren et al. (2018)	No noise model
1	0.1	98.28 ± 0.01	-93.27 ± 0.01 ^^	90.87 ± 0.05
1	0.8	98.14 ± 0.00	91.27 ± 0.01	87.32 ± 0.03
5	0.1	98.77 ± 0.00	93.44 ± 0.01	91.01 ± 0.02
5	0.8	98.65 ± 0.00	91.29 ± 0.01	88.85 ± 0.02
10	0.1	98.93 ± 0.00	94.87 ± 0.03	90.99 ± 0.02
10	0.8	98.54 ± 0.01	91.77 ± 0.02	90.16 ± 0.03
Table 4: We compare the label re-weighting scheme of Ren et al. (2018) with our proposed model.
The models compared for different sizes of clean dataset and different noise rates α. The base model
in each case is a standard LeNet as in Ren et al. (2018)
methods. The success of the model in this setting suggests its unique robustness to feature-dependent
noise.
6	Conclusion
We have proposed an energy-based framework to correct mislabelled training instances. By mini-
mizing a contrastive loss, the proposed method learns a representation that is valuable in relabeling
noisy training sets. We evaluate the proposed model across six datasets and three noise models to
demonstrate the method’s empirical value in correcting feature-dependent label noise. Furthermore,
we demonstrate our method’s improvement upon existing work in making machine learning more
robust to label noise processes.
8
Under review as a conference paper at ICLR 2019
References
Dana AnglUin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343-370,
1988.
Peter L Bartlett, Michael I Jordan, and Jon D McAUliffe. Convexity, classification, and risk boUnds.
Journal of the American Statistical Association,101(473):138-156, 2006.
Eyal Beigman and Beata Beigman Klebanov. Learning with annotation noise. In Proceedings
of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pp.
280-287, Stroudsburg, PA, USA, 2009. Association forComputationalLinguistics. ISBN 978-1-
932432-45-9. URL http://dl.acm.org/citation.cfm?id=1687878.1687919.
Beata Beigman Klebanov and Eyal Beigman. From annotator agreement to noise models. Comput.
Linguist., 35(4):495-503, December 2009. ISSN 0891-2017. doi: 10.1162∕coli.2009.35.4.35402.
URL http://dx.doi.org/10.1162/coli.2009.35.4.35402.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal OfmaChine learning research, 3(Feb):1137-1155, 2003.
David Berthelot, Thomas Schumm, and Luke Metz. Began: boundary equilibrium generative adver-
sarial networks. arXiv preprint arXiv:1703.10717, 2017.
A.	Blum, A. Frieze, R. Kannan, and S. Vempala. A polynomial-time algorithm for learning noisy
linear threshold functions. Algorithmica, 22(1):35-52, Sep 1998. ISSN 1432-0541. doi: 10.1007/
PL00013833. URL https://doi.org/10.1007/PL00013833.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Pro-
ceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pp.
92-100, New York, NY, USA, 1998. ACM. ISBN 1-58113-057-0. doi: 10.1145/279943.279962.
URL http://doi.acm.org/10.1145/279943.279962.
Carla E. Brodley and Mark A. Friedl. Identifying mislabeled training data. J. Artif. Int. Res., 11(1):
131-167, July 1999. ISSN 1076-9757. URL http://dl.acm.org/citation.cfm2id=
3013545.3013548.
Tom Bylander. Learning linear threshold functions in the presence of classification noise. In In
Proceedings of the Seventh Annual Workshop on Computational Learning Theory, pp. 340-347.
ACM Press, 1994.
Tom Bylander. Learning probabilistically consistent linear threshold functions. In Proceedings of
the Tenth Annual Conference on Computational Learning Theory, COLT ,97, pp. 62-71, New
York, NY, USA, 1997. ACM. ISBN 0-89791-891-6. doi: 10.1145/267460.267479. URL http:
//doi.acm.org/10.1145/267460.267479.
Tom Bylander. Learning noisy linear threshold functions. Submitted for journal publication, 1998.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In Aistats,
volume 10, pp. 33U0. Citeseer, 2005.
Raj S. Chhikara and Jim McKeon. Linear discriminant analysis with misallocation in training sam-
ples. Journal of the American Statistical Association, 79(388):899-906, 1984. ISSN 01621459.
URL http://www.jstor.org/stable/2288722.
E. Cohen. Learning noisy perceptrons by a perceptron in polynomial time. In Proceedings 38th
Annual Symposium on Foundations of Computer Science, pp. 514—523, Oct 1997. doi: 10.1109/
SFCS.1997.646140.
Koby Crammer and Daniel D Lee. Learning via gaussian herding. In Advances in neural information
processing systems, pp. 451T59, 2010.
Thierry Denoeux. A k-nearest neighbor classification rule based on dempster-shafer theory. IEEE
Transactions on Systems, Man, and Cybernetics, 25(5):804-813, May 1995. ISSN 0018-9472.
doi: 10.1109/21.376493.
9
Under review as a conference paper at ICLR 2019
Thierry Denoeux. Analysis of evidence-theoretic decision rules for pattern classification. Pattern
Recogn.,30(7):1095-1107, July 1997. ISSN0031-3203. doi: 10.1016∕S0031-3203(96)00137-9.
URL http://dx.doi.org/10.1016/S0031-3203(96)00137-9.
Thierry Denoeux. A neural network classifier based on dempster-shafer theory. IEEE Transactions
on Systems, Man, and Cybernetics - Part A: Systems and Humans, 30(2):131-150, Mar 2000.
ISSN 1083-4427. doi: 10.1109/3468.833094.
Yifan Ding, Liqiang Wang, Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach
to learning from noisy labels. arXiv preprint arXiv:1802.02679, 2018.
B.	Frenay and M. Verleysen. Classification in the presence of label noise: A survey. IEEE Transac-
tions on Neural Networks and Learning Systems, 25(5):845-869, May 2014. ISSN 2162-237X.
doi: 10.1109/TNNLS.2013.2292894.
Beno^t Frenay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE
transactions on neural networks and learning systems, 25(5):845-869, 2014.
Aritra Ghosh, Naresh Manwani, and P.S. Sastry. Making risk minimization tolerant to label
noise. Neurocomputing, 160:93 - 107, 2015. ISSN 0925-2312. doi: https://doi.org/10.1016/
j.neucom.2014.09.081. URL http://www.sciencedirect.com/science/article/
pii/S0925231215001204.
Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G
Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank,
physiotoolkit, and physionet: components of a new research resource for complex physiologic
signals. Circulation, 101(23):e215-e220, 2000.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Zaven S. Khachaturian. Diagnosis of alzheimer’s disease. Archives of Neurology, 42(11):1097-
1105, 1985. doi: 10.1001/archneur.1985.04060100083029. URL +http://dx.doi.org/
10.1001/archneur.1985.04060100083029.
Aleksander Kolcz and Gordon V. Cormack. Genre-based decomposition of email class noise. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD’09, pp. 427-436, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-495-
9. doi: 10.1145/1557019.1557070. URL http://doi.acm.org/10.1145/1557019.
1557070.
Peter A. Lachenbruch. Discriminant analysis when the initial samples are misclassified. Techno-
metrics, 8(4):657-662, 1966. ISSN 00401706. URL http://www.jstor.org/stable/
1266637.
Peter A. Lachenbruch. Discriminant analysis when the initial samples are misclassified ii: Non-
random misclassification models. Technometrics, 16(3):419-424, 1974. ISSN 00401706. URL
http://www.jstor.org/stable/1267672.
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1(0), 2006.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38, 11 2014.
Philip M. Long and Rocco A. Servedio. Random classification noise defeats all convex poten-
tial boosters. Machine Learning, 78(3):287-304, Mar 2010. ISSN 1573-0565. doi: 10.1007/
s10994- 009- 5165- z. URL https://doi.org/10.1007/s10994-009-5165-z.
V Mondejar-Guerra, J Novo, J Rouco, MG Penedo, and M Ortega. Heartbeat classification fusing
temporal and morphological information of ecgs via ensemble of classifiers. Biomedical Signal
Processing and Control, 47:41-48, 2019.
10
Under review as a conference paper at ICLR 2019
Kevin P Murphy. Machine learning: a probabilistic perspective. The MIT Press, Cambridge, MA,
2012.
Melissa E. Murray, Adel Aziz, Owen A. Ross, Ranjan Duara, Dennis W. Dickson, and Neill R.
Graff-Radford. Alzheimer’s disease may not be more common in women; men may be more
commonly misdiagnosed. Alzheimer’s & Dementia: The Journal of the Alzheimer’s Association,
12(7):1097-1105, 2016. doi: doi:10.1016/j.jalz.2016.06.527. URL http://dx.doi.org/
10.1016/j.jalz.2016.06.527.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
C.	G. Northcutt, T. Wu, and I. L. Chuang. Learning with Confident Examples: Rank Pruning for
Robust Classification with Noisy Labels. In Uncertainty in Artificial Intelligence, August 2017.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proc. IEEE Conf.
Comput. Vis. Pattern Recognit.(CVPR), pp. 2233-2241, 2017.
Umaa Rebbapragada and Carla Brodley. Class noise mitigation through instance weighting. In
Machine Learning: ECML 2007: 18th European Conference on Machine Learning, pp. 708-715,
09 2007a.
Umaa Rebbapragada and Carla E Brodley. Class noise mitigation through instance weighting. In
European Conference on Machine Learning, pp. 708-715. Springer, 2007b.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. CoRR, abs/1803.09050, 2018. URL http://arxiv.org/abs/1803.
09050.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694, 2017.
Brendan van Rooyen, Aditya Krishna Menon, and Robert C. Williamson. Learning with symmet-
ric label noise: The importance of being unhinged. In Proceedings of the 28th International
Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pp. 10-18, Cam-
bridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=
2969239.2969241.
Joseph L. Schafer and John W. Graham. Missing data: our view of the state of the art. Psychological
Methods, 7 2:147-77, 2002.
Rocco A Servedio. On pac learning using winnow, perceptron, and a perceptron-like algorithm. In
Proceedings of the twelfth annual conference on Computational learning theory, pp. 296-307.
ACM, 1999.
Padhraic Smyth. Bounds on the mean classification error rate of multiple experts. Pattern
Recognition Letters, 17(12):1253 - 1257, 1996. ISSN 0167-8655. doi: https://doi.org/
10.1016/0167-8655(96)00105-5. URL http://www.sciencedirect.com/science/
article/pii/0167865596001055.
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.
Jiang-wen Sun, Feng-ying Zhao, Chong-jun Wang, and Shi-fu Chen. Identifying and correcting mis-
labeled training instances. In Future generation communication and networking (FGCN 2007),
volume 1, pp. 244-250. IEEE, 2007.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network.
arXiv preprint arXiv:1609.03126, 2016.
Xingquan Zhu and Xindong Wu. Class noise vs. attribute noise: A quantitative study. Artificial Intel-
ligence Review, 22(3):177-210, Nov 2004. ISSN 1573-7462. doi: 10.1007/s10462-004-0751-8.
URL https://doi.org/10.1007/s10462-004-0751-8.
11
Under review as a conference paper at ICLR 2019
Appendices
A	Effect of Label Noise on Empirical Risk
Here we show how label noise affects optimization of empirical risk. Recall that our task is to
improve the labels of a training set in order to enable effective learning. In the following sections,
we explore the Type I, Type II, and Type III noise. We first introduce the key variables.
We focus on binary classification, where the probability of class 0 is p and consequently, the proba-
bility of class 1 is (1 - p):
p(y = 0) = p
p(y = 1) = 1 - p
T represents the training set with correct labels, while T represents the training set with label noise.
In our setting, we are given T and propose a method to transform T into T. We will call D the true
label distribution and P the noisy label distribution.
T = {(Xi IYi) for i = 1, 2,...,n}
T = {区,Yi) for i = 1, 2,...,n}
In the following calculations, we describe the difference in empirical risks over each training set and
the subsequent effect on stochastic gradient descent. We refer to the empirical risk over the clean
training set as R [`, T]. Similarly, we refer to the empirical risk over the corrupted training set as
R [',T].
1n
R[', Ti = -Σ' (φ(Xi, θ),γi),
n i=1
In order to explain the effects of label noise on stochastic gradient descent (SGD), we are primarily
interested in the gradient of the empirical risk with respect to θ. Below, we break the empirical risk
term into two class-dependent terms and introduce short-hand to represent each of these terms.
E(X,Y)〜D [Vθ R[', T]]
1n
E(X,Y)〜D Vθ — E' (Φ(Xi, θ),Yi)
n
i=1
1n
vθ — EX E(X,Y)〜D [' (φ(xi, θ), Yi)]
n i=1
Vθ
Vθ
-X Z
i=1
p(xi,yi)' (Φ(xi, θ),yi)
dxi dyi
-X Z
i=1
p(χi∣yi)p(yi)' (Φ(χi, θ),yi)
dxi dyi
p(yi = 0)Vθ
p(yi = )Vθ
-X Z
i=1
p(xi∣0)'o (Φ(xi, θ)) dxi
(11)
(12)
(13)
(14)
(15)
-n
—/ p(xi∣1)'ι (Φ(xi, θ)) dxi
n i=1
+
We define shorthand based on Equation (5):
12
Under review as a conference paper at ICLR 2019
βo = Vθ
1X Z
i=1
p(xi∣0)'o (Φ(xi, θ)) dxi
β1 = Vθ
1X Z
i=1
p(xi∣1)'ι (Φ(xi, θ)) dxi
(16)
(17)
Rewriting the original equation in terms of the above, we arrive at the following form for empirical
risk:
E(X,Y )~D hvθ R[', T] i = p(yi = O)β° + p(yi = I)β°	(18)
We revisit Equation 8 in the follow sections to interpret empirical risk under varying noise models.
Note that β0 and β1 are the same in both the expected empirical risk with respect to the true label
distribution (D) and the corrupted label distribution (P) for Type I and Type II noise. Additionally,
we introduce shorthand for the true expectation of empirical risk (Etrue) and the expectation of
empirical risk when labels are assigned at random (Erand).
Etrue = pβ0 + (1 - p)β1
Erand = .5β0 +.5β1
(19)
(20)
A.1 Type I: Random Noise
In the random noise scenario, each label has an equal probability of being flipped. We parameterize
this noise by γ, resulting in :
Y = P(Ii = yi)	QI)
We may define both p(yIi = 0) in terms of γ and p:
p(yIi = 0) = p(yIi = 0|yi = 0) + p(yIi = 0|yi =1)	(22)
p(yIi = 0) = (1 - γ)p + γ(1 - p)	(23)
(24)
Likewise, we define p(yIi = 1) as:
p(yIi =1) = p(yIi = 1|yi = 0) + p(yIi = 1|yi =1)	(25)
p(yIi =1) = γp+ (1- γ)(1-p)	(26)
Below, we substitute these expressions into Equation 8:
E(x,Y)~P	[VθR[',T]i	=	P(yi = O)βo + P(yi = 1)βo	(27)
=	((1- γ)p+ γ - γp)β0 + ((1- γ) (1-p) + γp) β1	(28)
=	pβ0 - 2γpβ0 + γβ0 + β1 - γβ1 - pβ1 + 2γpβ1	(29)
=	pβ0 + β1 - pβ1 - 2γpβ0 - γβ1 + 2γpβ1 + γβ0	(30)
=	Etrue - 2γpβ0 - γβ1 + 2γpβ1 + γβ0	(31)
=	Etrue - 2γpβ0 - γβ1 + 2γpβ1 + γβ0 - γβ1 + γβ1	(32)
=	Etrue - 2γpβ0 - 2γβ1 + 2γpβ1 + γβ0 + γβ1	(33)
=	Etrue-2γ(pβ0+β1 -pβ1)+γβ1+γβ0	(34)
=	(1 - 2γ) Etrue + 2γErand	(35)
13
Under review as a conference paper at ICLR 2019
A.2 Type II Noise
Type II noise implies class-dependent label noise. Thus, we define γ0 and γ1 , the class-dependent
noise rates. In addition, We define γ* as the average class-dependent noise rate and E as the absolute
distance of γo and γι from Y*. Without loss of generality, we assume γo > γι.
γ0
γ1
*
Y
p(yi = 1|yi = O)
p(yu = 0|yi = I)
Y0 + Y1
-2-
Y0 - Y1
-2-
We may redefine p(yi = 0) and p& = 1) using these terms:
p(yi = 0)=	((1 - Y0) p + Y1 (1 - p))	(36)
	((1 - Y* - E)p+ (Y* - E) (1 -p))	(37)
	(p - Y*p - Ep + Y* - Y*p - E + Ep)	(38)
p(yi = I) = ((I - YI)(I - P) + YOp)	(39)
=	((1	- Y	* + E) (1 - p) + (Y * +	E) p)	(40)
=	(1 - Y *	+ E - p + Y *p - Ep +	Y *p + Ep)	(41)
Plugging this into Equation 8,
E(X,Y)〜P hvθR[',	Tli	=	(P	- Y *p -Ep + Y * - Y*p -E	+ ep)β0	(42)
+	(1	- Y* + E - p + Y*p -	Ep +	Y*p + Ep)β1
= (p - 2Y*p + Y* - E)β0	(43)
+	(1 - 2Y* + E - p + 2Y*p + Y*)β1
Similar to our approach with Type I noise, we aim to rephrase this equation in terms of Etrue and
Erand.
(1 -2Y*)pβ0+(Y* -E)β0+(1 -2Y*)(1 -p)β1+(E+Y*)β1	(44)
(1 - 2Y* ) Etrue + (Y* - E) β0 + (E + Y*) β1	(45)
(1 - 2Y* ) Etrue + 2Y * Erand + E (β1 - β0)	(46)
A.3 Type III Noise
In this section, we analyze a specific case of type III noise where label corruption is restricted to a
given region of the feature space. As above, we want to study the effect on learning by determining
how the true gradient is modified in the presence of noise. We assume a scalar input space and define
the feature dependent noise as follows:
p(yi|x)
Y x ∈ [a, b]
0 x ∈/ [a, b]
The noise model above asserts that label noise depends only on the feature space and is label-
agnostic - thus, examples from both class 1 and class 0 with x ∈ [a, b] are equally likely to suffer
from a label flip.
14
Under review as a conference paper at ICLR 2019
Revisiting
E(x,Y)〜P [vθR[', T]i
V0
V0
1X Z
i=1
1n
n X=XZ
p(yi∣χi)p(χi)' (Φ(χi, θ),yi) dχ dm ,
b
p(yi∣Xi)p(xi)' (Φ(xi, θ), yi) dxi dyi
+	Vθ
1n
p Sy	p(yi∣χi)p(χi)' (Φ(χi, θ),yi) dxi dyi .
n i=1 x∈/ [a,b]
Now we add and subtract the following term:
V0
1 n b
_£	p(yi∣χi)p(χi)' (Φ(χi, θ),yi) dx% dy
n i=1 a
(47)
In order to obtain:
E(X,Y)〜P [vθR[', T]i
Etrue + V0
1 n b
p(yi∣xi)p(xi)' (Φ(xi, θ),yi) dx% dy
n i=1 a
—
V0
1n b
n∑∕ p(yi∣Xi)p(xi)' (Φ(xi, θ), yi) dx% dy
n i=1 a
Etrue + V0
Where we have used the fact that:
1 n b
-V" ( (p(yi∣χi) - p(y∕χi)) P(Xi)' (Φ(χi, θ),yi) dx% dy%
n
i=1 a
1 n b
Etrue = Vθ 一£ I p(yi∣χi)p(χi)' (Φ(χi, θ),yi) dxi dyi
n i=1 a
1n
+ vθ - V' /	p(yi∣Xi)p(χi)' (Φ(χi, θ),yi) dxi dy
n i=1 x∈/ [a,b]
B	Algorithm Pseudo-Code
In this section, we include a more detailed form of the algorithms proposed in the paper.
15
Under review as a conference paper at ICLR 2019
Algorithm 1 Train energy-based autoencoder
Require: T
Require: T
Require: η
Require: Forward (x, θ)
Require: E (X)
Require: L (B, O; θ)
Require: GetBalancedBatch (T )
1:	for i in {1,2,. . . ,m} do
2:	Bi = {xj,yj } — GetBalancedBatch (T)
3:	Oi = {xj} — Forward (Bi, θi)
4：	'i JL (Bi, Oi； θi)
5:	Vθ ('i) J Backward ('%; θi)
6：	θi+ι J Step (θi,η, Vθ (`i))
7:	end for
8:	Initialize: E = {}
9:	for {xi,yi} in T do
10:	Xi J Forward (xi； θ?)
11:	ei J E (Xi)
12:	E[i] J ei
13:	end for
. full training data
. training data with known labels
. learning rate
.autoencoder transformation that maps ∀x → X
. energy function, i.e. reconstruction loss
. objective function as defined in eq. 5
. get mini-batch with an equal number
of instances from each class
. where m is total number of batches
. L (Bi , Oi ; θi ) is defined in Eq. 5
. empty dictionary that will store energies
. where θ? is the output of previous loop
. E[i] is the value of E at index i
Algorithm 2 Correct training data
Require: T	. full training data
Require: T	. training data with known labels
Require: E	. energies from algorithm 1
Require: E (X)	. energy function, i.e. reconstruction loss
Require: GetNegativeSamples (T)	. get all samples from class 0
Require: GetPositiveSamples (T)	. get all samples from class 1
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
T0 J GetNegativeSamples (T)
T1 J GetPositiveSamples (T)
E0 J ∣T1 *0∣ Pχ∈T0 E (X)
EI J ∣⅛ Pχ∈Tι E (X)
_	1
L	(E3θ)-E0)2 2
σ0 J 乙χ∈T0	∣T0∣-1
1	_	1
「P	(E(x；e)-Ei)2 2
σ1 J 乙x∈T1	∣Tι∣-1
TC J T ∖t
for {Xi , yi } in Tc do
ei J E[i]
if yi = 1 and ei < Eo + σ0 then
yi J 0
else if yi = 0 and e% > Ei — σ0 then
yi J 1
end if
end for
. get mean w.r.t class 0
. get mean w.r.t class 1
. get standard deviation w.r.t class 0
. get standard deviation w.r.t class 1
.i.e. TC = {3,yi) ∈T k(Xi,yi) ∈ T}
. get energy corresponding to Xi
16