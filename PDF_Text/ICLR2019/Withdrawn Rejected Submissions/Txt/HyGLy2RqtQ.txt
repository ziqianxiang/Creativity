Under review as a conference paper at ICLR 2019
Over-parameterization Improves Generaliza-
tion in the XOR Detection Problem
Anonymous authors
Paper under double-blind review
Ab stract
Empirical evidence suggests that neural networks with ReLU activations general-
ize better with over-parameterization. However, there is currently no theoretical
analysis that explains this observation. In this work, we study a simplified learning
task with over-parameterized convolutional networks that empirically exhibits the
same qualitative phenomenon. For this setting, we provide a theoretical analysis
of the optimization and generalization performance of gradient descent. Specifi-
cally, we prove data-dependent sample complexity bounds which show that over-
parameterization improves the generalization performance of gradient descent.
1	Introduction
Most successful deep learning models use a number of parameters that is larger than the num-
ber of parameters that are needed to get zero-training error. This is typically referred to as over-
parameterization. Indeed, it can be argued that over-parameterization is one of the key techniques
that has led to the remarkable success of neural networks. However, there is still no theoretical
account for its effectiveness.
One very intriguing observation in this context is that over-parameterized networks with ReLU ac-
tivations, which are trained with gradient based methods, often exhibit better generalization error
than smaller networks (Neyshabur et al., 2014; 2018; Novak et al., 2018). This somewhat counter-
intuitive observation suggests that first-order methods which are trained on over-parameterized net-
works have an inductive bias towards solutions with better generalization performance. Understand-
ing this inductive bias is a necessary step towards a full understanding of neural networks in practice.
Providing theoretical guarantees for this phenomenon is extremely challenging due to two main rea-
sons. First, to show a generalization gap, one needs to prove that large networks have better sample
complexity than smaller ones. However, current generalization bounds that are based on complex-
ity measures do not offer such guarantees. Second, analyzing the dynamics of first-order methods
on networks with ReLU activations is a major challenge. Indeed, there do not exist optimization
guarantees even for simple learning tasks such as the classic XOR problem in two dimensions. 1
To advance this issue, we focus on a particular learning setting that captures key properties of the
over-parameterization phenomenon. We consider a high-dimensional extension of the XOR prob-
lem, which we refer to as the “XOR Detection problem (XORD)”. The XORD is a pattern recogni-
tion task where the goal is to learn a function which classifies binary vectors according to whether
they contain a two-dimensional binary XOR pattern (i.e., (1, 1) or (-1, -1)). This problem con-
tains the classic XOR problem as a special case when the vectors are two dimensional. We consider
learning this function with gradient descent trained on an over-parameterized convolutional neural
network (i.e., with multiple channels) with ReLU activations and three layers: convolutional, max
pooling and fully connected. As can be seen in Fig. 1, over-parameterization improves generaliza-
tion in this problem as well. Therefore it serves as a good test-bed for understanding the role of
over-parameterization.
1We are referring to the problem of learning the XOR function given four two-dimensional points with
binary entries, using a moderate size one-hidden layer neural network (e.g., with 50 hidden neurons). Note that
there are no optimization guarantees for this setting. Variants of XOR have been studied in Lisboa & Perantonis
(1991); Sprinkhuizen-Kuyper & Boers (1998) but these works only analyzed the optimization landscape and
did not provide guarantees for optimization methods. We provide guarantees for this problem in Sec. 9.
1
Under review as a conference paper at ICLR 2019
Figure 1: Over-parameterization improves generalization in the XORD problem. The network in Eq. 1 and
Fig. 4 is trained on data from the XORD problem (see Sec. 3). The figure shows the test error obtained for
different number of channels k. The blue curve shows test error when restricting to cases where training error
was zero. It can be seen that increasing the number of channels improves the generalization performance.
Experimental details are provided in Section 8.2.1.
In this work we provide an analysis of optimization and generalization of gradient descent for
XORD. We show that for various input distributions, ranges of accuracy and confidence parame-
ters, sufficiently over-parameterized networks have better sample complexity than a small network
which can realize the ground truth classifier. To the best of our knowledge, this is the first example
which shows that over-paramaterization can provably improve generalization for a neural network
with ReLU activations.
Our analysis provides a clear distinction between the inductive bias of gradient descent for over-
parameterized and small networks. It reveals that over-parameterized networks are biased towards
global minima that detect more patterns in the data than global minima found by small networks. 2
Thus, even though both networks succeed in optimization, the larger one has better generalization
performance. We provide experiments which show that the same phenomenon occurs in a more gen-
eral setting with more patterns in the data and non-binary input. We further show that our analysis
can predict the behavior of over-parameterized networks trained on MNIST and guide a compression
scheme for over-parameterized networks with a mild loss in accuracy (Sec. 6).
2	Related Work
In recent years there have been many works on theoretical aspects of deep learning. We will refer to
those that are most relevant to this work. First, we note that we are not aware of any work that shows
that generalization performance provably improves with over-parameterization. This distinguishes
our work from all previous works.
Several works study convolutional networks with ReLU activations and their properties (Du et al.,
2017a;b; Brutzkus & Globerson, 2017). All of these works consider convolutional networks with
a single channel. Daniely (2017) and Li & Liang (2018) provide guarantees for SGD in general
settings. However, their analysis holds for over-parameterized networks with an extremely large
number of neurons that are not used in practice (e.g., the number of neurons is a very large poly-
nomial of certain problem parameters). Furthermore, we consider a 3-layer convolutional network
with max-pooling which is not studied in these works.
Soltanolkotabi et al. (2018), Du & Lee (2018) and Li et al. (2017) study the role of over-
parameterization in the case of quadratic activation functions. Brutzkus et al. (2018) provide gen-
eralization guarantees for over-parameterized networks with Leaky ReLU activations on linearly
separable data. Neyshabur et al. (2018) prove generalization bounds for neural networks. However,
these bounds are empirically vacuous for over-parameterized networks and they do not prove that
networks found by optimization algorithms give low generalization bounds.
2See Definition 5.1 for a formal definition of detection of a pattern.
2
Under review as a conference paper at ICLR 2019
3	Problem Formulation
We begin with some notations and definitions. Let d ≥ 4 be an integer. We consider a classification
problem in the space {±1}2d. Namely, the space of vectors of 2d coordinates where each coordinate
can be +1 or -1. Given a vector x ∈ {±1}2d, we consider its partition into d sets of two coordinates
as follows x = (x1, ..., xd) where xi ∈ {±1}2. We refer to each such xi as a pattern in x.
Neural Architecture: We consider learning with the following three-layer neural net model. The
first layer is a convolutional layer with non-overlapping filters and multiple channels, the second
layer is max pooling and the third layer is a fully connected layer with 2k hidden neurons and
weights fixed to values ±1. Formally, for an input x = (x1, ..., xd) ∈ R2d where xi ∈ R2, the
output of the network is given by:
NW (x)
(1)
where W ∈ R2k×2 is the weight matrix whose rows are the w(i) vectors followed by the u(i)
vectors, and σ(x) = max{0, x} is the ReLU activation applied element-wise. See Figure 4 for an
illustration of this architecture.
Remark 3.1. Because there are only 4 different patterns, the network is limited in terms of the
number of different rules it can implement. Specifically, it is easy to show that its VC dimension is at
most 15 (see Sec. 10). Despite this limited expressive power, there is a generalization gap between
small and large networks in this setting, as can be seen in Figure 1, and in our analysis below.
Data Generating Distribution: Next we define the classification rule we will focus on. Let
PXOR correspond to the following two patterns: PXOR = {(1, 1), (-1, -1)}. Define the clas-
sification rule:
f*(x) = { -11
∃i ∈ {1, . . . , d} : xi ∈ PXOR
otherwise
(2)
Namely, f * detects whether a pattern in PXOR appears in the input. In what follows, We refer to
PXOR as the set of positive patterns and {±1}2 \ PXOR as the set of negative patterns.
Let D be a distribution over X X {±1} such that for all (x, y) ~ D we have y = f *(x). We say
that a point (x, y) is positive if y = 1 and negative otherwise. Let D+ be the marginal distribution
over {±1}2d of positive points and D- be the marginal distribution of negative points.
In the following definition we introduce the notion of diverse points, which will play a key role in
our analysis.
Definition 3.2 (Diverse Points). We say that a positive point (x, 1) is diverse if for all z ∈ {±1}2
there exists 1 ≤ i ≤ d such that xi = z. We say that a negative point (x, -1) is diverse if for all
z ∈ {±1}2 \ PXOR there exists 1 ≤ i ≤ d such that xi = z.
For φ ∈ {-, +} define pφ to be the probability that x is diverse with respect to Dφ. For example,
if both D+ and D- are uniform, then by the inclusion-exclusion principle it follows that p+ =
1 - 4∙3d-62d+4 andP- = 1 - ɪ.
For each set of binary patterns A ⊆ {±1}2 define pA to be the probability to sample a point which
contains all patterns in A and no patterns in Ac (the complement of A). Let A1 = {2}, A2 = {4},
A3 = {2, 4, 1} and A4 = {2, 4, 3}.The following quantity will be useful in our analysis:
p* = min pAi	(3)
1≤i≤4
Learning Setup: Our analysis will focus on the problem of learning f * from training data with a
three layer neural net model. The learning algorithm will be gradient descent, randomly initialized.
As in any learning task in practice, f * is unknown to the training algorithm. Our goal is to analyze
the performance of gradient descent when given data that is labeled with f*. We assume that we are
given a training set S = S+ ∪ S- ⊆ {±1}2d × {±1}2 where S+ consists of m IID points drawn
from D+ and S- consists of m IID points drawn from D-.3
3For simplicity, we consider this setting of equal number of positive and negative points in the training set.
3
Under review as a conference paper at ICLR 2019
Importantly, We note that the function f * can be realized by the above network with k = 2. Indeed,
the network N defined by the filters w(1) = (3, 3), w(2) = (-3, -3), u(1) = (-1, 1), u(2) =
(1, -1) satisfies sign (N (x)) = f* (x) for all x ∈ {±1}2d. It can be seen that for k = 1, f* cannot
be realized. Therefore, any k > 2 is an over-parameterized setting.
Training Algorithm: We will use gradient descent to optimize the following hinge-loss function.
'(W) = m1	X	max{γ - NW(xi), 0} + ；	X	max{1 + NW(xi), 0} (4)
(Xi ,yi)∈S+:yi = 1	(xi,yi)∈S'.yi = -1
for γ ≥ 1. * 1 * * 4We assume that gradient descent runs with a constant learning rate η and the weights
are randomly initiliazed with IID Gaussian weights with mean 0 and standard deviation σg . Further-
more, only the weights of the first layer, the convolutional filters, are trained. 5 * *
We will need the following notation. Let Wt be the weight matrix in iteration t of gradient descent.
For 1 ≤ i ≤ k, denote by wt ∈ R the ith convolutional filter at iteration t. Similarly, for
1 ≤ i ≤ k we define ut(i) ∈ R2 to be the k+i convolutional filter at iteration t. We assume that each
(i)	(i)	t
w0 and u0 is initialized as a Gaussian random variable where the entries are IID and distributed
as N(0, σg). In each iteration, gradient descent performs the update Wt+ι = Wt - η箓(Wt).
4 Main Result
In this section we state our main result that demonstrates the generalization gap between over-
parameterized networks and networks with k = 2. Define the generalization error to be the differ-
ence between the 0-1 test error and the 0-1 training error. For any , δ and training algorithm let
m(, δ) be the sample complexity of a training algorithm, namely, the number of minimal samples
the algorithm needs to get at most generalization error with probability at least 1 - δ. We consider
running gradient descent in two cases, when k ≥ 120 and k = 2. In the next section we exactly
define under which set of parameters gradient descent runs, e.g., which constant learning rates.
Fix parameters p+ and p- of a distribution D and denote by c < 10-10 a negligible constant.
Assume that gradient descent is given a sample of points drawn from D+ and D-. We denote the
sample complexity of gradient descent in the cases k ≥ 120 and k = 2, bym1 andm2, respectively.
The following result shows a data dependent generalization gap (recall the definition of p* in Eq. 3).
Theorem 4.1. Let D be a distribution with paramaters p+, p- andp*. Let δ ≥ 1 - p+p-(1 -
c-
16e-8) and 0 ≤ < p*. Then m1 (, δ) ≤ 2 whereas m2 (, δ) ≥
2log( 3348⅛ )
Iog(P+P-)
6
The proof follows from Theorem 5.2 and Theorem 5.3 which we state in the next section. The proof
is given in Sec. 8.8. One surprising fact of this theorem is that m1(0, δ) ≤ 2. Indeed, our analysis
shows that for an over-parameterized network and for sufficiently large p+ and p-, one diverse
positive point and one diverse negative suffice for gradient descent to learn f * with high probability.
We note that even in this case, the dynamics of gradient descent is highly complex. This is due to the
randomness of the initialization and to the fact that there are multiple weight filters in the network,
each with different dynamics. See Sec. 5 for further details.
We will illustrate the guarantee of Theorem 4.1 with several numerical examples. In all of the
examples we assume that for the distribution D, the probability to sample a positive point is 2 and
4In practice it is common to set γ to 1. In our analyis we will need γ ≥ 8 to guarantee generalization. In
Section 8.3 we show empirically, that for this task, setting γ to be larger than 1 results in better test performance
than setting γ = 1.
5 Note that Hoffer et al. (2018) show that fixing the last layer weights to ±1 does not degrade performance
in various tasks. This assumption also appeared in other works (Brutzkus et al., 2018; Li & Yuan, 2017).
6We note that this generalization gap holds for global minima (0 train error). Therefore, the theorem can be
read as follows. For k ≥ 120, given 2 samples, with probability at least 1 - δ, gradient descent converges to
a global minimum with at most test error. On the other hand, for k = 2 and given number of samples less
than
2lθg( 3348-C))
log(P+P-)
, with probability greater than δ, gradient descent converges to a global minimum with error
greater than . See Section 5 for further details.
4
Under review as a conference paper at ICLR 2019
min{与p+, N}
*
P
(it is easy to constuct such distributions). In the first example, we assume
that p+ = p- = 0.98 and δ = 1 - 0.982(1 - c - 16e-8) ≤ 0.05. In this case we get that for
any 0 ≤	< 0.005, m1(, δ) ≤ 2 whereas m2(, δ) ≥ 129. For the second example consider the
case where p+ = p- = 0.92. It follows that for δ = 0.16 and any 0 ≤	< 0.02 it holds that
m1(, δ) ≤ 2 and m2 (, δ) ≥ 17.
For = 0 and any δ > 0, by setting p+ and p- to be sufficiently close to 1, we can get an arbitrarily
large gap between m1(, δ) and m2(, δ). In contrast, for sufficiently small p+ and p- , e.g., in which
p+ , p- ≤ 0.7, our bound does not guarantee a generalization gap.
5 Proof S ketch and Insights
In this section we sketch the proof of Theorem 4.1. The theorem follows from two theorems: The-
orem 5.2 for over-parameterized networks and Theorem 5.3 for networks with k = 2. We formally
show this in Sec. 8.8. In Sec. 5.1 we state Theorem 5.2 and outline its proof. In Sec. 5.2 we state
Theorem 5.3 and shortly outline its proof. Finally, for completeness, in Sec. 9 we also provide a
convergence guarantee for the XOR problem with inputs in {±1}, which in our setting is the case
of d = 1. In what follows, we will need the following formal definition for a detection of a pattern
by a network.
Definition 5.1. Let cd ≥ 0 be a constant. For each positive pattern vp define Dvp =
Pk=I σ (w(i) ∙ Vp) and for each negative pattern Vn define Dvn = Pk=1 σ (u(i) ∙ Vn). We say
that a pattern v (positive or negative) is detected by the network NW with confidence cd ifDv > cd.
The above definition captures a desired property of a network, namely, that its filters which are
connected with a positive coefficient in the last layer, have high correlation with the positive patterns
and analogously for the remaining filters and negative patterns. We note however, that the condition
in which a network detects all patterns is not equivalent to realizing the ground truth f*. The former
can hold without the latter and vice versa.
Theorem 5.2 and Theorem 5.3 together imply a clear characterization of the different inductive
biases of gradient descent in the case of small (k = 2) and over-parameterized networks. The
characterization is that over-parameterized networks are biased towards global minima that detect
all patterns in the data, whereas small networks with k = 2 are biased towards global minima that
do not detect all patterns (see Definition 5.1). In Sec. 8.5 we show this empirically in the XORD
problem and in a generalization of the XORD problem.
In the following sections we will need several notations. Define x1 = (1, 1), x2 = (1, -1), x3 =
(-1, -1), x4 = (-1, 1) to be the four possible patterns in the data and the following sets:
j | arg max Wyj ∙ xι = i > , U+ (i)
1≤l≤4
j | arg max u(j) ∙ xι = i
1≤l≤4
W- (i) = < j | arg max w(j) ∙ xι = i > , U- (i) = < j | arg max Utj) ∙ xι = i >
l∈{2,4}	l∈{2,4}
(5)
We denote by x+ a positive diverse point and x- a negative diverse point. Define the following
sum:
S+ =	^X	[max {σ (w(j) ∙x+),…,σ (w(j) ∙x+)}]
j∈Wt+(1)∪Wt+(3)
Finally, in all of the results in this section we will denote by c < 10-10 a negligible constant.
5.1 Sample Complexity Upper Bound for Over-parameterized Networks
The main result in this section is given by the following theorem.
Theorem 5.2. Let S = S+ ∪ S- be a training set as in Sec. 3. Assume that gradient descent runs
with parameters η = Cn where 5 ≤ 击,σg ≤ -cητ, k ≥ 120 and Y ≥ 8. Then, with probability
5
Under review as a conference paper at ICLR 2019
at least (p+p-)m(1 — C — 16e-8) after running gradient descent for T ≥ 28(Y+1+8Cn) iterations,
it converges to a global minimum which satisfies:
1. sign (NWT(X)) = f *(x) for all X ∈ {±1}2d.
2. Let α(k)
k +2√k e,	/ 1-5Cη	„	7	7 . ,	C 7
4—√^. Thenfor Cd ≤ 仪⑹; ι, all patterns are detected With confidence Cd.
This result shows that given a small training set size, and sufficiently large p+ and p- , over-
parameterized networks converge to a global minimum which realizes the classifier f * with high
probability and in a constant number of iterations. Furthermore, this global minimum detects all
patterns in the data with confidence that increases with over-parameterization. The full proof of
Theorem 5.2 is given in Sec. 8.6.
We will now sketch its proof. With probability at least (p+p-)m all training points are diverse and
we will condition on this event. From Sec. 10 we can assume WLOG that the training set consists
of one positive diverse point X+ and one negative diverse point X- (since the network will have the
same output on all same-label diverse points). We note that empirically over-parameterization im-
proves generalization even when the training set contains non-diverse points (see Fig. 1 and Sec. 8.2).
Now, to understand the dynamics of gradient descent it is crucial to understand the dynamics of the
sets in Eq. 5. This follows since the gradient updates are expressed via these sets. Concretely, let
j ∈ Wt+(i1) ∩ Wt-(i2) then the gradient update is given as follows: 7
(j)	(j)
wt+1 = Wt + ηxiι ɪNW(x+)<γ - ηxi2 ɪNW(x- )<1
Similarly, for j ∈ Ut+(i1) ∩ Ut-(i2) the gradient update is given by:
ut+1 = ut - - ηxiι INW (x+ )<γ + ηxi2 INW (x-)<1
(6)
(7)
Furthermore, the values of NW(X+) and NW(X-) depend on these sets and their corresponding
weight vectors, via sums of the form St+, defined above.
The proof consists of a careful analysis of the dynamics of the sets in Eq. 5 and their corresponding
weight vectors. For example, one result of this analysis is that for all t ≥ 1 and i ∈ {1, 3} we have
W+ (i) = W+ (i) and the size of W+ (i) is at least 号-2√k with high probability.
There are two key technical observations that we apply in this analysis. First, with a small initial-
ization and with high probability, for all 1 ≤ j ≤ k and 1 ≤ i ≤ 4 it holds that ∣w Oj) ∙ Xil ≤ 4 and
IuOj) ∙ Xi ∣ ≤ 4. This allows US to keep track of the dynamics of the sets in Eq. 5 more easily. For
example, by this observation it follows that if for some j* ∈ Wt+(2) it holds that j* ∈ Wt++1(4),
then for all j such that j ∈ Wt+(2) it holds that j ∈ Wt++1(4). Hence, we can reason about the
dynamics of several filters all at once, instead of each one separately. Second, by concentration of
measure we can estimate the sizes of the sets in Eq. 5 at iteration t = 0. Combining this with results
of the kind Wt+(i) = WO+(i) for all t, we can understand the dynamics of these sets throughout the
optimization process.
The theorem consists of optimization and generalization guarantees. For the optimization guarantee
we show that gradient descent converges to a global minimum. To show this, the idea is to char-
acterize the dynamics of St+ using the characterization of the sets in Eq. 5 and their corresponding
weight vectors. We show that as long as gradient descent did not converge to a global minimum,
St+ cannot decrease in any iteration and it is upper bounded by a constant. Furthermore, we show
that there cannot be too many consecutive iterations in which St+ does not increase. Therefore, after
sufficiently many iterations gradient descent will converge to a global minimum.
We will now outline the proof of the generalization guarantee. Denote the network learned by
gradient descent by NWT . First, we show that the network classifies all positive points correctly.
7Note that with probability 1, σ0(w(j) ∙ XiJ = 1, σ0(w(j) ∙ Xi2) = 1 for all t, and therefore We omit these
from the gradient update. This follows since σ0(w(j) ∙ XiJ = 0 for some t only if WOj) ∙ x由 is an integer
multiple of η.
6
Under review as a conference paper at ICLR 2019
Define the following sums for all 1 ≤ i ≤ 4:
Xt+ (i) =	X max σ	(w(j)	∙	xk+	,	Yt+ (i)	= X max σ	(u(j)	∙ x+)	(8)
j∈WT+(i)	j∈UT+(i)
First we notice that for all positive z we have NWT (z) ' min{XT+(1), XT+(3)} - YT+(2) - YT+(4).
Then by the fact that NWT (x+) ≥ γ at the global minimum, we can show that XT+ (1) + XT+ (3)
is sufficiently large. As mentioned previously, by concentration of measure, W0+ (i) is sufficiently
large. Then by a symmetry argument we show that this implies that both XT+ (1) and XT+ (3) are
sufficiently large. This shows that patterns x1 and x3 are detected. Finally, we show that YT+ (2) +
YT+(4) is not too large due to an upper bound on -NWT (x-). Hence, we can show that each
positive point is classified correctly. The proof that all negative points are classified correctly and
patterns x2 and x4 are detected is similar but slightly more technical. We refer the reader to Sec. 8.6
for further details.
5.2 SAMPLE COMPLEXITY LOWER BOUND FOR SMALL NETWORKS (k = 2)
The following theorem provides generalization lower bounds of global minima in the case that k = 2
and in a slightly more general setting than the one given in Theorem 5.2.
Theorem 5.3. Let S = S+ ∪ S- be a training set as in Sec. 3. Assume that gradient descent runs
with parameters η = ^n where c” ≤ *, σg ≤ ɪ^ɪ, k = 2 and Y ≥ L Then IhefoUowing holds:
1.	With probability at least (p+p-)m (1 一 C) 43, gradient descent converges to a global min-
imum that has non-zero test error. Furthermore, for cd ≥ 2cη, there exists at least one
pattern which is not detected by the global minimum with confidence cd.
2.	The non-zero test error above is at least p*.
The theorem shows that for a training set that is not too large and given sufficiently large p+ and
p- , with constant probability, gradient descent will converge to a global minimum that is not the
classifier f *. Furthermore, this global minimum does not detect at least one pattern. The proof of
the theorem is given in Sec. 8.7.
We will now provide a short outline of the proof. Let w(T1), w(T2), u(T1) and u(T2) be the filters
of the network at the iteration T in which gradient descent converges to a global minimum. The
proof shows that gradient descent will not learn f * if one of the following conditions is met: a)
W+ ⑴=0. b) Wt+(3) = 0. c) uT • X2 > 0 and UT2 ∙ x2 > 0. d) U1) ∙ x4 > 0 and UT) ∙ x4 > 0.
Then by using a symmetry argument which is based on the symmetry of the initialization and the
training data it can be shown that one of the above conditions is met with high constant probability.
Finally, it can be shown that if one of these conditions hold, then at least one pattern is not detected.
6	Experiments
We perform several experiments that corroborate our theoretical findings. In Sec. 8.5 we empirically
demonstrate our insights on the inductive bias of gradient descent. In Sec. 6.2 we evaluate a model
compression scheme implied by our results, and demonstrate its success on the MNIST dataset.
6.1	Pattern Detection
In this section we perform experiments to examine the insights from our analysis on the inductive
bias of gradient descent. Namely, that over-parameterized networks are biased towards global min-
ima that detect more patterns in the data than global minima found by smaller networks. We check
this both on the XORD problem which contains 4 possible patterns in the data and on an instance
of an extension of the XORD problem, that we refer to as the Orthonormal Basis Detection (OBD)
problem, which contains 60 patterns in the data. In Sec. 8.5 we provide details on the experimental
setups.
7
Under review as a conference paper at ICLR 2019
(a)
(b)	(c)
Figure 2: Inductive bias in XORD and OBD problems. (a) Over-parameterization improves generalization in
the OBD problem (b) Pattern detection phenomenon in the XORD problem (c) Pattern detection phenomenon
in the OBD problem. In both (b) and (c) we see that as the number of channels increase, gradient descent is
biased towards %0 training error solutions with more detected patterns.
Figure 3: Model compression in MNIST. The plot shows the test error of the small network (4 channels) with
standard training (red), the small network that uses clusters from the large network (blue), and the large network
(120 channels) with standard training (green). It can be seen that the large network is effectively compressed
without losing much accuracy.
Due to space considerations, we will not formally define the OBD problem in this section. We
refer the reader to Sec. 8.5 for a formal definition. Informally, The OBD problem is a natural
extension of the XORD problem that contains more possible patterns in the data and allows the
dimension of the filters of the convolutional network to be larger. The patterns correspond to a set of
orthonormal vectors and their negations. The ground truth classifier in this problem can be realized
by a convolutional network with 4 channels.
In Fig. 2 we show experiments which confirm that in the OBD problem as well, overparameteri-
zation improves generalization. We further show the number of patterns detected in %0 training
error solutions for different number of channels, in both the XORD and OBD problems. It can be
clearly seen that for both problems, over-parameterized networks are biased towards %0 training
error solutions that detect more patterns, as predicted by the theoretical results.
6.2	Network Compression
By inspecting the proof of Theorem 5.2, one can see that the dynamics of the filters of an over-
parameterized network are such that they either have low norm, or they have large norm and they
point to the direction of one of the patterns (see, e.g., Lemma 8.4 and Lemma 8.6). This suggests
that by clustering the filters of a trained over-parameterized network to a small number of clusters,
one can create a significantly smaller network which contains all of the detectors that are needed for
good generalization performance. Then, by training the last layer of the network, it can converge
to a good solution. Following this insight, we tested this procedure on the MNIST data set and a
3 layer convolutional network with convolutional layer with multiple channels and 3 × 3 kernels,
max pooling layer and fully connected layer. We trained an over-parameterized network with 120
channels, clustered its filters with k-means into 4 clusters and used the cluster centers as initialization
for a small network with 4 channels. Then we trained only the fully connected layer of the small
8
Under review as a conference paper at ICLR 2019
network. In Fig. 3 we show that for various training set sizes, the performance of the small network
improves significantly with the new initialization and nearly matches the performance of the over-
parameterized network.
7	Conclusions
In this paper we consider a simplified learning task on binary vectors and show that over-
parameterization can provably improve generalization performance of a 3-layer convolutional
network trained with gradient descent. Our analysis reveals that in the XORD problem over-
parameterized networks are biased towards global minima which detect more relevant patterns in
the data. While we prove this only for the XORD problem and under the assumption that the train-
ing set contains diverse points, our experiments clearly show that a similar phenomenon occurs in
other settings as well. We show that this is the case for XORD with non-diverse points (Figure 1)
and in the more general OBD problem which contains 60 patterns in the data and is not restricted
to binary inputs (Figure 2). Furthermore, our experiments on MNIST hint that this is the case in
MNIST as well (Figure 3).By clustering the detected patterns of the large network we could achieve
better accuracy with a small network. This suggests that the larger network detects more patterns
with gradient descent even though its effective size is close to that of a small network.
We believe that these insights and our detailed analysis can guide future work for showing similar
results in more complex tasks and provide better understanding of this phenomenon. It would also be
interesting to further study the implications of such results on model compression and on improving
training algorithms.
References
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In International Conference on Machine Learning, pp. 605-614, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. 2018.
Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422-2430, 2017.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with
quadratic activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de-
scent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint
arXiv:1712.00779, 2017b.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the
last weight layer. 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix recovery. arXiv preprint arXiv:1712.09203, 2017.
PJG Lisboa and SJ Perantonis. Complete solution of the local minima in the xor problem. Network:
Computation in Neural Systems, 2(1):119-124, 1991.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
9
Under review as a conference paper at ICLR 2019
Figure 4: Network architecture used for training in the XORD problem.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. To-
wards understanding the role of over-parametrization in generalization of neural networks. arXiv
preprint arXiv:1805.12076, 2018.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 2018.
Ida G Sprinkhuizen-Kuyper and Egbert JW Boers. The error surface of the 2-2-1 xor network: The
finite stationary points. Neural Networks, 11(4):683-690,1998.
Roman Vershynin. High-dimensional probability. An Introduction with Applications, 2017.
8	Appendix
8.1	Network Architecture in the XORD Problem
8.2	Experimental Setups
8.2.1	Experiment in Figure 1
We tested the generalization performance in the setup of Section3. We considered networks with
number of channels 4,6,8,20,50,100 and 200. The distribution in this setting has p+ = 0.5 and
p- = 0.9 and the training sets are of size 12 (6 positive, 6 negative). Note that in this case the
training set contains non-diverse points with high probability. The ground truth network can be
realized by a network with 4 channels. For each number of channels we trained a convolutional
network 100 times and averaged the results. In each run we sampled a new training set and new
initialization of the weights according to a gaussian distribution with mean 0 and standard deviation
0.00001. For each number of channels c, We ran gradient descent with learning rate 004 and stopped
it if it did not improve the cost for 20 consecutive iterations or if it reached 30000 iterations. The
last iteration was taken for the calculations. We plot both average test error over all 100 runs and
10
Under review as a conference paper at ICLR 2019
Figure 5: Higher confidence of hinge-loss results in better performance in the XORD problem.
(b)
(a)
Figure 6: Demonstration of generalization gap for values not included in Theorem 5.3. The experimental
setup is the same as in Section 8.2.1 with the following exceptions. For all number of channels we changed the
standard devation σg and only for k = 2 we changed the learning rate η, as described next for each subfigure.
Furthermore, the number of runs is 40 for each channel (instead of 100). (a) Experiments for η = 0.1 and
σg = 0.01. (b) Experiments for η = 0.1 and σg = 0.1. Finally, we note that there is a genereliazation gap
for gradient descent when comparing the performance for k = 2 in these experiments and for larger k in the
experiments in Figure 1.
average test error only over the runs that ended at 0% train error. In this case, for each number of
channels 4,6,8,20,50,100,200 the number of runs in which gradient descent converged to a 0% train
error solution is 62, 79, 94, 100, 100, 100, 100, respectively.
8.3	Hinge Loss Confidence
Figure 5 shows that setting γ = 5 gives better performance than setting γ = 1 in the XORD problem.
The setting is similar to the setting of Section 8.2.1. Each point is an average test error of 100 runs.
8.4	Experiments for Section 5.2
Theorem 5.3 holds for any η ≤ 七 and σ ≤ ɪ^. Because the result is a lower bound, it is desirable
to understand the behaviour of gradient descent for values outside these ranges. In Figure 6 we
empirically show that for values outside these ranges, there is a generalization gap between gradient
descent for k = 2 and gradient descent for larger k .
11
Under review as a conference paper at ICLR 2019
8.5	Experimental Setup in Section 6
We will first formally define the OBD problem. Fix an even dimension parameter d1 ≥ 2. In this
problem, we assume there is an orthonormal basis B = {v1, ..., vd1 } of Rd1 . Divide B into two
equally sized sets Bi and B?, each of size d1. Now define the set of positive patterns to be P =
{v | v ∈ B1} ∪ {-v | v ∈ B1} and negative patterns to be N = {v | v ∈ B2} ∪ {-v | v ∈ B2}.
Let POBD = P ∪ N . For d2 > 0, we assume the input domain is X ⊆ Rd1d2 and each x ∈ X is
a vector such that x = (x1, ..., xd2) where each xi ∈ POBD. We define the ground truth classifier
fOBD : X → {±1} such that fOBD (x) = 1 if and only there exists at least one xi such that
xi ∈ P. Notice that for d1 = 2 and by normalizing the four vectors in {±1}2 to have unit norm, we
get the XORD problem. We note that the positive patterns in the XORD problem are defined to be
PXOR and the negative patterns are {±1}2 \ PXOR.
Let D be a distribution over X2d X {±1} such that for all (x, y)〜 D, y = foBD(x). As in the
XORD problem we define the distributions D+ and D-. We consider the following learning task
which is the same as the task for the XORD problem. We assume that we are given a training set S =
S+ ∪ S- ⊆ {±1}d1d2 × {±1} where S+ consists ofm IID points drawn from D+ and S- consists
of m IID points drawn from D- . The goal is to train a neural network with randomly initialized
gradient descent on S and obtain a network N : Rd1d2 → R such that sign (N (x)) = fOBD(x) for
all x ∈ {±1}d1d2 .
We consider the same network as in the XORD problem (Eq. 1), but now the filters of the convolution
layer are d1-dimensional. Formally, for an input x = (x1, ..., xd) ∈ X the output of the network is
given by
(9)
where W ∈ R2k×d1 is the weight matrix which contains in the first k rows the vectors w(i) ∈ Rd1,
in the next k rows the vectors u(i) ∈ Rd1 and σ(x) = max{0, x} is the ReLU activation applied
element-wise. We performed experiments in the case that d1 = 30, i.e., in which there are 60
possible patterns.
In Figure 2a, for each number of channels we trained a convolutional network given in Eq. 9 with
gradient descent for 100 runs and averaged the results. The we sampled 25 positive points and 25
negative points in the following manner. For each positive point we sampled with probability 0.25
one of the numbers [4,6,8,10] twice with replacement. Denote these numbers by m1 and m2. Then
we sampled m1 different positive patterns and m2 different negative patterns. Then we filled a 60d1-
dimensional vectors with all of these patterns. A similar procedure was used to sample a negative
point. We considered networks with number of channels 4,6,8,20,100 and 200 and 500. Note that
the ground truth network can be realized by a network with 4 channels. For each number of channels
we trained a convolutional network 100 times and averaged the results. For each number of channels
c, We ran gradient descent with learning rate W and stopped it if it did not improve the cost for 20
consecutive iterations or if it had 0% training error for 200 consecutive iterations or if it reached
30000 iterations. The last iteration was taken for the calculations.
We plot both average test error over all 100 runs and average test error only over the runs that
ended at 0% train error. For each number of channels 4,6,8,20,100,200,500 the number of runs in
which gradient descent converged to a 0% train error solution is 96, 99, 100, 100, 100, 100, 100,
respectively. For each 0% train error solution we recorded the number of patterns detected with
cd = 0.0001 according to the Definition 5.1 (generalized to the OBD problem). In the XORD
problem we recorded similarly the number of patterns detected in experiments which are identical
to the experiments in Section 8.2.1, except that in this case p+ = p- = 0.98.
12
Under review as a conference paper at ICLR 2019
8.6	Proof of Theorem 5.2
We will first need a few notations. Define x1 = (1, 1), x2 = (1, -1), x3 = (-1, -1), x4 = (-1, 1)
and the following sets:
Wt+(i)	j ∖ a | arg max Wj) ∙ xι 二 1≤l≤4	i,	Ut+(i) =	=< a | arg max Uj) ∙ xι = 1≤l≤4	i
Wt- (i) =	=< a | arg max Wy) ∙ xι = l∈{2,4}	i),	Ut- (i) =	=< a | arg max Uj) ∙ xι 二 l∈{2,4}	i)
We can use these definitions to express more easily the gradient updates. Concretely, let j ∈
Wt+ (i1) ∩ Wt- (i2) then the gradient update is given as follows:8
(j)	(j)
wt+ι = Wt + ηχiι 1nw(χ+)<γ - ηχi2INW(x-)<i	(10)
Similarly, for j ∈ Ut+(i1) ∩ Ut-(i2) the gradient update is given by:
ut+1 = ut - - ηxiι INW (x+ )<γ + ηxi2 INW (x-)<1	(II)
We denote by x+ a positive diverse point and x- a negative diverse point. Define the following
sums for φ ∈ {+, -}:
Sφ =	X [max {σ (Wj) ∙ xφ),…,σ(w(j) ∙xφ)}]
j∈Wt+ (1)∪Wt+ (3)
Pt =	X	[max {σ (u(j) ∙ xφ) ,...,σ (u(j) ∙xφ)}]
j∈Ut+(1)∪Ut+(3)
Rφ =	X	[max	{σ	(Wj)	∙ xφ),…,σ(w(j)	∙xφ)}]
j∈Wt+ (2)∪Wt+ (4)
— X	[max {σ (u(i) ∙ xφ),…,σ (u(i) ∙ xφ)}]
j∈Ut+(2)∪Ut+(4)
Note that Rt+ = Rt- since for z ∈ {x+, x-} there exists i1, i2 such that zi1 = x2, zi2 = x4.
By the conditions of the theorem, with probability at least (p+p-)m all the points in the training set
are diverse. From now on we will condition on this event. Furthermore, without loss of generality,
we can assume that the training set consists of one diverse point x+ and one negative points x-.
This follows since the network and its gradient have the same value for two different positive diverse
points and two different negative points. Therefore, this holds for the loss function defined in Eq. 4
as well.
We will now proceed to prove the theorem. In Section 8.6.1 we prove results on the filters at ini-
tialization. In Section 8.6.2 we prove several auxiliary lemmas. In Section 8.6.3 we prove upper
bounds on St-, Pt+ and Pt- for all iterations t. In Section 8.6.4 we characterize the dynamics of St+
and in Section 8.6.5 we prove an upper bound on it together with upper bounds on NWt (x+) and
-NWt (x-) for all iterations t.
We provide an optimization guarantee for gradient descent in Section 8.6.6. We prove generalization
guarantees for the points in the positive class and negative class in Section 8.6.7 and Section 8.6.8,
respectively. We complete the proof of the theorem in Section 8.6.9.
8Note that with probability 1, σ0(w(j) ∙ XiJ = 1, σ0(w(j) ∙ Xi2) = 1 for all t, and therefore We omit these
from the gradient update. This follows since σ0 (Wyj ∙ x^) = 0 for some t if and only if WOj) ∙ x由 is an integer
multiple of η.
13
Under review as a conference paper at ICLR 2019
8.6.1 Initialization Guarantees
Lemma 8.1. With probability at least 1 - 4e-8, it holds that
k
∣W+(1) ∪ W+(3)∣- 2 ≤ 2√k
and
,,	,	, k	L
∣u+⑴ ∪ U+(3)∣- 2 ≤ 2√k
Proof. Without loss of generality consider ∣∣W0+(1) ∪ W0+(3)∣∣. Since P j ∈ W0+(1) ∪ W0+(3) =
1, We get by Hoeffding's inequality
P ∣W0+(1) ∪ W+(3)∣ - k < 2√k ≤ 2e-2(⅛k) = 2e-8
The result now follows by the union bound.	□
Lemma 8.2. With probability ≥ 1 一 √∏e8k, for all 1 ≤ j ≤ k and 1 ≤ i ≤ 4 it holds that
∣w0j) ∙ Xi∣ ≤ 4 and ∣u0j) ∙ Xi∣ ≤ 4∙
Proof. Let Z be a random variable distributed as N (0, σ2). Then by Proposition 2.1.2 in Vershynin
(2017), we have
2σ	t2
P[∣Z∣≥ t] ≤√=-e-2σ2
2πt
Therefore, for all 1 ≤ j ≤ k and 1 ≤ i ≤ 4,
P h∣wj) ∙ xi∣ ≥ ηi ≤ ,1	e-8k
U0 i∣ ≥ 4」≤ √32∏k
and
P h∣uj) ∙ xi∣≥ 4 i≤√⅛e-8k
The result follows by applying a union bound over all 2k weight vectors and the four points xi ,
1	≤ i ≤ 4.	□
From now on we assume that the highly probable event in Lemma 8.2 holds.
Lemma 8.3. NWt (x+) < 1 and -NWt (x-) < 1for 0 ≤ t ≤ 2.
Proof. By Lemma 8.2 we have
…,σ (u0i) ∙ x + )}]
and similarly -NW0(x-) < 1. Therefore, by Eq. 10 and Eq. 11 we get:
1.	For	i	∈	{1, 3}, l ∈ {2, 4}, j ∈ W0+(i) ∩ W0-(l), it holds that w(1j)	= w	(0j)	- ηxl	+	ηxi.
2.	For	i	∈	{2, 4} andj ∈ W0+(i), it holds that w(1j) = w(0j).
3.	For	i	∈	{1, 3}, l ∈ {2, 4}, j ∈ U0+(i) ∩ U0- (l), it holds that	u(1j) =	u(0j)	-	ηxi	+	ηxl.
4.	For	i	∈	{2, 4} andj ∈ U0+(i), it holds that u(2j) = u(0j).
14
Under review as a conference paper at ICLR 2019
Applying Lemma 8.2 again and using the fact that η ≤ 册 We have Nwι (x+) < Y and
-NW1 (x-) < 1. Therefore we get,
1.	For i ∈	{1, 3}, l ∈ {2, 4}, j ∈ W0+(i) ∩ W0- (l), it holds that	w(2j)	=	w(0j)	+ 2ηxi.
2.	For i ∈	{2, 4} and j ∈ W0+(i), it holds that w(2j) = w(0j).
3.	For i ∈	{1, 3}, l ∈ {2, 4}, j ∈ U0+(i) ∩ U0- (l), it holds that u(2j) =	u(0j) -	ηxi	+ ηxl.
4.	For i ∈	{2, 4} and j ∈ U0+(i), it holds that u(2j) = u(0j).
As before, by Lemma 8.2 we have Nw2 (x+) < Y and -Nw2 (x-) < 1.	口
8.6.2 Auxiliary Lemmas
Lemma 8.4. For all t ≥ 1 we have Wt+(i) = W0+(i) for i ∈ {1, 3}.
Proof. We will first prove that W0+(i) ⊆ Wt+(i) for all t ≥ 1. To prove this, we will show by
induction on t ≥ 1, that for all j ∈ W0+(i) ∩ W0+(l), where l ∈ {2, 4} the following holds:
1.	j ∈ Wt+(i).
2.	Wtj) ∙ xι = Wj) ∙ xι — η or w(j) ∙ xι = W(O) ∙ xι.
(j)
3.	Wt ∙ Xi > η.
The claim holds for t = 1 by the proof of Lemma 8.3. Assume it holds for t = T. By the induction
hypothesis there exists an l0 ∈ {2, 4} such that j ∈ WT+(i) ∩ WT-(l0). By Eq. 10 we have,
W (Tj+) 1 = W(Tj) + aηxi + bηxl0	(12)
where a ∈ {0, 1} and b ∈ {-1, 0}.
(j )	(j )	0	(j )	(j)	(j)	(j)
If woτ	x 1 —	Wu0	x ι Ihen 1 — ι	aɪid either wuT ∣ ɪ	X1	—	wu0	X1 if b — o or	wuT ∣ ɪ	X1	—	wu0	X1 〃
(j)	(j)	(j)
if b = —1. Otherwise, assume that uT' ∙ Xi — u0 ' ∙ Xi — η. By Lemma 8.2 we have 0 < u0 ' ∙ Xi <
4. Therefore —η < WT) ∙ Xi < 0 and l0 — l. It follows that either uT+i ∙ Xi — uj ∙ Xi — η if
b — 0 or WT∣ι ∙ Xi — WOj) ∙ Xi if b — — 1. In both cases, we have ∣WT+1 ∙ Xi ∣ < η. Furthermore, by
Eq. 12, WT+1 ∙ Xi ≥ WT) ∙ Xi > η. Hence, arg max1≤i≤4 WT+1 ∙ Xi — i which by definition implies
that j ∈ WT∣∣1 (i). This concludes the proof by induction which shows that W0∣ (i) ⊆ Wt∣ (i) for
all t ≥ 1.
In order to prove the lemma, it suffices to show that W0+(2) ∪ W0+(4) ⊆ Wt+(2) ∪ Wt+(4). This
follows since Si4=1 Wt+(i) — {1, 2, ..., k}. We will show by induction on t ≥ 1, that for all j ∈
W0+(2) ∪ W0+(4), the following holds:
1.	j ∈ Wt+(2) ∩ Wt+(4).
2.	Wt(j) — W(0j) + mX2 for m ∈ Z.
The claim holds for t — 1 by the proof of Lemma 8.3. Assume it holds for t — T. By the induction
hypothesis j ∈ WT+(2) ∩ WT+(4). Assume without loss of generality that j ∈ WT+(2). This implies
that j ∈ WT- (2) as well. Therefore, by Eq. 10 we have
W(Tj+) 1 — W(Tj) + aηX2 + bηX2	(13)
15
Under review as a conference paper at ICLR 2019
where a ∈ {0, 1} and b ∈ {0, -1}. By the induction hypothesis, w(Tj+) 1 = w(0j) + mx2 for m ∈ Z.
If a = 1 or b = 0 we have for i ∈ {1, 3},
w(j) ∙	Xn ≥ Wj)	∙ Xn	> Wj)	∙ x	— Wj)	∙ x
UUT +1	x2 ≥ UUT x 2	> *wT Xi	— UUT +1 Xi
where the first inequality follows since j ∈ WT+ (2) and the second by Eq. 13. This implies that
j ∈ WT++1(2) ∩ WT++1(4).
Otherwise, assume that a — 0 and b = -1. By Lemma 8.2 We have WOj) ∙ x2 < 4. Since
j ∈ WT+ (2), it follows by the induction hypothesis that U(Tj) — U(0j) + mx2, where m ∈ Z and
m ≥ 0. To see this, note that if m < 0, then WT) ∙ x2 < 0 and j / WT+ (2), which is a contradiction.
Let i ∈ {1, 3}. If m — 0, then WT+1 — w0")— x2, WT+1 ∙ x4 > 2 and WT+1 ∙ Xi — w，，∙ Xi < 4
by Lemma 8.2. Therefore, j ∈ WT++1(4).
Otherwise, if m > 0, then WT+1 ∙ X2 ≥ WOj) ∙ X2 > WOj) ∙ Xi = WT+1 ∙ Xi. Hence, j / W++1(2),
which concludes the proof.	口
Lemma 8.5. For all t ≥ 0 we have UO+(2) ∪ UO+ (4) ⊆ Ut+(2) ∪ Ut+(4).
Proof. Let j ∈ UO+ (2) ∪ UO+ (4). It suffices to prove that ut(j) — u(Oj) + αtηX2 for αt ∈ Z. This
follows since the inequalities IuOj) ∙ x1I < IuOj) ∙ x2∣ ≤ 4 imply that in this case j / U+ (2) ∪
Ut+(4). Assume by contradiction that there exist an iteration t for which u(tj) — u(Oj) + αtηX2 +
βtηXi where βt ∈ {-1, 1}, αt ∈ Z, i ∈ {1, 3} and ut(-j)1 — u(Oj) + αt-1ηX2 where αt-1 ∈ Z.
9 Since the coefficient of Xi changed in iteration t, we have j ∈ Ut+-1(1) ∪ Ut+-1(3). However,
this contradicts the claim above which shows that if ut(j-)1 — u(Oj) + αt-1ηX2, then j ∈ Ut+-1(2) ∪
Ut-1(4).	口
Lemma 8.6. Let i ∈ {1, 3} and l ∈ {2, 4}. For all t ≥ 0, ifj ∈ UO+(i) ∩ UO-(l), then there exists
at ∈ {0, -1}, bt ∈ N such that u(tj) — u(Oj) + atηXi + btηXl.
Proof. First note that by Eq. 11 we generally have u(tj) — u(Oj) + αηXi + βηXl where α, β ∈ Z.
Since IuOj) ∙ X1∣ ≤ η, by the gradient update in Eq. 11 it holds that at / {0, -1}. Indeed, a0 — 0
and by the gradient update if at-1 — 0 or at-1 — -1 then at ∈ {-1, 0}.
Assume by contradiction that there exists an iteration t > 0 such that bt — -1 and bt-1 — 0.
Note that by Eq. 11 this can only occur if j ∈ Ut+-1 (l). We have ut(-j)1 — u(Oj) + at-1ηXi where
at-1 / {0, -1}. Observe that ∣u(j)1 ∙ Xi∣ ≥ IuOj) ∙ Xi∣ by the fact that ∣uOj) ∙ Xi∣ ≤ 4. Since
UOj) ∙ Xi > UOj) ∙ xι — u(j)1 ∙ xι we have j / Ut-I ⑴ ∪ Ut-I (3), a contradiction.	口
Lemma 8.7. Let
Xt+ — X	[max {σ (w(i)∙ x+),…,b(w(i)∙ x+)}]
j∈Wt+(1)
and
Y+—	X	[max {σ (w(i)∙ x+),…,σ(w(i) ∙ x+)}]
j∈Wt+(3)
Xt+-X0+	Yt+-Y0+
Thenora t，EI —府商・
9 Note that in each iteration βt changes by at most η.
16
Under review as a conference paper at ICLR 2019
Proof. We will prove the claim by induction on t. For t = 0 this clearly holds. Assume it holds for
t = T. Let jι ∈ W7+(1) and j ∈ Wτ+(3). By Eq. 10, the gradient updates of the corresponding
weight vector are given as follows:
wj[∖ = wj1) + αηxι + b1ηx2
and
WjjL = wj2) + aηx3 + b2ηx2
where a ∈ {0,1} and b1,b2 ∈ {-1,0,1}. ByLemma 8.4, j1 ∈ Wcj+1(1) and j2
Therefore,
- xd)} + aη
•x+)}+aη
By Lemma 8.4 we have I Wj(1)∣ = I Wj(I) I and I Wt+(3)∣ = I Wj(3)I for all t. It follows that
Xj+1- Xj _ αη1Wj(1)∣+ Xj - Xj
W+1 ⑴ I
∣wj ⑴ I
Yj -招+
αη +- -∣T- -j-
I Wj (3) I
aη∣Wc^3)∣+ Yj
—
—
=U⑶I
where the second equality follows by the induction hypothesis. This proves the claim.
□
8.6.3	Bounding Pj, P- and S-
Lemma 8.8. Thefollowing holds
1.	S- ≤ I Wj(1) U Wj (3) Iη for all t ≥ 1.
2.	Pj ≤ I Uj(1) U Uj(3) Iη for all t ≥ 1.
3.	P- ≤ I Uj(I) U Uj(3)η forall t ≥ 1.
Proof. In Lemma 8.4 we showed that for all t ≥ 0 and j ∈ Wj (1) U Wj (3) it holds that
I Wy) • x21 ≤ η . This proves the first claim. The second claim follows similarly. Without loss
of generality, let j ∈ Uj(1). By Lemma 8.5 it holds that Uj (1) ⊆ Uj (1) U Uj (3) for all t0 ≤ t.
Therefore, by Lemma 8.6 we have〔 Uj)X11 < η, from which the claim follows.
For the third claim, without loss of generality, assume by contradiction that for j ∈ U j (1) it holds
that I Uy) • X2 I > η. Since Uy) • xι < η by Lemma 8.6, it follows that j ∈ Uj(2) U Uj(4),
a contradiction. Therefore, Uj) • X2 ≤ η for all j ∈ Uj(1) U Uj(3), from which the claim
follows.	□
8.6.4	DYNAMICS of Sj
Lemma 8.9. Thefollowing holds:
1.	If NWt (xj) < Y and -NWt (x-) < 1,then Stjι = Sj + η I Wj (1) U Wj ⑶ I .
17
Under review as a conference paper at ICLR 2019
2.	If NWt (x+) ≥ Y and -NWt (x-) < Lthen	S++1 =	S+.
3.	If NWt (x+) < Y and -NWt (x-) ≥ Lthen	St+1 =	S+ +	η ∣ W+ (1)	U W+ (3)∣.
Proof. 1. The equality follows since for each i ∈ {1,3}, l ∈ {2,4} and j ∈ W+ (i) ∩ W— (l)
we have wjʌ = Wy) + ηxi — ηxι and W+1(1) U W+1(3) = W+(1) U W+(3) by
Lemma 8.4.
2.	In this case for each i ∈ {1,3}, l ∈ {2,4} and j ∈ W+ (i) ∩ W— (l) we have w(+1 =
Wy) — ηxι and Wt+1(1) U Wt+1(3) = W+ (1) U W+ ⑶ by Lemma 8.4.
3.	This equality follows since for each i ∈ {1,3}, l ∈ {2,4} and j ∈ W+ (i) ∩ W— (l) we
have w(y1 = Wy) + ηxi and Wt+1(1) U Wt+1(3) = W+ (1) U W+ (3) by Lemma 8.4.
□
8.6.5 Upper Bounds on NWt(x+), -NWt(x-) AND S+
Lemma 8.10. Assume that NWt (x+) ≥ Y and -NWt (x-) < 1 for T ≤ t <T + b where b ≥ 2.
Then NWT+6 (x+) ≤ NWT (x+) - (b - 1)5 + η ∣ W+(2) U W+(4) ∣ .
Proof. Define R+ = Yt+ - Z + where
Yt+ =	X	[max {σ (w⑶∙ x+) ,...,σ (w ⑴∙ x+)}]
j∈W+(2)UW+(4)
and
Z+ =	X [max {σ (u(i) ∙ x+) ,...,σ (u(i) ∙ x+)}]
j∈u+(2)UU+(4)
Let l ∈ {2,4}, t = T and j ∈ U++1(l). Then, either j ∈ U+(2) U U+(4) or j ∈ U.+ (1) U U.+ (3). In
the first case, Uy)I = Uy) + ηxι. Note that this implies that U+(2) U U+(4) ⊆ U+1(2) U U+1(4)
(since xι will remain the maximal direction). Therefore,
X	[max {σ (Uy)1 ∙ x+) ,∙∙∙,σ (Uy)1 ∙ x+)}]
J∈(Ut+ι(2)UU⅛ι(4)) ∩(u+(2)UU+(4))
-	X	[max {σ(Uy) ∙ x+) ,∙∙∙,σ (u(+ι ∙ x+)}]
j∈u+(2)uu+(4)
=η ∣ (U +1(2) U Ut+ι(4)) ∩ (U+(2) U Ut+(4)) ∣
=η∣ U+(2) U U+(4) ∣	(14)
In the second case, where we have j ∈ Ut+(1) U Ut+(3), it holds that Ut(+y)1 = Uy) + ηxι, j ∈ U—(l)
and Uy)ι ∙ xι > η. Furthermore, by Lemma 8.6, Uy) ∙ Xi < η for i ∈ {1,3}. Note that by Lemma
8.6, any jι ∈ U+ (1) U U+ (3) satisfies jι ∈ U+ι(2) U Uj+ι (4). By all these observations, we have
X	[max {σ (Uy)1 ∙ x1 ) ,∙∙∙,σ (Uy)1 ∙ Xd)}]
J∈(Ut+ι(2)UU⅛ι(4)) ∩(u+(1)UU+(3))
-	X	[max {σ(Uy) ∙ x+) ,∙∙∙,σ (U(+)1 ∙ x+)}]
y∈u+(1)uu+(3)
≥ 0	(15)
18
Under review as a conference paper at ICLR 2019
By Eq. 14 and Eq. 15, it follows that, Z+1 + Pt+1 ≥ Zj ≥ Z+ + P+ + η ∣U+ ⑵ U U+ (4) ∣ .
By induction we have Z^ b + Pt+ b ≥ Z + + Pj + Pb-I η ∣ U++ i(2) U U++ i(4) ∣ . By Lemma 8.6
for any 1 ≤ i ≤ b — 1 we have ∣ Uj(2) U Uji(4)∣ = {1,…，k}. Therefore, Z + + Pjb ≥
Z+ + Pj + (b—1)Cn.
Now, assume that j ∈ Wj(l) for l ∈ {2,4}. Then WTjι = WT) — ηxι. Thus either
max {σ (wT，∙ x+),…,σ (wT，∙ x;) } — max {σ (WT) ∙ x+),…,σ (WT) ∙ x+) } = —η
in the case that j ∈ W7t^+ι(l), or
max {σ (wT)] ∙ x+),…,σ (wT)] ∙ xj) ≤ ≤ η
if j ∈WT+ι(l).
Applying these observations b times, we see that Yj+b — Yj is at most η ∣ Wτ++b(2) U Wj+b(4)∣ =
η ∣ W+ (2) U W+ (4) ∣ where the equality follows by Lemma 8.4. By Lemma 8.9, we have Sj+b =
S+.
Hence, we can conclude that
NWT+b (X+) — NWt (x+) = S++b + RTjb — PT+b — ST — RT + PT)
=γT+b— Z++b— PT+b— γ+ + ZT + pT'
≤—(b —1)cn + η∣ W+(2) U W+(4) ∣
□
Lemma 8.11. Assume that NWt (x+) < Y and -NWt (x-) ≥ 1 for T ≤ t <T + b where b ≥ 1.
Then -Nwt+b(x-) ≤ -NWT(X-) — bη ∣ W+(2) U W÷(4)∣ + Cn.
Proof. Define
Yt- =	X	[max {σ (w(i) ∙ XT) ,...,b(w(i) ∙ x+)}]
j∈w+(2)UW+(4)
and
k
Z- = X [max {σ (uT) ∙ x；) , ...,σ (uT) ∙ x+)}]
j=i
First note that by Lemma 8.4 we have W；i(2) U W+jι(4) = W.T(2) U W.T(4). Next, for any
l ∈ {2,4} and j ∈ W； (l) we have W(T)I = WT) + ηxι. Therefore,
YTTb ≥ YT + bη ∣ W+(2) U W+(4)∣= YT + bη ∣ W÷(2) U W÷(4) ∣
where the second equality follows by Lemma 8.4.
Assume that j ∈ UT(l) for l ∈ {1, 3}. Then UTTI = UT) 一 ηxι and
max
{σ (uTTi
σ (uT)
...,σ i UT ∣ι
∙ x-) } — max {σ (UT)
...,σ (UT) ∙ x-) ∣=0
(16)
(T)	(T)
To see this, note that by Lemma 8.6 and Lemma 8.5 it holds that UT = uy+ aτηxι where
a,T ∈ { — 1, 0}. Hence, UTTi Uy) + a，TTiηxι where a，T)1 ∈ { —1,0}. Since ∣ UT) ∙ x2 ∣ < 4 it
follows that UTTi ∙ x2 = UT) ∙ x2 = UT) ∙ x2 and thus Eq. 16 holds.
Now assume that j ∈ UT(l) for l ∈ {2,4}. Then
max
{σ (UTTi
...,σ (UT% ∙ x-)} — max {σ (UT)
∙ xi

19
Under review as a conference paper at ICLR 2019
if l ∈ {2,4} and j ∈ U++1(l),or
max {σ (UT+1
if l ∈ {2,4} and j ∈ U++1(l).
• x1 ,…,σ (u
,j+1 • x-) } ≤ η

Applying these observations b times, we see that Z-+b - Z- is at most η 口++/2) U U.++b(4)∣.
Furthermore, for j ∈ WT+ (l), l ∈ {1, 3}, it holds that Wj+1 = Wj) + ηxι. Therefore
max {σ (Wjh • x-),…,σ (Wj+1 • x-) } = max {σ (Wj) • x-),…,σ (Wj) ∙ x-) }
and since W++i(1) U W-+^+ι(3) = WT+ (1) U WT+ ⑶ by Lemma 8.4, we get S-+b = S-. Hence,
we can conclude that
-NWT十b (x ) + NWT (x ) = -S-+b - YT⅛b + Z-+b + S- + YT - Z-
≤ -bη ∣ W+(2) U W+(4) ∣+ η ∣U++b(2) U U÷+b(4) ∣
≤-bη∣ W+(2) U W+(4)∣+ Cn
□
Lemma 8.12. Forall t, NWt (x+) ≤ Y + 3cn, -NWt (x-) ≤ 1 + 3cη and S+ ≤ Y + 1 + 8cη.
Proof. The claim holds for t = 0. Consider an iteration T. If NWT (x+) < Y then NWT+ι (x+) ≤
NWT (x+)+2ηk ≤ Y+2cη. Now assume that NWt (x+) ≥ Y for T ≤ t ≤ T+b and Nwt-ɪ (x+) <
Y. By Lemma 8.10, it holds that NWT+b(x+) ≤ NWT(X+) + ηk ≤ NWT(X+) + cη ≤ Y + 3cη,
where the last inequality follows from the previous observation. Hence, NWt (x+ ) ≤ Y + 3cη for
all t.
The proof of the second claim follows similarly. It holds that -NWT +ɪ (x- ) < 1 + 2cη if
-Nwt (x-) < 1. Otherwise if -NWt (x-) ≥ 1 for T ≤ t ≤ T + b and -NWT-ɪ (x-) < 1
then -NWT +b (x- ) ≤ 1 + 3cη by Lemma 8.11.
The third claim holds by the following identities and bounds NWT (x+) 一 NWT (x-) = S+ - P/ +
P- - S-, P- ≥ 0, ∣ P+ ∣ ≤ cη, ∣ S- ∣ ≤ cη and NWT(x+) — NWT(x-) ≤ Y + 1 + 6cη by the
previous claims.	□
8.6.6 Optimization
We are now ready to prove a global optimality guarantee for gradient descent.
Proposition 8.13. Let k > 16 and Y ≥ 1. With probabaility at least 1 — √∏∣∣fe — 4e-8, after
T = 7(γ+1√cn) iterations, gradient descent converges to a global minimum.
Proof. First note that with probability at least 1 - √∏∣8fe - 4e-8 the claims of Lemma 8.1 and
Lemma 8.2 hold. Now, if gradient descent has not reached a global minimum at iteration t then
either NWt (x+) < Y or -NWt (x-) < 1. If -NWt (x+) < Y then by Lemma 8.9 it holds that
S++1 ≥ S+ + η∣ W+(1) U W+(3) ∣ ≥ S+ + G - 2√k)η	(17)
where the last inequality follows by Lemma 8.1.
If Nwt (x+) ≥ Y and -NWt (x-) < 1 we have S+1 = St by Lemma 8.9. However, by Lemma
8.10, it follows that after 5 consecutive iterations t < t0 < t + 6 in which Nwt0 (x+) ≥ Y and
-Nwt0(x-) < 1, we have Nwt+θ (x+) < Y. To see this, first note that for all t, NWt (x+) ≤ Y+3cη
by Lemma 8.12. Then, by Lemma 8.10 we have
Nwt+6 (x+) ≤ Nwt (x+) - 5cη + η∣ W+(2) U W+(4) ∣
≤ Y + 3cη - 5cη + cη
<Y
20
Under review as a conference paper at ICLR 2019
where the second inequality follows by Lemma 8.1 and the last inequality by the assumption on k.
Assume by contradiction that GD has not converged to a global minimum after T
7(γ+1+8cη)
(2-2√k)η
iterations. Then, by the above observations, and the fact that S0+ > 0 with probability 1, we have
ST+ ≥ S0+ +
> γ + 1 + 8cη
However, this contradicts Lemma 8.12.
□
8.6.7 Generalization on Positive Class
We will first need the following three lemmas.
Lemma 8.14. With probability at least 1 - 4e-8, it holds that
∣W+(1)∣- 4 ≤ 2√k
and
,	,	, k	L
∣W+(3)∣- 4 ≤ 2√k
Proof. The proof is similar to the proof of Lemma 8.1.	□
Lemma 8.15. Assume that gradient descent converged to a global minimum at iteration T. Then
there exists an iteration T2 < T for which St+ ≥ γ + 1 - 3cη for all t ≥ T2 and for all t < T2,
-NWt (x- ) < 1.
Proof. Assume that for all 0 ≤ t ≤ T1 it holds that NWt (x+) < γ and -NWt (x-) < 1. By
continuing the calculation of Lemma 8.3 we have the following:
1.	For i ∈ {1, 3}, l ∈ {2, 4}, j ∈ W0+(i) ∩ W0- (l), it holds that w(Tj) = w(0j) + T1 ηxi -
2(I - (-I)TI)ηxι.
2.	For i ∈ {2, 4} and j ∈ W0+(i), it holds that w(Tj) = w(0j).
3.	For i ∈ {1, 3}, l ∈ {2, 4}, j ∈ U0+(i) ∩ U0- (l), it holds that u(Tj) = u(0j) - ηxi + ηxι.
4.	For i ∈ {2, 4} and j ∈ U0+(i), it holds that u(Tj) = u(0j).
Therefore, there exists an iteration T1 such that NWT (x+) ≥ γ and -NWT (x-) < 1 and for
all t < T1, NWt (x+) < γ and -NWt (x-) < 1. Let T2 ≤ T be the first iteration such that
-NWT (x-) ≥ 1. We claim that for all T1 ≤ t ≤ T2 we have NWT (x+) ≥ γ - 2cη. It suffices to
show that for all T1 ≤ t < T2 the following holds:
1.	IfNWt(x+) ≥ γ then NWt+1(x+) ≥ γ - 2cη.
2.	If NWt (x+) < γ then NWt+1 (x+) ≥ NWt (x+).
The first claim follows since at any iteration NWt (x+) can decrease by at most 2ηk = 2cη. For
the second claim, let t0 < t be the latest iteration such that NWt0 (x+ ) ≥ γ. Then at iteration t0
it holds that -NWt0 (x-) < 1 and NWt0 (x+) ≥ γ. Therefore, for all i ∈ {1, 3}, l ∈ {2, 4} and
j ∈ U0+(i) ∩ U0+(l) it holds that u(t0j+) 1 = ut(0j) +ηxι. Hence, by Lemma 8.5 and Lemma 8.6 it holds
that U++ι(1) ∪ U++ι(3) = 0. Therefore, by the gradient update in Eq. 11, for all 1 ≤ j ≤ k, and
all t0 < t00 ≤ t we have ut(0j0)+1 = u(t0j0), which implies that NWt00+1 (x+) ≥ NWt00 (x+). For t00 = t
we get NWt+1 (x+) ≥ NWt (x+).
21
Under review as a conference paper at ICLR 2019
The above argument shows that NWT (x+) ≥ γ - 2cη and -NWT (x-) ≥ 1. Since NWT (x+) -
Nwt2 (x-) = ST2 -Pt+, +P-2-S-2, PT2 ,S- ≥ 0 and ∖P- ∖ ≤ Cn it follows that S^2 ≥ γ+1-3%.
Finally, by Lemma 8.9 We have St ≥ Y + 1 - 35 for all t ≥ T2.	口
Lemma 8.16. Let
X+ = X	[max {σ (w(j) ∙x1),…,σ(w(j) 々+)}]
j∈Wt+(2)∪Wt+(4)
and
Σ
j∈Ut+(2)∪Ut+(4)
(u(j) ∙ x+)}]
Assume that k ≥ 64 and gradient descent converged to a global minimum at iteration T. Then,
XT+ ≤ 34Cη and YT+ ≤ 1 + 38Cη.
Proof. Notice that by the gradient update in Eq. 10 and Lemma 8.2, Xt+ can be strictly larger than
max {Xt-ι,η ∖W+ ⑵ ∪ W+ (4)∖} only if Nw- (x+) < Y and -NWt-I (x-) ≥ 1. Furthermore,
in this case Xt+ - Xt+-1 = η ∖Wt+(2) ∪ Wt+(4)∖. By Lemma 8.9, St+ increases in this case by
η ∖∖Wt+(1) ∪ Wt+(3)∖∖. We know by Lemma 8.15 that there exists T2 < T such that ST+ ≥ Y+1-3Cη
and that Nwt (x+) < Y and -Nwt(x-) ≥ 1 only for t > T2. Since St+ ≤ Y + 1 + 8Cη for all t
by Lemma 8.12, there can only be at most
iterations in which Nwt (x+ ) < Y and
-Nwt (x-) ≥ 1. It follows that
______11cη____
n∣w+(i)∪w+(3)∣
Xt+ ≤η∖WT+(2)∪WT+(4)∖+
11cnη ∖W+(2) ∪ w+ (4)∖
η ∖w+⑴ ∪ Wτ+(3)∖
≤ 34Cη
where the second inequality follows by Lemma 8.1 and the third inequality by the assumption on k.
At convergence we have NwT (x-) = ST- + XT+ - YT+ - PT- ≥ -1 - 3Cη by Lemma 8.12 (recall
that Rt- = Rt+ = Xt+ - Yt+ ). Furthermore, PT- ≥ 0 and by Lemma 8.8 we have ST- ≤ Cη .
Therefore, we get Y1+ ≤ 1 + 38crι.	口
We are now ready to prove the main result of this section.
Proposition 8.17. Define β(γ) = γq9404cη. Assume that Y ≥ 2 and k ≥ 64 (β(γ)+1] . Then
39cη +1	β(γ)-1
with probability at least 1 — √28k — 8e-8, gradient descent converges to a global minimum which
classifies all positive points correctly.
Proof. With probability at least 1 -
- 8e-8 Proposition 8.13, and Lemma 8.14 hold. It suf-
fices to show generalization on positive points. Assume that gradient descent converged to a global
minimum at iteration T. Let (z, 1) be a positive point. Then there exists zi ∈ {(1, 1), (-1, -1)}.
Assume without loss of generality that zi = (-1, -1) = x3. Define
X+(i) =	X	[max {σ (w(j) ∙ x+) ,...,σ (w(j) ∙x+)}]
j∈wT+(i)
Y+(i) = X [max {σ (u(j) ∙ x+),…,σ (u(j) ∙x+)}]
j∈UT+ (i)
for i ∈ [4].
22
Under review as a conference paper at ICLR 2019
Notice that
NWT(x+) = X+(1) + X+(3) - PT + RT
=X+(1) + X+(3) - PT + RT
=XT(1)+ XT(3) - PT + NWT(x-) - ST + PT
Since Nwt (x+) ≥ Y, -NWT(X-) ≥ 1, ∣P-1 ≤ Cn by Lemma 8.8 and P7+, ST ≥ 0 , we obtain
XT (I) + XT ⑶ ≥ Y + 1 - cη	(18)
Furthermore, by Lemma 8.7 we have
XT(I)- XT(I) = XT(3)- XT(3)	(19)
I咛(1)	wt(3)	,)
and by Lemma 8.14,
4-2√ ≤ I 呼(1) I ≤ 4 + 2√k	(20)
4 + 2√k - 1 咛(3) 1 - 4 - 2√⅛	()
Let α(k) = 4]；/. By Lemma 8.2 we have IXT(I) ∣ ≤ 竽 ≤ c4η. Combining this fact with Eq. 19
and Eq. 20 we get
XT(I) ≤ α(k)XT(3) + XT(I) ≤ α(⅛)X÷(3) + cη
........ .	..一一.	j/…γT1-5cη i C
which implies together with Eq. 18 that XT (3) ≥ \+以点.Therefore,
Nwt(z) ≥ XT(3) - PT - YT(2) - YT(4)
≥ Y + 1-苧
― 1 + α(k)
=Y+1-苧
1 + α(k)
——Cη ——1 ——3(8Cη)——14Cη
-39cη - 1 > 0
where the first inequality is true because
(21)
…,σ (u⑺∙ Xd)}]
(22)
=PT + γT-(2) + γT(4)	(23)
The second inequality in Eq. 21 follows since PT ≤ Cη and by appyling Lemma 8.16. Finally, the
last inequality in Eq. 21 follows by the assumption on k. 10 Hence, N is classified correctly. 口
8.6.8 Generalization on Negative Class
We will need the following lemmas.
Lemma 8.18. With probability at least 1 — 8e-8, it holds that
k ,	, k	L
∣ UT (2) I - 4 ≤ 2√k
I UT ⑷ I-4 ≤ 2√k
,,,	，	、	， k	L
I (UT(I) U UT (3)) ∩ U-(2) I-4 ≤ 2√k
k
I (UT(1) U UT (3)) ∩ U-(4) I-4 ≤ 2 √k
1_,	一.	y+1-5cn	一	.	…	.一	一	.................. .
10The inequality 1+@帛-----39cη — 1 > 0 is equivalent to α(k) < β(γ) which is equivalent to k >
64 (β(Y)+1 )2.
<β(γ)τ)
23
Under review as a conference paper at ICLR 2019
Proof. The proof is similar to the proof of Lemma 8.1 and follows from the fact that
Pj∈U0+(2) =Pj∈U0+(4)
=Pj ∈ (U+(1) ∪ U+(3)) ∩ U-(2)]
=Pj ∈ (U+(1) ∪ U+(3)) ∩ U-(4)]
1
=—
4
□
Lemma 8.19. Let
and
Yt- = X	[max {σ(u(j) ∙ x-),…,σ(u(j) ∙ x-)}]
j∈U0+(4)
Thenfor all t, there exists X, Y ≥ 0 such that |X | ≤ η∣U+(2)∣, |Y | ≤ η ∣U+(4)∣ and Xt+-X
|U0 (2)|
Y--Y
|u+(4)「
Proof. First, we will prove that for allt there exists at ∈ Z such that for j1 ∈ U0-(2) and j2 ∈ U0-(4)
it holds that ut(j1) = u(0j1) + atηx2 and ut(j2) = u(0j2) - atηx2. 11 We will prove this by induction
on t.
For t = 0 this clearly holds. Assume it holds for an iteration t. Let j1 ∈ U0- (2) andj2 ∈ U0- (4).
By the induction hypothesis, there exists aT ∈ Z such that ut(j1) = u(0j1) + atηx2 and ut(j2) =
u0j2) - atηx2. Since for all 1 ≤ j ≤ k it holds that ∣uj) ∙ x2∣ < 4, it follows that either
U0- (2) ⊆ Ut- (2) and U0- (4) ⊆ Ut- (4) or U0- (2) ⊆ Ut- (4) and U0- (4) ⊆ Ut- (2). In either
case, by Eq. 11, we have the following update at iteration t + 1:
ut(j+11) = ut(j1) + aηx2
and
(j2)	(j2)
ut+21 = ut 2 - aηx2
where a ∈ {-1, 0, 1}. Hence, u(tj+11) = u(0j1) + (at + a)ηx2 and u(tj2) = u(0j2) - (at + a)ηx2. This
concludes the proof by induction.
Now, consider an iteration t, j1 ∈ U0+(2), j2 ∈ U0+(4) and the integer at defined above. If at ≥ 0
then
ηat
and
max {σ (utj2) ∙ x-),…,σ (u(j2) ∙ x-)} —max {σ (Uj2) ∙ x-),…，σ (Uj2) ∙ x-) = = ηat
Define X = X0- and Y = Y0- then |X| ≤ η ∣∣U0-(2)∣∣, |Y | ≤ η ∣∣U0-(4)∣∣ and
X- - X = ∣U-(2)∣ ηat =	= ∣U-(4)∣ ηat = YJ - Y
∣U-(2)∣ — ∣U-(2)∣ TaL ∣U-(4)∣ — ∣U-(4)∣
which proves the claim in the case that at ≥ 0.
If at < 0 it holds that
11Recall that by Lemma 8.5 we know that U0+(2) ∪ U0+(4) ⊆ Ut+(2) ∪ Ut+(4).
24
Under review as a conference paper at ICLR 2019
max
and
max
{σ (ujI) ∙x-) ,...,σ (u(jI) ∙x-) } - max {σ ( (UjI)-次)∙ x-),…，σ ( (UjI)-次)∙ x-) } = η(-αt - 1)
{σ (Uj2) ∙ x-) ,...,σ (Uj2) ∙ x-) } - max {σ ((Uj2) + x2) ∙ x-),…,σ ((Uj2) + x2) ∙ x-) } = η(-at - 1)
Define
X = X [max {σ ((Uj)-X2) ∙ x-),…,σ((Uj)-X2) ∙吗)}]
j∈u+(2)
and
Y = X [max {σ ((Uj)+g) ∙ x-),…,σ((Uj) + %) ∙ x-)}]
j∈u+(4)
Since for all 1 ≤ j ≤ k it holds that I Uj) ∙ x2 I < 4, we have ∣X ∣ ≤ η I U-(2) I , ∣Y ∣ ≤ η I U- (4) I .
Furthermore,
Xt - X _ IU-(2)I η(-at - 1)
I U-⑵ I
I u-⑵ I
=η(-at -1)
I U-(4) I η(-at - 1) _ 匕--Y
∣u-⑷ ∣
which concludes the proof.
I U-⑷ I
□
Lemma 8.20. Let
Xt- =
j∈(u+(1)UU+(3))∩UJ(2)
and
Thenforall t,
Σ
j∈(u+(1)UU+(3))∩U-(4)
XJ-X-	_ _
∣(U+(1)UU+(3))∩U-(2)∣ = ∣(U+(1)UU+(3))∩U-(4)∣'
Proof. We will first prove that for all t there exists an integer at ≥ 0 such that for j1 ∈
(U+(1) U U+(3)) ∩ U-(2) and j2 ∈ (U÷(1) U "(3)) ∩ U-(4) it holds that UjI) ∙ x2 =
UjI) ∙ x2 + ηat and U(j2) ∙ x4 = u j2) ∙ x4 + ηat. We will prove this by induction on t.
For t = 0 this clearly holds. Assume it holds for an iteration t. Let jι ∈ (U+ (1) U U+ (3)) ∩ U- (2)
and j2 ∈ (U+ (1) U U+ (3)) ∩ U-(4). By the induction hypothesis, there exists an integer at ≥ 0
such that UjI) ∙ x2 = UjI) ∙ x2 + ηat and UyG ∙ x4 = u j2) ∙ x4 + ηat. Since for all 1 ≤ j ≤ k it
holds that I Uj) ∙ x1 I < 4, it follows that if at ≥ 1 we have the following update at iteration T + 1:
UjL) = UjI) + aηx2
and
Uji2) = Uj2)+ aηx4
where a ∈ {-1, 0,1}. Hence, UjL/ ∙x2 = UjI) ∙x2 +η(at+a) and Uj2) ∙x4 = Uj2) ∙x4+η(at+a).
Otherwise, if at = 0 then
UtiL) = UjI) + aηx2 + b1x1
and
Uti2) = utj2) + aηx4 + b2x1
25
Under review as a conference paper at ICLR 2019
such that a ∈ {0,1} and b1,b2 ∈ {-1,0,1}. Hence, u(j1) ∙ x2 = u0j1)
• x2 + η(at + a) and
u(+2) ∙ x4 = u0j2) ∙ x4 + η(at + a). This concludes the proof by induction.
Now, consider an iteration t,j1 ∈ U0+(1)∪U0+(3) ∩U0-(2)andj2 ∈ U0+(1)∪U0+(3) ∩U0-(4)
and the integer at defined above. We have,
ηat
and
ηat
It follows that
Xt- - X0-
ηat
∣(U0+⑴ ∪ U+(3)) ∩ U-(4)∣ηat
∣(Uo+(1) ∪ Uo+ (3)) ∩ U-(4)I
Yt- -万
∣(Uo+(1) ∪ Uo+(3)) ∩ U-⑷∣
which concludes the proof.
□
We are now ready to prove the main result of this section.
Proposition 8.21. Define β = 1—364cη ∙ Assume that k > 64 (β∣-ɪ) . Then with probability at
least 1 — √∏ekk — 8e-8, gradient descent converges to a global minimum which classifies all negative
points correctly.
Proof. With probability at least 1 - √8k — 16e-8 Proposition 8.13 and Lemma 8.18 hold. It
suffices to show generalization on negative points. Assume that gradient descent converged to a
global minimum at iteration T. Let (z, -1) be a negative point. Assume without loss of generality
that zi = x2 for all 1 ≤ i ≤ d. Define the following sums for l ∈ {2, 4},
Xt-
j∈Wt+(2)∪Wt+(4)
Yt-(l) = X hmax nσ (ut(j)
• x1-	, ..., σ ut • xd-

j∈U0+(l)
Zt-(l) =	X	hmax nσ (u(j) • x1-) , ..., σ (u(j) • xd-)oi
j∈(U0+(1)∪U0+(3))∩U0-(l)
First, we notice that
NWT(x-) = ST- +XT- -YT-(2) -YT-(4) - ZT-(2) - ZT-(4)
XT- , ST- ≥ 0
and
NWT (x-) ≤ -1
imply that
YT-(2) + YT- (4) +ZT-(2) +ZT-(4) ≥ 1	(24)
26
Under review as a conference paper at ICLR 2019
We note that by the analysis in Lemma 8.18, it holds that for any t, j1 ∈ U0+(2) and j2 ∈ U0+(4),
either j1 ∈ Ut+ (2) and j2 ∈ Ut+ (4), or j1 ∈ Ut+ (4) and j2 ∈ Ut+ (2). We assume without loss of
generality that j1 ∈ UT+(2) and j2 ∈ UT+(4). It follows that in this case NWT (z) ≤ ST- + XT- -
ZT- (2) - YT- (2). 12Otherwise we would replace YT- (2) with YT- (4) and vice versa and continue
with the same proof.
Let α(k) = 4+2√. By Lemma 8.20 and Lemma 8.18
ZT ⑷ ≤ α(k)Z-⑵+Z-⑵ ≤ α(k) ZT ⑵+4η~
and by Lemma 8.19 and Lemma 8.18 there exists Y ≤ cη such that:
YTT(4) ≤ α(k)YTT(2) +Y ≤ α(k)YTT(2) +cη
Plugging these inequalities in Eq. 24 we get:
α(k)ZT ⑵ + 才 + α(k)Y^τ ⑵ + Cη + YT ⑵ + ZT ⑵ ≥ 1
which implies that
1	5cη
Y-(2) + Z-(2) ≥ —ɪ
T T	α(k) + 1
By Lemma 8.16 we have XTT ≤ 34cη. Hence, by using the inequality STT ≤ cη we conclude that
1	5cη
NWT(Z) ≤ ST + XT — Z-(2) - Y-(2) ≤ 35cη - -ɪ < 0
α(k) + 1
where the last inequality holds for k > 64 (β-1 )2. 13 Therefore, z is classified correctly.
□
8.6.9 Finishing the Proof
First, for k ≥ 120, with probability at least 1 - √√kfe - 16e-8, Proposition 8.13, Lemma 8.14
and Lemma 8.18 hold. Also, for the bound on T, note that in this case
≥ 7(γ+1+8cη)
_ (2T2√)η .
28(Y+1+8cη)
Define βι = ^-æ+ ? and β2 = 1-3364cη and let β = max{β1,β2}. For Y ≥ 8 and cη ≤ 击 it
holds that 64 (β-1) < 120. By Proposition 8.17 and Proposition 8.21, it follows that for k ≥ 120
gradient descent converges to a global minimum which classifies all points correctly.
We will now prove pattern detection results. In the case of over-paramterized networks, in Propo-
sition 8.17 We proved that X +(1),X +(3) ≥ 才-篙.Since for i ∈ {1,3} it holds that
Dxi ≥ X+ (i), it follows that patterns x1 and x3 are detected. Similarly, in Proposition 8.21
our analysis implies that, without loss of generality, YT (2) + Z- (2), YT (4) + Z- (4) ≥ ；弟)+1.
Since, for l ∈ {2, 4}, Dxl ≥ YTT (l) + ZTT (l) (under the assumption that we assumed without loss
of generality), it follows that patterns x2 and x4 are detected. The confidence of the detection is at
1-5cη
least α(k)+1.
8.7 Proof of Theorem 5.3
1. We refer to Eq. 17 in the proof of Proposition 8.13. To show convergence and provide
convergence rates of gradient descent, the proof uses Lemma 8.1. However, to only show
12The fact that we can omit the term -ZT- (4) from the latter inequality follows from Lemma 8.6.
1-5Cη	∕a∖2
13It holds that 355 — ɑ(卜)+1 < 0 if and only if α(k) < β which holds if and only if k > 64 ( e+1 )
27
Under review as a conference paper at ICLR 2019
convergence, it suffices to bound the probability that W+ (1) ∪ W+ (3) = 0 and that the
initialization satisfies Lemma 8.2. Given that Lemma 8.2 holds (with probability at least
1 — λ∕8e-32), then Wj⑴ ∪ W+(3) = 0 holds with probability
By the argument above, with probability at least (p+p-)m
3
4 .
∏∕e-e-32) 3 all training
points are diverse, Lemma 8.2 holds with k = 2 and W0+(1) ∪ W0+(3) 6= 0 which implies
that gradient descent converges to a global minimum. For the rest of the proof we will
condition on the corresponding event. Let T be the iteration in which gradient descent
converges to a global minimum. Note that T is a random variable. Denote the network at
iteration T by N. For all z ∈ R2d denote
2
N(Z) = ^X [max {σ (w(j) ∙ zι),…,σ (w(j) ∙ Zd) } - max {σ (u(j) ∙ zι),…,σ (u(j)
j=1
Let E denote the event for which at least one of the following holds:
(a)	WT+(1) =0.
(b)	WT+(3) =0.
(c)	U(I) ∙ X2 > 0 and u(2) ∙ x2 > 0.
(d)	U⑴∙ x4 > 0 and U⑵∙ x4 > 0.
Our proof will proceed as follows. We will first show that ifE occurs then gradient descent
does not learn f *, i.e., the network N does not satisfy sign (N(x)) = f * (x) for all X ∈
{±1}2d. Then, we will show that P [E∖ ≥ 121. This will conclude the proof.
Assume that one of the first two items in the definition of the event E occurs. Without
loss of generality assume that WT+ (1) = 0 and recall that x- denotes a negative vector
which only contains the patterns x2 , x4 and let Z+ ∈ R2d be a positive vector which
only contains the patterns x1, x2, x4. By the assumption WT+(1) = 0 and the fact that
x1 = -x3 it follows that for all j = 1, 2,
max {σ (w(j) ∙ z+),…,σ (w(j) ∙ zj) } = max {σ (w(j) ∙ x-),…,σ (w(j) ∙ x-) }
Furthermore, since Z+ contains more distinct patterns than x-, it follows that for all j =
1,2,
• zd)}]
max {σ (u(j) ∙ z+) , ..., σ (u(j) ∙ z+)} ≥ max {σ (u(j) ∙
x1-	, ..., σ

u(j) ∙ x-)}
Hence, N(z+) ≤ N(x-). Since at a global minimum N(x-) ≤ -1, we have N(z+) ≤
-1 and z2 is not classified correctly.
Now assume without loss of generality that the third item in the definition ofE occurs. Let
z- be the negative vector with all of its patterns equal to x4. It is clear that N (z- ) ≥ 0
and therefore z- is not classified correctly. This concludes the first part of the proof. We
will now proceed to show that P [E] ≥ 11.
Denote by Ai the event that item i in the definition of E occurs and for an event A denote by
Ac its complement. Thus Ec = ∩i4=1Aic and P [Ec] = P [Ac3 ∩ Ac4 | Ac1 ∩ Ac2 ] P [Ac1 ∩ Ac2 ].
We will first calculate P [Ac1 ∩ Ac2 ]. By Lemma 8.4, we know that for i ∈ {1, 3}, W0+(i) =
WT+(i). Therefore, it suffices to calculate the probabilty that W0+(1) 6= 0 and W0+(3) 6= 0,
provided that W0+(1) ∪ W0+(3) 6= 0. Without conditioning on W0+(1) ∪ W0+(3) 6= 0, for
each 1 ≤ i ≤ 4 and 1 ≤ j ≤ 2 the event that j ∈ W+ (i) holds with probability 1. Since
the initializations of the filters are independent, we have P [Af ∩ Aξ∖ = 1. 14
We will show that P [Ac ∩ A4 | Ac ∩ AC∖ = ∣ by a symmetry argument. This
will finish the proof of the theorem. For the proof, it will be more convenient
to denote the matrix of weights at iteration t as a tuple of 4 vectors, i.e., Wt =
w(01), w(02), u(01), u(02). Consider two initializations W0(1)
w(01), w(02), u(01), u(02)
and W0(2) = w(01), w(02),
-u(01), u(02) and let Wt(1) and Wt(2) be the corresponding
weight values at iteration t. We will prove the following lemma:
14Note that this holds after conditioning on the corresponding event of Lemma 8.2.
28
Under review as a conference paper at ICLR 2019
Lemma 8.22. For all t ≥	0,	if	Wt(1)	=	wt(1), w(t2),	ut(1), u(t2)	then	Wt(2)	=
wt(1), wt(2), -u(t1), u(t2).
Proof. We will show this by induction on t. 15This holds by definition for t = 0. Assume
it holds for an iteration t. Denote Wt(+2)1 = (z1, z2, v1, v2). We need to show that z1 =
wt(1+)1, z2 = wt(2+)1, v1 = -ut(1+)1 and v2 = ut(+2)1. By the induction hypothesis it holds that
NW (1) (x+) = NW (2) (x+) and NW (1) (x-) = NW (2) (x-). This follows since for diverse
points (either positive or negative), negating a neuron does not change the function value.
Thus, according to Eq. 10 and Eq. 11 we have z1 = wt(+1)1, z2 = wt(2+)1 and v2 = u(t2+)1.
We are left to show that v1 = -ut(+1)1. This follows from Eq. 11 and the following facts:
(a)	x3 = -x1.
(b)	x2 = -x4.
(C) arg max1≤1≤4 U ∙ xι	= 1 if and only if arg max1≤1≤4 -U	∙	xι	=	3.
(d)	arg max1≤1≤4 u ∙ xι	= 2 if and only if arg max1≤1≤4 —u	∙	x(	=	4.
(e)	arg max1∈{2 4} U ∙ xι = 2 if and only if arg max1∈{2 4} —u ∙ xι = 4.
To see this, we will illustrate this through one case, the other cases are similar. Assume,
for example, that arg max1≤1≤4 U(I) ∙ xι = 3 and argmax1∈{2,4} U(I) ∙ xι = 2 and as-
sume without loss of generality that NW(1)(x+) = NW(2)(x+) < γ and NW(1)(x-) =
NW (2) (x-) > -1. Then, by Eq. 11, Ut(1+)1 = Ut(1) - x3 + x2. By the induction hypothesis
and the above facts it follows that v1 = -Ut(1) - x1 + x4 = -Ut(1) + x3 - x2 = -Ut(1+)1.
This concludes the proof.	□
Consider an initialization of gradient descent where w(01) and w(02) are fixed and the event
that we conditioned on in the beginning of the proof and Ac1 ∩ Ac2 hold. Define the set B1
to be the set of all pair of vectors (v1, v2) such that if U(01) = v1 and U(01) = v2 then at
iteration T, U(I) ∙ x2 > 0 and u(2) ∙ x2 > 0. Note that this definition implicitly implies
that this initialization satisfies the condition in Lemma 8.2 and leads to a global minimum.
Similarly, let B2 be the set of all pair of vectors (v1, v2) such that if U(01) = v1 and
U01) = v2 then at iteration T, U⑴∙ x4 > 0 and u(2) ∙ x2 > 0. First, if (vι, v2) ∈ Bi then
(-v1, v2) satisfies the conditions of Lemma 8.2. Second, by Lemma 8.22, it follows that
if (v1, v2) ∈ B1 then initializating with (-v1, v2), leads to the same values of NWt (x+)
and NWt (x-) in all iterations 0 ≤ t ≤ T. Therefore, initializing with (-v1, v2) leads
to a convergence to a global minimum with the same value of T as the initialization with
(v1, v2). Furthermore, if (v1, v2) ∈ B1, then by Lemma 8.22, initializing with U(01) =
-vi andU01) = v results in U⑴∙x2 < 0 andu(2) ∙x2 > 0. Itfollows that (vi, v2) ∈ Bi
if and only if (-v1 , v2 ) ∈ B2 .
Forl1,l2 ∈ {2,4} define3册=P IU⑴∙ xiι > 0 ∧ U⑵∙ x^ > 0 | Ac ∩ AC, w01), w02)]
Then, by symmetry of the initialization and the latter arguments it follows that P2,2 = P4,2.
By similar arguments we can obtain the equalities P2,2 = P4,2 = P4,4 = P2,4.
Since all of these four probabilities sum to 1, each is equal to 4. 16Taking expectations of
these probabilities with respect to the values of w(0i) and w(02) (given that Lemma 8.2 and
Aci ∩ Ac2 hold) and using the law of total expectation, we conclude that
P [A3 ∩	A4	| Ai	∩ A2] = P [u(D ∙	X4 > 0 ∧ U(2)	∙ X2	> 0 |	A4	∩ A2∣
+ P [u⑴∙	X2 > 0 ∧ u(2)	∙ x4	> 0 |	Ac	∩ Aci	= ɪ
15Recall that we condition on the event corresponding to Lemma 8.2. By negating a weight vector we still
satisfy the bounds in the lemma and therefore the claim that will follow will hold under this conditioning.
16Note that the probablity that u(i) ∙ Xj = 0 is 0 for all possible i and j.
29
Under review as a conference paper at ICLR 2019
Finally, we show results for detection of a pattern. To see this, we will show that if one of
the four conditions of the event E defined above is met, then for cd > 2cη , the network
does not detect all patterns. If one of the last two conditions hold, then this is true even for
Cd ≥ 0. Now, assume without loss of generality that Wτ+(1) = 0. In this case by Lemma
8.4 and Lemma 8.2, it follows that
j∈WT+(2)∪WT+(4)
and therefore, x1 cannot be detected with confidence greater than 2cη .
2. Let Z1 be the set of positive points which contain only the patterns x1, x2, x4, Z2 be the
set of positive points which contain only the patterns x3, x2, x4. Let Z3 be the set which
contains the negative point with all patterns equal to x2 and Z4 be the set which contains
the negative point with all patterns equal to x4 . By the proof of the previous section, if
the event E holds, then there exists 1 ≤ i ≤ 4, such that gradient descent converges to a
solution at iteration T which errs on all of the points in Zi. Therefore, its test error will be
at least p* (recall Eq. 3).
8.8 Proof of Theorem 4.1
Let δ ≥ 1 -p+p-(1 - c- 16e-8). By Theorem 5.2, given 2 samples, one positive and one negative,
with probability at least 1 - δ ≤ p+p-(1 - c - 16e-8), gradient descent will converge to a global
minimum that has 0 test error. Therefore, for all ≥ 0, m(, δ) ≤ 2. On the other hand, by Theorem
2 lθg ( qq48δ ∖ )
5.3, if m < log；PJp-))，then with probability greater than
log( 3348-C)
(p+p-) log(p+p-)
)(1 - c)33 = δ
'7 48
gradient descent converges to a global minimum with test error at least p*. It follows that for
0 ≤ e<P*，m(e,δ) ≥ Tog").
9 XOR
In this section we assume that we are given a training set S ⊆ {±1}2 × {±1}2 consisting of
points (x1, 1), (x2, -1), (x3, 1), (x4, -1), where x1 = (1, 1), x2 = (-1, 1), x3 = (-1, -1) and
x4 = (1, -1). Our goal is to learn the XOR function with gradient descent.
Note that in the case of two dimensions, the convolutional network introduced in Section 3 reduces
to the following two-layer fully connected network.
k
NWt(X) = X hσ (Wf) ∙ x) - σ (Uf) ∙ x)i
i=1
We consider running gradient descent with a constant learning rate η ≤ ckη，Cη ≤ 1 and IID gaussian
initialization with mean 0 and standard deviation σg = ɪɛk^. We assume that gradient descent
minimizes the hinge loss
'(W) = ^X max{1 — yNw(x), 0}
(x,y)∈S
where optimization is only over the first layer. We will show that gradient descent converges to the
global minimum in a constant number of iterations.
For each point xi ∈ S define the following sets of neurons:
W++(i) = {j | Wj) ∙ Xi > θ}
W-(i) = {j | Wj) ∙ Xi < θ}
30
Under review as a conference paper at ICLR 2019
U+(i) = {j I Utj) ∙ Xi > 0}
U+(i)=nj i u(j) ∙ Xi < 0o
Lemma 9.1. For i ∈ {1, 3}, if j ∈ W0+(i) then j ∈ Wt+ (i) for all t > 0 and if j ∈ W0-(i) then
j ∈ Wt- (i) for all t > 0. Similarly, for i ∈ {2, 4}, ifj ∈ Ut+ (i) then j ∈ Ut+ (i) for all t > 0 and if
j ∈ Wt-(i) then j ∈ Ut-(i) for all t > 0.
Proof. Without loss of generality it suffices to prove the claim for Wt+ (1). This follows by
symmetry and the fact that j ∈ Wt+ (1) if and only if j ∈ Wt-(3). The proof is by induc-
tion. Assume that j ∈ Wti,+ . The gradient of w(i) with respect to a point (X, y) is given
by d∂Wy- (W) = -yσ0(w⑴∙ x)xIyNW(x)<i. Therefore, by the facts w(j) ∙ X3 < 0 and
xι ∙ X2, xι ∙ x4 = 0 it follows that wj)ι ∙ xι > 0, which concludes the proof.	口
Lemma 9.2. With probability at least 1 - 8e-8, for all 1 ≤ j ≤ 4
k	k
2 - 2√k ≤∣W+(j)∣,∣U+(j )1 ≤ 2 + 2√k
Proof. Without loss of generality consider W0+ (1). Since the sign ofa one dimensional Gaussian
random variable is a Bernoulli random variable, we get by Hoeffding’s inequality
P ( ∣W+(1)∣ - k < 2√k) ≤ 2e-2(22k- = 2e-8
Since ∣W+(1)∣ + ∣W+(3)∣ = k with probability 1, we get that if ∣∣W+(1)∣ - 2∣ < 2√k then
11W+ (3)∣ - 21 < 2 √k. The result now follows by symmetry and the union bound.	口
For each point Xi , define the following sums:
S+⑶=X σ (Wtj) ∙ Xi)
j∈Wt+(i)
S-⑶=X σ(w(j) ∙ Xi)
j∈Wt-(i)
R+⑶=X σ(utj) ∙ Xi)
j∈Ut+(i)
R-⑶=X σ(u(j) ∙ Xi)
j∈Ut-(i)
We will prove the following lemma regarding St+ (1), St-(1), Rt+ (1), Rt-(1) for i = 1. By symme-
try, analogous lemmas follow for i 6= 1.
Lemma 9.3. Thefollowing holds with probability ≥ 1 - √∏2kfe:
1.	For all t ≥ 0, Rt+(1) + Rt-(1) ≤ kη.
2.	For all t ≥ 0, St- (1) = 0.
3.	Let t ≥	0.	If -yNwt (xi)	<	1,	then	St+1 ⑴ ≥	S+⑴ + ∣W+ (1) ∣	4.	Otherwise, if
-yNWt(X1) ≥ 1 then St++1(1) = St+(1).
31
Under review as a conference paper at ICLR 2019
Proof. 1. For t = 0 the claim holds by Lemma 8.2. Assume by contradiction that there exists
t > 0, such that Rt+(1) + Rt- (1) > kη. It follows that, without loss of generality, there
exists j ∈ U+(1) such that σ (u(j) ∙ xι) > η. In each iteration u(j) ∙ xι can increase by
at most η, therefore by Lemma 8.2 there exists 0 ≤ t0 < t such that 0 < σ (u(j) ∙ x，≤ η
and σ (u(j]ι ∙ x，> η. However, in this case u(j+ι = u(j) + αxι + βx2 + Yx4 where
(j)	(j)
ɑ ≤ 0. Therefore, u；[i ∙ xι < Uy ∙ xι ≤ η, a contradiction.
2.	This is a direct consequence of Lemma 9.1.
3.	ByLemma9.1,if j ∈ W+(1) and-yNwt (xι) < 1 then w(j)ι = Wyj + 4xι+βx2 +γx4,
from which the first part of the claim follows. The second claim follows similarly.
□
Proposition 9.4. Assume that k ≥ 25. With probability ≥ 1 — √28k — 8e-8, for all i, if until
iteration T there were at least l ≥ k_：^ + 10 iterations, in which — yNwt (Xi) < 1, then it holds
that -yNWt (xi) ≥ 1 for allt ≥ T.
Proof. Without loss of generality assume that i = 1. By Lemma 9.3 and Lemma 8.2, with probabil-
ity ≥ 1 — √∏esk — 8e-8, if —yNwt (xι) < 1 then S++ι(1) ≥ S+(1) + (2 - 2√k) η. Therefore,
by Lemma 9.3, for all t ≥ T
NWt(x)=St+(1)+St-(1) — Rt+(1) + Rt-(1)
lη — kη
≥1
where the last ineqaulity follows by the assumption on l.	口
Theorem 9.5. Assume that k ≥ 25. With probability ≥ 1 — √28k — 8e-8, after at most 卜 ^√^ +40
iterations, gradient descent converges to a global minimum.
Proof. Proposition 9.4 implies that there are at most J4√ + 40 iterations in which there exists
(xi, yi) such that —yiNWt (xi) < 1. After at most that many iterations, gradient descent converges
to a global minimum.	口
10 VC Dimension
As noted in Remark 3.1, the VC dimension of the model we consider is at most 15. To see this, we
first define for any z ∈ {±1}2d the set Pz ⊆ {±1}2 which contains all the distinct two dimensional
binary patterns that z has. For example, for a positive diverse point z it holds that Pz = {±1}2.
Now, for any points z(1), z(2) ∈ {±1}2d such that Pz(1) = Pz(2) and for any filter w ∈ R2 it holds
that maxj σ (W ∙ zj1j) = maxy σ (W ∙ z(2j). Therefore, for any W, NW(z(1j) = NW(z(2j).
Specifically, this implies that if both z(1) and z(2) are diverse then NW(z(1)) = NW(z(2)). Since
there are 15 non-empty subsets of {±1}2, it follows that for any k the network can shatter a set of
at most 15 points, or equivalently, its VC dimension is at most 15. Despite these expressive power
limitations, there is a generalization gap between small and large networks in this setting, as can be
seen in Figure 1.
32