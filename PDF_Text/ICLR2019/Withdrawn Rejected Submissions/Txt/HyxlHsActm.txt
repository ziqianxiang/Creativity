Under review as a conference paper at ICLR 2019
Efficient Dictionary Learning with Gradi-
ent Descent
Anonymous authors
Paper under double-blind review
Abstract
Randomly initialized first-order optimization algorithms are the method of
choice for solving many high-dimensional nonconvex problems in machine
learning, yet general theoretical guarantees cannot rule out convergence to
critical points of poor objective value. For some highly structured nonconvex
problems however, the success of gradient descent can be understood by
studying the geometry of the objective. We study one such problem -
complete orthogonal dictionary learning, and provide converge guarantees
for randomly initialized gradient descent to the neighborhood of a global
optimum. The resulting rates scale as low order polynomials in the dimension
even though the ob jective possesses an exponential number of saddle points.
This efficient convergence can be viewed as a consequence of negative
curvature normal to the stable manifolds associated with saddle points, and
we provide evidence that this feature is shared by other nonconvex problems
of importance as well.
1 Introduction
Many central problems in machine learning and signal processing are most naturally for-
mulated as optimization problems. These problems are often both nonconvex and high-
dimensional. High dimensionality makes the evaluation of second-order information pro-
hibitively expensive, and thus randomly initialized first-order methods are usually employed
instead. This has prompted great interest in recent years in understanding the behavior
of gradient descent on nonconvex objectives (18; 14; 17; 11). General analysis of first- and
second-order methods on such problems can provide guarantees for convergence to critical
points but these may be highly suboptimal, since nonconvex optimization is in general an
NP-hard probem (4). Outside of a convex setting (28) one must assume additional structure
in order to make statements about convergence to optimal or high quality solutions. It is
a curious fact that for certain classes of problems such as ones that involve sparsification
(25; 6) or matrix/tensor recovery (21; 19; 1) first-order methods can be used effectively. Even
for some highly nonconvex problems where there is no ground truth available such as the
training of neural networks first-order methods converge to high-quality solutions (40).
Dictionary learning is a problem of inferring a sparse representation of data that was originally
developed in the neuroscience literature (30), and has since seen a number of important
applications including image denoising, compressive signal acquisition and signal classification
(13; 26). In this work we study a formulation of the dictionary learning problem that can be
solved efficiently using randomly initialized gradient descent despite possessing a number
of saddle points exponential in the dimension. A feature that appears to enable efficient
optimization is the existence of sufficient negative curvature in the directions normal to the
stable manifolds of all critical points that are not global minima 1 . This property ensures
that the regions of the space that feed into small gradient regions under gradient flow do
not dominate the parameter space. Figure 1 illustrates the value of this property: negative
curvature prevents measure from concentrating about the stable manifold. As a consequence
randomly initialized gradient methods avoid the “slow region” of around the saddle point.
1 As well as a lack of spurious local minimizers, and the existence of large gradients or strong
convexity in the remaining parts of the space
1
Under review as a conference paper at ICLR 2019
Figure 1: Negative curvature
helps gradient descent. Red:
“slow region” of small gradient
around a saddle point. Green: sta-
ble manifold associated with the
saddle point. Black: points that
flow to the slow region. Left: global
negative curvature normal to the
stable manifold. Right: positive
curvature normal to the stable man-
ifold - randomly initialized gradient
descent is more likely to encounter
the slow region.
The main results of this work is a convergence rate for randomly initialized gradient descent
for complete orthogonal dictionary learning to the neighborhood of a global minimum of the
objective. Our results are probabilistic since they rely on initialization in certain regions of
the parameter space, yet they allow one to flexibly trade off between the maximal number of
iterations in the bound and the probability of the bound holding.
While our focus is on dictionary learning, it has been recently shown that for other impor-
tant nonconvex problems such as phase retrieval (8) performance guarantees for randomly
initialized gradient descent can be obtained as well. In fact, in Appendix C we show that
negative curvature normal to the stable manifolds of saddle points (illustrated in Figure 1)
is also a feature of the population objective of generalized phase retrieval, and can be used
to obtain an efficient convergence rate.
2	Related Work
Easy nonconvex problems. There are two basic impediments to solving nonconvex
problems globally: (i) spurious local minimizers, and (ii) flat saddle points, which
can cause methods to stagnate in the vicinity of critical points that are not minimizers.
The latter difficulty has motivated the study of strict saddle functions (36; 14), which have
the property that at every point in the domain of optimization, there is a large gradient, a
direction of strict negative curvature, or the function is strongly convex. By leveraging this
curvature information, it is possible to escape saddle points and obtain a local minimizer
in polynomial time.2 Perhaps more surprisingly, many known strict saddle functions also
have the property that every local minimizer is global; for these problems, this implies that
efficient methods find global solutions. Examples of problems with this property include
variants of sparse dictionary learning (38), phase retrieval (37), tensor decomposition (14),
community detection (3) and phase synchronization (5).
Minimizing strict saddle functions. Strict saddle functions have the property that at
every saddle point there is a direction of strict negative curvature. A natural approach to
escape such saddle points is to use second order methods (e.g., trust region (9) or curvilinear
search (15)) that explicitly leverage curvature information. Alternatively, one can attempt
to escape saddle points using first order information only. However, some care is needed:
canonical first order methods such as gradient descent will not obtain minimizers if initialized
at a saddle point (or at a point that flows to one) - at any critical point, gradient descent
simply stops. A natural remedy is to randomly perturb the iterate whenever needed. A line
of recent works shows that noisy gradient methods of this form efficiently optimize strict
saddle functions (24; 12; 20). For example, (20) obtains rates on strict saddle functions that
match the optimal rates for smooth convex programs up to a polylogarithmic dependence on
dimension.3
2 This statement is nontrivial: finding a local minimum of a smooth function is NP-hard.
3 This work also proves convergence to a second-order stationary point under more general
smoothness assumptions.
2
Under review as a conference paper at ICLR 2019
Randomly initialized gradient descent? The aforementioned results are broad, and
nearly optimal. Nevertheless, important questions about the behavior of first order methods
for nonconvex optimization remain unanswered. For example: in every one of the aforemented
benign nonconvex optimization problems, randomly initialized gradient descent rapidly obtains
a minimizer. This may seem unsurprising: general considerations indicate that the stable
manifolds associated with non-minimizing critical points have measure zero (29), this implies
that a variety of small-stepping first order methods converge to minimizers in the large-time
limit (23). However, it is not difficult to construct strict saddle problems that are not
amenable to efficient optimization by randomly initialized gradient descent - see (12) for an
example. This contrast between the excellent empirical performance of randomly initialized
first order methods and worst case examples suggests that there are important geometric
and/or topological properties of “easy nonconvex problems” that are not captured by the strict
saddle hypothesis. Hence, the motivation of this paper is twofold: (i) to provide theoretical
corroboration (in certain specific situations) for what is arguably the simplest, most natural,
and most widely used first order method, and (ii) to contribute to the ongoing effort to
identify conditions which make nonconvex problems amenable to efficient optimization.
3	Dictionary Learning over the Sphere
Suppose we are given data matrix Y = y1, . . . yp ∈ Rn×p. The dictionary learning problem
asks us to find a concise representation of the data (13), of the form Y ≈ AX, where X is
a sparse matrix. In the complete, orthogonal dictionary learning problem, we restrict the
matrix A to have orthonormal columns (A ∈ O(n)). This variation of dictionary learning is
useful for finding concise representations of small datasets (e.g., patches from a single image,
in MRI (32)).
To analyze the behavior of dictionary learning algorithms theoretically, it useful to posit that
Y = A0X0 for some true dictionary A0 ∈ O(n) and sparse coefficient matrix X0 ∈ Rn×p,
and ask whether a given algorithm recovers the pair (A0, X0).4 In this work, we further
assume that the sparse matrix X0 is random, with entries i.i.d. Bernoulli-Gaussian5. For
simplicity, we will let A0 = I ; our arguments extend directly to general A0 via the simple
change of variables q → A0q.
(34) showed that under mild conditions, the complete dictionary recovery problem can be
reduced to the geometric problem of finding a sparse vector in a linear subspace (31). Notice
that because A0 is orthogonal, row(Y ) = row(X0). Because X0 is a sparse random matrix,
the rows of X0 are sparse vectors. Under mild conditions (34), they are the sparsest vectors
in the row space of Y , and hence can be recovered by solving the conceptual optimization
problem
min ∣∣q*Yk0	s.t. q*Y = 0.
This is not a well-structured optimization problem: the objective is discontinuous, and the
constraint set is open. A natural remedy is to replace the `0 norm with a smooth sparsity
surrogate, and to break the scale ambiguity by constraining q to the sphere, giving
1p
min ∕dl(q) ≡ — fhμ(q *yk s.t. q ∈ S .	(1)
pk=1
Here, We choose hμ(t) = μlog(cosh(t∕μ)) as a smooth sparsity surrogate. This objective was
analyzed in (35), which showed that (i) although this optimization problem is nonconvex,
when the data are sufficiently large, with high probability every local optimizer is near a
signed column of the true dictionary A0 , (ii) every other critical point has a direction of
strict negative curvature, and (iii) as a consequence, a second-order Riemannian trust region
method efficiently recovers a column of A0.6 The Riemannian trust region method is of
mostly theoretical interest: it solves complicated (albeit polynomial time) subproblems that
involve the Hessian of fDL .
4This problem exhibits a sign permutation symmetry: AoX0 = (A0Γ)(Γ*X0) for any signed
permutation matrix Γ. Hence, we only ask for recovery up to a signed permutation.
5[Xo]ij = VijΩij, with Vij ZN(0,1), Ωij 〜Bern(θ) independent.
6 Combining with a deflation strategy, one can then efficiently recover the entire dictionary A0 .
3
Under review as a conference paper at ICLR 2019
Separable Objective fSeP
1.6
1.5
1.4
1.3
1.2
Dictionary Learning Jdl
0.36
0.34
0.32
0,3
0.28
0.26
Figure 2: Left: The
separable ob jective for
n = 3. Note the
similarity to the dictio-
nary learning objective.
Right: The objective
for complete orthogo-
nal dictionary learning
(discussed in section 6)
for n = 3.

In practice, simple iterative methods, including randomly initialized gradient descent are
also observed to rapidly obtain high-quality solutions. In the sequel, we will give a geometric
explanation for this phenomenon, and bound the rate of convergence of randomly initialized
gradient descent to the neighborhood of a column of A0 . Our analysis of fDL is probabilistic
in nature: it argues that with high probability in the sparse matrix X0 , randomly initialized
gradient descent rapidly produces a minimizer.
To isolate more clearly the key intuitions behind this analysis, we first analyze the simpler
separable objective
n
min fsep(q) ≡ Xhμ(qi) st q ∈ Sn-1.	(2)
i=1
Figure 2 plots both fSep and fDL as functions over the sphere. Notice that many of the key
geometric features in fDL are present in fSep ; indeed, fSep can be seen as an “ultrasparse”
version of fDL in which the columns of the true sparse matrix X 0 are taken to have only
one nonzero entry. A virtue of this model function is that its critical points and their stable
manifolds have simple closed form expressions (see Lemma 1).
4 Outline of Important Geometric Features
Our problems of interest have the form
min f(q) s.t. q ∈ Sn-1,
where f : Rn → R is a smooth function. We let Vf (q) and V2f (q) denote the Euclidean
gradient and hessian (over Rn), and let grad [f] (q) and Hess [f] (q) denote their Riemannian
counterparts (over Sn-1). We will obtain results for Riemannian gradient descent defined by
the update
q → expq(-ηgrad[f](q))
for some step size η > 0, where expq : Tq Sn-1 → Sn-1 is the exponential map. The
Riemannian gradient on the sphere is given by grad[f](q) = (I 一 qq*)Vf(q).
We let A denote the set of critical points of f over Sn-I - these are the points q s.t.
grad [f ] (q) = 0. We let A denote the set of local minimizers, and A its complement. Both
fsep and ∕dl are Morse functions on SnT,7 we can assign an index a to every q ∈ A, which
is the number of negative eigenvalues of Hess [f] (qq).
Our goal is to understand when gradient descent efficiently converges to a local minimizer.
In the small-step limit, gradient descent follows gradient flow lines γ : R → M, which are
solution curves of the ordinary differential equation
γ(t) = -grad [f](γ(t))
To each critical point α ∈ A of index λ, there is an associated stable manifold of dimension
dim(M) 一 λ, which is roughly speaking, the set of points that flow to α under gradient flow:
Ws(α) ≡ q ∈ M
lim γ (t) = α
t→∞
γ a gradient flow line s.t. γ(0) = q
7Strictly speaking, fDL is Morse with high probability, due to results of (38).
4
Under review as a conference paper at ICLR 2019
Figure 3: Negative curvature and efficient gradient descent. The union of the light
blue, orange and yellow sets is the set C . In the light blue region, there is negative curvature
normal to ∂C , while in the orange region the gradient norm is large, as illustrated by the
arrows. There is a single global minimizer in the yellow region. For the separable ob jective,
the stable manifolds of the saddles and maximizers all lie on ∂C (the black circles denote the
critical points, which are either maximizers "a", saddles "", or minimizers "`"). The red
dots denote ∂Cζ with ζ = 0.2.
Our analysis uses the following convenient coordinate chart
,(W) = (w, J1 - IHI2) ≡ q(W)
where w ∈ B1 (0). We also define two useful sets:
C≡{q ∈ SnT |qn ≥ kwk∞}
(3)
CZ ≡ {q ∈ Snτ∣⅛∞ ≥ 1 + 4	⑷
Since the problems considered here are symmetric with respect to a signed permutation of
the coordinates we can consider a certain C and the results will hold for the other symmetric
sections as well. We will show that at every point in C aside from a neighborhood of a global
minimizer for the separable objective (or a solution to the dictionary problem that may only
be a local minimizer), there is either a large gradient component in the direction of the
minimizer or negative curvature in a direction normal to ∂C . For the case of the separable
objective, one can show that the stable manifolds of the saddles lie on this boundary, and
hence this curvature is normal to the stable manifolds of the saddles and allows rapid
progress away from small gradient regions and towards a global minimizer 8 . These regions
are depicted in Figure 3.
In the sequel, we will make the above ideas precise for the two specific nonconvex optimization
problems discussed in Section 3 and use this to obtain a convergence rate to a neighborhood
of a global minimizer. Our analysis are specific to these problems. However, as we will
describe in more detail later, they hinge on important geometric characteristics of these
problems which make them amenable to efficient optimization, which may obtain in much
broader classes of problems.
8 The direction of this negative curvature is important here, and it is this feature that distinguishes
these problems from other problems in the strict-saddle class where this direction may be arbitrary
5
Under review as a conference paper at ICLR 2019
5	Separable Function Convergence Rate
In this section, we study the behavior of randomly initialized gradient descent on the separable
function fSep . We begin by characterizing the critical points:
Lemma 1 (Critical points of fSep). The critical points of the separable problem (2) are
A = {Psn-ι [a]∣a ∈{-1,0,1 产,|同 › 0 }.	⑸
For every α ∈ A and corresponding a(α), for μ < √nCogn the stable manifold of a takes the
form
Wsg)= [Psn-ι[a(α) + b ]	supp(a(a2「s<pp㈤=0，]	⑹
∣	k k∞ <
where c > 0 is a numerical constant.
Proof. Please see Appendix A	□
By inspecting the dimension of the stable manifolds, it is easy to verify that that there
are 2n global minimizers at the 1-sparse vectors on the sphere ±bei, 2n maximizers at the
least sparse vectors and an exponential number of saddle points of intermediate sparsity.
This is because the dimension of Ws (α) is simply the dimension of b in 6, and it follows
directly from the stable manifold theorem that only minimizers will have a stable manifold
of dimension n - 1. The objective thus possesses no spurious local minimizers.
When referring to critical points and stable manifolds from now on we refer only to those
that are contained in C or on its boundary. It is evident from Lemma 1 that the critical
points in A“ all lie on ∂C and that S Ws (α) = ∂C , and there is a minimizer at its center
a∈A
given by q(0) = ben .
5.1	The effect of negative curvature on the gradient
We now turn to making precise the notion that negative curvature normal to stable manifolds
of saddle points enables gradient descent to rapidly exit small gradient regions. We do this by
defining vector fields u(i) (q), i ∈ [n - 1] such that each field is normal to a continuous piece
of ∂Cζ and points outwards relative to Cζ defined in 4. By showing that the Riemannian
gradient pro jected in this direction is positive and proportional to ζ , we are then able to show
that gradient descent acts to increase Z(q(w)) = kwqk 1 geometrically. This corresponds
to the behavior illustrated in the light blue region in Figure 3.
Lemma 2 (Separable objective gradient projection). For any w ∈ Cζ, i ∈ [n - 1], we define
a vector u(i)∈ Tq(w)Sn-1 by
I0
u(ji) =	sign(wi)
I _iwii
I qn
j ∈/ {i, n},
j =i,
j = n.
If μ log(1)≤ Wi and μ < =, then
u(i):gradf⅛ep](Q(W)) ≥ C ∣∣wk∞ Z,
where c > 0 is a numerical constant.
(7)
Proof. Please see Appendix A.	□
Since we will use this property of the gradient in Cζ to derive a convergence rate, we will
be interested in bounding the probability that gradient descent initialized randomly with
respect to a uniform measure on the sphere is initialized in Cζ . This will require bounding
the volume of this set, which is done in the following lemma:
6
Under review as a conference paper at ICLR 2019
Lemma 3 (Volume of Cζ). For Cζ defined as in (4) we have
VOl(CZ) ≥ 1 - log(n) Z
Vol(SnT) ≥ 2n	~ζ
Proof. Please see Appendix D.3.
□
5.2	Convergence rate
Using the results above, one can obtain the following convergence rate:
Theorem 1 (Gradient descent convergence rate for separable function). For any 0 < ζ0 < 1,
r > μ log(1), Riemannian gradient descent with step size η < min { Cn, μ } on the separable
objective (2) with μ < √nC0g n,enters an L∞ ball of radius r around a global minimizer in
T<C (√n + log
η r 2
iterations with probability
P ≥ 1- 2 log(n)ζ0,
where ci, C > 0 are numerical constants.
Proof. Please see Appendix A.
□
We have thus obtained a convergence rate for gradient descent that relies on the negative
curvature around the stable manifolds of the saddles to rapidly move from these regions
of the space towards the vicinity of a global minimizer. This is evinced by the logarithmic
dependence of the rate on ζ. As was shown for orthogonal dictionary learning in (38), we also
expect a linear convergence rate due to strong convexity in the neighborhood of a minimizer,
but do not take this into account in the current analysis.
6	Dictionary Learning Convergence Rate
The proofs in this section will be along the same lines as those of Section 5. While we
will not describe the positions of the critical points explicitly, the similarity between this
objective and the separable function motivates a similar argument. It will be shown that
initialization in some Cζ will guarantee that Riemannian gradient descent makes uniform
progress in function value until reaching the neighborhood of a global minimizer. We will
first consider the population ob jective which corresponds to the infinite data limit
fDLp⑷ ≡ E fDL ⑷=Eχ~i.i.d.BG(θ)[ hμ(q*χH∙	⑻
X0
and then bounding the finite sample size fluctuations of the relevant quantities. We begin
with a lemma analogous to Lemma 2:
Lemma 4 (Dictionary learning population gradient). For w ∈ Cζ,r < ∣Wi∣,μ < Cιr5/2√Z
the dictionary learning population objective 8 obeys
U⑴*grad[fDLp](q(w)) ≥ cθr3Z
where cθ depends only on θ, c1 is a positive numerical constant and u(i) is defined in 7.
Proof. Please see Appendix B	口
Using this result, we obtain the desired convergence rate for the population objective,
presented in Lemma 11 in Appendix B. After accounting for finite sample size fluctuations
in the gradient, one obtains a rate of convergence to the neighborhood of a solution (which
is some signed basis vector due to our choice A0 = I )
7
Under review as a conference paper at ICLR 2019
Theorem 2 (Gradient descent convergence rate for dictionary learning). For any 1 > ζ0 >
0,s > 4√2, Riemannian gradient descent with step size η < 一径。「on the dictionary learning
objective 1 with μ < ：5/10, θ ∈ (0, 2), enters a ball of radius c3s from a target solution in
T<C2 (1+ n log；)
ηθ s	ζ0
iterations with probability
P ≥ 1 - 2 log(n)ζ0 - Py - c8p-6
where y = c7"9-?"0, Py is given in Lemma 10 and Ci,C are positive constants.
Proof. Please see Appendix B	□
The two terms in the rate correspond to an initial geometric increase in the distance from
the set containing the small gradient regions around saddle points, followed by convergence
to the vicinity of a minimizer in a region where the gradient norm is large. The latter is
based on results on the geometry of this ob jective provided in (38).
7 Discussion
The above analysis suggests that second-order properties - namely negative curvature normal
to the stable manifolds of saddle points - play an important role in the success of randomly
initialized gradient descent in the solution of complete orthogonal dictionary learning. This
was done by furnishing a convergence rate guarantee that holds when the random initialization
is not in regions that feed into small gradient regions around saddle points, and bounding
the probability of such an initialization. In Appendix C we provide an additional example of
a nonconvex problem that for which an efficient rate can be obtained based on an analysis
that relies on negative curvature normal to stable manifolds of saddles - generalized phase
retrieval. An interesting direction of further work is to more precisely characterize the class
of functions that share this feature.
The effect of curvature can be seen in the dependence of the maximal number of iterations
T on the parameter ζ0 . This parameter controlled the volume of regions where initialization
would lead to slow progress and the failure probability of the bound 1 - P was linear in ζ0 ,
while T depended logarithmically on ζ0 . This logarithmic dependence is due to a geometric
increase in the distance from the stable manifolds of the saddles during gradient descent,
which is a consequence of negative curvature. Note that the choice of ζ0 allows one to flexibly
trade off between T and 1 - P. By decreasing ζ0 , the bound holds with higher probability,
at the price of an increase in T . This is because the volume of acceptable initializations now
contains regions of smaller minimal gradient norm. In a sense, the result is an extrapolation
of works such as (23) that analyze the ζ0 = 0 case to finite ζ0.
Our analysis uses precise knowledge of the location of the stable manifolds of saddle points.
For less symmetric problems, including variants of sparse blind deconvolution (41) and
overcomplete tensor decomposition, there is no closed form expression for the stable manifolds.
However, it is still possible to coarsely localize them in regions containing negative curvature.
Understanding the implications of this geometric structure for randomly initialized first-order
methods is an important direction for future work.
One may hope that studying simple model problems and identifying structures (here, negative
curvature orthogonal to the stable manifold) that enable efficient optimization will inspire
approaches to broader classes of problems. One problem of obvious interest is the training
of deep neural networks for classification, which shares certain high-level features with the
problems discussed in this paper. The objective is also highly nonconvex and is conjectured
to contain a proliferation of saddle points (11), yet these appear to be avoided by first-order
methods (16) for reasons that are still quite poorly understood beyond the two-layer case
(39).
8
Under review as a conference paper at ICLR 2019
References
[1]	Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal
tensor decomposition via alternating rank-1 updates. arXiv preprint arXiv:1402.5180,
2014.
[2]	Radu Balan, Pete Casazza, and Dan Edidin. On signal reconstruction without phase.
Applied and Computational Harmonic Analysis, 20(3):345-356, 2006.
[3]	Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank
approach for semidefinite programs arising in synchronization and community detection.
In Conference on Learning Theory, pages 361-382, 2016.
[4]	Dimitri P Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
[5]	Nicolas Boumal. Nonconvex phase synchronization. SIAM Journal on Optimization,
26(4):2355-2377, 2016.
[6]	Michael M Bronstein, Alexander M Bronstein, Michael Zibulevsky, and Yehoshua Y
Zeevi. Blind deconvolution of images using optimal sparse representations. IEEE
Transactions on Image Processing, 14(6):726-736, 2005.
[7]	Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via
wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory,
61(4):1985-2007, 2015.
[8]	Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random
initialization: Fast global convergence for nonconvex phase retrieval. arXiv preprint
arXiv:1803.07726, 2018.
[9]	Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1.
Siam, 2000.
[10]	John V Corbett. The pauli problem, state reconstruction and quantum-real numbers.
Reports on Mathematical Physics, 57:53-68, 2006.
[11]	Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
and Yoshua Bengio. Identifying and attacking the saddle point problem in high-
dimensional non-convex optimization. In Advances in neural information processing
systems, pages 2933-2941, 2014.
[12]	Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Barnabas Poczos, and Aarti Singh.
Gradient descent can take exponential time to escape saddle points. arXiv preprint
arXiv:1705.10412, 2017.
[13]	Michael Elad and Michal Aharon. Image denoising via sparse and redundant represen-
tations over learned dictionaries. IEEE Transactions on Image processing, 15(12):3736-
3745, 2006.
[14]	Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points?online
stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages
797-842, 2015.
[15]	Donald Goldfarb. Curvilinear path steplength algorithms for minimization which use
directions of negative curvature. Mathematical programming, 18(1):31-40, 1980.
[16]	Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing
neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
[17]	Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical
systems. arXiv preprint arXiv:1609.05191, 2016.
[18]	Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better:
Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
9
Under review as a conference paper at ICLR 2019
[19]	Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion
using alternating minimization. In Proceedings of the forty-fifth annual ACM symposium
on Theory of computing, pages 665-674. ACM, 2013.
[20]	Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How
to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
[21]	Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion
from a few entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010.
[22]	Ken Kreutz-Delgado. The complex gradient operator and the cr-calculus. arXiv preprint
arXiv:0906.4835, 2009.
[23]	Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan,
and Benjamin Recht. First-order methods almost always avoid saddle points. arXiv
preprint arXiv:1710.07406, 2017.
[24]	Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent
only converges to minimizers. In Conference on Learning Theory, pages 1246-1257,
2016.
[25]	Kiryung Lee, Yihong Wu, and Yoram Bresler. Near optimal compressed sensing of
sparse rank-one matrices via sparse power factorization. arXiv preprint, 2013.
[26]	Julien Mairal, Francis Bach, Jean Ponce, et al. Sparse modeling for image and vision
processing. Foundations and TrendsR in Computer Graphics and Vision, 8(2-3):85-283,
2014.
[27]	Jianwei Miao, Tetsuya Ishikawa, Bart Johnson, Erik H Anderson, Barry Lai, and
Keith O Hodgson. High resolution 3d x-ray diffraction microscopy. Physical review
letters, 89(8):088303, 2002.
[28]	Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2013.
[29]	Liviu Nicolaescu. An invitation to Morse theory. Springer Science & Business Media,
2011.
[30]	Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties
by learning a sparse code for natural images. Nature, 381(6583):607, 1996.
[31]	Qing Qu, Ju Sun, and John Wright. Finding a sparse vector in a subspace: Linear
sparsity using alternating directions. In Advances in Neural Information Processing
Systems, pages 3401-3409, 2014.
[32]	Saiprasad Ravishankar and Yoram Bresler. Mr image reconstruction from highly
undersampled k-space data by dictionary learning. IEEE transactions on medical
imaging, 30(5):1028-1041, 2011.
[33]	Yoav Shechtman, Yonina C Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei
Miao, and Mordechai Segev. Phase retrieval with application to optical imaging: a
contemporary overview. IEEE signal processing magazine, 32(3):87-109, 2015.
[34]	Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used
dictionaries. In Conference on Learning Theory, pages 37-1, 2012.
[35]	Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. In
Sampling Theory and Applications (SampTA), 2015 International Conference on, pages
407-410. IEEE, 2015.
[36]	Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv
preprint arXiv:1510.06096, 2015.
10
Under review as a conference paper at ICLR 2019
[37]	Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In
Information Theory (ISIT), 2016 IEEE International Symposium on, pages 2379-2383.
IEEE, 2016.
[38]	Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere
i: Overview and the geometric picture. IEEE Transactions on Information Theory,
63(2):853-884, 2017.
[39]	Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic
dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
[40]	Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
Understanding deep learning requires rethinking generalization. arXiv preprint
arXiv:1611.03530, 2016.
[41]	Yuqian Zhang, Yenson Lau, Han-wen Kuo, Sky Cheung, Abhay Pasupathy, and John
Wright. On the global geometry of sphere-constrained sparse blind deconvolution. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
4894-4902, 2017.
11
Under review as a conference paper at ICLR 2019
A Proofs - Separable Objective
Proof of Lemma 1: (Critical point structure of
separable ob jective)	.
elements tanh(q)i = tanh( qi) We have
Denoting by tanh( qμ) a vector in Rn
grad[fsep](q)i = (I - qq*)tanh(q)
μ
.Thus critical points are ones where either tanh(q) = 0 (which cannot happen on Sn-I)
or tanh( qμ) is in the nullspace of (I — qq*), which implies tanh( q) = Cq for some constant
b. The equation tanh(X) = bx has either a single solution at the origin or 3 solutions at
{0, ±r(b)} for some r(b). Since this equation must be solves simultaneously for every element
of q, we obtain ∀i ∈ [n] : qi ∈ {0, ±r(b)}. To obtain solutions on the sphere, one then uses
the freedom we have in choosing b (and thus r(b)) such that kqk = 1. The resulting set of
critical points is thus
A = PSn-1 {-1, 0, 1}n \ {0} .
To prove the form of the stable manifolds, we first show that for qi such that |qi | = kqk∞
and any qj such that |qj | + ∆ = |qi | and sufficiently small ∆ > 0, we have
-grad[fSep](q)isign(qi) > -grad[fSep](q)isign(qj)	(9)
For ease of notation we now assume qi , qj > 0 and hence ∆ = qi - qj , otherwise the argument
can be repeated exactly with absolute values instead. The above inequality can then be
written as
(qi — qj)X tanh(qk)qk — tanh(qi) + tanh( —) > 0.
k=ι	μ	μ	μ
'--------------------------------------------}
{z^
≡h
n — 1	______________
If We now define	s2 =	E	q2	and	qn	=，1 — s2	—	(qj + ∆)2We have
k=1
k 6= i, n
(	tanh(中)(%• + △) +
h = I tanh( √ 1-s2 —(qj32 )，1 — s2 — (%∙+ ∆)2 )
+△ P tanh(qk)qk - tanh( j^) + tanh( j)
k6=i,n
∆
/ P tanh(詈)qk + tanh( j )qj	∖
k=i,n
+ tanh( √1-12-qj )qi — s2 — q2
'--------------{-------------}
≡hι
—sech2 (—)—
μ μ
∖	'≡{z_}}	/
+ O(∆2)
where the O(∆2) term is bounded. Defining a vector r ∈ Rn by
k = i,n :rk = qk,r = tanh(qj)qj,『n = ʌ/l 一 s2
—
12
Under review as a conference paper at ICLR 2019
we have krk2 = 1. Since tanh(x) is concave for x > 0, and |ri| ≤ 1, we find
n	r	1n	1
hi =	tanh( — )r% ≥ tanh(-)1^^r2 = tanh(-).
μ^	μ	μ	μ
k=1	k=1
From |qi| = ∣∣q∣∣∞ it follows that q% ≥ % and thus qj ≥ √1n 一 ∆. Using this inequality and
properties of the hyperbolic secant we obtain
h2 ≤ 4exp(-2 — — logμ) ≤ exp(^^ —
2
———log μ + log4)
μ√n
μ
μ
and PlUggingin μ = √ncogn
for some c < 1
2∆
≤ exp(—
μ
— 2 Iog n — log c + ∣o log n + log log n + log 4).
We can bound this quantity by a constant, say h2 ≤ ɪ ,by requiring
A ≡	— log c + (ɪ - -) log n + log log n ≤ — log 8
μ	2c
and for and c < 1, using 一 logn + log log n < 0 we have
2∆	2
A <-----log C — (---1) log n.
c
μ
Since ∆ can be taken arbitrarily small, it is clear that c can be chosen in an n-independent
manner such that A ≤ 一 log 8. We then find
hi - h? ≥ tanh(-) - - ≥ tanh(√n log n) — — > 0
since this inequality is strict, ∆ can be chosen small enough such that O(∆2) < ∆(h1 — h2)
and hence
h > 0,
Proving 9.
It follows that under negative gradient flow, a point with |qj| < ∣∣q∣∣∞ cannot flow to a point
q0 such that |qj| = ∣∣q0∣∣∞. From the form of the critical points, for every such j, q must
thus flow to a point such that qj0 = 0 (the value of the j coordinate cannot pass through
0 to a point where |qj| = ∣∣q0∣∣∞ since from smoothness of the objective this would require
passing some q00 with qj00 = 0, at which point grad [fSep] (q00)j = 0).
As for the maximal magnitude coordinates, if there is more than one coordinate satisfying
|qi1 | = |qi2 | = kqk∞ , it is clear from symmetry that at any subsequent point q0 along
the gradient flow line qi0 = qi0 . These coordinates cannot change sign since from the
smoothness of the objective this would require that they pass through a point where they
have magnitude smaller than 1∕√n, at which point some other coordinate must have a larger
magnitude (in order not to violate the spherical constraint), contradicting the above result for
non-maximal elements. It follows that the sign pattern of these elements is preserved during
the flow. Thus there is a single critical point to which any q can flow, and this is given by
setting all the coordinates with |qj | < kqk∞ to 0 and multiplying the remaining coordinates
by a positive constant to ensure the resulting vector is on Sn . Denoting this critical point
by α, there is a vector b such that q = Psn-ι [a(α) + b] and supp(a(α)) ∩ supp(b) = 0,
13
Under review as a conference paper at ICLR 2019
kbk∞ < 1 with the form of a(α) given by 5 . The collection of all such points defines the
stable manifold of α.
□
Proof of Lemma 2: (Separable ob jective gradient pro jection). i) We consider the
sign(wi) = 1 case; the sign(wi) = -1 case follows directly. Recalling that
U⑴*gradf⅛ep](q(w)) = tanh (詈)-tanh (贲)Wn, we first prove
tanh (Wi) — tanh ( qn ) W ≥ c(qn — Wi)
∖ μ √	∖μ J qn
(10)
for some c > 0 whose form will be determined later. The inequality clearly holds for Wi = qn .
To verify that it holds for smaller values of Wi as well, we now show that
∂- [tanh WWi) - tanh ⑷ W - c5 - w°)
∂wi L	∖ μ √	μ μ q qn
<0
which will ensure that it holds for all Wi. We define s2 = 1 - ||w||2 + Wi2 and denote
qn = ∖JS — W to extract the Wi dependence, giving
≤
+ 2c
Where in the last inequality we used properties of the sech function and qn ≥ Wi . We thus
want t shw
-2 wi l	-2 qn
(e μ + e μ
+ 2c ≤ tanh
(qn ∖ q2 + w2
μ J	qn
and using log( 1 )μ ≤ Wi ≤ qn and C
ι-μ2
ι+μ2-8”
we have
4
μ
2
4 / _9 Wi	C qn ∖
—(e-2 石 + e-2 万 J +2c
8e-2 W
≤ —-------+ 2c ≤ 8μ + 2c ≤
μ
1 — μ2
1 + μ2
1
tanh
< tanh q qn
μ
≤ tanh q qn
μ
A qn + W2
q	qn
qn
μ
and it follows that 10 holds. For μ < R we are guaranteed that c > 0.
14
Under review as a conference paper at ICLR 2019
From examining the RHS of 10 (and plugging in qn =，s2 — WIi) We see that any lower
bound on the gradient of an element wj applies also to any element |wi| ≤ |wj |. Since for
∣Wjl = ∣∣w∣∣∞ We have qn — Wj = WjZ, for every log( 1 )μ ≤ Wi We obtain the bound
u(i)*gradf⅛ep](Q(W)) ≥ c ∣∣wk∞ Z
□
Proof of Theorem 1: (Gradient descent convergence rate for separable function).
We obtain a convergence rate by first bounding the number of iterations of Riemannian
gradient descent in。《° \C1, and then considering C↑∖B∞.
From Lemma 16 we obtain。(° \C1 ⊆。(0 ∖B∞√n^. Choosing c? so that μ < 2, we can apply
Lemma 2, and for u defined in 7, We thus have
1	、	(i)*
∣Wi∣ > μlog(-) ⇒ u( ) grad[fsep](q(w)) > c∣∣w∣∣∞Zo.
μ
Since from Lemma 7 the Riemannian gradient norm is bounded by √n, we can choose ci, c2
such that μ log( 1) < 2√n+3, η < 6『.2+3二.ThiS choice of η then satisfies the conditions of
Lemma 17 with r = μlog(μ), b = √=, M = √n, which gives that after a gradient step
Z 0 ≥ ζ (1 + 2c rn+3 η)≥ζ (1 + cη)	(II)
for some suitably chosen c > 0. If we now define by w(t) the t-th iterate of Riemannian
gradient descent and	Z(t)	≡	|句也|---1, Z(0) ≡	Zo,	for iterations such that w(t)	∈	CZ\Ci	we
find	∞
Z(t) ≥ Z(t-i) (1 + Cn) ≥ Zo (1 + Cn))
and the number of iterations required to exit Cζ0 \C1 is
t1
log(圭)
iog(1 + Cn)
(12)
To bound the remaining iterations, we use Lemma 2 to obtain that for every W ∈。(0 ∖B∞,
∣grad[fSep](q(W))∣2 ≥
∣∣u(i)*grad[fsep](q(w))∣∣2
∣∣u(i)∣∣2
≥ Z02c2r2
2
where we have used ||u(i)||2 = 1 +	≤ 2. We thus have
qn
T-1
X ∣∣grad[fSep](q(W)(i))
i=0
)1-1	2 T-1
X ∣∣grad[fSep](q(W)(i))∣∣ + X ∣∣
i=0	i=)1
grad[fSep](q(W)(i))
> (n¾)t1 + (T - t1)c2r2.
(13)
Choosing n < 2L where L is the gradient Lipschitz constant of fs, from Lemma 5 we obtain
2	(fsep(q (O))-fSep) > Tg ∣∣grad[fsep](q(i))∣∣2 .
n	i=0
According to Lemma B, L = 1∕μ and thus the above holds if we demand n < 2. Combining
12 and 13 gives
15
Under review as a conference paper at ICLR 2019
T<
2 fSep S(O)) - fSep) + (1 - (n+¾r2) log( Z0)
ηc2r2	log(1 + Cn)
To obtain the final rate, We use in g(w0) 一 g* ≤ √n and cη < 1 ⇒ ]。目.：..)< Cn for some
CC > 0. Thus one can choose C > 0 such that
T<C (√n + log(Z0)).	(14)
From Lemma 1 the ball Br∞ contains a global minimizer of the objective, located at the
origin.
The probability of initializing in Cζ0 is simply given from Lemma 3 and by summing over
A
the 2n possible choices of Cζ0, one for each global minimizer (corresponding to a single signed
basis vector).
□
Lemma 5 (Riemannian gradient descent iterate bound). For a Riemannian gradient descent
algorithm on the sphere with step size tk < 玲,where L is a Iipschitz constant for Vf (q),
one has
f(qι) - f (q?) ≥ f(qι) - f(qτ)
≥ tkkgrad [f] (qk)k2.
Proof. Just as in the euclidean setting, We can obtain a loWer bound on progress in function
values of iterates of the Riemannian gradient descent algorithm from a loWer bound on the
Riemannian gradient. Consider f : Sn-1 → R, Which has L-lipschitz gradient. Let qk denote
the current iterate of Riemannian gradient descent, and let tk > 0 denote the step size. Then
We can form the Taylor approximation to f ◦ Expq (v) at 0qk :
^ .	-1 .	..
f： Bι(0qk) ∩ Tqk SnT → R : v→ f(qk) + hv, Vf(qJ.
From Taylor’s theorem, We have for any v ∈ B1 (0q ) ∩ Tq Sn-1
1
lf(v) - f ◦ EXpqk (v)| ≤ 2 IIHess[f ](qfc )kkv - 0qk k ,
Where the matrix norm is the operator norm on Rn×n . Using the gradient-lipschitz property
of f , We readily compute
IHess[f](qk)I ≤ IV2f(qk)I + |hVf(qk),qki|
≤ 2L,
since Vf (0) = 0 and qk ∈ Sn-1 . We thus have
f ◦ EXpqk(V) ≤ f(q⅛)+ hv, Vf(qfc)i+ Lkvk2.
If we put v = —tkgrad[f](qk) and write qfc+1 = EXpqJ-tkgrad [f] (qfc)), the previous
expression becomes
f(qk+1) ≤ f(qk) -tkkgrad[f] (qk)k2 +t2kLkgrad[f] (qk)k2
≤ f (qk) - tkkgrad [f](qk)k2
if tk < 2L. Thus progress in objective value is guaranteed by lower-bounding the Riemannian
gradient.
16
Under review as a conference paper at ICLR 2019
As in the euclidean setting, summing the previous expression over iterations k now yields
T-1
Xf(qk) -f(qk+1) =f(q1) -f(qT)
k=1
t T-1
≥ 2k Ekgrad[fMqQk2;
k=1
in addition, it holds f (q1) - f(qT) ≤ f (q1) - f (q?). Plugging in a constant step size gives
□
the desired result.
Lemma 6 (Lipschitz constant of Vf). For any xι, x2 ∈ Rn, it holds
W (XI)-VfM)k≤ 1 …k.
Proof. It will be enough to study a single coordinate function of Vf. Using a derivative
given in section D.1, we have for x ∈ R
tθn tanh(x∕μ) = Lsech2 (X).
dx	μ μμ)
A bound on the magnitude of the derivative of this smooth function implies a lipschitz
constant for X → tanh(x∕μ). To find the bound, We differentiate again and find the critical
points of the function. We have, using the chain rule,
sech Q) ∙ (ex/决 + bχ呼
1 exμ _ 1 e-x/*、
μ μ
1 exμ - e-χ∕μ
=------  ；-------；~~TT .
μ2 (eχ∕μ + e-x/*)3
The denominator of this final expression vanishes noWhere. Hence, the only critical point
satisfies x∕μ = —x∕μ, which implies X = 0. Therefore it holds
ɪ tanh(x∕μ) ≤ Lsech2(0) = ɪ,
which shows that tanh(x∕μ) is (l∕μ)-lipschitz.
Now let x1 and x2 be any two points of Rn . Then one has
kVf(xι) — Vf(x2)k = (X (tanh(xii∕μ) — tanh(x2i∕μ))2)
μ
1/2
2
completing the proof.
□
Lemma 7 (Separable objective gradient bound). The separable objective gradient obeys
kVwg(w)k ≤ √2n
kgrad[f](q)k ≤ √
17
Under review as a conference paper at ICLR 2019
Proof. Recalling that the Euclidean gradient is given by ▽ fsep(q)i = tanh (qi) We use
Jensen’s inequality, convexity of the L2 norm and the triangle inequality to obtain
kVgs(w)k2 ≤ kVfsep(q)k2 + tanh (qnJ ʧ ≤ 2n
∖ μ √	qn
While
IIgrad[fsep](q)k = k(I - qq*)Vfsep(q)∣∣ ≤ IlVfSeP(q)∣∣ = √
□
B Proofs - Dictionary Learning
Proof of Lemma 4:(Dictionary learning population gradient). For simplicity We
consider the case sign(wi) = 1. The converse folloWs by a similar argument. We have
U⑴*grad[fD∕(q(w))=
Ex
tanh (qjwx) (Tnw + Xi
(15)
Following the notation of (38), We write Xj = bjVj where bj 〜 Bern(θ), Vj 〜N(0,1) and de-
note the vectors of these variables by J , v respectively. Defining Y (n) =	q(w)j xj , X(n) =
j6=n
qnVn, Y is Gaussian conditioned on a certain setting of J . Using Lemma 40 in (38) the first
term in 15 is
—
wiθ
-E Ev,J|bn = 1
qn
Y (n) + X(n)
tanh ------------
X(n)
---θ θEv,J∣bn = 1
μ
2 Y (n) + X(n)
sech2 --------------
μ
μ
and similarly the second term in 15 is, with X(i) = wiVi, Y (i) = P q(w)j Xj
j6=i
θ
wi EvJ 也=1
tanh
i) + X(i)
X(i)
μ
wiθ E
——Ev,J∣bi=1
μ
sech2 q q^wx
μ
if we now define X = E q*(w)jXj we have
j6=n,i
U⑴*grad[fDL)](q(w))
wiθ
Ev,J |bi=1
-Ev,J |bn=1
[sech2 (丁)]
i [sech2 (T 力
μ
sech2 X X + bnqnVn+WiVi
*	/
-Sech2 X ' + I,Vn + WibiV
∖	N
w≡F Ev,J∖{n,i}
μ
sech2 (x+wivi)
-sech2 ( x+qnvn )
(16)
18
Under review as a conference paper at ICLR 2019
B.1 Bounds for E sech2(Y )
We already have a lower bound in Lemma 20 of (38) that we can use for the second term, so
We need an upper bound for the first term. Following from p. 865, We define Y 〜 N (0, σY2 ) ,
Z = exp (-1Y), and defining β = 1 一 力 for some T > 1 we have
sech2(Y∕μ)
4Z /	4Z
(1 + Z)2 ≤ (1 + βz)2
∞
XbkZk+1
k=0
Where bk = (—β)k(k + 1). Using B.3 from Lemma 40 in (38) we have
∞∞
E XbkZk+11Y>0 = XbkE 卜-2(k+I)Y/μly>o]
k=0	k=0
Xbk exp (I (千)卜(千σγ)
Where Φc(x) is the complementary Gaussian CDF (The exchange of summation and expec-
tation is justified since Y > 0 implies Z ∈ [0, 1], see proof of Lemma 18 in (38) for details).
Using the following bounds √2∏ (1 — χ3) e-x2/2 ≤ Φc(x) ≤ √2∏ (1 — χ3 + χ5) e-x2/2 by
applying the upper (lower) bound to the even (odd) terms in the sum, and then adding a
non-negative quantity, we obtain
∞
≤√∏一(k+1)
+
/
1
1
—
[2(+) σY	( 2≡ σY )3
/	∖
√∏Xek (k+1)
3
2 2(k+1)
σY
∞
∞
and using P (—β)k
k=0
ɪ P , bk,„ >
1+β , , J (k+1)3 >
k=0
taking T → ∞ so that β → 1 we have
0 P |bk|
0,k=L
≤ 2 (from Lemma 17 in (38)) and
∞
XbkE [Zk+1 lY>o]
k=0
1	1
2√2π 2 σY
1	6
+√2π E
≤
giving the upper bound
∞
E IseCh2(Y∕μ)] = E [1 — tanh2(Y∕μ)] ≤ 8工bkE [Zk+11Y>0]
k=0
≤ r2上工	3μ5
-V ∏ σγ	2√2πσY
while the lower bound (Lemma 20 in (38)) is
2" μ	2μ3	3μ5
π σγ	√2πσY 2√2πσY
≤ E [seCh2 (Y)]
19
Under review as a conference paper at ICLR 2019
B.2 Gradient bounds
After conditioning on J \{n, i} the variables X + qnvn , X + qivi are Gaussian. We can thus
plug the bounds into 16 to obtain
U⑺*grad[fDop](q(w)) ≥ ʌ/∏2Wiθ(1 - θ)
1__________μ2____________3μ4
√σX+w2	(σX+w2)3/2	4(σX +w2)5/2
1	3μ4
--
√σX+qn	4(σχ+qn)/
≥ 2WiWiθ(1 - θ)
√σX+qn - √σX+w2
√σX+qn √σX+w2
.μ2 _ 3μ4	'
F - 2w5
the term in the expectation is positive since qn > ∣∣w∣∣∞ (1 + Z) >Wi giving
/2	E Ej∖{ni} [ σ2+ + qn2 1 ʌ
≥ y ∏Wiθ(i - θ)	, ʃ [ -σX+ + wi」I
∖	- W - 2w5	)
. To extract the ζ dependence we plug in qn > Wi (1 + ζ) and develop to first order in ζ
(since the resulting function of ζ is convex) giving
/	Γ w2ζ ] ∖
≥r"d :/H )
∖	- W3 - 2w5	)
≥ r θ(1-θ) (w3ζ - W2 - 3μ4)
Given some Z and r such that Wi > r, if We now choose μ such that μ < γ S+∖r ζ-1 r
we have the desired result. This can be achieved by requiring μ < cιr5/2√ζ for a suitably
chosen c1 > 0.
□
Lemma 8 (Point-wise concentration of pro jected gradient). For u(i) defined in 7, the
gradient of the objective 1 obeys
PhWi)*grad[fDL](q) - E [u⑴*grad[fDL](q)] ∣ ≥ t]
≤2exp (-4⅛)
Proof of Lemma 8: (Point-wise concentration of pro jected gradient). If we de-
note by Xi a column of the data matrix with entries Xj 〜BG(θ), we have
u(i)*grad[fDL](q(w))
1p
一 〉tanh
p
k=1
q*(w)xk
μ
XxTa ) ≡ P X Zk
20
Under review as a conference paper at ICLR 2019
.Since tanh(x) is bounded by 1,
4ι ≤ ∣(xk - xn qi) J ≡ I uT xk I
2 T , w2
.Invoking Lemma 21 from (38) and ∣∣u∣∣ = 1 + 茁 ≤ 2 we obtain
E [∖Zk|m] ≤ EZ〜N(0,2)[|Z|m] ≤√2m(m - 1)!!
≤ 2√2m-2 型
一	2
and using Lemma 36 in (38) with R = √2, σ = √2 we have
P [|VgDL(w)i - E [VgDL(w)i]| ≥ t]
≤2exp (-ri‰)
□
Lemma 9 (Projection Lipschitz Constant). The Lipschitz constant for u(i)*grad[∕pL](q(w))
is
L = 2√nIlX∣∣∞ (≡k∞ + ι)
Proof of Lemma 9: (Projection Lipschitz Constant). We have
Iuj)*grad[fD.](q(w)) - u(j)*grad[/DL](q(w/))|
=1XX J tanh(q⅜M)Bj- qn⅛) Wj) 1
P⅛ [ -tanh( q^) Qj- q⅛Wj) j
≡ 1X [tanh(-U )s(w) - tanh(^wx)s(w1]
p M L M	μ 」
where we have defined s(w) = xj — q Xw) Wj. Using q(w), q(w0) ∈ C ⇒ qn(w), qn(wz) ≥ 2√√n
we have
IS(W)- S(W5 = i Xn i
wj
Wj
---1-. - ---1---
qn (W) qn(w0)
≤ |xn| 2√n ∣∣w - Wll
Lemma 25 in (38) gives
tanh(^^) - tanh()≤ @同 IIW - Wil
μ	μ	μ
We also use the fact that tanh is bounded by 1 and S(W) is bounded by ∣∣X∣∣∞. We can
then use Lemma 23 in (38) to obtain
Iuj)*grad[fDL](q(W)) - u(j)*grad[/DL](q(W/))|
≤ 2√n X(: a 匕+WkW-w'∣
21
Under review as a conference paper at ICLR 2019
≤ 2√n kX k∞ (kX∞ + ι)kw - w0k
We thus have L = 2√n∣∣X∣∣∞ (kX* + 1).
□
Lemma 10 (Uniformized gradient fluctuations). For all w ∈ Cζ, i ∈ [n], with probability
P > Py
we have
U⑴*grad[fDL ](q(w))
-E [u(i)*grad[fDL](q(w))]
≤ y(θ, ζ)
where
Py ≡ 2 exp
/	-1 py(θ<)2	+ ιog(n)
4 4+√2y(θ,Z) + g( )
∕48√n( 4⅛μnp)+√lOg(M
∖ +n log ( —	y(θ,ζ)------
Proof: B
Proof of Lemma 10:(Uniformized gradient fluctuations). For X ∈ Rn×p With i.i.d.
BG(θ) entries, we define the event E∞ ≡ {1 ≤ ∣∣X∣∣∞ ≤ 4√log(np)}. We have
P[E∞c ] ≤ θ(np)-7 + e-0.3θnp
For any ε ∈ (0,1) we can construct an ε-net N for C^B2?o√E(——Iy(O) With at most (3∕ε)n
points. Using Lemma 9, on E∞ , grad[fDL](q)i is L-Lipschitz with
L = 8√n (4^og(np) + pιog(np))
.If we choose ε =喝：> we have
6L
|N l≤ (^V )n
y(θ, ζ)
. We then denote by Eg the event
U⑴*grad[fDL](q(w))	≤ y(-Q
-E [u(i)*grad[fDL](q(w))]	—	2
and obtain that on Eg ∩ E∞
sup	∣VgDL(w)i - E [VgDL(w)i]∣ ≤ y(θ,ζ)
w∈Cζ,i∈[n]
max
w∈N,i∈[n]
.Setting t = b(θ) in the result of Lemma 8 gives that for all W ∈ Cζ,i ∈ [n],
p Γ	u(i)*grad[fDL](q(w))	≥ y(θ,Z
I -E[u(i)*grad[fDL](q(w))]	—	2
≤ 2 exp
(-1	py(θ,Z)2	)
44 + 2√2y(θ,Z))
and thus
P Egc ≤ 2 exp
_ 1 py(θ,ζ)2
4 4+√2y(θ,Z)2
+n log (⅛) + log(n)
□
22
Under review as a conference paper at ICLR 2019
Lemma 11 (Gradient descent convergence rate for dictionary learning - population). For
any 1 > Zo > 0 and S > 4√, Riemannian gradient descent with step size η < Cns on the
dictionary learning population objective 8 with μ < cn5∕40, θ ∈ (0,1), enters a ball of radius
c3 s from a target solution in
T<C (1+ n logɪ )
ηθ s	ζ0
iterations with probability
P≥ 1 - 2 log(n)ζ0
where the ci , Ci are positive constants.
Proof of Lemma 11: (Gradient descent convergence rate for dictionary learning
The rate will be obtained by splitting Cζ0 into three regions. We consider convergence to
Bs2(0) since this set contains a global minimizer. Note that the balls in the proof are defined
with respect to w .
population).
B.3	CZ0 ∖B2∕20√≡(0)
The analysis in this region is Completely analogous to that in the first part of the proof of
Lemma 1. For every point in this set we have
1
. From Lemma 16 we know that
set ζ < 8. If we choose r
kwk∞
> ---/
20√5(n - 1)
(2+Z(⅛+n < 20√5 ⇒ w(t) ∈ B2∕20√5(0) hence in this
40λ∕5(n-1),
since for every point in this region r3ζ < 1, we have
r5∕2√ζ < ∖/ '1+ 4r ζ-1 r = z(r, Z) and we thus demand μ < 7-√ζ0,5∕2--- ≤ 吸2铲 and
2√3	3	`z	N	(40√5n-1))5∕22√3 — 2√3
obtain from Lemma 4 that for |wi | > r
U⑴*grad[fDop](q(w)) ≥
cDL
(8000(n - 1))3/2
. We now require η <
360λ∕5θn(n-1)
3Mr We Can aPPly Lemma 17 with b
20λ∕5(n-1),
40λ∕5(n-1),
M = ʌ/θn (since the maximal norm of the Riemannian gradient is √θn from
Lemma 12), obtaining that at every iteration in this region
ζ0 ≥ ζ 1 +
√ncDL
2(8000(n - 1))3/2η
and the maximal number of iterations required to obtain ζ > 8 and exit this region is given
by
t1 =
log 1 +
log(8∕Z0)
√nCDL
(17)
2(8000(n-1))3/
2η
1
1
1
r
1
B.4 B2∕20√5(0)∖B2(0)
According to Proposition 7 in (38), which we can apply since S ≥ 4√2,μ < 磊,in this region
we have
w*vwg-(W) ≥ cθ
kwk
A simple calculation shows that Vwg^ɪ(w)= (罂) grad[fDop](q(w)) where φ is the map
defined in 3, and thus
23
Under review as a conference paper at ICLR 2019
w* (∂w) grad[fDopKq(W))
kwk
grad[fDpoLp](q(w))
kwk
> θc
(18)
.Defining h(q)= 嵋口 , and denoting by q0 an update of Riemannian gradient descent with
step size η, we have (using a Lagrange remainder term)
η
h(q0) = h(q) + 丁 η+Z dtT	(η -1)
∂η	∂η η=t
0
X----------{-----------}
≡R
kwk2	popu ∂ ∂h(q)∖
=F— (grad[fDLp](q), ~∂q~+ + R
where in the last line we used q0 = cos(gη)q - sin(gη) gradfDL](q) where g ≡ ∣∣grad[fDOP](q)k.
Since〈gradfDT](q),等E =(gradfDOp](q), (I - qq*)等E and
*∖ ∂h(q)	* w }
(I Tq) F = (I - qq)[ -qn )
=(Wqn ) - (kwk2 - qn)q = 2(1 - kwk2) ( - Wk2 !
we obtain (using 18)
可=kw2 +2(1 -kwk2)η *grad[fD0p](q), ( -短))+ R
< 孚-2(1 -kwk2) kwk θcη + R
It remains to bound R. Denoting r
W、*
grad[f](q) we have
-qn
∂2h(q0)	(∂q0)* ∂2h(q) ∂q0 ∂h(q) * ∂2q0
∂η2 η=t ∖∂η )	∂q∂q ∂η η=t +	∂q ∂η2 η=t
cos2 (gt) (grad[fD°p](q)2 - grad[fD°p](q)n)
+g2 (sin2(gt) — cos(gt)) (∣∣w∣∣2 — qn)
+g sin(gt)r(1 + 2 cos(gt))
hence for some C > 0, if kgrad[fDpoLp](q)k < M we have
R< CM2η2
and thus choosing η < (1—吧/：皿忸。we find
kw0k2 < kwk2 -2(1 - kwk2) kwk cθη
24
Under review as a conference paper at ICLR 2019
and in our region of interest ∣∣w0k2 ‹ ||w『一csθη for some c > 0 and thus summing over
iterations, We obtain for some C¾ > 0
t2 = ɪ.	(19)
sθη
From Lemma 12, M = √θn and thus with a suitably chosen c? > 0, η < Cns satisfies the
above requirement on η as Well as the previous requirements, since θ < 1.
B.5 Final rate and distance to minimizer
Combining these results gives, we find that when initializing in Cζ0 , the maximal number of
iterations required for Riemannian gradient descent to enter Bs2 (0) is
T≤t1+12 <C (nlogZ0 + S)
for some suitably chosen C1, where t1, t2 are given in 17,19. The probability of such an
initialization is given by the probability of initializing in one of the 2n possible choices of Cζ ,
which is bounded in Lemma 3.
Once w ∈ Bs2 (0), the distance in Rn-1 between w and a solution to the problem (which is a
signed basis vector, given by the point w = 0 or an analog on a different symmetric section of
the sphere) is no larger than s, which in turn implies that the Riemannian distance between
ψ(w) and a solution is no larger than c3s for some c3 > 0. We note that the conditions on μ
can be satisfied by requiring μ < c45∕ζ0.
□
Lemma 12 (Dictionary learning gradient upper bound). The dictionary learning population
gradient obeys
∣Vw gpop(w)∣ ≤√2θn
∣∣grad[fD°p](q)k ≤ √θn
while in the finite sample case
kVwgDL(w)l ≤ √2n ∣∣Xk∞
kgrad[fDL](q)∣ ≤√n ∣X∣∞
where X is the data matrix with i.i.d. B G(θ) entries.
Proof. Denoting X ≡ (X,xn) we have
l∣Vwgpop(w)k2 = E tanh (q"x) (X - Xn W
2
and using Jensen’s inequality, convexity of the L2 norm and the triangle inequality to obtain
2
≤E
tanh
≤ E ∣∣xk2 +
2
while
∣grad[fDpoLp](q)∣ ≤ ∣VfDpoLp(q)∣
E
tanh
(W) x]∣ ≤√θn
25
Under review as a conference paper at ICLR 2019
Similarly, in the finite sample size case one obtains
∣VwgDL(w)∣2 ≤ 1 XJxiIl2 +
pi=1
1p
∣∣grad[fDL](q)k ≤ pɪ^
Xi W
n qn
≤ √n kx k∞
2
≤ 2n kX	k2∞
Proof of Theorem 2: (Gradient descent convergence rate for dictionary learning).
The proof will follow exactly that of Lemma 11, with the finite sample size fluctuations
decreasing the guaranteed change in ζ or ||w|| at every iteration (for the initial and final
stages respectively) which will adversely affect the bounds.
B.6 CZo ∖B2∕20√5(0)
To control the fluctuations in the gradient pro jection, we choose
_____ZθCDL______
2(8000(n - 1))3/2
which can be satisfied by choosing y(θ, Z0) = -θ(1-θ"0 for an appropriate c7 > 0 . According
n
to Lemma 10, with probability greater than Py we then have
U⑴*grad[fDL ](q(w))
-E [u(i)*grad[fDL](q(w))]
≤ y(θ, ζ)
With the same condition on μ as in Lemma 11, combined with the UniformiZed bound on
finite sample fluctuations, we have that at every point in this set
U⑴* grad[fDop](q(w))
≥ ______CDL_______
≥ 2(8000(n - I))3/2
tanh (彳)"
□
. According to Lemma 12 the Riemannian gradient norm is bounded by M = √n ∣∣X∣∣∞.
Choosing r,b as in Lemma 11, We require η < ----1 /	= b-r and obtain from
360kXk∞ 5n(n-1)	3M
Lemma 17
Z0 ≥ Z (1 + 4(8000(n -LI))3/2 η)
________log(8∕Z0)
log (1 + 4(8000nCDι))3/2 η
(20)
B.7	B1∕20√5 (O)出⑼
From Theorem 2 in (38) there are numerical constants cb , c? such that in this region
W*VwgDL(w) = w* (∂w)*grad[f](q(W))	θ
-∣w∣- =	∣w∣	≥ c?
with probability P > 1 一 Cbp-6. Following the same analysis as in Lemma 11, since
from Lemma 12 the norm of the gradient gradient is bounded by √n∣∣X∣∣∞ we require
η < (1-kWk∣Xkwkθc? which is satisfied by requiring η < 刀雷产 for some chosen C > 0. We
then obtain ∞	∞
t3=sCη
(21)
26
Under review as a conference paper at ICLR 2019
for a suitably chosen C2 > 0.
B.8	Final rate and distance to minimizer
The final bound on the rate is obtained by summing over the terms for the three regions as
in the population case, and convergence is again to a distance of less than c3 s from a local
minimizer. The probability of achieving this rate is obtained by taking a union bound over
the probability of initialization in Cζ0 (given in Lemma 3) and the probabilities of the bounds
on the gradient fluctuations holding (from Lemma 10 and (38)). Note that the fluctuation
bound events imply by construction the event E∞ = {1 ≤ ∣∣X∣∣∞ ≤ 4,log(np)} hence We
can replace ∣∣X∣∣∞ in the conditions on η above by 4√log(np). The conditions on η, μ can
be satisfied by requiring η < 二窿篙,μ <。；65/4 for suitably chosen c5 ,c6 > 0. The bound on
the number of iterations can be simplified to the form in the theorem statement as in the
population case.	口
C Generalized Phase Retrieval
We shoW beloW that negative curvature normal to stable manifolds of saddle points in strict
saddle functions is a feature that is found not only in dictionary learning, and can be used
to obtain efficient convergence rates for other nonconvex problems as Well, by presenting an
analysis of generalized phase retrieval that is along similar lines to the dictionary learning
analysis. We stress that this contribution is not novel since a more thorough analysis Was
carried out by (8). The resulting rates are also suboptimal, and pertain only to the population
objective.
Generalized phase retrieval is the problem of recovering a vector x ∈ Cn given a set of
magnitudes of projections yk = ∣x*ak | onto a known set of vectors ak ∈ Cn. It arises in
numerous domains including microscopy (27), acoustics (2), and quantum mechanics (10)
(see (33) for a review). Clearly x can only be recovered up to a global phase. We consider the
setting where the elements of every ak are i.i.d. complex Gaussian, (meaning (ak)j = u + iv
for u,v 〜N(0,1/√2)). We analyze the least squares formulation of the problem ⑺ given
by
1p	2
mCnf(Z) =而X (y2 Tz*ak|).
k=1
Taking the expectation (large p limit) of the above objective and organizing its derivatives
using Wirtinger calculus (22), we obtain
E[f ] = kxk4 + kzk4-kx『kz『-|x*z|2	(22)
VE[f]
Vz E[f]
VzE[f]
=一 ((2 kz『-||x『)I-xx*) J
((2 ∣∣zk2 - kχk2)I - XxT)z
For the remainder of this section, we analyze this ob jective, leaving the consideration of
finite sample size effects to future work.
C.1 The geometry of the objective
In (37) it was shown that aside from the manifold of minima
A ≡ xeiθ,
the only critical points of E[f] are a maximum at z = 0 and a manifold of saddle points
given by
A ∖{0} ≡ ʃz Z ∈ W,∣∣z∣∣ = ⅛ I
27
Under review as a conference paper at ICLR 2019
where W ≡ {z∣z*x = 0}. We decompose Z as
Z = W + ζeiφ 亩,	(23)
where ζ > 0, w ∈ W. This gives kzk2 = kwk2 + ζ2 . The choice of w, ζ, φ is unique up to
factors of 2π in φ, as can be seen by taking an inner product with x. Since the gradient
decomposes as follows:
VzE[f] =(2 问2 I -∣∣x∣∣2 I - xx] (w + Zeiφ春)
=(2 kz『- kx『)w + 2Zeiφ (||z『 -||x『)高	(24)
the directions eiφ ^^, ^^ are unaffected by gradient descent and thus the problem reduces
to a two-dimensional one in the space (ζ, kwk). Note also that the objective for this two-
dimensional problem is a Morse function, despite the fact that in the original space there
was a manifold of saddle points. It is also clear from this decomposition of the gradient that
the stable manifolds of the saddles are precisely the set W .
It is evident from 24 that the dispersive property does not hold globally in this case. For
Z ∈/ B||x|| we see that gradient descent will cause ζ to decrease, implying positive curvature
normal to the stable manifolds of the saddles. This is a consequence of the global geometry
of the objective. Despite this, in the region of the space that is more "interesting", namely
B||x|| , we do observe the dispersive property, and can use it to obtain a convergence rate for
gradient descent.
We define a set that contains the regions that feeds into small gradient regions around saddle
points within B||x|| by
QZo ≡ {z(Z, kWk)IZ ≤ ζ0}.
We will show that, as in the case of orthogonal dictionary learning, we can both bound
the probability of initializing in (a subset of) the complement of Qζ° and obtain a rate for
convergence of gradient descent in the case of such an initialization. 9
We now define four regions of the space which will be used in the analysis of gradient descent:
Si ≡ {z 同2 ≤ 2 32}
S2 ≡ {z 1 kx『< kzk2 ≤ (1-c)3∣2}
S3 ≡ {Z (1-c)kxk2 < kZk2 ≤ kxk2o
S4 ≡ {Z kxk2 < kZk2 ≤(1+c)kxk2o
defined for some c < 4. These are shown in Figure 4.
We now define
Z0 ≡ Z — ηVzE[f] ≡ W0 + ζ0eiφk∣k	(25)
and using 24 obtain
Z0 = 1 - 2η(kZk2 -kxk2)Z	(26a)
kW0k=1-η2kZk2 - kxk2	kWk.	(26b)
9Qζo is equivalent to the complement of the set CZ used in the analysis of the separable objective
and dictionary learning.
28
Under review as a conference paper at ICLR 2019
Figure 4: The projection of the objective of generalized phase retrieval on the (^^, kχk)
plane. The full red curves are the boundaries between the sets Si,S2,S3,S4 used in the
analysis. The dashed red line is the boundary of the set Qq。that contains small gradient
regions around critical points that are not minima. The maximizer and saddle point are
shown in dark green, while the minimizer is in pink.
These are used to find the change in ζ , kwk at every iteration in each region:
On Si:	Z0 ≥ (1 +η ∣x∣2)Z	(27a)
	∣w0 ∣ ≥ ∣w∣	(27b)
On S2 :	Z0≥ (1 + 2cη ∣x∣2)Z	(27c)
	∣w0 ∣ ≤ ∣w∣	(27d)
On S3 :	1-η∣x∣2 ∣w∣ ≤ ∣w0∣	
	≤ 1 - (1 - 2c)η ∣x∣2 ∣w∣	(27e)
	Z≤Z0≤ (1 + 2cη ∣x∣2)Z	(27f)
On S4 :	1-(1+2c)η∣x∣2∣w∣ ≤ ∣w0∣	
	≤ 1-η∣x∣2 ∣w∣	(27g)
	(1-2cη∣x∣2)Z≤Z0≤Z	(27h)
C.2 Behavior of gradient descent in ∪i4=1Si
We now show that gradient descent initialized in Si ∖Qq0 cannot exit ∪4= Si or enter QZQ.
Lemma 14 guarantees that gradient descent initialized in ∪i4=1Si remains in this set. From
equation 27 we see that a gradient descent step can only decrease ζ if z ∈ S4 . Under the
mild assumption Z2 < ɪ6 ∣∣xk2 We are guaranteed from Lemma 13 that at every iteration
Z ≥ Zo∙ Thus the region with Z < Z0 can only be entered if gradient descent is initialized in
it. It follows that initialization in S1∖Qζ0 rules out entering Qq。at any future iteration of
gradient descent. Since this guarantees that regions that feed into small gradient regions are
avoided, an efficient convergence rate can again be obtained.
C.3 Convergence rate
Theorem 3 (Gradient descent convergence rate for generalized phase retrieval). Gradient
descent on 22 with step size η <	C2, c < 1, initialized uniformly in Si converges to a point
4kxk	4
29
Under review as a conference paper at ICLR 2019
Z SuCh that dist(z,A) < √5c ∣∣xk in
T < log(k√2), + log(2),
J log(1+ηkxk2)十 2log(1+2cηkxk2)
+__________log(2C)log( √7)_______
十 log(l-(1-2c)ηkxk2) log(1 + 2cηkxk2)
iterations with probability
P ≥ 1—r8f √2nζ!,
Proof. Please see Appendix C.3.
□
We find that in order to prevent the failure probability from approaching 1 in a high
dimensional setting, if we assume that ∣x∣ does not depend on n we require that ζ scale
like √n. This is simply the consequence of the well-known concentration of volume of a
hypersphere around the equator. Even with this dependence the convergence rate itself
depends only logarithmically on dimension, and this again is a consequence of the logarithmic
dependence of ζ due to the curvature properties of the objective.
Lemma 13. For any iterate Z of gradient descent on 22, assuming η < 4高2, c < 1 and
defining ζ0 as in 25, we have i)
Z ∈ [Si ⇒kw∣2 dʧ
i=1	2
ii)
7
Z ∈ S4 ⇒ Z02 ≥ 16 kxk2
Proof of Lemma 13. i) From 27 we see that in S Si the quantity ∣w∣2 cannot increase,
i=2
hence this can only happen in S1. We show that for some Z ∈ S1, a point with ∣w∣ =
(1 一 ε)k√k, ε < 1 cannot reach a point with ||w『=k√k by a gradient descent step. This
would mean
(1- η (2 kwk2 + 2Z 2 - kx『))kwk
=(1 一η((1一02 kχk2+2ζ 2-kxk2)) (1—ε) ∙√2
一回
√2
and since ζ2 ≥ 0 this implies
(1 + εη kxk2 (2-ε))(1-ε) ≥ 1
by considering the product of these two factors, this in turn implies
2b(2 — ε) ≥ η kx∣2 (2 - ε) ≥ 1
where We have used η <:看产,c < 4. Thus if We choose b = 4 this inequality cannot be
satisfied.
Additionally, if We initialize in Si ∩Qζ° then We cannot initialize at a point where ||w『
and hence the inequality is strict.
kχk
30
Under review as a conference paper at ICLR 2019
ii) Since only a step from S4 can decrease ζ, we have that for the initial point kzk2 > kxk2 .
Combined with ||w『≤ kx2k- this gives
Z2 ≥ kxk2
Z -	2
and using the lower bound (1 - 2η kxk2 c)ζ ≤ ζ0 we obtain
Z02 ≥ 甲(1 - 2η kxk2 c)2 ≥ 甲(1 - 4η kxk2 c)
≥ (1 - ɪ)⅛
-(	2b) 2
where in the last inequality We used c < 4, η < 巳高-.Choosing b = 4 gives
ζ02 ≥ 176 kxk2
If We require Z2 < | ||x『 this also ensures that the next iterate cannot lie in the small
gradient regions around the stable manifolds of the saddles.	口
Lemma 14. Defining z0 as in 25, under the conditions of Lemma 13 and we have
i)
44
z ∈ [ Si ⇒ z0 ∈ [ Si
i=2	i=2
ii)
z ∈ S1 ⇒ z0 ∈ S1 ∪S2
Proof of Lemma 14. We use the fact that for the next iterate we have
kz0k2 = 1 - η(2 kzk2 - kxk2)2 kwk2
+ 1 - 2η(kzk2 - kxk2)2 Z2
(28)
We will also repeatedly use η < rXc-, c < 14 and Z ∈ U Si ⇒∣∣w∕≤ ⅜2L which is a shown
bkxk	i=1
in Lemma 13.
4
C.4 z ∈ S3 ⇒ z0 ∈ USi
i=2
We want to show kx2^ < ∣∣z0k2 ≤ (1 + C) ∣∣xk2.
2	(1)	(2)
1)	We have z ∈ S3 ⇒ kzk2 = (1 - ε) kxk2 for some ε ≤ c and using 28 we must show
|x『< (1- η kxk2 (1 - 2ε)) Ilwk2
2	+(1 + 2η kxk2 εj ζ2
or equivalently
A ≡ ε-K
2
31
Under review as a conference paper at ICLR 2019
≤ η kxk2
-2(1-2ε)+(1-2ε)2ηkxk2 kwk2
+4 ε + ε2η kxk2 ζ2
≡B
and using η < bkXk2,c < 4
while on the other hand
-kxk2
b
-2kxk √C
< -2η kxk4 ≤ B
A≤c-
kxk2 <
2	<
kxk2
4
<
b
—
thus picking b = 4 guarantees the desired result.
2)	By a similar argument, kz0k2 ≤ (1 + c) kxk2 is equivalent to
A≡ηkxk2
-2(1-2ε)+ηkxk2(1-2ε)2 kwk2
+4 ε+ηkxk2ε2 ζ2
≤kxk2(c+ε)≡B
.Since ∣∣wk2 ≤ kχ2k- and ∣∣zk2 ≤ ∣∣xk2 ⇒ Z2 ≤ kx2k- We obtain
A≤η η kxk4
+ 4 (kχk2 ε + η kxk4 F)] kχk-
1
< 2b
1 + 2(1 + 81b)]c kxk2
. If We choose b = 4 We thus have A < B Which implies
∣z0∣2 < (1+c)∣x∣2
4
C.5 z ∈ S4 ⇒ z0 ∈ SSi
i=2
We have z ∈ S4 ⇒ ∣z ∣2 = ∣w∣2 + ζ2 = (1 + ε) ∣x∣2 forsome ε ≤ c .
1) kx2^ < kz0k2 is equivalent to
≤ η ∣x∣2
A ≡-(ε +2)kxk2
-4(1+2ε)+η∣x∣2(1+2ε)2)∣w∣2
+4 -ε+η∣x∣2ε2) ζ2
B ≥ -4η kxk2 ((1 + 2ε) kwk2 + εZ2) ≥ -8∣ kxk2
where the last inequality used kwk2 ≤ kxL and kzk2 ≤ kxk2 (1 + C) ⇒ Z2 ≤ kxk2 (2 + c).
The choice ∣ = 4 gaurantees A ≤ B Which ensures the desired result.
2) This is trivial since ∣z∣2 ≤ (1 + C) ∣x∣2 and in S4 both Z and ∣w∣decay at every iteration
(ref eq).
32
Under review as a conference paper at ICLR 2019
4
C.6 z ∈ S2 ⇒ z0 ∈ SSi
i=2
1) We use Z ∈ S2 ⇒ ||z『=Ilw『+ Z2 = ( 1 + ε) ||x『 for some ε ≤ 2 - C . Using a similar
argument as in the previous section, we are required to show
-ε IxI2 <ηIxI2
4 -ε + ε2η IxI2 IwI2
+2(1-2ε)+(1-2ε)2ηIxI2ζ2
≡B
where B ≥ —εkχbk- implies that b = 4 gives the desired result.
2) The condition is equivalent to
A≡ηIxI2
4 -ε + ε2η IxI2 IwI2
+2(1-2ε)+(1-2ε)2ηIxI2ζ2
≤(2 + c)kx『三 B
+εIxI2
One can show by looking for critical points of A(ε) in the range 0 ≤ ε ≤ 1 that A is
maximized at ε = 0, since there is only one critical point at ε*
while
4-√+2 √
8 √
and A(ε*) < 0,
A(2) ≤ ](-2 亨+b2)kwk[+2 kxk2
A(0) ≤ 2b (2+2b)号
and in both cases b = 4 ensures A ≤ B.
C.7 z ∈ S1⇒ z0 ∈ S1∪S2
We must show ∣∣z0k ≤ (1 - C) ∣∣x∣2 using ∣∣z∣2 = (1 - ε)kχ2L for 0 ≤ ε ≤ 1.
2	22
kz0k2 =(1 + εη kxk2) kwk2 + 1 + 2(ε +1)η哈
A≡η∣x∣2
2ε + ε2η ∣x∣2 ∣w∣2
(ε +1) +(ε +1)2η kx4k
≤(1 -c)kx『三 B
-ε∣x∣2
and since A ≤ 2b [2 + b] kx2L and B ≥ kχL once again b = 4 suffices to obtain the desired
result.	□
Lemma 15. For z parametrized as in 23,
Ilwk2 < c Ilxk2 ∨ Z2 > (I-C) Ixk2
⇒ dist(z, A) < √5c kxk
33
Under review as a conference paper at ICLR 2019
Proof of Lemma 15. Once kwk2 < c kxk2 for some z ∈ S3 ∪ S4 we have
kzk2=ζ2+kwk2≥(1-c)kxk2
ζ2≥(1-c)kxk2-kwk2>(1-2c)kxk2	(29)
For some Z = W + Zeiφ ^^ We have
dist2(z, A) = min ∣ eiθX — W — ζeiφ ^x^ ∣∣
=kwk2+min ∣eiθ x - ζeiφ 尚 ∣∣
=kwk2 + (1 - ɪ)2kxk2 = kzk2 + kxk2 - 2Z kxk
kxk
if We assume kzk2 ≤ (1 + c) kxk2
dist2 (z,A) ≤ (C + 2) kxk2 - 2Z kxk	(30)
plugging in the value of Z from 29 and using fact that 一 √1 — x ≤ — 1 + X for x < 1 we have
dist2(z, A) < (c + 2) kxk2 — 2√1 — 2c kxk2 ≤ 5c kx『
Alternatively, if ζ2 > (1 - c) kxk2 we have from 30
dist2(z,A) ≤ (c + 2) kxk2 — 2Z kxk
< (c + 2) kxk2 — 2√T≡τ kxk2 ≤ 3c kxk2
12
which gives the desired result. In particular, if We choose C =泰 We converge to dist (z, A) <
kxL, a region which is strongly convex according to (38).	口
Proof of Theorem 3: (Gradient descent conver-
gence rate for generalized phase retrieval) .
We now bound the number of iter-
ations that gradient descent, after random initialization in S1, requires to reach a point
where one of the convergence criteria detailed in Lemma 15 is fulfilled. From Lemma 14, we
4
know that after initialization in S1 we need to consider only the set Si . The number of
iterations in each set will be determined by the bounds on the change in ζ, ||W|| detailed in
27.
C.7.1 Iterations in S1
Assuming we initialize with some ζ = ζ0 . Then the maximal number of iterations in this
region is
Zo(i + η kxk2)t1 = √k
t1
iog(i + η kxk2)
since after this many iterations ||z『≥ Z2 ≥ kx2k-.
34
Under review as a conference paper at ICLR 2019
4
C.7.2 Iterations in S Si
i=2
The convergence criteria are kwk2 < c kxk2 or ζ2 > (1 - c) kxk2.
After exiting S1 and assuming the next iteration is in S2 , the maximal number of iterations
required to reach S3 ∪ S4 is obtained using
ζ0 ≥ (1+2ηkxk2c)ζ
and is given by
k√χk (i + 2η kχk2c)t2 = (i-c)kχk2
2
—log (P2(I - C)) <	log⑵
2 log(1 + 2η kxk2c) — 2log(1 + 2η kx『c)
since after this many iterations kzk2 ≥ ζ2 ≥ (1 - c) kxk2.
For every iteration in S3 ∪ S4 we are guaranteed
kw0k ≤ 1 - (1 - 2c)η kxk2 kwk
thus using Lemmas 13.i and 15 the number of iterations in S3 ∪ S4 required for convergence
is given by
ɪ(1-(1-2c)ηkx『)t3+4 = Ckx『
,	__	log(2c)
t3+4 ——	7	K-
log 1 - (1 - 2C)η kxk
The only concern is that after an iteration in S3 ∪ S4 the next iteration might be in S2 .
To account for this situation, we find the maximal number of iterations required to reach
S3 ∪ S4 again. This is obtained from the bound on ζ in Lemma 13.
Using this result, and the fact that for every iteration in S2 we are guaranteed ζ0 ≥
(1 + 2η kxk2 C)ζ the number of iterations required to reach S3 ∪ S4 again is given by
彳 kxk (1 + 2η Ilxk2 c)tr = √1 -c kxk
+ log (4√√-c)	/ log(√)
tr —	≤	*≤	Q
log(1 +2ηkxk C) log(1 +2ηkxk C)
C.8 Final rate
The final rate to convergence is
T < t1 + t2 + t3+4tr
=lθg( k√2)	+ log(2)
log(1+ηkxk2)十 2log(1+2cηkxk2)
+___________log(2C)log( √7)_________
log(l-(1-2c)ηkxk2) log(1 + 2cηkxk2)
C.9 Probability of the bound holding
The bound applies to an initialization with Z ≥ Zo, hence in Sι∖Qζ0. Assuming uniform
initialization in Si, the set Qq。is simply a band of width 2Zo around the equator of the
35
Under review as a conference paper at ICLR 2019
ball Bkxk/√2 (in R2n, using the natural identification of Cn with R2n). This volume can be
calculated by integrating over 2n - 1 dimensional balls of varying radius.
Denoting r = ζkx2 and by V(n) = nT：：) the hypersphere volume, the probability of
initializing in Si ∩ Q^o (and thus in a region that feeds into small gradient regions around
saddle points) is
P(fail)
VOl(QZo)
Vol(Bkxk/√2)
r
V(2n - 1) R (1 - χ2)^-dx
-r
V (2n)
V(2n - 1) Rr e-
≤ ----------r—
_	V (2n)
1 n Γ(n)
2n-1 χ2 dx
qTJ nɪɪ γ(≡)
erf(v 2n2 1r)
≤ y —erf(√nr)
. For small ζ we again find that P(fail) scales linearly with ζ, as was the case for the previous
problems considered.
D Auxiliary Lemmas
D.1 Separable objective
∂gs(w)	= tanh	3	- tanh	'qn)	W
∂Wi		` μ )		,μ √	qn
∂ 2gs(w)=	Jsech2	(W )	- tanh	(qn、	)ɪ
∂wi ∂wj	[μ	∖ μ J		卜μ J	qn
δij
+
Lech2
μ
D.2 Dictionary Learning
Vw gPOP(w) = E
D.3 Properties of Cζ
tanh(£(wX”x -心w∖]
μ ) qn	qn(W) )∖
Proof of Lemma 3: (Volume of Cζ ). We are interested in the relative volume
Vool(C-1)≡ Vζ. Using the standard solid angle formula, it is given by
Vζ
∞
lim-e e
ε→0 εn/2
0
χι∕(1+ζ)
εx2 ∏ Z	e-∏x2
i=2
-x1 /(1+ζ )
dxidx1
∞
1∙	1	- - ∏ χ2
lim e e X
ε→0 ∖fε J
0
erf(
(1+ζ) ε
n-1
dx
□
x
π
36
Under review as a conference paper at ICLR 2019
changing variables to X
π x
ε (I+Z)
Vζ
∞
(1+≤) ee-(1+ζ)2x2erfn-1(x)dx
π
0
This integral admits no closed form solution but one can construct a linear approximation
around small ζ and show that it is convex. Thus the approximation provides a lower bound
for Vζ and an upper bound on the failure probability.
From symmetry considerations the zero-order term is V0 = *. The first-order term is given
by
∂Vζ
冠ζ=0
∞
----1= X x2 e-χ2 erfn-1(x)dx
nπ
0
We now require an upper bound for the second integral since we are interested in a lower
bound for Vζ . We can express it in terms of the second moment of the L∞ norm of a
Gaussian vector as follows:
x2
∞	∞x
/
0
=4n (Var[kXk∞] + (E[kXk∞D2)
where μ(X) is the Gaussian measure on the vector X ∈ Rn. We can bound the first term
using
Var[kXk∞] ≤ maxVar[|Xi|] = Var [|Xi|] < Var [Xi] =1
i
To bound the second term, we use the fact that for a standard Gaussian vector X (Xi 〜
N(0, 1)) and any λ > 0 we have
exp (λE [kXk∞]) ≤ E exp λmax |Xi|
≤ E Xexp(λ |Xi|) = nE[exp(λ 区|)]
i
(using convexity and non-negativity of the exponent respectively)
∞
nE [exp (λ |Xi |)]
2n
0
exp(λXi) dμ(Xi)
≤ 2nE [exp (λXi)]
2n exp
37
Under review as a conference paper at ICLR 2019
taking the log of both sides gives
E hmaχXi∣i≤ logλ2n) + 2
and the bound is minimized for λ =，2 log(2n) giving
E [max |X/] ≤ p2 log(2n) ~ P2 log(n)
Combining these bounds, the leading order behavior of the gradient is
∂VZ	〉3 — 4log(2n)〉 log(n)
∂κ ζ=o ≥	4n	≥ ―-『
This linear approximation is indeed a lower bound, since using integration by parts twice we
have
∞
⅞Z2ζ = √∏ ZeT(Wl +4(1；Z))W4) erfn-1(x)dx
0
∞
—2(n— Ze-(1+ζ)2χ2 (1 — 2(1 + ζ)2x2) e-x2erfn-2(x)dx
0
∞
=4(n - 1)(na-2'1 + Z ∕e-((1+Z)2+2)x2erfn-3(x)dx > 0
π3/2	J
0
where the last inequality holds for any n > 2 since the integrand is non-negative everywhere.
This gives
VZ ≥ E - IogH Z
□
Lemma 16. B∞Z)(O) ⊆CZ ⊆ Bjyn-Is(Z)(0) whereS(Z) = √(2+Z)ζ+n. B∞ζ)(0) is the largest
L∞ ball contained in Cζ, and Bjn-is© (0) is the smallest L2 ball containing CZ (where these
balls are defined in terms of the w vector). All three intersect only at the points where all
the coordinates of W have equal magnitude. Additionally, CZ ⊆ B∞√2+ζ(0) and this is the
smallest L∞ ball containing CZ .
Proof. Given the surface of some L∞ ball for w , we can ask what is the minimal Z such
that ∂CZm intersects this surface. This amounts to finding the minimal qn given some kwk∞ .
Yet this is clearly obtained by setting all the coordinates of w to be equal to kwk∞ (this is
possible since We are guaranteed qn ≥ ∣∣w∣∣∞ ⇒ ∣∣w∣∣∞ ≤ √n), giving
1-- (n -1) kwk∞
kwk∞
= 1 + Zm
kwk∞
1
p(1 + Zm)I2 + n - 1
38
Under review as a conference paper at ICLR 2019
thus, given some Z, the maximal L∞ ball that is contained in CZ has radius √======. The
minimal L∞ norm containing CZ can be shown by a similar argument to be B∞√————— (0),
where one instead maximizes qn with some fixed kwk∞ .
Given some surface of an L2 ball, we can ask what is the minimal CZ such that CZ ⊆ Br2(0).
This is equivalent to finding the maximal ζM such that ∂CZM intersects the surface of the
L2 ball. Since qn is fixed, maximizing ζ is equivalent to minimizing kwk∞ . This is done by
setting ∣∣wk∞ = ^kn=, which gives
,1-kw『
Ilwk
1 + ζM
1∕∕2	? -— = kwk
(2 + ζM )ζM + n
The statement in the lemma follows from combining these results.	□
Lemma 17 (Geometric Increase in Z). For w ∈。《0∖B∞ (where Z ≡ ^^n 1), assume
|wi| > r ⇒ u(i)*grad[f](q(w)) ≥ c(w)Z where u(i) is defined in 7 and 1 > b > r. Then if
kgrad[f](q(w))k < M and we define
q0 ≡ expq (-ηgrad[f](q))
for η < 3Mr, defining Z0 in an analogous way to Z we have
Z0 ≥ Z (1 + √2nηc(W))
Proof: D.3
Proof of Lemma 17:(Geometric Increase in Z). Denoting g ≡ kgrad[f](q)k, we have
q' = cos(gη)q — sin(gη)gradf ](q)
hence, using Lagrange remainder terms,
qn
Wi
gη
qn- ηgrad[f](q)n -	cos(t)(gη - t)dtqn
0
gη
+ R sin(t)(gη - t)dtgradf](Gn
0g
gη
wi - ηgrad[f](q)i - cos(t)(gη - t)dtwi
0
gη
+ R sin(t)(gη - t)dtgradf](Gi
0g
. We assume wi > 0, and the converse case is analogous. From convexity of
1
1+x
gη
sin(t)(gη-t)dt
'	qn + η-------0-------------
qn	wi wi	wig
'一
wi
* (grad[f](q)i - Wgrad[f](q)nJ
W+等"d[f](q)i- Wi grad[f](q)n
39
Under review as a conference paper at ICLR 2019
=qn + sin(gη) u(i)*grad[f ](q(w))
wi	wig
We now use η < bMr < 翁 ⇒ gη < ∏ ⇒ sin(gη) ≥ gη and consider two cases. If ∣w∕ > r
we use the bound on the gradient projection in the lemma statement to obtain
qn
Wi
≥ qn + ɪe(w)z ≥ qn + *ηc(w)ζ
wi	2wi	wi 2
hence
qn
Wi
-1≥
qn
kwk∞
-1 + √√nηc(WX = C (1 + √nηc(W))
(31)
If |Wi| < r we rule out the possibility that |Wi0 | = kW0k∞ by demanding η < b3Mr. Since
b(b — r) < 1 We have 1 + 3b(b — r) <，1 + b(b — r) hence the requirement on η implies
η<
，1 + b(b — r) — 1	—2g +，4g2 + 4g2b(b — r)
gb	2g2b
. If we now combine this with the fact that after a Riemannian gradient step cos(gη)qi —
sin(gη) ≤ qi ≤ cos(gη)qi +sin(gη), the above condition on η implies the inequality (*), which
in turn ensures that |Wi| < r ⇒ |Wi0 | < kW0k∞:
|Wi0| < |Wi| +sin(gη) < r +gη < (1 — g2η2)b — gη
(*)
< cos(gη) kWk∞ — sin(gη) ≤ kW0k∞
Due to the above analysis, it is evident that any Wi0 such that |Wi0 | = kW0 k∞ obeys |Wi | > r,
from which it follows that we can use 31 to obtain
qn
kw0k∞
—1 = ζZ ≥ ζ (ι + 字ηc(w))
□
40