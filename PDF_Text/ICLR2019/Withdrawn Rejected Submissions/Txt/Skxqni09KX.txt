Under review as a conference paper at ICLR 2019
Online Bellman Residue Minimization via Sad-
dle Point Optimization
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of Bellman residual minimization with nonlinear function
approximation in general. Based on a nonconvex saddle point formulation of Bell-
man residual minimization via Fenchel duality, we propose an online first-order
algorithm with two-timescale learning rates. Using tools from stochastic approx-
imation, we establish the convergence of our problem by approximating the dy-
namics of the iterates using two ordinary differential equations. Moreover, as a
byproduct, we establish a finite-time convergence result under the assumption that
the dual problem can be solved up to some error. Finally, numerical experiments
are provided to back up our theory.
1	Introduction
Reinforcement learning (RL) (Sutton & Barto, 1998) studies the problem of sequential decision
making under uncertainty. In these problems, an agent aims to make optimal decisions by interacting
with the environment, which is modeled as a Markov Decision Process (MDP). Thanks to the recent
advancement of deep learning, reinforcement learning has demonstrated extraordinary empirical
success in solving complicated decision making problems, such as the game of Go (Silver et al.,
2016; 2017), navigation (Banino et al., 2018), and dialogue systems (Li et al., 2016).
However, when nonlinear function approximation such as neural networks are utilized, theoretical
analysis of RL algorithms becomes intractable as it involves solving a highly nonconvex statistical
optimization problems. Whereas in the tabular case or in the case with linear function approxi-
mation, using tools for convex optimization and linear regression, the statistical and computational
properties of the reinforcement learning algorithms are well-understood under these settings. In
consequence, although RL algorithms with nonlinear function approximation great empirical suc-
cess, their theoretical understanding lags behind, which makes it difficult to design RL methods in a
principled fashion.
Moreover, from a statistical perspective, with nonlinear function approximation, RL methods such as
fitted value iteration (Munos & Szepesvari, 2008), fitted Q-iteration (Antos et al., 2008a), and Bell-
man residual minimization Antos et al. (2008b) can be cast as nonlinear regression problems. Using
nonparametric regression tools, statistical properties of batch RL methods with nonlinear function
approximation are established (Farahmand et al., 2016). However, when it comes to computational
properties, due to the fundamental hardness of nonconvex optimization, theoretical understanding
of the convergence of RL methods remains less explored, which is contrast to the case with linear
function approximation, where the convergence of online algorithms based on temporal-difference
(TD) learning are well studied.
In this work, we we make the first attempt to study an online algorithm for Bellman residual mini-
mization, with nonlinear function approximation. In the batch form, Bellman residual minimization
is formulated as a bilevel optimization, which cannot be solved with computational efficiency. To
tackle this problem, we formulate the Bellman residual itself as the optimal value of another max-
imization problem. In this way, Bellman residual minimization becomes a saddle point problem,
1
Under review as a conference paper at ICLR 2019
where the value function is the primal variable, and the dual variable tracks the TD-error of the
primal variable. By also parametrizing the dual variable using a parametrized function class, we
propose a primal-dual subgradient method which is an online first-order method for Bellman residue
minimization.
Furthermore, since the saddle-point problem is not convex-concave, the order between the inner
maximization and outer minimization problems plays a significant role. Similar to the batch al-
gorithm, ideally we would fix the primal variable and solve the inner maximization problem to its
global optima, and then updated the primal variable. However, in the online setting, this approach
is not tractable. To achieve computational efficiency, we apply the two-timescale updates to the
primal and dual variables. Specifically, we update the primal and dual variables using two sets of
learning rates, where the learning rate of the dual variable is much larger than that of the primal vari-
able. Using stochastic approximation (Borkar, 2008; Kushner & Yin, 2003), two-timescale updating
rules ensures that we could safely fix the primal variable when studying the convergence of the dual
variable. In this case, the dual variable converges to a local maximum of the inner maximization
problem. Moreover, the dynamics of the iterates are characterized by two ordinary differential equa-
tions (ODE) running at different timescales.
Our contributions are three-fold. First, we formulate the problem of Bellman residual minimization
as a nonconvex saddle point problem, for which we propose an online first-order algorithm using
two-timescale learning rates. Second, using stochastic approximation, we show that the online al-
gorithm converges almost surely to the asymptotically equilibria of an ODE. Third, assuming the
existence ofan optimization oracle which solves the dual problem up to some error, we show that the
stochastic gradient method in the primal step converges to a stationary point of the squared Bellman
residual up to some fundamental error.
Related Work. The statistical properties of Bellman residual minimization is studied in Antos et al.
(2008b); Maillard et al. (2010); Farahmand et al. (2016) for policy evaluation, where the problem
is solved using least-squares regression under the batch setting. Moreover, in these work, Bellman
residue minimization is an intermediate step of least-squares policy iteration Lagoudakis & Parr
(2003). These work are not comparable to our work since our study an online algorithm for Bellman
residue minimization and its convergence.
In addition, our work is related to the line of research on the online algorithms for policy evaluation
with function approximation. Most existing work focus on linear function approximation. Specifi-
cally, Tsitsiklis & Van Roy (1997) study the convergence of the on-policy TD(λ) algorithm based
on temporal-difference (TD) error. To handle off-policy sampling, Maei et al. (2010); Sutton et al.
(2009; 2016); Yu (2015); Hallak & Mannor (2017) propose various TD-learning methods with con-
vergence guarantees. Utilizing two-timescale stochastic approximation in Borkar (2008) and strong
convexity, they establish global convergence results for the proposed methods. The finite-sample
analysis of these methods are recently established in Dalal et al. (2017b;a). More related works
are Liu et al. (2015); Du et al. (2017), which formulate minization of the mean-squared projected
Bellman error as a saddle point problem using Fenchel duality. However, since they consider lin-
ear function approximation, the corresponding saddle point problem is convex-concave, whereas
our objective is nonconvex. When it comes to nonlinear function approximation, to the best of our
knowledge, the only convergent algorithm is the nonlinear-GTD algorithm proposed in Bhatnagar
et al. (2009). Their algorithm depends on the Hessian of the value function, and thus might be costly
in practice.
Moreover, it is worth noting that Dai et al. (2017b) apply the same saddle point formulation to soft Q-
learning. However, they consider a batch algorithm with the assumption that the inner maximization
can be solved to the global optima. Due to nonconvexity, this assumption could stringent.
Furthermore, Chen & Wang (2016); Wang (2017) propose primal-duality of reinforcement learning
based on the Lagrangian duality of linear programming for MDP (Puterman, 2014). These work
establish convergence results in the tabular case. Applying neural networks to the same duality
2
Under review as a conference paper at ICLR 2019
formulation, Dai et al. (2017a); Cho & Wang (2017) propose variants of the actor-critic algorithms
(Konda & Tsitsiklis, 2000), which does not have convergence guarantees.
Notation. We use the following notations throughout this paper. For any vector x ∈ Rn, we use
k x k 2 and k x k ∞ to denote the Euclidean norm and the '∞ -norm of x, respectively. For a finite set M,
we use |M| to denote its cardinality. We denote by P(M) the set of all probability measures on M
and we write B(M) for the set of all bounded functions defined on M. For a function f ∈ B(M),
We define the '∞-norm of f as ∣∣f k∞ = supχ∈M |f (x)|. Moreover, for any probability measure
μ ∈ P(M), we write ∣∣f ∣μ for the '2-norm with respect to μ, i.e., ∣f ∣μ = [JM |f (x)∣2dμ(x)]1/2.
Finally, we use [n] to denote the set of integers {1, ∙∙∙ ,n}.
2	Value Function Estimation in RL
In this section, we introduce some background on reinforcement learning that will be used in the
presentation our main results.
In reinforcement learning, the environment is often modeled as a Markov decision process (MDP)
denoted by a tuple (S, A, P, r, γ), where S is the set of states, A is the set of all possible actions,
P : S × A → P(S) is the Markov transition kernel, r : S × A → R is the reward function, and
γ ∈ (0, 1) is the discount factor. More specifically, an agent interacts with the MDP sequentially in
the following way. At the t-th step for any t ≥ 0, suppose the MDP is at state st ∈ S and the agent
selects an action at ∈ A; then, the agent observes reward r(st, at) and the MDP evolves to the next
state st+1 〜P (∙ | st, at). Here P (∙ | s, a) is the probability distribution of the next state when taking
action a at state s. The discounted cumulative reward is defined as R = Pt≥0 Yt ∙ r(st, at).
In addition, a policy ∏: S → P(A) specifies a rule of taking actions. Specifically, ∏(a∣s) is the
probability of selecting action a at state s under policy π . We note that π induces a Markov chain
on SXA with transition probability p(s0, a01 s,a) = π(a01 s0) ∙ P(s01 s, a). We define the (action)
value function of policy π as Qπ(s, a) = E(R | s0 = s, a0 = a, π), which is the expected value
of the discounted cumulative reward when the agent takes action a at state s, and follows policy π
afterwards. Moreover, we define the optimal value function Q*: S ×A→ R by letting Q*(s, a)=
supπ Qπ(s, a) for all (s, a) ∈ S × A, where the supremum is taken over all possible policies. By
definition, Q* (s, a) is the largest reward obtainable by the agent when starting from (s, a). It is
well-known that Qπ and Q* are the unique fixed points of the Bellman evaluation operator Tπ and
the Bellman optimality operator T *, respectively. Specifically, Bellman operators Tπ : B(S × A) →
B(S × A) and T* : B(S × A) → B(S × A) are defined respectively by
(TnQ)(s, a) =r(s,a) + Y ∙ E[Q(st+1, at+ι) ∣ St = s,at = a, at+1 〜∏(∙ | st+1)],	(2.1)
(T*Q)(s, a) =r(s, a) + Y ∙ E[maxQ(st+1, a) ∣ st = s, at = a],	(2.2)
where st+1 〜P(∙ | st, at). The problems of estimating Qn and Q* are usually referred to as policy
evaluation and optimal control, respectively. Both of these problems lie at the core of reinforce-
ment learning. Specifically, policy evaluation is the pivotal step of dynamic programming methods.
Moreover, estimating Q* by applying the Bellman optimality operator in (2.2) repeatedly gives the
classical Q-learning algorithm (Watkins & Dayan, 1992), based on which a number of new algo-
rithms are developed.
In this work, we propose a saddle framework for stochastic fixed-point problems in general. 3
3 A Saddle Point Formulation of Bellman Residue Minimization
In this section, we formulate the problem of estimating the value functions introduced in §2 as saddle
point optimization problems. Before going into the details, we first introduce a standard assumption
on the MDP.
Assumption 3.1 (MDP Regularity). The MDP (S, A, P, r, Y) satisfies the following conditions.
3
Under review as a conference paper at ICLR 2019
(i)	. The action space A is a finite set, and there exists a constant RmaX > 0 such that |r(s, a)| ≤
RmaX for any (s, a) ∈ S × A.
(ii)	. For policy evaluation problem, we assume that the Markov chain on S × A induced by
policy π has a stationary distribution dπ ∈ P(S × A). For estimating Q*, we consider the
off-policy setting where we collect data using a behavioral policy πb : S → A. Moreover,
we assume that πb induces a stationary distribution dπb over S × A. Moreover, we assume
that it is possible to draw i.i.d. samples from dπ and dπb.
The first condition in Assumption 3.1 ensures that both Q* and Qn are bounded by RmaX/(1 - γ).
The stationary distributions dπ and dπb in the second condition is are the natural measures to evaluate
the error of estimating Qπ and Q*. This condition holds if the Markov chains induced by π and πb
are irreducible and aperiodic. Moreover, when these two Markov chains possess the rapid mixing
property, the observations are nearly independent, which justifies the assumption of i.i.d. sampling
from dπ and dπb .
In the following, we first focus on estimating Q*; the results for Qπ can be similarly obtained by
replacing T* by Tπ in (2.1). To simplify the notation, we denote dπb by ρ. When the capacity
of S is large, to estimate Q* efficiently, we estimate Q* using a parametrized function class F =
{Qθ : S × A → R, θ ∈ Rd}, where θ is the parameter. Then the problem is reduced to finding a
parameter θ ∈ Rd such that Qθ is close to Q*.
Since Q* is unknown, it is impossible to minimize the mean-squared error kQθ - Q* kρ2. Since Q*
is the unique fixed point of T* , a direct idea is to minimize the mean-squared Bellman residual
minimize J(θ) = ∣∣Qθ -T*Qθ∣∣2 = E(s,a)〜ρ{[Qθ(s,a)
(T*Qθ)(s, a)]2}.	(3.1)
—
via (stochastic) subgradient descent. By definition, the subgradient of J(θ) is
Vθ J (θ) = E(s,a)〜ρ([Qθ (s, a) - (T*Qθ )(s, a)] ∙ {(J Qθ )(s, a) - Vθ (T*Qθ)](s, a)}), (3.2)
where V(T*Qθ)(s, a) is the subgradient of T*Qθ(s, a) with respect to θ, which is given by
(Vθ T *Qθ )(s, a) = γ ∙ E[∙Vθ Qθ (st+ι, a0) ∣ St = s,at = a],
(3.3)
where a0 = argmaxb∈A Qθ(st+1, b). Although combining (3.2) and (3.3) yields the closed form of
VJ (θ), there exists a fundamental challenge when applying gradient-based methods to (3.1). Specif-
ically, notice that both (T*Qθ)(s, a) and (VθT*Qθ)(s, a) involves conditional expectation given
(s, a) ∈ S × A, and these two terms are multiplied are together in VθJ(θ) in (3.2). Thus, to con-
struct an unbiased estimator of V J(θ), given an observation (s, a)〜ρ, We need to draw two inde-
pendent samples from P(∙ | s, a) to ensure that the estimators of (T*Qθ)(s, a) and (VθT*Qθ)(s, a)
constructed respectively using these two samples are conditionally independent given (s, a). Such
an issue is called the “double sampling” problem in reinforcement learning literature (Baird et al.,
1995).
To resolve this issue, inspired by the saddle point formulation of soft Q-learning in Dai et al. (2017b),
we formulate the objective function J(θ) as
J(θ) = maximizeEρ{-1∕2 ∙ [μ(s,a)]2 + [Qθ(s, a) - (T*Qθ)(s, a)] ∙ μ(s,a)},	(3.4)
μ∈B(S×A)	'	J
where the maximization is taken over all functions μ: S×A→ R. In particular, for any fixed θ, the
solution of the optimization problem in (3.4) is δθ(s, a) = Qθ(s, a) - (T*Qθ)(s, a), which is known
as the temporal-difference (TD) error in the literature. Moreover, we parametrize μ in (3.4) using
function class G = {μω : S × A → R,ω ∈ Rp}. Then combining (3.1) and (3.4), we formulate
Bellman residual minimization as a stochastic saddle point problem
min max L(θ,ω) = Es,a,s，{-1∕2 ∙ [μω(s, a)]2 +
θ∈Rd ω∈Rp	s,a,s	ω
[Qθ(s, a) - r(s,a) - Y ∙ max Qθ(s0, a0)] ∙ μω (s, a)},	(3.5)
a0∈A
4
Under review as a conference paper at ICLR 2019
where (s, a)〜P and s0 〜 P(∙ | s, a) is the next state given (s, a). Here θ and ω can be viewed
as the primal and dual variables respectively. The gradients of L(θ, ω) with respect to θ and ω are
given by
Vθ L(θ, ω) = Es,a,s0 {μω (s, a) ∙ [Vθ Qθ (s, a) - Y ∙Vθ Qθ (s0, a0)]},	(3.6)
VωL(θ,ω) = Es,a,s0{Vωμω(s,a) ∙ [Qθ(s,a) - R(s,a) — Y ∙ maxQθ(s,b) - μω(s,a)] }, (3.7)
b∈A
where a0 in (3.6) satisfies that a0 = argmaxb∈A Qθ(s0, b). From (3.6) and (3.7), replacing
VθL(θ, ω) and VωL(θ, ω) by the stochastic gradients based on one observation (s, a, s0), we estab-
lish an stochasitc subgradient algorithm, whose details are given in Algorithm 1. Since the saddle
point problem (3.5) is nonconvex, in the algorithm we project the iterates onto compact sets Θ ⊆ Rd
and Ω ⊆ Rp respectively. Moreover, notice that ideally We would like to solve the inner maximiza-
tion problem in (3.5) for fixed θ. To achieve such a goal in an online fashion, we perform the primal
and dual steps in different paces. Specifically, the learning rates at and βt satisfy βt/α → ∞ as t
goes to infinity. Using results from stochastic approximation (Borkar, 2008; Kushner & Yin, 2003),
such a condition on the learning rates ensures that the dual iterates {ωt}t≥0 asymptotically track the
sequence {argmaxω∈Rp L(θt, ω)}t≥0, thus justifying our algorithm.
Algorithm 1 A Primal-Dual Algorithm for Q-Learning
Input: Initial parameter estimates θ0 ∈ Rd and ω0 ∈ Rp , primal and dual learning rates
{αt, βt }t≥0 .
for t = 0, 1, 2, . . . until convergence do
Sample (st, at)〜ρ, observe reward r(st, at) and the next state s；.
Let a0t = argmaxa∈A Qθt (s0t, a).
Update the parameters by
ωt+1 — πn{ωt + Bt ∙ Vω μωt (* st, at) ∙ [Qθt (st, at) - Ir(St, at) - Y ∙ Qθt (St, at) - μωt (st, at)]},
(3.8)
θt+1 J πΘ {θt - αt ∙23t(St,at) ∙ [vΘQθt (St,at) - Y ∙ VθQΘt (St,at)]}.	(3.9)
end for
Furthermore, for policy evaluation, we replace T* by Tπ in (3.4) to obtain
min max Es,a,s，,a { — 1/2	∙儿(s, a)]2	+	[Qθ(s, a) —	r(s, a) — Y ∙	Qθ(s0,a0)]	∙ μω (s,	a)},	(3.10)
Θ∈Rd ω∈Rp
where (s, a)〜 dπ, s0 〜 P(∙ | s, a), and a0 〜 π(∙ | s0). Here dπ is the stationary distribution on
S × A induced by policy π. Similarly, by computing the gradient of the objective in (3.10), we
obtain a stochastic gradient algorithm for policy evaluation.
Finally, it is worth noting that, various saddle point formulations of Bellman residual minimization
are proposed to avoid the issue of double sampling (Antos et al., 2008b; Farahmand et al., 2016; Dai
et al., 2017b). Our formulation is the same as the one for soft Q-learning in Dai et al. (2017b), and is
equivalent to that in Antos et al. (2008b); Farahmand et al. (2016) for policy evaluation. All of these
work are batch algorithms, with the assumption that the global optima of the inner maximization can
be reached. Whereas we propose an online first-order algorithm with nonlinear function approxi-
mation. Moreover, our analysis utilize tools from stochastic approximation of iterative algorithms
(Borkar, 2008; Kushner & Yin, 2003), which is significantly different from their theory. .
4	Theoretical Results
In this section, we lay out the theoretical results. For ease of presentation, we focus on the estimation
of Q* while our theory can be easily adapted to the policy evaluation problem. We first state the
assumption on function class F = {Qθ : θ ∈ Rd}.
5
Under review as a conference paper at ICLR 2019
Assumption 4.1. Here we assume that, for any (s, a) ∈ S × A, Qθ (s, a) is a differentiable func-
tion of θ such that ∣Qθ(s, a)| ≤ QmaX ∣∣VθQθ(s, a)k2 ≤ Gmax, and that VθQθ(s, a) is LiPschitz
continuous in θ, where Qmax ≥ Rmax/(1 - γ) and Gmax > 0 are two constants.
Here our assumPtion on F allows nonlinear function aPProximation of the value function in gen-
eral. Moreover, We only consider bounded value functions since Q* is bounded by RmaX/(1 一 γ).
Moreover, we assume that VθQθ (s, a) is bounded and LiPschitz continuous for regularity. This
assumPtion can be readily satisfied if θ is restricted to a comPact subset ofRd.
In addition, we assume that G = {μω : ω ∈ Rp} is a class of linear functions as follows.
Assumption 4.2. We assume that μω(s, a) = ω>φ(s, a) for any ω ∈ Rp, where φ: S × A → Rp
is a feature maPPing such that φ(s, a) is uniformly bounded for any s ∈ S, a ∈ A. Furthermore, we
assume that there exists a constant σmin > 0 such that E(s,a)〜ρ[φ(s, a)φ(s, a)>]占 σmin ∙ Ip.
Here we assume the dual function is linear for the PurPose of theoretical analysis. In this case, the
inner maximization Problem maxω L(θ, ω) has a unique solution
ω(θ) = {E(s,a)〜ρ[φ(s,a)φ(s,a)>]}-1E(s,a)〜ρ{φ(s,a) ∙ [Qθ(s,a) —(T*Qθ)(s,a)]} (4.1)
for any θ ∈ Rd. Thus, μ(θ) is the minimizer of ∣∣Qθ — T*Qθ — μω(θ)k2, i.e., μω(θ) is the best
aPProximation of the TD-error using function class G. It is Possible to extend our result to nonlinear
μω following the analysis in Heusel et al. (2017) under stronger assumptions. In addition, we note
that most online TD-learning algorithms uses linear function aPProximation. Moreover, it is shown
in Tsitsiklis & Van Roy (1997) that TD-learning with nonlinear function approximation may fail to
converge. To the best of our knowledge, for nonlinear function approximation, the nonlinear GTD
algorithm in Bhatnagar et al. (2009) is the only convergent online algorithm. However, their focus
solely on policy evaluation, and their approach depends on the Hessian Vθ2Vθ. As a comparison, our
method also consider nonlinear function approximation and can be applied to both policy evaluation
and optimal control.
Now we are ready to present the convergence result for Algorithm 1.
Theorem 4.3. We assume the learning rates {αt, βt}t≥0 in (3.8) and (3.9) satisfy
Xat = Xβt = ∞, X a + β2 < ∞, lim αt∕βt = 0.	(4.2)
t→∞
t≥0	t≥0	t≥0
In addition, let Θ ⊆ Rd and Ω ⊆ Rp be the Euclidean balls with radius Rθ and Rω respectively. For
function L(θ, ω) defined in (3.5), we define
K = {θ ∈ Θ: VθL(θ, ω)∣ω=ω(θ) =0} ∪ {θ ∈ ∂Θ: VθL(θ,ω)∣ω=ω(θ) = λθ for some λ ≥ 0}.
(4.3)
Then under Assumptions 3.1, 4.1, and 4.2, the iterates {(θt, ωt)}t≥0 created by Algorithm 1 con-
verges almost surely to the set {[θ*, ω(θ*)], θ* ∈ K}.
In addition to the Robbins-Monro condition, (Robbins et al., 1951), the learning rates in (4.2) also
satisfies limt→∞ at∕βt = 0. Intuitively, this means that the sequence {ωt}t≥ι tracks {ω(θt)}t≥ι
asymptotically. In other words, (4.2) enables our online algorithm to approximately solve the inner
maximization problem maxω L(θ, ω) with θ fixed. Using two-timescale stochastic approximation
(Borkar, 2008; Kushner & Yin, 2003), when studying the convergence {θt}t≥0, we could replace ωt
in (3.9) by ω(θt). In this case, using ODE approximation, {θt}t≥0 converges almost surely to K in
(4.3), which is the asymptotically stable equilibria of the projected ODE θ = VθL(θ, ω) ∣ω=ω(θ) +ξθ,
where ξθ(t) is the correction term caused by projection onto Θ.
Furthermore, even when Θ is sufficiently large such that {(ωt, θt)}t≥0 converges to [ω(θ*), θ*] with
Vθ L[θ*,ω(θ*)] = 0, due to the error of function approximation, θ* is not a stationary point of J (∙).
Specifically, let δθ = Qθ — T*Qθ be the TD-error. Then, by (3.6) we have
Vθ J(θ*) = E(s,a)〜ρ[Vθδθ*(s,a) ∙ δθ*(s,a)]
=E(s,a)〜P{Vθδθ*(s,a) ∙ [δθ*(s,a) — μω(θ*)(s,a)"∙
(4.4)
6
Under review as a conference paper at ICLR 2019
Since μω(θ*) is the best approximator of δθ* within G, θ* is in a neighborhood of a stationary point
of J(∙) if the function approximation error is small.
To see error incurred in estimating the TD-error reflects the fundamental limit of our method, we also
provide a finite-time analysis provided that there exists an optimization oracle which approximately
solves the inner maximization problem in (3.1).
Assumption 4.4 (Optimization Oracle). We assume that there exists an optimization oracle that
returns a bounded function 而 ∈ B(S X A) when queried by any θ ∈ Rd. Moreover, we assume
that ∣μθ(s,a)∣ ≤ 2Qmaχ for any (s,a) ∈ S × A, and there exists a constant e > 0 such that
keθ - δθkp ≤ ε.
Here We assume that the estimator 而 for δθ is bounded by 2Qmaχ since δθ is bounded by
2Rmax/(1 - γ) under Assumption 3.1. Moreover, in light of Algorithm 1, ε can be viewed as
the estimation error of the TD-error using {μω, ω ∈ Ω}, i.e., supθ∈θ infω∈Ω ∣∣δθ 一 μω kρ. Now We
update {θt}t≥0 by
θt+ι J θt	一	αt	∙ μθt (st,	at)	∙ [VθQθt (st,	at)	- Y ∙	VθQθt (st,	at)],	(4.5)
where st is the next state given (st, at) 〜ρ, and at = argmax。*/ Qθt (st, a). The following
theorem shows that any limit point of {θt}t≥o is in the vicinity of a stationary point of J(∙) with
error proportional to ε.
Theorem 4.5. Assume that the learning rates {αt}t≥0 in (4.5) satisfy Pt≥0 αt = ∞ and
Pt≥0 αt2 < ∞. Under Assumptions 3.1, 4.1, and 4.4, with probability one, we have
lim supt≥0 ∣VθJ(θt)∣2 ≤ 8G
max ∙ ε.
To understand this theorem, first notice that the update direction in (4.5) is a noisy version of
E(s,a)〜ρ["θt (s, a) ∙ Vθδθt (s, a)], which is a biased estimate of Vθ J(θt). Moreover, under Assump-
tions 4.1 and 4.4, such a bias can be bounded by 8Gmaχ ∙ ε. Due to this bias in gradient estimation,
{θt }t≥1 can only enter a neighborhood of a stationary point. In addition, for Algorithm 1 with G
being the class of linear functions, the optimization oracle outputs μω(θ) = φ>ω(θ) for any θ ∈ Θ.
Applying Cauchy-Schwarz inequality to (4.4), we have ∣∣VJ(θ*)∣∣2 ≤ 2Gmaχ ∙ ∣∣δθ* - μω(θ*)kρ∙
Thus, Theorems 4.3 and 4.5 yield consistent results, which implies that the error incurred by esti-
mating the TD-error using function class G leads to unavoidable error of our approach.
5	Experiments
To justify our proposed method for estimating value function, we compare it with the classical deep
Q-network (DQN) on several control tasks from the OpenAI Gym (Brockman et al., 2016). For a
fair comparison, we use the codes from OpenAI Baselines (Dhariwal et al., 2017) for DQN, and use
the same parametrization for both our method and DQN. The parameters are fine-tuned for each task
to achieve the state-of-art performance. We run the algorithms with 5 random seeds and report the
average rewards with 50% confidence intervals. Figure 1 (a)-(c) illustrates the empirical comparison
results for the environments CartPole-v0, MountainCar-v0 and Pendulum-v0. We can see that in all
these tasks, our method achieves the equivalent performance to DQN. The experiment settings are
as follows.
• CartPole-v0. For both methods, we set the learning rate for Q-network, i.e. αt, as 10-3
with batch size 32. The Q-network is a one-layer network with 30 hidden nodes. The
μ-network's learning rate and structure are the same as the Q-network.
• Pendulum-v0. For both methods, we set αt as 10-4 with batch size 1000. The Q-network
is a two-layer network with 40 hidden nodes for each layer. The learning rate and structure
of the μ-network are the same as Q-network.
• MountainCar-v0. For both methods, we set αt as 10-4 with batch size 1000. The Q-
network is a two-layer network with 20 hidden nodes for each layer. The learning rate for
the μ-network, i.e. βt, is also 10-4 with the same structure as Q-network.
7
Under review as a conference paper at ICLR 2019
As is shown in Theorem 4.3, we require limt→∞ at∕βt = 0 to guarantee the solution of inner
maximization problem maxω L(θ, ω) asymptotically. To justify this theoretical result, we test our
method on CartPole-v0 with different setting ofat and βt. The results are illustrated in Figure 1-(d).
We set at∕βt as 100, 1, and 0.01, respectively. As the learning rate ratio at∕βt deceases, the reward
increases faster and become more stable around the solution. From Case I, when at∕βt is too big,
our method cannot guarantee the solution of control task, which is in accordance with Theorem 4.3.
6	Conclusion
In this work, we propose an online first-order algorithm for the problem of Bellman residual mini-
mization with nonlinear function approximation in general. Our algorithm is motivated by a duality
formulation. Moreover, we establish the convergence of our problem by via ODE approximation,
utilizing tools from stochastic approximation. In addition, we also establish a finite-time conver-
gence result under the assumption of a computational oracle. Finally, numerical experiments are
provided to back up our theory.
(a) CartPole-v0
Pendulum-vO
200
-200
0
MountainCar-v0
,20⑷,60,8°
TTTT
50000 100000 150000 200000 250000 300000
Frame
(b) PendUlum-v0
CartPole-vO
O O
15
PJBMH
25000 50000 75000 100000 125000
Frame
(b) CartPole-v0 with various α= /βt

(a) MoUntainCar-v0
FigUre 1: In (a)-(c) we compare oUr method and DQN on three classical control tasks. Each plot
shows the average reward during training across 5 random runs, with 50% confidence interval. As
shown in these plots, our method achieves the equivalent performance to the classical DQN. In
addition, in (d) we show the performances of our method for CartPole-v0 under different learning
rates. For Case I, We set at = 10-2 ,βt = 1e - 4 with ɑt∕βt = 100. For Case II, We set at =
10-4, βt = 10-4 with at∕βt = 1. For Case III, we set a = 10-4, βt = 10-2 with at∕βt = 0.01.
The figure shows that at∕βt → 0 is critical for our method to work, which agrees with the theory.
8
Under review as a conference paper at ICLR 2019
References
Andras Antos, Csaba Szepesvari, and Remi Munos. Fitted Q-iteration in continuous action-space
MDPs. In Advances in neural information processing Systems, pp. 9-16, 2008a.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71(1):89-129, 2008b.
Leemon Baird et al. Residual algorithms: Reinforcement learning with function approximation. In
International Conference on Machine Learning, pp. 30-37, 1995.
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,
Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based
navigation using grid-like representations in artificial agents. Nature, pp. 1, 2018.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba
Szepesvari. Convergent temporal-difference learning with arbitrary smooth function approxi-
mation. In Advances in Neural Information Processing Systems, pp. 1204-1212, 2009.
Vivek S Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University
Press, 2008.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of rein-
forcement learning. arXiv preprint arXiv:1612.02516, 2016.
Woon Sang Cho and Mengdi Wang. Deep primal-dual reinforcement learning: Accelerating actor-
critic using bellman duality. arXiv preprint arXiv:1712.02467, 2017.
Bo Dai, Albert Shaw, Niao He, Lihong Li, and Le Song. Boosting the actor with dual critic. arXiv
preprint arXiv:1712.10282, 2017a.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Jianshu Chen, and Le Song. Smoothed dual
embedding control. arXiv preprint arXiv:1712.10285, 2017b.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Concentration bounds for two
timescale stochastic approximation with applications to reinforcement learning. arXiv preprint
arXiv:1703.05376, 2017a.
Gal Dalal, BaIazS Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis for td (0) with
linear function approximation. arXiv preprint arXiv:1704.01161, 2017b.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/
openai/baselines, 2017.
Joseph L Doob. Stochastic processes, volume 7. Wiley New York, 1953.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In International Conference on Machine Learning, pp. 1049-1058,
2017.
Amir M Farahmand, Mohammad Ghavamzadeh, Shie Mannor, and Csaba Szepesvari. Regularized
policy iteration. In Advances in Neural Information Processing Systems, pp. 441-448, 2009.
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvari, and Shie Mannor. Reg-
ularized policy iteration with nonparametric function spaces. The Journal of Machine Learning
Research, 17(1):4809-4874, 2016.
9
Under review as a conference paper at ICLR 2019
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361,
2017.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. arXiv preprint
arXiv:1702.07121, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6629-6640, 2017.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems, pp. 1008-1014, 2000.
Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and
Applications. Springer, New York, NY, 2003.
Harold Joseph Kushner and Dean S Clark. Stochastic Approximation Methods for Constrained and
Unconstrained Systems. Springer Science & Business Media, 1978.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforce-
ment learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In Conference on Uncertainty in Artificial Intelli-
gence, pp. 504-513. AUAI Press, 2015.
Hamid Reza Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy
learning control with function approximation. In nternational Conference on International Con-
ference on Machine Learning, pp. 719-726, 2010.
Odalric-Ambrym Maillard, Remi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh.
Finite-sample analysis of bellman residual minimization. In Proceedings of 2nd Asian Conference
on Machine Learning, pp. 299-314, 2010.
Michel Metivier and Pierre Priouret. Applications of a Kushner and Clark lemma to general classes
of stochastic algorithms. IEEE Transactions on Information Theory, 30(2):140-151, 1984.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal OfMachine
Learning Research, 9(May):815-857, 2008.
Jacques Neveu. Discrete-parameter martingales. Elsevier, 1975.
HL Prasad, LA Prashanth, and Shalabh Bhatnagar. Actor-critic algorithms for learning Nash equi-
libria in n-player general-sum games. arXiv preprint arXiv:1401.2086, 2014.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
Herbert Robbins, Sutton Monro, et al. A stochastic approximation method. The Annals of Mathe-
matical Statistics, 22(3):400-407, 1951.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
10
Under review as a conference paper at ICLR 2019
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354-359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. Cambridge: MIT
press, 1998.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learn-
ing with linear function approximation. In International Conference on Machine Learning, pp.
993-1000. ACM, 2009.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):
2603-2631, 2016.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in Neural Information Processing Systems, pp. 1075-1081, 1997.
Mengdi Wang. Primal-dual pi-learning: Sample complexity and sublinear run time for ergodic
markov decision problems. arXiv preprint arXiv:1710.06100, 2017.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Huizhen Yu. On convergence of emphatic temporal-difference learning. In Conference on Learning
Theory, pp. 1724-1751, 2015.
11
Under review as a conference paper at ICLR 2019
A Algorithms for Policy Evaluation and Soft Q-Learning
For policy evaluation, We replace T* in (3.4) by the Bellman evaluation operator Tπ, which yields
a saddle point problem
min max Lπ(θ,ω) = Es,a,s，,a，{-1/2 ∙儿(s, a)]2 + [Qθ(s, a) - r(s,a) - Y ∙ Qθ(s0,a0)] ∙ μω(s,a)},
θ∈Rd ω∈Rp
(A.1)
where (s, a)〜 ρ, s0 〜 P(∙ | s, a) is the next state given (s, a), and a 〜 π(∙ | s0). The gradients of
Lπ(θ, ω) with respect to θ and ω are given by
Vθ Ln (θ,ω) = Es,a,s0,a0 {μω (s,a) ∙ [Vθ Qθ (s, a) - Y ∙ Vθ Qθ (s0,a0)]},
Vω Lπ (θ,ω) = Es,a,s0,a0 {Vω μω (s,a) ∙ [Qθ (s,a) - R(s,a) - Y ∙ Qθ (s0,a0) - μω (s,a)]}.
Therefore, replacing the primal and dual gradients by their unbiased estimates, we obtain Algo-
rithm 2.
Algorithm 2 A Primal-Dual Algorithm for Policy Evaluation
Input: Initial parameter estimates θ0 ∈ Rd and ω0 ∈ Rp, primal and dual stepsizes {αt, βt }t≥0 .
for t = 0, 1, 2, . . . until convergence do
Sample (st, at)〜dπ, observe r(st, at) and the next state s[. Sample action a0 〜π(∙ | s0).
Update the parameters by
ωt+1 J πn{ωt + Bt ∙ Vωμωt (st, at) ∙ [Qθt (st, at) - r(st, at) - Y ∙ Qθt (St, a0) - μ<ωt (st, at)]},
θt+ι J πθ {θt - αt ∙ μ(ω (St,at) ∙ [vθ Qθt (St,at) - γ ∙ N θQθt(St, a0)]}.
end for
Furthermore, soft Q-learning is proposed Haarnoja et al. (2017) based on the maximum entropy
principle. This problem aims to find the fixed point of the soft Bellman operator T] defined by
(T ]Q)(s, a) = r(s,a) + Y ∙ E (T ∙ log{ X exp[Q(st+ι ,a)∕τ ]}卜 t = s,at = a),	(A.2)
where τ > 0 is the temperature parameter. By definition, T] can be seen as a smooth approximation
of the Bellman optimality operator, where the max function in (2.2) is replaced by the softmax
function, where parameter τ controls the approximation error. It is known that T] also admits a
unique fixed point, denoted by Q] : S × A → R. To estimate Q] with function approximation,
similar to (3.5) and (A.1), we consider the saddle point problem minθ∈Rd maxω∈Rp L](θ, ω), where
L](θ, ω) is given by
L](θ,ω) = Es,a,s0 -1/2 ∙ [μω(s, a)]2 + (qθ(s,a) - r(s, a) - Y ∙ τ ∙ log{ X exp[Qθ(s0,a)∕τ]}) ∙ μω(s,a)
where (s, a)〜P and s0 〜P(∙ | s, a). Here P is the stationary distribution on SXA induced by the
behavioral policy πb. By direct computation, the gradients of L] (θ, ω) with respect to θ and ω are
given by
Vθ L](θ,ω) = Es,a,s0,a0< μω (s, a) ∙ Vθ Qe (s, a) - Y ∙ E^V (s0,a) ∙VQθ (s0,a)
a∈A
Vωμω (s,a) ∙
Qθ(s, a) - R(s, a) -
Y ∙ T ∙
exp[Qθ (s0,a)∕τ ]
where we define νθ : S × A → [0, 1] by
exp[Qθ (s,a)∕τ ]
νθ(S,a) = Pa0∈A exp[Qθ (s,a)∕τ ],
∀(s, a) ∈ S × A,∀θ ∈ Rd.
Thus, we obtain a stochastic gradient algorithm for estimating Q], which is stated in Algorithm 3.
12
Under review as a conference paper at ICLR 2019
Algorithm 3 A Primal-Dual Algorithm for Soft Q-Learning
Input: Initial parameter estimates θ0 ∈ Rd and ω0 ∈ Rp, primal and dual stepsizes {αt , βt }t≥0 .
for t = 0, 1, 2, . . . until convergence do
Sample (st, at)〜ρ, observe reward r(st, at) and the next state s；.
Sample action a0t = b with proabrbility νθt (s0t, b) for any b ∈ A.
Compute the TD error δ; = Qθt(St,a；) - r(s；,a；) - Y ∙ T ∙ log{pa∈/exp[Qθt (st,a)/T]}.
Update the parameters by
ωt+ι J πn{ωt + Bt ∙ Vω μ(ω (st, at) ∙ [δt - μ(ω (st, at)]},
θt+ι J πθ{θt - αt ∙ μ(ω (st, at) ∙ [VθQθt (st, at) - γ ∙ VθQθt (St, at)]}.
end for
B Proofs of the Main Results
B.1	Proof of Theorem 4.3
Proof. The proof of this theorem consists of two steps, where we consider the faster and slower
timescales seperately.
Step 1. Faster timescale. We first consider the convergence of {θt, ωt}t≥1 in the faster timescale.
Using ODE approximation, we will show that the sequence of dual variables {ωt}t≥0 generated
from (3.8) tracks the solution of the inner maximization problem, i.e., argmaxω L(θt, ω). To begin
with, for notational simplicity, we define φt = φ(st, at),
δt = Qθt (st,at) - r(st,at) - Y ∙ Qθt (st, at), At = VθQθt (st, at) - Y ∙ VθQθt (st, at), (B.1)
for all t ≥ 0, where a0t = argmaxa∈A Qθt (s0t, a). Then the updating rules in Algorithms 1 reduce
to
ωt+1 J πΩ [ωt	+ Bt ∙	(δt	- φ>ωt)	∙ φt],	θt+1 J	πΘ [θt	- αt	∙	φ>ωt	∙ At],	(B.2)
where {αt, βt}t≥0 are the learning rates, and ΠΘ is the projection operator. Moreover, we define
Ft = σ({sτ, aτ, r(sτ, aτ), s0τ}τ≤t) as the σ-algebra generated by the history until time t. Thus, θt
and ωt are Ft-1-measurable for any t ≥ 1. Furthermore, for any θ ∈ Rd and ω ∈ Rp, we define
h(θ,ω) = E(s,a)〜ρ{φ(s,a) ∙ [Qθ (s,a) - (T*Qθ )(s,α) - φ(s,a)>ω]},	(B.3)
g(θ,ω) = E(s,a)〜ρ(φ(s,a)>ω ∙ {vθQθ (s,a) - [vθ (T*Qθ )](s,a)}).	(B.4)
Then by definition, we have E[δt-φ>ωt)∙φt ∣Ft-ι] = h(θt,ωt) and E[φ>ωrAt | Ft-ι] = g(θt,ωt)
for any t ≥ 1, which implies that {ζt}t≥1 and {ηt}t≥1 defined by
Zt =	(δt	-	φ>ωt) ∙ φt	-	h(θt, ωt),	ηt	=	φ>ωt	∙	At	— g(θt,	ωt)	(B.5)
are two martingale difference sequence with respect to filtration {Ft}t≥1. Moreover, we define
Cθ(θ) as the outer normal cone of Θ at θ ∈ Θ, and define Cn(ω) for Ω similarly.
Thus, (B.2) can be written as
_ β (αt∕βt ∙ At ∙ φ>ωt∖
t <φtφ>ωt - δt ∙ φt )
at/et ∙ At ∙ φ>ωt∖ + (ξθ
φtφ>ωt- δt∙ φt) <ξω
where ∏θ×ω is the projection onto product set Θ X Ω = {(u, v): U ∈ Θ,v ∈ Ω}, and Zθ and Ztω in
(B.5) are the correction terms induced by projection. Thus, by definition, we have ξtθ ∈ -CΘ(θt+1)
and ξω ∈ -CΩ(ωt+ι), respectively.
Furthermore, under Assumption 4.1, both Qθ(s, a), VθQθ(s, a), and φ(s, a) are bounded, which
implies that there exist a constant C > 0 such that E[kζtk22 + kηtk22 | Ft-1] ≤ C for all t ≥ 1.
(B.6)
13
Under review as a conference paper at ICLR 2019
Moreover, let MT = PT=I βt ∙ (ξθ>,ξω>)> ∈ Rd+p for any T ≥ 1. Since Pt≥1 β2 < ∞,
{MT}T ≥1 is a square-integrable martingale sequence. Moreover, by the Martingale convergence
theorem (Proposition VII-2-3(c) on page 149 of Neveu (1975)), {MT}T ≥1 converges almost surely.
Thus, for any > 0, by Doob’s martingale inequality Doob (1953), we have
suup	H su su H . λ , SUPN ≥TE[∣IMN- MT k2]	2C2 Pt≥τ β2
P sup IlMN - MTk≥ e ≤ -------------=------2----------- ≤ -------十------,
N≥T	2	2
which converges to zero as T goes to infinity. Furthermore, recall that, under the two-timescale
assumption of the learning rates, we have αt /βt → 0 as t goes to infinity, which implies that
limt→∞ at /βt ∙ At ∙ φt ωt = 0.
Now we apply the Kushner-Clark lemma (Kushner & Clark, 1978, see also Theorem D.2 in §D for
details) to sequence {(θt>, ωt>)>}t≥1, which implies that the asymptotic behavior of {(θt, ωt)}t≥1
is characterized by the projected ODE
θ = 0 + ξθ, ω = h(θ,ω)+ ξω,	(B.7)
where h is defined in (B.3), and ξθ and ξω satisfy ξθ(t) ∈ -CΘ(θ(t)) and ξω (t) ∈ -CΘ(ω(t)) for
any t ≥ 0. Specifically, sequence {(θt>, ωt>)>}t≥1 converges almost surely to the set of asymptoti-
cally stable equilibria of the projected ODE in (B.7), which is given by
{(θ*,ω*): θ* ∈ Θ,h(θ*,ω*) ∈CωQ*)}.	(B.8)
Recall that Ω is the Euclidean ball in Rp with radius Rω. Thus, the boundary of Ω, ∂Ω, is
{ω: ∣ω∣2 = Rω}. For any ω ∈ ∂Ω, the outer normal cone is Cn(ω) = {λ ∙ ω: λ ≥ 0}.
In the sequel, We show that, for any (θ*,ω*) in the equilibria in (B.8), ω* is in the interior of Ω,
i.e., ∣∣ω*∣∣2 < Rω. This implies that {(θt,ωt)}t≥ι converges almost surely to {(θ*,ω*): θ* ∈
Θ,h(θ* ,ω*)=0}.	一
Then, by definition, we have h[θ, ω(θ)] = 0 for any θ ∈ Rd, which implies that ω(θ) is the unique
maximizer of maxω∈Rp L(θ, ω) since L(θ, ω) is a strictly concave quadratic function of ω and
h(θ,ω) = VωL(θ,ω). Moreover, since ∣∣φ(s, a)∣2 is bounded for any (s,a) ∈ S × A and the
eigenvalues of E(s,Ο)〜ρ[φ(s, a)φ(s, a)>] are at least σma > 0, by norm inequality, we have
∣∣ω(θ)∣2 ≤ 2Qmaχ∕σmin ∙ SUP ∣φ(s, a)∣∣2∙	(B.9)
(s,a)
Thus, if Rω is larger than the right-hand side of (B.9), ω(θ) is in the interior of Ω for any θ ∈ Θ.
Now we assume to the contrary that ω* ∈ ∂Ω. By (B.8), there exists λ > 0 such that
λ ∙ ω* = h(θ*,ω*) = E(s,a)〜ρ[φ(s, a)φ(s, a)>] ∙ [ω(θ*) - ω*],	(B.10)
where the second equality follows from (B.3) and (4.1). For the right-hand side of (B.10), we have
[ω(θ*) - ω*]> ∙ E(s,a)〜ρ[φ(s, a)φ(s, a)>] ∙ [ω(θ*) - ω*] ≥ Cmin ∙ ∣∣ω* - ω(θ*)k2 > 0∙ (B.1I)
However, since ω(θ*) is in the interior of Ω, we have
λ ∙(ω(θ*) — ω*,ω*i = λ ∙ ∣∣ω*∣∣2 ∙ ∣∣ω(θ*)∣∣2 — λ ∙ ∣∣ω*∣∣2 < 0.	(B.12)
Thus, the contradiction between (B.11) and (B.12) implies that our assumption that ω* ∈ ∂Ω is not
true, i.e., ω* is in the interior of Ω. In this case, Cn(ω*) is an empty set. Thus the asymptotically
stable equilibria of the ODE in (B.7) are given by
{(θ*,ω*): θ* ∈ Θ,h(θ*,ω*) =0} = {[θ*,ω(θ*)]: θ* ∈ θ}.	(B.13)
This implies that, in the faster timescale, we could fix the primal parameter at θ ∈ Θ, and the
dual variable converges to ω(θ), which is the unique solution of the dual optimization problem
maxω L(θ, ω). In other words, using two timescale updates, we essentially solve the dual problem
for each θ.
14
Under review as a conference paper at ICLR 2019
Step 2. Slower timescale. Note that (B.13) cannot characterize the asymptotic behavior of the
primal variable. Now we proceed to establish a finer characterization of the asymptotic behavior of
{θt, ωt}t≥1 by looking into the slower timescale. For ease of presentation, let Et = σ(θτ, τ ≤ t) be
the σ-field generated by {θτ , τ ≤ t}. In addition, we define
ψ(1) = -φ>ωt ∙ At + E[φ>ωt ∙ At | Et] ψ(2) = -E[φ>ωt ∙ At | Et] + g[θt, ω(θt)↑, (B.14)
where function g is defined in (B.4), At and φt are defined in (B.1). It holds that {ψt(1)}t≥1 is a
martingale difference sequence. Note that by the definition of g, We have E[φ>ω(θt) ∙ At | Et] =
g[θt, ω(θt)]. Using the notation in (B.14), the primal update in (B.2) can be written as
θt+ι = πθ [θt - αt ∙ g[θt, ω(θt)] + αt ∙ ψ(1) + αt ∙ ψ(2)].	(B.15)
As shoWn in the first step of the proof, ω(θt) - θt converges to zero as t goes to infinity. Moreover,
under Assumption 4.1, both VθQθ(s, a) and φ(s, a) are bounded. By Cauchy-Schwarz inequality,
kψ(2) k2 = ∣∣E{Φ>[ω(θt)-ωt]∙ At ∣Et}∣∣2
≤ sup ∣∣Φ(s,a)k2 ∙ sup ∣∣VθQθ(s, a)∣∣2 ∙ E[∣∣ω(θt) - ωt∣∣2 |Et],
(s,a)	(s,a)
which converges to zero almost surely. Besides, the boundedness of VθQθ (s, a) and φ(s, a) also
implies that there exist a constant C >0 such that E[∣ψt(1) ∣22 | Et] ≤ C for all t ≥ 1.
Furthermore, we define WT = PT=I αt ∙ ψ(1) ∈ Rd for any T ≥ 1. Since Pt≥ι 02 < ∞,
{WT}T ≥1 is a square-integrable martingale sequence, which converges almost surely by the Mar-
tingale convergence theorem (Neveu, 1975). Moreover, Doob’s martingale inequality (Doob, 1953)
implies that
lim P sup ∣WN - WT ∣ ≥	≤ lim
T→∞	N≥T	T→∞
supN ≥T E[∣WN - WT ∣22]
≤ lim
T→∞
2C2 Pt≥T αt2
0.
2
2
To apply the Kushner-Clark lemma, we additionally need to verify that function g(θ) = g[θ, ω(θ)]
is a continuous. To see this, note that ω(θ) defined in (4.1) is a continuous function, and g(θ, ω) in
(B.4) is continuous in both θ and ω.
Finally, applying by the Kushner-Clark lemma (Kushner & Clark, 1978) to sequence {θt}t≥1, it
holds that {θt}t≥1 converges almost surely to the set of asymptotically stable equilibria of the ODE
θ = g(θ)+ ξθ,	ξθ(t) ∈-Cθ(θ(t)),	(B.16)
where CΘ (θ) is the outer normal cone of Θ. Since Θ is a Euclidean ball with radius Rθ, if θ is on
the boundary of Θ, i.e., ∣θ∣2 = Rθ, we have Cθ(Θ) = {λ ∙ θ,λ ≥ 0}. Thus, for any asymptotically
stable equilibrium θ* of (B.16), if θ* is in the interior of Θ, i.e., ∣θ∣ < Rθ, we have g(θ*') = 0.
Additionally, if ∣θ*∣2 = Rmax, we have g(θ*) ∈ Cθ(Θ*), which implies that there exists λ > 0
such that g(θ*) = λ ∙ θ*. Therefore, we conclude the proof of Theorem 4.3.
□
B.2 Proof of Theorem 4.5
Proof. In the sequel, to simplify the notation, we use C to denote absolute constant, whose value
might change from line to line. In addition, for any θ ∈ Rd, we define
VJ(θ) = Εs,a,s0 {eθ(s, a) ∙ [VθQθ(s,a) - Y ∙ £ l{a0 = argmax Qθ(s0,b)} ∙VθQθ(s0,a0)]}
a∈A	b∈A
=Ε(s,a)〜ρ["θ(s,a) ∙ Vθδθ(s, a)],	(B.17)
where δθ = Qθ 一 T*Qθ is the TD-error, and eθ is the output of the optimization oracle for query θ.
15
Under review as a conference paper at ICLR 2019
Note that θt in (4.5) is updated in the direction of an unbiased estimate of VJ(θt). To simplify the
notation, we let
Zt = μθt (st, at) ∙ [vθQθt (st, at) - vθQθt (st, at)] - VθJ(θt),	(B.18)
where vθJ(θt) is defined in (B.17), a0t = argmaxa∈A Qθt(s0t, a). Then the update rule in (4.5) can
be written as θt+ι = θt 一 αt ∙ [VJ(θt) + Zt]. By Assumptions 4.1 and4.2, {Zt}t≥o is a sequence of
bounded and centered random vectors. Moreover, since both 而 and VQθ are bounded by QmaX
and Gmax on S × A, respectively, Zt defined in (B.18) is a bounded random variable satisfying
kZt∣∣2 ≤ 4Qmax∙Gmax. LetFt betheσ-fieldgeneratedby {θj,j ≤ t}. ThenwehaveE(ξt ∣Ft) = 0
and E(kξtk2 | Ft) ≤ C for some constant C > 0.
Moreover, note that the gradient of J (θ) can be written as Vθ J (θ) = E(s,a)〜ρ[δθ (s, a) ∙Vθ δθ (s, a)].
By the definition of the TD-error, under Assumption 4.1, δθ and Vθδθ are bounded by 2Qmax and
2Gmax on S × A respectively. Moreover, since Vθδθ is Lipschitz in θ, there exists a constant L > 0
such that VθJ(θ) is L-Lipschitz. Thus, we have
J(θ) ≤ J(θt) - hvθJ(θt), θt+ι — θti + L/2 ∙ kθt+ι — θtk2
≤ J(∏θt) — at ∙ <Vθ J(θt), VJ(θt) + Q + α2 ∙ L/2 ∙ ∣∣VJ(θt) + Zt∣∣2∙	(B.19)
Taking conditional expectation on both sides of (B.19) given Ft, since Zt is centered with finite
variance, we obtain that
E[J(θt+ι) |	Gt]	≤	J(θt)	- at ∙〈Ve J(θt), VJ(%)〉一 a2 ∙ L/2	∙	∣∣Vθ J(θt)k2 — C ∙ α,t,	(B.20)
where C > 0 is an absolute constant. Note that VJ (θ) is a biased estimate of VθJ(θ). Under
Assumptions 4.1 and 4.4, the bias can be bounded via Cauchy-Schwarz inequality:
kvθ J (θ) - V J (θ)k2 = ||E(s,a)〜ρ{vθ δθ (s,a) ∙ [δθ (s, a) - μθ (s, a)]}∣∣2
≤ E(s,a)〜ρ [kvθδθ (s,a)∣∣2] ∙ kδθ - eeθ IlP ≤ 2Gmax ∙ ε,	(B.21)
where ε is the error of the optimization oracle in Assumption 4.4. We denote VJ(θt) 一 VθJ(θt) by
ψt hereafter to simplify the notation. Moreover, by Cauchy-Schwarz inequality, we have
hVθJ(θt), VJ(θt)i = hVθJ(θt), VθJ(θt) - ψti ≥ kVθJ(θt)k2 一 kVθJ(θt)k2 ∙ kψtk2. (B.22)
Similarly, for IVe θJ(θt)I22, we obtain that
kVJ(θt)k2 ≤ kVθJ(θt)k2 + kψtk2 + 2 ∙kVθJ(θt)k2 ∙kψtk2.	(B.23)
Combining (B.20), (B.21), (B.22), and (B.23), we have
E[J(Θt+1) IFt]
≤ J(θt) + C ∙ at - at ∙ (1 - at ∙ L/2) ∙ ∣Vθ J(θt)∣2
+ at ∙ (1 + at ∙ L) ∙ ∣VθJ(θt)kt ∙ kψtk2
≤ J(θt) + C ∙ at - at ∙ (1 - at ∙ L/2) ∙ ∣Vθ J(θt)kt
+ at ∙ 2(1 + at ∙ L) ∙ GmaX ∙ ε	∙ kvθJ(θt)k2∙	(B∙24)
Since Pt≥0 att	<	∞, at converges to zero as t goes to infinity.	When t is sufficiently large such
that 4at ∙ L < 1, (B.24) implies that
E[J(θt+ι)	I Ft]	≤	J(θt)	- at/2 ∙ ∣Vθ J(θt)k2 + 4at ∙ GmaX ∙	ε ∙ ∣∣Vθ J(θt)∣∣2 + C ∙ at
=J(θt)	- at/2 ∙ kvθ J(θt)∣∣2 ∙ [kvθ J(θt)∣∣2	- 8Gmax ∙ ε] + C ∙ a2 .	(B.25)
Furthermore, since J(θ) is a nonnegative function and Pt≥1 att < ∞, by (B.25) we have
X at ∙ kvθJ(θt)∣∣2 ∙ [kvθJ(θt)∣∣2 - 8Gmax ∙ ε] < ∞.	(B.26)
t≥0
16
Under review as a conference paper at ICLR 2019
In the sequel, We show by contradiction that every limit point θ* of {θt}t≥ι satisfies that
∣∣Vθ J(θ*)k∣2 ≤ 8Gmax ∙ ε. Let ν > 0 be an arbitrary number. We first show that {t: ∣U J(θt)k2 <
8Gmax ∙ ε + ν} is an infinite set. Suppose this is false, then there exists an integer to such that
∣∣Vθ J (θt)k2 ≥ 8Gmaχ ∙ ε + V for all t ≥ to. Since Pt≥0 at = ∞, we have
X at ∙ kvθJ(Ot)Il2 ∙ [∣∣vθJ(θt)k2 - 8Gmax ∙ ε] ≥ X at ∙ ν ∙ (8Gmax ∙ ε + V)= ∞,
t≥t0	t≥t0
which contradicts (B.26). Thus, there are infinite θjs satisfying ∣∣Vθ J(θt)∣∣2 ≤ 8Gmax∈ + V. Since
V can be arbitrarily small, this implies
liminf ∣∣Vθ J(θt)∣2 ≤ 8Gmaχ ∙ ε.	(B.27)
Let ε = 8Gmaχ ∙ ε. It remains to show that limsupt≥o ∣∣Vθ J(θt)∣∣2 ≤ εb, which, combined with
(B.27), establishes the theorem.
Suppose this argument does not hold, then for any V > 0, there exists ε > 0 such that ∣VθJ(θt)∣2 ≥
δb + 2V for infinitely many t. Moreover, for this particular V, ∣VθJ(θt)∣2 ≤ δb + V also holds for
infinitely many t. We define index sets N1 and N2 by
N1 ={θt:	∣VθJ(θt)∣2 ≥δb+2V},	N2	={θt:	∣VθJ(θt)∣2 ≤δb+V},
which are two disjoint and closed sets by the continuity of ∣VθJ(θ)∣2. Moreover, we define
D(N1,N2)= inf inf ∣θ - θ0∣2,	(B.28)
θ∈N1 θ0∈N2
which is a positive number since N1 and N2 are disjoint. In addition, since both N1 and N2 are
infinite sets, there exists an index set I ⊆ N such that the subsequence {θt}t∈I of {θt}t≥o crosses
N1 and N2 infinitely often. That is, there exists two sequences {si}i≥1 and {ti}i≥1 ⊆ N such that
{θt}t∈I = Si≥1{θsi, θsi+1, . . . , θti-1}. Furthermore, we have {θsi}i≥1 ⊆ N1, {θti}i≥1 ⊆ N2,
and
δb + ε ≤ ∣∣Vθ J (θ')k2 ≤ δb + 2ε for all ' ∈ ∪i≥ι {si + 1,... ,ti — 1}.
Hence, by triangle inequality, we have
∞ ti-1	∞	∞
X ∣θt+1 — θt∣2 = XX
∣Θ'+1-θ` k2 ≥ X kθti-θsik2 ≥ X D(Nι, M) = ∞, (B.29)
t∈I	i=1 '=Si	i=1	i=1
where D(N1, N2) > 0 is defined in (B.28). Moreover, by (B.26), we have
∞ > ^X αt ∙ kvθJ (Ot)Il2 ∙ [∣∣vθJ (Ot)Il2 - δb] ≥ ^X at ∙ (δb + V) ∙ ν,
which further implies that Pt∈I αt < ∞. However, by the update rule in (4.5), under Assumption
4.1, it holds that ∣θt+ι 一 θt∣2 ≤ at ∙ 2Qmaχ ∙ Gmax, where we use the boundedness of eθ(s, a) and
VθQθ(s, a). Thus, it holds that
E kθt+1 — θt∣2 ≤ 2Qmaχ ∙ GmaXE °t < ∞,
which contradicts (B.29). This contradiction implies that lim supt≥o ∣VθJ(Ot)∣2 ≤ εb. Therefore,
we conclude the proof of Theorem 4.5.	□
C Statistical Error
In this section, using tools from nonparametric regression, we establish the statistical error of our
saddle point formulation in (3.5). To this end, we assume that the state space S is a closed compact
17
Under review as a conference paper at ICLR 2019
subset of the Euclidean space Rm and let H be the Sobolev space Wk (Rm) restricted on S. In
addition, we define
1/2
HA = {Q ∈ B(S × A): Q(∙, a) ∈H, ∀a ∈ A}, kQkH = E kQ(∙, a)kW%
a∈A
for any Q ∈ HA, where ∣∣ ∙ ∣∣wk is the Sobolev norm.
We consider the Bellman residual minimization problem with function class HA based on n i.i.d.
observations {(si, ai, si)}, where (Si,ai)〜P and Si is the next state. Replacing the loss function
in (3.5) by its sample-based counterpart, we define
n
Ln(Q,μ) = E{ — 1/2 ∙ [μ(si,ai)]2 + [Q(si, ai) - r(si,ai) — Y ∙ max Q(si, a。] ∙ μ(si, a/}
i=1	a0i ∈A
(C.1)
1n
大∙ y^{[Q(si, ai) — r(si, ai) — Y ∙ max Q(Si, ai)]
2	a0i ∈A
i=1	i
—[Q(si,ai) — μ(si,ai) — r(si,ai) — Y ∙ maxQ(si, ai)]2}.
a0i∈A
Hence, if We denote Q — μ in (C.1) by h ∈ HA, Ln(Q, μ) becomes
Ln(Q,h) = 1 ∙ X{[Q(si,ai) — r(si,ai) — Y ∙ maxQ(si,ai)]2
2	a0i ∈A
i=1	i
—[h(si,ai) — r(si,ai) — Y ∙ mmax Q(si,ai)]2}.
Therefore, the sample-based sample-based functional optimization problem
♦	∕C7∖ ʌ	11 7 Il . ʌ	11 11
min, max Ln9, h) — λμ ∙ ∣h∣H + Xq ∙ ∣Q∣h
Q∈HA h∈HA
(C.2)
(C.3)
reduces to the batch Bellman Residual Minimization algorithm studied in Antos et al. (2008b);
Farahmand et al. (2009; 2016). Here λμ and Xq in (C.3) are two positive regularization parameters,
and We adopt regularization to avoid overfitting. Moreover, since μ = Q — h, the optimization
problem in (C.3) is equivalent to
mm max Ln(Q, h) — λμ ∙ ∣∣Q — μ∣H + Xq ∙ ∣∣Q∣h.
Q∈HA μ∈HA
(C.4)
T ,/公 ^∖ 1 . 1	1	Γ∙ ，C Λ ∖ T T ♦	.1 .1	. ∙ 1	1 . ∙ 1 	<	1 . 1 /CCCC CCT
Let (Q, μ) be the solution of (C.4). Using the theoretical result in Farahmand et al. (2009; 2016),
we obtain the statistical rate of Q, which is stated in the following theorem.
Theorem C.1. Besides Assumption 3.1, we further make the following assumptions.
(a)	. The Sobolev space Wk (Rm) satisfies that 2k > m.
(b)	. We assume that any Q ∈ HA satisfies that |Q(s, a)| ≤ Qmax with Qmax ≥ Rmax/(1 — Y).
Moreover, HA contains the optimal value function Q*.
(c)	. For any Q ∈ HA, there exists positive constants Ki and K such that ∣∣T*Q∣∣h ≤ Ki +
K ∙ IIQIh.
Moreover, let α = m/(2k). We set the regularization parameters in (C.4) to be
λμ = λQ = (n %*%)-1/(1 + 闲
T ,/公 ^∖ 1 . 1	1	Γ∙ ZZ-X Λ ∖ EI	Γ∙	一 /C T ∖	1
Let (Q, μ) be the solution of (C.4). Then for any η ∈ (0,1), We have
ʌ	r	n-1/(1+a)
∣Q — Q*∣p ≤ (1 — Y)2 ∙ [Cι(Q*,Ki,K2) ∙ iog(i∕η) + C2(Q*,K1,K2)]
with probability at least 1 — η, where we define C1(Q*,K1, K2) = C ∙ (Ki + K) ∙ [^"1110^(1+0)
and C2(Q*,K1,K2) = C ∙ K2 ∙ (1 + ∣∣Q* ∣∣H2∕(1+α)) for some absolute constant C > 0.
18
Under review as a conference paper at ICLR 2019
Proof. The proof follows from the results in Farahmand et al. (2009; 2016). Note that the functional
optimization problems in (C.4) and (C.3) are equivalent. Applying Theorem 11 in Farahmand et al.
(2016) to the problem in (C.3), we obtain that
kQ - T*Qkp ≤ n-1/(1+a) ∙ [C1(Q*,K1,K2) ∙ log(1∕η) + C2(Q*, Ki, K2)]	(C.5)
with probability at least 1 - η. In addition, since Q* is the unique fixed point of T*, by triangle
inequality, we have
..^	...	.. ^ 一 ^..	一 .. ^ 一， 一 一 ^ 一 ^..	.. ^ ...
kQ - Q*kρ ≤ kQ -T*Qkρ + kT*Q -T*Q*kρ ≤ kQ -T*Qkρ + Y ∙kQ - Q*kρ,
where the last inequality holds since T* is γ-contractive. Thus it holds that kQ - Q* kρ ≤ (1 -
γ)-1 ∙ kQ - T*Qkρ. Therefore, by (C.5), we obtain that
kQ - Q*kp ≤ (1 - Y)-2 ∙kQ-τ*Qkp
≤ n-1/(1+a) ∙ (1 - γ)-2 ∙ [C1 (Q*,K1,K2) ∙ log(1∕η) + C2(Q*,K1,K2)],
which concludes the proof of Theorem C.1.	□
D Kushner-Clark Lemma
We state here the well-known Kushner-Clark Lemma (Kushner & Clark, 1978; Metivier & Priouret,
1984; Prasad et al., 2014) in the sequel.
Let Γ be an operator that projects a vector onto a compact set X ⊆ RN. Define a vector Γ(∙) as
b[h(x)]= lim ( r[x + ηh(X)I -X ),
0<η→0	η
for any X ∈ X and with h : X → RN continuous. Consider the following recursion in N dimensions
Xt+1 =Γ{xt + Yt [h(xt) + ξt + βt]}.	(D.1)
The ODE associated with (D.1) is given by
X = Γ[h(x)].	(D.2)
Assumption D.1. We make the following assumptions:
•	h(∙) is a continuous RN-valued function.
•	The sequence {βt}, t ≥ 0 is a bounded random sequence with βt → 0 almost surely as
t → ∞.
•	The stepsizes Yt, t ≥ 0 satisfy Yt → 0 as t → ∞ and Pt Yt = ∞.
•	The sequence ξt, t ≥ 0 satisfies for any > 0
lim P sup X Yτξτ	≥	= 0.
t	n≥t τ=t	2
Then the Kushner-Clark Lemma says the following.
Theorem D.2. Under Assumption D.1, suppose that the ODE (D.2) has a compact set K* as its set
of asymptotically stable equilibria. Then Xt in (D.1) converges almost surely to K* as t → ∞.
19