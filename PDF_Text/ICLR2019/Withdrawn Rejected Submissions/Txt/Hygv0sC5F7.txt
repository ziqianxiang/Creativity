Under review as a conference paper at ICLR 2019
When Will Gradient Methods Converge to
Max-margin Classifier under ReLU Models ?
Anonymous authors
Paper under double-blind review
Ab stract
We study the implicit bias of gradient descent methods in solving a binary classi-
fication problem over a linearly separable dataset. The classifier is described by
a nonlinear ReLU model and the objective function adopts the exponential loss
function. We first characterize the landscape of the loss function and show that
there can exist spurious asymptotic local minima besides asymptotic global min-
ima. We then show that gradient descent (GD) can converge to either a global or
a local max-margin direction, or may diverge from the desired max-margin direc-
tion in a general context. For stochastic gradient descent (SGD), we show that it
converges in expectation to either the global or the local max-margin direction if
SGD converges. We further explore the implicit bias of these algorithms in learn-
ing a multi-neuron network under certain stationary conditions, and show that the
learned classifier maximizes the margins of each sample pattern partition under
the ReLU activation.
1 Introduction
It has been observed in various machine learning problems recently that the gradient descent (GD)
algorithm and the stochastic gradient descent (SGD) algorithm converge to solutions with certain
properties even without explicit regularization in the objective function. Correspondingly, theoreti-
cal analysis has been developed to explain such implicit regularization property. For example, it has
been shown in Gunasekar et al. (2018; 2017) that GD converges to the solution with the minimum
norm under certain initialization for regression problems, even without an explicit norm constraint.
Another type of implicit regularization, where GD converges to the max-margin classifier, has been
recently studied in Gunasekar et al. (2018); Ji & Telgarsky (2018); Nacson et al. (2018a); Soudry
et al. (2017; 2018) for classification problems as we describe below. Given a set of training samples
zi = (xi, yi) for i = 1, . . . , n, where xi denotes a feature vector and yi ∈ {-1, +1} denotes the
corresponding label, the goal is to find a desirable linear model (i.e., a classifier) by solving the
following empirical risk minimization problem
1n
min L(w)： = — T'(yiWlXi).
w∈Rd	n
i=1
(1)
It has been shown in Nacson et al. (2018a); SoUdry et al. (2017; 2018) that if the loss function '(∙)
is monotonically strictly decreasing and satisfies proper tail conditions (e.g., the exponential loss),
and the data are linearly separable, then GD converges to the solution w with infinite norm and the
maximum margin direction of the data, although there is no explicit regularization towards the max-
margin direction in the objective function. Such a phenomenon is referred to as the implicit bias of
GD, and can help to explain some experimental results. For example, even when the training error
achieves zero (i.e., the resulting model enters into the linearly separable region that correctly classi-
fies the data), the testing error continues to decrease, because the direction of the model parameter
continues to have an improved margin. Such a study has been further generalized to hold for various
other types of gradient-based algorithms Gunasekar et al. (2018). Moreover, Ji & Telgarsky (2018)
analyzed the convergence of GD with no assumption on the data separability, and characterized the
implicit regularization to be in a subspace-based form.
The focus of this paper is on the following two fundamental issues, which have not been well ad-
dressed by existing studies.
1
Under review as a conference paper at ICLR 2019
•	Existing studies so far focused only on the linear classifier model. An important question one
naturally asks is what happens for the more general nonlinear leaky ReLU and ReLU models.
Will GD still converge, and if so will it converge to the max-margin direction? Our study here
provides new insights for the ReLU model that have not been observed for the linear model in the
previous studies.
•	Existing studies mainly analyzed the convergence of GD with the only exceptions Ji & Telgarsky
(2018); Nacson et al. (2018b) on SGD. However, Ji & Telgarsky (2018) did not establish the
convergence to the max-margin direction for SGD, and Nacson et al. (2018b) established the
convergence to the max-margin solution only epochwisely for cyclic SGD (not iterationwise for
SGD under random sampling with replacement). Moreover, both studies considered only the
linear model. Here, our interest is to explore the iterationwise convergence of SGD under random
sampling with replacement to the max-margin direction, and our result can shed insights for
online SGD. Furthermore, our study provides new understanding for the nonlinear ReLU and
leaky ReLU models.
1.1	Main Contributions
We summarize our main contributions, where our focus is on the exponential loss function under
ReLU model.
We first characterize the landscape of the empirical risk function under the ReLU model, which is
nonconvex and nonsmooth. We show that such a risk function has asymptotic global minima and
asymptotic spurious local minima. Such a landscape is in sharp contrast to that under the linear
model previously studied in Soudry et al. (2017), where there exist only equivalent global minima.
Based on the landscape property, we show that the implicit bias property in the course of the con-
vergence of GD can fall into four cases: converges to the asymptotic global minimum along the
max-margin direction, converges to an asymptotic local minimum along a local max-margin di-
rection, stops at a finite spurious local minimum, or oscillates between the linearly separable and
misclassified regions without convergence. Such a diverse behavior is also in sharp difference from
that under the linear model Soudry et al. (2017), where GD always converges to the max-margin
direction.
We then take a further step to study the implicit bias of SGD. We show that the expected averaged
weight vector normalized by its expected l2 norm converges to the global max-margin direction or
local max-margin direction, as long as SGD stays either in the linearly separable region orin a region
of the local minima defined by a subset of data samples with positive label. The proof here requires
considerable new technical developments, which are very different from the traditional analysis of
SGD, e.g., Bottou et al. (2016); Duchi & Singer (2009); Nemirovskii et al. (1983); Shalev-Shwartz
et al. (2009); Xiao (2010); Bach & Moulines (2013); Bach (2014). This is because our focus here is
on the exponential loss function without attainable global/local minima, whereas traditional analysis
typically assumed that the minimum of the loss function is attainable. Furthermore, our goal is to
analyze the implicit bias property of SGD, which is also beyond traditional analysis of SGD.
We further extend our analysis to the leaky ReLU model and multi-neuron networks.
1.2	Related Work
Implicit bias of gradient descent: Gunasekar et al. (2018) studied the implicit bias of GD and SGD
for minimizing the squared loss function under bounded global minimum, and showed that some of
these algorithms converge to a global minimum that is closest to the initial point. Another collection
of papers Gunasekar et al. (2018); Ji & Telgarsky (2018); Nacson et al. (2018a); Soudry et al. (2017);
Telgarsky (2013); Soudry et al. (2018) characterized the implicit bias of algorithms for the loss
functions without attainable global minimum. Telgarsky (2013) showed that AdaBoost converges
to an approximate max-margin classifier. Soudry et al. (2017; 2018) studied the convergence of GD
in logistic regression with linearly separable data and showed that GD converges in direction to the
solution of support vector machine at a rate of 1/ ln(t). Nacson et al. (2018a) improved this rate to
ln(t)/√t under the exponential loss via normalized gradient descent. Gunasekar et al. (2018) further
showed that steepest descent can lead to margin maximization under generic norms. Ji & Telgarsky
(2018) analyzed the convergence ofGD on an arbitrary dataset, and provided the convergence rates
2
Under review as a conference paper at ICLR 2019
along the strongly convex subspace and the separable subspace. Our work studies the convergence
ofGD and SGD under the nonlinear ReLU model with the exponential loss, as opposed to the linear
model studied by all the above previous work on the same type of loss functions.
Implicit bias of SGD: Ji & Telgarsky (2018) analyzed the average SGD (under random sampling)
with fixed learning rate and proved the convergence of the population risk, but did not establish the
parameter convergence of SGD in the max-margin direction. Nacson et al. (2018b) established the
convergence of cyclic SGD epochwisely in direction to the max-margin classifier at a rate O(1/ ln t).
Our work differs from these two studies first in that we study the ReLU model, whereas both of these
studies analyzed the linear model. Furthermore, we showed that under SGD with random sampling,
the expectation of the averaged weight vector converges in direction to the max-margin classifier at
a rate O(1∕√lnt).
Generalization of SGD: There have been extensive studies of the convergence and generalization
performance of SGD under various models, of which we cannot provide a comprehensive list due
to the space limitations. In general, these type of studies either characterize the convergence rate
of SGD or provide the generalization error bounds at the convergence of SGD, e.g., Brutzkus et al.
(2017); Wang et al. (2018); Li & Liang (2018), but did not characterize the implicit regularization
property of SGD, such as the convergence to the max-margin direction as provided in our paper.
2 ReLU Clas sification Model
We consider the binary classification problem, in which we are given a set of training samples
{z1, . . . , zn}. Each training sample zi = (xi, yi) contains an input data xi and a corresponding
binary label yi ∈ {-1, +1}. We denote I+ := {i : yi = +1} as the set of indices of samples
with label +1 and denote I- := {i : yi = -1} in a similar way. Their cardinalities are denoted as
n+ and n-, respectively, and are assumed to be non-zero. We consider all datasets that are linearly
separable, i.e., there exists a linear classifier w such that yiw|xi > 0 for all i = 1, . . . , n.
We are interested in training a ReLU model for the classification task. In specific, for a given input
data x, the model outputs σ(wlXi), where σ(v) = max{0,v} is the ReLU activation function
and w denotes the weight parameters. The predicted label is set to be sgn(w|x). Our goal is to
learn a classifier by solving the following empirical risk minimization problem, where we adopt the
exponential loss.
1n
min L(W) := — ɪ2 '(w, Zi), where '(w, Zi) = exp(-yiσ(wlx,).
w∈	i=1
(P)
The ReLU activation causes the loss function in problem (P) to be nonconvex and nonsmooth. There-
fore, it is important to first understand the landscape property of the loss function, which is critical
for characterizing the implicit bias property of the GD and SGD algorithms.
3 Implicit Bias of GD in Learning ReLU Model
3.1	Landscape of ReLU Model
In order to understand the convergence of GD under the ReLU model, we first study the landscape
of the loss function in problem (P), which turns out to be very different from that under the linear
activation model. As been shown in Soudry et al. (2017); Ji & Telgarsky (2018), the loss function
in problem (P) under linear activation is convex, and achieves asymptotic global minimum, i.e.,
VL(aw*) → 0 and L(aw*) → 0 as the scaling constant a → +∞, only if W is in the linearly
separable region. In contrast, under the ReLU model, the asymptotic critical points can be either
global minimum or (spurious) local minimum depending on the training datasets, and hence the
convergence property of GD can be very different in nature from that under the linear model.
The following theorem characterizes the landscape properties of problem (P). Throughout, we de-
note the infimum of the objective function in problem (P) as L* = n--. Furthermore, We call a
direction W asymptotically critical if it satisfies VL(aw*)→0 as α → +∞.
3
Under review as a conference paper at ICLR 2019
Theorem 3.1 (Asymptotic landscape property). For problem (P) under the ReLU model, any corre-
Sponding asymptotic Critical direction w* fall into one ofthefollowing cases:
1.	(Asymptotic global minimum): yiW*lXi > 0 for all i ∈ I+ ∪ I-. Then,
L(αw* ) → L* as α → +∞.
2.	(Asymptotic local minimum): w*|xi > 0for all i ∈ J+ and w*| xi ≤ 0for all i ∈ (I+ \ J+ ) ∪
I-, where J+ ⊆ I+. Then,
L(aw*) → L + n+ —|J+| as α → +∞.
3.	(Local minimum): w*|xi ≤ 0for all i ∈ I+ ∪ I-. Then,
L(w*) = L* + n+.
To further elaborate Theorem 3.1, if w* classifies all data correctly (i.e., item 1), then the objective
function possibly achieves global minimum L* along this direction. On the other hand, if w* clas-
sifies some data with label +1 as -1 (item 2), then the objective function achieves a sub-optimal
value along this direction. In the worst case where all data samples are classified as -1 (item 3), the
ReLU unit is never activated and hence the corresponding objective function has constant value 1.
We note that the cases in items 2 and 3 may or may not take place depending on specific datasets,
but if they do occur, the corresponding w* are spurious (asymptotic) local minima. In summary, the
landscape under the ReLU model can be partitioned into different regions, where gradient descent
algorithms can have different implicit bias as we show next.
3.2	Convergence of GD
In this subsection, we analyze the convergence of GD in learning the ReLU model. At each iteration
t, GD performs the update
Wt+1 = Wt- NL(Wt),	(GD)
where η denotes the stepsize. For the linear model whose loss function has infinitely many asymp-
totic global minima, it has been shown in Soudry et al. (2017) that GD always converges to the
max-margin direction. Such a phenomenon is regarded as the implicit bias property of GD. Here,
for the ReLU model, we are also interested in analyzing whether such an implicit-bias property
still holds. Furthermore, since the loss function under the ReLU model possibly contains spurious
asymptotic local minima, the convergence of GD under the ReLU model should be very different
from that under the linear model.
Next, we introduce various notions of margin in order to characterize the implicit bias under the
ReLU model. The global max-margin direction of samples in I+ is defined as
wb+ = arg min max(w|xi).
kwk=1 i∈I+
Such a notion of max-margin is natural because the ReLU activation function can suppress negative
inputs. We note that here wb + may not locate in the linearly separable region, and hence it may not
be parallel to any (asymptotic) global minimum. As we show next, only when wb + is in the linearly
separable region, GD may converge in direction to such a max-margin direction under the ReLU
model. Furthermore, for each given subset J+ ⊆ I+, we define the associated local max-margin
direction wb J+ as
wb J+ = arg min max(w|xi).
kwk=1 i∈J+
We further denote the set of asymptotic local minima with respect to J+ ⊆ I+ (see Theorem 3.1
item 2) as
WJ+ := {w|xi > 0, ∀i ∈ J+ and w|xi ≤ 0, ∀i ∈ (I+ \ J+) ∪ I-}.
Of course, WJ+ may or may not be empty for a certain J+, and wbJ+ may or may not belong to WJ+
depending on the specific training dataset. As we show next, only when there exists a non-empty
WJ+ and the corresponding wb J+ ∈ WJ+ , GD may converge to such an asymptotic local minimum
wb J+ direction under the ReLU model. Next, we present the implicit bias of GD for learning the
ReLU model in problem (P).
4
Under review as a conference paper at ICLR 2019
Theorem 3.2. Apply GD to solve problem (P) with arbitrary initialization and a small enough
constant stepsize. Then, the sequence {wt}t generated by GD falls into one of the following cases.
1.	L(Wt) → L, and k kWwtk 一 W +k = O( Innnt), where W+ is in linearly separable region;
2.	the direction of wt does not converge and oscillates between linearly separable and misclassified
regions, where Wb + is not in linearly separable region;
3.	L(Wt) → L + n+-lJ +|, and ∣∣ ^^ — W+k = O(Innnt), where J+ = 0, and W+ ∈ W+;
4.	L(Wt) = L + n+, and Wt = WJ, where J+ = 0, i.e., GD terminates within finite steps.
Theorem 3.2 characterizes various instances of implicit bias of GD in learning the ReLU model,
which the nature of the convergence is different from that in learning the linear model. In spe-
cific, GD can either converge in direction to the global max-margin direction Wb + that leads to the
global minimum, or converge to the local max-margin direction Wb J+ that leads to a spurious local
minimum. Furthermore, it may occur that GD oscillates between the linearly separable region and
the misclassified region due to the suppression effect of ReLU function. In this case, GD does not
have an implicit bias property and convergence guarantee. We provide two simple examples in the
supplementary material to further elaborate these cases.
3.3 Implicit Bias of SGD in Learning ReLU Models
In this subsection, we analyze the convergence property and the implicit bias of SGD for solving
problem (P). At each iteration t, SGD samples an index ξt ∈ {1, . . . , n} uniformly at random with
replacement, and performs the update
Wt+1 = Wt — ηtV'(Wt, Zξt).	(SGD)
Similarly to the convergence of GD characterized in Theorem 3.2, SGD may oscillate between the
linearly separable and misclassified regions. Therefore, our major interest here is the implicit bias
of SGD when it does converge either to the asymptotic global minimum or local minimum. Thus,
without loss of generality, we implicitly assume that Wb + is in the linearly separable region, and the
relevant Wb J+ ∈ WJ+ . Otherwise, SGD does not even converge.
The implicit bias of SGD with replacement sampling has not been studied in the existing literature,
and the proof of the convergence and the characterization of the implicit bias requires substantial new
technical developments. In particular, traditional analysis of SGD under convex functions requires
the assumption that the variance of the gradient is bounded Bottou et al. (2016); Bach (2014); Bach
& Moulines (2013). Instead of making such an assumption, we next prove that SGD enjoys a nearly-
constant bound on the variance up to a logarithmic factor of t in learning the ReLU model.
Proposition 1 (Variance bound). Apply SGD to solve problem (P) with any initialization. If there
exists T such that for all t > T, Wt either stays in the linearly separable region, or in WJ+, then
with stepsize ηk = (k + 1)-α where 0.5 < α < 1, the variances of the stochastic gradients sampled
by SGD along the iteration path satisfy that for all t,
t-1
X η2Ekv'(Wk,zξk )k2
k=0
ln t
≤O T
Proposition 1 shows that the summation of the norms of the stochastic gradients grows logarithmi-
cally fast. This implies that the variance of the stochastic gradients is well-controlled. In particular, if
we choose ηk = (k+1)-1/2, then the bound in Proposition 1 implies that the term E∣V'(Wk,飞卜)∣2
stays at a constant level. Based on the variance bound in Proposition 1, we next establish the con-
vergence rate of SGD for learning the ReLU model. Throughout, we denote Wt := -t Pk=O Wk as
the averaged iterates generated by SGD.
Theorem 3.3 (Convergence rate of loss). Apply SGD to solve problem (P) with any initialization.
If there exist T such that for all t > T, Wt either stays in the linearly separable region, or in WJ+,
5
Under review as a conference paper at ICLR 2019
then with the stepsize ηk = (k + 1)-α, where 0.5 < α < 1, the averaged iterates generated by SGD
satisfies
EL(Wt) -L* ≤ O (.-α ) ,	IlEWtIl ≥ O(In t).
If there exist T such that for all t > T, wt stays in WJ+, then with the same stepsize
EL(Wt)-(L* +
「)≤O( t⅛ )
IIEwtk ≥ O(lnt).
Theorem 3.3 establishes the convergence rate of the expected risk of the averaged iterates generated
by SGD. It can be seen that the convergence of SGD achieves different loss values corresponding to
global and local minimum in different regions. The stepsize is set to be diminishing to compensate
the variance introduced by SGD. In particular, if a is chosen to be sufficiently close to 0.5, then the
convergence rate is nearly of the order O(ln2 t∕√t), which matches the standard result of SGD in
convex optimization up to an logarithmic order. Theorem 3.3 also implies that the convergence of
SGD is attained as ∣∣Ewtk → +∞ at a rate of O(ln t). We note that the analysis of Theorem 3.3 is
different from that of SGD in traditional convex optimization, which requires the global minimum
to be achieved at a bounded point and assumes the variance of the stochastic gradients is bounded
by a constant Shalev-Shwartz et al. (2009); Duchi & Singer (2009); Nemirovski et al. (2009). These
assumptions do not hold here.
Theorem 3.4 (Implicit bias of SGD). Apply SGD to solve problem (P) with any initialization. If
there exist T such that for all t > T, Wt either stays in the linearly separable region, or in WJ+,
then with the stepsize ηk = (k + 1)-α where 0.5 < α < 1, the sequence of the averaged iterate
{Wt}t generated by SGD satisfies
Ewt	ʌ +
-------w
∣EWtk
2=O(ɪ
ln t
If there exist T such that for all t > T, Wt stays in WJ+, then with the same stepsize
EWt	ʌ +
-------WT
∣EWtk J
2 = O (ɪ
ln t
Theorem 3.4 shows that the direction of the expected averaged iterate E[Wt] generated by SGD
converges to the max-margin direction Wb +, without any explicit regularizer in the objective function.
The proof of Theorem 3.4 requires a detailed analysis of the SGD update under the ReLU model
and is substantially different from that under the linear model Soudry et al. (2018); Ji & Telgarsky
(2018); Nacson et al. (2018a;b). In particular, we need to handle the variance of the stochastic
gradients introduced by SGD and exploit its classification properties under the ReLU model.
We next provide an example class of datasets (which has been studied in Combes et al. (2018)), for
which we show that SGD stays stably in the linearly separable region.
Proposition 2. If the linear separable samples {z1, . . . , zn} satisfy the following conditions given
in Combes et al. (2018):
1.	For all	(i, j) ∈	I+ ×	I+∪ I- × I-,	it holds that xi|xj	>	0;
2.	For all	(i, j) ∈	I+ ×	I- ∪ I- × I+,	it holds that xi|xj	<	0,
then there exists a t ∈ N such that for all t ≥ t the Sequence generated by SGD stays in the
linearly separable region, as long as SGD is not initialized at the local minima described in item 3
of Theorem 3.1.
4 Further Extensions and Discussions
4.1	Leaky ReLU Models
The leaky ReLU activation takes the form σ(v) = max(αv, v), where the parameter (0 ≤ α ≤ 1).
Clearly, leaky ReLU takes the linear and ReLU models as two special cases, respectively corre-
sponding to α = 0 and α = 1. Since the convergence of GD/SGD of the ReLU model is very
6
Under review as a conference paper at ICLR 2019
different from that of the linear model, a natural question to ask is whether leaky ReLU with inter-
mediate parameters 0 < α < 1 takes the same behavior as the linear or ReLU model.
It can be shown that the loss function in problem (P) under the leaky ReLU model has only asymp-
totic global minima achieved by W in the separable region with infinite norm (there does not exist
asymptotic local minima). Hence, the convergence of GD is similar to that under the linear model,
where the only difference is that the max-margin classifier needs to be defined based on leaky ReLU
as follows.
For the given set of linearly separable data samples, We construct a new set of data z* = (x*,y*),
in which x* = Xi, ∀i ∈ I+, x* = αxi, ∀i ∈ I-, and y* = yi, ∀i ∈ I+ ∪ I-. Essentially, the
data samples with label -1 are scaled by the parameter α of leaky ReLU. Without loss of generality,
we assume that the max-margin classifier for data {xi*} passes through the origin after a proper
translation. Then, we define the max-margin direction of data X* as
wb * = arg min max (yi*w|xi* ).
kwk=1 i∈I+∪I-
Then, following the result under the linear model in Soudry et al. (2017), it can be shown that
GD with arbitrary initialization and small constant stepsize for solving problem (P) under the leaky
ReLU model satisfies that L(w) converges to zero, and w converges to the max-margin direction,
i.e., limt→∞ k^tk = W*, with its norm going to infinity.
Furthermore, following our result of Theorem 3.4, it can be shown that for SGD applied to solve
problem (P) with any initialization, if there exists T such that for all t > T wt stays in the linearly
separable region, then with the stepsize ηk = (k + 1)-α, 0.5 < α < 1, the sequence of the averaged
iterate {Wt}t generated by SGD satisfies
Ewt
Pwl
Thus, for SGD under the leaky ReLU model, the normalized average of the parameter vector con-
verges in direction to the max-margin classifier.
4.2	Multi-neuron Networks
In this subsection, we extend our study of the ReLU model to the problem of training a one-hidden-
layer ReLU neural network with K hidden neurons for binary classification. Here, we do not assume
linear separability of the dataset. The output of the network is given by
K
f (x) = X Vk b(w|x) = vlσ(W>x),	(2)
k=1
where W = [ wι, w2, •一，WK] with each column Wk representing the weights of the kth neuron
in the hidden layer, v| = [ v1,v2,…,VK] denotes the weights of the output neuron, and σ(∙)
represents the entry-wise ReLU activation function. We assume that v is a fixed vector whose
entries are nonzero and have both positive and negative values. Such an assumption is natural as it
allows the model to have enough capacity to achieve zero loss. The predicted label is set to be the
sign of f (x), and the objective function under the exponential loss is given by
1n
L(W)=反 EeXp(-yif(xi)).	⑶
n i=1
Our goal is to characterize the implicit bias ofGD and SGD for learning the weight parameters W of
the multi-neuron model. In general, such a problem is challenging, as we have shown that GD may
not converge to a desirable classifier even under the single-neuron ReLU model. For this reason, we
adopt the same setting as that in (Soudry et al., 2017, Corollary 8), which assumes that the activated
neurons do not change their activation status and the training error converges to zero after a sufficient
number of iterations, but our result presented below characterizes the implicit bias of GD and SGD
in the original feature space, which is different from that in (Soudry et al., 2017, Corollary 8). We
define a set of vectors {Ai ∈ Rk×1}in=1, where Aij = 1 if the sample xi is activated on the jth
7
Under review as a conference paper at ICLR 2019
neuron, i.e., wj|xi > 0, and set Aij = 0 otherwise. Such an Ai vector is referred to as the activation
pattern of Xi. We then partition the set of all training samples into m subsets Bi, B2,…，Bm, so that
the samples in the same subset have the same ReLU activation pattern, and the samples in different
subsets have different ReLU activation patterns. We call Bh, h ∈ [m] as the h-th pattern partition.
Let Wh = Pk∈{j.Aj =1} VkWk. Then, for any sample X ∈ Bh, the output of the network is given by
K
f(X) = Evkσ(WIX)= E	vkw|x = W|x.
k=i	k∈{j: Ah = 1}
We next present our characterization of the implicit bias property of GD and SGD under the above
ReLU network model. We define the corresponding max-margin direction of the samples in Bh as
Wbh = arg min max (W|X).
kwk=1 x∈Bh
Then the following theorem characterizes the implicit bias of GD under the multi-neuron network.
Theorem 4.1. Suppose that GD optimizes the loss L(W) in eq. (3) to zero and there exists T
such that for all t > T, the neurons in the hidden layer do not change their activation status. If
Ahi ∧ Ah2 = 0 (where ”∧” denotes the entry-wise logic operator “AND” between digits zero or
one) for any h1 6= h2, then the samples in the same pattern partition of the ReLU activation have
the same label, and
〜
W
kWeht k
- Wb h
O( *),
lnt
for all h ∈ [m] .
t
h
Differently from (Soudry et al., 2017, Corollary 8) which studies the convergence of the vectorized
weight matrix so that the implicit bias of GD is with respect to features being lifted to an extended
dimensional space, Theorem 4.1 characterizes the convergence of the weight parameters and the
implicit bias in the original feature space. In particular, Theorem 4.1 implies that although the ReLU
neural network is a nonlinear classifier, f(X) is equivalent to a ReLU classifier for the samples in
the same pattern partition (that are from the same class), which converges in direction to the max-
margin classifier Wh of those data samples. We next let Wt := 1 Pk=O Wh(t). Then the following
theorem establishes the implicit bias of SGD.
Theorem 4.2. Suppose that SGD optimizes the loss L(W) in eq. (3) so that there exists T such that
for any t > T, L(W) < 1/n, the neurons in the hidden layer do not change their activation status,
and for any h1 6= h2, Ah1 ∧ Ah2 = 0. Then, for the stepsize ηk = (k + 1)-α, 0.5 < α < 1, the
samples in the same pattern partition of the ReLU activation have the same label, and
Il _EWL _
U kEW hk
for all h ∈ [m] .
Similarly to GD, the averaged SGD in expectation maximizes the margin for every sample partition.
At the high level, Theorem 4.1 and Theorem 4.2 imply the following generalization performance of
the ReLU network under study. After a sufficiently large number of iterations, the neural network
partitions the data samples into different subsets, and for each subset, the distance from the samples
to the decision boundary is maximized by GD and SGD. Thus, the learned classifier is robust to
small perturbations of the data, resulting in good generalization performance.
5 Conclusion
In this paper, we study the problem of learning a ReLU neural network via gradient descent methods,
and establish the corresponding risk and parameter convergence under the exponential loss function.
In particular, we show that due to the possible existence of spurious asymptotic local minima, GD
and SGD can converge either to the global or local max-margin direction, which in the nature of
convergence is very different from that under the linear model in the previous studies. We also
discuss the extensions of our analysis to the more general leaky ReLU model and multi-neuron
networks. In the future, it is worthy to explore the implicit bias of GD and SGD in learning multi-
layer neural network models and under more general (not necessarily linearly separable) datasets.
8
Under review as a conference paper at ICLR 2019
References
Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con-
vergence rate O(1/n). In Proc. Advances in neural information processing systems (NIPS), 2013.
Francis R Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for
logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.
Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and
examples. Springer Science & Business Media, 2010.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. arXiv preprint arXiv:1606.04838, 2016.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Remi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, and Yoshua
Bengio. On the learning dynamics of deep neural networks. arXiv preprint arXiv:1809.06848,
2018.
John Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899-2934, 2009.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Sre-
bro. Implicit regularization in matrix factorization. In Proc. Advances in Neural Information
Processing Systems (NIPS), 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. ArXiv:1802.08246, 2018.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression.
ArXiv:1803.07300, 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Convergence
of gradient descent on separable data. ArXiv:1803.01905, 2018a.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. arXiv preprint arXiv:1806.01796, 2018b.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-
1609, 2009.
Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and
method efficiency in optimization. Wiley, 1983.
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-
mization. In Proc. Conference on Learning Theory (COLT), 2009.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. ArXiv:1710.10345, 2017.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In Proc. International Conference on Learning Representations (ICLR), 2018.
Matus Telgarsky. Margins, shrinkage, and boosting. In Proc. International Conference on Machine
Learning (ICML), 2013.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11:2543-2596, 2010.
9
Under review as a conference paper at ICLR 2019
Supplementary Materials
A Proof of Theorem 3.1
The gradient VL(w) is given by
1n	1n
VL(W) = — £V'(w, zi) = — - Eyi1{w∣χi>o} exp(-yiwlxi)xi.
ni=1 ni=1
If yiW*\χi ≥ 0 for all Xi ∈ I + ∪ I-, then as a → +∞, We have,,
L(αw*) =1X eχp(O)=彳=Lt,
i∈I-
and
VL(aw*) =----e^p exp(一awlXi)xi = 0.
i∈I+
Recall that J+ ⊆ I+. If w*lXi ≤ 0 for all i ∈ (I+ \ J+) ∪ I-, then as α → +∞, we obtain
L(αw*) = 1 X exp(0) = n+-1J+1 + n = L* + n+ -1J +l.
n	nn
i∈(I+∖J+)∪I-
and
VL(ɑw*) =----exp(-αwlXi)xi = 0.
i∈J+
If w*|xi ≤ 0 for all i ∈ I+ ∪ I-, then
1 n	n+
L(w ) = 一 y^exp(0) = 1 = L +	, VL(w ) = 0.
i=1
The proof is now complete.
B Proof of Theorem 3.2
First consider the case when wb + is in linearly separable region and the local minimum does not
exist along the updating path. We call the region where all vectors w ∈ Rd satisfy w|xi < 0
for all i ∈ I- as negative correctly classified region. As shown in Soudry et al. (2017), L(w) is
non-negative and L-smooth, which implies that
L(wk+ι) ≤ L(Wk) + VL(Wk)l(wk+ι — Wk) + IL∣∣wk+ι — wk『
=L(Wk) - η∣VL(wk)k2 + Lη2∣VL(wk)k2
=L(wk) — η(1 — Ln )∣VL(wk )k2.
Based on the above inequality, we have
L(Wk L-LLWk+1) ≥ ∣VL(wk )∣2,
n(1 - 胃)
which, in conjunction with 0 < η < 2/L, implies that
tt
X ∣VL(wk)∣2 ≤ X
k=0	k=0
L(Wk) - L(wk+ι)
n(i- 粤)―
L(wo) - L(wt+ι)
n(i- 粤)
Thus, we have ∣VL(wk)∣2 → 0 as k → +∞. By Theorem 3.1, ∣VL(wk)∣ vanishes only when
all samples with label -1 are correctly classified, and thus GD enters into the negative correctly
10
Under review as a conference paper at ICLR 2019
classified region eventually and diverges to infinity. Soudry et al. (2017) Theorem 3 shows that when
GD diverges to infinity, it simultaneously converges in the direction of the max-margin classifier of
all samples satisfying wt|xi > 0. Thus, under our setting, GD either converges in the direction of
the global max-margin classifer wb + :
2W- - w + = o(皿)
kwt k	I lnt 卜
or the local max-margin classifier wbJ+ :
wt +	lnlnt
由-W J =O( Inr).
Next, consider the case when wb+ is not in linearly separable region, and the local minimum does
not exist along the updating path. In such a case, we conclude that GD cannot stay in the linearly
separable region. Otherwise, it converges in the direction of wb+ that is not in linearly separable
region, which leads to a contradiction. If the asymptotic local minimum wbJ+ exists, then GD may
converge in its direction. If wbJ+ does not exist, GD cannot stay in both the misclassified region and
linearly separable region, and thus oscillates between these two regions.
In the case when GD reaches a local minimum, by Theorem 3.1, We have VL(w*) = 0, and thus
GD stops immediately and does not diverges to infinity.
C Examples of Convergence of GD in ReLU model
Example 1 (Figure 1, left). The dataset consists of two samples with label +1 and one sample with
label -1. These samples satisfy x1| x3 < 0 and x1|x2 < 0.
For this example, if we initialize GD at the green classifier, then GD converges to the max-margin
direction of the sample (x1, +1). Clearly, such a classifier misclassifies the data sample (x2, +1).
Example 2 (Figure 1, right). The dataset consists of one sample with label +1 and one sample with
label -1. These two samples satisfy 0 < x1|x2 ≤ 0.5kx2k2.
For this example, if we initialize at the green classifier, then GD oscillates around the direction
x2/kx2k and does not converge.
Figure 1: Failure of GD in learning ReLU models
Proof of Example 1
Consider the first iteration. Note that the sample z3 has label -1, and from the illustration of Figure 1
(left) we have w0|x3 < 0, w0|x2 < 0 and w0|x1 > 0. Therefore, only the sample z1 contributes to
the gradient, which is given by
Vw0L(w0) = - exp(-w0|x1)x1 .	(4)
11
Under review as a conference paper at ICLR 2019
By the update rule of GD, we obtain that for all t
wt+1 = Wt + η exp(-w|xi)xi.	(5)
By telescoping eq. (5), it is clear that any wt|x2 < 0 for all t since x1|x2 < 0. This implies that the
sample z2 is always misclassified.
Proof of Example 2
Since we initialize GD at w0 such that w0|x1 > 0 and w0|x2 < 0, the sample z2 does not
contribute to the GD update due to the ReLU activation. Next, we argue that there must ex-
ists a t such that wt|x2 > 0. Suppose such t does not exist, we always have wt|x1 = (w0 +
Ptk-=10 exp(-wk|x1)x1)|x1 > 0. Then, the linear classifier wt generated by GD stays between x1
and x2 , and the corresponding objective function reduces to a linear model that depends on the sam-
ple z1 (Note that z2 contributes a constant due to ReLU activation). Following from the results in
Ji & Telgarsky (2018); Soudry et al. (2017) for linear model, we conclude that wt converges to the
max-margin direction kxj as t → +∞. Since x∣x2 > 0, this implies that w|x2 > 0 as t → +∞,
contradicting with the assumption.
Next, we consider the t such that wt|x1 > 0 and wt|x2 > 0, the objective function is given by
L(wt) = exp(-wt|x1) + exp(wt|x2),
and the corresponding gradient is given by
Rwt L(Wt) = - exp(-w|xi)xi +exp(w|X2)X2.
Next, we consider the case that wt|x1 > 0 for all t. Otherwise, both ofx1 and x2 are on the negative
side of the classifier and GD cannot make any progress as the corresponding gradient is zero. In the
case that wt|x1 > 0 for all t, by the update rule of GD, we obtain that
w|+iX2 - w|X2 = ηexp(-w|xi)x|x2 - ηexp(w|x2)kx2『≤ -0.5ηkx2∣∣2.	(6)
Clearly, the sequence {wt>x2}t is strictly decreasing with a constant gap, and hence within finite
steps we must have wt|x2 ≤ 0.
D Proof of Proposition 1
Since SGD stays in the linearly separable region eventually, and hence only the data samples in I+
contribute to the gradient update due to the ReLU activation function. For this reason, we reduce
the original minimization problem (P) to the following optimization
min L(w)
w∈Rd
1n
n+ E'(w, xi),
i=1
'(w, Xi) = exp(-wlXi)1(Xi∈ι+),
(7)
which corresponds to a linear model with samples in I+ . Similarly, if SGD stays in WJ+, only
the data samples in J+ contribute the the gradient update, the original minimization problem (P) is
reduced to
1n
w∈iRd L(W) =4二 '(w, Xi),
'(w, Xi) = exp(-wlXi)1(χi∈j +),	(8)
The proof contains three main steps.
Step 1: For any u, bounding the term Ekwt - uk2: By the update rule of SGD, we have
l∣wt - U- = ∣∣wt-i - U- - 2ηt-ihV'(wt-i,Zξ) wt-1 - Ui + η2-ikV'(wt-i, Zξt )k2
=∣∣wt-ι - u∣2 - 2ηt-ιhVL(wt-ι), wt-1 - Ui + η2-ι∣V'(wt-ι, Zξt)∣2 + Mt,
(9)
12
Under review as a conference paper at ICLR 2019
where
Mt = 2ηt-ιhVL(wt-ι) - V'(wt-ι, z&), wt-i - u).
By convexity We obtain that <VL(wt-ι), wt-i - u) ≥ L(Wt-I) — L(u). Then, eq. (9) further
becomes
IlWt- uk2 ≤ IlWt-I- uk2 - 2ηt-ι(L(wt-ι) - L(U)) + η2-ι∣∣V'(wt-ι,ZξJk2 + Mt (10)
Telescoping the above inequality yields that
t-i	t-i	t-i	t
kwt-uk2 ≤ Ilwo-uk2 -2XηkL(Wk) + 2(Xηk)L(U) + Xηkkv'(Wk,zξk)k2 + XMk.
k=0	k=0	k=0	k=i (11)
Taking expectation on both sides of the above inequality and note that EMt = 0 for all t, We further
obtain that
t-i	t-i	t-i
EkWt- uk2 ≤ kwo- uk2 - 2XηkEL(Wk) + 2(Xη)L(U) + XηkEkv'(Wk,zξk)k2.
k=0	k=0	k=0
(12)
Note that ` ≤ 1 Whenever the data samples are correctly classified and for all i ∈ I+, Ixi I ≤ B,
and without loss of generality, we can assume B < √2. Hence, the term E∣V'(wk,飞飞)∣2 can be
upper bounded by
E∣V'(wk,zξk)∣2 = E'(wk,zξk)kz2kk ≤ B2E'(wk,Zξk)2 ≤ B2E'(wk,zg%) = B2EL(wk).
Then, noting that ηk ≤ 1, eq. (12) can be upper bounded by
t-i	t-i
EIwt - UI2 ≤ Iw0 - UI2 - (2 - B2) XηkEL(wk) + 2(X ηk)L(U).	(13)
k=0	k=0
Next, set U = (ln(t)/Y)W + and note that W>xii ≥ Y for all i ∈ I+, we conclude that L(u)=
(1∕n+) P exp(-u>xi) ≤ t. Substituting this into the above inequality and noting that ηk =
i∈I+
(k + 1)-α and 0.5 < α < 1, we further obtain that
ln2 t	t-i	1	t-i
EkWt- u∣2 ≤ ∣∣wok2 +----2 + 2(Xηk)- - (2 - B2) XηkEL(Wk)
Y	k=0	k=0
ln2 -	2	t-i
≤ kwok +----2-^+ ∩------∖~t - - (2 - B ) ^XηkEL(Wk).	(14)
Y2	(1 - α)	k=0
Step 2: lower bounding EIwt - UI2 : Note that only the samples in I+ contribute to the update
rule. By the update rule of SGD, we obtain that
t-i	t-i
wt = wo - EnkV'(wk,zξk) = wo + Enk'(wk,zξk)χξk,
k=0	k=0
which further implies that
t-i	t-i
w>w + = w>w + + Enk '(wk ,zξk )χ>s W + ≥ w>w + + YEnk '(wk,zξk).
k=0	k=0
Then, we can lower bound Iwt - UI as
IlWt — Uk ≥ hwt — u, W +) ≥ w>W + — In") W>W+
t	Y+
>	t-i	ln(-)
≥ wo W + + Yrnk '(wk, zξk)-------------.
k=0	Y
13
Under review as a conference paper at ICLR 2019
Taking the expectation of kwt - uk2 :
Ekwt - uk2
t-1
≥ E (w>w + + Y X ηk'(Wk,zξk) -
k=0
(i)
≥
t-1
(w>w + + γ X ηk E'(wk ,zξk) -
k=0
t-1
W + + γ X ηkEL(Wk) -
k=0
where (i) follows from Jensen’s inequality.
t-1
Step 3: Upper bounding P ηkEL(Wk): Combining the upper bound obtained in step 1 and the
k=0
lower bound obtained in step 2 yields that
(w>W + + γXηkEL(Wk) — ln(t)) ≤ ∣∣wok2 + lnγ2t + 1-2αt-α
Solving the above quadratic inequality yields that
t-1	ln t
^X ηk EL(Wk ) ≤ O( -2 ).	(15)
k=0	γ
E	Proof of Theorem 3.3
The proof exploits the iteration properties of SGD and the bound on the variance of SGD established
in Proposition 1.
We start the proof from eq. (10), following which we obtain
L(Wt-I) ≤ 2η1^^kwtτ - uk2 - kwt - uk2) + L(U) + 1 nt-ikv`(Wt-ι,zξt)k2 + 2η1-^Mt∙
t-	t- (16)
Taking the expectation on both sides of the above inequality yields that
EL(Wt-1) ≤ —^(E∣Wt-ι — u∣2 — EkWt — u∣2) + L(U) + 1 ηt-1E∣N'(Wt-1,Zξt)k2,
2ηt-1	2
which, after telescoping, further yields that
t-1	1 t-1 1	1 t-1
X EL(Wk ) ≤ tL(U) + 2 X 丁 (EkWk- uk2 - EkWk+1 - uk2) + 2 Xηk Ek▽'(Wk ,Zξk )k2.
k=0	k=0 ηk	k=0
(17)
14
Under review as a conference paper at ICLR 2019
t-1	t-1
By convexity of L in the linearly separable region, We have L (t P Wk) ≤ t P L(Wk), which,
k=0	k=0
in conjunction with eq. (17), yields that
EL
1 t-1 1	B 2 t-1
≤ L(U) + 2； £ — (EkWk - uk2 - EIlWk+ι - UlF) + — EnkEL(Wk)
2t k=0 ηk	2t k=0
1 t-1
L(u) + 2t∑kα(EkWk - uk2
k=0
B2 t-1
- EllWk+ 1 - uk2 ) + ~2t Enk EL(Wk )
1 t-1	B 2 t-1
L(u) + 2t ∑[(k +1)α - kα] EkWk- uk2 - taE∣Wt - u∣2 + 五 EnkEL(Wk)
k=0	k=0
1 t-1 (k + 1)2α k2α	B2 t-1
≤ L(U) + 2t X (k+1)α + kɑ EkWk-Uk + W X nkEL(Wk)
k=0	k=0
≤ L(U) + 2t IIw0 - uk2 + 2t X ——：：)------EkWk - uk2 + -tr X nkEL(Wk)
2t	2t	2k	2t
k=1	k=0
(i)	1	2	α4α-1	2	ln2 t 2	1 α	t-1	1	B2	t-1
≤L(U)+	2t kw0 -	Uk	+ ~~t- (kwok	+	~γτ +1-αt	)∑	kɪ-a	+	^2t∑Snk EL(Wk)
1	2	α4α-1	2 ln t 2	1 α tα	B 2
≤ L(U)	+	2t kw0	- Uk +-1(kW°k	+	2^	+ 1 - ɑt	) α +	"2t	Σnk EL(Wk )
γ	k=0
1	1	2	ln2 t	4α-1	2	ln2 t	2 α B2 t-1
≤ t + 2t(kw0k + T) + tɪ-a(kw0k + T + 1=展t	) 十 方 EnkEL(Wk)
γ	γ - α	k=0
=O(ln2(t)∕t1-α)
where ⑴ follows from the fact that EkWk - Uk2≤kwok2 + lnγ2t + ɪ-ɑt-1-α for k ≤ t
Thus, we can see that L(Wt) decreases to 0 ata rate of O(ln2 (t)∕t1-α). If we choose α to be close
to 0.5, the best convergence rate that can be achieved is O(ln2(t)/ʌ/t).
F Proof of Theorem 3.4
F.1 Main Proof of Theorem 3.4
We first present four technical lemmas that are useful for the proof of the main theorem.
Lemma F.1. Given the stepsize nk+1 = 1∕(k + 1)-α and the initialization W0s, then fort ≥ 1, we
have
IIEWtk ≥ - B ln(;+2t (iMosii2+12")
4α-1	ln2 t	2	B2 t-1
+ tɪ-ɑ(kW°sk + ~γr + ɪ-ɑt	) + "2t XnkEL(Wk))	(18)
Lemma F.2. Let X+ represent the data matrix of all samples with the label +1, with each row
representing one sample. Then we have:
min kX+T qk ≥ max min(X+W)i = γ.
q∈∆n-i	∣∣wk = 1 i
15
Under review as a conference paper at ICLR 2019
△n-1 is the simplex in Rn. Ifthe equality holds (i.e., the strong duality holds) at q and W+, then
they satisfy
W+ = 1 X+q,
γ
and W+ is the max-margin classifier of samples with the label +1.
LemmaF.3. Let Yp = ∣∣EVL(wp)k∕EL(wp) and ^^p+ι = ηp+ιEL(wp) for k ≥ 1. Then we have
k-1	1 k-1
lnEL(Wk) ≤ lnL(W0)— Xηp+ιγpγ + 2B4SX η2+ι.
p=0	p=0
Lemma F.4. For 0 ≤ k ≤ t — 1, we have
Eh—Wk, W+ i ≤ 1(lnE(L(Wk)) + lnn+).
γ
We next apply the lemmas to prove the main theorem. Taking the expectation of the SGD update
rule yields that
EWk = EWk-1 — ηk-1EVL(Wk-1).
Applying the above equation recursively, we further obtain that
k-1
EWk = W0 —	ηpEVL(Wp),
p=0
which further leads to
t-1	t-1 k-1	t-2
X∣EWk∣ ≤ t∣W0∣ +XXηp∣EVL(Wp)∣ = t∣W0∣ + X(t — 1 — p)ηp∣EVL(Wp)∣. (19)
k=0	k=1 p=0	p=0
Next, We prove the convergence of the direction of E[Wt] to the max-margin direction as follows.
1«≡⅛-W+『
1—
t-1
/K_ y∖	P Eh-Wk, W +i
hEWt, W+ i = ι + k=o
IlEWtk	t∣∣EWt∣∣
(i) ι	X lnE(L(Wk)) + lnn+
≤ 十 k=0 —YtpWTk
(ii)	ln n+ + ln L(w0)
≤	+	TpWtk
t-1
+X
k=1
k-1	k-1
-P ηp+iYpY + 2B4S P ηp+ι
p=0	p=0
YtkEWtk
k-1
t-1 Σ ηp+ιYp
1 — X P=0________
k=1 tkEWtk
+
k-1
ln n+ + ln L(w0)	1 B4S X P=O ηp+1
-YkEWTk — + 2	占 γtkw
t-1	t-2	t-2
IlP EWkIl- P (t — 1 — ρ)ηp+ιkEVL(Wp)k	∣	+	、1	P (t-1-ρ)ηp+ι
11 k=o 11 * * * * * * * p=o	+ ln n+ +ln L(w0 )+1 B4S p=o
tkEWtk	YkEWtk	2	YtkEWtk
t-1	t-2	t-2
P kEWkk— P (t — 1 — p)ηp+1kEVL(Wp)k	+	P ηp2+1
k=o	p=o	ln n+ + ln L(wo)	1 ^4^ p=o
≤	班两	+ 一乖而—+ 2	YkEWtk
Ciii) kwok -μ lnn+ +ln L(wo)	1	Β4 S1 - t1-2α
≤ PWI + —亚而一+ 4(α - 0.5)	YkEWtk ,
16
Under review as a conference paper at ICLR 2019
where (i) follows from Lemma F.4, (ii) follows from Lemma F.3 and (iii) is due to eq. (19). Since
following from Lemma F.1 We have that kEWtk = O(ln(t)),the above inequality then implies that
EWt
kEWtk
-W +∣∣2 = o
F.2 Proof of Technical Lemmas
Proof of Lemma F.1. Since kxik ≤ B for all i, we obtain that
exp(-EWTXi) ≥ exp(-∣∣EWtkkxik) ≥ exp(-B∣∣EWt∣∣),
L(EWt) = 1- X exp(-Ew>Xi) ≥ exp(-B∣∣EWtk).
n+
i∈I+
By convexity, we have that EL(Wt) ≥ L(Ewt), combining which with the above bounds further
yields
IlEwtk ≥ -B In(L(Ewt)) ≥ -B In(EL(Wt)) ≥ -B ln (tι-α).
That is, the increasing rate of Ekwtk is at least O(ln(t)).	□
Proof of Lemma F.2. Following from the definition of the max-margin, we have
γ = max min(X+ W)i = max min(X+W)i
kwk=1 i	kwk≤1 i
= max(- max(-X+W)i - 1kwk≤1)
wi
=max(-f*(-X+w) - g*(w))
w
where f * (a) = max(a)i and g*(b) = 1∣∣b∣∣≤ι, and their conjugate functions are f (c) = 1c∈∆n-1
and g(e) = kek, respectively, where a, b, c, d are generic vectors. We also denote ∂f(c) and ∂g(e)
the subgradient set of f and g at e and c respectively. By the Fenchel-Rockafellar duality Borwein
& Lewis (2010), we obtain that
γ = max -f *(-X+W) - g(W) ≤ min f(q) + g(X+> q) = min kX+>qk.
w	q	q∈∆n-1
In particular, the strong duality holds at q and W+ if and only if -X+w ∈ ∂f (q) and W+ ∈
∂g(X+>q). Thus, we conclude that W+ = ∂g(X+>q) = ^XX+>q = YX+>q.	□
Proof of Lemma F.3. By Taylor’s expansion and the update of SGD, we obtain that
L(Wk)
=L(Wk-I)- ηk▽£(Wk-I)TV'(wk-1, zξk) + ；η2V'(wk-1,。)T矿L(W)▽'(Wk-1, zξk),
(20)
where We = θWk-1 + (1 - θ)Wk for certain 0 ≤ θ ≤1, and is in the linear separable region. Note
that for any v,
VTV2L(W)v = ɪ X exp(-W>Xi)v>XiX>v ≤ ɪ X exp(-W>~)|乂『何『
n+	n+
i∈I+	i∈I+
≤ kvk2B2-+ X exp(-WTXi) = kvk2B2L(W) ≤ ∣∣vk2B2S,
n+
i∈I+
where S is the maximum of L(W) in the linearly separable region. We note that S < +∞ because
kWk → ∞ in the linearly separable region and hence L(W) → 0. Taking the expectation on both
sides of eq. (20) and recalling that
E∣∣V'(wk-ι,Xik)k2 ≤ B2EL(Wk-1),
17
Under review as a conference paper at ICLR 2019
we obtain that
EL(Wk) ≤ EL(Wk-I)- ηkEkVL(Wk-ι)k2 + 1 η2B2SEkV'(Wk-ι, Zξk)k2
<Er(w √1 JlEkVL(Wk-ι)k2 + 1 „2 B2 SEkV'(Wk-1,zξk )k2
≤ EL(Wk-1)(1-ηk	EL(Wk-1)	+2ηkB S	EL(Wk-1)
EkV'(Wk-1, Xik )k2
≤ EL(Wk-1)(1-ηk EL(Wk-I) k≡-f + 1 η2 B2S
≤ EL(Wk-1)(1 — ηkEL(Wk-ι)γ2-ι + 2ηkB4S).
EL(wk-1)
Define ^ = ηkEL(Wk-1). Then，applying the above bound recursively yields that
k-1
EL(Wk) ≤ L(Wo) Y (1 - ηp+1γp + 2η2+1Β4S)	(21)
p=0
k-1	1
≤ L(WO) Y eχp(-ηp+1Yp + 2ηp+1B4S).	(22)
p=0
Denote X ∈ Rn×d as the data matrix with each row corresponding to one data sample. The
derivative of the empirical risk can be written as VL(W) = XT l(W)/n, where l(W) =
['(w,Z1),'(W, Z2),...,'(W,Zn)]. Then, we obtain that
EL(Wp) = n+ X E eχp(-w> Xi) = n+ kE(i(Wp))k1
i∈I+
and
EVL(wp) = — X Eexp(-w>Xi)xi = — X+E(l(wp)).
i∈I+
Based on the above relationships and Lemma F.2, we obtain that
=kEVL(wp)k = kX+E(l(wp))k	E(l(wp)) k ≥
Yp= EL(Wp)	= kE(l(wp))k1 = k kE(l(wp))k1 k≥Y,
Taking logarithm on both sides of eq. (22) and utilizing the above facts, we further obtain that
k-1	1 k-1
lnEL(Wk) ≤ lnL(WO)- Xηp+1γp + 2B4SX ηp+1,
p=0	p=0
k-1	1 k-1
≤ln L(WO)- X ηp+1γpγ + 2 B4S X ηp+1.
p=O	p=O
□
ProofofLemmaF.4. Define h(y) = ln (n+ P exρ(yi)), and then its dual function h*(q) =
ln n+ + qi ln(qi) ≤ ln n+. Following from Lemma F.2, W+ = Y X+T q. Then, by the Fenchel-
Young inequality, we obtain that
Eh-Wk, W+i
Y h-Ewk, x+>qi=Y h-x+Ewk, qi
≤ 1(h(-X+EWk) + h*(q)) ≤ 1(ln(L(EWk)) + lnn+)
YY
≤ ɪ(lnE(L(wk)) + lnn+).
Y
□
18
Under review as a conference paper at ICLR 2019
G	Proof of Proposition 2
Under our ReLU model, in the linearly separable region, the gradient VL(w) is given by
1n	1
VL(w) = — —Eyi1{w∣χi>o} exp(-yiw|Xi)Xi = — — E exp(-w|xi)x〃
n i=1	n i∈I+
Thus, only samples with positive classification output, i.e. σ(w∣XξJ > 0, contribute to the SGD
updates.
We first prove kwt k < +∞ when there exist misclassified samples. Suppose, toward contradiction,
that kwt k = +∞ as t → +∞ when misclassified samples exist. Note that
n
wt = w0 + η	αiyiXi	(23)
i=0
Since ∣∣wtk is infinite, at least one of the coefficients ai,i = 1, ∙∙∙ , — is infinite. No loss of gener-
ality, we assume αp = +∞. Then, the inner product
n
w| Xj = w| Xj + ηfαiyiX~i Xj + αpypX^xj.	(24)
i=0
i6=p
Based on the data selected in Proposition 2, we obtain for ∀i ∈ I- ∪ I+
∀j ∈ I+ ,	yiXi|Xj > 0
∀j ∈ I-,	yiXi|Xj < 0,
which, in conjunction with eq. (24), implies that, if there exist j ∈ I+, then the first term in the right
side of eq. (24) is finite, the second term is positive, and the third term is positive and infinite. As
a result, we conclude that for ∀j ∈ I+, wtXj > 0 as t → +∞. Similarly, we can prove that for
∀j ∈ I-, wtXj ≤ 0 as t → +∞, which contracts that wtXj > 0 . Thus, if there exist misclassified
samples, then we have ∣wt ∣ < +∞.
Based on the update rule of SGD, we have, for any j
w|+i Xj- wXj = η eχp(-yξt w|Xa)y&X|t Xj = △&，•	(25)
It can be shown that
Vj ∈ I- ∪ I +,yξt4ξt,j > 0,
which, combined with eq. (25), implies that, if one sample is correctly classified at iteration t, it
remains to be correctly classified in the following iterations. Next, we prove that when ∣wt ∣ < +∞,
all samples are correctly classified within finite steps. Define
++ = min	|Xi|1Xi2 |;
i1∈I+,i2∈I+
-- = min	|Xi|1 Xi2 |;
i1∈I-,i2∈I-	1
+- = min	|Xi| Xi2 |.
i1∈I+,i2∈I- i1 i2
Since ∣wt∣ < ∞, there exists a constant C such that ∣wt∣ < C for all t. Let D = maxi∈I+ ∣Xi∣.
Then, we obtain, for any j ∈ I + and ξt ∈ I + ,
4ξt,j = ηexp(一w|X&)X|tXj ≥ ηexp(-CD)e++,
and for any j ∈ I + and ξt ∈ I - ,
4ξt,j = -ηeχp(W|X&)X|tXj ≥ ηe+-.
Combining the above two inequalities ∀j ∈ I + yields
4ξt,j ≥ ηmin{exp(-CD)e++,ηe+"-}.	(26)
19
Under review as a conference paper at ICLR 2019
Similarly, we can prove ∀j ∈ I-
4ξt,j ≤ -η min {exp(-CD)+-, η--}.	(27)
Combining eq. (25), eq. (26) and eq. (27), we have, when GD is in the misclassified region, the inner
product wt|xi increases at least ηmin {exp(-CD)++, η+-} after each iteratio for ∀xi ∈ I+ or
decreases at least η min {exp(-CD)++, η+-} after each iteration for ∀xi ∈ I-. Thus, for a
sufficiently large t, we have
∀i ∈ I+ ,	wt|xi > 0
∀i ∈ I- , wt|xi < 0,
which shows that SGD enters into linearly separable eventually. Recall that once a sample is cor-
rectly classified, it remains to be correctly classified in the following iterations. As a result, there
exists t ∈ N such that the SGD stays in linearly separable region for all t ≥ t.
H Proof of Theorem 4.1
After T GD iterations, we randomly pick a Ar from {Ai}. Without loss of generality, we assume
that only the first Kr neurons are activated for all xi ∈ Br, and suppose there are nr samples in Br .
We first use contradiction to show that all elements in the set Vr = {vι, v2,…，VKr} must be either
all positive or all negative.
According to the update rule of GD, we have, for any 1 ≤ K1 < K2 ≤ Kr
Vwtf L(W) = -vK1 X exp(-yiWTxi)yiXi,
K1	n	r
xi∈Br
VwK2 L(W) = -vK2 X exp(—yiW；Txi)yixi,
xi∈Bi
which implies that
VwKL(W) = VK1 VwKL(W),
wKt+11 = wKt 1 - ηVwtK L(W),
wK21 = WK2 — ηVwtκ L(W) = WK2 — vK2ηVwtκ L(W).	(28)
K2	vK1	K1
Define the empirical risk Lr(Wer) over the samples in Br as
Lr(Wr) = — X exp(—yif(xi)) = — X eχp(-yWr|xi).
nr xi∈Br	nr xi∈Br
Using that facts that L(W) converges to 0 and Lr (We rt ) ≤ (n/nr)L(W) implies that Lr (We rt ) con-
verges to 0. Thus, we have yi WetrT xi ≥ 0 for all xi ∈ Br, and kWe rt k → +∞ as t → +∞. Based on
eq. (28) we have, for any1 ≤ k ≤ Kr
Wk = WT + vk ∆wt.	(29)
v1
Recalling that Wetr = PkK=r1 vkWtk, we rewrite Wert as
Kr	Kr 2
Wr = X VkWT + X V ∆wt	(30)
k=1	k=1 v1
Noting that the norm of the first term in the right side of eq. (30) is finite and recalling that kWe rt k =
+∞, we have, k∆Wtk = +∞, which, in conjunction with eq. (29), implies that kWtk k = +∞ for
all1 ≤ k ≤ Kr.
Next, let us look at the update of W1 and W2. If there exist two elements in Vr that have different
signs (without loss of generality, we assume v1 > 0 and v2 < 0), then
W1t+1 = W1t — ηVw1t L(W),	(31)
w2+1 = w2 — ηVwt L(W) = w1 + η∣v1∣Vws L(W)	(32)
2	v2	1
20
Under review as a conference paper at ICLR 2019
which can be rewriten as
t-1
Wt = WT - X ηVwt L(W)= WT + ∆wt,
s=T +1
w2 = wT — J v1 * * * * J ∆wt.
Recalling that k∆Wt k = +∞ as t → +∞ and noting that the first two neurons are activated after
T iterations, we have, for ∀xi ∈ Br
w?Xi = WT|Xi + ∆wtlXi > 0,
w2|Xi = WTIXi - v1 ∆wtlXi > 0.	(33)
2	2	v2
Since ∆wt belongs to the space spanned by the samples in Br , ∆wt cannot be perpendicular to
all Xi ∈ Br. Thus, When t → +∞, We can find a sample Xr ∈ Br such that ∣∣∆wtlXrk = +∞.
If ∆wtlXr > 0, then We have w2|x『< 0, which contradicts eq. (33). If ∆wtlXr < 0, then
w1t|Xr < 0, Which also contradicts eq. (33). As a result, all elements in Vr have the same sign.
Next, We prove that all samples in the same pattern partition have the same label. First consider the
case When all elements in Vr are positive. If there exists a sample Xsp ∈ Br such that ysp = -1
When t = +∞, then
1 n	1	1	Kr	1
L(W) = —£exp(-yif(xi)) >	exp(-yspWrrXsp) = exp ( EvkWXsp) > ,
n	nn	n
i=1	k=1
Which contradicts that L(W) converges to 0.
Next, consider the case When all elements in Vr are negative. If there exists a sample Xsp ∈ Br such
that ysp = +1, We have
1 n	1	1	Kr t	1
L(W) = 一 £exp(-yif(Xi)) > — exp(-yspwTXsp) = — exp(-£vkWkTXsp) > -,
n	nn	n
i=1	k=1
Which also leads a contradiction. Combining these tWo results, We conclude that if all elements in
Vr are positive, then all samples in Br have label +1, and if all elements in Vr are negative, then all
samples in Br have label -1.
Finally, note that we r is updated by
Kr	Kr
wr+1 = wr + η X Vk VwkL(W)= wr + XX v2) X exp(-yi WrTXi)yiXi,
k=1	k=1	xi ∈Br
Which can be reWritten as
Kr
wr+ι=wr+η称(X v2)VLr(Wr).
n k=1
(34)
Applying Theorem 3.2 to eq. (34) with stepsize η = ηn/(nτ PK=I v2),we obtain that Wr converges
in the direction of the max-marigin classifier over all samples in Br , i.e.,
〜
w
t
r
kw rtk
Oflnln t
ln t
I Proof of Theorem 4.2
After T GD iterations, we randomly pick a Ar from {Ai}. Without loss of generality, we assume
that only the first Kr neurons are activated for all Xi ∈ Br, and suppose there are nr samples in Br .
We first use contradiction to show that all elements in the set Vr = {v1 ,v2,…，VKr} must be either
all positive or all negative.
21
Under review as a conference paper at ICLR 2019
According to the update rule of SGD, for any 1 ≤ K1 < K2 ≤ Kr
VwKj(W) = -vKι exp(-yξtwriχξjyξtχξt,
which implies that
VwK/(W) = -vK2 exP(-yξtwriχξjyξtxξt,
vwKι'(W) = VK1 vwK2 '(W),
wK11 = wK 1 -ηt VwKj(W),
wt+21 = WK2 - ηtvwK '(W)= wK2...........- ηtvwK '(W).
K2	vK1	K1
(35)
Then we prove we rt diverges to infinity as t → +∞. If we tr does not diverges to infinity, then there
exist a positive constant F < +∞ such that ∀t ≥ 0, kwe rt k < F. According to the update rule of
SGD
Kr
W r+1 = W r + nt(£ v2) eχp(-yξtW riχξjyξtχξt∙
k=1
For all χξt ∈ Br, we have y^t Wr|χ^t > 0, thus k Wr k is strictly increasing at each step. Since k Wr k
is upper bounded by F and is in the linearly separable region, we can find a constant r > 0 such
that
kWe rt+1k ≥ kWe rtk+ηtr.
Recall ηt = 1/(t + 1)-α, telescoping the above inequality from step T to t = +∞
t-1
kWe rtk ≥ kWe rTk+r X
s=T +1
1
(1 + s)α
since 0.5 < α < 1, the R.H.S of the above inequation goes to infinity, thus kWe rt k = +∞ when
t → +∞, which is a contradiction. Thus, We rt diverges to infinity.
Based on eq. (28) we have, for any 1 ≤ k ≤ Kr
Wk = WT + vk ∆Wt.	(36)
v1
Recalling that We tr = PkK=r 1 vk Wtk , we rewrite We rt as
Kr	Kr 2
Wr = X VkWT + (X 子 I ∆Wt	(37)
k=1	k=1 v1
Noting that the norm of the first term in the right side of eq. (37) is finite and recalling that kWe rt k =
+∞, we have, k∆Wtk = +∞, which, in conjunction with eq. (36), implies that kWtkk = +∞ for
all 1 ≤ k ≤ Kr .
Next, let us look at the update of W1 and W2 . If there exist two elements in Vr that have different
signs (without loss of generality, we assume v1 > 0 and v2 < 0), then
w1+1 = w1 - ηtVwt'(W),	(38)
w2+1 = w2 - ηtVwt '(W) = w1 + ηt∣V1∣ Vwt `(W).	(39)
which can be rewriten as
t-1
w1 = wT - X ηtVwt '(W) = WT + ∆wt,
s=T +1
w2 = wT — ∣ v11 ∆wt.
22
Under review as a conference paper at ICLR 2019
Recalling that k∆wt k = +∞ as t → +∞ and noting that the first two neurons are activated after
T iterations, we have, for ∀xi ∈ Br and t > T, the following two inequalities always hold
w11Xi = WT|Xi + ∆wtlXi > 0,
Wtfxi = WTIXi — v1 ∆wtlXi > 0.	(40)
2	2	v2
Since ∆Wt belongs to the space spanned by the samples in Br , ∆Wt cannot be perpendicular to
all Xi ∈ Br. Thus, When t → +∞, We can find a sample Xr ∈ Br such that ∣∣∆wtlXrk = +∞.
If ∆wtlXr > 0, then We have w2|x『< 0, which contradicts eq. (40). If ∆wtlXr < 0, then
w1t|Xr < 0, Which also contradicts eq. (40). As a result, all elements in Vr have the same sign.
Next, We prove that all samples in the same pattern partition have the same label. First consider the
case When all elements in Vr are positive. If there exists a sample Xsp ∈ Br such that ysp = —1
When t = +∞, then
1	n	1	1	Kr	t	1
L(W) = n∑exp(—yif(xi)) > — exp(—yspWr|Xsp) = — exp(E VkWkTXsp) > -,
n	nn	n
i=1	k=1
Which contradicts that L(W) < 1/n.
Next, consider the case When all elements in Vr are negative. If there exists a sample Xsp ∈ Br such
that ysp = +1, We have
1 n	1	1	Kr t	1
L(W) = — £exp(—yif(Xi)) >	exp(—ysp^vr|Xsp) =	exp(—£VkWkTXsp) >	,
n	nn	n
i=1	k=1
Which also leads a contradiction. Combining these tWo results, We conclude that if all elements in
Vr are positive, then all samples in Br have label +1, and if all elements in Vr are negative, then all
samples in Br have label —1.
Finally, note that we r is updated by
Kr	Kr
wr+1 = W； + ηt X VkVwk'(W) = wr + ηt(X Vk) exp(—yiWrτXi)yiXi,
k=1	k=1
Which can be reWritten as
Kr
wr+1=wr+nt(x Vk )v`r(wr).	(4i)
k=1
Applying Theorem 3.4 to eq. (41) with stepsize ηt = ηt (PK=I v2) 1, We obtain that W∖ converges
in the direction of the max-marigin classifier over all samples in Br , i.e.,
23