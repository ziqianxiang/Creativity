Under review as a conference paper at ICLR 2019
On Generalization Bounds of a Family
of Recurrent Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Recurrent Neural Networks (RNNs) have been widely applied to sequential data
analysis. Due to their complicated modeling structures, however, the theory be-
hind is still largely missing. To connect theory and practice, we study the gener-
alization properties of vanilla RNNs as well as their variants, including Minimal
Gated Unit (MGU) and Long Short Term Memory (LSTM) RNNs. Specifically,
our theory is established under the PAC-Learning framework. The generalization
bound is presented in terms of the spectral norms of the weight matrices and the
total number of parameters. We also establish refined generalization bounds with
additional norm assumptions, and draw a comparison among these bounds. We
remark: (1) Our generalization bound for vanilla RNNs is significantly tighter
than the best of existing results; (2) We are not aware of any other generaliza-
tion bounds for MGU and LSTM in the exiting literature; (3) We demonstrate the
advantages of these variants in generalization.
1	Introduction
Recurrent Neural Networks (RNNs) have successfully revolutionized sequential data analysis, and
been widely applied to many real world problems, such as natural language processing (Cho et al.,
2014; Bahdanau et al., 2014; Sutskever et al., 2014), speech recognition (Graves et al., 2006;
Mikolov et al., 2010; Graves, 2012; Graves et al., 2013), computer vision (Gregor et al., 2015; Xu
et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015), healthcare (Lipton et al., 2015; Choi
et al., 2016a;b), and robot control (Ku and Lee, 1995; Lee and Teng, 2000; Yoo et al., 2006). Quite
a few of these applications can be approached easily in our daily life, such as Google Translate,
Google Now, Apple Siri, etc.
The sequential modeling nature of RNNs is significantly different from feedforward neural net-
works, though they both have neurons as the basic components. RNNs exploit the internal state
(also known as hidden unit) to process the sequence of inputs, which naturally captures the depen-
dence of the sequence. RNNs can also be viewed as nonlinear dynamical systems, and reduced
to linear dynamical systems given identity activation operators. Besides the vanilla version, RNNs
have many other variants. A large class of variants incorporate the so-called “gated” units to trim
RNNs for different tasks. Typical examples include Long Short-Term Memory (LSTM, Hochre-
iter and Schmidhuber (1997)), Gated Recurrent Unit (GRU, Jozefowicz et al. (2015)) and Minimal
Gated Unit (MGU, Zhou et al. (2016)).
The success of RNNs owes not only to their special network structures and the ability to fit training
data, but also to their good generalization property: They can provide accurate predictions on unseen
data. For example, Graves et al. (2013) report that after training with merely 462 speech samples,
deep LSTM RNNs achieve a test set error of 17.7% on TIMIT phoneme recognition benchmark,
which is the best recorded score. Mikolov et al. (2010) also show that RNNs outperform significantly
state-of-the-art backoff models for speech recognition. When using RNNs in Wall Street Journal task
to predict the next word in textual data given the context, word error rate is reduced around 18%
compared to backoff models trained on the same amount of data, and 12% when backoff model is
trained on 5 times more data. Despite of the popularity of RNNs in applications, their theory is
less studied than other feedforward neural networks (Bartlett et al., 2017; Neyshabur et al., 2017;
Golowich et al., 2017; Li et al., 2018). There are still several long lasting fundamental questions
regarding the approximation, trainability, and generalization of RNNs.
1
Under review as a conference paper at ICLR 2019
In this paper, we propose to understand the generalization ability of RNNs and their variants. We
aim to answer two questions from a theoretical perspective:
Q.1) Do RNNs suffer from significant curse of dimensionality?
Q.2) What are the advantages of MGU and LSTM over vanilla RNNs?
The investigation of generalization properties of neural networks including RNNs has a long history.
Most of these works adopt a layer wise analysis and establish their results by induction. For exam-
ple, Haussler (1992) establishes a complexity bound for feedforward neural networks. The result
assumes average Lipschitz constant of each layer to be greater than 1 and involves the product of
these Lipschitz constants. Thus, the resulting bound is inevitably exponential in the depth of the
network. Later, Dasgupta and Sontag (1996) and Koiran (1998) adopt a VC-dimension argument to
show complexity bounds of RNNs that are polynomial in the size of the network. Both of the works,
however, are based on oversimplified assumptions: Dasgupta and Sontag (1996) only consider linear
RNNs for binary classification tasks; Koiran (1998) assumes RNNs take the first coordinate of their
hidden states as outputs. More recently, Bartlett et al. (2017) propose anew technique for developing
generalization bounds for feedforward neural networks based on empirical Rademacher complexity
under the PAC-Learning framework. Neyshabur et al. (2017) further adapt the technique to establish
their generalization bound using the PAC-Bayes approach. Then the follow-up work Zhang et al.
(2018) use the PAC-Bayes approach to establish a generalization bound for vanilla RNNs.
Our theory is partially motivated by Bartlett et al. (2017), but is quite different from Bartlett et al.
(2017) and Zhang et al. (2018). In particular, our analysis exploits the compositional nature of
RNNs, and decouples the spectral norms of weight matrices and the number of weight parameters.
This makes our analysis conceptually much simpler (e.g. avoid layer wise analysis), and also yields
better generalization bound than Zhang et al. (2018).
Taking a sequence to sequence multiclass classification problem as an
example, we observe m sequences of data points (xi,t, zi,t)tT=1, where
xi,t ∈ Rdx and the class label zi,t ∈ {1, . . . , K} for all t = 1, ..., T
and i = 1, ..., m. Each sequence is drawn independently from some
underlying distribution over Rdx×T × {1, . . . , K}. The vanilla RNNs
compute hi,t and yi,t iteratively as follows,
hi,t = σh (U hi,t-1 + Wxi,t) , and yi,t = σy (V hi,t) ,
where σy and σh are activation operators, hi,t ∈ Rdh is the hidden state with hi,0 = 0, and U ∈
Rdh ×dh, V ∈ Rdy ×dh, and W ∈ Rdh ×dx are weight matrices. The activation operators σh and σy
are entrywise, i.e., σh([v1, . . . , vd]>) = [σh(v1), . . . , σh(vd)]>, and Lipschitz with parameters ρh
and Py respectively. For simplicity, We assume σh(∙) = tanh(∙), σy (0) = 0, and Py = 1. Extensions
to general activation operators are given in Section 2. For a new testing sequence (xt, zt)tT=1, we
predict the label sequence using
zet = argmaxj [yt]j , for all t = 1, . . . , T.
ht-1 -U*
Xt —w*
σh —p^>ht
Figure 1: A basic building
block of vanilla RNNs
To establish the generalization bound, We need to define the “model complexity” of vanilla RNNs.
In this paper, We adopt the empirical Rademacher complexity (ERC, see more details in Section 2),
Which has been Widely used in the existing literature on PAC-Learning. For many nonparametric
function classes, We often need complicated argument to upper bound their ERC. Our analysis,
hoWever, shoWs that We can upper bound the ERC of vanilla RNNs in a very simple manner by
exploiting their Lipschitz continuity With respect to (W.r.t) the model parameters, since they are
essentially in parametric forms. More specifically, denote Ft = {ft : {x1, ..., xt} 7→ yt} as the
class of mappings from the first t inputs to the t-th output computed by vanilla RNNs. For a matrix
A, kAk2 denotes the spectral norm, and for a vector v, kvk2 denotes the Euclidean norm. Define
xχ--1 = t for X = 1. Then, informally speaking, the “model complexity" of vanilla RNNs satisfies
Complexity = O
d min
kW k2 3
kUk2-1
where d =，dXdh + d∖ + dhdy. We then give the generalization bound in the following statement.
2
Under review as a conference paper at ICLR 2019
Theorem 1 (informal). Given a collection of samples S = (xi,t, zi,t)tT=1 , i = 1, ..., m with
kxi,tk2 ≤ 1 and a new testing sequence (xt, zt)tT=1, with probability at least 1 - δ over S, for
every margin value γ > 0 and ft ∈ F for integer t ≤ T , we have,
P(et = Zt) ≤Rγ,t + o( co√Pleχity + SR ,	⑴
mγ	m
Where Rγ,t = m1 Pi=I 1([yi,t]Zi,t ≤ maxj=Zi,t [yi,t]j + Y).
Please refer to Section 2 for a complete statement. The generalization bound in Theorem 1 can
be interpreted under three different scenarios1: (I) When kUk2 < 1, the generalization bound is
O(√dγ)，which only has a logarithmic dependence on t; (II) When ∣∣U∣∣2 = 1, the generaliza-
tion bound is O(√tγ), which has a linear dependence on d and t; (III) When ∣∣U∣∣2 > 1, the
generalization bound is O(√∣∣), which has a polynomial dependence on d and t.
We theoretically justify that vanilla RNNs do not suffer from significant curse of dimensionality.
Because they compute outputs yt recursively using the same weight matrices, and their hidden states
ht are entrywise bounded.
Compared with the generalization bound in Zhang et al. (2018), which is of the order
O( dt2∣W k2kVk2 max{1,∣U 图)
t	√mγ	J,
our bound is tighter by a factor of t2 for case (I), a factor of t for case (II). Additionally, Zhang
et al. (2018) fail to incorporate the boundedness condition of hidden state into their analysis, thus
the generalization bound is exponential in t for case (III). Our generalization bound, however, is
still polynomial in d and t for case (III).
Moreover, (II) is closely related to a few recent results on imposing orthogonal constraints on weight
matrices to stabilize the training of RNNs (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2016;
Vorontsov et al., 2017; Zhang et al., 2018). We remark that from a learning theory perspective, (II)
also implies that the orthogonal constraints can potentially help generalization.
We also present refined generalization bounds with additional matrix norm assumptions. These
assumptions allow us to derive norm-based generalization bounds. We draw a comparison among
these bounds and highlight their advantage under different scenarios.
Our theory can be further extended to several variants, including MGU and LSTM RNNs. Specifi-
cally, we show that the gated units in MGU and LSTM RNNs can introduce extra decaying factors
to further reduce the dependence on d and t in generalization. Such an advantage in generalization
make these RNNs do not suffer from significant curse of dimensionality either. To the best of our
knowledge, these are the first results on generalization guarantees for these neural networks.
The rest of the paper is organized as follows: Section 2 presents the generalization bound of vanilla
RNNs; Section 3 presents the proof outline of the generalization bound; Section 4 presents refined
generalization bounds and their comparison; Section 5 presents the generalization bound of MGU
and LSTM RNNs; Section 7 discusses related works and collects open problems.
Notations: Given a vector v ∈ Rd, we denote the vector Euclidean norm by ∣v∣22 = Pid=1|vi|2, and
the infinity norm by ∣v∣∞ = maxj |vj |. Given a matrix M ∈ Rm×n , we denote the spectral norm
by ∣M∣2 as the largest singular value of M, the Frobenius norm by ∣M ∣F2 = trace(M M >), and the
(2,1) norm by ∣∣M∣∣2,1 = Pn=IIlM：,ik2. Given a function f, we denote the function infinity norm
by ∣∣f ∣∞ = sup |f |. We adopt the standard O(∙) notation, which is defined as f(χ) = O (g(χ)) for
x → ∞ if and only if there exists M > 0 and x0, such that |f (x)| ≤ M g(x) for x ≥ x0. We use
O(∙) to denote O(∙) with hidden log factors.
1To ease the discussion, we assume kU k2 does not scale with t. Therefore, kU k2 < 1 is equivalent to
kU k2 ≤ 1 - ∆ for a constant ∆ > 0. A precise statement can be found following Theorem 2.
3
Under review as a conference paper at ICLR 2019
2	Generalization of Vanilla RNNs
To establish the generalization bound, we start with imposing some mild assumptions.
Assumption 1. Input data are bounded, i.e., kxi,tk2 ≤ Bx for all i = 1, . . . , m and t = 1, . . . , T.
Assumption 2. The spectral norms of weight matrices are bounded respectively, i.e., kUk2 ≤ BU,
kVk2 ≤ BV,andkWk2 ≤ BW.
Assumption 3. Activation operators σh and σy are Lipschitz with parameters ρh and ρy respec-
tively, and σh (0) = σy (0) = 0. Additionally, σh is entrywise bounded by b.
Assumptions 1 and 2 are moderate assumptions. Moreover, Assumption 3 holds for most commonly
used activation operators, such as σh(∙) = tanh(∙) and σy(∙) = ReLU(∙) = max{∙, 0} (1-LiPschitz).
Recall that vanilla RNNs compute hi,t and yi,t as follows,
hi,t = σh (U hi,t-1 + Wxi,t) and yi,t = σy (V hi,t) ,
where U ∈ Rdh×dh, V ∈ Rdy ×dh, and W ∈ Rdh×dx . Given a sequence (xt, zt)tT=1, we define Xt ∈
Rdx ×t by concatenating x1, . . . , xt as columns of Xt. Recall that we denote Ft = {ft : Xt 7→ yt}
as the class of mappings from the first t inputs to the t-th output computed by vanilla RNNs. Then
we define the functional margin for the t-th output in vanilla RNNs as
M(ft(Xt),zt)= [ft(Xt)]zt - maxj6=zt[ft(Xt)]j.
We further define a ramp loss 'γ (-M(ft(Xt), Zt)) : R → R+ to each margin, where 'γ is a
piecewise linear function defined as
'γ(a) = 1{a > 0} + (1 + a)1{-γ ≤ a ≤ 0},
where 1{A} denotes the indicator function of a set A. Accordingly, the ramp risk is defined
as RY(ft) = E ['γ (-M(ft(Xt),zt))], and its empirical counterpart is defined as RY(ft) =
ml Pm=I 'γ (-M(ft(Xi,t), Zi,t)). We then present the formal statement of Theorem 1.
Theorem 2.	Let activation operators σh, and σy be given, and Assumptions 1-3 hold. Then for
(xt, zt)tT=1 and S = (xi,t, zi,t)tT=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution
over Rdx ×T × {1, . . . , K}, with probability at least 1 - δ over S, for every margin value γ > 0 and
every ft ∈ Ft for integer t ≤ T , we have
P (zet 6= zt) ≤ RbY(ft) +O
dpy BV min{ b√d,ph Bx BWe--I
√mγ
where d = y∕dXdh+~dh+~dhd and β = PhBU.
We remark that the generalization bound depends on the total number of weights, and the range of
PhBU in three cases as indicated in Section 1. More precisely, if PhBU . (1 + ta) for constant
ɑ > 0 bounded away from zero, the generalization bound is of the order O (√d=γ), which has a
polynomial dependence on dand t. As can be seen, with proper normalization on model parameters,
vanilla RNNs do not suffer from significant curse of dimensionality.
We also highlight a tradeoff between generalization and representation of vanilla RNNs. As can be
seen, when PhBU is strictly smaller than 1, the generalization bound is nearly independent on t.
The hidden state, however, only has limited representation ability, since its magnitude diminishes as
t grows large. On the contrary, when PhBU is strictly greater than 1, the representation ability of
hidden state is amplified but the generalization becomes worse. As a consequence, recent empirical
results show that imposing extra constraints or regularization, such as U>U = I or kUk2 ≤ 1 (Saxe
et al., 2013; Le et al., 2015; Arjovsky et al., 2016; Vorontsov et al., 2017; Zhang et al., 2018), helps
balance the generalization and representation of RNNs.
4
Under review as a conference paper at ICLR 2019
3 Proof of Main Results
Our analysis is based on the PAC-learning framework. Due to space limit, we only present an outline
of our proof. More technical details are deferred to Appendix A. Before we proceed, we first define
the empirical Rademacher complexity as follows.
Definition 1 (Empirical Rademacher Complexity). Let H be a function class and S = {s1, . . . , sm}
be a collection of samples. The empirical Rademacher complexity of H given S is defined as
1m
ɪ -Veih(Si)
h∈H m i=1
Empirical Rademacher Complexity: RS (H) = E sup
where ei’s are i.i.d. Rademacher random variables, i.e., P(ei = 1) = P(ei = -1) = 0.5.
We then proceed with our analysis. Recall that Mohri et al. (2012) give an empirical Rademacher
complexity (ERC)-based generalization bound, which is restated in the following lemma with
Fγ,t = {(Xt,zt) → 'γ(-M(ft(Xt),zt)) : ft ∈ Ft}.
Lemma 1. Given a testing sequence (xt, zt)tT=1, with probability at least 1 - δ over samples S =
(xi,t, zi,t)tT=1, i = 1, . . . , m , for every margin value γ > 0 and any ft ∈ Ft, we have
P(zet 6= zt) ≤ Rγ(ft) ≤ Rbγ(ft) + 2RS(Fγ,t) + 3
(2)
Note that Lemma 1 adapts the original version (Theorem 3.1, Chapter 3.1, Mohri et al. (2012)) for
the multiclass ramp loss, and we have P(zet 6= zt) ≤ Rγ(ft) by definition.
Now we only need to bound the ERC RS(Fγ,t). Our analysis consists of three steps. First, we
characterize the Lipschitz continuity of vanilla RNNs w.r.t model parameters. Next, we bound the
covering number of function class Ft. At last, we derive an upper bound on RS(Fγ,t) via the
standard machinery in the PAC-learning framework. Specifically, consider two different sets of
weight matrices (U, V, W) and (U0, V 0, W0). Given the same activation operators and input data,
denote the t-th output as yt and yt0 respectively. We characterize the Lipschitz property of kyt k2 w.r.t
model parameters in the following lemma.
Lemma 2. Under Assumptions 1-3, given input (Xt)T=I and for any integer t ≤ T, ∣∣yt∣∣2 is LiPs-
chitz in U, V and W, i.e.,
kyt - yt0 k2 ≤ LU,t kU - U0kF + LV,t kV - V0kF + LW,t kW - W0kF ,
Where Lu,t = PhBVBWtat, Lv,t = BWat, and Lw,t = BVat With at = PyPhBx (PhBU--1.
The detailed proof is provided in Appendix A.2. We give a simple example to illustrate the proof
technique. Specifically, We consider a single layer netWork that outputs y = σ(W x), Where x is
the input, σ is an activation operator With Lipschitz parameter P, and W is a Weight matrix. Such a
netWork is Lipschitz in both x and W as folloWs. Given Weight matrices W and W0, We have
ky - y0k2 = kσ(W x) - σ(W 0x)k2 ≤ Pkxk2kW - W 0kF.
Additionally, given inputs x and x0 , We have
ky - y0k2 = kσ(Wx) - σ(Wx0)k2 ≤ PkWk2kx - x0k2.
Since vanilla RNNs are multilayer netWorks, Lemma 2 can be obtained by telescoping.
We remark that Lemma 2 is the key to the proof of our generalization bound, Which separates the
spectral norms of Weight matrices and the total number of parameters.
Next, we bound the covering number of Ft. Denote by N(Ft, e, dist(∙, ∙)) the minimal cardinality
of a subset C ⊂ Ft that covers Ft at scale e w.r.t the metric dist(∙, ∙), such that for any ft ∈ Ft, there
exists fbt ∈ C satisfying dist(ft, fbt) = supXtkft(Xt) - fbt(Xt)k2 ≤ e. The following lemma gives
an upper bound on N (Ft, e, dist(∙, ∙)).
Lemma 3. Under Assumptions 1-3, given any e > 0, the covering number of Ft satisfies
N(Ft,e, dist(∙,∙)) ≤
I + 6c√dt ((PhBU)t- 1) Yd2
e (PhBU - 1)	)
where c= PyPhBVBWBx max {1, PhBU}.
5
Under review as a conference paper at ICLR 2019
The detailed proof is provided in Appendix A.3. We briefly explain the proof technique. Given
activation operators, since vanilla RNNs are in parametric forms, ft has a one-to-one correspondence
to its weight matrices U, V, and W. Lemma 2 implies that dist(∙, ∙) is controlled by the FrobeniUs
norms of the difference of weight matrices. Thus, it suffices to bound the covering numbers of
three weight matrices, which can be obtained by the standard machinery. The prodUct of covering
nUmbers of three weight matrices gives Us Lemma 3.
Lastly, we give an Upper boUnd on RS (Fγ,t) in the following lemma.
Lemma 4. Under Assumptions 1-3, given activation operators and samples S =
{(xi,t, zi,t)tT=1, i = 1, . . . , m}, the empirical Rademacher complexity RS(Fγ,t) satisfies
Rs(Fγ,t) = O (dρyBV min ^t)√dl,fihBχBw ThBU)［； } M√m产)--D).
The detailed proof is provided in Appendix A.4. Our proof exploits the Lipschitz continuity of
M and 'γ, and uses Dudley's entropy integral as the standard machinery to establish Lemma 4.
Combining Lemma 1 and Lemma 4, we complete the proof.
4 Refined Generalization Bounds
When additional norm constraints on weight matrices U, V and W are available, we can further
refine generalization bounds. Specifically, we consider the following assumptions.
Assumption 4. The weight matrices satisfy kU k2,1 ≤ MU, kV k2,1 ≤ MV, and kW k2,1 ≤ MW.
Assumption 5. The weight matrices satisfy kUkF ≤ BU,F, kVkF ≤ BV,F, and kWkF ≤ BW,F.
Note that Assumption 4 appears in Bartlett et al. (2017) and Assumption 5 appears in Neyshabur
et al. (2017). We have an equivalent relation between matrix norms, i.e.,『k2 ≤k∙k2,1 ≤ √⅜kF ≤
dk ∙ ∣∣2. Comparing to Assumption 2, Assumptions 4 and 5 further restrict the model class. We then
establish the following generalization bounds.
Theorem 3.	Let activation operators σh and σy be given, and Assumptions 1-3 hold. Then for
(xt, zt)tT=1 and S = (xi,t, zi,t)tT=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution
over Rdx ×T × {1, . . . , K}, with probability at least 1-δ over S, for every margin value γ > 0 and
every ft ∈ Ft for integer t ≤ T , the following two bounds hold:
•	Suppose Assumption 4 also holds. We have
P (zet 6= zt) ≤ Rbγ(ft) +O
α(Mu + Mv +Mw)t ββ-1 √log d log(√dm)
+庠),⑶
where α = PhPyBVBWBx, d = VZdxdh + dh + dhdy, and β = ρhBu.
•	Suppose Assumption 5 also holds. We have
P (zet 6= zt) ≤ Rbγ(ft) +O
ρyρh
-1 dn IMd)(BU ,f+bW,f+bV ,f))
√mγ	卜 ()
where λ) = min {b√d,ρh.BxBWe--I}, d = ∙√dxdh + dh + dhdy, and β = PhBU.
The detailed proof is provided in Appendix B.1. The first bound (3) adapts the matrix covering
lemma in Bartlett et al. (2017). The second bound (4) adapts the PAC-Bayes approach (Neyshabur
et al., 2017) by analyzing the divergence when imposing small perturbations on the weight matrices.
We compare generalization bounds in Table 1 by differentiating ranges2 of β. It can be seen that
when β > 1, both bounds (3) and (4) involve an exponential term in β. Thus, Theorem 2 yields
a better result. When β ≤ 1, we distinguish two cases: the extreme case and the approximately
low rank case. Specifically, remember that in the extreme case, we have ∣∣ ∙ ∣2,ι = d∣ ∙ ∣2 and
2For simplicity, we assume again that β does not scale with t. By β < 1, we mean β ≤ 1 - ∆ for a constant
∆ > 1. A similar characterization applies to β > 1.
6
Under review as a conference paper at ICLR 2019
k ∙ ∣∣f = √dk ∙ k2, e.g., orthogonal weight matrices. Therefore, bound (4) meets Theorem 2 for
β < 1 and is worse for β = 1. Bound (3) is worse than Theorem 2 for β ≤ 1. On the other hand, if
the weight matrices are approximately low rank, We have ∣∣ ∙ ∣∣2,1《 d∣ ∙ ∣∣2 and ∣∣ ∙ ∣∣f《 √d∣∣ ∙ ∣∣2.
In this case, bound (4) improves Theorem 2 for β < 1 by reducing dependence on d. Additionally,
if t (MU + MV + MW) < d, bound (3) yields tighter results both for β < 1 and β = 1. Note that
t (MU + MV + MW ) < d also implies that the input sequence is relatively short.
Table 1: Generalization bounds in Theorems 2 and 3 with respect to different ranges of β .
	Theorem 2	Theorem 3	
		Bound (3)	Bound (4)
β < 1	0( √mγ )	O (t(Mu+Mv+Mw)) ∖	√mY	)	Oe q qd(Bu ,F+Bw ,f+bv ,F)!― √	√mγ	I
β = 1	0( √mγ)	O (t2(Mu+Mv + Mw)) (	√mγ	J	-万 Idt√BU,F+Bw,f+b2VF∖ (	√θ	)
β > 1	O( √d3t) mγ	O (tβt(Mu+Mv+Mw)) ∖	√mY	)	方(dβ √bU ,F + Bw ,F + BV ,；\- o (	√mγ	)
5 Extensions to MGU and LSTM RNNs
We extend our analysis to Minimal Gated Unit (MRU) and Long Short-Term Memory (LSTM)
RNNs. We show that these RNNs introduce extra decaying factors to further reduce the dependence
on d and t in generalization.
The MGU RNNs are the simplest GRU RNNs,
which compute output yt as follows,
rt = σ(Wrxt + Urht-1),
丁	Zttt-	t t Z - i ∖ ∖
ht = σh (Whxt + Uh (rt ht-1)) ,
ht = (1 - rt)	ht-1 + rt eht ,
yt = σy(Vht),
ht-1
xt
ht
K θ 芯 σh 一 9	I
Wh Trt	V
_________________Ur
ht
Figure 2: A basic building block of MGU RNNs
where Wr, Wh ∈ Rdh×dx, Ur, Uh ∈ Rdh ×dh, V ∈ Rdy ×dh, and rt ∈ Rdh. The notation de-
notes the Hadamard product (entrywise product) of vectors. Denote by Fg,t the class of mappings
from the first t inputs to the t-th output computed by gated (MGU or LSTM) RNNs. For simplicity,
We consider σ being the sigmoid function, i.e., σ(x) = (1 + exp(-x))-1, σ%(∙) = tanh(∙), and
σy being ρy-Lipschitz with σy (0) = 0. Extensions to general Lipschitz activation operators as in
Assumption 3 are straightforward. Suppose we have h0 = 0 and the following assumption.
Assumption 6. All the weight matrices have bounded spectral norms respectively, i.e. ∣Wr ∣2 ≤
BWr, ∣Wh∣2 ≤ BWh , ∣Ur∣2 ≤ BUr, ∣Uh∣2 ≤ BUh , and ∣V ∣2 ≤ BV .
Using a similar argument for vanilla RNNs yields a generalization bound of MGU RNNs as follows.
Theorem 4. Let the activation operator σy be given and Assumptions 1 and 6 hold. Then for
(xt, zt)tT=1 and S = (xi,t, zi,t)tT=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution
over Rdx ×T × {1, . . . , K}, with probability at least 1 - δ over S, for every margin value γ > 0 and
every ft ∈ Fg,t for integer t ≤ T, we have
P (zet 6= zt) ≤ Rbγ(ft) +O
dpy BV min{ √d,Bwh Bx
where d = P2dςcdh + 2dh + dhdy, β = maxj≤t {∣∣1 — r7∙∣∣∞ + BUh Kjk∞}, and θ = β +
2BUr + BUr BUh.
The detailed proof is provided in Appendix C.1. As can be seen, rt shrinks the magnitude of hidden
state to reduce the dependence on d and t in generalization. Specifically, the hidden state ht is
entrywise bounded by 1. Then for any integer t ≤ T , we have
krtk∞ ≤ 1+exp(--W,Bx — ∣Urkι) and k1-rtk∞ ≤ 1 + exp(-gw,Bx — k%kι),
7
Under review as a conference paper at ICLR 2019
where kUrk1 denotes the maximal absolute row sum. By restricting BWr and kUrk1 sufficiently
small, we can guarantee that β and θ are strictly smaller than 1 when BUh = 1 or even BUh > 1.
As a result, with proper normalization of weight matrices, the generalization bound of MGU RNNs
is less dependent on d and t than that of vanilla RNNs.
The LSTM RNNs are more complicated than MGU RNNs, which introduce more gates to control
the information flow in RNNs. LSTM RNNs
have two hidden states, and compute them as,
gt = σ(WgXt + Ug hi),
r = σ(Wr Xt + Ur ht-l),
ot = σ(Woxt + Uoht-1),
cet = σc (WcXt + Ucht-1) ,
ct = gt	ct-1 + rt	cet ,
ht = ot	tanh(ct),
Figure 3: A basic building block of LSTM RNNs
where Wg, Wr, Wo, Wc ∈ Rdh×dx,	Ug,Ur, Uo, Uc ∈	Rdh×dh, and	gt,rt,ot	∈ Rdh.	For sim-
plicity, we also consider σ being the	sigmoid function,	and σc(∙) =	tanh(∙).	The t-th	output is
yt = σy(Vht), where V ∈ Rdy×dh, and σy is Py-Lipschitz with σy(0) = 0. Suppose we have
ho = co = 0 and the following assumption.
Assumption 7. The spectral norms of weight matrices are bounded respectively, i.e. ∣∣Wg∣∣2 ≤
BWg JWrk2 ≤ BWr ,kWo∣2 ≤ BWo ,kWc∣2 ≤ BWc ,∣Ug ∣2 ≤ BUg JUrk2 ≤ BUr ,∣Uo∣2 ≤
BUo,kUh∣2 ≤ BUh, and IlV∣∣2 ≤ BV.
For properly normalized weight matrices Wo and Uo, the generalization bound of LSTM RNNs is
given in the following theorem.
Theorem 5. Let the activation operator σy be given and Assumptions 1 and 7 hold. Then for
(Xt, zt)tT=1 and S = (Xi,t, zi,t)tT=1, i = 1, . . . , m drawn i.i.d. from any underlying distribution
overRdx×T × {1, . . . , K}, with probability at least 1 - δ over S, for every margin value γ > 0 and
every ft ∈ Fg,t for integer t ≤ T, we have
d dpy BV min	d,Bwc
P (zet 6= zt) ≤Rbγ(ft)+O
where d = VZldxdmdh+dhdy, β = maxj≤t {∣∣gj∣∣∞ + BUckrjk∞ l∣θjk∞} and θ = β +
BUg + BUr + BUo .
The detailed proof is provided in Appendix C.2. Similar to MGU RNNs, LSTM RNNs also intro-
duce extra decaying factors to reduce the dependence on d and t in generalization. However, LSTM
RNNs are more complicated, but more flexible than MGU RNNs, since three factors, rt, ot and gt
are used to jointly control the spectrum of Uc . We further remark that LSTM RNNs also need the
spectral norms of weight matrices, Wg, Wr, Wo, Ug, Ur, and Uo, to be properly controlled such that
better generalization bounds can be obtained.
Remark 1. We also extend our analysis to convolutional RNNs (Conv RNNs, Pinheiro and Col-
lobert (2014); Liang and Hu (2015); Xingjian et al. (2015)), and show that the convolutional filters
in Conv RNNs can reduce the dependence on d through parameter sharing in generalization. Due to
space limit, the detailed discussion is provided in Appendix D.
6	Numerical Evaluation
We demonstrate a comparison among our obtained generalization bound with Bartlett et al. (2017),
Neyshabur et al. (2017), and Zhang et al. (2018). Specifically, we train a vanilla RNN on the wikitext
long term dependency language modeling dataset. We take σh = tanh and set the hidden state
h ∈ R128 and the input X ∈ [0, 1]14. Accordingly, we have d = 128 and take the sequence length
t = 56. We list the complexity bounds for vanilla RNNs in Theorem 2 (Ours), Zhang et al. (2018)
(Bound 1), (3) of Theorem 3 (Bound 2), and (4) of Theorem 3 (Bound 3) in the following by
neglecting common log factors in d and t,
8
Under review as a conference paper at ICLR 2019
• Ours: dBV min
Bwbu- O jog (BU-I)；
• Bound 1: dt2BV BW max{1, BUt };
• Bound 2: BvBW (MU + MV + MW) t BU—；;
• Bound 3:
(min n √d, BW BU-I} BU + BW)
The corresponding complexity bounds are shown in
Figure 4 in the logarithmic scale. As can be seen, our
complexity bound in Theorem 2 is much smaller than
Bounds 1-3. In more detail, the trained vanilla RNN
takes BU = 2.6801 > 1. As discussed earlier, for
BU > 1, only our bound in Theorem 2 is polynomial in
the size of the network, while Bounds 1-3 are all expo-
nential in t. The resulting complexity bounds corrobo-
rate such a conclusion.
We also observe that Bound 3 is smaller than Bound
2. The reason behind is that the weight matrices in
the trained vanilla RNN have relatively small Frobe-
nius norms but large (2, 1) norms. For example, we
have BUF = 13.6823 and MU = 154.5439. Then, We
can calculate the stable rank BUF = 5.1 < 空,and
BU	2
OUrS BoUndI BoUnd2	BoUnd3
Figure 4: A comparison between general-
ization bounds for the same vanilla RNN
trained on the wikitext dataset. The verti-
cal axis is the logarithmic scale of the cor-
responding bounds.
MU
Βu,f
11.3 ≈ √d, which implies the singular values of U are not evenly distributed, while the
norms of row vectors in U are mostly approximately equal.
7	Discussions and Open Problems
(I)	Tighter bounds: Our obtained generalization bounds depend on the spectral norms of weight
matrices and the network size (the total number of parameters). Can we exploit other modeling
structures to further reduce the dependence on the network size? Or can we find better choices of
norms of weight matrices that yield better bounds?
(II)	Margin value: Our generalization bounds depend on the margin value of the predictors. As can
be seen, a larger margin value yields a better generalization bound. However, establishing a sharp
characterization of the margin value is technically very challenging, because of its complicated
dependence on the underlying data distribution and the training algorithm.
(III)	Implicit bias of SGD: Numerous empirical evidences have already shown that RNNs trained
by stochastic gradient descent (SGD) algorithms have superior generalization performance. There
have been a few theoretical results showing that SGD tends to yield low complexity models, which
can generalize (Neyshabur et al., 2014; 2015; Zhang et al., 2016; Soudry et al., 2017). Can we
extend their argument to RNNs? For example, can SGD always yield weight matrices with well
controlled spectra? As mentioned, this is crucial to the generalization of MGU and LSTM RNNs,
since not well controlled spectra may even hurt generalization.
(IV)	Adaptivity to the underlying distribution: The current PAC-Learning framework focuses on
the worst case. Taking classification as an example, the theoretical analysis holds even when the
input features and labels are completely independent. Therefore, this often yields very pessimistic
results. For many real applications, however, data are not obtained adversarially. Some recent
empirical evidences suggest that the generalization of neural networks seems very adaptive to the
underlying distribution: Easier tasks lead to low complexity neural networks, while harder ones
lead to highly complex neural networks. Unfortunately, none of the existing analysis can take the
underlying distribution into consideration.
9
Under review as a conference paper at ICLR 2019
References
Arjovsky, M., Shah, A. and Bengio, Y. (2016). Unitary evolution recurrent neural networks.
In International Conference on Machine Learning.
Bahdanau, D., Cho, K. and B engio, Y. (2014). Neural machine translation by jointly learning
to align and translate. arXiv preprint arXiv:1409.0473.
Bartlett, P. L., Foster, D. J. and Telgarsky, M. J. (2017). Spectrally-normalized margin
bounds for neural networks. In Advances in Neural Information Processing Systems.
Cho, K., Van MerriEnboer, B., GULCEHre, C., Bahdanau, D., Bougares, F., Schwenk,
H. and Bengio, Y. (2014). Learning phrase representations using rnn encoder-decoder for sta-
tistical machine translation. arXiv preprint arXiv:1406.1078.
Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W. F. and Sun, J. (2016a). Doctor ai:
Predicting clinical events via recurrent neural networks. In Machine Learning for Healthcare
Conference.
Choi, E., Schuetz, A., S tewart, W. F. and Sun, J. (2016b). using recurrent neural network
models for early detection of heart failure onset. Journal of the American Medical Informatics
Association, 24 361-370.
Dasgupta, B. and Sontag, E. D. (1996). Sample complexity for learning recurrent perceptron
mappings. In Advances in Neural Information Processing Systems.
Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,
S., Saen ko, K. and Da rrell, T. (2015). Long-term recurrent convolutional networks for
visual recognition and description. In Proceedings of the IEEE conference on computer vision
and pattern recognition.
Golowich, N., Rakhlin, A. and Shamir, o. (2017). Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541.
GRAVES, A. (2012). Sequence transduction with recurrent neural networks. arXiv preprint
arXiv:1211.3711.
Graves, A., FERNANdez, S., Gomez, F. and Schmidhuber, J. (2006). Connectionisttemporal
classification: labelling unsegmented sequence data with recurrent neural networks. In Proceed-
ings of the 23rd international conference on Machine learning. ACM.
Graves, A., Mohamed, A.-r. and Hinton, G. (2013). Speech recognition with deep recurrent
neural networks. in Acoustics, speech and signal processing (icassp), 2013 ieee international
conference on. iEEE.
Gregor, K., Danihelka, i., Grave s, A., Rezende, D. J. and Wierstra, D. (2015). Draw:
A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623.
Haussler, D. (1992). Decision theoretic generalizations of the pac model for neural net and other
learning applications. Information and Computation, 100 78-150.
HoCHREiTER, S. and SCHMiDHuBER, J. (1997). Long short-term memory. Neural computation,
9 1735-1780.
Huang, L., Liu, X., Lang, B., Yu, A. W. and Li, B. (2017). orthogonal weight normalization:
Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv
preprint arXiv:1709.06079.
Jozefowicz, R., Zaremba, W. and Sutskever, i. (2015). An empirical exploration of recur-
rent network architectures. in International Conference on Machine Learning.
Karpathy, A. and Fei-Fei, L. (2015). Deep visual-semantic alignments for generating image
descriptions. in Proceedings of the IEEE conference on computer vision and pattern recognition.
10
Under review as a conference paper at ICLR 2019
KOIRAN, P. (1998). Vapnik-chervonenkis dimension of recurrent neural networks. Discrete Applied
Mathematics, 86 63-79.
Krizhev s ky, A., Sutskever, I. and Hinton, G. E. (2012). Imagenet classification with deep
convolutional neural networks. In Advances in neural information processing systems.
Ku, C.-C. and Lee, K. Y. (1995). Diagonal recurrent neural networks for dynamic systems control.
IEEE transactions on neural networks, 6 144-156.
Le, Q. V., Jaitly, N. and Hinton, G. E. (2015). A simple way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941.
Lee, C.-H. and Teng, C.-C. (2000). Identification and control of dynamic systems using recurrent
fuzzy neural networks. IEEE Transactions on fuzzy systems, 8 349-366.
Li, X., Lu, J., Wang, Z., Haupt, J. and Zhao, T. (2018). On tighter generalization bound for
deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159.
Liang, M . and Hu, X. (2015). Recurrent convolutional neural network for object recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Lipton, Z. C., Kale, D. C., Elkan, C. and Wetzel, R. (2015). Learning to diagnose with lstm
recurrent neural networks. arXiv preprint arXiv:1511.03677.
MIKOLOV, T., Karafiat, M., Burget, L., CERNOCky, J. and KHUDANpur, S. (2010). RecUr-
rent neural network based language model. In Eleventh Annual Conference of the International
Speech Communication Association.
MOHRI, M., ROSTAMIZADEH, A. and TALWALKAR, A. (2012). Foundations of machine learning.
MIT press.
Ne yshabur, B., Bhojanapalli, S., McAllester, D. and Srebro, N. (2017). A pac-
bayesian approach to spectrally-normalized margin boUnds for neUral networks. arXiv preprint
arXiv:1707.09564.
Ne yshabur, B., Salakhutdinov, R. R. and Srebro, N. (2015). path-sgd: path-normalized
optimization in deep neUral networks. In Advances in Neural Information Processing Systems.
Ne yshabur, B., Tomioka, R. and Srebro, N. (2014). In search of the real indUctive bias: On
the role of implicit regUlarization in deep learning. arXiv preprint arXiv:1412.6614.
Pinheiro, p. H. and Collobert, R. (2014). RecUrrent convolUtional neUral networks for scene
labeling. In 31st International Conference on Machine Learning (ICML). EpFL-CONF-199822.
Saxe, A. M., McClelland, J. L. and Ganguli, S. (2013). Exact solUtions to the nonlinear
dynamics of learning in deep linear neUral networks. arXiv preprint arXiv:1312.6120.
S oudry, D., Hoffer, E. and Srebro, N. (2017). The implicit bias of gradient descent on sepa-
rable data. arXiv preprint arXiv:1710.10345.
Sutskever, I., Vin yals, O. and Le, Q. V. (2014). SeqUence to seqUence learning with neUral
networks. In Advances in neural information processing systems.
Vorontsov, E., Trabelsi, C., Kadoury, S. and Pal, C. (2017). On orthogonality and learning
recUrrent networks with long term dependencies. arXiv preprint arXiv:1702.00071.
Xie, D., Xiong, J. and Pu, S. (2017). All yoU need is beyond a good init: Exploring better solUtion
for training extremely deep convolUtional neUral networks with orthonormality and modUlation.
arXiv preprint arXiv:1703.01827.
XINGJIAN, S., CHEN, Z., WANG, H., YEuNG, D.-Y., WONG, W.-K. and WOO, W.-C. (2015).
ConvolUtional lstm network: A machine learning approach for precipitation nowcasting. In Ad-
vances in neural information processing systems.
11
Under review as a conference paper at ICLR 2019
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R. and
Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention.
In International Conference on Machine Learning.
Yoo, S. J., Park, J. B. and Choi, Y. H. (2006). Adaptive dynamic surface control of flexible-
joint robots using self-recurrent wavelet neural networks. IEEE Transactions on Systems, Man,
and Cybernetics, PartB (Cybernetics), 36 1342-1355.
Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O. (2016). Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.
Zhang, J., Lei, Q. and Dhillon, I. S. (2018). Stabilizing gradients for deep neural networks via
efficient svd parameterization. arXiv preprint arXiv:1803.09327.
Zhou, G.-B., Wu, J., Zhang, C.-L. and Zhou, Z.-H. (2016). Minimal gated unit for recurrent
neural networks. International Journal of Automation and Computing, 13 226-234.
12
Under review as a conference paper at ICLR 2019
A Proofs in Section 2
A.1 LIPSCHITZ Continuity of M and 'γ
We show the LiPschitz continuity of the margin operator M and the loss function 'γ in the following
lemma.
Lemma 5. The margin operator M is 2-Lipschitz in its first argument with respect to vector Eu-
clidean norm, and 'γ is Y-Lipschitz.
Proof. Let y, y0 and z be given, then
M(y, z) - M (y0, z) = yz - yz0 + max yj0 - max yj
j 6=z	j6=z
≤ yz - yz0 + max yj0 - yj
j 6=z
≤2ky-y0k∞≤ky-y0k2.
For function 'γ, it is a piecewise linear function. Thus, it is straightforward to see that 'γ is Y -
Lipschitz.	□
A.2 Proof of Lemma 2
Proof. The Lemma is stated with matrix Frobenius norms. However, we can show a tighter
bound only involving the spectral norms of weight matrices. Given weight matrices U, V, W and
U0, V 0, W0, consider the t-th outputs yt and yt0 of vanilla RNNs,
kyt - yt0k2 = kσy(Vht) - σy(V0h0t)k2
≤ ρy kV ht - V 0ht + V 0 ht - V 0h0t k2
≤ ρy (k(V - V 0)htk2 + kV 0(ht - h0t)k2)
≤ ρy (kht k2 kV - V 0 k2 + BV kht - h0t k2 ) .	(5)
We have to bound the norm of ht as in the following lemma.
Lemma 6. Under Assumptions 1 to 3, for t ≥ 0, the norm of ht is bounded by
kht k2 ≤
min /b√d,ρhBwBx (PhBU)'- 1 J .
ρhBU-1
(6)
Proof. We prove by induction. observe that for t ≥ 1, we have
khtk2 = kσh(Wxt + Uht-1)k2
≤ ρh kWxt + Uht-1k2
≤ ρh (kWxtk2 + kUht-1k2)
≤ ρh (BW Bx + BU kht-1k2) .	(7)
Applying equation (7) recursively with h0 = 0, we arrive at,
IlhtIl2 ≤ PhBWBx X^(,Pth,Bu)j = PhBWBx (PhfU)一J,
j=0	ρhBU-1
We also have Iht I∞ ≤ b. Thus, combining with the above upper bound, we get Iht I2 ≤
min {b√d,ρh.BwBx (PhBU--1 } . Clearly, ∣ho∣∣2
0 satisfies the upper bound.
□
When PhBU = 1, the ratio is defined, by L’Hospital’s rule, to be the limit,
(PhBU)t - 1
lim ------K----- = t.
ρhBU→1 Ph BU - 1
13
Under review as a conference paper at ICLR 2019
With Lemma 6 in hand, we plug the bound (6) into equation (5) and end up with
Ilyt - yt k2 ≤ Py PhBW Bx (Ph TU~~kV - V 0k2 + Py BVkht - htk2 ∙	(8)
ρhBU - 1
The remaining task is to bound kht - httk2 in terms of the spectral norms of the difference of weight
matrices, kW - Wtk2 and kU - U tk2.
Lemma 7. Under Assumptions 1 to 3, for t ≥ 1, the difference of hidden states ht and htt satisfies
kht - httk2 ≤ LW,t kW - Wtk2 + LU,t kU - Utk2 ,
where LWt = PhBx ShBU>-1 and LUt = P2BWBxt(PhB)2-1.
W,t h x ρh BU -1	U,t h W x (ρh BU )-1
Proof. Similar to the proof of Lemma 6, we use induction.
kht - ht∣∣2 = I∣σh (Wxt + Uht-I)- σh (W0xt + U0ht-1) ∣∣2
≤ Ph (W - Wt)xt + U ht-1 - Uthtt-1 2
≤ Ph (II(W - WO)Xtk2 + ||Uht-I- Utht-1∣∣2)
≤ Ph (Bx kW - Wt∣∣2 + ∣∣Uht-I- U0ht-1 + U0ht-1 - U0ht-1 H2)
≤ PhBx kW - W0k2 + Ph (kht-1k2 IIU - Ulb + BU Hht-1 - ht-1∣2).
Repeat this derivation recursively, we have
kht - h0tk2 ≤ PhBx kW - W0k2 + Ph kht-1 k2 kU - U0k2 + PhBU ∣∣ht-1 - h0t-1 ∣∣2
≤ PhBx (1 + PhBU) kW - W0k2 + Ph (kht-1k2 + PhBU kht-2k2) kU - U0k2
+ (PhBU)2 ∣∣ht-2 - h0t-2 ∣∣2
≤ ......
t-1	t-1
≤ PhBx X (PhBu)j kW - W0k2 + Ph X ((PhBU)t-1-j khjk2) k。- U他
j=0	j=0
+ (PhBU)t kh0 - h00k2
≤ PhBx
(PhBU)t - 1
PhBU - 1
t-1
kW - W0k2 + Ph X((PhBU)t-1-j khjk2) kU - U0k2 .(9)
j=0
We now plug in the upper bound (6) to calculate the summation involving the Euclidean norms of
the hidden state ht .
t-1	t-1	t-1
X(Ph BU)t-1-j khj k2 ≤ X(j + 1)(PhBU)jPhBWBx ≤ t X(PhBU)jPhBWBx
j=0	j=0	j=0
≤ PhBwBxt (PhBU)t - 1.
PhBU - 1
Plugging back into equation (9), we have as desired,
kht - htk2 ≤ PhBx (PhBU)t - 1 kW - W0k2 + PhBWBxt (PhBU)t- 1 kU - U0k2 ∙
PhBU - 1	PhBU - 1
□
Combining equation (8) and Lemma 7, and ∣∣W ∣f ≥ ∣∣W k2,we immediately get Lemma 2.	□
14
Under review as a conference paper at ICLR 2019
A.3 Proof of Lemma 3
Proof. Our goal is to construct a covering C (Ft, e, dist(∙, ∙)), i.e., for any ft ∈ Ft, there exists
fbt ∈ Ft, for any input data (xt)tT=1, satisfying
sXup ft (Xt) - fbt (Xt)2 ≤ e.
Note that f is determined by weight matrices U, V and W. By Lemma 2, we have
sXupf(Xt)	-fbt(Xt)2 ≤	LV,t	V -Vb	+ LW,t W	- Wc	+LU,tU-Ub	.
Then it is enough to construct three matrix coverings, C (u, , ∣∣∙∣∣f), C(V, 3^77, I∣∙∣∣f) and
C (w, 3Lw t, ∣∣∙∣∣f). Their Cartesian product gives US the covering C (Ft, e, dist(∙, ∙)). The following
lemma gives an upper bound on the covering number of matrices with a bounded Frobenius norm.
Lemma 8. Let G = A ∈ Rd1 ×d2 : kAk2 ≤ λ be the set of matrices with bounded spectral norm
and e > 0 be given. The covering number N(G, e, |卜|山)is upper bounded by
N(G,e, k∙kF) ≤ (l + 2min {√d1, H λ
!d1 d2
Proof. For any matrix A ∈ G, we define a mapping φ : Rd1 ×d2 7→ Rd1d2, such that φ(A) =
[A:>,1, A:>,2, . . . , A:>,h]>, where A:,i denotes the i-th column of matrix A. Denote the vector space
induced by the mapping φ by V(G) = {φ(A) : A ∈ G}. Note that we have kAkF2 = Pih=1A:>,iA:,i =
kφ(A)k22 and the mapping φ is one-to-one and onto. By definition, the square of Frobenius norm
equals the square of sum of singular values and the spectral norm is the largest singular value. Hence,
the equivalence of Frobenius norm and spectral norm is given by the following inequalities,
kAk2 ≤ IAIf ≤ min {两,Pd2} kA∣∣2∙
Now, We see that if We construct a covering C (V (G ),e, k∙∣∣2), then φ-1C(V (G),e, k∙∣∣2)=
{φ-1 (V) : V ∈ C (V (G), e, k∙∣∣2)} is a covering of G at scale e with respect to the matrix Frobe-
nius norm. Therefore, we get N(G, e, ∣∙∣f) ≤ N(V(G), e, k∙∣∣2). As a consequence, it is suffices
to upper bound the covering number of V(G). In order to do so, we need another closely related
concept, packing number.
Definition 2 (Packing). Let G be an arbitrary set and e > 0 be given. We say P(G, e, ∣∙∣) is a
packing of G at scale e with respect to the norm ∣∣∙∣, if for any two elements A, B ∈ P, we have
IA - BI > e.
Denote by M(G,e, ∣∙∣) the maximal cardinality of P (G, e, ∣∙∣).
By the maximality, we can check that N (C, e, ∣∙∣) ≤ M(C, e, ∣∙∣). Indeed, let P *(G ,e,∣∙∣) be a
maximal packing. Suppose there exists A ∈ G such that for any B ∈ P*(G, e, ∣∙∣), the inequality
∣∣A 一 Bk > e holds. Then we can add A to P*(G, e, ∣∙∣), while still keeping it being a packing,
which contradicts the maximality of P *(G, e, ∣∙∣). Thus, we have N (G, e, ∣∙∣) ≤ M(G, e, ∣∙∣).
Observe that V(G) is contained in an Euclidean ball B(0; R) ∈ Rd1d2 of radius at most
R = maχkφ(A)∣2 ≤ min ∣pdι, pd2∣ ∣∣A∣∣2 ≤ min ∣pdι, pd2∣ λ.
Additionally, the union of Euclidean balls B(V; e/2) ⊂ Rd1d2 with radius e/2 and center V ∈
P (V (G ),e, ∣∣∙∣∣2) is further contained in an Euclidean ball B(0; Re) of slightly enlarged radius
Re = min {√¾ √d2} λ + e/2. Those balls B(v; e/2) are disjoint by the definition of packing,
thus we have
N (V (C ),ej⅛) ≤P (V(C),e,k⅛) ≤
vol(B(0,Re))
vol(B(v; e/2))
d1d2
1+2
min{√d!, √d2}λ
e
d1 d2
where vol(∙) denotes the volume.
□
15
Under review as a conference paper at ICLR 2019
By Lemma 8, we can directly write out the upper bounds on the covering numbers of weight matri-
ces,
N & 3⅛,
N (V, 3⅛，
N (W，3⅛
Νf)≤ (l + 6*星广
HA ≤ (i + 6min{Pdy，√dhBvLV,t Yydh, and

MkF)≤ (l + 6mm{√dx' √dh}BWLW Yxdh

Then we immediately have,
N (Ft, e，dist(∙, ∙)) ≤ N(U，3Lutt，∣U∣f) X N (V，M，IMIf) X N (W，3^，IMIf)
≤ A +6√dhBULu,t∖h 1+ + min{6pdy，√dh}BvLv,t! y
e
e
X(1+ 6min{3, √dh}BwLw,t Adxdh
e
Substituting the coefficients LU,t ， LV,t and LW,t from Lemma 2, we get
N(Ft，e，dist(・，∙))
≤ ^ι + 6√dρyρhBvBWBx ((PhBU--
3d2
(6c√dt 3B”-1 ∖
≤ 1 + —-PhBU-I	，
1 + 6√,ρyρlBu BV BW Bxt (PhBU--1
e
where c = ρy ρhBV BW Bx max {1， ρhBU}. For future usage, we also write down for small e > 0,
6c√dt (PhBU)t-1
such that-------PhBUT > 1, the logarithm of covering number satisfies,
(i2c√dt (PhBU )t-1 ∖
log N (Ft，e，dist(・，∙))≤ 3d2 log I V	I .
□
A.4 Proof of Lemma 4
Proof. Define FM,t = {(Xt， zt) 7→ M(ft(Xt)， zt) : ft ∈ Ft}. By Lemma 5, we see that M is
2-Lipschitz in its first argument. In order to cover FM,t at scale e, it suffices to cover Ft at scale f.
This immediately gives US the covering number N (Fm』，e，k∙∣∣∞) ≤ N (Ft，e/2, dist(・，∙)).
We then give the statement of Dudley’s entropy integral.
Lemma 9. Let H be a real-valued function class taking values in [-r， r] for some constant r, and
assume that 0 ∈ H. Let S = (s1， . . . ， sm) be given points, then
RS(H) ≤ inf
α>0
,log N(H，e，k-k) de
The proof can be found in Bartlett et al. (2017). Taking H
FM,t takes values in [-r， r] with r = ρyBV Iht I2 ≤ ρyBV min
FM,t, we can easily verify that
{b√d，PhBwBx (PhBU)--1O and
16
Under review as a conference paper at ICLR 2019
0 ∈ FM . Thus, directly applying Lemma 9 yields the following bound,
RS(FM,t) ≤ α>f0 √m+ + m Z	q∕logN(FM,t,e, k∙k∞)de).
We bound the integral as follows,
2 r √m /- /
l	,logN(FM,t,e, k∙k∞)de ≤ /
αα
∙2r √m
≤ 2r
∖
24c√dt (PhB)t-1 ∖
3d2 log --------ephBUτ	de
24c√dt ShB)t-1
3d2 log -----PhBUT
α
Picking α = √= is enough to give Us an upper bound on RS(Fm,/,
RS(FM) ≤ -4 + %"r log (24c√dmt(PhBU二1).
m m	ρhBU-1
Finally, by Talagrand,s lemma (Mohri et al., 2012) and 'γ being Y -Lipschitz, We have
4
≤——
mγ
RS(Fγ,t) ≤ YRS(FM,t)
3d2r2 log 224c√dmt phh^U^--
ρh BU- 1
□
B Proof in Section 4
B.1 Proof of Theorem 3
Proof. Under additional Assumption 4, We only need to shoW that, With the additional matrix in-
duced norm bound, We have a refined upper bound on the matrix covering number. The proof relies
on the folloWing lemma adapted from Bartlett et al. (2017) Lemma 3.2.
Lemma 10. Let G = A ∈ Rd1 ×d2 : kAk2,1 ≤ λ . We have the folloWing matrix covering upper
bound
log N (G,e, k∙k2) ≤ W log(2d1d2).
The above Lemma is a direct consequence of Lemma 3.2 in Bartlett et al. (2017) With X being
identity, a = λ, b = 1, and m= d1, d = d2. We apply the same trick to split the overall covering
accuracy e into 3 parts, ,	, and 3^w t, corresponding to U, V, W respectively. Then we
derive a refined bound on the covering number of Ft :
logN(Ft,e, dist(∙, ∙)) ≤ — MVLV,t + MWLW't)log(2d2),	(10)
where d = max {dx, dy, dh}. Substituting (10) into the Dudley integral as in the proof of Lemma 4
yields
RS(FM,t) ≤ inf
α>0
12 r2r√m ________ ∖
+ m /	PiogWtTWOde .
We bound the integral as follows,
∕∙2r√m __________
plog N (Ft
α
/2 kk )d ≤ r2r√m 36 JMULU,t + MVLV,t + MWLW,t pl	(2d2)d
αe
=36 ∕ MU LU,t + MV LV,t + MW LW,t plog(2d2)log —α^~.
17
Under review as a conference paper at ICLR 2019
Choosing α = √m yields
Rs (FMM) ≤ m + √m q Mu Lu,t + mv LV,t + mw LW,t Plog(2d2) log (2m√d).
Finally, substituting the Lipschitz constant LU,t, LV,t , LW,t into the expression, we have
RS(FY,t) ≤ YRS(FM,t) ≤ mγ + γ√m qMULu,t + MVLV^,t + MWLW,t plog(2d2) log (2m√d)
α max{Mu, MV, MW}t ββ--1
≤O
γ√m
Combining with Lemma 1 completes the proof.
Under additional Assumption 5, our proof is based on the following result from Lemma 1 in
Neyshabur et al. (2017).
Lemma 11. Let fα (x) : X → Rd be any predictor with parameter α, and P be any distribu-
tion on the parameter that is independent of training data. Then, for any γ, δ > 0, with proba-
bility at least 1 - δ over the training set of size m, for any α and any random perturbation β s.t.
Pe [maXχ∈χ ∣fα+β (x) - fα (x)∣∞ < 4] ≥ 1, We have
R0 (fa)-RY (fɑ) ≤ A/ — + Y*-,
Where KL (α + β kP) is KL divergence of distributions α + β and P.
For convenience, We omit the superscript for sample index. Denote ht (α) and ht (α + β) as the
hidden variables With parameters α and α + β respectively. Then We provide an upper bound of the
gap of hidden layers before and after the perturbation. Denote the parameters α = vec ({W, U, V })
and the perturbation β = vec ({δW, δU, δV }).
For any t ∈ {1, 2, . . . , T }, We have
kht (α + β) - ht (α)k2
(i)
≤ ρh k(U+δU)ht-1(α+β)+(W+δW)xt-Uht-1(α)-Wxtk2
(ii)
≤ ρhBU kht-1 (α + β) - ht-1 (α)k2 + δρhBU kht-1 (α + β)k2 + δρhBxBW
t	t-1
≤ (ρhBU)t kh0 (α + β) - h0 (α)k2 + δ X(ρhBU)i h(t-i) (α + β)2 + δρhBxBW X(ρhBU)i,
i=1	i=0
(11)
By Lemma 6, We have that for any t ≤ T ,
Ilht (α)∣∣2 ≤ min [b√P,phBχBw (PhBU)~~-1 ∖ = λ%.	(12)
ρhBU - 1
Combining (11), (12), and h(0) = 0, We have
t	t-1
h(t) (α + β) - h(t) (α)2 ≤ δλht X(ρh BU)i + δρhBxBW X(ρh BU)i
i=1	i=0
≤ δ (A/PhBU + PhBxBW) (Phg)__-ɪ.	(13)
t	ρhBU - 1
Denote yt (α) and yt (α + β) as the out With parameters α and α + β respectively. Then We have
(	i)
y(t) (α+β) - y(t) (α)2 ≤ Py I(1+ δ)Vht (α+β) - Vht (α)I2
≤	PyBV Iht (α + β) - ht (α)I2 + δPy BV Iht (α + β)I2
≤	δρyBV (λhtPhBU + PhBxBW) (Ph /- --------+ δρyBVλh, (14)
PhBU - 1
18
Under review as a conference paper at ICLR 2019
where (i) is from Lipschitz continuity of σy and (ii) is from (12) and (13).
Then choosing the prior distribution and the perturbation distribution as N 0, σ2I , and from the
concentration result for the spectral norm bounds, we have
PA〜N(0,σ2Id×d) [kAk2 >ξ] ≤ 2peχp (2d^) ∙
This implies with probability at least 1/2, We have max {δBu,δBw,δBv} ≤ σ/2dln (12d).
Taking σ = (γ∕4ρy ((1仅PhBU + PhBxBW) (PhBU)--1 + λ%j、2dln(12d)) and combining
with (14), with probability at least 1/2, we have
max kyt (α+β) - yt (α)k2
x∈Xm
≤ ((λhtPhBU + PhBxBW) S" --------；--+ λht) ∙ σP2dln (12d) ≤ "7∙
t	PhBU - 1 t	4
Finally, we calculate the KL divergence of P and α + β with respect to this choice of σ,
KL(α + βkP) ≤⅛0∣
2σ2
=O (多•卜 λht PhBU + PhBxBW ) SP BUU 1-------+ λht) d ln (d) (BU ,F + BW,F + BV,F))
Py (λht PhBU + PhBxBW)2 (βt - 1)2 P ln (P)(BU,F + bW,F + BV,f)
=(	Y2 (β -1)2	..
We complete the proof by applying Lemma 11.	□
C Proofs in Section 5
C.1 Proof of Theorem 4
Proof. We use the same argument from the analysis of vanilla RNNs to investigate the Lipschitz
continuity of MGU RNNs. Consider ht and h0t computed by different sets of weight matrices.
kht- h0t k2 = (1 - rt) ht-1 + rt eht- (1- rt0 ) h0t-1 - rt0 eh0t 2
≤ (rt0 -	rt)	h0t-12 + (1 -	rt)	(ht-1 -	h0t-1)2 + (rt - rt0) eh0t2 + rt	(eht-	eh0t)2
≤ krt0 -	rtk2	h0t-1∞ + k1-	rtk∞	ht-1 -	h0t-12 + krt- rt0k2 eh0t∞	+ krtk∞	eht -	eh0t2
τ-<	t .1	∙	。丁、τ,,l,	♦	J	I Il Il	/r El	1	II 7 II	，r
Expand the expression of ht. Note that rt is nonnegative, and krtk∞ ≤ 1. Then we have khtk∞ ≤ 1.
Additionally tanh(∙) is I-LiPschitz. Thus we get
eht - eh0t2 ≤	Uh(ht-1 rt)	+ Whxt- Uh0 (h0t-1	rt0)-	Wh0xt2
≤ Uh(ht-1	rt)- Uh0 (h0t-1 rt0)2 + Bx kWh-	Wh0 k2
≤	kUh-	Uh0 k2	kht-1	rtk2 + BUh krtk∞	ht-1 -	h0t-1 2 + BUh	h0t-1	∞	krt-	rt0 k2
+ Bx kWh- Wh0 k2
≤ khtk2 kUh- Uh0 k2 + BUhkrtk∞ ht-1 - h0t-12 + BUh krt - rt0 k2 + Bx kWh- Wh0 k2.
We have to expand rt- rt0 as follows,
krt- rt0 k2 = Wr xt + Urht-1- Wr0xt- Ur0 h0t-1 2
≤ Bx kWr- Wr0k2 + BUr ht-1 - h0t-1 2 + kh0t-1k2kUr- Ur0k2.
19
Under review as a conference paper at ICLR 2019
We also need to bound kht k2 ,
khtk2 ≤ k1 - rtk∞ kht-1k2 + krtk∞ eht2
≤ k1 - rtk∞ kht-1k2 + krtk∞ (BWh Bx + BUh krtk∞ kht-1k2)
= k1 - rtk∞ +BUh krtk2∞ kht-1k2 +BWhBx,
≤ mj≤atxnk1 -rjk∞ +BUh krjk2∞o kht-1k2 +BWhBx.
Applying the above inequality recursively and remember kht k∞ ≤ 1, we get khtk2 ≤
min {√d, β-1 BWh Bχ} with β = maχj≤t {k1-rjk∞ + BUh krj k∞ }.
gredients together, we have
Put all the above in-
kht - htk2 ≤ (β + 2BUr + BUr BUh) Ilht-I- ht-lll2
+ √d kUh - Uh k2 +	Bx kWh - Wh k2
+ (2 + BUh) √d IIUr	- Urk2 + (2Bx	+	BUh	Bx)	kWr	- Wrk2 ∙
Apply the above inequality recursively, denote by θ = β + 2BUr + BUr BUh, we have
tt
kht - htk2 ≤√dX θj kUh - Uhk2 + Bx X θj kWh - Whk2
j=1	j=1
tt
+ (2√d + BUh √d X θj kUr - Urk 2 + (2Bx + BUh BX) X θj kWr - Wr k2 .
j=1	j=1
We then derive the Lipschitz continuity of kyt k2,
kyt - y0k2 ≤ PyBV kht - htk2 + Py√dkV - V0k2
二θt - 1	θt - 1
≤ PyBV√d~h-f kUh - Uhk2 + PyBVBXTj-f kWh - Whk2 + Py√dkV - V k2
θ-1	θ-1
(L	L、θt — 1 ..	...	θt — 1 ..	...
2 ʌ/d + BUh ʌ/d)/	kUr - Ur k2 + Py BV (2Bx + BUh BX)	kWr - Wr k 2 .
θ-1	θ-1
Following the same argument for proving the generalization bound of vanilla RNNs, we can get the
generalization bound for MGU RNNs as
dPy BV min n √d, BWh Bx β-1 O 1 /log (d√mθ-1 )	八 1 \
P(Zt = Zt) ≤ RI(ft) + O ---7=——Y---------- + Jq I .
mγ	m
\ /
□
C.2 Proof of Theorem 5
Proof. We first bound the norm of ht as follows,
khtk2 ≤ kotk∞ktanh(ct)k2 ≤ kotk∞kctk2
≤ kgtk∞kct-1k2+ krtk∞kcetk2
≤ kgtk∞kct-1k2 + krtk∞ (BWc Bx + BUc kht-1k2)
≤ kgtk∞kct-1k2+ krtk∞(BWcBx+BUckotk∞kct-1k2)
≤ (kgtk∞ + krt k∞ kotk∞BUc) kct-1k2 + BWcBx.
By applying the above inequality recursively, we have kht k2 ≤ I∣ctk2 ≤ BWcBXe-, where
β = maxj≤t {kgjk∞ + krjk∞kθjk∞BUc}. We also have ∣∣htk2 ≤ √d. Thus, PUt together, we
20
Under review as a conference paper at ICLR 2019
have IlhtIl2 ≤ min {√d, BWCBXe-}. Next, we investigate the Lipschitz continuity of ht.
Ilht- ht∣∣2 ≤ IIot G) tanh(ct) - o[ Θ tanh(ct)∣∣2
≤ l∣ot - 0tl∣2∣∣tanh(Ct)I∣∞ + l∣otk∞kct - ctk2
We have to expand ot - ot,
∣∣ot - 0tll2 ≤ BXkWo- Woll2 + BUokht-I- ht-ιl∣2 + Ilht-1II2IIUo- Uol12∙
NOtethatkBuo 12 is usually small, ot and o[ are close, and we have ∣∣ht-ι -ht-∕∣2 ≤ ∣∣ot∣∣∞∣∣ct-ι -
Ct-Ik2 ≤ ∣∣ct-ι - Ct-Ik2. Thus, we can derive
∣∣ot - otl∣2 ≤ BXkWo- Wo k2 + BUokCt-I- Ct-Ik2 + √d∣∣uo - u4∣2∙
We also expand Ct - Ct to get,
kCt-Ctk2 ≤kCt-ιk∞kgt-g0k2 + MUCt-I-Ct-1k2 + FtIUrt-r0k2 + 忱k∞Kt-etk2.
We also have,
ket - etk2 ≤ BUCkht-I- ht-1l∣2 + ∣∣ht-1∣∣2∣∣uc - Uck2 + BXkWC- Wc∣∣2,
kgt - g01∣2 ≤ BXkWg- Wg ∣∣2 + BWgkht-I- ht-1k2 + √dkUg - Ugk2,	and
∣∣rt - r0 ∣∣2 ≤ BXkWr - Wrk 2 + BWrkht-I- ht-1∣∣2 + √dkUr - U； ∣∣2∙
Putting together, we get
kCt - Cot k2
≤ Bx (|Wc - WCk2 + IIWg - Wg ∣∣2 + kWr -卬；|12)
+ √d (|Uc - UCk2 + IIUg - Ug ∣∣2 + IIUr - Ur |12)
+ kgt∣∣∞ ∣∣Ct-1 - ct-1∣∣2 + (∣∣rt∣∣∞BUC + BUg + BUr) kht-1 - ht-1l∣2
≤ BX (|Wc - WCk2 + kWg - Wg ∣∣2 + kWr - Wr ∣∣2 + (BUC + BUg + BUr )||Wo - Wok2)
+ √d (∣uc - UC∣2 + kUg - Ug ∣∣2 + kUr - Urk2 + (BUC + BUg + BUr )IIUo - Uok2)
+ (kot∣∣∞∣∣rt∣∣∞BUC + BUg + BUr+ BUo) ∣∣Ct-1 - ct-1∣∣2.
By induction, we have
kCt - Cotk2
θt — 1 ,..	....... .............. ...	...
≤ BX 万一丁 (∣wc - WCk2 + kWg - Wg ∣∣2 + kWr - Wrk2 + (BUC + BUg + BUr )||Wo - Wok2)
θ 一 1
Lθt — 1 ,..	....... .............. ...	...	∙..、
+ √d^Z-----f (∣uc	-	uc∣2 + kUg	-	Ug	∣∣2 + kUr	-	Ur l∣2 + (BUC	+	BUg	+ BUr)IIUo	-	Uok2),
θ 一 1
where θ = β + BUg + BUr + BUo. Now we immediately have
kht - hot k2
θt - 1
≤ BX ^θ―丁 (kWc - WCk2 + kWg - Wg ∣∣2 + kWr - Wr ∣2 + (BUC + BUg + BUr )∣∣Wo - Wok2)
θ-1
Lθt — 1	,..	....... ............. ...	...	....
+ √d^Z----f	(∣uc	- uc∣2 + kUg	-	Ug ∣∣2 + kUr -	Ur ∣∣2 +	(BUC + BUg	+ BUr)||Uo	-	Uok2)∙
θ-1
Then the Lipschitz continuity of yt can be written as
kyt - y0 ∣∣2 ≤ PyBV ∣∣ht - ht∣2 + Py √dkV - V 1∣2.
Following the same argument for proving the generalization bound of vanilla RNNs, we can get the
generalization bound for LSTM RNNs as
一 ^
P (≡t = Zt) ≤Rγ(ft) + O
Jpy BV min
∖
{ √d, BWC bx β--1 O jlθg (d√mθ⅛)
√mγ
□
21
Under review as a conference paper at ICLR 2019
D Extension to Convolutional RNNs
We further extend our analysis to Convolutional RNNs (Conv RNNs). Conv RNNs integrate convo-
lutional filters and recurrent neural networks. Specifically, we consider input x ∈ Rd and k-channel
k-dimensional convolutional filters I1, . . . ,Ik ∈ Rk followed by an average pooling layer over the
k channels for reducing dimensionality. Extensions to convolution with strides and other kinds of
average pooling layers (e.g., blockwise pooling) are straightforward.
Here we denote the circulant-like matrix gen-
erated by Ii as
Ci
Γ I>	0........0-
i X------{-----}
d-k
0 I>	0.....0
i V-----{---}
d—k — 1
.	..	.
.	.	.
0......0	I>	0
'—{—}	i
d—k—1
0 ....... 0	Ii>
'----{----} i
d—k
∈ R(d-k + 1)×d
WI
Figure 5: Illustration of input x ∈ R6 convolving
with 3-channel 3-dimensional convolutional filters
I1 , I2, and I3, followed by an average pooling.
and write WI = [C>,...,C>]>. We further denote P = 1 [Id—k+i Id—k+i •-Id—k+i] where
、	{Z	}
totally k identity matrices
Id denotes the d-dimensional identity matrix. Define I = [Ii,..., Zk], and I * X = PWIx. Given
a sample (xt, zt)tT=i, the Conv RNNs compute ht and yt as follows,
ht = σh (U * ht—i + W * xt) , and yt = σy (V * ht) ,
where ht , xt ∈ Rd, and U , V, W ∈ Rk×k are matrices with column vectors being k-dimensional
convolutional filters. We use zero-padding to ensure the output dimension of convolutional filters
matches the input (Krizhevsky et al., 2012). To get yt, we convolve ht with V followed by an
average pooling to reduce the dimension to K . Since we aim to show that Conv RNNs reduce the
dependence on d in generalization through parameter sharing, we simplify the notations to assume
h0 = 0, and impose the following assumption. Extensions to general settings are straightforward.
Assumption 8. The activation operators σh and σy are 1-Lipschitz with σh (0) = σy (0) = 0. σh
is entrywise bounded by 1. The convolutional filters U, V, and W are orthogonal with normalized
columns, i.e., U>U = UU> = ɪIk, V>V = VV> = ɪ Ik, and W> W = WW> = ɪ Ik.
We remark that the orthogonality constraints enhance the diversity among convolutional filters (Xie
et al., 2017; Huang et al., 2017). Additionally, the normalization factor 1 is to control the spectral
norms ofWU, WV, and WW, which prevents the blowup of hidden state. Denote by Fc,t the class of
mappings from the first t inputs to the t-th output computed by Conv RNNs. Then the generalization
bound is given in the following theorem.
Theorem 6. Let activation operators σh and σy be given, and Assumptions 1 and 8 hold. Then for
(xt, zt)tT=i and S = (xi,t, zi,t)tT=i, i = 1, . . . , m drawn i.i.d. from any underlying distribution
over Rd×T × {1, . . . , K}, with probability at least 1 - δ over S, for every margin value γ > 0 and
every ft ∈ Fc,t for integer t ≤ T , we have
P (zet 6= zt) ≤ Rbγ(ft)+O
Bxk vzlog(dt√m)
√mγ
The detailed proof is provided in D.1. Similar to the analysis of vanilla RNNs, our proof is based
on the Lipschitz continuity of Conv RNNs with respect to its model parameters in the convolutional
filters. Specifically, by Assumption 8, the spectral norms of WU, WV, and WW are all bounded by
1. Combining with the inequality, kW∣∣f ≤ √d∣∣U∣∣F, We have ∣∣yt - y0∣∣2 ≤ Lv,t∣∣V — VIIF +
LU,tkU - U0kF + LW,tkW - W0kF, where LU,t, LV,t, and LW,t are polynomials in d and t.
Additionally, observe that the total number of parameters in a Conv RNN is at most 3k2, which is
independent of input dimension d. As a consequence, the generalization bound of Conv RNNs only
has a lieanr dependence on kand t.
22
Under review as a conference paper at ICLR 2019
D.1 Proof of Theorem 6
Proof. We first characterize the Lipschitz continuity of kyt k2 with respect to model parameters U ,
W and V . We have
kyt - yt0k2 ≤ ρykhtk2kWV - WV0k2 + ρy kWV k2 kht - h0tk2.
Since Ilhti∣∞ ≤ 1, we have IlhtIl2 ≤ √d. Then we expand ht - h1,
kht - htk2 ≤ PhIlU 0 * ht-ι + W* Xt-U 0 * ht-i - W0 * xt∣∣2
=ρhIPWUht-1+PWWxt - P WU0 h0t-1 -PWWxtI2
≤ ρh IP I2 IWU ht-1 + WWxt - WU0 h0t-1 - WW0xt I2
≤ PhIlPk2 hBxkWW - WW0 k2 + √dkWU - WU0k2 + IlWUll2kht-1 - ht-ig]
Observe that we have by the definition of circulant matrix,
IWU -WU0I22 ≤ IWU -WU0I2F= (d-k)IU-U0IF2 ≤dIU-U0IF2.
The same holds for WW - WW0 and WV - WV0. We also have IPI2 = 1. The remaining task
is to bound the spectral norm of WU and WV. Consider the matrix product WU>WU. We claim
that the diagonal elements of WU> WU is bounded by Pik=1 IUi I22, and the off-diagonal elements
are zero. To see this, denote by CUi the circulant like matrix generated by Ui . Then we have
WU = [CU> , . . . , CU> ]>. The diagonal elements of WU> WU are
kk
(WUWU)ii = X(C>jCUj). ≤ X IUiI2.
j=1	ii	i=1
By the orthogonality of U, the off-diagonal elements are
kk
(WJ WU )pq = X (CUjCUj)	= X (CUj)Ip (CU)q = 0.
j=1	pq	j=1
Thus, the spectral norm ∣∣Wu∣∣2 ≤ JPk=I IlUik2 ≤ 1, and ∣∣Wv∣∣2, ∣∣Ww∣∣2 ≤ 1 also hold. Then
we can derive
∣∣ht - ht∣∣2 ≤ PhBx√d∣∣W - W0IlF + PhdkU - U0∣∣F + Phkht-I- ht-i ∣∣2∙
Apply the above inequality recursively, we get
∣∣ht - ht∣∣2 ≤ PhBx√d -.- IIW - W0∣∣F + Phd ——- IIU - U0∣∣F
t	Ph - 1	Ph - 1
≤ Bx√dt∣W - W0IIf + dt∣U - U0∣F.
Thus, we have the following Lipschitz continuity of Iyt I2,
∣yt - y0∣2 ≤ d∣V - V0If + Bχ√⅜W - W0If + dt∣U - U0∣F.
We also bound the norm of ht by induction. Specifically, we have
IhtI2 ≤PhIPWUht-i+PWWxtI2 ≤PhIWUht-iI2+PhIWWxtI2 ≤ Iht-iI2 + Bx.
Applying the above expression recursively, We have ∣∣htk2 ≤ min{√d,Bχt} ≤ Bχt. Then fol-
lowing the same argument for proving the generalization bound of vanilla RNNs, we can get the
generalization bound for Conv RNNs as
P(et = Zt) ≤Rγ(ft)+ o( Bxkp√≡ + S .
□
23