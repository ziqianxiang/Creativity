Under review as a conference paper at ICLR 2019
Recurrent Kalman Networks:
Factorized Inference in High-Dimensional
Deep Feature Spaces
Anonymous authors
Paper under double-blind review
Ab stract
In order to integrate uncertainty estimates into deep time-series modelling,
Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learn-
ing models. Yet, such approaches typically rely on approximate inference tech-
niques such as variational inference which makes learning more complex and of-
ten less scalable due to approximation errors. We propose a new deep approach
to Kalman filtering which can be learned directly in an end-to-end manner using
backpropagation without additional approximations. Our approach uses a high-
dimensional factorized latent state representation for which the Kalman updates
simplify to scalar operations and thus avoids hard to backpropagate, computation-
ally heavy and potentially unstable matrix inversions. Moreover, we use locally
linear dynamic models to efficiently propagate the latent state to the next time
step. While our locally linear modelling and factorization assumptions are in
general not true for the original low-dimensional state space of the system, the
network finds a high-dimensional latent space where these assumptions hold to
perform efficient inference. This state representation is learned jointly with the
transition and noise models. The resulting network architecture, which we call
Recurrent Kalman Network (RKN), can be used for any time-series data, similar
to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representa-
tion of uncertainty. As shown by our experiments, the RKN obtains much more
accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs)
(Cho et al., 2014) while also showing a slightly improved prediction performance
and outperforms various recent generative models on an image imputation task.
1	Introduction
State-estimation in unstructured environments is a very challenging task as observations or measure-
ments of the environment are often high-dimensional and only provide partial information about the
state. Images are a good example: Even for low resolution, the number of pixels can quickly exceed
tens or hundreds of thousands and it is impossible to obtain any information about the dynamics,
such as velocities, from a single image. Additionally, the observations may be noisy or may not
contain useful information for the task at hand. Such noise can for example be introduced by poor
illumination or motion blur and occlusions can prevent us from observing some or all relevant as-
pects of the scene. In addition to state estimation, it is also often desirable to predict future states
or observations, for example, in order to assess the consequences of future actions. To this end, an
initial estimate of the current state is necessary which again has to be inferred from observations.
In such environments we typically also have to deal with a high uncertainties in the state estimates.
Being able to model this uncertainty is crucial in many decision making scenarios, e.g., if we need
to decide to perform an action now or wait until more information about the scene is available.
Deep learning models have been very successful for time-series modelling in unstructured environ-
ments. Classical models such as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al.,
2014) perform well but fail to capture the uncertainty of the state estimate. Recent probabilistic
deep learning approaches have used the Kalman filter (KF) as a tool to integrate uncertainty esti-
mates into deep time-series modelling (Haarnoja et al., 2016; Watter et al., 2015; Archer et al., 2015;
Fraccaro et al., 2017; Krishnan et al., 2017). These approaches use the KF to perform inference in
1
Under review as a conference paper at ICLR 2019
a low-dimensional (latent) state space that is typically defined by a deep encoder. However, using
KF in such a state space comes with two main limitations. In order to be usable for non-linear dy-
namics, we have to introduce approximations such as the extended KF (Haarnoja et al., 2016) and
variational inference methods (Krishnan et al., 2017; Fraccaro et al., 2017). Moreover, the KF equa-
tions require computationally expensive matrix inversions that are hard to scale to high dimensional
latent spaces for more complex systems and computationally demanding to fully backpropagate in
an end-to-end manner. Most of these methods are implemented as (variational) auto-encoders and
are therefore also limited to predicting future observations or imputing missing observations and can
not be directly be applied to state estimation.
We introduce the Recurrent Kalman Network, an end-to-end learning approach for Kalman filtering
and prediction. While Kalman filtering in the original state space requires approximations due to
the non-linear models as well as matrix inversions that are hard to back-propagate and computation-
ally expensive, the RKN uses a learned high-dimensional latent state representation that allows for
efficient inference using locally linear transition models and a factorized belief state representation
which avoids expensive and numerically problematic matrix inversions. Conceptually, this idea is
related to kernel methods which use high-dimensional feature spaces to approximate nonlinear func-
tions with linear models (Gebhardt et al., 2017). However, in difference to kernel feature spaces, our
feature space is given by a deep encoder and learned in an end-to-end manner.
The RKN can be used for any time-series data set for which LSTMs and GRUs are currently the state
of the art. In contrast to those, the RKN uses an explicit representation of uncertainty which gov-
erns the gating between keeping the current information in memory or updating it with the current
observation. While the RKN shows a slightly improved performance in terms of state estimation
errors, both LSTMs and GRUs struggle with estimating the uncertainty of the prediction while the
RKN can provide accurate uncertainty estimates. In relation to existing KF-based approaches, our
approach can be used for state estimation as well as for generative tasks such as image imputation.
We also show that we outperform state of the art methods on a complex image imputation task.
1.1	Related Work
Using encoders for time-series modelling of high-dimensional data such as images is a common
approach. Such encoders can also be easily integrated in well known deep time-series models such
as LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al., 2014). These models are
very effective but do not provide good uncertainty estimates as shown in our experiments. Pixel to
Torques (P2T) (Wahlstrom et al., 2015) employs an autoencoder to obtain low dimensional latent
representations from images together with a transition model. They subsequently use the models to
perform control in the latent space. Embed to Control (E2C) (Watter et al., 2015) can be seen as an
extension of the previous approach with the difference that a variational autoencoder (Kingma and
Welling, 2013) is used. However, both of these approaches are not recurrent and rely on observations
which allow inferring the whole state from a single observation. They can therefore not deal with
noisy or missing data.
Another family of approaches interprets encoder-decoder models as latent variable models that can
be optimized efficiently by variational inference. They derive a corresponding lower bound and
optimize it using the stochastic gradient variational Bayes approach (Kingma and Welling, 2013).
Black Box Variational Inference (BB-VI) (Archer et al., 2015) proposes a structured Gaussian vari-
ational approximation of the posterior, which simplifies the inference step at the cost of maintaining
a tri-diagonal covariance matrix of the full state. To circumvent this issue, Structured Inference Net-
works (SIN) (Krishnan et al., 2017) employ a flexible recurrent neural network to approximate the
dynamic state update. Deep Variational Bayes Filters (DVBF) (Karl et al., 2016) integrate general
Bayes filters into deep feature spaces while the Kalman Variational Autoencoder (KVAE) (Fraccaro
et al., 2017) employs the classical Kalman Filter and allows not only filtering but also smoothing.
Variational Sequential Monte Carlo (VSMC) (Naesseth et al., 2017) uses particle filters instead,
however they are only learning the proposal function and are not working in learned latent spaces.
Yet, these models can not be directly used for state estimation as they are formulated as generative
models of the observations without the notion of the real state of the system. Moreover, the use of
variational inference introduces an additional approximation that might affect the performance of
the algorithms. The approaches given in (Archer et al., 2015; Fraccaro et al., 2017; Haarnoja et al.,
2
Under review as a conference paper at ICLR 2019
	scale- able	state est.	uncer- tainty	noise	direct opt.
LSTM	X	=T=	x/X	X	X
-GRU-	-X-	~^Γ~	-XlX-	X	-X-
P2T	X	~^T~	x/X	X	X
^^2C-	X	X	-X-	X	X
BB-VI	X	X	X	X	X
-SiN-	X	X	-X-	X	X
KVAE	×	X	-X-	X	X
DVBF	-X-	X	-X-	X	X
VSMC	-X-	X	-X-	X	X
RKN	X	~^T~	X	X	X
Table 1: Qualitative comparison of our approach to re-
cent related work.
2016) directly use the Kalman update equations in the latent state, which limits these approaches to
rather low dimensional latent states due to the expensive matrix inversions.
The BackpropKF (Haarnoja et al., 2016)
applies a CNN to estimate the observable
parts of the true state given the observa-
tion. Similar to our approach, this CNN
additionally outputs a covariance matrix
indicating the models certainty about the
estimate and allows the subsequent use of
an (extended) Kalman filter with known
transition model. In contrast, we let our
model chose the feature space that is used
for the inference such that locally linear
models can be learned and the KF com-
putations can be simplified due to our fac-
torization assumptions.
A summary of all the approaches, listing
also their basic properties, can be seen
in Table 1. We compare the approaches
whether they are scaleable to high dimen-
sional latent states, whether they can be
used for state estimation, whether they can provide uncertainty estimates, whether they can han-
dle noise or missing data and whether the objective can be optimized directly or via a lower bound.
All probabilistic generative models rely on variational inference which optimizes a lower bound,
which potentially affects the performance of the algorithms. The P2T and E2C approaches rely on
the Markov assumption and therefore can not deal with noise (or need very large window sizes). Tra-
ditional recurrent models (LSTMs, GRUs) can be trained directly by backpropagation through time
and therefore typically yield very good performance but are lacking uncertainty estimates (which,
however, can be artificially added as in our experiments). Our RKN approach combines the advan-
tages of all methods above as it can be learned by direct optimization without the use of a lower
bound and it provides a principled way of representing uncertainty inside the neural network.
2	Factorized Inference in Deep Feature S paces
Lifting the original input space to a high-dimensional feature space where linear operations are feasi-
ble is a common approach in machine learning, e.g., in kernel regression and SVMs. The Recurrent
Kalman Network (RKN) transfers this idea to state estimation and filtering, i.e., we learn a high di-
mensional deep feature space that allows for efficient inference using the Kalman update equations
even for complex systems with high dimensional observations. To achieve this, we assume that the
belief state representation can be factorized into independent Gaussian distributions as described in
the following sections.
2.1	Latent Observation and State Spaces
The RKN encoder learns a mapping to a high-dimensional latent observation space W = Rm .
The encoder also outputs a vector of uncertainty estimates σtobs, one for each entry of the latent
observation wt. Hence, the encoder can be represented by (wt, σtobs) = enc(ot). The latent state
space Z = Rn of the RKN is related to the observation space W by the linear latent observation
model H = Im 0m×(n-m) , i.e., w = Hz with w ∈ W and z ∈ Z. Im denotes the m × m
identity matrix and 0m×(n-m) denotes a m × (n - m) matrix filled with zeros.
The idea behind this choice is to split the latent state vector zt into two parts, a vector pt for holding
information that can directly be extracted from the observations and a vector mt to store information
inferred over time, e.g., velocities. We refer to the former as the observation or upper part and the
latter as the memory or lower part of the latent state. For an ordinary dynamical system and images
as observations the former may correspond to positions while the latter corresponds to velocities.
3
Under review as a conference paper at ICLR 2019
Figure 1: The Recurrent Kalman Network. An encoder network extracts latent features wt from
the current observation ot . Additionally, it emits an estimate of the uncertainty in the features via
the variance σtobs. The transition model At is used to predict the current latent prior zt- , Σt-
using the last posterior zt+-1, Σt+-1 and subsequently update the prior using the latent observation
(wt, σtobs). As we use a factorized representation of Σt, the Kalman update simplifies to scalar
operations. The current latent state zt consists of the observable units pt as well as the corresponding
memory units mt . Finally, a decoder produces either st+ , σt+ , a low dimensional observation and
an element-wise uncertainty estimate, or ot+, a noise free image.
Clearly, this choice only makes sense for m ≤ n and in this work we assume n = 2m, i.e., for each
observation unit pi , we also represent a memory unit mi that stores its velocity information.
For each sequence, We initialize z+ with an all zeros vector and Σ+ with 10 ∙ I. In practice, it is
beneficial to normalize wt since the statistics of noisy and noise free images differ considerably.
2.2	The Transition Model
To obtain a locally linear transition model we learn K constant transition matrices A(k) and combine
them using state dependent coefficients α(k) (zt), i.e., At = PkK=0 α(k) (zt)A(k). A small neural
network with softmax output is used to learn α(k) . Similar approaches are used in (Fraccaro et al.,
2017; Karl et al., 2016).
Using a dense transition matrix in high-dimensional latent spaces is not feasible as it contains
too many parameters and causes numerical instabilities and overfitting, as preliminary exper-
iments showed. Therefore, we design each A(k) to consist of four band matrices A(k) =
[B(1k1) , B(1k2) ; B(2k1) , B(2k2)] with bandwidth b. This choice reduces the number of parameters while
not affecting performance since the network is free to choose the state representation.
We assume the covariance of the transition noise to be diagonal and denote the vector containing the
diagonal values by σtrans. The noise is learned and independent of the state. Moreover, it is crucial
to correctly initialize the transition matrix. Initially, the transition model should focus on copying
the encoder output so that the encoder can learn how to extract good features if observations are
available and useful. On the other hand it is crucial that A does not yield an instable system. We
choose B11) = B22) = I and B12) = -B)? = 0.2 ∙ I.
2.3	Factorized Covariance Representation
Since the RKN learns high-dimensional representations, we can not work with the full state covari-
ance matrices Σt+ and Σt-. We can also not fully factorize the state covariances by diagonal matrices
as this neglects the correlation between the memory and the observation parts. As the memory part
is excluded from the observation model H, the Kalman update step would not change the memory
nodes nor their uncertainty if we would only use a diagonal covariance matrix Σt+. Hence, for each
observation node pi, we compute the covariance with its corresponding memory node mi . All the
other covariances are neglected. This might be a crude approximation for many systems, however,
as our network is free to choose its own state representation it can find a representation where such
a factorization works well in practice. Thus, we use matrices of the form Σt = [Σtu , Σts ; Σst , Σlt],
where each of Σtu , Σts , Σlt ∈ Rm×m is a diagonal matrix. Again, we denote the vectors containing
the diagonal values by σtu , σlt and σts .
4
Under review as a conference paper at ICLR 2019
2.4	Factorized Inference in the Latent Space
Inference in the latent state space can now be implemented, similar to a standard KF, using a predic-
tion and an observation update.
Prediction Update. Equivalently to the classical Kalman Filter, the current prior zt- , Σt- is
obtained from the last posterior zt+-1, Σt+-1 by
Z- = AtZitI and Σ- = Az=N + I∙σtrans.	(1)
However, the special structure of the covariances enables us to significantly simplify the equation
for the covariance. While being straight forward, the full derivations are rather lengthy, thus, we
refer to the supplementary material where the equations are given in Eqs. 7,8 and 9.
Observation Update. Next, the prior is updated using the latent observation (wt, σtobs). Similar
to the state, we split the Kalman gain matrix Qt into an upper Qtu and a lower part Qlt . Both Qtu
and Qlt are squared matrices. Due to the simple latent observation model H =	Im 0m×(n-m)
and the factorized covariances all off-diagonal entries of Qtu and Qlt are zero and we can work with
vectors representing the diagonals, i.e., qtu and qlt . Those are obtained by
qU = σu,- 0 (σ" + σθbs) and q； = σt,- 0 (σu,- + b；bs),	⑵
where denotes an elementwise vector division. With this the update equation for the mean sim-
plifies to
Zt+ = Zt- +
qtu
qlt
u
wt - Zt
u
wt - Zt
(3)
where denotes the elementwise vector product. The update equations for the individual covariance
parts are given by
σtu,+ = (1m - qtu) σtu,-, σts,+ = (1m - qtu) σts,- and σlt,+ = σlt,- - qlt σts,-,	(4)
where 1m denotes the m dimensional vector consisting of ones. Again, we refer to the supplemen-
tary material for a more detailed derivations.
Besides avoiding the matrix inversion in the Kalman gain computation, the factorization of the
covariance matrices reduces the total amount of numbers to store per matrix from n2 to 3m. Addi-
tionally, working with σs makes it trivial to ensure that the symmetry and positive definiteness of
the state covariance are not affected by numerical issue.
2.5	Loss and Training
We consider two different potential output distributions, Gaussian distributions for estimating low
dimensional observations and Bernoulli distributions for predicting high dimensional observa-
tions/images. Both distributions are computed by decoders that use the current latent state estimate
Zt as well its uncertainty estimates σtu,+ , σts,+ , σlt,+ .
Inferring states. Let s1:T be the ground truth sequence with dimension Ds, the Gaussian log-
likelihood for a single sequence is then computed as
L(S(1:T)) = T X log N b t dec” (Zth), decΣ(σu,+ , σs,+ , σt+')'∖ ,	⑸
t=1
where dec*(∙) and dec∑(∙) denote the parts of the decoder that are responsible for decoding the
latent mean and latent variance respectively.
Inferring images. Let o1:T be images with Do pixels. The Bernoulli log-likelihood for a single
sequence is then given by
1 T Do
L (o(1:T)) = T XX o(i) log (deco,i (z++)) + (1 - oti)) log (1 - decο,i (z++)),	⑹
t=1 i=0
5
Under review as a conference paper at ICLR 2019
where ot(i) is the ith pixel of the tth image. The pixels are in this case represented by grayscale
values in the range of [0; 1]. The ith dimension of the decoder is denoted by deco,i zt+ , where we
use a sigmoid transfer function as output units for the decoder.
Gradients are computed using (truncated) backpropagation through time (BPTT) (Werbos, 1990)
and clipped. We optimize the objective using the Adam (Kingma and Ba, 2014) stochastic gradient
descent optimizer with default parameters.
2.6 The Recurrent Kalman Network
The prediction and observation updates results in anew type of recurrent neural network, that we call
Recurrent Kalman Network, which allows working in high dimensional state spaces while keeping
numerical stability, computational efficiency and (relatively) low memory consumption. Similar to
the input gate in LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014) the
Kalman gain can be seen as a gate controlling how much the current observation influences the
state. However, this gating explicitly depends on the uncertainty estimates of the latent state and
observation and is computed in a principled manner. Using sparse transition models allows working
in higher dimensional spaces with considerably less parameters than LSTMs or GRUs. For a fixed
bandwidth b and number of basis matrices k, the number of parameters of the RKN scales linear in
the state size while it scales quadratically for LSTMs and GRUs. Moreover, the RKN provides a
principled method to deal with absent inputs by just omitting the update step and setting the posterior
to the prior.
3	Evaluation and Experiments
A full listing of hyperparameters and data set specifications can be found in the supplementary
material1. We compare to LSTM and GRU baselines for which we replaced the RKN transition
layer with generic LSTM and GRU layers. Those were given the encoder output as inputs and have
an internal state size of 2n. The internal state was split into two equally large parts, the first part was
used to compute the mean and the second to compute the variance. We additionally executed most
of the following experiments using the root mean squared error to illustrate that our approach is also
competitive in prediction accuracy. The RMSE results can be found in the appendix.
3.1	Pendulum
We evaluated the RKN on a simple simulated pendulum with images of size 24 × 24 pixels as ob-
servations. Gaussian transition noise with standard deviation of σ = 0.1 was added to the angular
velocity after each step. In the first experiment, we evaluated filtering in the presence of high ob-
servation noise. We compare against LSTMs and GRUs as these are the only methods that can
also perform state estimation. The amount of noise varies between no noise at all and the whole
image consisting of pure noise. Furthermore, the noise is correlated over time, i.e., the model
may observe pure noise for several consecutive time steps. Details about the noise sampling pro-
cess can be found in the appendix. We represent the joint angle θt as a two dimensional vector
st = θt = (sin(θt), cos(θt))T to avoid discontinuities. We compared different instances of our
model to evaluate the effects of assuming sparse transition models and factorized state covariances.
The results are given in Table 2. The results show that our assumptions do not affect the perfor-
mance, while we need to learn less parameters and can use much more efficient computations using
the factorized representation. Note that for the more complex experiments with a higher dimen-
sional latent state space, we were not able to experiment with full covariance matrices due to lack of
memory and massive computation times. Moreover, the RKN outperforms LSTM and GRU in all
settings.
In a second experiment we evaluated the image prediction performance and compare against exist-
ing variational inference approaches. We randomly removed half of the images from the sequences
and tasked the models with imputing those missing frames, i.e., we train the models to predict im-
ages instead of the position. We compared our approach to the Kalman Variational Autoencoder
1A link to source code will be added here in the final version.
6
Under review as a conference paper at ICLR 2019
Model	Log Likelihood	Model	Log Likelihood
RKN (m =15,b = 3,K = 15)	6.182 ± 0.1z55=	LSTM m =W=	5.773 ± 0.23F=
RKN m = b =15,K = 3)	6.248 ± 0.1715	LSTM m = 6	6.019 ± 0.122
RKN (m =15,b = 3,K = 15, fc )	6.161 ± 0.23	GRU m = 50	5.649 ± 0.197
RKN (m = b =15,K = 15, fc )	6.197 ± 0.249	GRU m = 8	6.051 ± 0.145
Table 2: Our approach outperforms the generic LSTM and GRU Baselines. The GRU with m = 8
and the LSTM with m = 6 where designed to have roughly the same amount of parameters as the
RKN with b = 3. In the case where m = b the RKN uses a full transition matrix. fc stands for full
covariance matrix, i.e., we do not use factorization of the belief state.
Model	Log Likelihood	Model	Log Likelihood
RKN (informed)	-12.782 ± 0.0160	KVAE (informed, filter)	-14.383 ± 0W
RKN (uninformed)	-12.788 ± 0.0142	KVAE (informed, smooth)	-13.337 ± 0.236
E2C	-95.539 ± 1.754	KVAE (uninformed, filter)	-46.320 ± 6.488
SIN		-101.268 ± 0.567	KVAE (uninformed, smooth)	-38.170 ± 5.399
Table 3: Comparison on the image imputation task. The informed models where given a mask of
booleans indicating which images are valid and which not. The uninformed models where given
a black image whenever the image was not valid. E2C and SIN only work in the informed case.
Since the KVAE is capable of smoothing in addition to filtering, we evaluated both. Our approach
outperforms all models. Only the informed KVAE yields comparable, but still slightly worse results
while E2C and SIN fail to capture the dynamics. The uninformed KVAE fails at identifying the non
valid images.
(KVAE) (Fraccaro et al., 2017), Embed to Control (E2C) (Watter et al., 2015) and Structured Infer-
ence Networks (SIN) (Krishnan et al., 2017). The results can be found in Table 3. Again, our RKN
outperforms all other models. This is surprising as the variational inference models use much more
complex inference methods and in some cases even more information such as in the KVAE smooth-
ing case. Sample sequences can be found in the supplementary material. All hyperparameters are
the same as for the previous experiment.
3.2	Multiple Pendulums
Dealing with uncertainty in a principled manner becomes even more important if the observation
noise affects different parts of the observation in different ways. In such a scenario the model has to
infer which parts are currently observable and which parts are not. To obtain a simple experiment
with that property, we repeated the pendulum experiments with three colored pendulums in one
image. The image was split into four equally sized square parts and the noise was generated individ-
ually for each part such that some pendulums may be occluded while others are visible. Exemplary
images are shown in the supplementary material. A comparison to LSTM and GRU baselines can
be found in Figure 2 and an exemplary trajectory in Figure 3. The RKN again clearly outperforms
the competing methods. We also computed the quality of the uncertainty prediction by showing the
histograms of the normalized prediction errors. While this histogram has clearly a Gaussian shape
for the RKN, it looks like a less regular distribution for the LSTMs.
3.3	Quad Link
We repeated the filtering experiment on a system with much more complicated dynamics, a quad link
pendulum on images of size 48 × 48 pixels. Since the individual links of the quad link may occlude
each other different amounts of noise are induced for each link. Two versions of this experiment
were evaluated. One without additional noise and one were we added noise generated by the same
process used in the pendulum experiments. You can find the results in Table 4.
Furthermore, we repeated the imputation experiment with the quad link. We compared only to the
informed KVAE, since it was the only model producing competitive results for the pendulum. Our
approach achieved -44.470 ± 0.208 (informed) and -44.584 ± 0.236 (uninformed). The KVAE
7
Under review as a conference paper at ICLR 2019
Model		Log Likelihood
RKN m = 45= b = 3, k = 15	11.51 ± 1.70F=
LSTM m = 50 LSTM m = 12	7.5224 ± 1.564 7.429 ± 1.307
GRU m = 50 GRU m = 12	7.541 ± 1.547 5.602 ± 1.468
Figure 2: Results of the multiple pendulum experiments. To evaluate the quality of our uncertainty
(j)	(j),+
prediction We compute the normalized error S σ-S,+for each entry j of S for all time steps in all
test sequences. This normalized error should follow a Gaussian distribution with unit variance if the
prediction is correct. We compare the resulting error histograms With the a unit variance Gaussian.
The left histogram shoWs the RKN, the right one the LSTM. The RKN perfectly fits the normal
distribution While the LSTM’s normalized error distribution has several modes. Again We designed
the smaller LSTM and GRU to have roughly the same amount of parameters as the RKN.
0	50	100	150	0
50	100	150	0
50	100	150
covered ∣	] observable
Figure 3: Predicted sine value of the tree links With 2 times standard deviation (green). Ground truth
displayed in blue. The crosses visualize the current visibility of the link With yelloW corresponding
to fully visible and red to fully occluded. If there is no observation for a considerable time the
predictions become less precise due to transition noise, hoWever the variance also increases.
achieved -52.608 ± 0.602 for smoothing and -59.0218 ± 0.580 for filtering . Sample images can
be found in the appendix.
3.4	KITTI Dataset for Visual Odometry
We evaluated the RKN on the KITTI Data set for visual odometry (Geiger et al., 2012). FolloW-
ing (Zhao et al., 2018) We constructed an encoder consisting of FloWNet2 (Ilg et al., 2017), four
convolutional and tWo branches of fully connected layers.
FolloWing the standard KITTI evaluation procedure achieve an transnational error of 4.24% and
a rotational error of 0.0404rad /100m . Those result are competitive to other recent results on the
KITTI dataset using only monocular images (Zhao et al., 2018; Wang et al., 2018) and demonstrate
the applicability of our approach to real World applications.
Model	without noise Log Likelihood	with noise Log Likelihood
RKN (m = 100,b = 3,K = 15) LSTM (m = 50) LSTM (m = 100) GRU (m = 50) GRU (m = 100)		14.534 ± 0.1W= 11.960 ± 1.24 7.858 ± 4.680 10.346 ± 2.70 5.82 ± 2.80	6.259 ± 0.41T= 5.21 ± 0.305 3.87 ± 0.938 4.696 ± 0.699 1.2 ± 1.105
Table 4: Comparison of our approach With the LSTM and GRU Baselines on the Quad Link Pen-
dulum. Again the RKN performs significantly better than LSTM and GRU Who fail to perform
Well.
8
Under review as a conference paper at ICLR 2019
4	Conclusion
In this paper, we introduced the Recurrent Kalman Network that jointly learns high-dimensional
representations of the system in a latent space with locally linear transition models and factorized
covariances. The update equations in the high-dimensional space are based on the update equa-
tions of the classical Kalman filter, however due to the factorization assumptions they simplify to
scalar operations that can be performed much faster and with greater numerical stability. Our model
outperforms generic LSTMs and GRUs on various state estimation tasks while providing reason-
able uncertainty estimates. Additionally, it outperformed several generative models on an image
imputation task. Training is straight forward and can be done in an end-to-end fashion.
In future work we want to exploit the principled notion of a variance provided by our approach in
scenarios where such a notion is beneficial, e.g. reinforcement learning. Similar to (Fraccaro et al.,
2017) we could expand our approach to not just filter but smooth over trajectories in offline post-
processing scenarios which could potentially increase the estimation performance significantly.
References
Rudolph Emil Kalman et al. A new approach to linear filtering and prediction problems. Journal of
basic Engineering, 82(1):35-45, l96θ.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder aPProaches. arXiv preprint arXiv:1409.1259,
2014.
Tuomas Haarnoja, Anurag Ajay, Sergey Levine, and Pieter Abbeel. Backprop kf: Learning discrim-
inative deterministic state estimators. In Advances in Neural Information Processing Systems,
pages 4376-4384, 2016.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in neural
information processing systems, pages 2746-2754, 2015.
Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box
variational inference for state space models. arXiv preprint arXiv:1511.07367, 2015.
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition
and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information
Processing Systems, pages 3601-3610, 2017.
Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state
space models. In AAAI, pages 2101-2109, 2017.
G.H.W. Gebhardt, A.G. Kupcsik, and G. Neumann. The kernel kalman rule - efficient nonparametric
inference with recursive least squares. In Proceedings of the Thirty-First AAAI Conference on
Artificial Intelligence, 2017.
Niklas Wahlstrom, Thomas B Schon, and Marc Peter Deisenroth. From pixels to torques: Policy
learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep varia-
tional bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint
arXiv:1605.06432, 2016.
Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M Blei. Variational se-
quential monte carlo. arXiv preprint arXiv:1705.11140, 2017.
9
Under review as a conference paper at ICLR 2019
Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE ,78(10):1550-1560,1990.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti
vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR),
2012.
Cheng Zhao, Li Sun, Pulak Purkait, Tom Duckett, and Rustam Stolkin. Learning monocular visual
odometry with dense 3d mapping from dense 3d flow. arXiv preprint arXiv:1803.02286, 2018.
Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.
Flownet 2.0: Evolution of optical flow estimation with deep networks. In IEEE conference on
computer vision and pattern recognition (CVPR), volume 2, page 6, 2017.
Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. End-to-end, sequence-to-sequence
probabilistic visual odometry through deep neural networks. The International Journal of
Robotics Research, 37(4-5):513-542, 2018. doi: 10.1177/0278364917734298. URL https:
//doi.org/10.1177/0278364917734298.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Djork-Ame Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
10
Under review as a conference paper at ICLR 2019
Appendix
The Kalman filter (Kalman et al., 1960) works by iteratively applying two steps, predict and update.
It assumes additive Gaussian noise with zero mean and covariances Σtrans and Σobs on both transi-
tions and observations, which need to be given to the filter. During the prediction step the transition
model A is used to infer the next prior state estimate xt-, Σt- , i.e., a-priori to the observation,
from the last posterior estimate xt+-1 , Σt+-1 , by
xt- = Axt+-1 and Σt- = AΣt+-1AT + Σtrans.
The prior estimate is then updated using the current observation wt and the observation model H to
obtain the posterior estimate (x+, ∑+), i.e.,
x+ = x-+Qt (Wt- Hx-) and ∑+ = (I- QtH) ∑-, With Qt = Σ-HT (h∑-Ht + ∑obs)-1
where I denotes the identity matrix. The matrix Qt is referred to as the Kalman gain. The whole
update step can be interpreted as a Weighted average betWeen state and observation estimate, Where
the Weighting, i.e., Qt, depends on the uncertainty about those estimates. If currently no observation
is present or future states should be predicted, the update step is omitted.
A Simplified Kalman Filter Formulas
As stated above the simple latent observation model H = Im 0m×(n-m) , as Well as the
assumed factorization of the covariance matrices alloW us to simplify the Kalman Filter equations.
A. 1 Notation
In the folloWing derivations We neglect the time indices t and t + 1 for brevity. For any matrix M,
MM denotes a diagonal matrix with the same diagonal as M, m denotes a vector containing those
diagonal elements and M(ij) denotes the entry at roW i and column j. Similarly, v(i) denotes the i-th
entry of a vector v. The point wise product between two vectors of same length (Hadamat Product)
will be denoted by and the point wise division by .
A.2 Prediction Step
Mean:
Covariance:
z- = Az+
Σ- = aς+at + Σtrans
The computation of the mean can not be further simplified, however, depending on the state size
and bandwidth, sparse matrix multiplications may be exploited. For the covariance, let T = AΣ+ .
Then,
ς+ = J B11	B12	U	Σu,+	Σs,十	一
B21	B22	_| [	∑s,+	∑l,+
B11∑u,+ + B12∑s,+	B11∑s,+ + B12∑l,十
B21Σu,+ + B22Σs,+	B21Σs,+ + B22Σl,+
and
Σ- = TAT + Σtrans

,+ ,+
ss
<<
2
1
B
+
,+ ,+
uu
<<
1
1
B
++
,,
ll
<ς<ς
2
1
B
+
,+ ,+
ss
<<
1
1
B
T11T
BB
T21T
BB
∑ u,- ∑ s,-
Σ s,- Σ l,-
11
Under review as a conference paper at ICLR 2019
with
Σu,- = (B11∑u,+ + B12∑ s,+) BT1 + (B11∑s,+ + B12∑ l,+) BT2 + ∑u,trans
=B11∑ u,+BTι + B12∑ s,+BTι + B11∑ s,+BT2 + B12∑ l,+BT2 + ∑ u,trans
Σl,- = (B21∑u,+ + B22∑s,+) BTι + (B21∑s,+ + B22∑l,+) BT2 + Σl,trans
=B21∑ u,+BTι + B22∑ s,+BT1 + B21∑ s,+BT2 + B22∑ l,+BT2 + Σ l,trans
Σs,- = (B21∑u,+ + B22∑s,+) BT + (B21∑s,+ + B22∑l,+) BT
=B21∑ u,+BTι + B22∑ s,+BTι + B21∑ s,+BT2 + B22∑ l,+BT2
Since we are only interested in the diagonal parts of Σu,-, Σl,- and Σs,- i.e. Σu, , Σl, and Σs,,
we can further simplify these equations by realizing two properties of the terms above. First, for any
matrix M, N and a diagonal matrix Σ it holds that
(M∑Nτ Ji" = XX A(ik)B(ik)σ(k) = (N∑Mτ)..
k=1	ii
Hence, we can simplify the equations for the upper and lower part to
∑u,- = B11∑u,+BTι + 2 ∙ B12∑s,+ BTι + B12∑l,+BT2 + Σ…：
Σl,- = B21∑u,+BTι + 2 ∙ B22∑s,+ BTι + B22∑l,+BT2 + Σl,trans.
Second, since we are only interested in the diagonal of the result it is sufficient to compute only
the diagonals of the individual parts of the sums which are almost all of the same structure i.e.
++
S = MΣ NT. Let T = MΣ , then each element of T can be computed as
T(ij)
n
^X M (ik)∑ (kj)
k=1
M(ij)σ(j).
Consequently, the elements of S = TAT can be computed as
n
n
S(ij)
T (ik)A(kj) =	M(ik)σkN(jk).
k=1
k=1
T Tl . ∙	. 1	. ∙	. 1 ∙ r-ι 1	1 ∙ A
Ultimately, we are not interested in S but only in S
n
Seii)= X M (ik)N (ik)σ(k)
k=1
Using this we obtain can obtain the entries of σu,-, σl,- and σs,- by
m2	m	m2
σu,-,(i) = X	B1(i1k)	σu,+,(i) +2X B1(i1k)B1(i2k)σs,+,(i) + X	B1(i2k)	σl,+,(i) + σu,trans,(i)
k=1	k=1	k=1
(7)
mmm
σl,-,(i) = X	B2(i1k) σu,+,(i) + 2X B2(i2k)B2(i1k)σs,+,(i) + X	B2(i2k)	σl,+,(i) + σl,trans,(i)
k=1	k=1	k=1
(8)
mm
σs,-,(i) =X B2(i1k)B1(i1k)σu,+,(i) + X B2(i2k)B1(i1k)σs,+,(i)...	(9)
k=1	k=1
mm
...+X B2(i1k)B1(i2k)σs,+,(i) + X B2(i2k)B1(i2k)σl,+,(i),
k=1	k=1
which can be implemented efficiently using elementwise matrix multiplication and sum reduction.
12
Under review as a conference paper at ICLR 2019
A.3 Update Step
Kalman Gain
Mean
Covariance
Q = Σ-HT HΣ-HT + Σobs-1
z+ = Z- + Q (W — Hz-)
Σ+=(I-QH)Σ-
First, note that
u,-
ς-HT =	∑ s,-	and hς-HT + ς obs = ς u,- + ς obs
and thus the computation of the Kalman Gain only involves diagonal matrices. Hence the Kalman
Gain matrix also consists of two diagonal matrices, i.e., Q
Q
Q
whose diagonals can be
computed by
qu = σu,- 0 (σu,- + σobs) and ql = σs,- 0 (σu,- + σobs) .	(10)
Using this result, the mean update can be simplified to
z+ = z- +
W — zu,-
W — zu,-
(11)
For the covariance we get:
∑ u,+
∑ s,+
Σ s,十
Σ l,十
ΣU
Σ s
Σs,-
Σl,-
(In — QH) Σ- =
"(Im - QU) Σu,
-Q lΣu,- + Σs,
Hence the diagonals of the individual parts can be computed as σU,+ = (1m - qU)	σU,- σs,+ = (1m - qU)	σs,- σl,+ = σl,- - ql	σs,-.	(12) (13) (14)
B Root Mean S quare Error Results
To evaluate the actual prediction performance of our approach we repeated some experiments using
the RMSE as loss function. Other than that and removing the variance output of the decoder no
changes were made to the model, hyperparameters and learning procedure. The results can be found
in Table 5.
C Visualization of Imputation Results
Exemplary results of the data imputation experiments conducted for the Pendulum and Quad Link
experiment can be found in Figure 4
D Network Architectures and Hyper Parameters
For all experiments Adam (Kingma and Ba, 2014) with default parameters (α = 10-3, β1 = 0.9,
β2 = 0.999 andε = 10-8) was used as an optimizer. The gradients were computed using (truncated)
Backpropagation Trough Time (BpTT) (Werbos, 1990). Further in all (transposed) convolutional
layers layer normalization (LN) (Ba et al., 2016) was employed to normalize the filter responses.
”Same” padding was used. The elu activation function (Clevert et al., 2015) plus a constant 1 is
denoted by (elu + 1) was used to ensure that the variance outputs are positive.
13
Under review as a conference paper at ICLR 2019
Model		Root Mean Squared Error
Pendulum	
RKN (m =15,b = 3,K = 15)	0.0779 ± 0.0082
RKN (m = b = 15,K = 15)	0.0758 ± 0.0094
LSTM (m = 50)	0.0920 ± 0.0774
LSTM (m = 6)	0.0959 ± 0.0100
GRU (m = 50)	0.0821 ± 0.0084
GRU (m = 8)		0.0916 ± 0.0087
Multiple Pendulums	
RKN (m = 45,b = 3,k = 15)	0.0878 ± 0.0036
LSTM (m = 50)	0.098 ± 0.0036
LSTM (m = 12)	0.104 ± 0.0043
GRU (m = 50)	0.112 ± 0.0371
GRU (m = 14)		0.105 ± 0.0055
Quad Link (without additional noise)	
RKN (m = 100, b = 25,k = 15)	0.103 ± 0.00076
LSTM (m = 100)	0.175 ± 0.182
LSTM (m = 25)	0.118 ± 0.0049
GRU (m = 100)	0.278 ± 0.105
GRU (m = 25)		0.121 ± 0.0021
Quad Link (with additional noise)	
RKN (m = 100, b = 25,k = 15)	0.171 ± 0.0039
LSTM (m = 75)	0.175 ± 0.0022
GRU (m = 25)		0.204 ± 0.0023
Table 5: RMSE Results
D.1 Pendulum and Multiple Pendulum Experiments
Observations Pendulum: Grayscale images of size 24 × 24 pixels. Multiple Pendulum: RGB images
of size 24 × 24 pixels. See Figure 5 for examples.
Dataset: 1000 Train and 500 Test sequences of length 150. For the filtering experiments noise
according to section E was added , for imputation 50% of the images were removed randomly.
Encoder: 2 convolution + 1 fully connected + linear output & (elu + 1) output:
•	Convolution 1: 12, 5 × 5 filter, ReLU, 2 × 2 max pool with 2 × 2 stride
•	Convolution 2: 12, 3 × 3 filter with 2 × 2 stride, ReLU, 2 × 2 max pool with 2 × 2 stride
•	Pendulum: Fully Connected 1: 30, ReLU
•	Multiple Pendulum: Fully Connected 1: 90, ReLU
Transition Model Pendulum: 15 dimensional latent observation, 30 dimensional latent state. Mul-
tiple Pendulum: 45 dimensional latent observation, 90 dimensonal latent state. Both: bandwidth: 3,
number of basis: 15
•	α(zt): No hidden layers - softmax output
Decoder (for st+): 1 fully connected + linear output:
•	Fully Connected 1: 10, ReLU
Decoder (for ot+): 1 fully connected + 2 transposed convolution + transposed convolution output:
•	Fully Connected 1: 144 ReLU
•	Transposed Convolution 1: 16, 5 × 5 filter with 4 × 4 stride, ReLU
•	Transposed Convolution 2: 12, 3 × 3 filter with 2 × 2 stride, ReLU
•	Transposed Convolution Out: Pendulum: 1 Multiple Pendulum: 3, 3 × 3 filter with 1 × 1
stride, Sigmoid
14
Under review as a conference paper at ICLR 2019
(a) Pendulum
Figure 4: Each of (a) and (b) shows from left to right: true images, input to the models, imputation
results for RKF, imputation results for KVAE(Fraccaro et al., 2017).
(b) Quad Link
Figure 5: Example images for the multiple pendulum experiments. Left: Noise free image. Middle:
sequence of images showing how the noise affects different pendulums differently. Right: Image
without useful information.
Decoder (for σt+ or σt+): 1 fully connected + (elu + 1):
•	Fully Connected 1: 10, ReLU
15
Under review as a conference paper at ICLR 2019
D.2 Quad Link
Observations: Grayscale images of size 48x48 pixels.
Dataset: 4000 Train and 1000 Test sequences of length 150. For the filtering with additional noise
experiments noise according to section D was added, for imputation 50% of the images were re-
moved randomly.
Encoder: 2 convolution + 1 fully connected + linear output & (elu + 1) output:
•	Convolution 1: 12, 5 × 5 filter with 2 × 2 stride, ReLU, 2 × 2 max pool with 2 × 2 stride
•	Convolution 2: 12, 3 × 3 filter with 2 × 2 stride, ReLU, 2 × 2 max pool with 2 × 2 stride
•	Fully Connected 1: 200 ReLU
Transition Model 100 dimensional latent observation, 200 dimensional latent state, bandwidth: 3,
number of basis: 15
•	α(zt): No hidden layers - softmax output
Decoder (for st+): 1 fully connected + linear output:
•	Fully Connected 1: 10, ReLU
Decoder (for ot+): 1 fully connected + 2 transposed convolution + transposed convolution output:
•	Fully Connected 1: 144 ReLU
•	Transposed Convolution 1: 16, 5 × 5 filter with 4 × 4 stride, ReLU
•	Transposed Convolution 2: 12, 3 × 3 filter with 4 × 4 stride, ReLU
•	Transposed Convolution Out: 1, 1 × 1 stride, Sigmoid
Decoder (for σt+ or σt+): 1 fully connected + (elu + 1):
•	Fully Connected 1: 10, ReLU
D.3 Kitti
Observation and Data Set: Images from the KITTI (Geiger et al., 2012) dataset for visual odom-
etry. Sequences 00, 01, 02, 08, 09 used for training. Sequences 03, 04, 05, 06, 07, 10 for testing.
Encooder: FlowNet2 + 4 Convolutional + 2 Branches of dense Layers. See (Zhao et al., 2018)
Transition Model: 100 dimensional latent observations, 200 dimensional latent state, bandwidth:
5, number of basis: 20
•	α(zt): No hidden layers - softmax output
Decoder (for st+): 2 fully connected + linear output:
•	Fully Connected 1: 128, ReLU
•	Fully Connected 1: 128, ReLU
Decoder (for σt+ or σt+): 2 fully connected + (elu + 1):
•	Fully Connected 1: 128, ReLU
•	Fully Connected 1: 128, ReLU
E Observation Noise generation process
Let U(x, y) denote the uniform distribution from x to y.To generate the noise for the pendulum task
for each sequence a sequence of factors ft of same length was generated. To correlate the factors
they were sampled as fo 〜 U (0,1) and ft+ι = min(max(0, f + rj 1) with r 〜 U (-0.2,0.2).
Afterwards, for each sequence two thresholds tι 〜U(0.0,0.25) and t2〜U(0.75,1) were sampled.
16
Under review as a conference paper at ICLR 2019
All ft < t1 were set to 0, all ft > t2 to 1 and the rest was linearly mapped to the interval [0, 1].
Finally, for each image it an image consisting of pure uniformly distributed noise itnoise was sampled
and the observation computed as ot = ft ∙ it + (1 - ft) "noise.
17