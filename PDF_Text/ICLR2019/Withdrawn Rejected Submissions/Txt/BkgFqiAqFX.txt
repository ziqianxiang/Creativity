Under review as a conference paper at ICLR 2019
Recovering the Lowest Layer of Deep Net-
works with High Threshold Activations
Anonymous authors
Paper under double-blind review
Ab stract
Giving provable guarantees for learning neural networks is a core challenge of
machine learning theory. Most prior work gives parameter recovery guarantees for
one hidden layer networks, however, the networks used in practice have multiple
non-linear layers. In this work, we show how we can strengthen such results
to deeper networks - We address the problem of uncovering the lowest layer in
a deep neural network under the assumption that the lowest layer uses a high
threshold before applying the activation, the upper network can be modeled as a
well-behaved polynomial and the input distribution is gaussian.
1	Introduction
Understanding the landscape of learning neural networks has been a major challege in machine
learning. Various works gives parameter recovery guarantees for simple one-hidden-layer networks
where the hidden layer applies a non-linear activation u after transforming the input x by a matrix
W, and the upper layer is the weighted sum operator: thus f(x) = P aiu(wiTx). However, the
networks used in practice have multiple non-linear layers and it is not clear how to extend these
known techniques to deeper networks.
We consider a multilayer neural network with the first layer activation u and the layers above rep-
resented by an unknown polynomial P such that it has non-zero non-linear components. More
precisely, the function f computed by the neural network is as follows:
fW(x) = P (u(w1T x), u(w2T x), . . . , u(wdT x)) for P(X1, . . . ,Xd) = Σ Cr ∙ YXjrj.
We assume that the input x is generated from the standard Gaussian distribution and there is an un-
derlying true network (parameterized by some unknown W*)1 from which the labels are generated.
In this work we strengthen previous results for one hidden layer networks to a larger class of func-
tions representing the transform made by the upper layer functions if the lowest layer uses a high
threshold (high bias term) before applying the activation: u(a - t) instead of u(a). Intuitively, a
high threshold is looking for a high correlation of the input a with a direction wi*. Thus even if the
function f is applying a complex transform after the first layer, the identity of these high threshold
directions may be preserved in the training data generated using f .
Learning with linear terms in P . Suppose P has a linear component then we show that in-
creasing the threshold t in the lowest layer is equivalent to amplifying the coefficients of the lin-
ear part. Instead of dealing with the polynomial P it turns out that we can roughly think of it as
P(μXι,…,μXd) where μ decreases exponentially in t (μ ≈ e-t ). As μ decreases it has the effect
of diminishing the non-linear terms more strongly so that relatively the linear terms stand out. Tak-
ing advantage of this effect we manage to show that if t exceeds a certain threshold the non linear
terms drop in value enough so that the directions wi can be learned by relatively simple methods. We
show that we can get close to the wi applying a simple variant of PCA. While an application of PCA
can be thought ofas finding principal directions as the local maxima ofmax||z||=1 E[f(x)(zTx)2],
1We suppress W when it is clear from context.
1
Under review as a conference paper at ICLR 2019
We instead perform maxE[f(x)h2(ztχ)2]=1 E[f (x)H4(zτx)4]]2. If W* has a constant condition
number then the local maxima can be used to recover directions that are transforms of wi .
Theorem 1 (informal version of Claim 2, Theorem 11). If t > c√log d for large enough constant
c > 0 and P has linear terms with absolute value of coefficients at least 1/poly(d) and all coeffi-
cients at most O(1), we can recover the weight vector wi within error 1/poly(d) in time poly(d).
These approximations of wi obtained collectively can be further refined by looking at directions
along Which there is a high gradient in f ; for monotone functions We shoW hoW in this Way We can
recover wi exactly (or Within any desired precision.
Theorem 2. (informal version of Theorem 5) Under the conditions of the previous theorem, for
monotone P, there exists a procedure to refine the angle to precision in time poly(1/, d) starting
from an estimate that is 1/poly(d) close.
The above mentioned theorems hold for u being sign and ReLU.3
When P is monotone and u is the sign function, learning W is equivalent to learning a union of half
spaces. We learn W* by learning sign of P Which is exactly the union of halfspaces wiT x = t.
Thus our algorithm can also be vieWed as a polynomial time algorithm for learning a union of large
number of half spaces that are far from the origin - to our knowledge this is the first polynomial time
algorithm for this problem but With this extra requirement (see earlier Work Vempala (2010) for an
exponential time algorithm). Refer to Appendix B.6 for more details.
Such linear components in P may easily be present: consider for example the case where P (X) =
u(vTX - b) where u is say the sigmoid or the logloss function. The taylor series of such functions
has a linear component - note that since the linear term in the taylor expansion of u(χ) has coefficient
u0(0), for expansion ofu(x-b) it will be u0(-b) which is Θ(e-b) in the case of sigmoid. In fact one
may even have a tower (deep network) or such sigmoid/logloss layers and the linear components will
still be present - unless they are made to cancel out precisely; however, the coefficients will drop
exponentially in the depth of the networks and the threshold b.
Sample complexity with low thresholds and no explicit linear terms. Even if the threshold
is not large or P is not monotone, we show that W* can be learned with a polynomial sample
complexity (although possibly exponential time complexity) by finding directions that maximize
the gradient of f .
Theorem 3 (informal version of Corollary 1). If u is the sign function and wi ’s are orthogonal
then in poly(1/, d) samples one can determine W* within precision if the coefficient of the linear
terms in P(μ(X1 + l),μ(X2 + 1), μ(X3 + 1),...) is least 1∕poly(d)
Learning without explicit linear terms. We further provide evidence that P may not even need
to have the linear terms - under some restricted cases (section 4), we show how such linear terms
may implicitly arise even though they may be entirely apparently absent. For instance consider the
case when P = P XiXj that does not have any linear terms. Under certain additional assumptions
we show that one can recover Wi as long as the polynomial P(μ(X1 +1), μ(X2 +1), μ(X3 +1),..)
(where μ is e-t has linear terms components larger than the coefficients of the other terms). Note that
this transform when applied to P automatically introduces linear terms. Note that as the threshold
increases applying this transform on P has the effect of gathering linear components from all the
different monomials in P and penalizing the higher degree monomials. We show that if W* is a
sparse binary matrix then we can recover W* when activation u(a) = eρa under certain assumptions
about the structure of P. When we assume the coefficients are positive then these results extend
for binary low l1 - norm vectors without any threshold. Lastly, we show that for even activations
(∀a, u(a) = u(-a)) under orthogonal weights, we can recover the weights with no threshold.
Learning with high thresholds at deeper layers. We also point out how such high threshold
layers could potentially facilitate learning at any depth, not just at the lowest layer. If there is any
cut in the network that takes inputs X1 , . . . , Xd and if the upper layers operations can be modelled
by a polynomial P, then assuming the inputs Xi have some degree of independence we could use
this to modularly learn the lower and upper parts of the network separately (Appendix E)
2Here H4 and H2 are the fourth and second order hermite polynomials respectively.
3Theorem 1 holds for sigmoid with t ≥ c log d.
2
Under review as a conference paper at ICLR 2019
Related Work. Various works have attempted to understand the learnability of simple neural net-
works. Despite known hardness results Goel et al. (2016); Brutzkus & Globerson (2017), there
has been an array of positive results under various distributional assumptions on the input and the
underlying noise in the label. Most of these works have focused on analyzing one hidden layer
neural networks. A line of research has focused on understanding the dynamics of gradient descent
on these networks for recovering the underlying parameters under gaussian input distribution Du
et al. (2017b;a); Li & Yuan (2017); Zhong et al. (2017a); Zhang et al. (2017); Zhong et al. (2017b).
Another line of research borrows ideas from kernel methods and polynomial approximations to ap-
proximate the neural network by a linear function in a high dimensional space and subsequently
learning the same Zhang et al. (2015); Goel et al. (2016); Goel & Klivans (2017b;a). Tensor de-
composition methods Anandkumar & Ge (2016); Janzamin et al. (2015) have also been applied to
learning these simple architectures.
The complexity of recovering arises from the highly non-convex nature of the loss function to be
optimized. The main result we extend in this work is by Ge et al. (2017). They learn the neural net-
work by designing a loss function that allows a ”well-behaved” landscape for optimization avoiding
the complexity. However, much like most other results, it is unclear how to extend to deeper net-
works. The only known result for networks with more than one hidden layer is by Goel & Klivans
(2017b). Combining kernel methods with isotonic regression, they show that they can provably learn
networks with sigmoids in the first hidden layer and a single unit in the second hidden layer in poly-
nomial time. We however model the above layer as a multivariate polynomial allowing for larger
representation. Another work Arora et al. (2014) deals with learning a deep generative network
when several random examples are generated in an unsupervised setting. By looking at correlations
between input coordinates they are able to recover the network layer by layer. We use some of their
ideas in section 4 when W is a sparse binary matrix.
Notation. We denote vectors and matrices in bold face. || ∙ ||p denotes the lp-norm of a vector.
∣∣∙∣∣ without subscript implies the l2-norm. For matrices ∣∣∙∣∣ denotes the spectral norm and ∣∣∙∣∣f
denotes the forbenius norm. N(0, Σ) denotes the multivariate gausssian distribution with mean 0
and covariance Σ. For a scalar x we will use φ(x) to denote the p.d.f. of the univariate standard
normal distribution with mean zero and variance 1 .For a vector x we will use φ(x) to denote
the p.d.f. of the multivariate standard normal distribution with mean zero and variance 1 in each
direction. Φ denotes the c.d.f. of the standard gausssian distribution. Also define Φc = 1 - Φ. Let
hi denote the ith normalized Hermite polynomial Wikipedia contributors (2018). For a function f,
let f denote the ith coefficient in the hermite expansion of f, that is, f = Eg〜N(o,i)[f (g)hi(g)].
For a given function f computed by the neural network, we assume that the training samples (x, y)
are such that x ∈ Rn is distributed according to N(0, 1) and label has no noise, that is, y = f(x).
Note: Most proofs are deferred to the Appendix due to lack of space.
2	Approximate Recovery with Linear term
In this section we consider the case when P has a positive linear component and we wish to recover
the parameters of true parameters W*. The algorithm has two-steps: 1) uses existing one-hidden
layer learning algorithm (SGD on carefully designed loss Ge et al. (2017)) to recover an approximate
solution , 2) refine the approximate solution by performing local search (for monotone P). The in-
tuition behind the first step is that high thresholds enable P to in expectation be approximately close
to a one-hidden-layer network which allows us to transfer algorithms with approximate guarantees.
Secondly, with the approximate solutions as starting points, we can evaluate the closeness of the
estimate of each weight vector to the true weight vector using simple correlations. The intuition of
this step is to correlate with a function that is large only in the direction of the true weight vectors.
This equips us with a way to design a local search based algorithm to refine the estimate to small
error.
For simplicity in this section we will work with P where the highest degree in any Xi is 1. The
degree of the overall polynomial can still be n. See Appendix B.8 for the extension to general P .
More formally,
3
Under review as a conference paper at ICLR 2019
Assumption 1 (Structure of network). We assume that P has the following structure
P (Xι,...,Xk) = C0 + Pi∈[d] CiXi + ps⊆[d]"s∣>ι CS Qj∈s Xjsuchthat Ci = Θ(1)4 for all
i ∈ [d] and for all S ⊆ [d] such that |S| > 1, |cs | ≤ O ⑴.W* has constant condition number.
Thus f(x) = C0 + Pi∈[d] Ciu((W*)TX) + Ps⊆[d]"S∣>1 CS Qj∈S u((wj*)T x). Denote flin(x) =
C0 + Pi∈[d] Ciu((wi*)Tx) to be the linear part of f.
Next we will upper bound expected value of u(x): for ”high-threshold” ReLU, that is, ut(a) =
t2
max(0,a - t), Eg〜N(。炉)[ut(g)] is bounded by a function ρ(t,σ) ≈ e-2σ2 (see Lemma 10). We
also get a lower bound on ∣u41 in terms of ρ(t, σ)4 5 This enables Us to make the following assumption.
Assumption 2. Activation function u is a positive high threshold activation with threshold t, that is,
the bias term is t. Eg 〜N (o,σ2) [ut(g)] ≤ ρ(t, σ) where P is a positive decreasing function of t. Also,
|uk | = tθ(1)p(t, 1) for k = 2,4.
Assumption 3 (Value of t). t is large enough such that ρ(t, ||W* ||) ≈ d-η and ρ(t, 1) ≈ d-pη
with for large enough constant η > 0 and p ∈ (0, 1].
For example, for high threshold ReLU, ρ(t, 1) = e-t2/2 andμ = ρ(t, ||W*||) = e-t2/2llW*ll2, thus
t = √2η log d for large enough d suffices to get the above assumption (κ(W*) is a constant).
These high-threshold activation are useful for learning as in expectation, they ensure that f is close
to flin since the product terms have low expected value. This is made clear by the following lemmas:
Lemma 1. For |S| > 1, under Assumption 2 we have,
E Y ut((wj*)Tx) ≤ ρ(t, 1) (κ(W*)ρ(t, ||W*||))|S|-1.
j∈S
So if μ ：= κ(W*)ρ(t, ||W*||), then E[∏j-∈sXj[x]] ≤ ρ(t, 1)μlSl-1
Lemma 2. Let ∆(x) = f(x) - flin (x). Under Assumptions 1, 2 and 3, if t is such that
dρ(t, ||W* ||) ≤ C for some small enough constant C > 0 we have,
E[∣∆(x)∣] ≤ O (d3ρ(t, 1)ρ(t, ||W*||)) = O (d-(1+p)n+3).
Note: We should point out that f(x) and flin (x) are very different point wise; they are just close
in expectation under the distribution of x. In fact, if d is some constant then even the difference in
expectation is some small constant.
This closeness suggests that algorithms for recovering under the labels from flin can be used to
recover with labels from f approximately.
Learning One Layer Neural Networks using Landscape Design. Ge et al. (2017) proposed an
algorithm for learning one-hidden-layer networks. Intuitively, the approach of Ge et al. (2017) is to
design a well behaved loss function based on correlations to recover the underlying weight vectors.
They show that the local minima of the following optimization corresponds to some transform of
each of the Wi - thus it can be used to recover a transform of wi, one at a time.
max
Z：E[flin(X)H2(ZT x)]=U2
sgn(u4)E[fiin(x)H4(zτ x)]
which they optimize using the Lagrangian formulation (viewed as a minimization):
min Giin(z) ：= -sgn(u4)E[fiin(x)H4(zτx)] + λ(E[fiin(x)H2(zτx)] - u2)2
z
where H2(ZTx) = ||z||2h2 (ZTX) = (z√χ2 -寻 and H4(ZTx) = ||z||4h4 (得)=
√6(ZIx) - ||z|| (Z X) + |^1L (see Appendix A.1 for more details). Using properties
4We can handle ∈ [d-C, dC] for some constant C by changing the scaling on t.
5For similar bounds for sigmoid and sign refer to Appendix B.7.
4
Under review as a conference paper at ICLR 2019
X Ci(ZTWi*)2 - 1! .
of Hermite polynomials, We have E[fiin(x)H2(zτx)] = ^2 Pi Ci(ZTw*)2 and similarly
E[fiin(x)H4(zτx)] = U4 Pi(ZTw*)4. Thus
Glin(Z) = -∣U4∣ X Ci(ZTw*)4 + λu2
i
Using results from Ge et al. (2017), it can be shoWn that the approximate local minima of this
problem are close to columns of (TW*)-1 where T is a diagonal matrix with Tii = √ci.
Definition 1 ((e,τ)-loCalminimUm/maximum). Z is an (e,τ)-local minimumof F if ||VF(z)|| ≤ E
and λmin(V2F(z)) ≤ τ.
Claim 1 (Ge et al. (2017)). An (E, τ)-local minima of the Lagrangian formulation Z with E ≤
O (PT3∕∣U4∣) is such that for an index i IzTw/ = 1 ± O(e∕λu2) ± O(dτ∕∣U4∣) and ∀j =
i, |vTWj∣ = O(√τ∕∣U^4∣) where Wi are columns of (TW*)-1.
Ge et al. (2017) do not mention ^ but it is necessary in the non-orthogonal weight vectors case for
the correct reduction. Since for us, this value can be small, we mention the dependence.Note that
these are not exactly the directions wi* that we need, one way to think about is that we can get the
correct directions by estimating all columns and then inverting.
One-hidden-layer to Deep Neural Network. Consider the loss with f instead of flin:
minZ : G(z) = —sgn(U4)E[f(x)H4(ztx)] + λ(E[f(x)H2(Zτx)] — U2)2
We previously showed that f is close to flin in expectation due to the high threshold property. This
also implies that Glin and G are close and so are the gradients and (eignevalues of) hessians of the
same. This closeness implies that the landscape properties of one approximately transfers to the
other function. More formally,
Theorem 4. Let Z be an (E, τ)-local minimum of function A. If ∣∣V(B - A)(Z)∣∣ ≤ ρ and ∣∣V2(B -
A)(Z)∣∣ ≤ γ then Z is an (E + ρ, τ + γ)-local minimum of function B and vice-versa.
We will now apply above lemma on our Glin(Z) and G(Z).
Claim 2. For λ = Θ(∣U4∣∕U2) ≈ dη, an (e, τ)-approximate local minima of G (for small enough
E, τ ≤ d-2η) is an (O(log d)d-(1+p)η+3, O(log d)d-(1+p)η+3)-approximate local minima of Glin.
This implies Z is such that for an index i, ∣zt Wi∣ = 1 ± O(1)d-2/3pn+3 and ∀j = i, IzT Wj ∣ =
O(1)d-1/3pn+3/2 where Wi are columns of (TW*)-1 (ignoring log dfactors).
Note: For ReLU, setting t = √C log d for large enough C > 0 we can get closeness 1∕poly(d) to
the columns of (TW*)-1. Refer Appendix B.7 for details for sigmoid.
The paper Ge et al. (2017) also provides an alternate optimization that when minimized simultane-
ously recovers the entire matrix W* instead of having to learn columns of (TW*)-1 separately.
We show how applying our methods can also be applied to that optimization in Appendix B.4 to
recover W* by optimizing a single objective.
2.1	APPROXIMATE TO ARBITRARILY CLOSE FOR MONOTONE P
Assuming P is monotone, we can show that the approximate solution from the previous analysis can
be refined to arbitrarily closeness using a random search method followed by approximately finding
the angle of our current estimate to the true direction.
The idea at a high level is to correlate with δ0(ZTx - t) where δ is the Dirac delta function. It
turns out that the correlation is maximized when z is equal to one of the wi . Correlation with
δ0(ZT x-t) is checking how fast the correlation off with δ(ZT x-t) is changing as you change t. To
understand this look at the case when our activation u is the sign function then note that correlation
of ut (WTx - t) with δ0(WT x - t) is very high as its correlation with δ(WT x - t0) is 0 when t0 < t
and significant when t0 > t. So as we change t’ slightly from t - E to t+ E there is a sudden increase.
If z and w differ then it can be shown that correlation ofut(WTx - t) with δ0 (ZTx - t) essentially
depends on cot(α) where α is the angle between w and z (for a quick intuition note that one can
5
Under review as a conference paper at ICLR 2019
prove that E[ut(wT x)δ0(zT x)] = c cot(α). See Lemma 16 in Appendix). In the next section we
will show how the same ideas work for non-monotone P even if it may not have any linear terms but
we only manage to prove polynomial sample complexity for finding w instead of polynomial time
complexity.
In this section we will not correlate exactly with δ0(zTx - t) but instead we will use this high level
idea to estimate how fast the correlation with δ(zT x - t0) changes between two specific values as
one changes t0, to get an estimate for cot(α). Secondly since we can’t to a smooth optimization over
z, we will do a local search by using a random perturbation and iteratively check if the correlation
has increased. We can assume that the polynomial P doesn’t have a constant term c0 as otherwise it
can easily be determined and cancelled out6.
We will refine the weights one by one. WLOG, let Us assume that Wi = eι and We have Z such that
ZTwi = zι = cos-1(αι). Let l(z, t, E) denote {x : ZTX ∈ [t — e, t]} for Z ∈ Sn-1.
Algorithm 1 RefineEstimate
1:	Run EstimateT anAlpha on Z to get s = tan(α) where α is the angle between Z and w1i.
2:	Perturb current estimate Z by a vector along the d — 1 dimensional hyperplane normal to Z with
the distribution n(0, Θ(α∕d))d-1 to get z0.
3:	Run EstimateT anAlpha on Z0 to get s0 = tan(α0) where α0 is the angle between Z0 and w1i.
4:	if a0 ≤ O(a∕d) then
5:	z — z0
6:	Repeat till α0 ≤ E.
Algorithm 2 EstimateTanAlpha
1	: Find t1 and t2 such that P r[sgn(f (x))|x ∈ l(Z, t0, E)] at t1 is 0.4 and at t2 is 0.6.
2	: RetUm φ-i(0.62-⅞1-i(o⑷.
The algorithm (Algorithm 1) estimates the angle of the current estimate with the true vector and then
subsequently perturbs the vector to get closer after each successful iteration.
Theorem 5. Given a vector Z ∈ Sd-1 such that it is 1/poly(d)-close to the underlying true vector
w1i, that is cos-1 (ZTw1i) ≤ 1/poly(d), running Ref ineE stimate for O(T) iterations outputs a
vector Zi ∈ SdT such that CosT((Zi)TWi) ≤(1 — d) Y for some constant c > 0. Thus after
O(d log(1/E)) iterations cos-1((Zi)T w1i) ≤ E.
We prove the correctness of the algorithm by first showing that EstimateT anAlpha gives a multi-
plicative approximation to tan(α). The following lemma captures this property.
Lemma 3. EstimateT anAlpha(Z) outputs y such that y = (1 ± O(η)) tan(α) where α is the
angle between Z and w1i.
Proof. We first show that the given probability when computed with sgn(xT W1i —t) is a well defined
function of the angle between the current estimate and the true parameter up to multiplicative error.
Subsequently we show that the computed probability is close to the one we can estimate using
f (x) since the current estimate is close to one direction. The following two lemmas capture these
properties.
Lemma 4. For t, t0 and E ≤ 1/t0, we have
Pr[xTWi ≥ t and X ∈ 1(z, t0, e)|x ∈ 1(z, t, e)] = Φc f * J Cos(：1)) ±。⑹y
∖	| SingI)I	)
Lemma 5. For t0 ∈ [0, t/ Cos(α1)], we have
Pr[sgn(f (x))∣x ∈ 1(z, t0, e)] = Pr[sgn((wi)TX — t)|x ∈ l(Z,t, e)] + de~^(tt').
6for example with RELU activation, f will be c0 most of the time as other terms in P will never activate.
So c0 can be set to say the median value of f .
6
Under review as a conference paper at ICLR 2019
Using the above, we can show that,
t2 — tι = (Φ-1(0.6 — ηι ± O(e)tι) — Φ-1(0.4 —小 士。(七总))tan(α)
=(φ-1(0.6) — φ-1(0.4) — (ηι ± O(e)tι)(Φ-1)0(pι) + (η ± O(e)t2)(Φ-l)0(p2)) tan(α)
where η1, η2 > 0 are the noise due to estimating using f and p1 ∈ [0.6 — η1 ± O()t1, 0.6] and
p2 ∈ [0.4 — η2 ± O()t2, 0.4] as long as t1, t2 ∈ [0, t/ cos(α1)]. The following lemma bounds the
range of t1 and t2 .
Lemma 6. We have 0 ≤ tι ≤ t2 ≤	).
Thus, we have,
Φ-1(0.62 — Φ-1(0.4) =(I ± O m + η + 由))tan(α)
as long as η2+O()t2 ≤ cfor some constant c > 0. Thus, we can get a multiplicative approximation
to tan(α) up to error η ( can be chosen to make its contribution smaller than η).
Finally we show (proof in Appendix ??) that with constant probability, a random perturbation re-
duces the angle by a factor of (1 — 1/d) of the current estimate hence the algorithm will halt after
O(dlog(1∕ν)) iterations.
Lemma 7. By applying a random Gaussian perturbation along the d — 1 dimensional hyperplane
normal to Z with the distribution n(0, Θ(α∕d))d-1 and scaling back to the unit sphere, with Constant
probability, the angle ɑ (< π∕2) with the fixed vector decreases by at least Ω(α∕d).
□
3	Sample Complexity
We extend the methods of the previous section to a broader class of polynomials but only to obtain
results in terms of sample complexity. The main idea as in the previous section is to correlate with
δ0(zT x —t) (the derivative of the dirac delta function) and find arg max||z||2=1 E[f(x)δ0(zTx — t)].
We will show that the correlation goes to infinity when Z is one of Wi and bounded if it is far from
all of them. From a practical standpoint we calculate δ0(zT x — s) by measuring correlation with
土(δ(zTX — S + E) — δ(zTX — S — e). In the limit as E → 0 this becomes δ0(zTX — s). δ(zTX - S)
in turn is estimated using ɪ(Sgn(ZTX — S + E) — Sgn(ZTX — s)), as in the previous section, for an
even smaller E; however, for ease of exposition, in this section, we will assume that correlations with
δ(ZT X — S) can be measured exactly.
Let us recall that f(X) = P(u((w1i)TX), u((w2i)TX), . . . , u((wdi)TX)). Let C1(f, Z, S) denote
E[f(X)δ(ZTX — S)] and let C2(f, Z, S) denote E[f (X)(δ(ZTX — S — E) — δ(ZT X — S + E)].
If U = sgn then P has degree at most 1 in each Xi. Let ∂P denote the symbolic partial derivative
of P with respect to Xi; so, it drops monomials without Xi and factors off Xi from the remaining
ones. Let us separate dependence on Xi in P as follows:
P(X1, , .., Xd) = XiQi(X1, ..Xi-1, Xi+1, .., Xd) + R1(X1, .Xi-1, Xi+1, .., Xd)
then ∂XPi = Qi
We will overload the polynomial P such that P[X] to denote the polynomial computed by substitut-
ing Xi = u((w1i )TX) and similarly for Q and R. Under this notation f (X) = P [X]. We will also
assume that |P(X)| ≤ ||X ||O(1) = ||X ||c1 (say). By using simple correlations we will show:
Theorem 6. If u is the sgn function, P(X) ≤ ||X ||c1 and for all i, E[Qi[X]|(wii)TX = t] ≥ E3
then using poly( ^d^) samples one can determine the Wi ,s within error e2.7
Note that if all the wii’s are orthogonal then Xi are independent and E Qi[X](wii)TX = t is just
value of Qi evaluated by setting Xi = 1 and setting allthe the remaining Xj = μ where μ = E [Xj].
This is same as 1∕μ times the coefficient of Xi in P(μ(X1 + 1),..., μ(Xd + 1)).
7The theorem can be extended to ReLU by correlating with the second derivative δ00 (see Appendix C.1).
7
Under review as a conference paper at ICLR 2019
Corollary 1. If u is the Sgnfunction and Wis are orthogonal then in sample complexity poly(^d-)
one can determine Wi within error e? in each entry, if the coefficient of the linear terms in
P(μ(Xι + 1), μ(X2 + l),μ(X3 + 1),..) is larger than sμ, where μ = E[Xi].
The main point behind the proof of Theorem 6 is that the correlation is high when z is along one of
wii and negligible if it is not close to any of them.
Lemma 8. Assuming P(X) < ||X∣∣c1. If Z = Withen C2(f, z,t) = φ(t)E [∂dXp7∣zτX = t] +
dO(1). Otherwise if all angles αi between z and wii are at least 2 it is at most dO(1) /2.
We will use the notation g(x)x=s to denote g(x) evaluated at x = s. Thus Cauchy’s mean value
theorem can be stated as g(x + ) - g(x) = [g0(s)](s = s0 ∈ [x, x + ]). We will over load
the notation a bit: φ(zT x = s) will denote the probability density that vzTx = s; so if z is a
unit vector this is just φ(s); φ(z1T x = s1, z2Tx = s2) denotes the probability density that both
z1Tx = s1, z2Tx = s2; so again if z1, z2 are orthonormal then this is just φ(s1)φ(s2).
The following claim interprets correlation with δ(zT x - s) as the expected value along the corre-
sponding plane zTx = s.
Claim 3. E[f(x)δ(zTx - s)] = E[f(x)|zTx = s]φ(zTx = s).
The following claim computes the correlation of P with δ0(zT x - s).
Claim 4. E[P[x]δ0(zTX = s)] is equal to P" cot(ai)∣φ(zTX = s, (w*)TX = t)
E h∂dXp7[x]∣zTx = s, (wi)Tx = t] + φf(s)E[P[x]∣zTX = s].
We use this to show that the correlation is bounded if all the angles are lower bounded.
Claim 5. IfP(X) ≤ ||X ||c1 and ifz has an angle ofat least 2 with all the Wii ’s then C2(f, z, s) ≤
dO(1)/2.
Above claims can be used to prove main Lemma 8. Refer to the Appendix C for proofs.
Proof of Theorem 6. If we wish to determine Wii within an angle of accuracy 2 let us set to be
O(32φ(t)d-c). From Lemma 8, for some large enough c, this will ensure that if all αi > 2 the
correlation is o(φ(t)g). Otherwise it is φ(t)s(1 ±o(1)). Since φ(t) = poly(1∕d), givenPoIy(^d-)
samples, We can test if a given direction is within accuracy ⑦ of a Wi or not.	□
4	Stronger Results under Structural Assumptions
Under additional structural assumptions on Wi such as the weights being binary, that is, in {0, 1},
sparsity or certain restrictions on activation functions, we can give stronger recovery guarantees.
Proofs have been deferred to Appendix D.
Theorem 7.	For activation ut (a) = eρ(a-t). Let the weight vectors Wii be 0, 1 vectors that select
the coordinates of X. For each i, there are exactly d indices j such that Wij = 1 and the coefficient
of the linear terms in P(μ(X1 + l),μ(X2 + l),μ(X3 + 1),..) for μ = e-ρt is larger than the
coefficient of all the product terms (constant factor gap) then we can learn the Wi .
In order to prove the above, we will construct a correlation graph over x1, . . . , xn and subsequently
identify cliques in the graph to recover Wii ’s.
With no threshold, recovery is still possible for disjoint, low l1 -norm vector. The proof uses simple
correlations and shows that the optimization landscape for maximizing these correlations has local
maximas being Wii ’s.
Theorem 8.	For activation u(a) = ea. If all Wii ∈ {0, 1}n are disjoint, then we can learn Wii as
long as P has all positive coefficients and product terms have degree at most 1 in each variable.
For even activations, it is possible to recover the weight vectors even when the threshold is 0. The
technique used is the PCA like optimization using hermite polynomials as in Section 2. Denote
C(S, μ) = Ps⊆S0⊆[n] cS0μlS l.
8
Under review as a conference paper at ICLR 2019
Theorem 9.	If the activation is even and for every i,j: C({i},U°) + C({j},Uo) >
6U2
-^-2-C({i,j}, uo) then there exists an algorithm that can recover the underlying weight vectors.
5 Conclusion
In this work we show how activations in a deep network that have a high threshold make it easier to
learn the lowest layer of the network. We show that for a large class of functions that represent the
upper layers, the lowest layer can be learned with high precision. Even if the threshold is low we
show that the sample complexity is polynomially bounded. An interesting open direction is to apply
these methods to learn all layers recursively. It would also be interesting to obtain stronger results if
the high thresholds are only present at a higher layer based on the intuition we discussed.
References
Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in
non-convex optimization. arXiv preprint arXiv:1602.05908, 2016.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep
representations. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International
Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research,
pp. 584-592, Bejing, China, 22-24 JUn 2014. PMLR. URL http://proceedings.mlr.
press/v32/arora14.html.
Alon BrUtzkUs and Amir Globerson. Globally optimal gradient descent for a convnet with gaUssian
inpUts. arXiv preprint arXiv:1702.07966, 2017.
Simon S DU, Jason D Lee, and YUandong Tian. When is a convolUtional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017a.
Simon S DU, Jason D Lee, YUandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de-
scent learns one-hidden-layer cnn: Don’t be afraid of spUrioUs local minima. arXiv preprint
arXiv:1712.00779, 2017b.
Rong Ge, Jason D Lee, and TengyU Ma. Learning one-hidden-layer neUral networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
SUrbhi Goel and Adam Klivans. EigenvalUe decay implies polynomial-time learnability for neUral
networks. arXiv preprint arXiv:1708.03708, 2017a.
SUrbhi Goel and Adam Klivans. Learning depth-three neUral networks in polynomial time. arXiv
preprint arXiv:1709.06010, 2017b.
SUrbhi Goel, VarUn Kanade, Adam Klivans, and JUstin Thaler. Reliably learning the ReLU in
polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Majid Janzamin, Hanie Sedghi, and Anima AnandkUmar. Beating the perils of non-convexity: GUar-
anteed training of neUral networks Using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
YUanzhi Li and Yang YUan. Convergence analysis of two-layer neUral networks with ReLU activa-
tion. arXiv preprint arXiv:1705.09886, 2017.
Jorge Nocedal and Stephen J Wright. NUmerical optimization 2nd, 2006.
Santosh S. Vempala. A random-sampling-based algorithm for learning intersections of halfspaces.
J. ACM, 57(6):32:1-32:14, November 2010. ISSN 0004-5411. doi: 10.1145/1857914.1857916.
URL http://doi.acm.org/10.1145/1857914.1857916.
Wikipedia contribUtors. Hermite polynomials — Wikipedia, the free encyclopedia, 2018. URL
https://en.wikipedia.org/w/index.php?title=Hermite_polynomials&
oldid=851173493. [Online; accessed 26-September-2018].
9
Under review as a conference paper at ICLR 2019
Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi. Electron-proton dynamics in deep
learning. arXiv preprint arXiv:1702.00458, 2017.
Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and
neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural
networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
10
Under review as a conference paper at ICLR 2019
A	Prerequisites
A. 1 Hermite Polynomials
Hermite polynomials form a complete orthogonal basis for the gaussian distribution with unit vari-
ance. For more details refer to Wikipedia contributors (2018). Let hi be the normalized hermite
polynomials. They satisfy the following,
Fact 0. E[hn(x)] = 0 for n > 0 and E[h0(x)] = 1.
Fact 1. Ea〜N(o,i)[hi(a)hj(a)] = δj where δj = 1 iff i = j.
This can be extended to the following:
Fact 2. For a, b with marginal distribution N(0, 1) and correlation ρ, E[hi(a)hj(b)] = δijρj.
Consider the following expansion of u into the hermite basis (hi),
∞
u(a) =	^i hi (a).
i=0
Lemma 9. For unit norm vectors u, V, E[u(vTx)hj(WTx)] = Uj (VTw)j.
Proof. Observe that vTx and wTx have marginal distribution N(0, 1) and correlation vTw. Thus
using Fact 2,
∞∞
E[u(vT x)hj (WT x)] = ^X UiE[hi(vTx)hj (wTx)] = ^X Uiδij (VT w)j = Uj (VT w)j.
i=1	i=1
□
For gaussians with mean 0 and variance σ2 define weighted hermite polynomials Hlσ (a) =
∣σ∣lhι(a∕σ). Given input VTX for X 〜 N(0, I), We suppress the superscript σ = ||v||.
Corollary 2. For a non-zero vector V (not necessarily unit norm) and a unit norm vector W,
E[Hi(VTx)hj (WTx)] = δij (VTW)j.
Proof. It follows as the proof of the previous lemma,
∞∞
E[u(vt x)hj (wt x)] = ^X UiE[hi(VTx)hj (WTx)] = ^X Uiδij (vt w)j = Uj (vt w)j.
i=1	i=1
□
Fact3. hn(x + y) = 2-2 £"=° ⑦hn-(x√2)hk(y√2).
Fact4. hn(γx) = Pk=0 Yn-2k(γ2 - 1)kQJ等2-k%-2k(x).
Fact 5. α(n,m,γ) = E[hm(x)hn(γx)] = Yn-2k(γ2 - 1产(藐)甯2-k for k = n-m if k ∈ Z+
else 0.
A.2 Properties of Matrices
Consider matrix A ∈ Rm×m. Let σi(A) to be the ith singular value of A such that σ1(A) ≥
σ2(A) ≥ ... ≥ σm(A) and set K(A) = σι(A)∕σm(A).
Fact 6. |det(A)| = Qim=1 σi (A).
Fact 7. Let B be a (mk) × (mk) principal submatrix ofA, then κ(B) ≤ κ(A).
11
Under review as a conference paper at ICLR 2019
A.3 Activation Functions
Lemma 10. For u being a high threshold ReLU, that is, ut (a) = max(0, a - t) we have for t ≥ C
t2
for large enough constant C > 0, Eg~N(0 σ2)[ut(g)] ≤ e 2σ2. Also, U4,U2 = tθ(1)e-ɪ.
Proof. We have	「/1	f∞	/	、- g2 Eg~N(0,σ2)[ut(g)]= √2∏σ J max(0,g - t)e 2σ2 dg 1	f∞	g2 =√2∏σ JJg- t)e 2σ2dg ,1 r -y， ≤ I		ge 2σ2 dg √ √2∏σ Jt y	J ∞ =			 I e~hdh 2σ2 σ	t2 =	,	e 2σ2 . √2∏
Also,	u4 = Eg~N(0,1) [ut (g)h4(g)] =-1= [ max(0,g - t)(g4 - 6g2 + 3)e-号dg 2∏ -∞ =√= Z (g - t)(g4- 6g2+3)e-ɪdg 2∏ t ≥ -ɪ(t4 - 6t2)1 e-12T-212 -√2∏v	't ≥ Ω (t3e-12).
To upper bound,	u4 =，- I max(0,g — t)(g4 — 6g2 + 3)e-号 dg 2∏ -∞ =√≡= Z (g - t)(g4- 6g2+3)e-ɪdg 2∏ t "J 2g5e-*dg 2∏ t =-L Z∞ BeThdh √2∏ J 咚 =O (t4e-12 ).
Similar analysis holds for U2.	口
Observe that sgn can be bounded very similarly replacing g - t by 1 which can affect the bounds up
to only a polynomial in t factor.
Lemma 11. For u being a high threshold sgn,	that is,	ut (a)	= sgn(a	- t) we have for t	≥ Cfor
r ,	_	t2	…八、 t2
large enough constant C > 0, Eg~N(0 σ2)[ut(g)] ≤ e 2σ2. Also, u4, ^2 = tθ(1)e-ɪ.
For sigmoid, the dependence varies as follows:
Lemma 12. For U being a high threshold sigmoid, that is, ut(a) = i+e-(a-t) we have for t ≥ C
σ2
for large enough constant C > 0, Eg~N(0,σ2) [ut(g)] ≤ e-t+ ɪ. Also, U4,U2 = Θ(e-t).
12
Under review as a conference paper at ICLR 2019
Proof. We have
Eg 〜N (0,σ2)[ut(g)]
1	∞	1	- B 1
,  _______________7__TVe 2σ2 dg
√2∏σ J-∞ 1 + e-(g- )
e-t
√2πσ
Z∞
∞
1
e-t + e-g
e-邑 dg
e-t	∞ --正 T
≤	-	eg e 2σ2 dg
2πσ -∞
2
e-te 号 / ∞
-∞
-1 σ2
e te 2
-(g-σ2)2
e	2σ2 dg
Also,
u4 = Eg 〜N (0,1)[ut(g)h4(g)]
ɪ「
22π --∞
U厂
√2π J-∞
------------e
1 + e-(g-t)
e⅛g (g4- 6g2+3)e--dg
〉」/
_ √2π Jo
〉U /
一 √2π Jo
=Ω(e-t).
二-g (g4 - 6g2 + 3)e-* dg
∞
1

□
We Can upper bound similarly and bound U.
B Approximate Recovery with Linear Terms
B.1 Constrained Optimization View of Landscape Design
Let Us consider the linear case with WyS are orthonormal. Consider the following maximization
problem for even l ≥ 4,
max Sgn(Ul) ∙ E f (x) ∙ Hl (ZTx)]
z∈Sn-1
where hl is the lth hermite polynomial. Then we have,
Sgn(Ul) ∙ E [f (x) ∙ hl (ZTx)] = Sgn(Ul) ∙ E [(工 Ciut((W：)Tx)J ∙ hl (ZTx)
k
=sgn(Ul) ∙ XCiE [ut((wi)Tx) ∙ hl (ZTx)]
i=1
k
=lull XCi((Wi产z)l.
i=1
It is easy to see that for Z ∈ Sn-1, the above is maximized at exactly one of the Wi’s (up to sign flip
for even l) for l ≥ 3 as long as Ul 6= 0. Thus, each Wi is a local minima of the above problem.
Let L(Z) = - Pik=1 Cizil. For constraint llZll2 = 1, we have the following optimality conditions
(see Nocedal & Wright (2006) for more details).
13
Under review as a conference paper at ICLR 2019
First order:
VL(Z) - z |；『Z = 0 and ||z||2 = L
P ci zl
This applied to our function gives Us that for λ = 一 ∕z∣∣2i (λ < 0),
-lcizil-1 - 2λzi = 0
The above implies that either Zi = 0 or ZJ-= 一/ with ∣∣z∣∣2 = 1. For this to hold z is such that
for some set S ⊆ [n], |S| > 1, only i ∈ S have zi 6= 0 and Pi∈S zi2 = 1. This implies that for all
l -2 Q J-I2 —	2λ
i ∈ S, Zi	= 一百.
Second order:
For all w 6= 0 such that wTz = 0, wT (V2L(z) 一 2λI)w ≥ 0.
For our function, we have:
V2L(z) = -l(l - 1)diag(c ∙ z)l-2
⇒ (V2L(z))	2(l 一 1)λ ifi=jandi∈S
ij 0	otherwise.
The last follows from using the first order condition. For the second order condition to be satisfied
we will show that |S | = 1. Suppose |S | > 2, then choosing w such that wi = 0 for i 6∈ S and such
that wTz = 0 (it is possible to choose such a value since |S| > 2), we get wT (V2L(z) 一 2λI)w =
2(l — 2)λ∣∣w∣∣2 which is negative since λ < 0, thus these cannot be global minima. However, for
|S | = 1, we cannot have such a w, since to satisfy wT z = 0, we need wi = 0 for all i ∈ S, this
gives Us WT(V2L(z) 一 2λI)w = -2λ∣∣w∣∣2 which is always positive. Thus z = ±ei are the only
local minimas of this problem.
B.2	Important Results from Ge et al. (2017)
Lemma 13 (GeetaL(2017)). If z is an (e, τ)-local minima of F (z) = — Pi aiZ4 + λ(Pi Zil — 1)2
for E ≤，丁3∕αmiη where ɑmin = mini α, then
•	(Lemma 5.2) ∣z∣2nd ≤ ，oT- Where ∣z∣2nd denotes the magnitude of the second largest
entry in terms of magnitude of z.
•	(Derived from Proposition 5.7) zmax = ±1 ± O (dτ∕αmin) ± O(E∕λ) where |z|max is the
value of the largest entry in terms of magnitude of z.
B.3	Omitted Proofs for One-by-one Recovery
Proof of Lemma 1. Let O ∈ Rd×d be the orthonormal basis (row-wise) of the subspace spanned by
Wi for all i ∈ [d] generated using Gram-schmidt (with the procedure done in order with elements
of |S| first). Now let OS ∈ RlSl×d be the matrix corresponding to the first S rows and let O⊥ ∈
R(d-lSl)×n be that corresponding to the remaining rows. Note that OWi (Wi also has the same
ordering) is an upper triangular matrix under this construction.
E	ut((Wji)Tx)
j∈S
函1M Z YUt(XTwi)e- Wdx
x i∈S
1	f t—r	T	IIOSχll2 + ∣∣o⊥χll2
(2∏)n∕2 J ∏ut((OSWi) Osx)e	2	dx
x i∈S
(⅛L-UUt((OSWi)TxWdx)(西
IIx0II2 j 0
e 2 dx
0 ∈Rd-ISI
14
Under review as a conference paper at ICLR 2019
(⅛ ∕o∈r∣s∣YS Ut((OS 2"中 dx'
Idet(OS WS )|-1
(2π) ɪ
-	π 人、-MOS WS 厂T b112 %
ut(bi)e	2	db
Jb∈RlSl i∈S
NoW observe that OS WS is also an upper triangular matrix since it is a principal sub-matrix of
OW*. Thus using Fact 6 and 7, we get the last equality. Also, the single non-zero entry row has
non-zero entry being 1 (∣∣w*∣∣ = 1 for all i). This gives us that the inverse will also have the single
non-zero entry row has non-zero entry being 1. WLOG assume index 1 corresponds to this row.
Thus we can split this as following
E	ut((wj* )Tx)
j∈S
≤
Idet(OS WS )∣T(W / ut(bι)e-b1 db) I Y WLut2||OSbW S1|2 d%
Idet(OSWS* )I-1
≤
ut(b1)e-ɪ dbl) ( Y √2~ / - ut(bi)e ||W*||2 dbi
≤ ρ(t, 1) (κ(W*)ρ(t, IIW*II))|S|-1
□
ProofofClaim 1. Consider the SVD of matrix M = UDUT. Let W = UDT/2 and y%
√ciWτw* for all i. It is easy to see that yi are orthogonal. Let F(Z) = G(Wz):
F(Z) = |u4|XCi(ZTWTw*)4 - λu2 (XCi(ZTWTw*)2 - 1)
2
∣u4∣X 1(ztyi)4-λu2 (X(ZTyi)2-1
ii	i
Since yi are orthogonal, for means of analysis, we can assume that yi = ei, thus the formulation
reduces to maxz ∣u4∣ Pi C-(zi)4 - λ0 (∣∣z∣∣2 - 1) up to scaling of λ0 = λu2. Note that this is of
the form in Lemma 13 hence using that we can show that the approximate local minimas of F(Z) are
close to yi and thus the local maximas of G(Z) are close to Wyi = √ciWWTWi = √ciM-1w*
due to the linear transformation. This can alternately be viewed as the columns of (TW*)-1 since
TW*M-1(TW*)t = I.	□
ProofofTheorem 4. Let Z be an (e, τ)-local minimum of A, then we have ∣∣VA(Z)∣∣ ≤ E and
λmin(V2A(Z)) ≥ -τ. Observe that
IIVB(Z)II = IIV(A + (B - A)(Z)II ≤ IIVA(Z)II + IIV(B - A)(Z)II ≤ E + ρ.
Also observe that
λmin(V2B(Z)) = λmin(V2(A + (B - A))(Z))
≥ λmin(V2A(Z)) + λmin(V2(B - A)(Z))
≥ -τ - IIV2(B - A)(Z)II ≥ -τ-γ
Here we use Iλmin(M)I ≤ IIMII for any symmetric matrix. To prove this, we have IIMII =
maxx∈Sn-1 IIMxII. We have x = Pi xivi where vi are the eigenvectors. Thus we have Mx =
Pi Xiλi(M)vi and P x2 = 1. Which gives us that ∣∣M∣∣ = PPi χ2λ2(M) ≥ ∣λmin(M)∣.	□
15
Under review as a conference paper at ICLR 2019
ProofofLemma 2. Expanding f, we have
E[∣∆(x)∣]= E	E	CS ∏ut((w;)Tx)
SC[d]：|S|>1 j∈S
≤ E	∣cs∣e ɪɪut((w;)Tx)
SC[d]：|S|>1	j∈S
using Lemma 1
≤ C E	ρ(t,i)
SC[d]：|S|>1
(bmin(W*)
ρ(t,∣∣w*∣∣))
|S|-1
using
using assumption on t
=CX CIP也 D (σ-⅛)ρ(t,∣∣w*∣∣))i 1
i=1 ∖i∕	\0mm( W )	)
d	/ A	、i—1
≤C XdPg UnTW) ρ(t,∣∣w*∣∣))
≤ cd2ρ亿 1)(σmdW) ρ(t,∣∣w*∣∣))
□
Lemma 14. For anyfunction L such that 11 L(z, x) ∣ ∣ ≤ C(z) ∣ ∣ x∣ ∣。⑴ where C is afunction that is
not dependent on x, we have ∣ ∣ E[∆(x)L(x)] 11 ≤ C(z)d-(1+p)η+3O(log d).
Proof. We have
E[∆(x)L(x)]
≤ E[ ∆(x) L(x) ]
≤ E[∆(x)C(z)∣∣x ∣ ∣ O(1)]
=C(z) (E[∣∆(x) ∣∣ ∣ X『⑴ ∣∣ ∣ x∣ ∣ ≥ c]Pr[∣∣x ∣ ∣ ≥ c]
+ E[∣∆(x) ∣∣ ∣ x∣ ∣ o⑴ ∣∣ ∣ X ∣∣ < c]Pr[∣∣ X ∣∣<c])
≤ C(z)(E[∣∣ X ∣∣o⑴ ∣∣∣x ∣ ∣ ≥ c]Pr[∣∣ X ∣ ∣ ≥ c] + cE[∣∆(x) ∣])
2
=C(Z)(CO ⑴ e-万 + CO ⑴ E[∣∆(x) ∣ ]).
Now using Lemma 2 to bound E[ ∣ ∆(x) ∣ ], for C = Θ(√η log d we get the required result.	□
Lemma 15. For 11 Z∣ ∣ = Ω(1) and λ = Θ(∣ U41 /U2) ≈ dη, 11 VG(z) ∣ ∣ ≥ Ω(1)d-η.
Proof. Let K = κ(W*) which by assumption is θ(1). We will argue that local minima of G cannot
have Z with large norm. First lets argue this for GIin(Z). Weknow that Gim(z) = -α P(ZTW勃4 +
λβ2((P(ZTw*)2) - 1)2 where α = ∣ U4 ∣ and β = U2. We will argue that zτVGiin(Z) is large if Z
is large.
ZTVGiin(Z) = -4α X(ZTw*)3(ZTw*) + 2λβ2 (f(ZTw*)2 - 1) (X 2(ZTw*)(ZTw*))
=-4α X(ZTw*)4 + 4λβ2 (X(ZTw*)2 - 1) (X(ZTw*)2)
Let y = W*z then K 11 Z ∣ ∣ ≥ ∣ ∣ y∣ ∣ ≥ ∣ ∣ Z ∣ ∣ /K since K is the condition number of W*. Then this
implies
zTVGiin(z) = -4α X y4 + 4λβ2(∣ ∣ y∣ ∣ 2 - 1) ∣ ∣ y∣ ∣ 2
=41 ∣ y ∣ ∣ 2 ((-α + λβ2) ∣∣y ∣∣2 + λβ2)
≥ ∣ ∣ y∣∣4(-α + λβ2) ≥ Ω(1)d-η 11 y∣ ∣ 4
16
Under review as a conference paper at ICLR 2019
Since ||y|| ≥ ||z||/K = Ω(1) by assumptions on λ, Z we have zτVGii∏(z) ≥ Ω(λβ2∣∣y∣∣4)=
Ω(1)d-η||z||4. This implies ||VGiin(z)|| = Ω(1)d-η||z||3.
Now we need to argue for G.
G(Z)- GIin(Z)
=-Sgn(U4)E[(fii∏(x) + ∆(x))H4(zτx)] + λ(E[(fii∏(x) + ∆(x))H2(zτx)] - β)2
+ Sgn(U4)E[(fiin(x))H4(zτx)] - λE[(力in(x))H2(zτx)] - β]2
=-Sgn(U4)E[∆(x)H4 (zτ x)] + λE[∆(x)H2(ZT x)]2 + 2λE[∆(x)H2(ZT x)]E[fιin (x)H2(zτ x) — β]
=-Sgn(U4)||z||4 E[∆(x)h4(ZTX/||z||)] + X||z||4E[4(x)h2(ZTX/||z||)]2
+ 2X||z||4ERx)h2(ZTX/||z||)]E[fim(x)h2(ZTX/||z||)] - 2λβ||z||2E[△(x)h2(ZTX/||z||)]
Now h4(zτx/||z||) doesn,t have a gradient in the direction of Z so zτVh4(zτx/||z||) = 0. Simi-
larly zτVh2(zτx/||z||) = 0. So
ZTV(G(Z) - Giin(Z))
=-4Sgn(U4)||z||4 E[∆(x)h4 (zτ x/||z||)] +4A||z||4 (E[∆(x)h2(zτ x/||z||)])2
+ 8X||z||4EHx)h2(ZTX/||z||)]E[fim(x)h2(ZTX/||z||)] - 4λβ||z||2E[△(x)h2(ZTX/||z||)]
We know that E[fiin(x)h2(ZTx/||z||)] has a factor of β giving US using Lemma 14:
|zTV(G(z) - Giin(z))| ≤ O(logd)d-d+p)n+3||z||4.
So zτVG(z) is also Q(||z||4). so ||VG(z)|| ≥ Ω⑴d-η	□
ProofofClaim 2. We have G - Giin as follows,
G(z) - Giin(z)
=-Sgn(U4)E[(力in(x) + ∆(x))H4(zτx)] + λ(E[(fiin(x) + ∆(x))H2(zTx)] - U2)2
+ sgn(U4)E[(fiin(x))H4(zτx)] - λ(E[(fiin(x))H2(zτx)] - U2)2
=-Sgn(U4)E[∆(x)H4(zτ x)] + λ(E[∆(x)H2(ZT x)])2
+ 2λE[∆(x)H2(ZTX)]E[fiin(x)H2(ZTX) - U2]
Thus we have,
V(G(z) - Giin(z))
=-sgn(U4)E[∆(x)VH4(zτ x)] + 2λE[∆(x)H2(ZT x)]E[∆(x) VH2(zτ x)]
+ 2λE[fiin(x)H2(ZTX) - U2]E[∆(x)VH2(zτx)]
+ 2λE[∆(x)H2(ZTX)]E[fiin(x)VH2(zτx)]
Observe that H2 and H4 are degree 2 and 4 (respectively) polynomials thus norm of gradient and
hessian of the same can be bounded by at most O(||z||||x||4). Using Lemma 14 we can bound each
term by roughly O(logd)d-(1+p)n+3||z||4. Note that λ being large does not hurt as it is scaled
appropriately in each term. Subsequently, using Lemma 15, we can show that ||z|| is bounded by a
constant since ||G(z)|| ≤ d-2η. Similar analysis holds for the hessian too.
Now applying Theorem 4 gives us that Z is an (O(log d)d-(1+p)η+3, O(log d)d-(1+p)η+3)-
approximate local minima of Gh∏. This implies that it is also an (Y := Clog(d)d-(1+2p)η+3, T0 :=
C log(d)d-(1+2p/3)n+3)-approximate local minima of Giin for large enough C >	0
by increasing T. Observe that pτ3/|u4| = C3/2 log3/2(d)d-(3/2+p)n+9/2/d-n/2 =
C3/2 log3/2 (d)d-(1+p)η+9/2 ≥ ez. Now using Claim 1, we get the required result.	□
B.4 Simultaneous Recovery
Ge et al. (2017) also showed simultaneous recovery by minimizing the following loss function Giin
defined below has a well-behaved landscape.
Giin(W) = E	fiin(x)	X	ψ(wj, Wk, x)	-	YE	fiin(x)	X	H4(wτ x)	(1)
j,k∈[d],j≠k	j∈[d]
17
Under review as a conference paper at ICLR 2019
+ λ X (E fiin(x)H2(WTx)] - U2)2	(2)
i
where ψ(v, w, x) = H2 (vTx)H2 (wTx) + 2(vT w)2 + 4(vT x)(wT x)vT w.
They gave the following result.
Theorem 10 (Ge et al. (2017)). Let c be a sufficiently small universal constant (e.g. c = 0.01
suffices), and suppose the activation function U satisfies U4 = 0. Assume Y ≤ c, λ ≥ Ω(∣U4∣∕U2),
and W* be the true weight matrix. Thefunction Glin satisfies the following:
1.	Any saddle point W has a strictly negative curvature in the sense that λmin(V2Glin(W)) ≥
一to where τo = Cmin{γ∣U4∣∕d, λU2}.
2.	Suppose W is an (, τ0)-approximate local minimum, then W can be written as W-T =
PDW* + E where D is a diagonal matrix with Dii ∈ {±1 ± O(γ∣U4∣∕λu2) ± O(e∕λ)},
P is a permutation matrix, and the error term ||E|| ≤ O(cd/U4).
We show that this minimization is robust. Let us consider the corresponding function G to Glin with
the additional non-linear terms as follows:
G(W)= E f(x) E	ψ(wj, Wd, x)	一 YE	f(x) EH4(Wj, x)
j,k∈[d],j 6=k	j∈[d]
+ λX (E [f (x)H2(wi, x)] — U)
i
Now we can show that G and Glin are close as in the one-by-one case.
R(W) := G(W) 一 Glin(W)
= E [∆(x)A(W, x)]	一	YE	[∆(x)B(W, x)]	+	λ E [f(x)C(W, x)]2	一 E	[flin(x)C(W, x)]2
= E [∆(x)A(W, x)]	一	YE	[∆(x)B(W, x)]	+	λE [(∆(x)C(W, x)(f	(x0)	+ flin(x0))C(W, x0)]
= E [∆(x)A(W, x)]	一	YE	[∆(x)B(W, x)]	+	λE [(∆(x)D(W, x)]
= E [∆(x)(A(W, x)	一	YB(W, x) + λD(W,	x))]
= E [∆(x)L(W, x)]
where A(W, x) = Pj,k∈[d],j6=k ψ(Wj, Wd, x), B(W, x) = Pj∈[d] H4(Wj,x), C(W, x) =
Pi H2(Wi, x), D(W, x) = C(W, x)E[(f (x0) + flin(x0))C(W, x0)] and L(W, x) = A(W, x) 一
YB(W, x) + λD(W, x).
Using similar analysis as the one-by-one case, we can show the required closeness. It is easy
to see that ||VL|| and ∣∣V2L∣∣ will be bounded above by a constant degree polynomial in
O(log d)d-(1+p)η+3 max ||Wi||4. No row can have large weight as if any row is large, then looking
at the gradient for that row, it reduces to the one-by-one case, and there it can not be larger than a
constant. Thus we have the same closeness as in the one-by-one case. Combining this with Theorem
10 and 4, we have the following theorem:
Theorem 11. Let c be a sufficiently small universal constant (e.g. c = 0.01 suffices), and under
Assumptions 1, 2 and 3. Assume Y ≤ c, λ = Θ(dη), and W* be the true weight matrix. The function
G satisfies the following
1.	Any saddle point W hasa strictly negative curvature in the sense that λmin(V2Glin(W)) ≥
—τ where to = O(log d)d-Q⑴.
2.	Suppose W is a (d~ςι(1), d~ςι(1))-approximate local minimum, then W can be written as
W-t = PDW* + E where D is a diagonal matrix with Dii ∈ {±1 ± O(Y) ± d-Q(1))},
P is a permutation matrix, and the error term ||E|| ≤ O (log d)d-Q(1).
Using standard optimization techniques we can find a local minima.
18
Under review as a conference paper at ICLR 2019
B.5 Approximate to Arb itrary Close
Lemma 16. If U is the sign function then E[u(wT x)δ0 (ZT x)] = c| cot(α)∣ where w, Z are unit
vectors and α is the angle between them and c is some constant.
Proof. WLOG we can work the in the plane spanned by Z and w and assume that Z is the vector
i along and w = i cos α + j sin α. Thus we can replace the vector x by ix + jy where x, y are
normally distributed scalars. Also note that u0 = δ (Dirac delta function).
E[u(wT x)δ0(ZT x)] = E[u(x cos α + y sin α)δ0(x)]
=	u(x cos α + y sin α)δ0(x)φ(x)φ(y)dxdy
Using the fact that x δ0(x)h(x)dx = h0(0) this becomes
=φ φ(y)[(∂/∂x)u(x cos α + y Sin a)φ(x)]χ=0dy
y
= φ(y)[n(x)u0(x cos α + y sin α) cos α +φ0(x)u(xcos α + y sin α)]x=0dy
y
= φ(y)φ(0)δ(y sin α) cos αdy
y=-∞
Substituting s = y sin α this becomes
Z∞/ sin α
=-∞/ sin α
(s/ sin α)φ(0)δ(s) cos α(1/ sin α)ds
=sgn(sin α) cot(α)φ(0)
φ(s/
s
sin α)δ(s)ds
=| cot(α) ∣φ(0)φ(0)
□
Proof of Lemma 4. Let us compute the probability of lying in the -band for any t:
Pr[x ∈ l(Z, t, )] = Pr[t - ≤ ZTx ≤ t]
= Pr	[t - ≤ g ≤ t]
g∈N (0,l∣z∣l2)	——
1 ft - g2
=—==---------- e 2||z||2 dg
√2∏l∣z∣l Jg=t-e
---------e- WF
√2πllzl1
where the last equality follows from the mean-value theorem for some t ∈ [t - e,t].
Next we compute the following:
Pr[xTw； ≥ t and X ∈ l(z, t0, e)]
1 f /	…「一 7/	,/	--LME T
= 仆、n	sgn(xι - t)l[x ∈ l(z,t,e)]e	2 dx
(2∏)2 Jx
1	∞ -χl (	1	f …	,,	、1-Iix-Iii2 T ʌ T
=---ɪ	e 2	—-^-T	I[x-1 ∈ l(z-i,t - zιxι,e)]e	2	dx-i dxι
(2π) 2 Xxi =t	∖(2∏) F Jx-I	)
1	∞ 垃
=(2)i J e 2 Pr[x-ι ∈ l(z-ι,t — zιxι, e)]dx-ι
e	t0	∞	x2 — (g-zιχι)2
二 2∏ra「el=「e 2i j12 dxιdg
19
Under review as a conference paper at ICLR 2019
1
2n||z-1||
1
A||z||
-	-_∞
JL-e 2llzll2 JLe
L e


GL群)2
2 IB-ill2
Ilzll2 dx∖dg
高φj恤12— gzι)) dg
I ||z-i||||z|| d g
e - t*2 示 C
-^=e 2 Φc
√2∏
t — t * cos(αι)
| sin(αι)|
where the last equality follows from the mean-value theorem for some t* ∈ [t0 — e, t0]. Combining,
we get:
Pr[xτwι* ≥ t and x ∈ l(z, t0, e)|x ∈ l(z, t, e)]
t* 2-e2
Φc
t — t* cos(αι)
| sin(αι)∣
Φc
t — t* cos(αι)
| sin(αι)∣
士 O(e)tz
e 2

for E ≤ 1∕t0.
□
ProofofLemma 5. Recall that P is monotone with positive linear term, thus for high threshold U (0
unless input exceeds t and positive after) we have sgn(f (x)) = Vsgn(xτw* — t). This is because,
for any i, P applied to Xi > 0 and ∀j = i, Xj = 0 gives us Ci which is positive. Also, P(0) = 0.
Thus, sgn(P) is 1 if any of the inputs are positive. Using this, we have,
Pr[sgn(f (x))|x ∈ l(z,t0, e)] ≥ Pr[sgn((w*)τx — t)|x ∈ l(z,t0,e)]
Also,
Pr[sgn(f(x))|x ∈ l(z,t0,e)]
≤ XPr[sgn(xτw* — t)|x ∈ l(z,t0,e)]
=Pr[sgn((w*)τX — t)|x ∈ l(z,t0, e)] + ^XPr[sgn(xTw* — t)|x ∈ l(z,t0,e)]
i=1
≤ Pr[sgn((w*)τx — t)|x ∈ l(z,t, e)] 十 η
where Pii=1 Pr[sgn(xτw* — t)|x ∈ l(z, t0, e)] ≤ η. We will show that η is not large since a Z is
close to one of the vectors, it can not be close to the others thus a will be large for all i = j. Let us
bound η,
X Pr[sgn(XT w*- t)|x ∈ l(z,t0,e)] ≤ X (φc (t —：：；；：『
≤ χ (φc (t -t* COSgi)
一⅛ι I I |Singi)I
i=1
≤ X (φC (t - t0 COS(Oi)
≤ 乎 k | sin(Oi)|
i=1
,__.	1	γ2
≤ X √2π7e-ɪ + O(e)kt'
i=1 V2πγi
where Yi = t-)：£(『.The above follows since Yi ≥ 0 by assumption on t0. Under the assumption,
let β = maxi=ι cos(αi) we have
Yi ≥
t (1 — cos(lι)
Ω(t)
under our setting. Thus we have,
^XPr[sgn(xτw* — t))|x ∈ l(z,t0, e)] ≤ de-Q(t2) + O(e)dt = de-Q(t2)
i=1
for small enough e.
□
20
Under review as a conference paper at ICLR 2019
Proof of Lemma 6. Let us assume that < c/t0 for sufficiently small constant c, then we have that
0.6	= Pr [sgn(f (x)) and x ∈ l(z,t2, E)|x ∈ l(z, t2, E)] ≥ Pr [xTw1* ≥ t and x ∈ l(z, t2, E)|x ∈ l(z, t2, E)] ≥ φc (t -t*cOS(ai) ) - 0.1 1	| sin(α1)1 J
=⇒	0.7	≥ φc (t - t* cos(aι)) ≥ I I sin(αι)∣ )
=⇒ (Φc)-1(0.7)	Vt — t* cos(αι) ≤ I sin(αι)∣
=⇒	t2	≤ t-(φc)T(07)sin(α1) + o(i)≤ t	+ o(i) cos(α1 )	cos(α)
Similarly for t1. Now we need to argue that t1, t2 ≥ 0. Observe that
P r[sgn(f (x)) and x ∈ l(z, 0, )|x ∈ l(z, 0, )]
≤ ^XPr[xTw* ≥ t and X ∈ l(z, 0, e)|x ∈ l(z, 0, e)]
=X φc(t - e co" ) + O(e2)d ≤ de-Q(t2) < 0.4
J ∖ | sin(α1 )| )	一
Thus for sufficiently large t = Ω(√log d), this will be less than 0.4. Hence there will be some
t1 , t2 ≥ 0 with probability evaluating to 0.4 since the probability is an almost increasing function of
t UP to small noise in the given range (see proof of Lemma 5).	□
Proof of Lemma 7. Let V be the plane spanned by w1* and z and let v1 = w1* and v2 be the basis
of this space. Thus, we can write z = cos(α)v1 + sin(α)v2.
Let us apply a Gaussian perturbation ρ along the tangential hyperplane normal to z . Say it has
distribution EN (0, 1) along any direction tangential to the vector z. Let E1 be the component of
ρ on to V and let E2 be the component perpendicular to it. We can write the perturbation as ρ =
E1(sin(α)v1 - cos(α)v2) + E2v3 where v3 is orthogonal to both v1 and v2.
So the new angle α0 of z after the perturbation is given by
0	v1T (z + ρ)
cos(α )=-----------—
l∣z + p||
cos(α) + E1 sin(α)
√1+TPF
Note that with constant probability 气 ≥ E as P is a Gaussian variable with standard deviation e. And
with high probability ∣∣ρ∣∣ < O(e√d - 1). We will set E = Θ(sin(α)∕d) = Θ(α∕d). Thus with
constant probability:
cos(α0) ≥
cos(α) + E sin(α)
pι + O(e2d)^"
≥ (cos(α) + E sin(α))(1 - O(E2 d))
≥ cos(α) + Ω(esin(α)) — O(E2d)
≥ cos(α) + Ω(esin(α)).
Thus change in cos(α) is given by ∆ cos(α) ≥ Ω(e sin(α)). Now change in the angle α satisfies by
the Mean Value Theorem:
∆cos(α) = ∆ɑ ʃ cos(x)
dx
=⇒ -∆α = ∆ cos(α)
x∈[α,α0]
1 一
Sin(X)L∈[α,α0]
21
Under review as a conference paper at ICLR 2019
≥ ^SH-m/d).
□
B.6	Learning Union of Halfspaces far from the Origin
Theorem 12. Given non-noisy labels from a union of halfspaces that are at a distance Ω(√log^d)
and are each a constant angle apart, there is an algorithm to recover the underlying weights to
closeness in polynomial time.
Proof. Observe that W Xi is equivalent to P (Xi, ∙,Xd) = 1 - Q(1 - Xi). Thus
f(X) = V Sgn(XT Wi-t) =1 - Y(I-Sgn(XT wi — t)).
Since P and Sgn here satisfies our assumptions 1, 2, for t = Ω(√log d) (see Lemma 11) We can
apply Theorem 11 to recover the vectors wii approximately. Subsequently, refining to arbitrarily
close using Theorem 5 is possible due to the monotonicity. Thus We can recover the vectors to
arbitrary closeness in polynomial time.	□
B.7	Sigmoid Activations
Observe that for sigmoid activation, Assumption 2 is satisfied for ρ(t,σ) = e-t+σ2/2. Thus to
satisfy Assumption 3, we need t = Ω(η log d).
Note that for such value of t, the probability of the threshold being crossed is small. To avoid this
we further assume that f is non-negative and we have access to an oracle that biases the samples
towards larger values off; that after X is drawn from the Gaussian distribution, it retains the sample
(x, f (x)) with probability proportional to f (x) - so Pr[x] in the new distribution. This enables us
to compute correlations even if ExN(O ɪ[f (x)] is small. In particular by computing E[h(x)] from
this distribution, we are obtaining E[f (X)h(X)]/E[f (X)] in the original distribution. Thus we can
compute correlations that are scaled.
We get our approximate theorem:
Theorem 13. For t = Ω(log d), columns of (TWi)Tcan be recovered within error 1/poly(d)
using the algorithm in polynomial time.
B.8	POLYNOMIALS P WITH HIGHER DEGREE IN ONE VARIABLE
In the main section we assumed that the polynomial has degree at most 1 in each variable. Let us
give a high level overview of how to extend this to the case where each variable is allowed a large
degree. P now has the following structure,
d
P(X1,...,Xd) = X cr Y Xiri
r∈Zd+	i=1
If P has a higher degree in Xi then Assumption 2 changes to a more complex (stronger) condition.
Let qi (x) =	r∈Zd+ ∣∀7-=i %=o CrXri, that is qi is obtained by setting all Xj for j = i to 0.
Assumption 4. Eg〜N(0,σ2)[∣ut(g)∣r] ≤ ρ(t,σ) forall r ∈ Z+8. E⅛i(ut(g))hk(g))] = tθ⑴ρ(t, 1)
for k = 2, 4. Lastly, for all d ≥ i > 1, P r∈Zd |cr| ≤ dO(i).
||r||0=i
The last assumption holds for the case when the degree is a constant and each coefficient is upper
bounded by a constant. It can hold for decaying coefficients.
Let us collect the univariate terms Puni(X) = Pid=1 qi(Xi). Corresponding to the same we get
funi . This will correspond to the flin we had before. Note that the difference now is that instead of
8For example, his would hold for any u bounded in [-1, 1] such as sigmoid or sign.
22
Under review as a conference paper at ICLR 2019
being the same activation for each weight vector, now we have different ones qi for each. Using H4
correlation as before, now we get that:
dd
E[funi(x)H4(zτx)] = Xξi^out4(zτWi )4 and E[funi(x)H2(zτx)] = X\Ut2(ZTw↑')2
i=1	i=1
where q\i ◦ut are hermite coefficients for qi ◦ ut. Now the assumption guarantees that these are
positive which is what we had in the degree 1 case.
Second we need to show that even with higher degree, E[|f (x) - funi(x)|] is small. Observe that
Lemma 17. For r such that ||r||0 > 1, under Assumption 4 we have,
d
E Y(Ut((w；)Tx))ri ≤ P(t,1)O(ρ(t,∣∣W*∣∣)严 10T.
i=1
The proof essentially uses the same idea, except that now the dependence is not on ||r||1 but only
the number of non-zero entries (number of different weight vectors). With this bound, we can now
bound the deviation in expectation.
Lemma 18. Let ∆(x) = f(x) - funi (x). Under Assumptions 4, ift is such that ρ(t, ||W； ||) ≤ d-C
for large enough constant C > 0, we have, E[∣∆(x)∣] ≤ dO(1)ρ(t, 1)ρ(t, ||W；||).
Proof. We have,
E[∣∆(x)∣] = E
d
X	Cr Y(Ut((W；)Tx))ri
r∈Zd+	i=1
ri≤D,l∣r∣∣o>1
d
≤ X	|Cr |E	Y(Ut ((W；)Tx))ri
r∈Zd+	i=1
ri≤D,l∣r∣∣o>1
X	∣Cr∣ρ(t,i)(ρ(t,∣∣W*∣∣))llrll0T
r∈Zd+
ri≤D,l∣r∣∣o>1
≤ CXd	diDiρ(t, 1) (ρ(t, ||W；||))i-1
d
≤ dC X ρ(t,1)(dCρ(t, ∣∣W*∣∣))iτ
i=1
≤ d2C+1ρ(t, 1)ρ(t, ||W；||)
using Assumption 4
since ρ(t, ||W；||) ≤ d-C.
□
Thus as before, if we choose t appropriately, we get the required results. Similar ideas can be used
to extend to non-constant degree under stronger conditions on the coefficients.
C S ample Complexity
Proof of Lemma 3. C1(f, z, s) = E[f(x)δ(zTx - s)] = x f (x)δ(zT x - s)φ(x)dx Let x0 be the
component of x along Z and y be the component along z⊥. So x = χ0Z + yz⊥. Interpreting x as
a function of x0 and y:
C1(f,Z
s)= y x0
f (x)δ(x0 -
s)φ(x0)φ(y)dx0dy
23
Under review as a conference paper at ICLR 2019
=	[f (x)]x0=sφ(y)dy
y
= φ(s)E[f (x)|x0 = s]
= φ(zTx = s)E[f(x)|x0 = s]
where the second equality follows from x δ(x - a)f (x) = [f (x)]x=a.
□
Proof of Claim 4. Let x0 be the component of x along z and y be the component of x in the space
orthogonal to z. Let Z denote a unit vector along z. We have X = χ0z + y and ∂χ = Z. So,
correlation can be computed as follows:
E[P [X].δ0 (zT X - s)] = φ(y)	δ0 (x0 - s)P [X]φ(x0)dx0dy
Since JX δ0(x 一 a)f (x)dx = [ f ](x = a) this implies:
E[P [X]δ0(zT X - s)]
([西 P [x]φ(X)
dy
x0=s
L φ(xo) X 羡.念+P[χ]φ0(χ0))
Xi y
i
∂P	∂Xi
φ(x0) ∂Xi .西
φ(y)dy
x0=s
φ(y)dy + φ0(s)	P [x]φ(y)dy
y
Note that ∂Xi =含U(XTwi - t) = UO(XTwi 一 t)zTw*. If u is the sign function then u0(x)
δ(x). So focusing on one summand in the sum we get
“ φ(x0)黑.学 1	φ(y)dy
y	∂Xi ∂x0 x0 =s
∂P
I [φ(x0)u (XTwi 一t)(zTwi)-^]χo=sφ(y)dy
y	∂ Xi
/(ZTw*)[Φ(xo)δ(Zτw*xo + (w*)Ty 一 t)∂X]xo=sφ(y)dy
(Zτw*) Z Φ(s)δ(s(w*)TZ + (w*)Ty 一 t)∂Pφ(y)dy
Again let y = y0(wi*)0 + z where z is perpendicular to wi* and Z. And (wi*)0 is perpendicular
component of w* to z. Interpreting X = tZ + yo(w*)0 + Z as a function of yo, Z We get:
zTw"L φ(s)δ(S(M)TZ +((W*)T(M)O)y0 一 t)φ(y0)φ(z)羡dy0dz
Note that by substituting V = ax we get R∞=-∞ f (x)δ(ax — b)dx = f∞-a∞∕a f (x)δ(ax — b)dx
sgn(a)a f (a)=高f (a). So this becomes:
ZT w*	/	∂P
I(W*)T(w*)0∣ / φ(s)[φ(y0)∂Xi]yo = (wt-=ST⅛0CKNzdN
24
Under review as a conference paper at ICLR 2019
ZT w*	/	∂P
“ *∖T∕ *∖0∣ [ [Φ(yo)Φ(xo)Φ(z⅛v^]	t-szTw* dz
I(W勃T(W*)0| Jz	dXi xo = s,yo= (w*)T(WiY
ZTw*	J- t- szτwi、口… 八 ∕∖"~∖dPι
I(w*)τ(w*)0Iφ V0 = (Wi)T(Wiy) φ(x0 = 11*∂Xi]xo=t,yo=(W*T⅛
ZT wi	/ “、∂P、	1
*~^* ∖T∕~~*777	[φ(X) αv ]	t-szTW* dz
I(Wi ) (Wi ) 1 Jz	dXi x0 = t,y0= (Wi)T(Wiy
ZT Wi	[	φ(x) ^Ldx
I(Wi)T(WiyI LzTx=s,χTw*=t φ( XdXi
Zt wi	-t	∂ ∂ dp. ∂P l T	rr φ
I(Wi)T(WiyI φ(z X = SsX Wi=t)E[∂Xi|x Z = SsX Wi= t].
Let αi be the angle between Z and Wii . Then this is
∂P
=I cot(αi)IΦ(ztx = t, (w* )tx = t)E[—-]IxτZ = s, xτw* = t]
∂Xi
Thus, overall correlation
∂P
72 I cot(α*) IΦ(ztX = S, X W* = t)E[-- IXTZ = s, xtw*
∂ Xi
i∈S	i
= t]
+ φ0(S)E[P[X]IXTZ = S]
□
ProofofClaim 5. Note that for small a, I cot αI = O(1∕ɑ) ≤ θ(l/o). Since P(X) ≤ IIXIIc1,
we have f(X) ≤ dc1 (as with sgn function each sgn((Wii)TX) ≤ 1) and all Qi[X], Ri[X] are at most
2dc1.
By Cauchy’s mean value theorem C2 (f, Z, t) = 2[E[f (X)δ0(ZT X = S)]]s∈t±. Note that
cot(α*)φ(ZTx = t, (Wi)Tx = t) = cot(α*)φ(t tan(α*∕2)) which is a decreasing function of
αi in the range [0, π] So if all αi are upper bounded by 2 then by above corollary,
C2 (f, Z, S) ≤ 2n cot(2)φ(ZTX = t, (W1i)TX = t)(2dc1 ) + (2dc1 )
= 2n cot(2)φ(t tan(2/2))(2dc1) + (2dc1 )
≤ j。⑴.
2
□
Observe that the above proof does not really depend on P and holds for for any polynomial of
u((Wii)TX) as long as the polynomial is bounded and the Wii are far off from Z.
Proof Of Lemma 8. If Z = Wii , then
C2(f, Z,t) = E[f (X)(δ(ZT X - t - ) - δ(ZT X -t + ))]
= E[u((Wii)TX)Qi [X](δ(ZTX - t - ) - δ(ZT X - t + ))]
+ E[Ri [X](δ(ZTX - t - ) - δ(ZT X - t + ))]
Since u((Wii)TX) = 0 for ZTX = t - and 1 for ZTX = t + , and using the Cauchy mean value
theorem for the second term this is
= E[Qi[X]δ(ZTX -t - )] + 2[E[Ri[X]δ0(ZT X - S1)]s1∈t±
= E[Qi[X]δ(ZTX - t)] + E [Ri [X](δ(ZTX - t - ) - δ(ZT X - t))]
= φ(t)E[Qi [X]IZTX = t + ] + [C2(Qi,Z,S2)]s2∈[t,t+] + 2[C2(Ri,Z, S)]s∈t±
25
Under review as a conference paper at ICLR 2019
=φ(t)E[Qi[x]∣zτ x = t] + EdO(I)
The last step follows from Claim 5 applied on Qi and R as all the directions of Wj are well separated
from Z = Wj and Wj is absent from both Qi and Ri. Also the corresponding Qi and Ri are
bounded.	□
C.1 ReLU activation
If u is the RELU activation, the high level idea is to use correlation with the second derivative δ00 of
the Dirac delta function instead of δ0. More precisely we will compute C3(f, z, s) = E[f.(δ0(zτ x-
s - E) - δ0(zτx - s + E)]. Although we show the analysis only for the RELU activation, the same
idea works for any activation that has non-zero derivative at 0.
Note that now u0 = sgn and u00 = δ.
For ReLU activation, Lemma 8 gets replaced by the following Lemma. The rest of the argument is
as for the sgn activation. We will need to assume that P has constant degree and sum of absolute
value of all coefficients is poly(d)
Lemma 19. Assuming polynomial P has constant degree, and sum of the magnitude of
all coefficients is at most poly(d), if Z = Wj then C3(f, z,t)	= E[φ(t) ∂χP +
Φ(t) Pj=i cos(aj)sgn(xτWj — t) ∂XP + φ0(t)PIxTWj = t] + EdO(I). Otherwise ifall angles a
between Z and Wi are at least E2 it is at most EdO(1)/E2.
We will prove the above lemma in the rest of this section. First we will show that Z is far from any
of the Wij ’s then E[P.δ00(Zτx — s)] is bounded.
Lemma 20. Ifthe sum ofthe absolute value of the coefficients ofP is bounded by poly(d), its degree
is at most constant, αi > E2 then E[P.δ00(Zτ x — s)] is dO(1) /E2.
Proof. Let x0 be the component of x along Z and y be the component of x in the space orthogonal
to z as before. We have x = xoZ + y and ∂∂x = Z. We will look at monomials Ml in P = Pl Ml.
As before since JX δ00(x — a)f (x)dx = df We get
x	x x=a
Eif(x).δ00(Zτx — s)] =	f (x)φ(x)δ00(Zτ x — s)dy
y
∂2
福(P [x]φ(x))
dy
x0=s
Xl y
∂2
∂χ2 (Mlix]φ(XO))
φ(y)dy
x0=s
y
Now consider a monomial M = X1i1 ..Xkik.
Take the symbolic second derivative 晟 of M[x]φ(χo) w.r.t x°. This will produce a polynomial
involving Xi's, ∂χi, ∂2χχ2i, φ, φ0, φ00. Let US examine each of these terms.
∂∂
丁Xi[x]=厂U(XTWi - t)
∂ x0	∂ x0
=sgn(xτ Wj — t)(Wj )τ 总 x
= sgn(xT Wij —t)((Wij)TZ)
=cos(αi)sgn(xτWij — t)
Thus 悬Xi [x] is a bounded function of x. We have
∂2	∂
福 Xi(X)=寂 sgn(x Wi
— t)((Wij)τZ) =cos2(αi)δ(xτWij — t).
26
Under review as a conference paper at ICLR 2019
Again as before
[[δ(
Jy
XT wi
-t)g(χ)φ(χ0)φ(y)]χ0=sdy
(1/| sin(αi)∣)E[g(x)∣X0 = s, XTw* = t]φ(xo = s, XTw* = t)
Note that if the degree is bounded, since ∣ sin(αj∣ is at least e? expected value of each monomial
obtained is bounded. So the total correlation is poly(d"%	□
ProofofLemma 20. As in the case of sgn activation, if N = w*,
E [P [x]δ0(xτ w* — s)]
P [x]φ(x)δ0 (ZT x — s)dx
∂φ(x)
∂X0
dy
X0 = s
/ φ(y)
Jy
sgn(x0 — t)φ(s)
∂P
(∂X ) [x] + ∑ Sgn(XTWj - t) COS(Qj)φ(S)
ij=i
(∂pj)
[x] + Pφ0(x0)
dy
X0 = s
/ Φ(y) sgn(s — t)φ(s)
y
(JdP) [x] + X Sgn(XTw* - t) COS(Qj)φ(S) ^dX) [x] + pφ0(X0)
dy
X0 = S
For S = t + e this is
φ(y) φ(s)
y
∂P
∂χ ) [x] + Σ COS(Qj )sgn(xT w* — t)φ(s)
ij	j=i
[x] + Pφ0(s)
dy
s = t+e
(/ φ(y) (φ(t)(彩)[x] + X COS(Qj)sgn(XTw* — t)φ(t)
[x] + Pφ0(t) dy + cd° ⑴
=E
φ(t)
∂P
(∂X ) [x] + φ(t) Ecos(Qj)sgn(XTwi - t
d i)	j=i
[x] + φ0 (t)P
xT wi* = t
+ edo ⑴
If z is away from every w* by at least ⑦ then again E[PC3(f, z,t)] = E[Pδ0(XTw* — s)](s ∈
[t — e, t + e]) = ed°(1)/62.	□
D Structural Restrictions Helps Learning
D.1 Proof of theorem 7
To construct this correlation graph, we will run the following Algorithm 3 Denote Ti := {j : Wij
1}. Let us compute E[f (X)XiX j:
E[f(x)xiXj] = E CSE	ɪɪu(XTWp) XiXj
S⊆[d]	[∖p∈S	)	_
=X CSe-pt|S|E heρPp∈s XTwp-XiXji
S⊆[d]
27
Under review as a conference paper at ICLR 2019
Algorithm 3 ConstructCorrelationGraph
1:	Let G be an undirected graph on n vertices each corresponding to the Xi's.
2:	for every pair i, j do
3:	Compute αij = E[f (x)xi xj].
4:	if αij ≥ ρ then
5:	Add edge (i, j) to the graph.
X CSe-ρtlSlE [eρPq∈∪p∈sTp XqXixji
S⊆[d]
X CSe-pt|S|l[i,j ∈∪p∈sTyE [xieρxi]E[xjeρx[	Y E [eρxq]
S⊆[d]	q∈∪p∈S Tp∖{i,j}
X CS e-ρtlSl l[i,j ∈ ∪p∈s TP]ρ2eρ2 E [xj eρxj]	Y	eρ2/2
S⊆[d]	q∈∪p∈STp∖{i,j}
X CSe-Ptlsll[i,j ∈∪p∈sTp]ρ2eð∪p∈STpl
S⊆[d]
By assumption, for all p, Tp are disjoint. Now, if i, j ∈ Tr for some r, we have
Eff (X)XiXj ] = P2	X	CS e -PSL(t-ρd)
S⊆[d]v∈S
Similarly, if i ∈ Tr1 andj ∈ Tr2 with r1 6= r2, we have
Ef/ 、	1	2	-PLSKt-Pd)
Ef(X)XiXj] = ρ 工	CS e	2
S⊆ [d]:ri ,r2∈S
It is easy to see that these correspond to coefficients of Xr and Xr1 Xr2 (respectively) in the follow-
ing polynomial:
Q(X1,.∙∙, Xn) = P2p(μ(xi + 1),..∙, μ(χn + 1))
-P(t-Pd)
for μ = e 2	. For completeness We show that this is true. We have,
Q(Xι,…，Xn)= P2 X CS Y μ(Xj + 1)
S⊆[d]	j∈S
=P2 X CSμlSlY(Xj + 1)
S⊆[d]	j∈S
The coefficient of Xr in the above form is clearly ρ2 PS⊆[djr∈S CSμlSl (corresponds to picking the
1 in each product term). Similarly coefficient of Xn Xr2 is ρ2 PS⊆[dj∏	∈s CS μlSl.
Now as in the assumptions, if we have a gap between these coefficients, then we can separate the
large values from the small ones and form the graph of cliques. Each clique will correspond to the
corresponding weight vector.
D.2 Proof of Theorem 8
.一 一， 、	--- -	__T_ ..* -	---- 一 ~	「 r	一 一 一	.......
Consider f (x) = P CS “记$ ex Wi where w* = Pj∈Si ej for Si ⊆ [n] such that for all i = j,
LLzLL2	T
Si ∩ Sj = n and CS ≥ 0. Let us compute g(z) = e 厂E f (x)ez X where Z = E Ziei for
some αi .
E f(x)ezTx
28
Under review as a conference paper at ICLR 2019
X CSE ](Y ePp∈si x] ePq∈[n] ZqXq
ECS E	ɪɪ	e(1+zp)Xp
∖p∈∪i∈s Si
ɪɪ	ezq Xq
q∈[n]∖∪i∈S Si
X cs I π E 卜(I+zp)χp]] I I π	E[ezqχq]
∖P∈ui∈s Si	) ∖q∈[n]∖∪i∈s Si
∑CS I ∏ e
∖p∈∪i∈s Si
(1 + zp)2
2
Π e学
q∈[n]∖∪i∈S Si
ΣUzU2
CS e 2
π
P∈∪i∈S Si
g(z) = ECS ∏ e 2 +如
P∈∪i∈S Si
e 2 +zp
Consider the following optimization problem:
maχ [g(z) — λ∣∣z∣∣ι -7∣∣z∣l2]
Z '-------------V------------/
h(Z)
for λ, γ > 0 tobe fixed later.
We can assume that Zi ≥ 0 for all i at a local maxima else we can move in the direction of ei and
this will not decrease g(z) since CS ≥ 0 for all S and will decrease ∣∣z∣∣ι and ∣∣z∣∣2 making h(z)
larger. From now on, we assume this for any Z at local maxima.
We will show that the local maximas of the above problem will have Z such that most of the mass is
equally divided among j ∈ Si for some i and close to 0 everywhere else.
Lemma 21. There exists at most one i such that there exists j ∈ Si with ∣Zj ∣ ≥ β for Y satisfying
β	β	l∪i∈SSiI
4γ < mi∩i=j [e" ∑2S=i∈S,jwSVi∈Sj∈S cSe	J-
Proof. Let us prove by contradiction. Assume that there is a local maxima such that there are at
least 2 indices say 1,2 such that ∃j ∈ Skk ∈ S2, ∣Zj∣, ∣Zk∣ ≥ β. Now we will show that there
exists a perturbation such that g(z) can be improved. Now consider the following perturbation,
z + Seej — Seek for S ∈ {±1}. Observe that ∣∣z∣∣1 remains unchanged for e < β also ∣∣z∣∣2 changes
by 2s2e2 + 2(zj - Zk)se. We have
Es [h(z + Seej — Seek) - h(z)]
= X	CS	∏	e1 +zp	(ES	[ese] - 1)+ X	CS	∏	e 1+zp	(Es	[e-se] - 1)
Sd∈S,2∈S	p∈Ui∈s Si	SR∈S,2∈S	p∈Ui∈s Si
-YES [2e2 + 2(Zj - Zk)Se]
≥ F ∣-4γ + X CS ∏ e 1+zp + X CS ∏ e 1+zJ
∖	Sd∈S,2∈S p∈Ui∈s Si	Sd∈S,2∈S p∈Ui∈s Si	I
The inequality follows since E [ese]=也。-≥ 1 + ^2. Observe that
ΣT-Γ	1 +z	、	l∪i∈SSiI Z -、	l∪i∈SSiI β
CS	]]	e2 +zp	≥	工 CSe 2	ezj	≥	工 CSe 2 eβ.
Sd∈S,2∈S p∈Ui∈s Si	S=1∈S,2?S	S=1∈S,2∈S
For chosen value of γ, there will always be an improvement, hence can not be a local maxima. □
29
Under review as a conference paper at ICLR 2019
Lemma 22. At the local maxima, for all i ∈ [n], z is such that for all j, k ∈ Si, zj = zk at local
maxima.
Proof. We prove by contradiction. Suppose there exists j, k such that zj < zk . Consider the
following perturbation: z + (zk - zj)(ej - ek) for 1 ≤ > 0. Observe that g(z) depends on
only Pr∈S zr and since that remains constant by this update g(z) does not change. Also note that
||z ||1 does not change. However ||z ||22 decreases by 2(1 - )(zk - zj )2 implying that overall h(z)
increases. Thus there is a direction of improvement and thus it can not be a local maxima. □
Lemma 23. At the local maxima, ||z ||1 ≥ α for λ < PS CS ⅛4 s^e⅛Sil - γ(2α + 1).
Proof. We prove by contradiction. Suppose ∣∣z∣∣ι < α, consider the following perturbation, Z + EI.
Then we have
h(z + EI) — h(z) = ^X CSe	2	+Pp∈∪i∈sSi Zp (ee∣∪i∈SSiI — 1) — nλe — nγc⑵∣z∣∣ι + E)
~"Λ	l∪ i ∈ SSi |
> TCSe_2-1 ∪i∈S Si|e - nλE - nγe(2α + 1)
□
For given λ there is a direction of improvement giving a contradiction that this is the local maxima.
Combining the above, we have that we can choose λ, γ = poly(n, 1/E, s) where s is a paramater
that depends on structure of f such that at any local maxima there exists i such that for all j ∈ Si ,
zj ≥ 1 and for all k 6∈ ∪j∈Si , zk ≤ E.
D.3 Proof of Theorem 9
Let function f = Ps⊆[h] CS Qi∈S u((w*)Tx) for orthonormal wi. WLOG, assume Wi = e%.
Let u(x) = P∞=o ^2ih2i(x) where h are hermite polynomials and u0(x) = u(x) - Uo =
P∞=ι ^2ih2i(x). This implies E[u0(x)] = 0. Observe that,
|S|
YU(Xi) = Y(u0(xi) + Uo) = XU0Sl-k	X Y u0(xi).
i∈S	i∈S	k=0	S0⊆S"S0∣=ki∈S0
Let us consider correlation with h4 (zTx). This above can be further simplified by observing that
when we correlate with h4, Qi∈S0 U0(xi)h4(zTx) = 0 for |S0| ≥ 2. Observe that h4(zTx) =
Pdl dn ∈[4JP di ≤4 c(dι,... ,dn) Q hdi (Xi) for some coefficients C which are functions of z. Thus
when we correlate Qi∈S0 U0(xi)h4(zTx) for |S0| ≥ 3 then we can only get a non-zero term if we
have at least h2k(Xi) with k ≥ 1 for all i ∈ S0. This is not possible for |S0| ≥ 3, hence, these terms
are 0. Thus,
2
E Y u(xi)h4(zτx) = X U0Sl-k	X E Y u (xi)h4(zτx)
.i∈S	」	k=0	S0⊆S"S0∣ = k Li∈S0
Lets compute these correlations.
E u0(Xi)h4(zτx)
E ^U2ph2p(xi)h4(ziXi + zTiX-i)
p>0
4 X u2pE
p>0
4 XU2pE [h2p(xi) (h4(ziXi√2) + 6h2(ziXi√2)(2∣∣z-i∣∣2 - 1) + 3(2∣∣z-i∣∣2 - 1)2)i
p>0
30
Under review as a conference paper at ICLR 2019
4 (u4α(4, 4, Zi√2) + U2α(4, 2, Zi√2) + 6U2α(2, 2, Zi√2)(2∣∣z-i∣∣2 - 1))
1 (4U4zi4 + 12u2z2(2z2 - 1) + 6u2(2z2)(2||z-i||2 - 1))
U4z4 + 3U2z2⑵|z||2 - 1)
E u0(xi)u0(xj)h4(zTx)
E
E u2pu2q h2p(Xi)h2q (Xj )h4(Zi Xi + Zj Xj + ZTij x—ij )
p,q>0
4
4 X u2pu2q E h2p(xi)h2q (Xj) X (4)
p,q>0	k=0
h4—k((ZiXi + Zj Xj ) λ∕2) hk (z-ij x-ij ʌ/2)
4 X U2pU2qE [h2p(xi)h2q(Xj) (h4((ziXi + ZjXj)√2) + 6h2((ziXi + ZjXj)√2)(2∣∣Z-ij||2 - 1)
p,q>0
+3(2||Z-ij ||2 - 1)2)
1	44
16 EupUqE k E [h2p (Xi)h2q (Xj )h4-k (2Zi Xi)hk (2Zj Xj )]
p,q	k=0
X--------------------------V---------------------------}
①2
+ 4⑵|z-ij ||2 - 1) X u2pu2q X O E[h2p(Xi)h2q (Xj )h2-k(2ZiXi)hk(2Zj Xj )]
p,q<0	k=0
X---------------------------------------------------------------------/
We will compute ɑɔ and (2):
144
①=16Σ J Σ u2pu2qE [h2p (Xi ) h4-k (2ZiXi)] E [h2q (Xj ) hk (2ZjXj )]
k=0 p,q>0
144
=16 X ( k ) X u2pu2qα(4 - k, 2P, 2Zi)a(k, 2q, 2Zj)
k=0 p,q>0
3
=du2α(2, 2, 2Zi)α(2, 2, 2Zj)
8
=6u2Z2Z2
Similarly,
3	22
②=4∙(2∣∣z-ij『-1) E u2pu2q	k2 E [h2p (Xi)h2q (Xj )h2—k (2Zi Xi)hk (2Zj Xj )]
p,q>0	k=0
=4(2∣∣Z-ij||2 - 1) X (k)u2u2α(2 - k, 2, 2Zi)α(k, 2, 2Zj) = 0.
k=0
Combining, we get
E [u0(Xi)u0(Xj)h4(zTx)] = 6u2z2Zj.
Further, taking correlation with f, we get:
E [f(X)h4(zTx)]
2
=X CS Xu0S1—k X E Y u0(Xi)h4(zTx)
S⊆[n] k=0	S0⊆S:|S0|=k Li∈S0	_
31
Under review as a conference paper at ICLR 2019
=X CSu0S1-2 I uo X(U4z4 + 3u2z2(2∣∣z∣∣2 - 1)) + 6u2 X z2zk I + constant
S⊆[n]	i∈S	j 6=k∈S
nn
=	αi zi4 + (2||z||2 - 1)	βi zi2 +	γij zj2zk2 + constant
i=1	i=1	1≤i6=j ≤n
nn
=	αi zi4 +	βi zi2 +	γij zj2zk2 + constant
where	ai	=	u4	Ps0⊆[n]∣i∈S0	cS0u0	1	， β =	3u2	Ps0⊆[n]∣i∈S0 cS0u0	1 and	Yrij	=
2	|S0|-2
6u2 工S0⊆[n]∣i,j∈S0 cS0u0	.
If γij < αi + αj for all i, j then the local maximas of the above are exactly ei . To show that this
holds, we prove by contradiction. Suppose there is a maxima where zi , zj 6= 0. Then consider the
following second order change zi2 → zi2 +s and zj2 → zj2 -s where ≤ min zi2, zj2 and s is 1 with
probability 0.5 and -1 otherwise. Observe that the following change does not violate the constraint
and in expectation affects the objective as follows:
∆ = Es	αi	(2szi	+ ) + αj	(-2szi	+ ) +	βi	s	- βj	s	+ γij	(szj - szi	-	)
= (αi + αj - γij )2 > 0
Thus there is a direction in which we can improve and hence it can not be a maxima.
E	Modular Learning by Divide and Conquer
Finally we point out how such high threshold layers could potentially facilitate the learning of deep
functions f at any depth, not just at the lowest layer. Note that essentially for Lemma 2 to hold,
outputs X1 , ., Xd needn’t be present after first layer but they could be at any layer. If there is
any cut in the network that outputs X1, ...Xd, and if the upper layer functions can be modelled
by a polynomial P , then again assuming the inputs Xi have some degree of independence one
can get something similar to Lemma 2 that bounds the non-linear part of P . The main property
we need is E[Πi∈sXi]∕E[Xj] < μlSl-1 for a small enough μ = 1∕poly(d) which is essentially
replaces Lemma 1. Thus high threshold layers can essentially reduce the complexity of learning deep
networks by making them roughly similar to a network with lower depth. Thus such a cut essentially
divides the network into two simpler parts that can be learned separately making it amenable to a
divide and conquer approach. If there a robust algorithm to learn the lower part of the network that
output Xi , then by training the function f on that algorithm would recover the lower part of the
network, having learned which one would be left with learning the remaining part P separately.
Remark 1. If there is a layer of high threshold nodes at an intermediate depth l, u is sign function,
if outputs Xi at depth l satisfy the following type of independence property: E [Πi∈S Xi ]∕E [Xj] <
μlSl-1 for a small enough μ = 1∕poly(d), if there is a robust algorithm to learn Xi from P CiXi
that can tolerate noise, then one can learn the nodes Xi , from a function P(X1, .., Xd)
32