Under review as a conference paper at ICLR 2019
TarMAC: Targeted Multi-Agent Communication
Anonymous authors
Paper under double-blind review
Ab stract
We explore a collaborative multi-agent reinforcement learning setting where a
team of agents attempts to solve cooperative tasks in partially-observable environ-
ments. In this scenario, learning an effective communication protocol is key. We
propose a communication architecture that allows for targeted communication,
where agents learn both what messages to send and who to send them to, solely
from downstream task-specific reward without any communication supervision.
Additionally, we introduce a multi-stage communication approach where the
agents co-ordinate via multiple rounds of communication before taking actions
in the environment. We evaluate our approach on a diverse set of cooperative
multi-agent tasks, of varying difficulties, with varying number of agents, in a vari-
ety of environments ranging from 2D grid layouts of shapes and simulated traffic
junctions to complex 3D indoor environments. We demonstrate the benefits of tar-
geted as well as multi-stage communication. Moreover, we show that the targeted
communication strategies learned by agents are both interpretable and intuitive.
1	Introduction
Effective communication is a key ability for collaborative multi-agents systems. Indeed, intelligent
agents (humans or artificial) in real-world scenarios can significantly benefit from exchanging infor-
mation that enables them to coordinate, strategize, and utilize their combined sensory experiences
to act in the physical world. The ability to communicate has wide-ranging applications for artificial
agents - from multi-player gameplay in simulated games (e.g. DoTA, Quake, StarCraft) or physical
worlds (e.g. robot soccer), to networks of self-driving cars communicating with each other to achieve
safe and swift transport, to teams of robots on search-and-rescue missions deployed in hostile and
fast-evolving environments.
A salient property of human communication is the ability to hold targeted interactions. Rather than
the ‘one-size-fits-all’ approach of broadcasting messages to all participating agents, as has been
previously explored (Sukhbaatar et al., 2016; Foerster et al., 2016), it can be useful to direct certain
messages to specific recipients. This enables a more flexible collaboration strategy in complex
environments. For example, within a team of search-and-rescue robots with a diverse set of roles
and goals, a message for a fire-fighter (“smoke is coming from the kitchen”) is largely meaningless
for a bomb-defuser.
In this work we develop a collaborative multi-agent deep reinforcement learning approach that sup-
ports targeted communication. Crucially, each individual agent actively selects which other agents to
send messages to. This targeted communication behavior is operationalized via a simple signature-
based soft attention mechanism: along with the message, the sender broadcasts akey which encodes
properties of agents the message is intended for, and is used by receivers to gauge the relevance of the
message. This communication mechanism is learned implicitly, without any attention supervision,
as a result of end-to-end training using a downstream task-specific team reward.
The inductive bias provided by soft attention in the communication architecture is sufficient to en-
able agents to 1) communicate agent-goal-specific messages (e.g. guide fire-fighter towards fire,
bomb-defuser towards bomb, etc.), 2) be adaptive to variable team sizes (e.g. the size of the local
neighborhood a self-driving car can communicate with changes as it moves), and 3) be interpretable
through predicted attention probabilities that allow for inspection of which agent is communicating
what message and to whom.
1
Under review as a conference paper at ICLR 2019
Our results however show that just using targeted communication is not enough. Complex real-
world tasks might require large populations of agents to go through multiple stages of collaborative
communication and reasoning, involving large amounts of information to be persistent in memory
and exchanged via high-bandwidth communication channels. To this end, our actor-critic framework
combines centralized training with decentralized execution (Lowe et al., 2017), thus enabling scaling
to a large number of agents. In this context, our inter-agent communication architecture supports
multiple stages of targeted interactions at every time-step, and the agents’ recurrent policies support
persistent relevant information in internal states.
While natural language, i.e. a finite set of discrete tokens with pre-specified human-conventionalized
meanings, may Seem like an intuitive protocol for inter-agent communication - one that enables
human-interpretability of interactions - forcing machines to communicate among themselves in
discrete tokens presents additional training challenges. Since our work focuses on machine-only
multi-agent teams, we allow agents to communicate via continuous vectors (rather than discrete
symbols), and via the learning process, agents have the flexibility to discover and optimize their
communication protocol as per task requirements.
We provide extensive empirical demonstration of the efficacy of our approach across a range of
tasks, environments, and team sizes. We begin by benchmarking multi-agent communication with
and without attention on a cooperative navigation task derived from the SHAPES environment (An-
dreas et al., 2016). We show that agents learn intuitive attention behavior across a spectrum of task
difficulties. Next, we evaluate the same targeted multi-agent communication architecture on the traf-
fic junction environment (Sukhbaatar et al., 2016), and show that agents are able to adaptively focus
on ‘active’ agents in the case of varying team sizes. Finally, we demonstrate effective multi-agent
communication in 3D environments on a cooperative first-person point-goal navigation task in the
rich House3D environment (Wu et al., 2018).
2	Related Work
Multi-agent systems fall at the intersection of game theory, distributed systems, and Artificial In-
telligence in general (Shoham & Leyton-Brown, 2008), and thus have a rich and diverse literature.
Our work builds on and is related to prior work in deep multi-agent reinforcement learning, the
centralized training and decentralized execution paradigm, and emergent communication protocols.
Multi-Agent Reinforcement Learning (MARL). Within MARL (see Busoniu et al. (2008) for a
survey), our work is related to recent efforts on using recurrent neural networks to approximate agent
policies (Hausknecht & Stone, 2015), algorithms stabilizing multi-agent training (Lowe et al., 2017;
Foerster et al., 2018), and tasks in novel application domains such as coordination and navigation in
3D simulated environments (Peng et al., 2017; OpenAI, 2018; Jaderberg et al., 2018).
Centralized Training & Decentralized Execution. Both Sukhbaatar et al. (2016) and Hoshen
(2017) adopt a fully centralized framework at both training and test time - a central controller pro-
cesses local observations from all agents and outputs a probability distribution over joint actions.
In this setting, any controller (e.g. a fully-connected network) can be viewed as implicitly encod-
ing communication. Sukhbaatar et al. (2016) present an efficient architecture to learn a centralized
controller invariant to agent permutations - by sharing weights and averaging as in Zaheer et al.
(2017). Meanwhile Hoshen (2017) proposes to replace averaging by an attentional mechanism to
allow targeted interactions between agents. While closely related to our communication architec-
ture, his work only considers fully supervised one-next-step prediction tasks, while we tackle the
full reinforcement learning problem with tasks requiring planning over long time horizons.
Moreover, a centralized controller quickly becomes intractable in real-world tasks with many agents
and high-dimensional observation spaces (e.g. navigation in House3D (Wu et al., 2018)). To ad-
dress these weaknesses, we adopt the framework of centralized learning but decentralized execution
(following Foerster et al. (2016); Lowe et al. (2017)) and further relax it by allowing agents to com-
municate. While agents can use extra information during training, at test time, they pick actions
solely based on local observations and communication messages received from other agents.
Finally, we note that fully decentralized execution at test time without communication is very re-
strictive. It means 1) each agent must act myopically based solely on its local observation and 2)
agents cannot coordinate their actions. In our setting, communication between agents offers a rea-
2
Under review as a conference paper at ICLR 2019
	Decentralized Execution	Targeted Communication	Multi-Stage Decisions	Reinforcement Learning
DIAL (Foerster et al., 2016)	Yes	No	No	Yes (Q-Learning)
CommNets (Sukhbaatar et al., 2016)	No	No	Yes	Yes (REINFORCE)
VAIN (Hoshen, 2017)	No	Yes	Yes	No (Supervised)
ATOC (Jiang & Lu, 2018)	Yes	No	No	Yes (Actor-Critic)
TarMAC (this paper)	Yes	Yes	Yes	Yes (Actor-Critic)
Table 1: Comparison with previous work on collaborative multi-agent communication with continuous vectors.
sonable trade-off between allowing agents to globally coordinate while retaining tractability (since
the communicated messages are much lower-dimensional than the observation space).
Emergent Communication Protocols. Our work is also related to recent work on learning commu-
nication protocols in a completely end-to-end manner with reinforcement learning - from perceptual
input (e.g. pixels) to communication symbols (discrete or continuous) to actions (e.g. navigating in
an environment). While (Foerster et al., 2016; Jorge et al., 2016; Das et al., 2017; Kottur et al., 2017;
Mordatch & Abbeel, 2017; Lazaridou et al., 2017) constrain agents to communicate with discrete
symbols with the explicit goal to study emergence of language, our work operates in the paradigm
of learning a continuous communication protocol in order to solve a downstream task (Sukhbaatar
et al., 2016; Hoshen, 2017; Jiang & Lu, 2018). While (Jiang & Lu, 2018) also operate in a decentral-
ized execution setting and use an attentional communication mechanism, their setup is significantly
different from ours as they use attention to decide when to communicate, not who to communicate
with (‘who’ depends on a hand-tuned neighborhood parameter in their work). Table 1 summarizes
the main axes of comparison between our work and previous efforts in this exciting space.
3	Technical Background
Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs). A Dec-
POMDP is a cooperative multi-agent extension of a partially observable Markov decision process
(Oliehoek (2012)). For N agents, it is defined by a set of states S describing possible configurations
of all agents, a global reward function R, a transition probability function T , and for each agent
i P 1,…,N a set of allowed actions Ai, a set of possible observations Ωi and an observation func-
tion Oi. Operationally, at each time step every agent picks an action ai based on its local observation
ωi following its own stochastic policy ∏θi (ai ∣ωi). The system randomly transitions to the next state
s1 given the current state and joint action Tps1|s, a1, ..., aNq. The agent team receives a global re-
ward r “ R(s, aι,...,aN) while each agent receives a local observation of the new state Oi(ωi∣s1).
Agents aim to maximize the total expected return J “ XT“° Ytrt where Y is a discount factor and
T is the episode time horizon.
Actor-Critic Algorithms. Policy gradient methods directly adjust the parameters θ of the policy in
order to maximize the objective J(θ) “ Es~pπ,。„n@Ps) [R(s, a)S by taking steps in the direction of
VJ(θ). We can write the gradient with respect to the policy parameters as
VθJ (θ) “ Es~p∏ ,a~∏θ(s) rvθ log ne(a|s)Qn(s,a)s ,
where Qπ(s, a) is called the action-value, it is the expected remaining discounted reward ifwe take
action a in state s and follow policy π thereafter. Actor-Critic algorithms learn an approximation
of the unknown true action-value function Q(s, a) by e.g. temporal-difference learning (Sutton &
Barto, 1998). This Q(s, a) is called the Critic while the policy πθ is called the Actor.
Multi-Agent Actor-Critic. Lowe et al. (2017) propose a multi-agent Actor-Critic algorithm adapted
to centralized learning and decentralized execution. Each agent learns its own individual policy
∏θi (ai∣ωi) conditioned on local observation ωi, using a centralized Critic which estimates the joint
action-value Q(s, aι,…,a，N).
4	TarMAC: Targeted Multi-Agent Communication
We now describe our multi-agent communication architecture in detail. Recall that we have N
agents with policies tπ1, ..., πNu, respectively parameterized by tθ1, ..., θNu, jointly performing a
3
Under review as a conference paper at ICLR 2019
-l
aggregatec,
observation
aggregated message
GRU policies
Agent inputs with parameter
Targeted
Communicatior
t + 1
Figure 1: Overview of our multi-agent architecture with targeted communication. Left: At every timestep, each
agent policy gets a local observation ωit and aggregated message cit as input, and predicts an environment action
ait and a targeted communication message mit . Right: Targeted communication between agents is implemented
as a signature-based soft attention mechanism. Each agent broadcasts a message mit consisting of a signature
kit , which can be used to encode agent-specific information and a value vit , which contains the actual message.
At the next timestep, each receiving agent gets as input a convex combination of message values, where the
attention weights are obtained by a dot product between sender's signature kt and a query vector qj'1 predicted
from the receiver’s hidden state.
observation
observation
aggregated message eɪ
aggregated message
observation
aggregated message C炉
[	SoftmaX	J
川C
I SCale	]
Targeted ∣
Communication I
J
cooperative task. At every timestep t, the ith agent for all i P t1, ..., Nu sees a local observation ωit,
and must select a discrete environment action ait „ πθi and a continuous communication message
mit , received by other agents at the next timestep, in order to maximize global reward rt „ R. Since
no agent has access to the underlying state of the environment st, there is incentive in communicating
with each other and being mutually helpful to do better as a team.
Policies and Decentralized Execution. Each agent is essentially modeled as a Dec-POMDP aug-
mented with communication. Each agent’s policy πθi is implemented as a 1-layer Gated Recurrent
Unit (Cho et al., 2014). At every timestep, the local observation ωit and a vector cit aggregating mes-
sages sent by all agents at the previous timestep (described in more detail below) are used to update
the hidden state hit of the GRU, which encodes the entire message-action-observation history up to
time t. From this internal state representation, the agent’s policy πθi pait | hitq predicts a categorical
distribution over the space of actions, and another output head produces an outgoing message vector
mit . Note that for all our experiments, agents are symmetric and policies are instantiated from the
same set of shared parameters; i.e. θ1 “ ... “ θN . This considerably speeds up learning.
Centralized Critic. Following prior work (Lowe et al., 2017; Foerster et al., 2018), we operate
under the centralized learning and decentralized execution paradigm wherein during training, a cen-
tralized critic guides the optimization of individual agent policies. The centralized Critic takes as
input predicted actions tat1, ..., atNu and internal state representations tht1, ..., htNu from all agents
to estimate the joint action-value Qt at every timestep. The centralized Critic is learned by tempo-
ral difference (Sutton & Barto, 1998) and the gradient of the expected return J pθiq “ ErRs with
respect to policy parameters is approximated by:
Vθi J(θi) “ E [Vθi log∏θi(at|htq QtPM,…,hN,at,…,aN)].
Note that compared to an individual critic Qi (ktt, att) for each agent, having a centralized critic leads
to considerably lower variance in policy gradient estimates since it takes into account actions from
all agents. At test time, the critic is not needed anymore and policy execution is fully decentralized.
Targeted, Multi-Stage Communication. Establishing complex collaboration strategies requires
targeted communication i.e. the ability to send specific messages to specific agents, as well as multi-
4
Under review as a conference paper at ICLR 2019
stage communication i.e. multiple rounds of back-and-forth interactions between agents. We use a
signature-based soft-attention mechanism in our communication structure to enable targeting. Each
message mt consists of 2 parts - a signature ktt P Rdk to target recipients, and a value Vt P Rdv:
signature
mi “r kt	Vt s.	⑴
I____I
value
At the receiving end, each agent (indexed by j) predicts a query vector qj'1 P Rdk from its hidden
state htj`1 and uses it to compute a dot product with signatures of all N messages. This is scaled by
1{?dk followed by a softmax to obtain attention weight a* for each message value vector:
αj “ softmax
qj'1T kt
αji
qj'1T kt
(2)
N
Ct'1 — y a∙∙vt
cj “	αjiVi .
i“1
(3)
Note that equation 2 also includes αii corresponding to the ability to self-attend (Vaswani et al.,
2017), which we empirically found to improve performance, especially in situations when an agent
has found the goal in a coordinated navigation task and all it is required to do is stay at the goal, so
others benefit from attending to this agent’s message but return communication is not needed.
For multiple stages of communication, aggregated message vector cj'1 and internal state hj are first
used to predict the next internal state h1tj taking into account a first round of communication:
hj - tanh (Wh—h，r cj'1 } hj ]) .	(4)
Next, h1tj is used to predict signature, query, value followed by repeating Eqns 1-4 for multiple
rounds until We get a final aggregated message vector cj'1 to be used as input at the next timestep.
5 Experiments
We evaluate our targeted multi-agent communication architecture on a variety of tasks and environ-
ments. All our models were trained with a batched synchronous version of the multi-agent Actor-
Critic described above, using RMSProp with a learning rate of 7 X lθ´4 and a — 0.99, batch size
16, discount factor γ — 0.99 and entropy regularization coefficient 0.01 for agent policies. All our
agent policies are instantiated from the same set of shared parameters; i.e. θ1 — ... — θN. Each
agent’s GRU hidden state is 128-d, message signature/query is 16-d, and message value is 32-d
(unless specified otherwise). All results are averaged over 5 independent runs with different seeds.
5.1 SHAPES
The SHAPES dataset was introduced by Andreas et al. (2016)1, and originally created for testing
compositional visual reasoning for the task of visual question answering. It consists of synthetic
images of 2D colored shapes arranged in a grid (3 X 3 cells in the original dataset) along with
corresponding question-answer pairs. There are 3 shapes (circle, square, triangle), 3 colors (red,
green, blue), and 2 sizes (small, big) in total (see Figure 2).
We convert each image from SHAPES into an active environment where agents can now be spawned
at different regions of the image, observe a 5 X 5 local patch around them and their coordinates, and
take actions to move around - tup, down, left, right, stayu. Each agent is tasked with navigating to
a specified goal state in the environment - t‘red’, ‘blue square’, ‘small green circle’, etc. u - and the
reward for each agent at every timestep is based on team performance i.e. rt —
# agents on goal
# agents
1github.com/jacobandreas/nmn2/tree/shapes
5
Under review as a conference paper at ICLR 2019
(a) 4 agents have to find rred, red, green, blue] respectively. t “ 1: inital spawn locations; t “ 2: 4 was on
red at t “ 1 so 1 and 2 attend to messages from 4 since they have to find red. 3 has found its goal (green) and
is self-attending; t “ 6: 4 attends to messages from 2 as 2 is on 4's target — blue; t “ 8: 1 finds red, so 1 and
2 shift attention to 1; t “ 21: all agents are at their respective goal locations and primarily self-attending.
(b) 8 agents have to find red on a large 100 X 100 environment. t = 7: Agent 2 finds red and signals all other
agents; t “ 7 to t “ 150: All agents make their way to 2’s location and eventually converge around red.
Figure 2: Visualizations of learned targeted communication in SHAPES. Figure best viewed in color.
	30 X 30, 4 agents, find[red]	50 X 50, 4 agents, find[red]	50 X 50, 4 agents, find[red,red,green,blue]
No communication	95.3±2.8%	83.6±3.3%	69.1±4.6%
No attention	99.7±0.8%	89.5±1.4%	82.4±2.1%
TarMAC	99.8±0.9%	89.5±i.7%	85.8±2.5%
Table 2: Success rates on 3 different settings of cooperative navigation in the SHAPES environment.
Having a symmetric, team-based reward incentivizes agents to cooperate with each other in finding
each agent’s goal. For example, as shown in Figure 2a, if agent 2’s goal is to find red and agent 4’s
goal is to find blue, it is in agent 4’s interest to let agent 2 know if it passes by red (t “ 2) during
its exploration / quest for blue and vice versa (t “ 6). SHAPES serves as a flexible testbed for
carefully controlling and analyzing the effect of changing the size of the environment, no. of agents,
goal configurations, etc. Figure 2 visualizes learned protocols from two different configurations, and
Table 2 reports quantitative evaluation for three different configurations. Benefits of communication
and attention increase with task complexity (30 X 30 → 50 X 50 & find[red] → find[red,red,green,blue]).
6
Under review as a conference paper at ICLR 2019
How does targeting work in the communication learnt by TarMAC? Recall that each agent
predicts a signature and value vector as part of the message it sends, and a query vector to attend
to incoming messages. The communication is targeted because the attention probabilities are a
function of both the sender’s signature and receiver’s query vectors. So it is not just the receiver
deciding how much of each message to listen to. The sender also sends out signatures that affects
how much of each message is sent to each receiver. The sender’s signature could encode parts
of its observation most relevant to other agents’ goals (for example, it would be futile to convey
coordinates in the signature), and the message value could contain the agent’s own location. For
example, in Figure 2a, at t “ 6, we see that when agent 2 passes by blue, agent 4 starts attending to
agent 2. Here, agent 2’s signature encodes the color it observes (which is blue), and agent 4’s query
encodes its goal (which is also blue) leading to high attention probability. Agent 2’s message value
encodes coordinates agent 4 has to navigate to, as can be seen at t “ 21 when agent 4 reaches there.
5.2 Traffic Junction
	Easy	Hard
No communication	84.9+4.3%	74.1+3.9%
CommNets (Sukhbaatar et al., 2016)	99.7+o.ι%	78.9+3.4%
TarMAC 1-stage	99.9+0.1%	84.6+3.2%
TarMAC 2-stage	99.9+0.1%	97.1+1.6%
Figure 3: Success rates for 1 ys. 2-stage
vs. message size on Hard. Performance
does not decrease significantly even when
the message vector is a single scalar, and
2 rounds of back-and-forth communication
before taking an environment action leads
to a significant improvement over 1-stage.
Table 3: Success rates on traffic junction. Our targeted 2-stage
communication architecture gets a success rate of 97.1% on the
‘hard’ variant of the task, significantly outperforming Sukhbaatar
et al. (2016). Note that 1- and 2-stage refer to the number of
rounds of communication between actions (Equation 4).
(a) Brake probabilities at dif-
ferent locations on the hard
traffic junction environment.
Cars tend to brake close to or
right before entering junctions.
(b) Attention probabilities at
different locations. Cars are
most attended to in the ‘inter-
nal grid' - right after the 1st
junction and before the 2nd.
(c) No. of cars being attended to. 1) is pos-
itively correlated with total cars, indicating
that TarMAC is adaptive to dynamic team
sizes, and 2) is slightly right-shifted, since it
takes few steps of communication to adapt.
Figure 4: Results on the traffic junction environment.
Environment and Task. The simulated traffic junction environments from Sukhbaatar et al. (2016)
consist of cars moving along pre-assigned, potentially intersecting routes on one or more road junc-
tions. The total number of cars is fixed at Nmax and at every timestep, new cars get added to the envi-
ronment with probability parrive. Once a car completes its route, it becomes available to be sampled
and added back to the environment with a different route assignment. Each car has a limited visibil-
ity of a 3 X 3 region around it, but is free to communicate with all other cars. The action space for
each car at every timestep is gas and brake, and the reward consists of a linear time penalty —0.01τ,
where T is the number of timesteps since car has been active, and a collision penalty rcollision “ —10.
Quantitative Results. We compare our approach with CommNets (Sukhbaatar et al., 2016) on the
easy and hard difficulties of the traffic junction environment. The easy task has one junction of
two one-way roads on a 7 X 7 grid with Nmax “ 5 and ParriVe = 0.30, while the hard task has
four connected junctions of two-way roads on a 18 X 18 grid with NmaX “ 20 and ParriVe = 0.05.
7
Under review as a conference paper at ICLR 2019
See Figure 4a, 4b for an example of the four two-way junctions in the hard task. As shown in
Table 3, a no communication baseline has success rates of 84.9% and 74.1% on easy and hard
respectively. On easy, both CommNets and TarMAC get close to 100%. On hard, TarMAC with
1-stage communication significantly outperforms CommNets with a success rate of 84.6%, while 2-
stage further improves on this at 97.1%, which is an „18% absolute improvement over CommNets.
Model Interpretation. Interpreting the learned policies, Figure 4a shows braking probabilities at
different locations: cars tend to brake close to or right before entering traffic junctions, which is
reasonable since junctions have the highest chances for collisions.
Turning our attention to attention probabilities (Figure 4b), we can see that cars are most-attended
to When in the ‘internal grid, - right after crossing the IstjUnction and before hitting the 2nd junc-
tion. These attention probabilities are intuitive: cars learn to attentively attend to specific sensitive
locations With the most relevant local observations to avoid collisions.
Finally, FigUre 4c compares total nUmber of cars in the environment vs. nUmber of cars being at-
tended to with probability > 0.1 at any time. Interestingly, these are (loosely) positively correlated,
With Spearman’s σ “ 0.49, Which shoWs that TarMAC is able to adapt to variable nUmber of agents.
CrUcially, agents learn this dynamic targeting behavior pUrely from task rewards with no hand-
coding! Note that the right shift between the two cUrves is expected, as it takes a few timesteps of
commUnication for team size changes to propagate. At a relative time shift of 3, the Spearman’s
rank correlation between the two cUrves goes Up to 0.53.
Message size vs. multi-stage communication. We stUdy performance of TarMAC with varying
message valUe size and nUmber of roUnds of commUnication on the ‘hard’ variant of the traffic
jUnction task. As can be seen in FigUre 3, mUltiple roUnds of commUnication leads to significantly
higher performance than simply increasing message size, demonstrating the advantage of mUlti-
stage commUnication. In fact, decreasing message size to a single scalar performs almost as well as
64-d, perhaps becaUse even a single real nUmber can be sUfficiently partitioned to cover the space of
meanings/messages that need to be conveyed for this task.
5.3 House3D
Finally, we benchmark TarMAC on a cooperative point-goal navigation task in HoUse3D (WU et al.,
2018). HoUse3D provides a rich and diverse set of pUblicly-available2 3D indoor environments,
wherein agents do not have access to the top-down map and mUst navigate pUrely from first-person
vision. Similar to SHAPES, the agents are tasked with finding a specified goal (sUch as ‘fireplace’),
spawned at random locations in the environment and allowed to commUnicate with each other and
move aroUnd. Each agent gets a shaped reward based on progress towards the specified target. An
episode is sUccessfUl if all agents end within 0.5m of the target object in 50 navigation steps.
Table 4 shows sUccess rates on a find[fireplace] task in HoUse3D. A no-commUnication navigation
policy trained with the same reward strUctUre gets a sUccess rate of 62.1%. Mean-pooled commUni-
cation (no attention) performs slightly better with a sUccess rate of 64.3%, and TarMAC achieves the
best sUccess rate at 68.9%. FigUre 5 visUalizes predicted navigation trajectories of 4 agents. Note
that the commUnication vectors are significantly more compact (32-d) than the high-dimensional
observation space, making oUr approach particUlarly attractive for scaling to large teams.
SUccess rate
No communication	62.1+5.3%
No attention	64.3+2.3%
TarMAC	68.9+1.1%
Table 4: Success rates on a 4-agent cooperative find[fireplace] navigation task in House3D.
6 Conclusions and Future Work
We introduced TarMAC, an architecture for multi-agent reinforcement learning which allows tar-
geted interactions between agents and multiple stages of collaborative reasoning at every timestep.
2
github.com/facebookresearch/house3d
8
Under review as a conference paper at ICLR 2019
Figure 5: Agents navigating to the fireplace in HouSe3D (marked in yellow). Note in particular that agent 4 is
spawned facing away from it. It communicates with others, turns to face the fireplace, and moves towards it.
Evaluation on three diverse environments show that our model is able to learn intuitive attention be-
havior and improves performance, with downstream task-specific team reward as sole supervision.
While multi-agent navigation experiments in House3D show promising performance, we aim to
exhaustively benchmark TarMAC on more challenging 3D navigation tasks because we believe this
is where decentralized targeted communication can have the most impact - as it allows scaling to a
large number of agents with large observation spaces. Given that the 3D navigation problem is hard
in and of itself, it would be particularly interesting to investigate combinations with recent advances
orthogonal to our approach (e.g. spatial memory, planning networks) with the TarMAC framework.
References
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural Module Networks. In
CVPR, 2016. 2,5
L. Busoniu, R. Babuska, and B. De Schutter. A Comprehensive Survey of Multiagent Reinforcement
Learning. Trans. Sys. Man Cyber Part C, 2008. 2
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP, 2014. 4
Abhishek Das, Satwik Kottur, Jose M.F. Moura, Stefan Lee, and Dhruv Batra. Learning Cooperative
Visual Dialog Agents with Deep Reinforcement Learning. In ICCV, 2017. 3
Jakob Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. Learning to communi-
cate with deep multi-agent reinforcement learning. In NIPS, 2016. 1, 2, 3
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI, 2018. 2, 4
Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs.
In AAAI, 2015. 2
Yedid Hoshen. VAIN: Attentional multi-agent predictive modeling. In NIPS. 2017. 2, 3
Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nico-
las Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray
Kavukcuoglu, and Thore Graepel. Human-level performance in first-person multiplayer games
with population-based deep reinforcement learning. arXiv preprint arXiv:1807.01281, 2018. 2
Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.
CoRR, 2018. 3
Emilio Jorge, Mikael Kageback, and Emil Gustavsson. Learning to play guess who? and inventing a
grounded language as a consequence. In NIPS workshop on Deep Reinforcement Learning, 2016.
3
Satwik Kottur, Jose MF Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge
‘naturally’ in multi-agent dialog. In EMNLP, 2017. 3
9
Under review as a conference paper at ICLR 2019
Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the
emergence of (natural) language. In ICLR, 2017. 3
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. In NIPS, 2017. 2, 3, 4
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. arXiv preprint arXiv:1703.04908, 2017. 3
Frans A. Oliehoek. Decentralized POMDPs. In Reinforcement Learning: State of the Art. Springer
Berlin Heidelberg, 2012. 3
OpenAI. OpenAI Five. https://blog.openai.com/openai-five/, 2018. 2
Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long, and Jun Wang.
Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv
preprint arXiv:1703.10069, 2017. 2
Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and
Logical Foundations. Cambridge University Press, 2008. 2
Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. In NIPS, 2016. 1, 2, 3, 7
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, 1998.
3, 4
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
LUkasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 5
Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building Generalizable Agents With a
Realistic And Rich 3D Environment. arXiv preprint arXiv:1801.02209, 2018. 2, 8
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In NIPS, 2017. 2
10