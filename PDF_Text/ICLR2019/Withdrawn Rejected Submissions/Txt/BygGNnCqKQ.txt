Under review as a conference paper at ICLR 2019
Architecture Compression
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we propose a novel approach to model compression termed Archi-
tecture Compression. Instead of operating on the weight or filter space of the
network like classical model compression methods, our approach operates on the
architecture space. A 1-D CNN encoder/decoder is trained to learn a mapping
from discrete architecture space to a continuous embedding and back. Additionally,
this embedding is jointly trained to regress accuracy and parameter count in order
to incorporate information about the architecture’s effectiveness on the dataset.
During the compression phase, we first encode the network and then perform
gradient descent in continuous space to optimize a compression objective function
that maximizes accuracy and minimizes parameter count. The final continuous
feature is then mapped to a discrete architecture using the decoder. We demonstrate
the merits of this approach on visual recognition tasks such as CIFAR-10/100,
FMNIST and SVHN and achieve a greater than 20x compression on CIFAR-10.
1	Introduction
The recent adoption of CNNs for real-time, on-device applications has fueled a great demand for
better model compression. Conventional methods such as quantization, pruning and distillation have
proven to be reliable approaches in reducing redundancies in networks. However, one avenue where
there has been little progress is that of architecture space based compression.
Approaches such as Iandola et al. (2016), Howard et al. (2017), Rastegari et al. (2016), have introduced
hand defined heuristics to design networks that are efficient without sacrificing accuracy. However,
designing such networks still remains a laborious task, requiring much human expertise. In such a
context, an efficient dataset-driven approach to determine the optimal architecture is desirable.
Recent methods such as Ashok et al. (2018); He et al.; Zhou et al. (2018); Tan et al. (2018) have pro-
posed dataset-driven architecture based model compression using reinforcement learning. However,
one drawback of such RL based methods is that they are less sample-efficient, often requiring a large
number of architecture evaluations and several hours or days of training to find a single compressed
model. Furthermore, it is unclear how these approaches compare to random search and whether they
only succeed due to large-scale exploration and evaluation of many architectures.
To the best of our knowledge, this is the first paper to introduce a novel gradient descent based
approach to perform architecture compression. To facilitate learning, we describe a mapping from
discrete architecture space to a continuous space that encodes the structure of architecture for a
specific dataset. We train a one-dimensional convolutional encoder/decoder neural network to learn
the structure of discrete architecture space while using accuracy and parameter regressors to impose
the structure inherent to learning the dataset. For compression, we encode an input architecture and
leverage this learned continuous latent space to perform gradient descent on the encoded feature. The
augmented feature is then decoded into a compressed discrete architecture.
We demonstrate the effectiveness of our approach on several visual learning tasks of varying difficulty
(FMNIST, SVHN, CIFAR-10, CIFAR-100) and show 5-20x compression on architectures. We
also compare this approach to conventional compression methods such as pruning, distillation and
reinforcement learning and show that our architectures are smaller and faster than those produced by
the baseline methods.
1
Under review as a conference paper at ICLR 2019
Figure 1: High level overview of 1D-CNN encoder-decoder system
2	Related works
2.1	Architecture Search
There have been recent works in architecture search such as Zoph & Le (2016), Baker et al. (2016),
Miikkulainen et al. (2017), Real et al. (2017), that use reinforcement learning or evolutionary
algorithms to traverse the space of architectures. These approaches characterize architectures as
discrete entities in a non-differentiable space and use either a constrained state space or heuristics to
discover architectures. These approaches have been subject to criticism from the machine learning
community for using large amounts of computation.
Work such as Ashok et al. (2018), Tan et al. (2018), Zhou et al. (2018), He et al. attempt to
search for efficient architectures instead of just the best. However these approaches also currently
rely on reinforcement learning. In contrast to these approaches, our method shows that we can
leverage classical optimization techniques such as gradient descent to effectively search the space
of architectures for a constrained objective. Methods such as Liu et al. (2018) and Luo et al. (2018)
attempt to make the space of architectures differentiable however this work is focused on discovering
repeatable convolutional cells instead of optimizing the architecture as a whole.
2.2	Pruning, Quantization, Hand designed models
Pruning-based methods preserve the weights that matter most and remove the redundant weights
LeCun et al. (1989), Hassibi et al. (1993), Srinivas & Babu (2015), Han et al. (2015b), Han et al.
(2015a), Mariet & Sra (2015), Anwar et al. (2015), Guo et al. (2016). While pruning-based approaches
typically operate on weight space, our approach operates on the the model architecture. Additionally,
our method offers greater flexibility as we can use memory, inference time, power, and other hardware
constraints to guide the compression process, resulting in the optimal architecture for a given dataset
and constraints.
Hand designed models such as Iandola et al. (2016) and Howard et al. (2017) have been shown to work
well in practical applications such as self-driving cars and mobile phones. These are good stepping
stones in making progress in designing efficient architectures. Our work focuses on designing a
general method to design such architectures in a fully automated data-driven manner.
2
Under review as a conference paper at ICLR 2019
3	Structure of architecture compression space
In this section we show how to map certain classes of architectures to a unique vector representation
in order to enable learning. We then analyze the cardinality of architecture space for compression
and prove that this space has several desirable properties for learning a mapping.
3.1	Topological representation of architectures
We observe that most modern neural network architectures A ∈ A, with the exception of recurrent
neural networks can be expressed as a directed acyclic graph (DAG). We use the property that every
DAG has a topological ordering (Proof in section 7) to represent an architecture as a partially ordered
sequence of layers, li ∈ L. Formally,
l1:N = [l0,l1...lN] such that ∀i, j ∈ [1,N],i >j,@li →lj	(1)
where the → operator denotes that the activation of li is the input of lj . Furthermore, this topological
ordering is unique (Proof in section 7.2) for standard convolutional networks and those with simple
skip connections (e.g. ResNet). We will denote this ordered representation of an architecture as
o : A → LN : A 7→ l1:N	(2)
where o is the topological sorting function. Note also that for unique, one-to-one mappings, there
exists an inverse function o-1 to recover A given l1:N
o-1 : LN → A : o(A) 7→ A	(3)
3.2	Cardinality of architecture compression space
Let A ∈ A be an architecture that is parameterized by θ and trained on some dataset D . Then we
can model the accuracy a ∈ [0, 1] as a continuous random variable stochastically sampled from a
distribution Pθ(a|A, D), since θ is typically optimized using a variant of stochastic gradient descent.
The expected accuracy Eθ [a| A, D], while not exactly computable due to the large dimension of θ,
can be approximated by training and evaluating the network under varying intializations θ0 , i.e.
1N
Eθ[a∣A,D] ≈ N∑>θ(A,。,%)	(4)
i
where g minimizes an objective function over D with respect to θ. Thus, we are interested in learning
a mapping from discrete architecture space to the expected accuracy
f : A → [0,1] ⊂ R : A → Eθ(a|A, D)	(5)
However, since we are interested in model compression, we also want to construct the inverse mapping
from the expected accuracy to discrete architecture space for a given parameter count p ∈ R+
fp-1 : R+ × ([0,1] ⊂ R) → A : Eθ(a∣A, D) → A	(6)
In the following subsections, we prove that such a mapping, while not unique, is feasible to learn
since the range of valid discrete architectures is finite.
For the following sections, we will denote parametric layers i.e. convolutional layers, batch normal-
ization and fully connected layers as fθ(x), where x is the activation of the previous layer and θ
represents the parameters of the layer.
Non-parametric pooling layers as max pooling and average pooling layers are written as qs (x), where
s > 1 (no upsampling) is the stride of the layer. Lastly activations such as sigmoid, ReLU, softmax
etc. are represented as σ(x).
We also make the following practical assumptions in the following proofs:
1.	The input X ∈ RH×W×C, where H, W, C ∈ N+ andH,W,C are fixed.
2.	For layers li ∈ A, if li ∈ Σ, li+1 ∈/ Σ, where Σ is the set of possible activation functions.
3
Under review as a conference paper at ICLR 2019
Figure 2: Structure of architecture compression spaces with forward (blue) and backward (red)
mappings visualized
3.	Pooling layers reduce dimensions of input image by some factor
Lemma 3.1. For a spatial input of dimension d0, the number of pooling layers possible in a valid
architecture is finite.
Proof. Let xi be the input of the ith layer with spatial dimension di ∈ N+. We observe that a pooling
operation with stride s ∈ N+ > 1 and kernel size k ∈ N+ xi+1 = qs (xi) results in an output of
di+ι = bdis-kC + 1 < di. Since do is fixed (Assumption 1) and S is discrete, for any do there is a
finite number of pooling layers possible in a valid architecture.	□
Theorem 3.2. For a given parameter constraint p, the set of valid architectures that meet the
parameter constraint Ap = {A| numParams(A) ≤ p} is finite.
Proof. Next, suppose we have a partially constructed network with parameter count po . Then by
assumption 2, we can add at most one activation function. Furthermore by Lemma 3.2, we have a
finite choice of pooling layers such that the architecture still remains valid.
Suppose We add a parametric layer li = fθi, the parameter constraint exceeded if po + | θi | > p. Thus,
We have the upperbound ∣θ∕ ≤ P - po. Since ∣θ∕ is discrete, we have a finite number of choices of
layers such that the partially constructed netWork is still valid.
We then observe that since di monotonically decreases if We add a pooling layer and the upperbound
of ∣θi∣ monotonically decreases if we add a parametric layer.
Since at least every other layer has to be one of these types, the length of any valid architecture is
finite. Thus, we arrive at the conclusion that there are a finite number of valid architectures for a
given parameter constraint.	□
4	Approach
Having proved that a mapping is feasible, we now describe our approach to train a 1-dimensional
convolutional encoder/decoder, an accuracy regressor and a parameter regressor to learn such a
mapping (Outline in Fig. 1).
The variable length input vector to the network denoted by x1:T = [xi, ..., xT] ∈ Z5×T is a
topologically ordered representation of the architecture as per Section 3.1. Each layer is represented
by the hyperparameters of each layer. Specifically, it is a discrete 5 dimensional vector as follows:
xi = [t, k, o, s, p] ∈ Z5	(7)
where t is the layer type, k the kernel size, o the number of output channels, s the stride and p the
padding.
4
Under review as a conference paper at ICLR 2019
4.1	Encoder and decoder
The encoder is a one-dimensional convolutional neural network that takes as input the discrete
architecture representation and outputs a continuous feature. Prior approaches use recurrent networks
such as LSTMs or bidirectional RNNs where backpropagating through large sequences can result in
vanishing gradients and difficult credit assignment Hochreiter et al. (2001).
In our approach, we use a 1D-CNN encoder for several reasons. It can operate on variable length
sequences, CNNs with sufficient depth have a large receptive field over the input sequence and it does
not suffer from vanishing gradients since the length of gradient propagation is no longer a function of
the sequence length but a function of the depth of the inference network.
The encoder E , computes the function:
y = E(x1:T)	(8)
where x1:T ∈ Z5×T is a variable length sequence representing the architecture and y ∈ RN a
continuous feature embedding. Details of the network can be found in the appendix.
The decoder has a similar residual 1D-CNN architecture as the encoder except that convolutions are
replaced with transposed convolutions.
The decoder D, computes the function:
x1:T = D(y)	(9)
where x；? ∈ R5×t is a continuous variable length sequence representing the architecture and
y ∈ RN a continuous feature embedding. The continuous variable x；：T is discretized to Xi：T ∈ Z5 ×t.
4.2	Accuracy and parameter regressors
The accuracy regressor and parameter regressor are two fully connected networks that take the
continuous embedding of the architecture y as input and predict the expected accuracy, Eθ [a] and
parameter count, p of the architecture A.
The accuracy regressor learns the function
a = fa(y)	(10)
where a ∈ [0, 1] ⊂ R is trained to predict a； = Eθ[a] (which is approximated as per 3.2 by training
the network with multiple intitializations). This network consists of 3 fc-relu blocks with a sigmoid
output activation to convert the output to [0, 1].
The parameter regressor learns the function
p = fp(y)	(11)
where p ∈ R+ is trained to match the ground truth parameter count p； . The ground truth parameter
count is computed as p； = Pi Jli|-p where |li| is the number of parameters of each layer in the
network. The mean parameter count P is subtracted to center the parameter count around 0 and
the standard deviation of the parameter count σp , is used to scale the parameter count for better
conditioning of the range of the output. This network consists of 3 fc-relu blocks with a ReLU
function to predict a non-negative real scalar.
4.3	Optimization
In order to learn to encode/decode the architecture and predict accuracy/parameter count, we jointly
optimize a multi-task loss function. While all four tasks are formulated as regression problems, each
has a differing domain. To account for this, we choose appropriate loss functions to best suit each
task.
The decoder is trained to approximate positive integral values of varying scale since we may have
small values for padding, kernel size, type and stride and large values for number of outputs. To
enable faster convergence, we use a mean squared error.
Ld(X1:T, xi：T)
1T
T £|xi- xi|2
i=0
(12)
5
Under review as a conference paper at ICLR 2019
A standard L-1 loss is used for the accuracy regressor since the output of the sigmoid function is
bounded between [0, 1].
La(^,α*) = |a - α*∣ι	(13)
For training the parameter regressor, we use the Huber loss with δ = 1. This loss is less sensitive
to outliers in the data and does not suffer from exploding gradients as described in Girshick (2015).
This is important since the domain of the parameter count is R and it is possible that the training set
may have outliers such as large networks that do not converge.
Lp(P,p*)
if|p-P*| ≤ 1
otherwise
The network is then trained to minimize the joint loss function:
L = Ld + λ1La + λ2Lp
(14)
(15)
4.4	Architecture compression using gradient descent
Algorithm 1 Architecture Compression
1:	procedure TRAIN(A, D)
2:	for A in Architectures do
3:	a* — train(A, D)
4:	p* — (num_params(A) — p)∕σp
5:	X — o(A)
6:	y — encoder0] (x)
7:	a — acc_regressor@] (y)
8:	P — param_regressor@3 (y)
9:	X — decoderθ4 (y)
10	L = Ld(X,x) + λι La(a,α*) + λ2Lp(p,p*)
11	:	θ = argminθL
12	:	end for
13	: end procedure
14	: procedure COMPRESS(A)
15	x — o(A)
16	y — encoder0] (x)
17	a — acc_regressor@2 (y)
18	P — param_regressore (y)
19	Lc = La(a, 1) + λLp(p, 0)
20	y — y + η 端
21	Xcomp — decoderθ4 (y)
22	Acomp 4- o	(XCOmP)
23	: end procedure
Once the inference network has converged and can predict the accuracy and number of parameters of
a network, we use the network for architecture compression.
In order to compress an architecture, we perform gradient descent on the continuous embedding
space using the compression objective function.
Lc(^,P) = La(a,1) + λLp(p, 0)	(16)
The compression objective function simultaneously minimizes the number of parameters and maxi-
mizes the accuracy of the network to achieve compression. We can also incorporate known hardware
constraints p0 or accuracy requirements a0 to relax this objective function by modifying it as follows
Lc(a,P)= La(a,αo) + λLp(p,Po)	(17)
The gradient from this objective is then backpropagated to the continuous embedding y with learning
rate η to generate a new embedding y0 .
y0 = y + ηdLc	(18)
dy
6
Under review as a conference paper at ICLR 2019
The decoder converts this embedding into a temporal sequence xcomp that is then discretized and
converted into an architecture Acomp = o-1 (xcomp).
5	Experiments
Figure 3: Joint training loss on (from top-bottom) FMNIST, SVHN, CIFAR-10, CIFAR-100
In this section, we empirically evaluate our approach, showing that it is capable of compressing
networks by upwards of 10x by benchmarking them on modern classification tasks such as CIFAR-10,
CIFAR-100, FMNIST and SVHN.We show that our approach outperforms current state of the art
architecture compression techniques by comparing the results to several baselines.
In the following experiments, we used a randomly generated dataset of 1500 architectures for training
the encoder/decoder and regressors. Each architecture was trained for each task for 5 epochs with
m=5 different random initializations to obtain a target expected accuracy. As observed in Ashok et al.
(2018); Zoph & Le (2016); Tan et al. (2018), 5 epochs of training seems to provide sufficient signal
about the convergence characteristics of the network. All of the following experiments were run on 2x
NVIDIA 1080TI GPUs. Additional training details such as choice of optimizer and hyperparameters
are included in Section 10 in the appendix.
5.1	Datasets
FMNIST The Fashion-MNIST Xiao et al. (2017) dataset consists of 28 × 28 pixel grey-scale images
depicting images of fashion products from 10 categories. We use the standard 60,000 training images
and 10,000 test images for experiments.
CIFAR-10 The CIFAR-10 Krizhevsky & Hinton (2009) dataset consists of 10 classes of objects
and is divided into 50,000 train and 10,000 test images (32x32 pixels). This dataset provides an
7
Under review as a conference paper at ICLR 2019
incremental level of difficulty over the FMNIST dataset, using multi-channel inputs to perform model
compression.
SVHN The Street View House Numbers Netzer et al. (2011) dataset contains 32x32 colored digit
images with 73257 digits for training, 26032 digits for testing. This dataset is slightly larger that
CIFAR-10 and allows us to observe the performance on a wider breadth of visual tasks.
CIFAR-100 To further test the robustness of our approach, we evaluated it on the CIFAR-100 dataset.
CIFAR-100 is a harder dataset with 100 classes instead of 10, but the same amount of data, 50,000
train and 10,000 test images (32x32). Since there is less data per class, there is a steeper size-accuracy
tradeoff.
5.2	Compression experiments
Table 1: Summary of compression results
Model	Acc.	#Params	∆ Acc. Compr	
Fashion-MNIST			
CONV-7	91.46% 708K	+0.98%	7.61x
CIFAR-10			
CONV-10	92.35% 24.4M	-0.04%	20.33x
CIFAR-100			
CONV-10	70.95% 24.4M	-1.32%	4.51x
SVHN			
CONV-10	96.02% 24.4M	-0.63%	8.76x
In this section we evaluate the ability of our approach to compress architectures. For our experiments,
we use a standard convolutional architecture consisting of stacked conv-bn-relu blocks and a fully
connected layer. We use a 7 block network CONV-7 for Fashion-MNIST and 10 block network
CONV-10 for the others. Table shows original average accuracy of the model on the validation
set, followed by the number of parameters in the original model followed by two columns for the
improvement in accuracy in the compressed architecture and the compression rate. We notice a
minor improvement in accuracy by the compressed architecture in Fashion-MNIST while observing
a minor drop in the other datasets. Additionally, our method is able to achieve solid compression
on all the datasets and up to 20x compression on CIFAR-10 (1.2M parameters for final compressed
architecture).
5.2.1	Baselines
We compare our approach to various model compression baselines including reinforcement learning,
pruning and knowledge distillation on the CIFAR-10 dataset. The experiments show that our approach
is able to find a smaller and more accurate model than the other approaches.
Table 2: Baselines on CIFAR-10
Model	Acc.	#Params
Romero et al. (2014) (Distillation)	91.33%	1.2M
Molchanov et al. (2016) (Pruning)	91.06%	2.3M
Ashok et al. (2018) (RL)	92.05%	1.7M
Ours	92.31%	1.2M
8
Under review as a conference paper at ICLR 2019
6	Conclusion
In conclusion, we have described a novel approach to compress CNNs by first training a continuous
embedding on a representation of the architecture and then performing gradient descent to determine
an optimal architecture for the given task. We also introduced a novel theoretical analysis of CNNs
which we hope will inspire future work. We also demonstrate that our method performs well over
a variety of computer vision datasets. Given that this is a novel direction of research, we note that
there exist multiple future directions to go. Expanding the search space to include networks of greater
complexity such as ResNets or DenseNets would be of practical interest. Analyzing the transfer
learning properties of this approach via the reuse of the weights for other tasks would be of great
practical use as well.
References
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural
networks. arXiv preprint arXiv:1512.08571, 2015.
Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, and Kris M Kitani. N2n learning: Network to
network compression via policy gradient reinforcement learning. 2018.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architec-
tures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.
Ross Girshick. Fast r-cnn. In Proceedings of the 2015 IEEE International Conference on Computer
Vision (ICCV),ICCV '15,pp. 1440-1448, Washington, DC, USA, 2015. IEEE Computer Society.
ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.169. URL http://dx.doi.org/10.
1109/ICCV.2015.169.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Advances
In Neural Information Processing Systems, pp. 1379-1387, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135-1143,
2015b.
Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In Neural Networks, 1993., IEEE International Conference on, pp. 293-299. IEEE, 1993.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model
compression and acceleration on mobile devices.
Sepp Hochreiter, Yoshua Bengio, and Paolo Frasconi. Gradient flow in recurrent nets: the difficulty
of learning long-term dependencies. In J. Kolen and S. Kremer (eds.), Field Guide to Dynamical
Recurrent Networks. IEEE Press, 2001.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, John S Denker, Sara A Solla, Richard E Howard, and Lawrence D Jackel. Optimal
brain damage. In NIPs, volume 2, pp. 598-605, 1989.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search, 2018.
9
Under review as a conference paper at ICLR 2019
Renqian Luo, Fei Tian, Tao Qin, and Tie-Yan Liu. Neural architecture optimization. arXiv preprint
arXiv:1808.07233, 2018.
Zelda Mariet and Suvrit Sra. Diversity networks. arXiv preprint arXiv:1511.05077, 2015.
Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala
Raju, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural networks. arXiv
preprint arXiv:1703.00548, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. 2016.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pp. 525-542. Springer, 2016.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc Le, and
Alex Kurakin. Large-scale evolution of image classifiers. arXiv preprint arXiv:1703.01041, 2017.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv
preprint arXiv:1507.06149, 2015.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. Mnasnet: Platform-aware
neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Yanqi Zhou, Siavash Ebrahimi, Sercan O Arik, Haonan Yu, Hairong Liu, and Greg Diamos. Resource-
efficient neural architect. arXiv preprint arXiv:1806.07912, 2018.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
7	Appendix
7.1	Topological ordering
Lemma 7.1. If G is a directed acyclic graph (DAG), then G has a topological ordering
Proof. We prove this by induction on n, where n = |G|.
Base case:
For n = 1, the statement is true since the topological ordering is G.
Hypothesis:
Assume that there exists a topological ordering on all DAGs G of size k.
Inductive step:
Given a DAG G0 with k + 1 nodes, select a node v with no incoming edges.
Then G* = G0 - {v} is a DAG since deleting V cannot induce a cycle on G0.
Since |G *| = k, the hypothesis implies that G* has a topological ordering T*.
10
Under review as a conference paper at ICLR 2019
Since v has no incoming edges, no partial order is violated by placing v at the head of the topologically
ordered sequence.
Thus T = [v, T*] is a valid topological ordering of G0 and the lemma is proven.	□
Lemma 7.2. If a topological sort l1:T = [l0...li...lT] has the property that ∀i, ∃ edge(i, i + 1), then
it is unique
Proof. It follows directly that the above property implies that there exists a Hamiltonian path on the
graph since traversing the graph in the topologically sorted order satisfies each node being visited
once. We can then show that the existence of a Hamiltonian path in a DAG implies unique ordering.
Suppose a DAG has two Hamiltonian paths, then let x ∈ P1, y ∈ P2 be the first nodes on the paths
(P1 , P2) that differ.
This implies that there is a path from x → y that is a subpath of P1 and a path from y → x that is a
subpath of P2. This implies a cycle in the graph, which is a contradiction.	□
7.2	Visualization of 1D-CNN filters
Figure 4: First 100 layer 1 filters for encoder (left) and decoder (right). Each column represents an
input channel and rows represent filters.
8	Details on 1-D CNN architecture
Let xi ∈ ZK ×T be the ith activation or in the case of the first layer, the discrete representation of an
architecture. Then a 1-d convolution operation applies a filter w ∈ RL×K to a window of L features
in the input vector. We then add a bias b ∈ R to this output, to produce a feature map hi . Finally we
apply a batch normalization function g, an activation function σ and add the residual to produce a
11
Under review as a conference paper at ICLR 2019
new activation xi+1 as follows. Appropriate padding is added to the output in order to preserve the
dimension and allow element-wise addition of the residual.
hi = W * Xi + b
7^	/ 7	∖
hi = g(hi)
xi+1 = σ(hi) + xi
In the case of a strided convolution where the temporal dimension of the output is smaller than
the original input, we apply a convolution with the same stride s. The operations then become the
following.
hi = fs(xi, w1, b1)
7^	/ 7	∖
hi = g(hi)
Xi = fs(χi,w2,b2)
/ 7^ ʌ . ʌ
Xi+1 = σ(hi) + Xi
where fs(X, w, b) performs convolutions on input X with stride s, weights w and bias b.
9	Sampling of random architectures
In section 4.4, we describe a set of randomly generated architectures that are trained and then used to
learn a continuous architecture search space. In this section, we detail how this set of architectures is
generated.
In order to generate a sensible set of architectures, we randomly sample architectures and then reject
degenerate architectures.
First We sample the length of the architecture randomly from a discrete Unifrom distribution l 〜
U (2, 50). We set the lower bound to be 2 since we need at least one classification layer (fully
connected) and one convolution layer for the architecture to be valid. Furthermore, We set the
upperbound to 50 layers as an arbitrary limit that is Within experimental constraints.
After this, We sample the 5 layer configuration variables for each of the l layers randomly. Specifically,
t 〜U (1, 8) where:
1.	Convolution
2.	MaxPool
3.	BatchNorm
4.	ReLU
5.	Sigmoid
6.	Dropout
7.	AveragePool
8.	Linear
Additionally, k 〜U(1,10), S 〜U(1,10), o 〜U(16,4096), P 〜U(1,10).
For layers that do not have certain configuration variables (e.g. batch normalization has no stride),
we set the variable to be 0. For certain layers like MaxPool, ReLU or BatchNorm that do not change
the number of output channels, we set the variable to the number of input channels. Furthermore, We
also enforce that the number of input channels to the first is the same as the input of the task example
and that the final layer is a fully connected layer with the number of outputs corresponding to the
number of classes in the task.
Lastly, we filter out degenerate architectures if they do not contain any convolutional layers, the
output dimension is too small to proceed or the memory/computational footprint is too large to be
practical.
12
Under review as a conference paper at ICLR 2019
10	Training details
10.1	Randomly generated architectures
This section describes the implementation details for the training procedure of the randomly
generated architectures. All the experiments used the Adam optimizer and were run on the PyTorch
framework. The same procedure was used to train the final output architecture. Fashion-MNIST
The architectures for FMNIST were trained for 50 epochs with a starting learning rate of 0.01. The
learning rate is reduced by a factor of 10 in the 30th epoch. A batch size of 64 was used.
CIFAR-10/100 The architectures for CIFAR-10/100 were trained for 150 epochs with a starting
learning rate of 0.001. The learning rate is decreased by a factor of 10 in the 80th and 120th epochs.
Standard data augmentation with horizontal mirroring (p=0.5), random cropping with padding of 4
pixels and mean subtraction of (0.5, 0.5, 0.5). A batch size of 128 was used.
SVHN The architectures for SVHN were trained for 150 epochs with a starting learning rate of 0.001.
The learning rate is decreased by a factor of 10 in the 80th and 120th epochs. Mean subtraction of
(0.5, 0.5, 0.5) and a batch size of 128 was used.
10.2	Architecture compression search
The encoder, decoder and regressors were optimized using the SGD optimizer with nesterov mo-
mentum=0.5 and a learning rate of 0.003. The CIFAR-10/100 networks were trained jointly for
100000 iterations while the SVHN and FMNIST ones were trained for 80000 iterations. Furthermore,
learning rate decay was used with period 30 and decay multiplier 1/5.
13