Under review as a conference paper at ICLR 2019
Meta Learning with Fast/Slow Learners
Anonymous authors
Paper under double-blind review
Ab stract
Meta-learning has recently achieved success in many optimization problems. In
general, a meta learner g(.) could be learned for a base model f(.) on a variety
of tasks, such that it can be more efficient on a new task. In this paper, we make
some key modifications to enhance the performance of meta-learning models. (1)
we leverage different meta-strategies for different modules to optimize them sepa-
rately: we use conservative “slow learners” on low-level basic feature representa-
tion layers and “fast learners” on high-level task-specific layers; (2) Furthermore,
we provide theoretical analysis on why the proposed approach works, based on a
case study on a two-layer MLP. We evaluate our model on synthetic MLP regres-
sion, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks.
We demonstrate that our approach is able to achieve state-of-the-art performance.
1	Introduction
The current success of deep learning hinges on the ability to apply gradient-based optimization in
high-capacity models. It has achieved impressive results on many large-scale supervised tasks such
as image classification (Krizhevsky et al. (2012); He et al. (2016); Szegedy et al. (2017)) and games
(et.al. (2016; 2013)). Notably, although these models achieve superior performance, they require
huge amount of data and iterations. In contrast, humans can rapidly learn a novel concept from only
a few examples. The ability of fast knowledge acquisition might be related to the meta learning
process in the human brain (Harlow (1949)).
To improve learning efficiency, a seminal work named “learning-to-learn”, or “meta-learning”
(Andrychowicz et al. (2016)) has been proposed to accelerate the learning process. Suppose in a
machine learning task, we aim to optimize an objective f (θ) defined over some domain θ ∈ Θ. At
step t, rather than applying stochastic gradient descent (SGD) as θt+ι = θt - ɑNf (θt), a meta
learner g can be learned to guide the training process of f:
θt+ι= θt + gt(Vf (θt ),φ)	(1)
An interesting phenomenon has been observed in (Andrychowicz et al. (2016)): although successful
on MNIST, meta-learning does not perform as well on CIFAR-10. This issue becomes more severe
in case of large-scale learning tasks such as ImageNet Training (Russakovsky & et.al. (2015)). To al-
leviate this issue, a more constrained form of parameter update model is introduced in (Wichrowska
et al. (2017)) to improve generalization.
The reason of failure, in our opinion, may lie in that, in a CNN, the low-level convolutional and
high-level fully-connected modules play different roles in visual recognition. The former tend to
learn generic features across categories, while the latter are generally more task-specific. This
phenomenon of feature transferability has been studied in (Yosinski et al. (2014); Azizpour et al.
(2016)). If we apply the same learning strategy for all modules, some parameters may not converge
well. Accordingly, we explore an improved multiple meta-strategies model. Drawing inspiration
from the common trick of “fine-tune” across tasks, we leverage a conservative “slow-learner” for
low-level modules and a “fast-learner” for high-level modules to enable rapid learning.
Notably, another factor enabling fast learning is that human-being could rely on prior knowledge and
memories. Low-level perceptional modules could be inheritable characteristics preservable in nat-
ural selection over successive generations, undergoing a lifelong learning process (Briscoe & Chit-
tka. (2001)). Accordingly, we adopt a lifelong learning scheme for the slow-learner with parameter
shared across new meta-learning tasks. The assumption we make here is that the generic low-level
1
Under review as a conference paper at ICLR 2019
representation is a good prior “knowledge”, which could in turn enables us to learn “smarter”. A
high level view of our fast/slow meta-learning is shown in Figure 1.
1.1	Contributions
The main contributions of our work are three-folds:
1.	We propose a fast/slow meta-learning approach to optimize a convolutional neural network.
By applying different learners on different modules, our model could further improve the
meta-learning performance method;
2.	We provide theoretical study on why the proposed meta-learning on a two-layer MLP;
3.	Experiments on the synthetic, Omniglot and ImageNet datasets validate our model.
2	Related Work
Early works on learning-to-learn or meta-learning can be dated back to (Mitchell & Thrun (1993);
Hochreiter et al. (2001)). The problem can generally be formulated as a two-level learning task:
a meta-level model across tasks and a specific-level model within each task. The former acquires
generic knowledge, ideally transferrable to the latter. The specific and meta-level models could
be framed as a single learner or separate learners. Along this line, a series of work Vilalta &
Drissi. (2002); Baxter. (1998) were proposed. Perhaps, the most general framework is (Schmid-
huber. (1993)), which considers a neural network able to modify their own weights.
Recently, a seminal work (Andrychowicz et al. (2016)) leverages one recurrent neural network (“op-
timizer”) to train another base neural network (“optimizee”). The proposed model achieves better
learning curves than hand-designed optimizers such as SGD and ADAM (Kingma & Ba (2014)).
Our model improves upon their work by explicitly applying fast/slow meta-learners on different
modules in the base neural network. (Li & Malik (2017)) relied on policy search to compute meta-
parameters. (Wichrowska et al. (2017)) constructed a hierarchical RNN to capture different levels
of dependencies.
In particular, meta-learning has shown strong performance in one-shot or low-shot learning tasks,
due to its ability to directly generate weights for the base neural network (Woodward & Finn.
(2017)). Meta learning with LSTM learners (Ravi & Larochelle. (2017); Munkhdalai & Yu (2017))
and Model-Agnostic learners (Mishra et al. (2017); Finn et al. (2017b;a)) both achieve SOA perfor-
mance. Additionally, memory-augmented meta-learning (Santoro et al. (2016); Kaiser et al. (2017);
Vinyals et al. (2016); Munkhdalai & Yu (2017)) also produce superior result on low-shot bench-
marks.
The most related work might be (Munkhdalai & Yu (2017)), where slow/fast weights are also pro-
posed. The main differences are: (1) our model applies lifelong slow learners with parameter shar-
ing; (2) for a specific parameter, we apply either a slow or a fast learner; in contrast, both learners
are applied and combined in (Munkhdalai & Yu (2017)). Our model is closely related to MAML
(Finn et al. (2017a)) where a good initialized representation is learned and shared across tasks to
adapt to new domains.
Meta-learning has also shown promising performance on various tasks such as reinforcement learn-
ing (Wang et al. (2017); Duan et al. (2017); Finn et al. (2017b)). It can be even generalized to the
scenario where gradients are not available (Chen et al. (2017)).
3	Meta-Learning with Slow/Fast Learners
Following (Andrychowicz et al. (2016)), we briefly introduce the notations. Suppose we try to
optimize our “optimizee” f(θ, θ0), where θ0 and θ stand for different modules in f. In our paper we
focus on f as a CNN, where θ0 and θ are bottom convolutional and top fully-connected modules.
We aim to train a pair of optimizers θ* (f, φ) and θ0 (f, φ0) parametrized by φ and φ0. Then, given a
distribution of task functions f, the expected loss can be formulated as:
L(φ,φ0)= Ef {f [θ*(f,φ),θ0*(f,φ0)]}	⑵
2
Under review as a conference paper at ICLR 2019
Figure 1: The computational graph of our fast/slow learning model: to optimize the cost function
f (θ, θ0), we apply different strategies on θ, θ0 separately. We leverage a lifelong-slow learning
scheme (SGD) on θ0 to acquire generic knowledge and a fast learning scheme g(.) on task-specific
θ. [Photo: inspired by (Andrychowicz et al. (2016))]
Figure 2: Decoupling the optimization of a CNN into the slow module (generic bottom convolutional
layers parametrized by θ0 in the red dashed box) and the fast module (top FC layers parametrized by
θ in the blue dashed box).
We sketch our algorithm in Table.1. A typical setup of our meta-optimizer on a CNN is shown
in Figure 2: the modules are decoupled into bottom convolutional θ0 = {Wconv , ...} and fully-
connected ones θ = {Wfc, ...}. We apply a lifelong-slow strategy on the former, and fast meta-
learning on the latter separately. The intuition is that, the more “knowledgeable” θ0 gets, the better
the fast meta-learner adapts to a new concept θ . Next, we will carry out a case study on a synthetic
shallow network to demonstrate why our approach works.
3.1	A Case Study on Two-Layer MLP
As shown in Figure 3, we carry out a case study on a two-layer linear neural network with an L2
regression loss. Bottom and top linear modules are parametrized by θ = {Wf } and θ0 = {Ws }.
We assume that the input X ∈ Rd observes an i.i.d. GauSSian distribution X 〜N(0, Σx) with Σx
as a positive-definite matrix; the output y is a scalar. Each time, we create a new linear system task
Figure 3: Case study: a two-layer linear MLP with two FC layers.
3
Under review as a conference paper at ICLR 2019
Proposed Fast/Slow Meta Learning Algorithm:
Input: a general learning task f (.; θ, θ0) parametrized by θ and θ0 With loss function L
Initialize the slow parameters once θ0 ;
For epoch i = 1...n:
Sample a neW learning task fi ∈ f ; Reinitialize the task-specific parameters θ;
For j = 1...m iterations:
1.	Sample a small training batch {Xj , Yj };
2.	Calculate batch loss and gradient:
∂Lj ∂Lj
{Lj, ~θθ~，∂θ0~ };
3.	SloW and fast module update:
θj+1 = θj + g( ∂θ * l * * * S(θj ,θj ),φ)
θj + 1 = θj - δ ∂L l(θj + 1,θj)
Update the parameter φ in fast learner g(.)ιφi+ι = φi — δ d "φi (φ)
Table 1: The proposed Fast/SloW Meta-Learning model. The generic sloW modules θ0 is only ini-
tialized once and shared across tasks. During testing, We fix the parameter θ0 and {φ, φ0}, and apply
the learned fast-learner g(., φ) to update θ.
by sampling a new W ∈ R1×d as our ground truth. We aim to predict {Wf, Ws} from noisy labels
y = WX + e, where E is a Gaussian noise with zero mean.
With batch size k, we denote the inputs and outputs as X = [xι, x2,…，Xk] and Y = WX + E =
[y1, ..., yk]. Then, denoting the parameters of first and second linear modules as Ws ∈ Rd×d and
Wf ∈ R1×d, our regression task can be formulated as:
L = 1[2 X(Wf Wsxi - yi)2]
i
and ideally the miracle solution Ws and Wf satisfying:
Wf = W W-I
could be learned on a new task W as soon as possible. At convergence, we should have:
Wf Ws = W = (XX T )-1XY T = Σ-1XY T
which is an second-order optimizer. If we are constrained to apply a first-order optimizer (SGD)
to optimize Ws and Wf , the solution undergoes a zig-zag path and takes many steps to converge,
depending on the conditioned number of the covariance matrix Σx .
3.1.1	A PERFECT Ws MAKES Wf CONVERGE WITHIN 1-STEP
To simplify notations, we denote S = WsT Ws for all the following analysis.
Lemma 1 If the following condition satisfies:
S (X Σ-1
then suppose we initialize Wf with 0, applying one-step SGD on Wf could make the optimization
converge s.t. Wf WS = W.
Proof 1 The gradient of L with respect toWf is:
∂Wf =1 X(WfWSXi-yi)χT WS
fi
If we initialize with Wf = 0, we have:
∂L
∂Wf
X
-E{XTWsTy}
-E{WxxtWT}	(∙; y = WX + E)
-E{W∑WT } = -E{W∑WT WsW-1}
-E{WW-1}	f; S = WTWs X Σ-1)
4
Under review as a conference paper at ICLR 2019
The direction ofthe gradient perfectly aligns with the ground truth solution Wf = W W-1. With an
appropriate choice of learning rate for Wf, SGD makes Wf converge in one step.
We can interpret Lemma 1 as: ifwe have a miracle representation h = W； X asa whitening operator,
the top module Wf can be learned by SGD within one step, regardless ofthe task target W.
3.1.2 FAST/SLOW META-LEARNING MAKES Ws CONVERGE TO MIRACLE
Since a miracle Ws (i.e., Ws is the whitening matrix of input x) makes Wf adapt to new tasks, it
is of interest if We could learn the miracle W；? Next, We demonstrate that the proposed fast/slow
meta-learning model could converge to the good solution.
FolloWing previous analysis, the gradient of our top module Wf With respect to loss L given a batch
{X1 , Y1 } of batch size k is:
∂Wf = 1(WfWSXI- YI)XTWT
Suppose We alWays initialize Wf = 0. Then suppose the fast-learner is a SGD With learning rate η,
We have the updated Wf as:
Wf = ηY1XT WT	(3)
k
To improve generality, We ideally expect the neW Wf perform Well on a neW sampled batch
{X2,Y2}:
L = 2(Wf WsX2 - Y2)2
=1(ηY1XTWTWSX2 - Y2)2	(∙.∙ Eqn(3))
2k	s
=1(ηY1XTSX2 — Y2)2	(； WTWS = S)
2k
Lemma 2 We denote A = E(W WT) as the expectation of weight covariance matrix across tasks.
If we follow the fast/lifelong-slow meta-learning strategy, at convergence S = WST WS satisfies:
ST= η(k +1)(Σχ + tr(*x+A) + a AT)	(4)
k+1
where k is the batch size of {X2 , Y2 }.
Proof 2
∂L	η
E()=ηE{X1 YiT(ηYιXTSX2 - WX2 - e)χT}
∂S	k
=ηE{X1 Yr(ηYιXTS - W)X2XT}
k
=ηE{X1 (XTWT + e)(η(WXi + e)XTS - W)}Σχ
k
2
=nE{X1(XTWTWX + e2)XT)SΣx} - nE{X1XTWTW}Σx
k	i	i	ki
=η((k + 1)ΣxAΣx) + (tr(ΣxA) + 2)Σx)SΣx - ηΣxAΣx	(Lemma - 3)
=ηΣxA(η((k + 1)Σx + (tr(ΣxA) + 2)A-i)S - I)Σx
At convergence, an equilibrium should be reached where E{IdL } = 0, then:
ST = η(k + 1)(Σχ + *+A) + 心 AT)
k+1
In the proof, We need the folloWing lemma:
5
Under review as a conference paper at ICLR 2019
Lemma 3 If X 〜N (0), then for a matrix A, we have:
E(xxT AxxT) = ΣxAΣx + ΣxATΣx + tr(ΣxA)Σx	(5)
and if X = [xι, X2,…]with each column SamPledfrom an i.i.d. Gaussian Xi 〜 N(0, Σχ), then:
E(XXT AXXT) = k(k+ 1)ΣxAΣx +ΣxATΣx + ktr(ΣxA)Σx	(6)
Proof 3 The moment-generating function of X is φ(t) = exp(t' I^x), then for index {i,j, k,l} we
have:	∂4φ(t) E(Xixjxk Xl)= ∂ti∂tj ∂tk ∂tι = σiσj σk σl	⑺
Hence,
E(XXT AXXT) =	E(xixkAklxlxj)
k,l
Ak,l (σi,j σk,l + σi,kσj,l + σi,lσj,k)
k,l
=[tr(ΣxA)Σx]i,j + [ΣxAΣx]i,j+[ΣxATΣx]i,j
and
k	k	kk
E(XXTAXXT)=(XXiXiT)A(XXjXjT)=XXXiXiTAXjXjT
i=1	j=1	i=1 j=1
=k(k - 1)ΣxAΣx + 2kΣxAΣx + ktr(ΣxA)Σx
=k(k + 1)ΣxAΣx + ktr(ΣxA)Σx
3.2 The Insight of the Case-Study
The above analysis provides why the fast/slow meta-learning works:
1.	A miracle knowledge of input signal x (i.e., Ws whitens x), makes top module Wf learn-
able in one SGD step for a new task; (Lemma 1)
2.	Alternating lifelong-slow Ws and fast Wf learning, we are able to learn Ws close to the
miracle knowledge of x with a regularization term. (Lemma 2)
4	Experiment
We evaluate our algorithm on three datasets: synthetic two-layer neural network, the Omniglot low-
shot benchmark and the large-scale ImageNet dataset.
4.1	Synthetic Dataset
Our first experiment is to verify our theoretical analysis on the two-layer MLP as mentioned in the
last section. We set the dimension as d = 10, such that input data X ∈ R10, each sampled new task
has a ground truth WW ∈ R1×10, Ws ∈ R10×10 and Wf ∈ R1×10. The input X is sampled from an
i.i.d. Gaussian X 〜 N(0, Σχ) with Σχ fixed across all tasks; each new task is sampled from another
Gaussian W 〜N(0,I). For each task, we set the batch size as k = 100.
For the slow learner g0(.), we use a conservative SGD with the learning rate δ = .001; for the fast
learner g(.), we use a large-step SGD with a learning rate η = 0.2. In Figure 4, we plot the learning-
curve of the regression problem. As shown in Figure 4 (a), we show the performance gain defined
as the relative error of 1-step SGD update: L1 = ||Y-WsWf,1Xb. We can see that as the slow
L0	||Y -Ws Wf,0 X ||
learner gradually captures the distribution of input signal x, the fast learner could be very efficient
(L1 /L0 decreases from 0.8 to 0). In Figure 4 (b), we evaluate how well Ws captures the knowledge
of x throughout our fast/slow meta learning process. The metric we use is the Frobenius-norm
between WsTWs and the true whitening matrix: ||(WsTWs) - 1 ∑-1∣∣Frob We can see clearly that
6
Under review as a conference paper at ICLR 2019
(a) Relative Training Loss of 1-Step Fast Up- (b) Estimation Loss bottom module between the learned
date	and optimal Ws
Figure 4: Experiments on the two-layer linear MLP. (a) x-axis: fast/slow training epochs; vertical-
axis: Relative loss L(Wf,1)/L(Wf,0) after one-step fast module update with a large learning rate
η = 0.2; (b) x-axis: fast/slow training epochs; vertical-axis: Frobenius-norm of the estimation
difference between slow module θ0 = {Ws} and the true whitening precision matrix Σx-1.
WTWs approach the true precision matrix Σ-1 /η, i.e., the slow learner evolves to capture the input
distribution “magically” after 100,000 iterations.
Summary: The experiments aligns perfectly with our theoretical analysis in above sections: the
combination of a slow learner and a fast learner achieves state-of-the-art performance.
4.2	Visual Recognition
We briefly introduce the setup of our fast/slow meta-learning on visual classification tasks. We
divide the datasets into 3-split: on Cbase classes, we first pre-train a base CNN f (θ, θ0) ; then on
Cmeta, we train our fast meta-learner g(φ) to generate θ and fine-tune our lifelong slow parameters
θ0 with a slow-learner (SGD); on Ctest we test our model. For our fast learner g(., φ), we use a
coordinate-wise LSTM with 10-steps.
To evaluate our approach in comparison to prior meta-learning and low-shot visual algorithms, we
apply our algorithm on the Omniglot benchmark (Lake et al. (2011)) and MiniImageNet, which are
most common recently-used low-shot benchmarks. Omniglot contains Cbase = 964 characters in
the training set, and we divide the 1,200 categories in the testing set into Cmeta = 600 and Ctest =
600. The MiniImagenet task proposed by (Ravi & Larochelle. (2017)) contains 100 categories with
Cbase = 64, Cmeta = 16 and Ctest = 20 respectively. We follow the experimental protocol of
(Vinyals et al. (2016)) such as multiple 90 degree rotation for Omniglot.
For a fair comparison, we follow the same model architecture as in Matching Network (Vinyals
et al. (2016)) and MAML (Finn et al. (2017a)). For Omniglot, we use a simple yet powerful CNN,
containing 4 modules with 3 × 3 convolution with 64 filters followed by batch-normalization (Ioffe
& Szegedy (2015)), ReLU and 2 × 2 max-pooling. For MiniImagenet, we use 32 filters per layer to
avoid overfitting as done in (Ravi & Larochelle. (2017); Finn et al. (2017a)). A top fully-connected
layer θ = {Wfc} followed by a softmax non-linearity is applied to define the Baseline Classifier.
The performance comparison of different approaches is shown in Table 2. Our approach is highly
competitive with the State-Of-the-Art (SOA) on both Omniglot and MiniImagenet.
FullImageNet:
Finally, it is of interest to test how our algorithm combined with SOA powerful CNN on the most
challenging real-life scenarios- Full ImageNet. We use the InCePtion-ResNet-V2 model (Szegedy
et al. (2017)) as our base model. The task contains 21,841 categories (non-empty synsets) and the
total number of images up to 14,197,122. Generally, the classification performance is tested on the
1,000 base classes, with 1,280,000 images. We adopt as our base CNN model and test our algorithm
in the one-shot scenario on the remaining 20,841 unseen categories. We use half of the data for
meta-learning training and the other half for testing.
7
Under review as a conference paper at ICLR 2019
Model	Omniglot (20-way, 1-shot)	MiniImagenet (5-way, 1-shot)
PiXeI-KNN	26.7%	-
MANN (Santoro et al. (2016))	36.4%	-
CNN (OUr implementation)	85.0%	28.8%
Siamese (Koch et al. (2015))	88.1%	40.1%
Feature Shrinking (Hariharan & Girshick (2017))	89.5%	44.0%
Matching Network (Vinyals et al. (2016))	93.8%	46.6%
Learning to remember (Kaiser et al. (2017))	95.0%	-
MAML (Finn et al. (2017a))	95.8%	48.7%
Prototypical Net (Snell et al. (2017)	96.0%	-
Meta Networks (Munkhdalai & Yu (2017))	97.0%	-
Temporal Conv (Mishra et al. (2017))	97.6%	-
Fast/SloW (Ours)	96.8%	一	48.8%	一
Table 2: Performance comparison of our algorithms and the state-of-the-art on Omniglot and Mini-
ImageNet.
ΓDOV UO4BU≡SSB-ɔ 4OIIS—I
Figure 5: FullImageNet: 1-shot Classification Accuracy w.r.t. class number. Red line: our meta-
learning; blue line: CNN metric-learning.
As shown in Figure 5, we plot the 1-shot classification accuracy on the novel classes with respect to
class number (for example, |class| = 10 means we carries out a 1-shot 10-way task). We can see
our algorithm (red) steadily outperforms the strong baseline (CNN with metric learning).
5	Conclusion
In this paper, we propose a fast/slow meta-learning approach to optimize a base convolutional neu-
ral network. By applying different strategies on bottom-generic/top-task-specific modules, our ap-
proach further improve the meta-learning performance. Theoretical analysis on a two-layer MLP
regression problem, and extensive experiments on the synthetic, Omniglot one-shot, as well as Ima-
geNet benchmarks validate the effectiveness of our approach.
6	Limitations and Future Work
A strong assumption made in our theoretical analysis is that the input data observes the same dis-
tribution across tasks X 〜N(0, Σ). However, this does not necessarily hold true. In case the data
from some new categories observe a different distribution, we might further extend our model to a
hierarchy:
Xi 〜Pz(θ0)	Z ∈ {1,…,m}
where we assume the input signal xi observes a mixture model and the latent variable z could be
regarded as part of the slow module z ∈ θ0 . In the future, we will explore how to extend our model
to the scenario of a mixture model.
8
Under review as a conference paper at ICLR 2019
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom
Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. NIPS,
2016.
Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Fac-
tors of transferability for a generic convnet representation. TPAMI, 2016.
Jonathan Baxter. Theoretical models of learning to learn. Learning to learn. Springer US, 1998.
Adriana Briscoe and Lars Chittka. The evolution of color vision in insects. Annual review of
entomology, 46(1):471-510, 2001.
Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha DeniL Timothy P. Lilli-
crap, Matt Botvinick, and Nando Freitas. Learning to learn without gradient descent by gradient
descent. ICML, 2017.
Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2: Fast
reinforcement learning via slow reinforcement learning. arxiv, 2017.
David Silver et.al. Mastering the game of go with deep neural networks and tree search. Nature,
2016.
Volodymyr Mnih et.al. Playing atari with deep reinforcement learning. NIPS Workshop, 2013.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. ICML, 2017a.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imita-
tion learning via meta-learning. CoLR, 2017b.
Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating
features. ICCV, 2017.
Harry Harlow. The formation of learning sets. Psychological review, 56(1):51, 1949.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CVPR, 2016.
Sepp Hochreiter, Steven Younger, and Peter Conwell. Learning to learn using gradient descent. In
Artificial Neural Networks ICANN, pp. 87-94, 2001.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. ICML, 2015.
Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. learning to remember rare events.
ICLR, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2014.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. ICML Workshop, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In NIPS, pp. 1097-1105, 2012.
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning
of simple visual concepts. In CogSci, 2011.
Ke Li and Jitendra Malik. Learning to optimize. ICLR, 2017.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal
convolutions. arxiv, 2017.
9
Under review as a conference paper at ICLR 2019
Tom Mitchell and Sebastian Thrun. Explanation-based neural network learning for robot control.
NIPS, 1993.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. ICML, 2017.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR, 2017.
Olga Russakovsky and et.al. Imagenet large scale visual recognition challenge. IJCV, 115(3), 2015.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-
shot learning with memory-augmented neural networks. ICML, 2016.
Jurgen Schmidhuber. A neural network that embeds its own meta-levels. ICNN, pp. 407-412, 1993.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. NIPS,
2017.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. AAAI, 2017.
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18(2):77-95, 2002.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. NIPS, 2016.
Jane Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles
Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arxiv, 2017.
Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio Gomez Colmenarejo,
Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and
generalize. ICML, 2017.
Mark Woodward and Chelsea Finn. Active one-shot learning. NIPS Deep Reinforcement Learning
Workshop, 2017.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? NIPS, 2014.
10