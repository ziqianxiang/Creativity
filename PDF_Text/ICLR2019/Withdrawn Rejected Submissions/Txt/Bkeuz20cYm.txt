Under review as a conference paper at ICLR 2019
Double Neural Counterfactual Regret
Minimization
Anonymous authors
Paper under double-blind review
Ab stract
Counterfactual Regret Minimization (CRF) is a fundamental and effective
technique for solving Imperfect Information Games (IIG). However, the original
CRF algorithm only works for discrete state and action spaces, and the resulting
strategy is maintained as a tabular representation. Such tabular representation
limits the method from being directly applied to large games and continuing
to improve from a poor strategy profile. In this paper, we propose a double
neural representation for the imperfect information games, where one neural
network represents the cumulative regret, and the other represents the average
strategy. Furthermore, we adopt the counterfactual regret minimization algorithm
to optimize this double neural representation. To make neural learning efficient,
we also developed several novel techniques including a robust sampling method,
mini-batch Monte Carlo Counterfactual Regret Minimization (MCCFR) and
Monte Carlo Counterfactual Regret Minimization Plus (MCCFR+) which may
be of independent interests. Experimentally, we demonstrate that the proposed
double neural algorithm converges significantly better than the reinforcement
learning counterpart.
1	Introduction
In Imperfect Information Games (IIG), a player only has partial access to the knowledge of her
opponents before making a decision. This is similar to real-world scenarios, such as trading, traffic
routing, and public auction. Thus designing methods for solving IIG is of great economic and
societal benefits. Due to the hidden information, a player has to reason under the uncertainty about
her opponents’ information, and she also needs to act so as to take advantage of her opponents’
uncertainty about her own information.
Nash equilibrium is a typical solution concept for a two-player extensive-form game. Many
algorithms have been designed over years to approximately find Nash equilibrium for large games.
One of the most effective approaches is CFR (Zinkevich et al., 2007). In this algorithm, the authors
proposed to minimize overall counterfactual regret and prove that the average of the strategies in all
iterations would converge to a Nash equilibrium. However, the original CFR only works for discrete
state and action spaces, and the resulting strategy is maintained as a tabular representation. Such
tabular representation limits the method from being directly applied to large games and continuing
to improve if starting from a poor strategy profile.
To alleviate CFR’s large memory requirement in large games such as heads-up no-limit Texas
Hold’em, Moravcik et al. (2017) proposed a seminal approach called DeepStack which uses fully
connected neural networks to represent players counterfactual values and obtain a strategy online as
requested. However, the strategy is still represented as a tabular form and the quality of this solution
depends a lot on the initial quality of the counterfactual network. Furthermore, the counterfactual
network is estimated separately, and it is not easy to continue improving both counterfactual network
and the tabular strategy profile in an end-to-end optimization framework.
Heinrich et al. (2015); Heinrich & Silver (2016) proposed end-to-end fictitious self-play approaches
(XFP and NFSP respectively) to learn the approximate Nash equilibrium with deep reinforcement
learning. In a fictitious play model, strategies are represented as neural networks and the strategies
are updated by selecting the best responses to their opponents’ average strategies. This approach is
advantageous in the sense that the approach does not rely on abstracting the game, and in theory, the
strategy should continually improve as the algorithm iterates more steps. However, these methods
1
Under review as a conference paper at ICLR 2019
do not explicitly take into account the hidden information in a game, because they are optimized
based on the transition memory and the reward of the intermediate node is the utility of the game
rather than the counterfactual value which consider the distribution of hidden variables (opponent’s
private information). In experiments for games such as Leduc Hold’em, these methods converge
slower than tabular based counterfactual regret minimization algorithms. Waugh et al. (2015) used
handcraft features of the information sets to estimates the counterfactual regret. However, it need
traverse the full game tree which is infeasible in large games.
Thus it remains an open question whether the purely neural-based end-to-end approach can achieve
comparable performance to tabular based CFR approach. In the paper, we partially resolve this
open question by designing a double neural counterfactual regret minimization algorithm which can
match the performance of tabular based counterfactual regret minimization algorithm. We employed
two neural networks, one for the cumulative regret, and the other for the average strategy. We show
that careful algorithm design allows these two networks to track the cumulative regret and average
strategy respectively, resulting in a converging neural strategy. Furthermore, in order to improve the
convergence of the neural algorithm, we also developed a new sampling technique which has lower
variance than the outcome sampling, while being more memory efficient than the external sampling.
In experiments with One-card poker and a large Leduc Hold’em containing more than 107 nodes,
we showed that the proposed double neural algorithm has a strong generalization and compression
ability even though only a small proportion of nodes are visited in each iteration. In addition, this
method can converge to comparable results produced by its tabular counterpart while performing
much better than deep reinforcement learning method. The current results open up the possibility
for a purely neural approach to directly solve large IIG.
2	Background
In this section, we will introduce some background on IIG and existing approaches to solve them.
2.1	Representation of Extensive-Form Game
We define the components of an extensive-form game following Osborne & Ariel (1994) (page
200 〜201). A finite set N = {0,1,..., n - 1} of players. Define h as the hidden variable of
player i in IIG, e.g., in poker game hiv refers to the private cards of player i. H refers to a finite
set of histories. Each member h = (hiv)i=0,1,...,n-1(al)l=0,...,L-1 = hv0 hv1 ...hvn-1a0a1...aL-1
of H denotes a possible history (or state), which consists of each player’s hidden variable and L
actions taken by players including chance. For player i, h also can be denoted as hivhv-ia0a1...aL-1,
where h- refers to the opponent's hidden variables. The empty sequence 0 is a member of H.
hj v h denotes hj is a prefix of h, where hj = (hiv)i=0,1,...,n-1(al)l=1,...,L0-1 and 0 < L0 < L.
Z ⊆ H denotes the terminal histories and any member z ∈ Z is not a prefix of any other sequences.
A(h) = {a : ha ∈ H} is the set of available actions after non-terminal history h ∈ H\ Z. A player
function P assigns a member of N ∪ {c} to each non-terminal history, where c denotes the chance
player id, which usually is -1. P(h) is the player who takes an action after history h. Ii ofa history
{h ∈ H : P (h) = i} is an information partition of player i. A set Ii ∈ Ii is an information set
of player i and Ii(h) refers to information set Ii at state h. Generally, Ii could only remember the
information observed by player i including player i0 s hidden variable and public actions. Therefore
Ii indicates a sequence in IIG, i.e., hiva0a2...aL-1. For Ii ∈ Ii we denote by A(Ii) the set A(h)
and by P(Ii) the player P(h) for any h ∈ Ii. For each player i ∈ N a utility function ui (z) define
the payoff of the terminal state z. A more detailed explanation of these notations and definitions is
presented in section B.
2.2	Strategy and Nash equilibrium
A strategy profile σ = {σ∕σi ∈ ∑i,i ∈ N} is a collection of strategies for all players, where
Σi is the set of all possible strategies for player i. σ-i refers to strategy of all players other than
player i. For play i ∈ N the strategy σi(Ii) is a function, which assigns an action distribution over
A(Ii) to information set Ii. σi (a∖h) denotes the probability of action a taken by player i ∈ N ∪{c}
at state h. In IIG, ∀h1, h2 ∈ Ii , we have Ii = Ii(h1) = Ii(h2), σi(Ii) = σi(h1) = σi(h2),
σi(a∖Ii) = σi(a∖h1) = σi(a∖h2). For iterative method such as CFR, σt refers to the strategy profile
at t-th iteration. The state reach probability of history h is denoted by πσ(h) if players take actions
according to σ. For an empty sequence πσ(0) = 1. The reach probability can be decomposed into
πσ (h) = Qi∈N∪{c} πiσ (h) = πiσ (h)π-σ i (h) according to each player’s contribution, where πiσ(h) =
Qh0avh,P(h0)=P(h) σi(a∖h0) and π-σi(h) = Qh0avh,P(h0)6=P(h) σ-i(a∖h0). The information set
reach probability of Ii is defined as πσ(Ii) = Ph∈I πσ (h). If h0 v h, the interval state reach
2
Under review as a conference paper at ICLR 2019
probability from state h0 to h is defined as πσ(h0, h), then We have πσ(h0, h) = πσ(h)∕πσ(h0).
πiσ(Ii), π-σi(Ii), πiσ(h0, h), and π-σi(h0, h) are defined similarly.
2.3	Counterfactual Regret Minimization
In large and zero-sum IIG, CFR is proved to be an efficient method to compute Nash equilibrium
(Zinkevich et al., 2007; BroWn & Sandholm, 2017). We present some key ideas of this method as
folloWs.
Lemma 1: The state reach probability of one player is proportional to posterior probability of the
opponent,s hidden variable, i.e.,p(h-∕Ii) α π-i(h), where hV and Ii indicate a particular h. (see
the proof in section G.1)
For player i and strategy profile σ, the counterfactual value (CFV) viσ(h) at state h is define as
viσ(h) = X π-σi(h)πσ(h, z)ui(z) = X πiσ(h,z)u0i(z).
hvz,z∈Z	hvz,z∈Z
(1)
where u0i(z) = π-σ i(z)ui(z) is the expected reward of player i with respective to the approximated
posterior distribution of the opponent’s hidden variable. The action counterfactual value of taking
action a is viσ(a|h) = viσ(ha) and the regret of taking this action is rf (a|h) = vf (a|h) 一 vf (h).
Similarly, the CFV of information set Ii is viσ(Ii) = Ph∈j. viσ(h) and the regret is r(a∣Ii)=
Pz∈Z,havz,h∈Ii πif(ha, z)u0i(z) - Pz∈Z,hvz,h∈Ii πif(h, z)u0i(z). Then the cumulative regret of
action a after T iterations is
T
RT(a∣Ii) = X(viσt (a|Ii) - vf (Ii)) = Ri-1(a∣Ii) + 吸T (a|Ii).
t=1
where Ri0(a|Ii) = 0. Define RiT,+(a|Ii) = max(RiT(a|Ii), 0), the current strategy (or behavior
strategy) at T + 1 iteration will be updated by
σT+ι(a|Ii) = / P≡R‰)	if P…)RT'+(a|Ii) > 0
[∣⅛	otherwise.
The average strategy σiT from iteration 1 to T is defined as:
-τf lr ʌ PL ∏f (Ii)σi(a∣Ii)
σi (叩i) =	ET.
t=1 πif (Ii)
(2)
(3)
(4)
Where πiσt (Ii) denotes the information set reach probability of Ii at t-th iteration and is used to
weight the corresponding current strategy σt(a∣Ii). Define st(a∣Ii) = πf (Ii)σi(a∣Ii) as the
additional numerator in iteration t, then the cumulative numerator can be defined as
T
ST (a|Ii) = X ∏σt (Ii)σt(a∣Ii) = ST-1(a∣Ii) + ST (a∣Ii).
t=1
(5)
Where S0(a|Ii) = 0.
2.4 MONTE CARLO CFR
When solving a game, CFR needs to traverse the entire game tree in each iteration, which will
prevent it from handling large games with limited memory. To address this challenge, Lanctot et al.
(2009) proposed a Monte Carlo CFR to minimize counterfactual regret. Their method can compute
an unbiased estimation of counterfactual value and avoid traversing the entire game tree. Since only
subsets of all information sets are visited in each iteration, this approach requires less memory than
standard CFR.
Define Q = {Q1, Q2, ..., Qm}, where Qj ∈ Z is a block of sampling terminal histories in each
iteration, such that Qj spans the set Z. Generally, different Qj may have an overlap according to
the specify sampling schema. Specifically, in the external sampling and outcome sampling, each
block Qj ∈ Q is a partition of Z. Define qQj as the probability of considering block Qj , where
Pm=I qQj = 1. Define q(z) = Pj.z∈Q. qQj as the probability of considering a particular terminal
history z. Specifically, vanilla CFR is a special case of MCCFR, where Q = {Z} only contain one
block and qQ1 = 1. In outcome sampling, only one trajectory will be sampled, such that ∀Qj ∈ Q,
3
Under review as a conference paper at ICLR 2019
|Qj | = 1 and |Qj | = |Z|. For information set Ii, a sample estimate of counterfactual value is
vσ(IilQj) = Ph∈ii,z∈Qj,hVz q⅛∏-i(z)∏σ(h,z)Ui(z).
Lemma 2: The sampling counterfactual value in MCCFR is the unbiased estimation of actual
CoUnterfactUal value in CFR. Ej〜qQ*(IilQj)]=呜(Ii) (Lemma 1, Lanctot et al. (2009).)
Define σrs as sampling strategy profile, where σirs is the sampling strategy for player i and σ-rsi are
the sampling strategies for players expect i. ParticUlarly, for both external sampling and oUtcome
sampling proposed by (Lanctot et al., 2009), σ-rsi = σ-i. The regret of the sampled action a ∈ A(Ii)
is defined as
rσMIIi)IQj) =	X	πi (ha,z)uis(z) - X	Tn(h,z)urs(Z),	⑹
z∈Qj ,havz,h∈Ii	z∈Qj ,hvz,h∈Ii
where Urs(Z) = Ursz)I is a new utility weighted by σrS.、. The sample estimate for cumulative
i	πiσ (z)	πiσ (z)
regret of action a after T iterations is RT((a∣Ii)∣Qj∙) = RtT-1((a∣Ii)∣Qj) + rσT((a∣Ii)∣Qj) with
R0((a∣Ii)∣Qj )=0.
3 Double Neural Counterfactual Regret Minimization
Figure 1: (A) tabular based CRF and (B) our double neural based CRF framework.
In this section, we will explain our double neural CFR algorithm, where we employ two neural
networks, one for the cumulative regret, and the other for the average strategy. As shown in
Figure 1 (A), standard CFR-family methods such as CFR (Zinkevich et al., 2007), outcome-sampling
MCCFR, external sampling MCCFR (Lanctot et al., 2009), and CFR+ (Tammelin, 2014) need to
use two large tabular-based memories MR and MS to record the cumulative regret and average
strategy for all information sets. Such tabular representation makes these methods difficult to apply
to large extensive-form games with limited time and space (Burch, 2017).
In contrast, we will use two deep neural networks to compute approximate Nash equilibrium of IIG
as shown in Figure 1 (B). Different from NFSP, our method is based on the theory of CFR, where the
first network is used to learn the cumulative regret and the other is to learn the cumulative numerator
of the average strategy profile. With the help of these two networks, we do not need to use two
large tabular-based memories; instead, we rely on the generalization ability of the compact neural
network to produce the cumulative regret and the average strategy. In practice, the proposed double
neural method can achieve a lower exploitability with fewer iterations than NFSP. In addition, we
present experimentally that our double neural CFR can also continually improve after initialization
from a poor tabular strategy.
3.1	Overall Framework
The iterative updates of the CFR algorithm maintain two strategies: the current strategy σit(aIIi),
and the average strategy σi(a∣Ii) for ∀i ∈ N, NIi ∈ Ii, ∀a ∈ A(Ii), ∀t ∈ {1,..., T}. Thus, our two
neural networks are designed to maintain these two strategies in iterative fashion. More specifically,
•	Current strategy. According to Eq. (3), current strategy σt+1(aIIi) is computed by the
cumulative regret Rt(aIIi). We only need to track the numerator in Eq. (3) since the
normalization in the denominator can easily be computed when the strategy is used. Given
information set Ii and action a, we design a neural network RegretSumNetwork(RSN)
R(a, IiIθRt ) to learn Rt(aIIi), where θRt is the parameter in the network at t-th iteration.
As shown Figure 1 (b), define memory MR = {(Ii,riσt((a∣Ii)∣Qj))∣∀i ∈ N,∀a ∈
A(Ii), h ∈ Ii, h v Z, Z ∈ Qj}. Each member of MR is the visited information set Ii and
4
Under review as a conference paper at ICLR 2019
the corresponding regret rσ((a∣Ii)∣Qj), where Qj is the sampled block in t-th iteration.
According to Eq. (2), We can estimate R(a, Ii∣θt+1) using the following optimization:
θ5+1 - argmin	X	^R(a,Ii∣θR)+ rf ((a∣Ii)∣Qj) - R(a,I/θt+1)) .(7)
θt+1	(Ii,rσt ((a∣ii)∣Qj))∈Mr
•	Average Strategy. According to Eq. (4), the approximate Nash equilibrium is the weighted
average of all previous strategies over T iterations. Similar to the cumulative regret, we
employ another deep neural network AvgStrategyNetwork(ASN) to learn the numerator
of the average strategy. Define MS = {(Ii, ∏σ(Ii)σt(a∣Ii))∣∀i ∈ N,∀a ∈ A(Ii),h ∈
Ii, h v z, z ∈ Qj}. Each member of MS is the visited information set Ii and the value of
∏σ (Ii)σi(a∖Ii), where Qj is the sampled block in t-th iteration. Then the parameter θ?1
can estimated by the following optimization:
θ	S+1 -	argmin X	卜(a,Ii∖θs)	+ sti(a∖Ii)	-S(a,Ii\Q))	.	(8)
θS+1	(Ii,st(a∣Ii))∈Ms	'	)
Remark 1: In each iteration, only a small subset of information sets are sampled, which may
lead to the neural networks forgetting values for those unobserved information sets. To address
this problem, we will use the neural network parameters from the previous iteration as the
initialization, which gives an online learning/adaptation flavor to the updates. Furthermore, due to
the generalization ability of the neural networks, even samples from a small number of information
sets are used to update the new neural networks, the newly updated neural networks can produce
very good value for the cumulative regret and the average strategy.
Remark 2: As we increase the number of iterations t, the value of Rit(a∖Ii) will become
increasingly large, which may make neural network difficult to learn. To address this problem,
we will normalize the cumulative regret by a factor of √t to make its range more stable.
This can be understood from the regret bound of online learning. More specifically, let ∆ =
maxii,a,t ∖Rt(a∖Ii) - Rt-1(a∖Ii )∖,∀Ii ∈ I, a ∈ A(Ii),t ∈ {1,...,T}. We have Rt(a∖Ii) ≤
∆,∖A∖t according to the Theorem 6 in (Burch, 2017), where ∖A∖ = maxii∈ι ∖A(Ii)∖. In practice,
we can use the neural network to track Rt(a∖Ii) = Rt(a∖Ii)/√t, and update it by
Rt(a∖Ii) = √τ-1 R√t- 1(a∖Ii) + rσ √a∖Ii), where R0(a∖Ii) = 0.	(9)
tt
Remark 3: The optimization problem for the double neural networks is different from that in
DQN (Mnih et al., 2015). In DQN, the Q-value for the greedy action is used in the update, while in
our setting, we do not use greedy actions. Algorithm E gives further details on how to optimize the
objectives in Eq. (7) and Eq. (8).
Relation between CFR, MCCFR and our double neural method. As shown in Figure 1, these
three methods are based on the CFR framework. The CFR computes counterfactual value and regret
by traversing the entire tree in each iteration, which makes it computationally intensive to be applied
to large games directly. MCCFR samples a subset of information sets and will need less computation
than CFR in each iteration. However, both CFR and MCCFR need two large tabular memories to
save the cumulative regrets and the numerators of the average strategy for all information sets, which
prevents these two methods to be used in large games directly. The proposed neural method keeps
the benefit of MCCFR yet without the need for two large tabular memories.
3.2	Recurrent Neural Network Representation for Information Set
In order to define our R and S network, we need to represent the information set Ii ∈ I in
extensive-form games. In such games, players take action in alternating fashion and each player
makes a decision according to the observed history. Because the action sequences vary in length,
in this paper, we model them with a recurrent neural network and each action in the sequence
corresponds to a cell in RNN. This architecture is different from the one in DeepStack (Moravcik
et al., 2017), which used a fully connected deep neural network to estimate counterfactual value.
Figure 2 (A) provides an illustration of the proposed deep sequential neural network representation
for information sets. Besides the vanilla RNN, there are several variants of more expressive RNNs,
5
Under review as a conference paper at ICLR 2019
Figure 2: (A) the key architecture of the sequential neural networks. (B) an overview of the novel
double neural counterfactual regret minimization method.
such as the GRU (Cho et al., 2014) and LSTM (Hochreiter & Schmidhuber, 1997). In our later
experiments, we will compare these different neural architectures as well as a fully connected
network representation.
Furthermore, different position in the sequence may contribute differently to the decision making,
we will add an attention mechanism (Desimone & Duncan, 1995; Cho et al., 2015) to the RNN
architecture to enhance the representation. For example, the player may need to take a more
aggressive strategy after beneficial public cards are revealed. Thus the information, after the public
cards are revealed may be more important. In practice, we find that the attention mechanism can
help the double neural CFR obtain a better convergence rate. In section D, we will provide more
details on neural network architectures.
3.3	Continual Improvement
With the proposed framework of double neural CFR, it is easy to initialize the neural networks
from an existing strategy profile based on the tabular representation or neural representation. For
information set Ii and action a, in an existing strategy profile, define R0i(a|Ii) as the cumulative
regret and S0 (a|Ii) as the cumulative numerator of average strategy. We can clone the cumulative
regret for all information sets and actions by optimizing
θR — argmin	X	(R(a/|0R)- R0(a∣Ii)) .	(10)
θR i∈N,Ii ∈Ii ,a∈A(Ii)
Similarly, the parameters θS for cloning the cumulative numerator of average strategy can be
optimized in the same way. Based on the learned θR and θS, We can warm start the double neural
networks and continually improve beyond the tabular strategy profile.
Remark: In the large extensive game, the initial strategy is obtained from an abstracted game
which has a manageable number of information sets. The abstracted game is generated by domain
knowledge, such as clustering similar hand strength cards into the same buckets. Once the strategy
of this abstract game is solved, it can be clone according to Eq. (10) and improved continuously
using our double neural CFR framework.
3.4	Overall Algorithm
Algorithm 1 provides a summary of the proposed double neural counterfactual regret minimization
algorithm. In the first iteration, if the system warm starts from tabular based CFR or MCCFR
methods, the techniques in section 3.3 will be used to clone the cumulative regrets and strategy.
If there is no warm start initialization, we can start our algorithm by randomly initializing the
parameters in RSN and ASN at iteration t = 1. Then sampling methods will return the
counterfactual regret and the numerator of average strategy for the sampled information sets in this
iteration, and they will be saved in memories MR and MS respectively. Then these samples will be
6
Under review as a conference paper at ICLR 2019
1
2
3
4
5
6
7
8
9
10
11
12
13
used by the NeuralAgent algorithm from Algorithm 2 to optimize RSN and ASN. Further details for
the sampling methods and the NeuralAgent fitting algorithm will be discussed in the next section.
Algorithm 1: Counterfactual Regret Minimization with Two Deep Neural Networks
Function Agent(T, b):
For t = 1 to T do
if t = 1 and using warm starting then
initialize θR and θS from an existing checkpoint
t <— t + 1	. skip cold starting
else
I initialize θR and θS randomly.
Mr, MS J sampling methods for CFV and average strategy.	. SUCh as AIgorithm3
sum aggregate the value in MR by information set. . according to the Lemma 5 and Equation 12
remove duplicated records in MS .
θR J— NeuraIAgent(R( ∙∣ θR 1), .M R, θR 1, βR)	. Update θR using Algorithm2
θ^S J— NeuralAgent(S(∙∣ΘS- 1), ^MS, θt- 1,βS)	. update θS USingAlgorithm2
return θR, θS
4 Efficient Training
In this section, we will propose two techniques to improve the efficiency of the double neural
method. These techniques can also be used separately in other CFR-based methods.
4.1 Robust Sampling Techniques
In this paper, we proposed a new robust sampling technique which has lower variance than outcome
sampling, while being more memory efficient than the external sampling. In this robust sampling
method, the sampling profile is defined as σrs(k) = (σirs(k), σ-i), where player i will randomly
select k actions according to sampling strategy σirs(k) (Ii) for each information set Ii and other
players will randomly select one action according to strategy σi .
Specifically, if player i randomly selects min(k, |A(Ii)|) actions according to discrete uniform
distribution unif (0, ∣A(Ii)∣) at information set Ii, i.e., σi4 s(k) (a∣Ii) = min AA(Ii)I) ,then
σrs(k)
πi	(Ii )
π
h∈Ii,h0 vh,h0 avh,h0 ∈Ii0
min(k, ∣A(I0)∣)
~∖A^∖~
(11)
and the weighted utility uirs(k) (z) will be a constant number in each iteration, which has a low
variance. In addition, because the weighted utility no longer requires explicit knowledge of the
opponent’s strategy, we can use this sampling method for online regret minimization. For simplicity,
k = max refers to k = maxIi ∈I ∖A(Ii)∖ in the following sections.
Lemma 3: If k = max and ∀i ∈ N, ∀Ii ∈ Ii, ∀a ∈ A(Ii)/丁⑻(a∖Ii)〜unif (0, ∖A(Ii)∖), then
robust sampling is the same as external sampling.
Lemma 4: If k = 1 and σirs(k) = σi , then robust sampling is the same as outcome sampling.
Lemma 3 and Lemma 4 provide the relationship between outcome sampling, external sampling,
and the proposed robust sampling algorithm. The detailed theoretical analysis are presented in
Appendix G.2.
4.2	Mini-batch Techniques
Mini-batch MCCFR: Traditional outcome sampling and external sampling only sample one block
in an iteration and provide an unbiased estimator of origin CFV according to Lemma 2. In this paper,
we present a mini-batch Monte Carlo technique and randomly sample b blocks in one iteration. Let
Qj denote a block of terminals sampled according to the scheme in section 4.1 at j-th time, then
mini-batch CFV with b mini-batches for information set Ii can be defined as
1b
vσ (Ii∖b) = b X
j=1
h∈Ii ,z∈Qj ,hvz
π-i(Z)碟(h,z)ui(Z)
q(Z)
X vσ (IilQj)
j=1	b
(12)
7
Under review as a conference paper at ICLR 2019
Furthermore, We can show that V?(IiIb) is an unbiased estimator of the CoUnterfactUal value of Ii:
Lemma 5: EQj^Robust Sampling [V? (Ii |b)] = viσ (Ii). (see the proof in section G.3) Similarly, the
cumulative mini-batch regret of action a is
RT((a∣Ii)|b) = RTT((a∣Ii)∣b)+ ViσT((a∣Ii)∣b) - V?T(IiIb)	,	(13)
where Ri0((aIIi)Ib) = 0. In practice, mini-batch technique can sample b blocks in parallel and help
MCCFR to converge faster.
Mini-Batch MCCFR+: When optimizing counterfactual regret, CFR+ (Tammelin, 2014)
substitutes the regret-matching algorithm (Hart & Mas-Colell, 2000) with regret-matching+ and can
converge faster than CFR. However, Burch (2017) showed that MCCFR+ actually converge slower
than MCCFR when mini-batch is not used. In our paper, we derive mini-batch version of MCCFR+
which updates cumulative mini-batch regret RT,十 ((a∣Ii) ∣b) up to iteration T by
RT,+ ((a∣Ii)∣b)
∫(ViσT ((a∣Ii)∣b) - V?T (Ii Ib)) +
I(RTT,+ ((a∣Ii)∣b)+ ViσT ((a∣Ii)∣b) - VT (Ii∣b)) +
ifT=0
ifT>0
(14)
where (x)+ = max(x, 0). In practice, we find that mini-batch MCCFR+ converges faster than
mini-batch MCCFR when specifying a suitable mini-batch size.
5 Experiment
The proposed double neural CFR algorithm will be evaluated in the One-Card-Poker game with 5
cards and a large No-Limit Leduc Hold’em (NLLH) with stack size 5, 10, and 15. The largest NLLH
in our experiment has over 2 × 107 states and 3.7 × 106 information sets. We will compare it with
tabular CFR and deep reinforcement learning based method such as NFSP. The experiments show
that the proposed double neural algorithm can converge to comparable results produced by its tabular
counterpart while performing much better than deep reinforcement learning method. With the help
of neural networks, our method has a strong generalization ability to converge to an approximate
Nash equilibrium by using fewer parameters than the number of information sets. The current results
open up the possibility for a purely neural approach to directly solve large IIG. Due to space limit, we
present experimental results for One-Card-Poker and the analysis in section C. The hyperparameters
and setting about the neural networks can be found in section E.
Settings. To simplify the expression, the abbreviations of different methods are defined as follows.
XFP refers to the full-width extensive-form fictitious play method. NFSP refers to the reinforcement
learning based fictitious self-play method. RS-MCCFR refers to the proposed robust sampling
MCCFR. This method with regret matching+ acceleration technique is denoted by RS-MCCFR+.
These methods only containing one neural network are denoted by RS-MCCFR+-RSN and
RS-MCCFR+-ASN respectively. RS-MCCFR+-RSN-ASN refers to the proposed double neural
MCCFR. According to Lemma 3, if k = max, ES-MCCFR is the same with RS-MCCFR. More
specifically, we investigated the following questions.
Is mini-batch sampling helpful? Figure 3(A) presents the convergence curves of the proposed
robust sampling method with k = max under different mini-batch sizes (b=1, 1000, 5000, 10000
respectively). The experimental results show that larger batch sizes generally lead to better strategy
profiles. Furthermore, the convergence for b = 5000 is as good as b = 10000. Thus in the later
experiments, we set the mini-batch size equal to 5000.
Is robust sampling helpful? Figure 3 (B) and (C) presents convergence curves for outcome
sampling, external sampling(k = max) and the proposed robust sampling method under the
different number of sampled actions. The outcome sampling cannot converge to a low exploitability
smaller than 0.1 after 1000 iterations The proposed robust sampling algorithm with k = 1, which
only samples one trajectory like the outcome sampling, can achieve a better strategy profile after
the same number of iterations. With an increasing k, the robust sampling method achieves an even
better convergence rate. Experiment results show k = 3 and 5 have a similar trend with k = max,
which demonstrates that the proposed robust sampling achieves similar strategy profile but requires
less memory than the external sampling. We choose k = 3 for the later experiments in Leduc
Hold’em Poker. Figure 3 (C) presents the results in a different way and displays the relation between
exploitability and the cumulative number of touched nodes. The robust sampling with small k is just
as good as the external sampling while being more memory efficient on the condition that each
algorithm touches the same number of nodes.
8
Under review as a conference paper at ICLR 2019
A
C
exploitability by mini-batch size (k=ma×)
IO0
Figure 3: Comparison of different CFR-family methods in LedUc Hold'em. (A) Performance
of robust sampling with different batch size. (B) Performance of robust sampling with different
parameter k by iteration. (C) Performance by the number of touched node.
A /卡> = ∙≡e <>tNF⅛P ⅛no ɔoub 八euιR 同河ι"
Neural Network Accuracy Analysis
Warm Starting from Different Methods
Figure 4: Performance of different methods in Leduc Hold,em. (A) comparison of NSFP, XFP
and the proposed double neural method. (B) each contribution of RSN and ASN. (C) continue
improvement from tabular based CFR and RS-MCCFR+.
B
How does double neural CRF compare to the tabular counterpart, XFP, and NFSP? To obtain
an approximation of Nash equilibrium, Figure 4(A) demonstrates that NFSP needs 106 iterations
to reach a 0.06-Nash equilibrium, and requires 2 × 105 state-action pair samples and 2 × 106
samples for supervised learning respectively. The XFP needs 103 iterations to obtain the same
exploitability, however, this method is the precursor of NFSP and updated by a tabular based
full-width fictitious play. Our proposed neural method only needs 200 iterations to achieve the same
performance which shows that the proposed double neural algorithm converges significantly better
than the reinforcement learning counterpart. In practice, our double neural method can achieve an
exploitability of 0.02 after 1000 iterations, which is similar to the tabular method.
What is the individual effect of RSN and ASN? Figure 4(B) presents ablation study of the effects
of RSN and ASN network respectively. Both MCCFR+-RSN and MCCFR+-ASN, which only
employ one neural network, perform only slightly better than the double neural method. All the
proposed neural methods can match the performance of the tabular based method.
How well does continual improvement work? In practice, we usually want to continually improve
our strategy profile from an existing checkpoint (Brown & Sandholm, 2016). In the framework
of the proposed neural counterfactual regret minimization algorithm, warm starting is easy and
friendly. Firstly, we employ two neural networks to clone the existing tabular based cumulative
9
Under review as a conference paper at ICLR 2019
regret and the numerator of average strategy by optimizing Eq. (10). Then the double neural methods
can continually improve the tabular based methods. As shown in Figure 4(C), warm start from
either full-width based or sampling based CFR the existing can lead to continual improvements.
Specifically, the first 10 iterations are learned by tabular based CFR and RS-MCCFR+. The
remaining iterations are continually improved by the double neural method, where b = 5000, k =
max.
0.5
0.1
A
1
Figure 5: Performance analysis from different perspectives: (A) Generalization: by observed nodes.
(B) Compression: by embedding size. (C) Large Game: by game size. (D) Architecture: by attention
or not.
Do the neural networks generalize to unseen information sets? To investigate the generalization
ability, we perform the neural CFR with small mini-batch sizes (b=50, 100, 500), where only
3.08%, 5.59%, and 13.06% information sets are observed in each iteration. In all these settings, the
double neural can still converge and arrive at exploitability less than 0.1 within only 1000 iterations
(Figure 5(A)).
Do the neural networks just memorize but not generalize? One indication that the neural
networks are generalizing is that they use much fewer parameters than their tabular counterparts.
We experimented with LSTM plus attention networks, and embedding size of 8 and 16 respectively.
These architectures contain 1048 and 2608 parameters respectively in NLLH(5), both of which
are much less than the tabular memory (more than 104 number here). Note that both these two
embedding sizes still leads to a converging strategy profile as shown in Figure 5(B).
Does neural method converge in the larger game? Figure 5(C) presents the log-log convergence
curve of NLLH with different stack size (5, 10 and 15 respectively). The largest game size contains
over 2 × 107 states and 3.7 × 106 information sets. Let mini-batch size be 500, there are 13.06%,
2.39% and 0.53% information sets that are observed respectively in each iteration. Even though
only a small subset of nodes are sampled, the double neural method can still converge.
Is attention in the neural architecture helpful? Figure 5(D) presents the convergence curves
of several different deep neural architectures, such as a fully connected deep neural network(FC),
LSTM, LSTM plus attention, original RNN plus attention, and GRU plus attention. The recurrent
neural network plus attention helps us obtain better strategies rate than other architectures after
hundreds of iterations.
6 Conclusion and Future Work
In this paper, we present a novel double neural counterfactual regret minimization method to solve
large imperfect information game, which has a strong generalization and compression ability and
can match the performance of tabular based CFR approach. We also developed a new sampling
technique which has lower variance than the outcome sampling, while being more memory efficient
than the external sampling. In the future, we plan to explore much more flexible methods and apply
the double neural method to larger games, such as No-Limit Texas Hold’em.
10
Under review as a conference paper at ICLR 2019
References
Noam Brown and Tuomas Sandholm. Strategy-based warm starting for regret minimization in
games. pp. 432-438. AAAI, 2016.
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats
top professionals. Science, pp. eaao1733, 2017.
Neil Burch. Time and space: Why imperfect information games are hard. PhD thesis, 2017.
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Kyunghyun Cho, Aaron Courville, and Yoshua Bengio. Describing multimedia content using
attention-based encoderdecoder networks. arXiv preprint arXiv:1507.01053, 2015.
Robert Desimone and John Duncan. Neural mechanisms of selective visual attention. Number 18,
pp. 193-222. Annual review of neuroscience, 1995.
Geoffrey J. Gordon. No-regret algorithms for structured prediction problems. Number
CMU-CALD-05-112. CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF
COMPUTER SCIENCE, 2005.
David Harris and Sarah Harris. Digital design and computer architecture (2nd ed.), volume ISBN
978-0-12-394424-5. San Francisco, Calif.: Morgan Kaufmann.
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, (65(5)):1127-1150, 2000.
Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in
imperfect-information games. arXiv preprint arXiv:1603.01121, 2016.
Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games.
pp. 805-813. International Conference on Machine Learning, 2015.
Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Number 8, pp. 1735-1780.
Neural computation, 1997.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Marc Lanctot, Waugh Kevin, Zinkevich Martin, and Michael Bowling. Monte carlo sampling for
regret minimization in extensive games. In Advances in neural information processing systems,
2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Matej Moravcik, Schmid Martin, Burch Neil, Lis Viliam, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, (6337):508-513, 2017.
Martin J. Osborne and Rubinstein Ariel. A course in game theory, volume 1. MIT Press, 1994.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2017.
Finnegan Southey, Michael P. Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse
Billings, and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. arXiv preprint
arXiv:1207.1411, 2012.
Oskari Tammelin. Solving large imperfect information games using cfr+. arXiv preprint, 2014.
11
Under review as a conference paper at ICLR 2019
Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael Bowling. Solving games with
functional regret estimation. In AAAI, volume 15, pp. 2138-2144, 2015.
Martin Zinkevich, Johanson Michael, Bowling Michael, and Carmelo Piccione. Regret minimization
in games with incomplete information. Advances in neural information processing systems, 2007.
12
Under review as a conference paper at ICLR 2019
Appendix A	Game Rules
Leduc Hold’em a two-players IIG of poker, which was first introduced in (Southey et al., 2012). In
Leduc Hold’em, there is a deck of 6 cards comprising two suits of three ranks. The cards are often
denoted by king, queen, and jack. In No-Limit Leduc Hold’em(NLLH), the player may wager any
amount of chips up to a maximum of that player’s remaining stack. There is also no limit on the
number of raises or bets in each betting round. There are two rounds. In the first betting round, each
player is dealt one card from a deck of 6 cards. In the second betting round, a community (or public)
card is revealed from a deck of the remaining 4 cards. In this paper, we use NLLH(x) refer to the
No-Limit Leduc Hold’em, whose stack size is x.
One-Card Poker is a two-players IIG of poker described by (Gordon, 2005). The game rules are
defined as follows. Each player is dealt one card from a deck of X cards. The first player can pass
or bet, If the first player bet, the second player can call or fold. If the first player pass, the second
player can pass or bet. If second player bet, the first player can fold or call. The game ends with two
pass, call, fold. The fold player will lose 1 chips. If the game ended with two passes, the player with
higher card win 1 chips, If the game end with call, the player with higher card win 2 chips.
Appendix B	Definition of Extensive-Form Games
player 0
C
Zl	ft7	工2	Z3	Z4	九8	ZS	z6 z'l	Ufl'7	z'2	z'3 z'4	i½'8	Z 5	z 6
C
z'7	Zi
Z‘9	Z’
Figure 6: Illustration of extensive-form game. The left and right denote two different kinds of dealt
private cards. We use same color other than gray for each state in the same information set. F, C, P,
B refer to fold, call, pass, bet respectively.
B.1 Additional Definitions
For player i, the expected game utility uiσ = Pz∈Z πσ (z)ui (z) of σ is the expected payoff of all
possible terminal nodes. Given a fixed strategy profile σ-i, any strategy σ* = maxσθ∈∑i u(σ0,σ-i)
of player i that achieves maximize payoff against π-σ i is a best response. For two players’
extensive-form games, a Nash equilibrium is a strategy profile σ* = (σ0,σj) such that each
player’s strategy is a best response to the opponent. An -Nash equilibrium is an approximation
of a Nash equilibrium, whose strategy profile σ* satisfies: ∀i ∈ N,心 + e ≥ maxσ0∈∑i uiσi,σ-i).
*	(σa,σ[)
Exploitability of a strategy σi is defined as i (σi) = uiσ - ui	-i . A strategy is unexploitable if
*
i(σi) = 0. In large two player zero-sum games such poker, uiσ is intractable to compute. However,
if the players alternate their positions, the value of a pair of games is zeros, i.e., u0σ* + u1σ* = 0 .
We define the exploitability of strategy profile σ as e(σ) = (u，0,,1) + u0σ0 ,σ1))∕2.
B.2 Explanation by Example
To provide a more detailed explanation, Figure 6 presents an illustration of a partial game tree in
One-Card Poker. In the first tree, two players are dealt (queen, jack) as shown in the left subtree and
(queen, king) as shown in the right subtree. zi denotes terminal node and hi denotes non-terminal
node. There are 19 distinct nodes, corresponding 9 non-terminal nodes including chance h0 and 10
terminal nodes in the left tree. The trajectory from the root to each node is a history of actions. In
an extensive-form game, hi refers to this history. For example, h3 consists of actions 0:Q, 1:J and P.
13
Under review as a conference paper at ICLR 2019
h7 consists of actions 0:Q, 1:J, P and B. h8 consists of actions 0:Q, 1:K, P and B. We have h3 v h7,
A(h7) = {P, B} and P (h3) = 1
In IIG, the private card of player 1 is invisible to player 0, therefore h7 and h8 are actually the same
for player 0. We use information set to denote the set of these undistinguished states. Similarly, h1
and h2 are in the same information set. For the right tree of Figure 6, h03 and h05 are in the same
information set. h04 and h06 are in the same information set.
Generally, any Ii ∈ I could only remember the information observed by player i including player
i0s hidden variable and public actions. For example, the information set of h7 and h8 indicates a
sequence of 0:Q, P, and B. Because h7 and h8 are undistinguished by player 0 in IIG, all the states
have a same strategy. For example, I0 is the information set of h7 and h8, we have I0 = I0 (h7) =
I0(h8), σ0(I0) = σ0(h7) = σ0(h8), σ0(a∣I0) = σ0(a∣h7) = σ0(a∣h8).
Appendix C	Additional Experiment Details
C.1 Feature Encoding of Poker Games
The feature is encoded as following. As shown in the figure 2 (A), for a history h and player
P (h), we use one-hot encoding (Harris & Harris) to represent the observed actions including chance
player. For example, the input feature xl for l-th cell is the concatenation of three one-hot features
including the given private cards, the revealed public cards and current action a. Both the private
cards and public cards are encoded by one-hot technique, where the value in the existing position
is 1 and the others are 0. If there are no public cards, the respective position will be filled with
0. Because the action taking by chance is also a cell in the proposed sequential model. Thus in a
No-Limit poker, such as Leduc Hold’em, action a could be any element in {fold, cumulative spent}∪
{public cards} , where cumulative spent denotes the total chips after making a call or raise. The
length of the encoding vector of action a is the quantities of public cards plus 2, where cumulative
spent is normalized by the stack size.
C.2 Additional Experiment Results
C
RS-M CC FR+-RSN-ASN
performance of double neural methods
Figure 7: Comparison of different CFR-family methods and neural network methods in
One-Card-Poker. (A) Comparison of the robust sampling with different mini-batch size. (B)
Comparison of the outcome sampling and the robust sampling with different sample actions k. (C)
Comparison of tabular based RS-MCCFR+ and the double neural method.
Figure 7 (A) presents the convergence rate for the proposed robust sampling method of different
mini-batch size b = (1, 100, 500, 1000). The experimental results are similar to Leduc Hold’em
poker, larger mini-batch size indicates a better exploitability.
Figure 7 (B) demonstrates that the convergence rate for different sampling methods including
outcome sampling and robust sampling under k = 1, 2. The conclusion is that RS-MCCFR+
14
Under review as a conference paper at ICLR 2019
converges significantly faster than OS-MCCFR+ after touching the same number of nodes.
Experiment results show that k = 1 has a similar trend with k = 2 (external sampling). Because only
one trajectory is sampled, the proposed RS-MCCFR+ will require less memory than the external
sampling.
Figure 7 (C) compares the performance between the tabular method and the double neural method.
Experimental results demonstrate that RS-MCCFR+-RSN-ASN can achieve an exploitability of less
than 0.0004 in One-Card Poker, which matches the performance of the tabular method. For RSN
and ASN, we set neural batch size 4, hidden size 32 and learning rate 0.001.
Appendix D	Details of Recurrent Neural Network
In order to define our R and S network, we need to represent the information set Ii ∈ I in
extensive-form games. In such games, players take action in alternating fashion and each player
makes a decision according to the observed history. In this paper, we model the behavior sequence
as a recurrent neural network and each action in the sequence corresponds to a cell in RNN.
Figure 2 (A) provides an illustration of the proposed deep sequential neural network representation
for information sets.
In standard RNN, the recurrent cell will have a very simple structure, such as a single tanh or sigmoid
layer. Hochreiter & Schmidhuber (1997) proposed a long short-term memory method (LSTM)
with the gating mechanism, which outperforms the standard version and is capable of learning
long-term dependencies. Thus we will use LSTM for the representation. Furthermore, different
position in the sequence may contribute differently to the decision making, we will add an attention
mechanism (Desimone & Duncan, 1995; Cho et al., 2015) to the LSTM architecture to enhance the
representation. For example, the player may need to take a more aggressive strategy after beneficial
public cards are revealed. Thus the information, after the public cards are revealed may be more
important.
More specifically, for l-th cell, define xl as the input vector (which can be either player or chance
actions), eι as the hidden layer embedding, φ* as a general nonlinear function. Each action is
represented by a LSTM cell, which has the ability to remove or add information to the cell state with
three different gates. Define the notation ∙ as element-wise product. The first forgetting gate layer
is defined as glf = φf (wf [xl, el-1]), where [xl, el-1] denotes the concatenation ofxl and el-1. The
second input gate layer decides which values to update and is defined as gli = φi(wi[xl, el-1]). A
nonlinear layer output a vector of new candidate values Cl = φc(wl [χι,eι-ι]) to decide what can
be added to the state. After the forgetting gate and the input gate, the new cell state is updated by
Ci = gf ∙ Cι-ι + gi ∙ Cι. The third output gate is defined as gθ = φo(wo[xι, eι-ι]). Finally, the
updated hidden embedding is eι = g； ∙ φe(Cι). As shown in Figure 2 (A), for each LSTM cell j,
the vector of attention weight is learned by an attention network. Each member in this vector is a
scalar αj = φa(waej). The attention embedding of l-th cell is then defined as ef = Pj=I αj ∙ ej,
which is the summation of the hidden embedding ej and the learned attention weight αj . The final
output of the network is predicted by a value network, which is defined as
yι := f(a, Ii∣θ)= Wyφv(ea) = WyΦv (X φa(w%j) ∙ e) ,	(15)
where θ is the parameters in the defined sequential neural networks. Specifically, φf , φi , φo are
sigmoid functions. φc and φe are hyperbolic tangent functions. φa and φv are rectified linear
functions. The proposed RSN and ASN share the same neural architecture, but use different
parameters. That is R(a,I∕θR) = f(a,I∕θR) and S(a,I∕θS) = f(a,I∕θS). R(∙,I∕θR) and
S(∙, Ii∣θS) denote two vectors of inference value for all a ∈ A(Ii).
Appendix E	Neural Agent for Optimizing Neural Representation
Define βepoch as training epoch, βιr as learning rate, βιoss as the criteria for early stopping,
βre as the upper bound for the number of iterations from getting the minimal loss last time,
θt-1 as the parameter to optimize, f(∙∣θt-1) as the neural network, M as the training sample
consisting information set and the corresponding target. To simplify notations, we use β* to
denote the set of hyperparameters in the proposed deep neural networks. βR and βS refer to the
15
Under review as a conference paper at ICLR 2019
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Algorithm 2: Optimization of Deep Neural Network
Function NeuralAgent(f(∙∣θτT), M, θτT, β*):
initialize optimizer, scheduler	. gradient descent optimizer and learning rate scheduler
θT4—θT 1, lbest4—∞,tbest4—0	. warm starting fromthe checkpoint ofthe last iteration
For t = 1 to βepoch do
loss <- []	. initialize loss as an empty list
For each training epoch do
{x(i), y(i) }m=1 〜^M	. sampling a mini-batch from M
batchdoss ―* Pm=1(f (x(i)∣θT-1) + y⑻-f (x(i)∣θτ))2
back propagation batch joss with learning rate lr
clip gradient of θT to [- , ]d	. d is the dimension of θT
optimizer (batchdoss)
loss.append(bαtchdoss)
lr <- ShedUler (lr)	. reduce learning rate adaptively when loss has stopped improving
if avg(loss) < βloss then
I θbTest《-θT, early stopping.	. if loss is small enough, using early stopping mechanism.
else if avg(loss) < lbest then
I IbeSt = avg(loss), tbest《-t, θbest《-θ
if t - tbest > βre then
I lr《— βir	. reset learning rate to escape from potential saddle point or local minima.
return θT
sets of hyperparameters in RSN and ASN respectively. According to our experiments, we find a
carefully designed optimization method can help us obtain a relatively higher convergence rate of
exploitability. Algorithm 2 presents the details of how to optimize the proposed neural networks.
Both R(a,Ii∣ θR+1) and S (a,Ii∣θS) are optimized by mini-batch stochastic gradient descent method.
In this paper, we use Adam optimizer (Kingma & Ba, 2014) with both momentum and adaptive
learning rate. Some other optimizers such as Nadam, RMSprop, Nadam from (Ruder, 2017) are
also tried in our experiments, however, they do not achieve better experimental results. In practice,
existing optimizers may not return a relatively low enough loss because of potential saddle point or
local minima. To obtain a relatively higher accuracy and lower optimization loss, we use a carefully
designed scheduler to reduce the learning rate when the loss has stopped decrease. Specifically, the
scheduler reads a metrics quantity, e.g, mean squared error, and if no improvement is seen for a
number of epochs, the learning rate is reduced by a factor. In addition, we will reset the learning
rate in both optimizer and scheduler once loss stops decrease in βre epochs. Gradient clipping
mechanism is used to limit the magnitude of the parameter gradient and make optimizer behave
better in the vicinity of steep cliffs. After each epoch, the best parameter will be updated. Early
stopping mechanism is used once the lowest loss is less than the specified criteria βloss .
In experiments, we set the network hyperparameters as follow. For RSN, we set the hyperparameters
as follows: neural batch size is 256 and learning rate βlr = 0.001. A scheduler, who will reduce the
learning rate based on the number of epochs and the convergence rate of loss, help the neural agent
to obtain a high accuracy. The learning rate will be reduced by 0.5 when loss has stopped improving
after 10 epochs. The lower bound on the learning rate of all parameters in this scheduler is 10-6. To
avoid the algorithm converging to potential local minima or saddle point, we will reset the learning
rate to 0.001 and help the optimizer to learn a better performance. θbTest is the best parameters to
achieve the lowest loss after T epochs. If average loss for epoch t is less than the specified criteria
βloss=10-4, we will early stop the optimizer. We set βepoch = 2000 and update the optimizer 2000
maximum epochs. For ASN, we set the loss of early stopping criteria as 10-5. The learning rate
will be reduced by 0.7 when loss has stopped improving after 15 epochs. Other hyperparameters in
ASN are similar to RSN.
16
Under review as a conference paper at ICLR 2019
Appendix F Optimize Counterfactual Regret Minimization with
Two Deep Neural Networks
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
Algorithm 3: Mini-Batch RS-MCCFR with Double Neural Networks
Function Mini-Batch-MCCFR-NN(t):
MR — 0, MS — 0
For all i = 1 to b do in parallel then
MCCFR-NN(t, 0,0,1,1)
MCCFR-NN(t, 0,1,1,1)
return MR, MS
Function MCCFR-NN(t, h, i, πi, πirs(k) ):
Ii — Ii(h)
if h ∈ Z then
information set at state h
I return ∏ih
else if P (h) = -1 then
a 〜σ-i(Ii)
return MCCFR-NN(t, ha, i, πi, ∏is(k)
else if P (h) = i then
R^i(∙∣Ii) 一 R(∙,Ii∣ΘR) if t > 1 else →
σi(Ii) -CalculateStrategy(Ri(IIi)Ji)
vi(h) J 0,ri(IIi) 一 ~,si(∙IIi) - ~
. return game payoff
. Sample an action from σ-i(h)
inference the vector of cumulative regret ∀a ∈ A(Ii)
. calculate current strategy
.ri(∙∣Ii) and i(∙∣Ii) are two vectors over A(Ii)
Ars(k) (Ii) - sampling k different actions according to σirs(k)
For a ∈ Ars(k) (Ii) do
Vi(a∣h) -MCCFR-NN(t, ha, i, ∏iσi(a∣Ii), ∏rsσrs(k)(a∣Ii))
vi (h) - vi (h) + vi (aIh)σi (aIIi )	. update counterfactual value
For a ∈ Ars(k) (Ii) do
ri(a∣Ii) — Vi(a∣h) 一 Vi(h)
_ Si(a∣Ii) - ∏σ(Ii)σi(a∣Ii)
. update cumulative regret
. update average strategy numerator
Store updated cumulative regret tuple (Ii, ri(∙∣Ii)) in MR
Store updated current strategy dictionary (Ii, Si(∙∣Ii)) in MS
else
R-i(∙∣Ii) — R(∙,Ii∣θR) if t > 1 else →	. inference cumulative regret
/ τ ∖	/A ∕lr∖r∖
σ-i(Ii ) --CalCUlateStrategy(R-i(∙∣Ii),Ii)	. calculate current strategy
a 〜 σ-i(Ii)	. Sample an action from σ-i(Ii)
return MCCFR-NN(t, ha, i, πi, πirs(k))
Function CalculateStrategy (Ri(∙∣Ii),Ii):
sum - Pa∈A(Ii) max(Ri(aIIi),0)
For a ∈ A(Ii) do
L σi(a∣Ii) = max(RumIIjO) if sum > 0 else ɪ
return σi(Ii)
Algorithm 3 presents one application scenario of the proposed double neural method, which is based
on the proposed mini-batch robust sampling method. The function MCCFR-NN will traverse the
game tree like tabular MCCFR, which starts from the root history h = 0. Define Ii as the information
set of h. Suppose that player i will sample k actions according to the robust sampling. Then the
function can be defined as follows. (1) If the history is terminal, the function returns the weighted
utility. (2) If the history is the chance player, one action a ∈ A(Ii) will be sampled according to
the strategy σ-i(Ii). Then this action will be added to the history, i.e., h - ha. (3) If P(Ii) = i,
17
Under review as a conference paper at ICLR 2019
the current strategy can be updated by the cumulative regret predicted by RSN. Then we sample k
actions according the specified sampling strategy profile σirs(k) . After a recursive updating, we
can obtain the counterfactual value and regret of each action at Ii . For the visited node, their
counterfactual regrets and numerators of the corresponding average strategy will be stored in MR
and MS respectively. (4) If P (Ii) is the opponent, only one action will be sampled according the
strategy σ-i (Ii).
The function Mini-Batch-MCCFR-NN presents a mini-batch sampling method, where b blocks will
be sampled in parallel. This mini-batch method can help the MCCFR to achieve a more accurate
estimation of CFV. The parallel sampling makes this method efficient in practice.
Appendix G	Theoretical Analysis
G.1 Reach Probability and Posterior Probability
Lemma 1: The state reach probability of one player is proportional to posterior probability of the
opponent's hidden variable, i.e.,p(h-i∣Ii) α ∏-i(h).
Proof: For player i at information set Ii and fixed i0s strategy profile σi, i.e., ∀h ∈ Ii, πiσ (h) is
constant. Based on the defination of extensive-form game in Section 2.1, the cominbation of Ii and
opponent’s hidden state hv-i can indicate a particular history h = hivhv-ia0a1 ...aL-1. With Bayes’
Theorem, we can inference the posterior probability of opponent’s private cards with Equation16
/.v	P(h-i,Ii)	p(h)
p(h-ilIi) = ~^IT =而(Xp(h)
L
X p(hiv)p(hv-i)
σP (hiv hv-i a0 a1 ...al-1)(al|hivhv-ia0a1...al-1)	(16)
l=1
X πσ (h) = πiσ (h)π-σ i(h)
X π-σ i (h)
G.2 Robust Sampling, outcome sampling and external sampling
Lemma 3: If k = maxIi∈i∣A(Ii)| and ∀i ∈ N, ∀Ii ∈ I ∀a ∈ A(Ii)Hs(k)(a|Ii)〜
unif (0, |A(Ii)|), then robust sampling is same with external sampling. (see the proof in section
G.2)
Lemma 4: If k = 1 and σirs(k) = σi, then robust sampling is same with outcome sampling. (see
the proof in section G.2)
For robust sampling, given strategy profile σand the sampled block Qj according to sampling profile
σrs(k) = (σirs(k), σ-i), then q(z) = πiσ rs(k) (z)π-σ i(z), and the regret of action a ∈ Ars(k)(Ii) is
r((a∣Ii)lQj)=谒((a∣Ii)lQj) - V(IiIQj)
= E	-(Z)π-i(z)πi (ha,z)ui(z) - E	-(Z)π-i(z)πi(h,z)Ui(Z)
z ∈Qj ,havz,h∈Ii	z ∈Qj ,hvz
Ui(Z)	σ	Ui(Z)	σ
= 工	πσrs(k) (Z) πi (ha, Z)- 工	πσrs(k) (Z) πi (h, Z)
z ∈Qj ,havz,h∈Ii i	z∈Qj ,hvz,h∈Ii i
= X	πiσ (ha, Z)Uirs(Z) - X	πiσ (h, Z)Uirs(Z),
z ∈Qj ,havz,h∈Ii	z ∈Qj ,hvz,h∈Ii
(17)
where UrS(Z) = -Ui(Z- is the weighted utility according to reach probability ∏	(z). Because
i	πiσ	(z)	i
the weighted utility no long requires explicit knowledge of the opponent’s strategy, we can use this
sampling method for online regret minimization.
18
Under review as a conference paper at ICLR 2019
Generally, if player i randomly selects min(k, |A(Ii)|) actions according to discrete uniform
distribution unif (0, ∣A(Ii)∣) at information set L, i.e., 0：")(。也)=min AA(Ii)I) ,then
σrs(k)
πi	(Ii )
π
h∈Ii,h0 vh,h0 avh,h0 ∈Ii0
min(k, ∣A(I0)∣)
AITl
(18)
and uirs (z) is a constant number when given the sampling profile σrs(k).
Specifically,
•	if k = maxIi∈I lA(Ii)l, then σirs(k)(Ii) = 1, uirs(k) (z) = ui(z), and
rσ ((alIi )lQj) =	E	ui(z)(πσ (ha,z)-n (h,z))
z∈Qj ,hvz,h∈Ii
(19)
Therefore, robust sampling is same with external sampling when k = maxIi∈IlA(Ii)l.
•	if k = 1 and σirs(k) = σi, only one history z is sampled in this case,then uirs(k) (z)
∏Uσi(Z)), ∃h ∈ Ii,fora ∈ ArMk)(Ii)
πi (z)
rσ ((a∣Ii )lQj ) = rσ ((a∣h)Q∙)
= X	πiσ(ha, z)uirs(z) - X	πiσ(h, z)uirs(z)
z∈Qj ,havz	z∈Qj ,hvz
_ (1 - σi(a∣h))ui(Z)
∏σ (ha)
(20)
For a ∈ Ars(k)(Ii), the regret Will be r：((a∣h)∣j) = 0 -可(h|j). Therefore, robust
sampling is same with outcome sampling when k = 1 and σirs(k) = σi.
•	ifk = 1, and player i randomly selects one action according to discrete uniform distribution
Unif (0, ∣A(Ii)I) at information set Ii, then Urs(I)(Z) = —Us(Z- is a constant, ∃h ∈ Ii,
πiσ	(z)
fora ∈ Ars(k)(Ii)
rσ ((a∣Ii)∣Qj )=	X	∏σ (ha,z)urs(z) - X	∏σ (h,z)urs(z)
z∈Qj ,havz,h∈Ii	z∈Qj ,hvz,h∈Ii	(21)
= (1 - σi (alh))πiσ (ha, Z)Uirs(1) (Z)
if action a is not sampled at state h, the regret is r：((a∣h)∣j) = 0 - v：(h∣j). Compared
to outcome sampling, the robust sampling in that case have a lower variance because
of the constant Uirs(1) (Z).
G.3 Mini-Batch MCCFR gives an unbiased estimation of counterfactual value
Lemma 5: EQj〜RObUStSamPling[V：(Ii∣b)] = V(Ii).
19
Under review as a conference paper at ICLR 2019
Proof:
EQj〜RObUStSamPlingi¾ (ʃdb)] =Eb0 〜unif(0,b)[V∙T (Ii∣b0)]
EbO 〜unif(0, b)
X E
j=l h∈Ii,z∈Qj ,hVz
π-i(z)πσ (h,z)Ui(Z)
q(z)b0
Eb0~unif(0, b) ( b E 可(ii∣Qj)
1 ∑( 1E 可(IiIQj)
b0 = 1	j = 1
1E (1EE(竭(IiIQj))
b0=1	j = 1
1E G E 熄(Ii)
b0=1 ∖ j = 1	)
熄(Ii)
(22)
20