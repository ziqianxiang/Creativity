Under review as a conference paper at ICLR 2019
Knowledge Representation for Reinforce-
ment Learning using General Value Functions
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning (RL) is a very powerful approach for learning good con-
trol strategies from data. Value functions are a key concept for reinforcement
learning, as they guide the search for good policies. A lot of effort has been
devoted to designing and improving algorithms for learning value functions. In
this paper, we argue that value functions are also a very natural way of provid-
ing a framework for knowledge representation for reinforcement learning agents.
We show that generalized value functions provide a unifying lens for many algo-
rithms, including policy gradient, successor features, option models and policies,
and other forms of hierarchical reinforcement learning. We also demonstrate the
potential of this representation to provide new, useful algorithms.
1	Introduction
Value functions are at the heart of reinforcement learning algorithms (RL) as a very useful tool
in guiding the progress towards good policies. Usually, a value function estimate, representing
the expected discounted return of a policy, is maintained in order to guide policy improvement.
However, the goal of having realistic AI agents requires us to consider how they may go beyond
the goal of solving one task, to a setting in which such agents can acquire and maintain diverse
knowledge about the world. This brings about the question of how such knowledge should be
expressed and maintained in an RL agent.
In this paper, we highlight the fact that value functions can in fact be used as the main building
block for knowledge representation in RL agents. We build on the framework of General Value
Functions (GVFs) (White, 2015; Sutton & Tanner, 2005; Sutton et al., 2011; Schaul & Ring, 2013),
which have been proposed in prior work as a way to capture knowledge about the world in a flexible
fashion. GVFs represent long-term predictions about different aspects of an agent’s observations.
For example, an RL agent interested in hockey can ask questions like “what is the expected time until
I hit the puck, given my current strategy?” or “what are the chances ofmy team winning this game?”
and represent the answers to these questions as GVFs. This form of knowledge representation
provides two clear benefits. First, since it is grounded in actual observations, it can be learned
incrementally from a single stream of experience, using off-policy learning methods. Second, as
illustrated by the example questions above, it can be used to model the world at different time
scales (Modayil et al., 2014). The Horde architecture (Sutton et al., 2011) illustrates in fact the
successful large-scale learning of GVFs about a diversity of signals and at many different time scales,
in parallel, from a single stream of data. This aspect of GVFs is very appealing, as it allows using
the experience (which may be difficult to acquire) in order to learn many different things. GVFs
also have the advantage of being able to profit from a long history of RL algorithm development for
value function learning (Sutton & Barto, 1998).
Our main contribution in this paper is to unify and interpret several different RL algorithms, includ-
ing policy gradients (Sutton et al., 1999a) and different ways of achieving temporal abstraction, such
as successor features (Barreto et al., 2017), option models and value functions (Sutton et al., 1999b),
feudal networks (Vezhnevets et al., 2017) and universal value functions (Schaul et al., 2015) through
the lens of GVFs. We also explore how GVFs can be used to facilitate the development of new
algorithmic ideas in RL, and we illustrate the benefits of this approach through two algorithms: a
new approach to policy gradient, which produces much more stable and data-efficient convergence,
and a combination of feudal networks and successor features.
1
Under review as a conference paper at ICLR 2019
2	Background
Consider a Markov Decision Process (MDP) with state space S, action space A, and transition
function P(s0|s, a). A task is specified using an extrinsic reward function R : S × A × S → [0, 1],
and a discount value γ ∈ [0, 1], which define the expected discounted return: Eπ[Pt∞=0 γtRt+1] =
Eπ[Pt∞=0(Qit=1 γ)Rt+1], also known as the value function of a given policy π.
General Value Functions (GVFs) Sutton et al. (2011) provide a unified way of expressing predic-
tions about signals other than extrinsic rewards, under policies that are different from the behavior
policy, and under flexible, state-dependent discount schemes. Given a policy π : S × A → [0, 1], a
(possibly multi-dimensional) cumulant function C : S × A × S → RK and a continuation function
γ : S → [0, 1], the associated generalized value function (GVF) vC,γ,π : S → RK predicts the
expected cumulant-based return, discounted by the continuation function:
vC,γ,π (S) = E hP∞=0 (Qt=I Y(SiD CtlSO = s, A0g 〜πi .
Note that this definition extends easily to predictions associated with different initial conditions (e.g.
for pairs (S, a), rather than just states).
Usual value functions are trivially GVFs vR,γ,π , where the cumulant is simply the MDP reward
function, and the discount factor is constant across the state space. Similarly, when the cumulant C
depends on state-action pairs, the corresponding GVF is equivalent to the state action Q-values.
In the following sections, we will explain how we can use GVFs to express different RL algorithms,
gaining insight both into their behavior, as well as into paths for their possible improvement.
3	GVFs for successor features
Successor representation. Under any distribution over the state space, as induced by a fixed pol-
icy, one can define a prediction for the discounted number of times a state S is visited. That is,
we predict the one-hot cumulant I(S) such that Ct = 1 if St = S and 0 otherwise1. For a fixed
starting state s°, the associated prediction defines a measure over the state space S: μ(s∣s0, γ, ∏).
Dayan (1993) introduced the idea of using this representation of states through their possible futures,
called successor representation, as a way to provide ideal features This becomes useful in describing
predictions about other cumulants C: vc,γ,∏ (so) = C Cdμ(∙; s0,γ,π) (see appendix for proof).
Successor features Successor features (SF) (Barreto et al., 2017) generalize the idea of the suc-
cessor representation to the case of function approximation in a natural way. Given a function
φ : S × A × S → RK defining feature representations for s → a → s0 transitions, the SF vector is
defined as
ψ(s, a; π) = E [Pt∞=0γtφ(St,At+1, St+1)|S0 = s, A1 = a,π] .
If the reward function is linear in the features (i.e. R(s, a, s0) = φ(s, a, s0)Tw for some weight vec-
tor w), SFs become very useful in the context of transfer learning, where different tasks correspond
to different w. Once SFs for a given policy are computed, they can be used for zero-shot policy
evaluation, and in generalized policy improvement.
SF (Barreto et al., 2017) are naturally formulated as a GVF vφ,γ,π, where φ is a multi-dimensional
cumulant and γ is constant A key result on SFs Barreto et al. (2017) can then be expressed and
generalised immediately in the language of GVFs as follows:
Proposition 1. For a fixed pair (π, γ), given a cumulant C : S × A × S → Rk, and w ∈ Rk,
vwT C,γ,π(s) = wT vC,γ,π(s).
This property is exploited in Barreto et al. (2017) for transfer learning. Given anew task correspond-
ing to scalar cumulant C0 : S × A → R, the value of any given policy can be evaluated on this new
task by approximating C0 with the L2 projection on the space spanned by a multi-dimensional cumu-
lant C. That is, given wC0 := arg minw(wT C-C0)2, one can estimate vC0,γ,π(s) ≈ wCT 0 vC,γ,π (s).
We will use the GVF version of SF in our experiments.
1Similar one-hot cumulants can be defined for (s, a) pairs or s → a → s0 transitions and derive the
corresponding measure over spaces S × A and S × A × S, respectively.
2
Under review as a conference paper at ICLR 2019
R γ	π(θ)
s,a A	GVF I
▽a log ∏(θ)"∣
× γ	π(θ)
Y
S0 A GVF -)
Ψ
▽aV
(a)	The policy gradient algo-
rithm describes the interdepen-
dence between the GVFs corre-
sponding to the evaluation of a
policy of a specific task (i.e. R)
and the gradient of that evalua-
tion with respect to the policy pa-
rameters (i.e. VθV)
.I Y	∏(θ)
C」7X ψ
so -> GVF ɔ
[wι, W2,…，Wk]	×
a1	1
SF2	W
.	------A ×	≈ V0V
SFk
(b)	The GVFs involved in the computation of policy gradients can
be naturally decoupled by using successor features to approximate
the second GVF. The policy is evaluated on a set of cumulants (i.e.
features) to compute successor features, which are then used to
approximate the gradient Vθv (see Proposition 1).
Figure 1: GVFs for Policy Gradient Methods
4	GVFs for policy gradient methods
Performance gradient estimation techniques of score function and likelihood ratio, as introduced
by Aleksandrov et al. (1968) and Rubinstein (1969), are based on sampling trajectories to estimate
both the value of a policy and the gradient used for improving the policy. Given a score function
f that depends on a random variable sampled from a parameterised distribution q(θ), the gradient
with respect to the parameters is given by VEX〜q(θ)[f (X)] = EX〜q(θ) [f (X)Vθ log q(X； θ)].
This principle is used in several popular RL algorithms, including REINFORCE (Williams, 1992),
actor-critic (Sutton, 1984), policy gradient, eg. (Sutton et al., 1999a; Konda, 2000; Bhatnagar et al.,
2009; Lillicrap et al.; Schulman et al., 2015) and other algorithms that use an estimate of the value
function to compute the gradient of the policy parameters. The policy gradient theorem (Sutton
et al., 1999a) provides a formal description of the procedure used in this broad class of algorithms.
We are now going to re-express this result in the language of GVFs, by re-casting the gradient
computation in terms of an explicit cumulant.
Theorem 1. Let πθ be a policy parameterised by a vector θ, γ be a constant continuation function,
and C : S × A → R be a one-dimensional cumulant. The gradient of the general value function
VC,γ,πθ (s) with respect to θ is itself a general value function that depends on the cumulant:
C(s,a) := vc,γ,∏θ(s,a)Vθ log∏θ(a|s).
That is, vθvC,γ,∏θ (S) = vC,γ,∏θ (S)∙
Figure 1a provides a visual depiction of this result and the appendix contains the proof.
The policy gradient theorem shows that these widely used methods are based on the estimation of
two inter-dependent GVFs: the cumulant for one GVF is obtained as the output of another GVF.
Note that this separation means we can easily consider other ways of estimating these cumulants.
4.1	B ootstrapped policy gradient
To illustrate the intuition that we gain from Theorem 1, we first run a procedure that is designed
to follow the analytic description of the policy gradient: given policy parameters θ, (1) generate a
number of rollouts from random initial states (uniformly sampled over the entire state space); (2)
use TD(λ) on the sampled data to compute an estimate for the value function VR,γ,πθ ; (3) compute
cumulants C in Theorem 1 based on the estimates in (1); and (4) generate a separate set of rollouts
to estimate the GVF VR,γ,πθ, using the same TD(λ) procedure. We use TD(λ) to estimate GVFs
since it has the flexibility of reusing previous estimates (i.e., bootstrapping).
3
Under review as a conference paper at ICLR 2019
(b) Plots corresponding to individual hyper-parameter configurations illustrate the increased stability of the
bootstrapped policy gradient approach when gradients are approximated with successor features.
Figure 2: Policy gradient updates with TD(λ) estimates. L1 norm of the exact value function vs.
number of updates to policy parameters. We ran all versions of the algorithm over different eligibil-
ity trace parameters λ, constant learning rates for the value function αtd , and constant learning rates
αpg for policy parameters. For performance on various hyper-parameter settings, see Figure 6 (in
Appendix). Mean and variance for the value of the policy as a function of policy updates are shown
in blue when only the critic value is bootstrapped, in red when all GVF estimates are bootstrapped,
and green when successor features are also used.
4.2	Using successor features for policy gradient
Theorem 1 provides a natural linear approximation to the gradient Vθv. Given a finite number
of auxiliary predictions (i.e. cumulants {Ci}in=1) and a linear approximation to an input cumulant
C ≈ EWiCi, We can compute VθVC 7 ∏ ≈ EWivCi,γ,∏, using the successor features idea (see
Figure 1b for a corresponding illustration). The intuition is that successor features would help in
tracking the continual change in cumulants that is a natural by-product of policy gradient. This
should help When the gradient estimates are unstable; sampled data Would be used to estimate the
long-term accumulation of a fixed set of features instead of a set of parameter gradients.
4.3	Empirical illustration
In Figure 2a, We illustrate the importance of bootstrapping not only the critic value vC,γ,πθ but also
in any General Value Functions involved in the computation of the gradient With respect to the policy
parameters. All algorithms Were run on a simplified version of the four-room environment described
in (Sutton et al., 1999a), With discount factor set to 0.9, and the feature map used for linear value
function approximation Was computed using tile-coding (Sutton & Barto, 1998) (exact details of
the experiments are in the appendix). Using small domains alloWs us to compute the exact value
functions, and hence to compare the algorithms against ground truth. As can be seen, the proposed
bootstrapped policy gradient approach provides significant improvement in data efficiency. While
the advantage of bootstrapping compared to Monte Carlo estimation has been observed before, the
effect is magnified through the use of multiple GVFs.
4
Under review as a conference paper at ICLR 2019
Figure 3: Actor-Critic with bootstrapped gradient estimates. L1 norm of the exact value function
vs. number of rollouts used for policy updates. Top: four-room maze introduced in (Sutton et al.,
1999a))(≈ 60 states and ≈ 15 total features). Bottom: large maze grid-world with sparse reward
(≈ 3000 states and ≈ 90 total features). Each plot corresponds to a fixed number of steps L taken by
the agent for every rollout, The plots displayed here correspond to the highest performing hyperpa-
rameter setting: αA is the learning rate for updates to the policy parameters, αC is the learning rate
for updates to the critic component, and αG is the learning rate for updates to the gradient estimates.
For performance on specific hyperparameter setting, see Figure 7 and 8 (in Appendix). Mean and
variance for the value of the policy as a function of policy updates are shown in blue for baseline
actor-critic, in green when actor-critic maintains bootstrapped estimates for parameter updates.
We tested the approach with the same tile-coding features as used for value function approximation.
The results shown in Figure 2b demonstrate that estimating policy gradients using bootstrapped
successor features exhibits much stronger robustness to changes in hyper-parameters.
Algorithm 1: Actor-Critic with GVFs for gradient estimation.
Input: policy π(a∣s, θ); critic state-value fn. v(s, w); general-value fn. g(s, η).
Algorithm parameters: step size αθ > 0, αw > 0, and αη > 0.
repeat
Initialize S (first state of episode); I J 1
repeat
A 〜 π(a∣s, θ) and take action A, observe S0, R
δcrit J R + Yv(S0, w) — v(S, w)
W J W + αwIδcritVwv(S, w)
δgrad J δcritvθ log π(A|S, θ) + Yg(S0, η) - g(S, η)
η J η + αηiδgradVηg(S, η)
θ J θ + αθg(S, η)
I	J YI and S J YS0
until end of episode
until convergence
Next, we tested the approach in the context of Actor-Critic (Konda, 2000), an approach commonly
used to scale the policy gradient algorithm to large or continuous environments. We modified the
actor-critic algorithm in (Konda, 2000) to both the value function corresponding to the MDP reward
function (i.e. the critic) and the gradients w.r.t. to the policy parameters. That is, just as the original
actor-critic algorithm uses parameters w to approximate bootstrap values for the critic function (i.e.
vR,γ,∏(s) ≈ ^w (s), We use a similar approximation for the GVF corresponding to policy gradient
5
Under review as a conference paper at ICLR 2019
R
Q(∙,ωo)…A C(ω)
Q(∙,ωι)	./
.
.
.
Q("k)
γ(1 - β(ω))	π(ω)
so → C GVF -)
Ψ
Q(s0, ω)
R
π(W, θ) R + RI (g(M))	γW
vM
(a)	GVFs for Option-value function
(b)	GVFs for Feudal Networks.
π(g(M), η)
(、
→(gvfj
vW
C GVF J
s
Figure 4: Left: Every option-value corresponds to a separate GVF, concerned both with external
reward R, as well as all other GVFs. Right: In FuN, manager (M) and worker (W) are trained
using separate GVFs: vM, concerned with external return as corresponding to W’s policy; vW,
conditioned on goals specified by M.
estimates (i.e. VC 7 ∏ (S) ≈ g(s, η)). By doing so, We replace updates to the policy parameters that
consider only Vθ log ∏ ∙ Q with long term estimates of the GVFs derived from the analytic form of
the policy gradient. These estimates contain gradient information from past gradient computations
(i.e. they are bootstrapped). See Algorithm 2 in the appendix for details on the proposed procedure
and other implementation details. See Figure 3 for a comparative analysis of the baseline actor-
critic algorithm and the aforementioned modification; the results clearly demonstrate the benefit
of a principled approach in estimating GVFs corresponding to policy gradients in an end-to-end
algorithmic framework.
5	GVFs for Hierarchical Reinforcement Learning
Hierarchical reinforcement learning refers to a collection of methods which aim to structure the
policy ofan agent into “chunks” which have a temporal extent and can be re-used for different tasks.
A large variety of methods have been proposed, including options (Sutton et al., 1999b; Precup,
2000), MAXQ (Dietterich, 2000), feudal architectures (Dayan & Hinton, 1993; Vezhnevets et al.,
2017) and others. We will now illustrate how GVFs can be used in a natural way to analyze and
extend such approaches. We will focus on the options framework and on feudal architectures, since
these two approaches have recently been shown to yield algorithms that can learn the hierarchical
structure from scratch (Bacon et al., 2017; Vezhnevets et al., 2017).
GVFs for options An option ω is defined as a tuple containing a set of states in which the option
can be initiated, an internal policy πω : S × A → [0, 1], which is used to pick actions when the
option is executing, and a termination function βω : S → [0, 1]. As in other recent work (Bacon
et al., 2017), we ignore the initiation set in this paper. The typical execution model for options uses
a policy over options, ∏ω, which chooses options from a set Ω. Whenever an option is picked, its
policy chooses actions until the termination condition is met. Note that at a state s, an option ω will
continue with probability 1 - βω (s).
Options also have associated reward and transitions models (Sutton et al., 1999b). For example, the
reward model of an option is given by:
r(s,ω) = E [P∞=0 γtRt+ι∣St = s,ωt = ω]
=E[R(S, At+1) + Y(I - β(st+1))r(St+1, ω) lSt = s, At+1 〜πω (S, ∙), st+1 〜P(Is, At+1)]
It is easy to see then that r(s, ω) can be written as a GVF which has as cumulant the environment
reward R, the continuation function is γ(1 - βω(s)) and the policy is πω. A similar development can
be done for option transition models.
Another important quantity in the context of options is the option-value function, Q∏ω (s, ω), which
generalizes the action-value function, and is used to drive option construction algorithms such as
option-critic. Indeed, the option-critic architecture (Bacon et al., 2017) uses a policy gradient
algorithm to learn the internal policies and termination functions for a fixed number of options
Ω = {ωι,… ,ωk}. We now show that the option-value function can be expressed as a GVF.
6
Under review as a conference paper at ICLR 2019
Proposition 2. Given a set ofoptions Ω and a fixed policy over options ∏ω, define the cumulant Cω
for option ω as Cω(s, a, s0) = R(s, a) + γβω(s')E [Q(s0, ω0) | ω0 〜∏ω( ∙ |s0)]. Then:
Q∏Ω (S, ω) = vCω,γ(1-βω ),∏ω (S, "^
The proof is in the appendix. Figure 4a illustrates the statement. The GVF view highlights the fact
that the option-value function depends both on a short term signal coming from the reward and a
long-term signal summarizing the performance of other options. This leads to a very hard task, since
each option has to be aware of the performance of the rest of the options. In the option-critic frame-
work, this can lead to degenerate solutions, e.g. when each option considers itself less qualified than
the others and wishes to cede control immediately (which leads to all options collapsing to single-
step actions), or when one option decides it is better than the others, and hence never terminates and
tries to learn the optimal policy internally. Note that the option models, on the other hand, achieve
a separation of concerns naturally. Hence, in the context of transfer learning, option models may be
more useful than option-value functions.
GVFs for feudal networks Feudal Networks (FuN) (Vezhnevets et al., 2017) is an HRL approach
designed to provide an explicit separation of concerns between levels of hierarchy. It uses a network
which consists of two parts. The top level, or the Manager M, chooses goals g(M) at a slower time
scale in a latent state-space that is itself learned. The lower level, or the Worker W, operates at a
faster time scale and produces primitive actions, conditioned on the goals received from the Manager.
The Worker is motivated to follow the goals by a combination of intrinsic and extrinsic reward. No
gradients are propagated between W and M; M receives its learning signal from the environment
alone, adjusting g(M) to maximize the extrinsic reward using transition policy gradient (Vezhnevets
et al., 2017).
We now show how GVFs can be used to provide a general version of the transition policy gradient
introduced in Vezhnevets et al. (2017).
Proposition 3. Let z ∈ Z ⊆ Rk represent some latent variables used to model a hierarchical
policy parameterised as: π(a∣s) = ∏ π(a∣s, z; θ2)dπ(z∣s; θι). Let V be a set of random variables
corresponding to predictions about π. Given continuation function γ and cumulant C : S ×A×Z →
R, Vθι vc,γ,∏(s) = vc1,γ,∏(s) and ^θ2vc,γ,∏(s) = vg2,γ,∏(S) With
C1,t = vC,γ,π (st, zt, Vt)▽6] log π(Zt, vt | st; θI), and C2,t = vC,γ,π (st, at, zt) Vθ2 log π(at | st, zt; θ2 ).
The proof is in the appendix. This result allows the manager in FuN to move from fixed temporal
difference in state representations (e.g. Vt := St+c), to any general prediction conditioned on π. It
also shows that the transition policy gradient can be used for broader choices of latent attributes than
those used in Vezhnevets et al. (2017), where logπ(zt, Vt) was proportional to the cosine distance
between (St+c - St) and zt.
When vC,γ,π (S, a) in Prop. 3 is independent of z, low-level action choices that ignore z (i.e.
π(a∣s, z; θ2) = π(a∣s; θ2)) end UP as valid local minima for the corresponding optimisation Prob-
lem. One way to avoid this kind of degeneracy is to use intrinsic rewards corresponding to some
notion of alignment between the observed trajectory and z (Dayan & Hinton, 1993; Barto et al.,
2004; KUlkarni et al., 2016; Baranes & OUdeyer, 2013). Alternatively, one coUld Use Universal
ValUe FUnctions as GVFs, consider latents z that determine specific goals (i.e. (C, γ) pairs) and
train low-level controllers to achieve those goals (SchaUl et al., 2015).
5.1	Empirical illustration
To illustrate the ideas above, We have used the following GVFs: V := vc,^,∏, with Ct = st+c 一 St
and some discount factor ^ possibly different from the discount of the GVF vr,y,∏ used for eval-
uating the agent on the given task. We assumed a von Mises-Fisher distribution for the higher-
level policy: ∏(zt, VtISt) α exp(dcos(zt, Vt)). The intrinsic reward for the low-level compo-
nent was chosen as : rt = Pj=0 (^) (Vt-St-：)晟-°-1 . See Appendix for details on an effi-
cient recursive computation of this reward and a proof that this corresponds to an intrinsic return
RI = Pi∞=0 γi log π(Vi, ziISi). We also use successor features (SF) for GVFs. We conducted a
number of experiments in the ATARI games (Bellemare et al., 2013) for which the original FuN
7
Under review as a conference paper at ICLR 2019
Figure 5: Training curves for FuN with high-level choices over GVFs in ATARI games (Bellemare
et al., 2013). In Seaquest and Enduro, agents for which the manager uses successor features consis-
tently outperform the baseline algorithm. In Gravitar and Ms. Pacman, the proposed modifications
have no impact on performance. See appendix for more results.
architecture was shown to exhibit substantial empirical improvements over corresponding network
structures (Vezhnevets et al., 2017). The experimental setup and all the hyper-parameters were set
to those used in the original paper on FuN (Vezhnevets et al., 2017). The results in Figure 5 show
that using GVFs either reproduces the results in baseline FuN or improves the learning speed.
6	Other related work
Schaul et al. (2015) address the issue of learning a large number of GVFs individually, which is not
scalable and misses out on learning shared feature structure among different GVFs. They propose
that an agent may want to focus on learning a special type of GVF, which is optimal with respect
to a specific minimum-time-to-goal based task. Such a task can be specified using a (C, γ) pair, in
which C is positive at the goal state(s) and 0 otherwise, and γ < 1 is constant at all states except the
goal, where it is 0. In general, however, we could have GVFs that are optimal with respect to any
cumulant and continuation, which We can be denoted as VC γ.
additional requirements such as having a bounded output and
Note that, if C is multidimensional,
an ordering on the co-domain of C
are necessary in this case. In Universal Value Functions (UVFs), the task (C, γ) can be summarized
through a goal state g, which can then be presented as an input, to allow an agent to generalize
between tasks. Hence, in this approach, a shared approximator v (s, g) is estimated from data.
Synthetic gradients (Jaderberg et al., 2016) allow a learner to estimate directly a gradient for certain
input data points, without backpropagation. The idea is to use a secondary function approximator
for this estimation. The policy gradient approach that we presented is similar in spirit, as we use a
separate GVF as a gradient estimator. Further connections are left for future investigation. Auxiliary
tasks Jaderberg et al. (2017) are an approach which uses secondary reward functions in order to learn
a better internal representation for an RL agent. The idea of using many GVFs, corresponding to
many different cumulants, is very much in the same spirit.
7	Conclusion and future work
We proposed to use GVFs as a unifying language for describing several popular reinforcement learn-
ing algorithms. This approach provides interesting connections between algorithms and helps us
understand certain pathological behaviors (such as the collapse encountered in certain temporal
abstraction algorithms). Moreover, we can use the GVF framework in order to generate new algo-
rithms, by making different choices for how these functions are encoded. We illustrated this ability
by generating new versions of policy gradient and of feudal networks. In the case of policy gradient,
the proposed approach, which uses bootstrapping as well as successor features, greatly improves
learning speed and stability. However, more experience is needed with this algorithm, especially in
conjunction with function approximation.
GVFs can enable a natural injection of domain knowledge, as the designer of the system can specify
cumulants and continuation functions, then let the system learn about all of them. We will investigate
this approach in future work. GVFs promise to enable life-long, incremental learning, as an agent
could add new GVFs to its repertoire over time, perhaps following a meta-learning or curriculum
learning approach.
8
Under review as a conference paper at ICLR 2019
References
V. M. Aleksandrov, V. I. Sysoyev, and V. V. Shemeneva. Stochastic optimization. In Engineering
Cybernetics, 1968.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, 2017.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically mo-
tivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.
Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and
David Silver. Successor features for transfer in reinforcement learning. In NIPS, 2017.
Andrew G. Barto, Satinder Singh, and Nuttapong Chentanez. Intrinsically motivated learning of
hierarchical collections of skills. In Development and Learning, pp. 112-119, 2004.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
jun 2013.
Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural Actor-Critic
Algorithms. Automatica, 45(11), 2009.
Peter Dayan. Improving generalization for temporal difference learning: The successor representa-
tion. Neural Computation, 1993.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In NIPS, 1993.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-
sition. J. Artif. Intell. Res.(JAIR), 2000.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and
Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. 2016. URL http:
//arxiv.org/abs/1608.05343.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In
ICLR, 2017.
Vijaymohan Konda. Actor-critic algorithms. In NIPS, 2000.
Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. Hierarchical
deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In NIPS,
2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. URL
http://arxiv.org/abs/1509.02971.
Joseph Modayil, Adam White, and Richard S. Sutton. Multi-timescale nexting in a reinforcement
learning robot. Adaptive Behavior, 2014.
Doina Precup. Temporal abstraction in reinforcement learning. PhD thesis, University of Mas-
sachusetts, 2000.
R. Y. Rubinstein. Some Problems in Monte Carlo Optimization. Ph.D. thesis, 1969.
Tom Schaul and Mark B Ring. Better Generalization with Forecasts. In IJCAI, 2013.
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In ICML, 2015.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In ICML, pp. 1889-1897, 2015.
9
Under review as a conference paper at ICLR 2019
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Richard S SUtton and Brian Tanner. Temporal-difference networks. In NIPS, pp. 1377-1384, 2005.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with fUnction approximation. In NIPS, 1999a.
Richard S SUtton, Doina PrecUp, and Satinder Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artificial intelligence, 1999b.
Richard S. SUtton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White,
and Doina PrecUp. Horde: A scalable real-time architectUre for learning knowledge from UnsU-
pervised sensorimotor interaction. In AAMAS, 2011.
Richard StUart SUtton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, Uni-
versity of MassachUsetts Amherst, 1984.
Alexander Sasha Vezhnevets, Simon Osindero, Tom SchaUl, Nicolas Heess, Max Jaderberg, David
Silver, and Koray KavUkcUoglU. FeUdal networks for hierarchical reinforcement learning. In
ICML, 2017.
Adam White. Developing a predictive approach to knowledge. PhD thesis, University of Alberta,
2015.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256, 1992.
A Proofs
Proposition. Let μ be the measure corresponding to predictions on one-hot cumulants, as defined
in Section 3. For every General Value Function, v(so; C, γ,π) = J Cdμ(∙; so, γ, π).
Proof. WLOG, assUme C is defined over state-action pairs, i.e C : S × A → R and fix a starting
state so. Let So, A0,S1,A1, ∙,Sn,An,… be the random trajectory generated when following policy
π from initial state S0 = s0 and Using γ(Si) as a BernoUlli process that determines the termination
of a trajectory. Following the definition of general valUe fUnctions (see Section 2),
/ Cdμ(∙;
so,	γ, π) =	C(s, a)vI(s,a),γ,π(so)
s,a
∞t
X C(s,a)E X(Y Y(Si)) I((St ,At) = (s,a)) So = so,Ao;∞ 〜∏
s,a	t=o i=1
E
E
∞t
XY
Y(Si)) X C(s, a)I((St, At) = (s, a))卜o = so Aog 〜∏
∞ t
X I Y Y(Si) ) Ct So = so,Ao=∞ 〜∏ = v(so; C,Y,∏)
t=o i=1
□
Theorem. Sutton et al. (1999a). (Policy gradient theorem) Let πθ be a policy parameterised by
a vector θ, Y be a constant continuation function, and C : S × A → R be a one-dimensional
cumulant. The gradient of the general value function vC,γ,πθ (s) with respect to θ is itself a general
value function that depends on the cumulant:
A /	ʌ	/	∖ V^7 1 .	/ I ∖
C(s,a) := vc,γ,∏θ(s, a)Vθ log∏(a|s).
10
Under review as a conference paper at ICLR 2019
Proof. (sketch) Recall from Section 3 that v(s; C,γ,π) = J Cdμ(∙; s,γ,π) and vc,γ,∏(s, a)
J Cdμ(∙; s, a, γ, π). The derivation below follows,
VθvC,γ,∏(θ)(SO) = J ∑α∈A vC,γ,π(S, aWθπ(als; θ)dμ(s; s0, γ, π)
=RPα∈An(a|s；θ) "γ,∏(s,a Vnn⅞θ)) dμ(S; so,γ,π)
=ʃ vc,γ,∏(s, a)Vθ log ∏(a∣s; θ)dμ(s, a; so, π(θ), γ).
□
Proposition. Fix a Set of options Ω and a meta-poIicy πω and define the cumulant C(ω) for
every option ω as Ct(ω) = R(St,At) + γβ(St+ι; ω)E [Q(St+ι,ω0) | ω0 〜πω( St+ι)]. Then,
Q(S0, ω) = v(S0; C(ω), γ(1 - β(ω)), π(ω)).
Proof. Let V(s) := E[Q(s, ω) ∣ω 〜πω]. Based on the definition provided in Bacon et al. (2017),
Q(so,ω) = X ∏(a∣so) R(s°,a) + Y X P(SlSo,a)[(1 - β(s0; ω)Q(s0,ω) + β(s0; ω)V(s0)]
α∈A	s0∈S
=X ∏(a∣so) R(so, a) + γ X P(s0∣so, a)β(s0; ω)V(s0) +
α∈A	s0∈S
+ X n(a|so)Y X P(SlS0,a)(1- β(s0; ω)Q(s0,ω)
α∈A	s0∈S
=E [C(so, A, S0; ω) | A 〜π(s°); S0 〜P(∙∣s0, A)]
+ E [γ(1 - β(S0； ω))Q(S0, ω) | A 〜π(so); S0 〜P(∙∣s0, A)].
Note that the statement above is equivalent to the Bellman equation for General Value Functions,
where the cumulant is C(ω) and continuation function is γ(1 - β(ω)). Since the Bellman equation
is known to have a unique solution, the statement of the proposition follows as a direct consequence.
□
Proposition. Assume latent attributes z are elements of Z ⊆ Rk. Let θ1 and θ2 be two sets of
parameters used to model distributions π(z∣s; θι) and π(a∣s, z; θ2), correspondingly. These models
describe the low-level action choice π(a∣s) = J π(a∣s, z)dπ(z∣s). Let Y be a constant continuation
function, and C be any cumulant defined on S × A × Z → R. Additionally, let v be a set of random
variables corresponding to predictions about the policy π. Then ▽6】v(so； C, γ, π) = v(so； Cι,γ, π)
and Vθ2v(so； C, γ, π) = v(so； C2,γ, π) with
Cι,t = v(st,zt, Vt； C, γ,∏)Vθι log∏(zt, vt|st； θι),and
C2,t = v(st,at, zt； C,γ,∏)Vθ2 log∏(at∣st, zt； θ2).
Proof. The statement is a corollary to Theorem 1, where the gradient with respect to the flat policy
is:
Vθιπ(a, z, v|s; θι, θ2) = Vθιπ(a∣z, s; θ2)π(z, v|s; θι) = π(a∣z, s; θ2)Vθ1 π(z, v|s; θι)
= π(a, z, v|s； θ1, θ2)Vθ1 log π(z, v|s； θ1)
Vθ2π(a∣s; θι, θ2) = Vθ2π(a∣z, s; θ2)π(z, v|s; θι) = π(z, v|s; θ1)Vθ2π(a∣z, s; θ2)
=∏(a, z, v|s; θι, θ2)Vθ2 log∏(a∣z, s; θ2)
11
Under review as a conference paper at ICLR 2019
Now, when computing gradients,
Vθ2v(so； C,γ,∏) = J X v(s,a,z, v; C, γ,∏)Vθ2π(a, z, v|s; θ1,θ2)dμ(s; so,γ,∏)
a,z,v
,a, z, v； C, Y, ∏)∏(a, z, v|s； θ1,θ2)Vθ2 log π(a∣z, s; θ2)dμ(s; so, γ, π)
X v(s,a, z； C, Y, ∏)∏(a, z|s； θι, θ2)Vθ2 logπ(a∣z, s; θ2)dμ(s; so, γ,π)
a,z
/ v(s, a, z; C,γ, ∏)Vθ2 log∏(a∣z, s； θ2)dμ(s, a, z; so, γ,π)
V(S0； CC2, Y,π)
A similar results can easily be derived for θ1 to obtain the gradient updates described in the Propo-
sition.	□
Proposition 4. Let st ∈ RK represent the observation at time t and zt ∈ RK be the latent attributes
used to model the hierarchical policy described in Section ??. Define the intrinsic reward rtI at time
t to be
't-C Y Y∖' (st-st —C)T Zt — c — j
j = 0 ⑴	kvt -C —jkkzt -C —jk .
(1)
Given a sequence so,
and RI = Pt∞=o YtrtI
zo,sι, zι, ∙∙∙ ,sτ, ZT, the intrinsic rewards rI can be computed iteratively,
= i∞=o Yi logπ(vi, zi|si), where logπ(vi, zi|si) is assumed to follow von
Mises-Fisher distribution.
Proof. Note that this reward value can be efficiently computed using the concept of a running sum.
Define the vector mt at every timestep to be mt := Pj=O (^) ∣" Zt-CIZj_ _.k ； it is not hard to
check that this can be computed iterative using the following update: mt :=	Zt-c_ ∣∣ + ^mt—1.
From this, computing the intrinsic reward amounts to rtI = (st - st-c)Tmt.
Apart from being computationally convenient, the intrinsic reward in Equation 1 has a corresponding
return which is maximized when the latent values z align with predictions v. That is,
∞	∞	t—c j
RI = XYtrI = XYtX (Y)
t=o	t=o	j=o	Y
(St - St-C)TZt— c—j
kvt —c—j llkzt-c—j Il
∞ t—c
XX Yt-j^j 上
=j=0	Ykvt-C-j kkzt - c-j k
st—C) zt—C—j
∞∞	T
X X √+c^j (Si+j+c - si+j ) Zi
⅛ j⅛Y	Y 一河南一
∞
Yc	Yi log π(vi, zi|si).
i=o
□
B Actor-Critic with GVFs for gradient estimation
Algorithm 2 describes the modifications we hade made to the Actor-Critic algorithm (Konda, 2000)
in order to illustrate the benefits of bootstraping not only the values of the critic function, but the
general value functions involved in the computation of policy gradients.
To generate the results presented in Figure 3, we first defined a tile-coding feature map
φ : S → RK (Sutton & Barto, 1998). The policy is parameterized with a vec-
tor θ ∈ Rκ×lAl and the critic values with W ∈ RK using the following func-
tional forms: V(s, w) = wtφ(s) and π(a∣s, θ) = SOftmax(θTφ(s)). For pol-
12
Under review as a conference paper at ICLR 2019
icy gradients estimation, we used parameters η ∈ Rκ×lAl and g(s, η)	=	ητφ(s).
Algorithm 2: Actor-Critic with GVFs for gradient estimation.
Input: policy π(a∣s, θ); critic state-value fn. v(s, w); general-value fn. g(s, η).
Algorithm parameters: step size αθ > 0, αw > 0, and αη > 0.
repeat
Initialize S (first state of episode); I J 1
repeat
A 〜∏(a∣s, θ) and take action A, observe S0, R
δcrit J R + γV(S0, w) — V(S, w)
W J W + αwIδcritVwV(S, w)
δgrad J δcritVθ log ∏(A∣S, θ) + γg(S0, η) — g(S, η)
η J η + αηiδgradVη g(s, η)
θ J θ + αθg(S, η)
IJ γIandSJ γS0
until end of episode
until convergence
C Details of GVF-Fun Experiments
We follow the same experimental procedure described in (Vezhnevets et al., 2017), where hyper-
parameters are tuned using randomized search on the following domain: the learning rate for the
policy gradient updates is sampled from Log-Uniform(10-4.5, 10-3.5); the entropy penalty was
sampled from Log-Uniform(10-4, 10-3); the weight of the intrinsic reward was sampled from
Uniform(0, 1). Moreover, to increase stability, the reward is clipped in [—1, 1].
For the purpose of illustrating the concept of separation of concerns, the worker and the manager
were trained with respect to GVFs of different discount values: manager’s discount γM = 0.99 and
worker’s discount γW = 0.95.
We compare baseline FuN with a modified network where the goal g picked by the manager cor-
responds to the latent z in Proposition 3 and the manager is trained with transition policy gradient
Vgt = AtM dcos (vt, gt(θ)). Since the prediction horizon of the baseline FuN agent is set to c = 10,
we compute successor features vt using a discount value γ = 0.8 to make sure that the correspond-
ing temporal resolution is essentially the same2. The intrinsic reward corresponding to specific goals
set by the manager was computed using Equation 1. Since our intent was to test the hypothesis that
a Feudal Network would be capable of reasoning over successor features, all other components were
kept invariant.
See Figure 9 for results for all the games for which we have ran the experiments. Specifically, the
plots in the figure provide the average and standard deviations over 5 different runs of the algorithm,
where each run was based on a randomized search picking the top performing hyper-parameter
setting among 20 samples, and the policy gradient algorithm is performed over 80 epochs3.
2 When a GVF is using a constant discount value γ and no explicit horizon length is used, rewards obtained
after logγ become numerically insignificant (i.e. ≤ Rmax).
3A training epoch is equivalent to 106 steps.
13
Under review as a conference paper at ICLR 2019
critic-only
Figure 6: Policy gradient updates with TD(λ) estimates. L1 norm of the exact value function vs.
number of updates to policy parameters. Each plot corresponds to a fixed number N of rollouts for
estimating GVFs, a fixed λ as eligibility for TD(λ), fixed learning rate for αtd, and fixed learning
rate αpg for policy parameters. Mean and variance for the value of the policy as a function of
policy updates are shown in (blue) when only the critic value is bootstrapped, in (red) when all GVF
estimates are bootstrapped, and (green) when su1cc4essor features are used to stabalized the learning
algorithm.
Under review as a conference paper at ICLR 2019
Figure 7: Actor-Critic with bootstrapped gradient estimates - results on four-room maze (in-
troduced in (Sutton et al., 1999a)) L1 norm of the exact value function vs. number of rollouts used
for policy updates. Each plot corresponds to a fixed number of steps L taken by the agent for every
rollout, the learning rate αA for updates to the policy parameters, a learning rate αC for updates
to the critic component, and a learning rate αG for updates to the gradient estimates. Mean and
variance for the value of the policy as a function of policy updates are shown in (blue) for baseline
actor-critic, in (green) when actor-critic maintains bootstrapped estimates for parameter updates.
15
Under review as a conference paper at ICLR 2019
Figure 8: Actor-Critic with bootstrapped gradient estimates - results on large maze grid-world
with sparse reward. L1 norm of the exact value function vs. number of rollouts used for policy
updates. Each plot corresponds to a fixed number of steps L taken by the agent for every rollout,
the learning rate αA for updates to the policy parameters, a learning rate αC for updates to the critic
component, and a learning rate αG for updates to the gradient estimates. Mean and variance for the
value of the policy as a function of policy updates are shown in (blue) for baseline actor-critic, in
(green) when actor-critic maintains bootstrapped estimates for parameter updates.
16
Under review as a conference paper at ICLR 2019
Figure 9: Training curves for FuN with high-level choices over GVFs. Results on ATARI
games (Bellemare et al., 2013) demonstrate gains in data-efficiency for some ATARI games. In
games such as Seaquest, Alien, and Enduro, agents for which the manager uses successor features
consistently outperform the baseline algorithm. In Gravitar, Amidar and Ms. Pacman, the proposed
modifications have no impact on performance. In Frostbite, both versions of the algorithm are usu-
ally stuck in a local minimum (average reward ≈ 250), and the noticeable improvement is due to
one of the agents luckily getting out of the local minimum for a limited number of steps.
17