Under review as a conference paper at ICLR 2019
Lyapunov-based Safe Policy Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In many reinforcement learning applications, it is crucial that the agent interacts
with the environment only through safe policies, i.e., policies that do not take the
agent to certain undesirable situations. These problems are often formulated as
a constrained Markov decision process (CMDP) in which the agent’s goal is to
optimize its main objective while not violating a number of safety constraints.
In this paper, we propose safe policy optimization algorithms that are based on
the Lyapunov approach to CMDPs, an approach that has well-established theo-
retical guarantees in control engineering. We first show how to generate a set of
state-dependent Lyapunov constraints from the original CMDP safety constraints.
We then propose safe policy gradient (PG) algorithms that train a neural network
policy using deep deterministic policy gradient (DDPG) or proximal policy op-
timization (PPO), while guaranteeing near-constraint satisfaction at every policy
update by projecting either the policy parameter or the action onto the set of fea-
sible solutions induced by the linearized Lyapunov constraints. Unlike the ex-
isting (safe) constrained policy gradient algorithms, ours are more data efficient
as they are able to utilize both on-policy and off-policy data. Furthermore, the
action-projection version of our algorithms often leads to less conservative policy
updates and allows for natural integration into an end-to-end PG training pipeline.
We evaluate our algorithms and compare them with constrained policy optimiza-
tion (CPO) and the Lagrangian method on several high-dimensional continuous
state and action simulated robot locomotion tasks, in which the agent must satisfy
certain safety constraints while minimizing its expected cumulative cost.
1	Introduction
In many real-world reinforcement learning (RL) problems, the agent must satisfy a number of con-
straints while minimizing its expected cumulative cost. In particular, these constraints might be
related to safety, i.e., the policy learned by the agent should not take it to certain undesirable parts of
the state and/or action space. In some problems, it is not only important that the policy learned by
the agent (that will be deployed) be safe (Amodei et al., 2016), but also crucial that all the policies
generated during training (with which the agent interacts with the environment to collect samples)
be safe (Achiam et al., 2017). For example, a robot should always avoid taking actions that irrevo-
cably harm its hardware. We often formulate the agent’s interaction with the environment in con-
strained sequential decision-making problems as a constrained Markov decision process (CMDP).
CMDPs are extensions of MDPs in which, in addition to the original cost function, there exists a
constraint cost function, whose expected cumulative value should remain bounded. The additional
constraint cost function gives more flexibility to CMDPs in modeling problems with trajectory-
based constraints compared to approaches that artificially re-cast the cost of MDPs to enforce the
constraints (Regan & Boutilier, 2009). Under the CMDP framework, we consider a policy to be safe
if it satisfies the (expected) cumulative cost constraints.
A common approach to solve CMDPs is to use the Lagrangian method (Altman, 1998; Geibel &
Wysotzki, 2005) that augments the original objective function with a penalty on constraint violation
and computes the saddle-point of the constrained policy optimization via primal-dual method (Chow
et al., 2017). Although safety is ensured when the policy converges asymptotically, a major draw-
back of this approach is that it makes no guarantee with regards to the safety of the policies generated
during training. To address this issue (safety during training), heuristic algorithms for policy search
with safety constraints have been proposed (e.g., Uchibe & Doya 2007). While some of them work
reasonably well in practice, none is theoretically grounded to generate constraint-satisfying poli-
cies. Achiam et al. (2017) recently proposed the constrained policy optimization (CPO) method that
extends the trust-region policy optimization (TRPO) algorithm (Schulman et al., 2015a) to handle
CMDP constraints. Although there is no theoretical guarantee that the policies generated by CPO are
all safe, the empirical results reported in the paper are promising in terms scalability, performance,
1
Under review as a conference paper at ICLR 2019
and constraint satisfaction, both during training and after convergence. Moreover, CPO is based on
a principled approach to generate constraint-satisfying policies, and not just on heuristics. However,
the CPO methodology is closely connected to TRPO. While adopting this methodology to PPO for
constrained optimization is straight-forward (this is exactly how we derive our SPPO algorithm in
this paper), it is not clear how it can be combined with algorithms that do not belong to the family of
proximal PG algorithms (PG algorithms that are regularized with relative entropy), such as DDPG.
Another recent approach to solve CMDPs, while remaining safe during training, is by Chow et al.
(2018). This work is based on the notion of Lyapunov function that has a long history in control
theory to analyze the stability of dynamical systems (Khalil, 1996; Neely, 2010). Using Lyapunov
functions in RL was first studied by Perkins & Barto (2002), where they were used to guarantee
closed-loop stability of an agent. In a recent work, Berkenkamp et al. (2017) used Lyapunov func-
tions to guarantee that a model-based RL agent can be brought back to a “region of attraction”
during exploration. Using the theoretical underpinnings of the Lyapunov approach, Chow et al.
(2018) proposed two dynamic programming (DP) algorithms to solve a CMDP and provided theo-
retical analyses for the feasibility and performance of the resulted policies. The proposed algorithms
correspond to the two celebrated DP algorithms: policy and value iteration. They then extended their
algorithms to learning (the scenario in which the MDP model is unknown) and proposed RL algo-
rithms that correspond to approximate policy and value iteration. However, since their algorithms
are all value-function-based, applying them to continuous action problems is not straightforward.
In this paper, we extend the Lyapunov-based approach to solving CMDPs of Chow et al. (2018) to
continuous action problems that play an important role in control and robotics. Our contributions
can be summarized as follows: 1) We formulate the problem of safe RL as a CMDP and propose a
Lyapunov-function-based policy optimization framework that can handle continuous action CMDPs.
2) By leveraging the theoretical underpinnings of the Lyapunov-based approach to CMDPs in Chow
et al. (2018), we present two classes of safe policy optimization algorithms that can work with any
standard policy gradient algorithm such as deep deterministic policy gradient (DDPG) (Lillicrap
et al., 2015) and proximal policy optimization (PPO) (Schulman et al., 2017). The first class of our
algorithms is based on constrained optimization w.r.t. policy parameter (similar to what is done in
constrained policy optimization (CPO)). The second class hinges on the concept of a safety layer
introduced by Dalal et al. (2018) and transforms the constrained policy optimization problem into an
unconstrained one, by integrating the Lyapunov constraints into the policy network via safety-layer
augmentation. 3) We evaluate our algorithms and compare them to two baselines, CPO (Achiam
et al., 2017) and the Lagrangian method, on several robot locomotion tasks, in which the agent must
satisfy certain safety constraints while minimizing its expected cumulative cost. Our results show
that our algorithms outperform the baselines in terms of balancing the performance and constraint
satisfaction at every policy update.
2	Preliminaries
We consider the RL problem in which the agent’s interaction with the environment is modeled as a
Markov decision process (MDP). A MDP is a tuple (X , A, γ, c, P, x0), where X is the state space;
A is the action space; γ ∈ [0, 1) is the discounting factor; c(x, a) ∈ [0, Cmax] is the immediate cost
function (negative reward); P(∙∣x, a) is the transition probability distribution; and xo ∈ X is the ini-
tial state. Our results easily generalize to random initial states and random costs, but for simplicity
we will focus on the case of deterministic initial state and immediate cost. In a more general set-
ting where cumulative constraints are taken into account, we define a constrained Markov decision
process (CMDP), which extends the MDP model by introducing additional costs and associated con-
straints. A CMDP is defined by (X, A, γ, c, d, P, x0, d0), where the components X, A, γ, c, P, x0
are the same as in the unconstrained MDP; d(x) ∈ [0, Dmax] is the immediate constraint cost; and
d0 ∈ R≥0 is an upper-bound on the expected cumulative (through time) constraint cost. To formalize
the optimization problem associated with CMDPs, let ∆ be the set of Markov stationary policies,
i.e., ∆(x) = {∏(∙∣x) : X → R≥os : Pa π(a∣x) = 1} for any state X ∈ X. For notational conve-
nience, at each state x ∈ X, we define the generic Bellman operator w.r.t. policy π ∈ ∆ and generic
cost function h as Tπ,h[V](x)
Pa π⑷x) Ih(X, a)+Y Pχ0∈X
P(X0 |X,
a)V (X0) .
Given a policy π ∈ ∆ and an initial state X0, the expected cumulative cost is defined as Cπ(X0) :=
E Pt∞=0 γtc(Xt, at) | X0, π , and the safety constraint is defined as Dπ(X0) ≤ d0, whereDπ(X0) :=
E Pt∞=0 γtd(Xt) | X0 , π is the safety constraint function (expected cumulative constraint cost).
2
Under review as a conference paper at ICLR 2019
The goal in CMDPs is to solve the constrained optimization problem
∏* ∈ min {C∏(xo): D∏(xo) ≤ do} ∙	(1)
π∈∆
Using the discounting property of a CMDP, Theorem 3.1 in Altman (1999) shows that if the fea-
sibility set is non-empty, then there exists an optimal policy in the class of stationary Markovian
policies ∆. To motivate the CMDP formulation, we refer the reader to Chow et al. (2018), which
presents two real-world examples in modeling safety using (i) the reachability constraint, and (ii)
the constraint that limits the agent’s visits to undesirable states.
3	A Lyapunov Approach for Solving CMDPs
In this section, we revisit the Lyapunov approach to solving CMDPs proposed by Chow et al.
(2018) and report the mathematical results that are important in developing our safe policy op-
timization algorithms. To start, without loss of generality, we assume that we have access to a
baseline feasible policy of Equation 1, πB; i.e. πB satisfies DπB (x0) ≤ d0. We define a set of
Lyapunov functions w.r.t. initial state x0 ∈ X and constraint threshold d0 as LπB (x0, d0) = {L :
X→R≥0 : TπB,d[L](x) ≤ L(x), ∀x ∈ X ; L(x0) ≤ d0}, and call the constraints in this feasibil-
ity set Lyapunov constraints. For any arbitrary Lyapunov function L ∈ LπB (x0 , d0), we denote
by FL(x) = {∏(∙∣χ) ∈ ∆ : T∏,d[L](x) ≤L(χ)} the set of L-induced Markov stationary policies.
Since Tπ,d is a contraction mapping (Bertsekas, 2005), any L-induced policy π has the property
Dπ (x) = limk→∞ Tπk,d[L](x) ≤ L(x), ∀x ∈ X . Together with the property that L(x0) ≤ d0,
they imply that any L-induced policy is a feasible policy of Equation 1. However, in general, the
set FL(x) does not necessarily contain an optimal policy of Equation 1, and thus it is necessary
to design a Lyapunov function (w.r.t. a baseline policy πB) that provides this guarantee. In other
words, the main goal is to construct a Lyapunov function L ∈ LπB (x0, d0) such that
L(X) ≥ %,d[L](x),	L(xo) ≤ do.	(2)
Chow et al. (2018) show in their Theorem 1 that 1) without loss of optimality, the Lyapunov function
can be expressed as L(x) := E Pt∞=0 γt(d(xt) + (xt)) | πB, x , where (x) ≥ 0 is some auxil-
iary constraint cost uniformly upper-bounded by e*(x) := 2DmaχDτv(∏*∣∣∏b)(x)∕(1 - γ),
and 2) if the baseline policy ∏b satisfies the condition maxχ∈χ e*(x) ≤ Dmax ∙
min{(1 - γ)(do-DnB (xo))∕Dmax, Dmax - (1 - γ)D∕Dmax +(1 - γ)D}, where D =
maxx∈X maxπ Dπ (x) is the maximum constraint cost, then the Lyapunov function candidate
Le* also satisfies the properties of Equation 2, and thus, its induced feasible policy set Fl及
contains an optimal policy. Furthermore, suppose that the distance between the baseline and
optimal policies can be estimated effectively. Using the set of Le* -induced feasible policies and
noting that the safe Bellman operator T [V](x) = minπ∈FL * (x) Tπ,c[V](x) is monotonic and
contractive, one can show that T [V ](χ) = V (x), ∀x ∈ X has a unique fixed point V *, such that
V * (χo) is a solution of Equation 1, and an optimal policy can be constructed via greedification,
i.e., π*(∙∣x) ∈ argmin∏∈FL *(x)T∏,c[V*](x). This shows that under the above assumption, Equa-
tion 1 can be solved using standard dynamic programming (DP) algorithms. While this result
connects CMDP with Bellman’s principle of optimality, verifying whether πB satisfies this
assumption is challenging when a good estimate of DTV(∏*∣∣∏b ) is not available. To address this
issue, Chow et al. (2018) propose to approximate * with an auxiliary constraint cost e, which is the
largest auxiliary cost satisfying the Lyapunov condition Lee(x) ≥ TπB,d[Lee](x), ∀x ∈ X and the
safety condition Lee(x0) ≤ d0. The intuition here is that the larger e, the larger the set of policies
FLe. Thus, by choosing the largest such auxiliary cost, we hope to have a better chance of including
the optimal policy π* in the set of feasible policies. Specifically, e is computed by solving the
following linear programming (LP) problem:
e ∈ arg max	(x)
UX→R≥1 x∈x
：do - DnB (xo) ≥ 1(xo)> (I - γ{P(x0∣x,∏B)}x,xo∈χ)
),
(3)
where 1(x0 ) represents a one-hot vector in which the non-zero element is located at x = x0 .
When πB is a feasible policy, this problem has a non-empty solution. Furthermore, accord-
ing to the derivations in Chow et al. (2018), the maximizer of Equation 3 is an indicator func-
tion of the form e(x) = (do -DnB (xo)) ∙ 1{x = x}∕E[P∞=o Yt1{xt = x} | xo,∏B] ≥ 0, where
x ∈ arg minχ∈χ E [P∞=0 γt 1{xt = x} | xo,∏b] . They also show that by further restricting e(x) to
be a constant function, the maximizer is given by e(x) = (1-γ)∙(do-D∏B (xo)), ∀x ∈ X. Using the
construction of the Lyapunov function Lee, Chow et al. (2018) propose the safe policy iteration (SPI)
3
Under review as a conference paper at ICLR 2019
algorithm (see Algorithm 1 in Appendix A) in which the Lyapunov function is updated via boot-
strapping, i.e., at each iteration Le is recomputed using Equation 3 w.r.t. the current baseline policy.
This algorithm has the following properties: 1) Consistent Feasibility, i.e., if the current policy πk is
feasible, then πk+1 is also feasible; 2) Monotonic Policy Improvement, i.e., Cπk+1 (x) ≤ Cπk (x) for
any x ∈ X ; and 3) Asymptotic Convergence. Despite all these nice properties, SPI is still a value-
function-based algorithm, and thus it is not straightforward to use it in continuous action problems.
The main reason is that the greedification step becomes an optimization problem over the continu-
ous set of actions that is not necessarily easy to solve. In Section 4, we show how we use SPI and
its nice properties to develop safe policy optimization algorithms that can handle continuous action
problems. Our algorithms can be thought as combinations of DDPG or PPO (or any other on-policy
or off-policy policy optimization algorithm) with a SPI-inspired critic that evaluates the policy and
computes its corresponding Lyapunov function. The computed Lyapunov function is then used to
guarantee safe policy update, i.e., the new policy is selected from a restricted set of safe policies
defined by the Lyapunov function of the current policy.
4	Safe Policy Gradient Algorithms with Lyapunov Functions
Policy gradient (PG) algorithms optimize a policy end-to-end by computing sample estimates of
the gradient of the cumulative cost induced by the policy and then updating the policy in the
gradient direction. In general, stochastic policies that give a probability distribution over ac-
tions are parameterized by a κ-dimensional vector θ, so the space of policies can be written as
{∏θ(∙∣x),x ∈ X, θ ∈ RK}. Since in this setting a policy π is uniquely defined by its parame-
ter vector θ, policy-dependent functions can be written as a function of θ or π, and they are used
interchangeably in this paper.
Recently there are two PG algorithms which have emerged as generally well-performing, namely
DDPG and PPO. These algorithms are widely used in many continuous control tasks. DDPG (Lil-
licrap et al., 2015) is an off-policy Q-learning style algorithm. A deterministic policy πθ(x) and a
Q-value approximator Q(x, a; φ) are trained jointly. The Q-value approximator is trained to min-
imize Bellman errors with respect to π; i.e., it is trained to fit the true Q-value function satisfying
Q(x, a) = c(x, a) + γ Px0∈X P(x0|x, a)Q(x0, πθ(x0)). The policy π is then trained to optimize
Q(x, πθ(x); φ) via chain-rule. The PPO algorithm we use is a penalty form of TRPO (Schul-
man et al., 2017) with an adaptive rule to tune the DKL penalty weight βk. Specifically, PPO
trains a Gaussian policy πθ(x) according to the standard policy gradient objective augmented with
a penalty on KL-divergence from a previous version of the policy; i.e., the penalty is of the form,
DKL (θ, θ0) = E[Dkl(∏θo (∙∣xt)∣∣∏θ (∙∣xt))∣xo, θ0], where θ0 is a previous version of the policy.
In CMDPs, the presence of a constraint Dπθ (x0) ≤ d0 may be naively incorporated into
the standard forms of DDPG and PPO via the Lagrangian method. That is, one may trans-
form the constrained optimization problem to a penalty form, in which the constraint costs
d(x) are added to the task costs c(x, a). The resulting penalized form of the objective is
minθ maxλ≥o E [P∞=o c(xt, at) + λd(xt)∣xo, θ] 一 λd0. In this form, both θ and λ must be op-
timized jointly to find a saddle-point of the objective. The optimization of θ may be performed
by either DDPG or PPO on the augmented cost c(x, a) + λd(x). The optimization of λ may be
performed by stochastic gradient descent on xt taken from trajectories sampled according to θ.
Although the Lagrangian approach is easy to implement (see Appendix B for details), in practice it
does not lead to safety in training. While the objective encourages finding a solution which is safe,
any intermediate step in the optimization may lead to an unsafe policy. In contrast, the Lyapunov
approaches we propose are guaranteed to return a safe policy, not only at convergence, but also
during training. In the subsections below, we elaborate how to transform DDPG and PPO to their
Lyapunov safe counterparts below:
θ* = arg min C∏g (χ0) subject to
θ
/
a∈A
(∏θ (a|x)
一 πB (a|x))QL (x, a)da ≤ e(x), ∀x ∈ X. (4)
where QL(x, a) = d(x) + e0(x) + γ x0 P (x0|x, a)Le0(x0) is the state-action Lyapunov function.
We will describe two approaches to incorporating Lyapunov constraints in PG: θ-projection and
a-projection. In Section 4.1, we formulate the Lyapunov-based PG using constrained policy opti-
mization (which we call θ-projection), and in Section 4.2 we show how the Lyapunov constraints
can be embedded into the policy network via a safety layer (which we call a-projection). In Section
A.1, we also discuss two practical techniques to further enforce safety during policy training.
4
Under review as a conference paper at ICLR 2019
4.1	Constrained Optimization Approach to Lyapunov-based PG
For demonstration purposes, we hereby show how one can perform constrained policy optimization
with PPO and Lyapunov constraints. With almost identical machinery, this procedure can also be
applied to DDPG. Consider the following constrained optimization at iteration k with semi-infinite
dimensional Lyapunov constraints for policy update:
θ ∈ arg min	h(θ	-	θk), VθEx〜“。…。〜∏。	[Qθk (x,a)] +	βkh(θ	- θk), v2Dkl(Θ∣∣ΘQ	∣θ=θ%	∙(θ	- θk)	∣θ=θQ
θ∈Θ	k
s	.t.	h(θ	-	θk), VθEa〜∏θ	∣QLθk	(x,a)]	∣θ=θki ≤	e(x),	∀x	∈ X,
where μek is the Y-ViSiting distribution w.r.t. ∏θk, and βk is the adaptive penalty weight of the
DκL(θ∣∣θk) regularize]. Clearly if one updates the policy parameter by solving the above op-
timization, and if the approximation errors from neural network parameterizations of Qθk and
QLθ and first-order Taylor series expansion are small, then safety may ensure safety during
training. However, the presence of infinite-dimensional Lyapunov constraints makes solving the
above optimization (in real-time) numerically intractable. To tackle the issue of infinite dimen-
sionality, (without loss of optimality) we re-write the Lyapunov constraint in the following form:
maxχ∈x h(θ - θk), VθEa〜∏θ Q^ikkk (x, a)] ∣θ=θ%i - e(x) ≤ 0. ThiS might still lead to numeri-
cal instability in gradient descent algorithms, because the max-operator in the constraint is non-
differentiable. Similar to the surrogate constraint used in TRPO (to transform the max DKL con-
straint into an average DKL constraint), a more numerically stable way is to approximate the Lya-
punov constraint using the following average constraint surrogate:
MM
h(θ - θk),MM X vθEa~∏θ [QLθk (Xi,a)] ∣θ=θki≤ MM Xe(Xi).
(5)
i=1
i=1
where N is the number of on-policy trajectories of πθk . In practice, if one adopts e =
(1 - γ)(d0 - Dπk (x0)) from Section 4.1, then the linear term in Equation 5 can be sim-
plified as VθEa^,∏θ QlLθk (xi,a)] = Vθ Ra∈∕ ∏(a∣x)Vθ logπ(a∣x)Q0,θk (xi,a)da. On the
other hand, by setting M = O(1∕(1 一 γ)), the constraint threshold becomes * PMI e(xi) ≈
d0 - Dπθ (x0). Collectively, the average constraint surrogate in Equation 5 becomes h(θ -
θk), M PM=I VθEa〜∏θ [QD,θk (xi, a)] ∣θ=θki ≤ do 一 Dnkk (xo), which is equivalent to the con-
straint used in the CPO algorithm (see Section 6.1 in Achiam et al. (2017)). This draws the connec-
tion between CPO and Lyapunov-based PG with θ-projection.
The Lyapunov-based algorithms with θ-projection in constrained policy update is summarized by
Algorithm 4 in Appendix A. In the experiment section, we denote the DDPG version and the PPO
version of this algorithm by SDDPG and SPPO respectively.
4.2 Embedding Lyapunov Constraints into a Safety Layer
Notice that the main contribution of the Lyapunov approach is to break down a trajectory-based
constraint into a sequence of single-step, state dependent constraints. When the state space is in-
finite/continuous, it is counter-intuitive to directly enforce these Lyapunov constraints (instead of
the original trajectory-based constraint) in the optimization w.r.t. policy parameter θ, because the
feasibility set is characterized by infinite dimensional constraints. Rather, leveraging the ideas of a
safety layer from Dalal et al. (2018) that was applied to single-step constraints, we propose a novel
approach to embed the set of Lyapunov constraints into the policy network. In this way, one refor-
mulates the CMDP problem into an unconstrained one, whose policy parameter θ (of the augmented
network) can then be optimized by any standard unconstrained PG algorithms. At every given state,
the unconstrained action is first computed and is then passed through the safety layer, where a fea-
sible action mapping is constructed by projecting the unconstrained actions onto the feasibility set
w.r.t. the corresponding Lyapunov constraint. Therefore, safety during training w.r.t. the original
CMDP problem is guaranteed by the Lyapunov theorem.
We hereby demonstrate how one can find a feasible action mapping using the safety layer with
DDPG, whose role is to solve the following projection problem at given state x ∈ X :
a*(x) ∈ argmin{2Ila ― ∏θ,unc(x)∣∣2 : (a ― ∏B(x))>V°Ql(x, a) ∣a=∏B(x)≤ e(x)} ∙	(6)
In the above optimization problem, πθ,unc is the unconstrained policy whose policy parameter is up-
dated by standard DDPG, πB is the current data-generation policy that is safe, and the left side of the
5
Under review as a conference paper at ICLR 2019
constraint is the first-order Taylor series approximation of a∈A QL(x, πθ(x)) - QL(x, πB(x))da
over actions, w.r.t. the action πB (x) induced by the baseline policy. As in Section 4.1, since the
auxiliary cost e is state-dependent, one can readily find NaQL(x, a) ∣a=∏B(x) by computing the
gradient of the constraint action value function NaQD (x, a) ∣a=∏B(x). Generally, the safety layer
perturbs the unconstrained action as little as possible in the Euclidean norm in order to satisfy the
Lyapunov constraints. Notice that the objective function is positive-definite and quadratic and the
constraint approximation is linear. Therefore, we can find the global solution to this convex problem
effectively via an in-graph iterative QP-solver, such as the one from Amos & Kolter (2017). Further-
more, since the optimization in Equation 6 only has a single Lyapunov constraint, one can express
a*(x) using the following analytical solution.
Proposition 1. At any given state x ∈ X, the solution to the optimization problem in Equation 6
has thefollowingform: a* (x) = ∏θ,Unc(x) + λ*(x)N0QL(x, a) ∣a=∏B(χ), where
λ* (X) = (((^aQL (X, a) L = ∏b (x))> (πθ, Imc(X) — πB (X)) — e(X)) / PaQL(x, a) |a = nR (x))> ^aQL (X, a) |a = nR (x)^
The closed-form solution is essentially a linear projection of the unconstrained action πθ,unc(x) to the
safe hyperplane characterized with slope NaQL(x, a) ∣a=∏B(x) and intercept e(x) = (1 一 Y)(d0 —
DπB (x0)). Implementing a*(x) is very simple; it only consists of several arithmetic operations such
as matrix products and ReLU. Extending this closed-form solution to handle multiple constraints is
possible, under the assumption of having at most one constraint active at a time.
In general, the safety layer approach can also be applied to policy gradient algorithms, such as PPO,
that learn a non-deterministic policy. For example in the PPO case when the policy is parameter-
ized with a Gaussian distribution, then one simply need to project both the mean and the standard-
deviation vector, in order to obtain a feasible action probability. The Lyapunov-based algorithms
with a-projection in safety layer is summarized by Algorithm 5 in Appendix A. In the experiment
section, we denote the DDPG version and the PPO version of this algorithm by SDDPG-modular
and SPPO-modular respectively.
5	Experiments
We empirically validate the Lyapunov-based PG algorithms on several robot locomotion continuous
control tasks. In these experiments we aim to address the following questions about our proposed
algorithm: (i) How is the performance (in terms of cost and safety during training) of Lyapunov-
based PG compared to other baseline methods such as CPO and the naive Lagrangian approach? (ii)
In the presence of approximation errors in value functions and policies, how robust is Lyapunov-
based PG algorithm with respect to constraint violations?
To understand the performance of these algorithms in terms of both cost and safety guarantees, we
designed several interpretable experiments in simulated robot locomotion continuous control tasks,
whose notions of safety are motivated by physical constraints. We consider three domains using the
MuJoCo simulator (Todorov et al., 2012) with a variety of different agents: (i)HalfCheetah-Safe:
The HalfCheetah agent is rewarded for running, but its speed is limited for stability and safety; (ii)
Point-Circle: The Point agent is rewarded for running in a wide circle, but is constrained to stay
within a safe region defined by |x| ≤ Xlim (AChiam et al., 2017); (iii) POint-Gather & Ant-Gather:
The agent, which is either a Point or an Ant, is rewarded for collecting target objects in a terrain
map, while being constrained to avoid bombs (Achiam et al., 2017).
Visualizations of these tasks as well as more detailed descriptions (immediate cost and constraint
cost functions, constraint thresholds) are given in Appendix C. In these experiments there are three
different agents: (1) a point-mass (X ⊆ R9, A ⊆ R2); an ant quadruped robot (X ⊆ R32, A ⊆ R8);
(3) a half-cheetah (X ⊆ R18, A ⊆ R6). For all experiments, we use two neural networks with
two hidden layers of size (100, 50) and ReLU activations to model the mean and log-variance of
the Gaussian actor policy, and two neural networks with two hidden layers of size (200, 50) and
tanh activations to model the critic and constraint critic. To build a low variance sample gradient
estimate, we use GAE-λ (Schulman et al., 2015b) to estimate the advantage and constraint advantage
functions, with a hyper-parameter λ ∈ (0, 1) optimized by grid-search.
5.1 Comparison with Unconstrained PG, Lagrangian Approach, and CPO
For comparison purposes, we implement the naive Lagrangian approach. Details of this algorithm
are available in Appendix B. For fair comparisons, we also optimize the Lagrangian using a natural
6
Under review as a conference paper at ICLR 2019
policy gradient when needed. To understand the optimal unconstrained performance on these envi-
ronments, we also include the learning performance of two state-of-the-art unconstrained reinforce-
ment learning algorithms, namely DDPG (Lillicrap et al., 2015) and PPO (Schulman et al., 2017). In
the PPO case, we aim to compare the performance of CPO (which is equivalent to Lyapunov-based
PG with θ-projection) with Lyapunov-based PG (with a-projection) . Notice that the original imple-
mentation of CPO in Achiam et al. (2017) is based on TRPO (Schulman et al., 2015a). Instead of
the original CPO algorithm, we use its PPO alternative (which coincides with the SPPO algorithm
derived in Section 4.1) as the safe RL baseline for comparison. SPPO preserves the essence of CPO
by adding a 1st order constraint to the proximal policy optimization. The main difference between
CPO and SPPO is that the latter does not perform backtracking line-search. The decision to compare
with SPPO instead of CPO is 1) to avoid the additional computational complexity of line-search in
TRPO, while maintaining the performance of PG using the popular PPO algorithm, 2) to have a
back-propagatable version of CPO, and 3) to have a fair comparison with other back-propagatable
safe RL algorithms, such as the DDPG and safety layer counterparts.
HalfCheetah-Safe, Return
HalfCheetah-Safe, Constraint
Point-Gather, Return
Point-Gather, Constraint
IO-1
O IO 20	30	40	50
Figure 1:	Results of various DDPG algorithms on safe robot locomotion tasks, with x-axis in
thousands of episodes. We include runs from DDPG (red), DDPG-Lagrangian (magenta), SD-
DPG (blue), SDDPG-modular (green) on HalfCheetah-Safe and Point-Gather. We discover that
the Lyapunov-based approaches can perform safe learning, despite the fact that the environment dy-
namics model and cost functions are not known, control actions are continuous, and deep function
approximations are necessary.
HalfCheetah-Safe, Return
HalfCheetah-Safe, Constraint
Point-Gather, Return
Point-Gather, Constraint
Figure 2:	Results of various PPO algorithms on safe robot locomotion tasks, with x-axis in thou-
sands of episodes. We include runs from PPO (red), PPO-Lagrangian (magenta), SPPO (blue),
SPPO-modular (green) on HalfCheetah-Safe and Point-Gather. Similar to Figure 1, the Lyapunov-
based approaches can perform safe learning in the control tasks when function approximations on
policies and value functions are necessary.
Learning curves for unconstrained DDPG, Lagrangian DDPG, SDDPG and SDDPG-modular are
shown in Figure 1, and the learning curves for unconstrained PPO, Lagrangian PPO, SPPO and
SPPO-modular are shown in Figure 6. Due to space restrictions, more results are included in
Appendix C. From the comparison plots, one can clearly see that the unconstrained DDPG and
PPO agents are constraint-violating in these environments. Although the Lagrangian approaches in
DDPG and PPO converge to feasible policies with reasonably good performance, in accord with our
earlier claims these algorithms cannot guarantee constraint satisfaction during training. Furthermore
it is worth-noting that the Lagrangian approach can be sensitive to the initialization of the Lagrange
multiplier λ0 . If λ0 is too large, it would make policy updates overly conservative, while if λ0 is too
small then constraint violation will be more pronounced. By default, we initialize λ0 = 10, which
assumes no knowledge about the environment.
Generally in both DDPG and PPO experiments, the Lyapunov-based PG algorithms lead to more
stable learning and constraint satisfaction than the Lagrangian approach. The Lyapunov approaches
quickly stabilize the constraint cost to be below the threshold, while the constraint costs from the
Lagrangian approach tend to jiggle around the threshold. In many cases the a-projection Lyapunov-
based PG (SDDPG-modular, SPPO-modular) converges faster than the θ-projection counterpart
(SDDPG, SPPO). This corroborates with the hypothesis that the a-projection approach is less con-
servative during policy updates than the θ-projection approach (which is what CPO is based on).
7
Under review as a conference paper at ICLR 2019
Finally, in most experiments (HalfCheetah, PointGather, and AntGather) the DDPG class of algo-
rithms tends to have faster learning than the PPO counterpart. This is potentially due to the improved
data-efficiency when using off-policy samples in PG updates. Although this benefit is not directly
related to the addition of Lyapunov constraints, this also supports our claim that some Lyapunov-
based safe PG algorithms (SDDPG, SDDPG-modular) can learn more effectively than CPO (which
is analogous to SPPO).
5.2 Constraint Violation During Training
Due to function approximation errors in policies and value functions, in practice most safe learning
algorithms including the Lyapunov-based PG methods may still take abad step and lead to constraint
violation. While methods like safeguard policy update and constraint tightening from Section A.1
might help to remedy this issue, it is still unclear how robust each algorithm is regarding constraint
satisfaction during training. To study the degree of constraint violation in different PG algorithms,
we show their constraint violation plots in Figure 3 for DDPG-based and PPO-based algorithms. To
make the comparison fair, we apply both safeguard policy update and the constraint tightening to
all safe PG algorithms. From the constraint violation plots, it is clear that the Lagrangian methods
(both DDPG and PPO) violate safety constraints more often than the Lyapunov-based counterparts.
In many cases the Lyapunov-based PG algorithms with a-projection have lower constraint violation
than its θ-projection counterparts. We speculate this is due to the fact that the safety layer generates
smoother gradient updates during end-to-end training.
HalfCheetah-Safe, DDPG
HalfCheetah-Safe, PPO
Point-Gather, DDPG
Point-Gather, PPO
Figure 3:	Constraint violations (logarithmic scale, i.e., log(1 + ccv), where ccv represents the area
under constraint learning curve that is above the constraint threshold) of various PG algorithms on
safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG
(red), DDPG-Lagrangian (magenta), SDDPG (blue), SDDPG-modular (green), PPO (red), PPO-
Lagrangian (magenta), SPPO (blue), SPPO-modular (green) on HalfCheetah-Safe and Point-Gather.
Compared with Lagrangian approach, we discover that the Lyapunov-based approaches generally
have more stable and safe learning, which lead to lower constraint violations.
6 Conclusions
In this paper, we formulated the problem of safe RL as a CMDP and used the notion of Lyapunov
function to develop policy optimization algorithms that learn policies that are both safe and have low
expected cumulative cost. Our algorithms extend the Lyapunov-based approach to solving CMDPs
of Chow et al. (2018) to continuous action problems. Our algorithms combine DDPG or PPO (or any
other on-policy or off-policy policy optimization algorithm) with a critic that is inspired by the safe
policy iteration (SPI) algorithm of Chow et al. (2018) and both evaluates the policy and computes its
corresponding Lyapunov function. The computed Lyapunov function is then used to guarantee safe
policy update. We can categorize our algorithms into two classes in terms of the way they perform
safe policy update. The first class is based on constrained optimization w.r.t. policy parameter,
similar to what is done in CPO. The second class relies on the safety layer concept (Dalal et al., 2018)
that integrates the Lyapunov constraints into the policy network by adding an action projection layer
to it. We evaluated our algorithms on four high-dimensional simulated robot locomotion tasks and
compared them with CPO (Achiam et al., 2017) and the Lagrangian method in terms of minimizing
the expected cumulative return and constraint violation during training. Our results indicate that
our Lyapunov-based algorithms 1) achieve safe learning, 2) have better data-efficiency, and 3) can
be more naturally integrated within the standard end-to-end differentiable policy gradient training
pipeline. In general, our work is a step forward in deploying RL to real-world problems in which
safety guarantees are of paramount importance.
Future work includes 1) developing more stable and safe learning algorithms by further exploiting
the properties of Lyapunov functions, 2) exploring more efficient ways to include Lyapunov con-
straints in constrained policy optimization, and 3) applying the Lyapunov-based PG algorithms to
real-world continuous control problems, particularly in robotics.
8
Under review as a conference paper at ICLR 2019
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv
preprint arXiv:1705.10528, 2017.
E. Altman. Constrained Markov decision processes with total cost criteria: Lagrangian approach
and dual linear program. Mathematical methods ofoperations research, 48(3):387-417, l998.
E.	Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane. Concrete problems in
ai safety. arXiv preprint arXiv:1606.06565, 2016.
B. Amos and Z. Kolter. Optnet: Differentiable optimization as a layer in neural networks. arXiv
preprint arXiv:1703.00443, 2017.
F.	Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learn-
ing with stability guarantees. In Advances in Neural Information Processing Systems, pp. 908-
918, 2017.
D. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
D. Bertsekas. Dynamic programming and optimal control, volume 1-2. Athena scientific Belmont,
MA, 2005.
Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning
with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070-6120, 2017.
Y. Chow, O. Nachum, M. Ghavamzadeh, and E. Duenez-Guzman. A Lyapunov-based approach to
safe reinforcement learning. In Accepted at NIPS, 2018.
G.	Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa. Safe exploration in
continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
P. Geibel and F. Wysotzki. Risk-sensitive reinforcement learning applied to control under con-
straints. Journal of Artificial Intelligence Research, 24:81-108, 2005.
H.	Khalil. Noninear systems. Prentice-Hall, New Jersey, 2(5):5-1, 1996.
T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
M. Neely. Stochastic network optimization with application to communication and queueing sys-
tems. Synthesis Lectures on Communication Networks, 3(1):1-211, 2010.
T. Perkins and A. Barto. Lyapunov design for safe reinforcement learning. Journal of Machine
Learning Research, 3(Dec):803-832, 2002.
K. Regan and C. Boutilier. Regret-based reward elicitation for markov decision processes. In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 444-
451. AUAI Press, 2009.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
International Conference on Machine Learning, pp. 1889-1897, 2015a.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
9
Under review as a conference paper at ICLR 2019
R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Proceedings of Advances in Neural Information Pro-
cessing Systems 12,pp. 1057-1063, 2000.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
E. Uchibe and K. Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards. In
International Conference on Development and Learning, pp. 163-168. IEEE, 2007.
10
Under review as a conference paper at ICLR 2019
A Details of the Safe Policy Gradient Algorithms
Algorithm 1 Safe Policy Iteration (SPI)
Input: Initial feasible policy π0 ;
for k = 0, 1, 2, . . . do
Step 0: With πb = πk , evaluate the Lyapunov function Lk , where k is a solution of Equation 3
Step 1: Evaluate the cost value function Vπk (x) = Cπk (x); Then update the policy by solving the
following problem: ∏k+ι(∙∣x) ∈ argmin∏∈FL ⑺ T∏,c[V∏k](x),∀x ∈ X
Lk
end for
Return Final policy ∏k*
Algorithm 2 Trajectory-based Policy Gradient Algorithm for CMDP
Input: parameterized policy π3∙;θ)
Initialization: policy parameter θ = θ0 , and the Lagrangian parameter λ = λ0
while TRUE do
for i = 0, 1, 2, . . . do
for j = 1, 2, . . . do
Generate N trajectories {ξj,i }jN=1 by starting at x0 and following the policy θi .
end for	1 N
θ Update： θi+ι = θi - α2,iN N Ng log Pθ(ξj,i)∣θ=θi (C(ξj,i) + %D(ξj,i))
1N
,i( 一 do + N >jD(ξj,i)
j=1
λ Update: λi+1 = ΓΛ λi + α1,
end for
if {λi } converges to λmax then
Set λmaχ4-2λmaχ.
else
return parameters ν, θ, λ and break
end if
end while
Algorithm 3 Actor-Critic Algorithms for CMDP
Input: Parameterized policy ∏(∙∣∙; θ) and value function feature vector φ(∙)
Initialization: policy parameters θ = θ0 ; Lagrangian parameter λ = λ0 ; value function weight v = v0
while TRUE do
for k = 0, 1, 2, . . . do
Sample ak 〜π(∙∣Xk; θk); Cλk (xk,ak) = C(xk,ak) + λkD(xk,ak); Xk+ι 〜P(∙∣Xk,ak);
// AC Algorithm:
TD Error:	δk(vk)	= Cλk (xk , ak) + γVφk (xk+1) — Vφk (xk)	(7)
Critic Update:	vk+1 =	vk + ζ3 (k)δk (vk)ψ(xk)	(8)
θ Update:	θk+1 =	θk — Z2(k)Vθ log∏θ(ak∖xk) ∙ δk(vk)/1 — Y	(9)
λ Update:	λk+1 =	N 二 γλ (λk + ζι(k)( — do + N X D(ξj,i)))	(10)
// NAC Algorithm:		j=1	
Critic Update: Wk+ι =(I — Z3(k)Vθ logπ(ak∣Xk)∣θ=θk (Vg logπ(ak∖xk)∣θ=θk)>) Wk
+ Z3(k)δk(vk)Vθ log∏θ(ak∖xk)∖θ=θk
(11)
θ Update: θk+1 = θk — ζ2 (k)wk/1 — γ
(12)
Other Updates: Follow from Eqs. 7, 8, and 10.
end for
end while
A.1 Practical Implementations of Safe PG
Due to function approximation errors, even with the Lyapunov constraints in practice the safe
PG algorithm may take a bad step and produce an infeasible policy update and cannot auto-
matically recover from such a bad step. To tackle this issue, similar to Achiam et al. (2017)
11
Under review as a conference paper at ICLR 2019
Algorithm 4 Lyapunov-based Policy Gradient with θ-projection (SDDPG and SPPO)
Input: Initial feasible policy π0 ;
for k = 0, 1, 2, . . . do
Step 0: With πb = πθk , generate N trajectories {ξj,k }jN=1 of T steps by starting at x0 and following the
policy θk
Step 1: Using the trajectories {ξj,k}jN=1, estimate the critic Qθ (x, a) and the constraint critic QD,θ (x, a);
•	For DDPG, these functions are trained by minimizing the MSE of Bellman residual, and one
can also use off-policy samples from replay buffer (Schaul et al., 2015);
•	For PPO these functions can be estimated by the generalized advantage function technique from
Schulman et al. (2015b)
Step 2: Based on the closed form solution of a QP problem with an LP constraint in Section 10.2 of
AChiam et al. (2017), calculate λk With the following formula:
λ* = - -βke- (VθQθ(x,a) ∣θ=θk)> H(θk)-,θQD,θ(x,a) ∣θ=θk ∖
k [	(VθQd,θ(x,a) ∣θ=θk)> H(θk)-1VθQd,θ(x,a) ∣θ=θk	1+
where
1	T-1
v θ qθ (X, a) = N	γtV
Θ log∏θ(α∣x)Qθ(x,α),
x,a∈ξj,k,1≤j≤N t=0
1	T-1
VθQD,θ (χ,%) = N	γtVθ log∏Θ(a∣x)Qθ(x,α),
x,a∈ξj,k,1≤j ≤N t=0
βk is the adaptive penalty weight of the DKL(π∣∣∏θk) regularize" and H(θk) = V2Dkl(π∣∣∏θ) ∣θ=θk is
the Hessian of this term
Step 3: Update the policy parameter by following the objective gradient;
• For DDPG
θk + 1 &- θk-αk ∙ ~N~T	X :	V θ πθ(x) |e = 0k ∙(VaQθk (x,a) + λk VaQD,θk (x,a)) |& =冗@仁(x)
•	x∈ξj,k,1≤j≤N
• For PPO,
T-1
θk + 1 - θk - Ne- (H(θk)) 1 X	XYt • vθ log πθ(aj,t|xj,t) lθ=θk •
k	xj,t,aj,t∈ξj,k,1≤j≤N t=0
(Q θk (Xj,t,aj,t) + λk QD,θk (Xj,t,aj,t))
Step 4: At any given state X ∈ X, compute the feasible action probability a* (x) via action projec-
tion in the safety layer, that takes inputs VaQL(X, a) = Va QD,θk (X, a) and (X) = (1 - γ)(d0 -
QD,θk (X0, πk(X0))), for any a ∈ A.
end for
Return Final policy ∏θχ*,
12
Under review as a conference paper at ICLR 2019
Algorithm 5 Lyapunov-based Policy Gradient with a-projection (SDDPG-modular and SPPO-
modular)
Input: Initial feasible policy π°;
for k = 0, 1, 2, . . . do
Step 0: With πb = πθk , generate N trajectories {ξj,k }jN=1 of T steps by starting at x0 and following the
policy θk
Step 1: Using the trajectories {ξj,k}jN=1, estimate the critic Qθ (x, a) and the constraint critic QD,θ (x, a);
•	For DDPG, these functions are trained by minimizing the MSE of Bellman residual, and one
can also use off-policy samples from replay buffer (Schaul et al., 2015);
•	For PPO these functions can be estimated by the generalized advantage function technique from
Schulman et al. (2015b)
Step 2: Update the policy parameter by following the objective gradient;
•	For DDPG
θk+1 4- θk - αk ∙ N T X :	R θ πθ (X) | θ=θk，N aQ θ k (x,a) | a = ∏θk (x);
•	x∈ξj,k ,1≤j≤N
• For PPO,
T-1
θk+ι J θk- Nk- (H (θk)) T	X	X Ze log πθ(aj,tlxj,t) lθ=θk ∙Qθk (Xj,t,aj,t)
k	xj,t,aj,t∈ξj,k,1≤j≤N t=0
where βk is the adaptive penalty weight of the DκL(π∣∣∏θk) regularize], and H ®) =
RθDkl(∏∣∣∏θ) ∣e=θk is the Hessian of this term
Step 3: At any given state X ∈ X, compute the feasible action probability a* (x) via action ProjeC-
tion in the safety layer, that takes inputs RaQL(X, a) = Ra QD,θk (X, a) and (X) = (1 - γ)(d0 -
QD,θk (X0, πk(X0))), for any a ∈ A.
end for
Return Final policy ∏θχ*,
we propose the following safeguard policy update rule to purely decrease the constraint cost:
θk+ι = θk - αsg,kVθD∏θ (xo)θ=θk, where asg,k is the learning rate for safeguard update. If
αsg,k >> αk (learning rate of PG), then with the safeguard update θ will quickly recover from
the bad step but it might be overly conservative. This approach is principled because as soon as πθk
is unsafe/infeasible w.r.t. CMDP, the algorithm uses a limiting search direction. One can directly
extend this safeguard update to the multiple-constraint scenario by doing gradient descent over the
constraint that has the worst violation. Another remedy to reduce the chance of constraint violation
is to do constraint tightening on the constraint cost threshold. Specifically, instead of d0, one may
pose the constraint based on do ∙ (1 - δ), where δ ∈ (0,1) is the factor of safety for providing
additional buffer to constraint violation. Additional techniques in cost-shaping have been proposed
in Achiam et al. (2017) to smooth out the sparse constraint costs. While these techniques can fur-
ther ensure safety, construction of the cost-shaping term requires knowledge from the environment,
which makes the safe PG algorithms more complicated.
13
Under review as a conference paper at ICLR 2019
B	Lagrangian Approach to Safe RL
There are a number of mild technical and notational assumptions which we will make throughout
this section, so we state them here:
Assumption 1 (Differentiability). For any state-action pair (x, a), πθ (a|x) is continuously differ-
entiable in θ and Vθ∏θ(a|x) is a Lipschitzfunction in θ for every a ∈ A and X ∈ X.
Assumption 2 (Strict Feasibility). There exists a transient policy ∏θ (∙∣x) such that D∏θ (xo) < do
in the constrained problem.
Assumption 3 (Step Sizes). The step size schedules {α3,k}, {α2,k}, and {α1,k} satisfy
α1,k k	α2,k =	α3,k = ∞,	(13)
Xα21,k, l^	α22,k,	α23,k < ∞,	(14)
α1,k = o α2,k ,	ζ2(i) = o α3,k .		(15)
Assumption 1 imposes smoothness on the optimal policy. Assumption 2 guarantees the existence
of a local saddle point in the Lagrangian analysis introduced in the next subsection. Assumption 3
refers to step sizes corresponding to policy updates that will be introduced for the algorithms in this
paper, and indicates that the update corresponding to {α3,k} is on the fastest time-scale, the updates
corresponding to {α2,k} is on the intermediate time-scale, and the update corresponding to {α1,k}
is on the slowest time-scale. As this assumption refer to user-defined parameters, they can always
be chosen to be satisfied.
To solve the CMDP, we employ the Lagrangian relaxation procedure (Bertsekas, 1999) to convert it
to the following unconstrained problem:
max min ( L(θ,λ) 4 C∏θ (xo) + λ ① ∏ (xo) - do) ),	(16)
λ≥0 θ
where λ is the Lagrange multiplier. Notice that L(θ, λ) is a linear function in λ. Then there exists a
local saddle point (θ*, λ*) for the minimax optimization problem maxλ≥o minθ L(θ, λ), such that
for some r > 0, ∀θ ∈ RK ∩ Bg*(r) and ∀λ ∈ [0, λmaχ], We have
L(θ,λ*) ≥ L(θ*,λ*) ≥ L(θ*,λ),	(17)
where Bθ*(r) is a hyper-dimensional ball centered at θ* with radius r > 0.
In the folloWing, We present a policy gradient (PG) algorithm and an actor-critic (AC) algorithm.
While the PG algorithm updates its parameters after observing several trajectories, the AC algo-
rithms are incremental and update their parameters at each time-step.
We now present a policy gradient algorithm to solve the optimization problem Equation 16. The
idea of the algorithm is to descend in θ and ascend in λ using the gradients of L(θ, λ) w.r.t. θ and λ,
i.e.,
VθL(θ, λ) = Vθ (C∏θ(xo) + λD∏θ(xo)), VλL(θ, λ) = D∏(xo) - do.	(18)
The unit of observation in this algorithm is a system trajectory generated by following policy πθk .
At each iteration, the algorithm generates N trajectories by following the current policy, uses them
to estimate the gradients in Equation 18, and then uses these estimates to update the parameters θ, λ.
Let ξ = {xo, ao, co, x1, a1, c1, . . . , xT-1, aT-1, cT-1, xT } be a trajectory generated by following
the policy θ, where xT = xTar is the target state of the system and T is the (random) stopping
time. The cost, constraint cost, and probability of ξ are defined as C(ξ) = PkT=-o1 γkC(xk, ak),
D(ξ) = PT-o YkD(Xk,ak), and Pθ(ξ) = Po(xo) QT-； ∏θ® M)P(xk+ι∣xk,ak), respectively.
Based on the definition of Pθ(ξ), one obtains Vθ log Pθ(ξ) = PT-(1 Vθ log∏θ(ak∣Xk).
Algorithm 2 contains the pseudo-code of our proposed policy gradient algorithm. What appears
inside the parentheses on the right-hand-side of the update equations are the estimates of the gradi-
ents of L(θ, λ) w.r.t. θ, λ (estimates of the expressions in 18). Gradient estimates of the Lagrangian
14
Under review as a conference paper at ICLR 2019
function are given by
VθL(θ,λ) = £Pe(ξ) ∙Vθ log Pθ(ξ) (C∏θ (ξ) + λD∏° (ξ)),
ξ
VλL(θ,λ) = -do + X Pθ(ξ) ∙D(ξ),
ξ
where the likelihood gradient is
(T-1
Elog P (xk+ι∣Xk ,ak) + log ∏θ (ak |xk )+log1{xo = x0}
k=0
T-1	T-1	1
=E Vθ log∏θ(ak|xk) = E -V^^I^^yVθ∏θ(ak|xk).
k=0	M πθ(ak Ixk)
In the algorithm, Γλ is a projection operator to [0, λmaχ], i.e., Γλ(X) = argmin^∈[o 入 ]∣∣λ 一
λ∣22, which ensures the convergence of the algorithm. Recall from Assumption 3 that the step-
size schedules satisfy the standard conditions for stochastic approximation algorithms, and ensure
that the policy parameter θ update is on the fast time-scale ζ2,i , and the Lagrange multiplier λ
update is on the slow time-scale ζ1,i . This results in a two time-scale stochastic approximation
algorithm, which has shown to converge to a (local) saddle point of the objective function L(θ, λ).
This convergence proof makes use of standard in many stochastic approximation theory, because in
the limit when the step-size is sufficiently small, analyzing the convergence of PG is equivalent to
analyzing the stability of an ordinary differential equation (ODE) w.r.t. its equilibrium point.
In policy gradient, the unit of observation is a system trajectory. This may result in high variance
for the gradient estimates, especially when the length of the trajectories is long. To address this
issue, we propose two actor-critic algorithms that use value function approximation in the gradient
estimates and update the parameters incrementally (after each state-action transition). We present
two actor-critic algorithms for optimizing Equation 16. These algorithms are still based on the
above gradient estimates. Algorithm 3 contains the pseudo-code of these algorithms. The projection
operator ΓΛ is necessary to ensure the convergence of the algorithms. Recall from Assumption 3 that
the step-size schedules satisfy the standard conditions for stochastic approximation algorithms, and
ensure that the critic update is on the fastest time-scale α3,k , the policy and θ-update α2,k is on
the intermediate timescale, and finally the Lagrange multiplier update is on the slowest time-scale
α1,k . This results in three time-scale stochastic approximation algorithms.
Using the policy gradient theorem from Sutton et al. (2000), one can show that
1
1 一 Y
VθL(θ, λ) = VθVθ(x0)
^μθ(x, a∣xo) V log ∏(a∣x) Qθ(x, a),
x,a
(19)
where μθ is the discounted visiting distribution and Qθ is the action-value function of policy θ. We
can show that y-γ V log ∏θ (ak∣xk) ∙ δk is an unbiased estimate of VθL(θ, λ), where
C /ɜ (	∖ K TΛ/ ∖ τ≥/ ʌ
δk = Cλ(xk,ak) + γV (xk+1) 一 V (xk)
....	I	/f、	ICT	I	..	…
is the temporal-difference (TD) error, and V is the value estimator of Vθ .
Traditionally, for convergence guarantees in actor-critic algorithms, the critic uses linear approxima-
tion for the value function Vq(x) ≈ φ>ψ(x) = Vθ,φ(x), where the feature vector ψ(∙) belongs to a
low-dimensional space Rκ2. The linear approximation Vθ,φ belongs to a low-dimensional subspace
SV = {Ψφ∣φ ∈ Rκ2}, where Φ is a short-hand notation for the set of features, i.e., Ψ(x) = ψ>(x).
Recently with the advances of deep neural networks, it has become increasingly popular to model
the critic with a deep neural network architecture, based on the objective function of minimizing the
MSE of Bellman residual w.r.t. Vθ or Qθ (Mnih et al., 2013).
15
Under review as a conference paper at ICLR 2019
C Experimental Setup
Our experiments are performed on safety-augmented versions of standard MuJoCo do-
mains (Todorov et al., 2012).
HalfCheetah-Safe. The agent is a the standard HalfCheetah (a 2-legged simulated robot rewarded
for running at high speed) augmented with safety constraints. We choose the safety constraints to
be defined on the speed limit. We constrain the speed to be less than 1, i.e., constraint cost is thus
1[|v| > 1] . Episodes are of length 200. The constraint threshold is 50.
Point Circle. This environment is taken from (Achiam et al., 2017). The agent is a point mass
(controlled via a pivot). The agent is initialized at (0, 0) and rewarded for moving counter-clockwise
along a circle of radius 15 according to the reward ^-dx∙y+dy工∣, for position χ, y and velocity
dx, dy. The safety constraint is defined as the agent staying in a position satisfying |x| ≤ 2.5. The
constraint cost is thus 1[|x| > 2.5]. Episodes are of length 65. The constraint threshold is 7.
Point Gather. This environment is taken from (Achiam et al., 2017). The agent is a point mass
(controlled via a pivot) and the environment includes randomly positioned apples (2 apples) and
bombs (8 bombs). The agent given a reward of 10 for each apple collected and a penalty of -10
for each bomb. The safety constraint is defined as number of bombs collected during the episode.
Episodes are of length 15. The constraint threshold is 2.
Ant Gather. This environment is is the same as Point Circle, only with an ant agent (quadrapedal
simulated robot). Each episode is initialized with 8 apples and 8 bombs. The agent given a reward
of 10 for each apple collected, a penalty reward of -20 for each bomb collected, and a penalty
reward of -20 is incurred if the episode terminates prematurely (because the ant falls). Episodes
are of length at most 500. The constraint threshold is 10 for DDPG algorithms and is 5 for PPO
algorithms.
Figure 4 shows the visualization of the above domains used in our experiments.
HalfCheetah-Safe
Figure 4: The Robot Locomotion Control Tasks
Ant-Gather
Point-Gather
On top of the GAE parameter λ, in all numerical experiments and for each algorithm (SPPO, SD-
DPG, SPPO-modular, SDDPG-modular, CPO, Lagrangian, and the unconstrained PG counterparts),
we systematically explored different parameter settings by doing grid-search over the following fac-
tors: (i) learning rates in the actor-critic algorithm, (ii) batch size, (iii) regularization parameters of
the policy relative entropy term, (iv) with-or-without natural policy gradient updates, (v) with-or-
without the emergency safeguard PG updates (see Appendix A.1 for more details). Although each
algorithm might have a different parameter setting that leads to the optimal performance in training,
the results reported here are the best ones for each algorithm, chosen by the same criteria (which is
based on value of return plus certain degree of constraint satisfaction). To account for the variability
during training, in each learning curve a 95% confidence interval is also computed over 10 separate
random runs (under the same parameter setting).
C.1 Additional Experimental Results
In this section, we include more experimental results of various PG algorithms on safe robot loco-
motion tasks.
16
Under review as a conference paper at ICLR 2019
Point-Circle
Illl
14-
Ant-Gather
30
—25
—20
15
10
O
&
odαɑ
Figure 5: Constraint violations (logarithmic scale, i.e., log(1 + ccv)) of various PG algorithms on
safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG
(red), DDPG-Lagrangian (magenta), SDDPG (blue), SDDPG-modular (green), PPO (red), PPO-
Lagrangian (magenta), SPPO (blue), SPPO-modular (green) on Ant-Gather and Point-Circle.
Ant-Gather
Point-Circle
Figure 6: Results of various PG algorithms on safe robot locomotion tasks, with x-axis in thousands
of episodes. We include runs from PPO (red), PPO-Lagrangian (magenta), SPPO (blue), SPPO-
modular (green) on Ant-Gather and Point-Circle.
UJmOx JUIEi3SUOD
17
Under review as a conference paper at ICLR 2019
Point-Circle
UJmOx JUIEi3SUOD
Ant-Gather
Illlll
1400 -	-
：o：l ⅝⅜
800-	-
600- Z	-
400 -	-
Ul	I	I	I	I	Γ
0	10	20	30	40	50
O 5 IO 15	20	25	30
Figure 7: Results of various PG algorithms on safe robot locomotion tasks, with x-axis in thousands
of episodes. We include runs from DDPG (red), DDPG-Lagrangian (magenta), SDDPG (blue),
SDDPG-modular (green) on Ant-Gather and Point-Circle.
18