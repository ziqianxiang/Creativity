Under review as a conference paper at ICLR 2019
Revisting negative transfer using
ADVERSARIAL LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
An unintended consequence of feature sharing is the model fitting to correlated
tasks within the dataset, termed negative transfer. In this paper, we revisit the
problem of negative transfer in multitask setting and find that its corrosive effects
are applicable to a wide range of linear and non-linear models, including neural
networks. We first study the effects of negative transfer in a principled way and
show that previously proposed counter-measures are insufficient, particularly for
trainable features. We propose a adversarial training approach to mitigate the
effects of negative transfer by viewing the problem in a domain adaptation setting.
Finally, empirical results on attribute prediction multi-task on AWA and CUB
datasets further validate the need for correcting negative sharing in an end-to-end
manner.
1	Introduction
Advances in machine learning have led to proficient supervised learning models with powerful rep-
resentations in various prediction tasks. We now expect an ideal classification model to restrict itself
to a pertinent set of evidences available to it from the input for prediction. Further, we expect the
model to disregard any unrelated evidences in the data to enable better generalization.
(a) typical sample (b) typical sample	(c) rare sample
(d) rare sample
Figure 1: A supervised classifier ‘cheetah vs. snow-leopard’ that uses unrelated evidence (of
habitat) over relevant evidence (of fur patterns). As shown by the pixel importance maps, the model
suffers from the negative transfer prevalent in a typical animal image dataset skewed towards the
animal’s typical habitat and fails to generalize to rare samples.
Let us consider the task of training an animal classifier “cheetah vs. snow-leopards” from a dataset
of images of these animals, such as those illustrated in Figure 1 - a task which ideally should
focus on the animal’s appearance features. However, a large portion of these images also contain
various cues of the typical habitat of the animals in the background, i.e., tall grass and snow (see
Figures 1 (a) and (b)) which are, in principle, unrelated to the animal’s appearance. An archetypal
model is deceived by the co-occurrence of such unrelated, yet easily detectable cues of habitat over
the animal’s appearance features such as complex fur patterns. However, a proficient supervised
learning model must identify relevant evidences for the label of interest and at the same time discard
various unrelated evidences such as presence of snow, even though it tends to co-occur frequently
with snow-leopard. Consequently, it would be more likely that such a model would perform better
on rare-instances (such as those in Figures 1 (c) and (d)) and generalize better to unseen instances.
1
Under review as a conference paper at ICLR 2019
This phenomenon of co-occurring but unrelated evidences being present in training data and thereby
having a debilitating effect on model performance has been described in literature (Jayaraman et al.
(2014); Zhou et al. (2013; 2010); Romera-Paredes et al. (2012); Yoon & Hwang (2017); Zhou et al.
(2011)). These techniques utilize the easy availability of labels for unrelated evidences (e.g. back-
ground habitat labels above), called negative labels which constitutes an auxiliary task, and seek
to mitigate its debilitating performance on the primary task (e.g. animal classification above) with
techniques referred to as negative-sharing or negative-transfer.
While all of these techniques have tackled this problem utilizing various forms of regularization,
we describe several shortcomings of this class of approaches, most notable of which is their inap-
plicability to the popular paradigm of trainable features obtained via neural representation learning.
Motivated by these limitations, in this paper we depart from the direction of regularization-based
approaches and examine methods inspired from a domain-adaptation viewpoint to propose an ad-
versarial training-based formulation. We uniquely view such a scenario as an instance of adversarial
multi-task learning, where the classification tasks are either the primary task of interest (i.e., pre-
dicting the presence of fur pattern and color) or the auxiliary negative tasks (i.e., characteristics of
habitat) to be avoided. Since the 2 tasks are unrelated, any label correlation between primary and
auxiliary labels in the training data is only by chance and therefore from a domain-adaptation per-
spective, we envision a target-domain as possibly having a different correlation between the primary
and auxiliary labels. The effects of negative transfer are hence mitigated when the classification task
is trained in this domain.
We discuss advantages of our proposed formulation, inspired from domain-adaptation, to alleviate
the negative transfer over existing techniques, including ready applicability to neural networks in an
end-to-end fashion. It must be noted that, while the formulation of the problem is motivated with
multi-task learning, negative-transfer is a disposition of any supervised learning task from simple
binary classification to recent popular supervised tasks such as image detection, captioning, or visual
dialog. We present motivating literature that prelude this work next.
2	Related work on tac kling negative sharing
Image classification literature often characterize mid-level features as attribute predictors in multi-
label learning. The inability of models to learn predictors with the correct semantic concept is at-
tributed to negative transfer. To our knowledge, the predominent approach to tackle negative transfer
in such setting was the use of specific regularizers (Romera-Paredes et al., 2012; Jayaraman et al.,
2014). Specifically, the primary model avoid using features which are important for the auxiliary
task, leading to models competing for features. Jayaraman et al. (2014) further extends this idea
to attribute groups, where feature selection sparsity is induced across group, but encourage within
them. We highlight three limitations of feature competition techniques below:
•	Repeated features: Consider the simple scenario where some features in the feature represen-
tations are repeated or dependent on others. Feature competition enforces tasks to pick unique
features, however they are implicitly the same. Going back to the case of ‘cheetah vs snow leop-
ard’ example mentioned earlier, when there are two copies of a feature which captures ‘snow
in the background’, then both primary and auxiliary classifiers would just pick different copies
of that feature. Here, the idea of resisting cheetah/snow leopard classifier from picking features
which capture snow is negated.
•	Trainable features: Neural representations have become extremely popular owing the power of
learning trainable features. However, feature competition techniques fail with prediction models
with trainable features. If there are features that are important for both primary and auxiliary
task, models involving a trainable feature setup will lead to duplicating the same features, thereby
resulting in the repeated feature scenario.
•	Easy auxiliary task with abundant features: Consider a scenario of an easy auxiliary task, that
does not require a large number of features to be predicted correctly. Similar to the previous case,
the spared features from the auxiliary task can be picked by the primary task resulting in negative
transfer.
Motivated by these shortcomings, we proceed to examine the negative transfer problem in a domain
adaptation setting.
2
Under review as a conference paper at ICLR 2019
3	Proposed Framework
In this section we first formalize and explain the problem setting of negative transfer. We present
scenarios where earlier attempts (regularization-based approaches) to tackle this problem fails. We
then explain our formulation for posing then negative transfer problem in a domain adaptation setting
and derive adversarial learning algorithms to solve negative transfer. Lastly, we present a thorough
analysis of the proposed formulation to address negative transfer by experimenting on a carefully
designed synthetic dataset.
3.1	Problem setup
In typical 2-label classification problem, we assume that training data and all future test examples
are drawn i.i.d from a probability distribution D on instance space X and a labeling function fp :
X → {+1, -1} for the primary task labels and fa : X → {+1, -1} for the auxiliary task labels.
Every instance x ∈ X has primary and auxiliary labels: yp and ya respectively. The goal is to
learn a classifier which performs well on future samples from D, which may have a different label
correlation since the tasks are unrelated. Formally, we capture this label correlation via a joint label
distribution P(Y ) = P(Ya, Yb), and we assume that P(Y ) in training and test are different.
This problem setup is different from earlier works on negative transfer in the way how unrelated
tasks are defined. We define unrelated tasks as the ones which can have different correlation in
training and test data. In (Romera-Paredes et al., 2012) unrelated tasks is referred as tasks which are
defined over orthogonal sets of features. Zhou et al. (2010) uses the term negative correlation for
unrelatedness among tasks. Two tasks are negatively correlated when one feature is deemed to be
important to first task makes it unlikely to be important to the second task.
3.2	Negative transfer as a domain adaptation problem
Let the instances in training data be drawn from source distribution DS . The aim is to train a
classifier on this which performs well on instances drawn from target distribution DT . Instances in
DS has primary and auxiliary labels correlated with joint label distribution PS(Y ) = PS(Yp, Ya)
whereas in DT the labels have a different correlation with different joint label distribution PT (Y ) =
PT (Yp , Ya). We assume that the labelling function fp and fa are universal and common for both
source and target domains.
Typical unsupervised domain adaptation setting has labelled instances from source distribution and
also unlabelled instances from target distribution. However, we have no information (neither labels
nor instances) about the target distribution. The only information we have about the target domain
is PT(Y ). We are either provided an estimate of PT (Y ) from an oracle, or we make an assumption
on PT (Y ) (for instance, a uniform distribution over the space of Y ).
Consider UT, the space of all distributions over X which has label distribution PT (Y ). It is ex-
tremely challenging to adapt from the source domain to all members of UT with given PT (Y ).
However, we can settle for a particular DT . We pick such a DT ∈ UT as that distribution over X
which is nearest to DS. We model this in terms of KL divergence as a constrained optimization as
follows.
DT? = arg min KL(DS ||D)	(1)
D∈UT
Before explaining the solution to the above optimization problem, we show the relationship between
D and P(Y ). To articulate our intuition, we use Figure 3(a) which illustrates a sample source
distribution DS over 2D instance space (green). The labelling functions (in this case, fp and fa)
partitions the space of X into regions such that each region corresponds to a label combination
(y = {yp, ya}). We denote each of these regions as Ry. Notice that P(Y = y) becomes the integral
of D over the region Ry .
3
Under review as a conference paper at ICLR 2019
Let φS and φT be density functions of DS and DT respectively. Then, the above optimization
problem becomes:
φτ = arg min Φ φs (x) log φS(x) dx
φ∈∆	φ(x)
s.t.	φ(x) dx = PT(y) ∀ y
Ry
(2)
Where ∆ ⊃ UT is the set of all distributions over X and region Ry = {x : fp (x) = yp , fa (x) = ya}.
The Lagrangian equivalent for the above problem can be stated as,
L(φ) = Φ φs(x) logφS(X) dx + X λy / φ(x) dx,
φ(x)	y Ry
where λy ∈ R. Using the Euler-Lagrange equation to solve above problem,
φT (x) = φS(x) ×
PT (U)
PS (y)
(3)
Intuitively we find that DT? is a scaled version of DS with scaling factor as the ratio of PT(Y ) and
PS(Y ). Note that this scaling factor may vary for different x ∈ X depending on which region
Ry it falls in. This is depicted in Figure. 3(b) which is a target distribution derived from source
distribution in 3(a). The regions R-1,+1 and R+1,-1 are scaled up whereas regions R+1,+1 and
R-1,-1 are scaled down. Though the above derivation is for two labels, one can see that Eq. 3
extends to any number of labels in y.
As mentioned earlier in this section, we have no instances from tar-
get domain. However φT (x) allows us to assign soft domain labels
to the source domain instances, which indicates the probability of
that instance belonging to the target domain. Specifically, let yD be
the binary label indicating if an instance belongs to target domain.
Then,
P(yD |x) =
φT (X)
φτ (x) + Φs (x)
PT (y)
PT (y) + PS (y)
(4)
Two possible assumptions that could be made on PT(Y ) (if not
provided) are uniform distribution over space of y or uncorrelated
(independent labels). Next we present two methods that leverage
these soft domain labels.
3.3 Algorithms
In the previous section we modeled negative transfer as a domain
adaptation problem with soft domain labels. Next, we present meth-
ods to leverage the domain adversarial neural network (DANN) by
Ganin & Lempitsky (2015) to induce domain-invariance features.
These methods are based on the theoretical analysis of domain
adaptation by Ben-David et al. (2010; 2007). They provide a gen-
eralization bound on target error as following:
Theorem 1 (Ben-David et al. (2010)) Let h ∈ H be a prediction
model of the form h : X → {-1, +1} where H is the hypothesis
space. Let S and T be generalization errors for source (DS) and
target (DT) domains. For any classifier h ∈ H
CT(h) ≤ CS(h) + 1 dH∆H(Ds, DT) + min CS(h0) + CT(h0) (5)
2	h0∈H
where dH∆H denotes the H∆H-divergence between DS and DT,
(b) DT
Figure 2: An illustration of
instance source distribution
over 2D space.(a) Source dis-
tribution with correlated la-
bels (b) Closest uncorrelated
distribution is the desired tar-
get distribution.
dH∆H(DS,DT) = 2 sup Ex∈DS [h(x) 6= h0(x)] - Ex∈DT [h(x) 6= h0(x)]
h,h0∈H
(6)
4
Under review as a conference paper at ICLR 2019
Above theorem states that the target error is bounded by sum of source error and distance be-
tween source and target distributions. dH∆H can be seen as the maximum accuracy of classifying
source and target domain by using a classifier from hypothesis space H∆H. Further, any classi-
fier g ∈ H∆H is the function XOR of some h, h0 ∈ H. DANN introduced an objective function
which minimizes both source error as well as divergence between domains. Divergence between the
domains can be looked at as the prediction accuracy of domain classifier.
DANN models starts with a mapping Jf : X → Rd with a parameter θf, which projects instances
to a latent representation space, we call this the feature extractor. These features are then mapped
to primary label space with a mapping function Jy (label predictor) with parameter θy . The same
features from latent representation space are mapped to domain label by Jd (domain classifier), with
parameter θd. We denote the training set as {xi, (ypi , yDi )}iN=1, with every instance we are provided
with a primary label yp and a soft domain label yD . The objective here is to find a feature extractor
Jf which projects instances to a latent representation space where achievable label prediction accu-
racy is high and domain prediction accuracy is low. Let Ly (θf , θy) be the prediction loss for label
prediction and let Ld(θf, θd) be that for domain classifier, then objective function is
1N
θf ,θy,θd = argmin argmax 方 E Ly (θf ,θy) - λL%(θf ,θd),	(7)
θf ,θy	θd Ni=1
whereLy(θf , θy) captures S (h) and Ld(θf , θd) captures dH∆H(DS, DT) from Eq. 5. Note that
Jy ∈ H, then Jd ∈ H∆H. However, in using this formulation together with aforementioned soft
domain labels, results in a weak JD since the soft domain labels are highly skewed towards the
source domain. In such settings, Ld(θf , θd) on class imbalance domain labels no longer captures
dH∆H(DS, DT). We address this issue again in Section 4.
The effort is to make Jf provide a latent representation that is unable to discriminate source and
target domain. As the soft domain labels are assigned according to y = {yp, ya } (see Eq. 4), if
that latent representation can be used to correctly predict yp and ya with label predictors hp ∈ H
and ha ∈ H, then there exists a g ∈ H∆H which can predict source and target domains well (with
g(x) = hp(x)㊉ ha(χ)). Conversely, if the representations cannot be used to predict both the labels
yp and ya correctly implies poor performance on domain classification. From this observation,
we propose to replace domain classification loss (second loss term of Eq. 7) with auxiliary label
classifier loss La(θf , θa), where θa is the parameter for the auxiliary label classifier Ja. We solve
the optimization problem in Eq. 7 by using gradient reversal layer (Ganin & Lempitsky, 2015).
Gradient reversal layer multiplies the gradient by a negative constant during the backpropagation.
We call the auxiliary label classifier as an adversarial to the primary classifier. In this form, our idea
is closely related to Zhang et al. (2018)
We extend this two label scenario to multilabels, by partitioning labels into groups, such that related
labels are together. If a label is unrelated to all other labels than it forms a singleton group. We
propose a model architecture, with a latent representation (with Jf ) for each group. Further, the
latent representation of a given group must be unable to predict any task from all other groups. We
achieve this by adding all label classification other than the group member as the adversarial. In the
next section, we showcase the empirical performance of multitask models trained in this way.
Another interesting alternative that departs from the usual view of feature projections is to utilize
feature selection for Jf (feature extractor). We discuss this alternative next.
3.3.1 Feature Selection
An intuitive method to prevent negative transfer between correlated features is to use a feature se-
lection approach to explicitly select relevant features appropriate for a task.
We use Recursive Feature Elimination (RFE) (Guyon et al., 2002) for the task of feature selection
using CNN as the classifier, since wrapper method (J. Tang, 2013) is computationally effective and
noticeably time consuming when deep nets are used as the classifier. Recursive feature selection
(Guyon et al., 2002) consider all the features and eliminated them at each iteration till the desired
criterion is met. At each iteration, the current feature set is used to evaluate a task using a classifier.
Each of the features obtain a score from the classifier, based on which one or more features are
eliminated from the set. This step is repeated until the criterion is met, which can be in terms
5
Under review as a conference paper at ICLR 2019
of the final number of features to be chosen or desired classification performance to be attain. In
order to perform RFE, we need to score each of the feature based on its effect on the classifier. For
classifiers like logistic regression, the feature importance can be obtained by the weights assigned by
the classifier for each of the features. In adversarial settings for multilabel classification, importance
of the kth feature can be calculated as:
1	|Yp |
fk =同 X (wpi)2 - min{(wkι)2,..., (wk∣γa∣ )2}	(8)
where, |Yp| and |Ya| denote the number of labels to be predicted for the primary and the auxiliary
task respectively.
4	Experimental Analysis
4.1	Synthetic experiments and analysis
We study the adverse effects of negative transfer and how our proposed algorithms could resist this
negative transfer, on synthetic data. Our synthetic dataset consists of 10-dimensional feature vectors,
with two binary labels yp and ya (primary and auxiliary). We generate the data from a generative
system with one class label distribution for training set and one for test set. First 5 features are
sampled according to the primary label from a mixture of two Gaussian distributions with identity
covariance matrix. If the primary label is 1 then 5 features are sampled from first Gaussian, otherwise
from the second Gaussian. Similarly second set of 5 features are sampled same way from another
set of mixture of two Gaussian distributions, this time sampled according to the auxiliary label.
Note that for these experiments we are only interested in predicting primary label yp in the test
set. In all the experiments we keep P(Yp = 1) = P (Ya = 1) = 0.5. We also Keep distance
between 2 Gaussian distributions corresponding to primary label to fixed to 1.5. We use conditional
label distribution P (Yp|Ya) as measure label correlation. We compare following algorithms in our
experiments: (1) baseline - A logistic regression classifier trained on primary label. (2) DadvC -
One hidden layer (5 neurons) MLP having domain classification as the adversarial task. (3) ALadvC
- Same as DadvC, with auxiliary label classification as the adversarial task. (4) fs-adv - Adversarial
feature selection with auxiliary label classification as adversarial task.
Two sets of experiments where conducted to study the following aspects of negative transfer:
•	Difference between train and test label correlation: For this experiment, we trained a model on
training set with P (Yp|Ya) = 0.8 and tested out it’s performance on test set which only differs
from training set in P(Yp|Ya). Figure 4.1(a) Shows the mean average precision of classifiers on
test set. One can see that the baseline mAP drops as correlation in test set goes down. This indi-
cates that baseline classifier captured wrong set of features for prediction. We can see that DadvC
is performing marginally better than baseline. ALadvC and fs-adv are consistently performing
well irrespective of varying label correlation in test. This shows that these methods picked the
correct set of labels.
•	Easiness of auxiliary task: We fix P (Yp|Ya) = 0.8 for training set and P (Yp|Ya) = 0.5 for
test. We vary the distance between Gaussian distribution corresponding to auxiliary label while
keeping that of primary label fixed. By doing this we vary the easiness of auxiliary task. In Figure
4.1(b) we can see that, as the auxiliary task gets easier, more features related to auxiliary task will
be used by the primary classifiers which results in decreasing performance in test set. One can see
that Both baseline and DadvC are performing bad as easiness increases. The proposed algorithms
performs consistently better.
4.2	Real data experiments and analysis
Datasets: We use the Animals-with-Attributes (AwA) datasets (Xian et al., 2017) and Caltech-
UCSD Birds 200-2011 (CUB) (Wah et al., 2011) for the multilabel attribute prediction task. With
both datasets we follow the experimental protocol from (Xian et al., 2017). The Animals with
Attributes-2 (AWA) consists of 30, 475 images of animals in natural settings, with 50 animals and
85 annotated attributes each. According to Jayaraman et al. (2014), 85 attributes can be grouped
6
Under review as a conference paper at ICLR 2019
(a)
(b)
1.00
0.75
0.50
0.25
0.00
1.00 -
0.75 -
0.50 -
0.25 -
0.00 -
Figure 3: Synthetic experiment results:(a) Mean average precision over test set with varying
P(Yp|Ya), when trained on data with P(Yp|Ya) = 0.8. (b) Mean average precision over test set
with P(Yp|Ya) = 0.5 when train set with P(Yp|Ya) = 0.8, varying max auxiliary accuracy.
10	15	20	25
epochs
(b)
(a)
Figure 4: Mean average precision over test split of AwA per group. The proposed ALadvC out per-
forms the MLP baseline model, while the LR+FS-adv outperforms its baseline LR and LR+FS, for
multilabel attribute prediction on held-out classes. (b) The training performance of the model with
adversarial loss is corrected to reflect the true performance of the model, improving generalization.
into 10 relevant groups. The dataset is split across the animals as 27/13/10 for train, validation, and
test respectively as done in (Xian et al., 2017). The Caltech-UCSD Birds 200-2011 (CUB) consists
of 11,788 images of birds, captured in natural settings, corresponding to 200 birds. The dataset
also consists of 312 attributes of the birds annotated per image. They are aggregated into 28 groups
corresponding to anatomical parts, and split across classes: 100/50/50 for train, validation and test
over attributes, as done in (Xian et al., 2017). The split ensures different correlations between
attribute labels in the three splits, which highlights the problems of negative-transfer.
Feature representations: We directly utilize the the feature representation (of length 2048) obtained
from ResNet-101 (He et al., 2016) model that is pre-trained on ImageNet (Deng et al., 2009) for AwA
and CUB. We explain below the specific architectures and parameters used for each experiment.
Mean average precision (mAP) on validation and test sets are reported. Attribute prediction is an
unbalanced prediction task, as individual attributes are rare. We use a balance corrected binary cross-
entropy loss function for all experiments, with the balance count obtained from the training set.
Further, we utilize early stopping criteria based on the performance of the model on the validation
set.
LR, LR+FS and LR+FS-adv: As discussed in Section 3.3.1, one way to prevent the adverse
effects of negative transfer is by selecting the optimal feature set for each of the task, based on the
proposed adversarial objective function. We use Recursive Feature Elimination (RFE) (Guyon et al.,
2002) method for the task of feature selection wrapped over a Logistic Regression (LR). We perform
feature selection experimentation using Logistic Regression (instead of multi-layer perception, as
7
Under review as a conference paper at ICLR 2019
0.8
A 0,6
E 0.4
0.2
0.0
u」① 4⅛dl6u 一 M
Jo-OulUM0b
⅛-oul-5
,Jo-Oυl6 @
JOouIΛJfoUJμd
E 省QdIA=Bq
EB⅛CΓ=J3
E9tfDdl>pCDq
① decs
①Ns
BdelISIeM
JOoυlA-φq
,JO-OUI① d(ŋu
,Jo-Oolpπ3①-⊂①」OJ
S 6uθΞl一q
,IO-O»8
,JO-OUJeCMS
JO-SJSe①」q
E①七fŋdlpfŋ① q
BdQlISI=C
,Jo-Oυl>PQq
IU 省(Ddl⅛Qgq
」oooIE」pd」① PUn
,JO-OUIS t PE-① ddn
-JO-SIBfM
① dws≡lq
Figure 5: Mean average precision over validation and test split of CUB per group. The proposed
ALadvC out performs the MLP baseline model, while the LR+FS-adv outperforms its baseline LR
and LR+FS, for multilabel attribute prediction on held-out classes. (b) mAP of MLP and proposed
ALadvC model on train and test against epochs. The training performance of the model with adver-
sarial loss is corrected to reflect the true performance of the model, improving generalization. (c)
The CUB dataset consists of 28 groups, increasing number of adversarial tasks from most correlated
to least helps improve performance on test splits.
20	30	40	50	60	70
epochs
(b)
0.10	0.15	0.20	0.25	0.30	0.35	0.40
adv task threshold
(C)
Table 1: Table showing the improvement in mean average precision (%) using our proposed methods
for AwA and CUB datasets._____________________________________
Approach	AWA		CUB 一	
	val	test	Val	test
LR	二	65.27	63.22	16.00	15.98
LR+FS	66.41	61.13	24.80	17.57
LR+FS-adv-	71.07	64.62	25.11	23.16
MLP	二	71.27	63.05	37.51	37.53
ALadVC —	74.15	69.92	42.34	41.05
done in next set of experiments) for time efficiency. For CUB, we transform the 2048 features into
a 500 dimensional space by using a dense layer followed by ReLU, which is then used for feature
selection. Based on the primary task at hand, the feature importance scores are calculated using Eq.
8, by using the learned weights of LR. At each iteration, we remove λ × 0.33 numbers of features,
where we decrease λ at each iteration as: 0.95 × λ. The final number of features to be retained is
decided using the validation based on the mAP. We also observe the performance of feature selection
without the adversarial setting (LR+fs). In this case, we only select the features that perform well
for the primary task, without considering any effect on the auxiliary tasks. Performances of RFE
have been observed in both adversarial and non-adversarial settings, which is reported in Table 1,
and group-wise test mAP in Figures 4(a) and 5 (a) respectively.
As shown in Table 1, the mAP of RFE improves the baseline accuracy when LR has been used (com-
pare rows 2 and 3 with row 1). The efficiency of the proposed criterion for scoring features, as shown
by equation 8, can be observed when we compare between rows 2 and 3 in the Table. There is an
improvement of 2.3% and 45% compared to LR (row 1), and 5.7% and 31.8% compared to LR+FS
(w/o adv) (row 2) for AwA and CUB datasets respectively showcasing the advantage of the proposed
method (LR+FS-adv). The average number of features selected are 321 (out of 2048) and 140 (out
of 500) for AwA and CUB respectively. It is evident from the Table that the performance of MLP
and ALadvC outperforms LR+FS with adv by a large margin. The performance can be attributed to
MLP and ALadvS can apply projections to the feature vectors rather than selection/omission.
8
Under review as a conference paper at ICLR 2019
MLP and ALadvC: We test the performance of the proposed adversarial
approach as described in Section 3.3. We utilize the representation vector
obtained from ResNet-101 as the base model. Next, we attach a train-
able layers (AwA:500, CUB:500k100) with ReLU. Next, we add latent
representation per group (AwA:10, CUB:5) with a linear connect to the
group specific attribute prediction (with sigmoid activation). The smaller
size of the latent layer ensures high feature transfer leading to more neg-
ative transfer. The baseline model (MLP) and the proposed model with
auxiliary labels for each group as an adversarial classifier (ALadvC) are
both trained with learning rate of lr = 0.01, which is decayed exponen-
tially. Additionally, the gradient reversal weight λ = ι+eχpK-ιo*ι., is
related to the ith step of training, where li = i/numsteps. The Sched-
uler increases the weight exponentially towards K (described in (Ganin
et al., 2016)). The model configuration (number and size of intermedi-
ate layers, K, lr) have all been picked from a large parameter sweep for
best validation error. Due to the large number of adversarial branches (28
groups corresponding to 27 adversarial branches per group), we utilize
the pairwise-task-label overlap to threshold the number of tasks (details
below). Table 1 reports mean average precision for attribute prediction
and Figure 4 (a) and 5 (a) show the group-wise performance of the pro-
posed model compared to its corresponding MLP (best baseline model in
parameter search).
ALadvC performance improves by 2.8% and a 6.8% mAP improvements
on the validation and test splits for AwA, respectively. Similarly, 4.8%
and 3.5% improvements are observed in CUB dataset. Note that the splits
ensure different correlations between attributes in the validation and test
splits. Further, 4 (b) and 5 (b), show the effect of the gradient reversal on
the learning process as the λ progressively improves. We notice a drop in
the train mAP and simultaneous improvement over test mAP as the model
(a) AwA-Train
(b) CUB-Train
Figure 6: Maximum
cross-correlation be-
tween the grouped
attributes from train
splits of AwA and CUB
dataset.
sheds improvement obtained from negative transfer. Ultimately the performance on both datasets
improves, suggesting the overall adverse effect of negative transfer. The group-wise performance
measured on the test sets show drop in mAP for certain groups. We assert that the drop represents
the true performance of the model on these tasks. As mentioned above, we identify an additional
imbalance in the attribute prediction task, we term class-combination imbalance. The measure indi-
cates the pairwise co-concurrence of attributes, which is computed as the pairwise Jaccard similarity
between each attribute (illustration of the maximum Jaccard similarity cross group for both datasets
in Figure 6, for their corresponding train splits). We utilize the measure to identify the subset of
adversarial tasks per group for the CUB dataset. As shown in Figure 5 (c), the performance of AL-
advC improves when all group tasks are utilized. The empirical result eludes to the advantage of the
number or hardness of the adversarial task in improving prediction performance.
5	Conclusion and Future Work
In this work, we show that adversarial learning is the natural answer to prevent negative transfer. This
leads to potential improvement in any supervised learning of natural data that is seeking generaliza-
tion. We find that even in relatively straight-forward linear models presented above, co-occurrence
of unrelated labels hampers performance and must be explicitly treated. We address the problem
of negative transfer in a multi-task scenario, and also show the applicability of our solution in any
supervised task. Supervised learning practitioners can utilize domain expertise to acquire and lever-
age additional negative labels for this purpose. Recent work in explainability of machine learning
models can also be appropriately leveraged to facilitate this task.
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137—144, 2007.
9
Under review as a conference paper at ICLR 2019
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on,pp. 248-255. Ieee, 2009.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International Conference on Machine Learning, pp. 1180-1189, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research ,17(1):2096-2030, 2016.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. Gene selection for cancer
classification using support vector machines. Machine Learning, 46(1-3), 2002. ISSN 0885-6125.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
H. Liu J. Tang, S. Alelyani. Feature selection for classification: A review. Data Classification:
Algorithms and Applications, 2013.
Dinesh Jayaraman, Fei Sha, and Kristen Grauman. Decorrelating semantic visual attributes by
resisting the urge to share. In Proceedings of the IEEE Conference on Computer Vision and
PatternRecognition, pp. 1629-1636, 2014.
Bernardino Romera-Paredes, Andreas Argyriou, Nadia Berthouze, and Massimiliano Pontil. Ex-
ploiting unrelated tasks in multi-task learning. In International Conference on Artificial Intelli-
gence and Statistics, pp. 951-959, 2012.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning-a com-
prehensive evaluation of the good, the bad and the ugly. arXiv preprint arXiv:1707.00600, 2017.
Jaehong Yoon and Sung Ju Hwang. Combined group and exclusive sparsity for deep neural net-
works. In International Conference on Machine Learning, pp. 3958-3966, 2017.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. arXiv preprint arXiv:1801.07593, 2018.
Denny Zhou, Lin Xiao, and Mingrui Wu. Hierarchical classification via orthogonal transfer. Inter-
national conference on machine learning, 2011.
Qiang Zhou, Gang Wang, Kui Jia, and Qi Zhao. Learning to share latent tasks for action recognition.
In IEEE International Conference on Computer Vision, pp. 2264-2271. IEEE, 2013.
Yang Zhou, Rong Jin, and Steven Chu-Hong Hoi. Exclusive lasso for multi-task feature selection.
In Proceedings ofthe Thirteenth International Conference on Artificial Intelligence and Statistics,
pp. 988-995, 2010.
10