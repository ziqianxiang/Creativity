Under review as a conference paper at ICLR 2019
Complementary-Label Learning
for Arbitrary Losses and Models
Anonymous authors
Paper under double-blind review
Ab stract
In contrast to the standard classification paradigm where the true (or possibly
noisy) class is given to each training pattern, complementary-label learning only
uses training patterns each equipped with a complementary label. This only spec-
ifies one of the classes that the pattern does not belong to. The seminal paper
on complementary-label learning proposed an unbiased estimator of the classifi-
cation risk that can be computed only from complementarily labeled data. How-
ever, it required a restrictive condition on the loss functions, making it impossible
to use popular losses such as the softmax cross-entropy loss. Recently, another
formulation with the softmax cross-entropy loss was proposed with consistency
guarantee. However, this formulation does not explicitly involve a risk estima-
tor. Thus model/hyper-parameter selection is not possible by cross-validation—
we may need additional ordinarily labeled data for validation purposes, which is
not available in the current setup. In this paper, we give a novel general framework
of complementary-label learning, and derive an unbiased risk estimator for arbi-
trary losses and models. We further improve the risk estimator by non-negative
correction and demonstrate its superiority through experiments.
1	Introduction
Modern classification methods usually require massive data with high-quality labels, but prepar-
ing such datasets is unrealistic in many practical domains. To mitigate the problem, many
previous works have investigated ways to learn from weak supervision: semi-supervised
learning (Chapelle et al., 2006; Miyato et al., 2016; Kipf & Welling, 2017; Sakai et al., 2017;
Tarvainen & Valpola, 2017; Oliver et al., 2018), learning from noisily-labeled data (Natarajan et al.,
2013; Patrini et al., 2017; Ma et al., 2018), learning from positive-unlabeled data (Elkan & Noto,
2008; du Plessis et al., 2014; 2015; Kiryo et al., 2017), learning from similar-unlabeled data
(Bao et al., 2018), learning from positive-confidence data (Ishida et al., 2018), and others.
In this paper, we consider learning from another type of weak but natural supervision called
complementary-label learning (Ishida et al., 2017; Yu et al., 2018), where the label only specifies
one of the classes that the pattern does not belong to. In contrast to the ordinary case where the
true class is given to each pattern (which often needs to be chosen out of many candidate classes
precisely), collecting these complementary labels is obviously much easier and less costly. A nat-
ural question is, however, is it possible to learn from such complementary labels (without any true
labels)?
The problem has previously been tackled by Ishida et al. (2017), showing that the classification risk
can be recovered only from complementarily labeled data. They also gave consistency gaurantee
in theoretical analysis. However, they required strong restrictions on the loss functions, allowing
only one-versus-all and pairwise comparison multi-class loss functions (Zhang, 2004) with certain
non-convex binary losses. This is a severe limitation when we use deep learning since the softmax
cross-entropy loss is often used to boost the classification performance.
Later, Yu et al. (2018) proposed a different formulation for complementary labels by employing
the forward loss correction technique (Patrini et al., 2017) to adjust the learning objective. Their
proposed risk estimator is not necessarily unbiased but the minimizer is theoretically guaranteed to
be consistent with the minimizer of the risk for ordinary labels (under an implicit assumption on the
model for convergence analysis).
1
Under review as a conference paper at ICLR 2019
Table 1: Comparison of two proposed complementary-label methods with previous works.
Methods	loss assump. model assump. unbiased explicit risk free	free	estimator correction
Ishida etal. (2017) Yuet al. (2018)	×	X	X× ×	×	××
Proposed (General formulation) Proposed (Non-negative formulation)	X	X	X	× X	X	×	X
They also extended the problem setting to where complementary labels are chosen in an uneven
(biased) way. This is a realistic problem setting because labelers are more likely to complementarily
label a pattern when they feel it is not a certain class which they have more knowledge or experience
about.
In this paper, we first derive an unbiased risk estimator with a general loss function, making any loss
functions available for use: not only the softmax cross-entropy loss function but other convex/non-
convex loss functions can also be applied. We also do not have implicit assumptions on the classifier,
allowing both linear and non-linear models.
Yu et al. (2018) does not have an unbiased risk estimator, which means users will need clean data
with true labels to calculate the error rate during the validation process. On the other hand, our
proposed unbiased risk estimator can handle complementarily labeled validation data not only for
our learning objective, but also for Yu et al. (2018). This is helpful since collecting clean data is
usually much more expensive.
Finally, our proposed unbiased risk estimator has an issue that it is unbounded from below and
suffers from the classification risk going to negative after learning, leading to overfitting. We further
propose a non-negative correction to the original unbiased risk estimator to improve our estimator.
We experimentally show that our proposed method is comparable to or better than previous methods
(Ishida et al., 2017; Yu et al., 2018) in terms of classification accuracy.
2	Review of previous works
In this section, we explain the notations and review the formulations of learning from ordinary labels,
learning from complementary labels, and learning from both ordinary and complementary labels.
Learning from ordinary labels Let X be an instance space and D be the joint distribution over
X × [K] for class label set [K] := {1, 2,..., K}, with random variables (X, Y)〜 D. The data
at hand is sampled independently and identically from the joint distribution: {(Xi, y%)}n=ι L* D.
The joint distribution D can be either decomposed into class-conditionals {Pk}kK=1 and base rate
{πk}kK=1, where Pk := P(X|Y = k) and πk := P(Y = k), or the marginal M and class-probability
function η : X → ∆k, where M := P(X) and ηk(x) := P(Y = k|X = x). A loss is any
` : [K] × RK → R+ and the decision function is any g : X → RK . The risk for the decision
function g with respect to loss ` and implicit distribution D is:
R(g; `) :=E( XY )2D [' (Y, g (X))],	⑴
where E denotes the expectation. Two useful equivalent expressions of classification risk (1) used
in later sections are
K
R(g;'): = EX[η(x)t'(g(X))] = f∏kEpJ'(k,g(X))],	⑵
k=1
where ' := ['(1, g),'(2, g),...,'(K, g)]T. The goal of classification is to learn the decision func-
tion g that minimizes the risk. In the usual classification case with ordinarily labeled data at hand,
n
approximating the risk empirically is straightforward: R(g;') := n Ei=ι '(yi, g(Xi)).
2
Under review as a conference paper at ICLR 2019
Learning from complementary labels Next we consider the problem of learning from comple-
mentary labels (Ishida et al., 2017). We observe patterns each equipped with a complementary label
{(Xio ,yio)}rn0=ι sampled independently and identically from a different joint distribution D = D.
We denote random variables as (X, Y) 〜 D. As before, we assume this distribution can be de-
composed into either class-conditionals {Pk}K=ι and base rate {∏}3ι, or marginal M and class-
probability function η : X → ∆K, where Pk := P(X∣Y = k),不k := P(Y = k), M := P(X),
η (x) := P(Y = k∣X = x), Y is the complementary label, and ∆K is the conditional probability
simplex for K classes. Without any assumptions on D, it is impossible to design a suitable learning
procedure. The assumption for unbiased complementary learning used in Ishida et al. (2017) was
η (M = Tn (M,	(3)
where T ∈ Rk×k is a matrix that takes 0 on diagonals and K-^ on non-diagonals. Under this
assumption, Ishida et al. (2017) proved that they can recover the classification risk (1) from an al-
ternative formulation using only complementarily labeled data when he loss function satisfies cer-
tain conditions. More specifically, usable loss functions are pairwise comparison or one-versus-all
multi-class loss functions (Zhang, 2004) each with binary loss function ' (Z) : R → R+ that sat-
isfies '(Z) + '(—z) = 1, such as ramp loss 'R(Z) = 2 max(0, min(2, 1 — Z)) or sigmoid loss
'S(Z) = I⅛.
Having an unbiased risk estimator is also helpful for the validation process. Since we do not have
ordinary labels in our validation set in the complementary-label learning setting, we cannot follow
the usual validation procedure that uses zero-one error or accuracy. Ifwe have an unbiased estimator
of the original classification risk (which can be interpreted as zero-one error), we can use the empir-
ical risk for (cross)-validated complementary data to select the best hyper-parameter or deploy early
stopping.
An extension of the above method was considered in Yu et al. (2018) by using a different assumption
than the unbiased complementary learning of Ishida et al. (2017): there is some bias amongst the
possible complementary labels that can be chosen, thus the non-diagonals of T is not restricted to
K-1. However, one will need to prepare a separate dataset with ordinary labels in order to estimate
T beforehand.
Unlike Ishida et al. (2017), Yu et al. (2018) did not directly provide a risk estimator, but they showed
that the minimizer of their learning objective agrees with the minimizer of the original classification
risk (1). Note that, in their formulation, the loss function is restricted to the softmax cross-entropy
loss. Furthermore, the use of a highly non-linear model is supposed for consistency guarantee in
their theoretical analysis. Since the learning objective of Yu et al. (2018) does not correspond to
the classification risk, one will need clean data with true labels to calculate the error rate during the
validation process. On the other hand, our proposed risk estimator can cope with complementarily
labeled validation data not only for our own learning objective, but can be used to select hyper-
parameters for others such as Yu et al. (2018).
Learning from both ordinary and complementary labels In many practical situations, we may
also have ordinarily labeled data in addition to complementarily labeled data. Ishida et al. (2017)
touched on the idea of crowdsourcing for an application with both types of data. For example, we
may choose one of the classes randomly by following the uniform distribution, with probability K-ι
for each class, and ask crowdworkers whether a pattern belongs to the chosen class or not. Then
the pattern is treated as ordinarily labeled if the answer is yes; otherwise, the pattern is regarded
as complementarily labeled. If the true label was y for a pattern, we can naturally assume that the
crowdworker will answer yes by P(Y = y|X = x) and no by 1 — P(Y = y|X = x). This way,
ordinarily labeled data can be regarded as samples from D, and complementarily labeled data from
D, justifying the assumption of unbiased complementary learning (3). In Ishida et al. (2017), they
considered a convex combination of the classification risks derived from ordinarily labeled data and
complementarily labeled data: αR(g;2) + (1 — α)R(g;'), where α ∈ [0,1] is a hyper-parameter
that interpolates between the two risks. The combined (also unbiased) risk estimator can utilize
both kinds of data in order to obtain better classifiers, which was demonstrated to perform well in
experiments.
3
Under review as a conference paper at ICLR 2019
3	Proposed method
As discussed in the previous section, the method by Ishida et al. (2017) works well in practice, but it
has restriction on the loss functions—the popular softmax cross-entropy loss is not allowed. On the
other hand, the method by Yu et al. (2018) allows us to use the softmax cross-entropy loss, but it does
not directly provide an estimator of the classification risk and thus model selection is problematic
in practice. We first describe our general unbiased risk formulation in Section 3.1. Then we discuss
how the estimator can be further improved in Section 3.2. Third, we propose a way for our risk
estimator to avoid overfitting by a non-negative risk estimator in Section 3.3. Finally, we show
practical implementation of our risk estimator with stochastic optimization methods in Section 3.4.
3.1	General risk formulation
First, we describe our general unbiased risk formulation. We give the following theorem, which
allows unbiased estimation of the classification risk from complementarily labeled samples:
Theorem 1. For any ordinary distribution D and complementary distribution D related by (3) with
decision function g, and loss `, we have
R (g;' ) = R (g;')	(4)
for the complementary loss
'(g):= ( —(K - 1)IK + K-11J)∙ '(g),	(5)
where 1 is a K-dimensional column vector with 1 in each element. Proof can be found in Appendix
A. The key idea of the proof is to not rely on the condition PK=I'(k, g) = 1 used in Ishida et al.
(2017), which is a condition inspired by the property of binary 0-1 loss 'o—ι, where 'o _1(Z) is 1
if z < 0 and 0 otherwise. Note that such a technique was also used when designing unbiased risk
estimators for learning from positive and unlabeled data in a binary classification setup (?), but was
later shown to be unnecessary (du Plessis et al., 2015).
According to Theorem 1, we can derive an equivalent form,
K
`(k,g) = -(K - 1) ∙ `(k,g) + X `(j,g).	(6)
j=1
Therefore, the classification risk can be written as
KK
R (g ；' ) = X Πk EP k [-(K - 1) ∙ ' (k, g) + X ' (j, g)].	⑺
k=1	j=1
This expression of the classification risk allows us to naively approximate it in an unbiased fashion
using complementarily labeled data as
K ∏ nk	K
R(g；') = X kX X [ - (K - 1) ∙ '(k, g(Xi)) + X '(j, g(Xi))],	(8)
k=1 nk i=1	j=1
where nk is the number of samples complementarily labeled as the kth class. It is worth noting
that, in the above derivation, there are no constraints on the loss function and classifier. Thus, we
can use any convex/non-convex loss and any linear/non-linear parametric/non-parametric model for
complementary learning.
3.2	Necessity of risk correction
The original expression of the classification risk (1) includes an expectation over non-negative loss
` : [K] × RK → R+, so the risk and its empirical approximator are both lower-bounded by zero. On
the other hand, the expression (7) derived above contains an negative element. Although (7) is still
non-negative by definition, due to the negative term, its empirical estimator can go negative, leading
to over-fitting.
4
Under review as a conference paper at ICLR 2019
Figure 1: The left and middle graphs shows the total risk (8) (in black color) and the risk decomposed into
each ordinary class term (9) (in other colors) for training data with linear and MLP models, respectively. As
an MLP model, a one-hidden-layer neural network with 500 units was used, with ReLU (Nair & Hinton, 2010)
as the activation function, Adam (Kingma & Ba, 2015) for optimization with learning rate 5e - 5 and weight
decay of 1e - 4. The right graph shows the corresponding test accuracy for both models.
We elaborate on this issue with an illustrative numerical example. In the left graph of Figure 1,
we show an example of training a linear model trained on the handwritten digits dataset MNIST1,
with complementary labels generated to satisfy (3). We used Adam (Kingma & Ba, 2015) for op-
timization with learning rate 5e - 5, and weight decay of 1e - 4 with 300 epochs. The empirical
classification risk (8) is shown in black. We can see that the empirical classification risk continues
decreasing and can go below zero at around 100 epochs. The test accuracy on the right graph hits
the peak also at around epoch 100 and then the accuracy gradually deteriorates.
This issue stands out even more significantly when we use a flexible model. The middle graph shows
the empirical classification risk for a multilayer perceptron (MLP) with one hidden layer (500 units),
where ReLU (Nair & Hinton, 2010) was used as the activation function. The optimization setup was
the same as the case of the linear model above. We can see the empirical risk decreasing much more
quickly and going negative. Correspondingly, as the right graph shows, the test accuracy drops
significantly after the empirical risk goes negative.
In fact, a similar issue has already been conceivable in the original paper by Ishida et al. (2017):
According to Theorem 1 in Ishida et al. (2017), the unbiased risk estimator includes subtraction of
a positive constant term which increases with respect to the number of classes. This means that the
learning objective of Ishida et al. (2017) has a (negative) lower bound. Our objective, however, is
unbounded from below and thus can end up in even heavier overfitting.
3.3	Non-negative risk estimator
As we saw in Section 3.2, our risk estimator can suffer from overfitting due to the non-negative
issue. Here, we propose a correction to the risk estimator to overcome this problem.
Each term in the risk with ordinary labels (right-hand side of (2)), which corresponds to each class,
is non-negative. We can reformulate (7) in order to show the counterpart for each non-negative term
in right-hand side of (2) for complementarily labeled data as
KK
R (g ；') = Σ,∏k[ - (K - 1) ∙ E耳['(k, g)] + E E^ ['(k, g)]] ∙	(9)
k=1	j=1
These counterparts (9) were originally non-negative when ordinary labels were used. In the left
and middle graphs of Figure 1, we plot the decomposed risk with respect to each ordinary class (9)
(shown in different colors). We can see that the decomposed risks for all classes become negative
eventually. Based on this observation, our basic idea for correction is to enforce non-negativity
for each ordinary class, with the expression based on complementary labels. More specifically, we
propose a non-negative (nn) version by
KK
Rnn(g; ') = X max {0, ∏k [-(K - 1) ∙ EPk ['(k, g)] + X EPj ['(k, g)]] } ∙	(10)
k=1	j=1
1See http://yann.lecun.com/exdb/mnist/.
5
Under review as a conference paper at ICLR 2019
This non-negative risk can be naively approximated by the sample average as
K
Rbnn (g；') = X max{0,亓[-
k=1
K-1
nk
nk	K 1 ni0
£'( k,g (Xi )) + £— £'(j,g (Xi0))	.	(11)
ni0
i=1	j=1 i i0=1
Enforcing the reformulated risk to become non-negative was previously explored in Kiryo et al.
(2017), in the context of binary classification from positive and unlabeled data. The positive class
risk is already bounded below by zero in their case (because they have true positive labels), so there
was a max operator only on the negative class risk. We basically follow their footsteps, but since our
setting is a multi-class scenario and also differs by not having any true labels, we put a max operator
on every K class.
3.4	Implementation
Implementation with max operator We show practical implementation under stochastic opti-
mization for our non-negative risk estimator. An unfortunate issue is that the minimization of (11)
is not point-wise due to the max-operator, thus cannot be used directly for stochastic optimization
methods with mini-batch. However, an upper bound of the risk can be minimized in parallel by
using mini-batch as the following,
NK	K
NFEEmaX {0，a[-(K - 1)E耳['(k, g);埠 + £ E巴['(k, g); Xj ]] },	(12)
i=1 k=1	j=1
i
where E is the empirical version of the expectation and Xji denotes the samples ComPlementanly
labeled as the jth class in the ith mini-batch.
Implementation with gradient ascent If the objective is negative for a certain mini-batch, the
previous implementation based on the max operator will avoid the objective to further decrease.
However, if the objective is already negative, that mini-batch has already started to overfit. There-
fore, it would be preferable to increase itself to make this mini-batch less overfitted.
Our idea is the following. We denote the risk that corresponds to the kth ordinary class for the ith
mini-batch as
K
Tik (θ) = ∏k [-(K - 1)EPk ['(k, g); Xk] + E EP3 ['(k, g); Xj]],	(13)
j=1
and the total risk as Li (θ) = PkK=1 rki (θ). When mink{rki (θ)}kK=1 ≥ -β, we conduct gradient
descent as usual with gradient VθLi(θ). On the other hand, if mink{r1k(θ)}K=ι < -β, We first
squash the class-decomposed risks over -β to -β with a min operator, and then sum the results:
Lei(θ) = PkK=1 min{-β, rki (θ)}.
Next we set the gradient in the opposite direction with -VθLi(θ). Conceptually, we are going UP
the gradient 5Li (θ) for only the class-decomposed risks below -β, to avoid the class-decomposed
risks that are already large to further increase. Note that β is a hyper-parameter that controls the
tolerance of negativity. β = 0 would mean there is zero tolerance, but in practice we can also have
-β 6= 0 for a threshold that allows some negative (-β < 0) or positive (-β > 0) amount. The
procedure is shown in detail in Algorithm 1.
4	Experiments
In this section, we experimentally compare our three proposed methods (Algorithm 1, (8) and (12),
with two baseline methods from Ishida et al. (2017) and Yu et al. (2018). Table 2 describes the
summary statistics of the benchmark datasets used in this section. The implementation is based on
Pytorch2 and our code will be available on http://anonymized for reproducing results.
2https://pytorch.org
6
Under review as a conference paper at ICLR 2019
Algorithm 1 Proposed algorithm with gradient ascent
Input: ComPlementarily labeled training data {Xk}£1，Where Xk denotes the samples comple-
mentarily labeled as class k;
Output: model parameter θ for g(x; θ)
1:	Let A be an external SGD-like stochastic optimization algorithm such as Kingma & Ba (2015)
2:	Denote {Xji} as the i-th mini-batch for complementary class j
3:	Denote Li(θ) = PkK=1 rki (θ)
4:	Denote r}(θ) = πk [-(K - 1)EPk ['(k, g); Xk] + PK=r EPj ['(k, g); Xj]]
5:	Denote Lei(θ) = PkK=1 min{-β, rki (θ)}
6:	while no stopping criterion has been met:
7:	ShUfle {Xj}k into N mini-batches;
8:	for i = 1 to N:
9:	if mink[r1i (θ), . . . , rki (θ), . . . , rKi (θ)] > -β:
10:	Set gradient ^θLi(θ);
11:	Update θ by A With its cUrrent step size η ;
12:	else:
13:	Set gradient - JL (θ);
14:	Update θ by A With a discoUnted step size γη;
4.1 Setup
For MNIST and Fashion-MNIST, a linear-in-inpUt model With a bias term and a MLP model (d -
500 - 1) Was trained With softmax cross-entropy loss fUnction. Weight decay of 1e - 4 for Weight
parameters and learning rate of 5e - 5 for Adam (Kingma & Ba, 2015) Was Used.
For CIFAR-10, DenseNet (HUang et al., 2017) and Resnet-18 (He et al., 2016) With defaUlt param-
eter settings Were trained. Weight decay of 5e - 4 and initial learning rate of 1e - 2 Was Used. For
optimization, stochastic gradient descent Was Used With the momentUm set to 0.9. Learning rate Was
halved every 30 epochs.
We trained and compared 5 methods (Free (8), Max operator (12), Gradient ascent (Alg.1), PC
(Ishida et al., 2017) and Forward (YU et al., 2018)) With only complementarily labeled data. Note
that the first three are the proposed methods. We complementarily labeled oUr benchmark datasets
so that the assUmption of (3) is satisfied. This is straightforWard When the dataset has a Uniform
(ordinarily-labeled) class prior, becaUse it redUces to jUst choosing a class randomly other than the
trUe class. For Gradient ascent, We Used β = 0 and γ = 0 for simplicity. We trained 300 epochs,
Where mini-batch Was set to 100.
4.2 Results
Instead of shoWing the test accUracy for a single chosen model based on validation, We shoW the
accUracy for all 300 epochs on test data to demonstrate hoW the issUes discUssed in Section 3.2 ap-
pear and hoW different implementations Section 3.4 is effective. In FigUre 2, We shoW the mean test
accUracy and standard deviation for 4 trials for the three benchmark datasets, on test data evalUated
With ordinary labels.
First We compare oUr three proposed methods With each other. For linear models in MNIST and
Fashion-MNIST, all proposed methods Work similarly. HoWever in the case of Using a more flexi-
ble model (MLP model for MNIST/Fashion-MNIST, Densenet/Resnet for CIFAR-10), We can see
that Free is the Worst, Max operator is better and Gradient ascent is the best oUt of the proposed
three methods at the end of all epochs (Free < Max operator < Gradient ascent). These resUlts
are consistent With the discUssions of overfitting in Section 3.2 and the motivations for different
implementations in Section 3.4.
Next, We compare With baseline methods. For linear models, all methods have similar performance.
HoWever for deep models, the sUperiority stands oUt for Gradient ascent for all datasets.
7
Under review as a conference paper at ICLR 2019
70
60
^orol-⊃oo<
operator
(a) MNIST, linear
forward
50	100	150	200
Epoch
(d) Fashion MNIST, MLP
(e) CIFAR 10, Densenet
(f) CIFAR 10, Resnet-18
(b) MNIST, MLP
-----max operator
-----free
(c) Fashion MNIST, linear
gradient ascent
max operator
Figure 2: Experimental results for various datasets and models. Dark colors show the mean accuracy of 4 trials
and light colors show standard deviation.


5 Conclusion
We first proposed a general risk estimator for learning from complementary labels that does not
require restrictions on the form of the loss function or the model. However, since the proposed
method suffers from overfitting, we proposed a modified version to alleviate this issue in two ways
and have better performance. At last, we conducted experiments to show our proposed method
outperforms or is comparable to current state-of-the-art methods for various benchmark datasets
and for both linear and deep models.
8
Under review as a conference paper at ICLR 2019
References
Han Bao, Gang Niu, and Masashi Sugiyama. Classification from pairwise similarity and unlabeled
data. In ICML, 2018.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien (eds.). Semi-Supervised Learning. MIT
Press, 2006.
Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of learning from
positive and unlabeled data. In NIPS, 2014.
Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Convex formulation for learn-
ing from positive and unlabeled data. In ICML, 2015.
Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In KDD,
2008.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In CVPR, 2017.
Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from complementary
labels. In NIPS, 2017.
Takashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classification from positive-confidence
data. In NIPS, 2018. To appear.
Diederik P. Kingma and Jimmy L. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positive-
unlabeled learning with non-negative risk estimator. In NIPS, 2017.
Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah M. Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In ICML,
2018.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. In ICLR, 2016.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In ICML, 2010.
Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep K. Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In NIPS, 2013.
Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic
evaluation of deep semi-supervised learning algorithms. In NIPS, 2018. To appear.
Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, and Lizhen Qu. Making deep
neural networks robust to label noise: A loss correction approach. In CVPR, 2017.
Tomoya Sakai, Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Semi-
supervised classification based on classification from positive and unlabeled data. In ICML, 2017.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning results. In NIPS, 2017.
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary
labels. In ECCV, 2018.
Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal
ofMachine Learning Research, 5:1225-1251, 2004.
9
Under review as a conference paper at ICLR 2019
A Proof of Theorem 1
Proof. First of all,
1
P(X, Y = y) = K-I E P(X, Y = y)
y=y
(14)
(15)
(16)
1-^π(p( X) - p( x,y = y)) ∙
K-1
The first equality holds since the marginal distribution is equivalent for D and D and We assume (3).
Consequently,
P(Y = y∣x = x ) = P(X UY = y) P( X = x)	(17)
=(1 _ P(X,Y = y) λ =K - 1 ∙ V	P(X = x))	(18)
-ɪ ∙ (1 - P(Y = y∣X = x)) K-1	(19)
=-K-I p(Y = yX = X) + K-I ∙	(20)
More simply, we have	
η (x) = — (K — 1) η (x) +1 ∙	(21)
Finally, we transform the classification risk,	
R(g；') = E(XY)~D ['(Y, g(X))]	(22)
=E X~M [ η>' (g (X))]	(23)
=EX~M [(- (K -1)η> +1>)`(g(X))]	(24)
=EX~M [ - (K - 1)η>'(g(X)) + 1 >'(g(X))]	(25)
=E(XY)~D [ - (K - 1) ∙ '(F g(X))] + 1 >EX~M ['(g(X))]	(26)
K =E Exufc ∏kk ∙ ( - (K - 1) ∙ '(k, g(X)) + 1 >'(g(X)))]	(27)
k=1	
=R (g ；')	(28)
for the complementary loss,	
'(k, g) ：= -(K - 1)'(k, g) + 1 >'(g),	(29)
which concludes the proof.	口
B Details of datasets used in Section 4
In Table 2, we explain the details of the datasets
tion 4.	See http://yann.lecun.com/exdb/mnist/
https://github.com/zalandoresearch/fashion-mnist for
used in Sec-
for MNIST,
Fashion-MNIST,
and https://www.cs.toronto.edu/~kriz/cifar.html for CIFAR-10.
10
Under review as a conference paper at ICLR 2019
Table 2: Summary statistics of benchmark datasets.
Name	I # Train	# Test I	# Dim	# Classes	Model
MNIST	I 60,000	10,000 I	784	10	Linear, MLP
Fashion MNIST	I 60,000	10,000 I	784	10	Linear, MLP
CIFAR-10	I 60,000	10,000 I	2,048	10	DenseNet, Resnet
11