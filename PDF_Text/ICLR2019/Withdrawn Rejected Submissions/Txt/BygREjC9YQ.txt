Under review as a conference paper at ICLR 2019
A unified theory of adaptive stochastic
gradient descent as Bayesian filtering
Anonymous authors
Paper under double-blind review
Ab stract
We formulate stochastic gradient descent (SGD) as a novel factorised Bayesian
filtering problem, in which each parameter is inferred separately, conditioned on
the corresponding backpropagated gradient. Inference in this setting naturally
gives rise to BRMSprop and BAdam: Bayesian variants of RMSprop and Adam.
Remarkably, the Bayesian approach recovers many features of state-of-the-art
adaptive SGD methods, including amongst others root-mean-square normaliza-
tion, Nesterov acceleration and AdamW. As such, the Bayesian approach pro-
vides one explanation for the empirical effectiveness of state-of-the-art adaptive
SGD algorithms. Empirically comparing BRMSprop and BAdam with naive RM-
Sprop and Adam on MNIST, we find that Bayesian methods have the potential to
considerably reduce test loss and classification error.
1	Introduction
Deep neural networks have recently shown huge success at a range of tasks including machine trans-
lation (Wu et al., 2016), dialogue systems (Serban et al., 2016), handwriting generation (Graves,
2013) and image generation (Radford et al., 2015). These successes have been facilitated by the
development of a broad range of adaptive SGD methods, including ADAGrad (Duchi et al., 2011),
RMSprop (Hinton et al., 2012), Adam (Kingma & Ba, 2015), and variants thereof, including Nes-
terov acceleratation (Nesterov, 1983; Bengio et al., 2013; Dozat, 2016) and AdamW (Loshchilov
& Hutter, 2017). However, such a broad range of approaches raises the question of whether it is
possible to obtain a unified theoretical understanding of adaptive SGD methods. Here we provide
such a theory by reconciling state-of-the-art adaptive SGD algorithms with very early work that used
Bayesian (Kalman) filtering to optimize the parameters of neural networks (Puskorius & Feldkamp,
1991; Sha et al., 1992; Puskorius & Feldkamp, 1994; 2001; Feldkamp et al., 2003; Ollivier, 2017).
There have recently been attempts to connect adaptive SGD algorithms to natural gradient vari-
ational inference (VI) (Zhang et al., 2017; Khan et al., 2017; 2018). These approaches give a
momentum-free algorithm with a mean-square normalizer, in contrast to perhaps the most popu-
lar adaptive method, Adam (Kingma & Ba, 2015), which combines momentum with a root-mean-
square normalizer. To achieve a closer match to Adam, they modified their natural gradient VI
updates, without a principled justification based on approximate inference, to incorporate momen-
tum (Zhang et al., 2017; Khan et al., 2018), and the root-mean-square normalizer (Khan et al., 2017;
2018). As such, there appears to be only a loose connection between successful adaptive SGD
algorithms such as Adam, and natural gradient VI.
There is a formal correspondence between natural gradient VI (Zhang et al., 2017; Khan et al.,
2017; 2018) and Bayesian filtering (Ollivier, 2017). While Ollivier (2017) did not examine the
relationship between their filtering updates and RMSprop/Adam, the equivalence of this particular
filtering approach and natural gradient VI indicates that they would encounter the issues described
above, and thus be unable to obtain momentum or the root-mean-square normalizer (Zhang et al.,
2017; Khan et al., 2017; 2018). More problematically, Ollivier (2017) introduces dynamics into
the Kalman filter, but these dynamics correspond to the “addition of an artificial process noise Qt
proportional to [the posterior covariance] Pt-1”. Thus, their generative model depends on inferences
made under that model: a highly unnatural assumption that most likely does not correspond to any
“real” generative process.
1
Under review as a conference paper at ICLR 2019
A
μ-i(t - 2)-----> μ-i(t - I)
μ-i (t)
μ-i(t + 1)-----> μ-i(t + 2)
W弘t - 2)	wt(t - 1)
μi(t - 2)	μi(t - 1)	μi(t)
gi(t - 2)	gi(t - 1)
W 弘 t)	W弘 t + 1)	W弘 t + 2)
μi(t + 1)	μi(t + 2)
gi(t)	gi(t+1)	gi(t + 2)
μi(t -
μi(t -
W*(t - 2)
2)
gi(t
zi(t
2)
gi(t
W弘t - 1)
Wi(t)
W*(t +1)
W*(t + 2)
μi(t - 1)
2)
2)
μi(t)
μi(t + 1)
μi(t + 2)
gi(t
÷ Zi (t
μi(t - 1)
2)
gi(t
1)
1)
1)
μi(t)
gi(t)
zi(t)
gi(t)
gi(t+1)
gi(t + 2)
÷ Zi(t + 1)
÷ Zi(t + 2)
μi(t + 1)
μi(t + 2)
gi(t+1)
gi(t + 2)
B
C

—
—
—
—



—
—
Figure 1: The heirarchy of generative models underlying our updates. A Full model for the gradients
for a single parameter. The current estimate for all the other parameters, μ-i(t) vary SloWly over
time, and give rise to the current optimal value for the ith parameter, w*. The gradient then arises
from the current estimate of the ith parameter, μi(t) (which is treated as an input here), and the
optimal value, ith parameter, Wi*. B The graphical model obtained by integrating over trajectories
for the other parameter estimates, μ-i(t). In practice, we use a simplified model as reasoning about
all possible trajectories of μ-i(t) is intractable. C To convert the model in B into a tractable hidden
Markov model (HMM), We define a neW variable, zi (t), Which incorporates Wi* along With other
information about the dynamics.
HoW might We obtain a principled Bayesian filtering approach that recovers the tWo key features of
state-of-the-art adaptive SGD algorithms: momentum and the root-mean-square normalizer? Here,
We note that past approaches including natural gradient VI take a complex generative model over all
N parameters jointly, and use a very strong approximation: factorisation. Given that We knoW that
the true posterior is a highly complicated, correlated distribution, it is legitimate to Worry that these
strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-
to-optimal updates. Here We take an alternative approach, baking factorisation into our generative
model, so that We can use Bayesian inference to reason about (Bayes) optimal updates under the
constraints imposed by factorisation. In particular, We split up the single large inference problem
over all N parameters, w, into N small inference problems over a single parameter. Remarkably, by
incorporating factorisation into the problem setting, We convert intractable, high-dimensional cor-
relations in the original posterior into tractable loW-dimensional dynamics in the factorised model.
This dynamical prior has a “natural” form, at least compared With Ollivier (2017), in that it does
not depend on the posterior. Next, We give a generic derivation shoWing that Bayesian SGD is an
adaptive SGD method, Where the uncertainty is used to precondition the gradient. We then adapt the
generic derivation to the tWo cases of interest: RMSprop (Hinton et al., 2012) and Adam (Kingma
& Ba, 2015). Finally, We discuss the general features of Bayesian adaptive SGD methods, includ-
ing AdamW (Loshchilov & Hutter, 2017) and Nesterov acceleration (Nesterov, 1983; Dozat, 2016),
amongst others.
2
Under review as a conference paper at ICLR 2019
2 Factorisation implies a rich dynamical prior
The most obvious factorised approach is to compute the true posterior, P (wi|D), over a single
weight, wi , conditioned on all the data, D. However, this approach immediately fails, because
symmetries involving permuting the hidden units (Sussmann, 1992) imply that marginal posteriors
such as P (wi|D) are always so broad as to be useless.
To obtain an informative, narrow posterior, one approach is to eliminate these symmetries by condi-
tioning on our current estimate of the other parameters. In particular, we define a random variable,
w*, representing the optimal value for the ith weight, conditioned on the other weights, w-i, being
equal to our current estimate of those parameters, μ-i,
w* = arg max E [logP (d|wi = wi, w—i = μ-i)],	(1)
wi
where d is a random minibatch drawn from the true, unknown underlying data distribution (and not
just the data we have), so this is analogous to maximum-likelihood with infinite data. As such, even
if μ-i is fixed, w* will be unknown if only have finite data, and therefore we do not know the true
underlying data distribution. The optimal weight, wi*, is a random variable because it depends on
our current estimate of the other weights, μ-i, which is a random variable because it depends on a
random initialization, and potentially on a randomised optimization algorithm. However, we cannot
now use the usual Bayesian approach of inferring wi* based on the data, because, in the Bayesian
setting, the data are assumed to be generated from a model with parameters, w, not wi*. Instead, note
that the updates for almost all neural network optimization algorithms depend only on the current
value of that parameter, and the (history of) backpropagated gradients. This suggests a potential
approach: writing down a generative model for the backpropagated gradients that depends on wi* ,
then inverting that model to infer wi* from those gradients.
Following standard Laplace/Extended Kalman Filter-like approximations (e.g. Zhang et al., 2017;
Khan et al., 2017; Ollivier, 2017; Khan et al., 2018) as closely as possible, we approximate the like-
lihood as a second-order Taylor series expansion. In particular, we consider the log-likelihood for a
single minibatch, d, where the minibatch is treated as a random variable drawn from the underlying
true data distribution, and we expand in the ith direction, keeping the other parameters, w-i, fixed
to their current estimates, μ-i,
where,
IogP (d|wi = Wi, w—i = μ-i) ≈ - 2 Λlike,i (Wi - μlike,i)2 + const,
μlike,i = arg max IogP (d|wi = Wi, w—i = μ-i),
wi
Λlike,i
∂2 IogP (d|wi = Wi, w—i = μ-i)
∂2Wi
(2)
(3)
(4)
Wi= μlike ,i
Here, both the mode, μnke,i, and the negative Hessian, Λlike,i, are random variables as they are
deterministic functions of the minibatch, d, and the current estimate of all the other parameters,
μ-i, which are both treated as random variables. As such, the gradient of the log-likelihood, which
depends on Λlike,i and μ-i, is also a random variable,
∂ logP(d∣wi = Wi, w—i = μ-i)
gi =	∂w	ʃ ≈ Alike,i (μlike,i - μi) ∙
(5)
where the approximation comes from the second-order Taylor expansion in Eq. (2). Our goal is to
understand the distribution of the gradients, conditioned on the random variable Λlike,i being equal
to some specific value, λlike,i . In practice, we will set Λlike,i, using the standard approximations in
the natural gradient literature (Zhang et al., 2017; Khan et al., 2017; 2018). The expected gradient
is zero at μ% = w*,
0 = E [gi∣w* = W*] I ≈ E [Λlike,iμlike,i |w* = W*] - E [Λlike,i|w* = W*] Wi	(6)
∣μi=w*
where the approximation again comes only from the second-order Taylor expansion. Thus,
E [μlike,i ^like,i = λlike,i, Wi = Wi ] ≈ E [μlike,i | wi = Wi ] ≈ Wi ,	(7)
3
Under review as a conference paper at ICLR 2019
with equality if the expected value of μiike,i is independent of Λ%, Substituting this into Eq. (5), We
obtain,
E [gi|Alike,i = λlike,i, Wi = wi ] ≈ λlike,i (Wi - μi) .	(8)
However, obtaining an analytic form for the full distribution of the gradient is more difficult. Given
that all Work in this domain makes fairly strong assumptions (e.g. Zhang et al., 2017; Khan et al.,
2017; 2018), an alternative approach opens up: choosing a model for the gradients to match as
closely as possible any given approximation of the original likelihood in Eq. (2). In particular, We
take,
P (gi = gi ∣Λlike,i = λlike,i, W* = W*) ≈ N (gi； λlike,i (wi - μi) , λlike,i) .	(9)
Note that the variance is the negative Hessian, λlike,i, Which is reminiscent of Fisher-Information
like results (Sec. 9.7). HoWever, We chose this form such that, for any given approximation to the
original log-likelihood (Eq. 2), the log-likelihood induced by conditioning on the gradient (Eq. 9)
has the same form,
log P (gi = gi ∣Λlike,i = λlike,i, W* = w*)+ const ≈
-(gi-λu2λ (Wii))) = -1 λlike,i (W; -μlike,i) . (10)
2λlike,i
The only material difference betWeen the right-hand-side of this expression and the original likeli-
hood (Eq. 2) is that here the parameter of interest is the optimal Weight, Wi*, as opposed to the Weight
in the underlying generative model, Wi . Also note that here We have specified the values for all the
random variables: gi = gi and Λlike,i = λlike,i which fix the value of μlike,i to μlike = λgi^ + μi.
The key difference betWeen the methods is that in the original problem, Where We infer a single
distribution over all Weights jointly, the underlying Weights, w, Were fixed, and as such, Bayesian
filtering reduces to recursively applying Bayes theorem, Which does not give rise to interesting dy-
namics. In contrast, When We consider N separate inference problems, in Which Wi* is inferred from
the gradient, then We are forced to introduce dynamics, because Wi* varies over time, as it depends on
allthe other parameters in the network, μ-i, which are also being optimized (Fig. 1A). However, the
full generative process in Fig. 1A is very difficult to reason about, as it requires us to integrate over
all possible trajectories for the other parameters, μ-i. Instead, we write down simplified approx-
imate dynamics over wi* directly (Fig. 1B), and evaluate the quality of these simplified dynamics
empirically. We choose the model in Fig. 1 such that it has finite-length temporal dependencies, and
hence can be written in terms of a Markov model with an expanded state-space. In particular, we de-
fine a new random variable, zi, incorporating wi*, whose generative process is Markovian (Fig. 1C).
For Adam, zi will have two elements representing a parameter, Wi* and the associated momentum,
pi , whereas for RMSprop it will have only one element representing just the parameter,
zRMSprop,i(t) = (Wi* (t))
zAdam,i (t)
Wi*(t)
pi(t)
(11)
As zi represents the optimal setting for a parameter it will change over time, and we assume a simple
Gaussian form for these changes,
P(zi(t)|zi(t-1))=N(zi(t);(I-A)zi(t-1),Q),	(12)
where Q is the covariance of Gaussian perturbations and A is the dynamics matrix incorporating
weight decay and momentum. We can write the second-order Taylor expansion of the likelihood as
a function of the expanded latent, zi (Eq. 11),
IogP (gi∣W*) = IogP (gi∣Zi) ≈ -2 (Zi - μlike,i)T Λlike,i (Zi- μlike,i) ,	(13)
where we have omitted conditioning on Λlike,i, as in practice Λlike,i is estimated from the gradients,
and we have omitted time indices for clarity,
gi =	gi(t)	wi	= wi (t)	zi	=	zi(t)	μlike,i = μlike,i(t)	Alike,i	= Alike,i(t).
This likelihood is equivalent to our original likelihood (Eq. 10) because the gradients depend only
on wi*(t). As such, the negative Hessians must take the form,
ΛRMSprop,like,i = (Λlike,i )	ΛAdam,like,i =	0 ,	0 .	(14)
4
Under review as a conference paper at ICLR 2019
3 Bayesian (Kalman) filtering as adaptive SGD
The Gaussian prior and approximate likelihood allows us to use standard two-step Kalman filter
updates. First, we propagate the previous time-step’s posterior, P (zi(t - 1)|D(t - 1)), with mean
μpost,i(t - 1) and covariance Σpost,i(t - 1), forward in time to obtain a prior at time t,
P (zi(t)|G(t - 1)) = R dzi(t - 1) P (zi(t)|zi(t - 1)) P (zi(t - 1)|Gi(t - 1))
=N (zi(t); μρrior,i⑴，£prior,i⑴),
where,
μρrior,i = μρrior,i(t) = (I - A) μρost,i(t - I),
(15a)
Σprior,i = Σprior,i(t) = (I - A) Σpost,i(t - 1) (I - A)T + Q,	(15b)
and where Gi(t - 1) = {gi(1), gi(2), . . . , gi(t - 1)} is all gradients up to time t - 1. Note that we
have also defined abbreviations for μprior,i(t) and Σprior,i(t), omitting the temporal index, t. Second
we use Bayes theorem to incorporate new data,
P (gi(t)|zi(t)) P (zi(t)|G(t - 1))
P (zi(t)|D(t))=	P(gi(t)∣G(t- 1))	= N(Zi(t); μPo“"⑴,'PoSt,i(t)).
where,
Epost,i = £post,i(t) = (ςPrior,i + Alike,，，
μpost,i = μpost,i (t) = μprior,i + £post,iAlike,i (μlike,i - μprior,i) ∙
(16a)
(16b)
Thus far, we have simply repeated standard Kalman filtering results. To relate Kalman filtering to
gradient ascent, We compute the gradient ofEq.(13)at Zi = μprior,i,
gi
∂logP(gi∣z)	_ ʌ	z	、
7；	= AIike (μlike,i - μprior,i) ♦
∂Zi	z =
Zi =μprior,i
Note that as the log-likelihood depends only on wi, we have,
gRMSprop,i = (gi )
gAdam,i
g0i .
Now, we identify this gradient (Eq. 17) in the mean updates (Eq. 16b),
μpost,i = μprior,i + £post,i gi ♦
(17)
(18)
(19)
This form is extremely intuitive: it states that the uncertainty should be used to precondition gradient
updates, such that the updates are larger when there is more uncertainty, and smaller when past data
gives high confidence in the current estimates.
As the precision is always rank-1 (Eq. 14), we can always write it as,
Alike,i = ei ei ,
where
eRMSprop,i = (ei )
eAdam,i
(20)
As such, the updates for the posterior covariance (Eq. 16a) can be rewritten using the Sherman
Morrison formula (Hager, 1989),
yl _ yl	EpriorNeief £prior,i
Npost，i = ςPrior，i	1 + eT ∑ . .e.
+ ei	prior,i ei
(21)
To estimate ei, we use the Fisher information (see SI Sec. 9.7). In particular, we could use the gra-
dient itself, but could also (under weaker approximations) use the centred gradient (Graves, 2013),
or the gradient for data sampled under the prior.
5
Under review as a conference paper at ICLR 2019
4 Bayesian RMSprop (BRMSprop)
Here, we develop a Bayesian variant of RMSprop, which we call BRMSprop. We consider each
parameter to be inferred by considering a separate Bayesian inference problem, so the latent variable,
Z = w*,isa single scalar, representing a single parameter (We omit the index i on Z for brevity). We
use A = η2∕(2σ2) and Q = η2 giving a dynamical prior,
P (z(t + 1)∣z(t)) = N ((l -岳)z(t),η2) ,	(22)
such that the stationary distribution over Z has standard-deviation σ. To obtain a complete de-
scription of our updates, We substitute these choices into the updates for the prior (Eq. 15) and the
posterior (Eq. 19 and Eq. 21),
μprior = (1 - ⅛2") μpost(t - I)	μpost = μprior + σp2ostg,	(23)
2	σ2 e2
2	η2 2 2	2	2	2	σprior e
σprior = ( 1 - 2σ2 J σpost(t - 1) + η	σpost = σprior I 1 - ι ∣	2	2 )	(24)
1	+ σpriore
For an efficient implementation of the full algorithm, see SI Alg. 1.
4.1	Recovering RMSprop from BRMSprop
NoW We shoW that With in steady state, BRMSprop closely approximates RMSprop. Making this
comparison is non-trivial because the “additional” variables in RMSprop and BRMSprop (i.e. the
average squared gradient and the uncertainty respectively) are not directly comparable. HoWever,
the implied learning rate is directly comparable. We therefore look at the learning rate When the
average squared gradient and the uncertainty have reached steady-state. As t → ∞, We expect σp2ost
to reach steady-state, at Which point, σp2ost = σp2ost(t) = σp2ost(t + 1). In SI Sec. 9.1, We consider
steady-state in the general case. Applying those results to RMSprop, We obtain,
σp4ost(t+ 1)he2i ≈ η2.
Solving for σp2ost,
Substituting this form into Eq. (23) recovers the root-mean-square normalization used in RMSprop.
5 Bayesian Adam (BAdam)
We noW develop a Bayesian variant of Adam (Graves, 2013; Kingma & Ba, 2015), Which We call
BAdam. To introduce momentum into our Bayesian updates, We introduce an auxiliary momentum
variable, p(t), corresponding to each parameter, w*(t),
Zztt =(濯),
then we infer both the parameter and momentum jointly. Under the prior, the momentum, p(t),
evolves through time independently of w*(t), obeying an AR(1) (or equivalently a discretised
Ornstein-Uhlenbeck) process, with decay ηp and injected noise variance ηp2,
p(t+ 1) = (1 - ηp)p(t) +ηp ξp(t),	(25)
where ξp (t) is standard Gaussian noise. This particular coupling of the injected noise variance
and the decay rate ensures that the influence of the momentum on the weight is analogous to unit-
variance Gaussian noise (see SI Sec. 9.2). The weight obeys similar dynamics, with the addition of
a momentum-dependent term which in practice causes changes in w* (t) to be correlated across time
(i.e. multiple consecutive increases or decreases in w* (t)),
w*(t + 1) = (1 - n2+n2) w*(t) + ηp(t) + ηw ξw(t),	(26)
6
Under review as a conference paper at ICLR 2019
where ξw(t) is again standard Gaussian noise, η is the strength of the momentum coupling, η+ηw
is the strength of the weight decay, and ηw2 is the variance of the noise injected to the weight. It is
possible to write these dynamics in the generic form given above (Eq. 12), by using,
η0p2	,
(27)
and these settings fully determine the updates, according to Eq. (15) and Eq. (16). For an efficient
implementation of the full algorithm, see SI Alg. 2.
5.1	Recovering Adam from BAdam
Now we show that with suitable choices for the parameters, BAdam closely approximates Adam. In
particular, we compare the updates for the (posterior) parameter estimate, μw, and the momentum,
μp, when We eliminate weight decay by setting σ2 = ∞, and eliminate noise injected directly into
the weight by setting ηw = 0,
μw(t + I)= μpost,w(t + I)= μw(t) + ημp (t) + ςWWg(t),	(28a)
μp(t + I) = μpost,p(t + I) = (I - ηP) μp⑴ + ςWPg⑴.	(28b)
These updates depend on two quantities, Σww and Σwp, which are related to he2i, but have no direct
analogue in standard Adam. As such, to make a direct comparison, we use the same approach as
we used previously for RMSprop: we compare the updates for the parameter and momentum, when
Σww, Σwp in BAdam and he2i in Adam have reached their steady-state values. To find the steady-
states for Σww and Σwp, we again use the simplified covariance updates derived in SI Sec. 9.1,
Q ≈ AΣ + ΣAT + ΣheeT iΣ.	(29)
Substituting Eq. (27) and Eq. (20) into Eq. (29), and again using σ2 = ∞ and ηw2 = 0, we obtain,
0 0 ≈ 0	Σwp	-	2Σwp	Σpp	+ h	2i	Σ2ww	ΣwwΣwp	(30)
0	ηp2	≈ ηp	Σwp	2Σpp	- η	Σpp	0	+	he	i Σww Σwp	Σ2wp	.	(30)
We now assume that the data is informative, in the sense that it is strong enough to give a narrow
posterior relative to the prior (without which any neural network training algorithm is unlikely to be
able to obtain good performance). This implies Σpp《η/2 (see SI Sec. 9.2), allowing us to solve
for Σwp, using the lower-right element of the above expression (i.e. ηp2 ≈ 2ηpΣpp + he2iΣ2wp),
(31)
To recover a very close approximation to Adam, we need a specific relationship between learning
rates and evidence strength, such that,
While this may seem restrictive, it is only necessary to achieve the closest possible match between
Bayesian filtering and plain Adam. However, our goal is not to match Adam exactly, given that
Adam does not even converge (Reddi et al., 2018). Instead, our goal is to capture the essential
dynamical insights of Adam in a Bayesian method. Indeed, we hope that the resulting Bayesian
method constitutes an improvement over Adam, which implies that it must exhibit some differences.
Nonetheless, focusing on the regime where filtering and Adam are most similar, the filtering updates
become,
μw(t +I) ≈ μw(t) + ημp⑴ + ηηp-7== ≈ μw(t) + ημp(t + I)
he2i
μp(t + 1) ≈ (I - ηp) μp⑴ + ηp-grɪ,
he2i
(32a)
(32b)
where we have substituted for μp(t + 1) into Eq. 32a, which assumes that η《1. This is very close
to the Adam updates, except that the root-mean-square normalization is in the momentum updates,
7
Under review as a conference paper at ICLR 2019
(B)RMSprop	(B)Adam
Figure 2: Comparing RMSprop and BRMSprop on MNIST. The vertical bars represent one standard
error and are based on 30 independent runs. Adam and BAdam both use ηw = 0.1.
rather than the parameter updates. To move the location of the root-mean-square normalization, we
rewrite the updates in terms of a rescaled momentum,
μp(t + 1) = μp(t +1) Vz he2〉
giving,
μw(t +I) ≈ μw⑴ + ημpPh+1),
μp (t + I) ≈ (I- ηP ) μp(t) + ηPg(t),
which recovers Adam. See SI Sec. 9.6 for a similar discussion for NAG/NAdam
(33)
(34a)
(34b)
6	Experiments
We compared Bayesian and standard methods on MNIST. In particular, we trained a CNN with
relu nonlinearities and maxpooling for 50 epochs. The network had two convolutional layers with
10 channels in the first layer and 20 in the second, with 5 × 5 convolutional kernels, followed by
a single fully connected layer with 50 units, and was initialized with draws from a Gaussian with
variance 2/Ninputs. The network did not use dropout (or any other form of stochastic regularisation),
which would increase the variance of the gradients, artificially inflating he2i.
The key Bayesian-specific parameters are the variance of the stationary distribution and the ini-
tial uncertainty; both of which were set to 1/(2Ninputs). In principle, they should be similar to the
initialization variance, but in practice we found that using a somewhat lower value gave better per-
formance, though this needs further investigation. For the RMSprop and Adam specific parameters,
we used the PyTorch defaults.
Comparing the Bayesian and non-Bayesian methods, we do not expect to see very large discrepan-
cies, because they are approximately equivalent in steady-state. Nonetheless, the Bayesian methods
show considerably lower test loss, somewhat lower classification error, and similar training loss and
training error (Fig. 2). The local maximum in test-loss may arise because we condition on each
datapoint multiple times, which is not theoretically justified (correcting this is non-trivial in the
dynamical setting, so we leave it for future work).
7	Features of Bayesian stochastic gradient descent
Here we summarise the features of Bayesian stochastic gradient descent schemes, pointing out where
we recover previously known best practice and where our approach suggests new ideas.
8
Under review as a conference paper at ICLR 2019
7.1	Weight decay, L2 regularization and Bayesian priors
Loshchilov & Hutter (2017) recently examined different forms of weight decay in adaptive SGD
methods such as Adam. In particular, they asked whether or not the root-mean-square normalizer
should be applied to the weight decay term. In common practice, we take weight decay as arising
from the gradient of an L2 regularizer, in which case it is natural to normalize the gradient of the
objective and the gradient of the regularizer in the same way. However, there is an alternative: to
normalize only the gradient of the objective, but to keep the weight decay constant (in which case,
weight decay cannot be interpreted as arising from an L2 normalizer). Loshchilov & Hutter (2017)
show that this second method, which they call AdamW, gives better test accuracy than the standard
approach. Remarkably, AdamW arises naturally in BRMSprop and BAdam (e.g. Eq. 23), providing
a potential explanation for its improved performance.
7.2	Nesterov accelerated gradients/weight decay
In Nesterov accelerated gradients (NAG), we compute the gradient at a “predicted” location formed
by applying a momentum update to the current parameters (Nesterov, 1983). These updates arise
naturally in our Bayesian scheme, as the required gradient term (Eq. 17) is evaluated at μprE
(Eq. 15a), which is precisely a prediction formed by combining the current setting of the parameters,
μpost(t), with momentum and decay, embodied in A (Eq. 27) to form a prediction, μprior(t + 1). It
should be noted that the original Nesterov acceleration (Nesterov, 1983) had no gradient precon-
ditioning. Instead, our approach corresponds to NAdam (Dozat, 2016). Interestingly, as we also
implement weight decay through the dynamics matrix, A, we should also apply the updates from
weight decay before computing the gradient, giving, to our knowledge, a novel method that we
denote ”Nesterov accelerated weight decay” (NAWD).
7.3	Convergence
A series of recent papers have discussed the convergence properties of RMSprop and Adam, noting
that they may fail to converge (Wilson et al., 2017; Reddi et al., 2018) if the exponentially decaying
average over the squared gradients is computed with a too-small timescale (Reddi et al., 2018).
Our method circumvents these issues by coupling the learning rate to the timescale over which the
implicit exponential moving average for the mean-square gradient is performed. As such, in the
limit as the learning rates go to zero (i.e. η → 0 for BRMSprop and ηw → 0 and ηp → 0 for
BAdam), our method becomes SGD with adaptive learning rates that scale as 1/t and is therefore
likely to be convergent for convex functions (Robbins & Monro, 1951; Bottou, 1998), though we
leave a rigorous proof to future work.
7.4	Updating the preconditioner before applying the update
ADAM (Kingma & Ba, 2015) first updates the root-mean-square gradient normalizer before com-
puting the parameter update. This is important, because it ensures that updates are bounded in the
pathological case that gradients are initially all very small, such that the root-mean-square normal-
izer is also small, and then there is a single large gradient. Bayesian filtering naturally recovers this
choice as the gradient preconditioner in Eq. (19) is the posterior, rather than the prior, covariance
(i.e. updated with the current gradient).
8	Discussion
Here, we developed BRMSprop and BAdam, Bayesian versions of RMSprop and Adam respec-
tively. These methods naturally recover many features of state-of-the-art adaptive SGD methods,
including the root-mean-square normalizer, Neseterov acceleration and AdamW. As such, BRM-
Sprop and BAdam provide a possible explanation for the empirical success of RMSprop, Adam,
Nesterov acceleration and AdamW. Experimentally, we find, BRMSprop and BAdam are superior
to the standard methods in the tasks that we examined. Finally, the novel interpretation of adaptive
SGD as filtering in a dynamical model opens several avenues for future research, including new
stochastic regularisation methods and combinations with Kronecker factorisation.
9
Under review as a conference paper at ICLR 2019
We believe that it should be possible to combine the temporal changes induced by the factorised
approximations with other types of inference, including probabilistic backpropagation (Hernandez-
Lobato & Adams, 2015) or assumed density filtering (Ghosh et al., 2016). At present, our method
bears closest relation to natural gradient variational inference methods (Zhang et al., 2017; Khan
et al., 2018), as they also use the Fisher Information to approximate the likelihood. Indeed, our
method becomes equivalent to these approaches in the limit as we send the learning rate, η to zero.
The key difference is that because they do not formulate a factorised generative model, they are
unable to provide a strong justification for the introduction of rich dynamics, and they are unable to
reason about optimal inference under these dynamics.
Bayesian filtering presents a novel approach to neural network optimization, and as such, there are
variety of directions for future work. First, Bayesian filtering converts the problem of neural network
optimization into the statistical problem of understanding the dynamics of changes in the optimal
weight induced by optimization in the other parameters. In particular, we can perform an empirical
investigation in large scale systems, or attempt to find closed-form expressions for the dynamics in
simplified domains such as linear regression. Second, here we wrote down a statistical model for the
gradient. However, there are many circumstances where the gradient is not available. Perhaps a low
precision or noisy gradient is available due to noise in the parameters (e.g. due to dropout Srivastava
et al., 2014), or perhaps we wish to consider a biological setting, where the gradient is not present at
all (Aitchison et al., 2014). The Bayesian approach presented here gives a straightforward recipe for
developing (Bayes) optimal algorithms for such problems. Third, stochastic regularization has been
shown to be extremely effective at reducing generalization error in neural networks. This Bayesian
interpretation of adaptive SGD methods presents opportunities for new stochastic regularization
schemes. Fourth, it should be possible to develop filtering methods that represent the covariance
of a full weight matrix by exploiting Kronecker factorisation (Martens & Grosse, 2015; Grosse &
Martens, 2016; Zhang et al., 2017).
References
Laurence Aitchison, Alex Pouget, and Peter E Latham. Probabilistic synapses. arXiv preprint
arXiv:1410.1029, 2014.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251—
276, 1998.
Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimizing
recurrent networks. In Acoustics, Speech and Signal Processing, pp. 8624-8628. IEEE, 2013.
Leon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,
17(9):142, 1998.
Michael B Cohen, Jelena Diakonikolas, and Lorenzo Orecchia. On acceleration with noise-
corrupted gradients. arXiv preprint arXiv:1805.12591, 2018.
Timothy Dozat. Incorporating nesterov momentum into ADAM. In International Conference on
Learning Representations Workshop, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Lee A Feldkamp, Danil V Prokhorov, and Timothy M Feldkamp. Simple and conditioned adaptive
behavior from kalman filter trained recurrent networks. Neural Networks, 16(5-6):683-689, 2003.
Soumya Ghosh, Francesco Maria Delle Fave, and Jonathan S Yedidia. Assumed density filtering
methods for learning bayesian neural networks. In AAAI, pp. 1589-1595, 2016.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. In International Conference on Machine Learning, pp. 573-582, 2016.
William W Hager. Updating the inverse ofa matrix. SIAM review, 31(2):221-239, 1989.
10
Under review as a conference paper at ICLR 2019
Jose MigUel Hemandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861-
1869, 2015.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Overview of mini-batch gradient descent.
COURSERA: Neural Networks for Machine Learning: Lecture 6a, 2012.
Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, and Yarin Gal. Vprop: Variational infer-
ence using rmsprop. arXiv preprint arXiv:1712.01038, 2017.
Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivas-
tava. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint
arXiv:1806.04854, 2018.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Donald E Knuth. Big omicron and big omega and big theta. ACM Sigact News, 8(2):18-24, 1976.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint
arXiv:1711.05101, 2017.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence O(1/k2). In Doklady AN USSR, volume 269, pp. 543-547, 1983.
Yann Ollivier. Online natural gradient as a kalman filter. arXiv preprint arXiv:1703.00209, 2017.
Gintaras V Puskorius and Lee A Feldkamp. Decoupled extended kalman filter training of feedfor-
ward layered networks. In Neural Networks, 1991., IJCNN-91-Seattle International Joint Con-
ference on, volume 1, pp. 771-777. IEEE, 1991.
Gintaras V Puskorius and Lee A Feldkamp. Neurocontrol of nonlinear dynamical systems with
kalman filter trained recurrent networks. IEEE Transactions on neural networks, 5(2):279-297,
1994.
Gintaras V Puskorius and Lee A Feldkamp. Parameter-based kalman filter training: theory and
implementation. In Kalman filtering and neural networks. 2001.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR,
2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 400-407, 1951.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau.
Building end-to-end dialogue systems using generative hierarchical neural network models. In
AAAI, volume 16, pp. 3776-3784, 2016.
S Sha, F Palmieri, and M Datum. Optimal filtering algorithms for fast learning in feedforward neual
networks. Neural Networks, 1992.
Samuel L Smith, Daniel Duckworth, Quoc V Le, and Jascha Sohl-Dickstein. Stochastic natural
gradient descent draws posterior samples in function space. arXiv preprint arXiv:1806.09597,
2018.
Yang Song and Stefano Ermon. Accelerating natural gradient with higher-order invariance. arXiv
preprint arXiv:1803.01273, 2018.
11
Under review as a conference paper at ICLR 2019
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
LearningResearch,15(1):1929-1958, 2014.
Hector J Sussmann. Uniqueness of the weights for minimal feedforward nets with a given input-
output map. Neural networks, 5(4):589-593, 1992.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Lihua Xie, Dan Popa, and Frank L Lewis. Optimal and robust estimation: with an introduction to
stochastic control theory. CRC press, 2007.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as
variational inference. arXiv preprint arXiv:1712.02390, 2017.
9	Supplementary Information
9.1	S teady- s tate covariance
To understand the steady-state behaviour of the covariance, we begin by substituting Eq. 21 into
Eq. 15b to form a combined update,
∑prior(t + 1) = (I- A) (∑prior(t) -	) (I- A)T + Q.	(35)
1 + e Σprior (t)e
Note that this gives updates for Σprior(t), rather than Σpost(t), as this simplifies the algebra, but
we will move back to Σpost(t) at the end. As t → ∞, we reach steady state for which, Σprior =
Σprior(t) = Σprior (t + 1). Substituting this asympototic covariance into Eq. (35) and rearranging, we
obtain,
(I - A) (ΣprioreeTΣprior) (I - A)T
(ACpriorAT - ACprior- ςPriorAr + Q)(I + eςPriorer)
(36)
Now our goal is to find the asymptotic scaling of Cprior with η so as to obtain a valid approximation
to this steady-state expression. To begin, we know that,
A ∈ O (η)	Q ∈ θ (η2),
where O denotes an asymptotic upper bound, and Θ denotes an asymptotic bound on both sides
(Knuth, 1976). Here, η is the learning rate, which is assumed to be small, and hence we take the
limit of η → 0 for the purposes of the limits in O and Θ. Note that for Adam we have AAdam ∈ Θ(η)
but for RMSprop we have ARMSprop ∈ Θ(η2 ), and we summarise both using the asymptotic upper
bound above.
The right-hand-side of Eq. (36) is thus bounded above,
(AEpriorAT - ACPriOr- ςPriOrAT + Q) (1 + eTςPriOreT) ∈ O (η2 + nX# + η^ ⑺)(37)
where Σ(η) describes the scaling of Cprior,
Cprior ∈ Θ(Σ(η)).
12
Under review as a conference paper at ICLR 2019
The left-hand-side can be asymptotically bounded on both sides as limη→0 A = 0,
(I - A) (ΣprioreeτΣprior) (I - A)T ∈ Θ (∑2 (η)) .	(38)
The asymptotic upper bound (Eq. 37) for the right-hand-side of Eq. (36) must bound the asymptotic
lower bound for the left-hand-side (Eq. 38),
Σ2(η) ∈ O (η2 + η∑(η) + η∑2(η)).	(39)
Now, we consider the case in which each of the terms η2, ηΣ(η) or ηΣ2 (η) is asymptotically domi-
nant (potentially along with other terms). The ηΣ2 (η) term cannot dominate, because it cannot form
an asymptotic upper bound on Σ2(η). As such, Σ2 (η) could be asymptotically bound either by η2,
or by ηΣ(η) (or both simultaneously). If it is bound by η2, we have,
Σ2(η) ∈ O (η2)	=⇒	Σ(η) ∈ O (η)	(40)
Alternatively, if it is bound by ηΣ(η), we have,
Σ2 (η) ∈ O (ηΣ(η))	=⇒	Σ(η) ∈ O (η)	(41)
And as it must be bound by one (or both) of these terms, we know that Σ(η) and hence Σprior are
bounded by η,
Σ(η) ∈ O (η)	=⇒	Σprior ∈ O (η) .	(42)
Now that we have the scaling of Σprior, deriving a simplified approximation to Eq. (36) is straight-
forward. In particular, we consider only terms that scale with η2, neglecting those that scale with
η3. We now consider terms to be eliminated in Eq. (36). In particular, on the left-hand-side,
A (ΣprioreeτΣprior) ∈ O(η3),	(43a)
(ΣprioreeτΣprior) AT ∈ O(η3),	(43b)
A (ΣprioreeτΣprior) AT ∈ O(η4),	(43c)
and on the right-hand-side,
AEpriorAT (1 + eT ςPriore) ∈ O(n3),
(Q - AEprior- EpriorAT) (eTςPriore) ∈ O(η3).
Neglecting these terms, and instead focusing only on the O(η2) terms we obtain,
Eprior ee Eprior ≈ Q - AEprior - EpriorAT .
(43d)
(43e)
(44)
However, for the purposes of our updates, we are interested in the asymptotic behaviour of Epost, not
Eprior. We therefore substitute for Eprior from Eq. (15b) in Eq. (44), and neglect third-order terms.
First, we consider the the left-hand-side of Eq. (44),
EprioreeTEprior= ((I-A)Epost(I-A)+Q)eeT((I-A)Epost(I-A)+Q)
≈ EposteeT Epost .	(45)
And we do the same for the right-hand-side of Eq (44),
Q - AEprior - EpriorAT = Q - A((I - A) Epost (I - A) + Q) - ((I - A) Epost (I - A) + Q) AT
≈ Q - AEpost - EpostAT	(46)
Combining these approximations to the left and right hand sides, we obtain the result used in the
main text for Adam.
Epost eeT Epost ≈ Q - AEpost - EpriorAT .	(47)
For RMSprop, remember that ARMSprop ∈ O(η2 ), so AEpost ∈ O(η3), and we can obtain an even
simpler result,
EposteeT Epost ≈ Q.	(48)
These equations bear a remarkable resemblance to the continuous time Kalman filter that emerges
when you take the limit of small timesteps (Xie et al., 2007).
13
Under review as a conference paper at ICLR 2019
9.2 Setting the momentum decay
In the main text, we briefly note that we couple the injected noise and decay in Eq. (25) such that the
effect of p(t) on w(t) is analogous to that of unit-variance Gaussian noise. In particular, we require
that the stationary variance ofw(t) is again σ2, despite the momentum term in Eq. (26) being treated
as if it was unit-variance Gaussian noise. To show that the stationary variance of w(t) is indeed σ2,
we solve the Lyapunov equation for the stationary covariance, Ψ,
Ψ=(I-A)Ψ(I-A)T+Q,
where we do not condition on data. Neglecting the second-order term in A,
Q ≈ AΨ+ (AΨ)T ,	(49)
and as,
AΨ = (η2+η2 -η) (ψww ψwp) = (η2+η2ψww - ηψwp η⅛2ψwp — ηψpp
0	ηp	Ψwp Ψpp	ηp Ψwp	ηpΨpp
Eq. (49) becomes,
2 C2σ2W ψww - η^ψwp)	"2σ2W ψwp - η^ψpp + ηpψwp)
^2σ2w ψwp - η^ψpp + ηp ψwp	2ηpψpp	J
(50)
Now, we can solve for Ψpp using the ppth element of Eq. (50),
ψ = η
ψpp = 2 ,
and we can solve for Ψwp using the wpth element of Eq. (50), in combination with our solution to
Ψpp,
Ψwp
n%/2
ηp + η2+ηw
≈ η∕2.
This approximation becomes exact in the limit we consider to recover Adam (i.e. σ2 = ∞), but it
also holds when σ2 is finite, because η, ηw and ηp are all smaller than 1, and ηp — the “learning
rate” for the momentum — is usually larger than η and ηw (in fact, ηp can be as high as 0.1). Finally,
we can solve for Ψww using the wwth element of Eq. (50),
Ψww ≈ σ2,
as required.
9.3	Efficient implementation of BRMSprop
See Alg. 1 for a complete specification of our approach. There are two important points to note.
First, the method is as memory efficient as RMSprop, as the updates for Σ and μ can be done in-
place, so there is no need to maintain separate μprior and μpost and σprior and σpost in memory. Second,
we simplified the covariance updates in Eq. (24),
σ2	e2
2	_	2 J ι σ priore
σpost = σprior I 1 - 1 ,	2^^^2
1 + σprior e
σ2
prior
ι + σ^^e2.
1 + σprior e
(51)
9.4	Efficient implementation of BAdam
See Alg. 2 for a complete specification of our approach. There are two important points to note.
First, we have carefully ordered the updates so that they can all be done in-place, improving memory
efficiency. Second, we have simplified the derivation, by defining,
T = I - A.	(52)
14
Under review as a conference paper at ICLR 2019
Algorithm 1 BRMSprop
Require: η
Require: ηg
Require: σ2
1: Σ 一 σ2
2： μ 〜N (O) 2/NinpUts)
3:	g — 0
4:	while not converged do
5： g — VLt (μ)
6： ς 一夕/(1 + (9 -勺产对
7:	μ《—μ + Σg
8： g — (I — ηg) g + ηgg
9:	Σ —(1 - η2∕(2σ2))2 ∑ + η2
10:	μ —(1 — η2∕(2σ2)) μ
11:	end while
12:	return μ
. Learning rate
. Learning rate for average gradient (0.01)
. Prior variance (1/(2Ninputs))
. Initialize uncertainties
. Initialize parameters
. Initialize average gradient
.ς = σprior ⑴，μ = μprior ⑴
. Gradients of minibatch t
. Σ = σp2ost(t)
.μ = μpost(t)
. Update average gradient
. Σ = σp2rior(t + 1)
>μ = μprior(t + I)
9.5	Trick to allow learning rates to vary across parameters
We require that the initialization for Σ varies across parameters, according to the number of inputs
(as in typical neural network initialization schemes). While this is possible in automatic differentia-
tion libraries, including PyTorch, it is extremely awkward. As such, we reparameterise the network,
such that all parameters are initialized with a draw from a standard Gaussian, and to ensure that the
outputs have the same scale, we explicitly scale the output.
9.6	Nesterov Accelerated Gradient
To begin, we note that accelerated stochastic gradient descent remains an open research problem, in
which the optimality of plain NAG is unclear (Cohen et al., 2018). As such, our goal is again, not to
match NAG exactly, but to capture its essential insights within a Bayesian framework, so that we can
suggest improved methods. To highlight the link between our approach and Nesterov accelerated
gradient, We rewrite Eq. (32) in terms of v(t +1) = ημp (t + 1),
μw(t) ≈ μw(t — 1) + v(t)
V⑴ ≈ (I ― ηP) v(t ― I) + ηηp-g(=⅛,
he2i
(53a)
(53b)
which results in updates with a very similar form to standard momentum and Nesterov accelerated
gradient (e.g. Sutskever et al., 2013), with the addition of root-mean-square normalization for the
gradient. The key difference between momentum and Nesterov accelerated gradient is where we
evaluate the gradient: for momentum, we evaluate the gradient at μw(t — 1) (i.e. at μpost(t — 1)), and
for Nesterov accelerated gradient, we evaluate the gradient at a “predicted” location (i.e. at μprior (t)),
gMom(t) =	g (μw(t — I)) = g (μpost, w(t — I))	(54a)
gNAG(t) =	g (μw(t - 1) + (1 — ηp)v(t — I)) ≈ g	(μw(t -	1) + v(t	— I)) = g (μprior,w(t))	(54b)
As noted in Eq. (17), Bayesian filtering requires us to evaluate the gradient at μprior, implying that we
use updates based on NAG (Eq. 54b), rather than updates based on standard momentum (Eq. 54a).
9.7 Estimating the negative Hessian
One approach that works well in practice in natural-gradient methods (Amari, 1998; Martens &
Grosse, 2015; Grosse & Martens, 2016; Song & Ermon, 2018; Smith et al., 2018) is to use a Fisher-
15
Under review as a conference paper at ICLR 2019
Algorithm 2 BAdam
Require: η	. Learning rate
Require: ηg	.	Learning rate for average gradient (0.01)
Require: ηp	. Learning rate for momentum (0.1)
Require: ηw	. Learning rate for weight aione (0)
Require: σ2	. Prior variance (1/(2Ninputs))
1： Tww = 1 - (η2 + nW)∕(2σ2)	. Set dynamics matrix
2: Twp = η	. Set dynamics matrix
3： Tpp = 1 - ηp	. Set dynamics matrix
4：	
5: ∑ww《-σ2	. Initiaiize uncertainties
6: ςWP — 0	. Initiaiize uncertainties
7: ςpp — 0	. Initiaiize uncertainties
8: μw 〜N (0, 2/NinPUtS)	. Initiaiize parameter
9: μp4—0	. Initiaiize momentum
10: g — 0	. Initiaiize average gradient
11: while not converged do	
12:	g — VLt (μw)	. Gradients of minibatch t
13:	e2 — V(g — g)2	
14:	ςWW — ςWW - ςWWe2/(1 + ςWWe2)	
15:	ςWP《-ςWP - ςwwςwpe/(1 + ςwwc2)	. Σ = Σpost(t)
16:	ςpp J ςpp - ΣWρe2/(1 + ςwwC2)	
17:	μw J- μw + ςWWg	.μ = μρost(t)
18:	μp J- μp + ςWPg	
19:	g — (1 - ng) 0 + ngg	. Update average gradient
20:	Σww J Tww Σww + 2Tww Twp Σwp + Twp Σpp + ηw	
21:	Σwp J TwwTpp Σwp + TwpTpp Σpp	. Σ = Σprior (t + 1)
22:	Σpp J Tp2p Σpp + np2	
23:	μw J- TWwμw + Twpμp	.μ = μρrior(t + 1)
24:	μp J- TPPμp	
25: end while	
26: return μ	
Information based estimate, of the negative Hessian,
Alike ≈ EP(dmodei∣z)	-∂Z2 logP (dmodel|Z) = EP(dmodei∣z) [eeT],
where
e
∂ lθgP (dmodel∣z)
∂Z	Z=隅.
Z μprior
(55)
(56)
Importantiy, this Fisher-Information based estimate does not depend on actuai data, d. Instead,
it uses expectations (and gradients) taken over data sampied from the modei, dmodei, and this is
fundamentaiiy necessary for the the second equaiity in Eq. (55) to hoid. As such, to use Eq. (55) we
need to sampie surrogate data from the modei, and compute gradients with respect to that sampied
data. Unfortunateiy, sampiing surrogate data increases compiexity and the required computationai
cost. As such, a simpie iow-cost aiternative is to note that the key difference between e and g is that
e has zero-mean, whereas g has non-zero mean, suggesting that we shouid correct the gradient by
subtracting its mean,
e ≈ g - g
where g is an empirical estimate of the mean gradient.
16