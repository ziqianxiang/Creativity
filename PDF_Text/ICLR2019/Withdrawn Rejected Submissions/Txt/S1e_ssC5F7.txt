Under review as a conference paper at ICLR 2019
Hyper-Regularization: A Framework of Adap-
tive Choice for the Learning Rate in Gradient
Descent
Anonymous authors
Paper under double-blind review
Ab stract
We present a novel approach for adaptively selecting the learning rate in gradient
descent methods. Specifically, we impose a regularization term on the learning
rate via a generalized distance, and cast the joint updating process of the param-
eter and the learning rate into a maxmin problem. Some existing schemes such
as AdaGrad (diagonal version) and WNGrad can be rederived from our approach.
Based on our approach, the updating rules for the learning rate do not rely on the
smoothness constant of optimization problems and are robust to the initial learn-
ing rate. We theoretically analyze our approach in full batch and online learning
settings, which achieves comparable performances with other first-order gradient-
based algorithms in terms of accuracy as well as convergence rate.
1	Introduction
The automatic choice of the learning rate remains crucial in improving the efficiency of gradient
descent algorithms, especially for solving nonconvex optimization problems. It is desirable to adap-
tively update the learning rate during the training process with a certain strategy. The convergence
guarantees of such a strategy in theories usually require the Lipschitz constant or smoothness con-
stant of the objective function to be explicitly known (Nesterov, 2013; Bubeck et al., 2015), which
is inaccessible in most cases, e.g., in deep neural networks.
Using the received gradient information to adjust the current learning rate is a natural approach.
In particular, the Steepest Descent uses the received gradient direction and an exact or inexact line
search to obtain proper learning rates. Another important idea is to approximate second-order meth-
ods like Newton method (Nocedal & Wright, 2006) and Quasi-Newton Methods (Liu & Nocedal,
1989). Along this idea, for example, the Barzilai-Borweinin (BB) method (Barzilai & Borwein,
1988) in classical optimization and AdaGrad (Duchi et al., 2011) in online learning have been pro-
posed.
In this paper we propose a novel framework to learn the learning rate that we call Hyper-
Regularization. More specifically, we regard the learning rate as a hyperparameter and cast its
adaptive choice with the parameter training into a joint process. We formulate this process as a
maxmin framework by imposing a regularizer on the hyperparameter. Furthermore, we demonstrate
that AdaGrad and WNGrad (Wu et al., 2018) can be derived using a streamlined scheme based on
Hyper-Regularization.
In addition to solving the saddle point problem exactly, we also provide an alternating strategy to
solve the problem approximately. We respectively give theoretical analysis for these two updating
rules in full batch setting and online learning setting. Specifically, our results of runtime bounds
in full batch setting and regret bounds in online learning setting are comparable to the best known
bound in corresponding settings and indicate our algorithms converge for any initial learning rate.
1.1	related work
Steepest Descent uses the received gradient direction and an exact or inexact line search to obtain
proper learning rates. Although Steepest Descent uses the direction that descends most and the best
learning rate that gives the most function reduction, Steepest Descent may converge very slow for
1
Under review as a conference paper at ICLR 2019
convex quadratic functions when the Hessian matrix is ill-conditioned (see, Yuan, 2008). In practice,
some line search conditions such as Goldstein conditions or Wolfe conditions (see, Fletcher, 2013)
can be applied to compute the learning rate. In online or stochastic settings, one observes stochastic
gradients rather than exact gradients and line search methods become less effective.
The Barzilai-Borwein method (Barzilai & Borwein, 1988) which was motivated by quasi-Newton
methods presents a surprising result that it could lead to superlinear convergence in convex quadratic
problem of two variables. Although numerical results often show the Barzilai-Borwein method con-
verges superlinearly in solving nonlinear optimization problem, no superlinear convergence results
have been established even for an n-dimensional strictly convex quadratic problem with the order
n > 2 (Barzilai & Borwein, 1988; Dai, 2013). In minimizing the sum of cost functions and stochas-
tic setting, SGD-BB proposed by Tan et al. (2016) takes the average of the stochastic gradients in
one epoch as an estimation of the full gradient. But this approach can not directly be applied to
online learning settings.
In online convex optimization (Zinkevich, 2003; Shalev-Shwartz et al., 2012; Hazan et al., 2016),
AdaGrad (Duchi et al., 2011) adapts the learning rate on per parameter basis dynamically. Intuitively,
AdaGrad constructs approximation to the Hessian with diagonal of accumulated outer products of
gradients. This leads to many variants such as RMSProp (Tieleman & Hinton, 2012), AdaDelta
(Zeiler, 2012), Adam (Kingma & Ba, 2015), etc.
Additionally, Cruz (2011) analyzed Adaptive Stochastic Gradient Descent (ASGD) which is a gen-
eralization of Kesten’s accelerated stochastic approximation algorithm (Kesten et al., 1958) for the
high-dimensional case. ASGD uses a monotone decreasing function with respect to a time variable
to get learning rates. Recently, Baydin et al. (2018) proposed Hyper-Gradient Descent to learn the
global learning rate in SGD, SGD with Nesterov momentum and Adam. Hyper-Gradient Descent
can be viewed as an approximate line search method in the online learning setting and it uses the
update rule for the previous step to optimize the leaning rate in the current step. However, Hyper-
Gradient Descent has no theoretical guarantee.
It is worth mentioning that Gupta et al. (2017) proposed a framework named Unified Adaptive
Regularization from which AdaGrad and Online Newton Step (Hazan et al., 2007) can be derived.
However, Unified Adaptive Regularization gives an approach for approximating the Hessian matrix
in second order methods.
1.2	Notation
Before introducing our approach, we present the notation. We denote the set {x > 0 : x ∈ R} by
R++ . For two vectors a, b ∈ Rd, we use a/b to denote element-wise division, a ◦ b for element-
wise product (the symbol ◦ will be omitted in the explicit context), an = (a1n, a2n, . . . , adn), and
a ≥ b if aj ≥ bj for all j. Let 1 be the vector of ones with an appropriate size, and diag(β) be
a diagonal matrix with the elements of the vector β on the main diagonal. In addition, We define
ka∣∣A = √(a, Aai.
Given a set X ⊆ Rd, a function f : X → R is said to satisfy f ∈ CL1,1(X) if f is continuously
differentiable on X , and the derivative of f is Lipschitz continuous on X with constant L:
INf (X)- Vf(y)∣∣2 ≤ Lkx - y∣∣2.
More general definition can be found in Nesterov (2013).
1.3	Problem Statement
For an online learning problem, a learner faces a sequence of convex functions {ft } with the same
domain X ⊆ Rd, receives (sub)gradient information gt ∈ ∂ft (xt) at each step t, and predicts a
point xt+1 ∈ X.
Our theoretical analysis is based on two settings: full batch setting and online learning setting. A
full batch setting (or optimization setting) is to optimize a certain function F with exact gradient at
each step, i.e., ft = F . In this setting we assume F ∈ CL1,1 but it is not necessarily convex. We
analyze the convergence of our algorithms by capping the runtime T such that the minimum value
2
Under review as a conference paper at ICLR 2019
of the norm of received gradients so far is less than a given error accuracy ε, that is,
t=mTn 1 kVF(χt)k2 ≤ ε
In online learning settings, our analysis follows from Duchi et al. (2011) and Kingma & Ba (2015).
We only assume that ft ’s are convex and try to give an upper bound for the regret
T-1	T-1
R(T) = X ft(xt) - mTn X ft(x).	(1)
t=0	t=0
2	Hyper-Regularization
Following the setting in AdaGrad (Duchi et al., 2011), we consider a generalization of the standard
(sub)gradient descent as
Xt+1 = ∏Xiag(βt)ι∕2 (xt - diag(βt)-"gt) =argmin∣∣xt-diag(βt)-1∕2gt ∣∣	.(2)
、	X x∈X H	IIdiag(βt )1/2
This procedure can be viewed as the minimization problem:
min hgt, X - Xti + 1 kx - Xtkdiag(βt)∙	⑶
x∈X	2	t
To derive our hyper-regularization approach, we then formulate this minimization problem as a
saddle point problem by adding a hyper-regularizer about the difference between the new learning
rate β and an auxiliary vector ηt . Accordingly, we have
maxminψt(χ, β) , hgt, X - Xti + 1 (kx - Xtkdiag(e)- D(β,ηt)),	⑷
β∈Bt x∈X	2
where D(β, η) is defined as our hyper-regularizer and Bt is a subset in Rd. We solve the saddle
point problem for new predictor and new learning rate.
Our framework stems from the work of Daubechies et al. (2010), where the authors adjust the
weights of the weighted least squares problem by solving an extra objective function which added
a regularizer about the weights to origin objective function. The following subsections will explain
some details about our framework.
2.1	THE 夕-DIVERGENCE
It is reasonable to choose a distance function to measure the difference between β and η . In this
paper, We use the 夕-divergence1 as our hyper-regularizer.
Definition 1 (夕-divergence). Let 夕:R++ → R be a differentiable strongly Convexfunction in R++
such that 夕(1)= 夕0(1) = 0, where 夕0 is the derivative function of 夕.For Such a function 夕,the
function DW: R；+ X R；+ → R, which is define by
d
Dw(u, V), Evj 夕(Uj/Vj),
j=1
is referred to as the 夕-divergence.
Remark. Note that convex function 夕 with 夕(1)= 夕 0(1) = 0 satisfies 夕(Z) ≥ 0 for all z > 0,
thus DW(u, v) ≥ 0 for all u, v ∈ Rd++, with equality iffu = v.
Remark. For any convex function f,夕(Z) = f (z) — f 0(1)(z — 1) — f (1) is a proper function for
our 夕-divergence.
Remark. In our framework, in order to solve the problem (4) feasibly, we always assume that
limz→+∞ 20(z) = +∞. 1 *
1Comparing with Bregman divergence,g-divergence requires nonnegative β and η which is apparent for
learning rates.
3
Under review as a conference paper at ICLR 2019
If one only requires 夕 to be convex and 夕⑴=0, the resulting distance function DW is called a
f -divergence (Liese & Vajda, 1987; 2006). The f -divergence has been widely applied in statistical
machine learning (e.g., Nguyen et al., 2009).
Using the 夕-divergence as our hyper-regularizer, We can rewrite the problem (4) as
maχminψt(χ,β),g>(X — Xt) + 1 ∣∣χ — Xtkdiag(β)- 1 Dφ(β,ηt), or
β∈Bt x∈X	2	2
d1
maχminψt(χ, β) , Egtj (Xj- xt,j) + - (1¾(Xj- xt,j)2 — ηt,jS>βj/ηt,j)).
β∈Bt x∈X	2
j=1
(5)
(6)
The form of problem (6) implies that we can solve the problem for each dimension separately, and
only a few extra calculations are required for each step.
2.2	max-min or min-max
Consider a problem similar to (5),
xm∈iXnβm∈aBxtΨt(X,β).	(7)
The solution of problem (5) is same as (7) in unconstrained case2, i.e. X = Rd, Bt = Rd++ .
However, if we set Bt = [bt,ι,Bt,ι] ×∙∙∙× [bt,d, Bt,d] to constrain the range of βt+ι and suppose
X = Rd, the solution of (7) is more difficult to get from the solution of the unconstrained problem,
while the solution of (5) can be easily obtained by clipping the solution of the unconstrained problem
to required range (see Lemma 2). Thus, we choose (5) as our basic problem.
Lemma 2. Suppose that Bt = [bt,1,Bt,1] X …X [bt,d,Bt,d], and X = Rd. Let β* be
the solution of unconstrained problem maxβ (minx Ψt(X, β)). Then the solution of problem
maxβ∈Bt (minx Ψt(X, β)) is
βj = min{max{β*,bt,j},Bt,j}, for j = 1,…，d.
2.3	SELECTION OF ηt
There are many ways to choose the sequence {ηt}:
•	The sequence {ηt} is chosen in advance, before our methods start the job. For example,
set 1) ηt = η 1; 2) ηt = η√t + 11, where η > 0 is a prespecified constant.
•	ηt can be obtained from other adaptive learning rate methods such as AdaGrad.
•	Set ηt = βt. In this case, the hyper-regularizer is the penalty for the change between βt+1
and βt .
The first two ways can be treated as a smoothing technique to stabilize the learning rate. We want
to ensure the learning rate sequence {βt} close to another learning rate sequence {ηt}. Although
setting ηt = βt is our main focus, we keep the sequence of {ηt} to maintain this flexibility.
2.4	Isotropic Hyper-Regularization
We now show a special form of our framework that only adaptively maintains a single scalar learning
rate. We modify Hyper-regularization so that β is optimized over the set Bt ⊆ {θ1 : θ ∈ R++} of
all positive multiples of the vector 1 ∈ Rd. Let ηt = ηt1, and rewrite problem (5) as
max minψt(X,β)= g>(X — xt) + βkχ — χtk2 — 1 ntv(e/nt).	⑻
β1∈Bt x∈X	2	2
We prefer to use Isotropic Hyper-Regularization as smoothing technique in online learning settings.
2Using partial derivative, we can obtain the saddle point of Ψt and ensure this fact.
4
Under review as a conference paper at ICLR 2019
3	Update Rules and Algorithms
In this section we present two update rules of our hyper-regularization framework. The first update
rule is solving the saddle point problem (5) exactly. That is,
Ψt (xt+1, βt+1) = maxminΨt(x,β).	(9)
β∈Bt x∈X
The following lemma and Algorithm 1 give the concrete scheme of our iterations.
Lemma 3. Considering problem (6) without constraints and solving the problem exactly, we get
new predictor xt+1 and new learning rate βt+1 such that
βt+1,j 2/(et+1,j/ηt,j ) = gt,j,j = * 1,...,d,	(IO)
xt+ι = χt - gt/8t+∖.
Algorithm 1 GD with Hyper-regularization
Input: βo > 0, xo
1:	for t = 1 to T do
2:	Suffer loss ft (xt);
3:	Receive subgradient gt ∈ ∂ft(xt) of ft at xt;
4:	Update βt+ι,j as the solution of the equation β2 3 4 5 6φf(β∕ηt,j) = g2,j, j = 1,...,d;
5:	Update xt+ι = Xt - gt∕βt+1
6:	end for
Remark. Note that AdaGrad (Duchi et al., 2011) and WNGrad (Wu et al., 2018) are special cases
of Algorithm 1 with a particular choice of 夕(detailed derivation inAppendixB).
•	If 夕(Z) = Z + 1 — 2, then we can derive AdaGradfrom Algorithm 1.
•	If 夕(Z) = 1 — log( 1) — 1, then we can derive WNGradfrom Algorithm 1.
3.1	Alternating Update Rule
However, it is sometimes difficult to solve the equation (10), especially in the case where φ is a
quadratic function (equation (10) will be a cubic equation in one variable for any j ). In practice,
using an alternating update strategy is more recommended. Under the assumption that the optimal
value of β is close to ηt, we solve an approximate equation for βt+1
βt+1 = arg max Ψt argminΨt(x,ηt),β ,	(11)
β∈Bt	x∈X
and update the new predictor xt+1 using
xt+1 = argminΨt(x,βt+1).
x∈X
Applying the alternating update rule under the same assumption in Lemma 3, we obtain the follow-
ing lemma and Algorithm 2.
Lemma 4. Considering problem (6) without constraint and following from the alternating update
rule, we get new predictor xt+1 and new learning rate βt+1 as
Bt+ι,j = ηt,j (d)T(g2,j/ηt2,j ),j = 1,...,d,	(12)
xt+1 = xt - gt/βt+1.
Algorithm 2 GD with Hyper-regularization using alternating update rule
Input: βo > 0, xo
1: for t = 1 to T do
2:	Suffer loss ft (xt);
3:	Receive gt ∈ ∂ft(xt) offt at xt;
4:	UPdate 8t+ι,j = ηt,j3') 1g2j∕η2,j),j = ι,...,d;
5:	Update xt+ι = Xt - gt∕βt+1
6: end for
5
Under review as a conference paper at ICLR 2019
Computing the inverse function of 夕0 is usually easier than solving the equation (10) in practice,
especially for the widely used 夕-divergences (more details can be found in Appendix D.1).
Remark. If ηt = βt, the following simplified alternating rule can be employed:
xt+1 = arg min Ψt(x, βt),
x∈X
βt+1 = arg max Ψt (xt+1, β) .
β∈Bt
We leave the corresponding algorithm 3 in Appendix C.
3.2	Monotonicity
Before giving more analysis, let us show monotonicity of both the two update rules. The mono-
tonicity implies that only the property of convex function 夕 on interval [1, +∞) will influence the
efficiency of our algorithms.
Lemma 5. βt+1 obtained from equation (9) or (11) satisfies βt+1 ≥ ηt.
When setting βt = ηt , we have that βt+1 ≥ βt .
4	Theoretical Analysis
In this section we always set ηt = βt and assume that x and β are unconstrained, i.e., X = Rd
and Bt = Rd++ . We first discuss the convergence rate of the two update rules in full batch setting
with assumption that the objective function F is L-smooth but not necessarily convex in Section
4.1. Next we turn to online convex learning setting and establish a theorem about the regret bounds
in Section 4.2. Our results for both the settings show that our algorithms are robust to the choice of
initial learning rates and do not rely on the Lipschitz constant or smoothness constant.
4.1	Isotropic Hyper-Regularization in full batch setting
Recall that we set ft = F in the full batch setting, and assume that F ∈ CL1,1 without convexity. In
this case, two update rules can be written as
(β2+1 20(et+1/et) = kgtk2 ,	(13)
Iχt+1 = Xt- βt+1 gt.
Jβt+1 = βt(ψ0)-1(kgtk2 /β2),
lxt+1 = Xt- βt+ι gt.
(14)
Next we show that both update rules (13) and (14) are robust to the choice of initial learning rate.
Theorem 6. Suppose that 夕 ∈ C1,1 ([1, +∞)),夕 is α-strongly convex, F ∈ cL,1(Rd), and F * =
infx F(X) > -∞. For any ε ∈ (0, 1), the sequence {Xt} obtained from update rules (13) or (14)
satisfies
j=m⅛n_1kvF(Xj)k2 ≤ε,
after T = O (ε) steps.
More detailed results of Theorem 6 for runtime can be found in Theorems 21 and 22 in Appendix I.
Theorem 6 shows that both runtime of the two update rules can be bound as O(1∕ε) for any constant
L and initial learning rate β0. As a comparison, in classical convergence result ((1.2.13) in Nesterov
(2013) or Theorem 20 in Appendix), the upper bound of runtime is O(1∕ε) only for a certain range
(related to L) of initial learning rates.
4.2	Hyper-Regularization in Online Learning Setting
We now establish the result of convergence rate for Algorithms 1 and 2 in online convex learning,
i.e., the ft are convex. Especially, We try to bound regrets (1) by O(√T) for Algorithms 1 and 2.
6
Under review as a conference paper at ICLR 2019
Theorem 7. Suppose that 夕 ∈ C1,1 ([1, +∞)), and 夕 is α-strongly convex. Assume that kgtk∞ ≤
G, and ∣∣xt 一 x*k∞ ≤ D∞. Then the Sequence {xt} obtained from Algorithm 1 satisfies
2R(T) ≤ (α + D∞)p2lβ2 *■ XX kg0:T-ι,j∣2 + β0kx0 - x*k2,
αβ0	j =1
and the sequence {xt} obtained from Algorithm 2 (or 3) satisfies
2R(T) ≤ (ι+-α∞)max {√2l, β} ^X kg。：T-ι,jι∣2 + β0∣∣χ0 — x*ι∣2∙
Note that under the assumption in Theorem 7, Pd=Ikg0：T-1,jk2 ≤ dG√T, hence R(T) =
O(VT). Our result is comparable to the best known bound for convex online learning problem
(see Hazan et al., 2016; Duchi et al., 2011; Kingma & Ba, 2015).
5	Experiments
In this paper our principal focus has been to develop a new approach for adaptive choice of the
learning rate in first-order gradient-type methods. However, this new approach also brings some
insights into the resulting algorithms. Thus, it is interesting to conduct empirical analysis of the
learning algorithms with different choice methods for the learning rate.
5.1	The Set-up
To derive a learning algorithm from the Hyper-Regularization framework, we have to first give the
夕 divergences (Pardo, 2005). Specifically, the algorithms from our framework in the following
experiments are derived from the following four 夕 divergences (full implementations are displayed
in the Appendix D.1):
•	夕(t) = t log t -1 + 1 from KL-divergence inducing KL algorithm.
•	夕(t) = — log t +1 — 1 from Reverse KL-divergence inducing RKL algorithm.
•	夕(t) = (√t - 1)2 from Hellinger distance inducing Hellinger algorithm.
•	夕(t) = (t - 1)2 from χ2 distance inducing χ2 algorithm.
As mentioned in Section 3, even with a fixed 夕 divergence, the generated algorithm still varies with
different update rules. For simplicity, different update rules were compared in advance to select the
specific one for any 夕 divergence in the following experiments.
To maintain stable performance, the technique of growth clipping is applied to all algorithms in our
framework. Actually, growth clipping fulfills the constraints placed on the increasing speed of βt
by Bt in Lemma 2. Specifically, βt+1 in our experiments falls in [βt, 2βt]. Detailed observations
on how the βt of our algorithms increases are left in Appendix D.3.
Experiments involve the four algorithms generated above as well as other first-order gradient-based
algorithms including SGD (with no learning rate decay), SGD-BB, and Hyper-Gradient Descent
algorithms. These algorithms are evaluated on tasks of image classification with a logistic classifier
on the databases of MNIST (LeCun et al., 2010) and CIFAR-10 (Krizhevsky & Hinton, 2009). Initial
learning rate (in the usual sense, i.e., 1∕β0) varies from 10-3 to 101 for the test of convergence
performance of these algorithms. Experiments are run using Tensorflow (Abadi et al., 2016), on a
machine with Intel Xeon E5-2680 v4 CPU, 128 GB RAM, and NVIDIA Titan Xp GPU.
5.2	Update Rule Selection
Taking the RKL algorithm as an example, we refer to algorithms deduced from Algorithm 1, 2, and
3 as RKL1, RKL2, and RKL3. We train a two-layer neural network with a hidden layer of 500 units
on the MNIST database. Experiments are in online learning setting with a batch size of 128, and `2
regularization is applied with a coefficient of 10-4 .
7
Under review as a conference paper at ICLR 2019
Epoch
Epoch
Figure 1: Convergence performances of RKL1 , RKL2, and RKL3 on the database of MNIST in
online learning setting, up: performances at different initial learning rates, and down: the whole
training process with the given initial learning rate in the bracket for each algorithm.
Figure 1 demonstrates the training loss and test accuracy of these algorithms with various initial
learning rates. After fixing the learning rate with the least training loss, we compare their perfor-
mances throughout the training process. Sharing comparable performances at small initial learning
rate with RKL3 , RKL1 and RKL2 perform better at relatively large learning rate.
Generally speaking, Algorithm 1 often suffers a higher computation complexity than Algorithm 2
for the difficulty of getting an analytical solution. Detailed observations are left in Appendix D.1.
Therefore, we apply the second update rule to our algorithms without explicit notifications.
5.3	Full Batch Setting and Online Learning Setting
We investigate our algorithms in the full batch setting on the MNIST database where algorithms
receive the exact gradients of the objective loss function each iteration. 3
Epoch
Figure 2: Convergence performances of algorithms on MNIST in the full batch setting. left: the
train loss of the last training epoch at different initial learning rates, right: the whole training process
with the given initial learning rate in the bracket for each algorithm.
In terms of online learning setting, we train a VGG Net (Simonyan & Zisserman, 2014) with batch
normalization on the CIFAR-10 database with a batch size of 128, and an `2 regularization coeffi-
cient of 10-4. We as well perform data augmentation as He et al. (2016) to improve the training.
Figure 2 and Figure 3 show the convergence performances on both settings, respectively. Achieving
general comparable performances with other first-order gradient-based algorithms, our algorithms
outperform most of other algorithms at risky large initial learning rate. Even at the fixed initial
learning rate with the least training loss, the performances of our algorithms still achieve the same
training performances with others.
3Since it is a pure optimization problem, testing performance is out of our main consideration.
8
Under review as a conference paper at ICLR 2019
0.5
0.4
Initial Learning Rate
0.3
0.2
0.1
0.0
RKL(IOT25)
Heiiinger (10-1∙25)
X2 (io-10)
SGD (IO-125)
SGD-BB (IOT25)
Hyper-Gradlent (lθ^oa5)
KL (IQ-125)
20	40	60	80 IOO
Epoch
Epoch
3
C
P
Figure 3: Convergence performances of algorithms on CIFAR-10 in the online learning setting, left:
the train loss of the last training epoch at different initial learning rates, right: the whole training
process with the given initial learning rate in the bracket for each algorithm.
6	Discussion
As a supplement, we point out that logarithmic regret bounds under assumption ft is strongly con-
vex, like Hazan et al. (2007); Mukkamala & Hein (2017), can be established with a different class
of distance function. We leave the details in Appendix H.
In this paper, we propose a novel framework distinct from previous main approaches like line search
and approximate second-order methods. It worth noting that Hyper-Regularization can generates
efficient algorithms for optimization problems with regularization terms shown above and we expect
new ideas of more efficient terms in the future.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-
scale machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Jonathan Barzilai and Jonathan M Borwein. Two-point step size gradient methods. IMA journal of
numerical analysis, 8(1):141-148, 1988.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. On-
line learning rate adaptation with hypergradient descent. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=BkrsAzWAb.
Sebastien BUbeck et al. Convex optimization: Algorithms and complexity. Foundations and
TrendsR in Machine Learning, 8(3-4):231-357, 2015.
Pedro Cruz. Almost sure convergence and asymptotical normality of a generalization of kesten’s
stochastic approximation algorithm for multidimensional case. arXiv preprint arXiv:1105.5231,
2011.
Yu-Hong Dai. A new analysis on the barzilai-borwein gradient method. Journal of the operations
Research Society of China, 1(2):187-198, 2013.
Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan GUntUrk. Iteratively
reweighted least squares minimization for sparse recovery. Communications on Pure and Ap-
plied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 63(1):
1-38, 2010.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
9
Under review as a conference paper at ICLR 2019
Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 2013.
Vineet Gupta, Tomer Koren, and Yoram Singer. A unified approach to adaptive regularization in
online and stochastic optimization. arXiv preprint arXiv:1706.06569, 2017.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. Machine Learning, 69(2-3):169-192, 2007.
Elad Hazan et al. Introduction to online convex optimization. Foundations and TrendsR in Opti-
mization, 2(3-4):157-325, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Harry Kesten et al. Accelerated stochastic approximation. The Annals of Mathematical Statistics,
29(1):41-59, 1958.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [On-
line]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
F Liese and I Vajda. Convex statistical distances. 1987.
Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory.
IEEE Transactions on Information Theory, 52(10):4394-4412, 2006.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of RMSProp and Adagrad with log-
arithmic regret bounds. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 2545-2553, International Convention Centre, Sydney, Australia, 06-11 Aug 2017.
PMLR. URL http://proceedings.mlr.press/v70/mukkamala17a.html.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
XuanLong Nguyen, Martin J Wainwright, Michael I Jordan, et al. On surrogate loss functions and
f-divergences. The Annals of Statistics, 37(2):876-904, 2009.
Jorge Nocedal and Stephen J Wright. Numerical optimization 2nd, 2006.
Leandro Pardo. Statistical inference based on divergence measures. Chapman and Hall/CRC, 2005.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
TrendsR in Machine Learning, 4(2):107-194, 2012.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-borwein step size for stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 685-693, 2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
10
Under review as a conference paper at ICLR 2019
X. Wu, R. Ward, and L. Bottou. WNGrad: Learn the Learning Rate in Gradient Descent. ArXiv
e-prints, March 2018.
Ya-xiang Yuan. Step-sizes for the gradient method. AMS IP Studies in Advanced Mathematics, 42
(2):785, 2008.
M. D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. ArXiv e-prints, December 2012.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings ofthe 20th International Conference on Machine Learning (ICML-03),pp. 928-936,
2003.
11
Under review as a conference paper at ICLR 2019
A	S olution Existence
Note that the function h(β) = β2φ0(β∕ηtj) is an increasing continuous function and
limz→+∞ 夕0(z) = +∞ 夕0(l) = 0, so [0, +∞) is a subset of the range of h(β) and the SolU-
tion of (10) exists.
For the same reason, the solution of (12) exists.
B S pecial Cases of Algorithm 1
In this section, We will point out that Adagrad (Duchi et al., 2011) and WNGrad (Wu et al., 2018)
are special cases of Algorithm 1.
If we set 夕(Z) = Z + 1 - 2, then the new learning rate βt+ι can be obtained by
βt+ι,j (1 - ej： ) = gt,j，j = 1，∙∙∙，d，
that implies,
βt2+1 = βt2 +gt2,
and we drive AdaGrad from Hyper-Regularization.
Similarly, We can get WNGrad by setting 夕(Z) = 1 - log 1 - 1. In fact, βt+ι employs update
β2+ι,j (βtj)!= g2j,j = 1,…，d,
on the other words,
βt+1 = β + t,,
i.e., the update rule of WNGrad.
C S implified Alternating Update Rule
Algorithm 3 GD with Hyper-regularization using simplified alternating update rule and ηt = βt
Input: β0 > 0, x0
1:	for t = 1 to T do
2:	Suffer loss ft(xt);
3:	Receive gt ∈ ∂ft(xt) offt at xt;
4:	Update xt+ι = Xt - gt∕βt
5:	UPdate βt+ι,j = βt,j 3W：/偌》,j = 1,...,d;
6:	end for
The weakness of the simplified alternating update rule is that we get the new predictor xt+1 by
βt which is unrelated to current gradient gt . The regret bound of Algorithm 3 has been shown in
Theorem 7.
D Supplementary for Numerical Experiments
Full versions of Hellinger, KL, RKL, and χ2 algorithms through Algorithm 1, 2, and 3 are listed
below. Apparently, it is of great complexity to compute an analytical solution for βt+1 for the
equations marked red.
12
Under review as a conference paper at ICLR 2019
D. 1 Full Versions of Generated Algorithms
KL (β2+ι log(βt+ι∕βt) = g2
1 1xt+1 = Xt - gt∕βt+1
KL2 (βt+ι = β exP(g2/e2)
2	xt+1 = xt - gt∕βt+1
KL ∫Xt+ι = Xt - gt∕βt
3	[βt+ι = βt exp(g2∕β2)
RKL1
(
et+1
Xt+1
2 St+ Pe + 4g2)
Xt - gt∕et+1
RKL et+1 = et3∕(et2 - gt2 )
RKL2 Xt+1 = Xt - gt∕et+1
RKL3 Xt+1
RKL3 et+1
Xt - gt∕et
et3∕(et2 - gt2 )
H	βt2+1 (I - Yetlet+1 = g2
Xt+1 = Xt - gt∕βt+1
H	et+1 = et5∕(et2 - gt2)2
2 Xt+1 = Xt - gt∕et+1
H	Xt+1 = Xt - gt∕et
H3 et+1 =et5∕(et2 -gt2)2
χ2 2et2+1(et+1∕et - 1) = gt2
1 Xt+1 = Xt - gt∕et+1
χ2 ∫βt+ι = βt (1 + g2∕(2β2))
2 Xt+1 = Xt - gt∕et+1
2 Xt+1 = Xt - gt∕et
χ3[βt+ι = βt (1 + g2∕(2β2))
D.2 Version Comparison on Other Algorithms
Figure 4, 5 and 6 shows the results of comparison between versions of KL, Hellinger, and χ2 algo-
rithms on the base of MNIST in the online learning setting.
Epoch
Figure 4: Convergence performances of KL2 and KL3 on the database of MNIST in online learning
setting, up:at different initial learning rates, and down:the whole training process with the given
initial learning rate in the bracket.
13
Under review as a conference paper at ICLR 2019
Figure 5: Convergence performances of H2 and H3 on the database of MNIST in online learning
setting, up:at different initial learning rates, and down:the whole training process with the given
initial learning rate in the bracket.
Epoch
0.01
0.00
SSolU-E」J_
Epoch
Figure 6: Convergence performances of χ22 and χ23 on the database of MNIST in online learning
setting, up:at different initial learning rates, and down:the whole training process with the given
initial learning rate in the bracket.
D.3 FIGURES ON βt INCREASING PROCESS
All the following experiments are based on the MNIST database in online learning setting with a
batch size of 128. For simplicity, all initial β0s are fixed to be 10-0.5. Considering the sparse feature
of parameters in this task, figure 7 only observe the changing process of the maximum of βt .
14
Under review as a conference paper at ICLR 2019
母：3∙35
占 3.30
E
× 3∙25
(ŋ
W 3.20
3.40
点 EnE-XBW
Train Step
3.26
CQ
ma3∙24
E
E 122
X 3.20
ra
W 3.18
3.16
10000	15000	20000
Train Step
0	5000
10000	15000	20000
Train Step
Figure 7: The values of the maximum βt at each training step for various algorithms.
E Proof of Lemma 2
Proof. First, it is trivial to get
Ψt,χ(β)，minχΨt(x, β) = Ψt (Xt- g, β
=-2kgtkdiag(β)-1 - 2DW(e，ηt)
=-1XXX (gj+ηtjψ 目)).
The partial derivative of Ψt,x (β) with respect to βj is
dψt,χ(β) = 1 (脸”(宣)1
dβj	2	βj	η ∖ηt,J
Note that 夕 is a convex function, so φ0 is a non-decreasing function, and 々彳⑶ is a non-increasing
function. Recall that β* be the solution of unconstrained problem maxβ(minx Ψt(x, β)), hence,
βj is a zero of function dψ∂,β(β).
Moreover, if βj > Bt,j, We have 猥萨) ≥ 0. Thus, Ψt,χ (β) With respect to βj is a non-
decreasing function, and arg maxej Ψt,χ(β) = Btj. For a similar reason, if βj < bt,j, then
arg maxβj Ψt,x (β) = bt,j . In conclusion,
arg max Ψt,x(β) = min{max{β*,bt,j },Bt,j }, for j = 1,…，d.
βj ∈[bt,j ,Bt,j]
□
F Proof of Lemma 5
In this section, We denote that Ψt,x (β) = minx∈X Ψt (x, β).
Lemma 8. βt+1 obtained from equation (9) satisfies βt+1 ≥ ηt.
15
Under review as a conference paper at ICLR 2019
Proof. Recall that 夕⑴ =夕0(1) = 0, so 夕(x) ≥ 0 for all X and DJβ,ηt) = ηt中(β/ηt) ≥ 0. If
β < ηt, then for all x ∈ X
ψt(x,β) = g>(X - Xt) + 2 Ilx - χtk2 - 2Dφ(β,ηt)
< g>(X — xt) + ηt Iix - xt ∣∣2
= Ψt(X, ηt).
Hence, minx∈X Ψt(x, β) < minx∈X Ψt(x, ηt), i.e., Ψt,x(β) < Ψt,x(ηt).
It means βt+1 = arg maxβ∈B Ψt,x(β) ≥ ηt.
Lemma 9. βt+1 obtained from equation (11) satisfies βt+1 ≥ ηt.
□
Proof. Let y = arg minx Ψ(x, ηt). Ifβ < ηt, then
β1
ψt(y,β) = g>(y — xt) + 2 l∣y — xtk2 — 2Dψ(β,ηt)
< g>(y — xt) + ηt l∣y — xt I∣2
= Ψt (y, ηt).
Hence βt+1 = argmaxβ∈B Ψt(y, β) ≥ ηt.
□
G Convergence rates in online learning setting
Recall the define of regret
T-1
R(T) = X(ft(xt) - ft(x*)),
t=0
(15)
where x* = argminχ∈χ PT=-o1 ft(x). We show our hyper-regularizer method with three update
rules (Algorithm 1, 2 and 3) have O(√T) regret bounds.
Lemma 10 (Lemma 4 in Adagrad). Consider an arbitrary real-valued sequence {ai} and its vector
representation ai：i = (aι, …，ai)>. Then
T
X
t=1
a2
lalM∣2
≤ 2Ia1:T I2
(16)
holds.
Proof. Let us use induction on T to prove inequality (10). For T = 1, the inequality trivially holds.
Assume the bound (16) holds true for T - 1, in which case
X I⅛∣K ≤ 2|ai：T-lk2 + ∣μθ⅛.
We denote bT = PtT=1 at2 and have
2|a1：T-1ι2+∣μa⅛=2qbτ - aT+√⅛
≤ 2∖]bτ - aT + 学 + /
4bτ	bτ
= 2pT.
□
16
Under review as a conference paper at ICLR 2019
Lemma 11. Suppose the Sequence {xt} and Sequence {βt} satisfy xt+ι = Xt — gt/βt+∖. Then
the regret satisfies
T-1	T-1
2R(T) ≤ X kgtkdiag(βt+ι)-1 + X kxt - x*kdiag(βt+ι-βt) + kx0 - x*kdiag(βo)
t=0	t=0
Proof. Note that
xt+1 = xt — diag(βt+1)-1gt,
and
kxt+1 — x*kdiag(βt+ι)
=kxt — χ* — diag(βt+1)-1gtkdiag(βt+ι)
=kxt - x*kdiag(βt+ι) + kgtkdiag(βt+1)-1 - 2g> (Xt - x*),
i.e.,
2g> (Xt- x*) = kgtkdiag(βt+1)-1 + (kxt -x kdiag(βt+1 ) —kxt+1- x*kdiag(βt+ι)) .	(17)
Hence
T-1
2R(T) = 2 X(ft(xt) — ft(x*))
t=0
T-1
≤2 X g> (Xt- x*)
t=0
T-1	T-1
=X kgtkdiag(βt+1)-1 + X (kxt - x*kdiag(βt+ι) - kxt+1 - x*kdiag(βt+ι))
t=0	t=0
T-1	T-1
≤ X kgtkdiag(βt+1)-1 + X kxt - x kdiag(βt+1 -βt) + kx0 - x kdiag(β0).
t=0	t=0
□
Lemma 12. Suppose an increasing function ψ satisfies ψ(1) = 0 and ψ(x) ≤ l(x - 1). Consider
a real valued Sequence {gt}t=0：T-ι and a positive Sequence {βt}t=0:T which satisfies |gt| ≤ G,
β2+ιψ (β⅛1) = g2, t = 0, ∙∙∙ ,T - 1, βo ≥ 0. We can boundβτ as
βt ≥ ct
t-1
β2 + TXg2, t = 1,…，T
l i=0
(18)
where C = J §2+^2 八.Moreover, we have
T-1	2
X人≤
t=0 βt+1
,2lβ2 + 4G2
β0
T-1
X gt2 .
t=0
(19)
Remark. We point out that
•	βt+ι ≥ βt (If βt+ι < βt, then βt+1Ψ(βt+1∕βt) < 0 ≤ g2),
•	βt+ι is unique with respect to βt due to the fact that the function ψ(β) = β2ψ(β∕βt) is
strictly increasing.
17
Under review as a conference paper at ICLR 2019
Proof. Assume that βt ≥ CJβ0 + 2 P；-0 g2, where c > 0 is a variable coefficient.
Let us find out a specific c such that βt+1 ≥ cJβ2 + 2l Pt=0 g2.
Note that
g2 = β2+1ψ (ββr) ≤ lβ2+1 (βt⅛i - 1) .	(20)
Define a cubic polynomial
h(β) = β3β- - lβ2 - g2,
βt
and h is an increasing function when β ≥ βt .
If h (Cqβ2 + 2∙ Pt=0 g) ≤ O, according to h(βt+ι) ≥ 0, then βt+1 ≥ c,β2 + 2∙ Pt=0 g2.
Denote b = β2 + ： Pt=0 g2. So we just need to choose C such that
h (Ct β2+1X g2 j ≤ lc2(b+2g2/i)(√^ √Jgtt ~- 1!- g2 ≤0,
where the first inequality holds for the assumption βt ≥ cJ β2 + ： Pt-I g2, or
or
+ 2gt2/l)
2g2∕ι
Pp + 2g2” + √b
2g2∕l) ≤ b/+ + 2g2∕l + √b.
Thus, C just need to satisfy
c2 ≤ b
-b + 2g2∕l
According to + ≥ β02, gt2 ≤ G2, hence
b ≥ β2
b + 2g2∕l ≥ β0 + 2G2∕l∙
So ifwe choose C
/ ,、 β2
V β'0+2G2∕l,
then β1 > β0 > Cβ0, hence
u 2 t-1
β ≥ c' β2+7 y？g2,t=1,…，T.
l i=0
Moreover, following from Lemma 10, we have
T-1	2 T-1	2
X 工 ≤ X_______________gt
t=0 βt+1	t=0 c√2Tl JPt=0 g2
□
Lemma 13. Suppose an increasing function ψ satisfies ψ(1) = 0 and ψ(x) ≤ l(x - 1). Consider
a real valued Sequence {gt}t=0.τ-1 and a positive Sequence {βt}t=0:T which satisfies |gt| ≤ G,
β2ψ (β+1) = g2, t = 0,…，T — 1, β0 ≥ 0. We can bound βτ as
u 2 t-1
βt ≥ ʌ β0 + 7∑g2,t = 1,…，T.
l i=0
(21)
Moreover, we have
T-1	2
gt
〉≤ max
βt
t=0 t
(22)
18
Under review as a conference paper at ICLR 2019
Proof. Same as inequality (20), we have
hence
βt+ι =(βt + W )∖	β2 +	齐	≥ βO +	2 XX g2	≥ min {l, 2< 1	2 XXX g2,.
lβt	l	l i=0	2G	l i=0
Furthermore, following from Lemma 10, we have
T-1 g2
ɪθ βt ≤ m min{1,lβ2∕(2G2)}
l/2
T-1
X
=0
gt2
VzPi=O g2
≤ max
□
Theorem 14.	Suppose that 夕 ∈ C1,1 ([1, +∞)), and 夕 is α-strongly convex. Assume that kgtk∞ ≤
G, and ∣∣xt 一 x*k∞ ≤ D∞. Then the Sequence {xt} obtained from Algorithm 1 satisfies
2R(T) ≤
(α + D∞ ),2lβ2 +4G2
αβo
d
X kg0:T -1,j l∣2 + β0kx0 - x*k2,
j=1
Proof. Following from Lemma 11,
T-1	T-1
2R(T) ≤ X kgtkdiag(βt+ι)-1 + X lxt - x*kdiag(βt+ι-βt) + Ilx0 - x* kdiag(βo)
t=0	t=0
T-1	T-1
≤ X ∣gt∣2diag(βt+1)-1 + X ∣xt - x* ∣2∞ ∣βt+1 - βt∣1 + ∣x0 - x*
∣diag(β0)
t=0	t=0
T-1 d
≤ Xt=0 Xj=1
βt+1,j
T-1 d
+ 0m≤ta<xT ∣xt - x*∣2∞	(βt+1,j
t=0 j=1
- βt,j) + ∣x0 - x* ∣2diag(β0) .
Recall φ is a α-strongly convex function, so,
2	2	0 βt+1,j	βt+1,j
gt，j=βt+1,产 Iitr ≥ ≥ αβt+1 ,jβtj (-j--1),
and
t=0
The function ψ = 夕0 satisfies ψ(1)
Following from Lemma 12, we have
T-1	1 T-1
X B+ι,j - ％) ≤ α X
t=0
βt+1,j
(23)
0 and ψ(χ) ≤ l(χ 一 1) according to the smoothness of 夕.
X1 βgr ≤	uX2r=
t=0 t+1,j	0,j	i=0
Combining inequality (23) and (24), we have
J2lβ2,j+4G2
β0,j
kg0:T-1,j ∣∣2.
(24)
9D∕τ∖ V(I J max0≤t<T kxt - x*k∞、XXI gt,j	H	*∣∣2
2R(T) ≤ C + -------------a-----------j j=1 t=0 β+j + kx0 一 x kdiag(β0)
J2lβ2,j+4G2
β0,j
kg0:T-1,j ∣2 + IxO - x*kdiag(βo)
(α + D∞ )∖Z2lβ2 + 4G2
αβ0
d
X kg0:T -1,j l∣2 + β0kx0 - x*k2.
j=1
□
19
Under review as a conference paper at ICLR 2019
Theorem 15.	Suppose that 夕 ∈ C1,1 ([1, +∞)), and 夕 is α-strongly convex. Assume that kgtk∞ ≤
G, and ∣∣xt 一 x*k∞ ≤ D∞. Then the sequence {xt} obtained fromAlgorithm2(or3) satisfies
2R(T) ≤ (1+-α∞) max ∣√2l,访} ^X kg。：T -ι,j ∣∣2+mix。一 x*∣∣2∙
Proof. Similar to the proof of Theorem 14, for Algorithm 2, we have
T-1 d 2	T-1 d
2R(T) ≤ ΣΣ βgj + 0mtaxT B - x*k∞ X X(βt+ι,j - %) + kxo -x ∣diag(β0)
t=。 j=1 βt+1,j	t=。 j=1
T-1 d g2	T-1 d
≤ XX 答 +OmaXr kxt 一 x*k∞ X X(βt+1,j - βt,j ) + kx0 - x*kdiag(βo).
O0 0	0 ' t<r
t=。 j=1 t,j	t=。 j=1
With same reason, for Algorithm 3, we have
T-1
2R(T) ≤ X ∣gt ∣2d
t=。
T-1
≤ X ∣gt ∣2d
t=。
iag(βt)-1
iag(βt)-1
T-1
+ X (kxt 一 x*kdiag(βt) -kxt+1 一 x*kdiag(βt))
t=0
T-1
+ X kxt — x*kdiag(βt -βt-1) + kx0 一 x kdiag(β0)
t=1
T-1 d 2
≤X X W
T-1 d
+ 0maxr kxt - x*k∞ X X(βt,j - βt-1,j) + kx0 — x*kdiag(β0),
t=1 j=1
Note that for both algorithms
gt,j = β2,j”
βt+1,j
βt,j
≥ αβ[ - 1
holds, so
T-1	T-1	1 T-1 g2
X (βt,j- βt-ι,j) ≤ X (βt+ι,j- βt,j) ≤ - X j,
t=1	t=0	α t=0 βt,j
Thus, following from Lemma 13, for both Algorithm 2 and 3, we have
D2 d T-1 g2
2R(T) ≤ (1 + -∞)£ E 受 + βokxo 一 x*k2
α j=1 t=0 t,j
j=1
kg0:T -1,j k2 + β0 kx0 一
x*k2∙
□
H Logarithmic Bounds
In this section, we will use a different class of ‘distance’ function for problem 4, and establish
logarithmic regret bounds under assumption ft is strongly convex. Our analysis and proof follow
from Hazan et al. (2007); Mukkamala & Hein (2017).
First, We define μ-strongly convexity.
Definition 16 (Definition 2.1 in (Mukkamala & Hein, 2017)). Let X ⊆ Rd be a convex set. We say
that afunction f : X → R is μ-strongly convex, ifthere exists μ ∈ Rd with μj > 0 for j = 1, ∙∙∙ ,d
such that for all x, y ∈ X,
f(y) ≥ f(x) + "f(x), y - xi +1 ky - xkdiag(“”
Let ξ = minj=i：d μj, then this function is ξ-strongly convex (in the usual sense), that is
f (y) ≥ f (X) + Nf(X), y - xi + 2ky - xk2∙
20
Under review as a conference paper at ICLR 2019
The modification SC-Hyper-Regularization of Hyper-Regularization which we propose in the fol-
lowing uses a family of distance function D : Rd++ × Rd++ → R formulated as
d
D(U, V) = X 2(Uj/vj),	(25)
j=1
where 夕 is convex function with 夕(1)=夕0(1) = 0 like We used in 夕-divergence.
Remark. Same as 夕-divergence, D(u, V) ≥ 0 for any u, V ∈ R；+.
Different from Algorithm 1 and 2, we add a hyper-parameter λ > 0 like AdaGrad to SC-Hyper-
Gradient. Rewrite problem (5) as
1	λd
max minψt(x, β) , g>(x - Xt) + 5kx - Xtkdiag(β) - 5 EiXej/βt,j ),	(26)
β∈Bt x∈X	2	2
j=1
and corresponding two algorithms as
Algorithm 4 GD with SC-Hyper-regularization
Input: β0 > 0, X0
1:	for t = 1 to T do
2:	Suffer loss ft (Xt);
3:	Receive gt ∈ ∂ft(Xt) offt at Xt;
4:	Update βt+ιj as the solution of the equation λ(β2∕βtj)φ0(β∕βtj) = g2j,j = 1, ∙∙∙ ,d;
5:	Update xt+ι = Xt - gt∕βt+1
6:	end for
Algorithm 5 GD with SC-Hyper-regularization using alternating update rule
Input: βo > 0, X0
1:	for t = 1 to T do
2:	Suffer loss ft (Xt);
3:	Receive gt ∈ ∂ft(Xt) offt at Xt;
4:	Update βt+ι,j = βt,j (i)-1(gt,j/(λβt,j )),j = 1,...,d;
5:	Update xt+ι = Xt - gt/βt+1
6:	end for
Remark. Same as Lemma 5, the monotonicity of Algorithm 4 and 5 also holds.
Theorem 17. Suppose that ft is μ-strongly convex for all t,夕 ∈ CjI ([1, +∞)), and 夕 is a-
strongly convex. Assume that IlgtI∣∞ ≤ G, and λ ≥ G2∕(α minj=±d μj∙). Then the Sequence {xt}
obtained from Algorithm 4 satisfies
2R(T) ≤ l(l + 告Y Xln (l+ kg。：--1,jk2! + βo∣X0- x*k2,
λlβ0	j=1	λlβ0	2
and the sequence {Xt} obtained from Algorithm 5 satisfies
2R(T) ≤ lXln (l+ kg0：：12! + βokX0- x*k2,
Remark. Under assumption in Theorem 17, we have kg0:T-1,jk22 ≤ G2T, so R(T) = O(ln(T)).
To prove Theorem 17, we first prove following lemma.
Lemma 18. For an arbitrary real-valued sequence {ai} and a positive real number b,
T
X
t=1
a
b + Pt=I αi
≤ ln
(27)
21
Under review as a conference paper at ICLR 2019
Proof. Let b0 = b, bt = b + Pit=1 ai2, t ≥ 1, then
T
X
X a
⅛ b + Pt=I a
bt - bt-1
T	bt
≤XZ
t=1	bt-1
—dx
x
t=1
bT 1
—dx
bx
bt
T	bt
XZ
t=1	bt-1
- dx
bt
ln(l + ”
□
Like Lemma 12 and 13, similar lemma holds for Algorithm 4 and 5.
Lemma 19. Suppose an increasing function ψ satisfies ψ(1) = 0 and ψ(x) ≤ l(x - 1). Consider
a real valued Sequence {gt}t=0：T-ι and a positive Sequence {βt}t=0:T which satisfies |gt| ≤ G,
β0 > 0.
If (β2+ι∕βt)Ψ(βt+ι∕βt) = g2, t = 0,…，T - 1, then we have
β≥(优
βt ≥ [βo + G2/l
1 t-1
βo + ∣∑g2i ,t = 1, ∙∙∙ ,t
(28)
and
T-1	2
X gr- ≤ ι
t=0 βt+1
β0 + G2/l
β0
2 ln(l +m
lβ0
(29)
Meanwhile, if βtψ(βt+ι∕βt) = gt, t = 0,…，T 一 1, then we have
1 t-1
βt ≥ βo + 7∑>2, t = ι,…，t
i=0
(30)
and
T-1	2	PT-1
X g^- ≤ l ln 1 + ≡<t=0-
t=0 βt+ι	∖	lβ0
(31)
Proof. Using same methods in proof of Lemma 12 and 13, the conclusion can be deduced from
Lemma 18 easily.
proof of Theorem 17. Like Lemma 11, in strongly convex case, we have
T-1
2R(T) = 2 X ft(xt) - ft(x*)
t=0
T-1	T-1
≤ 2 X hgt, xt -x*i - X kxt -x*kdiag(μ)
t=0	t=0
T-1	T-1
T-1
X kgtkdiag(βt+ι)T + X (kxt - x kdiag(βt+ι) - kxt+1 - x kdiag(βt+ι)) 一 X kxt - x kdiag(μ)
t=0	t=0
T-1	T-1
≤ X kgtkdiag(βt+ι)T + X kxt - x kdiag(βt+ι-βt-μ) + kx0 一 x kdiag(βo)∙
t=0	t=0
Note that in Algorithm 4,
βt+1,j 一 βt,j = βt,j (βt+1,j ∕βt,j 一 1)
≤ β,j 0 (βt+1,j ʌ = β2,j gt,j ≤ G2
_ α # \ βt,j ) — β2+ι,j λα - λα
t=0
□
22
Under review as a conference paper at ICLR 2019
holds, and in Algorithm 5, same conclusion holds:
βt+1,j - βt,j = βt,j (βt+1,j /βt,j - 1)
≤ βtj ψ' (βt+1,j ) = g2j ≤ G2.
α	βt,j	λα λα
Hence, if λ ≥ maxj=i：d O2, then βt+ι 一 βt ≤ μ, and
T-1
X kxt 一 x*kdiag(βt+ι-βt-μ) ≤ 0.
t=0
On the other hand,
T-1
kgtk2diag(βt+1)-1
t=0
βt+1,j
following from Lemma 19, we have
d T-1
Xj=1Xt=0
T-1
kgtk2diag(βt+1)-1
t=0
kg0:T-Ij k2
-λlβ0-
in Algorithm 4,
Σ kgtkdiag(βt+1)-1 ≤ l X ln(l+ ⅛⅛jfi
t=0	j=1	0
in Algorithm 5.
□
I Convergence rates in Full batch Setting
In this section, we will discuss the convergence of our methods in full batch settings.
We first review a classical result on the convergence rate for gradient descent with fixed learning
rate.
Theorem 20. Suppose that F ∈ C；,1 (Rd) and F * = infχF (x) > 一∞. Consider gradient descent
with constant step size, xt+ι = Xt — "bxt'. If b > L, then
min
0≤t≤T -1
INF(Xt)II2 ≤ ε
after at most a number of steps
2b2(F(xo)- F*)	(1
=	ε(2b — L)	= V
Proof. Following from the fact that F is L-smooth, we have
F(Xt+1) ≤f(Xt) + VF(Xt)>(xt+1 — xt) + 2 kxt+1 - xtk2
=F(Xt) — b kVF(Xt)k2 + 2L kVF(Xt)k2
=F(Xt) — b (1 — L)kVF(Xt)k2.
When b > L, 1 — L > 0. So
T-1	2b2	2b2
E kVF (Xt)k2 ≤ w~r (F (xo) — F (XT)) ≤ w~r (F (xo) — F *),
2b — L	2b — L
t=0
(32)
and
min
0≤t≤T -1
IVF(Xt)I22
1 T-1
≤ T EkVF(Xt)k2 ≤
T t=0
2b2
E可(F(…F *) ≤ ε
□
23
Under review as a conference paper at ICLR 2019
Remark. Ifwe choose b ≤ L, then convergence of gradient descent with constant learning rate is
not guaranteed at all.
Like Algorithm 3, another update rule is worth considering in full batch setting:
∫xt+ι = Xt — β gt,
[βt+ι = βt(" )-1(kgtk2 /β2)
(33)
Next we will show that both update rules (13) and (14) are robust to the choice of initial learning
rate. Our proof is followed from the proof of Theorem 2.3 in WNGrad (Wu et al., 2018). Note that
in update rule (13), βt+1 satisfies
β2+1^0(βt+1 /βt) = kgtk2 ,
while in update rule (14) and (33), βt+1 satisfies
β2 "βt+"β/ = kgtk2.
Theorem 21 (Convergence rate of update rule (13)). Suppose that 夕 ∈ C1,1 ([1, +∞)),夕 is a-
Strongly ConVex, and F ∈ C^,1(Rd), F * = inf X F (x) > -∞. For any ε ∈ (0, 1), the sequence
{xt } obtained from update rule (13) satisfies
j=mT-JNF(Xj)k2 ≤ε,
after T steps, where
卜 + 2(e0+2(F(XO)-F；)/a)(F(XO)-FDm if β0 ≥ L or β1 ≥ L,
T = ∖ 1 , Γ iog(βL0) ] , Γ(l+(1+2)(F(XO)-F*+⅛≡))2]	.
I 1 + iog(金。+i) + ʌ-------------------ε-------------- otherwise.
Theorem 22 (Convergence rate of update rule (14)). Suppose that φ ∈ Cl1,1 ([1, +∞)), φ is α-
strongly convex, and F ∈ CLi,i(Rd), F* = infX F(X) > -∞. For any ε ∈ (0, 1), the sequence
{Xt } obtained from update rule (14) satisfies
j=0miι-1 kVF(Xj)k2 ≤ ε
after T steps, where
T
1+
1+
l2(βo + kgok2∕(αβo) + 2(F(XO)-F*)∕α)(F(XO)-F* )
ε
-iog(卷)]Γ (L+泰L2+αL+(1+8)(F(XO)-F*+
log( lL2+ 1) +	ε
≥ L or β1 ≥ L,
IL(L-βO) ))2
2βO 〃
otherwise.
Theorem 23 (Convergence rate of update rule (33)). Suppose that 夕 ∈ Cl1,1 ([1, +∞)), φ is a -
strongly convex, and F ∈ CL1,1(Rd), F* = infX F(X) > -∞. For any ε ∈ (0, 1), the sequence
{Xt } obtained from update rule (33) satisfies
min
j=0:T-1
kVF(Xj)k2 ≤ ε
after T steps, where
1+
2+
2(β0 + 2(F(XO)-F*)∕α)(F(xo)-F* )
ε
+kg¾O2+(1+2)(F (XO)-F*+
)	if β0 ≥ L,
lL∣∣gO∣∣2∖∖2-
2αβ2 刀
if β0 < L, β1 ≥ L,
I ∕L∖	^^∣	"j2l'r I 2lL2 I < -I I 2 ∖ (口(2 *	17*1 IL ((Λ ∖2l ∖ τ ∣ 2lL2 Q ∖∖∖2
log(访)	((1+ 2l )L+ OLO + (1+ 2 )(F(XO)-F + K ((1+ 2l)L+ ~αβO-价)〃
+ iog(后+1)	+	ε
otherwise.
We begin our proof by following lemma.
24
Under review as a conference paper at ICLR 2019
Lemma 24. Suppose 夕 ∈ C1,1(R++). Fix ε ∈ (0,1]. In both update rules (13) and (14), after
T = ιog(gεβ+i) + 1 steps, either mint=。：T-1 IlgtlI2 ≤ ε, or βτ ≥ L holds.
Proof. Assume that βT < L and mint=0:T -1 Igt I22
increasing sequence. Hence, βt < L for 0 ≤ t ≤ T.
So, for all 0 ≤ t ≤ T - 1,
> ε. Recall that the sequence {βt } is an
ψ,
βt+1	Igt I22
βt
β2+ι
ε
L2
(in Algorithm 1),
ψ,
βt+1	Igt I22
βt
ε
-j-( (in Algorithm 2).
愣
>
>
Note that 夕 is a l-smooth convex function, and βt+ι∕βt ≥ 1. So
「≤ 1(手-1
βt	βt
(34)
then
βt+ι > ε , 1
丁 > 取+1.
In this case,
L>βT=β0
T
holds but it is impossible according to the setting of T in the lemma.
□
We first prove Theorem 21 using following lemma.
Lemma 25. In update rule (13), suppose F ∈ cL,1(Rd),夕 ∈ C1,1(R++), and 夕 is α-strongly
convex function. Denote F * = inf X F (x) > 一∞. Let t。≥ 1 be the first index such that βt0 ≥ L.
Then for all t ≥ t0,
2
Bt ≤ βto-1 +--(F(XtO-I)- F ),	(35)
α
and moreover,
F(Xt0-1) - F* ≤ F(XO) - F* + M~(Bt0-1 - BO)	(36)
2β0
Proof. Same as equation (32),
F(Xt+1) ≤ F(Xt) - N— (1 -	—) kgtk2 .
Bt+1	2Bt+1
For t ≥ tO - 1, Bt+1 ≥ L, so
1
F(Xt+i) ≤ F(Xt) - 2β^ I∣gtk2.
Hence, for all k ≥ 0,
F(Xto+k) ≤ F(Xto-ι) - 1 XT kgtθ+i-1k2 ,
2 i=O	Bt0+i
i.e.,
XXX kgt0+iτk2 ≤ 2(F(Xto-1) - F*).
i O	Bt0+i
(37)
(38)
25
Under review as a conference paper at ICLR 2019
Note that 夕 is α-strongly convex and βW+3(βt+ι∕βt) = IlgtII2∙ So
and
kgtk2
βt+1
βt+" (*
≥ αβt (*-1
βt+1 -
βt ≤ 1 里
α βt+1
(39)
Combining equation (38) and equation (39), we have
βt0+k ≤ βt0 -1 +
1 XX kgto+i-1k2
α i=0	βto+i
2
≤ βt0-1+ α(F(xt0-1)- F ).
We remain to give an a upper bound for F(xt0-1) in the case t0 > 1. Using equation (32) again,
we get
t0-2	1	L
F(xt0T) — F(x0) ≤ X -西(1- K,
L 0-~^ 0 (βi+1 ʌ r Ll
=2 S M = )≤ ɪ
Ll 0 - βi+1 - βi
≤ ɪ ⅛ V I =
kgi k2 ≤ 2X2 厚<
2 i=0 βi+1
Fβr-i)
2β0 RtOT- β0).
In the above, the second inequality follows from the assumed l-smoothness of 夕，and the last in-
equality follows from βt ≥ βo for all t ≥ 0.	□
proof of Theorem 21. If t0 = 1, by equation (37), for all t ≥ 1, we have
F(xt) ≤ F(x0) -
I X ⅜⅛
2 i=0 βi+1
≤ F(x0) -
ι X	kgik2
2 i=0 βo + 2 (F (χo) — F *)
Then after T = 1 + [2(e0+2(F(XO)-F；"闲(F(XO)-FD] steps,
min
t=o:T -1
kgtk2 ≤ T ∑ kgtk
T t=o
2
2
22
≤ T(F (xo)—F *)(侑 r(F (xo)-F *)) ≤ J
Otherwise, ifto > 1, we have βtO-1 < L. Then for all t ≥ to,
βt ≤ L + 2
α
IL(L - βo) A
-2βo-)
(40)
Denote the right hand of equation (40) as βmax . Using equation (37) again, for we have
F(xtO+M) ≤ F(xtO-1) -
1 X kgto+i-1k2
2 ⅛	βto+i
≤ F(xtO-1) -
1
2βmax
M
X IgtO+i-1I22.
i=o
26
Under review as a conference paper at ICLR 2019
Hence,
一nm⅛w Jgtk2 ≤ 一 曹n" 1 k9tkl
t—0. to + ^M —1	t—to — 1. ⅛0 + Jm —1
1 A11	„2
≤ M + 1 JS kgt0+i-1 k2
+ i—0
≤ M+12βmax(F(xto —1) - F*)
≤ '2βmax (F (X(I) - F * + IL(L - β0 )
≤ M + 1 C(X0) F +	2βo )
At last, with recalling the conclusion of Lemma 24, after
log(春)
log(击 +1)
2βmax (F (X0) - F * + IL(LZ- β°))
+1
+
steps, we have mint—0：T—1 IIgt ∣∣2 ≤ ɛ.
□
Next we prove Theorem 22.
Lemma 26. In update rule (14), suppose F ∈ C^i(Rd),夕 ∈ CjI(R++) ,and 夕 is α-strongly
COnveXfUnCtion. Denote F * = inf X F (x). Let t0 ≥ 1 be the first index such that βt° ≥ L. Thenfor
all t ≥ t0,
_	_	8	.
βt ≤βto + a(F(XtOT) - F),
and moreover,
F(Xt0 —1) - F* ≤ F(XO) - F* + ；^-(民0 —1 - β0),
2β0
βt	≤ (β0 + 嗯2 if t0 =1,
CI - 1l+忌L2+αL if t0 ≥ 2,
(41)
(42)
(43)
Proof. Same as the proof of Lemma 25, we first get for all k ≥ 0,
k
X
i—0
∣gt0+i_1 k 2
βt0+i
≤ 2(F(Xt0 —1) - F*).
Note that in update rule (14), β2夕，(βt+1∕βt) = IlgtIl2. So
βt0 +k+1 = βt0 +k + βt0 +k
β βt0+k+1
∖ βt0+k
-1
≤ βt0+k +
≤ βt0+k +
≤ βt0+k +
≤ βt0+k +
βt0+k S
α
β βt0+k+1
∖ βt0+k
βt0+k +
1	kgt0+k∣∣2
α βt0+k
2	kgt0+k- gt0+k—1∣∣2 + kgt0+k—1∣∣2
α	βt0+k
2 L2 kXt0 + k - Xt0+k—1∣∣2 + kgt0 + k—1∣∣2
α	βt0 + k
2 L2 kgt0+k—1∣∣2 . 2 kgt0+k—1∣∣2
------------------I---------------
α	βt30+k	ɑ	βt0+k
≤
“0+k + 4 kgt0+k-1k2
Q	βt0+k
≤ βt0 +
4 X kgt0+"1∣∣2
α ―	βt0+i
8
≤ βt0 + Q(F(Xt0-1)- F).
27
Under review as a conference paper at ICLR 2019
If t0 = 1, then
βt0 ≤ β0 +
kgok2
αβo
and if t0 ≥ 2, then
βt0
≤ βt0-1 +
βt0-1 +
kgto-ιk2
αβt0-i
2L2 kgt0-2k22
≤ βt0-1 +
α	傥0-1
2L2 l(βt0-1
+ 2 kgto-2k2
α	βt0 -2
- βt0 -2)βt0 -2
α
≤ L +斗L2 + 2lL.
αβ0	α
At last, for t0 > 0, we have
β3o-1
2
+ αl(βt0-1- βt0-2)
F(x
2⅛) kg，"2
≤ f tX2 照
2 i=0 βi+1
V L tX kgik2
≤工⅛尸
—
Y tX2 "(")≤ IL tX2( + 1)
≤ 2 X2 (;)W(小…
□
proof of Theorem 22. The proof is completely similar to the proof of Theorem 21.	□
Next, we prove Theorem 23.
Lemma 27. In update rule (33), suppose F ∈ cL,1(Rd),夕 ∈ C1,1(R++), and 夕 is α-strongly
Convexfunction. Denote F * = inf X F (x). Let to ≥ 0 be the first index such that βt° ≥ L. Thenfor
all t ≥ t0,
and moreover,
2
β ≤ β + a(F (χto)- F *)，
F(xt0 ) - F* ≤ F(x0) - F* + g(∕βt0 - β0),
2β0
βt0 ≤
(β0 + kgβ02 , if to ≥ 1,
1(1 + ⅛) L + 赢 L2, if to ≥2.
(44)
(45)
(46)
Proof. Same as the proof of Lemma 25, we first get
F(Xt0+k+1) ≤ F(xt0+k) -，(1 - 2βL+k)
12
≤ F (xto+k ) - Lβ古十卜 kgto + k k2
kgt0 +k k2
≤ F(xt0) -
1 X kgto+ik2
2 =	βto+i
28
Under review as a conference paper at ICLR 2019
and
X 叱。/2 ≤ 2(F(xto) - F*).
i=0 βto+i
Note that in Algorithm 2, β2夕0 (βt+ι∕βt) = IlgtI∣2∙ So
βto + k+1 = βto+k + βto +k ('to + k+1 - 1
∖ βto + k
「 β	I βto + k 0 ( βto + k + l ʌ
≤ βto+k +-------2 F----------
Q ∖ βto + k J
_ 1	I 1 llgto + k l∣2
=βto+k +--------5-----
Q βto + k
≤ βto + 1 X ⅛+<
Q = βto+i
2
≤ βto +	(F(XtO ) - F ).
Q
At last, for to > 0, we have
F(xto) - F(xo) ≤ L XI 亨
i=0 βi
L tχ 0 ( βi+1、
=2 S D
≤ 2β0(βto-β0)∙
Recall that
αβt(βt+ι - βt) ≤ IlgtII2 ≤ iβt(βt+ι - βt).
So if to = 1,
βto ≤ βθ + lg0τ
Qβ0
And if t0 ≥ 2, we have
βto ≤ βto-1 +
kgto-ιk2
≤ βto-1 + 2
Qβto-1
,llgto-1 - gto-2∣∣2 + ∣∣gto-2∣∣2
≤ βto-1 + 于眄
αβto-i
；o-1 - xto-2∣∣2
βto
βto-1 +
2L2 kgt0-2k
)-1
2
2
2 IIgtO-2∣∣
+—
Q	β2o-2βto-1
2 llgto-2Il
+----
βto-2
≤ βto-1 +
2L2 l(βto-1 - βto-2)
Q	βto-2βto-1
βto-2
2
2
2
+ αl(βto-1- βto-2)
α
α
2
2
≤ L+与 L2 + -L.
ɑβo Q
□
proof of Theorem 23. The proof is completely similar to the proof of Theorem 21.	□
29