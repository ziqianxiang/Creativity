Under review as a conference paper at ICLR 2019
Transfer Learning via Unsupervised Task
Discovery for Visual Question Answering
Anonymous authors
Paper under double-blind review
Ab stract
We study how to leverage off-the-shelf visual and linguistic data to cope with out-of-
vocabulary answers in visual question answering. Existing large-scale visual data
with annotations such as image class labels, bounding boxes and region descriptions
are good sources for learning rich and diverse visual concepts. However, it is not
straightforward how the visual concepts should be captured and transferred to
visual question answering models due to missing link between question dependent
answering models and visual data without question or task specification. We tackle
this problem in two steps: 1) learning a task conditional visual classifier based on
unsupervised task discovery and 2) transferring and adapting the task conditional
visual classifier to visual question answering models. Specifically, we employ
linguistic knowledge sources such as structured lexical database (e.g. Wordnet)
and visual descriptions for unsupervised task discovery, and adapt a learned task
conditional visual classifier to answering unit in a visual question answering model.
We empirically show that the proposed algorithm generalizes to unseen answers
successfully using the knowledge transferred from the visual data.
1	Introduction
People see and understand a visual scene from diverse perspectives. For example, from a single image
of a chair, people effortlessly recognize diverse visual concepts such as color, material, style, usage,
and so on. These diverse perspectives may be associated with different questions in natural language
for which people find appropriate answers. Recently visual question answering (VQA) (Antol et al.,
2015) is proposed as an effort to learn deep neural network models with capability to perform diverse
visual recognition tasks defined adaptively by questions.
Approaches to VQA rely on a large-scale dataset of image, question and answer triples, and train
a classifier taking an image and a question as inputs and producing an answer. Despite recent
remarkable progress (Yang et al., 2016; Fukui et al., 2016; Anderson et al., 2018), this direction has a
critical limitation that image, question and answer triples in datasets are the only source for learning
visual concepts. Such drawback may result in lack of scalability because the triplets may be collected
artificially by human annotators with limited quality control and have weak diversity in visual concepts.
In fact, VQA datasets (Goyal et al., 2017; Agrawal et al., 2018) suffer from inherent bias, which
hinders learning true visual concepts from the datasets. On the contrary, people answer a question
based on visual concepts learned from diverse sources such as personal experience, books, pictures,
and videos, which are not necessarily associated with target questions. Even for machines, there
exist more natural and scalable sources for learning visual concepts: image class labels, bounding
boxes and region descriptions. Such information is already available in large-scale (Deng et al., 2009;
Krasin et al., 2017; Krishna et al., 2017) and can scale further with reasonable cost (Papadopoulos
et al., 2016; 2017). This observation brings a natural question; can we learn visual concepts without
question annotations and transfer them for VQA?
To address this question, we introduce VQA with out-of-vocabulary answers, which is illustrated
in Figure 1. External visual data provide a set of labels A and only a subset of these labels B ⊂ A
appears in VQA training set as answers. The goal of this setting is to handle out-of-vocabulary
answers a ∈ A - B successfully by exploiting visual concepts learned from external visual data.
To address this problem, this paper studies how to learn visual concepts without questions and how
to transfer the learned concepts to VQA models. To learn transferable visual concepts, we train a
1
Under review as a conference paper at ICLR 2019
VQA
example
Answers
Figure 1: VQA with out-of-vocabulary answers. Given a set of labels in visual data A and a set
of answers in VQA training set B, we evaluate the model on VQA test set with answers a ∈ A - B.
External visual data provide a set of bounding box labels and visual descriptions.
task conditional visual classifier, whose task is defined by a task specification vector. The classifier
is used as an answering unit where a task specification vector is inferred from a question. To train
the task conditional visual classifier without task annotations, we propose an unsupervised task
discovery technique based on linguistic knowledge sources such as structured lexical databases, e.g.,
Wordnet (Fellbaum, 1998), and region descriptions. We present that the proposed transfer learning
helps generalization in VQA with out-of-vocabulary answers.
The main contribution of our paper is three-fold:
•	We present a novel transfer learning algorithm for visual question answering based on a task
conditional visual classifier.
•	We propose an unsupervised task discovery technique for learning task conditional visual
classifiers without explicit task annotations.
•	We demonstrate that the proposed method enables to answer with out-of-vocabulary answers
based on knowledge transfer from visual data without question annotations.
The rest of the paper is organized as follows. Section 2 discusses prior works related to our approach.
We describe the overall transfer learning framework in Section 3. Learning visual concepts by
unsupervised task discovery is described in Section 4. Section 5 analyzes experimental results and
Section 6 makes our conclusion.
2	Related Works
Standard VQA evaluation assumes identically distributed train and test set (Malinowski & Fritz, 2014;
Antol et al., 2015; Zhu et al., 2016). As this evaluation setting turns out to be vulnerable to models
exploiting biases in training set (Goyal et al., 2017), several alternatives have been proposed. One
approach is to reduce observed biases either by balancing answers for individual questions (Goyal
et al., 2017) or by providing different biases to train and test sets intentionally (Agrawal et al., 2018).
Another approach is to construct compositional generalization split (Johnson et al., 2017; Agrawal
et al., 2017) whose question and answer pairs in test set are formed by novel compositions of visual
concepts and question types appearing in the training set. This split is constructed by repurposing an
existing VQA dataset (Agrawal et al., 2017) or by constructing a synthetic dataset (Johnson et al.,
2017). The problem setting studied in this paper is similar to Teney & Hengel (2016) in the sense that
out-of-vocabulary answers are used for testing, but unlike the prior work, we formulate the problem
as a transfer learning where out-of-vocabulary answers are learned from external visual data.
External data are often employed in VQA for better generalization. Convolutional neural net-
works (Krizhevsky et al., 2012; He et al., 2016) pretrained on ImageNet (Deng et al., 2009) is a
widely accepted standard for diverse VQA models (Yang et al., 2016; Fukui et al., 2016). As an alter-
native, object detector (Ren et al., 2015) trained on Visual Genome (Krishna et al., 2017) is employed
to extract pretrained visual features (Anderson et al., 2018). Pretrained language models such as word
embeddings (Pennington et al., 2014) or sentence embeddings (Kiros et al., 2015) are frequently used
2
Under review as a conference paper at ICLR 2019
leather
)anana§
Visual Data pv(a, r)
Linguistic Knowledge Sources
_ A I L the girl holding a <blank> :
ion t： task specification
：¼vqa(I,q^-Jτηvqa(q)j
■ I j--1----- -----j-----
∖ [ I: image ∖ 'l q : question
Unsupervised Task Discovery
Pre-training	Transfer to VQA
Figure 2: Overview of the proposed algorithm. (Left) Unsupervised task discovery learns a task
conditional visual classifier by leveraging off-the-shelf visual data without task specification t. It also
defines a task distribution pT (t|a, r) using linguistic knowledge sources, where stochastic sampling
associates a task specification t with a visual annotation (a, r). (Center) A visual annotation with
a task specification, denoted by (a, r, t), is employed to pretrain a task conditional visual classifier.
(Right) Pretrained task conditional visual classifier is transferred to VQA with the learned parameters
and adapts input representations vφvqa (I, q) and τηvqa (q) without fine-tuning.
Γ: region a： answer


to initialize parameters of question encoders (Noh et al., 2016; Fukui et al., 2016; Teney et al., 2018).
Exploiting information retrieval from knowledge base (Auer et al., 2007; Bollacker et al., 2008) or
external vision algorithms (Wang et al., 2017b) to provide additional inputs to VQA models was
investigated in (Wu et al., 2016; Wang et al., 2017a;b). Sharing aligned image-word representations
between VQA models and image classifiers was proposed in (Gupta et al., 2017) to exploit external
visual data. However, transfer learning from external data to cope with out-of-vocabulary answers in
VQA has hardly been investigated so far.
Transfer learning from external data to cope with out-of-vocabulary words is actively studied in novel
object captioning (Anne Hendricks et al., 2016; Venugopalan et al., 2017; Yao et al., 2017; Lu et al.,
2018). Anne Hendricks et al. (2016) and Venugopalan et al. (2017) decompose image captioning
into visual classification and language modeling and exploit unpaired visual and linguistic data as
additional resources to train visual classifier and language model respectively. Recent approaches
incorporate pointer networks (Vinyals et al., 2015) and learn to point an index of word candidates (Yao
et al., 2017) or an associated region (Lu et al., 2018), where the word candidates are detected by a
multi-label classifier (Yao et al., 2017) or an object detector (Lu et al., 2018) trained with external
visual data. These algorithms are not directly applicable to our problem setting because they focus on
predicting object words without task specification while the task conditional visual recognition is
required for VQA.
3	Algorithm Overview
The main objective of this paper is to learn visual concepts using off-the-shelf visual data and transfer
the concepts to VQA models. To adapt the learned visual concepts to diverse questions about a
visual scene, we should learn not only a name of a visual concept but also a type of the concept. For
example, a question about a dog image can be about name of the animal as well as its visual attributes
such as breed and color. We introduce a task conditional visual classifier for the purpose, in which a
type or granularity of the concept is specified by a task specification vector and the final answer is an
output of the specified recognition task. We pretrain this task conditional visual classifier using visual
data without question or task specifications, and adapt it to VQA models by transferring the learned
parameters. Figure 2 illustrates overview of the proposed algorithm.
3.1	Task Conditional Visual Classifier
Task conditional visual classifier is a function taking a visual feature v ∈ Rd and a task specification
vector τ ∈ Rk and producing a probability distribution of answers a ∈ [0, 1]l. It is formulated as a
neural network with parameter θ, and models a conditional distribution pθ (a|v, τ). The inputs v and
T are typically constructed by external feature encoders vφ(∙) and τη(∙), respectively.
3
Under review as a conference paper at ICLR 2019
In the proposed transfer learning scenario, a task conditional visual classifier is pretrained with
off-the-shelf visual data and transferred to VQA. In the pretraining stage, the model parameter θ
and the parameters for external feature encoders φ and η are jointly learned by back-propagation
as described in Equation 2. This stage allows a task conditional visual classifier to learn diverse
visual recognition tasks pτ (a|v) by varying task specification vector τ. Transfer learning to VQA is
achieved by reusing parameter θ and adapting another external feature encoders Vφvqa(∙) and TnVqa(∙)
to task conditional visual classifier. We first describe transfer learning to VQA in Section 3.2 while
pretraining a task conditional visual classifier with off-the-self visual data by unsupervised task
discovery is described in Section 4.
3.2	Transfer Learning for Visual Question Answering
As illustrated in Figure 2, the proposed VQA model contains a task conditional visual classifier
Pθ (a|v, τ). The pretrained visual concepts are transferred to VQA by defining the function with the
learned parameters θ. Then, learning a VQA model is now formulated as learning input representa-
tions V and T for p&(a|v, T), which is given by
φvqa, ηvqa = arg max EPVqa(a,I,q)logpθ (alvφvqa (L q),¾∙qa ⑹),	⑴
φvqa ,ηvqa
where vφvqa (I, q) is an encoded visual feature with an image I and a question q using an attention
mechanism with parameter φvqa, and Tηvqa (q) is a task specification vector encoded with a question
q using an encoder parameterized by ηvqa. The pvqa(a, I, q) is a training data distribution of VQA
dataset. We learn φvqa and ηvqa by stochastic gradient descent with maximum likelihood objective
while the parameter for pretrained task conditional visual classifier θ remains fixed.
Matching visual features To reuse pretrained visual classifier in VQA without fine-tuning, visual
features v should not be changed. This is fulfilled in recent approaches for VQA that do not fine-tune
pretrained visual feature extractors. In this setting, all we need to do is using an identical visual
feature extractor for both pretraining and VQA. Specifically, we use attention mechanism (Kim et al.,
2016) on a pretrained bottom-up attention features (Anderson et al., 2018), where attention based on
bounding boxes is used for pretraining and question-based attention is employed for VQA.
Weakly supervised task regression Utilizing a pretrained task conditional visual classifier to perform
visual recognition specified by a question q requires to infer an task specification vector Tqv . This
requirement introduces a learning problem—task regression—that optimizes an encoder Tηvqa (q) to
correctly predict Tqv . Instead of directly minimizing E (Tqv , Tηvqa (q)) with additional supervision, we
exploit VQA data as a source of weak supervision. We propose to optimize a mapping from a visual
feature to an answer using an indirect loss denoted by E(p「* (a|v), p6(a|v, TnVqa(q))); the resulting
training objective is identical to Equation 1.
Out-of-vocabulary answering We learn VQA by adapting input representation while fixing the
pretrained task conditional visual classifier p&(a|v, T). This strategy allows a model to focus on
learning to infer a visual recognition task Tnvqa (q) from questions, which does not require data for all
possible answers. Once the task specification vector T is inferred, the learned task conditional visual
classifier p&(a|v, T) can answer pretrained visual concepts including out-of-vocabulary answers.
4	Unsupervised Task Dis covery
Learning a task conditional visual classifier with the off-the-shelf visual data (Krishna et al., 2017) is
not straightforward due to missing annotation for task specifications. To address this issue, we propose
unsupervised task discovery, which exploits a modeled task distribution to sample a task specification
instead of collecting additional data. The motivation of this approach is that output domain of a visual
recognition task reflects our understanding about hierarchy of concepts or knowledge of the world.
For example, classification problems about electronic devices and holdable objects both restricts
possible outputs to the corresponding word groups, which are defined by our prior knowledge. This
knowledges is often accessible with linguistic knowledge sources, which is used for modeling a task
distribution. We describe an algorithm to learn task conditional visual classifier with modeled task
distribution in Section 4.1 and discuss how linguistic knowledge sources are used to model a task
distribution in Section 4.2.
4
Under review as a conference paper at ICLR 2019
[gamenO1 J
______________________________________
CarcLgame.n.Ol ] ∣ atheleti Jgame.n.O；
CriCket.n.θΓj KoOtbalI.n.01)1 boxing.n.01
name ts	answer set Wts
appliance.n.02	freezer, hair dryer, refrigerator, fridge, oven, dishwasher,..
opening.n.10	manhole, rear window, exit, nozzle, car window, coin slot, ..
food.n.02	potatoes, french fries, chicken, melted cheese, tomatoes, ..
move.v.03	twisted, ruffled, curving, curved, wheeled, rolled, coiled, ..
act.n.02	fishing, skateboarding, sailing, playing baseball, surfing, ..
area.n.01	middle, in corner, parking space, playground, landscape, ..
color.n.01	yellow, pink, red, beige, royal blue, amber, aqua, dark red, ..
Figure 3: Illustration of Wordnet and extracted answer set. (Left) A subgraph of the Word-
net (Fellbaum, 1998). Complex hierarchy of words reveals the diverse categorization of each words.
(Right) A set of words sharing common parents in the tree is grouped as a single answer set. Diverse
grouping of words reveals diverse level of understanding about the world.
4.1	Pretraining with Decomposed Data Distribution
Learning task conditional visual classifier is naturally formulated as maximizing expected log
likelihood as
θ* ,φpre ,nPre = arg max EpD (a,r,t) logPθ (a|v0pre S),τηpre (t)),	⑵
θ,φpre ,ηpre
where vφpre (r) is a visual feature encoded by a region of interest r given by a whole image or a
bounding box annotation, τηpre (t) is a task specification vector regressed from a task specification
t, and {θ, φpre, ηpre} are model parameters. This objective requires a joint distribution pD (a, r, t),
which is modeled by dense annotations of (a, r, t) triples. However, this approach limits the utility
of existing large scale visual annotations, which often consist of (a, r) pairs only.
We decompose the joint distribution into two components—one is for visual annotation pV (a, r) and
the other is for task conditioned on visual annotation pT (t|a, r)—which is formally given by
pD(a,r,t) =pT(t|a,r)pV(a,r).	(3)
Note that this formulation facilitates utilizing existing visual annotations pV (a, r) by appropriately
modeling pT (t|a, r). With the decomposed data distribution, the joint distribution for (a, r, t) is
approximated by sampling (a, r) from pV (a, r) followed by sampling t from pT (t|a, r). Modeling
pT (t|a, r) with linguistic knowledge sources is described next.
4.2	Leveraging Linguistic Knowledge Sources
A visual recognition task defines a mapping from visual inputs to a set of answers, where a range of
the mapping is a finite set of answers, which is a subset of a common category set. Especially, when
there is no ambiguity about which entity in a visual scene is referenced1, a visual recognition task is
uniquely defined by specifying the range of a mapping. This intuition leads to a simple task modeling
approach by treating a task as a set of answers. We exploit linguistic knowledge sources to extract
the answer set. Specifically, we consider the following two sources: 1) a structured lexical database
called Wordnet (Fellbaum, 1998) and 2) visual descriptions that are provided with a visual data.
Wordnet Wordnet (Fellbaum, 1998) is a lexical database represented with a directed acyclic graph of
disambiguated word entities, called synsets. The graph represents a hierarchical structure of Wordnet,
where the parents of a node correspond to hypernyms of the word in the child. We make a simple
assumption that a set of words sharing common ancestors in the graph can construct an answer set in
our concept hierarchy. A sample subgraph with extracted answer sets is illustrated in Figure 3. Given
a list of answer sets extracted from Wordnet, a task specification ts is a name of the answer set Wts
in the list. A task distribution is modeled by a distribution of answer set pT (ts|a) given an answer
a, where a task specification ts is independent of region annotation r. We assign zero probability
for answer sets that do not contain the target answer {ts |a 6∈ Wts } and uniform probability to all
answer sets that contain the answer {ts|a ∈ Wts}, where Wts denotes an answer set named ts. Word
embedding is used to encode ts into a task specification vector τηpsre (ts ).
1In the proposed VQA model, ambiguity of reference is usually resolved by an attention model.
5
Under review as a conference paper at ICLR 2019
Total [Object 十 Attribute]
Proposed
Separable Classifier ----------- Answer Embedding
Attribute Answers
----Standard VQA
Object Answers
Figure 4: Model comparisons. Exploiting external data with unsupervised task discovery boosts
performance of the proposed model and separable classifier significantly. However, separable
classifier showed limited performance gain on attribute answers, which have significant variations
depending on tasks.
-----Description + Wordnet ----------- Description --------- Wordnet
Total [Object + Attribute]
Object Answers
Attribute Answers
Figure 5: Data comparisons. Using visual description and Wordnet shows different generalization
characteristics and combining them brings additional improvement.
Visual description We construct an answer set by selecting a word from a description and iden-
tifying possible alternatives of the selected one. Specifically, we employ a blanked description
td = [w1, ..., wTd] as a task specification, which is constructed by spotting an answer a from a
description and replacing it to a special token <blank>. We define a task distribution pT (td|a, r) as a
distribution of description p(d|a, r) because a blanked description td is deterministically constructed
from a description d and an answer a. Note that the distribution of description p(d|a, r) is determined
by a dataset. Given td, a task specification vector τηd (td) is encoded by a gated recurrent unit (Chung
et al., 2014).	p
5	Experiments
We evaluate how effectively the proposed framework leverages the external data without questions to
answer out-of-vocabulary words in visual question answering. We compare the proposed method
with baselines equipped with idea from zero-shot image classification (Frome et al., 2013) and novel
object captioning (Anne Hendricks et al., 2016; Venugopalan et al., 2017), which are related to the
proposed problem. We also analyze the impact of the external data used for pretraining and visualize
the mapping between questions and task specifications learned by weakly supervised task regression.
5.1	Datasets
Pretraining We learn visual concepts about most frequently observed 3,000 objects and 1,000
attributes in Visual Genome dataset (Krishna et al., 2017). We construct external visual data with
region bounding box annotations, which are provided with region descriptions. Then, we extract
visual words—answer candidates—from region descriptions and construct visual data pairs (r, a).
We also construct blanked description (d) from the region description by replacing visual words
with <blank>. Note that distribution p(d|a, r) is either zero or one depending on whether the
description d is from the region r or not. We use 1,169,708 regions from 80,602 images for training
data construction. To use Wordnet (Fellbaum, 1998), we map visual words to synset using synset
annotations from visual genome dataset, and words that are not covered by the annotation are mapped
using Textblob (Loria, 2018) python library.
6
Under review as a conference paper at ICLR 2019
OIOOS <o<> ΦTOSΦ><
.86 4 2
Figure 6: Complementary characteristics of visual description and Wordnet. Wordnet shows
advantage on answers related to specific categorizations such as species of a bird (e.g. goose and
pigeon) and visual description is more effective on answers about interactions (e.g. holding).
Dataset construction We repurpose VQA v2 dataset to construct a train/test split as illustrated in
Figure 1. We use training and validation set of VQA v2. To ensure that every out-of-vocabulary
answer appears during pretraining, we select out-of-vocabulary answers from the pretrained visual
words. Among 3,813 visual words observed in VQA dataset, we randomly select 954 answers
as out-of-vocabulary answers. Since we focus on transferability of visual words, answers about
yes/no and numbers are not considered in our evaluation. Based on the selected out-of-vocabulary
answers, we split questions into 462,788 training, 51,421 validation, 5,176 test-validation and 20,802
test splits. The training and validation splits do not contain out-of-vocabulary answers while test-
validation and test splits consist of out-of-vocabulary answers only. We plan to make our splits
publicly available. Evaluation is performed on the test split using the standard VQA protocol with 10
ground-truth answers annotated for each question (Antol et al., 2015). The VQA score is given by
100 ∙ min(3 p1==11(g%, a), 1), where a is a predicted answer, g% is the i-th ground truth answer
and 1(∙) is an indicator function.
5.2	Baselines
Since leveraging external visual data for visual question answering with out-of-vocabulary answers
has hardly been explored, there is no proper evaluation benchmark and we employ the following
baselines to compare with the proposed model:
•	Answer embedding employs idea from zero-shot image classification (Frome et al.,
2013) that learns mapping from visual features to pretrained answer embedding. We
use GloVe (Pennington et al., 2014) to embed each answer.
•	Separable classifier adopts idea from novel object captioning (Anne Hendricks et al., 2016;
Venugopalan et al., 2017) that learns visual and language classifier separately and combines
them by element-wise sum of logits for joint inference. This baseline and the proposed
model are trained with the same data.
5.3	Results
Model comparisons Figure 4 illustrates model comparison results with standard VQA, answer
embedding and separable classifier. For this experiment, we perform VQA adaptation with 4 different
random seed and plot mean and standard deviation of VQA accuracy. The standard VQA model
achieves 0 VQA score because there is no clue for inferring out-of-vocabulary answers. Answer
embedding baseline generalizes slightly better by exploiting similarity of answer words in the
embedding space. However, we observe very marginal performance improvement because mapping
visual features onto answer embedding space is learned with insufficient information available only
in the subset of answers. Using off-the-shelf visual data and task specifications from linguistic
knowledge sources dramatically improves performance both for the separable classifier baseline and
the proposed model. However, independent consideration of visual data and task specifications as
in separable classifier has a critical limitation to model joint interaction between task specification
and visual features. Especially, this baseline illustrates substantially lower performance on attribute
answers, which have significant variations depending on tasks. Note that the bias in the VQA
training set cannot be exploited in the proposed evaluation setting, as the evaluation is performed
with out-of-distribution answers only.
7
Under review as a conference paper at ICLR 2019
Q: what type of place is this animal Q: is this furniture modern or antique? Q: what kind of bird is this? Q: what holiday are these bears for?
located?
Ours: modern
Ours: seagull	Ours: Christmas
Ours: zoo / Sep-cls: zebra* Sep-cls: commercial*
Figure 7: Out-of-vocabulary answers with diverse types of concepts. Green and red color denote
correct and wrong answers respectively. Asterisk(*) denotes answers appearing in the training set.
Answers without asterisks are out-of-vocabulary answers.
Sep-cls: multicolored*
Table 1: Learned mapping between a question and a task specification. We retrieved questions
for each answer sets based on the similarity score between task specification vectors. Results show
that appropriate task specifications are regressed from each questions. No explicit supervision is used
for mapping between questions and task specifications.
Answer set ts
organiC-ProCeSS.n.01
athletic _game.n.01
furniture.n.01
fruit.n.01
time_period.n.01
tool.n.01
hair.n.01
Questions
What are the giraffes doing? / What are the animals doing?
what type of sport ball is shown? / what type of sport are the men partiCipating in?
What pieCe of furniture are the Cats sitting on? / What furniture is the Cat sitting on?
What type of fruit is the animal eating? / What type of fruit juiCe is on the Counter?
What kind of season is it? / What type of season is it?
What utensil is in the person ’s hand? / What utensil is laying next to the bread?
what hairstyle does the surfer have ? / what type of hairstyle does this man have ?
Data comparisons Figure 5 illustrates the effeCt of different linguistiC sourCes on the proposed
model. Visual desCription and Wordnet for the proposed model show Complementary CharaCteristiCs
and additional improvement is aChieved by their Combination. To study the Complementary CharaCter-
istiC of visual desCription and Wordnet, we visualize average VQA sCore for 50 answers in Figure 6.
As shown in answers goose and pigeon, Wordnet has advantage on the questions about fine-grained
ClassifiCation suCh as speCies of a bird. This is beCause this Categorization is expliCitly modeled in the
word hierarChy in Wordnet. On the other hand, visual desCription is more effeCtive on the questions
related to type of interaCtions, for example, holding.
Qualitative results Figure 7 shows prediCtion of proposed model and the baselines, where Sep-cls
denotes separable Classifier. The proposed model CorreCtly prediCts out-of-voCabulary answers for
questions asking diverse visual ConCepts suCh as plaCes, styles, speCies of bird and holiday names.
Weakly supervised task regression Given that answer sets extraCted from Wordnet models diverse
visual reCognition tasks, matChing these answer sets to relevant questions is CritiCal for Categorization
of VQA data and model interpretation. As we learn VQA models by task regression, this matChing
Can be performed by Comparing the enCoded task speCifiCation veCtor from a question τηvqa (q) and
the task speCifiCation veCtor of an answer set τηpre (ts). For eaCh τηpre (ts), we sorted questions in a
desCending order of dot produCt similarity between τηpre (ts) and τηvqa (q). In the sorted question list,
top 2 questions whose length fits in a table is visualized in Table 1. The proposed model is trained
with Wordnet data for this experiment. Note that we perform the task regression without expliCit
supervision on matChing between a question and a task speCifiCation.
6	Conclusion
We present a transfer learning approaCh for visual question answering (VQA) with out-of-voCabulary
answers. We pretrain a task Conditional visual Classifier with off-the-shelf visual and linguistiC data
based on unsupervised task disCovery. The pretrained task Conditional visual Classifier is transferred to
VQA adaptively. The experimental results show that exploiting off-the-shelf visual and linguistiC data
boosts performanCe in the proposed setting and jointly training the task Conditional visual Classifier is
important to model interaCtion between visual features and task speCifiCations.
8
Under review as a conference paper at ICLR 2019
References
Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. C-vqa: A compositional
split of the visual question answering (vqa) v1. 0 dataset. arXiv preprint arXiv:1704.08243, 2017.
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don’t just assume; look
and answer: Overcoming priors for visual question answering. In CVPR, 2018.
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In
CVPR, 2018.
Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko,
Trevor Darrell, Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, et al. Deep
compositional captioning: Describing novel object categories without paired training data. In
CVPR, 2016.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. VQA: visual question answering. In ICCV, 2015.
Soren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.
DbPedia: A nucleus for a Web of open data. In The semantic web, pp. 722-735. Springer, 2007.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabora-
tively created graph database for structuring human knoWledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of data, pp. 1247-1250. ACM, 2008.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural netWorks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009.
Christiane Fellbaum. Wordnet: An electronic database, 1998.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise:
A deep visual-semantic embedding model. In NIPS, 2013.
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question ansWering and visual grounding. In
EMNLP, 2016.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question ansWering. In CVPR, 2017.
Tanmay Gupta, Kevin Shih, Saurabh Singh, and Derek Hoiem. Aligned image-Word representations
improve inductive transfer across vision-language tasks. In CVPR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C LaWrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In CVPR, 2017.
Jin-HWa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak
Zhang. Hadamard product for loW-rank bilinear pooling. In ICLR, 2016.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun,
and Sanja Fidler. Skip-thought vectors. In NIPS, 2015.
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova,
Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav
Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy.
Openimages: A public dataset for large-scale multi-label and multi-class image classification.
Dataset available from https://github.com/openimages, 2017.
9
Under review as a conference paper at ICLR 2019
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language
and vision using crowdsourced dense image annotations. IJCV, 123(1):32-73, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In NIPS, 2012.
Steven Loria. TextBlob: Simplified Text Processing. http://textblob.readthedocs.io/
en/dev/, 2018. [Online; accessed 3-May-2018].
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR, 2018.
Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about real-world
scenes based on uncertain input. In NIPS, 2014.
Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. Image question answering using convolu-
tional neural network with dynamic parameter prediction. In CVPR, 2016.
Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio Ferrari. We dont need no
bounding-boxes: Training object class detectors using only human verification. In CVPR, 2016.
Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio Ferrari. Extreme clicking for
efficient object annotation. In ICCV, 2017.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In EMNLP, 2014.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In NIPS, 2015.
Damien Teney and Anton van den Hengel. Zero-shot visual question answering. arXiv preprint
arXiv:1611.05546, 2016.
Damien Teney, Peter Anderson, Xiaodong He, and Anton van den Hengel. Tips and tricks for visual
question answering: Learnings from the 2017 challenge. In CVPR, 2018.
Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell,
and Kate Saenko. Captioning images with diverse objects. In CVPR, 2017.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015.
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Fvqa: Fact-based
visual question answering. TPAMI, 2017a.
Peng Wang, Qi Wu, Chunhua Shen, and Anton van den Hengel. The vqa-machine: Learning how to
use existing vision algorithms to answer new questions. In CVPR, 2017b.
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Ask me anything:
Free-form visual question answering based on knowledge from external sources. In CVPR, 2016.
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for
image question answering. In CVPR, 2016.
Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Incorporating copying mechanism in image
captioning for learning novel objects. In CVPR, 2017.
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering
in images. In CVPR, 2016.
10
Under review as a conference paper at ICLR 2019
A	Weakly Supervised Task Regression
This section describes how task regression is performed by Equation 2 in the main paper.
Let q be a question from a VQA dataset and Tq be a true task specification defined by a question.
Our intuition is that a task is defined by a mapping from visual inputs to a set of answers, which is
represented by a conditional distribution PT* (a|v). As the task is defined by a conditional distribution,
the objective of a task regression becomes to find a task specification vector τηvqa (q) that approximates
this conditional distribution using a pretrained task conditional visual classifier p& (a|v, TnVqa (q)). We
can formulate this objective as a maximum log-likelihood, which is formally written as follows.
EP(VIq)ETq (a|v) [log pθ (a|v,Tnvqa (Q))],
(4)
where p(v|q) is conditional distribution of visual features v given a question q.
As we are interested in learning ηvqa, which is a parameter of a question encoder working over all
question q, we need to optimized the objective expected over the distribution of q, which is formally
written as follows.
Ep(q)Ep(v|q)ETq*(a|v)[log pθ (a|v, Tηvqa (q))]
=Ep(q)Ep(vφvqa (I,q)∣q)Eτq(a∣Vφvqa (I,q))[log pθ (alvφvqa (L Q), Tnvqa (Q))]
=Ep(q)Ep(I∣q)Eτq (a∣I,q)[log pθ (alvφvqa (L Q),τnvqa (q))]
=EPVqa(a,I,q)[log pθ (alvφvqa (L q),Tnvqa (4))],
(5)
where, pvqa(a, L q) = τq(a∣I, q)p(I∣q)p(q) and a visual feature V is altered to Vφvqa(L q) because
the visual feature is inferred by a visual encoder from an image I. This derivation relates the task
regression to standard VQA objective in Equation 1 in the main paper.
B	Combining Knowledge Learned by VQA
While we focus on learning visual concepts from external visual data, VQA dataset is still a valuable
source of learning diverse knowledges. Especially, some answers in the VQA dataset are not visual
words and require visual reasoning. For example, yes and no are one of the most frequent answers
in the VQA dataset Antol et al. (2015) but it is not straightforward to learn these answers only with
the external visual data. Therefore, we consider combining knowledge learned from VQA and from
external visual data.
We construct a split of the VQA dataset consisting of 405,228 training, 37,031 validation, 43,171
test-validation and 172,681 test questions. The training and validation set does not contain any out-
of-vocabulary answers and test-validation and test set contains out-of-vocabulary answers. Contrary
to the split used in the main paper, this split also contains training answers in the test-validation and
test set and the training answers includes logical answers, numbers and visual words. The list of
out-of-vocabulary answers are identical to the main paper. Among 172,681 test questions, 103,013
questions could be correctly answered only with the training answers.
To combine knowledge from VQA and external visual data, we construct two task conditional visual
classifier and fine-tune one classifier to learn newly appearing answers from VQA training set and fix
one classifier to answer known visual answers including out-of-vocabulary answers. After training,
we simply combine two logits by element-wise sum and pick answer with largest score for prediction.
The result is illustrated in Figure 8. We compare the proposed model with the standard VQA model
and the answer embedding baseline. Each models are trained with 3 different random seeds and
their mean and standard deviation are plotted. Overall, the proposed model performs the best. While
the standard VQA model achieved the best performance for training answers, it cannot predict
any out-of-vocabulary answers. The answer embedding baseline achieves some generalization to
out-of-vocabulary answers, but constraints in the answer embedding degrades its performance on
training answers.
11
Under review as a conference paper at ICLR 2019
Figure 8: Combining knowledge from VQA and external visual data. Evaluation results on
a test set containing both out-of-vocabulary answers and trained answers. The proposed model
showed relatively lower performance on trained answers but significantly better performance on
out-of-vocabulary answers. In total, the proposed model showed the best performance.
Table 2: Learned mapping between a question and a task specification. We retrieved questions
for each answer sets based on the similarity score between task specification vectors. Results show
that appropriate task specifications are regressed from each questions. No explicit supervision is used
for mapping between questions and task specifications.
Answer set ts	Questions
animal.n.01 building_material.n.01 meat.n.01 liquid.n.01 plant.n.02 Plant_organ.n.01 structure.n.01 furniture.n.01 sloPe.n.01 color.n.01 consumPtion.n.01 artifact.n.01 fruit.n.01 building.n.01 hair.n.01 communication.n.02 body_of_water.n.01 tool.n.01 time_Period.n.01 aPPliance.n.02 public-transport.n.01 fabric.n.01 tree.n.01 beverage.n.01 home_appliance.n.01 Consumer_goods.n.01 cutlery.n.02 ediblefruit.n.01 shape.n.02 meal.n.01 sport.n.01	what type of animals are near the road? / what species of animals are these? what kind of material is the flooring made from? / what is the type of flooring made of? what kind of meat is next to the broccoli? / what kind of meat is next to the veggies? what kind of soda are the people drinking? / what type of soda are they drinking ? what kind of flower is in the tall vase? / what kind of plant leaves are on the plate? what type of fruit is the animal eating? / what kind of fruit is the kid eating? what type of structure are they in? / what kind of building structure is she in? what piece of furniture are the cats sitting on? / what furniture is the cat sitting on? is the man going uphill or downhill? / is the bus parked uphill or downhill? what color is the trash bag? / what color is the garbage bag? what are the cows doing? / what are the animals doing? what object is the cat laying under? / what type of structure are they in? what type of fruit is the animal eating? / what type of fruit juice is on the counter? what kind of building structure is she in? / what type of building is he standing in? what hairstyle does the surfer have ? / what type of hairstyle does this man have ? what type of language is on the buildings? / what type of language is on the signs? what type of body of water is the man on? / what type of body of water is in photo? what utensil is in the person ’s hand? / what utensil is laying next to the bread? what kind of season is it? / what type of season is it? what kind of appliance is the cat standing in? / what appliance is she standing next to? what type of transportation is passing by? / what type of vehicle is shown in the sign? what type of fabric are the bears made of? / what type of fabric is the chair made of? what kind of trees are under all that snow? / what type of trees are the tall ones? what kind of soda is on the desk? / what kind of soda is in the bottle? what kind of appliance is the cat standing in? / what appliance is she standing next to? what clothing item is this person wearing? / what clothing item is the girl wearing? what type of utensil is on the tray? / what utensil is under the fork? what type of fruit is the animal eating? / what kind of fruit is the purple fruit? what shape are most of the windows? / what shape is the tall structure to the right? what meal of the day are these designed for? / what meal are these typically eaten for? which sport are they doing? / what type of sport ball is shown?
C Additional Task Regression Results
Table 2 shows examples of task regressed questions corresponding to each answers sets, which are
represented as the synset of common hypernyms. Similarity is computed by dot product between task
specification vector of the answer set embedding τηpre(ts) and the task specification vector regressed
by a question τηvqa (q). We manually select top ranked questions that fit within the table in terms of
the number of characters.
12
Under review as a conference paper at ICLR 2019
Q: what is the man cutting the
Q: is the terrain rocky or flat? Q: what are they eating?	Q: what types of workers are in the
paper with?	Ours: flat
Ours: scissors / Sep-cls: paper* Sep-cls: snowy
Ours: grass
Sep-cls: sheep*
street?
Ours: construction / Sep-cls: people*
Q: what brand is written on	t shirt?	Q: what sport is shown	here?	Q: what sport does this athlete play	Q: is it light or dark in the
Ours: adidas	Ours: surfing	professionally ?	picture?
Sep-cls: nike*	Sep-cls: surfers	Ours: baseball / Sep-cls: sports	Ours: dark / Sep-cls: paint*
Figure 9: Out-of-vocabulary answers with diverse types of concepts. Green and red color denotes
correct and wrong answers respectively. Asterisk(*) denotes answers appearing in the training set.
Answers without asterisks are out-of-vocabulary answers. The proposed model correctly predict
out-of-vocabulary answers for diverse visual recognition tasks.
D	Additional Qualitative Results
To illustrate that the proposed model could answer diverse questions with out-of-vocabulary answers,
we present additional qualitative results. Figure 9 illustrates that the proposed model correctly predict
out-of-vocabulary answers to diverse visual recognition tasks. Figure 10 illustrates that the proposed
model performs question and image dependent answering and could predict out-of-vocabulary
answers depending on both image and question.
13
Under review as a conference paper at ICLR 2019
Q: what sport is shown here?
Ours: surfing
Sep-cls: surfers
Q: what kind of top is the boy Q: what is he wearing?
wearing?	Ours: snowsuit
Ours: tank top / Sep-cls: shirt* Sep-cls: jacket*
Q: which sport is this?
Ours: skateboarding
Sep-cls: skateboard
Q: what type of bird is shown?
Ours: eagle
Sep-cls: flying*
Q: what kind of bird is this?
Q: what is the child eating?
Q: what are they eating?
Ours: seagull
Sep-cls: multicolored*
Ours: banana
Sep-cls: ballon*
Ours: grass
Sep-cls: sheep*
(a)	Same task with different out-of-vocabulary answers. Pair of questions are asking similar visual recogni-
tion task. The proposed model correctly predict out-of-vocabulary answers depending on different images.
Q: What tvoe OfflOorinq is in the
room ?
Ours: carpet / Sep-cls: tile*
Q: What is the COffee table Inade
oG
Ours: wood / Sep-cls: metal*
Q: What color is the couch?
Ours: blue
Sep-cls: gray*
Q: What COlOr Skateboard he is
using?
Ours: green / Sep-cls: black*
Q: What color are the wheels?
Ours: blue
Sep-cls: white*
Q: VVho hold the banana?
Ours: man
Sep-cls: woman*
Q: what color is the hat on the
left?
Ours: green / Sep-cls: black*
Q: What the CIirIS Wearincl On
their necks?
Ours: necklace / Sep-cls: buttons
Q: What is StKnCiled On the WaH?
Ours: flowers
Sep-cIs: pictures*
Q: What kind σf floor is this
house?
Ours: carpet / Sep-cls: tile*
Q: What is the WbIe made of?
Q: What is the man Dosina
Q: What is On top Ofthe aiɔiɔle?
Q: What SPort is beincι iɔlaved?
Ours: wood
Sep-cls: metal*
beside?
Ours: tree / Sep-cls: elephant*
Ours: orange
Sep-cls: apples
Ours: soccer
Sep-cls: tennis*
Q: is this IiJrnitUre modern Or antiαue? Q: What cokM^is the grass?
Ours: modern	Ours: green
Sep-cls: commercial*	Sep-cls: brown*
Q: What CoIOr are these hddIks?
Ours: green
Sep-cls: red and green*
Q: What brand is Written On t shirt?
Ours: adidas
Sep-cls: nike*
(b)	Same image with diverse tasks This qualitative example visualizes diverse question answering for a single
image. The proposed model correctly predicts out-of-vocabulary answers depending on the question.
Figure 10: Image and question dependent answering. Green and red color denote correct and
wrong answers respectively. Asterisk(*) denotes answers appearing in the training set. Answers
without asterisks are out-of-vocabulary answers.
14