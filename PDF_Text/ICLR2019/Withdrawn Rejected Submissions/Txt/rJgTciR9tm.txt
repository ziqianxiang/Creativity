Under review as a conference paper at ICLR 2019
Learning Information Propagation in the Dy-
namical Systems via Information Bottleneck
Hierarchy
Anonymous authors
Paper under double-blind review
Ab stract
Extracting relevant information, causally inferring and predicting the future states
with high accuracy is a crucial task for modeling complex systems. The en-
deavor to address these tasks is made even more challenging when we have to
deal with high-dimensional heterogeneous data streams. Such data streams often
have higher-order inter-dependencies across spatial and temporal dimensions. We
propose to perform a soft-clustering of the data and learn its dynamics to pro-
duce a compact dynamical model while still ensuring the original objectives of
causal inference and accurate predictions. To efficiently and rigorously process
the dynamics of soft-clustering, we advocate for an information theory inspired
approach that incorporates stochastic calculus and seeks to determine a trade-off
between the predictive accuracy and compactness of the mathematical representa-
tion. We cast the model construction as a maximization of the compression of the
state variables such that the predictive ability and causal interdependence (relat-
edness) constraints between the original data streams and the compact model are
closely bounded. We provide theoretical guarantees concerning the convergence
of the proposed learning algorithm. To further test the proposed framework, we
consider a high-dimensional Gaussian case study and describe an iterative scheme
for updating the new model parameters. Using numerical experiments, we demon-
strate the benefits on compression and prediction accuracy for a class of dynam-
ical systems. Finally, we apply the proposed algorithm to the real-world dataset
of multimodal sentiment intensity and show improvements in prediction with re-
duced dimensions.
1	Introduction
The use of machine learning for making inference and prediction from the real-world data has shown
unprecedented growth. There exist a plethora of approaches for complex system (CS) modeling
(e.g., multi-input multi-output state space identification (Stoica & Jansson, 2000), expectation max-
imization (EM) (Martens, 2010), regularization (Chiuso, 2016), graphical models (Meinshausen &
Buhlmann, 2006), combined regularization and Bayesian learning (Fox et al., 2008; 2011; Bonet-
tini et al., 2015), kernel-based regularization (Pillonetto & Chiuso, 2015)). With the increase in the
size of data, the complexity of the accurate models also increases, making inference and predic-
tions slower. The major challenges of the upcoming era, hence, are likely to deal with the massive
and diverse data sources, and still making quick decisions. Therefore, the compact modeling of
time-varying complex systems1 is a challenging task and appealing for more investigation. The
real-world data has complex inter-dependencies across spatial and temporal dimensions. We aim
to identify such dependencies and carefully construct a compact representation of the given CS
model while still ensuring accurate predictions. We do so by performing a soft-clustering of such
inter-dependencies to preserve only the relevant information. For the CS model in the form of a
dynamical system, we additionally argue that similar to the data, the most relevant information also
gets transformed at each hop in an alternate dynamical system. From a bird’s-eye view, we track
1In this work, we interpret complex systems (networks) (Barrat et al., 2008; Gao et al., 2014) as graphs com-
prising of nodes interacting spatially and temporally, i.e. both inter and intra dependence, with node activities
available in the form of time-series.
1
Under review as a conference paper at ICLR 2019
how the most relevant information propagates across the given dynamical system. We represent
this propagation via an alternate dynamical system (compact model) and develop an unsupervised
learning technique of such process.
The most relevant work in this regard is information bottleneck (IB) principle (Tishby et al., 2000).
For fixed two random variables, it performs a soft-clustering to compress one variable while predict-
ing another, given the joint probability distribution. The IB has been successfully applied to speech
recognition (Hecht & Tishby, 2005), document classification (Slonim & Tishby, 2000), gene expres-
sion (Friedman et al., 2001) and deep learning (Tishby & Zaslavsky, 2015), etc., and it has shown
good performance. In contrast, we aim to learn a dynamics of the soft-clustering across the given
dynamical system, and propose a general optimization framework to study the trade-offs between
compactness and the resulting accuracies.
The problem statement addressed in this work is: Given a dynamical system, we aim to develop a
compact model by learning the dynamics of the soft-clustering in an unsupervised manner, or alter-
nate dynamical process, through Information Bottleneck hierarchy (IBH). The main contributions
of the present work are as follows: (i) By learning the dynamics of the soft-clustering, we propose
an alternate compact dynamical system of the given process, with emphasis on the prediction accu-
racies. (ii) We formulate a novel optimization setup, compact perception problem, and characterize
general solution to the information theoretic problem. (iii) We quantify how most relevant informa-
tion about future gets transformed at each hop in the alternatively designed dynamical system.
A brief mention of the mathematical notations is provided in the next part.
1.1	Definitions and notations
In this manuscript, we use capital letters to denote random variables (RVs), and lowercase letters
are used for the realizations. The bold letters are used for multi-variate RVs. For a RV X , with
little abuse of notation, we denote the probability mass function pX (x) as p(X), unless specified
otherwise. The expectation operator is denoted as E[.]. A Gaussian distributed multi-variate RV
is denoted as X 〜N(μχ, Σχ), where μχ and Σχ are the mean vector and covariance matrix,
respectively. Next, we present a few information theoretic definitions relevant to this work.
Definition 1. The Kullback-Leibler (KL) divergence (Cover & Thomas, 1991) between two proba-
bility massfunctionsp(∙) and q(∙) is written as
DκL(pllq)=X p(x)log 需
(1)
The KL divergence is in general not symmetric, and DKL ≥ 0 with DKL(p||q) = 0 if and only
if p = q. Using (1), the mutual information between two RVs X and Y is defined as I(X; Y ) =
DKL (p(X, Y )||p(X)p(Y )). Next, we state the problem statement addressed in this work.
2	Problem formulation
Complex systems consist of a large number of interacting dynamic components. In practical situa-
tions, we have high-dimensional time-series (node activities) having complex spatial and temporal
correlations. The challenge lies in identifying such complicated inter-dependencies and recover the
true dimensionality of the system in an unsupervised manner. We equivalently call it as identifying
the compact model while preserving the relevant information across spatio-temporal states. Under
stationary assumptions, (Tishby et al., 2000) proposed the IB to process information compactly. For
the sake of completeness, we provide a brief overview of the IB principle.
2.1	Information bottleneck approach
The IB compresses a variable X into a new stochastic variable B via soft-clustering, while maintain-
ing as much information as possible about another variable of interest Y . The variable B operates to
minimize the information compression task and to maximize the relevant information. Provided that
the relevance of one RV to another is measurable by an information-theoretic metric (i.e., mutual
information), the trade-off can be written as the following variational problem
Pmin I(X； B)- βι (B； γ),	⑵
2
Under review as a conference paper at ICLR 2019
where β controls the trade-off between the tasks mentioned above. Hence, the variable B, solving
the minimization problem (2), encodes the most informative part from the input X about output Y .
Inspired by the IB concept and its internal predictive coding, we propose a causal inference frame-
work for discrete-time stochastic dynamical systems. More precisely, we label two consecutive
dynamic states Xk and Xk+1 to be the input and the output of the bottleneck Bk, respectively.
Hence, Bk carries the most informative part (relevant information) from the past about the future.
Next, we formalize this idea to create a sequence of bottlenecks (or dynamics of soft-clustering) to
compactly and accurately represent the given dynamical system.
2.2	Learning information propagation: Setup
We consider a stochastic dynamical system, involving large number n of random processes X =
[X1 , X2 ,  Xn], interacting and evolving in time. A fundamental problem for decision making or
prediction is to learn a compressed representation of the system dynamics from high-dimensional
data (i.e., system identification). The higher-order dependencies hiding behind the high-dimensions
make this task challenging, and it needs sophisticated techniques to extract meaningful information
for an appropriate application. In this paper, we propose a framework that focuses only on the
dynamics of the relevant information, by learning an alternate representation of the given process.
Figure 1: A N -length stochastic dynamical system with corresponding IBH in parallel. We study
the shaded 3-hop process in isolation, i.e., given three consecutive states Xk-1, Xk and Xk+1 of the
input process, the stochastic variables Bk , Bk+1 represent dynamics of the alternatively designed
process to capture the relevant information.
Adopting an information theoretic representation, we aim to determine Bk and Bk+1 jointly since
they provide compressed predictive information about the system dynamics. Figure 1 summarizes
our objectives, and we study the shaded region (3 states) of the dynamical system in isolation, i.e.
without the influence of any other RVs in the complete system. An argument to generalize this study
to any N -length is provided later in Section 3.1. We determine the stochastic variable Bk that not
only compresses Xk-1 as much as possible while preserving the relevant information about Xk, but
also delivers this information to Bk+1. The variable Bk+1 quantifies the meaningful information
about the state of the system at time k + 1, building upon the compressed information received via
Bk, thus forming a dynamics of the relevant information.
By construction, the new mapping Bk is designed from Xk-1 to preserve the consistent information
about Xk, therefore, given Xk-1, Bk and Xk are independent. Similarly, the mapping Bk+1 is
independent of Xk+1 given Bk . Since, Bk+1 carries information from Bk which is compressed
representation ofXk-1, then given Bk, Xk-1 and Bk+1 are also independent. With this framework,
the following Markov chains are considered: Xk-Xk-I-Bk, Xk+ι-Bk-Bk+ι, Xk-I-Bk-Bk+ι
and Bk-Xk-1-Xk+1.
The trade-off between the compression and preservation of relevant information is defined as the
minimum achievable rate I(Xk-1; Bk) subject to constraints on the information processing. We call
it a compact perception problem which determines a trade-off between compression representation
and predictive characteristics. Formally, this can be written as the following optimization.
min	I(Xk-1; Bk)
p(Bk|Xk-1),p(Bk+1 |Bk)
subject to
I(Xk-1; Xk) - I(Bk; Xk)
I(Bk; Bk+1) - I(Xk-1; Xk)
≤ 1
≤ 2
(3)
I(Xk-1; Xk+1) - I(Xk+1; Bk+1)	≤3
where the constraints characterize the bounds on the desired prediction/compression at each step.
We wish to lower bound (to guarantee) the prediction accuracies via lower bounding I(Bk; Xk)
3
Under review as a conference paper at ICLR 2019
and I(Xk+1; Bk+1), and upper bound (to limit) the compression level across hop via I(Bk; Bk+1).
For example, 1 bounds the accuracy of the prediction of Xk by Bk . The information flow across
alternate dynamical process is controlled by 2. Lastly, 3 defines closeness of prediction of Xk+1 by
Bk+1. We show in the Section 3.1, and results in Section 4, that such trade-off can be alternatively
studied by choice of the Lagrange parameters.
3	Learning information processing for dynamical systems
This section provides the main results concerning solving the compact perception optimization prob-
lem in equation (3) under the most general case. Next, this general result is used to study a high-
dimensional continuous Gaussian distribution. Lastly, we describe an iterative method to update the
corresponding parameters.
3.1	Learning information propagation: Solution
Finding the alternate dynamical representation, or Bk and Bk+1, as stated in (3) requires to solve
a variational problem. To solve this problem, we introduce the Lagrange multipliers β , λ and γ for
the information processing constraints. Hence, we find the alternative representation (or IBH) by
minimizing the following functional written using (3):
F[p(Bk|Xk-1), p(Bk), p(Xk|Bk), p(Xk+1|Bk), p(Bk+1|Bk), p(Bk+1)] =
I(Xk-1; Bk) - βI(Bk; Xk) + λ (I(Bk; Bk+1) - γI(Xk+1; Bk+1)) .	(4)
We can argue the following regarding the optimal solution which minimizes the equation (4).
Theorem 1. The optimal solution that minimizes functional F in (4) satisfy the following self-
consistent equations:
P(BkIXk-I)	= M r7 J × exp {-βDKL (P(XkIXk-I)UP(Xk IBk ))
Z1
- λDKL (P(Bk+1IBk)IIP(Bk+1))
-λγEBk+ι∣Bk [DKL (P(Xk+JXk-I)IIP(Xk+ 1 |Bk+1))]} ,	(5)
p(Bk+ι∣Bk) = P(Bk+1) exp{-yDkl (P(Xk+1IBk)IIP(Xk+1 R+ι))},	⑹
Z2
where Z1 and Z2 are normalizing partition functions.
The functional F in (4) may not be convex in the product space of the associated probability sim-
plexes. Hence, it is difficult to obtain a global optimum, however, a stationary point (and locally
optimal solution, in most of the cases) can be obtained using the following result.
Corollary 1. The self-consistent equations in Theorem 1 can be used to write an iterative procedure
to update the associated probabilities as equations (14)-(20).
The iterative approach of Corollary 1 is detailed in the Appendix B. The idea is similar to the Blahut-
Arimoto algorithm (Arimoto, 1972; Blahut, 1972), also observed in (Tishby et al., 2000). Finally, it
remains to show the existence of a stationary point of the functional in (4). The convergence of the
iterations in Corollary 1 is established through the following result.
Lemma 1. The iterative procedure in Corollary 1 to minimize the functional F in (4) is convergent
to a stationary point.
The idea of proof (in Appendix C) is also somewhat similar to the EM McLachlan & Krishnan (1996)
but with minimization of the functional. As in standard EM, in most of the cases, the stationary
convergence point is local minimum (maximum in EM) of the functional.
Corollary 2. The IBH solution in Theorem 1 reduces to IB in (2) upon setting Xk-1 = Xk, and
β→ ∞.
Proof. With Xk-1 = Xk, the functional of IBH in equation (4) can be written as
F =	(1 - β)I(Xk-1; Bk) + λ (I(Bk; Bk+1) - γI(Xk+1; Bk+1)).
The functional F minimization in the limit of β → ∞ has one solution of P(BkIXk-1) =
1Bk=Xk-1 (such that I(Xk-1; Bk) > 0), and the problem reduces to minimize the following
F = I(Xk-I; Bk + 1) - YI(Xk+1; Bk+1),
where λ can be dropped, as λ ≥ 0.
4
Under review as a conference paper at ICLR 2019
An advantage of using the optimization framework in (3) is that it can be generalized to any length
of the given input dynamical process by properly repeating the second and third constraint. In this
work, we have studied the case of length three of the input process. However, the same principles
can be used to write the solution for any length N of the dynamical system (the complete Figure 1),
as will be presented in the future work.
3.2 Measuring information flow for linear dynamics
Many applications in machine learning involve time-series with multi-dimensional observations
where each dimension is very likely to be correlated with others, and the state of observations
evolves in time. Assuming that these observations are corrupted by Gaussian noise, then a linear
dynamical system is a promising model for analyzing the provided time-series data. Thus, the sys-
tem dynamics can be modeled similarly as the evolution of a stochastic time-invariant linear system.
Finally, if we have Gaussian distributed initial-state of the linear dynamical system with additive
Gaussian noise at each time step, then all future states are jointly Gaussian distributed. In this case,
our framework learns the IBH through Gaussian random vectors.
Recall that the states Xk of the dynamical system under study are jointly Gaussian, and without loss
of generality we assume that they are centered. As explained previously, we aim to design Bk and
Bk+1 to define an alternate representation which captures the dynamics of the relevant information.
It is shown in the prior works of (Globerson & Tishby, 2004; Chechik et al., 2005) that for the
problem setup of IB in which the input and output variables are jointly Gaussian, the optimum
solution of the IB Lagrangian obtained by a stochastic transformation is also jointly Gaussian with
the bottleneck’s input. Consequently, Bk is jointly Gaussian with Xk-1 and Bk+1. Since RVs
in consideration are mean centered and Xk-1 , Bk, Bk+1 are jointly Gaussian; the IBH variables
can be very well represented as linear transformations of each other. Additionally, using the MC
conditions from Section 2.2 we write the following linear relations.
Bk = ΦXk-1 + ξk,
Bk+1 = ∆Bk + ξk+1,
(7)
where ξk and ξk+1 are centered Gaussian random vectors independent of Xk-1 and Bk , respec-
tively. Given the aforementioned settings, the solution of the minimization problem in equation (3)
is determined by finding the matrices Φ and ∆, and the covariance matrices Σξk and Σξk+1 . An
iterative procedure using Corollary 1 to update the concerned parameters in (7) is presented as the
following result.
Theorem 2. Given the parameters β, λ and γ, the Gaussian bottlenecks Bk = ΦXk-1 + ξk and
Bk+1 = ∆Bk + ξk+1 are obtained by performing the following iterations over the parameters
Σ(t+1) = β Σ-1 (t) - (β-1)Σ-1(t) +λ∆T(t)Σ-1(t)∆(t)-1,
ξk	Bk |Xk	Bk	Bk+1	,
Φ(t+1) = Σ(t+1) β Σ-1 (t) Φ(t)(I - ΣX |X Σ-1 )
ξk	Bk |Xk	Xk-1 |Xk Xk-1
+λγ ∆T (t)Σ-1(t)	∆(t)Φ(t)Σ-X1 ΣX	X	Σ-X1
Bk+1 |Xk+1	Xk+1 Xk+1Xk-1 Xk-1
(8)
∆(t+1) = I -
1k+(t1)|Xk+1 - (γ - 1)Σ-B1k+(t1)-1,
Y - 1 y'(t)	?一1⑶、	ʌ (t) (τ y'(t+1) 尸-1 (t+I)
^Bk+ι∣Xk+ι ^Bk+ι J	δ I1 - ^Bk∣Xk+ι ^Bk
where t is the iteration index.
The detailed proof of the Theorem 2 is provided in the Appendix. In the next section, we show some
numerical results generated using synthetic data.
4 Simulation experiments
We numerically evaluate the results of Theorem 1 and Theorem 2 using synthetically generated
Gaussian distribution. Specifically, the covariance matrices for the input dynamical process
5
Under review as a conference paper at ICLR 2019
Figure 2: A comparison among single IB (designed locally) and IBH for various size tuples of
(Xk-1, Xk, Xk+1), respectively. The horizontal axis denotes required level of information transfer,
while vertical is inversely proportional to compression levels.
(a)
Figure 3: Variations of rank(∆) with Lagrange parameters β, λ and γ, when dimension of input
dynamics (Xk-1, Xk, Xk+1) is (40, 30, 20). The rank variation is presented by fixing one parameter
in each scenario: (β = 100, λ, γ) in (3a), (β, γ = 10, λ) in (3b), and (β, γ, λ = 0.01) in (3c).
(b)
(c)
Xk-I-Xk-Xk+ι are generated numerically for a given size tuple, and we compute the parame-
ters of (7) using (8). The quantities of interest are I (Bk+1; Xk+1) and I(Bk+1; Xk-1) which are
indicators of prediction information and inverse of compression, respectively. We compare the pre-
diction/compression behavior of the proposed approach of the alternate design of the dynamical
system vs. designing local IB’s between each hop. The local IB’s are designed between two consec-
utive RVs in the dynamical system independently while the IBH is designed jointly. We show the
distinction upon varying the dimensions of the input process (Xk-1, Xk, Xk+1) in Figure 2. It is
observed that the gap between prediction (for a fixed level of compression) grows with an increase in
the input dimensions. The IBH by design takes into account the entire input dynamical system, and
construct an alternate representation which provides better prediction at each step, by appropriate
choice of the Lagrange parameters β, λ, γ.
The Lagrange parameters (β, λ, γ) control the trade-off between compression and prediction at each
step of the alternatively designed dynamical process. In the optimization problem (3), β corresponds
to the first constraint, and hence plays a deciding role in prediction accuracy of Bk. The λ corre-
sponds to the second constraint, and will control the flow of relevant information across Bk. Finally,
λ and γ together tune the accuracy of prediction using Bk+1. For example, in (7), the informa-
tion tapping behavior can be visualized by inspecting the ranks of Φ and ∆ matrices upon varying
(β, λ, γ). In Figure 3a, the rank(∆) increases upon increasing γ for each choice of λ. Since λ ap-
pears in front of both prediction and compression expression in (4), it has little effect on the rank(∆)
for a fixed γ and β. Next, in Figure 3b, we observe dynamical effects of the information flow. By
fixing λ, we limit the information acceptance of Bk+1, hence the parameter β can only increase the
rank(∆) up to a certain limit by allowing maximum information through Bk. We witness in Fig-
ure 3b, that with higher λ, the parameter β quickly increase the rank(∆). Now, for a fixed λ, both
6
Under review as a conference paper at ICLR 2019
Figure 4: The alignment of three modalities (text, visual and audio) with averaging of visual and
audio features corresponding to the time boundaries obtained from text.
β and γ intertwine with each other to decide rank(∆). By fixing β, we limit the input information
through Bk to Bk+1, or availability, and hence γ can only increase the rank(∆) up to certain extent.
Similarly, fixing γ limits the maximum information that Bk+1 can process, and therefore β can do
best narrowly up to some extent. We witness this hyperbolic behavior in Figure 3c.
5	Real-world data: Multimodal sentiment intensity
In this section, we apply the ideas of IBH to extract the features from the challenging multimodal
datasets available in the form of time-series. Particularly, we have used the CMU Multimodal Sen-
timent Analysis (CMU-MOSI) dataset (Zadeh et al., 2016) which consists of a total of 2198 videos.
Each video comprises one speaker expressing their opinion in front of the camera. The available
modalities from the dataset are text, visual and audio. The goal of the dataset is to perform a dis-
criminative task of predicting the speaker sentiment using the available modalities. From the three
modalities of text, visual and audio, the corresponding features are extracted using GloVe word em-
beddings (Pennington et al., 2014), Facet (iMotions, 2017) and COVAREP (Degottex et al., 2014),
respectively. The extracted feature size are 300 for text, 74 for audio, and 46 for visual component.
The extracted features in the form of time-series are aligned across modalities as shown in Figure 4.
Specifically, multiple features of visual and audio modality (due to high frame/sampling rate) are
time averaged with boundaries corresponding to the text component. For each speaker, we have a
maximum of 20 words with three modalities and the corresponding sentiment intensity being a real
number ranging from -3 to 3, with negative values representing negative sentiments and vice-versa.
Some of the prior work in the multimodal representation learning include Discriminative representa-
tion learning (Zadeh et al., 2018a; 2017; 2018b; Chaplot et al., 2017) and Generative representation
learning (Sohn et al., 2014; Srivastava & Salakhutdinov, 2014; Suzuki et al., 2016). Interestingly, re-
cent work Tsai et al. (2018) proposed Multimodal Factorized Model (MFM) that exploits the fusion
of both these techniques. The work factorizes the data into discriminative factors and modality-
specific generative factors. We note that, at the very core, the challenge in learning patterns from
multiple modalities is to address the complex inter and well as intra dependencies across them. Since
IBH is capable of compressing the given dynamical model into an alternate version stochastically,
we propose to, (i) first map the multiple modalities into a time-varying linear dynamical system;
and then (ii) use IBH to identify the complex inter and well as intra dependencies across them in a
reduced dimensional model for better discrimination using simple machine learning classifiers.
The IBH is applied to various modalities to capture the information flow patterns across them and
in the compressed fashion. We have taken three modalities to be in the following Markov Chain,
text-audio-visual. The intuitive reason behind this assumption is that text is the most informative
modality for sentiment and hence the first state, while audio and visual follows text in the Markov
chain assuming the speaker is being honest in speaking and making visual expressions for a par-
ticular sentiment. The data is mean centered and the covariances of three modalities are estimated
for each speaker in the training as well as the testing dataset. However, we have only 20 words and
hence 20 samples to estimate the covariance matrix of much larger dimensions. To remedy this fewer
samples problem, we have resorted to something called pooling of the covariance matrix as follows.
The covariance matrix of any modality for the ith speaker is written as Σi = αςi + (I - aApool,
where Σi is the estimated covariance from 20 samples, and Σpool is called pooled covariance matrix
7
Under review as a conference paper at ICLR 2019
Feature Selection	# features	binary	7-class	MAE
Early Fusion	8400	51.9% =	19.1% =	1.382 =
MFM (Tsai etal., 2018)	8400	773%	354%	0.961
IBH	1150	77.4% 一	39.4%	0.922 —
Table 1: Comparison of sentiment prediction with three different methods of evaluations for early
fusion of features vs features extraction by IBH for CMU-MOSI.
which is estimated by taking all of the training data. The parameter α is used to make a trade-off
between these two matrices and is usually chosen close to 1. The covariance matrices are fed to
the algorithm in Theorem 2 to estimate Φ and ∆ matrices. The Φ matrix has components for in-
teractions across text and audio, while ∆ has entries representing interactions with the compressed
version of text-audio inter-dependencies as well as with the visual modality. Therefore, the entries
of ∆ matrix are good candidates for representing inter as well as intra dependencies across all three
modalities. Hence, we use ∆ matrix as a feature to predict the sentiment intensity, as this matrix is
central to information propagation from the text, through audio, to visual component. The low rank
of ∆ will be key in reducing the number of features. The values of parameters (β, λ, γ) are chosen
such that maximum information flow to first bottleneck, by setting high value for β, and λ close to
1, and low values for γ for better compression.
The results are reported for various evaluation methods, namely binary (with the positive and neg-
ative sentiment) and 7- class classification, and Mean Average Error (MAE) for regression. We
compare our performance with naive early fusion of features in raw format, most recent results in
the multimodal neural networks (Tsai et al., 2018) vs. features processed by IBH, and the numerical
results are presented in Table 1. Support Vector Machines (SVM) is used in the cases of early fusion
and IBH. The processing of features by IBH has a two-fold advantage: First, due to compression,
the dimensionality of the input can be reduced from 8400 to 1150. Second, the performance with
respect to various metrics is better.
6	Conclusion
In this paper, we have introduced a novel information-theoretic inspired approach to learn the com-
pact dynamics of a time-varying complex system. The trade-off between the predictive accuracy and
the compactness of the mathematical representation is formulated as a multi-hop compact perception
optimization problem. Akey ingredient to solve the aforementioned problem is to exploit variational
calculus in order to derive the general solution expressions. Additionally, we have investigated the
guaranteed convergence of the proposed iterative algorithm. Moreover, considering a specific class
of distributions (Gaussian), we have provided closed-form expressions for the model parameters’
update in our algorithm. Interestingly, the proposed compact perception shows improvements in
prediction with reduced dimension on challenging real-world problems.
The quantification of information flow across a dynamical system can have an enormous impact on
understanding and improving the current state-of-the-art in neural networks as realized in (Tishby
& Zaslavsky, 2015). Moreover, modeling with dynamical systems is a standard approach, and by
using the proposed framework, we can make a better compact representation of the system. The
driving force of a dynamical system can enforce different behaviors of information flow, as real-
ized in defining dynamical entropy by (Sinai, 1959). Therefore, measuring the information flow
can help in estimating/differentiating the actual driving component behind the observed activities.
Such concepts are useful in predicting brain imagined tasks from observed electroencephalogram
activities.
8
Under review as a conference paper at ICLR 2019
References
S. Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels.
IEEE Transactions on Information Theory,18(1):14-20, January 1972.
Alain Barrat, Marc Barthlemy, and Alessandro Vespignani. Dynamical Processes on Complex Net-
works. Cambridge University Press, 2008.
R.	Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on
Information Theory, 18(4):460-473, July 1972.
S.	Bonettini, A. Chiuso, and M. Prato. A scaled gradient projection method for bayesian learning in
dynamical systems. SIAM J. Sci. Comput., 37(3):A1297A1318, 2015.
Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Ra-
jagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language
grounding, 2017.
Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for gaussian
variables. Journal of machine learning research, 6(Jan):165-188, 2005.
A. Chiuso. Regularization and bayesian learning in dynamical systems: Past, present and future.
Annual Reviews in Control - in press, 2016.
T.	M. Cover and J. A. Thomas. Elements of Information Theory. New York: Wiley, 1991.
G.	Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer. COVAREP; a collaborative voice
analysis repository for speech technologies. In 2014 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 960-964, May 2014.
Gal Elidan and Nir Friedman. The information bottleneck EM algorithm. CoRR, arxiv:1212.2460,
2012.
E. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. Bayesian nonparametric inference of switch-
ing dynamic linear models. Trans. Sig. Proc., 59(4):1569-1585, April 2011.
Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky. Nonparametric bayesian
learning of switching linear dynamical systems. In Proceedings of the 21st International Confer-
ence on Neural Information Processing Systems, NIPS’08, pp. 457-464, 2008.
Nir Friedman, Ori Mosenzon, Noam Slonim, and Naftali Tishby. Multivariate information bottle-
neck. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp.
152-161. Morgan Kaufmann Publishers Inc., 2001.
Jianxi Gao, Yang-Yu Liu, Raissa M. D'Souza, and Albert-Laszlo Barabasi. Target control of Com-
plex networks. In Nature communications, 2014.
Amir Globerson and Naftali Tishby. On the optimality of the gaussian information bottleneck curve.
Technical report, 2004.
Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins University
Press, Baltimore, MD, USA, 1996.
Ron M Hecht and Naftali Tishby. Extraction of relevant speech features using the information
bottleneck method. In Ninth European Conference on Speech Communication and Technology,
2005.
iMotions. Facial expression analysis, 2017.
James Martens. Learning the linear dynamical system with asos. In Proceedings of the 27th Inter-
national Conference on International Conference on Machine Learning, ICML’10, pp. 743-750,
2010.
Geoffrey McLachlan and Thriyambakam Krishnan. The EM Algorithm and Extensions. John Wiley
& Sons, New York, 1996.
9
Under review as a conference paper at ICLR 2019
N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the lasso.
TheAnnals ofStatistics, 34(3):1436-1462, 2006.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In In EMNLP, 2014.
Gianluigi Pillonetto and Alessandro Chiuso. Tuning complexity in regularized kernel-based regres-
sion and linear system identification. Automatica, 58(C):106-117, August 2015.
Ya.G. Sinai. On the notion of entropy of a dynamical system. Doklady of Russian Academy of
Sciences, 124:768-771, 1959.
Noam Slonim and Naftali Tishby. Document clustering using word clusters via the information
bottleneck method. In Proceedings of the 23rd annual international ACM SIGIR conference on
Research and development in information retrieval, pp. 208-215. ACM, 2000.
Kihyuk Sohn, Wenling Shang, and Honglak Lee. Improved multimodal deep learning with variation
of information. In Advances in Neural Information Processing Systems 27, pp. 2141-2149. 2014.
Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines.
J. Mach. Learn. Res., 15(1):2949-2980, January 2014.
P. Stoica and M. Jansson. Mimo system identification: state-space and subspace approximations
versus transfer function and instrumental variables. IEEE Transactions on Signal Processing, 48
(11):3087-3099, Nov 2000.
Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint multimodal learning with deep
generative models. CoRR, abs/1611.01891, 2016.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Learning factorized multimodal representations, 2018. arXiv:1806.06176.
A. Zadeh, R. Zellers, E. Pincus, and L. P. Morency. Multimodal sentiment intensity analysis in
videos: Facial gestures and verbal messages. IEEE Intelligent Systems, 31(6):82-88, Nov 2016.
Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor
fusion network for multimodal sentiment analysis, 2017. arXiv:1707.07250.
Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. Memory fusion network for multi-view sequential learning, 2018a. arXiv:1802.00927.
Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe
Morency. Multi-attention recurrent network for human communication comprehension, 2018b.
arXiv:1802.00923.
10
Under review as a conference paper at ICLR 2019
The appendix is arranged as follows: In the Section A, we provide the proof of Theorem 1. In the
Section B, we provide the iterative procedure (mentioned as Corollary 1) to minimize the functional
in (4). Next, in Section C, we present the detailed proof of the Lemma 1, and finally, in Section D, a
detailed proof of Theorem 2 is presented.
A Proof of Theorem 1
Proof. For the sake of simplicity, a sketch of the proof is given for discrete variables. The La-
grangian associated with the minimization problem is the following
L = F- ɪ2	α1(XkT)P(BkIXkT)-	α2(Bk)P(Bk+ 1|Bk),	⑼
Bk,Xk-1	Bk,Bk+1
where α1(Xk-1) and α2(Bk) are Lagrange multipliers for the normalization of the distributions
P(Bk|Xk-1) and P(Bk+1|Bk), respectively. Taking the derivative of each term of the Lagrangian
L with respect to P(Bk|Xk-1), we have
δI (Xk-1； Bk)
δp(Bk∣Xk-i)
δI(Xk； Bk)
δp(Bk ∣Xk-ι)
δI (Bk ； Bk+1)
δp(Bk ∣Xk-ι)
δI(Xk+i； Bk+ι)
δp(Bk |Xk-i)
P(Xk-1) log
P(Bk |Xk-i)
P(Bk)
P(Xk-1)	P(Xk|Xk-1) log
Xk
P(Xk |Bk)
P(Xk)
P(Xk-1)DKL (P(Bk+1 |Bk)||P(Bk+1)),
-P(Xk-1)	P(Bk+1 |Bk)DKL (P(Xk+1 |Xk-1)||P(Xk+1 |Bk+1))
Bk+1
+ P(Xk-1)DKL(P(Xk+1|Xk-1)||P(Xk+1)).	(10)
Setting the derivative of the Lagrangian equal to zero and arranging the terms we obtain the self
consistent equation (5). Note that all the constant terms in the derivative independent of Bk will be
captured by the Lagrange multiplier α1(Xk-1). The derivative of the Lagrangian L with respect to
P(Bk+1|Bk) involves only the two last terms from the functional F and the term that ensures the
normalization condition. Then, we have
δI(Bk； Bk+1)
δP(Bk+ι∣Bk)
δI(Xk+i； Bk+i)
δP(Bk+ι∣Bk)
P(Bk) log
P(Bk+ι∣Bk)
P(Bk+ι)
P(Bk)	P(Xk+i |Bk) log
Xk+1
P(Xk+ι∣Bk+ι)
P(Xk+1)
(11)
(12)
Thus, the variational condition is written as follows
P(Bk)(Iog ^^ + γDKL (P(XkHBk 川P(Xk+11Bk+I))-α2(Bk))=0，(13)
where a2(Bk) is the summation of the Lagrange multiplier a2(Bk) and the terms independent of
Bk+1, and hence the equation (6) follows.
B Iterative solution in Corollary 1
The self-consistent equations derived in Theorem 1 to minimize the functional F in (4) can be used
to write the following set of iterative equations.
P(t+1)(Bk∣Xk-i)= …P(t)(Bk)--------- × exp n-βDκL (P(Xk∣Xk-i)∣*(Xk∣Bk))
Z(t+1)(β,λ,γ, Xk-i)	I	'	j
- λDKL (P(t)(Bk+1|Bk)||P(t)(Bk+1)
-λγEBk+1∣Bk [Dkl (P(Xk+1∣Xk-1 )∣*(Xk+1∣Bk+1))i} ,	(14)
11
Under review as a conference paper at ICLR 2019
p(t+1)(Bk) =	p(t+1)(Bk|Xk-1) p(Xk-1),	(15)
Xk-1
p(t+1)(Xk|Bk) = X p(Xk|Xk-1) p(t+1)(Xk-1|Bk),	(16)
Xk-1
p(t+1)(Xk+1|Bk) = Xp(Xk+1|Xk)p(t+1)(Xk|Bk),	(17)
P(M(Bk+ι∣Bk )= p(t)(Bk+1) exp {-γDκL (P(M(Xk+ι∣Bk )||p⑴(Xk+"Bk+ι))},
Z2(t+1)(γ,Bk)
(18)
P(t+1)(Bk+1) = X P(t+1)(Bk+1|Bk) P(t+1)(Bk),	(19)
P(t+1)(Bk+1|Xk+1) =	X P(t+1)(Bk+1|Bk) P(t+1)(Bk|Xk-1) P(Xk-1|Xk+1),	(20)
Bk,Xk-1
where in equations (15)-(17), (19) and (20) we have used the Markov assumption as stated in Sec-
tion 2.2.
C Proof of Lemma 1
Proof. The proof of the Lemma 1 can be divided into two parts. First, we show that the F in (4)
is lower-bounded. Next, we show that each iteration using the equations (14)-(20) monotonically
decrease the functional.
Lower bound: Let us consider the following alternate functional.
F = I (Xk-I; Bk) + βEXk-ι ,Bk DKL(P(Xk IXk-I) HP(Xk IBk )) + λI (Bk; Bk+1)
λγ EXk-1,Bk EBk+1 |Bk DKL(P(Xk+1|Xk-1)||P(Xk+1|Bk+1)).	(21)
It can be readily verified that the functional F ≥ 0 for given non-negative constants β, λ and γ .
Also, the F in (21) can be expanded as I *
F
I(Xk-1; Bk) + βEXk-ι,Βk XP(XkIXk-1)l0gpPXXXf
Xk
+ λ I(Bk; Bk+1) + λγ EXk-1,Bk
Σ
Bk+1,Xk+1
P(Xk+1 IXk-1)log
P(Xk+1∣Xk-l)
P(Xk+1∣Bk+1)
I(Xk-1; Bk) + β(I(Xk-1; Xk) - I(Xk; Bk)) + λ I(Bk; Bk+1)
+ λγ(I(Xk-1; Xk+1) - I(Xk+1; Bk+1)).	(22)
Since the functional F in the equation (22) differs from the functional F in (4) only in constants,
therefore, F is lower bounded as well.
Monotonicity: For proving monotonic decrement of the functional F, we will use the formulation
similar to (Arimoto, 1972). First, let us consider the following observation made in (Tishby et al.,
2000).
For a given joint distribution P(X, Y ), we can write the following.
P(Y) =argminEXDKL(P(YIX)IIφ(Y)),	(23)
φ(Y)
where the minimization is performed over the probability simplex of φ(Y ) such that the joint distri-
bution is P(X, Y ). The functional F in (22) can be written in its most general form as
F(P1,P2, φ1, φ2)
EXk-ι,Bk ∣Xk-ι-Pι
P1
l g PwI
+ βEXk-ι,Bk∣Xk-ι 〜pi DKL(P(Xk IXk-I)IM)
12
Under review as a conference paper at ICLR 2019
+ λ EXk-ι,Bk∣Xk-ι〜piEBk+ι
p2
IBk~p2log P(Bk)
+ λγ EXk-i,Bk ∣Xk-ι-Pι EBk+ι∣Bk 〜P2 DKL(P(Xk+1 |Xk-1) ”02),
(24)
where Pi = P(Bk∣Xk-1),P2 = p(Bfc+ι∣Bfc),φι = φι(XkR) and φ2 = φ2(Xk+1 ∣Bfc+ι),
therefore, F reduces to the form in (22) upon setting φι = P(Xk|Bk) and Φ2 = p(Xk+ι|Bk+i).
With the objective of minimizing the functional F, We can write its value at iteration t as F(t)=
F(PIt) ,P2t, Φιt), φ2t)). The iterations to minimize F will involve the successive choice of tuple
(P1,P2, φ1, φ2). At iteration t, let us assume that we have chosen P(1t) and P(2t), and we define the
following
G(t,t)	= min Flpt,Pt,Φι,Φ2
φ1 ,φ2
=F(PRP2t),φRφ2t))∙	(25)
Using equation (23), it easily follows that φ(1t) = P(t)(Xk |Bk) and φ(2t) = P(t)(Xk+1 |Bk+1), and
hence G(t, t) = FF(t). Now, for fixedp；",。，，φ2t), it can be easily realized that the F is convex
in P1 , therefore, minimizing F with respect to (w.r.t.) P1 will involve writing Lagrangian, and then
differentiation, and setting to zero. This step is similar to the Theorem 1, and we can write that
Pit+1) = argmin-F(pι,p2t), φit), φ2t)),	(26)
p1
where the resulting solution is (5), and hence P(1t+1) will have the expression as in (14). Similarly, for
fixed pit+1), φit), φ2t), the F is convex in p2, and the same steps follow to obtain p2t+1) as written
in (18). It should be noted that the choice to perform minimization w.r.t. P1 before P2 is arbitrary,
and the reverse can also be performed. This will change the order of iteration index in equations
(14)-(20) accordingly. Using equations (25) and (26), we can conclude that
F(t+1) = G(t + ι,t + 1)≤ G(t + ι,t) ≤ G(t,t) = F(t),
and therefore, iterating equations (14)-(20) written using the self-consistent equations of Theorem 1
minimizes F(t) monotonically. Since F and F differs only in constant, it reduces F as well mono-
tonically from above.
D Proof of Theorem 2
Proof For a multivariate random variable X, X ∈ Rn with Gaussian distribution, i.e. X 〜
N(μ, Σ), the entropy can be written as
H (X) = 2 logdet(Σ) + c,	(27)
where c is constant for a given dimension n. Using equation (27), the KL-divergence between
two Gaussian distributed random variables, X1 〜N(μ1, ∑1) and X2 〜N(μ2, ∑2) of the same
dimensions, is written as
DKL(P(X1)∣∣p(X2)) = 1 logdet(∣4 + 1 "(∑-1∑1) + 1(μ2 — μ1)τ ∑-13 — μ1).	(28)
2	det(Σ1 )	2	2
We have assumed that the given data is centered, hence all considered random variables will have
zero mean, i.e., for characterizing each random variable, we only need the corresponding covariance
matrix. Let us revisit the linear transformation model for the IBH.
Bk = ΦXk-1 + ξk,
Bk+1	= ∆Bk + ξk+1.
Now, to completely specifying the model, we have to determine the constant matrices Φ, ∆ and
the covariances ofξk, ξk+1. Since entropy is well defined for Gaussian distribution, as in (27), due
to their nice tail distribution, we can use Theorem 1 and equation (28) to write the self-consistent
13
Under review as a conference paper at ICLR 2019
transition probabilities. First, expressions of the necessary KL-divergence terms are expanded as
follows.
DKL(P(Xk|Xk-i)||p(XkIBk)) = c +1(EXkIXl- EXk∣Bfc)t∑x1ib
k	k Ik
(E Xk |Xk-1 - E XkIBk ),	(29)
DKL(P(Bk+ι∣Bk )||p(Bk+i)) = C +∣(E Bk+i|Bk )t ∑-k+1 E Bk+i|Bk,	(30)
DKL(P(Xk+1]Xk-I)IIp(Xk+1]Bk+1)) = C + 3(E Xk + 1|Xk-1 - E Xk+1|Bk+I)T ∑X1	∣b
2	k十11 k十1
(E Xk+1|Xk-1 - E Xk+1|Bk+1),	(31)
where C are different constants at each step and,
E Xk |Bk	二	二	XXk Bk	∑Bk Bk = Uk Bk,
xXk | Bk	二	二	XXk -	XXkBk XBk XBkXk
	二	XXk -	XXhXh ∙,ΦτX-1 ΦXχ, 1X,, -ʌkʃɪ-k-1	Bk	-ʌk-1 -ʌk ,
E Bk+1 |Bk	二 ABk,	
xBk + 11 Bk 二	- X£k + 1 ,	
E Xk+1|Bk+1	二 ∑Xk+1Xk-1 Φτ Aτ ∑Bk+1 Bk+1 = Θk+1Bk+1,	
XXk+11 Bk+1	-	二 xXk+1	—XXjlXh 1小, AT XBI	AΦXχ,. ιXi,∣-∣ -zvk + 1 -ʌk-1	Bk + 1	-ʌk-1 -zvk+1
Also, using (31), we can write that
EBk十ι∣Bk DKL (P(Xk+1]Xk-I)IIP(Xk+1]Bk+1)) = C
-EBk十1 |Bk BT+1 θτ+1∑χk+1∣Bk+1 E Xk+1 IXk-1
+ 1 EBk十ι∣Bk BT+1θT+1∑χk十IB”Θk+1Bk+1
2	k十1 1 k十1
=C0 - BT∆τθT+1∑χk+dBk+1 E Xk+1IXk-1.	(32)
Upon substituting the equations (29)-(31) and (32) in Theorem 1, we obtain the following self-
consistent equations.
_	.	1_T_ ■,_	B_- 1____________
logP(Bk IXk-I)= C - $Bk ∑Bk Bk- £Bk Uk Eχk∣Bk UkBk
-2BT∆τ∑b∖1 ∆Bk + βBTΞT∑χk∣BkEXkIXk-1
+ λγBT ∆τ θT+1∑χk+1∣Bk+1 E Xk+1IXk-1.	(33)
Now, Bk|Xk-1 〜 N(ΦXk-1, Σξk), therefore upon comparing terms, we obtain the following
self-consistent equations.
ς-1 = ςb[+βUT ∑χk | Bk ≡k + X AT ∑Bk+1 △，
φ = ∑ξk (β UT∑χk | Bk ςXkχk-1 ∑χk-1
+χγ AT Θτ+1∑χk+1∣ Bk+1 ∑χk+1χk-1 ∑χk-1).	(34)
Setting up the iterations, similar to Blahut-Arimoto equations, we obtain the following iterative
procedure with t as iteration index.
∑(t+1) —	(Σ-1 (t) I NUT (t)∑-1(t) U(t)λ AT (t)∑-1 (t)ʌ(t)ʌ 1
2ξk	=	(*Bk	+ β Uk	XXk | Bk Uk + A ʌ *Bk+1 ʌ )
=(β Σ-1悔-(β - 1)∑b1⑴ + λ AT ⑴ ∑b1⑴△㈤,T,
B χk Bk Bk+1
φ(t+1) = ∑(t+1)(β UT ⑴∑-1(2 ∑X%X% ι∑χ1
ξk	L k Xk | Bk Xk Xk-I Xk-1
14
Under review as a conference paper at ICLR 2019
I λT (t) QT (t) V-1 (t) V	v-1	、
+λγ △ θk+1 >Xk+ι ∣Bk+ι >Xk+ιXk-ι ,Xk-ι)
= 理+%β ςb[Q φ(W-ςXjXk ςxL)
+λγ∆τ(t)∑-1(')χ	△⑴Φ⑴Σχ1 Σχfc+1χfc ]Σχ1 ),	(35)
Bk+1∣Xk+1	Xk+1 χk+ιχk-ι Xk-J'
where in (a) and (b) we have used the expansion of Ξk, Θfc+ι, matrix inversion lemma (Golub &
Van Loan, 1996), and the identity Σχ∣γΣχγ∑-1 = ∑χ1Σχγ∑-χ, where X, Y and Z are Nor-
mal distributed random variables. We can also write the transition probability of the bottleneck vari-
ables, Bfc+ι ∣Bfc using Theorem 1 and the following useful expansion of the required KL-divergence
term.
DKL(P(Xk+ι∣Bk)∣∣p(Xk+1∣Bk+1))	= C + 1(EXfc+ι∣Bfc - EXfc+ι∣Bfc+ι)τ∑xL∣b…
2	k+11 k+ι
(EXk+ι∣Bk - EXk+1∣Bk+1),	(36)
where,
E Xk+ι∣Bk = ∑Xk+1Xk-1 Φt ∑Bk Bk = Yk Bk,
Eχk+1 ∣Bk = Eχk+1 - ∑Xk+1Xk-1 φTς-1 φςXk-IXk+1 ∙
After substituting equation (36) in (6) of Theorem 1, we obtain
logp(Bk+ι∣Bk) = C - 1 BT+ι∑Bk+1 Bk+1 - YBτ+1Θτ+1∑χk 1∣ Bk+1 Θk+1Bk+1
+	k+1 k+1
+ YBT+1θT+KXk+1∣Bk+Jk Bk.	(37)
Again, since Bk+1 ∣Bk 〜 N(∆Bk, Σξk+1), after comparing the terms on both sides of the equation
(37), we obtain
ς—+1	= ∑Bk+1 + γθτ+ι∑χk+1∣ Bk+1 θk+1,
A = Kξk+1 θτ+Aχk+1∣Bk+Jk.	(38)
After setting up the iterations like we did for equation (34), and using the (18)-(20) we first write
that
Σ黑 = △㈤∑(t+1)∆⑴ + ∑(t),
Bk+1	Bk	' ξk+1 ,
Σ 黑	IX	=	△㈤ CM?	△㈤	+ Σf),
Bk+11 Xk+1	Bk ∣ Xk+1	ξk+1)
∑(I)	X	=	∆(t)∑(t+1)	.	(39)
Bk+1Xk+1	BkXk+1
Using (38) and (39), we obtain the following.
∑(t+1)	=	(Y∑-1")X	-(Y - 1)∑-1 ㈤)T,
ξk + 1	∖/ Bk + 1 ∣ Xk+1	'/ B Bk+1 J ,
ʌ (t+1)	—	〜yι(t+1) v—1 (t)	v(t)	v-1 v	而 T (t)下一1(t+1)
— Y ξk+1	Bk+11 Xk+1 Bk+1χk+1 χk+1 Xk+1Xk-1	Bk
=) (I Y - 1 ∑(t)	∑-1(t)∖ ʌ(t) (I _ ∑(t+1) ∑-1(t+1)∖ (40)
=I1	Y~cBk+11 Xk+1 cBk + 1 J ʌ I1 - CBk ∣ Xk+1 CBk	) , (40)
where (a) is written using the previously used identity Σχ ∣ γΣχγ ∑-1 = ∑X1∑xy ∑- ；x, and
matrix inversion lemma is used at each step.	■
15