Under review as a conference paper at ICLR 2019
Isonetry: Geometry of critical initializations
AND TRAINING
Anonymous authors
Paper under double-blind review
Ab stract
Recent work on critical initializations of deep neural networks has shown that
by constraining the spectrum of input-output Jacobians allows for fast training of
very deep networks without skip connections. The current understanding of this
class of initializations is limited with respect to classical notions from optimiza-
tion. In particular, the connections between Jacobian eigenvalues and curvature
of the parameter space are unknown. Similarly, there is no firm understanding
of the effects of maintaining orthogonality during training. With this work we
complement the existing understanding of critical initializations and show that the
curvature is proportional to the maximum singular value of the Jacobian. Further-
more we show that optimization under orthogonality constraints ameliorates the
dependence on choice of initial parameters, but is not strictly necessary.
1	Introduction
Deep neural networks have been proven extremely powerful, achieving empirical success in a vast
array of problems, ranging from image recognition (He et al., 2015), amortized probabilistic in-
ference (Ritchie et al., 2016) to inferring the dynamics of neural data (Pandarinath et al., 2018).
The practical hindrance in their application often stems from the difficulty in training them, both
due to the excessive computational cost of running many epochs of gradient descent but also due
to the inherent instability in gradient computation. It is therefore of great practical and theoretical
interest to devise effective gradient optimization techniques for deep neural networks. A new and
promising approach exploits mean field assumptions and random matrix theory to devise initializa-
tion strategies that ensure the boundedness of the backpropagated gradients (Schoenholz et al., 2016;
Pennington et al., 2017). In particular, Pennington et al. shows that for orthogonal networks with
appropriately chosen parameters, the hidden layer input-output Jacobian matrix is nearly isometric,
approximately preserving the `2 norm of the gradients. Despite achieving an impressive increase
in training speed, the exact mechanism by which this initialization confers its advantage is not well
understood. In particular, it invites questions from an optimization perspective on how the bound-
edness of the Jacobian matrix relates to notions such as gradient smoothness, or (negative) strong
convexity that describe the optimization landscape.
Recently, there have been several orthogonal motivations ranging from optimization speed, robust-
ness against adversarial examples, and improving quality of generated samples for orthogonal ini-
tialization procedures (Xie et al., 2017; Ozay & Okatani, 2016; Cisse et al., 2017; Odena et al.,
2018). All of which raise the question about the relative importance of maintaining orthogonality
during training as compared to critical initializations proposed in (Pennington et al., 2017). In this
work we study simple feed-forward, fully connected networks. We make theoretical advances in
understanding the connection between eigenvalues of the Jacobian and local measures of gradient
smoothness. Subsequently we go onto analyze experimentally, through the lens of manifold opti-
mization the importance of maintaining orthogonality or near orthogonality throughout training of
deep neural networks with 200 layers. This allows us to show that nearly isometric networks opti-
mize worse because their gradients rapidly become less smooth. In contrast, less isometric networks
gradually degrade in smoothness, which allows them to be trained with a higher learning rate and
converge faster. Moreover, we show that networks trained with orthogonality and near orthogonality
constraints are considerably less sensitive to initialization and optimize rapidly regardless. Contrary
to the recent conjecture by Santurkar et al. (2018), this effect is not attributable to an increase in
gradient smoothness. Our work suggests that maintaining near orthogonality constraint throughout
1
Under review as a conference paper at ICLR 2019
training provides robust performance in practice compared to the orthogonal weight initialization
scheme which requires a highly specific parameter tuning. This offers insight into the role of Weight
Normalization (Salimans & Kingma, 2016), of which the near orthogonal constraint can be seen as
a variant.
2	Background
2.1	Formal description of the network
Following (Pennington et al., 2017; 2018; Schoenholz et al., 2016), we consider a feed-forward,
fully connected neural network with L hidden layers. Each layer l ∈ {1, . . . , L} is given as a
recursion of the form
xl = φ(hl),	hl = Wlxl-1 +bl	(1)
where xl are the activations, hl are the pre-activations, Wl ∈ RN ×N are the weight matrices, bl are
the bias vectors and φ(∙) is the activation function. For consistency, the input is denoted as x0. The
output layer of the network computes y = g-1(hg) where g is a link function and hg = Wg XL+bg.
The hidden layer input-output Jacobian matrix Jxx0L is given by,
JxL, dXL = YY Dl Wl
l=1
(2)
where Dl is a diagonal matrix with entries Dli,i = φ0(hli). As pointed out in (Pennington et al.,
2017; Schoenholz et al., 2016), the conditioning of the Jacobian matrix affects the conditioning of
the back-propagated gradients for all layers.
2.2	Initialization Strategy for Maximizing Signal Propagation
Extending the classic result on the Gaussian process limit for wide layer width obtained by Neal
(1996), recent work (Matthews et al., 2018; Lee et al., 2017) has shown that for deep untrained net-
works with elements of their weight matrices Wij drawn from a Gaussian distribution N(0, σW)
the empirical distribution of the pre-activations hl converges weakly to a Gaussian distribution
N(0, qlI) for each layer l in the limit of the width N → ∞. Under this mean-field condition
the variance the of the distribution is defined recursively given the pre-activations in the preceding
layer:
ql = σW / φ PqlTh) dμ(h + σb	⑶
where dμ(h) denotes the standard Gaussian measure √∏ exp (-2h2). The variance of the pre-
activations of the first layer q1 depends on the '2 norm of the inputs q1 = σW ∣∣χ0∣∣2 + σb. The
recursion defined in equation 3 has a fixed point q*,
q* = σW / φ √∕q*h) dμ(h) + σb
(4)
which can be satisfied for all layers by appropriately choosing σW , σb and scaling the input x0
accordingly. In order to ensure that Jxx0L is well conditioned, Pennington et al. (2017) require that in
addition to the variance of pre-activation being constant for all layers, two additional constraints are
met. Firstly, they require that the mean square singular value of DW for each layer have a certain
value in expectation.
X = NE [Tr [(DW)>DW]] = σW / [φ,√q*h)]2 dμ(h)	(5)
Given that the mean squared singular value of the Jacobian matrix Jxx0L is (χ)L, they require that
χ = 1 which corresponds to a critical initialization where the gradients are asymptotically stable as
L → ∞. Secondly, they require that the maximal squared singular value s2max of the Jacobian Jxx0L
be bounded. In (Pennington et al., 2017) it was shown that for weights with Gaussian distributed
2
Under review as a conference paper at ICLR 2019
elements, the maximal singular value increases linearly in depth even if the network is initialized
with χ = 1. Fortunately, for orthogonal weights, the maximal singular value smax is bounded even
as L → ∞ (Pennington et al., 2018) and for piecewise-linear φ(.), it is analytically derivable and
admits a solution in q* for Smax(JX。) = 1 for any choice of L. For arbitrary φ(.)'s, Smax Can be
obtained numerically from the solution of a functional equation describing the density of singular
values.
The described, theoretically derived intialization scheme has been tested and has shown a substan-
tial speed up in training times on CIFAR-10. The impressive results invite further inquest into the
connection between the class of critical initializations with bounded maximal singular values of the
Jacobian matrix and optimization, as well as the specific conditions under which the Gaussian limit
on pre-activations holds. Particularly, we show that for random neural networks the maximum sin-
gular value of the Jacobian matrix is related to the curvature of the parameter space as measured
by the Fisher information matrix G, which in turn affects the upper bound on the maximum stable
learning rate (Bottou, 2010). Subsequently, using manifold optimization we analyze how enforcing
orthogonality affects the convergence speed. Finally, we show that for random networks with or-
thogonal weights pre-activations do not necessarily converge in distribution to a Gaussian, raising
questions about the conditions under which the mean field approximation holds.
3	Results
3.1	Connection between Smax(JxL) AND THE maximum eigenvalue λmax(G)
Seen from a probabilistic perspective, the output ofa neural network defines a conditional probabil-
ity distribution pθ (y∣χ0), where θ = {vec(W1),..., Vec(WL), b1,..., bL} is the set of all hidden
layer parameters (Botev et al., 2017; Amari, 2016). in this context, the Fisher information matrix
defined as,
G，Eχ0,y [口 logPθ(y∣χ0)Vθ logpθ(y∣χ0)>i	(6)
can be expanding using the chain rule:
G = Eχ0,y [jχL>Wg>Vhg logpθ(y∣x0)Vhg logpθ(y∣x0)>WgJχ] .	(7)
Each block of the Fisher information matrix with respect to parameters a, b ∈ θ can further be
expressed as
Ga,b = Ex。[J?>Jha>Wg>HgWgJheJhei	(8)
where the final layer Hessian Hg is defined as Vhg logpθ(y∣x0). We can re-express the outer prod-
uct of the score function Vhg log pθ (y∣x0) as the second derivative of the log-likelihood, provided it
is twice differentiable and it does not depend on y, which also allows us to drop y from the expecta-
tion. This condition naturally holds for all canonical link functions and matching generalized linear
model loss functions. We define the matrix of partial derivatives of the α-th layer pre-activations
with respect to the layer specific parameters separately for Wα and bα as:
Jha = xα-1> 0 I for a = Vec(Wα),	Jha = I for a = bα	(9)
We can further simplify the expression for the blocks of the Fisher information matrix equation 8,
using the fact that the pre-activations converge weakly to an isotropic Gaussian distribution in the
limit of wide layers (Matthews et al., 2018; Lee et al., 2017). This justifies assuming independence
of the pre-activations of all the layers. We can additionally assume for convenience that the input
to the network x0 has been whitened before being scaled by q*. The simplified expressions are as
follows:
Gvec(wa),vec(wβ) = E [*°-"*1> 0 Jha>Wg>HgWgJhei	(10)
= q*2I0E hJxhLa>Wg>HgWgJxhLe i	(11)
Gba,bβ = E [Jhha>Wg>HgWgJhei	(12)
3
Under review as a conference paper at ICLR 2019
G Vec(Wa),bβ = E [(xα-1> ㊈ I)Jha >Wg >Hg Wg JheJ
E [χα-1> ㊈ I] E Jha >Wg >Hg Wg Jhβ] = 0
=0
(13)
(14)
We then consider a block diagonal approximation to the Fisher information matrix. Under this
simplification the maximum eigenvalue of the the Fisher information matrix is λmaχ(G) ≈
maxa λmaχ(Ga,a). The quality of this approximation is given by the generalization of the Ger-
shgorin circle theorem to block-partitioned matrices, which is detailed in the Appendix 4.1. Fol-
lowing equation 10, each diagonal block a with respect to the weight matrices Wα is bounded by
s2max(JxhLa)s2max(Wg)smax(Hg) , while the diagonal blocks a with respect to the biases bα are
bounded by (q*2smaxJh： )s^^(wg )smax(Hg)) using equation 12. These results can be further
simplified if we initialize Wg as a scaled semi-orthogonal matrix such that Wg>Wg = σW2 I and
consider a neural network regression model with mean-square loss. Then the respective bounds be-
come Smax(Jha )σW and q*2smaxJhɑ )σW. Assuming Smax(Jha) is a monotonically increasing
function of L it is sufficient to ensure that Smax(JhI) is small in order for λm,ax(G) to be small.
Furthermore since q* 《1 and since Smax(JxL) is at most σw larger than Smax(JhL), bounding
the maximum value of the hidden layer input-out Jacobian bounds the maximum eigenvalue of the
Fisher information matrix.
3.2	Optimization over manifolds
Optimizing neural network weights subject to manifold constraints has recently attracted consid-
erable interest, with several lines of research focusing on either the approximate preservation of
gradient norms in recurrent neural networks (Arjovsky et al., 2015; Henaff et al., 2016; Vorontsov
et al., 2017), increasing the robustness to adversarial examples (Cisse et al., 2017), or heuristically
motivated improvements to prediction accuracy (Xie et al., 2017; Cho & Lee, 2017; Ozay & Okatani,
2016). In this work we probe how constraining the weights of each layer to be orthogonal or near
orthogonal affects the spectrum of the hidden layer input-out Jacobian and of the Fisher information
matrix.
We briefly review notions from differential geometry and optimization over matrix manifolds (Edel-
man et al., 1998; Absil et al., 2007), in order to lay ground for a discussion of the specifics of
the optimization techniques used in our experiments. Informed readers are encouraged to skip to
Sec. 3.2.1. The potentially non-convex constraint set constitutes a Riemannian manifold, when it is
locally isomorphic to Rn, differentiable and endowed with a suitable (Riemannian) metric, which
allows us to measure distances in the tangent space and consequentially also define distances on
the manifold. There is considerable freedom in choosing a Riemannian metric; here we consider the
metric inherited from the Euclidean embedding space which is defined as hW, W0i , Tr(W0>W).
Stiefel Manifold Let St(p, n)	(p ≤ n) denote the set of all n × p orthonormal matrices
St(p, n) , {W ∈ Rn×p : W>W = Ip}	(15)
Notice that for p = n, the Stiefel manifold parametrizes the set of all orthogonal matrices.
Oblique Manifold Let Ob(p, n) denote the set of all n × p matrices with unit norm columns
Ob(p, n) , {W ∈ Rn×p : diag(W>W) = 1}	(16)
where diag(M) denotes an operator that extracts the diagonal entries ofM. This manifold is equiv-
alent to the direct product P unit-norm spheres Sn-I X Sn-I × ∙ ∙ ∙ × Sn-I.
To optimize a cost function with respect to parameters lying in a non-Euclidean manifold we
must define a descent direction. This is done by defining a manifold equivalent of the directional
derivative. An intuitive approach replaces the movement along a vector t with movement along a
geodesic curve γ(t), which lies in the manifold and connects two points W, W0 ∈ M such that
γ(0) = W, γ(1) = W0. The derivative of f(γ(t)) with respect to t then defines a tangent vector
for each t.
4
Under review as a conference paper at ICLR 2019
Tangent vector ξW is a tangent vector at W if ξW satisfies γ(0) = W and
ξw , df (γ(t))	, 70(0)/	(17)
dt	t=0
The set of all tangents to M at W is referred to as the tangent space to M at W and is denoted by
TWM. The geodesic importantly is then specified by a constant velocity curve γ00(t) = 0 with ini-
tial velocity ξW. To perform a gradient step, we must then move along ξW while respecting the man-
ifold constraint. This is achieved by applying the exponential map defined as ExpW(ξW) , γ(1),
which moves W to another point W0 along the geodesic. While certain manifolds, such as the
Oblique manifold, have efficient closed-form exponential maps, for general Riemannian manifolds,
the computation of the exponential map involves numerical solution to a non-linear ordinary dif-
ferential equation (Absil et al., 2007). An efficient alternative to numerical integration is given by
an orthogonal projection onto the manifold. This projection is formally referred to as a retraction
RtW : TW M → M.
Finally, gradient methods using Polyak (heavy ball) momentum (e.g. ADAM (Kingma & Ba,
2014)) require the iterative updating of terms which naturally lie in the tangent space. The par-
allel translation Tζ(ξ) : TM L TM → TM generalizes vector composition from Euclidean to
non-Euclidean manifolds, by moving the tangent ξ along the geodesic with initial velocity ζ ∈ T
and endpoint W0, and then projecting the resulting vector onto the tangent space TW0 M. As with
the exponential map, parallel transport T may require the solution of non-linear ordinary differen-
tial equation. To alleviate the computational burden, we consider vector transport as an effective,
projection-like solution to the parallel translation problem. We overload the notation and also denote
it as T, highlighting the similar role that the two mappings share. Technically, the geodesics and
consequentially the exponential map, retraction as well as transport T depend on the choice of the
Riemannian metric. Putting the equations together the updating scheme for Riemmanian stochastic
gradient descent on the manifold is
Wt+1 = ΠWt (-ηt gradf)	(18)
where Π is either the exponential map Exp or the retraction Rt and gradf is the gradient of the
functionf (W) lying in the tangent space TWM. The updating scheme for optimizers using mo-
mentum, as mentioned before, additionally requires updating the momentum term using parallel
transport.
3.2.1	Optimizing over the Oblique manifold
Cho & Lee (2017) proposed an updating scheme for optimizing neural networks where the weights
of each layer are constrained to lie in the oblique manifold Ob(p, n). Using the fact that the manifold
itself is a product ofp unit-norm spherical manifolds, they derived an efficient, closed-form Rieman-
nian gradient descent updating scheme. In particular the optimization simplifies to the optimization
over Ob(1, n) for each column wi∈{1,...,p} of W.
Oblique gradient The gradient gradf of the cost function f with respect to the weights lying in
Ob(1, n) is given as a projection of the Euclidean gradient Gradf onto the tangent at w
gradf = Gradf - (w>Gradf)w	(19)
Oblique exponential map The exponential map Expw moving w to w0 along a geodesic with
initial velocity ξw
w
EXpw = ξw cos(kwk) + kw sin(kwk)	(20)
Oblique parallel translation The parallel translation T moves the tangent vector ξw along the
geodesic with initial velocity ζw
Tζw (ξw ) = ξw -
cos(kζwk)) +wsin(kζwk))
kζwk
(21)
5
Under review as a conference paper at ICLR 2019
Cho & Lee (2017) derived a regularization term which penalizes the distance between the point in
the manifold W and the closest orthogonal matrix with respect to the Frobenius norm.
ρ(λ, W) = 2∣∣W>W - IIlF
(22)
3.2.2	Optimizing over the Stiefel manifold
Optimization over Stiefel manifolds in the context of neural networks has been studied by (Harandi
& Fernando, 2016; Wisdom et al., 2016; Vorontsov et al., 2017). However, (Wisdom et al., 2016;
Vorontsov et al., 2017) proposed a different parametrization resulting from an alternative choice of
the Riemannian metric. Here we propose the parametrization using the Euclidean metric, which
results in a different definition of vector transport.
Stiefel gradient The gradient gradf of the cost function f with respect to the weights lying in
St(p, n) is given as a projection of the Euclidean gradient Gradf onto the tangent at W (Edelman
et al., 1998; Absil et al., 2007)
gradf = (I - WW>)Gradf + 2W (W>Gradf - Gradf>W)
(23)
Stiefel retraction The retraction RtW(ξW) for the Stiefel manifold is given by the Q factor of the
QR decomposition (Absil et al., 2007).
RtW(ξW) = qf(W + ξW)
(24)
Stiefel vector transport The vector transport T moves the tangent vector ξw along the geodesic
with initial velocity ζw for W ∈ St(p, n) endowed with the Euclidean metric.
TZw(ξw) = (I- YY>) ξw + 2Y (Y>ξw - ξWY)
(25)
where Y , RtW(ζW). It is easy to see that the transport T consists of a retraction of tangent ζW
followed by the orthogonal projection of ηW at RtW (ζW). The projection is the same as the one
mapping P : Gradf → gradf in equation 23.
3.3	Optimizing over non-compact manifolds
The critical weight initialization yielding a singular spectrum of the Jacobian tightly concentrating
on 1 implies that a substantial fraction of the pre-activations lie in expectation in the linear regime
of the squashing nonlinearity and as a consequence the network acts quasi-linearly. To relax this
constraint during training we allow the scales of the manifold constrained weights to vary. We chose
to represent the weights as a product of a scaling matrix and matrix belonging to the manifold. Then
the optimization of each layer consists in the optimization of the two variables in the product. In
this work we only consider isotropic scalings, but the method generalizes easily to the use of any
invertible square matrix.
3.4	Numerical Experiments
To experimentally test the potential effect of maintaining orthogonality throughout training and com-
pare it to the unconstrained optimization (Pennington et al., 2017), we trained a 200 layer network
tanh network on CIFAR-10 and SVHN. We present the results for the former in the main text, while
the latter can be found in the Appendix 4.2 in the interest of space. Following (Pennington et al.,
2017) we set the width of each layer to be N = 400 and chose the σW, σb in way to ensure that both
X concentrates on 1 but SmaX Varies as a function of q* (See Fig. 1). We considered two different
critical initializations with q* = = and q* ≈ 9 X 10-4, which differ both in spread of the singular
values as well as in the resulting training speed and final test accuracy, as reported by (Pennington
et al., 2017). To test how enforcing strict orthogonality or near orthogonality affects convergence
speed, and the spectrum of hidden layer input-output Jacobians and the maximum eigenvalues of the
Fisher information matrix, we trained Stiefel and Oblique constrained networks and compared them
6
Under review as a conference paper at ICLR 2019
Distribution of λmax(G) VS Smax(JhL) as a function of q*
103.5
O
102.25
S
Y
101.0
×	Networks used in experiments
★	Networks not used in experiments
O Isometric linear networks
©
A***★
0.5
0.4 1
I
0.2
0.1
Figure 1: At initialization the maximum
eigenvalue of the Fisher information ma-
trix GG correlates highly with the maximum
singular value of the Jacobian Jxx0L . While
the dependence is not perfectly quadratic as
predicted, the deviation can be understood in
terms of the bound implied by the Gershgorin
theorem for block matrices - the bound be-
comes tight in the limit of q * → ∞. The net-
works not used in the experiments were ran-
domly initialized with χ = 1 and q * vary-
ing uniformly on a logarithmical grid between
9 × 10-4 and 0.5.
100
100.5	101.0	101.5
Figure 2: Manifold constrained networks are insensitive to the choice of q* : Train loss and test
accuracy for Euclidean, Stiefel and Oblique networks with two different values ofq*. The manifold
constrained networks minimize the training loss at approximately the same rate, being faster than
both Euclidean networks. Despite this, there is little difference between the test accuracy of the
Stiefel and Oblique networks and the Euclidean networks initialized with q* = 9 × 10-4. Notably,
the latter attains a marginally higher test set accuracy towards the end of training.
to the unconstrained “Euclidean” network described in (Pennington et al., 2017). We used a Rieman-
nian version of ADAM (Kingma & Ba, 2014). When performing gradient descent on non-Euclidean
manifolds, we split the variables into three groups: (1) Euclidean variables (e.g. the weights of the
classifier layer, biases), (2) non-negative scaling σW both optimized using the regular version of
ADAM, and (3) manifold variables optimized using Riemannian ADAM. The initial learning rates
for all the the groups, as well as the the non-orthogonality penalty (see 22) for Oblique networks
were chosen with Bayesian optimization, maximizing validation set accuracy after 50 epochs. All
networks were trained with a minibatch size of 1000. We trained 5 networks of each kind, and col-
lected eigenvalue and singular value statistics every 5 epochs, from the first to the fiftieth, and then
after the hundredth and two hundredth epochs.
Based on the bound the maximum eigenvalue of the Fisher information matrix derived in Section
3.1, we predicted that at initialization λmax(G) should covary with σ22iax(JxL). Our prediction is
vindicated in that we find a strong, significant correlation between the two, with a Pearson coef-
ficient of ρ = 0.88. The numerical values are presented in Fig. 1. Additionally we see that both
the maximum singular value and maximum eigenvalue increase monotonically as a function of q* .
Based on the previous work by Saxe et al. (2013) showing depth independent learning dynamics in
linear orthogonal networks, we included 5 instantiations of this model in the comparison. The input
to the linear network was normalized using the following scheme applied to critical, non-linear net-
works with q* = 1/64. Surprisingly, the deep linear networks had a substantially larger λmax (G)
than its non-linear counterparts initialized with identically scaled input (Fig. 1).
Having established a connection between q* the maximum singular value of the hidden layer input-
output Jacobian and the maximum eigenvalue of the Fisher information, we wish to investigate the
effects of initialization on subsequent optimization. As reported by Pennington et al. (2017), learning
7
Under review as a conference paper at ICLR 2019
speed and generalization peak at intermediate values of q* ≈ 10-0-5. This result is counterintuitive
given that the maximum eigenvalue of the Fisher information matrix, much like that of the Hessian
in convex optimization, upper bounds the maximal learning rate (Boyd & Vandenberghe, 2004;
Bottou et al., 2016). To gain insight into the effects of choice q* on convergence rate, we trained the
Euclidean networks and estimated the local values of smax and λmax during optimization. At the
same time we asked whether we can effectively control the two aforesaid quantities by constraining
the weights of each layer to be orthogonal or near orthogonal. To this end we trained Stiefel and
Oblique networks and recorded the same statistics as for before.
We present the results of training in Fig. 2, where it can be seen that Euclidean networks with
q* ≈ 9 × 10-4 perform worse with respect to training loss and test accuracy than those initialized
with q* = 1/64. On the other hand, manifold constrained networks are insensitive to the choice
of q* . Moreover, Stiefel and Oblique networks perform marginally worse on the test set compared
to the Euclidean network with q* ≈ 9 × 10-4, despite attaining a lower training loss. This latter
fact indicates that manifold constrained networks the are perhaps prone to overfitting. We observe
that reduced performance of Euclidean networks initialized with q* ≈ 9 × 10-4 may partially be
explained by their rapid increase in λmaχ(G) the initial 5 epochs of optimization (see Fig. 4).
While all networks undergo this rapid increase, it is most pronounced for Euclidean networks with
q* ≈ 9 X 10-4. The increase λmaχ(G) correlates with the inflection point in the training loss curve
that can be seen in the inset of Fig. 2. Interestingly, the manifold constrained networks optimize
efficiently despite differences in λmaχ(G), showing that their performance cannot be attributed to
increasing the gradient smoothness as postulated by (Santurkar et al., 2018). Finally, we observe that
for manifold constrained networks the maximum singular value of the Jacobian remains predictive
of the maximum eigenvalue of the Fisher information matrix with an average Pearson correlation of
0.93 (see Fig.3). For Euclidean networks the correlation is considerably lower, with ρ = 0.26 for
q* = 1/64 and 0.83 for q* = 9 × 10-4.
Training loss Vs Smax(JXo) and λmaχ(G)
Figure 3: The maximum singular value of the Jacobian smax(Jxx0L ) is predictive of the maxi-
mum eigenvalue of the Fisher information matrix λmaχ (G) during training. The dashed colored
lines represent λmaχ(G), while the continuous colored lines represent Smax(JxL). The correlation
captures particularly well the initial increase in the λmax(G) during the first few epochs. Moreover,
for Stiefel and Oblique networks the correlation is stronger.
3.4.1	Non-Gaussian limit of pre-activation
The derivation of the spectra of hidden layer input-output Jacobians presented in (Pennington et al.,
2017; 2018) crucially depends on the existence of the Gaussian limit of the distribution of pre-
activations, which in turn assumes that the elements of the weight matrices are sampled iid from
some probability measure with finite first two moments. This condition is trivially violated for ma-
trices sampled from the Haar (uniform) measure over the Stiefel manifold. We invoke Theorem
1 from Meckes (2012) which asserts that for a random semi-orthogonal projection of an arbitrary
random vector to converge to a Gaussian distribution in the bounded Lipschitz distance, the pro-
8
Under review as a conference paper at ICLR 2019
Gradient smoothness λmaχ(GG)
Figure 4: For manifold constrained net-
works, gradient smoothness is not predic-
tive of optimization rate. Euclidean net-
works with a low initial λmaχ(GG) rapidly
become less smooth, whereas Euclidean net-
works with a larger λmaχ(GG) remain rela-
tively smoother. Notably, the Euclidean net-
Work with q* = 1/64 has &most an order of
magnitude smaller λmaχ(G) than the StiefeI
and Oblique networks, but reduces training
loss at a slower rate.
jection must go from d input dimensions to 京爆：)))dimensions. We also show empirically that
in orthogonally initialized random networks, the pre-activations do not necessarily converge to a
multivariate Gaussian distribution (see Fig. 5). We pose as an open problem the conditions under
which the random matrix argument of Pennington et al. (2017) holds.
Theorem 1. (Meckes, 2012) Let X be a random vector in Rd with
E kXk2 = σ2d,
E [∣∣Xk2 σ-2 - d] ≤ L√d,
sup E (ξ>X)2 ≤ 1
ξ∈Sd-1
Let XW denote the projection WX for W ∈ St(k, d), with k = δ Ko需gd()d))and 0 ≤ δ ≤ 2 ,then
there is a c > 0 such that for e = 2exp [—clog (Iog(d))] there exists a subset I ⊆ St(k, d) with
probability mass p(I) with respect to the Haar measure. Then p(I) ≥ 1 - C exp -c0d2 such that
for all W ∈ I
sup	|Ex [f(Xw)] — E[f(σZ)]∣ ≤ C0e
max(kf k∞,lflL)≤1
士 JO Uqe∙i0aUe-SSne'Uouisow
Figure 5: Non-Gaussian projection of the pre-activations for random networks q* = 1/64 and
evaluated on CIFAR-10. The projection was obtained using the algorithm in (Blanchard et al.,
2006). The rightmost panel shows an overlay of the pre-activations compared to the most non-
Gaussian projection of a set of 400 dimensional Gaussian random variables with variance matched
to the real data.
4	Discussion
In this work we have analyzed the geometry of critical initializations and during training with or-
thogonal and near orthogonal manifold constraints. In the process we derived a novel bound on
the maximum eigenvalue of the Fisher information matrix and related it to the expected maximum
eigenvalue of the hidden layer input-output Jacobian, thereby elucidating the empirical success of
the initialization proposed by (Pennington et al., 2017). We then probed numerically the benefits of
maintaining orthogonality and near orthogonality of weight matrices during training, while relating
them to the gradient smoothness measured by the maximum eigenvalue of the Fisher information
matrix. We observed several interesting phenomena, which include a rapid decrease of the gradient
smoothness of the Euclidean networks that are smoother at initialization. This paradoxical result
9
Under review as a conference paper at ICLR 2019
partially explains the observations made in (Pennington et al., 2017) but further analysis needs to be
done to understand it fully. Another conclusion is that the robustness to the choice of q* and low
computational overhead makes optimization on the Oblique manifold an appealing alternative to
searching for the optimal q*. This could be particularly advantageous in situations where the input
data is non-stationary. Finally we pose an open question the conditions under which the mean field
approximation to the pre-activations hold.
10
Under review as a conference paper at ICLR 2019
Acknowledgments
References
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton
University Press, Princeton, N.J. ; Woodstock, December 2007. ISBN 978-0-691-13298-3.
Shun-ichi Amari. Information Geometry and Its Applications. Springer, Japan, 1st ed. 2016 edition
edition, February 2016. ISBN 978-4-431-55977-1.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary Evolution Recurrent Neural Networks.
arXiv:1511.06464 [cs, stat], November 2015. arXiv: 1511.06464.
Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, and Vladimir Spokoiny. In Search of
Non-Gaussian Components of a High-Dimensional Distribution. Journal of Machine Learning
Research, pp. 36, 2006.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton Optimisation for
Deep Learning. In International Conference on Machine Learning, pp. 557-565, July 2017.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010. 00829.
Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine
Learning. arXiv:1606.04838 [cs, math, stat], June 2016. arXiv: 1606.04838.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization, With Corrections 2008. Cambridge
University Press, Cambridge, UK ; New York, 1 edition edition, March 2004. ISBN 978-0-521-
83378-3.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in
Neural Information Processing Systems, pp. 5229-5239, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
Networks: Improving Robustness to Adversarial Examples. arXiv:1704.08847 [cs, stat], April
2017. arXiv: 1704.08847.
Alan Edelman, TomaS A. Arias, and Steven T. Smith. The Geometry of Algorithms with Orthogo-
nality Constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303-353, January
1998. ISSN 0895-4798, 1095-7162. doi: 10.1137/S0895479895290954.
Mehrtash Harandi and Basura Fernando. Generalized backpropagation, Etude De Cas: Orthogonal-
ity. arXiv:1611.05927 [cs], November 2016. 00004 arXiv: 1611.05927.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. arXiv:1512.03385 [cs], December 2015. 01528 arXiv: 1512.03385.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent Orthogonal Networks and Long-Memory
Tasks. arXiv:1602.06662 [cs, stat], February 2016. arXiv: 1602.06662.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980
[cs], December 2014. 01869 arXiv: 1412.6980.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. arXiv:1711.00165 [cs, stat],
October 2017. arXiv: 1711.00165.
Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian Process Behaviour in Wide Deep Neural Networks. arXiv:1804.11271 [cs, stat],
April 2018. arXiv: 1804.11271.
Elizabeth Meckes. Projections of Probability Distributions: A Measure-Theoretic Dvoretzky The-
orem. In Geometric Aspects of Functional Analysis, Lecture Notes in Mathematics, pp. 317-
326. Springer, Berlin, Heidelberg, 2012. ISBN 978-3-642-29848-6 978-3-642-29849-3. doi:
10.1007/978-3-642-29849-3,18.
11
Under review as a conference paper at ICLR 2019
Radford M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statis-
tics. Springer New York, New York, NY, 1996. ISBN 978-0-387-94724-2 978-1-4612-0745-0.
doi: 10.1007/978-1-4612-0745-0.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raf-
fel, and Ian Goodfellow. Is Generator Conditioning Causally Related to GAN Performance?
arXiv:1802.08768 [cs, stat], February 2018. arXiv: 1802.08768.
Mete Ozay and Takayuki Okatani. Optimization on Submanifolds of Convolution Kernels in CNNs.
arXiv preprint arXiv:1610.07008, 2016. 00003.
Chethan Pandarinath, Daniel J. O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D. Stavisky,
Jonathan C. Kao, Eric M. Trautmann, Matthew T. Kaufman, Stephen I. Ryu, Leigh R. Hochberg,
Jaimie M. Henderson, Krishna V. Shenoy, L. F. Abbott, and David Sussillo. Inferring single-trial
neural population dynamics using sequential auto-encoders. Nature Methods, September 2018.
ISSN 1548-7091, 1548-7105. doi: 10.1038/s41592-018-0109-9.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. arXiv:1711.04735 [cs, stat], Novem-
ber 2017. arXiv: 1711.04735.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The Emergence of Spectral Univer-
sality in Deep Networks. arXiv:1802.09979 [cs, stat], February 2018. arXiv: 1802.09979.
Daniel Ritchie, Paul Horsfall, and Noah D. Goodman. Deep Amortized Inference for Probabilistic
Programs. arXiv:1610.05735 [cs, stat], October 2016. arXiv: 1610.05735.
Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to
Accelerate Training of Deep Neural Networks. arXiv:1602.07868 [cs], February 2016. 00003
arXiv: 1602.07868.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Nor-
malization Help Optimization? (No, It Is Not About Internal Covariate Shift). arXiv:1805.11604
[cs, stat], May 2018. arXiv: 1805.11604.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. arXiv:1312.6120 [cond-mat, q-bio, stat],
December 2013. arXiv: 1312.6120.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. arXiv:1611.01232 [cs, stat], November 2016. 00003 arXiv: 1611.01232.
Christiane Tretter. Spectral Theory of Block Operator Matrices and Applications. IMPERIAL
COLLEGE PRESS, October 2008. ISBN 978-1-86094-768-1 978-1-84816-112-2. doi: 10.1142/
p493.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. arXiv:1702.00071 [cs], January 2017. arXiv:
1702.00071.
Scott Wisdom, Thomas Powers, John R. Hershey, Jonathan Le Roux, and Les Atlas. Full-
Capacity Unitary Recurrent Neural Networks. arXiv:1611.00035 [cs, stat], October 2016. arXiv:
1611.00035.
Di Xie, Jiang Xiong, and Shiliang Pu. All You Need is Beyond a Good Init: Exploring Better
Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and
Modulation. arXiv:1703.01827 [cs], March 2017. arXiv: 1703.01827.
12
Under review as a conference paper at ICLR 2019
Appendix
4.1	S ketch of the bound using the block diagonal Gershgorin circle theorem
Using remark 1.13.2 from Tretter (2008), we consider a a block partitioned matrix A ∈ Rdn×dn with
n d × d blocks Ai,j i, j ∈ [1, . . . , n]. Then consider diagonal blocks Aii which we additionally
require to hermitian. Then define sets for each ith block-diagonal element the set of Gershgorin
disks Γi
Γi , s(Aii) ∪
[
k=1
C b k (Aii),	X k Aij Il
j=1,j6=i
(26)
where s(∙) denotes the spectrum of a matrix and C(c, r) denotes a ball centered on C with radius r
C(c,r) , {λ : Iλ - cI ≤ r}
(27)
Therefore each Γi denotes a union of balls each centered on the eigenvalues of λ(Ai,i) with
radius given by the spectral norm of the off-diagonal blocks. The collection of Γi ’s contains the
spectrum of A. Let us assume for convenience that we are trying to predict continuous variables in
a regression context and with a mean-square error loss. This simplifies the analysis in Section 3.1,
by setting Hg = I. Then the block-diagonal elements of G become JhL > JhL. The off diagonal
elements have a spectral radius of smax(JθhiL>JθhjL) ≤ smax (JθhiL )smax (JθhjL ). As a consequence
the Gershgorin disks for ith block diagonal matrix is a set sum of the eigenvalues of JθhiL >JθhiL and
the disks centered on each of them, all with radius at most smax(JθhiL )smax(JθhjL ). Since we care
only about the maximum eigenvalue of G, we consider the maximum eigenvalue of each diagonal
block and the disk around it.
λ
Γi = C
smax
n
X
j=1,j6=i
⊆C
smax
n
X
j=1,j6=i
λ
n
X
j=1,j6=i
λ-
smax
(28)
(29)
(30)
(31)
So the relative size of the disk compare to smax (JθhiL ) becomes small whenever smax (JθhiL ) >
Pjn=1,j6=i smax(JθhjL) and smax (JθhiL) → ∞ since the magnitude of s2max (JθhiL) grows quadratically.
4.2	SVHN FIGURES
13
Under review as a conference paper at ICLR 2019
SVHN performance
ssol gniniarT
LIX 6 x*b
ssol gniniar
寸9/1 U *b
≥ 0.0020
O
C
OJ
段 0.0015
O
υ
<v
^Q. 0.0010
E
业 0.0005
0.00225
0.00200
0.00175
0.00150
0.00125
0.00100
0.00075
0.00050
0.00225
0.00200
0.00175
0.00150
0.00125
0.00100

0.00075 f
0.00050
0	25
Training loss Vs Smax(JhL) and λmaχ(G)
EUClidean	104.0
-----------------------------W
103.0
--------------------------102.5
102.0
101.5
101.0
100.5
-----------------------------------------100
50	75	100	125	150	175	200
λdna s
.0 .5 .0 .5 .0 .5 .0 .5
4. 3. 3. 2. 2. 1. 1. 0.
10 10 10 10 10 10 10 10
50	75	100	125	150	175
EpoChs
104.0
103.5
_ 103.0
102.5
-102.0
101.5
101.0
100.5
」100
200
λdna s
.0 .5 .0 .5 .0 .5 .0 .5
4. 3. 3. 2. 2. 1. 1. 0.
10 10 10 10 10 10 10 10
14