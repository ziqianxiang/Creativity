Under review as a conference paper at ICLR 2019
Curiosity-Driven Experience Prioritization
via Density Estimation
Anonymous authors
Paper under double-blind review
Ab stract
In Reinforcement Learning (RL), an agent explores the environment and collects
trajectories into the memory buffer for later learning. However, the collected tra-
jectories can easily be imbalanced with respect to the achieved goal states. The
problem of learning from imbalanced data is a well-known problem in supervised
learning, but has not yet been thoroughly researched in RL. To address this prob-
lem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to en-
courage the agent to over-sample those trajectories that have rare achieved goal
states. The CDP framework mimics the human learning process and focuses more
on relatively uncommon events. We evaluate our methods using the robotic en-
vironment provided by OpenAI Gym. The environment contains six robot ma-
nipulation tasks. In our experiments, we combined CDP with Deep Deterministic
Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER).
The experimental results show that CDP improves both performance and sample-
efficiency of reinforcement learning agents, compared to state-of-the-art methods.
1	Introduction
Reinforcement Learning (RL) (Sutton & Barto, 1998) combined with Deep Learning (DL) (Good-
fellow et al., 2016) led to great successes in various tasks, such as playing video games (Mnih
et al., 2015), challenging the World Go Champion (Silver et al., 2016), and learning autonomously
to accomplish different robotic tasks (Ng et al., 2006; Peters & Schaal, 2008; Levine et al., 2016;
Chebotar et al., 2017; Andrychowicz et al., 2017).
One of the biggest challenges in RL is to make the agent learn sample-efficiently in applications
with sparse rewards. Recent RL algorithms, such as Deep Deterministic Policy Gradient (DDPG)
(Lillicrap et al., 2015), enable the agent to learn continuous control, such as manipulation and lo-
comotion. Furthermore, to make the agent learn faster in the sparse reward settings, Andrychowicz
et al. (2017) introduced Hindsight Experience Replay (HER) that encourages the agent to learn from
whatever goal states it has achieved. The combination use of DDPG and HER lets the agent learn to
accomplish more complex robot manipulation tasks. However, there is still a huge gap between the
learning efficiency of humans and RL agents. In most cases, an RL agent needs millions of samples
before it becomes good at the tasks, while humans only need a few samples (Mnih et al., 2015).
One ability of humans is to learn with curiosity. Imagine a boy learning to play basketball and he
attempting to shoot the ball into the hoop. After a day of training, he replayed the memory about the
moves he practiced. During his recall, he realized that he missed most of his attempts. However, a
few made contact with the hoop. These near successful attempts are more interesting to learn from.
He will put more focus on learning from these. This kind of curiosity-driven learning might make
the learning process more efficient.
Similar curiosity mechanisms could be beneficial for RL agents. We are interested in the RL tasks,
in which the goals can be expressed in states. In this case, the agent can analyze the achieved goals
and find out which states have been achieved most of the time and which are rare. Based on the
analysis, the agent is able to prioritize the trajectories, of which the achieved goal states are novel.
For example, the goal states could be the position and the orientation of the target object. We want
to encourage the agent to balance the training samples in the memory buffer. The reason is that the
policy of the agent could be biased and focuses on a certain group of achieved goal states. This
causes the samples to be imbalanced in the memory buffer, which we refer to as memory imbalance.
1
Under review as a conference paper at ICLR 2019
Figure 1: Robot arm Fetch and Shadow Dexterous hand environment: FetchPush,
FetchPickAndPlace, FetchSlide, HandManipulateEgg, HandManipulateBlock,
and HandManipulatePen.
To overcome the class imbalance issue in supervised learning, such as training deep convolutional
neural networks with biased datasets, researchers utilized over-sampling and under-sampling tech-
niques (Deng et al., 2009; Felzenszwalb et al., 2008; Buda et al., 2018; Galar et al., 2012). For in-
stance, the number of one image class is significantly higher than another class. They over-sampled
the training images in the smaller class to balance the training set and ultimately to improve the
classification accuracy. This idea could be combined with experience replay in RL. We investi-
gate into this research direction and propose a novel curiosity-based prioritization framework for
reinforcement learning agents.
In this paper, we introduce a framework called Curiosity-Driven Prioritization (CDP) which allows
the agent to realize a curiosity-driven learning ability similar to humans. This approach can be
combined with any off-policy RL algorithm. It is applicable whenever the achieved goals can be
described with state vectors. The pivotal idea of CDP is to first estimate the density of each achieved
goal and then prioritize the trajectories with lower density to balance the samples that the agent
learns from. To evaluate CDP, we combine CDP with DDPG and DDPG+HER and test them in the
robot manipulation environments.
2	Background
In this section, we introduce the preliminaries, such as the experiment environments, the reinforce-
ment learning approaches and the density estimation algorithm we used in the experiments.
2.1	Environments
The environment we used in our experiments is the robotic simulations provided by OpenAI Gym
(Brockman et al., 2016; Plappert et al., 2018), using the MuJoCo physics engine (Todorov et al.,
2012). The robotic environment is based on currently existing robotic hardware and is designed as
a standard benchmark for Multi-goal RL. The robot agent is required to complete several tasks with
different goals in each scenario. There are two kinds of robot agents in the environment. One is
a 7-DOF Fetch robotic arm with a two-finger gripper as an end-effector. The other is a 24-DOF
Shadow Dexterous robotic hand. We use six challenging tasks for evaluation, including push, slide,
pick & place with the robot arm, and hand manipulation of the block, egg, and pen, see Figure 1.
Goals: The goals g are the desired positions and the orientations of the object.
States: The system states s in the simulation consist of positions, orientations, linear and angular
velocities of all robot joints and of an object. The state s consists of two sub-vectors, the achieved
goal state sg and the context state sc, i.e. s = (xg kxc), where k denotes concatenation. In our
case, the achieved goal state sg represents the positions and the orientations of the object, which has
the same dimension as the real goal g. The context state sc contains the reset system information,
including the linear and angular velocities of all robot joints and of an object. The sate input to the
universal value function, see Section 2.2, is the system state s combined with the real goal g, i.e.
(skg).
Rewards: In all environments, we consider sparse rewards r. There is a tolerant range between the
desired goal states and the achieved goal states. If the object is not in the tolerant range of the real
goal, the agent receives a reward signal -1 for each transition; otherwise, the reward signal is 0.
2
Under review as a conference paper at ICLR 2019
2.2	Reinforcement Learning
Markov Decision Process: We consider an agent interacting with an environment. We assume the
environment is fully observable, including a set of state S, a set of action A, a distribution of initial
states p(s0), transition probabilities p(st+1|st, at), a reward function r: S × A → R, and also a
discount factor γ ∈ [0, 1]. These components formulate a Markov decision process represented as a
tuple, (S, A,p, r, γ). A policy π maps a state to an action, π : S → A.
Deep Deterministic Policy Gradient: The objective, expected return Es0 [R0|s0], can be max-
imized using temporal difference learning, policy gradients, or the combination of both, i.e. the
actor-critic methods (Sutton & Barto, 1998). For continuous control tasks, Deep Deterministic Pol-
icy Gradient (DDPG) shows promising performance, which is essentially an off-policy actor-critic
method (Lillicrap et al., 2015).
Universal Value Function Approximators: For multi-goal continuous control tasks, DDPG can
be extended with Universal Value Function Approximators (UVFA) (Schaul et al., 2015a). UVFA
essentially generalizes the Q-function to multiple goal states g ∈ G. Now, the Q-value depends not
only on the state-action pairs, but also depends on the goals: Qπ(st, at, g) = E[Rt|st, at, g].
Hindsight Experience Replay: For robotic tasks, if the goal is challenging and the reward is sparse,
then the agent could perform badly for a long time before learning anything. Hindsight Experi-
ence Replay (HER) encourages the agent to learn from whatever goal states that it has achieved.
Andrychowicz et al. (2017) show that HER makes training possible in challenging robotic environ-
ments. However, the episodes are uniformly sampled in the replay buffer, and subsequently, the
virtual goals are sampled from the episodes. More sophisticated replay strategies are requested for
improving sample-efficiency (Plappert et al., 2018).
2.3	Density Estimation Methods
For estimating the density ρ of the achieved goals in the memory buffer, we use a Gaussian mix-
ture model because it can be trained reasonably fast for RL agents. GMM is also much faster
in inference compared to Kernel Density Estimate (KDE) (Rosenblatt, 1956). Gaussian Mixture
Model (GMM) (Duda & Hart, 1973; Murphy, 2012) is a probabilistic model that assumes all the
data points are generated from K Gaussian distributions with unknown parameters, mathematically:
P(X) = Pk=I CkN(x∣μk, ∑k). Every Gaussian density N(x∣μk, ∑k) is a component of the GMM
and has its own mean 模卜 and covariance ∑k. The parameters Ck are the mixing coefficients. In
our experiments, we use Variational Gaussian Mixture Model (V-GMM) (Blei et al., 2006). The
reason is that V-GMM has a natural tendency to set some mixing coefficients Ck close to zero and
generalizes better. Therefore, we decide to use V-GMM in our framework as a proof of concept.
3	Method
In this section, we formally describe our method, including the motivation, the framework, a math-
ematical grounding, and a comparison with prioritized experience replay (Schaul et al., 2015b).
3.1	Motivation
The motivation of incorporating curiosity mechanisms into RL agents is motivated by the human
brain. Recent neuroscience research (Gruber et al., 2014) has shown that curiosity can enhance
learning. They discovered that when curiosity motivated learning was activated, there was increased
activity in the hippocampus, a brain region that is important for human memory. To learn anew skill,
such as playing basketball, people practice repeatedly in a trial-and-error fashion. During memory
replay, people are more curious about the episodes that are relatively different and focus more on
those. This curiosity mechanism has been shown to speed up learning.
Secondly, the inspiration of how to design the curiosity mechanism for RL agents comes from the su-
pervised learning community, in particular the class imbalance dataset problem. Real-world datasets
commonly show the particularity to have certain classes to be under-represented compared to other
classes. When presented with complex imbalanced datasets, standard learning algorithms, includ-
ing neural networks, fail to properly represent the distributive characteristics of the data and thus
3
Under review as a conference paper at ICLR 2019
provide unfavorable accuracies across the different classes of the data (He & Garcia, 2008; Galar
et al., 2012). One of the effective methods to handle this problem is to over-sample the samples in
the under-represented class. Therefore, we prioritize the under-represented trajectories with respect
to the achieved goals in the agent’s memory buffer to improve the performance.
3.2	Curiosity-Driven Prioritization
In this section, we formally describe the Curiosity-Driven Prioritization (CDP) framework. In a
nutshell, we first estimate the density of each trajectory according to its achieved goal states, then
prioritize the trajectories with lower density for replay.
3.2.1	Collecting Experience
At the beginning of each episode, the agent uses partially random policies, such as -greedy, to start
to explore the environment and stores the sampled trajectories into a memory buffer for later replay.
A complete trajectory τ in an episode is represented as a tuple (S, A, p, r, γ). A trajectory contains
a series of continuous states st, see Section 2.1, where t is the timestep t ∈ {0, 1, .., T}. Each state
st ∈ S also includes the state of the achieved goal stg . The density of a trajectory, ρ, only depends
on the goal states, sg0, sg1, ..., sgT.
3.2.2	Density Estimation
After the agent collected a number of trajectories, we can fit the density model. The density model
we use here is the Variational Gaussian Mixture Model (V-GMM) as introduced in Section 2.3.
The V-GMM fits on the data in the memory buffer every epoch and refreshes the density for each
trajectory in the buffer. During each epoch, when the new trajectory comes in, the density model
predicts the density ρ based on the achieved goals of the trajectory as:
K
P = V-GMM(T) = X CkN(T|〃k, Σk)	(1)
k=1
where T = (s0gks1gk...ksgT) and each trajectory T has the same length. We normalize the trajectory
densities using
ρi
Pi = PN	(2)
n=1 ρn
where N is the number of trajectories in the memory buffer. Now the density P is between zero and
one, i.e. 0 ≤ P ≤ 1, After calculating the trajectory density, the agent stores the density value along
with the trajectory in the memory buffer for later prioritization.
3.2.3	Prioritization
During replay, the agent puts more focus on the under-represented achieved states and prioritizes the
according trajectories. These under-represented achieved goal states have lower trajectory density.
We defined the complementary trajectory density as:
P H 1 一 ρ.
(3)
When the agent replays the samples, it first ranks all the trajectories with respect to their comple-
mentary density values ρ, and then uses the ranking number (starting from zero) directly as the
probability for sampling. This means that the low-density trajectories have high ranking numbers,
and equivalently, have higher priorities to be replayed. Here we use the ranking instead of the density
directly. The reason is that the rank-based variant is more robust because it is not affected by outliers
nor by density magnitudes. Furthermore, its heavy-tail property also guarantees that samples will
be diverse (Schaul et al., 2015b). Mathematically, the probability ofa trajectory to be replayed after
the prioritization is:
) =	rank(P(Ti))
"	PN=I rank((P(Tn))
where N is the total number of trajectories in the buffer, and rank(∙) ∈ {0,1,..., N 一 1}.
(4)
4
Under review as a conference paper at ICLR 2019
3.2.4	Complete Algorithm
We summarize the complete training algorithm in Algorithm 1.
Algorithm 1 Curiosity-Driven Prioritization (CDP)
Given:
•	an off-policy RL algorithm A	. e.g. DDPG, DDPG+HER
•	a reward function r : S × A × G → R.	. e.g. r(s, a, g) = -1 (fail), 0 (success)
Initialize neural networks of A, density model V-GMM, and replay buffer R
for epoch = 1, M do
for episode = 1, N do
Sample a goal g and an initial state s0 .
Sample a trajectory τ = (stkg, at, rt, st+1 kg)tT=0 using πb from A
Calculate the densities P and P using Equation (1), (2) and (3)	. estimate density
Calculate the priority p(τ) using Equation (4)
Store transitions (Stkg, at,rt, st+ιkg,P, ρ)T=o in R
Sample trajectory τ from R based on the priority, p(τ)	. prioritization
Sample transitions (St, at, St+1) from τ
Sample virtual goals g0 ∈ {St+1, ..., ST -1} at a future timestep in τ
rt0 := r(St, at, g0)	. recalculate reward (HER)
Store the transition (Stkg0, at, r0, st+ιkgl,P, P) in R
Perform one step of optimization using A
end for
Train the density model using the collected trajectories in R	. fit density model
Update the density in R using the trained model	. refresh density
end for
3.3	An Importance Sampling Perspective
The mathematical explanation for the efficiency of CDP is based on importance sampling. Im-
portance sampling is a general technique to estimate an integral f (x)p(x)dx of a function f (x),
with the exact distribution p(x) (Murphy, 2012; Owen, 2013). Here, we consider using importance
sampling to estimate the integral of the loss function L(τ) of the reinforcement learning agent:
I = E[L] = Z L(T)P(T)q(T)dτ ≈ W X ωif (Ti)= I,
q(τ)	N i=1
where T is a trajectory, q(T) is a proposal distribution, and ωi = p(Ti)/q(Ti) is an importance weight.
The idea here is to draw samples T from the buffer in regions which have a high probability, p(T),
but also where L|(T)| is large. Since, p(T) is a uniform distribution, i.e. the agent replays trajectories
at random, we only need to draw samples which has large errors L|(T)|. The result can be highly
efficient, meaning the agent needs less samples than sampling from the uniform distribution p(T).
The CDP framework finds the samples that have large errors based on the ‘surprise’ of the trajectory.
The variance of the estimate I is: Varq[L(τ)ω(τ))] = Eq[L2(τ)ω2(τ)] - I2. Since the last term is
independent of q, we can ignore it. Using Jensen’s inequality, we have the following lower bound:
Eq[L2(τ)ω2(τ)] > (Eq[∣L(τ)ω(τ)|])2.
To reduce the variance, we set the importance weight as a constant ω(T) = 1. This bias-variance
trade-off also saves computational time and does not lead to instabilities in our experiment. With
CDP, the agent estimates the loss function more efficiently and therefore learns faster. Any density
estimation method that can approximate the trajectory density can provide a more efficient proposal
distribution q(T) than the uniform distribution p(T).
3.4	Comparison with Prioritized Experience Replay
To the best our knowledge, the most similar method to CDP is Prioritized Experience Replay (PER)
(Schaul et al., 2015b). To combine PER with HER, we calculate the TD-error of each transition
5
Under review as a conference paper at ICLR 2019
FetchPush-v0
0	50	100	150	200
Figure 2: Mean test success rate with standard deviation in all six robot environments
FetchPickAndPlace-v0
0	50	100	150	200
HandManipulateBlockFull-v0
0	50	100	150	200
Epoch
FetchSlide-v0
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
HandManipulatePenRotate-v0
0	50	100	150	200
based on the randomly selected achieved goals. Then we prioritize the transitions with higher TD-
errors for replay. It is known that PER can become very expensive in computational time (Schaul
et al., 2015b), especially when the memory size N is very large. The reason is that PER uses TD-
errors for prioritization. After each update of the model, the agent needs to update the priorities of
the transitions in the replay buffer, which is O(log N). In our experiments, see Section 4, we use the
efficient implementation based on the ”sum-tree” data structure, which can be relatively efficiently
updated and sampled from (Schaul et al., 2015b).
Compared to PER, CDP is much faster in computational time because it only updates the trajectory
density once per epoch. Due to this reason, CDP is much more efficient than PER in computational
time and can be easily combined with any multi-goal RL methods, such as DDPG and HER. In
the experiments, Section 4, we first compare the performance improvement of CDP and PER. After-
wards, we compare the time-complexity of PER and CDP. We show that CDP improves performance
with much less computational time than PER. Furthermore, the motivations of PER and CDP are
different. The former uses TD-errors, while the latter is based on the density of the trajectories.
4	Experiments
In this section, we investigate the following questions:
-	Does incorporating CDP bring benefits to DDPG or DDPG+HER?
-	Does CDP improve the sample-efficiency in robotic manipulation tasks?
-HoW does the density P relate to the TD-errors of the trajectory during training?
Performance: To test the performance difference among DDPG, DDPG+PER, and DDPG+CDP,
We run the experiment in the three robot arm environments. We use the DDPG as the baseline here
because the robot arm environment is relatively simple. In the more challenging robot hand environ-
ments, We use DDPG+HER as the baseline method and test the performance among DDPG+HER,
DDPG+HER+PER, and DDPG+HER+CDP.
We compare the mean success rates. Each experiment is carried out across 5 random seeds and the
shaded area represents the standard deviation. The learning curve With respect to training epochs
is shoWn in Figure 2. For all experiments, We use 19 CPUs and train the agent for 200 epochs.
After training, We use the best-learned policy as the final policy and test it in the environment. The
testing results are the final mean success rates. A comparison of the final performances along With
the training time is shoWn in Table 1.
6
Under review as a conference paper at ICLR 2019
Table 1: Final mean success rate (%) and the training time (hour) for all six environments
Method	Push		Pick & Place		Slide	
	success	time	success	time	success	time
DDPG	99.90%	5.52h	39.34%	5.61h	75.67%	5.47h
DDPG+PER	99.94%	30.66h	67.19%	25.73h	66.33%	25.85h
DDPG+CDP	99.96%	6.76h	76.02%	6.92h	76.77%	6.66h
	Egg		Block			Pen
Method	success	time	success	time	success	time
DDPG+HER	76.19%	7.33h	20.32%	8.47h	27.28%	7.55h
DDPG+HER+PER	75.46%	79.86h	18.95%	80.72h	27.74%	81.17h
DDPG+HER+CDP	81.30%	17.00h	25.00%	19.88h	31.88%	25.36h
FetchSlide-v0
0.2	0.4	0.6	0.8
1e5 HandManipulatePenRotate-v0
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Mean Success Rate
s-dEeS CT⊂-⊂-ratl⅛
Figure 3:	Number of training samples needed with respect to mean test success rate for all six
environments (the lower the better)
From Figure 2, we can see that CDP converges faster in all six tasks than both the baseline and PER.
The agent trained with CDP also shows a better performance at the end of the training, as shown in
Table 1. In Table 1, we can see that the training time of CDP lies in between the baseline and PER.
To be more specific, CDP consumes much less computational time than PER does. For example in
the robot arm environments, on average DDPG+CDP consumes about 1.2 times the training time of
DDPG. In comparison, DDPG+PER consumes about 5 times the training time as DDPG does. In
this case, CDP is 4 times faster than PER.
Table 1 shows that baseline methods with CDP give a better performance in all six tasks. The
improvement goes up to 39.34 percentage points compared to the baseline methods. The average
improvement over the six tasks is 9.15 percentage points. We can see that CDP is a simple yet
effective method, improves state-of-the-art methods.
Sample-Efficiency: To compare the sample-efficiency of the baseline and CDP, we compare the
number of training samples needed for a certain mean test success rate. The comparison is shown in
Figure 3. From Figure 3, in the FetchPush-v0 environment, we can see that for the same 99%
mean test success rate, the baseline DDPG needs 273,600 samples for training, while DDPG+CDP
only needs 112,100 samples. In this case, DDPG+CDP is more than twice (2.44) as sample-efficient
as DDPG. Similarly, in the other five environments, CDP improves sample-efficiency by factors of
2.84, 0.92, 1.37, 1,28 and 2.87, respectively. In conclusion, for all six environments, CDP is able to
improve sample-efficiency by an average factor of two (1.95) over the baseline’s sample-efficiency.
7
Under review as a conference paper at ICLR 2019
HandManipulateBlockFuII-VO
pearsonr=0.76
125
0.0000∞.000050.0001 ∞.000150.00020
CompIementaryTrajeCtoryDenSity
Figure 4:	Pearson correlation between the density P and TD-errors in the middle of training
Insights: We also investigate the correlation between the complementary trajectory density P and
the TD-errors of the trajectory. The Pearson correlation coefficient, i.e. Pearson’s r (Benesty et al.,
2009), between the density P and the TD-errors of the trajectory is shown in Figure 4. The value of
Pearson’s r is between 1 and -1, where 1 is total positive linear correlation, 0 is no linear correlation,
-1 is total negative linear correlation. In Figure 4, we can see that the complementary trajectory
density is correlated with the TD-errors of the trajectory with an average Pearson’s r of 0.7. This
proves that the relatively rare trajectories in the memory buffer are more valuable for learning.
Therefore, it is helpful to prioritize the trajectories with lower density during training.
5	Related Work
Experience replay was proposed by Lin (1992) and became popular due to the success of DQN
(Mnih et al., 2015). In the same year, prioritized experience replay was introduced by Schaul et al.
(2015b) as an improvement of the experience replay in DQN. It prioritized the transitions with higher
TD-error in the replay buffer to speed up training. Schaul et al. (2015a) also proposed universal func-
tion approximators, generalizing not just over states but also over goals. There are also many other
research works about multi-task RL (Schmidhuber & Huber, 1990; Caruana, 1998; Da Silva et al.,
2012; Kober et al., 2012; Pinto & Gupta, 2017; Foster & Dayan, 2002; Sutton et al., 2011). Hind-
sight experience replay (Andrychowicz et al., 2017) is a kind of goal-conditioned RL that substitutes
any achieved goals as real goals to encourage the agent to learn something instead of nothing.
Curiosity-driven exploration is a well-studied topic in reinforcement learning (Oudeyer & Kaplan,
2009; Oudeyer et al., 2007; Schmidhuber, 1991; 2010; Sun et al., 2011). Pathak et al. (2017) en-
courage the agent to explore states with high prediction error. The agents are also encouraged to
explore ”novel” or uncertain states (Bellemare et al., 2016; Lopes et al., 2012; Poupart et al., 2006;
Houthooft et al., 2016; Mohamed & Rezende, 2015; Chentanez et al., 2005; Stadie et al., 2015).
However, we integrate curiosity into prioritization and tackle the problem of data imbalance (Galar
et al., 2012) in the memory buffer of RL agents. A recent work (Narasimhan et al., 2015) intro-
duced a form of re-sampling for RL agents based on positive and negative rewards. The idea of our
method is complementary and can be combined. The motivation of our method is from the curiosity
mechanism in the human brain (Gruber et al., 2014). The essence of our method is to assign priority
to the achieved trajectories with lower density, which are relatively more valuable to learn from.
In supervised learning, similar tricks are used to mitigate the class imbalance challenge, such as
over-sampling the data in the under-represented class (Hinton, 2007; He & Garcia, 2008).
6	Conclusion
In conclusion, we proposed a simple yet effective curiosity-driven approach to prioritize agent’s
experience based on the trajectory density. Curiosity-Driven Prioritization shows promising experi-
mental results in all six challenging robotic manipulation tasks. This method can be combined with
any off-policy RL methods, such as DDPG and DDPG+HER. We integrated the curiosity mech-
anism via density estimation into the modern RL paradigm and improved sample-efficiency by a
factor of two and the final performance by nine percentage points on top of state-of-the-art methods.
8
Under review as a conference paper at ICLR 2019
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. In
Noise reduction in speech processing, pp. 1-4. Springer, 2009.
David M Blei, Michael I Jordan, et al. Variational inference for dirichlet process mixtures. Bayesian
analysis, 1(1):121-143, 2006.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.
Rich Caruana. Multitask learning. In Learning to learn, pp. 95-133. Springer, 1998.
Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, and Sergey Levine.
Path integral guided policy search. In Robotics and Automation (ICRA), 2017 IEEE International
Conference on, pp. 3381-3388. IEEE, 2017.
Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement
learning. In Advances in neural information processing systems, pp. 1281-1288, 2005.
Bruno Da Silva, George Konidaris, and Andrew Barto. Learning parameterized skills. arXiv preprint
arXiv:1206.6398, 2012.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pp. 248-255. Ieee, 2009.
Richard O Duda and Peter E Hart. Pattern classification and scene analysis. A Wiley-Interscience
Publication, New York: Wiley, 1973, 1973.
Pedro Felzenszwalb, David McAllester, and Deva Ramanan. A discriminatively trained, multiscale,
deformable part model. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE
Conference on, pp. 1-8. IEEE, 2008.
David Foster and Peter Dayan. Structure in the space of value functions. Machine Learning, 49
(2-3):325-346, 2002.
Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera.
A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based
approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews), 42(4):463-484, 2012.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Matthias J Gruber, Bernard D Gelman, and Charan Ranganath. States of curiosity modulate
hippocampus-dependent learning via the dopaminergic circuit. Neuron, 84(2):486-496, 2014.
Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on Knowledge
& Data Engineering, (9):1263-1284, 2008.
Geoffrey E Hinton. To recognize shapes, first learn to generate images. Progress in brain research,
165:535-547, 2007.
9
Under review as a conference paper at ICLR 2019
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems ,pp.1109-1117, 2016.
Jens Kober, Andreas Wilhelm, Erhan Oztop, and Jan Peters. Reinforcement learning to adjust
parametrized motor primitives to new situations. Autonomous Robots, 33(4):361-379, 2012.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 8(3-4):293-321, 1992.
Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-
based reinforcement learning by empirically estimating learning progress. In Advances in Neural
Information Processing Systems, pp. 206-214, 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-
cally motivated reinforcement learning. In Advances in neural information processing systems,
pp. 2125-2133, 2015.
Kevin P Murphy. Machine learning: A probabilistic perspective. adaptive computation and machine
learning, 2012.
Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based
games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.
Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and
Eric Liang. Autonomous inverted helicopter flight via reinforcement learning. In Experimental
Robotics IX, pp. 363-372. Springer, 2006.
Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computa-
tional approaches. Frontiers in neurorobotics, 1:6, 2009.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-
tonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286,
2007.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning (ICML), volume
2017, 2017.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682-697, 2008.
Lerrel Pinto and Abhinav Gupta. Learning to push by grasping: Using multiple tasks for effective
learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 2161-
2168. IEEE, 2017.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv preprint
arXiv:1802.09464, 2018.
10
Under review as a conference paper at ICLR 2019
Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete
bayesian reinforcement learning. In Proceedings of the 23rd international conference on Ma-
chine learning,pp. 697-704. ACM, 2006.
Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of
Mathematical Statistics, pp. 832-837, 1956.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International Conference on Machine Learning, pp. 1312-1320, 2015a.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015b.
Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neu-
ral controllers. In Proc. of the international conference on simulation of adaptive behavior: From
animals to animats, pp. 222-227, 1991.
Jurgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE
Transactions on Autonomous Mental Development, 2(3):230-247, 2010.
Jurgen Schmidhuber and Rudolf Huber. Learning to generate focus trajeCtoriesfor attentive vision.
Institut fur Informatik, 1990.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Yi Sun, Faustino Gomez, and Jurgen Schmidhuber. Planning to be surprised: Optimal bayesian
exploration in dynamic environments. In International Conference on Artificial General Intelli-
gence, pp. 41-51. Springer, 2011.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and
Multiagent Systems-Volume 2, pp. 761-768. International Foundation for Autonomous Agents
and Multiagent Systems, 2011.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
11