Under review as a conference paper at ICLR 2019
DADAM: A consensus-based distributed adap-
tive gradient method for Online Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Online and stochastic optimization methods such as SGD, ADAGRAD and
ADAM are key algorithms in solving large-scale machine learning problems in-
cluding deep learning. A number of schemes that are based on communications
of nodes with a central server have been recently proposed in the literature to
parallelize them. A bottleneck of such centralized algorithms lies on the high
communication cost incurred by the central node. In this paper, we present a
new consensus-based distributed adaptive moment estimation method (DADAM)
for online optimization over a decentralized network that enables data paralleliza-
tion, as well as decentralized computation. Such a framework not only can be
extremely useful for learning agents with access to only local data in a communi-
cation constrained environment, but as shown in this work also outperform cen-
tralized adaptive algorithms such as ADAM for certain realistic classes of loss
functions. We analyze the convergence properties of the proposed algorithm and
provide a dynamic regret bound on the convergence rate of adaptive moment es-
timation methods in both stochastic and deterministic settings. Empirical results
demonstrate that DADAM works well in practice and compares favorably to com-
peting online optimization methods.
1	Introduction
Online optimization is a fundamental procedure for solving a wide range of machine learning prob-
lems Shalev-Shwartz et al. (2012); Hazan et al. (2016). It can be formulated as a repeated game
between a learner (algorithm) and an adversary. The learner receives a streaming data sequence,
sequentially selects actions, and the adversary reveals the convex or nonconvex losses to the learner.
A standard performance metric for an online algorithm is regret, which measures the performance
of the algorithm versus a static benchmark Zinkevich (2003); Hazan et al. (2016). For example, the
benchmark could be an optimal point of the online average of the loss (local cost) function, had the
learner known all the losses in advance. In a broad sense, if the benchmark is a fixed sequence, the
regret is called static. Recent work on online optimization has investigated the notion of dynamic
regret Zinkevich (2003); Hall & Willett (2015); Besbes et al. (2015). Dynamic regret can take the
form of the cumulative difference between the instantaneous loss and the minimum loss. For convex
functions, previous studies have shown that the dynamic regret of online gradient-based methods can
be upper bounded by O(√TDT), where DT is a measure of regularity of the comparator sequence
or the function sequence Zinkevich (2003); Hall & Willett (2015). This bound can be improved to
O(DT) Mokhtari et al. (2016); Zhang et al. (2017) when all the cost functions are strongly convex
and smooth.
Decentralized nonlinear programming has received a great deal of interest in diverse scientific and
engineering fields Tsitsiklis et al. (1986); Li et al. (2002); Rabbat & Nowak (2004); Lesser et al.
(2012). The key problem involves optimizing a cost function f (ɪ) = ɪ ∑3 f (X), where X ∈ RP
and each fi is only known to the individual agent i in a connected network of n agents. The agents
collaborate by successively sharing information with other agents located in their neighborhood with
the goal of jointly converging to the network-wide optimal argument Nedic & Ozdaglar (2009).
Compared to optimization procedures involving a fusion center that collects data and performs the
computation, decentralized nonlinear programming enjoys the advantage of scalability to network
sizes, robustness to dynamic topologies, and privacy preservation in data-sensitive applications Yuan
et al. (2016); Shi et al. (2015); Jiang et al. (2017); Lian et al. (2017).
1
Under review as a conference paper at ICLR 2019
Appropriately choosing the learning rate that scale coordinates of the gradient and the way of up-
dating them are crucial issues driving the performance of first Duchi et al. (2011); Kingma & Ba
(2014) and second order optimization procedures Peyghami & Tarzanagh (2015). Indeed, the under-
standing that adaptation of the learning rate is advantageous, particularly on a per parameter basis
dynamically, led to the development of a family of widely-used adaptive gradient methods including
ADAGRAD Duchi et al. (2011), RMSPROP Tieleman & Hinton, and ADAM Kingma & Ba (2014).
The ADAM optimizer computes adaptive learning rates for different parameters from estimates of
first and second moments of the gradients and performs a local optimization. Numerical results show
that ADAM can achieve significantly better performance compared to ADAGRAD, RMSPROP and
other gradient descent procedures when the gradients are sparse, or in general small in magnitude.
However, its performance has been observed to deteriorate in settings where the loss functions are
nonconvex and gradients are dense. Further, there is currently a gap in the theoretical understanding
of these methods, especially in the nonconvex and stochastic setting Ward et al. (2018); Zeng & Yin
(2018).
1.1	Contents and Contributions
In this paper, we develop and analyze a new consensus-based distributed adaptive moment esti-
mation (DADAM) method that incorporates decentralized optimization and uses a variant of the
adaptive moment estimation methods Duchi et al. (2011); Kingma & Ba (2014); Tieleman & Hin-
ton; McMahan & Streeter (2010). Existing distributed stochastic and adaptive gradient methods for
deep learning are mostly designed for central network topology Dean et al. (2012); Li et al.. The
main bottleneck of such a topology lies on the communication overload on the central node, since all
nodes need to concurrently communicate with it. Hence, performance can be significantly degraded
when network bandwidth is limited. These considerations motivate us to study an adaptive algorithm
for network topologies, where all nodes can only communicate with their neighbors and none of the
nodes is designated as “central”. Therefore, the proposed method is suitable for large scale machine
learning problems, since it enables both data parallelization and decentralized computation.
Next, we briefly summarize the main technical contributions of the work.
-	Our first main result (Theorem 4) provides guarantees of DADAM for constrained convex
minimization problems defined over a closed convex set X . We provide the convergence
bound in terms of dynamic regret and show that when the data features are sparse and
have bounded gradients, our algorithm’s regret bound can be considerably better than the
ones provided by standard mirror descent and gradient descent methods Nedic & Ozdaglar
(2009); Hall & Willett (2015); Besbes et al. (2015). It is worth mentioning that the regret
bounds provided for adaptive gradient methods Duchi et al. (2011) are static and our results
generalize them to dynamic settings.
-	In Theorem 5, we give a new local regret analysis for distributed gradient-based algorithms
for nonconvex minimization problems computed over a network of agents. Specifically, we
prove that under certain regularity conditions, DADAM can achieve a local regret bound of
order O(T) for nonconvex distributed optimization. To the best of our knowledge, rigor-
ous extensions of existing adaptive gradient methods to the distributed nonconvex setting
considered in this work do not seem to be available.
-	In this paper, we also present regret analysis for distributed stochastic optimization prob-
lems computed over a network of agents. Theorems 8 and 10 provide regret bounds of
DADAM for minimization problem equation 3 with stochastic gradients and indicate that
the result of Theorems 4 and 5 holds true in expectation. Further, in Corollary 11 we
ξ2
show that DADAM can achieve a local regret bound of order O(√nT + T) for noncon-
vex distributed stochastic optimization where ξ is an upper bound on the variance of the
stochastic gradient. Hence, DADAM outperforms centralized adaptive algorithms such as
ADAM for certain realistic classes of loss functions when T is sufficiently large.
The remainder of the paper is organized as follows. Section 2 gives a detailed description of
DADAM, while Section 3 establishes its theoretical results. Section 4 explains a network correction
technique for our proposed algorithm. Section 5 illustrates the proposed framework on a number of
synthetic and real data sets. Finally, Section 6 concludes the paper.
2
Under review as a conference paper at ICLR 2019
The detailed proofs of the main results established are delegated to the Appendix.
1.2	Mathematical Preliminaries and Notations.
Throughout the paper, Rp denotes the real coordinate space of p dimensions. For any pair of vectors
x,y ∈ Rp, hx, yi indicates the standard Euclidean inner product. We denote the `1 norm by kXk1 =
∑ij |xij|, the infinity norm by kXk∞
maxi j |xij|, and the Euclidean norm by kXk =	∑ij |xij|2. The
above norms reduce to the vector norms if X is a vector. The diameter of the set X is given by
γ∞ = sup kx - yk∞
x,y∈X
(1)
Let S+p be the set of all positive definite p × p matrices. ΠX,A [x] denotes the Euclidean projection
of a vector x onto X for A ∈ S+p :
∏X,x [H= argminkA2(%-J)k.
y∈X
The subscript t is often used to denote the time step while Ji,t,d stands for the d-th element of Ji,t .
Further, Ji,1:t,d ∈ Rt is given by
Ji,1:t,d = [Ji,1,d , Ji,2,d, . . . ,Ji,t,d] .
We let gi,t to denote the gradient of f at %i,t . The i-th largest singular value of matrix X is denoted
by σi(X). We denote the element in the i-th row and j-th column of matrix X by [X]ij. In several
theorems, we consider a connected undirected graph G = (V , E ) with nodes V = {1, 2, . . . ,n} and
edges E . The matrix W ∈ Rn×n is often used to denote a symmetric mixing matrix Shi et al. (2015)
of graph G . Indeed, for any agent i, we assign a positive weight [W]ij for the information received
from agent j 6= i so that [W]ij > 0 if and only if (i, j) ∈ E and
nn
∑[W]ij=∑[W]ij=1.	(2)
i=1	j=1
The role of W is the similar as that in decentralized gradient descent methods Yuan et al. (2016) and
average consensus Xiao et al. (2007). It has a few common choices, which can significantly affect
performance (see, Shi et al. (2015)). Throughout this paper, we consider Metropolis constant edge
weight matrix Xiao & Boyd (2004); Xiao et al. (2007) defined as follows
{max{deg(i),deg(j)}+ε , if (i, j) ∈ E,
0,	if (i, j) ∈/ E and i 6= j,
1 - ∑ [IV]ik,	if i = j,
k∈V
for some small positive ε > 0. When W is chosen according this scheme, W = I42^ is found to
be very efficient. Note, the above doubly stochastic matrix implies uniqueness of σ1 ( V) = 1 and
warrants that other singular value ofW are strictly less than one in magnitude.
The Hadamard (entrywise) and Kronecker product are denoted by Θ and ⑼ respectively. Finally,
the expectation operator is denoted by E.
2 Problem Formulation and Algorithm
In this section, we propose a new optimization method for online optimization that employs data
parallelization and decentralized computation over a network of agents. In our new structure, given
a connected undirected graph G = (V , E), we let each node i ∈ V at time t ∈ {1, . . . , T} hold its
own measurement and training data mi, and set fi,t (X) = * ∑：= 1 fijt (X). We also let each agent i hold
a local copy of the global variable % at time t ∈ {1, . . . , T}, which is denoted by %i,t ∈ Rp. With this
setup, we present a distributed adaptive gradient method for solving the minimization problem
1Tn
minimize F (X) = ∑∑ fi, t(X),	(3)
X∈X	n t=1 i=1
3
Under review as a conference paper at ICLR 2019
where fi,t : X → R is a continuously differentiable mapping on the convex set X .
It is known that it is impossible to achieve a sub-linear dynamic regret bound Zhang et al. (2017),
because of the arbitrary fluctuations in problem equation 3. We want to characterize the hardness of
the problem via a complexly measure that captures the pattern of the minimizer sequence {x*}3,
where Xt = argminX∈χ f (X). Subsequently, We would like to provide a regret bound in terms of
T-1
DT,d = ∑ |Xtt+1,d-Xtt,d|	for d ∈ {1,..., p},	(4)
t =1
which represents the variations in {Xtt}tT=1.
DADAM uses a new distributed adaptive gradient method in which a group of n agents seeking to
solve a sequential version of problem equation 3. Here, we assume that each component function
fi,t : X → R becomes only available to agent i ∈ V , after having made its decision at time t ∈
{1, . . . , T}. In the t-th, the i-th agent chooses a point Xi,t corresponding to what it considers the
network as a whole should have selected. After committing to this choice, the agent has access to
a cost function f7 : X → R and the network cost is then given by f (X) = ɪ ∑3 fi,t(X). Note that
this function is not known to any of the agents and is not available at any single location.
The procedure of our proposed method is outlined in Algorithm 1.
Algorithm 1: A distributed adaptive moment estimation method (DADAM)
input : X1 ∈ X, a positive sequence {αt}tT=1, exponential decay rates for the moment
estimate β1,β2,β3 ∈ [0, 1), and a mixing matrix W satisfying equation 2;
ι for all i ∈ V, initialize moment vectors mi,o = Ui,o = Ui,o = 0 and Xi,ι = Xι;
2	for i ∈ V do
3	for t - 1 to T do
4	gi, t = V fi, t (Xi,t);
5	mi,t = β1mi,t-1 + (1 -β1)gi,t;
6	Ui,t = β2Ui,t-1 + (1 - β2)gi,t gi,t;
7	Ui,t = 03Ui,t + (1 — 03)max(Ui,t-i, U邙);
8	Xi, t +1 = ∑n =iW]ijXj,t；
9	_ Xi,t +1 = πX,√diag(Uit) [Xi,t + 2 - α √⅛];
output: resulting parameter X = ɪ ∑n=ι Xi,t+ι
It is worth mentioning that DADAM computes adaptive learning rates from estimates of both first
and second moments of the gradients similar to Duchi et al. (2011); Tieleman & Hinton; Kingma
& Ba (2014); Reddi et al. (2018). However, DADAM uses a larger learning rate in comparison to
AMSGrad Reddi et al. (2018) and yet incorporates the intuition of slowly decaying the effect of
previous gradients on the learning rate. The key difference of DADAM with ADAM and AMSGrad
is that it maintains the maximum of all second moment estimates of the gradient vectors until time
step t and uses
Ui,t = 03 Ui,t + (1 — 03) max( Ui,t7, Ui,t),
for normalizing the running average of the gradient instead of Ui-,t in ADAM and max(Ui∙,t-1, U印)
in AMSGrad. The learning rate Ui,t is an important component of the DADAM framework, since
it enables us to develop a convergent adaptive method similar to AMSGrad while maintaining the
efficiency of ADAM.
Next, we provide the notion of regret which measures the performance of DADAM against a se-
quence of local minimizers.
In the framework of online convex optimization, the performance of algorithms is assessed by regret
that measures how competitive the algorithm is with respect to the best fixed solution MateOS-NUnez
& COrteS (2014); Hazan et al. (2016). However, the notion of regret fails to illustrate the performance
of online algorithms in a dynamic setting. To overcome this issue, we consider a more stringent
4
Under review as a conference paper at ICLR 2019
metric-dynamic regret Hall & Willett (2015); Besbes et al. (2015); Zinkevich (2003), in which the
cumulative loss of the learner is compared against the minimizer sequence {x*}3, i.e.,
1nT	T
Regr := - ∑∑ fi,t(Xi,t) - ∑ ft(X)
n i=1t=1	t=1
where x： = argminX∈χ f (x).
On the other hand, in the framework of nonconvex optimization, it is usual to state convergence
guarantees of an algorithm towards an ε-approximate stationary point-that is, there exist some iter-
ates Xi,t for which ∣∣Vf (Xi,t)k ≤ ε. Inspired by Hazan et al. (20l7), We next provide the definition
of projected gradient and introduce local regret, a new notion of regret which quantifies the moving
average of gradients over network.
Definition 1. (Local Regret). Assume fi : X → R is a differentiable function on a closed conveX set
X ⊆ Rp. Given a step-size α > 0, we define GX (X, fi, α) : X → Rp the projected gradient offi at
X,by
√	√	∖HSi.	∣.
GX (X, fi, α) = ^^α~ (X - Xi ),	∀i ∈ V,	(5)
where
X+ = argmin{h y, m i + ɪ Il y - ∑ W ] ijXj k2}.	(6)
y∈X	υi	2α	j=1
Then, the local regret of an online algorithm is given by
1n
Red := ∑ min ∣GX(x*, f,o⅞)k2,
n i=1 t ∈{1,...,T}
where f,t (Xi, t) = 1 ∑ S=ι fi,s (Xi, t) is an aggregate loss.
We analyze the convergence of DADAM as applied to minimization problem equation 3 using re-
grets RegrT and RegTN . It is worth to mention that DADAM is initialized at Xi,t = 0 to keep the
presentation of the convergence analysis clear. In general, any initialization can be selected for
implementation purposes.
3 Convergence Analysis
In this section, our aim is to establish convergence properties of DADAM under the following as-
sumptions:
Assumption 2. For all i ∈ V and t ∈ {1, . . . , T}, the function fi,t (X) is continuously differentiable
over X , and has Lipschitz continuous gradient on this set, i.e. there eXists a constant ρ ≥ 0 so that
∣V fi,t (X) - V fi,t (y)∣ ≤ρ∣X-y∣,	∀X,y∈ X .
Further, fit(∙) is Lipschitz continuous on X with a uniform constant L ≥ 0, i.e.,
| fi,t (X) -fi,t(y)| ≤L∣X-y∣,	∀X,y ∈ X .	(7)
There are many cost functions fi,t that satisfy this type of Lipschitz condition. For example, it holds
for any convex function on a compact set X, or for any polyhedral function on an arbitrary domain
Duchi et al. (2011).
Assumption 3. For all i ∈ V and t ∈ {1, . . . , T}, the stochastic gradient, as notation we use bold
letters g = ▽ fi,t (x" ), satisfies
E [▽ fi,t(Xi,t)∣Ft-i] = Vfi,t(Xi,t),	E h∣Vfi,t(Xi,t)k2∣FI-ii ≤ ξ2,
where Ft is the ξ -field containing all information prior to the outset of round t + 1.
5
Under review as a conference paper at ICLR 2019
3.1	Convex Case
Next, we focus on the case where for all i ∈ V and t ∈ {1, . . . , T}, the agent i at time t has access to
the exact gradient gi,t = VfH (Xi,t). The following results apply to convex problems, as well as their
stochastic variants in a dynamic environment.
Theorem 4. Suppose that the parameters β1,β2 ∈ [0,1) satisfy η = -√= < 1. Let β1,t = β1λt-1, λ ∈
(0,1) and ∣∣Vfi,t(X)k∞ ≤ G∞ forall t ∈ {1,..., T}. Then, using a step-size at = √ for the Sequence
xi,t generated by Algorithm 1, we have
RegCT ≤
(I-βI)α√1+IogT 6 H HI 6	G∞γ∞
2√n√(i-β2)=kg 1:T,dk + =(1-β1)(1-λ)
l γ ‰(‰ + DT,d) Γ^~ l
+ ∑1 √(1-β1)α TυυT,d +
4α √1 + logT ∑ d J g t T, d k
(I - 02”力 VZ(I - βI) VZ(I - η) VZ(I - β2)
Next, we analyze the stochastic convex setting and extend the result of Theorem 4 to the noisy case
where agents have access to stochastic gradients of the objective function equation 3.
Theorem 5. Suppose that Assumption 3 holds. Further, the parameters β1, β2 ∈ [0, 1) satisfy η =
-√= < 1. Let β1,t = β1λt-1, λ ∈ (0,1). Then, using a step-size at = √ for the sequence Xi,t
generated by Algorithm 1, we have
E Reg多]≤ O-β1p√1 +ogT ∑ E [∣gLT,dk] + ∑	ξγ∞
2 Vn Vz(I-β2)	d=1	d=I(I-β1)(1 - λ)
+ ∑∑ γ∞(γ∞ + DT,d) √TE h [υ-i +	4α√T+l°gT∑p=1E [kgγ,d∣]
+d∑1 √(1-β1)α V [V T,d∖ +(1-0而PfFp(EPT-两∙
Remark 6. Theorems 4 and 5 show that the regret bound of DADAM can be considerably better than
the ones provided by standard mirror descent and gradient descent methods for both centralized and
decentralized settings Zinkevich (2003); Hall & ”illett (2015); Besbes et al. (2015); Shahrampour
& Jadbabaie (2018) because
∑ k g 1： t , d k=∑ qg 2, d+g 2, d+	+ g t , d ≤ ∑ qG ∞+G ∞ +	+G ∞
d=1	d=1	d=1
P
=∑ TGG∞ = pG∞ VT,
d=1
p
≤ E TGg∞ ≤ PG∞ VT.
d=1
It is easy to show that the regret of DADAM is upper bounded by O (G-IDG”) where DT =
maxd∈{1,.∙∙,P} Dt,d. Indeed, the term Z = |gi,t,d ∣∕√t in the proof of Lemma 14 can be bounded by
O (G ∞ √G) instead of O (G ∞√ T log T). Hence, the regret of DADAM is upper bounded by minimum
of O(G-D”T ) and the bound presented in Theorems 4 and 5, and thus the worst case dependence
on T is √G rather than √ T log T.
Remark 7. ”e note that in the static setting, i.e. DT = 0, the regret of DADAM is upper bounded
by
O(
G ∞ a )
1 - σ2(W)
where 1 - σ2(W) is the spectral gap of the network.
6
Under review as a conference paper at ICLR 2019
3.2 Nonconvex Case
In this section, we provide convergence guarantees for DADAM for the nonconvex minimization
problem equation 3 defined over a closed convex set X . To do so, we use the projection map ΠX
instead of Π 鸳 √dag(^7j for updating parameters xb for t ∈{1,..., T}, i ∈ V (see, Algorithm 1 for
details).
Next, we derive an upper and lower bound for the second moment of gradient, υi,t . Assume that
there exists a positive number U such that
max gi,t,d ≤ U.
d ∈{1,..., p}
(8)
Now, using the update rule of Ui,t, we have Ui,t = (1 -β2) ∑tl=1 β2t-lgi2,l. This together with equation 8
imply that
Ei ≤ d ∈max, P}PUtd=d ∈max,p}β2 ∑ βt-端，d
≤ U

t
(i-β2)∑β2- ≤ U.
l=1
(9)
On the other hand, since Ui,t,d is non-decreasing and gi,t,d is required to be element-wise non-zero,
we have
Ui,t，d ≥ υ2 > 0,	(10)
for some constant U.
Now, using the update rule of Ui,t along with equation 10 and equation 9, We have
U2 ≤ β3U2 + (1 - β3)U2
≤ Ui,t = β3Ui,t + (1 - β3)max(U4-1, Ui,t)
≤ β3 U2 + (1-β3)U2 ≤ U2.	(11)
Theorem 8. Suppose that Assumption 2 holds. Further, the parameters β1, β2 ∈ [0, 1) satisfy η =
-√= < 1. Choose the positive sequence {at}T=I such that 0 < Ot ≤ Q"pU)U With Ot < Q-pU)U for
at least one t. Then, for the sequence xi,t generated by Algorithm 1, we have
RegN ≤ ɪ [(2 + log T)2L max ------2√n	∑ ∑ αSσ2-s-1(Ψ)
^⅛t	t∈{2,...,τ}(1-η)P(T-β2)∑0	2 ι J
+ ∑----------(Oβ1,tU .------],	(12)
+t∑2(1-β1,t)(1 -η)2(1-β2)],	( )
where Nt = ∑3[(2-fαt - PI].
The following corollary shows that DADAM using a certain step-size leads to a near optimal regret
bound for nonconvex functions.
Corollary 9. Under the same ConditionS of Theorem 8, using the step-sizes αt = %PUU and
β1,t = β1λt-1,λ ∈ (0, 1) for all t ∈ {1, . . . , T}, we have
RegN ≤ (
2U2	)1
(2 - β1)(1 - β1)(1 - η )2(1 - β2)(1 - λ))t
+
16 √n U L
(2- β1 )(1 - η)p(1 - β2)(I- σ2(W))
(2 + log T)
T
(13)
To complete the analysis of our algorithm in the nonconvex setting, we provide the regret bound for
DADAM, when stochastic gradients are accessible to the learner.
7
Under review as a conference paper at ICLR 2019
Theorem 10. Suppose that Assumption 2 holds. Further, the parameters β1,β2 ∈ [0, 1) satisfy
η = √√= < 1. Choose the positive Sequence {αt}T=1 such that 0 < α ≤ Q-pU)υ With α < Q-pU)υ
for at least one t. Then, for the sequence xi,t generated by Algorithm 1, we have
E 1Re党]≤ — [(2 + log T)2L max -2可	∑ αSσt-s-1(W)
L 」一 Nt	t ∈{2,...,t } (1 - η )P(1-β2) ∑o	2	''
+ ∑_______a⅛U_ + Jξ2 ∑ αtL
+t∑2(i - βι, t )(1 - η )2(i-β2)+ U2(i-β1)t∑ αt ],
where Nt = ∑3∣5 -翳].
(14)
3.2.1 When does DADAM Outperform ADAM?
We next theoretically justify the potential advantage of the proposed decentralized algorithm
DADAM over centralized adaptive moment estimation methods such as ADAM . More specifically,
the following corollary shows that when T is sufficiently large, the 1 term will be dominated by the
√nT term which leads to a √nT convergence rate.
Corollary 11. Suppose that Assumptions 2 and 3 hold. Moreover, the parameters β1,β2 ∈ [0, 1)
satisfy η = √β= < 1 and β1,t = β1λt-1, λ ∈ (0,1). Choose the step-size sequence as Ot
α = (2-pU)υ . Then, for the sequence xi,t generated by Algorithm 1, we have
√nTwith
E [RegT] ≤( U 28υ0β1))√ξnT+2(fI(X 1)- fI(X ")T'	(15)
if the total number of time steps T satisfies
T≥ (I1+I2),	(16a)
F、 r	4p2U2	4U2n 、	.....
≥	{ nU4(2 - β1)2, (2 - β1)2 },	(16b)
where
U 2	16 nL 2U4 (1 - β1)2
1 = 2(1-η )2(1-β2)(1-λ )ξ2,	2 = (1-η )2 (1-β2)(1-σ2(W ))2υ2ξ4 *
Let ε-approximation solution of equation 3 be defined by L T T ≤ ε. Corollary 11 indicates that
the total computational complexity of DADAM to achieve an ε -approximation solution is bounded
by O(*).The key setup which leads to this regret bound is that We do not use the boundedness
assumption for domain or gradient. These assumptions can simplify the proof but lose some of the
sophisticated structures in the problem.
4	An Extension of DADAM with a Corrected Update Rule
Compared to classical centralized algorithms, decentralized algorithms encounter more restrictive
assumptions and typically worse convergence rates. In order to improve the convergence rate, Shi
et al. (2015) introduced a corrected decentralized gradient method called EXTRA, which has a linear
rate of convergence if the objective function is strongly convex. Similar to EXTRA, we provide next
a corrected update rule for i ∈ V , given by
t-1 n
XC-+ADAM = xi,t+1 + ∑∑W-W]〃"S,	t = 0,1,.∙∙∙
s=0 j=1
、-------V-------}
correction
(17)
8
Under review as a conference paper at ICLR 2019
where xi,t+1 is generated by Algorithm 1.
Our numerical results on some test problems show the efficiency and effectiveness of the corrected
DADAM (C-DADAM) in practice. Note that a C-DADAM update is a DADAM update with a
cumulative correction term. The summation in equation 17 is necessary, since each individual term
∑ n=i\W - W ] ijXj, s is asymptotically vanishing and the terms must work cumulatively.
5	Numerical results
In this section, we present numerical results for Algorithm 1 (DADAM), on both synthetic and real
data sets. The employed data sets and the code will be made available on github upon acceptance
of this manuscript. We first present numerical results on a sparse binary classification problem,
comparing the performance of DADAM to that of standard decentralized gradient descent (DGD)
Nedic & Ozdaglar (2009); Yuan et al. (2016) and its corrected version EXTRA Shi et al. (2015).
In Section 5.2, we demonstrate the efficacy of DADAM in comparison with a decentralized ADAM
(Algorithm 1 with β3 = 1) and the recently proposed distributed federated averaging SGD (FedAvg)
algorithm on benchmark datasets such as MNIST and CIFAR-10. All algorithms have been imple-
mented in a Mac machine equipped with a 1.8 GHz Intel Core i5 processor and 8 GB 1600 MHz
DDR3 of memory.
5.1	Binary classification based on synthetic data in the static environment
Consider the following online distributed learning setting: at each time t , mi randomly generated
data points are given to every agent i in the form of (yt,i,j,xt,i,j), where yt,i,j ∈ {±1} is a binary label
and xt,i,j ∈ Rp is a feature vector for j = 1, . . . , mi . Our goal is to train a classifier θ ∈ Rp such that
for an arbitrary new feature vector X it assigns 夕=sign((θ,X)). We design decentralized convex
logistic and nonconvex sigmoid loss functions with `2 regularization as follows:
1 mi	1
f11(θ) =  ∑ (1 + exp(IOhXt,i,j, θiyt,i,j)Γ + νkθ∣∣2,
mi j=1
1 mi
Kt(θ) = — ∑ Iog(I + eχp(Txt,i,j, θiyt,i,j)) + νkθ∣2,	(18)
mi j=1
where ν = 1/mi and mi is the size of the training data on node i.
For X, we consider the '1 ball X'1 = {θ ∈ RP : ∣θ∣∣1 ≤ r}, when a sparse classifier is preferred.
We show results for the decentralized problem solved by DADAM, C-DADAM, DGD and EXTRA
over a medium-scale network. We have implemented EXTRA and DGD with their default settings.
The connected network is randomly generated with n = 50 agents and connectivity ratio r = 0.2.
Each agent holds 10 samples, i.e., mi = 10, ∀i ∈ V . The agents shall collaboratively obtain p co-
efficients via loss functions equation 18. All samples are randomly generated, and the reference
classifier θ* is pre-computed using a centralized method. As it is easy to implement in practice, we
use the Metropolis constant edge weight matrix W Shi et al. (2015) in our experimental evaluation
1. We have tested the DADAM and C-DADAM with β1 = β2 = β3 = 0.9. The numerical results are
illustrated in Figure 1. It can be seen that DADAM and C-DADAM outperform DGD and EXTRA,
showing almost linear convergence in the convex setting to the reference logistic classifier x*.
5.2 CIFAR and MNIST experiments
Next, we present the experimental results using the CIFAR-10 image recognition dataset
and the MNIST digit recognition task. The model architecture for training CNN to
classify CIFAR10 dataset was taken from the TensorFlow tutorial 2. Also, the model
for training a simple multilayer percepton (MLP) on the MNIST dataset was taken from
https://github.com/keras-team/keras/blob/master.
1 http://www.math.ucla.edu/ wotaoyin/software.html
2 http://www.tensorflow.org/tutorials
9
Under review as a conference paper at ICLR 2019
(b)
(c)
Figure 1: The residual 聘∖-[] on iteration (time step t ∈ {1,…，T}) for binary classification prob-
lem using convex and nonconvex (with α/√t) loss functions. Constant α = σn/ρ is the theoretical
critical step-size given for DGD Yuan et al. (2016). (a) classification with p = 10 and ν = 0. (b)
classification with p = 20 and ν = 0. (c) classification with p = 20 and ν = 1/mi.
We compare the accuracy of DADAM with that of the decentralized ADAM (Algorithm 1 with β3 =
1 ) and the Federated Averaging (FedAvg) algorithm McMahan et al. (2016) which also performs
data parallelization without decentralized computation. The connected network is generated with
n = 5 agents and connectivity ratio r = 0.8. The parameters for DADAM and ADAM are selected
in a way similar to the previous experiments. In our implementation, we use same number of agents
and choose E = C = 1 as the parameters in the FedAvg algorithm since it is close to a connected
topology scenario as considered in the DADAM and ADAM. It can be easily seen from Figure 2 that
DADAM and the decentralized ADAM (β3 = 1) outperform FedAvg. Further, DADAM can achieve
high accuracy in comparison with the decentralized ADAM and FedAvg.
6 Conclusion
A decentralized adaptive moment estimation method DADAM was proposed for the distributed
learning of deep networks based on adaptive moment of first and second moment of estimations.
Convergence properties of the proposed algorithm were established for convex (and dynamic) and
nonconvex functions in both stochastic and deterministic settings. Numerical results on some
datasets show the efficiency and effectiveness of the new proposed method in practice.
10
Under review as a conference paper at ICLR 2019
----DADAM with 5 agents
ADAM with 5 agents
----FedAvg
SSo- 6ucrob


Figure 2: Training loss and accuracy over 15 epochs. (a) MNIST digit recognition task (b) CIFAR10
image recognition dataset.
References
Hedy Attouch, Jerome Bolte, and Benar FUx Svaiter. Convergence of descent methods for semi-
algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized
gauss-seidel methods. Mathematical Programming, 137(1-2):91-129, 2013.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations
Research, 63(5):1227-1244, 2015.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223-1231, 2012.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Eric C Hall and Rebecca M Willett. Online convex optimization in dynamic environments. IEEE
Journal of Selected Topics in Signal Processing, 9(4):647-662, 2015.
Elad Hazan, Karan Singh, and Cyril Zhang. Efficient regret minimization in non-convex games.
arXiv preprint arXiv:1708.00075, 2017.
Elad Hazan et al. Introduction to online convex optimization. Foundations and TrendsR in Opti-
mization, 2(3-4):157-325, 2016.
Roger A Horn, Roger A Horn, and Charles R Johnson. Matrix analysis. Cambridge university press,
1990.
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar. Collaborative deep learning in
fixed topology networks. In Advances in Neural Information Processing Systems, pp. 5904-5914,
2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Victor Lesser, Charles L Ortiz Jr, and Milind Tambe. Distributed sensor networks: A multiagent
perspective, volume 9. Springer Science & Business Media, 2012.
11
Under review as a conference paper at ICLR 2019
Dan Li, Kerry D Wong, Yu Hen Hu, and Akbar M Sayeed. Detection, classification, and tracking of
targets. IEEE signal processing magazine, 19(2):17-29, 2002.
Mu Li, David G Andersen, and Jun Woo Park. Scaling distributed machine learning with the param-
eter server.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
David Mateos-Nunez and Jorge Cortes. Distributed online convex optimization over jointly Con-
nected digraphs. IEEE Transactions on Network Science and Engineering, 1(1):23-37, 2014.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. arXiv preprint arXiv:1002.4908, 2010.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization
in dynamic environments: Improved regret rates for strongly convex problems. In Decision and
Control (CDC), 2016 IEEE 55th Conference on, pp. 7195-7201. IEEE, 2016.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Transactions on Automatic Control, 54(1):48-61, 2009.
M Reza Peyghami and D Ataee Tarzanagh. A relaxed nonmonotone adaptive trust region method
for solving unconstrained optimization problems. Computational Optimization and Applications,
61(2):321-341, 2015.
Michael Rabbat and Robert Nowak. Distributed optimization in sensor networks. In Proceedings of
the 3rd international symposium on Information processing in sensor networks, pp. 20-27. ACM,
2004.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Shahin Shahrampour and Ali Jadbabaie. An online optimization approach for multi-agent tracking
of dynamic parameters in the presence of adversarial noise. In American Control Conference
(ACC), 2017, pp. 3306-3311. IEEE, 2017.
Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments
using mirror descent. IEEE Transactions on Automatic Control, 63(3):714-725, 2018.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
TrendsR in Machine Learning, 4(2):107-194, 2012.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentral-
ized consensus optimization. SIAM Journal on Optimization, 25(2):944-966, 2015.
T Tieleman and G Hinton. Divide the gradient by a running average of its recent magnitude. cours-
era: Neural networks for machine learning. Technical report, Technical Report. Available on-
line: https://zh. coursera. org/learn/neuralnetworks/lecture/YQHki/rmsprop-divide-the-gradient-
by-a-running-average-of-its-recent-magnitude (accessed on 21 April 2017).
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic
and stochastic gradient optimization algorithms. IEEE transactions on automatic control, 31(9):
803-812, 1986.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control
Letters, 53(1):65-78, 2004.
12
Under review as a conference paper at ICLR 2019
Lin Xiao, Stephen Boyd, and Seung-Jean Kim. Distributed average consensus with least-mean-
square deviation. Journal ofParallel and Distributed Computing, 67(1):33-46, 2007.
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM
Journal on Optimization, 26(3):1835-1854, 2016.
Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. IEEE Transactions on
Signal Processing, 66(11):2834-2848, 2018.
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic regret for
non-degenerate functions. In Advances in Neural Information Processing Systems, pp. 732-741,
2017.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. 2003.
Next, we establish a series of lemmas used in the proof of main theorems.
6.1 Properties of DADAM
Lemma 12. Beck & Teboulle (2003) Let X be a nonempty closed convex set in Rp. Then, for any
d ∈ X , we have
α*-d,G ≤ 2Ild- ck2 - 2Ild -x*∣∣2 - 2IlX*- ck2,
where
x * = argmin{h a, x i + -∖∖ x — c∣2}.
x∈X	2
Lemma 13. McMahan & Streeter (2010) For any A ∈ S+p and convex feasible set C ⊂ Rp, suppose
a1 = ΠC,A[b1],a2 = ΠC,A[b2], we have
k A 1(a 1 - a 2)k≤k A 2 (b 1 - b 2)|.
Lemma 14. For all i ∈ V ifβ1, β2 ∈ [0, 1) satisfy η = --β= < 1, then we have
β2
∑ ∑ αm2t, d
t=1 d=1 υ i, t, d
≤	α √1 + log T
-(1 - β1)(1 - η )P(1-β2)
p
∑ Igi,1:T,dI,
d=1
where at = √ for all t ∈ {1,..., T }.
Proof. Using the update rules in Algorithm 1, we have
∑ αm2,d = τ∑ αm2t,d +αTmtT,d
t=1 Pt U i,t, d	t=1 PtU i,t, d	p(1 - β3) max{υ i, T-1, d, υ i, T, d } + β3 υ i, T, d
≤ τ∑ αm2,d +	αTm2T,d
— t=1 tυiυi,t,d	√(1 - β3)υi,T,d + β3υi,T,d
=∑1 α m2t, d + α(∑ T=1(1- β1)βT-lgi,1, d A
t=1 PU^	qt∑T=1(1 - β2)βT-ιg2,ι,d
(≤) T∑ αm2t,d + α	(∑T=1 B「l)(∑T=1 βT-lg2ι,d)
-t∑ R PT(T-^	q∑T=1 βT-ιg2ι,d
(≤) T∑ αm2t,d +α∑ βT-lg2ι,d
一 ∑ PU^	(1-β1 )pT(1-两∑ qβTTg2l,d
T -1	2	T
≤ ∑ Pmtd + n Qom Q ∑ η TT 忸〃,d∣,
t=1 √t Ui,t,d (1 - β1) √t (1 - β2) l=1
13
Under review as a conference paper at ICLR 2019
where (i) follows from the fact that the update rules of mT and υT can be written as mT = (1 -
β1 ) ∑lT=1 β1T-lgl and υT = (1 - β2) ∑lT=1 β2T-lgl2, respectively. (ii) follows from Cauchy-Schwarz
inequality and the fact that 0 ≤ βι < 1. Inequality (iii) follows since ∑T=I βTT ≤ ɪ-^. Hence, We
have
∑	α m21, d
t=1 (I- βI)PtUi,t,d
Tαt
≤t∑厂而P(TF∑η lggi,l,d 1
(1 - βι)P(T-β2) 区 √I==J	,8ι,ι,d^
α	T	T ηl-t
(1 -βι)P(T-β2) ∑1 g^t,d 1 ∑飞
α T T ηl-t
≤ (1 - β1)P0-β2) ∑1 gi, t, d 1 ∑ ^√
≤------α ∑ |g；, M _1_=
一 (1-β1)pi^F∑1 gi,t,d 1 (1 - η)√t
≤ (1 -β1)(1 -η)P0-β2)kgi,1:T,dk∖Σ
(≤)____α 尸两________k gi 1 Td k
一(1-β1)(1-η)P(1-βJ,dll,
where inequality (i) follows since ∑T= t ηIT ≤ (山).Inequality (ii) follows from Cauchy-Schwarz
inequality. The inequality (iii) follows since
T1	T1
∑ - ≤ 1 + ( ɪ -dt = 1 + logt|T = 1 + logT.
(19)
□
Next, we provide an upper bound on the deviation of the local estimates at each iteration from
their consensual value. A similar result has been proven in Shahrampour & Jadbabaie (2017) for
online decentralized mirror descent; however, the following lemma extends that of Shahrampour &
Jadbabaie (2017) to the online adaptive setting and takes into account the sparsity of gradient vector.
Lemma 15. ( Network Error with Sparse Data) If β1, β2 ∈ [0,1) satisfy η = -1= < 1, then the
β2
sequence xi,t generated by Algorithm 1 satisfies
Tn 1	1
∑∑ ɪ 陀 ,4(无t-xt )k2 ≤
t=1 i=1 αt
n √n α √1 + log T ∑ d =1 k g 1： T, d k
(1 - σ2"))2(I -βI)(I - η)p(1 -β2)
where V',t = diag(Ui,t) andXt = n ∑n=1 Xi,t.
14
Under review as a conference paper at ICLR 2019
Proof. Let ei,t :=xi,t+1 - ∑nj=1 [W]ijxj,t, where W satisfies equation 2. Using the update rule of xi,t+1
in Algorithm 1, we have
T 1 ι CT 1 ι	n	C
∑ ɪ kV^i^, t k2 = ∑ ɪ 陀 M,t+ι-∑ W jj,t)k2
t=1 αt	t=1 αt	j=1
=∑ O-kVV'4 (πχ,√Vt [ ∑ W]ijχj,t- Om] - ∑ W]ijxj,t)Il2
t=1 αt	, i,t j=1	υi,t	j=1
T 1	1 n	-1	n	C
≤ ∑ ;花■ ,4( ∑ W ] ijxj, t - αt Vr mi,t- ∑ [W]ijXj,t )k2
t=1 αt	j=1	j=1
=∑ ∑ α m t, d
t=1 d=1 OUi,t,d '
where the first inequality follows from Lemma 13.
(20)
Further, from the definition of ei,t , we have
n
xi,t+1 = ∑ [W]ijxj,t + ei,t .	(21)
j=1
Now, from equation 2 and equation 21, we have
1n	1n n	1n
Xt+1 = 1 ∑ Xi, t+1 = 1 ∑∑ W ]ijXj,t +1 ∑ ei,t
n i=1	n i=1 j=1	n i=1
1n n	1n
=n ∑(∑W]ij)Xj,t + 1 ∑ei,t
n j=1 i=1	n i=1
=Xt + et,
where et = 1 ∑n=1 ei,t. Hence,	t Xt +1 = ∑ eS.	(22) S=0
It follows from equation 21 that	tn Xi,t+1 = ∑ ∑ [Wt-S]ijei,S.	(23) S=0 j=1
Now, using equation 23 and equation 22, we have
tn	1
xi,t+1 - Xt+1 =∑∑(∖Wt-s]ij- n)ei,S.	(24)
Now, taking the Euclidean norm of equation 24 and summing over t ∈ {1, . . . , T}, one has:
1
T 1 U/4∕	-	"12 9 T,^∕6 IMt-Sl	11、2 陀i 4sei, Sk2
∑ -kV4(Xi,t+1 -Xt+1)k2 ≤ Zt ∑(∑ |[Wt s]ij- nI)2-La—
i,t	,	t=1 S=0 j=1	n	S 1 (ii) T ∣∣V>.4ei Jl2 T ≤ ∑ ⅛s1 ∑ nt σ2t-2S (W) S=0	-S	t=1 1 (≤)	n	T kvi∙,tei,tk2 一(1 - σ2(W))2 ∑0	at ≤)	n	∑ ∑ - m2t,d ≤ (I- b2(W))2 Σ1 ∑ Ed ≤	n a √1 + log T ∑ Pd=1k gi,1: T,dk (1 - σ2(W))2(1 -01)(1 - η)O(1 - β2) ,
15
Under review as a conference paper at ICLR 2019
where step (i) follows from ∣∣ ∑之1 的∣∣2 ≤ n∑n=Ikq∣∣2, step (ii) follows from the following property
of mixing matrix W Horn et al. (1990),
K 「 ，r 1	一一、
∑ [w〔 —— ≤√nσ2(W),	(26)
j=1	n
step (iii) follows from ∑T=I tσ2(W) < °-σI(W))2, step (iv) follows from equation 20 and step (v)
follows from Lemma 14.
Now, summing the equation 25 over i ∈ V and using
n	n	1
∑ Ilgi,1:τ,dIl ≤ √n(X Ilgi,1:t,dll2)2 = √nIlg 1:t,dll,
i=1	i=1
we complete the proof.
Lemma 16. For the SeqUenCe Xi,t generated by Algorithm 1, we have
1 ∑ £(呼Ix； - ∑ [W]ijXj,tk2 -噜Ix； -Xi,t+1k2)
ni=11=1	α	j=1	α
2 2γ2 V PT,d _L 2γ∞ V PT,d T-Il V*	V* I
≤√n d∑1 ɑT + √ d∑1 αT t£1Xt+1d-xt,dl∙
(27)
□
(28)
Proof. From the left side of equation 28, we have
^pt Ilx；- ∑[W ] ijxj, t 12 - ^pt Ilx；- xi, t+1∣l2
a	j=1	3
="~^^,t IlX； - ∑ [W]ijxj,t 12 - V^i,t+1 Ilx;+1 - ∑ [W]ijxj,t +112
α	j=1	α+1	j=1
+ * IX;+1 - ∑ [W]ijXj,t +1I2 -，IX； - ∑ [W]ijXj,t+1∣∣2
at+1	j=1	at+1
+ C；+1 i x;- xi, t+1∣∣2 - ʒɪiɪɪ Iix;- Xit+1∣∣2.
at+1	at
(29)
j=1
(30)
By construction of equation 29, we have
Jυit+1 IlX；+1- ∑[W]ijxj,t+1 Il2 - JUit+1 IlX；
j=1
n
∑ [W ]jXjt+1k2
j=1
n
1, d-xt d, xt+1, d + x*, d - 2 ∑ [W] ijxj,t+1, d i
j=1
α+1
—
≤ 2γ∞∑，际+1,dlx;+1,d-x*,dl,
d=1
(31)
where the last inequality holds due to equation 1.
Summing equation 30 over t ∈ {1,..., T}, the first term telescopes, while the second is handled with
equation 31. Hence,
∑ ( ʒpɪt Ilx； - ∑ [W]ijXj,t||2 - ʒpɪt Ilx； - xi, t+1||2)
t=1 ɑ	j=1	α
≤ ∑ 2∞^ + τ∑ ∑ 2%pwlx;+1,d -x*,dl + γ∞ ∑1 ∑ (
d=1	α1	t=1 d=1	αt+1	t=1 d=1
≤ ∑ 2法Ed + τ∑ ∑ 2γ∞p"+1,d lx；+1 d -X；dl,
—J	ατ	J J	a l 1	l t +1,a t,aυ
d=1	ɑ T	t=1 d=1	α+1
vz'υ i, t+1, d	Vυ i, t, d
—
αt+1
α+1	at
)
(32)
16
Under review as a conference paper at ICLR 2019
where the last inequality follows since from definition of Ui,t, We have
αt+1	αt
(33)
Now, summing equation 32 over i ∈ V and using the inequality ∑n=γ a/UJ ≤ √n√υ^t, the claim in
equation 28 follows.	口
Lemma 17. Suppose that the parameters β1,β2 ∈ [0,1) satisfy η = -√= < L Let β1,t = β1 λt-1, λ ∈
(0,1) and ∣∣Vfi,t(ɪ)k∞ ≤ G∞ forall t ∈ {1,..., T}. Then, using a step-size at = √ for the Sequence
xi,t generated by Algorithm 1, we have
1nT
1 ∑∑ f (Xi,t) - f (K)
n i=1 t=1
V (I - βDα√1 + log T PT' Il Il , ,p _G∞γ∞_
≤ —2pr-β2y	d∑1k g LT, d k+d∑1 (1-β1)(1-λ)
+	2α √T+T⅞T∑d=1 k g 1: T, d k
(I - σ2(W)) P(I - β1) P(I - η) p(1 - β2)
+ 递 ∑ VTUTd + & ∑
√n d∑1 (1 - β1)α	√n d∑1
VTUTd
(1 - β1)α
T-1
∑ | xt+1, d - xt, d |.
t=1
Proof. From convexity of fi,t(∙), we have
1nT	1nT
-∑∑ fi, t (Xi,t) - fi, t (K) ≤ - ∑∑ hV fi, t(χi,t) χi, t - Ki
n i=1 t=1	n i=1 t=1
1nT
- ∑∑ hVfi,t(Xi,t),Xi,t+1 -
n i=11=1X----------{z-------
I1
1nT	n
X}+n ∑∑hV fi,t (xi,t), xi,t - ∑[W]ijx	j,ti
-}	n i=1t=1	j=1
V------------------------------{------------------------------}
I2
1nT	n
+ - ∑∑ hV fi,t (Xi,t ),∑ [W ]ijxj, t-xi,t+1 i .
n i=1 t=1	j=1
(34)
X----------------------------------
I3
}
Individual terms in equation 34 can be bounded in the following way. From the Young’s inequality
for products3, we have
1nT	n
/3 = - ∑∑ hV ft (Xi, t ),∑ [W ]ijXj, t- Xi,t+1i
n i=1 t=1	j=1
≤ - Σ Σ 2αp⅝k Σ [W]jXj,t -Xi,t+1k2 + - Σ Σ ¾=-β⅛kVfi,t(Xi,t)k2.	(35)
n i=1t=1 2αt (1 - β1) j=1	n i=1t=1 2 Ui,t
Note also that:
∑ αtgjt,d = ∑1 αg21,d + αTg2T,d
t=1 VZ υi,t, d	t=1 VZ υi,t, d	VZ Ui,T, d
=T∑1 α g21, d +________α g2 T, d______
=t⅛ PUZ	PW-^ q∑oRZ
≤ 3∑1 αgi,t,d +	α Igi萋,d |
^t∑ kd	Pt(1 -β2).
3An elementary case of Young,s inequality is ab ≤ a2 + b22.
17
Under review as a conference paper at ICLR 2019
Using Cauchy-Schwarz inequality, equation 19 and equation 27, we have
nT
∑∑
i=1 t=1
α gi J, d ≤ α	∑∑ ∑ | gi, t, d |
p‰ - P(l-β2) ∑ ∑ F
α
≤	,
p P(1-β2)
n	/ T 1
£ k gi ,1: T, d 111 ∕∑7
i=1	t=1 t
≤
α √1 + log T
p(1 - β2)
n
∑	kgi,1:T,dk
i=1
≤
√n α √1 + log T
p(1 - β2)
kg1:T,dk.
(36)
Using equation 35 and equation 36, we have
nT
11 ≤ n ∑∑
n i=1 t=1
F ɪ	C
⅛β7 k ∑mv¾t- "k2+
(I- βI)√nα√1 + IOgT II II
一k西一k g IT,d k.
(37)
In addition, we have
1nT	n
12 = 1 ∑∑hgi,t,Xi,t - ∑[WMi
n i=1 t=1	j=1
1nT	n
=-∑∑ h gi, t, χi, t- ∙xt+Xt- ∑ W ] ijxj, t i
n i=1 t=1	j=1
1nT	1nT n
=1 ∑∑ h gi,t, Xi, t - Xt i +1 ∑∑∑ [W ]ijh gi,, Xt-Xjt
n i=1 t=1	n i=1 t=1 j=1
1
1 n T	-1	»4
=^∑∑ h√αtV', t gi, t, √r≡ (Xi, t - ∙xt )i
n i=1 t=1	αt
1
1 n t n	-1	V/
+1 ∑∑∑ [W ]ijh√αt V： gi,t, √= (Xt - Xj,t )i
n i=1 t=1 j=1	αt
v---------- 1
1 n /T -T V?	ɔ
≤ 1 i∑ Vt∑ at 疗 g t t∑ V(Xi, t- xt)2
v----------------- 1
1 n n	T T	-1	T V∙2
+1 ∑∑ [W j∑ atV； g2tt ∑ V (Xj, t - Xt )2
n i=1 j=1	t=1	t=1 αt
(38)
where equation 38 follows from Cauchy-Schwarz inequality.
Now, using equation 38 we obtain
l---------------------- 1
1 InTp -1	n T p β?
12≤nʌ ∑∑∑a庠dg,t,dt ∑∑∑詈(X^t,d-Xt,d)2
n i=1 t=1 d=1	i=1 t=1 d=1 αt
l-------------- 1
i n	InTP	-1	n T p U2
+1 ∑ [W j∑∑∑ at Wdgi,t,dt ∑∑∑ 詈(Xj,t,d - Xt,d )2	(39)
n j=1	i=1 t=1 d=1	i=1 t=1 d=1 αt
≤	2a √1+!⅞τL d=1k g 1：T, d k
(1 - σ2(WA P(I - βI) P(I - η) P(I - β),
where equation 39 utilize Cauchy-Schwarz inequality and equation 40 follows from equation 2,
Lemma 15 and equation 36.
18
Under review as a conference paper at ICLR 2019
To bound 11, using the update rule of m" in Algorithm 1, we have
hαt-7==, xi, t +1 — xti = hɑt ( -^==t= mi,t-1 + -~~7=,t^ V fi, t ( xi, t)), xi, t+1 — xti
√υ i, t	√υ i, t	√υ *
=h 鸣 mu xi,t+1 -x；)+ h ^t(1 -β1, t) V f,t(xi,t), xi,t+1 -x》
√υ i, t	√υ i, t
Now, by rearranging the above equality, we obtain:
∑ h(
d=1
P
(I -01,t)
∑ h
d=1
υ i, t, d
mi,t, d
V fi, t, d (xi, t, d ), xi, t+1, d - xt, d i
, xi,t +1,d -x；, d i + ∑	^It- hmi,t-1,d, xt,,d - xi,t+1,d i
d=1 V υ i, t, d
υ i, t, d
≤ h Pm^, xi, t+1 - x；i + ll x； - xi, t+1ll∞∑ BIpm- ,d
√υ i, t	d=1	√υ i, t, d
1	Jn	1	1	Jn	C
≤ 2α ll xt - ∑ W ] ijxj, tll -拓 ll xt - xi, t+1H -拓 ll xi, t +1 - ∑ W ] ijxj, t H
S----------------------------------------V-------------------------------------}
(II1)
+ Il x；- xi, t+1∣l∞ G ∞∑	,
d=1 V υ i, t, d
X------------------------------/
(41)
"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^""^
(II2 )
where (II1) follows from Lemma 12. The term (II2) is obtained by induction as follows: using the
assumption, kgi,t∣∣∞ ≤ G∞; now, using the update rule of mi,t in Algorithm 1, we have
Ilmi,tk∞ ≤ (β1 + (1 -β1))maχ(kgi,t∣∣∞,∣∣mi,t-"∣∞) = max(∣∣gi,t∣∣∞,∣∣mi,t-1k∞) ≤ G∞,	(42)
where ∣∣mi,-1∣∞ ≤ G∞ by induction hypothesis.
Note that from our assumption, we have β1,t = β1λt-1, λ ∈ (0,1) and β1,t ≤ β1. Substituting this
into equation 41, we get
i1 = hVfi,t(xi,t), xi,t+1 - xt i
ʌ/u:	1 士 CI	C
≤ (1 -β )[2—llxt - ∑W]ijxj,tll - 2—llxt -xi,t+1 ll
(1 - β1,t ) 2—t	j=1	2—t
—
1	n	P β
-2— H xi, t+1 -∑[W]ijxj,tll2] + llx；-xi,t+1ll∞G∞ ∑ -ʃ⅞-
2at	j=1	d=1 (1 - B1,t)
K 1 ɪ	CI	C
≤ (Γ-βo[拓kxt - ∑[W]ijxjttk -拓kxt -xi,t+1k ]
、局二	&	C	P β.
(1 -β1)2αt11 xi,t+1 -∑W]ijxj,tll + γ∞G∞区(Γ-⅞1J'
(43)
where the last line comes from equation 1.
Plugging equation 37, equation 40, and equation 43 into equation 34, we obtain
- ∑∑ (fi, t(xi, t) - fi, t (x；)) ≤
n i=1 t=1
(1-β1)— √1 + logT
2√n ∙p∕(1 - β2)
P	T p
∑ kg 1：T,dk + γ∞G∞∑∑
d=1	t=1 d=1
β1,t
(1 - β1, t)
+	2― √T+⅞T ∑ ∑d=1k g 1： T, d k
(1 - σ2(W))/(I - β1 ) /(I - η )/(I - β2)
+n ∑∑ 2(1-t 1) Bll x；- ∑ W ] ijxj, t n2——n x；- xi, t+1 仔]∙
n i=1 t=1 ( - 1) —t	j=1	—t
19
Under review as a conference paper at ICLR 2019
Now, since β1,t = β1λt-1,λ ∈ (0, 1) and β1,t ≤ β1, we have
∑ (T⅛) ≤ ∑ (⅛) ≤ (T-β⅛λ).
(44)
Finally, using Lemma 16 and equation 44, We obtain the desired result.	□
6.1.1 Proof of Theorem 4
Proof. From convexity of f7(∙) it follows that
ft (Xi,t)- ft (K) = ft(xi, t)- ft(X)+f (X) - ft (K)
≤ hgi,t, xi,t- Xi+ ft(xt)- ft(xt)
1n	1n
=-∑ fi, t(Xt)--∑ fi, t(xt) + h gi, t, xi, t- xti,
n i=1	n i=1
and
1n	1n	1n
ft(xi,t) - ft(xt ) ≤ 一 ∑ fi,t(xi,t)-∑ fi,t(xt ) + hgi,t, xi,t -xt i + 一 ∑hgi,t, xi,t - xt i∙	(45)
n i=1	n i=1	n i=1
Summing over t ∈ {1, . . . , T} and i ∈ V , and applying Lemma 17 and equation 40 gives the desired
result.	□
6.1.2 Proof of Theorem 5
Proof. We just need to rework the proof of Lemma 17 and Theorem 4 using stochastic gradients by
tracking the changes. Indeed, using stochastic gradient at the beginning of Lemma 17, we have
TT
∑ (fi,t (Xi,t) - fi,t(xt)) ≤ ∑ h fi,t(xi,t) xi,t - xti
t =1	t =1
TT
=∑(▽ fi, t(xi, t), xi, t- xti + ∑ h fi, t (xi,t)- ▽ fi, t (Xi,t) xi, t- xti
t =1	t =1
T	Tn
=∑Bfit(xi,t),xi,t+1 -xti + ∑Bfi,t(xi,t),xi,t - ∑ W]ijxj,ti
t =1	t =1	j=1
Tn	T
+ ∑(▽ fi, t(xi, t), ∑ W ] ijxj, t - xi, t+1i + ∑ hV fi, t(xi, t) - ▽ fi, t(xi, t), xi, t - xti.
t =1	j=1	t =1
Further, if we replace any bound involving G∞ which is an upper bound on the exact gradient with
the norm of stochastic gradient, we obtain
1Σ ∑ (fitt(xi,t) - f (xt)) ≤ Q-√√p√1+ lo" Σ kgiz”k + γ∞kmi,t-ik∞∑ Σ r⅛π
ni=ιt=1'	2	2√n,(1-02) d=ι	t=1 d=ι (I-仇,“
+	2α√T+T⅞Γ∑Pd =ι kgi：T,dI1
(I - σ2(W)) P(I - β1 ) P(I - η) P(I -㈤
+n ∑∑ 2(iZ-t ∣) [ α || xt- ∑ [w ] ijxj, t ||2- α || xt- xi, t+1||2]
n i=1 t =1 ( - β1 ) αt	j=1	αt
1n
+ - ∑hvfitt(xi,t) - ▽ fit(xi,t),xi,t -xti.	(46)
n i=1
Note that using Assumption 3, we have
E [hV fi, t (xi, t) - ▽ fi, t (xi, t), xi, t - xti]= 0,
and
E[kVfi,t(xi,t)k] ≤ (E [kVfi,t(xi,t)k2]f ≤ ξ.
20
Under review as a conference paper at ICLR 2019
Hence taking expectation from equation 46, using the above inequality and equation 42, we have
1 在 Gλ17"/ X1 , , nλ .(1 -βι)α√T+logT Λ r l,-l ,	≡ T V	βι,t
^ ∑∑	(E [fi,t (xi,t)] - f (xt)) ≤ -°	厂 K a、- ∑ E [kg1:T,dII]	+ γ∞ξ ∑ ∑(1 _	β~)
n i=1t=1	2	n (1 -β2) d=1	t=1 d=1	(1 -β1,t)
+	2α √T+0gT ∑d=T E[∣∣gττ, d∣∣]
(1 - σ2(W)) P(I - 01)P(I - η) P(I - 02)
+^ ∑ 2(118)E" ^Pt k xt- ∑ W ] ijxj, t k2 - ^Pt k xt- xi, t+1k2.
n i=1 2(1 - 01 )	αt	j=1	αt
According to the result in Lemma 16 and equation 44, we have
n∑ IT (e[ f (xi, t)]-fit t (x^ ≤(I -√n*+2T d∑I	,	d=1 (1-βξ Y∞1-λ)
+	2α√1+T0gT∑d=严[kg1jT,dI
(1 - σ2 (W))P(I - 01) P(I - η ) P(I -。2)
+ 金g √E[PυTJ + 上g √E[PυTJ Q∖*	_* 1
+ √n ∑	(1-β1)α	+ √n ∑	(1-β1)α	1 | xt+1, L xt，d 1.
Finally, using equation 45 we complete the proof.
□
Next, we establish a series of lemmas used in the proof of Theorems 8 and 10.
Lemma 18. Attouch et al. (2013) Let GX be the projected gradient defied in equation 5. Then,
GX (①,fi, α) = 0 ifand only if G is a CriticaI point ofequation 3.
Next, we show that the projection map GX in Definition 1 is Lipschitz continuous.
Lemma 19. Suppose the second moment U satisfies equation 11. Let x+i and x+i be given in
Definition 1 with m‰ replaced by -‰ and -m=^, respectively. Then,
√υi	√υi1	√υi2
..1 ,	- , ɔ ,	-	...	D., 1 C...............
kGX(Xi,f,α)-GX(Xi,f,α)k ≤ Ukm1-m2k,	∀i ∈ V,
where G1 and G2 are the projection maps corresponding to x1+,i and x2+,i, respectively.
Proof. Consider the optimality condition of equation 6, for any u ∈ X . For each i ∈ V, observe
that
m1	1	n
h q=∣z+α(X 1t i- ∑ W] ijxj )，u - x 1, i i ≥0,	(47)
m2	1	n
h~τ^ + ~(x2,i - ∑ W]ijxj)，u -x2,ii ≥ 0.	(48)
√U2	α	j=1
Taking u = x2+,i in equation 47, we have
hmi,x+i-x +ii≥ 身hIW]jxj-x +i,x+,i-x+ii.	(49)
α j=1
Likewise, setting u = x1+,i in equation 48, we get
国n
h m2, x+i - x + i i ≥ ɪh Σ [W ] ijxj - x 2； i, x + i - x + i i.	(5。)
α j=1
Now, using equation 49, equation 50 and equation 11, we obtain
km1-m2k ≥ Ukx+ i -x+ ik.	(51)
21
Under review as a conference paper at ICLR 2019
Without loss of generality, assumes that ʌ/υ^ 1 ≤ ʌ/υ^2. Then, using equation 5, We have
_	_	∖ υ^ 1
k G X (Xi ,f, α ) - G X (xi,f, α )k = k a (X - X+ i)
—
—
U1 /	+ mi (i) Ull +	+ ll (ii) υ ll ι 2ll
—(X - X+i )k ≤ α kX+i- X+i k ≤ U k mi- m2k,
where (i) follows from equation 11 and (ii) follows from equation 51.
□
Lemma 20. Let β1,β2 ∈ [0, 1) satisfy η =% < L Then, for the sequence Xit generated by
β2
Algorithm 1, we have
1n
一 Σ k xi, t - xtk ≤
n i=1
(I - η)vz(1 - β2)
t-1
∑αsσ2t-s-1(W) :=Bt.
s=0
Proof. In the proof of Lemma 15 We shoWed that
tn	1
Xi,t +1 --xt+1 = ∑ ∑ ([Wt-1ij-)ei,
s=0 j=1	n
,s.
Now, using the above equality, we have
tn	1
kXi,t+1-Xt +ιk = ∑∑ WT ij - - Ik ei,sk.
s=0 j=1	n
(52)
Also, using Lemma 14, we get
mi, ≤ ________
Pi	(I - η)p(1 - β2)
(53)
The above inequality together with the update rule ofxi,t+1, imply that
kei,t k = kXi,t+1 - ∑[W]ijXj,tk = kΠX [∑[W]ijXj,t -αt
j=1
j=1
n
—
1
n
黑 H [WMk
n	mn
≤ k Σ [W] ijXj,t - α p⅛= - Σ [W] ijXj,t k
j=1	Ui,t	j=1
=αt k T k
√υ i, t
αt
(54)
≤ ------,	,
一 (i-η)√(i-β2),
where the first inequality follows from the nonexpansiveness property of the Euclidean projection4.
Substituting equation 26 and equation 54 into equation 52, we get
k Xt-Xtk ≤
(I - η)vz(1 - β2)
t-1
∑ αsσ2t-s-1 (W).
s=0
Summing the above inequality over i ∈ V , we conclude that
n
n
n
∑ k Xi∙,tYk≤ ∑ k Xi,t-xt k + ∑ k Xt-x；ks
i=1
i=1
i=1
(ιdρ⅛ ∑s 引-I(W).
□
4kΠX [X1] -ΠX [X2]k ≤ kX1 -X2k, ∀X1,X2 ∈ Rp.
22
Under review as a conference paper at ICLR 2019
Lemma 21. For the sequence xi,t generated by Algorithm 1, we have
,  _	_ .	，  .,
hf,t, Gx (Xi,t,f,t, αt)i ≥
(2-βι,t) S (rf 〜)k2_	βι,tUi,t
2(1-βι,t)kGX(X辱,fitt,⑷k - 2(1-βι,t)(1 - η)2(i-β2).
Proof. A quick look at optimality condition of equation 6 verifies that
h it— + —(χi,t+1 - ∑ W]ijxj,t), z - χi,t +1i ≥ 0,	∀Z ∈ X.
√υ i, t	αt	j=ι
Substituting z = Xi,t into the above inequality, we get
h it—, χi, t - χi, t+1i ≥ — h χi, t+1 - ∑ W ] ijxj, t, χi, t +1 - χi, t i.
υi,t	αt	j=1
Now using equation 2, we have
Il xi, t - xi, t+1k2 ≤ h mi, t, xi, t - xi, t+1i = hβ1,tmi,t-1 + (I - 01,t Nf,, xi, t - xi, t +1i
αt
_ β1,t ʌ/U i,t-1 Iatmi, t -1	∖	∕1 n 、p )	∖
=	-	h -/八	,xi,t - xi,t +1i + (I - β1,t ∖h fi,t, xi,t - xi,t+1i
at	√υ i, t -1
≤ "1 Ita^ ((I - η Ft(1 - β2 ) + k xi, t - xi, t+1k2) + (1 - β1,t )hV f,, xi, t - xi, t+1i,
where the first equality follows from the update rule of mi,t . The last inequality is valid since Cauchy-
SchWarz inequality and inequality，Ui,t-1 β1,t ≤ v¼Jβ1. The claim then follows after using Defi-
nition 1.	□
6.1.3 Proof of Theorem 8
>Λ	/■ T . 7* /	∖	1 τ~' t	/' / ∖ T T	A	. ∙ C rɪl 1 ，	♦	1 1 ʌ {' ∙ . ∙	<
Proof. Let fi,t (xi,t) = 1 ∑S=1 fi,s(xi,t). Using Assumption 2, Taylor S expansion and Definition 1, we
have
ρ2
fi,t(xi, t +1 ) ≤ fi, t(xi, t) + h fi,t(xi, t), xi, t +1 - xi, t i + 2 k xi,t+1 - xi, t k
2
≤ f,t(xi, t)-7=h^f (xi, t), G X (xi, t, ft, at )i +	k G X (xi, t, ft, at )k2.
U i, t	2υ i, t
The above inequality together with Lemma 21, equation 11 and β1,t ≤ β1, imply that
f,(xi,t+1) ≤ f,(xi,t) - αa(22υβ1) kGX(xi,t,f,,at)k2
+ 2(1-β1,taβ-t；)2(1-β2) + Pa2kGX(xi,t, f t, at)k2.	(55)
Let ∆i,t = fi,t (xi,t) - fi,t (x；) denotes the instantaneous loss at round t. We have
t1
δ i, t +1 = --7 ( f, (xi, t +1) - f, (xt+1))+7T^f( fi, t+1( xi, t +1) - fi, t +1 (xt+1)).	(56)
t + 1、	-Y-	'	J t + 1
(I)
Note that (I) can be bounded as follows:
f,t(xi, t+1) - f, (x；+1) ≤ f, (xi, t+1) - f, (x；) ≤ A i, t - [ -~~2UU^~t - 2U22 ]k G X (xi, t,f,, at )k2
at β1,t U
+ 2(1-β1,t )(1-η )2(1-β2),
(57)
23
Under review as a conference paper at ICLR 2019
where the first inequality is due to the optimality of x； and the second inequality follows from
equation 55.
Now, combining equation 57 and equation 56 gives
ʌ Vt ʌ r(2 - β1)at
ʌi,t+1 ≤ t+1 (ʌi,t - [	2U
Il G X (χi,t，fi, t,at )∣∣2
at βι,t U
—————......————)+
2(1 - β1,t)(1 - η)2(I- β2)	t + 1
(fi,t +1(xi,t+1) - fi,t+1(χ+1))∙
By rearranging the above inequality, we have
t r(2- β1)0t	Pa2]“「 /	-	、”2 t , ɪ
m[- 2U	运]kGX(Xi,t, f,t, at)k ≤ t+1 (ʌi,t +
a β1,t U
2(1 - β1, t )(1 - η )2(1-β2)
+ t+1 Ifi,+1(xi,t+1) - f∕+1(K+1)) - ʌi,t+1.	(58)
Using the definition of &,+1, we get 吉(fi, t +1( Xi,t+1) - f∙,t+1X+1)) - ʌ i, t +1 =-(占)(f,t (xi,t+1)-
fi,t(x；+1)). Hence, using equation 58 and simplifying terms, we obtain
[(2-β1)αt
[	2U
IlG X(χi, t, f t, at )∣∣2
+
—
)
—
—
—
a β1,t U
2(1 - β1, t)(I-η )2 (I-β2)
Note that:
1 n t /	_	_
^ ∑∑ (ʌi,t - (f-,t (xi,t+1) - f-,t(xt.+1))
n i=11=1 v
n ∑ t∑.(( f Mt
—
—
—
-f',t (Xi,t+1) ) - ( fi,t(xt ) - fi,t(xt+1
1 n	_	_	_
=-∑[-f,T(Xi,T+1) + f；1 (Xi,1) - f,1 (X1)+ f∙,T(XT+1)]
n i =1
+1 ∑ ∑ 厂1 (f, (Xi,t) - f,t-1 (Xi, t) - (f, t (X；) - fi,t-1(x1)))
n i =11=2	V	'
≤ 一 ∑ L (kxi,T+1 - xT+1k + Ilxi,1 - x；k + ∑ 21-1kxi,t -x"l)
n i =1 V	t=2	/
T
≤ 2L max Bt(1 + ∑ t-1) ≤ 2L max Bt(2 + logT),
-t ∈{2,..., T}	M 一 t ∈{2,..., T}
where the first equality uses
f, ( xi, t) - fi,t-1(Xi,t )= t-1( fi, t (Xi,t) - fi,t-1(Xi,t)).
(59)
(60)
(61)
(62)
The first inequality follows from Assumption 2. The second inequality follows from Lemma 20 and
equation 19.
Summing equation 59 over i ∈ V, t ∈{1,..., T} and using equation 62, we have
n∑teminT}kgX(Xt,f,,α)k2∑[(2 2β11)Ot -第]
n i =1t∈{1,…,T }	t = 1	NU	NU
≤ 1 GT [(2 - β1)αt
-n∑t∑[	2υ
IlG x (Xi,t, f t, at )k2
—
≤ (2 + log T)2L
2 √n
max	/_________
t∈{2,∙∙∙,T} (I - η) ʌ/(1 - β2)
t-1
∑ aS a；」”)
S=0
T
+∑
t=1
a β1,t U
2(1 - β1, t)(1 - η )2(1 -β2)
(63)
24
Under review as a conference paper at ICLR 2019
Note that ∑T=I [(2-β1 )α -嘉]> 0. Therefore, dividing both sides of the equation 63 by
∑T=I [(2-2U)αt -嗨],we obtain equation 14.	□
6.1.4 Proof of Corollary 9
Proof. With the constant step-sizes at = ',铲 for all t ∈{1,..., T}, we have
T
Nt=∑ [
t=1
(2 - βl)αt
-2U-
T (2 - βι Fυ2
8ρU2
Therefore, using the above equality and equation 12 together with α ='露〃 for all t ∈
{1, . . . , T}, we obtain
and
1T
初∑2(1-βι,t)(1-η)2(1-β2) = T(2-βι) t
α βι,t U
4U
4U
βι,t U
2(i-β1,t )(i-η )2(1 - β2)
λt-1U
2(i-βι)(i-η )2(1 - β2)
2U2
≤ T(2-β1)(1-β1 )(1-η)2(1-β2)(1-λ),
(64)
-!-(2 + logT)2L max	J________
Nt	t ∈{2,...,τ}(1 - η )√(1 - β2)
t-1
∑ αsσ2t-s-1 (W)
s=0
16U (2 + log T) Lyβ
t-1
≤------ L '~LJ', == max ∑ σ2t-s-1 (W)
t(2-β1)(1 - η)p(1 -β2)t∈{2,..∙,T}S=0
16U (2 + log T) L/n
≤	'______ .
t (2 — β1)(1 - η )p(1 - β2) (1 - σ2(w))
(65)
Combine the results in equation 64 and equation 65, we have equation 13.
6.1.5 Proof of Theorem 10
Proof. Let
δi, t = ▽ ft (Xi, t) - V f,t (Xi, t),	∀ t ≥ 1,
where ▽ f (Xi,t) = t ∑S=1 ▽ f∙,s(xi,t) denotes the minibatch stochastic gradient on node i.
Since fi,t is ρ-smooth, it follows that fi,t is also ρ-smooth. Hence for every i ∈ V and t ∈ {1, . . . , T},
we have
ρ2
fi,t (Xi,t+1) ≤ fi,t (Xi,t) + hVfi,t (Xi,t),χi,t+1 - χi,ti + ~ ∣∣χi,t +1 - χi,t k
α	ρα2
≤ f, (Xi,t)-7=hV f, (Xi,t), GX (Xi,t, f,t, ɑt )i + TTτ-∣GX (Xi,t, f,, αt )∣
√U i, t	2υ i, t
2
=f, (Xi,t)--7==hVf, (Xi, t), GX (Xi, t, f,, αt )i + TΓτA IlGX (Xi,t, f,, αt )∣∣2
√U i, t	2υ i, t
+--T= hδi, t, GX (Xi,t, f,, α )i,
√υ i, t
1	y~l /	^7∙	_ . ∖ Λ	.	. 1	∙ . Λ . Λ . ∙	Λ ∙	Λ
where GX(Xi,t,fi,t, αt) denotes the projected stochastic gradient on node i.
≤ b而t
□
25
Under review as a conference paper at ICLR 2019
The above inequality together with Lemma 21, equation 11 and β1,t ≤ β1, imply that
α at 、/工 at a	αt(2- βI)II2	~∖ι∣2 l	αtβ1,tU
f,t (xi,t+ι) ≤ f,t (xi,t)	2υ	k Gχ (χi,t,f ,t,α川 + 2(ι - 0] ,(I - η)2(i - 0?)
2
+ -2Γ⅛ kGX (xi,t, f,t, αt )k2 + ■—7=hδi, t, G X (xi,t, ft αt )i
2υ	V υi,t
z t /	7	_. ∖ ∖
GX (xi,t, fi,t, αt)i
+
~J= hδi,t, GX (xi,t, ft, αt) -
√υ i, t
≤ f (xi, t)-kGX (xi,t,f,αt)k2 + 2(1-β1,t)(1-1 η )2(1-β2)
2
+ -27⅛ kGX (xi,t, f,, αt )k2 + ■—7=hδi, t, G X (xi,t, f,, αt )i
2υ	V υi,t
+ Tt= kδi, t kkGX (xi,t, f,, αt) - G X (xi,t ,f,t, αt )k
√υ i, t
≤ f, (xi,t) - αt (；U β1) k GX (xi, t, f,, αt )k2 +
α βι,t U
2(1-01,t)(1 - η)2(1-02)
2
+ ~^2 kGX (xi,t, f,, αt )k2 +	75^hδi, t, G X (xi,t, ft, αt )i
2υ	V Ui,t
+ p⅜ Mi,tk" ∑(1 - β1, r )βlt-rkδiz k,
(66)
where the second inequality follows from the Cauchy-Schwartz inequality. The last inequality
follows from the fact that 01,t = 01λt-1,λ ∈ (0, 1) and Lemma 19 if We Set Gɪ (xi, fi, α) =
GX (xi, t, f,t, a), and G X (xi ,f, α) = G X (xi, t, f,t, αt).
Let ∆i,t = fi,t (xi,t) - fi,t (x；) denotes the instantaneous stochastic loss at time t. We have
t1
δ i, t +1 = T~7 ( f, (xi, t +1) - f, (xt+1))+7TT( fi, t+1( xi, t +1) - fi, t +1 (xt+1)).	(67)
t 十 1、	-Y-	'	J t 十 1
(I)
Observe that (I) can be bounded as folloWs:
f,t (xi, t+1) - f, (x；+1) ≤ f, (xi, t+1) - f, (x；) ≤ A i, t - [ ^~~2υUɑ-t - 2υt2 ]k G X (xi, t,f,, αt )k2
+ 2(1-β1,t)αf-tυ)2(1-β2) + p⅜也1 GX(xi,t, f t, αt)i + 箸∑1 底-rkδi,rkkδi,tk,	(68)
Where the first inequality is due to xt；+1 ∈ X and the optimality of xt； and the second inequality is
due to equation 66 and the fact that 0 ≤ 01,r < 1. Thus, using equation 67 and equation 68, We get
M+1 ≤ S (Ai,t - [(2 ZU1"t - pl2]k GX (xi, t tf t, αt )k2 + 2(1-β1,t)αf- 'υ)2(1-β2)
+ 产—hδi,t, GX (xi,t, fi,t, αt )i + 骼 ∑ β1-rkδi,rkkδi,tk) + 士 (fit t+1(xi,t+1) - fit+1( x；+1)).
VZU i, t	U r =1	1 t 十 1
By rearranging the above inequality, We have
£ [(2 -U1)αt - Pir ]k GX (xi,t, ft,at )k2 ≤ S (Ai,t + 2(1-β1∕ ：Ie- 1；)2(1- 02)
α*	一	U α* L	∖
+ »hδi, t, G X (xi,t ,f, αt )i + --α^ ∑ βt-rkδi,rkkδi,tk)
V U i, t	U r=1	'
+ t+1 ( fi,t+1(xi,t +1) - fi,t +1(x；+1)) - Ai,t+1∙	(69)
26
Under review as a conference paper at ICLR 2019
Now, using defiηitiοη of Ai,t+1,圭 (f；t+i(Xi,t+1) - f∙,t+ιx+1)) - ∆t+1 = T 击)(f t (Xi,t+1) -
fi,t (x^+1)), together with equation 69, We obtain
[^~~2^ι)t - 2υt2]kGx(xi,t，f,，αt)k2 ≤ δi,t - (f (x"+1) - f,(X；+1))
+ 2(1-β1,t );f-'υ)2(1-β2) + p⅝ 回,GX (xi,t, f t, α) + Ua ∑1 β1 - r kδi,r kkδ-t k.	(70)
Note that from Assumption 3, we have E [(δ",GX (Xi,t, f, at))] = 0 and
E [kV fi,t(xi,t)- V fi,t(xi,t )k2] = E [kV ft(Xi,t )k2] -kE [V ft(Xi,t)] k2 ≤ E [kV ft(Xi,t )k2] ≤ ξ 2,
which implies that
E[kδ∙,tk2] = E [kV f,t(Xi,t) - V f,t( Xi, t )k2]
1t
=E k7 ∑(Vf s(xi,t) - Vf s(xi,t))k2
ts=1
1t
≤ t ∑ E [kVf s(xi,t)-Vf S(xi,t)k2] ≤ ξ2.
ts=1
The above inequality together with Cauchy-Schwarz for expextations, imply that
E[kδi,t kkδi,rk] ≤ qE[kδi,rk2] ,E[kδi,tk2] ≤ ξ 2.	(71)
Now, using equation 71 and equation 62, summing equation 70 over i ∈ V and t ∈ {1, . . . , T}, and
taking expectation, we obtain
n∑ t 措%}叫GX (Xi,t, f, at )k2] t∑ [≡Pat -嘉]
≤t∑[(ɪ -空η∑E[kGX(Xi,t, ff,αa)k2]
≤ (2+log r )2 Lt ∈max η (1-η )2pnι-β2) SE aS σ2-s-1")
+ ∑_________a β1,t U________L υυξ2 ∑ a ∑ β t - r	(72)
+1∑2(1 -βι,t)(1 -η)2(l-β2)+ υ2 ∑Rβ1 .	(7 )
Note that ∑T=ι[(2-⅛ - ρ∣2] > 0, and
Tt	TT	T
∑ a ∑ β1-r = ∑ at ∑ βL ≤ ∑ ɪ.	(73)
t=1 r=1	t=1 r=t	t=1 ( - β1)
Therefore, dividing both sides of the equation 75 by ∑3[(2-2U1)% - Pa2]，using equation 73, we
complete the proof.	口
6.1.6 Proof of Corollary 11
Proof. Let ⅛t = ∑T=I [(2-βU)at -第].We claim that if
… r 4ρ 2U2	4U2 n 、	,.
≥ max{ nυ4(2 - βι)2, (2 - βι)2 },	(74)
then ⅛t ≥ 1/2. One can easily seen that T must satisfy (2-ββpat - Pa2 ≥ 2T which results in
-TPa2U + Tυ2(2 - βι)at - Uυ2 ≥ 0.
27
Under review as a conference paper at ICLR 2019
By rearranging the above inequality, we obtain
Y	Pat υ	υ	,	,
1 ≥ U2(2-β1) + T(2-βι)at =4+42.
Now, if A 1, A 2 ≤ 2, then a ≤ U 22-β1), and at ≥ T(2%),respectively. These bounds together with
our assumption on at give equation 74.
Now, let equation 74 holds. Using equation 70, and since dt ≥ 1/2, we have
1n
T工 t ∈min 7｝叫品 g，f t，ɑa)k2]
≤ ⅛∑ tHT即GX	，f，aaM ,£ [≡F — 票]
≤ 2 ∑ [(ɪ -唔]1 ∑ E[kGX (xi,t, fit at )k2]
t=1	i =1
2n	T
≤ ψ- ∑ (f；I(XiiI)- fi,1(X 1)) + L(Ilχi,T+1 - χT+1k + ∑ 21-1kχi,t - x?k)
Tn i=1	t=2
+ 2 T__________aβ1,tU__________I	2υξ 2	T a
+T==1 2(1-β1,t)(1-η)2(1-β2)+U2(1-β1)T ∑at,
where the last inequality follows from equation 60, equation 61 and equation 71.
(75)
We proceed to bound RHS of equation 75. By substituting a = √T and β1,t = β1λt-1, λ ∈ (0,1)
into the last two terms of equation 75, we have
2U ξ2	T _	2αU ξ2
υ2(1 -β1)T ∑ɑt = u2(1-β1)√TT
and
1 f	at β1, tU	_	aυ	+ β1, t
T ∑ (1-β1,t)(1-η)2(1-β2) = T√T(1-η)2(1-β2) t∑dJ
—
_	au
=T √T (1-η )2 (1-β2)(1-β1)(1-λ)
Further, using equation 75 and Lemma 20, we get
T⅛ ∑k xi, T+1- x t+1k ≤ 斤I—pn7ff^√~τ∑ ∑ σ2-S (W)
Tn i=1	T(1 - η) (1 - β2) nT s=0
<	2√naL
≤ T(1 - η)√(1-β2)√nT(1 - σ2(W))'
(76)
(77)
(78)
Similarly, using equation 75 and Lemma 20, we have
LnT
T~ ΣΣ t-1 k xi, t - x*tk ≤
Tn i=1 t=2
2 √n a L
T	t-1
=—	,______ ∑ t-1 ∑ σ2-s-1 (W)
刷(1 - η )√(T-β2) t∑	s=0 2 J
2 √n a L
≤ √T(1-η)√(1-β)T
Γr	I t t-1
= t-2 = (= σ2t-s-1(W))2
t=2	t=2 s=0
V__________2 √n a L_______
≤ √nT(1 -η)√(1 -β2)T(1 -σ2(W)),
(79)
where the second inequality is due to Cauchy-Schwarz inequality and the last inequality follows
because ∑=1	≤ 1.
Using equation 16a, equation 77, equation 78 and equation 79 are bounded by equation 76. This
completes the proof.	□
28
Under review as a conference paper at ICLR 2019
6.2 Sensitivity of DADAM to its parameters
Next, we examine the sensitivity of DADAM on the parameters related to the network connection
and update of the moment estimate. As it is clear from Figure 3 the convergence of training accuracy
value happens faster for sparser networks (higher σ2(W)). This is similar to the trend observed for
FedAvg algorithm while reducing parameter C which makes the agent interaction matrix sparser.
This is also expected as discussed in Theorems 4 and 5. Note that with the availability of a central
parameter server (as in FedAvg algorithm), sparser topology may be useful for a faster convergence,
however, topology density of graph is important for a distributed learning scheme with decentralized
computation on a network.
Q 8 6 4 2.0
Lo.o,o.o,6
>U2T-UuraCnu⊂-E 匕
— Sparse Network c⅛ =0.76
0	5	10	15	20	25	30
epoch
0.5.05
NLL0.
5s°6u∙≡ra上
Figure 3: Performance of the DADAM algorithm with varying network topology: training loss and
accuracy over 30 epochs based on the MNIST digit recognition library.
We also empirically evaluate the effect of the β3 in Algorithm 1. We consider a range of hyper-
parameter choices, i.e. β3 ∈ {0, 0.9, 0.99}. From Figure 4 it can be easily seen that DADAM per-
Figure 4: Performance of the DADAM algorithm with varying decay rate β3. DADAM1 (β3 = 0),
DADAM2 (β3 = 0.9), and DADAM3 (β3 = 0.99) for training loss and accuracy over 30 epochs
based on the MNIST digit recognition library.
forms equal or better than AMSGrad (β3 = 0), regardless of the hyper-parameter setting for β1 and
β2.
29