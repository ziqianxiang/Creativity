Under review as a conference paper at ICLR 2019
Adversarial Attacks on Node Embeddings
Anonymous authors
Paper under double-blind review
Ab stract
The goal of network representation learning is to learn low-dimensional node em-
beddings that capture the graph structure and are useful for solving downstream
tasks. However, despite the proliferation of such methods, there is currently no
study of their robustness to adversarial attacks. We provide the first adversarial
vulnerability analysis on the widely used family of methods based on random
walks. We derive efficient adversarial perturbations that poison the network struc-
ture and have a negative effect on both the quality of the embeddings and the
downstream tasks. We further show that our attacks are transferable since they
generalize to many models, and are successful even when the attacker is restricted.
1	Introduction
Unsupervised node embedding (network representation learning) approaches are becoming increas-
ingly popular and achieve state-of-the-art performance on many network learning tasks (Cai et al.,
2017). The goal is to embed each node in a low-dimensional feature space such that the graph’s
structure is captured. The learned embeddings are subsequently used for downstream tasks such
as link prediction, node classification, community detection, and visualization. Among the vari-
ety of proposed approaches, techniques based on random walks (RWs) (Perozzi et al.; Grover &
Leskovec) are highly successful since they incorporate higher-order relational information. Given
the increasing popularity of these method, there is a strong need for an analysis of their robustness.
In particular, we aim to study the existence and effects of adversarial perturbations. A large body
of research shows that traditional (deep) learning methods can easily be fooled/attacked: even slight
deliberate data perturbations can lead to wrong results (Goodfellow et al., 2014; Mei & Zhu, 2015;
Carlini & Wagner, 2017; Liang et al., 2017; Cisse et al., 2017; Lin et al., 2017; Chen et al., 2017a).
So far, however, the question of adversarial perturbations for node embeddings has not been ad-
dressed. This is highly critical, since especially in domains where graph embeddings are used (e.g.
the web) adversaries are common and false data is easy to inject: e.g. spammers might create fake
followers on social media or fraudsters might manipulate friendship relations in social networks.
Can node embedding approaches be easily fooled? The answer to this question is not immediately
obvious. On one hand, the relational (non-i.i.d.) nature of the data might improve robustness since
the embeddings are computed for all nodes jointly rather than for individual nodes in isolation. On
the other hand, the propagation of information might also lead to cascading effects, where perturba-
tions in one part of the graph might affect many other nodes in another part of the graph.
Compared to the existing works on adversarial attacks our work significantly differs in various as-
pects. First, by operating on plain graph data, we do not perturb the features of individual instances
but rather their interaction/dependency structure. Manipulating the structure (the graph) is a highly
realistic scenario. For example, one can easily add or remove fake friendship relations on a social
network, or write fake reviews to influence graph-based recommendation engines. Second, the node
embedding works are typically trained in an unsupervised and transductive fashion. This means that
we cannot rely on a single end-task that our attack might exploit to find appropriate perturbations,
and we have to handle a challenging poisoning attack where the model is learned after the attack.
That is, the model cannot be assumed to be static as in most other adversarial attack works. Lastly,
since graphs are discrete classical gradient-based approaches (Li et al.; Mei & Zhu, 2015) for find-
ing adversarial perturbations that were designed for continuous data are not well suited. Particularly
for RW-based methods, the gradient computation is not directly possible since they are based on
a non-differentiable sampling procedure. How to design efficient algorithms that are able to find
adversarial perturbations in such a challenging - discrete and combinatorial - graph domain?
1
Under review as a conference paper at ICLR 2019
We propose a principled strategy for adversarial attacks on unsupervised node embeddings. Ex-
ploiting results from eigenvalue perturbation theory (Stewart, 1990) we are able to efficiently solve
a challenging bi-level optimization problem associated with the poisoning attack. We assume an
attacker with full knowledge about the data and the model, thus, ensuring reliable vulnerability
analysis in the worst case. Nonetheless, our experiments on transferability demonstrate that our
strategy generalizes - attacks learned based on one model successfully fool other models as well.
Overall, we shed light on an important problem that has not been studied so far. We show that node
embeddings are sensitive to adversarial attacks. Relatively few changes are needed to significantly
damage the quality of the embeddings even in the scenario where the attacker is restricted. Further-
more, our work highlights that more work is needed to make node embeddings robust to adversarial
perturbations and thus readily applicable in production systems.
2	Related work
We focus on adversarial attacks on unsupervised node embedding approaches based on random
walks (RWs), and further show how one can easily apply a similar analysis to attack other node
embeddings based on factorization. For a recent extensive survey, also of other non-RW based
approaches, we refer to Cai et al. (2017). Moreover, while many (semi-)supervised learning methods
(Kipf & Welling, 2016; Defferrard et al.) have been introduced, we focus on unsupervised methods
since they are often used in practice due to their flexibility in solving various downstream tasks.
Adversarial attacks. Attacking machine learning models has a long history, with seminal works
on SVMs and logistic regression (Biggio et al., 2012; Mei & Zhu, 2015). Deep neural networks
were also shown to be highly sensitive to small adversarial perturbations to the input (Szegedy et al.,
2013; Goodfellow et al., 2014). While most works focus on image classification, recent works have
shown the existence of adversarial examples also in other domains (Liang et al., 2017; Grosse et al.).
Different taxonomies exist characterizing the attacks/adversaries based on their goals, knowledge,
and capabilities (Biggio et al.; Papernot et al.; Mufioz-Gonzalez et al., 2017). The two dominant
attacks types are poisoning attacks that target the training data (the model is trained after the attack)
and evasion attacks that target the test data/application phase (the learned model is assumed fixed).
Compared to evasion attacks, poisoning attacks are far less studied (Koh & Liang, 2017; Mufioz-
Gonzalez et al., 2017; Li et al.; Mei & Zhu, 2015; Chen et al., 2017a) since they require solving a
challenging bi-level optimization problem.
Attacks on semi-supervised graph models. The robustness of semi-supervised graph classification
methods to adversarial attacks has recently been analyzed (Zugner et al., 2018; Dai et al., 2018a).
The first work, introduced by Zugner et al. (2018), linearizes a graph convolutional network (GCN)
(Kipf & Welling, 2016) to derive a closed-form expression for the change in class probabilities
for a given edge/feature perturbation. They calculate a score for each possible edge flip based on
the classification margin and greedily pick the top edge flips with highest scores. Later, Dai et al.
(2018a) proposed a reinforcement (Q-)learning formulation where they decompose the selection of
relevant edge flips into selecting the two end-points. Both approaches focus on targeted attacks
(misclassify a given node) for the semi-supervised graph classification task. In contrast, our work
focuses on general attacks (decrease the overall quality) on unsupervised node embeddings.
Manipulating graphs. In the context of graph clustering, Chen et al. (2017b) measure the changes
in the result when injecting noise to a bi-partite graph of DNS queries, but do not focus on automati-
cally generating attacks. There is an extensive literature on works that optimize the graph structure to
manipulate e.g. information spread in a network (Chen et al.; Khalil et al.), user opinions (Amelkin
& Singh, 2017; Chaoji et al.), shortest paths (Phillips; Israeli & Wood), page rank scores and other
metrics (Avrachenkov & Litvak; Chan et al.). Remotely related are poisoning attacks on multi-task
relationship learning (Zhao et al., 2018). While they exploit the relations between different tasks,
they still deal with the classical scenario of i.i.d. instances within each task.
Robustness and adversarial training. The robustification of machine learning models has also
been studied - known as adversarial machine learning or robust machine learning. Such approaches
are out of scope for this paper and we do not discuss them. The goal of adversarial training (e.g. via
GANs (Dai et al., 2018b)) is to improve the embeddings, while our goal is to damage the embeddings
produced by existing models by perturbing the graph structure.
2
Under review as a conference paper at ICLR 2019
3	Attacking node embeddings
Here We explore poisoning attacks on the graph structure - the attacker is capable of adding or re-
moving (flipping) edges in the original graph within a given budget. We focus mainly on approaches
based on random Walks and extend the analysis to spectral approaches (Sec. 6.2 in the appendix).
3.1	Background and preliminaries
Let G = (V, E) be an undirected unWeighted graph Where V is the set of nodes, E is the set of
edges, and A ∈ {0,1}|V l×lV | is the adjacency matrix. The goal of network representation learning
is to find a loW-dimensional embedding zv ∈ RK for each node With K |V |. This dense loW-
dimensional representation should preserve information about the network structure - nodes similar
in the original network should be close in the embedding space. DeepWalk (Perozzi et al.) and
node2vec (Grover & Leskovec) learn an embedding based on RWs by extending and adapting the
skip-gram architecture (Mikolov et al., 2013) for learning word embeddings. They sample finite
(biased) RWs and use the co-occurrence of node-context pairs in a given window in each RW as a
measure of similarity. To learn zv they maximize the probability of observing v’s neighborhood.
3.2	Attack model
We denote with A the adjacency matrix of the graph obtained after the attacker has modified certain
entries in A. We assume the attacker has a given, fixed budget and is only capable of modifying
f entries, i.e. ||A — A||o = 2f (We have 2f since G is undirected). The goal of the attacker is to
damage the quality of the learned embeddings, which in turn harms subsequent learning tasks such
as node classification or link prediction that use the embeddings as features. We consider both a
general attack that aims to degrade the embeddings of the network as a whole, as well as a targeted
attack that aims to damage the embedding regarding a specific target or specific task.
The quality of the embeddings is measured by the loss L(A, Z) of the model under attack, with lower
loss corresponding to higher quality, where Z ∈ RN ×K is the matrix containing the embeddings
of all nodes. Thus, the goal of the attacker is to maximize the loss. We can formalize this as the
following bi-level optimization problem:
A = arg max	L(A,Z ) Z = mm L(A,Z) subj. to ||A - A||o = 2f , A = AT (1)
A∈{0,i}N ×N	Z
Here, Z* is always the 'optimal' embedding resulting from the (to be optimized) graph A, i.e. it
minimizes the loss, while the attacker tries to maximize the loss. Solving such a problem is highly
challenging given its discrete and combinatorial nature, thus we derive efficient approximations.
3.3	General attack
Since the first step in the embedding approaches is to generate a set of random walks that serve as
a training corpus for the skip-gram model, the bi-level optimization problem is even more compli-
cated. We have Z* = mmz L({r1,r2,... },Z) with r ~ RWl(A), where RWl is an intermediate
stochastic procedure that generates RWs of length l given the graph A which we are optimizing. By
flipping (even a few) edges in the original graph, the attacker necessarily changes the set of possi-
ble RWs, thus changing the training corpus. Therefore, this RW generation process precludes any
gradient-based methods. To tackle this challenge we leverage recent results that show that (given
certain assumptions) RW based node embedding approaches are implicitly factorizing the Point-
wise Mutual Information (PMI) matrix (Yang & Liu, 2015; Qiu et al., 2017). We study DeepWalk
as an RW-based representative approach since it’s one of the most popular methods and has many
extensions. Specifically, we use the results from Qiu et al. (2017) to sidestep the RW stochasticity.
Lemma 1 (Qiu et al. (2017)). DeepWalk is equivalent tofactorizing M = log(max(M, 1)) with
T
M = voTA) S, where S = (XPr)D-1, where P = DTA	(2)
r=1
where the embedding Z* is obtained by the Singular Value Decomposition of M = UΣVT using
the top-K largest singular values /vectors, i.e. Z* = UK Σ%2.
3
Under review as a conference paper at ICLR 2019
Here, D is the diagonal degree matrix with Dii = j Aij , T is the window size, b is the number of
negative samples and vol(A) = Pi,j Aij is the volume. Since M is sparse and has many zero entries
the matrix log(M) where the log is elementwise is ill-defined and dense. To cope with this, similar
to the Shifted Positive PMI (PPMI) approach the elementwise maximum is introduced to form MM.
Using this insight, We see that DeepWalk is equivalent to optimizing min^ ||M - MMK∣∣F where
MK is the best rank-K approximation to M. This in turn means that the loss for DeepWalk when
using the optimal embedding Z* for a given graph A is Ldwi (A, Z*) = ʌ/PPV=K+ι σp where σp
are the singular values of M(A) ordered decreasingly σι ≥ σ ∙∙∙ ≥ σ∣y∣. This result shows that
we do not need to construct random walks, nor do we have to (explicitly) learn the embedding Z*
一 it is implicitly considered Via the singular values of M(A). Accordingly, we have transformed
the bi-level problem into a single-level optimization problem. However, maximizing LDW1 is still
challenging due to the singular value decomposition and the discrete nature of the problem.
Gradient based approach. Maximizing LDW1 with a gradient-based approach is not straightfor-
ward since we cannot easily backpropagate through the SVD. To tackle this challenge we exploit
ideas from eigenvalue perturbation theory (Stewart, 1990) to approximate LDW1 (A) in closed-form
without needing to recompute the SVD. This enables us to efficiently calculate the gradient.
Theorem 1. Let A be the initial adjacency matrix and M(A) be the respective co-occurrence ma-
trix. Let up be the p-th eigenvector corresponding to the p-th largest eigenvalue of M. Given a
perturbed matrix A , with A = A + ∆A, and the respective Change ∆M. We can approximately
compute the loss: Ldwi (A0) ≈ JPN=K+1 (UT(M + ∆M)up)2 =: Ldw? (A) and the approxi-
mation error is bounded by ∣Ldwi (Ar) — Ldw2 (A0)∣ ≤ ∣∣∆M||f.
The proof is given in the appendix. For a small ∆A and thus small ∆M we obtain a very good ap-
proximation, and if ∆A = ∆M = 0 then the loss is exact. Intuitively, we can think of using eigen-
value perturbation as analogous to taking the gradient of the loss w.r.t. M (A). Now, gradient-based
optimization is efficient since ^aLdw2 (A) avoids recomputing the eigenvalue decomposition. The
gradient provides useful information fora small change, however, here we are considering discrete
flips, i.e. = ±1 so its usefulness is limited. Furthermore, using gradient-based optimization re-
quires a dense instantiation of the adjacency matrix, which has complexity O(N 2) in both runtime
and memory (infeasible for large graphs). This motivates the need for our more advanced approach.
Sparse closed-form approach. Our goal is to efficiently compute the change in the loss LDw1 (A)
given a set of flipped edges. To do so we will analyze the change in the spectrum of some of the
intermediate matrices and then derivate a bound on the change in the spectrum of the co-occurrence
matrix, which in turn will give an estimate of the loss. First, we need some results.
Lemma 2. The matrix S in Eq. 2 is equal to S = U (PrT=1 Λr)UT where the matrices U and Λ
contain the eigenvectors and eigenvalues solving the generalized eigen-problem Au = λDu.
The proof is given in the appendix. We see that the spectrum ofS (and, thus, the one ofM by taking
scalars into account) is obtainable from the generalized spectrum ofA. The difference to Qiu et al.
(2017)’s derivation where a factorization of S using Anorm := D-1/2AD-1/2 is important. As we
will show, our formulation using the generalized spectrum ofA is key for an efficient approximation.
Let A0 = A + ∆A be the adjacency matrix after the attacker performed some edge flips. As above,
by computing the generalized spectrum of A0, we can estimate the spectrum of the resulting S0 and
M0 . However, recomputing the eigenvalues λ0 of A0 for every possible set of edge flips is still not
efficient for large graphs, preventing an effective application. Thus, we derive our first main result:
an efficient approximation bounding the change in the singular values of M0 for any edge flip.
Theorem 2. Let ∆A be a matrix with only 2 non-zero elements, namely ∆Aij = ∆Aji = 1 - 2Aij
corresponding to a single edge flip (i, j), and ∆D the respective change in the degree matrix, i.e.
A0 = A + ∆A and D0 = D + ∆D. Let uy be the y-th generalized eigenvector of A with generalized
eigenvalue λy. Then the generalized eigenvalue λ0y ofA0 solving λ0y A0 = λ0y D0u0y is approximately:
Xy ≈ λy = Xy + △%	△% = ʌwij (2uyi ∙ Uyj - λy (Uyi + Uyj)	(3)
where uyi is the i-th entry of the vectoruy, and ∆wij = (1 - 2Aij) indicates the edge flip, i.e ±1.
4
Under review as a conference paper at ICLR 2019
The proof is provided in the appendix. By working with the generalized eigenvalue problem in
Theorem 2 we were able to express A0 and D0 after flipping an edge as additive changes to A
and D, this in turn enabled us to leverage results from eigenvalue perturbation theory to efficiently
approximate the change in the spectrum. If we used Anorm instead, the change to A0norm would
be multiplicative preventing efficient approximations. Using Eq. 3, instead of recomputing λ0 we
only need to compute ∆λ, significantly reducing the complexity when evaluating different edge flips
(i, j). Using this result, we can now efficiently bound the change in the singular values of S0.
Lemma 3. Let A0 be defined as before and S0 be the resulting matrix. The singular values of S0 are
bounded: σp(S0) ≤ σp(i,j) := d1- ∙ IPT=ι(λ[())r ∣ where π is a permutation simply ensuring
min	π(p)
that the final σp(i,j) are Sorted decreasingly, where d'γnin is the smallest degree in A0.
We provide the proof in the appendix. Using this result, we can efficiently compute the loss for a
rank-K approximation/factorization of M 0, which we would obtain when performing the edge flip
(i,j), i.e. Ldw3(A0) = To(AT+^w [pp=lK+1 σp(i, j)2]1/2. While the original loss Ldwi is
based on the matrix M = log(max(M, 1)), there are unfortunately currently no tools available to
analyze the spectrum of M given the spectrum of M . Therefore, we use LDW3 as a surrogate loss
for LDW1 (Yang et al. similarly exclude the element-wise logarithm). As our experimental analysis
shows, the surrogate loss is effective and we are able to successfully attack the node embeddings
that factorize the actual co-occurrence matrix MM, as well as the original skip-gram model. Similarly,
methods based on spectral embedding, factorize the graph Laplacian and have a strong connection
to the RW based approaches. We provide a similar detailed analysis in the appendix (Sec. 6.2).
The overall algorithm. Our goal is to maximize LDW3 by performing f edge flips. While Eq. 3
enables us to efficiently compute the loss fora single edge, there are still O(n2) possible flips. To re-
duce the complexity when adding edges (see Sec. 4.2 for removing) we instead form a candidate set
by randomly sampling C candidate flips. This introduces a further approximation that nonetheless
works well in practice. For every candidate we compute its impact on the loss via LDW3 and greedily
choose the top f flips.1 The runtime complexity of our overall approach is: O(N ∙∖E | + C ∙ N log N).
First, We can compute the generalized eigenvectors of A ina sparse fashion in O(N ∙ ∖E∖). Then We
sample C candidate edges, and for each we can compute the approximate eigenvalues in constant
time (Theorem 2). To obtain the final loss, We sort the values leading to the overall complexity. The
approach is easily parallelizable since every candidate edge flip can be evaluated in parallel.
3.4	Targeted attack
If the goal of the attacker is to attack a specific node t ∈ V , called the target, or a specific doWn-
stream task, it is suboptimal to maximize the overall loss via Ldw* . Rather, we should define some
other target specific loss that depends on t's embedding - replacing the loss function of the outer
optimization in Eq. 1 by another one operating on t’s embedding. Thus, for any edge flip (i,j) we
now need the change in t's embedding - meaning changes in the eigen vectors - which is inher-
ently more difficult to compute compared to changes in eigen/singular-values. We study two cases:
misclassifying a target node and manipulating the similarity of node pairs (i.e. link prediction task).
Surrogate embeddings. To efficiently compute the change in eigenvectors, we define surrogate
embeddings Z*. Specifically, instead of performing an SVD decomposition on M (or equivalently S
T
with upscaling) and using the results from Lemma 2 we define Z* = U(Er=1 Λr). Experimentally,
using Z* instead of Z* as the embedding showed no significant change in the performance on
downstream tasks (even on the clean graph; suggesting its general use since it is more efficient to
compute). Now, we can approximate the generalized eigenvectors, and thus Z* (A0), in closed-form:
Theorem 3. Let ∆A, ∆D and ∆wij be defined as before, and ∆λy be the change in the y-th
generalized eigenvalue λy as derived in Theorem 2. Then, the y-th generalized eigenvector u0y of A0
after performing the edge flip (i, j) can be approximated with:
uy ≈ uy - ∆wij (A - λD) (-∆λy uy ◦ d + Ei (uyj - λy uyi ) + Ej (uyi - λy uyj ))	(4)
where Ei(x) returns a vector of zeros except at position i where the value is x, d is a vector of the
node degrees, ◦ is the Hadamard product, and (∙)+ is the pseudo inverse.
1A greedy approach where we sequentially flip the single best candidate, followed by a periodic recompu-
tation of the eigenvalues did not show benefits, nor did an evolutionary strategy using Theorem 2 as a heuristic.
5
Under review as a conference paper at ICLR 2019
We provide the proof in the appendix. Computing Eq. 4 seems expensive at first due to the pseudo
inverse term. However, note that this term does not depend on the particular edge flip we perform.
Thus, we can pre-compute it once and furthermore, parallelize the computation for each y. Similarly,
we can pre-compute uyd, while the rest of the terms are all computable in O(1). For any edge flip
We can now efficiently compute the optimal embedding Z* (A0) using Eqs. 3 and 4. The t-th row of
Z* (A0) is the desired embedding for a target node t after the attack.
Targeting node classification. The goal is to enforce misclassification of the target t for the down-
stream task of node classification (i.e. node labels are partially given). To fully specify the targeted
attack we need to define the candidate flips and the target-specific loss responsible for scoring the
candidates. As candidates we use {(v, t)|v 6= t}. For the loss, we first pre-train a classifier C on the
clean embedding Z*. Then we predict the class probabilities Pt of the target t using the compro-
mised Zi. and we calculate the classification margin m(t) = Pt,c(t)- maxc=c(t)pt,c, where c(t) is
the ground-truth class for t. That is, our loss is the difference between the probability of the ground
truth and the next most probable class after the attack. Finally, we select the top f flips with smallest
margin m (note when m(t) < 0 node t is misclassified). In practice, we average over 10 randomly
trained classifiers. Another (future work) approach is to treat this as a tri-level optimization problem.
Targeting link prediction. The goal of the attack is: given a set of target node pairs T ⊂ V × V ,
decrease the similarity between the nodes that have an edge, and increase the similarity between
nodes that do not have an edge, by modifying other parts of the graph 一 i.e. it is not allowed to
directly flip pairs in T. For example, in an e-commerce graph representing users and items, the goal
might be to increase the similarity between a certain item and user, by adding/removing connections
between other users/items. To achieve this, we first train the initial clean embedding without the
target edges. Then, for a candidate set of flips, we estimate Z* using Eqs. 3 and 4 and use them to
calculate the average precision score (AP score) on the target set T, with Z；(Z；)T as a similarity
measure. Finally, we pick the top f flips with lowest AP scores and use them to poison the network.
4	Experimental evaluation
Since this is the first work considering adversarial attacks on node embeddings there are no known
baselines. Similar to works that optimize the graph structure (Chen et al.) we compare with several
strong baselines. Brnd randomly flips edges (we report averages over ten seeds), Beig removes edges
based on their eigencentrality in the line graph L(A), and Bdeg removes edges based on their degree
centrality in L(A) - or equivalently sum of degrees in the original graph. When adding edges we
use the same baselines as above, now calculated on the complement graph, except for Beig since
it is infeasible to compute even for medium size graphs. ADW2 denotes our gradient based attack,
ADW3 our closed-form attack, Alink our link prediction attack, Aclass our node classification attack.
The size of the sampled candidate set for adding edges is 20K (for removing edges see Sec. 4.2).
We aim to answer the following questions: (Q1) how good are our approximations of the loss; (Q2)
how much damage is caused to the embedding quality by our attacks/baselines; (Q3) can we still per-
form a successful attack when restricted; (Q4) what characterizes selected (top) adversarial edges;
(Q5) how do the targeted attacks affect downstream tasks; and (Q6) are the attacks transferable.
We set DeepWalk’s hyperparameters to: T = 5, b = 5, K = 64 and use a logistic regression for
classification. We analyze three datasets: Cora (N = 2810, |E| = 15962, McCallum et al.), Citeseer
(N = 2110, |E| = 7336, Giles et al.), and PolBlogs (N = 1222, |E| = 33428, Adamic & Glance
(2005)). In all experiments, after choosing the top f flips we retrain the embeddings and report the
final performance since this is a poisoning attack. Note, for the general attack, the downstream node
classification task is only a proxy for estimating the quality of the embeddings after the attack, it is
not our goal to damage this task, but rather to attack the unsupervised embeddings in general.
4.1	Approximation quality
To estimate the approximation quality we randomly select a subset of 20K candidate flips and com-
pute the correlation between the actual loss and our approximation as measured by Pearson’s R
score. For example, for K = 32 we have R(LDW2 , LDW1 ) = 0.11 and R(LDW3 , LDW1 ) = 0.90,
clearly showing that our closed-form strategy approximates the loss significantly better compared to
the gradient-based one. Similarly, LDW3 is a better approximation than LDW2 for K = 16, 64, 128.
6
Under review as a conference paper at ICLR 2019
Bdeg	M	Beig
Ad W2	A	Adw3	▼	Aabr	★	Brnd
3s°1,b37b374
0」。。Sl ⅛
-1000-800 -600 -400 -200 200 400 600 800 1000	-1000-800 -600 -400 -200 200 400 600 800 1000	-1000-800-600-400-200 200 400 600 800 1000	-1000-800 -600-400-200 200 400 600 800 1000
12,5 10.0 7.5 5.0 2.5	2.5 5.0 7,5 10.0 12.5	12.5 10.0 7.5 5.0 2.5	2.5 5.0 7,5 10.0 12.5	6.0 4.8 3.6 2.4 1.2	1.2 2.4 3.6 4.8 6.0	3.1 2.5 1.9 1.3 0.6	0.6 1.3 1.9 2.5 3.1
number of flips / % edges	number of flips / % edges	number of flips / % edges	number of flips / % edges
(a) Cora: DeePWaIk's loss (b) Cora: classification (c) PolBlog: classification (d) Cora: restricted class.
Figure 1: Vulnerability of the embeddings under the general attack for increasing number of flips.
The dotted line shows the performance before attacking.
4.2 General attack
To obtain a better understanding We investigate the effect of removing and adding edges separately.
Since real graphs are usually sparse, for removing we set the candidate set to be the set of all edges,
with one edge set aside for each node to ensure we do not have singleton nodes. To obtain candidate
edges for adding we randomly sample a set of edges. We then simply select the top f edges from
the candidate set according to our scoring function. For adding edges, we also implemented an
alternative add-by-remove strategy denoted as Aabr . Here, we first add cf -many edges randomly
sampled from the candidate set to the graph and subsequently remove (c - 1)f -many of them. This
strategy performed better empirically. Since the graph is undirected, for each (i,j) we also flip (j, i).
Fig. 1 answers question (Q2). Removed/added edges are denoted on the x-axis with negative/positive
values respectively. On Fig. 1awe see that our strategies achieve a significantly higher loss compared
to the baselines when removing edges. To analyze the change in the embedding quality we consider
the node classification task (i.e. using itas a proxy to evaluate quality; this is not our targeted attack).
Interestingly, Bdeg is the strongest baseline w.r.t. to the loss, but this is not true for the downstream
task. As shown in Fig. 1b and 1c, our strategies significantly outperform the baselines. As expected,
ADW3 and Aabr perform better than ADW2 . On Cora our attack can cause up to around 5% more
damage compared to the strongest baseline. On PolBlogs, by adding only 6% edges we can decrease
the classification performance by more than 23%, while being more robust to removing edges.
Restricted attacks. In the real world, attackers cannot attack any node, but rather only specific
nodes under their control, which translates to restricting the candidate set. To evaluate the restricted
scenario, we first initialize the candidate sets as before, then we randomly choose a given percentage
pr of nodes as restricted and discard every candidate that includes them. As expected, the results in
Fig. 1d show that for increasingly restrictive sets with pr = 10%, 25%, 50%, our attack is able to do
less damage. However, we always outperform the baselines (not plotted), and even in the case when
half of the nodes are restricted (pr = 50%) we are still able to damage the embeddings. With this
we are can answer question (Q3) affirmatively - the attacks are successful even when restricted.
Analysis of selected adversarial edges. In Fig. 2a we analyze the top 1K edges on Cora-ML. For
each edge we consider its source node degree (destination node, resp.) and plot it on the x-axis (y-
axis). The heatmap shows adversarial edge counts divided by total edge counts for each bin. We see
that low, medium and high degree nodes are all represented. In Fig. 2b we plot the edge centrality
distribution for the top 1K adversarial edges and compare it with the distribution of the remaining
edges. There is no clear distinction. The findings highlight the need for a principled method such as
ours since using intuitive heuristics such as degree/edge centrality cannot identify adversarial edges.
9SZ 8≈^i9 zes8^iZ _
99fe9p ©Pou UO-suqs9p
1	2	4	8	16 32 64 128 256
source node degree
(a) Degree centrality
-一 Suφp way
(b) Edge centrality
Figure 2: Analysis of the adversarial edges.
number of flips / % edges
(a) Cora
Figure 3: Targeted attack on the link prediction
-1000-800 -600 -400 -200 200 400 600 800 1000
27.3 21.8 16.4 10.9 5.5	5,5 10.9 16.4 21.8 27.3
number of flips / % edges
(b) Citeseer
7
Under review as a conference paper at ICLR 2019
^
.01
AT
O fg⅛-∣
3 Bfl
* Hr-
断≡⅞n
诩中
-Is%⅛τ-
0%
⅛
63% 65%
- .	1	2	4	8	16 32 64 128+
node degree, log2 bins
(c) Baseline Bdeg attack
95% 89% 85% 70% 52% 42% 45% 29%
τ?
THT
T-Hvl
TJBL-
I®f
τ⅛≡
τ⅛-
τ⅛
1	2	4	8	16 32 64 128+
node degree, log2 bins
(a)	Before attack
- .	1	2	4	8	16 32 64 128+
node degree, log2 bins
(b)	Baseline Brnd attack
1	2	4	8	16 32 64 128+
node degree, log2 bins
(d) Our Aclass attack
Figure 4: Margin distribution for different attacks binned according to their degrees (lower is better).
4.3	Targeted attack
To obtain a better understanding of the performance we study the margin m(t) before and after the
attack considering every node t as a potential target. We allow only (dt + 3) flips for attacking
each node ensuring the degrees stay similar. Each dot in Fig. 4 represents one node grouped by its
degree in the clean graph (logarithmic bins). We see that low-degree nodes are easier to misclassify
(m(t) < 0), and that high degree nodes are more robust in general - the baselines have 0% success.
Our method, however, can successfully attack even high degree nodes. In general, our attack is
significantly more effective across all bins - as shown by the numbers on top of each box - with
77.89% nodes successfully misclassified on average compared to e.g. only 33.64% for Brnd . For the
link prediction task (Fig. 3) we are similarly able to cause significant damage - e.g. Alink achieves
almost 10% decrease in performance by flipping around 12.5% of edges on Cora, significantly better
than all other baselines. Here again, compared to adding edges, removing has a stronger effect.
Overall, answering (Q5), both experiments confirm that our attacks hinder the downstream tasks.
4.4	Transferability
The question of transferability - do attacks learned for one model generalize to other models - is
important since in practice the attacker might not know the model used by the system under attack.
However, if transferability holds, such knowledge is not required. To obtain the perturbed graph, we
remove the top f adversarial edges with the ADW3 attack. The same perturbed graph is then used
to learn node embeddings using several other state-of-the-art approaches. Table 1 shows the change
in node classification performance compared to the embeddings learned on the clean graph for each
method respectively. We tune the key hyperparameters for each method (e.g. p and q for node2vec).
Table 1: Transferability: The change in F1 score (in percent) compared to the clean/original graph.
Cora/CiteSeer IDeePWaIk(SVD) DeePWalk(SGNS) node2vec SPeCt. Embd. LabelProp. GCN
f	250(03.1%)	-3.59	-3.97	-2.04	-2.11	-5.78	-3.34
f	500(06.3%)	-5.22	-4.71	-3.48	-4.57	-8.95	-2.33
f	250(06.8%)	-7.59	-573	-6.45	-3.58	-4.99	-2.21
f	500(13.6%)	-9.68	-11.47	-10.24	-4.57	-6.27	-8.61
Answering (Q6), the results show that our attack generalizes: the adversarial edges have a notice-
able imPact on other models as well. We can damage DeePWalk trained with the skiP-gram objective
with negative samPling (SGNS) showing that the factorization analysis is successful. We can even
damage the Performance of semi-suPervised aPProaches such as GCN and Label ProPagation. Com-
Pared to the transferability of the baselines (Sec. 6.3) our attack causes significantly more damage.
5	Conclusion
We demonstrate that node embeddings are vulnerable to adversarial attacks which can be efficiently
comPuted and have a significant negative effect on node classification and link Prediction. Further-
more, successfully Poisoning the system is Possible with relatively small Perturbations and under
restriction. More imPortantly, our attacks generalize - the adversarial edges are transferable across
different models. Future work includes modeling the knowledge of the attacker, attacking other
network rePresentation learning methods, and develoPing effective defenses against such attacks.
8
Under review as a conference paper at ICLR 2019
References
Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided
they blog. In Proceedings ofthe 3rd international workshop on Link discovery, pp. 36-43. ACM,
2005.
Victor Amelkin and Ambuj K Singh. Disabling external influence in social networks via edge
recommendation. arXiv preprint arXiv:1709.08139, 2017.
Konstantin Avrachenkov and Nelly Litvak. The effect of new links on google pagerank. Stochastic
Models, 22(2).
Battista Biggio, Giorgio Fumera, and Fabio Roli. Security evaluation of pattern classifiers under
attack. IEEE transactions on knowledge and data engineering, 26(4).
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma-
chines. In ICML, 2012.
Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph
embedding: Problems, techniques and applications. arXiv preprint arXiv:1709.07604, 2017.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017,
pp. 39-57, 2017. doi: 10.1109/SP.2017.49. URL https://doi.org/10.1109/SP.2017.
49.
Hau Chan, Leman Akoglu, and Hanghang Tong. Make it or break it: Manipulating robustness in
large networks. In Proceedings of the 2014 SIAM International Conference on Data Mining.
Vineet Chaoji, Sayan Ranu, Rajeev Rastogi, and Rushi Bhatt. Recommendations to boost content
spread in social networks. In Proceedings of the 21st international conference on World Wide
Web.
Chen Chen, Hanghang Tong, B Aditya Prakash, Tina Eliassi-Rad, Michalis Faloutsos, and Christos
Faloutsos. Eigen-optimization on large graphs by edge manipulation. ACM Transactions on
Knowledge Discovery from Data (TKDD), 10(4).
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. CoRR, abs/1712.05526, 2017a. URL http://arxiv.
org/abs/1712.05526.
Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto Perdisci, Manos
Antonakakis, and Nikolaos Vasiloglou. Practical attacks against graph-based clustering. arXiv
preprint arXiv:1708.09056, 2017b.
Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Democratizing adver-
sarial examples. Advances in Neural Information Processing Systems, 2017.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on
graph structured data. arXiv preprint arXiv:1806.02371, 2018a.
Quanyu Dai, Qiang Li, Jian Tang, and Dan Wang. Adversarial network embedding. In AAAI, 2018b.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems.
C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing
system. In Proceedings of the third ACM conference on Digital libraries.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. CoRR, abs/1412.6572, 2014. URL http://arxiv.org/abs/1412.6572.
9
Under review as a conference paper at ICLR 2019
Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel.
Adversarial examples for malware detection. In European Symposium on Research in Computer
Security.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Eitan Israeli and R Kevin Wood. Shortest-path network interdiction. Networks, 40(2).
Elias Boutros Khalil, Bistra Dilkina, and Le Song. Scalable diffusion-aware optimization of network
topology. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. arXiv
preprint arXiv:1703.04730, 2017.
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on
factorization-based collaborative filtering. In Advances in neural information processing systems.
Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text
classification can be fooled. arXiv preprint arXiv:1704.08006, 2017.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tac-
tics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2).
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on
machine learners. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,
January 25-30, 2015, Austin, Texas, USA., pp. 2871-2877, 2015. URL http://www.aaai.
org/ocs/index.php/AAAI/AAAI15/paper/view/9472.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013.
Luis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. arXiv preprint arXiv:1708.08689, 2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium
on Security and Privacy.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining.
Cynthia A Phillips. The network inhibition problem. In Proceedings of the twenty-fifth annual ACM
symposium on Theory of computing.
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network em-
bedding as matrix factorization: Unifyingdeepwalk, line, pte, and node2vec. arXiv preprint
arXiv:1710.02971, 2017.
Gilbert W Stewart. Matrix perturbation theory. 1990.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4).
10
Under review as a conference paper at ICLR 2019
Cheng Yang and Zhiyuan Liu. Comprehend deepwalk as matrix factorization. arXiv preprint
arXiv:1501.00358, 2015.
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation
learning with rich text information. In IJCAI.
Mengchen Zhao, Bo An, Yaodong Yu, Sulin Liu, and Sinno Jialin Pan. Data poisoning attacks on
multi-task relationship learning. 2018.
Daniel Zugner, Amir Akbarnejad, and StePhan Gunnemann. Adversarial attacks on neural networks
for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 2847-2856, 2018.
6	Appendix
6.1	Proofs and derivations
Proof. Theorem 1. Applying eigenvalue perturbation theory we obtain that λ0p = λp + UT (∆M)up
where λ0p is the eigenvalue of M 0 obtained after perturbing a single edge based on A0 . Using the
fact that λp = UTMMup, and the fact that singular values are equal to the absolute value of the
corresponding eigenvalues We obtain the desired result.	口
Proof. Theorem 2. Denote with ei the vector of all zeros and a single one at position i. Then, we
have ∆A = ∆wij (eiejT + ejeiT) and ∆D = ∆wij (eieiT + ejejT). From eigenvalue perturbation
theory Stewart (1990), we get: λy ≈ λy + UT(∆A - λy∆D)uy. Substituting ∆A∕∆D concludes
the proof.	口
We include an intermediate result which is required for proving Lemma 2 and Lemma 3.
Lemma 4. λ is an eigenvalue of D-1/2 AD-1/2 := Anorm With eigenVector U = D1/2U ifand only
ifλ and u solve the generalized eigen-problem Au = λDu.
Proof. Lemma 4. We have Az = λDz =⇒ (Q-1AQ-T)(QTz) = λ(QTz) for any real symmet-
ric A and any positive definite D, where D = QQT using the Cholesky factorization. Substituting
the adjacency/degree matrix and noticing that Q = QT = D1/2 we obtain the result.	口
Proof. Lemma 2. S is equal to a product of three matrices S = DT/2(U7(PT=I Λr)UT)D-1/2
where UΛUT = D-1/2AD-1/2 =: Anorm is the eigenvalue decomposition of Anorm (QiU et al.
(2017)). From Lemma 4 we have the fact that λ is an eigenvalue of D-1/2AD-1/2 with eigenvector
U = D1/2U if and only if λ and U solve the generalized eigen-problem Au = λDu. Substituting
Λ = Λ and U = D1/2 U in S, and since D is diagonal, we obtain the result.	口
Proof. Lemma 3. Following Qiu et al. (2017), the singular values ofS can be bounded by σp(S) ≤
d-ɪ-1 PT=1(μ∏(p))r I where μ are the (standard) eigenvalues of Anorm. Using Lemma 4, the same
bound applies using the generalized eigenvalues λpof A. Now using Theorem 2, we obtain λ0p an
approximation of the p-th generalized eigenvalue of A0 . Plugging it into the singular value bound
we obtain: σp(S) ≤ d⅛ι PT=Ian(P))I which concludes the proof.	口
Please note that the permutation π does not need be computed/determined explicitly. In practice, for
every λp, we compute the term ∣ PT=1 (λp)r ∣. Afterwards, these terms are simply sorted.
11
Under review as a conference paper at ICLR 2019
6.2	Analysis of spectral embedding methods
Attacking spectral embedding. Finding the spectral embedding is equivalent to the following
trace minimization problem:
K
min	Tr(ZTLxyZ) = Xλi(Lxy) = LSC	(5)
z∈RιV ι×K	y
i=1
subject to orthogonality constraints, where Lxy is the graph Laplacian. The solution is obtained via
the eigen-decomposition of L, with Z* = UK where UK are the K-first eigen-vectors corresponding
to the K-smallest eigenvalues λi . The Laplacian is typically defined in three different ways: the
unnormalized Laplacian L = D - A, the normalized random walk Laplacian Lrw = D-1L = I -
D-1A and the normalized symmetric Laplacian Lsym = D-1/2LD-1/2 = I - D-1/2AD-1/2 =
I - Anorm, where A, D, Anorm are defined as before.
Lemma 5 (Von Luxburg). λ is an eigenvalue of Lrw with eigenvector u if and only if λ is an
eigenvalue of Lsym with eigenvector w = D1/2u. Furthermore, λ is an eigenvalue of Lrw with
eigenvector u if and only if λ and u solve the generalized eigen-problem Lu = λDu.
From Lemma 5 we see that we can attack both normalized versions of the graph Laplacian with a
single attack strategy since they have the same eigenvalues. It also helps us to do that efficiently
similar to our previous analysis (Theorem. 3).
Theorem 4.	Let Lrw (or equivalently Lsym) be the initial graph Laplacian before performing aflip
and λy and uy be any eigenvalue and eigenvector of Lrw. The eigenvalue λ0y of L0rw obtained after
flipping a single edge (i, j) is
λy ≈ λy + ∆wij ((uyi - uyj) - λy (uyi + uyj))	(6)
where uyi is the i-th entry of the vector uy.
Proof. From Lemma 5 we can estimate the change in Lrw (or equivalently Lsym) by estimating
the eigenvalues solving the generalized eigen-problem Lu = λDu. Let ∆L = L0 - L be the
change in the unnormalized graph Laplacian after performing a single edge flip (i, j) and ∆D be
the corresponding change in the degree matrix. Let ei be defined as before. Then ∆L = (1 -
2Aij)(ei - ej)(ei - ej)T and ∆D = (1 - 2Aij)(eieiT + ejejT). Based on the theory of eigenvalue
perturbation we have λ0y ≈ λy + uyT (∆L - λy∆D)uy. Substituting ∆L and ∆D are re-arranging
We get the above results.	□
Using now Theorem 4 and Eq. 5 we finally estimate the loss of the spectral embedding after flipping
an edge LSC (L0rw , Z) ≈ PpK=1 λ0p. Note that here we are summing over the K-first smallest
eigenvalues. We see that spectral embedding and the random walk based approaches are indeed
very similar.
We provide similar analysis for the the unnormalized Laplacian:
Theorem 5.	Let L be the initial unnormalized graph Laplacian before performing a flip and λy and
uy be any eigenvalue and eigenvector of L. The eigenvalue λ0y of L0 obtained after flipping a single
edge (i, j) can be approximated by:
λ0y ≈ λy - (1 - 2Aij)(uyi - uyj)2	(7)
Proof. Let ∆A = A0 - A be the change in the adjacency matrix after performing a single edge flip
(i, j ) and ∆D be the corresponding change in the degree matrix. Let ei be defined as before. Then
∆L= L0 -L = (D+∆D) - (A+∆A) - (D-A) = ∆D-∆A = (1 -2Aij)(eieiT +ejejT -
(eiejT + ej eiT)). Based on the theory of eigenvalue perturbation we have λ0y ≈ λy + UT。L)Uy.
Substituting ∆L and re-arranging we get the above results.	□
12
Under review as a conference paper at ICLR 2019
9n-e> ,J-n6u-s
8 6 4 2
9n-e> ,J-n6u-s
(a) PolBlogs	(b) Cora	(c) Citeseer
Figure 5: The singular value of S and our derived upper bound.
9n-e> ,J-n6u-s
6.3 Further experimental evidence
Upper bound on singular values. From Lemma 3 we have that LDW3 is an upper bound on
LDW1 (excluding the elementwise logarithm) so maximizing LDW3 is principled. To gain a better
understanding of the tightness of the bound we visualize the singular values of S and their respective
upper-bound for all datasets. As we can see in Fig. 5, the gap is different for different datasets and
relatively small. Furthermore we can notice that the gap tends to increase for larger singular values.
Transferability of the baselines. To further support the transferability of our proposed attack we
also examine the transferability of the baseline attacks. Specifically, we examine the transferability
of Beig since it is the strongest baseline when removing edges as shown in Fig. 1b. We use the same
experimental setup as in Sec. 4.4 and show the results in Table 2. We can see that compared to our
proposed attack the baseline can do a significantly smaller amount of damage (compare to results in
Table 1). Interestingly, it can do significant damage to GCN when removing 250 edges on Cora, but
not when removing 500 edges. We plan on exploring this counterintuitive finding in future work.
Table 2: Transferability: The change in F1 score (in percent) compared to the clean/original graph.
Cora/Citeseer IDeePWaIk(SVD) DeePWaIk(SGNS) node2vec Spect. Embd. LabelProp. GCN
f	250(03.1%)	-061	-0.65	-0.57	-0.86	-1.23	-6.33
f	500(06.3%)	-0.71	-1.22	-0.64	-0.51	-2.69	-0.64
f	250(06.8%)	-0.40	∏6	-0.26	+0.11	-1.08	-0.70
f	500(13.6%)	-2.15	-2.33	-1.01	+0.38	-3.15	-1.40
13