Under review as a conference paper at ICLR 2019
Neural Causal Discovery with Learnable In-
put Noise
Anonymous authors
Paper under double-blind review
Ab stract
Learning causal relations from observational time series with nonlinear interac-
tions and complex causal structures is a key component of human intelligence,
and has a wide range of applications. Although neural nets have demonstrated
their effectiveness in a variety of fields, their application in learning causal rela-
tions has been scarce. This is due to both a lack of theoretical results connecting
risk minimization and causality (enabling function approximators like neural nets
to apply), and a lack of scalability in prior causal measures to allow for expres-
sive function approximators like neural nets to apply. In this work, we propose
a novel causal measure and algorithm using risk minimization to infer causal re-
lations from time series. We demonstrate the effectiveness and scalability of our
algorithms to learn nonlinear causal models in synthetic datasets as comparing to
other methods, and its effectiveness in inferring causal relations in a video game
environment and real-world heart-rate vs. breath-rate and rat brain EEG datasets.
1	Introduction
From an early age, humans have a remarkable ability to infer causal relations from pure observations
(White & Milne (1997); Scholl & Tremoulet (2000); Buchsbaum et al. (2012)). By observing that a
left dot moving towards a right dot and the right dot moves correspondingly (the launching effect, see
Michotte (1963)), a human can quickly infer that the left dot causes the right dot to move (Scholl &
Tremoulet (2000)). In fact, much of our way of thinking is via cause and effect. Causal analysis also
permits counterfactual reasoning, answering what would have happened if the cause had happened
differently. The learning of causality also constitutes much of the scientific endeavor, for example
finding the cause of a certain cancer (Bosch et al. (2002)), or discovering gene regulatory networks
(Lozano et al. (2009)). In addition, causality plays a key role in neuroscience (Neves et al. (2008);
Seth et al. (2015)), economics (e.g. Granger (1969); Stock & Watson (1989)) and finance (Hiemstra
& Jones (1994); Granger et al. (2000)).
The study of causality has a long history, yet the application of neural nets to learning causal relations
has been scarce. There have been various works that propose methods to infer causal structures with
limited model space (e.g. Granger (1969)) which may not be able to model complex nonlinear
causal relations, or propose measures to quantify causal strength (e.g. Schreiber (2000); Janzing
et al. (2013)) that may not scale to high-dimensional data. For these, the use of universal function
approximators like neural nets may be beneficial in inferring causality, which motivate us to propose
causal measures that are not only theoretically founded, but also amenable to the learning of function
approximators. On the other hand, in the deep learning community, the learning of causal models has
not been prevalent, in part due to the lack of theoretical understanding between learning a prediction
model and learning a causal model. This also motivates us to propose causal measures, obtained via
learning a prediction model, that can deduce causality.
The contributions of this work are as follows:
•	We propose a novel measure to quantify causality from observational data, and an effective
algorithm, Causal Inference with Learnable Noise (CILN), to estimate it. It is based on
minimizing a learnable noise risk of a prediction model, allowing function approximators
such as neural nets to learn complex causal relationships.
1
Under review as a conference paper at ICLR 2019
•	We demonstrate on nonlinear synthetic datasets that our method outperforms other causal
measures by a large margin, and can scale to a larger number of time series. We also
demonstrate that our models are effective on real-world datasets.
2	Related work
The study of causality has a long history, and has been approached from different perspectives. Pearl
(e.g. Pearl (2002; 2009); Pearl et al. (2009)) defines causality in terms of intervention and struc-
tural dependence, under the structural equation models (SEM). Granger (Granger (1969); Granger
& Newbold (1986)) defines causality via prediction: if the prediction of Y via a linear model can
be improved by including the information of X, then X causes Y in the Granger sense. Since its
proposal, Granger causality has been widely applied in economics (e.g. Joerding (1986)) and neu-
roscience (e.g. Deshpande et al. (2009); Seth et al. (2015)). To learn nonlinear causal relations,
later works also extend Granger causality to kernel methods (Ancona et al. (2004); Marinazzo et al.
(2008a;b); Sindhwani et al. (2012)). To clear up the relations between these two notions of causality,
White et al. (2011) provide conditions under which Granger causality can deduce direct structural
causality in a general settable system framework (White & Chalak (2009)), with direct structural
causality as a natural extension of Pearl causality in settable systems. Our method also utilizes the
high-level idea of inferring causal relations via prediction, and building on the work of White et al.
(2011) we propose a novel measure that can likely uncover direct structural causality under certain
conditions.
Numerous methods have been proposed to discover causal structures from data. One important class
is constraint-based methods, for example PC (Spirtes et al. (2000)), rankPC (Harris & Drton (2013)),
IC (Pearl (2002)), and FCI (Spirtes et al. (2000)), which require repetitive conditional independence
tests. Hybrid methods, e.g. MMHC (Tsamardinos et al. (2006)), require repetitive estimation of
conditional association score (e.g. using conditional mutual information). In comparison, under
the scope of time series, our method can simultaneously discover multiple variables that directly
cause the variable of interest. Score-based methods search for the structure that yields the optimal
score w.r.t. the data, generally using greedy search methods, for example GES (Chickering (2002)),
rankGES (Nandy et al. (20l8)) and GIES (Hauser & Buhlmann (2012)). This in general requires
Θ(N2) steps (N denoting the number of nodes in the graph), and the number of neighboring states
may grow very large at each step. In comparison, our method only requires training N models for
causal discovery with time series.
In addition, various measures have been proposed to quantify causality. Schreiber (2000) proposes
transfer entropy as a measure for causality. It measures the mutual information between the current
Y and the past of X, conditioned on the past of Y, to quantify the directional information transfer.
As noted in Marinazzo et al. (2008a), Granger causality as defined in Granger & Newbold (1986)
implies nonzero transfer entropy. Janzing et al. (2013) analyze several causal measures, and con-
clude that they are unsatisfactory measures of causal strength. They then propose causal influence,
defined via the KL-divergence between the original joint distribution and the distribution with a set
of causal arrows broken. We note that both the calculation of transfer entropy and causal influence
requires density estimation of the full joint distributions of the input and output, which could easily
become difficult in high dimensions. This motivates us to propose new causal measures based on
risk minimization, which is much easier with high input dimensions. Besides, various other works
have also proposed methods to infer causal structure under some specific conditions, for example,
causal additive noise models (Hoyer et al. (2009)), information-geometric causal inference (Daniusis
et al. (2012); Janzing et al. (2012)), dynamic causal modeling (Friston et al. (2003)), etc.
There has also been works that approach causality from a machine learning perspective, or have ar-
chitectures that put causality to mind. Lopez-Paz et al. (2015) study causal inference as a supervised
learning problem, where the pairwise causal directions are given as labels for training. Louizos
et al. (2017) utilize a variational autoencoder (VAE) structure to estimate the unknown latent space
summarizing the confounders and the causal effect. Kipf et al. (2018) propose a neural relational
inference architecture, which simultaneously infers the interactions and learning the dynamics from
observational data. Designing causal inference into the model architecture can also be beneficial in
certain applications. For example, Kansky et al. (2017) propose a Schema Network that allows re-
2
Under review as a conference paper at ICLR 2019
gression planning from a goal through causal chains, and demonstrates zero-shot learning in a suite
of variations of Atari Breakout games.
Our method also relates to sparse learning/feature selection methods, for example L1 regularization
and group L1 regularization (e.g. Meier et al. (2008); Scardapane et al. (2017)). Although L1
or group L1 regularization encourage sparsity in the weights of neural nets, the regularization is
model and input dependent, in contrast to our method’s invariance to model structure change and
inputs rescaling. For example, a three-layer linear network and a collapsed one-layer linear network
representing the same function can induce different L1 regularizations. For another example, if one
of the input time series is scaled by a factor of 0.1 while other variables remain unchanged, the L1
regularization will be different due to a differently learned model. In comparison, our learnable noise
risk when minimized is invariant to the above two kinds of changes, making it especially suitable
to discover causality where the scale of different time series may span orders of magnitude, and the
model structure may vary, as demonstrated in Section 4.1.
3	Method
3.1	Problem Definition
Although causal data can be inferred from observations at individual time points, we are pri-
marily concerned with time series. Time series have extra structure that is particularly useful
for causal inference: causes must precede their effects. Therefore, we consider N time series
x(1) , x(2) , ...x(N),	where each time series	x(i)	=	(x(1i) , x(2i) ,	...x(ti) ,	...) and each	xt(i)	∈	RM	is
an M -dimensional vector. Denote Xt(-i)1 = (xt(i-)K, xt(i-)K +1, ...xt(i-)1) with maximum time horizon
of K, and Xt-1 = {Xt(-i)1}, i = 1, 2, ...N. We also denote X(tj-)1 = Xt-1 \Xt(-j)1, i.e. Xt-1 ex-
cluding Xt(-j)1 , to notationally differentiate with the variable of interest Xt(-j )1 . We assume that the
time series is generated by a canonical settable system (White et al. (2011)). We adopt the settable
system (White & Chalak (2009)) paradigm due to the following reasons: (1) as a natural extension to
SEMs, it facilitates optimization, equilibrium, and learning; (2) it can formally link Granger causal-
ity with Pearl causality; (3) it is general enough to encompass a large number of practical scenarios,
including time series. Intuitively, each variable in a settable system can either be determined by
direct setting (which formalizes interventions), or by a response function on the settings of other
variables. A canonical settable system is a settable system where each variable’s setting equals its
response, allowing the system to evolve naturally without intervention, formalizing time series. In
this paper, we also assume causal sufficiency (Peters et al. (2017)), i.e., each time series x(i) can
only be structurally caused by the time series from x(1), x(2), ...x(N), and generated by stationary
response functions hi that are unknown to the learner:
h1 (Xt-1, u1)
h2(Xt-1,u2)
(1)
xt	:= hN(Xt-1, uN)
for t = K + 1, K + 2, ...... Here ui ∈ RM, i = 1, 2, ...N are noise variables that are mutually
independent, are independent of any Xt(-i)1, xt(i), i ∈ {1, 2, ...N}, and are effective arguments of the
response functions hi. Also in this paper, we only consider “causality in mean", i.e. the causal rela-
tions, if exists, influence the mean value of other variables. For any i, j ∈ {1, 2, ...N}, we assume
that the variables (Xt(-j)1, Xt(-j)1, xt(i)) have probability density function P(Xt(-j)1, Xt(-j)1, xt(i)). We
will leave time series with hidden variables (therefore confounding can occur) for future work. We
note that even without considering hidden variables, Eq. 1 is very general and already encompasses
a wide range of scenarios.
In order to state the goal of the learner who wants to learn causality from the observations
x(1) , x(2) , ...x(N), we need to rigorously define causality. Here, we restate the definitions of di-
rect structural causality (White et al. (2011)) and Granger causality (Granger & Newbold (1986))
using our notations of the system Eq. 1, the former being a natural extension to Pearl causality (Pearl
(2009)) in the settable systems.
3
Under review as a conference paper at ICLR 2019
Direct structural causality (White et al. (2011)) We say Xt(-j )1 , j 6= i does not directly struc-
turally cause
hi(Xt-1,ui)
xt(i), if for all possible values of Xt(-j)1 and ul, l ∈ 1, 2, ...N, the function Xt(-j)1
is constant in Xt(-j)1. Otherwise, we say Xt(-j)1 directly structurally causes x(ti).
→
Granger (1969) defines causality in terms of conditional expectations. Later, Granger & Newbold
(1986) define it using conditional distributions. We use the latter definition, since it is more general,
and also facilitates connection with direct structural causality, and the algorithms proposed in this
paper. In this work, we only consider causality between different time series, i.e. requiring j 6= i as
default.
Granger causality (Granger & Newbold (1986)) We say Xt(-j)1,j 6= i does not Granger-cause xt(i),
if P (xt(i) |Xt(-j)1, Xt(-j)1) = P (xt(i) |X(tj-)1), i.e. the conditional distribution function of x(ti) given
Xt(-j)1, X(tj-)1 is identical to the conditional distribution function of x(ti) given Xt(j-)1. Otherwise, we
say Xt(-j)1 Granger-causes x(ti).
The goal of the learner is, given only x(1), x(2), ...x(N), determine for any xt(i), whether Xt(-j)1 di-
rectly structurally causes xt(i) for each j = 1, 2, ...N.
3.2	Granger causality implies direct structural causality in Eq. (1)
For our system Eq. (1), applying the results by White et al. (2011), we have that Granger causality
is a sufficient condition for direct structural causality. See Appendix A for the detailed theorem and
proof.
For system Eq. 1, for any i, j ∈ {1, 2, ...N}, i 6= j, if Xt(-j)1 Granger-causes xt(i), then Xt(-j)1 directly
structurally causes xt(i).
The reason that here Granger causality can deduce direct structural causality is in part due to the fact
that for system Eq. (1), conditional exogeneity (White et al. (2011)) is automatically satisfied by the
assumptions of the system.
Note that the reverse of the statement is not true, i.e. Granger non-causality does not necessarily
imply direct structural non-causality (White & Lu (2010) give several examples). They also note
that these instances are exceptional, in that mild perturbations to their structures destroy the Granger
non-causality.
3.3	Our method
Based on section 3.2, if we have an algorithm that can deduce Granger causality in system Eq. (1),
then we can immediately deduce direct structural causality. In general, a direct test of Granger
causality is difficult. In particular, when the number of time series is large or the dimension M
of each time series is large, density or mutual information-based methods like transfer entropy
(I(x(ti); Xt(-j)1 |Xt(-j)1), see Schreiber (2000)), may not give a good estimate. Alternatively, vari-
ous works have resorted to testing whether the prediction of xt(i) given Xt(-j)1, Xt(j-)1 is better than
given only X(tj-)1 , under some limited functional space. For example, in his original work, Granger
(1969) investigates causality with linear function predictors. Later works have extended it to ker-
nel methods, e.g., Ancona et al. (2004); Marinazzo et al. (2008a;b); Sindhwani et al. (2012), which
essentially estimate linear Granger causality on the feature space of the kernel.
To enable causal learning with potentially highly nonlinear response functions, it may be desirable to
use universal function approximators (Hornik (1991)) such as neural nets. As the main contribution
of this paper, we will provide a novel causal measure and corresponding algorithm to estimate it.
The measure is based on optimizing an objective containing a function approximator, thus allowing
neural nets to apply.
Our algorithm is inspired by asking a counterfactual question during the learning of a prediction
model. Specifically, it asks:
4
Under review as a conference paper at ICLR 2019
Algorithm 1 Causal Inference with Learnable Noise (CILN)
Require xt(i), Xt-1, for i ∈ {1,2,...N},t ∈ T = {K+ 1,K+2,...}.
Require η0 : a small value for initialization of η.
Require λ: coefficient for the mutual information term.
1:	for i in {1, 2, ...N} do:
2:	Initialize function approximator fθ.
3:	Initialize η = (η1, η2, ...ηN) = (η01, η01, ...η01), where each element η01 is a KM-
dimensional vector, same dimension as Xt(-j)1.
4：	(fθ*, η*) - Minimize(fθ,η)Rχ,χ(i),Jfθ, η] (Eq. 4) With e.g. gradient descent
5:	Wji 一 I(X(-)(η*); XiB), for j = 1, 2,…N.
6： end for
7: return W
How much noise can I add to Xt(-j)1 , without making the best prediction of x(ti) worse?
To give a quantitative ansWer to this question, We define a learnable noise risk:
RX,x(i)[fθ,η] = EXt-1,xt(i)
(2)
where XX(η)1 := Xt-1 + η © e (or element-wise, Xt-Sn) := X(PI + ηj ∙ ej, j = 1,2,…N) is
the noise-corrupted inputs with learnable noise amplitudes ηj ∈ RKM, and ej 〜N(0, I). λ > 0
is a positive hyperparameter for the mutual information I(∙, ∙). Intuitively, the minimization of the
second term I(Xt-)(ηn); Xt-I) requires the noise amplitude ηj to go up. The minimization of the
first term requires the noise amplitude ηj to go down, and the larger causal strength from Xt(-j )1 to
xtti) , the larger this force. The minimization of the two terms strikes a balance, at which point the
I(X(-)(ηn); X(-)ι) measures how many bits of information does time series j need to provide to the
learner, without making the prediction worsened. Thus, we propose to use
Wji = I (X(-)(η*); Xt-1)	(3)
as another measure for causality, where (fθ*, η*) = argmin(f,n)Rχ,χ(i) fθ, η] 1. In Appendix B,
we analyze the qualitative and quantitative properties of the learnable noise risk, and give intuitions
why it is likely to select the variables that directly structurally causes xtti).
Empirically, we minimize the following empirical risk:
Rχ,x(i),Jfθ,η] = l⅛ X (x(i) - fθ(X(η)ι))2 + λXI(X"ηn);Xt-)1)	(4)
|T| t∈T	j=1
In general, it may be difficult to estimate the mutual information I(Xt-)(ηn);Xt-I) with
large dimension of Xtt-j)1 such that the expression is also differentiable w.r.t. ηj . Utiliz-
ing the property of Gaussian channels, in Appendix C we prove that I(Xt-，n);Xt-I) ≤
1 PK=MM log(1 + Var(X2t-1,l)), where l denotes the lth element of a vector, and Var(Xt-)11) is the
variance of Xtt-j)1,l across t. Therefore, in practice to improve efficiency, we can optimize an upper
bound of the learnable noise risk:
RXxri)*,η] = ⅛X M)-fθ(Xt-)ι))2 + 2XKMlog (ι + Var(Xjɪŋ	(5)
| | t∈T	j=1 l=1	ηj,l
1 Note also that throughout this paper, when talking about causal matrices, the (j, i)th element always denotes
the causal strength from j to i.
5
Under review as a conference paper at ICLR 2019
When the dimension of Xt(-j )1 is large, differentiable estimate of the mutual information, such as
MINE (Belghazi et al. (2018)), can be applied. We provide Algorithm 1 to empirically estimate
Wji , which we term Causal Inference with Learnable Noise (CILN).
4	Experiments
To demonstrate that our proposed method works, we test it on both synthetic and real datasets. We
first use synthetic datasets, where we know the underlying causal structure and compare with previ-
ous causal measures. We then test whether our algorithm can infer causal structure from watching an
agent playing video games. Finally, we apply our algorithm to real-world heart-rate vs. breath-rate
and rat EEG datasets to test its effectiveness. We use the RXpxri) e[fθ, η] (Eq. 5) for optimization
for all experiments. The metrics we use are the standard metrics of area under the precision-recall
curve (AUC-PR) (Davis & Goadrich (2006)), and area under the ROC curve (AUC-ROC).
4.1	Synthetic experiment with log-normal causal strengths
In this experiment, we evaluate our method together with other methods with a nonlinear synthetic
dataset generated to have a known causal structure (hidden to the methods being compared). We
study how they perform with varying number N of time series, with N up to 30. To generate the
data, we let each xt(i) have dimension M = 1, and also set the maximum time horizon K = 3, so
(j)
each Xt-1 is a K × M = 3 × 1 matrix. We use the following realization of the response function
hi in Eq. (1):
Xti) =	hi(Xt-ι,ut)	=	Hi	(XX	[Aji	Θ H2(Bj Θ	X(-)ι)) + Ut, i = 1, 2,…N	(6)
where Ut 〜 N(0, I) ∈ RM, Θ denoting element-wise multiplication, and Hi and H2 are two
nonlinear functions to make the response functions nonlinear. In this experiment, we use H1(x) =
softplus(x) = log(1 + ex), and H2 (x) = tanh(x), we also find similar performance with other
choices of nonlinear functions. Bj is a K × M random matrix, whose element is sampled from
U [-1, 1]. Aji is a K × M matrix, with 0.5 probability of being a zero matrix and 0.5 probability of
being a nonzero random matrix, characterizing the underlying causal strength from j to i. Crucially,
to reflect that the causal strength may span different orders of magnitude, if Aji is sampled to be a
nonzero matrix, then the amplitude of each of its element is sampled from a log-normal distribution
with μ = 1,σ = 0, their sign sampling from U{-1,1}. Denote Aindi as the 0-1 indicator matrix of
causality (Aindi,ji = 1 if |Aji| > 0; 0 otherwise). The goal of each algorithm being evaluated is to
produce an N × N causal matrix A, where each entry Aji characterizes the causal strength from j
to i. Then the flattened A is evaluated against the flattened Aindi (excluding diagonal elements of
the matrices), producing the metrics of AUC-PR and AUC-ROC.
In general, for a large N, the number of possible causal graphs grows double exponentially: there
are 2N2 possible matrix of Aindi. To give an estimate, for N = 3, 4, 5, 8, 10, 20, 30, there are
512, 6.6 × 104, 3.3 × 107, 1.8 × 10i9, 1.2 × 1030, 2.6 × 10i20, 8.5 × 10270 number of possible
graphs, respectively. Therefore, estimating the causal graph is in general a non-trivial task when N
is large. We compare our algorithm, with previous methods including transfer entropy (Schreiber
(2000)), causal influence (Janzing et al. (2013)), linear Granger causality (Granger (1969); Ding
et al. (2006)), and a baseline of mutual information Aji = I(Xt-1； x(i)) (which gives Aji = Aij).
The implementation details for each method and each experiment are provided in Appendix D and
Appendix E, respectively. Table 1 and 2 shows the average AUC-PR and AUC-ROC with each N,
each with four random initializations of the true underlying causal matrix A and dataset.
We see that not only does our method outperform other methods by a large margin across all Ns, it
also shows good performance when N is as large as 30, demonstrating our method’s capability and
scalability to infer complex causal structures from interacting time series. For the Causal Influence
method, although it has very good mathematical properties, it may be impractical in practice, as is
also shown in the table. This is due to that it is defined as the KL-divergence between (Xt-i, xt(-i)i)
6
Under review as a conference paper at ICLR 2019
N method	3	4	5	8	10	15	20	30
Ours	0.9757	0.9820	0.9604	0.9424	0.9302	0.9266	0.8614	0.7740
Transfer Entropy	0.9401	0.9308	0.9059	0.7376	0.6764	0.6197	0.5723	0.4818
Mutual Information	0.9139	0.9318	0.8775	0.8292	0.8058	0.7687	0.7147	0.6980
Linear Granger	0.7262	0.9431	0.8054	0.8062	0.7566	0.6589	0.6643	0.5071
Causal Influence	0.7910	0.7068	0.5481	0.3693	0.4149	0.4798	0.4612	0.4254
Table 1: Average AUC-PR vs. N, with random sampling of Aindi . Bold font marks the top method
for each N .
N Method	3	4	5	8	10	15	20	30
Ours	0.9722	0.9580	0.9525	0.9406	0.9382	0.9251	0.8546	0.7810
Transfer Entropy	0.8854	0.8469	0.9035	0.7893	0.7731	0.6609	0.6006	0.5275
Mutual Information	0.8889	0.8817	0.8611	0.8414	0.8403	0.7711	0.7380	0.7156
Linear Granger	0.6806	0.8969	0.7787	0.8166	0.7881	0.6887	0.6777	0.5840
Causal Influence	0.7083	0.6334	0.6077	0.4155	0.4705	0.5707	0.5153	0.5218
Table 2: Average AUC-ROC vs. N, with random sampling of Aindi . Bold font marks the top
method for each N .
and (Xt-)1 ,x(-J, each of which is an (NK + 1)M -dimensional vector, which can quickly go
to high dimensions, where density estimation required to calculate KL-divergence is in general
data-hungry and difficult. In comparison, our method that estimates causal strength via minimizing
prediction errors is comparatively easier in high dimensions (only have to predict a M dimensional
vector conditioned on the inputs), which contributes to a better performance when N is large.
Since in practice, we do not know the underlying causal structure a priori, it presents a greater
challenge to select the model capacity for fθ, as compared with supervised learning method where
we can do cross-validation. To see how the capacity of the function approximator fθ influences our
method, we vary the number of layers and the number of neurons in each layer at N = 10. Table
3 summarizes the result. We see that our method’s performance here is hardly influenced by the
model capacity, with only a slight degradation at very low capacity. This shows that our method is
quite tolerant and stable with model capacity variations.
Neurons in hidden layers	AUC-PR	AUC-ROC
18)	0.8969	0.9086
(8, 8)	0.9309	0.9388
(16, 16)	0.9404	0.9455
(8, 8, 8)	0.9339	0.9410
(16, 16, 16)	0.9284	0.9288
(8, 8, 8, 8)	0.9312	0.9350
(16,16,16,16)	0.9199	0.9202
Table 3: Average AUC-PR and AUC-ROC for different network structures for N = 10 with our
method. Here for example, (8, 8, 8) means that the fθ has 3 hidden layers, each with 8 neurons.
4.2	Experiments with Video games
To see how our method can infer causal relations in real videos games, and potentially helping
reinforcement learning (RL) or imitation learning (IL), we apply our method to the causal inference
between the trajectories of different objects from a trained CNN RL-agent playing Atari Breakout
7
Under review as a conference paper at ICLR 2019
(a)
action paddle ball-x ball-y brick reward
Figure 1: Causal strength Wji inferred by our method with watching a trained CNN playing Break-
out. The (j, i) element denotes the inferred causal strength from j to i.
action
paddle
b∙ll-x
b∙ll-y
brick
reward
action paddle t>all-x b»ll-y brick reward
0.00
-0.05
-0.1C
-0.15
-0.20
-0.25
-0.3C
(b)	(c)	(d)
Figure 2: Causal strength inferred by (a) mutual information (b) transfer entropy (c) linear Granger
(d) causal influence
games2 (Bellemare et al. (2013), implementation details see Appendix F). Fig 1 shows the inferred
Wji matrix, with the (j, i)th element denoting the inferred causal strength from j to i. We see
that there is a prominent causal direction from the ball’s y position to the reward, which correctly
summarizes that the ball’s y position has a large influence on the reward. Additionally, the other
discovered causal directions with causal strength greater than 1 include: brick → reward, ball-y →
brick, reward → paddle, ball-x → action, ball-y → action. The former two correctly summarize
causal chains from ball-y → brick → reward, the latter two show that the ball’s x and y positions
also have big causal influences on the trained agent’s action: in order that the ball does not fall to
the bottom, the agent has to position itself at the right position depending on the x and y positions
of the ball.
In comparison, mutual information (Fig. 2 (a)) gives a symmetric matrix that does not differentiate
the two possible causal directions. Moreover, it discovers a high mutual information between brick
and paddle, which does not related to the underlying causal mechanism, and also missed the causal
arrows ball-y→brick→reward. For transfer entropy, although it discovers a number of prominent
causal arrows (e.g. ball-x→action, ball-y→action, ball-y→reward, brick→reward), it also gives
relative high scores for some incorrect causal arrows: brick→ action, ball-y→ball-x, and missed an
important causal arrow ball-y→brick. For linear Granger, it gives most arrows 0 or negative score,
failing to discover any useful causal arrows. For causal influence, it also fails to discover useful
causal arrows.
The inferred causal matrix and learned models may be useful for downstream learning in RL/IL.
The learned models fθ can serve as a succinct subset of the environment model p(s0, r|s, a) that
predicts future state s0 and reward r based on current state s and action a. Moreover, the agent can
utilize the causal matrix to learn policy much more quickly, by reasoning with a causal chain from
the reward backward, and from the action forward and backward, and focusing learning policies
that can maximally influence the entities in the causal chains. Take the current Breakout game for
example. After discovering the causal matrix (Fig. 1) by watching a teacher agent playing, the agent
can reason from the reward backward on the causal chain ball-y→brick→reward, and understand
that in order to maximize its reward, it has to influence the ball’s y position and the number of
2A video showing the game playing can be seen at https://goo.gl/XGzppc
8
Under review as a conference paper at ICLR 2019
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Figure 3: (a) Causal strength Wji inferred by our method with the heart-rate vs. breath-rate dataset,
averaged over 50 initializations of fθ . The shaded areas in this figure and Fig. 4 are the 95%
confidence interval. (b) Upper: the filtered causality index vs. varying width of Gaussian kernel σ
(Marinazzo et al. (2008a)); lower: transfer entropy vs. r, the length scale (Schreiber (2000)); (c)
The causality index for breath→heart (lower) and heart→breath (upper) in Ancona et al. (2004).
bricks. Reasoning from the action backward on causal chains ball-x→action and ball-y→action, it
understands that the teacher agent can obtain high rewards by making actions based on the ball’s x
and y position (there are no causal chains from the action forward in Fig. 1 so the agent as to learn
on its own). The agent Can then utilize the above reasoning to develop policies π(a∣s) where the
state s can focus on the ball’s x and y position instead of the entire environment state, and the action
a can focus on those that can maximally influence not only the reward, but also the ball-y and brick
that is on the reward’s backward causal chain. This significantly reduces the search space for the
policies, and the agent may probably require significantly less number of episodes to learn a good
policy. It will be an exciting future research direction to incorporate this causal reasoning into RL
and IL to improve sample efficiency.
4.3 heart-rate vs. breath-rate and rat brain EEG datasets
Now we test our algorithm with real-world datasets. As a common dataset studied in previous causal
works, we use the time-series of the breathing rate and instantaneous heart rate of a sleeping patient
suffering from sleep apnea (samples 2350-3550 of data set B from Santa Fe Institute time series
contest held in 1991, available in PhysioNet). We apply our method to infer the causal strength
between the breathing rate and heart rate, with different maximum time horizon K . The result is
shown in Fig. 3. The causal strength from heart to breath is significantly higher than the reverse
direction, consistent with the results from previous causal inference methods (Schreiber (2000);
Ancona et al. (2004); Marinazzo et al. (2008a)) as also shown in Fig. 3. Notably, the causal strength
remains at roughly the same level for different Ks, in contrast to the decaying causality index w.r.t.
increasing history length in (Ancona et al. (2004), Fig. 3 (c)) showing a merit of our method in
estimating causal strength across different time-horizons, aided by the flexibility of fθ in extracting
the right information to predict the future. The implementation details in this section is provided in
Appendix G.
As a second real-world example, we apply our algorithm in estimating the causal strength of the
EEG signals between the right and left cortical intracranial electrodes (Quiroga), also studied in
Ancona et al. (2004); Quiroga et al. (2002); Marinazzo et al. (2008a). Figure 4 (left) shows the
inferred causal strength Wji for the EEG signals of a normal rat. We see that there is only a slight
asymmetry, with the right channel having a slightly stronger influence on the left channel than the
reverse direction. Figure 4 (right) shows Wji for the EEG signals with unilateral lesion in the rostral
pole of the reticular thalamic nucleus. We see that there is stronger causal influence from the left
to the right channels. Compared with the result of previous works Ancona et al. (2004); Marinazzo
et al. (2008a) as also shown in Fig. 7, we see that all methods correctly infer the causal relations
before and after brain lesion. In addition, our method shows non-decaying causal strength with
increasing history length, in contrast to the decaying causality index in Ancona et al. (2004), again
demonstrating our method’s insensitivity against history length, due to its flexibility in extracting the
right amount of information in order to predict the future. The above two applications demonstrate
our method’s capability in inferring the causal relations from noisy, real-world data.
9
Under review as a conference paper at ICLR 2019
Figure 4: Causal strength inferred by our method with the EEG datasets, for different maximum time
horizon K, averaged over 50 initializations of fθ. (Left) Causal strength Wji for the EEG signal for
a normal rat. (Right) Causal strength Wji of EEG signal from the same rat, after brain lesion.
5 Discussion and Conclusion
In this paper, we have addressed causal inference by proposing a novel causal measure, defined via
a novel learnable noise risk, that allows function approximators like neural nets to learn complex
causal relations via the Causal Inference with Learnable Noise (CILN) algorithm. We provide in-
tuitions that the algorithm is likely to discover variables that directly structurally cause the variable
of interest. We demonstrated in synthetic nonlinear datasets that our method outperforms previous
methods by a large margin in inferring causal relations with complex structures, can scale to a large
number of time series, and is hardly influenced by the capacity of the function approximator. Our
method also correctly infers the causal arrows by watching a trained CNN playing Breakout, and
give causal directions consistent with prior works in real-world heart-rate vs. breath-rate and rat
EEG datasets.
Our CILN algorithm provides many opportunities for downstream tasks. As discussed in Section
4.2, we can incorporate CILN into reinforcement learning or imitation learning, and by reasoning
backward and forward with the discovered causal chains, the agent may be able to learn useful
policies with much less number of episodes. It may also help interpret the learned neural nets, by
quantifying the causal strength from the input to different hidden neurons to the output neurons.
Since Wji is based on mutual information which is scale-free, it may provide additional useful in-
formation on the internal mechanisms of neural net, in addition to the weights and gradients. CILN
can also help decipher complex real-world systems, for example in neuroscience, economics and
finance. As for the error effect of the algorithm, it depends on how the inferred causal matrix is
utilized qualitatively or quantitatively downstream. For application in RL/IL, a falsely discovered
causal arrow will increase the policy search space and thus reduce sample efficiency, but may not
influence final performance since the RL/IL algorithm will eventually learn a (sub)optimal policy
within the larger search space. Missed causal arrows (false negatives) may lead to the agent’s neg-
ligence of certain good policies, therefore it is important for the RL/IL algorithm to always allow a
certain amount of exploration in addition to focusing policies on the causal chain. For helping inter-
pretability of neural nets and deciphering complex real-world systems, the false positives/negatives
of causal relations will influence the qualitative understanding and potential decision making down-
stream, and it is important to set a higher/lower threshold for the causal score depending on whether
false positives or negatives are deemed less desirable.
Above all, we believe our work can not only pave the way for future exciting advancements in en-
abling machine learning models to understand causality, a key component of human intelligence, but
also endow researchers with a useful tool for deciphering the causal relations in complex systems.
References
Nicola Ancona, Daniele Marinazzo, and Sebastiano Stramaglia. Radial basis function approach to
nonlinear granger causality of time series. Physical Review E, 70(5):056221, 2004.
10
Under review as a conference paper at ICLR 2019
Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. Mine:
mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Francesc X Bosch, Attila Lorincz, NUbia Munoz, CJLM Meijer, and Keerti V Shah. The causal
relation between human papillomavirus and cervical cancer. Journal of clinical pathology, 55(4):
244-265, 2002.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Daphna Buchsbaum, Sophie Bridgers, Deena Skolnick Weisberg, and Alison Gopnik. The power of
possibility: Causal learning, counterfactual reasoning, and pretend play. Philosophical Transac-
tions of the Royal Society of London B: Biological Sciences, 367(1599):2202-2212, 2012.
David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine
learning research, 3(Nov):507-554, 2002.
Povilas Daniusis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun
Zhang, and Bernhard Scholkopf. Inferring deterministic causal relations. arXiv preprint
arXiv:1203.3475, 2012.
Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In
Proceedings of the 23rd international conference on Machine learning, pp. 233-240. ACM, 2006.
Gopikrishna Deshpande, Stephan LaConte, George Andrew James, Scott Peltier, and Xiaoping Hu.
Multivariate granger causality analysis of fmri data. Human brain mapping, 30(4):1361-1373,
2009.
Mingzhou Ding, Yonghong Chen, and Steven L Bressler. Granger causality: basic theory and ap-
plication to neuroscience. Handbook of time series analysis: recent theoretical developments and
applications, pp. 437-460, 2006.
Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient
reinforcement learning. In Proceedings of the 25th international conference on Machine learning,
pp. 240-247. ACM, 2008.
Karl J Friston, Lee Harrison, and Will Penny. Dynamic causal modelling. Neuroimage, 19(4):
1273-1302, 2003.
Clive Granger and Paul Newbold. Forecasting Economic Time Series. Elsevier, 2 edition, 1986.
URL https://EconPapers.repec.org/RePEc:eee:monogr:9780122951831.
Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica: Journal of the Econometric Society, pp. 424-438, 1969.
Clive WJ Granger, Bwo-Nung Huangb, and Chin-Wei Yang. A bivariate causality between stock
prices and exchange rates: evidence from recent asianfluaYE⅛. The QuarterlyReviewofEconomics
and Finance, 40(3):337-354, 2000.
Naftali Harris and Mathias Drton. Pc algorithm for nonparanormal graphical models. The Journal
of Machine Learning Research, 14(1):3365-3383, 2013.
Alain Hauser and Peter Buhlmann. Characterization and greedy learning of interventional markov
equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 13(Aug):
2409-2464, 2012.
Craig Hiemstra and Jonathan D Jones. Testing for linear and nonlinear granger causality in the stock
price-volume relation. The Journal of Finance, 49(5):1639-1664, 1994.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
11
Under review as a conference paper at ICLR 2019
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlin-
ear causal discovery with additive noise models. In Advances in neural information processing
SyStemS,pp. 689-6§6, 2009.
Dominik Janzing, Joris Mooij, KUn Zhang, Jan Lemeire, Jakob Zscheischler, Povilas Daniusis, Bas-
tian Steudel, and Bernhard Scholkopf. Information-geometric approach to inferring causal direc-
tions. Artificial Intelligence,182:1-31,2012.
Dominik Janzing, David Balduzzi, Moritz Grosse-Wentrup, Bernhard Scholkopf, et al. Quantifying
causal influences. The AnnalS of StatiSticS, 41(5):2324-2358, 2013.
Wayne Joerding. Economic growth and defense spending: Granger causality. Journal of Develop-
ment EconomicS, 21(1):35-40, 1986.
Ken Kansky, Tom Silver, David A M6ly, Mohamed Eldawy, Miguel Ldzaro-Gredilla, Xinghua Lou,
Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot
transfer with a generative causal model of intuitive physics. arXiv preprint arXiv:1706.04317,
2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In AdvanceS in Neural Information ProceSSing SyStemS, pp. 971-980, 2017.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. PhyS-
ical review E, 69(6):066138, 2004.
Joseph T Lizier, Mikhail Prokopenko, and Albert Y Zomaya. Local information transfer as a spa-
tiotemporal filter for complex systems. PhySical Review E, 77(2):026110, 2008.
David Lopez-Paz, Krikamol Muandet, Bernhard Scholkopf, and Iliya Tolstikhin. Towards a learning
theory of cause-effect inference. In International Conference on Machine Learning, pp. 1452-
1461, 2015.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max
Welling. Causal effect inference with deep latent-variable models. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), AdvanceS in Neural Information ProceSSing SyStemS 30, pp. 6446-
6456. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7223- causal- effect- inference- with- deep- latent- variable- models.
pdf.
AUrelie C Lozano, Naoki Abe, Yan Liu, and Saharon Rosset. Grouped graphical granger modeling
for gene expression regulatory networks discovery. BioinformaticS, 25(12):i110-i118, 2009.
Daniele Marinazzo, Mario Pellicoro, and Sebastiano Stramaglia. Kernel method for nonlinear
granger causality. PhySical review letterS, 100(14):144103, 2008a.
Daniele Marinazzo, Mario Pellicoro, and Sebastiano Stramaglia. Kernel-granger causality and the
analysis of dynamical networks. PhySical review E, 77(5):056215, 2008b.
Lukas Meier, Sara Van De Geer, and Peter Buhlmann. The group lasso for logistic regression.
Journal of the Royal StatiStical Society: SerieS B (StatiStical Methodology), 70(1):53-71, 2008.
A Michotte. The perception of causality. 1963.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
12
Under review as a conference paper at ICLR 2019
M Morf, A Vieira, T Kailath, et al. Covariance characterization by partial autocorrelation matrices.
TheAnnalsofStatistics, 6(3):643-648,1978.
Preetam Nandy, Alain Hauser, Marloes H Maathuis, et al. High-dimensional consistency in score-
based and hybrid structure learning. The Annals of Statistics, 46(6A):3151-3183, 2018.
Guilherme Neves, Sam F Cooke, and Tim VP Bliss. Synaptic plasticity, memory and the hippocam-
pus: a neural network approach to causality. Nature Reviews Neuroscience, 9(1):65, 2008.
A Papoulis. Probability, random variables and stochastic processes. 1985.
Judea Pearl. Causality: models, reasoning, and inference. IIE Transactions, 34(6):583-589, 2002.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl et al. Causal inference in statistics: An overview. Statistics surveys, 3:96-146, 2009.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements ofcausal inference: foundations
and learning algorithms. MIT press, 2017.
PhysioNet. Physionet data bank. URL http://www.physionet.org/.
R Quian Quiroga, A Kraskov, T Kreuz, and Peter Grassberger. Performance of different synchro-
nization measures in real data: a case study on electroencephalographic signals. Physical Review
E, 65(4):041903, 2002.
Rodrigo Quian Quiroga. The dataset can be downloaded from. URL www.vis.caltech.edu/
~rodri.
Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regu-
larization for deep neural networks. Neurocomputing, 241:81-89, 2017.
Brian J Scholl and Patrice D Tremoulet. Perceptual causality and animacy. Trends in cognitive
sciences, 4(8):299-309, 2000.
Thomas Schreiber. Measuring information transfer. Physical review letters, 85(2):461, 2000.
Anil K Seth, Adam B Barrett, and Lionel Barnett. Granger causality analysis in neuroscience and
neuroimaging. Journal of Neuroscience, 35(8):3293-3297, 2015.
Vikas Sindhwani, Minh Ha Quang, and Aurelie C Lozano. Scalable matrix-valued kernel learn-
ing for high-dimensional nonlinear multivariate regression and granger causality. arXiv preprint
arXiv:1210.4792, 2012.
Peter Spirtes, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory
Cooper, and Thomas Richardson. Causation, prediction, and search. MIT press, 2000.
James H Stock and Mark W Watson. Interpreting the evidence on money-income causality. Journal
of Econometrics, 40(1):161-181, 1989.
Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing
bayesian network structure learning algorithm. Machine learning, 65(1):31-78, 2006.
Halbert White and Karim Chalak. Settable systems: an extension of pearl’s causal model with
optimization, equilibrium, and learning. Journal of Machine Learning Research, 10(Aug):1759-
1799, 2009.
Halbert White and Xun Lu. Granger causality and dynamic structural systems. Journal of Financial
Econometrics, 8(2):193-243, 2010.
Halbert White, Karim Chalak, and Xun Lu. Linking granger causality and the pearl causal model
with settable systems. In NIPS Mini-Symposium on Causality in Time Series, pp. 1-29, 2011.
Peter A White and Alan Milne. Phenomenal causality: Impressions of pulling in the visual percep-
tion of objects in motion. The American journal of psychology, 110(4):573, 1997.
13
Under review as a conference paper at ICLR 2019
Appendix
A Theorem 1 and proof
Here, we formally propose a theorem corresponding to the statement in section 3.2, and provide
proof afterwords.
Theorem 1. For system Eq. 1, for any i, j ∈ {1, 2, ...N}, i 6= j, if Xt(-j)1 Granger-causes xt(i), then
Xt(-j)1 directly structurally causes xt(i).
Proof. We base the proof on the Theorem 5.6 in White et al. (2011). Firstly, by definition, the system
Eq. (1) belongs to the canonical settable system (Def. 3.3 in White et al. (2011)), on which their
Theorem 5.6 is based. To prove that in our system Granger causality can deduce direct structural
causality, we only have to prove that the assumption A.1 and assumption A.2 in White et al. (2011)
are satisfied by our system. If we identify our xt(i) with their Y1,t, our Xt-1 with their Yt-1,
our xt(j) with their Y2,t, our ui,t (our ui at time t) with their U1,t, our uj,t with their U2,t, their
Zt = 0, Wt = 0, then our system Eq. (1) satisfies their Assumption A.1. Additionally, by
definition, our ui ∈ RM, i = 1, 2, ...N are random variables that are mutually independent, and
also independent of any Xt(-i)1 , x(ti), i ∈ {1, 2, ...N}. Therefore, our system satisfies their strict
exogeneity (Yt-1 , Zt) ⊥ U1,t (in our representation (Xt-1, 0) ⊥ ui,t), which is a sufficient
condition for Assumption A.2. Therefore, both their Assumption A.1 and Assumption A.2 are
satisfied by our system Eq. (1). Applying their Theorem 5.6, we prove Theorem 1.
B Analyzing the property of the learnable noise risk
Firstly we state the assumption that will be used throughout this section:
Assumption 1. Assume that fθ ∈ F is a continuous function and has enough capacity so that it can
approximate any R dxt(i) P (xt(i) |Xt-1)x(ti). Let j 6= i and assume that P(Xt(-j)1) has support with
intrinsic dimension of KM .
Also we emphasize that in this paper, the expected risks (with symbol R) are w.r.t. the distributions,
and the empirical risks (with symbol R) are w.r.t. a dataset drawn from the distribution, with finite
number of examples. The theorems in this paper are all proved w.r.t. distributions (assuming infinite
number of examples). Sample complexity results will be left for future work.
The structure of this section is as follows. First in subsection B.1 and B.2, we prove three lemmas
that will be helpful for the following analysis. Then in subsection B.3, we analyze the property of
the learnable noise risk both qualitatively and quantitatively, and argue why the learnable noise risk
is likely to select the variables that directly structurally causes x(ti) .
B.1	Proving a lemma
Here we prove Lemmas 1.1.
Lemma 1.1. Suppose that Assumption 1 holds, we have
argminfθ RX,x(i)[fθ] =	dxt(i)P(x(ti)|Xt-1)xt(i) and minfθRX,x(i)[fθ] = EXt-1,x(i)	xt(i) -	dxt(i)P(xt(i)|Xt-1)xt(i)	(7) (8)
In other words, for the MSE risk, its minimum is attained when fθ (Xt-1) is the expectation of xt(i)
conditioned on Xt-1.
14
Under review as a conference paper at ICLR 2019
Proof. The proof of the lemma is adapted from Papoulis (1985). The risk
RX,x(i) [fθ] = EX	x
,	t-1,x
- fθ(Xt-1)
(ti)
/ dXt-ιdXti) ∙ P(Xt-ι,x(i))卜(i) - fθ(Xt-1))2
dXt-1P(Xt-1) dx(ti)P(x(ti)|Xt-1) x(ti) - fθ(Xt-1)
2
Note that here	(xt(i)	-	fθ(Xt-1))2	≡	xt(i)	-	fθ(Xt-1),	xt(i)	-	fθ(Xt-1)	is an inner product in
RM.
For any Xt-1, treating fθ(Xt-1) ∈ RM as a vector, let’s calculate its value such that the integral
F (fθ (Xt-1)) := R dxt(i)P(xt(i) |Xt-1) x(ti) - fθ(Xt-1)	attains its minimum.
Let
∂
0= ∂f^ F (fθ (XtT))	2
=dfθ(XT) Z dXt(iP(x(i)∣Xt-i) (x(i) - fθ(Xt-1))2
= -2	dxt(i)P(xt(i)|Xt-1) xt(i) - fθ(Xt-1)
we have
/ dx(i)P (χti)∣Xt-ι)x(i) = / dxti)P (χti)∣Xt-ι)fθ (Xt-1)
=fθ(Xt-ι) / dx(i)P(x(i)∣Xt-ι)
= fθ(Xt-1)
Therefore, for any Xt-1, fθ(Xt-1) = R dx(ti)P(x(ti) |Xt-1)xt(i) is the only stationary point for
F(fθ(Xt-1)).
Taking the second derivative, we have
Wd2、、2 F(fθ(Xt-I)) = 2 I dx(i)P(x(i) Xt-I)I = 2I
(∂fθ(Xt-1))2
where I is an M × M identity matrix, which is always positive definite.
Therefore, for any Xt-1, fθ(Xt-1) = R dxt(i) P (xt(i) |Xt-1)xt(i) is the only global minimum of
F(fθ(Xt-1)) w.r.t. fθ(Xt-1).
Since
RX,x(i) [fθ] =	dXt-1P(Xt-1)F(fθ(Xt-1))
The minimum of the risk RX,x(i) [fθ] is attained iff F (fθ (Xt-1)) attains minimum at every Xt-1,
i.e.,
fθ(Xt-1) =	dxt(i)P(xt(i) |Xt-1)x(ti)
is true for any Xt-1. Given Assumption 1, we know that fθ ∈ F has enough capacity such that it
can approximate any R dx(ti)P(xt(i) |Xt-1)xt(i). Therefore,
and
argminfθ RX,x(i) [fθ]
Z dxt(i) P (xt(i) |Xt-1)xt(i)
minfθ RX,x(i) [fθ] = EXt-1,x(i)
15
Under review as a conference paper at ICLR 2019
B.2	Minimum MSE with different variables
Lemma 1.2. Suppose that Assumption 1 holds, and Xt(-U1),Xt(-V1), Xt(-W1) ⊂ Xt-1 are mutually ex-
clusive sets of variables satisfying
X(W1) ⊥ x(i)X(U),X(V),	X(V) K x(i)X(U), X(W1)
Then
minfθ EX (U) X(V) (i)	xt(i) - fθ(Xt(-U1),Xt(-V1))	< minfθEX(U) X(V) (i)
Xt-1 ,Xt-1 ,xt	Xt-1 ,Xt-1 ,xt
Proof. Since Assumption 1 holds, according to Lemma 1.1, Lemma 1.2 is equivalent to
xt(i) - fθ (Xt(-U1), Xt(-W1))
EX(U) X(V)	(i)
Xt-1 ,Xt-1 ,xt
xt(i) -Z dx(ti)P(x(ti)|Xt(-U1),Xt(-W1))xt(i)2
We have
EX(U) X(W)	(i)
Xt-1 ,Xt-1 ,xt
x(ti) - Z dx(ti)P(x(ti)|Xt(-U1),Xt(-W1))xt(i)2
Z	dXt(-U1)dXt(-W1)dx(ti)P(Xt(-U1),Xt(-W1),x(ti))x(ti)-Z	dx(ti)P(xt(i)|Xt(-U1),Xt(-W1))xt(i)2
Z	dXt(-U1)dXt(-V1)dXt(-W1)dxt(i)P(Xt(-U1),Xt(-V1),Xt(-W1),xt(i))	xt(i)	-Z	dx(ti)P(x(ti)|Xt(-U1),Xt(-W1))xt(i)2
=/……)P(X(U),X(V))P(XnV))∙
Z dxt(i)P(xt(i)|Xt(-U1),Xt(-V1))	xt(i) -Z dx(ti)P(xt(i)|Xt(-U1),Xt(-W1))xt(i)
> f dχ(u)dX(V)dX(W1)p (X(u),χ(V))p (X(W1) ∣X(U),X(V))∙
Z dxt(i)P(xt(i)|Xt(-U1),Xt(-V1))	xt(i) -Z dx(ti)P(xt(i)|Xt(-U1),Xt(-V1))xt(i)
2
2
Z dX(U) dX(V) P (X(U) X(V)) Z d (i)P( (i) |X(U) X(V))	(i) Z d (i)P( (i) |X(U) X(V)) (i)
dXt-1 dXt-1 P (Xt-1 , Xt-1 ) dxt P(xt |Xt-1, Xt-1 ) xt - dxt P(xt |Xt-1 , Xt-1 )xt
EX(U) X(V)	(i)
Xt-1 ,Xt-1 ,xt
The third equality (the one before the inequality) is due to that Xt(-W1) ⊥ xt(i) |Xt(-U1), Xt(-V1), lead-
ingtoP(Xt(-U1),Xt(-V1),Xt(-W1),xt(i)) = P(Xt(-U1),Xt(-V1))P(Xt(-W1)|Xt(-U1),Xt(-V1))P(xt(i)|Xt(-U1),Xt(-V1)).
The inequality step first uses the setting in Eq. (1) that the noise variables ui are
effective arguments of the response functions hi , and that each hi is “causality in
mean". Therefore, R dxt(i) P (xt(i) |Xt(-U1), Xt(-V1))x(ti) 6= R dxt(i)P(xt(i) |Xt(-U1), Xt(-W1))x(ti). Us-
ing Lemma 1.1, we have fθ(Xt(-U1), Xt(-V1))	= R dxt(i)P(x(ti) |Xt(-U1), Xt(-V1))xt(i) minimizes
R dxt(i)P(xt(i) |Xt(-U1), Xt(-V1)) xt(i) - fθ(Xt(-U1), Xt(-V1)) , hence the inequality.
Lemma 1.3. Suppose that Assumption 1 holds, and Xt(-D1) ⊆ Xt-1 are the set of variables that
directly structurally causes xt(i). Then ∀Xt(-S)1 ⊆ Xt-1 with Xt(-S)1 6= Xt(-D1), we have
16
Under review as a conference paper at ICLR 2019
Specifically, we have
minfθ EX (D)
t-1
minfθ EX(D)
< minfθ E (S)
Xt-1
- fθ(Xt(-D1))2
< minfθEX(D)
Xt-1
— fθ (Xt(D)))2
where X(D) = Xt-i\X(D).
Proof. For 孙 X(S1,letX(U)= X(D) ∩X(S1, X(V) = X(D)\X(S1, X(T= Xt(S 1∖X(D).Then
Xt(-U1), Xt(-V1), Xt(-W1) are mutually exclusive, and Xt(-D1) = Xt(-U1) ∪Xt(-V1), Xt(-S)1 = Xt(-U1) ∪Xt(-W1). Now
we prove that ∀Xt(-S)1 ⊆ Xt-1 with Xt(-S)1 6= Xt(-D1) , the corresponding Xt(-U1) , Xt(-V1) , Xt(-W1) , xt(i) sat-
isfy the condition for Lemma 1.2. Since Xt(-D1) are the set of variables that directly structurally causes
xt(i) , there does not exist a Xt(-S)1 such that the corresponding Xt(-V1) ⊥ x(ti) |Xt(-U1) , Xt(-W1) (otherwise
it violates the definition of direct structural causality). Thus Xt(-V1) ⊥6⊥ xt(i) |Xt(-U1) , Xt(-W1) . To prove
Xt(-W1) ⊥ xt(i)|Xt(-U1),Xt(-V1)
, note that Xt(-W1) does not directly structurally cause xt(i) , then by Theo-
rem 1, Xt(-W1) does not Granger-cause xt(i), i.e. P (xt(i) |Xt(-U1), Xt(-V1)) = P (x(ti) |Xt(-U1), Xt(-V1), Xt(-W1)),
which is equivalent to Xt(-W1) ⊥ xt(i) |Xt(-U1) , Xt(-V1) . The special case of Xt(-D1) follows directly that
X(D) = Xt-ι∖X(D) = X(D) and letting Xt-I = X(D).	■
B.3 Qualitative and quantitative behaviors of the learnable noise risk
In this section, we analyze the qualitative and quantitative behaviors of the learnable noise risk
(Eq. 2), with varying noise levels ηj. For each variable Xt(-j)1 ∈ Xt-1, j = 1, 2, ...N, define
Pj= tanh (I(Xtj)i； XM，))) ∈ [0,1]as a “rescaled" mutual information between Xt(-j)1 and
Xt-)(ηj). When n=0 so that Xt-ηηj = Xt-、, ρj = 1. When all elements of / → ∞, Pj = 0.
Denoting ρ = (ρ1, ρ2, ...ρN), we can then rewrite the learnable noise risk (Eq. 2) as
N
Rχ,x(i) [fθ, p] = MMSEci)(P) + λ ∙ X arctanh(ρj)	(9)
j=1
where MMSE(i)(p)	=	mi□η,fθ EX一,xf： ](xCi)- fθ(XCn)ι)) [ subject to Pj =
tanh (I(Xtj)i； Xtj)(nn)) ,j = 1,2,…N. Let X(D) ⊆ Xt-1 be the set of variables that
directly structurally causes x(ti) , and denote the corresponding set of Pj as ρ(D) .	Denote
^ ^)∖
Xtj1 = Xtj1\Xtj1 and the corresponding set of Pj as ρ(D). For any i = 1, 2, ...N, we have the
following properties:
1.	MMSE(i)(ρ) attains maximum at ρ = 0.
2.	MMSE(i)(ρ) is monotonically decreasing w.r.t. each Pj.
3.	MMSEti)(P)IP(0 = ι,ρ5) = 0 < MMSECi)(P)IP(D)=0,ρ(D)=1 (using Lemma 1.3).
4.	MMSE(i)(p) attains minimum at P(D) = 1. MMSE(i)(p)|p(n)=i is constant w.r.t. P(D).
17
Under review as a conference paper at ICLR 2019
To get a better intuition of the landscape of RX,x(i) [fθ, ρ], let’s investigate a simple example. Let
the response function be:
{xt : := h1 (UI) = √ςx ∙ u1
x(2):= h2(X(1)i，U2) = X(1)1 + √ΩX ∙ U2	(IO)
X(3)：= h3(X(2)i，U3) = X(2)i + p∕∏y ∙ U3
where u1 , u2 , u3 are independent unit Gaussian variables, and Xt-1
(Xt(-1)1,Xt(-2)1,Xt(-3)1)
((X(I)2,x(1)i), (Xt2)2,x(2)i), (X(3)2,x(3)i))∙	For RXN [fθ, ρ] = MMSE⑶(P) + λ ∙
Pj3=1 arctanh(ρj), since only xt(-1)2 and xt(-2)1 are d-connected to xt(3), at the minimization of
Rx,χ(3) [fθ, ρ], only x(1)2 and χ(2)ι may have a finite j (the other j are all infinite). There-
fore, setting the ηj,l not corresponding to X(t1-)2 and Xt(2-)1 as infinity, and letXt-)2 = x(1)2 + ηx ∙ eχ,
X(2)1 = x(2)1 + ηy ∙ Ey, Ex and Ex being independent unit Gaussian variables. Let fθ(χ(1)2, χ(2)ι)=
a ∙ χ(1)2 + b ∙ χ(2)ι, then We can get an analytic expression for Rχ,x⑶[fθ, ηx, ηy]:
RX,x(3)[fθ,ηx,ηy]
=a2% + (b-1)2(ςx + ωX) + a2ηx + b2ηy + 2a(b — 1»x + Cy + 2log (1 +—χ) + — log (1 +
Σx + Ωx ʌ
η2	)
Minimizing RX,x(3) [fθ, ηx, ηy] W.r.t. a and b, We get
a* =________________η2∑χ__________________
ηx η2 + ηx ςx + ηy*x + η2αx + ax£x
b* =	ηx (£x + αx) + ςxωx
ηx η2 + ηx ςx + 呜立 + η2αx + ax£x
Substituting into RX,x(3) [fθ, ηx, ηy], We have
RX,x(3)[ηx,ηy]
= min RX,x(3) [fθ, ηx, ηy]
fθ
=________ny(*x°x + η2(*x + °x))_________+ λloJι + 殳ʌ + λlog (1 + ⅛+⅛ʌ
ηxη2 + ηx∑x + η2∑x + ηxΩx + Ωx∑x + 2 g1+ η2) + 2 g1+ η2	)
Here we have neglected the constant Ωy. To obtain Rχ,x(3) [ρ], let ρι = tanh (2log(1 + ∑∑χ)),
P2 = tanh (2log(1 + ςx+Ωx)),we have ηx = 1-p~Σx, η2 = ^P(Σx + Ωx). Substituting, we
have
2
Rχ,x⑶[ρ] = MMSE⑶(P) + λ ∙ X arctanh(ρj)
j=1
(p2 - 1)(ςx + ωx)((pi - 1)ςx - (pi + I)Cx)
(1 + pi + p2 - 3p1p2»x + (1 + PI)(I + p2)Cx
+ λ ∙ arctanh(pι) + λ ∙ arctanh(p2)
Fig. 5 shows the landscape ofMMSE(3) (ρ) and Rχ,x(3) [ρ], for Σx = 1, Ωx = 2, λ = 1. We see that
MMSE(3)(ρ) satisfies the above mentioned four properties. Particularly, MMSE(3)(ρ)ρ =i,ρ =0 >
MMSE(3)(ρ)∣ρι =0 ρ2=r After adding λ ∙ arctanh(pi) + λ ∙ arctanh(p2), the Rχ,x(3) [ρ] has global
minimum along pi = 0 largely due to this property. Therefore, for this particular example, when
Rχ,x(3) [ρ] is minimized, Pi = 0, i.e. I(Xtl)2, χt-)2ηι)) = 0.
18
Under review as a conference paper at ICLR 2019
(a)	(b)
Figure 5:	(a) MMSE(3) (ρ) and (b) Rx,x(3) [ρ] in section B.3, for Σx = 1, Ωx = 2, λ = 1.
By varying the value of λ, we can tune the relative influence of the two terms MMSE(3) (ρ) and
Pj2=1 arctanh(ρj). The landscape corresponding to λ = 0.01, 0.5, 2, 10 are plotted in Fig. 6. We
see that when λ	1, the MMSE term dominates, and it is possible that the global minimum
of RX,x(3) [ρ] is not at ρ1 = 0. This is similar to the effect of a L1 regularization, where if the
coefficient λ for the L1 is vanishingly small, the L1 regularization will barely influence the loss
landscape. When λ is not vanishingly small, as in Fig. 6 (b), we see that the global minimum of
RX,x(3) [ρ] lies on ρ1 = 0. When λ → +∞, the Pj2=1 arctanh(ρj) term dominates and the global
minimum is at ρ1 = 0, ρ2 = 0.
In general, we expect RX,x(i) [ρ] behave qualitatively similar. When λ → +∞, the global minimum
for RX,x(i)[ρ] is at ρ* = 0. As We ramp down λ, the dimension that has largest influence on MMSE
Win first host the global minimum with nonzero Pj, which is most likely the variable that directly
structurally causes xi(i). When λ is further ramping down, we expect that the variables that host the
(i)
global minimum with nonzero ρj will more likely be those that directly structurally causes xi , due
to the landscape influenced by the four properties of MMSE. This can justify the learnable noise
risk as a good objective for causal discovery/variable selection. The experiments in the paper will
empirically test the performance of the learnable noise risk.
C Upper bound for the learnable noise risk
In this section, we prove that I(X(-)(ηj); Xt-)1) ≤ 1 PlK=M log (l + Var(XtT,l)) . We formally
state the theorem as follows:
Theorem 2. Let Xt(-)-(ηj) := Xt-)1 + % ∙ 6j∙, j = 1, 2, ...N be the noise-corrupted inputs with
learnable noise amplitudes ηj∙ ∈ Rkm, and ej∙〜N(0, I). We have
(11)
where l is the lth element of a vector, std(Xt(-j)1,l ) is the standard deviation of Xt(-j)1,l across t. The
equality is reached when Xt(-j)1 obeys a multivariate Gaussian distribution with diagonal covariance
matrix Σ satisfying Σl,l = Var(Xt(-j)1,l ) + ηj2,l .
19
Under review as a conference paper at ICLR 2019
(a)	(b)
(c)	(d)
Figure 6:	(a) RX,x(3) [ρ] for (a) λ = 0.01, (b) λ = 0.5, (c) λ = 2 and (d) λ = 10 in section B.3, for
Σx = 1, Ωx = 2.
20
Under review as a conference paper at ICLR 2019
Proof. We have
I (Xj(Ij); XjI) = H(Xj(Ij)) - H (ηj ∙ q)
=H (XjSj))-(KM lθg(2πe) + X 1 lθg(η2,ι)!
Here H(∙) is differential entropy. For X(-)(ηj), its variance at the lth dimension is
Var(Xt-R)) = Var(X(-)ι,ι + ηj ∙ ð)
=Var(Xt-)ι,ι) + Var%ι ∙ j)
= Var(Xt(-j)1,l) + ηj2,l
The second equality is due to that Xtt-j)1 is independent of j. Using the principle of maximum
entropy, the distribution that maximizes H(Xt-)(ηj)) subject to the constraint of Var(X(-)(7))=
Var(Xtt-j)1,l) + ηj2,l, l = 1, 2, ...KM is a Gaussian distribution whose diagonal covariance matrix Σ
satisfies ∑ι,ι = Var(X(-]j+ *厂 Itsentropyis H(X(-)(ηj)) = KMlog(2πe) + PKM 2log(η2,ι +
Var(Xtt-j)1,l)). Therefore,
I(Xjtj； Xj)
≤ (KMlog(2πe) + X1 log(η2,ι + Var(X(-)ι,ι))! - (KMlog(2πe) + X1 log(η2,ι)
l=1	l=1
1 KM. V -Var(X(j)ι,ι)∖
=2X log (1 + FH
The equality is reached when Xt(-j)1 obeys a multivariate Gaussian distribution with diagonal covari-
ance matrix Σ satisfying Σl,l = Var(Xt(-j)1,l) + ηj2,l.
D Implementation details for the methods
Here we state the implementation details for our method, as well as other methods being compared.
Throughout this paper, unless otherwise specified, we use the standard technique in Kraskov et al.
(2004) to estimate the KL-divergence and mutual information, which is used in our implementations
of Mutual information, Transfer Entropy and Causal Influence.
D.1 Our method
Without stating otherwise, our method (Algorithm 1) as a default uses a three layer neural net, with
two hidden layers having 8 neurons and SELU (Klambauer et al. (2017)) activation, and the last layer
having linear activation. Adam (Kingma & Ba (2014)) optimizer with learning rate = 10-4 is used
as default throughout this paper. We set η0 = 0.01 and λ = 0.01. We use 10000 epochs, with early-
stopping such that if the best validation loss does not go down for 40 monitoring points (we monitor
the validation loss every 20 epochs), do early-stop. It also has a 400 epoch warm-up period where
the mutual information term is turned off, to allow fθ to find a good initial model as a start. We use
the approximation I(Xj(In); Xt-I) ` 2 PKM log(1 + 1∕χ2 J in the risk and also in estimating
Wji, as discussed in the main text in 3.3. Here Xjl =——η⅛— is the relative noise scale w.r.t. the
,	std(Xt-1,l)
standard deviation of each element, l denoting the lth element of the K M -dimensional vector. In
this work, we fix χj,l to be the same for each j, and let χjbe a single parameter instead ofa vector.
This simplifies the risk calculation, and also to a first order invariant to the reparameterizaiton of
each time series Xt(-j)1 .
21
Under review as a conference paper at ICLR 2019
D.2 Transfer Entropy
We use the definition of transfer entropy as defined in Schreiber (2000). In that work the transfer
()	()	(.∙ (	(	( . ”(j)
entropy is defined for two time series. To deal with multiple time series, we let Xt-1 also include
other time series, similar to the extension of transfer entropy as in Lizier et al. (2008).
D.3 Causal Influence
For causal influence, we use the same network architecture as in our method, to learn a prediction
model. Then the KL divergence is estimated via the technique in Kraskov et al. (2004).
D.4 Linear Granger
We use the definition of linear Granger causality (Eq. (7) and (8) in Ding et al. (2006)) to calculate
linear Granger causality. Specifically, we estimate the variance of the residue of a linear predictor
(i)	(j(i')	(	(r(i')
of xt-1 with and without Xt-1 (also conditioned on Xt-1), using Levinson-Whittle(-Wiggins) and
Robinson algorithm (Morf et al. (1978)). Then the linear Granger causality equals the log of the
ratio of the two variances.
E	Implementation details for synthetic experiments
For all experiments in this section, each metrics is obtained by performing the experiments (includ-
ing generation of the dataset and the training) four times with seed = 0, 30, 60, 90 and averaging
the resulting metrics. For the ground-truth causal tensor A, each element Aji is a K × M matrix,
with 0.5 probability of being an all-zero matrix, and 0.5 probability of being a nonzero matrix. If
Aji is a nonzero matrix, its each element is sampled from a log-normal distribution with μ = 0 and
σ = 1. For B, each B is also a K × N matrix, with each element sampling from U [-1, 1]. We
use H1 (x) = softplus(x) = log(1 + ex), and H2 (x) = tanh(x) in equation (6). As a default, 500
time series each with length of 22 are generated from Eq. (6), each of which is wrapped into 19
(Xt-1, x(ti ) pairs (since K = 3), so there are in total 500 × 19 = 9500 examples for each dataset.
The train-test-split is 4:1 for all experiments in this paper.
F Details for the video game data s et
Here, we implement a custom Atari Breakout game in the OpenAI Gym (Brockman et al. (2016))
environment, mimicking the original game, where we can access the state of the ball, paddle and
bricks, etc. This representation is also used in the OO-MDP (Diuk et al. (2008)) paradigm for a
more efficient representation of the environment state. We use the DQN algorithm, the same CNN
architecture as in Mnih et al. (2015) to train an RL agent. Then we let it play the game for 20000
steps, obtaining a dataset with time-length of 20000 steps (if the agent dies, we restart the game)
and 6 time series: action, paddle’s x position, ball’s x position, ball’s y position, number of bricks
and reward. We then feed the time series (each time series normalized to mean of 0 and variance of
1) to our method, the same procedure as performed in the synthetic experiment, to let it produce an
inferred causal matrix Wji , which is shown in Fig. 1. All the datasets used in this paper and code
will be open-sourced upon publication of the paper.
G Implementation details for the real-world dataset
For the two real-world datasets, we obtain the data with the same procedure as in Ancona et al.
(2004). Then the data (each time series normalized to mean of0 and variance of 1) are fed into our
algorithm to infer the causal strength Wji with the default settings as described in Appendix D and
E. For each K = 1, 2, ...20, the experiments are run for 50 times with seed from 0 to 49, and Fig. 3
and Fig. 4 are obtained by averaging over the inferred W matrix.
H Causality results for the rat-EEG dataset by previous works
22
Under review as a conference paper at ICLR 2019
⑶
Figure 7: Causal indices for the rat EEG dataset with previous methods. (a) By Ancona et al. (2004).
Left: the variance for the left EEG (open circles) and right EEG (diamonds) vs. time lag m before
brain lesion. Right: the causality index after brain lesion. (b) By Marinazzo et al. (2008b). The
filtered causality index vs. varying p, the order of the inhomogeneous polynomial kernel, before
(upper) and after (lower) brain lesion.
23