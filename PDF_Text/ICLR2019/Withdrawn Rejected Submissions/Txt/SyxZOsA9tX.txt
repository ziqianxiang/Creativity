Accelerated Value Iteration via Anderson Mixing
Anonymous authors
Paper under double-blind review
Ab stract
Acceleration for reinforcement learning methods is an important and challenging theme.
We introduce the Anderson acceleration technique into the value iteration, developing an
accelerated value iteration algorithm that we call Anderson Accelerated Value Iteration
(A2VI). We further apply our method to the Deep Q-learning algorithm, resulting in the
Deep Anderson Accelerated Q-learning (DA2Q) algorithm. Our approach can be viewed as
an approximation of the policy evaluation by interpolating on historical data. A2VI is more
efficient than the modified policy iteration, which is a classical approximate method for
policy evaluation. We give theoretical analysis of our algorithm and conduct experiments
on both toy problems and Atari games. Both the theoretical and empirical results show the
effectiveness of our algorithm.
1	Introduction
In reinforcement learning (Sutton & Barto, 1998), an agent seeks for the optimal policy in a specific sequential
decision problem. Several algorithms have been proposed over the course of time, including the famous
Q-learning (Watkins & Dayan, 1992), SARSA (Rummery & Niranjan, 1994; Sutton & Barto, 1998), and
policy gradient methods (Sutton et al., 2000). In complicated decision problems where tabular representations
are intractable, function approximations are usually used for estimating state-action values (Kaelbling et al.,
1996; Sutton & Barto, 1998; Sutton et al., 2000). Inspired by the success of deep learning, Deep Q-Learning
(DQN) (Mnih et al., 2013) and its variants (Bellemare et al., 2017; Schaul et al., 2015; Van Hasselt et al.,
2016; Wang et al., 2015) utilize a deep neural network as the value approximator, which has successfully
solved end-to-end decision problems such as Atari2000.
The value iteration (VI) Bellman (1957) and policy iteration (PI) (Howard, 1964) are the most classical
methods for value updating. The main difference between them is that PI evaluate the current policy accurately
during the iteration while VI does not. Thanks to the accurate evaluation of the current policy, policy iteration
uses significantly less policy improvement steps to converge to the optimal value. Although PI has a faster
convergence rate than VI, most of the existing methods employ a rather slow value iteration procedure,
because thoroughly evaluating a policy is costly or even intractable under complex environments. To retain
the fast convergence property of policy iteration while reducing its computation overhead, researchers have
proposed several modifications to the original policy iteration (Alla et al., 2015; Puterman, 1994). The
modified policy iteration (MPI) method (Puterman, 1994) tries to deal with this problem by approximating
the solution to policy evaluation via the truncated Neumann expansion of an inverse matrix. However, this
approximation requires extra iterative steps, which is still computationally inefficient for complex decision
problems where sampling is costly, compared with the value iteration procedure where the policy iteration
step is skipped.
Interpolation methods have been widely used in first order optimization problems (Bubeck et al., 2015; Scieur
et al., 2016; 2017; Xie et al., 2018). These methods extract information from historical data and are proven to
converge faster than vanilla gradient methods. However, the interpolation method is not widely applied in
1
reinforcement learning. The most recent work related to interpolation is the averaged-DQN (Anschel et al.,
2016), which calculates the average Q-value over the history and demonstrated that such an operation is
effective for variance reduction.
Acceleration in value iteration and policy iteration has attracted researchers’ great attention. Classical
methods for accelerating value iteration include Gauss-Seidel value iteration (Puterman, 1994) and Jacobi
value iteration (JAC) (Puterman, 1994). More recently, Alla et al. (2015) proposed an acceleration method
that switches between a coarse-mesh value iteration and a fine-mesh policy iteration during different stages.
Laurini et al. (2016) performed a Jacobi-like acceleration method on dynamic programming problems. In
a recent work (Laurini et al., 2017), the value iteration procedure is accelerated by only updating a part
of the values. None of the previous methods have proposed acceleration methods with an application of
interpolation.
In this paper, to solve the policy evaluation problem more efficiently, we propose an alternative algorithm
based on multi-step interpolation. Explicitly, the solution to the policy evaluation problem is approximately
represented by a weighted combination of historical values, whose weights are adaptively updated by an
optimization procedure. To reduce the computational complexity, we resort to the Anderson mixing method
(Anderson, 1965; Walker & Ni, 2011; Toth & Kelley, 2015) to do the approximation with only a short length
of history. Our approach fits the gap between value iteration and policy iteration, ending in an updating
rule without adding much extra computational complexity to the original value iteration procedure. We also
extend this approach to deep reinforcement learning problems.
The remainder of this paper is organized as follows. In Section 2, we introduce the foundations of reinforce-
ment learning and present typical value updating algorithms. In Section 3, we derive the Anderson accelerated
methods. In Section 4, we give a theoretical analysis of the convergence of our method. In Section 5, we test
our method in different environments and empirically show the effectiveness of it. Finally, we conclude our
work in Section 6.
2	Preliminaries
In this paper we mainly consider a finite-state and finite-action scenario in reinforcement learning. In this case,
an Markov Decision Process (MDP) system is defined by a 5-tuple (S, A, P, r, γ), where S is a finite state
space, A is a finite action space, P ∈ R(lSl×lAl)×lSl is the collection of State-to-state transition probabilities,
r ∈ RlSl×lAl is the reward matrix, Y is the discount factor. A policy ∏ ∈ A|S| is a vector of actions at
each state. The transition matrix Pn ∈ RlSl×lSl and reward vector r∏ ∈ R|S| under policy ∏ are defined
as Pπ(i, j) = P ((i, π(i)), j), rπ(i) = r(i, π(i)). We further define the value vπ ∈ R|S| and the Q-value
qπ ∈ RlSl×lAl under a given MDP and policy, where each element of vπ and qπ is defined as
∞
V (S) = Eso=s,st+ι~p∏ (st,…)	γtrπ(st),
t=0
∞
qπ (s, a) = r(s, a) + EsI 〜P ((s,a),…),st+ι 〜Pn (st,...) EY trn(St).
t=1
We can verify that qπ = r + YPvπ. We define q∏ ∈ R1S1 by q∏(i) = qπ(i, ∏(i)), and say a vector to be
the maximum among a set if each entry of it is bigger than that of the other vectors. The values satisfy the
Bellman equation:
vπ = Γπ(vπ) = rπ + YPπvπ.
2
The policy π* = argmax∏ qπ is called the optimal policy, whose value or Q-ValUe is denoted as v* or q*.
Note that v* satisfies the Bellman optimality equation
v* = Γ(v*) = max(rπ + γPπv*).
Therefore, finding the optimal policy is equivalent to finding the fixed point of the operator Γ(v).
2.1 Fixed Point Iteration Methods
Value iteration (VI) is the most widely used and best-understood algorithm for solving Markov decision
problems. It solves the fixed point problem by iterating the following steps repeatedly,
v(t+1) = Γ(v(t)) = max(rπ + γPπv(t)).
π
An alternative solution is policy iteration (PI), which maintains both the value v (t) and the policy π(t) during
each iteration. The procedure alternatively iterates the following two steps:
•	Policy evaluation: Find a v(t) such that
v(t) = Γπ(t) (v(t)) = rπ(t) + γPπ(t)v(t),	(1)
which can be directly computed by
v(t) = (I - γPπ(t))-1rπ(t).	(2)
•	Policy improvement: Improve the current policy by
π(t+1) = argmax(rπ + γPπv(t)).
π
Theoretical analysis has shown that VI enjoys a γ-linear convergence rate (i.e., kv(t) - v* k∞ ≤ γkv(t-1) -
v* k∞), while PI converges much faster with kv(t) - v* k∞ ≤ Kkv(t-1) - v* k2∞ (Puterman, 1994), where K
is some constant related with γ and the given MDP. Both VI and PI are model-based, because the greedy policy
cannot be determined when r and P are unknown. The VI under q-notation is well-known as Q-learning
(Watkins & Dayan, 1992). We will analyze our method under v -notation, but our analysis also works under
the corresponding q-notation.
The main difference between VI and PI is whether the current policy is fully evaluated. Though PI converges
faster than VI, this advantage diminishes under several settings. In reinforcement learning, we can only
access an oracle that returns the reward and next state given the current state and selected action. Under such
a setting, each value iteration step can be performed by estimating Γ(v) through sampling. But the policy
evaluation step based on equation (2) becomes intractable because it is quite time-consuming to compute
(I - γPπ(t))-1. The modified policy iteration method (Puterman & Brumelle, 1978) partially solves the
problem by setting vt ≈ (Γπ(t) )mt (v(t-1)) where mt is a (possibly large) integer related to t. However, this
method requires to evaluate a series of values (Γπ(t) )i(v(t-1)) for i = 1, 2, . . . , mt, which is computationally
inefficient.
3	Anders on Accelerated Value Iteration
Based on the observation that full policy evaluation accelerates convergence, we propose an approximate policy
evaluation method. The method aims to approximately solve the policy evaluation problem, circumventing
the matrix inversion and iterative procedures mentioned above.
3
We first utilize the linearity of equation (1), defining Bπ(v) = Γπ(v) - v and converting the prob-
lem into an equivalent form of solving the equation Bπ(v) = 0. Suppose we have obtained a set of
values Bπ(v1), Bπ(v2), . . . , Bπ(vk) with respect to v1 , v2, . . . , vk. Consider to find a set of weights
α = (α1, α2, . . . , αk)T, subject to Pik=1 αi = 1, which satisfies that
k
X αiBπ(vi) = 0.
i=1
Then the combination V = Pk=I aivl will satisfy the following relationship:
k
Bn(V) = r∏ + γP∏V - V = X αi(r∏ + γP∏Vi- Vi)	(3)
i=1
k
= X αiBπ(Vi) = 0.	(4)
i=1
This relation implies V can be viewed as an approximate solution to equation (1) provided the sampling
estimations are accurate enough. However, this step needs to keep track of the previous values and recompute
Γπ on all of them. To reduce the huge memory usage and computation, we choose Vi from the recent
history, i.e., Vi = V(t-i), i = 1, 2, . . . , k, and replace Bπ(t) (V(t-i)) with the previously computed values
Bπ(t-i) (V(t-i)). This modification is based on the observation that the recent successive policies do not
change sharply and therefore Bπ(t-i) (V(t-i)) ≈ Bπ
(t) (V(t-i)). This modification approximately solves the
policy evaluation problem without model estimation or extra function evaluations.
Another critical issue is that we cannot guarantee the existence of α given that k is small, because the
dimension of Bπ (V) is usually much higher than k. Inspired by the Anderson acceleration technique
(Anderson, 1965; Ortega & Rheinboldt, 1970; Walker & Ni, 2011), we instead look for a combination of
{Bπ(t-i)(V(t-i))}ik=1,
α(t) = argmin kB(t)αk,	(5)
a∈Ω∩Λ
where B(t = (B∏(t-i)(V(t-1)), B∏(t-2)(V(t-2)),..., B∏(t-k) (V(t-k))), Ω = {α∣1Tα = 1}, Λ is an extra
constraint on the values attainable by α. Typically, Λ can be chosen from the following forms:
•	Total space, Λtot = Rk ;
•	Boxing constraint, Λboχ = {α∣ — ml ≤ α ≤ m1};
•	Convex combination constraint, Λcvχ = {α∣0 ≤ α ≤ 1};
•	Extrapolation constraint, Λeχp = {ɑ∣αι ≥ 1, αi ≤ 0,i = 2, 3,...,k}.
When the `2 norm is used and Λ = Λtot , the solution can be written explicitly as α(t) =
[(B(t))>B(t)]-11/1>[(B(t))>B(t)]-11, whose derivation is placed in the appendix. Note that ifwe simply
set V(t) = Pik=1 αi(t)V(t-i), the values will always locate in the subspace expanded by historical values
V(t-1), V(t-2), . . . , V(t-k). When the solution to equation (1) does not lie in such a subspace, there is no hope
for convergence with application of such updating rule directly. To jump out of the subspace, we perform an
extra value iteration step to this combination. Then we will get the updated value,
k
V(t) = max rπ + γPπ X αi(t)V(t-i) .
π	i=1
4
3.1	The Algorithm
Based on our previous discussion, we present the k-step Anderson Accelerated Value Iteration (A2VI) in
Algorithm 1. In the first k steps, the value is updated according to the original VI. Otherwise, we perform
an interpolation procedure, where the weights are attained from solving the problem (5). The original value
iteration algorithm can be viewed as a special case of our algorithm with k = 1.
Algorithm 1 Anderson Accelerated Value Iteration (A2VI)
Input: v(0),P, r,γ,k,T
1:	for t = 1, 2, . . . , T do
2:	Bπ(t-1) (v(t-1)) = max (rπ + γPπv(t-1)) - v(t-1)
π
3:	if t < k then
4:	v(t) = max(rπ + γPπv(t-1))
π
5:	else
6:	Calculate (α(1t) , α(2t) , . . . , α(kt)) by solving the optimization problem (5)
7:	v(t) = max rπ + γPπ hPik=1 αi(t)v(t-i)i
8:	end if
9:	end for
10:	π(T) = argmaxπ (rπ + γPπv(T))
11:	return v(T) , π(T)
Both Anderson Acceleration (AA) and A2VI have the same spirit of interpolating on historical data. However,
A2VI does not straightforwardly apply AA to the Bellman optimality equation. Note that AA has the updating
rule Vt = P aiB(vt-z), while a2vi exchange the order of the operator sum and B(∙) due to the motivation
from equation (3). This exchange puts the nonsmooth operator max out of the affine combination, simplifying
the theoretical analysis.
We present a geometric explanation on the iterative steps of VI, PI, A2VI under 1-dimensional case in Figure
1. In value iteration, v(t) is attained by making a vertical line at (v(t-1) , 0), finding its intersection with
the function line at (v(t-1) , B(v(t-1))), then drawing a line with slope -1 through (v(t-1) , B(v(t-1)))
and finding its intersection with the horizon axis at (v(t) , 0); In policy iteration, v(t) is attained by first
getting (v(t-1), B(v(t-1))) in the same way as value iteration, then calculating the tangent line through
(v(t-1), B(v(t-1))) and finding its intersection with the horizon axis. In Anderson accelerated value iteration,
each step is first performed in a similar style to policy iteration except that the tangent line is replaced with a
secant line. Then a value iteration step is performed to get v(t).
From the figure, we can see that VI only utilizes the current value of the Bellman residual, while PI is similar
to Newton’s method Puterman & Brumelle (1978), utilizing the gradient information to achieve a faster
convergence rate. Our method serves as an intermediate between them, each step of which is composed of an
ordinary value iteration step and a secant step. In specific, the replacement of the tangent line to a secant line
can be viewed as a quasi-Newton’s method, which is shown computationally more efficient while keeping a
fast convergence rate in several particular settings. Both PI and A2VI converge to the fixed point in a smaller
number of steps than VI. Compared with PI, A2VI is more practical because it approximates the tangent line
by a secant line, which circumvents the costly model estimation step.
3.2	Extension to Model-free Learning Algorithm
We can rewrite our algorithm under q-notation, and get the Anderson Accelerated Q-Learning (A2Q)
Algorithm shown in the appendix. Combined with the technique of deep learning, our method can be applied
to end-to-end decision problems, resulting in the Deep Anderson Accelerated Q-Learning (DA2Q) Algorithm
(Algorithm 2).
5
Figure 1: Geometric interpolation of VI, PI and A2VI.
Algorithm 2 Deep Anderson Accelerated Q-learning (DA2Q)
Input: M, N, T, γ, b, B, ε, η, K, C
1:	Initialize replay memory D to capacity N, initialize Q-value function Q with random weights θ
2:	θ-k = θ, α1 = 1, αk = 0 for k = 2, ..., K
3:	s = 0
4:	for episode = 1, 2, . . . , M do
5:	Initialize si 〜 ρ(s)
6:	for t = 1, 2, . . . , T do
7:	With probability ε select a random action at, otherwise select at = arg maxa Q(st, a; θ)
8:	Execute action at, observe reward rt and state st+1
9:	Store transition (st, at, rt, st+1) in D
10:	Sample a random minibatch of transitions {(sj, aj, rj, s0j)}jb=1 from D
11:	for j = 1, 2, . . . , b do
rj	for terminal state
rj + γ max X αk Q(s0j, a; θ-k)	for non-terminal state
13:	end for
14:	L(θ) = b Pj=i(yj-Q(Sj ,aj ； θ))2
15:	θ = θ -	η∂
16:	s = s +	1
17:	if s mod	C	= 0 then
18:
19:
20:
21:
22:
23:
24:
Assign θ-k = θ-(k-1) for k = K, K - 1, . . . , 2. Assign θ-1 = θ.
if s ≥ K(C - 1) then
Sample a random minibatch of transitions {(sj, aj, rj, s0j)}jB=1 from D
for j = 1, 2, . . . , B do
for k = 1, 2, . . . , K do
rj - Q(sj , aj; θ-k)	for terminal state
rj + γ max Q(s0j , a; θ-k) - Q(sj, aj; θ-k)	for non-terminal state
a
end for
djk =
25:	end for
26:	end if
27:	(α1,α2,. . . ,αK) = argmin(α1,α2,...,αK) PjB=1(PkK=1αkdjk)2 s.t. PkK=1 αk = 1
28:	end if
29:	end for
30:	end for
4 Theoretical Analysis
We first analyze of the local convergence of the A2VI algorithm under boxing constraint. Our result shows
that in a small neighborhood of the optimal value, our algorithm enjoys an exponential convergence rate.
6
Theorem 1. For any MDP with a unique optimal policy, there exists some δ > 0, such that for any initial
value v(0) ∈ U(v*) = {v∣∣∣v — v*k∞ ≤ δ}, the A2VI algorithm under boxing constraint maintains the
following properties:
⑴ kΓ(v㈤)—v⑴k∞ ≤ Ykr(V(tT))- V(T)k∞, ∀t = 1, 2,...;
(ii) kv㈤—v*∣∣∞ ≤ 舌 ∣∣Γ(v⑼)—v⑼k∞, ∀t = 1, 2,....
Generally, it is difficult to obtain the global convergence rate of A2VI, since the operation max is nonsmooth.
To guarantee the convergence, we introduce a rejection step to the original algorithm. We say v is monotonic
improving if Γ(v) ≥ v, and denote the set of such values as VB. We propose the A2VI algorithm with the
rejection step, which only differs with Algorithm 1 at line 6. After calculating α(t), we test whether the affine
combination Pik=1 αi(t)vt-i lies in VB. If the answer is negative, the interpolation step will be replaced with
an ordinary value iteration step. We put the pseudocode of A2VI with the rejection step in Appendix. With
this modification, we can have the following convergence properties.
Theorem 2.	For the A2VI algorithm with the rejection step with Λ = Λcvx, if v(0) ∈ VB, then we have
v(t) ∈ VB, kΓ(v(t)) — v(t)k ≤ γkΓ(v(t-1)) — v(t-1)k, ∀t = 1,2,...
Theorem 3.	For the A2VI algorithm with Λ = Λexp, if v0 ≥ 0 and v(0) ∈ VB, then we have
(a)	Monotone improving values, v(t-1) ≤ v(t) ≤ v*, v(t) ∈ VB, ∀t = 1,2,...
(b)	γ-Hnearconvergencerate, kv* — v(t)k∞ ≤ Y∣∣v* — v(t-1)k∞.
5	Experiments
To validate the effectiveness of our method, we conduct several experiments.
5.1	Experiments on Toy Models
We first test our method on three toy models. The first model is a randomly generated MDP with |S| = 100
and |A| = 50. The transition probabilities of the MDP are generated from a uniform distribution on [0, 1], and
the rewards are generated from a standard normal distribution. The second model is the N -Chain problem
with N = 100, where a reward of 0.1 is given at state 0 and a reward of 1 is given at state N. At each state, the
agent can either choose to move forward or backward, and will move to the selected direction with probability
0.9 and to the opposite direction with probability 0.1. The last model is a 20 × 20 Gridworld model, where a
reward of 1 is given at state (20, 20). At each state, the agent can choose one of the 4 directions and will move
to that direction with probability 0.7, or move to one of the other directions with probability 0.1 for each. We
perform the standard value iteration, policy iteration and Anderson accelerated value iteration with/without
the rejection step on these models. In our experiment, each policy iteration step is approximately solved by
the modified policy iteration method with 100 inner iterations. To compare our method with the averaged
updating scheme (Anschel et al., 2016), we further construct and compare our algorithm with the averaged
value iteration. The value of ∣∣vt — v*∣ w.r.t. step t is shown in Figure 2, where the results are averaged from
30 independent experiments.
From the results we can see that the policy iteration converges fastest for all of the three models, however,
since each of its steps includes 100 inner iterations, the actual computation cost is very high. Among value
iteration methods, the Anderson accelerated value iteration converges fastest. The acceleration effect is
remarkable in randomly generated MDPs, but A2VI slows down at the first few steps in the latter two
experiments. However, adding a rejection step solves the problem and attains a faster convergence rate.
Another observation is that in the toy model case, the averaged value iteration cannot be used for acceleration.
7
104
102
f 100
■I 10-2
O
S 10-4
I 10^6
δ
10-8
10-10
Figure 2: Experiment results on several toy models.
104
102
f 100
■I 10-2
O
S 10-4
I 10-
δ
10-8
10-10
—A2VI-5
---- A2VI-5, Rej
AVE-5
VI
PI
5.2	Experiments on Atari Games with Deep Learning Based Techniques
To figure out the performance of our method on complex environments, we apply our method to Atari games
from Gym (Brockman et al., 2016), which is a Python API to Arcade Learning Environment (Bellemare et al.,
2013). We compare DA2Q with DQN (Mnih et al., 2013) and Averaged-DQN (Anschel et al., 2016). Details
of the experiment settings are given in Appendix D.
As Figure 3 points out, our algorithm DA2Q obtains a significant improvement over both the original DQN
algorithm and the Averaged DQN algorithm. When compared with other interpolation method such as
Averaged-DQN, the overall performance of our method also tends to be stabler, always being superior than
other methods among all of the three environments, while the performance of Averaged-DQN varies a lot.
Alien	AnIldar
35M
Hme	Hme
Figure 3: Training Performance on Atari games, score is smoothed with 250 windows while the shaded area
is the 0.25 standard deviation.
Compared with DQN, the extra computational cost is actually low, since the α is updated only once every C
steps, which only involves an inversion on a very small-size matrix (k × k). The k target values are computed
parallelly in the TensorFlow (Abadi et al., 2016), which cost the same time as in DQN. Moreover, the extra
runtime can be ignored when compared with the costly back propagations and interaction with environments.
6 Conclusion
We have proposed the Anderson accelerated value iteration method, which is a novel acceleration approach
for reinforcement learning. We have proved the convergence property of our method under certain conditions.
Our algorithm empirically achieves a superior performance on toy models and several Atari games. Despite
the success of our algorithm, several questions remain open. The convergence analysis for the general case is
lacking, and we only provide convergence guarantees but do not give a theoretical analysis of the acceleration
effect of A2VI, which we leave for future work.
8
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning.
In OSDI, volume 16,pp. 265-283, 2016.
Alessandro Alla, Maurizio Falcone, and Dante Kalise. An efficient policy iteration algorithm for dynamic
programming equations. SIAM Journal on Scientific Computing, 37(1):A181-A200, 2015.
Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM (JACM), 12
(4):547-560, 1965.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep
reinforcement learning. arXiv preprint arXiv:1611.01929, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement learning.
arXiv preprint arXiv:1707.06887, 2017.
Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, pp. 679-684,
1957.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and TrendsR in
Machine Learning, 8(3-4):231-357, 2015.
Ronald A Howard. Dynamic programming and markov processes. 1964.
Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal
of artificial intelligence research, 4:237-285, 1996.
Mattia Laurini, Piero Micelli, Luca Consolini, and Marco Locatelli. A jacobi-like acceleration for dynamic
programming. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pp. 7371-7376. IEEE,
2016.
Mattia Laurini, Luca Consolini, and Marco Locatelli. A consensus approach to dynamic programming.
IFAC-PapersOnLine, 50(1):8435-8440, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
James M Ortega and Werner C Rheinboldt. Iterative solution of nonlinear equations in several variables,
volume 30. Siam, 1970.
Martin L Puterman. Markov decision processes. j. Wiley and Sons, 1994.
Martin L Puterman and Shelby L Brumelle. The analytic theory of policy iteration. In Dynamic Programming
and its applications, pp. 91-113. Elsevier, 1978.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37.
University of Cambridge, Department of Engineering, 1994.
9
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.
Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceleration. In Advances
In Neural Information Processing Systems,pp. 712-720, 2016.
Damien Scieur, Francis Bach, and Alexandre d’Aspremont. Nonlinear acceleration of stochastic algorithms.
In Advances in Neural Information Processing Systems, pp. 3982-3991, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press
Cambridge, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in neural information processing
systems, pp. 1057-1063, 2000.
Alex Toth and CT Kelley. Convergence analysis for anderson acceleration. SIAM Journal on Numerical
Analysis, 53(2):805-819, 2015.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In
AAAI, volume 16, pp. 2094-2100, 2016.
Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on Numerical
Analysis, 49(4):1715-1735, 2011.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling
network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Guangzeng Xie, Yitan Wang, Shuchang Zhou, and Zhihua Zhang. Interpolatron: Interpolation or extrapolation
schemes to accelerate optimization for deep neural networks. arXiv preprint arXiv:1805.06753, 2018.
10
Appendix
A.	A2Q
Here is the pseudocode of A2Q, which is A2VI in q-notation.
Algorithm 3 Anderson Accelerated Q-Learning (A2Q)
Input: q0,P, r,γ,k,T
1:	for t = 1, 2, . . . , T do
2:	Bπ(t-1)(qt-1) = vec(r) + γP maxπ qπt-1 - vec(qt-1)
3:	if t < k then
4:	vec(qt) = vec(r) + γP maxπ qπt-1
5:	else
6:	Calculate (α1, α2, . . . , αk) by solving the optimization problem (5)
7:	vec(qt) = vec(r) + γPmaxπ(Pik=1 αiqπt-i)
8:	end if
9:	end for
10:	πT = arg maxπ qπT
11:	return qT , πT
B.	A2VI with the Rejection Step
Here is the pseudocode of A2VI with the rejection step, the only difference between this algorithm with
algorithm 1 is before the interpolation step, we first check whether the affine combination is in VB. If the
answer is negative, then this interpolation step is replaced with an ordinary value iteration step.
Algorithm 4 Anderson Accelerated Value Iteration with the Rejection Step
Input: v(0), P, r, γ, k, T
1: 2: 3: 4: 5: 6: 7: 8: 9:	for t = 1,2,…，T do Bπ(t-1) (v(t-1)) = max (rπ + γPπv(t-1)) - v(t-1) π if t < k then v(t) = max(rπ + γPπv(t-1)) else Calculate (α(t), ɑ2t),…,αf)) by solving the optimization problem (5) V = Pi=I α(t)v(I) if max (r∏ + γP∏ V) ≥ V then π V(t) = max (r∏ + γP∏V) π
10:	else
11: 12:	α1(t) = 1, αi(t) = 0fori 6= 1 V(t) = max rπ + γPπV(t-1)
13:	end if
14:	end if
15:	end for
16:	π(T) = argmaxπ (rπ + γPπ V(T))
17:	return V(T), π(T)
11
C.Proofs
Lemma 1. For any MDP whose optimal policy is unique, there exists a δ > 0 and a policy π* such thatfor
any v ∈ Uδ(v*) = {v∣∣∣v - v*∣∣∞ ≤ δ},
Γ(v) = r∏* + γP∏* V
Proof Because the optimal policy is unique, for any nonoptimal policy π, for any state S such that π(s)=
π*(s) We have that [Γπ* (v*)]s > [Γ∏(v*)]s, [∙]s means executing operations on state s. Denote A(π)=
{s∣π(s) = n*(s)}.
Suppose the optimal policy is π*, then there exists ε such that
min min [Γ(v*) — Γ∏(v*)]§ > ε > 0,
π=π* s∈A(π)
since the optimal policy is unique and the state space and the action space are finite. We choose δ = 3γ-, then
for any policy π and any V ∈ U(v*) we have
.......	....	..	..	...	...	.. ε
∣∣Γ∏(v)- Γ∏(v)∣∣∞ = ∣∣γP∏(v - v)∣∣∞ ≤ YkV - Vh ≤ 3
Then for any policy π, for any state S ∈ A(π) , we have that
[Γ∏*(v)- Γ∏(v)]s = [(Γ∏*(v)- Γ∏*(v*)) + (Γ∏*(v*)- Γ∏(v*)) + (Γ∏(v*)- Γπ(v))]s
≥ε - I∣γ∏*(v) - γ∏*(v*)II∞ - I∣γ∏(v*) - γ∏(v)II∞
≥ ε -
ε
=—
3
ε
3
—
3
which means π does not choose the optimal action in state s. Therefore, if π selects the optimal action
in every state S ∈ S, then we must have πs = π*, ∀s ∈ S, which implies π* ∈ argmaxπ Γ∏(v), i.e.,
Γ(v) = r∏* + γP∏* v.
Lemma 2. For any given MDP with optimal value v* and any value V, we always have
(I-γ)∣∣v - v*k∞ ≤ kr(V)-Vk∞ ≤ (1 + γ)∣∣v - v*k∞
Proof
l∣v - v*k∞ =
≤
≤
⇒(1 - Y)k∞v - v*k∞ ≤
l∣v - v*k∞ =
≥
≥
⇒ (1 + Y)kv - v*k∞ ≥
∣v - Γ(v) + Γ(v) - Γ(v*)k∞
∣v - Γ(v)∣∞ + ∣Γ(v)- Γ(v*)∣∞
∣v - Γ(v)∣∞ + YkV - v*∣∞
kΓ(v)- v∣∞
kv - Γ(v) + Γ(v) - Γ(v*)k∞
kv - Γ(v)k∞ - ∣Γ(v) - Γ(v*)k∞
kv - Γ(v)k∞ - YkV - v*k∞
kΓ(v) - vk∞
12
ɪʌ t> t>	ι ■	ι	<	ι	.ι	∙	i ∙	∙> U-C ∙>	,∙>	, J	i
Proof of Theorem	1 From lemma	1	we know	there	exists a policy	π and a δ > 0	such that the optimal
Bellman operator is a linear function on US(V*). We now set δ sufficiently small such that
km(1 + Y IIv⑼一V* k∞ < km(1 + γ) δ < δ,
1 — Y	1 — Y
The result is trivial for the first k — 1 steps, which are performed exactly by standard value iteration. When
t>k,we prove the result by induction. Suppose the conclusion is correct for previous steps, then we have
k k X QF)V(I)-V*k∞ ≤	k X |Q(t)|||V(I)-V*∣∣∞	k ≤ X IQf)IɪIB(v(I))I∞
i=1	i=1	i=ι	1 - Y
≤	k X |Q(t)|ɪIIB(V(O))Il i=1	1 - Y	∞ ≤ 产IB(v(0))I∞ 1—Y
≤	空Iq …v*∣∣∞	km(1 + y),- < --1	-δ < δ
	1 - Y	1—Y
It follows that
k	k
kv(t) — V*k∞ = IIr(Xα(t)vi) — Γ(v*)k∞ ≤ YIlX寸)”一)—v*∣∣∞
i=1	i=1
k
≤kX α(%(I)- v*k∞ <δ
i=1
Therefore, Pk=I QF)v(t-i) ∈ U^(v*), v(t) ∈ U^(v*), which implies
kk
Γ(X QF)V(I))= % + % χ QT)V(I), r(v(t)) = r∏* + γP∏ V⑴.
i=1	i=1
Then we can get
B(Vt) = r∏* + (γP∏* — I )v(t)
k
=r∏* + (γPπ* — I)(r∏* + γPπ* XQT)V(J))
i=1
k
=XQT) k∏* + (γPπ* — I)(r∏* + γPπ*V(J)))
i=1
k
=X QT)YPπ* (r∏* + γPπ* V(I)-V(I)
i=1
k
= γP∏* X Qf)B(V(I))
i=1
13
Taking norm on both side of the equation and utilizing the definition of α(t), i = 1, 2,…，k,we get
k
kΓ(v(t)) - v(t)k∞ ≤ Y ∣∣P∏* IU X a(t)B(v(I))k∞ ≤ YkB(V(T))k∞ = Ykr(VJ))-V(T) ∣∣∞.
i=1
Therefore, we justify property (i). Property (ii) then follows directly from Lemma 2.
Lemma 3. For any MDP, suppose the values u and V satisfy u ≥ V, then
Γ(u) ≥ Γ(V).
Proof As stated in section 2, U ≥ V means that u(s) ≥ V(S) for any state s. Suppose π = argmax∏ r∏ +
YPπV, therefore for any s we have that
Γ(u)(s) ≥ Γ∏(U)(S) ≥ Γ∏(V)(S) = Γ(v)(s).
Proof of Theorem 2 On the one hand,
k
Γ(V(t)) - V(t) = max(rπ + YPπV(t)) - max(rπ + YPπ X αi(t)V(t-i))
π	πi
i=1
k
≤ rπ(t) + YPπ(t) V(t) - rπ(t) - YPπ(t) X αi(t)V(t-i)
i=1
kk
= Y Pπ(t) (max(rπ + YPπ Xαi(t)V(t-i)) - X αi(t)V(t-i))
i=1	i=1
k
≤ YPπ(t) X αi(t) (max(rπ + YPπV(t-i)) - V(t-i))
π
i=1
k
= YPπ(t) Xαi(t)B(V(t-i))
i=1
On the other hand, We denote π = argmax∏(r∏ + YPn Pk=I α(t)V(t-i)), then
k
V(t) - Γ(V(t)) = max(rπ + YPπ X αi(t)V(t-i)) - max(rπ + YPπV(t))
ππ
i=1
(k)
≤ r∏ + γP∏ X α(t)V(t-i) - r∏ - YPnV(t)
i=1
kk
= YPn(X α(t)V(t-i) - max(r∏ + YPn Xα(t)V(t-i)))
i=1	n	i=1
k
=-YPn B (X a(t)V(I))
i=1
14
The above two results shows
kk
γP∏B(Xα(t)v(t-i)) ≤ B(V㈤)≤ γP∏(t) Xα(t)B(v(I)).
i=1	i=1
Now, consider the rejection step. First we show that if v ∈ VB then Γ(v) ∈ VB. If v ∈ VB, then Γ(v) ≥ v.
With Lemma 3, we have that Γ(Γ(v)) ≥ Γ(v), i.e. Γ(v) ∈ VB.
Next we show that if v(i) ∈ VB for i < t, then v(t) ∈ VB . According to the rejection algorithm, we have
v(t) = Γ(Pik=1αi(t)v(t-i)). IfΓ(Pik=1αi(t)v(t-i)) ≥ Pik=1 αi(t)v(t-i), then Pik=1 αi(t)v(t-i) ∈ VB and
v(t) ∈ VB. If Γ(Pik=1 αi(t)v(t-i)) < Pik=1 αi(t)v(t-i), then v(t) = Γ(v(t-1)) due to the rejection step.
Since v(t-1) ∈ VB, we have that v(t) ∈ VB. Therefore, we also have that B(Pik=1 αi(k)v(t-i)) ≥ 0. We
have that
k
kΓ(v(t)) - vtk = kB(v(t))k ≤γkPπ(t)kkXαi(t)B(v(t-i))k
i=1
≤ γkB(v(t-1))k =γkΓ(v(t-1))-v(t-1)k
The second inequality is due to the definition of αi(t), i = 1, 2, ..., k.
Proof of Theorem 3 We prove the conclusion by induction. It is evident that the conclusion holds for the
first k - 1 steps. Suppose the conclusion holds for the first t - 1 steps, then
k
v (t) = max(rπ + γPπ X αi(t)v(t-i))
π
i=1
k
≥ r∏ + γP∏(Xα(t)v(I))
i=1
≥ r∏ + γP∏V(I)
≥ v(t-1),
where π = argmax∏ r∏ + γP∏v(t-1). The second inequality comes from the extrapolation restriction. The
Third inequality is due to that if V ∈ VB then Γ(V) ∈ VB , which is shown in Theorem 2.
As shown in Theorem 2, we have that v(t) ∈ VB and Pk=I α(t)v(t-i) ∈ VB. Therefore, v(t) ≤ v*.
k
V* - V(t) = V* - max(rπ + γPπ X αi(t)V(t-i))
π
i=1
k
≤ v* - (r∏* + γP∏* X α(t)v(t-i))
i=1
k
= γPπ* X αi(t)(v* -v(t-i))
i=1
≤ γPπ* (v* - v(t-1))
15
Taking the infinite norm on both sides, we get
kv* - v(t)k∞ ≤ YkP∏*k∞kv* - v(t-1)k∞ ≤ Ykv* - v(t-1)k∞,
which completes the proof.
D.	Experiment Details
D.1 Model Architecture and Hyper-parameters
For our experiments, we used the DQN(Mnih et al., 2013) architecture, where the Q-value network is
composed of 3 convolutional layers, 1 fully connected layer, and 1 output fully connected layer. Each layer
except the final layer is followed with a rectified linear activation(ReLU). The first convolutional layer use 32
8 × 8 filters with stride 4, the second has 64 4 × 4 filters with stride 2, and the third convolutional layer has 64
3 × 3 filters with stride 1. The fully connected layer consists of 512 units and the final layer outputs a single
value for each action. We used the Adam optimizer with learning rate 0.0001 and = 0.0015. The discount
was set to Y = 0.99. Training is done over 20M or 40M frames. We updated the target networks every 10000
steps. The size of experience replay buffer is 100000 tuples, where 32 mini batches were sampled every 4
steps to update the network. The exploration policy is ε-greedy policy with fixed ε = 0.01.
D.2 Preprocessing of Environments
We preprocess the environment in the same way as the original DQN paper (Mnih et al., 2013) does. We
utilize the action repeat technique, i.e., each action is repeated for the next four consecutive frames. The
frames are firstly grey-scaled and then rescaled to the size of 84 × 84 pixels. Each state is represented by a
concatenation of4 consecutive frames. We fix all positive rewards to be 1 and all negative rewards to be -1,
leaving 0 rewards unchanged. Transitions associated with the loss of a life are considered terminal.
E.	S OLVING (5) WHEN USING THE `2 NORM AND TOTAL SPACE CONSTRAINT
Under the given setting, we may rewrite the original problem in the following form,
minimize α>(B(t))>B(t)α
subject to 1>α = 1.
This problem can be directly solved with an application of Lagrange multiplier method, namely, let λ be the
Lagrange multiplier, then we solve the problem
maxα>(B(t))>B(t)α + λ(1>α - 1)
α
whose solution can be written explicitly as α = - 2 [(B(t))>B(t)]-11. Combine this result with the constraint,
[	[	[(B(t))> B(t)] — 11
we can get -2 = ι>[(B(t))>B(t)]-ιι, which imPlies α = ι>[(B(t)))>B(t)]-ιι.
16