Under review as a conference paper at ICLR 2019
Representation compression and
generalization in Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Understanding the groundbreaking performance of Deep Neural Networks
(DNNs) is one of the greatest challenges to the scientific community today. In
this work we introduce an information theoretic viewpoint on the behavior of
deep networks optimization processes and their generalization abilities. Specif-
ically, we study DNNs on the Information Plane, the plane of the mutual infor-
mation between each layer with and the input variable, and with the desired label,
during the training dynamics. We show that the training of the network is char-
acterized by a rapid increase in the mutual information (MI) between the layers
and the target label, followed by a longer decrease in the MI between the lay-
ers and the input variable.vFurther, we explicitly show that these two fundamental
information-theoretic quantities govern the generalization error of the network, by
introducing a new generalization-gap bound that is exponential in the input rep-
resentation compression. The analysis focuses on typical patterns of large-scale
problems. For this purpose, we introduce a novel analytic bound on the mutual in-
formation between consecutive layers in the network. An important consequence
of our analysis is a super-linear boost in training time with the number of non-
degenerate hidden layers, demonstrating the computational benefit of the hidden
layers.
1	Introduction
Deep Neural Networks (DNNs) heralded a new era in predictive modeling and machine learning.
Their ability to learn and generalize has set a new bar on performance, compared to state-of-the-art
methods. This improvement is evident across almost every application domain, and especially in
areas that involve complicated dependencies between the input variable and the target label (Le-
Cun et al., 2015). However, despite their great empirical success, there is still no comprehensive
understanding of their optimization process and its relationship to their (remarkable) generalization
abilities.
This work examines DNNs from an information-theoretic viewpoint. For this purpose we utilize
the Information Bottleneck principle (Tishby et al., 1999). The Information Bottleneck (IB) is a
computational framework for extracting the most compact, yet informative, representation of the
input variable (X), with respect to a target label variable (Y ). The IB bound defines the optimal
trade-off between representation complexity and its predictive power. Specifically, it is achieved by
minimizing the mutual information (MI) between the representation and the input, subject to the
level of MI between the representation and the target label.
Recent results (Shwartz-Ziv & Tishby, 2017), demonstrated that the layers of DNNs tend to converge
to the IB optimal bound. The results pointed to a distinction between two phases of the training pro-
cess. The first phase is characterized by an increase in the MI with the label (i.e. fitting the training
data), whereas in the second and most important phase, the training error was slowly reduced with a
decrease in mutual information between the layers and the input (i.e. representation compression).
These two phases appear to correspond to fast convergence to a flat minimum (drift) following a
random walk, or diffusion, in the vicinity of the training error’s flat minimum, as reported in other
studies (e.g. (Zhang et al., 2018a)).
These observations raised several interesting questions: (a) which properties of the SGD optimiza-
tion cause these two training phases? (b) how can the diffusion phase improve generalization perfor-
1
Under review as a conference paper at ICLR 2019
mance? (c) can the representation compression explain the convergence of the layers to the optimal
IB bound? (d) can this diffusion phase explain the benefit of many hidden layers?
In this work we attempt to answer these questions. Specifically, we draw important connections be-
tween recent results inspired by statistical mechanics and information-theoretic principles. We show
that the layers of a DNN indeed follow the behavior described by Shwartz-Ziv & Tishby (2017). We
claim that the reason may be found in the Stochastic Gradient Decent (SGD) optimization mecha-
nism. We show that the first phase of the SGD is characterized by a rapid decrease in the training
error, which corresponds to an increase in the MI with the labels. Then, the SGD behaves like
non-homogeneous Brownian motion in the weights space, in the proximity of a flat error minimum.
This non-homogeneous diffusion corresponds to a decrease in MI between the layers and the input
variable, in “directions” that are irrelevant to the target label.
One of the main challenges in applying information theoretic measures to real-world data is a reason-
able estimation of high dimensional joint distributions. This problem has been extensively studied
over the years (e.g. (Paninski, 2003)), and has led the conclusion that there is no “efficient” solution
when the dimension of the problem is large. Recently, a number of studies have focused on calculat-
ing the MI in DNNs using Statistical Mechanics. These methods have generated promising results
in a variety of special cases (Gabrie et al., 2018), which support many of the observations made by
Shwartz-Ziv & Tishby (2017).
In this work we provide an analytic bound on the MI between consecutive layers, which is valid for
any non-linearity of the units, and directly demonstrates the compression of the representation during
the diffusion phase. Specifically, we derive a Gaussian bound that only depends on the linear part of
the layers. This bound gives a super linear dependence of the convergence time of the layers, which
in turn enables us to prove the super-linear computational benefit of the hidden layers. Further, the
Gaussian bound allows us to study mutual information values in DNNs in real-world data without
estimating them directly.
1.1	Preliminaries and Notations
Let X ∈ X and Y ∈ Y be a pair of random variables of the input patterns and their target label
(respectively). Throughout this work, we consider the practical setting where X and Y are con-
tinuous random variables that are represented in a finite precision machine. This means that both
X and Y are practically binned (quantized) into a finite number of discrete values. Alternatively,
X, Y may be considered as continuous random variables that are measured in the presence of small
independent additive (Gaussian) noise, corresponding to their numerical precision. We use these
two interpretations interchangeably, at the limit of infinite precision, where the limit is applied at the
final stage of our analysis.
We denote the joint probability of X and Y as p(x, y), whereas their corresponding MI is defined
as I(X; Y ) = D [p(y|x)||p(y)] = D [p(x|y)||p(x)] . We use the standard notation D[p||q] for the
Kullback-Liebler (KL) divergence between the probability distributions p and q. Let fWK (x) denote
a DNN, with K hidden layers, where each layer consists of dk neurons, each with some activation
function σk (x), for k = 1, . . . , K. We denote the values of the kth layer by the ranom vector
Tk. The DNN mapping between two consecutive layers is defined as Tk = σk (WkTk-1), where
Wk is a dk × dk-1 real weight matrix. Note that we consider both the weights, Wk and the layer
representations, Tk, as stochastic entities, because they depend on the stochastic training rule of the
network and the random input pattern (as described in Section 2.1). However, when the network
weights are given, the weights are fixed realizations of the random training process (i.e. they are
”quenched”). Note that given the weights, the layers form a Markov chain of successive internal
representations of the input variable X: Y → X → T1 → ... → TK, and their MI values obey a
chain of Data Processing Inequalities (DPI), as discussed by Shwartz-Ziv & Tishby (2017).
We denote the set of all K layers weight matrices as WK = {W1, . . . , WK}. Let the training
sample, Sn = {(x1, y1) , . . . , (xn, yn)} be a collection of n independent samples from p(x, y). Let
'wk (xi, yi) be a (differentiable) loss function that measures the discrepancy between a prediction
of the network fWK (xi) and the corresponding true target value yi, for a given set of weights WK.
Then, the empirical error is defined as LWK (Sn) = 1 PZ1 'wK (xi,yi) . The corresponding
error gradients (with respect to the weights) are denoted as VwKLWK (Sn)
2
Under review as a conference paper at ICLR 2019
2	Deep Neural Networks
2.1	TRAINING THE NETWORK - THE SGD ALGORITHM
Training a DNN corresponds to the process of setting the values of weights WK from a given set
of samples Sn . This is typically done by minimizing the empirical error, which approximates the
expected loss. The SGD algorithm is a common optimization method for this purpose (Robbins &
Monro, 1951).
Let S(m) be a random set of m samples drawn (uniformly, with replacement) from Sn , where m <
n. We refer to S(m) as a mini-batch of Sn . Define the corresponding empirical error and gradient
of the mini-batch as LWK (S(m))=* P{xi,yi}∈s(m) 'wK (xi,yi) and VwKLWK(S(m))=
mm P{χ. y. }∈s(m) VW K 'w k (xi, yi) respectively. Then, the SGD algorithm is defined by the update
rule: WK(l) = WK(l - 1) 一 ηVwK(i-1)LwK(i-1)(S(m)), where WK(l) are the weights after l
iterations of the SGD algorithm and η ∈ R+ is the learning rate.
2.2	The Different Phases of SGD Optimization
The SGD algorithm plays a key role in the astonishing performance of DNNs. As a result, it has
been extensively studied in recent years, especially in the context of flexibility and generalization
(Chee & Toulis, 2017). Here, we examine the SGD as a stochastic process, that can be decomposed
into two separate phases. This idea has been studied in several works (Murata, 1998; Jin et al., 2017;
Hardt et al., 2015). Murata argued that stochastic iterative procedures are initiated at some starting
state and then move through a fast transient phase towards a stationary phase, where the distribution
of the weights becomes time-independent. However, this may not be the case when the SGD induces
non-isotropic state dependent noise, as argued, for example, by Chaudhari & Soatto (2017).
In contrast, Shwartz-Ziv & Tishby (2017) described the transient phase of the SGD as having two
very distinct dynamic phases. The first is a drift phase, where the means of the error gradients in
every layer are large compared to their batch-to-batch fluctuations. This behaviour is indicative of
small variations in the gradient directions, or high-SNR gradients. In the second part of the transient
phase, which they refer to as diffusion, the gradient means become significantly smaller than their
batch-to-batch fluctuations, or low-SNR gradients. The transition between the two phases occurs
when the training error saturates and weights growth is dominated by the gradient batch-to-batch
fluctuations. Typically, most SGD updates are expended in the diffusion phase before reaching
Murata’s stationary phase. In this work we rigorously argue that this diffusion phase causes the
representation compression; the observed reduction in I(Tk; X), for most hidden layers.
2.3	Drift and Diffusion with SGD
It is well known that the discrete time SGD (2.1) can be considered as an approximation ofa contin-
uous time stochastic gradient flow if the discrete-time iteration parameter l is replaced by a contin-
uous parameter τ. Li et al. (2015) recently showed that when the mini-batch gradients are unbiased
with bounded variance, the discrete-time SGD is an approximation of a continuous-time Langevin
dynamics,
dWK(τ) = -VWK(T)LwK(T) (Sn) dτ +，2eTC(WK(T))dB (τ)	(1)
where C WK (τ) is the sample covariance matrix of the weights, B(τ) is a standard Brownian
motion (Wiener process) and β is the Langevin temperature constant. The first term in (1) is called
the gradient flow or drift component, and the second term corresponds to random diffusion. Al-
though, this stochastic dynamics hold for the entire SGD training process, the first term dominates
the process during the high SNR gradient phase, while the second term becomes dominant when the
gradients are small, due to saturation of the training error in the low SNR gradient phase. Hence,
these two SGD phases are referred to as drift and diffusion.
The mean L2 displacement (MSD) measures the Euclidean distance from a reference position over
time, which is used to characterize a diffusion process. Normal diffusion processes are known to
exhibit a power-law MSD in time, E [∣∣ WK(τ) 一 WK(0) |图 〜γtα, where t is the diffusion time,
3
Under review as a conference paper at ICLR 2019
γ is related to the diffusion coefficient, and 0 < α ≤ 0.5 is the diffusion exponent. For a standard
flat space diffusion, the MSD increases as a square root of time (α = 0.5). Hu et al. (2017) showed
(empirically) that the weights’ MSD, in a DNNs trained with SGD, indeed behaves (asymptotically)
like a normal diffusion, where the diffusion coefficient γ depends on the batch size and learning
rate. In contrast, Hoffer et al. (2017) showed that the weights’ MSD demonstrates a much slower
logarithmic increase. This type of dynamics is also called “ultra-slow” diffusion.
3	Information Plane analysis
Following Tishby & Zaslavsky (2015) and Shwartz-Ziv & Tishby (2017), we study the layer repre-
sentation dynamics in the two-dimensional (I(X; Tk), I(Tk; Y )) plane. Specifically, for any input
and target variables, X, Y , let T , T (X) denote a representation, or an encoding (not necessarily
deterministic), of X . Clearly, T is fully characterized by its encoder, the conditional distribution
p(t|x). Similarly, let p(y|t) denote any (possibly stochastic) decoder of Y from T. Given a joint
probability function p(x, y), the Information Plane is defined the set of all possible pairs I(X; T)
and I(T; Y ) for any possible representation, p(T |X).
It is evident that not all points on the plane are feasible (achievable), as there is clearly a tradeoff
between these quantities; the more we compress X (reduce I(X; T )), the less information can be
maintained about the target, I(T; Y ).
Our analysis is based on the fundamental role of these two MI quantities. We argue that for large
scale (high dimensional X) learning, for almost all (typical) input patterns, with mild assumptions
(ergodic Markovian input patterns): (i) the MI values concentrate with the input dimension; (ii)
the minimal sample complexity for a given generalization gap is controlled by I(X; T); and (iii)
the accuracy - the generalization error - is governed by I(T; Y ), with the Bayes optimal decoder
representation.
Here, we argue that the sample-size - accuracy trade-off, of all large scale representation learning,
is characterized by these two MI quantities. For DNNs, this amounts to a dramatic reduction in
the complexity of the analysis of the problem. We discuss these ideas in the following sections
and prove the connection between the input representation compression I(T ; X), the generalization
gap (the difference between training and generalization errors), and the minimal sample complexity
(Theorem 1 below).
3.1	Label Information and Generalization Error
Optimizing mutual information quantities is by no means new in either supervised and unsupervised
learning (Deco & Obradovic, 2012; Linsker, 1988; Painsky et al., 2016). This is not surprising, as
it can be shown that I(T; Y ) corresponds to the irreducible error when minimizing the logarithmic
loss (Painsky & Wornell, 2018b; Harremoes & Tishby, 2007). Here, we emphasize that I(T; Y ), for
the optimal decoder of the representation T, governs all reasonable generalization errors (under the
mild assumption that label y is not completely deterministic; p(y|x) is in the interior of the simplex,
∆(Y ), for all typical x ∈ X). First, note that with the Markov chain Y - X - T, I(T; Y ) =
I(X; Y ) - EX,T D [p(y|x)||p(y|t)]. By using the Pinsker inequality (Cover & Thomas, 2012) the
variation distance between the optimal and the representation decoders can be bound by their KL
divergence,
12
D [p⑶X)MyIt)] ≥ 2jn2Ip⑶X)-p(yIt)11.
(2)
Hence, by maximizing I(T; Y ) we minimize the expected variation risk between the representation
decoder p(yIt) and p(yIX). For more similar bounds on the error measures see (Painsky & Wornell,
2018a).
3.2	Representation Compression and Sample Complexity
The Minimum Description Length (MDL) principle (Rissanen, 1978) suggests that the best repre-
sentation for a given set of data is the one that leads to the minimal code-length needed to represent
of the data. This idea has inspired the use of I(X; T) as a regularization term in many learning
4
Under review as a conference paper at ICLR 2019
problems (e.g. Chigirev & Bialek (2004); Painsky et al. (2018)). Here, we argue that I(X; T) plays
a much more fundamental role; we show that for large scale (high dimensional X) learning and for
typical input patterns, I (X; T ) controls the sample complexity of the problem, given a generaliza-
tion error gap.
Theorem 1 (Input Compression bound). Let X be a d-dimensional random variable that obeys an
ergodic Markov random field probability distribution, asymptotically in d. Let T , T (X) be a
representation of X and denote by Tm = {(t1, y1), . . . , (tm, ym)} an m-sample vector of T and
Y, generated with m independent samples of xi, with p(y|xi) and p(t|xi). Assume that p(x, y) is
bounded away from 0 and 1 (strictly inside the simplex interior). Then, for large enough d, with
probability 1 - δ, the typical expected squared generalization gap satisfies
2	2i(x；T)+ log 2
IL(Tm)- ETm [L (Tm)] I2 ≤ ——2m	δ .	(3)
where the typicality follows the standard Asympthotic Equipartition Property (AEP) (Cover &
Thomas, 2012).
A proof of this Theorem is given in Appendix A. This Theorem is also related to the bound proved
by Shamir et al. (2010), with the typical representation cardinality, |T(X)| ≈ 2I(T;X). The ergodic
Markovian assumption is common in many large scale learning problems. It means that p(x) ≈
Qi=1:dp(xi|Pa(xi)), where Pa(xi) is a finite set of adjacent ”parents” of xi in the d dimensional
pattern X .
The consequences of this input-compression bound are quite striking: the generalization error de-
creases exponentially with I(X; T), once I(T; X) becomes smaller than log 2m - the query sample-
complexity. Moreover, it means that M bits of representation compression, beyond log 2m, are
equivalent to a factor of 2M training examples. The tightest bound on the generalization bound is
obtained for the most compressed representation, or the last hidden layer of the DNN. The input-
compression bound can yield a tighter and more realistic sample complexity than any of the worst-
case PAC bounds with any reasonable estimate of the DNN class dimensionality, as typically the
final hidden layers are compressed to a few bits.
Nevertheless, two important caveats are in order. First, the layer representation in Deep Learning
are learned from the training data; hence, the encoder, the partition of the typical patterns X , and the
effective ”hypothesis class”, depend on the training data. This can lead to considerable over-fitting.
Training with SGD avoids this potential over-fitting because of the way the diffusion phase works.
Second, for low I(T; Y ) there are exponentially (in d) many random encoders (or soft partitions of
X) with the same value of I(T; X). This seems to suggest that there is a missing exponential factor
in our estimate of the hypothesis class cardinality. However, note that the vast majority (almost all)
of these possible encoders are never encountered during typical SGD optimization. In other words,
they act like a ”dark hypothesis space” which is never observed and does not affect the generalization
bound. Moreover, as I(T; Y ) increases, the number of such random encoders rapidly collapses all
the way to O(1) when I(T; Y ) approaches the optimal IB limit, as we show next.
3.3	The Information Bottleneck limit
As presented above, we are interested in the boundary of the achievable region in the information
plane, or in encoder-decoder pairs that minimize the sample complexity (minimize I(X; T)) and
generalize well (maximize I(T; Y )).
These optimal encoder-decoder pairs are given precisely by the Information Bottleneck frame-
work (Tishby et al., 1999), which is formulated by the following optimization problem:
minp(t|x) I (X; T) - βI (T; Y ) , over all possible encoders-decoders pairs that satisfy the Markov
condition Y - X - T. Here β is a positive Lagrange multiplier associated with the decoder infor-
mation on I(T; Y ), which also determines the complexity of the representation.
The Information Bottleneck limit defines the set of optimal encoder-decoder pairs, for the joint
distribution p(x, y). Furthermore, it characterizes the achievable region in the Information Plane,
similar to Shannon’s Rate Distortion Theory (RDT) (Cover & Thomas, 2012). By our previous
analysis it also determines the optimal tradeoff between sample complexity and generalization error.
The IB can only be solved analytically in very special cases (e.g., jointly Gaussian X, Y (Chechik
5
Under review as a conference paper at ICLR 2019
et al., 2005)). In general, a (locally optimal) solution can be found by iterating the self-consistent
equations, similar to the Arimoto- Blahut algorithm in RDT (Tishby et al., 1999). For general distri-
butions, no efficient algorithm for solving the IB is known, though there are several approximation
schemes (Chalk et al., 2016; Painsky & Tishby, 2017). The self-consistent equations are exactly
satisfied along the IB limit, aka the Information Curve.
4	The Information Plane and SGD dynamics for DNNs
By applying the DPI to the Markov chain of the DNN layers we obtain the following chains:
I(X； Ti) ≥ I(X； T2) ≥ …≥ I(X; Tk) ≥ I(X; Y) and I(X； Y) ≥ I(T1； Y) ≥ …≥ I(Tk； Y) ≥
I (Y; Y) where Y is the output of the network. The pairs (I (X; Tk), I (Tk, Y)), for each SGD UP-
date, form a unique concentrated Information Path for each layer of a DNN, as demonstrated by
Shwartz-Ziv & Tishby (2017).
For any fixed realization of the weights, the network is, in principle, a deterministic map. This does
not imply that information is not lost between the layers; the inherent finite precision of the layers,
with possible saturation of the nonlinear activation functions σk, can result in non-invariable map-
ping between the layers. Moreover, we argue below that for large networks this mapping becomes
effectively stochastic due to the diffusion phase of the SGD.
On the other hand, the Information Plane layer paths are invariant to invertible transformations of
the representations Tk. Thus the same paths are shared by very different weights and architectures,
and possibly different encoder-decoder pairs. This freedom is drastically reduced when the target
information, I(Tk , Y), increases and the layers approach the IB limit. Minimizing the training error
(ERM), together with standard uniform convergence arguments clearly increase I(T; Y), but what
in the SGD dynamics can lead to the observed representation compression which further improves
generalization? Moreover, can the SGD dynamics push the layer representations all the way to the
IB limit, as claimed in Shwartz-Ziv & Tishby (2017)?
We provide affirmative answers to both questions, using the properties of the drift and diffusion
phases of the SGD dynamics.
4.1	Representation Compression by Diffusion
In this section we quantify the roles of the drift and diffusion SGD phases and their influence on
the MI between consecutive layers. Specifically, we show that the drift phase corresponds to an
increase in information with the target label I(Tk; Y), whereas the diffusion phase corresponds
to representation compression, or reduction of the I(X; Tk). The representation compression is
accompanied by further improvement in the generalization.
The general idea is as follows: the drift phase increases I(Tk; Y) as it reduces the cross-entropy
empirical error. On the other hand, the diffusion phase in high dimensional weight space effectively
adds an independent non-uniform random component to the weights, mostly in the directions that
do not influence the loss - i.e, irrelevant directions. This results in a reduction of the SNR of
the irrelevant features of the patterns, which leads to a reduction in I(X; Tk), or representation
compression. We further argue that different layers filter out different irrelevant features, resulting
in their convergence to different locations on the Information Plane.
4.2	The SGD compression mechanism
First, we notice that the DPI implies that I(X; Tk) ≤ I(Tk-1; Tk). We focus on the second term
during the diffusion phase and prove an asymptotic upper bound for I(Tk-1; Tk), which reduces
sub-linearly with the number of SGD updates. For clarity, we describe the case where Tk ∈ Rdk is a
vector and Tk+1 ∈ R is a scalar. The generalization to higher dk+1 is straightforward. We examine
the network during the diffusion phase, after τ iterations of the SGD beyond the drift-diffusion
transition. For each layer, k, the weights matrix, Wk(τ) can be decomposed as follows,
W k(τ) = W k? + δW k(τ).	(4)
The first term, Wk?, denotes the weights at the end of the drift phase (τ0 = 0) and remains constant
with increasing τ . As we assume that the weights converge to a (local, flat) optimum during the
6
Under review as a conference paper at ICLR 2019
drift phase, Wk is close to the weights at this local optimum. The second term, δWk(τ), is the
accumulated Brownian motion in τ steps due to the batch-to-batch fluctuations of the gradients
near the optimum. For large T We know that δWk(T)〜N(0, TC(Wk(τ0))) where τo is the time
that the diffusion phase began. Note that for any given τ, we can treat the weights as a fixed
(quenched) realization, wk (T), of the random Brownian process Wk (T). We can now model the
mapping between the layers Tk and Tk+1 at that time as
Tk+1 = σk (w*TTk + δwk(T)TTk + Z)	(5)
where w* ∈ Rdk is the SGD's empirical minimizer, and δw ∈ Rdk is a realization from a Gaussian
vector δw 〜N(0, Cδw), of the Brownian process discussed in Section 2.3. In addition, we consider
Z 〜N(0, σ2) to be the small Gaussian measurement noise, or quantization, independent of δwk
and Tk . This standard additive noise allows us to treat all the random variables as continuous.
For simplicity we first assume that the dk components ofTk have zero mean and are asymptotically
independent for dk → ∞.
Proposition 2. Under mild technical conditions which are met with probability 1 in standard deep
learning (see Appendix B), we have that
δwτ Tk
|Bw||2
T
---D-→N(0,I)
dk →∞
(6)
almost surely, where σT2 is the variance of the components of Tk.
A proof for this CLT proposition is given in Appendix B.
Proposition 2 shows that under standard conditions, w*T Tk and δwTTk are asymptotically jointly
Gaussian and independent, almost surely. We stress that the components of Tk do not have to be
identically distributed to satisfy this property; Proposition 2 may be adjusted for this case with
different normalization factors. Similarly, the independence assumption on Tk can easily be relaxed
to Markovian ergodic, as we assume the input patterns. Finally, it is easy to verify that Proposition 2
can be extended to the general case where w*, δw ∈ Rdk ×dk+1, under similar technical conditions.
We can now bound the mutual information between Tk+1 and the linear projection of the previous
layer W*Tk, during the diffusion phase, for sufficiently high dimensions dk, dk+1, under the con-
ditions above. Note that in this case, (5) behaves like an additive Gaussian channel where w*T Tk
is the signal and δwT Tk + Z is an independent additive Gaussian noise (i.e., independent of signal
and normally distributed). Hence, for sufficiently large dk and dk+1, we have
I(Tk+1; Tk|w*) ≤I(Tk+1; w*T Tk|w*) ≤ I w*T Tk + δwT Tk + Z; w*T Tk|w*) =	(7)
σT2 w*T w* + σT2 δwT δw + σz2I
σT2 δwT δw + σ2I
Tk	z
almost surely, where the first inequality is due to DPI for the Markov chain Tk - w*TTk - Tk+1.
Finally, we apply an orthogonal eigenvalue decomposition to this multivariate Gaussian channel (7).
Let δwT δw = QΛQT where QQT = I and Λ is a diagonal matrix whose diagonal elements are the
corresponding eigenvalues, λi , of δwT δw. Then, we have that
2
∣σTkw*Tw* + σTkδwτδw + ZI =σTk|Q| ∙ |QTw*Tw*Q + Λ + 仔QTQ| ∙ |QT| =	(8)
σTk
2	dk+1	2
σTk |QTw*Tw*Q + λ+------2z-I|	≤	σTk	Y I	Aii	+	λi	+--2z~	)
σTk	i=1	σTk
2log
7
Under review as a conference paper at ICLR 2019
where A，QTW*TW*Q. The last inequality is due to the Hadamard inequality. Plugging (8) into
(7) yields that for sufficiently large dk and dk+1,
I(Tk+ι; Tk |w*) ≤ 1log
(9)
1
2
dk+1
X log
i=1
_→ 1 X log (1 +
σZ→0 2 i=1	V
As previously established, δw is a Brownian motion along the SGD iterations during the diffusion
phase. This process is characterized by a low (and fixed) variance of the informative gradients (rel-
evant dimensions), whereas the remaining irrelevant directions suffer from increasing variances as
the diffusion proceeds (see, for example, (Sagun et al., 2017; Zhu et al., 2018; Jastrzebski et al.,
2017)). In other words, we expect the “informative” λi to remain fixed, while the irrelevant consis-
tently grow, sub-linearly with time. Denote the set of "informative/reIevant" directions as Λ* and
the set of ”non-informative” as ΛNI. Then our final limit (9), as the number of SGD steps grow, is
I(Tk+ι; Tk∣w*) ≤ 2 Pλ*∈Λ* log (1 + Aλi) . Note that the directions that are compressed and the
ones that are preserved depend on the required compression level. This is the reason that different
layers converge to different values of I(Tk; X).
4.3	Relation to other works
The analysis above suggests that the SGD compresses during the diffusion phase in many directions
of the gradients. We argue that these directions are the ones in which the variance of the gradients
is increasing (non-informative) whereas the information is preserved in the directions where the
variance of the gradients remain small.
This statement is consistent with recent (independent) work on the statistical properties of gradi-
ents and generalization. Sagun et al. (2017); Zhu et al. (2018); Zhang et al. (2018b) showed that
typically, the covariance matrix of the gradients is highly non-isotropic and that this is crucial for
generalization by SGD. They suggested that the reason lies in the proximity of the gradients’ covari-
ance matrix to the Hessian of the loss approximation. Furthermore, it was argued by Zhang et al.
(2018b); Keskar et al. (2016); Jastrzebski et al. (2017) that SGD tends to converge to flat minima.
These flat minima often correspond to a better generalization. Zhang et al. (2018b) emphasized that
SGD converges to flat minima values characterized by high entropy due to the non-isotropic nature
of the gradients’ covariance and its alignment with the error Hessian at the minima. In other words,
all of the finding above suggest that good generalization performance is typically characterized by
non-isotropic gradients and Hessian, that are in orthogonal directions to the flat minimum of the
training error objective.
5	The computational benefit of the hidden layers
Our Gaussian bound on the representation compression (9) allows us to relate the convergence time
of the layer representation information, I(Tk; X), to the diffusion exponent α, defined in section
2.3.
Denote the representation information at the diffusion time τ as I(X; Tk)(τ). It follows from (9)
that
I(X ； Tk)(T) ≤ C +2 X log(1 +
2 λi∈ΛNI
≤ C+2 λX (篇)
(10)
where C depends on the informative information for this layer, but not on τ .
Notice thatλi(τ) are the singular values of the weights of a diffusion process, which grow as τα
where α is the diffusion exponent. Hence,	λi(τ)	=	λ0	∙	Tα.	Therefore,	I(X;Tk)(τ)	≤ C +
Ta ∑λi∈ΛNI
8
Under review as a conference paper at ICLR 2019
(a) The change of weights, the SNR of the gradi-
ents, the MI and the Gaussian bound during the
training for one layer. In log-log scale
Figure 1: MNIST data-set
(b) The transition point of the SNR (Y -axis) ver-
sus the beginning of the information compression
(X-axis), for different mini-batch sizes
Inverting this relation, the time to compress the representation Tk by ∆I(X; Tk) = ∆Ik scales
as: T(∆Ik) Z QI(X；T))。，where R = 2 Pλ.∈λNI (4⅛i} Note that R depends solely on the
problem, f (x) or p(y, x), and not on the architecture. The idea behind this argument is as follows
- one can expand the function in any orthogonal basis (e.g. Fourier transform). The expansion
coefficients determine both the dimensionality of the relevant/informative dimensions and the total
trace of the irrelevant directions. Since these traces are invariant to the specific function basis, these
traces remain the same when expanding the function in the network functions using the weights.
Now, with K hidden layers, where each layer only needs to compress from the previous (com-
pressed) layer, by ∆Ik and the total compression is ∆IX = k ∆Ik . Under these assumptions,
even if the layers compress one after the other, the total compression time breaks down into K
SmWaSat ( ɪ) 1	W	P	( ɪ )	1 ifthe δ λ	a re similar	we	obtain	a super linear
Smaller steps , as at P^ ∆I^ )	^W	) Jk	I ∆I^ ) if the ∆ik	are similar,	We	Obtain	a super-linear
boost in the computational time by a factor K 1. Since α ≤ 0.5 this is at least a quadratic boost in
K. For ultra-slow diffusion we obtain an exponential boost (in K) in the convergence time to a good
generalization. This is consistent with the observations reported by Shwartz-Ziv & Tishby (2017).
6	Experiments
We now illustrate our results in a series of experiments. We examine several different setups.
MNIST dataset- In the first experiment, we evaluate the MNIST handwritten digit recognition task
(LeCun et al., 1990). For this data set, we use a fully-connected network with 5 hidden layers of
width 500 - 250 - 100 - 50 - 20, with an hyperbolic tangent (tanh) activation function. The relative
low dimension of the network and the bounded activation function allow us to empirically measure
the MI in the network. The MI is estimated by binning the neurons’ output into the interval [-1, 1].
The discretized values are then used to estimate the joint distributions and the corresponding MI, as
described by Shwartz-Ziv & Tishby (2017).
Figure 1a depicts the norms of the weights, the signal-to-noise ratio (the ratio between the means of
the gradients and their standard deviations), the compression rate I(X ; T) and the Gaussian upper
bound on I(X; T), as defined in (9). As expected, the two distinct phases correspond to the drift
and diffusion phases. Further, these two phases are evident by independently observing the SNR, the
change of the weights ||W (l) - W (0)||, the MI and the upper bound. In the first phase, the weights
grow almost linearly with the iterations, the SNR of the gradients is high, and there is almost no
change in the MI. Then, after the transition point (that accrues almost at the same iteration for all
9
Under review as a conference paper at ICLR 2019
lθ-ɪ
IO-2
0.7
IO1 IO2 IO3 IO4 IO5
Iterations
(a) CIFAR-10
o.ι
IO1 IO2 IO3 IO4 IO5
Iterations
(b) CIFAR-100
Figure 2: Change in the SNR of the gradients and the Gaussian bound on the MI during the training
of the network for one layer on ResNet-32, in log-log scale.
the measures above), the weights behave as a diffusion process, and the SNR and the MI decrease
remarkably. In this phase, there is also a clear-cut reduction of the bound.
CIFAR-10 and CIFAR-100 - Next, we validate our theory on large-scale modern networks. In
the second experiment we consider two large-scale data sets, CIFAR-10 and CIFAR-100. Here, we
train a ResNet-32 network, using a standard architecture (including ReLU activation functions as
described in (He et al., 2016). In this experiment we do not estimate the MI directly, due to the
large scale of the problem. Figure 2 shows the SNR of the gradients and the Gaussian bound for
one layer in CIFAR-10 and CIFAR-100 on the ResNet-32 network, averaged over 50 runs. Here,
we observed similar behavior as reported in the MNIST experiment. Specifically, there is a clear
distinction between the two phases and a reduction of the MI bound along the diffusion phase. Note
that the same behavior was observed in most of the 32 layers in the network.
Recently there have been several attempts to characterize the correspondence between the diffusion
rate of the SGD and the size of the mini-batch (Hu et al. (2017); Hoffer et al. (2017)). In these
articles, the authors claimed that a larger mini-batch size corresponds to a lower diffusion rate.
Here, we examined the effect of the mini-batch size on the transition phase in the Information Plane.
For each mini-batch size, we found both the starting point of the information compression and the
gradient phase transition (the iteration where the derivative of the SNR is maximal). Figure 1b
illustrates the results. The X-axis is the iteration where the compression started, and the Y -axis is
the iteration where the phase transition in the gradients accrued for different mini-batch sizes. There
is a clear linear trend between the two. This further justifies our suggested model, since that the two
measures are strongly related.
Next, we validate our results on the computational benefit of the layers. We train networks with a
different number of layers (1-5 layers) and examine the iteration for which the network converge.
Then, we find the ɑ which fits the best trend K 1 where K is the number of layers. Figure 3 shows
the results for two data-sets - MNIST and the symmetric dataset from Shwartz-Ziv & Tishby (2017).
As our theory suggest, as we increase the number of layers, the convergence time decreases with a
factor of k 1 for different values of α.
7	Discussion and Conclusions
In this work we study DNNs using information-theoretic principles. We describe the training pro-
cess of the network as two separate phases, as has been previously done by others. In the first
phase (drift) we show that I(Tk; Y ) increases, corresponding to improved generalization with ERM.
In the second phase (diffusion), the representation information, I (X; Tk) slowly decreases, while
I(TK; Y ) continues to increase. We rigorously prove that the representation compression is a di-
rect consequence of the diffusion phase, independent of the non-linearity of the activation function.
10
Under review as a conference paper at ICLR 2019
Figure 3: The computational benefit of the layers - The converged iteration as function of the number
of layers in the network
We provide a new Gaussian bound on the representation compression and then relate the diffusion
exponent to the compression time. One key outcome of this analysis is a novel proof of the compu-
tational benefit of the hidden layers, where we show that they boost the overall convergence time of
the network by at least a factor of K2, where K is the number of non-degenerate hidden layers. This
boost can be exponential in the number of hidden layers if the diffusion is ”ultra slow”, as recently
reported.
References
Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational informa-
tion bottleneck. In Advances in Neural Information Processing Systems, pp. 1957-1965, 2016.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. CoRR, abs/1710.11029, 2017. URL http://
arxiv.org/abs/1710.11029.
Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for gaussian
variables. Journal of machine learning research, 6(Jan):165-188, 2005.
Jerry Chee and Panos Toulis. Convergence diagnostics for stochastic gradient descent with constant
step size. arXiv preprint arXiv:1710.06382, 2017.
Denis V Chigirev and William Bialek. Optimal manifold representation of data: an information
theoretic approach. In Advances in Neural Information Processing Systems, pp. 161-168, 2004.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Gustavo Deco and Dragan Obradovic. An information-theoretic approach to neural computing.
Springer Science & Business Media, 2012.
Marylou Gabrie, Andre Manoel, Clement Luneau, Jean Barbier, NiColaS Macris, Florent Krzakala,
and Lenka Zdeborova. Entropy and mutual information in models of deep neural networks. arXiv
preprint arXiv:1805.09785, 2018.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Peter Harremoes and Naftali Tishby. The information bottleneck revisited or how to choose a good
distortion measure. In Information Theory, 2007. ISIT 2007. IEEE International Symposium on,
pp. 566-570. IEEE, 2007.
11
Under review as a conference paper at ICLR 2019
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American statistical association, 58(301):13-30, 1963.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of noncon-
vex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.
Stanisaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape
saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E
Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation net-
work. In Advances in neural information processing systems, pp. 396-404, 1990.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 2015.
Qianxiao Li, Cheng Tai, et al. Stochastic modified equations and adaptive stochastic gradient algo-
rithms. arXiv preprint arXiv:1511.06251, 2015.
Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
Noboru Murata. A statistical study of on-line learning. Online Learning and Neural Networks.
Cambridge University Press, Cambridge, UK, pp. 63-92, 1998.
Amichai Painsky and Naftali Tishby. Gaussian lower bound for the information bottleneck limit.
The Journal of Machine Learning Research, 18(1):7908-7936, 2017.
Amichai Painsky and Gregory W Wornell. Bregman divergence bounds and the universality of the
logarithmic loss. arXiv preprint arXiv:1810.07014, 2018a.
Amichai Painsky and Gregory W Wornell. On the universality of the logistic loss function. arXiv
preprint arXiv:1805.03804, 2018b.
Amichai Painsky, Saharon Rosset, and Meir Feder. Generalized independent component analysis
over finite alphabets. IEEE Transactions on Information Theory, 62(2):1038-1053, 2016.
Amichai Painsky, Meir Feder, and Naftali Tishby. An information-theoretic framework for non-
linear canonical correlation analysis. arXiv preprint arXiv:1810.13259, 2018.
Liam Paninski. Estimation of entropy and mutual information. Neural Comput., 15(6):1191-1253,
2003. ISSN 0899-7667. doi: 10.1162/089976603321780272.
Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465-471, 1978.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
12
Under review as a conference paper at ICLR 2019
Norbert Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series A, 13(1):
145-147,1972.
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the informa-
tion bottleneck. Theoretical Computer Science, 411(29):2696 - 2711, 2010. ISSN 0304-3975.
doi: https://doi.org/10.1016/j.tcs.2010.04.006. URL http://www.sciencedirect.com/
science/article/pii/S030439751000201X. Algorithmic Learning Theory (ALT
2008).
Saharon Shelah. A combinatorial problem; stability and order for models and theories in infinitary
languages. Pacific Journal of Mathematics, 41(1):247-261, 1972.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Comput-
ing, 1999.
Vladimir N Vapnik and Aleksei Yakovlevich Chervonenkis. The uniform convergence of frequencies
of the appearance of events to their probabilities. In Doklady Akademii Nauk, volume 181, pp.
781-783. Russian Academy of Sciences, 1968.
Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Brando Miranda, Noah Golowich, and Tomaso A.
Poggio. Theory of deep learning iib: Optimization properties of SGD. CoRR, abs/1801.02254,
2018a. URL http://arxiv.org/abs/1801.02254.
Yao Zhang, Andrew M Saxe, Madhu S Advani, and Alpha A Lee. Energy-entropy competition and
the effectiveness of stochastic gradient descent in machine learning. Molecular Physics, pp. 1-10,
2018b.
Z. Zhu, J. Wu, B. Yu, L. Wu, and J. Ma. The Anisotropic Noise in Stochastic Gradient Descent: Its
Behavior of Escaping from Minima and Regularization Effects. ArXiv e-prints, 2018.
Appendix A	- proof of Theorem 1
We first first revisit the well-known Probably Approximately Correct (PAC) bound. Let H be a fi-
nite set of hypotheses. Let `h (xi, yi) be a bounded loss function, for every h ∈ H. For example,
`h (xi, yi) = (yi - h(xi))2 is the squared loss while `h (xi, yi) = -yi log h(xi) is the logarith-
mic loss (which may be treated as bounded, assuming that the underlying distribution is bounded
away from zero and one). Let Lh (Sm)= 煮 Pm=I 'h (xi, yi) be the empirical error. Hoeffding's
inequality Hoeffding (1963) shows that for every h ∈ H,
P [∣Lh (Sm)- ESm [Lh (Sm)]∣ ≥ e] ≤ 2exp (-2e2m).	(11)
Then, we can apply the union bound and conclude that
P ∃h ∈H∣Lh (Sm) - ESm [Lh (Sm)] ∣ ≥ e ≤ 2∣H∣ exp (-2e2m).
We want to control the above probability with a confidence level of δ. Therefore, we ask that
2∣H∣ exp -2e2m ≤ δ. This leads to a PAC bound, which states that for a fixed m and for every
h ∈ H, we have with probability 1 - δ that
∣Lh (Sn) - ESm [Lh (Sm)] f ≤ 叱:2 ∙	(12)
Note that under the definitions stated in Section 1.1, we have that |H| ≤ 2X. However, the PAC
bound above also holds for a infinite hypotheses class, where log |H| is replaced with the VC di-
mension of the problem, with several additional constants (Vapnik & Chervonenkis, 1968; Shelah,
1972; Sauer, 1972).
13
Under review as a conference paper at ICLR 2019
Let us now assume that X is a d-dimensional random vector which follows a Markov random field
structure. As stated above, this means that p(xi) = Qip(xi|Pa(xi)) where Pa(Xi) is a set of
components in the vector X that are adjacent to Xi . Assuming that the Markov random field is
ergodic, we can define a typical set of realizations from X as a set that satisfies the Asymptotic
Equipartition Property (AEP) (Cover & Thomas, 2012). Therefore, for every > 0, the probability
of a sequence drawn from X to be in the typical set A is greater than 1 - and |A | ≤ 2H(X)+ .
Hence, if we only consider a typical realization of X (as opposed to every possible realization), we
have that asymptotically H ≤ 2H(X). Finally, let T be a mapping of X. Then, 2H(X|T) is the
number of typical realizations of X that are mapped to T . This means that the size of the typical
set of T is bounded from above by 2H(X)2H(X|T) = 2I(X;T). Plugging this into the PAC bound
above yields that with probability 1 - δ, the typical squared generalization error of T ,2T satisfies
2I(X；T) + log 2
2m
(13)
Appendix B	- Proof of Proposition 2
We make the following technical assumptions:
1.	w* and δw satisfy limdk→∞ w*Tδw = 0 almost surely.
2.	The moments of Tk are finite.
3.	The components of w* and δw are	in-general-positions,	satisfying
limdk→∞Pid=k1wi*4Pid=k1wi*22=0andlimdk→∞Pid=k1δwi4Pid=k1δwi22=0
almost surely.
Consider a sequence of i.i.d. random variable, {Xi}id=1 with zero mean and finite moments,
E [Xir] < ∞ for every r ≥ 1.
Let {ai }id=1 be a sequence of constants. Denote Yi = aiXi, so that {Yi }id=1 are indepen-
dent with zero mean and Var(Yi) = ai2E X2. Let S = Pid=1 aiXi = Pid=1 Yi and denote
Ud2 = Pid=1 Var(Yi) = E[X2] Pid=1 ai2.
The Lyapunov Central Limit Theorem (CLT) Billingsley (2008) states that if there exists some δ > 0
for which
then
1d
dl→∞ 可 X E[M2+"°
d	i=1
(14)
(15)
Plugging δ = 2 yields the following sufficient condition,
Pd=。4	E[X 4]
(PM2 E2X1
(16)
Let us apply the Lyapunov CLT to our problem. Here, the components ofTk are i.i.d. for sufficiently
large dk, with zero mean and finite rth moments for every r ≥ 1. Further, we assume that the
components of w* and δw are in-general-positions. This means that Lyapunov condition (16) is
satisfied for both w* TTk and δwT Tk almost surely, which means that
---D-→N(0, 1)
dk→∞
(17)
14
Under review as a conference paper at ICLR 2019
and
---D-→N(0, 1).
dk→∞
(18)
almost surely, where σT2 is the variance of the components of Tk .
Further, for every pair of constants a and b, the linear combination (aw* + bδw)T Tk also satisfies
Lyapunov's condition almost surely, which means that w* TTk and δwTTk are asymptotically jointly
Gaussian, with
E w*T Tk δwT Tk T = σT2 w*T δw -→ 0
almost surely.

15