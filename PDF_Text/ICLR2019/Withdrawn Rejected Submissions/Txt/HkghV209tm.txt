Under review as a conference paper at ICLR 2019
Optimistic Acceleration for Optimization
Anonymous authors
Paper under double-blind review
Ab stract
We consider new variants of optimization algorithms. Our algorithms are based on the observation
that mini-batch of stochastic gradients in consecutive iterations do not change drastically and conse-
quently may be predictable. Inspired by the similar setting in online learning literature called Opti-
mistic Online learning, we propose two new optimistic algorithms for AMSGrad and Adam,
respectively, by exploiting the predictability of gradients. The new algorithms combine the idea of
momentum method, adaptive gradient method, and algorithms in Optimistic Online learning,
which leads to speed up in training deep neural nets in practice.
1	Introduction
Nowadays deep learning has been shown to be very effective in several tasks, from robotics (e.g. Levine et al. (2017)),
computer vision (e.g. He et al. (2016); Goodfellow et al. (2014)), reinforcement learning (e.g. Mnih et al. (2013),
to natural language processing (e.g. Graves et al. (2013)). Typically, the model parameters of a state-of-the-art deep
neural net is very high-dimensional and the required training data is also in huge size. Therefore, fast algorithms are
necessary for training a deep neural net. To achieve this, there are number of algorithms proposed in recent years,
such as AMSGRAD (Reddi et al. (2018)), ADAM (Kingma & Ba (2015)), RMSPROP (Tieleman & Hinton (2012)),
ADADELTA (Zeiler (2012)), and NADAM (Dozat (2016)), etc.
All the prevalent algorithms for training deep nets mentioned above combines two ideas: the idea of adaptivity in
AdaGrad (Duchi et al. (2011); McMahan & Streeter (2010)) and the idea of momentum as Nes terov’ s Method
(Nesterov (2004)) or the Heavy ball method (Polyak (1964)). AdaGrad is an online learning algorithm that works
well compared to the standard online gradient descent when the gradient is sparse. The update of AdaGrad has a
notable feature: the learning rate is different for different dimensions, depending on the magnitude of gradient in
each dimension, which might help in exploiting the geometry of data and leading to a better update. On the other
hand, Nesterov’ s Method or the Momentum Method (Polyak (1964)) is an accelerated optimization algorithm
whose update not only depends on the current iterate and current gradient but also depends on the past gradients (i.e.
momentum). State-of-the-art algorithms like AMSGrad (Reddi et al. (2018)) and ADAM (Kingma & Ba (2015))
leverages these two ideas to get fast training for neural nets.
In this paper, we propose an algorithm that goes further than the hybrid of the adaptivity and momentum approach.
Our algorithm is inspired by Optimistic Online learning (Chiang et al. (2012); Rakhlin & Sridharan (2013);
Syrgkanis et al. (2015); Abernethy et al. (2018)). Optimistic Online learning considers that a good guess
of the loss function in the current round of online learning is available and plays an action by exploiting the good
guess. By exploiting the guess, those algorithms in Optimistic Online learning have regret in the form of
O( PtT=1 kgt - mt k), where gt is the gradient of loss function in round t and mt is the “guess” of gt before seeing
the loss function in round t (i.e. before getting gt). This kind of regret can be much smaller than O(√T) when
one has a good guess mt of gt. We combine the OPTIMISTIC ONLINE LEARNING idea with the adaptivity and the
momentum ideas to design new algorithms in training deep neural nets, which leads to New-Optimistic-AMSGrad
and new-optimistic-adam. We also provide theoretical analysis of New-Optimistic-AMSGrad. The proposed
Optimistic- algorithms not only adapt to the informative dimensions and exhibit momentums but also take advantage
ofa good guess of the next gradient to facilitate acceleration. We evaluate our algorithms with (Kingma & Ba (2015)),
(Reddi et al. (2018)) and (Daskalakis et al. (2018)). Experiments show that our Optimistic-algorithms are faster
than the baselines. We should explain that Daskalakis et al. (2018) proposed another version of optimistic algorithm
for ADAM, which is referred to as Adam-DISZ in this paper. We apply the idea of (Daskalakis et al. (2018)) on
AMSGRAD, which leads to AMSGRAD-DISZ. Both ADAM-DISZ and AMSGRAD-DISZ are used as baselines.
2	Preliminaries
Both AMSGRAD (Reddi et al. (2018)) and ADAM (Kingma & Ba (2015)) are actually ONLINE LEARNING algo-
rithms. They use regret analysis to provide some theoretical guarantees of the algorithms. Since one can convert
an online learning algorithm to an offline optimization algorithm by online-to-batch conversion (Cesa-Bianchi et al.
1
Under review as a conference paper at ICLR 2019
(2004)), one can design an offline optimization algorithm by designing and analyzing its counterpart in online learning.
Therefore, we would like to give a brief review of Online learning and Optimistic-Online learning.
2.1	B rief review of online learning
In the typical setting of online learning, there is a learner playing an action and then receiving a loss function in
each round t. Specifically, the learner plays an action wt ∈ K in round t, where wt is chosen in a compact and
convex set K ⊆ Rn, known as the decision space. Then, the learner sees the loss function 't(∙) and suffers loss
't(wt) for the choice. No distributional assumption is made on the loss functions sequence {'1(∙),'2(∙),...,'t(∙)}
in onl ine learning. Namely, the loss functions can be adversarial. The goal of an online learner is minimizing its
regret, which is
TT
RegretT :=	`t(wt) - min	`t(w).	(1)
w∈K
t=1	t=1
We can also define average regret as RegretT := RegTtT, which is regret divided by number of rounds T. In
Online learning literature, No-regret algorithms means online learning algorithms satisfying Regretτ → 0
as T → ∞.
2.2	Brief review of Optimistic Online learning
in recent years, there is a branch of works in the paradigm of Optimistic Online learning (e.g. chiang et al.
(2012); Rakhlin & sridharan (2013); syrgkanis et al. (2015); Abernethy et al. (2018)). The idea of oPTiMisTic
Online learning is as follows. Suppose that, in each round t, the learner has a good guess mt(∙) of the loss
function 't(∙) before playing an action wt. (Recall that the learner receives the loss function after the learner commits
an action!) Then, the learner should exploit the guess mt(∙) to choose an action wt, as mt(∙) is close to the true loss
function 't(∙). 1
For example, (syrgkanis et al. (2015)) proposed an optimistic-variant of the so called Follow-the-Regularized-
Leader (FTRL). The update rule of FTRL (see e.g. Hazan (2016)) is
Wt = argmin hw,Lt-ιi + ^R(w),
w∈K	η
(2)
where R(∙) is a 1-strong convex function with respect to a norm (∣∣∙ ∣∣) on the constraint set K, Lt-ι := P：=； gs is
the cumulative sum of gradient vectors of the convex loss functions (i.e. g： := ▽'：(Ws) ) up to but not including t,
and η is a parameter. The oPTiMisTic-FTRL of (syrgkanis et al. (2015)) has update
Wt = arg minhw, Lt—； + mti + 工R(w),
w∈K	η
(3)
where mt is the learner,s guess of the gradient vector gt := V't(wt). Under the assumption that loss functions are
convex, the regret of oPTiMisTic-FTRL satisfies RegretT ≤
O(√PT=1 kgt - mtk*),
which can be much smaller
than O(√T) of FTRL if mt is close to gt. Consequently, OPTIMISTIC-FTRL will have much smaller regret than
FTRL. on the other hand, if mt is far from gt, then the regret of oPTiMisTic-FTRL would be a constant factor
worse than that of FTRL without optimistic update. in the later section, we provide a way to get mt . Here, we just
want to emphasize the importance of leveraging a good guess mt for updating Wt to get a fast convergence rate (or
equivalently, small regret). We also note that the works of Optimistic Online learning chiang et al. (2012);
Rakhlin & sridharan (2013); syrgkanis et al. (2015)) has been shown to accelerate the convergence of some zero-sum
games.
2.3	Adam and AMSGrad
Adam (Kingma & Ba (2015)) is a very popular algorithm for training deep nets. it combines the momentum idea
Polyak (1964) with the idea of ADAGRAD (Duchi et al. (2011)), which has individual learning rate for different dimen-
sions. The learning rate of AdaGrad in iteration t for a dimension j is proportional to the inverse of y∕∑ts=ιgs[j∖2,
where gs [j] is the jth element of the gradient vector gs in time s. This adaptive learning rate may help for accelerating
the convergence when the gradient vector is sparse (Duchi et al. (2011)). However, when applying AdaGrad to train
deep nets, it is observed that the learning rate might decay too fast (Kingma & Ba (2015)). Therefore, (Kingma &
1Imagine that if the learner would had been known 't(∙) before committing its action, then it would exploit the knowledge to
determine its action and minimizes the regret.
2
Under review as a conference paper at ICLR 2019
Algorithm 1 AMSGRAD (Reddi et al. (2018))
1:	Required: parameter β1 , β2 , and ηt .
2:	Init: w1 .
3:	for t = 1 to T do
4:	Get mini-batch stochastic gradient vector gt ∈ Rd at wt .
5:	θt = β1θt-1 +(1 -β1)gt.
6:	vt = β2vt-1 + (1 - β2)gt2 .
7： Vt = maχ(Vt-1, Vt).
8:	wt+ι = Wt - ηt √~. (element-wise division)
9： end for
Ba (2015)) proposes using a moving average of gradients (element-wise) divided by the root of the second moment
of the moving average to update model parameter w (i.e. line 5,6 and line 8 of Algorithm 1). Yet, ADAM (Kingma
& Ba (2015)) fails at some online convex optmization problems. AMSGrad (Reddi et al. (2018)) fixes the issue.
The algorithm of AMSGrad is shown in Algorithm 1. The difference between Adam and AMSGrad lies on line
7 of Algorithm 1. ADAM does not have the update of line 7. (Reddi et al. (2018)) adds the step to guarantee a non-
increasing learning rate, √t-. which helps for the convergence (i.e. average regret RegretT → 0.) For the parameters
of AMSGrad, it is suggested that βι = 0.9, β2 = 0.99 and η = n/ʌ/t for a number η.
3 new-optimistic-amsgrad
As mentioned in the introduction, Daskalakis et al. (2018) proposed one version of optimistic algorithm for ADAM,
which is referred to as Adam-DISZ in this paper. Daskalakis et al. (2018) did not propose an optimistic algorithm for
AMSGrad but such an extension is straightforward which is referred to as AMSGrad-DISZ.
In this section, we propose a new algorithm for training deep nets: new-optimistic-amsgrad, shown in Algo-
rithm 2. new-optimistic-amsgrad has an optimistic update, which is line 9 of Algorithm 2. It exploits the guess
mt+1 of gt+1 to get wt+1, since the vector ht+1 uses mt+1. Notice that the gradient vector is computed at wt instead
of Wt_1 and the moving average of gradients is used to update Wt+1. One might want to combining line 8 and line 9
and getting a single line: wt+1 = Wt-1 - η√——ηt+ι↑-4β^1h√^. From this, We see that wt+1 is updated from Wt_1
instead of Wt . Therefore, while NEW-OPTIMISTIC-AMSGRAD looks like just doing an additional update compared to
AMSGrad, the difference of update is subtle. We also want to emphasize that although the learning rate on line 9
contains ɪ-4^ factor. We suspect that it is due to the artifact of our theoretical analysis. In our experiments, the learning
rate on line 9 does not have the factor of ɪ-4^. That is, in practice, We implement line 9 as Wt+ι = Wt+1 - ηt+ι h√+1.
We leave closing the gap between theory and practice as a future work.
We see that new-optimistic-amsgrad inherits three properties
•	Adaptive learning rate of each dimension as ADAGRAD (Duchi et al. (2011)). (line 8)
•	Exponentially moving average of the past gradients as NESTEROV’ S METHOD (Nesterov (2004)) and the
Heavy-Ball method (Polyak (1964)). (line 5)
•	Optimistic update that exploits a good guess of the next gradient vector as optimistic online learning algo-
rithms (Chiang et al. (2012); Rakhlin & Sridharan (2013); Syrgkanis et al. (2015); Abernethy et al. (2018)).
(line 9)
The first property helps acceleration when the gradient has sparse structure. The second one is the well-recognized
idea of momentum which can achieve acceleration. The last one, perhaps less known outside the Online learning
community, can actually achieve acceleration when the prediction of the next gradient is good. We are going to
elaborate this property in the later section where we give the theoretical analysis of new-optimistic-amsgrad.
To obtain mt, we use the extrapolation algorithm of (Scieur et al. (2016)). Extrapolation studies estimating the limit of
sequence using the last few iterates (Brezinski & Zaglia (2013)). Some classical works include Anderson acceleration
(Walker & Ni. (2011)), minimal polynomial extrapolation (Cabay & Jackson (1976)), reduced rank extrapolation
(Eddy (1979)). These method typically assumes that the sequence {xt} ∈ Rd has a linear relation
Xt = A(χt-ι - χ*) + χ*,	(4)
for an unknown matrix A ∈ Rd×d (not necessarily symmetric). The goal is to use the last few iterates {xt } to estimate
the fixed point x* on (4). (Scieur et al. (2016)) adapt the classical extrapolation methods to the iterates/updates of
3
Under review as a conference paper at ICLR 2019
Algorithm 2 NEW-OPTIMISTIC-AMSGRAD
1:	Required: parameter β1 , β2 , and ηt .
2:	Init: w1 .
3:	for t = 1 to T do
4:	Get mini-batch stochastic gradient vector gt ∈ Rd at wt .
5:	θt = β1θt-1 +(1 -β1)gt.
6:	vt = β2vt-1 + (1 - β2)gt2 .
7：	Vt = maχ(Vt-1, Vt).
8：	wt+1 = Wt-1 - ηt -√v-. (element-wise division)
9：	Wt+1 = Wt+1 - ηt+i1-4β1 h√+1, where ht+1 ：= β1θt-1 + (1 - βι)mt+ι
and mt+ι is the guess of gt+ι. (In practice, use wt+ι = Wt+1 - ηt+ι h√+1.)
2	V V t
10： end for
Algorithm 3 REGULARIZED APPROXIMATE MINIMAL POLYNOMIAL EXTRAPOLATION (RMPE) (Scieur et al.
(2016))
1： Input： some sequence {xs ∈ Rd}ss==r0, parameter λ > 0
2： Compute matrix U = [x1 - x0, . . . , xr - xr-1] ∈ Rd×r
3： Obtain z by solving (U>U + λI)z = 1.
4： Get c = z/(z>1)
5: Output: ∑r-o1CiXi, the approximation of the fixed point x* *.
an optimization algorithm and propose an algorithm that produces a solution that is better than the last iterate of the
underlying optimization algorithm in practice. The algorithm of (Scieur et al. (2016)) (shown in Algorithm 3) allows
the iterates {xt} to be nonlinear
xt -x* = A(xt-1 -x*) +et,	(5)
where et is a second order term (namely, satisfying ketk2 = O(kxt-1 - x* k22). Some theoretical guarantees regarding
the distance between the output and x* is provided in (Scieur et al. (2016)).
In NEW-OPTIMISTIC-AMSGRAD, we use Algorithm 3 to get mt . Specifically, mt is obtained by
• Call Algorithm 3 with input being a sequence of some past r + 1 gradients, {gt, gt-1, gt-2, . . . , gt-r} to
obtain mt, where r is a parameter.
• Set mt ：= Σir=-01cigt-r+i from the output of Algorithm 3.
If the past few gradients can be modeled by (4) approximately, the extrapolation method should be expected to work
well in predicting the gradient. In practice, it helps to achieve faster convergence.
new-optimistic-adam By removing line 7 in Algorithm 2, the step of making monotone weighted second
moment, we obtain an algorithm which we call it new-optimistic-adam, as the resulting algorithm can be viewed
as an Optimistic-variant of Adam.
3.1 Theoretical analysis of new-optimistic-amsgrad
We provide the regret analysis here. We denote the Mahalanobis norm k • kH = VZr,H) for some PSD matrix
H. For the PSD matrix diag{Vt}, where diag{Vt} represents the diagonal matrix such that its ith diagonal element
is Vt[i] in Algorithm 2, We define the the corresponding Mahalanobis norm k • kψt ：= ，h, diag{Vt}1/2), where
We use the notation ψt to represent the matrix diag{Vt}1/2. We can also define the the corresponding dual norm
k • kψ: ：= PTdiag⅛F17τi.
We assume that the model parameter W is in d-dimensional space. That is, W ∈ Rd . Also, the analysis of NEW-
OPTIMISTIC-AMSGRAD is for unconstrained optimization. Thus, we assume that the constraint K of the benchmark
in the regret definition, minw∈K PtT=1 `t (W), is a finite norm ball that contains the optimal solutions to the underlying
offline unconstrained optimization problem.
4
Under review as a conference paper at ICLR 2019
Now we can conduct our analysis. First of all, we can decompose the regret as follows.
T	TT	T
RegretT := X't(wt) - miy X't(w) ≤ Xhwt - w*, V't(wt)i ：= Xhwt - w*,gti
w∈K
t=1	t=1	t=1	t=1
TT
ΣShwt-1 - w*,gti + ^hwt - wt-2 ,gti
(6)
T
Xhwt-2
t=1
TT
-w*,gti+Ehwt- wt- 1 ,gt - hti + Ehwt- wt- 1, hti.
where the first inequality is by assuming that the loss function't(∙) is convex and that We use the notation gt :=
V't (wt) which we adopt throughout the following proof for brevity.
Given the decomposition, let US analyze the first term PT=I hwt-1 - w*,gti in (6).
Lemma 1. Denote D∞ = maxt Ilwt-1-w*k∞∙ WehavethatPT=Ihwt-1-w*,gti ≤ 24"-.])D∞ Pd=ι VT[i]2 +
PT ηt kθ k2 + d2 PT Pd	βιvt[i]1/2
2=t=1 (i-βι) ∣∣θtkψ: + d∞ 2=t=1 2^i=1 2ηt(i-βι).
Astute readers may realize that the bound in Lemma 1 is actually the bound of AMSgrad. Indeed, since in online
learning setting the loss vectors gt come adversarially, it does matter how gt is generated. Therefore, the regret of
PT=Ihwt-1 - w*,gti can be bounded in the same way as AMSgrad. In Appendix B, we provide the detail proof
of Lemma 1.
Now we switch to bound the other sums PT=Ihwt — wt-1, gt 一 hti + PT=Ihwt — wt-1, hti in (6). The proof is
available in Appendix C.
Lemma 2. PT=Ihwt - wt-1 ,gt - hti + PT=Ihwt - wt-1 ,hti ≤ PT=I ηtkgt - Mjι-βιψ-[)* - ⅛kwt -
wt- 1 k⅛ ψt-1 .	8
Combining (6) and Lemma 1 and 2 leads to
dT
RegretT ≤ 2ηT^D∞ XvT+ X (1⅛同品
i=1	t=1	(7)
+ D∞ XX 2βlvi-βι) + X η2tkgt - htk2⅛ψt-1)* - ^nt kwt - wt-1 k⅛Ψt-1.
Now we can conclude the following theorem. The proof is in Appendix D.
Theorem 1.	Denote Y := β1∕√β2 < 1 and D∞ = maxt Ilwt-1 — w*k∞∙ Then,
RegretT ≤
1
2ηT(1 - βι)
d	Td
D∞ X vT M2 +D∞ XX
i=1	t=1 i=1
βιvt [i]1/2
2ηt(i - βι)
TT
+ X 2ηt(I- βι)kgt - mt向+ X ηkgt - htk21-β1 ψtτ)*.
t=1	t=1	8
(8)
One should compare the bound with that of AMSGrad (Reddi et al. (2018)), which is
d	T d	1/2
Regret ≤ J7D∞ XvT评 + D∞ XX「
I i=1	(9)
η√1 +log T	X
+ (1 - βι)2(i - Y)√1 - β2 ⅛ kghT[i]k2,
where η = η∕√t in their setting. We need to compare the last two terms in (8) with the last term in (9). We are
going to show that, under certain conditions, the bound is smaller than that of AMSGRAD. Let us suppose that gt is
5
Under review as a conference paper at ICLR 2019
close to mt so that PT=I 2ηt(1 - βι)kgt - mtkψ* is much smaller than the last term of (8). Yet, the last term of (8),
PT=I η2t Ilgt - htk( 1-β1 ψt 1)*, might be actually o(√T) and consequently might also be smaller than the last term of
(9). To see this, let US rewrite PT=I η2t∣∣gt - ht|( 1-β1 ,力七)* and set η = η∕√t as (Reddi et al. (2018)); it is
C(X XX	(gt [i]2 - ht[i]2) ∖	C(X XX η (gt [i]2 - ht[i]2)、
O(S3ηtFtF) = O(S之我 E ).
(10)
Assume that if each 3√--h[i]~) in the inner sum is bounded by a constant c, We will get PT= ι O(√1t) = O(√T).
Yet, the denominator ,Vt-ι is non-decreasing so that we can actually have a smaller bound. That is, in practice,
v∕vt-γ∖im might grow over time, and the growth rate is different for different dimension i. If ,Vt-ι[i] in the de-
nominator grows like O( √t) then the last term is just O (log T), which might be better than that of the last term on
(9). One can also get a data dependent bound of the last term of (8). To summarize, when mt is close to gt, NEW-
optimistic-amsgrad can have a smaller regret (thus better convergence rate) than Adam (Kingma & Ba (2015))
and AMSGRAD (Reddi et al. (2018)).
3.2 Comparison of Adam-DISZ in Daskalakis et al. (2018)
Algorithm 4 ADAM-DISZ Daskalakis et al. (2018)
1:	Required: parameter β1 , β2, and ηt .
2:	Init: w1 ∈ K.
3:	for t = 1 to T do
4:	Get mini-batch stochastic gradient vector gt ∈ Rd at wt .
5:	θt = β1θt-1 + (1 - β1)gt.
6:	vt = β2vt-1 + (1 - β2 )gt2 .
7:	wt+1 = π-k[wt - 2ηt √θVt + η √Vv-1ι].
8:	end for
We are aware of Algorithm 1 in Daskalakis et al. (2018), which was also motivated by Optimistic Online learn-
ing 2. For comparison, we replicate Adam-DISZ in Algorithm 4. We are going to describe the differences of the
algorithms and the differences of the contributions between our work and their work. First of all, by comparing new-
optimistic-amsgrad (Algorithm 2) or new-optimistic-adam (Algorithm 2 without line 7) with Adam-DISZ
(Algorithm 4), we see that the updates are indeed different. The update cannot be written into the same form as
our new-optimistic-amsgrad (and vise versa). new-optimistic-amsgrad (Algorithm 2) actually uses two
interleaving sequences of updates {wt}T=ι, {w~ ɪ }T=ι, Also, Daskalakis et al. (2018) does not have any theoretical
analysis of Adam-DISZ. The theoretical analysis in Daskalakis et al. (2018) is for other algorithms. Second, the
contributions are very different. Daskalakis et al. (2018) focus on training GANs Goodfellow et al. (2014). GANs is
a two-player zero-sum game. There have been some related works in Optimistic Online learning like Chiang
et al. (2012); Rakhlin & Sridharan (2013); Syrgkanis et al. (2015)) showing that if both players use some kinds of
Optimistic-update, then acceleration to the convergence of the minimax value of the game is possible. Daskalakis
et al. (2018) was inspired by these related works and showed that Optimistic-Mirror-Descent can avoid the
cycle behavior in a bilinear zero-sum game, which accelerates the convergence. Our work is about solving minx f(x)
(e.g. empirical risk) quickly.
We also show that ADAM-DISZ suffers the non-convergence issue as Adam. The proof is available in Appendix E.
Theorem 2.	There exists a convex online learning problem such that ADAM-DISZ has nonzero average regret (i.e.
ReTret = o(τ ))
One might wonder if the non-convergence issue can be avoided if one let the weighted second moment of ADAM-
DISZ be monotone by adding the step Vt = max(Vt-ι,vt) as AMSGrad, which we call it AMSGRAD-DISZ.
Unfortunately, we are unable to prove if the step guarantees convergence or not.
2 We notice that the term new-optimistic-adam was already used in Daskalakis et al. (2018) to describe their Algorithm 1
in the paper. To avoid confusion, let us still use new-optimistic-adam to refer to Algorithm 2 without line 7 in this paper, while
we use Adam-DISZ to refer to their algorithm, where DISZ are the initials of the authors’ last names of the paper Daskalakis et al.
(2018).
6
Under review as a conference paper at ICLR 2019
4 EXPERIMENTS
To demonstrate the effectiveness of our proposed method, we test its performance with various neural network ar-
chitectures, including fully-connected neural networks, convolutional neural networks (CNN’s) and recurrent neural
networks (RNN’s). The results illustrate that new-optimistic-amsgrad is able to speed up the convergence of
state-of-art AMSGrad algorithm, making the learning process more efficient.
For AMS GRAD algorithm, we set the parameter β1 and β2, respectively, to be 0.9 and 0.999, as recommended in
Reddi et al. (2018). We tune the learning rate η over a fine grid and report the results under best-tuned parameter
setting. For NEW-OPTIMISTIC-AMSGRAD and AMSGRAD-DISZ, we use same β1, β2 and learning rate as those for
AMS Grad to make a fair comparison of the enhancement brought by the optimistic step. We use the same weight
initialization for all algorithms. The remaining tuning parameter of NEW-OPTIMISTIC-AMSGRAD is r, the number
of previous gradients that we use to predict the next move. We conduct new-optimistic-amsgrad with different
values of r and observe similar performance (See Appendix A). Hence, we report r = 15 for all experiments for
tidiness of plots. To follow previous works of Reddi et al. (2018) and Kingma & Ba (2015), we compare different
methods on MNIST, CIFAR10 and IMDB datasets in our experiment. For MNIST, we use a noisy version named as
MNIST-back-rand in Larochelle et al. (2007) to increase the training difficulty.
4.1	MNIST-back-Rand + Multi-layer Neural Networks
Our experiments start with fully connected neural network for multi-class classification problems. MNIST-back-rand
dataset consists of 12000 training samples and 50000 test samples, where random background is inserted in original
MNIST hand written digit images. The input dimension is 784 (28×28) and the number of classes is 10. We investigate
a multi-layer neural networks with input layer followed by a hidden layer with 200 cells, which is then connected to
a layer with 100 neurons before the output layer. All hidden layer cells are rectifier linear units (ReLu’s). We use
mini-batch size 128 to calculate stochastic gradient in each iteration. Model performance is evaluated by multi-class
cross entropy loss. The training loss with respect to number of iterations is reported in Figure 1.
# Iterations
Figure 1: Fully connected neural networks on MNIST-Back-Rand dataset. Left panel: training loss vs. number of
iterations. Right panel: training loss only for 5 epochs. One epoch means all training data points are used once. For
this dataset, since the training size is 12000 and the mini-batch size is 128, one epoch means 93.75 iterations. This
allows us to better visualize the speed in the first few epochs. In large applications, data-loading is often the bottleneck.
In the plots, “AMSGrad-Proposed” is the proposed new-optimisic-amsgrad.
# Epochs
On this dataset, we empirically observe obvious improvement of new-optimistic-amsgrad in terms of both con-
vergence speed and training loss. On the other hand, AMSGrad-DISZ performs similarly to AMSGrad in general.
4.2	CIFAR 1 0 + CONVOLUTIONAL NEURAL NETWORKS (CNN)
Convolutional Neural Networks (CNN) have been widely studied and is important in various deep learning applications
such as computer vision and natural language processing. We test the effectiveness of new-optimistic-amsgrad
in deep CNN’s with dropout. We use the CIFAR10 dataset, which includes 60,000 images (50,000 for training and
10,000 for testing) of size 32 × 32 in 10 different classes. ALL-CNN architecture proposed in Springenberg et al.
(2015) is implemented with two blocks of 3 × 3 convolutional filter, 3 × 3 convolutional layer with stride 2 and
dropout layer with keep probability 0.5. Another block of 3 × 3, 1 × 1 convolutional layer and a 6 × 6 global averaging
pooling is added before the output layer. We apply another dropout with keep probability 0.8 on the input layer. The
cost function is multi-class cross entropy. The batch size is 128. The images are all whitened. The training loss is
provided in figure 2. The result shows that new-optimistic-amsgrad accelerates the learning process significantly
7
Under review as a conference paper at ICLR 2019
and gives lowest training cost after 10000 iterations. For this dataset, the performance of AMSGrad-DISZ is worse
than original AMSGrad.
2.5
2.5
2
SSOl
5 1
6u_u_e」j_
0.5
1
-AMSGrad
一 AMSGrad-DISZ
一AMSGrad-Proposed
2000 4000 6000 8000 10000
0.5
0	1
51
6U-U-E匚
2
SSOl
2	3	4	5
# Iterations	# EPoChes
Figure 2: CIFAR10 c96-c96-c96-c192-c192-c192-c192-c192-c10 ConVNet with dropout. Left panel: training loss vs.
number of iterations. Right panel: training loss in the first 5 epochs.
4.3 IMDB + LONG-SHORT TERM MEMORY (LSTM)
As another important application of deep learning, natural language processing tasks often benefit from considering
sequence dependency in the models. Recurrent Neural Networks (RNN’s) achieves this goal by adding hidden state
units that act as ”memory”. Long-Short Term Memory (LSTM) is the most popular structure in building RNN’s. We
use IMDB movie review dataset from Maas et al. (2011) to test the performance of new-optimistic-amsgrad
in RNN’s under the circumstance of high data sparsity. IMDB is a binary classification dataset with 25000 training
and test samples respectively. Our model includes a word embedding layer with 5000 input entries representing most
frequent words in the dataset and each word is embedded into a 32 dimensional space. The output of embedding layer
is passed to 100 LSTM units, which is then connected to 100 fully connected ReLu’s before reaching the output layer.
Binary cross-entropy loss is used and the batch size is 128. We provide the results in figure 3.
Figure 3: Embedding-LSTM100-f100 RNN: on the left is the training loss against number of iterations. On the right
is the plot for first 5 epochs.
# EPoChes
We observe a considerable improvement in convergence speed. In the first epoch, the result is more exciting. At
epoch 0.5, new-optimistic-amsgrad already achieves the training loss that vanilla AMSGrad can produce with
more than 1 epoch. The sample efficiency is significantly improved. On this dataset, AMSGrad-DISZ performs less
effectively and may be trapped in local minimum. We remark that in each iteration, only a small portion of gradients
in embedding layer is non-zero. Thus, this experiment demonstrates that new-optimistic-amsgrad could also
perform well with sparse gradient. (See additional experiments in Appendix A.)
5 Extensions and Future Work
In this paper, we propose new-optimistic-amsgrad, which combines optimistic learning and AMSGrad to
strengthen the learning process of optimization problems, in particular, deep neural networks. The idea of adding
optimistic step can be easily extended to other optimization algorithms, e.g ADAM and ADAGRAD. We provide
Optimistic-AdaGrad algorithm and theoretical results in Appendix F. A potential direction based on this work is
to improve the method for predicting next gradient. We expect that optimistic acceleration strategy could be widely
used in various optimization problems.
8
Under review as a conference paper at ICLR 2019
References
Jacob Abernethy, Kevin A. Lai, Kfir Y. Levy, and Jun-Kun Wang. Faster rates for convex-concave games. Computa-
tional Learning Theory (COLT), 2018.
C. Brezinski and M. R. Zaglia. Extrapolation methods: theory and practice. Elsevier, 2013.
S. Cabay and L. Jackson. A polynomial extrapolation method for finding limits and antilimits of vector sequences.
SIAM Journal on Numerical Analysis, 1976.
Nicol0' Cesa-Bianchi, Alex Conconi, and ClaUdio Gentile. On the generalization ability of on-line learning algorithms.
Information Theory, IEEE Transactions, 2004.
Chao-Kai Chiang, Tianbao Yang, Chia-JUng Lee, Mehrdad Mahdavi, Chi-Jen LU, Rong Jin, and ShenghUo ZhU. Online
optimization with gradUal variations. Computational Learning Theory (COLT), 2012.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. Inter-
national Conference on Learning Representations (ICLR), 2018.
Timothy Dozat. Incorporating nesterov momentUm into adam. International Conference on Learning Representations
(ICLR) Workshop Track, 2016.
John DUchi, Elad Hazan, and Yoram Singer. Adaptive sUbgradient methods for online learning and stochastic opti-
mization. Journal of Machine Learning Research (JMLR), 2011.
R. Eddy. Extrapolating to the limit of a vector seqUence. Information linkage between applied mathematics and
industry, Elsevier, 1979.
Ian Goodfellow, Jean PoUget-Abadie, Mehdi Mirza, Bing XU, David Warde-Farley, Sherjil Ozair, Aaron CoUrville,
and YoshUa Bengio. Generative adversarial nets. Annual Conference on Neural Information Processing Systems
(NIPS), 2014.
Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recUrrent neUral networks.
International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.
Elad Hazan. IntrodUction to online convex optimization. Foundations and Trends in Optimization, 2016.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image recognition. Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on
Learning Representations (ICLR), 2015.
HUgo Larochelle, DUmitrU Erhan, Aaron CoUrville, James Bergstra, and YoshUa Bengio. An empirical evalUation
of deep architectUres on problems with many factors of variation. International Conference on Machine Learning
(ICML), 2007.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visUomotor policies.
Annual Conference on Neural Information Processing Systems (NIPS), 2017.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan HUang, Andrew Y. Ng, and Christopher Potts. Learning word
vectors for sentiment analysis. Annual Meeting of the Association for Computational Linguistics (ACL), 2011.
H. Brendan McMahan and Matthew J. Streeter. Adaptive boUnd optimization for online convex optimization. Com-
putational Learning Theory (COLT), 2010.
Volodymyr Mnih, Koray KavUkcUoglU, David Silver, Alex Graves, Ioannis AntonogloU, Daan Wierstra, and Martin
Riedmiller. Playing atari with deep reinforcement learning. Annual Conference on Neural Information Processing
Systems (NIPS) Deep Learning Workshop, 2013.
YUrii Nesterov. IntrodUctory lectUres on convex optimization: A basic coUrse. Springer, 2004.
B. T. Polyak. Some methods of speeding Up the convergence of iteration methods. Mathematics and Mathematical
Physics, 1964.
9
Under review as a conference paper at ICLR 2019
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. Annual
Conference on Neural Information Processing Systems (NIPS), 2013.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. International Conference
on Learning Representations (ICLR), 2018.
Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceleration. Annual Conference
on Neural Information Processing Systems (NIPS), 2016.
Jost Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convo-
lutional net. International Conference on Learning Representations (ICLR), 2015.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in
games. Annual Conference on Neural Information Processing Systems (NIPS), 2015.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA:
Neural Networks for Machine Learning, 2012.
H. F. Walker and P. Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on Numerical Analysis, 2011.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. arXiv:1212.5701, 2012.
A More Experimental Results
A.1 CHOOSING PARAMETER r
In NEW-OPTIMISTIC-AMSGRAD, parameter r, the number of previous gradients used, may affect the performance
dramatically. If we choose r too small (e.g r < 10), the optimistic updates will start at very early stage, but the
information we collect from the past is limited. This may make our training process off-track at first several iterations.
On the other hand, if r is chosen to be too large (e.g r >= 30), although we may get a better prediction of next
gradient, the optimistic step will start late so new-optimistic-amsgrad may miss great chances to improve the
learning performance at early stages. Additionally, we need more room to store past gradients, hence the operational
time will increase.
# Iterations
Figure 4: The training performance on two datasets for different r values.
# Iterations
The empirical impact of different r value is reported in figure 4. We suggest 10 ≤ r ≤ 20 as an ideal range, and r = 15
tend to perform well in most training tasks. Actually, we may make the algorithm more flexible by ”early start”. For
example, when we set r = 20, we can instead start adding optimistic step at iteration 10, and gradually increase the
number of past gradients we use to predict next move from 10 to 20 in next 10 iterations. After iteration 21, we fix the
moving window size for optimistic prediction as 20. This may bring enhancement to new-optimistic-amsgrad
because it can seek opportunities to accelerate learning in first several iterations, which is critical for indicating a better
direction towards the minimum loss.
10
Under review as a conference paper at ICLR 2019
A.2 Optimistic ADAM
We also conduct experiments on new-optimistic-adam and Adam-DISZ. The experiment setting is similar to
NEW-OPTIMISTIC-AMSGRAD, where we fix β1, β2 and compare the performance using the best learning rate with
respect to ADAM. We provide a brief summary of the results in figure 5, with r = 15. The improvement brought
by new-optimistic-adam is obvious, indicating that adding optimistic step could also enhance the performance of
Adam optimizer.
0.8
0.2
1
ω
o 0.6
6
'Ξ
回0.4
500	1000	1500	2000
# Iterations
—ADAM
一 ADAM-DISZ
一ADAM-Proposed
—ADAM
ADAM-DISZ
——ADAM-Proposed
2.5
0l-------1-----1------1------1------
1	2000 4000 6000 8000 10000
# Iterations
Figure 5: Training loss of new-optimistic-adam. Left panel: MNIST-Back-Rand. Right panel: CIFAR10 (CNN).
In the plots, “ADAM-Proposed” is the proposed new-optimisic-adam.
B Proof of lemma 1
Lemma 1 Denote D∞ = maxt ||wt- 1 - w*k∞. PT=Ihwt-2 - w*,gti ≤ 2ηT(I-IeI)D∞ Pd=I ^t[i]2 +
PT ηt	kθ k2	+	d2	PT	Pd	βιvt[i]1/2
2=t=1 (i-βι)	∣∣θtkΨ:	+	d∞	2=t=1	2^i=1	2ηt(i-βι).
Proof. The proof of this lemma basically follows that of (Reddi et al. (2018)). We have that
kwt+ 2 - w*kψt = kwt-2 - ηtdiag(Vt)T∕2θt - w*kψt
=Ilwt- 1 — w*kψt + η2kθt 临一2ηthθt,wt-2 — w*i	(11)
=Ilwt- 1 — w*kψt + η2kθt 隘一2」--1 +(I- βI)gt,wt-2 — w*i.
By rearranging the terms above and summing it from t = 1, . . . , T,
TT
Xhwt- 1- w*,gti = X 2η(1 - βι){kwt-2 - w*kψt -kwt+1- w*kψt} + 2(1 Jel)kθt 临
+ 1 - βι hθt-1,wt- 1 — w*i
T1	η
≤ X 2ηt(1 - β1) {kwt-2 - W kΨt - kwt+ 2 - W kΨt} + 2(1 - β1) kθtkΨ:
(12)
+ 2(1β-βι) kθt-lkψ
, βι i,,
+ 2ηt(1 - βι)11 t-2
- w* I2ψt
1η
≤ X 2ηt(1 - β1){kwt-2 - w kΨt - kwt+ 2 - w kΨt} + (1 - β1) kθtkψr
,	βl	k,,,
+ 2ηt(1- β1)11 t-2
-w*I2ψt.
where the first inequality is due to Young’s inequality and the second one is due to the constraint 0 < β1 ≤ 1 so that
ηt + Bmt ≤	ηt
2(i-β1) + 2(i-β1) ≤ (i-β1).
11
Under review as a conference paper at ICLR 2019
Now We can bound PT=Ihwt-1 - w*,gt> in (6) as
T
Xhwt-2
t=1
T1
-w*,gti ≤ ∑2η(1 -β1){kwt-1 - w*kψt - kwt+ 2
-w*kψj
Tη	T β
+ X (⅛) kθt临 + X 2ηt(1- βι) kwt-2 - w*kψt
---7~~~τττkw 1
2ηι(1-βι)” 2
-w*kψι +
1	X Jwt+1- w*kψt
2(1- βι) = { η
kwt+1 - w*kψt-ι }
ηt-ι
TT
+ X (⅛kθtkψt* + X,kwt- 1-w*kψt
d	T d	1/2	1/2[i]
=2η (1-βι)	X v1[i]	(w 1	[i]-w*[i])	+2T-βI	XX(Wt-1	[i]-w*[i])	{k -下}
i=1	t=2 i=1
TT
+ X (⅛kθtkψt* + X,kwt-2-w*kΨt,
(13)
≤
where the first inequality is due to (12).
Continue the analysis, we have
T
Ehwt- 2 - w*,gti
t=1
d	T d	1/2	1/2 [i]
≤ 2ηι (1-βι) X v1[i] (w 2 [i] - w*[i]) +2(T-β1) XX(wt-1 [i]-w*[iD {k - I--T)
i=1	t= i=1
TT
+ X (⅛kθtkψt* + X,kwt-2-w*kΨt,
≤ 2ηι(1 - βι)
d1
D∞ Xvι[i]2+2(1⅛
t1-/21[i]
T d 1/2
D∞ X X{T	ηt-ι
(14)
+ X而必而kwt-2
-w*kψt
1
2ητ(1 - βι)
dT
D∞ X Vτ[i]2 + X
i=1	t=1
τ
(1⅞) kθtkψ:+X
βι	.
2ηt(1 - βι)11 t-2
-w*kψt
1
2ητ(1 - βι)
dτ
D∞ X Vτ[i]2 + X
i=1	t=1
τd
(1⅞) kθtkψ:+d∞ X X
βιVt [i]1/2
2ηt(1- βι),
≤
1
—
T
}+X π⅛ k*
where the last equality is by telescoping sum.
12
Under review as a conference paper at ICLR 2019
C Proof of lemma 2
Lemma 2	P=Ihwt	- wt-1 ,gt	-	hti	+ P2ihwt	- wt-1 ,hti	≤	PT=1	ηt kgt	- Mjl-βl ψt-y -	23ηt kwt	-
Wt-1 k⅛ ψt-1	8
Proof. From the update, wt = wt_1 一 ηt ɪ-^ √ht≡f，
hwt - wt-1, hti = -ηthht, 1⅛diag{htt}-1∕2i
2	4	ht	1- β1	1/2	4 ht
=一加hηt =及 diag{vt}1/2, "ɪdiag{vt}	ηt 1一瓦 diag{vt}1/2 2
2
=- η kwt - wt-1k 中 ψt-1.
Therefore, we have that
(15)
TT
Ehwt — wt- 2 ,gt — hti+Ehwt —
T2
wt-1 ,hti =之hwt -wt-2,gt - hti- ηtkwt - wt-2k⅛
(a)	T
≤ 2∑kwt - wt-1k 1-β1 ψt-1kgt - htk( 1-8β1 ψt-1)
t=1
(b)	T 1	2
≤ 而kwt - wt-2k 1-8配
T
2
—kwt - wt-1 k
ηt	2
2
1-β1
—8ɪ ψt-1
(16)
Ψt-1 + η2t kgt - htk⅛1 Ψt-1)
22
- 加 kwt - wt-1 k 中Ψt-1
X η2t kgt-htk⅛1ψ-1)*
t=1
- 23ηt kwt - wt-1 k⅛ Ψt-1
*
—
*
where (a) is by Holder,s inequality and (b) is by Young,s inequality.
□
D Proof of Theorem 1
Theorem 1 Denote Y := βι∕√β2^ < 1 and D∞ = maxt kwt-1 - w*k∞. Then,
RegretT ≤
1
2ητ(i - βι)
d	T d
D∞ X ^t M2 +D∞ XX
i=1	t=1 i=1
βιVt[i]1∕2
2ηt(1 - βι)
TT
+ X22(I- βI)kgt - mtkψ* + X 2tkkgt - htk21-β1 ψt-1)*.
t=1	t=1	8
(17)
Proof. Recall that from (7), we show that
dT
RegretT ≤ 布LD∞ XvT或 + X (i⅛同品
+ D∞ XX 2*-βι) + X η2tkgt - htk2⅛ψt-1)* - 23tkwt - wt-2k⅛
To proceed, let us analyze PT=I (Γ-⅛)kθtkψ* - 肃 kwt - wt_1 kLβ1^ above. Notice that
(18)
ηt	kθ k2 _	ηt	X θt[i]2
(1-βι)k tkψ* = (i-βι) M Ppi,
(19)
13
Under review as a conference paper at ICLR 2019
and
y-kwt - Wt- 1 k2-eM	= y^hηtr^⅝^^√^~~, 1 -β1 diag(Vt-I)"2ηtrA^^^√^~~i
2ηt	2 -ɪ-ψt-1	2ηt	1 - βι √^t-ι	8	1 - βι √^t-ι
=3ηt XX ht[i]2
1 - βι i=1 pVt-i[i]，
where We use the update rule, Wt = w- 1 - η -r4r ht . Therefore, We have that
t 2	1一e1 Vvt-I
(20)
T
X (T-tβ17kθtkψ* -嬴kwt- Wt-2k⅛Ψt-ι
T
≤X
t=1
T
≤X
t=1
T
=X
t=1
T
≤X
t=1
T
=X
t=1
d
ηt	X
i=1
d
X
i=1
d
X
i=1
d
X
i=1
(I- β1) ηt	
(1	-β1) ηt_
(1	-β1) 2η
(1	-β1) 2ηt
θt[i]2 - X
PvtW t=1
θt[i]2 - X
Pvt W t=1
(θt[i]2-ht[i]2)
√vt[i]
(θt[i]-ht[i])2
Vt[i]
(1 - βι) kθt - htkψ,
3ηt
(1 - β1)
d
X
i=1
d
X
i=1
ηt
(I - βI)
(21)
T
X 2ηt(I- β1 )kgt - mtkψ*,
t=1
where the second inequality is due to that the sequence {Vt[i]} is non-decreasing and the last equality is because
θt - ht = β1θt-1 + (1 - β1)(gt) - (β1θt-1 + (1 - βI)(mt力=(1 - β1)(gt - mt).
To summarize,
RegretT ≤
1
2ητ(I- βι)
d	Td
D∞ X Vτ[i]2 + D∞ XX
i=1	t=1 i=1
βιvt[i]1/2
2ηt(1 - βι)
ττ
+ X2nt(1 - βι)kgt - mtkψ: + X ^tkgt - htk2ι-βιψt-ι)*.
t=1	t=1	8
(22)
□
14
Under review as a conference paper at ICLR 2019
E Proof of Theorem 2
Theorem 2 There exists a convex online learning problem such that ADAM-DISZ has nonzero average regret (i.e.
ReTret = o(t))
We basically follow the same setting as Theorem 1 of Reddi et al. (2018). In each round, the loss function `t(w) is
linear and the learner’s decision space is K = [-1, 1]
ft(w)
Cw , for t mod 3 = 1
-w , otherwise
where C ≥ 4. For this loss function sequences, the point w = -1 achieves the minimum regret, i.e. - 1 =
arg minw∈K PtT=1 ft(w) when T → ∞.
Consider the execution of ADAM-DISZ with βι = 0, β2 = ɪ+^, ηt = √t with η < ɪ √1 - β2. Notice that
β2/√β2 < 1 in this case, which satisfies the conditions of Kingma & Ba (2015).
The goal is to show that for all t, w3t+1 = 1, and Wt > 0. Let us denote Wt+ι := Wt - 2ηt √θv- + ηt √-1 and
wt+1 := ∏k [Wt+ι]. Assume the initial point is 1. As pointed it out by Reddi et al. (2018), the assumption of the
initial point is without loss of generality. If the initial point is not 1, then one can translate the coordinate system to a
new coordinate system so that the initial point is 1 and then choose the loss function sequences for the new coordinate
system. Therefore, the base case x1 = 1 is true.
Now assume that for some t > 0, x3t+1 = 1 and xs > 0 for all s ≤ 3t + 1. Observe that.
C , for t mod 3 = 1
Nft(W) = <	1 必.
-1 , otherwise
According to the update of ADAM-DISZ
W3t+2 = w3t+1 -
_____________2ηc_____________
，(3t + 1)(e2v3t + (I - e2)C2)
η_______________
∙p∕(3t + 1)(β2v3t-i + (1 - β2))
=1-
≥1-
≥1-
2ηC	η
—.	——.
P(3t + 1)(β2v3t + (1 - β2)C2)	p∕(3t + 1)(β2v3t-1 + (1 - β2))
2η	η
—,	:——,
∕(3t + I)(I -/2)	∕(3t + I)(I -e2)
(23)
So, W3t+2 = w3t+2∙
，(3t + I)(I - 82)
W3t+3 = W3t+2 +---， .	. .	.= +--, .	. .	. M
∕(3t + 2)(β2v3t+1 + (1 - β2))	∕(3t + 2)(β2v3t + (1 - β2)C2)
(24)
15
Under review as a conference paper at ICLR 2019
For ι^3t+4, let US first consider the case that W3t+3 < 1
W3t+4 =	min(W3t+3,1) +-,	=-------,	=
p∕(3t +	3)(β2v3t+2	+ (1 — β2))	∙∖∕(3t	+ 3)(β2v3t+1	+ (1 — β2)
w3t+2 +
2η Cη
p∕(3t + 2)(β2v3t+1 + (1 — β2))	ρ∕(3t + 2)(β2v3t + (1 — β2)C2)
+________2η η
/(3t + 3Xβ2v3t+2 + (I - β2 D	∖∕(3t÷-3)(β2v3t+Γ÷^T1-^β21y
2ηC	η
≥ 1 — —Z	— — —Z	=
p∕(3t + 1)(β2v3t + (1 — β2)C2)	p∕(3t + 1)(β2v3t-1 + (1 — β2))
+η+__________________________Cη__________+_________2η_______
∕ (3t + 2)(β2v3t+1 + (1 — β2))	∕∕ (3t + 2)(β2v3t + (1 — β2 )C2)	∙∕∕(3t + 3)(β2v3t+2 + (1 — β2))
1 — (1 — √3t+1)	ηC
√3t + 2' √(3t + 1)(β2V3t + (1 — β2)C2)
η+η
∕ (3t + 1)(β2v3t-1 + (1 — β2))	，(3% + 1)(β2v3t+1 + (1 - β2))
ηC	2η
∕ (3t + 1)(β2v3t + (1 — β2)C2)	，(3% + 3)(β2v3t+2 + (1 - β2))
〉
1 — (1 — √S) ∕W⅛τW
—	2η	_ + η +
∕(3t + 1)(1 — β2))	∕(3t + 1)(β2c 2 + (1 — β2))
=) 1 — (1 — √≡Ξ) , η =
√3t + 2)，(3t +1)(1 — β2))
___________2η___________
∕(3t + 3)(β2c 2 + (1 — β2))
(25)
(2 —√fe )η	,	2η
----/	二 + -Z
∕(3t + 1)(1 — β2))	∕(3t + 3)(β2c 2 + (1 — β2))
≥ 1 — (1—予), η
-	'	√3t + 2'，(3t + 1)(1 — β2))
(2 —√fc )η	,	√3η
----，	二 + -,
∕(3t + 1)(1 — β2))	∕(3t + 1)(β2c 2 + (1 — β2))
(0 1	/1	√3t + 1 ʌ	η	(2 — √3∕2 )η	l	√2η
——1 — (1 ——,	=)—,	二——,	二 + —,	=
√3t + 2，(3% + 1)(1 - β2))	，(3% + 1)(1 - β2))	，(3% + 1)(1 - β2))
≥ 1 — (1 — √≡Ξ) / η	+ 0.23 , η
√3t + 2 ∕(3t + 1)(1 — β2))	∕(3t + 1)(1 — β2)
(b)
≥ 1 - 0.12
η________
，(3% + 1)(1 - β2))
+ 0.23 /	/	------
，(3% + 1)(1 - β2)
≥1
where (a) uses ʌ∕β2C2+(I-⅞Σ = √1 — β2 for the choice of β2 = 1/(1 + 2C2), (b) is because 1 — √√Hii is a
V 3/ 2	7 3t+2
decreasing function of t and has its largest value at t = 1 for any t > 0, which is 1 —(号)≤ 0.12. To summarize, we
have that w3t+4 = 1.
Now if it is the case that i^3t+3 ≥ 1
W3t+4 = min(τ^3t+3, 1) +
2η
ʌ/ (3t + 3)(β2v3t+2 + (1 — β2))	a/ (3t + 3)(β2v3t+1 + (1 — β2))
1+
2η______________
，(3t + 3)(β2v3t+2 + (1 - β2))
η______________
∕(3t + 3)(β2v3t+1 + (1 — β2))
(26)
>1
—
η
16
Under review as a conference paper at ICLR 2019
where the last inequality is because
2Vβ2v3t+1 + (1 - β2) ≥ Vβ2v3t+2 + (1 - β2)	(27)
To see this,
√β2v3t+2 + (1 - β2) = √β2(β2v3t+1 + (1 - β2)) + (l - β2) = √(β2)v3t+ι + 1 - β2	(28)
So, the above inequality is equivalent to
2pβ2v3t+1 + (1 - β2) ≥ J(β2)v3t+1 + 1 - β2,	(29)
which is true. This means that w3t+4 = 1. Therefore, we have completed the induction.
To summarize, We have '3t+1(w3t+1) + '3t+2(w3t+2) + '3t+3(w3t+3) - '3t+ι(-1) - '3t+2(w-1) - '3t+3(-1) ≥
2C - 4. That is, for every 3 steps, the algorithms suffers regret at least 2C - 4, this means that the regret over T
rounds Would be (2C - 4)T /3, Which is not sublinear to T. NoW We have completed the proof. One may folloW the
analysis of Theorem 2 of Reddi et al. (2018) to generalize the result so that ADAM-DISZ does not converge for any
β1,β2 ∈ [0,1) such that βι < √β2>.
F Optimistic-AdaGrad
The optimistic update step can also be extended to other algorithms as Well. For example, based on AdaGrad, We
propose Optimistic-AdaGrad by including the optimistic update step.
Algorithm 5 OPTIMISTIC-ADAGRAD (UNCONSTRAINED)
1:	Required: parameter η .
2:	Init: w1 .
3:	for t = 1 to T do
4:	Get current gradient gt at wt .
5:	wt+1 = wt-2 - ηdiag(Gt)T/2gt
(PS=I gt[1]2	0	0
Where diag(Gt) :=	0	Pts=1 gt [2]2 0
∖	0	0	∙
6:	wt+1 = wt+1 - ndiag(Gt)-1/2mt+i,
Where mt+1 is the guess of gt+1.
7:	end for
F.1 Analysis:
We provide the regret analysis here. Let us recall the notations and assumptions first. We denote the Mahalanobis norm
k ∙ ∣∣h = Ph,H.〉for some PSD matrix H. We let ψt(χ) :=(x, diag{Vt}1/2Xi for the PSD matrix diag{Vt}, where
diag{Vt} represents the diagonal matrix such that its ith diagonal element is Vt [i] in Algorithm 2. Consequently, ψt(∙)
is 1-strongly convex with respect to the norm ∣∣ ∙ ∣∣ψt :=(J(∙, diag{Vt}1/2^i. Namely, ψt(∙) satisfies ψt(u) ≥ ψt(v) +
(ψt(v), U - Vi + 2∣∣u - v∣ψt for any point u, v. A consequence of 1-strongly convexity of ψt(∙) is that Bψt (u, V) ≥
2 Ilu-VI∣ψ亡,where the Bregman divergence Bψt (u, V) is defined as Bψt (u, V) := ψt (u)-ψt(v)-(ψt(v),u-vi and ψt
is called the distance generating function. We can also define the associate dual norm as ψt (x) := (x, diag{Vt}-1/2Xi.
We assume that the model parameter w is in d-dimensional space. That is, w ∈ Rd. It suffices to analyze Algorithm 6,
which holds for any convex set K. The algorithm reduces to Algorithm 5 when K = Rd .
17
Under review as a conference paper at ICLR 2019
Algorithm 6 OPTIMISTIC-ADAGRAD
1:	Required: parameter ηt.
2:	Init: wι.
3:	for t = 1 to T do
4:	Get current gradient gt at wt.
5:	wt+1 = argminw∈κ ηt {w,gt) + Bψt (W)Wt-1).
6:	wt+ι = arg minw∈κ ηt+ι(w,mt+ι〉+ Bψ (w,Wt+1)
where mt+1 is the guess of gt+1.
7:	end for
By regret decomposition, we have that
Regretτ := ɪ2't(wt) - min f't(w) ≤ fg - w*, V't(wt))
w—w	w∈K
t=1	t=1	t=1
T
EhWt - wt+1, V't(wt) - mt + hwt - wt+1 ,mti + (w^ι - w*, V't(wt)).
t=1
(30)
Now we are going to exploit a useful inequality: for any update of the form W = arg minw∈κ(w, θ) + Bψ (w, v), we
have that
(W — u,θ∖ ≤ Bψ (u, v) — Bψ (u, W) — Bψ (W, v),	(31)
for any U ∈ K. Using the above inequality, we have
hwt - wt+1 ,mti ≤ "(BΨt-1 (wt+1 ,wt-1) - BΨt-1 (wt+2, Wt) - Bψt-ι (Wt,wt-2)),	(32)
and
hwt+2 - w*, Net(Wt)) ≤ —(Bψt (w*,wt-2) - Bψt (w*,wt+2) - Bψt (wt+1 ,wt-1 ))∙	(33)
So, by (30) (32) and (33), we obtain
T
Regretτ ≤ EhWt - Wt+1, V't(wt) - mt) + (wt - Wt+1 ,mt) + (wt+1 - w*, V't(wt))
t=1
T
≤	∣∣wt- wt+1 kΨt-1 kVf(Wt)- mtkψ-1
+η (Bψ-1 (wt+1 ,wt-I) -Bψ-1 (wt+1 ,wt) -Bψ-1 (wt,wt-1)
+Bψt (w*,wt-2)-Bψt (w*,wt+2)-Bψt (wt+2 ,wt-2))
T1	η
≤ X 历∣∣wt - wt+1 llψt-1 + y kVf(Wt)- mt临一
+η (Ba- (wt+2 )wt-2)- 2 ∣∣wt+2 - wt∣ψt-1 -Bψ-1 (wt,wt-2)
+ Bψt(w*,wt-2)- Bψt(w*,wt+ 2) - Bψt(wt+2,wt-2))
T
≤ X η2t∣Vf(wt)- mtk*
t=1
+ η (Bψt (w*,wt-1)-Bψt (w*,wt+2)+BaT (wt+2 ,wt-2)-Bψt (wt+1 ,wt-1)),
where the third inequality is because
„	„	„	,	1 „	119 β∖∖一, 、	119
l∣wt - wt+2 ∣Ψt-11Vf(Wt) - mt∣∣ψ-1 = β% 2βl∣wt - wt+1 llψt-1 + 2lVf(Wt) - mt∣ψ之 1,
(34)
(35)
18
Under review as a conference paper at ICLR 2019
and that ψt-ι(∙) is 1-strongly convex with respect to ∣∣ ∙ kψt-ι. To proceed, notice that, by definition of Bregman
divergence, we also have
BΨt+1 (W*,wt+ 2 ) — Bψt (W*,wt+ 2 ) = hw* — wt+ 2 , diag(St+1 — St)(W* — wt+1 )i
≤ max(w* [i] — wt+1 [i])2kst+1 — st∣1.
and
BΨt-1(Wt+1 ,wt-1) — Bψt(Wt+ 2,wt-2) = hwt+2 — Wt- 11,diag(St-I — St)(Wt+1 — Wt-2)i ≤ 0.
where the vector St ∈ Rd is defined as follows: its ith entry St [i] is
(36)
(37)
St[i] := t
t
X gs [i]2 .
s=1
(38)
Now we have
T
RegretT ≤ X ηtIW(Wt)- mt∣∣ψκ1
t=1
十η (Bψt(W*,wt-1) ― Bψt(W*,wt+1)+Bψt-1(Wt+1 ,wt-1) ― Bψt(Wt+2 ,wt-1 ))
≤ —Bψι (w*,w1∕2) + X η IVf (Wt) - mtkψ*-ι + X — maχ(w*[i] - wt+1 [i])2kst+1 - stk1
η1	t=1 2	t=1 ηt i	2
d
T
≤ —Bψ1 (w*, W1/2) +-------max ∣∣w
η1	1	ηmin t≤T
声—Wt- 2 k∞ X kg1:T [i]∣∣2 + X η ∣∣Vf (wt) — mtkψ 之 1.
i=1
t=1
(39)
Therefore, we conclude the following theorem.
Theorem 3. For any learning rate ηt := η, OPTIMISTIC-ADAGRAD (Algorithm 6) has regret
RegretT ≤ -^Bψ1 (w*,w1∕2) + m m^ ∣∣w*
dT
一 wt-2 k∞ X kgi:T [i]k2+ X 2 INf (Wt)- mt llψ*-1
i=1	t=1
(40)
One need to compare it with the bound of AdaGrad Duchi et al. (2011), which has regret 3
RegretT = O(max ∣W*
d
-wt-2 k∞ X kgl：T [i]k2).
i=1
(41)
We see that when mt predicts Vf (Wt) very well, the last term is dominated by the second one. One can set η of
Optimistic-AdaGrad to be a large step size so that Optimistic-AdaGrad has a smaller regret than AdaGrad.
3The bound here is the result of optimizing over η of ADAGRAD.
19