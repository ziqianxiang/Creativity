Under review as a conference paper at ICLR 2019
Learning Backpropagation-Free Deep Archit-
ectures with Kernels
Anonymous authors
Paper under double-blind review
Ab stract
One can substitute each neuron in any neural network with a kernel machine and
obtain a counterpart powered by kernel machines. The new network inherits the
expressive power and architecture of the original but works in a more intuitive way
since each node enjoys the simple interpretation as a hyperplane (in a reproducing
kernel Hilbert space). Further, using the kernel multilayer perceptron as an example,
we prove that in classification and under certain losses, an optimal representation
that minimizes the risk of the network can be characterized for each hidden layer.
This result removes the need of backpropagation in learning the model and can be
generalized to any feedforward kernel network. Moreover, unlike backpropagation,
which turns models into black boxes, the optimal hidden representation enjoys an
intuitive geometric interpretation, making the dynamics of learning in a deep kernel
network transparent. Empirical results are provided to complement our theory.
1	Introduction
Any neural network (NN) can be turned into a kernel network (KN) by replacing each artificial
neuron (McCulloch & Pitts, 1943), i.e., learning machine of the form f(x) = σ(w>x + b), with
a kernel machine, i.e., learning machine of the form f (x) = hw, φ(x)i + b with kernel function
k(x, y) = hφ(x), φ(y)i. This combination of connectionism and kernel method enables the learning
of hierarchical, distributed representations with kernels.
In terms of training, similar to NN, KN can be trained with backpropagation (BP) (Rumelhart et al.,
1986). In the context of supervised learning, the need for BP in learning a deep architecture is caused
by the fact that there is no explicit target information to tune the hidden layers (Rumelhart et al.,
1986). Moreover, BP is usually computationally intensive and can suffer from vanishing gradient.
And most importantly, BP results in hidden representations that are notoriously difficult to interpret
or assess, turning deep architectures into “black boxes”.
The main theoretical contribution of this paper is the following: Employing the simplest feedforward,
fully-connected KN as an example, we prove that in classification and under certain losses, the
optimal representation for each hidden layer that minimizes the risk of the network can be explicitly
characterized. This result removes the need for BP and makes it possible to train the network in a
feedforward, layer-wise fashion. And the same idea can be generalized to other feedforward KNs.
The layer-wise learning algorithm gives the same optimality guarantee as BP in the sense that it
minimizes the risk. But the former is much faster and evidently less susceptible to vanishing gradient.
Moreover, the quality of learning in the hidden layers can be directly assessed during or after training,
providing more information about the model to the user. For practitioners, this enables completely
new model selection paradigms. For example, the bad performance of the network can now be traced
to a certain layer, allowing the user to debug the layers individually. Most importantly, the optimal
representation for each hidden layer enjoys an intuitive geometric interpretation, making the learning
dynamics in a deep KN more transparent than that in a deep NN. A simple acceleration method
that utilizes the “sparse” nature of the optimal hidden representations is proposed to further reduce
computational complexity.
Empirical results on several computer vision benchmarks are provided to demonstrate the competence
of the model and the effectiveness of the greedy learning algorithm.
1
Under review as a conference paper at ICLR 2019
(a)
Figure 1: (a) Any NN (left, presented in the usual weight-nonlinearity abstraction) can be abstracted
as a “graph” (right) with each node representing a neuron and each edge the input-output rela-
tionship between neurons. If a node receives multiple inputs, we view its input as a vector in
some Euclidean space, as indicated by the colored rectangles. Under this abstraction, each neuron
(f (x) = σ(w>x + b)) can be directly replaced by a kernel machine (f (x) = hw, φ(x)i + b with
kernel k(x, y) = hφ(x), φ(y)i) mapping from the same Euclidean space into the real line without
altering the architecture and functionality of the model.
(b) Illustration for layer-wise optimality drifting away from network-optimality. Consider a two-layer
network and let T1 , T2 be the target function of the first and second layer, respectively. If the first
layer creates error, which is illustrated by F (1) (x) being far away from T1(x), the composed solution
F(2) ◦ F(1) on the right is better than that on the left and hence the F(2) on the right corresponds to
the network-wise optimality of the second layer. But the F(2) on the left is clearly a better estimate
to the layer-wise optimality T2 if the quality of estimation is measured by the supremum distance.
(b)
2	From Neural to Kernel Networks
In this section, we discuss how to build a KN using a given NN. First, the generic approach is described
in Fig. 1a. Note that KN inherits the expressive power of the original NN since a kernel machine is
a universal function approximator under mild conditions (Park & Sandberg, 1991; Micchelli et al.,
2006) and the two models share the same architecture. However, KN works in a more intuitive way
since each node is a simple linear model in a reproducing kernel Hilbert space (RKHS).
We now concretely define the KN equivalent of an l-layer Multilayer Perceptron (MLP), which we
shall refer to as the kernel MLP (kMLP).1 Given a random sample (xn, yn)nN=1, where (xn, yn) ∈
X1 × Y ⊂ Rd0 × R, denote (xn)nN=1 as S and (yn)nN=1 as YS for convenience. For i = 1, 2, . . . , l,
consider kernel k(i) : Xi × Xi → R, Xi ⊂ Rdi-1 (for i > 1, di-1 is determined by the width of the
i - 1th layer).	k(i)(x,	y)	=	φ(i)(x),	φ(i)(y)H , where	φ(i)	is a mapping into RKHS	Hi.
For i ≥ 1, the ith layer in a kMLP, denoted F (i), is an array of di kernel machines: F(i) : Xi →
Rdi, F(i) = (f1(i), f2(i), . . . , fd(i)), a di-tuple. Let F(0) be the identity map on Rd0, each fj(i) : Xi →
R is a hyperplane in Hi: fj(i) (x) = Dwji),φ⑴(FUT) ◦…O F(O) (X))EH + bj", Wji) ∈ Hi, bji) ∈ R.
In practice, f^(x) is usually implemented as PN=I α*k(i) (F(CT) ◦…O F(O) (x), F(CT) ◦…O
F(0) (xn )) + b(ji), where the α(nij) , b(ji) ∈ R are the learnable parameters.2 The set of mappings
{F(') ◦…O F(I) : α(j, bji) ∈ R for all admissible n,j, i} defines an l-layer kMLP. In the rest of
this paper, we shall restrict our discussions to this kMLP.
3	Assumptions and Notations
We now specify the assumptions that we impose on all kernels considered in this paper. First, we
consider real, continuous, symmetric kernels only and we call a kernel positive semidefinite (PSD)
or positive definite (PD) if for any S, the kernel matrix defined as (G)mn = k (xm , xn ) is PSD
or PD, respectively. We shall always assume that any kernel considered is at least PSD and that
1A PyTorch-based (Paszke et al., 2017) library for implementing kMLP and the proposed layer-wise learning
algorithm is available at: anonymized URL.
2The optimality of this expansion will be later justified in Section 4 under the layer-wise setting using
representer theorem (SchOlkopf et al., 2001).
2
Under review as a conference paper at ICLR 2019
k(i) (x, x) = c < +∞ for all x ∈ Xi and inf x,y∈Xi k(i) (x, y) = a > -∞. It is straightforward to
check using Cauchy-Schwarz inequality that the first condition implies maxx,y∈Xi k(i) (x, y) = c.
For each fixed x ∈ Xi, we assume that k(i)(x, y), as a function of y, is L(xi)-Lipschitz with respect to
the Euclidean metric on Xi. Let supx∈Xi L(xi) = L(i), which we assume to be finite.
The following notations will be used whenever convenient: We use the shorthand F (1) (S) for
(F ⑴(Xn))N=ι. For i = 2,3,...,l, F (i)(S) := F⑴。FCT) ◦.・.◦ F (I)(S) and the Same is true
with S substituted by any x. Throughout this paper, notations such as F(i) can either be used to
denote a set of functions or a specific function in some set depending on the context. Also, when
there is no confusion, we shall suppress the dependency of any loss function on the example for
brevity, i.e., for a loss function `, instead of writing `(f (x), y), we shall write `(f).
4	A Layer-wise Learning Algorithm
To simplify discussion, we shall restrict ourselves to binary classification (Y = {+1, -1}) and
directly give the result on classification with more than two classes in the end. A generalization to
regression is left as future work. Again, we only focus on kMLP although the idea can be directly
generalized to all feedforward KNs. We now discuss the layer-wise learning algorithm, beginning by
addressing the difficulties with training a deep architecture layer-by-layer.
4.1	Fundamental Difficulties
There are two fundamental difficulties with learning a deep architecture layer-wise. First, the hidden
layers do not have supervision (labels) to learn from. And it depends on BP to propagate supervision
from the output backward (Rumelhart et al., 1986). We shall prove that for kMLP, one can characterize
the optimal target representation for each hidden layer, which induces a risk for that layer. The target
is optimal in the sense that it minimizes the risk of the subsequent layer and eventually that of the
network if all layers succeed in learning their optimal representations. This optimal representation
defines what we call “layer-wise optimality”.
The other difficulty with layer-wise learning is that for any given hidden layer, when the upstream
layers create error, layer-wise optimality may not coincide with “network-wise optimality”, i.e., the
solution of this layer that eventually leads to the composed solution that minimizes the risk of the
network in this suboptimal case. Indeed, when a hidden layer creates error, the objective of any layer
after it becomes learning a solution that is a compromise between one that is close to the layer-wise
optimality and one that prevents the error from the “bad” layer before it from “getting through” easily.
And the best compromise is the network-wise optimality. The two solutions may not coincide, as
shown in the toy example in Fig. 1b. Clearly, we would like to always learn the network-wise
optimality at each layer, but the learner is blind to it if it is only allowed to work on one layer at a
time. By decomposing the overall error of the network into error at each layer, we prove that in fact,
network-wise optimality is learnable for each hidden layer even in a purely layer-wise fashion and
that the proposed layer-wise algorithm learns network-wise optimality at each layer.
4.2	The Optimal Hidden Representations
We now address the first difficulty in layer-wise learning. The basic idea is first described in Section
4.2.1. Then we provide technical results in Section 4.2.2 and Section 4.2.3 to fill in the details.
4.2.1	Basic Idea
Given a deep architecture F := F(I) ◦•••◦ F(I) and a loss function 'ι defined for this network which
induces a risk that We wish to minimize: Rl = E ' (F). BP views this problem in the following way:
Rl is a function of F. The learner tries to find an F that minimizes Rl using the random sample S
with labels YS according to some learning paradigm such as Empirical Risk Minimization (ERM)
or Structural Risk Minimization (SRM) (Vapnik, 2000; Shalev-Shwartz & Ben-David, 2014). S is
considered as fixed in the sense that it cannot be adjusted by the learner.
3
Under review as a conference paper at ICLR 2019
Alternatively, one can view Rl as a function of F(l) and the learner tries to find an F(l) minimizing
Rl using random sample Sl-1 with labels YS according to some learning paradigm, where Sl-1 :=
F (IT)◦…oF (1)(S)3. The advantage is that the learner has the freedom to learn both the function F(l)
and the random sample Sl-1. And since Sl-1 determines the decision of the learning paradigm, which
then determines Rl, Rl is now essentially a function of both F(I) and Si-i： Rl = E 'ι(F(l), S1-1).
The key result is that independently of the actual learning of F(l), one can characterize the sufficient
condition on Sl-1 under which Rl, as a function of Sl-1, is minimized, as we shall prove. In other
words, the “global minimum” of Rl w.r.t. Sl-1 can be explicitly identified prior to any training. This
gives the optimal Sl-1, which we denote as Sl?-1.
Moreover, the characterization of Sl?-1 gives rise to a new loss function `l-1 and thus also a new
risk Rl-ι that is a function of F(l-1) ◦•••◦ F(1). Consequently, the same reasoning would allow
us to deduce Sl?-2 before the learner learns F (l-1). And this analysis can be applied to each layer,
eventually leading to a greedy learning algorithmthat sequentially learns F ⑴,F ⑵◦F (1)",...,F(I) ◦
F(l-1)*Q…。F⑴*, in that order, where the asterisk on the superscript indicates that the corresponding
layer has been learned and frozen.
The layer-wise learning algorithm provides a framework that enjoys great flexibility. To be specific,
one could stop the above analysis at any layer i, then learn layers i + 1, . . . , l in a greedy fashion but
still learn layers 1, . . . , i together with BP. Thus, it is easy to see that BP can be brought under this
framework as a special case. Nevertheless, in later text, we shall stay on the one end of the spectrum
where each layer is learned individually for clarity.
We now present the formal results that give the optimal hidden representations. By the reasoning
above, the analysis starts from the last hidden layer (layer l - 1) and proceeds backward.
4.2.2	FORMAL RESULTS： Sl?-1
To begin with, we need to approximate the true classification error Rl since it is not computable. To
this end, we first review a well-known complexity measure.
Definition 4.1 (Gaussian complexity (Bartlett & Mendelson, 2002)). Let P be a probability distribu-
tion on a metric space X and suppose x1 , . . . , xN are independent random elements distributed as
P. Let F be a set of functions mapping from X into R. Define
ʌ
GN (F) = E
sup
f∈F
2N
N〉： gnf (Xi)
n=1
x1 , . . . , xN
where g1 , . . . , gN are independent standard normal random variables. The Gaussian complexity of
ʌ
F is Gn (F) = E GN (F)
Intuitively, Gaussian complexity quantifies how well elements in a given function class can be
correlated with a noise sequence of length N, i.e., the gn (Bartlett & Mendelson, 2002). Based on
this complexity measure, we have the following bound on the expected classification error.
Theorem 4.2. (Bartlett & Mendelson, 2002) For each F mapping S into Xl-1, let Fl,A
f:
x 7→
(W,Φ(X)(F(x)))H + b∣kwkHl ≤ A,b ∈ R}・
Fix A, γ > 0, with probability at least 1 - δ
and for any N ∈ N, every function f(l) in Fl,A satisfies
P(yf(I)(X) ≤ 0) ≤ RI(f(Il)+ 2Gn(Fl,A)
log(4∕δ)
2N
where Rl (f (I)) = N PN=I max(0,1 — yuf (I)(Xn)∕γ) ,the empirical hinge loss.
Given the assumptions on k(l), for any F, we have
GN (Fl,A) ≤ 2A
3This is in fact a set of random samples asF(IT) ◦・・.◦ F⑴ isa set of functions.
4
Under review as a conference paper at ICLR 2019
Without loss of generality, we shall set hyperparameter γ = 1. We now characterize Sl?-1. Note that
for a given f(l), A = wf(l) H is the smallest nonnegative real number such that f(l) ∈ Fl,A and it
is immediate that this gives the tightest bound in Theorem 4.2. Let κ = ~N PN=I 1{yn=+}.
Lemma 4.3 (optimal Sl-1). Given a learning paradigm minimizing Rl (f (I)) + T IIWf ⑴ IIH using
representation Sl-ι = F (S), where T is any positive constant satisfying T <，2(C — a) min(κ, 1 一
κ). Denote as Sl?-1 any representation satisfying
k(l)(F (x+), F (x-)) = a and k(l) (F(x), F(x0)) = C	(1)
for all pairs of x+, x- from distinct classes in S and all pairs of x, x0 from the same class. Suppose
the learning paradigm returns f(l)? under this representation. Let S1]be another representation
under which the learning paradigm returns f(l)o. If f(l)o achieves zero hinge loss on at least one
examplefrom each class, thenforany N ∈ N, Rl(f (l)?) + T∣∣Wf(1)? ||可 ≤ Rl(f(l)o) + T∣∣Wf⑴。||氏.
The optimal representation Sl?-1, characterized by Eq. 1, enjoys a straightforward geometric inter-
pretation: Examples from distinct classes are as distant as possible in the RKHS whereas examples
from the same class are as concentrated as possible (see proof (C) of Lemma 4.3 for a rigorous
justification). Intuitively, it is easy to see that such a representation is the “easiest” for the classifier.
The conditions in Eq. 1 can be concisely summarized in an ideal kernel matrix G? defined as
(G )mn = a, if ym 6= yn ;
(G )mn = C, if ym = yn .
And to have the l 一 1th layer learn Sl?-1, it suffices to train it to minimize some dissimilarity measure
between G? and the kernel matrix computed from k(l) and F (l-1)(S), which we denote Gl-1.
Empirical alignment (Cristianini et al., 2002), L1 and L2 distances between matrices can all serve as
the dissimilarity measure. To simplify discussion, we let the dissimilarity measure be the L1 distance
'l-l(F(l-1), (Xm, ym), (Xn, Jn)) = |(G?)mn —(Gl-l)mn∣.
This specifies Rl-I(F(IT)) as the sample mean of ('l-ι(F(l-1), (χm,ym), (Xn,Jn)))N,n=ι and
Rl-1 as the expectation of `l-1 over (X1, Y )×(X1, Y ). Note that due to the boundedness assumption
on k(l), `l-1 ≤ 2 max(|C|, |a|).
4.2.3 FORMAL RESULTS: Sl?-2, . . . , S1?
Similar to Section 4.2.2, we first need to approximate Rl-1.
Lemma 4.4. For j = 1, 2, . . . , dl-1, let fj(l-1) ∈ Fl-1, where Fl-1 is a given hypothesis class.
There exists an absolute constant C > 0 such that for any N ∈ N, with probability at least 1 一 δ,
Rl-ι(F(IT)) ≤ Rl-I(F(IT)) +
4L(l)Cdl-ι	/8log(2∕δ)
max(∣c∣,∣α∣) GN(FlT) + V N
We are now in a position to characterize Sl?-2 . For the following lemma only, we further assume
that k(l-1)(x, y), as a function of (x, y), depends only on and strictly decreases in kx 一 yk2 for all
x, y ∈ Xl-1 with k(l-1) (x, y) > a, and that the infimum infx,y∈Xl-1 k(l-1) (x, y) = ais attained in
Xl-1 at all x,y With Ilx - y∣∣2 ≥ η. Also assume that inf χ,y∈X1fkχ-yk2<η |dk(lT)(X,y)∕∂∣∣x 一
y k2 = ι(l-1) is defined and is positive.
Consider an F mapping S into Xl-ι, let Fl-ι,A = {f : x → (w,φ(lT)(F(x)))	+
b IwIHl-1 ≤ A, b ∈ Ro. For a given F (l-1) = f1(l-1), . . . , fd(l-1), it is immediate that
A = maxj ∣∣wf(l-1) ∣∣	is the smallest nonnegative real number such that fj(l-1) ∈ Fl-1,A for all
j, giving the tightest bound in Lemma 4.4 (recall the bound on GN(Fl-1,A) in Theorem 4.2). Let
ψ = PNm,n=1 1{ym6=yn}∕N2.
5
Under review as a conference paper at ICLR 2019
Lemma 4.5 (optimal S1-2). Given a learning paradigm minimizing Rl-I(F(I-I)) +
using representation Sl-2
F(S), where τ is any positive constant sat-
τ maxj	wfj(l-1) Hl-1
isfying T < /2dl-ι(c - a)ψι(l-1). Denote as S：-? any representation satisfying
k (l-1) (F (x+), F(x-)) = a and k(l-1)(F(x), F(x0)) = c
for all pairs of x+, x- from distinct classes in S and all pairs of x, x0 from the same class. Suppose
the learning paradigm returns F (l-1)? = f1(l-1)?, . . . , fd(l-1)? under this representation. Let
Sl-2 be another representation under which the learning paradigm returns F(l-1)°. If F(l-1)°
achieves zero loss on at least one pair of examples from distinct classes, then for any N ∈ N,
RI-I(F(f*) + τ maxj wf (l-1)? U ≤ RI-I(F(I-I)CI) + τ maxj ∣∣Wf (iτ)° U	.
Applying this analysis to the rest of the hidden layers, it is evident that the ith layer, i = 1, 2, ..., l - 1,
should be trained to minimize the difference between G? and the kernel matrix computed with k(i+1)
and F(i) (S), denoted Gi. Generalizing to classification with more than two classes requires no
change to the algorithm since the definition of G? is agnostic to the number of classes involved in
the classification task. Also note that the sufficiency of expanding the kernel machines of each layer
on the training sample (see Section 2) for the learning objectives in Lemma 4.3 and Lemma 4.5 is
trivially justified since the generalized representer theorem directly applies (Scholkopf et al., 2001).
Now since the optimal representation is consistent across layers, the dynamics of layer-wise learning
in a kMLP is clear: The network maps the random sample sequentially through layers, with each
layer trying to map examples from distinct classes as far as possible in the RKHS while keeping
examples from the same class in a cluster as concentrated as possible. In other words, each layer
learns a more separable representation of the sample. Eventually, the output layer works as a classifier
on the final representation and since the representation would be “simple” after the mappings of the
lower layers, the learned decision boundary would generalize better to unseen data, as suggested by
the bounds above.
4.3	Learning Network-wise Optimality
We now discuss how to design a layer-wise learning algorithm that learns network-wise optimality
at each layer. A rigorous description of the problem of layer-wise optimality drifting away from
network-wise optimality and the search for a solution begins with the following bound on the total
error of any two consecutive layers in a kMLP.
Lemma 4.6. Forany i = 2,..., l, let the targetfunction and the approximation function be Ti, F(i)* :
Xi → Rdi, respectively. Let Ei = ∣∣F(i)* —黑|[§ ：= suPχ∈χ- ∣∣FG)*(x) — TXx)U?, we have
* ◦ F(i-1)* - Ti ◦ Ti-ι∣∣
≤ q + dt 2L(i)
s
di	2
XUwfyUH
j=1
(2)
By applying the above bound sequentially from the input layer to the output, we can decompose the
error of an arbitrary kMLP into the error of the layers. This result gives a formal description of the
problem: The hypothesis with the minimal norm minimizes the propagated error from upstream, but
evidently, this hypothesis is not necessarily close to the layer-wise optimality Ti .
Moreover, this bound provides the insight needed for learning network-wise optimality individually
at each layer: For the ith layer, i ≥ 2, searching for network-wise optimality amounts to minimizing
the r.h.s. of Eq. 2. Lemma 4.3 and Lemma 4.5 characterized Ti for i < l and learning objectives that
bound Ei were provided earlier in the text accordingly. Based on those results, the solution that mini-
mizes the new learning objective Ri(F(i)) + T0 maxj ∣∣wf⑸ ∣∣ , where T0 > 0 is a hyperparameter,
provides a good approximate to the minimizer of the r.h.s. of Eq. 2 if, of course, τ0 is chosen well.
Thus, taking this as the learning objective of the ith layer produces a layer-wise algorithm that learns
network-wise optimality at this layer. Note that for BP, one usually also needs to heuristically tune
the regularization coefficient for weights as a hyperparameter.
6
Under review as a conference paper at ICLR 2019
4.4	Accelerating the Upper Layers
There is a natural method to accelerate the upper layers (all but the input layer): The optimal
representation F(S) is sparse in the sense that φ(F (xm)) = φ(F (xn)) if ym = yn and φ(F (xm)) 6=
φ(F (xn)) if ym 6= yn (see the proof (C) of Lemma 4.3). Since a kernel machine built on this
representation of the given sample is a function in the RKHS that is contained in the span of the
image of the sample, retaining only one example from each class would result in exactly the same
hypothesis class because trivially, We have {PnN=ι ɑnφ(F(Xn))∣αn ∈ R} = {α+φ(F(x+)) +
α-φ(F(x-))∣α+,a- ∈ R} for arbitrary x+,x- in S when the representation F(S) is optimal.
Thus, after training a given layer, depending on hoW close the actual kernel matrix is to the ideal one,
one can (even randomly) discard a large portion of centers for kernel machines of the next layer to
speed up the training of it without sacrificing performance. As we will later show in the experiments,
randomly keeping a fraction of the training sample as centers for upper layers produces performance
comparable to or better than that obtained with using the entire training set.
5	Related Works
The idea of combining connectionism with kernel method was initiated by Cho & Saul (2009). In
their work, an “arc cosine” kernel was so defined as to imitate the computations performed by a
one-layer MLP. Zhuang et al. (2011) extended the idea to arbitrary kernels with a focus on MKL,
using an architecture similar to a two-layer kMLP. As a further generalization, Zhang et al. (2017)
independently proposed kMLP and the KN equivalent of CNN. However, they did not extend the idea
to any arbitrary NN. Scardapane et al. (2017) proposed to reparameterize each nonlinearity in an NN
with a kernel expansion, resulting in a network similar to KN but is trained with BP. There are other
works aiming at building “deep” kernels using approaches that are different in spirit from those above.
Wilson et al. (2016) proposed to learn the covariance matrix of a Gaussian process using an NN in
order to make the kernel “adaptive”. This idea also underlies the now standard approach of combining
a deep NN with an SVM for classification, which was first explored by Huang & LeCun (2006) and
Tang (2013). Such an interpretation can be given to KNs as well, as we point out in Appendix B.5.
Mairal et al. (2014) proposed to learn hierarchical representations by learning mappings of kernels
that are invariant to irrelevant variations in images.
Much works have been done to improve or substitute BP in learning a deep architecture. Most aim
at improving the classical method, working as add-ons for BP. The most notable ones are perhaps
the unsupervised greedy pre-training techniques proposed by Hinton et al. (2006) and Bengio et al.
(2007). Among works that try to completely substitute BP, none provided a comparable optimality
guarantee in theory as that given by BP. Fahlman & Lebiere (1990) pioneered the idea of greedily
learn the architecture of an NN. In their work, each new node is added to maximize the correlation
between its output and the residual error signal. Several authors explored the idea of approximating
error signals propagated by BP locally at each layer or each node (Bengio, 2014; Carreira-Perpinan
& Wang, 2014; Lee et al., 2015; Balduzzi et al., 2015; Jaderberg et al., 2016). Kulkarni & Karande
(2017) proposed to train NN layer-wise using an ideal kernel matrix that is a special case of that in
our work. No theoretical results were provided to justify its optimality for NN. Zhou & Feng (2017)
proposed a BP-free deep architecture based on decision trees, but the idea is very different from ours.
Raghu et al. (2017) attempted to quantify the quality of hidden representations toward learning more
interpretable deep architectures, sharing a motivation similar to ours.
6	Experiments
We compared kMLP learned using the proposed greedy algorithm with other popular deep archi-
tectures including MLP, Deep Belief Network (DBN) (Hinton & Salakhutdinov, 2006) and Stacked
Autoencoder (SAE) (Vincent et al., 2010), with the last two trained using a combination of un-
supervised greedy pre-training and standard BP (Hinton et al., 2006; Bengio et al., 2007). Note
that we only focused on comparing with the standard, generic architectures because kMLP, as the
KN equivalent of MLP, does not have a specialized architecture or features designed for specific
application domains. Several optimization and training techniques were applied to the MLPs to
boost performance. These include Adam (Kingma & Ba, 2014), RMSProp (Tieleman & Hinton,
2012), dropout (Srivastava et al., 2014) and batch normalization (BN) (Ioffe & Szegedy, 2015).
7
Under review as a conference paper at ICLR 2019
Table 1: Test errors (%) and 95% confidence intervals (%). When two results have overlapping
confidence intervals, they are considered equivalent. Best results are marked in bold. The numbers
following the model names indicate the number of hidden layers used. For kMLPFAST, we also include
(in parentheses) the portion of centers retained, i.e., the number of training examples randomly chosen
as centers for the given layer divided by the size of the training set.
(a) Comparing kMLPs (trained layer-wise) with other popular deep architectures trained with BP and BP
enhanced by unsupervised greedy pre-training.
	RECTANGLES	RECTANGLES-IMAGE	CONVEX	MNIST (10K)	MNIST (10K) ROTATED	FASHION-MNIST
MLP-1 (SGD)	7.16± 0.23	33.20± 0.41	32.25± 0.41	4.69± 0.19	18.11± 0.34	15.47± 0.71
MLP-1 (ADAM)	5.37± 0.20	28.82± 0.40	30.07± 0.40	4.71± 0.19	18.64± 0.34	12.98± 0.66
MLP-1 (RMSPROP+BN)	5.37± 0.20	23.81± 0.37	28.60± 0.40	4.57± 0.18	18.75± 0.34	14.55± 0.69
MLP-1 (RMSPROP+DROPOUT)	5.50± 0.20	23.67± 0.37	36.28± 0.42	4.31± 0.18	14.96± 0.31	12.86± 0.66
MLP-2 (SGD)	5.05± 0.19	22.77± 0.37	25.93± 0.38	5.17± 0.19	18.08± 0.34	12.94± 0.66
MLP-2 (ADAM)	4.36± 0.18	25.69± 0.38	25.68± 0.38	4.42± 0.18	17.22± 0.33	11.48± 0.62
MLP-2 (RMSPROP+BN)	4.22± 0.18	23.12± 0.37	23.28± 0.37	3.57± 0.16	13.73± 0.30	11.51± 0.63
MLP-2 (RMSPROP+DROPOUT)	4.75± 0.19	23.24± 0.37	34.73± 0.42	3.95± 0.17	13.57± 0.30	11.05± 0.61
DBN-1	4.71± 0.19	23.69± 0.37	19.92± 0.35	3.94± 0.17	14.69± 0.31	N/A
DBN-3	2.60± 0.14	22.50± 0.37	18.63± 0.34	3.11± 0.15	10.30± 0.27	N/A
SAE-3	2.41± 0.13	24.05± 0.37	18.41± 0.34	3.46± 0.16	10.30± 0.27	N/A
KMLP-1	2.24± 0.13	23.29± 0.37	19.15± 0.34	3.10± 0.15	11.09± 0.28	11.72± 0.63
KMLP-1FAST	2.36± 0.13 (0.05)	23.86± 0.37 (0.01)	20.34± 0.35 (0.17)	2.95± 0.15 (0.1)	12.61± 0.29 (0.1)	11.45± 0.62 (0.28)
KMLP-2	2.24± 0.13	23.30± 0.37	18.53± 0.34	3.16± 0.15	10.53± 0.27	11.25± 0.62
KMLP-2FAST	2.21± 0.13 (0.3/0.3)	23.24± 0.37 (0.01/0.3)	19.32± 0.35 (0.005/0.03)	3.18± 0.15 (0.3/0.3)	10.94± 0.27 (0.1/0.7)	11.15± 0.62 (1/0.28)
(b) Further testing the proposed layer-wise learning algorithm and acceleration method using standard MNIST.
MLP-1 (RMSProp+BN)	MLP-1 (RMSProp+Dropout)	MLP-2 (RMSProp+BN)	MLP-2 (RMSProp+Dropout)	kMLP-1 (BP)	kMLP-1 (GREEDY)	KMLP-1RFF (BP)
2.05± 0.28	1.77± 0.26	1.58± 0.24	1.67± 0.25	3.44± 0.36	1.77± 0.26	2.01± 0.28
KMLP- 1PARAM (BP)	KMLP- 1FAST (GREEDY)	kMLP-2 (BP)	KMLP-2 (GREEDY)	KMLP-2RFF (BP)	KMLP-2PARAM (BP)	KMLP-2FAST (GREEDY)
1.88± 0.27	1.75± 0.26 (0.54)	3.66± 0.37	1.56± 0.24	1.92± 0.27	2.45± 0.30	1.47± 0.24 (1/0.19)
kMLP accelerated using the proposed method (kMLPFAST) was also compared. For these models,
we randomly retained a subset of the centers of each upper layer before its training. As for the
benchmarks used, rectangles, rectangles-image and convex are binary classification datasets, mnist
(10k) and mnist (10k) rotated are variants of MNIST (Larochelle et al., 2007; LeCun et al., 2010).
And fashion-mnist is the Fashion-MNIST dataset (Xiao et al., 2017). To further test the proposed
layer-wise learning algorithm and the acceleration method, we compared greedily-trained kMLP with
MLP and kMLP trained using BP (Zhang et al., 2017) using the standard MNIST (LeCun et al., 2010).
Two popular acceleration methods for kernel machines were also compared on the same benchmark,
including using a parametric representation (i.e., for each node in a kMLP, f (x) = k(w, x), w
learnable) (kMLPPARAM) and using random Fourier features (kMLPRFF) (Rahimi & Recht, 2008).
More details for the experiments can be found in Appendix A4.
From Table 1a, we see that the performance of kMLP is on par with some of the most popular and
most mature deep architectures. In particular, the greedily-trained kMLPs compared favorably with
their direct NN equivalents, i.e., the MLPs, even though neither batch normalization nor dropout
was used for the former. These results also validate our earlier theoretical results on the layer-wise
learning algorithm, showing that it indeed has the potential to be a substitute for BP with an equivalent
optimality guarantee. Results in Table 1b further demonstrate the effectiveness of the greedy learning
scheme. For both the single-hidden-layer and the two-hidden-layer kMLPs, the layer-wise algorithm
consistently outperformed BP. It is worth noting that the proposed acceleration trick, despite being
extremely simple, is clearly very effective and even produced models outperforming the original ones.
This shows that kMLP together with the greedy learning scheme can be of practical interest even
when dealing with the massive data sets in today’s machine learning.
Last but not least, we argue that it is the practical aspects that makes the greedy learning framework
promising. Namely, this framework of learning makes deep architectures more transparent and
intuitive, which can serve as a tentative step toward more interpretable, easy-to-understand models
with strong expressive power. Also, new design paradigms are now possible under the layer-wise
framework. For example, each layer can now be “debugged” individually. Moreover, since learning
becomes increasingly simple for the upper layers as the representations become more and more
well-behaved, these layers are usually very easy to set up and also converge very fast during training.
4Scripts for all experiments performed in this paper are available at: anonymized URL.
8
Under review as a conference paper at ICLR 2019
References
N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society,
68(3):337-404,1950.
F. R. Bach, G. R. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo
algorithm. In Proceedings of the twenty-first international conference on Machine learning, pp. 6.
ACM, 2004.
David Balduzzi, Hastagiri Vanchinathan, and Joachim M Buhmann. Kickback cuts backprop’s
red-tape: Biologically plausible credit assignment in neural networks. In AAAI, pp. 485-491, 2015.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Y	. Bengio. Learning deep architectures for ai. Foundations and trendsR in Machine Learning, 2(1):
1-127, 2009.
Y	. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks.
In Advances in neural information processing systems, pp. 153-160, 2007.
Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target
propagation. arXiv preprint arXiv:1407.7906, 2014.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In
Artificial Intelligence and Statistics, pp. 10-19, 2014.
Y	. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in neural information
processing systems, pp. 342-350, 2009.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273-297,
1995.
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola. On kernel-target alignment.
In Advances in neural information processing systems, pp. 367-373, 2002.
Scott E Fahlman and Christian Lebiere. The cascade-correlation learning architecture. In Advances
in neural information processing systems, pp. 524-532, 1990.
P. Gehler and S. Nowozin. Infinite kernel learning. 2008.
M. Gonen and E. AlPaydin. Multiple kernel learning algorithms. Journal of machine learning
research, 12(Jul):2211-2268, 2011.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
science, 313(5786):504-507, 2006.
G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural
computation, 18(7):1527-1554, 2006.
Geoffrey E Hinton. Distributed representations. 1984.
Fu Jie Huang and Yann LeCun. Large-scale learning with svm and convolutional for generic
object categorization. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society
Conference on, volume 1, pp. 284-291. IEEE, 2006.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv
preprint arXiv:1608.05343, 2016.
9
Under review as a conference paper at ICLR 2019
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. Lp-norm multiple kernel learning. Journal of
Machine Learning Research,12(Mar):953-997, 2011.
Mandar Kulkarni and Shirish Karande. Layer-wise training of deep networks using kernel similarity.
arXiv preprint arXiv:1703.07115, 2017.
G.	R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semidefinite programming. Journal of Machine learning research, 5(Jan):27-72, 2004.
H.	Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep
architectures on problems with many factors of variation. In Proceedings of the 24th international
conference on Machine learning, pp. 473-480. ACM, 2007.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online].
Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
In Joint european conference on machine learning and knowledge discovery in databases, pp.
498-515. Springer, 2015.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
In Advances in neural information processing systems, pp. 2627-2635, 2014.
Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.
The bulletin of mathematical biophysics, 5(4):115-133, 1943.
C.	A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. Journal of Machine Learning Research, 7
(Dec):2651-2667, 2006.
J. Park and I. W. Sandberg. Universal approximation using radial-basis-function networks. Neural
computation, 3(2):246-257, 1991.
Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. Pytorch: Tensors and dynamic
neural networks in python with strong gpu acceleration, 2017.
Gilles Pisier. The volume of convex bodies and Banach space geometry, volume 94. Cambridge
University Press, 1999.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In Advances in
Neural Information Processing Systems, pp. 6076-6085, 2017.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
D.	E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. Nature, 323(6088):533-538, 1986.
Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, and Aurelio Uncini. Kafnets: kernel-
based non-parametric activation functions for neural networks. arXiv preprint arXiv:1707.04035,
2017.
B. SchOlkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem. In Computational
learning theory, pp. 416-426. Springer, 2001.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
10
Under review as a conference paper at ICLR 2019
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
LearningResearch,15(1):1929-1958, 2014.
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural
networks: A theoretical view. In AAAI, pp. 2066-2072, 2016.
Yichuan Tang. Deep learning using linear support vector machines. arXiv preprint arXiv:1306.0239,
2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine
learning. University of Toronto, Technical Report, 2012.
Vladimir Vapnik. The nature of statistical learning theory. 2000.
M. Varma and B. R. Babu. More generality in efficient multiple kernel learning. In Proceedings of
the 26th Annual International Conference on Machine Learning, pp. 1065-1072. ACM, 2009.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. Journal of
Machine Learning Research, 11(Dec):3371-3408, 2010.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial Intelligence and Statistics, pp. 370-378, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Z. Xu, R. Jin, I. King, and M. Lyu. An extended level method for efficient multiple kernel learning.
In Advances in neural information processing systems, pp. 1825-1832, 2009.
Shuai Zhang, Jianxin Li, Pengtao Xie, Yingchun Zhang, Minglai Shao, Haoyi Zhou, and Mengyi
Yan. Stacked kernel network. arXiv preprint arXiv:1711.09219, 2017.
Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. arXiv
preprint arXiv:1702.08835, 2017.
J. Zhuang, I. W. Tsang, and S. C. Hoi. Two-layer multiple kernel learning. In Proceedings of the
Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 909-917, 2011.
11
Under review as a conference paper at ICLR 2019
Appendix A	Experimental Setup
The first data set, known as rectangles, has 1000 training images, 200 validation images and 50000
test images. The learning machine is required to tell if a rectangle contained in an image has a larger
width or length. The location of the rectangle is random. The border of the rectangle has pixel value
255 and pixels in the rest of an image all have value 0. The second data set, rectangles-image, is the
same with rectangles except that the inside and outside of the rectangle are replaced by an image
patch, respectively. rectangles-image has 10000 training images, 2000 validation images and 50000
test images. The third data set, convex, consists of images in which there are white regions (pixel
value 255) on black (pixel value 0) background. The learning machine needs to distinguish if the
region is convex. This data set has 6000 training images, 2000 validation images and 50000 test
images. The fourth data set contains 10000 training images, 2000 validation images and 50000
test images taken from MNIST. The fifth is the same as the fourth except that the digits have been
randomly rotated. Sample images from the data sets are given in Fig. 2. For actual training and
testing, the pixel values were normalized to [0, 1]. For standard MNIST and Fashion-MNIST, no
preprocessing was performed. For detailed descriptions of the data sets, see (Larochelle et al., 2007;
LeCun et al., 2010; Xiao et al., 2017).

Figure 2: From left to right: example from rectangles, rectangles-image, convex, mnist (10k) and
mnist (10k) rotated.
The experimental setup for the greedily-trained kMLPs is as follows, kMLP-1 corresponds to a
one-hidden-layer kMLP with the first layer consisting of 15 to 150 kernel machines using the same
Gaussian kernel (k(x,y) = e-kx-yk。。2) and the second layer being a single or ten (depending
on the number of classes) kernel machines using another Gaussian kernel. Note that the Gaussian
kernel does not satisfy the condition that the infimum a is attained (see the extra assumptions before
Lemma 4.5), but for practical purposes, it suffices to set the corresponding entries of the ideal kernel
matrix to some small value. For all of our experiments, we set (G?)mn = 1 if ym = yn and 0
otherwise. Hyperparameters were selected using the validation set. The validation set was then
used in final training only for early-stopping based on validation error. For the standard MNIST and
Fashion-MNIST, the last 5000 training examples were held out as validation set. For other datasets,
see (Larochelle et al., 2007). kMLP-1FAST is the same kMLP for which we accelerated by randomly
choosing a fraction of the training set as centers for the second layer after the first had been trained.
The kMLP-2 and kMLP-2FAST are the two-hidden-layer kMLPs, the second hidden layers of which
contained 15 to 150 kernel machines. We used Adam (Kingma & Ba, 2014) as the optimization
algorithm of the layer-wise scheme. Although some of the theoretical results presented earlier in
the paper were proved under certain losses, we did not notice a significant performance difference
between using L1 , L2 and empirical alignment as loss function for the hidden layers. And neither
was such difference observed between using hinge loss and cross-entropy for the output layer. This
suggests that these results may be proved in more general settings. To make a fair comparison with
the NN models, the overall loss functions of all models were chosen to be the cross-entropy loss.
Settings of all the kMLPs trained with BP can be found in (Zhang et al., 2017). Note that because it
is extremely time/memory-consuming to train kMLP with BP without any acceleration method, to
make training possible, we could only randomly use 10000 examples from the entire training set of
55000 examples as centers for the kMLP-2 (BP) from Table 1b.
We compared kMLP with a one/two-hidden-layer MLP (MLP-1/MLP-2), a one/three-hidden-layer
DBN (DBN-1/DBN-3) and a three-hidden-layer SAE (SAE-3). For these models, hyperparameters
were also selected using the validation set. For the MLPs, the sizes of the hidden layers were chosen
from the interval [25, 700]. All hyperparameters involved in Adam, RMSProp and BN were set to
the suggested default values in the corresponding papers. If used, dropout and BN was added to
12
Under review as a conference paper at ICLR 2019
each hidden layer, respectively. For DBN-3 and SAE-3, the sizes of the three hidden layers varied
in intervals [500, 3000], [500, 4000] and [1000, 6000], respectively. DBN-1 used a much larger
hidden layer than DBN-3 to obtain comparable performance. A simple calculation shows that the
total numbers of parameters in the kMLPs were fewer than those in the corresponding DBNs and
SAEs by orders of magnitude in all experiments. Like in the training for the kMLPs, the validation
set were also reserved for early-stopping in final training. The DBNs and SAEs had been pre-trained
unsupervisedly before the supervised training phase, following the algorithms described in (Hinton
et al., 2006; Bengio et al., 2007). More detailed settings for these models were reported in (Larochelle
et al., 2007).
Appendix B	Further Analysis
In this section, we provide some further analysis on kMLP and the layer-wise learning algorithm.
Namely, in Appendix B.1, we give a bound on the Gaussian complexity of an l-layer kMLP, which
describes the intrinsic model complexity of kMLP. In particular, the bound describes the relationship
between the depth/width of the model and the complexity of its hypothesis class, providing useful
information for model selection. In Appendix B.2, we give a constructive result stating that the
dissimilarity measure being optimized at each hidden layer will not increase as training proceeds from
the input layer to the output. This also implies that a deeper kMLP performs at least as well as its
shallower counterparts in minimizing any loss function they are trained on. In Appendix B.3, a result
similar to Lemma 4.3 is provided, stating that the characterization for the optimal representation
can be made much simpler if one uses a more restricted learning paradigm. In fact, in contrast to
Lemma 4.3, both necessary and sufficient conditions can be determined under the more restricted
setting. In Appendix B.4, we provide a standard, generic method to estimate the Lipschitz constant
of a continuously differentiable kernel, as this quantity has been repeatedly used in many of our
results in this paper. In Appendix B.5, we state some advantages of kMLP over classical kernel
machines. In particular, empirical results are provided in Appendix B.5.1, in which a two-layer kMLP
consistently outperforms the classical Support Vector Machine (SVM) (Cortes & Vapnik, 1995) as
well as several SVMs enhanced by Multiple Kernel Learning (MKL) algorithms (Bach et al., 2004;
Gonen & AlPaydin, 2011).
B.1	Gaus sian Complexity of kMLP
We first give a result on the Gaussian comPlexity of a two-layer kMLP.
Lemma B.1. Given kernel k : X2 × X2 → R, where X2 ⊂ Rd1 . Let F2 = {f : X2 → R, f (x) =
νm=1 ανk(xν, x) + b | α = (α1, . . . , αm) ∈ Rm, kαk1 ≤ A, b ∈ R}, where the xν are arbitrary
examples from X2.
Consider Fi = {(fι,...,fdj : Xi → X2 | f ∈ Ω}, where Ω is a given hypothesis class
that is closed under negation, i.e., if f ∈	Ω,	then	—f	∈	Ω.	Define	F?	◦	Fi	= {h : x →
PV=ι ανk(F(XV), F(X)) + b ∣∣∣α∣∣ 1 ≤ A,b ∈ R,F ∈ Fi}.
Ifthe range ofsome element in Ω contains 0, we have
Gn(F ◦Fi) ≤ ALdiGN(Ω).
The above result can be easily generalized to kMLP with an arbitrary number of layers.
Lemma B.2. Given an l-layer kMLP, for each fj(i)(X) = Pνm=i α(νij)k(i)(F (i-i)(Xν), F (i-i) (X)) + b,
(i)
αj(i) := (α(iij), . . . , α(mi)j) ∈ Rm, b ∈ R, assume kαj
functions implemented by this kMLP as Fl, we have
ki ≤ Ai and let dl = 1. Denote the class of
l
Gn(Fl) ≤ di Y AiLtidiGN(Ω).
i=2
Proof. It is trivial to check that the hyPothesis class of each layer is closed under negation and that
there exists a function in each of these hyPothesis classes whose range contains 0. Then the result
follows from repeatedly applying Lemma B.1.	□
13
Under review as a conference paper at ICLR 2019
B.2	Nonincreasing Loss Across Layers
Lemma B.3. For i ≥ 2, assume k(i) is PD and fix layers 1, 2, . . . , i - 1 at arbitrary states
F(1), F(2), . . . , F (i-1). Let the loss functions `i, `i-1 be the same up to their domains, and de-
note both `i and `i-1 as `. Suppose layer i is trained with a gradient-based algorithm to minimize the
loss '(F(i)). Denote the state oflayer i after training by F(i)*. If di-ι = d%, there is an initialization
for F(i) such that
'(F⑴*) ≤ '(F(i-1)).	(3)
Calculation for this initialization is specified in the proof.
For i = 1, under the further assumption that Xi ⊂ Rd0 and do = di, Eq. 3 becomes '(F(1)*) ≤
'(F(0)), where F(O) is the identity map on Xi.
Remark B.3.1. For the greedily-trained kMLP, Lemma B.3 applies to the hidden layers and implicitly
requires that k(i+i) = k(i) since the loss function for layer i, when viewed as a function ofF(i),
takes the form '(k(i+1)(F (i))) and can be rewritten as 加中)(F(i)). Similarly, '(k(i)(F (i-1))) is
`k(i) (F(i-i)). Since Lemma B.3 assumes ` to be the same across layers (otherwise it does not make
sense to compare between layers), thisforces 'k(i+i)=线⑸.Then itfollows that k(i+1) = k(i).
Further, if k(i+1) and k(i) have the property that k(x, y) = k(x, y), where x, y denote the images of
x, y under an embedding ofRp into Rq (p ≤ q) defined by the identity map onto a p-dimensional
subspace ofRq, then the condition di-i = di can be relaxed to di-i ≤ di.
This lemma states that for a given kMLP, when it has been trained upto and including the ith hidden
layer, the i + 1th hidden layer can be initialized in such a way that the value of its loss function will
be lower than or equal to that of the ith hidden layer after training. In particular, the actual hidden
representation “converges” to the optimal represetation as training proceeds across layers. On the
other hand, when comparing two kMLPs, this result implies that the deeper kMLP will not perform
worse in minimizing the loss function than its shallower counterpart.
In deep learning literature, results analogous to Lemma B.3 generally state that in the hypothesis class
of a NN with more layers, there exists a hypothesis that approximates the target function nontrivially
better than any hypothesis in that of another shallower network (Sun et al., 2016). Such an existence
result for kMLP can be easily deduced from the earlier bound on its Gaussian complexity (see Lemma
B.2). However, these proofs of existence do not guarantee that such a hypothesis can always be found
through learning in practice, whereas Lemma B.3 is constructive in this regard. Nevertheless, one
should note that Lemma B.3 does not address the risk R = E'. Instead, it serves as a handy result
that guarantees fast convergence of upper layers during training in practice.
B.3	Simpler Optimal Representation under More Restricted Learning
Paradigms
The following lemma states that if we are willing to settle with a more restricted learning paradigm,
the necessary and sufficient condition that guarantees the optimality of a representation can be
characterized and is simpler than that described in Lemma 4.3. The setup for this lemma is the same
as that of Lemma 4.3 except that the assumption that the numbers of examples from the two classes
are equal is not needed.
Lemma B.4. Consider a learning paradigm that minimizes Rι(f(l)) + T GN (Fi,a) using represen-
tation Sl-i = F(S) under the constraint that it returns an f (l)? ∈ Fl,A with Rl (f(l)?) = 0, where
τ is any positive constant. For any N ∈ N, Rl (f(l)?) + T GN (Fl,A) is minimized over all linearly
separable representations if and only if the representation F(S) satisfies
k(l)(F(x+),F(x-)) = a
for all pairs of x+ , x- from distinct classes in S.
14
Under review as a conference paper at ICLR 2019
B.4	Lipschitz Constant for Continuously Differentiable Kernels
In general, for a continuously differentiable function f : R → R with derivative f0 and any a, b ∈ R,
a < b, we have
|f (b) - f (a)| = Z f0(x)dx ≤ Z |f 0 (x)|dx ≤ max |f0(x)|(b - a).
a	a	a≤x≤b
This simple result can be used to bound the Lipschitz constant of a continuously differentiable kernel.
For example, for Gaussian kernel k : X X X → R, X ⊂ R, k(χ,y) = e-(x-y)3σ2, We have
∂k(x, y)∕∂y = 2(x — y)k(x, y)∕σ2. Hence for each fixed X ∈ X, k(x, y) is LiPschitz in y with
Lipschitz constant bounded by supy∈χ ∣2(x — y)k(x, y)∕σ2∣. In practice, X is always compact and
can be a rather small subspace of some Euclidean space after normalization of data, hence this would
provide a reasonable approximation to the Lipschitz constant of Gaussian kernel.
B.5	Comparing kMLP with Classical Kernel Machines
There are mainly two issues with classical kernel machines besides their usually high computational
complexity. First, despite the fact that under mild conditions, they are capable of universal function
approximation (Park & Sandberg, 1991; Micchelli et al., 2006) and that they enjoy a very solid
mathematical foundation (Aronszajn, 1950), kernel machines are unable to learn multiple levels
of distributed representations (Bengio et al., 2013), yet learning representations of this nature is
considered to be crucial for complicated artificial intelligence (AI) tasks such as computer vision,
natural language processing, etc. (Bengio, 2009; LeCun et al., 2015). Second, in practice, performance
of a kernel machine is usually highly dependent on the choice of kernel since it governs the quality
of the accessible hypothesis class. But few rules or good heuristics exist for this topic due to its
extremely task-dependent nature. Existing solutions such as MKL (Bach et al., 2004; Gonen &
Alpaydin, 2011) view the task of learning an ideal kernel for the given problem to be separate from
the problem itself, necessitating either designing an ad hoc kernel or fitting an extra trainable model
on a set of generic base kernels, complicating training.
kMLP learns distributed, hierarchical representations because it inherits the architecture of MLP.
To be specific, first, we see easily that the hidden activation of each layer, i.e., F (i)(x) ⊂ Rdi, is a
distributed representation (Hinton, 1984; Bengio et al., 2013). Indeed, just like in an MLP, each layer
of a kMLP consists of an array of identical computing units (kernel machines) that can be activated
independently. Further, since each layer in a kMLP is built on top of the previous layer in exactly the
same way as how the layers are composed in an MLP, the hidden representations are hierarchical
(Bengio et al., 2013).
Second, kMLP naturally combines the problem of learning an ideal kernel for a given task and the
problem of learning the parameters of its kernel machines to accomplish that task. To be specific,
kMLP performs nonparametric kernel learning alongside learning to perform the given task. Indeed,
for kMLP, to build the network one only needs generic kernels, but each layer F (i) can be viewed as
a part of a kernel of the form k(i+1)(F(i)(x), F (i)(y)). The fact that each F(i)is learnable makes
this kernel “adaptive”, mitigating to some extent any limitation of the fixed generic kernel k(i+1).
The training of layer i makes this adaptive kernel optimal as a constituent part of layer i + 1 for the
task the network was trained for. And it is always a valid kernel if the generic kernel k(i+1) is. Note
that this interpretation has been given in a different context by Huang & LeCun (2006) and Bengio
et al. (2013), we include it here only for completeness.
B.5.1	Empirical Results
We now compare a single-hidden-layer kMLP using simple, generic kernels with SVMs enhanced by
MKL algorithms that used significantly more kernels to demonstrate the ability of kMLP to automat-
ically learn task-specific kernels out of standard ones. The standard SVM and seven other SVMs
enhanced by popular MKL methods were compared (Zhuang et al., 2011), including the classical
convex MKL (Lanckriet et al., 2004) with kernels learned using the extended level method proposed
in (Xu et al., 2009) (MKLLEVEL); MKL with Lp norm regularization over kernel weights (Kloft et al.,
2011) (LpMKL), for which the cutting plane algorithm with second order Taylor approximation of
15
Under review as a conference paper at ICLR 2019
Table 2: Average test error (%) and standard deviation (%) from 20 runs. Results with overlapping
95% confidence intervals (not shown) are considered equivalent. Best results are marked in bold. The
average ranks (calculated using average test error) are provided in the bottom row. When computing
confidence intervals, due to the limited sizes of the data sets, we pooled the 20 random samples.
	Size/Dimension	SVM	MKLLEVEL	LpMKL	GMKL	IKL	MKM	2LMKL	2LMKLINF	kMLP-1
Breast	683/10	3.2± 1.0	3.5± 0.8	3.8± 0.7	3.0± 1.0	3.5± 0.7	2.9± 1.0	3.0± 1.0	3.1± 0.7	2.4± 0.7
Diabetes	768/8	23.3± 1.8	24.2± 2.5	27.4± 2.5	33.6± 2.5	24.0± 3.0	24.2± 2.5	23.4± 1.6	23.4± 1.9	23.2± 1.9
Australian	690/14	15.4± 1.4	15.0± 1.5	15.5± 1.6	20.0± 2.3	14.6± 1.2	14.7± 0.9	14.5± 1.6	14.3± 1.6	13.8± 1.7
Iono	351/33	7.2± 2.0	8.3± 1.9	7.4± 1.4	7.3± 1.8	6.3± 1.0	8.3± 2.7	7.7± 1.5	5.6± 0.9	5.0± 1.4
Ringnorm	400/20	1.5± 0.7	1.9± 0.8	3.3± 1.0	2.5± 1.0	1.5± 0.7	2.3± 1.0	2.1± 0.8	1.5± 0.8	1.5± 0.6
Heart	270/13	17.9± 3.0	17.0± 2.9	23.3± 3.8	23.0± 3.6	16.7± 2.1	17.6± 2.5	16.9± 2.5	16.4± 2.1	15.5± 2.7
Thyroid	140/5	6.1± 2.9	7.1± 2.9	6.9± 2.2	5.4± 2.1	5.2± 2.0	7.4± 3.0	6.6± 3.1	5.2± 2.2	3.8± 2.1
Liver	345/6	29.5± 4.1	37.7± 4.5	30.6± 2.9	36.4± 2.6	40.0± 2.9	29.9± 3.6	34.0± 3.4	37.3± 3.1	28.9± 2.9
German	1000/24	24.8± 1.9	28.6± 2.8	25.7± 1.4	29.6± 1.6	30.0± 1.5	24.3± 2.3	25.2± 1.8	25.8± 2.0	24.0± 1.8
Waveform	400/21	11.0± 1.8	11.8± 1.6	11.1± 2.0	11.8± 1.8	10.3± 2.3	10.0± 1.6	11.3± 1.9	9.6± 1.6	10.3± 1.9
Banana	400/2	10.3± 1.5	9.8± 2.0	12.5± 2.6	16.6± 2.7	9.8± 1.8	19.5± 5.3	13.2± 2.1	9.8± 1.6	11.5± 1.9
Rank	-	4.2	6.3	7.0	6.9	4.3	5.4	5.0	2.8	1.6
Lp was adopted; Generalized MKL in (Varma & Babu, 2009) (GMKL), for which the target kernel
class was the Hadamard product of single Gaussian kernel defined on each dimension; Infinite Kernel
Learning in (Gehler & Nowozin, 2008) (IKL) with MKLLEVEL as the embedded optimizer for kernel
weights; 2-layer Multilayer Kernel Machine in (Cho & Saul, 2009) (MKM); 2-Layer MKL (2LMKL)
and Infinite 2-Layer MKL in (Zhuang et al., 2011) (2LMKLINF).
Eleven binary classification data sets that have been widely used in MKL literature were split evenly
for training and test and were all normalized to zero mean and unit variance prior to training. 20
runs with identical settings but random weight initializations were repeated for each model. For each
repetition, a new training-test split was selected randomly.
For kMLP, all results were achieved using a greedily-trained, one-hidden-layer model with the number
of kernel machines ranging from 3 to 10 on the first layer for different data sets. The second layer was
a single kernel machine. All kernel machines within one layer used the same Gaussian kernel, and the
two kernels on the two layers differed only in kernel width σ. All hyperparameters were chosen via
5-fold cross-validation. As for the other models compared, for each data set, SVM used a Gaussian
kernel. For the MKL algorithms, the base kernels contained Gaussian kernels with 10 different widths
on all features and on each single feature and polynomial kernels of degree 1 to 3 on all features and
on each single feature. For 2LMKLINF, one Gaussian kernel was added to the base kernels at each
iteration. Each base kernel matrix was normalized to unit trace. For LpMKL, p was selected from
{2, 3, 4}. For MKM, the degree parameter was chosen from {0, 1, 2}. All hyperparameters were
selected via 5-fold cross-validation. From Table 2, kMLP compares favorably with other models,
which validates our claim that kMLP learns its own kernels nonparametrically hence can work well
even without excessive kernel parameterization. Performance difference among models can be small
for some data sets, which is expected since they are all rather small in size and not too challenging.
Nevertheless, it is worth noting that only 2 Gaussian kernels were used for kMLP, whereas all other
models except for SVM used significantly more kernels.
Appendix C	Proofs
Proof of Lemma 4.3. Throughout this proof we shall drop the layer index l for brevity. Given that
the representation satisfies Eq. 1, the idea is to first collect enough information about the returned
f? = (w?, b?) such that we can compute R(f ?) + τ kw?kH and then show that for any other F0(S)
satisfying the condition in the lemma, suppose the learning paradigm returns f0 = (w0, b0) ∈ FA0,
then R(f 0) + τkw0kH ≥ R(f ?) + τ kw?kH. We now start the formal proof.
First, note that in the optimal representation, i.e., an F(S) such that Eq. 1 holds, it is easy to see that
kφ(F(x-)) - φ(F (x+))kH is maximized over all representations for all x-, x+.
Moreover, note that given the representation is optimal, we have φ(F (x)) = φ(F (x0)) if y = y0
and φ(F (x)) 6= φ(F (x0)) if y 6= y0: Indeed, by Cauchy-Schwarz inequality, for all x, x0 ∈ S,
k(F (x), F (x0)) = hφ(F (x)), φ(F (x0))iH ≤ kφ(F (x))kH kφ(F(x0))kH and the equality holds if
16
Under review as a conference paper at ICLR 2019
and only if φ(F(x)) = pφ(F(χ0)) for some real constantp. Using the assumption on k, namely, that
∣∣φ(F(X))IIH = √c for all F(x), We further conclude that the equality holds if and only if P = 1.
And the second half of the claim follows simply from c > a. Thus, all examples from the + and -
class can be vieWed as one vector φ(F(x+)) and φ(F(x-)), respectively.
The returned hyperplane f? cannot pass both F(x+) and F(x-), i.e., f?(F(x+)) = 0 and
f?(F(x-)) = 0 cannot happen simultaneously since if so, first subtract b?, rotate While keep-
ing ∣∣w?∣∣h unchanged and add some suitable b0 to get a new f such that f(F(X-)) < 0 and
f (F(x+)) > 0, then it is easy to see that R(F) + TIlwllH < R(f ?) + T∣∣w*∣∣h. But by construc-
tion of the learning paradigm, this is not possible.
Now suppose the learning paradigm returns an f? such that
y+f*(F(x+)) + y-f*(F(x-)) = hΦ(F(x+)) - Φ(F(x-)),w*>H
=∣φ(F(x+)) - φ(F(x-))∣Hkw*∣HcosθF,w? =: ζ. (4)
First note that for an arbitrary θF,w?, ζ is less than or equal to 2 since one can always adjust b?
such that y+f?(F(x+)) = y-f?(F(x-)) without changing ζ and hence having a larger ζ will
not further reduce R(f?), which is 0 when ζ = 2, but will result in a larger Iw?IH according to
Eq. 4. On the other hand, θF,w? must be 0 since this gives the largest ζ with the smallest Iw?IH.
Indeed, if the returned f? does not satisfy θF,w? = 0, one could always shift, rotate while keeping
Iw? IH fixed and then shift the hyperplane back to produce another f 0 with θF,w0 = 0 and this
f 0 results in a larger ζ if ζ < 2 or the same ζ if ζ = 2 but a smaller Iw0IH by rescaling. Hence
R(f ) + τIlwlIH < R(f ?) + τ∣∣w*∣∣h but again, this is impossible.
Together with what we have shown earlier, we conclude that 2 ≥ ζ > 0. Then for some t ∈ R, we
have
R(f ) = K max(0,1 — t) + (1 — κ) max(0,1 — (Z — t)).
First note that we can choose t freely while keeping w? fixed by changing b?. If κ = 1/2, we have
fl - Z/2,	if 1 ≥ t ≥ Z - 1
R(f*)=< 2(1+1 - Z), if t> 1
12(1 -1),	if t <ζ - 1.
Evidently, the last two cases both result in R(f ?) > 1 - Z/2 hence f? must produce a t in [Z - 1, 1]
ʌ ,
and R(f?) = 1 - Z/2.
Now, when κ 6= 1/2, first observe that if 1 ≥ t ≥ Z - 1,
ʌ , . , ..
R(f?) = κ(1 - t) + (1 - κ)(t-(Z - 1))
= (1 - 2κ)t + κ - (1 - κ)(Z - 1).
If κ > 1/2, R(f?) decreases in t hence t must be 1 for f?, which implies R(f?) = (1 一 κ)(2 一 Z).
Similarly, if κ < 1/2, t = Z — 1 and hence R(f?) = κ(2 一 Z).
Now suppose t ≥ 1, R(f?) = (1 - κ)(1 + t - Z), which increases in t and hence t = 1 and
R(f?) = (1 - κ)(2 - Z). If κ < 1/2, since (1 - κ)(2 - Z) > κ(2 - Z), this combination of K and t
contradicts the optimality assumption of f? .
Ift ≤ Z - 1, R(f ?) = K(1 - t) = K(2 - Z), where the second equality is because R(f?) decreases
in t. Again, K > 1/2 leads to a contradiction.
Combining all cases, we have
TZ
R(f ) + TIlw IIh = min(K, 1 - K)(2 - Z) + ∣τ-7^7~~vʒ~~-7w(~~TVij-
Iφ(F(x+)) - φ(F(x-))IH
TZ
=min(κ, 1 - K)(2 - Z) + IZ 、
2(c - a)
=2min(K, 1 - k) + ^p==C=a= - min(K, 1 - K)) J
17
Under review as a conference paper at ICLR 2019
which, by the assumption on τ, strictly decreases in ζ over (0, 2]. Hence the returned f? must satisfy
A r、 ι ♦ ι ♦ ι ∙ τ∖ / ∖cι	1
ζ = 2, which implies R(f?) = 0 and we have
ʌ ，	，	√2τ
R(f* )+ T kw?kH = √==.
c-a
Now, for any other F0(S), suppose the learning paradigm returns f0. Let xw+0 , xw-0 be the pair of
examples with the largest f0(F0(x+)) - f0(F0(x-)). We have
yW0f0(F(X+)) + yW0f0(F0(χWj) = kΦ(F0(χ+0)) - Φ(F0(χ-0))kH|31山cosΘf，,w，= Z0.
Then
R(f)+ T kw0kH ≥ T kw0kH
=_________________τZ0________________
=kΦ(F0(xw0)) - φ(F0(xW0))kH cosΘf0,w0
〉	τ |Z 01
≥ kΦ(F0(xW0))- Φ(F0(xW0))kH
2T
≥	,
ʌ/2(C - a
ʌ
≥ R(f?)+ T kw*kH,
where we have used the assumption that there exists x- , x+ with y- f0(F0(x- )), y+ f0(F0(x+ )) ≥ 1.
This proves the desired result.	□
Lemma C.1. Suppose f1 ∈ F1, . . . , fd ∈ Fd are elements from sets of real-valued functions defined
on all of Xi, X?, ..., Xm, where Xj ⊂ Rd for all j, F ⊂ Fi X…X Fd. For f ∈ F, define ω ◦ f :
Xi X …X Xm X Y → R as (xι,...,Xm,y) → ω(f1(x1),...,fd(x1),f1(x2),...,fd(xm),y),
where ω : Rmd X Y → R+ ∪ {0} is bounded and L-Lipschitz for each y ∈ Y with respect to the
Euclidean metric on Rmd. Let ω ◦ F = {ω ◦ f : f ∈ F}. Denote the Gaussian complexity of Fi on
Xj as GjN(Fi), if the Fi are closed under negation, i.e., for all i, if f ∈ Fi, then -f ∈ Fi, we have
dm
GN (ω ◦ F) ≤ 2L XX
GNj (Fi).	(5)
i=i j=i
In particular, for all j, if the xjn upon which the Gaussian complexities of the Fi are evaluated are sets
ofi.i.d. random elements with the same distribution, we have GN(Fi) = .…=GN(Fi) := GN(Fi)
for all i and Eq. 5 becomes
GN(ω ◦ F) ≤ 2mL X GN (Fi).
i=i
This lemma is a generalization of a result on the Gaussian complexity of Lipschitz functions on Rk
from (Bartlett & Mendelson, 2002). And the technique used in the following proof is also adapted
from there.
Proof. For the sake of brevity, we prove the case where m = 2. The general case uses exactly the
same technique except that the notations would be more cumbersome.
Let F be indexed by A. Without loss of generality, assume |A| < ∞. Define
N
Xα =	ω(fα,i (xn), . . . , fα,d(xn), yn )gn ;
n=i
Nd
Yα = L	(fα,i (xn)gn,i + fα,i (x0n)gN +n,i),
n=i i=i
18
Under review as a conference paper at ICLR 2019
where α ∈ A, the (xn, x0n) are a sample of size N from X1 × X2 and g1, . . . , gN, g1,1, . . . , g2N,d
are i.i.d. standard normal random variables.
Let arbitrary α, β ∈ A be given, define kXα - Xβ k22 = E(Xα - Xβ)2, where the expectation is
taken over the gn . Define kYα - Yβ k22 similarly and we have
N
kXα - Xβ k22 = X ω(fα,1 (xn), .
n=1
2
. . , fα,d(xn), yn) - ω(fβ,1 (xn), . . . , fβ,d(xn), yn
Nd
≤ L2 XX (fα,i(xn) - fβ,i(xn))2 + (fα,i(x0n) - fβ,i(x0n))2
n=1 i=1
= kYα - Yβk22.
By Slepian’s lemma (Pisier, 1999) and since the Fi are closed under negation,
N
GN θω (ω ◦F) = Egn SuP Xa
2	α∈A
≤ 2Egn,i,gN+n,i suP Yα
α∈A
Nd
≤ "2^2L E(GN (Fi) + GN (Fi)).
i=1
Taking the expectation of the xn and x0n on both sides, we have
-2^GN(ω ◦ F) ≤ "2^2L X(GN(Fi) + GN(Fi)).
i=1
□
Proof of Lemma 4.4. Normalize `l-1 to [0, 1] by dividing 2 max(|c|, |a|). Then the loss function
becomes
'l-l(F (T),(Xm,ym), (Xn,yn)) =-_^^Ik(I)(F (l-1)(Xm),F (IT)(Xn)) — (G?)mn|.
2 max(|c|, |a|)
For each fixed (G?)mn,
∣'l-l(F (IT), (xm,ym), (xn,yn)) - 'l-l(F (IT),(或用工 ∖ IXnHn )) |
≤ 2maχ∩c∣ ∣α∣jk(l)(F (T)(Xm),F (IT)(Xn )) — 即(F (IT)(Xm),F ”1)0))1
2 max(|c|, |a|)
≤ 2πlaχ[l IaI) (Ik(I)(F (j1)(Xm),F (I-I)(Xn)) — k(I)(F (l-1) (Xm),F (T)(Xn))|
2 max(|c|, |a|)
+ ∣k(I) (F (I-I)(Xm),F (I-I)(Xn)) — k(l)(F (T)(Xm),F (T)(Xn))∣)
≤ 2maX(∣c∣, ∣a∣) (IIF(I-I)(Xn)- F(I-I)(Xn)( + IIF(I-I)(Xm)- F(I-I)(Xm)ID
≤ max(∣c∣, ∣a∣) I(F(I-I)(Xn)- F(I-I)(Xn), F(I-I)(Xm)- F(I-I)(Xm)) (∙
Hence `l-1 is L(l)/ max(IcI, IaI)-Lipschitz in (F(l-1) (Xm), F (l-1)(Xn)) with respect to the Eu-
clidean metric on R2dl-1 for each (G?)mn.
The result follows from Lemma C.1 with m = 2, d = dl-1 and Corollary 15 in (Bartlett & Mendelson,
2002).	口
Proof of Lemma 4.5. This proof uses essentially the same idea as that of Lemma 4.3. Due to the com-
plete dependence of k(l) (X, y) on kX - yk2, we can rewrite k(l) F(l-1)(F(Xm)), F(l-1) (F(Xn))
19
Under review as a conference paper at ICLR 2019
as h(l) (IlF(IT)(F(Xm))- F(IT)(F(Xn))∣0 for some h(I). Define 2m-。= ∣∣φ(lT)(F(Xm))-
φ(l-1)(F(xn))kHl-1,wehave
Rl-I(F (IT))
1N
=N X
m,n=1
1N
=N X
m,n=1
1N
=N X
m,n=1
h(l) IIF(l-1)(F(Xm))-F(l-1)(F(Xn))II2 -(G?)mn
dl-1
h(l)(t Xfj(IT)(F(Xm))-fj(IT)
j
dl-1
h(l) (t X(μmn1)k
j
wf (l-1) kHl-1 cos θF,w (l-1)
j	fj
1
1
Given that the representation is optimal, we have
Rl-I(F(j))+ T 1≤madX-1 kwfj(l-1) kHl-I
N X (h(l)(tX (μmΛwf产
ym 6=yn	j =1
kHl-1 cos θF,w (l-1)
fj
+ τ 1≤mj≤adxl-1 kwfj(l-1)kHl-1.
The returned F (l-1)? must satisfy
udl-1
t X(μmn1)kwf (l-1)? kHl-1 cos θF,w (l-1)? 2 ≤ η;
j=1	j
Ilwf (l-1)? kHl-I =…=kwf (l-1)? kHl-1 =: kw?kHl-1 ；
f1	fdl-1
cos θF,w? = 1.
The first observation is trivial since if otherwise, one can always reduce the largest kw (l-1)? kHl-1
fj
to obtain equality to η, this gives the same Rl-I(F(l-1)?) with a smaller Tmaxj kwf(i-i)?kHi-i.
fj
Note that if during shrinking the largest kw (l-1)? kHl-1 , this element ceases to be the largest among
fj
all j , we shall continue the process with the new largest instead. To see the rest of the claim,
note that for the largest of the kw (l-1)? kHl-1, we must have θF,w (l-1)? = 0 since if not, one
fj	fj
could shift and rotate the hyperplane and again obtain the same Rl-I(F(l-1)?) with a smaller
T maxj kw (l-1)? kHl-1 . Reducing the largest kw (l-1)? kHl-1 and increasing the second largest by
fj	fj
scaling, one would get a smaller risk. It is immediate that the minimal risk (w.r.t. only the first and
second largest kw (l-1)? kHl-1) is attained when the largest and the second largest kw (l-1)? kHl-1
fj	fj
are equal. Then a similar argument as before gives θF w l 1
fj(l-1)?
claim follows via repeatedly applying this argument to all the k
0 for both of them. The rest of the
wf (l-1)? kHl-1 .
Define
If?(F(x+)) - f?(F(x-))l = kφ(lT)(F(x+)) - φ(IT(F(x-))kHι-ιkw*kHι-ι∙
20
Under review as a conference paper at ICLR 2019
Then we have
Rl-I(F(l-1)?) + T 1≤m≤ax Jwf(1-1)? I∣H1-1
=ψ (h(I) (Pdl-Γ∣f?(F(x+)) - f*(F(x-))∣) - a
+	T∣f*(F(x+))- f*(F(x-))∣
十 ∣φ(IT)(F(x+)) - φ(IT)(F(X-))∣Hι-ι
=ψ QS (pdl-7∣f*(F(x+)) - f*(F(x-))∣) - a
+ T∣f*(F(x^)- fUF(x-))∣.
p2(c - a)
As we have shown, Vzdl-I ∣f*(F(x+)) - f*(F(x—))| ∈ [0,η]. Let λ = ∣f*(F(x+)) - f*(F(x—))|
and differentiate the r.h.s. of the above equation w.r.t. λ and using the assumption on T, we have
d——，dh(l)(t)	τ
pd^ψ-dΓ~ + √2(∑-τy <0.
Hence the overall risk decreases in λ over [0, η//dl-ι], which implies that the returned F(l-1)*
must have λ = η//dl-ι and
Rl-I(F(l-1)*) + Tymx	kwf(1-1)? kHi-1
1≤j ≤dl-1	fj
Tη
√2dl-i(c - a)
Now for any other representation Sl0-2 = F0(S), suppose the learning paradigm returns F (l-1)0.
Assume without loss of generality that the largest fj(l-1)0(F 0(x+)) - fj(l-1)0(F0(x-)) over all j, x+
and x- is attained at j = 1 and write w0 = wf (l-1)0, f0 = f1(l-1)0 for convenience. Let xw+0, xw-0
be the pair with the largest f 0(F0(χ+)) — f0(F0(x-)). Note that the assumption on F0 implies that
f0(F0(x+)) - f 0(F0(x-)) ≥ η/Pd；-i. Then we have
Rl-I(F(IT)0) + T 1≤m≤axτ kwf(1-1)0 kHi-1
≥ TIw0IHl-1
=___________TIf0(F0(x+))- f0(F0(x-))∣_________
=kφ(IT)(F0(x+))- φ(IT)(F0(x-))kHι-ι ∣ cosΘf，田|
≥ Tn
一 √2dl-i(c - a)
=Rl-I(F(T)*) + T max kw (l-1)? kHl 1 ,
1≤j≤dl-1	fj	-
proving the lemma.
□
21
Under review as a conference paper at ICLR 2019
Proof of Lemma 4.6. First, it is trivial that the so-defined s metric is indeed a metric. In particular, it
satisfies the triangle inequality. For i = 2, . . . , l,
=Xsux』k" (T*(X))-M …(X)》MX Mi)IHi+q
≤ XuXi r(LF(f(χ)+LT-I(X))IlF (T*(X)- Ti-I(X”2 tX 帧 )* IlHi+q
≤ LiX 帕 力 r∣F-F-I+ɛi，
which proves the lemma.	□
ProofofLemma B.1. Since Ω and F? are both closed under negation, We have
CN (尸2 ◦Fl) = E sup
α,F
Nm
⅛ΣΣαν k (F (Xν ), F (Xn ))gn
n=1 ν=1
2m N
≤ E suP卡HEk(F(Xν ), F(Xn))gn
2N
≤ MAE supmaxj^ k(F(XV),F(xn))gn
N Fν
n=1
2N
=NAE supmax X (k(F(%ν),F(Xn)) — k(F(XV), 0))gn
2N
+ MAE supmax^ k(F(XV), 0)gn
N FV
n=1
2N
≤ MAE sup maXE LF(x“ikF (Xn)k2gn
N FV
n=1
2N
≤ NALE sup £ IlF(Xn)∣∣2gn
F n=1
2N
≤ NALE sup£ IlF(Xn)IlIgn
F n=1
.— ʌ ,一、
≤ ALdICN(Ω).
22
Under review as a conference paper at ICLR 2019
Taking expectation with respect to the xn finishes the proof.
□
Proof of Lemma B.3. For i ≥ 1, given the assumption that di-1 = di, we first show that one can
find an initialization for layer i, denoted F(i)°, such that F(i"(S) = F(i-1)(S).
Without loss of generality, assume i = 1, then it amounts to showing that a set of parameters can
be found for the kernel machines f1(1), . . . , fd(1) of F(1) such that when these kernel machines are
initialized with this certain set of parameters, this layer agrees with the identity function when
restricted to S. Recall that fj(i)(x) = PnN=1 α(jin)k(i)(xn, x) and {αj(in) ∈ R | n = 1, 2, . . . , N} is the
set of trainable parameters for this kernel machine.
Solve equation AG = D for A, where A is the d0 × N matrix of parameters defined as (A)nm = α(n1m) ,
G is the N × N kernel matrix defined as (G)nm = k(1) (xn, xm), D is a d0 × N matrix whose nth
column is the nth example xn = (xn1, xn2, . . . , xnd0)> for n = 1, 2, . . . , N. It is straightforward
that the set of parameters A initializes F⑴ to the desired F⑴◦. Moreover, A can always be solved
in closed-form by inverting G since k(1) being PD guarantees that G is invertible.
Combining the above result and the defining property of gradient of the loss function with respect to
the parameters, i.e., that gradient points at the direction of the greatest rate of increase of the loss
function, we then have
'(F(O))= '(F⑴◦) ≥ '(F⑴)
□
Proof of Lemma B.4. In this proof we shall again drop the layer index l for brevity.
Identify f ∈ FA using w, b upto R-scalar multiplication. Fix an F such that φ(F(S)) is linearly
separable (otherwise the learning paradigm is not applicable).
Let x+, x- be the closest pair of examples from distinct classes in H. For a given f = (w, b),
let xw- , xw+ be a pair of examples from different classes that are closest to f in H in the sense that
f (F (xw+)), f(F(xw-)) are the smallest in absolute value among all x+, x-, respectively. Denote the
canonical form of f with respect to φ(F(S)) with equality attained by at least one of xw+, xw- as
fF = (wF, bF), i.e., fF satisfies yfF (x) ≥ 1 for all examples in (xn, yn)nN=1 and for at least one of
xw-, xw+, the equality is attained. Define AfF := infbF ∈R kwF kH.
Suppose the algorithm returns f? = (w?, b?) and recall that A is the smallest such that f? ∈ FA,
then for all f = (w, b) ∈ FA such that R(f) = 0,
A ≥ kwkH ≥ kwF kH ≥ AfF .
And it is easy to see that the last equality holds if and only if bF is such that y-wfF (xw-) = y+wfF (xw+).
Then ff ∈ Fa. Since Rf) = 0 and we have the freedom to choose bp such that the last equality
holds, by construction of the learning paradigm and the minimality of A, we have
A = min Af = Af? .
f∈FA	fF	fF
ʌ,.
R(f )=0
In canonical form, for any fF ∈ FA, choose bF such that kwF kH = AfF , then
21
厂=—(φ(F(Xw))- φ(F(x-)), WF〉H
AfF	AfF	H
≤ A1— <φ(F (x+ )) - φ(F (x-)),wF }h = ∣∣φ(F (X+)) - φ(F (x-))∣∣H COS θF,w .
f
By construction of the learning paradigm, f? must make the equality hold and satisfy θF,w? = 0.
Hence we have
2
A = AfF?
∣∣φ(F(x+))- φ(F(x-))∣∣H,
23
Under review as a conference paper at ICLR 2019
which proves that, as a function of F , A achieves its minimum if and only if F maximizes
φ(F (x+)) - φ(F (x-))H. Since
arg max φ(F (x+)) - φ(F (x-))H
F
=argmax (∣∣φ(F (x+))∣∣H+∣∣φ(F (XJ)IlH - 2{φ(F (x+)),φ(F (XJ)〉H)
F
= arg min k F(X+), F(X-) ,
F
where we have used the assumption on k, namely, that k(X, X) = hφ(X), φ(X)iH = kφ(X)k2H = c,
for all X. It immediately follows that any minimizer F of A must minimize k(F (X+), F(X-)) for all
pairs of examples from opposite classes. This proves the desired result.	□
24