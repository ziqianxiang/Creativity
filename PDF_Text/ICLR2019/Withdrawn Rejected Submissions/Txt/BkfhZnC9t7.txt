Under review as a conference paper at ICLR 2019
Zero-shot learning for speech recognition
WITH UNIVERSAL PHONETIC MODEL
Anonymous authors
Paper under double-blind review
Ab stract
There are more than 7,000 languages in the world, but due to the lack of training
sets, only a small number of them have speech recognition systems. Multilin-
gual speech recognition provides a solution if at least some audio training data is
available. Often, however, phoneme inventories differ between the training lan-
guages and the target language, making this approach infeasible. In this work, we
address the problem of building an acoustic model for languages with zero audio
resources. Our model is able to recognize unseen phonemes in the target language,
if only a small text corpus is available. We adopt the idea of zero-shot learning,
and decompose phonemes into corresponding phonetic attributes such as vowel
and consonant. Instead of predicting phonemes directly, we first predict distri-
butions over phonetic attributes, and then compute phoneme distributions with a
customized acoustic model. We extensively evaluate our English-trained model
on 20 unseen languages, and find that on average, it achieves 9.9% better phone
error rate over a traditional CTC based acoustic model trained on English.
1	Introduction
Over the last decade, Automatic Speech Recognition (ASR) has achieved great successes in many
well-resourced languages such as English, French and Mandarin (Amodei et al., 2016; Xiong et al.,
2016; Collobert et al., 2016). On the other hand, speech resources are still sparse for the major-
ity of languages (Lewis, 2009). They cannot thus benefit directly from recent technologies. As a
result, there is an increasing interest in building speech recognition systems for low-resource lan-
guages. As collecting annotations for low resource languages is expensive and time-consuming,
researchers have exploited data augmentation (Ragni et al., 2014; Kanda et al., 2013), articulatory
features (Stuker et al., 2003a) and multilingual techniques (Schultz & Waibel, 2001; VU & Schultz,
2013; TUske et al., 2013; Dalmia et al., 2018b).
An even more challenging problem is to recognize utterances in a language with zero training data.
This task has significant implications in documenting endangered languages and preserving the as-
sociated cultures (Gippert et al., 2006). This data setup has mainly been studied for unsupervised
speech processing field(Glass, 2012; Jansen et al., 2013; Versteegh et al., 2015; Dunbar et al., 2017;
Heck et al., 2017; Hermann & Goldwater, 2018), which typically uses an unsupervised technique
to learn representations which can be used towards speech processing tasks, detailed in Section 4.
Some of them also deal with the Out-Of-Vocabulary (OOV) problem in the language modeling side
of ASR (Maas et al., 2015), or visual-only speech recognition (Stafylakis & Tzimiropoulos, 2018).
In terms of zero-shot learning in speech recognition, which consist of learning an acoustic model
without any audio data for a given target language, there has not been much work. Previous works
typically consider some amount of audio corpus to be present, because the phonetics of a target
language usually differ from existing training languages, especially when there are unseen phonemes
which are unavailable in existing language resources. In this work, we aim to solve this problem
without even considering any target audio data.
The prediction of unseen objects has also been a long-time critical problem in the computer vision
field. For specific object classes such as faces, vehicles and cats, a significant number manually
labeled data is usually available, but collecting sufficient data for every object human could recog-
nize is impossible. Zero-shot learning attempts to solve this problem to classify even unseen objects
using mid-level side information. For example, zebra can be recognized by detecting attributes such
1
Under review as a conference paper at ICLR 2019
Figure 1: Illustration of the proposed zero-shot learning framework. Each utterance is first mapped
into acoustic space H. Then we transform each point in the acoustic space into phonetic space
P with a linear transformation V . Finally phoneme distributions can be obtained by applying a
signature matrix S
as stripped, black and white. Inspired by approaches in computer vision research, we propose the
Universal Phonetic Model (UPM) to apply zero-shot learning to acoustic modeling. In this model we
decompose the phoneme into its attributes and learn to predict a distribution over various phonetic
attributes. This can then be used to infer the unseen phonemes for the test language. For example,
the phoneme /a/ can be decomposed into its attributes: vowel, open, front and unrounded.
Our approach is summarized in Figure 1. First, frames are extracted and a standard acoustic model
is applied to map each frame into the acoustic space H. Next we transform it into the phonetic
space P which reflects the phonetic properties of each frame (such as whether it indicates a vowel
or a consonant). Then, we compute the distribution of phonemes for that frame using a predefined
signature matrix S which describes relationships between phonetic attributes and phonemes in each
language. Finally we adjust the phoneme distribution with a prior distribution estimated from a
small set of text corpus (usually 1000 sentences). To the best of our knowledge, this is the first paper
applying zero-shot learning for acoustic model in speech recognition without any audio data.
To evaluate our UPM model, we trained our model on English, and evaluated the model on 20
languages. We also trained a standard English acoustic model as a baseline for comparison. The
result indicates that we consistently outperform the baseline CTC model, and we achieved 9.9%
improvements in phone error rate on average. As we mainly focus on acoustic model in this paper,
we adopt phone error rate as our main metric in this work.
The main contributions of this paper are as follows:
1.	We propose the Universal Phonetic Model (UPM) that can recognize phonemes that are
unseen during training by incorporating knowledge from the phonetics domain.1
2.	We introduce a sequence prediction model to integrate a zero-shot learning framework for
sequence prediction problem.
3.	We show that our model is effective for 20 languages, and our model gets 9.9% better phone
error rate over the baseline on average.
2	Approach
This section explains details of our Universal Phonetic Model (UPM). In the first section, we de-
scribe how we constructed a proper set of phonetic attributes for acoustic modeling. Next, we
1Our code would be released upon acceptance
2
Under review as a conference paper at ICLR 2019
demonstrate how to assign attributes to each phoneme by giving an algorithm to parse X-SAMPA
format. Finally we show how we integrate the phonetic information into the sequence model with a
CTC loss (Graves et al., 2006).
2.1	Phonetic Attributes
Unlike attributes in the computer vision field, attributes of phones are independent of the corpus and
dataset, they are well investigated and defined in the domain of articulatory phonetics (Ladefoged &
Johnson, 2014). Articulatory phonetics describes the mechanism of speech production such as the
manner of articulation and place of articulation, and it tends to describe phones using discrete fea-
tures such as voiced, bilabial (made with the two lips) and fricative. These articulatory features have
been shown to be useful in speech recognition (Kirchhoff, 1998; Stuker et al., 2003b; Muller et al.,
2017), and are a good choice for phonetic attributes for our purpose. We provide some categories of
phonetic attributes below. The full list of phonetic attributes we used is attached in the Appendix.
Consonants. Consonants are formed by obstructing the airstream through the vocal tract. They can
be categorized in terms of the place and the manner of this obstruction. The places can be largely
divided into three classes: labial, coronal, dorsal. Each of the class have more fine-grained classes.
The manners of articulation can be grouped into: stop, fricative, approximant etc.
Vowel. In the production of vowels, the airstream is relatively unobstructed. Each vowel sound can
be specified by the positions of lips and tongue (Ladefoged & Johnson, 2014). For instance, the
tongue is at its highest point in the front of the mouth for front vowels. Additionally, vowels can be
characterized by properties such as whether the lips are rounding or not (rounded, unrounded).
Diacritics. Diacritics are small marks to modify vowels and consonants by attaching to them. For
instance, nasalization marks a sound for which the velopharyngeal port is open and air can pass
through the nose. To make the phonetic attribute set manageable, we assign attributes of diacritics
to some existing consonants attributes if they share similar phonetic property. For example, nasal-
ization is treated as the nasal attribute in consonants.
In addition to phonetic attributes mentioned above, we note that we also need to allocate an special
attribute for blank in order to predict blank labels in CTC model, and backpropagate their gradients
into the acoustic model. Thus, our phonetic attribute set Aphone is defined as the union of these
three domain attributes as well as the blank label.
Aph
one
Aconsonants
∪ Avowels ∪ Adiacritics ∪ {blank}
(1)
2.1.1	Attribute Assignment
Algorithm 1 A simple algorithm to assign attributes to phonemes
input : X-SAMPA representation of phoneme P
output: Phonetic attribute set A ⊆ Aphone for p
A J empty set
while p 6∈ Pbase do
find the longest suffix ps ∈ Pbase
Add fP	(ps) toA
base
Remove suffix ps from p
end
Add f P	(p) to A
base
Next, we need to assign each phoneme with appropriate attributes. There are multiple approaches
to retrieve phonetic attributes. The most simple one is to use tools to collect articulatory features
for each phoneme (Mortensen et al., 2016; Moran et al., 2014). However, those tools only provide
coarse-grained phonological features and we expect more fine-grained and customized phonetic
features. In this section, we propose a naive but useful approach for attribute assignment. We note
that we use X-SAMPA format to denote each IPA (Decker et al., 1999) in this work. X-SAMPA was
devised to produce a computer-readable representation for IPA (Wells, 1995). Each IPA segment can
3
Under review as a conference paper at ICLR 2019
Figure 2: Illustration of the sequence model for zero-shot learning. The input layer is first processed
with a Bidirectional LSTM acoustic model, and produces a distribution over phonetic attributes.
Then it is transformed into a phoneme distribution by a language dependent signature matrix S
be mapped to X-SAMPA with appropriate rule-based tools (Mortensen et al., 2018). For example,
IPA /@/ can be represented as @ in X-SAMPA.
The assignment can be formulated as a problem to construct an assignment function f : Pxsampa →
2Aphone where Pxsampa is the set of all valid X-SAMPA phonemes. The assignment function should
map each phone into its corresponding subset of Aphone . To construct the function in the entire
domain, we first manually mapped a small subset Pbase ⊂ Pxsampa and constructed a restricted
assignment function f . The mapping is customizable and has been verified with the IPA
Pbase
handbook(Decker et al., 1999). Then for every p ∈ Pxsampa, we continue to remove diacritics
suffix from it until it could be find in Pbase. For example, to recognize ts_>, We can first match the
suffix, _>as an ejective, and then recognize ts as a consonant defined in Pbase. The Algorithm 1
summarizes our approach, and the mapping of f is provided in Appendix.
Pbase
2.2	Sequence model for zero-shot learning
Zero-shot learning has rarely been applied to sequence prediction problems so far, in this section We
describe a novel sequence model architecture for zero-shot learning. We adapt a modified ESZSL
architecture from Romera-Paredes & Torr (2015). While the original architecture is devised to solve
the classification problem With CNN(DECAF) features (Donahue et al., 2014), our model aims to
optimize a CTC loss over a sequence model as shoWn in Figure 2. We note our architecture is a
general model, and it can also be used for other sequence prediction problems in zero-shot learning.
Given the training set S = {(xn, yn), n = 1...N} Where each input xn ∈ X is an utterance, and
yn ∈ Y is the corresponding phonetic transcription. Suppose that xn = (x1n, ..., xTn ) is the input
sequence Where xtn is the frame of time step t, and T is the length of xn . Each frame xtn is first
projected into a feature vector htn ∈ Rd in the acoustic space H With a Bidirectional LSTM model.
htn = θ(xtn ; WLSTM)	(2)
Where WLSTM is the parameter of Bidirectional LSTM model. We assume that our phoneme in-
ventory consists of z phonemes in the training set, each of them having a signature of a attributes
constructed as mentioned above. Then We can represent our attributes in a constant signature matrix
S ∈ [0, 1]z×a. Then, We transform htn into points in the phonetic space P With V ∈ Ra×d, and
further into the phoneme logits ltn With S.
lnt = SV htn	(3)
The logits ln = (ln1 , ..., lTn ) is then combined With yn to compute the CTC loss (Graves et al.,
2006). Additionally, regularizing V has been proved to be useful in the original ESZSL architec-
ture (Romera-Paredes & Torr, 2015). So our target is to minimize folloWing loss function.
minimiZeCTC(xn, Nn; V,Wlstm) + Ω(V)	(4)
V,WLSTM
4
Under review as a conference paper at ICLR 2019
where Ω(V) is an simple L2 regularization. This objective can be easily optimized using standard
gradient descent methods.
At inference stage, we usually consider a new language Ltest with a new phoneme inventory. Sup-
pose that the new inventory is composed of z0 classes, then we can create a new signature matrix
S0 ∈ [0, 1]z0×a, and estimate probability distribution of each phoneme Pacoustic(p|xtn) from log-
its using S0 instead of S . Additionally, we expect the phoneme should distribute similarly to the
phoneme distribution in Ltest. We adjusted the posterior as follows.
Pupm (p|xn ) = Pacoustic (p|xn)P (p|Ltest)	(5)
where α is a hyperparameter, and P(p|Ltest) is the prior distribution of Ltest which can be easily
estimated from a small set of the text corpus in Ltest (we used 1000 sentences here). Finally we
exploit greedy decoding to extract phoneme from the adjusted posterior Pupm(Graves et al., 2006).
3	Experiments
3.1	Dataset
We prepared two datasets for this experiment. The training set consists of three English corpora,
and the test set is composed of corpora from 20 languages. They are used by both our model and
the baseline model described later. Details regarding each corpus and phonetic information about
each language are provided at Table 1. Our model is trained with three English corpora because we
tried to make the acoustic model robust to various channels and speech styles: TED (Rousseau et al.,
2012) is the conference style, Switchboard (Godfrey et al., 1992) is the spontaneous conversation
style and Librispeech is reading style (Panayotov et al., 2015). We note that 5 percent of the entire
corpus was used as the validation set. The evaluation set were selected from a variety of languages:
not only from well-resourced languages, but also low-resourced languages from Asia and Africa.
We list phoneme details for each language on the right sides in Table 1. All the phonemes were
obtained using Epitran (Mortensen et al., 2018). The phonemes column shows the number of distinct
phonemes in each language, the shared column is the number of shared phonemes with English, and
the unseen column indicates the number of phonemes unseen in English, which means those unseen
phonemes are not available in the training set.
3.2	Experimental Settings
We used the EESEN framework for the acoustic modeling (Miao et al., 2015). Each corpus is
first re-sampled to 8000Hz, and we extracted 40 dimension Mel-frequency cepstral coefficients
(MFCCs) (Davis & Mermelstein, 1990) from each utterance, the length of each frame is 25ms,
and the shift between two continuous frames is 10ms. All the transcripts were transcribed into
phonemes with Epitran (Mortensen et al., 2018). We used a 5 layer Bidirectional LSTM model,
each layer having 320 cells. The signature matrix is designed as we discussed above, and we used
different signature matrices for different languages. We train the acoustic model with stochastic
gradient descent, using a learning rate of 0.005. The prior distribution P (p|Ltest) over phonemes is
estimated by taking 1000 randomly selected sentences in the test language. Those 1000 sentences
were removed from the test corpus. We fixed prior hyperparameter α = 1 for all evaluations.
A standard English acoustic model is used as our baseline model (Miao et al., 2015). Phoneme
distribution P (p|xtn) is estimated with only Bidirectional LSTM θ(xtn; WLSTM). Then we decode
phonemes with greedy decoding as we used in our approach. We use the same configuration of
LSTM architecture as well as the training criterion. We also tried applying prior of target language
to our baseline model as well. However, it was not helpful to improve the results because the
phoneme inventory of English and target language are different, thus unique phonemes in target
language cannot benefit from the prior distribution. As we focus on acoustic modeling in this work,
we use phone error rate as the metric for evaluation.
3.3	Results
Our results are summarized in Table 2. As it shows, our approach has consistently outperformed
the baseline in terms of phone error rate. For example, the baseline achieves 77.3% phone error rate
5
Under review as a conference paper at ICLR 2019
Language	Corpus Name	# Phonemes	# Shared	#Unseen
English	TED (Rousseau et al., 2012)	38	38	0
English	Switchboard (Godfrey et al., 1992)	38	38	0
English	Librispeech (Panayotov et al., 2015)	38	38	0
Amharic	ALFFA Amharic (Tachbelie et al., 2014)	57	26	31
Bengali	OpenSLR37 (Gutkin et al.)	70	38	32
Cebuano	IARPA-babel301b-v2.0b	39	38	1
Dutch	Voxforge	49	34	15
French	Voxforge	42	33	9
German	Voxforge	37	25	12
Haitian	IARPA-babel201b-v0.2b	40	37	3
Italian	Voxforge	43	26	17
Javanese	OpenSLR35	45	36	9
Kazakh	IARPA-babel302b-v1.0a	45	36	9
Kurmanji	IARPA-babel205b-v1.0a	27	23	4
Lao	IARPA-babel203b-v3.1a	39	21	18
Mongolian	IARPA-babel401b-v2.0b	50	18	32
Russian	Voxforge	48	22	26
Sinhala	openSLR52	50	38	12
Tagalog	IARPA-babel106b-v0.2g	25	24	1
Turkish	IARPA-babel105b-v0.4	40	38	2
Swahili	ALFFA Swahili (Gelas et al., 2012)	47	38	9
Vietnamese	IARPA-babel107b-v0.7	47	37	10
Zulu	IARPA-babel206b-v0.1e	40	25	15
Table 1: Corpus of training set and test set used in the experiment. The acoustic model is trained
with three English corpus, and tested on the 20 other languages below.
when evaluated with Amharic, and our approach obtained 68.7% in the same test set. For each lan-
guage in our evaluation, we observed that we improved the phone error rate from 0.8% (French) to
21.7% (Italian) respectively. On average, the baseline model has 82.9% and our model get 9.9 % bet-
ter phone error rate. The table also indicates one interesting relationship across different languages.
The language which has close linguistic relationship with English tends to obtain better phone error
rate. Dutch and German are classified in the West Germanic branch in the Indo-European language
family like English. For Dutch, we obtained a 64.5% phone error rate and for German, a 61.6%
phone error rate, which are the fourth and third best phone error rates among 20 languages. Addi-
tionally Italian and Russian belong to other branches of the Indo-European language family, but they
also have very good phone error rate: 50.8% and 59.6% which are the first and second best phone
error rates.
On the other hand, languages which are not a member of the Indo-European family tend to have
worse phone error rate. The languages whose UPM phone error rate is worse than 80% are Lao,
French, Zulu, Vietnamese. All languages except French are members of a different language family.
Lao is in the KraDai language family, Zulu belongs to NigerCongo language family, and Vietnamese
is a member of Austroasiatic language family. While French is a Indo-European language, its per-
formance is much worse than other members of the language family. The reason is that French is
a language known for its ambiguous orthographies (Ziegler et al., 1996), which makes it harder to
make rule based grapheme-to-phoneme mappings. As our phonetic tool is designed as a rule-based
grapheme-to-phoneme mappings, the prediction of French phonemes is difficult.
To further investigate the reason of improvements for our model, we computed the (phone) substi-
tution error rate, shown in the two right columns of Table 2. It indicates that our model improves
significantly over substitution errors: it goes down from 52.0% in baseline model to 38.2% in our
model. The numbers shows that we have 13.8% improvement in substitution error rate. The im-
provements suggests that our model is good at recognizing confusing phonemes. Given the fact that
largest improvement here is Italian and 3rd largest one is Russian, we can infer that the model is
especially robust at recognizing phones in the same language family. We note that this number also
6
Under review as a conference paper at ICLR 2019
Language	Baseline PER%	UPM PER%	Baseline Sub%	UPM Sub%
Amharic	77.3	68.7	53.1	37.6
Bengali	82.6	67.3	62.9	38.2
Cebuano	88.7	72.1	48.9	27.2
Dutch	70.8	64.5	53.7	43.7
French	84.1	83.3	57.6	57.4
German	71.0	61.6	48.7	31.0
Haitian	84.5	74.4	54.5	41.5
Italian	72.5	50.8	56.4	27.5
Javanese	86.4	79.4	45.8	37.9
Kazakh	89.0	79.3	50.9	39.8
Kurmanji	91.3	79.1	46.6	30.5
Lao	87.9	85.6	55.2	54.8
Mongolian	87.6	79.8	43.7	28.6
Russian	73.4	59.6	55.2	33.9
Sinhala	76.3	71.2	55.3	45.5
Tagalog	86.2	70.9	46.6	25.7
Turkish	84.4	75.1	49.5	43.2
Swahili	83.8	70.8	45.3	27.0
Vietnamese	85.2	82.1	54.3	51.6
Zulu	95.8	85.1	56.5	41.9
Average	82.9	730	52.0	38.2
Table 2: Phone error rate (%PER) and phone substitution error rate (%Sub) of the baseline model,
and our approach. Our model (UPM) outperforms the baseline model for all languages, by 9.9%
(absolute) in phone error rate, and 13.8% in phone substitution error rate.
suggests addition and insertion error rate were getting worse by 3.9% than the baseline model, but
our improvement in substitution error is enough to compensate for it.
labels	phonemes
Gold Labels baseline Prediction UPM Prediction	biks' e b_tkar irah\ amLd ats\ aLd ae.^ bIkOr∖ejth { mlEkEli bikoean_dLdakaLd e
Table 3: A comparison of predictions in Bengali test set. The phonemes in bold font are unseen ones
in English set, indicating our model’s capability at predicting unseen phonemes.
To highlight the ability of our model to predict unseen phonemes, we show an example of Bengali in
Table 3. According to Table 1, Bengali has the largest number of unseen phonemes among our test
languages. Table 3 marks the unseen phonemes in English using bold font. The row of gold labels
shows that target labels contain 7 unseen phonemes out of 22 phonemes. The baseline model was
able to recognize part of seen phonemes such as the first phoneme /b/, however it fails to recognize
all unseen phonemes due to its limitation. On the contrary, our model could correctly recognize two
unseen phonemes /l_d/ which aligned with two /Ld/ phonemes in the gold labels. Random examples
from other languages are available in Appendix.
4	Related Work
We briefly outline several areas of related works, and describe their connections and differences
with this paper. Firstly zero-shot learning has been applied to recognize unseen objects during
training in the computer vision field. One line of works has focused on two-stage approach (Lampert
et al., 2009; Palatucci et al., 2009; Lampert et al., 2014; Jayaraman & Grauman, 2014; Al-Halah
7
Under review as a conference paper at ICLR 2019
et al., 2016; Liu et al., 2011). Another line of approaches adapts multimodal models for zero-shot
learning (Socher et al., 2013; Frome et al., 2013; Huang et al., 2012; Lei Ba et al., 2015; Rohrbach
et al., 2011). Socher et al. (2013). However those works rarely mention speech recognition.
There has been growing interests in zero-resource speech processing (Glass, 2012; Jansen et al.,
2013), most of the work focusing on tasks like acoustic unit discovery (Heck et al., 2017), unsu-
pervised segmentation (Kamper et al., 2017), representation learning (Lake et al., 2014; Hermann
& Goldwater, 2018), spoken term discovery (Dunbar et al., 2017; Jansen et al., 2013). These mod-
els are useful for various extrinsic speech processing tasks like topic identification (Kesiraju et al.,
2017; Liu et al., 2017). However, those unsupervised concepts cannot be grounded to actual words
or phonemes, hence making it impracticable to do speech recognition or acoustic modeling. The
usual intrinsic evaluations that these zero resource tasks are tested on is ABX discriminability task
(Dunbar et al., 2017; Heck et al., 2017) or the unsupervised word error rate (Ludusan et al., 2014;
Kamper et al., 2017) which are good for quality estimates but not practical as they use an oracle or
ground truth labels to assign cluster labels. Secondly these approaches demands a modest size of
audio corpus of targeting language (e.g: 2.5h to 40h). In contrast, our approach assumes no audio
corpus but a small set of text corpus to estimate phone prior. This is a reasonable assumption as text
corpus is usually easier to obtain than audio corpus. The idea of decomposing speech into concepts
was also discussed by Lake et al. (2014), where the authors propose a generative model to learn
representations for spoken words which they then use to classify words with only one training sam-
ple available per word. Though this is also in the same line as the zero-resource speech processing
papers, we feel the motivation behind the decomposition is very similar to this work.
The authors in Scharenborg et al. (2017) focus on a pipeline towards adaptation to a low resource lan-
guage with little training data, they present an interesting method to map Dutch/English phonemes
in the same space using an extrapolation approach to predict phones in English that are unseen in
Dutch. Our work proposes a generic algorithm to recognize any unknown phones by decomposing
them into its phone attributes. We have also shown that our approach is effective over 20 languages
from different language families.
Another group of researchers explore adaptation techniques for multilingual speech recognition,
especially for low resource languages. In these multilingual settings, the hidden layers are either
HMM or DNN models which are shared by multiple languages, and the output layer is either lan-
guage specific phone set or a universal IPA-based phone set (Tong et al., 2017; Vu & Schultz, 2013;
Tuske et al., 2013; Thomas et al., 2010; VU et al., 2014; Lin et al., 2009; Chen & Mak, 2015; Dalmia
et al., 2018b). However predictable phonemes are restricted to the phonemes in the training set,
thus they fail to predict unseen phonemes in the test set. In contrast, our model can predict unseen
phonemes by taking advantage of their phonetic attributes.
Articulatory features have been shown to be useful in speech recognition under several situation.
For example, articulatory features has been used to improve robustness under noisy and reverberant
environment (Kirchhoff, 1998), compensate for crosslingual variability (Stuker et al., 2003b), im-
prove word error rate in multilingual models (Stuker et al., 2003a), be beneficial for low resource
languages (Muller et al., 2016), clustering phoneme-like units for unwritten languages (Muller et al.,
2017). Those approaches generally treat articulatory features as additional feature for classifications,
and did not provide a model to predict unseen phones.
5	Conclusion and Future Work
In this work, we propose the Universal Phonetic Model to apply zero-shot learning to acoustic mod-
els in speech recognition. Our experiment shows that it outperforms the baseline by 9.9 % phone
error rate on average for 20 languages. While the performance of our approach is still not practical
in the actual applications, it paves the way to tackle zero-shot learning of speech recognition with
a new framework. We note that several approaches can be investigated on top of this framework.
For instance, multilingual acoustic features can be used instead of standard ones (Hermann & Gold-
water, 2018; Dalmia et al., 2018a). Label smoothing can be applied to regularize distributions of
phonemes or attributes (Pereyra et al., 2017). The training objective can be replaced with discrimi-
native training objectives such as MMI and sMBR (Vesely et al., 2013; Povey). Additionally, various
encoder and decoder models can be explored with this framework (Chan et al., 2016; Zhou et al.,
2018).
8
Under review as a conference paper at ICLR 2019
References
Ziad Al-Halah, Makarand Tapaswi, and Rainer Stiefelhagen. Recovering the missing link: Predict-
ing class-attribute associations for unsupervised zero-shot learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 5975-5984, 2016.
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl
Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-
to-end speech recognition in english and mandarin. In International Conference on Machine
Learning, pp. 173-182, 2016.
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural
network for large vocabulary conversational speech recognition. In Acoustics, Speech and Signal
Processing (ICASSP), 2016 IEEE International Conference on, pp. 4960-4964. IEEE, 2016.
Dongpeng Chen and Brian Kan-Wing Mak. Multitask learning of deep neural networks for low-
resource speech recognition. IEEE/ACM Transactions on Audio, Speech and Language Process-
ing (TASLP), 23(7):1172-1183, 2015.
Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve. Wav2letter: an end-to-end convnet-
based speech recognition system. arXiv preprint arXiv:1609.03193, 2016.
Siddharth Dalmia, Xinjian Li, Florian Metze, and Alan W Black. Domain robust feature extraction
for rapid low resource asr development. arXiv preprint arXiv:1807.10984, 2018a.
Siddharth Dalmia, Ramon Sanabria, Florian Metze, and Alan W Black. Sequence-based multi-
lingual low resource speech recognition. In 2018 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 4909-4913. IEEE, 2018b.
Steven B Davis and Paul Mermelstein. Comparison of parametric representations for monosyllabic
word recognition in continuously spoken sentences. In Readings in speech recognition, pp. 65-74.
Elsevier, 1990.
Donald M Decker et al. Handbook of the International Phonetic Association: A guide to the use of
the International Phonetic Alphabet. Cambridge University Press, 1999.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Inter-
national conference on machine learning, pp. 647-655, 2014.
Ewan Dunbar, Xuan Nga Cao, Juan Benjumea, Julien Karadayi, Mathieu Bernard, Laurent Besacier,
Xavier Anguera, and Emmanuel Dupoux. The zero resource speech challenge 2017. In Automatic
Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE, pp. 323-330. IEEE, 2017.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise:
A deep visual-semantic embedding model. In Advances in neural information processing systems,
pp. 2121-2129, 2013.
Hadrien Gelas, LaUrent Besacier, and Francois Pellegrino. Developments of SWahili resources for
an automatic speech recognition system. In Spoken Language Technologies for Under-Resourced
Languages, 2012.
Jost Gippert, Nikolaus Himmelmann, Ulrike Mosel, et al. Essentials of language documentation,
volume 178. Walter de gruyter, 2006.
James Glass. ToWards unsupervised speech processing. In Information Science, Signal Processing
and their Applications (ISSPA), 2012 11th International Conference on, pp. 1-4. IEEE, 2012.
John J Godfrey, EdWard C Holliman, and Jane McDaniel. SWITCHBOARD: Telephone speech cor-
pus for research and development. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-
92., 1992 IEEE International Conference on, volume 1, pp. 517-520. IEEE, 1992.
Alex Graves, Santiago Fernandez, Faustino Gomez, and JUrgen Schmidhuber. Connectionist tem-
poral classification: labelling unsegmented sequence data With recurrent neural netWorks. In Pro-
ceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.
9
Under review as a conference paper at ICLR 2019
Alexander Gutkin, Linne Ha, Martin Jansche, Knot Pipatsrisawat, and Richard Sproat. TTS for low
resource languages: A bangla synthesizer.
Michael Heck, Sakriani Sakti, and Satoshi Nakamura. Feature optimized dpgmm clustering for un-
supervised subword modeling: A contribution to zerospeech 2017. In Automatic Speech Recog-
nition and Understanding Workshop (ASRU), 2017 IEEE,pp. 740-746. IEEE, 2017.
Enno Hermann and Sharon Goldwater. Multilingual bottleneck features for subword modeling in
zero-resource languages. arXiv preprint arXiv:1803.08863, 2018.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word rep-
resentations via global context and multiple word prototypes. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pp. 873-882.
Association for Computational Linguistics, 2012.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwater, Mark Johnson, Sanjeev Khudanpur, Kenneth
Church, Naomi Feldman, Hynek Hermansky, Florian Metze, Richard Rose, et al. A summary of
the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language
acquisition. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing,
pp. 8111-8115. IEEE, 2013.
Dinesh Jayaraman and Kristen Grauman. Zero-shot recognition with unreliable attributes. In Ad-
vances in neural information processing systems, pp. 3464-3472, 2014.
Herman Kamper, Karen Livescu, and Sharon Goldwater. An embedded segmental k-means model
for unsupervised segmentation and clustering of speech. In Automatic Speech Recognition and
Understanding Workshop (ASRU), 2017 IEEE, pp. 719-726. IEEE, 2017.
Naoyuki Kanda, Ryu Takeda, and Yasunari Obuchi. Elastic spectral distortion for low resource
speech recognition with deep neural networks. In Automatic Speech Recognition and Under-
standing (ASRU), 2013 IEEE Workshop on, pp. 309-314. IEEE, 2013.
Santosh Kesiraju, Raghavendra Pappagari, Lucas OndeL Lukas Burget, Najim Dehak, Sanjeev Khu-
danpur, Jan Cernocky, and Suryakanth V Gangashetty. Topic identification of spoken docu-
ments using unsupervised acoustic unit discovery. In Acoustics, Speech and Signal Processing
(ICASSP), 2017 IEEE International Conference on, pp. 5745-5749. IEEE, 2017.
Katrin Kirchhoff. Combining articulatory and acoustic information for speech recognition in noisy
and reverberant environments. In Fifth International Conference on Spoken Language Processing,
1998.
Peter Ladefoged and Keith Johnson. A course in phonetics. Nelson Education, 2014.
Brenden Lake, Chia-ying Lee, James Glass, and Josh Tenenbaum. One-shot learning of genera-
tive speech concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society,
volume 36, 2014.
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object
classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pp. 951-958. IEEE, 2009.
Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for
zero-shot visual object categorization. IEEE Transactions on Pattern Analysis and Machine In-
telligence, 36(3):453-465, 2014.
Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al. Predicting deep zero-shot convolutional neural
networks using textual descriptions. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 4247-4255, 2015.
M Paul Lewis. Ethnologue: Languages of the world. SIL international, 2009.
Hui Lin, Li Deng, Dong Yu, Yi-fan Gong, Alex Acero, and Chin-Hui Lee. A study on multilingual
acoustic modeling for large vocabulary ASR. 2009.
10
Under review as a conference paper at ICLR 2019
Chunxi Liu, Jan Trmal, Matthew Wiesner, Craig Harman, and Sanjeev Khudanpur. Topic identifi-
cation for speech without asr. arXiv preprint arXiv:1703.07476, 2017.
Jingen Liu, Benjamin Kuipers, and Silvio Savarese. Recognizing human actions by attributes. In
Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 3337-3344.
IEEE, 2011.
Bogdan Ludusan, Maarten Versteegh, Aren Jansen, Guillaume Gravier, Xuan-Nga Cao, Mark John-
son, Emmanuel Dupoux, et al. Bridging the gap between speech technology and natural language
processing: an evaluation toolbox for term discovery systems. 2014.
Andrew Maas, Ziang Xie, Dan Jurafsky, and Andrew Ng. Lexicon-free conversational speech
recognition with neural networks. In Proceedings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language Technologies,
pp. 345-354, 2015.
Yajie Miao, Mohammad Gowayyed, and Florian Metze. EESEN: End-to-end speech recognition
using deep RNN models and WFST-based decoding. In Automatic Speech Recognition and Un-
derstanding (ASRU), 2015 IEEE Workshop on, pp. 167-174. IEEE, 2015.
Steven Moran, Daniel McCloy, and Richard Wright. PHOIBLE online. Leipzig: Max Planck
Institute for Evolutionary Anthropology, 2014.
David R Mortensen, Patrick Littell, Akash Bharadwaj, Kartik Goyal, Chris Dyer, and Lori Levin.
Panphon: A resource for mapping ipa segments to articulatory feature vectors. In Proceedings
of COLING 2016, the 26th International Conference on Computational Linguistics: Technical
Papers, pp. 3475-3484, 2016.
David R Mortensen, Siddharth Dalmia, and Patrick Littell. Epitran: Precision G2P for many lan-
guages. In LREC, 2018.
Markus Muller, Sebastian Stuker, and Alex WaibeL Towards improving low-resource speech recog-
nition using articulatory and language features. In Proceedings of the 11th International Workshop
on Spoken Language Translation (IWSLT), 2016.
Markus Muller, Jorg Franke, Sebastian Stuker, and Alex Waibel. Improving phoneme set discovery
for documenting unwritten languages. Elektronische Sprachsignalverarbeitung (ESSV), 2017,
2017.
Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell. Zero-shot learning with
semantic output codes. In Advances in neural information processing systems, pp. 1410-1418,
2009.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR cor-
pus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP),
2015 IEEE International Conference on, pp. 5206-5210. IEEE, 2015.
Gabriel Pereyra, George Tucker, Jan ChoroWski, Eukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548,
2017.
Daniel Povey. Discriminative training for large vocabulary speech recognition. PhD thesis.
Anton Ragni, Kate M Knill, Shakti P Rath, and Mark JF Gales. Data augmentation for low re-
source languages. In Fifteenth Annual Conference of the International Speech Communication
Association, 2014.
Marcus Rohrbach, Michael Stark, and Bernt Schiele. Evaluating knowledge transfer and zero-shot
learning in a large-scale setting. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
Conference on, pp. 1641-1648. IEEE, 2011.
Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learn-
ing. In International Conference on Machine Learning, pp. 2152-2161, 2015.
11
Under review as a conference paper at ICLR 2019
Anthony Rousseau, Paul Deleglise, and Yannick Esteve. TED-LIUM: an automatic speech recogni-
tion dedicated corpus. In LREC, pp. 125-129, 2012.
Odette Scharenborg, Patrick W. Ebel, Francesco Ciannella, Mark Hasegawa-Johnson, and Najim
Dehak. Building an asr system for mboshi using a cross-language definition of acoustic units
approach. 2017.
Tanja Schultz and Alex Waibel. Language-independent and language-adaptive acoustic modeling
for speech recognition. Speech Communication, 35(1-2):31-51, 2001.
Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning
through cross-modal transfer. In Advances in neural information processing systems, pp. 935-
943, 2013.
Themos Stafylakis and Georgios Tzimiropoulos. Zero-shot keyword spotting for visual speech
recognition in-the-wild. arXiv preprint arXiv:1807.08469, 2018.
Sebastian Stuker, Florian Metze, Tanja Schultz, and Alex WaibeL Integrating multilingual articula-
tory features into speech recognition. In Eighth European Conference on Speech Communication
and Technology, 2003a.
Sebastian Stuker, Tanja Schultz, Florian Metze, and Alex Waibel. Multilingual articulatory fea-
tures. In Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03). 2003 IEEE
International Conference on, volume 1, pp. I-I. IEEE, 2003b.
Martha Yifiru Tachbelie, Solomon Teferra Abate, and Laurent Besacier. Using different acoustic,
lexical and language modeling units for ASR of an under-resourced language-amharic. Speech
Communication, 56:181-194, 2014.
Samuel Thomas, Sriram Ganapathy, and Hynek Hermansky. Cross-lingual and multi-stream poste-
rior features for low resource LVCSR systems. In Eleventh Annual Conference of the International
Speech Communication Association, 2010.
Sibo Tong, Philip N Garner, and Herve Bourlard. An investigation of deep neural networks for
multilingual speech recognition training and adaptation. Technical report, 2017.
Zoltan Tuske, Joel Pinto, Daniel Willett, and Ralf Schluter. Investigation on cross-and multilingual
MLP features under matched and mismatched acoustical conditions. In Acoustics, Speech and
Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 7349-7353. IEEE,
2013.
Maarten Versteegh, Roland Thiolliere, Thomas Schatz, Xuan Nga Cao, Xavier Anguera, Aren
Jansen, and Emmanuel Dupoux. The zero resource speech challenge 2015. In Sixteenth Annual
Conference of the International Speech Communication Association, 2015.
Karel Vesely, Arnab Ghoshal, LUkaS Burget, and Daniel Povey. Sequence-discriminative training of
deep neural networks. In Interspeech, pp. 2345-2349, 2013.
Ngoc Thang Vu and Tanja Schultz. Multilingual multilayer perceptron for rapid language adaptation
between and across language families. In Interspeech, pp. 515-519, 2013.
Ngoc Thang Vu, David Imseng, Daniel Povey, Petr Motlicek, Tanja Schultz, and HerVe Bourlard.
Multilingual deep neural network based acoustic modeling for rapid language adaptation. In
Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pp.
7639-7643. IEEE, 2014.
John C Wells. Computer-coding the IPA: a proposed extension of SAMPA. Revised draft, 4(28):
1995, 1995.
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Michael Seltzer, Andreas Stolcke,
Dong Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition.
IEEE/ACM Transactions on Audio, Speech, and Language Processing, PP, 2016.
12
Under review as a conference paper at ICLR 2019
Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu. Syllable-based sequence-to-sequence speech
recognition with the transformer in mandarin chinese. arXiv preprint arXiv:1804.10752, 2018.
Johannes C Ziegler, Arthur M Jacobs, and Gregory O Stone. Statistical analysis of the bidirec-
tional inconsistency of spelling and sound in french. Behavior Research Methods, Instruments, &
Computers, 28(4):504-515,1996.
13
Under review as a conference paper at ICLR 2019
A
Phonetic Attribute full list
vowel consonant alveolar affricate alveolo-palatal approximant aspirated back bilabial breathy-voice
central click close close-mid dental ejective epiglottal flap fricative front glottal implosive labial-
palatal labial-velar labial labiodental lateral long nasal near-close near-open non-syllabic open open-
mid palatal palatal-velar pharyngeal plosive postalveolar retroflex rounded schwa stop syllabic trill
unrounded uvular velar voiced voiceless vowel coronal dorsal blank
14
Under review as a conference paper at ICLR 2019
B
Phonemes and Attributes
phoneme I	attributes
abκbcd<1/ efdfg8-<hhξi∙J八 k Ir 八 mnn oppξqrr≤∖jls∖*trlsiuvwxxξyzzzξ
vowel open front unrounded
consonant voiced bilabial stop labial
consonant voiced bilabial stop implosive labial
consonant voiceless palatal stop dorsal
consonant voiced alveolar plosive coronal
consonant voiced retroflex plosive coronal
consonant voiced alveolar implosive coronal
vowel close-mid front unrounded
consonant voiceless labiodental fricative labial
consonant voiceless labiodental fricative bilabial plosive labial
consonant voiced velar plosive dorsal
consonant voiced velar implosive dorsal
consonant voiceless glottal fricative
consonant voiced glottal fricative
vowel close front unrounded
consonant voiced palatal approximant dorsal
consonant voiced palatal fricative dorsal
consonant voiceless velar plosive dorsal
consonant voiced alveolar lateral approximant coronal
consonant voiced retroflex lateral approximant coronal
consonant alveolar lateral flap coronal
consonant bilabial nasal labial
consonant voiced alveolar nasal coronal
consonant retroflex nasal coronal
vowel close-mid back rounded
consonant voiceless bilabial plosive labial
consonant voiceless bilabial fricative labial
consonant voiceless uvular plosive
consonant alveolar trill coronal
consonant retroflex flap coronal
consonant alveolar approximant coronal
consonant retroflex approximant coronal
consonant voiceless alveolar fricative coronal
consonant voiceless retroflex fricative coronal
consonant voiceless alveolo-palatal fricative coronal
consonant voiceless alveolar plosive coronal
consonant voiceless retroflex plosive coronal
consonant voiceless retroflex plosive fricative coronal
vowel close back rounded
consonant voiced labiodental fricative labial
consonant labial-velar approximant labial
consonant voiceless velar fricative dorsal
consonant voiceless palatal-velar fricative dorsal
vowel close front rounded
consonant voiced alveolar fricative coronal
consonant voiced retroflex fricative coronal
consonant voiced alveolo-palatal fricative
Table 4: phonemes and attributes
15
Under review as a conference paper at ICLR 2019
phoneme	attributes
9 G F E D C 巴 B A	vowel open back unrounded consonant voiced bilabial fricative labial consonant bilabial trill labial consonant voiceless palatal fricative consonant voiced dental fricative coronal vowel open-mid front unrounded consonant labiodental nasal labial consonant voiced velar fricative dorsal consonant voiced uvular plosive dorsal
GY	consonant voiced uvular implosive
H	consonant labial-palatal approximant labial
H\	consonant voiceless epiglottal fricative
I	vowel near-close front unrounded
I\	vowel near-close central unrounded
J	consonant palatal nasal
J\	consonant voiced palatal plosive
JY	consonant voiced palatal implosive
K	consonant voiceless alveolar lateral fricative coronal
K\	consonant voiced alveolar lateral fricative coronal
L	consonant palatal lateral approximant
L\	consonant velar lateral approximant dorsal
M	vowel close back unrounded
M\	consonant velar approximant dorsal
N	consonant velar nasal dorsal
N\	consonant uvular nasal dorsal
O	vowel open-mid back rounded
O\	consonant bilabial click labial
P	consonant labiodental approximant labial
Q	vowel open back rounded
R	consonant voiced uvular fricative
R\	consonant uvular trill
S	consonant voiceless postalveolar fricative coronal
T	consonant voiceless dental fricative coronal
U	vowel near-close back rounded
U\	vowel near-close central rounded
V	vowel open-mid back unrounded
W	consonant voiceless labial-velar fricative labial
X	consonant voiceless uvular fricative
X\	consonant voiceless pharyngeal fricative
Y	vowel near-close front rounded
Z	consonant voiced postalveolar fricative coronal
Table 5: phonemes and attributes
16
Under review as a conference paper at ICLR 2019
phoneme	attributes
	long
’	palatal
@	vowel close-mid open-mid central rounded unrounded
{	vowel near-open front unrounded
}	vowel close central rounded
1	vowel close central unrounded
2	vowel close-mid front rounded
3	vowel open-mid central unrounded
4	consonant alveolar flap coronal
5	consonant velar alveolar lateral approximant coronal dorsal
6	vowel near-open central
7	vowel close-mid back unrounded
8	vowel close-mid central rounded
9	vowel open-mid front rounded
&	vowel open front rounded
?	consonant glottal stop
?\	consonant voiced pharyngeal fricative
—\	consonant dental click coronal
—\—\	consonant alveolar lateral click coronal
=\	consonant palatal click
一＜	implosive
,>	ejective
~	nasal
_h	aspirated
_w	labial
,t	breathy-voice
	non-syllabic
,d	dental coronal
ts	consonant voiceless alveolar affricate coronal
dz	consonant voiced alveolar affricate coronal
dz\	consonant voiced alveolo-palatal affricate coronal
dZ	consonant voiced postalveolar affricate coronal
dZ\	consonant voiced alveolo-palatal affricate coronal
tS ts\	consonant voiceless postalveolar affricate coronal consonant voiceless alveolo-palatal affricate coronal
s\	consonant voiceless palatal fricative dorsal
tK	consonant voiceless alveolar lateral affricate coronal
Table 6: phonemes and attributes
17
Under review as a conference paper at ICLR 2019
labels	∣ phonemes
Amharic Gold Amharic Baseline Amharic UPM	1n1s1rawt@S@n@ k _> or @ VntVr∖VVteSVnopkwet ntatej anok_>_w wat
Bengali Gold Bengali Baseline Bengali UPM	ebis‘ e.^otike IbIdZojdigej e b i dz\ oige
Cebuano Gold Cebuano Baseline Cebuano UPM	awlagiaN Vij { aj mn Atbih onaij ajmnatbiha
Dutch Gold Dutch Baseline Dutch UPM	tP e: e: PlY Str i: e: IsvEjf dZejpIzdVtiISajd dZHeHpHIzdVt i: I S a: j d
French Gold French Baseline French UPM	ilEstsyspEndyd@d@lalEgj O~ ilEtjispAwndiDVr∖AbADVlElIZVnVnr∖ HilEtHisHaHndlHidH A HH A nElElid A Hn A nE
German Gold	imj a: rvurd @d@ ralg o: ritmusbeSr i: eb @n
German Baseline	mijA r\n OtVnVnDVsiwVnsipsIkwo dDE r\A gIdmowstbISr∖ivVn
German UPM	mianatinindtsiminsisikwotde: agtmostbislivin
Haitian Gold	wimpak O~ nsapumwfesimteb O~ n v i w O~ msaasemmZ
Haitian Baseline	WibAkVnbAtInsVsVmdEbVnAldZor∖mA TOfjUsVmEmVzOl
Haitian UPM	WiPakemPatmsesemdePenaljolm O~ s aj O~ s ememi O~ l
Italian Gold Italian Baseline Italian UPM	guardabenetukehailavistapiuakutadime wVVbinitukej l { vistVpj Or∖kutVdimej Vetabenitukelevistapiokutadimen
Javanese Gold Javanese Baseline Javanese UPM	ij aikiij aij ad_de wi VEj {j { Ir\e iaJd' aai
Kazakh Gold Kazakh Baseline Kazakh UPM	AllwUAl i-^ @jkwmAssAlAwm oww El Aww i-^ l H A
Table 7: A list of random samples from the first 10 test languages. Bold phonemes are unseen
phonemes in English
18