Under review as a conference paper at ICLR 2019
On Difficulties of Probability Distillation
Anonymous authors
Paper under double-blind review
Ab stract
Probability distillation has recently been of interest to deep learning practitioners
as it presents a practical solution for sampling from autoregressive models for de-
ployment in real-time applications. We identify a pathological optimization issue
with the commonly adopted stochastic minimization of the (reverse) KL diver-
gence, owing to skewed gradient distribution due to curse of dimensionality. We
also explore alternative principles for distillation, and show that one can achieve
qualitatively better results than with KL minimization.
1 Introduction
Deep autoregressive models are currently among the best choices for unsupervised density modeling
tasks (Van den Oord et al., 2016b;a) However, due to the factorization induced by such models on
the data space by the chain rule of probability, sampling from such models requires O(T) sequential
computation steps, where T is the dimension/sequence-length of the data. This makes sampling
slow and inefficient for most practical purposes. A recent solution to alleviating this bottleneck was
proposed by Van den Oord et al. (2018), who show how to distill an autoregressive WaveNet model
(Van den Oord et al., 2016a) into a student network , which is significantly faster to sample from, and
therefore much more suitable for deployment in real-time applications. In this paper, we identify a
fundamental issue with the approach taken by Van den Oord et al. (2018), and provide alternative
views and principles for distilling the sampling process of a teacher model.
The solution proposed by Van den Oord et al. (2018) is to minimize the reverse Kullback-Leibler
divergence (KL) between the student and the teacher distribution. Generally, this relies on two
essential components: (1) the gradient signal from the teacher network (the WaveNet model) and
(2) invertibility of the student network (the Parallel WaveNet model). This allows samples drawn
from the student to be evaluated under the likelihood of both models, and the student can then
be updated via stochastic backpropagation from the teacher. The student is implemented with an
inverse autoregressive transformation (Kingma et al., 2016) that, unlike an autoregressive model,
admits fast sampling at the cost of slow likelihood estimation.
In theory, it seems that minimizing the KL should be sufficient for distilling a teacher into a student.
However, in practice, it has been necessary to implement additional heuristics in order to achieve
samples with similar levels of realism as that of the teacher (Van den Oord et al., 2018; Ping et al.,
2018). In particular, a power loss that biases the power across frequency bands of the student sampler
to match that of human speech patterns has been crucial in recent work for generating synthesized
speech without the student collapsing to unrealistic ‘whispering’.
The main contribution of this work is to identify that the reverse KL is ill-suited for the task of
distillation, since the teacher distribution typically possesses a low-rank nature for high-dimensional
structured data, which renders the effective gradient signal sparse. We explore some possible al-
ternatives for distillation, by recasting the problem of distilling a generative model as learning a
transformation of probability density from a prior space to the data space, connecting decoder-
based generative models with probability density distillation. We present experimental results which
demonstrate that it is possible to learn qualitatively better samplers than distillation with reverse KL.
1
Under review as a conference paper at ICLR 2019
2	Autoregressive Models
Given a joint probability distribution p(x), where x denotes a T -dimensional vector, one can factor-
ize the distribution according to the chain rule of probability, for arbitrary ordering of dimensions:
T
p(x) = p(x1)p(x2|x1)...p(xT|x1, ..., xT-1) =	p(xt|x1:t-1)
t=1
For the tabular case, i.e. when xt can take V different possible values, the size of the table is
O(V T ). When the event set of xt is uncountable, the joint density is not even tractable. This
motivates the use ofa parametric model to compress the conditional probabilityp(xt|x1:t-1), where
one has pθ(x) = QT=Ipθ(xt|xi：t-i). This is referred to as an autoregressive model. Parameters
are usually shared across the dimensions of x, since T might not stay constant, for example in the
case of recurrent neural networks or convolutional neural networks, which have been empirically
demonstrated to possess good inductive biases for tasks involving images (Van den Oord et al.,
2016b) and speech data (Van den Oord et al., 2016a). However, sampling is sequential, requiring T
passes, which is why state-of-the-art autoregressive models have been slow to sample from, which
makes them impractical for tasks requiring sampling, such as speech generation. This has motivated
the work of Van den Oord et al. (2018), who propose probability density distillation to learn a
student network with a structure that allows for parallel sampling, by distilling a state-of-the-art
autoregressive teacher into it.
3 Probability Distillation with Normalizing Flows
Van den Oord et al. (2018) propose to distill the probability distribution parameterized by a WaveNet
model (denoted by T, which stands for the teacher network) by minimizing its reverse KL with a
student network (denoted by S):
DKL(PS || PT)= Ex〜PS [logPS(X)- logPT(x)]	(1)
The idea is to leverage recent advances in change of variable models (also known as normalizing
flows) to parallelize the computation of the sampling process. First, the student distribution is con-
structed by transforming an initial distribution PS (z) (e.g. normal or uniform distribution) in a way
such that each dimension xt in the output x depends only on up to t preceding variables (according
to a chosen ordering) in the input z:
Zt 〜PS(Z),
Xt J gt(zt; ∏t(zi,…,Zt-l)),
where gt is an invertible map between Zt and xt. Unlike the sampling process of an autoregressive
model, where one needs to accumulate all X1:t-1 to sample Xt from PT(Xt|X1:t-1), which scales
O(t), the transformations gt can be carried out independently oft, allowing for O(1) time sampling.
Second, the entropy term ofPS can be estimated using the change of variable formula:
Ex〜PS(x) [logPS(X)]
=Ez 〜PS
log PS (Z)
(2)
where g is the overall transformation g(Z)t =. gt(Zt; Z<t). Furthermore, owing to the partial depen-
dency of gt, g has a triangular Jacobian matrix, reducing the computation of the log-determinant of
Jacobian in (2) to linear time:
dgt(zt； z<t)
dzt
Finally, when the teacher network has a tractable explicit density, one can evaluate the likelihood of
samples drawn from PS under PT efficiently (in the case of autoregressive models such as WaveNets,
one can use teacher forcing to compute log-likelihood in parallel).
2
Under review as a conference paper at ICLR 2019
(a) Gradients from teacher (red) and student (blue).
(b) Rotated by eigenvectors of ΣT .
Figure 1: When distilling the teacher distribution (red) with a student distribution (blue) by min-
imizing the KL divergence, the student receives two counteracting gradient signals from the two
corresponding likelihood terms. The probability of receiving a signal to push out the student distri-
bution to fill the teacher is proportional to the relative amount of space occupied by the shaded area,
which vanishes when the covariance matrix of the teacher has trivial eigenvalues.
3.1	Curse of Dimensionality
In this section, we explain the failure mode of distillation with reverse KL that we have identified.
Intuitively, for each sample x drawn from pS, the negative gradient of the integrand in Equation (1)
with respect to x, is made up of two counteracting factors: one that pushes x away from the mode
of the student density pS and one that pulls x towards the mode of the teacher density pT . These
two counteracting terms form the “error” component of the path derivative (Roeder et al., 2017)
when the student is updated. (see Appendix C for more details). Assume the student density pS lies
within a certain family Q. Ideally, ifpT ∈ Q, the solution to the minimization problem in (1) would
be pS = pT . However, this is not trivial in practice when an oracle solver is not accessible. We
empirically find out that training with reverse KL consists of two stages:
(i)	pS starts to fit to the mode of pT (Figure 2a)and
(ii)	pS gradually expands from the mode of pT to fit the shape of the distribution.
Stage (i) is fast due to the well-known zero forcing property of the reverse KL. Turner & Sahani
(2011) (see Fig 1.3) shows that pS tends to be more concentrated when independence assumption
is made. We show that even when Q contains pT , stochastic optimization can result in slow con-
vergence of stage (ii) and thus a more concentrated, suboptimal pS . This is because the gradient
signal via the path derivative can be increasingly unlikely to be effective when the dimensionality
in x grows and when pT is highly structured. This makes it harder for pS to expand its probability
mass along the high density manifold under pT . We illustrate this as follows.
Consider the case when both the student and teacher are multivariate normal centered at the origin:
assume without loss of generality1 that pS = N(0, I) and pT = N (0, ΣT ) (both centered at 0
assuming stage (i) has been completed), where I is an n-by-n identity matrix and ΣT ∈ S+n + is a
positive definite matrix. The following proposition establishes the connection between the eigenval-
ues of the covariance matrix ΣT and the probability of getting a positive signal that pushes x away
from the origin.
Proposition 1. Let PS = N(0, I) andPT(0, ∑t). Draw X 〜ps. Let AU be the surface area ofthe
unit sphere: U = {x : kxk2 = 1} and AU∩ρ be the surface area of{x ∈ U : i ρixi2 > 0}. Then
the probability of Vχ [log PT 一 log qs] pointing away from the origin along X is given by
au∩p
AU
(3)
1 When the covariance matrix of the student distribution ΣS is not an identity matrix, one can transform
both pS and pT via the change of variable: x0 = U->x where ΣS = U>U is the Cholesky decomposition of
the covariance matrix; such that pS (x0) is standardized, pT (x0) has a “relative” covariance (due to the rotation
under U -> ), and our analysis carries on.
3
Under review as a conference paper at ICLR 2019
(a) Distillation with KL
(b) With z-reconstruction
Figure 2: We distill a Gaussian teacher with a Gaussian student. x-axis: likelihood under the teacher; y-axis:
count of samples drawn from the teacher (real samples) and the learned student (generated samples). (a-d) in
the subfigures correspond to {4, 16, 32, 64}- dimensional multivariate Gaussians.
(c) With x-reconstruction
where Pi = 1 -/ and d is the i-th eigenvalue ofthe covariance matrix.
Proof. Let gχ = Vχ(logPT(x) — logPS(x)) be the gradient. Since both PS and PT are Gaussian
distributions centered at the origin, the gradient when projected onto x, either points towards or
away from the origin, depending on whether x>gx < 0 or x>gx > 0. By definition, we have
gx = -ΣT-1 x + x. Let ΣT = ΛDΛ-1 be the eigen-decomposition of the covariance, where
Dii = di2 is the i-th eigenvalue and the columns of Λ are the eigenvectors. Due to the rotational
invariance and uniformity of the density on the level set {x : kxk2 = r} for any r > 0 of the
standard normal PS ,
P{χ>gx > 0}
=P {x>X — x>ΛD-1Λ-1x > 0}
au∩p
AU
What the proposition implies is that the chances of receiving a gradient signal that points outward
depend on the eigenvalues of the covariance matrix of the teacher: the greater the number of eigen-
values that are smaller than 1 (more ill-conditioned), the lower the chances. Consider Figure 1a
for example, where the red contour plot and blue contour plot represent the density of PT and PS ,
respectively. For a random sample drawn from PS , marked by the yellow star, the gradients of
log PT and - log PS with respect to it are represented by the red arrow and blue arrow. The net
gx here can be decomposed into two parts: one that is perpendicular to x, gx,⊥, and one that is
parallel with x, gx,k . In this example, x and gx,k point towards the opposite direction, meaning the
back-propagated signal would draw x towards the mode of PT. On average, the chances of getting
a stochastic gradient signal that push the points away from the mode is the percentage of the area
of the unit sphere intersecting with the hypercone, represented by the shaded area in Figure 1b. In
practice, such a condition coefficient can be very small, as it is well known that a high dimensional
distribution over structured data is effectively low-rank. In fact, an almost-sure convergence result of
the smallest eigenvalue of a random Wishart matrix scaled by 1/T was proven by Silverstein et al.
(1985) to be effectively zero with large enough T . The Marchenko-Pastur Law describes a more
general asymptotic distribution of the eigenvalues (Marchenko & Pastur, 1967).
3.2	Empirical demonstration
We showed above that with increasing dimensionality, and for a structured teacher, there is a dimin-
ishing probability of gradient signals that can push the student to expand around the mode of the
teacher. It seems intuitive that the distilled density of the student will therefore be collapsed around
the mode of the teacher density. We validate this hypothesis in the following experiment.
We take both PT and PS to be multivariate Gaussian distributions, with the sampling process defined
as x J μ + R ∙ Z where μ ∈ RT, R ∈ RT×T and Z ~ N(0, I). We randomly initialize each element
of R for T independently according to the standard Gaussian, set μ = [2,…,2]> to be a vector
of T 2’s, and fix them while training S to distill T. For T ∈ [4, 16, 32, 64], training proceeds as
follows: we sample x from the student, estimate log PS (x) using the change of variable formula, and
evaluate x under log PT. We use a minibatch size of 64 and learning rate of 0.005 with the Adam
4
Under review as a conference paper at ICLR 2019
optimizer (Kingma & Ba, 2014), and make 5000 updates. For evaluation, we draw 1000 samples
from both pT andpS, and display the empirical distribution of log pT (x) in Figure 2a.
First, with increasing number of dimensions, we observe that pS does indeed concentrate more on
the high density region of pT . This suggests that the unbalanced gradient signal poses an optimiza-
tion problem for distillation of higher dimensional structured distributions; getting a sample that gets
pushed away along the thin manifold under the teacher density is very unlikely, which is consistent
with Proposition 1.
Second, we also observe that the log-likelihood of the teacher samples deviate from 0 as dimension-
ality grows. In fact, assuming Xt is sampled i.i.d., the l2 norm of x = √T would almost surely
converge. This is a phenomenon known as the concentration of measure. To see this,
T	T x 2	1T
kxk2 = χ>χ =	Xx2	= X	( √=	) = T Xx2 -→ E[χ2]	as as T	→ ∞
t=1 t=1 T T t=1
The concentration is due to the compromise between density and volume of space (which vanishes
exponentially as dimensionality grows). The consequence is that when one samples from a high
dimensional Gaussian, the norm of the sample can be well described by its expected value, which
means one is effectively sampling from the shell of the Gaussian ball. This suggests that the use of
KL would result in a mismatch of certain important statistics (such as norm of the samples, which is
a perceivable feature in images and audio frames) even when pS is fairly close to pT .
Finally, in the above study, we only identify this optimization difficulty in the convex setup. How-
ever, it is also well known that the reverse KL tends to be mode-seeking (see Figure 3b,3c for
example), and is not well-suited for learning multimodal densities (Turner & Sahani, 2011; Huang
et al., 2018b).
4 Probability Distillation with Inverse Matching
In this section, we discuss possible alternatives for distilling a teacher. We assume there exists an
invertible mapping from a prior space Z to the data space X, such that one can trivially sample from
a prior distribution Z 〜PT(Z) and pass the sample through this invertible map such that the sample
is distributed according to pT (x). For Gaussian conditional autoregressive models, for example, one
would sequentially pass scalar standard Gaussian noise Zt through the following recursive function
Xt = μt(x1∙,t-1) + σt(xn) ∙ zt. For notational convenience, We denote the “inverse” of this
transformation by T : X → Z, as this is the inverse autoregressive transformation that can be
parallelized. The goal is to learn the sampling transformation, i.e. T-1. Similarly, we define the
forward pass of the student as the mapping S : Z → X .
Ideally, since the transformation is deterministic, it’s most natural to simply minimize the prediction
loss according to some distance metric d(TT (z), S(z)), where Z 〜PT,z. When this loss function
equals zero almost everywhere, passing the prior sample through S would induce an identical dis-
tribution as PT. We refer to this setup as distillation with oracle prediction. However, preparing
such a dataset of T-1(Z) samples would typically be time-consuming. We present the following
two alternatives.
1.	Distillation with Z-reconstruction. We consider minimizing d(Z, T ◦ S(Z)), which is a recon-
struction loss and the student network and teacher network are viewed as the encoder and decoder,
respectively. In this case, since T is invertible and fixed, the only functional form ofS that gives zero
reconstruction would be T-1, which means the random variable S(Z) should also be distributed ac-
cording to PT . In fact, minimizing the Z-reconstruction loss corresponds to a parametric distance
induced by the teacher network. Define dT (a, b) =. d(T (a), T (b)), where d is a distance metric.
Then
dT(T-1(Z), S(Z)) = d(T ◦ T-1(Z), T ◦ S(Z)) = d(Z, T ◦ S(Z)).
Interestingly, dT is also a metric:
Proposition 2. dT is a metric if and only if T is injective.
Proof. Trivially, positive-definiteness and symmetry are inherited from d if and only if T is an
injection. To see that subadditivity is also preserved, for some a, b and c, let Ta = T (a), Tb = T(b)
5
Under review as a conference paper at ICLR 2019
636 0
e0⅛ 6 修
βΛs2f
®& 3 空
DDDba
GUDbO
UGDQQ
U D。。。
QDOaD
⅛s<2r5a
EGN 3 6
3a633
aN2 夕<b
β-s2G6
42 2 2 ʌð
r¾ S G03
-36⅛∙4 4
匕4玄A6
Figure 3: Density distillation of teacher models trained on MNIST (first row) and Fashion-MNIST(second
row). Column 1: samples from teacher network T. Column 2: samples from student trained with the KL
loss SKL. Column 3: samples from student trained with the z-reconstruction loss Sz. Column 4: samples
from student trained with the x-reconstruction loss Sx . Column 5: samples from student trained with the
x-reconstruction loss where x is sampled from the teacher SO .
and Tc = T (c). Since d(Ta, Tb) ≤ d(Ta, Tc) + d(Tb, Tc), due to the subadditivity of d, for any Ta,
Tb and Tc, We have dτ(a, b) ≤ dτ(a, C) + dτ(b, C) for any a, b and c.	□
This means z-reconstruction loss behaves like a distance between T-1(z) and S(z). So when z-
reconstruction is minimized, it implies S gets closer to T-1 in the sense of the induced metric
dT.
2.	Distillation with x-reconstruction. Finally, We consider minimizing the reconstruction loss
d(χ, S ◦ T(x)), where X 〜PD, the (empirical) data distribution, taking the teacher network as
the encoder, and the student netWork as the decoder. When the teacher density coincides With the
underlying data distribution, this would be equivalent to training with oracle prediction, as T(X)
would be distributed according to pT (z). This is a reasonable assumption when pT approximates
pD well, and this is in fact true as pT is usually trained with Maximum likelihood under pD .
Now we revisit the two essential components required for distillation with reverse KL:
(1)	Invertibility: None of the three training criteria we explored involves estimating the en-
tropy ofpS, so in principle, we do not require invertibility of the student. In fact the entropy
of pS is implicitly maximized since T is bijective. To prevent degenerate pS, one simply
needs to avoid using hidden units of dimensionality smaller than the input size without skip
connectivity, which compresses the noise.
(2)	Differentiability: For distillation with oracle prediction and x-reconstruction, we only
require the translation between X and Z, via T and T-1, which is readily accessible
for many standard distributions, e.g. linear map between Gaussians (both T and T -1),
logistic-linear map from mixture of logistics to uniform (T only, but sampling is achievable
by sampling the mixture component first), and neural transformation (Huang et al., 2018a)
(T only). We also note that it is possible to recover uniform density from discrete data, by
injecting noise proportional to the probability per class to break ties when passing the data
through the cumulative sum of the probability (CDF).
6
Under review as a conference paper at ICLR 2019
-χ) Q & ?
Ao b 2、
Cr 6刀。2)
，4 Q。乙
C 5 oən
k rJ3 久•一>
。N 9 SG
H % O O CO
OSCD。
%JaaD
C72 sr) G
。-O 3 工 a
qxa O。已
C 5 60C
<0 (/A久 I
C72 3 C
PD工,，
(rɔ.£ M 4
.sca 4 2
Z?小。BC
⅛L3S4 5
(a) l2 reconstruction (b) l1 reconstruction (c) l2, noise	(d) l1, noise	(e) (l2 + l1)/2, noise
Figure 4: Experiments with a ResNet student using (a) l2 x-reconstruction loss, (b) l1 x-reconstruction loss,
(c) adding N(0, 0.5) noise to the encodings z and using l2 x-reconstruction loss, (d) adding N(0, 0.5) noise
to z and using l1 x-reconstruction loss, and (e) adding N(0, 0.5) noise to z and using a mixed loss l2 + l1. In
general, we observe that adding noise significantly improves sample quality, and training with l1 losses lead to
sharper samples.
4.1	Experiments
4.1.1	Linear Model with Increasing Dimensionality
We replicate the experiment in Section 3.2 with z-reconstruction and x-reconstruction loss (equiv-
alent to oracle prediction in this case). Mapping T from X to S is simply inverse of the sampling
transformation. We observe that both models outperform distillation with reverse KL, since likeli-
hood of samples (under the teacher) drawn from them follow more closely the shape of the empirical
distribution of likelihood of samples drawn from the teacher. It is worth noting that z-reconstruction
also starts to fail with an increasing number of dimensions, suggesting that it might be subject to
poor gradient signal if the transformation T is more complex; we elaborate more on this in the
next section. On the other hand x-reconstruction is quite robust to dimensionality (and potentially
complexity) of the underlying distribution.
4.1.2	Distillation with Different Losses
In this section, we distill PixelCNN++ (Salimans et al., 2017) teacher networks trained on the
MNIST handwritten digits dataset (LeCun et al., 1998) and the Fashion-MNIST dataset (Xiao et al.,
2017). We trained the teacher model for 100 epochs and distilled it into the student with another 100
epochs of updates, using minibatch size of 64, learning rate of 0.0005 for the Adam optimizer with
a decay rate of 0.95 per epoch, 3 ResNet blocks per downsampling and upsampling convolution, 32
hidden channels, and a single Gaussian conditional. The data is preprocessed with uniform noise
between pixel values and rescaled using the logit function. We use l2 loss for the reconstruction and
prediction methods.
First, we observe that when trained with the reverse-KL loss, the students collapse on undesirable
modes. As shown in (Figure 3b), the inductive bias of the causal convolution leads to higher density
of the samples with striped textures. When trained with the z-reconstruction loss, the MNIST student
samples all collapse to the same digit. Interestingly, when we visualize the corresponding T-1 (z)
as we slightly perturb the norm of z (see Appendix A), we observe that the digits abruptly change
identity. This suggests that when moving onto a different sublevel set of norm in z-space, the
corresponding x jumps from one digit manifold to another, and the direction of z does not preserve
digit identity. This might explain why the student collapses to a digit: this is due to the bad local
minimum that corresponds to relatively low reconstruction cost in the z-space.
Next, we observe that the student trained with x-reconstruction loss (Figure 3d) does not have good
quality samples while the reconstructions are visually perfect. We hypothesize this is due to the
well-known problem of mismatch between the empirical distribution of the encodings T(x) and
the prior distribution pS(z) of training decoder based generative models (Kingma et al., 2016). We
contrast this with a student trained on oracle predictions (Figure 3e) and observe that the latter’s
samples match the teacher’s samples better.
Finally, we see that the samples from the teacher trained on Fashion-MNIST with the x-
reconstruction loss (Figure 3i) have a smoother texture than the one trained with oracle samples
7
Under review as a conference paper at ICLR 2019
(Figure 3j), which again, are perceptually closer to samples from the teacher. We elaborate more on
this discrepancy in the next section.
4.1.3	Learning to Distill and Learning to Generate
Since we are not constrained in our modeling choice for the student, we experiment with a ResNet
student which is trained to directly map an encoded datapoint from the teacher back to the datapoint.
The ResNet is deep enough so that the receptive field at the output is sufficient to span all of the
encoded z, so that far-away influence is still exploitable.
Using the l1 reconstruction cost leads to sharper samples from the student (contrast 4a with 4b).
It appears to us that the l2 loss tends to maintain global details, while the l1 loss can sometimes
sacrifice global coherence for local structure, potentially due to the sparsity induced by it. In 4e,
we use an average of both losses in an attempt to maintain both these characteristics, but we notice
evidence of the failings manifesting to some extent as well.
A significant improvement in sample quality occurs upon adding Gaussian noise to the encodings
before training the student (contrast 4a,4b with 4c,4d). Our intuition for this is as follows: providing
a decoder network with pairs of points in a data space and an encoded (or latent) space would typi-
cally result in almost all of z-space not being “trained”, especially since image data (and therefore
the corresponding encodings in z-space) usually lies in a low-dimensional manifold. Adding noise
enhances the support of the distribution, effectively spreading the distribution of “responsibility” of
an encoding to cover more volume, which smoothens the mapping learned by a decoder. When the
goal is to sample from a prior, training methods that encourage such z-space-filling strategies and
smoother mappings improve sample quality, in the vein of decoder-based sampling models such as
variational auto-encoders (Kingma & Welling, 2014).
This leads us to an important point about these experiments: since the student is trained on noised
(encoded) points from the data distribution, this is no longer purely density distillation. The student
no longer aims to reproduce the sampling behavior of the teacher (as in 3e), but rather uses the
teacher to provide structural information through its encodings. This information, when “spread
out” through noise-injection and used by a student to learn decodings into real data (through a
reconstruction penalty or more sophisticated losses) results in a network that can now be considered
as a stand-alone generator, with the teacher acting as an inference machine that preserves information
in the latent space. This can potentially allow a student to outperform its teacher in terms of sample
quality, by enabling the learning of a smoother mapping from z-space to data space.
4.1.4	Neural Vocoder
We compare distillation with x-reconstruction and the reverse-KL approach on the neural
vocoder Sotelo et al. (2017) task for speech synthesis. The neural vocoder is an essential com-
ponent of many text-to-speech models proposed recently (Wang et al., 2017; Shen et al., 2018; Arik
et al., 2017; Ping et al., 2018). We train our teacher to map vocoders (Morise et al., 2016) to raw
audio using the SampleRNN (Mehri et al., 2016) model. We model the conditional distribution of
the teacher with a unimodal gaussian distribution, making it easier to compute the corresponding z .
We specifically compare against closed form regularized KL with Gaussian conditionals as proposed
in (Ping et al., 2018). The student network has a WaveNet architecture with six flows, and performs
sampling as in Parallel WaveNet (Van den Oord et al., 2018). Each flow is a dilated residual block of
10 layers with a convolution kernel width of 2 and 64 output channels. We use our x-reconstruction
method with l1 as the reconstruction loss. We empirically find that our method results in a stu-
dent with samples without the characteristic whispering of the reverse-KL trained student. We have
uploaded samples for comparison here 2.
5 Conclusion
In this paper, we investigate problems with distilling an autoregressive generative model under a
reverse KL cost between the student and the teacher, where the student can perform efficient parallel
generation. Specifically, we show that distillation with the reverse KL can suffer from imbalanced
2 https://soundcloud.com/inverse-matching/sets/samples-for-inverse-matching
8
Under review as a conference paper at ICLR 2019
gradient signals due to the curse of dimensionality. Further, we explore different alternatives which
work qualitatively better when compared with distillation with reverse KL.
References
Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan
Raiman, and Yanqi Zhou. Deep voice 2: Multi-speaker neural text-to-speech. arXiv preprint
arXiv:1705.08947, 2017.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In International Conference on Machine Learning, 2018a.
Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, and Aaron Courville. Improving explorability
in variational inference with annealed variational objectives. arXiv preprint arXiv:1809.01818,
2018b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2014.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Informa-
tion Processing Systems, pp. 4743-4751, 2016.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for
some sets of random matrices. Matematicheskii Sbornik, 114(4):507-536, 1967.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo,
Aaron Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio
generation model. arXiv preprint arXiv:1612.07837, 2016.
Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-quality speech
synthesis system for real-time applications. IEICE TRANSACTIONS on Information and Systems,
99(7):1877-1884, 2016.
Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-end text-to-
speech. arXiv preprint arXiv:1807.07281, 2018.
Geoffrey Roeder, Yuhuai Wu, and David Duvenaud. Sticking the landing: An asymptotically zero-
variance gradient estimator for variational inference. In Advances in Neural Information Process-
ing Systems, 2017.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. In International
Conference on Learning Representations, 2017.
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,
Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by con-
ditioning wavenet on mel spectrogram predictions. In 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 4779-4783. IEEE, 2018.
Jack W Silverstein et al. The smallest eigenvalue of a large dimensional wishart matrix. The Annals
of Probability, 13(4):1364-1368, 1985.
Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville,
and Yoshua Bengio. Char2wav: End-to-end speech synthesis. 2017.
Richard E Turner and Maneesh Sahani. Two problems with variational expectation maximisation
for time-series models. Bayesian Time series models, 1(3.1):3-1, 2011.
9
Under review as a conference paper at ICLR 2019
Aaron Van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. In SSW, pp. 125, 2016a.
Aaron Van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
In International Conference on Machine Learning, 2016b.
Aaron Van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray
Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,
et al. Parallel wavenet: Fast high-fidelity speech synthesis. In International Conference on Ma-
chine Learning, 2018.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly,
Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: A fully end-to-end
text-to-speech synthesis model. arXiv preprint, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
10
Under review as a conference paper at ICLR 2019
A VISUALIZATION OF T-1 (z) WITH PERTURBED NORM OF z
<⅛c ⅛6/ Ba aa
00线/心女匠& A白总@以？X4>∕3G介与l<i2a
0 0⅛kM⅛e 6LO&&&>,□>方4Na以。66^t⅛3
a C 4 & Z 2 64 宅 3白邑之 3^6/3<2.££6a33
e 0⅛¾O 名 s£ GJ 3S&方 3‰^G3幺06b在冬 2
Ue⅛¾1QS2w3sa⅛iz‰Gba2o66忘 冬 2
UeT * BS2o3eG5a‰√5sa2e662¾z
Q/^L^J<gs6o3s⅛⅛-32βsaAg6QAa2
Q02⅛DGβ5o3sG3~B乙。xaΛ663^az
Q023oc62o3sa3~ CO 乙@Na £3632 & 2
。6230666033&>3~3%。33幺363乙23
Q^gmɔesg D36a2s 乙&N3caGXΔ23
QeOo 3oqs£o3£&% S 乙 G301>caG3ΔA3
QeCΛ3oGSco3s⅛⅛3 乙β3σ⅛ca6322m
Qe 幺 3z>csrJo3s⅛⅛。乙β33幺a6co∆23
Q6g3c><}6bo3sLL 七 g 乙βN3白e6oL右23
β6g35c6Go3sa力 g‰βN，白 36na2 23
βsg32cSGo3s3 也 g 乙βN3e36coΔ2m
e 3o0y>2α6Go3s6Jg 乙β2σ¾6363∆23
<≥23K3 2 6Ga3soa.g68N2636 3 32 3
QiS382Q6Gc)33o2R2β226 333 乙 33
QJOQ3/3Q6a33g/moaa0ag修 333K33
OS3&2交 eα3MG∕agao936-33α28 3
Figure 5: We randomly sample z from N(0, I784) (for each row) and rescale the vector such that it
has norm r ∙ √784, where r ∈ [0.700, 0.750, 0.800, 0.850, 0.900, 0.920, 0.940, 0.960, 0.980, 0.990,
0.995, 1.000, 1.005, 1.010, 1.020, 1.040, 1.060, 1.080, 1.100, 1.150, 1.200, 1.250, 1.300], which
correspond to the change along the horizontal axis. We observe that directional information does
not preserve digit identity in the data space, and the manifold per digit can be stretched around the
origin on different level sets of norm.
11
Under review as a conference paper at ICLR 2019
B Additional result on distilling PixelCNN trained on CIFAR- 1 0
with x-reconstruction (noise injection)
Figure 6: (left) Teacher samples with a PixelCNN on CIFAR-10, (right) Student samples using
x-reconstruction under the l1 loss, with noise injection.
C Path derivative
When x is real-valued and when reparameterization of the sample drawn from pS (x) allows for
separation of a random variable Z 〜p(z) independent of the parameters of PS and a determinis-
tic transformation x = Sφ(z), we can decompose the gradient of the KL divergence (1) with the
parameters φ (of S) as (Roeder et al., 2017):
VφEx〜psφ (x) [log pSφ (x) - log PT(x)]
=vφ Ez 〜p(z)[lθg pSφ (X) — log pτ(X)]
=Ez〜p(z) Vφ(logPSφ(X)- logPT(x))];	(x = Sφ(z))
=Ez 〜p(z) [Vx(log pSφ(x) - log pt (x))VφSφ(z) + Vφ log psφ (x)]
S----------------{z-----------------} S------{z----}
path derivative	score function
where the last equality is the total derivative because the term PSφ (x) depends on φ through both the
sample being evaluated, x = Sφ(z), and the evaluating log-likelihood function, log PSφ.
The score function is in expectation zero, since
/ PSφ(X)Vφ log PSφ(X)dx = / PSφ(X) ：；：；)dx = vΦ / PSφ(X)dx = vΦ1 = 0,
so it can be thought of as a control variate: an unbiased term used to reduce variance in gradient
estimate. The path derivative term measures the dependency on the parameters φ through the repa-
rameterized sample x = Sφ(z). As a result, the gradient direction wrt the sample x directly affects
how the parameters of the distribution will be updated to change the shape of PSφ .
12