Under review as a conference paper at ICLR 2019
Padam: Closing the Generalization Gap of
Adaptive Gradient Methods in Training Deep
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods, which adopt historical gradient information to auto-
matically adjust the learning rate, despite the nice property of fast convergence,
have been observed to generalize worse than stochastic gradient descent (SGD)
with momentum in training deep neural networks. This leaves how to close the
generalization gap of adaptive gradient methods an open problem. In this work,
we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes
“over adapted”. We design a new algorithm, called Partially adaptive momen-
tum estimation method (Padam), which unifies the Adam/Amsgrad with SGD by
introducing a partial adaptive parameter p, to achieve the best from both worlds.
Experiments on standard benchmarks show that Padam can maintain fast conver-
gence rate as Adam/Amsgrad while generalizing as well as SGD in training deep
neural networks. These results would suggest practitioners pick up adaptive gra-
dient methods once again for faster training of deep neural networks.
1	Introduction
Stochastic gradient descent (SGD) is now one of the most dominant approaches for training deep
neural networks (Goodfellow et al., 2016). In each iteration, SGD only performs one parameter
update on a mini-batch of training examples. SGD is simple and has been proved to be efficient,
especially for tasks on large datasets. In recent years, adaptive variants of SGD have emerged and
shown their successes for their convenient automatic learning rate adjustment mechanism. Adagrad
(Duchi et al., 2011) is probably the first along this line of research, and significantly outperforms
vanilla SGD in the sparse gradient scenario. Despite the first success, Adagrad was later found
to demonstrate degraded performance especially in cases where the loss function is nonconvex or
the gradient is dense. Many variants of Adagrad, such as RMSprop (Hinton et al., 2012), Adam
(Kingma & Ba, 2015), Adadelta (Zeiler, 2012), Nadam (Dozat, 2016), were then proposed to address
these challenges by adopting exponential moving average rather than the arithmetic average used in
Adagrad. This change largely mitigates the rapid decay of learning rate in Adagrad and hence
makes this family of algorithms, especially Adam, particularly popular on various tasks. Recently,
it has also been observed (Reddi et al., 2018) that Adam does not converge in some settings where
rarely encountered large gradient information quickly dies out due to the “short momery” problem of
exponential moving average. To address this issue, Amsgrad (Reddi et al., 2018) has been proposed
to keep an extra “long term memory” variable to preserve the past gradient information and to correct
the potential convergence issue in Adam.
On the other hand, people recently found that for largely over-parameterized neural networks, e.g.,
more complex modern convolutional neural network (CNN) architectures such as VGGNet (He
et al., 2016), ResNet (He et al., 2016), Wide ResNet (Zagoruyko & Komodakis, 2016), DenseNet
(Huang et al., 2017), training with Adam or its variants typically generalizes worse than SGD with
momentum, even when the training performance is better. In particular, people found that carefully-
tuned SGD, combined with proper momentum, weight decay and appropriate learning rate decay
strategies, can significantly outperform adaptive gradient algorithms eventually (Wilson et al., 2017).
As a result, even though adaptive gradient methods are relatively easy to tune and converge faster at
the early stage, recent advances in designing neural network structures are all reporting their perfor-
mances by training their models with SGD-momentum (He et al., 2016; Zagoruyko & Komodakis,
1
Under review as a conference paper at ICLR 2019
2016; Huang et al., 2017; Simonyan & Zisserman, 2014; Ren et al., 2015; Xie et al., 2017; Howard
et al., 2017). Moreover, it is difficult to apply the same learning rate decay strategies that work
well in SGD with momentum to adaptive gradient methods, since adaptive gradient methods usually
require a much smaller base learning rate that will soon die out after several rounds of decay. We
refer to it as the “small learning rate dilemma” (see more details in Section 3).
With all these observations, a natural question is:
Can we take the best from both Adam and SGD-momentum, i.e., design an algorithm that not only
enjoys the fast convergence rate as Adam, but also generalizes as well as SGD-momentum?
In this paper, we answer this question affirmatively. We close the generalization gap of adaptive
gradient methods by presenting a new algorithm, called partially adaptive momentum estimation
(Padam) method, which unifies Adam/Amsgrad with SGD-momentum to achieve the best of both
worlds.
In particular, we make the following contributions:
•	We propose a new algorithm, Padam, which unifies Adam/Amsgrad and SGD with mo-
mentum by a partially adaptive parameter. We show that Adam/Amsgrad can be seen as
a special fully adaptive instance of Padam. The intuition behind our algorithm is that, by
controlling the degree of adaptiveness, the learning rate in Padam does not need to be as
small as other adaptive gradient methods to prevent the gradient update from exploding.
This resolves the “small learning rate dilemma” for adaptive gradient methods and allows
for faster convergence, hence closing the gap of generalization.
•	We show that the Padam’s performance is also theoretically guaranteed. We provide a
convergence analysis of Padam in the convex setting, based on the analysis of Kingma &
Ba (2015); Reddi et al. (2018), and prove a data-dependent regret bound.
•	We also provide thorough experiments about our proposed Padam method on training mod-
ern deep neural architectures. We empirically show that Padam achieves the fastest con-
vergence speed while generalizing as well as SGD with momentum. These results suggest
that practitioners should pick up adaptive gradient methods once again for faster training
of deep neural networks.
Notation: Scalars are denoted by lower case letters, vectors by lower case bold face letters, and
matrices by upper case bold face letters. For a vector θ ∈ Rd, We denote the '2 norm of θ by
I∣θk2 = JPd=I θ2 ,the '∞ norm of θ by ∣∣θk∞=maχd=ι |%|. ForaseqUenceofveCtors {θj}j=ι,
we denote by θj,i the i-th element in θj. We also denote θi:t,i = [θι,i, θ2,i,..., θt,i]>. With slightly
abuse of notation, for any tWo vectors a and b, We denote a2 as the element-Wise square, ap as the
element-wise power operation, a/b as the element-wise division and max(a, b) as the element-wise
maximum. We denote by diag(a) a diagonal matrix with diagonal entries a1, . . . , ad. Let S+d+ be
the set of all positive definite d × d matrices. We denote by ΠX,A(b) the projection of b onto a
convex set X, i.e., argmina∈X IA1/2(a - b)I2 for b ∈ Rd, A ∈ S+d+. Given two sequences {an}
and {bn}, we write an = O(bn) if there exists a constant 0 < C < +∞ such that an ≤ C bn. We
use notation O(∙) to hide logarithmic factors. We write an = o(bn) if an/bn → 0 as n → ∞.
2	Review of Adaptive Gradient Methods
Various adaptive gradient methods have been proposed in order to achieve better performance on
various stochastic optimization tasks. Adagrad (Duchi et al., 2011) is among the first methods
with adaptive learning rate for each individual dimension, which motivates the research on adaptive
gradient methods in the machine learning community. In detail, Adagrad1 adopts the following
1The formula here is equivalent to the one from the original paper (Duchi et al., 2011) after simple manip-
ulations.
2
Under review as a conference paper at ICLR 2019
update form:
gt	1 t	2
Θt+1 = θt - at-^, where Vt = - Vg2	(Adagrad)
√vt	t M
Here gt stands for the stochastic gradient Vft(θt), and at = α∕√7 is the step size (a.k.a., learning
rate). Adagrad is proved to enjoy a huge gain in terms of convergence especially in sparse gradient
situations. Empirical studies also show a performance gain even for non-sparse gradient settings.
RMSprop (Hinton et al., 2012) follows the idea of adaptive learning rate and it changes the arithmetic
averages used for vt in Adagrad to exponential moving averages. Even though RMSprop is an
empirical method with no theoretical guarantee, the outstanding empirical performance of RMSprop
raised people’s interests in exponential moving average variants of Adagrad. Adam (Kingma & Ba,
2015)2 is the most popular exponential moving average variant of Adagrad. It combines the idea of
RMSprop and momentum acceleration, and takes the following update form:
θt+ι = θt - atmt=, where mt = βιmt—i + (1 - βι)gt, Vt = β2Vt-1 + (1 - β2)g∣ (Adam)
vt
Adam also requires at = a/ʌ/t for the sake of convergence analysis. In practice, any decaying step
size or even constant step size works well for Adam. Note that if we choose β1 = 0, Adam basically
reduces to RMSprop. Reddi et al. (2018) identified a non-convergence issue in Adam. Specifically,
Adam does not collect long-term memory of past gradients and therefore the effective learning rate
could be increasing in some cases. They proposed a modified algorithm namely Amsgrad. More
specifically, Amsgrad adopts an additional step to ensure the decay of the effective learning rate
at/√vt, and its key update formula is as follows:
θt+ι = θt - at m=, where Vt = max(bt-ι, Vt)	(Amsgrad)
Vbt
The definitions of mt and Vt are the same as Adam. Note that the modified Vbt ensures the conver-
gence of Amsgrad. Reddi et al. (2018) also corrected some mistakes in the original proof of Adam
and proved an O(1∕√T) convergence rate of Amsgrad for convex optimization. Note that all the
theoretical guarantees on adaptive gradient methods (Adagrad, Adam, Amsgrad) are only proved for
convex functions.
3	The Proposed Algorithm
In this section, We propose a new algorithm that not only inherits the O(1∕√T) convergence rate
from Adam/Amsgrad, but also has comparable or even better generalization performance than SGD
with momentum. Specifically, we introduce the a partial adaptive parameter p to control the level of
adaptivity of the optimization procedure. The proposed algorithm is displayed in in Algorithm 1.
Algorithm 1 Partially adaptive momentum estimation method (Padam)
input: initial point θ1 ∈ X; step sizes {at}; momentum parameters {β1t}, β2; partially adaptive
parameter p ∈ (0, 1/2]
set m0 = 0, V0 = 0, Vb0 = 0
for t = 1, . . . , T do
gt = Vft(θt)
mt = β1tmt-1 + (1 - β1t)gt
Vt = β2Vt-1 + (1 - β2 )gt2
Vbt = max(Vbt-1, Vt)
θt+1 = πX,diag(bp) (θt - at ∙ mt/vp)
end for
In Algorithm 1, gt denotes the stochastic gradient and Vbt can be seen as a moving average over
the second order momentum of the stochastic gradients. As we can see from Algorithm 1, the key
2Here for simplicity and consistency, we ignore the bias correction step in the original paper of Adam. Yet
adding the bias correction step will not affect the argument in the paper.
3
Under review as a conference paper at ICLR 2019
difference between Padam and Amsgrad (Reddi et al., 2018) is that: while mt is still the first order
momentum as in Adam/Amsgrad, it is now “partially adapted” by the second order momentum, i.e.,
θt+ι = θt - at mt, where Vt = max(bt-ι, Vt)
vt
(Padam)
We call p ∈ (0, 1/2] the partially adaptive parameter. Note that 1/2 is the largest possible value
for p and a larger p will result in non-convergence in the proof (see the details of the proof in the
appendix). When p → 0, Algorithm 1 reduces to SGD with momentum3 and when p = 1/2, Algo-
rithm 1 is exactly Amsgrad. Therefore, Padam indeed unifies Amsgrad and SGD with momentum.
Now the question is, what value for p should we choose? Or in another way, is p = 1/2 the
best choice? The answer is negative. The intuition behind this is simple: it is very likely that
Adam/Amsgrad is “over-adaptive”. One notable fact that people found when using adaptive gradient
methods to train deep neural networks is that the learning rate needs to be much smaller than that of
SGD-momentum (Keskar & Socher, 2017; Wilson et al., 2017). For many tasks, the base learning
rate for SGD-momentum is usually set to be 0.1 while that of Adam is usually set to be 0.001. In
fact, the key reason that prohibits Adam from adopting a more aggressive learning rate is the large
adaptive term 1 /√vt. The existence of such a large adaptive term makes the effective learning rate
(αt/ √Vt) easily explode with a larger ah Moreover, the learning rate decaying strategy used in
modern deep neural network training makes things worse. More specifically, after several rounds of
decaying, the learning rates of the adaptive gradient methods are too small to make any significant
progress in the training process. We call it “small learning rate dilemma”. This explains the relatively
weak performances of adaptive gradient methods at the later stage of the training process, where the
non-adaptive gradient methods like SGD start to outperform them.
The above discussion suggests that we should consider using Padam with a proper adaptive parame-
ter p, which will enable us to adopt a larger learning rate to avoid the “small learning rate dilemma”.
And we will show in our experiments (Section 5) that Padam can adopt an equally large base learning
rate as SGD with momentum.
Figure 1: Performance comparison of Padam with different choices of p for training ResNet on
CIFAR-10 / CIFAR-100 dataset.
Figure 1 shows the comparison of test error performances under the different partial adaptive param-
eter p for ResNet on both CIFAR-10 and CIFAR-100 datasets. We can observe that a larger p will
lead to fast convergence at early stages and worse generalization performance later, while a smaller
p behaves more like SGD with momentum: slow in early stages but finally catch up. With a proper
choice ofp (e.g., 1/8 in this case), Padam can obtain the best of both worlds.
Note that besides Algorithm 1, our partially adaptive idea can also be applied to other adaptive
gradient methods such as Adagrad, Adadelta, RMSprop, AdaMax (Kingma & Ba, 2015). For the
sake of conciseness, we do not list the partially adaptive versions for other adaptive gradient methods
here. We also would like to comment that Padam is totally different from the p-norm generalized
version of Adam in Kingma & Ba (2015), which induces AdaMax method when p → ∞. In their
3The only difference between Padam with p = 0 and SGD with momentum is an extra constant factor
(1 - β1 ), which can be moved into the learning rate such that the update rules for these two algorithms are
identical.
4
Under review as a conference paper at ICLR 2019
case, p-norm is used to generalize 2-norm of their current and past gradients while keeping the
scale of adaptation unchanged. In sharp contrast, we intentionally change (reduce) the scale of the
adaptive term in Padam to get better generalization performance.
Finally, note that in Algorithm 1 we remove the bias correction step used in the original Adam paper
following Reddi et al. (2018). Nevertheless, our arguments and theory are applicable to the bias
correction version as well.
4 Convergence Analysis of the Proposed Algorithm
In this section, we establish the theory of convergence for our proposed algorithm in the online
optimization setting (Zinkevich, 2003), where we try to minimize the cumulative objective value
of a sequence of loss functions: f1, f2, . . . , fT . In particular, at each time step t, the optimization
algorithm picks a point θt ∈ X, where X ∈ Rd is the feasible set. A loss function ft is then revealed,
and the algorithm incurs loss ft(θt). Let θ* be optimal solution to the cumulative objective function
as follows
θ* ∈ argmin X ft(θ),
θ∈X t=1
where X is a feasible set for all steps. We evaluate our algorithm using the regret, which character-
izes the sum of all previous loss function values ft(θt) relative to the performance of the best fixed
parameter θ* from a feasible set. Specifically, the regret is defined as
T
RT = X(ft(θt)- ft(θ*)),
t=1
and our goal is to predict the unknown parameter θt and minimize the overall regret RT. Our theory
is established for convex loss functions, where the following assumption holds.
Assumption 4.1 (Convex function). All ft(θ) are convex functions on X for 1 ≤ t ≤ T, i.e., for
all x, y ∈ X ,
ft(y) ≥ ft(x) + Vft(X)>(y — x).
Assumption 4.1 is a standard assumption in online learning and the same assumption has been used
in the analysis of Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2015) and Amsgrad (Reddi
et al., 2018).
Next we provide the main convergence rate result for our proposed algorithm.
Theorem 4.2. Under Assumption 4.1, if the convex feasible set X has bounded diameters, i.e.,
∣∣θ 一 θ*k∞ ≤ D∞ for all θ ∈ X, and f has bounded gradients, i.e., ∣∣Vft(θ)k∞ ≤ G∞ for all
θ ∈ X, 1 ≤ t ≤ T, let at = α∕√t, βιt = βιλt-1,λ ∈ (0,1),0 ≤ β1,β2 < 1,p ∈ (0,0.5], the
regret of Algorithm 1 satisfies:
RT ≤ 2⅛ X √T ∙ bT,i +
+	βιdD∞ G∞
+ 2a(1- βι)(1- λ) ,
αG∞-2p)√1 + log T
(1 - β1 )2 (1 - γ )(1 - β2 )p
d
E kg1：T,ik2
i=1
(4.1)
where Y = β1∕√β2 < 1.
Remark 4.3. Theorem 4.2 suggests that, similar to Adam (Kingma & Ba, 2015) and Amsgrad
(Reddi et al., 2018), the regret of Padam can be considerably better than online gradient descent
(which is known to have a regret bound of O(√dT)) when Pd=IIlg±τ,i∣∣2《√dT (DUChi et al.,
2011) and Pid=1 GTi《 √d. Also, from the proof of Theorem 4.2 in Appendix A, We can see
that the regret bound can remain in the same order even with a more modest momentum decay
βιt = βι∕t.
5
Under review as a conference paper at ICLR 2019
The following corollary demonstrates that our proposed algorithm enjoys a regret bound of Oe(√T),
which is comparable to the best known bound for general convex online learning problems.
Corollary 4.4. Under the same conditions of Theorem 4.2, for all T ≥ 1, the regret of Algorithm 1
satisfies RT = OI√T).
Corollary 4.4 suggests that Padam attains RT = o(T) for all situations (no matter whether the data
features are sparse or not). This suggests that Algorithm 1 indeed converges to the optimal solution
when the loss functions are convex, as shown by the fact that limT →∞ RT/T → 0.
5 Experiments
In this section, we empirically evaluate our proposed algorithm for training various modern deep
learning models and test them on several standard benchmarks4. We show that for nonconvex loss
functions in deep learning, our proposed algorithm still enjoys a fast convergence rate, while its gen-
eralization performance is as good as SGD with momentum and much better than existing adaptive
gradient methods such as Adam and Amsgrad.
5.1	Environmental Setup
All experiments are conducted on Amazon AWS p3.8xlarge servers which come with Intel Xeon E5
CPU and 4 NVIDIA Tesla V100 GPUs (16G RAM per GPU). All experiments are implemented in
Pytorch platform version 0.4.1 within Python 3.6.4.
5.2	Baseline Algorithms
We compare our proposed algorithm against several state-of-the-art algorithms, including: (1) SGD
with momentum, (2) Adam (Kingma & Ba, 2015), (3) Amsgrad (Reddi et al., 2018), and (4) AdamW
(Loshchilov & Hutter, 2017).
Note that we do not perform the projection step explicitly in all experiments as in Reddi et al.
(2018). Also both Amsgrad and Padam will perform the bias correction step as in Adam for a fair
comparison.
5.3	Datasets / Parameter Settings / CNN Architectures
We use several popular datasets for image classifications: CIFAR-10 (Krizhevsky & Hinton, 2009),
CIFAR-100 (Krizhevsky & Hinton, 2009), ImageNet dataset (ILSVRC2012) (Deng et al., 2009).
We adopt three popular CNN architectures to evaluate the performance of our proposed algorithms:
VGGNet (Simonyan & Zisserman, 2014), Residual Neural Network (ResNet) (He et al., 2016),
Wide Residual Network(Zagoruyko & Komodakis, 2016). We test CIFAR-10 and CIFAR-100 tasks
for 200 epochs. For all experiments, we use a fixed multi-stage learning rate decaying scheme: we
decay the learning rate by 0.1 at the 50th, 100th and 150th epochs. We test ImageNet tasks for 100
epochs with the same multi-stage learning rate decaying scheme at the 30th, 60th and 80th epochs.
We perform grid search on validation set to choose the best base learning rate for all algorithms and
also the second order momentum parameter β2 for all the adaptive gradient methods. In terms of the
choice ofp, we recommend doing binary search from 1/4 to 1/8, to 1/16, etc. Yet in most cases we
tested, 1/8 is a stable and reliable choice ofp.
Details about the datasets, CNN architectures and all the specific model parameters used in the
following experiments can be found in the supplementary materials.
5.4	Experimental Results
We compare our proposed algorithm with other baselines on training the aforementioned three mod-
ern CNN architectures for image classification on the CIFAR-10, CIFAR-100 and also ImageNet
4The code to reproduce the experimental results is available online. The URL is removed for double-blind
review
6
Under review as a conference paper at ICLR 2019
」0」」wsφl
0.06
ð
C.2
",SOn C-Pp
(a) Train Loss for VGGNet
0.0-.........................................
0 2β 4« 6C 80	100 120 14« 160 180 200
Epochs
",SOn C-Pc-
(b) Train Loss for ResNet
0.0-..........................................
0 2β 4«	60	80 IM 120 14β 160 180 2C0
Epochs
",SOn C-Pc-
(c) Train Loss for Wide ResNet
20 «	60	80 ICO 120 14« 160 180 200
Epochs
(d) Test Error for VGGNet
」0」」wsφl
0	20 40 60 M IM 12« M« 160 180 2()0
Epochs
(e) Test Error for ResNet
086420 M 0
Oooooooo
」0」」wl
---SGD-Momentum
Adam
---Amsgrad
0 2β 4«	60	80 ICO 12β 14« Ieo 180 200
Epochs
⑴ Test Error for Wide ResNet
Figure 2: Train loss and test error (top-1 error) of three CNN architectures on Cifar-10. In all cases,
Padam achieves the fastest training procedure among all methods and generalizes slightly better than
SGD-momentum.
datasets. Due to the paper length limit, we leave all our experimental results regarding CIFAR-
100 dataset in the supplementary materials. Figure 2 plots the train loss and test error (top-1 error)
against training epochs on the CIFAR-10 dataset. As we can see from the figure, at the early stage
of the training process, (partially) adaptive gradient methods including Padam, make rapid progress
lowing both the train loss and the test error, while SGD with momentum converges relatively slowly.
After the first learning rate decaying at the 50-th epoch, different algorithms start to behave differ-
ently. SGD with momentum makes a huge drop while fully adaptive gradient methods (Adam and
Amsgrad, AdamW) start to generalize badly. Padam, on the other hand, maintains relatively good
generalization performance and also holds the lead over other algorithms. After the second decaying
at the 100-th epoch, Adam and Amsgrad basically lose all the power of traversing through the pa-
rameter space due to the “small learning dilemma”, while the performance of SGD with momentum
finally catches up with Padam. AdamW improves the performance compared with original Adam
but there are still generalization gaps left behind, at least in our test settings. Overall we can see that
Padam indeed achieves the best of both worlds (i.e., Adam and SGD with momentum): it maintains
faster convergence rate while also generalizing as well as SGD with momentum in the end. Table
1 shows the test accuracy of VGGNet on CIFAR-10 dataset, from which we can also observe the
fast convergence of Padam. Specifically, Padam achieves the best test accuracy at 100th, 150th and
200th epochs for CIFAR-10, except for the 50th, where fully adaptive methods like Adam, Amsgrad
and AdamW still enjoy their fast early stage convergence performances. This suggests that practi-
tioners should once again, use fast to converge partially adaptive gradient methods for training deep
neural networks, without worrying about the generalization performances.
Table 1: Test accuracy of VGGNet on CIFAR-10. Bold number indicates the best result.
Methods	Test Accuracy (%)			
	50th Epoch	100th Epoch	150th Epoch	200th Epoch
SGD Momentum	84.83 ± 0.43	91.18 ± 0.14	93.27 ± 0.10	93.32 ± 0.08
Adam	89.37 ± 0.37	91.83 ± 0.23	92.17 ± 0.20	92.21 ± 0.06
Amsgrad	90.25 ± 0.39	92.25 ± 0.18	92.36 ± 0.11	92.36 ± 0.09
AdamW	89.36 ± 0.29	90.23 ± 0.22	92.86 ± 0.15	92.97 ± 0.08
Padam	88.47 ± 0.55	92.49 ± 0.16	93.28 ± 0.12	93.45 ± 0.12
7
Under review as a conference paper at ICLR 2019
Figure 3 plots the Top-1 and Top-5 error against training epochs on the ImageNet dataset for both
VGGNet and ResNet5. We can see that on ImageNet, all methods behave similarly as in our CIFAR-
10 experiments. Padam method again obtains the best from both worlds by achieving the fastest con-
vergence while generalizing as well as SGD with momentum. Even though Amsgrad and AdamW
have greatly improve the performance comparing with standard Adam, it still suffers from the gen-
eralization gap on ImageNet.
0.2
O
10	20	30	40	50	60	70	80	90 IOO
Epochs
(a) Top-1 Error for VGGNet
」ou 川»9J_
O 10	20	30	40	50	60	70	80	90 IOO
Epochs
(b) Top-5 Error for VGGNet

O
O IO 20	30	40	50	60	70	80	90 IOO
Epochs
(c) Top-1 Error for ResNet
」OJJ川⅛ΦF
o.ιc ...............................
O 10	20	30	40	50	60	70	80	90 IOO
Epochs
(d) Top-5 Error for ResNet
Figure 3: Top-1 and Top-5 error for VGGNet and ResNet on ImageNet dataset. In all cases, Padam
achieves the fastest training speed and generalizes as well as SGD with momentum.
6	Related Work
We briefly review the related work in this section. There are only several studies closely related
to improving the generalization performance of Adam. Zhang et al. (2017) proposed a normalized
direction-preserving Adam (ND-Adam), which changes the adaptive terms from individual dimen-
sions to the whole gradient vector. This makes it more like a SGD-momentum with learning rate
adjusted in each iteration, rather than an adaptive gradient algorithm. The empirical result also
shows that its performance resembles SGD with momentum. Keskar & Socher (2017) proposed
to improve the generalization performance by switching from Adam to SGD. Yet the empirical re-
sult shows that it actually sacrifices some of the convergence speed for better generalization rather
than achieving the best from both worlds. Also deciding the switching learning rate and the best
switching point requires extra efforts on parameter tuning since they can be drastically different for
different tasks according to the paper. Loshchilov & Hutter (2017) proposed to fix the weight decay
regularization in Adam by decoupling the weight decay from the gradient update and this improves
the generalization performance of Adam.
7	Conclusions and Future Work
In this paper, we proposed Padam, which unifies Adam/Amsgrad with SGD-momentum. With an
appropriate choice of the partially adaptive parameter, we show that Padam can achieve the best
from both worlds, i.e., maintaining fast convergence rate while closing the generalization gap. We
also provide a theoretical analysis towards the convergence rate of Padam and show a similar data-
dependent regret bound as in Duchi et al. (2011); Reddi et al. (2018).
5We did not conduct WideResNet experiment on Imagenet dataset due to GPU memory limits.
8
Under review as a conference paper at ICLR 2019
It would be interesting to see how well Padam performs in other types of neural networks, such
as recurrent neural networks (RNNs) (Hochreiter & Schmidhuber, 1997) and generative adversarial
networks (GANs) (Goodfellow et al., 2014). We leave it as a future work.
References
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 248-255. Ieee, 2009.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770-778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent, 2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1729-1739, 2017.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint
arXiv:1711.05101, 2017.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. COLT 2010, pp. 244, 2010.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing Systems,
pp. 91-99, 2015.
9
Under review as a conference paper at ICLR 2019
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4151-4161, 2017.
Saining Xie, Ross Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5987-5995. IEEE, 2017.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A Walk With sgd. arXiv preprint
arXiv:1802.08770, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. arXiv preprint
arXiv:1605.07146, 2016.
MattheW D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam. arXiv
preprint arXiv:1709.04546, 2017.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML), pp. 928-936,
2003.
10
Under review as a conference paper at ICLR 2019
A Proof of the Main Results
A.1 Proof of Theorem 4.2
The proof of Theorem 4.2 needs the following lemmas.
Lemma A.1. Under the same conditions as in Theorem 4.2, we have
Td
Xt=1Xi=1
≤
αG∞-2p)√1+log T
(I- βI)(I- Y)(I- β2)p
d
kg1:T,ik2.
i=1
(A.1)
Lemma A.1 basically describes the bound of the key quantity (PT=I Pd=ι at ∙ m； /戏%) in the Con-
vergence analysis of Algorithm 1. Its proof is inspired by Lemma 2 in Reddi et al. (2018). Note that
the original proof in Kingma & Ba (2015) for the corresponding quantity contains some mistakes,
which result in a bound without the logarithmic term in (A.1). Here we follow the corrected version
in Reddi et al. (2018).
Lemma A.2. For any V ∈ S+d+ and convex feasible set X ⊂ Rd, suppose a1 = ΠX,Vp (b1),
a2 = ΠX,Vp (b2) forp ∈ (0, 1/2], we have
kVp(a1 - a2)k2 ≤ kVp(b1 -b2)k2,	(A.2)
where p ∈ (0, 1/2] is an absolute constant.
Lemma A.2 is an adaptation of Lemma 5 in McMahan & Streeter (2010) (or Lemma 4 in Reddi
et al. (2018)).
Now we are ready to prove Theorem 4.2.
Proof of Theorem 4.2. By Assumption 4.1, all ft’s are convex, we have
TT
Xft(θt) — ft(θt) ≤ Xhgt, θt — θ)	(A.3)
Consider the update rule for Algorithm 1, let Vt = diag(Vt), then we have θt+ι = Πχ VP (θt —
αt^Vt p ∙ mJ. Since ∏χ vP (θ*) = θ* for all θ* ∈ X, by Lemma A.2 we have
∣∣Vp∕2(θt+ι — θ*)∣∣2 ≤ IlVp∕2(θt — αtV-P ∙ mt — θ*)∣∣2∙	(a.4)
Now expand the square term on the right hand side of (A.4), we have
∣∣vp∕2 (θt+ι — θ*)∣∣2 ≤ ∣∣vp∕2(θt — θ*)∣∣2 + a2∣∣v-p∕2mt∣∣2 — 闪 hmt, θt — θ*i
=∣∣Vp∕2(θt — θ*)∣∣2 + a2∣∣V-p∕2mt∣∣2 — 2athβιtmt-i + (1 — βιt)gt, θt — θ*i,
(A.5)
where the second equality follows from the definition for mt. Rearrange the items in (A.5), we have
hgt, θt —	θ*i	= 20 (11	&)[^20+1 - θ*)∣∣2 - ∣∣Vp∕2(θt	— θ*)∣∣" + 2(1 at ʌ	∙ ∣∣V-p∕2mt∣∣2
2αt (1 —	β1t)	2(1 — β1t)
— 1⅛; hmt-1, θt — θ*i
≤ 2O(1⅛)修酌味 — θ*)∣∣2 TIVp∕2(θt - θ*)∣H + 2(⅛) ∙ ∣∣v-P∕2mt∣∣2
+ β1tat-1
2(1 — β1t)
• ∣∣v 'mt-i∣∣2 + 2θd‰
∣∣V p-21 (θt - θ*)k2,
(A.6)
11
Under review as a conference paper at ICLR 2019
where the inequality holds due to Young’s inequality. Summing over (A.6) for all 1 ≤ t ≤ T and
submitting it back into (A.3), we obtain
T
X ft(θt)- ft(θt)
t=1
T
≤X
t=1
2αt(1 - β1t)
T
+X
t=2
Td
≤XX
t=1 i=1
β1tαt-1
I|Vp/2e+i- θ* )∣∣2 -∣∣vy2(θt-⑻俏
2(1 - β1t)
bP,i
T
・ ∣∣V N2mt-1∣∣2+X
t=1
αt
2(1 - β1t)
2αt(1 - β1)
T
+X
t=2
β1t
2αt-1 (1 - β1t)
• IIV-p/2mt|l2
Td
(θt+ι,i- θi )2 - (θt,i - θi)2 + XX
{z
I1
t=1 i=1
}、
IlV p-1(θt
β1t∙ % (θ	θ* y
2αt(1-βι) (Ui)
{z
I2
θ*)k2
1
}
Td	2
1+ β1	Qa • mt,i
+ 2(1-11) ⅛ = ^¾T
(A.7)
I3
where the last inequality holds due to the fact that β1t = β1 λt-1 is monotonically decreasing with
t. For term I1 , we have
T d	vbp
X X 2αt(1- βι) [(θt+1,i - θi) - (θt,i - θi) _
=	1 X bP,i ・(% - θi) +	1 X X (bPi _ bP-1,i ʌ (θ , _ θ*)2
2(1 — βI) =	q1	2(1 — 尸I)与=V αt	αt-1 ) t' i
≤	D∞	X X bpi + X X (bPi _ bP-1,i ʌ!
—2(I- βI) Vi=I αι	= = (αt	αt-1 ))
=	D∞	X v£i
2(1 - βι) i=1 Qt
= 2θ(Γ‰ X √T • bT,i,	(A⑻
where the inequality follows from the bounded diameter condition and the definition of vbt ensures
that vbtP,i/Qt - vbtP-1,i/Qt-1 > 0, and the second equality holds due to simple telescope sum. For
term I2 , note that simple calculation yields that vbtP,i ≤ G2∞P, we have
Td
Xt=1 Xi=1
βιtbp,i
2αt(1 - βI)
(θt+1,i - θt)2 ≤
βιdD∞ G∞
2α(1 - 尸1)
T
X √tλt-1
t=1
βιdD∞ G∞
2α(1 - βι)(1 - λ)2 ,
(A.9)
≤
where the last inequality follows from first relaxing √t to t and then using the geometric series
summation rule. For term I3, by Lemma A.1 we have
Td	2
1 + B1	Qt • mt,i
21F t=1 i=1 Fr
≤
QG∞-2p)√1+log T
(1 - β1)2(1 - Y)(I- β2)p
d
X kg1:T,ik2.
i=1
(A.10)
Submitting (A.8), (A.9) and (A.10) back into (A.7) yields the desired result.	□
12
Under review as a conference paper at ICLR 2019
A.2 Proof of Corollary 4.4
Proof of Corollary 4.4. By Theorem 4.2, we have
Rτ ≤ 2Q(⅛) X √t ∙ bτ,i+
,	βιdD∞ G∞
+ 2α(1 - βι)(1 - λ)2 .
Note that we have
αG∞-2p)√1 + log T
(1 - β1 )2 (1 - γ )(1 - β2 )p
d
kg1:T ,ik2
i=1
d
kg1:T ,i
i=1
d
T
k2 ≤ EtEgt,i ≤ dG∞√T,
i=1	t=1
and by definition we also have
T
p
(1 - β2)∑βτj2,i	≤ (i-β2)p ∙ G出
j=1
Combine the above results we can easily have
RT = O(√T).
This completes the proof.
□
B Proof of Technical Lemmas in Appendix A
B.1 Proof of Lemma A.1
Proof. Consider
τd	2
αt ∙ mt,i
Vp
t=1 i=1	t,i
T -1 d
= Xt=1 Xi=1
T -1 d
≤ Xt=1 Xi=1
T -1 d
≤ Xt=1 Xi=1
2
Qt ∙ m2,i
-¾τ
2
αt ∙ m2,i
-¾τ
d
+X
i=1
d
+X
i=1
2
ατ ∙ mτ,i
~p~^~
2
ατ ∙ mτ,i
vT,i
Qt ∙ m2,i , α X (PT=ι(1-βj)βτ-jgj,i)
Fr + √T M ((1-β2) pτ=i βτ-g2,i)
p,
(B.1)
2
where the first inequality holds due to the definition of vbT,i and the second inequality follows from
the fact that β1t = β1λt-1 and λ ∈ (0, 1). (B.1) can be further bounded as:
τd	2
Qa ∙ mt,i
与⅛ Fr
T -1 d	2
αt ∙ mt,i
≤乙2	bP,
t=1 i=1 t,i
T -1 d	2
≤ XX a⅛Pm,i
+ √T(I - β2)p
d
X
i=1
αG(∞1-2p)
(P" βT-jj-2 P))(PT=1 βT-jj+2P)
t=1 i=1	t,i
T -1 d	2
≤ XX 卞 +
t=1 i=1	t,i
Tt(1 - βι)(i - β2)p
αG(∞1-2p)
d
X
i=1
T
(pτ=i βτ-g2,i)p
(PT=I βτ-j+2P))
(pτ=i βτ-g2,i)p
√T(1 - βι)(1 - β2)p
ΣΣ
(βT-gj1i+2P)
T -1 d	2
αt ∙ mt,i
≤乙2	bP,
t=1 i=1 t,i
αG(∞1-2p)
+-------------------
√T(I- βI)(I- β2)p
j=1 i=1
Td
Xj=1Xi=1γ
厂嵋i)p
T-j|gj,i|,
+
α
d
where the first inequality holds due to Cauchy-Schwarz inequality and the fact that 0 ≤ β1j < 1,
the second inequality follows from the bounded gradient condition and PjT=1 β1T -j ≤ 1/(1 - β1),
13
Under review as a conference paper at ICLR 2019
and the last inequality is due to the fact that βι /βp ≤ βι/√β2 = γ. NoW repeat the above process
for 1 ≤ t ≤ T , we have
Td
Xt=1Xi=1
2
αt ∙ m2,i
^br
≤
αG∞-2P)
(I- βI)(I- β2)p
T	td
X √t X X Y t-j|gj，i1
	αG∞-2p)	X XllX γj-t =(1 - βi)(1 - β2)p⅛⅛ lgt,il j=t √j G(i-2p)	d T	T ≤	αG∞	X X |gt,i| X γj-t 一(1- βι)(1- β2)p⅛ = √t j=tY (i-2p)	d T ≤ —αG∞	X X lgt,il	(B2) ≤ (1 - βi)(1 - β2)P⅛ = (1 - γ)√t,	(	)
Where the equality holds due to the change of the order of summation, the last inequality folloWs
from the fact that PjT=t γj-t ≤ 1/(1 - γ). By Cauchy-SchWarz inequality , (B.2) can be further
Written as
Td
Xt=1Xi=1
αG(∞1-2p)	d
≤ (1 - βι)(1 - γ)(1 - β2)P i=1 kg1：T，ik2t
T1
X t
t=1
≤
αG∞-2p)√1 + log T
(I - βI)(I - Y)(I - β2)p
d
kg1:T,i k2 .
i=1
This completes the proof.
□
B.2 Proof of Lemma A.2
Proof. The proof is inspired from Lemma 4 in Reddi et al. (2018) and Lemma 5 in McMahan &
Streeter (2010). Since a1 = argminx∈X kVp(x - b1)k2 and a2 = argminx∈X kVp(x - b2)k2, by
projection property We have
hV2p(a1 - b1), a2 - a1i ≥ 0,
hV2p(a2 - b2), a1 - a2i ≥ 0.
Combine the above inequalities We have
hV2p (a1 - b1),	a2	-	a1i	- hV2p(a2	- b2), a2 -	a1i	≥	0.	(B.3)
Rearrange (B.3) yields that
(b2 - b1)> V2p (a2 - a1) ≥ (a2 - a1)>V2p(a2 - a1).	(B.4)
Also since V is positive definite, the fact that (b2 - b1 - (a2 -a1))>V2p(b2 -b1 - (a2 -a1)) ≥ 0
immediately implies that
(b2 - bι)>V2p(b2 - bi) ≥ -(a2 - a1)>V2p(a2 - aι) + 2(a2 - aι)>V2p(b2 - bi). (B.5)
Combining (B.4) and (B.5) we have the desired result.	□
C Experiment Details
C.1 Datasets
We use several popular datasets for image classifications.
•	CIFAR-10 (Krizhevsky & Hinton, 2009): it consists of a training set of 50, 000 32 × 32
color images from 10 classes, and also 10, 000 test images.
•	CIFAR-100 (Krizhevsky & Hinton, 2009): itis similar to CIFAR-10 but contains 100 image
classes with 600 images for each.
•	ImageNet dataset (ILSVRC2012) (Deng et al., 2009): ILSVRC2012 contains 1.28 million
training images, and 50k validation images over 1000 classes.
14
Under review as a conference paper at ICLR 2019
C.2 Parameter Settings
We perform grid searches to choose the best base learning rate for all algorithms over
{0.0001, 0.001, 0.01, 0.1, 1}, the partial adaptive parameter p from {1/4, 1/8, 1/16} and the sec-
ond order momentum parameter β2 from {0.9, 0.99, 0.999}. For SGD with momentum, the base
learning rate is set to be 0.1 with a momentum parameter of 0.9. For Adam and Amsgrad, we set the
base learning rate as 0.001. For Padam, the base learning rate is set to be 0.1 and the partially adap-
tive parameter p is set to be 1/8 due to its best empirical performance. We also tune the momentum
parameters for all the adaptive gradient methods. After tuning, for Adam, Amsgrad, the momentum
parameters are set to be β1 = 0.9, β2 = 0.99. And for Padam, we set β1 = 0.9, β2 = 0.999. Padam
and SGD with momentum use a weight decay factor of 5 × 10-4 and for Adam/ Amsgrad we use a
weight decay factor of 1 × 10-4. For AdamW, the normalized weight decay weight decay factor is
set to 2.5 × 10-2 for CIFAR and 5 × 10-2 for ImageNet. All experiments use cross-entropy as the
loss function. The minibatch sizes for CIFAR-10 and CIFAR-100 are set to be 128 and for ImageNet
dataset we set it to be 256.
C.3 CNN Architectures
VGGNet (Simonyan & Zisserman, 2014): We use a modified VGG-16 architecture for this exper-
iment. The VGG-16 network uses only 3 × 3 convolutional layers stacked on top of each other
for increasing depth and adopts max pooling to reduce volume size. Finally, two fully-connected
layers 6 are followed by a softmax classifier.
ResNet (He et al., 2016): Residual Neural Network (ResNet) introduces a novel architecture with
“skip connections” and features a heavy use of batch normalization. Such skip connections are
also known as gated units or gated recurrent units and have a strong similarity to recent successful
elements applied in RNNs. We use ResNet-18 for this experiment, which contains 2 blocks for each
type of basic convolutional building blocks in He et al. (2016).
Wide ResNet (Zagoruyko & Komodakis, 2016): Wide Residual Network further exploits the “skip
connections” used in ResNet and in the meanwhile increases the width of residual networks. In
detail, we use the 16 layer Wide ResNet with 4 multipliers (WRN-16-4) in our experiments.
D Additional Experiments
Figure 4 plots the train loss and test error (top-1 error) against training epochs on the CIFAR-100
dataset. We can see that Padam achieves the best from both worlds by maintaining faster conver-
gence rate while also generalizing as well as SGD with momentum in the end.
Table 2: Test accuracy of VGGNet on CIFAR-100. Bold number indicates the best result.
Methods	Test Accuracy (%)			
	50th Epoch	100th Epoch	150th Epoch	200th Epoch
SGD Momentum	57.37 ± 0.55	68.63 ± 0.35	72.89 ± 0.18	73.03 ± 0.13
Adam	62.62 ± 0.41	66.73 ± 0.23	66.99 ± 0.20	67.17 ± 0.20
Amsgrad	63.54 ± 0.29	66.92 ± 0.33	67.24 ± 0.17	67.11 ± 0.18
AdamW	63.88 ± 0.38	67.97 ± 0.20	68.75 ± 0.19	69.00 ± 0.10
Padam	61.92 ± 0.22	70.52 ± 0.30	72.84 ± 0.25	72.95 ± 0.07
Table 2 shows the test accuracy of VGGNet on CIFAR-100 dataset. We can see that for CIFAR-100
dataset, Padam achieves the best test accuracy at 100th, and also keep really close to SGD with
momentum at 150th and 200th epochs (differences less than 0.1%).
As a side product of our experiments, we also examine the distance that θt has traversed in the
parameter space, i.e., kθt - θ0k2, as shown in many other studies (Hoffer et al., 2017; Xing et al.,
6For CIFAR experiments, we change the ending two fully-connected layers from 2048 nodes to 512 nodes.
For ImageNet experiments, we use batch normalized version (vgg16_bn) provided in Pytorch official package
15
Under review as a conference paper at ICLR 2019
sφl
0.5-
",SOn C-Pc-
0.0............................
0 2β 4« 6C 80 XOO 120
Epochs
160 180 2ββ
(a) Train Loss for VGGNet
",SOn C-Pc-
(b) Train Loss for ResNet
",SOn C-Pc-
0.0-.........................................
0 2C 4«	60	80 ICO 120 14« 160 180 200
Epochs
(c) Train Loss for Wide ResNet
(d) Test Error for VGGNet
0 2C 4«	60	80 ICO 12β 14« 160 180 200
Epochs
(e) Test Error for ResNet
0 2β 4«	60	80 IM 12« M« ɪeθ 180 2β0
Epochs
0.50
0.45
⅛
a035
0.30
0.25
0 2β 4« 60	80 ICO 120 14« Ieo 180 200
Epochs
(f) Test Error for Wide ResNet
20
10
Oooooo
8 7 6 5 4 3
-OBLB一
0	20	40	60	80	100 120 140 160 180 200
Epochs
(a)
Figure 4: Train loss and test error (top-1 error) of three CNN architectures on CIFAR-100. In all
cases, Padam achieves the fastest training procedure among all methods and generalizes as well as
SGD with momentum.
120100β06040
1 —工。。—时一
20
0
0	20	40	60	80 100 120 140 160 180 200
Epochs
(b)
Figure 5: Plots for kθt-θ0k2 against training epochs. Both experiments adopts VGGNet on CIFAR-
10 and CIFAR-100 datasets.
2018) discussing the generalization performance of various optimization algorithms for deep learn-
ing. Figure 5 shows the plot of the Euclidean distance of weight vector θt from initialization θ0,
against training epochs for ResNet on CIFAR-10 dataset7. First, we would like to emphasize that
these are the plots only regrading the Euclidean distance, which does not contain any directional
information. In other words, the cross in the plot does not mean that the weight vectors in different
optimization algorithms actually meet somewhere during the training process. As we can see from
the plots, SGD with momentum tends to overshoot a lot at the early stage of the entire training pro-
cess. This could explain why the convergence of SGD with momentum is slower at early stages. And
for Adam/Amsgrad, despite the quick start and less overshooting, the “small learning rate dilemma”
(see Section 3) largely limits the weight vector’s ability of exploring the parameter space in the mid-
dle and later stages, which could explain the bad generalization performance of Adam/Amsgrad.
On the other hand, Padam behaves inbetween SGD with momentum and Adam/Amsgrad, with less
severe overshooting compared with SGD with momentum, while maintaining a better capability
of traversing through the parameter space compared with Adam/Amsgrad. This partly justifies the
outstanding generalization performance of Padam.
7We did not compare with AdamW here since it changes the definition of weight decay and thus have a way
larger distance compared with other baselines.
16