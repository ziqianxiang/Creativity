Under review as a conference paper at ICLR 2019
Finding Mixed Nash Equilibria of
Generative Adversarial Networks
Anonymous authors
Paper under double-blind review
Abstract
We reconsider the training ob jective of Generative Adversarial Networks
(GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the
classical prox methods, we develop a novel algorithmic framework for GANs
via an infinite-dimensional two-player game and prove rigorous convergence
rates to the mixed NE. We then propose a principled procedure to reduce
our novel prox methods to simple sampling routines, leading to practically
efficient algorithms. Finally, we provide experimental evidence that our
approach outperforms methods that seek pure strategy equilibria, such as
SGD, Adam, and RMSProp, both in speed and quality.
1	Introduction
The Generative Adversarial Network (GAN) (Goodfellow et al., 2014) has become one of the
most powerful paradigms in learning real-world distributions, especially for image-related
data. It has been successfully applied to a host of applications such as image translation
(Isola et al., 2017; Kim et al., 2017; Zhu et al., 2017), super-resolution imaging (Wang et al.,
2015), pose editing (Pumarola et al., 2018b), and facial animation (Pumarola et al., 2018a).
Despite of the many accomplishments, the major hurdle blocking the full impact of GAN
is its notoriously difficult training phase. In the language of game theory, GAN seeks for a
pure strategy equilibrium, which is well-known to be ill-posed in many scenarios (Dasgupta
& Maskin, 1986). Indeed, it is known that a pure strategy equilibrium might not exist (Arora
et al., 2017), might be degenerate (S0nderby et al., 2017), or cannot be reliably reached by
existing algorithms (Mescheder et al., 2017).
Empirically, it has also been observed that common algorithms, such as SGD or Adam
(Kingma & Ba, 2015), lead to unstable training. While much efforts have been devoted into
understanding the training dynamics of GANs (Balduzzi et al., 2018; Gemp & Mahadevan,
2018; Gidel et al., 2018a;b; Liang & Stokes, 2018), a provably convergent algorithm for
general GANs, even under reasonably strong assumptions, is still lacking.
In this paper, we address the above problems with the following contributions:
1.	We propose to study the mixed Nash Equilibrium (NE) of GANs: Instead of search-
ing for an optimal pure strategy which might not even exist, we optimize over the
set of probability distributions over pure strategies of the networks. The existence of
a solution to such problems was long established amongst the earliest game theory
work (Glicksberg, 1952), leading to well-posed optimization problems.
2.	We demonstrate that the prox methods of (Nemirovsky & Yudin, 1983; Nemirovski,
2004), which are fundamental building blocks for solving two-player games with
finitely many strategies, can be extended to continuously many strategies, and hence
applicable to training GANs. We provide an elementary proof for their convergence
rates to learning the mixed NE.
3.	We construct a principled procedure to reduce our novel prox methods to certain
sampling tasks that were empirically proven easy by recent work (Chaudhari et al.,
2017; 2018; Dziugaite & Roy, 2018). We further establish heuristic guidelines to
greatly scale down the memory and computational costs, resulting in simple algo-
rithms whose per-iteration complexity is almost as cheap as SGD.
1
Under review as a conference paper at ICLR 2019
4.	We experimentally show that our algorithms consistently achieve better or com-
parable performance than popular baselines such as SGD, Adam, and RMSProp
(Tieleman & Hinton, 2012).
Related Work: While the literature on training GANs is vast, to our knowledge, there
exist only few papers on the mixed NE perspective. The notion of mixed NE is already
present in (Goodfellow et al., 2014), but is stated only as an existential result. The authors
of (Arora et al., 2017) advocate the mixed strategies, but do not provide a provably conver-
gent algorithm. (Oliehoek et al., 2018) also considers mixed NE, but only with finitely many
parameters. The work (Grnarova et al., 2018) proposes a provably convergent algorithm for
finding the mixed NE of GANs under the unrealistic assumption that the discriminator is a
single-layered neural network. In contrast, our results are applicable to arbitrary architec-
tures, including popular ones (Arjovsky et al., 2017; Gulra jani et al., 2017).
Due to its fundamental role in game theory, many prox methods have been applied to study
the training of GANs (Daskalakis et al., 2018; Gidel et al., 2018a; Mertikopoulos et al.,
2018). However, these works focus on the classical pure strategy equilibria and are hence
distinct from our problem formulation. In particular, they give rise to drastically different
algorithms from ours and do not provide convergence rates for GANs.
In terms of analysis techniques, our framework is closely related to (Balandat et al., 2016),
but with several important distinctions. First, the analysis of (Balandat et al., 2016) is
based on dual averaging (Nesterov, 2009), while we consider Mirror Descent and also the
more sophisticated Mirror-Prox (see Section 3). Second, unlike our work, (Balandat et al.,
2016) do not provide any convergence rate for learning mixed NE of two-player games.
Finally, (Balandat et al., 2016) is only of theoretical interest with no practical algorithm.
Notation: Throughout the paper, we use z to denote a generic variable and Z ⊆ Rd its
domain. We denote the set of all Borel probability measures on Z by M(Z), and the set
of all functions on Z by F(Z).1 We write dμ = Pdz to mean that the density function
of μ ∈ M(Z) with respect to the Lebesgue measure is ρ. All integrals without specifying
the measure are understood to be with respect to Lebesgue. For any ob jective of the
form minx maxy F(x, y), we say that (xT , yT) is an O T -1/2 -NE if maxx,y {F (xT , y) -
F(x, yτ)} = O (T-1/2). Similarly We can define O (T-1)-NE. The Symbolk ∙∣∣l∞ denotes the
L∞-norm of functions, and ∣∣∙∣∣tv denotes the total variation norm of probability measures.
2	Problem Formulation
We review standard results in game theory in Section 2.1, whose proof can be found in
(Bubeck, 2013a;b;c). Section 2.2 relates training of GANs to the two-player game in Sec-
tion 2.1, thereby suggesting to generalize the prox methods to infinite dimension.
2.1	Preliminary: Prox Methods for Finite Games
Consider the classical formulation of a two-player game with finitely many strategies:
min max hq, ai - hq, Api ,
p∈∆
m q∈∆n
(1)
where A is a payoff matrix, a is a vector, and ∆d := z ∈ Rd | Pid=1 zi = 1 is the proba-
bility simplex, representing the mixed strategies (i.e., probability distributions) over d pure
strategies. A pair (pNE, qNE) achieving the min-max value in (1) is called a mixed NE.
Assume that the matrix A is too expensive to evaluate whereas the (stochastic) gradients of
(1) are easy to obtain. Under such settings, a celebrated algorithm, the so-called entropic
Mirror Descent (entropic MD), learns an O (T-1/2)-NE: Let φ(z) := Pd=Izi log Zi be the
entropy function and φ*(y) := logPid=1 eyi = supζ∈∆d{hz, yi — φ(z)} be its Fenchel dual.
1Strictly speaking, our derivation requires mild regularity (see Appendix A.1) assumptions on
the probability measure and function classes, which are met by most practical applications.
2
Under review as a conference paper at ICLR 2019
For a learning rate η and an arbitrary vector b ∈ Rd , define the MD iterates as
z e-ηbi
z0 = MDn (z, b)	≡	z0	= Vφ?	(Vφ(z) —	ηb)	≡	z0	= —厂----------,	∀1	≤ i ≤ d. (2)
i	Pid=1 zi e-ηbi
The equivalence of the last two formulas in (2) can be readily checked.
Denote by PT := T PT=I Pt and qτ := T1 PT=I qt the ergodic average of two sequences
{pt }tT=1 and {qt }tT=1. Then, with a properly chosen step-size η, we have
ʃ pt+1 = MD"”,-'：号)、	⇒	(Pt, qτ) is an O (τT2)-NE.
qt+1 = MDη (qt, -a + APt)	T T
Moreover, a slightly more complicated algorithm, called the entropic Mirror-Prox (en-
tropy MP) (Nemirovski, 2004), achieves faster rate than the entropic MD:
Pt = MDn (Pt, -A>qt) , Pt+1 = MDn (Pt, -A>qt)	⇒
qt = MDn (qt, -a + APt), <7t+ι = MDn ®, -a + APt)
(PT, qτ) is an O (T-1)-NE.
If, instead of deterministic gradients, one uses unbiased stochastic gradients for entropic MD
and MP, then both algorithms achieve O T -1/2 -NE in expectation.
2.2	Mixed Strategy Formulation for Generative Adversarial Networks
For illustration, let us focus on the Wasserstein GAN (Arjovsky et al., 2017), and we perform
a common bilinearization trick that dates back at least to the early game theory literature
(Glicksberg, 1952), and is also well-known in optimal transport theory (Villani, 2003).
The training objective of Wasserstein GAN is
min maχEχ〜Preai[fw(X)] - EX〜p0 [fw(X)],
θ∈Θ w∈W
(3)
where Θ is the set of parameters for the generator and W the set of parameters for the
discriminator f , typically both taken to be neural nets. As mentioned in the introduction,
such an optimization problem can be ill-posed, which is also supported by empirical evidence.
The high-level idea of our approach is, instead of solving (3) directly, we focus on the mixed
strategy formulation of (3). In other words, we consider the set of all probability distributions
over Θ and W , and we search for the optimal distribution that solves the following program:
min max Ew〜μEχ〜%技[fw(X)] - Ew〜*Ee“EX〜Pθ [fw(X)].	(4)
ν∈M(Θ) μ∈M(W)
Define the function g : W → R by g(w) := EX〜Preal [fw(X)] and the operator G : M(Θ) →
F(W) as (GV)(w) := Eθ〜ν,x〜Pθ[fw(X)]∙ Denoting (μ,h> := Eμh2 for any probability
measure μ and function h, we may rewrite (4) as
min max
ν∈M(Θ) μ∈M(W)
hμ, gi - hμ, GVi.
(5)
Furthermore, the FreChet derivative (the analogue of gradient in infinite dimension) of (5)
with respect to μ is simply g — Gν, and the derivative of (5) with respect to V is -G*μ,
where Gt : M(W) → F(Θ) is the adjoint operator of G defined via the relation
∀μ ∈ M(W), V ∈ M(Θ),	hμ,Gνi = <ν,Gtμ> .	(6)
One can easily check that (Gtμ)(θ) := EX〜p®,w〜μ[fw(X)] achieves the equality in (6).
To summarize, the mixed strategy formulation of Wasserstein GAN is (5), whose derivatives
can be expressed in terms of g and G. We now make the crucial observation that (5) is
the infinite-dimensional analogue of (1): The distributions over finite strategies are replaced
with probability measures over a continuous parameter set, the vector a is replaced with
a function g, the matrix A is replaced with a linear operator3 G, and the gradients are
replaced with FreChet derivatives. Based on Section 2.1, it is then natural to ask:
2It should be noted that hμ, hi is NOT an inner product, and rather is the dual pairing in Banach
spaces (Halmos, 2013).
3 The linearity of G trivially follows from the linearity of expectation.
3
Under review as a conference paper at ICLR 2019
Can the entropic Mirror Descent and Mirror-Prox be extended to infinite
dimension to solve (5)? Can we retain the convergence rates?
We provide an affirmative answer to both questions in the next section.
Remark. The derivation in Section 2.2 can be applied to any GAN ob jective.
3	Infinite-Dimensional Prox Methods
This section builds a rigorous infinite-dimensional formalism in parallel to the finite-
dimensional prox methods and proves their convergence rates. While simple in retrospect,
to our knowledge, these results are new.
3.1	Preparation: The Mirror Descent Iterates
We first recall the notion of (Frechet) derivative in infinite-dimensional spaces. A (nonlinear)
functional Φ : M(Z) → R is said to possess a derivative at μ ∈ M(Z) if there exists a
function dΦ(μ) ∈ F(Z) such that, for all μ0 ∈ M(Z), We have
Φ(μ + eμ0) = Φ(μ) + E hμ0, dΦ(μ)i + o(e).
Similarly, a (nonlinear) functional Φ? : F(Z) → R is said to possess a derivative at h ∈ F(Z)
if there exists a measure dΦ?(h) ∈ M(Z) such that, for all h0 ∈ F(Z), we have
Φ*(h + eh0) = Φ*(h) + E (dΦ*(h), h0i + o(e).
The most important functionals in this paper are the (negative) Shannon entropy
and its Fenchel dual
μ ∈M(Z),
Φ(μ) := / dμ log
dμ
dZ
h∈ F(Z),
Φ*(h) := log /
ehdz.
The first result of our paper is to show that, in direct analogy to (2), the infinite-dimensional
MD iterates can be expressed as:
Theorem 1 (Infinite-Dimensional Mirror Descent, informal). For a learning rate η and an
arbitrary function h, we can equivalently define
e-ηh dμ
μ+ = MDn (μ,h)	≡ μ+ =dΦ* (dΦ(μ) - ηh) ≡	dμ+ = R e-ηhdμ.
(7)
Moreover, most the essential ingredients in the analysis of finite-dimensional prox methods
can be generalized to infinite dimension.
See Theorem 4 of Appendix A for precise statements and a long list of “essential ingredients
of prox methods” generalizable to infinite dimension.
3.2 Infinite-Dimensional Prox Methods and Convergence Rates
Armed with results in Section 3.1, we now introduce two “conceptual” algorithms for solving
the mixed NE of Wasserstein GANs: The infinite-dimensional entropic MD in Algorithm
1 and MP in Algorithm 2. These algorithms iterate over probability measures and cannot
be directly used in practice, but they possess rigorous convergence rates, and hence motivate
the reduction procedure in Section 4 to come.
Algorithm 1: Infinite-Dimensional Entropic MD
Input: Initial distributions μι,νι, learning rate η
for t = 1, 2, . . . , T - 1 do
|_ νt+ι = MDn (νt, -G*μt) , μt+ι = MDn (μt, -g + Gνt);
return VT = T PT=I Vt and μτ = T PT=I μt.
4
Under review as a conference paper at ICLR 2019
Algorithm 2: Infinite-Dimensional Entropic MP
Input: Initial distributions “ι,Vι, learning rate η
for t = 1, 2, . . . , T do
Vt = MDn (Vt, -G*μt) , μt = MDn (μt, -g + GVt);
_ νt+ι = MDn (Vt, -G*μt) , μt+ι = MDn (μt, -g + Gνt);
return DT = T PT=I Vt and μτ = T PT=I μt.
Theorem 2 (Convergence Rates). Let Φ(μ) = J dμlog 翌.Let M be a constant such that
max [∣∣-g + Gνk∞ , ∣∣G*μ∣∣L∞] ≤ M, and L be such that ∣∣G(ν - V0)k∞ ≤ L IlV - ν0∣∣τv
and ∣∣G^(μ — μ0)I∣L∞ ≤ * L kμ —μ0∣τv. Let D(∙, ∙) be the relative entropy, and denote by
Do := D(μNE, μι) + D(vne,vi) the initial distance to the mixed NE. Then
1. Assume that We have access to the deterministic derivatives {-G*μt}T=ι
{g — Gv}T=ι. Then Algorithm 1 achieves O (T-'"1-NE with η = M JDO,
Algorithm 2 achieves O (T-、-NE with η = L4.
and
and
T
2. Assume that we have access to stochastic derivatives ∣-(G^μt∣	and
such that max [e∣∣-g + Gv∣∣	,E∣∣G*μ∣∣	≤ ≤ M0, and the
^--G v}
variance is
t=1
up-
per bounded by σ2 . Assume also that the bias of stochastic derivatives sat-
isfies max [∣∣E[-g + GV] + g — Gν∣∣	, ∣∣E[G*μ] — G*μ∣∣	] ≤ T. Then Algo-
rithm 1 with stochastic derivatives achieves O T -1/2 -NE in expectation with
JT(4 D0w0/4), and Algorithm 2 with stochastic derivatives achieves
(O (T-1/2) + O(T))-NE in expectation with η = min
2Do
3Tσ2
η
The proof can be found in Appendix B and C.
Remark. If, as in previous work (Arora et al., 2017), we assume the output of the discrim-
inator to be bounded by U, then we have M, M0 ≤ 2U and L ≤ U in Theorem 2. The
constant error term for stochastic MP is standard; see, e.g., (Juditsky et al., 2011).
4 From Theory to Practice
Section 4.1 reduces Algorithm 1 and Algorithm 2 to a sampling routine (Welling &
Teh, 2011) that has widely been used in machine learning. Section 4.2 proposes to further
simplify the algorithms by summarizing a batch of samples by their mean.
For simplicity, we will only derive the algorithm for entropic MD; the case for entropic MP is
similar but requires more computation. To ease the notation, we assume η = 1 throughout
this section as η does not play an important role in the derivation below.
4.1	Implementable Entropic MD: From Probability Measure to Samples
Consider Algorithm 1. The reduction consists of three steps.
Step 1: Reformulating Entropic Mirror Descent Iterates
The definition of the MD iterate (7) relates the updated probability measure μt+ι to the
current probability measure μt, but it tells us nothing about the density function of μt+ι,
from which we want to sample. Our first step is to express (7) in a more tractable form.
By recursively applying (7) and using Theorem 4.10 in Appendix A, we have, for some
5
Under review as a conference paper at ICLR 2019
constants C1, ..., CT-1,
dΦ(μτ) = dΦ(μτ —i) — (—g + GVT-i) + CT-1
=dΦ(μτ-2) — (—g + GVT-2) — (—g + GVT-1) + CT-1 + Ct-2
T-1	T-1
—(T —1)g+GXVs +XCs.
s=1	s=1
For simplicity, assume that μι is uniform so that dΦ(μι) is a constant function. Then,
by (13) and that dΦ? (dΦ(μT)) = dμγ, we see that the density function of μT is simply
dμτ
exP{(T-I)g-GPT=II Vs}dw	Si il l	h	d"	_	exp{Gt PT=11 μs}dθ
R exp{(T-1)g-G PT=11 Vs}dw .	SimiIarly,	We have	dνT	=	R eχp{Gt p：=1 μs}dθ .
Step 2: Empirical Approximation for Stochastic Derivatives
The derivatives of (5) involve the function g and operator G. Recall that g requires taking
expectation over the real data distribution, which we do not have access to. A common
approach is to replace the true expectation with its empirical average:
1n
g(w) = Eχ~Preai[fw (X)] ' — Efw (Xreal)二 g(w)
n i=1
where Xi's are real data and n is the batch size. Clearly, g is an unbiased estimator of g.
On the other hand, GVt and G*μt involve expectation over Vt and μt, respectively, and also
over the fake data distribution Pθ . Therefore, if we are able to draw samples from μt and
Vt , then we can again approximate the expectation via the empirical average:
n n0
θ⑴,θ⑵,…,θg~Vt, nXi(j)}iι~Pθ(j), GVt(W)`nnoXX/we，))
i=1	i=1 j=1
0
nn
W⑴,W⑵，…,WSO) 〜 μt, {Xi}n=ι ~ Pθ,	Gtμt(θ)'	XX fw(j) (Xi) .
Now, assuming that we have obtained unbiased stochastic derivatives — P：=i G弋μs and
PS=ι (—^ + GVs), how do we actually draw samples from μt+ι and Vt+ι? Provided we can
answer this question, then we can start with two easy-to-sample distributions (μι,Vι), and
then we will be able to draw samples from (μ2,V2). These samples in turn will allow us
to draw samples from (μ3,V3), and so on. Therefore, it only remains to answer the above
question. This leads us to:
Step 3: Sampling by Stochastic Gradient Langevin Dynamics
For any probability distribution with density function e-hdz, the Stochastic Gradient
Langevin Dynamics (SGLD) (Welling & Teh, 2011) iterates as
Zk+1 = Zk — YV h(zk) + vz2γeξk,	(8)
where Y is the step-size, Vh is an unbiased estimator of Vh, E is the thermal noise, and
ξk 〜N(0, I) is a standard normal vector, independently drawn across different iterations.
C	J J J /	∖ T-»l	♦	1	z^(÷	1	1	ʌ .	♦	,	∕c∖	1 」	♦
Suppose we start at (μι, vi). Plugging h ‹--G'μι and h <----^ + Gvi into (8), we obtain,
for {Xi}n=ι 〜Pθk, {w(j)}n= 1 〜μι, standard normal ξk,ξk, and X；eal ~ Preal, {θ⑶ j= 1 〜
νι, {X(j)}〜 Pθ(j ) , the following update rules:
(1 n n	∖
---0 ɪsfw(j) (Xi) I + Vz2γcξk
nn0
i=1 j=1
(In	InnO	∖
-Xfwk(Xireal) - nn0XXfwk (x* 1 + SW、.
i=1	i=1 j=1
6
Under review as a conference paper at ICLR 2019
The theory of (Welling & Teh, 2011) states that, for large enough k, the iterates of SGLD
above (approximately) generate samples according to the probability measures (μ2, ν2). We
can then apply this process recursively to obtain samples from (μ3, ν3), (μ4, ν4),…(μτ, VT).
Finally, since the entropic MD and MP output the averaged measure (μτ, VT), it suffices to
pick a random index t ∈{1, 2, ...,T} and then output samples from (μ^, ν^).
Putting Step 1-3 together, we obtain Algorithm 4 and 5 in Appendix D.
Remark. In principle, any first-order sampling method is valid above. In the experimental
section, we also use a RMSProp-preconditioned version of the SGLD (Li et al., 2016).
4.2 Summarizing Samples by Averaging: A Simple yet Effective Heuristic
Although Algorithm 4 and 5 are implementable, they are quite complicated and resource-
intensive, as the total computational complexity is O(T2 ). This high complexity comes from
the fact that, when computing the stochastic derivatives, we need to store all the historical
samples and evaluate new gradients at these samples.
An intuitive approach to alleviate the above issue is to try to summarize each distribution
by only one parameter. To this end, the mean of the distribution is the most natural
candidate, as it not only stablizes the algorithm, but also is often easier to acquire than the
actual samples. For instance, computing the mean of distributions of the form e-hdz , where
h is a loss function defined by deep neural networks, has been empirically proven successful in
(Chaudhari et al., 2017; 2018; Dziugaite & Roy, 2018) via SGLD. In this paper, we adopt the
same approach as in (Chaudhari et al., 2017) where we use exponential damping (the β term
in Algorithm 3) to increase stability. Algorithm 3, dubbed the Mirror-GAN, shows how
to encompass this idea into entropic MD; the pseudocode for the similar Mirror-Prox-GAN
can be found in Algorithm 6 of Appendix D.
Algorithm 3: Mirror-GAN: Approximate Mirror Decent for GANs
Input: Wι, θι - random initialization, {γt}T=ι, {∈t}T=ι, {Kt}T-1,β (see Appendix D
for meaning of the hyperparameters), standard normal noise ξk , ξk0 .
for t = 1, 2, . . . , T - 1 do
(1)
Wt, Wt — Wt；
θt, θ(1) 一 θt;
for k = 1, 2, . . . , Kt do
Generate A = {Xι,..., Xn}〜P8,)；
θ(k+1) = θ(k) + Yt Vθ Pχi∈A fwt (Xi) + √2γtetξk;
Generate B = {X；eal,..., Xneal}〜Preai；
Generate B0 = {X：,..., Xn}〜Pθt;
w(k+1) = w(k) + γtVw X fw(k)(Xreal)- γtVw X fw(k)X) + R&;
n	tn	t
Xireal ∈B	Xi0 ∈B0
Wt — (1 - β)Wt + βw(k+1);
L θt J (1 — β)θt + βθ(k+1) ；
Wt+1 J (1 — β)wt + βWt；
_ θt+1 J (1 — β)θt + βθt;
return WT, θT.
5	Experimental Evidence
The purpose of our experiments is twofold. First, we use established baselines to demonstrate
that Mirror- and Mirror-Prox-GAN consistently achieve better or comparable performance
than common algorithms. Second, we report that our algorithms are stable and always
improve as the training process goes on. This is in contrast to unstable training algorithms,
such as Adam, which often collapse to noise as the iteration count grows. (Cha, 2017).
7
Under review as a conference paper at ICLR 2019
We use visual quality of the generated images to evaluate different algorithms. We avoid
reporting numerical metrics, as recent studies (Barratt & Sharma, 2018; Borji, 2018; Lucic
et al., 2018) suggest that these metrics might be flawed. Setting of the hyperparameters
and more auxiliary results can be found in Appendix E.
5.1	Synthetic Data
We repeat the synthetic setup as in (Gulrajani et al., 2017). The tasks include learning
the distribution of 8 Gaussian mixtures, 25 Gaussian mixtures, and the Swiss Roll. For
both the generator and discriminator, we use two MLPs with three hidden layers of 512
neurons. We choose SGD and Adam as baselines,
Mirror-Prox-GAN. All algorithms are run up to 105
and we compare them to Mirror- and
iterations4 . The results of 25 Gaussian
mixtures are shown in Figure 1; An enlarged figure of 25 Gaussian Mixtures and other cases
can be found in Appendix E.1.
(a) SGD
(b) Adam
-1,S	-1Λ -Os g Os 1Λ 13
(c) Mirror-GAN
-IS -IQ -OS g OS l∙0 IS
(d) Mirror-Prox-GAN
Figure 1: Fitting 25 Gaussian mixtures up to 105 iterations. Blue dots represent the true
distribution and red ones are from the trained generator.
As Figure 1 shows, SGD performs poorly in this task, while the other algorithms yield
reasonable results. However, compared to Adam, Mirror- and Mirror-Prox-GAN fit the true
distribution better in two aspects. First, the modes found by Mirror- and Mirror-Prox-
GAN are more accurate than the ones by Adam, which are perceivably biased. Second,
Mirror- and Mirror-Prox-GAN perform much better in capturing the variance (how spread
the blue dots are), while Adam tends to collapse to modes. These observations are consistent
throughout the synthetic experiments; see Appendix E.1.
5.2	Real Data
For real images, we use the LSUN bedroom dataset (Yu et al., 2015). We have also conducted
a similar study with MNIST; more results can be found in Appendix E.2.
We use the same architecture (DCGAN) as in (Radford et al., 2015) with batch normaliza-
tion. As the networks become deeper in this case, the gradient magnitudes differ significantly
across different layers. As a result, non-adaptive methods such as SGD or SGLD do not
perform well in this scenario. To alleviate such issues, we replace SGLD by the RMSProp-
preconditioned SGLD (Li et al., 2016) for our sampling routines. For baselines, we consider
two adaptive gradient methods: RMSprop and Adam. We also include two contemporary
algorithms, the Simultaneous and Alternated Extra-Adam, from the concurrent ICLR sub-
mission (Gidel et al., 2018a).
Figure 2 shows the results at the 105th iteration. The RMSProp, Alternated Extra-Adam
and Mirror-GAN produce images with reasonable quality, while Adam and simultaneous
Extra-Adam output noise. The visual quality of Alternated Extra-Adam and Mirror-GAN
are comparable, and are better than RMSProp, as RMSProp sometimes generates blurry
images (the (3, 3)- and (1, 5)-th entry of Figure 8.(b)).
It is worth mentioning that Adam can learn the true distribution at intermediate iterations,
but later on suffers from mode collapse and finally degenerates to noise; see Appendix E.2.2.
4One iteration here means using one mini-batch of data. It does not correspond to the T in our
algorithms, as there might be multiple SGLD iterations within each time step t.
8
Under review as a conference paper at ICLR 2019
Figure 2: Dataset LSUN bedroom, 105 iterations.
(e) Alternated Extra-Adam
6	Conclusions
Our goal of systematically understanding and expanding on the game theoretic perspective
of mixed NE along with stochastic Langevin dynamics for training GANs is a promising
research vein. While simple in retrospect, we provide guidelines in developing approximate
infinite-dimensional prox methods that mimic closely the provable optimization framework
to learn the mixed NE of GANs. Our proposed Mirror- and Mirror-Prox-GAN algorithm
feature cheap per-iteration complexity while rapidly converging to solutions of good quality.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). In International Conference on Machine
Learning, pp. 224—232, 2017.
Maximilian Balandat, Walid Krichene, Claire Tomlin, and Alexandre Bayen. Minimizing
regret on reflexive banach spaces and nash equilibria in continuous zero-sum games. In
Advances in Neural Information Processing Systems, pp. 154—162, 2016.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore
Graepel. The mechanics of n-player differentiable games. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 354-363, Stockholmsmassan,
Stockholm Sweden, 10-15 Jul 2018. PMLR.
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint
arXiv:1801.01973, 2018.
9
Under review as a conference paper at ICLR 2019
Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.03446, 2018.
Sebastien Bubeck. Orf523: Mirror descent, part i/ii, 2013a. URL https://blogs.
princeton.edu/imabandit/2013/04/16/orf523-mirror-descent-part-iii/.
Sebastien Bubeck. Orf523: Mirror descent, part ii/ii, 2013b. URL https://blogs.
princeton.edu/imabandit/2013/04/18/orf523-mirror-descent-part-iiii/.
Sebastien Bubeck. Orf523: Mirror prox, 2013c. URL https://blogs.princeton.edu/
imabandit/2013/04/23/orf523-mirror-prox/.
Junbum Cha. Implementations of (theoretical) generative adversarial networks and compar-
ison without cherry-picking. https://github.com/khanrc/tf.gans-comparison, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Chris-
tian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing
gradient descent into wide valleys. In International Conference on Learning Representa-
tions, 2017.
Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and Guillaume Car-
lier. Deep relaxation: partial differential equations for optimizing deep neural networks.
Research in the Mathematical Sciences, 5(3):30, Jun 2018.
Partha Dasgupta and Eric Maskin. The existence of equilibrium in discontinuous economic
games, i: Theory. The Review of economic studies, 53(1):1-26, 1986.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training
GANs with optimism. In International Conference on Learning Representations, 2018.
Gintare Karolina Dziugaite and Daniel Roy. Entropy-sgd optimizes the prior of a pac-
bayes bound: Generalization properties of entropy-sgd and data-dependent priors. In
International Conference on Machine Learning, pp. 1376-1385, 2018.
Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using
variational inequalities. arXiv preprint arXiv:1808.01531, 2018.
J Willard Gibbs. Elementary principles in statistical mechanics. Yale University Press,
1902.
Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien. A variational
inequality perspective on generative adversarial nets. arXiv preprint arXiv:1802.10551,
2018a.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Remi Lep-
riol, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved
game dynamics. arXiv preprint arXiv:1807.04740, 2018b.
Irving L Glicksberg. A further generalization of the kakutani fixed point theorem, with
application to nash equilibrium points. Proceedings of the American Mathematical Society,
3(1):170-174, 1952.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems, pp. 2672-2680, 2014.
Robert M Gray. Entropy and information theory. Springer Science & Business Media, 2011.
Paulina Grnarova, Kfir Y Levy, Aurelien Lucchi, Thomas Hofmann, and Andreas Krause. An
online learning approach to generative adversarial networks. In International Conference
on Learning Representations, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C
Courville. Improved training of wasserstein gans. In Advances in Neural Information
Processing Systems, pp. 5767-5777, 2017.
10
Under review as a conference paper at ICLR 2019
Paul R Halmos. Measure theory, volume 18. Springer, 2013.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation
with conditional adversarial networks. arXiv preprint, 2017.
Anatoli Juditsky and Arkadi Nemirovski. First order methods for nonsmooth convex large-
scale optimization, ii: utilizing problems structure. Optimization for Machine Learning,
pp.149-183, 2011.
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities
with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to
discover cross-domain relations with generative adversarial networks. In International
Conference on Machine Learning, pp. 1857-1865, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
International Conference on Learning Representations, 2015.
Chunyuan Li, Changyou Chen, David E Carlson, and Lawrence Carin. Preconditioned
stochastic gradient langevin dynamics for deep neural networks. In AAAI, 2016.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local
convergence of generative adversarial networks. arXiv preprint arXiv:1802.06132, 2018.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are
gans created equal? a large-scale study. In Advances in neural information processing
systems, 2018.
Panayotis Mertikopoulos, Houssam Zenati, Bruno Lecouat, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Mirror descent in saddle-point problems: Going the
extra (gradient) mile. arXiv preprint arXiv:1807.02629, 2018.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances
in Neural Information Processing Systems, pp. 1825-1835, 2017.
Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities
with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization, 15(1):229-251, 2004.
AS Nemirovsky and DB Yudin. Problem complexity and method efficiency in optimization.
1983.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical pro-
gramming, 120(1):221-259, 2009.
Frans A Oliehoek, Rahul Savani, Jose Gallego, Elise van der Pol, and Roderich GroB. Beyond
local nash equilibria for adversarial networks. arXiv preprint arXiv:1806.07268, 2018.
Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto Sanfeliu, and Francesc
Moreno-Noguer. Ganimation: Anatomically-aware facial animation from a single image.
In Proceedings of the European Conference on Computer Vision (ECCV), pp. 818-833,
2018a.
Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, and Francesc Moreno-Noguer. Unsu-
pervised person image synthesis in arbitrary poses. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 8620-8628, 2018b.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434,
2015.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amor-
tised map inference for image super-resolution. 2017.
11
Under review as a conference paper at ICLR 2019
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a
running average of its recent magnitude. COURSERA: Neural networks for machine
learning, 4(2):26-31, 2012.
Cedric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc.,
2003.
Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, and Thomas Huang. Deep networks
for image super-resolution with sparse prior. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 370-378, 2015.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp.
681-688, 2011.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.
Lsun: Construction of a large-scale image dataset using deep learning with humans in the
loop. arXiv preprint arXiv:1506.03365, 2015.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In IEEE International Conference
on Computer Vision, 2017.
12
Under review as a conference paper at ICLR 2019
A A Framework for Infinite-Dimensional Mirror Descent
A.1 A note on the regularity
It is known that the (negative) Shannon entropy is not Frechet differentiable in general.
However, below We show that the Frechet derive can be well-defined if We restrict the Prob-
ability measures to within the set
M(Z) :={all probability measures on Z that admit densities w.r.t. the Lebesgue measure,
and the density is continuous and positive almost everywhere on Z}.
We will also restrict the set of functions to be bounded and integrable:
F(Z) := all bounded continuous functions f on Z such that	e-f < ∞
It is important to notice that μ ∈ M(Z) and h ∈ F(Z) implies μ0 = MDn (μ, h) ∈ M(Z);
this readily follows from the formula (7).
A.2 Properties of Entropic Mirror Map
The total variation of a (possibly non-probability) measure μ ∈ M(Z) is defined as (Halmos,
2013)
∣∣μkτv = sup I hdμ = sup hμ, h .
khkL∞ ≤1	khkL∞ ≤1
Recall the standard topology induced by ∣∣∙∣∣tv and ∣∣∙∣∣l∞ for measures and functions (Hal-
mos, 2013), respectively. Whenever we speak about continuity or differentiability below, it
is understood to be w.r.t. to the standard topology. Notice also that the G operator defined
in (5) is bounded if the discriminator fw is bounded, and hence continous (Halmos, 2013).
We depart from the fundamental Gibbs Variational Principle, which dates back to the ear-
liest work of statistical mechanics (Gibbs, 1902). For two probability measures μ, μ0, denote
their relative entropy by (the reason for this notation will become clear in (14))
DΦ(μ, μ0) := J dμlog dμ0.
By the definition of M(Z), it is clear that the relative entropy is well-defined for any
μ, μ ∈ M(Z).
Theorem 3	(Gibbs Variation Principle). Let h ∈ F(Z) and μ0 ∈ M(Z) be a reference
measure. Then
log
ehdμ0
sup hμ, hi 一 Dφ(μ, μ0),
μ∈M(Z)
(9)
Z
and equality is achieved by dμ?
ehdμ0
RZ ehdμ0 .
Part of the following theorem is folklore in the mathematics and learning community. How-
ever, to the best of our knowledge, the relation to the entropic MD has not been systemat-
ically studied before, as we now do.
Theorem 4.	For a probability measure dμ = ρdz, let Φ(μ) = J P log ρdz be the negative
Shannon entropy, and let Φ*(h) = log JZ ehdz. Then
1.	Φ? is the Fenchel conjugate of Φ:
Φ*(h) = sup hμ, h) — Φ(μ);
μ∈M(Z)
Φ(μ) = sup hμ, h) — Φ*(h).
h∈F(Z)
(10)
(11)
13
Under review as a conference paper at ICLR 2019
2.	The derivatives admit the expression
dΦ(μ) = 1+ log P = arg max (μ, h)一 Φ? (h);
h∈F(Z)
dΦ*(h)
ehdz
Rz ehdz
arg max hμ, h) 一 Φ(μ).
μ∈M(Z)
(12)
(13)
3.	The Bregman divergence of Φ is the relative entropy:
DΦ(μ, μ0) = φ(M)- φ(μ0) -
hμ - μ, dΦ(μ0)i = / dμ iog-dμ0.
JZ	dμ
(14)
4.	Φ is 4-strongly convex with respect to the total variation norm: For all λ ∈ (0, 1),
φ(λμ +(I- λ)μ0) ≤ λφ(μ) +(I- λ)φ(μ0) - 2 ∙ 4λ(1 - λ)kμ - μ0llTv∙	(15)
5.	The following duality relation holds: For any constant C, we have
∀μ, μ ∈ M(Z), Dφ(μ, μ0) = Dφ? (dΦ(μ0), dΦ(μ)) = Dφ? (dΦ(μ0) + C, dΦ(μ)) .
(16)
6.	Φ? is 4-smooth with respect to k∙∣∣L∞:
∀h,h0 ∈F (Z),	kdΦ*(h)- dΦ*(h0)kτv ≤ 1 kh - h0kL∞ .	(17)
7.	Alternative to (17), we have the equivalent characterization of Φ?:
∀h, h0 ∈ F(Z),	Φ*(h) ≤ Φ?(h0) + hdΦ*(h0), h — h0) + 2 ∙ ɪ kh — h0kL∞ .	(18)
8.	Similar to (16), we have
∀h,h0,	Dφ*(h,h0) = Dφ(dΦ*(h0), dΦ*(h)).	(19)
9.	The following three-point identity holds for all μ, μ0, μ00 ∈ M(Z):
hμ0 — μ, dΦ(μ0) — dΦ(μ)) = Dφ(μ, μ0) + Dφ(μ00, μ) — Dφ(μ00, μ0).	(20)
10.	Let the Mirror Descent iterate be defined as in (7). Then the following statements
are equivalent:
(a)	μ+ = MDn (μ, h).
(b)	There exists a constant C such that dΦ(μ+) = dΦ(μ) — ηh + C.
In particular, for any μ0, μ00 ∈ M(Z) we have
Let hμ0 — μ0, ηh) = hμ0 — μ00, dΦ(μ) — dΦ(μ+)) .	(21)
Proof.
1.	Equation (10) is simply the Gibbs variational principle (9) with dμ — dz.
By (10), we know that
∀h ∈ F(Z),
Φ(μ) ≥ hμ,h) — log / ehdz.
Z
(22)
But for dμ = ρdz, the function h :=1 + logP saturates the equality in (22).
14
Under review as a conference paper at ICLR 2019
2.	We prove a more general result on the Bregman divergence DΦ in (23) below.
Let dμ = ρdz,dμ0 = ρ0dz, and dμ00 = ρ00dz ∈ M(Z). Let e > 0 be small enough
such that (P + eρ00)dz is absolutely continuous with respect to dμ0; note that this is
possible because μ,μ0, and μ00 ∈ M(Z). We compute
DΦ(ρ +	ρ00, ρ0)
ZZ (P + "og ;
/ P log(1 + eρp) + e/ P00 log P + e/ P00 log(1 + ep
=P P log -ρ0 +
Z	P0
=Z P log B +
Z	P0
= DΦ (P, P0) +
ejzPπ + Jj0 log P + e2
1 + log ~ρo) + o(e),
P0
+ o()
where (i) uses log(1 +1) = t + o(t) as t → 0. In short, for all μ0, μ00 ∈ M(Z),
dμDΦ(μ, μ0 )(μ00) = Q0,1 + log 力	(23)
which means dμDφ(μ, μ0) = 1 + log P. The formula (12) is the special case when
dμ0 — dz.
We now turn to (13). For every h ∈ F(Z), we need to show that the following holds
for every h0 ∈ F(Z):
Φ*(h + eh0) — Φ?(h) = log Z eh+elh'dz — log Z ehdz = e Z h0 e dz + o(e). (24)
Z	Z	Z Z eh
Define an auxiliary function
eh	0
T(e) := log	h~~he h dz.
Z Ze
Notice that T(0) = 0 and T is smooth as a function of E. Thus, by the Intermediate
Value Theorem,
Φ*(h + eh0) — Φ*(h) = T (E) — T (0)
= (e- 0) ∙ WT(∙)
0
for some 0 ∈ [0, ]. A direct computation shows
drτ (∙
dE
Hence it suffices to prove
sup |h0 | < ∞. Then
eh
广
eh+0 h0
RZ eh + e0h0
eh+0 h0
h0-7-1小0 dz.
JZ eih
Reeh + o(1) in e.
To this end, let C
h+0 h0	h
e-2e0C ≤ e ≤ e p2e'C
R Rz eh+e0h0 - Rz eh
It remains to use et = 1 + t + o(t) and E0 ≤ E.
3.	Let dμ = Pdz and dμ0 = P0dz. We compute
DΦ(μ,μ0) = φM) - φM') - hμ - μ,d©M0)i
=/ P log Pdz — / P0 log P0dz — hμ — μ0,1 + log p'〉	by (12)
=/ P log -ρ0 dz
z	P0
=Z dμ log-dμ0.
JZ	dμ0
15
Under review as a conference paper at ICLR 2019
4.	Define μλ = λμ +(1 — λ)μ0. By (14) and the classical Pinsker's inequality (Gray,
2011), we have
Φ(μ) ≥ Φ(μλ) + h(1 — λ)(μ — μ0), dΦ(μλ)i + 2∣∣(1 — λ)(μ — μ0)kTv,	(25)
φgO) ≥ ^μλ) + hλ(μ0 — μ),d©^》+ 2kλ(μ — μO)IITv∙	(26)
Equation (15) follows by multiplying with λ and 1 — λ respectively and summing
the two inequalities up.
5.	Let μ = Pdz and μ0 = ρ0dz. Then, by the definition of Bregman divergence and
(12), (13),
e1+log ρdz
D®*(d©〃0), d©〃))=F①以〃0))一①?^©〃)) — (R e1+log P, 1 + log P- 1 — log P
= log [ e1+log ρ0 — log [ e1+log P + P ρ log 与
Z	Z	Z	PO
=LP log P = DΦ(μ, μO)
since Z Pdz = Z POdz = 1. This proves the first equality.
For the second equality, we write
e1+log Pdz
dφ* (dφ(μ0) + C, dφ(μ))=①?①①仪0) + C) —①?①①仪))—(R ei+iog P , 1+log P0 + C — 1 — log P
= log / e1+log ρ0+C — log / e1+log P + P ρ log ρ~ — C
Z	Z	Z	PO
=ZZP log P0
=Dφ (μ,μ0) = Dφ*(dΦ(μ0), dΦ(μ))
where we have used the first equality in the last step.
6.	Let μh = dΦ?(h), μ%o = dΦ*(h0), and μλ = λμ% + (1 — λ)μ%o for some λ ∈ (0, 1).
By Pinsker’s inequality and (14), we have
①仪工)≥ φ(m%) + hμλ — μh,d©Mh)i + 2kμλ — μh∣Tv,	(27)
①仪入)≥ ©Mh0) + hμλ — μh0,d^Mh0)〉+ 2kμλ — μh0∣∣Tv∙	(28)
Now, notice that
hμλ — μh, dΦ(μh)i = hμλ — μh, dΦ(dΦ?(h))i
=(μλ Tjh dΦ (需))
=(μλ — μh1 + h — log/ eh)
by (13)
by (12)
hμλ — μh,h
and, similarly, We have hμλ 一 μh, dΦ(μ%o))= hμλ 一 μh,h0). Multiplying (27) by λ
and (28) by 1 — λ, summing the two up, and using the above equalities, we get
①标入)—。①仪九) + (1 —1)©Mho)) + λ(1 — λ) hμh — μho, h — h0i ≥ 2λ(1 — λ) kμh — μ%oIITV.
By (15), We knoW that
①仪工)一(1©Mh) + (1 — λ)F(μh0)) ≤ —2λ(1 — λ) kμh — μh0 IITV.
Moreover, by definition of the total variation norm, it is clear that
hμh — μho, h - h0i ≤ kμh — μ%o kτv IIh — h0kL∞ .	(29)
Combing the last three inequalities gives (17).
16
Under review as a conference paper at ICLR 2019
7.	Let K be a positive integer and k ∈ {0,1, 2,..., K}. Set λk = K and h00 = h — h0.
Then
Φ*(h) — Φ*(h0) = Φ*(h0 + λκ h00') — Φ*(h0 + λoh00)
K -1
=X (Φ*(h0 + λk+ιh00) — Φ*(h0 + λkh")
k=0
By convexity of Φ? , we have
φ*(A0 + λk+ιh0O)- φ*(A0 + λkh0O) ≤ 旧①？^0 + λk+ιh0O), (λk+ι — λk)h00i
=⅛ hdΦ*(h0 + λk+ιh00),h00i.
K
(30)
(31)
By (29) and (17), we may further upper bound (31) as
Φ?(h0 + λk+ιh'r) — Φ*(h0 + λkh00) ≤ K (hdΦ?(h0), h00i + hdΦ*(h0 + λk+ιh'r) — dΦ*(h0), h00i )
≤ K ( hdΦ?(h0), h00i + kdΦ*(h0 + λk+ιh00) - dΦ*(h0)kτv kh00kL∞ )
≤ ɪ ( hdΦ? (h0),h00i + λk+ kh00kL∞ ).	(32)
K4
Summing up (32) over k, we get, in view of (30),
1	K -1
Φ?(h) — Φ*(h0) ≤ hdΦ?(h0), h00i + - kh00kL∞ E λk+ι
k=0
=hdΦ*(h0),h00i + -∙ K+- kh00kL∞ .	(33)
4	2K
Since K is arbitrary, we may take K → ∞ in (33), which is (18).
8.
Straightforward calculation shows
Dφ*(h,h0) = log Z eh — log Z eh — Z R-^7 (h — h0).
Z	Z	Ze
On the other hand, by definition of the Bregman divergence and (12), (13), we have
Dφ(dΦ*(h0), dΦ*(h))= Z TehVh0 — log Z eh - Z 'h + log Z eh
Z Zeh	Z	Z Zeh	Z
—
1 + h - logLeh)(占
—
eh
Z L (h0-h)-log Z eh+log Z eh
Z eh	Z	Z
Φ*(h) — Φ*(h0) — hdΦ*(h0), h — h0i
DΦ? (h, hO).
9.	By definition of the Bregman divergence, we have
Dφ(μ, μ0) = φ(μ) - φ(μ0) -hμ - μ, dφ(μ0)i,
Dφ(μ00,μ) = φ(μ00) - φ(μ) - hμ0 — μ,dφ(μ)i,
DΦ(μ00, μ0) = φ(μ00) - φ(μ0) - hμ00 - μ, dφ(μ0)i.
Equation (20) then follows by straightforward calculations.
10.	First, let μ+ = MDn (μ, h). Then if μ+ = ρ+dz and μ = ρdz, then (7) implies
ρ+
ρe-nh
RZ Pe-nh
17
Under review as a conference paper at ICLR 2019
By (12), we therefore have
dΦ(μ+) = 1 + log ρ+
= 1 + logρ - ηh - log	ρe-ηh
Z
whence (21) holds with C = - log Z ρe-ηh.
Conversely, assume that dΦ(μ+) = dΦ(μ) — ηh + C for some constant C, and apply
dΦ? to both sides. The left-hand side becomes
dΦ*(dΦ(μ+)) =dΦ*(1 + log ρ+)
ρ+dz
=R7+dZ = ρ+dz = dμ+,
where as the formula (13) implies that
e1+log ρ-ηh+C
dΦ? (dΦ(μ) - ηh + C) = -7——巾------年TFdz
e1+log ρ-ηh+C
ρe-ηhdz
=RZ Peih
__ e-nhdμ
一Rz e-nhdμ.
Combining the two equalities gives dμ+
MDn (μ, h).
e-ηhdμ
RZ e-ηhdμ
which exactly means μ+
□
B Proof of Convergence Rates for Infinite-Dimensional
Mirror Descent
B.1 Mirror Descent, Deterministic Derivatives
By the definition of the algorithm, (21), and the three-point identity (20), we have, for any
μ ∈M(W),
hμt - μ, -g + GVti = η hμt - μ, d©M» - d以μt+1)i
=—(Dφ(μ, μt) - DΦ(μ, μt+1) + DΦ(μt, μt+ι
η
By item 10 of Theorem 4, there exists a constant Ct such that
dΦ(μt+i) = dΦ(μt) - η (-g + GVt) + C
(34)
(35)
Using (16), we see that
Dφ(μt,μt+ι) = Dφ? (dΦ(μt+ι), dΦ(μt))
=dφ? (d©Mt+1)- Ct, d^M/)
≤ C kd©Mt+1)- Ct- dΦ(Mt)kL∞
8
2
="8 k-g + GVtkL∞
η2M2
≤	8	.
by (18)
by (35)
18
Under review as a conference paper at ICLR 2019
Consequently, we have
T	T1
^X hμt - μ, -g + GVti = ^X η (DΦ(μ, μt) - DΦ(μ, μt+1) + DΦ(μt, μt+ι
≤ Dφ(μ,μι) + ηM2T
≤ η 十 8	.
Exactly the same argument applied to Vt's yields, for any V ∈ M(Θ),
(36)
T
X Wt- ν, -G*μt> ≤
t=1
Dφ(v, νι) + ηM2T
η 十 8
(37)
Summing up (36) and (37), substituting μ 一 μNE, V — VNE and dividing by T, We get
1 L ( I	l 厂 ∖ 、 /	E ʌ Do l ηM2
T ʌ, (	hμt	- μNE,	-g	+ GVti	+ (Vt	-	νNE,	-G μt)) ≤ ≤ 彳 +-4—
t=1	η
(38)
The left-hand side of (38) can be simplified to
1T	1T
T ^X (hμt - μNE, -g + GVti +〈Vt - vne, -G μt)) = T ^X (hμNE - μt, gi - hμNE, GVti + hμt, GVNEi
=hμNE, g - GVTi — hμτ, g - GVNEi .
(39)
By definition of the Nash Equilibrium, We have
hμT, g - GVNEi ≤ hμNE, g - GVNEi ≤ hμNE, g - GDTi ,	(40)
hμT, g - GVNEi ≤ hμT, g - GVTi ≤ hμNE, g - GVTi ,
Which implies
K“T,g - GVti - hμNE,g - GVNEiI ≤ hμNE,g - GVTi - h“T,g - GVNEi .	(41)
Combining (51)-(54), We conclude that
2 [D
MV亍
⇒
η
Ihμτ, g - GVti - hμNE,g - GVNEiI ≤ M
.
B.2 Mirror Descent, Stochastic Derivatives
We first Write
(μt - μ, η(-g + GUt)I = hμt - μ, η(-g + GVt)i + μt- - μ, η- - g + GVt + g - GVJ).
Taking conditional expectation and using the bias estimate of stochastic derivatives, We
conclude that
E (μt - μ, η(-g + GVt)〉≤ hμt - μ, η(-g + GVt)) + kμt - μkτv ∙ ητ
≤ hμt - μ, η(-g + GUt) + 2ητ.
Therefore, using exactly the same argument leading to (36), We may obtain
E XX Dμt - μ, -g + GVtE ≤ E⅛^ + T + 2ηTτ.
t=1	η
The rest is the same as With deterministic derivatives.
19
Under review as a conference paper at ICLR 2019
C Proof of Convergence Rates for Infinite-Dimensional
Mirror-Prox
We first need a technical lemma, which is Lemma 6.2 of (Juditsky & Nemirovski, 2011)
tailored to our infinite-dimensional setting. We give a slightly different proof.
Lemma 5. Given any μ ∈ M(Z) and h,h0 ∈ F(Z), let μ = MDn (",h) and μ+ =
MDn (μ, h0). Let Φ be α-strongly convex (recall that α = 4 when Φ is the entropy). Then,
for any μ? ∈ M(Z), We have
2
hμ—μ?,ηh0i ≤ DΦ(μ?,μ) — DΦ(μ?, μ+) + 2α kh - h0kL∞ - 2 ι∣μ - μkTv.	(42)
Proof. Recall from (15) that entropy is ɑ-strongly convex with respect to |卜||丁9 We first
write
hμ 一 μ大,ηh0i = hμ+ 一 μ*,ηh0i + hμ — μ+ ,ηh + hμ 一 μ+,η(h0 一 h)i.	(43)
For the first term, (20) and (21) implies
hμ+ — μ?, ηh0i = hμ+ — μ?, dφ(μ) — dφ(μ+)i
=一Dφ(μ+,μ) - dφ(μ*,μ+) + Dφ(μ*, μ)∙	(44)
Similarly, the second term of the right-hand side of (43) can be written as
hμ - μ+,ηhi = 一Dφ(μ,μ) - Dφ(μ+,μ) + Dφ(μ+,μ).	(45)
Holder's inequality for the third term gives
hμ - μ+, η(h - h)i ≤ kμ - μ+kτv llη(h0 - h)k∞
≤ 2 kμ - μ+kTv + 2α kη(h0 - h)kL∞.	(46)
Finally, recall that Φ is α-strongly convex, and hence we have
- Dφ("+,μ) ≤ -2 llμ - μ+kTv,	-DΦ(μ,μ) ≤ -2 ∣∣μ - μkTv.	(47)
The lemma follows by combining inequalities (44)-(47) in (43).	□
C.1 Mirror-Prox, Deterministic Derivatives
Letα =4, μτ := T PPt=I μt, and VT := T PPt=I νt.
In Lemma 5, substituting μ? 一 μNE, μ — μt, h - -g + GVt (so that μ = μt) and
h0 <---g + GVt (so that μ+ = "t+ι), We get
hμt - μNE, η(-g + GVty) ≤ DΦ(μNE,μt)-dφ(mne,瓦+。+太 IIG(νt - Vt)I∣L∞
Similarly, we have
-2 kμt - μtkTv.
2	(48)
(Vt - νNE, 一ηG μt) ≤ DΦ(νNE, Vt)-Dφ(vNE, Vt+1) +太 IIG, (μt - μt)∣∣L∞ - 2 k Vt - Vt l∣TV .
(49)
Since ∣∣G(νt - Vt)k∞ ≤ L ∙ k% - VtkTV and ∣∣Gt(μt - at)∣∣L∞ ≤ L ∙ kμt - μtkτv, summing
up (48) and (49) yields
hμt - μNE, η(-g + GVt)i + Wt - vne, 一ηGtμt)> ≤ DΦ(μNE,μt) - DΦ(μNE,μt+ι) + dφ(vne, Vt) - dφ(vne, vt+ι)
η2L2
+ (丁
—
2) (kμt -μt∣lTv + llVt - VtkTV
≤ Dφ(μNE,μt) - Dφ(μNE,μt+ι) + Dφ(vne, Vt) - Dφ(vne, Vt+ι)
20
Under review as a conference paper at ICLR 2019
if η ≤ L = Ll. Summing UP the last inequality over t and using Dφ(∙, ∙) ≥ 0, We obtain
T
1	( :	/	, c ∖∖、］	<4	/ Dφ(μNE, μι) + dφ(vne, VI) _ DO
T ʌ, ((μt - μNE,η(-g + Gνt)i + ∖ Vt - νNE, -ηG μt)) ≤ ≤	T	=亍.
t=1
(50)
The left-hand side of (50) can be simPlified to
1T	ηT
T ^X (hμt - μNE, η(-g + Gνt)i +(Vt - vne, 一ηG,μt))) = T ^X (hμNE - μt, gi - hμNE, GVti + hμt, GVNEi
=η( hμNE, g - GDT i — hμτ, g — GVNEi).
(51)
By definition of the (μNE, vne), We have
hμT, g - GVNEi ≤	hμNE, g - GVNEi	≤	hμNE, g -	GDTi ,	(52)
hμT, g - GVNEi ≤	hμT, g - GDTi	≤	hμNE, g -	GVTi ,
Which imPlies
| h“T, g — Gvti — hμNE, g — GVNEiI ≤ hμNE, g — GVTi — h“T, g — GVNEi .	(53)
Combining (50)-(53), We conclude
η ≤ L ⇒	I hμτ ,g - GVt i — hμNE,g - GVNEi ∣≤ D.
C.2 Mirror-Prox, Stochastic Derivatives
Let α = 4, μτ := T P Pt—1 μt, and VT := T P Pt—〔 Vt.	Set
the steP-size to η
min
T T	L	1 J ∙ J J ∙	~^~7^	〜 、 人 ~
In Lemma 5, substituting μ? 一 μNE, μ — Vt, h — —^ + GVt
h V----g + GVt (So that μ+ = μt+ι), We get
(So that μ = μt), and
2
η2
—μNE,n(—g + GVt)) ≤ DΦ(μNE,μt)-DΦ(μNE,μt+ι)+丁
2α
—2 kμt — μtkTv .
2	(54)
Note that
—
2
E
—
2
≤
L∞
3 (e∣∣Gvl GVt∣∣2
+ EkGVt- GVtkL∞ + E l∣Gi×t —
2
≤ 6σ2 + 3L2E kVt — VVt kTV .
Therefore, taking exPectation conditioned on the history for both sides of (54) and using
the bias estimates of the stochastic derivatives, We get
3η2 σ2
hμt — μNE, n(—g + GVt)i ≤ EDΦ(μNE, μt) — EDΦ(μNE, μt+ι) +-
,3η2L2 E H - ∣∣2
+	2^E kVt - VtkTV
α
—2E kμt — μtkTv + 2ητ.
Similarly, We have
E 、	Tr, c /	,	3η2σ2
Vt-VNE, —nG μt) ≤ EDφ(VNE, Vt)-EDφ(VNE, νt+1) +
+ ∖α e kμt - μtIITV - 2EIlVt - VtlITV + 2ητ.
21
Under review as a conference paper at ICLR 2019
Summing up the last two inequalities over t with η ≤ √α^ then yields
1 £ 八	厂、j	E 八、Do 6ησ2	,
T ^X (hμt - μNE, -g + GVti + Wt - vne, -G μt) ) ≤ -T +———+ 4τ
t=1	η	α
66σ2Do 2√3LDo]
≤ max 2∖ ----——,----——— + 4τ.
-	NaT ' αT +
by definition of η. The rest is the same as with deterministic derivatives.
Algorithm 4: APPRox INf Mirror Decent
Input: W[1], Θ[1] - n0 samples from random initialization,
{γt}tT=-11,{t}tT=-11,{K}tT=-11, n, n0, standard normal noise ξk , ξk0 .
for t = 1, 2, . . . , T - 1 do
C 一 ∪S=ιW[s], D - ∪S=ιΘ[s];
w(1) ― UNIF(W [t]),	θ(1) — UNIF(Θ[t]);
for k = 1, 2, . . . , Kt, . . . , Kt + n0 do
Generate A = {Xι,..., Xn}〜 P8(k);
θ(k+1) = θ(k) + nYt0Vθ Pχi∈A pW∈c fw(Xi) + √2⅞tξk;
Generate B = {χreal,..., Xnneal}〜Pgi;
B0 T};
for each θ ∈ D do
Generate B = {Xj,..., Xj}〜Pθ;
B0 J B0 ∪ B;
w(k+1) = Wy) + 斗Vw X fw* (Xreal)-得Vw X fw* (χ0) + 产解维;
n	t	nn	t
Xireal ∈B	Xi0∈B0
_ W [t + 1] J {w(K+1),..., w(K+n')} ,	Θ[t + 1] J {θ(K+1),..., θ(K+n0};
idx J UNIF(1, 2,..., T);
return W [idx], Θ[idx].
D Omitted Pseudocodes in the Main Text
We use the following notation for the hyperparameters of our algorithms:
n : number of samples in the data batch.
n0 : number of samples for each probability measure.
γt : SGLD step-size at iteration t.
t : thermal noise of SGLD at iteration t.
Kt : warmup steps for SGLD at iteration t.
β : exponential damping factor in the weighted average.
The approximate infinite-dimensional entropic MD and MP in Section 4.1 are depicted in
Algorithm 4 and 5, respectively. Algorithm 6 gives the heuristic version of the entropic
Mirror-Prox.
E Details and More Results of ExPeriments
This section contains all the details regarding our experiments, as well as more results on
synthetic and real datasets.
22
Under review as a conference paper at ICLR 2019
Algorithm 5: Approx Inf Mirror-Prox
Input: W[1], Θ[1] - n0 samples from random initialization,
{γt}tT=1, {t}tT=1, {Kt}tT=1,n,n0, standard normal noise ξk,ξk0 ,ξk00,ξk000.
for t = 1, 2, . . . , T do
C 一 W[t] ∪ (∪s=1w[s]) , D - Θ[t] ∪ (∪s=1θ[s]);
w(1) ― UNIF(W[t]), θ(1) — UNIF(Θ[t]);
for k = 1, 2, . . . , Kt, . . . , Kt + n0 do
Generate A = {Xι,..., Xn}〜P8*;
θ(k+1) = θ(k) + nYto Vθ Pχi∈A Pw∈C fw (Xi) + √27tetξk;
Generate B = {χreal,..., Xnneal}〜Pgi;
B0 一 {};
for each θ ∈ D do
Generate B = {Xj,..., X!n}〜Pθ;
B0 J B0 ∪ B;
w(k+1) = Wy) + 斗Vw χ fw* (Xreal)-得Vw χ fw* (χo) + 石居；
n	t	nn	t
Xireal ∈B	Xi0∈B0
W[t] J {w(K+1),..., w(K")} ,	θ[t] J {θ(K+1),..., θ(K")};
C0J∪ts=1W[s], D0 J ∪ts=1Θ[s] ;
w(+)1 J UNIF(W[t]), θ(+)1 J UNIF(Θ[t]);
for k = 1, 2, . . . , Kt, . . . , Kt + n0 do
Generate A = {Xι,..., Xn}〜Pj(k);
θ(++1) = θ(+)ι + nn0 Vθ Pχi∈A Pw∈c, fw (Xi) + EetWk
Generate B = {X『al,..., Xnneal}〜Preal;
B0 J {};
for each θ ∈ D0 do
Generate B = {Xj,..., Xj}〜Pθ;
B0 J B0 ∪ B;
w(++1) = w(+ι + 当vw X fw(2 (Xreal)-焉Vw X fw(N(X0) + H忒X
n	t+1	nn	t+1
Xireal ∈B	Xi0∈B0
W[t + 1] J {w(K+1),..., w(K+n')},	θ[t + i] J {θ(K1+1),..., θ(K+n')};
idx J UNIF(1, 2, . . . , T);
return W [idx], Θ[idx].
Network Architectures: For all experiments, we consider the gradient-penalized discrim-
inator (Gulrajani et al., 2017) as a soft constraint alternative to the original Wasserstein
GANs, as it is known to achieve much better performance. The gradient penalty parameter
is denoted by λ below.
For synthetic data, we use fully connected networks for both the generator and discriminator.
They consist of three layers, each of them containing 512 neurons, with ReLU as nonlinearity.
For MNIST, we use convolutional neural networks identical to (Gulrajani et al., 2017) as the
generator and discriminator.5 The generator uses a sigmoid function to map the output to
range [0, 1].
5Their code is available on https://github.com/igul222/improved_wgan_training.
23
Under review as a conference paper at ICLR 2019
Algorithm 6: Mirror-Prox-GAN: Approximate Mirror-Prox for GANs
Input: wι, θι - random initialization,
wo — wι, θo - θι, {Yt}T=INet}T=INKt}T=1, β, standard normal noise
ξ ξ0 ξ00 ξ000
ξk , ξk , ξk , ξk .
for t = 1, 2, . . . , T do
Wt, Wt+1, W(1), W(+)ι — Wt, θt,θt+ι, θ(1), θ(+)ι - θt；
for k = 1, 2, . . . , Kt do
Generate A = {X1,..., Xn}〜 P®(k)；
θ(k+1) = θ(k) + Yt Vθ Pχi∈A fwt (Xi) + √2γtetξk;
Generate B = {X；eal,..., Xnneal}〜Preai；
Generate B0 = {X1,..., Xnl}〜Pgj
w(k+1) = w(k) + γtVw X fw(k)(Xreal)- γtVw X fw(k)X) + Reξ；
n	tn	t
Xireal ∈B	Xi0 ∈B0
Wt — (1 - β)wt + βw(k+1);
L θt J (1 — β)θt + βθ(k+1) ；
Wt — (1 — β)wt-1 + gw t；
θt — (I — β)θt-1 + e伍；
for k = 1, 2, . . . , Kt do
Generate A = {X1,..., Xn}〜Pg(k)；
θ(++1) = θ(+1 + γt vθ Pχi∈A fwt (Xi) + √2γtetξk,
Generate B = {X『al,..., Xnreal}〜Preal；
Generate B0 = {X1,..., Xn}〜Pgt ；
W(++1) =	W(+)1 + YtVw	X fw(k)	(Xreal)- YtVw	X fw(k) (X0) +	E"
n	t+1	n	t+1
χireal ∈B	χi0 ∈B0
Wt+1 - (1 — β)W t+1 + β W(++1)；
_ θt+1 J (I- 0θt+1 + βθ(++1)；
Wt+1 J (1 - β)Wt + βWt+1；
_ θt+1 J (1 - e)0t + βθt+1:
return WT , θT .
For LSUN bedroom, we use DCGAN (Radford et al., 2015), except that the number of the
channels in each layer is half of the original model, and the last sigmoid function of the
discriminator is removed. The output of the generator is mapped to [0, 1] by hyperbolic
tangent and a linear transformation. The architecture contains batch normalization layer
to ensure the stability of the training. For our Mirror- and Mirror-Prox-GAN, the Gaussian
noise from SGLD is not added to parameters in batch normalization layers, as the batch
normalization creates strong dependence among entries of the weight matrix and was not
covered by our theory.
Hyperparameter setting: The hyperparameter setting is summarized in Table 1. For
baselines (SGD, RMSProp, Adam), we use the settings identical to (Gulra jani et al., 2017).
For our proposed Mirror- and Mirror-Prox-GAN, we set the damping factor β to be 0.9. For
24
Under review as a conference paper at ICLR 2019
Algorithm	SGD		RMSProp	Adam			Entropic MD/MP		
Dataset	S	M	L	S	M	L	S	M	L
Step-size Y	10-2		10-4	10-4			10-2		10-4
Gradient penalty λ	0.1	10		0.1	10		0.1	10	
Noise e							10-2	10-3	10-6
Batch Size n	1024	50	64	1024	50	64	1024	50	64
Table 1: Hyperparameter setting. “S”, “M”, “L” stands for synthetic data, MNIST and LSUN
bedroom, respectively. MD for LSUN bedroom uses a RMSProp preconditioner, so the step-
size is the same as one in RMSProp.
(a) SGD
(b) Adam
1.5
1.0
0.5
0.0
-0.5
-1.0
-1.5
-1.5	-1.0	-0.5	0.0	0.5	1.0	1.5
(C) Mirror-GAN
Figure 3: Fitting 8 Gaussian mixtures up to 105 iterations.
1.5
1.0
0.5
0.0
-0.5
-1.0
-1.5
-1.5	-1.0	-0.5	0.0	0.5	1.0	1.5
(d) Mirror-Prox-GAN
Kt , γt and t , we use the simple exponential scheduling:
Kt = b(1 + 10-5)tc.
γt = γ × (1 - 10-5)t,	γ in Table 1.
t = × (1 - 5 × 10-5)t,	in Table 1.
The idea is that the initial iterations are very noisy, and hence it makes sense to take less
SGLD steps. As the iteration counts grow, the algorithms learn more meaningful parameters,
and we should increase the number of SGLD steps as well as decreasing the step-size γt and
thermal noise t to make the sampling more accurate. This is akin to the warmup steps in
the sampling literature.
E.1 Synthetic Data
Figure 3, 4, and 5 show results on learning 8 Gaussian mixtures, 25 Gaussian mixtures, and
the Swiss Roll. As in the case for 25 Gaussian mixtures, we find that Mirror- and Mirror-
25
Under review as a conference paper at ICLR 2019
(a) SGD
(b) Adam
(c) Mirror-GAN
Figure 4: Fitting the ‘Swiss Roll’ up to 105 iterations.
(d) Mirror-Prox-GAN
Prox-GAN can better capture the variance of the true distribution, as well as finding the
unbiased modes.
In Figure 6, we plot the data generated after 104, 2 × 104, 5 × 104, 8 × 104, and 105 iterations
by different algorithms fro 25 Gaussian mixtures. It is clear that Mirror- and Mirror-Prox-
GAN find the modes of the distribution faster. In practice, it was observed that the noise
introduced by SGLD quickly drives the iterates to non-trivial parameter regions, whereas
SGD tends to get stuck at very bad local minima. Adam, as an adaptive algorithm, is capable
of escaping bad local minima, however at a rate slower than Mirror- and Mirror-Prox-GAN.
The quality of Adam’s final solution is also not as good as Mirror- and Mirror-Prox-GAN;
see the discussions in Section 5.1.
E.2 Real Data
E.2.1 MNSIT
Results on MNIST dataset are shown in Figure 7. The models are trained by each algo-
rithm for 105 iterations. We can see that all algorithms achieve comparable performance.
Therefore, the dataset seems too weak to be a discriminator for different algorithms.
E.2.2 LSUN Bedroom
Algorithm	RMSProp	Adam	Entropic MD	EXtra-Adam
Simultaneous	-	-	3.0955	2.0015
Alternated	3.0555	1.3730	-	3.1620
Table 2: Inception Score of generator trained on LSUN dataset. The reported scores are
based on the average of 6400 images from each generator.
26
Under review as a conference paper at ICLR 2019
(a) SGD
-1.□
-1.0
-1.0
-1.0
-1.5
Figure 5: Fitting 25 Gaussian mixtures up to 105 iterations.
(c) Mirror-GAN
-1.5
■	♦ * I	*	就 *
Φ*	**■ ♦ +	4⅛++	弁	.
			*
*	+ + d	费 +#	÷*>	+ ♦，♦
	*	t		
.	*… 1	*■	*
*	Me+	* i+		
-1.5	―æo		1.0	1.5
	(b) Adam		
♦ *	* ++ +**t∙ ⅛∙÷f	+	* * *	⅛ .T
		+ ⅛	+ »
	Z	+	*
I			
.	$ + £+ "	■	+期
*■	成	/+	♦	. W
*	“+ +⅛+ ≠	.	+" + <
-1.5	-ɪɑi æo		1.0	1.5
(d) Mirror-Prox-GAN
More results on the LSUN bedroom dataset are shown in Figure 8. We show images generated
after 4 × 104, 8 × 104, and 105 iterations by each algorithm. We can see that the Mirror-
GAN and Alternated Extra-Adam outperform vanilla RMSProp. Adam was able to obtain
meaningful images in early stages of training. However, further iterations do not improve
the image quality of Adam. In contrast, they lead to severe mode collapse at the 8 × 104th
iteration, and converge to noise later on. Simultaneous Extra-Adam completely fails in this
task.
Finally, for reference, we report the Inception Score in Table 2.
27
Under review as a conference paper at ICLR 2019
104 iterations
2 × 104 iterations
5 × 104 iterations
8 × 104 iterations
105 iterations
1.5-
1.0-
∙√∙
-is -ιt> -os o∙o as i∙α 1.5
-14	-1.0	-0.5	04	04	14	1.5
14-
1Λ∙
04-
。耻
14-
3
04-
。耻
(b) Adam
14-
1Λ∙
04-
。耻
-13 .
-14	-1.0	-0.5	(M)	0.5	14 IJ
-1.5	-14	-04	(M)	0.5	1.0	1.5
(a) SGD
*⅜3im学**∙4: #:e*
-1.5	-14	-04	0∙0	04	14	14
14
IA
04
。4
-14
-IJO
-IS
-10	12
-14	-14	-04	(M)	0.5	1.0	1.5
-14-
-14	-i∙0	-04 oh 04 Ih IJ
3
3
03 ∙
。耻
-03 ∙
T∙0∙
-13 ∙
-14	-i∙0	-04	0：。	& M i.5
13 ∙
3
03 ∙
。耻
-03 •
T∙0∙
-13 •
-14	-1.0	-04 oh 04 Ih LS
Figure 6: Learning 25
(c) Mirror-GAN
(d) Mirror-Prox-GAN
Gaussian mixtures accross different iterations.
28
Under review as a conference paper at ICLR 2019
■7弓// 7√z /夕3
/、G S⅛o∕o∕q3
56∕% / a 今。S 夕
6。&。〃干60(9/
G3k∩s3∕3j0 +
ΓG¾SW — to∕y35
0ʃ了(2，。7 夕rl
W。/33J30，。
3/。⅛7 & 夕QlJ
6 /,U/OJG ,IJ，夕 S
(a) True Data
9 ▲。7 q 2 7G 3 #
，poq 夕/ɪfro c-93
3rl∕g∕39673
S∕3%□-√56 33
Qz8⅛/ W √G 5
2∕3¾F5W Ui 73
∙7。& 655CrG 54
086053 1ΓSS6%
∕l47x-76 g75
r+7736gos —3
∕3λ7795^clQc-
≤Γα4 0 2l55l7
二,yo,NgeagF
长/4，¥21，0 O
OZr 〃夕 4C1 7。
57-。5 / 8 431/
1o∕,ZH0∕l/
7H97∙8∕3Jli≤?
N3tPH23∕6∕⅜
∖Aog∕69入夕W小
(c) Adam
(b) SGD
r∙6∕28;.ΓG / I
$5 +J6 3dV 7/P
39∕a⅛□Q473
ʃαl3∕∕Γ<夕3S
loqo2e£，7x
⅛F√¾qss 1 73 7
7，56& 7CO 夕47
¢0 / Λc S «b fy—。3
I 7045Qo7。y
fg∖sbJ√5NI
qf「*7QGQSO
Oa4∕93 9 m<3G
¥140£, 263S I
7SITQ∕Λ-H7OΠ
夕 0>o6Gα97uq
<ΓZ F3d22 夕Z3，
73a440七 53，
4 5 q q S。夕7J3
J qc⅛7r∩3773分
修 Qa4272 270
(e) Mirror-Prox-GAN
(d) Mirror-GAN
Figure 7: True MNIST images and samples generated by different algorithms.
29
Under review as a conference paper at ICLR 2019
4 × 104 iterations
8 × 104 iterations
105 iterations
(a) RMSProp
30
Under review as a conference paper at ICLR 2019
(d) Simultaneous Extra-Adam
(e) Alternated Extra-Adam
Figure 8: Image generated by RMSProp, Simultaneous and Alternated Extra-Adam, Adam,
and Mirror-GAN on the LSUN bedroom dataset.
31