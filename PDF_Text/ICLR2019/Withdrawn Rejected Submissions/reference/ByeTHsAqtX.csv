title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Optimization methods for large-scale machinelearning,2016, arXiv preprint arXiv:1606
 Entropy-sgd: Biasing gradientdescent into wide valleys,2016, arXiv preprint arXiv:1611
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in Neural Information Processing Systems 27
 Flat minima,1997, Neural Computation
 ARPACK Usersâ€™ Guide: Solution of Large-scale Eigen-value Problems with Implicitly Restarted Arnoldi Methods,1998, Society for Industrial and AppliedMathematics
 On the saddle point problemfor non-convex optimization,2014, arXiv preprint arXiv:1405
 Learning internal representationsby error propagation,1985, Technical report
 Learning representations by back-propagating errors,1986, nature
 Eigenvalues of the hessian in deep learning: Singu-larity and beyond,2016, arXiv preprint arXiv:1611
 Empirical analysis ofthe hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
