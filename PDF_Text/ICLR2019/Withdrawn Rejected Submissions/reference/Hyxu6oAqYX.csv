title,year,conference
 Learning from noisy examples,1988, Machine Learning
 Began: boundary equilibrium generative adver-sarial networks,2017, arXiv preprint arXiv:1703
 A polynomial-time algorithm for learning noisylinear threshold functions,1432, Algorithmica
 Identifying mislabeled training data,1076, J
 Learning probabilistically consistent linear threshold functions,1997, In Proceedings ofthe Tenth Annual Conference on Computational Learning Theory
 On contrastive divergence learning,2005, In Aistats
 Learning noisy perceptrons by a perceptron in polynomial time,1997, In Proceedings 38thAnnual Symposium on Foundations of Computer Science
 Learning via gaussian herding,2010, In Advances in neural informationprocessing systems
 A k-nearest neighbor classification rule based on dempster-shafer theory,0018, IEEETransactions on Systems
 Analysis of evidence-theoretic decision rules for pattern classification,1997, PatternRecogn
 A neural network classifier based on dempster-shafer theory,2000, IEEE Transactionson Systems
 A semi-supervised two-stage approachto learning from noisy labels,2018, arXiv preprint arXiv:1802
 Classification in the presence of label noise: a survey,2014, IEEEtransactions on neural networks and learning systems
 Making risk minimization tolerant to labelnoise,0925, Neurocomputing
 Diagnosis of alzheimerâ€™s disease,1985, Archives of Neurology
 Genre-based decomposition of email class noise,2009, InProceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery andData Mining
 Discriminant analysis when the initial samples are misclassified ii: Non-random misclassification models,0040, Technometrics
 A tutorial on energy-basedlearning,2006, Predicting structured data
 Heartbeat classification fusingtemporal and morphological information of ecgs via ensemble of classifiers,2019, Biomedical SignalProcessing and Control
 Machine learning: a probabilistic perspective,2012, The MIT Press
 Learning withnoisy labels,2013, In Advances in neural information processing systems
 Learning with Confident Examples: Rank Pruning forRobust Classification with Noisy Labels,2017, In Uncertainty in Artificial Intelligence
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Bounds on the mean classification error rate of multiple experts,0167, PatternRecognition Letters
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 Identifying and correcting mis-labeled training instances,2007, In Future generation communication and networking (FGCN 2007)
 Class noise vs,1573, attribute noise: A quantitative study
