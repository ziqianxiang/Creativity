title,year,conference
 Emergence of invariance and disentangling in deep represen-tations,2017, arXiv preprint arXiv:1706
 Network dissection:Quantifying interpretability of deep visual representations,2017, CVPR
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, ICML
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Open problem: The landscape of the losssurfaces of multilayer networks,2015, In Conference on Learning Theory
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing systems
 Gradientdescent can take exponential time to escape saddle points,2017, In Advances in Neural InformationProcessing Systems
 Gradient descentlearns one-hidden-layer cnn: Don’t be afraid of spurious local minima,2018, ICML
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Identity matters in deep learning,2017, ICLR
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, ICML
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, ICLR
 Towards a theoretical understanding of batch normalization,2018, arXiv preprintarXiv:1805
 Imagenet classification with deep convolu-Honal neural networks,2012, In Advances in neural information processing Systems
 ExPonentialexPressivity in deeP neural networks through transient chaos,2016, In Advances in neural informationprocessing systems
 Exact solutions to the nonlinear dynamicsof learning in deeP linear neural networks,2013, arXiv preprint arXiv:1312
 On the information bottleneck theory of deePlearning,2018, In International Conference on Learning Representations
 Masteringthe game of go with deeP neural networks and tree search,2016, nature
 Mastering the game of go withouthuman knowledge,2017, Nature
 Very deeP convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 A differential equation for modeling nesterov’saccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
 An analytical formula of PoPulation gradient for two-layered relu network and itsaPPlications in convergence and critical Point analysis,2017, ICML
 DeeP learning and the information bottleneck PrinciPle,2015, InInformation Theory Workshop (ITW)
 Neural networks with finite intrinsic dimensionhave no sPurious valleys,2018, arXiv preprint arXiv:1802
 UnderstandingdeeP learning requires rethinking generalization,2016, ICLR
