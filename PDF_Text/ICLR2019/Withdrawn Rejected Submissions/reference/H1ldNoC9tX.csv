title,year,conference
 Semi-Supervised Learning,2010, The MIT Press
 Convex formulation for learning from positive and unla-beled data,2015, In International Conference on Machine Learning (ICML)
 Class-prior estimation for learning from positive andunlabeled data,0885, Maching Learning
 Learning classifiers from only positive and unlabeled data,2008, In KDD
 Social media text classification under negative covariate shift,2015, In Proceedingsof the 2015 Conference on Empirical Methods in Natural Language Processing
 Semi-supervised learning by entropy minimization,2005, In Advances inNeural Information Processing Systems (NIPS)
 Identity mappings in deep residual networks,2016, In EuropeanConference on Computer Vision (ECCV)
 Sample selection bias as a specification error,1979, Econometrica
 Inlier-based outlier detection viadirect density ratio estimation,2008, In Proceedings of IEEE International Conference on Data Mining(ICDM)
 Correcting sample selectionbias by unlabeled data,2007, In Advances in Neural Information Processing Systems (NIPS)
 Binary classification from positive-confidence data,2018, arXivpreprint arXiv:1710
 Estimating the class prior and posterior from noisy positives andunlabeled data,2016, In Advances in Neural Information Processing Systems (NIPS)
 Positive-unlabeled learning with non-negativerisk estimator,2017, In Advances in Neural Information Processing Systems (NIPS)
 A positive and unlabeled learning algorithm for one-class classificationof remote-sensing data,2011, IEEE Transactions on Geoscience and Remote Sensing
 Negative training data can be harmful to text classification,2010, InProceedings of the 2010 Conference on Empirical Methods in Natural Language Processing
 Partially supervised classification of text documents,2002, InInternational Conference on Machine Learning (ICML)
 Building text classifiers using positive and unlabeledexamples,2003, In Proceedings of IEEE International Conference on Data Mining (ICDM)
 Foundations of machine learning,2012, MIT press
 A bagging svm to learn from positive and unlabeled examples,2014, PatternRecognition Letters
 Positive unlabeled leaning for time series classification,2011, InIJCAI
 Realistic evaluation of deepsemi-supervised learning algorithms,2018, arXiv preprint arXiv:1804
 Deepcontextualized word representations,2018, In Proc
 Dataset shift in machinelearning,2009, 2009
 Mixture proportion estimation via kernel embeddings ofdistributions,2016, In International Conference on Machine Learning (ICML)
 On the convergence of adam and beyond,2018, In InternationalConference on Learning Representations (ICLR)
 Concatenated -mean word embeddings as universalcross-lingual sentence representations,2018, arXiv preprint arXiv:1803
 Understanding Machine Learning: From Theory to Algo-rithms,2014, Cambridge University Press
 Machine Learning in Non-Stationary Environments: Introductionto Covariate Shift Adaptation,2012, MIT Press
 Mixture regression for covariate shift,2007, In Advances in NeuralInformation Processing Systems (NIPS)
 Direct importanceestimation with model selection and its application to covariate shift adaptation,2008, In Advances inNeural Information Processing Systems (NIPS)
 Presence-only data and the emalgorithm,2009, Biometrics
 Learning and evaluating classifiers under sample selection bias,2004, In InternationalConference on Machine learning (ICML)
 Statistical behavior and consistency of classification methods based on convex risk mini-mization,2004, Annals of Statistics
