title,year,conference
 Probabilistic synapses,2014, arXiv preprintarXiv:1410
 Natural gradient works efficiently in learning,1998, Neural computation
 Advances in optimizingrecurrent networks,2013, In Acoustics
 On acceleration with noise-corrupted gradients,2018, arXiv preprint arXiv:1805
 Incorporating nesterov momentum into ADAM,2016, In International Conference onLearning Representations Workshop
 Simple and conditioned adaptivebehavior from kalman filter trained recurrent networks,2003, Neural Networks
 Assumed density filteringmethods for learning bayesian neural networks,2016, In AAAI
 Generating sequences with recurrent neural networks,2013, arXiv preprintarXiv:1308
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, In International Conference on Machine Learning
 Updating the inverse ofa matrix,1989, SIAM review
 Probabilistic backpropagation for scalable learn-ing of bayesian neural networks,2015, In International Conference on Machine Learning
 Vprop: Variational infer-ence using rmsprop,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Big omicron and big omega and big theta,1976, ACM Sigact News
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Online natural gradient as a kalman filter,2017, arXiv preprint arXiv:1703
 Decoupled extended kalman filter training of feedfor-ward layered networks,1991, In Neural Networks
 Neurocontrol of nonlinear dynamical systems withkalman filter trained recurrent networks,1994, IEEE Transactions on neural networks
 Parameter-based kalman filter training: theory andimplementation,2001, In Kalman filtering and neural networks
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, arXiv preprint arXiv:1511
 On the convergence of adam and beyond,2018, ICLR
 Optimal filtering algorithms for fast learning in feedforward neualnetworks,1992, Neural Networks
 Stochastic naturalgradient descent draws posterior samples in function space,2018, arXiv preprint arXiv:1806
 Accelerating natural gradient with higher-order invariance,2018, arXivpreprint arXiv:1803
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Optimal and robust estimation: with an introduction tostochastic control theory,2007, CRC press
 Noisy natural gradient asvariational inference,2017, arXiv preprint arXiv:1712
