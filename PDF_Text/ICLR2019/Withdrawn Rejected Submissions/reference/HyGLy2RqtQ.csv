title,year,conference
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, In International Conference on Machine Learning
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2018, 2018
 Gradient de-scent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima,2017, arXiv preprintarXiv:1712
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Algorithmic regularization in over-parameterizedmatrix recovery,2017, arXiv preprint arXiv:1712
 Complete solution of the local minima in the xor problem,1991, Network:Computation in Neural Systems
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 To-wards understanding the role of over-parametrization in generalization of neural networks,2018, arXivpreprint arXiv:1805
 Sensitivity and generalization in neural networks: an empirical study,2018, arXiv preprintarXiv:1802
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 The error surface of the 2-2-1 xor network: Thefinite stationary points,1998, Neural Networks
 High-dimensional probability,2017, An Introduction with Applications
1	Network Architecture in the XORD Problem8,2019,2	Experimental Setups8
 With probability at least 1 - √8k — 16e-8 Proposition 8,2019,13 and Lemma 8
