title,year,conference
 Stronger generalization bounds for deep nets via acompression approach,2018, Technical Report Preprint: arXiv:1802
 The spectrum of heavy tailed random matrices,2008, Communicationsin Mathematical Physics
 Poisson convergence for the largest eigenvalues of heavytailed random matrices,2009, Ann
 Phase transition of the largest eigenvalue for nonnull complexsample covariance matrices,2005, The Annals of Probability
 Comparing dynamics: deep neural networks versus glassy systems,2018, Technical ReportPreprint: arXiv:1803
 Un-reasonable effectiveness of learning neural networks: From accessible states and robust ensembles tobasic algorithmic schemes,2016, Proc
 A: Math,2006, Gen
 Extreme value problems in random matrix theory andother disordered systems,2007, J
 Cleaning large correlation matrices: tools from randommatrix theory,2017, Physics Reports
 Heavy-tailed random matrices,2009, Technical Report Preprint:arXiv:0909
 Entropy SGD: Biasing gradient descent into wide valleys,2016, Technical Report Preprint:arXiv:1611
 On the energy landscape of deep networks,2015, Technical Report Preprint:arXiv:1511
 The loss surfaces ofmultilayer networks,2014, Technical Report Preprint: arXiv:1412
 Theory of Levy matrices,1994, Physical Review E
 Statistical physics and representationsin real and artificial neural networks,2017, Technical Report Preprint: arXiv:1709
 Self-organized criticality and near-criticality inneural networks,2014, In D
 Identifying andattacking the saddle point problem in high-dimensional non-convex optimization,2014, In Annual Advancesin Neural Information Processing Systems 27: Proceedings of the 2014 Conference
 Limit theory for the largest eigenvalues of sample covariancematrices with heavy-tails,2014, Stochastic Processes and their Applications
 Random matrix theory,2005, Acta Numerica
 Statistical mechanics of learning,2001, Cambridge UniversityPress
 The spectrum edge of random matrix ensembles,1993, Nuclear Physics B
 Topology and geometry of deep rectified network optimization land-scapes,2016, Technical Report Preprint: arXiv:1611
 The energy landscape of a simple neural network,2017, Technical ReportPreprint: arXiv:1706
 AllenNLP: A deep semantic natural language processing platform,2018, Technical ReportPreprint: arXiv:1803
 Qualitatively characterizing neural network optimiza-tion problems,2014, Technical Report Preprint: arXiv:1412
 Rigorous learning curve bounds from statisticalmechanics,1996, Machine Learning
 Deep residual learning for image recognition,2015, Technical ReportPreprint: arXiv:1512
 An empirical analysis of deep network loss surfaces,2016, TechnicalReport Preprint: arXiv:1612
 Batch renormalization: Towards reducing minibatch dependence in batch-normalized mod-els,2017, Technical Report Preprint: arXiv:1702
 Three factorsinfluencing minima in SGD,2017, Technical Report Preprint: arXiv:1711
 On the distribution of the largest eigenvalue in principal components analysis,2001, TheAnnals of Statistics
 Second order properties of error surfaces: learning time andgeneralization,1991, In Advances in neural information processing systems 3
 Recent results about the largest eigenvalue of random covariance matrices and statis-tical applications,2005, Acta Physica Polonica
 On large-batch training fordeep learning: generalization gap and sharp minima,2016, Technical Report Preprint: arXiv:1609
 ImageNet classification with deep convolutionalneural networks,2012, In Annual Advances in Neural Information Processing Systems 25: Proceedings ofthe 2012 Conference
 Regularization for deep learning: A taxonomy,2017, TechnicalReport Preprint: arXiv:1710
 Random matrix theory and financial corre-lations,2005, Mathematical Models and Methods in Applied Sciences
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Efficient backprop in neural networks: Tricks of the trade,1988, LecturesNotes in Computer Science
 Deep learning and quantum entangle-ment: Fundamental connections with implications to network design,2017, Technical Report Preprint:arXiv:1704
 A surprising linear relationshippredicts test performance in deep networks,2018, Technical Report Preprint: arXiv:1807
 The dynamics of learning: a random matrix approach,2018, Technical ReportPreprint: arXiv:1805
 On the spectrum of random feature maps of high dimensional data,2018, TechnicalReport Preprint: arXiv:1805
 A random matrix approach to neural networks,2017, TechnicalReport Preprint: arXiv:1702
 Approximate computation and implicit regularization for very large-scale dataanalysis,2012, In Proceedings of the 31st ACM Symposium on Principles of Database Systems
 Implementing regularization implicitly via approximate eigenvectorcomputation,2011, In Proceedings of the 28th International Conference on Machine Learning
 Extreme value statistics of correlated random variables,2014, TechnicalReport Preprint: arXiv:1406
 Rethinking generalization requires revisiting old ideas: statisticalmechanics approaches and complex learning behavior,2017, Technical Report Preprint: arXiv:1710
 Revisiting small batch training for deep neural networks,2018, TechnicalReport Preprint: arXiv:1804
 A mean field view of the landscape of two-layer neuralnetworks,2018, Proc
 An analysis of training and generalization errors in shallow and deepnetworks,2018, Technical Report Preprint: arXiv:1802
 On the implicit bias of Dropout,2018, Technical Report Preprint:arXiv:1806
 Estimation of the covariance structure of heavy-tailed distributions,2017, TechnicalReport Preprint: arXiv:1708
 Exploring generalization in deeplearning,2017, Technical Report Preprint: arXiv:1706
 In search of the real inductive bias: on the role of implicitregularization in deep learning,2014, Technical Report Preprint: arXiv:1412
 Norm-based capacity control in neural network,2015, InProceedings of the 28th Annual Conference on Learning Theory
 The loss surface of deep and wide neural networks,2017, In Proceedings of the34th International Conference on Machine Learning
 Criticality and deep learning II: Momentum renormalisation group,2017, TechnicalReport Preprint: arXiv:1705
 On the saddle point problem for non-convexoptimization,2014, Technical Report Preprint: arXiv:1405
 Resurrecting the sigmoid in deep learning throughdynamical isometry: theory and practice,2017, Technical Report Preprint: arXiv:1711
 Nonlinear random matrix theory for deep learning,2017, In Annual Advancesin Neural Information Processing Systems 30: Proceedings of the 2017 Conference
 Regularized Laplacian estimation and fast eigenvector approxi-mation,2011, In Annual Advances in Neural Information Processing Systems 24: Proceedings of the 2011Conference
 Why and when can deep—butnot shallow—networks avoid the curse of dimensionality: a review,2016, Technical Report Preprint:arXiv:1611
 Exponential expressivity in deepneural networks through transient chaos,2016, In Annual Advances in Neural Information ProcessingSystems 29: Proceedings of the 2016 Conference
 Fluctuations of nuclear reaction widths,1956, Phys
 On theinformation bottleneck theory of deep learning,2018, In ICLR 2018
 Deep unsupervised learningusing nonequilibrium thermodynamics,2015, Technical Report Preprint: arXiv:1503
 Spectra of large random asymmetricmatrices,1988, Phys
 Exponentially vanishing sub-optimal local minima in multilayer nerualnetworks,2017, Technical Report Preprint: arXiv:1702
 Training convolutional networkswith noisy labels,2014, Technical Report Preprint: arXiv:1406
 Spectral ergodicity in deep learning architectures via surrogaterandom matrices,2017, Technical Report Preprint: arXiv:1704
 Local minima in training of deep networks,2016, TechnicalReport Preprint: arXiv:1611
 Going deeper with convolutions,2015, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Deep learning and the information bottleneck principle,2015, In Proceedingsof the 2015 IEEE Information Theory Workshop
 The distributions of random matrix theory and their applications,2009, InV
 Random matrix theory and wireless communications,2004, Foundations andTrends in Communications and Information Theory
 Measuring the VC-dimension of a learning machine,1994, NeuralComputation
 Batch Kalman normalization: Towards trainingdeep networks with micro-batches,2018, Technical Report Preprint: arXiv:1802
 The general inefficiency of batch training for gradient descentlearning,2003, Neural Networks
 Microscopic equations in rough energy landscape for neural networks,1997, In AnnualAdvances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference
 A walk with SGD,2018, Technical Report Preprint:arXiv:1802
 Hessian-based analysis of large batchtraining and robustness to adversaries,2018, Technical report
 Spectral norm regularization for improving the generalizability of deeplearning,2017, Technical Report Preprint: arXiv:1705
 Understanding convolutional neural network training withinformation theory,2018, Technical Report Preprint: arXiv:1804
 Visualizing and understanding convolutional networks,2013, Technical ReportPreprint: arXiv:1311
 Understanding deep learning requiresrethinking generalization,2016, Technical Report Preprint: arXiv:1611
