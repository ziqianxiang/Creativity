title,year,conference
 Adaptive dropout for training deep neural networks,2013, In Advances in NeuralInformation Processing Systems
 Understanding Dropout,2013, In Advances in Neural InformationProcessing Systems 26
 Entropy-SGD: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Improving deep neural networks for LVCSR usingrectified linear units and dropout,2013, In Proc
 Dropout as a Bayesian Approximation : Representing ModelUncertainty in Deep Learning,2016, 48
 Speech recognition with deep recurrent neural networks,2013, InINTERSPEECH
 Deep Residual Learning for ImageRecognition,2015, arXiv preprint arXiv:1512
 Improving neuralnetworks by preventing co-adaptation of feature detectors,2012, arXiv:1207
 Flat minima,1997, Neural computation
 Batch Normalization: Accelerating Deep Network Trainingby Reducing Internal Covariate Shift,2015, arXiv
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Variational Dropout and the Local Reparame-terization Trick,2015, arXiv
 Learning multiple layers of features from tiny images,2009, TechnicalReport
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 RNNDROP : A Novel Dropout forRNNs in ASR,2015, In ASRU
 Rectified linear units improve restricted boltzmann machines,2010, InProc
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Dropout improves recurrent neural networksfor handwriting recognition,2014, In ICFHR
 Improvements to deep convolutional neural networks for LVCSR,2013, In ASRU
 Deep learning in neural networks: An overview,2015, Neural Networks
 Very deep convolutional networks for large-scale imagerecognition,2014, CoRR
 Efficient object localization usingconvolutional networks,2015, In Computer Vision and Pattern Recognition (CVPR)
 Attention Is All You Need,2017, arXiv preprint arXiv:1706
 Extracting andcomposing robust features with denoising autoencoders,2008, In ICML
 Regularization of neuralnetworks using dropconnect,2013, In ICML
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
