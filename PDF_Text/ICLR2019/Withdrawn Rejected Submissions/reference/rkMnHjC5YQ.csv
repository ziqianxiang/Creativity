title,year,conference
 A polynomial time algorithm that learns two hidden unit nets,1990, Neural Computation
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Gradient de-scent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima,2017, arXiv preprintarXiv:1712
 Escaping from saddle points ´ online stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 No spurious local minima in nonconvex low rank problems:A unified geometric analysis,2017, In Proceedings of the 34th International Conference on MachineLearning
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Eigenvalue decay implies polynomial-time learnability for neuralnetworks,2017, arXiv preprint arXiv:1708
 Learning depth-three neural networks in polynomial time,2017, arXivpreprint arXiv:1709
 Reliably learning the ReLU inpolynomial time,2016, arXiv preprint arXiv:1611
 Learning one convolutional layer with overlappingpatches,2018, arXiv preprint arXiv:1802
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Beating the perils of non-convexity: Guar-anteed training of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 How to escape sad-dle points efficiently,2017, In Proceedings of the 34th International Conference on Machine Learning
 Deep learning without poor local minima,2016, In Advances In Neural InformationProcessing Systems
 Convergence analysis of two-layer neural networks with ReLU activa-tion,2017, arXiv preprint arXiv:1705
 The loss surface of deep and wide neural networks,2017, arXivpreprint arXiv:1704
 The loss surface and expressivity of deep convolutional neuralnetworks,2017, arXiv preprint arXiv:1710
 Provable methods for training neural networks with sparseconnectivity,2014, arXiv preprint arXiv:1412
 Learning ReLUs via gradient descent,2017, arXiv preprint arXiv:1705
 An analytical formula of population gradient for two-layered ReLU network and itsapplications in convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Gershgorin circle theorem,2003, 2003
 Learning halfspaces andneural networks with random initialization,2015, arXiv preprint arXiv:1511
 Learning non-overlapping convolutional neuralnetworks with multiple kernels,2017, arXiv preprint arXiv:1711
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
 The landscape of deep learning algorithms,2017, arXiv preprintarXiv:1705
