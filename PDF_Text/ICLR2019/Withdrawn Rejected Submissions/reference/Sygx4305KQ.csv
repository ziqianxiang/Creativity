title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Adaptive cubic regularisation methods forunconstrained optimization,2011, part i: motivation
 Return of the devil in thedetails: Delving deep into convolutional nets,2014, arXiv preprint arXiv:1405
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing Systems
 Why momentum really works,2017, Distill
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Understanding black-box predictions via influence functions,2017, InInternational Conference on Machine Learning
 Towards a theoretical understanding of batch normalization,2018, arXiv preprintarXiv:1805
 Sub-sampled cubic regularization for non-convex opti-mization,2017, arXiv preprint arXiv:1705
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep learning via hessian-free optimization,2010, In ICML
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 The levenberg-marquardt algorithm: implementation and theory,1978, In Numerical analysis
 Dynamics and algorithms for stochastic learning,1995, PhD thesis
 Revisiting natural gradient for deep networks,2013, In InternationalConference on Learning Representations 2013(workshop Track)
 Fast exact multiplication by the hessian,1994, Neural computation
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 Topmoumoute online naturalgradient algorithm,2008, In J
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Numerical optimization,1999, Springer Science
 Newton-type methods for non-convexoptimization under inexact hessian information,2017, arXiv preprint arXiv:1708
 Complexity issues in natural gradient descent method fortraining multilayer perceptrons,1998, Neural Computation
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Block-diagonal hessian-freeoptimization for training neural networks,2017, arXiv preprint arXiv:1712
