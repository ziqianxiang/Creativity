title,year,conference
 Threat of adversarial attacks on deep learning in computer vision:A survey,2018, arXiv preprint arXiv:1801
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Thermometer encoding: One hotway to resist adversarial examples,2018, 2018
 Stochastic activation pruning for robust adversarial de-fense,2018, arXiv preprint arXiv:1803
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Adversarial logit pairing,2018, arXiv preprintarXiv:1803
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Delving into transferable adversarial exam-ples and black-box attacks,2016, arXiv preprint arXiv:1611
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS workshop on deep learningand unsupervised feature learning
 Transferability in machine learning: fromphenomena to black-box attacks using adversarial samples,2016, arXiv preprint arXiv:1605
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia Conference on Computer and Communications Security
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In ICCV
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Ensemble adversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Mitigating adversarialeffects through randomization,2017, arXiv preprint arXiv:1711
 Deep representationlearning with target coding,2015, In AAAI
 Wide residual networks,2016, arXiv preprintarXiv:1605
