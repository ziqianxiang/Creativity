title,year,conference
 Tensorflow: Large-scale machinelearning on heterogeneous distributed systems,2016, arXiv preprint arXiv:1603
 Finding approxi-mate local minima faster than gradient descent,2017, In Proceedings of the 49th Annual ACM SIGACTSymposium on Theory ofComputing
 Variance reduction for faster non-convex optimization,2016, InInternational Conference on Machine Learning
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 The convergence ofa class of double-rank minimization algorithms: 2,1970, the newalgorithm
 Convex optimization: Algorithms and complexity,2015, Foundations andTrendsR in Machine Learning
 MxNet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, arXiv preprint arXiv:1512
 On the convergence ofa class of adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Universal stagewiselearning for non-convex problems with convergence on averaged solutions,2018, arXiv preprintarXiv:1808
 Incorporating Nesterov momentum into Adam,2016, 2016
 Shake-shake regularization,2017, arXiv preprint arXiv:1705
 A family of variable-metric methods derived by variational means,1970, Mathematicsof Computation
 Shampoo: Preconditioned stochastic tensor opti-mization,2018, arXiv preprint arXiv:1802
 Logarithmic regret algorithms for online convexoptimization,2007, Machine Learning
 Long short-term memory,1997, Neural computation
 ImProving generalization Performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Zoneout: Regularizing rnnsby randomly Preserving hidden activations,2016, arXiv preprint arXiv:1606
 Scalable adaPtive stochastic oPtimization using random Projections,2016, In Advancesin Neural Information Processing Systems
 On the convergence of stochastic gradient descent with adaPtivestePsizes,2018, arXiv preprint arXiv:1805
 Sgdr: stochastic gradient descent with restarts,2016, arXiv preprintarXiv:1608
 Efficient second orderonline learning by sketching,2016, In Advances in Neural Information Processing Systems
 The Penn treebank: annotating Predicate argumentstructure,1994, In Proceedings of the workshop on Human Language Technology
 OPtimizing neural networks with kronecker-factored aPProximatecurvature,2015, In International Conference on Machine Learning
 Kronecker-factored curvature aPProximations forrecurrent neural networks,2018, 2018
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Regularizing and oPtimizing lstm lan-guage models,2017, arXiv preprint arXiv:1708
 An analysis of neural language modelingat multiPle scales,2018, arXiv preprint arXiv:1803
 On the difficulty of training recurrent neuralnetworks,2013, In International Conference on Machine Learning
 Svcca: Singular vec-tor canonical correlation analysis for deep understanding and improvement,2017, arXiv preprintarXiv:1706
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning Representations
 Conditioning of quasi-newton methods for function minimization,1970, Mathematicsof Computation
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, arXiv preprint arXiv:1808
