title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Learning with agenerative adversarial network from a positive unlabeled dataset for image classification,2018, In IEEEInternational Conference on Image Processing
 Class-prior Estimation for Learning fromPositive and Unlabeled Data,2016, In Asian Conference on Machine Learning
 The cityscapes dataset for semanticurban scene understanding,2016, In Proc
 Biovision: a biomimetics platform for intrinsically motivatedvisual saliency learning,2018, IEEE Transactions on Cognitive and Developmental Systems
 Convex formulation for learning frompositive and unlabeled data,2015, In International Conference on Machine Learning
 Learning classifiers from only positive and unlabeled data,2008, InProceedings of the 14th ACM SIGKDD international conference on Knowledge discovery anddata mining
 A generative adversarial framework forpositive-unlabeled classification,2018, arXiv preprint arXiv:1711
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Estimating the class prior and posterior fromnoisy positives and unlabeled data,2016, In D
 Positive-UnlabeledLearning with Non-Negative Risk Estimator,2017, In I
 Learning with Confident Examples: RankPruning for Robust Classification with Noisy Labels,2017, arXiv preprint arXiv:1705
 Loss-sensitive generative adversarial networks on lipschitz densities,2017, arXiv preprintarXiv:1701
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, arXiv preprint arXiv:1511
 Mixture proportion estimation via kernelembeddings of distributions,2016, In International Conference on Machine Learning
 Efficient training for positiveunlabeled learning,2018, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Presence-only data andthe EM algorithm,2009, Biometrics
 LSUN: construction of a large-scale image dataset using deep learning with humans in the loop,2015, CoRR
 The axe in depth represents the training iteration axe on which the clearest histogramsgradually correspond to the closest training iterations,2016, Other alternative normalization techniqueshave been applied to GANs
