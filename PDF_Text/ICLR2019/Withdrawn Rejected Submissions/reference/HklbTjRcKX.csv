title,year,conference
 Understanding intermediate layers using linear classifierprobes,2016, arXiv preprint arXiv:1610
 Deep variational information bottle-neck,2017, In Proceedings of the International Conference on Learning Representations
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Thedifficulty of training deep architectures and the effect of unsupervised pre-training,2009, In ArtificialIntelligence and Statistics
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Deep residual learning for image recog-nition,2016, In Proceedings of the Conference on Computer Vision and Pattern Recognition
 i-RevNet: Deep invertible net-works,2018, In Proceedings of the International Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Gradient-based learning applied to documentrecognition,0018, Proceedings of the IEEE
 Inverting supervised representationswith autoregressive neural density models,2018, arXiv preprint arXiv:1806
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprintarXiv:1707
 Automatic differentiation inPyTorch,2017, In Proceedings of the Advances in Neural Information Processing Systems Workshop
 PixelCNN++: Improving thePixelCNN with discretized logistic mixture likelihood and other modifications,2017, arXiv preprintarXiv:1701
 Real-time single image and video super-resolution using an efficientsub-pixel convolutional neural network,2016, In Proceedings of the Conference on Computer Visionand Pattern Recognition
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Deep learning and the information bottleneck principle,2015, InInformation Theory Workshop
 Con-ditional image generation with PixelCNN decoders,2016, In Proceedings of the Advances in NeuralInformation Processing Systems
 Understandingdeep learning requires rethinking generalization,2017, In Proceedings of the International Conferenceon Learning Representations
 Evenafter 250 epochs of training convergence was yet to be reached,1080, On a single Titan 1080ti GPU eachof these training runs took approximately 15 days
