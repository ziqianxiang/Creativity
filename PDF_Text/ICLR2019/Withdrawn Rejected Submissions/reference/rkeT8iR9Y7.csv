title,year,conference
 Online algorithms and stochastic approximations,2012, In David Saad (ed
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Convergence diagnostics for stochastic gradient descent with constantlearning rate,2018, In International Conference on Artificial Intelligence and Statistics
 Tests of concentration for low-dimensional and high-dimensional directional data,2017, In Big and Complex Data Analysis
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 How to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Imagenet classification with deep convo-lutional neural networks,2012, In F
 Efficient backprop,2012, InNeural networks: Tricks ofthe trade
 Network in network,2013, arXiv preprint arXiv:1312
 Deep learning via hessian-free optimization,2010, In ICML
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprintarXiv:1511
 Skip connections eliminate singularities,2017, arXiv preprintarXiv:1701
 Topmoumoute online natural gra-dient algorithm,2008, In Advances in neural information processing systems
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
