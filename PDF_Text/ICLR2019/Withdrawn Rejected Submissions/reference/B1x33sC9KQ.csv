title,year,conference
 Scalable methods for 8-bit training ofneural networks,2018, arXiv preprint arXiv:1805
 Uniq: Uniform noise injection for the quantization of neural networks,2018, arXivpreprint arXiv:1804
 Pact: Parameterized clipping activation for quantized neuralnetworks,2018, arXiv preprint arXiv:1805
 Highly efficient 8-bit low precision inference of convolutionalneural networks with intelcaffe,2018, arXiv preprint arXiv:1805
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2017, arXiv preprint arXiv:1712
 gemmlowp: a small self-contained low-precision gemm library,2017,(2017)
 Joint training of low-precision neural network with quantization interval param-eters,2018, arXiv preprint arXiv:1808
 Quantizing deep convolutional networks for efficient inference: Awhitepaper,2018, arXiv preprint arXiv:1806
 Towards accurate binary convolutional neural network,2017, InAdvances in Neural Information Processing Systems
 The validity of the additive noise model for uniform scalarquantizers,2005, IEEE Transactions on Information Theory
 Discovering low-precision networks close to full-precision networks for efficient embedded inference,2018, arXiv preprint arXiv:1809
 8-bit inference with tensorrt,2017, In GPU Technology Conference
 Value-aware quantization for training and inferenceof neural networks,2018, arXiv preprint arXiv:1804
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European Conference on ComputerVision
 Training and inference with integers in deepneural networks,2018, arXiv preprint arXiv:1802
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
