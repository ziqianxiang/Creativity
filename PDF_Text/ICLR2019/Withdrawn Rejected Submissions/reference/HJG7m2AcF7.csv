title,year,conference
 Near-linear time approximation algorithmsfor optimal transport via sinkhorn iteration,2017, In Advances in Neural Information Processing Systems
 A simple but tough-to-beat baseline for sentenceembeddings,2017, ICLR
 Multimodal word distributions,2017, arXiv preprintarXiv:1704
 Enriching word vectors withsubword information,2016, arXiv preprint arXiv:1607
 Distributional inclusionvector embedding for unsupervised hypernymy detection,2017, arXiv preprint arXiv:1710
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 SentEval: An Evaluation Toolkit for Universal Sentence Repre-sentations,2018, arXiv preprint arXiv:1803
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, In Advances inneural information processing systems
 Hyperbolic entailment cones forlearning hierarchical embeddings,2018, arXiv preprint arXiv:1804
 Stochastic optimization for large-scale optimal transport,2016, In D
 Unsupervised Alignment of Embeddings withWasserstein Procrustes,2018, arXiv preprint arXiv:1805
 Distributional structure,1954, Word
 Learning word embeddings for hyponymy with entailment-based distributionalsemantics,2017, arXiv preprint arXiv:1710
 Long short-term memory,1997, Neural computation
 Supervised wordmoverâ€™s distance,2016, In Advances in Neural Information Processing Systems
 On the translocation of masses,1942, In Dokl
 Directional distributionalsimilarity for lexical inference,2010, Natural Language Engineering
 From word embeddings to documentdistances,2015, In International Conference on Machine Learning
 Dependency-based word embeddings,2014, In Proceedings of the52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)
 Neural word embedding as implicit matrix factorization,2014, In Advancesin neural information processing systems
 Improving distributional similarity with lessons learnedfrom word embeddings,2015, Transactions of the Association for Computational Linguistics
 The stanford corenlp natural language processing toolkit,2014, In Proceedings of 52ndannual meeting of the association for computational linguistics: system demonstrations
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 Memoire sur la theorie des deblais et des remblais,1781, HiStOire de IiAcademie ROyaledes Sciences de Paris
 Generalizing Point Embeddings using the Wasserstein Space ofElliptical Distributions ,2018, arXiv preprint arXiv:1805
 Unsupervised learning of sentence embeddingsusing compositional n-gram features,2017, arXiv preprint arXiv:1703
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Distributional lexical entailment by topic coherence,2014, In Proceedings of the 14thConference of the European Chapter of the Association for Computational Linguistics
 Fast dictionary learning with a smoothed Wassersteinloss,2016, In Arthur Gretton and Christian C
 Contextual correlates of synonymy,1965, Communicationsof the ACM
 Evalution 1,2015,0: an evolvingsemantic dataset for training and evaluation of distributional semantic models
 Twitter Sentiment Analysis with Deep ConvolutionalNeural Networks,2015, In 38th International ACM SIGIR Conference
 Gaussian Word Embedding with a WassersteinDistance Loss,2018, arXiv preprint arXiv:1808
 Experiments with three approaches to recognizing lexicalentailment,2015, Natural Language Engineering
 Word representations via gaussian embedding,2014, arXiv preprintarXiv:1412
 A general framework for distributional similarity,2003, In Proceedings of the2003 conference on Empirical methods in natural language processing
 Learning to distinguishhypernyms and co-hyponyms,2014, In Proceedings of COLING 2014
 Distilled wasserstein learning for wordembedding and topic modeling,2018, arXiv preprint arXiv:1809
 Determining gains acquiredfrom word embedding quantitatively using discrete distribution clustering,2017, In Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, arXiv preprint arXiv:1506
