title,year,conference
 Dynamical isometry and a mean field theoryof RNNs: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligenceand statistics
 On the selection of initialization and activation functionfor deep neural networks,2018, arXiv preprint arXiv:1805
 Cauchy distribution,2013, In Encyclopaedia of Mathematics: Volume 6
 Delving deep into rectifiers: Surpassing human-levelperformance on imagenet classification,2015, In Proceedings of the IEEE international conferenceon computer vision
 A central limit theorem for convex sets,2007, Inventiones mathematicae
 Efficient backprop,1998, In Neural networks:Tricks of the trade
 Deep neuralnetworks as gaussian processes,2018, ICLR
 Statistics in theory and practice,1993, Princeton University Press
 Resurrecting the sigmoid in deep learningthrough dynamical isometry: theory and practice,2017, In Advances in neural information processï¿¾ing systems
 The emergence of spectral universality indeep networks,2018, arXiv preprint arXiv:1802
 Exponential expressivity indeep neural networks through transient chaos,2016, In Advances in neural information processingsystems
 Mean field residual networks: On the edge of chaos,2017, In Advancesin neural information processing systems
