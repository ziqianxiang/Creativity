title,year,conference
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, arXiv preprint arXiv:1406
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Capacity and trainability in recurrentneural networks,2016, arXiv preprint arXiv:1611
 Finding structure in time,1990, Cognitive science
 Lstm recurrent networks learn simple context-free and context-sensitive languages,2001, IEEE Transactions on Neural Networks
 Recurrent nets that time and count,2000, In Neural Networks
 Generating sequences with recurrent neural networks,2013, arXiv preprint arXiv:1308
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Lstm:A search space odyssey,2017, IEEE transactions on neural networks and learning systems
 Long short-term memory,1997, Neural computation
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, In Advances in neural information processing systems
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Glove: Global vectors for wordrepresentation,2014, In Empirical Methods in Natural Language Processing (EMNLP)
 Simple recurrent networks learn context-free and context-sensitive languages bycounting,2001, Neural computation
 Learning nonregular languages: A comparison ofsimple recurrent networks and lstm,2002, Neural Computation
 Highway networks,2015, arXiv preprintarXiv:1505
 Learning longer-term dependencies inrnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 Memory networks,2014, CoRR
 Memory architectures in recurrent neural network language models,2018, In InternationalConference on Learning Representations
