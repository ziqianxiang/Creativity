title,year,conference
 Identifying beneficial task relations for multi-task learningin deep neural networks,2017, In Proceedings of the 15th Conference of the European Chapter ofthe Association for Computational Linguistics: Volume 2
 Multi-task learning for sequence tagging: An em-pirical study,2018, In Proceedings of the 27th International Conference on Computational Linguistics
 A fast and accurate dependency parser using neural net-works,2014, In Proceedings of the 2014 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 The PASCAL recognising textual entailmentchallenge,2006, In Machine learning challenges
 Semi-supervised sequence learning,2015, In NIPS 2015
 AllenNLP: A deep semantic natural languageprocessing platform,2017, 2017
 Learning distributed representations of sentencesfrom unlabelled data,2016, pp
 CCGbank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank,2007, Computational Linguistics
 Learning visually groundedsentence representations,2018, pp
 Adam: A method for stochastic optimization,2015, In ICLR
 Skip-Thought vectors,2015, In Advances in neural information processingsystems
 The Winograd schema challenge,2011, In Aaaispring symposium: Logical formalizations of commonsense reasoning
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Multi-tasksequence to sequence learning,2016, In ICLR
 Building a Large AnnotatedCorpus of English: The Penn Treebank,1993, Computational Linguistics
 Learned in translation:Contextualized Word vectors,2017, In Advances in Neural Information Processing Systems
 Pointer sentinel mixturemodels,2017, In ICLR
 Distributed representa-tions of words and phrases and their compositionality,2013, In C
 Dissecting contextualword embeddings: Architecture and representation,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Towards a unified natural language inference framework to evaluatesentence representations,2018, arXiv preprint 1804
 Improving language under-standing by generative pre-training,2018, Unpublished manuscript accessible via the OpenAI Blog
 On the convergence of Adam and beyond,2018, InICLR
 Bidirectional attentionflow for machine comprehension,2017, In ICLR
 Learning generalpurpose distributed sentence representations via large scale multi-task learning,2018, In ICLR
 Learning semantic textual similarity from conver-sations,2018, arXiv preprint 1804
 Taskonomy: Disentangling task transfer learning,2018, In IEEE Conference on ComputerVision and Pattern Recognition (CVPR)
 Language modeling teaches you more syntax than translationdoes: Lessons learned through auxiliary task analysis,2018, arXiv preprint 1809
 Ordinal common-senseinference,2017, Transactions of the Association of Computational Linguistics
