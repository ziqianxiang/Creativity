title,year,conference
 Learning polynomials withneural networks,2014, In International Conference on Machine Learning
 Provable bounds for learning somedeep representations,2014, In International Conference on Machine Learning
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Adam: A method for stochastic optimization,2014, CoRR
 On the computational efficiency of trainingneural networks,2014, In Advances in Neural Information Processing Systems
 Deep learning and hierarchal generative models,2016, arXiv preprintarXiv:1612
 A probabilistic framework for deep learn-ing,2016, In Advances in Neural Information Processing Systems
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Factoring variations in natural images with deepgaussian mixture models,2014, In Advances in Neural Information Processing Systems
 Learning halfspaces andneural networks with random initialization,2015, arXiv preprint arXiv:1511
 l1-regularized neural networks are improperlylearnable in polynomial time,2016, In International Conference on Machine Learning
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
