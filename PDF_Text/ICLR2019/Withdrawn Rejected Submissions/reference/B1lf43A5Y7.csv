title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, ICLR
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Gated-attention readers for text comprehension,2016, ACL
 Neuralmodels for reasoning over multiple mentions using coreference,2018, NAACL
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Teaching machines to read and comprehend,2015, In Advances inNeural Information Processing Systems
 The Goldilocks Principle: Read-ing Childrenâ€™s Books with Explicit Memory Representations,2015, arXiv preprint arXiv:1511
 Reinforced mnemonic reader for machine comprehen-sion,2017, CoRR
 TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehension,2017, ACL
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Ask me anything: Dynamic memory networks fornatural language processing,2016, In International Conference on Machine Learning
 RACE: Large-scale ReAdingComprehension Dataset From Examinations,2017, arXiv preprint arXiv:1704
 Neural machine translation withsupervised attention,2016, COLING
 Towards neural network-based reason-ing,2015, arXiv preprint arXiv:1508
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Weaver: Deep Co-Encoding of Questions and Documents for Machine Reading,2018, In Proceedings of the InternationalConference on Machine Learning (ICML)
 MCTest: A Challenge Dataset forthe Open-Domain Machine Comprehension of Text,2013, In EMNLP
 Get to the point: Summarization with pointer-generator networks,2017, ACL
 Bidirectional attentionflow for machine comprehension,2016, arXiv preprint arXiv:1611
 Reasonet: Learning to stop readingin machine comprehension,2017, In Proceedings of the 23rd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Exploringgraph-structured passage representation for multi-hop reading comprehension with graph neuralnetworks,2018, arXiv preprint arXiv:1809
 Iterative alternatingneural attention for machine reading,2016, arXiv preprint arXiv:1606
 End-to-end memory networks,2015, In Advancesin neural information processing systems
 Making neural qa as simple as possible but notsimpler,2017, CoNLL
 Constructing datasets for multi-hop read-ing comprehension across documents,2017, arXiv preprint arXiv:1710
 QANet: Combining Local Convolution with Global Self-Attention for ReadingComprehension,2018, arXiv preprint arXiv:1804
