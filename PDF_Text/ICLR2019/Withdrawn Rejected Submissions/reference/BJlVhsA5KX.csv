title,year,conference
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks ofthe trade
 Entropy-sgd: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 Shake-shake regularization,2017, arXiv preprint arXiv:1705
 Why random reshuffling beats stochasticgradient descent,2015, arXiv preprint arXiv:1510
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Densely connectedconvolutional networks,2016, arXiv preprint arXiv:1608
 Deep networks withstochastic depth,2016, In European Conference on Computer Vision
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Learning multiple layers of features from tiny images,2009, 2009
 Efficient backprop,1998, InNeural networks: Tricks of the trade
 Deeply-supervised nets,2015, In Artificial Intelligence and Statistics
 Network in network,2013, arXiv preprint arXiv:1312
 Convergence analysis of distributedstochastic gradient descent with shuffling,2017, arXiv preprint arXiv:1709
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Training very deep networks,2015, InAdvances in neural information processing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
