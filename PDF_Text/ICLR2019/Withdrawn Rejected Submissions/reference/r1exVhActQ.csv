title,year,conference
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Dynamic network surgery for efficient dnns,2016, In AdvancesIn Neural Information Processing Systems
 Variational dropout sparsifies deep neuralnetworks,2017, arXiv preprint arXiv:1701
 Net-trim: Convex pruning ofdeep neural networks with performance guarantee,2017, In Advances in Neural Information ProcessingSystems
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Clip-q: Deep network compression learning by in-parallel pruning-quantization,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Learning the number of neurons in deep networks,2016, InAdvances in Neural Information Processing Systems
 Learning structured sparsity indeep neural networks,2016, In Advances in Neural Information Processing Systems
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 An entropy-based pruning method for cnn compression,2017, arXiv preprintarXiv:1706
 Pd-sparse: Aprimal and dual sparse approach to extreme multiclass and multilabel classification,2016, In InternationalConference on Machine Learning
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
