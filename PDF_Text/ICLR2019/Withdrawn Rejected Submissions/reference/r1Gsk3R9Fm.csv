title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2018, International COnferenCe on Learning RePresentatiOns (ICLR)
 Provable bounds for learning some deeprepresentations,2014, In IntematiOnal COnferenCe on MaChine Learning
 Breaking the curse of dimensionality with convex neural networks,2014, arXiv PrePrintarXiv:1412
 Approximation and estimation bounds for artificial neural networks,1994, MaChineLearning
 Convexneural networks,2006, In AdVanCeS in neuralinfOrmatiOn PrOCeSSing systems
 Greedy layer-wise training ofdeep networks,2007, In AdVanCeS in neural information PrOCeSSing systems
 Freezeout: Accelerate training byprogressively freezing layers,2017, arXiv Preprint arXiv:1706
 Adanet: Adaptivestructural learning of artificial neural networks,2016, arXiv PrePrint arXiv:1607
 Approximation by superpositions of a sigmoidal function,1989, MathematiCS of Control
 Discrimina-tive unsupervised feature learning with convolutional neural networks,2014, In AdVanceS in NeUralInfOrmatiOn PrOCeSSing Systems
 Improved learning of one-hidden-layer convolutional neural networkswith overlaps,2018, CoRR
 The cascade-correlation learning architec-ture,1990, In D
 The cascade-correlation learning architecture,1990, In AdVanceSin neural information PrOCeSSing systems
 Experiments with a new boosting algorithm,1996, In Icml
 Greedy function approximation: a gradient boosting machine,2001, AnnalS ofStatiStics
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv PrePrint arXiv:1711
 Highway and residual networks learnunrolled iterative estimation,2016, arXiv PrePrint arXiv:1612
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE COnferenCe on COmPUter ViSiOn and Pattem Recognition
 Dark knowledge,2014, PreSented as the keynote inBayLearn
 A fast learning algorithm for deep beliefnets,2006, NeUral computation
 Learning deep resnet blockssequentially using boosting theory,2017, arXiv PrePrint arXiv:1706
 Decoupled neural interfaces using synthetic gradients,2016, arXivPrePrint arXiv:1608
 Beating the perils of non-convexity:Guaranteed training of neural networks using tensor methods,2015, arXiv PrePrint arXiv:1506
 Imagenet classification with deeP convo-IUtionalneUralnetworks,2012, In AdVanCeS in neuralinformation ProCeSSing systems
 Optimal brain damage,1990, In AdVanCeS in neuralinformation ProCeSSing systems
 Training mlps layer by layer using an objective function forinternal representations,1996, NeUraI Networks
 A provably correct algorithm for deep learning that actuallyworks,2018, arXiv PrePrint arXiv:1803
 Understanding deep convolutional networks,2016, Phil
 Deep incremental boosting,2017, arXiv PrePrint arXiv:1708
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv PrePrintarXiv:1805
 UnsuPervised learning of visual rePresentations by solving jigsawpuzzles,2016, In EUroPean ConferenCe on ComPUter Vision
 Building a regular decision boundary with deep networks,2017, In 2017 IEEEConferenCe on ComPUter ViSion and Pattern Recognition
 Scaling the scattering transform: Deephybrid networks,2017, In IEEE International ConferenCe on ComPUter Vision
 Fisher vectors meet neural networks: A hybrid classificationarchitecture,2015, In ProCeedingS of the IEEE ConferenCe on ComPUter ViSion and Pattern recognition
 Approximation theory of the mlp model in neural networks,1999, ACta numerica
 ImageNetLarge Scale Visual Recognition Challenge,2015, International JOUrnaI of COmpUter VisiOn (IJCV)
 EvolUtion strategies as ascalable alternative to reinforcement learning,2017, arXiv preprint arXiv:1703
 Failures of deep learning,2017, arXiv preprintarXiv:1703
 Mastering the game of go withouthuman knowledge,2017, Nature
 Attention is all you need,2017, In AdVanCeS in NeUraI InfOrmatiOnPrOCeSSing Systems
 Neural networks with finite intrinsic dimensionhave no spurious valleys,2018, arXiv preprint arXiv:1802
 Deep growing learning,2017, InPrOCeedingS of the IEEE International COnferenCe on COmpUter ViSion
 Understanding neuralnetworks through deep visualization,2015, arXiv preprint arXiv:1506
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Visualizing and understanding convolutional networks,2014, InEUrOpean COnferenCe on COmpUter ViSion
 This choice is to reduce architectural elements which may be inherently lossy such asaverage pooling,2019, Compared to maxpooling operations it also helps to maintain the network in Sec
