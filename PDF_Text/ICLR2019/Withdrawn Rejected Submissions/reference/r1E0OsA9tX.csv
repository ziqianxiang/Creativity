title,year,conference
 Weight uncertainty inneural networks,2015, arXiv preprint arXiv:1505
 Convex optimization,2004, Cambridge university press
 Reducing overfit-ting in deep networks by decorrelating representations,2015, arXiv preprint arXiv:1511
 Steinâ€™s paradox in statistics,1977, Scientific American
 Generalized cross-validation as a method forchoosing a good ridge parameter,1979, Technometrics
 Recasting gradient-based meta-learning as hierarchical bayes,2018, arXiv preprint arXiv:1801
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Probabilistic backpropagation for scalable learningof bayesian neural networks,2015, In International Conference on Machine Learning
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Learning multiple layers of features from tiny images,2009, 2009
 Determinantal point processes for machine learning,2012, Foundationsand TrendsR in Machine Learning
 Deep learning,2015, nature
 Diversity networks: Neural network compression using determinantalpoint processes,2015, arXiv preprint arXiv:1511
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 An empirical bayes approach to optimizing machine learning algorithms,2017, InAdvances in Neural Information Processing Systems
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprintarXiv:1707
 An empirical bayes approach to statistics,1956, Technical report
 Noisy natural gradient asvariational inference,2017, arXiv preprint arXiv:1712
 Efficient multitask feature and relationshiplearning,2017, arXiv preprint arXiv:1702
