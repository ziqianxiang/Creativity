title,year,conference
 Sparse communication for distributed gradient descent,2017, InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
 QSGD: randomized quantization forcommunication-optimal stochastic gradient descent,2016, CoRR
 Accel-erating asynchronous stochastic gradient descent for neural machine translation,2018, arXiv preprintarXiv:1808
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Communication quantizationfor data-parallel training of deep neural networks,2016, In Proceedings of the Workshop on MachineLearning in High Performance Computing Environments
 Adam: A method for stochastic oPtimization,2015, 3rd InternationalConference for Learning Representations
 DeeP gradient comPression: Re-ducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Large-scale deeP unsuPervised learning usinggraPhics Processors,2009, In Proceedings of the 26th annual international conference on machinelearning
 1-bit stochastic gradient descent and itsaPPlication to data-Parallel distributed training of sPeech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
