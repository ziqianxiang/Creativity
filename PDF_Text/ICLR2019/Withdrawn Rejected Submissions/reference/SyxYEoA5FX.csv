title,year,conference
 Emergence of invariance and disentangling in deep represen-tations,2017, arXiv preprint arXiv:1706
 InVertible residual networks,2018, arXivpreprint arXiv:1811
 Signal recovery from pooling representations,2014, InProceedings of the 31st International Conference on Machine Learning
 The preimageof rectifier activities,2017, In International Conference on Learning Representations (workshop)
 Parsevalnetworks: improving robustness to adversarial examples,2017, In Proceedings of the 34 InternationalConference on Machine Learning
 Linear programming and extensions,1963, Princeton University Press
 Towards understanding theinvertibility of convolutional neural networks,2017, In 26th International Joint Conference on ArtificialIntelligence
 The reversible residual net-work: backProPagation without storing activations,2017, In Advances in Neural Information ProcessingSystems 30
 ExPlaining and harnessing adversarialexamPles,2014, 2014
 Regularisation of neural networksby enforcing liPschitz continuity,2018, arXiv preprint arXiv:1804
 i-revnet: deep invertiblenetworks,2018, In International Conference on Learning Representations
 Improving training of deep neural networks via singular value bounding,2017, IEEE Conferenceon Computer Vision and Pattern Recognition
 Adam: a method for stochastic optimization,2015, Proceedings of the32nd International Conference on International Conference on Machine Learning
 Inverting feedforward neural networks using linear andnonlinear programming,1999, IEEE Transactions on Neural Networks
 Rectifier nonlinearities improve neural networkacoustic models,2013, In Proceedings of the 30th International Conference on Machine Learning
 Understanding deep image representations by invertingthem,2015, In In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
 Understanding deep convolutional networks,1364, Philosophical Transactionsof the Royal Society of London A: Mathematical
 Spectral normalization forgenerative adversarial networks,2018, In International Conference on Learning Representations
 On the number of linearregions of deep neural networks,2014, In Advances in Neural Information Processing Systems 27
 RegU-larizing cnns with locally constrained decorrelations,2017, In International Conference on LearningRepresentations
 The singUlar valUes of convolUtional layers,2018, arXivpreprint arXiv:1805
 Understanding and improvingconvolUtional neUral networks via concatenated rectified linear Units,2016, In Proceedings of the 33rdInternational Conference on Machine Learning
 RObUSt large margin deepneUral networks,2017, IEEE Transactions on Signal Processing
 IntrigUing properties of neUral networks,2014, International Conference on LearningRepresentations
 Deep learning and the information bottleneck principle,2015, InInformation Theory Workshop (ITW)
 Lipschitz-margin training: Scalable certificationof pertUrbation invariance for deep neUral networks,2018, arXiv preprint arXiv:1802
 Analysis of deep neUral networkswith extended data jacobian matrix,2016, In Proceedings of the 33rd International Conference onMachine Learning
