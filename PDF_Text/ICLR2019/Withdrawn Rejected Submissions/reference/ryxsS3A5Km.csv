title,year,conference
 Accelerating neural architecturesearch using performance prediction,2017, arXiv preprint arXiv:1705
 Towards robust evaluations of continual learning,2018, arXiv preprintarXiv:1805
 Pathnet: Evolution channels gradient descent in superneural networks,2017, arXiv preprint arXiv:1701
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Lifelong learning with dynamicallyexpandable networks,2017, arXiv preprint arXiv:1708
 Overcomingcatastrophic forgetting by incremental moment matching,2017, In Advances in Neural InformationProcessing Systems
 Supportnet:solving catastrophic forgetting in class incremental learning with support data,2018, arXiv preprintarXiv:1806
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Gradient episodic memory for continual learning,2017, In Advances in NeuralInformation Processing Systems
 Variational continual learn-ing,2018, In International Conference on Learning Representations
 Connectionist models of recognition memory: constraints imposed by learning andforgetting functions,1990, Psychological review
 Learning multiple visual domains withresidual adapters,2017, In Advances in Neural Information Processing Systems
 icarl:Incremental classifier and representation learning,2017, In Proc
 Efficient parametrization of multi-domain deep neural networks,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Active learning for convolutional neural networks: A core-setapproach,2018, 2018
 Overcoming catastrophicforgetting with hard attention to the task,2018, arXiv preprint arXiv:1801
 Continual learning with deep generativereplay,2017, In Advances in Neural Information Processing Systems
 Evolving neural networks through augmenting topolo-gies,2002, Evolutionary computation
 Lifelong robot learning,1995, In The biology and technology ofintelligent autonomous agents
 The mnist database of handwritten digits,1998, 1998
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 ”Individual” meansthat each task is trained individually and Weights are initialized randomly,2019, ”Classifier” means thatonly the last layer classifier could be tuned While the former 25 layers are transfer from ImageNetpretrained model and kept fixed during training
