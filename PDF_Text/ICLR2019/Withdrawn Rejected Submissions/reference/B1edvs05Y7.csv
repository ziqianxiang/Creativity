title,year,conference
 Sparse communication for distributed gradient descent,2017, arXivpreprint arXiv:1704
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems 
 signsgd:compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Project adam:Building an efficient and scalable deep learning training system,2014, In OSDI
 Introduction toalgorithms,2009, MIT press
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International conference on machine learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Communication efficient distributedmachine learning with the parameter server,2014, In Advances in Neural Information Processing Systems
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Communication-efficientlearning of deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Sparknet: Training deep networksin spark,2015, arXiv preprint arXiv:1511
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Privacy-preserving deep learning,2015, In Proceedings of the 22ndACM SIGSAC conference on computer and communications security
 Scalable distributed dnn training using commodity gpu cloud computing,2015, In SixteenthAnnual Conference of the International Speech Communication Association
 Variance-based gradient compression for efficientdistributed deep learning,2018, arXiv preprint arXiv:1802
 Tern-grad: Ternary gradients to reduce communication in distributed deep learning,2017, arXiv preprintarXiv:1705
 Petuum: A new platform for distributed machine learningon big data,2015, IEEE Transactions on Big Data
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
 Parallelized stochastic gradientdescent,2010, In Advances in neural information processing systems
