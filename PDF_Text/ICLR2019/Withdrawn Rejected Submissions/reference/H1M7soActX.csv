title,year,conference
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Escaping saddles withstochastic gradients,2018, arXiv preprint arXiv:1803
 Flat minima,1997, Neural Computation
 On the diffusion approximation of noncon-vex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 HoW to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 On large-batch trainingfor deep learning: Generalization gap and sharp minima,2017, In In International Conference onLearning Representations (ICLR)
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Stochastic gradient descent as approximatebayesian inference,2017, arXiv preprint arXiv:1704
 Generalization bounds of sgld for non-convex learning: TWo theoretical vieWpoints,2017, arXiv preprint arXiv:1707
 Empirical analysis ofthe hessian of over-parametrized neural netWorks,2017, arXiv preprint arXiv:1706
 Opening the black box of deep neural netWorks via informa-tion,2017, arXiv preprint arXiv:1703
 A bayesian perspective on generalization and stochastic gra-dient descent,2018, International Conference on Learning Representations
 ToWards understanding generalization of deep learning: Perspectiveof loss landscapes,2017, arXiv preprint arXiv:1706
 A hitting time analysis of stochastic gradientlangevin dynamics,2017, arXiv preprint arXiv:1702
