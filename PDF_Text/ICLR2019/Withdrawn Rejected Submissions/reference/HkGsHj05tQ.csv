title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Scalable methods for 8-bit training ofneural networks,2018, arXiv preprint arXiv:1805
 Understanding batch normalization,2018, arXiv preprintarXiv:1806
 Accelerating compute-intensive applications with gpus and fpgas,2008, In Application Specific Processors
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 Tensorflow: Large-scale machine learning on heterogeneous distributedsystems,2016, 2016
 A novel channel Pruningmethod for deeP neural network comPression,2018, arXiv preprint arXiv:1805
 Densely connectedconvolutional networks,2016, PP
 Batch renormalization: Towards reducing minibatch dePendence in batch-normalizedmodels,2017, In Advances in Neural Information Processing Systems
 Batch normalization: Accelerating deeP network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Self-normalizingneural networks,2017, In Advances in Neural Information Processing Systems
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Fast algorithms for convolutional neural networks,2016, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Deep gradient compression: Re-ducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, arXiv preprint arXiv:1707
 On the importance ofsingle directions for generalization,2018, arXiv preprint arXiv:1803
 Batch kalmannormalization: Towards training deep neural networks with micro-batches,2018, arXiv preprintarXiv:1802
 Learning structured sparsity indeep neural networks,2016, In Advances in Neural Information Processing Systems
 L1-norm batch nor-malization for efficient training of deep neural networks,2018, IEEE Transactions on Neural Networksand Learning Systems
 Group normalization,2018, arXiv preprint arXiv:1803
 Struc-turally sparsified backward propagation for faster long short-term memory training,2018, arXiv preprintarXiv:1806
