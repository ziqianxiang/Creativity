title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 An elementary introduction to modern convex geometry,1997, Flavors ofgeometry
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Convexneural networks,2006, In Advances in neural information processing systems
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Entropy-sgd: Biasing gradient descentinto wide valleys,2016, arXiv preprint arXiv:1611
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, arXiv preprint arXiv:1805
 Gradient descent learnsone-hidden-layer cnn: Don’t be afraid of spurious local minima,2017, arXiv preprint arXiv:1712
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Implicit bias of gradient descent onlinear convolutional networks,2018, arXiv preprint arXiv:1806
 Risk and parameter convergence of logistic regression,2018, arXiv preprintarXiv:1803
 Empirical margin distributions and bounding thegeneralization error of combined classifiers,2002, The Annals of Statistics
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Conference On Learning Theory
 On the computational efficiency of training neuralnetworks,2014, In Advances in Neural Information Processing Systems
 A mean field view of the landscape oftwo-layers neural networks,2018, arXiv preprint arXiv:1804
 On the importance ofsingle directions for generalization,2018, arXiv preprint arXiv:1803
 Convergenceof gradient descent on separable data,2018, arXiv preprint arXiv:1803
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprintarXiv:1707
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, arXiv preprintarXiv:1805
 The loss surface of deep and wide neural networks,2017, arXiv preprintarXiv:1704
 Margin maximizing loss functions,2004, In Advances inneural information processing systems
 Neural networks as interacting particle systems:Asymptotic convexity of the loss landscape and universal scaling of the approximation error,2018, arXivpreprint arXiv:1805
 On the quality of the initial basin in overspecified neural networks,2016, InInternational Conference on Machine Learning
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Mean field analysis of neural networks,2018, arXivpreprint arXiv:1805
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 Neural networks with finite intrinsic dimensionhave no spurious valleys,2018, arXiv preprint arXiv:1802
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 1-norm support vector machines,2004, InAdvances in neural information processing systems
 Let θ ∈ K-τ,1997, From Lemma E
