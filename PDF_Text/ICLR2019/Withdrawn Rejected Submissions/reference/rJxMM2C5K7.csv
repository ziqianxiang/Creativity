title,year,conference
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems 
 Online algorithms and stochastic approximations,2012, In David Saad (ed
 Convex optimization: Algorithms and complexity,2015, Foundations and TrendsÂ® inMachine Learning
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Communication quantizationfor data-parallel training of deep neural networks,2016, In Proceedings of the Workshop on MachineLearning in High Performance Computing Environments
 Dithered quantizers,1993, IEEE Transactions on InformationTheory
 More effective distributed ML via a stale synchronous parallelparameter server,2013, In Advances in Neural Information Processing Systems 26
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprint
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in Neural Information Processing Systems24
 Regularizing deep neuralnetworks by noise: Its interpretation and optimization,2017, In Advances in Neural InformationProcessing Systems
 Dither signals and their effect on quantization noise,1964, IEEE Transactions onCommunication Technology
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Interspeech
 Scalable distributed DNN training using commodity GPU cloud computing,2015, InINTERSPEECH
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In I
 Lattices are everywhere,2009, In 2009 Information Theory and Applications Workshop
 Hogwild++: A new mechanism for decentralized asynchronousstochastic gradient descent,2016, In 2016 IEEE 16th International Conference on Data Mining (ICDM)
