title,year,conference
 Guaranteed non-orthogonal tensor decomposition viaalternating rank-1 updates,2014, arXiv preprint arXiv:1402
 Provable bounds for learning some deep representa-tions,2014, In International Conference on Machine Learning
 Convolutional rectifier networks as generalized tensor decompositions,2016, InInternational Conference on Machine Learning
 A unified architecture for natural language processing: Deep neuralnetworks with multitask learning,2008, In Proceedings of the 25th international conference on Machine learning
 Tail bounds via generic chaining,2013, arXiv preprint arXiv:1309
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2017, arXiv preprint arXiv:1712
 Local geometry of one-hidden-layer neural networks for logisticregression,2018, arXiv preprint arXiv:1802
 Escaping from saddle points - online stochastic gradient fortensor decomposition,2015, In Conference on Learning Theory
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 Beating the perils of non-convexity: Guaranteedtraining of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 Tensorly: Tensor learning in python,2016, arXiv preprintarXiv:1610
 Tensor regressionnetworks,2017, arXiv preprint arXiv:1707
 Imagenet classification with deep convolutional neuralnetworks,2012, In Advances in neural information processing systems
 Convergence analysis of two-layer neural networks with relu activation,2017, In Advancesin Neural Information Processing Systems
 A mean field view of the landscape of two-layers neuralnetworks,2018, arXiv preprint arXiv:1804
 Learning compact neural networks with regularization,2018, arXiv preprint arXiv:1802
 Empirical analysis of the hessian ofover-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Tensor decomposition for signal processing and machine learning,2017, IEEE Transactions on SignalProcessing
 Learning ReLUs via gradient descent,2017, arXiv preprint arXiv:1705
 Theoretical insights into the optimization landscapeof over-parameterized shallow neural networks,2017, arXiv preprint arXiv:1707
 No bad local minima: Data independent training error guarantees for multilayerneural networks,2016, arXiv preprint arXiv:1605
 Spectral norm of random tensors,2014, arXiv preprint arXiv:1407
 Learning non-overlapping convolutional neural networks withmultiple kernels,2017, arXiv preprint arXiv:1711
 Recovery guarantees for one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
