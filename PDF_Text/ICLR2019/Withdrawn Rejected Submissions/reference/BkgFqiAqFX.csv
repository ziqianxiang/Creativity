title,year,conference
 Efficient approaches for escaping higher order saddle points innon-convex optimization,2016, arXiv preprint arXiv:1602
 Provable bounds for learning some deeprepresentations,2014, In Eric P
 Globally optimal gradient descent for a convnet with gaUssianinpUts,2017, arXiv preprint arXiv:1702
 Gradient de-scent learns one-hidden-layer cnn: Donâ€™t be afraid of spUrioUs local minima,2017, arXiv preprintarXiv:1712
 Learning one-hidden-layer neUral networks with landscapedesign,2017, arXiv preprint arXiv:1711
 EigenvalUe decay implies polynomial-time learnability for neUralnetworks,2017, arXiv preprint arXiv:1708
 Learning depth-three neUral networks in polynomial time,2017, arXivpreprint arXiv:1709
 Reliably learning the ReLU inpolynomial time,2016, arXiv preprint arXiv:1611
 Beating the perils of non-convexity: GUar-anteed training of neUral networks Using tensor methods,2015, arXiv preprint arXiv:1506
 Convergence analysis of two-layer neUral networks with ReLU activa-tion,2017, arXiv preprint arXiv:1705
 Electron-proton dynamics in deeplearning,2017, arXiv preprint arXiv:1702
 Learning halfspaces andneural networks with random initialization,2015, arXiv preprint arXiv:1511
 Learning non-overlapping convolutional neuralnetworks with multiple kernels,2017, arXiv preprint arXiv:1711
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
 Let x0 be the component of x along z and y be the component of x in the spaceorthogonal to z,2019, Let Z denote a unit vector along z
