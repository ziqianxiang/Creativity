title,year,conference
 Extrapolation methods: theory and practice,2013, Elsevier
 Incorporating nesterov momentUm into adam,2016, International Conference on Learning Representations(ICLR) Workshop Track
 Generative adversarial nets,2014, Annual Conference on Neural Information Processing Systems(NIPS)
 IntrodUction to online convex optimization,2016, Foundations and Trends in Optimization
 Deep residUal learning for image recognition,2016, Conferenceon Computer Vision and Pattern Recognition (CVPR)
 Adam: A method for stochastic optimization,2015, International Conference onLearning Representations (ICLR)
 An empirical evalUationof deep architectUres on problems with many factors of variation,2007, International Conference on Machine Learning(ICML)
 Learning wordvectors for sentiment analysis,2011, Annual Meeting of the Association for Computational Linguistics (ACL)
 Playing atari with deep reinforcement learning,2013, Annual Conference on Neural Information ProcessingSystems (NIPS) Deep Learning Workshop
 IntrodUctory lectUres on convex optimization: A basic coUrse,2004, Springer
 Some methods of speeding Up the convergence of iteration methods,1964, Mathematics and MathematicalPhysics
 On the convergence of adam and beyond,2018, International Conferenceon Learning Representations (ICLR)
 Regularized nonlinear acceleration,2016, Annual Conferenceon Neural Information Processing Systems (NIPS)
 Striving for simplicity: The all convo-lutional net,2015, International Conference on Learning Representations (ICLR)
 Fast convergence of regularized learning ingames,2015, Annual Conference on Neural Information Processing Systems (NIPS)
 Rmsprop: Divide the gradient by a running average of its recent magnitude,2012, COURSERA:Neural Networks for Machine Learning
 Adadelta: An adaptive learning rate method,2012, arXiv:1212
