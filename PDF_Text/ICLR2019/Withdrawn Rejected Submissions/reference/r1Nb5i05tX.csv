title,year,conference
 Information dropout: Learning optimal representationsthrough noisy computation,2018, IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI)
 Deep variational informationbottleneck,2018, In International Conference on Learning Representations (ICLR)
 Learning representations for neural network-based classi-fication using the information bottleneck principle,2018, arXiv preprint arXiv:1802
 Mutual information neural estimation,2018, In International Conferenceon Machine Learning (ICML)
 Relevant sparse codes with variational informa-tion bottleneck,2016, In Advances in Neural Information Processing Systems (NIPS)
 Asymptotic evaluation of certain markov processexpectations for large time,1983, iv
 Efficient estimation of mutual information forstrongly dependent variables,2015, In Artificial Intelligence and Statistics
 Auto-encoding total correla-tion explanation,2018, arXiv preprint arXiv:1802
 Variational dropout and the local reparame-terization trick,2015, In Advances in Neural Information Processing Systems (NIPS)
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Estimation of entropy and mutual information,2003, Neural computation
 Analyzing noise in autoencoders and deepnetworks,2014, arXiv preprint arXiv:1406
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Deep learning and the information bottleneck principle,2015, InIEEE Information Theory Workshop (ITW)
 The information bottleneck method,1999, InThe 37th annual Allerton Conference on Communication
 Maximally informative hierarchical representations of high-dimensional data,2015, In Artificial Intelligence and Statistics
