title,year,conference
 Unitary evolution recurrent neural networks,2016, InICML
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, In ICML
 Layer normalization,2016, In NIPS
 On the complexity of shallow and deep neural networkclassifiers,2014, In ESANN
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In ICML
 Understanding the difficulty of training deep feedforward neu-ral networks,2010, In AISTATS
 Deep pyramidal residual networks,2017, In CVPR
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In ICCV
 Identity mappings in deep residual net-works,2016, arXiv preprint arXiv:1603
 Orthogonal recurrent neural networks with scaledcayley transform,2018, In ICML
 Untersuchungen zu dynamischen neuronalen netzen,1991, Masterâ€™s thesis
 Batch normalization: Accelerating deeP network training by re-ducing internal covariate shift,2015, In ICML
 Self-normalizingneural networks,2017, In NIPS
 On the representationalefficiency of restricted boltzmann machines,2013, In NIPS
 Deep vs,2016, shallow networks: An approximation theoryperspective
 On the number of linearregions of deep neural networks,2014, In NIPS
 On the difficulty of training recurrent neuralnetworks,2013, In ICML
 Resurrecting the sigmoid in deep learningthrough dynamical isometry: theory and practice,2017, In NIPS
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In NIPS
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural networks,2014, ICLR
 Deep informationpropagation,2017, In ICLR
 Representation benefits of deep feedforward networks,2015, arXiv preprintarXiv:1509
 Deep mean field theory: Variance and width variation by layeras methods to control gradient explosion,2018, In ICLR workshop
 Mean field residual networks: On theedge of chaos,2017, In NIPS
 The advantages oforthogonal over Gaussian matrices have been documented by e,2017,g
