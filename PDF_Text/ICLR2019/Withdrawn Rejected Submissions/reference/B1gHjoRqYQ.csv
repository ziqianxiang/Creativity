title,year,conference
 Threat of adversarial attacks on deep learning in computer vision:A survey,2018, arXiv preprint arXiv:1801
 Towards evaluating the robustness of neural networks,2017, InSecurity and Privacy (SP)
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Black-box adversarial attacks withlimited queries and information,2018, arXiv preprint arXiv:1804
 Learning multiple layers of features from tiny images,2009, 2009
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 DeepFool: a simple andaccurate method to fool deep neural networks,2016, In Proceedings of 2016 IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 cleverhans v2,2017,0
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia Conference on Computer and Communications Security
 One pixel attack for fooling deepneural networks,2017, arXiv preprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Distributionally adversarial attack,2018, arXiv preprintarXiv:1808
 This concludes the proof,2019, â–¡Theorem 2
