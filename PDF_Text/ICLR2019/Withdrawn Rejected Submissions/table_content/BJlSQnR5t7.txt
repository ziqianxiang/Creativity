Table 1: Datasets statisticsnetwork via stochastic gradient descent. In the case of multiple kernels, k∖,...,kι, l Deepstromlayers can be computed in parallel then merged to encode the information provided by the differentkernel representations. Learning the weights W1, . . . , Wl in this case is, in a way, related to multiplekernel learning. Alternatively one may exploit a Deepstrom layer on top of each of the output featuremap by the convolutional part conv and concatenate these as input to a classification layer, we callthis a multiple Deepstrom architecture hereafter.
Table 2: Classification accuracy of Dense layers architectures (D); Adaptive DeepFried (ADF),Adaptive Deepstrom with linear (ADSL), RBF (ADSR), Chi2 (ADSC) kernels, on small trainingsets with 5 and 20 training samples per class. Variance of results, computed on 30 runs, are given inbrackets.
Table 3: Multiple Deepstrom experiments on CIFAR100 obtained on top of VGG19 convolutions.
