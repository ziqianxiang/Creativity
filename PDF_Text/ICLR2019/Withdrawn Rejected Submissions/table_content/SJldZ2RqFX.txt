Table 1: Practical D-GAN proposed approach advantages.
Table 2: Comparative results of Accuracy prediction performances on MNIST in a One vs. Onemode. NP is the number of positive labeled samples, and NU is the number of unlabeled sampleswhich mixes all the rest of positive and negative samples. GenPU, uPU and nnPU results come from(Hou et al., 2018)._____________________________________________________________________One vs. One	∣	I	,3,vs.,5,	I				I	,8,vs.,3,			Dataset: MNIST	∣	D-GAN (proposed)	GenPU	nnPU	uPU	I D-GAN (proposed)	GenPU	nnPU	uPUNo need of prior knowledge ∣	X	×	×	×	I	X	×	×	×NP=100 : NU=9900 NP=50 : NU=9950	0.987 0.964	0.983 0.982	0.969 0.966	0.914 0.854	0.989 0.974	0.982 0.979	0.974 0.965	0.932 0.873Concerning the relatively more challenging One vs. Rest task, the D-GAN globally outperformsthe PGAN and RP methods in terms of F1-Score predictions performances on the table 3 both onMNIST and CIFAR-10 datasets.
Table 3: Comparative results of two-stage PU methods not using prior knowledge. These are the av-erage F1-Score prediction performances in a One vs. Rest mode on MNIST and CIFAR-10 with re-spectively 35 Divergent-GAN epochs on MNIST and 275 Divergent-WGAN-GP epochs on CIFAR-10. PGAN and RP results come from (Chiaroni et al., 2018). PNGAN represents a PN (PU whenπP = 0) training such that the negative samples are replaced by WGAN generated ones. Thishighlights the GANs data augmentation effect on complex datasets like CIFAR-10.
Table 4: Comparative results of F1-Score prediction performances on MNIST and CIFAR-10 in aOne vs. Rest mode with 20 classifier epochs.
