Table 1: Test set clustering error rate and standard deviation for MNIST data. Methods using dataaugmentation are denoted with *.
Table 2: Semi-supervised test set missclassification rate and standard deviation for SVHN data. ↑and ↑ denote similar encoder/CIaSSifier architectures.
Table 3: MNIST: Gz (x)Gy (x,Gz (x))Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationx	-	1	-	-	-	-	-y1	x	32	2	1	0.2	-	-z1	x	32	2	1	0.2	-	-y1a	y1 + z1	32	-	-	-	yes	Leak 0.2z1a	z1	32	-	-	-	yes	Leak 0.2y2	y1a	32	3	2	0.2	-	-z2	z1a	32	3	2	0.2	-	-y2a	y2 + z2	32	-	-	-	yes	Leak 0.2z2a	z2	32	-	-	-	yes	Leak 0.2y3	y2a	64	3	2	0.2	-	-z3	z2a	64	3	2	0.2	-	-y3a	y3 + z3	64	-	-	-	yes	Leak 0.2z3a	z3	64	-	-	-	yes	Leak 0.2y4	y3a	64	3	1	0.2	-	-z4	z3a	64	3	1	0.2	-	-y4a	y4 + z4	64	-	-	-	yes	Leak 0.2z4a	z4	64	-	-	-	yes	Leak 0.2y5	y4a	128	4	1	0.2	-	-
Table 4: MNIST: Gz (y)Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationy	-	k	-	-	-	-	-z1	y	64	1	1	-	yes	Leak 0.2z2	z1	64	1	1	0.2	yes	Leak 0.2zμ	z2	64	1	1	-	-	-zσ	zeros	64	-	-	-	-	ExpNz	zμ + zσ	64	-	-	-	-	-13Under review as a conference paper at ICLR 2019Table 5: MNIST: Gx (y,Gz (y))Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationy	-	k	-	-	-	-	-z	-	64	-	-	-	-	-y1	y	128	1	1	-	-	-z1	z	128	1	1	-	-	-y1a	y1 + z1	128	-	-	-	yes	-z1a	z1	128	-	-	-	yes	Leak 0.2y2	y1a	64	4	1	0.2	-	-z2	z1a	64	4	1	0.2	-	-
Table 5: MNIST: Gx (y,Gz (y))Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationy	-	k	-	-	-	-	-z	-	64	-	-	-	-	-y1	y	128	1	1	-	-	-z1	z	128	1	1	-	-	-y1a	y1 + z1	128	-	-	-	yes	-z1a	z1	128	-	-	-	yes	Leak 0.2y2	y1a	64	4	1	0.2	-	-z2	z1a	64	4	1	0.2	-	-y2a	y2 + z2	64	-	-	-	yes	-z2a	z2	64	-	-	-	yes	Leak 0.2y3	y2a	64	3	1	0.2	-	-z3	z2a	64	3	1	0.2	-	-y3a	y3 + z3	64	-	-	-	yes	-z3a	z3	64	-	-	-	yes	Leak 0.2y4	y3a	32	3	2	0.2	-	-z4	z3a	32	3	2	0.2	-	-y4a	y4 + z4	32	-	-	-	yes	-z4a	z4	32	-	-	-	yes	Leak 0.2
Table 6: MNIST: D(x,y,z)Name Input Channels Width Stride Dropout BatchNorm Activation2222 2 2 222-------------.................-.-.-...
Table 7: SVHN: Gz(x)Gy(x,Gz (x))Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationx	-	1	-	-	-	-	-x1	x	96	3	-	-	yes	Leak 0.2x2	x1	96	3	-	-	yes	Leak 0.2x3	x2	96	3	Max2	-	yes	Leak 0.2x4	x3	192	3	-	-	yes	Leak 0.2x5	x4	192	3	-	-	yes	Leak 0.2x6	x5	192	3	Max2	-	yes	Leak 0.2x7	x6	384	3	-	-	yes	Leak 0.2y1	x7	192	1	-	-	-	-z1	x7	192	1	-	-	-	-y1a	y1 + z1	192	-	-	-	yes	Leak 0.2z1a	z1	192	-	-	-	yes	Leak 0.2y2	y1a	96	1	-	-	-	-z2	z1a	96	1	-	-	-	-y2a	y2 + z2	96	Avg6	-	-	yes	Leak 0.2z2a	z2	96	Avg6	-	-	yes	Leak 0.2yμ	y2a	k	1	1	-	-	-yσ	y2a	k	1	1	-	-	ExpN
Table 8: SVHN: Gz (y)Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationy	-	k	-	-	-	-	-zμ	y	64	1	1	-	-	-zσ	y	64	1	1	-	-	ExpNz	zμ + zσ	64	-	-	-	-	-15Under review as a conference paper at ICLR 2019Table 9: SVHN: Gx (y,Gz (y))Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationy	-	k	-	-	-	-	-z	-	64	-	-	-	-	-y1	y	512	1	1	-	-	-z1	z	512	1	1	-	-	-y1a	y1 + z1	512	-	-	-	yes	-z1a	z1	512	-	-	-	yes	Leak 0.2y2	y1a	256	4	1	-	-	-z2	z1a	256	4	1	-	-	-y2a	y2 + z2	256	-	-	-	yes	-z2a	z2	256	-	-	-	yes	Leak 0.2
Table 9: SVHN: Gx (y,Gz (y))Name	Input	Channels	Width	Stride	Dropout	BatchNorm	Activationy	-	k	-	-	-	-	-z	-	64	-	-	-	-	-y1	y	512	1	1	-	-	-z1	z	512	1	1	-	-	-y1a	y1 + z1	512	-	-	-	yes	-z1a	z1	512	-	-	-	yes	Leak 0.2y2	y1a	256	4	1	-	-	-z2	z1a	256	4	1	-	-	-y2a	y2 + z2	256	-	-	-	yes	-z2a	z2	256	-	-	-	yes	Leak 0.2y3	y2a	256	3	1	-	-	-z3	z2a	256	3	1	-	-	-y3a	y3 + z3	256	-	-	-	yes	-z3a	z3	256	-	-	-	yes	Leak 0.2y4	y3a	128	3	2	-	-	-z4	z3a	128	3	2	-	-	-y4a	y4 + z4	128	-	-	-	yes	-z4a	z4	128	-	-	-	yes	Leak 0.2
Table 10: SVHN: D(x,y,z)Name	Input	Channels	Width	Pooling	Dropout	BatchNorm	Activationx	-	1	-	-	-	-	-y	-	k	-	-	-	-	-z	-	64	-	-	-	-	-x1	x	96	3	-	-	-	Leak 0.2x2	x1	96	3	-	-	-	Leak 0.2x3	x2	96	3	Max2	-	-	Leak 0.2x4	x3	192	3	-	0.1	-	Leak 0.2x5	x4	192	3	-	-	-	Leak 0.2x6	x5	192	3	Max2	-	-	Leak 0.2x7	x6	384	3	-	0.1	-	Leak 0.2x8	x7	192	1	-	-	-	Leak 0.2x9	x8	96	1	Avg6	-	-	Leak 0.2y1	y	96	1	1	-	-	Leak 0.2y2	y1	96	1	1	0.1	-	Leak 0.2z1	z	96	1	1	-	-	Leak 0.2z2	z1	96	1	1	0.1	-	Leak 0.2p0	x5 | y2 | z2	288	-	-	-	-	-p1	p0	288	1	1	0.1	-	Leak 0.2
