Table 1: Performance comparison on Omniglot dataset.					MODEL	FINE TUNE	5-WAY		20-WAY			1-shot	5-shot	1-shot	5-shotConv Siamese Nets Koch et al. (2015)	N	96.7%	98.4%	88.0%	96.5%Conv Siamese Nets Koch et al. (2015)	Y	97.3%	98.4%	88.1%	97.0%Mann Santoro et al. (2016)	N	82.8%	94.9%	-	-Matching Nets Vinyals et al. (2016)	N	98.1%	98.9%	93.8%	98.5%Matching Nets Vinyals et al. (2016)	Y	97.9%	98.7%	93.5%	98.7%NEURAL STAT Edwards & Storkey (2016)	N	98.1%	99.5%	93.2%	98.1%ConvNet with Memory Kaiser et al. (2017)	N	98.4%	99.6%	95.0%	98.6%META NETS Munkhdalai & Yu (2017)	N	99.0%	-	97.0%	-Prototypical Nets Snell et al. (2017)	N	98.8%	99.7%	96.0%	98.9%MAML Finn et al. (2017)	Y	98.7±0.4%	99.9±0.1%	95.8±0.3%	98.9±0.2%META-SGD Li et al. (2017)	Y	99.5±0.3%	99.9±0.1%	95.9±0.4%	99.0±0.2%Learning2Compare Sung et al. (2017)	N	99.6±0.2%	99.8±0.1%	97.6±0.2%	99.1±0.1%Ours	N	99.8±0.1%	99.9±0.1%	98.2 ±0.1%	99.5±0.1%The detailed configuration of our networks is illustrated in Figure 3. ‘Conv‘ denotes a block of 3layers, namely convolution layer, batch normalization layer and ReLU layer. The associated numbersin the box are for the number of filters and kernel size respectively. The numbers on the right side ofeach box are the output feature map shape, which is interpreted as (number of channels, height, width).
Table 2: Performance comparison on miniImageNet dataset.					MODEL	FINE TUNE	5-WAY		20-WAY			1-shot	5-shot	1-shot	5-shotMETA-SGD	Y	50.5 ± 1.9%	64.0 ± 1.0%	17.6 ± 0.6%	29.0 ± 0.4%Matching Nets	N	43.6 ± 0.8%	55.3 ± 0.7%	17.3 ± 0.2%	22.7 ± 0.2%META LSTMRavi & Larochelle (2016)	N	43.4 ± 0.8%	60.6 ± 0.7%	16.7 ± 0.2%	26.1 ± 0.3%Maml	Y	48.7 ± 1.8%	63.1 ± 1.0%	16.5 ± 0.6%	19.3 ± 0.3%Meta Nets	N	49.2 ± 0.9%	-	-	-Prototypical Nets	N	49.4 ± 0.8%	68.2 ± 0.7%	-	-Learning2Compare	N	51.4 ± 0.8%	67.1 ± 0.7%	-	-TCML Mishra et al. (2017)	N	55.7 ± 1.0%	68.9 ± 0.9%	-	-Learning2Compare Deep	N	50.4 ± 0.8%	65.3 ± 0.7%	-	-Ours	N	59.0 ± 1.0%	70.9± 0.5%	22.2 ± 0.3%	32.2± 0.2%5	DiscussionIn this paper, we exploit the object-level relation to infer the image relation. In particular, we considereach ‘pixel’ on the feature map as an object in the input image, and use the values across all channelsas the object feature. In fact, one ‘pixel’ corresponds to one patch of the original image. Smallpatches may not contain any objects, while in big patches, there could be multiple objects. In theextreme case where the feature map size is 1x1, the corresponding patch is the whole image. Ourmodel is then equivalent to LearningToCompare Sung et al. (2017). In fact, we capture object pairs
