Table 1: Overall network layout. B is the number of blocks at each stage. At the first block of eachstage except the first stage down-sampling is performed and the channel number is doubled.
Table 2: Comparisons to illustrate the relationship between the information field and the modelaccuracy. We tune the number of group to achieve different parameter efficiency. Width here is thenumber of input channels to the first stage in the network. InfoSize is the size of information fieldwith regards to the input to the first stage. Numbers within the parentheses represent the numberof groups. For example, GConv(1) means group convolution with only 1 group, which is also thestandard convolution.
Table 3: Comparisons of different sparse kernel designs. All designs share the same network layout.
Table 4: Comparisons with different state-of-the-art sparse kernel designs. All settings are restoredfrom the original papers. Specifically, bottleneck ratio is 1 : 4 for ResNet and ResNeXt adoptscardinality of 16 and bottleneck ratio of 1 : 2. Meanwhile 4 groups are used for ShuffleNet.
