Table 1: One-shot learning on MNIST and CIFAR10 datasets compared to matching networks.
Table 2: One-shot learning on Omniglot datasetModel	5-wa 1-shot	y Acc 5-shotMatching Networks (no FCE) Vinyals et al. (2016)	42.4%	58.0%Matching Networks (FCE) Vinyals et al. (2016)	46.6%	60.0%Prototypical Networks Snell et al. (2017)	49.42 ± 0.78%	68.20 ± 0.66%Meta Networks MUnkhdalai & YU (2017)	49.21 ± 0.96%	-MAML Finn et al. (2017)	48.70 ± 1.84%	63.15 ± 0.91%TCML MishraetaL(2017)	55.71 ± 0.99%	68.88 ± 0.92%ABM Networks (OUrs)	48.92 ± 0.69%	57.98 ± 0.77%ABM + Self Reg	52.54 ± 0.70%	63.05 ± 0.72%ABM + Self Reg (layers 4-6)	53.47 ± 0.67%	61.82 ± 0.73%Table 3: One-shot learning on MiniImageNet dataset compared to matching networks.
Table 3: One-shot learning on MiniImageNet dataset compared to matching networks.
Table 4: Open set recognition in a one-shot learning setup on MNIST dataset. Of the N classes,N - 1 have reference with the last one being the open set. Matching via alignment yields bettergeneralization accuracies and F1 scoresWe train the model using the one-shot open-set simulation strategy. Hence, during testing, all the 5classes reference images are sampled from are previously unseen classes during training, and one ofthese new classes is selected at random to have belong to the open set (is held out from the one-shotepisode).
Table 5: Open set recognition in a one-shot learning setup on Omniglot dataset. Of the N classes,N - 1 have reference with the last one being the open set. Matching via alignment yields bettergeneralization accuracies and F1 scoresAs a baseline, we use Matching Networks with the learned distance threshold below which the testimage needs to match a reference image to not be considered as part of the open set. We then test theone-shot open-set recognition problem on ABM networks both with and without self regularization.
