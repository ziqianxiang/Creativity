Table 1: Pruning one layer in LeNet-5 on MNIST dataset (Top-1 Accuracy).
Table 2: Pruning two layers in LeNet-5 on the MNIST dataset. Each column indicates the percentageof parameter saved on these two layers. Com-Rat, is short for the compression ratio of the totalnetwork, i.e., the ratio of saved parameters divided the total number of parameters of LeNet-5.
Table 3: Pruning each block in ResNet-18 on Cifar-10 dataset. Note that each block has two CNNlayers.
Table 4: Pruning multiple blocks in ResNet-18 on Cifar-10 dataset. (Chance-level = 10%). Com-Rat, is short for compression ratio of the total network, as in Tab. 2.
Table 5: Top 5 accuracy on miniImagenet by pruning ResNet-18, the fully connected layer,Block#4.0 and #4.1 layers.
Table 6: Pruning one layer in LeNet-5 on MNIST dataset.
Table 7: Pruning one block in ResNet-18 on Cifar-10 dataset.
