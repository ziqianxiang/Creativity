Table 1: Number of parameters and operations required by a compressed convolutional layer byvarious types of tensor decompositions (for the special case of X = Y = X' = Y' = D, S = T =N , H = W = k and D ≫ k). General settings are summarized in Tables 10 and 12.
Table 2: Parallel time complexity of forward pass using various types of tensor decompositions onconvolutional layers. The uncompressed parallel complexity of forward pass is O(k2N).
Table 3: Percentage test accuracy of baseline PD with E2E tuning vs. our RTD with Seq tuning onCIFAR10. The uncompressed ResNet-32 achieves 93.2% accuracy with 0.46M parameters.
Table 4: Percentage accuracy of our Seq vs. baseline E2E tuning using PD on CIFAR10.
Table 5: Percentage accuracy of our RTD vs. baseline PD using Seq tuning on CIFAR10.
Table 6: Convergence of percentage accuracies of uncompressed vs. PD (TT decomposition) vs.
Table 7: Reshaped tensor decomposition combined with sequential for fully-connected layers onMNIST. The uncompressed network achieves 99.31% accuracy.
Table 8: Summary of tensor operations. In this table, X ∈ RI0××1mτ, γ∈ RJ0×∙∙∙× Jn—1 andmatrix M ∈ RIk×J. Mode-(k, l) tensor contraction and mode-(k, l) tensor partial-outer product arelegal only if Ik = Jl. T(0) is an (m + n - 2)-order tensor, T(1) is an m-order tensor, T(2) is an(m + n - 1)-order tensor and T(3) is an (m + n - 1)-order tensor.
Table 9: Summary of tensor decompositions. In this table, we summarize three types of tensordecompositions in tensor notations, and list their numbers of parameters and time complexities tobackpropagate the gradient of a tensor T ∈ RI0 × I1 × Im -1 to its m factors (and an additional coretensor C for Tucker decomposition). For simplicity, we assume all dimensions Il’s of T are equal,and denote the size of T as the product of all dimensions I = ∏m-1 Il. Furthermore, We assumeall ranks Rl’s (in Tucker and Tensor-train decompositions) share the same number R.
Table 10: Summary of plain tensor decomposition on convolutional layer. We list the nUmberof parameters and the nUmber of operations reqUired by forward/backward passes for varioUs plaintensor decomposition on convolUtional layer. For reference, a standard convolUtional layer maps aset of S featUre maps with height X and width Y, to another set of T featUre maps with height X 'and width Y'. All filters in the convolUtional layer share the same height H and width W.
Table 11: Summary of reshaped tensor decomposition on dense layer. In this table, we listthe numbers of parameters and time complexities of forward/backward passes required by variousreshaped tensor decomposition s on dense layer. For simplicity, we assume that the number of inputunits S and outputs units T are factorized evenly, i.e. Sl = Smm, Tl = Tm1, ∀l ∈ [m] and all ranks(in r-TK and r-TT) share the same number R, i.e. Rl = R, ∀l ∈ [m].
Table 12: Summary of reshaped tensor decomposition on convolutional layer. In this table, welist the number of parameters and time complexities required by various reshaped tensor decompo-sition s on convolutional layer. Recall that a convolutional layer, composed with ST filters of sizeH X W, maps a set ofS feature maps of size X X Y to another set ofT feature maps of size X' X Y'.
