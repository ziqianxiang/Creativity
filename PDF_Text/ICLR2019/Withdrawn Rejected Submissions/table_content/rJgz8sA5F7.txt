Table 2: Experimental results of H-Net				PackNet	99.38 HCNet	99.41	91.93 89.36	66.34 74.93	85.88 87.90on the basic datasets			HC-Net (without H-Net)	99.41		91.84	75.02	88.76Table 3: Mean classification results on the basic datasets(5 runs).
Table 3: Mean classification results on the basic datasets(5 runs).
Table 4: Mean classification results for the realistic dataset (2 runs)Initialization	Old(%)	NeW(%)	Avg.(%)Random	99.42	91.67	95.55Pretrained parameters	99.43	92.37	95.90Table 5: Performances of the old (MNIST) and new (SVHN) tasks with different initialization meth-ods: 1) initialization with only random Gaussian noise and 2) the pretrained parameter added byrandom Gaussian noiseare referred to as ImageNet-A and ImageNet-B respectively in this paper. To show the adaptabilityof our method on structures having shortcut connections, ResNet-50 is used for the experiment. Weuse the same experimental setting as in (He et al., 2016) with a batch size of 128. We compare ourC-Net with LwF and PackNet.
Table 5: Performances of the old (MNIST) and new (SVHN) tasks with different initialization meth-ods: 1) initialization with only random Gaussian noise and 2) the pretrained parameter added byrandom Gaussian noiseare referred to as ImageNet-A and ImageNet-B respectively in this paper. To show the adaptabilityof our method on structures having shortcut connections, ResNet-50 is used for the experiment. Weuse the same experimental setting as in (He et al., 2016) with a batch size of 128. We compare ourC-Net with LwF and PackNet.
Table 6: The performances on the new taskwhile increasing the filter index of the model us-ing parameter sharing and the model with fixedrandom parameters.
