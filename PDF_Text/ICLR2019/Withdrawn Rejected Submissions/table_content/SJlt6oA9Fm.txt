Table 1: Comparison of performances on CIFAR-10/100 datasets in terms of their total parameterusage in R-parts of the models, and their classification error rates. Here, “S” denotes whether SCUis used, and “D”, “R” denote the use of dealloc and realloc, respectively. We indicate k bythe growth rate of DenseNet. All the values in the table are taken from averaging over 5 trials.
Table 2: Comparison of model performances onImageNet classification dataset. Here, we mea-sure the single-crop validation error rates. “S”denotes whether the model uses SCU or not,and “D”, “R” denote the use of dealloc andrealloc, respectively.
Table 3: Comparison of performance on CIFAR-10 between different CNN models includingours: CondenseNet-SCU-182. Models named “X-Pruned” are the results by Liu et al. (2017).
Table 4: Configurations.
Table 5: Listing of definition for each architecture block used in our experiments. Here, BRCIK→×IK0denotes ConvIK→×IK0 ◦ ReLU ◦ BN, GConvIK→×IK0 denotes a group convolution with 4 groups withkernel size K × K , and LGC denotes the learned group convolution layer originally proposed byHuang et al. (2017a).
Table 6: Listing of N values used for each model, with respect to the dataset that each model istrained. For ImageNet models, we use different values of N for each stage.
Table 7: The generic model configurations used in our experiments with CIFAR-10/100 and Im-ageNet datasets. Here, AvgPool and MaxPool denotes the average pooling and the max poolinglayer with kernel size 2 × 2 of stride 2, respectively. GAvgPool indicates the global average poolinglayer, and FullyConnected indicates a fully-connected layer. Unless otherwise specified, the strideof the other operations are set to 1.
