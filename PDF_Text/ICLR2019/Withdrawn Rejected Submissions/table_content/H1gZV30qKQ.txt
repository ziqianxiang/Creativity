Table 1: The Optimal Value and Policy in the Source Domain and the Target Domain (Left: SourceDomain; Right: Target Domain)Appendix C Implementation DetailsWe use feed forward networks for all models and the activation function is ReLU. For transitionnetwork, reward network, and termination network, the networks have 2 hidden layers of size 512.
Table 2: Environment modificationIn transferring experiments, we select the agents get similar scores in the original environment forfair comparison. Table 3 shows the scores of the agents we chose in different environments. To getthese scores, we evaluate each agent 10 times and calculate the mean episode reward.
Table 3: Scores in the original environments.
