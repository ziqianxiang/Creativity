Table 1: SUNet architectures for ImageNet. N denotes the number of filters per convolutional layer.
Table 2: Error rates for classification networks on the Ima-geNet 2012 validation set. 0f0 denotes error rates from theofficial PyTorch implementation.
Table 4: Performance com-parison of multigrid dilationagainst strided convolutioninside each u-net module,using the SUNet-7-128model and evaluated us-ing mean IoU. OS denotesoutput_Stride during train-ing.
Table 5: Performance comparison at various output_Stride and infer-ence strategies. MS: Multi-scale, DL: with Degridding LayersMethods	mIoUPiecewise (VGG16) (Lin et al., 2016)	78.0LRR+CRF (Ghiasi & Fowlkes, 2016)	77.3DeepLabv2+CRF (Chen et al., 2016a)	79.7Large-Kernel+CRF (Peng et al., 2017)	82.2Deep Layer Cascade* (Li et al., 2017)	82.7Understanding Conv (Wang et al., 2017)	83.1RefineNet (Lin et al., 2017a)	82.4RefineNet-ResNet152 (Lin et al., 2017a)	83.4PSPNet Zhao et al. (2017) SUNet-7-128 (OS = 16)	85.4 84.3Table 6: Performance comparison on PASCALVOC 2012 test set. For fair comparison, onlythe methods pre-trained using MS-COCO are dis-played. PSPNet uses 82% more parameters.
Table 6: Performance comparison on PASCALVOC 2012 test set. For fair comparison, onlythe methods pre-trained using MS-COCO are dis-played. PSPNet uses 82% more parameters.
Table 7: Performance comparison on CityscapesteSt set. All methods were trained only using the“fine” set. All nets utilize ResNet-101 as a basenetwork, except if specified or marked with *.
