Table 1: Comparison of premature performance and representational similarity measure in architec-ture search using RL and TPE algorithms. P: premature performance as validation score; P+TG:combined premature performance and RDMs as the validation score. Values are μ ± σ across 3search runs.
Table 2: Performance of discovered cells on CIFAR10 and CIFAR100 datasets. *indicates error ratesfrom locally training the network using the same training pipeline on 2-GPUs. *we did not furtherexplore these hyperparameters because of compute limitations and adopted the values reported in(ZoPh et al.,2017).________________________________________________________________________Network	B	N	F	# Params	C10Error	C100 Error	Mi	E1	M2	E2	CostAmoebaNet-A	5	6	36	3.2M	3.34	-	20000	1.13M	100	27M	25.2BNASNet-A	5	6	32	3.3M	3.41 (3.72*)	17.88*	20000	0.9M	250	13.5M	21.4-29.3BPNASNet-5	5	3	48	3.2M	3.41 (4.06*)	19.26*	1160	0.9M	0	0	1.0BENAS	5	6	-	4.6M	3.54	-	310	50k	0	0	15.5MSAGENet	5	6t	32t	6.0M	3.66	17.42	1000	90K	10	13.5M	225MSAGENet-sep				2.7M	3.88	17.51					We compared our best found cell structure with those found using NAS (Zoph et al., 2017) andPNAS (Liu et al., 2017) methods on CIFAR-10, CIFAR-100, and Imagenet datasets (Tables 2 and3). To rule out any differences in performance that might have originated from slight differences intraining procedure, we used the same training pipeline to train our proposed network (SAGENet)as well as the as well as the two baselines (NASNet and PNASNet). We found that on all datasets,SAGENet performed on par with the other two baseline networks we had considered.
Table 3: Performance of discovered cells on Imagenet dataset in mobile settings.*indicates errorrates from training all networks using the same training pipeline on 2-GPUs.
