Table 1: Comparison of various methods on ActivityNet for 5, 10, 20 or 50 unseen classes. Theseresults are averaged over 10 trials where each trial has a different set of unseen activity classes.
Table 2: Results on HMDB51 and UCF101 compared to previous state-of-the-art results. We findthat learning a shared embedding space is beneficial and that augmented with unpaired data providesthe best results.
Table 3: Unseen activity recognition results on ActivityNet, HMDB51 and UCF101, evaluated byusing both unseen and seen classes for the testing.
Table 4: Comparison of various source of unpaired data on ActivityNet with 10 unseen classes.
Table 5: Comparison of unsupervised activityclassification on MLB-YouTube.
Table 7: Comparison of several models for unseen activity captioning using the ActivityNet dataset,using METEOR and CIDEr scores. This evaluation was done on 10 unseen classes held out duringtraining. Higher values are better.
Table 8: Comparison of temporal pooling methods for 5 unseen classes in the ActivityNet dataset.
Table 9: Comparison of different ratios of paired and unpaired data methods for 5 unseen classes inthe ActivityNet dataset.
Table 10: Comparison using 40k paired examples and varying amounts of unpaired samples for 5unseen classes in the ActivityNet dataset.
Table 11: Comparison of several models for standard, seen video captioning using the MLB-YouTubedataset, using Bleu, METEOR and CIDEr scores. Higher values are better.
Table 12: Comparison of various pronouns on the UCF101 dataset with 50 unseen classes.
