Table 1: Average slowness of 5 output features over 5 runs extracted by greedy layer-wise train-ing and gradient-based training from non-linearly distorted trigonometric polynomials. Results forthree-layer quadratic expansion network and neural network with tanh activation.
Table 2: A network with multiple quadratic expansions, each preceded by linear dimensionality-reduction. These kind of networks are typically used in closed-form SFA as they trade-off modelexpressivity with memory requirements.
Table 3: A simple multi-layer neural network using a tanh activation function to induce non-linearity.
