Table 1: Parameter settings for batch neural networks training	MNIST	CIFAR-10Neurons per layer	50	50Learning rate	0.01	0.001L2 regularization	10-6	10-5Minibatch size	32	32Epochs	10	10Weights initialization	N(0, 0.01)	N (0, 0.01)Bias initialization	0.1	0.14.1	Parameter settings for the baselinesWe first formally define the ensemble procedure. Let yj ∈ ∆K-1 denote probability distribu-tion over K classes output by neural network trained on data from batch j for some test inputx. Then uniform ensemble prediction is arg max J P J= y^. To define weighted ensemble, letknj,k denote number of examples of class k on batch j and nk = PjJ=1 nj,k denote total num-ber of examples of class k across all batches. Prediction of the weighted ensemble is as followsarg max n^ Pj=ι nj,kyj,k. This is a heuristic approach We defined to potentially better handlekkheterogeneous partitioning with ensemble.
