Table 1: Zero-shot capability of UG and OpenNMT representation from English training. For allother methods we trained on the available training data. Table shows error of sentiment model.
Table 2: Error in terms of accuracy for the following methods. For Unlexicalized features + Unigram+ Bigram features we trained on 200 out of the 400 Russian samples and tested on the other 200 asa baseline.
Table 3: Example of samples from UG-WGAN with λ = 0.0 and λ = 0.1between two languages. From the heatmap we see that English and Spanish are the most similarlanguages while Russian and Arabic are the most different, which aligns with intuition.
