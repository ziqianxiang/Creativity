Table 1: Various GAN methods can learn some, but not all, parts of our framework. These partsmay exist implicitly in each of the models, but their extraction is non-trivial.
Table 2: Table for all lossesLoss name	DetailIgi	EXi〜Pdata(Xi)[Di(Xi)] - EZi~Pz [Di(gi(Zi))]lc	Ey~Pdata(y) [Dc(Y)] - EZl~Pzι ,…,Zm~Pzm [Dc(C(OI,…,Om))]ld	Ezi~pzi ,…,zm~pzm[Df (OI,…，om)] - Ey〜Pdata(y)[Df(d(y))]Ic-Cyc	EZI 〜Pzi ,...,Zm-Pzm [ ikd(c(O1,...,Om))i-Oik1]ld-cyc	Ey-Pdata (y) [kc(d(y)) - yk1]Task 1: Given component generators gi, i	∈ {1,	.	.	. ,	m} and	c, train d.
Table 3: We show Frechet inception distance (Heusel et al., 2017) for generators trained usingdifferent datasets and methods. The “Foreground” column and “Foreground+background” reflectperformance of trained generators on generating corresponding images. WGAN-GP is trained onforeground and composed images. Generators evaluaged in the “By decomposition” row are ob-tained as described in Task 3 - on composed images, given background generator and compositionoperator. The information processing inequality guarantees that the resulting generator cannot beatthe WGAN-GP on clean foreground data. However, the composed images are better modeled usingthe learned foreground generator and known composition and background generator.
