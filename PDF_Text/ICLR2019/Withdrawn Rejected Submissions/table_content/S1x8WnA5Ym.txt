Table 1: Sample quality and degree of mode collapse on mixtures of Gaussians. GDPP-GAN consistentlycaptures the highest number of modes and produces better samples.
Table 2: GDPP loss Ablation study on GAN. Lsu is the same as Ls without min-max eigen value normalization7Under review as a conference paper at ICLR 2019Stacked-MNISTCIFAR-10#Modes(Max 1000) KLdiv. Inception score IvODCGAN (Radford et al., 2016)	427	3.163	5.26 ± 0.13	0.0911DeLiGAN (Gurumurthy et al., 2017)	767	1.249	5.68 ± 0.09	0.0896Unrolled-GAN (Metz et al., 2017)	817	1.430	5.43 ± 0.21	0.0898RegGAN (Che et al., 2017)	955	0.925	5.91 ± 0.08	0.0903WGAN (Arjovsky et al., 2017)	961	0.140	5.44 ± 0.06	0.0891WGAN-GP (Gulrajani et al. (2017))	995	0.148	6.27 ± 0.13	0.0891GDPP-GAN (Ours)	1000	0.135	6.58 ± 0.10	0.0883Table 3: Performance of various methods on real datasets. Stacked-MNIST is evaluated using the number ofcaptured modes (Mode Collapse) and KL-divergence between the generated class distribution and true classdistribution (Quality of generations). CIFAR-10 is evaluated by Inference-via-Optimization (Mode-Collapse)and Inception-Score (Quality of generations).
Table 3: Performance of various methods on real datasets. Stacked-MNIST is evaluated using the number ofcaptured modes (Mode Collapse) and KL-divergence between the generated class distribution and true classdistribution (Quality of generations). CIFAR-10 is evaluated by Inference-via-Optimization (Mode-Collapse)and Inception-Score (Quality of generations).
Table 4: Average and Minimum Sliced WassersteinDistance over the last 10K iterations.
Table 5: Performance on real datasets using the challenging experimental setting of (Srivastava et al., 2017).
Table 6: Average Iteration time for each of the baseline methods on CIFAR-10. GDPP-GAN obtains the closesttime to the default DCGAN.
Table 7: NDB/K - numbers of statistically different bins, with significance level of 0.05, divided by the numberof bins K (lower is better).
