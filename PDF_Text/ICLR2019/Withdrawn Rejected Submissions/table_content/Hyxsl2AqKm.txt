Table 1: Validation and test accuracy for the pure classification task (λ = 1), with different numbersof 2D-CNN and 3D-CNN features used for video encoding.
Table 2: Comparison of classification accuracy of fine-grained and coarse-grained models, tested onfine-grained actions (using action categories) versus coarse-grained actions (using action groups).
Table 3: Classification results on 174 action categories using VGG16 and ResNet152 as frameencoders, along with different strategies for temporal aggregation.
Table 4: Captioning baselines USing a Conventional encoder-decoder architectureVideo ID		81955	Action Group	Holding [something]Action Category	Holding [something] in front of [something]Somethings	“a blue plastic cap”，“a men,s short sleeve shirt”Simplified Somethings's	“cap”, “shirt”Simplified-object Caption	Holding cap in front of shirtFull Caption	Holding a blue plastic cap in front of a men short sleeve shirtTable 5: An example of annotation file for a Something-Something videoFigure 3: Captioning examples:Model Outputs: [Piling coins up], [Removing mug, revealing cup behind].
Table 5: An example of annotation file for a Something-Something videoFigure 3: Captioning examples:Model Outputs: [Piling coins up], [Removing mug, revealing cup behind].
Table 6: Performance of our two-channel models with different sizes of channel features on forsimplified objects. For this task we use (λ = 0.1). The maximum sequence length is 14.
Table 7: Comparing models trained with pure captioning task vs joint captioning and classification.
Table 8: Performance of captioning models with different sizes of channel features on full objectplaceholders. For this task we use (λ = 0.1). The maximum sequence length is 14.
Table 9: The 13 action categories represented in 20bn-kitchenware.
