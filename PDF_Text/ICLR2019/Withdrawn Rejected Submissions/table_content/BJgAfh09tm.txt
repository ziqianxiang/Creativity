Table 1: The *â€™ed experiments use a vocabulary size of 15k words. The Multi30k experiments usea vocabulary size of 6800 words.
Table 2: Generation BLEU scores for Text Generation on Europarl and Multi30k Datasetsand French. We can note that the English sentences have a higher BLEU score which could be a biasfrom our NMT. We can also note that lower BLEU scores for the Multi30k because of the smallertest size.
Table 3: Forward (F) and Reverse (R) perplexity (PPL) results for the Europarl and Multi30k datasetsusing synthetic sentences of maximum length 20 and 15 respectively. F-PPL: Perplexity of a lan-guage model trained on real data and evaluated on synthetic samples. R-PPL: Perplexity of a lan-guage model trained on the synthetic samples from Bilingual-GAN and evaluated on the real testdata.
Table 4: Translation BLEU score for translation quality on Europarl and Multi30k datasets measuredon 1000 generated sentences each. The reference set is approximated using Google TranslateEngHsh	J	FrenchEuroparl Supervisedthe vote will take place tomorrow at 12 noon tomorrow.	le vote aura lieu demain a 12 heures.
Table 5: Examples of aligned generated sentences4.5	Human EvaluationThe subjective judgments of the generated sentences of the models trained using the Europarl andthe Multi30k datasets with maximum sentence length of size 20 and 15 is reported in Table 6. Weused 25 random generated sentences from each model and give them to a group of 4 people. Weasked them to rate the sentences based on a 5-point Likert scale according to their fluency. Theraters are asked to score 1 which corresponds to gibberish, 3 corresponds to understandable butungrammatical, and 5 correspond to naturally constructed and understandable sentences (Semeniutaet al., 2018). From Table 6, we can note that the proposed Bilingual-GAN approach gets good rate.
Table 6: Human evaluation on the generated sentences by Bilingual-GAN using the Europarl andthe Multi30k dataset.
