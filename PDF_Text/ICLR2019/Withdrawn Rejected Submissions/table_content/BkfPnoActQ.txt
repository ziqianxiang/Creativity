Table 1: The table shows on which fraction of the tested games one approach performs at least aswell as the other. The scores used for the comparison are using the no-op starts regime. As describedin Sec. 4.2, we compare the agentsâ€™ scores to the scores obtained by an average human player and anexpert player. Ape-X DQfD (deeper) out-performs the average human on 40 of 42 games.
Table 2: The table shows the human-normalized performance of our algorithm and the baselines.
Table 3: The table compares the hyper parameters of Ape-X DQN (Horgan et al., 2018) and Ape-XDQfD. In addition to highlighting the differences, we explain the reason behind the change.
Table 4: Scores obtained by evaluating the best checkpoint for 200 episodes using the no-op startsregime.
Table 5: Scores obtained by evaluating the best checkpoint for 200 episodes using the human startsregime.
Table 6: The table shows the performance of our expert player and the amount of available demon-strations per game. Note that the total number of episodes/trajectories is very low.
Table 7: The table shows all of our hyper parameters.
