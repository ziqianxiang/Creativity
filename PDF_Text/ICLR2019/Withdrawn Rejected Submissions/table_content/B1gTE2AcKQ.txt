Table 1: Examples from 10 of the 1500 classes contained in the Text-Concepts dataset. The fulldataset can be found at [redacted for anonymous review]2	Background: Helmholtz Machines and Variational BayesSuppose we wish to learn generative models of spoken words unsupervised, using a large set ofaudio recordings. We may aim to include domain knowledge that words are built up from differentshort phonemes, without defining in advance exactly what the kinds of phoneme are, or exactlywhich phonemes occur in each recording. This means that, in order to learn a good model of wordsin general, we must also infer the particular latent phoneme sequence that generated each recording.
Table 2: Comparison of VAE and Wake-Sleep algorithms for training Helmholtz machines. Wake-sleep uses an approximation to the correct update for q, which may be heavily biased. VAE updatesare unbiased, but for discrete variables they are often too high variance for learning to succeed.
Table 3: The probabilistic regular expression ‘decoder' pθ(x|z), used in our model of text concepts8Under review as a conference paper at ICLR 2019Figure 6: Quantitative comparison of models trained on the Text-Concepts dataset.
Table 4: Learned latent representations and posterior predictive samples from models trained on theText-Concepts dataset (from five examples per class).
Table 5: Novel concepts hallucinated by the WSR model.
