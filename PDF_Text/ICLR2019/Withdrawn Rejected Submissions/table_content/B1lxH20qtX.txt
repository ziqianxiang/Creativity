Table 1: Testing generalization for the standing up task. We show quantitative evaluation of thegeneralization ability of the learned policies. For each of the methods, we first pick the best performingmodel from the training run and then evaluate it on each of the novel scenarios without any furtherfinetuning, i.e., in a zero-shot manner. We report first the score attained by the self-assembling agentand then report, in parenthesis, the percentage of training performance retained upon transfer. Thehigher the numbers, the better it is.
Table 2: Testing generalization for the standing up task in the presence of random push-n-pulls (i.e.
Table 3: Testing generalization for the locomotion task. The best performing model from the trainingis evaluated on each of the novel scenarios without any further finetuning. The score attained by theself-assembling agent is reported first and then, in parenthesis, the percentage of training performanceretained upon transfer.
Table 4: Testing generalization for the manipulation task. The score attained by the self-assemblingagent is reported first and then, in parenthesis, the percentage of training performance retained.
