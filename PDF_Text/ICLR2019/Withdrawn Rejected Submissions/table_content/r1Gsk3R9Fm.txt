Table 1: Single crop validation acc. on ImageNet.
Table 2: Results on CIFAR-10. Comparedto the few existing methods using onlylayerwise training schemes we report sub-stantial performance improvement. Over-all our models are competitive with wellknown benchmarks models that like oursdo not use skip connections.
Table 3: Network structure for k = 2, 3 imagenet models, not including auxiliary networks. Note aninvertible downsampling is applied on the input 224x224x3 image to producie the initial input. Thedefault auxillary networks for both have Mf = 2048 with 1 and 2 auxillary layers, respectively. Noteauxiliary networks always reduce the spatial resolution to 2x2 before the final linear layer.
Table 4: Network structure for k = 1 ImageNet models, not including auxiliary networks. Note aninvertible down-sampling is applied on the input 224x224x3 image to produce the initial input. Notethis network does not include any batch-norm.
Table 5: Comparison of different downsampling operationsModels	Number of ParametersOur model k = 3, Mf = 512	46MOur model k = 3	102MOur model k = 2	64MOur model k = 1,J = 8	406MOur model k = 1,J = 6	96MAIexNet	60MVGG-16	ä¸€	138MTable 6: Overall parameter counts for imagenet models trained in Sec. 4 and from literature.
Table 6: Overall parameter counts for imagenet models trained in Sec. 4 and from literature.
Table 7: For k = 3 model we report the effect of width. We compare halving the size of the modeland the accuracy at each layer we report the accuracy of the auxiliary model.
Table 8: Accuracy obtained by a linear model using the features of the k = 1 network at a given layeron the Caltech-101 dataset. We also give the reference accuracy without transfer.
