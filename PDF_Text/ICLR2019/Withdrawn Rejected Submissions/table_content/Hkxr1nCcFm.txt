Table 1: (a) Performance comparison in Gridworld, size 32x32, after 10M environment steps. VIN(Tamar et al., 2016) and experimental setup detailed in Appendix. (b) Comparison of test perfor-mance on (unfiltered) Sokoban levels for various methods. I2A (RacaniBre et al., 2017) results arere-rerun within our framework. ATreeC (Farquhar et al., 2017) results are detailed in Appendix.
Table 2: Best results obtained on the test set for the different difficulty levels across RL agents within2e9 steps of training (averaged across 5 independent runs).
Table 3: Number of levels in each subset of the dataset and number of overlaps between them.
Table 4: Number of parameters for various modelsD.2 RL training setupWe use the V-trace actor-critic algorithm described by Espeholt et al. (2018), with 4 GPUs for eachlearner and 200 actors generating trajectories. We reduce variance and improve stability by usingλ-returns targets (λ = 0.97) and a smaller discount factor (γ = 0.97). This marginally reduces themaximum performance observed, but increases the stability and average performance across runs,allowing better comparisons. For all experiments, we use a BPTT (Backpropagation Through Time)unroll of length 20 and a batch size of 32. We use the Adam optimizer (Kingma & Ba, 2014). Thelearning rate is initialized to 4e-4 and is annealed to 0 over 1.5e9 environment steps with polynomialannealing. The other Adam optimizer parameters are β1 = 0.9, β2 = 0.999, =1e-4. The entropyand baseline loss weights are set to 0.01 and 0.5 respectively. We also apply a L2 norm cost with aweight of 1e-3 on the logits, and a L2 regularization cost with a weight of 1e-5 to the linear layersthat compute the baseline value and logits.
