Table 1: RMSE error of the teachermodels on all samples, compared tothe error on samples sampled fromregions where the predicted featureshapes “agree” or “disagree” with theground truth shape.
Table 2: Performance of neural net teachers. ForRMSE, lower is better. For AUC, higher is better.
Table 3: Accuracy and fidelity of global additive explanations for 2H neural nets. Accuracy is interms of RMSE for regression tasks and AUROC for classification tasks; fidelity is always RMSEbetween the student’s predictions and the teacher’s scores or logits (equation 2). Results for 1H-8neural nets in Appendix.
Table 4: Quantitative results from user study. Since SAT-2, DT-2, and SPARSE only had twofeatures, the task to rank five features does not apply. Since the data error only appeared in theoutput of SAT-5, DT-4, and S-RULES, the other subjects could not have caught the error.
Table A1: Accuracy and fidelity of global explanation models across 1H and 2H teacher neural netsand datasets. Table 3 is a subset of this table with only 2H neural nets.
