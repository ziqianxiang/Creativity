Table 1: BLEU scores with different β on English-German dev setModel I Baseline ∣ β = 1 ∣ β = 0.5 ∣ β = 0BLEU I^^20.11 ^^I 20.41 ∣ 20.94 ∣ 21.32Figure 2: BLEU scores with different beam sizes on English-German dev set(β = 0).
Table 2: Performance on English-GermantaskModel	Method	BLEURNNsearch-LV-	ML+beam	19.40LoCal-Attn	ML+beam	20.90MRT	MRT+beam	20.45Baseline-SMT	MERT+greedy	18.83	MERT+beam	19.91Baseline-NMT	ML+greedy	20.89	ML+beam	22.13	ML+deep	24.64DSD-NMT	DSD+greedy	22.02++	DSD+beam	22.60+	DSD+deep	25.00++Table 3: Performance on English-French taskModel	Method	BLEURNNsearch-LV	ML+beam	34.60MRT	MRT+beam	34.23Bahdanau-LL	ML+greedy ML+beam	29.33 30.71Bahdanau-AC+LL	ML+AC+greedy ML+AC+beam	30.85 31.13
Table 3: Performance on English-French taskModel	Method	BLEURNNsearch-LV	ML+beam	34.60MRT	MRT+beam	34.23Bahdanau-LL	ML+greedy ML+beam	29.33 30.71Bahdanau-AC+LL	ML+AC+greedy ML+AC+beam	30.85 31.13Baseline-SMT	MERT+greedy^^ MERT+beam	31.55 33.82Baseline-NMT	ML+greedy ML+beam	32.10 34.70DSD-NMT	DSD+greedy DSD+beam	33.56++ 35.04+4.6 Results on Test SetsThe results about English-German and English-French translation tasks on test sets are reported inTables 2 and 3.3 For previous work, the best BLEU scores of single models are listed from theoriginal papers. From Tables 2 and 3, we see that DSD-NMT outperforms all the other models andour own baselines using standard cross entropy loss with both greedy search and beam search.
