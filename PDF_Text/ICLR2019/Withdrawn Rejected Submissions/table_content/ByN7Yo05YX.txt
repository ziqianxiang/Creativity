Table 1: Comparison of tree-structured NNs. The first column denotes if each path on the tree is aNN, and the second column denotes if the routers learn features from data. The last column indicatesif the method grows an architecture, or uses a pre-specified one.
Table 2: Primitive module specification for ANTs. The 1st & 2nd rows describe modules for MNISTand CIFAR-10. “conv5-40” denotes a 2D convolution with 40 kernels of spatial size 5 × 5. “GAP”,“FC” and “LC” stand for global-average-pooling, fully connected layer and linear classifier, respec-tively. “Downsample Freq” denotes the frequency at which 2 × 2 max-pooling is applied.
Table 3: Comparison of performance of different models on MNIST and CIFAR-10. The columns“Error (Full)” and “Error (Path)” indicate the classification error of predictions based on the fulldistribution and the single-path inference. The columns “Params. (Full)” and “Params. (Path)”respectively show the total number of parameters in the model and the average number of parametersutilised during single-path inference. “Ensemble Size” indicates the size of ensemble used to attainthe reported accuracy. An entry of “-” indicates that no value was reported. Methods marked with 1are from our implementations trained in the same experimental setup. * indicates that the parametersare initialised with a pre-trained CNN.
Table 4: Ablation study to Compare the effeCts of different Components of ANTs on ClassifiCationperformanCe. “CNN” refers to the Case Where the ANT is groWn Without routers While “SDT/HME”refers to the Case Where transformer modules on the edges are disabled.
Table 5: Training time comparison. Time and number of epochs taken for the growth and refinementphase are shown. along with the time required to train the baseline, All-CNN (Springenberg et al.,2015).
Table 6: Comparison of classification performance between the default single-path inference schemeand the prediction based on the least likely expert. between theModule Spec.	Error % (Selected path)	Error % (Least likely path)ANT-MNIST-A-	069	86Γ8ANT-MNIST-B	O.73	81.98ANT-MNIST-C	1.68	98.84ANT-CIFARIO-A	8.32	74.28ANT-CIFARIO-B	9.18	89.74ANT-CIFARIO-C	9.34	97.5217Under review as a conference paper at ICLR 2019G	Visualisation of discovered architecturesFig. 5 shows ANT architectures discovered on the MNIST (i-iii) and CIFAR-10 (iv-vi) datasets. Weobserve two notable trends. Firstly, most architectures learn a few levels of features before resortingto primarily splits. However, over half of the architectures (ii-v) still learn further representationsbeyond the first split. Secondly, all architectures are unbalanced. This reflects the fact that somegroups of samples may be easier to classify than others. This property is reflected by traditional DTalgorithms, but not “neural” tree-structured models that stick to pre-specified architectures (Laptev& Buhmann, 2014; Frosst & Hinton, 2017; Kontschieder et al., 2015; Ioannou et al., 2016).
Table 7: Comparison of performance of different models on SARCOS. The columns “Error (Full)”and “Error (Path)” indicate the mean squared error of predictions based on the full distribution andthe single-path inference. The columns “Params. (Full)” and “Params. (Path)” respectively showthe total number of parameters in the model and the average number of parameters utilised duringsingle-path inference. “Ensemble Size” indicates the size of ensemble used to attain the reportedaccuracy. Results from Zhao et al. (2017) are included as a reference value from prior work, but arenot directly comparable as they hold out 30% of the training examples as a validation set.
Table 8: Ablation stUdy to compare the effects of different components of ANTs on regressionperformance. “NN” refers to the case where the ANT is grown withoUt roUters while “SDT/HME”refers to the case where transformer modUles on the edges are disabled.
Table 9: Comparison of prediction errors of a single ANT versus an ensemble of 8, with predictionsaveraged over all ANTs in the ensemble.
Table 10: Parameter counts for a single ANT versus an ensemble of 8.
