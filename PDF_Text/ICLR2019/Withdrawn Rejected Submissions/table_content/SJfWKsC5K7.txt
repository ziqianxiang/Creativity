Table 1: Entropy of contribution distributions estimated by the explainer. A lower entropy of contri-bution distributions reflects more significant bias-interpreting. Our method suffered much less fromthe bias-interpreting problem than the baseline. Please see the appendix for more results.
Table 2: Classification accuracy and relative deviations of the explainer and the performer. We usedrelative deviations and the decrease of the classification accuracy to measure the information thatcould not be explained by pre-defined visual concepts. Please see the appendix for more results.
Table 3: Entropy of contribution distributions. The entropy of contribution distributions reflects thelevel of bias-interpreting. The lower entropy indicates a larger bias. Our method suffered much lessfrom the bias-interpreting problem than the baseline.
Table 4: Classification accuracy of the explainer and the performer. We use the the classification ac-curacy to measure the information loss when using an explainer to interpret the performer. Note thatthe additional loss for bias-interpreting successfully overcame the bias-interpreting problem, but didnot decrease the classification accuracy of the explainer. Another interesting finding of this researchis that sometimes, the explainer even outperformed the performer in classification. A similar phe-nomenon has been reported in (Furlanello et al., 2018). A possible explanation for this phenomenonis given as follows. When the student network in knowledge distillation had sufficient representationpower, the student network might learn better representations than the teacher network, because thedistillation process removed abnormal middle-layer features corresponding to irregular samples andmaintained common features, so as to boost the robustness of the student network.
Table 5: Relative deviations of the explainer. The additional loss for bias-interpreting successfullyovercame the bias-interpreting problem and just increased a bit (ignorable) relative deviation of theexplainer.
