Table 1: Avg. feature count (p) in testsamples used to evaluate Mah´'s expla-nations of target models. f Superpixelsare used to reduce the dimensionalityof images (§4.1). ffFor Transformer, pand the dataset (Merity et al., 2016) arebased on experimental design (§5.3).
Table 2: Data generating functionswith interactionsF1 (x) =	10xιX2 + PI=3	xiF2 (x) =	,∖-^∙10 x1x2 + ∑i=3	xiF3 (x) =	eχp(IxI + x2|)+ PI=3	xiF4 (x) =	IOX1X2X3 + PI=4	xi2018). Similar to Hooker (2007), we evaluate interactions in a subset region of a synthetic functiondomain. We generate synthetic data using functions F1 - F4 (Table 2) with continuous featuresuniformly distributed between -1 to 1, train complex base models (as specified in §5.1) on thisdata, and run different local interaction interpretation methods on 10 trials of 20 data instances atrandomly sampled locations on the synthetic function domain. Between trials, base models with dif-ferent random initializations are trained to evaluate the stability of each interpretation method. Weevaluate how well each method fits to interactions by first assuming the true interacting variablesare known, then computing the Mean Squared Error (MSE) between the predicted interaction attri-bution of each interpretation method and the ground truth at 1000 uniformly drawn locations withinthe local vicinity of a data instance, averaged over all randomly sampled data instances and trials(Figure 3a). We also evaluate the interaction detection performance of each method by comparingthe average R-precision (Manning et al., 2008) of their interaction rankings across the same sampleddata instances (Figure 3b). R-precision is the percentage of the top-R items in a ranking that arecorrect out of R, the number of correct items. Since F1 - F4 only ever have 1 ground truth interac-
Table 3: Average prediction performance (lower is better; 1-AUC for Transformer, MSE otherwise)with (K > 0) and without (K = 0) interactions for random data instances in the test sets ofrespective base models. Only results with detected interactions are shown. For each model, atleast 80% of all tested data instances possessed interactions, yielding ≥ 320 instances for eachPerformance Statistic. InclUding interactions results in SignficantPerformance improvements.
Table 5: Examples of En.-Fr. translations before and aftermodifying Transformer. Interacting elements are bolded.
Table 4: cet interactions before and af-ter modifying Transformer. Ns is num-ber of samples, and %cet is % of Nssamples + or - contributing to cet.
Table 6: Examples of context-dependent hierarchical explanations on Sentiment-LSTM. The inter-action attribution of g!K(∙) is shown at each K - 1 level, K ≥ 1 (§4.1) in color. Green meanspositively contributing to sentiment, and red the opposite. Visualized attributions of linear LIMEand Mahe are normalized to the max attribution magnitudes (max magn.) shown. Top-5 attributionsby magnitude are shown for LIME.
Table 7: Examples of context-dependent hierarchical explanations on Transformer. The interactionattribution of gK(∙) is shown at each K - 1 level, K ≥ 1 (§4.1) in color. Green contributes to-wards cet translations, and red contributes the opposite. Visualized attributions of linear LIME andMah´ are normalized to the max attribution magnitudes (max magn.) shown. Top-5 attributions bymagnitude are shown for LIME.
Table 8: Shown below are more examples of interactions with consistent polarities found by Mahein Sentiment-LSTM. Num samples is the number of sentences from which the same interaction isfound. Percent polarity is the percentage of interactions that have the specified attribution polarity.
