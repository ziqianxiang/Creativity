Table 1: Comparison of semi-supervised learning between our DSGAN and other state-of-the-artresults. For fair comparison, We only consider the GAN-based methods. * indicates the use of thesame architecture of classifier. f indicates a larger architecture of classifier. ∣ indicates the use ofdata augmentation.
Table 2: Comparing the semi-supervised learning results on MNIST whether to use the samplingtricks.
Table 3: Hyperparameters in semi-supervised learning.
Table 4: Network architectures for semi-supervised learning on MNIST. (GN: Gaussian noise)Generator G	Discriminator D	Classifier CInput z ∈ R100 from unif(0, 1)	Input 28 × 28 gray image	Input 28 × 28 gray image100 × 500 FC layer with BN Softplus 500 × 500 FC layer with BN Softplus 500 × 784 FC layer with WN Sigmoid	250 × 400 FC layer ReLU 400 × 200 FC layer ReLU 200 × 100 FC layer ReLU 100 × 1 FC layer	GN, std = 0.3 784 × 1000 FC layer with WN ,ReLU GN, std = 0.5 1000 × 500 FC layer with WN, ReLU GN, std = 0.5 500 × 250 FC layer with WN, ReLU GN, std = 0.5 250 × 250 FC layer with WN, ReLU GN, std = 0.5 250 × 250 FC layer with WN, ReLU 250 × 10 FC layer with WNTable 5:	Architectures of generator and discriminator for semi-supervised learning on SVHN andCIFAR-10. N was set to 128 and 192 for SVHN and CIFAR-10, respectively.
Table 5:	Architectures of generator and discriminator for semi-supervised learning on SVHN andCIFAR-10. N was set to 128 and 192 for SVHN and CIFAR-10, respectively.
Table 6:	The architecture of classifiers for semi-supervised learning on SVHN and CIFAR-10. (GN:Gaussian noise, lReLU(leak rate): LeakyReLU(leak rate))Classifier C for SVHNInput 32 × 32 RGB imageGN, std = 0.05Dropout2d, dropping rate = 0.153 ×	3	conv.	64	stride =	1 with	WN, lReLU(0.2)3 ×	3	conv.	64	stride =	1 with	WN, lReLU(0.2)3 ×	3	conv.	64	stride =	2 with	WN, lReLU(0.2)Dropout2d, dropping rate = 0.53 ×	3	conv.	128	stride = 1 with WN, lReLU(0.2)3 ×	3	conv.	128	stride = 1 with WN, lReLU(0.2)3 ×	3	conv.	128	stride = 2 with WN, lReLU(0.2)Dropout2d, dropping rate = 0.53 ×	3	conv.	128	stride = 1 with WN, lReLU(0.2)1 ×	1	conv.	128	stride = 1 with WN, lReLU(0.2)1 ×	1	conv.	128	stride = 1 with WN, lReLU(0.2)Global average Pooling128 × 10 FC layer with WNClassifier C for CIFAR-10
Table 7:	The architecture of classifier for adversarial training on CIFAR-10. (lReLU(leak rate):LeakyReLU(leak rate))Classfier C for CIFAR-10InPUt 32 X 32 RGB image3 × 3 conv. 96 stride = 1 with WN, lReLU(0.2)3 × 3 conv. 96 stride = 2 with WN, lReLU(0.2)DroPoUt, droPPing rate = 0.53 × 3 conv.	192 stride =	1	with WN,	lReLU(0.2)3 × 3 conv.	192 stride =	2	with WN,	lReLU(0.2)DroPoUt, droPPing rate = 0.53 × 3 conv.	192 stride =	1	with WN,	lReLU(0.2)1 × 1 conv.	192 stride =	1	with WN,	lReLU(0.2)Global average Pooling192 × 192 FC layer with WN, lReLU(0.2)192 × 192 FC layer with WN, lReLU(0.2)192 × 192 FC layer with WN, lReLU(0.2)192 × 10 FC layer with WN17Under review as a conference paper at ICLR 20190.00	0.05	0.10	0.15	0.20	0.25	0.30	0.35	0.40
