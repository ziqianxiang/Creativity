Table 1: Datasets and models used in experimentsDataset MNIST	ICIFAR10	IImagenetModel	LeNet-300-100 (LeCun etal.,1998)	WRN-28-2 (Zagoruyko & Komodakis, 2016)	Resnet-50 (He et al.,2015)Architecture	F300 F100 F10	C16∕3×3 [C16∕3×3,C16∕3×3]×4 [C64∕3×3,C64∕3×3]×4 [C128∕3×3,C128∕3×3]×4 GlobalAvgPool, F10	C64/7 X 7-2, MaxPool/3 ×3-2 [C64/1 × 1, C64/3 x3,C256∕1 ×1]×3 [C128∕1x1,C128∕3×3,C512∕1×1]×4 [C256∕1x1,C256∕3×3,C1024∕1×1]×6 [C512∕1x1,C512∕3×3,C2048∕1×1]×3 GlobalAvgPool, F1000# Parameters∣267K	∣1.5M	∣25.6MFor brevity architecture specifications omit batch normalization and activations. Fully connected (F) andconvolutional (C) layers are specified with output size and kernel size, Max pooling (MaxPool) with kernelsize and none with global average pooling (GlobalAvgPool). Brackets enclose residual blocks postfixed withrepetition numbers; downsampling convolution in the first block of a scale group is implied.
Table 2: Test accuracy% (top-1, top-5) of Resnet-50 trained on ImagenetFinal overall sparsity (# Parameters) ∣ 0.8 (7.3M) ∣ 0.9 (5.1M) ∣ 0.0 (25.6M)Reparameterization	static	Thin dense Static sparse	71.6 90.3 [-3.3] [-2.1] 70.4 89.8 [-4.5] [-2.6]	69.4 89.2 [-5.5] [-3.2] 66.4 87.4 [-8.5] [-5.0]	74.9	92.4 [0.0] [0.0]	dynamic	DeePR (Bellec et al., 2017) SET (Mocanu et al., 2018) Dynamic sparse (Ours)	-- [-]	[-] 72.6 91.2 [-2.3] [-1.2] 73.3 92.4 [-1.6] [ 0.0]	-- [-]	[-] 70.4 90.1 [-4.5] [-2.3] 71.6 90.5 [-3.3] [-1.9]			Compressed sparse (Zhu & Gupta, 2017)	73.2 91.5 [-1.7] [-0.9]	70.3 90.0 [-4.6] [-2.4]	CompressionThiNet	68.4	88.3(Luo et al., 2017)	[-4.5 ]	[-2.8]SSS	71.8	90.8(Huang & Wang, 2017)	[-4.3]	[-2.1](at 8.7M parameter count)(at 15.6M parameter count)Numbers in square brackets are differences from the full dense baseline. Romanized numbers are results of ourexperiments, and italicized ones taken directly from the original paper. Performance of two structured pruningmethods, ThiNet and Sparse Structure Selection (SSS), are also listed for comparison (below the double line, seeAppendix C for discussion of their relevance); note the difference in parameter counts.
Table 3: Computational overhead of dynamic reparameterization during training	WRN-28-2 on CIFAR10	Resnet-50 on Imagenet	 DeepR (Bellec et al., 2017)	4.466 ± 0.358	5.636 ± 0.218SET (Mocanu et al., 2018)	1.087 ± 0.049	1.009 ± 0.002Dynamic sparse (Ours)	1.083 ± 0.051	1.005 ± 0.004Shown are median ratios of wall-clock epoch times for training with over without reparameterization, standarddeviation estimated from 25 epochs. WRN-28-2 on CIFAR10 was trained on a single Nvidia Titan Xp GPU, andResnet-50 on Imagenet on four with data parallelism (also see Appendix A for implementation details).
Table 4: Hyperparameters for all experiments presented in the paperExperiment	LeNet-300-100 on MNIST	WRN-28-2 on CIFAR10	Resnet-50 on ImagenetHyperparameters for training			Number of training epochs	100	200	100Mini-batch size	100	100	256Learning rate schedule (epoch range: learning rate)	1-25: 0.100 26 - 50: 0.020 51 - 75: 0.040 76 - 100: 0.008	1 - 60: 0.100 61 - 120: 0.020 121 - 160: 0.040 161 - 200: 0.008	1 - 30: 0.1000 31 - 60: 0.0100 61 - 90: 0.0010 91 - 100: 0.0001Momentum (Nesterov)	0.9	0.9	0.9L1 regularization multiplier	0.0001		0.0	0.0L2 regularization multiplier	0.0	0.0005	0.0001Hyperparameters for sparse compression (compressed sparse) (Zhu & Gupta, 2017)			Number of pruning iterations (T )	10	20	20Number of training epochs between pruning iterations	2	2	2Number of training epochs post-pruning	20	10	10Number of epochs during pruning	40	50	50Learning rate schedule during pruning (epoch range: learning rate)	1 - 20: 0.0200 21 - 30: 0.0040 31 -40: 0.0008	1 - 25: 0.0200 25 - 35: 0.0040 36 - 50: 0.0008	1 - 25: 0.0100 26 - 35: 0.0010 36 - 50: 0.0001Hyperparameters for dynamic sparse reparameterization (dynamic sparse) (ours)			Number of parameters to prune (K)	600	20,0001	200,000	Fractional tolerance of K (δ)	0.1		01	0.1Initial pruning threshold (H(0))	0.001	0.001	0.001Reparameterization period (P) schedule (epoch range: P)	1-25:	100 26 - 50:	200 51 - 75:	400 76 - 100:	800	1 - 25:	100 26 - 80:	200 81 - 140:	400 141 - 200:	800	1 - 25:	1000 26 - 50:	2000 51 - 75:	4000 76 - 100:	8000
Table 5: Test accuracy% (top-1, top-5) of Resnet-50 on Imagenet for dynamic sparse vs. HashedNetFinal overall sparsity (# Parameters)	0.8 (7.3M)	0.9 (5.1M)HashedNet Dynamic sparse (ours)	70.0 [-4.9]	89.6 [-2.8] 73.3 [-1.6]	92.4 [ 0.0]	66.9 [-8.0]	87.4 [-5.0] 71.6 [-3.3]	90.5 [-1.9]Numbers in square brackets are differences from the full dense baseline.
Table 6: Representative examples of training methods that yield “sparse” deep CNNsMethod	Strict parameter budget throughout training and inference	Granularity of sparsity	Automatic layer sparsityDynamic Sparse Reparameterization (Ours)	yes	non-structured	yesSparse Evolutionary Training (SET) (Mocanu et al., 2018)	yes	non-structured	noDeep Rewiring (DeepR) (Bellec et al., 2017)	yes	non-structured	noNN Synthesis Tool (NeST) (Dai et al., 2017; 2018)	no	non-structured	yestf.contrib.model_pruning (Zhu & Gupta, 2017)	no	non-structured	noRNN Pruning (Narang et al., 2017)	no	non-structured	noDeep Compression (Han et al., 2015)	no	non-structured	noGroup-wise Brain Damage	no	channel	no(Lebedev & Lempitsky, 2015) L1 -norm Channel Pruning				no	channel	no(Li et al., 2016) Structured Sparsity Learning (SSL) (Wen et al., 2016) ThiNet (Luo et al., 2017) LASSO-regression Channel Pruning (He et al., 2017) Network Slimming (Liu et al., 2017) Sparse Structure Selection (SSS)				no no no no no	channel/kernel/layer channel	yes no no yes yes		channel channel layer	(Huang & Wang, 2017)			Principal Filter Analysis (PFA) (Suau et al., 2018)	no	channel	yes/noWe provide examples of different categories of methods. This is not a complete list of methods.
Table 7: Test accuracy% (top-1, top-5) of Resnet-50 on Imagenet for different levels of granularityof sparsityFinal overall sparsity (# Parameters)	0.8 (7.3M)	0.9 (5.1M)Thin dense	71.6 [-3.3]	90.3 [-2.1]	69.4 [-5.5]	89.2 [-3.2]Dynamic sparse (kernel granularity)	72.6 [-2.3]	91.0 [-1.4]	70.2 [-4.7]	89.8 [-2.6]Dynamic sparse (non-structured)	73.3 [-1.6]	92.4 [ 0.0]	71.6 [-3.3]	90.5 [-1.9]Numbers in square brackets are differences from the full dense baseline.
