Table 1:	Yelp results with k = 8 reviews being summarized. Note for Best/Worst Review WOscores: we exclude the best/worst review when calculating the average. Numbers are not providedfor models that degenerated into non-natural language. As noted earlier, the NLLâ€™s are only providedfor our abstractive models.
Table 2:	Mechanical Turk results comparing different methods.
Table 3: Results on Amazon datasetModel	Rating Accuracy	Word Overlap	NLLAbstractive (ours)	47.90	27.02	1.23No Training	38.04	18.10	1.37Extractive (Rossiello et al., 2017)	43.86	30.41	1.38Best review	45.05	24.59	1.29Worst review	38.88	13.79	1.36Multi-Lead-1	44.90	32.18	1.335.5	Qualitative Error AnalysisAlthough most summaries look reasonable, there are occasionally failure modes. We discuss thecommon failure modes as follows, with examples in Appendix D.
