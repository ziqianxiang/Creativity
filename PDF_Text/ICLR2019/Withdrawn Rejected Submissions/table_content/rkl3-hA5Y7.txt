Table 1: Perplexity results on PTB and 1B data sets. All HRR models have two word-level roles,and each role has 320, 200, and 50 basis filler embeddings for Fixed-big, Fixed-medium and Fixed-small, respectively.
Table 2: Averaged scores for intrinsic evaluation (18 datasets, numbers on the left) and extrinsicevaluation (6 tasks, numbers on the right).
Table 3: Accuracy (top 1) on the Semantic-Syntactic analogy benchmark. The most frequent 200Kwords are used. We use CosMul for all analogy tests Levy & Goldberg (2014)Figure 3: Examples of ROC curves for two categories. The left figure is for a syntax-driven catego-rization of gerunds, while the right is a semantics-driven categorization of plural verbs. Isometric-big-f1 means the first filler embedding, and Isometric-big-f2 the second.
Table 4: Average AUC on verb-related categories for the baseline, and also the two sets of filler em-beddings from Isometric-big. All models are trained on the full 1B dataset. The first row correspondsto the first syntactic-driven categorization experiment, and the second row to the semantic-driven ex-periment.
Table 5: Cluster analysis for a random selection of 100 sentences containing the company. The mostdominant role in each cluster is identified by a human judge.
Table 6: Hit rate and purity score analysis for chunk embeddings. See Sec.4.4 for more details.
Table 7: Word analogy test results on different datasets.
Table 8: Detailed word analogy test results in each category.
