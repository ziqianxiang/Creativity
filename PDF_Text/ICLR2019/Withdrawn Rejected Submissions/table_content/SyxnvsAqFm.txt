Table 1: Accuracy improvement by each method incrementally for ResNet32 on CIFAR-10.
Table 2: Accuracy Comparison for CIFAR using ResNet32 and WideResNet for TWN and binarymodels. TTQ: Zhu et al. (2017) (TWN model), Wide-1b: McDonnell (2018) (binary model)Config	Accuracy %						CIFAR-10 ResNet32		CIFAR-10 WideResNet		CIFAR-100 WideResNet		Ours	TTQ	Ours	Wide-1b	Ours	Wide-1bFull Precision	92.47	92:33	95	95:77	78.3	8137Binary Model	92.36	92.37	95.02	95.54	78.3	81.06sured with Perplexity Per Word metric (PPW). Full precision PPW is 100.2. Activation quantizationrequires quantization to be performed every iteration for inference, wiping out the speed up obtainedwith quantized weights for inference. Activation quantization slows down training as well. Thus,we use 32bit activations while 3bit activations used by Xu et al. (2018). Our 2-bit alternating quanti-zation (greedy quantization replaced with alternating quantization in our proposed method) reachesfull precision PPW (Table 3).
Table 3: Comparison of Perplexity Per Word (PPW) for LSTM models on WikiText-2 dataset. Multi-table models use 1 quantization bit with multiple tables (8 tables (8 Î± per row) for Embedding andSoftmax layer, 16 tables for LSTM layer). Hybrid models use 1 bit quantization for Embeddinglayer and TWN quantization for Softmax layer. 2 to 4 quantization bits for LSTM layer provides1.53 to 1.65 bits per weight models. Results for Guo et al. (2017) are taken from Xu et al. (2018).
Table 4: Robustness of Step Training to Quantization Step Size for CIFAR-10 with ResNet32. Ac-curacy varies within a range of 3% with QSS ranging from 10 to 2500.
Table 5: Accuracy Comparison for Imagenet using ResNet18 for 1 bit quantization.
