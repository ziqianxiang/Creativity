Table 1: Simulation Parameters for Figure 7 (Anonymous, 2018). The models with highest accuracyare used in the main paper.
Table 2: Simulation Parameters for DenseNet on CIFAR datasetsDataset	Activation	Batch Size	γmax	γmin	Test AccuracyCIFAR-10	-ReLU^^	64	10-1	10-1	80.2%CIFAR-100	-ReLU^^	64	10-1	10-1	80.2%C Experimental Details and Complementary ExperimentsA fully connected ANN with four hidden layers of five neurons each, as FCNN, is trained on thespirals dataset. For the MNIST dataset, the popular convolutional network called LeNet-5 LeCunet al. (1999) is used. To train these networks we let the learning rate γ ∈ R start at a given γmax ∈ Rand then decay by 40% per epoch until reaching some given minimum learning rate γmin < γmax,that is γ = max{γmax0.6bepochc , γmin}. For the CIFAR-10 dataset we train a 100 layer DenseNetarchitecture as done in Huang et al. (2017), but we stop the training after 10 epochs instead ofthe original 300 used by the authors. The different configurations used for these experiments aresummarized in Table 1. For instance, Figure 2 shows 1 realization of SGD training for DenseNet onCIFAR-100. In that figure we observe a rather stable trajectory, with not much oscillation. However,in Figure 8 we average over 2 realizations of SGD learning for DenseNet on CIFAR-10 and obtainhighly oscillating trajectories. As expected, both trajectories follow a similar behavior as the α-SMLC model.
Table 3: Dataset Sizes for Figure 5(b)Dataset Size200p0.10.2	4000.4	100017Under review as a conference paper at ICLR 2019MNISTTraining StePS (％)f c4Z L(W) HTraining StePS (％)方A1Γ V0∙O
