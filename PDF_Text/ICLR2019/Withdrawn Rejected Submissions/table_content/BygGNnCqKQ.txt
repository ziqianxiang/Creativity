Table 1: Summary of compression resultsModel	Acc.	#Params	âˆ† Acc. Compr	Fashion-MNIST			CONV-7	91.46% 708K	+0.98%	7.61xCIFAR-10			CONV-10	92.35% 24.4M	-0.04%	20.33xCIFAR-100			CONV-10	70.95% 24.4M	-1.32%	4.51xSVHN			CONV-10	96.02% 24.4M	-0.63%	8.76xIn this section we evaluate the ability of our approach to compress architectures. For our experiments,we use a standard convolutional architecture consisting of stacked conv-bn-relu blocks and a fullyconnected layer. We use a 7 block network CONV-7 for Fashion-MNIST and 10 block networkCONV-10 for the others. Table shows original average accuracy of the model on the validationset, followed by the number of parameters in the original model followed by two columns for theimprovement in accuracy in the compressed architecture and the compression rate. We notice aminor improvement in accuracy by the compressed architecture in Fashion-MNIST while observinga minor drop in the other datasets. Additionally, our method is able to achieve solid compressionon all the datasets and up to 20x compression on CIFAR-10 (1.2M parameters for final compressedarchitecture).
Table 2: Baselines on CIFAR-10Model	Acc.	#ParamsRomero et al. (2014) (Distillation)	91.33%	1.2MMolchanov et al. (2016) (Pruning)	91.06%	2.3MAshok et al. (2018) (RL)	92.05%	1.7MOurs	92.31%	1.2M8Under review as a conference paper at ICLR 20196	ConclusionIn conclusion, we have described a novel approach to compress CNNs by first training a continuousembedding on a representation of the architecture and then performing gradient descent to determinean optimal architecture for the given task. We also introduced a novel theoretical analysis of CNNswhich we hope will inspire future work. We also demonstrate that our method performs well overa variety of computer vision datasets. Given that this is a novel direction of research, we note thatthere exist multiple future directions to go. Expanding the search space to include networks of greatercomplexity such as ResNets or DenseNets would be of practical interest. Analyzing the transferlearning properties of this approach via the reuse of the weights for other tasks would be of greatpractical use as well.
