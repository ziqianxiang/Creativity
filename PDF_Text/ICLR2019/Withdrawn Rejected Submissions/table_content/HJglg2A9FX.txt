Table 1: Classification with systematic label error: Performance for MNIST, CIFAR-10, CIFAR-100 datasets as the ratio of good samples varies from 60% to 90%. Here Baseline : Naive trainingusing all the samples; ILFB : Our iterative update algorithm; Oracle : Training with all goodsamples. We see significant improvement of ILFB over Baseline for all the settings.
Table 2: Generative models from mixed training data: A quantitative measure of the efficacy ofour approach is to find how many of the good training samples the final discriminator can identify;this is shown here for the three different “good”/“bad” dataset pairs. For each pair, the fractionof “good” samples is 90%, 80% or 70%. The table depicts the ratio of the good samples in thetraining data that are recovered by the discriminator when it is run on the training samples. Thehigher this fraction, the more effective the generator. For MNIST-Fashion and CelebA-CIFAR10,our approach shows significant improvements with iteration count. For CIFAR10-CelebA dataset,the error is extremely simple to be corrected, likely because faces are easier to discriminate againstwhen compared to natural images.
Table 3: MNIST-5% dataset with systematic label error using ILFB with different initializationmethods. The accuracy at the 5-th iteration is shown for both initialization methods. Results areaveraged over 5 runs.
Table 4: MNIST classification: comparison with other choicesdataset					MNIST			τ? =	# good # total	Baseline	ILFB	Centroid	1-Step	∆τ = 10%	∆τ = 15%	∆τ = 20%60%		70.26	90.00	74.52	78.48	88.24	86.48	84.4970%		85.95	92.09	88.47	89.31	90.83	89.19	88.1080%		92.62	94.30	92.79	92.78	92.78	90.17	88.6390%		93.88	94.41	94.32	93.94	92.88	91.21	89.90Table 5: MNIST GAN: comparison with other choicesdataset					MNIST			τ? =	# good # total	Baseline	ILFB	Centroid	1-Step	∆τ = 10%	∆τ = 15%	∆τ = 20%70%		70	97.00	61.46	77.77	83.33	78.06	83.5980%		80	100.00	77.46	76.84	98.80	99.56	97.7790%		90	100.00	89.57	91.90	98.85	99.01	98.04Table 6: MNIST classification with random error.
Table 5: MNIST GAN: comparison with other choicesdataset					MNIST			τ? =	# good # total	Baseline	ILFB	Centroid	1-Step	∆τ = 10%	∆τ = 15%	∆τ = 20%70%		70	97.00	61.46	77.77	83.33	78.06	83.5980%		80	100.00	77.46	76.84	98.80	99.56	97.7790%		90	100.00	89.57	91.90	98.85	99.01	98.04Table 6: MNIST classification with random error.
Table 6: MNIST classification with random error.
