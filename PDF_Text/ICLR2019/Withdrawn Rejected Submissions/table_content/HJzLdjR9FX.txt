Table 1: Pruning rate and accuracy comparison using LeNet-300-100 and LeNet-5 models onMNIST dataset. DC (Deep Compression) and Sparse VD represent a magnitude-based technique(Han et al., 2016) and variational dropout method (Molchanov et al., 2017), respectively.
Table 2: Test accuracy (average of 10 runs for the choice of each SD) when pruning weights ofLeNet-5 model using DeepTwist. Pruning rates are described in Table 1.
Table 3: Comparison on perplexity using various pruning rates. pf is the target pruning rates for theembedded layer, LSTM layer, and softmax layer.
Table 4: Perplexity comparison using various quantization methods with a hidden LSTM layer ofsize 300 on the PTB dataset. For DeepTwist, SD=2000 and the initial learning rate for retraining is20.0. The function used for weight distortion (fD) is described in comments.
Table 5: Test perplexity of the large PTB model after DeepTwist-basd SVD compression usingvarious initial learning rates for retraining (without a projection layer). Test perplexity after the firstSVD compression (which is the test perplexity by using the conventional SVD) is shown on the leftside. We train the model for 55 epochs for both pre-training and retraining.
