Table 1: BLEU (cased) scores on WMT’14 testsets for English-German (En-De) and English-Russian (En-Ru) language pairs (in both directions). All models were trained with 1 GPU. The# Parameters is shown in approximate terms. For homogeneous models, the N-grams denote howwe distribute the 8 heads to different n-gram types; e.g., ‘3/2/3’ means 3 heads on unigrams, 2 onbigrams and 3 on trigrams. For heterogeneous, the numbers indicate the phrase lengths of the col-lection of n-gram components jointly attended by each head; e.g., ‘1-2’ means attention scores arecomputed across unigram and bigram logits.
Table 2: BLEU scores on WMT’14 English-to-German translation task for different models withdifferent training settings. The first block presents reported results along with the number ofGPUs, batch size, and number of update steps used from the respective papers. The second andthird block show results in our training setting with 1 GPU.
Table 3: For model trained with seed 100, percentage (%) of activations for different attention typesin each layer of the Interleaved model for English-to-German translation task in newstest2014.
Table 4: For model trained with seed 20, percentage (%) of activations for different attention typesin each layer of the Interleaved model for English-to-German translation task in newstest2014.
