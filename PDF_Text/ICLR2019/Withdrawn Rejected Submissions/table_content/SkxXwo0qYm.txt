Table 1: Accuracy and Performance of State-of-the-art works for NMT on WMT2014 English toFrench Dataset.______________________________________________________________________________________Paper	Model	BLEU	Training TimeCho et al. (2014)	Phrase table With neural features	34.50	3 DaysSutskever et al. (2014)	Reranking phrase-based SMT best list +	36.5	10 Days With 8 GPUs	LSTM seq2seq		Wu et al. (2016)	Residule LSTM seq2seq + RL refining	41.16	6 Days With 96GPUsGehring et al. (2017)	seq2seq With CNN	41.29	37 Days With 8 GPUsVaswani et al. (2017)	Attention mechanism	41.0	3.5 Days With 8 GPUsthese cases, the programmers write code for the computation of processing an individual trainingsample and add another leading dimension to the tensors representing the inputs and outputs so thatmultiple tensors can be packed into and represented as only one. Then the program will process thenew tensor which actually contains the data from multiple inputs as an individual one using the highperformance computing technology like SIMD instructions or BLAS library to execute it efficientlyin parallel (the correctness is guaranteed by the batch learning which allows multiple inputs to beprocessed using the same parameters).
Table 2: Sentences per second processed in the training phase of different benchmarks on CPU.
Table 3: Sentences per second processed in the training phase of different benchmarks on GPU.
Table 4: The elapsed time (s) of different parts of BiLSTM w/char using by-agenda strategy and ourstrategy, running on GPU.
