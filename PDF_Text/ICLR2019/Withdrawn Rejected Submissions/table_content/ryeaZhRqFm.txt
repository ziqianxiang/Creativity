Table 1: Statistics of the four metabolic networks used as undirected hypergraphsdataset	CORA	DBLP# authors, | V |	1072	685# actual papers (collaborations), |E|	2708	1590# candidate papers, |E|	5416	3180# features (vocabulary size), P	1433	602Table 2: Statistics of the two coauthorship networks used as undirected hypergraphsWe denote WL+ = {(t, h) ∈ E} to be a set of positively labelled directed pairs in loss Ld. In otherwords dij1 = 1 and dij0 = 0 for all (i, j) ∈ WL+ . The tail hyperlink and the corresponfding headhyperlink are separate hypernodes in the dual and form directed hyperlinks in the primal (with thedirection from t to h). The set WL+ consists of directed hyperlinks that currently exist in the givendirected hypergraph. Note that, for loss Lu , the set of positively labelled hypernodes will be,VL+ =	t ∪ h.	(5)(t,h)∈WL+We sample |WL+| = |E| hypernodes (in the dual) from the unlabelled data using the positiveunlabelled approach of 3 to get the set of WL- pairs. We label these pairs negative i.e. dij1 = 0and dij0 = 1 for all (i, j) ∈ WL-. We construct VL- = S(t,h)∈W - t ∪ h similarly as in 5. The setsVL = VL+ ∪ VL- and WL = WL+ ∪ WL- are used to minimise the objective 6.
Table 2: Statistics of the two coauthorship networks used as undirected hypergraphsWe denote WL+ = {(t, h) ∈ E} to be a set of positively labelled directed pairs in loss Ld. In otherwords dij1 = 1 and dij0 = 0 for all (i, j) ∈ WL+ . The tail hyperlink and the corresponfding headhyperlink are separate hypernodes in the dual and form directed hyperlinks in the primal (with thedirection from t to h). The set WL+ consists of directed hyperlinks that currently exist in the givendirected hypergraph. Note that, for loss Lu , the set of positively labelled hypernodes will be,VL+ =	t ∪ h.	(5)(t,h)∈WL+We sample |WL+| = |E| hypernodes (in the dual) from the unlabelled data using the positiveunlabelled approach of 3 to get the set of WL- pairs. We label these pairs negative i.e. dij1 = 0and dij0 = 1 for all (i, j) ∈ WL-. We construct VL- = S(t,h)∈W - t ∪ h similarly as in 5. The setsVL = VL+ ∪ VL- and WL = WL+ ∪ WL- are used to minimise the objective 6.
Table 3: mean AUC (higher is better) over 10 trials. NHP achieves consistently superior performanceover its baselines for all the datasets. Refer to section 5 for more details.
Table 4: mean (± std) number of hyperlinks recovered over 10 trials (higher is better) among the topranked ∣∆E∣ hyperlinks. NHP achieves consistently superior performance over its baselines for allthe datasets. Refer to section 5 for more details.
Table 5: Statistics of the four metabolic netWorks used as directed hypergraphsdataset	iAF692	iHN637	iAF1260b	iJO1366node2vec (Grover & Leskovec, 2016) + MLP	-053-	0.52-	0.58-	-0:56-CMM (Zhang et al., 2018) + MLP	-053-	0.52-	0.52	-051-GCN on star expansion (Ying et al., 2018) + MLP	-048-	0.52-	0.53-	-050-NHP-D (sequential)	-057-	-048-	-0:63-	-060-NHP-D (joint)	0.58	0.51	0.62	0.59Table 6: mean AUC over 10 trials for all the datasets. Both the proposed models achieve similarresults. Refer to section 6 for more details.
Table 6: mean AUC over 10 trials for all the datasets. Both the proposed models achieve similarresults. Refer to section 6 for more details.
Table 7: mean (± std) number of hyperlinks recovered over 10 trials (higher is better) among the topranked ∣∆E∣ hyperlinks. Both the proposed models achieve similar results. Refer to section 6 formore details.
Table 8: mean AUC (higher is better) over 10 trials. Mixed consistently achieves the best AUCvalues. It provides benefits of both positive unlabeled learning and random negative sampling. Referto section 7 for more details.
Table 9: mean (± Std) number of hyperlinks recovered over 10 trials among the top ranked ∣∆E∣hyperlinks. Positive unlabeled learning of 3 achieves consistently lower standard deviations than thethe other two. The standard deviations of random negative sampling are on the higher side. Refer tosection 7 for more details.
Table 10: Hyperparameters of GCN used for all the datasets•	DBLP: We used the DBLP database v43. We filtered out papers without abstracts, andprocessed each abstract by tokenizing it and removing stop-words removal. Further, wefiltered out papers with one author only. This left 540532 papers.
