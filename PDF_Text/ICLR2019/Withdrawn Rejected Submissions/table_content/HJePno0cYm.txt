Table 1: Comparison with state-of-the-art results on WikiText-103.
Table 2: Comparison with state-of-the-art results on enwiki8.
Table 3: Comparison With state-of-the-art results on Penn Treebank word-level language modeling. f indicatesusing two-step finetuning.
Table 4: Comparison with state-of-the-art results on One Billion Word.
Table 5: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-rameters). f indicates that the corresponding row is reduced to the same setting as the Transformer network inAl-Rfou et al. (2018), except that two auxiliary losses are not implemented in our experiments. “PPL init” refersto using the same length as training. “PPL best” indicates the perplexity obtained by using the optimal length.
Table 6: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. Thefirst three models and the last four models are compared as two model groups when we calculate RECL (RECLis computed on a model group rather than a single model). Each group has the same parameter budget.
Table 7: Slowdown in terms of computational time during evaluation. Evaluation is based on per-token timeon one GPU.
Table 8: Ablation study on WikiText-103 with the same GPU memory constraints.
