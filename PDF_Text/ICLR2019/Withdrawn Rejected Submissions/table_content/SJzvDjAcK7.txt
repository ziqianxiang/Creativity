Table 1: Test AccuracyTo see whether the learned representations can be used in a different task, we conduct a transferlearning exercise where embeddings generated from a ResNet-50 model, trained on the coarselabels of CIFAR-100, are used to predict the fine labels of CIFAR-100. A set of ResNet-50-MAXGclassifiers are trained for this purpose on these embeddings. (see Appendix C.1 for further details).
Table 2: Accuracy of low dimensional projections of learned representations at discriminative tasks.
Table 3: Perturbation required for Adversarial MisclassificationAn interesting observation is that the values of œÅ, here in Table 3, are lower than in Figure 2 thoughthe attacks have a higher rate of success. To explain this behaviour, we show empirical evidence toindicate that an attack that adds noise for a fixed number of steps (Kurakin et al., 2017; 2016) to theinput is significantly weaker than one that stops on successful misclassification (See Appendix D.3).
Table 4: Accuracy of classification of adversarial examples by Max Margin Classifiers.
Table 5: Representation from before the third last FC layer of a VGG19trained on CIFAR-10.
Table 6: Value for required for Adversarial Misclassification corresponding to Table 3.
