Table 1: Episodes descriptions in one realistic life-long learning experiment scenario4.1.1	Experiment details and ResultsWe used a basic CNN architecture composed of 3 conv-layers of 32 filters and kernel-size - (3x3)followed by 2 dense layers “fc1” and “fc2” of feature length 256 and a softmax. Our feature trans-former networks at every episode aims to transform “fc1” features using 2 additional dense layersof feature length 256. Feature transformers from previous episode serve as initialization for currentepisode and these models were optimized for the cumulative loss (equation 7), with λ = 0.2. Allthe feature transformers were trained for only 3 epochs, with batch size of 32.
Table 2: MNIST rotations	Table 3: Pneumothorax datasetTable: Performance comparisons with limited memory budgetservative estimates. A standard medical image can be of much larger size (1024*1024) and in 3-D(minimum 10 slices). Any exemplar-based method (iCARL) will have severe storage limitationsthan our method. Additionally, storing 50 low-dimensional features occupies same memory as stor-ing one exemplar image. This directly leads to storing more history compactly while addressingcatastrophic forgetting and privacy.
Table 4: MNIST rotations	Table 5: Pneumothorax datasetTable: Performance comparisons with limited incremental compute5 DiscussionIn the final section, we discuss various points concerning our proposed approach.
