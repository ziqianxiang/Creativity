Table 1: AlexNet: Learning schedule for training and retraining procedures.
Table 2: ResNet50: Learning schedule for training and retraining procedures.
Table 3: AlexNet, number of parameters per layer. Compared to the ‘Standard’ version, ‘Smallrepresents a network with 50% less feature maps and fully connected neurons, whereas ‘Largerepresents a model with 50% more feature maps and fully connected neurons.
Table 4: ResNet50, number of parameters per layer.
Table 5: A3C, number of parameters per layer.
Table 6: Rainbow, number of parameters per layer.
Table 7: IMPALA reinforcement learning agent, number of parameters per layer.
