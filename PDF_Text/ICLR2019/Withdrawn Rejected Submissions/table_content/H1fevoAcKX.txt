Table 1: Pruning results on VGG-16 in CIFAR-10 and comparisons to other approaches. Here, “Lo-cal” refers to setting the pruning rate for each layer, and “Global” refers to setting the global pruningrate from the entire model. “Hard” refers to pruning once per iteration or pruned without gradientupdate, “Soft” refers to pruning once per epoch and the pruned filters can be updated when train-ing the model after pruning. “Normalized” formula reference formula 5(The GHFP2 Normalizedformula has slightly difference) and “Cumulative Normalized” formula reference formula 6.
Table 2: Results after pruning unimportant filters and channels in LeNet-50	1	2	3	4	5	6	7	8	9	10 11 12 13 14 15 16 17 18 19Figure 3: The saliency scores visualization for LeNet-5 on 10 epoch. The top figure shows saliencyscores for CONV1, bottom figure shows the saliency scores for CONV2. The pruning rate are 70%,80%, 85% and 90%, respectively.
Table 3: VGG-16 pruning model detail comparison on CIFAR-10.
Table 4: Comparison of pruning result on CIFAR-10. The MFLOPs denotes million floating-pointoperations. The values in parentheses represent the global filter pruning ratio.
Table 5: Comparison of pruning results on ImageNet. The GFLOPs is the giga floating-point oper-ations. The Acc Drop is the accuracy of the pruned model minus that of the baseline model, so asmaller number of Acc Drop is better.
