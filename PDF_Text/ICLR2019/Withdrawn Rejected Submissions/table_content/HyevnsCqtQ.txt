Table 1: Summary of IPnetsNetwork	MLP-3	ConvNet-5	AlexNet	ResNet-32Dataset	MNIST	CIFAR-10	ImageNet	CIFAR-10Orig Acti Function	ReLU	ReLU	ReLU	Leaky ReLUAccuracy Baseline	98.41%	86%^	57.22%	95.01%Accuracy IP	98.42%	85.94%	57.26%	94.58%Activation %	17.1%	43.6%	44.2%	38.6%Weight %	10%	40.4%	38.8%	32.4%MAC %	3.65%	27.7%	28.9%	13.7%% uo=Eti4Dense	WP	IP100%80%60%40%20%MLP-3 ConvNet-5 Alexnet ResNet-32(a) Comparison of non-zero activation percentage.
Table 2: MLP-3 on MNISTLayer	Shape	Weight#	MAC#	Acti%	Weight %	MAC %fc1	784×300	235.2K	235.2K	12%	10%	3.77%fc2	300×100	30K	30K	24%	10%	2.62%fc3	100×10	1K	1K	100%	20%	6.81%Total		266.2K	266.2K	17.1%	10%	3.65%5Under review as a conference paper at ICLR 20194.2	CONVNET-5 ON CIFAR- 1 0For digit images in MNIST dataset have specific sparse features, the results on small-footprint MLP-3 are very promising. IP is further applied for a 5-layers CNN, ConvNet-5, on a more complicateddataset, CIFAR-10. With two conv layers and three fc layers, the original model has an 86% ac-curacy. As shown in Table 3, the IPnet for ConvNet-5 only needs 27.7% of total MACs comparedto the dense model through pruning 59.6% of weights and 56.4% of activations at the same time.
Table 3: ConvNet-5 on CIFAR-10Layer	Shape	Weight #	MAC#	Acti %	Weight %	MAC %conv1	5×5, 64	-4.8K^^	0.69M	50.6%	70%	70%conv2	5×5, 64	102.4K	3.68M	17.3%	50%	25.3%fc1	2304×384	884.7K	884.7K	9.9%	40%	6.92%fc2	384×192	73.7K	73.7K	44.8%	30%	3%fc3	192×10	1.92K	1.92K	100%	50%	22.4%Total		1.07M	5.34M	43.6%	40.4%	27.7%4.3	AlexNet on ImageNetWe push IP onto AlexNet for ImageNet ILSVRC-2012 dataset which consists of about 1.2M train-ing images and 50K validating images. The ALexNet comprises 5 conv layers and 3 fc layers andachieves 57.22% top-1 accuracy on the validation set. Similar to ConvNet-5, the computation bottle-neck of AlexNet exits in conv layers by consuming more than 9/10 of total MACs. We focus on convlayers here. As shown in Table 4, deeper layers have larger pruning strength on weights and activa-tions because of the sparse high-level feature abstraction of input images. For example, the MACsof layer conv5 can be reduced 10×, while only a 1.2× reduction rate is realized in layer conv1. Intotal, the needed MACs are reduced 3.5× using IP with 38.8% weights and 44.2% activations.
Table 4: AlexNet on ImageNetLayer	Shape	Weight#	MAC#	Acti%	Weight %	MAC %conv1	11×11, 96	34.85K	112.2M	68.7%	85%	85%conv2	5×5, 256	307.2K	240.8M	35.8%	40%	27.5%conv3	3×3, 384	884.7K	149.5M	25%	35%	12.6%conv4	3×3, 384	663.5K	112.1M	25%	40%	10%conv5	3×3, 256	442.4K	74.8M	27.7%	40%	10%Total		2.33M	689.5M	44.2%	38.8%	28.9%4.4	Going deeperCNN models are getting deeper with tens to hundreds of conv layers. We verify the IP method onResNet-32 as shown in Table 5. The ResNet-32 consists of 1 conv layer, 3 stacked residual units and1 fc layer. Each residual unit contains 5 consecutive residual blocks. The filter numbers in residualunits increase rapidly, and same for weight amount. An average pooling layer is connected before thelast fc layer to reduce feature dimension. Compared to conv layers, the last fc layer can be neglectedin terms of weight volume and computation cost. The original model has a 95.01% accuracy onCIFAR-10 dataset with 7.34G MACs per image. Weight and activation pruning strength is designedunit-wise to reduce the exploration space of hyperparameters, i.e., threshold settings. Notice thatleaky ReLU is used as the activation function, thus zero activations are extremely hard to occur inthe original and WP model. Only with IP, the activation percentage can be reduced down to 38.6%.
Table 5: ResNet-32 on CIFAR-10Layer	Shape	Weight #	MAC#	Acti%	Weight %	MAC %conv1	3×3, 16	0.43K	0.44M	40%	40%	40%unit2 {	3×3, 160 3×3, 160 } × 5	2.1M	2.15G	40%	40%	16%unit3 {	3×3, 320 3×3, 320 } × 5	8.76M	2.6G	40%	40%	16%unit4 {	3×3, 640	5 3×3, 640 } × 5	35.02M	2.6G	30%	30%	9.5%Total		45.87M	7.34G	38.6%	32.4%	13.7%long tails towards both positive and negative directions. The activation distribution after IP areshown in Figure 3 (b). Activations near zero are pruned out, and the major contribution comes fromremoving small negative values. In addition, the kept activations are trained to be stronger withlarger magnitude, which is consistent with the phenomenon that the non-zero activation percentageincreases after weight pruning when using ReLU as illustrated in Figure 2 (a).
