Table 1: Words for which semi-supervised method pre-dicts correctly, but unsupervised method doesn’t. Theunsupervised method is able to guess the general fam-ily but fails to pinpoint exact matchFigure 1: Language Pairs and their GHdistanceChazal et al. (2009) showed that the Gromov-Hausdorff distance can be lower bounded by the Bot-tleneck Distance between the Persistence Diagrams of the Vietoris-Rips Filtration of the two spaces.
Table 2: Correlation of GH and Eigenvector similarity with performance of BLI methodsstrong negative correlation with these accuracies, implying that as the GH distance increases, it be-comes increasingly difficult to align these language pairs. S0gaard et al. (2018) also proposed theeigenvector similarity metric between embedding spaces for measuring similarity between the em-bedding spaces. We compute their metric over top n (100, 500, 1000, 5000 and 10000) embeddings(Column Λ in Table 2 shows correlation for the best setting of n) and show that the GH distance(Column GH) correlates better with the accuracies than eigenvector similarity.
Table 3: Performance comparison of BLISS against various baseline models on the MUSE dataset.
Table 4: Performance of different models on the VecMap datasetAfter learning the final mapping matrix, the translations of the words in the source language aremapped to the target space and their nearest neighbors according to the CSLS distance (Lampleet al., 2018) are chosen as the translations.
Table 6: Unsupervised accuracies for different values of β (MUSE) and our autoencoding loss.
Table 7: Performance with different levels of suPervision.										Table 7 shows the performance of different models by varying the amount of supervised data points.
