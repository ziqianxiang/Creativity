Table 1: Comparison of existing results with ours on norm based generalization error bounds forDNNs. For ease of illustration, we suppose the upper bound of input norm R and the Lipschitzconstant 1 of the class of loss functions g7 are generic constants. We use Bd,2, Bd,F, and Bd,2→1as the upper bounds of kWdk2, kWdkF, and kWdk2,1 respectively. For notational convenience, wesuppose the width pd = p for all layers d = 1, . . . , D. We further show the results when kWdk2 = 1for all d = 1,...,D, where IlWdkF = Θ(√r) and IlWdk2 1 = Θ(r) in generic scenarios.
Table 2: Comparison with existing norm basedbounds of CNNs. We suppose R and γ are genericconstants for ease of illustration. The results ofCNNs in existing works are obtained by substitut-ing the corresponding norms of the weight matri-ces generated by orthogonal filters, i.e., kWd k2 =p/s, kWd∣∣F = √p, and kWdk2,1 = p.
