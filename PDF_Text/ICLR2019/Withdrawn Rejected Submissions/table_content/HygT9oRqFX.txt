Table 1: Benchmark results on the CIFAR-10 and -100 datasets. We show the average andstandard errors for five trials. C10 and C100 respectively indicate CIFAR-10 and CIFAR-100.
Table 2: Comparison of the results of various regularization methods on the CIFAR-10 and -100datasets using a 20-layer ResNet (pre-activation). Vanilla indicates that nothing was done to avoidover-fitting, mixup (Zhang et al., 2018a) and manifold mixup (Verma et al., 2018) are used as thelearning schemes, and MixFeat is located immediately after each convolution layer. Shake-shake(Gastaldi, 2017), ShakeDrop (Yamada et al., 2018), and swapout (Singh et al., 2016) are shake meth-ods. Moreover, shake-shake has parallel convolution layers and hence twice the number of param-eters of the other methods. Input MixFeat mixes only input images (not labels) as a special case ofMixFeat. Overall, MixFeat achieves the best performance.
Table 3: Comparison of MixFeat lo-cations in a -(1)-BN-(2)-ReLU-(3)-Conv-(4)- pre-activation unit. MixFeat is per-formed regardless of the location, but thebest location is after convolution.
