Table 1: mean rewards on multi-camera intelligent surveillance task4.2	Large-scale battle taskThe large-scale battle task is one MARL scenario included in the MAgent environment[22]. In thetask our algorithm controls a group of agents to eliminate the opponents. Each agent can move byone cell or attack one nearby enemy at one time step. This task is challenging due to a large numberof participating agents (here 64 in our experiments). The difficulty is even larger considering theenvironment’s distributed setting and agents’ partial visibility. We adopt a self-play training for allthe methods following [22]. All methods are trained for 2000 episodes. Each episode contains 400steps. Learning rate is set to 0.0001 for all.
Table 2: mean rewards on larget-scale battle task5 ConclusionIn this paper, we propose a SSoC network for MARL tasks. Unlike previous methods which oftenassume a predestined communication structure, the SSoC agent learns when to start a communi-cation or transfer its received message via a novel “Speak” action. Similar to the agent’s originalaction, this “Speak” can also be learned in a reinforcement manner. With such a spontaneous com-munication action, SSoC is able to establish a dynamic self-organizing communication structureaccording to the current state. Experiments have been performed to demonstrate better collaborativepolicies and improved on communication efficiency brought by such a design. In future work, wewill continue to enhance the learning of “Speak” action e.g. encoding a temporal abstraction tomake the communication flow more stable or develop a specific reward for this “Speak” action.
