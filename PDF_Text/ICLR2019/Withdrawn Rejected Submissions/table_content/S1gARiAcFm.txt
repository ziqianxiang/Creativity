Table 1: Performance of various learning methods for dynamical systems on generating frames fromthe teapot dataset.
Table 2: DyMoN architecture and training details. Architecture refers to alternative DyMoN ar-chitectures shown in Figure 10. Where step size is given, samples are provided to DyMoN as(xt , xt+step_size).
Table 3: Training loss for DyMoN with and without skip connection.
Table 4: Training time for empirical tests. All networks were trained with 2617MB of RAM on aNVIDIA Titan X Pascal GPU.
Table 5: Performance of various learning methods for dynamical systems on generating a distributionof samples from the GMM dataset.
