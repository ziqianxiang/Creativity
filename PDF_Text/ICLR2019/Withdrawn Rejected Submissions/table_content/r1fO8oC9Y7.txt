Table 1: Utterances’ Commonalities.
Table 2: Statistics of the dataset Snips.
Table 3: Several examples of Snips. Utterance is the user’s request which is a natural languageexpression. Intent and slots are in formats from original dataset. Cross-domain sketch (CDS) hastwo levels (action-level and attribute-level). Target is the final logic form with numbers indicatingcopying words from utterance (index starting from 0).
Table 4: Multi-task Results. Single Seq2Seq means each task has a sequenece-to-sequence model.
Table 5: GDNN Results. Full results of general-to-detailed neural network (GDNN) with differentlevels of CDS (action-level/attribute level) and different utterance encoding mechanisms (identicalencoding/separate encoding).
Table 6: Seq2Seq results Vs traditional results. The first four results show IC_SF models' Perfor-mance. The last three results are based on the Seq2Seq architecture.
Table 7: Results of CDS generation in dataset Snips by two methods. IC_SF is using intent clas-sification and slot filling with evaluation metric (intent accuracy, slot labelling accuracy and finalaccuracy). Seq2Seq generates CDS based on utterance using an encoder-decoder.
