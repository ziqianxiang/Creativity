Table 1: 2-norm statistics on the layers of a PoPulation of networks samPled from HyPerGAN,comPared to 10 standard networks trained from different random initializations. Both HyPerGANand the standard models were trained on MNIST to 99% accuracy. Its easy to see that HyPerGANgenerates far more diverse networksthe scoring rule p(y∣x)=焉 PN=I Pn(y | x, θn). It should be noted that We did not perform finetuning, or any additional training on the samPled networks. The results are shown in Table 2. Wegenerate ensembles of different sizes and compare against both Bayesian (Louizos & Welling, 2016)(Krueger et al., 2017) and non-Bayesian (Lakshminarayanan et al., 2017) methods, as Well as MCdropout (Gal & Ghahramani, 2016). We outperform all other methods by using a 100 netWorkensemble, across all datasets. For all other methods, We report the mean of 100 trials.
Table 2: Classification performance of HyperGAN on MNIST and CIFAR-10. In order to compareagainst MNF and Deep Ensembles, We also train a HyperGAN on only the first 5 classes of CIFAR-10, Which We denote as CIFAR-5. In addition We examine our generalization ability by training ononly 5000 examples of MNIST and CIFAR-10 With a small target netWork. We do not attempt tooutperform state of the art, but We perform better than other probabilistic neural netWork approaches4.3	1-D Toy Regression TaskWe next evaluate the ability of HyperGAN to fit a simple 1D function from noisy samples. Thisdataset was first proposed by (Hernandez-Lobato & Adams, 2015), and consists of a training set of20 points draWn uniformly from the interval [-4, 4]. The targets are given by y = x3 + WhereE 〜N(0,32). We used the same target architecture as in (Hernandez-Lobato & Adams, 2015)Lakshminarayanan et al. (2017) and (Louizos & Welling, 2016): a one layer neural network with100 hidden units and ReLU nonlinearity. For HyperGAN we use two layer generators, and 128hidden units across all networks. Because this is a small task, we use only a 64 dimensional latentspace. MSE loss is used as our target loss function to train HyperGAN.
Table 3: Statistics on the layers of a population of networks sampled from HyperGAN, compared to10 standard networks trained from different random initializations. Without the mixing network orthe discriminator, HyperGAN suffers from a lack of diversityHyperGAN - CIFAR-10	Conv1	Conv2	Conv3	Linear1	Linear2Mean	1.87	16.83	9.35	10.66	20.35σ2	0.11	2.44	1.02	0.16	0.76Standard Training - CIFAR-10						Conv1	Conv2	Conv3	Linear1	Linear2Mean	5.13	15.19	16.15	11.79	2.45σ2	1.19	4.40	4.28	2.80	0.13Table 4: Statistics on the layers of networks sampled from HyperGAN without the mixing networkor discriminator, compared to 10 standard networks trained from different random initializationsA.4 HyperGAN Network DetailsIn tables 5 and 6 we show how the latent points are transformed through the generators to become afull layer of parameters. For a MNIST based HyperGAN we generate layers from small latent points11Under review as a conference paper at ICLR 2019of dimensionality 128. For CIFAR-10 based HyperGANs we use a larger dimensionality of 256 forthe latent points.
Table 4: Statistics on the layers of networks sampled from HyperGAN without the mixing networkor discriminator, compared to 10 standard networks trained from different random initializationsA.4 HyperGAN Network DetailsIn tables 5 and 6 we show how the latent points are transformed through the generators to become afull layer of parameters. For a MNIST based HyperGAN we generate layers from small latent points11Under review as a conference paper at ICLR 2019of dimensionality 128. For CIFAR-10 based HyperGANs we use a larger dimensionality of 256 forthe latent points.
Table 5: MNIST HyperGAN Target Size Table 6: CIFAR-10 HyperGAN Target SizeLayer	Latent size	Output Layer Size	Layer	Latent Size	Output Layer SizeConv 1	128x 1	32 x 1 x 5 x 5	Conv 1	256x1	16x3x3x3Conv 2	128 x 1	32 x 32 x 5 x 5	Conv 2	256 x 1	32x16x3x3Linear	128 x 1	512x10	Conv 3	256 x 1	32 x 64 x 3 x 3	—		Linear 1	256 x 1	256 x 128			Linear 2	256 x 1	128x10A.5 Connections to GANs and Wasserstein Auto-encodersDespite being largely applied to the same task of learning probability distributions, implicit gen-erative models such as Generative Adversarial Networks (GAN) and Wasserstein Auto-encoders(WAE) are presented under different frameworks. GANs are a likelihood free density estimator,specifying only a generator, and do not have any notion of reconstruction. WAEs find an optimaltransport map between target and model distributions by first encoding samples to a latent spaceand reconstructing the original data samples using a generator. Even with these differences, manycombinations of these models have been proposed. Creating composite models and loss functionsfrom the GAN and WAE building blocks is commonplace.
