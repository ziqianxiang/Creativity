Table 1: Fine-tuning After Quantization (FAQ) exceeds or matches the accuracy of the fp32baseline networks on the Imagenet benchmark for both 8 and 4 bits on representative state-of-the-art network architectures, and outperforms all comparable quantization methods inall but one instance. Baselines are popular architectures He et al. (2016); Huang et al. (2017);Szegedy et al. (2016); Simonyan & Zisserman (2014) from the PyTorch model zoo. Other resultsreported in the literature are shown for comparison, with methods exceeding or matching their top-1 baseline (which may be different than ours) in bold. Precision is in bits, where w= weight ,and a=activation function. Accuracy is reported for the ImageNet classification benchmark. FAQResNet-18, 4-bit result shows meanÂ±std for 3 runs. Compared methods: Apprentice (Mishra &Marr, 2017), Distillation (Polino et al., 2018), UNIQ (Baskin et al., 2018), IOA (Jacob et al., 2017),Joint Training (Jung et al., 2018), EL-Net (Zhuang et al., 2018). Since only one epoch was necessaryto fine-tune 8-bit models, we were able to study more 8 than 4-bit models.
Table 2: Sensitivity experiments indicate that longer training duration, initialization from apretrained model, larger batch size, lower weight decay, and initial activation calibration allcontribute to improved accuracy when training the 4-bit ResNet-18 network, while the exactlearning rate decay schedule contributed the least. The standard parameters are on row 1. Eachsubsequent row shows the parameters and score for one experiment with changed parameters inbold. * Note that to keep the number of weight updates approximately the same, the number ofepochs was inceased, since larger batches result in fewer updates per epoch.
