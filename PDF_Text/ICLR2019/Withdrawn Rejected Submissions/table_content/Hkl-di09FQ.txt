Table 1: Mean reward performance and standard error in RL (using PPO) per episode (average on 100episodes) at the end of training for all the environments tested.
Table 2: GTC, GT Cmean, and mean reward performance in RL (using PPO) per episode after 5 millionssteps, with standard error (SE) for each SRL method in mobile robot navigation 2D random target environ-ment.
Table 3: Mean reward performance in RL (using PPO) perepisode (average on 100 episodes) for different budgets, withstandard error in robotic arm with random target environment.
Table 4: Influence of the weights for theSRL Splits model performance in Navi-gation 2D random target environment.
Table 5: GTC, GT Cmean, and mean reward performance in RL (using PPO) per episode after 2 millionssteps, with standard error (SE) for each SRL method in Navigation 1D target environment.
Table 6: Mean reward performance in RL (using PPO) per episode (average on 100 episodes) for differentbudgets, with standard error in Navigation 1D target environment.
Table 7: Mean reward performance in RL (using PPO) per episode (average on 100 episodes) for differentbudgets, with standard error in Navigation 2D random target environment.
Table 8: GTC, GT Cmean, and mean reward performance in RL (using PPo) per episode after 5 millionssteps, with standard error for each SRL method in robotic arm with random target environment.
Table 9: GTC, GT Cmean, and mean reward performance in RL (using PPO) per episode after 5M steps,with standard error (SE) for each SRL method in robotic arm with moving target environment.
Table 10: Mean reward performance in RL (using PPO) per episode (average on 100 episodes) for differentbudgets, with standard error in robotic arm with moving target environment.
Table 11: GTC, GT Cmean, and mean reward performance in RL (using PPO) per episode after 2 millionssteps, with standard error for each SRL method in Navigation 2D random target environment. The slash /stands for using different splits of the state representation, and the plus + for combining methods on a sharedrepresentation; e.g Auto-Encoder + Reward stands for combining an Auto-Encoder to a Reward model.
