Table 1: Chomsky HierarchyAutomata	Languages	Time	SpaceFinite State (FSA)	Regular	O(n)	O(1)Push Down (PDA)	Context Free (CF)	Matrix Multiply	Matrix MultiplyLinear Bounded	Context Sensitive (CS)	Worse	WorseTuring Machines	Recursively Enumerable	Beyond Worse	That said, it may not be all that helpful to focus too much on time and space, since training dataappears to be more of a limiting factor on progress than time and space complexity. This paper willpropose an organization of the deep nets literature based on training data, as opposed to time andspace. The ranking of problems is intended to make it easier to generalize across tasks. If we havea new problem, can we compare it to a bunch of known problems in a meaningful way? Can thisranking help us estimate how easy or hard the new problem is? How do we estimate requirementson computational resources for a new problem?This paper will suggest a particular proposal for ranking deep net problems based on learning curves.
Table 2: The proposed hierarchy: a ranking of deep learning tasks sorted by βg . In general, thereis no data like more data, but some tasks are more effective than others in taking advantage of moredata. Note that speech & vision have better βgs than language modeling. This may help explain whythere is relatively more excitement about deep nets in speech and vision (vs. language modeling).
Table 3: Breadth of domains, model features, optimizers, loss functions testedDomain	Model	Model Features	Optimizer	Loss Function	Exponent (βg)Machine Translation	LSTM-	Encoder-decoder with attention, with and without dropout	Adam	Token Error	-0.128Word LMS	LSTM-	GEMMs, σ+tanh non-linearities	^SGD	Xentropy	-0.066	^HN	GEMMs, σ+tanh non-linearities-	-SGD	XentrOPy-	-0.070CharLMS 一	RHN	GEMMs, σ+tanh non-linearities	SGD, Adamr	Xentropy	-0.094Image Classification	ResNet	Feed-forward, CONV blocks, pooling and skip connections	Nesterov Momentum	Classify Error	-0.309				X-entropy	-0.350Speech Recognition	^^DS2	Bi-LSTM, CTC loss	Adam	-CER	-0.299	Attention	Bi-LSTM, CONVs, attention layer	Adam	CER	-0.296	—A. 1 Neural Machine TranslationGiven input and output vocabularies, VS and VT , NMT models learn a mapping DS → DT whereD. = Vλ* (Kleene star). In this work, We use a word-piece vocabulary shared between the sourceand target languages. After applying pre-processing methods2 adopted in many recent models, thereare 36545 sub-word tokens. We include UNK and PAD tokens for unknown words and minibatchpadding for the source domain (German, |VS | = 36547); for the target domain (English), UNK,PAD, SOS (start-of-sequence), and EOS (end-of-sequence) are included (|VT | = 36549). The Ger-man and English sentences in newstest2016 were on average 27 and 25 tokens long with the longestsequences having 101 and 94 tokens respectively.
