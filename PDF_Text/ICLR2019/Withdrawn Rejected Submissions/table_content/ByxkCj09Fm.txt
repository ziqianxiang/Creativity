Table 1: Soft hierarchical probabilities for a specific class (’tabby’) with the shrink vector[f2,f3,4,f5,6]= [10, 100, 1K]7.2	Appendix B: Additional table resultsSoft hierarchical based probabilities - choosing hyper-parameters results: Table 2 indicatesthe impact of different sets of f values on the flat-hit and hierarchical-precision metrics on theILSVRC 2012 1K benchmark. To provide a baseline for comparison, we indicate the performanceof our model next to hard-NLL model, where all models trained with Renset50.
Table 2: Shirk factor fd impact on model performance on ImageNet ILSVRC12 1K validation settrained with Resnet50Model name	Flat hit@k(%)				Hierarchical Precision@k				1	2	5	10	2	5	10	20NLL-Alexnet	58.6	70.1	-808^^	86.7	0.461	0.345	0.314	0.317DeVise(dim=500)-	53.2	65.2	^^6^	83.3	0.447	0.352	0.331	0.341NLL-Resnet50	75.9	85.8	-928^^	95.8	0.571	0.423	0.377	0.360Soft-NLL-Alexnet	57.7	69.2	^^9^	85.6	0.542	0.493	0.491	0.490soft-NLL-Resnet50	75.7	85.6	92.0	96.0	0.683	0.615	0.614	0.623Table 3: Comparison of model performance on test set (ImageNet ILSVRC12 1K validation set)7.3	Appendix C: Explanation of semantic weighting techniqueWe found an interesting evidence which gives validity and intuition to our semantic empiricalweighting technique proposed in Section 4.1. This evidence stems from analyzing the error hard-model behavior. We found that this behavior is common to different deep architectures, where inthis section the details regarded to resnet50 topology trained on ILSVRC12-1K dataset. All labelsin this dataset are defined as leaf nodes in the taxonomy graph as mentioned in Section 4. Figure 8(a) presents the Top-1 hard model error prediction occurrences at each distance level, where thesum of all columns gives the total model error. Although we expect that this graph will decreasemonotonically according to the distance 3 it has much complex behavior. As expected there is amaximum in level-2 which is the closest related leaf nodes distance level, means that the the maxi-
Table 3: Comparison of model performance on test set (ImageNet ILSVRC12 1K validation set)7.3	Appendix C: Explanation of semantic weighting techniqueWe found an interesting evidence which gives validity and intuition to our semantic empiricalweighting technique proposed in Section 4.1. This evidence stems from analyzing the error hard-model behavior. We found that this behavior is common to different deep architectures, where inthis section the details regarded to resnet50 topology trained on ILSVRC12-1K dataset. All labelsin this dataset are defined as leaf nodes in the taxonomy graph as mentioned in Section 4. Figure 8(a) presents the Top-1 hard model error prediction occurrences at each distance level, where thesum of all columns gives the total model error. Although we expect that this graph will decreasemonotonically according to the distance 3 it has much complex behavior. As expected there is amaximum in level-2 which is the closest related leaf nodes distance level, means that the the maxi-mum confusion corresponds to taxonomy distance similarity. Moreover, starting from level-7 thereis a monotonic decrease behavior. However, in level-3 there is a decrease and an immediate salientincrease in level-4 which is opposed to intuition.
