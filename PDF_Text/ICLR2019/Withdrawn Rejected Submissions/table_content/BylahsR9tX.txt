Table 1: The table shows the total parameters and perplexity (average among all weights andacross all ranks) of the model with the best efficiency on test sets for PTB (left) and WT2 (right)for Language Modeling tasks. We choose a 3-layer AWD LSTM (tied) as our baseline (Mer-ity et al., 2018). Lower E(r) and PPL are better. We reproduced the baseline model results. fParameter numbers are estimated with reference to (Grachev et al., 2017).
Table 2: The table shows the total parameters and accuracy (average among all weights and acrossranks) of the model with the best efficiency on test sets for SST-5 (left), SNLI (right), and devset6 for SQUAD (bottom). We reproduced the results of all the baseline models.
Table 3: Summary of Op, Cost and Memory consumption of compression method used in the paper.
Table 4: Hyper-parameters setting	r	initial lrSemi-NMF	^T0-	10-	^200^	1	^400^	-0.1-SVD	^T0-	10-	^200^	1	^400^	1Prune	-T0-	-30-	^200^	-30-	-W^	30We achieved lower perplexity after fine-tuning steps and the results are shown in Figure 5.
Table 5: Results of fine-tuning on PTB.
Table 6: Datasets used for the evaluation. In PTB and WikiText-2 we show the number of tokens, inthe other three the number of samples.
Table 7: Penn Tree Bank (PTB) language modeling results.
Table 8: WikiText-2 (WT2) language modeling results.
Table 9: SST-5 sentiment analysis results.
Table 10: SNLI textual entailment results.
Table 11: SQuAD question answering results.
Table 12: Conversion from rank to pruning.
