Table 1: Error rates for the balanced parentheses (BP) test sets (d=depth, l=length, k = 5).
Table 2: Error rates of the SR-LSTM-P on the small BP testdata for various numbers of centroids (d=depth, l=length).
Table 3: Error rates in % on se-quences of varying lengths from thePalindrome test set.
Table 4: Error rates (on training and test splits of the IMDBdata) in % and averaged training time in seconds when train-ing only on truncated sequences of length 10.
Table 5: Error rates in % of some sr-LSTM-ps Table 7: Accuracy in % of some sr-LSTM-ps andand state of the art methods for IMDB.	state of the art methods for sequential MNIST.
Table 8: The words with the top-4 highest transition probabili-ties for four centroids. This visualizes the sr-LSTM-p’s behav-ior and associates centroids with prototypical words.
Table 9: A summary of dataset and experiment characteristics. The values in parentheses are thenumber of positive sequences.
Table 10: The lengths (l) and depths (d) of the sequences in the training, validation, and test sets ofthe various tasks.
Table 11: Error rates in % on sequences of varying lengths from the Palindrome test set.
Table 12: Error rates for the balanced parentheses (BP) test sets (d=depth, l=length, k = 5).
Table 13: Error rates (on training and test splits of the IMDB data) in % andaveraged training time in seconds when training only on truncated sequencesof length 10.
Table 14: The perplexity results for the SR-LSTMs and state of the art methods. Here, θW+M are thenumber of model parameters and θM the number of model parameters without word representations.
Table 15: The seven Tomita grammars (Tomita, 1982).
Table 16: List of prototypical words for the k = 10 centroids of an SR-LSTM-P trained on theIMDB dataset. The top-5 highest transition probability words are listed for each centroid. Wecolored the positive centroid words in green and the negative centroid words in red.
