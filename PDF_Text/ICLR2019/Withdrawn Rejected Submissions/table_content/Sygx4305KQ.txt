Table 1: Optimiser comparison on small degenerate datasets. For each optimiser, we report themean±standard deviation of the number of iterates taken to reach the solution. For the stochasticRosenbrock function, U[λ1, λ2] denotes noise drawn from U[λ1, λ2] (see Sec. 4 for details)	Deterministic	Rosenbrock U[0, 1]	U[0, 3]	Raihimi & RechtSGD + momentum	370 ± 40	846 ± 217	4069 ± 565	95 ± 2Adam (Kingma & Ba, 2014)	799 ± 160.5	1290 ± 476	2750 ± 257	95 ± 5Levenberg-Marquardt (Mor6, 1978)	16 ± 4	14±3	17±4	9±4BFGS (Wright & Nocedal, 1999, p. 136)	19±4	44 ± 21	63 ± 29	43 ± 21Exact Hessian	14 ± 1	10±3	17±4	9± 0.5CurveBall (proposed)	13 ± 0.5	12± 1	13± 1	35 ± 11the operations to minimize the number of automatic differentiation (back-propagation) steps:4z = (Jφ)HlJ't + λI) Z + JφJ	(16)=Jφ (HL JTZ + JL) + λz	(17)In this way, the total number of passes over the model is two: we compute Jφv and JφTv0 products,implemented respectively as one RMAD (back-propagation) and one FMAD operation (section 2.2).
Table 2: Best error in percentage (training/validation) for different models and optimisation methods.
