Table 1: Numerical evaluations over the GMM (32 components) synthetic data. The “covered modes”metric is defined as the number of covered modes by a set of generated samples. The “realistic ratio”metric is defined as the proportion of generated samples which are close to a mode. The measurementdetails are presented in section 12.1 in the Supplement. Mean and SD are from 10 independent runs.
Table 2: Inception score (IS) and FID on CIFAR-10 for unsupervised and supervised learning.
Table 3: Comparison with state-of-the-art methods on three benchmark datasets. “CIFAR-10 IS”means the inception score for samples generated by SSL models trained on CIFAR-10. “『'is obtainedby running the released code accompanied by the corresponding papers. “-” means the results are notreported in the original work and without released code. “/” means not applicable, e.g. the modelscannot generate samples stochastically. “!” uses image data augmentation which significantly helpsclassification performance. The upper/lower blocks show generative/discriminative SSL methodsrespectively.
Table 4: Ablation study of our inclusive-NRF method on CIFAR-10, regarding the effects of usingSGLD or SGHMC in training and of applying sample revision in inference (generating samples).
Table 5: Network architectures and hyperparameters for the 2D GMM data.
Table 6: Network architectures and hyperparameters for semi-supervised inclusive-NRFs on MNISTRandom Field	GeneratorInput 28 × 28 Gray Image MLP 1000 units, Leaky ReLU, Weight norm MLP 500 units, Leaky ReLU, Weight norm MLP 250 units, Leaky ReLU, Weight norm MLP 250 units, Leaky ReLU, Weight norm MLP 250 units, Leaky ReLU, Weight norm MLP 10 units, Linear, Weight norm	Noise h (100-dim) MLP 500 units, Sotfplus, Batch norm MLP 500 units, Sotfplus, Batch norm MLP 784 units, SigmoidBatch size Number of epochs Leaky ReLU slope Learning rate Optimizer Sample revision steps α in SSL	100 200 0.2 0.001 Adam (β1 = 0.0, β2 = 0.9) L = 20 αd = 10, αc = 10, αp = 0Table 7: Network architectures and hyperparameters for semi-supervised inclusive-NRFs on SVHNRandom Field	GeneratorInput 32 X 32 Colored Image	Noise h (100-dim)3 × 3 conv. 64, Leaky ReLU, Weight norm	MLP 8192 units, ReLU, Batch norm3 × 3 conv. 64, Leaky ReLU, Weight norm	Reshape 512 × 4 × 43 × 3 conv. 64, Leaky ReLU, Weight norm	5 × 5 deconv. 256, ReLU, Stride=2stride=2, dropout2d=0.5	5 × 5 deconv. 128, ReLU, Stride=23 × 3 conv. 128, Leaky ReLU, Weight norm	5 × 5 deconv. 3, Tanh, Stride=23 × 3 conv. 128, Leaky ReLU, Weight norm	3 × 3 conv. 128, Leaky ReLU, Weight norm stride=2, dropout2d=0.5	3 × 3 conv. 128, Leaky ReLU, Weight norm	1 × 1 conv. 128, Leaky ReLU, Weight norm	1 × 1 conv. 128, Leaky ReLU, Weight norm MLP 10 units, Linear, Weight norm	Batch size	100Number of epochs	400Leaky ReLU slope	0.2
Table 7: Network architectures and hyperparameters for semi-supervised inclusive-NRFs on SVHNRandom Field	GeneratorInput 32 X 32 Colored Image	Noise h (100-dim)3 × 3 conv. 64, Leaky ReLU, Weight norm	MLP 8192 units, ReLU, Batch norm3 × 3 conv. 64, Leaky ReLU, Weight norm	Reshape 512 × 4 × 43 × 3 conv. 64, Leaky ReLU, Weight norm	5 × 5 deconv. 256, ReLU, Stride=2stride=2, dropout2d=0.5	5 × 5 deconv. 128, ReLU, Stride=23 × 3 conv. 128, Leaky ReLU, Weight norm	5 × 5 deconv. 3, Tanh, Stride=23 × 3 conv. 128, Leaky ReLU, Weight norm	3 × 3 conv. 128, Leaky ReLU, Weight norm stride=2, dropout2d=0.5	3 × 3 conv. 128, Leaky ReLU, Weight norm	1 × 1 conv. 128, Leaky ReLU, Weight norm	1 × 1 conv. 128, Leaky ReLU, Weight norm MLP 10 units, Linear, Weight norm	Batch size	100Number of epochs	400Leaky ReLU slope	0.2Learning rate	0.001Optimizer	Adam (β1 = 0.0, β2 = 0.9)Sample revision steps	L= 10α in SSL	αd = 10, αc = 10, αp = 0
Table 8: Network architectures and hyperparameters for semi-supervised inclusive-NRFs on CIFAR-10Random Field	GeneratorInput 32 × 32 Colored Image	Noise h (100-dim)3 × 3 conv. 128, Leaky ReLU, Weight norm	MLP 8192 units, ReLU, batch norm3 × 3 conv. 128, Leaky ReLU, Weight norm	Reshape 512 × 4 × 43 × 3 conv. 128, Leaky ReLU, Weight norm	5 × 5 deconv. 256, ReLU, Stride=2stride=2, dropout2d=0.5	5 × 5 deconv. 128 ReLU, stride=23 × 3 conv. 256, Leaky ReLU, Weight norm	5 × 5 deconv. 3, Tanh, Stride=23 × 3 conv. 256, Leaky ReLU, Weight norm	3 × 3 conv. 256, Leaky ReLU, Weight norm	stride=2, dropout2d=0.5	3 × 3 conv. 512, Leaky ReLU, Weight norm	1 × 1 conv. 256, Leaky ReLU, Weight norm	1 × 1 conv. 128, Leaky ReLU, Weight norm	MLP 10 units, Linear, Weight norm	Batch size	100Number of epochs	600Leaky ReLU slope	0.2Learning rate	0.001
