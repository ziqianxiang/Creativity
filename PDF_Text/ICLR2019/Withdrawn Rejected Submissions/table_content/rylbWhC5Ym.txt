Table 1: Mean Square Error (MSE) averaged across 10 runsRewriting δt and denoting ∆φt = φ(st) - γφ(st+1), we haveE[φ(st)rt] = E[φ(st)∆φt> + ηM]θt,where M = E[Diag(φ(st) ◦ φ(st+1))] = E[Diag(φ(st)φ>(st+1))]. Thus we haveE[φ(st)(∆φ(st))> +ηM] = E[φ(st)φ>(st) -γφ(st)φ>(st+1) + ηDiag(φ(st)φ>(st+1))]If we set η → γ, we observe that the second and third terms in the RHS above cancel out in thediagonal element. Consider the scheme where we initialize η = γ and then reduce it as over thetraining process. It is equivalent to slowly introducing the discount factor into the error computation.
Table 2: Method specific hyper parameters for Mountain Car with linear FA11Under review as a conference paper at ICLR 2019B.2.1 Mountain CarMax steps per episode: 200, Number of episodes: 1000Technique	DQN	DQN+TC	HR-QLearning Rate	10-3	10-4	10-3Target update	500	500	500η 一	-	-	0.03Table 3: Method specific hyper parameters for Mountain CarB.2.2 AcrobotMax steps per episode: 500, Number of episodes: 200Technique	DQN	DQN+TC	HR-QLearning Rate	10-3	10-4	10-4Target update	500	500	500η 一	-	-	0.01Table 4: Method specific hyper parameters for AcrobotB.3 DQN for AtariNetwork: DQN architecture from Mnih et al. (2015), Optimizer: AdamReplay Memory size: 100000, minimum : 0.01
Table 3: Method specific hyper parameters for Mountain CarB.2.2 AcrobotMax steps per episode: 500, Number of episodes: 200Technique	DQN	DQN+TC	HR-QLearning Rate	10-3	10-4	10-4Target update	500	500	500η 一	-	-	0.01Table 4: Method specific hyper parameters for AcrobotB.3 DQN for AtariNetwork: DQN architecture from Mnih et al. (2015), Optimizer: AdamReplay Memory size: 100000, minimum : 0.01is decayed over 5% of the framesTraining Frames: 10M game frames, fed 4 at a time to network. (2.5 M agent steps)η is decayed as η = τη+ι, where T is integer value of 5^, and t is the iteration number.
Table 4: Method specific hyper parameters for AcrobotB.3 DQN for AtariNetwork: DQN architecture from Mnih et al. (2015), Optimizer: AdamReplay Memory size: 100000, minimum : 0.01is decayed over 5% of the framesTraining Frames: 10M game frames, fed 4 at a time to network. (2.5 M agent steps)η is decayed as η = τη+ι, where T is integer value of 5^, and t is the iteration number.
Table 5: Method specific hyper parameters for AtariC Policy Evaluation Learning Curves(a) Neural Net(b) Linear FAFigure 5: Comparison of policy evaluation on the Mountain Car domain.
