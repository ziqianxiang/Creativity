Table 1: Downstream tasks and datasets.
Table 2: Results on downstream tasks: Bold face indicates best result and underlined results showwhen fake sentence training is better than Skip-thought (full). COCO-Cap and COCO-Img arecaption and image retrieval tasks on COCO. We report Recall@5 for the COCO retrieval tasks.
Table 3: Probing task accuracies. Tasks: SentLen: predict sentence length, WC: is word in sentence,TreeDepth: depth of syntactic tree, TopConst: predict top-level constituent, BShift: is bigram inflipped in sentence, Tense: predict tense of word, Subj(Obj)Num: singular or plural subject, SOMO:semantic odd man out, CoordInv: is co-ordination is inverted.
Table 4: Example illustrating how Skip-thought (ST) and Fake sentence (FS) training allows repre-sentations to capture big shifts in meaning even with small changes to the sentence form. The tableshows where both models rank a specific fake version of a real sentence. The ranking is is obtainedusing cosine similarity over a random sample of 10000 real and fake sentences when projected viat-SNE.
