Table 1: Modeling perplexity, where training and testing timesteps are the same.
Table 2: Classification results with temporal word embeddings for the modeling configuration.
Table 3: Prediction perplexityS2			NYT		Reddit	Models	micro	macro	micro	macro	micro	macroLSTM	84.7	82.7	128.5	128.4	125.8	126.1DT	92.0	89.6	137.1	137.0	151.1	151.6DWE	87.0	84.8	140.1	140.0	136.5	139.9DRLM-Id	81.2	79.2	123.7	123.6	124.7	125.0DRLM	79.7	77.8	123.3	123.1	123.9	124.3Table 4: Prediction classificationS2				NYT			Reddit		Models	Precision	Recall	F1	top1	top3	top5	top1	top3	top5LSTM	0.378	0.144	0.190	35.1	54.7	65.9	322XΓ	52.8	63.4DT	0.357	0.111	0.154	19.1	45.1	62.1	12.5	26.0	33.2DWE	0.358	0.134	0.177	33.4	55.0	66.8	34.3	51.0	60.8DWE-F	0.317	0.166	0.186	31.4	53.4	65.0	34.8	50.7	59.7DRLM-Id	0.385	0.144	0.193	42.3	66.9	77.6	41.6	58.3	67.0DRLM	0.370	0.167	0.208	41.2	60.0	68.9	38.0	56.2	66.7Table 5: Text sequences generating with DMLR conditioned on different timesteps on the S2 corpus.
Table 4: Prediction classificationS2				NYT			Reddit		Models	Precision	Recall	F1	top1	top3	top5	top1	top3	top5LSTM	0.378	0.144	0.190	35.1	54.7	65.9	322XΓ	52.8	63.4DT	0.357	0.111	0.154	19.1	45.1	62.1	12.5	26.0	33.2DWE	0.358	0.134	0.177	33.4	55.0	66.8	34.3	51.0	60.8DWE-F	0.317	0.166	0.186	31.4	53.4	65.0	34.8	50.7	59.7DRLM-Id	0.385	0.144	0.193	42.3	66.9	77.6	41.6	58.3	67.0DRLM	0.370	0.167	0.208	41.2	60.0	68.9	38.0	56.2	66.7Table 5: Text sequences generating with DMLR conditioned on different timesteps on the S2 corpus.
Table 5: Text sequences generating with DMLR conditioned on different timesteps on the S2 corpus.
