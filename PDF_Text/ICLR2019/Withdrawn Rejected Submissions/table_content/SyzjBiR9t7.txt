Table 1: Comparison results on Moving MNISTWe tested the SPD-TCN on theMoving MNIST dataset (Srivas-tava et al., 2015). Recently, inChakraborty et al. (2018) au-thors developed a manifold val-ued recurrent network architec-ture, dubbed SPD-SRU, whichproduced state-of-the-art clas-sification results on MovingMNIST dataset in comparison to LSTM (Hochreiter & Schmidhuber, 1997), SRU (Oliva et al.,2017), TT-LSTM and TT-GRU (Yang et al., 2017) networks. For the LSTM and SRU networks,convolution layers are also used before the recurrent unit. We will compare directly with the resultspresented in Chakraborty et al. (2018). For details of the various architectures used please see section5 of Chakraborty et al. (2018). We will also compare with Huang & Van Gool (2017) , which proposesa set of layers for learning SPD matrices, dubbed SPDNet. When using SPDNet we first downsample6Under review as a conference paper at ICLR 20193:0	Recon. of ffig using PrinciPal subspaCe‚Öù, deleted by *IEf-
