Table 1: Number of Iteration Below the Threshold				Tasks	SARAPO	SVRPO	TRPO	Pendulum	-149.4	-167.9	-147.1	CartPole	500	500	500	Swimmer	343.0	355.7	336.5	Half-Cheetch 6845.1		5291.4	5080.9	Walker	5232.9	5529.2	5265.2	Hopper	3578.1	3625.8	3692.8	Table 2: Performance of Policythe threshold the fastest and in the other two tasks, SVRPO outperforms the other two algorithms.
Table 2: Performance of Policythe threshold the fastest and in the other two tasks, SVRPO outperforms the other two algorithms.
Table 3: The Best Hyper-Parameter of SARAPOTasks	Minibatch Size	Inner Loop Iteration	Max KL	CG Damping FactorPendulum	5000	50	0.01	0.1CartPole	5000	50	0.01	0.1Swimmer	1000	20	0.01	0.1Half-Cheetch	5000	50	0.01	0.1Walker	5000	50	0.01	0.1Hopper	5000	20	0.01	0.1Table 4: The Best Hyper-Parameter of SVRPOB	The Mean Episode Reward of the experiments in Figure 4.3We provided the learning curve for the experiments in Figure 4.3. We only plot the statistics usinga single run. We see that for this random seed, SARAPO achieve best performance on Swimmerand HalfCheetah tasks. SARAPO and SVRPO get similar performance on Walk2d task and bothoutperform than TRPO except Hopper task.
Table 4: The Best Hyper-Parameter of SVRPOB	The Mean Episode Reward of the experiments in Figure 4.3We provided the learning curve for the experiments in Figure 4.3. We only plot the statistics usinga single run. We see that for this random seed, SARAPO achieve best performance on Swimmerand HalfCheetah tasks. SARAPO and SVRPO get similar performance on Walk2d task and bothoutperform than TRPO except Hopper task.
