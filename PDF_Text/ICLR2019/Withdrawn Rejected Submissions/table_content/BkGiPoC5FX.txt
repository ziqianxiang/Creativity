Table 1: CNN Training Result in CIFAR-10 & CIFAR-100 (Small Learning Rate)CIFAR 10	BP	BP w/ BN	CDFA Random	CDFA Eqâ‘º	CDFA w/BN	CBDFA Random	CBDFA Eq (9)	CBDFA w/BNTop5	98.63	98.24	98.42	98.55	98.56	98.63	98.88	98.83Topl	81.11	76.91	88.68	86.36	87.41	89.39	87.65	86.46CIFAR 100	BP	BP w/ BN	CDFA Random	CDFA Eq (7)	CDFA w/BN	CBDFA Random	CBDFA Eq (9)	CBDFA w/BNTop5	67.80	63.91	77.05	72.82	77.55	-75.07	71.92	76.85Top1	40.29	37.80	61.42	48.24	55.11	59.92	47.48	54.47Figure 4: Training and Test Accuracy with Proposed Training Algorithmconventional BP even though the feedback weight is randomly initialized. In CIFAR-10, the CDFAand CBDFA are 7.5% and 8.3% higher in Top1 test accuracy than the BP respectively. The accuracyimprovement by the CDFA and the CBDFA seems much more remarkable in CIFAR-100. As shownin Figure 4, the training curve of the CDFA and CBDFA is much slower, but they achieve 21.3%and 19.6% better performance respectively compared with the BP. However, the feedback weightwith the random initialization has critical problems for training. One of the problems is the slowtraining curve described in Figure 4. DFA requires time to be adaptive to the randomly initializedfeedback weights, so it takes a long latency to be converged. In the BP approach, we generallytake the larger learning rate to make the training faster. However, the test accuracy of the DFA andBDFA is swung up and down dramatically when the large learning rate is applied. Moreover, it stillspends a long time to converge. In this problem, the initialization with the equation (7) and (9) canbe useful to solve the learning speed and stability problem. After the feedback weight is initialized
Table 2: CNN Training Result with Data Augmentation (CIFAR-10)	w/o Data Augmentation			w/ Data Augmentation			BP	CBDFA	Conv. only Training	BP	CBDFA	Conv. only TrainingTop5	99.15	99.07	98.84	99.33	99.49	99.46Topl	82.33	87.35		82.06		87.97	90.13		88.48	takes the initialization with equation (9). In table 2, the performance of the CBDFA shows the high-est accuracy compared with not only BP but also the training suggested by Hoffer et al. (2018). Ittrains only the convolutional layers, and the parameters of the FC layers are fixed. Even consideringthe data augmentation, CBDFA still shows higher training accuracy compared with the other twomethods. As a result, CBDFA seems robust to the size of the dataset.
