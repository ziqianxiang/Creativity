Table 1: Architecture search algorithmsAlgorithm	Learning	Direct exploitationRandom Search	No	NoNeural agent	Yes	NoEvolutionary agent	No	YesEvo-NAS agent	Yes	Yes5	ExperimentsTo properly evaluate the advantages and disadvantages of different approaches, we propose to considertwo characteristics:•	whether the agent is capable of learning patterns•	whether the agent is capable of efficiently exploiting good past experiencesThe two characteristics above are independent, and for a fixed architecture search algorithm, both ofthem may or may not be present. In Table 1, we summarize the characteristics of the methods we aimto compare .
Table 2:	Best ROC-AUC(%) on the testset for each algorithm and dataset. We report the average over10 runs, as well as ± 2 standard-error-of-the-mean (s.e.m.) Bolding indicates the best performingalgorithm or those within 2 s.e.m. of the best.
Table 3:	The search space defined for text classification experiments.
Table 4: Options for text input embedding modules. These are pre-trained text embedding tables,trained on datasets with different languages and size. The text input to these modules is tokenizedaccording to the module dictionary and normalized by lower-casing and stripping rare characters.
Table 5: The number of trials performed for the experiments from Figure 4. We report the averageover 10 runs, as well as ± 2 standard-error-of-the-mean (s.e.m.). Bolding indicates the algorithmwith the highest number of trials or those that have performed within 2 s.e.m. of the largest numberof trials.
Table 6: Statistics of the text classification tasks.
