Table 1: Kolmogorov-Smirnov tests on Imnet1k validation and test sets for various levels of leakage.
Table 2: Accuracy of membership in- ference attacks on the softmax layer of	Model	Augmentation	Bayes baseline	MAT	Resnet101	None	76.3	90.4the models (final-output). Data aug- mentation reduces the gap between the training accuracy and the held-out accu-		Flip, Crop ±5 Flip, Crop	69.5 65.4	77.4 68.0					VGG16	None	77.4	90.8racy, thus decreasing the accuracy of the				Bayes attack and the MAT attack.		Flip, Crop ±5	71.3	79.5		Flip, Crop	63.8	64.3Table 3: Accuracy of membership	Augmentation	Truncate	Resnet-101	VGG-16inference attacks on intermediate lay-	None	Softmax	73.4	74.8ers of Resnet-101 and VGG-16 mod- els (partial-layers). Last block corre-		Last block	53.1	51.7	Flip, Crop±5	Softmax	65.7	67.3sponds to the first fully connected layer				for VGG-16 and to the 4-th stage of		Last block	53.1	52.2Resnet-101.	Flip, Crop	Softmax	60.8	58.5		Last block	52.9	53.2initial learning rate of 0.01, divided by 10 every 30 epochs. Parameter optimization is conductedwith stochastic gradient descent with a momentum of 0.9, a weight decay of 10-4, and a batch sizeof 256. To assess the effect of data augmentation, we train different networks with varying dataaugmentation: flip+crop±5, flip+crop, flip+crop+resize, or none.
Table 3: Accuracy of membership	Augmentation	Truncate	Resnet-101	VGG-16inference attacks on intermediate lay-	None	Softmax	73.4	74.8ers of Resnet-101 and VGG-16 mod- els (partial-layers). Last block corre-		Last block	53.1	51.7	Flip, Crop±5	Softmax	65.7	67.3sponds to the first fully connected layer				for VGG-16 and to the 4-th stage of		Last block	53.1	52.2Resnet-101.	Flip, Crop	Softmax	60.8	58.5		Last block	52.9	53.2initial learning rate of 0.01, divided by 10 every 30 epochs. Parameter optimization is conductedwith stochastic gradient descent with a momentum of 0.9, a weight decay of 10-4, and a batch sizeof 256. To assess the effect of data augmentation, we train different networks with varying dataaugmentation: flip+crop±5, flip+crop, flip+crop+resize, or none.
Table 4: Duplicate statistics for the datasets we use.
Table 5: Accuracy of membership in-ference attacks before the softmax layerof the models (partial-layers), usingshadow models.
