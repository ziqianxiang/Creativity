Table 1: A comparison of encoder models. BLEU corresponds to the BLEU scores of non-VAEsequence generation tasks. FN corresponds to the false negative rate for the prediction task of wordsin input text. SA denotes self-attention Encoder and BoW denotes Bag-of-Words Encoder.
Table 2: Language modeling results. SA denotes self-attention and DCNN denotes Dilated CNN.
Table 3: Sampling from the posterior distribution of our model when different input is given to theself-attention and Bag-of-Words encoders. “SA” is a sentence given to self-attention encoder and“BoW” is a sentence given to the Bag-of-Words encoder. For details, see Chapter 4.4. For moresamples, see Table 7 in Appendix D.
Table 4: Samples from components of the prior distribution from cluster 1 (above) and 2 (below) inFigure 5. Components in cluster 1 share grammatical structure and components in cluster 2 sharetopics. Please see Chapter 4.5 for more details.
Table 5: Comparison of BLEU scores from multimodal prior distribution model with differentnumbers of components.
Table 6: Semi-supervised learning. LM-LSTM and SA-LSTM come from (Dai & Le, 2015), theydenotes the LSTM initialized with an autoencoder and a language model. The methods of semi-supervised learning with VAEs use the same scheme as (Yang et al., 2017). LSTM is a simplesupervised model.
Table 7: Sampling from posterior distribution of our model when different texts are input to self-attention and Bag-of-Words of the encoder. “SA” is a sentence given to self-attention encoder and“BoW” is a sentence to Bag-of-Words encoder. For detail, see Chapter 4.4.
Table 8: Samples from components of prior distribution. Component 1, 22, and 76 generate sentenceswith common structure. On the other hand, component 60, 68, and 83 generate structurally diversesentences on the same topics (computer, sports). <UNK>is a word not in the dictionary. For detail,see Chapter 4.5.
