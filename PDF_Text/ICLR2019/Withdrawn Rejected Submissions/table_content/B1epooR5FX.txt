Table 1: Training episodes required for the cumulative regret to become permanently negative (compared to allbaselines) for all combinations of Prediction, Reward, and use of initial functions (“-”: does not happen within5000 episodes).
Table 2: Binary Search reproducibility: average regret per episode (lower is better) and break-even pointPercentile	1	5	10	25	50	75	90	95	99Regret @5K episodes	-2.71	-2.66	-2.62	-2.45	-2.03	-1.01	0.44	0.70	0.78Regret @50K episodes	-3.99	-3.83	-3.76	-3.64	-3.34	-2.85	3.80	3.86	3.92Break-even point (episodes)	127	201	271	417	758	2403	∞	∞	∞B.2	QuickSortTo quantify the reproducibility, we ran the experiment described in Sec. 4.3 115× and report thecumulative regret per episode (average number of extra operations, as read=write=1, compare=0.5per sort) compared to vanilla QuickSort. On average, the cumulative regret per episode is -913 @1K(-1064 @10K) on a total operation cost of 25.1K per sort. The break-even point is reached in 94%of the cases, and in an average after 368 episodes. The performance breakdown by percentile, andthe number of steps at which the break-even point is reached are referenced in table 3.
Table 3: QuickSort reproducibility: average regret per episode (lower is better) and break-even pointPerCentile	1	5	10	25	50	75	90	95	99Regret @1K episodes	-1273	-1248	-1214	-1146	-1029	-916	-409	372	425Regret @10K episodes	-1356	-1306	-1267	-1219	-1146	-1034	-945	-285	393Break-even point (episodes)	0	-0-	-0-	-37-	-93-	141	307	7370	∞B.3	CachesIn order to quantify the reproducibility of our experiments we ran 100 times the same experimentillustrated in sec. 4.4 and we report the performance of the learned cache policy using predictedvariables when compared with LRU heuristic, by looking at the cumulative regret metric after 20000episodes. We break down the cumulative regret by percentiles in table 4.
Table 4: Caches reproducibility: average regret per episode (lower is better) and break-even pointPerCentile	1	5	10	25	50	75	90	95	99Regret @20K episodes	-8.25	-5.88	-3.49	-0.00	0.00	0.02	0.34	0.84	2.17Break even point (episodes)	32	157	472	∞	∞	∞	∞	∞	∞When counting the number of runs for which there exists an episode where the cumulative regret isstrictly negative until the end, we note that this happens for 26% of the runs. For 60% of the runs thecumulative regret does not become positive, meaning that using the learned cache policy is at leastas good as using the LRU heuristic. This leaves us with 14% of the runs resulting in strictly worseperformance than relying on the LRU heuristic.
Table 5: Parameters for the different experiments described below (FC=fully connected layer, LR=learningrate). See (Henderson et al., 2018) for details on these parameters.
