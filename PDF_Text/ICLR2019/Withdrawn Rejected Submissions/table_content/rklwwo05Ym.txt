Table 1: PTB training XEs with various dropout rate multipliers between deterministic and GMC. Observe themonotonic improvement in training fit when reducing the dropout rate at evaluation only.
Table 2: Validation XEs on some datasets varying the power α and the dropout rate mulitiplier λ. Deterministicdropout is not the best evaluation method for the language modelling datasets due to a simple smoothing effect.
Table 3: PTB training and validation XEs for AMC at λ ∈ {0, 0.8, 1} per word frequency. Note how DETdominates AMC on the training set, but AMC is better for rare words in the validation set.
Table 4: Validation and test perplexities on PTB and Wikitext-2 with various evaluation strategies and default oroptimal validation softmax temperatures. Our baseline results correspond to DET at temperature 1. Note thatAMC does not benefit from setting the optimal softmax temperature (“opt”), while DET is improved by it almostto the point of matching AMC which supports the smoothing hypothesis.
Table 5: Validation and test set perplexities on PTB with shared (S) or non-shared (NS) dropout masks fora small, 1 layer and a large, 4 layer LSTM with 10 and 24 million weights, respectively. Non-shared masksperform nearly as well as shared masks and as we have seen neither is “more variational” than the other.
