Table 1: Comparison between our method trained for 10 GD steps and 3 epochs and various baselines. Forbaselines using K-Nearest Neighbor (K-NN), best result among all combinations of distance metric ∈ {l1, l2}and K ∈ {1, 3} is reported. In K-NN and k-means, K and k can have different values. All methods use 10 imagesper class, except for the average real images baseline, which reuses the same images in different GD steps.
Table 2: Performance of our method and baselines in adapting models among MNIST (M), USPS (U), andSVHN (S). 100 distilled images are trained for 10 GD steps and 3 epochs. Few-shot domain adaptation methodby Motiian et al. (2017) and baselines use the same numbers image per class.
Table 3: Performance of our method and baselines in adapting an AlexNet pre-trained on ImageNet toPASCAL-VOC and CUB-200. Only one distilled image per class are trained to be applied in 1 GD step repeatedfor 3 epochs. Our method significantly outperforms the baselines. Results are collected over 10 runs.
