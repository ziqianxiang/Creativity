Table 1: Five settings of attack methods and defense models.		Number	Defense Model	Attack Method1	Naturally Trained Model	PGD2	Adversarial Training (Madry's)	PGD3	Naturally Trained Model	second-order (S-O) attack4	Adversarial Training (Madry's)	second-order (S-O) attack5	Stability trained model with Gaussian Noise (STN)	second-order (S-O) attackMNIST In the first plot in Figure 2, we monitor the average `2 norm of gradients ofthe loss function during the construction of adversarial examples. Specifically, we computeNb D∈i kVχL(θ, x,y)∣χ(t) k2 for each t, where I is the index set of a batch. We monitor thisquantity under the setting 1, 2 and 4 in Table 1. The result shows the `2 norms of the gradients of theadversarially trained model are much smaller than the ones of the naturally trained model, validatingour explanation in Section 3, that an adversarially trained model tends to make the loss functionflat in the neighborhood of natural examples. It also shows our attack method can find adversarialexamples with large loss more efficiently, by incorporating second-order derivative information.
Table 2: Different attack methods on Madry’s model.
