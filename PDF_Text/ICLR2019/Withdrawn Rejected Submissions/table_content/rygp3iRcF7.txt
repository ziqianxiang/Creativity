Table 1: The BLEU scores on token-level translation tasks for the variations of the Transformer-basedarchitecture.
Table 2: The BLEU scores on token-level translation tasks for the LSTM-based architecture withvarying model capacities.
Table 3: The BLEU scores on character-level translation tasks for the Transformer-based architecturewith varying model capacities.
Table 4: The BLEU scores on character-level translation tasks for the LSTM-based architecture withvarying model capacities.
Table 5: Test accuracy of image captioning models on COCO40 (in-domain) and Flickr 1K (out-of-domain) tasks.___________________________________________________________________________Model	COCO40		Flickr 1K		CIDEr	ROUGE-L	CIDEr	ROUGE-LBenchmark (Sharma et al., 2018)	1.032	0.700	0.359	0.416Benchmark RePlicate	1.034	0.701	0.355	0.4092 × 2Eq.3&4	1.060	0.704	0.364	0.4203 × 3Eq.3&4	1.060	0.706	0.377	0.4193 × 3Eq.9&4	1.045	0.707	0.372	0.420do not use any additional parameters beyond the benchmark model. 3 × 3 achieved the best resultsoverall. 3 × 3 Eq. 9 adds a small fraction of the number of parameters to the benchmark model anddid not seem to improve on the parameter-free version of area attention, although it still outperformedthe benchmark.
