Table 1: Experimental and predicted accuracy of classifiers for MNIST and CIFAR-10. The pre-dicted accuracy is the percentage of images for which < L . The empirical accuracy is the percentof images that survive a perturbation of size . Attacks on both the cross-entropy (X-ent) and logitsas in Carlini & Wagner (2017) (CW) are presented.
Table 2: Accuracy of different MNIST classifiers against PGD and FGSM attacks on X-ent and CWlosses under the white-box and black-box threat models. Attacks have maximum '∞ perturbation= 0.3. The iterative white-box attacks have an initial random step. The naturally trained model isused for generating black-box attacks. We use CW loss for the black-box attack.
Table 3: White-box attacks on CIFAR-10 models. We use '∞ attacks with E = 8. For the 20-stepPGD, similarly to Madry et al. (2017), we use an initial random perturbation. We do not use arandom perturbation for the FGSM attack since it decreased the attack’s effectiveness.
Table 4: Black-box attacks on CIFAR-10 models. Attacks are '∞ with E = 8. Similar to Madry et al.
Table 5: White-box iterative attacks on CIFAR-100 models. We use '∞ attacks with E = 8. Forbrevity, we only report the results for attacking the cross-entropy loss. We attack the models withadversaries having different strengths by varying the number of PGD steps.
Table 6: Accuracy of different models trained on MNIST with a 40 step PGD attack on the cross-entropy (X-ent) loss and the Carlini-Wagner (CW) loss under the white-box and black-box threatmodels. Attacks are '∞ attacks with a maximum perturbation of E = 0.3. The iterative white-boxattacks have an initial random step. The naturally trained model was used for generating the attacksfor the black-box threat model. We use the CW loss for the FGSM attack in the blackbox case. k isthe number of training iterations.
Table 7: White-box attacks on the CIFAR-10 models. All attacks are '∞ attacks with E = 8. For the20-step PGD, similar to Madry et al. (2017), we use an initial random perturbation.
Table 8: Black-box attacks on the CIFAR-10 models. All attacks are '∞ attacks with E = 8. Similarto Madry et al. (2017), We build 7-step PGD attacks and FGSM attacks for the public adversarialtrained model of MadryLab. We then use the built attacks for attacking the different models. *:Since we do not have the Madry model, we cannot evaluate it under the PGD attack with andwithout random initialization and therefore we use the same value that is reported by them for both.
Table 9: The effect of unbounded on the accuracy. The decline in the accuracy as a sanity checkshows that the sample model is at least not completely breaking the PGD attack and is not obfuscat-ing the gradients.
Table 10: The effect of the number of random restarts while generating the adversarial examples onthe accuracy of a model trained with the logit squeezing. It shows that the accuracy plateaus at 9random restarts.
Table 11: The effect of the number of steps of the white-box PGD attack on the CW loss (worst casebased on Table 3) for the model trained in 160k steps with logit squeezing parameters β = 10 andσ = 30 on CIFAR-10 dataset. The model remains resistant against '∞ attacks with e = 8Figure 9: The cross-entropy landscape of the first eight images of the validation set for the modeltrained for 160k iteration and hyper-parameters β = 10 and σ = 30. To plot the loss landscapeWe take walks in one random direction r1 〜RademaCher(0.5) and the adversarial direction a2 =SignExXent) where xent is the cross-entropy loss. We plot the cross-entropy (i.e. Xent) loss atdifferent points X = Xi + e1 ∙ r1 + e2 ∙ a2. Where Xi is the clean image and -10 ≤ e1 ,e2 ≤ 10. Asit can be seen moving along the adversarial direction changes the loss value a lot and moving alongthe random direction does not make any significant major changes.
