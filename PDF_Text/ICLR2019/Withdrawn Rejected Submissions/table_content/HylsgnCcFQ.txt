Table 1: Statistics of the datasets used in our experiments5.1	DatasetsWe use four dynamic graph datasets with two communication and bipartite rating networks each.
Table 2: Experiment results on dynamic link prediction (micro and macro averaged AUC with stan-dard deviation). We show GraphSAGE (denoted by G-SAGE) results with the best performingaggregators for each dataset (* represents GCN, f represents LSTM, and 去 represents max-pooling).
Table 3: Experimental study on removing temporal attention layers from DySAT (micro and macroaveraged AUC with standard deviation)A.2 Visualization of Temporal Attention WeightsWe conduct a qualitative analysis to obtain deeper insights into the distribution of temporal atten-tion weights learned by DySAT. In this experiment, we examine the temporal attention coefficientslearned at each time step t, which indicate the relative importance of each historical snapshot (< t)in predicting the links at t. We choose the Enron dataset to visualize the mean and standard deviationof temporal attention coefficients, over all the nodes. Figure 3 visualizes a heatmap of the learnedtemporal attention weights on Enron dataset for the first 10 time steps.
Table 4: Experiment results on dynamic new link prediction (micro and macro averaged AUC Withstandard deviation). We shoW GraphSAGE (denoted by G-SAGE) results With the best performingaggregators for each dataset (* represents GCN, f represents LSTM, and 去 represents max-pooling).
Table 5: Experimental results of IncSAT in comparison to DySAT (micro and macro averaged AUCwith standard deviation)snapshot GT over the historical representations of each node to compute the final node embeddings{evT ∀v ∈ V} at T, which are trained on random walks sampled from GT.
Table 6: Experiment results on dynamic link prediction (micro and macro average precision withstandard deviation). We show GraphSAGE (denoted by G-SAGE) results with the base performingaggregators for each dataset (* represents GCN, f represents LSTM, and 去 represents max-pooling).
