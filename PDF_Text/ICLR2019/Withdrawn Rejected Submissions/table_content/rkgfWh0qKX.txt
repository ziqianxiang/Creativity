Table 1: Example of full and partial scoring for the test "The trophy doesnâ€™t fit in the suitcase becauseit is too big." with two reference choices "the suitcase" and "the trophy".
Table 2: Accuracy on PDP-60Method	AccuracyUnsupervised Semantic Similarity Method (USSM)	48.3 %Single-model LM-full (ours)	60.0 %USSM + Cause-Effect + WordNet (Miller,1995) + ConCePtNet (Liu & Singh, 2004)	56.7 %USSM + Supervised Deepnet	53.3 %USSM + SuPervised DeePnet + 3 Knowledge Bases	66.7 %Ensemble of 5 Unsupervised LMs-full (ours)	70.0 %On the harder task WSC-273 where questions are designed to exclude relevant knowledge in theirwording, incorporating supervised learning and knowledge base to USSM (Liu et al., 2016) providesinsignificant gain this time (+3%), compared to the large gain on PDP-60 (+19%). On the otherhand, our single-model resolver can still outperform the other methods by a large margin as shown inTable 3. By ensembling predictions from multiple LMs, we obtain nearly 10% of absolute accuracyimprovement compared to the previous state-of-the-art. We note that Sharma et al. (2015) alsoattempted WSC but their approach is only applicable to 53 out of 273 test cases, therefore notcomparable to our results.
Table 3: Accuracy on Winograd Schema ChallengeMethod	AccuracyUSSM + Knowledge Base	52.0 %USSM + SuPervised DeePNet + Knowledge Base	52.8 %Single-model LM-partial	56.4%Ensemble of 10 Unsupervised LMs-partial	61.5 %Customizing training data for Winograd Schema Challenge As previous systems collect rele-vant data from knowledge bases after observing questions during evaluation (Rahman & Ng, 2012;Sharma et al., 2015), we also explored using this option. Namely, we build a customized text corpusbased on questions in commonsense reasoning tasks. It is important to note that this does not includethe answers and therefore does not provide supervision to our resolvers. In particular, we aggregatedocuments from the CommonCrawl dataset that have the most overlapping n-grams with the questions.
Table 4: F1 scores on full test set proposed by Li et al. (2016) and novelty-based test set proposed byJastrzebski et al. (2018).
Table 5: Accuracy of keyword detection from forward and backward scoring by retrieving top-2tokens with the highest value of qt	Resolution accuracy	Special word retrievedForward scoring	63.7%	97 / 133 (70%)Backward scoring	58.2%	18/45 (40%)For questions with keywords appearing before the reference, we detect them by backward-scoringmodels. Namely, we ensemble 6 LMs, each trained on one text corpus with word order reversed.
Table 6: One-dimensional convolutional layers used to process character inputs	Conv 1	Conv 2	Conv 3	Conv 4	Conv 5	Conv 6	Conv 7	Conv 8Kernel size	1	2	3	4	5	6	7	7Output channels	32	32	64	128	256	512	1024	2048For word inputs, we use an embedding lookup of 800000 words, each with dimension 1024. Forcharacter inputs, we use an embedding lookup of 256 characters, each with dimension 16. Weconcatenate all characters in each word into a tensor of shape (word length, 16) and add to its twoends the <begin of word> and <end of word> tokens. The resulting concatenation is zero-padded toproduce a fixed size tensor of shape (50, 16). This tensor is then processed by eight different 1-Dconvolution (Conv) kernels of different sizes and number of output channels, listed in Table 6, eachfollowed by a ReLU acitvation. The output of all CNNs are then concatenated and processed bytwo other fully-connected layers with highway connection that persist the input dimensionality. Theresulting tensor is projected down to a 1024-feature vector. For both word input and character input,we perform dropout on the tensors that go into LSTM layers with probability 0.25.
Table 7: All variants of recurrent LMs used in our experiments.
Table 8: Details of LMs and their training corpus reported in our experiments.
