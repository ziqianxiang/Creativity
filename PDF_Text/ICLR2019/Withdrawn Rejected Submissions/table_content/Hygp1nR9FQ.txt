Table 1: Recovery performance for manually chosen bilateral filter parameters. We measure recov-ery by the percentage of examples which, after filtering, revert to the classification label assigned tothe unperturbed image by the CNN. This shows that with adaptively chosen parameters accordingto the attack, we can recover nearly all adversarial examples3.2	Adaptive FilteringOne caveat to the above approach, is that the parameters for the bilateral filter must be carefullychosen to be able to recover the accuracy and confidence of the original classification. Large valuesfor the parameters σs and σr can create an excessively blurred image, and a small filter size Kmay capture insufficient information to remove the adversarial perturbations. With this in mind,we train a small network which will predict the parameters of the bilateral filter (K, σs , σr) for aninput image. This network will serve as a cheap preprocessing step that will remove adversarialperturbations without affecting the underlying class label.
Table 2: Performance of our adaptive bilateral filter (AF) network across different attacks. We show(A) the top-5 accuracy of recovering the original predicted classification label from the adversarialexample (note this is not necessarily the ground truth label), as well as (B) how often AF is able todefeat the adversarial attack - changing the prediction from the adversarial label to a new oneNetwork	Clean	FGSM	PGD	MIM	CW	DeepFool	L-BFGSInc V3 top1	78.8	30.1	0.2	0.1	0.1	0.7	0.0Inc V3 top5	94.4	65.2	4.8	5.5	7.3	0.5	12.1AF+IncV3top1	71.7	71.0	71.6	63.1	71.1	70.1	64.2AF+IncV3top5	89.6	84.0	86.3	74.6	84.1	85.2	76.7IncResNet V2 top1	80.4	55.3	0.8	0.5	0.3	2.5	0.0IncResNet V2 top5	95.3	72.1	15.8	10.2	10.3	8.5	19.2AF + IncResNet V2 top1	73.1	70.1	70.8	60.5	70.3	70.5	65.0AF + IncResNet V2 top5	86.7	83.1	82.8	71.7	83.6	85.6	77.0Table 3: Top-1 and top-5 accuracy of InceptionV3 and Inception-ResNetV2 on adversarial examples.
Table 3: Top-1 and top-5 accuracy of InceptionV3 and Inception-ResNetV2 on adversarial examples.
Table 4: Performance of BFNet against DeepFool and L-BFGS attacks. We report the average L2and L∞ distance of 1, 000 adversarial images on Inception V3 and Inception-ResNet V2.
Table 5: Performance of BFNet against FGSM and MI-FGSM adversaries for a range of perturbationsizes (lower is better). For our MI-FGSM attack, we use a momentum decay factor of 1.0, and runthe attack for 10 iterationseach followed by 2 x 2 max pooling and ReLU. We use a final fully connected layer with 1024units. We modify our network into a BFNet by adding our bilateral filter layer at the input of the firstconvolutional layer. We then train the entire BFNet (including the filtering layer) with adversarialtraining using three distinct adversaries: FGSM, PGD, and PGD with the proposed CW loss func-tion. We report the results in table 6. Our results perform well against the state-of-the-art adversarialtraining results. We also show that when our network is trained on a single strong adversary, we arerobust to attacks from other adversaries.
Table 6: LEFT: Comparison of our method with state of the art adversarial training results onMNIST. BFNetpgd denotes our model trained against a PGD adversary, while BFNetfgsm is trainedagainst a FGSM adversary. For Tramer We report A: the strongest white box attack given against anon-ensembled model from (Tramer et al., 2018), as well as B: the performance of architecture Bfrom (Madry et al., 2018); RIGHT: Performance of our two adversarially trained BFNets on CIFAR-10. BFpgd denotes our model trained against a PGD adversary, while BFNetfgsm is trained againsta FGSM adversary. Network type (A) refers to the ResNet network used in (Madry et al., 2018),while (B) refers to the smaller architecture.
Table 7: Performance of BFNet against the Boundary and Approximate L-BFGS black-box attacks.
