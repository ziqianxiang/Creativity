Table 1: Threat models: at-tacker knows θ and / or NFP.
Table 2: Parameters and model test-accuracy on PW-attacks for different datasets (without NeuralFP test). CW-L2 and JSMA attacks are unbounded. The bounds arerelative to images with pixel intensities in [0, 1].
Table 4: Detection AUC-ROC of blackbox-defenders (do not know attack strategy) against partial-whitebox-attackers (know model f(x; θ), but not defense details; see Section 2.1), on MNIST,CIFAR-10 on test-set (“real”) and corresponding adversarial (“fake”) samples (1328 pre-test sampleseach). NeuralFP outperforms baselines (LID, KD, BU) on MNIST & CIFAR-10 across attacks.
Table 5: Detection AUC-ROC of Neu-ralFP vs partial-whitebox-attacks onMiniImagenet-20, N = 20, ε = 0.05.
Table 6: Detection AUC-ROC for adaPtiVe attacks on datasets MNIST and CIFAR-10. Otherdefenses such as (Song et al., 2018; Liao et al., 2017), including the baselines KD and BU, failunder adaPtiVe-attacks (< 10% accuracy). For MNIST, the NeuralFP Parameters for FGSM, SPSAare (ε, N) = (0.1, 10) and (ε, N) = (0.05, 20) for the BIM-b, CW-L2 attacks. For CIFAR-10, theParameters are set at (ε, N) = (0.003, 30) across attacks.
Table 7: MiniImagenet-20 Model UsedLayer	ParametersConvolution + ReLU + BatchNorm	5× 5 ×32MaxPool	2×2Convolution + ReLU + BatchNorm	5 × 5 × 64MaxPool	2×2Fully Connected + ReLU + BatchNorm	200Fully Connected + ReLU + BatchNorm	200Softmax	10Table 8: MNIST Model UsedThe fingerprint in (6) is an example of such an η . However, if λ is large enough, that is:hη, ei = δ-max,	(14)(e.g. the fingerprint in (7)), for all negative examples (x- , -1) the class prediction will alwayschange (except for the x- that lie exactly δ-max from the hyperplane):signf(x-) = -1,	signf(x- +η) = +1,	(15)Note that if η has a component smaller (or larger) than δ±min , it will exclude fewer (more) examples,e.g. those that lie closer to (farther from) the hyperplane. Similar observations hold for fingerprints(8) and (9) and the positive examples x+. Hence, it follows that for any x that lies too close to thehyperplane (closer than δ±min), or too far (farther than δ±max), the model output after adding the fourfingerprints will never perfectly correspond to their behavior on examples x from the data distribution.
Table 8: MNIST Model UsedThe fingerprint in (6) is an example of such an η . However, if λ is large enough, that is:hη, ei = δ-max,	(14)(e.g. the fingerprint in (7)), for all negative examples (x- , -1) the class prediction will alwayschange (except for the x- that lie exactly δ-max from the hyperplane):signf(x-) = -1,	signf(x- +η) = +1,	(15)Note that if η has a component smaller (or larger) than δ±min , it will exclude fewer (more) examples,e.g. those that lie closer to (farther from) the hyperplane. Similar observations hold for fingerprints(8) and (9) and the positive examples x+. Hence, it follows that for any x that lies too close to thehyperplane (closer than δ±min), or too far (farther than δ±max), the model output after adding the fourfingerprints will never perfectly correspond to their behavior on examples x from the data distribution.
Table 9: CIFAR Model UsedHere p is a random variable with Pr(p = 1) = 0.5 and Pr(p = -1) = 0.5 that is resampledfor each ∆xi, making it prohibitively hard for a brute-force attacker to guess. For this NFP, weachieve AUC-ROCs of > 95% across attacks with N = 30, ε = 0.05, without extensive tuning. Thehigh model-capacity of neural networks allows to learn such complex patterns, that can be hard toreverse engineer. This also indicates that the proposed approach is robust and the specific choiceof fingerprints or the distributions they are sampled from do not actually influence the detectionperformance to a large extent.
Table 10: DeteCtion AUC-ROC for NeuralFP,whitebox-LID against whitebox-attackers (know modelf(x; θ), but not fingerprints; see SeCtion 2.1), on MNIST, CIFAR-10 tasks on test-set (“real”) andCorresponding adversarial (“fake”) samples (1328 pre-test samples eaCh). NeuralFP outperformsthe baselines (LID, KD, BU) on MNIST and CIFAR-10 aCross all attaCks, exCept CW-L2 whereit performs Comparably. A possibly explanation for LID’s improved performanCe against stronger,iterative attaCks is gradient masking Athalye et al. (2018).
Table 11: Detection AUC-ROC for NeuralFP against blackbox-attackers (know dataset but notmodel or fingerprints), on MNIST, CIFAR-10 tasks on test-set (“real”) and corresponding blackboxadversarial (“fake”) samples (1328 pre-test samples each). NeuralFP achieves near perfect AUC-ROC scores in this setting against FGM. Iterative attacks are known to not transfer well in theblackbox setting. For CIFAR-10, the hyperparameters are N = 30, ε = 0.003 and for MNIST, thehyperparameters are N = 10, ε = 0.03. We did not tune the parameters because this setting in itselfachieved near perfect detection rates.
Table 13: The detection AUC-ROC for NeuralFP against adaptive-PGD (112 pre-test samples each).
Table 14: The detection AUC-ROC for NeuralFP against Adaptive-CW-L2 with varying hyperparam-eter settings.
Table 15: The detection AUC-ROC for NeuralFP against SPSA with varying hyperparameter settings.
