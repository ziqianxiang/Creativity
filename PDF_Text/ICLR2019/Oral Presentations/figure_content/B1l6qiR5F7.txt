Figure 1: Binary parse tree inferred by our model (left) and its corresponding ground-truth (right).
Figure 2: Correspondences between a constituency parse tree and the hidden states of the proposedON-LSTM. A sequence of tokens S = (x1, x2, x3) and its corresponding constituency tree are illus-trated in (a). We provide a block view of the tree structure in (b), where both S and VP nodes spanmore than one time step. The representation for high-ranking nodes should be relatively consistentacross multiple time steps. (c) Visualization of the update frequency of groups of hidden state neu-rons. At each time step, given the input word, dark grey blocks are completely updated while lightgrey blocks are partially updated. The three groups of neurons have different update frequencies.
Figure 3: Test accuracy of the models, trained on short sequences (â‰¤ 6) in logic data. The horizontalaxis indicates the length of the sequence, and the vertical axis indicates the accuracy of modelsperformance on the corresponding test set.
