Figure 1: Self-attention computes attention weights by comparing all pairs of elements to each other(a) while as dynamic convolutions predict separate kernels for each time-step (b).
Figure 2: Illustration of self-attention, lightweight convolutions and dynamic convolutions.
