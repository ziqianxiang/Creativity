Figure 1: The gaps ∣γ2 - Y| and ∣ρ2 - ρ∣ versus the depth L, where Y and ρ2 (which are dependenton L) are as in Section 2.2, and γ and ρ (the infinite-L limits of γ2 and ρ2) are from Eq. (2) and(3). Here all activations are tanh, T2 = 1.2, σ2 = 0.211, σW = 2.312 ‹。54ɑ乂 ≈ 2.806, andτ2 ≈ 0.4276, which satisfies τ2 = E{σ (Tz)2 }. From left to right: a0.9, α1.0 and α = 1.5.
Figure 2: The mapping γ → G (γ, P) for T2 = 1 and β = 5 (blue), β = 2.7 (red), β = 0.8 (green).
Figure 3: γ and ρ versus β, as solved with Eq. (2) and (3). The vertical dotted line is β = σW2 ,max.
Figure 4:	Test loss ∣∣x - x∣∣2 / ||x『 of the schemes from Table 1. Left: the setting with 夕0 = tanh(Setting 1). Right: the setting where 夕0 is the identity (Setting 2).
Figure 5:	Schematic diagram of the weight-tied autoencoder as described in Section 2.1. Left: theencoder. Center: the decoder. Right: dimensions of the corresponding vectors.
Figure 6: The agreement among γ', γ' and γ, and among p`, p` and ρ, for ' = 2,…,51 andGaussian weights. The setting is described in Appendix B. We take a single run for the simulationIof the autoencoder. Here n0 = 500 (left) and n02000 (right).
Figure 7: Quantile-quantile plots for the empirical distribution of z, described in Appendix B, versusthe standard Gaussian distribution. Here n0 = 500 (left) and n0 = 2000 (right).
Figure 8:	The agreement among Y', γ' and γ, and among p`, p` and ρ, for ' = 2,..., 51, for differentdistributions of the weights. The setting is described in Appendix B. We take a single run for thesimulation of the autoencoder. Here n0 = 2000.
Figure 9:	Quantile-quantile plots for the empirical distribution of z, described in Appendix B, versusthe standard Gaussian distribution, for different distributions of the weights. Here n0 = 2000.
Figure 10:	The reconstructions by the schemes from Table 1, as described in Appendix D.1, inSetting 1 (i.e.,夕0 = tanh). From the top row: original images, reconstructions from Scheme 1,4 and 2. We omit the reconstructions from other schemes, since they are almost identical to thoseof Scheme 2. For each digit/letter category, the image is selected from the test set by ranking thereconstruction loss, averaged across Scheme 1 and 4, and picking one at the 75% percentile.
Figure 11:	Description similar to Fig. 10. The chosen images are the 25% percentile.
Figure 12: Description similar to Fig. 10. The setting is Setting 2 (i.e., identity 夕0). The reconstruc-tions are of Scheme 1, 4, 7 and 2. The chosen images are the 75% percentile. The ranking is byaveraging over Scheme 1, 4, and 7.
Figure 13: Description similar to Fig. 10. The setting is Setting 2 (i.e., identity 夕0). The reconstruc-tions are of Scheme 1, 4, 7 and 2. The chosen images are the 25% percentile. The ranking is byaveraging over Scheme 1, 4, and 7.
Figure 14:	The test loss for various pairs σW2 , σb2 at different iterations, as described in AppendixD.2, in Setting 1 (i.e.,夕0 = tanh). The horizontal axis is σ2, and the vertical axis is σW. Firstrow: φ = σ = ReLU; second row:夕=ReLU and σ = tanh; third row:夕=σ = tanh; fourthrow:夕=tanh and σ = ReLU. Red indicates higher loss, and black indicates lower loss. Whiteindicates a numerical error.
Figure 15:	The test loss for various pairs σW2 , σb2 at different iterations, as described in AppendixD.2, in Setting 2 (i.e.,夕0 is the identity). The description is similar to Fig. 14. White indicateseither a very large value or a numerical error, which are due to the fact that the chosen learning rateis not sufficiently small. Note that in this setting with 夕=ReLU, the training process is trapped innumerical errors for σW2 > 2 as we expect, and hence the test loss at σW2 > 2 is not plotted.
