Figure 1: An unrolled simulator as a model for protein structure. NEMO combines a neuralenergy function for coarse protein structure, a stochastic simulator based on Langevin dynamicswith learned (amortized) initialization, and an atomic imputation network to build atomic coordinateoutput from sequence information. It is trained end-to-end by backpropagating through the unrolledfolding simulation.
Figure 2: A neural energy function models coarse grained structure and is sampled by internalcoordinate dynamics. (A) The energy function is formulated as a Markov Random Field withstructure-based features and sequence-based weights computed by neural networks (Figure 6). (B)To rapidly sample low-energy configurations, the Langevin dynamics simulator leverages both (i) aninternal coordinate parameterization, which is more effective for global rearrangements, and (ii) aCartesian parameterization, which is more effective for localized structural refinement. (C) The basefeatures of the structure network are rotationally and translationally invariant internal coordinates(not shown), pairwise distances, and pairwise orientations.
Figure 3: A transform integrator simulates Langevin dynamics in a more favorable coordinate system(e.g. internal coordinates z) directly in terms of the untransformed state variables (e.g. Cartesian x).
Figure 4: Model generalizes and outperforms end-to-end baseline for unseen fold topologies.
Figure 5: Examples of fold generalization at topology and architecture level. These predictedstructures show a range of prediction accuracy for structural generalization (C and A) tasks, with theTM-score comparing the top ranked 3D-Jury pick against the target. The largest clusters are the threemost-populated clusters derived from 100 models per domain with a within-cluster cutoff of TM >0.5. CATH IDs: 2oy8A03; 5c3uA02; 2y6xA00; 3cimB00; 4ykaC00; 2f09A00; 3i5qA02; 2ayxA01.
Figure 6: Model schematic. The model generates an atomic structure X (top right) from sequenceinformation s (top left) via 3 steps: First, a sequence network takes in the sequence informations, processes it with a combination of 1D, 2D, and graph convolutions (MPNN, bottom left), andoutputs energy function weights l as well as simulator hyperparameters (top center). Second, thesimulator iteratively modifies the structure via Langevin dynamics based on the gradient of theenergy landscape (Forces, bottom center). Third, the imputation network constructs predicted atomiccoordinates X from the final simulator time step x(T) . During training, the true atomic coordinatesX(Data), predicted atomic coordinates X, simulator trajectory x(1),...,x(T), and secondary structurepredictions SS(Model) feed into a composite loss function (Loss, bottom right), which is then optimizedvia backpropagation.
Figure 7: Component architectures. (Left) The energy function is the inner product of sequence-based weights and structure-based features. A combination of low- and high-level features capturemulti-scale constraints on structure. (Center) The structure network is a lightweight convolutionalnetwork operating on both 1D (backbone) and 2D (interaction) features. (Right) Convolutional neuralnetwork modules used for sequence processing are composed of residual blocks that interleave spatialconvolutions with 1x1 convolutions.
Figure 8: Accounting for second order errors is essential for internal coordinate dynamics.
Figure 9: Chaos impedes meta-learning for gradient descent in a well. (a) Gradient descent of aparticle in a well with initial conditions x(0) and step size α. (b) Orbit diagrams visualize long-termdynamics from iterations 1000 to 2000 of the position X (top) and the gradient ddχ(0) (bottom). Whenthe step size α is small, these dynamics converge to a periodic orbit over 2k values where 0 ≤ k<∞.
Figure 10: Sampling speed. Per-protein sampling times for various batch sizes across NEMO andone of the RNN baselines on a single Tesla M40 GPU with 12GB memory and 20 cores. For allresults in the main paper, 100 models were sampled per protein followed by consensus clusteringwith 3D-jury, adding an additional factor of 102 cost between NEMO and the RNN.
Figure 11: Predictive performance of structures generated by the sequence-only model. (left)Structures in the test set are hierarchically organized by CATH classification. Groups further up thetree are broader generalization. (center-left) Ensembles of models with increasing certainty tend tohave a better average TM-score. (center-right) TM-score of 3D-jury-selected models versus distancefrom the training data. Withheld (right) Comparing the energy-based model with and without profiles.
Figure 12: Generalization results upon re-stratification. Profile-based model.
Figure 13: RNN baseline performance for different hyperparameters. Predictive performance ofthe two-layer bidirectional LSTM baseline models across a range ofhidden unit dimensions comparedto the energy model.
