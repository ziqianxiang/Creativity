Figure 1: Architecture of a Bayesianneural network. Computation is dividedinto (a) propagation of activations (a)from an input x and (b) computation ofa log-likelihood function L for outputsy. Weights are represented as high di-mensional variational distributions (blue)that induce distributions over activations(yellow). MCVI computes using sam-ples (dots); our method propagates a fulldistribution.
Figure 2: Approximation of hhjhli usingan asymptote and Gaussian correction for(a) Heaviside and (b) ReLU non-linearities.
Figure 3: Empirical accuracy of our approximation on toy 1-dimensional data. (a) We train a 2 layer ReLUnetwork to perform heteroscedastic regression on the dataset shown in (b) and obtain the fit shown in blue. (c)The output distributions for the activation units m and ` evaluated at x = 0.25 are in excellent agreement withMonte Carlo (MC) integration with a large number (20k) of samples both before and after training.
Figure 4: Runtime performance of VI methods.
Figure 5: Comparison of converged test log-likelihood with amanually tuned prior variance (orange) or empirical Bayes (blue).
Figure 6: Empirical accuracy of our approximation for 5-layer networks trained analogously tofigure 3. Progressively narrower networks of (a) 125 unit (b) 25 unit and (c) 5 unit are trained and ourCLT-based approximation is only seen to significantly break down in the 5-unit case. (d) Qualitativeverification of our approximation applied to an architecture with skip connections (orange).
Figure 7:	Performance of MCVI vs rMCVI. (a) Gradient variance for the model shown in figure 3 with batchsize B = 1. Variance values are normalized such that MCVI with 1 sample appears at unit relative variance.
Figure 8:	Learning trajectories for the models from table 2.
