Figure 1:	Classification of a standard ResNet-50 of (a) a texture image (elephant skin: only texturecues); (b) a normal image of a cat (with both shape and texture cues), and (c) an image with atexture-shape cue conflict, generated by style transfer between the first two images.
Figure 2:	Accuracies and example stimuli for five different experiments without cue conflict.
Figure 3: Visualisation of Stylized-ImageNet (SIN), created by applying AdaIN style transfer toImageNet images. Left: randomly selected ImageNet image of class ring-tailed lemur.
Figure 4: Classification resultsfor human observers (red circles)and ImageNet-trained networksAlexNet (purple diamonds), VGG-16 (blue triangles), GoogLeNet(turquoise circles) and ResNet-50(grey squares). Shape vs. tex-ture biases for stimuli with cueconflict (sorted by human shapebias). Within the responses thatcorresponded to either the correcttexture or correct shape category,the fractions of texture and shapedecisions are depicted in the mainplot (averages visualised by verticallines). On the right side, smallbarplots display the proportion ofcorrect decisions (either texture orshape correctly recognised) as afraction of all trials. Similar results
Figure 5: Shape vs. texture bi-ases for stimuli with a texture-shapecue conflict after training ResNet-50 on Stylized-ImageNet (orangesquares) and on ImageNet (greysquares). Plotting conventions andhuman data (red circles) for com-parison are identical to Figure 4.
Figure 6: Classification accuracy on parametrically distorted images. ResNet-50 trained on Stylized-ImageNet (SIN) is more robust towards distortions than the same network trained on ImageNet (IN).
Figure 7: Visualisation of stimuli in data sets. Top two rows: content and texture images. Bottomrows: cue conflict stimuli generated from the texture and content images above (silhouettes filledwith rotated textures; style transfer stimuli).
Figure 8: Visualisation of image distortions. One exemplary image (class bird, original imagein colour at the top left) is manipulated as follows. From left to right: additive uniform noise, lowcontrast, high-pass filtering, low-pass filtering. In the row below, a greyscale version for comparison;the other manipulations from left to right are: Eidolon manipulations I, II and III as well as phasenoise. Figure adapted from Geirhos et al. (2018) with the authorsâ€™ permission.
Figure 9:	Accuracies and example stimuli for five different experiments without cue conflict, com-paring training on ImageNet (IN) to training on Stylized-ImageNet (SIN).
Figure 10:	Classification results for human observers (red circles) and ImageNet-trained networksAlexNet (purple diamonds), VGG-16 (blue triangles), GoogLeNet (turquoise circles) and ResNet-50 (grey squares) on stimuli with a texture-shape cue conflict generated with style transfer, andbiased rather than neutral instructions to human observers. Plotting conventions and CNN data asin Figure 4.
Figure 11: Texture vs shape biases on of AlexNet and VGG-16 after training on Stylized-ImageNet.
Figure 12:	Classification results for human observers and CNNs on stimuli with a texture-silhouettecue conflict (filled silhouette experiment). Plotting conventions as in Figures 4 and 5.
Figure 13:	The texture bias on cue conflict stimuli is not specific to ImageNet-trained networks (left)and also occurs in very deep, wide and compressed networks (right).
