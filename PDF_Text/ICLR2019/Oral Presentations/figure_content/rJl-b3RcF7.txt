Figure 1: The iteration at which early-stopping would occur (left) and the test accuracy at that iteration(right) of the Lenet architecture for MNIST and the Conv-2, Conv-4, and Conv-6 architectures forCIFAR10 (see Figure 2) when trained starting at various sizes. Dashed lines are randomly sampledsparse networks (average of ten trials). Solid lines are winning tickets (average of five trials).
Figure 2: Architectures tested in this paper. Convolutions are 3x3. Lenet is from LeCun et al. (1998).
Figure 3:	Test accuracy on Lenet (iterative pruning) as training proceeds. Each curve is the averageof five trials. Labels are Pm—the fraction of weights remaining in the network after pruning. Errorbars are the minimum and maximum of any trial.
Figure 4:	Early-stopping iteration and accuracy of Lenet under one-shot and iterative pruning.
Figure 5: Early-stopping iteration and test and training accuracy of the Conv-2/4/6 architectures wheniteratively pruned and when randomly reinitialized. Each solid line is the average of five trials; eachdashed line is the average of fifteen reinitializations (three per trial). The bottom right graph plots testaccuracy of winning tickets at iterations corresponding to the last iteration of training for the originalnetwork (20,000 for Conv-2, 25,000 for Conv-4, and 30,000 for Conv-6); at this iteration, trainingaccuracy ≈ 100% for Pm ≥ 2% for winning tickets (see Appendix D).
Figure 6: Early-stopping iteration and test accuracy at early-stopping of Conv-2/4/6 When iterativelypruned and trained With dropout. The dashed lines are the same netWorks trained Without dropout(the solid lines in Figure 5). Learning rates are 0.0003 for Conv-2 and 0.0002 for Conv-4 and Conv-6.
Figure 7:	Test accuracy (at 30K, 60K, and 112K iterations) of VGG-19 When iteratively pruned.
Figure 8:	Test accuracy (at 10K, 20K, and 30K iterations) of Resnet-18 when iteratively pruned.
Figure 9: The early-stopping iteration and accuracy at early-stopping of the iterative lottery ticketexperiment on the Lenet architecture when iteratively pruned using the resetting and continuedtraining strategies.
Figure 10: The early-stopping iteration and accuracy at early-stopping of the iterative lottery ticketexperiment on the Conv-2, Conv-4, and Conv-6 architectures when iteratively pruned using theresetting and continued training strategies.
Figure 11:	The validation loss data corresponding to Figure 3, i.e., the validation loss as trainingprogresses for several different levels of pruning in the iterative pruning experiment. Each line isthe average of five training runs at the same level of iterative pruning; the labels are the percentageof weights from the original network that remain after pruning. Each network was trained withAdam at a learning rate of 0.0012. The left graph shows winning tickets that learn increasingly fasterthan the original network and reach lower loss. The middle graph shows winning tickets that learnincreasingly slower after the fastest early-stopping time has been reached. The right graph contraststhe loss of winning tickets to the loss of randomly reinitialized networks.
Figure 12:	Figure 4 augmented with a graph of the training accuracy at the end of 50,000 iterations.
Figure 13: Figure 5 augmented with a graph of the training accuracy at the end of the training process.
Figure 14: The test accuracy at the final iteration for each of the networks studied in this paper.
Figure 15: The distribution of initializations in winning tickets pruned to the levels specified in thetitles of each plot. The blue, orange, and green lines show the distributions for the first hidden layer,second hidden layer, and output layer of the Lenet architecture for MNIST when trained with theadam optimizer and the hyperparameters used in 2. The distributions have been normalized so thatthe area under each curve is 1.
Figure 16: Same as Figure 15 where the network is trained with SGD at rate 0.8.
Figure 17: The performance of the winning tickets of the Lenet architecture for MNIST when thelayers are randomly reinitialized from the distribution of initializations contained in the winningticket of the corresponding size.
Figure 18: The performance of the winning tickets of the Lenet architecture for MNIST whenmagnitude pruning is performed before the network is ever trained. The network is subsequentlytrained with adam.
Figure 19: Between the first and last training iteration of the unpruned network, the magnitude bywhich weights in the network change. The blue line shows the distribution of magnitudes for weightsthat are not in the eventual winning ticket; the orange line shows the distribution of magnitudes forweights that are in the eventual winning ticket.
Figure 20:	Between the first and last training iteration of the unpruned network, the magnitude bywhich weights move away from 0. The blue line shows the distribution of magnitudes for weightsthat are not in the eventual winning ticket; the orange line shows the distribution of magnitudes forweights that are in the eventual winning ticket.
Figure 21:	The fraction of incoming connections that survive the pruning process for each node ineach layer of the Lenet architecture for MNIST as trained with adam.
Figure 22: Same as Figure 21 where the network is trained with SGD at rate 0.8.
Figure 23:	The fraction of outgoing connections that survive the pruning process for each node ineach layer of the Lenet architecture for MNIST as trained with adam. The blue, orange, and greenlines are the outgoing connections from the input layer, first hidden layer, and second hidden layer,respectively.
Figure 24:	Same as Figure 23 where the network is trained with SGD at rate 0.8.
Figure 25: The performance of the winning tickets of the Lenet architecture for MNIST whenGaussian noise is added to the initializations. The standard deviations of the noise distributions foreach layer are a multiple of the standard deviations of the initialization distributions; in this Figure,we consider multiples 0.5, 1, 2, and 3.
Figure 26: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Lenet architecture trained with MNIST using the Adam optimizer at variouslearning rates. Each line represents a different learning rate.
Figure 27: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Lenet architecture trained with MNIST using stochastic gradient descent atvarious learning rates.
Figure 28: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Lenet architecture trained with MNIST using stochastic gradient descentwith momentum (0.9) at various learning rates.
Figure 29: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment when pruned at different rates. Each line represents a different pruning rate—thepercentage of lowest-magnitude weights that are pruned from each layer after each training iteration.
Figure 30: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment initialized with Gaussian distributions with various standard deviations. Each lineis a different standard deviation for a Gaussian distribution centered at 0.
Figure 31: The early-stopping iteration and validation accuracy at at that iteration of the iterativelottery ticket experiment on the Lenet architecture with various layer sizes. The label for each lineis the size of the first and second hidden layers of the network. All networks had Gaussian Glorotinitialization and were optimized with Adam (learning rate 0.0012). Note that the x-axis of this plotcharts the number of weights remaining, while all other graphs in this section have charted the percentof weights remaining.
Figure 32: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) architectures trainedusing the Adam optimizer at various learning rates. Each line represents a different learning rate.
Figure 33: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) architectures trainedusing SGD at various learning rates. Each line represents a different learning rate. The legend foreach pair of graphs is above the graphs.
Figure 34: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) architectures trainedusing SGD with momentum (0.9) at various learning rates. Each line represents a different learningrate. The legend for each pair of graphs is above the graphs. Lines that are unstable and contain largeerror bars (large vertical lines) indicate that some experiments failed to learn effectively, leading tovery low accuracy and very high early-stopping times; these experiments reduce the averages that thelines trace and lead to much wider error bars.
Figure 35: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) architectures with aniterative pruning rate of 20% for fully-connected layers. Each line represents a different iterativepruning rate for convolutional layers.
Figure 36: The early-stopping iteration and validation accuracy at that iteration of the iterative lotteryticket experiment on the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) architectures trainedusing dropout and the Adam optimizer at various learning rates. Each line represents a differentlearning rate.
Figure 37: Early-stopping iteration and accuracy of the Conv-2 (top), Conv-4 (middle), and Conv-6(bottom) networks when only convolutions are pruned, only fully-connected layers are pruned, andboth are pruned. The x-axis measures the number of parameters remaining, making it possible tosee the relative contributions to the overall network made by pruning FC layers and convolutionsindividually.
Figure 38:	Validation accuracy (at 30K, 60K, and 112K iterations) of VGG-19 When iterativelypruned with global (solid) and layer-wise (dashed) pruning.
Figure 39:	Validation accuracy (at 10K, 20K, and 30K iterations) of Resnet-18 When iterativelypruned With global (solid) and layer-Wise (dashed) pruning.
Figure 40: Test accuracy (at 30K, 60K, and 112K iterations) of VGG-19 when iteratively pruned withlayer-Wise pruning. This is the same as Figure 7, except With layer-Wise pruning rather than globalpruning.
Figure 41:	Test accuracy (at 10K, 20K, and 30K iterations) of Resnet-18 When iteratively pruned Withlayer-Wise pruning. This is the same as Figure 8 except With layer-Wise pruning rather than globalpruning.
Figure 42:	Validation accuracy (at 10K, 20K, and 30K iterations) of Resnet-18 when iterativelypruned and trained with various learning rates.
Figure 43:	Validation accuracy (at 30K, 60K, and 112K iterations) of VGG-19 when iterativelypruned and trained with various learning rates.
Figure 44:	Validation accuracy (at 10K, 20K, and 30K iterations) of Resnet-18 when iterativelypruned and trained with varying amounts of warmup at learning rate 0.03.
Figure 45:	Validation accuracy (at 30K, 60K, and 112K iterations) of VGG-19 When iterativelypruned and trained with varying amounts of warmup at learning rate 0.1.
