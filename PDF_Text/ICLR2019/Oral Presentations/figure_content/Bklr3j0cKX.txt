Figure 1:	The base encoder model in thecontext of image data. An image (in thiscase) is encoded using a convnet until reach-ing a feature map of M × M feature vec-tors corresponding to M × M input patches.
Figure 2:	Deep InfoMax (DIM) with aglobal MI(X; Y ) objective. Here, we passboth the high-level feature vector, Y , and thelower-level M ×M feature map (see Figure 1)through a discriminator to get the score. Fakesamples are drawn by combining the samefeature vector with a M × M feature mapfrom another image.
Figure 3: Maximizing mutual informationbetween local features and global features.
Figure 4:	Scatter plots of MI(x; y) versusJSD(p(x, y)||p(x)p(y))with discrete inputs and a given ran-domized and sparse joint distribution,p(x, y). 8 × 8 indicates a square jointdistribution with 8 rows and 8 columns.
Figure 5:	Concat-and-convolve architec-ture. The global feature vector is concate-nated with the lower-level feature map at ev-ery location. A 1 × 1 convolutional discrimi-nator is then used to score the “real” featuremap / feature vector pair, while the “fake” pairis produced by pairing the feature vector witha feature map from another image.
Figure 6:	Encode-and-dot-product archi-tecture. The global feature vector is en-coded using a fully-connected network, andthe lower-level feature map is encoded us-ing 1x1 convolutions, but with the same num-ber of output features. We then take the dot-product between the feature at each locationof the feature map encoding and the encodedglobal vector for scores.
Figure 7:	Matching the output of the en-coder to a prior. “Real” samples are drawnfrom a prior while “fake” samples from the en-coder output are sent to a discriminator. Thediscriminator is trained to distinguish between(classify) these sets of samples. The encoderis trained to “fool” the discriminator.
Figure 8: Nearest-neighbor using the L1 distance on the encoded Tiny ImageNet images, withDIM(G) and DIM(L). The images on the far left are randomly-selected reference images (query)from the training set and the four images their nearest-neighbor from the test set as measured in therepresentation, sorted by proximity. The nearest neighbors from DIM(L) are much more interpretablethan those with the purely global objective.
Figure 9: Classification accuracies (left: global representation, Y , right: convolutional layer) forCIFAR10, first training DIM, then training a classifier for 1000 epochs, keeping the encoder fixed.
Figure 10: Results from the ablation studies with DIM on CIFAR10. Values calculated are points onthe grid, and the heatmaps were derived by bilinear interpolation. Heatmaps were thresholded at theminimum value (or maximum for NDM) for visual clarity. Highest (or lowest) value is marked onthe grid. NDM here was measured without the sigmoid function.
Figure 11: Ablation study on CelebA over the global and local parameters, α and β . The classificationtask is multinomial, so provided is the average, minimum, and maximum class accuracies acrossattibutes. While the local objective is crucial, the global objective plays a stronger role here than withother datasets.
Figure 12:	A schematic of learning theNeural Dependency Measure. For agiven batch of inputs, we encode thisinto a set of representations. We thenshuffle each feature (dimension of thefeature vector) across the batch axis.
Figure 13:	Neural Dependency Mea-sures (NDMs) for various β-VAE (Alemiet al., 2016; Higgins et al., 2016) models(0.1, 0.5, 1.0, 1.5, 2.0, 4.0). Error bars areprovided over five runs of each VAE and es-timating NDM with 10 different networks.
Figure 14: Visualizing model behaviour when computing global features with and without occlusion,for NCE-based DIM. The images in each block of images come in pairs. The left image in eachpair shows the model input when computing the global feature vector. The right image shows theNCE loss suffered by the score between that global feature vector and the local feature vector at eachlocation in the 8 × 8 local feature map computed from the unoccluded image. This loss is equal tominus the value in Equation 5. With occluded inputs, this loss tends to be highest for local featureswith receptive fields that overlap the occluded region.
Figure 15: Histograms depiCting the disCriminator’s unnormalized output distribution for the standardGAN, GAN with - log D loss, Least Squares GAN, Wasserstein GAN and our proposed methodwhen trained with a 50:1 training ratio.
Figure 16: Samples of generated results used to get scores in Table 11. For every methods, the sampleare generated after 100 epochs and the models are the same. Qualitative results from these threemethods shoW no qualitative difference.
