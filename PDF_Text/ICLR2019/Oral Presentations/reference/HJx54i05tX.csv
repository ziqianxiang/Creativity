title,year,conference
 Characteristics of random nets of analog neuron-like elements,1972, IEEE Transactionson systems
 Statistical neurodynamics of deep net-works: Geometry of signal spaces,2018, arXiv preprint arXiv:1808
 Provable bounds for learning somedeep representations,2014, In International Conference on Machine Learning
 Comparing dynamics: Deep neuralnetworks versus glassy systems,2018, arXiv preprint arXiv:1803
 Generalized denoising auto-encodersas generative models,2013, In Advances in Neural Information Processing Systems
 State evolution for approximatemessage passing with non-separable functions,2017, arXiv preprint arXiv:1708
 An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model,2014, Communications in Mathematical Physics
 Exact information propagation through fully-connectedfeed forward neural networks,2018, arXiv preprint arXiv:1806
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, arXiv preprint arXiv:1805
 Emnist: an extension ofmnist to handwritten letters,2017, arXiv preprint arXiv:1702
 Inference in deep networks in high di-mensions,1884, In 2018 IEEE International Symposium on Information Theory (ISIT)
 Towards understanding theinvertibility of convolutional neural networks,2017, arXiv preprint arXiv:1705
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 On the selection of initialization and activa-tion function for deep neural networks,2018, arXiv preprint arXiv:1805
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Reducing the dimensionality of data with neuralnetworks,2006, Science
 Exploring the function space of deep-learning machines,2018, Physical ReviewLetters
 Multi-layer generalizedlinear estimation,2017, In 2017 IEEE International Symposium on Information Theory (ISIT)
 A mean field view of the landscape oftwo-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Refinements of universal approximation results for deep belief net-works and restricted boltzmann machines,2011, Neural COmputatiOn
 A deep learning approach to structured signalrecovery,2015, In 2015 53rd Annual AllertOn COnference On COmmunicatiOn
 Autoencoders learn generative linearmodels,2018, arXiv preprint arXiv:1806
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural infOrmatiOnprOcessing systems
 Sparse coding and autoencoders,2017, arXiv preprint arXiv:1708
 Additivity of information in multilayer networks via additive gaussian noise trans-forms,2017, In 2017 55th Annual AllertOn COnference On COmmunicatiOn
 Feature discovery by competitive learning,1985, COgnitive science
 Extend-ing the framework of equilibrium propagation to general dynamics,2018, 2018
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 Mean field analysis of neural networks,2018, arXivpreprint arXiv:1805
 A Bayesian perspective on generalization and stochastic gradientdescent,2018, 2018
 Deep mean field theory: Layerwise variance and width variationas methods to control gradient explosion,2018, 2018
 Mean field residual networks: On the edge of chaos,2017, InAdvances in neural information processing systems
