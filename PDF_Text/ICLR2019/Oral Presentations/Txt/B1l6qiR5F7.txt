Published as a conference paper at ICLR 2019
Ordered Neurons:
Integrating Tree Structures
into Recurrent Neural Networks
Yikang Shen*
Mila/Universite de Montreal and Microsoft Research
Montreal, Canada
Shawn Tan*
Mila/UniverSite de Montreal
Montreal, Canada
Alessandro Sordoni
Microsoft Research
Montreal, Canada
Aaron Courville
Mila/UniverSite de Montreal
Montreal, Canada
Ab stract
Natural language is hierarchically structured: smaller units (e.g., phrases) are
nested within larger units (e.g., clauses). When a larger constituent ends, all of
the smaller constituents that are nested within it must also be closed. While the
standard LSTM architecture allows different neurons to track information at dif-
ferent time scales, it does not have an explicit bias towards modeling a hierarchy
of constituents. This paper proposes to add such an inductive bias by ordering the
neurons; a vector of master input and forget gates ensures that when a given neu-
ron is updated, all the neurons that follow it in the ordering are also updated. Our
novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good
performance on four different tasks: language modeling, unsupervised parsing,
targeted syntactic evaluation, and logical inference* 1.
1	Introduction
Natural language has a sequential overt form as spoken and written, but the underlying structure of
language is not strictly sequential. This structure is usually tree-like. Linguists agree on a set of
rules, or syntax, that determine this structure (Chomsky, 1956; 1965; Sandra & Taft, 2014) and dic-
tate how single words compose to form meaningful larger units, also called “constituents” (Koopman
et al., 2013). The human brain can also implicitly acquire the latent structure of language (Dehaene
et al., 2015): during language acquisition, children are not given annotated parse trees. This obser-
vation brings more interest in latent structure induction with artificial neural network approaches,
which are inspired by information processing and communication patterns in biological nervous
systems. From a practical point of view, integrating a tree structure into a neural network language
model may be important for multiple reasons:
(i)	to obtain a hierarchical representation with increasing levels of abstraction, a key feature
of deep neural networks (Bengio et al., 2009; LeCun et al., 2015; Schmidhuber, 2015);
(ii)	to model the compositional effects of language (Koopman et al., 2013; Socher et al., 2013)
and help with the long-term dependency problem (Bengio et al., 2009; Tai et al., 2015) by
providing shortcuts for gradient backpropagation (Chung et al., 2016);
(iii)	to improve generalization via a better inductive bias and at the same time potentially reduc-
ing the need of a large amount of training data.
The study of deep neural network techniques that can infer and use tree structures to form bet-
ter representations of natural language sentences has received a great deal of attention in recent
* Equal contribution. {yi-kang.shen,jing.shan.shawn.tan}@umontreal.ca.
1The code can be found at https://github.com/yikangshen/Ordered-Neurons.
1
Published as a conference paper at ICLR 2019
Interest expense in the 1988 third quarter was 75.3 million
Figure 1: Binary parse tree inferred by our model (left) and its corresponding ground-truth (right).
Interest expense in the 1988 third quarter was 75.3 million
years (Bowman et al., 2016; Yogatama et al., 2016; Shen et al., 2017; Jacob et al., 2018; Choi et al.,
2018; Williams et al., 2018; Shi et al., 2018).
Given a sentence, one straightforward way of predicting the corresponding latent tree structure is
through a supervised syntactic parser. Trees produced by these parsers have been used to guide the
composition of word semantics into sentence semantics (Socher et al., 2013; Bowman et al., 2015),
or even to help next word prediction given previous words (Wu et al., 2017). However, supervised
parsers are limiting for several reasons: i) few languages have comprehensive annotated data for
supervised parser training; ii) in some domains, syntax rules tend to be broken (e.g. in tweets); and
iii) languages change over time with use, so syntax rules may evolve.
On the other hand, grammar induction, defined as the task of learning the syntactic structure
from raw corpora without access to expert-labeled data, remains an open problem. Many such
recent attempts suffer from inducing a trivial structure (e.g., a left-branching or right-branching
tree (Williams et al., 2018)), or encounter difficulties in training caused by learning branching poli-
cies with Reinforcement Learning (RL) (Yogatama et al., 2016). Furthermore, some methods are
relatively complex to implement and train, like the PRPN model proposed in Shen et al. (2017).
Recurrent neural networks (RNNs) have proven highly effective at the task of language modeling
(Merity et al., 2017; Melis et al., 2017). RNNs explicitly impose a chain structure on the data.
This assumption may seem at odds with the latent non-sequential structure of language and may
pose several difficulties for the processing of natural language data with deep learning methods,
giving rise to problems such as capturing long-term dependencies (Bengio et al., 2009), achieving
good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc. Meanwhile,
some evidence exists that LSTMs with sufficient capacity potentially implement syntactic processing
mechanisms by encoding the tree structure implicitly, as shown by Gulordava et al. (2018); Kuncoro
et al. (2018) and very recently by Lakretz et al. (2019). We believe that the following question
remains: Can better models of language be obtained by architectures equipped with an inductive
bias towards learning such latent tree structures?
In this work, we introduce ordered neurons, a new inductive bias for recurrent neural networks.
This inductive bias promotes differentiation of the life cycle of information stored inside each neu-
ron: high-ranking neurons will store long-term information which is kept for a large number of steps,
while low-ranking neurons will store short-term information that can be rapidly forgotten. To avoid a
strict division between high-ranking and low-ranking neurons, we propose a new activation function,
the cumulative softmax, or cumax(), to actively allocate neurons to store long/short-term informa-
tion. We use the cumax() function to produce a vector of master input and forget gates ensuring that
when a given neuron is updated (erased), all of the neurons that follow it in the ordering are also up-
dated (erased). Based on the cumax() and the LSTM architecture, we have designed a new model,
ON-LSTM, that is biased towards performing tree-like composition operations. Our model achieves
good performance on four tasks: language modeling, unsupervised constituency parsing, targeted
syntactic evaluation (Marvin & Linzen, 2018) and logical inference (Bowman et al., 2015). The
result on unsupervised constituency parsing suggests that the proposed inductive bias aligns with
the syntax principles proposed by human experts better than previously proposed models. The ex-
periments also show that ON-LSTM performs better than standard LSTM models in tasks requiring
capturing long-term dependencies and achieves better generalization to longer sequences.
2	Related Work
There has been prior work leveraging tree structures for natural language tasks in the literature.
Socher et al. (2010); Alvarez-Melis & Jaakkola (2016); Zhou et al. (2017); Zhang et al. (2015) use
supervised learning on expert-labeled treebanks for predicting parse trees. Socher et al. (2013)
and Tai et al. (2015) explicitly model the tree-structure using parsing information from an external
2
Published as a conference paper at ICLR 2019
parser. Later, Bowman et al. (2016) exploited guidance from a supervised parser (Klein & Manning,
2003) in order to train a stack-augmented neural network.
Theoretically, RNNs and LSTMs can model data produced by context-free grammars and context-
sensitive grammars (Gers & Schmidhuber, 2001). However, recent results suggest that introducing
structure information into LSTMs is beneficial. Kuncoro et al. (2018) showed that RNNGs (Dyer
et al., 2016), which have an explicit bias to model the syntactic structures, outperform LSTMs on
the subject-verb agreement task (Linzen et al., 2016). In our paper, we run a more extensive suite of
grammatical tests recently provided by Marvin & Linzen (2018). Bowman et al. (2014; 2015) also
demonstrate that tree-structured models are more effective for downstream tasks whose data was
generated by recursive programs. Interestingly, Shi et al. (2018) suggests that while the prescribed
grammar tree may not be ideal, some sort of hierarchical structure, perhaps task dependent, might
help. However, the problem of efficiently inferring such structures from observed data remains an
open question.
The task of learning the underlying grammar from data is known as grammar induction (Chen,
1995; Cohen et al., 2011). Early work incorporated syntactic structure in the context of language
modeling (Roark, 2001; Charniak, 2001; Chelba & Jelinek, 2000). More recently, there have been
attempts at incorporating some structure for downstream tasks using neural models (Grefenstette
et al., 2015; Sun et al., 2017; Joulin & Mikolov, 2015). Generally, these works augment a main
recurrent model with a stack and focus on solving algorithmic tasks. Yogatama et al. (2018) focus
on language modeling and syntactic evaluation tasks (Linzen et al., 2016) but they do not show the
extent to which the structure learnt by the model align with gold-standard parse trees. Shen et al.
(2017) introduced the Parsing-Reading-Predict Networks (PRPN) model, which attempts to perform
parsing by solving a language modeling task. The model uses self-attention to compose previous
states, where the range of attention is controlled by a learnt “syntactic distance”. The authors show
that this value corresponds to the depth of the parse tree. However, the added complexity in using
the PRPN model makes it unwieldy in practice.
Another possible solution is to develop models with varying time-scales of recurrence as a way
of capturing this hierarchy. El Hihi & Bengio (1996); Schmidhuber (1991); Lin et al. (1998) de-
scribe models that capture hierarchies at pre-determined time-scales. More recently, Koutnik et al.
(2014) proposed Clockwork RNN, which segments the hidden state of a RNN by updating at dif-
ferent time-scales. These approaches typically make a strong assumption about the regularity of the
hierarchy involved in modelling the data. Chung et al. (2016) proposed a method that, unlike the
Clockwork RNN, would learn a multi-scale hierarchical recurrence. However, the model still has a
pre-determined depth to the hierarchy, depending on the number of layers. Our work is more closely
related to Rippel et al. (2014), which propose to induce a hierarchy in the representation units by
applying “nested” dropout masks: units are not dropped independently at random but whenever a
unit is dropped, all the units that follow in the ordering are also dropped. Our work can be seen as a
soft relaxation of the dropout by means of the proposed cumax() activation. Moreover, we propose
to condition the update masks on the particular input and apply our overall model to sequential data.
Therefore, our model can adapt the structure to the observed data, while both Clockwork RNN and
nested dropout impose a predefined hierarchy to hidden representations.
3	Ordered Neurons
Given a sequence of tokens S = (x1, . . . , xT) and its corresponding constituency tree (Figure 2(a)),
our goal is to infer the unobserved tree structure while processing the observed sequence, i.e. while
computing the hidden state ht for each time step t. At each time step, ht would ideally contain
a information about all the nodes on the path between the current leaf node xt and the root S. In
Figure 2(c), we illustrate how ht would contain information about all the constituents that include
the current token xt even if those are only partially observed. This intuition suggests that each
node in the tree can be represented by a set of neurons in the hidden states. However, while the
dimensionality of the hidden state is fixed in advance, the length of the path connecting the leaf to
the root of the tree may be different across different time steps and sentences. Therefore, a desiderata
for the model is to dynamically reallocate the dimensions of the hidden state to each node.
Given these requirements, we introduce ordered neurons, an inductive bias that forces neurons to
represent information at different time-scales. In our model, high-ranking neurons contain long-term
3
Published as a conference paper at ICLR 2019
Figure 2: Correspondences between a constituency parse tree and the hidden states of the proposed
ON-LSTM. A sequence of tokens S = (x1, x2, x3) and its corresponding constituency tree are illus-
trated in (a). We provide a block view of the tree structure in (b), where both S and VP nodes span
more than one time step. The representation for high-ranking nodes should be relatively consistent
across multiple time steps. (c) Visualization of the update frequency of groups of hidden state neu-
rons. At each time step, given the input word, dark grey blocks are completely updated while light
grey blocks are partially updated. The three groups of neurons have different update frequencies.
Topmost groups update less frequently while lower groups are more frequently updated.
or global information that will last anywhere from several time steps to the entire sentence, repre-
senting nodes near the root of the tree. Low-ranking neurons encode short-term or local information
that only last one or a few time steps, representing smaller constituents, as shown in Figure 2(b). The
differentiation between high-ranking and low-ranking neurons is learnt in a completely data-driven
fashion by controlling the update frequency of single neurons: to erase (or update) high-ranking
neurons, the model should first erase (or update) all lower-ranking neurons. In other words, some
neurons always update more (or less) frequently than the others, and that order is pre-determined as
part of the model architecture.
4	ON-LSTM
In this section, we present a new RNN unit, ON-LSTM (“ordered neurons LSTM”). The new model
uses an architecture similar to the standard LSTM, reported below:
ft=σ(Wfxt+Ufht-1+bf)	(1)
it = σ(Wixt + Uiht-1 + bi)	(2)
ot = σ(Woxt + Uoht-1 + bo)	(3)
^t = tanh( Wc Xt + Uc ht-i + bc)	(4)
ht = ot ◦ tanh(ct)	(5)
The difference with the LSTM is that we replace the update function for the cell state ct with a
new function that will be explained in the following sections. The forget gates ft and input gates
it are used to control the erasing and writing operation on cell states ct , as before. Since the gates
in the LSTM act independently on each neuron, it may be difficult in general to discern a hierarchy
of information between the neurons. To this end, we propose to make the gate for each neuron
dependent on the others by enforcing the order in which neurons should be updated.
4.1	ACTIVATION FUNCTION: cumax()
To enforce an order to the update frequency, we introduce a new activation function:
g = CUmaX(...) = CUmSUm(SoftmaX(...)),	(6)
where CUmSUm denotes the cumulative sum. We Win show that the vector g can be seen as the
expectation of a binary gate g = (0, ..., 0, 1, ..., 1). This binary gate splits the cell state into two
segments: the 0-segment and the 1-segment. Thus, the model can apply different update rules on
the two segments to differentiate long/short-term information. Denote by d a categorical random
4
Published as a conference paper at ICLR 2019
variable representing the index for the first 1 in g:
p(d) = softmax(. . .)	(7)
The variable d represents the split point between the two segments. We can compute the probability
of the k-th value in g being 1 by evaluating the probability of the disjunction of any of the values
before the k-th being the split point, that is d ≤ k = (d = 0) ∨ (d = 1) ∨∙∙∙∨ (d = k). Since the
categories are mutually exclusive, we can do this by computing the cumulative distribution function:
p(gk = 1) = p(d ≤ k) = Xp(d = i)	(8)
i≤k
Ideally, g should take the form of a discrete variable. Unfortunately, computing gradients when a
discrete variable is included in the computation graph is not trivial (Schulman et al., 2015), so in
practice we use a continuous relaxation by computing the quantity p(d ≤ k), obtained by taking
a cumulative sum of the softmax. As gk is binary, this is equivalent to computing E[gk]. Hence,
g = E[g].
4.2	Structured Gating Mechanism
Based on the cumax() function, we introduce a master forget gate ft and a master input gate it:
ft = Cumax(Wfxt + Ufht-I + bf)
it = 1 - cumax(Wfixt + Ufiht-1 + bfi)
(9)
(10)
Following the properties of the cumax() activation, the values in the master forget gate are mono-
tonically increasing from 0 to 1, and those in the master input gate are monotonically decreasing
from 1 to 0. These gates serve as high-level control for the update operations of cell states. Using
the master gates, we define a new update rule:
ωt = ft ◦ it	(11)
O	.	， ~	~	, .	~	~ 、
ft =	ft ◦ ωt + (ft - ωt) = ft ◦ (ft ◦ it + 1 - it)	(12)
it =	it ◦ ωt + (it - ωt) = it ◦ (it ◦ ft + 1 - ft)	(13)
Ct =	ft ◦ ct-1 + it ◦ Ct	(14)
In order to explain the intuition behind the new update rule, we assume that the master gates are
binary:
•	The master forget gate ft controls the erasing behavior of the model. Suppose ft =
(0, . . . , 0, 1, . . . , 1) and the split point is dtf . Given the Eq. (12) and (14), the information
stored in the first dtf neurons of the previous cell state ct-1 will be completely erased. In a
parse tree (e.g. Figure 2(a)), this operation is akin to closing previous constituents. A large
number of zeroed neurons, i.e. a large dtf , represents the end ofa high-level constituent in
the parse tree, as most of the information in the state will be discarded. Conversely, a small
dtf represents the end ofa low-level constituent as high-level information is kept for further
processing.
•	The master input gate it is meant to control the writing mechanism of the model. Assume
that it = (1,..., 1,0,..., 0) and the split point is d；. Given Eq. (13) and (14), a large dt
means that the current input xt contains long-term information that needs to be preserved
for several time steps. Conversely, a small dit means that the current input xt just provides
local information that could be erased by ft in the next few time steps.
•	The product of the two master gates ωt represents the overlap of ft and it. Whenever an
overlap exists (∃k, ωtk > 0), the corresponding segment of neurons encodes the incomplete
constituents that contain some previous words and the current input word xt. Since these
constituents are incomplete, we want to update the information inside the respective blocks.
The segment is further controlled by the ft and it in the standard LSTM model to enable
more fine-grained operations within blocks. For example, in Figure 2, the word x3 is nested
5
Published as a conference paper at ICLR 2019
Model	Parameters	Validation	Test
Zaremba et al. (2014) - LSTM (large)	66M	82.2	78.4
Gal & Ghahramani (2016) - Variational LSTM (large, MC)	66M	-	73.4
Kim et al. (2016) - CharCNN	19M	-	78.9
Merity et al. (2016) - Pointer Sentinel-LSTM	21M	72.4	70.9
Grave et al. (2016) - LSTM	-	-	82.3
Grave et al. (2016) - LSTM + continuous cache pointer	一	-	72.1
Inan et al. (2016) - Variational LSTM (tied) + augmented loss	51M	71.1	68.5
Zilly et al. (2016) - Variational RHN (tied)	23M	67.9	65.4
Zoph & Le (2016) - NAS Cell (tied)	54M	-	62.4
Shen et al. (2017) - PRPN-LM	一	-	62.0
Melis et al. (2017) - 4-layer skip connection LSTM (tied)	24M	60.9	58.3
Merity et al. (2017) - AWD-LSTM - 3-layer LSTM (tied)	24M	60.0	57.3
ON-LSTM - 3-layer (tied)	25M	58.29 ± 0.10	56.17 ± 0.12
Yang et al.(2017) - AWD-LSTM-MoS*	22M	56.5	54.4
Table 1: Single model perplexity on validation and test sets for the Penn Treebank language model-
ing task. Models labelled tied use weight tying on the embedding and softmax weights (Inan et al.,
2016; Press & Wolf, 2017). Models labelled * focus on improving the softmax component of RNN
language model. Their contribution is orthogonal to ours.
into the constituents S and VP. At this time step, the overlap gray blocks would represent
these constituents, such that ft and it can decide whether to reset or update each individual
neurons in these blocks.
As the master gates only focus on coarse-grained control, modeling them with the same dimensions
as the hidden states is computationally expensive and unnecessary. In practice, We set ft and it
to be Dm = D dimensional vectors, where D is the dimension of hidden state, and C is a chunk
size factor. We repeat each dimension C times, before the element-Wise multiplication With ft and
it. The downsizing significantly reduces the number of extra parameters that we need to add to the
LSTM. Therefore, every neuron within each C-sized chunk shares the same master gates.
5	Experiments
We evaluate the proposed model on four tasks: language modeling, unsupervised constituency pars-
ing, targeted syntactic evaluation (Marvin & Linzen, 2018), and logical inference (Bowman et al.,
2015).
5.1	Language Modeling
Word-level language modeling is a macroscopic evaluation of the model’s ability to deal with various
linguistic phenomena (e.g. co-occurence, syntactic structure, verb-subject agreement, etc). We
evaluate our model by measuring perplexity on the Penn TreeBank (PTB) (Marcus et al., 1993;
Mikolov, 2012) task.
For fair comparison, we closely follow the model hyper-parameters, regularization and optimization
techniques introduced in AWD-LSTM (Merity et al., 2017). Our model uses a three-layer ON-
LSTM model with 1150 units in the hidden layer and an embedding of size 400. For master gates,
the downsize factor C = 10. The total number of parameters was slightly increased from 24 millions
to 25 millions with additional matrices for computing master gates. We manually searched some of
the dropout values for ON-LSTM based on the validation performance. The values used for dropout
on the word vectors, the output between LSTM layers, the output of the final LSTM layer, and
embedding dropout where (0.5, 0.3, 0.45, 0.1) respectively. A weight-dropout of 0.45 was applied
to the recurrent weight matrices.
As shown in Table 1, our model performs better than the standard LSTM while sharing the same
number of layers, embedding dimensions, and hidden states units. Recall that the master gates only
control how information is stored in different neurons. It is interesting to note that we can improve
6
Published as a conference paper at ICLR 2019
the performance of a strong LSTM model without adding skip connections or a significant increase
in the number of parameters.
5.2	Unsupervised Constituency Parsing
The unsupervised constituency parsing task compares the latent stree structure induced by the model
with those annotated by human experts. Following the experiment settings proposed in Htut et al.
(2018), we take our best model for the language modeling task, and test it on WSJ10 dataset and
WSJ test set. WSJ10 has 7422 sentences, filtered from the WSJ dataset with the constraint of 10
words or less, after the removal of punctuation and null elements (Klein & Manning, 2002). The
WSJ test set contains 2416 sentences with various lengths. It is worth noting that the WSJ10 test set
contains sentences from the training, validation, and test set of the PTB dataset, while WSJ test uses
the same set of sentences as the PTB test set.
To infer the tree structure of a sentence from a pre-trained model, we initialize the hidden states with
the zero vector, then feed the sentence into the model as done in the language modeling task. At
each time step, we compute an estimate of dtf :
Dm	Dm k	Dm
dt = E [df] = X kPf (dt = k) = XX Pf (dt = k) = Dm - X ftk	(15)
k=1	k=1 i=1	k=1
where Pf is the probability distribution over split points associated to the master forget gate and
Dm is the size of the hidden state. Given df, We can use the top-down greedy parsing algorithm
proposed in Shen et al. (2017) for unsupervised constituency parsing. We first sort the {df} in
decreasing order. For the first df in the sorted sequence, we split the sentence into constituents
((x<i), (xi, (x>i))). Then, we recursively repeat this operation for constituents (x<i) and (x>i),
until each constituent contains only one word.
The performance is shown in Table 2. The second layer of ON-LSTM achieves state-of-the-art un-
supervised constituency parsing results on the WSJ test set, while the first and third layers do not
perform as well. One possible interpretation is that the first and last layers may be too focused
on capturing local information useful for the language modeling task as they are directly exposed
to input tokens and output predictions respectively, thus may not be encouraged to learn the more
abstract tree structure. Since the WSJ test set contains sentences of various lengths which are unob-
served during training, we find that ON-LSTM provides better generalization and robustness toward
longer sentences than previous models. We also see that ON-LSTM model can provide strong re-
sults for phrase detection, including ADJP (adjective phrases), PP (prepositional phrases), and NP
(noun phrases). This feature could benefit many downstream tasks, like question answering, named
entity recognition, co-reference resolution, etc.
5.3	Targeted S yntactic Evaluation
Targeted syntactic evaluation tasks have been proposed in Marvin & Linzen (2018). It is a collection
of tasks that evaluate language models along three different structure-sensitive linguistic phenom-
ena: subject-verb agreement, reflexive anaphora and negative polarity items. Given a large number
of minimally different pairs of English sentences, each consisting ofa grammatical and an ungram-
matical sentence, a language model should assign a higher probability to a grammatical sentence
than an ungrammatical one.
Using the released codebase2 and the same settings proposed in Marvin & Linzen (2018), we train
both our ON-LSTM model and a baseline LSTM language model on a 90 million word subset of
Wikipedia. Both language models have two layers of 650 units, a batch size of 128, a dropout rate
of 0.2, a learning rate of 20.0, and were trained for 40 epochs. The input embeddings have 200
dimensions and the output embeddings have 650 dimesions.
Table 3 shows that the ON-LSTM performs better on the long-term dependency cases, while the
baseline LSTM fares better on the short-term ones. This is possibly due to the relatively small num-
2https://github.com/BeckyMarvin/LM_syneval. We notice that the test set generated from
the code is different from the one used in the original paper Marvin & Linzen (2018). Therefore, our results are
not strictly comparable with the results in Marvin & Linzen (2018).
7
Published as a conference paper at ICLR 2019
Model	Training Data	Training Object	Vocab Size	Parsing F1				Depth WSJ	Accuracy on WSJ by Tag			
				μ㈤	max	μ㈤	max		ADJP	NP	PP	INTJ
PRPN-UP	AllNLI Train	LM	76k	66.3(0.8)	68.5	38.3 (0.5)	39.8	5.8	28.7	65.5	32.7	0.0
PRPN-LM	AllNLI Train	LM	76k	52.4 (4.9)	58.1	35.0 (5.4)	42.8	6.1	37.8	59.7	61.5	100.0
PRPN-UP	WSJ Train	LM	15.8k	62.2 (3.9)	70.3	26.0 (2.3)	32.8	5.8	24.8	54.4	17.8	0.0
PRPN-LM	WSJ Train	LM	10k	70.5 (0.4)	71.3	37.4 (0.3)	38.1	5.9	26.2	63.9	24.4	0.0
ON-LSTM 1st-layer	WSJ Train	LM	10k	35.2 (4.1)	42.8	20.0 (2.8)	24.0	5.6	38.1	23.8	18.3	100.0
ON-LSTM 2nd-layer	WSJ Train	LM	10k	65.1 (1.7)	66.8	47.7 (1.5)	49.4	5.6	46.2	61.4	55.4	0.0
ON-LSTM 3rd-layer	WSJ Train	LM	10k	54.0 (3.9)	57.6	36.6 (3.3)	40.4	5.3	44.8	57.5	47.2	0.0
300D ST-Gumbel	AllNLI Train	NLI	一	-	-	19.0 (1.0)	20.1	-	15.6	18.8	9.9	59.4
w/o Leaf GRU	AllNLI Train	NLI	一	—	—	22.8 (1.6)	25.0	—	18.9	24.1	14.2	51.8
300D RL-SPINN	AllNLI Train	NLI	一	-	-	13.2 (0.0)	13.2	-	1.7	10.8	4.6	50.6
w/o Leaf GRU	AllNLI Train	NLI	一	-	-	13.1 (0.1)	13.2	-	1.6	10.9	4.6	50.0
CCM	WSJ10 Full	-	一	-	71.9	-	-	-	-	-	一	-
DMV+CCM	WSJ10 Full	-	一	-	77.6	-	-	-	-	-	一	-
UML-DOP	WSJ10 Full	-	一	-	82.9	-	-	-	-	-	一	-
Random Trees	-	-	一	31.7 (0.3)	32.2	18.4 (0.1)	18.6	5.3	17.4	22.3	16.0	40.4
Balanced Trees	-	-	一	43.4 (0.0)	43.4	24.5 (0.0)	24.5	4.6	22.1	20.2	9.3	55.9
Left Branching	-	-	一	19.6 (0.0)	19.6	9.0 (0.0)	9.0	12.4	-	-	一	-
Right Branching	—	—	一	56.6 (0.0)	56.6	39.8 (0.0)	39.8	12.4	—	—	一	—
Table 2: Unlabeled parsing F1 results evaluated on the full WSJ10 and WSJ test set. Our language
model has three layers, each of them provides a sequence of df. We provide the parsing performance
for all layers. Results with RL-SPINN and ST-Gumbel are evaluated on the full WSJ (Williams et al.,
2017). PRPN models are evaluated on the WSJ test set (Htut et al., 2018). We run the model with 5
different random seeds to calculate the average F1. The Accuracy columns represent the fraction of
ground truth constituents ofa given type that correspond to constituents in the model parses. We use
the model with the best F1 score to report ADJP, NP, PP, and INTJ. WSJ10 baselines are from Klein
& Manning (2002, CCM), Klein & Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). As
the WSJ10 baselines are trained using POS tags, they are not strictly comparable with the latent tree
learning results. Italics mark results that are worse than the random baseline.
ber of units in the hidden states, which is insufficient to take into account both long and short-term
information. We also notice that the results for NPI test cases have unusually high variance across
different hyper-parameters. This result maybe due to the non-syntactic cues discussed in Marvin &
Linzen (2018). Despite this, ON-LSTM actually achieves better perplexity on the validation set.
5.4	Logical Inference
We also analyze the model’s performance on the logical inference task described in Bowman et al.
(2015). This task is based on a language that has a vocabulary of six words and three logical opera-
tions, or, and, not. There are seven mutually exclusive logical relations that describe the relationship
between two sentences: two types of entailment, equivalence, exhaustive and non-exhaustive con-
tradiction, and two types of semantic independence. Similar to the natural language inference task,
this logical inference task requires the model to predict the correct label given a pair of sentences.
The train/test split is as described in the original codebase3, and 10% of training set is set aside as
the validation set.
We evaluate the ON-LSTM and the standard LSTM on this dataset. Given a pair of sentences
(s1 , s2), we feed both sentences into an RNN encoder, taking the last hidden state (h1 , h2) as the
sentence embedding. The concatenation of (h1, h2, h1 ◦ h2, abs(h1 - h2)) is used as input to a
multi-layer classifier, which gives a probability distribution over seven labels. In our experiment,
the RNN models were parameterised with 400 units in one hidden layer, and the input embedding
size was 128. A dropout of 0.2 was applied between different layers. Both models are trained on
sequences with 6 or less logical operations and tested on sequences with at most 12 operations.
Figure 3 shows the performance of ON-LSTM and standard LSTM on the logical inference task.
While both models achieve nearly 100% accuracy on short sequences (≤ 3), ON-LSTM attains
3https://github.com/sleepinyourhat/vector-entailment
8
Published as a conference paper at ICLR 2019
	ON-LSTM	LSTM
Short-Term Dependency		
Subject-verb agreement:		
Simple	0.99	1.00
In a sentential complement	0.95	0.98
Short VP coordination	0.89	0.92
In an object relative clause	0.84	0.88
In an object relative (no that)	0.78	0.81
Reflexive anaphora:		
Simple	0.89	0.82
In a sentential complement	0.86	0.80
Negative polarity items:		
Simple (grammatical vs. intrusive)	0.18	1.00
Simple (intrusive vs. ungrammatical)	0.50	0.01
Simple (grammatical vs. ungrammatical)	0.07	0.63
Long-Term Dependency		
Subject-verb agreement:		
Long VP coordination	0.74	0.74
Across a prepositional phrase	0.67	0.68
Across a subject relative clause	0.66	0.60
Across an object relative clause	0.57	0.52
Across an object relative (no that)	0.54	0.51
Reflexive anaphora:		
Across a relative clause	0.57	0.58
Negative polarity items:		
Across a relative clause (grammatical vs. intrusive)	0.59	0.95
Across a relative clause (intrusive vs. ungrammatical)	0.20	0.00
Across a relative clause (grammatical vs. ungrammatical)	0.11	0.04
Table 3: Overall accuracy for the ON-LSTM and LSTM on each test case. “Long-term dependency”
means that an unrelated phrase (or a clause) exist between the targeted pair of words, while “short-
term dependency” means there is no such distraction.
Figure 3: Test accuracy of the models, trained on short sequences (≤ 6) in logic data. The horizontal
axis indicates the length of the sequence, and the vertical axis indicates the accuracy of models
performance on the corresponding test set.
better performance on sequences longer then 3. The performance gap continues to increase on
longer sequences (≥ 7) that were not present during training. Hence, the ON-LSTM model shows
better generalization while facing structured data with various lengths and comparing to the standard
LSTM. A tree-structured model can achieve strong performance on this dataset (Bowman et al.,
2015), since it is provided with the ground truth structure as input. The recursive application of the
same composition function is well suited for this task. We also include the result of RRNet (Jacob
et al., 2018), which can induce the latent tree structure from downstream tasks. Note that the results
may not be comparable, because the hyper-parameters for training were not provided.
9
Published as a conference paper at ICLR 2019
6 Conclusion
In this paper, we propose ordered neurons, a novel inductive bias for recurrent neural networks.
Based on this idea, we propose a novel recurrent unit, the ON-LSTM, which includes a new gating
mechanism and a new activation function cumax(∙). This brings recurrent neural networks closer to
performing tree-like composition operations, by separately allocating hidden state neurons with long
and short-term information. The model performance on unsupervised constituency parsing shows
that the ON-LSTM induces the latent structure of natural language in a way that is coherent with
human expert annotation. The inductive bias also enables ON-LSTM to achieve good performance
on language modeling, long-term dependency, and logical inference tasks.
References
David Alvarez-Melis and Tommi S Jaakkola. Tree-structured decoding with doubly-recurrent neural
networks. 2016.
Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trendsR in Machine
Learning, 2(1):1-127, 2009.
Rens Bod. An all-subtrees approach to unsupervised parsing. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and the 44th annual meeting of the Association
for Computational Linguistics, pp. 865-872. Association for Computational Linguistics, 2006.
Samuel R Bowman, Christopher Potts, and Christopher D Manning. Recursive neural networks can
learn logical semantics. arXiv preprint arXiv:1406.1827, 2014.
Samuel R Bowman, Christopher D Manning, and Christopher Potts. Tree-structured composition in
neural networks without tree-structured architectures. arXiv preprint arXiv:1506.04834, 2015.
Samuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and
Christopher Potts. A fast unified model for parsing and sentence understanding. arXiv preprint
arXiv:1603.06021, 2016.
Eugene Charniak. Immediate-head parsing for language models. In Proceedings of the 39th Annual
Meeting on Association for Computational Linguistics, pp. 124-131. Association for Computa-
tional Linguistics, 2001.
Ciprian Chelba and Frederick Jelinek. Structured language modeling. Computer Speech & Lan-
guage, 14(4):283-332, 2000.
Stanley F Chen. Bayesian grammar induction for language modeling. In Proceedings of the 33rd
annual meeting on Association for Computational Linguistics, pp. 228-235. Association for Com-
putational Linguistics, 1995.
Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to compose task-specific tree structures.
In Proceedings of the 2018 Association for the Advancement of Artificial Intelligence (AAAI). and
the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP), 2018.
Noam Chomsky. Three models for the description of language. IRE Transactions on information
theory, 2(3):113-124, 1956.
Noam Chomsky. Aspects of the Theory of Syntax. The MIT Press, Cambridge, 1965. URL http://
www.amazon.com/Aspects-Theory-Syntax-Noam-Chomsky/dp/0262530074.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704, 2016.
Shay B Cohen, Dipanjan Das, and Noah A Smith. Unsupervised structure prediction with non-
parallel multilingual guidance. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, pp. 50-61. Association for Computational Linguistics, 2011.
10
Published as a conference paper at ICLR 2019
Stanislas Dehaene, Florent Meyniel, Catherine Wacongne, Liping Wang, and Christophe Pallier.
The neural representation of sequences: from transition probabilities to algebraic patterns and
linguistic trees. Neuron, 88(1):2-19, 2015.
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, pp. 199-209, 2016.
Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependen-
cies. In Advances in neural information processing systems, pp. 493-499, 1996.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pp. 1019-1027, 2016.
Felix A Gers and E Schmidhuber. Lstm recurrent networks learn simple context-free and context-
sensitive languages. IEEE Transactions on Neural Networks, 12(6):1333-1340, 2001.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a
continuous cache. arXiv preprint arXiv:1612.04426, 2016.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems, pp.
1828-1836, 2015.
Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. Colorless
green recurrent networks dream hierarchically. In Proc. of NAACL, pp. 1195-1205, 2018.
Phu Mon Htut, Kyunghyun Cho, and Samuel R Bowman. Grammar induction with neural language
models: An unusual replication. arXiv preprint arXiv:1808.10000, 2018.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A
loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Athul Paul Jacob, Zhouhan Lin, Alessandro Sordoni, and Yoshua Bengio. Learning hierarchical
structures on-the-fly with a recurrent-recursive model for sequences. In Proceedings of The Third
Workshop on Representation Learning for NLP, pp. 154-158, 2018.
Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent
nets. In Advances in neural information processing systems, pp. 190-198, 2015.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language
models. In AAAI, pp. 2741-2749, 2016.
Dan Klein and Christopher D Manning. A generative constituent-context model for improved gram-
mar induction. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, pp. 128-135. Association for Computational Linguistics, 2002.
Dan Klein and Christopher D Manning. Accurate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational Linguistics-Volume 1, pp. 423-430. Associa-
tion for Computational Linguistics, 2003.
Dan Klein and Christopher D Manning. Natural language grammar induction with a generative
constituent-context model. Pattern recognition, 38(9):1407-1419, 2005.
Hilda Koopman, Dominique Sportiche, and Edward Stabler. An introduction to syntactic analysis
and theory, 2013.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. arXiv
preprint arXiv:1402.3511, 2014.
Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark, and Phil Blunsom.
Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better. In
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pp. 1426-1436, 2018.
11
Published as a conference paper at ICLR 2019
Yair Lakretz, German Kruszewski, Theo Desbordes, Dieuwke Hupkes, Stanislas Dehaene, and
Marco Baroni. The emergence of number and syntax units in lstm language models. In Proc.
of NAACL, 2019.
Yann LeCun, YoshUa Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436—444,
2015.
Tsungnan Lin, Bill G Horne, Peter Tino, and C Lee Giles. Learning long-term dependencies is not
as difficult with narx recurrent neural networks. Technical report, 1998.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-
sensitive dependencies. arXiv preprint arXiv:1611.01368, 2016.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language models. arXiv preprint
arXiv:1808.09031, 2018.
Gabor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language
models. arXiv preprint arXiv:1707.05589, 2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM
Language Models. arXiv preprint arXiv:1708.02182, 2017.
TomaS Mikolov. Statistical language models based on neural networks. Presentation at Google,
Mountain View, 2nd April, 2012.
Ofir Press and Lior Wolf. Using the output embedding to improve language models. In Proceedings
of the 15th Conference of the European Chapter of the Association for Computational Linguistics:
Volume 2, Short Papers, volume 2, pp. 157-163, 2017.
Oren Rippel, Michael Gelbart, and Ryan Adams. Learning ordered representations with nested
dropout. In International Conference on Machine Learning, pp. 1746-1754, 2014.
Brian Roark. Probabilistic top-down parsing and language modeling. Computational linguistics, 27
(2):249-276, 2001.
Dominiek Sandra and Marcus Taft. Morphological Structure, Lexical Representation and Lexical
Access (RLE Linguistics C: Applied Linguistics): A Special Issue of Language and Cognitive
Processes. Routledge, 2014.
Jurgen Schmidhuber. Neural sequence chunkers. 1991.
JUrgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85-117,
2015.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3528-
3536, 2015.
Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron Courville. Neural language modeling by
jointly learning syntax and lexicon. arXiv preprint arXiv:1711.02013, 2017.
Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li. On tree-based neural sentence modeling. arXiv
preprint arXiv:1808.09644, 2018.
Richard Socher, Christopher D Manning, and Andrew Y Ng. Learning continuous phrase represen-
tations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning Workshop, volume 2010, pp. 1-9, 2010.
12
Published as a conference paper at ICLR 2019
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing ,pp.1631-1642, 2013.
Guo-Zheng Sun, C Lee Giles, Hsing-Hen Chen, and Yee-Chun Lee. The neural network pushdown
automaton: Model, stack and learning simulations. arXiv preprint arXiv:1711.05738, 2017.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.
Adina Williams, Andrew Drozdov*, and Samuel R Bowman. Do latent tree learning models identify
meaningful structure in sentences? Transactions of the Association of Computational Linguistics,
6:253-267, 2018.
Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li, and Ming Zhou. Sequence-to-dependency
neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 698-707, 2017.
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax
bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to
compose words into sentences with reinforcement learning. arXiv preprint arXiv:1611.09100,
2016.
Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil
Blunsom. Memory architectures in recurrent neural network language models. 2018.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
Xingxing Zhang, Liang Lu, and Mirella Lapata. Top-down tree long short-term memory networks.
arXiv preprint arXiv:1511.00060, 2015.
Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao, Fen Lin, Bo Chen, and Qing He. Generative
neural machine for tree structures. CoRR, 2017.
Julian Georg Zilly, RUPesh Kumar Srivastava, Jan Koutnlk, and Jurgen Schmidhuber. Recurrent
highway networks. arXiv preprint arXiv:1607.03474, 2016.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
13
Published as a conference paper at ICLR 2019
A Sample Parses from the model with the best perplexity
The RTC needs the most able competent management available
The RTC needs the most able competent management available
Resolution Funding Corp. to sell 4.5 billion 30-year bonds
Resolution Funding Corp. to sell 4.5 billion 30-year bonds
Interest expense in the 1988 third quarter was 75.3 million
Interest expense in the 1988 third quarter was 75.3 million
All prices are as of monday ’s close
All prices are as of Monday ’s close
That ’ll save us time and get people involved
That ’ll save us time and get people involved
A decision is n’t expected until some time next year
A decision is n’t expected until some time next year
Figure A.1: Left parses are from the 2nd layer of the ON-LSTM model, Right parses are converted
from human expert annotations (removing all punctuations).
14