Table 1: Single model perplexity on validation and test sets for the Penn Treebank language model-ing task. Models labelled tied use weight tying on the embedding and softmax weights (Inan et al.,2016; Press & Wolf, 2017). Models labelled * focus on improving the softmax component of RNNlanguage model. Their contribution is orthogonal to ours.
Table 2: Unlabeled parsing F1 results evaluated on the full WSJ10 and WSJ test set. Our languagemodel has three layers, each of them provides a sequence of df. We provide the parsing performancefor all layers. Results with RL-SPINN and ST-Gumbel are evaluated on the full WSJ (Williams et al.,2017). PRPN models are evaluated on the WSJ test set (Htut et al., 2018). We run the model with 5different random seeds to calculate the average F1. The Accuracy columns represent the fraction ofground truth constituents ofa given type that correspond to constituents in the model parses. We usethe model with the best F1 score to report ADJP, NP, PP, and INTJ. WSJ10 baselines are from Klein& Manning (2002, CCM), Klein & Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). Asthe WSJ10 baselines are trained using POS tags, they are not strictly comparable with the latent treelearning results. Italics mark results that are worse than the random baseline.
Table 3: Overall accuracy for the ON-LSTM and LSTM on each test case. “Long-term dependency”means that an unrelated phrase (or a clause) exist between the targeted pair of words, while “short-term dependency” means there is no such distraction.
