Table 1: Stylized-ImageNet cannot be solved with textUre featUres alone. AccUracy comparison(in percent; top-5 on validation data set) of a standard ResNet-50 with Bag of FeatUre networks(BagNets) with restricted receptive field sizes of 33×33, 17×17 and 9×9 pixels. Arrows indicate:train data→test data, e.g. IN→SIN means training on ImageNet and testing on Stylized-ImageNet.
Table 2: Accuracy comparison on the ImageNet (IN) validation data set as well as object detec-tion performance (mAP50) on PASCAL VOC 2007 and MS COCO. All models have an identicalResNet-50 architecture. Method details reported in the Appendix, where we also report similarresults for ResNet-152 (Table 4).
Table 3: Characteristics of human participants (p.) across experiments. The symbol ‘#’ refers to“number of”; ‘rt’ stands for “median reaction time (ms)” in an experiment.
Table 4: Accuracy and object detection performance for ResNet-152. Accuracy comparison on theImageNet (IN) validation data set as well as object detection performance (mAP50) on PASCALVOC 2007. All models have an identical ResNet-152 architecture.
Table 5: Corruption error (lower=better) on ImageNet-C (Hendrycks & Dietterich, 2019), consistingof different types of noise, blur, weather and digital corruptions. Abbreviations: mCE = meanCorruption Error (average of the 15 individual corruption error values); SIN = Stylized-ImageNet;IN = ImageNet; ft = fine-tuning. Results kindly provided by Dan Hendrycks.
