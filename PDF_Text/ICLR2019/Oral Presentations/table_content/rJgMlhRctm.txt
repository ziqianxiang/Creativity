Table 1: Comparison with other frameworks on the CLEVR VQA dataset, w.r.t. visual features,implicit or explicit semantics and supervisions.
Table 2: We also evaluate the learned visual con-cepts using a diagnostic question set containingsimple questions such as “How many red ob-jects are there?”. NS-CL outperforms both con-volutional and attentional baselines. The sug-gested object-based visual representation andsymbolic reasoning approach perceives betterinterpretation of visual concepts.
Table 3: We compare different variants of base-lines for a systematic study on visual features anddata efficiency. Using only 10% of the trainingimages, our model is able to achieve a compara-ble results with the baselines trained on the fulldataset. See the text for details.
Table 4: Our model outperforms all baselines using noprogram annotations. It achieves comparable resultswith models trained by full program annotations suchas TbD.
Table 5: We introduce a new simple DSL for image-caption retrieval to evaluate how well the learnedvisual concepts transfer. Due to the difference between VQA and caption retrieval, VQA baselinesare only able to infer the result on a partial set of data. The learned object-based visual concepts canbe directly transferred into the new domain for free.
Table 6: All operations in the domain-specific language for CLEVR VQA.
Table 7: The type system of the domain-specific language for CLEVR VQA.
Table 8: A step-by-step running example of the recursive parsing procedure. The parameter {ci } isomitted for better visualization.
Table 9: All operations in the domain-specific language for CLEVR VQA. γc = 0.5 and τc = 0.25are constants for scaling and shift the probability. During inference, one can quantify all operationsas Yi et al. (2018).
Table 10: Our model achieves comparable results on the Minecraft dataset with baselines trained byfull program annotations.
