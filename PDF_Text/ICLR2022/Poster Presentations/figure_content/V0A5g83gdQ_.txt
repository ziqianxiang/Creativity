Figure 1: Arrays in tensor diagram. A vector is de-noted as a node With 1 leg, a matrix as a node With 2legs and an N -dimensional array as a node with N legs.
Figure 2: Tensor diagrams for atomic operations. (1) (left) Contraction is an operation that generalizesthe matrix multiplication. It sums element-Wise products on a mode in object A and a corresponding mode(With the same dimension) in object B (i.e., along a leg in node A and a corresponding leg in node B). In thetensor diagram, multiplying tWo matrices (or higher-order tensors With more than 2 legs) corresponds to “gluing”their corresponding legs (on a certain mode). (2) (middle) Softmax is an element-Wise exponential functionnormalized along a certain mode. We propose to denote the α-scaled softmax function softmax(αA) on A as adotted box with a labeled filled ball (to distinguish itself from tensor objects, i.e., nodes Which are blank circles)attached to one leg. (3) (right) Batch multiplication is an element-Wise product along the connected legs.
Figure 4: MHSA as a special case of THSA. Figure 4a (left) represents MHSA. Figure 4b (middle) representsTHSA. As shown in Proposition 1, weight matrices in MHSA collaborate in a very specific way. WhenC = IH ⑥(1> 1d ) in Proposition 1, THSA degrades to to MHSA. Therefore, it is natural to make C fullytrainable for higher expressive power. THSA also reveals a design space. For example, we can extend MHSA byallowing trainable components C1 , C2 and C3 as in Figure 4c.
Figure 5: C naturally reveals a design space. We can obtain better generalization or better efficiency by alloWingfully trainable C, partially trainable C, pre-set fixed C and etc. We demonstrate some of them extending fromthe structure of MHSA Where C takes the form of a Kronecker product as shoWn in Figure 4c. The relationshipbetWeen the generalization and efficiency of these designs is summarized in Figure 6.
Figure 6: We can divide all structures listedin Figure 4 and Figure 5 into different groupsby their expressive poWer. The higher groupa structure is in, the higher expressive it has.
Figure 7: Performance comparison of THSA and MHSA in (left) language modeling on Penn Treebankdataset on TransformerXL, (middle) neural machine translation on WMT16 English-German dataset and (right)automatic speech recognition on Librispeech on Transformers. Under the same number of parameters, our THSAuniversally outperforms MHSA under all model sizes. The error bars displayed in shades are generated using 10random runs in LM and NMT and 3 random runs in ASR.
Figure 8: (1) Contraction as shown in Appendix A.2 is an operation that generalizes the matrix multiplication.
Figure 9: Tensor diagrams from single-head self-attention to multi-head self-attention. Figure 9a is thetensor diagram representation of a single-head self-attention. Figure 9c is the tensor-diagram representation of amulti-head self-attention.
Figure 10: (Left) THSA includes a single-head self-attention. In this case, the stable rank of the core tensorC is 1. It is interesting to observe that this is a single-head self-attention and this structure can be obtainedby removing the head contraction from Figure 4a. (Right) THSA includes a MHSA with H heads and latentdimension = 1. In this case, the stable rank equals to the rank of the identical matrix, which is H, correspondingto the number of heads of MHSA. Also, the structure on the right can be obtained by removing the latentcontraction edges D from Figure 4a.
Figure 11: This figure shows how to initialize THSA with MHSA. MHSA is a special case of THSA when thecore tensor C takes the Kronecker product of a D × D all-one matrix and a H × H identity matrix. Note thatin this case, the number of heads H in MHSA is equivalent to the stable rank of C .
Figure 12: (Left) Training and (Right) inference run-time comparison betWeen MHSA, THSA With the samenumber of parameters and THSA With the same number of parameters With check-pointing. The batch size is256.
