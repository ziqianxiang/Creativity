Figure 1: (a) Folded writing-reading mechanism of GPM. Red arrows indicates operations of computing thepseudo-inverses. The memory M, the weight W and the data Z interact in a dynamic way, where W isdynamically computed based on Z and M and (assume that) Z depends linearly on W and M. (b) Unfoldedwriting-reading mechanism of GPM. Orange boxes and circles contain trainable parameters of GPM. Thegreen circle consists of variational part for the generative scheme. Red arrows are operations of computing thepseudo-inverses. During writing, the episode X is encoded as Z. Using Z and prior memory M0 , we computethe writing weight W0 and then use W0 to write Z into the memory. The result of writing phase is the posteriormemory M. In reading, the query Xq is encoded as Zq. Note that the encoder in the reading phase needs not beidentical to the encoder in writing phase. The reading weight W is computed based on Zq and the posteriorM. The memory read-out Zr is simply the matrix product between W and M. Finally, Zr goes through thedecoder to produce the output Xo of the reading phase. If iterative reading is included, Xo will become the nextquery and be fed back to the encoder for next reading step. In training, we use the same encoder for writing andreading, the query Xq is identical with X and reading is non-iterative.
Figure 2: Hamming error measuring the denoising capibility of GPM and DKM (lower is better) with T = 16on Omniglot dataset. We test denoising capability of both models with different types of noise (salt & peppernoise, block noise, rotation noise) and levels of noise (5% - 20% salt & pepper noise, block noise with blocksize from 6 X 6 to 15 X 15, rotation noise With rotation angle from 150 to 450).
Figure 3: (a) GPM’s generated samples. Images in the box are nearest images in the episode compared withfinal generated images on their left. Left: Omniglot patterns. Distance between images is measured by theHamming distance. Right: CIFAR10 images. Images in the leftmost are the episode written to memory. Eachimage is repeated 3 times to help the generations better. We use Euclidean metric to measure distance betweenimages. (b) Running time per iteration. Left: Running time per iteration comparison between GPM and DKM.
Figure 4: (a) Reconstruction loss of GPM with fixed memory size and increasing episode length. (b) Denoisingsalt and pepper noise (5%) of the 24, 345-GPM with 32 × 100 memory. Patterns in the first column are originalones; patterns in the blue box are noisy queries; following columns are denoising process during iterative reading.
Figure 5:	Reconstructions of GPM. Left: original images. Right: reconstructions. (a) Omniglot (b) CIFAR10(c) CIFAR100 (d) CelebAB	DenoisingWe demonstrate denoising capibility of GPM in Fig. 7. The denoising process consists of two steps:we first write the correct patterns into the memory, then read from the memory using noisy queries.
Figure 6:	Average cosine similarity between exact data and read-out’s in DKM and GPM. Train and test onOmniglot dataset.
Figure 7: Retrieving pattern with noisy query during iterative reading. Top rows are correct patterns; rows inthe boxes are corrupted patterns; following rows are retrieval process with iterative reading. (a) Salt and peppernoise (15%). (b) Block noise (12 × 12 block) to cover a part of the correct pattern. (c) Rotation noise (rotationangle uniformly selected from -45 to 45 degrees). (d) Gaussian noise with standard deviation 0.5. (e) Illustrationof energy decrease during retrieving Omniglot patterns. Although sometimes the energy diverges and iterativereading leads to a meaningless pattern, overall, the energy decreases during iterations.
Figure 8: Hamming error measuring the denoising capibility of GPM and DKM (lower is better). Experimentsare conducted on binarized Omniglot dataset. We test denoising capability of both models with different typesof noise (salt & pepper noise, block noise, rotation noise) and levels of noise (5% - 20% salt & pepper noise,block noise With block size from 6 X 6 to 15 X 15, rotation noise With rotation angle from 15° to 45°).
Figure 9: The three cases of fixed points. (a) Fixed point is single pattern: patterns are stored if they arewell separated. Each pattern Xi has a single fixed point x* close to it. (b) Fixed point is average of similarpatterns: Xi and Xj are similar to each other and not well separated. The fixed point mX is a spuruous fixedpoint that is close to the mean mx of the similar patterns. (c) Fixed point is average of all patterns: no patternis well separated from the others. A single global fixed point m*x exists that is close to the arithmetic mean mxof all patterns. Images and caption are taken from (Ramsauer et al., 2020).
