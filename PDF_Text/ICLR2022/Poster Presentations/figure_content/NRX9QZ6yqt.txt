Figure 1: Common optimization algorithms placedover the spectrum of memory requirements. Theproposed algorithms the C variants have a fixedsized memory to store gradients during training.
Figure 2: (a) Showcases the acceleration in convergence (the unit being the number of gradientupdates with batch-size fixed) using a validation set provided by the C variants, compared pairwise(eg: Adam vs AdamC), on the different tasks.(b) denotes the difference in performance of the modelstrained with C variants with respect to its corresponding base method on the test set. A positivevalue indicates that C variant performed better than its base method in that task. The side-by-sidecomparison allows us to observe that the proposed optimizers enabled accelerated convergence tobetter test results. Of the 9 tasks considered, the C optimizers performed the best in 6 tasks. Further,(b) shows that C version of the base optimizers enhanced the performance to as much as 23%.
Figure 3: The validation performance comparison of the proposed optimizers and their vanillaversions showed that, in general, the proposed optimizers showed accelerated convergence thantheir counterparts. Also, in the 3 tasks — SNLI-I (a), SNLI-BL (b) and MWoZ-BL (d) where theC methods are not the optimal — only falls short by a negligible margin. For ease of visualization,confidence intervals across seeds are suppressed.
Figure 4: Histograms showing ages of the buffer as recorded at the end of every epoch, across fiveseeds for several decay values, for a MLP on MNIST. 0.99 decays at a much slower rate, keepingstale gradients in the memory, while 0.7 replaces the gradients frequently. Also, the size of the buffer(topC ), as expected, correlates directly with retention of stale gradients.
Figure 5: (a) compares gradient selection techniques based on different metrics, (b) comparesdifferent replacement strategies when flushing out a gradient from memory and (c) compares choiceof aggregating the gradients in the buffer. The algorithm converges in all the different choices.
Figure 6: We observe that non-trivial choices of decay and topC provided best performance forAdamC with different architectures on SNLI task. Also, the usefulness of explicit memory alignswith the evidence on gradient staleness from Figure 4.
Figure 7: The training on the three different convex datasets show that the C variants of the baseoptimizers empirically converge.
Figure 8: Histograms shoWing ages of the buffer as recorded at the end of every epoch, across fiveseeds, for Logistic Regression on MNIST.
Figure 9: Additional ablation experiments performed on CIFAR-10(c)F.5 Robustness to NoiseAs C variants retain larger gradients during training, and noisy data points may have gradients withhigher values of '2-norm, We experiment With the C variants by training Logistic Regression on asynthetic binary classification dataset where the labels are perturbed with probability Pnoise .
Figure 10: Pnoise is increased from 0 to 1 in increments of 0.1 indicating 10% more injection of noise.
Figure 11: Experiments on varying decay that across tasks. Graphs in the top row measure BLEUscore or accuracy (where a higher value is desired) and those in the bottom row measure perplexity(where a lower value is desired)F.7 topCThe optimizers’ sensitivity to topC (Figure 12) had a similar trend to other experiments, where theoptimizers showed better performance for lower values of topC .
Figure 12: Experiments on varying topC that across tasks. Top row: (a) MultiWoz - BiLSTM(BLEU Score) (b) CIFAR-10 - Convnet (Accuracy). Bottom row: (c) WikiText - LSTM (Perplexity)(d) MNIST - Log. Reg. and MLP (Accuracy).
Figure 13: The difference between average(||gc||2) and ||gt||2 shown in plots (a) and (b) indicatethat increasing topC could hurt the performance. These plots correlate to experiments in §621Published as a conference paper at ICLR 2022The plots of gt vs gc is crucial to the results in that they provide an explanations for the fasterconvergence (as well as smaller improvements) of the C variants over the base methods (Figure14). Since the parameter updates are reminded of large displacement of the gradients via the aggrmethod, the model updates the parameters more frequently. In the cases where gc is higher thangt , we observe better performance and no improvements in cases where the difference is not assignificant.
Figure 14: Additional results on gt vs gc, explaining rationale behind the better performances oflower values of topC across all the experiments. As the variance in aggr get diminished withincrease in topC the model does not gain much with increase in topC .
