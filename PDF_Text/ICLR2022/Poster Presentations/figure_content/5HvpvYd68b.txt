Figure 1:	The overall architecture of GLAT. The left half shows the training process of GLAT whilethe right half details the glancing sampling strategy.
Figure 2:	The overall architecture of switch-GLAT. The left module shows that GLAT is extendedto a multilingual version with the token-level language tag, the middle one describes how the code-switched sentence is produced through the code-switch decoder, and the right module introducesthe code-switch back-translation process, in which the generated code-switched target and its pairedsource is swapped to enhance training.
Figure 3: Inference speed evaluated on throughputs.
Figure 4: Representations learned by (a) Transformer (Vaswani et al., 2017), (b) GLAT (Qian et al.,2020), (c) M-Transformer and (d) switch-GLAT, projected to 2D.
Figure 5: Representations learned by (a) Transformer, (b) GLAT, (c) CLSR and (d) switch-GLAT,projected to 2D.
