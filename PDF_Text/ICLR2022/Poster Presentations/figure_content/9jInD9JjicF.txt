Figure 1: The illustration of the PoNet model architecture. The right enlarged view shows multi-granularity pooling (GA, SMP, LMP) and pooling fusion (Section 3).
Figure 2: MLM and SSO validation accuracy against the numbers of training steps from BERT-Base, FNet-Base, and PoNet-Base. All models are uncased.
Figure 3: MLM and SSO validation accuracy on 16GB datasets (Wikipedia and BooksCorpus)against the numbers of training steps from BERT-Base, FNet-Base, and PoNet-Base. All models areuncased.
Figure 4: The L2-Norm of the three pooing GA, SMP, and LMP and their average at different layersof PoNet.
Figure 5: The GA attention map and SMP argmax positions for the example “The word had filledhis head as though the girl had whispered directly into both ears.”This observation is consistent with the ablation results shown in Table 5, which shows thatremoving GA, removing SMP, or removing LMP in PoNet all significantly degrade theperformance on downstream single-sentence and sentence-pair tasks.
