Figure 1: Popular population learning algorithms implemented as directed interaction graphs (Bot-tom), or equivalently, a set of meta-game mixture strategies Σ ∈ R3×3 := {σi}i3=1 (Top). A directededge from i to j with weight σij indicates that policy i optimizes against j , with probability σij .
Figure 2:	Neural Population Learning in rock-paper-scissors induced by static interaction graphs.
Figure 3:	NeuPL in rock-paper-scissors induced by an adaptive interaction graph implementingPSRO-NASH. An epoch lasts 10 iterations. (Top) strategy space explored by the neural populationof strategies over time. (Middle) the learned payoff estimates between the population of strategies.
Figure 4: A NeuPL population developing an increasingly sophisticated set of diverse policies inrunning-with-scissors. The interaction graph is updated every 1,000 gradient updates (an epoch).
Figure 5: (Left) Relative Population Performance comparing independent NeuPL runs of differentmaximum population sizes. (Right) Effective population size over time, driven by FPSRO-N.
Figure 6: Learning progression of exploiters against incremental Nash mixture policies obtained viaNeuPL training (as shown in Figure 4). The red curve corresponds to learning an exploiter fromrandom initialization fully and the green curve corresponds to transferring encoder and memorycomponents from the trained NeuPL network at epoch 1,000. Each experiment is repeated five times.
Figure 7: Relative Population Performance between a NeuPL population and policy populationsobtained in PSRO baselines. Each PSRO variant is repeated over 3 trials, shown in shades.
Figure 8: NeuPL in 2-vs-2 MuJoCo Football implementing FPSRO-N . The red/blue/white tracescorrespond to the trajectories of red/blue players and the ball respectively.
Figure 9: An example view of the running-with-scissors environment upon initialization.
Figure 10: (Left) The conditional neural population network architecture for a Q-learning basedRL agent. The "	" sign denotes a concatenation of all inputs tensors and dashed components areonly used for training (and not necessary for acting); (Middle) A diagram illustrating the separationbetween the strategy-agnostic transitive skill dimension captured by the shared parameters θ andthe strategic cycles captured by σ at a skill-level; (Right) A diagram illustrating the mechanism ofa NeuPL self-play episode: at the start of an episode, a pair of conditioning vectors are sampled(σi , σj ) from the interaction graph Σ. The pair of policies, using the same conditional policy networkΠθ(a∣o≤, σ), act simultaneously in the environment. The circles correspond to the conditioningvectors, observations and actions for the exploiter policy (shaded) and its opponent (blank).
Figure 11: Visualization of empirical payoff matrices for each PSRO population after 8 iterationsacross three independent trials. Each iteration lasts 100k or 200k gradient steps. PSRO-C indicatesthat the policy trained at iteration i + 1 is initialized from the policy obtained at iteration i instead offrom random initialization. A payoff of 1.0 (-1.0) indicates a win (loss) probability of 100% for therow player.
Figure 12: Hyper-parameter sweep across proportion of matches used for empirical payoff evaluation() and interaction graph update interval in gradient steps (T). Population-level performance ismeasured in RPP, relative to the same held out population PSRO-C@200k with seed = 1.
Figure 13: Example NeuPL experiment with an interaction graph incorporating pre-trained policies.
