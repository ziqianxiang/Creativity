Figure 1: (a) Causal structure of Model 1. (b) A more practical extension of Model 1, where Z1 and Z2 are notdirectly observed and X is their observation. (c) A general version of (b), where we assume there exist multipleunobserved variables. Each of them could be either a parent, a child of Y , or has no direct connection with Y .
Figure 2: (a-d) Visualization of the samples (i.e., Z = (Z1, Z2)) in latent space recovered through differentalgorithms: (a) Samples from the true distribution; (b-c) Samples from the posterior inferred using VAE andiVAE, respectively. Apparently, our method (d) can recover the original data up to a permutation and a simplecomponentwise transformation. (e) The causal structure with Y having two causes describes the data generatingprocess of the synthetic dataset. (f) Mean correlation coefficient (MCC) scores for VAE, iVAE, and NF-iVAE onsynthetic data. (g) MCC scores for VAE, iVAE, and NF-iVAE on CMNIST. (h) The effects on the CMNISTimages of digit 8 (top two rows) and digit 3 (bottom two rows) when intervening on a causal factor Zi∈Ip andon a non-causal factor (effect) Zj∈Ic, respectively.
Figure 3: (a) General causal structure over {Xi, Y , X, E}, where the arrow from Zi to X is amust-have connection and the other four might not be necessary. (b) Ten possible causal structuresfrom (a) under Assumptions 1&2.
Figure 4:	(a) Causal structure with Y having two causes. (b) Data generating process correspondingto (a), where U{∙} denotes the discrete uniform distribution, N(∙) the Gaussian distribution, and g(∙)is given by a neural network with 2-dimensional input and 10-dimensional output, whose parametersare randomly set in advance.
Figure 5:	Visualization of the samples (i.e., Z = (Z1, Z2)) in latent space generated through differentalgorithms. (a) Samples from the true distribution. (b-e) Samples from the posterior of differentalgorithms. Apparently, our method (e) can recover the original data up to a permutation and a simplecomponentwise transformation.
