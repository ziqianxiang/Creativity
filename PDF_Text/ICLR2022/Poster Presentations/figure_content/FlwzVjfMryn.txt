Figure 1: Left: A basic setting in Multi-objective Optimization (MOO), optimizing M = 2 objectives inEqn. 1. (a) depicts the objective space (fι, f2) and (b) shows the search space X ∈ Ω. In (a), P denotes thePareto frontier, R is the reference point, the hypervolume HV is the space of the shaded area, and o(x) are thedominance numbers. In (b), once a few samples are collected within Ω, LaMOO learns to partition the searchspace Ω into sub-regions (i.e. Ωg°°d and Ωbad) according to the dominance number in objective space, and thenfocuses future sampling on the good regions that are close to the Pareto Frontier. This procedure can be repeatedto further partition Ωgo°d and Ωbad. Right: A table shows the properties of MOO methods used in experiments.
Figure 2: (a) The leaf nodes D and E that correspond to the non-splittable space Ωd and Ωe .(b). The nodeselection procedure based on the UCB value. (c). The new samples generation from the selected space Ωe forbayesian optimization.
Figure 3: Left: Branin-Currin with 2 dimensions and 2 objectives. Middle: VehicleSafety with 5 dimensionsand 3 objectives. Right: Nasbench201 with 6 dimensions and 2 objectives. We ran each algorithm 7 times(shaded area is ± std of the mean). Top: Bayesian Optimization w/o LaMOO. Bottom: evolutionary algorithmsw/o LaMOO. Note the two algorithm families show very different sample efficiency in MOO tasks.
Figure 4: DTLZ2 with many objectives, We ran each algorithm 7 times (shaded area is ± std of the mean).
Figure 5: Molecule Discovery: Left: Molecule discovery with two objectives (GSK3β+JNK3). Middle:Molecule discovery with three objectives (QED+SA+SARS). Right: Molecule Discovery with four objectives(GSK3β+JNK3+QED+SA). We ran each algorithm 15 times (shaded area is ± std of the mean).
Figure 6:	Visualization of selected region at different search iterations and nodes. (a) The Monte-Carlo treewith colored leaves. Selected Path is marked in red. (b) Visualization of the regions(Ωj, Ωk , Ωi , Ωe , Ωf , Ωg)that are consistent with leaves in (a) in the search space. (c) Visualization of selected path at final iteration. (d)Visualization of samPles during search; bottom left is the Pareto frontier estimated from one million samPles.
Figure 7:	Ablation studies on hyperparameters and sampling methods in LaMOO. Left: Sampling withoutBayesian/CMA-ES. Middle: Sampling with different Cp . Right: Partitioning with different svm kernelsAblation of Design Choices. We show how different hyperparameters and sampling methods play arole in the performance. We perform the study in VehicleSafety below.
Figure 8: Upper and lower bounds of g-1 (y) with different {wi}. Left: wi = 2linspace(-0.1,10) .
Figure 9:	Sampling with statiC Cp(10% of HVmax) and dynamiC Cp((10% of HVcurrent))As we mentioned in the paper, a "rule of thumb" is to set the Cp to be roughly 10% of the maximumhypervolume HVmax. If HVmax is unknown, Cp Can be dynamiCally set to 10% of the hypervolumeof Current samples in eaCh searCh iteration. The figures below demonstrate the differenCe between10% HVmax and 10% Current hypervolume in three problems(Branin-Currin, VehiCleSafety, andNasbenCh201 from left to right). The final performanCes by using 10% HVmax and 10% Currenthypervolume are similar.
Figure 10:	Wall clock time in different problemsFig. 10 shows the wall clock time of different search algorithms in BraninCurrin(Belakaria et al.,2019), VehicleSafefy (Liao et al., 2008) and Nasbench201 (Dong & Yang, 2020).
Figure 11: A general architecture of Nasbench201In Nasbench201, the architectures are made by stacking the cells together. The difference amongarchitectures in Nasbench201 is the design of the cells, see fig 11. Specifically, each cell contains 4nodes, and there is a particular operation connecting to two nodes including zeroize, skip-connect,1x1 convolution, 3x3 convolution, and 3x3 average pooling. Therefore, there are C42 = 6 edges ina cell and 56 =15625 unique architectures in Nasbench201. According to this background, Eacharchitecture can be encoded into a 6-dimensional vector with 5 discrete numbers (i.e., 0, 1, 2, 3, 4 thatcorresponds to zeroize, skip-connect, 1x1 convolution, 3x3 convolution, and 3x3 average pooling).
Figure 12: Visualization of Pareto-frontier in BraninCurrin, VehicleSafety as well as Nasbench201.
Figure 13: Dominance number distribution with 50 random samples on DTLZ2(10 objectives)Tgood	whole space	badselected regionFigure 14:	The range of hypervolume for 50 samples randomly generated from different regions in DTLZ2(10objectives). We generate 25 times of 50 samples in total.
Figure 14:	The range of hypervolume for 50 samples randomly generated from different regions in DTLZ2(10objectives). We generate 25 times of 50 samples in total.
Figure 15:	SearCh progress with sample24Published as a conference paper at ICLR 2022Q25 Q 5 QIlooαE0ΛJdAH 6oη0	50	100 150 200 250Average Cumulative Wall Clock Time(S)Q 5 Q 52 110EOΛJdAH 6oη0.06	250	500	750	1000Average Cumulative Wall CIockTime(S)tsφE-OΛJ,udAH 6oη0.0-0.2-0.4
Figure 16:	Search progress with timeInstead of traversing down the search tree to trace the current most promising search path, thisvariation of LaMOO directly select the leaf node with the highest UCB value. Algorithm. 3 illustratesthe detail of this variation. Therefore, this variation avoids calculating the hypervolume in thenon-leaf nodes of the tree, where hypervolume calculation is the main computational cost of LaMOOespecially in many-objective problems. Figure. 15 and figure. 16 validate the variation that is ableto reach similar performance of searched samples but saves lots of time. We leave the validation ofothers problems in the future works.
