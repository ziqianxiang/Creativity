Figure 1: Comparison of validation accuracies in percentage of both centralised learning and FL on the CI-FAR10 dataset with different sparsity and non-IID ratios. While centralised training suffers from minimaldegradation at very high sparsity ratios (95%), the opposite happens for FL: we observe a 10% accuracy drop.
Figure 2:	Evolution of the non-zero weights ratio after server aggregation (i.e. number of weights that arenon-zero divided by the total number of parameter in that layer) of all CNN layers of a ResNet-18 trainedon CIFAR10 with FL. Each of the 100 clients either send the top-10% or top-30% (i.e. weights with thehighest norm) to the server.
Figure 3:	Heatmaps of 6 CNN layers (layer 4-9) in ResNet-18 when trained on CIFAR10 with 100 clients byonly keeping the top 10% of weights. The weights are recorded every 20 communication rounds and flattenalong the y-axis. The consistency across rounds (x-axis) indicates that, for the most part, the locations ofnon-zero weights remains constant. A larger version of this picture is given in the Appendix A.3For instance, a weight that would be only sent by a single client as part of its top-10% parameterswould not be diluted with the noise of all the others clients. Conversely, this very same weight maybe completely corrupted if we send the entire dense model for aggregation.
Figure 4: Larger masking ratios do not offer a clear advantage over smaller values (e.g. 0.1) despite them up-loading a larger portion of the model parameters to the server after each round of local training. This experimentis conducted using a non-IID partitioning of CIFAR-10.
Figure 5: Heatmap, in bigger scale, of one CNN layers (layer 4) in ResNet-18 when trained on CIFAR10with 100 clients by only keeping the top 10% of weights. The weights are recorded every 20 communicationrounds and flatten along the y-axis. The consistency across rounds (x-axis) indicates that, for the most part, thelocations of non-zero weights remains constant.
Figure 6: Heatmap, of two CNN layers (layer 4 and 9) in ResNet-18 when trained on SpeechCommandswith 100 clients by only keeping the top 10% of weights. The weights are recorded every 20 communicationrounds and flatten along the y-axis. The consistency across rounds (x-axis) indicates that, for the most part, thelocations of non-zero weights remains constant.
