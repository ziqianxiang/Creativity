Figure 1: Covariance kernel KJ(a2c,2) constructed using Jacobi polynomials of type (2, 2) withλj = 1/j4, 1/j3, and Rj/j, where Rj is the Rissanen sequence (top). The bottom panels illus-trate functions sampled from GP(0, KJ(a2c,2)) with the different eigenvalue sequences. The series forgenerating the random functions are truncated to n = 500.
Figure 2: Left: Ratio between the average randomized SVD approximation error (over 10 runs) ofthe 2000 × 2000 matrix of the inverse of the differential operator Lu = d2u/dx2 - 100 sin(5πx)uon [0, 1], and the best approximation error. The error bars in light colour (blue and red) illustrateone standard deviation. Right: Average computational time of the algorithm (over 10 runs). Theeigenvalue decomposition of the covariance matrix has been precomputed before.
Figure 3: Kernels of three HS operators (top) together with the kernels learned by the randomizedSVD for HS operators (bottom), using the squared-exponential covariance kernel KSE with param-eter ` = 0.01 and one hundred functions sampled from GP(0, KSE).
Figure 4: Left: Scaled eigenvalues of the Jacobi covariance kernel KJ(a2c,2) with sequence λj = 1/j3and squared-exponential kernels KSE with parameters ` = 0.01, 0.1, 1, respectively. Right: Average(over 10 runs) relative approximation error in the L2-norm between the Bessel kernel G(x, y) =J0 (100(xy + y2)) and its low-rank approximation Gk (x, y), obtained from the randomized SVDby sampling the GPs k times. The error bars in light colour (blue and red) illustrate one standarddeviation and the black line indicates the best approximation error given by the tail of the singularvalues of G.
