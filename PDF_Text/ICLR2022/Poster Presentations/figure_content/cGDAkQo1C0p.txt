Figure 1: Multivariate time-series forecasting results comparing our method with the state-of-the-artbaselines, i.e., Informer (Zhou et al., 2021), N-BEATS (Oreshkin et al., 2020), and SCINet (Liu et al.,2021). The analysis is conducted on electricity consuming load (ECL) dataset, with a predictionlength of seven days. The predictions of the baselines are inaccurately (a) shifted and (b) scaled.
Figure 2: Overview of the proposed method. We illustrate an example of a univariate case, wherex(i) ∈ R1×Tx ; the input data x(i) is actually multivariate (See Section 3.1). In RevIN, the (a-1)instance normalization and (a-2) denormalization are symmetrically structured to remove (a-3) non-stationary information from one layer and restore it on the other layer. Here, RevIN is applied tothe input and output layers. The (a-3) non-stationary information includes statistical properties fromthe input data: mean μ, variance σ2, and learnable affine parameters γ, β. The normalization layertransforms the (b-1) original data distribution into a (b-2) mean-centered distribution, where thedistribution discrepancy between different instances is reduced. Using x, the model predicts thefuture values y following the (b-3) distribution where non-stationary information is eliminated. Torestore it (b-4), RevIN reverses the instance normalization in the output layer.
Figure 3: Effect of RevIN on distribution discrepancy between training and test data. From leftto right columns, we compare the training and test data distributions of a variable on each step of thesequential process in RevIN: (a) the original input x, (b) the input X normalized by RevIN, (C) themodel prediction output y, and (d) the output y denormalized by RevIN, the final prediction. Theanalysis is conducted on the ETT and ECL datasets using SCINet (Liu et al., 2021) as the baseline.
Figure 4:	Forecasting error for each time step. We compare the error of predicting 1 〜960 stepsahead between the baselines and RevIN on ETTh1 when the prediction length is 960 (40 days).
Figure 5:	Feature divergence between the training and test data in the intermediate layers of themodel The feature divergences are computed on the ETTh1, ETTh2, ETTm1, and ECL datasets usingthe features obtained from the first (Layer-1) and the second (Layer-2) encoder layers in Informer.
Figure 6: Prediction results on the Nasdaq dataset. The results on three variables in the data,Close, DTB6, and DE1, are shown. The prediction length is 60 days, and the seventh value is illus-trated to show the results on the entire test set. We compare RevIN with N-BEATS.
Figure 7: Impacts of the input sequence length on RevIN compared with the baselines. Weprolong the input length from 48 (two days) to 960 (40 days) when the prediction length is set as960 (40 days) on the ETThI dataset. The average errors for five runs, with standard deviation values,are reported. In N-BEATS, the standard deviation value for the mean squared error is too large tovisualize when the prediction length is 960; we write the value as “±4.955” instead.
Figure 8: Effect of RevIN on distribution discrepancy on training and test data compared toexisting dynamic normalization methods. We compare ReVIN with LSTNet*, which adds theautoregressive linear bypass module ofLSTNet to the baseline and ES-RNN*, which adds the expo-nential smoothing of ES-RNN to the baseline. The analysis is conducted on the ETTh2 dataset witha prediction length of 960 using N-BEATS as the baseline. From left to right, the columns comparethe training and test data distributions of each step of the sequential process in each method.
Figure 9: Additional multivariate time-series forecasting results comparing RevIN and state-of-the-art baselines. The analysis is conducted on the ETTh1, ETTh2, ETTm 1, and ECL datasets.
