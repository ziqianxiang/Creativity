Figure 1: Regional-to-Local Attentionfor Vision Transformers. (a) ViT usesa fixed patch size through the whole net-work, (b) PVT adopts a pyramid structureto gradually enlarge the patch size in the net-work. Both ViT and PVT uses all tokensin self-attention, which are computational-and memory-intensive. (c) Our proposed ap-proach combines a pyramid structure withan efficient regional-to-local (R2L) attentionmechanism to reduce computation and mem-ory usage. Our approach divides the inputimage into two groups of tokens, regional to-kens of large patch size (red) and local onesof small patch size (black). The two types oftokens communicate efficiently through R2Lattention, which jointly attends to the localtokens in the same region and the associatedregional token. Note that the numbers denotethe patch sizes at each stage of a model.
Figure 2: Architecture of the proposed RegionViT. Two paths are in our proposed network, includingregional tokens and local tokens, and their information is exchanged in the regional-to-local (R2L) transformerencoders. In the end, we average all regional tokens and use it for the classification. The tensor shape here iscomputed based on that regional tokens take a patch of 282 and local tokens take a patch of 42 .
Figure 3: Illustration of Regional-to-Local (R2L) TransformerEncoder. All regional tokens are first passed through regionalself-attention (RSA) to exchange the information among regionsand then local self-attention (LSA) performs parallel self-attentionwhere each takes one regional token and corresponding local to-kens. After that, all the tokens are passed through the feed-forwardnetwork and split back to the regional and local tokens. RSA andLSA share the same weights.
