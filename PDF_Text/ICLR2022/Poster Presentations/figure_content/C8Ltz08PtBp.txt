Figure 1: System Flow of SPL-DQN architecture3	Our AlgorithmOur method is originally motivated by NC-QR-DQN, where a special architecture is designed forthe last layer of the neural network to satisfy the monotonicity of F-1. The output represents theestimated quantile values at chosen quantile fractions. One drawback of discretization is that aprecise approximation for F -1 may need infinite fractions. But in practice, one can only use finitequantile fractions to estimate quantile values for decision making. In this work, we propose to learna dense approximation for F -1 using monotonic rational-quadratic splines (Gregory & Delbourgo,1982) as a building block.
Figure 2: (a) Windy Gridworld, with wind strength shown along bottom row. (b) & (c) The quantilefunctions for value distribution of the cyan square state and yellow circle state by MC, SPL-DQN(SPL), NC-QR-DQN (NC-QR), NDQFN, and QR-DQN (QR).
Figure 3: Performance comparisonin stochastic Cartpole. Each curveis averaged over 5 seeds.
Figure 4: Performance comparison in stochastic RoboSchool. Each curve is averaged by 7 seeds.
Figure 5: Performance comparison in two stochastic HalfCheetahs with enhanced randomness.
Figure 6: The learned quantile functions at the green square stateQuantile functions at green square stateA.2 CartpoleTraining hyperparameters We train SPL-DQN with QR-DQN, IQN, FQF, NC-QR-DQN, MM-DQN, and NDQFN in the stochastic Cartpole. The common parameter settings are summarized inTable 2. The model input is a vector of length 4, which contains the cart position, the cart velocity,the pole angle, and the pole angular velocity. The -greedy parameter decreases by 0.00005 everytime step. To make the results comparable, we use the same Feature Extractor for the methods.
Figure 7: Performance comparison in stochastic CartpoleA.3 PyB ulletGymNoise setting for different environments We introduce different noise levels in PyBulletGym’senvironments while ensuring that the robots won’t exhibit unrealistic motion. The noise settings areshown in Table 3.
Figure 8: Performance comparison of SPL and NDQFN when trained with uniformly spaced quan-tile fractions or random quantile fractions sampled from U ([0, 1]) in eight environments with DDPGas the baselineAlgorithm 1 DDPG with QR-based distributional critic (apart from FQF)Require: Initialize critic network Z(s, a∣ψ) with weights ψ; Initialize actor network μ(s∣θ) withweights θ; Initialize target network Z0 and μ0 with weights ψ0 J ψ, θ0 J θ; Initialize replybuffer D1:	for each episode do2:	Initialize random process OU for action exploration3:	for each time step t do4:	at 〜μ(st∣θ) + OUt5:	st+ι 〜p(st+ι |st ,at)6:	D J D ∪ {(st, at, r(st, at), st+1)}7:	Sample minibatch ofN transitions (si, ai, ri, si+1) from D8:	Choose current quantile fractions {τ } according to critic’s strategy9:	Compute corresponding current quantiles {qi} J Z(Si,ai∣ψ)10:	Choose target quantile fractions {τ} according to critic,s strategy11:	Compute corresponding target quantiles {qi+1} J ri + YZ0(si+1, μ0(si+1 ∣θ0)∣ψ0)12:	Update critic by minimizing QR loss13:	Compute expectation of quantiles Qs,a J E[Z(s, a∣ψ)]
