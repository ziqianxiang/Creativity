Figure 1: Our training method flattens the test weight-loss landscape. When moving away fromthe trained weight minimum (α = 0) in randomly-chosen directions, we find that our adversarialtraining method (Beta; Beta+Forward) finds deeper minima (for F-MNIST and Speech tasks) atflatter locations in the cross-entropy test loss landscape. See text for further details, and Fig. S2 forvisualisation over several random seeds.
Figure 2:	Adversarial attack during training protects networks against random mismatch-induced parameter noise. Networks were evaluated for the Speech (a), ECG (b) and F-MNISTtasks (c), under increasing levels of simulated mismatch (ζ). Networks trained using standard SGDwere disrupted by mismatch levels ζ > 0.1. At all mismatch levels our adversarial training approachperformed significantly better in the presence of mismatch (higher test accuracy). Red boxes highlightmisclassified examples.
Figure 3:	Our training method protects against task-adversarial attacks in parameter space.
Figure 4: Networks trained with our method show overall better performance when deployedon PCM-based CiM hardware. This figure shows the performance degradation as a consequence ofthe PCM devices drifting over time (x-axis, up to one year) of networks deployed on CiM hardware.
Figure 5: Our method consistently yields networks that outperform training with Gaussiannoise injection. This figure compares the robustness to Gaussian noise at various levels (ζ) fornetworks trained with our method (blue) and a networks trained with Gaussian noise injection (red),where the level of noise used during training (ηtrain) matches the noise used during inference. Eachrow represents a different type of noise: The first row models Gaussian noise with a standard deviationthat is proportional to the largest absolute weight in the individual weight kernels (Joshi et al., 2020)and the second row follows the model presented in this paper (see Eq. 1).
Figure S1:	Quantification of mismatch on analog neuromorphic hardware. Parameter valuesfor several weight parameters and membrane time constants were measured for a range of nominalparameter values, using an oscilloscope directly connected to a mixed-signal neuromorphic SNNprocessor. (a-c) Various parameters of the chip follow a Gaussian distribution with increasing width.
Figure S2:	Illustration of the test weight loss-landscape highlighting individual trials to mea-sure the loss landscapes. See the results text for more details.
Figure S3:	Robustness to weight attack targeting KL divergence during inference. When thenetwork is attacked by maximizing the KL-divergence between the normal and attacked network, ouradversarially trained networks are more robust than standard SGD or AWP.
Figure S4:	Constant magnitude versus magnitude-relative parameter noise. (left, middle) Whenparameter noise of constant magnitude (blue) is introduced by the adversarial attack during training,the networks learn to increase the magnitude of the weights to trivially improve robustness to theconstant-magnitude attack. When the parameter attack is relative to each parameter magnitude, as inthe main text (red), the weight magnitudes do not increase. These networks were trained for a rangeof βrob, i.e. varying emphasis on robustness.
Figure S5: Our parameter attack is effective at decreasing the performance of a network dur-ing inference, and using our attack during training protects a network from later disruption.
Figure S6:	Effect of βrob on the test loss landscape. Increasing βrob promotes robustness byflattening the test loss landscape. However, increasing βrob does not lead to a systematic rise in losson the test set.
Figure S7:	Effect of βrob on the training loss landscape. Weight loss landscape computed on thetraining set (c.f. Fig. S6). Increasing βrob leads to flatter loss-landscapes (increased robustness) asbefore, but also a systematic increase in training loss (higher values for cross-entropy loss; noteordering of curves).
Figure S8:	Increasing βrob during training improves robustness during inference. The CNNwas trained using different values of βrob and evaluated on the test set after the weights were perturbedeither randomly (red) or adversarially (black).
Figure S9:	Networks trained with our method are provably more robust than those trainedwith standard gradient descent or forward noise alone. We used interval bound propagation todetermine the proportion of test samples that are verifiably correctly classified, under increasingweight perturbations ζ. For both the Speech and ECG tasks, computed on the trained LSNNs, ourmethod was provably more robust for ζ < 5 × 10-4 .
