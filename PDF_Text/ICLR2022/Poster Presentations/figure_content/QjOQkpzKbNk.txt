Figure 2: (a) Synthesis of style-mixed triplets (xt1,xt2,yt). (b) Overview of our data-free knowledge distil-lation for I2I translation. We transfer knowledge from the teacher pretrained discriminator DT to both thestudent encoder ES and the reference encoder ER. In addition, we transfer from the pretrained generator GTto the student generator GS . Here xt1 is the input image, xt2 is the reference image, and yt is the target image.
Figure 3: (a) Qualitative comparison on Animalfaces, Birds and Foods datasets.(b) Ablation study of variantsof our method on Animalfaces, Birds and Foods. (c) User study.
Figure 4: Qualitative results of the proposed method on various image synthesis tasks.
Figure 5: Examples generated after semantic diversity loss. For each triplet, we depict the teacher’s out-put(left), the student’s output (middle) with condition cx and the one (right) with condition cy . Note thestudent model is SSA-GAN. Results could be found in the Appendix C when student model is S2IGAN.
Figure 6: Qualitative results of the proposed method on multi-class I2I translation task. Ame.Gol.:AmericanGoldfinch (Breeding Male), Red Cro.:Red Crossbill (Adult Male), Ind. Bun.: Indigo Bunting (Adult Male).
Figure 7: Examples generated after transfer leaning for I2I system. The student model is StarGANv2. Note itis under reference-guided mode.
Figure 8: Examples generated after transfer leaning for I2I system. For one input, wegenerate images. The student model is StarGANv2. Note it is under latent-guided mode.
Figure 9: Examples generated after semantic diversity loss. For each triplet, we depict the teacher’s out-put(left), the student’s output (middle) with condition cx and the one (right) with condition cy . Note thestudent model is SSA-GAN.
Figure 10: Examples generated after semantic diversity loss. For each triplet, we depict the teacher’s out-put(left), the student’s output (middle) with condition cx and the one (right) with condition cy . Note thestudent model is S2IGAN.
Figure 11: Examples generated without semantic diversity loss for tasks: audio2image (top) and text2image(bottom). For each triplet, we depict the teacher’s output, the student’s output with condition cx and the onewith condition cy .
Figure 12: Qualitative results on text-to-image synthesis task.
Figure 13: Qualitative results on audio-to-image synthesis task.
Figure 14: Qualitative results on segmentation map-to-image synthesis task.
Figure 15: Qualitative results with reference images on segmentation map-to-image synthesis task.
Figure 16: Qualitative results on image-to-image synthesis task. We translate the input image (top left) intoall 149 categories. Please zoom-in for details.
Figure 18: Qualitative comparison on Animal faces datasets.
Figure 19: Qualitative results on image-to-image synthesis task. We translate the input image (top left) intoall 555 categories. Please zoom-in for details.
Figure 20: Qualitative results with reference image for image-to-image synthesis task on Birds dataset . Refe.:reference image, Reco.: reconstructed image32PUbliShed as a ConferenCe PaPersICLR 2022Output Output Refe. Input Output Output Refe.
Figure 22: Qualitative results on image-to-image synthesis task. We translate the input image (top left) intoall 256 categories. Please zoom-in for details.
Figure 23: Qualitative results with reference image for image-to-image synthesis task on Foods dataset . Refe.:reference image, Reco.: reconstructed image35Published as a conference paper at ICLR 2022Output Output Refe. Input Output Output Refe. Input	Output	Output Refe. InputOurs StarGANv2	Ours StarGANv2	Ours StarGANv2Figure 24: Qualitative comparison on Foods datasets.
Figure 24: Qualitative comparison on Foods datasets.
