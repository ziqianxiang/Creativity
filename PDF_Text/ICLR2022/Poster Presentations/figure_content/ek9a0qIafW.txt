Figure 1: The architecture of DifferentiAble pRompT (DART) model comparing with MLM pre-training and conventional fine-tuning, where Ti and Yi are unused or special tokens in the vocabulary.
Figure 2: (a) Few-shot results using the ACE-2005. We used K = 4, 8, 16, and 32 (# examples perclass) with BERT. (FT= Fine-tuning) (b) BERT-large vs. GPT-2-medium results for the SemEval.
Figure 3: Visualization of masked tokens’ representation in different training steps (with training 10,30, 50, 70 steps from left to right) with fixed prompts.
Figure 4: Visualization of masked tokens’ representation in different training steps (with training 10,30, 50, 70 steps from left to right) with differentiable prompts.
Figure 5: A 3D visualization of severallabel representations learned in DART.
Figure 6: The RD ratio curve on dev set of CR task of fixed prompt and differentiable prompt duringtraining.
