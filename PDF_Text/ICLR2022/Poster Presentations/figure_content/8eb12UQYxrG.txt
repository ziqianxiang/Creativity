Figure 1: Overview of our experimental setup for investigating out-of-distribution generalization in down-stream tasks. (1) We train 240 Î²-VAEs on the robotic dataset from Dittadi et al. (2021c). (2) We then traindownstream policies to solve object reaching or pushing, using multiple random RL seeds per VAE. The input toa policy consists of the output of a pretrained encoder and additional task-related observable variables. Crucially,the policy is only trained on a subset of the cube colors from the pretraining dataset. (3) Finally, we evaluatethese policies on their respective tasks in four different scenarios: (a) in-distribution, i.e. with cube colors usedin policy training; (b) OOD1, i.e. with cube colours previously seen by the encoder but OOD for the policy;(c) OOD2-sim, having cube colours also OOD to the encoder; (d) sim-to-real zero-shot on the real-world setup.
Figure 2: Top: Average training success, aggregated over all policies from the sweep (median, quartiles, 5th/95thpercentiles). Bottom: Rank correlations between representation metrics and in-distribution reward (evaluatedwhen the policies are fully trained), in the case without regularization. Correlations are color-coded in red(positive) or blue (negative) when statistically significant (p<0.05), otherwise they are gray.
Figure 3: Correlations betweentraining (in distrib.) and OODrewards (p<0.05).
Figure 4: Rank correlations of representation properties with OOD1 and OOD2 reward on object reachingwithout regularization. Numbering when splitting metrics by FoV: (1) cube color; (2-4) joint angles; (5-7) cubeposition and rotation. Correlations are color-coded as described in Fig. 2.
Figure 5: Box plots: fractional success on object reaching split according to low (blue), medium-high (orange),and almost perfect (green) disentanglement. L1 regularization in the first layer of the MLP policy has a positiveeffect on OOD1 and OOD2 generalization with minimal sacrifice in terms of training reward (see scale).
Figure 6: Zero-shot sim-to-real on object reaching on over 2,000 episodes. Left: Rank-correlations on thereal platform with a red cube (color-coded as described in Fig. 2). Middle: Training encoders with additivenoise improves sim-to-real generalization. Right: Histogram of fractional success in the more challengingOOD2-real-{green,blue} scenario from 50 policies across 4 different goal positions.
Figure 7: We select pushing policies with high GS-OOD2-real score. When deployed on the real robotwithout fine-tuning, they succeed in pushing the cube toa specified goal position (transparent blue cube).
Figure 8: Rank correlations between metrics and in-distribution reward, with and without adjusting for informa-tiveness. Correlations are color-coded as described in Fig. 2.
Figure 9: Rank correlations between metrics and OOD reward, with and without adjusting for informativeness.
Figure 10: Fractional success on object reaching (top) and pushing (bottom), split according to low (blue),medium-high (orange), and almost perfect (green) disentanglement. Results for object reaching are also reportedin Fig. 5 in Section 4.2.
Figure 11: Sample efficiency analysis for object reaching. Rank correlations of rewards with relevant metricsalong multiple time steps. Correlations are color-coded as described in Fig. 2.
Figure 12: Sample efficiency analysis for pushing. Rank correlations of rewards with relevant metrics alongmultiple time steps. Correlations are color-coded as described in Fig. 2.
Figure 13: Testing policies for object reaching under the same in-distribution, OOD1, and OOD2 evaluationprotocols regarding object color in simulation, but replacing the cube with a sphere, which was never used intraining.
Figure 14: Transferring policies for object reaching to the real robot setup without any fine-tuning on a greensphere (unseen shape and color). Correlations are color-coded as described in Fig. 2.
