Figure 1: Federated training results demonstrating participation gaps for six different tasks.
Figure 2: Illustration of the three-way split via avisualization of the EMNIST digits dataset. Eachcolumn corresponds to the dataset of one client. Adataset is split into participating training, participatingvalidation, and unparticipating data, which enablesseparate measurement of out-of-sample and participa-tion gaps (unlike other works). Note we only presentthe digit “6” for illustrative purposes.
Figure 3:	T-SNE projection of different partitionings of EMNIST. The toppanel shows the naturally-partitioned dataset (partitioned by writer), the bottompanel shows the label-based synthetic dataset. The gray points are the projec-tions of examples from each dataset, obtained by aggregating the data from 100clients each. The blue points show projections of data from a single client. Thenaturally-partitioned client data appears much more tightly clustered, whereas thelabel-based partitioned data appears similarly distributed as the overall dataset,indicating that label-based partitioning may not fully represent realistic clientheterogeneity.
Figure 4:	Comparison of label-based synthetic partitioning and natural partitioning ofEMNIST-10. Observe that label-based partitioning shows greater label heterogeneity (a) thannatural partitioning (c), while the participation gap (part_val - unpart) for label-based syntheticpartitioning (b) is significantly smaller than that for the natural partitioning (d).
Figure 5: Comparison of label-based partitioning and semantic partitioning (ours). Results forCIFAR-10 and CIFAR-100 are shown. Observe that semantic partitioning recovers the participationgap typically observed in naturally-partitioned data.
Figure 7: Effect of the number of participating clients. See Section 5.1 for discussion.
Figure 8: Effect of diversity on generalization. We fix the totalamount of training data while varying the concentration acrossclients. The concentration varies from taking only 5% clients asparticipating clients where each client contributes 128 trainingdata, to the most diverse distribution with 80% clients as partic-ipating clients but each client only contributes 8 training data.
Figure 9:	Centralized training progress on six different federated tasks. Observe that the partici-pation gap still exists even with centralized optimizers. We refer readers to Table 1 for a quantitativecomparison between federated optimizers and centralized optimizers.
Figure 10:	Accuracies of the client at the 25th percentile versus the communication rounds.
Figure 11:	Consistency of participation gaps across hyperparameter choice (learning ratesconfiguration). We present the best four (4) combination of learning rates for federated trainingof EMNIST-10. Here ηc stands for client learning rate, and ηs stands for server learning rate. Weobserve that the participation gap is consistent across various configurations of learning rates.
Figure 12: Effect of multiple client epochs per round on EMNIST-62. We repeat the experimenton EMNIST-62 but instead let each sampled client run multiple local epochs per communicationround. The other settings (including the total communication rounds) remain the same. We observethat the participation gap is consistent across various settings of local epochs.
Figure 13: Consistency of participation gaps across hyperparameter choice (learning ratesconfiguration). We present the best four (4) combination of learning rates for federated training ofCIFAR-10. Here ηc stands for client learning rate, and ηs stands for server learning rate. We observethat the participation gap is largely consistent across various configurations of learning rates.
Figure 14:	Consistency of participation gaps across hyperparameter choice (learning ratesconfiguration). We present the best four (4) combination of learning rates for federated training ofCIFAR-100. Here ηc stands for client learning rate, and ηs stands for server learning rate. We observethat the participation gap is consistent across various configurations of learning rates.
Figure 15:	Effect of `2 weight decay onCIFAR-100 training. We federated train theResNet-18 networks for CIFAR-100 with vari-ous levels of weight decay ranging from 10-5to 10-2. We observe that a moderate scale ofweight decay might improve the unparticipat-ing accuracy and therefore decrease the par-ticipation gap. However, an overlarge weightdecay might hurt both participating validationand unparticipating performance.
Figure 16:	Effect of a deeper ResNet onCIFAR-100 training. We federatedly train aResNet-50 for CIFAR-100 to compare withour default choice (ResNet-18). We apply aconstant learning rate (instead of step decaylearning rate) for easy comparison. We observethat while using a deeper model improves theoverall accuracy, the participation gap is stillreasonably large for ResNet-50.
Figure 17: Visualization of semantic partitioning of CIFAR-100. We partition the CIFAR-100dataset into 100 clients without resorting to external user information (such as writer identification).
Figure 18: Visualization of semantic partitioning of MNIST. We partition the (classic) MNISTdataset into 300 clients without resorting to external user information (such as writer identification).
