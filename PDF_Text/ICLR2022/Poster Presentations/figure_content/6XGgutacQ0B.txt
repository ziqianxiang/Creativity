Figure 1: Cosine similarity of network weights relative to initialization for each singular (in de-scending order) value direction of the data. Here, we use the same setting in Figure 2. We comparetraining the BN model (9) to the equivalent whitened model (16), noting that the BN model fits highsingular value directions of the data first, while the whitened model fits all directions equally, therebyoverfitting to non-robust features. This shows implicit regularization effect of SGD.
Figure 2: CIFAR-10 classification accu-racies for a two-layer ReLU network in(9) and the equivalent whitened model(16) with SGD and (n, d, m1, β, bs) =(50000, 3072, 1000, 10-4, 1000), wherebs denotes batch size. The whitenedmodel overfits much more than the stan-dard BN model, indicating implicit regu-larization.
Figure 3: Test accuracy of two-layer BN ReLU networks including the standard non-convex (baseline),its convex equivalent (8), along with their truncated versions with k = (215, 200) respectively. Allproblems are solved with SGD. The learning rates are chosen based on test performance and weprovide learning rate and optimizer comparisons in Appendix A.
Figure 4: Comparing the objective val-ues of the closed-form solution in The-orem 2.4, (“Theory”), and GD (5 trials)applied to the non-convex problem (9)for three-class CIFAR classification with(n,d,m1,β) = (1500, 3072, 1000, 1).
Figure 5: Comparison of different batch sizes (bs) for four-layer CNN with BN layers, (ml , lr, β) =(1000, 10-6, 10-4). We demonstrate that different batch sizes perform roughly equivalently in bothtraining and test convergence.
Figure 6: Comparison of different truncation schemes of convex program compared to SGD baseline,with (m1, β, bs, lr) = (10000, 10-2, 900, 10-5), and k ∈ {10, 100, 200, 400, 856}, where k =856 issimply the convex program without truncation. Large values of k overfit, while low values underfit,and intermediate values match exactly the performance of SGD baseline.
Figure 7: Here, we provide a longer trained version of the CIFAR-10 and CIFAR-100 experimentsin Figure 3 (in the main paper). We see that training for longer does not affect the results, as allmodels begin to overfit, and they overfit at the same rate. For CIFAR-100, we observe that thebaseline overfits so much that the standard Convex model eventually overtakes it in generalizationperformance.
Figure 8: Batch size (bs) ablation study, considering bs ∈ {100, 500, 1000} for SGD (Baseline) andSGD-Truncated for the experiment in Figure 3 (in the main paper). We see that smaller batch sizes donot improve the performance of SGD. Therefore, we choose bs= 1000 for all the main experiments.
Figure 9: Learning rate (lr) ablation study for the experiment in Figure 3 (in the main paper),considering lr∈ {10-5, 5 × 10-5, 10-4} for SGD and SGD-Truncated. We see that larger lrs do notimprove the performance of SGD, because these models with larger lrs reach the same peak accuracybut overfit quicker. For CIFAR-10, we selected learning rates 10-5 for both SGD and SGD-Truncated.
Figure 10: A counterpart of the experiment in Figure 3 (in the main paper), where we show testaccuracies and all problems are solved with Adam. We see that these results mirror those with SGD,where Convex-Truncated performs equally well as or outperforms the baseline.
Figure 11: Here, we provide a longer trained version of the CIFAR-10 and CIFAR-100 experimentsin Figure 10. We see that training for longer does not change the results, as all models begin to overfit.
Figure 12: Learning rate (lr) ablation study for the experiment in Figure 10, where we consider lr ∈{10-5, 5 × 10-5, 10-4} for Adam. We see that larger learning rates do not improve the performanceof Adam or Convex-Truncated, because these models reach the same peak accuracy but simply overfitearlier. For all models, we select a learning rate of 10-5 for the CIFAR-10 experiment in Figure 10a.
Figure 13: Comparison of two-layer BN networks presented Figure 3 (in the main paper), to thenon-convex two-layer ReLU architecture without BN, and its equivalent convex dual as presentedin Pilanci & Ergen (2020). All problems are solved with SGD and the learning rates are chosenbased on test performance. We find that both the SGD and Convex-Truncated programs improve ingeneralization performance when adding BN, though the margin may vary depending on the problem.
