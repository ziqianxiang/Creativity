Figure 1: Left: ResNet50 (large neural network) benefits from regularization techniques, whileMobileNetV2-Tiny (tiny neural network) losses accuracy by these regularizations. Right: Largeneural networks suffer from over-fitting, thus require regularization such as data augmentation anddropout. In contrast, tiny neural networks tend to under-fit the dataset, thus requires more capacityduring training. NetAug augments the network (reverse dropout) during training to provide moresupervision for tiny neural networks. Contrary to regularization techniques, it improves the accuracyof tiny neural networks and as expected, hurts the accuracy of non-tiny neural networks.
Figure 2: Left: We augment a tiny network by putting it into larger neural networks. They sharethe weights. The tiny neural network is supervised to produce useful representations for largerneural networks beyond functioning independently. At each training step, We sample one augmentednetwork to provide auxiliary supervision that is added to the base supervision. At test time, onlythe tiny network is used for inference, which has zero overhead. Right: NetAug is implemented byaugmenting the width multiplier and expand ratio of the tiny network.
Figure 3: NetAug outperforms the baseline under different numbers of training epochs on ImageNetand ImageNet-21K-P for MobileNetV2-Tiny. With similar accuracy, NetAug requires 75% fewertraining epochs on ImageNet and 83% fewer training epochs on ImageNet-21K-P.
Figure 4:	Learning curves on ImageNet. Left: NetAUg alleviates the under-fitting issue of tinyneural networks (e.g., MobileNetV2-Tiny), leading to higher training and validation accuracy. Right:Larger networks like ResNet50 does not suffer from under-fitting; applying NetAug will exacerbateover-fitting (higher training accuracy, lower validation accuracy).
Figure 5:	On Pascal VOC and COCO, models pre-trained with NetAug achieve a better performance-efficiency trade-off. Similar to ImageNet classification, adding detection mixup (Zhang et al., 2019b)that is effective for large neural networks causes performance drop for tiny neural networks.
