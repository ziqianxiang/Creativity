Figure 1: The standard process of evaluating GGMs using NN-based metrics.
Figure 2: Top: Example of a grid graph corrupted by rewiring edges with various probabilities p. Bottom:Principal component analysis (PCA) of embeddings obtained using the pretrained and random GINs for gridgraphs with different rewiring probabilities. For a strong feature extractor, we expect the corrupted graphs todiverge from the real graphs in embedding space as the edge rewiring probability grows. The random GINextracts as strong a representation as the pretrained GIN which indicates it may be useful in GGM evaluation.
Figure 3: Results from the fidelity (top) and diversity experiments (bottom). NN-based metrics using a randomGIN are highlighted in red, while classical metrics are in blue. Results are aggregated across all datasets and allGIN configurations (if applicable). For the violin plots, white dots are the median, thick black lines are the IQR,and thin black lines are the whiskers. The plots on the right show the mean value of select metrics throughouta given experiment. Fidelity: Several NN-based and classical metrics perform nearly optimally across bothexperiments with median rank correlations close to 1.0. Diversity: Classical metrics are below average in themode collapse experiment, and suboptimal in the mode dropping experiment. Scalar metrics such as MMDRBF have median rank correlations close to 1.0 and perform extremely well across both experiments.
Figure 4:	Results from the mixing random graphs experiment across all datasets and all GINconfigurations (if applicable).
Figure 5:	Results from the rewiring edges experiment across all datasets and all GIN configurations(if applicable).
Figure 6:	Results from the mode collapse experiment across all datasets and all GIN configurations(if applicable).
Figure 7:	Results from the mode dropping experiment across all datasets and all GIN configurations(if applicable).
Figure 8:	Results from the sample efficiency experiment across all datasets and all GIN configura-tions (if applicable).
Figure 9: Results from the randomizing edge features (left) and randomizing node features (right)experiments on the ZINC dataset.
Figure 10: The wall-clock time of each metric as datasets scale in a single dimension. Activationsis the time to extract graph embeddings from GIN on a CPU.
Figure 11: The wall-clock time of each metric as datasets scale in a single dimension. Activationsis the time to extract graph embeddings from GIN on a GPU.
Figure 12: The estimated memory usage of each metric as the dataset scales in a single dimension.
