Figure 1: (a) A sequence of states Si from the expert dataset. The states Si are reached by samplingan action from the advisor policy Ï€a from every expert state. State S2 is less desirable than S2 sincethe latter is closer to the expert state s2 . The squiggly lines show the path that a learner, that isoptimized to match the state-action distribution of the advisor, may take. (b) A high-level overviewof our approach. While GAIfO directly matches the state-transition distributions of the expert andthe learner, we learn an intermediary policy (advisor) in the l-MDP that acts as the surrogate expertfor the learner. D is used to denote the corresponding distance metric.
Figure 2: Learning curves for AILO and the baselines for different environments with discrepancy in dynamicsAblation on the degree of dynamics mismatch. For the empirical results in Figure 2, the variationin mass, gravity, or friction, between the e-MDP and the l-MDP was kept at a constant factor.
Figure 3: Results with differentamount of dynamics mismatch8Under review as a conference paper at ICLR 20226	ConclusionIn this paper, we present AILO, our algorithm for imitation learning from observations under transi-tion model disparity between the expert and the learner environments. Rather than directly matchingthe state-transition distributions across environments, we train an intermediary policy (advisor) inthe learner environment and use it as a surrogate expert for the learner. Towards learning an advi-sor that acts as an effective surrogate, we propose to minimize the cross-entropy distance betweenthe state-conditional next-state distributions of the advisor and the expert. To realize this idea intoa scalable ILO algorithm, we leverage prior work on support estimation (RED). Our experimentson five MuJoCo locomotion tasks with different types of dynamics discrepancies show that AILOcompares favorably to the baseline ILO methods in many cases.
Figure 4: Illustration of the expert states, the actions by the advisor policy, and the path taken by thelearner policy in a grid-world environment.
