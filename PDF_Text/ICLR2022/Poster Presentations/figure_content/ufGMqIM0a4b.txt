Figure 1: Synthesizing infinite-pixel images from finite-sized training data. A 1024×2048image composed of 242 patches, independently synthesized by InfinityGAN with spatial fusion oftwo styles. The generator is trained on 101×101 patches (e.g., marked in top-left) sampled from197×197 real images. Note that training and inference (of any size) are performed on a single GTXTITAN X GPU. Zoom-in for better experience.
Figure 2: Overview. The generator of InfinityGAN consists of two modules, a structure synthesizerbased on a neural implicit function, and a fully-convolutional texture synthesizer with all positionalinformation removed (see Figure 3). The two networks take four sets of inputs, a global latentvariable that defines the holistic appearance of the image, a local latent variable that representsthe local and structural variation, a continuous coordinate for learning the neural implicit structuresynthesizer, and a set of randomized noises to model fine-grained texture. InfinityGAN synthesizesimages of arbitrary size by learning spatially extensible representations.
Figure 3: Padding-free generator. (Left) Conventional generators synthesize inconsistent pix-els due to the zero-paddings. Note that the inconsistency region grows exponentially as the net-work deepened. (Right) In contrast, our padding-free generator can synthesize consistent pixelvalue regardless of the position in the model receptive field. Such a property facilitates spatially-independently generating patches and forming into a seamless image with consistent feature values.
Figure 4: Qualitative comparison. We show that InfinityGAN can produce more favorable holisticappearances against related methods while testing with an extended size 1024×1024. (NCI: Non-Constant Input, PFG: Padding-Free Generator). More results are shown in Appendix E.
Figure 5: LSUN bridge and tower.
Figure 7: Spatial style fusion. We present a mechanism in fusing multiple styles together to increasethe interestingness and interactiveness of the generation results. The 512×4096 image fuses fourstyles across 258 independently generated patches.
Figure 8: Outpainting long-range area. Infin-ityGAN synthesizes continuous and more plau-sible outpainting results for arbitrarily large out-painting areas. The real image annotated with redbox is 256×128 pixels.
Figure 9: Multi-modal outpainting. Infinity-GAN can natively achieve multi-modal outpaint-ing by sampling different local latents in the out-painted region. The real image annotated withred box is 256×128 pixels. We present more out-painting samples in Appendix M.
Figure 10: Image inbetweening with inverted latents. The InfinityGAN can synthesize arbitrary-length cyclic panorama and inbetweened images by inverting a real image at different position. Thetop-row image size is 256×2080 pixels. We present more samples in Appendix N and Appendix O.
Figure 11: ALIS can suffer from blocky artifacts, inter-patch discontinuity and lattice artifacts.
Figure 12: InfinityGAN with free-form anchor placements. The spatial style fusion of Infinity-GAN can place any number of style centers (called anchors in ALIS) at any location. We show case(top) a five-anchor example and (bottom) the corresponding regions covered by each style center.
Figure 13: ALIS cannot synthesis with a single holistic appearance. ALIS synthesizes repetitivecontent if all anchors share the same latent vector.
Figure 14: MS-PIE creates repetitive content in the expand setting. We train the no-paddingStyleGAN2 with MS-PIE at 256, 384 and 512 scales, then synthesize at 1408 scale.
Figure 17: Qualitative results on satellite image dataset. InfinityGAN trained on satellite im-age dataset with saturating coordinates (i.e., tanh) on vertical direction and cyclic coordinates onhorizontal direction.
Figure 18:	A high-level overview of InfinityGAN model architecture.
Figure 19:	The low-level design of each module within InfinityGAN.
Figure 20: More qualitative comparisons. We show more samples on Flickr-Landscape at1024×1024 pixels.
Figure 21: More qualitative results. We provide more images synthesized at 1024×1024 pixelswith our InfinityGAN trained on Flickr-Landscape. All images are synthesized with the same modelpresented in the paper, which is trained with 101×101 patches cropped from 197×197 resolutionreal images. All images share the same coordinate and present a high structural diversity. Note thatthe images are down-sampled 2× to reduce file size.
Figure 22: More qualitative results. We provide more images synthesized at 1024×1024 resolutionwith our InfinityGAN trained on Flickr-Landscape. All images are synthesized with the same modelpresented in the paper, which is trained with 101×101 resolution patches cropped from 197×197resolution real images. All images share the same coordinate and present a high structural diversity.
Figure 23: More qualitative results. We provide more images synthesized at 1024×1024 resolutionwith our InfinityGAN trained on Flickr-Landscape. All images are synthesized with the same modelpresented in the paper, which is trained with 101×101 resolution patches cropped from 197×197resolution real images. All images share the same coordinate and present a high structural diversity.
Figure 24: InfinityGAN samples training at a higher resolution. We synthesize 4096×4096pixel images using InfinityGAN trained on Flickr-Landscape at 397×397 pixels patches croppedfrom 773×773 full images. The top and bottom rows are zoom-in view of the image. Note that thefigure is 2× down-sampled to reduce file size.
Figure 25: InfinityGAN samples training at a higher resolution. We synthesize 4096×4096pixel images using InfinityGAN trained on Flickr-Landscape at 397×397 pixels patches croppedfrom 773×773 full images. The top and bottom rows are zoom-in view of the image. Note that thefigure is 4× down-sampled to reduce file size.
Figure 26: InfinityGAN samples training at a higher resolution. We synthesize 4096×4096pixel images using InfinityGAN trained on Flickr-Landscape at 397×397 pixels patches croppedfrom 773×773 full images. The top and bottom rows are zoom-in view of the image. Note that thefigure is 4× down-sampled to reduce file size.
Figure 27: InfinityGAN samples training at a higher resolution. We synthesize 4096×4096pixel images using InfinityGAN trained on Flickr-Landscape at 397×397 pixels patches croppedfrom 773×773 full images. The top and bottom rows are zoom-in view of the image. Note that thefigure is 4× down-sampled to reduce file size.
Figure 28: LSUN bridge category. InfinityGAN synthesis results at 512×512 pixels on LSUNbridge category. The model is trained with 101×101 pixels patches cropped from 197×197 resolu-tion real images.
Figure 29: LSUN tower category. InfinityGAN synthesis results at 512×512 pixels on LSUN towercategory. The model is trained with 101×101 pixels patches cropped from 197×197 resolution realimages.
Figure 30: Generation diversity. We show that structure synthesizer and texture synthesizer sepa-rately models structure and texture by changing either the local latent or style while all other vari-ables are fixed. The results also show that InfinityGAN can synthesize a diverse set of landscapestructures at the same coordinate. All samples are synthesized at 389×389 pixels with InfinityGANtrained at 101×101.
Figure 31: We provide a 256×9984 pixels sample synthesized with InfinityGAN. The sample showsthat (a) our InfinityGAN can generalize to arbitrarily-large sizes, and (b) the synthesized contentsdo not self-repeat while using the sample global latent variable zg .
Figure 32: Illustration of fusion map creation procedure and spatial fusion generation. Witha toy architecture example shown in (a) and a style fusion map in the pixel space (bottom of (b)),we can reversely create spatially aligned fusion maps in all intermediate layers by padding or in-terpolating the fusion map in the previous layer. Spatial style fusion in (c) uses the fusion maps tosynthesize images with a natural style transition in the pixel space.
Figure 33: Implementation of spatial style fusion. We present (left) the original StyleGAN2forward function, and (right) a corresponding implementation for the spatial style fusion. We alignthe related code blocks on the left and right.
Figure 34: More outpainting via model inversion. We present more outpainting results fromInfinityGAN on Flickr-Scenery. We invert the latent variables from 256×128 pixels real images(marked with red box), then outpaint 256×640 area (5× real image size).
Figure 35: More image inbetweening with model inversion. By inverting the latent variablesthat reconstruct the two real images on two sides (marked with red box), InfinityGAN can natu-rally inbetween the two images arbitrarily distant away. We synthesize the 256×1280 images usingInfinityGAN trained on Flickr-Scenery at 101×101 pixels.
Figure 36: More cyclic panorama synthesized with model inversion. By setting the same realimage on two sides (marked with red box) and inverting the latent variables that reconstruct thereal image, InfinityGAN can naturally synthesize horizontally cyclic panoramic images with imageinbetweening. We synthesize the 256×1280 images using InfinityGAN trained on Flickr-Scenery at101×101 pixels.
Figure 37: We plot the FID curve for a training episode of our complete InfinityGAN (red curve)and InfinityGAN without feature unfolding (blue curve). We observe the FID saturates at early stageif without feature unfolding.
Figure 38: Qualitative results of TileGAN on Flickr-Landscape dataset. The results show thatrandom texture synthesis models are not direCtly appliCable to real-world image synthesis.
Figure 39: Qualitative results of ConSinGAN on Flickr-Landscape dataset. We run ConSinGANunder two different configurations released by the authors, (top) generation and (bottom) retarget.
Figure 40: Ablation on the mode-seeking diversity loss. We show that the mode-seeking diversityloss discourages the model from synthesizing similar appearance at the same coordinate. Both Infin-ityGAN models are trained on Flickr-Landscape data with 197×197 full-image size and 101×101patch size, then synthesize at 101×101 pixels at testing. In this figure, all images share the samecoordinate grid, and each row shares the same global latent variable. Therefore, only the local latentvariables are varying in each row. In Figure (a), regular InfinityGAN shows high diversity in eachrow, and no obvious structure-coordinate relation is presented. In contrast, in Figure (b), a consistenthigh-level layout is shared among each row, while the differences between samples are mostly localvariations. In particular, the third, sixth, and seventh rows share a similar layout, which is a signthat the model learns a correspondence between the image structure and the coordinates. However,it is difficult to quantify such a problem since the repetition is not an exact repetition of content buta structural/semantical level similarity.
Figure 41: Ablation on the auxiliary loss. We show that InfinityGAN with auxiliary loss (or-ange curve) can provide slight improvement compared to the variant without the auxiliary loss (pinkcurve). However, such a performance difference is not very significant. We believe the additionalsupervision in the vertical position should provide important clues in helping the model learn thespatial-varying distribution in the vertical direction. Our approach in modeling such informationwith two MLP-layers (see Figure 18) may be too naive. Future studies on improving the model-ing performance with better loss functions or architecture may improve the overall performance ofInfinityGAN further.
