Figure 1: In multi-domain learning every sample has a domain label. Latent domain learning studieshow models may best be learned without this information.
Figure 2: Cosine similarity between SLA gates for Office-Home (left) anddomains are more dissimilar, but similarities exist, e.g. (A)rt and (P)hoto.
Figure 3: Layerwise correlation between SLA convolutions Vk on Office-Home. Most correlationsoccur in the mid-to-late stages of the model.
Figure 4: SLA sparsity (Office-Home); dotted lines show residual pooling transitions.
Figure 5: Left: PCA of samples represented by their SLA activation paths, colored by their ground-truth domain label as assigned in PACS. SLA shares parameters between visually similar domainsart and photo (•,•), while isolating sketch (•). The arrow highlights one sample that has been labeleda photo in PACS. SLA categorizes it as a cartoon instead, a more adequate assignment for this par-ticular image. Right: sample pairs from different domains (di 6= dj ) with matching SLA activations.
Figure 7: Variation of accuracies on Office-Home (left) and PACS (right).
Figure 8: Change in AP between ResNet18 and ResNet18-SLA for different gender skews in CelebAattributes.
