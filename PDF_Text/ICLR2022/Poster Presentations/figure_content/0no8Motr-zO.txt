Figure 1: (a) A diagram of the BARL data-collection loop. (b) An illustration of the EIGã€Œ*computation over several sample paths Ti (multi-colors) sampled from P(T* | D) for a dataset ofpast queries (grey points). The optimizer (in pink) is a point that is maximally informative whenlearning a model for crossing the path between the lava pools (orange rectangles) to the goal (green).
Figure 2:	Progress and sampled points of BARL, showing trajectories through the normalized statespace of the pendulum problem from four fixed start points for the optimal controller and MPC with20 and 40 datapoints, respectively. It is clear that even with very few points the controller is able toclosely track the optimal paths. Here color is used only to disambiguate the trajectories.
Figure 3:	Learning Curves of RL methods, showing control performance averaged across 5 seeds.
Figure 4:	For a single run of BARL and of EIGT using the same prior model, we evaluate controlperformance, as well as modeling error, on both the predictions used by the MPC procedure and on aset of points uniformly sampled from the state-action space.
Figure 5: Performance of BARL when MPC budget for posterior function samples is varied whileMPC test time budget is held constant. The error regions are the standard error of the return seenacross 5 trials of the policy. The dashed lines are the performances that MPC with the equivalenthyperparameters achieves if executed at test time given the ground truth dynamics.
