Figure 1: Effective depth measures the depth of theunrolled network. For example, a network with 3iterations of its single-layer recurrent module and4 additional layers has an effective depth of 7.
Figure 2: Average accuracy of each model. The x-axis represents effective depth and the y-axismeasures test accuracy. The marker shapes indicate architectures, and the blue and orange markersindicate feed-forward and recurrent models, respectively. The horizontal bars are Â± one standarderror from the averages over several trials. Across all three families of neural networks, recurrencecan imitate depth. Note, the y-axes are scaled per dataset to best show each of the trends.
Figure 3: A sample maze and its solution.
Figure 4: The distribution of optimalpath lengths in each dataset (small,medium, and large mazes).
Figure 5: Recurrent models perform similarly to feed-forward models of the same effective depth.
Figure 6: The number of active neurons after eachiteration of a recurrent ConvNet with 5 iterations.
Figure 7: ImageNet filter visualization. Filters fromrecurrent model iterations alongside corresponding feed-forward layers.
Figure 8: CIFAR-10 filter visualization.
Figure 9: Relative activations at each layer in the recurrent module of an untrained CNN. This plotconfirms that the distributions in the other similar plots from trained models are significant.
Figure 10: Relative activations at each layer in the recurrent module of a CNN trained on SVHN.
Figure 11: Relative activations at each layer in the recurrent module of a residual network trained onCIFAR-10.
Figure 12: ImageNet filter visualization. Filters from recurrent model iterations alongside corre-sponding feed-forward layers.
Figure 13: CIFAR-10 filter visualization. Filters from recurrent model iterations alongside corre-sponding feed-forward layers.
