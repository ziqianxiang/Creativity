Figure 1: A step-by-step illustration showing the SCL objective underlying MAML. Assume thelinear layer w0 to be zero, we find that, during the inner loop, the ith column of w0 is added withthe support features of class i. In other words, the support features are memorized by the linear layerduring the inner loop. In the outer loop, the output of a query sample is the inner product of φ(q1)and w1 , which is essentially the inner product of the query features and all the support features. Theouter loop loss aims to minimize the MSE between the inner products and the one-hot label. Thus,MAML displays the characteristic of supervised contrastiveness. Besides, the support data acts aspositive samples when the labels of support and query data match, and vice versa.
Figure 2: The supervised contrastiveness entailed in MAML manifests when zero initializationor the zeroing trick is applied. The value in the heatmap is calculated by averaging the pair-wise cosine similarities between query features or between query features and support features. Weconsider the setting of having randomly initialized linear layer (top), zero-initialized linear layer(middle), and the zeroing trick (bottom), and experiment with various numbers of outer loop up-dates. The readers are encouraged to compare the results between different rows.
Figure 3: Using the zeroing trick at meta-testing stage improves performance. The left/rightsubplot shows the performance of models Without/with w0 zeroed at the beginning of meta-testingtime. The curves in red: w0 is randomly initialized. The curves in yellow: w0 is zeroed at initial-ization. The curves in green: the models trained with training trick applied in the training stage.
Figure 4: With the zeroing trick, a larger number of inner loop update steps is not necessary. Inoriginal MAML, a larger number of inner loop update steps is preferred as it generally yields bettertesting accuracy even with zeroing trick applied in the meta-testing stage (refer to the left figure).
Figure 5: Effect of initialization and the zeroing trick on testing performance. Both reducingthe norm of w0 and zeroing w0 each outer loop (i.e.,the zeroing trick) increase the testing accuracy.
Figure 6: Illustration of the distinction of FOMAML and SOMAML. Conceptually speaking,the objective function of FOMAML aims to change the features of the query data; in contrast,that of SOMAML seeks to change the query’s features and support data simultaneously. In thisfigure, the support data and query data features are plotted as solid and hollow circles, respectively.
Figure 7: Updating the encoder using the reformulated outer loop loss. We experimentallyvalidate that the testing accuracy of models trained using MAML (with no inner loop update ofencoder) consists with that of models using their corresponding supervised contrastive losses, i.e.,Eq. (7), Eq. (17) and Eq. (18).
Figure 8: The effect of the interference term and the noisy contrastive term. We perform anablation study of the reformulated loss in Eq. (7) by dropping the interference term (denoted as“ni")or dropping the noisy part in the noisy contrastive term (marked as "2").
Figure 9: The ratio of the contrastive term being positive or negative during training. With thezeroing trick, the MAML becomes a SCL algorithm, so its ratio of negative contrastive term is 0.2.
Figure 10: Both FOMAML and SOMAML benefit from the zeroing trick. We examine if re-ducing or removing the interference can increase the testing performance in models trained withFOMAML and SOMAML. The results suggest that SOMAML also suffers from the interferenceterm. Note that the second subplots from the right shows lower testing performance of modelstrained with the zeroing trick as compared to the zero-initialized model. This may result from theoverfitting problem. The curves in red: models trained with original MAML. The curve in orange:w0 is zero-initialized. The curve in green: models trained with the zeroing trick.
Figure 11: The supervised contrastiveness is verified even using dataset composing of semanti-cally similar classes of images. Considering a dataset composing of different species of dogs, weagain observe the tendency that the supervised contrastiveness is manifested when we zero-initializethe linear weight and apply the zeroing trick.
Figure 12: The zeroing trick works when it comes to larger number of shots. To examine if ourwork generalized to the scenario when the number of shots increases, we perform a 5-way 25-shotclassification task. The result agrees with our results of 5-way 1-shot and 5-way 5-shot.
Figure 13: The performance of the models trained on non-mutUally exclusive tasks. The mod-els are trained under a non-mutually exclusive tasks setting where there is a one-to-one assignmentbetween class and channel. Under this circumstance, the zeroing trick tackles the channel memo-rization problem and yields a performance similar to conventional few-shot settings.
Figure 14: Zeroing the final linear layer before testing time improves the testing accuracy onOmniglot. The two subplots on the left: original testing setting. The two subplots at the right: thefinal linear layer is zeroed before testing time. The curves in red: the models whose linear layer israndomly initialized. The curves in yellow: the models whose linear layer is zeroed at initialization.
Figure 15: Effect of initialization and the zeroing trick in testing accuracy on Omniglot. Thetest performance of the models with reducing the initial norm of the weights of final linear layeris similar to that with the final linear layer being zero-initialized. The distinction in performancebetween models trained using the zeroing trick and zero-initialized model is more prominent in 5-way 5-shot setting.
Figure 16: The effect of the zeroing trick on models trained using FOMAML and SOMAMLon Omniglot. The results suggest that both zero-initialization and zeroing trick mitigate the inter-ference empirically.
Figure 17: The performance of the models trained on non-mutually exclusive tasks on Om-niglot. The results are compatible to those on mini-ImageNet (cf. Figure 13 in the main manuscript),suggesting that the zeroing trick alleviates the channel memorization problem.
