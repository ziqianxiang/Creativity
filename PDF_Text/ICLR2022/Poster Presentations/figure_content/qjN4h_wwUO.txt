Figure 1: Schematic view of the GradMax algorithm. Growing new neurons requires initializingincoming (W'ιew) and outgoing W'+eW weights for the new neuron. GradMax sets incoming weightsto zero (dashed lines) in order to keep the output unchanged, and initializes outgoing weights us-ing SVD (equation 11). This maximizes the gradients on the incoming weights with the aim ofaccelerating training.
Figure 2: (a) We measure the norm of the gradients with respect to Wnew after growing a single neu-ron starting from checkpoints generated during the Random (Random) growth experiments. SinceRandom and GradMaxOpt are stochastic we repeat those experiments 10 times. (b) Gradient normof the flattened parameters (both layers) throughout training. (c) Similar to (a), we load checkpointsfrom each growth step (Growth 1-5) and grow a new neuron using GradMax (fg) and Random (fr).
Figure 3: Training curves averaged over 5 runs and provided with 80% confidence intervals. In bothsettings GradMax improves optimization over Random. (right) Run time of the growing algorithms.
Figure 4: We plot training loss over time, each experiment is averaged over 3 experiments. Numberof parameters of the network trained increases over time. Red lines indicate number of parametersover training.
Figure 5: We grow MobileNet-v1 networks during the ImageNet training to investigate the effect of(left) growing schedule, (center) ε, and (right) scale used in initialization on the test accuracy.
Figure 6: The alignment of the top-k left-singular vectors for WRN-28-1 during the CIFAR-10training. The alignment is calculated between the full gradient and minibatches of varying sizes. Wedo not apply random cropping or flips to the inputs. A total of 10 experiments are run and confidenceintervals of 95% are plotted.
Figure 7: Effect of learning rate on the optimization speed and quality. We repeat each experiment3 times with a different seed and report the average values with 80% confidence intervals.
Figure 8: Long-term effect of growing. Top: Growing once with one of 5 singular values starting atdifferent iteration (20th, 50th, 100th, 500th or 1000th). Blue curve represents no growing. Bottom:The correlation between the singular values and the loss function decrease after a certain growingiteration.
Figure 9: (a) In this plot we load checkpoints from each growth step generated during the Randomexperiments. We grow a single neuron and measure the norm of the gradients with respect to Wfew.
Figure 10: Training curves averaged over 5 different runs and provided with 80% confidence in----Random—GradMax—GradMaxOpt----BaseIine(SmaII)—BaseIine(Big)O 1000	2000	3000	4000	5000	6000Training Steps(b)	Large (K = 5, ft = 100 : 50 : 10)tervals. In both settings GradMax significantly improves optimization over Random. Split-basedmethods seems to cause some instability in large network settings causing frequent jumps in thetraining objective.
Figure 11: (a) In this plot we load checkpoints from each growth step generated during the Randomexperiments. We grow a single neuron and measure the norm of the gradients with respect to Wfew.
Figure 12: Training curves averaged over 5 different runs and provided with 80% confidence in-tervals. In both settings GradMax significantly improves optimization over Random. Split-basedmethods seems to cause some instability in large network settings causing frequent jumps in thetraining objective.
Figure 13: (a) We compare setting all-weights to zero (and having unit bias) to our approach ofsetting only the incoming weights to zero (e.g. Random and GradMax). (b) Effect of growingduring different parts of the training for GradMax. Labels correspond to the iteration when the firstneuron is grown. We grow 5 neurons in total with 200 step intervals. Training steps are adjusted sothat all runs have same amount of FLOPs. (c) Effect of initial student network size. As before weadjust total number of steps so that the total training FLOPs match for all experiments. Numbers inparentheses represent the size of the hidden layer at initialization. We grow neurons starting fromfirst training iteration. (d) Growing beyond teacher network shows that with larger capacity, grownnetworks exceed the baseline performance.
