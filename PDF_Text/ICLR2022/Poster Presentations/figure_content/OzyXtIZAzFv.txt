Figure 1: Overview of our pipeline for evaluating representation learning in visually complexscenes: given a multi-task dataset of prior experience We pre-train representations using unsupervisedobjectives, such as prediction and contrastive learning, or task-induced approaches, which leveragetask-information from prior tasks to learn to focus on task-relevant aspects of the scene. We thenevaluate the efficiency of the pre-trained representations for learning unseen tasks.
Figure 2: Instantiations of our task-induced representation learning framework. Left to right:Representation learning via multi-task value prediction (TARP-V), via multi-task offline RL (TARP-CQL), via bisimulation (TARP-Bisim) and via multi-task imitation learning (right, TARP-BC).
Figure 3: Environments with high visual complexity and substantial task-irrelevant detail for testingthe learned representations. (a) Distracting DMControl (StOne et al., 2021) with randomly overlayedvideos, (b) ViZDoom (Wydmuch et al., 2018) with diverse textures and enemy appearances, (c) Dis-tracting MetaWorld (Yu et al., 2020) with randomly overlayed videos, and (d) CARLA (Dosovitskiyet al., 2017) with realistic outdoor driving scenes and weather simulation.
Figure 4: Performance of transferred representations on unseen target tasks. The task-inducedrepresentations (blue) lead to higher learning efficiency than the fully unsupervised representationsor direct policy transfer and achieve comparable performance to the Oracle baseline. All resultsaveraged across three seeds. See Figure 9 for per-task MetaWorld performances.
Figure 5: Visualization of the learned representations. Left to right: saliency maps for represen-tations learned with task-induced representation learning (TARP-BC) and the highest-performingcomparisons for reconstruction-based (VAE) and reconstruction-free (ATC) representation learn-ing. Left: Distracting DMControl environment. Right: CARLA environment. Only task-inducedrepresentations can ignore distracting information and focus on the important aspects of the scene.
Figure 6: (a) Task-induced representations (TARP-BC/V) allow for more accurate prediction of thetask-relevantjoint states. Unsupervised approaches aim to model all information in the input - thusprobing networks can still learn to predict state information, but struggle to achieve comparably lowerror rates. (b) Task-induced representations successfully filter task-irrelevant background informationand thus cannot confidently classify the background video, while unsupervised approaches fail to filterthe irrelevant information and thus achieve perfect classification scores. (c) Transfer performance forIL in distracting DMControl. Task-induced representations achieve superior sample efficiency.
Figure 7: Transfer performance vs.
Figure 10: Transfer performance for single source and multiple source task-induced representation.
Figure 9: Transfer performance for each downstream task in the distracting MetaWorld environment.
Figure 11: Performance of transferred representations with finetuning on unseen target tasks ondistracting DMControl, ViZDoom, and CARLA environments. The task-induced representations(blue) show better sample efficiency and performance compared to the representations learned withunsupervised objectives.
