Figure 1: Illustration of Theo-rem 1 (no common randomness).
Figure 2: Architectures. ToP left: Definition 2. Bottom left: Theorem 1. ToP right: Definition 3.
Figure 3: Binary case distortion-rate tradeoffs. (a) qx = qγ = 0.3, where DnCr(B(qX), B(qY), R)and Dncr(B(qx), B(qγ), R) coincide with DnCr(B(qx), B(qγ), R); (b) qx = 0.3, qγ = 0.5, whereDnCr(B (qx), B(qγ ),r) is tight but D 0仃(B(qx), B(qγ ),R) is loose; (c) qχ = 0.3, qγ = 0.6, whereboth bounds are loose. Moreover, it can be seen from all these examples that common randomnesscan indeed help improve the distortion-rate tradeoff.
Figure 4: Illustration of our experimental setup. (a) shows the end-to-end learning system withcommon randomness, where the encoder and decoder have access to the same randomness u. (b)presents the network setup for verifying the architecture principle given in Theorem 1.
Figure 5: (a)(b) The experimental results of 4 times image super-resolution. (c)(d) The experimentalresults of image denoising. The noise pattern is synthesized by additive Gaussian noise with standarddeviation set to 20. (a)(c) Rate-distortion trade-offs. Blue points are the MSE distortion loss for aparticular rate under the setting of using common randomness, while orange points illustrate thesame trade-off without using common randomness. For both tasks, at any rate, the performance ofusing common randomness is better than the case without common randomness. (b)(d) Examples foroutputs from several models with different rates. As the rate increases, the outputs become clearer.
Figure 6: Illustration of the entropy-constrained optimal transport plan for the binary case (assumingqx + qγ ≤ 1), where PX (X)= PY (y) = 1 - H- (R) With X =亭：-1(；) and y = VY-H-IR ∙It is interesting to note that the quantizer PX∣χ does not depend on PY while the dequantizer PY∣ γdoes not depend on PX. So they are decoupled in a certain sense. Moreover, PX∣χ andPγ∣γ coinciderespectively with optimal quantizer PX*∣x and dequantizer Pγ∣γ* in the conventional rate-distortionsense when qX , qY ≤ 1/2.
Figure 7: Example of Uniform sources with a = 2 and b = 5. The left plot shows the lower boundW22(pX,pY ) in (39) and the upper bound Dn+cr (pX, pY , R) which is the sum of of the right handside in (46), (47) and (50). For comparison We also show the value of Wi(PX ,pγ). The right plotshows the distortions associated with the quantization and dequantization steps.
Figure 8: GauSSian case distortion-rate tradeoffs. (a) μx = μγ = 0, σx = σγ = 1,where D：；) (N(μχ, σX),N(μγ, σY),R) and Dn∞^ (N(μχ, σX),N(μγ, σY),R) coincide withDn∞r) (N(μχ,σχ),N(μγ,σY),R); (b) μχ = 0, σχ = 1, μγ = 1, σγ = 2, whereDn∞r) (N(μχ,σχ), N(μγ, σY), R) is tight but D∞r) (N(μχ,σ1χ), N(μγ, σYγ), R) is loose. More-over, it can be seen from both examples that common randomness can indeed help improve thedistortion-rate tradeoff.
