Figure 1: Architectures for search engine interpretability. Like classifier explanations, First-ordersearch explanations yield heatmaps of important pixels for similarity (bottom row third column).
Figure 3: Explanations relative to a backgrounddistribution show why a result is better than an al-ternative. When asked why the best result (lowerleft) was better than the second best result (topright) our method correctly selects the player.
Figure 4: Visualization of how regions oftwo similar images “correspond” according tothe second-order search interpretability methodSAM. We can use this correspondence to trans-fer labels or attention between similar images.
Figure 5: Convergence of Shapley-Taylor es-timation schemes with respect to the MeanSquared Error (MSE) on randomly initializeddeep networks with 15 dimensional input. Ourstrategies (Kernel) converge with significantlyfewer function evaluations.
Figure 6: Our Second-order explanation evalu-ation strategy. A good method should projectquery objects (top left and middle) to corre-sponding objects in the retrieved image (bottomleft and middle). When censoring all but theseshared objects (right column) the search engineshould view these images as similar.
Figure 7: First-order interpretation evaluation strategy. A good method should highlight pixels in thequery image (top left and middle) that, when censored (top right), have the largest possible impacton the cosine distance.
Figure 8: Interpretations of a function that purposely ignores the left half of the image. KSAM andIGSAM properly assign zero weight to these features. GradCAM does not and hence violates thedummy axiom of fair credit assignment.
Figure 9: Additional first-order search interpretations on random image pairs from the Pascal VOCdataset19Published as a conference paper at ICLR 2022A.7 Additional Results for Stanford Online ProductsTable 2: Comparison of performance of first-order search interpretation methods across differentvisual search systems on the Stanford Online Product dataset. Methods introduced in this work arehighlighted in pink. *Though SAM generalizes ZhU et al. (2019) We refer to it as a baseline. Foradditional details see Section 6Metric	Model	Model Agnostic				Architecture Dependent					DN121	0.18	0.23	0.20	0.22	0.09	0.13	0.12	0.18	0.18~Faith.	MoCoV2	0.24	0.30	0.27	0.18	0.14	0.2	0.21	0.24	0.24	RN50	0.11	0.14	0.12	0.13	0.03	0.07	0.07	0.10	0.10	VGG11	0.15	0.16	0.14	0.15	0.04	0.08	0.09	0.12	0.12	DN121	-	0.00	0.24	0.0T	-	11.2	0.54	0.02	O.0F⅛4 J o U I	MoCoV2	-	0.00	0.17	0.00	-	0.34	0.57	0.02	0.00	RN50	-	0.00	0.21	0.00	-	13.6	0.39	0.02	0.00	VGG11	-	0.00	0.24	0.00	-	4.13	0.47	0.04	0.00A.8 Additional Results for Caltech-UCSD Birds 200 (CUB) DatasetTable 3: Comparison of performance of first-order search interpretation methods across different
Figure 10: Kernel convergence for random functions generated by randomly choosing coefficients.
Figure 11: Explanation of why two images are similar (Red) and dissimilar (Blue). Blue regionshighlight major differences between the images such as the dog playing the guitar, and the chain-linkfence in the retrieved image.
