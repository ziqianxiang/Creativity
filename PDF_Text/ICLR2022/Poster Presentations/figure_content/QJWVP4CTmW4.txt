Figure 1: The noise edges problem in GCN-based face clustering. Different shapes in figures rep-resent different classes. (a) Face images to be clustered. (b) Noise edges are introduced when con-structing graphs based on naive kNN. (c) Connecting edges by feature distance may lead to noiseedges. (d) The existing “One-size-fits-all” solution using a fixed number of neighbours for each ver-tex introduces many noise edges.
Figure 2: The framework of Ada-NETS. (I). The features are transformed to the structure space toobtain better similarity metrics. (II). The neighbours of each vertex are discovered by an adaptivefilter. (III). A graph is built with the neighbour relations discovered by (II) and the graph is used bythe GCN model to classify vertex pairs. The final clustering results are obtained using embeddingsfrom GCNs to link vertex pairs with high similarities.
Figure 3: Adaptive neighbour discovery process. koff is the extreme point of Q(j). In trainingphase, adaptive filter learns to fit koff. In testing phase, adaptive filter estimates koff and removes theoffcandidate neighbours with orders beyond the predicted k .
Figure 4: Random examples of top 20 images ranked by the similarity with probe images.
Figure 5: (a) The sensitivity of clustering to k. Ada-NETS maintains a stable and outstandingperformance. (b) The ROC curves on MS-Celeb-IM part」. All embedded features have betterROC performances than the original feature embedding, and the graph embedding output by GCNwith the help of AND and SS has the best performance. (c) Feature distribution visualization forthree types of features: original feature embedding (c.1), graph embedding of GCN with the adaptiveneighbour discovery strategy in the original space (c.2) and structure space (c.3).
Figure 6: The calculation of precision Prj and recall Rcj .
