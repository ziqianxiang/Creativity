Figure 1: (a) Training procedure of contrastive learning. Two augmented views are generated byapplying random transformations to the same input batch. A backbone f (∙) and a projector g(∙)is learned through contrastive prediction tasks. (b) Meta-Learning framework for sSl. We adoptthe same data augmentation operations as contrastive learning. We generate a b-way classificationproblem, b is the batch size, by treating each image itself as a class. Two views are separatedas support data, on which the network is fine-tuned and the classification strategy is learned by A;query data, on which we apply the updated model and calculate the meta-loss. At the end of training,everything but f (∙) is discarded, and h is used as the image representation.
Figure 2: (a) Examples of novel classes created by rotation by {90°, 180°, 270°}. (b) Adding taskaugmentation into the data augmentation pipeline. We first apply random transformations on theinput batch, then for each augmented view from the same base image, We rotate them by the samedegree.
Figure 3: Training procedure with Large Rotation augmentation as an auxiliary loss. We randomlyrotate each augmented view by a degree in {0°, 90°, 180°, 270°}. We share the feature extractorused in the SSL algorithm and apply it along with a 4-way linear head r(∙) to predict the rotationangle. We sum up theSSL loss, Lssl, with the angle prediction loss, Llr, as our ultimate objective,L.
