Figure 1: An illustration of how twodata points, x1 and x2, are transformedin mixup (top) and noisy feature mixup(NFM) with S := {0} (bottom).
Figure 2: The decision boundaries and test accuracy (in parenthesis) for different training schemes ona toy dataset in binary classification (see Subsection F.2 for details).
Figure 3: Pre-actived ResNet-18 evaluated on CIFAR-10 with different training schemes. Shadedregions indicate one standard deviation about the mean. Averaged across 5 random seeds.
Figure 4: Wide ResNets evaluated on CIFAR-100. Averaged across 5 random seeds.
Figure 5: Pre-actived ResNet-18 evaluated on CIFAR-10 (left) and Wide ResNet-18 evaluated on20,0.0000.0250.050 0.075 0.100 0.125 0.150Adverserial Noise (e)CIFAR-100 (right) with respect to adversarially perturbed inputs.
Figure 6: Pre-actived ResNet-18 evaluated on CIFAR-10c.
Figure 7: Comparison of the original NFM loss with the approximate loss function during trainingand testing for a two layer ReLU neural network trained on the toy dataset of Subsection F.2.
Figure 8: The toy dataset in R2 that we use for binary classification.
Figure 9: Vision transformers evaluated on CIFAR-10 with different training schemes.
Figure 10: Pre-actived ResNet-18 evaluated on CIFAR-10 trained with NFM and varying levels ofadditive (σadd) and multiplicative (σmult) noise injections. Shaded regions indicate one standarddeviation about the mean. Averaged across 5 random seeds.
Figure 11: Pre-actived ResNet-18 evaluated on CIFAR-10 (left) and Wide ResNet-18 evaluated onCIFAR-100 (right) with respect to adversarial perturbed inputs. Shaded regions indicate one standarddeviation about the mean. Averaged across 5 random seeds.
Figure 12: The features learned by the NFM classifier are slightly stronger (i.e., different fromrandom noise) than the clean model. See Subsection F.7 for more details.
Figure 13: Train (a) and test (b) error for a pre-actived Wide-ReSNet-18 trained on CIFAR-100.
