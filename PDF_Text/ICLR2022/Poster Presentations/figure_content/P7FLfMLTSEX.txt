Figure 1: Comparison of learning speeds across different frequencies. The target signal. i.e., sinu-soids, is transformed in the Fourier domain and the learned components are compared to the trueamplitudes. On the colormap scale, 1 denotes the perfect approximation. We observe that the Π-Net(right) learns higher frequencies faster, i.e., lower in the y axis, than the standard network (left).
Figure 2: The plots track the progress of the denoising task in the DIP setup via measuring the PSNRw.r.t. the noisy image (left) and the true image (right). We ideally want to stop the optimizationprocess when the PSNR w.r.t the true image is maximum. We observe that for Π-Nets, the maximumPSNR point occurs much earlier, beyond which the PSNR w.r.t the true image starts to decrease asthe network begins to learn the noise. This indicates that the Π-Net shows a reduced impedancetowards high frequency information, compared to standard networks.
Figure 3: We compare the power spectral density curves at different checkpoints during 600 itera-tions of optimization, for standard (scale = 3) vs Π networks (scale = 2). The goal for each networkis to match the power spectral density of the target image. We observe that the Π-Net picks uphigh-frequency information in the image faster.
Figure 4: We compare the validation loss curves for the first 1000 iterations on the binary classifica-tion task, for Π-Net (left) and standard network (right). For the same frequency, the validation dipsfor Π-Net are much smaller, indicating a higher tendency to pick up high-frequency label noise.
Figure 5: Schematic illustration of the NCP [16].
Figure 6: Schematic illustration of the product of polynomials model [16].
Figure 8: The plots represent a comparison of log-scale convergence curve of error projectionlengths for standard vs Π-kernel for different order harmonics with K “ t1, 2, 4u, indicating aclear improvement in the rate of convergence of error for higher harmonicsFor the first setting, we look at K “ t1, 2, 4u, with corresponding weight ratio as A1 : A2 : A4 “1 : 1 : 1. For each frequency, we look at the rate of convergence for the two kernels.
Figure 9: The plots represent a comparison of log-scale convergence curve of error projectionlengths for standard vs Π-kernel for different order harmonics with K “ t1, 3, 4, 5, 8, 12u. We againsee a clear improvement in the rate of convergence of error for higher harmonics, and especially sofor odd harmonics greater than 1.
Figure 10: The heat map denotes a comparison on the effectiveness of increasing depth vs introduc-ing multiplicative interactions via Π-Nets for learning high-frequency information. The empiricalevidence shows that multiplicative layers are more effective for learning higher frequencies faster.
Figure 11: The heat map denotes a comparison on the effectiveness of multiplicative layers via Π-Nets vs only using additive skip connections. The additive skip connections do not seem to affectthe spectral bias over standard neural networks in terms of improving speed of learning for highfrequencies.
Figure 12: The heap maps present a comparison on the robustness to random parameter perturba-tions for six-layer standard vs six-layer Π-Net. The y-axis denotes the norm of the random pertur-bation. For standard networks, the high-frequency information is lost quickly as the perturbationnorm increases, while Π-Nets are much more effective at retaining higher frequency information,even under large perturbations.
Figure 13: The heap maps present a comparison on the robustness to random parameter perturbationsfor two variants of Π-Nets, one with three multiplicative injection layers and the other, a higherdegree polynomial with five multiplicative layers. We observe a higher degree polynomial leadsto higher robustness, which is consistent with our intuition that multiplicative layers expand thesolution space for learning high-frequency information.
Figure 14: Illustration for the U-net with scale = 2, i.e the standard network with two upsam-pling/downsampling operations.
Figure 15: Adapting the 2-scale U-net for the product of two polynomials Π-network, the verticaldotted line highlights the separation between the polynomials.
Figure 16: Visual comparison of the denoised image pertaining to the denoising experiment in 4.2.
Figure 19: Validation loss curves corresponding to the classification experiment, presenting a com-parison between Π-Net with one multiplicative layer and standard feedforward network. The smallerdip for Π-Net implies a tendency to pick up high frequency label noise faster.
Figure 20: Validation loss curves corresponding to the classification experiment to observe the effectof increasing multiplicative injections. We compare the Π-Net with one multiplicative layer to Π-Net with two multiplicative layers. The validation dip reduces even further for the Π-Net withmore multiplicative layers (i.e., a higher degree polynomial) indicating that more multiplicativeinteractions improve the network’s ability to learn more complex decision boundaries (introducedby the high frequency noise).
