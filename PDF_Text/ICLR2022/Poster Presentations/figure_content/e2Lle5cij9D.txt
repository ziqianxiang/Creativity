Figure 1: Numerical illustration of Theorem 2.2 for ReLU generator/discriminator with 1D datax = [-1, 1]T and Rg(w) = ∣w∣22. For βd = 0.1, we observe that the constraint set of the convexprogram in equation 17 is a convex polyhedron shown in (b) and the optimal generator output is thevertex w1 = (-1 + βd) and w2 = 1 - βd. In contrast, for βd = 1, the constraint set in (d) is thelarger scaled polyhedra and includes the origin. Therefore, the optimal generator output becomesw1 = w2 = 0, which corresponds to the overlapping points in (c) and demonstrates mode collapse.
Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, alinear generator Wi is used to model images at a given resolution Xi , attempting to fool quadratic-activationdiscriminator Di, for which the optimal solution can be found in closed-form via equation 15. Once stage i istrained, the input to stage i + 1 is given as the output of the previous stage with learned weights Wl, which isthen used to model higher-resolution images Xi+1. The procedure continues until high-resolution images canbe generated from successive application of linear generators.
Figure 3: Representative generated faces from ProCoGAN and Progressive GDA with stagewise training oflinear generators and quadratic-activation discriminators on CelebA (Figure 2). ProCoGAN only employs theclosed-form expression equation 15, where βd controls the variation and smoothness in the generated images.
Figure 4: Non-convex architecture trained on the dataset in Figure 1 using the Adam optimizer with(mg, md, βd, μ) = (150,150,10-3,4e - 6). Unlike our stable convex approach, the non-convextraining is unstable and leads to undamped oscillations depending on the initialization. In particular,for Trial#1 ((a) and (b)), we obtain unstable training so that the generator is unable to capture thetrend in the real data. However, in Trial#2 ((c) and (d)), the non-convex architecture is able to matchthe real data as predicted our theory in Theorem 2.2.
Figure 5: Numerical illustration of Theorem 2.2 for ReLU generator/discriminator with 1D datax = [-1, 0, 1]T andRg(w) = kwk22.
Figure 6: Loss curves of the final 64 × 64 stage of training of the non-convex generator and non-concave discriminator as trained with the baseline Progressive GDA method as used in the mainpaper, for images shown in Figure 3. Discriminator fake loss corresponds to the total network outputover the fake images, while real loss corresponds to the negative of the total network output over thereal images, output penalty corresponds to the e"iftEx〜px [D(x)2] penalty, gradient penalty refersto the GP loss with λ = 10, discriminator loss is the sum over all of the discriminator losses, andgenerator loss corresponds to the negative of the discriminator fake loss.
Figure 7:	Representative generated faces at 4 × 4 resolution from ProCoGAN and Progressive GDA withstagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2).
Figure 8:	Representative generated faces at 8 × 8 resolution from ProCoGAN and Progressive GDA withstagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2).
Figure 9: Representative generated faces at 16 × 16 resolution from ProCoGAN and Progressive GDA withstagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2).
Figure 10: Representative generated faces at 32 × 32 resolution from ProCoGAN and Progressive GDA withstagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2). ProCoGANonly employs the closed-form expression equation 15, where βd controls the variation and smoothness in thegenerated images.
Figure 11: Effect of βd(i) on generated faces from ProCoGAN and effect of βg(i) on generated faces fromProgressive GDA with stagewise training of linear generators and quadratic-activation discriminators on CelebA(Figure 2). ProCoGAN only employs the closed-form expression equation 15, where βd controls the variationand smoothness in the generated images, which can clearly be seen in the extreme example here. We also seethat βg has a similar effect for Progressive GDA, where high values of βg make output images less noisy butalso less diverse.
