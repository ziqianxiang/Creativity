Figure 1: (Left) The attack success rate when injecting Gaussian noise into LDD and HDD. SeeAppendix L for more corruption experiments. (Right) The attack success rate of the adversarialexamples for LDD and HDD by PGD ('∞, e=16∕255) against different target models (VGG19,RN152, DN201, SE154).1 The source model is ResNet-50.
Figure 2: (Left) AAI and accuracy of the normal ResNet-50 on ImageNet by replacing ReLU withdifferent Softplusβ . (Right) The attack success rate of adversarial examples generated by the pre-trained ResNet-50 modified with different Softplusβ . The baseline on each target model is similarto the success rate at β = 45. See Appendix G for illustration on DenseNet-121.
Figure 3: (Left) We illustrate the impact of applying the same λ on all the residual modules of thenormal ResNet-50 (ImageNet). X1:4 = 0.6 reaches the maximum of AAI while the accuracy of themodified model is only around 60%. (Middle) We illustrate the attack success rate when applyingthe same λ on all the residual modules of the normal ResNet-50. The X1:4 = 0.6 has the best successrate, which also has the maximum AAI. (Right) The impact of applying different λi on each block.
Figure 4: The training loss of Eq. (9)with Bayesian optimization on pre-trainedResNet-50. The search results are β = 20,λ1 = 0.98, λ2 = 0.87, λ3 = 0.73, λ4 =0.19.
Figure 5: The attack success rate (one-step attack) of different methods against different targetmodels. The source model is RN50. The horizontal axis represents different attack strengths. (Left)The target model is DN121. (Middle) The target model is SE154. (Right) The target model is IncV3.
Figure 6: We show that the adversarial perturbation generated by our IAA is imperceptible to humanobservers but can fool the images search engines. The first three lines show the recognition resultsof the image Blue Jay in different image search engines. Baidu Reverse Images Search can’t give theclassification results when identifying our adversarial examples, and the similar pictures searchedare not matched with Blue Jay. The last three lines show the recognition results of Red-spotted Toadin the image search engines. Both Google Reverse Images Search and Baidu Reverse Images Searchgive wrong recognition results.
Figure 7: The histogram shows the attack success rate of the adversarial examples for LDD and HDDUsing PGD ('∞, e=16/255) against different target models (VGG19, RN152, DN201 and SE154).
Figure 8: Some samples which have high adversarial transferable counterparts. These samples canbe correctly classified by different target models.
Figure 9: Frequency histogram of cosine similarity between adversarial perturbations generated byResNet-34, ResNet-50 and ResNet-152. The perturbations generated by our IAA methods are closerto each other with different source model than perturbations generated by PGD and SGM.
Figure 10: Frequency histogram of Pearson Correlation Coefficient between adversarial perturba-tions generated by ResNet-34, ResNet-50 and ResNet-152. The perturbations generated by our IAAmethods are closer to each other with different source model than perturbations generated by PGDand SGM.
Figure 11: (Left) The attack success rate of adversarial examples generated by the pre-trained(DenseNet-121) modified With different SoftPluse . (Right) The influence of applying λ on dif-ferent blocks. The source model is DenseNet-121 and the target model is VGG19. The green lineWith triangle mark shoWs attack success rate When applying λ to Block4 (the last block), and λ = 0.4has the best effect. The red line With star mark shoWs attack success rate When applying λ to Block3,and λ = 0.8 has the best effect. The purple line With cross mark shoWs attack success rate Whenapplying λ to Block2, and λ = 0.8 has the best effect. The blue line With circular mark shoWs attacksuccess rate When applying λ to Block1, and λ = 0.8 has the best effect. The horizontal line shoWsthe attack success rate When We respectively choose the best λ for different blocks.
Figure 12: We visualize the targeted adversarial examples by attacking the classifier deployed af-ter the different blocks of normally trained ResNet-50 and by attacking the robust ResNet-50. Theperturbation radius is 64/255. The target label is tench. Attack Block1 means generating adver-sarial example by attacking the classifier deployed after the first block. The Block4 is the last blockof ResNet-50 and Attack Block4 means generating adversarial example by attacking the normalResNet-50.
Figure 13: We visualize the images generated by the classifiers fine-tuned after different blocksof a normally trained model. The target label is tench. The early blocks of the model has bettergeneration capability than the last block, which also reflects that the early blocks has learned somedistribution-relevant information.
Figure 14: The influence of the hyper-parameter λ for the low-level feature loss. The target modelsare VGG19 and SE154. The source model is RN50. The horizontal lines show the success ratewhen using normal PGD attack (λ=0) to generate adversarial examples, and the target models arerespectively VGG19 (the purple line) and SE154(the blue line). Optimizing the classification lossand the low-level feature loss jointly during attacking can improve the adversarial transferability.
Figure 15: The CKA similarity between the features of natural image and adversarial images gen-erated by attacking different models. The horizontal axis represents the features of adversarial ex-amples extracted by different layers of ResNet-50, and the vertical axis represents the features ofnatural image extracted by different layers of the same ResNet-50. (a) Adversarial examples aregenerated by attacking normal ResNet-50 with PGD. (b) Adversarial examples are generated byattacking normal ResNet-50 with our IAA.
