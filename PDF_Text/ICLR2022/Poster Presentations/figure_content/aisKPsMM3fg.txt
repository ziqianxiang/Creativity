Figure 1: Overall illustration of ν-SDDP. For training, the algorithm iterates N times to solve differentproblem instances. For each instance, it repeats two passes: forward (solving LPs to estimatean optimal action sequence) and backward (adding new affine components to the value functionestimate). Once a problem instance is solved, the optimal value function and optimal actions are usedfor neural network training. During inference time for a new problem, it can predict high-qualityvalue function with little cost, which can be embedded into SDDP for further improvements.
Figure 2: Time-solution trade-off. In the left two plots, each dot represents a problem instance withSml-Sht-joint (μd & σd)	Mid-Lng-joint (μd & σd & μc)Figure 4: Performance of ν-Figure 3: ν -SDDP-fast with different # generated cutting planes. SDDP with low-rank projection.
Figure 4: Performance of ν-Figure 3: ν -SDDP-fast with different # generated cutting planes. SDDP with low-rank projection.
Figure 3: ν -SDDP-fast with different # generated cutting planes. SDDP with low-rank projection.
Figure 5: HyPemet style parameterization of neural V-function.
Figure 6: Illustration of the overall system design.
Figure 7: Time-solution trade-off.
Figure 8: Time-solution trade-off when ν-SDDP-accurate improves the solution from ν-SDDP-fastfurther.
Figure 9: Ablation: number of generated cutting planes.
Figure 10: Low-dim projection results when the underlying problem does not have a low-rankstructure.
Figure 11: Average mean return (values are normalized with optimal mean value as 1.736 ).
Figure 12: Evidence lower bound (ELBO) loss curve.
Figure 13: Probablistic Forecast of 5 Stocks with Different AR Orders.
Figure 14: Probablistic Forecast of 8 Stock Clusters.
