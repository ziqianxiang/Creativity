Figure 1: In a complete bipartite graph of “query nodes” {q0, ..., q9} and “key nodes” {k0, ..., k9}:standard GAT (Figure 1a) computes static attention - the ranking of attention coefficients is globalfor all nodes in the graph, and is unconditioned on the query node. For example, all queries (q0 toq9) attend mostly to the 8th key (k8). In contrast, GATv2 (Figure 1b) can actually compute dynamicattention, where every query has a different ranking of attention coefficients of the keys.
Figure 2: The Dictionary-LOOKUP problem of size k=4: ev-ery node in the bottom row has analphabetic attribute ({A, B, C, ...})and a numeric value ({1, 2, 3, ...});every node in the upper row hasonly an attribute; the goal is to pre-dict the value for each node in theupper row, using its attribute.
Figure 3: The DictionaryLookup problem: GATv2 easilyachieves 100% train and test accuracies even for k=100 andusing only a single head.
Figure 4: Test accuracy compared to the noise ratio: GATv2 is more robust to structural noisecompared to GAT. Each point is an average of 10 runs, error bars show standard deviation.
Figure 5: Train and test accuracy across graph sizes in the DictionaryLookup problem. GATv2easily achieves 100% train and test accuracy even for k=100 and using only a single head. GIN(Xu et al., 2019), although considered as more expressive than other GNNs, cannot perfectly fit thetraining data (with a model size of d = 128) starting from k=20.
Figure 6: An example for node representations that are linearly dependent, for which DPGAT cannotcompute dynamic attention, because no query vector q ∈ R2 can “select” h1.
Figure 7: Test accuracy compared to the noise ratio: GATv2 and DPGAT are more robust to structuralnoise compared to GAT. Each point is an average of 10 runs, error bars show standard deviation.
Figure 8: Test accuracy and statistical significance compared to the noise ratio: GATv2 is more robustto structural noise compared to GAT. Each point is an average of 10 runs, error bars show standarddeviation.
