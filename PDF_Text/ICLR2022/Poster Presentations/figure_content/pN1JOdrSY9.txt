Figure 1: Illustrations of semantically contrastive and non-language-agnostic (1a) vs. language-agnostic (1b) representations of English (red) and Romanian (cyan) sentences. For semanticallycontrastive models, sentences of the same topic will be clustered in the same region. If the modelsare also language-agnostic, the semantic regions will not differentiate sentences by language.
Figure 2: Comparison between vanilla SwAV (2a) and LAgSwAV (2b) losses. The “Proto” blockrepresents the prototype layer C . Unlike SwAV, our LAgSwAV loss differentiates which languagea sentence belongs to. As in Figure 2b, the cluster assignments of English sentences (i.e., q1s , q2s)are skewed towards left clusters (blue histogram), and those of Romanian (i.e., q1t , q2t) are skewedtowards right clusters (red dotted histogram), causing an imbalance. The Ω block rebalances theminto more evenly distributed assignments v1s, v2s, v1t and v2t .
Figure 3: T-SNE visualizations of sentence embeddings produced by XLM, XLM+SwAV andXLM+LAgSwAV for En-Fr and En-De. Visualizations for En-Ro is given in the Appendix.
Figure 4: Performances of our method on En-Ro Table 6: Translationese analysis on En-De unsu-w.r.t filtering ρ%.	pervised MT modelsTranslationese Effect. Edunov et al. (2020) found that back-translation only benefits translationese-source sentences but does not improve for natural-source texts. To investigate whether our method isaffected by the translationese effect, we use the test sets provided by Edunov et al. (2020) and repli-cate the translationese analysis in three setups: (i) Natural-source → Translationese-target (X→Y*),(ii) Translationese-source → Natural-target (X*→Y) and (iii) Translationese of translationese ofsource → Translationese-target (X**→Y*). The results in Table 6 show that our method improvesthe performance in De-En and En-De UMT tasks for both natural-source and translationese-sourcesetups. This demonstrates that our method is not affected by the translationese effect.
Figure 5: Complete diagram for our LAgSwAV. The diagram shows the paths each English (X) andRomanian (Y ) sentences take to compute the final LAgSwAV loss.
Figure 6: T-SNE visualizations of XLM, XLM+SwAV and XLM+LAgSwAV for En-Fr, En-De andEn-Ro. For each chart, we sample 50K sentences from the respective dataset and compute T-SNEon the embeddings produced by the models.
Figure 7: Performances of our method on Ro-Enw.r.t filtering ρ%.
