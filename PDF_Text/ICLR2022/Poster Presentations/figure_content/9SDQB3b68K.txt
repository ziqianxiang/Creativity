Figure 1: Solid and dashedlines denote offline Medium-Replay and Medium-Expertdata in D4RL (Walker2d) resp.
Figure 3: Internal dynamics shift: (left) sourceand target MDPs (range of the right-back-leg ofthe ant (state[11]) is limited: [-0.52, 0.52] insource MDP → [-0.26, 0.26] in target MDP);(right) the solid (orange) line denotes the stateof the right-back-leg over one trajectory col-lected in source, dashed (blue) line denotes thelearned reward modification -∆r over the tra-jectory, and green and red slices denote transi-tion pairs where -∆r ≥ and -∆r < 0, resp.
Figure 2: External dynamics shift: (left) sourceand target MDPs (target contains an obstacle rep-resented with the dashed line); (middle) top plots(w/o Aug.) depict the trajectories that are gen-erated by the learned policy with vanilla MPO;(middle) bottom plots (DARA) depict the tra-jectories that are generated by the learned pol-icy with DARA-based MPO; (right) learned Q-values on the state-action pairs in left subfigure.
Figure 4: Final performance on the D4RL (Walker2d) task: The orange bars denote the final per-formance with different amount (50%D, 20%D, 10%D, 5%D) of target offline data; The blue barsdenote the final performance of mixing 100% of source offline data D0 and different amount of targetdata x%D (x ∈ [50, 20, 10, 5]), i.e., training with {100%D0 ∪ x%D}; The red lines denote the finalperformance of training with 100% of target offline data D. We can observe that 1) the performancedeteriorates dramatically as the amount of (target) offline data decreases (100%D (red line) →50%D (orange bar) → 20%D (orange bar) → 10%D (orange bar) → 5%D (orange bar)), 2) aftertraining with the additional 100% of source offline data, {100%D0 ∪ x%D}, the final performanceis improved in some tasks, but most of the improvement is a pittance compared to the original per-formance degradation (compared to that training with the 100% of target offline data, i.e., the redlines), and 3) what is worse is that adding source offline data D0 even leads performance degradationin some tasks, e.g., CQL with 50%D and 20%D in Medium-Random.
Figure 5: We can observe that our reward augmentation 1) encourages (-∆r > 0, i.e., the greenslice parts) these transitions (-0.26 ≤ next-state[11] ≤ 0.26) that have the same dynamics with thetarget environment, and 2) discourages (-∆r < 0, i.e., the red slice parts) these transitions that havedifferent (unreachable) dynamics (next-state[11] ≤ -0.26 or next-state[11] ≥ 0.26) in the target.
Figure 6: Illustration of the real environment (for testing): (left) the flat and static environment,(right) the obstructive and dynamic environment.
Figure 7: Deployment on the flat and static environment of BCQ.
Figure 8: Deployment on the flat and static environment of CQL.
Figure 9: Deployment on the obstructive and dynamic environment of BCQ.
Figure 11: We exchange the source environment and the target environment in Figure 2 (in the maintext) so that the source environment has an obstacle and the target environment has no obstacles. Inthe source domain, we collect 100k of random transitions. In the target domain, we collect 0k, 1k,2k, 5k, and 10k random transitions respectively. We set η = 0.1. We can find that if we performDARA with only source offline data D0 (i.e., 0k target data), we indeed can not acquire the optimaltrajectory (eg. the short path without the obstacle). However, even there is no transition of passingthrough obstacles in the source data, performing DARA with {D0 ∪ D} enables us to acquire thebehavior of moving through obstacles. As we increase the number of target offline data D, trainingwith {D0 ∪ D} can gradually acquire optimal trajectories.
Figure 12: Illustration of the suite of tasks considered in this work: (from left to right) Hopper,Walker2d, Halfcheetah, simulated and real-world quadruped robots. These tasks require the RLagent to learn locomotion gaits for the illustrated characters.
Figure 13: Real-world terrains(for collecting the target offlinedata).
