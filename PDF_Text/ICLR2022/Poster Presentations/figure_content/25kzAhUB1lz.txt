Figure 1: Overview of UPSIDE. The black dot corresponds to the initial state. (A) A set of random policies isinitialized, each policy being composed of a directed part called skill (illustrated as a black arrow) and a dif-fusing part (red arrows) which induces a local coverage (colored circles). (B) The skills are then updated tomaximize the discriminability of the states reached by their corresponding diffusing part (Sect. 3.1). (C) Theleast discriminable policies are iteratively removed while the remaining policies are re-optimized. This is ex-ecuted until the discriminability of each policy satisfies a given constraint (Sect. 3.2). In this example twopolicies are consolidated. (D) One of these policies is used as basis to add new policies, which are then opti-mized following the same procedure. For the “red” and “purple” policy, UPSIDE is not able to find sub-policiesof sufficient quality and thus they are not expanded any further. (E) At the end of the process, UPSIDE hascreated a tree of policies covering the state space, with skills as edges and diffusing parts as nodes (Sect. 3.3).
Figure 2: Policies learned on the Bot-5 ExperimentsOur experiments investigate the following questions: i) Can UPSIDE incrementally cover an un-known environment while preserving the directedness of its skills? ii) Following the unsupervisedphase, how can UPSIDE be leveraged to solve sparse-reward goal-reaching downstream tasks?iii) What is the impact of the different components of UPSIDE on its performance?We report results on navigation problems in continuous 2D mazes6 and on continuous control prob-lems (Brockman et al., 2016; Todorov et al., 2012): Ant, Half-Cheetah and Walker2d. We evaluateperformance with the following tasks: 1) “coverage” which evaluates the extent to which the statespace has been covered during the unsupervised phase, and 2) “unknown goal-reaching” whose ob-jective is to find and reliably reach an unknown goal location through fine-tuning of the policy. Weperform our experiments based on the SaLinA framework (Denoyer et al., 2021).
Figure 3: Coverage on control environments: UPSIDE covers the state space signif-icantly more than DIAYN and RANDOM. The curve represents the number of bucketsreached by the policies extracted from the unsupervised phase of UPSIDE and DIAYNas a function of the number of environment interactions. DIAYN and UPSIDE havethe same amount of injected noise. Each axis is discretized into 50 buckets.
Figure 4: (a) & (b) Unsupervised phase on Ant: visualization of the policies learned by UPSIDE andDIAYN-20. We display only the final skill and the diffusing part of the UPSIDE policies. (c) Downstreamtasks on Ant: we plot the average success rate over 48 unknown goals (with sparse reward) that are sampleduniformly in the [-8, 8]2 square (using stochastic roll-outs) during the fine-tuning phase. UPSIDE achieveshigher success rate than DIAYN-20 and TD3.
Figure 5: Downstream task performance on Bottleneck Maze: UPSIDE achieves higher discounted cumulativereward on various unknown goals (See Fig. 15 in App. C for SMM and TD3 performance). From each of the16 discretized regions, we randomly sample 3 unknown goals. For every method and goal seed, we roll-outeach policy (learned in the unsupervised phase) during 10 episodes and select the one with largest cumulativereward to fine-tune (with sparse reward r(s) = I[ks - gk2 ≤ 1]). Formally, for a given goal g the reportedvalue is γτI[τ ≤ Hmax] with τ := inf {t ≥ 1 : kst - gk2 ≤ 1}, γ = 0.99 and horizon Hmax = 200.
Figure 6: For an unknown goallocation, UPSIDE identifies apromising policy in its tree andfine-tunes it.
Figure 7: The agent must assign (possiblystochastically) N skills to M states: under theassign them to M states indexed by m. We as-sume that the execution of each skill deterministi-cally brings it to the assigned state, yet the agent prior of uniform skill distribution, can the MI withmay assign stochastically (i.e., more than one state be increased by varying the number of skills N?per skill). (A non-RL way to interpret this is that wewant to allocate N balls into M boxes.) Denote by pn,m ∈ [0, 1] the probability that skill n isassigned to state m. It must hold that ∀n ∈ [N], Pm pn,m = 1. Denote by I the MI between theskill variable and the assigned state variable, and by I the MI under the prior that the skill samplingdistribution ρ is uniform, i.e., ρ(n) = 1/N. It holds thatn+ X NPn,m log p⅛n,m	ʌn Npn,mn,mpn,mlogpn,mn pn,mLet I ? (N, M) := max{pn,m} I (N, M) and {p[m } ∈ argmax{pn,m} I (N, M). We also define the
Figure 8: Decoupled structure of anUPSIDE policy: a directed skill fol-lowed by a diffusing part.
Figure 9: In the above UPSIDE tree example, executing policy z = 7means sequentially composing the skills of policies z ∈ {2, 5, 7} andthen deploying the diffusing part of policy z = 7.
Figure 10: High-level approach of UPSIDE.
Figure 11: Fine-grained evolution of the tree structure on a wall-free maze with N start = 4 and Nmax = 8.
Figure 12: Incremental expansion of the tree learned by UPSIDE towards unexplored regions of the state spacein the Bottleneck Maze.
Figure 13:	Environment divided in colors according to the most likely latent variable Z, according to (from leftto right) the discriminator learned by UPSIDE, the discriminator learned by DIAYN and the VQ-VAE learnedby EDL. Contrary to DIAYN, UPSIDE’s optimization enables the discriminator training and the policy trainingto catch up to each other, thus nicely clustering the discriminator predictions across the state space. EDL’sVQ-VAE also manages to output good predictions (recall that we consider the EDL version with the strongassumption of the available state distribution oracle, see Campos et al., 2020), yet the skill learning is unable tocover the entire state space due to exploration issues and sparse rewards.
Figure 14:	Complement to Fig. 2: Visualization of the policies learned on the Bottleneck Maze for the remain-ing methods.
Figure 15:	Complement of Fig. 5: Heatmaps of downstream task performance after fine-tuning for the remain-ing methods.
Figure 16: Visualization of the policies learned on U-Maze. This is the equivalent of Fig. 2 for U-Maze.
Figure 17: Heat maps of downstream task performance on U-Maze. This is the equivalent of Fig. 5 for U-Maze.
Figure 18: Average discriminability of the DIAYN-NZ policies. The smaller NZ is, the easier it is to obtaina close-to-perfect discriminability. However, even for quite large NZ (50 for mazes and 20 in control envi-ronments), DIAYN is able to achieve a good discriminator accuracy, most often because policies learn how to“stop” in some state.
Figure 19: Ablation on the length of UPSIDEpolicies (T, H): Visualization of the policieslearned on the Bottleneck Maze (top) and the U-Maze (bottom) for different values of T, H . (Righttable) Coverage values (according to the sameprocedure as in Table 2). Recall that T and H de-note respectively the lengths of the directed skilland of the diffusing part of an UPSIDE policy.
