Figure 1: Isotropic features generated as ψ 〜N(0, I) have qualitatively different learning curvesthan power-law features observed in real data. Black dashed lines are theory. (a) Online learningwith N-dimensional isotropic features gives a test loss which scales like Lt 〜e-t/N for any targetfunction, indicating that learning requires t 〜 N steps of SGD, using the optimal learning ratesη* = N +m.ι. (b) Power-law features ψ 〜N(0, Λ) with Akl = δk,ιk-2 have non-extensive give apower-law scaling Lt 〜t-β with exponent β = ON(1). (c) Learning to discrimninate MNIST 8'sand 9’s with N = 4000 dimensional random ReLU features (Rahimi & Recht, 2008), generates apower law scaling at large t, which is both quantitatively and qualitatively different than the scalingpredicted by isotropic features e-t/N. (d)-(f) The loss at a fixed compute budget C = tm = 100 for(d) isotropic features, (e) power law features and (f) MNIST ReLU random features with simulations(dots average and standard deviation for 30 runs). Intermediate batch sizes are preferable on realdata.
Figure 2: Optimal batch size depends on feature structure and noise level. (a) For power law featuresλk〜 k-b, λkv2 〜k-a, the m dependence of the loss LC/m depends strongly on the featureexponent b. Each color is a different b value, evenly spaced in [0.6, 2.5] with a = 2.5, C = 500.
Figure 3: Structure in the data distribution, nonlinearity, batchsize and learning rate all influencelearning curves. (a) ReLU random feature embedding in N = 4000 dimensions of MNIST andCIFAR images have very different eigenvalue scalings than spherically isotropic vectors in 784dimensions. (b) The task power spectrum decays much faster for MNIST than for random isotropicvectors. (c) Learning curves reveal the data-structure dependence of test error dynamics. Dashedlines are theory curves derived from equation. (d) Increasing the learning rate increases the initialspeed of learning but induces large fluctuations in the loss and can be worse at large t. Experimentcurves averaged over 20 random trajectories of SGD. (e) Increasing the batch size alters both theaverage test loss Lt and the variance. (f) Noise in the target values during training produces anasymptotic error L∞ which persists even as t → ∞.
Figure 4: Training and test errors of a model trained on a training set of size M can be computedwith the Ct matrix. Dashed black lines are theory. (a) The training error for MNIST random featuremodel approaches zero asymptotically. (b) The test error saturates to a quantity dependent on M .
Figure 5: ReLU neural networks of depth D and width 500 are trained with SGD on full MNIST. (a)-(b) Feature and spectra are estimated by diagonalizing the infinite width NTK matrix on the trainingdata. We fit a simple power law to each of the curves λk 〜k-b and Vk 〜k-a. (c) Experimental testloss during SGD (color) compared to theoretical power-law scalings t-a-~ (dashed black). Deepernetworks train faster due to their slower decay in their feature eigenspectra λk, though they havesimilar task spectra. (d)-(f) The spectra and test loss for convolutional and fully connected networkson CIFAR-10. The CNN obtains a better convergence exponent due to its faster decaying taskspectra. The predicted test loss scalings (dashed black) match experiments (color).
