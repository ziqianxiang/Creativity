Figure 1: Two different kinds of generalization, using Proc-gen and Meta-World as examples. Procedural generaliza-tion involves evaluating on unseen environment configura-tions, whereas task generalization evaluates adaptability tounseen tasks (reward functions).
Figure 2: The impact of planning and self-supervision on procedural generalization in Procgen(hard difficulty, 500 train levels). We plot the zero-shot evaluation performance on unseen levelsfor each agent throughout training. The Q-Learning agent (QL) is a replica of the MuZero (MZ)with its model-based components removed. MZ+Contr is a MuZero agent augmented with a tem-poral contrastive self-supervised loss that is action-conditioned (we study other losses in Figure 3).
Figure 3: Evaluation performance on Procgen (hard, 500 train levels). On the left, we ablate theeffectiveness of planning. The Q-Learning agent (QL) is a replica of MuZero (MZ) without model-based components. We then add a model to this agent (QL+Model) (see Section A.2) to disentanglethe effects of the model-based representation learning from planning in the full MuZero model (MZ).
Figure 4: Qualitative comparison of the information encoded in the embeddings learned by MuZerowith and without the auxiliary pixel reconstruction loss. For MuZero, embeddings are visualizedby learning a standalone pixel decoder trained with MSE. Visualized are environment frames (toprow) and decoded frames (bottom row) for two games (Chaser and Climber), for embeddings at thecurrent time step (k = 0) and 5 steps into the future (k = 5). Colored circles highlight importantentities that are or are not well captured (blue=captured, yellow=so-so, red=missing).
Figure 5: Interaction of self-supervision and data diversity on procedural generalization. Each plotshows generalization performance as a function of environment frames for different numbers oftraining levels. With only 10 levels, self-supervision does not bring much benefit over vanillaMuZero. Once the training set includes at least 100 levels, there is large improvement with self-supervised learning both in terms of data efficiency and final performance. For all plots, dark linesindicate median performance across seeds and shading indicates min/max seeds. See also Figure A.6for corresponding training curves.
Figure 6: Finetuning performance on ML-10 and ML-45, shown as cumulative regret over the suc-cess rate (lower is better). Both the pre-trained Q-Learning and MuZero agents have lower regretthan corresponding agents trained from scratch. MuZero also achieves lower regret than Q-Learning,indicating a positive benefit of planning (though the difference is small on ML-45). Self-supervision(pixel reconstruction) does not provide any additional benefits. Solid lines indicate median perfor-mance across seeds, and shading indicates min/max seeds. See also Figure A.8.
