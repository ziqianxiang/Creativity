Figure 1: A schema of our network architecture2.1	Training ProcedureThe training happens in two steps: The first step is to train the embedding space so that a good embeddingspace is formed at the output of the encoder. This is done by using the special MsDNN loss (Chauhan et al.,3Published as a conference paper at ICLR 20222021) on the embedding space. MsDNN loss is a variation of triplet loss (Weinberger & Saul, 2009) that usesGaussian estimations of the overall positive and negative samples as a substitute of the positive and negativesamples themselves. It has been shown to be effective in preserving the unseen subclasses of the data andconsequently is a good pick for our purpose. Optionally, a decoder that gets the transformed embeddings asinputs and reconstructs the original samples can also be used in this stage of the training, similar to MsDNN-AE (Chauhan et al., 2021).
Figure 2: A schema of the training procedures for DEPNTraining stops when accuracy using nearest-neighbor classification on a validation set is maximized. Ourexperiments show that unseen subclass accuracy, when measured by nearest-neighbor classification accuracy,generally follows the coarse-grained classification. This lets us optimize our embedding space using only thecoarse-grained class data and still get within the general vicinity of the best settings for fine-grained subclassaccuracy in the embedding space without having any subclass labels.
Figure 3: Coarse and fine grained network classification accuracy for different values of Ïƒ on CIFAR-10.
Figure 4: Coarse and fine grained nearest neighbor classification validation accuracy during the first stage oftraining on CIFAR-10.
Figure 5: The 40 prototype samples for one of our ImageNet experiments.
