Figure 1: Many GNN architectures (e.g. GAT (VelickoVic et al., 2018), PNA (Corso et al., 2020))incorporate sophisticated message functions to improve accuracy (left). This is problematic as wemust materialize messages, leading to O(E) memory consumption and OPs to calculate messages;these dataflow patterns are also difficult to optimize for at the hardware leVel. This work demon-strates that we can use simple message functions, requiring only O(V ) memory consumption(right) and improve performance over existing GNNs.
Figure 2: Visual representation of our EGC-S layer. In this visualization we have 3 basis filters (i.e.
Figure 3: Study over the number of heads (H) and bases (B). Study run on ZINC dataset withEGC-S. Metric is MAE (mean Â± standard deviation): lower is better. We study keeping the totalparameter count constant, and fixing the hidden dimension. Each experiment was tuned individually.
