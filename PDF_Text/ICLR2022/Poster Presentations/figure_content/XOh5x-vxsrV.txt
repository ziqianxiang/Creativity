Figure 1: Schematic view of CTRL’s key steps for every trajectory batch. (i) Generating trajectory views (topleft). For each trajectory in a batch, CTRL samples a subsequence of its time steps, computes belief-state/actionembeddings uti with encoder φ, and concatenates them into a trajectory representation (view) u. (ii) Clusteringtrajectory views (bottom right). CTRL uses the online Sinkhorn-Knopp clustering procedure (Caron et al.,2020): for each trajectory view u, it produces two new views v and w, soft-clusters all trajectories’ vs and wsinto C clusters, and uses a measure of consistency between these two clusterings as a loss Lclust. In the diagram,variables ec denote cluster centroids. (iii) Encouraging cross-cluster behavioral similarity (bottom left). Aftercomputing trajectory view clusters, CTRL applies a variant of MYOW (Azabou et al., 2021) to them. Namely, itrepeatedly samples a trajectory view v0 , computes a new view w0 for it, and computes a loss Lpred that penalizesdifferences between w0 and views vc0 i of randomly chosen trajectories from v0 ’s neighboring clusters. Encoderφ and auxiliary predictors used by CTRL are then updated using LCTRL = Lclust + Lpred’s gradients (top right).
Figure 2:	Training results over the 8M frames benchmark.
Figure 3:	Evaluation results over the 8M frames benchmark.
Figure 4:	Evaluation results over the 25M frames benchmark.
Figure 5: Ablation on the clustering timesteps used in the dynamics embeddingFigure 6: Ablation on the number of clusters used in CTRLLoss landscape of Lclust and Lpred Works relying on non-colinear signals, e.g. behavioral similarityand rewards, as is the case for DeepMDP (Gelada et al., 2019), show that interference can occurbetween various loss components. For example, (Gelada et al., 2019) showed how their dynamics and17Published as a conference paper at ICLR 2022Figure 7: Average within-trajectory cluster similarity over 1M consecutive timesteps.
Figure 6: Ablation on the number of clusters used in CTRLLoss landscape of Lclust and Lpred Works relying on non-colinear signals, e.g. behavioral similarityand rewards, as is the case for DeepMDP (Gelada et al., 2019), show that interference can occurbetween various loss components. For example, (Gelada et al., 2019) showed how their dynamics and17Published as a conference paper at ICLR 2022Figure 7: Average within-trajectory cluster similarity over 1M consecutive timesteps.
Figure 7: Average within-trajectory cluster similarity over 1M consecutive timesteps.
Figure 8: Average values of Lclust and Lpred over time.
Figure 9: Sample states from behavioral clusters found by CTRL after 2M of training frames for 5 representativeenvironments. The two gray squares in top left is added to indicate the agent’s velocity.
Figure 10: t-SNE of 1024 randomly sampled states from data collected by CTRL after 0.5,1,1.5 and 2M framesin Starpilot, with β = 0.1, T = 4. As learning progresses, agent behavior clusters become more and moredistinct.
Figure 11: Goodness-of-clustering measured by silhouette scores (top) and average test returns (bottom) as afunction of training samples.
Figure 12: The composite Ising matching problem: the agent has to match a given Ising configuration byswapping branches of various transition dynamicsTable 5 outlines the results we obtained by deploying CTRL with different number-of-clustersparameter value. One can see that the largest improvement in silhouette score occurs from E = 4to E = 5 (14.5%), suggesting that monitoring the largest change in silhouette score can be usedto set the true number of clusters in CTRL which, in turn, corresponds to the highest-return policydiscovered by CTRL.
