Figure 1: Failure of using confounded expert data under context distribution mismatch between online envi-ronment and expert data. Caregiver does not learn to perform well in a dressing task when covariate shift ofhidden confounders is present but not accounted for.
Figure 2: Causal Diagram Con-textual MDPEπ denotes the expectation induced by the policy π. We denote by Π the set of all Markovianpolicies and Πdet the set of deterministic Markovian policies. We define the optimal value and policyby VM = max∏∈∏ vm(∏), and ∏M ∈ argmax∏∈∏ vm(∏), respectively. Whenever appropriate,We simplify notation and write v*,∏*. We use ∏M to denote the set of optimal policies in M, i.e.,∏M = arg max∏∈∏ vm(∏). We also define the set of catastrophic policies ∏M as the set∏M = arg min vm(∏).
Figure 3: A spectrum for the difficulty of confounded imitation with covariate shift. Context-Independenttransitions may result in non-identifiable and catastrophic candidates, whereas context-independent rewardsreduce the problem to a standard imitation learning.
Figure 4: Plots compare training curves of using CTS vs. normal sampling of expert data for small (β = 0.3)and large (β = 0.8) covariate shift bias in four assistive-healthcare tasks. Dashed black lines show expert andRL (without data) scores. Runs were averaged over 5 seeds. Legend is shared across all plots.
Figure 5: Left plot shows comparison of different choices of f -divergences for pure imitation (without rewardand without covariate shift) on the BedBathingPR2 environment. Middle plot depicts execution of imitationwith hidden confounding (without reward) for different levels of covariate shift. Right plot compares our CTScorrection on the RecSim environment with strong covariate shift bias. All runs were averaged over 5 seeds.
Figure 6: A contextual MDP with state space S = {A, B, C}, action space A = {aB , aC} andcontext space X = {xι, x2}. We assume V(A|x) = 1 for all X ∈ X. The actions a，B,a° transitionthe agent to states B, C, respectively, after which the agent receives a reward r ∈ {0, 1} dependingon the context. We assume B, C are sink states.
Figure 7: Results for the rooms environment with covariate shift affecting only the distribution ofwalls. It is evident that whenever the reward is context-free comparable performance is obtained.
Figure 8: Contextual MDP Causal Diagram.
