Figure 1: Layer 7 neurons overlap, usingProbeless ranking. Blue squares are abovethe expected overlap between 2 rankings (Ap-pendix A.3), red are below. Major ticks are at-tributes, minor are languages.
Figure 2: Layer 2 neurons overlap between ev-ery pair of rankings, from 8 randomly selectedconfigs. Gray and black dashed lines show theexpected overlap between 2 and 3 random rank-ings, respectively (Appendix A.3).
Figure 3: Clustering of the three different patterns (3d), and an example of each of the patterns(3a-3c). Solid lines are top-to-bottom rankings; dashed are random rankings; dotted are bottom-to-top rankings. "X by Y" means classifier X using ranking Y. Some lines are omitted for clarity;complementing figures can be found in Appendix A.8.
Figure 4: Spanish gender layer 2, translationresults with β = 8. Solid lines are error rates,dashed are CLWVs. ttb and btt stand for top-to-bottom and bottom-to-top, respectively.
Figure 5: Layer 7 neurons overlap using Linear and Gaussian rankings.
Figure 6: XLM-R layer 7 neurons overlap, using Probeless. Blue squares are above expectedvalue, red are below.
Figure 7: Ranking evaluation by probing: The language model creates a word representation (e.g., ofthe word “was”), which is fed into a neuron-ranking method, to rank its neurons according to theirimportance for some attribute (e.g., tense). The k-highest ranked neurons are fed into a probe, whichis trained to predict the attribute.
Figure 8: Examples of each of the patterns, with all graph lines (complementing Fig 3). Solid linesare top-to-bottom rankings; dashed are random rankings; dotted are bottom-to-top rankings. "X byY" means classifier X using ranking Y.
Figure 9: Clustering of the three different patterns in XLM-R, and an example from one config. Solidlines are top-to-bottom rankings; dashed are random rankings; dotted are bottom-to-top rankings.
Figure 10: Two selectivity examples from M-BERT and XLM-R. Solid lines are top-to-bottomrankings; dashed are random rankings; dotted are bottom-to-top rankings. Some lines are omitted forclarity.
Figure 11: Ranking evaluation by interventions: The language model creates a word representation(e.g., of the word “was”), which is fed into a neuron-ranking method, to rank its neurons according totheir importance for some attribute (e.g., tense). The k-highest ranked neurons are modified by anintervention (to a different color in the figure), and the new representation is fed into the rest of thelanguage model’s layers, to observe the final model’s output.
Figure 12: Spanish gender layer 2, ablation results. Solid lines are error rates, dashed are CLWVs.
Figure 13: Translation results with β = 8, from two different configs. Solid lines are error rates,dashed are CLWVs. ttb and btt stand for top-to-bottom and bottom-to-top, respectively.
