Figure 1: (Left) In our active learning framework, we first pre-train our features with self-supervisedlearning, select samples to label by minimizing the discrete Wasserstein distance, and then trainour classifier. (Right) t-SNE plot of the feature space on the STL-10 data set with selected pointshighlighted. The baseline misses the top right region. See Appendix E.7 for full visual comparisons.
Figure 2: Low budget active learning onCIFAR-10, SVHN and STL-10 using SimCLRpre-trained features. See Table 3 for detailed re-sults for B ≤ 40. See Appendix E.3 for fullresults up to B ≤ 180. The solid lines are ourmodels and the dashed lines are baselines. Allplots show mean ± standard error over five runs.
Figure 3:	Active learning with domain adaptation for D→A, W→A, and D→W on Office-31 usingf -DAL pre-trained features. The solid line is our best model and the dashed lines are baselines. Allplots show mean ± standard error over three runs. See Appendix E.4 for full results.
Figure 4:	(Left) Upper and lowerbounds in the first 50 iterations atB = 10. (Right) Increasing thewall-clock runtime of the GBD al-gorithm. All plots show mean ±standard error over three runs.
Figure 5: Pseudocode of the GBD algorithm used to solve our optimization problem. The abovecode uses the Python-MIP (mip) and Python Optimal Transport (ot) libraries.)18Published as a conference paper at ICLR 2022D.3 Reducing the Computational ComplexityThe original problem (4) is a MILP with N2 + N variables and 2N + 1 constraints. This problemis too large to solve on standard computers when N ≥ 50, 000, which is typical for deep learningdata sets. Instead in Algorithm 1, we repeatedly solve two smaller sub-problems: (i) W-RMP(Λ) toobtain selections π and (ii) the Wasserstem distance W(C(π), D) to obtain dual variables λ.
Figure 6: Ablation of using Cosine distance as the base metric in the Wasserstein distance problemfor CIFAR-10.
Figure 7: t-SNE visualizations of the latent space obtained from pre-training. Images selected byeach strategy are marked. The first row shows STL-10 at B = 10, the second row shows CIFAR-10at B = 20, and the third row shows SVHN at B = 20.
Figure 8: Images selected for labeling on STL-10 with different methods: Wass. + EOC (top left),k-centers (top right), k-medoids (bottom left), Random (bottom right). The first two rows wereselected in the first two rounds and every two rows after were selected in the subsequent rounds.
Figure 9: Images selected for labeling on CIFAR-10 with different methods: Wass. + EOC (topleft), k-centers (top right), k-medoids (bottom left), Random (bottom right). The first two rows wereselected in the first two rounds and every two rows after were selected in the subsequent rounds.
Figure 10: Images selected for labeling on SVHN with different methods: Wass. + EOC + P (topleft), k-centers (top right), k-medoids (bottom left), Random (bottom right). The first two rows wereselected in the first two rounds and every two rows after were selected in the subsequent rounds.
