Figure 1: An example of how a tensor product of two circles, T2 = S1 × S1, is converted toa flattened latent representation that can be fed into the decoder. Each point on the torus can bedescribed by two angles. Each angle represents an ma tuple. The tensor product of the ma tuples isconcatenated with their respective α = 0 component to produce the input to the decoder.
Figure 2: Heatmap of the generative factors vs. the codes for the teapots dataset with our architecturewith six circles. The factors are ordered by azimuth and elevation angles, followed by RGB values.
Figure 3:	The latent representation of the T 8 -VAE. Each S1 circle corresponds to a different factor ofthe dataset that is varied as the angle changes. (a) The 5 most expressive circles on the Teapot dataset.
Figure 4:	The performance dependence on the latent torus and β parameter of the KL-divergenceterm with five generating factors.(a) The accumulated DC-score. (b) The reconstruction error. (c) TheFID score. We see a clear advantage for a latent space of at least the same dimension as the numberof generating factors.
Figure 5:	Heatmap of the generative factors vs. the codes for the teapot dataset with our architecturewith four circles (T4). The disentanglement, completeness and DC-scores are 0.36, 0.43 and 0.39,respectively. One can observe, for example that the second code θ1 controls all three RGB values.
Figure 6:	Heatmap of the generative factors vs. the codes for the teapot dataset with our architecturewith eight circles (T 8). The disentanglement, completeness and DC-scores are 0.69, 0.61 and 0.65,respectively.
