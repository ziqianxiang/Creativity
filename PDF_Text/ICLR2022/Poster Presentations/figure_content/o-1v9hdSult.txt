Figure 1: Explanatory dialogue starts when theuser presents a specific alternate plan. The systemexplains the preference over this alternate plan interms of model information expressed in terms ofpropositional concepts specified by the user, oper-ationalized as classifiers.
Figure 2: Algorithms for identifying (a) missing precondition and (b) cost functionIntuitively, CSabs({c1, .., ck}, a) = k can be understood as stating that executing the action a, in thepresence of concepts {c1, .., ck} costs at least k. We can use CSabs in an explanation by identifying asequence of concept set Cπf = hC1, ..., Cki, corresponding to each step of the foil πf = ha1, .., aki,such that (a) Ck is a subset of concepts in the corresponding state reached by the foil and (b) thetotal cost of abstract cost function defined over the concept subsets are larger than the plan costPi={i k} CSbs(Ci,ai) > C(I,π). For Foil2, the explanation would include the information -executing the action attack in the presence of the concept skull-on-left, will cost at least 500.
Figure 3: Simplified probabilisticgraphical models for confidenceof learned (A) Preconditions and(B) Cost).
Figure 4: The plan and foils used in the evaluation, plans are highlighted in blue and foilsin red: (i) shows the two montezuma’s revenge screens used (ii) the two variations of thesokoban game used. For Montezuma the concepts corresponding to missing precondition arenot_on_rope, not_on」eft」edge, not_skull_on」eft and is_clear_down_of_crab for failure points A, B, Cand D respectively (note all concepts were originally defined as positives and We formed the not.
Figure 5: The average probability as-signed to the correct model compo-nent by the search algorithms, calcu-lated over ten random search episodeswith std-deviation.
Figure 7: A simplified probabilistic graphical models for explanation inference, Subfigure (A) and (B) assumesclassifiers to be completely correct, while (C) and (D) presents cases where the classifier may be noisy.
Figure 8: Montezuma Foils: Left Image shows foils for level 1, (A) Move right instead of Jump Right (B) Goleft over the edge instead of using ladder (C) Go left instead of jumping over the skull. Right Image shows foilfor level 4, (D) Move Down instead of waiting.
Figure 9: Sokoban Foils: Left Image shows foils for Sokoban-switch, note that the green cell will turn pinkonce the agent passes it. Right Image shows foil for Sokoban-cell.
Figure 10: Saliency map based explanation shown to users as part of H2For H2, as mentioned the baseline was a saliency map based explanation. For generating the saliencymap, we trained the RL agent using DQN with prioritized experience replay Schaul et al. (2015)2.
Figure 11:	Results from Sokoban-switch on the distribution of non-precondition concepts for eachaction. For each action the table reports the concept with the maximum difference between thedistribution of concept for that specific version, versus the overall distribution and the average dif-ference in estimates across concepts.
Figure 12:	Results from Sokoban-switch and Sokoban-Cell on the distribution of action cost acrossdifferent concepts. Here we report only the cost for push actions, since only those actions result inhigher cost.
Figure 13: Screenshot from the survey done to collect sokoban concepts.
Figure 14:	Screenshot from the study interface for H1 for precondition.
Figure 15:	Screenshot from the study interface for H1 for cost function explanations.
Figure 16:	Screenshot from the study interface for H2.
