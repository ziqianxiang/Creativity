Figure 1: (a) The 2D item collection environment (Barreto et al., 2020). The environment consists of10 X 10 cells and the shape (traingle and diamond) of the items represents their type. (b) Five distinctpreference vectors that lie on the surface of the '2 2D ball: wι = (-√1∕2 +√1∕2), W = (0 +1),W3 = (+√1∕2 +√1∕2), W4 = (+10)and W5 = (+√1∕2 -√l∕2). (c) The trajectories taken bythe optimal policies (γ = 0.95) corresponding to the preference vectors w1, . . . , w5 in a simplifiedversion of the environment with two items of each type.
Figure 2: The normalized sum of rewards over 17 evenly spread tasks over the nonnegative quadrantsof the unit circle. The plots are obtained by averaging over 10 runs with 1000 episodes for each task.
Figure 3: The learned SFs of the poli-cies in Π15 and Π24 for the state inFig. 1c and the left action. Here, eachrow corresponds to a different pol-icy and each column corresponds toa different item (‘R’ed and ‘G’reen).
Figure 4: The normalized sum of re-wards of Π15, and the policy sets con-structed by DIAYN, SMP and DSP.
Figure 5: The normalized sum of rewards of the policy sets Π15 and Π24, and DQN on the (a)sequential reward collection and (b) balanced reward collection tasks. Shadowed regions are onestandard error over 10 runs.
Figure 6: The normalized sum of rewards of thepolicy sets Π15 and Π24, DQN, and MaxQInit ina lifelong RL setting described in the text. Shad-owed regions are one standard error over 100 runs.
Figure 7: Eight different preference vectorsthat lie on the surface of the `2 2D ball anda single preference vector (w9) that is at theorigin of this ball. The corresponding valuesare: wι = (-√1∕2 +√1∕2), W =(0+1),w3 = (+√1∕2 +√1∕2), w4 = (+10),W5 = (+√1∕2 -√1∕2) , W6 = (0 T) W7 =(-√1∕2 -√1∕2) , W8 = (T 0) , W9 = (0 0).
Figure 8: The unnormalized sum of rewards over 17 evenly spread tasks over the nonnegative quad-rants of the unit circle. The plots are obtained by averaging over 10 runs with 1000 episodes foreach task. The performance comparison of (a) Π15 with disjoint sets Π24 and Π5, and (b) Π15 withincrementally growing sets Π152, Π1523 and Π15234 .
Figure 9: The unnormalized sum of rewards of Π15 , and the policy sets constructed by DIAYN,SMP and DSP. Since the policy sets constructed by the prior methods depend on their particularinitialization, their plots are obtained by running each of the constructed policy sets for 5 runs andthen averaging over their results. For each task, the agent was evaluated on 1000 episodes. The plotfor Π15 is obtained in a similar way as in Fig. 8.
Figure 10: The unnormalized sum of rewards of the policy sets Π15 and Π24, and DQN on the (a)sequential reward collection and (b) balanced reward collection tasks. Shadowed regions are onestandard error over 10 runs.
Figure 11:	The unnormalized sum of rewards of the policy sets Π15 and Π24, DQN, and MaxQInitin a lifelong RL setting described in the text. Shadowed regions are one standard error over 100runs.
Figure 12:	The normalized sum of rewards over 17 evenly spread tasks over the nonnegative quad-rants of the unit circle. The results provided in this figure are for the stochastic 2D item collectionenvironment with “slip” probability 0.125. The plots are obtained by averaging over 10 runs with1000 episodes for each task. The performance comparison of Π15 (a) with disjoint sets Π24 and Π5,and (b) with incrementally growing sets Π152, Π1523 and Π15234.
Figure 13:	The normalized sum of rewards over 17 evenly spread tasks over the nonnegative quad-rants of the unit circle. The results provided in this figure are for the stochastic 2D item collectionenvironment with “slip” probability 0.25. The plots are obtained by averaging over 10 runs with1000 episodes for each task. The performance comparison of Π15 (a) with disjoint sets Π24 and Π5,and (b) with incrementally growing sets Π152, Π1523 and Π15234.
