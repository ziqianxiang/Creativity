Figure 1: An example of non-stationary influence kernel k(t0, t) of event time t0 and future time t > t0.
Figure 2: (a) The architecture of the proposed non-stationary neural spectral kernel. A hidden embedding willfirst summarize the data point; then, the embedding will be mapped to different features via multi-branch neuralnetworks. The kernel value is calculated by summing up the products between two sets of learned features, {ψr }and {φr}, weighted by the spectrum {νr}. The spectrum, feature functions, and hidden embeddings are jointlylearned from data. (b) The structure of the multi-branch neural network. There are two shared layers with nnodes per layer that generate the hidden embedding; d denotes the dimension of the input data point; p denotesthe dimension of the middle layer that generates the feature. Further specifications of neural networks will beprovided in the experiments in Section 4.
Figure 3: Kernel recovery results on a one-dimensional synthetic data set. The first three panels show the truekernel that generates the data, kernel learned by our model, and kernel learned by a vanilla Hawkes process,respectively. The fourth panel shows the true and predicted conditional intensity functions of a test sequence.
Figure 4: Kernel recovery results on a two-dimensional synthetic data set. Two rows presents the results ofthe true model and our learned model, respectively. The first three columns show different snapshots of kernelevaluation for each model; the last column shows their corresponding conditional intensity over marked-temporalspace given a test sequence, where the black dots indicate the location of the events.
Figure 5: Kernel recovery results on a one-dimensional synthetic data set generated by a vanilla Hawkes processwith a stationary exponentially decaying kernel. This experiment acts as a sanity check.
Figure 6: Predicted conditional intensity using our method and other baselines for a sequence selected from testdata. We aggregate the conditional intensity in mark space for ease of presentation and visualize the averageconditional intensity over time for two-dimensional synthetic data on the third panel.
Figure 7:	Additional kernel recovery results for two other one-dimensional synthetic data sets. The first threecolumns show the true kernel that generates the data, kernel learned by our model, and kernel learned by aHawkes process, respectively. The fourth column shows the true and predicted conditional intensity functionsfor a test sequence.
Figure 8:	Additional kernel recovery results for two other two-dimensional synthetic data sets. The first and thirdrows show the true models and the second and fourth rows show the learned models. The first three columns showdifferent snapshots of kernel evaluation for each model; the last column shows their corresponding conditionalintensity over marked-temporal space given a test sequence, where the black dots indicate the location of theevents.
Figure 9: Ablation studies on one-dimensional synthetic data sets used in Figure 3. “Proposed kernel withincreased network size” refers to the model with one more hidden layer and doubled layer width in the sub-networks (p = 20); “Proposed kernel with half training sample size” refers to the model with default architecturebut trained with only a half of the training samples.
Figure 10: Ablation studies on one-dimensional synthetic data set used in Figure 5.
