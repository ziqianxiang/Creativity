Figure 1: Can you predict which of these images are “tricky” for CNNs? Out of every of the sixpairs, one image is always correctly classified and one always incorrectly (answers on the next page1).
Figure 2: Dichotomous Data Difficulty (DDD) in a nutshell: Irrespective of model differences (e.g.
Figure 3: Error consistencies between the different conditions and the base network on the ImageNetvalidation set after 90 epochs. For conditions for which multiple models were trained the mean overall models of a condition is plotted in black.
Figure 4: Decisions on all 50K ImageNet validation images of all 13 networks with different inductivebiases (architectures, ...). Dark red indicates that the respective item was falsely classified by allnetworks. Light red indicates that the image was correctly classified by all networks. Images areordered according to the mean accuracy across networks in the last epoch.
Figure 5: Error consistency on the original ImageNet test-set (left panel) and on the test-set within-between images only (right panel) for the ResNet-variants (a) and the SOTA networks (b). Bothnetworks were trained on the whole ImageNet training set. Error consistency around 0 indicatesindependent responses. A diagonal element of 1 represents that only one network for comparisonwas available, otherwise the within condition consistency is calculated, see section 2.
Figure 6: Barplot displaying the proportions of answers over all observers and for the three questionsmentioned below the subfigures. Colour coded is the image subclass.
Figure 7: Correlations between the last fully connected layers of the different conditions and the basenetwork on the ImageNet validation set after 90 epochs. For conditions in which multiple modelswere trained, only the first model was used.
Figure 8: Decisions on all 50K ImageNet validation images of the single base network over theepochs. Blue indicates that the respective item was falsely classified during the specific epoch, whilewhite indicates that it was correctly classified. The items from the ImageNet validation set are orderedaccording to the mean accuracy the base network achieved on them over the course of the 90 epochs.
Figure 9: Kl-Divergence vs. accuracy for the Gaussian dataset. (Left) KL-divergence between Classiand Classi+1. (Centre) Acc. of Classi. (Right) Scatterplot between KL-divergence and accuracy. Forthe last plot we skip the first 20 classes (with accuracy close to 1) for better visibility.
Figure 10: DDD is neither explained by an exponential/sigmoidal decay in image difficulty, norby uniform example difficulty. (a) Binned functions modelling (exponentially) decaying (orange)and sigmoidal (purple) image difficulty. (b) Observed histogram (blue) with histograms obtained bysampling with a binomial observer given the exponential (orange) and sigmoid (purple) functions.
Figure 11: Barplot showing the proportions of items from each of the three image subsets (“trivial”,“in-between”, “impossible”) belonging to each superclass. Here, the values are normalized so thatthe proportions of items in each superclass sum to 1. The superclasses are ordered according to theproportion of impossibles to trivials.
Figure 12: Proportion of items from each of the three image categories (“Trivials”, “Inbetweens” and“Impossibles”) belonging to each superclass. The values are normalized so that the proportions ofeach subset sum to 1.
Figure 13: Accuracies of the different conditions and the base network on the ImageNet validationset after 90 epochs. The mean over all models of a condition is displayed here.
Figure 14: Accuracies for the SOTA models on the ImageNet validation set. Mean accuracy of allmodels is 0.68926Published as a conference paper at ICLR 2022OL0.
Figure 15: Error consistencies on the ImageNet validation set with VGG-11 as the base network.
Figure 16: Error consistencies on the ImageNet validation set with DenseNet-121 as the base network.
Figure 17: Lineplot showing the number of decisions that change from the current to the followingepoch. For epoch 0, this means that the number of decisions that are different between epoch 0 andepoch 1 are shown. For conditions in which multiple model instances were trained, only the lastinstance is shown for the sake of simplicity.
Figure 18: Pairs of impossible (top) and trivial images (bottom) from ImageNet.
Figure 19: Pairs of impossible (top) and trivial images (bottom) from CIFAR-100.
Figure 20: Class-wise accuracy per dataset. Shown is the decreasing accuracy for all classes in thevalidation sets and for the fully trained base network.
Figure 21: Error consistencies between the different conditions and the base network for the validationsets of CIFAR-100 and our Gaussian dataset. The conditions are ordered by the mean error consistencyon the ImageNet validation set (see Figure 3). For conditions in which multiple models were trained,the model-wise error consistencies are plotted with a lower opacity compared to the mean over allmodels for the conditions.
Figure 22:	Histogram showing how many models correctly classify validation sets images in thelast epoch. In blue, the densities of how many items were answered correctly are shown. “None”indicates that no models classified the items correctly (impossibles), while for “All” items wereclassified correctly by all models (“trivial images”). For the sake of simplicity, only the last modelwas used for conditions where multiple models were trained. In green, samples are drawn from abinomial distribution with n equal to the number of models and p equal to the mean accuracy over themodels. Additionally for ImageNet, the distribution of 5000 label errors as identified by the cleanlabpackage are shown in red (Northcutt et al., 2021a).
Figure 23:	Example trial from the first psychophysical experiment. Observers were asked: “Is theright or the left image easier to classify for a neural network?”. The number on the left indicates thetrial number and the letters “R” and “L” above the images were entered into the answer sheet by theobservers.
Figure 24:	Example trial from the second psychophysical experiment. Observers were asked: “Howmany objects belonging to different categories are in the image (e.g. three dogs are still one category:dog. But two dogs and one cat are two categories: dog and cat)?”, “Is there a main category inthe image? (No, maybe, Yes)” and “Is the presentation of the objects unusual in any manner (e.g.
Figure 25: Barplot displaying the proportions of answers over all observers. We did not remove labelerrors for this plot. The bars are normalized so that the proportions of the different answers add up to1 for each question.
Figure 26: Barplot displaying the proportions of answers over all observers. For this plot, we removedimages which were found to have label errors by Northcutt et al. (2021b) and balanced the imageclasses to have the same number of images. The bars are normalized so that the proportions of thedifferent answers add up to 1 for each question.
Figure 27:	Barplots displaying the proportions of answers for each individual observer. We did notremove label errors for this plot. The bars are normalized so that the proportions of the differentanswers add up to 1 for each question.
Figure 28:	Barplots displaying the proportions of answers for each individual observer. We removedimages which were found to have label errors by Northcutt et al. Northcutt et al. (2021b). The barsare normalized so that the proportions of the different answers add up to 1 for each question.
