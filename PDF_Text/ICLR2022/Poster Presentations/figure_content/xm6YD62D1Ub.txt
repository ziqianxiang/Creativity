Figure 1: VICReg: joint embedding architecture with variance, invariance and covarianceregularization. Given a batch of images I , two batches of different views X and X0 are producedand are then encoded into representations Y and Y 0 . The representations are fed to an expanderproducing the embeddings Z and Z0 . The distance between two embeddings from the same image isminimized, the variance of each embedding variable over a batch is maintained above a threshold, andthe covariance between pairs of embedding variables over a batch are attracted to zero, decorrelatingthe variables from each other. Although the two branches do not require identical architectures norshare weights, in most of our experiments, they are Siamese with shared weights: the encoders areResNet-50 backbones with output dimension 2048. The expanders have 3 fully-connected layers ofsize 8192.
Figure 2:	Conceptual comparison between different self-supervised methods. The inputs X andX0 are fed to an encoder f with weights θ. The representations Y and Y 0 are further processed by anetwork h with weights ψ. h can be a projector (narrowing trapeze) that reduces the dimensionalityof the representations, or an expander (widening trapeze) that increases their dimensionality. Acriterion is finally applied on the embeddings Z and Z0 . VICReg (a) works when both branches haveencoders f and f0 with different architectures and sets of weights θ and θ0. Each branch’s varianceand covariance are regularized by regularizers v and c, and the distance between both branches isminimized with a mean-squared error loss s. Barlow Twins (b) uses a loss c to decorrelate pairs ofdifferent dimensions in the batch-wise normalized (B-Norm) embeddings, and learns invariance witha loss i that makes similar dimensions highly correlated. W-MSE (c) uses a batch slicing operationthat shuffles batches into small sub-batches, and apply PCA as a whitening operation on the feature-wise normalized (F-Norm) embeddings of each sub-batch. BYOL (d) has an asymmetric architecturewhere the weights θm of one encoder are an exponential moving average (ema) of the other encoder’sweights θ. A predictor g with weights ψ is used in the branch with learnable weights. SimSiam(e) uses a predictor on one branch and a stop-gradient operation (sg) on the other one. SimCLR (f)uses the InfoNCE contrastive loss where all the feature-wise normalized embeddings are comparedbetween them inside a batch. Samples from distorted versions of the same input are brought close toeach other, while other samples are pushed away. SwAV (g) quantizes the feature-wise normalizedembeddings of a branch and use it as target for the other one. OBoW (h) uses bag-of-words (BoW)representations and a cross-entropy loss to compare the BoW generated by a teacher network from
Figure 3:	Incorporating variance regularization in BYOL and SimSiam. Top-1 accuracy on thelinear evaluation protocol for different number of pretraining epochs. For both methods pre-trainingfollows the optimization and data augmentation protocol of their original paper but is based on ourimplementation. Var indicates variance regularizationcorrelation matrix of the representations:2d(d1-D X C(Y 焉 +C(Y 0)2j,i6=j(9)where Y and Y 0 are the standardized representations and C is defined in Eq. (3). In BYOL thiscoefficient is much lower using covariance regularization, which translate in a small improvement ofthe performance, according to Table 4. We do not observe the same improvement in SimSiam, bothin terms of correlation coefficient, and in terms of performance on linear classification. The averagecorrelation coefficient is correlated with the performance, which motivates the fact that decorrelationand redundancy reduction are core mechanisms for learning self-supervised representations.
Figure 4:	Standard deviation of the features during BYOL and SimSiam pretraining. Evolutionof the average standard deviation of each dimension of the features with and without varianceregularization (Var). left: the standard deviation is measured on the representations, right: thestandard deviation is measured on the embeddings.
Figure 5: Average correlation coefficient of the features during BYOL and SimSiam pretrain-ing. Evolution of the average correlation coefficient measured by averaging the off-diagonal terms ofthe correlation matrix of the representations with BYOL, BYOL with variance-covariance regulariza-tion (BYOL VarCov), SimSiam, and SimSiam with variance-covariance regularization (SimSiamVarCov).
