Figure 1: RL-based query object localization: training and adaptation. Image-box pairs are onlyavailable in the training phase. Test environments include images without box annotations and anexemplary set specifying a different task objective. Thus, the trained policy needs to be adapted.
Figure 2: (a) Modules for ordinal representation learning. The RoI encoder extracts RoI feature thatwill be used as the state representation for localization. The projection head learns ordinal embeddingb for computing reward. (b) Controller network. In each step, the agent takes the output from the RoIencoder as state, while it also maintains an internal hidden state hr within a RNN, which encodesinformation from history observations. Then it outputs the action for next step.
Figure 3: CorLoc (%) Comparison with IoU-based reward on COCO dataset. “ours": ordinalembedding based reward; “iou": IoU based reward, ImageNet pre-trained embedding. Dotted linesare the results of directly training on target class, using embedding based reward but from a ImageNetpre-trained model, indicating the advantage of our approach, with ordinal embedding learned from asingle source class and policy adaptation.
Figure 4: CorLoc (%) comparison with ranking method using Faster RCNN pre-trained backbone.
Figure 5: Learning RoI Encoder and Projection Head.
Figure 7: Corrupted MNIST Datasets: 28 × 28 digits on four kinds of 84 × 84 noisy background.
Figure 8: Selective localization vs. co-localization on two-digits data with random patch background.
Figure 9: Comparison under different training set sizes. "AE+IoU": autoencoder, IoU based reward;"AE+Embed": autoencoder, embedding distance based reward; "AE+Ord+IoU": autoencoder andordinal projection head, IoU based reward; "AE+Ord+Embed": autoencoder and ordinal projectionhead, embedding distance based reward. (a) Test performance on the trained digits 4, (b) Average testperformance on the other nine digits.
Figure 10: CorLoc(%) comparison with ranking method using ImageNet pre-trained backbone.
Figure 11: Rank correlations between embedding distance and IoU: using different embeddingfunctions such as ImageNet and Faster RCNN, with or without the ordinal pre-training, and ViT.
Figure 12: Results using different training set size in stage 1, 2.
Figure 13: Annotation refinement. Left: Loose to tight transfer. Right: entire body to head transfer.
Figure 14: Starting from the whole image, the agent is able to localize digit 4 within 10 steps. Eachrow shows localizing one image, and each column is a time step. Green box means ground-truthbounding box.
Figure 15: The IoU relation between ground-truth and predicted box is accurately represented inembedding space. Left: the embedding distance between ground-truth and predicted box in eachstep. Right: the IoU between ground-truth and predicted box in each step.
