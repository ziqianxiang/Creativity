Figure 1: Part of the BPs used in our experiments (others in Apdx. D.3) and images with these BPsembedded. (a)-(d) are additive perturbation BPs and (e)-(f) are patch replacement BPs. BP in (a) isamplified for visualization. Spatial locations for BPs in (b)-(f) are randomly selected for each attack.
Figure 2: Comparison between our ET statistic (for both RE-AP and RE-PR configurations) andstatistic types used by existing REDs (L1, L2, and CS). Only for ET, there is a common range forall 2-class domains for choosing a threshold to distinguish BA target classes (blue) from non-targetclasses (orange). Such common range also contains the constant threshold 1/2 (red dashed line).
Figure 3: ROC curves for ET, L1, L2, and SC in distinguishing BA target classes from non-targetclasses for the large variety of classification domains and attack configurations considered in Fig. 2.
Figure 4: Histogram of l2 norm ratio betweenpair-wise additive perturbation and maximum ofthe two sample-wise perturbations for each ran-dom image pair for clean classifiers.
Figure 5: BPs used in our experiments that are not shown in Fig. 1 of the main paper due to spacelimitations; and images with these BPs embedded. BPs in (a) and (b) are amplified for visualization.
Figure 6: Average CS statistic versus the number of classes in the domain.
Figure 7: Examples for backdoor training images for clean-label BAs. These images are origi-nally from the BA target class, perturbed (in human-imperceptible fashion) to be misclassified by asurrogate classifier, embedded with the BP, and are still labeled to the target class.
Figure 8: Effectiveness of our defense, with the constant ET threshold 1/2, against the clean-labelBA proposed by Turner et al. (2019). Classifiers being attacked have a maximum ET (over the twoclasses) greater than 1/2; clean classifiers have a maximum ET (over the two classes) less than 1/2.
Figure 9: Histogram of l1 norm ratio between pair-wise common mask and maximum of the twosample-wise masks for each random image pair for clean classifiers.
Figure 10: Accuracy of detection inference on the ensemble of attack instances A1 and the ensembleof clean instances C1, when the number of images used for detection varies in [2, 5, 10, 15].
Figure 11: Example growing curves of pn(i) (with patience Ï„ = 8). In each figure, there are 20curves, each corresponding to a clean sample used for detection. ET is the average final p(ni) overthese 20 samples.
Figure 12: Images synthesized using a simpler version of the model inversion method used by Chenet al. (2019).
Figure 13: Histogram of ET statistics for classifiers in A6 and C6, when the images for backdoorpattern reverse-engineering are synthesized.
Figure 14: Execution time versus the number of samples for detection.
Figure 15: Example of (a) a failed BP reverse-engineering; and (b) a successful BP reverse-engineering. For both examples, the estimated BP is on the top, while the true BP used by theattacker is at the bottom.
Figure 16: Histogram of the maximum ET over the five classes for classifier ensembles with (a) 1attack, (b) 2 attacks, and (c) no attack.
