Figure 1: Left: supMIWAE computational struc-ture: encoding network gγ , decoding network hθand discriminative network fφ . Right: Graphi-cal model: supMIWAE seen as a VAE for missingvalues concatenated with neural discriminator.
Figure 2: Left: Kernel density of a two-class dataset (the burger dataset), where and y = 0 is blueand y = 1 is brown/orange. Observations with missing values are shown as sticks in the bottommargin of the plots and colored according to their class label. For observations with missing 2ndcoordinate there is an inherent ambiguity about the class with p(y = 1|xobs) = 2/3. The optimalsingle imputation is shown as the dashed black line. Middle: Decision surface for a neural classifiertrained on optimally-imputed data. The decision surface has to reflect that for the imputed datap(y = 11∣o(XObs)) = 2/3, thus the increased class y = 1 probability around the optimal imputations(here ι0 is the imputation function, see appendix A.1). Right: Decision surface as learnt by thesupMIWAE. It is similar to the decision surface that would be found in the absence of missing data.
Figure 3: Top row: (except top left) kernel density of the half-moons dataset together with im-putations from the given model. The supMIWAE does not rely on single imputations, instead itdraws multiple importance samples which are passed through the discriminator to give an impor-tance weighted prediction. For the supMIWAE multiple imputations at three different values of thehorizontal coordinate are shown and a kernel density of the multiple imputations are shown to theleft. For the rest of the methods single imputations are shown in red. Bottom row: Decision sur-faces learnt, depending on the strategy for handling missing values. The methods based on singleimputation need to warp the decision boundaries.
Figure 4: Learning curves on the half-moons dataset: train (full lines) and test(dashed lines) set accuracies, when theclassifier has full capacity.
Figure 5: Predictive performance when varying the capacity of the learner in terms of number of hid-den units. The discriminative model has the same architecture across methods, only the imputationstrategy differs.
Figure 6: Test set accuracies on the MNIST and Fashion MNIST datasets, with different missingmechanisms. Left column: observed squares. Middle column: missing squares. Right column:random dropout over a range of missing rates.
Figure 7: Test-set root mean square error on UCI datasets at varying missing rates.
Figure 8: 2D datasets with two or more classes.
Figure 9: Samples from the MIWAE, fitted to the data with missing values on the four 2D datasets.
Figure 10: First and third rows: (except top left) kernel density of the half-moons dataset togetherwith imputations from the given model. The supMIWAE does not rely on single imputations, in-stead it draws multiple importance samples which are passed through the discriminator to give animportance weighted prediction. For the supMIWAE multiple imputations at three different valuesof the horizontal coordinate are shown and a kernel density of the multiple imputations are shown tothe left. Permutation invariance, the joint GMM and discriminator approach of Smieja et al. (2018)and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on thestrategy for handling missing values.
Figure 11: First and third rows: (except top left) kernel density of the burger dataset togetherwith imputations from the given model. The supMIWAE does not rely on single imputations, in-stead it draws multiple importance samples which are passed through the discriminator to give animportance weighted prediction. For the supMIWAE multiple imputations at three different valuesof the horizontal coordinate are shown and a kernel density of the multiple imputations are shown tothe left. Permutation invariance, the joint GMM and discriminator approach of Smieja et al. (2018)and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on thestrategy for handling missing values.
Figure 12: First and third rows: (except top left) kernel density of the circles dataset togetherwith imputations from the given model. The supMIWAE does not rely on single imputations, in-stead it draws multiple importance samples which are passed through the discriminator to give animportance weighted prediction. For the supMIWAE multiple imputations at three different valuesof the horizontal coordinate are shown and a kernel density of the multiple imputations are shown tothe left. Permutation invariance, the joint GMM and discriminator approach of Smieja et al. (2018)and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on thestrategy for handling missing values.
Figure 13: First and third rows: (except top left) kernel density of the pin-wheel dataset togetherwith imputations from the given model. The supMIWAE does not rely on single imputations, in-stead it draws multiple importance samples which are passed through the discriminator to give animportance weighted prediction. For the supMIWAE multiple imputations at three different valuesof the horizontal coordinate are shown and a kernel density of the multiple imputations are shownto the left. Permutation invariance and gradient boosting does not rely on explicit imputations, forthe rest of the methods single imputations are shown in red. Second and fourth rows: Decisionsurfaces learnt, depending on the strategy for handling missing values.
Figure 14: Predictive performance when varyingthe capacity of the learner, in terms of hidden units.
Figure 15:	Observed squares. Left column contains data with missing values represented as redpixels. Second column contains single imputations from the MWIAE using self-normalized impor-tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-resampling.
Figure 16:	Missing squares. Left column contains data with missing values represented as redpixels. Second column contains single imputations from the MWIAE using self-normalized impor-tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-resampling.
Figure 17: Missing squares. Left column contains data with missing values represented as redpixels. Second column contains single imputations from the MWIAE using self-normalized impor-tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-resampling.
Figure 18:	Test set accuracies on the SVHN dataset, with different missing mechanisms. Leftcolumn: observed squares. Middle column: missing squares. Right column: random dropout withmissing rate 0.5.
Figure 19:	Fully observed test set accuracies on the SVHN dataset, with different missing mecha-nisms. Left column: observed squares. Middle column: missing squares. Right column: randomdropout with missing rate 0.5.
Figure 20: Test-set root mean square error on UCI datasets at varying missing rates.
