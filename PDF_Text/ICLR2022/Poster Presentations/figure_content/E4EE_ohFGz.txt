Figure 1: Training accuracy of language models. (Left): on-device training in a practical FL system. (Middle):simulation of the smooth distribution shift with qL,ι (t) on Stack Overflow (T = 256). In the beginning of eachperiod, the probability of clients coming from nighttime mode is 1, which linearly decreases to 0 at the middleof each period so that all clients are from daytime mode, corresponding to the peaks and valleys of the curvesrespectively. (Right): simulation of the block-cyclic shift on Stack Overflow (T = 256).
Figure 2: Comparing the results of FEDTEM and FEDTKM with baseline methods. On EMNIST and CIFAR,FedTKM can achieve similar performances as FedTEM and outperforms most of the other methods. On StackOverflow, FedTKM is not as good as FedTEM, but it still outperforms other methods, and can be even betterthan the model trained without distribution shift when the training population is less biased (p is closer to 1).
Figure 3: The effect of the temporal prior. Note We always use the same periodical linear or cosine priors,qL,ι (t) or qc,ι (t), under all types and p's for the distribution shift.
Figure 4: Comparing the effect of MLE on EMNIST and CIFAR.
Figure 5:	Compare the effect of the label smoothing regularization on FEDTEM under linear and cosinedistribution shifts. We use the linear prior in all cases.
Figure 6:	Comparing the Bayesian and MLE based branch selection on EMNIST and CIFAR under lineardistribution shifts. We also compare the greedy branch selection vs. sampling based branch selection onEMNIST.
Figure 7:	Probability of sampling of clients from I1 (t) in each round, with T = 256, under the linear andcosine transit functions and different values of p.
Figure 8: Evaluating the effect of the underlying T on FEDTEM with linear prior.
Figure 9: The effect of using multi-branch networks for the baselines. “Vanilla (s)” is the single-branchnetwork trained with FedAdam under various distribtuion shifts. “No Dist. Shift (s)” is the single-branchnetwork trained with FedAdam with no distributoin shift.
Figure 10: The effect of local training set sizes of each client. x-axis represents the maximum number oftraining samples per client. For “inf”，We use all available training samples from the original dataset. To addto the challenge, we ensure the distribution shift is different from the temporal prior. On EMNIST and CIFAR,we consider cosine data distribution shift with P = 1 and use linear prior for FedTEM. On Stack Overflow,we consider cosine distribution shift and linear prior. We also compare with the results for the baseline modeltrained without distribution shift (No Dist. Shift).
Figure 11: The training accuracy, out of vocabulary rates and total number of tokens at each round for traininga next word prediction model in a real FL system. In general, the out of vocabulary rates become lower onnighttime clients, and the sentence lengths become longer on nighttime clients. The plots also show training isfaster during nighttime, since more rounds are finished during nighttime.
