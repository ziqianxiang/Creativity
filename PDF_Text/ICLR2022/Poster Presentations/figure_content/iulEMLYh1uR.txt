Figure 1: Comparison of standard Transformers, Universal Transformers and Switch Transformers interms of three common cost metrics: number of parameters, FLOPs, and throughput. Relative rankingbetween models is reversed between the two cost indicators. Experiments and the computation of costmetrics were done with Mesh Tensorflow (Shazeer et al., 2018), using 64 TPU-V3.
Figure 2: The learning progress ofa ResNet-101 × 3 on JFT-300M withshort and long schedules, obtainedfrom (Kolesnikov et al., 2020). Decayingthe learning rate too early leads to higherperformance in lower steps, but the finalperformance is significantly worse.
Figure 3: The learning progressof a ResNet-101 × 3 on JFT-300Mwith different weight decays, obtainedfrom (Kolesnikov et al., 2020). Lowerweight decay leads to acceleration ofconvergence, while eventually resultsin an under-performing final model.
Figure 4:	Comparison of scaling a small ViT in depth (D, number of encoder blocks) vs scaling itinwidth (W, hidden dimension). Which architecture appears “better”, in terms of cost-quality trade-off,changes depending on which indicator is considered. Experiments and the computation of cost metricswere done with Scenic (Dehghani et al., 2021a), using 64 TPU-V3.
Figure 5:	Accuracy and value of different cost indicators for different models on ImageNet dataset.
