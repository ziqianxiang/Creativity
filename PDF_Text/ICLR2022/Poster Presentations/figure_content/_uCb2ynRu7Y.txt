Figure 1: Illustration of Path Integral Sampler (PIS). The optimal policy of a specific stochasticcontrol problem where a terminal cost function is chosen according to the given target density μ,can generate unbiased samples over a finite time horizon.
Figure 2: Sampling performance on rings-shape density function with 100 steps. The gradientinformation can help PIS-Grad and MCMC algorithm improve sampling performance.
Figure 4:	Sampling performance on a challenging 2D unnormalized density model with well-separated modes. Kernel density estimation plots are compared with 2k samples. AFT and SMCuse annealing trick with 10 decreasing temperate levels and HMC kernel following (Arbel et al.,2021). Even without annealing trick and resampling, Path Integral Sampler (PIS) generates visuallyindistinguishable samples from target density with 100 steps. PIS starts x0 from origin point whileothers start from a standard Gaussian. The underlying distribution is chosen deliberately to distin-guish the performance of different methods. In particular, 100 steps are not sufficient for generalMCMC to converge to the stationary distribution. We also note performance of compared methodscan be further improved with tuning temperature scheduling, samples initialization. Our genericalgorithm can explore more modes with similar initialization and less tuning parameters.
Figure 5:	Generated samples from SVGD (Liu & Wang, 2016) with 100 steps. We generated sampleswith batch size 100, 1000, 5000. We find with more particles, samples generated are more closed tothe ground truth data.
Figure 6:	Origin data images and their reconstructions from trained vanilla VAE. It can be seen thatreconstruction images are smoother compared with the original images.
Figure 7: Generated 5000 uncurated samples with T = 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0.
Figure 8: Large T with large δt deteriorates sample quality due to discretization error in Fig 8a butit can be eased with increasing number of steps in Fig 8b.
Figure 9: Another failure case with T = 2, ∆t = 0.02, N = 100 due to randomness of trainingnetworks.
