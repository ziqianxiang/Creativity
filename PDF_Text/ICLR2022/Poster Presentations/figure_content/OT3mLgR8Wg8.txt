Figure 1: We formulate a novel task of learning inter-object functional relationships (IFRs) in novel3D indoor scenes. Given an input 3D scene with multiple objects (left), We predict a functionalscene graph (middle) with IFRs (e.g., a knife cuts fruits, the microwave button triggers its dooropen). Our proposed method learns prior knowledge over object geometry (green dashed lines) andposteriors (yellow dashed lines) after performing several interactions (yellow hand signs). In cases ofrelationship uncertainty such as the stove example (right), learning from the interaction is necessary.
Figure 2: System Overview. During training (left), we jointly learn an RL policy to explore large-scale scenes and use the collected interaction observations to supervise the prior/posterior networks. Inthe test time (right), given a novel scene, We first propose possible inter-object functional relationshipsby applying the learned prior knowledge and then perform interactions to resolve uncertain cases.
Figure 3: Network Architecture. The BR-Prior-Net takes two object point clouds as input andestimates their functional relationship likelihood. By passing all object pairs through BR-Prior-Net,We obtain a binary functional graph prior RS. Then, We employ a Graph Convolutional Network(GCN) to model the scene context and predict a scene functional graph prior RS. We repurpose thesame GCN as SR-Posterior-Net which produces a scene posterior graph RSt) after each interactionstep t. The agent outputs a final functional scene graph prediction Rs after addressing all uncertainties.
Figure 4: Qualitative Results. We show top-down views of one example scene. In the bottom row,from left to right, we show the functional scene-graph predictions given by binary priors, scene priors,posteriors with 10% interactions, posteriors with 20% interactions. In these figures, we mark theground-truth relationships in blue lines, the predicted ones in red lines, the interaction observations ingreen lines, and the interacted objects with green cross symbols. We assign each object with a yellowID linking to one of the top-row subfigures for some example object zoom-ins. The green IDs in thetop row mean the object has been interacted during exploration.
