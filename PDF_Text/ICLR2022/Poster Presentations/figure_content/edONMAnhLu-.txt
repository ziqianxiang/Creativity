Figure 1: Consider original loss f (solid line), perturbed loss fp，max∣∣δ∣∣≤ρ f (w+δ) (dashed line),and surrogate gap h(w) , fp(w) - f(w). Intuitively, fp is approximately a max-pooled version off with a pooling kernel of width 2ρ, and SAM minimizes fp . From left to right are the local minimacentered at w1 , w2, w3, and the valleys become flatter. Since fp(w1) = fp(w3) < fp(w2), SAMprefers w1 and w3 to w2. However, a low fp could appear in both sharp (w1) and flat (w3) minima,so fp might disagree with sharpness. On the contrary, a smaller surrogate gap h indicates a flatterloss surface (Lemma 3.3). From w1 to w3, the loss surface is flatter, and h is smaller.
Figure 2: Vf is decomposed intoparallel and vertical (Vf⊥) Com-ponents by projection onto Vfp .
Figure 3: Consider the loss surface with a few sharp local minima. Left: Overview of the proceduresof SGD, SAM and GSAM. SGD takes a descent step at Wt using Vf(Wt) (orange), which points toa sharp local minima. SAM first performs gradient ascent in the direction of Vf (Wt) to reach Wadvwith a higher loss, followed by descent with gradient Vf (Wtadv) (green) at the perturbed weight.
Figure 4: Influence of ρ (set as constant for ease of comparison, other experiments use decayedρt schedule) and α on the training of ViT-B/32. Left: Top-1 accuracy on ImageNet. Middle:Estimation of the dominant eigenvalues from the surrogate gap, σmaχ ≈ 2h∕ρ2. Right: Dominanteigenvalues of the Hessian calculated via the power iteration. Middle and right figures match inthe trend of curves, validating that the surrogate gap can be viewed as a proxy of the dominanteigenvalue of Hessian.
Figure 5: Top-1 accuracy of Mixer-S/32 trained with different methods. “+ascent" representsapplying the ascent step in Algo. 1 to an optimizer. Note that our GSAM is described assAM+ascent(=GSAM) for consistency.
Figure 6: Top-1 accuracy ofViT-B/32 for the additional studies (Section 6.4). Left: from left to rightare performances under different data augmentations (details in Appendix B.3), where the vanillamethod is trained for 2× the epochs. Middle: performance with different base optimizers. Right:Comparison between min(fp, h) and min(f, h).
Figure 7: Performance of GSAM varying with P and α.
Figure 8: The value of cos θt varying with train-ing steps, where θt is the angle between Vf (Wt)and VfP(Wt) as in Fig. 2.
Figure 9: Surrogate gap curve under different ɑvalues.
