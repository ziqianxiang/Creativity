Figure 1: Overview of our approach. We discover motifs from data (a) and use them to decomposean input molecule (b) into motifs and single atoms. In the encoder (c), atom features (bottom) arecombined with motif embeddings (top), making the motif information available at the atom level.
Figure 2: Frechet ChemNet Distance (lower is better) for different generation orders and vocabularysizes. We consider generation from scratch (left), and generation starting from a scaffold (right).
Figure 3: Scaffold from a GuacaMolbenchmark (top) and a scaffold fromour additional benchmark (bottom).
Figure 4: Comparison on tasks from Lim et al. (2019). We show both single-property optimizationtasks as well as one where all properties must be optimized simultaneously. We plot averages andstandard error over 20 runs for each task; each run uses a different scaffold and property targets.
Figure 5: Interpolation between latent encodings of two molecules; unconstrained decoding (top),constrained with scaffold (bottom). The scaffold is highlighted in each molecule that contains it.
Figure 6: Samples from the prior of a trained MoLeR model.
Figure 7: Latent space neighborhood of a fixed molecule containing a chemically relevant scaffold.
Figure 8: Six pairs of similar motifs (one per column), as extracted from weights of a trained MoLeRmodel.
Figure 9: Optimization performance (higher is better) for different generation orders and vocabularysizes. We separately show the original GuacaMol benchmarks (left), and our new scaffold-basedbenchmarks (right).
Figure 10: Generation metrics during training, measured for MoLeR both with and withoutmotif embeddings. Using motif embeddings simplifies the learning task, improving the qualityof downstream samples.
Figure 11: Generation metrics during training, measured for MoLeR both with and without trainingstep subsampling. Subsampling training steps leads to faster convergence on all downstream metrics.
Figure 12: Mean difference in property value when decoding random pairs of latent codes with agiven cosine similarity, shown for MoLeR both with and without the Lprop loss. Adding the propertyprediction loss increases the correlation between latent space distance and property value.
