Figure 1: Empirical marginal weight distributionsof a layer of FCNNs and CNNs trained with SGDon MNIST, and an early layer of several ResNetstrained on CIFAR-10. We show weight histograms(left) and quantile-quantile (Q-Q) plots with differ-ent distributions (right). The empirical weights areclearly heavier-tailed than a Gaussian (green line),and better fit by a Laplace (orange line).
Figure 2: (a) Degrees of freedom for Student-t distributions fitted to the weights of a ResNet20trained on CIFAR-10. The degrees of freedom get larger in deeper layers, implying that the weightdistributions become less heavy-tailed and more similar to Gaussians. The layers marked withasterisks (*) are the first layers of their respective ResNet blocks. (b) Spatial covariance of theweights within CNN filters for a three-hidden layer network trained on MNIST, normalized by thenumber of channels. The weights correlate strongly with neighboring pixels, and anti-correlate (layer1) or do not correlate (layer 2) with distant ones. Each delineated square shows the covariances of afilter location (marked with Ã—) with all other locations.
Figure 3: Spatial covariances for the convolutional weights of the layers of a ResNet-20, normalizedby the maximum variance for each layer, which is shown on the bottom right. We trained the networkwith SGD on CIFAR-10 with data augmentation (10 times). Layer 1 is the closest to the input. Thefirst layer of every ResNet block is marked with an asterisk (*). We see that there are significantcovariances in all layers, but that their strength increases for later layers.
Figure 4:	Performances of fully connected BNNs with different priors on MNIST and FashionMNIST(see Sec. 4.2). The heavy-tailed priors generally perform better, especially at higher temperatures,and lead to a less pronounced cold posterior effect. Note the reversed y-axis for OOD detection on theright to ensure that lower values are better in all plots. Shaded regions represent one standard error.
Figure 5:	Performances of convolutional BNNs with different priors on MNIST, FashionMNIST,and CIFAR-10 (see Sec. 4.3). The (Fashion)MNIST experiments used CNNs, while the CIFAR-10experiments used ResNet20. The correlated prior generally performs better than the isotropic ones,but still exhibits a cold posterior effect, while the heavy-tailed priors reduce the cold posterior effect,but yield a worse performance. Note the reversed y-axis for OOD detection on the right to ensurethat lower values are better in all plots. Shaded regions represent one standard error.
