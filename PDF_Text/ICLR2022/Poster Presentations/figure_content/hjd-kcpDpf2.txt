Figure 1: The training graph and CKA similarity heatmaps of a MaxminDQN agent with 2 neuralnetworks. The letters on the plot show the time when CKA similarities were calculated. Heatmaps atA and C have relatively low CKA similarity and have relatively higher average return as compared toheatmaps at point B and D that have extremely high similarity across all the layers. See diagonalvalues from bottom left to top right.
Figure 2:	Left: Fitting a sine function using two different neural network architectures. The upperfunction was approximated using 64 neurons in each hidden layer while the lower function used32 neurons in each hidden layer. Right: Represents the CKA similarity heatmap between differentlayers of both neural networks before and after training. The right diagonal (bottom left to top right)measures representation similarity of the corresponding layers of both neural networks. The trainednetworks have learnt similar representations while their output was 98% similar. See diagonal valuesfrom bottom left to top right.
Figure 3:	Training curves and 95% confidence interval (shaded area) for the MED-RL augmentedvariants for SAC5000⊂ɔ 4000CC.
Figure 4:	Training curves and 95% confidence interval (shaded area) for the MED-RL augmentedvariants for TD37Published as a conference paper at ICLR 20225.2	Experimental SetupContinuous control tasks: We evaluated MED-RL augmented continuous control algorithms suchas TD3, SAC and REDQ on the Mujoco continuous control benchmark environments. We comparedthe results of MED-RL with un-regularized counterparts. We report the mean and standard deviationacross five runs after 1M timesteps on six complex environments: Cheetah, Walker, Hopper, Ant,Humanoid and Humanoid-Standup. For REDQ, we evaluated on Cheetah, Walker, Hopper and Antfor 300K timesteps only.
Figure 5: Training curves and 95% confidence interval (shaded area) for the augmented variants forREDQ together with baseline REDQ.
Figure 6:	All MaxminDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance ofLogarithms17Published as a conference paper at ICLR 2022B.1 Results in Tabular FormTable 5: Max Average Return for MED-RL MaxminDQN with two neural networks on PyGames andMinAtar environments. Maximum value for each task is bolded. ± corresponds to a single standarddeviation over trials.
Figure 7:	All EnsembleDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Varianceof Logarithms20Published as a conference paper at ICLR 2022C.1 Results in Tabular FormTable 8: Max Average Return for MED-RL EnsembleDQN with two neural networks on PyGamesand MinAtar environments. Maximum value for each task is bolded. ± corresponds to a singlestandard deviation over trials.
Figure 8: Clustering last layer activations from Catcher and PixelCopter after processing themwith t-SNE to map them in 2D. The regularized variants have visible clusters while the baselineMaxminDQN and EnsembleDQN activations are mixed together with no visible pattern.
Figure 9: Clustering last layer activations from PixelCopter after processing them witht-SNE to mapthem in 2D22Published as a conference paper at ICLR 2022E Plotting the Gini InequalityWe measured the L2 norm inequality of the baseline MaxminDQN and EnsembleDQN along withtheir regularized versions. We trained baseline MaxminDQN and EnsembleDQN with two neuralnetworks along with their Gini index versions with regularization weight of 10-8 on the PixelCopterenvironment on a fixed seed . Figure 10 represents the L2 norm inequality of the experiments alongtheir average return during training. Notably, despite each neural network being trained on a differentbatch, the L2 norm of the baseline MaxminDQN and EnsembleDQN are quite similar while the L2norm of the regularized MaxminDQN and EnsembleDQN have high inequality.
Figure 10: Left: Plot representing the L2 norm inequality between the two neural networks usingGini index trained on PixelCopter environment. Right: Plot representing the average return duringtraining.
