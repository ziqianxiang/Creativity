Figure 2: Approach Overview. Our proposed approach Latent Action Q-Learning (LAQ) startswith a dataset of (s, s0, r) triples. Using the latent action learning process, each sample is assigned alatent action ^. Q-learning on the dataset of quadruples produces a value function, V(s). Behaviorsare derived from the value function through densified RL, orby guiding low-level controllers.
Figure 3: We experiment with five environments: 2D Grid World, Freeway (Atari), 3D VisualNavigation, Maze2D (2D Continuous Control), and FrankaKitchen. Top right corner of Maze2D andFrankaKitchen, shows the embodiments for cross-embodiment transfer (ant and hook, respectively).
Figure 4: We show learning curves for acquiring behavior using learned value functions. We com-pare densified RL (Section 4.3) with sparse RL and BC/BC+RL. See Section 5.2 for more details.
Figure 5: Behavior acquisition across embodiments. Re-sults averaged over 50 seeds and show ± standard error.
Figure 6: Visualization of trajecto-ries and SPL numbers in the 3D vi-sual navigation environment.
Figure S7:	We plot correctness of value function estimate and behavior as a function of state-basedpurity of the intervening actions used for Q-learning. Left plot shows the mean squared error (loweris better) between the obtained value function and the optimal value function. Right plot showsfraction of states in which the learned value function induces the optimal action (higher is better).
Figure S8:	As a function of stochasticity in environment, we plot: state-based purity of the latentactions (top left), Spearman’s rank correlation between LAQ learned value function and groundtruth value function (top right), correctness of implied behavior by the LAQ learned value function(bottom right), and average state-value of LAQ learend value function, and the ground truth valuefunction (bottom left).
Figure S9:	We visualize the Freeway future prediction models’ reconstructions for ot+1. Fromleft to right, the ground truth ot, the ground truth ot+1, the reconstruction for ot+1 by the futureprediction model with one action, the reconstruction for ot+1 by the future prediction model withlatent actions. The agent is circled in each image.
