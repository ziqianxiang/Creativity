Figure 1: Effect of false negative for con-trastive learning. To study the effect of falsenegatives, we compare two frameworks: Sim-CLR (Chen et al. (2020b), blue) represent-ing instance-level contrastive learning whichis trained with false negatives, and SupCon(Khosla et al. (2020), red) benefiting fromhuman-labeled annotations to exclude falsenegatives. We observe that training with falsenegatives leads to larger performance gaps(orange) on the datasets with more semanticcategories. The proposed approach (green)effectively alleviates such adverse effects byexplicitly removing the detected false nega-tives for self-supervised contrastive learning.
Figure 2: Algorithmic overview. (a) Our method uses pseudo labels in an incremental manner withrespect to the gradually better-trained encoder and embedding space. (b) The proposed contrastivelearning framework uses pseudo labels to explicitly detect and eliminate false negatives from self-supervised contrastive learning.
Figure 3: Visualization of embedding spaces and pseudo label assignments in different trainingstages on CIFAR-10. The samples in the upper and lower row are respectively colored with theground truth labels and the assigned pseudo labels. M marks are the cluster centroids, and light greydots represent the instances without the assigned pseudo labels.
Figure 4: Visualization of the embedding spaces learned by different methods on CIFAR-10.
Figure 5: Hierarchical semantic concepts for ImageNet. We show six images and their respectiveclass labels hierarchically defined in Depth = {3, 5, 7, 9, 18} from top to bottom. Depth 18 (thebottommost) uses original class labels in ImageNet.
Figure 6: Clustering quality in different training stages on CIFAR-100. We train the model withthree different strategies to determine the acceptance rate of pseudo labels and measure the qualityof k-means clusterings (k = 100) on the embedding spaces during the training process.
Figure 7: Visualization of embedding spaces on CIFAR-10. We compare four contrastive learningframeworks based on their embedding spaces through the training process. We list their objectivefunctions in the bracket.
