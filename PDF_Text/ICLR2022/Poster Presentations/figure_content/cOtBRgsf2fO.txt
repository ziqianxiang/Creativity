Figure 1: Communication diagram of two-party split training for an example of online advertising. We studywhether it is possible for the communicated gradient g to leak private label information.
Figure 2: Distributions of quantities discussed in Observations 1-4 after the first 100, 200, 300, 400, 500 stepsof stochastic gradient descent training of the WDL model on Criteo (see experiments).
Figure 3: Norm and cosine leak AUC (computed every batch) at the cut layer and at the first layer under noprotection vs. Marvell with different scale hyperparameter s throughout the ISIC training.
Figure 4: Privacy (norm & cosine leak AUC) vs Utility (test loss & test AUC) trade-off of protection methods(Marvell, iso, no_noise, max,norm) at the cut and first layer on ISIC and Criteo.
Figure 5:	Norm and cosine leak AUC (computed every batch) at the cut layer and at the first layer ofno_noise (no protection) vs. MarVell with different scale hyperparameter S throughout the AVaZU training.
Figure 6:	Norm and cosine leak AUC (computed every batch) at the cut layer and at the first layer ofno_noise (no protection) vs. Marvell with different scale hyperparameter S throughout the Criteo training.
Figure 7:	Privacy (norm and cosine leak AUC) vs Utility (train loss, test loss, and test AUC) trade-off ofprotection methods (Marvell, iso, no_noise, max_norm) at the cut layer and first layer on Avazu.
Figure 8:	Privacy (norm and cosine leak AUC) vs Utility (train loss, test loss, and test AUC) trade-off ofprotection methods (Marvell, iso, no_noise, max_norm) at the cut layer and first layer on Criteo.
Figure 9: Privacy (norm and cosine leak AUC) vs Utility (train loss, test loss, and test AUC) trade-off ofprotection methods (Marvell, iso, no_noise, max_norm) at the cut layer and first layer on ISIC.
Figure 10: For a non-label party’s four-layer convolutional architecture trained on ISIC, we plot the progressionof the norm or cosine leak AUC computed using the activation gradients from each of the four layers throughouttraining in (a), (b) when no random protection is applied and in (c), (d) when using Marvell with privacyhyperparameter s = 4 at the cut layer (layer 4). The four curves overlap in (b) at the value of 1.0. For eachlayer’s data in each figure, a 1-d Gaussian kernel with standard deviation of 5 is convolved with the raw 1-darray of leak AUC values to smooth out the fluctuations and make the different layers’ degree of leakage morevisually distinct.
Figure 11: For a non-label party’s eight-layer MLP architecture trained on Criteo, we plot the progression ofthe norm or cosine leak AUC computed using the activation gradients from each of the eight layers throughouttraining in (a), (b) when no random protection is applied and in (c), (d) when using Marvell with privacyhyperparameter s = 4 at the cut layer (layer 4). The eight curves overlap in (b) at the value of 1.0. For eachlayer’s data in each figure, a 1-d Gaussian kernel with standard deviation of 100 is convolved with the raw 1-darray of leak AUC values to smooth out the fluctuations and make the different layers’ degree of leakage morevisually distinct.
