Figure 1: (a) The illustration of why historical states and actions are encoded in environment-specifiedfactor Z, (b)(c)(d) The PCA visualizations of estimated context (environmental-specific) vectors inPendulum task, where the dots with different colors denote the context vector (after PCA) estimatedfrom different environments. More visualization results are given at Appendix A.13.
Figure 2: An overview of our Relational Intervention approach, where Relational Encoder, PredictionHead and Relational Head are three learnable functions, and the circles denote states (Ground-Truthsare with red boundary, and estimated states are with black boundary), and the rectangles denotethe estimated vectors. Specifically, prediction Loss enables the estimated environmental-specifiedfactor can help the Prediction head to predict the next states, and the relation Loss aims to enforcethe similarity between factors estimated from the same trajectory or environments.
Figure 3: (a) The illustration of causal graph, and the red line denotes the direct causal effect from Zto Sa+Î¹. (b) The illustration of estimating the controlled causal effect.
Figure 4: The average prediction errors of dynamics models on training environments during trainingprocess (over three times). Specifically, the x axis is the training timesteps and y axis is the log valueof average prediction prediction errors. More figures are given at Appendix A.8.
Figure 5:	The average rewards of trained model-based RL agents on unseen test environments. Theresults show the mean and standard deviation of returns averaged over three runs. The fair comparisonbetween TMCL (no adaptation) and our method can be found in Appendix A.64.3 Ablation StudyIn this section, we evaluate the effect of the proposed relation head and intervention prediction onthe generalization improvement, respectively. Because the intervention prediction is based on therelational head, we compare the performance of our approach with and without the intervention.
Figure 6:	(a) The average rewards of trained model-based RL agents on unseen environments. Theresults show the mean and standard deviation of returns averaged over three runs. (b) The averageprediction errors over the training procedure. Prediction errors on test environments are given inAppendix A.95	ConclusionIn this paper, we propose a relational intervention approach to learn a generalized dynamics predictionmodel for dynamics generalization in model-based reinforcement learning. Our approach modelsthe dynamics change as the variation of environment-specified factor Z and explicitly estimatesZ from past transition segments. Because environment label is not available, it is challenging toextract Z from transition segments without introducing additional redundant information. We proposean intervention module to identify the probability of two estimated factors belonging to the sameenvironment, and a relational head to cluster those estimated Zsare from the same environmentswith high probability, thus reducing the redundant information unrelated to the environment. Byincorporating the estimated Z into the dynamics prediction process, the dynamics prediction modelhas a stronger generalization ability against the change of dynamics. The experiments demonstratethat our approach can significantly reduce the dynamics prediction error and improve the performanceof model-based agents on new environments with unseen dynamics.
Figure 7: The estimated similarities between ZS (learned by the model without Intervention module)from different environments and anchors (mass=1) on different tasks, Where the box representthe range of 50% samples, the line in the middle of a box denotes the average similarity and thetop/bottom lines denote the max/min similarities.
Figure 8: The estimated similarities between ZS (learned by the model with Intervention module)from different environments and anchors (mass=1) on different tasks, where the box representthe range of 50% samples, the line in the middle of a box denotes the average similarity and thetop/bottom lines denote the max/min similarities.
Figure 9: The estimated similarities between ZS (learned by the model without Intervention module)from different environments and anchors (mass=1) on different tasks, where the box representthe range of 50% samples, the line in the middle of a box denotes the average similarity and thetop/bottom lines denote the max/min similarities.
Figure 10: The average prediction errors of dynamics models on training environments during trainingprocess (over three times). Specifically, the x axis is the training timesteps and y axis is the log valueof average prediction prediction errors. More figures are given at Appendix A.8.
Figure 11: The average prediction errors of dynamics models on test environments during trainingprocess (over three times). Specifically, the x axis is the training timesteps and y axis is the averagelog value of prediction prediction errors.
Figure 12: The average rewards of trained model-based RL agents on unseen environments. Theresults show the mean and standard deviation of returns averaged over three runs.
Figure 13: The T-SNE visualization of estimated context (environmental-specific) vectors in thePendulum task, where mass = 0.5 and mass =1.3 are from test environments.
Figure 14: The T-SNE visualization of estimated context (environmental-specific) vectors in theHalfcheetah task, where mass = 0.5 and mass =1.5 are from test environments.
Figure 15: The PCA of estimated context (environmental-specific) vectors in Pendulum task, wheremass = 0.5 and mass =1.3 are from test environments.
Figure 16: The PCA of estimated context (environmental-specific) vectors in HalfCheetah task,where mass = 0.5 and mass =1.5 are from test environments.
Figure 17:	The PCA of estimated context (environmental-specific) vectors in the Hopper task.
Figure 18:	The PCA of estimated context (environmental-specific) vectors in the Crip-Ple-Halfcheetah task.
Figure 19:	The PCA of estimated context (environmental-specific) vectors in the Slim-Humanoidtask.
