Figure 1: Overall architecture of FILIP, a dual-stream model with Transformer-based image andtext encoders. On top of the image and text encoders, the representations of textual tokens andvisual tokens are linearly projected to the multi-modal joint space. A novel fine-grained contrastivelearning equipped with cross-modal late interaction is proposed, which uses a token-wise maximumsimilarity between visual and textual tokens.
Figure 2: Visualizations of word-patch alignment for 4 classes of the ImageNet dataset and “a photoof a {label}.” is the prompt. Numbers in the parentheses after the class label indicate the locationindices of the class label in the tokenized textual sequence. The correct predictions are highlightedby opaque patches with the class label indices in red.
Figure 3:	More visualizations on different classes of ImageNet dataset. Numbers in the parenthesesafter the class label indicate the location indices of class label in the tokenized textual sequence.
Figure 4:	Comparison of word-patch alignment between the proposed cross-modal late interactionand that in ColBERT (Khattab & Zaharia, 2020). “a photo of a {label}.” is the prompt. Numbers inthe parentheses after the class label indicate the location indices of the class label in the tokenizedtextual sequence. The correct predictions to the class labels are highlighted by opaque patches withthe class label indices in red. Incorrect predictions to the padded tokens are highlighted by opaquepatches with the padded token indices in blue.
