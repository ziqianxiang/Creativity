Figure 1: Left: A model’s accuracy on the UCR 85 datasets changes by tuning the model’s receptivefield sizes from 10 to 200. Right: The average rank results of each receptive field size are prettysimilar, which means that no single receptive field size can significantly outperform others on mostdatasets.
Figure 2: Left: The label of each line denotes receptive field size and the kernel configuration ofeach ID-CNN. For example, (9):5_5_LL1 means the ID-CNN has five layers and the receptivefield size is 9, and from the first layer to the last layer, kernel sizes of each layer are 5, 5, 1, 1,and 1. Lines of similar color are 1D-CNNs with the same receptive field size, and they are also ofsimilar performance. Right: Lines with similar colors are models which have the same best receptivefield size. For example, all (red/green/blue) lines have the receptive field size (9/39/99), and theirperformances are similar to the bright (red/green/blue) line which denotes the model only has thereceptive field size (9/39/99).
Figure 3:	The left image shows that every even number from 2 to 38 can be composed via twoprime numbers from 1 to 19. This phenomenon can be extended to all even numbers. Based on thisphenomenon, with the OS-block structure in the middle image, we could cover all receptive fieldsizes. Specifically, the first two layers have prime-sized kernels from 1 to Pk. Thus, the two layerscan cover all even number receptive field sizes. With kernels of sizes 1 and 2 in the third layer, wecould cover all integer receptive field sizes in a range via selecting the value pk. The OS-block iseasy to be applied on time series classification tasks. A simple classifier with the OS-block, namelyOS-CNN, is given in the right image, which achieves a series of SOTA performances.
Figure 4:	Examples of using OS-block with other deep learning structures.
Figure 5: Classification accuracies for OS-CNN vs. accuracies from 20 ID-CNNs with receptivefield size. As we can see, for most of the dataset, the orange points (accuracy of OS-CNN) are nearthe top of blue points. More analysis for this comparison can be found in Appendix A.1in the result table because it is the main criteria of the cd-diagram. The full cd-diagram results arelisted in Appendix A.3.
Figure 6: The class activation map of OS-CNN is similar to that of the model which has a betterperformance. For the ScreenType dataset, FCN(10) outperforms FCN(200), and the class activationmap of OS-CNN (green) is similar to FCN(10)(blue). For the InsectWingbeatSound dataset, theclass activation map is similar to FCN(200) for FCN(200) outperforms FCN(10).
Figure 7: The histogram statics the count of datasets by which percentile range of the blue line thatthe orange point belongs to. specifically, we could see that for more than 56% datasets (8+40 out of85 datasets), the result of os-block is larger than 0.95 percentile. This means that, for an unknowndataset, using os-block will have more than 56% chance to achieve a better result than grid searchfrom 20 candidate scales. When seeing the count of the number larger than 0.5 percentile, we couldsee that, for an unknown dataset, using os-block will have more than 96% chance to achieve a betterresult than selecting a random scale.
Figure 8:	The red line is the accuracy range obtained via subtracting the accuracy of OS-CNN fromthe accuracy range of FCN with various kernels. We sorted those datasets in ascending order. Wecould see that for most of the datasets, the highest value of the FCN accuracy range is lower thanthe accuracy of OS-CNN. Which supports the OS-block has the ability to capture the best scales.
Figure 9:	sort datasets by dataset type and max accuracy range - accuracy of the os-CNN. We couldsee that the best scale capture ability keeps the consistency cross different dataset types.
Figure 13: SOTA for UEA 30 multivariate dataset archive876543216.585.415.054.05OS-CNN拜土 ROCKETTS-CHIEFHIVE-COTEFigure 14: SOTA on the UCR 85 datasetsResNetInceptionTimeOS-CNNROCKETFigure 15: SOTA on the UCR 128 datasetsA.4 Examples of the two phenomenaIn the Figure 2, the Google speechcommands dataset is selected as the dataset to show the example.
Figure 14: SOTA on the UCR 85 datasetsResNetInceptionTimeOS-CNNROCKETFigure 15: SOTA on the UCR 128 datasetsA.4 Examples of the two phenomenaIn the Figure 2, the Google speechcommands dataset is selected as the dataset to show the example.
Figure 15: SOTA on the UCR 128 datasetsA.4 Examples of the two phenomenaIn the Figure 2, the Google speechcommands dataset is selected as the dataset to show the example.
Figure 16: The relationship between performance and receptive field size are proportionalSpeechCommands(0.2∕0.8): test acc	SpeechCommands(0.5/0.5)： test acc	SPeeChCommandS(0.8/0.2)： test acc0.8	0.80∙7	0.70.60.50.40.3O 200	400	600	800 IOOOEpoch	Epoch	EpochSet of receptive field size—{10,8.6.4.1}—{10,8}——{10}——OS {10-l}—{40,35,30,25}—{40,35,30}—{40,30}——{40}——OS {40-1}
Figure 17: Lines in the figure are models with different sets of receptive field sizes. Lines withsimilar colors are models which have the same best receptive field size. We could see that the bestreceptive field size mainly dominates the performance in the set of receptive fieldsizes.
Figure 18: The label of each line denotes the kernel configuration of each 1D-CNN. For example,5_5_1_1_1 means the ID-CNN has five layers, and from the first layer to the last layer, kernel sizesof each layer are 5, 5, 1, 1, and 1. Lines of similar color are 1D-CNNs with the same receptive fieldsize, and they are also of similar performance.
Figure 19:	Purple color in those images are the zero mask and yellow denotes the location wherehas the ability to hold weight. Left: Convolution layers in of OS-block can be calculated parallelly,thus, each layer can be viewed as one convolutional layer with zero masks.(s) Right: layers in theOS-block can work with the dilation design16Published as a conference paper at ICLR 2022A.6 Experiment result of OS-block with other structuresThe Figure 20 and Figure 21 show that applied OS-block with residual connection, ensemble, andmulti-channel architectures (individually or together) could further improve the performance. Theevaluation was on both UCR 85 and UEA 30 archives which contain datasets from different domainssuch as electrical devices analysis, Spectrum analysis, traffic analysis, EEG analysis.
Figure 20:	Using the OS-block with residual connection and ensemble (individually or together)could increase the performanceED-lNN(norm)ED-INNDTW-INN-Knorm)DTW-INN-IMLSTM-FCNDTW-lNND(norm)OS-block + multi-channelOS-blockWEASEL+MUSETapNetDTW-INNDFigure 21: Using the multi-channel architecture with OS-block could improve the performanceA.7 Compare the OS-block with other designsMathematically, finding the optimal kernel configuration is challenging, for it is a constrained com-binatorial optimization searching for the best configuration among an exponential number of can-didates. Our contribution is a simple and effective model design that does not need to solve thecomplex optimization problems and achieves state-of-the-art performance on several benchmarks.
Figure 21: Using the multi-channel architecture with OS-block could improve the performanceA.7 Compare the OS-block with other designsMathematically, finding the optimal kernel configuration is challenging, for it is a constrained com-binatorial optimization searching for the best configuration among an exponential number of can-didates. Our contribution is a simple and effective model design that does not need to solve thecomplex optimization problems and achieves state-of-the-art performance on several benchmarks.
