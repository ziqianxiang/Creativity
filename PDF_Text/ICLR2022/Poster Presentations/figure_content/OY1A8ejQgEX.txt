Figure 1: Overview of Mention Memory. A pre-trained mention encoder is used to generate denserepresentations for each entity mention in Wikipedia (approximately 150 million total) which arestored in a table. The tome model takes a passage annotated with entity mention boundaries asinput, and applies a Transformer block. Next, the tome model applies one or more TOMEBlocks.
Figure 2: Claim verification accuracy as a func-tion of fine-tuning memory size (in millions).
