Figure 1: Overview of our proposed model with respect to prior works. Left: In DALLE words in the in-put text act as the composable units for generating the desired novel image. The generated images have globalconsistency because each pixel depends non-linearly on all previous pixels and the input word embeddingsbecause of its transformer decoder. Middle: Unlike DALL∙E that requires text supervision, Slot Attentionprovides an auto-encoding framework in which object slots act as the composable units inferred purely fromraw images. However, during rendering, object slots are composed via a simple weighted sum of pixels ob-tained without any dependency on other pixels and slots which harms image consistency and quality. Right:Our model, SLATE, combines the best of both models. Like Slot Attention our model is free of text-basedsupervision and like DALL∙E, produces novel image compositions with global consistency.
Figure 2: Comparison of training curves of a discriminator to compare the quality of compositional generationbetWeen Slot-Attention and our model. A CNN discriminator tries to discriminate betWeen real images andmodel-generated images. We note that the curves converge more sloWly for our model than for Slot Attentionand shoW that our generations are more realistic.
Figure 3: Comparison of compositional generation between Slot Attention and our model in 3D Shapesand Bitmoji datasets. We visualize the slot prompts provided to the decoder and the resulting compositions.
Figure 4: Object Attention Masks in Textured Images. We show qualitatively that our model produces betterobject attention masks in textured images whereas Slot Attention suffers. In Textured-MNIST, we note that theSlot Attention baseline fails to correctly segment the digit while our model succeeds. In CelebA (bottom row),we note that Slot Attention, unlike our model, incorrectly merges the black hair and the black background intothe same slot due to their similar colors.
Figure 5: Compositional Generation in CLEVR. We show compositional generation in CLEVR by addingobject slots one at a time. In training, only 3-6 objects were shown. However, we note that our model cangeneralize in generating 1-2 and 7-8 objects including the empty scene. We also note that our mirror reflectionsare significantly more clear than those of Slot Attention.
Figure 6: Compositional generation in Shapestacks Dataset. We show compositional generation in Shapes-tacks by composing arbitrary blocks drawn from the concept library in a tower configuration. We note thatSLATE can render the details such as the texture of the floor and the wall significantly better than the mixturedecoder.
Figure 8: Architecture of SLATE. Our model receives an input image and we split it into patches. We encodeeach patch as a discrete token using a feedforward network (shown as the DVAE encoder). A DVAE decoder istrained to reconstruct each patch from its respective token using a simple MSE loss. We shall call this the DVAEreconstruction and this provides one of the two reconstruction pathways that we have in our architecture. Forthe second reconstruction pathway, the tokens obtained from the DVAE encoder are mapped to embeddings.
Figure 9: Clusters in the concept library obtained by applying K -means on slots obtained from the 3D Shapesdataset. We note that our slot representation space models the semantics as the objects of the same class tendto have more similar representation and automatically form a cluster when K-means is applied.
Figure 10: Clusters in the concept library obtained by applying K-means on slots obtained from the TexturedMNIST dataset.
Figure 11: Clusters in the concept library obtained by applying K-means on slots obtained from the Bitmojidataset. We note that our slot representation space models the semantics as the objects of the same class tendto have more similar representation and automatically form a cluster.
Figure 12: Object Replacement and Out-of-Distribution Compositions in Bitmoji. We show that in ourmodel, it is possible to edit the image by taking a specific slot and replacing it with an arbitrary slot drawn fromthe concept library for the same concept. We also show OOD compositions by replacing the slot with thosefrom the opposite gender.
Figure 13: Qualitative Results for Hair Slot Replacements in Bitmoji dataset. For each image, the first rowshows the hair slot that will be used as a replacement and the second row shows the generation from our modelafter the hair slot is replaced. This also provides examples of OOD compositions when the given image has theopposite gender from that of the source image from which the replacement slot is taken.
Figure 16: Qualitative Comparison of Compositional Generation in 3D Shapes.
Figure 17: Qualitative Comparison of Compositional Generation in Shapestacks.
Figure 18: Qualitative Comparison of Compositional Generation in TexturedMNIST.
Figure 19: Qualitative comparison of compositional generation in CLEVRTex.
Figure 21: Qualitative Samples of Compositional Generation in CLEVR from Slot Attention with pixel-mixture decoder.
Figure 22: Qualitative Results for Out-of-Distribution Compositional Generation in Shapestacks from ourmodel.
Figure 23: Qualitative Results for Slot Attention for Out-of-Distribution Compositional Generation in Shapes-tacks.
Figure 24: Analysis of decoder capacity in Slot Attention when using a mixture-based likelihood model fordecoding. We note that when we use a powerful CNN decoder to decode the RGB components and decodingmasks for the objects, then a single component tries to model all the objects in the scene instead of each slotrepresenting a single object as in the case of Spatial Broadcast decoder.
Figure 28: Reconstruction and attention maps of slots in SLATE given the input images for CLEVRTex.
Figure 29: Reconstruction and attention maps of slots in SLATE given the input images for 3D Shapes.
Figure 30: Demonstration of Color Bias of Mixture-based Models in CelebA with Hard-to-DistinguishBackground. In this comparison, we consider the images in which the background has a very similar color asthe hair color. We show the input image, the resulting object attention maps and the reconstruction generatedby Slot Attention and by our model. In these, we note that due to the color bias problem, in Slot Attention,the hair and background have become merged into the same slot in most of the cases. However, in our model,this problem is resolved and we can see that the hair and the background are assigned to separate slots whichis the desired decomposition. We also note how the mixture decoder produces blurry reconstructions while ourreconstructions are significantly more clear.
Figure 31: Analysis of Color Bias of Mixture-based Models in CelebA with Easy-to-Distinguish Back-ground. We analyze the images in which the background has a color distinct from the hair color. This com-parison is meant to show that mixture decoder may succeed when the colors of objects are distinct but whenthe colors are not distinct, it suffers as shown in Figure 30. Here, we show the input image, the resulting objectattention maps and the reconstruction generated by Slot Attention and by our model. In these, we note that thecolor bias issue of mixture decoder is not that severe and both models can identify the hair slot as distinct fromthe background slot. However, we still observe that the mixture decoder produces blurry reconstructions whileour reconstructions are significantly more clear.
Figure 32: Analysis of Color Bias of Mixture-based Models in Textured-MNIST. We analyze the objectattention maps of the mixture-based Slot Attention and those generated by our model. Here, we show the inputimage, the reconstruction and the resulting object attention maps produced by Slot Attention and by our model.
Figure 33: Analysis of Object Attention Maps in Ablation Models. We ablate our model by replacing thediscrete token input (VQ) to the slot attention encoder with a CNN feature map as originally used by Locatelloet al. (2020). We also ablate our model by replacing the Transformer decoder with the mixture decoder. Theseattention maps suggest that using DVAE for obtaining a discrete representation of the input image can besynergistic with Transformer decoder for achieving good object discovery.
Figure 34: Building a Position-based Concept Library. We consider a set of mask regions derived from aG × G grid. For a given slot, we compute the IOU of the attention region of the slot with each region in thelibrary and find the highest IOU match. We then assign this slot to this match region.
