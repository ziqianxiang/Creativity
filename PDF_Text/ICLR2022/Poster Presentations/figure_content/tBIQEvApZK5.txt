Figure 1: Feature Kernel Distillation (FKD) from the feature ex-TStractor of a teacher hT to that of a student hS .
Figure 2: CIFAR10 test prediction confusion matrices between a fixedreference model and a model with: (left) retrained last layer, and (right)independent initialisation.
Figure 3: Histogram of normalised fea-ture kernel values,——k(X,x) ,、, over, maxx0 k(x0 ,x0) ,the CIFAR-100 test set.
Figure 4: FKD as teacher ensemblesize changes. Error bars denote 95%confidence for mean of 10 runs.
Figure 5: Comparison between ReLU & ReLU, for % = 0.2.
Figure 6: Comparison of (normalised) squared differences in kx,x0 = k(x, x0) between student S & teacherT, across a minibatch of size 64 of CIFAR-100 training data, for SP (left) and FKD (right). We see that whereasFKD has zero diagonal differences, SP is largely dominated by non-zero diagonal differences. Note there is aslight abuse of notation here, in that we plot squared differences in normalised kernels, so that FKD uses thecorrelation kernel and SP uses row-normalisation (Tung & Mori, 2019).
Figure 7:values between FKD with & without Feature Regularisation(FR), across different Teacher→Student architectures, on CIFAR-100 test set. We see, like in Fig. 3 that FRencourages a more even distribution of k(x, x) across x, for all architectures.
Figure 8: Comparison of normalised k(x, x) values between FKD with & without Feature Regularisation(FR), across different Teacher→Student architectures, on CIFAR-100 test set. We see, like in Fig. 3 that FRencourages a more even distribution of k(x, x) across x, for all architectures.
