Figure 1: Overview of our KID decoding algorithm. For a given context xcontext, we first retrieve kmost relevant Wikipedia documents z[1,...,k] with a knowledge retriever (Step 1), and then convertthem into compressed knowledge trie Gext (Step 2). Meanwhile, the local memory Gloc which is afirst-in-first-out list will keep track of entities in current context, and in the final step (Step 3), it willcontinuously query the knowledge trie with max number of hops hmax. Current step LM decodingwill be guided by the query results with policy gradient, to generate new token yi .
Figure 2: Impact of hyperparameters on KID’s ELI5 performance when (a) more documents areretrieved, and (b) more hops taken when querying the knowledge trie. (c) Average human ratings ondifferent-length sequences generated by KID, sampling, beam search, and reflective decoding. KIDgeneration has more stable quality across lengths by restraining exposure bias.
Figure A1: A sample of KID’s retrieved documents for the ELI5 question “Does marijuana impairdriving ability?” and its corresponding knowledge trie. (a) The top three relevant documents retrievedby DPR. We annotate the triplets end nodes (subj and obj) picked by OpenIE for knowledge trieconstruction in green. (b) The partially observed knowledge trie when we query “driving” in thecurrent local knowledge memory (blue) by wmax hops. We perform co-reference resolution to replacepronouns with actual entities in the documents, and use the stems of the tokens as the query wordsand keys in the external knowledge trie. The retrieved demonstrations (values in the trie; purple) inmultiple hops will then serve as guidance for current step decoding, after split into single tokens.
