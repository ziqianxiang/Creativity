Figure 1: Illustration of Pix2Seq framework for object detection. The neural net perceives an imageand generates a sequence of tokens that correspond to bounding boxes and class labels.
Figure 2: Major components of the Pix2Seq learning framework.
Figure 3: Applying the proposed discritization of bounding box on an image of 480 X 640. Only aquarter of the image is shown for better clarity. With a small number of bins, such as 500 bins (7pixel/bin), it achieves high precision even for small objects.
Figure 4: Examples of sequence construction with nbins = 1000, and 0 is EOS token.
Figure 5: Illustration of language modeling with / without sequence augmentation. With sequenceaugmentation, input tokens are constructed to include both real objects (blue) and synthetic noiseobjects (orange). For the noise objects, the model is trained to identify them as the “noise” class, andwe set the loss weight of “n/a” tokens (corresponding to coordinates of noise objects) to zero sincewe do not want the model to mimic them.
Figure 6: Illustrations of randomly sampled noise objects (in white), vs. ground-truth objects (inred).
Figure 7: Ablations on sequence construction. (a) Quantization bins vs. performance. (b) and (C)show AP and AR@100 for different object ordering strategies.
Figure 8: Impact of sequence augmentationon when training from scratch on COCO.
Figure 9: Decoder’s cross attention to visual feature map when predicting the first 5 objects. (b)we reshape a prediction sequence of 25 into a 5x5 grid, so each row represents a prediction for 5tokens [ymin, xmin, ymax, xmax, c]. The attention is diverse when selecting the first token of the object,then quickly concentrates on the object. (c) Overlay of the cross attention (when predicting the classtoken) on the original image.
Figure 10: Varying parameter p in nucleus sampling during inference results in different AP and AR.
Figure 11:	(a) Cosine similarity among embeddings of coordinate tokens. (b) is part of (a) coveringonly the first 100 tokens. (c), (d) and (e) are the 500-th, 1000-th and 1500-th rows of (a), respectively.
Figure 12:	Each grid is a visualization of decoder’s attention after reading a small sequence ofcoordinates, i.e., [ymin, xmin, ymax, xmax]. Visualization is done for grids of different sizes. The networklearns to pay attention to pointed region at different scales.
Figure 13: Visualization of Transformer decoder’s cross attention (when predicting class tokens)conditioned on the given bounding boxes.
Figure 14: Examples of the model’s predictions (at the score threshold of 0.5). Original imagesaccessed by clicking the images in supported PDF readers.
