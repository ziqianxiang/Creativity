Figure 1: Dirac-GAN: Generator parameters whiletraining using simultaneous and alternating gradientdescent-ascent (left), and our framework (right) with& without optimizing through the discriminator. Un-der our framework, training is stable and converges tocorrect distribution. Further, differentiating throughthe discriminator results in faster convergence.
Figure 2:	Adversarial training:∣∣Vf(θ, A(θ))k as a function of numberof steps T taken by gradient ascent (GA)algorithm A evaluated at multiple pointsin the training procedure. The plot showsthat, in practice, the Lipschitz parameter ofGA does not grow exponentially in T .
Figure 3: Mixture of Gaussians: Generated distribution at various steps during course of training.
Figure 4: Adversarial training: Test accuracy during course of training where the attack used duringtraining is gradient ascent (GA) with learning rate (LR) of 4 and number of steps (Steps) of 10 butevaluated against attacks with different Steps and LR. These plots show that training with a singleattack gives more robustness to even other attacks with different parameters or algorithms of similarcomputational power. This empirical insight is complementary to our theoretical results that extend tothe setting of training against multiple smooth algorithms. Finally, using total gradient Vf (θ, A(θ))yields better robustness compared to using partial gradient Vθf (θ, A(θ)).
Figure 5: Dirac-GAN: Generator parameters while training using our framework with and withoutoptimizing through the discriminator where between each generator update the discriminator samplesan initial parameter choice uniformly at random from the interval [-0.5, 1] and then performsT = 100 (Figure 5b) and T = 1000 (Figure 5c) steps of gradient ascent.
Figure 6: Mixture of Gaussians: Figure 6a shows the real data distribution and Figures 6b-6k showthe final generated distributions after 150k generator updates from 10 separate runs of the trainingprocedure described in Section 6 using gradient ascent for the discriminator. Each run recovers agenerator distribution closely resembling the underlying data distribution.
Figure 7: Mixture of Gaussians: Figure 7a shows the real data distribution and Figures 7b-7h showthe final generated distributions after 150k generator updates from the 7 out of 10 separate runsof the training procedure using Adam optimization for the discriminator that produced reasonabledistributions.
Figure 8: Adversarial Training: Test accuracy during the course of training against gradient ascentattacks with a fixed learning rate of η2 = 4 and the number of steps T ∈ {5, 10, 20, 40}.
Figure 9: Adversarial Training: Test accuracy during the course of training against gradient ascentattacks with a fixed attack budget of Tη2 = 40 where T is the number of attack steps and η2 is thelearning rate (LR).
Figure 10: Adversarial Training: Test accuracy during the course of training against Adam opti-mization attacks with a fixed attack budget of Tη2 = 0.04 where T is the number of attack steps andη2 is the learning rate (LR).
Figure 11: Adversarial Training: ∣∣Vf (θ, A(θ))k as a function of the number of steps T taken bythe gradient ascent algorithm A evaluated at multiple points in the training procedure. Figure 11acorresponds to using η2 = 4 in the gradient ascent procedure and Figure 11b corresponds to usingη2 = 1 in the gradient ascent procedure.
