Figure 1: Illustration of the main idea in CCL-K, best viewed in color. Suppose we select color as theconditioning variable and we want to sample red data points. Left figure: The traditional conditional samplingprocedure only samples red points (i.e., the points in the circle). Right figure: The proposed CCL-K samples alldata points (i.e., the sampled set expands from the inner circle to the outer circle) with a weighting scheme basedon the similarities between conditioning variables’ values. The higher the weight, the higher probability of of adata point being sampled. For example, CCL-K can sample orange data with a high probability, because orangeresembles to red. In this illustration, the weight ranges from 0 to 1 with white as 0 and black as 1.
Figure 2: (KZ + λI)-1 KZ v.s. KZ. We apply min-max normalization (X - min(x))/ (max(x) - min(x))for both matrices for better visualization. We see that (KZ + λI)-1KZ can be seen as a smoothed version ofKZ, which suggests that each entry in (KZ + λI)-1KZ represents the similarities between zs.
Figure 3: Illustration of the problem of insufficient samples in conditional contrastive learning. When theaverage number of samples per outcome (cluster) of the conditional variable is small (towards the left of thex-axis in the figure), the previous conditional contrastive learning framework WeaklySupInfoNCE (blue) suffers,while the proposed WeaklySupCCLK (black) outperforms WeaklySupInfoNCE significantly and is very stableregardless of whether the samples are sufficient (towards the right of the x-axis) or insufficient (towards the leftof the x-axis).
Figure 4: Creation of ColorMNIST for experiments on Fair-InfoNCE validation.
