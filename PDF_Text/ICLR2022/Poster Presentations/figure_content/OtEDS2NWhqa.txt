Figure 1: An extract from a raw transcript.
Figure 2: For one of layers of SGNN model k, the upper figure showing an output is a schemagenerated for the word node “hopeless”. This schema can be treated as an “inner record” of thenode “hopeless”, which is formed as Uhkopeless ∈ n × dk, where the generated dk-dimensionalrepresentation of a node presents as a row. The blank rows correspond to word nodes that have notbeen encountered with this identifier node “hopeless” in the current text context. The figure on thebottom shows an example of a modified schema of the same word node “hopeless” after learning.
Figure 3: A histogram showing the dis-tribution of PHQ scores on the dataset(of 189 subjects). The X-axis representsa PHQ score ranging from 0 to 24. TheY-axis represents the number of subjectsfor each score.
Figure 4: Results on the development set for our graph-based PHQ prediction system, with the true PHQ plottedas a function of predicted PHQ. The red color representsthe performance of applying a generic GNN (Gilmeret al., 2017) algorithm, whereas the blue color representsthe performance of implementing our SGNN. The greenvertical line, as a cut-off, emphasizes a better generaliza-tion performance of a small group having scores higherthan 13 (on the right of the figure).
Figure 5: A word cloud depicting words from a transcript on the development set before and afterapplying SGNN model. The two word clouds on the left-hand column depict the most salient wordsbased on the frequency of their occurrences over two different raw transcripts respectively. Theword clouds on the right-hand column illustrate the most focused content selected by SGNN model.
