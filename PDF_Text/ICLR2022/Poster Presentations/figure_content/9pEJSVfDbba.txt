Figure 1: Didactic diagram of a hypothetical embedded-model architecture with model-free maskedautoregressive flow (MAF) layers and two gated layers corresponding to two different probabilisticmodels.
Figure 2: Diagram of a gated prior-embedding architecture with a sample of transformed variablesfor a Lorentz dynamics smoothing problem.
Figure 3: Comparison of densities learned by different models on the 8 Gaussians dataset.
Figure 5: Sampled MNIST digits for different models.
Figure 6: Surrogate posterior for Lorenz Smoothing.
Figure 7: Surrogate posterior for Lorenz Bridge.
Figure 8: Surrogate posterior for Van der Pol Smoothing.
Figure 9: Surrogate posterior for Van der Pol Bridge.
Figure 10: Training losses on 2D toy data. Each plotted value is the average loss of 100 iterations.
Figure 11: Training losses on MNIST. Some models trained for a smaller amount of epochs as theywould overfit quicker on the validation set.
Figure 12: Training losses on time series toy data. Each plotted value is the average loss of 100iterations.
Figure 13: Training losses on Iris (left) and Digits (right) datasets. Each plotted value is the averageloss of 100 iterations.
Figure 14: Samples for Brownian motion.
Figure 15: Samples for Ornstein-Uhlenbeck process.
Figure 16: Samples for Lorenz system.
Figure 17: Samples for Van der Pol oscillator.
