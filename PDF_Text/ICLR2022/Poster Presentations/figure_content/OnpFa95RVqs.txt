Figure 1: Number of architecturesused for training the GIN surrogatemodel vs MAE on the NAS-Bench-101 dataset.
Figure 2: Anytime performance of different optimizers on the real benchmark (left) and the surrogatebenchmark (GIN (middle) and XGB (right)) when training ensembles on data collected from alloptimizers. Trajectories on the surrogate benchmark are averaged over 5 optimizer runs and thestandard deviation is depicted.
Figure 3: Anytime performance of different optimizers on the real benchmark (left) and the surrogatebenchmark (GIN (middle) and XGB (right)) when training ensembles only on data collected byrandom search. Trajectories on the surrogate benchmark are averaged over 5 optimizer runs.
Figure 4: Number of architecturesused for training the XGB surrogatemodel vs. MAE and rank correla-tion on the FBNet search space.
Figure 5: Anytime performance of different optimizers on the real benchmark and the XGB surrogatebenchmark when training ensembles on data collected only from random search. Trajectories on thesurrogate benchmark are averaged over 5 optimizer runs and the standard error is depicted.
Figure 7: Empirical Cumulative DensityFunction (ECDF) plot comparing all optimiz-ers. Optimizers which cover good regions ofthe search space feature higher values in thelow validation error region.
Figure 8: Standard deviation of the val. accu-racy for multiple architecture evaluations.
Figure 9: Distribution of the validation error for Figure 10: Comparison between the normal anddifferent cell depth.	reduction cell depth for the architectures found byeach optimizer.
Figure 11: t-SNE projection colored by the depth Figure 12: t-SNE visualization of the sampledof the normal cell.	architectures ranked by validation accuracy.
Figure 14: Distribution of validation error in depen-dence of the number of parameter-free operationsin the reduction cell. Violin plots are cut off atthe respective observed minimum and maximumvalue.
Figure 13: Visualization of the exploration of different parts of the architectural t-SNE embeddingspace for all optimizers used for data collection. The architecture ranking by validation accuracy(lower is better) is global over the entire data collection of all optimizers.
Figure 15: Architecture with inputs in green,intermediate nodes in blue and outputs in red.
Figure 16: Anytime performance of blackbox optimizers, comparing performance achieved on thereal benchmark and on surrogate benchmarks built with GIN and XGB in an LOOO fashion.
Figure 17: Comparison between GIN, XGB andLGB in the cell topology analysis.
Figure 18: Scatter plots of the predicted performance against the true performance of differentsurrogate models on the test set in a Leave-One-Optimizer-Out setting.
Figure 19: (continued) Scatter plots of the predicted performance against the true performance ofdifferent surrogate models on the test set in a Leave-One-Optimizer-Out setting.
Figure 20:	Ground truth (GT) and surrogate trajeCtories on a Constrained searCh spaCe where thesurrogates are trained with all data, leaving out the trajeCtories under Consideration (LOTO), andleaving out all DARTS arChiteCtures (LOOO).
Figure 21:	(Left) Distribution of validation error in dependenCe of the number of parameter-freeoperations in the normal Cell on the Surr-NAS-BenCh-DARTS dataset. (Middle and Right) PrediCtionsof the GIN and XGB surrogate model. The ColleCted groundtruth data is shown as sCatter plot. Violinplots are Cut off at the respeCtive observed minimum and maximum value.
Figure 22:	Anytime performanCe of one-shot optimizers, Comparing performanCe aChieved on thereal benChmark and on surrogate benChmarks built with GIN and XGB in a LOOO fashion.
Figure 23: Scatter plot of GIN predictions on archi-tectures that achieved below 92% validation accuracy.
Figure 24: Scatter plots of the predicted performance against the true performance of the GNNGIN/XGB surrogate models trained with different ratios of training data. "RS" indicates that thetraining set only includes architectures from random search, "mixed" indicates the training setincludes architectures from all optimizers. Training set sizes are identical for the two cases. The testset contains architectures from all optimizers. For better display, we show 1000 randomly sampledarchitectures (blue) and 1000 architectures sampled from the top 1000 architectures (orange). Foreach case we also show the R2 and Kendall-τ coefficients on the whole test set.
Figure 25: Comparison between the observed true trajectory of BANANAS and RS with the surrogatebenchmarks only trained on well performing regions of the spaceO7P ① Pe① uo+3ep = e>+JS ① gFigure 26: Anytime performance of different optimizers on the real benchmark (left) and the surrogatebenchmark (GIN (middle) and XGB (right)) when training ensembles on 47.3% of the data collectedfrom all optimizers. Trajectories on the surrogate benchmark are averaged over 5 optimizer runs.
Figure 26: Anytime performance of different optimizers on the real benchmark (left) and the surrogatebenchmark (GIN (middle) and XGB (right)) when training ensembles on 47.3% of the data collectedfrom all optimizers. Trajectories on the surrogate benchmark are averaged over 5 optimizer runs.
Figure 27: Anytime performance of RE and RS on the tabular NAS-BenCh-101 benchmark (left) andon the surrogate benchmark version of it using the GIN model (right).
Figure 28: Scatter plot showing the predictedvalidation error by the XGBoost model of theconfigurations in the incumbent trajectory ofone RE run (1000 function evaluations) onSNB-FBNet (x axis) vs. the true validationerror values of the same configurations whenretrained from scratch (y axis).
