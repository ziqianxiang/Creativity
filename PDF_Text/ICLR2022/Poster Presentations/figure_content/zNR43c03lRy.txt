Figure 1: Illustration of learning to annotate with gradient matching. The procedure is as follows. (i)The gradients of segmentation network Sθ on labeled examples are computed, denoted as VθLi (θ).
Figure 2: Benchmark on CelebA (Left), Pascal-Horse (Middle), and Pascal-Aeroplane (Right). They-axes are FG-mIoU (%) on test set.
Figure 3: Cross-domain annotator learning demo. (a) Out-of-domain labelled examples: carimages and ground truth segmentations rendered from 3D CAD model. (b) Generated images withlearned annotations: our method learns a reasonable annotator for GAN generated images.
Figure 4: Ablation study w.r.t. K. Evolution of the learned annotator on Face-34 (first row) andCat-16 (second row). (a) Annotator performance (mIoU (%)) on training data w.r.t. number ofupdating annotator steps (K). (b) Examples of generated images. (c) Evolution of automatic labels.
