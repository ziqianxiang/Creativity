Figure 1: SSL representations should be encouraged to be either insensitive or sensitive to transfor-mations. The baseline is SimCLR with random resized cropping only. Each transformation on thehorizontal axis is combined with random resized cropping. The dataset is CIFAR-10 and the kNNaccuracy is on the test set. More experimental details can be found in Section 4.
Figure 2: E-SSL framework. Left: framework. Right: methods. Egomotion, Context, Colorizationand Jigsaw use other transformations than rotations, but their patterns looks like that of RotNet’s.
Figure 3: Sketch of E-SSL with four-fold rotations prediction, resulting in a backbone that is sensi-tive to rotations and insensitive to flips and blurring. ImageNet example n01534433:169.
Figure 4: Reducing the labels for training and the data augmentation for pre-training on CIFAR-10.
Figure 5: PhC datasets with transformations for sensitivity. The regression task is to predict theDOS labels (an example of a label in R400 is shown on the right) from 2D square periodic unit cells(examples of the inputs in R32×32 are shown on the left). We consider two types of input unit cells;at the top is the Blob dataset where the feature variation is always centered; at the bottom is theGroup pm (Gpm) dataset where inputs have a horizontal mirror symmetry.
Figure 6: Demonstration of the evolution of the invariance (top) and equivariance (bottom) measuresduring training. Left is E-SimCLR and right is E-SimSiam.
Figure 7: E-SimCLR gives sizable improvements for the Flowers-102 SSL pre-training. kNN accu-racy (%) is on the validation set.
Figure 8: The Flowers-102 is not completely invariant to rotation. The top row shows data pointswhich are roughly invariant to four-fold translations. The bottom row shows counterexamples tothat hypothesis.
