Figure 1: The difference between denoising and unrolled denoising. Original text (top) is randomlycorrupted, producing a text (middle) where some tokens are original (green) and others are corrupted(red). This text is denoised by sampling from the generative model to produce another noisy text(bottom). While standard denoising autoencoders only learn a mapping from the middle text tothe top text, Step-unrolled Denoising Autoencoder learns a mapping from bottom to top (includingmiddle as a special case of zero unroll steps). This has an intuitive meaning: during generation time,the network will mostly (right after the first step) encounter texts like the bottom one, not like themiddle one — so unrolls prepare the model during training for inputs it will get at generation time.
Figure 2: BLEU scores on EMNLP2017 News.
Figure 3: Overview of target length prediction. During SUNDAE training, we simultaneously trainthe length predictor with cross-entropy loss and give ground-truth target length as input to the de-coder (green dashed arrow), thus teacher-forcing it. During sampling, we give the most likely targetlength prediction from the network to the decoder (red dashed arrow).
Figure 4: Validation BLEU curves during training with and without target length prediction onWMT’14 EN→DE, DE→EN and EN→FR tasks shown in Figure 4a, 4b and 4c, respectively. Fig-ure 4d shows the effect of unrolled denoising terms on translation quality, here L(k) = L(1:k). Allmodels are trained with a batch size of 256 using 20 different random seeds to display uncertainty.
Figure 5: Model score (left) and BLEU (right) evolution throughout sampling for various tempera-tures. The reference score on the left hand side denotes the model score obtained for ground truthinputs. Overall, medium-low temperatures give the best results.
