Figure 1: Difference in human-normalized score for 55 Atari 2600 games, LogLinDQN versusthe worst (top) and best (bottom) of LogDQN and (Lin)DQN. Positive % means LogLinDQNoutperforms the per-game respective baseline.
Figure 2: Human-normalized mean (left) and median (right) scores across 55 Atari 2600 games.
Figure 3: The lines go, g1, and g? for all the different possibilities of fj. Cases A-D correspond toa ≤ b, and cases E-H are the same functions but for b ≤ a. We use the notations w0 = g-1(W) andV = g-1(滴).
Figure 4: The functions Log and LogLin are illustrated. Remark the compression property of Logas compared to LogLin for returns larger than one.
Figure 5: Difference in human-normalized score for 55 Atari 2600 games, LogLinDQN versusLogDQN (top) and (Lin)DQN (bottom). Positive % means LogLinDQN outperforms the respectivebaseline.
Figure 6:	Learning curves in all 55 Atari 2600 games for (Lin)DQN, C51, Rainbow, LogDQN,and LogLinDQN. Shaded regions indicate standard deviation.
Figure 7:	(Part 1/3) Reward density in the Atari suite. The rewards of each game are measured withthree behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedypolicy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments areaveraged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps witha moving average window of 10 steps.
Figure 8:	(Part 2/3) Reward density in the Atari suite. The rewards of each game are measured withthree behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedypolicy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments areaveraged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps witha moving average window of 10 steps.
Figure 9:	(Part 3/3) Reward density in the Atari suite. The rewards of each game are measured withthree behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedypolicy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments areaveraged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps witha moving average window of 10 steps.
