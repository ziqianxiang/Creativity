Figure 1: Zero-shot transfer performance of mBERT on MARC vs. conditional feature shift (upper-left; with averages marked by language), class prior shift (lower-left), and both (right). Each scatterrepresents a result evaluated on one subsampled target dataset.
Figure 2: Transfer performance of mBERT on MARC under two learning settings vs. conditionalshift of intermediate representations. mBERT is fine-tuned with the classification head added ontop of [CLS] embedding, but we plot the shifts of both [CLS] (upper panels) and mean-pooledembeddings (lower panels). This is because [CLS] embeddings are not informative until they arecontextualized in upper layers. Each curve represents a result evaluated on one subsampled targetdataset.
Figure 3: Percent improvement in mBERT transfer performance (macro-averaged F1) over zero-shotbaselines (0%) when applying unsupervised methods under presence of class prior shifts, evaluatedon datasets subsampled from MARC and CoNLL with varying priors.
Figure 4: Zero-shot transfer performance of mBERT on WikiANN vs. conditional shift of intermedi-ate representations (lower-left), final-layer features (upper-left; with averages marked by language),and jointly with class prior shift (right).
Figure 5: Zero-shot transfer performance of mBERT on XNLI vs. conditional shift of intermediaterepresentations (lower-left), and final-layer features (upper-left; with averages marked by language).
Figure 6: Zero-shot transfer performance of mBERT on two datasets vs. conditional feature shiftjointly with class prior shift. Models are trained on source data with skewed prior distributions.
Figure 7: mBERT transfer performance onCoNLL from English to German vs. updatesteps. Comparing IWDA to without IW.
Figure 8: Zero-shot transfer performance of XLM-R on MARC vs. conditional shift of intermediaterepresentations (lower-left), final-layer features (upper-left; with averages marked by language), andjointly with class prior shift (right). Each curve and scatter represents a result on one subsampleddataset, and the [CLS] embeddings are examined.
Figure 9: Percent improvement in XLM-R transfer performance (macro-averaged F1) over zero-shotbaselines (0%) when applying unsupervised methods under presence of class prior shifts, evaluatedon datasets subsampled from MARC and CoNLL with varying priors.
