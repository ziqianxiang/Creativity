Figure 1: Comparison of DETR, Conditional DETR, and our proposed DAB-DETR. For clarity, Weonly show the cross-attention part in the Transformer decoder. (a) DETR uses the learnable queriesfor all the layers Without any adaptation, Which accounts for its sloW training convergence. (b) Con-ditional DETR adapts the learnable queries for each layer mainly to provide a better reference querypoint to pool features from the image feature map. In contrast, (c) DAB-DETR directly uses dynam-ically updated anchor boxes to provide both a reference query point (x, y) and a reference anchorsize (w, h) to improve the cross-attention computation. We marked the modules With difference inpurple.
Figure 2: Comparison of self-attention in encoders and cross-attention in decoders of DETR. As theyhave the same key and value components, the only difference comes from the queries. Each queryin an encoder is composed of an image feature (content information) and a positional embedding(positional information), whereas each query in a decoder is composed of a decoder embedding(content information) and a learnable query (postional information). The differences between twomodules are marked in purple.
Figure 3: a): Training curves of the original DETR and DETR with fixed queries. b): Trainingcurves of the original DETR and DETR+DAB. We run each experiment 3 times and plot the meanvalue and the 95% confidence interval of each item.
Figure 4: We visualize the positional attention between positional queries and positional keys forDETR, Conditional DETR, and our proposed DAB-DETR. Four attention maps in (a) are randomlysampled, and we select figures with similar query positions as in (a) for (b) and (c). The darkerthe color, the greater the attention weight, and vice versa. (a) Each attention map in DETR iscalculated by performing dot product between a learned query and positional embeddings from afeature map, and can have multiple modes and unconcentrated attentions. (b) The positional queriesin Conditional DETR are encoded in the same way as the image positional embeddings, resultingin Gaussian-like attention maps. However, it cannot adapt to objects of different scales. (c) DAB-DETR explicitly modulates the attention map using the width and height information of an anchor,making it more adaptive to object size and shape. The modulated attentions can be regarded ashelping perform soft ROI pooling.
Figure 5: Framework of our proposed DAB-DETR.
Figure 6: Positional attention maps modu-lated by width and height.
Figure 7: Positional attention maps with dif-ferent temperatures.
Figure 8: Comparison of DETR-like models. For clarity, we only show two layers of Transformerdecoder and omit the FFN blocks. We mark the modules with difference in purple and marked thelearned high-dimensional queries in brown. DAB-DETR (c) is proposed in our paper, and DAB-Deformable-DETR (f) is a variant of Deformable DETR modified by introducing our dynamic an-chors boxes. All previous models (a,b,d,e) leverage high-dimensional queries (shaded in brown) topass positional information to each layers, which are semantic ambiguous and are not updated layerby layer. In contrast, DAB-DETR (c) directly uses dynamically updated anchor boxes to provideboth a reference query point (x, y) and a reference anchor size (w, h) to improve the cross-attentioncomputation. DAB-Deformable-DETR (f) uses dynamically updated anchor boxes to formulate itsqueries as well.
Figure 9: Comparison of the training of Deformable DETR and DAB-Deformable-DETR models.
Figure 10: Learned anchor points when learning 2D coordinates only (left), and anchor center points(middle) and partial anchor boxes (right) when learning anchor boxes directly.
Figure 11: We compare the layer-by-layer update of boxes of DAB-DETR (a) and ConditionalDETR (b). The green boxes are ground truth annotations while the red boxes are model predictions.
Figure 12: We visualize some images where our model does not predict well, including dense ob-jects (a,b,c), very small objects (d), and very large objects (e,f). The green boxes are ground truthannotations while red boxes are predictions of models.
Figure 13: Convergence curves of DETR, Conditional DETR, and our DAB-DETR. All models aretrained under the R50 (DC5) setting.
Figure 14: Visualizations for layer-by-layer anchor box update. We plot the initial anchor boxes(left), anchor boxes after the first decoder layer (middle), and the output of the last decoder layer(right), respectively. The green boxes are ground truth annotations, while the red boxes are predic-tions of our model. The results are obtained using the ResNet-50 backbone. More visualizations areavailable in Fig. 15.
Figure 15: More visualizations for layer-by-layer anchor box update. We plot the initial anchorboxes (left), anchor boxes after the first decoder layer (middle), and the output of the last decoderlayer (right), respectively. The green boxes are ground truth annotations, while the red boxes arepredictions of our model. The results are obtained using the ResNet-50 backbone.
