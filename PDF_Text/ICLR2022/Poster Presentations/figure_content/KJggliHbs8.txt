Figure 1: Left: Illustration of the standard GNN pipeline and our GIANT framework. Note thatonly the language model (i.e., the Transformers) in GIANT can use the correlation between graphtopology and raw text. Nevertheless, GIANT can work with other types of input data formats, suchas images and audio; the study of these models is deferred to future work. Right: Illustration of theconnection between our neighborhood prediction and the XMC problem. We use graph informa-tion to self-supervise fine-tuning of the text encoder Î¦ (i.e., the Transformer) in our neighborhoodprediction problem. The resulting fine-tuned text encoder is then used to generate numerical nodefeatures XGIANT for use in downstream tasks (see also Figure 2 and the description in Section 3).
Figure 2: Illustration of the use of XR-Transformers. Step 1: Perform semantic hierarchical cluster-ing of target labels (neighborhoods) to build a tree. Step 2: At each intermediate (internal node) levelof the tree, fine-tune the Transformers for the XMC sub-problem that maps raw text of nodes to labelclusters. Note that the results of higher levels are used to guide the Transformers at lower levels andhence improve their performance. The resulting Transformers are used as encoders that generatenumerical node features from raw texts. Note that We can change the encoder (e.g., Transformer) toaddress other raw data formats such as images or audio signals.
Figure 3: A counter-examplefor standard link predictionmethodology.
Figure 4: Illustration ofa cSBM:Node features are independentGaussian random vectors whileedges are modeled as indepen-dent Bernoulli random variables.
Figure 5: To further demonstrate that our GIANT-XRT indeed achieves a significant improvementover state-of-the-art methods, we plot the performance of top 8 models on OGB leaderboard (As ofNov. 11th, 2021). Note that our results on ogbn-products is better than those reported in Table 2,since we adopt the latest choice of hyperparameters provided in SAGN GitHub repository.
