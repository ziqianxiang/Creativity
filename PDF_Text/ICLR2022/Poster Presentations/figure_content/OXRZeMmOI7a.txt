Figure 1: The ordering of states used for updating Q-values directly effects the convergence speed.
Figure 2:  (a) The illustration of NChain environment.  Each node denotes a state, and the arrowrepresents a valid transition between two nodes.  (b) TER achieves higher performance with fewerupdates and reduces the value error of the Q-function faster than the baselines.
Figure 3: TER outperforms the baselines in (a) Minigrid and (b) Sokoban, demonstrating that TERcan work with high-dimensional state spaces.  In (b), the plots are ordered by task difficulty (i.e.,reward sparsity) from left to right.  The performance gain of TER is more salient when rewards aremore sparse. The x-axis is the environment steps. y-axis is the normalized mean return.
Figure 4: (a) Increasing the replay ratio does not consistently improve the performance of the base-lines. (b) Increasing replay ratios leads to Q-value overestimation in baselines.
Figure 5: The learning curves of TER with different batch mixing ratios.  In Sokoban the choice ofmixing ratio is more sensitive than that in Minigrid. TER with mixing ratio 0.1 and 0.2 perform wellin both Minigrid and Sokoban.
Figure 6:  The only difference between TER(singlepred, η  = 0) and EBU (β  = 0) is trajectory stitch-ing while TER(single pred, η = 0) still outperformsEBU (β  =  0).  This shows that trajectory stitchingresults is critical for TER’s performance gain.
