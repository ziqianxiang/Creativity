Figure 1: Top row: performance on the synthetic task. The relative error is defined as a ratiobetween current and initial errors (L(xk) - L?)/(L(x0) - L?). The complexity C() as defined in(8). Bottom row: performance on the hyper-parameter optimization task.
Figure 2: Evolution of the relative error vs. time in seconds for different AID based methods onthe synthetic example. Each column corresponds to a method (AID-CG, AmIGO-CG, AmIGO-GD,AID-N, AID-FP) and each row corresponds to a choice of the conditioning number κg . For eachmethod we consider T and N from a grid {1, 10, 102, 103} × {1, 10, 102, 103}. Lightest colorscorresponds to smaller values of N while nuances within each color correspond to increasing valuesof T.
Figure 3: Evolution of the relative error vs. time in seconds for different ITD based methods on thesynthetic example. From the left to the right, the first two columns correspond to Reverse and ITDmethod small conditioning numbers κg ∈ {1, 10, 103}, last two column are for higher conditioningnumbers κg ∈ {104, 105, 107}. For each method we consider T ∈ {1, 10, 102, 103}. Lightest colorscorrespond to smaller values of T .
Figure 4: Evolution of the validation loss (left column), validation accuracy (middle column) andtest accuracy (right column) in time (s) for different methods on the logistic regression task. Eachrow correspond to different choices for the size of the batch |D| ∈ {100, 1000, 2000, 4000} chosento be the same for all gradient, Hessian and Jacobian-vector products evaluations. Time is reportedin seconds.
Figure 5: Performance of various bi-level algorithms on the dataset distillation task on MNISTdataset.
