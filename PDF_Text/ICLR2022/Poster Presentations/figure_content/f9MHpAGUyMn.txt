Figure 1: The visualization of token difference in magnitude when different normalizers are em-PreservDifferenceployed including LN (a), IN (b) and the proposed DTN (c). The results are obtained using a trainedViT-S with different normalizers on a randomly chose sample. The cube represent a feature of to-kens whose dimension is B X T X C, and each token is a vector with C-dimension embedding. Weexpress IN, LN and DTN by coloring different dimensions of those cubes. We use a heatmap to vi-sualize the magnitude of all the tokens, i.e., the norm of token embedding for each head. (a) showsthat LN operates within each token. Hence, it makes token magnitude have uniform magnitude re-gardless of their positions. Instead, (b) and (c) show that IN and our DTN can aggregate statisticsacross different tokens, thus preserving variation between different tokens.
Figure 2: The visualization of mean attention distance in multi-head self-attention (MHSA) moduleof a trained ViT-S with (a) LN, (b) IN, and (c) DTN. The mean attention distance denotes the averagenumber of patches between the center of attention, and the query patch Dosovitskiy et al. (2020)(see details in Appendix Sec. B.1). A large mean attention distance indicates that the head wouldattend to most image patches, presenting excellent ability of modeling long-range dependencies. (a)shows that LN can perform excellent long-range contextual modeling while failing to capture localpositional context. (b & c) show that multiple heads in ViT-S with IN and DTN have a small meanattention distance. Hence, IN and our DTN can capture local context because they preserves thevariation between tokens as shown in Fig.1(b & c).
Figure 3: (a) illustrates the normalization constants in IN. IN obtains its normalization constants byconsidering all patches from the image. Hence the semantic distribution shift from different imagepatches may lead to inaccurate estimates of normalization constants in IN. (b) shows the relativevariation between token features and the statistics (mean), which is measured by the variation co-efficient defined by the norm of |(x — μ) /x | where μ is the mean in normalization layer. A largervariation coefficient indicates that the mean is further away from the token feature. From (b), INmay lead to inaccurate estimates due to the domain difference between tokens. LN and our proposedDTN can mitigate the problem of inaccurate estimates of statistics in IN.
Figure 4:	Illustration of different types of the banded matrix when token length T = 9. WhenPh in Eqn.(5) is a matrix in (a), DTN aggregates its mean and variance by considering all tokens.
Figure 5:	Illustration of construction of horizontal and vertical in relative positional embedding R.
Figure 6: Visualization of Ph initialized by (a) truncated normal distribution and (b-e) Eqn.(7)before training. We visualize the weights corresponding to 91-th token, i.e. Ph [91, :].view(14,14).
Figure 7: Visualization of Ph initialized by (Left) truncated normal distribution and (Right) Eqn.(7)after training. We visualize the weights corresponding to 91-th token, i.e. Ph [91,:].VieW(14,l4) forall layers. Lighter is larger.
Figure 8: Training dynamics of λh for mean and variance in different layers of ViTT.
Figure 9: Visualization of token directions using PCA for LN and DTN. The projected tokens inthe first two principle component are visualized before and after LN (rows 1-2), and before andafter DTN (rows 3-4). We see PCA projected tokens after DTN are closer to each other than LN. Itimplies that DTN encourages tokens to learn a less diverse direction than LN. Results are obtainedon the first layer of trained ViT-S with LN and DTN, respectively.
