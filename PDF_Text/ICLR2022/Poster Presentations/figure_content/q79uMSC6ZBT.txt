Figure 1: A sample snippet (left; abbreviated from Fig. 12 in Appx. A). A developer has just typedthe code and their cursor (in blue) is at line 3. Code completions provided by a number of modelsare shown on the right, where L → R is a standard LMC and GRAMMFORMER is our new model.
Figure 2:		Progress of	grammar-based		code generation	of the sketch	r = x * (■-foo(args)) by GRAMMFORMER. Each line represents consecutive x(t) in Alg. 1. Terminaltokens are shown in monospace blue font. The underlined non-terminal at position i(t) is se-lected by Ps and its expansion is generated by Pe, i.e. the output underneath the selected (under-lined) non-terminal. Fig. 5 and Fig. 6 in Appx. A show real example generation sequences from ourdatasets.
Figure 3: Sketch length vs. ground-truth lengthL → R +0 is trained to be more “conservative” (i.e. avoid incorrect suggestions) but is also unableto introduce holes beyond the last generated token. Finally, we can see that while Grammformeralready performs well after our pre-training procedure, we can further improve its performance withour fine-tuning technique. We believe that this is because when Ps and Pe are trained jointly, theyco-adapt: some of the capacity of the shared encoder module that is used to make predictions forhard-to-expand non-terminals is “freed” since Ps learns to not expand them.
Figure 4: Percent Correct (i.e., matching) sketches (top-1 generated sketch) vs. ground-truth lengthTable 2: Performance for GRAMMFORMER ablations (C#), for different Ps and reward functions.
Figure 5:	An example Grammf ormer generation for C#. Each line in the generation processshows subsequent states of xt in Alg. 1. Here, GRAMMFORMER predicts a sketch that matches theground-truth expansion, but places a hole at the key of the dictionary lookup, instead of predicting alow-likelihood string literal.
Figure 6:	An example Grammformer generation for Python. Each line in the generation processshows subsequent states of xt in Alg. 1. GRAMMFORMER here predicts that the user’s intent is toread-in a second argument and store it in a variable. However, within the current context, the nameof the variable storing the second argument would be impossible to predict. Grammformer—reasonably — places a hole at the given location and generates a matching sketch. In this example,any traditional left-to-right model would need to first predict an accurate target variable name (whichseems unlikely in the given context) before predicting the right-hand side of the assignment.
Figure 7:	A C# example and completion outputs from different models. REGEXACC score re-ported in red. Here, Grammformer correctly identifies that a method should be invoked onexchangeActivity, but does not predict the concrete method. If Grammformer was ex-tended with information from a static analysis about the ExchangeActivity (potentially a user-defined type) then an accurate suggestion could have potential been made.
Figure 8:	A C# example and completion outputs from different models. RegexAcc score reportedin red. Here, Grammformer correctly predicts that an AreEqual assert statement should bemade, checking the value of buffer[50]. However, within this context, the correct concreteexpected value (0) would be hard to predict, even for a human. Grammformer places a holethere and generates a correct line-level sketch. In contrast, L → R introduces a wrong completionand L → R +0 creates a correct, but much shorter sketch.
Figure 9:	A C# example and completion outputs from different models. RegexAcc score reportedin red. While all models predict that an assignment needs to be made to each data[i], the exactform of the constructor is hard to predict. Grammformer seems to be looking at the constructordefinition and predicts that some (StringLiteral) needs to be used as the second argument, althoughit is uncertain about its concrete form, hence introducing a hole.
Figure 10:	A Python example and completion outputs from different models. RegexAcc scorereported in red. Here both L → R and GRAMMFORMER predict the full line correctly, but L →R +0 seems to return a more conservative (but correct) sketch.
Figure 11:	A Python example and completion outputs from different models. REGEXACC scorereported in red. See main text in the introduction for a description.
Figure 12:	A Python example and completion outputs from different models. REGEXACC scorereported in red. Generation steps of GRAMMFORMER shown in Fig. 6. L → R and L → R + 0cannot generate correct sketches since the first token would be impossible to guess within this codecontext.
Figure 13:	A Python example and completion outputs from different models. REGEXACC scorereported in red. GRAMMFORMER completes the line creating a correct sketch with two holes atlocations avoiding to make the mistakes that L → R and L → R +0 makes.
Figure 14:	A C# example and incorrect completion outputs from different models. RegexAccscore reported in red. The prediction from Grammformer is almost right but should have createda hole at the first argument for the user to fill-in. This shows that improved methods for training thepolicy network may improve results in the future.
Figure 15:	AC# example and completion outputs from different models. REGEXACC score reportedin red. Grammformer suggests a correct sketch but the right-hand side of the assignment hasto stop expansion since (IntegerLiteral) cannot generate int.Parse(args[2]). This suggestssome of the limitations that the grammar-based generation of Grammformer may have, especiallyfor shorter sequences.
Figure 16:	A Python example and completion outputs from different models. RegexAcc scorereported in red. Although the sketch of the prediction from GRAMMFORMER is typically correct, itis not useful. Researching better evaluation metrics may improve GRAMMFORMER.
Figure 17: A Python example and completion outputs from different models. REGEXACC scorereported in red. All model fail to invoke the correct API of the library. A potential future directionto mitigate the problem is to incorporate definitions of the external or system classes.
