Figure 1: (a) Optimal representations for IDG must have invariant supports while being simultaneouslydiscriminative on all domains: (b) without the discriminative requirement, a source-risk minimizercan mispredict the target, and (c) without support match, some risk minimizer will perform poorly.
Figure 2: Image-text augmentations are practical domain-agnostic augmentations. Arrows denoteaugmenters. Bubbles denote inputs that have the same representations, as induced by predicting theaugmentations. (a) Standard augmentations are not domain-agnostic. (b) Supervised augmentationsuniformly augment inputs inside their label class, irrespective of domains. (c) Image-text augmenta-tions are (nearly) domain-agnostic as they map images across domains to similar descriptions.
Figure 3: (a) Adding bottlenecks significantly improves the worst-case DG performance and usingdomain-agnostic (DA) augmentations (H[A | Z]) performs as well as with labels (R [Y | Z]). (b) In-creasing the domain bottleneck weight λ will improve target performance until it decreases sourceperformance. (c) DA augmentations are crucial but approx. DA aug. might be also be sufficient.
Figure 4: Sweeping the sample weight using CE-Base and SupCon-Base. We selected 10-5 whichseemed to be reasonble for both cases.
Figure 5: The worst-case DG performance of Ent bottleneck is more sensitive to λ than CADWhat’s the effect of λ for different objectives on the worst-case DG performance? In Fig. 5,the worst-case target log likelihood versus λ values for different objectives is shown. We found thatEnt is much more sensitive to the choice of λ than CAD, which was part of the reason why we usedthe latter in most of our experiments. Note that for SupCon-Ent with small λ values, it was worsethan SupCon-Base because of the discretization introduced by the Ent bottleneck, which we verifiedby observing that setting λ = 0 lead to similar results.
