Figure 1: (a) Learned simulator framework. The model is trained to predict the difference betweenthe current and next state. Gaussian noise can be added to the inputs during training to producemodels that are robust to small perturbations. Shown below is the schematic for the Dilated ResNetmodel (Dil-ResNet). (b-e) Frames predicted by rollouts from the learned simulator model comparedto ground truth frames. (b) KS-1D: The model follows the ground truth closely for the first 150steps (t < 75) and remains plausible thereafter. (c) INCOMP-2D: The model remains accurate after119 model steps (91,392 solver steps). (d) CompDecay- 3 D: The model remains accurate after31 model steps (1984 solver steps). (e) CoolMixLayer-3D. The model remains qualitativelyaccurate after 59 model steps (59,000 solver steps) of a box size of (L = 0.75). Videos available atsites.google.com/view/learned-turbulence-simulators.
Figure 2: Comparison between learned 323 Dil-ResNet and same-resolution 323 Athena++, interme-diate resolution 643 Athena++, and ground truth high resolution 1283 Athena++ in COMPDECAY-3D. (a) Energy Field RMSE (y-axis) for Dil-ResNet (323 resolution (blue)) and Athena++ (643(dark gray), and 323 (light gray)) on a test trajectory the window of times seen during training.
Figure 3:	Effects of noise and temporal downsampling on rollout stability. (a) One step errors arelarger for models trained with noise. Note the error spikes are very small and are not model-relatedgeneral artifacts, but specific to particular frames of this test trajectory. (b) However, models trainedwithout noise can yield unstable rollouts, especially when using very small time steps, which is not aproblem for models trained with noise. (c, e) One-step model error rises monotonically with coarsertemporal downsampling. (d, f) Rollout error has a U-shaped curve over temporal downsamplingfactors, for a trajectory of the same time duration, with minimum error around âˆ†t = 0.032.
Figure 4:	Comparison across learned models, contrasting noise and no-noise training conditions,across the three primary tasks (a) KS-1D, (b) Incomp-2D, and (c) CompDecay-3D. With a fewexceptions, the various learned models had comparable performance, though the Dilated ResNets(Dil-ResNet, Con-Dil-ResNet) consistently have the lowest error. In KS-1D (a), the noise harmedperformance; in Incomp-2D (b) the noise particularly benefits the FNO rollouts; in CompDecay-3D(c) the noise mainly stabilized rollouts.
Figure 5: Generalization outside of the training distribution. (a) Generalization to different initialconditions. We vary the ratio of compressive and solenoidal components in the initial velocityfield. Solid markers indicated the training region. We find that, compared to coarse Athena++,generalization to more compressive initial states is challenging for learned models and inconsistentacross seeds, but that loss constraints (Con-Dil-ResNet) ameliorate this to some extent. (b) Coolingvelocity generalization as function of the box size in the mixing layer. The black line indicates theground truth cooling velocity averaged across time for 8 test trajectories with different box sizes,with one standard deviation represented as the shaded region. None of 3 seeds of Dil-ResNet trainedon a single length of L = 0.75 (blue) generalize outside the training range. Dil-ResNet trainedon multiple lengths (orange) shows better generalization performance. However, generalization farbeyond training box sizes remains a challenge.
