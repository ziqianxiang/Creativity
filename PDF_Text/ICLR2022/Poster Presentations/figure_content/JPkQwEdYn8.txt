Figure 1: Comparison of 1D regression predictions and asscociated attention weights as specifiedby ANP (Kim et al., 2019), ANP with Information dropout, Bootstrapping ANP (Lee et al., 2020)and ours. The training data for 1D regression is fairly noisy. (a) Ours more accurately captures thecontext datasets and significantly better predicts than baselines. (b) The horizontal axis indicatesthe value of features in the context dataset, while the vertical axis indicates the value of featuresin the target dataset in these heatmaps. The best pattern for this heat-map is diagonal because allfeature values are arranged in ascending order. Among all models, ours comes closet to the ideal.
Figure 2: Probabilistic graph-ical models for the proposedmethodAs with the Attentive neural process(Kim et al., 2019), the pro-posed method consists of the two types of encoders and a singledecoder architecture. The first encoder embeds (Xc, Yc) to a global representation z and the sec-ond encoder makes a local representation ri by (xi, Xc, Yc). The global representation z serves torepresent entire context data points (Xc, Yc), whereas the local representation ri is in charge of fine-grained information between target data xi and context (Xc, Yc) for predicting the output distributionp(yi|xi, (z, ri)). Unlike the ANP, the proposed model considers all intermediate representations(z, ri ) as stochastic variables. We exploit the Bayesian attention module to create a local representa-tion ri by the stochastic attention weights wi with the key-based contextual prior (Fan et al., 2020).
