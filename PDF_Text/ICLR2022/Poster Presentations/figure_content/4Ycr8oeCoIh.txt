Figure 1: Different G/D initialization patterns: the red dots denote pretrained generator samples,the arrows denote a pretrained discriminator gradient field, the blue distribution is the target. Fromleft to right: bad discriminators will lead good initial samples out of the target distribution; badgenerators will drop some of the modes even being guided by good discriminators; both properG/D serve as an optimal initialization for transfer to a new task.
Figure 2: Impact of GAN pretraining for synthetic data. 1) source and target distributions. 2-3) GANs pretrained on two source distributions. 4-6): GANs trained on the target distribution,initialized by the two source checkpoints and randomly. Each plot also reports the Wasserstein-1distance between the generated and the target distributions.
Figure 3: Scatter plots of the pretrained generator quality (Recall) and the pretrained discriminatorquality (VD similarity) Vs the quality of finetuned GAN (W 1 -distance). Each point representsa result of GAN finetuning, which started from a particular pair of pretrained discriminator andgenerator. The color indicates the W1 -distance between the final generator distribution and thetarget distribution. The Pearson correlation of the final W1-distance is equal -0.84 for the Recall,and -0.73 for the gradient similarity.
Figure 4: Standard deviations of Precision/Recall values for each target dataset computed over dif-ferent sources. Due to the symmetric nature of the quantities, the table reports the standard deviationfor precision and recall computed over an equal number of real and generated samples (minimumbetween a dataset size and 50K)Overall, despite having poor quality (FID=49.8), the Imagenet-pretrained unconditional StyleGAN2model appears to be a superior GAN initialization that typically leads to more efficient optimizationcompared to alternatives. This result contradicts the observations in (Wang et al., 2018b) showingthat it is beneficial to transfer from dense and less diverse sources rather than sparse and diverse ones,like Imagenet. We attribute this inconsistency to the fact that (Wang et al., 2018b) experimented withthe WGAN-GP models, which are significantly inferior to the current state-of-the-art ones.
Figure 5: Left: Number of modes covered by the generator snapshots during the training processfrom three different initializations. Right: samples of the 65-th class of the Flowers dataset, which iswell-covered by the GAN trained from the Imagenet initialization and poorly covered by the GANtrained from the FFHQ initialization. Top: real images; Middle: FFHQ; Bottom: Imagenet.
Figure 6: Evolution of the generated samples with different source initializations. Left: averageLPIPS-distance between images generated by the consecutive generator snapshots for the same la-tent code. Right: images generated with the same latent code evolving during training: top row:start from FFHQ, middle row: start from Imagenet, bottom row: start from random initialization.
Figure 7: Left: distribution of samples trajectories lengths with Flowers as a target dataset. Right:generated class change probability for individual latents during the training.
Figure 8: Samples for each of the target and source datasets.
Figure 9: Learning curves for different target and sources datasets, part 1.
Figure 10: Learning curves for different target and sources datasets, part 2.
