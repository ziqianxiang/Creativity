Figure 1: Illustration of the collapsing problem. For complete collapse, the embedding vectors collapse tosame point. For dimensional collapse, the embedding vectors only span a lower dimensional space.
Figure 2: Singular value spectrum of the embeddingspace. The embedding vectors are computed from apretrained SimCLR model on the validation set of Ima-geNet. Each embedding vector has a dimension of 128.
Figure 3: Weight matrix singular value spectrumwith different augmentation amplitude k. The set-ting is a single layer linear toy model with eachweight matrix of the size of 16x16, where theblock has the size of 8x8. Strong augmentationresults in vanishing singular values in weight ma-trices.
Figure 4: Two-layer Linear Model5.2	Gradient Flow DynamicsSimilar to Lemma 1, we derive the gradient flow on the two weight matrices W1 and W2.
Figure 5: Visualization of the alignment matrixA = V2T U1 after training. The setting is a 2-layerlinear toy model with each weight matrix of theSee proof in Appendix B.6. According to Eqn. 10, size of 16x16. The alignment matrix converges to(σ1k)2 = (σ2k)2 + C. We solve the singular an identity matrix.
Figure 6: Evolution of the singular values of the weight matrices and the embedding space covariance matrix.
Figure 7: (a) Definition of representation and the embedding space; (b) Singular value spectrums of therepresentation space of pretrained contrastive learning models (pretrained with or without a projector). Therepresentation vectors are the output from the ResNet50 encoder and directly used for downstream tasks. Eachrepresentation vector has a dimension of 2048. Without a projector, SimCLR suffers from dimensional collapsein the representation space.
Figure 8: DirectCLR: no trainable projector, sim-ply apply InfoNCE loss on the a fixed sub-vectorof the representationscan be found in the Appendix D. DirectCLR demonstrates better performance compared to SimCLRwith a trainable linear projector on ImageNet. The linear probe accuracies for each model are listedin Table 1.
Figure 9: Representation space spectrum of Di-rectCLR compared to SimCLR (a) with a 2-layernonlinear projector (b) with a 1-layer linear pro-jector (c) without projector. The spectrums arecomputed based on the output from the backbone,using ImgaeNet validation set. Similar to Sim-CLR with projectors, DirectCLR is able to preventdimensional collapse in the representation space.
Figure 10: Why is the whole representation vector rmeaningful in DirectCLR while only part of it receivesgradient? It takes advantage of the residual connectionin the backbone. Thus, the gradient passing through therepresentation vector is low-rank where only the first dochannel dimensions are non-zero. When the gradiententers the ResNet backbone and passes through the lastnonlinear conv block, it becomes full rank. Therefore,this hidden layer h receives gradients on all channels.
Figure 11: Embedding space singular value spectrum with different layers on (a) linear and (b) nonlinearnetworks. All models use weight matrices with a size of 16x16. Adding more layers in the network leads tomore collapsed dimensions. Adding nonlinearity leads to a similar collapsing effect.
Figure 12: Hyperparameter tuning on d0 based on ImageNet linear probe Top-1 accuracy.
