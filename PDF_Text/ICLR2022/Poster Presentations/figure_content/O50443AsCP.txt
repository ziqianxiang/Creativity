Figure 1:	The schematic overview of our method. For the sake of brevity, the table content in theinput is simplified with the symbol [Table].
Figure 2:	The illustration of the fine-tuning procedure in our method. During fine-tuning, we feedthe concatenation of an NL sentence and its corresponding table taken from the downstream task tothe model, and train it to output the answer (e.g., “Marisela Moreno Montero”).
Figure 3:	The illustration of the pre-training procedure in our method. During pre-training, we feedthe concatenation of a sampled SQL query and a sampled table to the model, and train it to outputthe corresponding execution result (e.g., “Pairs”).
Figure 4: The visualization results of attention weights from other tokens to the cell “adrian lewis”.
Figure 6: The amount of pre-training cor-pus vs. denotation accuracy on WikiTable-Questions dev set. TaPEx surpasses exist-ing table pre-training approaches with a muchsmaller corpus, showing its high efficiency.
Figure 5: The illustration of downstream tasks per-formance with different scales of pre-training corpus.
Figure 7:	The performance of downstream tasks (dev sets) at different pre-training difficulties withthe same amount of examples (0.5 Million). ≤ Medium means that we only use SQL query tem-plates with a difficulty level less than or equal to Medium when synthesizing its pre-training corpus.
Figure 8:	The fine-grained performance of different SQL difficulty levels in pre-training on differentquestion difficulty levels from WikiTableQuestions dev set.
Figure 9: The fine-grained statistics of typical operators, example SQLs, operator percentage andtheir execution accuracies on the held-out 20, 000 SQL queries.
