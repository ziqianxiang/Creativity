Figure 1: The training procedure of GCN layers, where only compressed activations are stored inmemory. For illustration convenience, ReLU is ignored. During the forward pass, each layer’saccurate activations (H(l)) is used to compute those for the subsequent layer. Then the accurateactivations will be compressed into the compressed activations ( HI(Nl)T and Hp(rlo)j), which overwritesthe the accurate activations and is retained in the memory. During the backward pass, we recoverthe compressed activations back to decompressed activation ( H(I)) for computing the gradient.
Figure 2: The performance of EXACT is mainlydetermined by the R ratio of random projection.
Figure 3: Training throughput comparison on a single RTX 3090 (24GB) GPU (higher is bet-ter). The time overhead of EXACT is roughly 12%-25%. We discuss about swapping and gradientcheckpointing in Appendix I.2.
Figure 4: The histogram of the projected node embeddings’ infinity norm at MM (the left figure) andSPMM (the right figure) operation of the first GCN layer.
Figure 5: The performance of EXACT is mainly determined by the D ratio of random projection.
Figure 6: The performance of EXACT is mainly determined by the D ratio of random projection.
Figure 7: Validation Accuracy on Reddit dataset using EXACT with different configurations.
Figure 8: The sensitivity study ofEXACT (RP+INT2) to the R84The D/R ratio8	4	2	1The D/R ratioratio, where the model is Graph-over(s?)>⅛0 EJnɔuv -səɪ.6,54 35 5 5 59 9 9 9(％) AoeJnOQV⅛9I64.0063.7563.505 O2 O63.63.
Figure 10: The sensitivity study of EXACT (RP+INT2)All reported results are averaged over ten random trials.
Figure 11: The sensitivity study of EXACT (RP+INT2) to the R ratio, where the model is GraPh-SAGE. All reported results are averaged over ten random trials.
Figure 12: The sensitivity study of EXACT (RP+INT2) to the R ratio, where the model is GCNII.
