Figure 1: Demonstration of three distillation tasks using proposed ProtoCPC objective. (a) Compressing asupervised model, (b) compressing a self-supervised model, (c) self-supervised learning via self-distillation. Ourmethod is a contrative objective with the probability distributions where it is mediated by the prototypes. Thefigures of white color is training parameters while gray colors are frozen throughout the training. The dashedlines represent the copying of parameters such as exponential moving average (EMA).
