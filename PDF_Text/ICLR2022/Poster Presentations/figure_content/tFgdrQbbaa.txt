Figure 1: (a) Transitions from positive to negative transfer caused by target similarity ρ. We setNA = NB . (b) Learning curves show negative transfer (EA→B /EB) in a highly non-linear waydepending on unbalanced sample sizes. We changed NA and set NB = 103.
Figure 2: Self-knowledge forget-ting: unbalanced sample sizes de-grade generalization.
Figure 3: (a1)-(c1) Learning curves for equal sample sizes. For noise-less case, they monotonicallydecreased (orange lines). For noisy case, decrease was suppressed (grey lines; we plotted learningcurves for several σ2 and those with larger test errors correspond to larger σ2). We trained MLPon MNIST and ResNet-18 on CIFAR-10. (a2)-(c2) Learning curves for unbalanced sample sizeswere non-monotonic (N1 = 4, 000 for NTK regression, N1 = 104 for SGD training of MLP andResNet-18). Numbers in the legend mean Nn (n ≥ 2).
Figure 5: Theoretical values of EA→B (1). (Left) noise-less case (σ2 = 0), (Right) noisy case(σ2 = 10-5). We set D = 100 and other settings are the same as in Figure 1. In noise-less case,generalization error monotonically decrease as NA or NB increases. In contrast, when noise is added,it shows multiple descents.
Figure 6: Negative backward transfer easily appears depending on target similarity and sample sizes.
