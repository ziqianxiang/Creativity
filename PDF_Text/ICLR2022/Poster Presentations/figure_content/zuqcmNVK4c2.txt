Figure 1: Self-joint paradigm effectively expands a c-class classification problem into a c2-classclassification problem. As a concrete example, assume we are facing a ternary cat vs. dog vs. frogclassification problem. Self-joint framework principally casts this into a new 9-way classificationproblem {(cat, cat), (cat, dog), ..., (f rog, dog), (f rog, f rog)}, which represents all possible pairsof three labels for two inputs (in our implementation we concatenate inputs channel-wise) as shownby the 3 Ã— 3 matrix above. During inference, one can extract output probabilities for each inputseparately through marginalization, i.e. summing up the rows or columns of the matrix. Intuitively,if the marginal prediction for a given test sample changes when we replace the second sample in thepair, this indicates the marginal predictions have weak confidence. However, if the model is consis-tent with its prediction across paired samples, the overall marginal prediction has high confidence.
Figure 2: Visualization of overfitting effect for similar models trained in standard and self-jointframework on CIFAR-10. Standard supervised models with high capacity, measured on the x-axis using the width in standard WideResNet-34, suffer from overfitting as shown by the drop inaccuracy of the blue line around 10. Self-joint learning can successfully train larger networks due toits regularization effect.
