Figure 1: Environment Predictive Coding: During self-supervised learning, our model is given video walk-throughs of various 3D environments. We mask out portions of the trajectory (dotted lines) and learn to inferthem from the unmasked parts (in red). The resulting EPC encoder builds environment-level representations ofthe seen content that are predictive of the unseen content (marked with a “？”)，conditioned on the camera poses.
Figure 2: We propose the masked-zone prediction task for self-supervised learning of environment embeddings.
Figure 3: Integrating environment-level pretraining for navigation. Right: First, We transfer image-levelrepresentations to encode each RGB-D image. We transfer weights from an image encoder pre-trained for theself-supervised MoCo-v2 task on video walkthrough images. Left: Next, and most importantly, We transferthe environment-level representations to encode the entire history of observations. We transfer weights fromthe environment encoder and projection function M pre-trained for the proposed EPC masked-zone predictiontask on video walkthroughs. Center: Finally, we finetune the SMT end-to-end on each task using RL.
Figure 4: Each row shows one zone prediction example. Left: Top-down view of the 3D environment fromwhich the video was sampled. Cyan viewing frusta correspond to the average pose for three input zones.
Figure 5: Sample efficiency on MatterPort3D val split: We compare the sample-efficiency of EPC to that ofthe standard approach of pretraining only image-level features. Our environment-level pretraining leads to 2×to 6× better training sample efficiency on all four tasks. See Appendix A6 for corresponding Gibson plots.
Figure A1: Baseline architectures: We show the detailed architectures for each of the key baselines. “Goa-lEncoder” is a 1-layer MLP. M is a 2-layer MLP. Embed, FC, Conv, GroupNorm, and ReLU are PyTorchlayers corresponding to nn.Embedding(), nn.Linear(), nn.Conv2d(), nn.GroupNorm(), andnn.ReLU(), respectively (Paszke et al., 2019). The architectures shown are used for the RoomNav task. Forthe remaining tasks, we remove the room goal “Embed” and “GoalEncoder” layers and keep the rest of thearchitecture unchanged.
Figure A2: EPC hyperparameter search on weak exploration videos: We show the MP3D validation resufor different zone sizes on 3 tasks. The X-axis indicates the zone size as (N, m) where N is the numberframes in each zone, and m is the number of zones masked in the video. For N = 5-40, we randomly samɪN from 5 to 40 for each video during training.
Figure A3: EPC hyperparameter search on strong exploration videos: We show the MP3D validationresults for different zone sizes on 3 tasks. The X-axis indicates the zone size as (N, m) where N is the numberof frames in each zone, and m is the number of zones masked in the video. For N = 5-40, We randomly sampleN from 5 to 40 for each video during training.
Figure A4: Sample efficiency on Gibson Val split.OUr environment-level pre-training leads to 2 - 6× highersample efficiency when compared to SoTA image-level pre-training.
Figure A5: We highlight the downstream task performance as a function of episode time on both MatterPort3Dand Gibson. The legend shown near the top two rows also applies to the bottom two rows. Note that themaximum episode length on Gibson scenes is T = 500. For MP3D, we use a maximum episode length ofT = 1000 for all tasks except RoomNav where we use T = 500 (following Narasimhan et al. (2020)).
