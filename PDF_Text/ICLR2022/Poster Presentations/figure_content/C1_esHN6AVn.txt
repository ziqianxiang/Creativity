Figure 1: We use an agent-agnostic meta-learning approach to learn neural proxy RL environments(left) and reward networks (right) for a target task. In the inner loop, we train RL agents on theproxy and use the evaluation performance on the target task in the outer loop to evolve the proxy.
Figure 2: Multiple NES runs of Alg. 1 for CartPole(top) and Acrobot (bottom) which can teach agents thetask effectively. Each thin line is the average of 16worker scores from EvaluateAgent of such a run.
Figure 3: Top row: Densities based on each 4000 cumulative test rewards collected by DDQN(left), Dueling DDQN (center), and discrete TD3 (right) agents on CartPole. We show three settings:agents trained on a real environment without any involvement of SEs (blue, baseline), on SEs wherethe agent HPs were fixed during SE training (green), and on SEs when agent HPs were varied duringSE training (orange). After training, the evaluation is always done on the real environment. In allthree settings, we randomly sample the agent HPs before agent training. Bottom row: Average trainsteps and episodes corresponding to the densities along with the mean reward (below bars). Whentraining on SEs, we train 30-65% faster compared to training on the real environment.
Figure 4: Top row: Histograms of next state s0 and reward r produced by 10 DDQN agents whentrained on a CartPole SE (blue) and afterwards tested for 10 episodes on a real environment (orange).
Figure 5: The average cumulative test rewards of agents when trained on different RN variants forone episode and evaluated on the real environments for one episode alternatingly. Top row: Trainingand evaluation with the same agent using default agent hyperparameters (HPs). Center row: Withvaried agent HPs. Bottom row: Transfer to unseen agents. We show the std. error of the mean andthe flat curves in TD3 are due to filling the replay buffer before training.
Figure 6: Results from 40 different NES runs with 16 workers each (using random seeds) show thatour method is able to identify SEs that allow agents to solve the target tasks. Each thin line corre-sponds to the average of 16 worker evaluation scores returned by EvaluateAgent in our algorithm asa function of the NES outer loop iterations.
Figure 7: Evaluation of performance and transferability of SEs on the Acrobot-v1 task.
Figure 8: Histograms of approximate next state s0 and reward r distributions produced by 10 DDQNor Dueling DDQN agents when trained on an SE (blue) and when afterwards tested for 10 episodeson a real environment (orange) for each task. 1st row: CartPole and DDQN, 2nd row: CartPoleand Dueling DDQN, 3rd row: CartPole and discrete TD3, 4th row: Acrobot and DDQN, 5th row:Acrobot and Dueling DDQN, 6th row: Acrobot and discrete TD3The average cumulative rewards across the 10 runs for each agent variant and task are:•	CartPole and DDQN (1st row): 199.3•	CartPole and Dueling DDQN (2nd row): 193.75•	CartPole and discrete TD3 (3rd row): 197.2616Published as a conference paper at ICLR 2022•	Acrobot and DDQN (4th row): -91.62•	Acrobot and Dueling DDQN (5th row): -88.56•	Acrobot and discrete TD3 (6th row): -93.17A.5 Synthetic Environment HyperparametersThe following table provides an overview of the agent and NES hyperparameter (HP) ranges that weoptimized in an additional outer loop of our algorithm. The HPs in the 2nd and 3rd columns representthe results of this optimization and which we used for learning SEs. Note that in experiments wherewe sampled agent HPs from ranges given by Table 2 we overwrite a small subset (namely DDQNlearning rate, batch size, DDQN no. of hidden layers, and DDQN hidden layer size) of the listed
Figure 9: Performance of different RN variants for the HalfCheetah-v3 environment. Left: perfor-mance of RNs that train the known TD3 agent with default hyperparameters. Center: performanceof RNs when the hyperparameters of known agents are varied. Right: Transfer performance to theunseen PPO agents with default hyperparameters. All curves denote the mean cumulative rewardacross 25 runs (5 agents × 5 RNs). Shaded areas show the standard error of the mean and the flatcurves in TD3 stem from a fixed number of episodes that are used to fill the replay buffer beforetraining. Notice that using the observation vector v in the RN variants (denoted by “+augm.”) onlymarginally yields a higher train efficiency.
Figure 10: This is the zoomed in version of Figure 9.
Figure 11: Learned Cliff Walking reward for different reward network types. Shown is the start state(S), goal state (G), all cliff states in the lower row and the learned reward of each state-action pair(red: low reward, yellow: medium reward, green: high reward). The plots on the left side showaveraged values across 50 reward networks when considering all rewards, the plots on the right sideignore all rewards that are ≤ -50. Values have been normalized to be in a [0,1] range in both cases(left with full reward range, right without ignored rewards range).
Figure 12: The Cliff environment RNs optimized with the AUC objective.
Figure 13: The Cliff environment RNs optimized with the max reward objective.
Figure 14: The Cliff environment RNs optimized with the reward threshold objective (similar to theresults reported in Section 6).
Figure 15: The CartPole environment RNs optimized with the AUC objective.
Figure 16: The CartPole environment RNs optimized with the max reward objective.
Figure 17: The CartPole environment RNs optimized with the reward threshold objective (similar tothe results reported in Section 6).
Figure 18: The MountainCarContinuous environment RNs optimized with the AUC objective.
Figure 19: The MountainCarContinuous environment RNs optimized with the reward thresholdobjective (similar to the results reported in Section 6).
Figure 20: The HalfCheetah environment RNs optimized with the AUC objective.
Figure 21: The HalfCheetah environment RNs optimized with the reward threshold objective (simi-lar to the results reported in Appendix Section B.1).
Figure 22: Top: Comparing the cumulative test reward densities of agents trained on SEs (greenand orange), supervised baseline (purple), and baseline on real environment (blue). Agents trainedon the supervised model underperform the SE models and the real baseline. Bottom: Comparingthe needed number of test steps and test episodes to achieve above cumulative rewards. Left: realbaseline, center: SEs, right: supervised (MBRL) baseline.
Figure 23: Top: Comparing the cumulative test reward densities of agents trained on SEs (green andorange), supervised baseline (purple), and baseline on real environment (blue). Agents trained onthe supervised model underperform the SE models and the real baseline and using only the best twosupervised models (purple) does not seem to change this. Bottom: Comparing the needed numberof test steps and test episodes to achieve above cumulative rewards only using the best two of thefive models. Left: real baseline, center: SEs, right: supervised (MBRL) baselineAppendix E NES Score TransformationE.1 Score Transformation VariantsThe following overview lists eight score transformations for calculating Fi based on the expectedcumulative reward (ECR) Ki or the associated rank Ri . These score transformations where alsoused in the hyperparameter optimization configuration space for finding SEs and RNs (e.g, in Tables3 and 7). The rank Ri is defined as the number of population members that have a lower ECR thanthe i-th member, i.e. the rank associated with the lowest ECR is 0, the one with the highest ECRnp - 1 (the population size minus 1). We denote the ECR associated with the current SE or RNincumbent as Kψ .
Figure 24: Evaluation of different score transformation schemes for synthetic environments: Shownis the required number of NES outer loop iterations (green: 0, yellow: 25, red: 50) for differenthyperparameter configurations until an SE is found that solves the real environment (here: a 2x2grid world environment).
Figure 25: Model-wise evaluation: 1000 randomly sampled DDQN agents a` 10 test episodes per SEmodel.
Figure 26: Model-wise evaluation based on the cumulative rewards of 1000 randomly sampledDueling DDQN agents a` 10 test episodes per SE model.
Figure 27: Model-wise evaluation based on the cumulative rewards of 1000 randomly sampleddiscrete TD3 agents a` 10 test episodes per SE model.
Figure 28: Model-wise evaluation based on the cumulative rewards of 1000 randomly sampledDDQN agents a` 10 test episodes per SE model.
Figure 29: Model-wise evaluation based on the cumulative rewards of 1000 randomly sampledDueling DDQN agents a` 10 test episodes per SE model.
Figure 30: Model-wise evaluation based on the cumulative rewards of 1000 randomly sampleddiscrete TD3 agents a` 10 test episodes per SE model.
