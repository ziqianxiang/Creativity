Figure 1: Overview of neural network conditioning methods with two inputs, z ∈ Rb×d conditioningvector and X ∈ Rb× input feature: (a) conditioning via concatenation, (b) modulation and (C) theproposed learned activation (LA) functions, illustrated here together with a dense layer.
Figure 2: Learned activations (LA) for models trained on Voxforge, for speaker embedding vectorsof randomly selected users in a PSE application. To take into account that basic activations mayhave different ranges, the plots show LAs minus average of basic activations, i.e., LA(h | zj ) -a-1 Pia=1 Ai (h) for h ∈ [-3, 3] and values of j corresponding to the selected users. The plotshighlight that LA exhibit different profiles across different users and layers.
Figure 3: Learned activations (LA) for models trained on LibriSpeech, for speaker embedding vectorsof randomly selected users in a PASR application. To take into account that the non-personalizedbasic activations are relu or swish, the plots show LAs minus average of those two activations, i.e.,LA(h | zj) - (relu(h) + swish(h))/2 for h ∈ [-3, 3] and values ofj corresponding to the selectedusers. The plots highlight that LA exhibit distinct profiles across different users and layers.
Figure 4: WER relative and absolute improvement on LibriSpeech test-clean fine-tuning speakerembedding and LAs (top), training everything (bottom) for CTC greedy (a) and beam search withbeam width four (b) decoders, with two seconds of enrollment data, for character text encoder.
Figure 5: A pictorial representation of the RNN (a), TDS (b) and TDS-RNN (C) models used in theevaluation of conditioning based on concatenation (top, C), modulation (top, M) and based on theproposed learned activations (LAs) (bottom).
Figure 6: Correlation analysis of conditioning vectors with softmax values of LA for models trainedon Librispeech. In sub-figure (a) and (b), the bottom row shows the overall correlation of all LAs inmodel with conditioning vectors, and the top row depicts the distance matrix corresponding to LAwith the highest overall correlation for each model as well as for conditioning vectors (fig c).
