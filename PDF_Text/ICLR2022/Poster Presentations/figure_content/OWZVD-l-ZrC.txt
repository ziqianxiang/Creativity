Figure 1: Illustration of RUNE. The agent interacts with the environment and learns an ensembleof reward functions based on teacher preferences. For each state-action pair, the total reward is acombination of the extrinsic reward, the mean of the ensemble’s predicted values, and the intrinsicreward, the standard deviation between the ensemble’s predicted values.
Figure 2:	Examples of rendered images of robotic manipulation tasks from Meta-world. We considerlearning several manipulation skills using preferences from a scripted teacher.
Figure 3:	Learning curves on robotic manipulation tasks as measured on the success rate. Ex-ploration methods consistently improves the sample-efficiency of PEBBLE. In particular, RUNEprovides larger gains than other existing exploration baselines. The solid line and shaded regionsrepresent the mean and standard deviation, respectively, across five runs.
Figure 4: Ablation study on (a) Door Open, (b) Window Close, and (c) Door Close. We measure (a)Effects of different number of learned reward model ensembles on RUNE; (b) Effects of differentnumber of queries received per feedback session on PEBBLE and RUNE; and (c) EPIC distancebetween the ensemble of learned reward functions and true reward in the task environment. Thesolid line and shaded regions represent the mean and standard deviation, respectively, across fiveruns. The dotted line in Figure 4(b) indicates asymptotic success rate of PEBBLE baseline withsame hyperparameters.
Figure 5:	RUNE performs better than PEBBLE and PrefPPO in variety of manipulation tasks usingdifferent budgets of feedback queries. The solid/dashed line and shaded regions represent the meanand standard deviation, respectively, across five runs.
Figure 6:	RUNE performs better than PEBBLE in Door Open using different budgets of feedbackqueries. The solid/dashed line and shaded regions represent the mean and standard deviation, re-spectively, across five runs.
Figure 7:	RUNE performs better than PEBBLE in Drawer Open using different budgets of feed-back queries. The solid/dashed line and shaded regions represent the mean and standard deviation,respectively, across five runs.
Figure 8:	RUNE performs better than PEBBLE in variety of manipulation tasks using differentbudgets of feedback queries. The solid/dashed line and shaded regions represent the mean andstandard deviation, respectively, across five runs.
Figure 9:	(a) RUNE achieves better sample-efficiency on manipulation tasks from Meta-Worldbenchmark. (b/c) EPIC distance between the ensemble of learned reward functions and true rewardin task environments. The solid/dashed line and shaded regions represent the mean and standarddeviation, respectively, across five runs.
Figure 10:	Time series of normalized learned reward (blue) and the ground truth reward (red) usingrollouts from a policy optimized by RUNE.
