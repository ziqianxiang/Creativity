Figure 1: The t-SNE of poisoned samples in the hidden space generated by different models. (a)-(b): DNNs trained with supervised learning. (c)-(d): DNNs trained with self-supervised learning.
Figure 2: The main pipeline of our defense. In the first stage, we train the whole DNN model viaself-supervised learning based on label-removed training samples. In the second stage, we freezethe learned feature extractor and adopt all training samples to train the remaining fully connectedlayers via supervised learning. After that, we filter high-credible samples based on the training loss.
Figure 3: Loss values of models under BadNets attack with 20% poisoning rate trained on CIFAR-10 dataset with the symmetric cross-entropy (SCE) and cross-entropy (CE) in the second stage. Allloss values are normalized to [0, 1]. As shown in the figure, adopting SCE can significantly increasethe loss differences between poisoned samples and benign ones compared with the CE.
Figure 4: The illustration of poisoned samples generated by different attacks.
Figure 5: An example of poisoned samples generated by different attacks on VGGFace2 dataset.
Figure 6: The effects of filtering rate.
Figure 7: The effects of poisoning rate.
