Figure 1: Continuous Kernel Convolution (CKConv). CKConv views a convolutional kernel as avector-valued continuous function ψ : R → RNOutXNin parameterized by a small neural network MLPψ.
Figure 2: Discrete centered, causal, and dilated causal convolutions.
Figure 3: Functional family of recurrent units, discrete convolutions and CKConvs. For max.
Figure 4: Approximation quality of MLPs with ReLU, LeakyReLU, Swish, and Sine nonlinearities.
Figure 5: An step function approximated via aspline basis (left) and a periodic basis (right). As thetarget function is defined uniformly on a given in-terval, uniformly initializing the knots of the splinebasis provides faster and better approximations. Pe-riodic bases, on the other hand, periodically bendspace, and thus can be tuned easier to approximatethe target function at arbitrary points in space.
Figure 6: The sensitivity of networks with layer-wise exponential growing to slight changes. Takenfrom Hanin & Rolnick (2019). The sawtooth function with 2n teeth (left) can be easily expressedvia a ReLU network with 3n + 4 neurons (bottom). However, a slight perturbation of the networkparameters -GaUSSian noise With standard deviation 0.1- greatly simplifies the linear regions capturedby the network, and thus the distribution of the knots in the basis (right).
Figure 7: Function approximation via ReLU, LeakyReLU, Swish and Sine networks. All networkvariants perform a decent job in approximating simple functions. However, for non-linear, non-smooth functions, all networks using nonlinearities other than Sine provide very poor approximations.
Figure 8: Graphical description of continuous kernel convolutional networks. Dot-lined blocksdepict optional blocks, and blocks without borders depict variables. KernelNet blocks use Sinenonlinearities. We replace spatial convolutions by Fourier Convolutions (FFTConv), which leveragesthe convolution theorem to speed up computations.
Figure 9: High-frequency components in Sine continuous kernels. We observe that continuouskernels parameterized by Sine networks often contain frequency components of frequency higherthan the resolution of the grid used during training. Here, for instance, the kernel looks smooth onthe training grid. However, several high-frequency components appear when sampled on a finergrid. Though this may be a problematic phenomenon, we believe that, if tuned properly, these high-frequency components can prove advantageous to model fine details in tasks such as super-resolutionand compression cheaply.
