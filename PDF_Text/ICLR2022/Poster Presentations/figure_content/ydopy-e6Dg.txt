Figure 1: Linear probing accuracy on ImageNet.
Figure 2: Masked imagemodeling. I denotes an im-age and Tok. denotes a vi-sual tokenizer.
Figure 3: Overview of iBOT framework, performing masked image modeling with an onlinetokenizer. Given two views u and v of an image x, each view is passed through a teacher networkht ◦ ft and a student network hs ◦ fs . iBOT minimizes two losses. The first loss L[CLS] is self-distillation between cross-view [CLS] tokens. The second loss LMIM is self-distillation betweenin-view patch tokens, with some tokens masked and replaced by e[MASK] for the student network.
Figure 4: Pattern layout of patch tokens. TWo left figures shoWcase patterns, headlight of thevehicle and ear of the dog, that share part semantics. TWo right figures shoWcase patterns, strippedand curly surface, that share part textures.
Figure 5: Part-wise linear probing accu-racy. Top-k tokens with the highest attentionscores are averaged for classification.
Figure 6: Visualization for self-attentionmap. Self-attention map from multiple headsare visualized with different color.
Figure 7: Computation pipelines for iBOT with or without multi-crop augmentation. (a) iBOTw/o multi-crop augmentation. (b), (c), and (d) are three pipelines w/ multi-crop augmentation. (b)does not perform MIM for local crops, whereas (c) performs MIM for all crops. (d) only performsMIM for one of the two global crops. iBOT uses (b) with random MIM.
Figure 8:	Training curves of different multi-crop strategy.
Figure 9:	Robustness against occlusion. Model’s robustness against occlusion with different in-formation loss ratios is studied. 3 patch dropping settings: Random Patch Dropping (left), SalientPatch Dropping (middle), and Non-Salient Patch Dropping (right) are considered.
Figure 10: Robustness against shuffle. Model,s robustness against shuffle with different grid ShUf-fle sizes is studied.
Figure 11: Impact of the prediction ratio.
Figure 12: Impact of the training epochs.
Figure 13: Visualization for pattern layout of patch tokens that share high-level semantics. Inthe first row, we visualize different human-related semantic parts. We observe clear patterns ac-counting for human hair, human shoulder & arm, and human elbow respectively in the left, middle,and right figure. In the figures from the second row and the left figure from the the third row, wevisualize animal-related semantic parts. dog’s ear, dog’s nose, bird’s wing, and dragonfly’s wing canbe observed. In the rest of figures from the third row, we visualize semantic parts related to outdoorscenes. front window of the vehicle and window of the architecture can be observed. In the last row,we visualize indoor objects like ceiling and glass bottle.
Figure 14: Visualization for pattern layout of patch tokens that share low-level details. In thefirst two columns, we visualize patches that share similar textures. In the first figure, fur of leopardand the skin of lizard share a similar dotted texture. In the second figure, shell of hedgehog andthe skin of elephant share similar striped texture. In the third column, we visualize pattern layoutsrelated to shape. For example, the shape of objects in the left and middle figures share similarcurvature. The rightmost patterns clearly depict the shape of a straight line. We visualize patternlayout related to color in the last column, where blue, green and white can be observed.
Figure 15: Top-4 representative patches with each of their pattern layout. Order index 0, 1,2, 3 are ranked according to its self-attention score. In the top-left corner for each pattern layoutsubfigure, its order index and cluster index are annotated. In the top panel, we can observe thatpattern 0,2,3 show explicit semantic information of nose, eyes, ears respectively. Interestingly, patch1 also locates around the eyes of the Samoyed but its corresponding pattern share visual similarityin shape instead of semantics. This illustrate the diverse behaviour for each learned pattern. In thebottom panel, a library is represented by 0 two- or multi-color joints, 1,3 knurlling texture, 2 texts.
Figure 16: Visualization for pattern layout of patch tokens using BEiT (top) and DINO (bot-tom). In the layout extracted from the DALL-E encoder, we observe minimal semantic patterns. Inmost cases, patches with similar color (e.g., black area in left figure) or texture (e.g., line in right fig-ure) are clustered. In the layout extracted from DINO, while more complex textures are visible, mostpatches share similar local details instead of high-level semantics. In the right figure, the semanticpart eyes can be somehow observed, yet it is mixed with plenty of irrelevant semantic parts.
Figure 17: Visualization for pattern layout of [CLS] token. We here indicate the high quality ofsemantic layout brought by self-distillation of cross-view images on [CLS] token. This property isnot brought by MIM and is also prominent in DINO.
Figure 18: Visualization for self-attention map from Multiple Heads. In the first 8 columns,we showcase iBOT’s attention map along with DINO’s. In the last 10 columns, we showcase moreattention map from iBOT. We indicate that iBOT shows visually stronger ability to separate differentobjects or different parts of one object apart by giving more attentive visualized results for eachpart, compared with DINO. For example, in the fifth column, there is an attention head in iBOTaccounting for the ear of the fox solely, while in DINO, it emerges with other parts; In the eighthcolumn, iBOT separates the mushroom into more semantically meaningful parts.
Figure 19: Visualization for sparse correspondence. The top panel are images pairs sampledfrom two views of one image. The extracted correspondence from iBOT is mostly correct despiteaugmentations on scale and color. The bottom panel are image pairs sampled from two images ofone class. The first row is images with salient objects but different sizes, positions and textures. Thesecond row are images draw from animals, and we can observe more clearly that iBOT matches thesemantic parts of animals correctly (e.g., tails of the fox, beak of the bird). The third row is human-centered images with human bodies or clothing. The fourth row is natural or domestic scenes wheresalient objects are invisible. Although no explicit semantic parts can be matched visible to human’sunderstanding, we can still observe the iBOT can extract correspondence based on their texture orcolor (e.g., wooden texture of signboard and boxes. All these visual results demonstrate strongcapability for iBOT in part retrieval or matching in a local scale.
