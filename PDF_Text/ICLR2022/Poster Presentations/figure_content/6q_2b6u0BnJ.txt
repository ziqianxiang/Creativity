Figure 1: The TRAIL framework. Pretraining learns a factored transition model TZ ◦ φ and anaction decoder ∏a on Doff. Downstream imitation learns a latent policy ∏z on Dπ* with expertactions reparametrized by φ. During inference, πZ and πα are combined to sample an action.
Figure 2: Tasks for our empirical evaluation. We include the challenging AntMaze navigation tasksfrom D4RL (Fu et al., 2020) and low (1-DoF) to high (21-DoF) dimensional locomotaion tasks fromDeepMind Control Suite (Tassa et al., 2018).
Figure 3: Average success rate (%) over 4 seeds of TRAIL EBM (Theorem 1) and temporal skillextraction methods - SkiLD (Pertsch et al., 2021), SPiRL (Pertsch et al., 2020), and OPAL (Ajayet al., 2020) - pretrained on suboptimal Doff. Baseline BC corresponds to direct behavioral cloningof expert Dπ* without latent actions.
Figure 4: Average rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (The-orem 3), and baseline methods when using a variety of unimodal (ant-medium), low-quality(ant-medium-replay), and random (ant-random) offline datasets Doff paired with a smallerexpert dataset Dπ* (either 10k or 25k expert transitions).
Figure 5: Average task rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (Theo-rem 3), and OPAL (other temporal methods are included in Appendix D) pretrained on the bottom80% of the RL Unplugged datasets followed by behavioral cloning in the latent action space on 今of the top 20% of the RL Unplugged datasets following the setup in Zolna et al. (2020). BaselineBC achieves low rewards due to the small expert sample size. Dotted lines denote the performanceof CRR (Wang et al., 2020), an offline RL method trained on the full RL Unplugged datasets withreward labels.
Figure 6: Average task rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (The-orem 3), and OPAL, SkiLD, SPiRL trained on the bottom 80% (top) and bottom 5% (bottom) ofthe RL Unplugged datasets followed by behavioral cloning in the latent action space. Baseline BCachieves low rewards due to the small expert sample size. Dotted lines denote the performance ofCRR (Wang et al., 2020) trained on the full dataset with reward labels.
Figure 7: Average rewards (over 4 seeds) of TRAIL EBM (Theorem 1), TRAIL linear (Theorem 3),and baseline methods pretrained on kitchen-mixed and kitchen-partial from D4RL toimitate kitchen-complete. TRAIL linear without temporal abstraction performs slightly betterthan SKiLD and OPAL with temporal abstraction over 10 steps.
Figure 8: Average task rewards (over 4 seeds) of TRAIL EBM (Theorem 1) and vanilla BC (right)in a discrete four-room maze environment (left) where an agent is randomly placed in the maze andtries to reach the target ‘T’. TRAIL learns a discrete latent action space of size 4 from the discreteoriginal action space of size 12 on 500 uniform random trajectories of length 20 shows clear benefitover vanilla BC on expert data.
Figure 9: Ablation study on action decoder finetuning, latent dimension size, and pretraining base-line BC on suboptimal data in the AntMaze environment. TRAIL with default embedding dimension64 and finetuning the action decoder corresponds to the second row. Other dimension size (256 and512) lead to worse performance. Finetuning the action decoder on the expert data has some smallbenefits. Pretraining BC on suboptimal data before finetuning on expert does not lead to significantlybetter performance.
Figure 10: Ablation study on latent dimension size in the Ant environment. TRAIL is generallyrobust to the choices of the latent action dimension (64, 256, 512) for the Ant task.
Figure 11: Ablation study on finetuning the action decoder in the Ant environment. Finetuning theaction decoder leads to a slight benefit.
Figure 12: PCA and t-SNE visualizations of the random, medium-replay, medium, andexpert D4RL Ant datasets. Without action representation learning (left), the distinction betweenexpert and suboptimal actions is not obvious. The latent actions of TRAIL (right), on the other hand,results in the expert latent actions being more visually separable from suboptimal actions.
