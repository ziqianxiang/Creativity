Figure 1: Implicit environment fields in 2D and 3D space. With implicit functions, We learn a continuousenvironment field that encodes the reaching distance between any pair of points in 2D and 3D space. We presentutilization of the environment field for 2D maze navigation in (d)-(e) and human scene interaction modeling in3D scenes in (f). Note that We flip the environment field in (e) upside down for visualization purposes.
Figure 2: Trajectory searching. (a) The 8 possibledirections that an agent can take in a 2D maze. (b)Searched trajectory by querying the environment fieldvalues of positions the agent can reach within one step.
Figure 3: Variants of implicit environment functions (IEFs). The IEF in (a) assumes a fixed environmentand a fixed goal position; (b) generalizes to different goal positions in the same environment; (c) further gen-eralizes to different goal positions in different environments. Goal positions are represented by blue circles. Inthis figure, We use the 2D maze as an example, but models in (a) and (b) can be generalized to 3D scenes bysimply changing the inputs from 2D coordinates to 3D coordinates.
Figure 4: Birdâ€™s eye view navigation results. We show the searched trajectories for two people in (a).
Figure 5: Conditional VAE for accessible space generation.
Figure 6: Grid-world maze navigation. Blue paths in(a) are planned by FMM while red paths are searched bythe learned environment field. Green circles and orangesquares represent starting positions and goal positions.
Figure 7: Dynamic human motion modeling on PROX (Hassan et al., 2019) The accessible region in (b)includes generated torso locations by the VAE model discussed in Section 4.2. In the environment field in (a),the brighter a point is, the closer it is to the goal.
Figure 8: HypernetWork architecture. Blue trapezoid is the hypernetwork that predicts the parameters of thehyponetwork (the yellow trapezoid).
Figure 9: Aligned walking sequence to searched trajectory. The figure is for visualization purposes only.
Figure 10: Visualization of path planning by RRT (Bry & Roy, 2011) and PRM (Kavraki et al., 1996).
Figure 11: Grid-world maze navigation results.
Figure 12: Failure cases.
Figure 13: Dynamic human motion modeling on PROX (Hassan et al., 2019). We highly recommendviewing the accompanying video for dynamic visualizations.
Figure 14: Trajectory visualization in 3D scenes. The ground truth trajectory, the trajectory searched byHMP Cao et al. (2020) and the trajectory searched by our algorithm are shown in red, blue and green.
