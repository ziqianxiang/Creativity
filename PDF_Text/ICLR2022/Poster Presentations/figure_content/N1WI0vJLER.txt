Figure 1: Parallel distribution of work in parallel-in-time training.
Figure 2: For cf = 4 with T = 16, this image shows F - and F C -relaxation. In both cases, forwardpropagation is performed serially starting at the open green circle and ending at the closed one. Eachpropagation step is performed in parallel, with four-way parallelism depicted in the image.
Figure 3: The two level MGRIT method. Green arrows represent relaxation, and forward propaga-tion. Grid transfers are represented by red arrows. Numbers in parentheses are lines in Algorithm 2.
Figure 4: For the UCI-HAR dataset: (left) Accuracy for classic, implicit and parallel training of aGRU network on the UCI-HAR dataset. (right) Run time per epoch for training.
Figure 5: HMDB51 Subset: (left) MGRIT converges rapidly for cf = 2, 4, and 8. Sufficient accu-racy for training is achieved with 2 forward iterations, and 1 backward. (right) Parallel performanceof training with different sequence lengths.
Figure 6: HMDB51 Subset: The accuracy for GRU, and implicit GRU, using serial and paralleltraining. The right image highlights serial vs. parallel inference with the MGRIT trained network.
Figure 7: For HMDB51: Batch run times for training a GRU network with ResNet18, 34, and 50.
Figure 8: For HMDB51: (left) Per epoch breakdown of run times with ResNet18 preprocessingimages. (right) Test accuracy as a function of epochs for a ResNet34 GRU network, trained on32 MPI ranks with 9 OpenMP threads per rank. The thin blue lines are different initializations forparallel training, and the red line provides a baseline for serial training.
Figure 9: This figure is a sequence of images representing the error in the hidden state for an ODEusing 64 time steps. Each color is a different component of the hidden state (e.g., h(t) âˆˆ R3 forall time). The left column shows the error, while the right column shows the Fourier transform ofthe error. The first row (images A and B) is the initial error. The second row (images C and D) isthe error after 4 sweeps of F CF -relaxation. The final row (images E and F) is the restriction of therelaxed error (in the second row) onto the coarse grid. Note that the scale in the Fourier coefficients,and wave numbers axes vary by row.
Figure 10: For a Subset of HMDB51: The training loss for the Classic GRU method and ImplicitGRU using the parallel MGRIT propagation algorithm.
