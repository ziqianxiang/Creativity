Figure 1:	Zero-shot performance of CLIP-ResNet50 and our DeCLIP-ResNet50 when us-ing different amounts of data. (88M, 62.5%) de-notes the use of 88M data with top-1 accuracy62.5% on the ImageNet-1K validation dataset.
Figure 2:	Transfer the DeCLIP-ResNet50 (abbr.
Figure 3: Examples of NearestNeighbor from Conceptual Cap-tions dataset.
Figure 4: (a) CLIP and ALIGN jointly train an image encoder and a text encoder to predict thecorrect pairings of a batch of (image, text) training examples. (b) Our DeCLIP overview.① meansSelf-Supervision(SS). For image SS, We maximize the similarity between two augmented viewsof the same instance. For text SS, we leverage Masked Language Modeling(MLM) within a textsentence.② represents cross-modal Multi-View Supervision(MVS). We first have two augmentedviews of both image and text, then contrast the 2 X 2 image-text pairs. ③ indicates Nearest-Neighbor Supervision(NNS). We sample text NN in the embedding space to serve as additionalsupervision. The combination of the three supervision leads to efficient multi-modal learning.
Figure 5: Self-Supervision with eachmodality. We adopt SimSiam andMLM for image and text SS.
Figure 6: Nearest-Neighbor Supervision.
Figure 7: Ablation on pre-training cost onCC3M dataset. The proposed method performsbetter with less training time.
Figure 8: Class activation maps (CAM) for the CLIP vs. our DeCLIP model trained on the YFCCdataset. The CAMS of our model segment the complete object, while the CLIP model only looksat a few components.
Figure 9: Nearest neighbor samples from different datasets. As we can see, the NN pair is verysimilar to the original pair, thus it can provide high-quality supervision.
Figure 10:	Example image-text pairs randomly sampled from the training dataset. (a) ConceptualCaptions, (b) YFCC, (c) Conceptual 12M, (d) Web-crawled.
Figure 11:	The prompts for zero-shot testing.
