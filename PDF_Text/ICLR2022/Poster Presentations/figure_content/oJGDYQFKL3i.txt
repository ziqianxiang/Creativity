Figure 1: ODDN could be used to handle multipledownstream tasks.
Figure 2: Overview of ODDN. The inference model assigns inputs into K slots per image, eachslot encodes object or background into latent space, shown in different colors. We take use of theattention mechanism of transformer to align objects’ latents of two input images and encode theaggregated representation into a low-dimensional vector to obtain disentangled object dynamics.
Figure 3: Illustration of video prediction results on CLEVRER. All models take as input the first twoframes and predict the next six frames. “IODINE linear” is implemented by simply concatenatinglatent features of the current and the previous frames and predicting the future frame with a feed-forward neural network. “ODDN w/o relation” is the version of ODDN without relation module. Inthe video, a collision happens between the blue and brown cylinder from time step 6 to 8.
Figure 4: Comparison of reconstruction results of ODDN with IODINE and PROVIDE, we highlightdense areas with dashed boxes.
Figure 8: Qualitative comparison of predic-tion performance of ODDN with PROVIDE andSRVP on realistic block towers.
Figure 7: Comparison of MSE for 6 predictedframes of different models on CLEVRER.
Figure 9: Addition prediction results of ODDN on CLEVRER and detailed attention coefficientsvisualization for relation module. We plot attention the scores all objects have on the rubber spherefrom the second frame. The collision happens between the rubber sphere and the metal sphere.
Figure 10: (a) Two consecutive frames with only the blue cylinder moving. (b) Visualization ofthe Transformer attention map of the blue cylinder in the matching phase of dynamics distillation.
