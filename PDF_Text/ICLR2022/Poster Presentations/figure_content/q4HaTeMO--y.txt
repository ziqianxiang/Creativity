Figure 1: Implicit network modules such as DDNs and DEQs output solutions to problems de-pending on their inputs and parameters (top). We establish a connection between DDNs with anoptimisation layer (middle) and DEQs with a fixed point layer (bottom). Under mild conditions,a DDN that solves a kGLM as its inner problem over dataset (X, Y ) and forms predictions at X(middle) is equivalent to a fully connected or convolutional DEQ accepting a datapoint Y with un-derlying fixed coordinates X and X (bottom). In an inpainting task, the inner yellow and outerregions represent pixel coordinates X and X . Y and Y represent corresponding image values. TheDEQ consists of a fixed point/linear layer with known fixed parameters W1, W2, W3, V1, V2 deter-mined by the kernel k and coordinates X, X . The activation functions σ, ρ, T are determined bythe exponential family and kernel regulariser of the kGLM. Red indicates that gradient signals areblocked for exact equivalence; equivalence is exact under our initialisation scheme.
Figure 2: (Blue) Layers (1) on the interval [-1, 1] when Y = 0, (Orange) identity. (a) tanh(0.9z +2)has derivative ≤ 0.9 so it admits a unique fixed point. (b) tanh(z) has derivative ≤ 1. It satisfies theconditions of Proposition 2 on (-1, 0) and (0, 1) and so admits a unique fixed point. (c) tanh(-3z)is not a contraction but admits a unique fixed point; contractions are not necessary. (d, e, f) tanh(3z),ReLU(z + 0.2), ReLU(z) are not contractions and admit 3, 0 and uncountably many fixed points.
Figure 3: Grouped in sets of four. (Left to right) Uncorrupted target image Ys, noisy image Ys,image output by randomly initialised DEQ, image output by kGLM initialised DEQ. kGLM but notrandom initialisation preserves some qualitative properties of the input.
Figure 4: A particular choice of kernel induces a sparse matrix K = k(X, X) with repeated entries.
Figure 5: (a) Test error on a smooth sequence-to-sequence task for 100 random seeds for each ini-tialisation scheme. kGLM initialsation offers a better starting point and faster training than randominitialisation. (b) Sample model outputs on smooth sequence-to-sequence task. Black curves showmean h and colours represent different samples. (Top left) Ground-truth test data Ys . (Top right)Random and (Bottom left) Naive GLM initialisation after 20 epochs. (Bottom right) InformedkGLM initialisation without any training (i.e. epoch 0). kGLM initialisation, particularly informed,preserves some of the qualitative properties of the target sequence.
Figure 6: Test MSE against the average spectral norm of each layer for image denoising task usingkGLM initialisation (blue, ours) and random initialisation (red). The vertical axes change betweenplots. Markers at the edge of the top border indicate that a value greater than the axis limit or NaNwas observed. Our initialisation scheme shows superior performance at all epochs.
Figure 7: Connections between implicit layer architectures.
Figure 8:	Empirically measured cost (in seconds) for initialisation schemes measured on a DELLLaptop (16GB RAM, Intel®CoreTM i7-8665U CPU), averaged over 100 runs. (Left) Fully con-nected DEQ layer. (Right) Convolutional DEQ layer. In all cases, initialisation represents a smallcost compared with training.
Figure 9:	First 4 channels of HSI dataset. Test MSE against the average spectral norm of each layerfor image denoising task using kGLM initialisation (blue, ours) and random initialisation (red). Thevertical axes change between plots. Markers at the edge of the top border indicate that a value greaterthan the axis limit or NaN was observed. Our initialisation scheme shows superior performance atall epochs.
Figure 10:	First 8 channels of HSI dataset. Test MSE against the average spectral norm of each layerfor image denoising task using kGLM initialisation (blue, ours) and random initialisation (red). Thevertical axes change between plots. Markers at the edge of the top border indicate that a value greaterthan the axis limit or NaN was observed. Our initialisation scheme shows superior performance atall epochs.
Figure 11:	First 10 channels of HSI dataset. Test MSE against the average spectral norm of eachlayer for image denoising task using kGLM initialisation (blue, ours) and random initialisation (red).
Figure 12:	First 11 channels of HSI dataset. Test MSE against the average spectral norm of eachlayer for image denoising task using kGLM initialisation (blue, ours) and random initialisation (red).
Figure 13: Visualisation of the 11 datapoints where KLR and DEQ-kernel predictions did notagree exactly. KLR predictions (top), DEQ-kernel predictions (middle), and DEQ-random pre-dictions (bottom). The top and middle rows are very close, in contrast with the bottom row. The redpixel in the middle row shows where it disagrees with the top. On all other 59989 examples, the topand middle rows agree exactly.
Figure 14: Testing and training error for DEQ-kernel-trained (green) andDEQ-random-trained (blue) models. The left column shows the first 100 iterationsonly, and the right column shows the entire training process. The initial models DEQ-kernel andDEQ-random are correspondingly represented by iteration 0. KLR is equivalent to DEQ-kernel(up to tie predictions). The top two plots show mean squared error of the binarised images on thetest set. The bottom two curves show mean squared error of the greyscale images on the trainingset. Kernel initialisation out-performs random initialisation. Note that the initial test error of kernelinitialisaton is smaller than after training until about 30 training iterations have passed.
