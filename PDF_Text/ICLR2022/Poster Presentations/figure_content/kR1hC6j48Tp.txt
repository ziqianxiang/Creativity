Figure 1: GATSBI uses a conditional generative adversarial network (GAN) for simulation-based inference (SBI). We sample parameters θ from a prior π(θ), and use them to generatesynthetic data x from a black-box simulator. The GAN generator learns an implicit approximateposterior qφ(θ∣x), i.e., it learns to generate posterior samples θ0 given data x. The discriminator istrained to differentiate between θ0 and θ, conditioned on x.
Figure 2: GATSBI performance on benchmark problems. Mean C2ST score (± standard errorof the mean) for 10 observations each. A. The classification score for GATSBI (red) on SCLPdecreases with increasing simulation budget, and is comparable to NRE. It outperforms rejectionABC and SMC-ABC, but has worse performance than NPE and NLE. B. The classification score forGATSBI (red) on the two-moons model decreases with increasing simulation budget, is comparableto REJ-ABC and SMC-ABC for simulation budgets of 1k and 10k, and is outperformed by all othermethods for a 100k simulation budget. However, GATSBI’s classification score improves when itsarchitecture and optimization parameters are tuned to the two-moons model (red, dashed).
Figure 3: Shallow water model inference with GATSBI, NPE and NLE. A. Ground truth, obser-vation and prior samples. Left: ground-truth depth profile and prior samples. Middle: surface wavesimulated from ground-truth profile as a function of position and time. Right: wave amplitudes atthree different fixed times for ground-truth depth profile (black), and waves simulated from multipleprior samples (gray). B. GATSBI inference. Left: posterior samples (red) versus ground-truth depthprofile (black). Middle: surface wave simulated from a single GATSBI posterior sample. Right:wave amplitudes for multiple GATSBI posterior samples, at three different times (red). Ground-truthwaves in black. C. NPE inference. Panels as in B. D. NLE inference.
Figure 4: SBC results on shallow water model. Empiricalcdf of SBC ranks, one line per posterior dimension (red,GATSBI; blue, NPE). In gray, region showing 99% of devia-tion expected under the ideal uniform cdf (black).
Figure 5: Camera model inference. Top: ground-truth parameter samples from the implicit priorand corresponding blurred camera model observations. Bottom: mean and standard deviation (SD) ofinferred GATSBI and NPE posterior samples for matching observation from top.
Figure 6: GATSBI, LFVI and Deep Poseterior Sampling (DPS) on benchmark problems. MeanC2ST score (± standard error of the mean) for 10 observations each. A. On Two Moons, the C2STscores for GATSBI (red), LFVI (navy) and Deep Posterior Sampling (DPS, yellow) are qualitativelysimilar across all simulation budgets B. On SLCP, DPS is slightly worse than LFVI and GATSBI.
Figure 7: Inference for one test observation of the SLCP problem. Posterior samples for GATSBItrained on 100k simulations (red), and reference posterior samples (black). The GATSBI posteriorsamples cover well the disjoint modes of the posterior, although GATSBI sometimes producessamples in regions of low density in the reference posterior.
Figure 8: Inference for one test observation of the Two Moons problem. Posterior samples forGATSBI trained on 100k simulations (red), and reference posterior samples (black). GATSBIcaptures the global bi-modal structure in the reference posterior, but not the local crescent shape. Italso generates some samples in regions of low density in the reference posterior.
Figure 9: Sequential GATSBI performance for the Two Moons Model. The energy-based correction(EBM) results in a slight improvement over amortised GATSBI for 1k and 10k simulations, but theinverse importance weights correction does not.
