Figure 1: Computational graph of the l-th variational layer in a hierarchy. The numbers denotethe order of the computations. The inference path forms the posterior q(zl | x, z<l) given thecondition clq. The generative path forms the prior p(zl | z<l) given the condition clp. clp representscontext information that consists of both deterministic features and latent samples with c0p beinga constant. hl represents hidden features of the data with hL+1 ≡ x. Multiple such blocks arestacked. p(x | Z) receives the sample ZL and the context CL of the last layer. The ㊉ symbol denotesa module responsible for combining two streams of features. In Figure 1a, the layer is connectedonly with the adjacent layers in the hierarchy. Latent information of earlier layers Z<l-1 is carriedthrough clp-1. In Figure 1b, the layer is connected with all the layers below (above) at the bottom-up pass (top-down pass). The ResNet transformations are extended to produce key k and querys feature maps. The generative network queries the layers of the generative network (inferencenetwork) above (below) to identify the most relevant conditioning factors of the prior (posterior)according to attention scores computed by the attention modules (see Figure 2).
Figure 2: Attend(C<l, sl, k<l)- a depth-wiseattention block. C<i, k<i are the sequences ofl - 1 contexts and corresponding keys with Cand Q feature maps accordingly, while si is thequery feature. The multiplication is applied tothe inner matrix dimensions. The normalizationof the softmax is applied to the last dimension,treating each pixel independently from the others.
