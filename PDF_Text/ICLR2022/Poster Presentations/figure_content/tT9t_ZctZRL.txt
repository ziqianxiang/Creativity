Figure 1: Convergence rate of GNKT. (a)-(c) Entries of the normalized (residual connection) GNTKas a function of the depth, defined as Rl + r. All entries tend to have the same value as thedepth grows. (d)-(f) The element (entry)-wise distance of the normalized (residual connection)GNTK as a function of the depth. The convergence rate can be bounded by an exponential functiony = exp(-0.15x) for vanilla and residual MLP GNTK, whereas the convergence rate of residualaggregation is bounded by y = exp(-0.11x).
Figure 2: Training and test accuracy w.r.t.
Figure 3: Overview of the information propagation in a general GCN.
Figure 4: Training loss as a function of epochs. (a) We implement 8-layer GCN, GCN with DropE-dge, GCN with C-DropEdge on Cora. (b) 12-layer GCN, GCN with DropEdge, GCN with C-DropEdge on Citeseer. The training loss of GCN and GCN with DropEdge has a slower convergencerate and converges to a larger error compared to GCN with C-DropEdge.
Figure 5: Convergence rate of GNTK with C-DropEdge. (a) Elements of the normalized (residualconnection) GNTK as a function of the depth, defined as Rl + r. Different elements tend to havedifferent value as depth grows (b) The element-wide distance of the normalized GNTK as a functionof the depth. The converge rate is no longer bounded by an exponential function. Instead, it remainsparallel to the horizontal depth axis.
