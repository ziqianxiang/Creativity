Figure 1: Overview of forward propagation of CGS. Given an input graph G , the parameter-generating network fθ constructs contracting linear transition maps Tθ. The fixed points Hm ofTθ are then computed via matrix inversion. The fixed points are aggregated into H*, and then thedecoder gθ decodes H * to produce Y *.
Figure 2: Diffusion experiment results. The x- and y-axis are the number of pores and the average MSE of thetest graphs. The error bars indicates the standard error ofpredictions (measured from 500 instances per each size).
Figure 3: Number of GNN layersvs. MSE (ns = 800).
Figure 4: Pore network graphary indicator of its corresponding pore. The boundarypressure is also as a node feature if the node corre-sponds to a boundary pore. The edge features are thecylinder volume, diameter, and length of its corre-sponding throat. We sample training graphs such that the graphs fit into 0.1 m3 cubes. The traininggraphs, which have 50-200 nodes, are then randomly generated (See Appendix C.1). We train CGSsuch that it minimizes the mean-squared error (MSE) between the predicted ones and Y*.
Figure 5: Solutions of GVI with CGS. The balls represent the states of MDP and the ball colors showthe prediction results and their corresponding targets. More details are described in the main text.
Figure 6: Training curves of the CGS models and the baselines. We repeat the training 5 times pereach model. The solid lines show the average training MSE over the training steps. The shadow areasvisualize the ± 1.0 standard deviation over the runs.
Figure 7: Sample efficiencyWe prepare 2048 training and 2048 test graphs and their labels. We then train CGS and IGNN by usingthe first 128, 256, 512, 1024, and 2048 training graphs. As shown in figure 7, we can confirm that thetrade-off between the sample efficiency and expressivities. IGNN only utilizes the input-dependent biasterms in the transition maps; thus, such structural assumptions can serve as an effective regularizerwhen the training samples are limited. However, IGNN shows more minor improvements along withthe number of training samples. On the other hand, CGS performs worse than IGNN when the traininggraphs are limited, but it starts to outperform IGNN as more training samples are used. Finally, whenthe number of training samples increases (≥ 512 graphs), CGS outperforms IGNN significantly.
Figure 8: Runtimes of CGS models. For GPU experiments, the memory usage of the direct methodwith ns ≥ 4000 exceeds 24GB VRAM.
