Figure 1: An illustration of different normalization methods using cube diagrams derived from Wu& He (2018). Each cube represents a feature map tensor with N as the batch axis, C as the channelaxis, and (H,W) as the channel axes. Pixels in blue are normalized by the same moments calculatedfrom different samples while pixels in orange are normalized by the same moments calculated fromwithin sample. Best viewed in colors.
Figure 2: The evolution of ACC(↑) on observed tasks so far on the Split CIFAR-10 and Split TinyIMN benchmarks, Task-IL screnario with DER++ and memory size of 5120 samples.
Figure 3: Changes of moments (L1 distance - y axis) between BN and BN* throughout training ona 2-layers MLP backbone. Standard deviation values are omitted due to small values (e.g. < 0.1)Table 9: Comparison with BN* on the 4-tasks Split CIFAR-100 benchmarks. Bold indicates the bestaveraged scoresER	Split CIFAR-100(4tasks)			ACC	FM	LABN	60.14±3.47	6.21±1.99	63.98±2.38BN*	61.38±2.46	5.87±1.37	65.01±1.78GN	55.96±2.43	2.23±0.79	53.25±2.44CN	62.18±0.56	5.66±0.76	64.94±1.68D.5 Additional Results of BN*We further investigate the performance of BN* on the Split-CIFAR100 benchmark and report andreport the result in Table 9. Since BN* requires calculating the true moments from all observed data,we cannot scale it to the standard benchmark of 17 tasks. Table 9 only consider a sequence of fourtasks, which is the maximum memory capacity that our GPU allows. Interestingly, we observe thatGN achieves low FM in this experiment, suggesting that GN does not suffer much from catastrophicforgetting early on during training, or when the sequence of tasks is short. As there are more tasks,we expect the gap among different methods to be more significant as demonstrated throughout ourwork.
