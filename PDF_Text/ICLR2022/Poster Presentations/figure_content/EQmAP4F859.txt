Figure 1: A conceptual drawing of empirical and oracle world learning curves. Stage 1: all curvesare together. Stage 2: training error goes to zero while test and oracle error stay together. Stage 3:test error remains constant while oracle error decays to the RKHS approximation error. See Section1.1 for a more detailed discussion. (Dotted lines in stage 3 indicate compressed time interval.)Over the past few years, researchers have used kernel machines as a tractable model to investigatemany neural network phenomena including benign overfitting, i.e., generalization despite the inter-polation of noisy data (Bartlett et al., 2020; Liang & Rakhlin, 2020) and double-descent, i.e, riskcurves that are not classically U-shaped (Belkin et al., 2020; Liu et al., 2021). Kernels have alsobeen studied to better understand certain aspects of neural network architectures such as invarianceand stability (Bietti & Mairal, 2017; Mei et al., 2021b). Although kernel methods cannot be used toexplain some phenomena such as feature learning, they can still be conceptually useful for under-standing other neural networks properties.
Figure 2: A conceptual drawing of the evolution of the empirical and oracle models ft and ftor . Instage 1, ft and for learn the best linear approximation of fd. At the start of stage 2, ft and for learnthe best quadratic approximation. At the end of stage 2, ft interpolates the training set but is closeto for in the L2 sense. Lastly in stage 3, for learns fd while ft stays the same as the end of stage 2.
Figure 3: Schematic drawings of the conclusions of Theorems 1 and 2 for three different noiselesssettings. Panels (3a) and (3b) illustrate the performance of a dot product kernel Hd for two differentscalings n(d). The full three stages appear in (3a), but the second stages disappears in (3b) sincelog n/ log d is nearly an integer (see Remark 1). Panel (3c) compares the performance of a dotproduct kernel Hd (red) and corresponding cyclic kernel Hd,inv (green) for a cyclic target functionfd . The cyclic kernel in (3c) generalizes better but optimizes more slowly (see Remark 2).
Figure 4: Top row: Log-scale plot of errors versus training time for dot product kernel (4a, 4b) and{dot product, cyclic} kernels in (4c). In (4a) σ = ReLU and (a0, aι, a2) = (1/2,1/√2,1/√8). In(4b), σ = ReLU+0.IHe3 and (a0,a1,a2,a3) = (1/2,1∕√2,0,1∕√24). In (4c), f? is Eq. (8) andσ = ReLU +0.1 He3. Bottom row: Same as the top row but with linear-scale time and zoomed in.
Figure 5: Top Row: SGD dynamics of random-feature models as described in Section 4.2. The tar-get function and data distribution of (5a), (5b), (5c) are that of (4a), (4b), (4c) respectively. BottomRow: The corresponding linear-scale errors of kernel least-squares gradient flow for comparison.
Figure 6: Soft-error curves for Resnet-18 trained on CIFAR-5m taken from ref. Nakkiran et al.
Figure 7: Panel (7a): Data-augmentation for Resnet-18 on CIFAR-5m. (Fig. 5a from Nakkiran et al.
Figure 8: Each column replicates a kernel gradient flow experiment from Fig. 4 over d ∈{50, 100, 200, 400}. Column 1: Replicates Fig. 4a Column 2: Replicates Fig. 4a but with σε2 = 0.2.
