Figure 1: The overall framework of UVC, that integrates three compression strategies: (1) Pruningwithin a block: In a transformer block, we targeting on pruning Self-Attention head numbers (s(l,1)),neuron numbers within a Self-Attention head (rl,i) and the hidden size of MLP module (s(l,3)) as well.
Figure 2: The two sparsity levels for pruning within a block: the head dimension level as controlledby r(l,i), and the head number level as controlled by s(l,1). When reaching the same pruning ratio,neuron-level sparsity will not remove any head, which is usually unfriendly to latency; while headlevel sparsity will only remove attention heads, which is usually unfriendly to accuracy.
Figure 3: Visualizations of: (a) sparse attention head patterns for DeiT-Tiny (left) and DeiT-Base(right); (b) skip connection patterns for DeiT-Tiny (top) and DeiT-Base (bottom).
