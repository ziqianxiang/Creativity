Figure 1: The trust-region decomposition network. “MP” denotes message passing. Each agent ifirst encodes oit and ait and concatenates them with the local trust-region embedding , and gets theagent's embedding (Input Layer). Next, We construct agents as a weighted undirected graph, Cf * arecalculated in advance, and the updated agent’s embedding is obtained through the GNN Layers. Eachagent then obtains another agent’s embedding. Finally, we concatenate the two embeddings togetherand input them into KL-Encoder to estimate the current joint policy divergence (Prediction Layer).
Figure 2: The performance comparisons under coordination tasks with different complexity.
Figure 3: Upper: The averaged KL divergence of each agent with the orange and green linerepresenting MAAC and MAMT respectively; Bottom: The Dns of all agents.
Figure 4: Mean and variance of local trust-region of each agent. The y-axis range from 0.01 to 100.0.
Figure 5: Mean and variance of pairwise coordination coefficients. Different colors represent differentagents in (a) and (b). In addition, since there are fewer agents in Spread and Multi-Walker, the displayeffect of the radar chart is poor, so we use the histogram for a more precise display in (a) and (b).
Figure 6: The probabilistic graphical models of two different modelings of the joint policy. Left:Modeling the joint policy with the mean-field variation family; Right: Modeling the joint policy as apairwise Markov random field. Each node represents the action of agent i at timestep t.
Figure 7: In the Spread-3 environment, 3 different Markov random fields are generated due to thedifferent definitions of the reward function of each agent. Note that these are not all possible Markovrandom fields, but three typical cases.
Figure 8: The performance of different trust-region decompositions in different scenarios. Theseresults indicate the existence of a trust-region decomposition dilemma.
Figure 9: The attention weights of the different agents of different trust-region decomposition indifferent scenarios. These results indicate the existence of a trust-region decomposition dilemma.
Figure 10:	Coordination environments with increasing complexity.(a) Spread; (b) Multi-Walker; (c)Rover-tower; (d) Pursuit.
Figure 11:	The actor-network (and modeling policy) architecture, critic-network architecture, andtrust-region decomposition network architecture from left to right and from top to bottom.
Figure 12:	The proportional relationship between the number of episodes experienced by the MAMTand MAMT-PPO algorithms when they achieve the results in Table 10.
Figure 13:	The mean and variance of coordination coefficient of each agent in Rover-Tower environ-ment.
Figure 14:	The mean and variance of coordination coefficient of each agent in Pursuit environment.
Figure 15:	The mean and variance of coordination coefficient of each agent in Spread environmentand Multi-Walker environment. Different color represents different agent.
Figure 16: The averaged KL-divergence of each agent in Rover-Tower environments. Red linerepresents MAAC and green line represents MAMT.
Figure 17: The averaged KL-divergence of each agent in Pursuit environments. Red line representsMAAC and green line represents MAMT.
Figure 18: The averaged KL-divergence of each agent in Multi-Walker environments. Red linerepresents MAAC and green line represents MAMT.
Figure 19: The averaged KL-divergence of each agent in Spread environments. Red line representsMAAC and green line represents MAMT.
Figure 20: The averaged KL of each agent in Rover-Tower environments.
Figure 21: The averaged KL of each agent in Pursuit environments.
Figure 22: The averaged KL of each agent in Multi-Walker environments.
Figure 23: The averaged KL of each agent in Spread environments.
