Figure 1: Samples from the source and the target (hatched) distributions under benign (a) covariateand (b) label shifts. (a) X 〜Unif([0,1] X [0,1]) on the source and Xi 〜Beta(1, 2), i = 1,2 onthe target. Labels satisfy: P (Y = 1 | X = x) = P (χ2 + x2 + ε ≥ 1/4)where ε 〜N(0,0.01).
Figure 2:	(a) The misclassification risk on the target of the Bayes-optimal predictors for three valuesofπ1S. Notice that label shift does not necessarily lead to an increase in the risk. (b) Upper confidencebounds US(f) on the misclassification risk on the source obtained via several possible concentrationresults. For each sample size, the results are aggregated over 1000 random data draws. The variance-adaptive confidence bounds (predictably-mixed empirical-Bernstein and the betting-based one) aremuch tighter when compared against the non-adaptive one.
Figure 3:	(a) Proportion of null rejections when testing for an increase in the misclassification riskafter processing 2000 samples from a shifted distribution. The vertical dashed yellow line separatesnull (benign) and alternative (harmful) settings. Testing procedures that rely on variance-adaptiveconfidence bounds (CBs) have more power. (b) Average sample size from the target that was neededto reject the null. Tighter concentration results allow to raise an alarm after processing less samples.
Figure 4:	(a) Different lower confidence bounds (LCB) on the target risk under the i.i.d. assumption.
Figure 5: Examples of MNIST-C ((a)-(d)) and CIFAR-IO-C ((e)-(h)) images.
Figure 6: (a) Performance of the testing framework on MNIST-C dataset. Only the translationeffect is consistently harmful to the classification performance of a CNN trained on clean data. (b)Performance of the testing framework on CIFAR-10-C dataset. Only the most severe version ofthe fog lead to a significant degradation in performance measured by a decrease in coverage of aset-valued predictor trained on top of a model trained on clean data.
Figure 7: False alarm rate for the CLT and betting-based lower confidence bound (LCB) under:(a) fixed-time monitoring and (b) continuous monitoring. Note that both bounds control the falsealarm rate at a prespecified level δ = 0.1 under fixed-time monitoring. However under continuousmonitoring, the false alarm rate of the CLT bound quickly exceeds the critical level δ = 0.1. At thesame time, the betting LCB successfully controls the false alarm rate.
Figure 8: Adapting the CLT lower bound to continuous monitoring via performing corrections formultiple testing: (a) each time a batch of 25 samples is received, (b) each time a new sample isreceived. Under both settings, the CLT-based lower bound is more conservative than the betting-based, which, in testing terminology, means that the resulting testing framework has less power.
Figure 9: 50 runs of conformal test martingales (blue dotted lines) under harmful distribution shiftwith: (a) cold start (shift happens in the beginning), (b) warm start (shift happens in an early stageof a model deployment). The horizontal red dashed line outlines to the rejection threshold due toVille’s inequality. Even though warm start improves detection properties, only a small fraction ofconformal test martingales detects a shift that leads to more than 10% drop in classification accuracy.
Figure 10: 50 runs of conformal test martingales (blue dotted lines) under gradual distribution drifts:(a) slow and benign, (b) slow and harmful, (c) sharp and harmful. The horizontal red dashed lineoutlines to the rejection threshold due to Ville’s inequality. Note that conformal test martingalesconsistently detect only sharp distribution drifts. Moreover, conformal test martingales illustratesimilar behavior under (a) and (b) but the corresponding settings are drastically different.
Figure 11:	(a) Visualization of 4-class classification problem with all classes being equally likely;(b) localized classic Brier score `brier (equation 11); (c) localized top-label Brier score `brier-top (equa-tion 12); (d) localized true-class Brier score `brier-true (equation 13).
Figure 12:	(a) Relative increase for different versions of the Brier score in the multiclass settingunder label shift; (b) Relative increase for different versions of the Brier score in the multiclasssetting under label shift when attention is restricted to the area where classes highly intersect (cubewith vertices at (±1/2, ±1/2)). While in general, all three versions of the Brier score suffer similarlyon average, in the localized area where classes highly intersect, the top-label Brier score does notincrease significantly under label shift.
Figure 13:	(a) Simulated dataset with well-separated classes. Presence of label shift presumably willnot lead to a high absolute increase in the misclassification risk. (b) In contrast, when the classes arenot well-separated, presence of label shift presumably might hurt the misclassification risk.
Figure 14:	(a) Upper confidence bounds US (f) on the Brier score for the source domain. Sim-ilar to the misclassification risk, variance-adaptive confidence bounds are tighter when comparedagainst the Hoeffding’s one. For each fixed number of data points from the source domain used tocompute US(f), presented results are aggregated over 1000 random data draws. (b) Proportion ofnull rejections made by the procedure when testing for 10% relative increase of the Brier score. (c)Average sample size from the target distribution that was needed to reject the null. Invoking tighterconcentration results allows to raise an alarm after processing less samples from the target domain.
Figure 15: Lines of the same color correspond to 5 different CNNs. For each network, the resultsaggregated over 25 random runs of the testing framework for randomly permuted test data. Applyingtranslate effect is consistently harmful to the performance of CNNs trained on clean MNIST data.
Figure 16: (a) Average size of prediction sets for β1 = 0.05 and β2 = 0.1 and different types ofinput data. First, lower β, corresponding to higher desired coverage, leads to larger prediction setson average. Second, average size of the prediction sets increases when more corrupted images arepassed as input, thus reflecting uncertainty in prediction. (b) Results of running the framework whenβ1 = 0.05 is used to construct a wrapper. Observe that setting a lower prescribed error level β andthus enlarging resulting prediction sets partially mitigates the impact of corrupting images with thefog effect. However, the most severe form of such corruption still consistently leads to rejecting thenull. The bars around dashed and solid lines correspond to 2 standard deviations.
Figure 17: (a) Data samples from the source (red) and target (blue) distributions for the covariateshift simulation. (b) Logistic regression predictor learnt on the source distribution plotted along witha data sample from the target distribution. While learnt predictor clearly has high accuracy on thesource domain, it fails to approximate the true underlying data generating distribution. (c) Results ofrunning the framework when testing for a 10% increase in the misclassification risk. The frameworkdetects a harmful shift after processing only a small number of samples.
