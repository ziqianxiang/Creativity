Figure 1: ClimateGAN, a model that generates extreme floods (right) on street-level images (left).
Figure 2: The ClimateGAN generation process: first, the input x goes through the shared encoder E .
Figure 3: Example inferences of ClimateGAN, along with intermediate outputs. The first row showshow the Masker is able to capture complex perspectives and how the Painter is able to realisticallycontextualize the water with the appropriate sky and building reflections. On the second row, we cansee that close-ups with distorted objects do not prevent the Masker from appropriately contouringobjects. Finally, the last row illustrates how, in unusual scenes, the Masker may imperfectly capturethe exact geometry of the scene but the final rendering by the Painter produces an acceptable imageof a flood. More inferences including failure cases are shown in Appendix H.
Figure 4: Example inferences of ClimateGAN and comparable approaches on three diverse streetscenes from the test set. We can see that ClimateGAN is able to generate both realistic water textureand color, as well as a complex mask that surrounds objects such as cars and buildings. Comparableapproaches are often too destructive, producing artifacts in the buildings and sky (e.g. 1st row ofMUNIT) or not destructive enough, resembling more rain on the ground than high floods (e.g. 3rdrow of CycleGAN and 2nd row of InstaGAN).
Figure 5: Statistical inference tests of the ablation study. Shaded areas indicate metric improvement.
Figure 6: Evaluation metrics for a subset of the models studied in the ablation study and presentedin Table 1—models trained without pseudo labels or with DADA for the Masker are excluded—aswell as the two baselines for comparison. We show the median of the distribution with bootstrapped99 % confidence intervals. The shaded area highlights the best model.
Figure 7: Results of the human evaluation: the blue bars indicate the rate of selection of Climate-GAN over each alternative. The error lines indicate 99 % confidence intervals.
Figure 8: Bird’s eye view of the urban area (city) and rural area (outskirts of the city) of our simulatedworldThe suburban environment (Figure 9) is a residential area of 0.5 km2 with many individual houseswith front yards.
Figure 9: Bird’s eye views of the suburban area of our simulated worldTo gather the simulated dataset, we captured ‘before’ and ‘after’ (without/with) flood pairs from2000 viewpoints with the following modalities:•	‘before’ : non-flooded RGB image, depth map, segmentation map•	‘after’ : flooded RGB image, binary mask of the flooded area, segmentation mapThe camera was placed about 1.5m above ground, and has a field of VieW of 120°, and the resolutionof the images is 1200×900. At each viewpoint, we took 10 pictures, by varying slightly the positionof the camera in order to augment the dataset.
Figure 10: Sample data obtained at one spot for one camera position in our virtual world. Thetop row shows the modalities of the ‘before’ flood image: RGB image of the scene, depth mapand segmentation map; and the bottom row shows those obtained in the ‘after’ configuration: RGBimage, segmentation map and binary flood mask.
Figure 11: Samples from our simulated dataset in urban (top row), rural (middle row), and suburban(bottom row) areas.
Figure 12: Examples of labeled images from our test set.
Figure 13: Distribution of the three metrics for the selected best Masker. The annotations correspondto the images in Fig. 19.
Figure 14: Bootstrapped distribution of the 20 % trimmed means of the difference in edge coherencebetween models that included pseudo labels and their counterparts. Equivalent distributions wereobtained for all other techniques and metrics in the ablation study.
Figure 15: Distribution of the metrics—error, F05 score and edge coherence—of the complete setof models tested in the ablation study, 1-18, excluding the two baselines, whose distributions areshown in Fig. 6. The solid symbols indicate the median of the distribution and the error lines thebootstrapped 99 % confidence intervals. The shaded area highlights the best model—7.
Figure 17: Detailed diagram of the training procedure of ClimateGAN. We highlight how the sim-ulated and real data paths in the model are similar yet different. We use dashed arrows and boxesto emphasize that pseudo labels are only used as noisy signal in the beginning of training. All thelosses represented in the white rounded boxes are detailed in Section 3.2 and Appendix B.
Figure 18: More samples of the full ClimateGAN forward pass: input image, inferences from thedepth, segmentation and flood mask decoders of the Masker, followed by the masked input imagefed to the Painter and finally the flooded image output by the Painter. Notably, the Painter is ableto produce consistently contextualized water, with color and reflections relevant to the surroundingobjects and to the sky. While generally able to appropriately understand a scene’s perspective andcircumvent objects, the Masker is however sometimes unable to predict plausible flood masks (asillustrated in the bottom two rows). It is difficult to exactly understand the source of this becauseboth the depth and segmentation maps look generally appropriate.
Figure 19: Examples of images that obtained good and bad—within the 2nd and 98th quantiles,respectively—Masker’s predictions metrics. From top two rows to bottom two: error, F05 scoreand edge coherence. The first row of each metric corresponds to an image with good values ofthe metric. The white segments in the images illustrating the edge coherence indicate the shortestdistance between the predicted and the ground truth mask. The legend of the column ”Labels” is thesame as in Fig. 12, and the images can be identified in Fig. 13 by the letters on the left.
