Figure 1: The diagram of algorithms. The left side denotes the general Q-based offline RL methods.
Figure 2:  Trade-offs of EVL between gen-eralization  and  conservatism  in  a  randomMDP.  The  green  line  shows  the  optimalvalue and the blue line shows the value ofbehavior policy.  The curve is averaged over20 MDPs.
Figure 3:  A toy example in the random MDP. In both figures,  the color darkens with a larger τ(τ        0.6, 0.7, 0.8, 0.9  ).   The size of the spots is proportional to the relative scale of the thirdvariable: (a) Change nmₐₓ. From magenta to blue, nmₐₓ is set as 1, 2, 3, 4 in order. (b) Change thebehavior polices µ, where µ(s) = softmax(Q∗(s, )/α). From light yellow to dark red, the α is setas 0.1, 0.3, 1, 3 in order.
Figure 4:  Visualization of the value estimation in various AntMaze tasks. Darker colors correspondto the higher value estimation.  Each map has several terminals (golden stars) and one of which isreached by the agent (the light red star). The red line is the trajectory of the ant.
Figure 5: Comparison results between expectile loss and quantile loss on Adroit tasks.  We respec-tively name our algorithm with expectile loss and quantile loss as VEM and VEM (abs).
Figure 6:  The results of VEM (τ ) with various τ in Adroit tasks.  The results in the upper row arethe performance. The results in the bottom row are the estimation value.
Figure 7:  Comparison results between VEM with TD3+BC. We adopt different hyper-parametersα      0.5, 2.5, 4.5   in TD3+BC to test its performance.  The upper row are the performance.  Theresults in the bottom row are the estimation error (the unit is 10¹²).
Figure 8: The comparison between episodic memory and n-step value estimation on AntMaze tasks.
Figure 9: The comparison between VEM, BCQ-EM and BCQ on Adroit-human tasks.  The resultsin the upper row are the performance. The results in the bottom row are the estimation error, wherethe unit is 10¹³.
Figure 10: Comparison between fixed τ (VEM) and auto-tuning τ (VEM(auto)) in the door-humantask.
Figure 11:  Value estimation of VEM (nmₐₓ) in adroit-human tasks,  where nmₐₓ  is the maximalrollout step for memory control (see Equation 11). We set τ = 0.5 in all tasks.
Figure 12: The training curves of VEM and BAIL on D4RL tasks.
Figure 13:  The value estimation error of VEM on D4RL tasks.  The estimation error refers to theaverage estimated state values minus the average returns.
