Figure 1: Left: the cliff walking task, where the agent is supposed to go to the lower right corneras quickly as possible and avoid the grey region. The red arrows show the optimal policy. In thisexample the system has the height of 4 and the width of 10. Right: results of learning the cliffwalking task in a tabular setting, using randomly sampled state-action pair data. The upper plotsshow the result with width 10 and γ = 0.9, and the lower show the result with width 20 andY = 0.95. |Q - Q*|2 is the squared distance between the learned Q function and the optimal Q*.3Both Q-table learning and RG use the learning rate of 0.5, averaged over 10 repeated runs.
Figure 2: Results of cliffwalking in Fig. 1 with dif-ferent γ, with system’s widthof 10, averaged over 10 runs.
Figure 3: Left: one-way cliff walking task, where the agent starts atthe lower left corner, and at each step it is allowed to move to theright to obtain a reward of 2, or move up and terminate with a rewardof -1. It terminates upon reaching the goal. Right: performanceof the learned greedy policy and min Q(s, a) for online Q-tablelearning and RG, following the -greedy policy for different valuesof, with γ = 1 and a learning rate of 0.5, averaged over 100 trials.
Figure 5: Training Perfor-mance and loss on Space In-vaders when half of the dataare randomly discarded.
Figure 4: Training Performance and training loss on games Pong (left) and Space Invaders (right).
Figure 6: Training performance and training loss on Space In-vaders when the memory adopts a random replacement strategy(left) and when the memory is smaller and adopts different strate-gies (middle and right).
Figure 7: Training performance on several difficult games in Atari 2600, with learning rate 4 × 10-5 .
Figure 8: Performance on the wet-chicken benchmark training on a dataset generated by the randompolicy (left) and the distances among the learned Q functions (right). The experiment is repeated for10 times, and the standard error of the performance and the standard deviation of the distances areshown as the shaded regions.
Figure 9: Training performance of C-DQN and DQN on the task of measurement feedback coolingof quartic oscillators. The vertical axis shows the energy of the cooled quartic oscillator, and asmaller energy represents better performance. The horizontal axis shows the simulated time ofthe oscillator system that is used to train the agent. Each curve represents a separate trial of theexperiment.
Figure 10:	Training performance and training loss of C-DQN and DQN on several other Atari 2600games, using the same experimental settings as in Sec. 5.1.
Figure 11:	Training performance using different update periods of the target network on gamesSpace Invaders (left) and Hero (right). In the game Hero, there appears to be a local optimum withreward 13000 where the learning can fail to make progress, which is also seen in Fig. 15.
Figure 12: A screenshot of the gameSkiing in Atari 2600.
Figure 13: Training performance of C-DQN on Skiingwith learning rate 2 × 10-5, following the experimentalprocedure in Sec. 5.3. The standard deviation amongthe three runs are shown as the shaded region.
Figure 14: Training performance and loss for DQN on Space Invaders, with different hyperparam-eters p and following the prioritization scheme in Schaul et al. (2015). The loss is calculated bymultiPlying Wi and 'DQN for each sampled data.
Figure 15: Training performance for C-DQN on several games in Atari 2600 compared with thehuman performance (Badia et al., 2020) and the double DQN (Hessel et al., 2018), using the sameexperimental setting as in Sec. 5.3, except for using Cγ = 10.
