Figure 1: The trend of test accuracyimprovement (%) on CIFAR-10 by self-training on CIFAR-10 (labeled) with dif-ferent amount of unlabeled data from 80Million Tiny Images matches our theoret-ical prediction.
Figure 2: Adding unlabeled data in the em-pirical risk function drives its local minimumcloser to W*, which minimizes the generaliza-tion function.
Figure 3: Illustration of the (1) ground truth W*,(2) iterations {W(')}L=o, (3) convergent pointW(L), and (4) WN =、W* + (1 -川 W⑼.
Figure 4: The generalizationfunction against the distance tothe ground truth neural networkFitted curve of θ(^j)fey Theorem 13Number of unlabeled data (M)200	400	600	800	1000Figure 6: The convergence ratewith different M when N <N *.
Figure 6: The convergence ratewith different M when N <N *.
Figure 5: The relative erroragainst the number of unlabeleddata.
Figure 7: Convergence ratewith different δ.
Figure 9: Empirical phase transition ofthe curves with (a) M = 0 and (b) M =1000.
Figure 8:	tw*> -when λ and N change.
Figure 10: The test accuracy against thenumber of unlabeled data1∕√M	×1Q-3Figure 11: The convergence rate against thenumber of unlabeled data5	ConclusionThis paper provides new theoretical insights into understanding the influence of unlabeled data inthe iterative self-training algorithm. We show that the improved generalization error and conver-gence rate is a linear function of 1/√M, where M is the number of unlabeled data. Moreover,compared with supervised learning, using unlabeled data reduces the required sample complexity oflabeled data for achieving zero generalization error. Future directions include generalizing the anal-ysis to multi-layer neural networks and other semi-supervised learning problems such as domainadaptation.
Figure 11: The convergence rate against thenumber of unlabeled data5	ConclusionThis paper provides new theoretical insights into understanding the influence of unlabeled data inthe iterative self-training algorithm. We show that the improved generalization error and conver-gence rate is a linear function of 1/√M, where M is the number of unlabeled data. Moreover,compared with supervised learning, using unlabeled data reduces the required sample complexity oflabeled data for achieving zero generalization error. Future directions include generalizing the anal-ysis to multi-layer neural networks and other semi-supervised learning problems such as domainadaptation.
Figure 12: The landscapes of the objection function and population risk function.
Figure 13: The subspace spanned by Wj； and Wj[p]h(φ(w*τ x) - φ(w/]Tx))Xi[p], let us define the angle between Wj； and Wjas θ1 . Figure13 shows the subspace spanned by the vector Wj； and Wej. We divide the subspace by 4 pieces, wherethe gray region denotes area I, and the blue area denotes area II. Areas III and IV are the symmetriesof II and I from the origin, respectively. Hence, we haveEx [(φ(w*τx) - φ(w?]TX))x]=Eχ∈ areaI [(φ(w)Tx) - φ(w/TX))Xi + Eχ∈ areaII [(φ(w)Tx) - φ(w/]TX))Xi+ Ex∈ areaIII [(φ(WjTX) - φ(w∕ X))Xi + Ex∈ area IV[(0(w；TX)- φ(Wjp]TX))Xi=Eχ∈ areaI [(0(w；Tx) - Φ(w∕Tx))x] + Eχ∈ areaII [w；Tee] - Eχ∈ areaIII [w?]Txx]=Ex∈ area I (Wj； - Wj[p])TXX + Ex∈ area II (Wj； - Wj[p])TXX(95)29Published as a conference paper at ICLR 2022Therefore, we haveKλ2Eχ[(φ(w;Tx) - φ(wjp]τx))x] + 1--2λEx [(φ(WTe) - φ(wjp]τe))ei U=2Kκ2Ex [(w； - wjp] )Txx] + 12-λEx [(Wj- wjp])Txx]	(96)2K	2K	2
