Figure 1: The Gromov-Wasserstein distance enables us to compare the stationary state-action distri-butions of two agents with different dynamics and state-action spaces. We use it as a pseudo-rewardfor cross-domain imitation learning.
Figure 2: Isomorphic policies (definition 2) have the same pairwise distances within the state-actionspace of the stationary distributions. In Euclidean spaces, isometric transformations preserve thesepairwise distances and include rotations, translations, and reflections.
Figure 3: Given a single expert trajectory in the expert’s domain (a), GWIL recovers an optimal pol-icy in the agent’s domain (b) without any external reward, as predicted by theorem 1. The green dotrepresents the initial state position and the episode ends when the agent reaches the goal representedby the red square.
Figure 4: Given a single expert trajectory in the pendulum’s domain (above), GWIL recovers theoptimal behavior in the agent’s domain (cartpole, below) without any external reward.
Figure 5: Given a single expert trajectory in the cheetah’s domain (above), GWIL recovers the twoelements of the optimal policy’s isometry class in the agent’s domain (walker), moving forwardwhich is optimal (middle) and moving backward which is suboptimal (below). Interestingly, theresulting walker behaves like a cheetah.
Figure 6: The proxy reward introduced in equation 9 gives a learning signal that is easily optimizedusing a standard RL algorithm.
Figure 7: In sparse-reward environments, GWIL obtains similar performance than a baseline learnerminimizing the Wasserstein distance to an expert in the same domain.
Figure 8: In the sparse maze environment, GWIL requieres the same order of wall-clock time thana baseline learner minimizing the Wasserstein distance to an expert in the same domain.
Figure 9: A GWIL walker imitating a cheetah reaches a walking speed faster than a SAC walkertrained to run in terms of wall-clock time.
