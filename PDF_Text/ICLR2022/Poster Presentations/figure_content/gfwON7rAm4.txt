Figure 1: an MDP with normal-from potential Figure 2: an MDP with normal-form potentialgames at each state which is not an MPG due to games at each state which is an OMPG but not anconflicting preferences over states.	MPG despite common preferences over states.
Figure 3: A 2-player MDP which is not potential at every state but which is overall an MPG. Whilestate s0 corresponds to a zero-sum game, the states inside the dotted rectangle do form a potentialgame which can be used to show the MPG property whenever p0 does not depend on agentsâ€™ actions.
Figure 4: The 2-state MDP.
Figure 5: Policy gradient in the 2-state MDP with 8 agents of Section 5. In all runs, the 8 agentslearn one of the deterministic Nash policies that leads to the optimal distribution among states (left).
Figure 6:	Figures similar to Figure 5, except noW With c > cA > cB > cC > cD as described in thetext. Independent policy gradient requires more iterations to converge compared to the symmetricshift setting, but still arrives at the same Nash policy.
Figure 7:	Convergence to deterministic Nash policies of independent policy gradient in a variationof the MDP of Section 5 with N = 16 agents and Ai = 5 facilities, Ai = {A, B, C, D, E} withwA < wB < wC < wD < wE (i.e., E is the most preferable by all agents). Again, while there areseveral (symmetric) deterministic Nash policies, all of them yield the same distribution of agentsamong states (leftmost panel). All runs converge successfully to that outcome.
Figure 8: Convergence to deterministic Nash policies of independent policy gradient in two variationsof the MDP of Section 5 with stochastic transitions between states.
Figure 9: The stochastic congestion game with N = 4 agents and 8 nodes. In this discrete time MPG,all agents start at s and at each point in time, they move to one of the nodes of the next layer. Theirgoal is to reach t at a minimum cost (as expressed by the congestion on the the vertices). The leftpanel shows the starting state and the right panel shows one possible state after two transitions (withintermediate states in pale colors).
Figure 10: Convergence of independent policy gradient in the stochastic congestion game with N = 4and 6 vertices between source and target (3 layers of 2 vertices each) as shown in Figure 9.
