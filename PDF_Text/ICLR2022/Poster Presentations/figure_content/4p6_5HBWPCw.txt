Figure 1: The number of fetches and the inference time of GNNs are both magnitudes more thanMLPs and grow exponentially as functions of the number of layers. Left: neighbors need to befetched for two GNN layers. Middle: the total number of fetches for inference. Right: the totalinference time. (Inductive inference for 10 random nodes on OGB Products (Hu et al., 2020))3Q∙5QLL《SIU》©ELL euuajJu_P3lpj3u∙s"PONft3	PreliminariesNotations. For GML tasks, the input is usually a graph and its node features, which we write asG = (V , E ), with V stands for all nodes, and E stands for all edges. Let N denote the total number ofnodes. We use X ∈ RN ×D to represent node features, with row xv being the D-dimensional featureof node v ∈ V. We represent edges with an adjacency matrix A, with Au,v = 1 if edge (u, v) ∈ E,and 0 otherwise. For node classification, one of the most important GML applications, the predictiontargets are Y ∈ RN×K, where row yv is a K-dim one-hot vector for node v. For a given G, usuallya small portion of nodes will be labeled, which we mark using superscript L, i.e. VL, XL, and Y L .
Figure 2: The GLNN framework: In offline training, a trained GNN teacher is applied on the graph forsoft targets. Then, a student MLP is trained on node features guided by the soft targets. The distilledMLP, now GLNN, is deployed for online predictions. Since graph dependency is eliminated forinference, GLNNs infer much faster than GNNs, and hence the name “Graph-less Neural Network.”5	Graph-less Neural NetworksWe introduce GLNN and answer exploration questions of its properties: 1) How do GLNNs compareto MLPs and GNNs? 2) Can GLNNs work well under both transductive and inductive settings? 3)How do GLNNs compare to other inference acceleration methods? 4) How do GLNNs benefit fromKD? 5) Do GLNNs have sufficient model expressiveness? 6) When will GLNNs fail to work?5.1	The GLNN FrameworkThe idea of GLNN is straightforward, yet as we will see, extremely effective. In short, we train a“boosted” MLP via KD from a teacher GNN. KD was introduced in Hinton et al. (2015), whereknowledge was transferred from a cumbersome teacher to a simpler student. In our case, we generatesoft targets zv for each node v with a teacher GNN. Then we train a student MLP with both truelabels yv and zv. The objective is as Equation 1, with λ being a weight parameter, Llabel being thecross-entropy between y。and student predictions 欧心,Lteacher being the KL-divergence.
Figure 3:	Enlarged MLPs (GLNNs) can match GNN accuracy, but infer dramatically faster. Plots areunder the same setting as Figure 1. Left: inference time of MLPs vs. GNN (SAGE) for differentmodel sizes. Right: model accuracy vs. inference time. Note: time axes are log-scaled.
Figure 4:	Loss curves on CPF datasets show GLNN distillation can help to regularize the training.
Figure 5:	Left: Node feature noise. GLNN has comparable performance to GNNs only when nodesare less noisy. Adding more noise decreases GLNN performance faster than GNNs. Middle: Inductivesplit rate. Altering the inductive:transduCtive ratio in the production setting doesn,t affect the accuracymuch. Right: Teacher GNN architecture. GLNNs can learn from different GNN teachers to improveover MLPS and achieve comparable results. Accuracies are averaged over five CPF datasets.
Figure 6: The transductive setting and inductive setting illustrated by a 2-layer GNN. The middleshows the original graph used for training. The left shows the transductive setting, where the testnode is in red and within the graph. The right shows the inductive setting, where the test node is anunseen new node.
Figure 7: Model inductive performance comparison between MLP, GNN(SAGE), and GLNN underdifferent inductive split rate in the production setting.
Figure 8: Model transductive performance comparison between MLP, GNN(SAGE), and GLNNunder different inductive split rate in the production setting.
Figure 9: Inductive (predicted) label distribution on the A-computer dataset. Left: true labels.
