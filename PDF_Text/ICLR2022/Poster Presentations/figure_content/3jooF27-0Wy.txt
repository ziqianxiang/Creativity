Figure 1: The Flexible Size Continuous Kernel Convolution (FlexConv). FlexConv defines convo-lutional kernels as the multiplication of a continuous convolutional kernel MLPψ, with a Gaussianmask of local support WgaUSs： ψ(x, y) = WgaUSS(x, y; θmask) ∙ MLPψ(x, y). By learning the parametersof the mask, the size of the convolutional kernel can be optimized during training. See also Fig. 7.
Figure 2: Existing approaches increase the size of convolutional kernels via (learnable) parametricdilations, e.g., by deformation (b) or by Gaussian blur (c). However, dilation limits the bandwidth ofthe dilated kernel and with it, the amount of detail it can describe. Contrarily, FlexNets extend theirkernels by passing a larger vector of positions to the neural network parameterizing them. As a result,FlexConvs are able to learn high bandwidth convolutional kernels of varying size end-to-end (a).
Figure 3: The importance of dynamic sizes in continuous kernel convolutions. Consider a neuralnetwork predicting pixel values at each position. If the entire image is considered, the network mustuse part of its capacity to learn to predict zeros outside of the flower region, which in turn degrades thequality of the approximation in the region of interest (b). Importantly, the better the localization of theflower, the higher the approximation fidelity becomes. FlexNets learn the size of their convolutionalkernels at each layer during training, and thus (i) use the capacity of the kernel efficiently, (ii) convergefaster to good approximations, and (iii) are faster in execution -Via dynamic cropping-.
Figure 4: Left: Final MSE after fitting each model to Gabor filters of different frequencies. N-Jets can-not fit high frequencies. Right: Kernels learned by each model. SIREN and MAGNet can fit all targets.
Figure 5: Alias-free FlexNet-16 on CIFAR-10. We report change in accuracy between source andtarget resolutions, directly after upsampling (left) and after fine-tuning (right) (means over five runs).
Figure 6: Learned FlexConv masks for FlexNets with 3, 5 and 7 residual blocks. FlexNets learn verysmall kernels at shallow layers, which become larger as a function of depth.
Figure 7: Example kernels, generated step by step. FlexConv samples a kernel from MLPψ (a), whichis attenuated by an anistropic Gaussian envelope with learned parameters θ(l) (b), creating (c) whichis cropped to contain only values of > 0.1 (d).
Figure 8: Example kernels from FlexNet-16 models trained (i) without regularization, (ii) withaliasing regularization of fM+AGNet, (iii) with aliasing regularization of fF+lexConv. In the columns, fromleft to right: (i) original kernel at 33 × 33, (ii) FFT of the original kernel, (iii) kernel inferred at 65 × 65,to find aliasing effects, (iiii) FFT of the 65 × 65 kernel, with the solid line showing the Nyquistfrequency of the 33 × 33 kernel, and the red dotted line showing the maximum frequency componentas computed by our analysis. For fF+lexConv the maximum frequency matches almost exactly withthe Nyquist frequency, showing that our aliasing regularization works. For fM+AGNet, the maximumfrequency is slightly higher than the Nyquist frequency, as the FlexConv mask is not included in thefrequency term derivation. This is reflected in the slightly worse resolution generalization resultsreported in Sec. 4.3. Furthermore, some aliasing effects are still apparent for the aliasing regularizedmodels, as discussed in Sec. 6.
Figure 9: Decomposition of a Gabor filter and its frequency spectrum. Top row: a decomposition of aGabor filter (right) into its Gaussian term (left) and its sine term (center). Bottom row: frequencyresponses for each respective filter. The Fourier transform of a Gaussian envelope is a Gaussianenvelope (blue circles show σF for h = {1, 2}). The Fourier transform of a sine pattern is a collectionof symmetrical impulse signals (red box shows the Nyquist frequency). The Gaussian envelope blursthe frequency response of the sine term (purple boxes show the frequency response for h = {1, 2, 3}).
Figure 10: FlexNet architecture. FlexNet-L consists of L FlexBlocks, where each FlexBlock is aresidual block of FlexConvs.
