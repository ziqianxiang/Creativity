Figure 1: Semantic segmentation with learnable downsampling operation on cancer histology images. Toprow: Our deformation module learns to adapt the sampling locations spatially (blue dots on the images) accordingto their utility for the downstream segmentation task and produces a deformed downsampled image (bottom-leftcorner), which leads to a more accurate low-resolution segmentation. Middle row: high-resolution segmentationreversed from low-resolution predictions. Bottom row: Compare to uniform downsampling (referred to as"Uniform"), our deformed downsampling samples more densely at difficult regions (yellow arrows) and ignoreimage contents that are less important (green arrows). We also demonstrate our method on the street and aerialbenchmarks (video and code link).
Figure 2: Architecture schematic. For each high-resolution image X ∈RH×W×C, we compute its lower reso-lution version Xlr ∈Rhd×wd×C. The deformation module, Dθ, parametrised by θ, takes the low-resolution imageXlr as input and generates a deformation map d = Dθ (Xlr), where d ∈ Rhd×wd×1, that predicts the samplingdensity at each pixel location. Next, the deformed sampler Gd is constructed by taking both X and d as input andcomputes the downsampled image 文=Gd(X,d), where 文 ∈ RhxwXC and sampling locations are shown as reddots masked on the image. The downsampled image X is then fed into the segmentation network to estimate thecorresponding segmentation probabilities P = Sφ(X). During training the label Y is downsampled with the samedeformed sampler to getY=Gd(Y,d) and {θ,φ} are jointly optimised by minimise the segmentation specificloss Ls (θ,φ[X,Y) = LS (θ,φ;P,Y). At inference time, the low-resolution prediction Sφ (X) is non-uniformlyupsampled to the original space through deterministic reverse sampler Gd-1() and interpolation function Π()).
Figure 3: Optimal sampling density varies over locations. We simulating a set "edge-based" samplers eachat different sampling density around edge, and evaluate class-wise performance in (a) IoU (Y0 ,Y) and (b) IoU.
Figure 5:	Class-wise IoU on three datasets. IoU gain indicates improvements from our method over "uniform"baseline. Classes are ordered with increasing pixel frequency which is also indicated in each plot.
Figure 6:	Cost-performance trade-offs in mIoU and mIoU(Y ,Y) on three datasets. Cost determines theamount of computation required at segmentation. The red lines indicates at same performance our method cansave 33%, 90% and 70% from the "uniform" baseline on the three datasets respectively.
Figure 7: Examples on Cityscapes (Cordts et al., 2016) comparing our method against both baselines, wheresegmentation is performed on 16 times downsampled images (at each dimension). Predictions are masked overand sampling locations are shown in white dots. Yellow/ red arrows indicated regions denser/ sparser samplinghelped to segment rider (red)/ sky (blue) classes, respectively.
Figure 8: Qualitative example on DeepGlobe dataset (Demir et al., 2018) where segmentation performed on8 times downsampled image (at each dimension). Predictions are masked over and sampling locations are shownin red dots. Yellow/ green boxed region indicated regions denser/ sparser sampling helped segmenting water(blue)/ forest (purple) classes, respectively.
Figure 9: (a) Experiments performed at comparable condition to reported "edge-based" results by Marin et al.
Figure 10: Visual comparison of our jointly trained downsampling against "interpolation" baseline(Talebi & Milanfar, 2021) on Cityscapes dataset where inputs of size 1024 × 1024 pixels aredownsampled to 64 × 64 pixels. Sampling locations are masked over prediction as white dots.
Figure 11: Semantic segmentation with learned deformed downsampling on Cityscapes. Our method learnsnon-uniform downsampling (white dots on the images) a high-resolution image (top-row) to preserve informationthat guides segmentation and ignore image content that does not contribute (green arrows) thereby producinga deformed downsampled image (bottom-left corner) which leads to more accurate low-resolution segmentation.
Figure 12: Qualitative example on PCa-Histo dataset (1) where segmentation performed on 50 times (at eachdimension) downsampled image. Predictions are masked over and sampling locations are shown in blue dots.
Figure 13: Class frequency change in the ratio between the sampled and original image. Higher value indictingmore sampling pixels are "invested" in one class with the learned sampler. Results suggest our method can learn tosampler denser at semantic important classes while ignoring those not contribute. Results are from three datasets:(a) a local prostate cancer histology dataset (PCa-Histo) at the dynamic downsampled size of 80 × 800 pixels(b) Cityscapes (Cordts et al., 2016) at the downsampled size of64 × 128 pixels and (c) DeepGlobe (Demir et al.,2018) at the downsampled size of 300 × 300 pixels.
Figure 14: The segmentation accuracy with a set simulated "edge-based" downsamplers. Each bar is thesegmentation accuracy ofa segmentation network trained with one simulated "edge-based" downsampler. Eachdownsampler is simulated with a different sampling density around edges, represented by blur radius. The lowblur radius indicates dense sampling around edges. The best-performed result is selected and referred to as the"edge-based" result presented in the main paper.
