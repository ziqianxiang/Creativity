Figure 1: Illustration of the Overall Architecture. Left: Action-awareness CycleGAN (A-CycleGAN): The image translation model establishes the bidirectional mapping between the humandemonstrations and the imagined videos of the robot. Then, the inverse dynamic model takes asinput the latent state to predict the corresponding robot action ar. The CycleGAN part and InverseDynamic Model part share the encoder layers of generator F (shown in pink). Right: Self-AdaptiveMeta-Imitation Learning. The Meta-Imitation Learning structure is similar to DAML and the meta-policy is trained With the data from the latent states lr and predicted actions ^r. The translatedimagined robot videos dr are evaluated by βqua to adaptively adjust the meta-ObjeCtive.
Figure 2: Examples of simulated drawing (left), pushing (middle), and real-world pushing (right)tasks. The top row shows the source domain demonstration, while the bottom shows policy execu-tion in target domain.
Figure 3: All of the objects used for training and evaluation for pushing task.
Figure 4: All simulated testing visualization results. From left to right: a demonstration from thesource domain, the policy execution in target domain. Note that the distractor is inconsistent fordemo and executed scenes in pushing tasks.
Figure 5: Translated images comparison on the simulated drawing and pushing task.
Figure 6: The real-world testing visualization results.
Figure 9: Instances of failure in real-world pushing tasks.
Figure 11: The examples of the translated videos on all tasks.
Figure 12: The correlation between the adaptive metric βqua and translation error e.
Figure 13: The test success rate on simulated pushing task as a function of the paired video numbers.
