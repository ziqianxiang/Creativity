Figure 1: These figures show how embeddings in the model flow through layers (bottom to top) and throughtime (left to right). There are two possible event types, e and f, which represent email messages. At the upperright corner of each figure, We obtain their modeled intensities at a certain time t, λe(t) and λf (t), based onthe embeddings of the three previous, irregularly spaced observed events. This requires embedding e. and f attime t as if they were observed. If either one actually occurs at time t, we will keep its embeddings, which willthen affect embeddings of events at times > t. Figure (a) shows the basic model of section 3, in which eachevent’s embedding at layer ' depends (->) on all preceding events at layer ' — 1. (The dashed arrows -->reflect residual connections.) Section 4 explains that the e -f f influences can be prevented by dropping therule f <- e. Figure (b) shows an A-NDTT model from section 5, in which the company forum's embedding atlayer ' depends (->) on all preceding events at layer ' — 1 (via < rules). The events or possible events at layer' do not depend directly on preceding events; instead, their embeddings at time t are derived (->) from theforum’s embedding at time t (via :- rules).
Figure 2: Evaluation re-sults (smaller is better) With95% bootstrap ConfidenCeintervals6 on the tWo real-World datasets, ComparingTHP, SAHP, and NHP Withour A-NHP model. RMSEevaluates the prediCted timeof the next event (root meansquared error), While errorrate evaluates its prediCtedtype given its time.
Figure 3: Evaluation re-sults With 95% bootstrapconfidence intervals6 onthe RoboCup and IPTVdatasets. Evaluation meth-ods are the same as inFigure 2. Note that thetraining objective Was log-likelihood.
Figure 4: The solid green arrows correspond to instantiations of the attentional <- rules 8 and 13. They cancapture the real-world property that thanks to Eve’s joke at time t5, the sales forum still feels more humorous attime t and Frank, another member of that forum, is still in a good mood. This raises the probability λe (t)dtthat Frank posts his own joke at time t, where e = message (frank,sales,joke). More formally, λe(t) isdetermined by the layer-L embedding of Frank's possible message e@t. In general, the layer-' embedding ofthis message reflects the layer-(' — 1) embeddings of both Frank and the forum at time t, as well as the factthat the possible message is a joke. If the message is actually sent (i.e., the possible event actually happens), itslayer-' embedding would in turn affect the layer-(' + 1) embeddings of the forum and its readers at times > t.
Figure 5: Log-likelihood on held-out data (in nats, with 95% bootstrap confidence intervals6). Larger values arebetter. Each column is a different experiment, on a single synthetic dataset generated from a different distributionfamily (shown at the bottom of the column). Within each column, the red dashed horizontal line represents thelog-likelihood of the true distribution that generated the data. Within each column, we train and test 4 models:THP, SAHP, NHP, and A-NHP (from left to right). The model from the correct family is shown in red; comparethis to our A-NHP model (the rightmost model). Other models are shown in lighter ink. Note that log-likelihoodfor continuous variables can be positive (as in the second row), since it uses the log of a probability density thatmay be > 1.
Figure 6: Results of NDTT and A-NDTT in Figure 3a broken down by action types, with horizontal and verticalerror bars, respectively.
Figure 7: Replications of Figure 6 (one per row) with different random seeds used during training.
