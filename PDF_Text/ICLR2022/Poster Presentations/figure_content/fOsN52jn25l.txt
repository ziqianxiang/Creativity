Figure 1: Comparisons between DLTH with other training strategies. 3 times average scores withstandard deviation shown as shadow are plotted. Both (a)/(b) (CIFAR10/CIFAR100) demonstrateours superiority in all non-pretrain based approaches (solid lines). Two pretrain based methods (dashlines) are plotted for reference. (This figure is best viewed in color.)(GraSP) Wang et al. (2020a) regards the gradient flow as an importance factor and correspondinglyuse it to design the PI criterion.
Figure 2: Diagram of Lottery Ticket Hypothesis (LTH) and Dual Lottery Ticket Hypothesis (DLTH).
Figure 4: Ablation study for iterative cycle number of DLTH. (a)/(b) show the 3 times averageperformance on CIFAR10/CIFAR100 with sparsity ratio from 0.5 to 0.98 and cycle number from 1to 5. Different colors represent different sparsity ratios. (This figure is best viewed in color.)Comparison with GraSP. We compare RST with Gradient Signal Preservation (GraSP) Wang et al.
Figure 5: Visualization of test accuracy comparison of LTH and DLTH on CIFAR10 usingResNet56. Our DLTH general outperforms LTH and achieves comparable performance for 98%pruning ratio.
Figure 6: Visualization of test accuracy comparison of LTH and DLTH on CIFAR100 usingResNet56. Our DLTH general outperforms LTH for all sparsities.
Figure 7: Visualization of test accuracy comparison of training from scratch and DLTH on CIFAR10using ResNet56. Our training strategy consistently surpasses training from scratch on differentpruning ratios. The performance gain increases when the pruning ratio becomes larger.
Figure 8: Visualization of test accuracy comparison of training from scratch and DLTH on CI-FAR100 using ResNet56. Our training strategy consistently surpasses training from scratch ondifferent pruning ratios. The performance gain increases when the pruning ratio becomes larger.
Figure 9: ResNet56 layer-wise sparsity ratio of different overall sparsity on CIFAR10 dataset. Theselayer-wise ratios are obtained by GraSP Wang et al. (2020a) algorithm. The last layer is the finalfully-connected layer whose value is relatively special compared with others. The layer-wise spar-sity generally starts from around 50% and increases when layer index increases for different overallsparsity (except for 50% whose layer-wise sparsity are distributed around 50% for all layers).
Figure 10: ResNet56 layer-wise sparsity ratio of different overall sparsity on CIFAR100 dataset.
