Figure 1: We conduct adversarial training on data protected by different types of noise with variedadversarial perturbation radius ρa. EM denotes error-minimizing noise, TAP denotes targeted adver-sarial poisoning noise, NTGA denotes neural tangent generalization attack noise, and REM denotesrobust error-minimizing noise. The curves of test accuracy vs. radius ρa are plotted. The resultsshow that as the radius ρa increases, (1) the protection brought by EM, NTGA and TAP graduallybecome invalid, and (2) the proposed REM can still protect data against adversarial learners.
Figure 2: The training loss curves of ERM training and adversarial training on CIFAR-10. Lowertraining losses suggest stronger unlearnability of data. The results show that: (1) error-minimizingnoise could not reduce the training loss as effectively as that in ERM training; (2) robust error-minimizing noise can preserve the training loss in significantly low levels across various learningscenarios. These observations suggest that robust error-minimizing noise is more favorable in pre-venting data from being learned via adversarial training.
Figure 3: Visualization results of different types of defensive noise as well as the correspondinglycrafted examples. EM denotes error-minimizing noise, TAP denotes targeted adversarial poisoningnoise, NTGA denotes neural tangent generalization attack noise, and REM denotes robust error-minimizing noise.
Figure 4: Visualization results of CIFAR-10. Examples of data protected by error-minimizingnoise (EM), targeted adversarial poisoning noise (TAP), neural tangent generalization attack noise(NTGA), and robust error-minimizing noise (REM). When ρu is set as 8/255 or 16/255, the adver-sarial perturbation radius ρa of REM is set as 4/255 or 8/255.
Figure 5: Visualization results of CIFAR-100. Examples of data protected by error-minimizingnoise (EM), targeted adversarial poisoning noise (TAP), neural tangent generalization attack noise(NTGA), and robust error-minimizing noise (REM). When ρu is set as 8/255 or 16/255, the adver-sarial perturbation radius ρa of REM is set as 4/255 or 8/255.
Figure 6:	Visualization results of ImageNet subset. Examples of data protected by error-minimizingnoise (EM), targeted adversarial poisoning noise (TAP), neural tangent generalization attack noise(NTGA), and robust error-minimizing noise (REM). When ρu is set as 8/255 or 16/255, the adver-sarial perturbation radius ρa of REM is set as 4/255 or 8/255.
Figure 7:	The curves of train and test accuracies to training iteration on data protected by differ-ent defensive noises. The defensive perturbation radius PU for every noise is set as 8/255, whilethe adversarial perturbation radius Pa for REM is set as 4/255. Besides, the adversarial trainingperturbation radius is set as 4/255 in every experiment.
Figure 8:	We conduct adversarial training on data protected by different robust error-minimizingnoise (REM). The curves of test accuracy vs. adversarial training radius are plotted. The defensiveperturbation radius ρu for every REM noise is set as 8/255.
