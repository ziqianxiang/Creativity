Figure 1: Automatic Dual Amortized Variational Inference (ADAVI) working principle. On the leftis a generative HBM, with 2 alternative representations: a graph template featuring 2 plates P0 , P1of cardinality 2, and the equivalent ground graph depicting a typical pyramidal shape. We note B1 =Card(P1) the batch shape due to the cardinality of P1. The model features 3 latent RV λ, κ andΓ = [γ1, γ2], and one observed RV X = [[x1,1, x1,2], [x2,1, x2,2]]. We analyse automatically thestructure of the HBM to produce its dual amortized variational family (on the right). The hierarchicalencoder HE processes the observed data X through 2 successive set transformers ST to produceencodings E aggregating summary statistics at different hierarchies. Those encodings are then usedto condition density estimators -the combination of a normalizing flow F and a link function l-producing the variational distributions for each latent RV.
Figure 2: panel (a): inference amortization on the Gaussian random effects example defined ineq. (6): as the number of examples rises, the amortized method becomes more attractive; panel (b):graph templates corresponding to the HBMs presented as part of our experiments.
Figure 3: Scaling comparison on the Gaussian random effects example. ADAVI -in red- maintainsconstant parameterization as the plates cardinality goes up (first panel); it does so while maintainingits inference quality (second panel) and a comparable amortization time (third panel). Are comparedfrom left to right: number of weights in the model; closeness of the approximate posterior to theground truth via the ELB median -that allows for a comparable numerical range as G augments;CPU amortization + inference time (s) for a single example -this metric advantages non-amortizedmethods. Non-amortized techniques are represented using dashed lines, and amortized techniquesusing plain lines. MF-VI, in dotted lines, plays the role of the upper bound for the ELBO. Resultsfor SNPE-C and NPE-C have to be put in perspective, as from G = 30 and G = 300 respectivelyboth methods reach data regimes in which the inference quality is very degraded (see table 2).
Figure 4: Results for our neuroimaging experiment. On the left, networks show the top 1% con-nected components. Network 0 (in blue) agrees with current knowledge in semantic/phonologic pro-cessing while network 1 (in red) agrees with current networks known in language production (Heimet al., 2009; Zhang et al., 2020). Our soft parcellation, where coloring lightens as the corticalpoint is less probably associated with one of the networks, also agrees with current knowledgewhere more posterior parts are involved in language production while more anterior ones in seman-tic/phonological processing (Heim et al., 2009; Zhang et al., 2020).
