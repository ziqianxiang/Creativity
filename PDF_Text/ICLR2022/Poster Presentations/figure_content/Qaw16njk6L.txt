Figure 1: An illustration of our ViT search space. MBConv refers to inverted residual blocks (Sandleret al., 2018). All CNN and transformer blocks contain a stack of dynamic layers with searchablearchitecture configurations. Additionally, we also search for the input resolutions.
Figure 2: (a-b) show the training curves of the smallest sub-network and the largest sub-network (i.e.,the supernet), respectively. Note that AlphaNet is trained without external teacher models.
Figure 3: A basic transformer layerwith scaling. Activated compo-nents are the neurons selected in theforward path for one sub-network.
Figure 4: Comparison with prior-art CNNs andViTs on ImageNet. Here “+T” indicates meth-ods that are trained with external teacher models.
Figure 5: Results of our method and baselines on semantic segmentation. (a-b) show the results onthe Cityscapes and ADE20K validation set, respectively.
Figure 6: A demonstration of our self-attention module. ‘RPE’, ’Dw Conv, ‘Talking Head’, ‘Proj’ and‘MLP’ refer to relative positional embedding, depth-wise convolutional layer, talking head attention,projection layer and MLP layer, respectively.
