Figure 1: (Left) An example program. NNθ is a neural network with parameters θ. DIFFAI fails tolearn safe parameters for this program. (Right) The program as an STS. Each location `i correspondsto line i in the program. For brevity, the updates only depict the variable that changes value.
Figure 3: The provably safe portion and the test data loss of Ablation, DiffAI+ and Dse whenvarying the size of the data to train.
Figure 4: Trajectories during training. Each row exhibits the concrete trajectories and symbolictrajectories of one case from different methods. From top to bottom, the cases are Thermostat (Fig-ure. 4a, 4b, 4c, 4d), AC (Figure. 4e, 4f, 4g, 4h), Racetrack with position property (Figure. 4i, 4j, 4k,4p) and distance property (Figure. 4m, 4n, 4o, 4p), and Cartpole (Figure. 4q, 4r, 4s, 4t). Each figureshows the trajectories (concrete: red, symbolic: green rhombus) of programs learnt by the methodfrom different training stages, which is denoted by “Middle” and “Final”. We separate the inputstate set into 100 components (81 components for Cartpole) evenly to plot the symbolic trajectoriesclearly. During evaluation, we separate the input set into 10000 components (204 components forCartpole) evenly to get more accurate symbolic trajectories measurement.
Figure 5: Aircraft Collision123456789101112131415thermostat(x):N = 20isOn = 0.0i=0
Figure 6: Thermostat17Published as a conference paper at ICLR 2022123456789101112131415racetrack(x):x1, y1,
Figure 7: Racetrack18Published as a conference paper at ICLR 2022A.5 Synthetic MicrobenchmarksFigure 8 exhibits the 5 synthetic microbenchmarks in our evaluation. Specifically, the neural networkmodules are used as conditions in pattern 1 and pattern 2. The neural network modules serve as bothconditions and the assignment variable in pattern 3, pattern 4 and pattern 5. To highlight the impactfrom the safety loss, we only train with safety loss for synthetic microbenchmarks.
Figure 8: Programs for Patterns19Published as a conference paper at ICLR 2022A.6 Data GenerationIn this section, we provide more details about our data generation process. We give our ground-truthprograms and the description of each benchmark:•	Thermostat: Figure 6 exhibits the program with initial temperature x ∈ [60.0, 64.0] intraining, where a 20-length loop is involved. The COOLING and WARMING are twodifferentiable functions updating the temperature, x. The πθcool and πθheat are two lin-ear feed-forward neural networks. We set the safe temperature to the area [55.0, 83.0].
Figure 9: Training performance of data loss on different benchmarks varying data points. The y-axisrepresents the safety loss (C# (θ)) and the x-axis gives the number of training epochs. Overall, DSEconverges within 1500, 1000 , 6000, and 2000 epochs for Thermostat, AC, Racetrack, Cartpole,while DiffAI+ easily gets stuck or fluctuates on these benchmarks.
Figure 10: Training performance of data loss on different benchmarks varying data points. They-axis represents the data loss (Q(θ)) and the x-axis gives the number of training epochs. SinceThermostat is initialized with data trained programs to give quicker convergence, the data loss doesnot change a lot during training. For other benchmarks, data loss of Dse and DiffAI+ are bothconverging. Overall speaking, Dse sacrifices some data loss to give safer programs.
Figure 11: Training and Test Results of ThermostatData Size	Approach	Q	C#	Test Data Loss	Provably safe Portion150	Dse	0.60	0.0	0.85	0.99	DiffAI+	0.53	21.46	0.87	0.22750	Dse	0.64	0.0	0.86	1.0	DiffAI+	0.57	20.66	0.87	0.261500	Dse	0.58	0.0	0.84	1.0	DiffAI+	0.53	28.28	0.87	0.173750	Dse	0.62	0.0	0.85	1.0	DiffAI+	0.53	26.31	0.87	0.217500	Dse	0.61	0.0	0.86	1.0	DiffAI+	0.52	27.23	0.88	0.27Figure 12: Training and Test Results of ACData size	Approach	Q	C#	Test Data Loss	Provably safe Portion200	Dse	0.22	1.12	0.28	0.66	DiffAI+	0.11	2.04	0.31	0.01000	Dse	0.22	0.0	0.25	0.99	DiffAI+	0.17	2.03	0.26	0.02000	Dse	0.21	0.0	0.25	0.99	DiffAI+	0.17	2.03	0.25	0.0
Figure 12: Training and Test Results of ACData size	Approach	Q	C#	Test Data Loss	Provably safe Portion200	Dse	0.22	1.12	0.28	0.66	DiffAI+	0.11	2.04	0.31	0.01000	Dse	0.22	0.0	0.25	0.99	DiffAI+	0.17	2.03	0.26	0.02000	Dse	0.21	0.0	0.25	0.99	DiffAI+	0.17	2.03	0.25	0.05000	Dse	0.22	0.0	0.27	1.0	DiffAI+	0.17	2.03	0.24	0.010000	Dse	0.22	0.0	0.24	0.99	DiffAI+	0.13	2.26	0.24	0.0Figure 13: Training and Test Results of Racetrack24Published as a conference paper at ICLR 2022A.9 Additional Pattern AnalysisFigure 2 exhibits safety performance of learnt patterns. The details of the patterns are in Figure 8.
Figure 13: Training and Test Results of Racetrack24Published as a conference paper at ICLR 2022A.9 Additional Pattern AnalysisFigure 2 exhibits safety performance of learnt patterns. The details of the patterns are in Figure 8.
Figure 14:	Results from Thermostat with Three Branches in Each Iteration.
Figure 15:	Results from Thermostat with 40-Length Loop.
Figure 16:	Results from Thermostat with Super Refined Input Size.
Figure 17:	Results from AC with Convolutional LayersWith AC, we use a NN with Conv1d(1, 1, 2)-ReLU()-Conv1d(1, 1, 2)-ReLU()-Linear(4, 32)-ReLU()-Linear(32, 6)-Sigmoid(), where Conv1d(X, Y, Z) means an input channel of X, an outputchannel of Y and a kernel size of Z and Linear(X, Y) means an input channel of X and an outputchannel of Y.
Figure 18: Results of Cart-Pole(angle)As indicated by the above figure, pure imitation learning (Ablation) can already learn a pole keepingupright. To show Dse ’s ability to train a safe program without too much help from the data loss,we add another experiment following the same setting above except the constraint, which is shownin the main paper.
Figure 19: Results from Highly Unsafe DataIn summary, Dse can scale to programs with reasonable length, branches and different neural net-work architectures. One challenge in Dse is that learning becomes harder when symbolic state rep-resentation becomes more refined(including super refined input space, safety constraint, and branchsplitting). We leave this open for future works to seek to identify better tradeoffs between precisionof symbolic states and ease of learning.
