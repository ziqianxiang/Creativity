Figure 1: Pareto curves of the same DEQwith different solvers on WikiText-103language modeling (on 1 GPU).
Figure 2:	2a: The canonical Anderson solver is based on a local least-squares solution at eachiteration, with β = βk set to a constant. 2b: Our neural fixed-point solver provides a better initialguess z[0] and learnable iterative updates.
Figure 3:	The training procedure of the neural deep equilibrium solver. With a given fθ and inputx, We optimize the hypersolver parameters ω = {φ,ξ} Via losses applied on the HyperAndersoniterations and the initializer (see Sec. 4.2).
Figure 4: 4a- 4c: Comparisons OfDEQS with classic and neural solvers. All speed/accuracy curveswithin the same plot are benchmarked on the same GPU with the same experimental setting, averagedover 6 independent runs. 4d: The overhead of DEQ hypersolver is extremely small.
Figure 5: Ablative studies on HyperDEQ (reg.).
Figure 6:	Visualization of the neural equilibrium solver design. We use a local context and shallowlayer to build the initializer, and perform global pooling over the input (1D sequences or 2D images)to create compressed (but representative) versions of the iterate G[k].
Figure 7:	Further ablations on the neural solver design (in terms of the α prediction and losscomponents. Note that we use an unregularized DEQ here (in contrast to Fig. 5) to better demonstratethe curve differences, which could sometimes be small.)In particular, we set p ≪ d (e.g., in the WikiText-103 experiment, d = 700 and p = 100) so thatthis module requires little parameters and is fast to evaluate. We reiterate here that this is possiblebecause the goal of the initializer is not to solve the original problem (e.g., high-dimensional languagemodeling), but only to give a “reasonable” initial guess to the fixed point solver. In other words,the initial guess itself may be bad in a task perspective (as we verified in Table 1) but good in anoptimization perspective (see Fig. 5).
Figure 8:	Left: Convergence analysis of the hypersolver at inference time. Right: Although trainedwith a frozen fθ, the neural equilibrium solver can also be used to accelerate DEQ training.
Figure 9: The benefit of hypersolver holds ontiny implicit models. Inference speed is bench-marked with batch size 16, averaged over 5 runs.
