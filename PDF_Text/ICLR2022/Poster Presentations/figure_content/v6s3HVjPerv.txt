Figure 1: We tested whether users can identify the class-relevant features of images showing two typesof animals. We biased attributes like the animal’s color to be predictive of the class and investigatedwhether explanation techniques enabled users to discover these biases. We tested a simple baseline (a)which shows random samples grouped by the model’s output logit, counterfactual samples generatedby an invertible neural network (b), and automatically discovered concepts (c). A participant viewedonly one of the above conditions.
Figure 2: The left panel depicts the main difference between Peeky and Stretchy: the legs’ position.
Figure 3: The joint distributions of legs’ position and the attributes background (left), shape (middle),and color (right). Datapoints are yellow for Peekies and blue for Stretchies. The background is notbiased. The shape is biased for legs’ position lower than (0.45) or greater (0.55), but is uniform inthe center. The color contains additional predictive information about the target class, as it allowto discriminate between Peeky and Stretchy where the legs’ position overlaps. However, for moreextreme arms’ positions the color is uniform and not biased.
Figure 4: The proportion of correctanswers for baseline (BASE), con-cepts (CON), and INN.
Figure 5: Attribute changes along counterfactual interpolations as measured by an observer convnet.
Figure 6: All attribute values as predicted by an observer convnet for sequences of counterfactualinterpolations. Each line corresponds to a single sample whose logit score is modified through linearinterpolations in classifier space.
