Figure 1: Mean regrets for Experiment 1 (Sec-tion 4.1). Regret is normalized to [0, 1] and Tas fractions of NXNCtar .
Figure 2: Mean regrets for Experiment 2 (Sec-tion 4.1).
Figure 3: CRM sales data-inspired experiments (Section 4.2)5	Conclusion and future directionsThis work presented a contextual bandits formulation that captures real-world nuances such as theability to conduct targeted interventions and the presence of causal side-information, along witha novel algorithm that exploits this to achieve improved sample efficiency. In addition to syntheticexperiments, we also performed real world-inspired experiments set up using actual CRM sales data.
Figure 4:	Frequency of choosing or encountering each value of Ctar . Highlighted in red color arethe ‘high-value’ contexts (i.e., contexts for which learning the right actions provides higher expectedrewards).
Figure 5:	Different baselines outperform each other in two different settings; however, our algorithmperforms close to the best baseline in both.
Figure 6: Experiment 1 results when probability of uniform exploration in TargInt_TS_UniExp waschosen to be 0.2 (instead of 0.5). Our algorithm continues to perform better than all baselines.
