Figure 1: (a) The proposed approach Explore-to-Extrapolate Risk Minimization which entails Kcontext generators that generate graph data of different (virtual) environments based on input datafrom a single (real) environment. The GNN model is updated via gradient descent to minimize aweighted combination of mean and variance of risks from different environments, while the contextgenerators are updated via REINFORCE to maximize the variance loss. (b) Illustration for ourAssumption 1. (c) The dependence among variables in the motivating example in Section 3.1.
Figure 2: Results on Cora with artificial distribution shifts. We run each experiment with 20 trials.
Figure 3: Experiment results on Amazon-Photo with artificial distribution shifts.
Figure 4: Test ROC-AUC on Twitch where we compare different GNN backbones.
Figure 5: Test F1 score on Elliptic where wegroup graph snapshots into 9 test sets (T1 〜T9).
Figure 6: Comparison of different leave-out data on Twitch-Explicit. We consider three GNNbackbones trained with Erm. The "‘OOD"' means that We train the model on one graph DE and reportthe metric on another graph ENGB. The "IID" means that we train the model on 90% nodes of DEand report the metric on the remaining nodes. The results clearly show that the model performancesuffers a significantly drop from the case "IID" to the case "OOD". This indicates that the graph-levelsplitting for training/VaIidation/testing splits used in Section 5.2 indeed introduces distribution shiftsand would require the model to deal with out-of-distribution data during test.
Figure 7: Comparison of node numbers, densities and maximum node degrees of fourteen graphsused in our experiments on Facebook-100. The index 0-13 stand for John Hopkins, Caltech,Amherst, Bingham, Duke, Princeton, WashU, Brandeis, Carnegie, Cornell, Yale, Penn, Brown andTexas, respectively. As we can see, these graphs have very distinct statistics, which indicates thatthere exist distribution shifts w.r.t. graph structures.
Figure 8: The label rates and positive label rates of training/validation/testing data splits ofElliptic. The positive class (illicit transaction) and negative class (licit transaction) are veryimbalanced. Also, in different splits, the distributions for labels exhibit clear differences.
Figure 9: T-SNE visualization of training/validation/testing nodes in OGB-Arxiv. We mark trainingnodes (within 1950-2011) and validation nodes (within 2011-2014) as red and blue, respectively.
Figure 10: Distribution of test accuracy results on Cora with artificial distribution shifts generatedby SGC as the GNN generator.
Figure 11: Distribution of test accuracy results on Cora with artificial distribution shifts generatedby GAT as the GNN generator.
Figure 12: Comparison of training accuracy using all the features v.s. removing the spurious featuresfor inference on Cora with artificial distribution shifts generated by SGC as the GNN generator.
Figure 13: Comparison of training accuracy using all the features v.s. removing the spurious featuresfor inference on Cora with artificial distribution shifts generated by GAT as the GNN generator.
Figure 14: Distribution of test accuracy results on Photo with artificial distribution shifts generatedby SGC as the GNN generator.
Figure 15: Distribution of test accuracy results on Photo with artificial distribution shifts generatedby GAT as the GNN generator.
Figure 16: Comparison of training accuracy using all the features v.s. removing the spurious featuresfor inference on Photo with artificial distribution shifts generated by SGC as the GNN generator.
Figure 17: Comparison of training accuracy using all the features v.s. removing the spurious featuresfor inference on Photo with artificial distribution shifts generated by GAT as the GNN generator.
