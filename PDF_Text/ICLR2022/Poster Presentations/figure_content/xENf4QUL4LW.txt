Figure 1: Illustrations of uncertainty of losses. Experiments are conducted on the imbalanced noisy MNISTdataset. Left: uncertainty of small-loss examples. At the beginning of training (Epochs 1 and 2), due to theinstability of the current prediction, the network gives a larger loss to the clean example and does not select itfor updates. If we consider the mean of training losses at different epochs, the clean example can be equippedwith a smaller loss and then selected for updates. Right: uncertainty of large-loss examples. Since the deepnetwork learns easy examples at the beginning of training, it gives a large loss to clean imbalanced data withnon-dominant labels, which causes such data unable to be selected and severely influence generalization.
Figure 2: Test accuracy vs. number of epochs on IM-MNIST and IM-F-MNIST. The error bar forstandard deviation in each figure has been shaded.
Figure 3: The illustration of the influence function for the soft estimator.
Figure 4: Synthetic class-dependent transition matrices used in our experiments on MNIST. The noiserate is set to 20%.
Figure 5: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-S. The error barfor standard deviation in each figure has been shaded.
Figure 6: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-H. The error barfor standard deviation in each figure has been shaded.
