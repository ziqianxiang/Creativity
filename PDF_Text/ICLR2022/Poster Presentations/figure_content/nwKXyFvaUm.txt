Figure 1: Training loss, mean and variance of test accuracies of DivFL compared with randomsampling and power-of-choice on the synthetic IID data. For DivFL (no overhead), we utilize onlythe gradients from clients participating in the previous round. We see that DivFL achieves fasterconvergence and converges to more accurate and slightly more fair solutions than all baselines.
Figure 2: Training loss and test accuracy of DivFL compared with random sampling andpower-of-choice on synthetic non-IID data. DivFL converges faster and to more accurate solutionsand much improved fairness than all baselines.
Figure 3:	Training loss and test accuracy of DivFL compared with random sampling andpower-of-choice on real datasets. We observe clear improvement for DivFL on FEMNIST. For theother two datasets, the communication-efficient DivFL “no-overhead” converges at the same rateas the fastest baseline.
Figure 4:	Variance of test accuracies over the 3 real datasets shows that DivFL has fairness benefitson the image datasets. Poorer fairness performance for Shakespeare could be attributed to the sparsityin the gradient updates for language models.
Figure 5: Final Testing Accuracies (smoothened) after 400 epochs on CelebA dataset shows thepower of DivFL to have robust learning beating baselines for all choices of K. Furthermore, DivFLcan achieve the highest test accuracy and lowest variance using the smallest number of clients perround in comparison to baselines.
Figure 6: We measure the training loss, mean and variance of test accuracies as well as 10thpercentile test accuracy for DivFL on synthetic IID dataset for K = 20.
Figure 7: The training loss, mean and variance of test accuracies as well as 10th percentile testaccuracy for DivFL on synthetic non-IID dataset for K = 20.
Figure 8: Training loss, mean and variance of test accuracies as well as 10th percentile test accuracyfor DivFL on FEMNIST dataset for K = 20.
Figure 9: Training Loss on Synthetic IID for different choices of τ .
Figure 10: Variance of Test Accuracy on Synthetic IIDK = 10, Local epochs τ =5	K = 10, Local epochs τ =103.0Sω o t-C> 2.562.0C⊂ 1.5,ro匚1.00.5I -------- Random Samplingι -------- Power-of-Choice⅛	---- DivFL (ideal)∖	---- DivFL (no overhead)0 5 0 5 0 53 2 2 1 1 0sso~∣ 6u 三屈0	50 100 150 200 250 300
Figure 12: Variance of Test Accuracy on Synthetic non-IID# RoundsK = 20, Local epochs τ =10① 0.16⊂ 0.14mɪ 0.12ra> 0.10&0.08目 0.06U 0.04« 0.02	A 	 Random Sampling ⅛∖ 	 Power-of-Choice	——DivFL (ideal) M ⅛ ~⅛ DivFL (no overhead)	0	50 100 150 200 250 300# RoundsFigure 13: Faster convergence benefits of DivFL over Random Sampling and Power-of-Choiceapproaches is preserved even when clients run multiple local epochs before sharing model updates.
Figure 13: Faster convergence benefits of DivFL over Random Sampling and Power-of-Choiceapproaches is preserved even when clients run multiple local epochs before sharing model updates.
