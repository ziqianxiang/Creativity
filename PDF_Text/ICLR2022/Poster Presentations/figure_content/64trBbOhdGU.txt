Figure 1: An overview of HyAR. Agentlearns a latent policy in the latent represen-tation space of discrete-continuous actions.
Figure 2: Illustrations of: (left) the framework DRL with HyAR; and (right) overall workflow ofhybrid action representation model, consisting of an embedding table and a conditional VAE.
Figure 3: Illustrations of representation unreliability andrepresentation shift. Dots denote the hybrid action rep-resentations selected by policy (red) and known by en-coder (blue). Gray lines form the areas, among whichrepresentations can be well decoded and estimated.
Figure 4: Benchmarks with discrete-continuous actions: (a) the agent selects a discrete action (run,hop, leap) and the corresponding continuous parameter (horizontal displacement) to reach the goal;(b) The agent selects a discrete strategy (move, shoot) and the continuous 2-D coordinate to score;(c) The agent selects a discrete action (move, catch) and the continuous parameter (direction) to grabthe target point; (d) The agent has n equally spaced actuators. It can choose whether each actuatorshould be on or off (thus 2n combination in total) and determine the corresponding continuousparameter for each actuator (moving distance) to reach the target area.
Figure 5: Comparisons of algorithms in different environments. The x- and y-axis denote the envi-ronment steps (×105) and average episode reward over recent 100 episodes. The curve and shadedenote the mean and a standard deviation over 5 runs.
Figure 6: 2D t-SNE visualizations of learned represen-tation for original hybrid actions, colored by 1D t-SNEof the corresponding environmental impact.
Figure 7: DDPG-based comparisons of related baselines in different environments. The x- and y-axis denote the environment steps (×105) and average episode reward over recent 100 episodes. Theresults are averaged using 5 runs, while the solid line and shaded represent the mean value and astandard deviation respectively.
Figure 8: Learning curves for variants of dynamics predictive representation in HyAR. (a) and (b)show the comparison between HyAR-TD3 with and without dynamics prediction auxiliary loss; (c)shows the effects of the balancing weight β in Eq.6. The results are averaged using 5 runs, while thesolid line and shaded represent the mean value and a standard deviation respectively.
Figure 9: Learning curves of ablation studies for HyAR (i.e., element-wise + cascaded head +representation relabeling + latent select range = 96% + latent action dim = 6) corresponding toTab. 6. From top to bottom is Goal and Hard Move (n = 8) environment. The shaded regiondenotes standard deviation of average evaluation over 5 runs.
Figure 10: t-SNE visualization diagram of continuous action embedding zx , color coded by discreteaction embedding e. The continuous actions related to the same discrete actions are mapped to thesimilar regions of the representation space.
Figure 11: t-SNE visualization diagram of hybrid action embedding pair (e, zx), color coded byδs,s0. The hybrid actions with a similar impact on the environment are relatively closer in the latentspace.
Figure 12: Another view of the learning curves shown in Fig. 5 where the number of samplesused in warm-up training (i.e., stage 1 in Algorithm 1) is also counted for HyAR. Comparisons ofalgorithms in different environments. The x- and y-axis denote the environment steps and averageepisode reward over recent 100 episodes. The results are averaged using 5 runs, while the solid lineand shaded represent the mean value and a standard deviation respectively.
Figure 13: Comparison between HyAR-TD3 (denoted by Unfixed HyAR) and HyAR-TD3 withfixed hybrid action representation space trained in warm-up stage (denoted by Fixed HyAR) in dif-ferent environments. The x- and y-axis denote the environment steps and average episode rewardover recent 100 episodes. The results are averaged using 5 runs, while the solid line and shadedrepresent the mean value and a standard deviation respectively. Conclusion: Unfixed HyAR outper-forms Fixed HyAR across all the environments; while Fixed HyAR performs very poorly in Goaland Hard Goal. We conjecture that in these environments, random policy is quite limited in collect-ing effective and meaning hybrid actions in these environments thus the learned fixed representationspace is not able to support the emergence of effective latent policy.
Figure 14: The effects of different choices of training frequency of subsequent representation train-ing of HyAR-TD3 in the environments Goal, Hard Goal. For example, 1/10ep denotes that thehybrid action representation is trained every 10 episodes for 1 batches with batch size 64. The blackdashed lines denote the convergence results of Fixed HyAR in Figure 13. The x- and y-axis denotethe environment steps and average episode reward over recent 100 episodes. The results are aver-aged using 5 runs, while the solid line and shaded represent the mean value and a standard deviationrespectively. Conclusion: all configurations of subsequent training outperform Fixed HyAR; whilea moderate training frequency works best.
