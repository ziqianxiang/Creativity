Figure 1: Overview of the implementation (Groupified VAE).(a) Illustration of permutation groupΦ = {夕g |g ∈ G} defined on a VAE-based model, where G = (Z∕nZ)m. The generators 夕i, Wj ∈Φ are permutations on O. Specifically, when optimized, Wi and Wj are horizontal and verticalmovements. Wi is defined as the solid orange arrows illustrate: encode an image o to representation z,perform η on Z to get z, add gi to z, and decode back to the image. This process can be regarded as anexchange of images in dataset (permutation), as the dashed orange arrow shows. These permutationsform a group Φ. (b) The Isomorphism Loss, which guarantees that Φ is isomorphic to G, includesAbel Loss La constraining the commutativity, and Order Loss Lo constraining the cyclicity.
Figure 3: Traversal results of two factors (floorcolor, scale) of Original and Groupified β-TCVAE. The traversal results of GroupifiedVAEs are cyclic.
Figure 2: Visual traversal comparison betweenOriginal and Groupified β-TCVAE. The traversalresults of Groupified VAEs are less entangled.
Figure 4: Performance distribution of Original and Groupified AnnealVAE on dSprites (demonstratedby the Violin Plot (Hintze & Nelson, 1998)). Variance is due to different hyperparameters and randomseeds. We observe that Groupified AnnealVAE improves the average performance with smallervariance in terms of BetaVAE score (a), DCI disentanglement (b), and MIG (c), and has a comparablemean performance with smaller variance in terms of FactorVAE score (d).
Figure 5: The representation space spanned by the learned factors by Original (bottom row) andGroupified AnnealVAE (top row). The position of each point is the disentangled representation of thecorresponding image. An ideal result is all the points form a cube and color variation is continuous.
Figure 6: 4-th Dihedral Group (D4): The groups of symmetries for square. Note that permutation(123) means 1 → 2,2 → 3, 3 → 1.
Figure 7: Overview of the Isomorphism Loss. The Abel Loss and Order Loss constrain the commuta-tivity and cyclicity of permutation group Φ, respectively. The dotted lines in the figure representsreconstruction loss.
Figure 8: Performance distribution on dSprites. Variance is due to different hyperparameters andrandom seeds. We consider four metrics: BetaVAE score, DCI disentanglement, MIG, and FactorVAEscore. We observe that Groupified VAEs outperform the original ones.
Figure 9: Performance distribution on Cars3d. Variance is due to different hyperparameters andrandom seeds. We observe that Groupified models outperform the Original ones.
Figure 10: Performance distribution on Noisy dSprites. Variance is due to different hyperparametersand random seeds. We observe that Groupified VAEs outperform the Original ones.
Figure 11: Performance distribution on Color dSprites. Variance is due to different hyperparametersand random seeds. We observe that Groupified VAEs outperform the Original ones.
Figure 12: Performance distribution on Shapes3d. Variance is due to different hyperparameters andrandom seeds. We observe that Groupified models outperform the Original ones.
Figure 13: Learned latent variables using Original and Groupified FactorVAE on Shapes3d dataset.
Figure 14: Learned latent variables using Original and Groupified β-TCVAE on Shapes3d dataset.
Figure 15: Learned latent variables using Original and Groupified FactorVAEtraversal range is (-2, 2).
Figure 16:	Learned latent variables using Original and Groupified β-TCVAE on Car3d dataset. Thetraversal range is (-2, 2).
Figure 17:	Learned latent variables using Original and Groupified AnnealVAE on CeleBa dataset.
Figure 18: Meaningful dimensions visualization for C = 10, end = 30000 (different random seeds).
Figure 19: Meaningful dimensions visualization for C = 10, end = 40000 (different random seeds).
Figure 20: Meaningful dimensions visualization for C = 10, end = 50000 (different random seeds).
Figure 21: Meaningful dimensions visualization for C = 20, end = 30000 (different random seeds).
Figure 22: Meaningful dimensions visualization for C = 20, end = 40000 (different random seeds).
Figure 23:	Meaningful dimensions visualization for C = 20, end = 50000 (different random seeds).
Figure 24:	Meaningful dimensions visualization for C = 30, end = 30000 (different random seeds).
Figure 25:	Meaningful dimensions visualization for C = 30, end = 40000 (different random seeds).
Figure 26:	Meaningful dimensions visualization for C = 30, end = 50000 (different random seeds).
Figure 27: The representation space span by the Groupified and Original β-VAE. We train themodels with the same hyperparameter but different random seeds for different runs. The 3D locationof each point is the disentangled representation of the corresponding image. Moreover, an ideal resultis that all the points form a cube, and color variation is continuous. Higher hyper-parameter β resultsin the collapse of representation space. The collapse is suppressed by the Isomorphism Loss, whichleads to better disentanglement.
Figure 28:	The representation space span by the Groupified and Original AnnealVAE. We train themodels with the same hyperparameter but different random seeds for different runs. The 3D locationof each point is the disentangled representation of the corresponding image. Moreover, an ideal resultis that all the points form a cube, and color variation is continuous. Higher hyper-parameter C resultsin the collapse of representation space. The collapse is suppressed by the Isomorphism Loss, whichleads to better disentanglement.
Figure 29:	The representation space span by the Groupified and Original FactorVAE. We train themodels with the same hyperparameter but different random seeds for different runs. The 3D locationof each point is the disentangled representation of the corresponding image. Moreover, an ideal resultis that all the points form a cube, and color variation is continuous. The collapse is suppressed by theIsomorphism Loss, which leads to better disentanglement.
Figure 30:	The representation space span by the Groupified and Original β-TCVAE. We train themodels with the same hyperparameter but different random seeds for different runs. The 3D locationof each point is the disentangled representation of the corresponding image. Moreover, an ideal resultis that all the points form a cube, and color variation is continuous. Higher hyper-parameter C resultsin the collapse of representation space. The collapse is suppressed by the Isomorphism Loss, whichleads to better disentanglement.
