Figure 1: Performance of PER-ETD(0) and comparisonIn Figure 1(a), we compare the performance of of TD, vanilla ETD(0) and PER-ETD(0) with b =2, 4, 8 in terms of the distance between the ground truth and the learned value functions. It can beobserved that our proposed PER-ETD(0) converges close to the ground truth at a properly chosenperiod length such as b = 4 and b = 8, whereas TD diverges due to no treatment on off-policy datahistorically, and ETD (0) also diverges due to the very large variance.
Figure 2: Performance of PER-ETD(λ) and dependence on features(b) Feature Φ2(c) Feature Φ3Figure 3: Fixed points of PER-ETD(λ) and project of the value function. (a): θ lies in 1-dimensionalEuclidean space R1 along horizontal direction; (b), (c): θ lies in 2-dimensional Euclidean space R2 .
Figure 3: Fixed points of PER-ETD(λ) and project of the value function. (a): θ lies in 1-dimensionalEuclidean space R1 along horizontal direction; (b), (c): θ lies in 2-dimensional Euclidean space R2 .
Figure 4: BAIRD example (Sutton & Barto, 2018)A.2 Features for Experiments in Section 5.2In the experiment in Section 5.2, we choose the following features:Φ1 =(0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.37)>;Φ2 =((0.3425, 0.0171)>, (0.1902, 0.4248)>, (0.1354, 0.76)>, (0.1357, 0.7973)>,(0.8674, 0.8774)>, (0.5166, 0.9493)>, (0.3094, 0.8535)>)>;Φ3 =((0.5162, 0.9013)>, (0.5128, 0.5999)>, (0.289, 0.4649)>, (0.3399, 0.5334)>,(0.315, 0.2278)>, (0.667, 0.461)>, (0.3706, 0.1457)>)>.
Figure 5: Performance of PER-ETD(0) and comparison(c) Feature Φ3Figure 6: Performance of PER-ETD(λ) and dependence on featuresB	More ExperimentsIn this section, we conduct further experiments to answer the following two intriguing questions:•	If the distribution mismatch parameter ρmax changes, how will different approaches per-form and compare with each other?•	Focusing on our algorithm PER-ETD, how do the choices of behavior policy and targetpolicy affect its convergence?13Published as a conference paper at ICLR 2022Z = Hl电里(a) ρmax = 1.17(b) ρmax = 5.60Figure 7: Comparisons of TD, ETD, PER-ETD(0) With different target policiesFigure 9: Performance of PER-ETD(0) under different behavior policies (marked by their differentresulting distribution mismatch parameter ρmax). The target policy is kept the same.
Figure 6: Performance of PER-ETD(λ) and dependence on featuresB	More ExperimentsIn this section, we conduct further experiments to answer the following two intriguing questions:•	If the distribution mismatch parameter ρmax changes, how will different approaches per-form and compare with each other?•	Focusing on our algorithm PER-ETD, how do the choices of behavior policy and targetpolicy affect its convergence?13Published as a conference paper at ICLR 2022Z = Hl电里(a) ρmax = 1.17(b) ρmax = 5.60Figure 7: Comparisons of TD, ETD, PER-ETD(0) With different target policiesFigure 9: Performance of PER-ETD(0) under different behavior policies (marked by their differentresulting distribution mismatch parameter ρmax). The target policy is kept the same.
Figure 7: Comparisons of TD, ETD, PER-ETD(0) With different target policiesFigure 9: Performance of PER-ETD(0) under different behavior policies (marked by their differentresulting distribution mismatch parameter ρmax). The target policy is kept the same.
Figure 9: Performance of PER-ETD(0) under different behavior policies (marked by their differentresulting distribution mismatch parameter ρmax). The target policy is kept the same.
Figure 8: Performance of PER-ETD(0) under different target policies (marked by their differentresulting distribution mismatch parameter ρmax). The behavior policy is kept the same.
