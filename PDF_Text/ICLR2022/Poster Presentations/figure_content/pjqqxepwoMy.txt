Figure 1: Implementation of VLOG for deep Q-learning during (A) learning and (B) execution.
Figure 2: Learning latent representations by oracle guiding in a simple maze. (A) Illustration of thetask. (B) Success rate of using VLOG and not using VLOG (baseline), each using 8 random seeds.
Figure 3: (A) Illustration of the MinAtar environments with randomly broken pixels. (B) Learningcurves showing normalized average return (divided by the average return of trained oracle model)of VLOG and alternative methods each using 8 random seeds.
Figure 4: Diagram of our implementation for Suphx-style and OPD-style oracle guiding. For OPD-style oracle guiding, the teacher model is trained and then fixed in prior to training the student model(executor model).
Figure 5: Picture of a Mahjong game. The executor observation includes the publicly visible infor-mation and the player’s private hand tiles. The oracle observation includes the executor observationand additional information about the opponents’ private tiles.
Figure 6: Sensitivity analysis of VLOG in noisy MinAtar environments w.r.t. the selection of β if βis fixed.
