Figure 1: (a) Distributed Adversarial Learning (DisAdv), (b) Concurrent Adversarial Learning(ConAdv). To ease the understanding, WejUSt show the system including two workers.
Figure 2: Augmentation AnalysisminθE(xi,yi)^D[L(θt; xi, yi) + maχ L(θt; Xi + δ, yi)],kδkp∈e(2)where L is the loss function and e represents the value of perturbation. Although many previous workin adversarial training focus on improving the trade-off between accuracy and robustness (Shafahiet al., 2019; Wong et al., 2020), recently Xie et al. (2020) show that using split BatchNorm foradversarial and clean data can improve the test performance on clean data. Therefore, we also adoptthis split BatchNorm approach.
Figure 3: Vanilla Training and Concurrent Train-ingAlgorithm 1 ConAdvfor t = 1,…，T dofor Xi ∈ B% doCompute Loss:L(θt; Xi,yi) using main BN,La(θt; Xi(θt-τ), yi) using adv BN,Lb (θt) = EBk,tL(θt; Xi,yi)+EBk,t (XiR-T ),yi)Minimize the LB (θt) and obtain gk (θt)end forfor Xi ∈ Bk,t+τ doCalculate adv gradient gk (θt)on Bc 右十「Obtain adv examples (Xi(θt), yi)end forend forAggregate: gt(θt) = K PK=I gk(θt)Update weight θt+1 on parameter sever4(仇)
Figure 4: (a): throughput on scaling up batch size for ResNet-50, (b): throughtput when the numberof processors reach the limit that each batch size can use for ResNet-50 .
