Figure 1: Comparisons between TAdaConv and the spatial convolutions in video models. (a)Standard spatial convolutions in videos share the kernel weights between different frames. (b) OurTAdaConv adaptively calibrates the kernel weights for each frame by its temporal context.
Figure 2: An instantiation OfTAdaConv and the temporal feature aggregation used in TAda2D.
Figure 3: The classification performance of TAda2D(a) with different channels (C.) and stages (S.) en-abled; (b) in comparison with other state-of-the-arts.
Figure 4: Grad-CAM visualization and pre-diction comparison between TSN, R(2+1)Dand TAda2D (more examples in Fig. A3).
Figure A1: Per-Category performance comparison of TAda2D against the baseline TSN. Weachieve an average Per-Category performance improvement of 30.35%.
Figure A2: Training and validation on Kinetics-400 and Something-Something-V2. On bothdatasets, TAda2D shows a stronger capability of fitting the data and a better generality to the valida-tion set. Further, TAda2D reduces the overfitting problem in Something-Something-V2.
Figure A3: Further qualitative evaluations on the Something-Something-V2 dataset. In mostcases, TAda2D captures meaningful areas in the videos for the correct classification. Further, theactivated region of TAda2D also lasts longer along the temporal dimension compared to other twomodels, thanks to the global temproal context in the weight generation function G.
