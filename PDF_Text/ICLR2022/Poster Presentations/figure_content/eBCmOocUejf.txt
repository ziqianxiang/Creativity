Figure 1: Overview of prefix-tuning as well as our robust prefix-tuning framework for text classifi-cation. We frame the samples into a SQuAD-like scheme consisting of context, question, and labeland optimize the original prefix Pθ. For the robust prefix-tuning framework, we fix the obtained pre-fix Pθ and tune an additional prefix Pψ0 for each test batch. The additional tuning follows the threesteps indicated in the figure, which aims to lead the summed prefix to steer correct activations at theposition of the [ANS] token with those activated by correctly classified training data as the standard.
Figure 2: Comparison between standard and adversarial prefix-tuning for 100 epochs with respectto (a) epoch and (b) clock time. While adversarial prefix-tuning gains strengths in epoch-wise con-vergence rate and generalization, it takes far greater training time than standard prefix-tuning.
Figure 3: Results on the SST-2 development set when applying robust prefixes of different layers.
Figure 4: Visualized attention weight maps of the final layer in the LM for the case study with BUGattack. Each row represents the time step at which the token (labeled on the left) is inputted. Eachcolumn in the row illustrates the attention weight assigned to the specific token (labeled on the top).
Figure 5: Importance visualization for the UAT case study. (a): the original input by the originalprefix-tuning; (b): the UAT-attacked input by the original prefix-tuning. Compared with (a), thetrigger tokens in (b) attract major importance. (c) and (d): the UAT-attacked input by our robustprefix-tuning with (c) N = 24 and (d) N = 3. For N = 3, the LM is steered to ignore thedistraction of the trigger tokens and assign high importance to “filler” at the time step of token “.”.
Figure 6: Training loss and clean accuracy on the dev set of SST-2 with KL-divergence regularizedadversarial prefix-tuning as well as the word-level adversarial prefix-tuning used in Section 4.2.
Figure 7: Visualized attention weights from the final layer in the LM for the first example in Table16. (a): Original input With original prefix-tuning; (b): PWWS-perturbed input With original prefix-tuning; (c) and (d): PWWS-perturbed input With robust prefix-tuning of (c) N = 24 and (d) N = 3.
Figure 8: Visualized attention weights from the final layer in the LM for the second example inTable 16. (a): Original input with original prefix-tuning; (b): PWWS-perturbed input with originalprefix-tuning; (c) and (d): PWWS-perturbed input with robust prefix-tuning of (c) N = 24 and (d)N = 3.
Figure 9: Visualized attention weights from the final layer in the LM for the third example in Table16. (a): Original input with original prefix-tuning; (b): PWWS-perturbed input with original prefix-tuning; (c) and (d): PWWS-perturbed input with robust prefix-tuning of (c) N = 24 and (d) N = 3.
Figure 10: Visualized importance matrices for the first example in Table 17. (a): Original inputwith original prefix-tuning; (b): UAT-attacked input with original prefix-tuning; (c) and (d): UAT-attacked input with robust prefix-tuning of (c) N = 24 and (d) N = 3.
Figure 11: Visualized importance matrices for the second example in Table 17. (a): Original inputwith original prefix-tuning; (b): UAT-attacked input with original prefix-tuning; (c) and (d): UAT-attacked input with robust prefix-tuning of (c) N = 24 and (d) N = 3.
Figure 12: Visualized importance matrices for the third example in Table 17. (a): Original inputwith original prefix-tuning; (b): UAT-attacked input with original prefix-tuning; (c) and (d): UAT-attacked input with robust prefix-tuning of (c) N = 24 and (d) N = 3. We omit the visualization ofthe question part for this example due to its context length.
