Figure 1: Fig 1(a) highlights the distinction between existing sampling-based approaches that may introducebias and variance. IGLU completely sidesteps these issues and is able to execute GCN back-propagation stepson the full graph owing to its use of lazy updates which offer no sampling variance and provably bounded bias.
Figure 2: Wall Clock Time vs Validation Accuracy on different datasets for various methods.
Figure 3: Effect of backprop and inverted order of updates in IGLU on Reddit and OGBN-Proteinsdatasets. The inverted order of updates offers more stability and faster convergence. It is notable thattechniques such as VR-GCN use stale node embeddings that correspond to the backprop variant.
Figure 4: Update frequency vs Accuracy. Experiments conducted on PPI-Large. As expected,refreshing the αk ’s too frequently or too infrequently can affect both performance and convergence.
Figure 5: Total Overheads in Wall Clock Time (Log Scale) for the different methods on OGBN-Proteins and Reddit dataset. ClusterGCN runs into runtime error on the OGBN-Proteins datasetand hence has not been included in the plot. IGLU frequently offers least total overhead compared tothe other methods and hence significantly lower overall experimentation time. Please refer to sectionA.2 for details.
Figure 6: Wall Clock Time vs Validation Accuracy on different datasets as compared to GN-NAutoScale. We perform experiments using GNNAutoScale in a setting identical to IGLU with2-layer models on PPI-Large, Reddit and Flickr datasets and 3-layer models on OGBN-Arxiv datasetand report the performance. IGLU offers competitive performance and faster convergence across thedatasets.
Figure 7: Fine-grained Validation ROC-AUC for IGLU on the Proteins Dataset for Epoch 1.
Figure 8: Test Convergence AUC plots across different number oflayers on the OGBN-Proteinsdataset. IGLU has consistently higher AUTC values compared to the other baselines, demonstratingincreased stability, faster convergence and better generalization. GraPhSAGE suffers from neighbor-hood explosion problem and the training became very slow as noted earlier. This results in a decreasein the AUTC while going from 3to4 layers. GraphSAGE,s AUTC for 4 layers is only 0.313, and isthus not visible in the plot. VRGCN also suffers from the neighborhood explosion problem and runsinto runtime errors for a 4 layer model. ClusterGCN runs into runtime error for the OGBN-Proteinsfor all of 2, 3 and 4 layers and is therefore not present in this analysis. Please refer to Section B.3.2for details.
Figure 9: Training Loss curves of different methods on the benchmark datasets against Wallclock time.
