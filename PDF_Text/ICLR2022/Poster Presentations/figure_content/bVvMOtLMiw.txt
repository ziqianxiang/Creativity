Figure 1: The DIVA dataset derivative is computed end-to-end from the final validation loss4Published as a conference paper at ICLR 20223.1	LinearizationIn real-world applications, the parametric model fw (x) is usually a deep neural network. Recentwork (Achille et al., 2020; Mu et al., 2020) have shown that in many cases, a deep neural network canbe transformed to an equivalent linear model that can be trained on a simple quadratic loss and stillreach a performance similar to the original model. Given a model fw(x), let w0 denote an initial setof weights, For example, w0 could be obtained by pre-training on a large dataset such as ImageNet(if the task is image classification). Following Achille et al. (2020); Mu et al. (2020), we consider alinearization fwlin(x) of the network fw(x) given by the first-order Taylor expansion of fw(x) aroundw0:fWin(x) = fwo (x)+ Vwfw0 (x) ∙ (W - W0).	⑵Intuitively, if fine-tuning does not move the weights much from the initial pre-trained weights w0,then fwlin (x) will remain a good approximation of the network while becoming linear in W (but stillremaining highly non-linear with respect to the input x). Effectively, this is equivalent to training alinear classifier using the gradients Zi := Vwfw0 (Xi) as features (MU et al., 2020).
Figure 2: Examples of the reweighting done by DIVA. (Left) Samples from the FGVC Aircraftclassification dataset that are up-weighted by DIVA and (Right) samples that are down-weightedbecause they increase the test error. Down-weighted samples tend to have planes in non-canonicalposes, multiple planes, or not enough information to classify the plane correctly.
Figure 3:	(Left) Distribution of LOO DIVA gradients for correctly labelled and mislabelled samplesin CUB-200 dataset (20% of the samples are mislabeled by replacing their label uniformly at random).
Figure 4:	DIVA Extend. We show the test error achieved by the model as we extend a dataset withsamples selected from a dataset pool using either DIVA Extend (red line) or uniform sampling (blueline). The pool set matches the same distribution as the training set. In all cases DIVA Extendoutperforms uniform sampling and identifies subsets of the pool set with better performance than thewhole pool. We also note that using only a subset selected by DIVA as opposed to using the wholepool, actually improves the test accuracy.
