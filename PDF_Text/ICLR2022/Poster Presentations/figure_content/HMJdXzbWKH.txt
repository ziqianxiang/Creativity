Figure 1: Illustration of Online Target Q-learning with Reverse EXperience Replay3 Our AlgorithmAs discussed in the introduction, we incorporate RER and OTL into Q-learning and introduce thealgorithms Q-ReX (Online Target Q-learning with reverse eXperience replay, Algorithm 1), its sam-ple efficient variant Q-ReXDaRe (Q-ReX + data reuse, Algorithm 2) and its episodic variant EpiQ-ReX (Episodic Q-ReX, Algorithm 3). Since Q-ReXDaRe and EpiQ-ReX are only minor modificationsof Q-ReX, we refer the reader to the appendiX for their pseudocode.
Figure 2: Mountain Car problemFigure 4: 100 State Random WalkFigure 3: Grid-world problemFigure 6: Feature embedding in the modifiedBaird’s counter-example; e~i represents the i-th canonical basis vector in R7 . Each transi-tion shown occurs with probability 1.
Figure 4: 100 State Random WalkFigure 3: Grid-world problemFigure 6: Feature embedding in the modifiedBaird’s counter-example; e~i represents the i-th canonical basis vector in R7 . Each transi-tion shown occurs with probability 1.
Figure 3: Grid-world problemFigure 6: Feature embedding in the modifiedBaird’s counter-example; e~i represents the i-th canonical basis vector in R7 . Each transi-tion shown occurs with probability 1.
Figure 6: Feature embedding in the modifiedBaird’s counter-example; e~i represents the i-th canonical basis vector in R7 . Each transi-tion shown occurs with probability 1.
Figure 5: Linear System ProblemFigure 7: Performance of vanilla Q-learningas compared to OTL+Q-learning, averagedover 10 independent runsLinear Dynamical System with Linear Rewards We also compare the algorithms on a lineardynamical systems problem. While the problem described next is strictly not a linear MDP, thevalue functions for certain policies can be written as a linearly in terms of the initial condition; thisis made precise next. Consider a linear dynamical system with initial state being X0 ∈ Rd . Thestate evolves as Xt+1 = AXt + ηt , where A ∈ Rd×d . The reward obtained for such transitionis given by hXt , θi for a fixed θ ∈ Rd . The maximum singular value of A is chosen less than 1to ensure that the system is stable. The infinite horizon γ-discounted expected reward is given byhX0, P∞=0(YAT)iθi∙ Thus, the expected reward can be written as hX0, w*i，whereW* = (I - YAT)-1 θ.
Figure 7: Performance of vanilla Q-learningas compared to OTL+Q-learning, averagedover 10 independent runsLinear Dynamical System with Linear Rewards We also compare the algorithms on a lineardynamical systems problem. While the problem described next is strictly not a linear MDP, thevalue functions for certain policies can be written as a linearly in terms of the initial condition; thisis made precise next. Consider a linear dynamical system with initial state being X0 ∈ Rd . Thestate evolves as Xt+1 = AXt + ηt , where A ∈ Rd×d . The reward obtained for such transitionis given by hXt , θi for a fixed θ ∈ Rd . The maximum singular value of A is chosen less than 1to ensure that the system is stable. The infinite horizon γ-discounted expected reward is given byhX0, P∞=0(YAT)iθi∙ Thus, the expected reward can be written as hX0, w*i，whereW* = (I - YAT)-1 θ.
