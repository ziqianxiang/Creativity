Figure 1: Forgetting frontier across model scales. Task A versus Task B performance for both pretrainedvision transformers (left) and ResNets (center). Each point represents a different choice of learning rate orfinetuning step for Task B. All models were pretrained on ImageNet21k for 90 epochs. (right) the best averageTask A/B error improves systematically with model size.
Figure 5: Scaling behavior on non-CIFAR, two-task sequences. (top) While the rate of improvement variesacross datasets, continual-learning performance in these two-task continual learning setups systematically im-proves with model size, consistent with a power-law. For a full discussion and plot of all forgetting frontiers,see Figure 2. (bottom) Table of power-law fit exponents.
Figure 2: Forgetting frontiers for two-task sequences beyond CIFAR10 and CIFAR100. Fine-tuning settings for these experiments are shown in table 4 (supplement), while the datasets them-selves are descrbed in section B.4 (supplement). As with the CIFAR10 and CIFAR100 datasets, wefind that model scale is beneficial from the perspective of continual learning, though especially forsmaller ResNets the behavior is less clean than on CIFAR. Scaling exponents also vary with dataset.
Figure 3: Split CIFAR-100 tasks. (left) Sequential training of a pretrained ViT-B model on a 10-task se-quence, where each task consists of classifying between 10 categories in the CIFAR-100 dataset. In this contin-ual learning setup, the model is able to learn all 10 tasks without much forgetting: on average, the model loses1.8% accuracy on each task; at most, the model loses 2.9%. (right) Forgetting frontiers for vision transformerson a 2-task, 50-class split CIFAR-100, showing the same increase with scale as on the split-CIFAR-10 version.
Figure 4: Single-head models on CIFAR-100 distribution shift task. As with the multi-head setup, theforgetting in Vision Transformers (left) and ResNets (right) is largely mitigated by pretraining and scale. Thefull task details are given in Appendix A.1.3reducing the number of steps for which we fine-tune. Figure 6 shows the results of this experiment,with additional results and details in Appendix C.1. While the trained-from-scratch ResNets achievethe same performance as their pretrained counterparts on Task A, their Task A performance degradessignificantly more than the pretrained ResNets when trained on Task B; pretrained ResNets forgetless, and this improves with model scale.
Figure 6: Pretrained versus trained-from-scratch models. ResNet models trained from scratch (gray)exhibit inferior forgetting performance than pretrained models (colored), even for the pretrained models shownhere, which were handicapped during fine-tuning, in order to match Task A performance. Futhermore, thetrained-from-sCratCh models are unable to take advantage of model scale - the forgetting frontier is largelyindependent of model size (see also Figure 19 in the Appendices). Dashed vertical lines show the maximumaCCuraCy on Task A.
Figure 7: Varying pretraining time. We take pretrained CheCkpoints at 50k, 100k, 150k, 200k, and 250ksteps (shown in Colored tiCks at the left) for a ViT-S model pretrained on ImageNet21k. (right) The forgettingfrontiers improve with inCreased pretraining time.
Figure 8: Varying dataset size and finetuning time. (left) The forgetting frontier for a vision transformer(ViT-S16) finetuned on a two-task CIFAR-10 sequence, pretrained on varying fractions of the ImageNet21kdataset shows improvement with pretraining dataset size. (center, right): The forgetting frontiers for modelspretrained on ImageNet21k, with varying amounts of fine-tuning - Task A performance saturates around 500steps; even training for ten times as many steps does not affect the forgetting frontier.
Figure 9: Trace overlap of CIFAR-10 class representations. (left) The cross-class representation overlapmatrices (Equation 1) are significantly lower for a pretrained ResNet101 than for one trained from scratch.
Figure 10: ImageNet21k is highly class-imbalanced. While roughly 7500 classes of the ImageNet21kdataset feature over 1000 examples, there are over 3000 classes with less than 100 examples. As mentioned inthe main text, in this work we do not class-balance ImageNet21k before pretraining.
Figure 11: Forgetting in sequential language-modeling. Token accuracy for decoder-only transformer mod-els show systematic improvement in joint Task A, Task B performance across scales.
Figure 12: Forgetting frontiers on a split-CIFAR10 task for models pretrained in an unsupervised man-ner on ImageNet1k. ResNets pretrained using unsupervised SimCLR on ImageNet1k exhibit a similar trendto that observed in the supervised setting of forgetting robustness improving with model scale.
Figure 13: Model scaling on 10-subtask split-CIFAR100. (left) Scaling of best average error with parametercount for vision transformer models. (right) Scaling of best average error with parameter count for ResNetmodels.
Figure 14: Data scaling on 10-subtask split-CIFAR100. Scaling of best average error with pre-trainingdataset size (measured in a fraction of the full ImageNet21k dataset) for the ViT-S_16 model.
Figure 15: Forgetting frontiers for 10-task Split CIFAR100, Resnet. Colors are the same as in Figure 1 ofthe main text. All images in each row share the same task for the x-axis, i.e. the x-axis of the top row representstask A, x-axis of the second-from-the-top row represents task B, etc. All images in each column share the sametask for y-axis, i.e. the y-axis of the leftmost column represents task B, the y-axis of the second-to-leftmostcolumn represents task C, etc.
Figure 16: Forgetting frontiers for 10-task Split CIFAR100, Vision Transformer. Colors are the same asin Figure 1 of the main text. All images in each row share the same task for the x-axis, i.e. the x-axis of thetop row represents task A, x-axis of the second-from-the-top row represents task B, etc. All images in eachcolumn share the same task for y-axis, i.e. the y-axis of the leftmost column represents task B, the y-axis of thesecond-to-leftmost column represents task C, etc.
Figure 17: Forgetting frontiers for 10-task Split CIFAR100 versus data scale. These frontiers are plottedfor a ViT-S_16 model. Colors are the same as in Figure 8(left) of the main text. All images in each row sharethe same task for the x-axis, i.e. the x-axis of the top row represents task A, x-axis of the second-from-the-toprow represents task B, etc. All images in each column share the same task for y-axis, i.e. the y-axis of theleftmost column represents task B, the y-axis of the second-to-leftmost column represents task C, etc.
Figure 18: Forgetting frontier for ResNets trained from scratch, vs. pretrained ResNets (on Ima-geNet21k). The pretrained ResNets were handicapped during fine-tuning, in order to match the Task A perfor-mance of the from-scratch model. This figure is identical to Figure 6 in the main text, with the addition of theResNet50 and ResNet152 models.
Figure 19: Forgetting frontier for ResNets trained from scratch, vs. pretrained ResNets (on Ima-geNet21k). The pretrained ResNets were handicapped during fine-tuning, in order to match the Task A perfor-mance of the from-scratch model. The data plotted here is the same as that plotted in Figure 6 in the main text;here, the intention is to make clear that the performance of the trained-from-scratch models do not exhibit anyapparent benefit with scale, while the pretrained models do (right) Scaling of the best average (task A / task B)accuracy shows that only in pretrained models does the performance improve with scale.
Figure 20: ResNet Learning curves on Split CIFAR10 task. Learning curves for the ResNet models ofFigure 1. The solid/dashed lines are Task A/B accuracy.
Figure 21: ViT Learning curves on Split CIFAR10 task. Learning curves for the ViT models of Figure 1.
Figure 22: Learning curves in sequential language-modeling. Token accuracy as a function of steps for thedifferent models. The dashed line shows the point at which training switches from Task A to Task B.
Figure 23:	Step and learning rate along Split CIFAR10 forgetting frontier for ResNets. We plot theforgetting frontier of Figure 1. The left column is colored by learning rate with transparency set by step (darker)is later during task 2 training. The right column is colored by step times learning rate.
Figure 24:	Step and learning rate along Split CIFAR10 forgetting frontier for ViT models. We plot theforgetting frontier of Figure 1. The left column is colored by learning rate with transparency set by step (darker)is later during task 2 training. The right column is colored by step times learning rate.
Figure 25:	Average error scaling at fixed learning rate and step. We plot the average Task A Task B errorfor the setup in Figure 1 for a fixed learning rate of 0.001 and 200 Task B training steps for both ViTs andResNets.
Figure 26:	Classwise representation matrices for Vision Transformer models taken (left) afterthe end of training on the first task (Task A) of a two-task split CIFAR-10 sequence, and (right) afterpretraining on ImageNet21k. Classes are ordered alphabetically, left to right and down to up, i.e.
Figure 27: Classwise representation matrices for ResNet models. (Left) Models are trained fromrandom initialization on the first task (task A) of a two-task split CIFAR-10 sequence. (Middle)Models are pretrained on ImageNet21k and then finetuned on task A of the two-task split CIFAR-10sequence. (Right) Models are pretrained on ImageNet21k, but no finetuning is done. Classes areordered alphabetically, left to right and down to up, i.e. the order is airplane, automobile, bird, cat,deer, dog, frog, horse, ship, truck.
