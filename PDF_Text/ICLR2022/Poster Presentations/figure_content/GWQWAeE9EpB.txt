Figure 1: DictFormer achieves similar BLEU score with prior works using fewer parameters andMUlt-Adds on WMT 2014 De-En translation. DictFormer is flexible and scalable, e.g., for mo-bile devices, DiCtFOrmer-tiny can reduce 2.2× model size of lite transformer (WU et al., 2020)that achieves state-of-the-art performance under mobile settings. Results of existing works comefrom their own implementations and #ParamS does not include embedding parameters, e.g., Trans-former (Vaswani et al., 2017) has 44M #Params.
Figure 2: Our DictFormer replaces N -layer unshared, large attention and FFN weights WiA , WiF(i ∈ [0, N - 1]) in transformer with smaller, shared dictionaries DA, DF and coefficients CiA,CiF . The Mult-Adds operations between weights and inputs in Transformer are also reduced by ourdictionary look-ups and few linear projections with coefficients.
Figure 3: (a) An example of looking up dictionary with indices and scaling it with coefficients. Thislookup and scaling is able to reconstruct N -layer weight. (b) An example of looking up group-wisedictionary and its scaling for large-dimension representation projection.
Figure 4: Since index I in DictFormer is not differentiable, we train sparse coefficients Z instead ofjointly training I and C. After training, sparse Z is converted to I and C for deployment.
Figure 5: The block comparison of (a) transformer and (b) our DictFormer.
Figure 6: (a) Transformer with weights sharing. It contains three parts including embedding, Nsencoder blocks and Ns decoder blocks. Each encoder/decoder block contains attention and Feed-Forward Network (FFN). The embedding size is ds and FFN feature size is 4 × ds . Weights in thei-th Attention and FFN are denoted as WiA and WiF. To match or improve accuracy of transformerw/o weights sharing, transformer w/ wights sharing should be wider (ds > d) or deeper (Ns > N),where d and N are the embedding size and blocks number of transformer w/o wight sharing. (b)Our DictFormer with dictionary sharing. WiA and WiF are represented by smaller dictionaries DA ,DF, and coefficients CiA and CiF, where dictionary size m < d, coefficient size t << d.
Figure 7: The performance of DictFormer improves with an increase in the number of model pa-rameters, across different corpora.
Figure 8: Increasing the size of dictionary (a) and coefficients (b) improves DictFormer's perfor-mance. (a) and (b) are tested on WMT'14 En-Fr dataset for machine translation task.
