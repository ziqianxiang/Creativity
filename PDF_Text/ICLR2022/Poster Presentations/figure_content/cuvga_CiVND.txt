Figure 1: Validation resultsof fine-tuning BERT-base atdifferent sparsity levels onthe RTE dataset (Wang et al.,2018) in Liang et al. (2021).
Figure 2: The sensitivity distribution of the BERT-base models fine-tuned on GLUE tasks. Note thatwe drop some outliers to ease visualization.
Figure 3: Upper: Model generalization performance at different pruning ratios; Lower: Change ingeneralization performance with respect to the full model. Pruning is conducted on the fine-tunedBERT-base models.
Figure 4: The local temporal variation of sensitivity (with β0 = 0.7) during training.
Figure 5: Decision boundary predicted on theSpiral dataset. The white curve on Adam-SAGE corresponds the decision boundary ofAdam.
Figure 6: Learning curves obtained by fine-tuning BERT-base on SST-2 dataset.
Figure 7: Validation accuracy obtained by fine-tuning BERT-base on RTE dataset with a wide rangeof hyper-parameters.
Figure 8: Parameter study on learning rate and β0 .
