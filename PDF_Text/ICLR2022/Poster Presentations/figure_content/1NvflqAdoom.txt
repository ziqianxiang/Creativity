Figure 1: A demonstration of the Silent Alignment effect. (a) We trained a 2-layer ReLU MLPon P = 1000 MNIST images of handwritten 0’s and 1’s which were whitened. Early in training,around t ≈ 50, the NTK aligns to the target function and stay fixed (green). The kernel’s overallscale (orange) and the loss (blue) begin to move at around t = 300. The analytic solution for themaximal final alignment value in linear networks is overlayed (dashed green), see Appendix E.2.
Figure 2: The evolution of the kernel’s eigenfunctions happens during the early alignment phase fortι ≈ S, but significant evolution in the network predictions happens for t >t2 = 11 log(sσ-2). (a)Contour plot Ofkernel's norm for linear functions f (x) = β ∙ x. The black line represents the spaceof weights which interpolate the training set, ie X>β = y. At initialization, the kernel is isotropic,resulting in spherically symmetric level sets of RKHS norm. The network function is representedas a blue dot. (b) During Phase I, the kernel’s eigenfunctions have evolved, enhancing power in thedirection of the min-norm interpolator, but the network function has not moved far from the origin.
Figure 3:	(a) Time to half loss scales in a power law with σ for networks with L ≥ 3:ti/2 〜(L-2)σ-L+2 (black dashed) is compared with numerically integrating the dynamicsC(t) = c2-2/L(S — c) (solid). The power law scaling of t1∕2 with σ is qualitatively different thanWhat happens for L = 2, where We identified logarithmic scaling t“ 〜log(σ-2). (b) Linearnetworks with D = 30 inputs and N = 50 hidden units trained on synthetic whitened data with∣β∣ = 1. We show for a L = 3 linear network the cosine similarity of W 1> W1 with ββ> (dashed)and the loss (solid) for different initialization scales. (c) The time to get to 1/2 the initial loss andthe time for the cosine similarity of W1> W1 with ββ> to reach 1/2 both scale as σ-L+2, howeverone can see that alignment occurs before half loss is achieved.
Figure 4: Anisotropy in the data introduces multiple timescales which can interfere with the silentalignment effect in a ReLU network. Here we train an MLP to do two-class regression using Adamat learning rate 5 × 10-3. (a) We consider the partial whitening transformation on the 1000 CIFAR-10 images λk → λkγ for γ ∈ (0, 1) for covariance eigenvalues Σvk = λkvk. (b) The loss dynamicsfor unwhitened data have a multitude of timescales rather than a single sigmoidal learning curve. Asa consequence, kernel alignment does not happen all at once before the loss decreases and the finalsolution is not a kernel machine with the final NTK. (c) The network’s test error on classification. (d)Anisotropic data gives a slower evolution in the kernel’s Frobenius norm. (e) The kernel alignmentvery rapidly approaches an asymptote for whitened data but exhibits a longer timescale for theanisotropic data. (f) The final NTK predictor gives a better predictor for the neural network whenthe data is whitened, but still substantially outperforms the initial kernel even in the anisotropic case.
