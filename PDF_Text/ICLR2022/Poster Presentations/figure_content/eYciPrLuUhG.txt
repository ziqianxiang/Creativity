Figure 1: Visualization of the two training stages of ENCO, distribution fitting and graph fitting,on an example graph with 3 variables (X1, X2, X3). The graph on the right further shows how theparameters γ and θ correspond to edge probabilities. We learn those parameters by comparingmultiple graph samples on how well they generalize from observational to interventional data.
Figure 2: ENCO estimates gradientsof significantly lower variance com-pared to (Bengio et al., 2020).
Figure 4: Experiments on graphs with interventions on fewer variables. Additional graphs are shownin Appendix D.2. ENCO outperforms DCDI on bidiag and jungle, even for very few interventions.
Figure 3: Evaluating SDL DCDL and ENCO onlarge graphs in terms of SHD (lower is better).
Figure 5: Visualizing the gradient calculation for the incoming edges of X2 in an example graph withthree variables. The intervention is being performed on X1, and the data is used to calculate the log-likelihood estimates under the three randomly sampled graphs: LC1 (X2), LC2 (X2) and LC3 (X2).
Figure 6: Plotting the gradient estimate variance of ENCO for three variables of γ compared toprevious REINFORCE-like approach by Bengio et al. (2020) on an example graph with K = 100.
Figure 7: Example graph for which we check the convergence conditions described in Appendix B.2.
Figure 8: Causal graph structures for which, under specific parameterization of the conditionaldistributions, the conditions for guaranteeing convergence can be violated.
Figure 9: Visualization of the different graph structures we need to consider in the guaranteediscussion of latent confounder detection. Latent variables Xl are shown in white, all other variablesare observed. (a) The two children of Xl, X1 and X2, are independent of each other. (b) X1 is anancestor of X3, and the two variables have a shared latent confounder Xl.
Figure 10: Visualization of the common graph structures for graphs with 8 nodes. The graphs used inthe experiments had 25 nodes. Note that the graph random is more densely connected for largergraphs, as the number of possible edges scales quadractically with the number of nodes.
Figure 11: Learning curves of ENCO in terms of recall and precision on edge predictions for syntheticgraph structures. The orientations for the ground-truth edges are not plotted as they have usually beencorrectly learned after the first epoch except for the graph full. Overall, we see that the edge recallstarts very high for all graphs, and the precision catches up over the epochs. This is in line the stepsin the convergence proof in Section B.
Figure 12: (a) Example graph of 100 variables (best viewed electronically). Every node has onaverage 8 edges and a maximum of 10 parents. (b) Plotting recall and precision of the edge predictionsfor the training on graphs with 1, 000 nodes. The small standard deviation across graphs shows thatENCO can reliably recover large graphs.
Figure 13: Left: Example of a latent confounder scenario, where Xl is not observed and introduces adependency between Xi and Xj on observational data. The dots on the left and right represent even-tual (observed) parents of Xi and Xj. Right: Plotting the average score lc(Xi, Xj) for confoundersXi — Xi → Xj in the true causal graph (orange) and maximum score of any other node pair (blue).
Figure 14: Results of ENCO for different graph structures under limited interventional data samplesize. Note the different scale of the y-axis for the six graphs. While the general trend is the same forall graphs, i.e. decreasing performance with fewer samples, the order heuristic can reduce the SHDerror by a considerable margin for most graphs.
Figure 15: Results of ENCO for different graph structures under limited observational data samplesize. Note the different scale of the y-axis for the six graphs. The structure learning performanceremains good for sparse graphs, and suffers for graphs with larger parent sets.
Figure 16: Results of ENCO and DCDI for different graph structures under fewer interventionsprovided. Note the different scale of the y-axis for the six graphs. For four out of six graphs, ENCOoutperforms DCDI even for as few as 4 interventions, especially when enforcing acyclicity. Thedetailed numbers of the results are listed in Table 13.
Figure 17: Example graph for the deterministic setup. We use the random setup with edge proba-bility 0.1 and limit number of parents to 3. All variables except the leaf nodes have deterministicdistributions.
Figure 18: Example graph for cancelling paths X1 → X2 → X3 and X1 → X3 . This can, forinstance, occur when the conditional distribution of X2 is a Dirac delta around X1: p(X2|X1) =δ[X2 = X1].
