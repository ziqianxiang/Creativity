Figure 1: The grouped truncation and its performance. The truncation of the 10th group, which hasthe smallest singular values resulting from SVD, is expected to have a minor performance impact(i.e., follow the ideal trend of red dashed line), but this may not be true in actual cases (blue bar).
Figure 2:	The dilemma of vanilla SVD. Some parameters (the overlap of meshed orange and green)that significantly impact the task performance may not be reconstructed well by SVD because theirassociated singular values are small and truncated.
Figure 3:	The schematic effect of our FiSher-Weighted SVD (FWSVD). IiSa diagonal matrixcontaining estimated Fisher information of parameters. By involving Fisher information to weighthe importance, our method reduces the overlap between meshed orange and green, making lessperformance drop after truncation.
Figure 4: The three paths to create compressed language models are examined in this paper. L/Sdenote the initial models, Lg/Sg are models after generic pre-training, L/St correspond to task-specific models, and Ltf /Stf are factorized task-specific models. Detailed elaborations are in Sec-tions 5.1 nad 5.2.2Table 1: Results of CoNLL and GLUE benchmark. G-Avg means the average of GLUE tasks,A-Avg denotes the average of all tasks, including CoNLL. Our FWSVD+fine-tuning is the bestperformer in terms of both average scores, without the expensive generic pre-training required bypath-1 models (e.g., DistillBERT costs 720 V100 GPU hours for training).
Figure 5: FWSVD versus SVD by varying the ratio of reserved ranks. The model with a rank ratio1.0 indicates the full-rank reconstruction with the same accuracy as the original model (i.e., the Ltin Figure 4). Note that all the models here do not have fine-tuning after factorization.
Figure 6: Results of grouped rank truncation on STS-B task. In (a), FWSVD shows a consis-tent trend of having less performance drop with the small singular value groups (group 10 has thesmallest singular values), mitigating the issue of Figure 1. In (b), FWSVD results in a larger recon-struction error with almost all truncated groups, although FWSVD retains the model accuracy betterthan SVD.
Figure 7: The grouped rank truncation experiment. The experiments are the same with Figure 6a,but we use ALBERTbase (11.7M parameters) model for this figure.
Figure 8:	The grouped rank truncation experiment. The experiments are the same with Figure 6a,but this figure includes all 8 language tasks with BERTbase (109.5M parameters) model. FWSVDhas a smaller performance drop with those groups truncated first (e.g., group 5 to 10) in all the cases.
Figure 9:	This figure shows only groups 3 to 10 of Figure 8 to better visualize the groups ofa smallerperformance drop.
