Figure 1: LIA animation examples. The two images of Marilyn Monroe and Emmanuel Macronare animated by LIA, which transfers motion of a driving video (smaller images on the top) fromVoxCeleb dataset (Chung et al., 2018) onto the still images. LIA is able to successfully animatethese two images without relying on any explicit structure representations, such as landmarks andregion representations.
Figure 2: General pipeline. Our objective is to transfer motion via latent space navigation. Theentire training pipeline consists of two steps. Firstly, we encode a source image xs into a latent codezs→r . By linearly navigating zs→r along a path wr→d, we reach a target latent code zs→d. Thelatent paths are represented by a linear combination between a set of learned motion directions (e.g.,d1 and d2), which is an orthogonal basis, and associated magnitudes. In the second step, we decodezs→d to a target dense optical flow field φs→d, which is used to warp xs into the driving image xd .
Figure 3: Overview of LIA. LIA is an autoencoder consisting of two networks, an encoder E and agenerator G. In the latent space, we apply Linear Motion Decomposition (LMD) towards learninga motion dictionary Dm , which is an orthogonal basis where each vector represents a basic visualtransformation. LIA takes two frames sampled from the same video sequence as source image xsand driving image xd respectively during training. Firstly, it encodes xs into a source latent codezs→r and xd into a magnitude vector Ar→d . Then, it linearly combines Ar→d and a trainable Dmusing LMD to obtain a latent path wr→d, which is used to navigate zs→r to a target code zs→d.
Figure 4: Qualitative results. Examples for same-dataset absolute motion transfer on TaichiHD(top-right) and TED-talk (bottom-right). On VoxCeleb (left), we demonstrate cross-dataset relativemotion transfer. We successfully transfer motion between x1 and xt from videos in VoxCeleb to xsfrom FFHQ, the latter not being used for training.
Figure 5: Visualization of reference images. Example source (top) and reference images (down)from VoxCeleb, TaichiHD and TED-talk datasets. Our network learns reference images of a consis-tently frontal pose, systematically for all input images of each dataset.
Figure 6: Linear manipulation of four motion directions on the painting of Mona Lisa. Manip-ulated results indicate that d6 represents eye movement, d8 represents head nodding, whereas d19and d7 represent facial expressions.
Figure 7: Encoder architecture. We show details of architecture of E in (a) and ResBlock in (b).
Figure 8: Generator architecture. We show details about architecture of G in (a) and G block in(b).
Figure 9: Reference image generation.
Figure 10: Generated results with and without Dm . We observe that the disentanglement ofappearance and motion is much better by using Dm .
Figure 11: Failure cases. We observed that it is still challenging for LIA to handle arm-leg occlusion(Taichi) and hand motion (TED-talk).
