Figure 1: Illustration showing the intu-ition behind critical states. The agent issupposed to navigate to a high-reward regionmarked as the yellow polygon, without crash-ing into the walls. For different states, A, Band C that we consider, the agent has a highvolume of actions that allow it to reach thegoal at states A and C, but only few actionsthat allow it to do so at state B. States aroundA and C are not critical, and so this task hasonly a small volume of critical states (i.e.,those in the thin tunnel).
Figure 2: Illustration showing the intu-ition behind noisy data. BC trained on ex-pert data (data composition is shoWn on theleft) may diverge aWay from the expert andfind a poor policy that does not solve the task.
Figure 3: IQM performance of various algorithms evaluated on 7 Atari games under various dataset composi-tions (per game scores in Table 3). Note that offline-tuned CQL with expert data (“Tuned CQL”) outperformscloning the expert data ("BC (Expert)”)，even though naive CQL is comparable to BC in this setting. When CQLis provided with noisy-expert data, it significantly outperforms cloning the expert policy.
Figure 4: (Figure 1 restated) Illustration showing the intuition behind critical points in a navigation task.
Figure 5: (Figure 2 restated) Illustration showing the intuition behind suboptimal data in a simplenavigation task. BC trained on expert data (data composition is shown on the left) may diverge away from theexpert and find a poor policy that does not solve the task. On the other hand, if instead of expert data, offline RLis provided with noisy expert data that sometimes ventures away from the expert distribution, RL can use thisdata to learn to stay on the course to the goal.
Figure 6: Renderings of three gridworld domains we evaluate on, where states are colored as: Start:blue,Goal:green, Lava:red, Wall:grey, and Open:white. The domains have varying number of critical points. Left:Single Critical. Middle: Multiple Critical. Right: Cliffwalk.
Figure 7: Filmstrip of the three tasks that We StUfy for robotic manipulation - open-grasp, close-open-grasp andpick-place-open-grasp.
Figure 8: Offline RL Vs BC on gridworld domains. Left: We compare offline RL and BC on three differentgridworlds with varying number of critical points for expert and near-expert data. Right: Taking the “MultipleCritical” domain, we examine the effect of increasing the noisiness of the dataset by interpolating it with onegenerated by a random policy, and show that RL improves drastically with increased noise over BC.
