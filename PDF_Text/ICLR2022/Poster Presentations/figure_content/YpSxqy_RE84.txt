Figure 1: Test error vs memory for ResNet-18 across 99low-precision floating point configurations. Figure (a)shows the tradeoff on CIFAR-10. (Non-dominated pointsare blue circles.) Figure (b) shows that the best precisionto use varies depending on the memory budget, on 87image datasets. See Section 4 for experimental details.
Figure 2: The PEPPP workflow. We begin with a collection of (meta-) training datasets and lowprecision configurations. In the meta-training phase, we sample dataset-configuration pairs to train,and compute the misclassification error. We use matrix factorization to compute a low dimensionalembedding of every configuration. In the meta-test phase, our goal is to pick the perfect precision(within our memory budget) for the meta-test dataset. We compute the memory required for eachconfiguration, and we select a subset of fast, informative configurations to evaluate. By regressingthe errors of these configurations on the configuration embeddings, we find an embedding for themeta-test dataset, which we use to predict the error of every other configuration (including moreexpensive ones) and select the best subject to our memory budget.
Figure 3: An 8-bit floating point number rep-resenting (-1)sign ∙ 2exponent-7 ∙ Lb3b2bMneural network (as discussed in Section 3.1), these two formats are called Format A and B. A specificcombination of these two formats is a hyperparameter setting of the neural network, and we call it alow-precision configuration.
Figure 4: Example error and memory matricesfor some datasets and low-precision configurations.
Figure 5: Memory us-age under two trainingparadigms. Both train aResNet-18 on CIFAR-10with batch size 32.
Figure 6: Kendall tau correlation of test er-ror of all configurations between all pairs ofdatasets, and singular value decay of corre-sponding error matrix. Strong correlations al-low PEPPP to succeed with a few measure-ments. Details in Appendix A.
Figure 7: Meta-test on CIFAR-10. After meta-trainingon all other datasets in Appendix A Table 2, we useED-MF to choose six informative measurements (or-ange squares) with a 275MB memory limit for eachmeasurement on CIFAR-10. Then we estimate test er-rors of other configurations by ED-MF, and restrictour attention to configurations that we estimate to benon-dominated (red x’s). Note some of these are in factdominated, since we plot true (not estimated) test error!Finally we select the estimated non-dominated configu-ration with highest allowable memory (blue square).
Figure 8: Illustration of Pareto frontier metrics.
Figure 9: Pareto frontier estimation in PEPPP meta-training, with uniform sampling of configurations.
Figure 10: Error vs memory on CIFAR-10 withtrue and estimated Pareto frontiers from uni-form sampling in PEPPP meta-training. A 20%uniform sample of entries yields a better esti-mate of the Pareto frontier (convergence 0.03and HyperDiff 0.02) compared to a 5% sample(convergence 0.09 and HyperDiff 0.16).
Figure 11: Pareto frontier estimates in meta-LOOCV Setting I and IV (with a 20% meta-trainingsampling ratio and an 816MB meta-test memory cap). Each error bar is the standard error acrossdatasets. The x axis measures the memory usage relative to exhaustively searching the permissibleconfigurations. ED-MF consistently picks the configurations that give the best PF estimates.
Figure 12: Relative performance with re-spect to ED-MF in meta-test Setting IVwhen making 3 measurements (memoryusage 〜10%) on 10 ImageNet partitions.
Figure 13: Convexification vs greedy for ED.
Figure 14: Explained variance of the first several singular values in Figure 6(b).
Figure 15: Histograms of error and memory. Thedashed lines are the respective medians.
Figure 16: Histogram of datasets by thenumber of configurations that take memo-ries less than the overall median of 816MB.
Figure 17: Pareto frontier estimation performance in PEPPP meta-training with non-uniform sampling of configurations. The violinsand scatters have the same meaning as Figure 9. The x axismeasures the memory usage relative to exhaustive search.
Figure 18: Pareto frontier estimates in meta-LOOCV Setting II (full meta-training error matrix, a816MB memory cap), Setting III (uniformly sample 20% meta-training measurements, no meta-testmemory cap), Setting V (non-uniformly sample 20% meta-training measurements, no meta-testmemory cap), and Setting VI (non-uniformly sample 20% meta-training measurements, an 816MBmeta-test memory cap). Each error bar is the standard error across datasets. ED-MF is among thebest in every setting and under both metrics.
Figure 19:	Errors of 99 configurations trained for different numbers of epochs.
Figure 20:	CIFAR-10 error-memory tradeoff. Figure (a) has learning rate 0.001 for all low-precisionconfigurations. Figure (b) shows the tradeoff with tuned learning rates: at each low-precisionconfiguration, the lowest test error achieved by learning rates {0.01, 0.001, 0.0001} is selected.
Figure 21:	Singular value decay of the LR-tuned error matrix.
Figure 22:	The Pareto frontier estimation performance in meta-training, with uniform sampling ofconfigurations on the LR-tuned error and memory matrices. Similar to Figure 9, the violins show thedistribution of the performance on individual datasets, and the error bars (blue) show the range. Thered error bars show the standard deviation of the error on CIFAR-100 aquatic mammals and learningrate 0.01, across 100 random samples of the error matrix. Figure (a) shows the matrix completionerror for each dataset; Figure (b) and (c) show the performance of the Pareto frontier estimates inconvergence and HyperDiff.
Figure 23:	The Pareto frontier estimation performance in meta-training, with non-uniform samplingof configurations on the LR-tuned error and memory matrices. The violins and scatters have the samemeaning as Figure 17 in the main paper.
Figure 24:	Pareto frontier estimates in meta-LOOCV settings on the LR-tuned error and memorymatrices. Each error bar is the standard error across datasets.
Figure 25:	Pareto frontier estimates in meta-LOOCV Setting I when learning across architectures:from ResNet-18 to either ResNet-34, or to VGG variants. Each error bar is the standard error acrossdatasets. The x axis measures the memory usage relative to exhaustively searching the permissibleconfigurations. ED-MF consistently picks the configurations that give the best PF estimates.
Figure 26: Benefit of meta-learning across architectures. Each error bar is the standard error acrossarchitecture-dataset combinations (e.g., ResNet-18 + n02470899 is a combination). The x axismeasures the memory usage relative to exhaustively searching the permissible configurations.
