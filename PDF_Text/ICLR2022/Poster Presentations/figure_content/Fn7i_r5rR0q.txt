Figure 1: The expected KL divergence (eKLD, Eq. 2) measures how the classifier’s estimated label probabil-ities Pw(∙∣x) change under nuisance transformation, With lower eKLD corresponding to more invariance.
Figure 2: Samples from MIITNs trained to learn the nuisance transformations of K49-BG-LT (backgroundintensity variation) and K49-DIL-LT (dilation/erosion). Each row shows multiple transformations of the sameoriginal image. We see a diversity of learned nuisances, even for inputs from the smallest class (bottom row).
Figure 3: We observe that the expected KL divergence(eKLD) is lower for smaller classes when using genera-tive invariance transfer (GIT). That is, GIT makes clas-sifiers more uniformly invariant to the nuisance trans-form regardless of class size.
Figure 4: Samples from MIITNs trained to learn the nuisances of GTSRB-LT and CIFAR-100-LT, for use inGIT training. Each row shows sampled transormations of a single input image. We see the diversity of learnedtransformations, including changing lighting, object color/texture, and background.
Figure 5: Test accuracy vs trainclass size for CIFAR-10 LT. Ap-plied naively (in red), GIT per-forms better on smaller classesand worse on larger ones.
Figure 6: Per-class eKLD curves on three K49-LT variants for classifiers trained by different meth-ods using two different architectures: ResNet20 (top) and a simple CNN (bottom). In each case weesee that classifiers learn invariance to the nuisance transform unevenly, with poor invariance for thetail classes. We also see that adding GIT helps reduce eKLD and improve invariance on the tail.
Figure 7: Random training examples from each of the 6 datasets considered in this work, withexamples from both the largest and smallest classes in each dataset.
Figure 8: The expected KL divergence (eKLD) on synthetic K49 datasets that contain the same number oforiginal K49 examples across all classes. These datasets are long-tailed only in the amount of transformationsobserved, and are designed to isolate the effect of the transformation. Each class starts with 5 examples fromthe original Kuzushiji-49 dataset. Larger classes are created by repeatedly sampling more transformations ofthe same 5 originals. This design ensures that the only difference between large and small classes is the amountof transformation diversity. These new datasets are easier to learn, but show the same qualitative trend as Fig. 1.
