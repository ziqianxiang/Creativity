Figure 2:	Estimated bound and empirical generalization gap (“gap”) as functions of network width((a) and (b)) and label noise level ((c) and (d)). Note that Y-axis is in log scale.
Figure 3:	Dynamics of gradient dispersion, in relation to training/testing accuracies.
Figure 4: Dynamic Gradient Clipping.
Figure 5: Comparison Between Theorem 2 and Lemma 2 in stochastic setting.
Figure 6: The impact of learning rate and batch size on the trajectory term and the flatness term inEq. 3A key observation in Figure 6 is that the learning rate impacts the trajectory term and the flatnessterm in opposite ways, as seen, for example, in (a) and (b), where the two set of curves swaptheir orders in the two figures. On the other hand, the batch size also impacts the two terms inopposite ways, as seen in (a) and (b) where curves decrease in (a) but increase in (b). This makesthe generalization bound, i.e., the sum of the two terms, have a rather complex relationship with thesettings of learning rate and batch size. This relationship is further complicated by the fact that asmall learning rate requires a longer training time, or a larger number T of training iterations, whichincreases the number that are summed over in the trajectory term. Nonetheless, we do observe thata smaller batch size gives a lower value of the flatness term ((b) and (d)), confirming the previouswisdom that small batch sizes enable the neural network to find a flat minima (Keskar et al., 2017).
Figure 7: Generalization gap of a linear network v.s. Theorem 3. Note y-axe is log-scale.
Figure 8: Generalization gap of a two-layer ReLU network v.s. Theorem 4.
Figure 9: Dynamic Gradient Clipping (AlexNet).
