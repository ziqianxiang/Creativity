Figure 1: We use the LiDAR point cloud as the weak supervision in training. (a): the aligningprocess; (b): the alignment ambiguity problem; (c): unevenly distributed LiDAR points.
Figure 2: Network architecture. In the inference stage, we require only a single RGB image asinput and output corresponding 3D boxes. In the training stage, we use LiDAR point cloud and 2Dboxes estimated from a pre-trained model to obtain object-LiDAR-points, which are used to buildlosses with 3D box predictions to train the network. Best viewed in color.
Figure 3: Adverse impacts ifusing only the center loss. Bluepoints refer to object-LiDAR-points. Even given an ideal ini-tial 3D box that is close to thegroundtruth, the network con-ducts a much worse predictionafter training. Best viewed incolor with zoom in.
Figure 4: Loss design. We introduce geometric alignment loss (Section 3.2) and ray tracing loss(Section 3.3) with loss balancing (Section 3.4) for each object-LiDAR-point and the corresponding3D box prediction. The geometric alignment loss focuses on tightly aligning 3D box predictionswith object-LiDAR-points. The ray tracing loss further takes occlusion constraints of camera imag-ing/LiDAR scanning into consideration, to eliminate alignment ambiguity (See Figure 5) in thescene. Best viewed in color with zoom in.
Figure 5: Alignment ambiguity whenaligning with object-LiDAR-points.
Figure 6: Orientation distribution. We calculate the direction of each pair of points among object-LiDAR-points and draw the histogram. We can observe that the direction that is highest in thehistogram is highly related to the object orientation. Best viewed in color with zoom in.
Figure 7: Regression network. nbdenotes the number of objects, andno denotes the number of required3D box parameters.
Figure 8: Details of obtaining Object-LiDAR-points. Best viewed in color.
Figure 9: Qualitative results. Best viewed in color.
Figure 10: Local orientation δy and global orien-tation θy . The same object with the same globalorientation at different viewpoints shows differentappearances on the image.
Figure 12: Distribution of directions of paired points. The red line on the box refers to themainly captured surface, meaning that LiDAR points captured from this surface dominate the object-LiDAR-Points. A low offset dx refers to the orientation close to π∕2 while a high offset indicatesthe orientation close to 0 or π . Best viewed in color with zoom in.
Figure 11:	Illustra-tions of Paired Points.
Figure 13: Statistics on 3D object labels on KITTI.
Figure 14: Theoretical failure cases caused by viewpoint differences between the LiDAR andcamera. A well known yet important fact is that the LiDAR and camera are installed in the samecar/robot, which indicates that their viewpoint differences are marginal compared to the extensive 3Dspace. We summarize the theoretical failure cases caused by the viewpoint differences. For case 1, itcannot happen since the camera and LiDAR always face towards objects. The occlusion constraintsfor surface 1 and 3 are the same for the camera and LiDAR. For case 2, the object width Wobjectis lower than Wcam->lidar (the offset between camera and LiDAR along X axis) meanwhile theobject center is very close to the ego-car/robot center along X axis. Such objects are few. Also, theadverse impact brought by this type of mismatching can be ignored since Wobject is very small. Forcase 3, the conditions are similar to case 2, but the object can have a bigger Wobj ect. In this case, theLiDAR mainly captures points from surface 3, such points dominate the loss since points capturedfrom surface 2 are very few. Thus mismatches on surface 2 can also be ignored in the overall loss.
