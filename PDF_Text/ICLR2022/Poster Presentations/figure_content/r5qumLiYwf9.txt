Figure 1: Random batches of StyleGAN2 (ψ = 0.5) samples with 1024 × 1024 resolution, generated usingstandard sampling (left), uniform sampling via MaGNET on the learned pixel-space manifold (middle), anduniform sampling on the style-space manifold (right) of the same model. MaGNET sampling yields a highernumber of young faces, better gender balance, and greater background/accessory variation, without the needfor labels or retraining. Images are sorted by gender-age and color coded red-green (female-male) accordingto Microsoft Cognitive API predictions. Larger batches of images and attribute distributions are furnished inAppendix E.
Figure 2: Visual depiction of Eq. 2with a toy affine spline mapping S :R2 7→ R3 . Left: latent space parti-tion Ω made of different regions shownwith different colors and with bound-aries shown in black. Right: affinespline image Im(S) which is a contin-uous piecewise affine surface composedof the latent space regions affinely trans-formed by the per-region affine mappings(Eq. 1). The per-region colors maintaincorrespondence from the left to the right.
Figure 3: Distribution of the number of MNIST training samples with η neighbors generated within an -ballradius. N samples are generated using standard sampling and MaGNET sampling using an NVAE model. Hereis taken to be the average nearest neighbor distance for the training samples. For Vanilla NVAE, heavier tailsare indicative of larger density variations on the manifold as N is increased, whereas for MaGNET the shortertails are indicative of fewer variations in neighborhood density, i.e., uniform generation on the data manifold.
Figure 4: (Left) Average log-likelihood and 10σ-bandwidth for 5 runs of a GMM trained on 10,000 samplesusing standard sampling (blue) and MaGNET sampling (black) for an NVAE trained on MNIST. The higherlog-likelihood (given the same number of clusters) in the standard sampling case demonstrates an increasedconcentration around a few modes, as opposed MaGNET. (Right) FID (J) of StyleGAN2 (Config-f) trained onFFHQ for 50,000 generated samples and 7 runs. With an increasing percentage of uniformly generated samplesto increase diversity, MaGNET reaches state-of-the-art FID of 2.66 achieved at a 4.1% mixture.
Figure 5: From left to right, samples from a toy 2D distribution with triangular support, biased samples ob-tained for training a GAN, standard sampling showing a biased distribution learned by the GAN, and MaGNETsampling recovering uniformly distributed samples on the support of the true distribution. Note that the samenumber of samples are obtained for both standard and MaGNET sampling.
Figure 6: Hue (color) distribution of samples ob-tained from standard and MaGNET sampling from atrained BVAE model on colored 8 digits from MNIST.
Figure 7: Random batch of samples generated from BigGAN (left) and MaGNET BigGAN (right), condi-tioned on the Samoyed class of ImageNet. While BigGAN samples contain homogeneous postures, MaGNETsamples represent the true span of the data manifold learned by BigGAN.
Figure 8: LiPSChitZ constant estimation using standard DGN sampling and MaGNET sampling (left). Eachline represents the mean over 200 Monte-Carlo runs. As expected, estimation of such statistics from samplesconverges faster when employed on uniformly distributed samples. The standard deviation of the Monte-Carloestimations are also provided (right), where it is clear that the uniform sampling via MaGNET reaches smallerstandard deviation at earlier steps i.e., it is faster to converge. This is a key application of MaGNET: speeding-up convergence of statistical estimation of quantities, such as the LipschitZ constant in this case. The x-axisrepresents the number of samples used for estimation in log-scale.
Figure 9: Random batch of samples generated from vanilla progGAN (left) and MaGNET progGAN (right).
Figure 10:	Evolution of the precision/recallcurves for varying number of samples N formthe monte-carlo sampling against the number ofsamples K = 5k for StyleGAN2.
Figure 11:	Precision-recall curves for K = 70ksamples from Vanilla StyleGAN2 and MaGNETStyleGAN2-----Original0.0050.0040.0030.0020.0010.0000.0ττ0.5ττ1.0ττ	1.5π 2.QπFigure 12:	Depiction of the imbalance hue distri-bution applied to color the MNIST digits.
Figure 12:	Depiction of the imbalance hue distri-bution applied to color the MNIST digits.
Figure 13:	Reprise of Fig. 3. Vanilla NVAE Left, MaGNET NVAE Right16Published as a conference paper at ICLR 2022Number of training samples havingx (x-axis) samples at most awayI «	I «	I «	I «	I «——N = 1000——N = 2000N = 5000W = 8000N = 10000IO3IO2IO1IO00	200	400	600	800 1000 1200 1400 1600	0	100	200	300	400	500number of samples within 1.5-ball radiusFigure 14:	Reprise of Fig. 3. Vanilla NVAE Left, MaGNET NVAE Right17Published as a conference paper at ICLR 2022
Figure 14:	Reprise of Fig. 3. Vanilla NVAE Left, MaGNET NVAE Right17Published as a conference paper at ICLR 2022Figure 15: Random batches of 245 samples from a StyleGAN2 trained on FFHQ, generated viastandard sampling (left), MaGNET sampling in the pixel-space (middle) and MaGNET sampling inthe style-space.
Figure 15: Random batches of 245 samples from a StyleGAN2 trained on FFHQ, generated viastandard sampling (left), MaGNET sampling in the pixel-space (middle) and MaGNET sampling inthe style-space.
Figure 16: Random Samples from vanilla progGAN (left) and MaGNET progGAN (right) trainedon the CelebA-HQ dataset. Samples are sorted by gender & age and color coded by gender asvisually predicted by the Microsoft Cognitive API. Samples not recognized by the API are colorcoded as white at the bottom.
Figure 17: Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from theCollie class of Imagenet.
Figure 18: Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from theSiamese class of Imagenet.
Figure 19: Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from theTabby class of Imagenet.
Figure 20: Facial Attributes of 5000 StyleGAN2 samples using vanilla sampling, MaGNET style-space sampling and MaGNET pixel-space sampling. We see that MaGNET style-space increasesuniformity in gender and age distributions whereas MaGNET pixel-space yields more variations inphysical attributes and accessories.
Figure 21: Facial Attributes of 5000 ProgGAN samples usingsampling.
Figure 22: Random Samples from vanilla NVAE (left) and MaGNET NVAE (right) trained on theMNIST dataset.
Figure 23: Random Samples from vanilla NVAE (left) and MaGNET NVAE (right) trained on theCIFAR dataset.
