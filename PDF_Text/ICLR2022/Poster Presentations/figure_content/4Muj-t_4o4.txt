Figure 1: (Left) An illustration of the process of learning a subspace of policies. (Right) Comparisonbetween PPO and our model in a test environment.
Figure 2: The adaptation of the PPO Algorithm with the LoP model. The different with the standardPPO algorithm is that: a) trajectories are sampled using multiple policies θzi b) The policy loss isaugmented with the auxiliary loss, and c) The critic takes the values Zi as an input.
Figure 3: (left) Performance of the models at train time that shows that for LoP β is not hurtingtrain performance while it is DIAYN+R. Standard error deviation is reported in Table B.2 for eachenvironment. We also report the performace at train time that shows that a too high value of βhurts DIAYN+R performance while is less critical in LoP. (right) Ablation study on the number ofpolicies K used at test time on 3 HalfCheetah environment variations (see Appendix B.1 for furtherdetails and additional results) together with the performance of the BoP and CoP variants. Standarddeviation is given in appendix, Table 4.
Figure 4: (left) Trajectories generated by K = 10 policies (one rows) on an unseen maze (objectiveis to go from left to right, see details in Appendix B.8) for DIAYN+R (left column with best βvalue) and LoP (right column with β = 1.0). It illustrates the diversity obtained with DIAYN+Rand LoP. (right) Number of times (y-axis) each policy (x-axis) (over K = 5 tested policies) is chosenover the 16 test environments of the HalfCheetah setting for each of the 10 seeds. Blue is LoP andorange is DIAYN+R. Different policies are used for different test environments showing the interestof learning a subspace of policies. Note that in LoP, the anchor policies are rarely chosen. Resultsfor K = 10 and K = 20 in Appendix (Figure 7).
Figure 5: Qualitative example of LoP trajectories on HalfCheetah "BigShins" test environment (5-shot setting). The best reward is obtained for z = 0.75.
Figure 6: Extreme case: when torso radius and mass are increased by 50%. Only one policy is ableto adapt without falling down (z = 0.5).
Figure 7: Number of times (y-axis) each policy (x-axis) is chosen by k-shot evaluation over the 16test environments of the HalfCheetah for each of the 10 seeds (one table per seed). In blue, LoP, inorange, DIAYN+R. Please note that the 10 same LoP models are used for K=5, K=10, K=20 whichis not the case for DIAYN+R.
Figure 8: We trained small discriminators over a dataset (100,000 environment interactions) of tra-jectories obtained with the learned policies of LoP and DIAYN+R when K=5. For each environment,for each seed, we trained a single discriminator and averaged the results. While the discriminatorstrained on DIAYN+R reach 100% accuracy rapidly on both train and test environments, they learnslower for LoP, with a slight advantage for the test environment, validating the fact that the diver-sity induced by the cosine similarity on the weights is more visible in variations of the environmentrather than the environment on which the model has been trained. We evaluated the discriminator ona validation dataset (also 100,000 environment interactions) resulting in 100% accuracy for DIAYNin both train and test environments. For LoP, we obtained 82% accuracy on the training environment,and 87% on the test environments. The discriminator architecture consists in a neural network oftwo hidden layers of size 16, taking the unprocessed states as an input and outputting the predictedpolicy used (like in DIAYN).
Figure 9: Evolution of the best reward obtained with respect to K for LoP (N=2) and CoP (N=3) foreach Halfcheetah test environment. We ran the K-shot evaluation for each K from K=1 to K=100using the method described in Appendix A.2: we simply sample K random coefficients using theuniform distribution over [0, 1] for LoP and the Dirichlet distribution over [0, 1]3 for CoP. Resultsare averaged over 10 run for each K, and over the 10 models we learned for each method.
Figure 10: Evolution of the cumulative reward during training on the generic Ant environment forLoP, DIAYN+R and Lc for different values of beta. On can see that DIAYN+R struggles to performwell on the train set for β = 1 and β = 10. Results are averaged over 10 seeds.
Figure 11: Trajectories learned by a Single Policy. First column is the training environment. Othercolumns are test environments. The lighter red the tiles are, the longer the agent stays on a particularlocation.
Figure 12: Trajectories learned by LoP (β = 1.0). Rows are environments, columns are the K = 10policies test during online adaptation. For each test environment, at least one policy is able to reachthe goal31Figure 13: Trajectories learned by DIAYN+R (β = 0.01). Rows are environments, columns are theK = 10 policies test during online adaptation. For many test environment, at least one policy is ableto reach the goal32Figure 14: Trajectories learned by DIAYN+R (β = 1). Rows are environments, columns are theK = 10 policies test during online adaptation. With a too high value of β, the training policy issuboptimal, and does not achieve good performance at train and test times.
Figure 13: Trajectories learned by DIAYN+R (β = 0.01). Rows are environments, columns are theK = 10 policies test during online adaptation. For many test environment, at least one policy is ableto reach the goal32Figure 14: Trajectories learned by DIAYN+R (β = 1). Rows are environments, columns are theK = 10 policies test during online adaptation. With a too high value of β, the training policy issuboptimal, and does not achieve good performance at train and test times.
Figure 14: Trajectories learned by DIAYN+R (β = 1). Rows are environments, columns are theK = 10 policies test during online adaptation. With a too high value of β, the training policy issuboptimal, and does not achieve good performance at train and test times.
