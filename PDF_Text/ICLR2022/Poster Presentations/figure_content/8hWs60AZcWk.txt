Figure 1: Overview ofthe proposed ViT us-ing discrete representa-tions. In addition to thepixel embeddings (or-ange), We introduce dis-crete tokens and em-beddings (pink) as theinput to the standardTransformer Encoder ofthe ViT model (Doso-vitskiy et al., 2020).
Figure 2: Comparisonof pixel tokens (top)and the reconstructedimage decoded fromthe discrete tokens(bottom). Discrete to-kens capture importantshapes and structuresbut may lose localtexture.
Figure 3: (a) Pseudo code for training the proposed ViT model. (b) Comparing visualized pixelembeddings of the ViT and our model. Top row shows the randomly selected filters and Bottomshows the first 28 principal components. Our filters capture more structural and shape patterns.
Figure 4: Visualization of theeight evaluation benchmarks.
Figure 5: We show the fraction of shape decisions on Stylized-ImageNet in Figure (a), and attentionon OOD images in Figure (b), where (i) is the attention map, and (ii) is the heat map of averagedattention from images in a mini-batch. See Appendix A.1.4 for details.
Figure 6: The robustness vs. #model-parameters on 4 robust test set. Our models (orange) achievebetter robustness with a similar model capacity.
Figure 7: Visualization of eight evaluation benchmarks. Each image consists of the original testimage (Left) and the decoded image (Right) from the finetuned discrete embeddings. The encoderand decoder are trained only on ImageNet 2012 data but generalize on out-of-distribution datasets.
Figure 8: Visualization of failure cases for the decoded images. Each image consists of the originaltest image (Left) and the decoded image (Right) from the finetuned discrete embeddings.
Figure 9: Comparing visualized pixel embeddings of the ViT and our model. The top row showsthe randomly selected filters and the Bottom shows the first 28 principal components. Our modelswith varying pixel dimensions are shown, where their classification performances are compared inTable 13. Ours (PiXeLdim=32) works the best and is used as the default model in the main paper.
Figure 10: Comparison of average attention of the ViT (top row) and the proposed model (bottomrow) on four validation datasets: ImageNet 2012, ImageNet-R, Stylized-ImageNet, and ObjectNet.
Figure 11: Attention comparison of the ViT and the proposed model on ImageNet 2012.
Figure 12: Attention comparison of the ViT and the proposed model on ImageNet-R.
Figure 13: Attention comparison of the VIT and the proposed model on ImageNet Sketch andObjectNet.
