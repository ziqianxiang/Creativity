Figure 1: Comparison of traditional and proposed methods.
Figure 2: Convergence of different-width models on DomainNet.
Figure 3: Illustration of dual batch-normalization(DBN) in training and inference. The BNc andBNn are for clean and noised samples.
Figure 4: Validation accuracy of budget-compatible full-wdith nets by iterations.
Figure 5: Client-wise statistics of test accuracy, training and communication efficiency by budgetCIfarQcnIIdDigitsDomainNetconstraints. The MACs quantify the complexity of one batch optimization in a client, and the numberof parameters per round round are the ones uploaded to (or downloaded from) a server. Test accuracyis by the full-width networks. The results of FedAvg are from budget-compatible ×0.125 nets.
Figure 6: Per domain in DomainNet, the total percentage of parameters that are locally trained (theleft figure) and the accuracy (%) drops compared to FedAvg individual models (right two figures).
Figure 7: Trade-off between robust accuracy (RA) and standard accuracy (SA) with full width (a,b)and customizable widths (c,d).
Figure 8: Illustration of training weight matrices on a ×1-net-capable or ×0.5-net-capable client. (1)Download the global weight matrix Wl of layer l or a selected subset Wkl. (2) Train weights on abatch data (x, y). (3) Upload trained weight matrix Wl.
Figure 9: Validation accuracy of the budget-compatibly-widest nets by wall-clock time. All algorithmsare run with the same number of iterations (200).
Figure 10: Sample images from multiple domain datasets.
Figure 11: Vary the budget distribution. The training budgets, i.e., width constraints, are depictedin the upper figures by group. The budget distribution name, for example, 8-4-2-1, means ×1∕8,×1∕4, × 1/2 and X1 width constraints for each group, respectively. The lower figures compare theperformance of trained models with customized withs.
Figure 12: Vary the number of clientsuploading models per round.
Figure 13: Convergence of different-width models.
