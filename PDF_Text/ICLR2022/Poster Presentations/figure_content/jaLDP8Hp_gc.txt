Figure 1: Visual correspondence hallucination.OUr network, called NeurHal, takes as input apair of partially overlapping source and target images and a set of keypoints detected in the sourceimage, and outputs for each keypoint a probability distribution over its correspondent’s location inthe target image. When the correspondent is actually visible, its location can be identified; whenit is not, its location must be hallucinated. Two types of hallucination tasks can be distinguished:1) if the correspondent is occluded, its location has to be inpainted; 2) if it is outside the field ofview of the target image, its location needs to be outpainted. NeurHal generalizes to scenes not seenduring training: For each of these three pairs of source/target images coming from the test scenes ofScanNet (Dai et al., 2017) and MegaDepth (Li & Snavely, 2018), we show (top row) the source imagewith a small subset of keypoints, and (bottom row) the target image with the probability distributionspredicted by our network and the ground truth correspondents: ◦ for the identified correspondents, +for the inpainted ones, and × for the outpainted correspondents.
Figure 2: Overview of NeurHal: See text for details.
Figure 3: Evaluation of the ability of NeurHal to hallucinate correspondences on the test scenesof ScanNet and MegaDepth. (left) Histograms of the NRE (see Eq. 1) for each task (identifying,outpainting, inpainting), computed on correspondence maps produced by NeurHaL The value ln ∣Ωcr |is the NRE of a uniform correspondence map. (right) Histograms of the errors between the argmax(mode) of a correspondence map and the ground truth correspondent’s location, for each task. Thevalue EU is the average error of a random prediction.
Figure 4: Ability to hallucinate - comparison against state-of-the-art local feature matchingmethods on ScanNet (S) and Megadepth (M). For each method, we report the percentage ofkeypoint’s correspondents whose distance w.r.t. the ground truth location is lower than x pixels, as afunction of x, for (a-c) the inpainting task and (b-d) the outpainting task.
Figure 5: Ability to hallucinate - Qualitative inpainting/outpainting results. To illustrate theability of NeurHal to hallucinate corresPondents, we disPlay corresPondence maPs Predicted byNeurHal on image Pairs (caPtured in scenes that were not seen at training-time): (toP row) outPaintingexamPles, (bottom row) inPainting examPles. In the source image, the red dot is a keyPoint. In thetarget image and in the (negative-log) corresPondence maP, the red dot rePresents the ground truthkeyPoint’s corresPondent. The dashed rectangles rePresent the borders of the target images. Moreresults on the NYU and ETH-3D datasets can be found in the aPPendix D.1.
Figure 6: Ablation study - Impact of learning to hallucinate for absolute camera pose estima-tion. We compare the influence of adding inpainting and outpainting (γ = 50%) tasks when trainingNeurHal. We report the percentage of camera poses being correctly estimated for image pairs havingan overlap between 2% and x%, as a function of x, on ScanNet (Dai et al., 2017), with thresholds fortranslation and rotation errors of Tt = 1.5m and Tr = 20.0°. Learning to hallucinate correspondences(especially outpainting) significantly improves the amount of correctly estimated poses.
Figure 7: Absolute camera pose experiment. We compare the performance of NeurHal againststate-of-the-art local feature matching methods on ScanNet (Dai et al., 2017). The "identity" methodconsists in systematically predicting the identity pose. We report the percentage of camera posesbeing correctly estimated for pairs of images that have an overlap between 2% and x%, as a functionof x, for two rotation and translation error thresholds. See discussion in Sec. 4.2.
Figure 8: Ability to hallucinate - Ablation study on ScanNet. We compare the influence of addinginpainting and outpainting when training NeurHal. (left column) We report the percentage ofkeypoint’s correspondents whose NRE cost is lower than x, as a function of x, for (a) identified(b) inpainted and (c) outpainted keypoints. (right column) We report the percentage of keypoint’scorrespondents whose distance w.r.t. the ground truth is lower than x pixels, as a function of x, forthe same categories.
Figure 9: Ability to hallucinate - Homography-based warping: We compare the performanceof 1) NeurHal trained to both identify and hallucinate correspondences against 2) a version ofNeurHal trained without correspondence hallucination followed by a homography-based warpingstage to hallucinate correspondences and 3) a homography estimated from the ground truth (GT)identified correspondences. We report the performance to predict (a) inpainted and (b) outpaintedcorrespondents locations. For each method we report the percentage of keypoint’s correspondentswhose distance w.r.t. the ground truth location is lower than x pixels, as a function of x. We show in(c) and (d) the performance of RANSAC-based homography estimation for various inlier thresholds.
Figure 10: Influence of the pose estimator: (Germain et al., 2021) vs. (Chum et al., 2003): Tostudy the influence of using the pose estimator proposed in (Germain et al., 2021) compared tousing the pose estimator from (Chum et al., 2003), we report the performance of NeurHal with bothestimators. We also include, for both estimators, the performance of NeurHal trained with identifiedcorrespondences only (i.e. without hallucination). We report the percentage of camera poses beingcorrectly estimated for pairs of ScanNet (Dai et al., 2017) images that have an overlap between 2%and x% (as a function of x).
Figure 11: Impact of the value of γ : For increasing values of γ, we report the percentage of cameraposes being correctly estimated for pairs of ScanNet images that have an overlap between 2% and x%(as a function of x), for Tt = 1.5m and Tr = 20.0。. We find that a small value of Y = 10% yieldsno benefit and even damages performance, while values of γ = 25% and γ = 50% bring significantimprovements, especially at small visual overlaps.
Figure 12: Field-of-view as a function of γ and relative viewpoint statistics: We report in (a) theaverage camera field-of-view as a function of γ on ScanNet (Dai et al., 2017) and Megadepth (Li& Snavely, 2018) images. We find that γ = 50% enables a significant amount of additional visualcontent to reproject within the image boundaries. We report in (b) the median absolute difference inrotation along the x, y and z axis, norm of the relative rotation, along with the difference in focallength on low-overlap image pairs for ScanNet (Dai et al., 2017) and Megadepth (Li & Snavely,2018). We report in (c) the histogram of absolute relative angle norm on both datasets. We findScanNet image pairs exhibit strong relative angular motion while Megadepth image pairs displaypredominantly zoom-ins and zoom-outs.
Figure 13: Camera pose estimation experiment - Worst cases: We report the performance ofNeurHal and state-of-the-art feature matching methods on ScanNet (Dai et al., 2017) image pairs withVisual oVerlaps between 2% and 5%. For eVery column, we subselect the 25% of images pairs withthe worst predictions for a giVen method. We find that in all cases, NeurHal strongly outperformsits competitors. On the contrary, on the worst NeurHal predictions state-of-the-art methods achieVea much lower performance, which is either on par or lower than the predictions obtained using theIdentity.
Figure 14: Camera pose estimation experiment - varying the threshold values: We report theperformance of NeurHal and state-of-the-art feature matching methods on ScanNet (Dai et al., 2017)image pairs with visual overlaps between 2% and 5%. For various angular and translation thresholdswe report the percentage of correctly localized images. We find that in all cases, NeurHal stronglyoutperforms its competitors.
Figure 15: Qualitative results on the ETH3D dataset: We evaluate NeurHal on outdoor imagepairs from the ETH-3D (Schops et al., 2017) dataset and find it is able to outpaint correspondencesdespite low visual overlaps. We report pairs of source and target images and overlay the upsampledcoarse loss map corresponding to the source detection (in red) on the target image.
Figure 16:	Qualitative results on the NYU dataset: We evaluate NeurHal on indoor images fromthe NYU (Nathan Silberman & Fergus, 2012) dataset and find it is able to outpaint correspondencesdespite low visual overlaps. We report pairs of source and target images and overlay the upsampledcoarse loss map corresponding to the source detection (in red) on the target image.
Figure 17:	Additional qualitative ScanNet (Dai et al., 2017) examples. See text for details.
Figure 18: Additional qualitative Megadepth (Li & Snavely, 2018) examples. See text for details.
Figure 20: NeurHal failure cases on low-overlap images from ScanNet (Dai et al., 2017): Wereport cases where NeurHal fails to estimate a camera pose with an error less than τt = 0.5m andTr = 10.0°. We find these cases often correlate with extremely low covisibility coupled with strongcamera rotations.
