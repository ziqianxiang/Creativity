Figure 1: Illustration of F reeT ickets with EDST Ensemble as an example. EDST Ensemble, consisting ofone exploration phase and M sequential refinement phases, produces M diverse subnetworks with very low cost(hence called “free tickets”). By combining all these free tickets, EDST Ensemble matches the performance ofthe dense ensemble with only half of FLOPs required to train a single dense model.
Figure 2: Left: KL divergence across intermediate feature maps of various sparse ensemble methods. Eachline is the averaged KL divergence among M = 3 subnetworks. Right: Diversity comparison between sparseensemble methods and non-sparse ensemble methods with Wide ResNet28-10 on CIFAR-10 (M = 3, S = 0.8).
Figure 3:	t-SNE projection of training trajectories of ensemble learners discovered by DST Ensemble and EDSTEnsemble with Wide ResNet28-10 on CIFAR-10/100. The sparsity level is S = 0.8.
Figure 4:	Performance and KL divergence of the subnetworks and the ensemble of subnetworks as sparsityvaries. The KL divergence is scaled to the test accuracy for better visualization.
Figure 5: Performance of the subnetworks and the en-semble of subnetworks as the ensemble size M varies(Wide ResNet28-10 on CIFAR-10).
Figure 6:	Prediction disagreement between ensemble learners with Wide ResNet28-10 on CIFAR-10. Eachsubfigure shows the fraction of labels on which the predictions from different ensemble learners disagree.
Figure 7:	Prediction disagreement between ensemble learners with Wide ResNet28-10 on CIFAR-100. Eachsubfigure shows the fraction of labels on which different ensemble learners disagree.
