Table 1: Comparison of Charformer against other subword and character-level models withdifferent parameter sizes on diverse standard English datasets.
Table 2: Results on comment classification on Civil Com-ments and Wiki Comments. Metrics are accuracy andAUC-PR.T5 baseline results are from (Tay et al., 2021).
Table 3: Results on text classification onlong documents.		Model	IMDb	NewsT5Base,Subword	94.2	93.5Byte-level T5Base	91.5	93.6Byte-level T5+LASCBase	91.1	93.5CharformerBase	91.5	94.0CHARFORMERSBase	94.4	94.1Results For all result tables, we divide the table into three sections: subword baseline(s), un-scaledbyte-level baselines, and scaled Charformer results. If a section and task combination has morethan one model result, we underline the best result. We show result for GLUE in Table 1. Char-former outperforms other character-level baselines trained under the same conditions with the samenumber of parameters across all tasks, while being considerably faster and requiring less computethan T5-style models that are directly applied to bytes or characters (see §4). CHARFORMERSBaseperforms even better despite having a smaller number of parameters compared to the Base configu-ration, demonstrating the usefulness of rescaling the transformer stack for character-level models.
Table 4: Multilingual comparison of Charformer against subword and byte-level models onin-language multi-task, translate-train multi-task, and cross-lingual zero-shot (training on English)settings. Model sizes are the same as those in Table 1. mBERT and mT5 baseline results are from(XUe et al., 2020)._______________________________________________________________________________________In-Language			Translate-Train-All				Zero-Shot	Model	∣θ∣	TyDiQA-GoldP	XQuAD	MLQA	XNLI	PAWS-X	XNLI	PAWS-XmBERTBase (Subword)	179M	77.6/68.0	-/-	-/-	-	-	65.4	81.9mT5Base (Subword)	582M	80.8/70.0	75.3/59.7	67.6/48.5	75.9	89.3	75.4	86.4Byte-level T5Base	200M	75.6/65.4	~68.6/54.3	61.8/44.4	69.4	87.1	57.4	80.9Byte-level T5+LASCBase	205M	70.6/59.7	66.8/52.1	58.8/41.1	67.9	84.8	55.2	79.0CharformerBase	203M	75.9/65.6	70.2/55.9	62.6/44.9	71.1	87.2	57.6	81.6CharformerSBase	134M	79.1/68.8	73.6/59.0	66.3/48.5	72.2	88.2	66.6	85.2CharformerSBase,LongPT	134M	81.2/71.3	74.2/59.8	67.2/49.4	72.8	88.6	67.8	83.7Table 5: Comparison of pre-training compute metrics for mT5 (Subword) versus comparable qual-ity Charformer models on the mC4 dataset. 64 TPUv3 chips were Used for this experiment.
Table 5: Comparison of pre-training compute metrics for mT5 (Subword) versus comparable qual-ity Charformer models on the mC4 dataset. 64 TPUv3 chips were Used for this experiment.
Table 6: Pre-training compute metrics of models at different input lengths, downsampling rates,and model sizes on the English C4 dataset. 16 TPUv3 chips were used for this experiment. Thesenumbers reflect a batch size of 64. Memory refers to per-device peak memory usage on TPUv3 chips.
Table 7: Ablation studies with CHARFORMERSmall on English tasks.
Table 8: Effect of freezing the GBST layer for XNLI and PAWS-X.
Table 9: Effect of ds on TyDiQA-GoldP (in-language multi-task).
Table 10: Comparison on TyDiQA at 1.23B parameters. *Due to resource constraints, the Charformerresult below uses 〜100K less pretraining steps than ByT5 and mT5.
Table 11: Compute metrics of base models at longer (2K) input length on the mC4 pre-trainingcorpus, using a batch size of 64 on 16 TPU-v3 chips.
Table 12: Per-language breakdown of in-language multi-task TyDiQA-GoldP results.
Table 13: Per-language breakdown of translate-train-all XQUAD results.
Table 14: Per-Ianguage breakdown of translate-train-all MLQA results.
Table 15: Per-IangUage breakdown of translate-train-all and cross-lingual zero-shot XNLI results.
Table 16: Per-language breakdown of translate-train-all and cross-lingual zero-shot PAWS-X results.									Model	∣θ∣	de	en	es	fr	ja	ko	zh	Avg.
