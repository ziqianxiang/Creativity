Table 1: Average episode rewards ± standard deviation of vanilla DQN and A2C agents under different evasionattack methods in Atari environments. Results are averaged over 1000 episodes. Note that RS works forcontinuous action spaces, thus is not included. MinQ is not applicable to A2C which does not have a Qnetwork. In each row, we bold the strongest (best) attack performance over all attacking methods.
Table 2: Average episode rewards ± standard deviation of vanilla PPO agent under different evasion attackmethods in MuJoCo environments. Results are averaged over 50 episodes. Note that MinBest and MinQ donot fit this setting, since MinBest works for discrete action spaces, and MinQ requires the agent’s Q network.
Table 3: Average episode rewards ± standard deviation of robustly trained PPO agents under different attackmethods. Results are averaged over 50 episodes. In each row corresponding to a robust agent, We bold thestrongest attack. The [gray cells are the most robust agents with the highest average rewards across attacks.
Table 4: Performance of PA-AD across difference choices of the relaxation hyperparameter λ		Pong	BoxingNature Reward		21 ± 0	96 ± 4λ =	0.2	-19 ± 2	16 ± 12λ =	0.4	-18 ± 2	17 ± 12λ =	0.6	-20 ± 2	19 ± 15λ =	0.8	-19 ± 2	14 ± 12λ =	1.0	-19 ± 2	15 ± 12λ =	2.0	-20 ± 1	21 ± 15λ =	5.0	-20 ± 1	19 ± 14(a) Atari		Ant	WalkerNature Reward		5687 ± 758	4472 ± 635λ =	0.2	-2274 ± 632	897± 157λ =	0.4	-2239 ± 716	923 ± 132λ =	0.6	-2456 ± 853	954 ± 105λ =	0.8	-2597±662	872 ± 162λ =	1.0	-2580 ± 872	804 ± 130λ =	2.0	-2378 ± 794	795 ± 124λ =	5.0	-2425 ± 765-	814 ± 140 一
Table 5: Average training time (in hours) of SA-RL and PA-AD in MuJoCo environments, using GeForceRTX 2080 Ti GPUs. For Hopper, Walker2d and HalfCheetah, SA-RL and PA-AD are both trained for 2 millionsteps; for Ant, SA-RL and PA-AD are both trained for 5 million stepsPA-AD is less sensitive to hyperparameters settings than SA-RL. In addition to better finalattacking results and convergence property, we also observe that PA-AD is much less sensitive tohyerparameter settings compared to SA-RL. On the Walker environment, we run a grid search over216 different configurations of hyperparameters, including actor learning rate, critic learning rate,entropy regularization coefficient, and clipping threshold in PPO. Here for comparison we plot twohistograms of the agent’s final attacked results across different hyperparameter configurations.
Table 6: Average episode rewards ± standard deviation of robust models with fewer training steps underdifferent evasion attack methods. Results are averaged over 50 episodes. We bold the strongest attack in eachrow. The gray cells are the most robust agents with the highest average rewards across all attacks.
Table 7: Average episode rewards ± standard deviation of SA-DQN, RADIAL-DQN, RADIAL-A3C robustagents under different evasion attack methods in Atari environments RoadRunner and BankHeist. All attackmethods use 30-step PGD to compute adversarial state perturbations. Results are averaged over 50 episodes.
Table 8: Average episode rewards ± standard deviation over 50 episodes of A2C, A2C with adv. training,SA-A2C and our PA-ATLA-A2C robust models under different evasion attack methods in Atari environmentBankHeist. In each row, We bold the strongest attack. The [gray cells are the most robust agents with thehighest average rewards across all attacks.
