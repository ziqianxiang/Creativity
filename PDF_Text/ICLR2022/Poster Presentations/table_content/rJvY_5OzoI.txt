Table 1: Learning different play-styles on Pong with rotatable player paddles	Agent	M	Single-Style	11 MT w/ one-hot			MN-MUltiCritiCAL	MH-MUltiCritiCALS	AggreSSive-Setting → Style = (1, 0)					Reward ↑ Win-rate ↑ Play-time，	0.89 ± 0.05 94 ± 2% 280 ± 102	-0.20 ± 0.35 40 ± 13% 170 ± 51	0.85 ± 0.01 86 ± 3% 255 ± 35	0.80 ± 0.03 84 ± 8% 291 ± 55	Defensive-Setting → Style = (0, 1)					Reward ↑ Win-rate Play-time ↑	162.75 ± 12.15 51 ± 3% 1536± 113	166.05 ± 13.04 50 ± 5% 1571± 122	176.60 ± 8.50 30 ± 5% 1661 ± 53	165.59 ± 13.01 41 ± 12% 1560 ± 154O d d	Aggressive-Setting → Style = (1, 0)					Reward ↑ Win-rate ↑ Play-time，	0.70 ± 0.12 81 ± 10% 586 ± 98	-0.35 ± 0.36 32 ± 23% 241± 111	0.83 ± 0.08 79 ± 2% 485 ± 44	0.70 ± 0.19 80 ± 15% 330 ± 98	Defensive-Setting → Sty			e=(0,1)		Reward ↑ Win-rate Play-time ↑	101.35 ± 17.90 54 ± 7% 993 ± 120	35.85 ± 16.08 41 ± 3% 347 ± 132	92.75 ± 12.39 40 ± 3% 841± 226	72.60 ± 14.06 61 ± 8% 726 ± 121Table 2: Interpolating between aggressive and defensive Pong play-styles → Style = (0.5, 0.5)Agent		sac				PPO			MN-MUltiCritiCAL	MH-MUltiCritiCAL	MN-MUltiCritiCAL	MH-MUltiCritiCALTraining with Binary Style Selection				Win-rate	71 ± 3%	64 ± 13%	64 ± 4%	78 ± 5%Play-time	770 ± 148	765 ± 113	615 ± 164	541 ± 67Training with Explicit Style Interpolation				Win-rate	73 ± 6%	72 ± 8%	71 ± 3%	70 ± 2%Play-time	834 ± 108	793 ± 120	621 ± 158	602 ± 106Multi-style performance is tested by introducing a one-hot style encoder and randomly uniformly
Table 2: Interpolating between aggressive and defensive Pong play-styles → Style = (0.5, 0.5)Agent		sac				PPO			MN-MUltiCritiCAL	MH-MUltiCritiCAL	MN-MUltiCritiCAL	MH-MUltiCritiCALTraining with Binary Style Selection				Win-rate	71 ± 3%	64 ± 13%	64 ± 4%	78 ± 5%Play-time	770 ± 148	765 ± 113	615 ± 164	541 ± 67Training with Explicit Style Interpolation				Win-rate	73 ± 6%	72 ± 8%	71 ± 3%	70 ± 2%Play-time	834 ± 108	793 ± 120	621 ± 158	602 ± 106Multi-style performance is tested by introducing a one-hot style encoder and randomly uniformlysampling either defensive or aggressive play for each new training episode. We observed that bothMTSAC and MTPPO failed to learn to properly distinguish between play-styles. MTSAC agentsoften defaulted to highly defensive play (likely due to the over-representation of the dense defensiverewards in the replay buffers) while MTPPO agents tended to learn intermediate behavior that wasneither aggressive nor defensive. MultiCriticAL agents were however able to successfully learn todistinguish between the styles with both base algorithms, with performance comparable to single-style agents. Table 1 and Figure A.2.1, show performance statistics averaged over 20 test episodesper seed for each algorithm.
Table 3: MTPPO vs. Multi-headed MultiCriticAL-PPO on UFCDominant Style	Style Setting	Normalized Per-Action Style Rewards × 103										 MTPPO							 MH-MuItiCritiCAL PPO							Aggression	Defensiveness	Blocking	Moving	Aggression	Defensiveness	Blocking	MovingAggressive Defensive Neutral Blocking Moving	(0.0,0.5) (1.0,0.5) (0.5,0.5) (0.5,0.0) (0.5,1.0)	7.08 ± 1.04 0.00 2.84 ± 0.90 2.25 ± 1.25 3.07 ± 1.01	0.00 20.08 ± 0.30 14.15 ± 0.74 18.86 ± 1.07 14.78 ± 1.13	0.10 ± 0.05 4.31 ± 0.41 1.32 ± 0.50 3.10 ± 3.78. 0.00	1.77 ± 0.15 1.19 ± 0.12 1.93 ± 0.07 0.00 4.19 ± 0.43	7.15 ± 1.51 0.00 2.16 ± 0.15 3.00 ± 1.14 1.75 ± 1.25	0.00 20.80 ± 0.07 10.80 ± 0.12 11.56 ± 1.32 10.13 ± 0.88	0.05 ± 0.02 4.27 ± 0.37 1.68 ± 1.68 1.97 ± 2.32 0.00	2.19 ± 0.21 2.33 ± 0.17 2.24 ± 0.15 0.00 4.50 ± 0.20Results MultiCriticAL allows PPO agents to successfully learn and smoothly transition betweenthe different fighting styles. The behavior styles are distinguished visually and Figure 5 attemptsto capture this through frames from captured gameplay with videos included in the SupplementaryMaterial. Reward statistics for each style are presented in Table 3 and Figure A.2.3. Aggressive anddefensive plays are the most distinct but there are also observable differences between control thatprioritizes blocking and moving, with the blocking-style conditioning agents to hold their groundwhile moving prompts agents to move around the ring more. When comparing MultiCriticAL toMTPPO, we note that, while rewards at the extremes of style distinction are similar, MultiCriticALappears to be better at more consistently maintaining intermediate styles (when a setting is at 0.5).
Table 4: Rewards achieved on Sonic the Hedgehog (1)Abbreviation	Level		Single-level Policy	MTPPO	MN-MultiCriticAL	MH-MultiCriticALGHZ1	GreenHiUZone.Act1		37.41 ± 15.84	38.70 ± 23.41	42.30 ± 5.32	40.72 ± 6.04GHZ2	GreenHiuZone.Act2		17.60 ± 9.66	26.40 ± 10.62	42.26 ± 2.07	32.63 ± 6.01GHZ3	GreenHiUZone.Act3		7.22 ± 4.07	9.37 ± 6.58	38.85 ± 25.18	12.02 ± 5.54MZ1	MarbleZone.Act1		28.03 ± 5.45	26.20 ± 11.30	39.93 ± 13.30	40.40 ± 8.45MZ2	MarbleZone.Act2		18.86 ± 5.23	16.79 ± 6.60	22.84 ± 1.70	22.92 ± 0.11MZ3	MarbleZone.Act3		27.31 ± 2.36	22.64 ± 7.04	29.82 ± 0.68	28.50 ± 0.17SYZ1	SpringYardZone.Act1		6.29 ± 2.73	7.82 ± 6.50	10.02 ± 5.34	9.16 ± 5.14SYZ2	SpringYardZone.Act2		5.55 ± 3.71	11.59 ± 2.85	18.65 ± 3.41	13.47 ± 6.06SYZ3	SpringYardZone.Act3		19.67 ± 2.19	17.17 ± 1.58	22.82 ± 4.97	26.14 ± 2.37LZ1	LabyrinthZone.Act1		22.50 ± 3.18	22.04 ± 5.00	22.00 ± 6.40	25.16 ± 12.77LZ2	LabyrinthZone.Act2		29.52 ± 0.40	29.65 ± 0.19	29.59 ± 0.31	29.46 ± 0.06LZ3	LabyrinthZone.Act3		25.70 ± 4.02	26.04 ± 0.09	26.33 ± 0.13	25.82 ± 0.19SLZ1	StarLightZone.Act1		35.03 ± 0.51	35.15 ± 1.06	47.80 ± 12.38	39.79 ± 4.38SLZ2	StarLightZone.Act2		6.76 ± 0.14	13.05 ± 4.62	18.25 ± 0.30	17.04 ± 3.21SLZ3	StarLightZone.Act3		14.47 ± 6.95	26.37 ± 5.01	30.40 ± 6.39	30.20 ± 1.00SBZ1	SCrapBrainZone.Act1		8.03 ± 1.65	10.73 ± 4.52	9.92 ± 4.13	11.68 ± 6.11SBZ2	SCrapBrainZone.Act2		12.52 ± 0.00	8.54 ± 4.27	11.61 ± 1.99	11.25 ± 1.51Average			19.42 ± 2.03	20.48 ± 2.k	27.39 ± 2.65	24.73 ± 1.31
Table 5: Rewards achieved on Sonic the Hedgehog 2Abbreviation	Level		Single-level Policy	MTPPO	MN-MUltiCritiCAL	MH-MultiCriticALEHZ1	EmeraldHillZone.Act1		48.98 ± 10.01	43.72 ± 17.08	65.82 ± 15.48	44.03 ± 6.07-EHZ2	EmeraldHillZone.Act2		21.82 ± 2.37	18.43 ± 10.09	28.31 ± 6.27	23.65 ± 13.28CPZ1	ChemicalPlantZone.Act1		26.58 ± 3.29	28.21 ± 2.01	37.48 ± 10.18	33.65 ± 11.64CPZ2	ChemicalPlantZone.Act2		28.82 ± 4.32	18.84 ± 8.90	24.60 ± 9.85	27.01 ± 5.00ARZ1	AquaticRuinZone.Act1		15.30 ± 7.31	5.85 ± 0.00	24.28 ± 5.56	19.18 ± 9.24ARZ2	AquaticRuinZone.Act2		37.40 ± 6.89	24.12 ± 2.85	39.41 ± 7.30	33.15 ± 10.23CNZ	CasinoNightZone.Act1		17.12 ± 0.97	12.02 ± 3.51	20.34 ± 5.01	15.65 ± 1.99HTZ1	HillTopZone.Act1		7.82 ± 0.20	6.36 ± 2.72	7.76 ± 0.85	5.65 ± 0.57HTZ2	HillTopZone.Act2		6.44 ± 4.23	4.22 ± 2.81	19.89 ± 2.96	21.30 ± 3.08MCZ1	MysticCaveZone.Act1		10.25 ± 0.34	7.42 ± 1.14	9.74 ± 1.44	10.23 ± 0.17MCZ2	MysticCaveZone.Act2		8.57 ± 2.31	5.47 ± 0.00	8.75 ± 3.32	11.96 ± 1.59OOZ1	OilOceanZone.Act1		14.95 ± 3.87	8.65 ± 4.18	19.14 ± 2.58	13.04 ± 4.28OOZ2	OilOceanZone.Act2		10.20 ± 0.90	9.92 ± 2.11	13.72 ± 4.16	14.06 ± 3.22MZ1	MetropolisZone.Act1		14.64 ± 3.25	6.07 ± 1.94	14.39 ± 2.58	14.03 ± 3.52MZ2	MetropolisZone.Act2		12.79 ± 2.68	11.50 ± 2.53	13.12 ± 0.37	16.90 ± 7.06MZ3	MetropolisZone.Act3		6.49 ± 2.27	10.31 ± 1.59	11.75 ± 2.29	17.34 ± 5.16WFZ	WingFortressZone		27.43 ± 0.88	8.74 ± 0.41	24.81 ± 3.98	21.29 ± 5.53Average			18.90 ± 1.71	13.73 ± 1.^06-	21.11 ± 2.44	19.22 ± 1.95
Table 6: Action probabilities for MTPPO and MH-MultiCriticAL-PPO on UFCDominant Style 11 No-action		Strike	Block	Move U Win Rate (%)	MTPPO					Aggressive	3.80	32.99	3.60	59.61	96Defensive	7.48	5.42	73.86	13.22	4(48% ties)Neutral	7.48	20.97	24.12	47.42	100Blocking	8.31	20.89	27.21	43.59	96Moving	6.46	26.75	15.91	50.89	88MH-MUltiCritiCAL-PPO					Aggressive	0.27	15.04	15.78	68.91	96Defensive	0.52	4.94	30.20	64.34	12 (56% ties)Neutral	0.57	11.30	22.88	65.25	88 (4% ties)Blocking	0.49	14.88	23.92	60.71	88Moving	0.59	8.70	20.88	69.82	64(8% ties)We observe from Figure A.2.3 and Table 6 that MultiCriticAL more consistently learns delineatedstyles that are consistent with the behaviors that the rewards try to encourage. Note that, with ourmethod, it is, in fact, the aggressive style that wins most fights consistently and the defensive stylelearns to block and move to not lose as quickly, whereas MTPPO tends seems to equate defensiveplay with just blocking, and this also contributes to a higher rate of losing. We see as well that theintermediate settings correspond to a more balanced blend of action probabilities between extremes
Table 7: Trainable Critic parameters per taskTaSk	U SingIe-StyIe		MT w/ one-hot	MN-MUItiCritiCAL	MH-MUItiCritiCAL ∣∣ MH-MUItiCritiCALVS.MT(%∏	SAC	一					Path Following	137	161	483	163	↑ 1.24%Pong	5061	7109	14218		7164			↑ 0.77%	PPO	一					Path Following	4545	4737	14211	4867	↑ 2.74%Pong	68353	76545	153090	76802	↑ 0.33%Sonic	197377	201729	3429393	205841		↑ 2.04%	Misc. details Additionally, a frame wrapper is used with Pong and the Sonic games to augmentstate information with information from the last 4 observed states. This wrapper is based on theFrameStack wrapper provided in OpenAI Baselines (Dhariwal et al., 2017).
