Table 1: A comparison of latent domain learning versus unsupervised, latent source (Mancini et al.,2018) and semi-supervised DA, domain generalization, and multi-domain learning. Sd denotes alabeled dataset from the d'th domain, Pd are partially labeled, Ud, Ud unlabeled.
Table 2: Per-domain performance on Office-Home. Multi-domain (MD) baselines use domain an-notations, and latent domain (LD) models do not. Best overall performance underlined; best latentdomain performance in bold.
Table 3: Results on PACS. Best performance underlined, best latent domain performance bold.
Table 4: Results on DomainNet, best performances in bold.							clipart	infograph painting quickdraw			real	sketch	OAcc	UAccπd	0.082	0.088	0.123	0.294	0.295	0.118	—	—ResNet26	66.46	27.99	51.70	67.46	66.58	56.95	60.47	56.19ResNet56	69.08	29.55	53.85	68.61	68.60	58.42	62.19	58.01RA (Rebuffi et al., 2018)	68.23	29.34	56.29	66.51	71.81	57.75	62.65	58.32MLFN (Chang et al., 2018)	68.20	24.64	50.39	69.85	65.78	56.27	60.54	55.85SLA	69.46	30.14	57.36	67.97	72.89	58.83	63.83	59.44modality in data. Crucially where our method is fine-grained and shares convolutions at every layer,MLFN instead enables and disables entire network blocks, allowing us to outperform it.
Table 5: An ablation study for SLA. UAcc is shown (on Office-Home) with activations other thanSparseMax (Martins & Astudillo, 2016) used in this paper. For all variants we fix K= 2.
Table 6: Average precision and bias amplification of SLA on the CelebA fair attribute recognitionbenchmark (Wang et al., 2020).
Table 7: Top-1 validation accuracy on imbalanced CIFAR benchmarks (Buda et al., 2018). SLAconsistently improves performance for standard ERM as well as existing long-tail approaches.
