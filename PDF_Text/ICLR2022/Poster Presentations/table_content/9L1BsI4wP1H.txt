Table S1: The influence of the adversarial attacks used in our experiments on the classifiers’ test accuracy.						Architecture	Training σ	CIFAR10		CIFAR100		ImageNet		Clean Test	Adversarial Test	Clean Test	Adversarial Test	Clean Test Adversarial TestResNet	0	89.94%	26.7%	71.03%	12.19%	75.69%	19.59%	0.0625	87.13%	68.96%	64.58%	41.17%	--	0.125	81.69%	67.84%	58.06%	41.95%	--	0.25	75.19%	65.18%	50.73%	40.82%	68.58%	56.32%	0.375	-	-	44.23%	37.14%	--	0.5	63.84%	57.34%	39.0%	33.68%	61.04%	53.09%	0.75	-	-	31.75%	28.1%	--	1	47.99%	44.41%	25.57%	23.56%	47.73%	42.89%DenseNet	0	95.42%	23.28%	77.07%	4.29%	--	0.25	80.09%	70.89%	52.69%	42.47%	--VGG	0	93.1%	54.96%	72.17%	23.12%	--	0.25	78.88%	69.82%	48.57%	39.54%	--possible labels y ∈ Y . The number of calls to the predictive model for creating a set for a single testimage is ns, and the number of classes L only affects the number of scores that need to be calculatedusing the model predictions. The training involves fitting a classifier, and the calibration requirescomputing the smoothed non-conformity scores for all calibration points, where both steps can beperformed only once.
Table S2: Inference time (seconds) for constructing a prediction set for a single image using differentconformal prediction methods.
