Table 1: Comparison of regularization techniques on Fashion-MNIST (top) and E-MNIST (bottom).
Table 2: Evaluation of the robustness of PN models on CIFAR-10. Each line refers to a differentadversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacksthe projection improves the accuracy by a significant margin.
Table 3: Comparison of regularization techniques on (a) Fashion-MNIST (top) and (b) E-MNIST (bot-tom) along With adversarial training (AT). The base netWork is a PN-10, i.e., 10th degree polynomial.
Table 4: Symbols for various matrix products. The precise definitions of the products are included inAppendix C.2 for completion.
Table 5: Operations and symbols on a matrix A.
Table 6: Core symbols in the proof of Lemma 11.
Table 7: Core symbols for proof of Theorem 1.
Table 8: Core symbols in the proof of Lemma 1.
Table 9: Core symbols for proof of Theorem 6.
Table 10: The accuracy of different PN models on Fashion-MNIST (top) and E-MNIST (bottom)when trained only with SGD (first row) and when trained with projection (last row).
Table 11: Comparison of regularization techniques on K-MNIST (top) and MNIST (bottom). In eachdataset, the base networks are PN-4, i.e., a 4th degree polynomial, on the top four rows, PN-10, i.e., a10th degree polynomial, on the middle four rows and PN-Conv, i.e., a 4th degree polynomial withconvolutions, on the bottom four rows. Our projection method exhibits the best performance in allthree attacks, with the difference on accuracy to stronger attacks being substantial.
Table 12: Comparison of regularization techniques on (a) K-MNIST (top) and (b) MNIST (bottom)along with adversarial training (AT). The base network is a PN-10, i.e., 10th degree polynomial. Ourprojection method exhibits the best performance in all three attacks.
Table 13: Comparison of regularization techniques on E-MNIST-BY. The base network are PN-4,i.e., 4th degree polynomial, on the top four rows, PN-10, i.e., 10th degree polynomial, on the middlefour rows and PN-Conv, i.e., a 4th degree polynomial with convolution, on the bottom four rows. Ourprojection method exhibits the best performance in all three attacks, with the difference on accuracyto stronger attacks being substantial.
Table 14: Evaluation of the robustness of PN models on NSYNTH. Each line refers to a differentadversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacksprojection improves the accuracy by a remarkable margin.
Table 15: Evaluation of the robustness of PN models on four datasets with three new types of attacks.
Table 16: Evaluation of our layer-wise bound versus our single bound. To avoid confusion withprevious results, note that ’single bound’ corresponds to ’Our method’ in the rest of the tables in thiswork. The different λi values are optimized on Fashion-MNIST FGSM-0.01 attack. Then, the sameλi values are used for training the rest of the methods. The proposed layer-wise bound outperformsthe single bound by a large margin, improving even further by baseline regularization schemes.
Table 17: Comparison of the proposed method against adversarial defense methods on featuredenoising (Xie et al., 2019) and guided denoising (Liao et al., 2018). Notice that the single bound (cf.
