Table 1: Results of likelihood values on the synthetic 1D regression experiment. The test samples aredrawn from GPs with various kernel functions; All methods are trained on the samples from the RBFGP function. To consider noisy setting, we artificially generate a periodic noise in the training step.
Table 2: Results of likelihood values on the Sim2Real experiment using predator-prey simulation andimage completion with the CelebA dataset. In the image completion, there are several experimentalcases; The number of context points is in {50, 100, 300, 500}. We report that likelihood of contextpoints is averaged over all experiment cases and elaborate individual likelihood values of target points.
Table 3: Comparison of RMSE scores on u.test inMoveLens-100kRMSE	GLocal-K (Han et al., 2021)	0.890GraphRec + Feat (Rashed et al., 2019)	0.897GraphRec(Rashed et al., 2019)	0.904GC-MC + Feat(Berg et al., 2017)	0.905GC-MC (Berg et al., 2017)	0.910ANP(Contexts:10)(KimetaL,2019)	0.909Ours (contexts: 10)	0.895ing result, 0.895 of the RMSE value. This experiment indicates that the proposed method can reliablyadapt to new tasks even if it provides small histories, and we identify again that our model canproperly work on noisy situations.
Table D.1: Architecture details and hyperparameters for the neural processes. Attention indicatesvariants of attentive neural processes used. Encoder and decoder indicate the MLP network sizesused.
Table D.2: The number of network parameters required for all experimental cases.
Table D.3: Time required for inference in all experiment cases. (1 epoch).
Table E.4: Results of likelihood values on the synthetic 1D regression without noises. Bold entriesindicates the best results.
Table F.5: Predator-prey model results. All models are trained on data with periodic noises.
Table H.6: Prediction performance of ours varying the hyper-parameter K and regularization. Allmodels employ the generative process of stocahstic attentions. The hyphen means a model converges.
Table H.7: Descriptive statistics of attention scores between diagonal and off-diagonal compoenentsin cases of regularization, I(Z, xi|D) and no-regularizationproperly emphasize on the context dataset according to property of context and target dataset. Theregularization cases in Figure H.12 illustrate this fact graphically. All heatmaps of stochastic attentionthat employ the proposed method regularization have very a comparable brightness. We identify thatthe proposed regularization ensures learning stability by ensuring the quality of context embeddingsremains consistent indepedent of the hyper-parameter K . As a consequence, we declare that theproposed method increases modelâ€™s insensitivity to hyper-parameters K by preserving the contextembeddings consistently.
Table I.8: Results of likelihood values on the MovieLens-10k. Bold entries indicates the best results.
