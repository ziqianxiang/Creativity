Table 1: Test accuracy (%) of FKD & baselines in a dataset transfer dis-tillation setting. Error bars indicate 95% confidence for mean of 10 runs.
Table 2: CIFAR-100 and ImageNet-1K accuracies (%) comparing FKD with KD baselines. * denotes resultfrom Tian et al. (2020); FKD uses the same teacher checkpoints provided by the authors,6 with error barsdenoting 95% confidence for the mean over 5 students.
Table 3: Hyperparameter values used in our theoretical analysis, corresponding to the setup of Allen-Zhu &Li (2020). (*) denotes undefined in our presentation, but appearing in Allen-Zhu & Li (2020), because thehyperparameter takes only one value in this work. We note that m is restricted to be polylog(C) in the settingof Theorem 2.
Table 4: Breakdown of predictive disagreements between reference and alternate models over 10000 CIFAR10test points, in terms of which model (if any) was correct. Mean Â± standard deviations over 3 independentinitialisations for top row, and over 3 independent reference models for bottom row. All models achievedbetween 8.0%-8.5% test error.
Table 5:	BLEU of the teacher model of (Tan et al., 2019) (Teacher), self-distillation of (Tan et al., 2019) (SD),SD with KD of (Hinton et al., 2015), SD with FKD, and SD with FKD loss obtained by replacing distillationloss (2) of (Tan et al., 2019) with FKD, in En - De neural machine translation tasks.
Table 6:	Phoneme error rate (PER) of methods in automatic speech recognition tasks.
