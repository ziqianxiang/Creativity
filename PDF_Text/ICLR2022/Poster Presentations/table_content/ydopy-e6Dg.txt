Table 1:	k-NN and linear probing on ImageNet-						Table 2: Fine-tuning on ImageNet-1K.			1K. T denotes using selective kernel. +					denotes pre-		Method	Arch.	Epo.1	Acc.
Table 4: Semi-supervised learning onImageNet-1K. 1% and 10% denotes labelfraction. SD denotes self-distillation.
Table 5: Unsupervised learning on ImageNet-1K. t denotes k-means clustering on frozen fea-tures.
Table 6: Object detection (Det.) & instance segmentation (ISeg.) on COCO and Semanticsegmentation (Seg.) on ADE20K. We report the results of ViT-S/16 (left) and ViT-B/16 (right).
Table 7: Transfer learning by fine-tuning pre-trained models on different datasets. We reportTop-1 accuracy of ViT-S/16 (left) and ViT-B/16 (right).
Table 8: Robustness evaluation of pre-trained models against background change, occlusion,and out-of-distribution examples.											Out-of-Dist.		Clean INMethod	O.F.	M.S.	Background Change				O.BT	Clean âˆ£ Occlusion								M.R.	M.N.	N.F.	O.BB.		IN-9	S.5	NS.5	IN-A	IN-C J	DINO	892	892	804	78.3	52.0	219	184	96.4	64.7	42.0	123	51.7	77.0iBOT	90.9	89.7	81.7	80.3	53.5	22.7	17.4	96.8	65.9	43.4	13.8	48.1	77.94.3.2	Discriminative Parts in Self-Attention MapTo analyze, we visualize the self-attention map with ViT-S/16. We choose [CLS] token as the queryand visualize attention maps from different heads of the last layer with different colors, as shown inFig. 6. Of particular interest, we indicate that iBOT shows a solid ability to separate different objectsor different parts of one object apart. For example, in the leftmost figure, we observe iBOT fairlydistinct the bird from the tree branch. Also, iBOT focuses mainly on the discriminative parts of theobject (e.g., the wheel of the car, the beak of the bird). These properties are crucial for iBOT toexcel at image recognition, especially in complicated scenarios with object occlusion or distractinginstances. While these properties are not unique strengths brought by MIM and we observe similarbehaviors in DINO, we show in Appendix G.2 that iBOT generally gives better visualized results.
Table 9: Effect of design choices of semanticallymeaningful tokenization.
Table 10: k-NN and linear probing accuracy on ImageNet-1K without multi-crop augmen-tation (left) and with multi-crop augmentation (right) multi-crop augmentation. We split thetable into results without or with multi-crop augmentation.
Table 13: Additional object detection, instance segmentation, and semantic segmentation re-sults with small-size models. We pre-train iBOT with ViT-S/16 for 800 epochs.
Table 14: Additional object detection, instance segmentation, and semantic segmentation re-sults with base-size models. We pre-train iBOT with ViT-B/16 for 400 epochs.
Table 15: k-NN and linear probing on ImageNet-1K with different pre-training datasets.
Table 16: Effectiveness of pre-trained features on nearest neighbor retrieval. We report theresults on different downstream tasks whose evaluation is based on nearest neighbor retrieval.
Table 17: Different head sharing strategy.
Table 18: Hard label versus soft la-bel. Cen. denotes centering. * denotessmaller temperature for teacher output.
Table 19: Time and Memory Requirements. We detail the actual training time (T) and GPUmemory (Mem.) of different methods, together with their respective linear probing (Lin.) and fine-tuning (Fin.) accuracy. All methods are trained on two 8-GPU V100 machines with a batch size of1024.
Table 20: Methodology comparison over different approaches to tokenize the patches. Wereport ImageNet-1K k-NN, linear and fine-tuning validation accuracy. Models are pre-trained withViT-S/16 and 300 epochs.
