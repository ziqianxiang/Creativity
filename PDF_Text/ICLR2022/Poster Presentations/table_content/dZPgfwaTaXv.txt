Table 1: Rank correlations between loss function (descending order) and metrics (ascending order)in different tasks, higher is better.
Table 2: Results on CIFAR-10, CIFAR-100, and ImageNet datasets.
Table 3: Results of human Pose estimation task on COCO dataset.
Table 4: Results of maChine reading ComPrehension task onDuReader 2.0 dataset. f: reported by (He et al., 2018).
Table 5: Results on SQuAD1.1 dataset ComPared withBERT(DevIin et al., 2018).
Table 6: Evaluation results of BLEU on Neural Machine Translation task. We report the perfor-mance of our methods on the WMT16 EN-RO dataset. Transformer denotes the auto-regressivemodel. * denotes the performance that we reproduced using the public code.
Table 7: Evaluation results on scene text recognition task comparing with CE and LS-ED. Thereported metrics are accuracy (ACC, higher is better), normalized edit distance (NED, higher isbetter), and total edit distance (TED,lower is better)._________________________________________Test dataset	↑ACC (%)			↑NED			(TED			CE	LS-ED	ReLoss	CE	LS-ED	ReLoss	CE	LS-ED	ReLossIIIT-5K (Mishra et al., 2012)	87.500	87.933	87.700	0.961	0.963	0.961	722	645	667SVT (Wang et al., 2011)	87.172	86.708	87.481	0.952	0.954	0.957	180	163	156ICDAR’03 (Lucas et al., 2005)	94.302	94.535	94.579	0.979	0.981	0.982	110	99	98ICDAR’13 (Karatzas et al., 2013)	92.020	92.299	92.709	0.966	0.979	0.981	137	108	101ICDAR’15 (Karatzas et al., 2015)	78.520	78.410	78.355	0.915	0.915	0.916	868	837	845SVTP (Phan et al., 2013)	78.605	79.225	80.310	0.912	0.913	0.915	346	333	316CUTE (Risnumawan et al., 2014)	73.171	74.216	75.958	0.871	0.875	0.884	224	219	195Wins	-1 ^^	1	5-	^^0^^	1	-^6-	0	2	5Average	84.470	84.761	85.299	0.937	0.940	0.943	370	343	340Transferability of learned ReLoss. In all our experiments, we use the same surrogate loss in eachtask. If we learn different surrogate losses on specific datasets, would the performance be better? Tovalidate this, we conduct experiments to train ReLoss independently on each dataset, as shown inTable 9. The ReLoss transferred from ImageNet dataset performs similar to the consistent ReLosslearned on corresponding datasets. It might be because we train the ReLoss using predicted andrandomly generated data, and it is sufficient to cover different distributions of datasets on image
Table 8: Results of different optimization methods on image classification task.
Table 9: Comparison of transferredReLoSS and ConSiStent ReLoss.____________Dataset	ACC (%)		transferred	consistentCIFAR-10	94.57 ± 0.08	94.61 ± 0.12CIFAR-100	74.15 ± 0.14	74.12 ± 0.095.4 Complexity AnalysisTable 10: Evaluation results w/ or w/o regular losses.
Table 10: Evaluation results w/ or w/o regular losses.
Table 11: Compare with alternate training on image classification.
Table 12: Results of ReLoss with or without gradient penalty.
Table 13: Results of ReLoss on CIFAR datasets in multiple independent runs.
Table 14: Compare with rank-based classification loss.
Table 15: Comparisons of GPU memory and training cost.
