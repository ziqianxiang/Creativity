Table 1: Order Agnostic model performance (in bpc) on the text8 dataset. The OA-Transformer learns arbitrary orders by permuting inputs and outputs as described in XLNet. A Transformer learning only a single order achieves 1.35 bpc.			Table 2: Order Agnostic modelling perfor- mance (in bpd) on the CIFAR-10 dataset. The upscaling model generates groups of four most significant categories, equivalent to 2 bits at a time.		Model	Steps	NLL	Model	Steps	NLLOA-Transformer D3PM-uniform	250 1000	1.64 1.61 ±0.020	ARDM-OA Parallel ARDM-OA	3072 50	2.69 ± 0.005 2.74D3PM-absorbing D3PM-absorbing OA-ARDM (ours)	1000 256 250	1.45 ±0.020 1.47 1.43 ±0.001	ARDM-Upscale 4 Parallel ARDM-Upscale 4	4 × 3072 4×50	2.64 ± 0.002 2.68D3PM-absorbing Parallelized OA-ARDM (ours)	20 20	1.56 ±0.040 1.51 ±0.007	D3PM Absorbing D3PM Gaussian	1000 1000	4.40 3.44 ± 0.007depth upscaling techniques, in addition to modern advances to fit larger scale data. An alternativeapproach for order agnostic modelling is via causally masked permutation equivariant models suchas Transformers (Yang et al., 2019; Alcorn & Nguyen, 2021), but these have had limited success inlikelihood-based tasks. In (Ghazvininejad et al., 2019) a mask predict method is proposed, althoughit does not contain a likelihood analysis. In other work, mixtures of ARMs over certain orders aretrained by overriding convolutional routines for masking (Jain et al., 2020). In a different context in(Liu et al., 2018) graph edges connected to a node are modelled without order. However, the modelis not entirely order agnostic because it models edges centered around focus nodes.
Table 3: CIFAR-10 lossless compression performance (in bpd).
Table 4: Audio (SC09) depth upscaling test set performance (in bpd). A WaveNet baseline learning only a single order achieves 7.77 bpd.			Table 5: Image (CIFAR-10) depth upscaling performance (in bpd).		Model	Steps	Performance	Model	Steps	PerformanceOA-ARDM	D = 16000	7.93	OA-ARDM	D = 3072	2.69ARDM Upscale 256	~^2 × D ^^	6.36	ARDM Upscale 16	2 × D	2.67ARDM Upscale 16	4×D	6.30	ARDM Upscale 4	4×D	2.64ARDM Upscale 4	8×D	6.29	ARDM Upscale 2	8×D	2.67ARDM Upscale 2	16 × D	6.29			the upscaling with larger values. Due to the constant training complexity of ARDMs, one can easilytrain models that have generative processes of arbitrary length. To test this, we train ARDMs onimage data from CIFAR-10 and audio data from SC09 (Warden, 2018). For the audio data, the totalnumber of categories is 216, which is typically too large in terms of memory to model as a singlesoftmax distribution. For that reason, the single stage OA-ARDM is trained using a discretizedlogistic distribution because it is computationally cheaper for a high number of categories. For thesame reason, the Upscale ARDMs for audio can only be trained using the direct parametrization,whereas for images they are trained with the data parametrization.
Table 6: CIFAR-10 generative modelling.
Table 7: Audio (SC09) depth upscaling test set performance (in bpd) for various computationalbudgets.
