Table 1: Loss functions. Anchor/positive/negative: X : embedding of input example from trainingset X by f; proxy: learnable parameter in Rd ; T : temperature. All loss functions are encompassedby (3) using the appropriate definition of functions τ, σ+, σ-, ρ+, ρ- as given here.
Table 2: Improving the SOTA with our Metrix (Metrix/feature) using Resnet-50 with embedding sized = 512. R@K (%): Recall@K; higher is better. *: reported by authors. Bold black: best baseline(previous SOTA, one per column). Red: Our new SOTA. Gain over SOTA is over best baseline.
Table 3: Comparison of our Metrix/embed with other mixing methods using R-50 with embeddingsize d = 512. R@K (%): Recall@K; higher is better. PA: Proxy Anchor, PS: Proxy Synthesis.
Table 4: Statistics and settings for the four datasets We use in our experiments. 1: average.
Table 5: Improving the SOTA with our Metrix (Metrix/feature) using Resnet-50 with embedding sized = 512. R@K (%): Recall@K; higher is better. *: reported by authors. Bold black: best baseline(previous SOTA, one per column). Red: Our new SOTA. Gain over SOTA is over best baseline.
Table 6: Ablation study of our Metrix using contrastive loss and R-50 with embedding size d = 512on Cars196. R@K (%): Recall@K ; higher is better.
Table 7: Ablation study of our Metrix using contrastive loss and R-50 with embedding size d = 128on CUB200. R@K (%): Recall@K; higher is better.
