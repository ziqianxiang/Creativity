Table 1: Overview of theoretical results in Section 4. “Certification” columns entail our certification theorems.
Table 2:	A concrete example of action predictions, where “1” means action a1 and “2” means action a2 . Whenthere is no poisoning attack, the corresponding time window spans by PARL, TPARL, and DPARL are shownby green ,[blue , and pink respectively. All aggregated policies choose action aι, but have different tolerablepoisoning thresholds as shown in the last column.
Table 3:	Expressions of possible action set A(K) given poisoning threshold K for different aggregationprotocols. Full theorem statements are in Theorems 4, 5 and 7 (in Section 4.4).
Table 4: Benign empirical performance of three aggregation protocols (PARL, TPARL, and DPARL) appliedon subpolicies trained using three offline RL algorithms (DQN, QR-DQN, and C51), with the number of sub-policies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of varying randomnessin the environment.
Table 5: The policy quality measured by empirical cumulative reward of the proposed aggregation proto-cols (PARL, TPARL, and DPARL) compared with the standard training. In our aggregation, we aggregateover u subpolicies with u equal to 30 or 50 . We report results averaged over 20 runs of varying randomnessin the environment, where each run is an episode of length at most 1000 for Atari games and 30 for Highwayenvironment. The offline RL algorithm used is DQN.
Table 6: Average window size (i.e., PtT=1 Wt /T) for the aggregation protocol DPARL (Wmax = 5) ap-plied on subpolicies trained using three offline RL algorithms (DQN, QR-DQN, and C51), with the number ofsubpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over all time steps in 20 runs.
Table 7: Runtime (unit: seconds) of the aggregation protocol DPARL (Wmax = 5) applied on subpoliciestrained using offline RL algorithm DQN, with the number of subpolicies (i.e., partition number) u equal to 30 or50. We compare with the standard testing which tests the runtime of a single trained DQN policy without usingour framework. We report results averaged over 20 runs of varying randomness in the environment, where eachrun is an episode of length at most 1000.
Table 8: Action change ratio (in percentage) of three aggregation protocols (PARL, TPARL, and DPARL)applied on subpolicies trained in Highway environment using three offline RL algorithms (DQN, QR-DQN, andC51), with the number of subpolicies (i.e., #partitions) u equal to 30, 50, or 100. We report results averagedover 20 runs of varying randomness in the environment.
Table 9: Average tolerable poisoning thresholds of three aggregation protocols (PARL, TPARL, andDPARL) applied on subpolicies trained using three offline RL algorithms (DQN, QR-DQN, and C51), withthe number of subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs ofvarying randomness in the environment.
