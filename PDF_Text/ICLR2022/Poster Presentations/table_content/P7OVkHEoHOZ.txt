Table 1: PEARL hypeparameters used for all experiments.
Table 2: Environment DetailsEnvironment	Discount	Horizon	Train Tasks	Test Tasks	Number of Exploration StepsAnt-Goal	-0:99-	200	100	30	400Ant-Vel	0.99	200	150	30	400Cheetah-Highdim	0.99	200	100	30	400Cheetah-Vel	0.99	200	100	30	400Cheetah-Vel-Sparse	0.99	200	100	30	400Four-Corners	0.90	20	4	4	380Sawyer-Push	0.99	150	50	10	450Sawyer-Reach	0.99	150	50	10	450Visual-Reacher	0.99	100	50	10	200Ant-Vel: We use the Ant environment from OpenAI gym. Tasks correspond to goal velocitiessampled uniformly in [0, 3]. The reward is the negative absolute value of the difference between theagent’s velocity and the goal velocity. We take this task from (Finn et al., 2017).
Table 3: Unnormalized trajectory returns (HIPI) and post-adaptation Q-values (HFR) for trajectories(a), (b), (c), (d) (Figure 11). The task for which the trajectory is most informative is in bold. Giventask ψ and trajectory τ , HIPI will not relabel trajectory τ with task ψ if the return of τ is low underψ , even though τ may be extremely useful for meta-training on ψ . By considering post-adaptationreturns, HFR does not suffer from this issue.
Table 4: Accuracy of classifier on relabeled and non-relabeled trajectoriesEnvironment	Accuracy on Relabeled Trajectories	Accuracy on Non-Relabeled TrajectoriesFour-Corners	89.65%	76.98%Cheetah-Vel-Sparse	74.33%	57.58%trajectories. This indicates that HFR is indeed correctly capturing task information when relabeling -our relabeled trajectories are easier to identify as coming from a specific task than the non-relabeledtrajectories and are thus more useful for adaptation.
