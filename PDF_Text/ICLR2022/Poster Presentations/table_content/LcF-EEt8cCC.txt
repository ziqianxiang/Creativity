Table 1: The experimental results on the inter-twining moon dataset. The quality of the sampled datafor different methods are measured in terms of the precision and recall metrics. The errors of thescore functions for different methods are measured using EU(X)[Dp(X, y)] and EU⑸[Dl(X, y)],where U(X) is a uniform distribution over a two-dimensional subspace (i.e., the space of each subplotin Fig. 1), y represents the classes specified in the second row of the table, and DP and DL aredefined in Eq. (7). The configurations and the hyperparameters for different experimental settings ofthis experiment are presented in Appendix A.6.2.
Table 2: The evaluation results on the Cifar-10 and Cifar-100datasets. The P/R/D /C metrics with ‘(CW)’ in the last fourrows represents the average class-wise metrics described inSection 5.2. The arrow symbols ↑ / ] represent that a higher/ lower evaluation result correspond to a better performance.
Table A1: The list of symbols used in this paper.
Table A2: The list of symbols used in this paper.
Table A3: The experimental results on the inter-twining moon dataset, where the classifiers are trainedwith LCE, LDLSM0, and LTotal, respectively. The errors of the score functions for different methodsare measured using EU⑸[Dp(X, y)] and EU⑸[Dl(X, y)], where U(X) is a uniform distributionover a two-dimensional subspace, y represents the classes specified in the second row of the table,and Dp and DL are defined in Eq. (7). The arrow symbol J indicates that lower values correspond tobetter performances.
Table A4: The experimental results of our method using LTotal with different value of λ. The arrowsymbols ↑ / J represent that a higher / lower evaluation result correspond to a better performance.
Table A5: A comparison of the evaluation results of the base method, the scaling method with differentα, ours method, and ours + scaling method with different α on the Cifar-10 dataset. The arrowsymbols ↑ / J represent that a higher / lower evaluation result correspond to a better performance.
