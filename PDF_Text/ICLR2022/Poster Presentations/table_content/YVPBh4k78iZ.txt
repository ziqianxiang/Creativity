Table 1: NLL values on UCI dataset. (m, d) denotes number of data points and features, respec-tively. We take results from Adlam et al. (2020) except our model.
Table 2: Classification accuracy and NLL of SVGP and SVTP for image datasets and their variants.
Table 3: Classification accuracy and NLL of ensemble models for image datasets and their variants.
Table 4: Experimental results of Classification with Gaussian LikelihoodDataset	Accuracy	NNGP	Inverse Gamma	Burr Type XIIMNIST	96.8 ± 0.2	9.29 ± 0.002	4.89 ± 0.001	4.96 ± 0.001+ Shot Noise	94.9 ±0.4	9.32 ± 0.001	4.88 ± 0.001	4.96 ± 0.001+ Impulse Noise	84.4 ± 2.3	9.63 ± 0.000	5.17 ± 0.001	5.25 ± 0.001+ Spatter	95.4 ± 0.4	9.27 ± 0.001	4.44 ± 0.002	4.49 ± 0.007+ Glass Blur	90.5 ±0.7	9.12 ± 0.001	4.20 ± 0.005	4.00 ± 0.017+ Zigzag	84.9 ± 1.5	9.51 ± 0.001	4.62 ± 0.006	4.49 ± 0.021EMNIST	70.4 ± 0.9	9.40 ± 0.001	4.15 ± 0.004	4.45 ± 0.013Fashion MNIST	61.3 ± 5.1	9.34 ± 0.002	4.96 ± 0.002	4.97 ± 0.002KMNIST	81.1 ± 1.3	9.48 ± 0.001	4.60 ± 0.002	4.34 ± 0.013SVHN	42.5 ± 1.5	6.01 ± 0.062	4.16 ± 0.013	4.15 ± 0.009E.2 Time Complexity AnalysisFor time complexity, as we mentioned in Appendix E, our posterior-predictive algorithm based onimportance sampling does not induce significant overhead thanks to the reuse of the shared termsfor calculation. More specifically, our algorithm with K sample variances spends O(K + N3)time, instead of O(KN3), for computing a posterior predictive, where N is the number of trainingpoints. Compare this with the usual time complexity O(N 3) of the standard algorithm for Gaussianprocesses. When it comes to SVGP and SVTP, one update step of both SVGP and SVTP takesO(BM2 + M3 ) time, where B is the number of the batch size of the input dataset and M is the
Table 5: RMSE values on UCI dataset. (m, d) denotes number of data points and features, respec-tively. We take results from Adlam et al. (2020) except our model.
Table 6: Additional regression results for Burr Type XII prior distribution.
