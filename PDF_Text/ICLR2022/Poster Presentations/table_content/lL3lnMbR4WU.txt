Table 1: Training with only base categories achieves comparable average recall (AR) for novel categorieson LVIS. We compare RPN trained with base only vs. base+novel categories and report the bounding box AR.
Table 2: Using CLIP for open-vocabulary detection achieves high detection performance on novel cate-gories. We apply CLIP to classify cropped region proposals, with or without ensembling objectness scores, andreport the mask average precision (AP). The performance on novel categories (APr) is far beyond supervisedlearning approaches. However, the overall performance is still behind.
Table 3: Performance of ViLD and its variants. ViLD outperforms the supervised counterpart on novelcategories. Using ALIGN as the teacher model achieves the best performance without bells and whistles. Allresults are mask AP. We average over 3 runs for R50 experiments. 1: methods with R-CNN style; runtime is630 x of Mask R-CNN style.æœˆ for reference, fully-supervised learning with additional tricks.
Table 4: Performance on COCO dataset compared with existing methods. ViLD outperforms all the othermethods in the table trained with various sources by a large margin, on both novel and base categories.
Table 5: Generalization ability of ViLD. We evaluate the LVIS-trained model with ResNet-50 backbone onPASCAL VOC 2007 test set, COCO validation set, and Objects365 v1 validation set. Simply replacing thetext embeddings, our approaches are able to transfer to various detection datasets. The supervised baselines ofCOCO and ObjeCtS365 are trained from scratch. t: the supervised baseline of PASCAL VOC is initialized withan ImageNet-pretrained checkpoint. All results are box APs.
Table 6: ALIGN on cropped regions achieves superior APr, and overall very good performance. It showsa stronger open-vocabulary classification model can improve detection performance by a large margin. Wereport box APs here.
Table 7: Hyperparameter sweep for visual distillation in ViLD. L1 loss is better than L2 loss. For L1 loss,there is a trend that APr increases as the weight increases, while APf,c decrease. For all parameter combi-nations, ViLD outperforms ViLD-text on APr . We use ResNet-50 backbone and shorter training iterations(84,375 iters), and report mask AP in this table.
Table 8: Performance of ViLD variants. This table shows additional box APs for models in Table 3 andResNet-152 results.
Table 9: Ablation study on prompt engineering. Results indicate ensembling multiple prompt templatesslightly improves APr . ViLD w/ multiple prompts is the same ViLD model in Table 3, and ViLD w/ singleprompt only changes the text embeddings used as the classifier.
