Table 1: Experimental results for different supervision levels with the presented Cl-InfoNCE objective. Forweakly-supervised methods that consider attributes-determined clusters, we report the best results by tuningthe hyper-parameter k . The results suggest that, with the help of auxiliary information, we can better close theperformance gap between supervised and self-supervised representations.
Table 2: Experimental results for weakly-supervised representation methods that leverage auxiliary informationand self-supervised representation methods. Best results are highlighted in bold. The results suggest that ourmethod outperforms the weakly-supervised baselines in most cases with the exception that the CMC methodperforms better than our method on the CUB-200-2011 dataset.
Table 3: Results on ImageNet-100 (Russakovsky et al., 2015) compare with a concurrent andindependent work Zheng et al. (2021).
Table 4: Comparison with IDFD (Tao et al., 2021) on CIFAR10 dataset (Krizhevsky et al., 2009).
Table 5: Additional Comparsion with SwAV (Caron et al., 2020) showing its similar performance asPCL on ImageNet-100 dataset.
Table 6: Preliminary results for WordNet-hierarchy-determined clusters + Cl-InfoNCE on ImageNet-1K.
