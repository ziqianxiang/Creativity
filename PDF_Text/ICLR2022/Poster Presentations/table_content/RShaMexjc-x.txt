Table 1: Partitioning performances on real datasetsmeasured by AMI. We see in bold (resp. italic) thefirst (resp. second) best method. NA: non applicable.
Table 2: Clustering performances on real datasets measured by Rand Index. In bold (resp. italic) wehighlight the first (resp. second) best method.
Table 3: Embedding computation times (in ms) averaged over whole datasets at a convergenceprecision of 10-5 on learned dictionaries. (-) (resp. (+)) denotes the fastest (resp. slowest)runtimes regarding DL configurations. We report here runtimes using F GW0.5 for datasets withnodes attributes. Measures taken on Intel(R) Core(TM) i7-4510U CPU @ 2.00GHz.
Table 4: Partitioning benchmark: Datasets statistics.
Table 5: Partitioning performances on real datasets measured by AMI. Comparison between srGWand Kmeans whose hard assignments are used to initialize srGW.
Table 6: Partitioning performances on real datasets measured by AMI: Ablation study of the param-eter involved in the power-law transformations parameterized by b ∈ [0, 1] of normalized degreedistributions for srGW and GW based methods. We denote different modes of transformation by’unif’ (b = 0), ’deg’ (b = 1) and ’inter’ (0 < b < 1). We see in bold (resp. italic) the first (resp.
Table 7: Partitioning performances on realdatasets measured by Adjusted Rand Index(ARI) corresponding to best configurationsreported in Table 1. We see in bold (resp.
Table 9: Clustering and Completion benchmark: Datasets descriptionsdatasets	features	#graphs	#classes	mean #nodes	min #nodes	max #nodes	median #nodes	mean connectivity rateIMDB-B	None	1000^"	2"	19.77	12"	136^	17^"	55.53IMDB-M	None	1500^"	3""	13.00	7	89""	10^"	86.44MUTAG	-{0..2T	W	2"	17.93	10^"	28""	17Γ"	14.79PTC-MR	{0,..,17}	344^	2"	14.29	2"	64^	13""	25^BZR	R3^"	W	2"	35.75	13""	57"	35^"	670-COX2	R3""	467"	2"	41.23	32^"	56^	4Γ^	524^PROTEIN	R2^"	1113""	2"	29.06	4~	620^"	26^	23.58ENZYMES	R18	600	6	32.63-	2	126 -	32^	17.14srGW runtimes: CPU vs GPU for large graphs. Par-titioning experiments with our methods were run on aGPU Tesla K80 as it brought a considerable speed up interms of computation time compared to using CPUs aslarge graphs had to be processed. To illustrate this mat-ter, we generated 10 graphs following Stochastic BlockModels with 10 clusters, a varying number of nodes in{100, 200, ..., 2900, 3000} and the same connectivity ma-trix. We report in Table the averaged runtimes of one CGiteration depending on the size of the input graphs. For
Table 10: Clustering performances on real datasets measured by Adjusted Rand Index(ARI. In bold(resp. italic) we highlight the first (resp. second) best method.
Table 11: Clustering performances on real datasets measured by Adjusted Mutual Information(AMI).
