Table 1: To preserve the reasoning ability for analysis, our SparseVD tool prunes the GNN-basedmodels without loss of accuracy on CommonsenseQA dataset. As the official test is hidden, here wereport the in-house dev (IHdev) and test (IHtest) accuracy, following the data split of Lin et al. (2019).
Table 2: Our GSC does not use initial node em-GSC layers are parameter-free and only keepthe basic graph operations: Propagation and ag-gregation. To overcome over-parameterization,one straightforward way is to reduce the hiddensize, and the extreme case is reducing it to only1. As shown in Figure 4, the GSC layer simplypropagates and aggregates the numbers on theedges and node following the two-step scheme:1) update the edge value with in-node value, andthis is done by simply indexing and adding; 2)update the node value by aggregating the edgevalues, and this is done by scattering edge valuesto out-node.
Table 3: GSC is extremely efficient compared tothe computation complexity of L-hop reasoningmodels with hidden dimension D on a dense /sparse graph G = (V, E) with the relation set R.
Table 4: Performance comparison on CommonsenseQAin-house split (controlled experiments). As the officialtest is hidden, here we report the in-house dev (IHdev)and test (IHtest) accuracy, following the data split ofLin et al. (2019).
Table 5: Test accuracy on CommonsenseQAâ€™s of-ficial leaderboard. The existing top system, Uni-fiedQA (11B params) is 30x larger than our model.
Table 6: Test accuracy on OpenBookQA. Methods withAristoRoBERTa use the textual evidence by Clark et al.
Table 7: Test accuracy on OPenBookQA leader-board. All listed methods use the provided sciencefacts as an additional input to the language con-text. The previous top 2 systems, UnifiedQA (11Bparams) and T5 (3B params) are 30x and 8x largerretrieval.
Table 8: Ablation study on the hard counter with MLP(upper) and the maximum number of retrieved nodes(bottom), showing that 1) the hard counter achievesperformance on par with GNN-based methods; 2) GSCworks well even when only using entities occurred inQA context, which typically contains less than 32 nodes.
Table 9: Our GSC achieves comparable performancewith the baselines on the MetaQA dataset, which indi-cates that our observations and hypotheses keep consis-tent multi-hop QA.
Table 10: Ablation study on the hidden dimensionsize of the GAT model, where we find that the modelstill works when the hidden size is extremely small,and the model get worse when the hidden size isextremely large due to the problem of over-fitting.
Table 11: We list the top-30 edge triplets with highest soft counts here, and it is generated by the edgeencoder of GSC model. The combination of edge types and node types with a higher count means itcan contribute more to the final graph score.
