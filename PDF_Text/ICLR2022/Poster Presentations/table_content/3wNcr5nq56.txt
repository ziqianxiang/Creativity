Table 1: Models with batch normalization. Performance shown for three different classificationdatasets. We report averages (± one standard error) from three trials.
Table 2: Dilated Filters. The average accuracy of models with dilated filters trained and tested onsmall mazes.	_________________________________________________Effective DepthModel	8	12	16	20Recurrent	75.07	90.41	94.70	95.19Feed-forward	75.59	90.58	93.84	95.005	Recurrent models reuse featuresTest accuracy is only one point of comparison, and the remarkable similarities in Figure 2 raise thequestion: What are the recurrent models doing? Are they, in fact, recycling filters, or are some filtersonly active on a specific iteration? In order to answer these questions, we look at the number ofpositive activations in each channel of the feature map after each recurrence iteration for a batch of1,000 randomly selected CIFAR-10 test set images. For each image-filter pair, we divide the numberof activations at each iteration by the number of activations at the most active iteration (for thatpair). If this measurement is positive on iterations other than the maximum, then the filter is beingreused on multiple iterations. In Figure 6, it is clear that a large portion of the filters have activationpatterns on their least active iteration with at least 20% of the activity of their most active iteration.
Table 3: Classifiability of Features. CIFAR-10 classification accuracy of linear classifiers trainedon intermediate features output by each residual block (or each iteration) in networks with 19-layereffective depths. Each cell reports the percentage of training/testing images correctly labeled.
Table 4: MLPs: The size of each fully connected layer in the MLPs we use for image classification.			Dataset	First Layer	Internal Layers	Last LayerCIFAR-10	3072 × 200	-~200 X 200~~	200 × 10SVHN	3072 × 500	500 × 500	500 × 10EMNIST	3072 × 500	500 × 500	500 × 47ConvNets. The ConvNets have two convolutional layers before the internal module and one convo-lutional and one fully connected layer afterward. Each convolutional layer is followed by a ReLUactivation, and there are pooling layers before and after the last convolutional layer. For all convo-lutional layers outside of the internal module, we use 3 × 3 filters with stride of 1 and no padding.
Table 5: ConvNets: The number of channels in the output of each layer.				Dataset	First Layer	Second Layer	Internal Layers	Last Conv Layer Linear LayerCIFAR-10	32	64	64	128	10SVHN	32	64	64	128	10EMNIST	32	64	64	128	47Residual Networks. The residual networks employ convolutional layers with 3 × 3 kernels. Thefirst layer has a stride of two pixels and padding of one pixel in every direction. Each layer in theinternal modules has striding by one pixel, while the final convolutional layer strides by two pixels.
Table 6: Classifiability of Features. CIFAR-10 classification accuracy of linear classifiers trainedon intermediate features output by each residual block (or each iteration) in networks with 19-layereffective depths. Average pooling is used to change the dimension of feature maps in this experiment.
Table 7: Classifiability of Features. CIFAR-10 classification accuracy of linear classifiers trainedon intermediate features output by each iteration. Each cell reports the percentage of training/testingimages correctly labeled.
