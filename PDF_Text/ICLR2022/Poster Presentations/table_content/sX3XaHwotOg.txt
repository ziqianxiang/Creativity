Table 1: Results on GLUE and SQuAD 2.0 development set (GLUE test set results can be found inAppendix D). All results are single-task, single-model fine-tuning. We use Spearman correlation forSTS, Matthews correlation for CoLA, and accuracy for the rest on GLUE. AVG is the average of theeight tasks on GLUE. All baseline results are reported by previous research. Results not available inpublic reports are marked as “-”.
Table 2: Ablations on MNLI/SQuAD 2.0 dev sets that				Table 3: Edge probing results using differ-			remove (-), add (+) or switch (w.) one component. Val-				ent MLM layers from the AMOS generator.			ues are differences (in absolute points) from AMOSBase.				Tasks are ordered based on suggested seman-			Group	Method	MNLI (m/mm)	SQuAD 2.0 EM/F1	tic depths in Tenney et al. (2019a).				AMOSBase	88.9/88.7	84.2/87.2	Tasks	layer 4	layer 6	layer 8Curriculum Setup	w. random layer w. layer switch	-0.3/-0.4 -0.3/-0.3	-0.6/-0.6 -0.9/-1.0	POS Consts.	92.6^^ 69.7	93.4 73.2	91.2 73.0Adversarial Setup	- adv. train + adv. MLM								-0.2/-0.2 -0.3/0.0	-0.3/-0.4 0.0/0.0	Deps. Entities	86.4 91.7	85.1 93.9	88.3 93.9							Multi-MLM	- stop grad.	-0.5/-0.1	-0.6/-0.6				Setup	w. separate MLM gen.	-0.1/0.0	-0.8/-0.7	SRL	76.9	74.2	79.2Backbone	4-layer gen.	-0.5/-0.5	-1.1/-1.2	Coref.	77.6	75.5	77.9(No Multi-MLM	6-layer gen.	-0.3/-0.4	-1.1/-1.1	SPR2	79.4	79.0	79.9No Adv. Train)	8-layer gen. 12-layer gen.	-0.6/-0.6 -1.1/-1.2	-0.9/-0.9 -1.8/-1.7	Relations	72.2	76.3	73.9pretraining steps. We have experimented switching at different steps of pretraining but do not observesignificantly better results. Manual trials are tedious and expensive in pretraining and underperformautomatically learned mixture over multiple training signals.
Table 4: GLUE task statistics and information.
Table 5: Hyperparameters used in pretraining.
Table 6: Hyperparameter ranges searched for fine-tuning.
Table 7: GLUE test set scores obtained from the GLUE leaderboard. We follow the standard in recentresearch to construct the test predictions: Searching for best hyperparameters with ten random seedson each task individually and using the best development set model on testing data. All results arefrom vanilla single-task fine-tuning (no ensemble, task-specific tricks, etc.).
Table 8: Ablations on the development sets of all GLUE tasks and SQuAD 2.0 that eliminate (-), add(+) or switch (w.) one component. We show the median and standard deviation (as subscripts) of fiverandom seeds on each task. The results are extensions of Table 2.
Table 9: Performance study with different numbers of MLM heads used in the generator. We showthe median and standard deviation (as subscripts) of five random seeds on each task.
Table 11: Examples of replaced tokens by different-sized MLM generators. Underlined words aremasked out; the replaced tokens by 4/6/8-layer generators are marked in different colors.
