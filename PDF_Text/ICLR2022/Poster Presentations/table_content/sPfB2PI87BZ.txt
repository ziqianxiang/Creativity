Table 1: Balanced accuracy (↑) over 10 runs. The best performing model is indicated in bold.
Table 2: Effect of OT on balanced accuracy ↑ (left), '1 estimation error ] (middle) and normalizedtransport cost LOT/n ] (right) for MNIST→USPS. The best model for accuracy is in bold With a“?“. We consider varying shifts (Line 1) and initialization gains (stdev of the NN’s weights) (Line2)MNIST→USPS - initialization gain 0.02		Shift	λOT = 0	λOT = 10-2balanced	94.92 ± 0.6	95.12 ± 0.6mild	88.28 ± 1.5	91.77 ± 1.2high	85.24 ± 1.6	88.55 ± 1.1■ OSTAR Aor = OOSTAR A0τ =10-2MNIST→USPS - high imbalance		Init Gain	λOT = 0	λOT = 10-20.02	85.24 ± 1.6	88.55 ± 1.10.1	84.62 ± 2.3	88.41 ± 1.30.3	83.11 ± 2.4	89.41 ± 1.6(Wu et al., 2019). When conditionals are unchanged i.e. TarS, pYT can be recovered without needsfor alignment (Lipton et al., 2018; Redko et al., 2019). Under GeTarS, there is the additionaldifficulty of matching conditionals. The SOTA methods for GeTarS are domain-invariant withsample reweighting (Rakotomamonjy et al., 2021; Combes et al., 2020; Gong et al., 2016). An ear-lier mapping-based approach was proposed in Zhang et al. (2013) to align conditionals, also under
Table 3: Balanced accuracy (↑) over 10 runs. The best performing model is indicated in bold.
Table 4: Semi-supervised learning for OSTAR and balanced accuracy (↑). Best results are in bold.
Table 5: IM for MARSc, IW-WD and OSTAR on balanced accuracy (↑). Best results are in bold.
Table 6: Best value over training epochs of term (A) (J), term (C) (J) and term (L)⑷ withoutand with IM in OSTAR. Best results are in bold. Terms (A) and (L) are computed with the primalformulation of OT using the POT package https://pythonot.github.io/.
Table 7: Detailed analysis of the impact of λOT on balanced accuracy (↑). Best results are in bold.
Table 8: Label imbalance settingsDataset	Configuration	PS	pYTDigits	balanced subsampled mild subsampled high	住,…,10} {E ,二,E} { 10 ,	, 10 }	{10，…，10 } {0,1,2,3,6} = 0.06, {4, 5} = 0.2, {7, 8, 9} = 0.1 {0,1,2,3,6,7,8,9} = 0.07, {4, 5} = 0.22VisDA12	original	{0 - 11} : 100%	{0 - 11} : 100%	subsampled	{0-5} : 30% {5-11} :	100%	{0 - 11} : 100%Office31	subsampled	{0 - 15} : 30% {15 - 31}	: 100%	{0 - 31} : 100%OfficeHome	subsampled	{0 - 32} : 30% {33 - 64}	: 100%	{0 - 64} : 100%Hyperparameters Domain-invariant methods weight alignment over classification; we tuned thecorresponding hyperparameter for WDβ=0 in the range [10-4, 10-2] and used the one that achievesthe best performance on other models. We also tuned λOT in problem (CAL) and fixed it to 10-2on Digits and 10-5 on VisDA12, Office31 and OfficeHome. Batch size is Nb = 200 andall models are trained using Adam with learning rate tuned in the range [10-4, 10-3]. We initializeNN for classifiers and feature extractors with a normal prior with zero mean and gain 0.02 and φwith orthogonal initialization with gain 0.02.
