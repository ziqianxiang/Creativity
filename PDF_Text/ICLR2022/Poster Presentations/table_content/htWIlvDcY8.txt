Table 1: Fast concept learning performance onthe CUB dataset. We use “E2E” for end-to-endmethods and “C.C.” for concept-centric meth-ods. Our model FALCON-G and FALCON-Routperforms all baselines.
Table 2: Fast concept learning with only visualexamples, evaluated on the CUB dataset. Thetask is also referred to as one-shot learning. Inthis setting, NSCL+GNN will fallback to a Pro-totypical Network (Snell et al., 2017).
Table 4: All program operations supported in CLEVR.
Table 5: Example questions and the programs recovered by the semantic parser. For descriptionsentences, the semantic parser parse the referential expression in the sentence into a program. OnCUB, since there is only one object in the image, the operation Scene() returns the only object inthe image.
Table 6: Templates for generating descriptive sentences, supplemental sentences and test questions inCUB.
Table 7:	The continual concept learning performance on the CUB dataset. Only concept-centricmethods can be applied to this setting. FALCON-G and FALCON-R outperform all baselines.
Table 8:	Templates for generating descriptive sentences, supplemental sentences and test questions inCLEVR.
Table 9: Ablated study of FALCON tasks w/. and w/o. supplemental sentences (+Supp.) on thebiased CLEVR dataset. All concept-centric methods use the box embedding space.
Table 10: Templates for generating descriptive sentences, supplemental sentences and test questionsin GQA.
Table 12: AUC-ROC score of classifying entailment relationship based on concept embeddings fromdifferent embedding spaces.
Table 13: Fast concept learning (with supplemental sentence) performance under different number ofbase concepts.
Table 14: Fast concept learning performance under different percentage of related concepts in thesupplemental sentence.
Table 15:	A visualization of three failure cases of the detection module. The first two descriptivesentences are evaluated to find the wrong example object, and the last two are evaluated to find theright example object.
Table 16:	A visualization of three failure cases of the semantic parser. All examples are from thequestion answering part of tasks on learning the novel concept cyan based on the image above.
