Table 1: Max Average Return for MED-RL SAC over 5 trials of 1 million time steps. Maximumvalue for each task is bolded. ± corresponds to a single standard deviation over trialsEnvironment	Baseline	MeanVector	Gini	Atkinson	Theil	VOLHalfCheetah	10380.3 ± 681.8	12278.1 ± 160.3	11691.2 ± 715.6	12117.2 ± 304.7	12212.7 ± 216.7	12339.5 ± 284.9Ant	4802.4 ± 605.1	6298.7 ± 101.8	6047.8 ± 167.4	6163.2 ± 207.6	6091.5 ± 222.3	5965.1 ± 196.4Hopper	2882.5 ± 738.3	3604.3 ± 27.8	3552.9 ± 60.5	3560.4 ± 82.2	3596.6 ± 57.7	3587.7 ± 42.9Walker2d	3954.9 ± 356.7	4525.7 ± 340.9	4523.0 ± 440.1	4659.8 ± 253.0	4753.8 ± 394.7	4653.4 ± 391.2Humanoid	4582.2 ± 592.4	5359.1 ± 42.0	5224.6 ± 105.1	5275.1 ± 40.4	5355.2 ± 137.3	5311.7 ± 49.1Humanoid-	153633.2	177666.5	170592.6	164967.6	180268.1	179645.1Standup	± 8256.6	± 30044.1	± 29346.3	± 19464.6	± 33080.4	± 29980.4Table 2: Max Average Return for MED-RL TD3 over 5 trials of 1 million time steps. Maximumvalue for each task is bolded. ± corresponds to a single standard deviation over trialsEnvironment	Baseline	MeanVector	Gini	Atkinson	Theil	VOLHalfCheetah	9583.1 ± 682.6	11539.4 ± 278.1	11477.9 ± 405.2	11442.5 ± 187.8	11232.7 ± 323.6	11393.6 ± 532.7Ant	3829.1 ± 675.7	4829.6 ± 1036.9	4611.5 ± 781.9	4565.7 ± 908.4	4810.7 ± 347.9	4881.1 ± 831.6Hopper	2965.3 ± 423.5	3651.3 ± 57.7	3629.5 ± 92.8	3582.3 ± 153.9	3649.1 ± 62.7	3614.1 ± 89.2Walker2d	4140.6 ± 334.2	4396.3 ± 837.5	4666.3 ± 319.7	4652.7 ± 310.0	4528.5 ± 507.1	4630.0 ± 405.1Humanoid	4347.4 ± 456.2	5060.5 ± 127.4	5048.8 ± 199.3	5116.3 ± 278.5	5096.1 ± 98.1	5040.0 ± 112.7Humanoid-	135176.6	160293.8	151123.1	154652.2	160481.5	146970.3Standup	± 7991.2	± 19657.2	± 12712.8	± 5607.7	± 15229.6	± 13199.6
Table 2: Max Average Return for MED-RL TD3 over 5 trials of 1 million time steps. Maximumvalue for each task is bolded. ± corresponds to a single standard deviation over trialsEnvironment	Baseline	MeanVector	Gini	Atkinson	Theil	VOLHalfCheetah	9583.1 ± 682.6	11539.4 ± 278.1	11477.9 ± 405.2	11442.5 ± 187.8	11232.7 ± 323.6	11393.6 ± 532.7Ant	3829.1 ± 675.7	4829.6 ± 1036.9	4611.5 ± 781.9	4565.7 ± 908.4	4810.7 ± 347.9	4881.1 ± 831.6Hopper	2965.3 ± 423.5	3651.3 ± 57.7	3629.5 ± 92.8	3582.3 ± 153.9	3649.1 ± 62.7	3614.1 ± 89.2Walker2d	4140.6 ± 334.2	4396.3 ± 837.5	4666.3 ± 319.7	4652.7 ± 310.0	4528.5 ± 507.1	4630.0 ± 405.1Humanoid	4347.4 ± 456.2	5060.5 ± 127.4	5048.8 ± 199.3	5116.3 ± 278.5	5096.1 ± 98.1	5040.0 ± 112.7Humanoid-	135176.6	160293.8	151123.1	154652.2	160481.5	146970.3Standup	± 7991.2	± 19657.2	± 12712.8	± 5607.7	± 15229.6	± 13199.6Table 3: Max Average Return for MED-RL REDQ over 5 trials of 300K time steps. Maximum valuefor each task is bolded. ± corresponds to a single standard deviation over trialsEnvironment	Baseline	MeanVector	Gini	Atkinson	Theil	VOLHalfCheetah	8368.3 ± 56.3	10067.9 ± 360.7	10234.4 ± 74.4	9926.9 ± 319.0	10161.6 ± 461.7	9664.8 ± 1975.2Ant	3001.3 ± 2083.5	5446.8 ± 186.7	5801.7 ± 42.3	5616.2 ± 86.3	5885.6 ± 181.0	5897.4 ± 16.7Hopper	2876.9 ± 584.7	3477.3 ± 43.6	3565.9 ± 40.9	3524.8 ± 2.8	3596.6 ± 72.1	3550.8 ± 50.1Walker2d	3722.3 ± 52.6	4282.7 ± 414.5	4217.1 ± 150.6	4133.9 ± 145.9	5028.4 ± 205.6	4249.2 ± 201.38Published as a conference paper at ICLR 20225.4	Sample Efficiency and Compute Time
Table 3: Max Average Return for MED-RL REDQ over 5 trials of 300K time steps. Maximum valuefor each task is bolded. ± corresponds to a single standard deviation over trialsEnvironment	Baseline	MeanVector	Gini	Atkinson	Theil	VOLHalfCheetah	8368.3 ± 56.3	10067.9 ± 360.7	10234.4 ± 74.4	9926.9 ± 319.0	10161.6 ± 461.7	9664.8 ± 1975.2Ant	3001.3 ± 2083.5	5446.8 ± 186.7	5801.7 ± 42.3	5616.2 ± 86.3	5885.6 ± 181.0	5897.4 ± 16.7Hopper	2876.9 ± 584.7	3477.3 ± 43.6	3565.9 ± 40.9	3524.8 ± 2.8	3596.6 ± 72.1	3550.8 ± 50.1Walker2d	3722.3 ± 52.6	4282.7 ± 414.5	4217.1 ± 150.6	4133.9 ± 145.9	5028.4 ± 205.6	4249.2 ± 201.38Published as a conference paper at ICLR 20225.4	Sample Efficiency and Compute TimeTables 1 to 3 show that MED-RL augmented continuous control algorithms outperform the baselineversions significantly and a visual inspection of Figures 3, 4 and 5 show that MED-RL augmentedalgorithms are more sample-efficient as well. But are they more sample-efficient than algorithmsthat are specifically designed for sample-efficiency such as REDQ? To answer this question, wetook the bolded results from Table 1, referred as MED-RL in this section, and evaluated the numberof environment interactions and wall-clock time it took for MED-RL to reach similar performanceas that of baseline REDQ. As shown in Table 4, MED-RL achieves similar performance to REDQin 50% and 20% few environment interactions on Ant and HalfCheetah environment respectivelyand have significantly surpassed REDQ on 300K environment interactions. MED-RL does not onlyimprove sample-efficiency but significantly improves compute time. As shown in Table 4, MED-RL
Table 4: Comparison of MED-RL augmented SAC with baseline REDQ on sample-efficiency andwall-clock time.
Table 5: Max Average Return for MED-RL MaxminDQN with two neural networks on PyGames andMinAtar environments. Maximum value for each task is bolded. ± corresponds to a single standarddeviation over trials.
Table 6: Max Average Return for MED-RL MaxminDQN with three neural networks on PyGamesand MinAtar environments. Maximum value for each task is bolded. ± corresponds to a singlestandard deviation over trials.
Table 7: Max Average Return for MED-RL MaxminDQN with four neural networks on PyGames andMinAtar environments. Maximum value for each task is bolded. ± corresponds to a single standarddeviation over trials.
Table 8: Max Average Return for MED-RL EnsembleDQN with two neural networks on PyGamesand MinAtar environments. Maximum value for each task is bolded. ± corresponds to a singlestandard deviation over trials.
Table 9: Max Average Return for MED-RL EnsembleDQN with three neural networks on PyGamesand MinAtar environments. Maximum value for each task is bolded. ± corresponds to a singlestandard deviation over trials.
Table 10: Max Average Return for MED-RL EnsembleDQN with four neural networks on PyGamesand MinAtar environments. Maximum value for each task is bolded. ± corresponds to a singlestandard deviation over trials.
Table 11: Hyperparameters for discrete control tasksHyperparameter	ValueTarget Weight τ Actor Learning Rate Regularization Weight Replay Buffer Batch Size Exploration Steps Optimizer	1e-3	= [1e-3,1e-4] 1e-6, 1e-7,1e-8 1e6 32 5000 AdamTable 12: Hyperparameters for continuous control tasksHyperparameter	ValueTarget Weight T	1e-3Actor Learning Rate	[1e-4, 3e-5]Critic Learning Rate	[le-4, 3e-5]Replay Buffer	1e6Batch Size	[256]Exploration Steps	25000Optimizer	AdamHidden Layer Size	256Number of critics (REDQ)	10Regularization Weight	1e-624Published as a conference paper at ICLR 2022F.1 Computing InfrastructureAll the experiments were performed on a Kubernetes managed cluster with Nvidia V100 GPUs andIntel Skylake CPUs. Each experiment was run as an individual Kubernetes job with 11 CPUs, 16GB
Table 12: Hyperparameters for continuous control tasksHyperparameter	ValueTarget Weight T	1e-3Actor Learning Rate	[1e-4, 3e-5]Critic Learning Rate	[le-4, 3e-5]Replay Buffer	1e6Batch Size	[256]Exploration Steps	25000Optimizer	AdamHidden Layer Size	256Number of critics (REDQ)	10Regularization Weight	1e-624Published as a conference paper at ICLR 2022F.1 Computing InfrastructureAll the experiments were performed on a Kubernetes managed cluster with Nvidia V100 GPUs andIntel Skylake CPUs. Each experiment was run as an individual Kubernetes job with 11 CPUs, 16GBof RAM and 1 GPU (if needed). This configuration allowed us to run experiments without anyinterference from other applications which was important to accurately measure the wall-clock time.
