Table 1: Number of FaceScrub users whose images are perturbed for each attack. Both theperturbed and unperturbed images of these users are used during robust training.
Table 2: Performance of a model trained to detect perturbed images. Detection performance isvery high across all attacks even when smaller perturbations are used (i.e. Fawkes “low” and “mid”).
Table 3: Performance of a model trained to detect perturbed images of one attack (source)when evaluated on another attack (destination).
Table 4: Transferability of adversarial examples from an ensemble of four models—GoogLeNet, VGG-16, Inception-v3, ResNet-50—to future ImageNet models. Numbers in boldshow the models that are most robust to the attack at a given point in time.
