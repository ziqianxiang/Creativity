Table 1: Optima found by training a linear VAE on data generated by a linear generator (i.e. a linearly trans-formed standard multivariate gaussian embedded in a larger ambient dimension by padding with zeroes) viagradient descent. The results reflect the predictions of Theorem 5: the number of nonzero rows of the decoderalways match the dimensionality of the input data distribution with no variance while the number of nonzerodimensions of encoder variance is greater than or equal to the nonzero rows. All VAEs are trained with a20-dimensional latent space. Clearly, the model fails to recover the correct eigenvalues and therefore has asubstantially wrong data density function.
Table 2: Optima found by training a VAE on the sigmoid dataset. The VAE training consistently yields encodervariances with number of 0 entries greater than or equal to the intrinsic dimension.
Table 3: Optima found by training a VAE on data generated by padding uniformly random samples from aunit r-sphere with zeroes, so that the sphere is embedded in a higher ambient dimension. We evaluated themanifold error as described in the setup. The VAE training on this dataset has consistently yielded encodervariances with number of 0 entries greater than the number of intrinsic dimension.
Table 4: Optima found by training a VAE on the sigmoid dataset. The VAE training yields encoder varianceswith number of 0 entries equal to the intrinsic dimension.
Table 5: Optima found by training a VAE on data generated by padding uniformly random samples from aunit r-sphere with zeroes, so that the sphere is embedded in a higher ambient dimension. We evaluated themanifold error as described in the setup. The VAE training on this dataset has consistently yielded encodervariances with number of 0.1 entries greater than the number of intrinsic dimension.
