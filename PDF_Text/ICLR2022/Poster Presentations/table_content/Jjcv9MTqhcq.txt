Table 1: Downstream transferring results with linear fine-tuning. For each method, “epochs”indicates their pre-training epochs and “aug++” indicates whether trained with strong data augmen-tation. f suggests that models are from official open-source codebases.
Table 2: Downstream transferring results with fully fine-tuning. See caption of Table 1 for detail.
Table 3: Downstream transferring results of different fine-tuning methods. The percentageunder each dataset indicates the sampling rate of training samples.
Table 4: Linear fine-tuning results of varying hyper-parameters in the LOOK pre-training,including the queue size q, momentum m and number of nearest neighbors k. Models are trainedwith the default settings that q =		65536, m = 0.99,	τ = 1.0 and deCaying k from 400 to 40.		queue size	fine-tuning	momentum m	fine-tuning	k of kNN	fine-tuning65,536	78.55	0.9999	78.44	100	75.1932,768	78.23	0.999	78.30	200	78.4216,384	77.72	0.99	78.55	400	77.938,192	77.71	0.9	78.44	800	77.51upstream over-fitting, with an improvement to C.E. with 10.9% accuracy. Compared with self-supervised learning, LOOK also surpasses state of the art method, i.e. SimSiam, by 2.7% meanaccuracy via effectively leveraging the label information. It is also observed that though strongdata augmentation boosts the self-supervised pre-training, it may introduce negative influence onsupervised C.E. pre-training. Since the encoder for extracting features is frozen in linear fine-tuning,the experimental results indicate that LOOK could present more generalized representation basedon pre-training, compared with existing supervised and self-supervised methods.
Table 5: Results of memory-based fine-tuning, including voting and clustering. The linear fine-tuning results of C.E. and LOOK are listed for reference.
Table 6: Averaged intra and inter class distance of different pre-training methods, where largervalue indicates higher variance of intra-class or inter-class samples.
Table 7: Statistics of downstream tranferring datasets, including the train/validation/test split, num-ber of classes and type of visual contentsDataset	# train	# val	# test	# classes	typeAircraft (Maji et al., 2013)	3,334	3,333	3,333	100	technicalCars (Krause et al., 2013)	5,700	2,444	8,401	196	technicalDTD (Cimpoi et al., 2014)	1,880	1,880	1,880	47	textureEuroSAT (Helber et al., 2019)	13,500	5,400	8,100	10	satelliteFlowers (Nilsback & Zisserman, 2008)	1,020	1,020	6,149	102	naturalISIC (Codella et al., 2019)	5,007	2,003	3,005	7	medicalKaokore (Tian et al., 2020)	6,568	826	821	8	illustrativeOmniglot (Lake et al., 2015)	6,590	2,636	3,954	1,623	symbolicPets (Patino et al., 2016)	2,575	1,105	3,669	37	naturalB Details of pre-training and fine-tuning methodsB.1	Supervised pre-training.
Table 8: Probability that all the c sub-classes are sampled with at least one image under the defaultsetting of queue size of 65, 536.
Table 9: Upstream accuracy on ImageNet (kNN and linear classifier) and downstream performanceon 9 fine-grained datasets (linear or fully fine-tuning).
Table 10: Transferring results of objection detection and instance segmentation on PASCALVOC and COCO. “2V” indicates training with two augmented views of each image. All the com-pared methods are fine-tuned with the 1× schedule.
