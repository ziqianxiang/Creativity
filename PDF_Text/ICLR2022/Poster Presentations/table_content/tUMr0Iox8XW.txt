Table 1: Best Test Accuracies on CIFAR10 and Omniglot, best of MLPs up to 4 hidden layers,width 2048, r 400, as well as random search over a host of hyperparameters; see Appendix B. Notethe μ-Net numbers are also the optimal numbers for standard parametrization finite networks, asdiscussed in Footnote 12. π-Limit ImageNet Transfer means pretraining a π-limit with r = 200 onImageNet32 and perform kernel regression with its feature kernel (i.e. kernel of last layer activations)on CIFAR10; see Section 4.2 and Appendix B.4 for details and finite network results. Also comparewith feature kernel regression without pretraining (Table 8).
Table 2: CIFAR10 Hyperparameter Grid for μ-Net, π-Net, and π-LimitHyperparameter	GridGradient Clip	0.4 ∙{2-3, 2-2 ..., 24}Learning Rate	0.5 ∙{2-3, 2-2 ..., 24}Weight Decay	2 ∙ 10-5 ∙ {2-3, 2-2 ..., 24}Bias Mult	0.5 ∙{2-3, 2-2 ..., 24}LR Drop Milestone	{30, 35, 40}Input Weight LR Mult	0.1 ∙{2-3, 2-2 ..., 24}Output Weight LR Mult	16 ∙{2-3, 2-2 ..., 24}Input Weight Mult	{2-3,2-2...,24}Output Weight Mult	{2-3,2-2...,24}Batch Size	{4, 8,16, 32}Table 3: CIFAR10 Hyperparameter Grid for NNGP and NTKHyperparameter	GP	NTKBias Variance	{2-4, 2-3.5,	...,22} 0.5 ∙ {2-4, 2-3.5,..., 22} {2-4, 2-3.5, . . . , 22}Bias LR Multiplier	n.a.	Input Weight LR Multiplier	n.a.	0.5 ∙ {2-4, 2-3.5,..., 22} {1, 20.25, . . . , 25}Output Weight LR Multiplier	n.a.	Ridge		{10-8,10-7∙∙∙ ,10-1}B.3.2	NNGP AND NTK
Table 3: CIFAR10 Hyperparameter Grid for NNGP and NTKHyperparameter	GP	NTKBias Variance	{2-4, 2-3.5,	...,22} 0.5 ∙ {2-4, 2-3.5,..., 22} {2-4, 2-3.5, . . . , 22}Bias LR Multiplier	n.a.	Input Weight LR Multiplier	n.a.	0.5 ∙ {2-4, 2-3.5,..., 22} {1, 20.25, . . . , 25}Output Weight LR Multiplier	n.a.	Ridge		{10-8,10-7∙∙∙ ,10-1}B.3.2	NNGP AND NTKFor NNGP and NTK, we perform kernel regression following Lee et al. (2018) using centered labels.
Table 4: ImageNet Transfer Hyperparameter Grid for μ-Net, ∏-Net, and ∏-LimitHyperparameter	π-Limit TransferBias Mult	0.5 ∙{2-3, 2-2 ..., 23}Batch Size	{6, 8, 16}Learning Rate	0.01 ∙ {2-5, 2-4 ..., 26}Input Weight Mult	0.5 ∙ {1.50,15 25 ..., 1.52.75}Output Weight Mult	{2-0.5,20...,23}Weight Decay	{2-7,2-6...,20}Gradient Clip	{0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 0}Ridge	{10-8,10-7 …，10-1}Table 5: Omniglot Hyperparameter Grid for μ-Net, π-Net, and π-LimitHyperparameter		Grid	Step Size	0.5	∙{2-2, 2-1.75,.	. . , 22}Meta Learning Rate	16	∙{2-3, 2-2.75 ..	.,23}Gradient Clip	0.1	∙{2-2, 2-2.75 .	. . , 22}Bias Mult	1 ∙	{2-2, 2-3.75..	.,22}Input Weight Mult	2 ∙	{2-2, 2-1.75,..	. , 22}Input Weight LR Mult	0.2	∙{2-2, 2-1.75 .	. . , 22}and ilr* and olr* are the optimal inner and outer learning rates from the 2-hidden-layer sweep. Thebest validation accuracies per depth are shown in Fig. 7. Then we take the models with overall best
Table 5: Omniglot Hyperparameter Grid for μ-Net, π-Net, and π-LimitHyperparameter		Grid	Step Size	0.5	∙{2-2, 2-1.75,.	. . , 22}Meta Learning Rate	16	∙{2-3, 2-2.75 ..	.,23}Gradient Clip	0.1	∙{2-2, 2-2.75 .	. . , 22}Bias Mult	1 ∙	{2-2, 2-3.75..	.,22}Input Weight Mult	2 ∙	{2-2, 2-1.75,..	. , 22}Input Weight LR Mult	0.2	∙{2-2, 2-1.75 .	. . , 22}and ilr* and olr* are the optimal inner and outer learning rates from the 2-hidden-layer sweep. Thebest validation accuracies per depth are shown in Fig. 7. Then we take the models with overall bestvalidation accuracies over all depth and evaluate them on the test set using 10000 batches. These testresults are shown in Table 1.
Table 6: Omniglot Hyperparameter Grid for NNGP and NTKHyperparameter	NNGP	NTK	Bias Variance	0.1 ∙ {	22, 22.5,..., 210}	{2-1,2-0.5,...	,25}Bias LR Mult	n.a.	{2-4,2-3.5,...	,22}Input Layer LR Mult	n.a.	0.1 ∙{2-4, 2-3.5,	. . . , 24}Output Layer LR Mult	n.a.	0.1 ∙(2-4, 2-3.5,	. . . , 24}Table 7: Pretraining on ImageNet32 and Evaluating on CIFAR10, Full Results. We pretrainedμ-net, π-net with r = 200, π-net with r = 400, and π-limit with r = 400 on ImageNet32 andevaluated the result model on CIFAR10. Here, the π-limit number 64.39 is the same as in Table 1under π-Limit ImageNet Transfer. For reference, we also include the NNGP and NTK numbers inthe left block. The * indicates we are comparing the π-limit transfer performance with r = 200 vsπ-limit CIFAR10 number with r = 400, so the +2.79 is an underestimate of the improvement due topretraining. The benefit of pretraining seems to be directly related to the capacity of the model, asπ-Net with r = 200 < π-Net with r = 400 < μ-Net < π-Limit.
Table 7: Pretraining on ImageNet32 and Evaluating on CIFAR10, Full Results. We pretrainedμ-net, π-net with r = 200, π-net with r = 400, and π-limit with r = 400 on ImageNet32 andevaluated the result model on CIFAR10. Here, the π-limit number 64.39 is the same as in Table 1under π-Limit ImageNet Transfer. For reference, we also include the NNGP and NTK numbers inthe left block. The * indicates we are comparing the π-limit transfer performance with r = 200 vsπ-limit CIFAR10 number with r = 400, so the +2.79 is an underestimate of the improvement due topretraining. The benefit of pretraining seems to be directly related to the capacity of the model, asπ-Net with r = 200 < π-Net with r = 400 < μ-Net < π-Limit.
Table 8: Feature Kernel Regression (FKR) on CIFAR10. We take the best performing μ-net, ∏-net,and π-limit, and evaluate their learned feature kernels on CIFAR10 via kernel regression. We listtheir test accuracies in the middle block. For reference, we also include the NNGP and NTK numbersin the left block, as well as the ImageNet transfer results in the right block.
Table 9: CIFAR10 Compute Time (in Seconds) Comparison. We measure the average trainingtime (in seconds) per epoch for 50 epochs of CIFAR10 using half precision on a NVIDIA V100GPU. We evaluate a μ-Net, ∏-Net, and the ∏-Limit, each of depth 1, 2, 3, and 4; the ∏-Nets and theπ-Limits have r = 400. Because the π-Limit has a linearly increasing compute time per epoch, wealso give an estimate expression for the compute time of the π-Limit in terms of t epochs.
Table 10: Omniglot Compute Time (in Seconds) Comparison. We measure average training time(in seconds) per epoch for 50 epochs of Omniglot using half precision on a NVIDIA V100 GPU. Weevaluate a μ-Net, ∏-Net, and the ∏-Limit, each of depth 1, 2, 3, and 4; the ∏-Nets and the ∏-Limitshave r = 400. Because the π-Limit has a linearly increasing compute time per epoch due to gradientconcatenation, we also give an estimate expression for the compute time of the π-Limit in terms of tepochs. This is not necessary for the 1-hidden-layer case, as there ANIL only trains the first layer,which does gradient accumulation.
