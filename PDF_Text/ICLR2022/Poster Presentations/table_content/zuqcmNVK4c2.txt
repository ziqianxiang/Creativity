Table 1: Classification accuracy (%) and model calibration measured by negative-log-likelihood(NLL) on different datasets. Higher accuracy and lower NLL are better.
Table 2: Adversarial robustness accuracy(%) of different models measured by the classificationaccuracy against adversarial examples generated by PGD with different levels of perturbation ().
Table 3: Average AUROC (%)/Detection Error (%) for different models. The OOD datasets includeCIFAR-10/100, SVHN, STL-10, CelebA, and VOC.
Table 4: Overview of the datasets we use in our experiments. Note that we always keep the first10% of test data from each class as validation set and report the test accuracy on the rest.
Table 5: Classification accuracy of different models using deterministic versus Monte Carlo infer-ence. ’MC’ denotes the accuracy computed by Monte Carlo procedure, and ’ASJ’ is representingthe self-joint models that apply auxiliary data during training.
Table 6: Classification accuracy of stochastic models with different sample sizes. Larger samplesize results in higher accuracy for self-joint models.
Table 7: Independence error for different self-joint models trained on CIFAR-10/100, SVHN, andSTL-10. Low independence errors indicate that self-joint models could learn the independence re-lation successfully. In addition, lower errors on the trained distribution than other datasets showcasethe advantage of learning independency relations over assuming it.
Table 8: Adversarial robustness of different models measured by the classification accuracy(% ±stdev) against adversarial examples generated by PGD for different levels of perturbation.
Table 9: AUROC (%) for different models trained on CIFAR-10/100, SVHN, and STL-10. Notethat each Outlier Exposure model is explicitely trained on a OOD dataset indicated by 什)as OODdata. Each ASJ model is also utilizing the dataset highlighted by (↑) as auxiliary data.
Table 10: Detection Error(%) for different models trained on CIFAR-10/100, SVHN, and STL-10.
