Table 1: Experiments on CIFAR-10 using ResNet-18 and ResNet-50. Models are initialized withKaiming Normal distribution and pruned using SynFlow with pruning ratio 90% at initialization.
Table 2: Experiment results on CIFAR-10 and CIFAR-100 using WideResNet-28 as testbeds. Allmodels are initialized with Kaiming Normal distribution and pruned using SynFlow with pruningratio 70% at initialization. * We perform a range of hyperparameter tuning on SignSGD and reportthe best accuracy we observe.
Table 3: Accuracy on CIFAR-10 with different initial-ization for ResNet-18. All networks are pruned usingSynFlow at initialization with 90% pruning ratio.
Table 4: Results of the iterative multi-shot variant of PaB. We run SynFloW for 2ã€œ5 times, evenlyspread over the first 50 epochs, so that the sparsity ratio linearly increases to a certain level (90%).
Table 5: Accuracy and training cost in BitOPs on CIFAR-10 With varying bit-Width configurationsfor ResNet-18. All netWorks are pruned using SynFloW at initialization With 90% pruning ratio.
