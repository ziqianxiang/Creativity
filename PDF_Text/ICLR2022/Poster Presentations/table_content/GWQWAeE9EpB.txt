Table 1: Comparison with state-of-the-art transformers on machine translation tasks. For a fair com-parison with existing tiny transformer, We follow Lite Transformer (Wu et al., 2020) to scaling downthe hidden size to meet the deployment requirements on mobile settings. For example, Transformeruses a 128 hidden size and 2.8M #Params; #Ops represents Mult-Adds and Entries with ± showsthe average across three independent runs.
Table 2: Comparison with state-of-the-art transformers on abstractive summarization and Languagemodeling tasks. #Ops is calculated by longer sequence with the input length of 100. Entries with ±represents the average across three independent runs.
Table 3: Ablation study of DictFormer techniques on IWLS’14 De-En and WMT’14 En-Fr. Entrieswith ± represents the average across three independent runs.
