Table 1: Median NMSE of PALM over the test set, with the best Î» chosen from the training set,compared to that of LPALM.
Table 2: Comparison of LPALM with state-of-the-art methods in terms of NMSE over AAlgorithm	MNNBU	SNMF-net	DNMF	LISTA	LPALMNMSE(Apred,A*)	0.0962	-0.2700-	0.0466	0.0067	0.0002Generalization (model evaluation)	0.4785	-0.3053-	0.5469	0.0088	0.0009Model retraining	0.3181	0.3073	0.0922	-	-the NMSE for the example of Figure 3, which clearly confirms LPALM superiority over other state-of-art methods. This is due to the fact that LPALM enables to leverage the knowledge containedwithin the whole training set (in particular, in the ground-truth), while the other methods only dealwith single images. Furthermore, DNMF is better than MNNBU and SNMF-net: this is probablyrelated to the fact that it uses a regularized cost function and not only the reconstruction error,which is less likely to lead to spurious solutions. We further benchmarked LPALM with the LISTAalgorithm5. Although LISTA gives good results, it is largely outperformed by LPALM, which isprobably due to the alternating structure of the latter, enabling to better deal with large variationsover the A* matrices. Said differently, LPALM performs much better in the semi-supervised settingthan LISTA.
Table 3: Comparison of the different possible updates within an unrolled PALM architecture. Themedian NMSE over the test set is displayed. The first column corresponds to the LPALM algorithm.
Table 4: Comparison of the estimation quality yielded by LPALM with four different losses.
