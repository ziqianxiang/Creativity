Table 1: Details of our FP-DETR variants. Parameters (Params) and top-1 accuracy (Acc@Top-1)here are calculated on the upstream ImageNet-1k (Deng et al., 2009) classification task.
Table 2: Comparision of FP-DETR with other detection transformers on COCO 2017 val set. Mod-els are categorized as encoder-only (Enc) and encoder-decoder (Enc-Dec) according to the trans-former structure. f indicates the model is pre-trained on ImageNet-21k.
Table 3: Ablations on the task adaptor on COCO 2017 val set.
Table 4: Comparison of model robustness on the COCO-C dataset.
Table 5: Comparison of model generalization on the small-size Cityscapes dataset.
Table 6: The architecture of the lightweight multi-scale tokenizer.
Table 7: Comparison of encoder-decoder transformers and encoder-only transformer for pre-trainingand fine-tuning.
Table 8: Comparision of models fine-tuned from ImageNet-1k and ImageNet-21k pre-trainedweights. f indicates the model is pre-trained on ImageNet-21k.
