Table 1: Experiments on CIFAR10/CIFAR100. “-” means failing to complete the test in our devicedue to memory limit. “*” indicates numbers published in (Wei et al., 2021).
Table 2: Test perplexity of training 1,2,3-layerLSTM on Penn Treebank. Lower is better.
Table 3: Test accuracy (%) for adversarial training.
Table 4: FID score for SN-GAN.
Table 5: The cost and final test accuracy compared with SGDM. The notations “m”,“t/e”, “e”,“t” and “a” are abbreviations of memory, per-epoch time, training epochs, total running time, andaccuracy, respectively. “*” indicates numbers published in (Wei et al., 2021).
Table 6: The cost to achieve comparable results of Adam. The notations “m”,“t/e” and “t” areabbreviations of memory, per-epoch time and total running time, respectively.
Table 7: Test perplexity of training 1,2,3-layer LSTM on Penn Treebank for 200 epochs. Lower isbetter. “*” indicates numbers published in (Wei et al., 2021).
Table 8: Test accuracy (%) for adversarial training.
Table 9:	The effect of αk for RST-AM.
Table 10:	TOP 1 test accuracy (%) w.r.t. epoch, the best TOP1 test accuracy (%), and the cost. Thememory, per-epoch time and total time of SGDM are set as the units. The total time is the time tofirst achieve the accuracy ≥ 75.90%.
Table 11: The BLEU score of training Transformer on IWSLT14.
Table 12: BLEU score evaluated at the 40/45/50-th epoch, and the cost. The memory, per-epochtime and total time of Adam are set as the units. The total time is the time of RST-AM to achievea BLEU score matching the final BLEU of Adam: | BLEU(RST-AM) - BLEU(Adam) | ≤ 0.03,where 0.03 is the standard deviation of the results of Adam.
