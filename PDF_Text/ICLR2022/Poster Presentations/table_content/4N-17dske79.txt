Table 1: Accuracy (mean ± standard deviation) of neural networks and corresponding AL versionson text classification datasets. Tran denotes Transformer, <X>-AL-full denotes full path inference onAL-form network <X>, and <X>-AL-SCi denotes shortcut inference through bridge function bi+1.
Table 2: Accuracy (mean ± standard deviation) and epoch (mean ± standard deviation) to reach thebest accuracy using different RNN models and their AL versions on IMDB and AGNews.
Table 3: Accuracy (mean ± standard deviation) and epoch (mean ± standard deviation) to reach thebest accuracy using different RNN models and their AL versions on SST and DBPedia.
Table 4: Accuracy (mean ± standard deviation) and epoch (mean ± standard deviation) to reach thebest accuracy using different CNN models and their AL versions.
Table 5: AccUracieS of different 山bel noise rates.
Table 6: accuracy is nearly unchanged. Therefore, given enough computing nodes, applying apipelined AL increases training throughput without sacrificing model accuracy.
Table 6: Ablation studies on AG’s News dataset to study why AL works.
Table 7: Bi-LSTM settings.
Table 8: Transformer settings.
Table 9: Layer depth vs. associated loss on LSTM-ALLayer	Associated Loss on DBPedia	Associated Loss on AGNewsEmbedding layer	5.8318 × 10-5	3.3179 × 10-4Layer-1	3.0053 × 10-6	1.2561 × 10-4Layer-2	7.4727 × 10-8		1.3976 × 10-5	Table 10: Layer depth vs. associated loss on Transformer-AL6.9	The Design of the Overcomplete Autoencoders in ALAn autoencoder’s hidden layer usually consists of fewer neurons than the input and output layers.
Table 10: Layer depth vs. associated loss on Transformer-AL6.9	The Design of the Overcomplete Autoencoders in ALAn autoencoder’s hidden layer usually consists of fewer neurons than the input and output layers.
