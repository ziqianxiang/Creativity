Table 1: Comparison between different methods for creating adversarial examples robust to intensitychanges. Column depicts the preturbation radius used by adversarial algorithm A. Columns #Corr,#Img, #Reg show the number of correctly classified images, adversarial images and adversarialregions for the network. For each method columns #VerReg, Time and #Size show the number ofverified regions, average time taken, and number of concrete adversarial examples inside the regions.
Table 2: Comparison between methods for creating adversarial examples robust to geometric changes.
Table 3: The robustness of different examples to L2 smoothing defenses.				Method	8X200	MNIST		CIFAR		ConvSmall	C o n vB i g	ConvSmal l C	o n vB i gBaseline	0.55	0.38	0.59	0.53	0.26PARADE	1.00	1.00	1.00	1.00	1.00Individual attacks mean	0.29	0.16	0.18	0.48	0.25Individual attacks 95% percentile	0.53	0.44	0.51	0.61	0.37precision of verification increases with the number of splits, to allow for fair comparison we selectthe number of baseline splits, so the time taken by the baseline is similar or more than for PARADE.
Table 4: Comparison between regions created with different values for the parameter c introduced inAlgorithm 1 on the MNIST ConvBig Sc(20) T(0,1,0,1) experiment. Columns Under and Over showthe median bounds on the number of concrete adversarial examples contained within the regions. Thecolumns Time and #It depict the average time and number of iterations taken by Algorithm 1.
Table 5: Neural networks used in our experiments. For each, we list the number of layers and neurons,as well as, their type (fully connected (FFN) or convolutional (Conv)).
