Table 1: Test BLEU scores of AR and non-AR systems on the WMT’14 English-to-German(EN→DE) and German-to-English (DE→EN) translation tasks. The number of reranked candi-dates is denoted n. SUNDAE does not use an AR model for re-ranking. We highlight BLEU scoresof the best non-AR systems in bold font. All the entries are ordered based on the EN→DE BLEUscore. * indicates the results of the Transformer baselines implemented by the authors of DisCo.
Table 2: Relative speed gain of SUNDAE overAR Transformer base (greedy decoding) onWMT’14 EN→DE validation set.
Table 3: Inpainting from our model trained on C4 (cherry-picked).
Table 4: Inpainting from our model trained on GitHub (cherry-picked).
Table 5: German-to-English translation process. Since initialization is quite long, we substitute thetrailing tokens with [...]. Tokens changed from previous step are highlighted in gray. The processconverges after 3 steps, while AR would take 10 steps (one for each token in the translation).
Table 6: Test BLEU scores of sUNDAE and imputer (saharia et al., 2020) distilled from ARTransformer-Base model on English-to-German (EN→DE) and German-to-English (DE→EN)translation tasks.
Table 7: Test BLEU without AR distillation (raw). Scores computed with SacreBLEU library aredenoted as BLEU? .
Table 8: Unconditional samples from our model trained on C4 (without cherry-picking). Since C4was crawled from the web, newline symbols are abundant both in the training data and the samples.
Table 9: Unconditional samples from our model SUNDAE trained on EMNLP2017 News (withoutcherry-picking). We also provide samples from ScratchGAN (d’Autume et al., 2019) for compari-son.
