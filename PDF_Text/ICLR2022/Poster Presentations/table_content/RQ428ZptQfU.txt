Table 1: Comparison of the proposed model to the related approaches: semi-supervised clustering(SSC), profile regression (PR) for survival data, survival cluster analysis (SCA), and deep survivalmachines (DSM). Here, t denotes survival time, x denotes explanatory variables, z corresponds tolatent representations, K stands for the number of clusters, and L(∙, ∙) is the likelihood function.
Table 2: Summary of the datasets. Here, N is the total number of data points, D is the number ofexplanatory variables, K is the number of clusters if known. We report the percentage of censoredobservations and whether the cluster sizes are balanced if known.
Table 3: Test set clustering performance on synthetic and survMNIST data. “VAE + Weibull”corresponds to an ablation of VaDeSC w/o the Gaussian mixture prior. “w/o t” corresponds to thecluster assignments made by VaDeSC when the survival time is not given. Averages and standarddeviations are reported across 5 and 10 independent simulations, respectively. Best results are shownin bold, second best - in italic.
Table 4: Test set time-to-event performance on SUPPORT, HGG, and Hemodialysis datasets. Aver-ages and standard deviations are reported across 5 independent train-test splits.
Table 5: Test set time-to-event prediction performance on NSCLC data. For Cox PH and WeibullAFT, radiomics features were extracted using tumour segmentation maps. Averages and standarddeviations are reported across 100 independent train-test splits, stratified by survival time.
Table 6: Cluster-specific statistics for a few demographic and clinical variables (not used duringtraining) from the NSCLC dataset for DSM and VaDeSC. T. Vol. stands for the tumour volume; M1denotes spread of cancer to distant organs and tissues; and ≥ T3 denotes a tumour stage of at least3. Kruskal-Wallis H -test p-values are reported at the significance levels of 0.001, 0.01, and 0.05.
Table 7: An example of an assignmentof MNIST digits to clusters and riskscores under K = 5.
Table 8: Encoder and decoder architectures used for synthetic, survMNIST, and HGG data. tfklstands for tensorflow.keras.layers. encoded_size corresponds to the number of la-tent dimensions, J; and inp_shape is the number of input dimesions, D. For SUrvMNIST, theactivation of the last decoder layer is set to 'sigmoid'.
Table 9: Encoder and decoder architectures used for SUPPORT, FLChain, and Hemodialysis data.
Table 10: Encoder and decoder architectures used for NSCLC data, based on VGG(de-)convolutional blocks (Simonyan & Zisserman, 2015).
Table 11: Hyperparameters used for training the VaDeSC across all datasets. Herein, J denotesthe number of latent dimensions; K is the number of clusters; and k is the shape parameter of theWeibUll distribution. MSE stands for mean squared error; BCE - for binary cross entropy.
Table 12: Training set clustering performance on synthetic and survMNIST data. Averages andstandard deviations are reported across 5 and 10 independent simulations, respectively. Best resultsare shown in bold, second best - in italic. The results are similar to the test-set performance reportedin Table 3.
Table 13: Test set clustering performance on a subsample of the synthetic dataset. Averages andstandard deviations are reported across 5 independent simulations. VaDeSC outperforms the PRbaseline by a large margin.
Table 14: Time-to-event test set performance on FLChain. Averages and standard deviations arereported across 5 train-test splits. All of the methods perform comparably w.r.t. CI. SCA achievesbetter RAE and calibration.
Table 15: The average and the standard deviation of the cluster-specific median survival time com-puted across 20 independent experiments. For each experiment We re-train the model on a differenttrain-test split. The survival time has been scaled betWeen 0 and 1. We also report the percentage ofexperiments that resulted in one or more collapsed clusters.
