Table 1: BLEU scores on the bilingual WMT’14 English-French (En-Fr), WMT’16 English-German(En-De) and WMT’16 English-Romanian (En-Ro) UMT tasks. Note that mBART is incomparableas it was trained on multilingual CC25 data, 45 times large than the bilingual NewsCrawl data usedin our experiments. The numbers in subscripts are the corresponding sacrebleu scores.
Table 2: Performance on the low-resource FLoRes Nepali-English (Ne-En)and Sinhala-English (Si-En) UMT tasks.
Table 3: Tatoeba, Global Accuracy (GlobAcc) and Word Distribution scores (WordDis) of baselineXLM and XLM fine-tuned with SwAV and LAgSwAV. We also include the corresponding down-stream UMT performance (BLEU) of models trained with the respective augmentation data.
Table 4: Comparison of our method LAgSwAV versus supervised LASER (Artetxe & Schwenk,2019a), along with the study on the effect of different components of our method: Rank-basedCross Entropy (Rank-XE) and Dynamic λaug (§3.3) and Filter Suite (§3.2).
Table 5: Agreement BLEU between the targets of the augmentation data mined by different pre-trained models and their translations produced by a fixed baseline MT model. Higher scores meansthe targets are close to the sources according to the baseline model.
Table 7: Translationese analysis on De-En unsu-pervised MT modelsFigure 7: Performances of our method on Ro-Enw.r.t filtering ρ%.
Table 8: Sample English sentences grouped by a K-means clustering method applying on theircluster assignments produced by our LAgSwAV model.
