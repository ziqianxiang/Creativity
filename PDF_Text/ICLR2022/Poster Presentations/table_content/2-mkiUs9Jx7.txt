Table 2: Performance comparison on imbalanced attributesDataset	Metric	InfoGAN	DeLiGAN	DeLiGAN+	ClusterGAN	SCGAN	CD-GAN	PGMGAN	SLOGAN	NMI ↑	0.58±0.07	0.68±0.05	0.65±0.01	0.60±0.02	0.60±0.06	0.59±0.01	0.24±0.02	0.66±0.06FMNIST-5	FID J	5.40±0.14	7.05±0.49	6.33±0.44	5.61±0.17	5.01 ±0.20	9.34±0.56	11.80±0.43	5.29±0.16	ICFID J	43.69±10.84	36.21±3.07	35.41 ±0.79	36.94±5.81	44.48±21.62	39.31±1.18	77.30±8.60	32.46±3.18CIFAR-2 (7:3)	NMI ↑	0.05±0.01	0.00±0.00	0.03±0.03	0.22±0.02	0.00±0.00	0.22±0.03	0.42±0.03	0.69±0.02	FID J	51.30±2.53	131.73±50.98	115.19±17.95	36.62±2.16	45.28±1.81	36.40±1.01	29.76±1.65	29.09±0.73	ICFID J	88.49±6.85	186.31±28.31	173.81±18.29	75.52±4.82	88.58±4.57	76.91±1.07	57.06±3.31	45.83 ±3.03(a) AFHQ (1:2)Figure 3: Generated high-fidelity images from SLOGAN on (a) AFHQ and (b) CelebA-HQ.
Table 3: Effectiveness of U2C lossDataset	Ablation	ICFID ；CIFAR-10	SLOGAN w/o 'u2c	78.26	SLOGAN	71.23MNIST-2	SLOGAN w/o 'u2c	9.43(7:3)	SLOGAN	5.91Synthetic	SLOGAN w/o 'u2c SLOGAN	X ✓Table 4: Effectiveness of implicit reparameterizationDataset	Ablation	Πy=0 (ground-truth: 0.7)	ICFID J	DeLiGAN with 'U2C	0.50	~~60.51CIFAR-2 (7:3)	DeLiGAN with 'U2C and implicit P update	1.00	86.48	SLOGAN	0.69	45.834.3	Evaluation ResultsWe compared SLOGAN with InfoGAN (Chen et al., 2016), DeLiGAN (Gurumurthy et al., 2017),ClusterGAN (Mukherjee et al., 2019), self-conditioned GAN (sCGAN) (Liu et al., 2020), CD-GAN(Pan et al., 2021), and PGMGAN (Armandpour et al., 2021). Following Mukherjee et al. (2019), weused k-means clustering on the encoder outputs of the test data to calculate NMI. DeLiGAN has noencoder network; hence the pre-activation of the penultimate layer of D was used for the clusteringmetrics. For a fair comparison, we also compared DeLiGAN with an encoder network (referredto as DeLiGAN+). The same network architecture and hyperparameters (e.g., learning rate) were
Table 4: Effectiveness of implicit reparameterizationDataset	Ablation	Πy=0 (ground-truth: 0.7)	ICFID J	DeLiGAN with 'U2C	0.50	~~60.51CIFAR-2 (7:3)	DeLiGAN with 'U2C and implicit P update	1.00	86.48	SLOGAN	0.69	45.834.3	Evaluation ResultsWe compared SLOGAN with InfoGAN (Chen et al., 2016), DeLiGAN (Gurumurthy et al., 2017),ClusterGAN (Mukherjee et al., 2019), self-conditioned GAN (sCGAN) (Liu et al., 2020), CD-GAN(Pan et al., 2021), and PGMGAN (Armandpour et al., 2021). Following Mukherjee et al. (2019), weused k-means clustering on the encoder outputs of the test data to calculate NMI. DeLiGAN has noencoder network; hence the pre-activation of the penultimate layer of D was used for the clusteringmetrics. For a fair comparison, we also compared DeLiGAN with an encoder network (referredto as DeLiGAN+). The same network architecture and hyperparameters (e.g., learning rate) wereused across all methods for comparison. Details of the experiments and DeLiGAN+ are presented inAppendices E and D.5, respectively.
Table 5: Quantitative results ofSLOGAN on CelebAImb. ratio	Male (1:1)	Eyeglasses (14:1)NMI ↑	0.65±0.01	0.29±0.07FID ；	5.18±0.20	5.83±0.44ICFID (	11.00±0.66	35.57±5.10πy=0	0.56±0.02	0.82±0.044.4	Ablation StudyU2C loss Table 3 (The complete version is given in Appendix A.3) shows the benefit of U2C losson several datasets. Low-level features (e.g., color) of the CIFAR dataset differ depending on theclass, which enables SLOGAN to function to some extent without U2C loss on CIFAR-10. In theMNIST dataset, the colors of the background (black) and object (white) are the same, and only theshape of objects differs depending on the class. U2C loss played an essential role on MNIST (7:3).
Table 6: Performance comparison on balanced attributesDataset	Metric	WGAN	InfoGAN	DeLiGAN	DeLiGAN+	ClusterGAN	SCGAN	CD-GAN	PGMGAN	SLOGAN	NMI	0.78±0.02	0.90±0.03	0.70±0.05	0.77±0.05	0.81±0.02	0.74±0.06	0.87±0.03	0.16±0.27	0.92±0.00MNIST	FID	3.05±0.20	1.72±0.17	1.92±0.12	2.00±0.16	1.71±0.07	3.06±0.53	2.75±0.04	5.76±1.67	1.67±0.15	ICFID	N/A	5.56±0.71	5.74±0.25	5.64±0.39	5.12±0.07	16.65±2.01	7.03±0.23	53.40±12.49	4.99±0.19	NMI	0.65±0.02	0.64±0.02	0.64±0.03	0.57±0.07	0.61±0.03	0.56±0.01	0.56±0.04	0.47±0.01	0.66±0.01FMNIST	FID	5.74±0.49	5.28±0.12	6.65±0.48	7.23±0.56	6.32±0.25	5.07±0.19	9.05±0.11	9.13±0.28	5.20±0.36	ICFID	N/A	32.18±2.11	34.87±5.29	30.53±8.71	37.20±5.50	26.23±7.10	36.61±0.47	40.00±4.38	23.31±2.77	NMI	0.14±0.02	0.05±0.03	0.15±0.13	0.12±0.12	0.34±0.02	0.00±0.00	0.38±0.01	0.67±0.00	0.78±0.03CIFAR-2	FID	29.54±0.59	58.84±13.11	338.97±70.85	116.95±19.42	36.28±1.12	39.44±1.72	34.45±0.74	29.49±0.51	28.99±0.36	ICFID	N/A	91.97±14.21	361.66±71.28	153.19±17.71	47.02±1.85	71.54±5.41	43.98±1.47	35.67±0.61	35.68±0.51	NMI	0.27±0.05	0.03±0.00	0.06±0.00	0.09±0.04	0.10±0.00	0.01±0.00	0.03±0.01	0.29±0.02	0.34±0.01CIFAR-10	FID	20.56±0.76	81.84±2.27	212.20±4.52	110.51±7.70	61.97±3.69	199.28±57.16	34.13±1.13	31.50±0.73	20.61±0.40	ICFID	N/A	139.20±2.09	305.32±5.05	215.63±11.16	124.27±5.95	262.54±59.29	95.43±3.58	81.25±11.55	71.23±6.76A.2 Performance Comparison on Imalanced AttributesTable 7: Performance comparison on imbalanced attributesDataset	Metric	WGAN	InfoGAN	DeLiGAN	DeLiGAN+	ClusterGAN	SCGAN	CD-GAN	PGMGAN	SLOGANMNIST-2 (7:3)	NMI	0.90±0.03	0.28±0.19	0.90±0.04	0.48 ±0.09	0.27±0.19	0.67±0.11	0.41±0.03	0.79±0.21	0.92±0.05	FID	4.27±0.19	4.92±0.85	4.21±0.84	4.63±2.02	4.25±1.06	4.34±0.73	4.67±1.92	8.90±14.82	4.02±0.86	ICFID	N/A	36.35±10.65	25.34±1.72	26.61±1.49	25.41±1.02	16.47±1.51	26.71±2.47	14.82±9.16	5.91±1.06
Table 7: Performance comparison on imbalanced attributesDataset	Metric	WGAN	InfoGAN	DeLiGAN	DeLiGAN+	ClusterGAN	SCGAN	CD-GAN	PGMGAN	SLOGANMNIST-2 (7:3)	NMI	0.90±0.03	0.28±0.19	0.90±0.04	0.48 ±0.09	0.27±0.19	0.67±0.11	0.41±0.03	0.79±0.21	0.92±0.05	FID	4.27±0.19	4.92±0.85	4.21±0.84	4.63±2.02	4.25±1.06	4.34±0.73	4.67±1.92	8.90±14.82	4.02±0.86	ICFID	N/A	36.35±10.65	25.34±1.72	26.61±1.49	25.41±1.02	16.47±1.51	26.71±2.47	14.82±9.16	5.91±1.06	NMI	0.65±0.00	0.58±0.07	0.68±0.05	0.65±0.01	0.60±0.02	0.60±0.06	0.59±0.01	0.24±0.02	0.66±0.06FMNIST-5	FID	6.55±0.20	5.40±0.14	7.05±0.49	6.33±0.44	5.61±0.17	5.01±0.20	9.34±0.56	11.80±0.43	5.29±0.16	ICFID	N/A	43.69±10.84	36.21±3.07	35.41±0.79	36.94±5.81	44.48±21.62	39.31±1.18	77.30±8.60	32.46±3.1810x_73k	NMI	0.22±0.04	0.42±0.06	0.61±0.01	0.60±0.01	0.66±0.02	0.47±0.02	0.68±0.03	0.33±0.07	0.76±0.02CIFAR-2 (7:3)	NMI	0.09±0.07	0.05±0.01	0.00±0.00	0.03±0.03	0.22±0.02	0.00±0.00	0.22±0.03	0.42±0.03	0.69±0.02	FID	29.16±0.90	51.30±2.53	131.73±50.98	115.19±17.95	36.62±2.16	45.28±1.81	36.40±1.01	29.76±1.65	29.09±0.73	ICFID	N/A	88.49±6.85	186.31±28.31	173.81±18.29	75.52±4.82	88.58±4.57	76.91±1.07	57.06±3.31	45.83±3.03CIFAR-2 (9:1)	NMI	0.04±0.04	0.00±0.00	0.02±0.02	0.09±0.11	0.02±0.01	0.00±0.00	0.05±0.03	0.16±0.03	0.38±0.01	FID	29.37±0.53	60.76±8.97	129.50±25.33	139.75±47.13	41.69±0.83	50.45±1.56	38.15±2.70	30.23±1.31	29.47±1.53	ICFID	N/A	138.24±10.23	205.26±10.93	196.00±17.86	133.31±2.03	123.35±6.56	128.46±3.03	101.68±3.87	86.75±1.87A.3 Ablation StudyTable 8 shoWs the ablation study on SLOGAN trained With CIFAR-2 (7:3). πy=0 and πy=1 representthe mixing coefficients of the latent components that correspond to the frogs and planes, respectively,and the ground-truth of πy=0 is 0.7.
Table 8: Ablation study on CIFAR-2 (7:3)Ablation	πy=0 (ground-truth: 0.7)	ICFID JFactor analysis SLOGAN without μ, Σ, P updates, 'u2c	0.50	84.44SLOGAN without μ, Σ, P updates	0.50	77.32SLOGAN without μ update	0.52	73.79SLOGAN without P update	0.50	63.09SLOGAN without Σ update	0.69	48.34SLOGAN without 'u2c	0.66	48.82Implicit reparameterization DeLiGAN with 'u2c	0.50	60.51DeLiGAN with 'u2c and implicit P update	1.00	86.48Loss for P update SLOGAN with 'u2c for P update	0.62	52.67SimCLR analysis SLOGAN without SimCLR	0.66	49.25SLOGAN without SimCLR on real data only	0.67	48.41SLOGAN without SimCLR on both real and fake data	0.69	47.93Attribute manipulation SLOGAN with probe data	0.71	44.97SLOGAN with probe data and mixup	0.70	44.26SLOGAN	0.69	45.83Table 9: Effectiveness of U2C lossDataset	Ablation NMI ↑ FID J ICFID JMNIST-2 (7:3)	SLOGAN w/o 'u2c	0.25	4.62	9.43 SLOGAN	0.92	4.02	5.91
Table 9: Effectiveness of U2C lossDataset	Ablation NMI ↑ FID J ICFID JMNIST-2 (7:3)	SLOGAN w/o 'u2c	0.25	4.62	9.43 SLOGAN	0.92	4.02	5.91FMNIST-5	SLOGAN w/o 'u2c	0.14	5.27	43.15 SLOGAN	0.66	5.29	32.46CIFAR-2	SLOGAN w/o 'u2c	0.01	29.18	41.72 SLOGAN	0.78	28.99	35.68CIFAR-2 (7:3)	SLOGAN w/o 'u2c	0.08	30.34	48.82 SLOGAN	0.69	29.09	45.83CIFAR-10	SLOGAN w/o 'u2c	0.08	20.91	78.26 SLOGAN	0.34	20.61	71.23decomposed into entropy and conditional entropy as follows:'U2c(z) ≈ -I(C; Xg) = H(C∣Xg) - H(C)	(12)The conditional entropy term reduces the uncertainty of the component from which the generateddata are obtained. The entropy term promotes that component IDs are uniformly distributed. In termsof ρ update, the entropy term H(C) drives p(C) toward a discrete uniform distribution. Therefore,using 'u2c for learning P pulls π to a discrete uniform distribution and can hinder the learned πfrom accurately estimating the imbalance ratio inherent in the data. In the 9th row of Table 8, weobserved that the unsupervised conditional generation performance was undermined and the estimatedimbalance ratio (∏y=o) was learned closer to a discrete uniform distribution (0.5) when 'u2c wasused for ρ update.
Table 10: Ablation study on feature scaleS	0.5	1	2	4	8ICFID J	14.18	17.03	6.65	5.91	33.98Intuitively, increasing the feature scale S makes the samples generated from the same componentcloser to each other in the embedding space. From these results, we observed that the optimal choiceof the temperature factor enhances the discriminative power of U2C loss.
Table 11: SLOGAN architecture used for the synthetic datasetG	D	Ez ∈R64	X ∈ R2	x ∈ R2Linear 128 +BN+ReLU	Linear 128 + LReLU	Linear 128 + SN + LReLULinear 128 +BN+ReLU	Linear 128 + LReLU	Linear 128 + SN + LReLULinear 2 + Tanh	Linear 1	Linear 64 + SNE.3 MNIST and Fashion-MNIST DatasetsThe MNIST dataset (LeCun et al., 1998) consists of handWritten digits, and the Fashion-MNIST(FMNIST) dataset (Xiao et al., 2017) is comprised of fashion products. Both the MNIST and Fashion-MNIST datasets have 60,000 training and 10,000 test 28×28 grayscale images. Each pixel Was scaledto a range of 0-1. The datasets consist of 10 classes, and the number of data points per class isbalanced. Table 12 shoWs the netWork architectures of SLOGAN used for the MNIST and FMNISTdatasets. Conv k × k, s, n denotes a convolutional netWork With n feature maps, filter size k × k,and stride s. Deconv k × k, s, n denotes a deconvolutional netWork With n feature maps, filter sizek × k, and stride s. For the MNIST dataset, We set λ = 10, η = 0.0001, γ = 0.002, s = 8, andm = 0.5. For MNIST-2, We set λ = 4, η = 0.0001, γ = 0.002, s = 4, and m = 0.5. For theFMNIST dataset, We setλ = 10, η = 0.0001, γ = 0.001, s = 1, andm = 0. For FMNIST-5, We setλ = 1, η = 0.0002, γ = 0.004, s = 4, and m = 0.5.
Table 15: SLOGAN architecture used for the CelebA datasetG	D	Ez∈R128	X ∈ R64×64×3	x ∈ R64×64×3Linear 8192 + Reshape 8, 8, 128	Resblock Down	Resblock DownResblock Up	Resblock Down	Resblock DownResblock Up	Resblock Down	Resblock DownResblock Up	Resblock	ResblockBN + ReLU	ReLU + GlobalAvgPool	ReLU + GlobalAvgPoolConv 3×3, 1, 3 + Tanh	Linear 1	Linear 128E.7 CelebA-HQ DatasetThe CelebA-HQ dataset (Karras et al., 2017) consists of 30,000 face attributes. We resized eachimage to 128×128 and 256×256 pixels, and scaled it to a range between -1 and 1. The imbalanceratio of male used in the experiment was 1.7:1. We used StyleGAN2 (Karras et al., 2020) architecturewith DiffAugment3 and applied implicit reparameterization to the input space of the mapping network.
