Table 1: Accuracy of fine-tuning for downstream tasks with RoBERTa-Large (in %). Our resultsachieve accuracy comparable to full fine-tuning non-privately, while simultaneously guaranteeingdifferential privacy. We choose δ =1e-5 for SST-2 and QNLI and δ =1e-6 for MNLI and QQP dueto their dataset sizes. Implementation details are in Section 4.1.
Table 2: Memory and speed comparison for RoBERTa-Large. The rank is chosen as r = 16 forRGP and LoRA. The speed is measured by the wall-clock time for training one epoch of the SST-2dataset on a single Tesla V100 GPU with gradient accumulation for batch size 2000.
Table 3: Accuracy for fine-tuning with RoBERTa-Base (in %). The privacy parameters are ε = 6.7,and δ =1e-5 for SST-2 and QNLI and 1e-6 for MNLI and QQP. Bold indicates the best accuracywith DP. Numbers for non-private fine-tuning are from Liu et al. (2019) and Hu et al. (2021).
Table 4: Accuracy for fine-tuning with RoBERTa-Large (in %). The privacy parameters are ε = 6.7,and δ =1e-5 for SST-2 and QNLI and δ =1e-6 for MNLI and QQP. Bold indicates the best accuracywith DP. Numbers for non-private fine-tuning are from Liu et al. (2019) and Hu et al. (2021).
Table 5: Metrics on the E2E NLG task (ε = 5.4, δ =1e-5). Non-DP results from Hu et al. (2021).
Table 6: Test accuracy for fine-tuning RoBERTa-Large with different privacy parameters. The num-ber of training samples is denoted by n. The values of σ are noise multipliers. Numbers in thebrackets are the changes compared to the results in Table 4 (ε = 6.7, δ = Θ(1∕n)).
Table 7: Accuracy for fine-tuning downstream tasks with RoBERTa-Base (in %). Experiments arerun with full-precision. We also scale up the batch size according to the dataset size compared toSST-2. The privacy parameters are ε = 6.7, and δ =1e-5 for SST-2 and QNLI and 1e-6 for MNLIand QQP.
Table 8: Accuracy for fine-tuning downstream tasks with RoBERTa-Large (in %). Experiments arerun with full-precision. We also scale up the batch size according to the dataset size compared toSST-2. The privacy parameters are ε = 6.7, and δ =1e-5 for SST-2 and QNLI and δ =1e-6 forMNLI and QQP.
Table 9: Non-private metrics on the E2E NLG task, using full fine-tuning.
Table 10: Metrics on the E2E NLG task. Non-DP results from Hu et al. (2021), except for GPT-2-XL, which was not reported in the paper. We ran GPT-2-XL with hyperparameters presented in Huet al. (2021). Bold indicates the best accuracy with DP. DP parameters are (ε = 6.0, δ = 1e-5). Valperp stands for validation perplexity.
Table 11: Metrics on the E2E NLG task. Bold indicates the best accuracy with DP. DP parameterssatisfy (ε = 3.0, δ = 1e-5), (ε = 3.4, δ = 1/10n), (ε = 3.9, δ = 1/100n) and (ε = 4.5, δ =1/1000n). Val perp stands for validation perplexity.
Table 12: Metrics on the DART dataset. Non-DP results from Hu et al. (2021), except for GPT-2-XL, which was not reported in the paper. We ran GPT-2-XL with hyperparameters presented in Huet al. (2021). Bold indicates the best accuracy with DP. DP parameters are (ε = 6.8, δ = 1e-5). Valperp stands for validation perplexity. Unlike all other metrics, the lower the TER metric is the betterfor the performance of the model.
