Table 1: Results on VQA v2.0. "t” marks resultsfrom (Jiang et al., 2020). CLIP visual encodersoutperform all baselines, including strong visualencoders pre-trained with in-domain detection data(VG-* and BUTD-*).
Table 2: Image Captioning results. B@4, M, C, andS are BLUE-4, METEOR, CIDEr and SPICE metric,respectively. “*” marks results from Luo (2020).
Table 3: Unseen test results for Room-to-Room(R2R) dataset. ‘SR’ and ‘SPL’ are Success Rateand Success rate normalized by Path Length.
Table 4: Results of Room-to-Room (R2R) and Room-across-Room (RxR) datasets with originalResNet features and CLIP feature variants. ‘BT-Agent’ is the agent trained with back translation(BT). ‘SR’ is Success Rate. ‘SPL’ and ‘nDTW’ are the main metrics for R2R and RxR, respectively.
Table 5: Unseen test results for Room-across-Room(RxR) dataset under mono-lingual setup. ‘SR’ and‘nDTW’ are Success Rate and normalized DynamicTime Warping.
Table 6: Evaluation results on three vision-and-language tasks. Our model with CLIP-Res50outperforms most BUTD-based models. Our model with CLIP-Res50x4 sets a new state-of-the-arton VQA and SNLI-VE. It surpasses VinVL, which is a scaled-up version of BUTD and undergoesmore intensive V&L pre-training than ours.
Table 7: Zero-shot performance of CLIP on VQAv2.0 mini-eval, “PE” denotes we follow simi-lar prompt engineering as suggested in CLIP pa-per.
Table 8: The importance of V&L pre-training (eval-uated on VQA test-dev). All three models benefitfrom V&L pre-traibing significantly.
Table 9: Comparison between grid features, CLIP features, and ImageNet-trained features on theR2R dataset. ‘SR’ and ‘SPL’ are success rate and success rate weighted by path length.
Table 10: The performance of finetuned CLIP text encoder and visual encoder without VLP on VQA.
