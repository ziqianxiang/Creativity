Table 1: Summary of Optimizee Specifications. Dataset, architecture, and training setting specifica-tions are given in sections B.1, B.2, and B.3 respectively.
Table 2: Convolutional Optimizee Architectures. Note that for the 28-layer ResNet, each residualblock consists of 2 layers, adding up to 28 convolutional layers in total.
Table 3: Optimizer pool update rules; allupdates include an additional learning ratehyperparameter.
Table 4: Amalgamation and baseline training times.
Table 5: Evaluation stability of analytical and amalgamated optimizers; all optimizers are amalga-mated from the small pool, except for Optimal Choice Amalgamation on the large pool, which isabbreviated as “large”. A dash indicates optimizer-problem pairs where optimization diverged.
Table 6: Meta-Stability with varyingmagnitudes of Input PerturbationMagnitude	Meta-Stabilityσ = 0	0!04σ = 10-2	0.485σ = 10-1	1.637E.5 Baselines with Random PerturbationOur perturbation methods can be applied to any gradient-based optimizer meta-training method,including all of our baselines. To demonstrate this application, we trained 8 RNNProp replicates withGaussian perturbations with magnitude 1 × 10-4; all other settings were identical to the RNNPropbaseline. With perturbations, the RNNProp baseline is significantly improved, though not enough tomatch the performance of our amalgamation method.
