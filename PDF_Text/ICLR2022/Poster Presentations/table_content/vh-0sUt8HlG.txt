Table 2: Segmentation w/ DeepLabv3.
Table 3: ViTs are slower than CNNs.
Table 4: MobileViT architecture. Here, d represents dimensionality of the input to the transformerlayer in MobileViT block (Figure 1b). By default, in MobileViT block, we set kernel size n as threeand spatial dimensions of patch (height h and width w) in MobileViT block as two.
Table 5: Multi-scale sampler is generic. All models are trained with basic augmentation on theImageNet-Ik. * Results are with exponential moving average.
Table 6: Impact of patch sizes. Here, the patch sizes are for spatial levels at 32 × 32, 16 × 16, and8 × 8, respectively. Also, results are shown for MobileViT-S model on the ImageNet-1k dataset.
Table 7: Effect of label smoothing (LS) and exponential moving average (EMA) on the perfor-mance of MobileViT-S on the ImageNet-1k dataset. First row results are with cross-entropy.
Table 8: Comparison between MobileNetv2 and MobileViT in terms of maximum memory (inkb) that needs to be materialized at each spatial resolution in the network. The top-1 accuracyis measured on the ImageNet-1k validation set. Here, OS (output stride) is the ratio of spatialdimensions of the input to the feature map.
Table 9: Comparison of different ViT-based networks. The performance of MobileViT-XS modelis reported at two different patch-size settings. See §A for details.
Table 10: MobileViT vs. MobileNetv2 on different tasks. The FLOPs and inference time in (a),(b) and (c) are measured at 224 × 224, 320 × 320, and 512 × 512 respectively with an exceptionto MobileViT-XS model in (a) which uses 256 × 256 as an input resolution for measuring inferencetime on iPhone 12 neural engine. Here, the performance of MobileViT-XS models is reported at twodifferent patch-size settings. See §A for details.
Table 11: Inference time on different devices. The run time of MobileViT is measured at 256 × 256while for other networks, it is measured at 224 × 224. For GPU, inference time is measured for abatch of 32 images while for other devices, We use a batch size of one. Here, f represents that Mo-bileViT model uses PyTorch’s Unfold and Fold operations. Also, patch sizes for MobileViT modelat an output stride of 8, 16, and 32 are set to two.
Table 12: Semantic segmentation on the MS-COCO validation set. MobileNetv3-Large resultsare from official torchvision segmentation models (PyTorch, 2021).
