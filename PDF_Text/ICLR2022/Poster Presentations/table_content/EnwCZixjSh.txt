Table 1: A summary of the datasets.
Table 3: Summary of each metric’s performance across experiments. Column headings indicate the experimentacross which results are aggregated. NN-based metrics are aggregated across all GIN configurations usingrandom networks unless otherwise stated. Computational efficiency is taken to be the maximum recorded timereported in Figure 10. Values reported are the mean ± std. error, and the average in the final row is takenstrictly across the NN-based metrics. Cells are colored if the results can be interpreted objectively for the givenexperiment (i.e., experiments that use rank correlation to measure performance).
Table 2: Evaluation of different GGMs at various percentagesof total epochs trained on the Grid dataset. Cells are coloredaccording to their rank in a given column.
Table 4: Results across fidelity, diversity, and sample efficiency experiments and all datasets groupedby GIN configuration. N is the node embedding size, P is the number of graph propagation rounds,and G is the dimensionality of the obtained graph embedding. Configurations are sorted by theirmean rank correlation across fidelity and diversity experiments.
Table 5: Results across fidelity, diversity, and sample efficiency experiments and all datasets groupedby GIN configuration and metric used. N is the node embedding size, P is the number of graphpropagation rounds, and G is the dimensionality of the obtained graph embedding. Configurationsare sorted by their mean rank correlation across fidelity and diversity experiments and the top 20results are shown.
Table 6: The performance of each evaluation metric across all experiments grouped by the datasetused. Part A.
Table 7: The performance of each evaluation metric across all experiments grouped by the datasetused. Part B.
Table 8: Comparing the performance of metrics on the mixing experiment when the mixed graphsare random E-R graphs or graphs generated by GRAN (Liao et al., 2019).
Table 9: The maximum recorded memory usage in GB of each metric during each of the computa-tional efficiency experiments.
Table 10: Comparing various GNN architectures on our experiments using all datasets. We presentonly the final two recommended metrics to keep this presentation concise. All models are randomlyinitialized and aggregated across the same underlying model architectures (number of propagationrounds, node embedding size) used in the main article.
Table 11: Measuring the impact of our σ selection process on the MMD RBF metric. While using astatic σ does result in a decrease in performance, it is still more expressive than pre-existing metricsthat rely on the RBF kernel (You et al., 2018).
Table 12: Evaluation of different GGMs at various percentages of total epochs trained on the Grid,Lobster, and Proteins datasets. Models are evaluated using three of the strongest NN-based metricsand classical metrics (You et al., 2018). All NN-based metrics are averaged across 10 different GINswith the strongest configuration. Cells are colored according to their rank in a given column. 50/50split represents the metric computed using a random 50/50 split of the dataset and represents thetheoretical ideal score for each metric.
Table 13: Evaluating GGMs at various stages of training on the Proteins dataset using all 20 GINarchitectures tested in the main-body.
Table 14: Evaluating GGMs at various stages of training on the Lobster dataset using all 20 GINarchitectures tested in the main-body.
Table 15: Evaluating GGMs at various stages of training on the Grid dataset using all 20 GINarchitectures tested in the main-body.
