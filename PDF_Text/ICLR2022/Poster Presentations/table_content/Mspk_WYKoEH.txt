Table 1: SimUlation dataset Performance: GNN-AK(+ ) boosts base GNN across tasks, empiricallyverifying expressiveness lift. (ACC: accUracy, MAE: mean absolUte error, OOM: oUt of memory)Method	EXP (ACC)	CoUnting SUbstrUctUres (MAE)	Graph Properties (log10(MAE)) SR25 		 (ACC) Triangle Tailed Tri. Star 4-Cycle IsConnected Diameter RadiUsGCN	50% GCN-AK+	100%	6.67%^^0.4186	0.3248	0.1798~~0.2822	-1.7057	-2.4705~^-3.9316 6.67%	0.0137	0.0134	0.0174 0.0183	-2.6705	-3.9102 -5.1136GIN	50% GIN-AK	100% GIN-AK+	100%	6.67%^^0.3569	0.2373	0.0224^^0.2185	-1.9239	-3.3079^^-4.7584 6.67%	0.0934	0.0751	0.0168	0.0726	-1.9934	-3.7573	-5.0100 6.67%	0.0123	0.0112	0.0150 0.0126	-2.7513	-3.9687	-5.1846PNA*	50% PNA*-AK+	100%	6.67%	0.3532	0.2648	0.1278	0.2430	-1.9395	-3.4382	-4.9470 6.67%	0.0118	0.0138	0.0166	0.0132	-2.6189	-3.9011	-5.2026PPGN	100% PPGN-AK+ 100%	6.67%^^0.0089	0.0096	0.0148~~0.0090	-1.9804	-3.6147~^-5.0878 100% OOM	OOM	OOM OOM	OOM	OOM	OOMTable 1 Presents the resUlts on simUlation datasets. To save sPace we Present GNN-AK+ with dif-ferent base models bUt only one one version of GNN-AK: GIN-AK. All GNN-AK(+) variants PerformPerfectly on EXP, while only PPGN alone do so PrevioUsly. Moreover, PPGN-AK+ reaches PerfectaccUracy on SR25, while PPGN fails. Similarly, GNN-AK(+ ) consistently boosts all MPNNs forsUbstrUctUre and graPh ProPerty Prediction (PPGN-AK+ is OOM as it is qUadratic in inPUt size).
Table 2: PPGN-AK exPressiveness on SR25.
Table 3: Real-world dataset performance: GNN-AK+ achieves SOTA performance for ZINC-12K,CIFAR10, and PATTERN. (OOM: out of memory, -: missing values from literature)Method	ZINC-12K (MAE)	CIFAR10 (ACC)	PATTERN (ACC)	MolHIV (ROC)	MolPCBA (AP)GatedGCN	0.363 ± 0.009	69.37 ± 0.48	^^84.480 ± 0.122	-	-HIMP	0.151 ± 0.006	-	-	0.7880 ± 0.0082	-PNA	0.188 ± 0.004	70.47 ± 0.72	86.567 ± 0.075	0.7905 ± 0.0132	0.2838 ± 0.0035DGN	0.168 ± 0.003	72.84 ± 0.42	86.680 ± 0.034	0.7970 ± 0.0097	0.2885 ± 0.0030GSN	0.115 ± 0.012	-	-	0.7799 ± 0.0100	-CIN	0.079 ± 0.006	-	-	0.8094 ± 0.0057	-GCN	0.321 ± 0.009	58.39 ± 0.73	85.602 ± 0.046	0.7422 ± 0.0175	0.2385 ± 0.0019GCN-AK+	0.127 ± 0.004	72.70 ± 0.29	86.887 ± 0.009	0.7928 ± 0.0101	0.2846 ± 0.0002GIN	0.163 ± 0.004	59.82 ± 0.33	85.732 ± 0.023	0.7881 ± 0.0119	0.2682 ± 0.0006GIN-AK	0.094 ± 0.005	67.51 ± 0.21	86.803 ± 0.044	0.7829 ± 0.0121	0.2740 ± 0.0032GIN-AK+	0.080 ± 0.001	72.19 ± 0.13	86.850 ± 0.057	0.7961 ± 0.0119	0.2930 ± 0.0044PNA*	0.140 ± 0.006	73.11 ± 0.11	85.441 ± 0.009	0.7905 ± 0.0102	0.2737 ± 0.0009PNA*-AK+	0.085 ± 0.003	OOM	OOM	0.7880 ± 0.0153	0.2885 ± 0.0006GCN-AK+-S	0.127 ± 0.001	71.93 ± 0.47	86.805 ± 0.046	0.7825 ± 0.0098	0.2840 ± 0.0036GIN-AK+-S	0.083 ± 0.001	72.39 ± 0.38	86.811 ± 0.013	0.7822 ± 0.0075	0.2916 ± 0.0029PNA*-AK+-S	0.082 ± 0.000	74.79 ± 0.18	86.676 ± 0.022	0.7821 ± 0.0143	0.2880 ± 0.00126.3	Scaling up by Subsampling
Table 4: Resource analysis of SubgraphDrop.
Table 5: Dataset statistics.
Table 6: Effect of various k-egonet size.	Table 7: Effect of various encodingsAblation of GIN-AK+	ZINC-12K	CIFAR10Full	0.080 ± 0.001	72.19 ± 0.13w/o Subgraph encoding	0.086 ± 0.001	67.76 ± 0.29w/o Centroid encoding	0.084 ± 0.003	72.20 ± 0.96w/o Context encoding	0.088 ± 0.003	69.25 ± 0.30w/o Distance-to-Centroid	0.085 ± 0.001	71.91 ± 0.22Next Table 7 illustrates the added benefit of various node encodings. Compared to the full design,eliminating any of the subgraph, centroid, or context encodings (Eq.s (3)-(5)) yields notably inferiorresults. Encoding without distance awareness is also subpar. These justify the design choices in ourframework and verify the practical benefits of our design.
Table 8: Study GNN-AK without context encoding (Ctx) and without distance-to-centroid (D2C).
Table 9: Study the effect of base model’s number of layers while keeping total number of layers inGNN-AK fixed. Different effect is observed for GNN-AK and GNN-AK without D2C.
Table 10: Results on TU Datasets. First section contains methods of graph kernels, second sectionhas GNNs, and third is the method in Bodnar et al. (2021a). The top two are highlighted by First,Second, Third.
