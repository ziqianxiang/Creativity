Table 1: Ablative study of the different components of our method. Overall 1st/2nd in black/blue.									Flavors of ARTEMIS	FIQ val split			Shoes			CIRR val split			R@10	R@50	I	. rank	R@10	R@50	m. rank	R@5	R@50	Rs@1Image only hr | ti	4.86	12.01	917.11	28.47	53.13	41.67	30.10	75.75	20.84Text only hm | ti	15.55	36.11	121.44	13.50	31.46	154.00	21.93	65.71	38.28Late fusion hr + m | ti	25.69	50.14	51.78	48.95	75.62	11.33	30.94	78.04	21.65IS module only	6.18	16.42	449.56	32.63	57.41	31.33	32.28	77.50	21.16EM module only	15.61	36.43	113.89	13.72	32.39	146.67	29.71	72.24	43.48Full ARTEMIS	26.05	50.29	52.89	53.11	79.31	8.67	48.95	89.19	41.42(shirt). The text modifier is composed of two relative captions produced by two different humanannotators, exposed to the same reference-target image pair. The Shoes dataset (Guo et al., 2018)is extracted from the Attribute Discovery Dataset (Berg et al., 2010). It consists of 10k training im-ages structured in 9k training triplets, and 4.7k test images including 1.7k test queries. The recentlyreleased CIRR dataset (Liu et al., 2021) is composed of 36k pairs of open-domain images, arrangedin a 80%-10%-10% split between the train/validation/test. The annotation process is such that themodifying text should only be relevant to one image pair, and irrelevant to any other image pairscontaining the same reference image.
Table 2: Fashion IQ, official validation set. We report the challenge metric (CM) and individualR@K scores. ↑ means our re-implementation. ? denotes the use of additional side information(e.g. extra captions from other datasets) at train time. Unless mentioned otherwise, each method usesResnet50 and LSTM as visual and textual backbones, respectively. Overall 1st/2nd in black/blueMethod	CM	R@10				R@50					Dress	Shirt	Toptee	Mean	Dress	Shirt	Toptee	MeanJVSM? (Chen & Bazzani, 2020)	19.27	10.70	12.00	13.00	11.90	25.90	27.10	26.90	26.63ComposeAE (Anwaar et al., 2021)	20.60	-	-	-	11.80	-	-	-	29.40TCIR (Chawla et al., 2021)	29.51	19.33	14.47	19.73	17.84	43.52	35.47	44.56	41.18CIRPLANT (Liu et al., 2021)	25.17	14.38	13.64	16.44	14.82	34.66	33.56	38.34	35.52CIRPLANT? (Liu et al., 2021)	30.20	17.45	17.53	21.64	18.87	40.41	38.81	45.38	41.53TIRGt (RN50 + LSTM)	36.16±0.22	24.19±0.04	20.13±0.54	26.05±0.95	23.46±0.31	50.42±1.19	43.56±0.47	52.57±0.33	48.85±0.27TIRGt (RN50 + BiGRU)	35.32±0.74	23.80±1.55	19.90±0.62	25.82±0.73	23.17±0.70	48.64±1.22	42.14±1.65	51.64±0.30	47.48±0.83VAL (Chen et al., 2020)	33.82	21.12	21.03	25.64	22.60	42.19	43.44	49.49	45.04VAL? (Chen et al., 2020)	35.38	22.53	22.38	27.53	24.15	44.00	44.15	51.68	46.61CoSMo (Lee et al., 2021)	31.26	21.39	16.90	21.32	19.87	44.45	37.49	46.02	42.65ARTEMIS (RN18 + LSTM) (ours)	34.70±0.10	25.23±0.36	20.35±0.56	23.36±o.22	22.98±0.07	48.64±0.61	43.67±0.94	46.97±0.47	46.43±0.13ARTEMIS (RN50 + LSTM) (ours)	36.51±0.46	27.34±0.44	21.05±1.89	24.91±0.57	24.43±0.61	51.71±0.78	44.18±0.39	49.87±0.56	48.59±0.40ARTEMIS (RN18 + BiGRU) (ours)	34.75±0.03	24.84±0.06	20.40±0.11	23.63 ±0.05	22.95±0.02	49.00±0.15	43.22±0.04	47.39±0.58	46.54±0.04ARTEMIS (RN50 + BiGRU) (ours)	38.17±0.35	27.16±0.52	21.78±0.26	29.20±0.69	26.05±0.33	52.40±0.20	43.64±1.01	54.83±0.30	50.29±0.40
Table 3: Fashion IQ, test set. We report the challenge metric (CM) and individual R@K scores.
Table 4: Shoes dataset. * means our re-implementation. ? denotes the use of additional information(e.g. extra-captions, attributes) at training time. All reported methods use a ResNet50 as backbone.
Table 5: CIRR dataset, test set. Recall@K and Recallsubset@K (according to Liu et al. (2021),Recallsubset@1 best assess fine-grained reasoning ability). Gray background is not directly compa-rable to our results, since it relies on a model pre-trained on a very large set of image-caption pairs.
Table 6: Fashion200K dataset results. * means results of our re-implementation. ^ denotes resultspublished by Vo et al. (2019). ? denotes the use of additional information (e.g. extra captions) attraining time. For our results, we report the average of 3 runs. Overall 1st/2nd/3rd in black/blue/red.
Table 7: Efficiency comparison. All models use Resnet50 as image encoder and LSTM as textencoder. In red, we give the added values compared to our reference point which only considers theencoders. Latency is computed over the 12k queries of the FashionIQ validation set.
