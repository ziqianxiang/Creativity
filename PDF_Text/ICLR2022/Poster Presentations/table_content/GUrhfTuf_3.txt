Table 1: Single model results for vision-language pretraining methods on popular VL banchmarks.
Table 2: Image captioning results on CoCo Karpathy-test split and NoCaps validation split. For No-Caps, {In, Near, Out} refer to in-domain, near-domain and out-of-domain respectively. * indicatesCider optimization. Model references: aAnderson et al. (2018) bHuang et al. (2019) cCornia et al.
Table 3: Zero-shot cross-modality transfer results on SNLI-VE and Multi30k. For SNLI-VE, thezero-shot model is finetuned on three source datasets: text-only SNLI-VE (Xie et al., 2019), SNLI(Bowman et al., 2015), and MNLI (Williams et al., 2017). For Multi30k, the model is finetuned ontext-only Multi30k data. Model reference: a(Specia et al., 2016).
Table 4: Comparison of discriminative and generative VQA methods. “Dev” refers to standardvqa-score on the VQA validation split. “Karpathy-test” is the setup used in Cho et al. (2021) forevaluation on the Karpathy split with rare answers. “Partial Train” refers to train the model only onpartial training data which contain subset of all candidate answers.
Table 5: Linear evaluation on ImageNet classifi-cation, compared to state-of-the-art representationlearning methods.
Table 6: Ablation study on VQA. “w/ LM” and“w/ span corruption” denote replacing the pro-posed PrefixLM loss with a different pretrainingobjective. “Image2Text” and “Text2Text” refer tothe noisy image-text data and the text-only dataused for pretraining. “conv blks” denotes numberof ResNet blocks.
Table 7: Text-only task performance on the GLUE benchmark (Dev set). Results for BERT andother VLP methods are obtained from Iki & Aizawa (2021). The overall best result is bolded whileUnderline signifies the best VLP model.
