Table 1: Accuracy of IGLU compared to SOTA algorithms. The metric is ROC-AUC for Proteinsand Micro-F1 for the others. IGLU is the only method with accuracy within 0.2% of the best accuracyon each dataset with significant SPeedUPs in training across datasets. On PPI-Large, IGLU is 〜8×faster than VR-GCN, the most accurate baseline. * denotes speedup in initial convergence based on ahigh validation score of 0.955. - denotes no absolute gain. k denotes a runtime error.
Table 2: Accuracy vs different update frequency on PPI-Large.
Table 3: OGBN-Proteins: Overheads of different methods. IGLU does not have any recurringoverhead while VRGCN, GraphSAGE and GraphSAINT all suffer from heavy recurring overheads.
Table 4: Reddit: Overheads of different methods. IGLU and GraPhSAINT do not have anyrecurring overhead for this dataset while VRGCN and GraPhSAGE incur heavy recurring overheads.
Table 5: URL’s and commit numbers to run baseline codesMethodURLCommitGCN	github.com/williamleif/GraphSAGE	a0fdefGraphSAGE	github.com/williamleif/GraphSAGE	a0fdefVRGCN	github.com/thu-ml/stochastic_gcn	da7b78ClusterGCN	github.com/google-research/google-research/tree/master/cluster_gcn	0c1bbe5AS-GCN	github.com/huangwb/AS-GCN	5436ecdL2-GCN	github.com/VITA-Group/L2-GCN	687fbaeMVS-GNN	github.com/CongWeilin/mvs_gcn	a29c2c5LADIES	github.com/acbull/LADIES	c10b526FastGCN	https://github.com/matenure/FastGCN	b8e6e64SIGN	https://github.com/twitter-research/sign	42a230cPPRGo	https://github.com/TUM-DAML/pprgo_pytorch	c92c32eBanditSampler	https://github.com/xavierzw/gnn-bs	a2415a9Discussion on Reddit: On the Reddit dataset, IGLU incurs UPto 〜 2.1 × less overhead thanVRGCN and upto 〜4.5 × less overhead than GraphSAGE. However, IGLU incurs marginally higheroverhead (〜1.4×) than GraphSAINT. This can be attributed to the non-optimized minibatch creationroutine currently used by IGLU compared to a highly optimized and parallelized implementation in
Table 6: Additional Memory Overheads incurred by IGLU on Large datasetsDataset	# of Train Nodes	Embedding Dimensions	Number of Layers	Memory Per Layer	Total GPU MemoryReddit	155k	128	二	2	75MB	150MBProteins	90k	256	3	85MB	260MBMillion Sized Graph	1M	256	3	0.95GB	2.86GBWe observe that IGLU enjoys significant speedups and improvements in training cost across datasetsas compared to the baselines as a result of using stale variables. Additionally, since IGLU requirestraining just a single layer at a time, there is scope for further reduction in memory usage by usingonly the variables required for the current layer and by re-using the computation graphs across layers,and therefore making IGLU even less memory expensive.
Table 7: Test Accuracy of IGLU compared to GNNAutoScale. * - We perform experiments usingGNNAutoScale in a setting identical to IGLU with 2-layer models on PPI-Large, Reddit and Flickrdatasets and 3-layer models on OGBN-Arxiv dataset (transductive) and report the performance.
Table 8: Datasets used in experiments along with their statistics. MC refers to a multi-classproblem, whereas ML refers to a multi-label problem.
Table 9: Performance on Test Set for IGLU compared to additional algorithms. The metric isROC-AUC for Proteins and Micro-F1 for the others IGLU still retains the state-of-the-art resultsacross all datasets even when compared to these new baselines. MVS-GNN ran into runtime error onthe Proteins dataset (denoted by ||). AS-GCN, FastGCN and BanditSampler run into a runtime erroron datasets that require more than two layers (denoted by **). Please refer to section B.2 for details.
Table 10: Per epoch time (in seconds) for different methods as the number of layers increaseon the OGBN-Proteins dataset. ClusterGCN ran into a runtime error on this dataset as noted earlier.
Table 11: Test Performance (ROC-AUC) at Best Validation for different methods as the numberof layers increase on the OGBN-Proteins Dataset. Results are reported for a single run but trendswere observed to remain consistent across repeated runs. VRGCN ran into runtime error for 4 layers(denoted by ||). IGLU offers steady increase in performance as the number of layers increase, as wellas state-of-the-art performance throughout. GraphSAGE shows a decrease in performance on movingfrom 3 to 4 layers while GraphSAINT shows only a marginal increase in performance. Please referto section B.3.2 for details.
Table 12:	Statistics for the OGB-Products datasets. MC refers to a multi-class problem, whereasML refers to a multi-label problem.
Table 13:	Performance on the OGBN-Products Test Set for IGLU compared to baseline algo-rithms. IGLU outperforms all the baseline methods on this significantly large dataset as well.
Table 14: Comparison of IGLU’s Test performance with the baseline methods in the trans-ductive setting on the OGBN-Arxiv and OGBN-Proteins datasets. The metric is Micro-F1 forOGBN-Arxiv and ROC-AUC for OGBN-Proteins.
Table 15: Comparison of IGLU +GAT’s testperformance with the baseline GAT on differ-ent datasets.
Table 16: Comparison of IGLU’s test perfor-mance with GCN and GraphSAGE architec-tures with the baseline methods on the OGBN-Arxiv dataset.
Table 17: Effect of Non-smooth vs Smooth activation functions: Test Performance of IGLUon different datasets using ReLU and GELU activation functions. Metrics are the same for thedatasets as rePorted in Table 1 of the main PaPer. Results rePorted are averaged over five differentruns.
Table 18: Accuracy vs different update frequency on PPI-Large: Backprop OrderUPdate Frequency	Train Micro-F1	Validation Micro-F1	Test Micro-F10.5	0.761	0.739	0.7561	0.805	0778	0.7962	0.794	0.769	0.784We observe that more frequent uPdates helP stabilize training better. We also observe that uPdatefrequencies 1 and 2 Perform comPetitively, and both significantly outPerform uPdate frequency 0.5.
