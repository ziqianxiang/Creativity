Table 1: Parameters size, FLOPs, and speed of ViT-Base with different patch sizes (i.e., 8 × 8, 16 × 16,32 × 32, and 64 × 64). Models are fed by input images of size 224 × 224 × 3, thus the input sequencelength is (224/Patch_size)2 +1 (for the CLS token). Numbers in the table are reported using the codein Scenic (Dehghani et al., 2021a) when running on 64 TPU-V3.
Table 2: Detailed configuration as well as cost versus quality scores for experiments on scaling depth orwidth of Vision Transformer.
