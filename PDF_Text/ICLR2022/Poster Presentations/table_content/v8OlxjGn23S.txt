Table 1: Head-to-head comparison of our solver using Enhanced Optimality Cuts (EOC) versus k-medoids on the objective function value of problem (4) at different budgets. Lower values are better.
Table 2: Wass. + EOC with limitedruntimes Versus k-medoids on prob-lem (4) for CIFAR-10. The best so-lution for each budget is bolded andunderlined. See Appendix E.2 forfull results and Appendix E.6 for de-tails on k-medoids runtime.
Table 3: Mean ± standard deviation of accuracy results in the extremely low budget regime.
Table 4: Mean ± standard deviation of accuracy results in the classic active learning setup of Shuiet al. (2020). The best method is bolded and underlined._________________________________________	I	I B	I	1000	2000	3000	4000	5000	6000CIFAR-10	WAAL Wass. + EOC + P	57.0 ± 1.2 58.1 ± 0.1	65.9 ± 0.4 68.9 ± 0.7	73.3 ± 0.6 73.2 ± 0.2	76.7 ± 0.8 77.3 ± 0.3	77.4 ± 0.7 79.1 ± 0.7	81.0 ± 0.2 81.0 ± 0.4	IB	I	500	1000	1500	2000	2500	3000SVHN	WAAL Wass. + EOC + P	69.6 ± 0.9 69.3 ± 1.1	76.9 ± 0.8 79.4 ± 1.7	84.2 ± 0.7 84.7 ± 0.6	86.2	± 1.7 86.3	± 0.6	87.5 ± 1.0 88.3 ± 0.8	88.9 ± 0.5 89.9 ± 0.5ClFAR-10: GBDAlgOrithm BOUndSWass. + EOCWass. + EOC + PIteration30	40	50^Figure 4:	(Left) Upper and lowerbounds in the first 50 iterations atB = 10. (Right) Increasing thewall-clock runtime of the GBD al-gorithm. All plots show mean ±standard error over three runs.
Table 5: Summary of our setup in the image classification experiments. *In each of the first twoiterations of each data set, we use only half of the labeling budget.
Table 6: Summary of our setup in the domain adaptation experiments. *For each experiment, werandomly split the target set to 70% training and 30% testing sets.
Table 7: Head-to-head comparison of our solver versus k-medoids on objective function value ofproblem (4) at different budgets. For each data set, the best solution for each budget is bolded andunderlined. All entries show mean ± standard deviation over five runs.
Table 8: Head-to-head comparison of Wass. + EOC with different runtimes versus k-medoids onobjective function value of problem (4) for the CIFAR-10 data set at different budgets. The bestsolution for each budget is bolded and underlined and the second best solution is underlined.
Table 9: Numerical values of main results in Figure 2. All entries show mean ± standard deviationover five runs. The best model for each budget range is bolded and underlined and the second bestmodel is underlined.
Table 10: Numerical values of results on domain adaptation. All entries show mean ± standarddeviation over five runs. The best model for each budget range is bolded and underlined and theSecond best model is underlined.
Table 11: Head-to-head comparison of Wass. + EOC + P versus k-centers on domain adaptationwith statistical t-tests to compare. The best model for each budget range is bolded and underlined.
Table 12: Runtime in minutes:seconds for all of the baselines on CIFAR-10. Times are averagedover five runs.________________________________________________________________B	Random	Confidence	Entropy	k-medoids	k-centers	WAAL10	0.03	0.03	0.03	7 : 56.81	0.09	0.0320	0.03	0.02	0.01	15 : 22.28	0.15	11.4340	0.02	0.01	0.01	7 : 25.69	1.33	12.1860	0.04	0.02	0.02	2 : 18.51	0.34	11.9380	0.03	0.02	0.02	2 : 18.37	1.75	11.97100	0.03	0.02	0.02	2 : 21.84	0.54	12.25120	0.03	0.02	0.01	2 : 17.18	1.31	11.69140	0.03	0.02	0.02	2 : 20.91	1.19	11.73160	0.03	0.02	0.01	2 : 18.31	3.50	11.59180	0.03	0.03	0.02	2 : 13.74	0.74	11.51In order to adapt our optimization problem to the high-budget setting, we make one modificationto our algorithm. Rather than solving for the entire labeling budget in one problem, we insteadrandomly partition the data set into two subsets and solve two optimization problems each for halfof the labeling budget respectively. For example, if we have a previously labeled set of 2, 000 images,an unlabeled pool of 48, 000, and a budget of B = 1, 000 we randomly split the problem into twosets of 1, 000 labeled images, 24, 000 unlabeled images, and B = 500 budgets. This ensures thatour optimization problem remains manageable in computational size. Furthermore, we implement
