Table 1: U2Net architecture used for MDAE and MEM2 parametrizations for all datasets. All layerwidths (number of channels) were expanded by a width factor for CIFAR-10 and FFHQ-256.
Table 2: Main hyperparameters and computational resources used for training MDAE models.
Table 3: MUVB Encoder architecture for MNIST. Each row indicates a sequence of transforma-tions in the first column, and the spatial resolution of the resulting output in the second column.
Table 4: FID results for unconditional MCMC-based sample generation on CIFAR-10	FID	long chain	single chain	MCMC stepsXie et al. (2018)	35.25	X	X	N/ANijkamp et al. (2019)	23.02	X	X	100Du & Mordatch (2019)	40.58	X	X	60Zhao et al. (2020)	16.71	X	X	60Xie et al. (2021)	36.20	X	X	50Nijkamp et al. (2022)	78.12	✓	X	2,000MDAE, 1 ⑥ 8 (our work)	43.95	✓	✓	1,000,000I FFHQ-256In this section, we provide several WJS chains for our MDAE 4 0 8 model on FFHQ-256 dataset. Werefer to Algorithm 1 by A1 and Algorithm 2 by A2, and MCMC parameters are listed as (δ, γ, u).
