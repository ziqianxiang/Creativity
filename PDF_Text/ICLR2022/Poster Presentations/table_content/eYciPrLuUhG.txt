Table 1: Comparing structure learning methods in terms of structural hamming distance (SHD) oncommon graph structures (lower is better), averaged over 25 graphs each. ENCO outperforms allbaselines, and by enforcing acyclicity after training, can recover most graphs with minimal errors.
Table 3: Results on graphs from the BnLearn library measured in structural hamming distance (loweris better). Results are averaged over 5 seeds with standard deviations listed in Appendix C.5. Despitedeterministic variables and rare events, ENCO can recover all graphs with almost no errors.
Table 2: Results of ENCO on detecting la-tent confounders. The missed confoundersdo not affect other edge predictions.
Table 4: Hyperparameter overview for the simulated graphs dataset experiments presented in Table 1.
Table 5: Extension of Table 1 with the metric structural intervention distance (SID) (lower isbetter), averaged over 25 graphs each. The conclusion is the same as for SHD, namely that ENCOoutperforms all baselines, while the acyclic heuristic has an even greater impact.
Table 6: Hyperparameter overview of ENCO for the scalability experiments presented in Table 7.
Table 7: Results for graphs between 100 and 1000 nodes. We report the average and standard deviationof the structural hamming distance (SHD) over 10 randomly sampled graphs. * The maximum runtimeof ENCO was measured on an NVIDIA RTX3090. Baselines were executed on the same hardware.
Table 8: Results of ENCO on detecting latent confounders averaged over 25 graphs with 25 nodesin the data limit (10k samples per intervention, 100k observational samples) and limited data (200samples per intervention, 5k observational samples). In the data limit, only false negative predictionsof latent confounders occured which did not affect other edge predictions. With little interventionaldata, more false positives occur reducing the precision.
Table 9: Results on graphs from the BnLearn library measured in structural hamming distance (loweris better). Results are averaged over 5 seeds with standard deviations.
Table 10: Results on graphs from the BnLearn library measured in structural hamming distance(lower is better), using 5k observational and 200 interventional samples.
Table 11: Repeating experiments of Table 1 with large sample sizes (10k samples per intervention,100k observational samples). In line with the theoretical guarantees, ENCO can reliably recover fiveout of the six graph structures without errors.
Table 12: Repeating experiments of Table 1 with very small sample sizes (20 samples per intervention,1k observational samples). Despite the limited data, ENCO can recover graphs with small parent setsreasonably well, while the graphs collider and full suffer for all methods.
Table 13: Detailed results of the experiments with fewer interventions. See Figure 16 for a visualiza-tion and discussion.
Table 14: Extension of Table 11 with ablation study of using Bengio et al. (2020) gradients withENCO.
Table 15: Experiments on graphs with deterministic variables. The performance over 10 experimentsis reported in terms of SHD with standard deviation in brackets. ENCO can recover most of thegraphs with less than two errors.
Table 16: Experiments on graph with continuous data from Brouillard et al. (2020). The suffix “-G”denotes that the neural networks model a Gaussian density, and “-DSF” a two-layer deep sigmoidalflow. ENCO outperforms all baselines in this scenario, verifying that ENCO also works on continuousdata well.
Table 17: Experiments on graph with continuous data from Brouillard et al. (2020) with smallersample sizes for both observational and interventional datasets (in brackets). ENCO shows to performmuch better in smaller sample sizes than a skeleton+orientation method, underlining the benefit oflearning the whole graph from observational and interventional data jointly.
Table 18: Comparing structure learning methods in terms of structural hamming distance (SHD) oncommon graph structures (lower is better), averaged over 25 graphs each. ENCO outperforms allbaselines, and by enforcing acyclicity after training, can recover most graphs with minimal errors.
Table 19: Experiments with a different data simulator, introducing independence among parents foreach variable. Similar to the neural-based synthetic data, ENCO recovers most graphs with a minorerror rate, outperforming other baselines.
