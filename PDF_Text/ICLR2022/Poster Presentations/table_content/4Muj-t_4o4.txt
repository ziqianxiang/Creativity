Table 1: Average cumulated reward of the different models over multiple testing environments aver-aged over 10 training seeds (higher is better). For DIAYN and Lc, we report the results and tested 10policies for LoP, Lc and DIAYN+R using 10 episodes per policy for stochastic environments, and 1episode per policy on deterministic ones. Performance is evaluated using the deterministic policy.
Table 2: Modified HalfCheetah environments used for testing. Morphological modifications includea variation on the mass and the radius of a specific part of the body (torso, thighs, shins, or feet). Wealso modified the dynamics (gravity and friction). Environment names are exhaustive: Big refers toa increase of 25% of radius and mass, Small refers to a decrease of 25%. For example, "BigFoot"refers to an HalfCheetah agent where feet have been increased in mass and radius by 25%. Forgravity and friction, we also tried an increase/decrease by 50% (respectively tagged "Huge" and"Tiny").
Table 3: Hyper-parameters for PPO over HalfCheetah15	Single Policy (K=1)	Ensemble (5 policies)	LoP	DIAYN + R	Lc	BoP	CoP	 K=5							BigFeet	7433 ± 1988	8115±344	8895 ± 289	8454 ± 655	8353± 804	7993 ± 702	8087 ± 496BigFriction	8579 ± 2224	10757±319	11635±355	9962 ± 2113	10649± 2085	8846 ± 2279	10596 ±1738BigGravity	7508± 2086	9131±260	10464± 798	8989 ± 1723	9360 ± 1531	7878± 1593	9287 ±1200BigShins	7274±898	7992 ± 294	8879± 98	8099 ± 806	8206± 770	7726 ± 903	8226 ±1144BigThig	7963 ± 1466	9331 ±310	10054± 724	8940 ± 1558	9360 ± 1669	8105±1622	9525 ± 832BigTorso	7091 ± 2221	8942± 94	9834±310	8701 ± 1472	9198±1342	7790 ±1713	8927 ±1128SmallFeet	5973 ± 2490	10012 ± 1921	8283±648	7580 ± 1411	8186±1512	6775 ± 2308	8996 ±1150SmallFriction	8652 ±1717	7299 ± 464	11391±348	9813 ± 2074	10181± 1617	8652 ± 2075	10459±1387SmallGravity	9004± 1665	8004± 595	11840±406	10132 ± 1903	10434±1951	9428 ±2180	10594±1438SmallShin	7492 ± 2999	10379±257	10840± 196	9540 ± 1592	9837 ± 1343	8451 ± 1889	9967 ±1079SmallThig	8914±1837	11133±384	11294±46	10078 ± 1410	10603±1160	9340 ± 1751	10603 ±1540SmallTorso	8885 ± 1522	9669 ±213	11433 ±360	9850 ± 1761	10010±1856	9182±1779	10092 ±1541HugeFriction	6999 ± 3441	10006±560	11537±613	9483 ± 2228	10305±2104	8387 ±2515	10749±1591HugeGravity	6133±2147	10452± 371	8425 ± 554	7700 ± 1216	7621 ± 1137	6532 ±1133	7632 ± 903TinyFriction	7953 ±1843	9444± 293	10425± 211	9132 ± 1862	9521 ± 1462	7867 ± 2003	9468 ±1468TinyGravity	7304± 3484	9887 ±1822	11385±169	9363 ± 1734	9620 ± 2041	9118± 2360	9020 ±2028
Table 4: Mean and standard deviation of cumulative reward achieved on HalfCheetah test sets permodel (see Table 2 for environment details). Results are averaged over 10 training seeds (i.e., 10models are trained with the same hyper-parameters and evaluated on the 16 test environments). K isthe number of policies tested at adaptation time, using 1 episode per policy since this environment isdeterministic. Ensembling with K = 5 models takes 5 times more iterations to converge and testingvalues of K > 5 is very costly in terms of GPU consumption.
Table 5: Modified Ant environments used for testing.
Table 6: Hyper-parameters for PPO over Ant22β=	Single Policy (K=1)	0.1	LoP 1	10	0.1	DIAYN + R 1	10	0.1	Lc 1	10K=5 BigFriction	7454 ±166	7544 ±140	7470 ± 96	7541± 202	7256 ± 1010	6403 ± 726	6267 ± 627	7666 ± 103	7573 ± 154	7538 ± 136BigGravity	6905±138	7038 ±211	6937 ± 23	7027±182	6858 ± 827	6082 ± 583	5865 ± 543	7123±184	7075 ± 168	7051 ± 131SmallFriction	6755 ± 2073	7695±156	7652 ± 126	7599 ± 167	7046 ± 1777	6491 ± 807	6133 ± 706	7846±127	7673 ± 183	7734 ± 211SmallGravity	7057±1875	7738 ±134	7639 ± 120	7748 ± 169	7533 ± 896	6582 ± 727	6486 ± 769	7876±100	7745 ± 105	7772 ± 84HugeFriction	7505± 252	7634 ±152	7616 ± 68	7601 ± 253	7220 ± 1331	6387 ± 752	6384 ± 733	7799 ± 103	7647 ± 128	7668 ± 77HugeGravity	380 ± 507	1111±538	1407 ± 557	1494 ± 934	846 ± 556	2924 ± 1992	2847 ± 1598	1134±934	915 ± 465	1440 ± 985TinyFriction	2747 ±1241	3716±996	4584 ± 801	4070 ± 1751	3540 ± 948	2950 ± 1113	2600 ± 572	3550± 843	4140 ± 294	3487 ± 508TinyGravity	-520±426	-106±125	-139 ± 274	116±322	-479 ± 374	-3 ± 202	224 ± 108	-234 ± 790	-401 ± 373	-83 ± 329DefectiveSensor 5%	4308 ±59	5243 ±322	5630 ± 124	5560± 272	4958 ± 126	4492 ± 211	4360 ± 338	5428 ±424	4988 ± 123	5225 ± 562DefectiveSensor 10%	2770 ± 171	3620± 238	3660 ± 328	3625±291	3440 ± 64	3371 ± 57	3223 ± 400	3519±76	3406 ± 215	3491 ± 207DefectiveSensor 15%	1531±90	2583 ± 247	2738 ± 312	2740 ±167	1774 ± 84	1984 ± 383	2055 ± 311	2408 ±340	2226 ± 78	2349 ± 271DefectiveSensor 20%	1026±77	1750± 249	1965 ± 234	2008 ± 199	1104 ± 101	1490 ± 387	1450 ± 347	1658 ± 245	1546 ± 108	1714 ± 238DefectiveSensor 25%	736 ± 95	1595±317	1757 ± 154	1526±338	762 ± 34	1107 ± 396	1120 ± 241	1344 ± 262	1271 ± 205	1363 ±254DefectiveSensor 30%	472 ± 50	695±112	793 ± 166	674 ±120	577 ± 34	859 ± 290	766 ± 212	673 ± 115	575 ± 49	622 ± 100DefectiveSensor 35%	424 ± 48	606±135	684 ± 182	635 ± 85	449 ± 68	650 ± 245	565 ± 151	618 ± 170	525 ± 92	602 ± 175Average	3338	3905	4035	3998	3558	3451	3356	3909	3820	3870K=10 BigFriction	7454 ±166	7591 ± 134	7487 ± 101	7556 ± 199	7695 ± 113	6771 ± 573	6021±748	7685 ± 103	7580 ± 163	7554 ± 116
Table 7: Mean and standard deviation of cumulative reward achieved on Ant test sets per model. Re-sults are averaged over 10 training seeds (i.e., 10 models are trained with the same hyper-parametersand evaluated on the 12 test sets). K is the number of policies tested at adaptation time, using 1episode per policy since this environment is deterministic. For this environment, we split the resultsper β value as it has been used for beta ablation study (see Figure 3)23Figure 10: Evolution of the cumulative reward during training on the generic Ant environment forLoP, DIAYN+R and Lc for different values of beta. On can see that DIAYN+R struggles to performwell on the train set for β = 1 and β = 10. Results are averaged over 10 seeds.
Table 8: CartPole train and test environmentsHyper-parameter	Valuelearning rate:	0.001n acquisition steps per epoch:	8n parallel environments:	32critic coefficient:	1.0entropy coefficient:	0.001discount factor:	0.99gae coefficient:	1.0gradient clipping:	2.0n neurons per layer:	8n layers:	2	LoP	β:	1.0	DIAYN	~J^-	-10-n neurons per layer discriminator:	8n layers discriminator:	2learning rate discriminator:	0.001Table 9: Hyper-parameters for A2C over CartPole
Table 9: Hyper-parameters for A2C over CartPole	Single	LoP	DIAYN+R	DIAYN+R L2HeavyPole CartPole	200.0 ± 0.0	200.0 ± 0.0	200.0 ± 0.0	200.0 ± 0.0LightPole CartPole	200.0 ± 0.0	200.0 ± 0.0	200.0 ± 0.0	200.0 ± 0.0LongPole CartPole	54.4±81.6	56.1±72.2	163.3 ± 73.3	123.8 ± 86.2ShortPole CartPole	67.0±33.1	78.9 ± 25.3	50.7 ± 18.1	64.8±31.2StrongPush CartPole	200.0 ± 0.0	200.0 ± 0.0	200.0 ± 0.0	199.9 ± 0.2WeakPush CartPole	138.9±43.8	164.4±18.3	194.3 ± 10.0	148.1±64.8Average	143.4	—	149.9	—	168.1	156.1 —Table 10: Results over CartPole, using 10 policies, and 10 episodes per policy at adaptation time.
Table 10: Results over CartPole, using 10 policies, and 10 episodes per policy at adaptation time.
Table 11: Acrobot train and test environmentsHyper-parameter	Valuelearning rate:	0.001n acquisition steps per epoch:	8n parallel environments:	32critic coefficient:	1.0entropy coefficient:	0.001discount factor:	0.99gae coefficient:	0.7gradient clipping:	2.0n neurons per layer:	16n layers:	2	LoP	β:	1.0	DIAYN	~J-	-10-n neurons per layer discriminator:	16n layers discriminator:	2learning rate discriminator:	0.001Table 12: Hyper-parameters for A2C over Acrobot
Table 12: Hyper-parameters for A2C over Acrobot	Single	LoP	DIAYN+R	DIAYN+R L2Heavy Acrobot	-108.4±3.2	-105.1±1.0	-108.0±1.8	-108.2±4.4HighInertia Acrobot	-108.7±5.6	-99.8 ± 2.8	-106.0±2.7	-106.8±8.9Light Acrobot	-120.7±71.3	-107.2±58.8	-115.2±33.1	-93.1±37.3Long Acrobot	-124.3±2.7	-115.8±9.6	-117.3±4.1	-117.5±5.3LoWInertia Acrobot	-71.3±2.3	-70.7 ± 0.7	-71.2±0.7	-71.3±1.9Short Acrobot	-65.1±2.8	-60.7 ± 0.6	-64.2 ± 2.6	-64.7 ± 5.6Average	-99.7	—	-93.2	―	-97.0	—	-93.6 —Table 13: Results over Acrobot, using 10 policies, and 10 episodes per policy at adaptation time.
Table 13: Results over Acrobot, using 10 policies, and 10 episodes per policy at adaptation time.
Table 14: Pendulum train and test environmentsHyper-parameter	Valuelearning rate:	0.001n acquisition steps per epoch:	8n parallel environments:	32critic coefficient:	1.0entropy coefficient:	0.001discount factor:	0.99gae coefficient:	0.7gradient clipping:	2.0n neurons per layer:	16n layers:	2LoP	β:	1.0	DIAYN	~J-	-10-n neurons per layer discriminator:	16n layers discriminator:	2learning rate discriminator:	0.001Table 15: Hyper-parameters for A2C over Pendulum
Table 15: Hyper-parameters for A2C over Pendulum	Single	LoP	DIAYN+R	DIAYN+R L2Light Pendulum	-36.5±58.5	-11.4±2.7	-39.3 ± 10.9	-32.1 ± 15.4Long Pendulum	-82.1±20.4	-64.5±13.0	-70.6±15.2	-71.9 ± 17.3Short Pendulum	-39.6 ± 66.9	-10.7±2.1	-31.3±11.7	-28.2 ± 13.3Average	-52.7	-	-28.9	-47.1	-	-44.0Table 16: Results over Pendulum, using 10 policies, and 10 episodes per policy at adaptation time.
Table 16: Results over Pendulum, using 10 policies, and 10 episodes per policy at adaptation time.
Table 17: Hyper-parameters for A2C over Minigrid	Single	LoP	DIAYN+R	DIAYN+R L2Two Rooms Maze 1	0.387 ± 0.447	0.619 ± 0.309	0.348 ± 0.348	0.656 ± 0.328Two Rooms Maze 2	0.433 ± 0.499	0.865 ± 0.0	0.627 ± 0.363	0.692 ± 0.346Two Rooms Maze 3	0.194±0.387	0.617 ± 0.309	0.348±0.353	0.656 ± 0.328Four Rooms Maze 1	0.0 ± 0.0	0.294 ± 0.36	0.0±0.0	0.24 ± 0.359Four Rooms Maze 2	0.0 ± 0.0	0.004 ± 0.007	0.0±0.0	0.137 ± 0.274Four Rooms Maze 3	0.0 ± 0.0	0.281±0.345	0.166±0.287	0.28 ± 0.345Average	0.169 —	0.447 —	0.248 —	0.443Table 18: Results over Minigrid, using 10 policies, and 1 episode per policy at adaptation time.
Table 18: Results over Minigrid, using 10 policies, and 1 episode per policy at adaptation time.
Table 19: Hyper-parameters for PPO over ProcGen	Single	LoP	DIAYN+RLevels 100 to 110	11.9±2.6	20.1±4.4	12.1±5.2Levels 200 to 210	7.1 ± 1.7	10.3 ± 0.2	5.1±1.7Levels 300 to 310	14.3±5.1	18.7±2.7	17.1±4.6Table 20: Results over Procgen, using K=10 at adaptation time, (averaged over 16 episodes pershot), averaged over 5 runs.
Table 20: Results over Procgen, using K=10 at adaptation time, (averaged over 16 episodes pershot), averaged over 5 runs.
Table 21: Hyper-parameters for PPO used for Maze 2dSingle Policy	LoP	DIAYN + Rβ=		0.01	0.1	1.	0.01	0.1	1.
Table 22: Results over Maze2d, using K=10, averaged over 5 runs.
