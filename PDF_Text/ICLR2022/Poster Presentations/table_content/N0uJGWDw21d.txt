Table 1: KNN classification accuracy on Ima-geNet. We report the results on the validationset with 10 nearest neighbors. ResNet-50×2is used as the teacher.
Table 2: Linear classification accuracy on ImageNet over different student architectures. Note thatwhen using R50×2 as the teacher, SEED distills for 800 epochs while DisCo and BINGO distill for200 epochs. The numbers in brackets indicate the accuracies of teacher models. “T” denotes theteacher and “S” denotes the student.
Table 3: Transfer learning accuracy (%) on COCO detection.
Table 4: Transfer learning accuracy (%) on COCO instance segmentation.
Table 5: Semi-supervised learning by fine-tuning 1% and 10% images on ImageNet using ResNet-18.
Table 6: Linear classification accuracy onCIFAR-10/100 with ResNet-18.
Table 7: Lower and Upper bound performanceexploration via the bagging criterion.
Table 8: Effects of utilizing teacher’s data-relation and teacher’s pretrained weights. The columnof Student Relation means that we bag data with features extracted from student model online andthe column of Teacher Relation means that we bag data with features extracted from a pretrainedteacher model. When teacher parameters are not used, we replace the pretrained teacher model as amomentum update of student model like He et al. (2020).
Table 10: Top-1 accuracy of linear classification results on ImageNet with ResNet-34 under differentepochs. ResNet-152 is used as teacher model. “T” denotes the teacher and “S” denotes the student.
Table 9: Top-1 accuracy of linear classifi-cation results on ImageNet using differentdistillation methods on ResNet-18 studentmodel (ResNet-50 is used as teacher model)Method	Top-1MoCo-V2 baseline (Gao et al., 2021) 52.2MoCo-V2 + KD (Fang et al., 2020)^^553"MoCo-V2 + RKD	61.6DisCo + KD (Gao et al., 2021)	60.6DisCo + RKD (Gao	et al.,	2021)	60.6BINGO	64.0Computational Complexity. As shown in Fig. 2, the batch size is doubled during each forwardpropagation. The computation cost is increased compared with SEED (Fang et al., 2020) and MoCo-v2 (Chen et al., 2020c). As for DisCo (Gao et al., 2021), the computational complexity is alsoincreased due to the multiple forward propagation for one sample: once for the mean student, twicefor the online student and twice for the teacher model. The total number of forward propagation is 5,2.5× bigger than SEED and MoCo-v2. For the above analysis, BINGO has a similar computationcost with DisCo. To compare with SEED and DisCo under the same cost, we distill ResNet-34 fromResNet-152 for 100 and 200 epochs respectively. We compare results of 100 epochs with SEED and200 epochs with DisCo. The results are shown in Table 10 and BINGO still shows significantly better
Table 11: Top-1 accuracy with different cluster numbers C in K-means clustering.
Table 12: Transfer learning performance on COCO Lin et al. (2014) object detection and instancediscrimination with ResNet-34 distilled from ResNet-152. “bb” denotes bounding box and “mk”denotes mask.
Table 13: Averaged distance betweenanchor and positive samples in the same bagMethod	DistanceMoCo-V2 baseline	0.38Distill w/o Bag-Aggregation	0.36Distill w/ Bag-Aggregation	0.32Table 14: Averaged intra-class distance on Ima-geNet validation setMethod	DistanceMoCo-V2 baseline	0.88Distill w/o Bag-Aggregation	0.72Distill w/ Bag-Aggregation	0.65We visualize the last embedding feature to understanding the aggregating properties of the proposedmethod. 10 classes are randomly selected from validation set. We provide the t-sne visualization ofthe student features. As shown in Fig. 4, the same color denotes features with the same label. It canbe seen that BINGO gets more compact representations compared with models without distillation ordistilling without pulling related samples in a bag.
Table 14: Averaged intra-class distance on Ima-geNet validation setMethod	DistanceMoCo-V2 baseline	0.88Distill w/o Bag-Aggregation	0.72Distill w/ Bag-Aggregation	0.65We visualize the last embedding feature to understanding the aggregating properties of the proposedmethod. 10 classes are randomly selected from validation set. We provide the t-sne visualization ofthe student features. As shown in Fig. 4, the same color denotes features with the same label. It canbe seen that BINGO gets more compact representations compared with models without distillation ordistilling without pulling related samples in a bag.
