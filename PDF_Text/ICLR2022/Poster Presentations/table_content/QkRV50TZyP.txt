Table 1: A comparison of datasets from different domains.
Table 2: Transferability comparisons on four coarse-grained classification tasks. Here we report thetop-1 accuracy after attacking (the lower, the better). The generator Gθ is trained in the ImageNetdomain, and adversarial examples are within the perturbation budget of '∞ ≤ 10.
Table 3: Transferability comparisons on three fine-grained classification tasks. Here we report thetop-1 accuracy after attacking (the lower, the better). The generator Gθ is trained in ImageNetdomain and adversarial examples are within the perturbation budget of '∞ ≤ 10.
Table 4: Transferability comparisons on ImageNet (source domain). Here we report the top-1 ac-curacy after attacking (the lower, the better). The generator Gθ is trained in ImageNet domain (“*”denotes white-box model) and adversarial examples are within the perturbation budget of '∞ ≤ 10.
Table 5: Transferability comparisons of CDA and our methods. Here we report the top-1 accuracyafter attacking (the lower, the better). The generator Gθ is trained in CUB-200-2011 domain andadversarial examples are within the perturbation budget of '∞ ≤ 10.
Table 6: Transferability comparisons of singe-model (i.e. VGG-16) attacks and ensemble-model(i.e. an ensemble of VGG-16, VGG-19, Res-152 and Dense-169) attacks. Here we report the top-1accuracy after attacking (the lower, the better) and adversarial examples are within the perturbationbudget of '∞ ≤ 10.
Table 7: We show the results of 5 random seeds for methods. Here we report the top-1 accuracyafter attacking (the lower, the better). The generator Gθ is trained against VGG-16 and adversarialexamples are within the perturbation budget of '∞ ≤ 10.
