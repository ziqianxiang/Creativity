Table 1: The number of parameters, maximal rank, and example. We assume that the weightsof the FC and convolutional layers are in Rm×n and RO×I×K1 ×K2 , respectively. The rank ofthe convolutional layer is the rank of the 1st unfolding tensor. As a reference example, we setm = n = O = I = 256, K1 = K2 = 3, and R = 16.
Table 2: Accuracy comparison between low-rank parameterization and FedPara. (a) The accuracyVGG16low and VGG16FedPara. We set the target rounds T = 200 for CIFAR-10, 400 for CIFAR-100, and 300 for CINIC-10. (b) The accuracy of LSTMlow and LSTMFedPara. We set the targetrounds T = 500 for the Shakespeare dataset.
Table 3: The compatibility of FedPara with other FL algorithms. The first row is the accuracy ofFedPara combined with other FL algorithms on the CIFAR-10 IID setting after 200 rounds, andthe second row is the required rounds to achieve the target accuracy 80%.
Table 4: Accuracy of FedPara with additional techniques. 95% confidence intervals are presentedwith eight repetitions.
Table 5: Y’s and their corresponding numbers of parameters for VGG160ri. and VGG16FedPara.
Table 6: HyPer-Parameters of our FedPara with FedAvgC.5 Hyper-parameters of Other OptimizersFor comPatibility exPeriment, we combine FedPara with other oPtimization-based FL algorithms:FedProx (Li et al., 2020), SCAFFOLD (Karimireddy et al., 2020), FedDyn (Acar et al., 2021),and FedAdam (Reddi et al., 2021). FedProx (Li et al., 2020) imPoses a Proximal term to theobjective function to mitigate heterogeneity; SCAFFOLD (Karimireddy et al., 2020) allows clientsto reduce the variance of gradients by introducing auxiliary variables; FedDyn (Acar et al., 2021)introduces dynamic regularization to reduce the inconsistency between minima of the local devicelevel emPirical losses and the global one; FedAdam emPloys Adam (Kingma & Ba, 2015) at theserver-side instead of the simPle model average.
Table 7: The required time during one round. We denote the computation time, the communicationtime, and the total time during one round as tcomp., tcomm., and t, respectively. We set the networkspeeds as 2, 10, and 50 Mbps.
Table 8: The real training time to achieve the target accuracy. We set the network speeds as 2, 10,and 50 Mbps, and the required rounds for VGG16ori. is 112, FedPara (γ = 0.1) is 115 to achievethe same target accuracy in the CIFAR-10 IID setting.
Table 9: The accuracy of VGG16 with original and our parameterization on the CIFAR-10 IIDsetting during 200 and 1000 rounds.
Table 10: The accuracy of VGG16Pufferfish and VGG16FedPara on CIFAR-10 IID setting. #parameters is the the ratio of each model parameter when the number of parameters of VGG16ori. isset 1.0.
Table 11: The accuracy of LSTMs on IID and non-IID setting. # parameters is the the ratio of eachmodel parameter when the number of parameters of LSTMori. is set 1.0.
Table 12: The accuracy of VGG16 on CIFAR-10 IID setting for original, quantization, and ourparameterization models.
