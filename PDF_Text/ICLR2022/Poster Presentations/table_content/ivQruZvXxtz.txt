Table 1: F1 and EM score on the TYDI-QA dataset for QA. The best result for multilingual models is markedwith bold while the underline denotes the best result among all the models including the monolingual model.
Table 2: F1 score on WikiAnn dataset for NER. The best result for multilingual models is marked with boldwhile the underline denotes the best result among all the models including the monolingual model.
Table 3: We evaluate the model, trained with TYDI-QA dataset, on five unseen languages from MLQA dataset.
Table 4: Zero-shot cross lingual transfer from English (SQuAD) to six unseen languages (MLQA).
Table 5: Zero shot cross lingual transfer from English (MNLI) to fourteen unseen languages (XNLI).
Table 6: The number of train/validation instances for each language from TYDI-QA dataset.
Table 7: The number of train/validation instances for each language from WikiAhn dataset.
Table 8: The number of train/validation instances for each language from MLQA dataset.
Table 9: We train MTL models on 8 tasks from GLUE dataset and report their performance.
Table 10: We finetune MTL models with pretrained ResNet18 backbone on 8 image classification tasks.
