Table 1: Cross-lingual transfer performances of POS and NER tasks on languages with differentdata resources or different language families, where there are only labeled training data in English.
Table 2: Spearman’s rank correlation ρ between the CKA score and cross-lingual transfer performanceon two XTREME tasks, where * denotes training on the source language, and ^ denotes the translate-train approach. * denotes the p-value is lower than 0.05. Results indicate the correlation is solid.
Table 3: Main results on the XTREME benchmark. * denotes using other data augmentation strategyin addition to machine translation. ^ denotes results from Ruder et al. (2021), which is an updatedversion of Hu et al. (2020).
Table 4: Comparisons between X-Mixup and xTune under the same setting: XLM-R-base modeland machine translation data augmentation. Results of xTune are from Zheng et al. (2021) Table 4.
Table 5: Ablation results on X-Mixup, where w/o mixup denotes remove the cross-lingual manifoldmixup during training and inference and λ = λ0 denotes a constant mixup ratio.
Table 6: Performances on XNLI test set, where Trans-train and X-Mixup are trained on 8 seenlanguages and tested on both these seen languages and 7 unseen languages. ∆ is the performancedifference between X-Mixup and Trans-train. Results show X-Mixup performs better than Trans-train by a large margin on both seen and unseen languages.
Table 7: CKA scores and performances on XNLI test set, where ∆ is the score or performancedifference between X-Mixup and Trans-train. Results show X-Mixup improves the CKA scoresevenly across different target languages and the performance improvements are diverse. There is noobvious correlation between the CKA score improvement and performance improvement.
Table 8: The cross-lingual transfer gap (lower is better) of different methods on the XTREMEbenchmark. For QA tasks, We only show EM scores. * denotes results from Wei et al. (2020). Overall,X-Mixup achieves the smallest cross-lingual transfer gap on four out of seven datasets.
Table 9: Ablation results on the consistency loss, which show the KL consistency loss contributesmore than the MSE consistency loss on the classification task.
Table 10: Hyper-parameters used for X-MIXUP, where α is used for balanced training in Eq 8 and pkis the scheduled sampling decay rate.
Table 11: XNLI accuracy scores for each language.
Table 12: PAWS-X accuracy scores for each language.
Table 13: POS results (F1) for each language.
Table 14: NER results (F1) for each language.
Table 15: XQuAD results (F1 / EM) for each language.
Table 16: MLQA results (F1 / EM) for each language.
Table 17: TyDiQA-GolP results (F1 / EM) for each language.
