Table 1: Categorical VAE experiment with stability regularization showing negative best validationELBO (best of 5 runs) where τ is the Gumbel-softmax temperature.
Table 2: Regularizationcoefficient with categor-ical VAE with Gumbel-Softmax at τ = 2Value	-V. ELBO80	124.6100	122.7120	122.2150	123.1No Reg.	132.4we do not use a separate stability layer and regularization is performed directly on the logits. This isa restriction but has the advantage that the methods remain directly comparable and also that the KLdivergences are easily computable because of independence.
Table 3: NRI Physics Simulation Accuracy. * indicatesresult different from number reported.
Table 4: Comparing with Paulus et al. (2021)in latent spanning tree recovery with 3,5 and10 steps, w.r.t. test ELBO, precision, recall.
Table 5: Unsupervised classification accuracy forMNIST_______________________________________Model	AccuracyAAE (Makhzani et al.)	95.9GMVAE (Dilokthanakul et al.)	92.7SB-VAE (Nalisnick & Smyth)	92.35Stab. Reg. (20 Clusters)	93.3The networks in p(x|Z), q(Z|x) are ResNets with two ResNet blocks, while gφ3 is implemented as anMLP. The categorical variable is implemented as a softmax at the output of a stability network in gφ3,which we regularize to produce categorical outputs. Computing KL divergence requires probabilitiesfor the categories, which we approximate using a separate two-layer MLP with softmax output,trained to regress over the categorical outputs. To train the model we maximize the ELBO plus theauxiliary stability and regression objectives. We do not use Gumbel Softmax and use standalonestability regularization for the categorical variables.
Table 6: FID scores for CIFAR-10Model	FIDPixelCNN (Ostrovski et al.; Oord et al., 2016)	65.93PixelIQN (Ostrovski et al.)	49.46DCGAN (Radford et al.; Arjovsky et al., 2017)	37.11NVAE (Vahdat & Kautz, 2020; Aneja et al.)	51.10Discrete AE+Stab. Reg.	64.39Discrete AE+Stab. Reg. (cond.)	54.40The output model in both cases is Gaussian.
Table 7: Negative validation ELBO when varying ρ with categorical VAE with Gumbel-Softmax atT = 1P -V. ELBO0.5	113.80.6	113.90.8	113.80.9	113.7B.3.2 Gumbel Noise in RegularizationWhen computing the stability regularization objective alongside Gumbel-Softmax we evaluate thestability layer without Gumbel noise in the experiments, using a plain softmax as the output of thestability layer. Gumbel noise is still used for the input to the downstream decoder network.
Table 8: Categorical VAE experiment with stability regularization using Gumbel noise for stability.
