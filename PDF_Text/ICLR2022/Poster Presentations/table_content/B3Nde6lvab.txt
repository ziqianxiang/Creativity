Table 1: Test accuracy and expected sharpness of different methods across different tasks. Thereported numbers are the averages over 5 replications. For 95% CL see Appendix A.
Table 2: Our method’s gain on test accuracy persists even when applied with techniques such as dataaugmentation and SchedUled learning rates. For 95% CL See APPendix A.______________CIFAR10-VGG11	Rep 1	Rep 2	Rep 3	Rep 4	Rep 5	AverageSB+Clip	89.40%	89.41%	89.89%	89.52%	89.47%	89.54%Our1	90.76%	90.57%	90.49%	90.85%	90.79%	90.67%Our2	90.67%	90.23%	90.52%	90.13%	90.70%	90.45%CIFAR100-VGG16	Rep 1	Rep 2	Rep 3	Rep 4	Rep 5	AverageSB+Clip	55.76%	56.8%	56.38%	56.35%	56.32%	56.32%Our1	67.43%	65.12%	65.14%	65.96%	63.57%	65.44%Our2	67.19%	61.17%	60.97%	64.75%	60.90%	62.99%comparison; the training duration is long enough so that LB and SB have attained 100% trainingaccuracy and close-to-0 training loss long before the end of training (the exception here is “SB +Noise” method; see Appendix A for the details); Second, to facilitate convergence to local minima forour methods 1 and 2, we remove heavy-tailed noise for last final 5,000 iterations and run LB instead4.
Table A.1: Test accuracy (percentage) and expected sharpness of different methods across differenttasks. The reported numbers are the averages and 95%CI over 5 replications.
Table A.2: Hyperparameters for training in the ablation studyHyperparameters	FashionMNIST, LeNet	SVHN, VGG11	CIFAR10, VGG11learning rate	0.05	0.05	0.05batch size for gSB	100	100	100training iterations	10,000	30,000	30,000gradient clipping threshold	5	20	20c	0.5	0.5	0.5α	1.4	1.4	1.4Table A.3: Sharpness of different methods across different tasks. The reported numbers are theaverages over 5 replications.____________________________________________________________________________PAC-Bayes Sharpness	Corrupted FMNIST, LeNet	SVHN, VGG11	CIFAR10, VGG11LB	5.9 X 103	2.97 × 104	4.87 × 104SB	3 × 103	6.9 × 103	7.2 × 103SB + Clip	3.3 × 103	7.3 × 103	6.8 × 103SB + Noise	3.1 × 103	7.76 × 104	6.74 × 104Our 1	1.9 × 103	2.1 × 103	4.8 × 103Our 2	1.6 X 103	2.3 × 103	5.8 × 103Maximal Sharpness	Corrupted FMNIST, LeNeT	SVHN, VGG11	CIFAR10, VGG11LB	1.01 × 104	3.78 × 104	5.46 × 104SB	4.9 × 103	9.1 × 103	8.5 × 103
Table A.3: Sharpness of different methods across different tasks. The reported numbers are theaverages over 5 replications.____________________________________________________________________________PAC-Bayes Sharpness	Corrupted FMNIST, LeNet	SVHN, VGG11	CIFAR10, VGG11LB	5.9 X 103	2.97 × 104	4.87 × 104SB	3 × 103	6.9 × 103	7.2 × 103SB + Clip	3.3 × 103	7.3 × 103	6.8 × 103SB + Noise	3.1 × 103	7.76 × 104	6.74 × 104Our 1	1.9 × 103	2.1 × 103	4.8 × 103Our 2	1.6 X 103	2.3 × 103	5.8 × 103Maximal Sharpness	Corrupted FMNIST, LeNeT	SVHN, VGG11	CIFAR10, VGG11LB	1.01 × 104	3.78 × 104	5.46 × 104SB	4.9 × 103	9.1 × 103	8.5 × 103SB + Clip	5.4 × 103	9.3 × 103	8 × 103SB + Noise	5.4 × 103	1.19 × 105	1.18 × 105Our 1	3.2 × 103	2.5 × 103	5.8 × 103Our 2	2.5 × 103	2.8 × 103	6.5 × 103The heavy-tailed multipliers Zn used in this experiment, whenever heavy-tailed noise is needed, areZn = cWn where Wn are iid Pareto(α) RVs. For each task, we first randomly initialize each model,and then run the 6 candidate methods in parallel starting from the same randomly initialized modelweights for a fair comparison.
Table A.4: Results and 95% CI in the experiments with data augmentation.
Table A.5: Sharpness of solutions obtained by different methods in CIFAR10/100 tasks with dataaugmentation. Numbers reported here are the average of 5 replications.
Table A.6: Power-law Indices Estimation throughout the Training, using PLFIT. All the estimationsare at least 5 for all cases tested in our experiments, and most of the times the estimation is above10. This means that even under the assumption that the gradient noises were from a heavy-taileddistribution, they should have much lighter tails than any α-stable distribution (which requires α < 2)or the heavy-tailed noises We injected during tail inflation experiments (α = 1.4).
Table L.1: Summary of notations frequently used in Section G[k]	{1,2,...,k}η	Learning rate (gradient descent step size)b	Truncation threshold of stochastic gradientAn accuracy parameter; typically used to denote an -neighborhood of si, miδ	A threshold parameter used to define large noisese	A constant defined for eq.(G.28)-eq. (G.29). Since e < ∈o, in eq. (G.8) the claim holdsfor |x - y| < e. Note that the value of the constant e does not vary with our choice ofη, , δ.
Table M.1: Power-law Indices Estimation throughout the Training, using PLFIT. All the estimationsare at least 5 for all cases tested in our experiments, and most of the times the estimation is above10. This means that even under the assumption that the gradient noises were from a heavy-taileddistribution, they should have much lighter tails than any α-stable distribution (which requires α < 2)or the heavy-tailed noises we injected during tail inflation experiments (α = 1.4).
