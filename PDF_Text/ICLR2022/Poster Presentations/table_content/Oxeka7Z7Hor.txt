Table 1: ModelNet10 classification results of our method in comparison with PointNet and PointNet++. Weshow two variants of competitor training: 1., on a set of points that matches the memory footprint of the GM,and 2., trained on the same GM inputs. The number of Gaussian (G) and the number of points (P) are indicatedin the header for each column.
Table 2: Accuracy and training time on the ModelNet10 data set for our model wrt. the number of layers. Ournetwork benefits from additional layers. However, accuracy saturates after five layers. While training times inour proof-of-concept system are not competitive, GMCNs leave much room for optimizations in future work.
Table 3: Evaluation of our proposed fitting algorithm. We find that TreeHEM achieves lower errors than themodified EM algorithm when fitting the output of different layers. Nonetheless, accuracy is similar. Moreover,we can see that the EM algorithm runs out of memory when using 128 input Gaussians and a batch size of 21.
Table 4: Accuracy of our model on the ModelNet10 data set for input GM generated using different algorithms.
Table 5:	Minimal memory footprint for the network shown in Figure 1, using Equations 10, if No = 2Np . Thetotal number of Gaussians for each convolution module is given as G. Theoretical memory usage in megabytesis given in columns M2D and M3D , using 2D and 3D Gaussians, respectively.
Table 6:	Minimal memory footprint for network shown in Figure 1, using Equations 10, if implemented withoutcombined convolution-fitting. The total number of Gaussians for each convolution module is given as G.
Table 7: Computation time for various fitting methods on a data set containing 800 mixtures, 256 Gaussians each.
Table 8: ModelNet10 with TreeHem(T = 2) fitting and batch size 14, evaluating varying network lengths.
Table 9: ModelNet10 with TreeHem(T = 2) fitting and batch size 21, testing number of input Gaussians. Forall layouts, the number of kernels is 3,336, and the number of parameters is 216,841.
Table 10: Comparing different fitting methods in a 1/64 → 8/32 → 16/16 → 32/8 → 10 GMCN with batch size21 on ModelNet10. We report the RMSE of each fitting against the ground truth in each layer.
Table 11: Comparing different fitting methods in a 1/128 → 8/64 → 16/32 → 32/16 → 64/8 → 10 GMCN withbatch size 21 on ModelNet10. We report the RMSE of each fitting against the ground truth in each layer.
Table 12: Comparing different input fitting methods in a 1/128 → 8/64 → 16/32 → 32/16 → 64/8 → 10 GMCNwith batch size 21 on ModelNet10.
Table 13: Different batch sizes and training epochs on our best-performing GMCN with four internal convolutionlayers (1/128 → 8/64 → 16/32 → 32/16 → 64/8 → 40) for ModelNet40.
Table 14: Results, parameters and training time for training and testing a GMCN model with a total of fiveconvolution layers (1/128 → 8/64 → 16/32 → 32/16 → 64/8 → 128/4 → 40) on ModelNet40.
