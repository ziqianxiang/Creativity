Table 1: Detailed configurations of the SPIRAL BASE and Large models.
Table 2: Comparison of pre-training cost between wav2vec 2.0 and SPIRAL.
Table 3: ASR results fine-tuned from low-resource train-clean-100. Language models used in de-coding are listed in LM. We compare SPIRAL Base pre-trained on LS-960 and SPIRAL Largepre-trained on LL-60k with previous methods. We report WER (%) on Librispeech dev/test sets.
Table 4: ASR results fine-tuned from high-resource LS-960. Language models used in decoding are listed in LM. We compare SPIRAL Large pre-trained on Libri-Light (LL-60k) with previous methods. We report WER (%) on Librispeech dev/test sets.						Model	Unlabeled	LM	dev		test		data		clean	other	clean	otherSupervised ContextNet (Han et al., 2020a)	-	LSTM	1.9	3.9	1.9	4.1Conformer (Gulati et al., 2020)	-	LSTM	2.1	4.3	1.9	3.9Semi-supervised CTC Transf. + PL (Synnaeve et al., 2020a)	LL-60k	CLM+Transf.	2.10	4.79	2.33	4.54S2S Transf. + PL (Synnaeve et al., 2020a)	LL-60k	CLM+Transf.	2.00	3.65	2.09	4.11Iter. pseudo-labeling Xu et al. (2020)	LL-60k	4-gram+Transf.	1.85	3.26	2.10	4.01Noisy student (Park et al., 2020b)	LL-60k	LSTM	1.6	3.4	1.7	3.4Self-supervised wav2vec 2.0 LARGE (Baevski et al., 2020b)	LL-60k	-	2.1	4.5	2.2	4.5SPIRAL Large frozen (ours)	LL-60k	-	4.0	6.2	3.5	6.4SPIRAL Large (ours)	LL-60k	-	2.1	4.3	2.2	4.6wav2vec 2.0 LARGE (Baevski et al., 2020b)	LL-60k	Transf.	1.6	3.0	1.8	3.3SPIRAL Large (ours)	LL-60k	Transf.	1.5	3.1	1.8	3.5on test-other, which is on par with wav2vec 2.0 Base. This suggests that SPIRAL indeed learnsmeaningful high-level representations in a self-supervised way. When we fine-tune the whole Basemodel, the model achieves WER of 5.4% and 11.2% on test-clean and test-other respectively, outper-forming wav2vec 2.0 Base with 11.5% and 15.8% relative WER reduction. When decoding withTransformer LM, the Base model achieves WER of 2.7% and 6.1% on test-clean and test-otherrespectively. The results are on par with wav2vec 2.0 Base.
Table 5: Evaluation on noise-robustness of the models. We use wav2vec 2.0 Base released by theauthors as the baseline. The SPIRAL Base models are pre-trained with LS-960 and fine-tuned withtrain-clean-100. We report WER (%) on Librispeech and CHiME-3 real data test sets.
Table 6: ASR results fine-tuned from low-resource train-clean-100 and high-resource train-960. Themodel units and language models for decoding are listed in fine-tuning units and LM respectively.
Table 7: Evaluation on noise-robustness of the models. We use wav2vec 2.0 Base released by theauthors as the baseline. The SPIRAL Base models are pre-trained with LS-960 and fine-tuned withtrain-clean-100. We report WER (%) on Librispeech test sets and synthetic noisy Librispeech testsets at0-30 dB (NS-Librispeech).
Table 8: Ablation studies of input perturbation with SpecAugment and computation noise on teacher.
Table 9: Ablation studies of predictor and projection head in SPIRAL. We apply SPIRAL Basefine-tuned with train-clean-100, and report WER (%) on the LibrisPeech dev-other set.
