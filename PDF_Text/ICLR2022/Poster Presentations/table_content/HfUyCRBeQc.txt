Table 1: Mean accuracy over 500 models trained over changes to random initialization and leave-one-outdifferences in training data. German Credit stands as an outlier due to its small sample size (|D| = 800).
Table 2: Percentage of points with disagreement between at least one pair of models (pflip >0) trained withdifferent random seeds (RS) or leave-one-out differences (LOO) in training data, for single models (n= 1)and selective ensembles (n> 1). Results are averaged over 10 runs of creating 24 selective ensemble models,standard deviations are in Appendix F. Selective ensemble results are together, as there is no disagreement.
Table 3: Accuracy and abstention rate of selective ensembles, with n ∈ {5,10,15,20} constituents. Resultsare averaged over 24 randomly selected models; standard deviations are given in Table 8 in Appendix F01	5	10	15	20	1	5	10	15	20FMNIST(a)Taiwanese105(b)Taiwanese1	5	10	15	20Figure 3:	Figure a: Percentage of test data with non-zero disagreement rate in normal (i.e., not selective)ensembles. Horizontal axis depicts ensemble size. Figure b: Average Spearman’s Ranking coefficient,ρ, (For FMNIST, SSIM) between feature attributions for saliency maps generated for each individual testpoint (y-axis) over number of ensemble models (x-axis). The lines indicated with (Sel) in the legend arethe same metrics for selective ensembles.
Table 4: Average top-5 intersection, Spearman’s Rank Correlation Coefficient (ρ), and Pearson’sCorrelation Coefficient (r) to demonstrate attribution inconsistency on the same test points across differentmodels. As a baseline, we compare against differences observed on different points in the same model.
Table 5: The percentage of points with disagreement between at least one pair of models (pflip >0) trainedwith different random seeds (RS) or leave-one-out differences in training data, for singleton models (n= 1)and selective ensembles (n> 1). Results for selective ensembles all selective ensembles are shown together,as they all have no disagreement. Note that these results are for α=0.01. But this different α also leadsto zero disagreement between predicted points.
Table 6: Accuracy (above) and abstention rate (below) of selective ensembles with n ∈ {5,10,15,20}constituents. Results are averaged over 24 models, standard deviation is presented. Note that these resultsare for α = 0.01.
Table 7: Percentage of points with disagreement between at least one pair of models (pflip > 0) trainedwith different random seeds (RS) or leave-one-out differences in training data, for singleton models (n= 1)and selective ensembles (n> 1). We present the mean and standard deviation of this percentage over 10runs of re-sampling ensemble models. Note that these results are for α=0.05 and α = 0.01, since bothresulted in zero inconsistent prediction over predicted points.
Table 8: Accuracy (above) and abstention rate (below) of selective ensembles with n ∈ {5,10,15,20}constituents. Results are averaged over 24 models, standard deviation is presented. Note that these resultsare for α=0.05, which are presented in the main paper.
Table 9: We present the selective ensemble accuracy and abstention rate group-by-group for severaldifferent demographic groups across four datasets: Adult, German Credit, Taiwanese Credit, and WarfarinDosing. We note that by and large, using selective ensembles did not exacerbate accuracy disparity byvery much (within 1% of the original disparity), although they did not ameliorate disparities in accuracythat already existed within the performance of the algorithm. The only exception to this was GermanCredit, where we note, as in the remainder of our results, that the entire dataset is only 1000 points, soresults may be slightly different in this regime. Overall, we note that subgroup abstention rates can varyby dataset, and so it should be studied whenever selective ensembles are used in a sensitive setting.
