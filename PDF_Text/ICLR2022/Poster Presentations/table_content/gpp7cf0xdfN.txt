Table 1: The performance comparison amongDO, DS and CDD-RED on the testing dataset.
Table 2: The d(x, xRED ), PAbenign, and PAadv performanceof the denoisers on the unforeseen perturbation type AutoAttack,Feature Attack, and Adaptive Attack.
Table A1: The performance comparison among DO, DS, and CDD-RED on the CIDAR-10 dataset.
Table A2: The performance comparison among DO, DS, and CDD-RED on the CIDAR-10 dataset.
Table A3: The performance comparison among DO, DS, and CDD-RED on Wasserstein minimizedattackers.	_____________________________________________	DO	DS	CDD-REDd(x, xRED)	9.79	17.38	-1T66-PAbenign	92.50%	96.20%	97.50%-PAadv	35.00%	37.10%	37.50%C.3 Performance on attacks against smoothed classifiersWe further show the performance on the attack against smoothed classifiers, which is an unseen attacktype during training. A smoothed classifier predicts any input x using the majority vote based onrandomly perturbed inputs N(x, σ2I) Cohen et αl.(2019). Here We consider the 10-step PGD-'∞attack generation method with the perturbation radius = 20/255, and σ = 0.25 for smoothing.
Table A4: The performance comparison among DO, DS, and CDD-RED on the PGD attack againstsmoothed classifiers. __________________________________________________________________	DO	DS	CDD-REDd(x, xRED)	15.53	22.42	-15.89PAbenign	68.13%	70.88%	76.10%-PAadv	58.24%	58.79%	61.54%D Computation cost of REDWe measure the computation cost on a single RTX Titan GPU. The inference time for DO, DS, andCDD-RED is similar as they use the same denoiser architecture. For the training cost, the maximum15Published as a conference paper at ICLR 2022training epoch for each method is set as 300. The average GPU time (in seconds) of one epoch forDO, DS, and CDD-RED is 850, 1180, and 2098, respectively. It is not surprising that CDD-RED isconducted over a more complex RED objective. Yet, the denoiser only needs to be trained once toreverse-engineer a wide variety of adversarial perturbations, including those unseen attacks duringthe training.
Table A5: The performance of CDD-RED using a different pretrained classifier f (either Res50 or一 一	一Z	............. .	?	______R-Res50) compared with the default setting f =VGG19.
Table A6: Ablation study on the selection ofT (T = Tand without (w/o) T)) for training CDD-RED,compared with T = {t ∈ T∣ F(t(x)) = F(X), F(t(x0)) = F(x0). }	T = T —	w/o Td(X, XRED) (1 is better)	15.52 (↑ 2.48)	13.50 (↑ 0.46)PAbenign (↑ is better)	83.64% (12.07%)	84.04% (11.67%)PAadv (↑ is better)	75.92% (14.51%)	79.99% (10.44%)As data augmentation might alter the classifier,s original decision in (4), we study how T affects theRED performance by setting T as the original data, i.e., without data augmentation, and all data, i.e.,T = T. Table A6 shows the performance of different T configurations, compared with the defaultsetting. The performance is measured on the testing dataset. As we can see, using all data or original16Published as a conference paper at ICLR 2022data cannot provide an overall better performance than CDD-RED. That is because the former mightcause over-transformation, and the latter lacks generalization ability.
Table A7: Ablation study on the regularization parameter λ (0, 0.0125, and 0.05) for CDD-REDtraining, compared with λ=0.025.
Table A8: RED performance for PGD with different hyperparameter settings, including stepsize as4/255 and 6/255, and with and without RI.
Table A9: The tranferability of RED-synthesized perturbations.
