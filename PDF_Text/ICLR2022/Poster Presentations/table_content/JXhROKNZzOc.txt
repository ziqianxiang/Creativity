Table 1: Results of data-free methods withResNet18 and ResNet50. “No BP” means that noback-propagation algorithm is used to generatedata, “No FT” means no fine-tuning (retraining)for weight quantization.
Table 2: Results of data-free methods with Incep-tionV3, SqueezeNext, and ShuffleNet.
Table 3: SQuant, ZeroQ and GDFQ 4-bit quantization time on GPU A100than the lower-bit quantization does because of higher precision. The benefit of SQuant becomes moreprominent as the bit-width decreases. SQuant outperforms the PTQ methods, i.e., DFQ, ZeroQ, andDSG, more than 30% on all models with 4-bit quantization. It is noteworthy that SQuant surpassesGDFQ in all cases and even surpasses more than 15% in ResNet50 under 4-bit quantization, althoughGDFQ is a quantization-aware training method.
Table 4: SQuant ablation results with				Table 5: ResNet18 results of SQuant, ZeroQ and DSG					ResNet18.				with AdaRound. “No SD” means no synthetiC data. 8					Published as a conference paper at ICLR 2022(i.e., rounding), and combining them leads to higher accuracy for ResNet18. SQuant-E&C has alower accuracy than SQuant-E&K because SQuant-C has a more significant approximation error thanSQuant-K. On the other hand, SQuant-E alone is not optimal because it uses a smaller granularityand ignores a large amount of Hessian information as we analyze in Section 3. This ablation studyshows that SQuant-E&K&C achieves the best accuracy by exploiting the most Hessian information(H-C), and SQuant-E&K also achieves a higher accuracy with H-K than SQuant-E with H-E.
Table 6: ResNet18 results under 4-bit weight-only quantization. “Flipped” is the number of theFlipped elements after SQuant optimization. “Correct” is the number of elements has the sameoptimization direction as the precise objective. AP is the approximation precision.
