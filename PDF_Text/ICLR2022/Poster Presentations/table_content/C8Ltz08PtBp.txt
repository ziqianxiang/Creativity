Table 1: Common hyperparameters for SPL-DQN, NC-QR-DQN, NDQFN, and QR-DQN.
Table 2: Common hyperparameters across SPL-DQN, QR-DQN, IQN, FQF, NC-QR-DQN, MM-DQN, and NDQFNHyPerParameter	ValUeOptimizer	AdamLearning rate	0.001Batch size	32DiscoUnt factor (γ)	0.99Initial -greedy	0.3Minimal -greedy	0.1Training ePisodes	800SamPling qUantiles nUmber (QR based methods)	8SamPles nUmber (MM-DQN)	8Feature Extractor hidden size	[128, 128]Additional results As NDQFN also learns continuous quantile functions, the number of quantilesamples can be enlarged when computing the QR loss. Here we train NDQFN with bin numberK = 8 and quantile number N = 24. The training curve is labeled by ’NDQFN1’ as shown inFigure 7. The curves labeled by ’SPL1’, ’SPL’, and ’NDFQN’ are taken from Figure 3.
Table 3: Noise settings for different environments in PyBulletGymEnvironments	NoiseInVertedPendUlUm	N(0, 0.02)InvertedDoublePendulum	N(0, 0.01)InvertedPendUlUmSwingUp	N(0, 0.05)Reacher	N(0, 0.01)Walker2D	N(0, 0.005)HalfCheetah	N(0, 0.005)HalfCheetah1	N(0, 0.008)HalfCheetah2	N(0, 0.01)Ant	N(0, 0.01)Hopper	N(0, 0.003)Humanoid	N(0, 0.003)14Published as a conference paper at ICLR 2022Training hyperparameters Hyperparameters for DDPG and DDPG based models are summarizedin Table 4. The critic also uses an L2 weight decay of 10-2 . The soft target update coefficientis 0.001. Omstein-Uhlenheck noise (OU(μ0, σ0)) (Uhlenbeck & Ornstein, 1930) is combined withactions for exploration in DDPG, where We use μ0 = 0 and σ0 = 0.1.
Table 4: Hyperparameters for DDPG and DDPG based methodsHyperparameter	ValueOptimizer	AdamActor learning rate	10-4Critic learning rate	10-3Batch size	64Discount factor (γ)	0.99Training frames	one millionSampling quantiles number	32Actor hidden size	[400, 300]Critic,s Feature Extractor hidden size	[400, 300]Table 5: Hyperparameters for SAC and SAC based methodsHyperparameter	ValueOptimizer	AdamActor learning rate	3 × 10-3Critic learning rate	3 × 10-3Entropy learning rate	3 × 10-3Batch size	64Discount factor (γ)	0.99Training frames	three million
Table 5: Hyperparameters for SAC and SAC based methodsHyperparameter	ValueOptimizer	AdamActor learning rate	3 × 10-3Critic learning rate	3 × 10-3Entropy learning rate	3 × 10-3Batch size	64Discount factor (γ)	0.99Training frames	three millionSampling quantiles number	32Actor hidden size	[256, 256]Critic,s Feature Extractor hidden size	[256, 256]Additional results We test the best models given by different methods with four random seeds. Theresults are shown in Table 6. We test all DDPG based agents without Ornstein-Uhlenheck noise for0.125 million frames, and SAC based agents for 2.5 thousand episodes. We treat DDPG and SACscores as baselines and scale other methods’ scores by them, i.e.
Table 6: Scaled testing scores across different stochastic environments. Scores are averaged over 4seeds.
