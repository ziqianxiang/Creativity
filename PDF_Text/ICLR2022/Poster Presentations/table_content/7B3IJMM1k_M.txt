Table 1: Summary of notations in this paperSymbol	Definition	Symbol	Definitionl	Layer index	小~	Unweighted PSP1i	Neuron index	sl(t)	Output spikesWl	Weight	Φl(T)	Average unweigthed PSP before time Tal	ANN activation values	zl	Weighted input from l - 1 layert	Time-steps	h(∙)	ReLU functionT	Total time-step	H(∙)	Heaviside step functionθl	Threshold	L	Quantization step for ANNλl	Trainable threshold in ANN	Errl	Conversion Errorml(t)	Potential before firing	l Err	Estimated conversion ErrorVl(t)	Potential after firing	炉	Shift of quantization clip-floor function1 Postsynaptic potentialANN-SNN conversion. The key idea of ANN-SNN conversion is to map the activation value of ananalog neuron in ANN to the firing rate (or average postsynaptic potential) of a spiking neuron inSNN. Specifically, We can get the potential update equation by combining Equation 2 - Equation 4:vl(t) -vl(t- 1) = Wlxl-1(t) - sl(t)θl.	(6)Equation 6 describes the basic function of spiking neurons used in ANN-SNN conversion. Bysumming Equation 6 from time 1 to T and dividing T on both sides, We have:Vl(T) - Vl(0)
Table 2: Comparison between the proposed method and previous works on CIFAR-10 dataset.
Table 3: Comparison between the proposed method and previous works on ImageNet dataset.
Table S1: Comparison between our method and the method of setting the maximum activation.
Table S2: Influence of different quantization steps.
Table S3: Compare with state-of-the-art supervised training methods on CIFAR-10 datasetModel	Method	Architecture	SNN Accuracy	TimestepsCIFAR-10				HC	Hybrid	VGG-16	92.02	200STBP	Backprop	CIFARNet	90.53	12DT	Backprop	CIFARNet	90.98	8TSSL	Backprop	CIFARNet	91.41	5DThIR1	ANN-SNN	CNet	77.10	256Ours	ANN-SNN	VGG-16	93.96	4Ours	ANN-SNN	CIFARNet2	94.73	41 Implemented on Loihi neuromorphic processor2 For CIFARNet, we use the same architecture as Wu et al. (2018).
Table S4: Comparison between the proposed method and previous works on CIFAR-100 dataset.
Table S5: Comparison of the energy consumption with previous worksMethod		ANN	T=2	T=4	T=8	T=16	T=32	T=64	ACCuraCy	77.89%	-	-	-	-	7.64%	21.84%RTS	OP (GFLOP/GSOP)	0.628	-	-	-	-	0.508	0.681	Energy (mJ)	7.85	-	-	-	-	0.039	0.052	ACCuraCy	77.89%	-	-	-	-	73.55%	76.64%SNNC-AP	OP (GFLOP/GSOP)	0.628	-	-	-	-	0.857	1.22	Energy (mJ)	7.85	-	-	-	-	0.660	0.094	ACCuraCy	76.28%	63.79%	69.62%	73.96%	76.24%	77.01%	77.10%Ours	OP (GFLOP/GSOP)	0.628	0.094	0.185	0.364	0.724	1.444	2.884	Energy (mJ)	7.85	0.007	0.014	0.028	0.056	0.111	0.22218Published as a conference paper at ICLR 2022Algorithm 1 Algorithm for ANN-SNN conversion.	Input: ANN model MANN(x; W) with initial weight W; Dataset D; Quantization step L; Initial dynamic thresholds λ; Learning rate . Output： MSNN(x; W)	1	: for l = 1 to MANN.layers do2	: if is ReLU activation then3	:	Replace ReLU(x) by QCFS(x; L, λl)4	: end if5	: if is MaxPooling layer then
