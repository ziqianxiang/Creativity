Table 1: Comparison of different estimators of the softmax kernel on the datapoints from two UCI datasets:wine and Boston in terms of the MSE (measured in 10-3 units). The non-HRF estimators apply 512 randomfeatures and the HRF-ones are set up to match the non-HRFs in terms of the number of FLOPS (for a faircomparison). We also reported standard deviations.
Table 2: Comparison of WERs of Conformer-Transducer applying different RF-mechanisms for the implicitattention. For methods other than clustering-based HRFs (HRF-C), numbers next to method names definethe values of m or (m, n). Method HRF-A stands for the angular hybrid variant. Numbers next to HRF-Ccorrespond to the number of clusters constructed in the query and key space respectively. HRF-C uses 64random features. We also report standard deviations averaged over 10 different training runs.
Table 3: Minimal and maximal empirical MSE with 20 repetitions26Published as a conference paper at ICLR 2022I Language Modeling Training Details and Additional ResultsFor the Language Modeling tasks, we trained a 2-layer LSTM of hidden sizes 200 and 650 on thePennTree Bank (Marcus et al., 1993) and the WikiText2 dataset (Merity et al., 2017) respectively.
Table 4: Results are computed over 10 runs on Penntree Bank Dataset. Boldface denotes the best one, andUnderline denotes the second best. Negative fractions for Favor+ is not reported as it produces positive randomfeatures.
Table 5: Results are computed over 10 runs on the Wikitext2 Dataset. Boldface denotes the best one, andunderline denotes the second best.
