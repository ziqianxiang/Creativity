Table 1: Accuracy on claim verification datasets. #Encoded refers to the number of passages en-coded by a BERT reader to answer a single question.
Table 2: Accuracy on open-domain QA datasets TriviaQA (TQA), ComplexWebQuestions (CWQ)and EntityQuestion (EQ). #Encoded refers to the number of passages encoded by a BERT reader toanswer a question. TQAe-dev corresponds to TQA with train and dev samples limited to those withWikipedia entity as an answer. See Appendix B.3 for full results.
Table 3:	tome-2 retrievals for the second HoVer dev sample. We show top-1 retrieval results for thefirst (-→1) memory attention layer for two passage mentions. Memory mentions are in brackets.
Table 4:	tome-2 retrievals for the first HoVer dev sample. We show top-1 retrieval results for thefirst (-→1) and the second (-→2) memory attention layers for passage mentions “Life Goes On” and“Hungry”3. Memory mentions are in brackets. The first retrieval for the “Life Goes On” is a differentsong with the same name and the first retrieval for “Hungry” is related but not useful. However, thesecond retrieval for “Life Goes On” identifies the correct song and describes its position on thealbum while the second retrieval for “Hungry” captures its position relative to “Life Goes On”.
Table 5: Accuracy on held-out subset of Trivi-aQA and ComplexWebQuestions (CWQ) ques-tions. tome-1-unseen was pre-trained and fine-tuned with memory without entities from held-out set and evaluated with full memory. Notethat performance is considerably lower than onthe full dev set as answers in the held-out set(which are in dev but not train) are more likelyto be rare entities.
Table 6: Accuracy on claim verification datasets. #Encoded refers to the number of passages en-coded by a BERT reader to answer a single question. EaE stands for Entities as Experts model.
Table 7: Accuracy on FM2 compared with original dataset baselines. Oracle refers to oracle retrievalfollowed by a BERT-Base reader.
Table 8: EntityQuestions recall@20Table 9: EntityQuestions top-1 accuracyModel	Recall@20	Model	AccuracyDPR (Sciavolino et al., 2021)	65.4	Entities as Experts	32.5BM25 (Sciavolino et al., 2021)	71.2	REALM	59.0TOME-1	83.3	tome-1	62.1tome-2	83.8	tome-2	66.013Published as a conference paper at ICLR 2022Table 10: Performance ablations for pre-training objectives experiments.
Table 9: EntityQuestions top-1 accuracyModel	Recall@20	Model	AccuracyDPR (Sciavolino et al., 2021)	65.4	Entities as Experts	32.5BM25 (Sciavolino et al., 2021)	71.2	REALM	59.0TOME-1	83.3	tome-1	62.1tome-2	83.8	tome-2	66.013Published as a conference paper at ICLR 2022Table 10: Performance ablations for pre-training objectives experiments.
Table 10: Performance ablations for pre-training objectives experiments.
Table 11: Proportion of time spent on ANNS for tome-1 pre-training setting.
Table 12: tome-2 retrievals for the second HoVer dev sample. We show top-1 retrieval results for thefirst (-→1) memory attention layer for passage mentions “the novel”, “the movie” and “the album”.
