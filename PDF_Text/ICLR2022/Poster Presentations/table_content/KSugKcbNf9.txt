Table 1: Aggregated performance on subsets of datasets with 30 training samples. Our novelPFN-BNN performs strongest overall. See Table 7 in the appendix for per-dataset results.
Table 2: Comparison of meta-learning algorithms in terms of test accuracy on the Omniglot dataset.
Table 3: Tabular datasets used for evaluation. Taken from the OpenML AutoML Benchmark (filteredfor Nf eatures â‰¤ 100, no missing values.)Name	Number of Features	Number of Symbolic Featureshaberman	4	2ionosphere	35	1sa-heart	10	2cleve	14	9four	2	0german	24	0Table 4: Tabular datasets used to find good general hyperparameters for hyper-priors as well ashyperparameters used in baseline cross-validation.
Table 4: Tabular datasets used to find good general hyperparameters for hyper-priors as well ashyperparameters used in baseline cross-validation.
Table 5: Hyperparameters considered during grid search tuning of the PFN-BNN on validationdatasets. The activation functions refer to the activation function used in the data generating BNNand not the ones used in the PFN transformer. Scaled_beta_sampler_f(a,b,max,min) refers to samplingfrom a beta distribution with parameters a, b (numpy.random.beta), scaling the output range from [0,1] to [max, min] and then rounding the output to the integer range. beta_sampler_f(a, b) refers to afunction that samples from the beta distribution without scaling and rounding.
Table 6: Hyperparameters used in cross-validation for each baseline method.
Table 7: ROC AUC Comparison: Evaluation is made on subsets of datasets with 30 training samples.
