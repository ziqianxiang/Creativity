Table 1: Comparison of representative related models and our DAB-DETR. The term “Learn An-chors?” asks if the model learns 2D points or 4D anchors as learnable parameters directly. Theterm ”Reference Anchors” means if the model predicts relative coordinates with respect to a ref-erence points/anchors. The term “Dynamic Anchors” indicates if the model updates its anchorslayer-by-layer. The term “Standard Attention” shows whether the model leverages the standarddense attention in cross-attention modules. The term “Object Scale-Modulated Attention” means ifthe attention is modulated to better match with object scales. The term “Size-Modulated Attention”means if the attention is modulated to better match with object scales. The term “Update SpatialLearned Queries?” means if the learned queries are updated layer by layer. Note that Sparse RCNNis not a DETR-like architecture. we list it here for their similar anchor formulation with us. See Sec.
Table 2: Results for our DAB-DETR and other detection models. All DETR-like models exceptDETR use 300 queries, while DETR uses 100. The models with superscript * use 3 pattern embed-dings as in Anchor DETR (Wang et al., 2021). We also provide stronger results of our DAB-DETRin Appendix G and Appendix C.
Table 3: Ablation results for our DAB-DETR. All models are tested over ResNet-50-DC5 backboneand the other parameters are the same as our default settings.
Table 4: GPU memory usage of each model.
Table 5: Comparison of the results of Deformable DETR and DAB-Deformable-DETR. The modelsin row 1 and row 2 are copied from the original paper, and the models in row 3 and row 4 are testedunder the same standard R50 multi-scale setting. Deformable DETR+ means the Deformable DETRmodel with iterative bounding box refinement and the result of Deformable DETR+ (open source)is reported by us using the open-source code. The only difference between row 3 and row 4 is theformulation of queries.
Table 6: Comparison of models with different temperatures. All models are trained with the ResNet-50 backbone, batch size 64, no multiple pattern embeddings, and no modulated attentions. DefaultSettings are used for the rest of the parameters.
Table 7: Comparison of models with different number of decoder layers. All models are trainedunder our standard ResNet-50-DC setting except the number of decoder layers.
Table 8: Comparison of DAB-DETR and DAB-DETR with fixed anchor centers x, y. When fixingx, y of queries with random values, the performance of the models is improved consistently. Themodels with superscript * use 3 pattern embeddings as in Anchor DETR.
Table 9: Comparison of the runtime of DETR, Conditional DETR, and our proposed DAB-DETR.
