Table 1: Comparisons among five NAS benchmarks. Existing benchmarks are either built on smalldatasets for image classification, or trained with a single simplified setting. In contrast, our NAS-Bench-MR is built on four widely-used visual recognition tasks and various realistic settings. Thearchitectures in our NAS-Bench-MR are trained following the common practices in real-world sce-narios, e.g., 512 × 1024 and 500 epochs on the CityScapes dataset (Cordts et al., 2016). It takesabout 400,000 GPU hours to build our benchmark Using Nvidia V100 GPUs.________________________________________Benchmarks	Datasets	Tasks	Scales	Epochs	Input Sizes	Settings per TaskNAS-Bench-101 (Ying et al., 2019)	Cifar-10	1	108	4/12/36/108	32 × 32	different training epochsNAS-Bench-201 (Dong & Yang, 2020)	ImageNet-16	1	104	200	16 × 16	single settingNAS-Bench-301 (Siems et al., 2020)	Cifar-10	1	1018	100	32 × 32	single settingTransNAS-Bench-101 (Duan et al., 2021)	Taskonomy	7	103	≤30	256 × 256	single settingNAS-Bench-MR (Ours)	ImageNet, Cityscapes, KITTI, HMDB51	4	1023	≥100	224 × 224 512 × 1024	different image sizes, data scale, number of classes, epochs, pretrainingMei et al., 2020; Guo et al., 2020; Dai et al., 2020) are designed for image classification usingeither a single-branch structure with a group of candidate operators in each layer or a repeated cellstructure, e.g., Darts-based (Liu et al., 2019b) and MobileNet-based (Sandler et al., 2018) searchspaces. Based on these spaces, several NAS benchmarks (Ying et al., 2019; Dong & Yang, 2020;Siems et al., 2020; Duan et al., 2021) have been proposed to pre-evaluate the architectures. However,the above search spaces and benchmarks are built either on proxy settings or small datasets, suchas Cifar-10 and ImageNet-16 (16 × 16), which is less suitable for other tasks that rely on multi-scale information. For those tasks, some search space are explored in segmentation (Shaw et al.,
Table 2: Performance comparison (%) on the Cityscapes validation set. All hyperparameter op-timization methods are searched using our NAS-Bench-MR. All models are trained from scratch.
Table 3: Performance (%) on five datasets of the models searched on different optimization objec-tives (including single- and multi-task optimization) using our NCP, e.g.“Cls + Seg” denotes themodel is searched using “Cls-A” and “Seg” benchmarks, “Four Tasks” denotes the model is propa-gated by using the predictors trained on the all four benchmarks (Cls-A, Seg, 3dDet, Video). Notethat in addition to cross-task evaluation, we also show the generalizability of NAS by applying thesearched model to a new dataset, ADE20K, which is not used to train neural predictors. For clearercomparisons, the FLOPs of all networks is measured using input size 128 × 128 under the segmen-tation task. The top-2 results are highlighted in bold.
Table 4: Our architecture transfer results between four different tasks. We first find an optimalarchitecture coding for each task and then use it as the initial coding to search other three tasks. “-F”and “-T” denote the architecture finetuning and transferring results, respectively.
Table 5: Single-crop top-1 error rates (%) on the ImageNet validation	Model	Params	FLOPs	ImNet-A	ImNet-B	ImNet-C	ResNet50	23.61M	4.12G	83.76	50.16	86.00set. ‘ImNet-A’, ‘ImNet-B’, ‘ImNet-						C’ denote the predictor is trained on benchmarks of ImageNet-50- 1000, ImageNet-50-100, ImageNet-	ResNet101 HRNet-W32 HRNet-W48	42.60M 39.29M 75.52M	7.85G 8.99G 17.36G	84.32 84.00 84.52	51.92 52.00 52.88	86.00 83.80 86.60	NCP-Net-A	4.39M	5.95G	85.80	56.04	86.8010-1000, respectively. FLOPs is						measured using input size 224 × 224.	NCP-Net-B	3.33M	3.55G	82.76	56.40	85.60λ = 0.7. Top-2 results are high-	NCP-Net-C	4.06M	5.51G	84.60	55.60	87.80lighted in bold.	NCP-Net-ABC	4.28M	6.72G	85.12	58.12	88.205 ConclusionThis work provides an initial study of learning task-transferable architectures in the network codingspace. We propose an efficient NAS method, namely Network Coding Propagation (NCP), whichoptimizes the network code to achieve the target constraints with back-propagation on neural predic-tors. In NCP, the multi-task learning objective is transformed to gradient accumulation across mul-tiple predictors, making NCP naturally applicable to various objectives, such as multi-task structureoptimization, architecture transferring across tasks, and accuracy-efficiency trade-offs. To facili-tate the research in designing versatile network architectures, we also build a comprehensive NASbenchmark (NAS-Bench-MR) upon a multi-resolution network space on many challenging datasets,enabling NAS methods to spot good architectures across multiple tasks, other than classification asthe main focus of prior works. We hope this work and models can advance future NAS research.
Table 6: Detailed statistics of our NAS-Bench-MR. NAS-Bench-MR contains 4 datasets and 9 set-tings. Considering the diversity and complexity of real-world applications (e.g.different scales/inputsizes of training data), we use a variety of challenging settings (e.g., full resolution and trainingepochs) to ensure that the model is fully trained. For classification, we train models with differentnumbers of classes, numbers of training samples, and training epochs. For semantic segmentation,we train models under different input sizes. We also evaluated video action recognition modelsunder two settings: trained from scratch and pretrained with ImageNet-50-1000. f denotes eachsample contains multiple classes. ∣ denotes there are three basic classes (car, pedestrian, cyclist)in KITTI, while for each object we also regress its 3D location (XYZ), dimensions (WHL), andorientation (α). “N” 一 training from scratch. “Y” 一 training with the ImageNet-50-1000 pretrainedmodel. We also give the mean, std, and the validation L1 loss (%) of the neural predictor under themain evaluation metric of each setting.
Table 7: Representations of our 27-dimensional coding.
Table 8: Comparative results (%) of NCP on the Cityscapes validation set. The first two architec-tures are searched using neural predictors that are trained on the Cityscapes dataset with an inputsize of 512 × 1024 (Seg) and the resized Cityscapes dataset with an input size of 128 × 256 (Seg-4x),respectively. The last model is searched by joint optimization of both the two predictors. λ is set to0.5 during the network codes propagation process. All models are trained from scratch. FLOPs ismeasured using 512 × 1024.
Table 9: Video recognition results (%) of NCP on the HMDB51 dataset. The first two models aresearched using neural predictors that are trained on HMDB51 from scratch (Video) and using modelsof ImageNet-50-1000 as pretraining (Video-p). The last model is searched by joint optimizationof both two predictors. We also show the classification accuracy of the pretrained model on theImageNet-50-1000 dataset (the column of “Cls-Pretraining”). Note that the last 2D convolutionallayer before the final classifier is replaced with a 3D convolutional layer (3 × 3 × 3) for all modelsto capture the temporal information. λ is set to 0.5. FLOPs is calculated using an input size of112 × 112.
Table 10: Searching time of predictor-based searching methods. Our NCP is the fastest as it evalu-ates all dimensions of the code with only one back-propagation without top-K ranking. The time ismeasured on a Tes山 V100 GPU._______________________________________________________________Model	Searching Time	NotesNeural Predictor Wen et al. (2020)	> 1 GPU day	Predict the validation metric of 10,000 random archite- ctures and train the top-10 models for final evaluation.
