Table 1: Test accuracy on Fashion-MNIST and CIFAR10 of different schedulersScheduler	Fashion-MNIST	CIFAR10Constant	93.0 ± 0.2	93.0 ± 0.4Polynomial	93.4 ± 0.1	93.5 ± 0.3Cosine	93.4 ± 0.1	93.6 ± 0.4Hypergradient	93.5 ± 0.2	93.7 ± 0.6SRLS	93.6 ± 0.3	93.6 ± 0.6GNS	94.6 ± 0.2	94.3 ± 0.55.3	Results on Language UnderstandingApart from image classification, we also focus on language understanding tasks on eight GLUEbenchmark datasets. Two models with different sizes, RoBERTa-base and RoBERTa-large, areevaluated and detailed results are reported in Table 2. As expected, adjusting learning rate rather thankeeping a constant one contributes to the test performance. In addition, a consistent performanceboost can be observed in GNS over all eight datasets for both base and large models. Specifically,it achieves a score of 85.2 and 87.8 on average in RoBERTa-base and -large model respectively,while the best baseline in each scenario only reaches 84.5 and 86.9. It should be noticed that SRLSperforms poorly and even cannot surpass the constant scheduling. It is likely that the simple statedesign of SRLS (only using information in the last layer) constrains its ability to adjust the learningrate dynamically, especially for large models like RoBERTa. Thus, SRLS fails to generate a goodscheduling to improve the performance.
Table 2: Test performance of RoBERTa on GLUE benchmarking of different schedulers.
Table 3: Transferred performance of GNS.
Table 4: Test performance of three scheduler variants for RoBERTa-base.
Table 5: Search space for image classificationHyperparameter	Tuning rangeαmax	[10-1, 10-2, 10-3]μ	[10-1, 10-2, 10-3]η2	[0.5, 1.0]p	[1, 2]Table 6: Search space for language understandingHyperparameter	Tuning rangeαmax	[1 X 10-5, 2 X 10-5]η1	[0, 0.025, 0.05, 0.075, 0.1]p	[1, 2]	B.2	Experimental settings on image classificationWe present a detailed hyperparameter configuration for two image classification tasks in Table 7.
Table 6: Search space for language understandingHyperparameter	Tuning rangeαmax	[1 X 10-5, 2 X 10-5]η1	[0, 0.025, 0.05, 0.075, 0.1]p	[1, 2]	B.2	Experimental settings on image classificationWe present a detailed hyperparameter configuration for two image classification tasks in Table 7.
Table 7: Hyperparameter configuration for GLUE benchmarking datasets.
Table 8: Hyperparameter configuration for GLUE benchmarking datasets.
Table 9: Test accuracy of SRLS and GNS under reward collection with p = 0.2.
Table 10: Comparison with the scheduler of a longer trajectory.
Table 11: Comparison between the constant learning rate and transferred GNS for RoBERTa-large.
Table 12: Test accuracy with SGD and Adam as the base optimizer respectively on CIFAR10.
Table 13: Sensitivity of hyperparameters.
Table 14: Comparison of schedulers with different configurations.
Table 15: Development performance of RoBERTa-large on GLUE of different schedulers.
Table 16: Expanded search space for image classificationHyperparameter	Tuning rangeαmax	LogUniform[10-3, 10-1]μ	LogUniform[10-3, 10-1]η2	Uniform[0.5, 1.0]Table 17: Expanded search space for language understandingHyperparameter	Tuning rangeαmax	LogUnifOTrm[10-5, 10-4]η1	Uniform[0.0, 0.1]50 evaluations are conducted in total for CMA-ES and we also train GNS for 50 episodes fora fair comparison. Results are demonstrated in Figure 5. We can see that CMA-ES can find asatisfactory hyperparameter configuration quickly in the beginning, but our GNS can reach a betterfinal performance. When we consider the whole search trajectory, it is still safe to say that theproposed GNS is a relatively efficient and effective method even compared with the more powerfulbaseline CMA-ES.
Table 17: Expanded search space for language understandingHyperparameter	Tuning rangeαmax	LogUnifOTrm[10-5, 10-4]η1	Uniform[0.0, 0.1]50 evaluations are conducted in total for CMA-ES and we also train GNS for 50 episodes fora fair comparison. Results are demonstrated in Figure 5. We can see that CMA-ES can find asatisfactory hyperparameter configuration quickly in the beginning, but our GNS can reach a betterfinal performance. When we consider the whole search trajectory, it is still safe to say that theproposed GNS is a relatively efficient and effective method even compared with the more powerfulbaseline CMA-ES.
Table 18: Performance of GNS with different input features.
Table 19: Performance of GNS with different forms of action.
Table 20: Test accuracy of a 3-layer MLP on Fashion MNIST.
