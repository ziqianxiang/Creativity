Table 1: Validation accuracies on the CIFAR-10 validation set for each experiment with data aug-mentations considered in Section 3. All validation accuracies are averaged over 5 runs.
Table 2: Validation accuracies on the CIFAR-10 validation set for each of the experiments with dataaugmentations considered in Section 3 for multiple modern CNNs.
Table 3: Validation accuracies on the CIFAR-10 validation set for fixed versions of the dataset withno random data augmentations in Section 4. Hyperparameters fixed from the previous section exceptfor SGD marked*, where the learning rate is doubled to 0.2 for a stronger baseline.
Table 4: Experiments considered in Table 1 with additional information in training loss (cross en-tropy loss on the CIFAR-10 training set) and full loss, including regularizations. Validation accu-racies on the CIFAR-10 validation set for each experiment with data augmentations considered inSection 3. Best viewed on screen.
Table 5: Hyperparameter ablation studies for experiments considered in Table 1. Validation accu-racies on the CIFAR-10 validation set for each experiment with data augmentations considered inSection 3. The number of trials for each experiment is included in parentheses.
Table 6: Summary of validation accuracies on the CIFAR-10 validation dataset for baseline types ofgradient noise in experiments with data augmentations considered in Section 3.
Table 7: Numerical Stability of Gradient Computations in several settings for a single sample ofdata, controlling for data augmentations. Shown are total and relative euclidean error averaged over5 gradient computations on a randomly initialized model. For baseline SGD this is the gradient noisebetween two randomly sampled mini-batches at batch size 128 plus numerical effects. For fullbatchgradient descent this is noise produced solely by numerical approximation due to non-determinismwhen computing the gradient by accumulation over the entire dataset.
Table 8: Validation accuracies for each experiment considered in Section 3 for deterministic runs(referring to disabled cudnn non-determinism) and runs in double (fp64) floating point preci-sion. All validation accuracies are averaged over 2-5 runs. These experiments show no significantdifference to the non-deterministic runs in single floating point precision.
