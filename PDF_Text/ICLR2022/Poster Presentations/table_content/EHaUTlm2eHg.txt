Table 1: Performance on the Umbrella-Lengthenvironment. We run our model for 1000episodes steps over 5 random seeds. TheBSuite score is calculated in terms of the regretnormalized [random, optimal] → [0, 1] (higheris better). The number after ± is the standardWe first aim to show that our method is effectivein a simple environment where credit assignment isthe paramount objective. We examine the Umbrella-Length task from BSUITE (Osband et al., 2020), atask involving a long sequential episode where onlythe first observation (a forecast of rain or shine) andaction (whether to take an umbrella) matter, whilethe rest of the sequence contains random unrelatedinformation. A reward of +1 is given at the end ofthe episode if the agent chooses correctly to take theumbrella or not depending on the forecast. This dif-ficult task is used to test the agent’s ability to assigncredit correctly to the first decision. We evaluatePGIF-versions of SAC and PPO to vanilla discrete-
Table 2: Performance on the offline RL tasks showing the average episodic return. The final averagereturn is shown after training the algorithm for 500, 000 episodes and then evaluating the policy over5 episodes. Results show an average of 5 random seeds. The value after ± shows the standard error.
