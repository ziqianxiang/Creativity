Table 1: - log p(x) forvarying depth L (bits/dim).
Table 2: Dynamically binarized MNIST (Burda et al., 2016) and OMNIGLOT (Lake et al., 2013)performance on the test set. All models except for IWAE are trained with a single importancesample. IWAE is trained with 50 importance samples. The marginal loglikelihood is estimatedwith 500 importance samples. Attentive VAE outperforms all state-of-the-art VAEs with or withoutautoregressive components.
Table 3: CIFAR-10 (Krizhevsky et al., 2009) performance on the test set. The marginal log-likelihood is estimated with 100 importance samples. A shallower Attentive VAE outperforms allstate-of-the-art VAEs with or without autoregressive components. Attentive VAE performs on parwith fully autoregressive generative models. However, it permits fast sampling that requires a singlenetwork evaluation per sample as opposed to D, where D the dimension of the data distribution.
Table 4: Comparison of the computational requirements for training deep state-of-the-art VAEmodels. All models are trained on 32GB V100 GPUs. The additional cost for computing theattention scores is compensated by the smaller number of stochastic layers in the hierarchy withoutsacrificing the generative capacity of the model, see Table 3.
Table 5: A comparison of the effect of the attention operations in a deep variational model onthe CIFAR-10 dataset. The NVAE (Vahdat & Kautz, 2020) is used as a baseline case (no attentionoperation).
