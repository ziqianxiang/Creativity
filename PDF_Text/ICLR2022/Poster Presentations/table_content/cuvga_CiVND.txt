Table 1: Percentage of overlappingbetween the 30% most redundantparameters in 5 BERT-base modelsfine-tuned using {1, 5, 8, 10, 20} ×10-5 as learning rates on SST-2.
Table 2: Single task fine-tuning dev results on GLUE. All results are from our implementations. ‘-’denotes missing results.
Table 3: Single task fine-tuning test results from the GLUE evaluation server.
Table 4: Neural machine translation BLEU scores on test set. All results are from our implementation.
Table 5: Image classification test accuracy. Results with * are from Dosovitskiy et al. (2020). ViT-B/32 and ViT-L/32 each denotes ViT-base and ViT-large model with 32 × 32 input patch size.
Table 6: Single task fine-tuning dev results on GLUE.
Table 7: Hyper-parameter configurations for GLUE experiments. “Epoch” refers to the total trainingepochs; we adopt early-stopping strategy in practice. “Dropout” refers to classification layer dropoutratio. “Warmup” refers to the ratio of learning rate linear warmup iterations to total training iterations.
Table 8: Standard deviation of the dev set results.
Table 9: The number of parallel sentences in NMT datasets.
Table 10: Hyper-parameter configurations for NMT experiments. “Warmup” refers to the learningrate linear warmup iterations.
Table 11: Hyper-parameter configurations for ViT experiments on CIFAR100 and ImageNet.
Table 12: Ablation study on parameter sensitivity and local temporal variations.
Table 13: Summary of the GLUE benchmark.
