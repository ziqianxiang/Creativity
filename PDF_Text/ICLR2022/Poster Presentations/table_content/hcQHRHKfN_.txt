Table 1: Types of strategies discovered by each methods inMonster-Hunt over 20 iterations.
Table 2: Population Diversity scores in MuJoCo.					Table 3: Number of visu- ally distinct policies over 4 iterations in SMAC.			H.-Cheetah	Hopper	Walker2d	Humanoid			PG	0.033 (0.013)	0.418(0.125)	0.188 (0.079)	0.965 (0.006)		2c64zg	2m1zDIPG	0.051 (0.009)	0.468 (0.054)	0.179 (0.056)	0.996 (0.000)	PG	2	2PBT-CE	0.160 (0.078)	0.620 (0.294)	0.512 (0.032)	0.999 (0.000)	DIPG	2	2DvD	0.275 (0.164)	0.656 (0.523)	0.542 (0.103)	1.000 (0.000)	PBT-CE	2	3SMERL	0.003 (0.002)	0.674 (0.389)	0.669 (0.152)	N/A	TrajDiv	3	1RSPO	0.359 (0.058)	0.989 (0.009)	0.955 (0.039)	0.999 (0.000)	RSPO	4	4training process. We remark that even without the smoothed-switching technique, RSPO achievescomparable performance with RPG â€” note that RPG requires a known reward function while RSPOdoes not assume any environment-specific domain knowledge.
Table 4: Strategies induced by baseline methods and RSPO in SMAC map 2c_vs_64zg.
Table 5: Strategies induced by baseline methods and RSPO in SMAC map 2m_vs_1z.
Table 6: Final evaluation performance of RSPO averaged over 32 episodes in MuJoCo continuouscontrol domain. Averaged over 3 random seeds with standard deviation in the brackets.
Table 7: Final evaluation winning rate of RSPO averaged over 32 episodes in SMAC.
Table 8: Population Diversity on the hard map 2c_vs_64zg in SMAC.
Table 9: Population Diversity scores of the first 2 policies with different hyperparameters in Humanoid.
Table 10: The number of distinct strategies discovered by PGA-MAP-Elites and RSPO and thehighest achieved rewards in 4-Goals Hard. Numbers are averaged over 3 seeds.
Table 11: PPO hyperparameters for different experiments.
Table 12: RSPO hyperparameters for different experiments.
