Table 1: Results of CoNLL and GLUE benchmark. G-Avg means the average of GLUE tasks,A-Avg denotes the average of all tasks, including CoNLL. Our FWSVD+fine-tuning is the bestperformer in terms of both average scores, without the expensive generic pre-training required bypath-1 models (e.g., DistillBERT costs 720 V100 GPU hours for training).
Table 2: Results of compressing an already compact model. The original task-specific models aredirectly downloaded from Huggingface pretrained models. Our FWSVD successfully reduces moreparameters from all the compact models, while achieving the same level of accuracy. (ft: fine-tuning)Original Compact Model(St)	∣∣	Path-3 Compression (St → Stf)Model-Task	#Param.	Perf.	#Param.	SVD	SVD+ft.	FWSVD	FWSVD+ft.
Table 3: Results of compressing an already compact model. This table compresses ALBERT (Lanet al., 2019), which uses the parameter-sharing strategy to create the compact model. FWSVD pre-serves the performance significantly better than SVD in all 8 tasks, indicating its excellent compati-bility in combining the parameter-sharing strategy. This experiment examines the path-3 process.
Table 4: The raw values for Figure 6a. We additionally include the averaged singular values foreach truncated group. The singular values from FWSVD are multiplied with Fisher information;thus their scales are different from SVD.
