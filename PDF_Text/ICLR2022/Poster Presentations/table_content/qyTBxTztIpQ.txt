Table 1: Dataset by game and recruitment channelTaskData Collected (hours)	MTurk	Social Media	Email Raffle	TotalBeamrider	7.90	-	-	7.90Breakout	11.45	-	-	11.45Montezuma’s Revenge	16.70	3.75	5.19	25.65Q*Bert	6.97	2.90	-	9.87Riverraid	17.64	4.47	3.10	25.20- of which multimodal	12.29	0.69	1.10	14.08Space Invaders	196.09	18.34	7.10	221.53- of which incentives	81.41	-	-	81.41- of which multimodal	101.46	0.52	1.12	103.09Space Invaders (2P)	6.35	0.38	0.81	7.54Space Invaders (2P w/AI)	54.98	2.54	1.41	58.93Total	318.07	32.38	17.60	368.05Figure 3: t-SNE embedding of action distributions of different (human) participant types in single-agent games (left), and human and AI agents in cooperative and standard multiagent games (right).
Table 2: List of all tasks in the datasetTaskData Collected (hours)	MTurk	Social Media	Email Raffle	TotalBeamrider	7.90	-	-	7.90Breakout	11.45	-	-	11.45Montezuma’s Revenge	16.70	3.75	5.19	25.65Q*Bert	6.97	2.90	-	9.87Riverraid	17.64	4.47	3.10	25.20- plain	5.35	3.78	2.00	11.12- left	6.78	0.45	0.70	7.94- right	5.51	0.23	0.40	6.14Space Invaders	196.09	18.34	7.10	221.53- plain	13.22	17.83	5.98	37.03- left	18.80	-	-	18.80- right	36.35	-	-	36.35- insideout	16.49	0.42	1.12	18.03- outsidein	13.44	-	-	13.44- rowbyrow	16.38	0.09	-	16.47- incentives (insideout)	81.41	-	-	81.41
Table 3: p values for “total” measurements in incentive experimentvs	Email Raffle	Social Media Users	All In- centives	Scaling Bonus	Quality Re- quire- ment	Active TimeNo incentives	^O000-	^0600-	^O000-	0.001	^0006-	0.621Active Time	^O000-	^0362-	^O000-	0.000	0.006	Quality Requirement	^0738-	^0368-	-On	0.105		Scaling Bonus	-0.341	^0256-	0.021			All Incentives	-0.658-	0135				Social Media Users	0.002					Table 4: p values for “fraction” measurements in incentive experimentvs	Email Raffle	Social Media Users	All In- centives	Scaling Bonus	Quality Re- quire- ment	Active TimeNo incentives	-0.001-	^0000-	^0000-	0.001	^0000-	0.440Active Time	^0000-	^0000-	^0000-	0.000	0.000	Quality Requirement	^0!52-	^0039-	-0.433-	0.035		Scaling Bonus	^0086-	^0031-	0.006			All Incentives	^0T123-	0.016				Social Media Users	0.318					For the embeddings in Section 5, the human data was filtered for quality using a condition ofat least80% of time spent on correct side of screen, respectively 80% of aliens shot in correct order. Forthe AI data, BC agents were trained using the hyperparameters described in the following section;four agents were trained using different random seeds; and each agent was used to generate 20
Table 4: p values for “fraction” measurements in incentive experimentvs	Email Raffle	Social Media Users	All In- centives	Scaling Bonus	Quality Re- quire- ment	Active TimeNo incentives	-0.001-	^0000-	^0000-	0.001	^0000-	0.440Active Time	^0000-	^0000-	^0000-	0.000	0.000	Quality Requirement	^0!52-	^0039-	-0.433-	0.035		Scaling Bonus	^0086-	^0031-	0.006			All Incentives	^0T123-	0.016				Social Media Users	0.318					For the embeddings in Section 5, the human data was filtered for quality using a condition ofat least80% of time spent on correct side of screen, respectively 80% of aliens shot in correct order. Forthe AI data, BC agents were trained using the hyperparameters described in the following section;four agents were trained using different random seeds; and each agent was used to generate 20trajectories. The features used were action distribution as well as five features of behavioral statisticsrelevant to the multimodal behavior types (fraction of time spent on left / right side of screen andfraction of aliens shot in inside-out / outside-in / row-by-row order).
Table 5: Mean Unnormalized return for offline RL algorithms. SI - Space Invaders and RR - RiverRaid. Scores are obtained via online evaluation. Numbers in parenthesis correspond to the bestreward achieved by human for the respective task.
Table 6: Mean Normalized scores for offline RL algorithms. SI - Space Invaders and RR - RiverRaid. Human performance score used to compute the normalized scores is available in parenthesisin Table 3. Random policy returns used to compute normalized scores are: 165.00 (shared across SItasks), 1220.00 (shared across RR tasks), 35.00 (Qbert) and 470.40 (Beam Rider).
Table 7: Hyper-Parameter Configuration TableDiscreteBC			HyperParameters learning_rate batch_size	Value 1e-3 100	HyperParameters beta	Value 0.5DiscreteBCQ			learning_rate	6.25e-5	batch_size	32gamma	0.99	n_critics	1target_redUctionJype	”min”	actionflexibility	0.3beta	0.5	target_update_interval	8000DiscreteCQL & DiscreteIQN			learning_rate	6.25e-5	batch_size	32gamma	0.99	n_critics	1target_redUctionJype	”min”	alpha	1.0target_update_interval	8000		DQN			Iearning_rate	6.25e-5	batch_size	32gamma	0.99	n_critics	1target_redUctionJype	”min”	target_update_interval	8000DiscreteSAC			actor_lr	3e-4	critic」r	3e-4temp_lr	3e-4	batch_size	64
