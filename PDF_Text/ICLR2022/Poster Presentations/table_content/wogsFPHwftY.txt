Table 1: Ablation on matching constraints: Impactof removing constraints on reciprocity, Lowe’s ratiotest and the Super-feature ID.
Table 2: Ablation on loss components: Im-pact of removing Lattn, using either a global lossLglobal or a loss directly on Super-features Lsuperor a combination of both.
Table 3: Comparison to the state of the art. All models are trained on SfM-120k. FCN denotesthe fully-convolutional network backbone, with R50— denoting a ResNet-50 without the last block.
Table 4: Impact of the update function in the LIT module. We compare the performance of theresidual combination of the previous template value with the cross-attention tensor compared to aGRU as used in slot attention (Locatello et al., 2020). In this experiment, we use T “ 3.
Table 5: Extended ablation on matching constraints. We study the impact of removing constraintson reciprocity, Lowe’s ratio test and the Super-feature ID.
Table 6: Fine-tuning the initial templates. Comparison where we either fine-tune the initial tem-plates Q 0 of LIT (bottom row, as in the main paper) or keep them frozen after ImageNet pretraining(top row).
Table 7: Visual localization results. Percentage of successfully localized images on the AachenDay-Night v1.1 dataset when changing the retrieval method in the Kapture pipeline from Humen-berger et al. (2020). In the most challenging scenario, i.e. night images at strictest localizationthreshold, using FIRe yields a 2% improvement compared to AP-GeM and a 1.5% improvementcompared to HOW. Bold number denotes the best performance, Underlined indicates performancewithin a 0.1 margin to the best one.
