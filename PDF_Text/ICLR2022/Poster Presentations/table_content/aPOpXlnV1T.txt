Table 1: Results for UCI Regression Datasets. We report predictive log-likelihood and RMSE (±standard deviation). Ties denotes the number of datasets (out of 12) for which the method cannot bestatistically distinguished from the best method (see Sec. B.4). We compare with Student-t (Detlefsenet al., 2019) and xVAMP/VBEM (Stirn & Knowles, 2020). Section B.4 lists the full results.
Table 2: Test results for dynamics models, using best configurations found in a grid search. Thereported standard deviations are over 5 random seeds. We compare with Student-t (Detlefsen et al.,2019) and xVAMP/VBEM (Stirn & Knowles, 2020).
Table 3: Selected results for generative modeling and depth-map prediction. Left: Training variationalautoencoders on MNIST and Fashion-MNIST. Right: Depth-map prediction on NYUv2. Full resultscan be found in Table S3 and Table S4.
Table S1: Results for UCI Regression Datasets. Predictive log-likelihood (higher is better) and standard deviation, together with dataset size, input and output dimensions. Best mean value in bold. Results that are not statistically distinguishable from the best result are marked with f.						carbon (10721, 5, 3)		concrete (1030, 8, 1)	energy (768, 8, 2)	housing (506, 13, 1)Lβ-NLL(β	0)	11.36 ± 2.11	-3.25 ± 0.31	-3.22 ± 1.41	-2.86 ± 0.50Lβ-NLL(β	0.25)	10.91 ± 2.42	-3.31 ± 0.51	-2.82 ± 0.82	-2.75 ± 0.42Lβ-NLL(β	0.5)	10.22 ± 4.00	-3.29 ± 0.36	-2.41 ± 0.72	-2.64 ± 0.36*Lβ-NLL(β	0.75)	10.82 ± 1.37	-3.27 ± 0.34	-2.80 ± 0.59	-2.72 ± 0.42*Lβ-NLL(β	1.0)	3.31 ± 20.88	-3.23 ± 0.33	-3.37 ± 0.58	-2.85 ± 0.87LMM		4.72 ± 5.80	-3.49 ± 0.38	-4.26 ± 0.50	-3.42 ± 1.01LMSE									Student-t		15.59 ± 0.43	-3.07 ± 0.14t	-2.46 ± 0.34	-2.47 ± 0.24*xVAMP		13.28 ± 0.19	-3.06 ± 0.15t	-2.47 ± 0.32	-2.43 ± 0.21*xVAMP*		13.17 ± 0.26	-3.03 ± 0.13t	-2.41 ± 0.32	-2.45 ± 0.22*VBEM		5.68 ± 0.70	-3.14 ± 0.07	-4.29 ± 0.16	-2.56 ± 0.15VBEM*		13.23 ± 0.36	-2.99 ± 0.13	-1.91 ± 0.21	-2.42 ± 0.22		kin8m	naval	power	protein		(8192, 8, 1)	(11934,16, 2)	(9568, 4, 1)	(45730, 9, 1)Lβ-NLL(β	0)	1.140 ± 0.039t	12.46 ± 1.18	-2.807 ± 0.057	-2.80 ± 0.05Lβ-NLL(β	0.25)	1.142 ± 0.026t	13.78 ± 0.33*	-2.801 ± 0.053	-2.79 ± 0.05Lβ-NLL(β	0.5)	1.141 ± 0.046t	13.99 ± 0.40	-2.805 ± 0.052	-2.78 ± 0.02Lβ-NLL(β	0.75)	1.137 ± 0.0411	13.63 ± 0.62*	-2.806 ± 0.053	-2.79 ± 0.02
Table S2: Results for UCI Regression Datasets. RMSE (lower is better) and standard deviation, together with dataset size, input and output dimensions. Best mean value in bold. Results that are not statistically distinguishable from the best result are marked with f.						carbon (10721, 5, 3)		concrete (1030, 8, 1)	energy (768, 8, 2)	housing (506, 13, 1)Lβ-NLL(β	0)	0.0068 ± 0.0029t	6.08 ± 0.65	2.25 ± 0.34	3.56 ± 1.07tLβ-NLL(β	0.25)	0.0069 ± 0.0028t	5.79 ± 0.74	1.81 ± 0.30	3.48 ± 1.15tLβ-NLL(β	0.5)	0.0068 ± 0.0029t	5.61 ± 0.65	1.12 ± 0.25	3.42 ± 1.04tLβ-NLL(β	0.75)	0.0069 ± 0.0028t	5.67 ± 0.73	1.31 ± 0.45	3.43 ± 1.07tLβ-NLL(β	1.0)	0.0073 ± 0.0026t	5.55 ± 0.77t	1.54 ± 0.54	3.50 ± 0.95tLMM		0.0097 ± 0.0034	6.28 ± 0.82	2.19 ± 0.28	4.02 ± 1.18LMSE		0.0068 ± 0.0028t	4.96 ± 0.64	0.92 ± 0.11	3.24 ± 1.08tStudent-t		0.0067 ± 0.0029t	5.82 ± 0.59	2.26 ± 0.34	3.48 ± 1.17txVAMP		0.0067 ± 0.0029t	5.44 ± 0.64t	1.87 ± 0.32	3.23 ± 1.00txVAMP*		0.0067 ± 0.0029t	5.35 ± 0.73t	2.00 ± 0.26	3.38 ± 1.15tVBEM		0.0074 ± 0.0026t	5.21 ± 0.58t	1.29 ± 0.33	3.32 ± 1.06tVBEM*		0.0067 ± 0.0029	5.17 ± 0.59t	1.08 ± 0.17	3.19 ± 1.02		kin8m	naval	power	protein		(8192, 8, 1)	(11934,16, 2)	(9568, 4, 1)	(45730, 9, 1)Lβ-NLL(β	0)	0.087 ± 0.004	0.0021 ± 0.0006	4.06 ± 0.18t	4.49 ± 0.11Lβ-NLL(β	0.25)	0.083 ± 0.003	0.0012 ± 0.0004	4.04 ± 0.18t	4.35 ± 0.05tLβ-NLL(β	0.5)	0.082 ± 0.003t	0.0006 ± 0.0002	4.04 ± 0.17t	4.31 ± 0.02tLβ-NLL(β	0.75)	0.081 ± 0.004t	0.0004 ± 0.0001t	4.04 ± 0.15t	4.28 ± 0.02t
Table S3: Results for generative modeling with variational autoencoders on MNIST and Fashion-MNIST. We report RMSE and posterior predictive log-likelihood (LL) with standard deviation over 5independent trials.
Table S4: Results for depth regression on the NYUv2 dataset (Silberman et al., 2012). We adapt a state-of-the-art network for depth regression from AdaBins (Bhat et al., 2021) and train it with differentloss functions. In contrast to AdaBins, our network does not include the Mini-ViT Transformermodule and thus our results are not directly comparable with those originally reported. For reference,we also report numbers from other recent literature on this task. Notably, the Gaussian NLL in allvariants clearly outperforms the Scale Invariant (SI) loss, despite the latter being a loss functionspecifically designed for the task of depth regression. We refer the reader to Bhat et al. (2021) for adescription of the metrics.
Table S5:	Architectures used for the sinusoidal regression task. Fully-connected feed-forward neuralnetworks with tanh activations.
Table S6:	Hyperparameter settings for our grid search on the ObjectSlide and Fetch-PickAndPlacedatasets. We run 96 configurations per loss function.
Table S7:	Best hyperparameters found by grid search on ObjectSlide and Fetch-PickAndPlacedatasets, measured by best log-likelihood on the validation set.
