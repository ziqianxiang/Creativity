Table 1: Process times (in msec) per overall loop (e.g., lines 3-10 in Algorithm 2). Process timesper Q-functions update (e.g., lines 4-9 in Algorithm 2) are shown in parentheses.
Table 2: Number of parameters	HoPPer-V2	Walker2d-v2	Ant-v2	HUmanoid-v2^SAC-	141,826	146,434 =	152,578	166,402 =REDQ	698,890	-721,930-	752,650	-821,770-DUVN	139,778	-144,386-	150,530	164,354-DroQ	141,826	146,434 —	152,578	166,402 —memory consumption is almost independent of the environment. This is because one of the mostmemory-intensive parts is the ReLU activation at the hidden layers in Q-functions4.
Table 3: Bottleneck memory consumption (in megabytes) with methods. Three worst bottleneckmemory consumptions are shown in form of “1st worst bottleneck memory consumption / 2nd worstbottleneck memory consumption / 3rd worst bottleneck memory consumption.
Table 4: Comparison between related studies and ours. We classify related studies into five types(e.g., “Ensemble Q-functions”) on basis of three criteria (e.g. “Type of model uncertainty”).
Table 5: Process times (in msec) per overall loop (e.g., lines 3-10 in Algorithm 2). Process timesper Q-functions update (e.g., lines 4-9 in Algorithm 2) are shown in parentheses.
Table 6: Number of parameters	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2DroQ	141,826	146,434 =	152,578	166,402 =REDQ2	139,778	-144,386-	150,530	164,354^^REDQ3	209,667	-216,579-	225,795	-246,531^^REDQ5	349,445	-360,965-	376,325	-410,885^^REDQ10	698,890	721,930 —	752,650	821,770 —Table 7: Bottleneck memory consumption (in megabytes) with methods. Three worst bottleneckmemory consumptions are shown in form of “1st worst bottleneck memory consumption / 2nd worstbottleneck memory consumption / 3rd worst bottleneck memory consumption.”	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2DroQ	73/72/ 6h	73/72/ 6h	73/72/ 7O=	73 / 72 / 70REDQ2	-73/51/51-	-73/51/51	-73/51/51	73 /52/52REDQ3	-94/64/ 62-	-94/64/ 62-	-94/64/ 62-	94 / 65 / 62REDQ5	136/ 106/ 100	136/ 106/ 100	136/ 106/ 100	136/ 107/ 101REDQ10	241/211/200~	241/211/200-	241/211/200~	241/212/20118Published as a conference paper at ICLR 2022C Additional ablation study of DroQDropout is introduced into three parts of the algorithm for DroQ (i.e., lines 6, 8 and 10 of Algorithm 2
Table 7: Bottleneck memory consumption (in megabytes) with methods. Three worst bottleneckmemory consumptions are shown in form of “1st worst bottleneck memory consumption / 2nd worstbottleneck memory consumption / 3rd worst bottleneck memory consumption.”	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2DroQ	73/72/ 6h	73/72/ 6h	73/72/ 7O=	73 / 72 / 70REDQ2	-73/51/51-	-73/51/51	-73/51/51	73 /52/52REDQ3	-94/64/ 62-	-94/64/ 62-	-94/64/ 62-	94 / 65 / 62REDQ5	136/ 106/ 100	136/ 106/ 100	136/ 106/ 100	136/ 107/ 101REDQ10	241/211/200~	241/211/200-	241/211/200~	241/212/20118Published as a conference paper at ICLR 2022C Additional ablation study of DroQDropout is introduced into three parts of the algorithm for DroQ (i.e., lines 6, 8 and 10 of Algorithm 2in Section 3). In this section, we conducted an ablation study to answer the question “which dropoutintroduction contributes to overall performance improvement of DroQ?” We compared DroQ withits following variants:-DO@TargetQ: A DroQ variant that does not use dropout in line 6. Specifically, dropout is not usedin QDr,φi (s0, a0) in the following part in line 6.
Table 8: Hyperparameter settingsMethod	Parameter	ValueSAC, REDQ, DroQ, and DUVN	optimizer learning rate discount rate (Y) target-smoothing coefficient (P) replay buffer size number of hidden layers for all networks number of hidden units per layer mini-batch size random starting data UTD ratio G	Adam (Kingma & Ba, 2015) 3 - 102 0.99 0.005 106 2 256 256 5000 20REDQ and DroQ	in-target minimization parameter M	^2REDQ	ensemble size N	10DroQ and DUVN	dropout rate	^001DUVN	in-target minimization parameter M	1	—I Experiments on the original REDQ codebaseExperiments presented in the main content of this paper have been implemented on top of theSAC codebase (https://github.com/ku2482/soft-actor-critic.pytorch). Inthis section, we report the results of our experiments on the original REDQ codebase (https://github.com/watchernyu/REDQ). Our experiments are to replicate experiments in themain content of this paper. To evaluate the computational efficiency, we plot the wallclock timerequired to complete a certain number of interactions with the environment, instead of the standarddeviation of bias. In addition, dropout rate values are re-tuned for each environment (Table 9). Fig-ure 14 shows the replica of the results presented in Section 4.1. Figure 15 shows the replica of theresults provided in Section 4.3. Figure 16 shows the replica of the results provided in Section A.1.
Table 9: Dropout rate setting for Appendix IEnvironment	ValueHopper-v2	0.0001Walker2d-v2	0.005Ant-v2	0.01Humanoid-v2	0.127Published as a conference paper at ICLR 2022environment interactionsWalker2d-v2U」n4①一①5g①>2①一①6g①>(σHopper-v2srts-q ① 69① >(σenvironment interactions 1∈5Walker2d-v2s<u-q ① 69① ><σU」n4①一①62」①>(0U」n4①一①62」①>(0environment interactions ie5
