Table 1: Performance on CUB-200-2011 when concepts are leveraged at training (CT) or withoutconcepts (CT [w/o]). Comparison against state-of-the-art approaches, classified by their trainingcomplexity: B-CNN (Lin et al., 2015b), Part R-CNN (Zhang et al., 2014), PS-CNN (Huang et al.,2016), PN-CNN (Branson et al., 2014), SPDA-CNN (Zhang et al., 2016), PA-CNN (Krause et al.,2015), MG-CNN (Wang et al., 2015), 2-level attn. (Xiao et al., 2015), FCAN (Liu et al., 2016),Neural const. (Simon & Rodner, 2015), ProtoPNet (Li et al., 2018), CAM (Zhou et al., 2016),DeepLAC (Lin et al., 2015a), ST-CNN (Jaderberg et al., 2015), MA-CNN (Zheng et al., 2017),RA-CNN (Fu et al., 2017). We report their best performance from (Li et al., 2018), irrespective ofwhether they are trained on full images or bounding boxes.
Table 2: Classification loss, explanation loss, accuracy on the PY test set and accuracy on the aPascaltest set for selected concept regularization parameter values Î» after 500 epochs.
