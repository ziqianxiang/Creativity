Table 1:	Benefit of hierarchical variational prototype in (%) on four cross-domain challenges underthe 5-way 5-shot setting. Hierarchical variational prototype achieves slighly better or comparableperformance to the variational prototype on all domains.
Table 2:	Hierarchical vs. flat variational memory in (%) on four cross-domain challenges underthe 5-way 5-shot setting. Hierarchical variational memory is more critical than the flat variationalmemory for cross-domain few-shot learning.
Table 3: Benefit of learning to weigh prototypes in (%) on four cross-domain challenges. Hierarchicalvariational memory with learning to weigh prototypes achieves better performance than with bagging.
Table 4: Comparative results of different algorithms on four proposed cross-domain few-shot chal-lenges. The results of other methods are provided by (Guo et al., 2020). Runner-up method isunderlined. Our hierarchical variational memory is a consistent top-performer.
Table 5: Comparative results for few-shot learning on miniImagenet and tieredImagenet using aResNet-12 backbone. Runner-up method is underlined. The proposed hierarchical variational memorycan also improve performance for few-shot learning within domains.
Table 6: Benefit of hierarchical variational prototype in (%) on four cross-domain challenges under5-way 20-shots setting.
Table 7: Benefit of hierarchical variational prototype in (%) on four cross-domain challenges under5-way 50-shots setting.
Table 8: Hierarchical vs. flat variational memory in (%) on four cross-domain challenges under 5-way20-shot setting.
Table 9: Hierarchical vs. flat variational memory in (%) on four cross-domain challenges under 5-way50-shot setting.
Table 10: Benefit of learning to weigh prototypes in (%) on four cross-domain challenges under5-way 20-shot setting.
Table 11: Benefit of learning to weigh prototypes in (%) on four cross-domain challenges under5-way 50-shot setting.
Table 12: Comparative results of few-shot learning methods on four proposed cross-domain few-shotchallenges under 5-way 50-shot setting.
Table 13: Comparative results of few-shot learning methods on miniImagenet and tieredImagenetusing a Conv-4 backbone. Runner-up method is underlined.
Table 14: Comparative results for few-shot learning on miniImagenet and tieredImagenet using aResNet-12 backbone under same data augmentation as Zhang et al. (2021). With data augmentation,our model also achieves competitive performance.
Table 15: Benefit of learning hierarchical structure for few-shot learning using a Conv-4 backbone.
Table 16: Comparative results for few-shot learning on miniImagenet and tieredImagenet usingdifferent backbones.
