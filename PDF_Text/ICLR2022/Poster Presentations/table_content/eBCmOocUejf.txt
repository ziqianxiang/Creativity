Table 1: Results of different baselines with our framework applied. Our framework substantiallyimproves robustness of both standard and adversarial prefix-tuning against all types of attacks.
Table 2: Results of PWWS adversarial data augmentation baselines as well as our methods on theSST-2 benchmark. Our methods consistently improve robustness over all baselines.
Table 3: Performance of our method with std. prefix-tuning with various test batch sizes on SST-2.
Table 4: Performance of our framework under mixed test data on SST-2. The ‘+’ denotes combina-tion of test set clean or under attack, and ‘C’ is short for “Clean”, ‘B’ for “BUG”, etc.
Table 5: Inputs from the SST-2 dev set and their perturbed versions by BUG/UAT attack underwhich the prediction is flipped. The two examples are used as case studies in Sections 5.1 and 5.2.
Table 6: Performance of our robust prefix-tuning with regular finetuning baselines for comparison.							Method	#Epochs	Clean	PWWS	VIPER	SCPN	BUG	UATstd. regular finetuning	9	93.68	24.77	4.61	30.15	17.74	23.89std. prefix-tuning	100	92.48	16.64	1.92	31.58	8.84	5.05+ our framework	-	92.59	50.36	44.65	58.54	46.68	85.72adv. regular finetuning	3	93.63	48.38	33.44	48.27	44.59	14.17adv. prefix-tuning	20	89.51	32.02	17.35	43.33	27.38	8.57+ our framework	-	89.57	53.93	48.38	57.88	49.70	73.97adv. prefix-tuning	100	93.74	30.53	8.18	33.11	27.51	8.95+ our framework	-	93.79	57.55	43.60	60.68	57.17	91.05Due to time limit, we have only conducted regular finetuning experiments on SST-2 with one adver-sarial training baseline (Miyato et al., 2017) for comparison. The aim of the comparison, however,is neither to set a new SOTA nor to demonstrate that our robust prefix-tuning framework has beatenthe robust finetuning techniques. On the contrary, we aim to demonstrate several special propertiesin the scenario of prefix-tuning, as well as challenges and opportunities in robustness:•	Difficulty in optimization. It can be seen that prefix-tuning requires more epochs to con-verge. A potential explanation is that the loss landscape of the prefix parameters is highlynon-convex, as the downstream task should be learned with far fewer amount of free pa-rameters. The difficulty in optimization might have also brought more challenges on ro-bustness to prefix-tuning. According to Table 6, all prefix-tuning baselines underperform
Table 7: Results of different baselines with our framework applied. This table covers the results ofadversarial prefix-tuning with both word- and sentence-level `2 balls and serves as the supplementaryfor Table 1. Our framework consistently improves robustness of all baselines on all benchmarks.
Table 8: Results of earlystopping of adv. prefix-tuning at 20 epochs with our framework on SST-2.
Table 9: Results of VIPER and SCPN data augmentation baselines as well as our methods on SST-2.
Table 10: Dataset statistics for each benchmark. We have also included the number of classes ineach benchmark and the accuracy of random classifier in theory for better understanding.
Table 11: Time used by different methods for all benchmarks. Compared with the time-consumingtraining-phase baseline methods, our test-phase robust prefix-tuning is significantly faster.
Table 12: Performance of our method with std. prefix-tuning with various test batch sizes on SST-2.
Table 13: Performance of our framework with both standard and adversarial prefix-tuning on bothSST-2 and AG’s News benchmarks, with test batch size of 1 or adaptive.
Table 14: Performance of our method with std. prefix-tuning with static or dynamic normalizationbefore projection during inference on SST-2.
Table 15: Performance of our framework under mixed test data on SST-2 and AG’s News. The ‘+’denotes combination of test set clean or under attack, and ‘C’ is short for “Clean”, ‘B’ for “BUG”,etc. The test batch size is set as 1 for all experiments with our framework. We also provide theaveraged results with the robust prefix in our framework separately tuned on the two test sets as anupper bound for comparison.
Table 16: Original inputs from the development set of SST-2 With perturbation by PWWS.
Table 17: Original inputs from the development set of SST-2 with perturbation by UAT.
Table 18: Calculated corpus-level Degree of Distraction of the original prefix-tuning as well as ourframework under UAT attack. Results with f are statistically significant over the that of the standardprefix-tuning baseline with p < 0.001.
Table 19: Calculated corpus-level Recognition of the Essential token of the original prefix-tuningas well as our framework under UAT attack. Result with f is statistically significant over that of thestandard prefix-tuning baseline with P < 0.001. Result with ∣ is statistically significant over that ofour framework with N = 24 with p < 0.001.
