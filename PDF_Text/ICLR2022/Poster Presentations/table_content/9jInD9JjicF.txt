Table 1: Results on the Long Range Arena (LRA) benchmark (AVG: average accuracy across alltasks). Results with (1) are cited from (Tay et al., 2021), with (2) are from (Lee-Thorp et al., 2021),with (3) are from (Xiong et al., 2021). We implement our PoNet and re-implement FNet based onthe PyTorch codebase from (Xiong et al., 2021) and use the same experimental configurations toensure a fair comparison. For each group, the best result for each task and AVG are bold-faced.
Table 2: Comparison of GPU training speed (in steps/s, the higher the better) and peak memoryconsumption (in GB, the lower the better) on various input sequence lengths on the LRA textclassification task (using the same hyper-parameter setting for this task as in (Xiong et al., 2021),with speed-up and memory-saving multipliers relative to Transformer shown in parentheses. Thebest results are bold-faced with the second-best results underlined.
Table 3: GLUE Validation results from PoNet, BERT, and FNet. All models are uncased and pre-trained with the same configurations (Appendix A.2) with 340K steps. We report the best GLUEresults for each model from multiple hyper-parameter configurations (Appendix A.3). We reportthe mean of accuracy and F1 for QQP and MRPC, matthews correlations for CoLA, spearmancorrelations for STS-B, and accuracy for other tasks. MNLI(m/mm) means match/mismatch splits.
Table 4: Fine-tuning results (in F1 and Acc) on long-text classification datasets. For the third groupof results, we use the official checkpoints of BERT-Base and FNet-Base (see Appendix A.4).
Table 5: Results of ablation study as accuracy for pre-training MLM and SST (Sentence StructureTask) tasks, matthews correlations for CoLA, and spearman correlations for STS-B. SST denotesNSP when using LMN and the SSO task otherwise. All pre-training experiments run 340K steps.
Table 6: Detailed hyperparameter settings for the pre-training and fine-tuning experiments. Forrows with a single hyperparameter value, the value is used across pre-training and fine-tuning onGLUE and long-text classification tasks.
Table 7: Extra GLUE Validation results. BERT-Base and PoNet-Base are uncased whereas FNet-Base is cased (since the official FNet checkpoint is cased). We report the mean of accuracy and F1scores for QQP and MRPC, matthews correlations for CoLA, spearman correlations for STS-B, andaccuracy scores for other tasks. The MNLI(m/mm) means the match/mismatch splits. Results with(1) are from (Lee-Thorp et al., 2021). Results with (2) and (3) are the best results from searching20 sets of hyper-parameter configurations based on Table 6 for fine-tuning the pre-trained models.
