Table 1: English STS. Spearman,s P rank correlations are reported. TENC models use only self-distillation while TENC-mutual models use mutual-distillation as well. Blue and red denotesmutual-distillation models that are trained in the base and large group respectively. Models withoutcolour are not co-trained with any other models. *Note that for base encoders, our results can slightlydiffer from numbers reported in (Gao et al., 2021) since different evaluation packages are used.
Table 2: Binary classification task results. AUC scores are reported. We only demonstrate results forone model for brevity. Full table can be found in Appendix (Tab. 11).
Table 3: A domain transfer setup: testing Trans-Encoder models trained with STS data directlyon binary classification tasks. We only demonstrate results for one model for brevity. Full table canbe found in Appendix (Tab. 12).
Table 4: English STS (models beyond SimCSE). Spearman’s ρ scores are reported.
Table 5: Binary classification (models beyond SimCSE). AUC scores are reported.
Table 6: Mean and standard deviation (S.D.) of five runs on STS.
Table 7: Compare Trans-Encoder models trained with all STS data (using STS-B’s dev set) andSICK-R data only (using SICK-R’s dev set). Large performance gains can be obtained when treatingSICK-R as a standalone task.
Table 8: Compare different loss function configurations.
Table 9: Compare Trans-Encoder with SimCSE/Mirror-BERT-style contrastive tuning on sentencepairs given by the task.
Table 10: Running time for all models across different datasets/tasks. We experimented with NVIDIAV100, A100 and 3090 and found the estimated time to be similar.
Table 11: Binary classification task results (SimCSE models; full table). AUC scores are reported.
Table 12: Full table for domain transfer setup: testing Trans-Encoder (SimCSE models) trainedwith STS data directly on binary classification tasks.
Table 13: Ablation: sequential training with the same set of weights vs. refreshing weights for allmodels.
Table 14: Ablation: compare Trans-Encoder with standard self-distillation.
Table 15: A listing of train/dev/test stats of all used datasets. ?: a collection of all individual sentence-pairs from all STS tasks.
Table 16: A listing of hyperpamters used for all Trans-Encoder models.
Table 17: A listing of HuggingFace URLs of all PLMs used in this work.
