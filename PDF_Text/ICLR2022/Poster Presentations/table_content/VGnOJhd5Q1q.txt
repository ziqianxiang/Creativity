Table 1: Ablation studies on the WikiText-103 validation data in the base setting. Lower perplexity(PPL) is better. All the models have a total of 16 attention layers and 10 heads. Non-Local (NL, i.e.,LSH or LHA) layers when present are always added at the top of the model. Attention size denoteseither local window size or hash bucket size. ↑ denotes that the results are taken from Transformer-XL(Dai et al., 2019).
Table 2: Performance of methods per task on the GLUE benchmark, where C and # denote thenumber of queries/keys per cluster and hashing rounds, respectively. ↑ denotes that the results aretaken from their original papers.
Table 3: Accuracy on LO, IMDb, AAN, and Image in Long Range Arena benchmark. Best and secondbest model per task is shown in boldface and underlined. Throughput is evaluated on IMDb andrelative to the vanilla transformer,s. * and ↑ denote being statistically significantly better (p < 0.05in one-tail proportion test) than vanilla Transformer and the second best model.* denotes that thethroughput comparison are run on a single NVIDIA Tesla V100 32GB GPU, while previous results(Tay et al., 2020C) are reported on 4 X 4 TPU V3 chips. B, ♦, and 4 denotes low-rank/kernelizedattention, content-based sparse attention, and location-based sparse attention, respectively.
Table 4: Univariate long sequence time series forecasting results on two datasets (lower is better).
Table 5: Multivariate long sequence time series forecasting results on two datasets (lower is better).
