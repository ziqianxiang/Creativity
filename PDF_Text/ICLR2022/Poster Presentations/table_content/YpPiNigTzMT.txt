Table 1: A variety of problems enabled by universal WS, with specifications for sets, distances, and models.
Table 2: End model performance With real-world rankings and regression LFs on movies. WS (3 scores, ∙) shows theresult of our algorithm combining 3 LFs. In ranking, high-quality LFs perform well (and better than fewer clean labels),but mixing in lower-quality LFs hurts majority vote (2) more than our proposed approach. In regression, our methodyields performance similar to fully-supervised with 50% data, while outperforming MV.
Table 3: UAS scores for semantic dependency pars-ing. Y is synthesized from off-the-shelf parsing LFs.
Table 4: Glossary of variables and symbols used in this paper.
Table 5: End model performance with true ranking LFs in BGG dataset.
Table 6: End model performance with true regression LFs in BGG dataset. The training data was pickedbased on the residuals in linear regression (resulting in a “bad” subset scenario for a challenging dataset).
Table 7: End model performance with true ranking LFs in MSLR-WEB10K dataset. Since the dataset has alot of tie scores and the number of items is not uniform across examples, we sampled the examples with fiveunique scores (0, 1, 2, 3, 4). Also, in each example, items are randomly chosen so that each score occursonly once in each item set.
