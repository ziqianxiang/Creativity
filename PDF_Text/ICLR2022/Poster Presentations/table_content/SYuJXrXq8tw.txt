Table 1: Performance showing the appearance of poor robust generalization/robust overfitting, and the effec-tiveness of our sparse proposals with various comparisons to other sparsification methods on CFAIR-10 withResNet-18. The difference between best and final robust accuracy indicates degradation in performance duringtraining. We pick the best checkpoint by the best robust accuracy on the validation set. Bold numbers indicatesuperior performance, and ] displays shrunk robust generalization gap compared to dense models. Note thatmodel picking criterion and the presentation style are consistent for all tables.
Table 2: Performance showing the effectiveness of our proposed approaches across different datasets withResNet-18. The subnetworks at 80% sparsity are selected here.
Table 3: Performance showing the effectiveness of our proposed approaches with other architectures, i.e.,VGG-16 on CIFAR-10/100. The subnetworks at 80% sparsity are selected here.
Table 4: Ablation of different sparse initialization in Table 5: Ablation of different update frequency inFlying Bird+. Subnetwroks at 80% initial sparsity are Flying Bird+. Subnetworks at 80% initial sparsitychosen on CIFAR-10 with ResNet-18.	are chosen on CIFAR-10 with ResNet-18.
Table A6: Comparison results of different training regimes for RB ticket finding on CIFAR-100 withResNet-18. The subnetworks at 90% and 95% are selected here.
Table A7: Transfer attack performance from/on an unseen non-robust model, where the attacks are generatedby/applied to the non-robust model. The robust generalization gap is also calculated based on transfer attackaccuracies between train and test sets. We use ResNet-18 on CIFAR-10/100 and sub-networks at 80% sparsity.
Table A8: Evaluation under improved attacks (i.e., Auto-Attack and CW-Attack) on CIFAR-10/100 withReSNet-18 at 80% sparsity. The robust generalization gap is computed under improved attacks.
Table A9: More results of different sparcification methods on CIFAR-10 with ResNet-18.
Table A10: More results of different SParcification methods on CIFAR-10 with VGG-16.
Table A11: More results of different sparcification methods on CIFAR-100 with ResNet-18.
Table A12: More results of different SParcification methods on CIFAR-100 with VGG-16.
Table A13: ComParison results of the unPruned dense network and our flying birds at more sParsity levels.
Table A14: Comparison results of the unpruned dense network and our flying birds at more sparsity levels.
Table A15: Comparison results of the unpruned dense network and our flying birds at more sparsity levels.
Table A16: Comparison results of the unpruned dense network and our flying birds at more sparsity levels.
Table A17: Comparison results of the unpruned dense network and our flying birds on CIFAR-10 withWideResNet-34-10.
