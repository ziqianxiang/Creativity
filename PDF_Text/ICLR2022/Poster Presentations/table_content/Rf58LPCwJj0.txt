Table 1: CLIP significantly outperforms the previous SOTA result on DomainBed, as supportedby our theoretical analysis. Finetuning CLIP with our CAD bottleneck consistently improves therobustness of its representations and achieves SOTA performance.
Table 2: Finetuning CLIP L on LAION with an entropy bottleneck improves its robustness comparedto finetuning without on 7 distribution shift datasets. The pretrained CLIP L is still better likely dueto end-to-end training with higher quality data. IN denotes ImageNet.
Table 3: We repeated most empirical analysis (in the scientific setting) in the more practical bridgesetting and observed similar results.
Table 4: Full results on DomainBed with ‘oracle selection’ method.
Table 5: Results on DomainBed with ‘source validation’ selection. Source validation selected modeltends to overfit more to the source domain and diminish the effect of bottlenecks.
Table 6: Finetuning CLIP L on LAION with an entropy bottleneck performs better on DomainBedthan finetuning without.
