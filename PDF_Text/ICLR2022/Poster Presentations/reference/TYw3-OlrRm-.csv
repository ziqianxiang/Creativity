title,year,conference
 Micronets: Neural network architecturesfor deploying tinyml applications on commodity microcontrollers,2021, Proceedings of MachineLearning and Systems
 Food-101-mining discriminative ComPo-nents with random forests,2014, In ECCV
 ProxylessNAS: Direct neural architecture search on target taskand hardware,2019, In ICLR
 Once for all: Train one networkand sPecialize it for efficient dePloyment,2020, In ICLR
 Randaugment: Practical automated dataaugmentation with a reduced search space,2020, In NeurIPS
 Large scale fine-grainedcategorization and domain-specific transfer learning,2018, In CVPR
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Dropblock: a regularization method for convolutionalnetworks,2018, In NeurIPS
 Learning both weights and connections forefficient neural network,2015, In NeurIPS
 Deep residual learning for imagerecognition,2016, In CVPR
 Distilling the knowledge in a neural network,2019, arXivpreprint arXiv:1503
 Deep networks withstochastic depth,2016, In ECCV
 3d object representations for fine-grained categorization,2013, In Proceedings of the IEEE International Conference on Computer VisionWorkshops
 Compoundingthe performance improvements of assembled techniques in a convolutional neural network,2020, arXivpreprint arXiv:2001
 Mcunet: Tiny deep learning on iotdevices,2020, In NeurIPS
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Learningefficient convolutional networks through network slimming,2017, In ICCV
 Automated flower classification over a large numberof classes,2008, In Sixth Indian Conference on Computer Vision
 Cats and dogs,2012, In CVPR
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Yolov3: An incremental improvement,2018, arXiv preprintarXiv:1804
 Imagenet-21k pretraining forthe masses,2021, In Thirty-fifth Conference on Neural Information Processing Systems Datasets andBenchmarks Track (Round 1)
 Fitnets: Hints for thin deep nets,2015, In ICLR
 An overview of multi-task learning in deep neural networks,2017, arXiv preprintarXiv:1706
 Rnnpool:Efficient non-linear pooling for ram constrained inference,2020, arXiv preprint arXiv:2002
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In CVPR
 Is labelsmoothing truly incompatible with knowledge distillation: An empirical study,2021, In InternationalConference on Learning Representations
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In CVPR
 Efficient objectlocalization using convolutional networks,2015, In CVPR
 Fbnet: Hardware-aware efficient convnet design viadifferentiable neural architecture search,2019, In CVPR
 Gradaug: A new regularization method for deep neuralnetworks,2020, In NeurIPS
 Revisiting knowledge distillation vialabel smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Paying more attention to attention: Improving theperformance of convolutional neural networks via attention transfer,2017, In ICLR
 mixup: Beyond em-pirical risk minimization,2018, In ICLR
 Be yourown teacher: Improve the performance of convolutional neural networks via self distillation,2019, InICCV
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In CVPR
 Bag of freebies fortraining object detection neural networks,2019, arXiv preprint arXiv:1902
 Trained ternary quantization,2017, In ICLR
