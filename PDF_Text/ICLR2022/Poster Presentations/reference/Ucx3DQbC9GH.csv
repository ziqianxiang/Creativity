title,year,conference
 Better fine-tuning by reducing representational collapse,2021, In International Conference onLearning Representations (ICLR)
 Curriculum learning,2009, InProceedings ofthe International Conference on Machine Learning (ICML)
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Identifying and reducing gender bias in word-level languagemodels,2019, arXiv preprint arXiv:1904
 Language models arefew-shot learners,2020, In Advances in Neural Information Processing Systems (NeurIPS)
 A simple framework forcontrastive learning of visual representations,2020, In Proceedings of the International Conference onMachine Learning (ICML)
 Mining topics in documents: standing on the shoulders of big data,2014, InProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and datamining
 Advaug: Robust adversarial aug-mentation for neural machine translation,2020, In Annual Meeting of the Association for ComputationalLinguistics (ACL)
 Autoaugment:Learning augmentation strategies from data,2019, In Conference on Computer Vision and PatternRecognition (CVPR)
 RandAugment: Practical automateddata augmentation with a reduced search space,2020, In Conference on Computer Vision and PatternRecognition (CVPR)
 Class-balanced loss based oneffective number of samples,2019, In Conference on Computer Vision and Pattern Recognition (CVPR)
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 A survey of data augmentation approaches for nlp,2021, In Annual Meeting of theAssociation for Computational Linguistics (ACL)
 Bae: Bert-based adversarial examples for text clas-sification,2020, In Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Keepaugment: A simpleinformation-preserving data augmentation approach,2020, arXiv preprint arXiv:2011
 Affinity and diversity:Quantifying mechanisms of data augmentation,2020, arXiv preprint arXiv:2002
 Augmenting data with mixup for sentence classifica-tion: An empirical study,2019, arXiv preprint arXiv:1905
 Faster autoaugment:Learning augmentation strategies using backpropagation,2020, In International Conference on ComputerVision (ICCV)
 Learning datamanipulation for augmentation and weighting,2019, Advances in Neural Information ProcessingSystems (NeurIPS)
 Categorical reparameterization with gumbel-softmax,2017, InInternational Conference on Learning Representations (ICLR)
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principled regu-larized optimization,2020, In Annual Meeting of the Association for Computational Linguistics (ACL)
 Association for Computational Linguistics,1225, doi:10
 M2m: Imbalanced classification via major-to-minor translation,2020, In Conference on Computer Vision and Pattern Recognition (CVPR)
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Newsweeder: Learning to filter netnews,1995, In Machine Learning Proceedings 1995
 An evaluationdataset for intent classification and out-of-scope prediction,2019, arXiv preprint arXiv:1909
 Learning question classifiers,2002, In COLING 2002: The 19th InternationalConference on Computational Linguistics
 Differentiable automatic data augmentation,2020, In International Conference on ComputerVision (ICCV)
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Sentence-bert: Sentence embeddings using siamesebert-networks,2019, In Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Text autoaugment: Learning composi-tional augmentation policy for text classification,2021, In Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 A simple but tough-to-beat data augmentation approach for natural language understanding and generation,2020, arXivpreprint arXiv:2009
 Training region-based object detectorswith online hard example mining,2016, In Conference on Computer Vision and Pattern Recognition(CVPR)
 A bayesian data augmentationapproach for learning deep models,2017, In Advances in Neural Information Processing Systems(NeurIPS)
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2019, In InternationalConference on Learning Representations (ICLR)
 Switchout: an efficient data augmentationalgorithm for neural machine translation,2018, In Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Eda: Easy data augmentation techniques for boosting performance on textclassification tasks,2019, arXiv preprint arXiv:1901
 Circumventing outliersof autoaugment with knowledge distillation,2020, arXiv preprint arXiv:2003
 On the generalization effects oflinear transformations in data augmentation,2020, In Proceedings of the International Conference onMachine Learning (ICML)
 Unsupervised dataaugmentation for consistency training,2020, In Advances in Neural Information Processing Systems(NeurIPS)
 Reweighting augmentedsamples by minimizing the maximal expected loss,2021, In International Conference on LearningRepresentations (ICLR)
 Graphcontrastive learning with augmentations,2020, In Advances in Neural Information Processing Systems(NeurIPS)
 Bertscore: Evaluatingtext generation with bert,2020, In International Conference on Learning Representations (ICLR)
 Revisiting few-samplebert fine-tuning,2020, arXiv preprint arXiv:2006
 Adversarial autoaugment,2020, In InternationalConference on Learning Representations (ICLR)
 Freelb: Enhancedadversarial training for natural language understanding,2020, In International Conference on LearningRepresentations (ICLR)
 Fine-tuning language models from human preferences,2019, arXivpreprint arXiv:1909
