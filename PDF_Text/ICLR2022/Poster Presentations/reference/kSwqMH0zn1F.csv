title,year,conference
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, Advances in Neural Information ProcessingSystems
 Stochastic training of graph convolutional networks withvariance reduction,2018, In International Conference on Machine Learning
 Minimal variance samplingwith provable guarantees for fast training of graph neural networks,2020, In Proceedings of the 26thACM SIGKDD International Conference on Knowledge Discovery & Data Mining
 On the importance of sampling in learninggraph convolutional networks,2021, arXiv preprint arXiv:2103
 P3: Distributed deep graph learning at scale,2021, In 15thUSENIX Symposium on Operating Systems Design and Implementation (OSDI 21)
 Generalization and representational limits ofgraph neural networks,2020, In International Conference on Machine Learning
 Inductive representation learning on large graphs,2017, InAdvances in neural information processing systems
 Pipedream: Fast and efficient pipeline parallel dnn training,2018, arXiv preprintarXiv:1806
 More effective distributed ml via a stale synchronousparallel parameter server,2013, Advances in neural information processing systems
 Open graph benchmark: Datasets for machine learning on graphs,2020, arXivpreprint arXiv:2005
 A fast and high quality multilevel scheme for partitioning irregulargraphs,1998, SIAM Journal on scientific Computing
 Communication efficient distributedmachine learning with the parameter server,2014, Advances in Neural Information Processing Systems
 A pac-bayesian approach to generalization boundsfor graph neural networks,2020, arXiv preprint arXiv:2012
 EXACT: Scalable graph neuralnetworks training via extreme activation compression,2022, In International Conference on LearningRepresentations
 Neugraph:parallel deep neural network computation on large graphs,2019, In 2019 USENIX Annual TechnicalConference (USENIX ATC 19)
 Hogwild!: A lock-free approachto parallelizing stochastic gradient descent,2011, arXiv preprint arXiv:1106
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Reducing communication in graph neural networktraining,2020, arXiv preprint arXiv:2005
 BNS-GCN: Efficient full-graphtraining of graph convolutional networks with partition-parallelism and random boundary nodesampling,2022, Fifth Conference on Machine Learning and Systems
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, Advances in neuralinformation processing systems
 Gradiveq: Vector quantization forbandwidth-efficient gradient aggregation in distributed cnn training,2018, In Proceedings of the 32ndConference on Neural Information Processing Systems (NIPSâ€™18)
 Graph-saint: Graph sampling based inductive learning method,2020, arXiv preprint arXiv:1907
 Link prediction based on graph neural networks,2018, In Advances inNeural Information Processing Systems
