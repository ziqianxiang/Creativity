title,year,conference
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 Feature purification: How adversarial training performs robustdeep learning,2020, arXiv preprint arXiv:2005
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Beyond linearization: On quadratic and higher-order approximation of wideneural networks,2019, In International Conference on Learning Representations
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, Proceedings ofthe National Academy ofSciences
 Latent dirichlet allocation,2003, the Journal ofmachine Learning research
 Deep clustering for unsuper-vised learning of visual features,2018, In European Conference on Computer Vision
 A simple framework forcontrastive learning of visual representations,2020, In International Conference on Machine Learning
 Learning parities with neural networks,2020, Advances in NeuralInformation Processing Systems
 The mnist database of handwritten digit images for machine learning research,2012, IEEE SignalProcessing Magazine
 Training neural networks as learning data-adaptive kernels: Prov-able representation and approximation benefits,2020, Journal of the American Statistical Association
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Mathematical models of overparameterized neuralnetworks,2021, Proceedings of the IEEE
 Agnostic learning of a single neuron with gradientdescent,2020, In Advances in Neural Information Processing Systems
 Disentangling feature and lazytraining in deep neural networks,2020, Journal of Statistical Mechanics: Theory and Experiment
 Rich feature hierarchies for accurateobject detection and semantic segmentation,2014, In Computer Vision and Pattern Recognition
 Deep residual learning for imagerecognition,2016, In Computer Vision and Pattern Recognition
 Momentum contrast forunsupervised visual representation learning,2020, In Computer Vision and Pattern Recognition
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in neural information processing systems
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow relu networks,2019, In International Conference on LearningRepresentations
 Self-supervised visual feature learning with deep neural networks: Asurvey,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Approximate is good enough: Probabilisticvariants of dimensional and margin complexity,2020, In Conference on Learning Theory
 Efficient noise-tolerant learning from statistical queries,1998, Journal of the ACM
 The comparative power of relu networks and polynomialkernels in the presence of sparse latent structure,2018, In International Conference on LearningRepresentations
 Learning multiple layers of features from tiny images,2012, University of Toronto
 Tiny imagenet visual recognition challenge,2015, CS 231N
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, Advances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, Advances in Neural Information Processing Systems
 Learning over-parametrized two-layer neuralnetworks beyond ntk,2020, In Conference on Learning Theory
 Quantifying the benefit of usingdifferentiable learning over tangent kernels,2021, arXiv preprint arXiv:2103
 Emergentlinguistic structure in artificial neural networks trained by self-supervision,2020, Proceedings of theNational Academy of Sciences
 Deepdouble descent: Where bigger models and more data hurt,2020, In International Conference on LearningRepresentations
 Readingdigits in natural images with unsupervised feature learning,2011,2011
 Bayesian convolutional neural networks with many channels aregaussian processes,2019, In International Conference on Learning Representations
 Random features for large-scale kernel machines,2008, In Advances inNeural Information Processing Systems
 Sparse coding and decorrelation in primary visual cortex duringnatural vision,2000, Science
 Kernel and rich regimes in overparametrized models,2020, InConference on Learning Theory
 Learning a single neuron with gradient methods,2020, In Conference onLearning Theory
 On the power and limitations of random features for understandingneural networks,2019, Advances in Neural Information Processing Systems
 Visualizing and understanding convolutional networks,2014, InEuropean Conference on Computer Vision
 A local convergence theory for mildly over-parameterized two-layerneural network,2021, In Conference on Learning Theory
 Gradient descent optimizes over-parameterized deep ReLU networks,1573, Machine Learning
