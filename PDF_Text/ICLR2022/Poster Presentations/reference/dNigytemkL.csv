title,year,conference
 Shaping the learning landscape in neuralnetworks around wide flat minima,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Essentially no barriers inneural network energy landscape,1309, In International conference on machine learning
 Deep ensembles: A loss landscape perspec-tive,2019, arXiv preprint arXiv:1912
 Linear modeconnectivity and the lottery ticket hypothesis,2020, In ICML
 Local minima and plateaus in hierarchical structures of multilayerperceptrons,2000, Neural Networks
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Multi-task zipping via layer-wise neuron sharing,2018, arXivpreprint arXiv:1805
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Over-parameterized deep neural networks have no strict localminima for any continuous activations,2018, arXiv preprint arXiv:1812
 Visualizing the loss landscapeof neural nets,2017, arXiv preprint arXiv:1712
 Loss landscapes and optimization in over-parameterizednon-linear systems and neural networks,2020, arXiv preprint arXiv:2003
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings ofthe National Academy of Sciences
 Deepdouble descent: Where bigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Reading dig-its in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learning andUnsupervised Feature Learning 2011
 Towards learning convolutions from scratch,2020, In Advances in Neural InformationProcessing Systems
 Path-SGD: Path-normalized opti-mization in deep neural networks,2015, arXiv preprint arXiv:1506
 Exploring gen-eralization in deep learning,2017, In Proceedings of the 31st International Conference on NeuralInformation Processing Systems
 On the loss landscape of a classof deep neural networks with no bad local valleys,2018, arXiv preprint arXiv:1809
 Caliban: Docker-based job manager forreproducible workflows,2020, Journal of Open Source Software
 Very deep convolutional networks for large-scale imagerecognition,2015, In Proceedings of the International Conference on Learning Representations
 Geometry of the loss landscape in overparameterized neural networks:Symmetries and invariances,2021, arXiv preprint arXiv:2105
 Model fusion via optimal transport,2020, Advances in Neural InformationProcessing Systems
 Optimiz-ing mode connectivity via neuron alignment,2020, arXiv preprint arXiv:2009
 Batchensemble: an alternative approach to efficientensemble and lifelong learning,2020, arXiv preprint arXiv:2002
 Implicit regularization and convergence for weight normalization,2019, arXivpreprint arXiv:1911
 List-based simulated annealing algorithmfor traveling salesman problem,2016, Intell
 Understandingdeep learning requires rethinking generalization,2017, In Proceedings of the International Conferenceon Learning Representations
