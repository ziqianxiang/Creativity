title,year,conference
 Exploring the limits oflarge scale pre-training,2021, arXiv preprint arXiv:2110
 The evolution ofout-of-distribution robustness throughout fine-tuning,2021, arXiv preprint arXiv:2106
 Dual pathnetworks,2017, arXiv preprint arXiv:1707
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Underspecificationpresents challenges for credibility in modern machine learning,2020, arXiv preprint arXiv:2011
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Diversity is all you need:Learning skills without a reward function,2018, arXiv preprint arXiv:1802
 Deep ensembles: A loss landscape perspec-tive,2019, arXiv preprint arXiv:1912
 Experiments with a new boosting algorithm,1996, In icml
 Imagenet-trained cnns are biased towards texture; increasing shape bias improvesaccuracy and robustness,2018, arXiv preprint arXiv:1811
 Beyond accuracy: Quantifying trial-by-trialbehaviour of cnns and humans by measuring error consistency,2020, arXiv preprint arXiv:2006
 Dropblock: A regularization method for convolutionalnetworks,2018, arXiv preprint arXiv:1810
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Fix your classifier: the marginal value of training thelast weight layer,2018, arXiv preprint arXiv:1801
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Av-eraging weights leads to wider optima and better generalization,2018, arXiv preprint arXiv:1803
 Scaling up visual and vision-language representation learningwith noisy text supervision,2021, arXiv preprint arXiv:2102
 A simple weight decay can improve generalization,1992, In Advances inneural information processing systems
 Adversarial attacks and defences competition,2018, InThe NIPS’17 Competition: Building Intelligent Systems
 Exploiting open-endedness to solve problems through thesearch for novelty,2008, In ALIFE
 Selective kernel networks,2019, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Model similaritymitigates test set overuse,2019, arXiv preprint arXiv:1905
 Popular ensemble methods: An empirical study,1999, Journal ofartificial intelligence research
 When networks disagree: Ensemble methods for hybridneural networks,1992, Technical report
 Meta pseudo labels,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Acceleration of stochastic approximation by averaging,1992, SIAMjournal on control and optimization
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Mitigating bias incalibration error estimation,2020, arXiv preprint arXiv:2012
 Does your dermatology classifier knowwhat it doesn’t know? detecting the long-tail of unseen conditions,2022, Medical Image Analysis
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Diversity inducing information bottleneck in model ensembles,2020, arXiv preprintarXiv:2003
 Revisiting unreasonableeffectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Training data-efficient image transformers & distillation through attention,2021, In InternationalConference on Machine Learning
 Deep high-resolution representation learning for visualrecognition,2020, IEEE transactions on pattern analysis and machine intelligence
 Batchensemble: an alternative approach to efficientensemble and lifelong learning,2020, arXiv preprint arXiv:2002
 Hyperparameter ensembles forrobustness and uncertainty quantification,2020, arXiv preprint arXiv:2006
 Stacked generalization,1992, Neural networks
 Deep layer aggregation,2018, In Pro-ceedings of the IEEE conference on computer vision and pattern recognition
 Neuralensemble search for uncertainty estimation and dataset shift,2021, Advances in Neural InformationProcessing Systems
