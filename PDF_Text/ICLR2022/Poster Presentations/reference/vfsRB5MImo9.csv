title,year,conference
 Rainbow memory:Continual learning with a memory of diverse samples,2021, In CVPR
 Language models arefew-shot learners,2020, In NeurIPS
 Recall andlearn: Fine-tuning deep pretrained language models with less forgetting,2020, In EMNLP
 Decontextualization: Making sentences stand-alone,2021, TACL
 Knowledge neurons in pretrained trans-formers,2021, ArXiv
 Episodicmemory in lifelong language learning,2019, In NeurIPS
 Editing factual knowledge in language models,2021, InEMNLP
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Time-aware language models as temporal knowledge bases,2021, arXiv preprintarXiv:2106
 Wizardof wikipedia: Knowledge-powered conversational agents,2019, In ICLR
 T-rex: A large scale alignment of natural language with knowledgebase triples,2018, In LREC
 Eli5:Long form question answering,2019, In ACL
 Don¡¯t stop pretraining: adapt language models to domains and tasks,2020, In ACL
 Realm: Retrieval-augmented language model pre-training,2020, In ICML
 news-please: A genericnews crawler and extractor,2017, In 15th International Symposium of Information Science (ISI 2017)
 Robust disambiguation of namedentities in text,2011, In EMNLP
 Lora: Low-rank adaptation of large language models,2021, arXiv preprint arXiv:2106
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In ACL
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Internet-augmented dialogue generation,2021, arXivpreprint arXiv:2107
 Natural questions: abenchmark for question answering research,2019, TACL
 Deduplicating training data makes language models better,2021, arXivpreprint arXiv:2107
 Zero-shot relation extraction viareading comprehension,2017, In CoNLL
 Retrieval-augmented gener-ation for knowledge-intensive nlp tasks,2020, In NeurIPS
 Question and answer test-train overlap inopen-domain question answering datasets,2020, arXiv preprint arXiv:2008
 Paq: 65 million probably-asked questions and what youcan do with them,2021, In EACL
 An efficient transformer decoder with compressedsub-layers,2021, arXiv preprint arXiv:2101
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Gradient episodic memory for continual learning,2017, InNeurIPS
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, Psychology of learning and motivation
 Carbon emissions and large neural network training,2021, arXivpreprint arXiv:2104
 Kilt: a benchmark for knowl-edge intensive language tasks,2021, In NAACL
 E-bert: Efficient-yet-effective entity embeddingsfor bert,2019, In Findings of EMNLP
 Gdumb: A simple approach that questionsour progress in continual learning,2020, In ECCV
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 AutoPrompt:Eliciting knowledge from language models with automatically generated prompts,2020, In EMNLP
 Lamol: Language modeling for lifelong languagelearning,2020, In ICLR
 Fever: a large-scaledataset for fact extraction and verification,2018, In NAACL
 OPUS-MT ¡ª Building open translation services for theWorld,2020, In EAMT
 Facts as experts: Adaptableand interpretable neural memory over symbolic knowledge,2021, In NAACL
 Causal mediation analysis for interpreting neural nlp:The case of gender bias,2020, In NeurIPS
 K-adapter: Infusing knowledge into pre-trained models with adapters,2021, InFindings of ACL
 Transformers: State-of-the-art naturallanguage processing,2020, In EMNLP System Demonstrations
 Beyond goldfish memory: Long-term open-domainconversation,2021, arXiv preprint arXiv:2107
 Lifelong learning with dynamicallyexpandable networks,2018, In ICLR
 Defending against neural fake news,2019, In NeurIPS
 Modifying memories in transformer models,2020, arXiv preprint arXiv:2012
