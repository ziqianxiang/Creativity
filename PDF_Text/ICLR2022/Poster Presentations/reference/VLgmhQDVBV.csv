title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 On the convergence rate of training recUrrentneural networks,2019, In H
 Approximation and estimation bounds for artificial neural networks,1994, Machinelearning
 Reproducing Kernel Hilbert Spaces in Probability andStatistics,2004, Springer
 Not-so-random features,2018, In International Confer-ence on Learning Representations
 On lazy training in differentiable program-ming,2019, In H
 Gradient descent finds globalminima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Optimization theory for ReLU neural networkstrained with normalization layers,2020, In Hal DaUme In and Aarti Singh (eds
 China Math,2020, 63
 Dynamics of deep neural networks and neural tangent hierar-chy,4542,In Hal Daume In and Aarti Singh (eds
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In S
 The implicit bias of gradient descent on nonseparable data,2019, In AlinaBeygelzimer and Daniel Hsu (eds
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In H
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2015, In Yoshua Bengio and Yann LeCun (eds
 Toward moderate overparameterization: Global con-vergence guarantees for training shallow neural networks,2020, IEEE Journal on Selected Areas inInformation Theory
 On the spectral bias of neural networks,2019, In Kamalika Chaudhuriand Ruslan Salakhutdinov (eds
 Uniform approximation of functions with random bases,2008, In 200846th Annual Allerton Conference on Communication
 Random features for large-scale kernel machines,2008, In J
 The convergence rateof neural networks for learned functions of different frequencies,2019, In H
 On learning with integral operators,2010, Journalof Machine Learning Research
 BackProPagation can give rise to spurious local minimaeven for networks without hidden layers,1989, Complex Systems
 Back propagation separates where perceptronsdo,1991, Neural Networks
 The im-plicit bias of gradient descent on separable data,2018, Journal of Machine Learning Research
 On learning over-parameterized neural networks: A functional approx-imation perspective,2019, In H
 Introduction to the non-asymptotic analysis of random matrices,2012, In CompressedSensing
 High-Dimensional Probability: An Introduction with Applications in Data Sci-ence,2018, Cambridge Series in Statistical and Probabilistic Mathematics
 Gradient dynamics of shallow univariate relu networks,2019, In H
 Training behavior of deep neural network infrequency domain,2019, In Tom Gedeon
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 A type of generalization error in-duced by initialization in deep neural networks,2020, In Jianfeng Lu and Rachel Ward (eds
 An improved analysis of training over-parameterized deep neu-ral networks,2019, In H
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine learning
 At the end of this section we will prove a high probability bound of the form,2022,
