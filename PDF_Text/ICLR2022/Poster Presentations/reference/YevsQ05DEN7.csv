title,year,conference
 Implicit regularization in deep matrix fac-torization,2019, In NeurIPS
 A theoreticalanalysis of contrastive unsupervised representation learning,2019, In ICML
 Semi-supervised learning of visual features by non-parametricallypredicting view assignments with support samples,2021, ArXiv
 Vicreg: Variance-invariance-covariance regularization forself-supervised learning,2021, ArXiv
 Implicit gradient regularization,2021, ArXiv
 Deep clustering for unsupervisedlearning of visual features,2018, In ECCV
 Emerging properties in self-supervised vision transformers,2021, ArXiv
 Cheap orthogonal constraints in neural net-works: A simple parametrization of the orthogonal and unitary group,2019, ArXiv
 A simple frameworkfor contrastive learning of visual representations,2020, 2020a
 Exploring simple siamese representation learning,2020, In CVPR
 Improved baselines with momentumcontrastive learning,2020, ArXiv
 Bootstrap your ownlatent: A new approach to self-supervised learning,2020, In NeurIPS
 Implicit regularization in matrix factorization,2018, 2018 Information Theory and ApplicationsWorkshop (ITA)
 Provable guarantees for self-supervised deep learning with spectral contrastive loss,2021, ArXiv
 Deep residual learning for image recog-nition,2016, In CVPR
 Momentum contrast forunsupervised visual representation learning,2020, 2020 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR)
 On featuredecorrelation in self-supervised learning,2021, ArXiv
 Gradient descent aligns the layers of deep linear networks,2019, ArXiv
 Implicit rank-minimizing autoencoder,2020, ArXiv
 Predicting what you already know helps: Prov-able self-supervised learning,2020, ArXiv
 Prototypical contrastive learning ofunsupervised representations,2021, ArXiv
 Self-supervised learning of pretext-invariant representations,2020, 2020IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
 Self-supervised learning of pretext-invariant representa-tions,2020, In CVPR
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2019, ArXiv
 On alignmentin deep linear neural networks,2020, arXiv: Learning
 A mathematical theory of semantic devel-opment in deep neural networks,2019, Proceedings of the National Academy of Sciences
 The implicit bias of gradientdescent on separable data,2018, ArXiv
 Understanding self-supervised learningwith dual deep networks,2020, arXiv preprint arXiv:2010
 Understanding self-supervised learning dynamicswithout contrastive pairs,2021, ArXiv
 Representation learning with contrastive predictivecoding,2018, ArXiv
 Barlow twins: Self-supervisedlearning via redundancy reduction,2021, arXiv preprint arxiv:2103
