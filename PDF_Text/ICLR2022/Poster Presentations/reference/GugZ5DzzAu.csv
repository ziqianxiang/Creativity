title,year,conference
 QSGD:Communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems (NIPS) 
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems (NeurIPS)
 On biased compres-sion for distributed learning,2020, arXiv preprint arXiv:2002
 Language models are few-shotlearners,2020, In H
 SPIDER: Near-optimal non-convexoptimization via stochastic path integrated differential estimator,2018, In NeurIPS Information Pro-cessing Systems
 Linearly convergingerror compensated SGD,2020, In 34th Conference on Neural Information Processing Systems (NeurIPS2020)
 MARINA: Faster non-convex distributed learning with compression,2021, In 38th International Conference on MachineLearning
 Natural compression for distributed deep learning,2019, arXiv preprint arXiv:1905
 Error feedback fixesSignSGD and other gradient compression schemes,2019, In 36th International Conference on MachineLearning (ICML)
 Better theory for SGD in the nonconvex world,2020, arXiv preprintarXiv:2002
 Distributed learning with com-pressed gradients,2018, arXiv preprint arXiv:1806
 Decentralized stochastic optimizationand gossip algorithms with compressed communication,2019, In International Conference on MachineLearning
 Acceleration for compressed gradi-ent descent in distributed and federated optimization,2020, In International Conference on MachineLearning
 Page: A simple and optimal prob-abilistic gradient estimator for nonconvex optimization,2021, In International Conference on MachineLearning
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 SARAH: A novel method for machinelearning problems using stochastic recursive gradient,2017, In The 34th International Conference onMachine Learning
 Error compensated loopless SVRGfor distributed optimization,2020, OPT2020: 12th Annual Workshop on Optimization for MachineLearning (NeurIPS 2020 Workshop)
 FedNL: Making Newton-typemethods applicable to federated learning,2021, arXiv preprint arXiv:2106
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 The error-feedback framework: Better rates for SGDwith delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Sparsified SGD with memory,2018, In Advancesin Neural Information Processing Systems (NeurIPS)
 Doublesqueeze: Parallel stochas-tic gradient descent with double-pass error-compensated compression,2019, In Kamalika Chaudhuriand Ruslan Salakhutdinov (eds
 Attention is all you need,2017, In I
 PowerSGD: Practical low-rank gradientcompression for distributed optimization,2019, In Neural Information Processing Systems
 Error compensated quantized SGDand its applications to large-scale distributed optimization,2018, In Jennifer Dy and Andreas Krause(eds
 Minibatch vs local sgd with shuffling: Tight conver-gence bounds and beyond,2022, In 10th International Conference on Learning Representations (ICLR)
