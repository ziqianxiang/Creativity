title,year,conference
 Exploring the un-certainty properties of neural networksâ€™ implicit priors in the infinite-width limit,2020, arXiv preprintarXiv:2010
 Scale mixtures of normal distributions,1974, Journal of the RoyalStatistical Society: Series B (Methodological)
 Infinite-channel deep stable convolutional neuralnetworks,2021, arXiv:2021
 Stable behaviour of infinitely wide deep neural networks,2020, InProceedings of The 23rd International Conference on Artificial Intelligence and Statistics (AIS-TATS 2020)
 Deep convolutional net-works as shallow Gaussian processes,2019, In International Conference on Learning Representations(ICLR)
 Infinite attention: NNGPand NTK for deep attention networks,2020, In Proceedings of The 37th International Conference onMachine Learning (ICML 2020)
 Neural tangent kernel: convergence and generalization inneural networks,2018, In Advances in Neural Information Processing Systems 31 (NeurIPS 2018)
 Wideneural networks of any depth evolve as linear models under gradient descent,2019, In Advances inNeural Information Processing Systems 32 (NeurIPS 2019)
 Deep neural networks as Gaussian processes,2018, In International Conference onLearning Representations (ICLR)
 Sample-then-optimize posteriorsampling for Bayesian linear models,2017, NeurIPS Workshop on Advances in Approximate BayesianInference
 Gaussian process behaviour in wide deep neural networks,2018, In International Conference onLearning Representations (ICLR)
 Bayesian deep convolutional networks withmany channels are Gaussian processes,2018, In International Conference on Learning Representations(ICLR)
 Variational learning of inducing variables in sparse Gaussian processes,2009, In Proceedingsof The 12th International Conference on Artificial Intelligence and Statistics (AISTATS 2009)
 Tensor programs I: Wide feedforward or recurrent neural networks of Any architectureare Gaussian processes,2019, arXiv preprint arXiv:1910
