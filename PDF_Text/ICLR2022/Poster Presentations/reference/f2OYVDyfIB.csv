title,year,conference
 Etc: Encoding long and structuredinputs in transformers,2020, arXiv preprint arXiv:2004
 Explaining neuralscaling laws,2021, arXiv preprint arXiv:2102
 Revisiting resnets: Improved training and scaling strategies,2021, arXivpreprint arXiv:2103
 Nuancedmetrics for measuring unintended bias with real data for text classification,2019, CoRR
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 UnsuPervisedcross-lingual representation learning at scale,2019, arXiv preprint arXiv:1911
 The benchmark lottery,2021, 2021
 Bert: Pre-training of deePbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Maskgan: better text generation via filling inthe_,2018, arXiv preprint arXiv:1801
 Switch transformers: Scaling to trillion Parametermodels with simPle and efficient sParsity,2021, arXiv preprint arXiv:2101
 Scaling laws for autoregressive generativemodeling,2020, arXiv preprint arXiv:2010
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Perceiver io: Ageneral architecture for structured inPuts & outPuts,2021, arXiv preprint arXiv:2107
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 Albert: A lite bert for self-suPervised learning of language rePresentations,2019, arXiv preprintarXiv:1909
 Fnet: Mixing tokens withfourier transforms,2021, arXiv preprint arXiv:2105
 M6: A chinese multimodal pretrainer,2021, arXiv preprintarXiv:2103
 Unicorn on rainbow:A universal commonsense reasoning model on a new multitask benchmark,2021, arXiv preprintarXiv:2103
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meeting ofthe Associationfor Computational Linguistics: Human Language Technologies
 Stabilizing transformersfor reinforcement learning,7487, In International Conference on Machine Learning
 Carbon emissions and large neural network training,2021, arXivpreprint arXiv:2104
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Get to the point: Summarization withpointer-generator networks,2017, CoRR
 Alphafold: Using ai for scientificdiscovery,2020, DeepMind
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 The evolved transformer,2019, arXiv preprint arXiv:1901
 Revisiting unreasonableeffectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 Lightweight and efficient neural natural language processing with quaternionnetworks,2019, arXiv preprint arXiv:1906
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Sparse sinkhorn attention,2020, arXivpreprint arXiv:2002
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Charformer: Fast character transformers viagradient-based subword tokenization,2021, arXiv preprint arXiv:2106
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceed-ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networksfor NLP
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, arXiv preprint arXiv:1905
 Ex machina: Personal attacks seen at scale,9781, InProceedings of the 26th International Conference on World Wide Web
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
