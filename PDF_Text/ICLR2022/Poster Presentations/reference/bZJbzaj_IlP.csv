title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv e-prints
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, Advances in Neural Information ProcessingSystems
 On deep learning as a remedy for the curse of dimensionalityin nonparametric regression,2019, Annals of Statistics
 Deep equals shallow for ReLU networks in kernel regimes,2021, InICLR 2021-International Conference on Learning Representations
 On the inductive bias of neural tangent kernels,2019, In NeurIPS2019-Thirty-third Conference on Neural Information Processing Systems
 Optimal rates for regularization of statistical inverse learningproblems,2018, Foundations of Computational Mathematics
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 Deep neural tangent kernel and Laplace kernel have the same RKHS,2020, InInternational Conference on Learning Representations
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Proceedings of the 32nd International Con-ference on Neural Information Processing Systems
 Kernel methods for deep learning,2009, Advances in Neural Infor-mation Processing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, In International Conference on Learning Representations
 Onthe similarity between the Laplace and neural tangent kernels,2020, arXiv preprint arXiv:2007
 Delving deep into rectifiers: Surpassinghuman-level performance on Imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Regularization matters: A nonparamet-ric perspective on overparametrized neural network,2021, In International Conference on ArtificialIntelligence and Statistics
 Simple and effective regularization methods for training onnoisily labeled data with generalization guarantee,2019, In International Conference on Learning Rep-resentations
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In NeurIPS
 Early-stopped neural networks are consistent,2021, Advancesin Neural Information Processing Systems
 Fast convergence rates of deep neural networks forclassification,2021, Neural Networks
 Over-parametrized deep neural networks do not generalizewell,2019, arXiv preprint arXiv:1912
 Deep neural networks as Gaussian processes,2018, In International Conference onLearning Representations
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In NeurIPS
 Optimal nonparametric inference via deep neuralnetwork,2019, arXiv preprint arXiv:1902
 Optimal rates for averaged stochastic gradient descent underneural tangent kernel regime,2020, In International Conference on Learning Representations
 Searching for activation functions,2017, arXivpreprint arXiv:1710
 Early stopping and non-parametric regression:an optimal data-dependent stopping rule,2014, The Journal of Machine Learning Research
 Nonparametric regression using deep neural networks with ReLU acti-vation function,2020, Annals of Statistics
 Gersgorin-type theorems for partitioned matrices,2004, In Gersgorin and His Circles
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, Advances in Neural Information ProcessingSystems
 Minimax optimal rates of estimation in high dimensional additivemodels,2016, AnnalsofStatistics
 An improved analysis of training over-parameterized deep neuralnetworks,2019, Advances in neural information processing systems
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arxiv e-prints
 Gradient descent optimizes over-parameterized deep ReLU networks,2020, Machine Learning
