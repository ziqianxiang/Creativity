title,year,conference
 A neural probabilisticlanguage model,2003, The journal of machine learning research
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Variational lossy autoencoder,2016, arXiv preprint arXiv:1611
 Probabilistic box embeddings for uncertain knowledge graph reasoning,2021, arXiv preprintarXiv:2104
 Hypersphericalvariational auto-encoders,2018, arXiv preprint arXiv:1804
 Implicit reparameterization gradients,2018, arXivpreprint arXiv:1805
 Made: Masked autoencoder fordistribution estimation,2015, In International Conference on Machine Learning
 Classes for fast maximum entropy training,2001, In 2001 IEEE International Conferenceon Acoustics
 Flow++: Improving flow-based generative models with variational dequantization and architecture design,2019, In InternationalConference on Machine Learning
 Integer discrete flows andlossless compression,2019, Advances in Neural Information Processing Systems
 Learning discrete distributions bydequantization,2020, In Third Symposium on Advances in Approximate Bayesian Inference
 Argmaxflows and multinomial diffusion: Towards non-autoregressive language models,2021, arXiv preprintarXiv:2102
 Learnable explicit density for continuous latent space and variationalinference,2017, arXiv preprint arXiv:1710
 Neural autoregressiveflows,2018, In International Conference on Machine Learning
 Zinc: afree tool to discover chemistry for biology,2012, Journal of chemical information and modeling
 Junction tree variational autoencoder formolecular graph generation,2323, In International conference on machine learning
 Compound probabilistic context-free grammars forgrammar induction,2019, arXiv preprint arXiv:1906
 Stochastic gradient vb and the variational auto-encoder,2014, InSecond International Conference on Learning Representations
 Smoothing the ge-ometry of probabilistic box embeddings,2018, In International Conference on Learning Representations
 Discrete denoising flows,2021, In ICML Workshop on InvertibleNeural Networks
 Constrained generation of semantically valid graphs viaregularizing variational autoencoders,2018, arXiv preprint arXiv:1809
 Graphnvp: An invertibleflow model for generating molecular graphs,2019, arXiv preprint arXiv:1905
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 A scalable hierarchical distributed language model,2008, Advancesin neural information processing systems
 Hierarchical probabilistic neural network language model,2005, InInternational workshop on artificial intelligence and statistics
 Machine learning: a probabilistic perspective,2012, MIT press
 Survae flows:Surjections to bridge the gap between vaes and flows,2020, Advances in Neural Information ProcessingSystems
 Normalizing flows for probabilistic modeling and inference,2019, arXiv preprintarXiv:1912
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deterministic decoding for discrete data in variationalautoencoders,3046, In International Conference on Artificial Intelligence and Statistics
 Graphaf: aflow-based autoregressive model for molecular graph generation,2020, arXiv preprint arXiv:2001
 A note on the evaluation of generativemodels,2015, arXiv preprint arXiv:1511
 Vae with a vampprior,2018, In International Conference on ArtificialIntelligence and Statistics
 Discrete flows: Invertiblegenerative models of discrete data,2019, Advances in Neural Information Processing Systems
 On the expressivityof bi-lipschitz normalizing flows,2021, arXiv preprint arXiv:2107
 Spherical latent spaces for stable variational autoencoders,2018, arXivpreprint arXiv:1808
 Improved variationalautoencoders for text modeling using dilated convolutions,2017, In International conference on machinelearning
 Latent normalizing flows for discrete sequences,2019, In Interna-tional Conference on Machine Learning
