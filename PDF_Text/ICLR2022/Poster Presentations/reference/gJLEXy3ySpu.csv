title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Random smoothing might be unableto certify 'âˆž robustness for high-dimensional images,2020, arXiv preprint arXiv:2002
 Thermometer encoding: One hotway to resist adversarial examples,2018, In ICLR
 A unifiedview of piecewise linear neural network verification,2018, In NeurIPS
 Mitigating evasion attacks to deep neural networks viaregion-based classification,2017, In ACSAC
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In AISec
 Provably minimally-distorted adversarialexamples,2017, arXiv
 Maximum resilience of artificial neuralnetworks,2017, In ATVA
 Certified adversarial robustness via randomizedsmoothing,2019, In ICML
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Stochastic activation pruning for robust adversarial defense,2018, InICLR
 Training verified learners with learned verifiers,2018, arXiv
 Adual approach to scalable verification of deep networks,2018, In UAI
 A framework for robustness certification of smoothedclassifiers using f-divergences,2019, In International Conference on Learning Representations
 Formal verification of piece-wise linear feed-forward neural networks,2017, In ATVA
 Deep neural networks and mixed integer linear optimization,2018, Con-straints
 Ai2: Safety and robustness certification of neural networks with abstract interpretation,2018, InIEEES&P
 Explaining and harnessing adversarialexamples,2015, In ICLR
 On the effectiveness of interval bound propagationfor training verifiably robust models,2018, arXiv
 Countering adversarialimages using input transformations,2018, In ICLR
 Certified robustness of com-munity detection against adversarial structural perturbation via randomized smoothing,2020, In WWW
 Reluplex: An efficientsmt solver for verifying deep neural networks,2017, In CAV
 Curse of dimensionality onrandomized smoothing for certifiable robustness,2020, In ICML
 Certifiedrobustness to adversarial examples with differential privacy,2019, In IEEE S & P
 Tight certificates of adversarialrobustness for randomly smoothed classifiers,2019, In NeurIPS
 Robustness certificates for sparse adversarial attacks by random-ized ablation,2019, arXiv preprint arXiv:1911
 Second-order adversarial attack andcertifiable robustness,2019, In NeurIPS
 Pointguard: Provably robust 3d point cloudclassification,2021, In CVPR
 Towards robust neural networks viarandom self-ensemble,2018, In ECCV
 An approach to reachability analysis for feed-forward reluneural networks,2017, arXiv
 Characterizing adversarial subspaces using localintrinsic dimensionality,2018, In ICLR
 On detecting adversarialperturbations,2017, In ICLR
 Differentiable abstract interpretation for provablyrobust neural networks,2018, In ICML
 Cascade adversarial machine learningregularized with a unified embedding,2018, In ICLR
 Theoretical evidence for adversarial robustness through randomization,2019, InNeurIPS
 Pointnet: Deep learning on point setsfor 3d classification and segmentation,2017, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Certified defenses against adversarialexamples,2018, In ICLR
 Semidefinite relaxations for certifyingrobustness to adversarial examples,2018, In NeurIPS
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, InNeurIPS
 Black-box smoothing: Aprovable defense for pretrained classifiers,2020, arXiv preprint arXiv:2003
 Towards verification ofartificial neural networks,2015, In MBMV
 Fast andeffective robustness certification,2018, In NeurIPS
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2018, In ICLR
 Peernets: Exploiting peer wisdom against adversarial attacks,2019, InICLR
 Adversarial risk and thedangers of evaluating against weak attacks,2018, In ICML
 On certifying robustness againstbackdoor attacks via randomized smoothing,2020, CVPR 2020 Workshop on Adversarial MachineLearning in Computer Vision
 Certified robustness of graphneural networks against adversarial structural perturbation,2021, In KDD
 Formal security analysisof neural networks using symbolic intervals,2018, In USENIX Security Symposium
 Towards fast computation of certified robustness for relu networks,2018, InICML
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In ICML
 Scaling provable adversarialdefenses,2018, In NeurIPS
 3d shapenets: A deep representation for volumetric shapes,2015, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Mitigating adversarial effectsthrough randomization,2018, In ICLR
 Randomizedsmoothing of all shapes and sizes,2020, In ICML
 Black-box certifica-tion with randomized smoothing: A functional optimization based framework,2020, arXiv preprintarXiv:2002
 Efficient neural networkrobustness certification with general activation functions,2018, In NeurIPS
 Backdoor attacks to graphneural networks,2021, In SACMAT
 Towards assessment of randomized mecha-nisms for certifying adversarial robustness,2020, arXiv preprint arXiv:2005
