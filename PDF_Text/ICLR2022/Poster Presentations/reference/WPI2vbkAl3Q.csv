title,year,conference
 Normal approxima-tion for stochastic gradient descent via non-asymptotic rates of martingale clt,2019, In Alina Beygelz-imer and Daniel Hsu (eds
 Explaining neuralscaling laws,2021, arXiv preprint arXiv:2102
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 On-line learning with a perceptron,1994, Europhysics Letters (EPL)
 Spectrum dependent learning curves inkernel regression and wide neural networks,2020, In International Conference on Machine Learning
 Spectral bias and task-model alignment explaingeneralization in kernel regression and infinitely wide neural networks,2020, Nature Communications
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 Risk bounds for over-parameterized maximum marginclassification on sub-gaussian mixtures,2021, In Thirty-Fifth Conference on Neural Information Pro-cessing Systems
 Finite-sample analysis of interpolating linear classifiers inthe overparameterized regime,2021, Journal of Machine Learning Research
 Nonparametric stochastic approximation with large step-sizes,2016, The Annals of Statistics
 Asymptotic optimality in stochastic optimization,2021, The Annals ofStatistics
 Statistical Mechanics of Learning,2001, Cambridge University Press
 Sobolev norm learning rates for regularized least-squares al-gorithms,2020, Journal of Machine Learning Research
 Dy-namics of stochastic gradient descent for two-layer neural networks in the teacher-studentsetup,2019, In H
 Modeling the influenceof data strUctUre on learning in neUral networks: The hidden manifold model,2020, Phys
 The heavy-tail phenomenon in sgd,2021, InInternational Conference on Machine Learning
 Learning processes in neural networks,1991, Phys
 Accelerat-ing stochastic gradient descent for least squares regression,2018, In Conference On Learning Theory
 Eigenvalues of covariance matrices: Application toneural-network learning,1991, Phys
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,1742, Journal of Statistical Mechanics: Theory and Experiment
 The power of interpolation: Understanding theeffectiveness of sgd in modern over-parametrized learning,2018, In ICML
 The deep bootstrap framework: Goodonline learners are good offline generalizers,2021, In International Conference on Learning Represen-tations
 Neural tangents: Fast and easy infinite neural networks in python,2020, InInternational Conference on Learning Representations
 Acceleration of stochastic approximation by averaging,1992, Siam Journal onControl and Optimization
 The intrinsicdimension of images and its impact on learning,2021, In International Conference on Learning Repre-sentations
 Random features for large-scale kernel machines,2008, In J
 A Stochastic Approximation Method,1951, The Annals of Math-ematical Statistics
 Efficient estimations from a slowly convergent robbins-monro process,1988, 02 1988
 Dynamics of on-line gradient descent learning for multilayer neuralnetworks,1999, 04 1999
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Asymptotic learning curves of kernel methods:empirical data versus teacher-student paradigm,1742, Journal of Statistical Mechanics: Theory andExperiment
 Benign overfitting in ridge regression,2020, arXiv preprintarXiv:2009
 Learning curves for stochastic gradientdescent in linear feedforward networks,2004, In S
 Which algorithmic choices matter at which batch sizes? insightsfrom a noisy quadratic model,2019, Advances in neural information processing systems
 Benign over-fitting of constant-stepsize sgd for linear regression,2021, arXiv preprint arXiv:2103
