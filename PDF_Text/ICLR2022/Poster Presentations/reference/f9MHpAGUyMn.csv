title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 On the relationship betWeen self-attention and convolutional layers,2019, arXiv preprint arXiv:1911
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Learning factored rePresentations in a deePmixture of exPerts,2013, arXiv preprint arXiv:1312
 Levit: A vision transformer in convnet¡¯s clothing for faster inference,2021, InProceedings ofthe IEEE/CVF International Conference on Computer Vision (ICCV)
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, arXiv preprint arXiv:1903
 Channel gatingneural networks,2018, arXiv preprint arXiv:1805
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing Systems
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Towards understanding regularizationin batch normalization,2018, arXiv preprint arXiv:1809
 Switchable normalization forlearning-to-normalize deep representation,2019, IEEE transactions on pattern analysis and machineintelligence
 Differentiabledynamic normalization for learning deep representation,2019, In International Conference on MachineLearning
 Listops: A diagnostic dataset for latent tree learning,2018, NAACLHLT 2018
 Transformers without tears: Improving the normalization ofself-attention,2019, arXiv preprint arXiv:1910
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPatternRecognition
 Imagenet large scale visualrecognition challenge,2015, International journal ofcomputer vision
 Powernorm: Re-thinking batch normalization in transformers,2020, In International Conference on Machine Learning
 Segmenter: Transformer forsemantic segmentation,2021, arXiv preprint arXiv:2105
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 Long range arena: A benchmark for efficienttransformers,2021, In International Conference on Learning Representations
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 Attention is all you need,2017, In Advances in neural informationprocessing SyStemS
 Pvtv2: Improved baselines with pyramid vision transformer,2021, arXiv preprintarXiv:2106
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Understanding andimproving layer normalization,2019, arXiv preprint arXiv:1911
 Improving visual ground-ing with visual-linguistic verification and iterative reasoning,2022, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, arXiv preprint arXiv:2101
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
 Differentiable learning-to-group channels via groupable convolutional neural networks,2019, InProceedings of the IEEE/CVF International Conference on Computer Vision
