title,year,conference
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Breaking the curse of dimensionality with convex neural networks,2017, Journal of MachineLearningResearch (JMLR)
 Deep equals shallow for ReLU networks in kernel regimes,2021, InProceedings of the International Conference on Learning Representations (ICLR)
 On the inductive bias of neural tangent kernels,2019, In Advances inNeural Information Processing Systems (NeurIPS)
 Invariant scattering convolution networks,2013, IEEE Transactions onPattern Analysis and Machine Intelligence (PAMI)
 DeeP neural tangent kernel and laPlace kernel have the same rkhs,2021, InProceedings of the International Conference on Learning Representations (ICLR)
 ImPlicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, In Conference on Learning Theory
 Kernel methods for deeP learning,2009, In Advances in NeuralInformation Processing Systems (NIPS)
 Localized structured Prediction,2019, In Advances inNeural Information Processing Systems (NeurIPS)
 Convolutional rectifier networks as generalized tensor de-comPositions,2016, In Proceedings of the International Conference on Machine Learning (ICML)
 Inductive bias of deep convolutional networks through poolinggeometry,2017, In Proceedings of the International Conference on Learning Representations (ICLR)
 On the mathematical foundations of learning,2002, Bulletin of theAmerican mathematical society
 Toward deeper understanding of neural networks:The power of initialization and a dual view on expressivity,2016, In Advances in Neural InformationProcessing Systems (NIPS)
 Spherical harmonics in p dimensions,2014, World Scientific
 Locality defeats the curse of dimen-sionality in convolutional teacher-student scenarios,2021, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Deep convolutionalnetworks as shallow gaussian processes,2019, In Proceedings of the International Conference onLearning Representations (ICLR)
 Onthe similarity between the laplace and neural tangent kernels,2020, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Implicit bias of gradient descent onlinear convolutional networks,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 A new scheme for the tensor representation,2009, Journal ofFourier analysis and applications
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Denoising and regularization via exploiting the structuralbias of convolutional generators,2020, In Proceedings of the International Conference on LearningRepresentations (ICLR)
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in Neural Information Processing Systems (NIPS)
 Finite versus infinite neural networks: an empirical study,2020, In Advances inNeural Information Processing Systems (NeurIPS)
 Enhanced convolutional neural tangent kernels,2019, arXiv preprint arXiv:1911
 Tensor product space anova models,2000, Annals of Statistics
 Object recognition from local scale-invariant features,1999, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 End-to-End Kernel Learning with Supervised Convolutional Kernel Networks,2016, InAdvances in Neural Information Processing Systems (NIPS)
 Computational separation between convolutional and fully-connected networks,2021, In Proceedings of the International Conference on Learning Representations(ICLR)
 Learning with invariances in randomfeatures and kernel models,2021, In Conference on Learning Theory (COLT)
 Deep vs,2016, shallow networks: An approximation theoryperspective
 Learning with convolution and pooling operations in kernelmethods,2021, arXiv preprint arXiv:2111
 Bayesian deep convolutional networks with manychannels are gaussian processes,2019, In Proceedings of the International Conference on LearningRepresentations (ICLR)
 Why andwhen can deep-but not shallow-networks avoid the curse of dimensionality: a review,2017, InternationalJournal of Automation and Computing
 Image classification withthe fisher vector: Theory and practice,2013, International Journal of Computer Vision (IJCV)
 Harmonic decompositions of convolutional networks,2020, InProceedings of the International Conference on Machine Learning (ICML)
 Nonparametric regression using deep neural networks with reluactivation function,2020, Annals of Statistics
 Neural kernels without tangents,2020, In Proceedings of the InternationalConference on Machine Learning (ICML)
 Tensor products of Sobolev-besov spaces and applications toapproximation from the hyperbolic cross,2009, Journal of Approximation Theory
 Regularization with dot-product kernels,2001, InAdvances in Neural Information Processing Systems (NIPS)
 The unreasonable effectivenessof patches in deep convolutional kernels methods,2021, In Proceedings of the International Conferenceon Learning Representations (ICLR)
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 A mathematical theory of deep convolutional neuralnetworks for feature extraction,2018, IEEE Transactions on Information Theory
 Visualizing and understanding convolutional networks,2014, InProceedings of the European Conference on Computer Vision (ECCV)
 Convexified convolutional neural networks,2017, In InternationalConference on Machine Learning (ICML)
 One successful strategy for learning image recognition models which predatesdeep learning is to rely on simple aggregations of local features,2017, These may be extracted usinghand-crafted procedures (Lowe
