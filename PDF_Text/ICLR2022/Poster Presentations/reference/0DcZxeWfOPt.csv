title,year,conference
 Language models are few-shot learners,2020, Neural InformationProcessing Systems
 Auto-scaling vision transformers without training,2022, In International Conference on Learning Represen-tations
 Knowledge neurons in pretrained trans-formers,2021, CoRR
 Editing factual knowledge in language models,2021, Proceedingsof the 2021 Conference on Empirical Methods in Natural Language Processing
 Model-agnostic meta-learning for fast adapta-tion of deep networks,2017, In ICML
 Meta-learning with warped gradient descent,2020, In International Conference on LearningRepresentations
 Transformer feed-forward layers arekey-value memories,2021, In EMNLP
 Openwebtext corpus,2019, http://Skylion007
 Generalized inner loop meta-learning,2019, arXiv preprint arXiv:1910
 Parameter-efficient transfer learning with diff prun-ing,2021, In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-tics and the 11th International Joint Conference on Natural Language Processing (Volume 1:Long Papers)
 Deep residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Learning the difference that makes a differ-ence with counterfactually-augmented data,2020, In International Conference on Learning Represen-tations
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun
 Overcoming catastrophic forget-ting in neural networks,27, Proceedings of the National Academy of Sciences
 Natural questions: a benchmark for question answering research,2019, Transactions of theAssociation of Computational Linguistics
 Mind the gap: Assessing tempo-ral generalization in neural language models,2021, In A
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, In International Conference on Machine Learning
 Zero-shot relation extraction viareading comprehension,2017, In Proceedings of the 21st Conference on Computational Natural Lan-guage Learning (CoNLL 2017)
 Meta-sgd: Learning to learn quickly forfew shot learning,2017, CoRR
 Catastrophic interference in connectionist networks:The sequential learning problem,1989, Psychology of Learning and Motivation
 Con-tinual lifelong learning with neural networks: A review,2019, Neural Networks
 Film: Visualreasoning with a general conditioning layer,2018, In AAAI
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, Journal of Machine Learning Research
 Connectionist models of recognition memory: constraints imposed by learning andforgetting functions,1990, Psychological review
 Optimization as a model for few-shot learning,2017, In ICLR
 Ed-itable neural networks,2020, In ICLR
 Provable repair of deep neural networks,2021, ArXiv
 FEVER: a large-scale dataset for fact extraction and VERification,2018, In NAACL-HLT
 Attention is all you need,9781, In Proceedings of the 31st In-ternational Conference on Neural Information Processing Systems
 GPT-J-6B: A 6 Billion Parameter Autoregressive LanguageModel,2021, https://github
 HUggingface'stransformers: State-of-the-art natural language processing,2019, CoRR
 Residual learning without normalization viabetter initialization,2019, In International Conference on Learning Representations
