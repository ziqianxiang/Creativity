title,year,conference
 Katyusha: The first direct acceleration of stochastic gradient methods,2017, In ProceedingsOfAnnualACM SIGACT Symposium on Theory ofComputing 49
 Nonasymptotic mixing of the MALA algorithm,2013, IMA Journal ofNumerical Analysis
 Convergence of unadjusted Hamiltonian Monte Carlo for mean-fieldmodels,2020, arXiv preprint arXiv:2009
 Concentration Inequalities: A Nonasymptotic Theory ofIndependence,2013, Oxford University Press
 Fast mixing of Metropolized Hamiltonian MonteCarlo: Benefits of multi-step gradients,2020, Journal of Machine Learning Research
 A generalized neural tangent kernel analysis for two-layerneural networks,2020, arXiv preprint arXiv:2002
 On the global convergence of gradient descent for over-parameterized modelsusing optimal transport,2018, In Advances in Neural Information Processing Systems 31
 On kernel-target alignment,2002, In Advancesin Neural Information Processing Systems 14
 Theoretical guarantees for approximate sampling from smooth and log-concavedensities,2017, Journal of the Royal Statistical Society
 Gradient descent provably optimizes over-parameterizedneural networks,2019, International Conference on Learning Representations 7
 Nonasymptotic convergence analysis for the unadjusted Langevinalgorithm,2017, The Annals of Applied Probability
 Limitations of lazy training of two-layersneural network,2019, In Advances in Neural Information Processing Systems 32
 Logarithmic Sobolev inequalities and stochastic ising models,1987, Journal ofstatistical physics
 Mean-field Langevin dynamics and energy landscape ofneural networks,2019, arXiv preprint arXiv:1905
 Mean-field neural ODEs via relaxed optimal control,2021, arXivpreprint arXiv:1912
 Neural tangent kernel: Convergence and generalization in neuralnetworks,2018, In Advances in Neural Information Processing Systems 31
 Sample estimate of the entropy of a random vector,1987, Problemsof Information Transmission
 Probability in Banach Spaces,1991, Isoperimetry and Processes
 A mean field view of the landscape of two-layer neuralnetworks,2018, Proceedings of the National Academy of Sciences
 POincare and logarithmic Sobolev inequalities by decomposition of theenergy landscape,2014, The Annals of Probability
 Foundations of Machine Learning,2012, The MIT Press
 Doubly accelerated stochastic variance reduced dual averaging method forregularized empirical risk minimization,2017, In Advances in Neural Information Processing Systems30
 Smooth minimization of non-smooth functions,2005, Mathematical programming
 Primal-dual subgradient methods for convex problems,2009, Mathematical programming
 Stochastic particle gradient descent for infinite ensembles,2017, arXiv preprintarXiv:1712
 Optimal rates for averaged stochastic gradient descent under neural tangentkernel regime,2020, arXiv preprint arXiv:2006
 Particle dual averaging: Optimization of mean field neuralnetworks with global convergence rate analysis,2021, In Advances in Neural Information ProcessingSystems 34
 Random coding strategies for minimum entropy,1975, IEEE Transactions on InformationTheory
 Convex Analysis,1970, Princeton University Press
 Augmented Lagrangians and applications of the proximal point algorithm inconvex programming,1976, Mathematics of Operations Research
 Duality and stability in extremum problems involving convex functions,1967, PacificJournal of Mathematics
 Trainability and accuracy of neural networks: An interactingparticle system approach,2018, arXiv preprint arXiv:1805
 Stochastic dual coordinate ascent methods for regularized lossminimization,2013, Journal of Machine Learning Research
 Accelerated mini-batch stochastic dual coordinate ascent,2013, InAdvances in Neural Information Processing Systems 26
 Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:Optimal rate and curse of dimensionality,2019, In International Conference on Learning Representations
 Mini-batch primal and dual methods for SVMs,2013, InProceedings of the 30th International Conference on Machine Learning
 Rapid convergence of the unadjusted Langevin algorithm: Isoperimetrysuffices,2019, In Advances in Neural Information Processing Systems 32
 Dual averaging method for regularized stochastic learning and online optimization,2009, InAdvances in Neural Information Processing Systems 22
 On the power and limitations of random features for understanding neuralnetworks,2019, In Advances in Neural Information Processing Systems 32
 Stochastic gradient descent optimizes over-parameterized deepReLU networks,2018, arXiv preprint arXiv:1811
