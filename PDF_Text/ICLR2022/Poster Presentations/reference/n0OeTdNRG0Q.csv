title,year,conference
 Entropy-SGD: Biasing gradi-ent descent into wide valleys,2019, Journal of Statistical Mechanics: Theory and Experiment
 When vision transformers outperform resnetswithout pretraining or strong data augmentations,2021, arXiv preprint arXiv:2106
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Query-efficient meta attack todeep neural networks,2019, In International Conference on Learning Representations
 Sharpness-aware minimiza-tion for efficiently improving generalization,2020, In International Conference on Learning Represen-tations
 Using early-learningregularization to classify real-world noisy data,2021, arXiv preprint arXiv:2105
 Deep pyramidal residual networks,2017, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Simplifying neural nets by discovering flat minima,1995, InAdvances in neural information processing systems
 Scaling up visual and vision-language representation learningwith noisy text supervision,4904, In International Conference on Machine Learning
 Fantas-tic generalization measures and where to find them,2019, In International Conference on LearningRepresentations
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 CIFAR-10 and CIFAR-100 datasets,2009, URl:https://www
 Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks,2021, In International Con-ference on Machine Learning
 Gradient harmonized single-stage detector,2019, In Proceedingsof the AAAI Conference on Artificial Intelligence
 Visualizing the loss land-scape of neural nets,2018, Advances in neural information processing systems
 On the loss landscapeof adversarial training: Identifying challenges and how to overcome them,2020, Advances in NeuralInformation Processing Systems
 Towards efficient and scalablesharpness-aware minimization,2022, arXiv preprint arXiv:2203
 Sgdr: Stochastic gradient descent with warm restarts,2017,2017
 Exploring general-ization in deep learning,2017, Advances in neural information processing systems
 Meta pseudo labels,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Adversarial robustness through locallinearization,2019, Advances in Neural Information Processing Systems
 Upanets: Learning from the universal pixel attentionnetworks,2021, arXiv preprint arXiv:2103
 Adversarial weight perturbation helps robust gener-alization,2020, Advances in Neural Information Processing Systems
 On robustness of neural ordinary differentialequations,2019, In International Conference on Learning Representations
 Volo: Vision outlooker forvisual recognition,2021, arXiv preprint arXiv:2106
 Wide residual networks,2016, In British Machine VisionConference 2016
 Regularizing neural networks via adversarialmodel perturbation,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Towards understandingwhy lookahead generalizes better than SGD and beyond,2021, In Proc
