title,year,conference
 Maximum a posteriori policy optimisation,2018, In Proc
 An optimistic perspective on offlinereinforcement learning,2020, In Proc
 Averaged-DQN: Variance reduction and stabiliza-tion for deep reinforcement learning,2017, In Proc
 Layer normalization,2016, arXiv PrePrintarXiv:1607
 OpenAI Gym,2016, arXiv PrePrint arXiv:1606
 Improving computational efficiency invisual reinforcement learning via stored embeddings,2021, arXiv PrePrint arXiv:2103
 Randomized ensembled double Q-learning: Learning fast without a model,2021, In Proc
 Deep reinforcementlearning in a handful of trials using probabilistic dynamics models,2018, In Proc
 Masksembles for uncertaintyestimation,2021, In Proc
 Addressing function approximation error in actor-critic methods,2018, In Proc
 Dropout as a Bayesian approximation: Representing modeluncertainty in deep learning,2016, In Proc
 Improving PILCO with Bayesian neuralnetwork dynamics models,2016, In Data-EffiCient MaChine Learning WOrkShOP on ICML
 Concrete dropout,2017, In Proc
 Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor,2018, In Proc
 Soft actor-critic algorithms and applica-tions,2018, arXiv PrePrint arXiv:1812
 Deep reinforcement learning with regularized convolutional neural fitted Q itera-tion,2016, https://www
 MEPG: A min-imalist ensemble policy gradient framework for deep reinforcement learning,2021, arXiv PrePrintarXiv:2109
 Meta-model-based meta-policy optimization,2021, In Proc
 Acme: A research framework for distributedreinforcement learning,2020, arXiv Preprint arXiv:2006
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proc
 When to trust your model: Model-based policy optimization,2019, In Proc
 Way off-policy batch deep reinforcement learning ofimplicit human preferences in dialog,2019, arXiv PrePrint arXiv:1907
 Uncertainty-awarereinforcement learning for collision avoidance,2017, arXiv PrePrint arXiv:1702
 Adam: A method for stochastic optimization,2015, In Proc
 Model-ensembletrust-region policy optimization,2018, In Proc
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Proc
 Maxmin Q-learning: Controllingthe estimation bias of Q-learning,2020, In Proc
 Context-aware dynam-ics model for generalization in model-based reinforcement learning,2020, In Proc
 SUNRISE: A simple unifiedframework for ensemble learning in deep reinforcement learning,2021, In Proc
 Continuous control with deep reinforcement learning,2015, arXivPrePrint arXiv:1509
 DSAC: Distributional softactor critic for risk-sensitive reinforcement learning,2020, In ReinfOrCement Learning for Real LifeWorkShOP at ICML 2019
 In Proc,2019, NeUrIPS
 Efficient exploration with doubleuncertain value networks,2017, arXiv PrePrint arXiv:1711
 Deep exploration viabootstrapped DQN,2016, In Proc
 Randomized prior functions for deep reinforcementlearning,2018, In Proc
 Can yoU trUst yoUr model's Uncertainty? evalUatingpredictive uncertainty under dataset shift,2019, In Proc
 Proximal policyoptimization algorithms,2017, arXiv Preprint arXiv:1707
 Model-based policy optimization with unsuper-vised model adaptation,2020, In Proc
 Attention is all you need,2017, In Proc
 Group normalization,2018, In Proc
 Understanding andimproving layer normalization,2019, In Proc
 MOPO: model-based offline policy optimization,2020, In Proc
 Learninginvariant representations for reinforcement learning without reconstruction,2021, In Proc
