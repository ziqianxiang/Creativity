title,year,conference
 Binarybert: Pushing the limit of BERT quantization,2020, CoRR
 Post training 4-bit quantization of convolutionnetworks for rapid-deployment,2018, CoRR
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL-HLT (1)
 Compressing bert: Studying the effects ofweight pruning on transfer learning,2020, arXiv preprint arXiv:2002
 Simultaneously optimizing weight and quantizer of ternary neuralnetwork using truncated gaussian approximation,2019, In IEEE CVPR
 Dynabert: Dynamic bertwith adaptive width and depth,2020, arXiv preprint arXiv:2004
 Albert: A lite bert for self-supervised learning of language representations,2020, In ICLR
 Bi-real net:Enhancing the performance of 1-bit cnns with improved representational capability and advancedtraining algorithm,2018, In Vittorio Ferrari
 Training binary neuralnetworks with real-to-binary convolutions,2020, In ICLR
 Structured pruning of a bert-based questionanswering model,2019, CoRR
 Forward and backward information retention for accurate binary neural networks,2020, In CVPR
 A stack-propagationframework with token-level intent detection for spoken language understanding,2019, arXiv preprintarXiv:1909
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In Bastian Leibe
 Q-BERT: hessian based ultra low precision quantization of BERT,2020, In AAAI
 Similarity-preserving knowledge distillation,2019, In CVPR
 Attention is all you need,2017, In NeurIPS
 A multiscale visualization of attention in the transformer model,2019, In Marta R
 Two-stepquantization for low-bit neural networks,2018, In CVPR
 Learning efficient binarized object detectors with in-formation compression,2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Google¡¯s neuralmachine translation system: Bridging the gap between human and machine translation,2016, CoRR
 Improving bert fine-tuning via self-ensemble and self-distillation,2020, arXiv preprint arXiv:2002
 Recu: Reviving the dead weights in binary neural networks,2021, arXiv preprintarXiv:2103
 GOBO: quantizingattention-based NLP models for low latency and energy efficient inference,2020, In MICRO
 Q8BERT: quantized 8bit BERT,2019, InNeurIPS
 Ternarybert:Distillation-aware ultra-low bit BERT,2020, In Bonnie Webber
 Dorefa-net: Traininglow bitwidth convolutional neural networks with low bitwidth gradients,2016, CoRR
