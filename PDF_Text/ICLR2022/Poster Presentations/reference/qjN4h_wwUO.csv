title,year,conference
 Keep the gradients flowing: Using gradientflow to study sparse network optimization,2021, ArXiv
 Theoretical analysis of auto rate-tuning by batchnormalization,2018, arXiv preprint arXiv:1812
 On warm-starting neural network training,2020, InH
 Breaking the curse of dimensionality with convex neural networks,2017, The Journal ofMachine Learning Research
 Random search for hyper-parameter optimization,2012, Journal ofmachine learning research
 Dota 2 with large scale deep reinforcementlearning,2019, ArXiv
 Net2net: Accelerating learning via knowledgetransfer,2016, In 4th International Conference on Learning Representations
 AdaNet:Adaptive structural learning of artificial neural networks,2017, In Proceedings of the 34th InternationalConference on Machine Learning
 Finding the needle in the haystackwith convolutions: on the benefits of architectural bias,2019, In Advances in Neural Information Pro-cessing Systems 32
 Sparse networks from scratch: Faster training without losingperformance,2019, ArXiv
 Simple and efficient architecture search forconvolutional neural networks,2017, arXiv preprint arXiv:1711
 Neural architecture search: A survey,2019, TheJournal of Machine Learning Research
 Rigging the lottery:Making all tickets winners,2020, In Proceedings of Machine Learning and Systems 2020
 Gradient flow in sparse neuralnetworks and how lottery tickets win,2022, In AAAI Conference on Artificial Intelligence
 The cascade-correlation learning architecture,1990, InD
 Greedy function approximation: a gradient boosting machine,2001, Annals ofstatistics 
 Local minima and plateaus in hierarchical structures ofmultilayer perceptrons,2000, Neural networks
 Lessons in neural network training: Overfitting may beharder than expected,1997, In Proceedings of the National Conference on Artificial Intelligence
 The largelearning rate phase of deep learning: the catapult mechanism,2020, ArXiv
 An exponential learning rate schedule for deep learning,2020, ArXiv
 NeST: A Neural NetworkSynthesis Tool Based on a Grow-and-Prune Paradigm,2017, ArXiv
 Rethinking the Value ofNetwork Pruning,2018, ArXiv
 Compnet: Neural networks growing via the compact networkmorphism,2018, arXiv preprint arXiv:1804
 Introductory lectures on convex optimization: A basic course,2003, Springer Science &Business Media
 Towards learning convolutions from scratch,2020, ArXiv
 A resource-allocating network for function interpolation,1991, Neural Computation
 `1 regularization in infinite dimen-sional feature spaces,2007, In International Conference on Computational Learning Theory
 Progressive neural networks,2016, ArXiv
 Knowledge-based cascade-correlation,2000, volume 5
 Very deep convolutional networks for large-scale imagerecognition,2015, CoRR
 Evolving neural networks through augmenting topolo-gies,2002, Evol
 Perturbation theory for the singular value decomposition,1998, Technical report
 Deep learningâ€™s diminish-ing returns,2021, IEEE Spectrum
 Parallel multi channel convolution usinggeneral matrix multiplication,2017, In 2017 IEEE 28th international conference on application-specificsystems
 No pressure! addressing the problem of local minima in manifold learningalgorithms,2019, arXiv preprint arXiv:1906
 Energy-aware neural architec-ture optimization with fast splitting steepest descent,2019, arXiv preprint arXiv:1910
 Network morphism,2016, In InternationalConference on Machine Learning
 Autogrow: Automatic layer growing in deep convolutionalnetworks,2019, Arxiv
 Firefly neural architecture descent: a gen-eral approach for growing neural networks,2020, In Advances in Neural Information Process-ing Systems
 Steepest descent neural architecture opti-mization: Escaping local optimum with signed neural splitting,2020, arXiv preprint arXiv:2003
 Growing efficient deep networksby structured continuous sparsification,2021, In International Conference on Learning Representations
 Wide residual networks,2016, ArXiv
