title,year,conference
 Zero-costproxies for lightweight nas,2021, arXiv preprint arXiv:2101
 Extremely large minibatch sgd: Training resnet-50on imagenet in 15 minutes,2017, CoRR
 Soft-nms-improving objectdetection with one line of code,2017, In Proceedings of the IEEE international conference on computervision
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, arXiv preprint arXiv:2005
 Glit: Neural architecture search for global and local image transformer,2021, arXiv preprintarXiv:2107
 Crossvit: Cross-attention multi-scale visiontransformer for image classification,2021, arXiv preprint arXiv:2103
 Pre-trained image processing transformer,2020, arXiv preprintarXiv:2012
 Encoder-decoder with atrous separable convolution for semantic image segmentation,2018, In Proceedings of theEuropean conference on computer vision (ECCV)
 Autoformer: Searching transformersfor visual recognition,2021, arXiv preprint arXiv:2107
 Chasing sparsityin vision transformers: An end-to-end exploration,2021, Advances in Neural Information ProcessingSystems
 Neural architecture search on imagenet infour gpu hours: A theoretically inspired perspective,2021, International Conference on LearningRepresentations (ICLR)
 Understanding and accelerating neural architecture search with training-free and theory-grounded metrics,2021, arXiv preprint arXiv:2108
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Up-detr: Unsupervised pre-training forobject detection with transformers,2020, arXiv preprint arXiv:2011
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Searching for a robust neural architecture in four gpu hours,2019, InProceedings of the IEEE Conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Fractrain: Fractionally squeezing bit savings both temporally and spatiallyfor efficient dnn training,2020, Advances in Neural Information Processing Systems
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Infinite attention: Nngp and ntkfor deep attention networks,4376, In International Conference on Machine Learning
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Highly scalable deep learning training system withmixed-precision: Training imagenet in four minutes,2018, arXiv preprint arXiv:1807
 Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neuralarchitecture search,2021, arXiv preprint arXiv:2103
 Searching for efficient multi-stage vision transform-ers,2021, arXiv preprint arXiv:2109
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Neural architecture search withouttraining,2020, arXiv preprint arXiv:2006
 Ia-red2 : Interpretability-aware redundancy reduction for vision transformers,2021, Advances in NeuralInformation Processing Systems
 Exponentialexpressivity in deep neural networks through transient chaos,2016, arXiv preprint arXiv:1606
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the aaai conference on artificial intelligence
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Revisiting unreasonableeffectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 Rethinking transformer-based setprediction for object detection,2020, arXiv preprint arXiv:2011
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Training data-effiCient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, Advances in neural information processingsystems
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 E2-train: Training state-of-the-art cnns with over 80% energy savings,2019, arXiv preprintarXiv:1910
 End-to-end video instance segmentation with transformers,2020, arXiv preprint arXiv:2011
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Disentangling trainability and general-ization in deep learning,2019, arXiv preprint arXiv:1912
 Tensor programs ii: Neural tangent kernel for any architecture,2020, arXiv preprintarXiv:2006
 Imagenet training inminutes,2018, Proceedings of the 47th International Conference on Parallel Processing - ICPP 2018
 Incorporatingconvolution designs into visual transformers,2021, arXiv preprint arXiv:2103
 Revisiting knowledge distillation vialabel smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Tokens-to-token vit: Training vision transformers from scratch on imagenet,2021, arXivpreprint arXiv:2101
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 End-to-end objectdetection with adaptive clustering transformer,2020, arXiv preprint arXiv:2011
 Deepvit: Towards deeper vision transformer,2021, arXiv preprint arXiv:2103
 Deformable detr:Deformable transformers for end-to-end object detection,2020, arXiv preprint arXiv:2010
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 Learning transferable architecturesfor scalable image recognition,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition
