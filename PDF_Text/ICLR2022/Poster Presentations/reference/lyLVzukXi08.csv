title,year,conference
 Learning to learn by gradient descent by gradientdescent,2016, In NeurIPS
 A closerlook at few-shot classification,2019, In ICLR
 Variational lossy autoencoder,2017, In ICLR
 Avoiding latent variable collapsewith generative skip models,2018, In AISTAT
 Towards A Neural Statistician,2017, In ICLR
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In PMLR
 Probabilistic model-agnostic meta-learning,2018, InNeurIPS
 Meta-learning stationary stochastic process prediction with convolutional neu-ral processes,2020, In NeurIPS
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In ICML
 Conditional neural processes,2018, In ICML
 Neural processes,2018, In ICML Workshop on Theoretical Foundations andApplications of Deep Generative Models
 Meta-learning probabilistic inference for prediction,2018, In ICLR
 Convolutional conditional neural processes,2020, In ICLR
 Recasting gradient-based meta-learning as hierarchical bayes,2018, In ICLR
 Probing uncertainty estimatesof neural processes,2019, In NeurIPS Workshop on Bayesian Deep Learning
 Training products of experts by minimizing contrastive divergence,2002, Neuralcomputation
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Elbo surgery: yet another way to carve up the varia-tional evidence lower bound,2016, In NeurIPS Workshop in Advances in Approximate Bayesian Infer-ence
 Meta-learning in neuralnetworks: A survey,2021, TPAMI
 Variational gaussian dropout isnot bayesian,2017, In NeurIPS
 Variational bayesian dropout:pitfalls and fixes,2018, In ICML
 Meta-learning with shared amortizedvariational inference,2020, In ICML
 Attentive neural processes,2019, In ICML
 Adam: A method for stochastic optimization,2015, In ICLR
 Auto-encoding variational bayes,2013, In ICLR
 Semi-supervised learn-ing with deep generative models,2014, In NeurIPS
 Variational dropout and the local reparameteri-zation trick,2015, In NeurIPS
 Im-proved variational inference with inverse autoregressive flow,2016, In NeurIPS
 Human-level concept learningthrough probabilistic program induction,2015, Science
 Empirical evaluation of neural process objectives,2018, In NeurIPS Workshop on Bayesian DeepLearning
 Gradient-based learning applied todocument recognition,1998, In Proceedings of the IEEE
 Deep learning,2015, nature
 Meta dropout: Learning toperturb latent features for generalization,2019, In ICLR
 Settransformer: A framework for attention-based permutation-invariant neural networks,2019, In ICML
 Meta-learning withdifferentiable convex optimization,2019, In CVPR
 Meta-sgd: Learning to learn quickly for few-shot learning,2017, arXiv preprint arXiv:1707
 Variational bayesian dropoutwith a hierarchical prior,2019, In CVPR
 Donâ€™t blame the elbo! alinear vae perspective on posterior collapse,2019, In NeurIPS
 Variational dropout sparsifies deep neuralnetworks,2017, In ICML
 Meta-neural networks that learn by learning,1992, In IJCNN
 Uncertainty in model-agnostic meta-learning using variational inference,2020, In WACV
 Stochastic differential equations,2003, Springer
 Amortized bayesian meta-learning,2019, In ICLR
 Optimization as a model for few-shot learning,2017, In ICLR
 Prototypical networks for few-shot learning,2017, InNeurIPS
 HoWto train deep variational autoencoders and probabilistic ladder networks,2016, In ICML
 Varia-tional autoencoder with implicit optimal priors,2019, In AAAI
 On statistical thinking in deep learning a blogpost,2019, https://imstat
 Vae with a vampprior,2018, In AISTATS
 Match-ing networks for one shot learning,2016, In NeurIPS
 Regularization of neuralnetworks using dropconnect,2013, In ICML
 Fast dropout training,2013, In ICML
 Tackling over-pruning in variationalautoencoders,2017, In ICML
