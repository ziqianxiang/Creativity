title,year,conference
 Learning representations by maximizingmutual information across views,2019, In NeurIPS
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, In ECCV
 Deep clustering for unsuper-vised learning of visual features,2018, In ECCV
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Pre-trained image processing transformer,2020, arXiv preprintarXiv:2012
 Generative pretraining from pixels,2020, In ICML
 A simple framework forcontrastive learning of visual representations,2020, ICML
 Bigself-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 An empirical study of training self-supervised visualtransformers,2021, arXiv preprint arXiv:2104
 Uniter: Learning universal image-text representations,2019, arXiv preprintarXiv:1909
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, Advances in neuralinformation processing systems
 UP-DETR: Unsupervised pre-training forobject detection with transformers,2020, arXiv preprint arXiv:2011
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, NAACL
 Unsupervised visual representation learning bycontext prediction,2015, In ICCV
 Large scale adversarial representation learning,2019, In NeurIPS
 An imageis worth 16x16 words: Transformers for image recognition at scale,2021, ICLR
 Training Batchnorm and only Batchnorm: Onthe expressive power of random features in CNNs,2020, arXiv preprint arXiv:2003
 Unsupervised representation learning bypredicting image rotations,2018, arXiv preprint arXiv:1803
 Self-supervised pretraining of visualfeatures in the wild,2021, arXiv preprint arXiv:2103
 Bootstrap your own latent: A new approach to self-supervised learning,2020, arXiv preprintarXiv:2006
 Deep residual learning for imagerecognition,2016, In CVPR
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Invariant information clustering for unsupervisedimage classification and segmentation,2019, In ICCV
 Learning representations for automaticcolorization,2016, In ECCV
 Self-supervisedpre-training with hard examples improves visual representations,2020, arXiv preprint arXiv:2012
 Unicoder-VL: A universal encoderfor vision and language by cross-modal pre-training,2019, arXiv preprint arXiv:1908
 Prototypical contrastivelearning of unsupervised representations,2020, arXiv preprint arXiv:2005
 Visualbert: A simpleand performant baseline for vision and language,2019, arXiv preprint arXiv:1908
 Webvision database: Visuallearning and understanding from web data,2017, arXiv preprint arXiv:1708
 Selective kernel networks,2019, In CVPR
 Oscar: Object-semantics aligned pre-training for vision-language tasks,2020, In ECCV
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Fixing weight decay regularization in Adam,2018, arXiv preprintarXiv:1706
 VilBERT: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, NeurIPS
 Unsupervised learning of visual representations by solving jigsawpuzzles,2016, In ECCV
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Image transformer,2018, In International Conference on Machine Learning
 Contextencoders: Feature learning by inpainting,2016, In CVPR
 Improving language under-standing by generative pre-training,2018, OpenAI Blog
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 Green AI,2020, Communications of theACM
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 VL-BERT: Pre-trainingof generic visual-linguistic representations,2019, arXiv preprint arXiv:1908
 LXMERT: Learning cross-modality encoder representations fromtransformers,2019, EMNLP
 Whatmakes for good views for contrastive learning,2020, arXiv preprint arXiv:2005
 MLP-mixer: Anall-MLP architecture for vision,2021, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 Selfie: Self-supervised pretraining for imageembedding,2019, arXiv preprint arXiv:1906
 Attention is all you need,2017, In NIPS
 Scaling local self-attention for parameter efficient visual backbones,2021, CVPR
 Max-deeplab:End-to-end panoptic segmentation with mask transformers,2020, arXiv preprint arXiv:2012
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Dense contrastive learning forself-supervised visual pre-training,2020, arXiv preprint arXiv:2011
 End-to-end video instance segmentation with transformers,2020, arXiv preprint arXiv:2011
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Detco:Unsupervised contrastive learning for object detection,2021, arXiv preprint arXiv:2102
 Self-supervisedlearning with swin transformers,2021, arXiv preprint arXiv:2105
 Propagate yourself:Exploring pixel-level consistency for unsupervised visual representation learning,2021, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Instance localization for self-superviseddetection pretraining,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Learning texture transformernetwork for image super-resolution,2020, In CVPR
 Joint unsupervised learning of deep representations andimage clusters,2016, In CVPR
 Divide and contrast: Self-supervisedlearning from uncurated data,2021, arXiv preprint arXiv:2105
 Online deep clusteringfor unsupervised representation learning,2020, In CVPR
 Colorful image colorization,2016, In ECCV
 Split-brain autoencoders: Unsupervised learningby cross-channel prediction,2017, In CVPR
 End-to-end objectdetection with adaptive clustering transformer,2020, arXiv preprint arXiv:2011
 Unifiedvision-language pre-training for image captioning and VQA,2020, AAAI
 Deformable detr:Deformable transformers for end-to-end object detection,2020, arXiv preprint arXiv:2010
 Local aggregation for unsupervised learning ofvisual embeddings,2019, In CVPR
