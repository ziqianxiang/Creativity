title,year,conference
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Breaking the curse of dimensionality with convex neural networks,2017, The Journal ofMachine Learning Research
 Learning two layer rectified neural networksin polynomial time,2019, In Conference on Learning Theory
 Approximation and estimation bounds for artificial neural networks,1994, Machinelearning
 Spectrally-normalized margin bounds for neuralnetworks,2017, arXiv preprint arXiv:1706
 Gradient descent with identity initialization effi-ciently learns positive definite linear transformations by deep residual networks,2018, In Internationalconference on machine learning
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, In International conference on machine learning
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, arXiv preprint arXiv:1805
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, In Conference on Learning Theory
 Depth separation for neural networks,2017, In Conference on Learning Theory
 Training neural networks as learning data-adaptive kernels:Provable representation and approximation benefits,2020, Journal of the American Statistical Associa-tion
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Domain transfer multiple kernel learning,2012, IEEE Transac-tions on Pattern Analysis and Machine Intelligence
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Conference on learning theory
 Escaping from saddle points â€” onlinestochastic gradient for tensor decomposition,2015, In Peter Grunwald
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Limitations of lazytraining of two-layers neural networks,2019, arXiv preprint arXiv:1906
 Deep neural networksfor acoustic modeling in speech recognition: The shared views of four research groups,2012, IEEESignal processing magazine
 Neural tangent kernel: Convergence and gener-alization in neural networks,2018, arXiv preprint arXiv:1806
 Beating the perils of non-convexity: Guar-anteed training of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 Analysis of a two-layer neural network via displace-ment convexity,2019, ArXiv
 Deep learning without poor local minima,2016, In nips
 Pac-bayesian margin bounds for con-volutional neural networks-technical report,2017, arXiv preprint arXiv:1801
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Just interpolate: Kernel ridgeless regression can general-ize,2020, Annals of Statistics
 A mean-field analysis of deepresnet and beyond: Towards provable optimization via overparameterization from depth,2020, In icml
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, In Conference on Learning Theory
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2018, In International Conference on Learn-ing Representations
 The loss surface of deep and wide neural networks,2017, In Interna-tional conference on machine learning
 On theexpressive power of deep neural networks,2017, In Doina Precup and Yee Whye Teh (eds
 Consistency of interpolation with laplace kernels is a high-dimensional phenomenon,2019, In Conference on Learning Theory
 More efficiency in mul-tiple kernel learning,2007, In Proceedings of the 24th international conference on Machine learning
 Depth separation in relu networks for approximating smooth non-linear functions,2016, arXiv preprint arXiv:1610
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Mean field analysis of neural networks: A centrallimit theorem,2020, Stochastic Processes and their Applications
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 Large scale multiplekernel learning,2006, The Journal of Machine Learning Research
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 Symmetry-breaking convergence analysis of certain two-layered neural networkswith relu nonlinearity,2016, Network
 On the margin theory of feedforward neuralnetworks,2018, 2018
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, 2019
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Recovery guaranteesfor one-hidden-layer neural networks,2017, In International conference on machine learning
 Non-vacuous gen-eralization bounds at the imagenet scale: a pac-bayesian compression approach,2018, arXiv preprintarXiv:1804
 Stochastic gradient descent optimizes over-parameterized deep relu networks,2018, arxiv e-prints
