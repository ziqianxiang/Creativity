title,year,conference
 Learning in high dimension always amountsto extrapolation,2021, arXiv preprint arXiv:2110
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT
 Entropy-sgd: Biasing gradientdescent into wide valleys,2019, Journal of Statistical Mechanics: Theory and Experiment
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of machine learning research
 Sharpness-aware minimiza-tion for efficiently improving generalization,2020, arXiv preprint arXiv:2010
 Shake-shake regularization,2017, arXiv preprint arXiv:1705
 Adamp: Slowing down the slowdown for momentum optimiz-ers on scale-invariant weights,2020, arXiv preprint arXiv:2006
 Simplifying neural nets by discovering flat minima,1995, InAdvances in neural information processing systems
 Averaging weights leads to wider optima and better generalization,2018, arXiv preprintarXiv:1803
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks,2021, arXiv preprintarXiv:2102
 Adaptive estimation of a quadratic functional by model selec-tion,2000, Annals of Statistics 
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 Extrapolation for large-batch training indeep learning,2020, In International Conference on Machine Learning
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Simplified pac-bayesian margin bounds,2003, In Learning theory and Kernel ma-chines
 Pac-bayesian model averaging,1999, In Proceedings of the twelfth annual confer-ence on Computational learning theory
 Praktische Verfahren der gleichungsauflÎ¸sung,1929, ZAMM-Journal of Applied Mathematics and MechanicsZZeitschrift fiir Angewandte Mathematik UndMechanik
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Automated flower classification over a large numberof classes,2008, In 2008 Sixth Indian Conference on Computer Vision
 Cats and dogs,2012, In 2012IEEE conference on computer vision and pattern recognition
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Learning internal representationsby error propagation,1985, Technical report
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Regularization matters: Generalization andoptimization of neural nets vs their induced kernel,2019, 2019
 Positive-negative momentum: Manip-ulating stochastic gradient noise to improve generalization,2021, arXiv preprint arXiv:2103
 Salr: Sharpness-aware learning rates for improvedgeneralization,2020, arXiv preprint arXiv:2011
 Adaptive meth-ods for nonconvex optimization,2018, In Advances in neural information processing systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Understandingdeep learning requires rethinking generalization,2017, 2017a
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Regularizing neural networks via adversarialmodel perturbation,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Towards theoretically under-standing why sgd generalizes better than adam in deep learning,2020, arXiv preprint arXiv:2010
 Adabelief optimizer: Adapting stepsizes by the belief in ob-served gradients,2020, arXiv preprint arXiv:2010
