title,year,conference
 Deep learning with differential privacy,2016, In Proceedings of the 2016 ACM Conferenceon Computer and Communications Security
 Intrinsic dimensionality explains the ef-fectiveness of language model fine-tuning,2020, arXiv preprint arXiv:2012
 Limits of private learning with access to public data,2019, InAdvances in Neural Information Processing Systems 32
 Public data-assisted mirror de-scent for private model training,2021, arXiv preprint arXiv:2112
 Large-scale differen-tially private BERT,2021, arXiv preprint arXiv:2108
 Private empirical risk minimization: Effi-cient algorithms and tight error bounds,2014, In Proceedings of the 55th Annual IEEE Symposium onFoundations of Computer Science
 Model-agnostic private learning,2018, InAdvances in Neural Information Processing Systems 31
 Learning from mixtures of private and publicpopulations,2020, In Advances in Neural Information Processing Systems 33
 Benchmarking differential privacy and federated learning for BERT models,2021, arXivpreprint arXiv:2106
 Private learning and sanitization: Pure vs,2016, approxi-mate differential privacy
 Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models,2021, arXiv preprint arXiv:
 Language models are few-shot learners,2020, In Advances inNeural Information Processing Systems 33
 Fingerprinting codes and the price of approximatedifferential privacy,2014, In Proceedings of the 46th Annual ACM Symposium on the Theory of Com-Puting
 The secret sharer:Evaluating and testing unintended memorization in neural networks,2019, In 28th USENIX SecuritySymposium
 Extracting training data from large language models,2021, In 30th USENIX Security Symposium
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Calibrating noise to sensitivityin private data analysis,2006, In Proceedings of the 3rd Conference on Theory of Cryptography
 Does learning require memorization? a short tale about a long tail,2020, In Proceedingsof the 52nd Annual ACM Symposium on the Theory of Computing
 Submix: Practical privateprediction for large-scale language models,2022, arXiv preprint arXiv:2201
 Long short-term memory,1997, Neural Computation
 Learning and evaluatinga differentially private pre-trained language model,2021, In Proceedings of the Third Workshop onPrivacy in Natural Language Processing
 Lora: Low-rank adaptation of large language models,2021, arXiv preprint arXiv:2106
 Differential privacy based on importance weighting,2013, MachineLearning
 Differentially private language models benefit frompublic pre-training,2020, arXiv preprint arXiv:2009
 Computing tight differential privacy guaranteesusing fft,2560, In International Conference on Artificial Intelligence and Statistics
 Tight differential privacy fordiscrete-valued mechanisms and for the subsampled gaussian mechanism using fft,2021, In Interna-tional Conference on Artificial Intelligence and Statistics
 The power of scale for parameter-efficient prompttuning,2021, In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-cessing (EMNLP)
 Measuring the intrinsic dimen-sion of objective landscapes,2018, In Proceedings of the 6th International Conference on LearningRepresentations
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 Large language models canbe strong differentially private learners,2022, In Proceedings of the 10th International Conference onLearning Representations
 Leveragingpublic data for practical private query release,2021, In Proceedings of the 38th International Conferenceon Machine Learning
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Decoupled weight decay regularization,2019, In Proceedings of the7th International Conference on Learning Representations
 Scalable differential privacy with sparsenetwork finetuning,2021, In Proceedings of the 2021 IEEE Computer Society Conference on ComputerVision and Pattern Recognition
 Compacter: Efficient low-rankhypercomplex adapter layers,2021, arXiv preprint arXiv:2106
 Learning differentially privaterecurrent language models,2018, In Proceedings of the 6th International Conference on LearningRepresentations
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Privately answering classification queries in the agnostic PACmodel,2020, In Proceedings of the 31st International Conference on Algorithmic Learning Theory
 The E2E dataset: New challenges forend-to-end generation,2017, In Proceedings of the 18th Annual SIGdial Meeting on Discourse andDialogue
 Semi-supervised knowledge transfer for deep learning from private training data,2017, In Proceedings ofthe 5th International Conference on Learning Representations
 Scalable private learning with PATE,2018, In Proceedings of the 6th International Conferenceon Learning Representations
 Adapter-fusion: Non-destructive task composition for transfer learning,2021, In Proceedings of the 16th Con-ference of the European Chapter of the Association for Computational Linguistics: Main Volume
 Training production language models without memorizing user data,2020, arXivpreprint arXiv:2009
 Adapterdrop: On the efficiency of adapters in transformers,2020, arXiv preprintarXiv:2010
 One size does not fit all: Investigatingstrategies for differentially-private learning across nlp tasks,2021, arXiv preprint arXiv:2112
 Membership inference at-tacks against machine learning models,2017, In Proceedings of the 38th IEEE Symposium on Securityand Privacy
 Stochastic gradient descent with dif-ferentially private updates,2013, In Proceedings of the 2013 IEEE Global Conference on Signal andInformation Processing
 Enabling fast differentially private sgdvia just-in-time compilation and vectorization,2021, In Advances in Neural Information ProcessingSystems 34
 Attention is all you need,2017, In Advances in Neural Infor-mation Processing Systems 30
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Do not let privacy overbill utility: Gradientembedding perturbation for private learning,2021, In Proceedings of the 9th International Conferenceon Learning Representations
 Large scale private learning vialow-rank reparametrization,2021, In Proceedings of the 38th International Conference on MachineLearning
 Bypassing the ambient dimension: Pri-vate SGD with gradient subspace identification,2021, In Proceedings of the 9th International Confer-ence on Learning Representations
