title,year,conference
 TensorFlow: A system for large-scalemachine learning,2016, In 12th USENIX symposium on operating systems design and implementation(OSD116)
 Near-optimal sample complexity bounds for robust learning of Gaussian mixtures viacompression schemes,2020, Journal of the ACM (JACM)
 Fast learning rates for plug-in classifiers,2007, TheAnnals of Statistics
 The capacity of feedforward neural networks,2019, Neural networks
 Spectrally-normalized margin bounds forneural networks,2017, In Proceedings of the 31st International Conference on Neural InformationProcessing Systems
 Nearly-tight VC-dimensionand pseudodimension bounds for piecewise linear neural networks,2019, Journal of Machine LearningResearch
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 To understand deep learning we need to under-stand kernel learning,2018, In International Conference on Machine Learning
 A mean field theory of quantized deep net-works: The quantization-depth trade-off,2019, In Advances in Neural Information Processing Systems
 Partition of space,1943, The American Mathematical Monthly
 Deep learning with low precision byhalf-wave Gaussian quantization,2017, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Finite-sample analysis of interpolating linear classifiers inthe overparameterized regime,2021, Journal of Machine Learning Research
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, In Conference on Learning Theory
 Geometrical and statistical properties of systems of linear inequalities with ap-plications in pattern recognition,1965, IEEE transactions on electronic computers
 Regularized bi-nary network training,2019, In Fifth Workshop on Energy Efficient Machine Learning and CognitiveComputing - NeurIPS Edition (EMC2-NIPS)
 Any discrimination rule can have an arbitrarily bad probability of error for finitesample size,1982, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Strong universal consistency of neural network classifiers,1993, IEEETransactions on Information Theory
 Larq: An open-source library for training binarized neural net-works,2020, Journal of Open Source Software
 Deep learning,2016, MIT press
 Implicit bias of gradient descenton linear convolutional networks,2018, In Advances in Neural Information Processing Systems
 A distribution-free theory ofnonparametric regression,2006, Springer Science & Business Media
 Stable sample compression schemes: New applications andan optimal SVM margin bound,2021, In Algorithmic Learning Theory
 Sample compression for real-valued learners,2019, In Algorithmic Learning Theory
 Quantizedneural networks: Training neural networks with low precision weights and activations,2017, The Jour-nal of Machine Learning Research 
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Fantas-tic generalization measures and where to find them,2019, In International Conference on LearningRepresentations
 Binaryduo: Reducing gradient mis-match in binary activation network by coupling binary activations,2019, In International Conferenceon Learning Representations
 Fast convergence rates of deep neural networks forclassification,2021, Neural Networks
 Self-normalizingneural networks,2017, In Advances in Neural Information Processing Systems
 On the rate of convergence of fully connected deeP neuralnetwork regression estimates,2020, arXiv preprint arXiv:1908
 Analysis of the rate of convergence of fully connected deeP neural network regres-sion estimates with smooth activation function,2021, Journal of Multivariate Analysis
 Trainingquantized nets: A deePer understanding,2017, In Proceedings of the 31st International Conference onNeural Information Processing Systems
 Defensive quantization: When efficiency meets robustness,2018, InInternational Conference on Learning Representations
 Relating data comPression and learnability,1986, 1986
 Neural nets with suPerlinear VC-dimension,1994, Neural Computation
 WRPN: Wide reduced-Precisionnetworks,2018, In International Conference on Learning Representations
 SamPle comPression schemes for VC classes,2016, Journal of theACM (JACM)
 Lower bounds over Boolean inPuts for deeP neural networkswith ReLU gates,2017, arXiv preprint arXiv:1711
 Uniform convergence may be unable to exPlain generaliza-tion in deeP learning,2019, In Advances in Neural Information Processing Systems
 ExPloring gener-alization in deeP learning,2017, In Proceedings of the 31st International Conference on Neural Infor-mation Processing Systems
 Binary neuralnetworks: A survey,2020, Pattern Recognition
 An exponentialimprovement on the memorization capacity of deep threshold networks,2021, In Advances in NeuralInformation Processing Systems
 XOR-Net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Explaining AdaBoost,2013, In Empirical inference
 Nonparametric regression using deep neural networks with ReLU acti-vation function,2020, Annals of Statistics
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Neural kernels without tangents,2020, In International Conference onMachine Learning
 On the uniform convergence of relative frequencies of eventsto their probabilities,1971, Measures of Complexity
 Memory capacity of neural networks with threshold and rectified linear unitactivations,2020, SIAM Journal on Mathematics of Data Science
 Improved expressivity through dendritic neuralnetworks,2018, In Proceedings of the 32nd International Conference on Neural Information ProcessingSystems
 Explaining the success ofAdaBoost and random forests as interpolating classifiers,2017, The Journal of Machine Learning Re-search
 Under-standing straight-through estimator in training activation quantized neural nets,2019, In InternationalConference on Learning Representations
