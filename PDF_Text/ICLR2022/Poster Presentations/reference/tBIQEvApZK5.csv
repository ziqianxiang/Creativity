title,year,conference
 Why bigger is not always better: on finite and infinite neural networks,2020, InInternational Conference on Machine Learning
 Implicit regularization via neural feature alignment,2021, In Interna-tional Conference on Artificial Intelligence and Statistics
 Random forests,2001, Machine learning
 Online knowledge distillationwith diverse peers,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
 Cross-layer distillation with semantic calibration,2021, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Label-aware neural tangent kernel: Toward better gen-eralization and local elasticity,2020, Advances in Neural Information Processing Systems
 Under-specification presents challenges for credibility in modern machine learning,2020, arXiv preprintarXiv:2011
 Repulsive deep ensembles are bayesian,2021, arXiv preprintarXiv:2106
 Deconstructing the regularization of batchnorm,2021, In Interna-tional Conference on Learning Representations
 Ensemble methods in machine learning,2000, In International workshop on multi-ple classifier systems
 Deep ensembles: A loss landscape per-spective,2019, arXiv preprint arXiv:1912
 Deep learning versus kernel learning: an empirical study of loss landscapegeometry and the time evolution of the neural tangent kernel,2020, Advances in Neural InformationProcessing Systems
 Distilling knowledge from ensembles of acousticmodels for joint ctc-attention end-to-end speech recognition,2020, volume abs/2005
 Neural network ensembles,1990, IEEE transactions on patternanalysis and machine intelligence
 Bayesian deep ensembles via the neu-ral tangent kernel,2020, In H
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Neural tangent kernel: convergence and gen-eralization in neural networks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Learning multiple layers of features from tiny images,2009, Technical report
 Simple and scalable predic-tive uncertainty estimation using deep ensembles,2017, Advances in Neural Information ProcessingSystems
 Deep Neural Networks as Gaussian Processes,2018, In International Conferenceon Learning Representations
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, Advances in neural information processing systems
 Finite versus infinite neural networks: an empirical study,2020, Advancesin Neural Information Processing Systems
 Fastadaptation with linearized neural networks,2021, In International Conference on Artificial Intelligenceand Statistics
 Sample-then-optimize posterior sampling for bayesian linear models,2017, 2017
 Bayesian deep convolutional networks with manychannels are gaussian processes,2018, In International Conference on Learning Representations
 Benchmarking the neural linear model for regres-sion,2019, arXiv preprint arXiv:1912
 The promises and pitfalls of deepkernel learning,2021, arXiv preprint arXiv:2102
 Can you trust your modelâ€™s uncertainty? evaluatingpredictive uncertainty under dataset shift,2019, Advances in Neural Information Processing Systems
 Relational knowledge distillation,2019, In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Uncertainty in neural networks: Approxi-mately bayesian ensembling,2020, In International conference on artificial intelligence and statistics
 Efficient kernel transfer in knowledge distillation,2020, arXiv preprintarXiv:2009
 Random features for large-scale kernel machines,2007, In Proceedingsof the 20th International Conference on Neural Information Processing Systems
 SpeechBrain: A general-purposespeech toolkit,2021, 2021
 Deep bayesian bandits showdown: Anempirical comparison of bayesian deep networks for thompson sampling,2018, arXiv preprintarXiv:1802
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Multilingual neural machine translation withknowledge distillation,2019, In International Conference on Learning Representations
 Contrastive representation distillation,2020, In Interna-tional Conference on Learning Representations
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE/CVF International Conference on Computer Vision
 On feature collapseand deep kernel learning for single forward pass uncertainty,2021, arXiv preprint arXiv:2102
 The lost honor of '2-based regularization,2013, InLarge scale inverse problems
 Hyperparameter ensembles forrobustness and uncertainty quantification,2020, arXiv preprint arXiv:2006
 Bayesian deep learning and a probabilistic perspectiveof generalization,2020, arXiv preprint arXiv:2002
 Kernel and rich regimes in overparametrized models,2020, InConference on Learning Theory
 Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architec-ture are Gaussian Processes,2019, arXiv preprint arXiv:1910
 Feature learning in infinite-width neural networks,2020, arXiv preprintarXiv:2011
 Tensor programs iib: Architectural universality of neural tangent kerneltraining dynamics,2021, arXiv preprint arXiv:2105
 Condi-tional temporal neural processes with covariance loss,2021, In Marina Meila and Tong Zhang (eds
 Paying more attention to attention: Improving the perfor-mance of convolUtional neUral networks via attention transfer,2016, arXiv preprint arXiv:1612
 NeU-ral ensemble search for Uncertainty estimation and dataset shift,2020, arXiv preprint arXiv:2006
 Be yoUrown teacher: Improve the performance of convolUtional neUral networks via self distillation,2019, InProceedings of the IEEE/CVF International Conference on Computer Vision
