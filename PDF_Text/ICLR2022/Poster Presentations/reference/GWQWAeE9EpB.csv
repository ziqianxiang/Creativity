title,year,conference
 Adaptive input representations for neural language modeling,2019, InInternational Conference on Learning Representations
 Losing heads in the lottery: Pruning transformer atten-tion in neural machine translation,2020, In Proceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP)
 Language models are few-shot learners,2020, CoRR
 Adaptively sparse transformers,2019, arXivpreprint arXiv:1909
 CRYPTOGRU: Low latency privacy-preservingtext analysis with GRU,2052, In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing
 Fast decoding in sequence models using discrete latent variables,2390, In Jennifer Dy andAndreas Krause (eds
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In International Conference on Ma-chine Learning
 Albert: A lite bert for self-supervised learning of language representations,2020, In InternationalConference on Learning Representations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 She: A fast and accurate deep neural network for encrypted data,2019, InAdvances in Neural Information Processing Systems
 Glyph: Fast and accurately training deep neuralnetworks on encrypted data,2019, CoRR
 Autoprivacy: Automated layer-wise parameter selection forsecure neural network inference,8638, In H
 Autoq: Automated kernel-wise neuralnetwork quantization,2020, In International Conference on Learning Representations
 In H,2019, Wallach
 Pointer sentinel mixturemodels,2016, CoRR
 Regularizing and optimizing lstm lan-guage models,2017, arXiv preprint arXiv:1708
 Fully quantized transformer for machinetranslation,2020, In Findings of the Association for Computational Linguistics: EMNLP 2020
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Fixed encoder self-attention patterns intransformer-based machine translation,2020, CoRR
 Subformer: Exploring Weight Sharingfor Parameter Efficiency in Generative Transformers,2021, In Findings of the Association for Computa-tional Linguistics: EMNLP 2021
 The evolved transformer,2019, CoRR
 Lessons on parameter sharing across layers in transformers,2021, CoRR
 Stochastic gradient descent trainingfor L1-regularized log-linear models with cumulative penalty,2009, In Proceedings of the Joint Confer-ence ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP
 Attention is all you need,2017, In I
 Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers,2020, arXiv preprintarXiv:2002
 Lite transformer with long-shortrange attention,2020, In International Conference on Learning Representations
 Tied transformers: Neural machinetranslation with shared encoder and decoder,5466, In The Thirty-Third AAAI Conference on Artifi-cial Intelligence
 Hard-coded gaussian attention for neural machinetranslation,2020, arXiv preprint arXiv:2005
 Automatic mixed-precisionquantization search of bert,2021, Proceedings of the Thirtieth International Joint Conference on Artifi-cial Intelligence
