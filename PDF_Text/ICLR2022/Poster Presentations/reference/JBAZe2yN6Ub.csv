title,year,conference
 Rewarding behaviors,1160, In Proceedings of theThirteenth National Conference on Artificial Intelligence - Volume 2
 Successor features for transfer in reinforcement learning,2017, InI
 Successor features for transfer in reinforcement learning,2017, InI
 Transfer in deep reinforcement learning using successor featuresand generalised policy improvement,2018, In Jennifer Dy and Andreas Krause (eds
 Fast reinforcementlearning with generalized policy updates,0027, Proceedings of the National Academy of Sciences
 What is a cognitive map? organizing knowl-edge for flexible behavior,2018, Neuron
 Dynamic Programming,1957, Dover Publications
 Addressing function approximation error inactor-critic methods,2018, In Proceedings of the 35th International Conference on Machine Learning
 Reinforcement learning with non-markovian rewards,2020, ProceedingsoftheAAAI Conference OnArtfiCialIntelligence
 Fast task inference with variational intrinsic successor features,2020, In International Confer-ence on Learning Representations
 Dynamical distance learn-ing for semi-supervised and unsupervised skill discovery,2020, In International Conference on LearningRepresentations
 Learning to achieve goals,1094, In Proceedings of IJCAI-93
 A natural policy gradient,2002, In Advances in neural information processing systems
 Urlb: Unsupervised reinforcement learning benchmark,2021, In NeurIPS2021 Datasets and Benchmarks Track
 Behavioral decisions made under the risk of predation: A review andprospectus,1990, Canadian Journal of Zoology-revue Canadienne De Zoologie - CAN J ZOOL
 Aps: Active pretraining with successor features,6736, In Marina Meilaand Tong Zhang (eds
 Count-based exploration with thesuccessor representation,2020, Proceedings of the AAAI Conference on Artificial Intelligence
 Behaviour suite for reinforcement learning,2020, In InternationalConference on Learning Representations
 Temporal difference models: Model-free deep RL for model-based control,2018, In International Conference on Learning Representations
 Markov decision processes: discrete stochastic dynamic programming,1994, JohnWiley & Sons
 Markov decision processes: discrete stochastic dynamic programming,2010, JohnWiley and Sons
 Constraint satisfaction propagation: Non-stationary policysynthesis for temporal logic planning,2019, CoRR
 On-line q-learning using connectionist systems,1994, TechnicalReport CUED/F-INFENG/TR 166
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Mice learn multi-step routes by memorizing subgoal locations,2021, bioRxiv
 Dynamics-awareunsupervised discovery of skills,2020, In International Conference on Learning Representations
 The hippocampus as apredictive map,2017, Nature Neuroscience
 An analysis of model-based interval estimationfor markov decision processes,2005, Journal of Computer and System Sciences
 Reinforcement Learning: An Introduction,2018, The MIT Press
 Between mdps and semi-mdps: Aframework for temporal abstraction in reinforcement learning,1999, Artificial Intelligence
 A neurally plausible model learns successor representations inpartially observable environments,2019, In H
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 The simplex and policy-iteration methods are strongly polynomial for the markov decisionproblem with a fixed discount rate,2011, Mathematics of Operations Research
 Planning in hierarchicalreinforcement learning: Guarantees for using local policies,2019, CoRR
 Discovering a set of policies for the worst case reward,2021, In InternationalConference on Learning Representations
