title,year,conference
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 The relaxation method of finding the common point of convex sets and its applica-tion to the solution of problems in convex programming,1967, USSR computational mathematics andmathematical physics
 Fast global convergence ofnatural policy gradient methods with entropy regularization,2020, arXiv preprint arXiv:2007
 Proximal minimization algorithm withd-functions,1992, Journalof Optimization Theory and Applications
 On the global convergence of momentum-based policygradient,2021, arXiv preprint arXiv:2110
 Spider: Near-optimal non-convex op-timization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 A theory of regularized markov decisionprocesses,2019, In Thirty-sixth International Conference on Machine Learning
 Mini-batch stochastic approximation meth-ods for nonconvex stochastic composite optimization,2016, Mathematical Programming
 Momentum-based policy gradient meth-ods,2020, In International Conference on Machine Learning
 Super-adam: Faster and universal framework of adaptivegradients,2021, Advances in Neural Information Processing Systems
 Efficiently solving mdps with stochastic mirror descent,2020, In Interna-tional Conference on Machine Learning
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In NIPS
 A natural policy gradient,2001, Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Actor-critic algorithms,2000, In Advances in neural informationprocessing systems
 Deep reinforcement learning: An overview,2017, arXiv preprint arXiv:1701
 A unified view of entropy-regularized markovdecision processes,2017, arXiv preprint arXiv:1705
 Ahybrid stochastic policy gradient algorithm for reinforcement learning,2020, In International Confer-ence on Artificial Intelligence and Statistics
 Trust regionpolicy optimization,2015, In International conference on machine learning
 High-dimensional continuous control using generalized advantage estimation,2016, In International Con-ference on Learning Representations (ICLR)
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Adaptive trust region policy optimization: Globalconvergence and faster rates for regularized mdps,2020, In Proceedings of the AAAI Conference onArtificial Intelligence
 A mathematical theory of communication,1948, The Bell system technical journal
 Hessian aided policygradient,2019, In International Conference on Machine Learning
 Mastering the game of gowithout human knowledge,2017, nature
 Policy gradientmethods for reinforcement learning with function approximation,2000, In Advances in neural informa-tion processing systems
 Mirror descent policyoptimization,2020, arXiv preprint arXiv:2005
 Hybrid stochastic gradi-ent descent algorithms for stochastic nonconvex optimization,2019, arXiv preprint arXiv:1905
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 An improved convergence analysis of stochastic variance-reduced policy gradient,2019, In Proceedings of the Thirty-Fifth Conference on Uncertainty in ArtificialIntelligence
 Sample efficient policy gradient methods with recursivevariance reduction,2019, arXiv preprint arXiv:1909
 Policyoptimization with stochastic mirror descent,2019, arXiv preprint arXiv:1906
 Policymirror descent for regularized reinforcement learning: A generalized framework with linear con-vergence,2021, arXiv preprint arXiv:2105
 On the con-vergence and sample efficiency of variance-reduced policy gradient method,2021, arXiv preprintarXiv:2102
 Sample efficient reinforce-ment learning with reinforce,2020, arXiv preprint arXiv:2010
 On the convergence rate of stochastic mirror descent for nonsmoothnonconvex optimization,2018, arXiv preprint arXiv:1806
