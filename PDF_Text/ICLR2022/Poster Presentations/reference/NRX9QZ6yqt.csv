title,year,conference
 Improved svrg for non-strongly-convex or sum-of-non-convexobjectives,2016, In International conference on machine learning
 Learning to learn by gradient descent bygradient descent,2016, In NeurIPS
 On the convergence of nesterov’s accelerated gradient methodin stochastic settings,2020, arXiv preprint arXiv:2002
 Correcting momentum in temporal differencelearning,2020, Deep Learning Workshop
 On-line learning and stochastic approximations,1999, On-Line Learning in Neural Networks
 Optimization methods for large-scale machinelearning,2018, Siam Review
 A largeannotated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing
 Convex optimization: Algorithms and complexity,2015, Foundations and Trends inMachine Learning
 Multiwoz-a large-scale multi-domain wizard-of-oz dataset fortask-oriented dialogue modelling,2018, In EMNLP
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In NeurIPS
 Adaptive subgradient methods for online learning andstochastic optimization,2011, JMLR
 Optimal mini-batch and step sizes forSAGA,2019, In ICML
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Neural turing machines,2014, arXiv
 Dynamic neural turingmachine with continuous and discrete addressing schemes,2018, Neural computation
 Long short-term memory,1997, Neural Computation
 Dissipativity theory for nesterov’s accelerated method,2017, In InternationalConference on Machine Learning
 On variancereduction in stochastic gradient descent and its asynchronous variants,2015, In NeurIPS
 Accelerating stochastic gradient descent using Predictive variancereduction,2013, In NeurIPS
 Adam: A method for stochastic oPtimization,2015, In Yoshua Bengioand Yann LeCun
 Learning multiPle layers of features from tiny images,2009, InCiteseer
 Object recognition with gradient-based learning,1999, In Shape
 Analysis and design of oPtimizationalgorithms via integral quadratic constraints,2016, SIAM Journal on Optimization
 Rcv1: A new benchmark collection fortext categorization research,2004, JMLR
 Learning to oPtimize,2017, In ICLR
 Roberta: A robustly oPtimized bert PretrainingaPProach,2019, arXiv
 Building a large annotatedcorPus of english: The Penn treebank,1993, Computational Linguistics
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Global convergence of online limited memory bfgs,2015, Journalof Machine Learning Research
 Introductory lectures on convex oPtimization: a basic course,2004, Kluwer AcademicPublishers
 SARAH: A novel method for machinelearning Problems using stochastic recursive gradient,2017, In ICML
 UPdating quasi-newton matrices with limited storage,1980, Mathematics of Computation
 DeeP learning on a data diet: FindingimPortant examPles early in training,2021, NeurIPS
 Some methods of sPeeding uP the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 A stochastic approximation method,1951, The Annals of MathematicalStatistics
 A stochastic gradient method with an exponentialconvergence _rate for finite training sets,2012, In NeurIPS
 Learning internal representations byerror propagation,1985, Technical report
 A stochastic quasi-newton method for onlineconvex optimization,2007, In AISTATS
 Stochastic dual coordinate ascent methods for regularized lossminimization,2013, JMLR
 On the importance of initializationand momentum in deep learning,2013, In ICML
 A neural conversational model,2015, arXiv
 Adadelta: an adaptive learning rate method,2012, arXiv
 Stochastic nested variance reduction for nonconvexoptimization,2018, arXiv preprint arXiv:1806
