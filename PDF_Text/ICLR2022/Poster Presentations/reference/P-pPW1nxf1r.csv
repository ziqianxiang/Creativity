title,year,conference
 Better fine-tuning by reducing representational collapse,2020, arXiv preprint arXiv:2008
 Intrinsic dimensionality explains theeffectiveness of language model fine-tuning,2020, arXiv preprint arXiv:2012
 Etc: Encoding long and structuredinputs in transformers,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Comparing automatic and human evaluation of nlg systems,2006, In 11thconference of the european chapter of the association for computational linguistics
 The fifth pascal recognizingtextual entailment challenge,2009, In TAC
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Electra: Pre-training textencoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 UnsuPervisedcross-lingual representation learning at scale,2019, arXiv preprint arXiv:1911
 The CommitmentBank:Investigating projection in naturally occurring discourse,2019, 2019
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Improving zero and few-shot abstractive summarization with interme-diate fine-tuning and data augmentation,2021, In NAACL
 The webnlgchallenge: Generating text from rdf data,2017, In Proceedings of the 10th International Conference onNatural Language Generation
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Teaching machines to read and comprehend,2015, In Advances in neuralinformation processing Systems
 zip: Compressing text classification models,2016, arXiv preprint arXiv:1612
 Abstractive summarization of reddit postsWith multi-level memory netWorks,2018, arXiv preprint arXiv:1811
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Meteor: An automatic metric for mt evaluation With high levels ofcorrelation With human judgments,2007, In Proceedings of the second workshop on statistical machinetranslation
 The Winograd schema challenge,2011, In AAAISpring Symposium: Logical Formalizations of Commonsense Reasoning
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Rouge: A package for automatic evaluation of summaries,2004, In Text summarizationbranches out
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Dart: Open-domain structured datarecord to text generation,2020, arXiv preprint arXiv:2007
 Annotated gigaWord,2012, InProceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scaleKnowledge Extraction (AKBC-WEKEX)
 The e2e dataset: New challenges forend-to-end generation,2017, arXiv preprint arXiv:1706
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 WiC: The word-in-context dataset forevaluating context-sensitive meaning representations,2019, In Proceedings of NAACL-HLT
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 It's not just size that matters: Small language models are alsofew-shot learners,2020, arXiv preprint arXiv:2009
 Improving andsimplifying pattern exploiting training,2021, ArXiv
 Cider: Consensus-based imagedescription evaluation,2015, In Proceedings of the IEEE conference on computer vision and patternrecognition
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-gies
 Pegasus: Pre-training with extractedgap-sentences for abstractive summarization,2019, arXiv preprint arXiv:1912
