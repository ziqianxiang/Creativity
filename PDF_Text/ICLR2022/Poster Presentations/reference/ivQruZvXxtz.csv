title,year,conference
 Massively multilingual neuralmachine translation in the wild: Findings and challenges,2019, arXiv preprint arXiv:1907
 Multilingual alignment of contextual word representa-tions,2019, In International Conference on Learning Representations
 Recall andlearn: Fine-tuning deep pretrained language models with less forgetting,2020, In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Gradnorm: Gradientnormalization for adaptive loss balancing in deep multitask networks,2018, In International Conferenceon Machine Learning
 Tydi qa: A benchmark for information-seeking question answering intypologically diverse languages,2020, Transactions of the Association for Computational Linguistics
 Cross-lingual language model pretraining,2019, Advances inNeural Information Processing Systems
 Xnli: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 UnsUPer-vised cross-lingual representation learning at scale,2020, In Proceedings of the 58th Annual Meetingofthe Associationfor Computational Linguistics
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for langUage Understanding,2019, In NAACL-HLT (1)
 Explicit alignmentobjectives for multilingual bidirectional encoders,2021, In Proceedings of the 2021 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Learning with whom to share in multi-task featurelearning,2011, In ICML
 Novel dataset for fine-grained image categorization: Stanford dogs,2011, In Proc
 Learning multiple layers of features from tiny images,2009, arXiv preprint
 Tiny imagenet visual recognition challenge,2015, arXiv preprint
 Asymmetric multi-task learning based on task relat-edness and loss,2016, In International conference on machine learning
 Learning to perturb word embeddingsfor out-of-distribution QA,2021, In Proceedings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th International Joint Conference on Natural LanguageProcessing
 Pre-training via paraphrasing,2020, Advances in Neural Information Processing Systems
 Mlqa: Evaluatingcross-lingual extractive question answering,2020, In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics
 Multilingual denoising pre-training for neural machine translation,2020, Trans-actions of the Association for Computational Linguistics
 Decoupled weight decay regularization,2019, In International Confer-ence on Learning Representations
 Fine-grainedvisual classification of aircraft,2013, arXiv preprint arXiv:1306
 Readingdigits in natural images with unsupervised feature learning,2011, arXiv preprint
 On first-order meta-learning algorithms,2018, arXivpreprint arXiv:1803
 Zero-shotcross-lingual transfer with meta learning,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Multilingualbert post-pretraining alignment,2021, In Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Cross-lingual name tagging and linking for 282 languages,2017, In Proceedings of the 55th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers)
 Conditionally adaptive multi-task learn-ing: Improving transfer learning in NLP using fewer parameters & less data,2021, In InternationalConference on Learning Representations
 Learning to learn without forgetting by maximizing transfer and minimizing interfer-ence,2019, In International Conference on Learning Representations
 Multi-task learning as multi-objective optimization,2018, In Proceedingsof the 32nd International Conference on Neural Information Processing Systems
 Large-scale meta-learning withcontinual trajectory shifting,2021, In Proceedings of the 38th International Conference on MachineLearning
 On the origin of implicit regular-ization in stochastic gradient descent,2021, In International Conference on Learning Representations
 Balancing training for multilingual neural ma-chine translation,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Characterizing and avoidingnegative transfer,2019, In IEEE Conference on Computer Vision and Pattern Recognition
 On negative interference in mUltilingUal lan-gUage models,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Gradient vaccine: Investigating and im-proving mUlti-task optimization in massively mUltilingUal models,2021, In International Conferenceon Learning Representations
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Transformers: State-of-the-art natural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 A convex formulation for learning task relationships in multi-tasklearning,2010, In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence
