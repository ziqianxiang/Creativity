title,year,conference
 Better fine-tuning by reducing representational collapse,2021, In International Confer-ence on Learning Representations
 Meteor: An automatic metric for mt evaluation with improvedcorrelation with human judgments,2005, In Proceedings of the acl workshop on intrinsic and extrinsicevaluation measuresfor machine translation and/or summarization
 Scheduled sampling for sequenceprediction with recurrent neural networks,2015, Advances in neural information processing systems
 Adabert: Task-adaptive bert compression with differentiableneural architecture search,2020, In Christian Bessiere (ed
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 BERT: Pre-training of deepbidirectional transformers for language understanding,4171, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Understanding back-translation atscale,2018, In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-cessing
 A survey of data augmentation approaches for nlp,2021, Findings of ACL
 Soft contextual data augmentation for neural machine translation,2019, In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics
 Knowledge distillation: Asurvey,2021, International Journal of Computer Vision
 Fully non-autoregressive neural machine translation: Tricks of thetrade,2021, In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021
 Non-autoregressiveneural machine translation,2018, In International Conference on Learning Representations
 Achieving human parityon automatic chinese to english news translation,2018, CoRR
 Revisiting self-training for neuralsequence generation,2019, In International Conference on Learning Representations
 Teaching machines to read and comprehend,2015, Advances in neuralinformation processing systems
 Iterative back-translation for neural machine translation,2018, In Proceedings of the 2nd Workshop on Neural Ma-chine Translation and Generation
 mixup: Beyond em-pirical risk minimization,2018, International Conference on Learning Representations
 Sequence-level knowledge distillation,2016, Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Contextual augmentation: Data augmentation by words with paradigmatic rela-tions,2018, Proceedings of the 2018 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Deterministic non-autoregressive neural se-quence modeling by iterative refinement,2018, Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing
 DailyDialog: A man-ually labelled multi-turn dialogue dataset,2017, In Proceedings of the Eighth International JointConference on Natural Language Processing (Volume 1: Long Papers)
 Rouge: A package for automatic evaluation of summaries,2004, In Text summarizationbranches out
 Scheduled sampling for transformers,2019, In Pro-ceedings of the 57th Annual Meeting of the Association for Computational Linguistics: StudentResearch Workshop
 ParlAI: A dialog research software platform,2017, In Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing: System Demonstrations
 SSMBA: Self-supervised manifold based dataaugmentation for improving out-of-domain robustness,2020, In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 Style trans-fer through back-translation,2018, Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 A study of non-autoregressive model for sequence generation,2020, Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics
 Improving neural machine translation modelswith monolingual data,2016, Proceedings of the 54th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers)
 Edinburgh neural machine translation systemsfor wmt 16,2016, Proceedings of the First Conference on Machine Translation: Volume 2
 Edinburgh neural machine translation systemsfor WMT 16,2016, In Proceedings of the First Conference on Machine Translation: Volume 2
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Relevance of unsupervised met-rics in task-oriented dialogue for evaluating natural language generation,2017, CoRR
 Minimumrisk training for neural machine translation,2016, In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers)
 The evolved transformer,5877, In Kamalika Chaudhuri and RuslanSalakhutdinov (eds
 Learning to summarize with human feedback,2020, Advancesin Neural Information Processing Systems
 Synthesizer: Re-thinking self-attention for transformer models,2021, In International Conference on Machine Learning
 Attention is all you need,2017, In I
 Cider: Consensus-based imagedescription evaluation,2015, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Improving neural language modeling via adversar-ial training,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 SwitchOut: an efficient data aug-mentation algorithm for neural machine translation,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 Timeseries data augmentation for deep learning: A survey,4653, In Zhi-Hua Zhou (ed
 Pay less attention withlightweight and dynamic convolutions,2019, In International Conference on Learning Representations
 A study of reinforcement learning forneural machine translation,2018, Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Sequence generation with mixed representations,2020, In ICML 2020
 In I,2017, Guyon
 Pegasus: Pre-training with extractedgap-sentences for abstractive summarization,2020, In International Conference on Machine Learning
 Bridging the gap between train-ing and inference for neural machine translation,4334, In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics
