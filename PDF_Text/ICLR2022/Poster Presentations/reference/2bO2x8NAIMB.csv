title,year,conference
 Muppet: Massive multi-task representations with pre-finetuning,2021, arXiv preprintarXiv:2101
 On Losses for Modern Language Models,2020, InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP)
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Learning many related tasks at the same time with backpropagation,1995, In Advances inneural information processing systems
 Multitask learning,1997, Machine learning
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 Semi-supervised sequence learning,2015, arXiv preprintarXiv:1511
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 The hitchhiker¡¯s guide to testingstatistical significance in natural language processing,2018, In Proceedings of the 56th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers)
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Don¡¯t stop pretraining: Adapt language models to domains and tasks,2020, arXivpreprint arXiv:2004
 Lessons from archives: Strategies for collecting sociocultural datain machine learning,2020, In Proceedings of the 2020 Conference on Fairness
 Measuring theevolution of a scientific field through citation frames,2018, Transactions of the Association for Com-putational Linguistics
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Skip-thought vectors,2015, In C
 Biobert: a pre-trained biomedical language representation model for biomedical textmining,2020, Bioinformatics
 Towards understand-ing and mitigating social biases in language models,2021, In International Conference on MachineLearning
 Reviving and improving recurrent back-propagation,2018, In InternationalConference on Machine Learning
 Adaptive auxiliary taskweighting for reinforcement learning,2019, Advances in neural information processing systems
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 S2orc: The semanticscholar open research corpus,2019, arXiv preprint arXiv:1911
 The natural languagedecathlon: Multitask learning as question answering,2018, arXiv preprint arXiv:1806
 Meta-learning up-date rules for unsupervised representation learning,2018, arXiv preprint arXiv:1804
 Distributed representa-tions of words and phrases and their compositionality,2013, In C
 Auxiliary learning byimplicit differentiation,2020, arXiv preprint arXiv:2007
 On first-order meta-learning algorithms,2018, arXivpreprint arXiv:1803
 Deep contextualized word representations,2018, In Proceedings of the 2018 Con-ference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies
 Semi-supervisedsequence tagging with bidirectional language models,2017, arXiv preprint arXiv:1705
 Meta-learning with implicitgradients,2019, arXiv preprint arXiv:1909
 An overview of multi-task learning in deep neural networks,2017, arXiv preprintarXiv:1706
 Self-diagnosis and self-debiasing: A proposalfor reducing corpus-based bias in nlp,2021, arXiv preprint arXiv:2103
 On learning how to learn learning strategies,1995,1995
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Lifelong learning algorithms,1998, In Learning to learn
 Word representations: A simple and generalmethod for semi-supervised learning,2010, In Proceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics
 Can you tell me how to get past sesamestreet? sentence-level pretraining beyond language modeling,2018, arXiv preprint arXiv:1812
 Optimizing data usage via differentiable rewards,2020, In International Conference on Ma-chine Learning
 Characterizing and avoiding neg-ative transfer,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Nlp from scratch without large-scale pretraining: A simple and efficient framework,2021, arXiv preprint arXiv:2111
 A survey on multi-task learning,2021, IEEE Transactions on Knowledge andData Engineering
