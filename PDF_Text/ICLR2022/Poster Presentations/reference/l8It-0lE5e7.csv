title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In arXiv:1802
 Obfuscated gradients give a false sense of security: Circum-venting defenses to adversarial examples,2018, In International Conference on Machine learning(ICML)
 Theory iii:Dynamics and generalization in deep networks,2019, In arXiv:1903
 Generalized gradients and applications,1975, 1975
 Optimization and nonsmooth analysis,1983, 1983
 A unified architecture for natural language processing: Deep neuralnetworks with multitask learning,2008, In Proceedings of the 25th international conference on Machinelearning(ICML)
 Stochastic subgradientmethod converges on tame functions,2018, In arXiv:1804
 Algorithmic regularization in learning deep homogeneous models:Layers are automatically balanced,2018, In arXiv: 1806
 Approximate kkt points and a proximity measure fortermination,2013, Journal ofGlobal Optimization
 Bridging the gap between adversarial robustness and optimization bias,2021, In arXiv:2102
 Convergence of adversarial training inoverparametrized neural networks,2019, In arXiv:1906
 Explaining and harnessing adversarial examples,2015, InInternational Conference on Learning Representations (ICLR)
 Implicit bias of gradient descenton linear convolutional networks,2018, In arXiv: 1806
 Countering adversarial images using inputtransformations,2018, In International Conference on Learning Representations(ICLR)
 Delving deep into rectifiers: Surpassing human-level per-formance on imagenet classification,2015, In International conference on computer vision (ICCV)
 Risk and parameter convergence of logistic regression,2018, In arXiv:1803
 Gradient descent aligns the layers of deep linear networks,2019, In InternationalConference on Learning Representations(ICLR)
 Directional convergence and alignment in deep learning,2020, In arXiv:2006
 Imagenent classification with deep convolutional neuralnetworks,2012, In Advances in Neural Information Processing Systems(NeurIPS)
 Inductive bias of gradient descent based adversarial training onseparable data,2020, In International Conference on Machine Learning(ICLR)
 Gradient descent maximizes the margin of homogeneous neural networks,2020, InInternational Conference on Learning Representations(ICLR)
 Towards deep learning models resistantto adversarial attacks,2018, In International Conference on Learning Representations(ICLR)
 Adversarialtraining methods for semi-supervised textclassification,2016, In arXiv:1605
 Lexicographic and depth-sensitive mar-gins in homogeneous and non-homogeneous deep models,2019, In Proceedings of the 36th InternationalConference on Machine Learning
 Stochastic gradient descent on separable data: Exactconvergence with a fixed learning rate,2019, In Proceedings of Machine Learning Research
 Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images,2015, In Conference on computer vision and pattern recognition(CVPR)
 Gradient methods never overfit on separable data,2021, In Journal of Machine LearningResearch
 The implicit bias of gradientdescent on separable data,2018, Journal ofMachine Learning Research
 Intriguingproperties of neural networks,2014, In International Conference on Learning Representations (ICLR)
 Regularization matters: Generalization and optimization of neuralnets v,2019,s
 Fast is better than free: Revisiting adversarial training,2020, InInternational Conference on Learning Representations (ICLR)
 Understanding gen-eralization in adversarial training via the bias-variance decomposition,2021, In arXiv: 2103
 Theoretically principled trade-offbetween robustness and accuracy,2019, In International Conference on Machine Learning(ICLR)
 Over-parameterized adversarialtraining: An analysis overcoming the curse of dimensionality,2020, In arXiv: 2002
