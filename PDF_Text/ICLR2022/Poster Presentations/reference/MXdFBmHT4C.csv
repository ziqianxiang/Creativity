title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In International Conference on Machine Learning
 Bloem-Reddy and Y,2019, W
 Biological sequence modeling with convolutionalkernel networks,2019, Bioinformatics
 Recurrent kernel networks,2019, In Advances in NeuralInformation Processing Systems
 Maximum likelihood from incomplete data via the emalgorithm,1977, Journal of the Royal Statistical Society
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In In Proceedings of the North AmericanChapter of the Association for Computational Linguistics (NAACL)
 Towards a neural statistician,2017, In International Conference onLearning Representations
 Conditional neural processes,2018, In International Conference onMachine Learning
 Differentiable Deep Clustering withCluster Size Constraints,2019, arXiv preprint arXiv:1910
 DeepSF: Deep convolutional neural network for mappingprotein sequences to folds,2019, Bioinformatics
 Attention-based deep multiple instance learning,2018, InInternational Conference on Machine Learning
 Reformer: The Efficient Transformer,2020, InInternational Conference on Learning Representations
 Human-level concept learning through proba-bilistic program induction,2015, Science
 Set trans-former: A framework for attention-based permutation-invariant neural networks,2019, In Proceedingsof the 36th International Conference on Machine Learning
 Deep Amortized Clustering,2019, arXiv preprintarXiv:1909
 A similarity measure between unordered vector sets with applicationto image categorization,2008, In IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)
 Neural ClusteringProcesses,2020, In International Conference on Machine Learning
 Improving the fisher kernel for large-scale image classifica-tion,2010, In European Conference on Computer Vision
 Neural episodic control,2017, In International Conference on Machine Learning
 Learning with Kernels,2002, MIT Press
 Transformer dissection: A unified understanding of transformer’s attention via the lens ofkernel,2019, In Empirical Methods in Natural Language Processing (EMNLP)
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Optimal Transport: Old and New,2008, Springer
 Order matters: Sequence to sequence for sets,2016, In InternationalConference on Learning Representations
 On the limitations of representingfunctions on sets,2019, arXiv preprint arXiv:1901
 Linformer: Self-Attentionwith Linear Complexity,2020, arXiv preprint arXiv:2006
 Using the Nystrom method to speed up kernelmachines,2001, In Advances in Neural Information Processing Systems
 Huggingface’stransformers: State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
 Attentional aggregation of deep feature sets formulti-view 3D reconstruction,2018, arXiv preprint arXiv:1808
 Predicting effects of noncoding variants with deep learning-based sequence model,2015, Nature methods
