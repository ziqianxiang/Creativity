title,year,conference
 Neural spectrahedra and semidefinite lifts: Global convex op-timization of polynomial activation neural networks in fully polynomial-time,2021, arXiv preprintarXiv:2101
 Training quantized neural networks to global optimality via semidefi-nite programming,2021, International Conference on Machine Learning (ICML)
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, Advances in Neural Information Processing Systems
 Geometrical and statistical properties of systems of linear inequalities withapplications in pattern recognition,1965, IEEE transactions on electronic computers
 Implicit convex regularizers of cnn architectures: Convex optimizationof two-and three-layer networks in polynomial time,2020, International Conference on LearningRepresentations (ICLR)
 Global optimality beyond two layers: Training deep relu networksvia convex programs,2021, In International Conference on Machine Learning
 Revealing the structure of deep neural networks via convex duality,2021, InInternational Conference on Machine Learning
 Demys-tifying batch normalization in relu networks: Equivalent convex optimization models and implicitregularization,2021, arXiv preprint arXiv:2103
 Characterizing implicit bias interms of optimization geometry,1832, In International Conference on Machine Learning
 Exact and relaxed convex formulationsfor shallow neural autoregressive models,2021, In International Conference on Acoustics
 Learning over-parametrized two-layer neuralnetworks beyond ntk,2020, In Conference on Learning Theory
 Gradient descent quantizes relu networkfeatures,2018, arXiv preprint arXiv:1803
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
 The inductive bias of relu networks on orthogonallyseparable data,2021, 2021
 Neural networks are convex regularizers: Exact polynomial-timeconvex optimization formulations for two-layer networks,2020, pp
 Hidden convexity of wasserstein gans: Interpretable generative models with closed-formsolutions,2021, arXiv preprint arXiv:2107
 Convex regularizationbehind neural reconstruction,2021, International Conference on Learning Representations (ICLR)
 The implicitbias of gradient descent on separable data,2018, The Journal of Machine Learning Research
 Learning two-layer relu networks is nearlyas easy as learning linear classifiers on separable data,2021, IEEE Transactions on Signal Processing
 A local convergence theory for mildly over-parameterized two-layerneural network,2021, arXiv preprint arXiv:2102
