title,year,conference
 Understanding double descent requires a fine-grained bias-variance decomposition,2020, arXiv preprint arXiv:2011
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Banerjee and Guido Montufar,2021, Information complexity and generalization bounds
 Spectrally-normalized margin bounds for neuralnetworks,2017, arXiv preprint arXiv:1706
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Stability of stochastic gradientdescent on nonsmooth convex losses,2020, arXiv preprint arXiv:2006
 Stability and generalization,2002, The Journal OfMachine LearningResearch
 Sharper bounds for uniformly stablealgorithms,2020, In Conference on Learning Theory
 Statistical inference,2021, Cengage Learning
 Rank-sparsityincoherence for matrix decomposition,2011, SIAM Journal on Optimization
 Nonparametric stochastic approximation with large step-sizes,2016, Annals of Statistics
 Entropy-sgd optimizes the prior of a pac-bayes bound:Generalization properties of entropy-sgd and data-dependent priors,2018, In International Conferenceon Machine Learning
 Generalization bounds for uniformly stable algorithms,2018, arXivpreprint arXiv:1812
 High probability generalization bounds for uniformly stablealgorithms with nearly optimal rate,2019, In Conference on Learning Theory
 Matrix completion has no spurious local minimum,2016, arXivpreprint arXiv:1605
 No spurious local minima in nonconvex low rank problems: Aunified geometric analysis,1233, In International Conference on Machine Learning
 The implicit bias of depth: How incrementallearning drives generalization,2019, arXiv preprint arXiv:1909
 Shape matters: Understanding the implicitbias of the noise covariance,2021, In Conference on Learning Theory
 Simple and accurate uncertaintyquantification from bias-variance decomposition,2020, arXiv preprint arXiv:2002
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Generalization in deep learning,2017, arXivpreprint arXiv:1710
 Rademacher penalties and structural risk minimization,2001, IEEE Transactionson Information Theory
 Rademacher processes and bounding the risk offunction learning,2000, In High dimensional probability II
 Local rademacher complexities and oracle inequalities in risk minimiza-tion,2006, Annals of Statistics
 Theory of point estimation,2006, Springer Science & BusinessMedia
 Fine-grained analysis of stability and generalization for stochasticgradient descent,2020, In International Conference on Machine Learning
 On generalization error bounds of noisy gradient methodsfor non-convex learning,2019, arXiv preprint arXiv:1902
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Conference On Learning Theory
 Foundations of machine learning,2018, MITpress
 Generalization bounds of sgld for non-convexlearning: TWo theoretical viewpoints,2018, In Conference on Learning Theory
 Uniform convergence may be unable to explain generalizationin deep learning,2019, arXiv preprint arXiv:1902
 The deep bootstrap: Good online learnersare good offline generalizers,2020, arXiv e-prints
 In defense of uniform convergence:Generalization via derandomization with an application to interpolating predictors,2020, In InternationalConference on Machine Learning
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Exploring general-ization in deep learning,2017, arXiv preprint arXiv:1706
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Generalization guaran-tees for neural networks via harnessing the low-rank structure of the jacobian,2019, arXiv preprintarXiv:1906
 Guaranteed minimum-rank solutions of linearmatrix equations via nuclear norm minimization,2010, SIAM review
 Controlling bias in adaptive data analysis using information theory,2016, InArtificial Intelligence and Statistics
 Optimistic rates for learning with a smoothloss,2010, arXiv preprint arXiv:1009
 Reasoning about generalization via conditional mutualinformation,2020, In Conference on Learning Theory
 Improved sample complexities for deep networks and robust classificationvia an all-layer margin,2019, arXiv preprint arXiv:1910
 Information-theoretic analysis of generalization capability of learningalgorithms,2017, arXiv preprint arXiv:1705
 Fast-rate pac-bayes generalization bounds via shiftedrademacher processes,2019, In NeurIPS
 Understanding generaliza-tion in adversarial training via the bias-variance decomposition,2021, arXiv preprint arXiv:2103
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Sharp restricted isometry bounds for theinexistence of spurious local minima in nonconvex matrix recovery,2019, Journal of Machine LearningResearch
 Non-vacuousgeneralization bounds at the imagenet scale: a pac-bayesian compression approach,2018, arXiv preprintarXiv:1804
 Generalization error bounds with probabilisticguarantee for sgd in nonconvex optimization,2018, arXiv preprint arXiv:1802
 On the computational andstatistical complexity of over-parameterized matrix sensing,2021, arXiv preprint arXiv:2102
 Benignoverfitting of constant-stepsize sgd for linear regression,2021, arXiv preprint arXiv:2103
