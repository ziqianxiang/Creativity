title,year,conference
 A generalizable approach tolearning optimizers,2021, arXiv preprint arXiv:2106
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information processing Systems
 Deep unfolding for communications systems:A survey and some new directions,2019, In 2019 IEEE International Workshop on Signal ProcessingSystems (SiPS)
 Neural optimizer search with reinforce-ment learning,2017, arXiv preprint arXiv:1709
 Amp-inspired deep networks for sparselinear inverse problems,2017, IEEE Transactions on Signal Processing
 Learning to optimize in swarms,2019, InAdvances in Neural Information Processing Systems
 Training stronger baselines for learning to optimize,2020, Advances in Neural InformationProcessing Systems
 Learning to optimize: A primer and a benchmark,2021, arXiv preprint arXiv:2103
 Theoretical linear convergence ofunfolded ista and its practical weights and thresholds,2018, Advances in Neural Information ProcessingSystems
 Hyperparameter tuning is all you needfor lista,2021, Advances in Neural Information Processing Systems
 Rna secondary structure prediction bylearning unrolled algorithms,2019, In International Conference on Learning Representations
 Learned imagedeblurring by unfolding a proximal interior point algorithm,2019, In 2019 IEEE International Conferenceon Image Processing (ICIP)
 Discovering symbolic models from deep learning with inductive biases,2020, NeurIPS2020
 Hyperparameter optimization,2019, In Automated Machine Learning
 Tradeoffs between con-vergence speed and reconstruction accuracy in inverse problems,1941, IEEE Transactions on SignalProcessing
 Knowledge distillation: Asurvey,2021, International Journal of Computer Vision
 Learning fast approximations of sparse coding,2010, In Proceedings ofthe 27th international conference on international conference on machine learning
 On improving genetic programmingfor symbolic regression,2005, In 2005 IEEE Congress on Evolutionary Computation
 Multilayer feedforward networks areuniversal approximators,1989, Neural networks
 Meta-learning inneural networks: A survey,2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Learning combinatorialoptimization algorithms over graphs,2017, Advances in neural information processing systems
 Halo: Hardware-awarelearning to optimize,2020, In Proceedings of the European Conference on Computer Vision (ECCV)
 Learning to optimize,2017, International Conference on Learning Representa-tions (ICLR)
 Learning to optimize neural nets,2017, arXiv preprint arXiv:1703
 Learning to combine quasi-newton methods,2020, NeruIPS workshopon on Optimization for Machine Learning
 Deep magnetic resonance image reconstruction:Inverse problems meet neural networks,2020, IEEE Signal Processing Magazine
 ALISTA: Analytic weights are as goodas learned weights in LISTA,2019, In International Conference on Learning Representations
 Learning gradient descent: Better generalization and longerhorizons,2017, arXiv preprint arXiv:1703
 Re-verse engineering learned optimizers reveals known and novel mechanisms,2020, arXiv preprintarXiv:2011
 Understanding the learned iterative soft thresholding algorithmwith matrix factorization,2017, In International Conference on Learning Representations (ICLR)
 Numerical optimization,2006, Springer Science & Business Media
 The evolution of a generalized neural learning rule,2016, In 2016 InternationalJoint Conference on Neural Networks (IJCNN)
 Pipps: Flexible model-basedpolicy search robust to the curse of chaos,2019, arXiv preprint arXiv:1902
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Automl-zero: Evolving machine learningalgorithms from scratch,8007, In International Conference on Machine Learning
 Evolution and design of distributed learningrules,2000, In 2000 IEEE Symposium on Combinations of Evolutionary Computation and NeuralNetworks
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Learning a minimax optimizer: A pilot study,2021, In International Conference on LearningRepresentations
 Glad: Learning sparse graph recovery,2020, In International Conference on LearningRepresentations
 Optimization for deep learning: theory and algorithms,2019, arXiv preprint arXiv:1912
 Theoretical interpretation of learned step size in deep-unfolded gradient descent,2020, arXiv:2001
 Unbiasing truncated backpropagation through time,2017, arXiv preprintarXiv:1705
 A perspective view and survey of meta-learning,2002, Artificialintelligence review
 Learned optimizers that scale and generalize,2017, InInternational Conference on Machine Learning
 Understanding short-horizon bias instochastic meta-optimization,2018, arXiv preprint arXiv:1803
 End-to-endsequential sampling and reconstruction for mr imaging,2021, arXiv preprint arXiv:2105
 L2-gcn: Layer-wise and learnedefficient training of graph convolutional networks,2020, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 Ista-net: Interpretable optimization-inspired deep network forimage compressive sensing,2018, In Proceedings of the IEEE conference on computer vision and patternrecognition
