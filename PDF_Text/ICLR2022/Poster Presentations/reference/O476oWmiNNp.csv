title,year,conference
 Language models arefew-shot learners,2020, Advances in Neural Information Processing Systems (NeurIPS)
 A note on over-smoothing for graph neural networks,2020, In InternationalConference on Machine Learning Workshop (ICMLW)
 End-to-end object detection with transformers,2020, In European Conference on ComputerVision (ECCV)
 Pre-trained image processing transformer,2021, In IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Chasing sparsity invision transformers: An end-to-end exploration,2021, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv:1810
 Attention is not all you need: Pureattention loses rank doubly exponentially with depth,2021, In International Conference on MachineLearning (ICML)
 An imageis worth 16x16 words: Transformers for image recognition at scale,2020, In International Conferenceon Learning Representations (ICLR)
 Vision transformers withpatch diversification,2021, arXiv: 2104
 Deep residual learning for imagerecognition,2016, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Approximation capabilities of multilayer feedforward networks,1991, Neural Networks
 Densely connectedconvolutional networks,2017, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Alltokens matter: Token labeling for training better vision transformers,2021, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Highly accurateprotein structure prediction with alphafold,2021, Nature
 The lipschitz constant of self-attention,2021, InInternational Conference on Machine Learning (ICML)
 Imagenet classification with deep con-volUtional neUral networks,2012, In Advances in Neural Information Processing Systems (NeurIPS)
 Deep learning,2015, Nature
 Swin transformer: Hierarchical vision transformer Using shifted windows,2021, In InternationalConference on Computer Vision (ICCV)
 The expressive power ofneUral networks: A view from the width,2017, In Advances in Neural Information Processing Systems(NeurIPS)
 Graph neUral networks exponentially lose expressive power for nodeclassification,2019, In International Conference on Learning Representations (ICLR)
 Image transformer,2018, In International Conference on Machine Learning (ICML)
 EqUivalence of approximation by convolUtional neUralnetworks and fUlly-connected networks,2020, Proceedings of the American Mathematical Society
 ImageNetLarge Scale VisUal Recognition Challenge,2015, International Journal of Computer Vision (IJCV)
 Rethinking transformer-based setprediction for object detection,2020, In IEEE International Conference on Computer Vision (ICCV)
 Augmentedshortcuts for vision transformers,2021, In Advances in Neural Information Processing Systems (NeurIPS)
 Benefits of depth in neural networks,2016, In Conference on Learning Theory
 Training data-efficient image transformers & distillation through attention,2021, In InternationalConference on Machine Learning(ICML)
 Goingdeeper with image transformers,2021, In International Conference on Computer Vision (ICCV)
 Attention is all you need,2017, In Advances in neural informationprocessing systems (NeurIPS)
 Graph attention networks,2018, In International Conference on Learning Representations(ICLR)
 Feastnet: Feature-steered graph convolutions for3d shape analysis,2018, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 End-to-end video instance segmentation with transformers,2021, In IEEE Conference on ComputerVision and Pattern Recognition (CVPR)
 Pytorch image models,2019, https://github
 Simpli-fying graph convolutional networks,2019, In International Conference on Machine Learning (ICML)
 Cvt:Introducing convolutions to vision transformers,2021, In IEEE International Conference on ComputerVision (ICCV)
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, In International Conference on Computer Vision (ICCV)
 On orthogonality constraints for transformers,2021, In Annual Meeting ofthe Association for Computational Linguistics (ACL)
 Improving deep transformer with depth-scaled initializa-tion and merged attention,2019, In Conference on Empirical Methods in Natural Language Processing(EMNLP)
 Featurepyramid transformer,2020, In European Conference on Computer Vision (ECCV)
 End-to-end objectdetection with adaptive clustering transformer,2021, In British Machine Vision Conference (BMVC)
 Delayedpropagation transformer: A universal computation engine towards practical control in cyber-physical systems,2021, In Advances in Neural Information Processing Systems (NeurIPS)
 Deepvit: Towards deeper vision transformer,2021, arXiv:2103
 Refiner: Refining self-attention for vision transformers,2021, arXiv:2106
 Universality of deep convolutional neural networks,2020, Applied and computationalharmonic analysis
 End-to-end densevideo captioning with masked transformer,2018, In IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Deformable detr:Deformable transformers for end-to-end object detection,2021, In International Conference on LearningRepresentations (ICLR)
