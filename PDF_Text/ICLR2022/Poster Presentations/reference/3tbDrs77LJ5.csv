title,year,conference
 A fast iterative shrinkage-thresholding algorithm for linear inverseproblems,2009, SIAM journal on imaging sciences
 Dropping convexity for fastersemi-definite optimization,2016, In Conference on Learning Theory
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Convex optimization,2004, Cambridgeuniversity press
 Unifying nuclearnorm and bilinear factorization approaches for low-rank matrix decomposition,2013, In Proceedings ofthe IEEE International Conference on Computer Vision
 Phase retrieval via wirtinger flow:Theory and algorithms,2015, IEEE Transactions on Information Theory
 Fast low-rank estimation by projected gradient descent:General statistical and algorithmic guarantees,2015, arXiv preprint arXiv:1509
 On landscape of la-grangian functions and stochastic search for constrained nonconvex optimization,2018, arXiv preprintarXiv:1806
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, In S
 Geometric measure theory,2014, Springer
 No spurious local minima in nonconvex low rank problems: Aunified geometric analysis,1233, In International Conference on Machine Learning
 Understanding alternating minimization for matrix completion,2014, In 2014 IEEE 55thAnnual Symposium on Foundations ofComputer Science
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Matrix completion from a fewentries,2010, IEEE transactions on information theory
 Tensor decompositions and applications,2009, SIAM review
 Stochasticity of deterministic gradient descent: Large learning rate formultiscale objective function,2020, In H
 Onoptimization methods for deep learning,2011, In ICML
 The largelearning rate phase of deep learning: the catapult mechanism,2020, 2020
 The non-convex geometry of low-rank matrix optimiza-tion,2019, Information and Inference: A Journal of the IMA
 Implicit bias of gradient descent based adversarialtraining on separable data,2020, In International Conference on Learning Representations
 Implicit regularization of bregman proximal pointalgorithm and mirror descent on separable data,2021, arXiv preprint arXiv:2108
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 An exponential learning rate schedule for deep learning,2019, arXivpreprint arXiv:1910
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Noisy gradient descent converges to flatminima for nonconvex matrix factorization,2021, 2021
 Beyond procrustes: Balancing-free gradient descent forasymmetric low-rank matrix sensing,2021, IEEE Transactions on Signal Processing
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Some methods of speeding up the convergence of iteration methods,1964, Ussr computa-tional mathematics and mathematical physics
 Introduction to optimization,1987, optimization software
 Towards flatter losssurface via nonmonotonic learning rate scheduling,2018, In UAI
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE winter conferenceon applications of computer vision (WACV)
 Super-convergence: Very fast training of neural networks usinglarge learning rates,2019, In Artificial Intelligence and Machine Learning for Multi-Domain OperationsApplications
 The implicitbias of gradient descent on separable data,2018, The Journal of Machine Learning Research
 A comparison of optimization algorithms for deep learning,2020, International Journalof Pattern Recognition and Artificial Intelligence
 A geometric analysis of phase retrieval,2018, Foundations ofComputational Mathematics
 Low-ranksolutions of linear matrix equations via procrustes flow,2016, In International Conference on MachineLearning
 Global convergence of gradient descent for asymmetric low-rank matrixfactorization,2021, arXiv preprint arXiv:2106
 Salr: Sharpness-aware learning rates for improvedgeneralization,2020, arXiv preprint arXiv:2011
 Toward understanding theimportance of noise in training neural networks,2019, In International Conference on Machine Learning
 Towards theoretically under-standing why sgd generalizes better than adam in deep learning,2020, arXiv preprint arXiv:2010
