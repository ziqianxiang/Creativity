title,year,conference
 ProxQuant: Quantized neural networks viaproximal operators,2019, In International Conference on Learning Representations
 NeWton acceleration on manifoldsidentified by proximal-gradient methods,2020, Technical report
 Penalized methods for bi-level variable selection,2009, Statisticsand its interface
 19Ashok Cutkosky and Francesco Orabona,2019, Momentum-based variance reduction in non-convex SGD
 On the ineffectiveness of variance reduced optimizationfor deep learning,2019, In Advances in Neural Information Processing Systems
 SAGA: A fast incremental gradientmethod With support for non-strongly convex composite objectives,2014, In Advances in neuralinformation processing systems
 Structured sparsity inducing adaptive optimizers for deeplearning,2021, Technical report
 Asymptotic optimality in stochastic optimization,2021, TheAnnals of Statistics
 Rigging thelottery: Making all tickets Winners,2020, In International Conference on Machine Learning
 A note on the group lasso and asparse group lasso,2010, Technical report
 Stochastic methods for solving nonsmooth extremal problems,1979, Naukova Dumka
 Identifying active manifolds in regularization problems,2011, In Fixed-PointAlgorithms for Inverse Problems in Science and Engineering
 Identifying active constraints via partial smoothnessand prox-regularity,2004, Journal of Convex Analysis
 Identifying active manifolds,2007, Algorithmic OperationsResearch
 Deep residual learning for imagerecognition,2016, In IEEE conference on computer vision and pattern recognition
 Loss-aware binarization of deep networks,2017, InInternational Conference on Learning Representations
 Denselyconnected convolutional networks,2017, In IEEE conference on computer vision and patternrecognition
 20Samy Jelassi and Aaron Defazio,2020, Dual averaging is surprisingly effective for deep learningoptimization
 Accelerating stochastic gradient descent using predictivevariance reduction,2013, Advances in neural information processing systems
 Adam: A method for stochastic optimization,2015, InInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 Regularized quasi-monotone method forstochastic optimization,2021, Technical report
 Gradient-based learningapplied to document recognition,1998, Proceedings of the IEEE
 Accelerating inexact successive quadratic approximation for regularizedoptimization through manifold identification,2020, Technical report
 Manifold identification in dual averaging for regu-larized stochastic online learning,2012, Journal of Machine Learning Research
 Manifold identification for ultimatelycommunication-efficient distributed optimization,2020, In International Conference on Ma-chine Learning
 Activity identification and local linear con-vergence of forward-backward-type methods,2017, 27(1):408-437
 Local convergence properties of douglas-rachford and alternating direction method of multipliers,2017, Journal of Optimization Theoryand Applications
 Inexact SARAH algorithm forstochastic optimization,2021, Optimization Methods and Software
 The quasigradient method for the solving of the nonlinearprogramming problems,1973, Cybernetics
 Adaptive restart for accelerated gradientschemes,2015, Foundations of computational mathematics
 ProxSARAH: Anefficient algorithmic framework for stochastic composite nonconvex optimization,2020, Journalof Machine Learning Research
 Prox-regular functions in variational analysis,1996, Transac-tions of the American Mathematical Society
 Very deep convolutional networks for large-scaleimage recognition,2015, 2015
 Are we there yet? manifold iden-tification of gradient-related proximal methods,2019, In International Conference on ArtificialIntel ligence and Statistics
 Primal averaging: A new gradient evalu-ation step to attain the optimal individual convergence,2018, IEEE transactions on cybernetics
 Hybrid stochasticgradient descent algorithms for stochastic nonconvex optimization,2019, Technical report
 Sparsifying networks via subdifferential inclu-sion,2021, In International Conference on Machine Learning
 Spiderboost and momen-tum: Faster variance reduction algorithms,2019, In Advances in Neural Information ProcessingSystems
 Learning intrinsic sparse structures within long short-termmemory,2018, In International Conference on Learning Representations
 Fashion-mnist: a novel image dataset forbenchmarking machine learning algorithms,2017, Technical report
 Dual averaging methods for regularized stochastic learning and online optimiza-tion,2010, Journal of Machine Learning Research
 A proximal stochastic gradient method with progressive variancereduction,2014, SIAM Journal on Optimization
 ProxSGD: Training structured neural networks under regularizationand constraints,2019, In International Conference on Learning Representations
 Adaptive proximal gradient methods forstructured neural networks,2021, Advances in Neural Information Processing Systems
 Nearly unbiased variable selection under minimax concave penalty,2006, TheAnnals of statistics
 Less is more: Towards compact CNNs,2016, InEuropean Conference on Computer Vision
