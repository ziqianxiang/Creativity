title,year,conference
 Binarybert: Pushing the limit of bert quantization,2020, arXiv preprint arXiv:2012
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 The design of competitive online algorithms via a primal-dualapproach,2009, Now Publishers Inc
 Challenges and advances in parallel sparse matrix-matrix mul-tiplication,2008, In 2008 37th International Conference on Parallel Processing
 End-to-end object detection with transformers,2020, In European Conference on ComputerVision
 Pre-trained image processing transformer,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 The lottery ticket hypothesis for pre-trained bert networks,2020, Advances in neuralinformation processing systems
 Chasing sparsity invision transformers: An end-to-end exploration,2021, arXiv preprint arXiv:2106
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Up-detr: Unsupervised pre-training forobject detection with transformers,2021, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Attention is not all you need: Pureattention loses rank doubly exponentially with depth,2021, arXiv preprint arXiv:2103
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Compressing large-scale transformer-based models:A case study on bert,2020, arXiv preprint arXiv:2002
 Compressing bert: Studying the effects ofweight pruning on transfer learning,2020, arXiv preprint arXiv:2002
 Power-bert: Accelerating bert inference via progressive word-vectorelimination,2020, In International Conference on Machine Learning
 Reweighted proximal pruningfor large-scale language representation,2019, arXiv preprint arXiv:1909
 Gdp: Stabilized neuralnetwork pruning via gates with differentiable polarization,2021, In Proceedings of the IEEE/CVFInternational Conference on Computer Vision
 Learning both weights and connections forefficient neural network,2015, In Advances in neural information processing systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Filter pruning via geometric medianfor deep convolutional neural networks acceleration,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Dynabert: Dynamic bertwith adaptive width and depth,2020, arXiv preprint arXiv:2004
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 Efficientvision transformers via fine-grained manifold distillation,2021, arXiv preprint arXiv:2107
 Comparingkullback-leibler divergence and mean squared error loss in knowledge distillation,2021, arXiv preprintarXiv:2105
 Imagenet classification with deep con-VolUtional neural networks,2012, Advances in neural information processing Systems
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Hrank: Filter pruning using high-rank feature map,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Accel-erating convolutional networks via global & dynamic filter pruning,2018, In IJCAI
 Learningefficient convolutional networks through network slimming,2017, In ICCV
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Apprentice: Using knowledge distillation techniques to improvelow-precision network accuracy,2018, In International Conference on Learning Representations
 Scalable vision transformers withhierarchical pooling,2021, In Proceedings of the IEEE/CVF International Conference on ComputerVision
 Image transformer,4055, In International Conference on Machine Learning
 Fine-tuned transformers show clusters of similarrepresentations across layers,2021, arXiv preprint arXiv:2109
 Umec:Unified model and embedding compression for efficient recommendation systems,2021, In InternationalConference on Learning Representations
 Scop:Scientific control for reliable neural network pruning,2020, arXiv preprint arXiv:2010
 Patchslimming for efficient vision transformers,2021, arXiv preprint arXiv:2106
 Efficient dc algorithm for constrained sparseoptimization,2017, arXiv preprint arXiv:1701
 Training data-effiCient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Max-deeplab: End-to-end panoptic segmentation with mask transformers,2021, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 End-to-end video instance segmentation with transformers,2021, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Circumventing outliersof autoaugment with knowledge distillation,2020, In Computer VSion-ECCV 2020: 16th EuropeanConference
 Bert-of-theseus: Compressingbert by progressive module replacing,2020, arXiv preprint arXiv:2002
 Learning texture transformernetwork for image super-resolution,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 On benefits of selection diversityvia bilevel exclusive sparsity,2016, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Automatic neural network compression bysparsity-quantization joint learning: A constrained optimization-based approach,2020, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Nisp: Pruning networks using neuron importance scorepropagation,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Revisiting knowledge distillation vialabel smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Accelerating training of transformer-based language models withprogressive layer dropping,2020, arXiv preprint arXiv:2010
 Ternarybert:Distillation-aware ultra-low bit bert,2020, arXiv preprint arXiv:2009
 Point transformer,2020, arXivpreprint arXiv:2012
 End-to-end objectdetection with adaptive clustering transformer,2020, arXiv preprint arXiv:2011
 Visual transformer pruning,2021, arXiv preprintarXiv:2104
 Deformable detr:Deformable transformers for end-to-end object detection,2020, arXiv preprint arXiv:2010
