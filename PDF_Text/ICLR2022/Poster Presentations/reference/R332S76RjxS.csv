title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Trellis networks for sequence modeling,2018, arXivpreprint arXiv:1810
 Deep equilibrium models,2019, arXiv preprintarXiv:1909
 Multiscale deep equilibrium models,2020, arXiv preprintarXiv:2006
 Neural ordinary differ-ential equations,2018, arXiv preprint arXiv:1806
 Recurrent stacking of layers for compact neural machine translationmodels,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 End-to-end differentiable physics for learning and control,2018, Advances in neural information processingsystems
 Differentiable learning of submodular models,2017, Advances inNeural Information Processing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Implicit deeplearning,2019, arXiv preprint arXiv:1908
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Ffjord:Free-form continuous dynamics for scalable reversible generative models,2019, International Confer-ence on Learning Representations
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 Linear convergence of gradient and proximal-gradient methods under the Polyak-IojasieWicz condition,2016, In Joint European Conference on Ma-chine Learning and Knowledge Discovery in Databases
 Learning overparameterized neural netWorks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Reviving and improving recurrent back-propagation,2018, In InternationalConference on Machine Learning
 On the proof of global convergence of gradient descent for deep relu netWorks Withlinear Widths,2021, arXiv preprint arXiv:2101
 Global convergence of deep netWorks With one Wide layerfolloWed by pyramidal topology,2020, arXiv preprint arXiv:2002
 ToWard moderate overparameterization: Global con-vergence guarantees for training shalloW neural netWorks,2020, IEEE Journal on Selected Areas inInformation Theory
 Scalable differentiable physics forlearning and control,2020, arXiv preprint arXiv:2007
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural netWorks,2013, arXiv preprint arXiv:1312
 High-Dimensional Probability: An Introduction with Applications in Data Sci-ence,2018, Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics
 Satnet: Bridging deep learning and log-ical reasoning using a differentiable satisfiability solver,2019, In International Conference on MachineLearning
 An improved analysis of training over-parameterized deep neuralnetWorks,2019, arXiv preprint arXiv:1906
 Gradient descent optimizes over-parameterized deep relu netWorks,2020, Machine Learning
