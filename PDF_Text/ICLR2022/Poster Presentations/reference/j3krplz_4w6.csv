title,year,conference
 Explainable ai for healthcare: from black box to inter-pretable models,2020, In Embedded Systems and Artificial Intelligence
 Yelp dataset challenge: Review rating prediction,2016, arXiv preprint arXiv:1605
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Morphologi-cal and molecular breast cancer profiling through explainable machine learning,2021, Nature MachineIntelligence
 Thermometer encoding: One hotway to resist adversarial examples,2018, In International Conference on Learning Representations
 Towards evaluating the robustness of neural networks,2017, In 2017ieee symposium on security and privacy (sp)
 Universal sentence encoder,2018, arXivpreprint arXiv:1803
 Robust Attribution Regu-larization,2019, In Advances in Neural Information Processing Systems
 Retain: An interpretable predictive model for healthcare using reverse time attentionmechanism,2016, arXiv preprint arXiv:1608
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In International Conference on MachineLearning
 Hotflip: White-box adversarial examplesfor text classification,2017, arXiv preprint arXiv:1712
 On the con-nection between adversarial robustness and saliency map interpretability,2019, arXiv preprintarXiv:1905
 Universal adversarial perturbation for text classification,2019, arXiv preprintarXiv:1910
 Interpreting recurrent and attention-based neuralmodels: a case study on natural language inference,2018, arXiv preprint arXiv:1808
 Interpretation of neural networks is fragile,2019, InProceedings of the AAAI Conference on Artificial Intelligence
 Patient risk assessment and warning symptom detection using deepattention-based neural networks,2018, arXiv preprint arXiv:1809
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Generating counterfactualexplanations with natural language,2018, arXiv preprint arXiv:1806
 Far: A general framework forattributional robustness,2020, arXiv preprint arXiv:2010
 Attention is not explanation,2019, arXiv preprint arXiv:1902
 Is bert really robust? natural languageattack on text classification and entailment,2019, arXiv preprint arXiv:1907
 A new measure of rank correlation,1938, Biometrika
 Speech and language processing daniel jurafsky and james h,2009, martin (stanford univer-sity and university of colorado at boulder) pearson prentice hall
 Captum: Aunified and generic model interpretability library for pytorch,2020, arXiv preprint arXiv:2009
 On guaranteed optimal robust explanations for nlp models,2021, arXiv preprintarXiv:2105
 Bert-attack: Adversarialattack against bert using bert,2020, arXiv preprint arXiv:2004
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Sparsefool: a few pixelsmake a big difference,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Deepfool: a simple andaccurate method to fool deep neural networks,2016, In Proceedings of the IEEE conference on com-puter vision and pattern recognition
 Universaladversarial perturbations,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Counter-fitting word vectors tolinguistic constraints,2016, arXiv preprint arXiv:1603
 Notes on regression and inheritance in the case of two parents,1895, Proceedings of theRoyal Society of London
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Deep inside convolutional networks: Vi-sualising image classification models and saliency maps,2013, arXiv preprint arXiv:1312
 On the benefits of attributional robustness,2019, arXiv preprintarXiv:1911
 Perturbing inputs forfragile interpretations in deep natural language processing,2021, arXiv preprint arXiv:2108
 Axiomatic attribution for deep networks,2017, InProceedings of the 34th International Conference on Machine Learning
 Bertviz: A tool for visualizing multihead self-attention in the bert model,2019, In ICLRWorkshop: Debugging Machine Learning Models
 Attention is not not explanation,2019, arXiv preprintarXiv:1908
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Visualizing and understanding convolutional networks,2014, In Euro-pean Conference on Computer Vision
 Character-level convolutional networks for textclassification,2015, In NIPS
