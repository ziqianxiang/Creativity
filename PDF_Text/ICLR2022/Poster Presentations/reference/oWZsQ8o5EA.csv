title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Chaining meets chain rule: Multilevel entropic regularizationand training of neural networks,2020, Journal of Machine Learning Research
 Spectrally-normalized margin bounds forneural networks,2017, In Proceedings of the 31st International Conference on Neural InformationProcessing Systems
 Learners that uselittle information,2018, In Algorithmic Learning Theory
 Stability of stochastic gradientdescent on nonsmooth convex losses,2020, Advances in Neural Information Processing Systems
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Training with noise is equivalent to tikhonov regularization,1995, Neural computation
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Stability and generalization,2002, The Journal ofMachine Learn-ing Research
 Tightening mutual information-basedbounds on generalization error,2020, IEEE Journal on Selected Areas in Information Theory
 Asymmetric heavy tails and implicit bias in gaussian noise injections,2021, arXiv preprintarXiv:2102
 Stability and convergence trade-off of iterative optimizationalgorithms,2018, arXiv preprint arXiv:1804
 Elements of Information Theory,2012, John Wiley & Sons
 Backpack: Packing more into backprop,2020, In8th International Conference on Learning Representations
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 A study of gradient variance in deeplearning,2020, arXiv preprint arXiv:2007
 High probability generalization bounds for uniformly stable al-gorithms with nearly optimal rate,2019, In Conference on Learning Theory
 Sharpness-aware minimiza-tion for efficiently improving generalization,2020, arXiv preprint arXiv:2010
 Deep sparse rectifier neural networks,2011, In 14thInternational Conference on Artificial Intelligence and Statistics
 Explaining and harnessing adversarialexamples,2015, In 3rd International Conference on Learning Representations
 Mutual information and minimum mean-squareerror in gaussian channels,2005, IEEE transactions on information theory
 Conditioningand processing: Techniques to improve information-theoretic generalization bounds,2020, Advances inNeural Information Processing Systems
 Control batch size and learning rate to generalizewell: Theoretical and empirical evidence,2019, In Advances in Neural Information Processing Systems32
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Flat minima,1997, Neural Computation
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Fantas-tic generalization measures and where to find them,2019, In International Conference on LearningRepresentations
 Accelerating stochastic gradient descent using Predictive variancereduction,2013, Advances in neural information processing systems
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, In 5thInternational Conference on Learning Representations
 Learning multiPle layers of features from tiny images,2009, Technical rePort
 Imagenet classification with deeP con-volutional neural networks,2012, Advances in neural information processing systems
 Fine-grained analysis of stability and generalization for stochasticgradient descent,2020, In International Conference on Machine Learning
 A Pac-bayesian analysis of randomized learning with aPPlication to stochastic gradientdescent,2017, In Proceedings of the 31st International Conference on Neural Information ProcessingSystems
 Regularizing and oPtimizing lstm lan-guage models,2018, In International Conference on Learning Representations
 DeePdouble descent: Where bigger models and more data hurt,2019, In International Conference on Learn-ing Representations
 Read-ing digits in natural images with unsuPervised feature learning,2011, In NeurIPS Workshop on DeepLearning and Unsupervised Feature Learning
 Information-theoretic generalization bounds for stochastic gradient descent,2021, In COLT
 Norm-based caPacity control in neuralnetworks,2015, In Conference on Learning Theory
 ExPloring general-ization in deeP learning,2017, In Advances in Neural Information Processing Systems
 A Pac-bayesian aPProach tosPectrally-normalized margin bounds for neural networks,2018, In International Conference on Learn-ing Representations
 The roleof over-parametrization in generalization of neural networks,2018, In International Conference onLearning Representations
 Deep contextualized word representations,2018, In Proceedings of the 2018 Con-ference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies
 Lecture notes on information theory,2019, Lecture Notes for 6
 On randomsubset generalization error bounds and the stochastic gradient langevin dynamics algorithm,2020, arXivpreprint arXiv:2010
 A stochastic gradient method with an exponen-tial convergence rate for finite training sets,2012, In Proceedings of the 25th International Conferenceon Neural Information Processing Systems-Volume 2
 How much does your data exploration overfit? controlling bias viainformation usage,2019, IEEE Transactions on Information Theory
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Very deep convolutional networks for large-scale imagerecognition,2015, In 3rd International Conference on Learning Representations
 Reasoning about generalization via conditional mutualinformation,2020, In Conference on Learning Theory
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Statistical learning theory,1998, Wiley
 Analyzing the generalization capability ofsgld using properties of gaussian channels,2021, Advances in Neural Information Processing Systems
 Learning while dissipating information:Understanding the generalization capability of sgld,2021, arXiv preprint arXiv:2102
 The implicit and explicit regularization effects ofdropout,2020, In International Conference on Machine Learning
 Anempirical study of stochastic gradient descent with structured covariance noise,2020, In InternationalConference on Artificial Intelligence and Statistics
 How sgd selects the global minima in over-parameterized learning:A dynamical stability perspective,2018, Advances in Neural Information Processing Systems
 Positive-negative momentum: Ma-nipulating stochastic gradient noise to improve generalization,2021, In International Conference onMachine Learning
 Information-theoretic analysis of generalization capability of learn-ing algorithms,2017, Advances in Neural Information Processing Systems
 Pyhessian: Neural networksthrough the lens of the hessian,2020, In 2020 IEEE International Conference on Big Data (Big Data)
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 mixup: Beyond em-pirical risk minimization,2018, In 6th International Conference on Learning Representations
 Why gradient clipping acceleratestraining: A theoretical justification for adaptivity,2019, In International Conference on Learning Rep-resentations
 Regularizing neural networks via adversarialmodel perturbation,2021, In CVPR
 Individually conditional individual mutual information boundon generalization error,2020, arXiv preprint arXiv:2012
