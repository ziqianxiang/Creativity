title,year,conference
 Intrinsic Dimensionality Explains theEffectiveness of Language Model Fine-Tuning,2020, arXiv:2012
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 Feature purification: How adversarial training performs robustdeep learning,2020, arXiv preprint arXiv:2005
 A convergence theory for deep learning via over-parameterization,2019, In ICML
 Language Models are Few-Shot Learners,2020, arXiv:2005
 A singular value thresholding algorithm formatrix completion,2010, SIAM Journal on optimization
 A unified architecture for natural language processing: deepneural netWorks With multitask learning,2008, In Proceedings of the 25th international conferenceon Machine learning
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2019, arXiv:1810
 The Webnlgchallenge: Generating text from rdf data,2017, In Proceedings of the 10th International Conference onNatural Language Generation
 Samsum corpus: A human-annotated dialogue dataset for abstractive summarization,2019, CoRR
 A literature survey of low-rank tensorapproximation techniques,2013, GAMM-Mitteilungen
 Grassmann discriminant analysis: a unifying view on subspace-basedlearning,2008, In ICML
 WARP: Word-level AdversarialReProgramming,2101, arXiv:2101
 Parameter-Efficient Transfer Learningfor NLP,2019, arXiv:1902
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv preprint arXiv:1405
 The Power of Scale for Parameter-Efficient PromptTuning,2021, arXiv:2104
 Measuring the Intrinsic Di-mension of Objective Landscapes,1804, arXiv:1804
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Conference On Learning The-ory
 Exploring versatile generative language modelvia parameter-efficient transfer learning,2020, In Findings of the Association for Computational Lin-guistics: EMNLP 2020
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Dart: Open-domain structureddata record to text generation,2020, arXiv preprint arXiv:2007
 The e2e dataset: New challenges for end-to-end generation,2017, arXiv preprint arXiv:1706
 Generalization guaran-tees for neural networks via harnessing the low-rank structure of the jacobian,2019, arXiv preprintarXiv:1906
 Semi-orthogonal low-rank matrix factorization for deep neural networks,2018, InInterspeech
 Know what you donâ€™t know: Unanswerable questionsfor squad,2018, CoRR
 Learning multiple visual domains withresidual adapters,1705, arXiv:1705
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 Attention is all you need,2017, In Proceedings of the 31st In-ternational Conference on Neural Information Processing Systems
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-gies
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Extracting deep neural network bottleneckfeatures using low-rank matrix factorization,2014, In 2014 IEEE international conference on acoustics
 Seq2sql: Generating structured queries fromnatural language using reinforcement learning,2017, CoRR
