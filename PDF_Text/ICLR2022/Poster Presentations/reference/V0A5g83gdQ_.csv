title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 On the expressive power of deep learning: A tensoranalysis,2016, In Conference on learning theory
 Multi-head attention: Collaborateinstead of concatenate,2020, arXiv preprint arXiv:2006
 The mnist database of handwritten digit images for machine learning research,2012, IEEE SignalProcessing Magazine
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 An imageis worth 16x16 words: Transformers for image recognition at scale,2020, In International Conferenceon Learning Representations
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Expressive power of recurrent neuralnetworks,2018, In International Conference on Learning Representations
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 On the adequacy of untuned warmup for adaptive optimization,2021, InProceedings of the AAAI Conference on Artificial Intelligence
 Building a large annotatedcorpus of english: the penn treebank,1993, Computational Linguistics
 Recurrent neural networkbased language model,2010, In INTERSPEECH
 Streaming automatic speech recognition with thetransformer model,2020, In ICASSP 2020-2020 IEEE International Conference on Acoustics
 Librispeech: an asr corpusbased on public domain audio books,2015, In 2015 IEEE international conference on acoustics
 Image transformer,4055, In International Conference on Machine Learning
 Applications of negative dimensional tensors,1971, Combinatorial mathematics and itsapplications
 Edinburgh neural machine translation systemsfor wmt 16,2016, In Proceedings of the First Conference on Machine Translation: Volume 2
 Transformer dissection: An unified understanding for transformerâ€™s attention via the lens ofkernel,2019, In Proceedings of the Conference on Empirical Methods in Natural Language Processing
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-gies
