title,year,conference
 Machine learning systems are stuck in a rut,2019, In Proceedings of theWorkshop on Hot Topics in Operating Systems
 Amazon ec2 update,2021, https://aws
 Understanding and overcoming thechallenges of efficient transformer quantization,2021, arXiv preprint arXiv:2109
 Scenic: AJAXlibrary for computer vision research and beyond,2021, arXiv preprint arXiv:2110
 The benchmark lottery,2021, arXiv preprint arXiv:2107
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Fast and accurate model scaling,2021, In Proceedings oftheIEEE/CVF Conference on Computer Vision and Pattern Recognition
 An image isworth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprint arXiv:2010
 Rigging the lottery: Mak-ing all tickets winners,2020, In Proceedings of the 37th International Conference on Machine Learning
 Multiscale vision transformers,2021, arXiv preprint arXiv:2104
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Slowfast networks for videorecognition,2019, In Proceedings of the IEEE/CVF international conference on computer vision
 Sharpness-aware minimizationfor efficiently improving generalization,2020, arXiv preprint arXiv:2010
 The state of sparsity in deep neural networks,2019, CoRR
 Sparse GPU kernels for deep learning,2020, InChristine Cuicchi
 Automl: A survey of the state-of-the-art,2021, Knowledge-BasedSystems
 The hardware lottery,2020, arXiv preprint arXiv:2009
 Parameter-efficient transfer learning for nlp,2019, arXivpreprint arXiv:1902
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Monas: Multi-objective neuralarchitecture search using reinforcement learning,2018, arXiv preprint arXiv:1806
 Densely connectedconvolutional networks,2017, In 2017 IEEE Conference on Computer Vision and Pattern Recognition
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv preprint arXiv:1405
 Perceiver io: A generalarchitecture for structured inputs & outputs,2021, arXiv preprint arXiv:2107
 Rethinking floating point for deep learning,2018, arXiv preprint arXiv:1811
 Model-based reinforcement learning for atari,2019, CoRR
 Scaling laws for neural language models,2020, arXivpreprint arXiv:2001
 Scalable and efficient moetraining for multitask multilingual models,2021, arXiv preprint arXiv:2109
 Movinets: Mobile video networks for efficient video recognition,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Imagenet classification with deep convo-lutional neural networks,2012, In Peter L
 Block pruning for fastertransformers,2021, arXiv preprint arXiv:2109
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Fnet: Mixing tokens withfourier transforms,2021, arXiv preprint arXiv:2105
 Aws to offer nvidiaâ€™s t4 gpus for ai inferencing,2021, www
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Compacter: Efficient low-rankhypercomplex adapter layers,2021, arXiv preprint arXiv:2106
 Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks,2021, arXiv preprintarXiv:2106
 Efficient large-scale language model training on gpu clusters,2021, arXiv preprint arXiv:2104
 Challenges in deploying machine learning:a survey of case studies,2020, arXiv preprint arXiv:2011
 Carbon emissions and large neural network training,2021, arXivpreprint arXiv:2104
 MS Windows NT kernel description,2021, https://web
 Efficient neural architecture searchvia parameters sharing,4095, In International Conference on Machine Learning
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Scaling vision with sparse mixture of experts,2021, arXivpreprint arXiv:2106
 The cost of training nlp models: A concise overview,2020, arXivpreprint arXiv:2004
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Very deep convolutional networks for large-scale imagerecognition,2015, In Yoshua Bengio and Yann LeCun
 Primer:Searching for efficient transformers for language modeling,2021, arXiv preprint arXiv:2109
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Going deeper with convolutions,2015, InIEEE Conference on Computer Vision and Pattern Recognition
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Omninet: Omnidirectional representations from transformers,2021, arXivpreprint arXiv:2103
 Scale efficiently: Insights frompre-training and fine-tuning transformers,2021, arXiv preprint arXiv:2109
 Charformer: Fast character transformers via gradient-based subword tokenization,2021, arXiv preprint arXiv:2106
 When to use parametric modelsin reinforcement learning? In Hanna M,2019, Wallach
 Attention is all you need,2017, In Advances in neural information processingsystems
 Scaling local self-attention for parameter efficient visUal backbones,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Pytorch image models,2019, https://github
 Fbnet: Hardware-aware efficient convnet design viadifferentiable neUral architectUre search,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Aggregated residualtransformations for deep neUral networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 Byt5: Towards a token-free future with pre-trained byte-to-byte models,2021, arXivpreprint arXiv:2105
 Accelerating very deep convolutionalnetworks for classification and detection,2015, IEEE transactions on pattern analysis and machineintelligence
 Efficient and accurateapproximations of nonlinear convolutional networks,2015, In Proceedings of the IEEE Conference onComputer Vision and pattern Recognition
