title,year,conference
 Quick and robust feature selection: the strength ofenergy-efficient sparse training for autoencoders,2021, Machine Learning
 Deep rewiring: Trainingvery sparse deep networks,2018, In International Conference on Learning Representations
 Bagging predictors,1996, Machine learning
 Language models are few-shot learners,2020, In H
 Chasing sparsityin vision transformers: An end-to-end exploration,2021, Advances in Neural Information ProcessingSystems
 The lottery tickets hypothesis for supervised and self-supervised pre-training incomputer vision models,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Ensemble methods in machine learning,2000, In Multiple Classifier Systems
 Fast sparse convnets,2020, In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition 
 On random graphs i,1959, Publicationes Mathematicae (Debrecen)
 The difficulty of training sparse neuralnetworks,2019, arXiv preprint arXiv:1906
 Rigging the lottery:Making all tickets winners,2943, In International Conference on Machine Learning
 Gradient flow in sparse neural networksand how lottery tickets win,2020, arXiv preprint arXiv:2010
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, Advances in Neural Information Processing Systems
 Deep ensembles: A loss landscape perspec-tive,2019, arXiv preprint arXiv:1912
 Linear modeconnectivity and the lottery ticket hypothesis,2020, In International Conference on Machine Learning
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,1050, In international conference on machine learning
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Estimation ofenergy consumption in machine learning,2019, Journal of Parallel and Distributed Computing
 Escaping from saddle points¡ªonline stochasticgradient for tensor decomposition,2015, In Conference on learning theory
 Evaluating scalable bayesian deeplearning methods for robust computer vision,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Learning both weights and connections forefficient neural network,2015, In Advances in neural information processing Systems
 Neural network ensembles,1990, IEEE transactions on patternanalysis and machine intelligence
 Training independent subnetworksfor robust prediction,2021, In International Conference on Learning Representations
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem,2019, InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, arXiv preprint arXiv:1903
 Natural adversarialexamples,2019, arXiv preprint arXiv:1907
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Sparsity in deeplearning: Pruning and growth for efficient inference and training in neural networks,2021, Journal ofMachine Learning Research
 Top-kast: Top-kalways sparse training,2020, Advances in Neural Information Processing Systems
 Efficient neural audiosynthesis,2018, In International Conference on Machine Learning
 Deep learning without poor local minima,2016, In D
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, Master¡¯s thesis
 On information and sufficiency,1951, The annals of mathemati-cal statistics
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, Advances in neural information processing systems
 Deep learning,2015, Nature
 Snip: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations
 A signal propagationperspective for pruning neural networks at initialization,2020, In International Conference on LearningRepresentations
 Whym heads are better than one: Training a diverse ensemble of deep networks,2015, arXiv preprintarXiv:1511
 A statistical approach to learning and generalizationin layered neural networks,1990, Proceedings of the IEEE
 Sparse evolutionary deep learning with over one million artificial neurons oncommodity hardware,2020, Neural Computing and Applications
 Sparse training via boostingpruning plasticity with neuroregeneration,2021, Advances in Neural Information Processing Systems
 Do we actually needdense over-parameterization? in-time over-parameterization in sparse training,2021, In Proceedings ofthe 39th International Conference on Machine Learning
 Towards neural networks that provably know when they don¡¯tknow,2019, arXiv preprint arXiv:1909
 A topological insight into restricted boltzmann machines,2016, Machine Learning
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature Communications
 Sparse training theory for scalable andefficient agents,2021, International Conference on Autonomous Agents and Multiagent Systems (AA-MAS)
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In International Conference on Machine Learning
 Obtaining well calibrated prob-abilities using bayesian binning,2015, In Twenty-Ninth AAAI Conference on Artificial Intelligence
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning 2011
 Neural networks with late-phase weights,2021, In International Confer-ence on Learning Representations
 Can you trust your model's uncertainty? evalu-ating predictive uncertainty under dataset shift,2019, In H
 Can you trust your model¡¯s uncertainty?evaluating predictive uncertainty under dataset shift,2019, arXiv preprint arXiv:1906
 Training adversarially robust sparse networks via bayesianconnectivity sampling,2021, In International Conference on Machine Learning
 When networks disagree: Ensemble methods for hybridneural networks,1992, Technical report
 Some methods of speeding up the convergence of iteration methods,1964, Ussr computa-tional mathematics and mathematical physics
 Statistical methods for categorical data analysis,2008, Emerald GroupPublishing
 Dense for the price of sparse: Improved performance of sparselyinitialized networks via a subspace offset,2021, arXiv preprint arXiv:2102
 Evaluating predictive uncertainty challenge,2005, In Machine Learning ChallengesWorkshop
 Sparse weight activation training,2020, arXiv preprintarXiv:2001
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Green ai,2019, arXiv preprintarXiv:1907
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Spacenet: Make free space forcontinual learning,2021, Neurocomputing
 Ensemble methods as adefense to adversarial perturbations against deep neural networks,2017, arXiv preprint arXiv:1709
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 On the importance of initializationand momentum in deep learning,1139, In International conference on machine learning
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, In Advances in Neural InformationProcessing Systems 33 pre-proceedings
 Keep the gradients flowing: Using gradientflow to study sparse network optimization,2021, arXiv preprint arXiv:2102
 Fixing the train-test resolutiondiscrepancy: Fixefficientnet,2020, arXiv preprint arXiv:2003
 Training data-efficient image transformers & distillation through attention,2021, In InternationalConference on Machine Learning
 Visualizing data using t-sne,2008, Journal of machinelearning research
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Batchensemble: an alternative approach to efficientensemble and lifelong learning,2020, In International Conference on Learning Representations
 Horizontal and vertical ensemble with deep representationfor classification,2013, CoRR
 Wide residual networks,2016, In Edwin R
 Multi-objective evolutionary federated learning,2019, IEEE transactions onneural networks and learning systems
