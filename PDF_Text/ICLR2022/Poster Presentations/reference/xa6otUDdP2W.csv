title,year,conference
 On sampling with and without replacement,1958, In Sankhya: The Indian Journal ofStatistics
 Deep rewiring: Train-ing very sparse deep networks,2018, In Proceedings of the International Conference on LearningRepresentations (ICLR)
 ImageNet: A large-scalehierarchical image database,2009, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Sparse networks from scratch: Faster training without losingperformance,2019, In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)
 Rigging the lottery:Making all tickets winners,2020, In Proceedings of the International Conference on Machine Learning(ICML)
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 DMCP: Differentiable Markov channelpruning for neural networks,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR)
 Dynamic network surgery for efficient DNNs,2016, InProceedings of the Advances in Neural Information Processing Systems (NeurIPS)
 Why random reshuffling beats stochasticgradient descent,2019, In Proceedings of the Mathematical Programming
 Applied machine learning at facebook:A datacenter infrastructure perspective,2018, In Proceedings of the IEEE International Symposium onHigh Performance Computer Architecture (HPCA)
 Sparse progressive distillation: Resolving overfitting underpretrain-and-finetune paradigm,2021, arXiv preprint arXiv:2110
 Soft threshold weight reparameterization for learnable sparsity,2020, In Proceedings ofthe International Conference on Machine Learning (ICML)
 Optimal brain damage,1990, In Proceedings of theAdvances in Neural Information Processing Systems (NeurIPS)
 SNIP: Single-shot network pruningbased on connection sensitivity,2019, In Proceedings of the International Conference on LearningRepresentations (ICLR)
 Hrank: Filter pruning using high-rank feature map,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR)
 Dynamic model pruningwith feedback,2020, In Proceedings of the International Conference on Learning Representations(ICLR)
 MetaPruning: Meta learning for automatic neural network channel pruning,2019, In Proceedingsof the IEEE/CVF International Conference on Computer Vision (ICCV)
 Rethinking the value ofnetwork pruning,2019, In Proceedings of the International Conference on Learning Representations(ICLR)
 SGDR: Stochastic gradient descent with warm restarts,2017, InProceedings of the International Conference on Learning Representations (ICLR)
 PCONV: The missing but desirable sparsity in DNN weight pruning for real-time executionon mobile devices,2020, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, In Proceedings of the Nature Communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In Proceedings of the International Conference on MachineLearning (ICML)
 Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weightpruning,2020, In Proceedings of the Twenty-Fifth International Conference on Architectural Support forProgramming Languages and Operating Systems (ASPLOS)
 Efficient neural architecture searchvia parameters sharing,2018, In Proceedings of the International Conference on Machine Learning(ICML)
 A call for clarity in reporting BLEU scores,2018, In Proceedings of the Third Conference onMachine Translation: Research Papers
 Pointnet++: Deep hierarchical feature learningon point sets in a metric space,2017, arXiv preprint arXiv:1706
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the AAAI Conference on Artificial Intelligence(AAAI)
 Accelerating sparse cnn inferenceon gpus with performance-aware weight pruning,2020, In Proceedings of the ACM InternationalConference on Parallel Architectures and Compilation Techniques (PACT)
 Green ai,2020, In Proceedings of theCommunications of the ACM
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, In Proceedings of the Advances in NeuralInformation Processing Systems (NeurIPS)
 Scop:Scientific control for reliable neural network pruning,2020, In Proceedings of the Advances in NeuralInformation Processing Systems (NeurIPS)
 Singleshot structured pruning before training,2020, arXiv preprint arXiv:2007
 Attention is all you need,2017, In Proceedings of the Advances in NeuralInformation Processing Systems (NeurIPS)
 Picking winning tickets before training by pre-serving gradient flow,2020, In Proceedings of the International Conference on Learning Representations(ICLR)
 Learning structured sparsity indeep neural networks,2016, In Proceedings of the Advances in Neural Information Processing Systems(NeurIPS)
 Freezenet: Full performance by reducedstorage costs,2020, In Proceedings of the Asian Conference on Computer Vision (ACCV)
 Machine learning at facebook: Understandinginference at the edge,2019, In Proceedings of the IEEE International Symposium on High PerformanceComputer Architecture (HPCA)
 Rethinking network pruning-under thepre-train and fine-tune paradigm,2021, NAACL
 Stochastic learning under randomreshuffling with constant step-sizes,2018, In Proceedings of the IEEE Transactions on Signal Processing
 Drawing early-bird tickets: Toward more effi-cient training of deep networks,2020, In Proceedings of the International Conference on LearningRepresentations (ICLR)
 Mest: Accurate and fast memory-economic sparse trainingframework on the edge,2021, Advances in Neural Information Processing Systems (NeurIPS)
 Growing efficient deep networksby structured continuous sparsification,2021, In Proceedings of the International Conference on LearningRepresentations (ICLR)
 mixup: Beyond empiricalrisk minimization,2018, In Proceedings of the International Conference on Learning Representations(ICLR)
 A unified dnn weight pruning framework using reweighted optimization methods,2021, In 202158th ACM/IEEE Design Automation Conference (DAC)
 Structadmm: Achieving ultrahigh efficiency in structuredpruning for dnns,2021, IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
 Effective sparsification of neural networkswith global sparsity constraint,2021, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR)
