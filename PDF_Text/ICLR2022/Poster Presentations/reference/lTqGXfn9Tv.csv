title,year,conference
 High-dimensional dynamics of gener-alization error in neural networks,2020, Neural Networks
 Generalization of two-layer neural networks: An asymptotic viewpoint,2020, In International Conference on Learning Rep-resentations
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,0027, Proceedings of the National Academyof Sciences
 Model selection in kernel based regressionusing the influence function,2008, Journal of Machine Learning Research
 Averaged least-mean-squares: Bias-variance trade-offs andoptimal sampling distributions,2015, In Artificial Intelligence and Statistics
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Scaling description of generalization withnumber of parameters in deep learning,2020, Journal of Statistical Mechanics: Theory and Experiment
 Variational learning for rectified factor analysis,2007, Signal Processing
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Flat minima,1997, Neural computation
 Topics in Matrix Analysis,1991, Cambridge University Press
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2016, arXivpreprint arXiv:1609
 On the role of oPtimization in double descent: A least squares study,2021, arXiv preprintarXiv:2107
 A brief prehistoryof double descent,1091, Proceedings of the National Academy of Sciences
 The generalization error of random features regression: Preciseasymptotics and the double descent curve,2019, Communications on Pure and Applied Mathematics
 Network information criterion-determining the number ofhidden units for an artificial neural network model,1994, IEEE Transactions on Neural Networks
 Harmless interpolation of noisy data inregression,2019, 2019 IEEE International Symposium on Information Theory (ISIT)
 More data can hurt for linear regression: Sample-wise double descent,2019, arXivpreprint arXiv:1912
 Tight bounds on the smallest eigenvalueof the neural tangent kernel for deep relu networks,2021, In International Conference on MachineLearning
 Fast exact multiplication by the hessian,1994, Neural computation
 Geometry of neural network loss surfaces via random matrixtheory,2017, In International Conference on Machine Learning
 Exponential convergence of testingerror for stochastic gradient methods,2018, In Conference on Learning Theory
 Analytic insights into structure and rank of neural net-work hessian maps,2021, In Advances in Neural Information Processing Systems 34 (NeurIPS 2021)
 An asymptotic equivalence of choice of model by cross-validation and akaikeâ€™scriterion,1977, Journal of the Royal Statistical Society: Series B (Methodological)
 Information matrices and generalization,2019, arXiv preprintarXiv:1906
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
