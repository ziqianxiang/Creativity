title,year,conference
 Learning from noisy examples,1988, Machine Learning
 On the efficacy of knowledge distillation,2019, In Proceedingsofthe IEEE/CVF International Conference on Computer Vision
 An embarrassingly simple approach for knowledge distillation,2018, arXiv preprintarXiv:1812
 On calibration of modern neUralnetworks,2017, In International Conference on Machine Learning
 MomentUm contrast forUnsUpervised visUal representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Distilling the knowledge in a neUral network,2015, arXivpreprint arXiv:1503
 Self-adaptive training: beyond empirical riskminimization,2020, Advances in Neural Information Processing Systems
 Neural tangent kernel: Convergence and gen-eralization in neUral networks,2018, In Advances in Neural Information Processing Systems
 Characterizing structural regu-larities of labeled data in overparameterized models,2021, Proceedings of International Conference onMachine Learning
 Understandingknowledge distillation,2019, https://openreview
 Early-learningregularization prevents memorization of noisy labels,2020, Advances in Neural Information ProcessingSystems
 Learning withnoisy labels,2013, Advances in Neural Information Processing Systems
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Humanuncertainty makes classification more robust,2019, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 S2sd:Simultaneous similarity-based self-distillation for deep metric learning,2021, Proceedings of Interna-tional Conference on Machine Learning
 Efficient estimations from a slowly convergent Robbins-Monro process,1988, Technicalreport
 Mo-bileNetV2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Very deep convolutional networks for large-scale imagerecognition,2015, Proceedings of International Conference on Learning Representations
 Going deeper with convolutions,2015, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Rethink-ing the Inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 EfficientNet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Under-standing and improving knowledge distillation,2020, arXiv preprint arXiv:2002
 Revisiting knowledge distillationvia label smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Paying more attention to attention: Improving the per-formance of convolutional neural networks via attention transfer,2017, Proceedings of InternationalConference on Learning Representations
 Be yourown teacher: Improve the performance of convolutional neural networks via self distillation,2019, InICCV
 Self-distillation as instance-specific label smoothing,2020, Advancesin Neural Information Processing Systems
 Distilling effectivesupervision from severe label noise,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
