title,year,conference
 Learning deep latent variable models by short-run MCMCinference with optimal transport correction,2021, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR) 
 Generalized energy based models,2021, In Proceedingsof the 9th International Conference on Learning Representations (ICLR)
 Monte Carlo Methods,2020, Springer Nature
 On contrastive divergence learning,2005, In Pro-ceedings of the 10th International Workshop on Artificial Intelligence and Statistics (AISTATS)
 Residual flows for in-vertible generative modeling,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Diagnosing and enhancing VAE models,2019, In Proceeding of the 7thInternational Conference on Learning Representations (ICLR)
 NICE: non-linear independent components esti-mation,2015, In Workshop Proceedings of the 3rd International Conference on Learning Representa-tions (ICLR Workshop)
 Density estimation using real NVP,2017, InProceedings of the 5th International Conference on Learning Representations (ICLR)
 Implicit generation and modeling with energy based models,2019, In Ad-vances in Neural Information Processing Systems (NeurIPS)
 Energy-based models foratomic-resolution protein conformations,2020, In Proceedings of the 8th International Conference onLearning Representations (ICLR)
 Improved contrastive divergencetraining of energy-based models,2021, In Proceedings of the 38th International Conference on MachineLearning (ICML)
 Learning generative con-vnets via multi-grid modeling and sampling,2018, In Proceedings of the 2018 IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Flowcontrastive estimation of energy-based models,2020, In Proceedings of the 2020 IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Learning energy-basedmodels by diffusion recovery likelihood,2021, In Proceeding of the 9th International Conference onLearning Representations (ICLR)
 Your classifier is secretly an energy based model and you should treat it likeone,2020, In Proceedings of the 8th International Conference on Learning Representations (ICLR)
 No MCMC for me: Amortized sampling for fast and stable training ofenergy-based models,2021, In Proceedings of the 9th International Conference on Learning Represen-tations (ICLR)
 Im-proved training of Wasserstein gans,2017, In Advances in Neural Information Processing Systems(NIPS)
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conferenceon Artificial Intelligence and Statistics (AISTATS)
 Alternating back-propagation for generatornetwork,2017, In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI)
 Joint trainingof variational auto-encoder and latent energy-based model,2020, In Proceedings of the 2020 IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Training products of experts by minimizing contrastive divergence,2002, NeuralComputation
 A practical guide to training restricted boltzmann machines,2012, In Neural networks:Tricks of the trade
 Flow++: Improving flow-based generative models with variational dequantization and architecture design,2019, In Proceedingsof the 36th International Conference on Machine Learning (ICML)
 Traininggenerative adversarial networks with limited data,2020, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Adam: A method for stochastic optimization,2015, In Proceedingsof the 3rd International Conference on Learning Representations (ICLR)
 Auto-encoding variational bayes,2014, In Proceedings of the 2ndInternational Conference on Learning Representations (ICLR)
 Learning multiple layers of features from tiny images,2009, 2009
 Videoflow: A conditional flow-based model for stochastic video gen-eration,2020, In Proceeding of the 8th International Conference on Learning Representations (ICLR)
 A tutorial on energy-basedlearning,2006, Predicting structured data
 Deep learning face attributes in the wild,2015, InProceedings of the 2015 International Conference on Computer Vision (ICCV)
 Spectral normalization forgenerative adversarial networks,2018, In Proceeding of the 6th International Conference on LearningRepresentations (ICLR)
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Learning non-convergent non-persistent short-run MCMC toward energy-based model,2019, In Advances in Neural InformationProcessing (NeurIPS)
 Learning energy-based model with flow-based backbone by neural transportMCMC,2020, arXiv preprint arXiv:2006
 Learningmulti-layer latent variable model via variational optimization of short run MCMC for approximateinference,2020, In Proceedings of the 16th European Conference on Computer Vision (ECCV
 Autoregressive quantile networks for generativemodeling,2018, In Proceedings of the 35th International Conference on Machine Learning (ICML)
 Learning latent spaceenergy-based prior model,2020, In Advances in Neural Information Processing Systems (NeurIPS)
 Waveglow: A flow-based generative network forspeech synthesis,2019, In Proceeding of the International Conference on Acoustics
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2016, In Proceedings of the 4th International Conferenceon Learning Representations (ICLR)
 Variational inference with normalizing flows,2015, InProceedings of the 32nd International Conference on Machine Learning (ICML)
 Weight normalization: A simple reparameterization to ac-celerate training of deep neural networks,2016, In Advances in Neural Information Processing Systems29 (NIPS)
 PixelCNN++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications,2017, In Proceeding ofthe 5th International Conference on Learning Representations (ICLR)
 Improved techniques for training score-based generative models,2020, InAdvances in Neural Information Processing Systems (NeurIPS)
 Score-based generative modeling through stochastic differential equations,2021, In Proceedingof the 9th International Conference on Learning Representations (ICLR)
 NVAE: A deep hierarchical variational autoencoder,2020, In Advances inNeural Information Processing Systems (NeurIPS)
 Stochastic normalizing flows,2020, In Advances in NeuralInformation Processing (NeurIPS)
 VAEBM: A symbiosis between varia-tional autoencoders and energy-based models,2021, In Proceeding of the 9th International Conferenceon Learning Representations (ICLR)
 A theory of generative convnet,2016, InProceedings of the 33nd International Conference on Machine Learning (ICML)
 Synthesizing dynamic patterns by spatial-temporal generative convnet,2017, In Proceeding of the IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Cooperative learning of energy-based modeland latent variable model via MCMC teaching,2018, In Proceedings of the 32nd AAAI Conference onArtificial Intelligence
 Learn-ing descriptor networks for 3d shape synthesis and analysis,2018, In Proceeding of the IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR)
 Cooperative training of de-scriptor and generator networks,2020, IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI)
 Gen-erative voxelnet: learning energy-based models for 3d shape synthesis and analysis,2020, IEEE Trans-actions on Pattern Analysis and Machine Intelligence (TPAMI)
 Learning energy-based model with variational auto-encoder as amortized sampler,2021, In Proceeding of the Thirty-Fifth AAAI Conference on ArtificialIntelligence (AAAI)
 Learning energy-based spatial-temporal genera-tive convnets for dynamic patterns,2021, IEEE Transactions on Pattern Analysis and Machine Intelli-gence (TPAMI)
 On the convergence of markovian stochastic algorithms with rapidly decreasingergodicity rates,1999, Stochastics: An International Journal of Probability and Stochastic Processes
 Learning energy-based generative models via coarse-to-fine expanding and sampling,2021, In Proceeding of the 9th International Conference on LearningRepresentations (ICLR)
 Patchwise generative convnet: Training energy-basedmodels from a single natural image for internal learning,2021, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR)
