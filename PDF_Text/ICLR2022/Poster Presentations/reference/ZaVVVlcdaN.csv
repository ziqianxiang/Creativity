title,year,conference
 Federatedlearning via posterior averaging: A new perspective and practical algorithms,2020, arXiv preprintarXiv:2010
 Communication complexity of distributed convex learning andoptimization,2015, arXiv preprint arXiv:1506
 A universallyoptimal multistage accelerated stochastic gradient method,2019, arXiv preprint arXiv:1901
 Lower bounds for finding stationarypoints i,2020, Mathematical Programming
 On the outsized importance of learning rates in local updatemethods,2020, arXiv preprint arXiv:2007
 Onlarge-cohort training for federated learning,2021, arXiv preprint arXiv:2106
 Emnist: Extending mnistto handwritten letters,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, Advances in neural informationprocessing systems
 Local stochastic gradient descent ascent: Convergence analysisand communication efficiency,2021, In International Conference on Artificial Intelligence and Statistics
 Adaptive personalized federatedlearning,2020, arXiv preprint arXiv:2003
 Distributionally robust federatedaveraging,2021, arXiv preprint arXiv:2102
 An optimal multistage stochastic gradientmethod for minimax problems,2020, In 2020 59th IEEE Conference on Decision and Control (CDC)
 Optimal stochastic approximation algorithms for stronglyconvex stochastic composite optimization i: A generic algorithmic framework,2012, SIAM Journal onOptimization
 Local sgd: Unified theory and new efficientmethods,2020, arXiv preprint arXiv:2011
 Accelerating stochastic gradient descent using predictive variancereduction,2013, Advances in neural information processing Systems
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Mime: Mimicking centralized stochastic algorithms infederated learning,2020, arXiv preprint arXiv:2008
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 A unifiedtheory of decentralized sgd with changing topology and local updates,2020, In International Conferenceon Machine Learning
 Learning multiple layers of features from tiny images,2009, Technical report
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Fast incremental method fornonconvex optimization,2016, arXiv preprint arXiv:1603
 Local sgd converges fast and communicates little,2018, arXiv preprint arXiv:1805
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 Adaptive communication strategies to achieve the best error-runtimetrade-off in local-update sgd,2019, In SysML
 Matcha: Speeding updecentralized sgd via matching decomposition sampling,2019, In 2019 Sixth Indian Control Conference(ICC)
 Slowmo: Improvingcommunication-efficient distributed sgd with slow momentum,2019, arXiv preprint arXiv:1910
 The minimax complexity of distributed optimization,2021, arXiv preprintarXiv:2109
 Minibatch vs local sgd for heterogeneousdistributed learning,2020, arXiv preprint arXiv:2006
 The min-max complexityof distributed stochastic convex optimization with intermittent communication,2021, arXiv preprintarXiv:2102
 Federated accelerated stochastic gradient descent,2020, arXiv preprintarXiv:2006
 Federated composite optimization,2020, arXiv preprintarXiv:2011
 Directacceleration of saga using sampled negative momentum,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
