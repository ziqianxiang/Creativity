title,year,conference
 High-dimensional dynamics of generalizationerror in neural networks,893, Neural Networks
 Scalable Second Order Optimizationfor Deep Learning,2021, arXiv:2002
 Fine-Grained Analysis of Optimiza-tion and Generalization for Overparameterized Two-Layer Neural Networks,2019, In International Conferenceon Machine Learning
 Implicit Gradient Regularization,2020, In International Conference on LearningRepresentations
 Stability of stochastic gradient descent onnonsmooth convex losses,2020, In Advances in Neural Information Processing Systems
 Large-Scale Machine Learning with Stochastic Gradient Descent,2010, In Proceedings of COMP-STAT¡¯2010
 Language Modelsare Few-Shot Learners,2020, In 34th Conference on Neural Information Processing Systems (NeurIPS 2020)
 Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,2020, In Eighth International Conference on Learning Representations
 cuDNN: Efficient Primitives for Deep Learning,2014, arXiv:1410
 Gradient Descent on NeuralNetworks Typically Occurs at the Edge of Stability,2020, In International Conference on Learning Representa-tions
 Towards Theoretical Understanding of Large Batch Training in Stochastic Gra-dient Descent,2018, arXiv:1812
 Can Implicit Bias Explain Generalization? StochasticConvex Optimization as a Case Study,2020, arXiv:2003
 Automated Inference with Adaptive Batches,2017, InArtificial Intelligence and Statistics
 Sharp Minima Can Generalize For DeepNets,2017, arXiv:1703
 Gradient Descent Finds Global Minima ofDeep Neural Networks,2019, In International Conference on Machine Learning
 GradientDescent Can Take Exponential Time to Escape Saddle Points,2017, Advances in Neural Information Pro-cessing Systems
 Sharpness-Aware Minimization forEfficiently Improving Generalization,2021, arXiv:2010
 Escaping From Saddle Points ¡ª Online Stochastic Gradientfor Tensor Decomposition,2015, In Conference on Learning Theory
 A Loss Curvature Perspective on Training Instability in DeepLearning,2021, arXiv:2110
 Truth or backpro-paganda? An empirical investigation of deep learning theory,2020, In Eighth International Conference on Learn-ing Representations (ICLR 2020
 On the Computational Inefficiency of Large Batch Sizes for Stochastic GradientDescent,2018, September 2018
 ShaPe Matters: Understanding the ImPlicit Bias ofthe Noise Covariance,2020, arXiv:2006
 Delving DeeP into Rectifiers: SurPassing Human-Level Performance on ImageNet Classification,2015, arXiv:1502
 Bag of Tricks for ImageClassification with Convolutional Neural Networks,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Flat Minima,1997, Neural Computation
 On the diffusion approximation of nonconvex stochas-tic gradient descent,2018, arXiv:1705
 Neural tangent kernel: Convergence and generalizationin neural networks,2018, In Proceedings of the 32nd International Conference on Neural Information ProcessingSystems
 Width of Minima Reached by Stochastic Gradient Descent is Influenced by Learning Rate to BatchSize Ratio,2018, In Artificial Neural Networks and Machine Learning - ICANN 2018
 Fantastic Gen-eralization Measures and Where to Find Them,2019, arXiv:1912
 Adam: A Method for Stochastic Optimization,2015, In International Conferenceon Learning Representations (ICLR)
 Stochasticity of Deterministic Gradient Descent: Large Learning Rate for Multi-scale Objective Function,2020, In NeurIPS
 Learning Multiple Layers of Features from Tiny Images,2009,2009
 Imagenet classification with deep convolutionalneural networks,2012, In Advances in Neural Information Processing Systems
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Efficient BaCkProp,1998, In Neural Net-works: Tricks of the Trade
 Gradient Descent Only Convergesto Minimizers,2016, In Conference on Learning Theory
 The large learningrate phase of deep learning: The catapult mechanism,2020, arXiv:2003
 Visualizing the Loss Land-scape of Neural Nets,2018, In Advances in Neural Information Processing Systems
 Towards Explaining the Regularization Effect of Initial Large LearningRate in Training Neural Networks,2020, arXiv:1907
 An Exponential Learning Rate Schedule for Deep Learning,2019, In Interna-tional Conference on Learning Representations
 Acceleration of Primal-Dual Methods by Preconditioning and FixedNumber of Inner Loops,2018, arXiv:1811
 SGDR: Stochastic Gradient Descent with Warm Restarts,2017, arXiv:1608
 Optimizing Neural Networks with Kronecker-factored Approximate Curva-ture,2020, arXiv:1503
 An Empirical Model of Large-Batch Training,2018, arXiv:1812
 Extreme Memorization via Scale of Initialization,2020, InInternational Conference on Learning Representations
 Towards Understand-ing the Role of Over-Parametrization in Generalization of Neural Networks,2018, arXiv:1805
 Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature forDeep Convolutional Neural Networks,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Automatic differentiation in PyTorch,2017, In NIPS 2017 AutodiffWorkshop
 Fast Exact Multiplication by the Hessian,1994, Neural Computation
 Loss landscape: Sgd has a better view,2020, Technical report
 SGD Implicitly Regularizes Generalization Error,2018, Neural Information Processing SystemsWorkshop on Integration of Deep Learning Theories
 On the information bottleneck theory of deep learning,1742, Journal of Statistical Mechanics:Theory and Experiment
 Descending through a Crowded Valley - BenCh-marking Deep Learning Optimizers,2020, arXiv:2007
 Measuring the Effects of Data Parallelism on Neural Network Training,1533, Journal of Machine LearningResearch
 A Tail-Index Analysis of Stochastic Gradient Noisein Deep Neural Networks,5827, In International Conference on Machine Learning
 On the Generalization Benefit of Noise in Stochastic GradientDescent,2020, In International Conference on Machine Learning
 On the Origin of Implicit Regularization inStochastic Gradient Descent,2020, In International Conference on Learning Representations
 An EmpiricalStudy of Stochastic Gradient Descent with Structured Covariance Noise,2020, In International Conference onArtificial Intelligence and Statistics
 The general inefficiency of batch training for gradient descent learn-ing,893, Neural Networks
 On the NoisyGradient Descent that Generalizes as SGD,2020, In International Conference on Machine Learning
 Group Normalization,2018, In Proceedings of the European Conference on Com-puter Vision (ECCV)
 A Diffusion Theory For Deep Learning Dynamics: StochasticGradient Descent Exponentially Favors Flat Minima,2020, In International Conference on Learning Representa-tions
 Understanding and Scheduling Weight Decay,2021, arXiv:2011
 A Walk with SGD,2018, arXiv:1802
 Making L-BFGS Work with Industrial-Strength Nets,2020, In BMVC 2020
 Yet Another Accelerated SGD: ResNet-50 Trainingon ImageNet in 74,2019,7 seconds
 Large-Batch Trainingfor LSTM and Beyond,2019, arXiv:1901
 Large Batch Optimization for Deep Learning: Training BERTin 76 minutes,2019, In International Conference on Learning Representations
 The Limit of the BatchSize,2020, arXiv:2006
 Improved Analysis of Clipping Al-gorithms for Non-convex Optimization,2020, Advances in Neural Information Processing Systems
 Which Algorithmic Choices Matter at Which Batch Sizes? In-sights From a Noisy Quadratic Model,2019, In Advances in Neural Information Processing Sys-tems
 Why Gradient Clipping Accelerates Training: ATheoretical Justification for Adaptivity,2019, In International Conference on Learning Representations
 The Anisotropic Noise in Stochastic GradientDescent: Its Behavior of Escaping from Sharp Minima and Regularization Effects,2019, In International Con-ference on Machine Learning
