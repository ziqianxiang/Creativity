title,year,conference
 Tweet-Eval: Unified benchmark and comparative evaluation for tweet classification,2020, In Findings ofthe Association for Computational Linguistics: EMNLP 2020
 A model of inductive bias learning,2000, Journal of artificial intelligence research
 Semantic parsing on Freebase fromquestion-answer pairs,2013, In Proceedings of the 2013 Conference on Empirical Methods in NaturalLanguage Processing
 Abductive commonsensereasoning,2020, In International Conference on Learning Representations
 Piqa: Reasoningabout physical commonsense in natural language,2020, In Thirty-Fourth AAAI Conference on ArtificialIntelligence
 Nuancedmetrics for measuring unintended bias with real data for text classification,9781, In Compan-ion Proceedings of The 2019 World Wide Web Conference
 Language models are few-shotlearners,2020, In Hugo Larochelle
 Multitask learning,1997, Machine learning
 Pre-trainingtasks for embedding-based large-scale retrieVal,2020, arXiv preprint arXiv:2002
 QuAC: Question answering in context,2018, In Proceedings of the 2018 Con-ference on Empirical Methods in Natural Language Processing
 A unified architecture for natural language processing: Deepneural networks with multitask learning,9781, In Proceedings of the 25th International Conferenceon Machine Learning
 The pascal recognising textual entailment chal-lenge,2006, In JoaqUin Quinonero-Candela
 The CommitmentBank: In-Vestigating projection in naturally occurring discourse,2019,2019
 GoEmotions: A dataset of fine-grained emotions,2020, In Proceedings of the 58th An-nual Meeting of the Association for Computational Linguistics
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies
 ERASER: A benchmark to eValuate rationalized NLP models,2020, In Proceed-ings of the 58th Annual Meeting of the Association for Computational Linguistics
 Wizardof Wikipedia: Knowledge-powered conversational agents,2019, In Proceedings of the InternationalConference on Learning Representations (ICLR)
 Semantic Noise Matters for Neural NaturalLanguage Generation,2019, In Proceedings of the 12th International Conference on Natural LanguageGeneration (INLG 2019)
 Multi-neWs: A large-scale multi-document summarization dataset and abstractive hierarchical model,2019, In Proceedingsof the 57th Annual Meeting of the Association for Computational Linguistics
 Wikilingua: A neW benchmarkdataset for multilingual abstractive summarization,2020, In Findings of EMNLP
 Efficientlyidentifying task groupings for multi-task learning,2021, arXiv preprint arXiv:2109
 Creating train-ing corpora for nlg micro-planners,2017, In Proceedings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers)
 SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization,2019, In Proceedings of the 2nd Workshopon New Frontiers in Summarization
 AutoSeM: Automatic task selection and mixingin multi-task learning,2019, In Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Bench-marking meaning representations in neural semantic parsing,2020, In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural Language Processing (EMNLP)
 Robust entity linking via random walks,2014, In Proceedings of the23rd ACM International Conference on Conference on Information and Knowledge Management
 To-ward semantics-based answer pinpointing,2001, In Proceedings of the First International Conferenceon Human Language Technology Research
 Cosmos QA: Machine readingcomprehension with contextual commonsense reasoning,2019, In Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP)
 Shakespearizing modern languageusing copy-enriched sequence to sequence models,2017, In Proceedings of the Workshop on StylisticVariation
 Neural CRF model forsentence alignment in text simplification,2020, In Proceedings of the 58th Annual Meeting of the Asso-ciationfor Computational Linguistics
 Span-bert: Improving pre-training by representing and predicting spans,2020, Transactions of the Associationfor Computational Linguistics
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Looking be-yond the surface: A challenge set for reading comprehension over multiple sentences,2018, In Proceed-ings of the 2018 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies
 UNIFIEDQA: Crossing format boundaries with a single QA system,2020, InFindings of the Association for Computational Linguistics: EMNLP 2020
 COGS: A compositional generalization challenge based on semanticinterpretation,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP)
 Investigating multilingual NMTrepresentations at scale,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th International Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP)
 Natural Questions: A Benchmark for Question Answering Research,276, Transactions ofthe Association for Computational Linguistics
 RACE: Large-scaleReAding comprehension dataset from examinations,2017, In Proceedings of the 2017 Conferenceon Empirical Methods in Natural Language Processing
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 On better exploring and exploiting task rela-tionships in multitask learning: Joint model and feature learning,2162, IEEE Transactions on NeuralNetworks and Learning Systems
 CommonGen: A constrained text generation challenge for generative commonsensereasoning,2020, In Findings of the Association for Computational Linguistics: EMNLP 2020
 Choosing Transfer Languages for Cross-Lingual Learning,2019, In Proceedings of ACL 2019
 Multi-task deep neural networksfor natural language understanding,2019, In Proceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics
 Unicorn on rainbow: Auniversal commonsense reasoning model on a new multitask benchmark,2021, AAAI
 StylePTB: A compositional benchmark for fine-grained controllabletext style transfer,2021, In Proceedings of the 2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Learning Word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Ad-versarial NLI: A new benchmark for natural language understanding,2020, In Proceedings of the58th Annual Meeting of the Association for Computational Linguistics
 Document ranking with apretrained sequence-to-sequence model,2020, In Findings of the Association for Computational Lin-guistics: EMNLP 2020
 AgreeSum: Agreement-orientedmulti-document summarization,2021, In Findings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021
 Totto: A controlled table-to-text generation dataset,2020, arXiv preprintarXiv:2004
 KILT: a benchmark for knowledge intensive languagetasks,2021, In Proceedings of the 2021 Conference of the North American Chapter of the Associ-ation for Computational Linguistics: Human Language Technologies
 WiC: the word-in-context dataset forevaluating context-sensitive meaning representations,2019, In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies
 Direct transfer of learnedinformation among neural networks,1991, In Aaai
 Dart: Open-domainstructured data record to text generation,2020, arXiv preprint arXiv:2007
 Gpt-2 output dataset,2019, https://github
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, Journal of Machine Learning Research
 Few-shot ques-tion answering by pretraining span selection,3066, In Proceedings of the 59th Annual Meetingof the Association for Computational Linguistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: Long Papers)
 Choice of plausible alternatives:An evaluation of commonsense causal reasoning,2011, In 2011 AAAI Spring Symposium Series
 To transferor not to transfer,2005, In In NIPS¡¯05 Workshop
 Learning to select data for transfer learning with Bayesianoptimization,2017, In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-guage Processing
 Transfer learningin natural language processing,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Tutorials
 A neural attention model for abstractive sen-tence summarization,2015, In Proceedings of the 2015 Conference on Empirical Methods in NaturalLanguage Processing
 Winogrande: An adver-sarial winograd schema challenge at scale,2020, ArXiv
 A Hierarchical Multi-task Approach for LearningEmbeddings from Semantic Tasks,2019, In Proceedings of AAAI 2019
 Social IQa: Com-monsense reasoning about social interactions,2019, In Proceedings of the 2019 Conference on Em-pirical Methods in Natural Language Processing and the 9th International Joint Conference onNatural Language Processing (EMNLP-IJCNLP)
 Get your vitamin C! robust fact verification Withcontrastive evidence,2021, In Proceedings of the 2021 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
 Get to the point: Summarization Withpointer-generator netWorks,2017, In Proceedings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers)
 Mesh-tensorfloW: Deep learning for supercomputers,2018, In Samy Bengio
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Scale efficiently: Insights frompre-training and fine-tuning transformers,2021, arXiv preprint arXiv:2109
 Hypergrid transformers: To-wards a single model for multiple tasks,2021, In International Conference on Learning Representa-tions
0 shared task,2018, In Proceedings of the Second Workshop on Fact Extraction andVERification (FEVER)
 Attention is all yoU need,2017, In Isabelle GUyon
 Exploring and predicting transferabilityacross NLP tasks,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 SUperglUe: A stickier benchmark for general-pUrposelangUage Understanding systems,2019, In Hanna M
 Gradient vaccine: Investigating andimproving mUlti-task optimization in massively mUltilingUal models,2021, In International Confer-ence on Learning Representations
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings ofthe 2018 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Ex machina: Personal attacks seen at scale,2017, InRick Barrett
 Optimiz-ing statistical machine translation for text simplification,2016, Transactions of the Associationfor Computational Linguistics
 DocNLI: A large-scale dataset for document-level natural language inference,2021, In Findings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021
 Aida:An online tool for accurate disambiguation of named entities in text and tables,2150, Proc
 This email could save your life: Introducing the task of email subjectline generation,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Character-level convolutional networks for textclassification,2015, In Corinna Cortes
 A convex formulation for learning task relationships in multi-tasklearning,9780, In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence
 Ernie: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
 Some of the lines in the table representexisting benchmarks that group several tasks together,2020, From each collection
