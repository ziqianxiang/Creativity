title,year,conference
 Pada: A prompt-based autoregressive approachfor adaptation to unseen domains,2021, arXiv preprint arXiv:2102
 One-shot unsupervised cross domain translation,2018, arXiv preprintarXiv:1806
 Continual lifelong learningin natural language processing: A survey,2020, arXiv preprint arXiv:2012
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Efficientlifelong learning with a-gem,2018, arXiv preprint arXiv:1812
 Net2net: Accelerating learning via knowledgetransfer,2015, arXiv preprint arXiv:1511
 Learning to learn without gradient descent by gra-dient descent,2017, In Doina Precup and Yee Whye Teh (eds
 Episodicmemory in lifelong language learning,2019, arXiv preprint arXiv:1906
 The commitmentbank: In-vestigating projection in naturally occurring discourse,2019, In proceedings of Sinn und Bedeutung
 Prompt-learning for fine-grained entity typing,2021, arXiv preprintarXiv:2108
 Making pre-trained language models better few-shotlearners,2020, arXiv preprint arXiv:2012
 Neuralsnowball for few-shot relation learning,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Ptr: Prompt tuning with rulesfor text classification,2021, arXiv preprint arXiv:2105
 Ontonotes:the 90% solution,2006, In Proceedings of the human language technology conference of the NAACL
 Few-shot charge prediction withdiscriminative legal attributes,2018, In Proceedings of the 27th International Conference on Computa-tional Linguistics
 Rational lamol: A rationale-based lifelong learning framework,2021, In Pro-ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 Scitail: A textual entailment dataset from sciencequestion answering,2018, In Thirty-Second AAAI Conference on Artificial Intelligence
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Ask me anything: Dynamic memory networks fornatural language processing,1378, In International conference on machine learning
 The power of scale for parameter-efficient prompttuning,2021, arXiv preprint arXiv:2104
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Learning without forgetting,2017, IEEE transactions on pattern analysisand machine intelligence
 Piggyback: Adapting a single network to multi-ple tasks by learning to mask weights,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 A sick cure for the evaluation of compositional distributional semantic models,2014, InLrec
 Image-based rec-ommendations on styles and substitutes,2015, In Proceedings of the 38th international ACM SIGIRconference on research and development in information retrieval
 The natural languagedecathlon: Multitask learning as question answering,2018, arXiv preprint arXiv:1806
 Abstractive text summarizationusing sequence-to-sequence rnns and beyond,2016, arXiv preprint arXiv:1602
 AdaPter-Fusion: Non-destructive task composition for transfer learning,2021, In Proceedings of the 16th Con-ference of the European Chapter of the Association for Computational Linguistics: Main Volume
 Progressive neUral networks,2016, arXiv preprintarXiv:1606
 Exploiting cloze questions for few shot text classification andnatUral langUage inference,2020, arXiv preprint arXiv:2001
 AUtoprompt:Eliciting knowledge from langUage models with aUtomatically generated prompts,2020, arXiv preprintarXiv:2010
 Lamol: LangUage modeling for lifelong langUagelearning,2019, arXiv preprint arXiv:1909
 Distill and replay for contin-Ual langUage learning,2020, In Proceedings of the 28th International Conference on ComputationalLinguistics
 Improving andsimplifying pattern exploiting training,2021, arXiv preprint arXiv:2103
 Lifelong robot learning,0921, Robotics and Au-tonomous Systems
 Sen-tence embedding alignment for lifelong relation extraction,2019, arXiv preprint arXiv:1903
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Lifelong learning with dynamicallyexpandable networks,2017, arXiv preprint arXiv:1708
 Character-level convolutional networks for text clas-sification,2015, Advances in neural information processing systems
 Meta-tuning language models to answerprompts better,2021, arXiv preprint arXiv:2104
