title,year,conference
 Self-supervised classification network,2021, arXiv preprintarXiv:2103
 Self-labelling via simultaneous clusteringand representation learning,2020, In ICLR
 SiT: Self-supervised vision transformer,2021, arXivpreprint arXiv:2104
 BEiT: BERT pre-training of image transformers,2021, arXivpreprint arXiv:2106
 Cascade R-CNN: High quality object detection and instancesegmentation,2019, TPAMI
 Deep clustering for unsu-pervised learning of visual features,2018, In ECCV
 Emerging properties in self-supervised vision transformers,2021, In ICCV
 A simple frameWork forcontrastive learning of visual representations,2020, In ICML
 Big self-supervised models are strong semi-supervised learners,2020, In NeurIPS
 Exploring simple siamese representation learning,2021, In CVPR
 An empirical study of training self-supervised visiontransformers,2021, In ICCV
 Uniter: Universal image-text representation learning,2020, In ECCV
 ImageNet: A large-scalehierarchical image database,2009, In CVPR
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Unsupervised visual representation learning bycontext prediction,2015, In ICCV
 An image is Worth 16x16 Words: Transformers for image recognition atscale,2021, In ICLR
 Bootstrap your oWn latent: AneW approach to self-supervised learning,2020, In NeurIPS
 Mask R-CNN,2017, In ICCV
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Data-efficient image recognition with contrastive predictive coding,2020, In ICML
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In ICLR
 Natural adversarialexamples,2021, In CVPR
 Distilling the knowledge in a neural network,2015, InNeurIPS
 Unsupervised representation learning by predicting imagerotations,2018, In ICLR
 Efficient self-supervised vision transformers for representation learning,2021, arXivpreprint arXiv:2106
 Microsoft coco: Common objects in context,2014, In ECCV
 Self-supervised learning: Generative or contrastive,2021, TKDE
 SWin transformer: Hierarchical vision transformer using shifted WindoWs,2021, arXiv preprintarXiv:2103
 Decoupled Weight decay regularization,2019, In ICLR
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, In NeurIPS
 Intriguing properties of vision transformers,2021, arXiv preprintarXiv:2105
 Unsupervised learning of visual representations by solving jigsaWpuzzles,2016, In ECCV
 Contextencoders: Feature learning by inpainting,2016, In CVPR
 Zero-shot text-to-image generation,2021, In ICML
 Faster r-cnn: ToWards real-time objectdetection With region proposal netWorks,2015, NeurIPS
 Discrete variational autoencoders,2017, In ICLR
 Neural machine translation of rare Words WithsubWord units,2016, In ACL
 Vl-bert: Pre-trainingof generic visual-linguistic representations,2020, ICLR
 Vimpac: Video pre-training via masked tokenprediction and contrastive learning,2021, arXiv preprint arXiv:2106
 Training data-efficient image transformers & distillation through attention,2021, InICML
 Scan: Learning to classify images without labels,2020, In ECCV
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, In ICCV
 Dense contrastive learningfor self-supervised visual pre-training,2021, In CVPR
 Google¡¯s neural machine trans-lation system: Bridging the gap between human and machine translation,2016, arXiv preprintarXiv:1609
 Unsupervised feature learning via non-parametric instance discrimination,2018, In CVPR
 Noise or signal: Therole of image backgrounds in object recognition,2020, In ICLR
 Unified perceptual parsing forscene understanding,2018, In ECCV
 Self-supervised learning with swin transformers,2021, arXiv preprint arXiv:2105
 Propagate yourself: Ex-ploring pixel-level consistency for unsupervised visual representation learning,2021, In CVPR
 Self-supervisedvisual representations learning by contrastive mask prediction,2021, arXiv preprint arXiv:2108
 Sceneparsing through ade20k dataset,2017, In CVPR
