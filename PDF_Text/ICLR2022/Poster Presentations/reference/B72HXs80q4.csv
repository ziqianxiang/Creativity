title,year,conference
 Better fine-tuning by reducing representational collapse,2020, ArXiv preprint
 Language models are few-shotlearners,2020, In Hugo Larochelle
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, ArXiv preprint
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, ArXiv preprint
 SMART:Robust and efficient fine-tuning for pre-trained natural language models through principled reg-ularized optimization,2020, In Proceedings of the 58th Annual Meeting of the Association for Com-putational Linguistics
 Scalable and ef-ficient moe training for multitask multilingual models,2021, ArXiv preprint
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, ArXiv preprint
 On the variance of the adaptive learning rate and beyond,2020, In 8th International Conferenceon Learning Representations
 Understanding the dif-ficulty of training transformers,2020, In Proceedings of the 2020 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP)
 Very deep transformers for neural ma-chine translation,2020, ArXiv preprint
 Roberta: A robustly optimized bert pretrainingapproach,2019, ArXiv preprint
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation: Research Papers
 A call for clarity in reporting BLEU scores,2018, In Proceedings of the Third Conference onMachine Translation: Research Papers
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-text transformer,2019, ArXiv preprint
 Hash layers for largesParse models,2021, ArXiv preprint
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, In 5th International Conference on Learning Representations
 Mixture models for diversemachine translation: Tricks of the trade,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov(eds
 The evolved transformer,2019, In Kamalika Chaudhuriand Ruslan Salakhutdinov (eds
 Re-thinking the inception architecture for computer vision,2016, In 2016 IEEE Conference on Com-puter Vision and Pattern Recognition
 Attention is all you need,2017, In Isabelle Guyon
 Learning deep transformer models for machine translation,2019, In Proceedings of the 57thAnnual Meeting of the Association for Computational Linguistics
 Pay less attentionwith lightweight and dynamic convolutions,2019, In 7th International Conference on Learning Rep-resentations
 Depth growing for neural machine translation,2019, In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics
 Exploring sparse expert models and beyond,2021, ArXiv preprint
