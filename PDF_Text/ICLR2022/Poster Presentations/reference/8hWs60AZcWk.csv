title,year,conference
 Quantifying attention flow in transformers,2020, arXiv preprintarXiv:2005
 All about VLAD,2013, In CVPR
 Beit: Bert pre-training of image transformers,2021, arXiv preprintarXiv:2106
 ObjectNet: A large-scale bias-controlled dataset for pushing thelimits of object recognition models,2019, In NeurIPS
 Understanding robustness of transformers for image classification,2021, arXiv preprintarXiv:2103
 When vision transformers outperform resnetswithout pretraining or strong data augmentations,2021, arXiv preprint arXiv:2106
 An empirical study of training self-supervised visiontransformers,2021, arXiv preprint arXiv:2104
 Twins: Revisiting the design of spatial attention in vision transformers,2021, arXivpreprint arXiv:2104
 Visualcategorization with bags of keypoints,2004, In In Workshop on Statistical Learning in Computer Vision
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 ImProved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Taming transformers for high-resolution imagesynthesis,2021, In CVPR
 ImageNet-trained CNNs are biased towards texture; increasing shaPe biasimProves accuracy and robustness,2019, In ICLR
 DeeP residual learning for image recog-nition,2016, In CVPR
 Benchmarking neural network robustness to common cor-ruPtions and Perturbations,2019, In ICLR
 Using self-suPervised learningcan imProve model robustness and uncertainty,2019, arXiv preprint arXiv:1906
 Natural adversarialexamPles,2021, CVPR
 The origins and Prevalence of texture bias inconvolutional neural networks,2020, In NeurIPS
 Self-challenging imProves cross-domaingeneralization,2020, In ECCV
 Self-challenging imProves cross-domaingeneralization,2020, In ECCV
 PercePtual losses for real-time style transfer andsuPer-resolution,2016, In ECCV
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Gen-erative interventions for causal learning,2021, In CVPR
 Towards robust vision transformer,2021, arXiv preprint arXiv:2105
 Learning visual representations for transfer learning by suppress-ing texture,2020, arXiv preprint arXiv:2011
 Intriguing properties of vision transformers,2021, arXiv preprintarXiv:2105
 Confident learning: Estimating uncertainty in datasetlabels,2021, Journal of Artificial Intelligence Research
 Neural discrete representation learn-ing,2017, arXiv preprint arXiv:1711
 Vision transformers are robust learners,2021, arXiv preprintarXiv:2105
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 On the adversarial robust-ness of visual transformers,2021, arXiv preprint arXiv:2103
 Video Google: a text retrieval approach to object matching in videos,2003, InCVPR
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention,2021, InICML
 Attention is all you need,2017, In NeurIPS
 Composingtext and image for image retrieval-an empirical odyssey,2019, In CVPR
 Learning robust global representa-tions by penalizing local predictive power,2019, In NeurIPS
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Robust and generalizablevisual representation learning via random convolutions,2020, arXiv preprint arXiv:2007
 Robust learning through cross-task consistency,2020, In CVPR
 mixup: Beyond empiri-cal risk minimization,2018, In ICLR
