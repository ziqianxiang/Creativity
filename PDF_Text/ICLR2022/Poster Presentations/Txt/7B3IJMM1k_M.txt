Published as a conference paper at ICLR 2022
Optimal ANN-SNN Conversion for High-
accuracy and Ultra-low-latency Spiking
Neural Networks
Tong Bu1 , Wei Fang1 , Jianhao Ding1 , PengLin Dai2, Zhaofei Yu1 *, Tiejun Huang1
1 Peking University, 2 Southwest Jiaotong University
* Corresponding author: yuzf12@pku.edu.cn
Ab stract
Spiking Neural Networks (SNNs) have gained great attraction due to their dis-
tinctive properties of low power consumption and fast inference on neuromorphic
hardware. As the most effective method to get deep SNNs, ANN-SNN conversion
has achieved comparable performance as ANNs on large-scale datasets. Despite
this, it requires long time-steps to match the firing rates of SNNs to the activation
of ANNs. As a result, the converted SNN suffers severe performance degradation
problems with short time-steps, which hamper the practical application of SNNs.
In this paper, we theoretically analyze ANN-SNN conversion error and derive the
estimated activation function of SNNs. Then we propose the quantization clip-
floor-shift activation function to replace the ReLU activation function in source
ANNs, which can better approximate the activation function of SNNs. We prove
that the expected conversion error between SNNs and ANNs is zero, enabling us
to achieve high-accuracy and ultra-low-latency SNNs. We evaluate our method
on CIFAR-10/100 and ImageNet datasets, and show that it outperforms the state-
of-the-art ANN-SNN and directly trained SNNs in both accuracy and time-steps.
To the best of our knowledge, this is the first time to explore high-performance
ANN-SNN conversion with ultra-low latency (4 time-steps). Code is available at
https://github.com/putshua/SNN_conversion_QCFS
1	Introduction
Spiking neural networks (SNNs) are biologically plausible neural networks based on the dynamic
characteristic of biological neurons (McCulloch & Pitts, 1943; Izhikevich, 2003). As the third gen-
eration of artificial neural networks (Maass, 1997), SNNs have attracted great attention due to their
distinctive properties over deep analog neural networks (ANNs) (Roy et al., 2019). Each neuron
transmits discrete spikes to convey information when exceeding a threshold. For most SNNs, the
spiking neurons will accumulate the current of the last layer as the output within T inference time
steps. The binarized activation has rendered dedicated hardware of neuromorphic computing (Pei
et al., 2019; DeBole et al., 2019; Davies et al., 2018). This kind of hardware has excellent advantages
in temporal resolution and energy budget. Existing work has shown the potential of tremendous en-
ergy saving with considerably fast inference (Stockl & Maass, 2021).
In addition to efficiency advantages, the learning algorithm of SNNs has been improved by leaps
and bounds in recent years. The performance of SNNs trained by backpropagation through time
and ANN-SNN conversion techniques has gradually been comparable to ANNs on large-scale
datasets (Fang et al., 2021; Rueckauer et al., 2017). Both techniques benefit from the setting of
SNN inference time. Setting longer time-steps in backpropagation can make the gradient of surro-
gate functions more reliable (Wu et al., 2018; Neftci et al., 2019; Zenke & Vogels, 2021). However,
the price is enormous resource consumption during training. Existing platforms such as TensorFlow
and PyTorch based on CUDA have limited optimization for SNN training. In contrast, ANN-SNN
conversion usually depends on a longer inference time to get comparable accuracy as the original
ANN (Sengupta et al., 2019) because itis based on the equivalence of ReLU activation and integrate-
and-fire model’s firing rate (Cao et al., 2015). Although longer inference time can further reduce the
conversion error, it also hampers the practical application of SNNs on neuromorphic chips.
1
Published as a conference paper at ICLR 2022
The dilemma of ANN-SNN conversion is that there exists a remaining potential in the conversion
theory, which is hard to be eliminated in a few time steps (Rueckauer et al., 2016). Although
many methods have been proposed to improve the conversion accuracy, such as weight normaliza-
tion (Diehl et al., 2015), threshold rescaling (Sengupta et al., 2019), soft-reset (Han & Roy, 2020)
and threshold shift (Deng & Gu, 2020), tens to hundreds of time-steps in the baseline works are still
unbearable. To obtain high-performance SNNs with ultra-low latency (e.g., 4 time-steps), we list the
critical errors in ANN-SNN conversion and provide solutions for each error. Our main contributions
are summarized as follows:
•	We go deeper into the errors in the ANN-SNN conversion and ascribe them to clipping
error, quantization error, and unevenness error. We find that unevenness error, which is
caused by the changes in the timing of arrival spikes and has been neglected in previous
works, can induce more spikes or fewer spikes as expected.
•	We propose the quantization clip-floor-shift activation function to replace the ReLU activa-
tion function in source ANNs, which better approximates the activation function of SNNs.
We prove that the expected conversion error between SNNs and ANNs is zero, indicating
that we can achieve high-performance converted SNN at ultra-low time-steps.
•	We evaluate our method on CIFAR-10, CIFAR-100, and ImageNet datasets. Compared
with both ANN-SNN conversion and backpropagation training methods, the proposed
method exceeds state-of-the-art accuracy with fewer time-steps. For example, we reach
top-1 accuracy 91.18% on CIFAR-10 with unprecedented 2 time-steps.
2	preliminaries
In this section, we first briefly review the neuron models for SNNs and ANNs. Then we introduce
the basic framework for ANN-SNN conversion.
Neuron model for ANNs. For ANNs, the computations of analog neurons can be simplified as the
combination of a linear transformation and a non-linear mapping:
al = h(Wlal-1), l= 1,2,...,M	(1)
where the vector al denotes the output of all neurons in l-th layer, Wl denotes the weight matrix
between layer l and layer l - 1, and h(∙) is the ReLU activation function.
Neuron model for SNNs. Similar to the previous works (Cao et al., 2015; Diehl et al., 2015; Han
et al., 2020), we consider the Integrate-and-Fire (IF) model for SNNs. If the IF neurons in l-th layer
receive the input xl-1(t) from last layer, the temporal potential of the IF neurons can be defined as:
ml(t) =vl(t- 1)+Wlxl-1(t),	(2)
where ml (t) and vl (t) represent the membrane potential before and after the trigger of a spike at
time-step t. W l denote the weight in l-th layer. As soon as any element mli (t) of ml (t) exceeds
the firing threshold θl, the neuron will elicit a spike and update the membrane potential vil (t). To
avoid information loss, we use the “reset-by-subtraction” mechanism (Rueckauer et al., 2017; Han
et al., 2020) instead of the “reset-to-zero” mechanism, which means the membrane potential vil (t)
is subtracted by the threshold value θl if the neuron fires. Based on the threshold-triggered firing
mechanism and the “reset-by-subtraction” of the membrane potential after firing discussed above,
we can write the uplate rule of membrane potential as:
sl(t) = H(ml(t) - θl),	(3)
vl (t) = ml (t) - sl(t)θl.	(4)
Here sl (t) refers to the output spikes of all neurons in layer l at time t, the element of which equals
1 if there is a spike and 0 otherwise. H(∙) is the Heaviside step function. θl is the vector of the
firing threshold θl. Similar to Deng & Gu (2020), we suppose that the postsynaptic neuron in l-th
layer receives unweighted postsynaptic potential θl if the presynaptic neuron in l - 1-th layer fires
a spike, that is:
xl(t) = sl(t)θl.	(5)
2
Published as a conference paper at ICLR 2022
Table 1: Summary of notations in this paper
Symbol	Definition	Symbol	Definition
l	Layer index	小~	Unweighted PSP1
i	Neuron index	sl(t)	Output spikes
Wl	Weight	Φl(T)	Average unweigthed PSP before time T
al	ANN activation values	zl	Weighted input from l - 1 layer
t	Time-steps	h(∙)	ReLU function
T	Total time-step	H(∙)	Heaviside step function
θl	Threshold	L	Quantization step for ANN
λl	Trainable threshold in ANN	Errl	Conversion Error
ml(t)	Potential before firing	l Err	Estimated conversion Error
Vl(t)	Potential after firing	炉	Shift of quantization clip-floor function
1 Postsynaptic potential
ANN-SNN conversion. The key idea of ANN-SNN conversion is to map the activation value of an
analog neuron in ANN to the firing rate (or average postsynaptic potential) of a spiking neuron in
SNN. Specifically, We can get the potential update equation by combining Equation 2 - Equation 4:
vl(t) -vl(t- 1) = Wlxl-1(t) - sl(t)θl.	(6)
Equation 6 describes the basic function of spiking neurons used in ANN-SNN conversion. By
summing Equation 6 from time 1 to T and dividing T on both sides, We have:
Vl(T) - Vl(0)
T
wl PW χl-1(i) PL sl (i)θl
----------------------------------
(7)
T
T
If we use φl-1(T) = Pi=1 T——(i) to denote the average PostsynaPtiC potential during the period
from 0 to T and substitute Equation 5 into Equation 7, then We get:
φl(T) = Wlφl-1(T) - VI(T) - VI(O).	(8)
Equation 8 describes the relationship of the average postsynaptic potential of neurons in adjacent
layers. Note that φl (T) > 0. If we set the initial potential Vl (0) to zero and neglect the remaining
term VTT) when the simulation time-steps T is long enough, the converted SNN has nearly the
same activation function as source ANN (Equation 1). However, high T would cause long inference
latency that hampers the practical application of SNNs. Therefore, this paper aims to implement
high-performance ANN-SNN conversion with extremely low latency.
3 conversion error analysis
In this section, we will analyze the conversion error between the source ANN and the converted
SNN in each layer in detail. In the following, we assume that both ANN and SNN receive the same
input from the layer l - 1, that is, al-1 = φl-1 (T), and then analyze the error in layer l. For
simplicity, we use zl = Wlφl-1 (T) = Wlal-1 to substitute the weighted input from layer l - 1
for both ANN and SNN. The absolute conversion error is exactly the outputs from converted SNN
subtract the outputs from ANN:
Errl = φl(T) - al = zl - Vl(T) - vl(0) - h(zl),	(9)
where h(zl) = ReLU(z l). It can be found from Equation 9 that the conversion error is nonzero if
Vl (T) - Vl (0) 6= 0 and zl > 0. In fact, the conversion error is caused by three factors.
Clipping error. The output φl(T) of SNNs is in the range of [0, θl] as φl(T) = Pi=Tx ⑺=
Pi=TS (i) θl (see Equation 5). However, the output al of ANNs is in a much lager range of [0, alrnaχ],
where almax denotes the maximum value of al. As illustrated in Figure 1a, al can be mapped to
φl (T) by the following equation:
Φl(T )= clip (θT aλT , 0,θl).	(10)
3
Published as a conference paper at ICLR 2022
1⅛ιe-3tq>	15me∙step
(b) Even spikes	(c) More spikes
-clipped—►
(a) Clipping error
Figure 1: Conversion error between source ANN and converted SNN. sl1-1 and sl2-1 denote the
output spikes of two neurons in layer l - 1, and sl1 denotes the output spikes of a neuron in layer l.
Time-step
(d) Fewer spikes
Here the clip function sets the upper bound θl and the lower bound 0. [∙] denotes the floor function.
λl represents the actual maximum value of output al mapped to the maximum value θl of φl(T).
Considering that nearly 99.9% activations of al in ANN are in the range of [0, am3ax], Rueckauer
et al. (2016) suggested to choose λl according to 99.9% activations. The activations between λl and
almax in ANN are mapped to the same value θl in SNN, which will cause conversion error called
clipping error.
Quantization error (flooring error). The output spikes sl (t) are discrete events, thus φl (T) are
discrete with quantization resolution θ (see Equation 10). When mapping al to φl (T), there exists
unavoidable quantization error. For example, as illustrated in Figure 1a, the activations of ANN in
the range of [λl, 2Tl) are mapped to the same value Tl of SNN.
Unevenness error. Unevenness error is caused by the unevenness of input spikes. If the timing of
arrival spikes changes, the output firing rates may change, which causes conversion error. There are
two situations: more spikes as expected or fewer spikes as expected. To see this, in source ANN,
we suppose that two analog neurons in layer l - 1 are connected to an analog neuron in layer l
with weights 2 and -2, and the output vector al-1 of neurons in layer l - 1 is [0.6, 0.4]. Besides, in
converted SNN, we suppose that the two spiking neurons in layer l - 1 fire 3 spikes and 2 spikes in
5 time-steps (T=5), respectively, and the threshold θl-1 = 1. Thus, φl-1(T) = Pi=IT__(i)θl-1 =
[0.6, 0.4]. Even though φl-1 (T) = al-1 and the weights are same for the ANN and SNN, φl (T)
can be different from al if the timing of arrival spikes changes. According to Equation 1, the ANN
output al = Wlal-1 = [2, -2][0.6, 0.4]T = 0.4. As for SNN, supposing that the threshold θl = 1,
there are three possible output firing rates, which are illustrated in Figure 1 (b)-(d). If the two
presynaptic neurons fires at t = 1, 3, 5 and t = 2, 4 (red bars) respectively with weights 2 and -2, the
postsynaptic neuron will fire two spikes at t = 1, 3 (red bars), and φl (T) = Pi=TS ⑴ θl = 0.4 = al.
However, if the presynaptic neurons fires at t = 1, 2, 3 and t = 4, 5, respectively, the postsynaptic
neuron will fire four spikes at t = 1, 2, 3, 4, and φl (T) = 0.8 > al. If the presynaptic neurons fires
at t = 3, 4, 5 and t = 1, 2, respectively, the postsynaptic neuron will fire only one spikes at t = 5,
and φl (T) = 0.2 < al.
Note that the clipping error and quantization error have been proposed in Li et al. (2021). There exist
interdependence between the above three kinds of errors. Specifically, the unevenness error will
degenerate to the quantization error if vl (T) is in the range of [0, θl]. Assuming that the potential
vl (T) falls into [0, θl] will enable us to estimate the activation function of SNNs ignoring the effect
of unevenness error. Therefore, an estimation of the output value φl (T) in a converted SNN can be
formulated with the combination of clip function and floor function, that is:
Φl(T) ≈ θl clip G ZlT +lvl(0)
The detailed derivation is in the Appendix. With the help of this estimation for the SNN output, the
estimated conversion error Egrrl can be derived from Equation 9:
Egrrl = θl clip (1 ZlT +vl(0) , 0,1)- h(zl) ≈ ErrL	(12)
T	θl
,0, 1 .	(11)
4
Published as a conference paper at ICLR 2022
0.12W1
-α125ef
■ - zi
-o1i2se,
0.12591
-0.1255«
=∖ɑ5
(a) L = T = 4
zi
(b) L = 4, T = 8
0,25Λl QW	0.7W	λl
(C) L = 4,T = 8, P = 0.5
Figure 2: Comparison of SNN output φl (T ) and ANN output al with same input zl
4	Optimal ANN-SNN conversion
4.1	quantization clip-floor activation function
ACCording to the Conversion error of Equation 12, it is natural to think that if the Commonly used
ReLU aCtivation funCtion h(zl) is substituted by a Clip-floor funCtion with a given quantization
steps L (similar to Equation 11), the Conversion error at time-steps T = L will be eliminated. Thus
the performanCe degradation problem at low latenCy will be solved. As shown in Equation 13, we
proposed the quantization Clip-floor aCtivation funCtion to train ANNs.
al = h(zl) = λl clip
(13)
where the hyperparameter L denotes quantization steps of ANNs, the trainable λl deCides the
maximum value of al in ANNs mapped to the maximum of φl (T ) in SNNs. Note that zl =
Wlφl-1 (T) = Wlal-1. With this new aCtivation funCtion, we Can prove that the estimated Con-
version error between SNNs and ANNs is zero, and we have the following Theorem.
Theorem 1. An ANN with activation function (13) is converted to an SNN with the same weights. If
T = L, θl = λl, and vl(0) = 0, then:
l
Err = φl(T) - al = 0.
(14)
Proof. ACCording to Equation 12, and the Conditions T = L, θl = λl, vl (0) = 0, we have Egrr =
Φl(T) - al = θl clip (T jZT+vl(0) k , 0,1)- λl clip (L j⅛l] , 0,1)= 0.	□
Theorem 1 implies that if the time-steps T of the Converted SNN is the same as the quantization
steps L of the sourCe ANN, the Conversion error will be zero. An example is illustrated in Figure 2a,
where T = L = 4, θl = λl . The red Curve presents the estimated output φl (T) of the Converted
SNNs with respeCtive to different input zl, while the green Curve represents the out al of the sourCe
ANN with respeCtive to different input zl . As the two Curve are the same, the estimated Conversion
error Egrr is zero. Nevertheless, in praCtiCal appliCation, we foCus on the performanCe of SNNs
at different time-steps. There is no guarantee that the Conversion error is zero when T is not equal
to L. As illustrated in Figure 2b, where L = 4 and L = 8, we Can find the Conversion error is
greater than zero for some zl . This error will transmit layer-by-layer and eventually degrading the
aCCuraCy of the Converted SNN. One way to solve this problem is to train multiple sourCe ANNs
with different quantization steps, then Convert them to SNNs with different time-steps, but it Comes
at a Considerable Cost. In the next seCtion, we propose the quantization Clip-floor aCtivation funCtion
with a shift term to solve this problem. SuCh an approaCh Can aChieve high aCCuraCy for different
time-steps, without extra Computation Cost.
4.2	quantization clip-floor-shift activation function
We propose the quantization Clip-floor-shift aCtivation funCtion to train ANNs.
al = b(zl) = λl clip (J
zlL
, 0, 1
(15)
K +中
5
Published as a conference paper at ICLR 2022
Compared with Equation 13, there exists a hyperparameter vector φ that controls the shift of the
activation function. When L 6= T , we cannot guarantee the conversion error is 0. However, we
can estimate the expectation of conversion error. Similar to (Deng & Gu, 2020), we assume that
Zi is uniformly distributed within intervals [(t - 1)λl∕T, (t)λl/T] and [(l - 1)λl/L, (l)λl∕L] for
t = 1, 2, ..., T and L = 1, 2, ..., L, we have the following Theorem.
Theorem 2. An ANN with activation function (15) is converted to an SNN with the same weights.
If θl = λl, vl(0) = θlφ, then for arbitrary T and L, the expectation of conversion error reaches 0
when the shift term φ in source ANN is 1.
∀ T,L	Ez (Err ) ∣ I= 0.	(16)
The proof is in the Appendix. Theorem 2 indicates that the shift term 1 is able to optimize the
expectation of conversion error. By comparing Figure 2b and Figure 2c, we can find that when the
shift term φ = 0.5 is added, the mean conversion error reaches zero, even though L = T. These
results indicate we can achieve high-performance converted SNN at ultra-low time-steps.
L is the only undetermined hyperparameter of the quantization clip-floor-shift activation. When
T = L, the conversion error reaches zero. So we naturally think that the parameter L should be set
as small as possible to get better performance at low time-steps. However, a too low quantization
of the activation function will decrease the model capacity and further lead to accuracy loss when
the time-steps is relatively large. Choosing the proper L is a trade-off between the accuracy at low
latency and the best accuracy of SNNs. We will further analyze the effects of quantization steps L
in the experiment section.
4.3	algorithm for training quantization clip-floor-shift activation function
Training an ANN with quantization clip-floor-shift activation instead of ReLU is also a tough prob-
lem. To direct train the ANN, we use the straight-through estimator (Bengio et al., 2013) for the
derivative of the floor function, that is dbχc = 1. The overall derivation rule is given in Equation 17.
dhi(ZI) =(1, if - 2λl < Z < λl - 2λl dbi(ZI)	= ( 21l , iif -金 < zi <λ -金
∂zi	(0, otherwise	，	∂λl	I-(z)2, otherwise
(17)
Here zil is the i-th element of zl . Then we can train the ANN with quantization clip-floor-shift
activation using Stochastic Gradient Descent algorithm (Bottou, 2012).
5	Related Work
The study of ANN-SNN conversion is first launched by Cao et al. (2015). Then Diehl et al. (2015)
converted a three-layer CNN to an SNN using data-based and model-based normalization. To ob-
tain high-performance SNNs for complex datasets and deeper networks, Rueckauer et al. (2016)
and Sengupta et al. (2019) proposed more accurate scaling methods to normalize weights and scale
thresholds respectively, which were later proved to be equivalent (Ding et al., 2021). Nevertheless,
the converted deep SNN requires hundreds of time steps to get accurate results due to the conver-
sion error analyzed in Sec. 3. To address the potential information loss, Rueckauer et al. (2016)
and Han et al. (2020) suggested using “reset-by-subtraction” neurons rather than “reset-to-zero”
neurons. Recently, many methods have been proposed to eliminate the conversion error. Rueck-
auer et al. (2016) recommended 99.9% percentile of activations as scale factors, and Ho & Chang
(2020) added the trainable clipping layer. Besides, Han et al. (2020) rescaled the SNN thresholds
to avoid the improper activation of spiking neurons. Massa et al. (2020) and Singh et al. (2021)
evaluated the performance of converted SNNs on the Loihi Neuromorphic Processor. Our work
share similarity with Deng & Gu (2020); Li et al. (2021), which also shed light on the conversion
error. Deng & Gu (2020) minimized the layer-wise error by introducing extra bias in addition to
the converted SNN biases. Li et al. (2021) further proposed calibration for weights and biases using
quantized fine-tuning. They got good results with 16 and 32 time-steps without trails for more ex-
treme time-steps. In comparison, our work aims to fit ANN into SNN with techniques eliminating
6
Published as a conference paper at ICLR 2022
→- Fna
—MlUS
QuanfcatiOn step L
(d) ResNet-20 on CIFAR-100
Quantization Btq> L
(a)	VGG-16 on CIFAR-IO
Quantization step L	Quantiaition step L
(b)	ReSNet-20 on CIFAR-IO	(c) VGG-16 on CIFAR-100
Figure 3: Compare ANNs accuracy.
the mentioned conversion error. The end-to-end training of quantization layers is implemented to
get better overall performance. Our shift correction can lead to a single SNN which performs well at
both ultra-low and large time-steps. Maintaining SNN performance within extremely few time-steps
is difficult even for supervised learning methods like backpropagation through time (BPTT). BPTT
usually requires fewer time-steps because of thorough training, yet at the cost of heavy GPU compu-
tation (Wu et al., 2018; 2019; Lee et al., 2016; Neftci et al., 2019; Lee et al., 2020; Zenke & Vogels,
2021). The timing-based backpropagation methods (Bohte et al., 2002; Tavanaei et al., 2019; Kim
et al., 2020) could train SNNs over a very short temporal window, e.g. over 5-10 time-steps. How-
ever, they are usually limited to simple datasets like MNIST (Kheradpisheh & Masquelier, 2020)
and CIFAR10 (Zhang & Li, 2020). Rathi et al. (2019) shortened simulation steps by initializing
SNN with conversion method and then tuning SNN with STDP. In this paper, the proposed method
achieves high-performance SNNs with ultra-low latency (4 time-steps).
6	Experiments
In this section, we validate the effectiveness of our method and compare our method with other
state-of-the-art approaches for image classification tasks on CIFAR-10 (LeCun et al., 1998), CIFAR-
100 (Krizhevsky et al., 2009), and ImageNet datasets (Deng et al., 2009). Similar to previous works,
we utilize VGG-16 (Simonyan & Zisserman, 2014), ResNet-18 (He et al., 2016), and ResNet-20
network structures for source ANNs. We compare our method with the state-of-the-art ANN-SNN
conversion methods, including Hybrid-Conversion (HC) from Rathi et al. (2019), RMP from Han
et al. (2020), TSC from Han & Roy (2020), RNL from Ding et al. (2021), ReLUThresholdShift
(RTS) from Deng & Gu (2020), and SNN Conversion with Advanced Pipeline (SNNC-AP) from Li
et al. (2021). Comparison with different SNN training methods is also included to manifest the su-
periority of low latency inference, including HybridConversion-STDB (HC-STDB) from Rathi et al.
(2019), STBP from Wu et al. (2018), DirectTraining (DT) from Wu et al. (2019), and TSSL from
Zhang & Li (2020). The details of the proposed ANN-SNN algorithm and training configurations
are provided in the Appendix.
6.1	Test accuracy of ANN with quantization clip-floor-shift activation
We first compare the performance of ANNs with quantization clip-floor activation (green curve),
ANNs with quantization clip-floor-shift activation (blue curve), and original ANNs with ReLU acti-
vation (black dotted line). Figure 3(a)-(d) report the results about VGG-16 on CIFAR-10, ResNet-20
on CIFAR-10, VGG-16 on CIFAR-100 and ResNet-20 on CIFAR-100. The performance of ANNs
with quantization clip-floor-shift activation is better than ANNs with quantization clip-floor activa-
tion. These two ANNs can achieve the same performance as original ANNs with ReLU activation
when L > 4. These results demonstrate that our quantization clip-floor-shift activation function
hardly affects the performance of ANN.
6.2	Comparison with the state-of-the-art
Table 2 compares our method with the state-of-the-art ANN-SNN conversion methods on CIFAR-
10. As for low latency inference (T ≤ 64), our model outperforms all the other methods with the
same time-step setting. For T = 32, the accuracy of our method is slightly better than that of ANN
(95.54% vs. 95.52%), whereas RMP, RTS, RNL, and SNNC-AP methods have accuracy loss of
33.3%, 19.48%, 7.42%, and 2.01%. Moreover, we achieve an accuracy of 93.96% using only 4
time-steps, which is 8 times faster than SNNC-AP that takes 32 time-steps. For ResNet-20, we
achieve an accuracy of 83.75% with 4 time-steps. Notably, our ultra-low latency performance is
comparable with other state-of-the-art supervised training methods, which is shown in Table S3 of
the Appendix.
7
Published as a conference paper at ICLR 2022
957555”
QaaC
X3um8v
—w∕βbifi
wA>sħiβ
X3nm8v
Simulation time-steps
(b) ResNet-20 on CIEAR-IO
X3nm8v
Simulation tinɪ^teps
(c) VGG4 6 OD CIFAr-IOO
X3nm8v
Simulation time-Stq)S
(d) ResNet-20 on CIFAR-100
Simulation time-steps
(a) VGG-16onCIEAR-10
Figure 4: Compare quantization clip-floor activation with/without shift term
Table 2: Comparison between the proposed method and previous works on CIFAR-10 dataset.
Architecture	Method	ANN	T=2	T=4	T=8	T=16	T=32	T=64	T≥512
	RMP	93.63%	-	-	-	-	60.30%	90.35%	93.63%
	TSC	93.63%	-	-	-	-	-	92.79%	93.63%
VGG-16	RTS	95.72%	-	-	-	-	76.24%	90.64%	95.73%
	RNL	92.82%	-	-	-	57.90%	85.40%	91.15%	92.95%
	SNNC-AP	95.72%	-	-	-	-	93.71%	95.14%	95.79%
	Ours	95.52%	91.18%	93.96%	94.95%	95.40%	95.54%	95.55%	95.59%
	RMP	91.47%	-	-	-	-	-	-	91.36%
ResNet-20	TSC	91.47%	-	-	-	-	-	69.38%	91.42%
	Ours	91.77%	73.20%	83.75%	89.55%	91.62%	92.24%	92.35%	92.41%
	RTS 1	95.46%	-	-	-	-	84.06%	92.48%	94.42%
ResNet-18	SNNC-AP 1	95.46%	-	-	-	-	94.78%	95.30%	95.45%
	Ours	96.04%	75.44%	90.43%	94.82%	95.92%	96.08%	96.06%	96.06%
1 RTS and SNNC-AP use altered ResNet-18, while ours use standard ResNet-18.
We further test the performance of our method on the large-scale dataset. Table 3 reports the results
on ImageNet, our method also outperforms the others both in terms of high accuracy and ultra-low
latency. For ResNet-34, the accuracy of the proposed method is 4.83% higher than SNNC-AP and
69.28% higher than RTS when T = 32. When the time-steps is 16, we can still achieve an accuracy
of 59.35%. For VGG-16, the accuracy of the proposed method is 4.83% higher than SNNC-AP
and 68.356% higher than RTS when T = 32. When the time-steps is 16, we can still achieve an
accuracy of 50.97%. These results demonstrate that our method outperforms the previous conversion
methods. More experimental results on CIFAR-100 is in Table S4 of the Appendix.
6.3	Comparison of quantization clip-floor and quantization clip-floor-shift
Here we further compare the performance of SNNs converted from ANNs with quantization clip-
floor activation and ANN with quantization clip-floor-shift activation. In Sec. 4, we prove that
the expectation of the conversion error reaches 0 with quantization clip-floor-shift activation, no
matter whether T and L are the same or not. To verify these, we set L to 4 and train ANNs with
quantization clip-floor activation and quantization clip-floor-shift activation, respectively. Figure 4
shows how the accuracy of converted SNNs changes with respect to the time-steps T . The accuracy
of the converted SNN (green curve) from ANN with quantization clip-floor activation (green dotted
line) first increases and then decreases rapidly with the increase of time-steps, because we cannot
guarantee that the conversion error is zero when T is not equal to L. The best performance is still
lower than source ANN (green dotted line). In contrast, the accuracy of the converted SNN from
ANN with quantization clip-floor-shift activation (blue curve) increases with the increase of T . It
gets the same accuracy as source ANN (blue dotted line) when the time-steps is larger than 16.
6.4	Effect of quantization steps L
In our method, the quantization steps L is a hyperparameter, which affects the accuracy of the con-
verted SNN. To analyze the effect of L and better determine the optimal value, we train VGG-
16/ResNet-20 networks with quantization clip-floor-shift activation using different quantization
steps L, including 2,4,8,16 and 32, and then converted them to SNNs. The experimental results
on CIFAR-10/100 dataset are shown in Table S2 and Figure 5, where the black dotted line denotes
the ANN accuracy and the colored curves represent the accuracy of the converted SNN. In order to
8
Published as a conference paper at ICLR 2022
0.9$0.750.550.35
X3nm8v
Simulation time-steps
(a) VGG-16onCIEAR-10
X3nm8v
Simulation time-steps
(b) ResNet-20 oα CIFAR-IO
X3um8v
0.15
1	2	4	8	16	32	64 1
Simulation tinɪ^teps
(c) VGG-16 on CIFAR-100
Figure 5: Influence of different quantization steps
X3nm8v
Simulation time-Stq)S
(d) ResNet-20 on CIFAR-100
Table 3: Comparison between the proposed method and previous works on ImageNet dataset.
Architecture	Method	ANN	T=16	T=32	T=64	T=128	T=256	T≥1024
	RMP	70.64%	-	-	-	-	-	65.47%
	TSC	70.64%	-	-	-	-	61.48%	65.10%
ResNet-34	RTS	75.66%	-	0.09%	0.12%	3.19%	47.11%	75.08%
	SNNC-AP	75.66%	-	64.54%	71.12%	73.45%	74.61%	75.45%
	Ours	74.32%	59.35%	69.37%	72.35%	73.15%	73.37%	73.39%
	RMP	73.49%	-	-	-	-	48.32%	73.09%
	TSC	73.49%	-	-	-	-	69.71%	73.46%
VGG-16	RTS	75.36%	-	0.114%	0.118%	0.122%	1.81%	73.88%
	SNNC-AP	75.36%	-	63.64%	70.69%	73.32%	74.23%	75.32%
	Ours	74.29%	50.97%	68.47%	72.85%	73.97%	74.22%	74.32%
balance the trade-off between low latency and high accuracy, we evaluate the performance of con-
verted SNN mainly in two aspects. First, we focus on the SNN accuracy at ultra-low latency (within
4 time-steps). Second, we consider the best accuracy of SNN. It is obvious to find that the SNN
accuracy at ultra-low latency decreases as L increases. However, a too small L will decrease the
model capacity and further lead to accuracy loss. When L = 2, there exists a clear gap between the
best accuracy of SNN and source ANN. The best accuracy of SNN approaches source ANN when
L > 4. In conclusion, the setting of parameter L mainly depends on the aims for low latency or best
accuracy. The recommend quantization step L is 4 or 8, which leads to high-performance converted
SNN at both small time-steps and very large time-steps.
7 Discussion and conclusion
In this paper, we present ANN-SNN conversion method, enabling high-accuracy and ultra-low-
latency deep SNNs. We propose the quantization clip-floor-shift activation to replace ReLU activa-
tion, which hardly affects the performance of ANNs and is closer to SNNs activation. Furthermore,
we prove that the expected conversion error is zero, no matter whether the time-steps of SNNs and
the quantization steps of ANNs is the same or not. We achieve state-of-the-art accuracy with fewer
time-steps on CIFAR-10, CIFAR-100, and ImageNet datasets. Our results can benefit the imple-
mentations on neuromorphic hardware and pave the way for the large-scale application of SNNs.
Different from the work of Deng & Gu (2020), which adds the bias of the converted SNNs to shift
the theoretical ANN-SNN curve to minimize the quantization error, we add the shift term in the
quantization clip-floor activation function, and use this quantization clip-floor-shift function to train
the source ANN. We show that the shift term can overcome the performance degradation problem
when the time-steps and the quantization steps are not matched. Due to the unevenness error, there
still exists a gap between ANN accuracy and SNN accuracy, even when L = T . Moreover, it is hard
to achieve high-performance ANN-SNN conversion when the time-steps T = 1. All these problems
deserve further research. One advantage of conversion-based methods is that they can reduce the
overall computing cost while maintaining comparable performance as source ANN. Combining the
conversion-based methods and model compression may help significantly reduce the neuron activity
and thus reduce energy consumptions without suffering from accuracy loss (Kundu et al., 2021;
Rathi & Roy, 2021), which is a promising direction.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work was supported by the National Natural Science Foundation of China under contracts
No.62176003 and No.62088102.
References
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Sander M Bohte, Joost N Kok, and Han La Poutre. Error-backpropagation in temporally encoded
networks of spiking neurons. NeurocomPuting, 48(1-4):17-37, 2002.
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421—
436. Springer, 2012.
Yongqiang Cao, Yang Chen, and Deepak Khosla. Spiking deep convolutional neural networks for
energy-efficient object recognition. International Journal of ComPuter Vision, 113(1):54-66,
2015.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In IEEE Conference on ComPuter Vision and Pattern
Recognition, pp. 113-123, 2019.
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha
Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic
manycore processor with on-chip learning. IEEE Micro, 38(1):82-99, 2018.
Michael V DeBole, Brian Taba, Arnon Amir, Filipp Akopyan, Alexander Andreopoulos, William P
Risk, Jeff Kusnitz, Carlos Ortega Otero, Tapan K Nayak, Rathinakumar Appuswamy, et al.
TrueNorth: Accelerating from zero to 64 million neurons in 10 years. ComPuter, 52(5):20-29,
2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on ComPuter Vision and Pattern Recognition,
pp. 248-255. Ieee, 2009.
Shikuang Deng and Shi Gu. Optimal conversion of conventional artificial neural networks to spiking
neural networks. In International Conference on Learning RePresentations, 2020.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv PrePrint arXiv:1708.04552, 2017.
Peter U Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer.
Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.
In International Joint Conference on Neural Networks, pp. 1-8, 2015.
Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang. Optimal ann-snn conversion for fast
and accurate inference in deep spiking neural networks. In International Joint Conference on
Artificial Intelligence, pp. 2328-2336, 2021.
Wei Fang, Zhaofei Yu, Yanqi Chen, TiejUn Huang, TimOthee Masquelier, and Yonghong Tian. Deep
residual learning in spiking neural networks. arXiv PrePrint arXiv:2102.04159, 2021.
Bing Han and Kaushik Roy. Deep spiking neural network: Energy efficiency through time based
coding. In EuroPean Conference on ComPuter Vision, pp. 388-404, 2020.
Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. RMP-SNN: Residual membrane poten-
tial neuron for enabling deeper high-accuracy and low-latency spiking neural network. In IEEE
Conference on ComPuter Vision and Pattern Recognition, pp. 13558-13567, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE conference on ComPuter Vision and Pattern Recognition, pp. 770-778, 2016.
10
Published as a conference paper at ICLR 2022
Nguyen-Dong Ho and Ik-Joon Chang. Tcl: an ann-to-snn conversion with trainable clipping layers.
arXiv preprint arXiv:2008.04509, 2020.
Eugene M Izhikevich. Simple model of spiking neurons. IEEE Transactions on neural networks,
14(6):1569-1572, 2003.
Saeed Reza KheradPisheh and Timothee Masquelier. Temporal backpropagation for spiking neural
networks with one spike per neuron. International Journal of Neural Systems, 30(06):2050027,
2020.
Jinseok Kim, Kyungsu Kim, and Jae-Joon Kim. Unifying activation- and timing-based learning
rules for spiking neural networks. In Advances in Neural Information Processing Systems, pp.
19534-19544, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Souvik Kundu, Gourav Datta, Massoud Pedram, and Peter A Beerel. Spike-thrift: Towards energy-
efficient deep spiking neural networks by limiting spiking activity via attention-guided compres-
sion. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
(WACV), pp. 3953-3962, 2021.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Chankyu Lee, Syed Shakib Sarwar, Priyadarshini Panda, Gopalakrishnan Srinivasan, and Kaushik
Roy. Enabling spike-based backpropagation for training deep neural network architectures. Fron-
tiers in Neuroscience, 14, 2020.
Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks using
backpropagation. Frontiers in Neuroscience, 10:508, 2016.
Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. A free lunch from ann: Towards
efficient, accurate spiking neural networks calibration. In International Conference on Machine
Learning, pp. 6316-6325, 2021.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In Interna-
tional Conference on Learning Representations, 2016.
Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models.
Neural Networks, 10(9):1659-1671, 1997.
Riccardo Massa, Alberto Marchisio, Maurizio Martina, and Muhammad Shafique. An efficient
spiking neural network for recognizing gestures with a DVS camera on the Loihi neuromorphic
processor. In International Joint Conference on Neural Networks, pp. 1-9, 2020.
Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.
The Bulletin of Mathematical Biophysics, 5(4):115-133, 1943.
Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-
neuron integrated circuit with a scalable communication network and interface. Science, 345
(6197):668-673, 2014.
Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking
neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
IEEE Signal Processing Magazine, 36(6):51-63, 2019.
Jing Pei, Lei Deng, Sen Song, Mingguo Zhao, Youhui Zhang, Shuang Wu, Guanrui Wang, Zhe
Zou, Zhenzhi Wu, Wei He, et al. Towards artificial general intelligence with hybrid tianjic chip
architecture. Nature, 572(7767):106-111, 2019.
Ning Qiao, Hesham Mostafa, Federico Corradi, Marc Osswald, Fabio Stefanini, Dora Sumislawska,
and Giacomo Indiveri. A reconfigurable on-line learning spiking neuromorphic processor com-
prising 256 neurons and 128K synapses. Frontiers in neuroscience, 9:141, 2015.
11
Published as a conference paper at ICLR 2022
Nitin Rathi and Kaushik Roy. Diet-snn: A low-latency spiking neural network with direct input
encoding and leakage and threshold optimization. IEEE Transactions on Neural Networks and
Learning Systems, 2021.
Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy. Enabling deep
spiking neural networks with hybrid conversion and spike timing dependent backpropagation. In
International Conference on Learning Representations, 2019.
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence
with neuromorphic computing. Nature, 575(7784):607-617, 2019.
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, and Michael Pfeiffer. Theory and
tools for the conversion of analog to spiking convolutional neural networks. arXiv preprint
arXiv:1612.04052, 2016.
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Con-
version of continuous-valued deep networks to efficient event-driven networks for image classifi-
cation. Frontiers in Neuroscience, 11:682, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking
neural networks: VGG and residual architectures. Frontiers in Neuroscience, 13:95, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Sonali Singh, Anup Sarma, Sen Lu, Abhronil Sengupta, Vijaykrishnan Narayanan, and Chita R
Das. Gesture-snn: Co-optimizing accuracy, latency and energy of snns for neuromorphic vision
sensors. In IEEE/ACM International Symposium on Low Power Electronics and Design, pp. 1-6,
2021.
Christoph Stockl and Wolfgang Maass. Optimized spiking neurons can classify images with high
accuracy through temporal coding with two spikes. Nature Machine Intelligence, 3(3):230-238,
2021.
Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothee Masquelier, and
Anthony Maida. Deep learning in spiking neural networks. Neural Networks, 111:47-63, 2019.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for
training high-performance spiking neural networks. Frontiers in Neuroscience, 12:331, 2018.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking
neural networks: Faster, larger, better. In AAAI Conference on Artificial Intelligence, pp. 1311-
1318, 2019.
Friedemann Zenke and Tim P Vogels. The remarkable robustness of surrogate gradient learning
for instilling complex function in spiking neural networks. Neural Computation, 33(4):899-925,
2021.
Wenrui Zhang and Peng Li. Temporal spike sequence learning via backpropagation for deep spiking
neural networks. In Advances in Neural Information Processing Systems, pp. 12022-12033, 2020.
12
Published as a conference paper at ICLR 2022
A Appendix
A. 1 network structure and training configurations
Before training ANNs, we first replace max-pooling with average-pooling and then replace the
ReLU activation with the proposed quantization clip-floor-shift activation (Equation 15). After
training, we copy all weights from the source ANN to the converted SNN, and set the threshold
θl in each layer of the converted SNN equal to the maximum activation value λl of the source ANN
in the same layer. Besides, we set the initial membrane potential vl (0) in converted SNN as θl/2 to
match the optimal shift φ = 1 of quantization clip-floor-shift activation in the source ANN.
Despite the common data normalization, we use some data pre-processing techniques. For CIFAR
datasets, we resize the images into 32 × 32, and for ImageNet dataset, we resize the image into
224 × 224. Besides, we use random crop images, Cutout (DeVries & Taylor, 2017) and AutoAug-
ment (Cubuk et al., 2019) for all datasets.
We use the Stochastic Gradient Descent optimizer (Bottou, 2012) with a momentum parameter of
0.9. The initial learning rate is set to 0.1 for CIFAR-10 and ImageNet, and 0.02 for CIFAR-100. A
cosine decay scheduler (Loshchilov & Hutter, 2016) is used to adjust the learning rate. We apply a
5 × 10-4 weight decay for CIFAR datasets while applying a 1 × 10-4 weight decay for ImageNet.
We train all models for 300 epochs. The quantization steps L is set to 4 when training all the
networks on CIFAR-10, and VGG-16, ResNet-18 on CIFAR-100 dataset. When training ResNet-20
on CIFAR-100, the parameter L is set to 8. When training ResNet-34 and VGG-16 on ImageNet,
the parameter L is set to 8, 16, respectively. We use constant input when evaluating the converted
SNNs.
A.2 Introduction of Datasets
CIFAR-10. The CIFAR-10 dataset (Krizhevsky et al., 2009) consists of 60000 32 × 32 images in
10 classes. There are 50000 training images and 10000 test images.
CIFAR-100. The CIFAR-100 dataset (Krizhevsky et al., 2009) consists of 60000 32 × 32 images in
100 classes. There are 50000 training images and 10000 test images.
ImageNet. We use the ILSVRC 2012 dataset (Russakovsky et al., 2015), which consists 1,281,167
training images and 50000 testing images.
A.3 Derivation of Equation 12 and Proof of Theorem 2
Derivation of Equation 11
Similar to zl = Wlal-1 , We define
ul(t) = W lxl-1(t).	(S1)
We use uli (t) and zil to denote the i-th element in vector ul (t) and zl , respectively. To derive
Equation 11, some extra assumptions on the relationship between ANN activation value and SNN
postsynaptic potentials are needed, which are showed in Equation S2.
if zil < 0, then ∀t uli (t) < 0,
if 0 6 zil 6 θl , then ∀t 0 6 uli (t) 6 θl ,
if zil > θl , then ∀t uli (t) > θl .
(S2)
With the assumption above, we can discuss the firing behavior of the neurons in each time-step.
When zil < 0 or zil > θl, the neuron will never fire or fire all the time-steps, which means φli(T) = 0
or φli(T) = θl. In this situation, we can use a clip function to denote φli(T).
φli(T) = clip(zil, 0, θl).
(S3)
13
Published as a conference paper at ICLR 2022
When 0 < zil < θl, every input from the presynaptic neuron in SNNs falls into [0, θl], then we have
∀t, vil(t) ∈ [0, θ]. We can rewrite Equation 8 into the following equation.
Considering that
Φi(τ )t
θ^
φli(T)T	zilT + vil (0)
θl
θl
Vi(T)
~lr~.
(S4)
PT=I si(t) ∈ N and 0 < vi(T) < 1, Equation S4 is changed to:
φli(T)
θl zilT + vil (0)
θl
(S5)
T
—
We combine these two situations (Equation S3 and Equation S4), and we have:
φl(T) = θl clip
1 zlT + vl (0)
θl
,0,1 .
(S6)
T
Proof of Theorem 2
Before prove Theorem 2, we first introduce Lemma 1.
Lemma 1. If random variable x ∈ [0, θ] is uniformly distributed in every small interval [mt, mt+1]
一 .	...... ,	..	.	,∙ C 一	E、 .	C	八	(t- 1 )θ
with the probability density function Pt (t = 0,1,...,T), where mo = 0, mτ +ι = θ,mt =，T
for t = 1, 2, ..., T, P0 = PT, we can conclude that
θ
Ex Ix - T
TX 1
T + 2
0.
(S7)
Proof.
~θ^ + 2
θ TX 1
θ∕2T
P0
θ XT 1
~θ +2
dx
τ-1 f(2t+1)θ∕2T
+X θ
t=1 7(2t-1)θ∕2τ
Pt ιx - T
θ xT
dx
+Zθ
(2T -1)θ∕2T
Pt (X - T
θ XT 1
dx
X ——
T
x ——
T
1
ɪ + 2
方+ 2
θ∕2T	T-1	(2t+1)θ∕2T	tθ	θ
Po J	x dx + y^pt J	(x - T)dx + PTJ	(x - θ)dχ
θ2
θ2
p0 8T2 + 0 - pT 8T2
θ2
(PO - Pt )8T2 = 0.
(S8)
□
Theorem 2. An ANN with activation function (15) is converted to an SNN with the same weights.
If θl = λl, vl(0) = θlφ, then for arbitrary T and L, the expectation of conversion error reaches 0
when the Shif term φ in source ANN is 1.
∀T,L Ez
P= 2
0.
(S9)
Proof.
θl zlT + vl (0)
λl	zlL
P=2
θl
(S10)
T
—
L
ɪ+中
14
Published as a conference paper at ICLR 2022
Figure S1: More spikes than expected exists for the method of setting the maximum activation.
As every element in vector z is identical, we only need to consider one element.
λ
L
ZiT+V (0)
θl
ZilL
+----+ Ψi
λ
(θ∣ ZiT + vi(0)
IT L θ
ZilL
飞+ M
λ
(S11)
According to Lemma 1, we have
θl
(T
ZiT + vi(0)
θι
ZL + 中i ∖	=0.
-)0=1/2
- Zil	= 0,
vil(0)=1/2
(S12)
(S13)
Thus the sum of both terms also equals zero.
□
A.4 Comparison of the methods with or without dynamic threshold on the
CIFAR- 1 00 dataset
In this paper we use a training parameter λι to decide the maximum value of ANN activation.
The previous works suggested to set the maximum value of ANN activation after training as the
threshold. If we set θι = maxs∈{0,1}n max(θι-1W ιs) , the situation of fewer spikes as expected
never happens, as we can prove that vι (T) < θι (see Theorem 3). Despite this, there still exists
the situation of more spikes as expected. An example is given in Figure S1. Here we consider
the same example as in Figure 1. In source ANN, we suppose that two analog neurons in layer
l - 1 are connected to an analog neuron in layer l with weights 2 and -2, and the output vector
aι-1 of neurons in layer l - 1 is [0.6, 0.4]. Besides, in converted SNN, we suppose that the two
spiking neurons in layer l - 1 fire 3 spikes and 2 spikes in 5 time-steps (T=5), respectively, and the
threshold θl-1 = 1. Thus, φl-1(T) = PiTT__(i)θl-1 = [0.6,0.4]. According to Equation 1,
the ANN output aι = Wιaι-1 = [2, -2][0.6, 0.4]T = 0.4. As for SNN, we suppose that the
presynaptic neurons fires at t = 1, 2, 3 and t = 4, 5, respectively. Even through we set the threshold
θl = 1 to the maximum activation 2, the postsynaptic neuron will fire three spikes at t = 1, 2, 3, and
φl (T) = 0.6 > al.
Besides, setting maxs∈{0,1}n max(θl-1Wls) as the threshold brings two other problems. First,
the spiking neurons will take a long time to fire spikes because of the large value of the threshold,
which makes it hard to maintain SNN performance within a few time-steps. Second, the quantization
error will be large as it is proportional to the threshold. If the conversion error is not zero for
one layer, it will propagate layer by layer and will be magnified by larger quantization errors. We
compare our method and the method of setting the maximum activation on the CIFAR-100 dataset.
The results are reported in Table S1, where DT represents the dynamic threshold in our method. The
results show that our method can achieve better performance.
15
Published as a conference paper at ICLR 2022
Table S1: Comparison between our method and the method of setting the maximum activation.
DT1	w/o shift	T=4	T=8	T=16	T=32	T=64	T=128	T=256	T≥512
VGG-16 on CIFAR-100 with L=4									
X	X	69.62%	73.96%	76.24%	77.01%	77.10%	77.05%	77.08%	77.08%
X	×	21.57%	41.13%	58.92%	65.38%	64.19%	58.60%	52.99%	49.41%
×	X	1.00%	0.96%	1.00%	1.10%	2.41%	13.76%	51.70%	77.10%
×	×	1.00%	1.00%	0.90%	1.00%	1.01%	2.01%	19.59%	70.86%
1 Dynamic threshold.
Theorem 3. If the threshold is set to the maximum value of ANN activation, that is θl =
maxs∈{0,1}n max(θl-1W ls) , and vil (0) < θl. Then at any time-step, the membrane potential
of each neuron after spike vil(t) will be less than θl, where i represents the index of each neuron.
Proof. We prove it by induction. For t = 0, it is easy to see vil (0) < θl . For t > 0, we suppose
that vil(t - 1) < θl. Since we have set the threshold to the maximum possible input, and xli-1 (t)
represents the input from layer l - 1 to the i-th neuron in layer l, xli-1(t) will be no larger than θl
for arbitrary t. Thus we have
mli(t) = vil(t- 1) +xli-1(t) < θl +θl = 2θl,	(S14)
sli(t) = H(mli(t) - θl),	(S15)
vil (t) = mli(t) - sli(t)θl.	(S16)
Ifθl 6 mli(t) < 2θl, then we have vil(t) = mli(t)-θl < θl. If mli (t) < θl, then vil(t) = mli(t) < θl.
By mathematical induction, VIlo < θl holds for any t > 0.	□
A.5 Effect of quantization steps L
Table S2 reports the performance of converted SNNs with different quantization steps L and differ-
ent time-steps T. For VGG-16 and quantization steps L = 2, we achieve an accuracy of 86.53% on
CIFAR-10 dataset and an accuracy of 61.41% on CIFAR-100 dataset with 1 time-steps. When the
quantization steps L = 1, we cannot train the source ANN.
A.6 Comparison with state-of-the-art supervised training methods on
CIFAR- 1 0 dataset
Notably, our ultra-low latency performance is comparable with other state-of-the-art supervised
training methods. Table S3 reports the results of hybrid training and backpropagation methods
on CIFAR-10. The backpropagation methods require sufficient time-steps to convey discriminate
information. Thus, the list methods need at least 5 time-steps to achieve 〜91% accuracy. On the
contrary, our method can achieve 94.73% accuracy with 4 time-steps. Besides, the hybrid training
method requires 200 time-steps to obtain 92.02% accuracy because of further training with STDB,
whereas our method achieves 93.96% accuracy with 4 time-steps.
A.7 Comparison on CIFAR-100 dataset
Table S4 reports the results on CIFAR-100, our method also outperforms the others both in terms of
high accuracy and ultra-low latency. For VGG-16, the accuracy of the proposed method is 3.46%
higher than SNNC-AP and 69.37% higher than RTS when T = 32. When the time-steps is only 4,
we can still achieve an accuracy of 69.62%. These results demonstrate that our method outperforms
the previous conversion methods.
16
Published as a conference paper at ICLR 2022
Table S2: Influence of different quantization steps.
quantization steps	T=1	T=2	T=4	T=8	T=16	T=32	T=64	T=128
VGG-16 on CIFAR-10								
L=2	86.53%	91.98%	93.00%	93.95%	94.18%	94.22%	94.18%	94.14%
L=4	88.41%	91.18%	93.96%	94.95%	95.40%	95.54%	95.55%	95.59%
L=8	62.89%	83.93%	91.77%	94.45%	95.22%	95.56%	95.74%	95.79%
L=16	61.48%	76.76%	89.61%	93.03%	93.95%	94.24%	94.25%	94.22%
L=32	13.05%	73.33%	89.67%	94.13%	95.31%	95.66%	95.73%	95.77%
ResNet-20 on CIFAR-10								
L=2	77.54%	82.12%	85.77%	88.04%	88.64%	88.79%	88.85%	88.76%
L=4	62.43%	73.2%	83.75%	89.55%	91.62%	92.24%	92.35%	92.35%
L=8	46.19%	58.67%	75.70%	87.79%	92.14%	93.04%	93.34%	93.24%
L=16	30.96%	39.87%	57.04%	79.5%	90.87%	93.25%	93.44%	93.48%
L=32	22.15%	27.83%	43.56%	70.15%	88.81%	92.97%	93.48%	93.48%
VGG-16 on CIFAR-100								
L=2	61.41%	64.96%	68.0%	70.72%	71.87%	72.28%	72.35%	72.4%
L=4	57.5%	63.79%	69.62%	73.96%	76.24%	77.01%	77.1%	77.05%
L=8	44.98%	52.46%	62.09%	70.71%	74.83%	76.41%	76.73%	76.73%
L=16	33.12%	41.71%	53.38%	65.76%	72.80%	75.6%	76.37%	76.36%
L=32	15.18%	21.41%	32.21%	50.46%	67.32%	74.6%	76.18%	76.24%
ResNet-20 on CIFAR-100								
L=2	38.65%	47.35%	55.23%	59.69%	61.29%	61.5%	61.03%	60.81%
L=4	25.62%	36.33%	51.55%	63.14%	66.70%	67.47%	67.47%	67.41%
L=8	13.19%	19.96%	34.14%	55.37%	67.33%	69.82%	70.49%	70.55%
L=16	6.09%	9.25%	17.48%	38.22%	60.92%	68.70%	70.15%	70.20%
L=32	5.44%	7.41%	13.36%	31.66%	58.68%	68.12%	70.12%	70.27%
A.8 Energy consumption analysis
We evaluate the energy consumption of our method and the compared methods (Li et al., 2021;
Deng & Gu, 2020) on CIFAR-100 datasets. Here we use the same network structure of VGG-
16. Following the analysis in Merolla et al. (2014), we use synaptic operation (SOP) for SNN to
represent the required basic operation numbers to classify one image. We utilize 77fJ/SOP for SNN
and 12.5pJ/FLOP for ANN as the power consumption baseline, which is reported from the ROLLS
neuromorphic processor (Qiao et al., 2015). Note that we do not consider the memory access energy
in our study because it depends on the hardware. As shown in Table S5, when the time-steps is the
same, the energy consumption of our method is about two times of SNNC-AP. However, to achieve
the same accuracy of 73.55%, our method requires less energy consumption.
A.9 pseudo-code for overall conversion algorithm
In this section, we summarize the entire conversion process in Algorithm 1, including training ANNs
from scratch and converting ANNs to SNNs. The QCFS in the pseudo-code represents the proposed
quantization clip-floor-shift function.
17
Published as a conference paper at ICLR 2022
Table S3: Compare with state-of-the-art supervised training methods on CIFAR-10 dataset
Model	Method	Architecture	SNN Accuracy	Timesteps
CIFAR-10				
HC	Hybrid	VGG-16	92.02	200
STBP	Backprop	CIFARNet	90.53	12
DT	Backprop	CIFARNet	90.98	8
TSSL	Backprop	CIFARNet	91.41	5
DThIR1	ANN-SNN	CNet	77.10	256
Ours	ANN-SNN	VGG-16	93.96	4
Ours	ANN-SNN	CIFARNet2	94.73	4
1 Implemented on Loihi neuromorphic processor
2 For CIFARNet, we use the same architecture as Wu et al. (2018).
Table S4: Comparison between the proposed method and previous works on CIFAR-100 dataset.
ArChiteCture	Method	ANN	T=2	T=4	T=8	T=16	T=32	T=64	T≥512
	RMP	71.22%	-	-	-	-	-	-	70.93%
	TSC	71.22%	-	-	-	-	-	-	70.97%
VGG-16	RTS	77.89%	-	-	-	-	7.64%	21.84%	77.71%
	SNNC-AP	77.89%	-	-	-	-	73.55%	76.64%	77.87%
	Ours	76.28%	63.79%	69.62%	73.96%	76.24%	77.01%	77.10%	77.08%
	RMP	68.72%	-	-	-	-	27.64%	46.91%	67.82%
ResNet-20	TSC	68.72%	-	-	-	-	-	-	68.18%
	Ours	69.94%	19.96%	34.14%	55.37%	67.33%	69.82%	70.49%	70.50%
	RTS	77.16%	-	-	-	-	51.27%	70.12%	77.19%
ResNet-18	SNNC-AP	77.16%	-	-	-	-	76.32%	77.29%	77.25%
	Ours	78.80%	70.79%	75.67%	78.48%	79.48%	79.62%	79.54%	79.61%
1 RTS and SNNC-AP use altered ResNet-18, while ours use standard ResNet-18.									
Table S5: Comparison of the energy consumption with previous works
Method		ANN	T=2	T=4	T=8	T=16	T=32	T=64
	ACCuraCy	77.89%	-	-	-	-	7.64%	21.84%
RTS	OP (GFLOP/GSOP)	0.628	-	-	-	-	0.508	0.681
	Energy (mJ)	7.85	-	-	-	-	0.039	0.052
	ACCuraCy	77.89%	-	-	-	-	73.55%	76.64%
SNNC-AP	OP (GFLOP/GSOP)	0.628	-	-	-	-	0.857	1.22
	Energy (mJ)	7.85	-	-	-	-	0.660	0.094
	ACCuraCy	76.28%	63.79%	69.62%	73.96%	76.24%	77.01%	77.10%
Ours	OP (GFLOP/GSOP)	0.628	0.094	0.185	0.364	0.724	1.444	2.884
	Energy (mJ)	7.85	0.007	0.014	0.028	0.056	0.111	0.222
18
Published as a conference paper at ICLR 2022
Algorithm 1 Algorithm for ANN-SNN conversion.	
Input: ANN model MANN(x; W) with initial weight W; Dataset D; Quantization step L; Initial dynamic thresholds λ; Learning rate . Output： MSNN(x; W)	
1	: for l = 1 to MANN.layers do
2	: if is ReLU activation then
3	:	Replace ReLU(x) by QCFS(x; L, λl)
4	: end if
5	: if is MaxPooling layer then
6	:	Replace MaxPooling layer by AvgPooling layer
7	: end if
8	: end for
9	: for e = 1 to epochs do
10	: for length of Dataset D do
11	:	Sample minibatch (x0, y) from D
12	:	for l = 1 to MANN.layers do
13	:	xl = QCFS(Wlxl-1; L, λl)
14	:	end for
15	:	Loss = CrossEntropy(xl , y)
16	:	for l = 1 to MANN.layers do
17	wl — wl - E ∂L°sr ∂W l
18	λl 一 λl - E dLoss
19	∂ λl :	end for
20	: end for
21	: end for
22	: for l = 1 to MANN.layers do
23	MsnnWl J Mann∙Wl
24	MSNN ∙θl — Mann ∙λl
25	msnn∙vl(O) J MsNN∙θl∕2
26	: end for
27	: return MSNN
19