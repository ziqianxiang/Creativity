Published as a conference paper at ICLR 2022

OFFLINE  REINFORCEMENT  LEARNING  WITH  VALUE-
BASED  EPISODIC  MEMORY

Xiaoteng Ma1∗, Yiqin Yang1∗, Hao Hu2∗, Qihan Liu1,

Jun Yang1†, Chongjie Zhang2†, Qianchuan Zhao1, Bin Liang1
¹Department of Automation, Tsinghua University

²Institute for Interdisciplinary Information Sciences, Tsinghua University

{ma-xt17,yangyiqi19,hu-h19,lqh20}@mails.tsinghua.edu.edu

{yangjun603,chongjie,zhaoqc,bliang}@tsinghua.edu.cn

ABSTRACT

Offline reinforcement learning (RL) shows promise of applying RL to real-world
problems by effectively utilizing previously collected data.  Most existing offline
RL algorithms use regularization or constraints to suppress extrapolation error for
actions outside the dataset.  In this paper, we adopt a different framework, which
learns the V -function instead of the Q-function to naturally keep the learning pro-
cedure within the offline dataset.  To enable effective generalization while main-
taining proper conservatism in offline learning, we propose Expectile V -Learning
(EVL), which smoothly interpolates between the optimal value learning and be-
havior cloning.  Further, we introduce implicit planning along offline trajectories
to enhance learned V -values and accelerate convergence.  Together, we present
a new offline method called Value-based Episodic Memory (VEM). We provide
theoretical analysis for the convergence properties of our proposed VEM method,
and empirical results in the D4RL benchmark show that our method achieves su-
perior performance in most tasks, particularly in sparse-reward tasks. Our code is
public online at https://github.com/YiqinYang/VEM.

1    INTRODUCTION

Despite the great success of deep reinforcement learning (RL) in various domains, most current al-
gorithms rely on interactions with the environment to learn through trial and error.  In real-world
problems, particularly in risky and safety-crucial scenarios, interactions with the environment can
be expensive and unsafe, and only offline collected datasets are available, such as the expert 
demon-
stration or previously logged data.  This growing demand has led to the emergence of offline rein-
forcement learning (offline RL) to conduct RL in a supervised manner.

The main challenge of offline RL comes from the actions out of the dataset’s support (Kumar et al.,
2019; 2020).  The evaluation of these actions that do not appear in the dataset relies on the gener-
alization of the value network, which may exhibit extrapolation error (Fujimoto et al., 2019). This
error can be magnified through bootstrapping, leading to severe estimation errors. A rapidly devel-
oping line of recent work (Fujimoto et al., 2019; Kumar et al., 2020; Ghasemipour et al., 2021; Yang
et al., 2021) utilizes various methods to constrain optimistic estimation on unseen actions, such as
restricting available actions with a learned behavior model (Fujimoto et al., 2019) or penalizing 
the
unseen actions with additional regularization (Kumar et al., 2020).   However,  confining learning
within the distribution of the dataset can be insufficient for reducing extrapolation errors.

Another line of methods, on the contrary, uses the returns of the behavior policy as the signal for
policy learning, as adopted in Wang et al. (2018); Peng et al. (2019); Chen et al. (2020).  By doing
so, they keep the value learning procedure completely within the dataset.  However, the behavior
policy of the dataset can be imperfect and insufficient to guide policy learning.  To achieve a 
trade-
off between imitation learning and optimal value learning while confines learning within the 
dataset,

*Equal contribution. Listing order is random.

†Equal advising.

1


Published as a conference paper at ICLR 2022


Memory

Regularization/
Constraint

Improvement

Episodic Memory

Expectile V-Learning

Improvement


Trans

Value

Trajs       0        1        2

3        …      T-1      T


Update Target

Evaluation

Update Memory

Q-based Offline RL                                                                V-based Episodic 
Memory

Figure 1: The diagram of algorithms. The left side denotes the general Q-based offline RL methods.
The right side is the framework of our proposed approach (VEM). Q-based methods learns boot-
strapped Q-values, but requires additional constraint or penalty for actions out of the dataset.  
Our
method, on the contrary, learns bootstrapped V -values while being completely confined within the
dataset without any regularization.

we propose Expectile V -learning (EVL), which is based on a new expectile operator that smoothly
interpolates between the Bellman expectation operator and optimality operator.

To better solve long-horizon and sparse-reward tasks, we further propose using value-based plan-
ning to improve the advantage estimation for policy learning.  We adopt an implicit memory-based
planning scheme that strictly plans within offline trajectories to compute the advantages 
effectively,
as proposed in recent advances in episodic memory-based methods (Hu et al., 2021).  Together, we
present our novel framework for offline RL, Value-based Episodic Memory (VEM), which uses ex-
pectile V -learning to approximate the optimal value with offline data and conduct implicit memory-
based planning to further enhance advantage estimation. With the properly learned advantage func-
tion, VEM trains the policy network in a simple regression manner.  We demonstrate our algorithm
in Figure 1, and a formal description of our algorithm is provided in Algorithm 1.

The contributions of this paper are threefold.  First, we present a new offline V -learning method,
EVL, and a novel offline RL framework, VEM. EVL learns the value function through the trade-offs
between imitation learning and optimal value learning. VEM uses a memory-based planning scheme
to enhance advantage estimation and conduct policy learning in a regression manner.  Second, we
theoretically analyze our proposed algorithm’s convergence properties and the trade-off between
contraction rate, fixed-point bias, and variance.  Specifically, we show that VEM is provably con-
vergent and enjoys a low concentration rate with a small fixed-point bias.  Finally, we evaluate our
method in the offline RL benchmark D4RL (Fu et al., 2020). Comparing with other baselines, VEM
achieves superior performance, especially in the sparse reward tasks like AntMaze and Adroit. The
ablation study shows that VEM yields accurate value estimates and is robust to extrapolation 
errors.

2    BACKGROUND

Preliminaries. We consider a Markov Decision Process (MDP) M defined by a tuple (S, A, P, r, γ),
where S is the state space, A is the action space, P (· | s, a)  :  S × A × S  → R is the transition
distribution function, r(s, a) : S×A → R is the reward function and γ ∈ [0, 1) is the discount 
factor.
We say an environment is deterministic if P (s′ | s, a)  =  δ(s′ =  f (s, a)) for some deterministic
transition function f , where δ(·) is the Dirac function. The goal of an RL agent is to learn a 
policy
π :  S × A → R, which maximΣizes thetexpectation of a discounted cumulative reward:  J (π)  =

Value-based Offline Reinforcement Learning Methods.     Current  offline  RL  methods  can  be
roughly  divided  into  two  categories  according  to  types  of  learned  value  function:  
Q-based  and
V -based methods.  Q-based methods, such as BCQ (Fujimoto et al., 2019), learn Q-function for
policy learning and avoid selecting unfamiliar actions via constraints or penalty.  On the contrary,
V -based methods (Peng et al., 2019; Siegel et al., 2020; Chen et al., 2020) learns the value of be-
havior policy V µ(s) with the trajectories in the offline dataset     and update policy as a 
regression
problem. Based on the learned V -function, V -based methods like AWR (Peng et al., 2019) updates
the policy using advantage-weighted regression, where each state-action pair is weighted according

2


Published as a conference paper at ICLR 2022

to the exponentiated advantage:

max Jπ(φ) = E₍s ,ₐ ₎∼D [log πφ(at | st) exp (Rt − V µ(st))] .                    (1)

Episodic Memory-Based Methods.  Inspired by psychobiology, episodic memory-based methods
store experiences in a non-parametric table to fast retrieve past successful strategies when encoun-
tering similar states. Model-free episodic control (Blundell et al., 2016a) updates the memory table
by taking the maximum return R(s, a) among all rollouts starting from same state-action pair (s, a).
Hu et al. (2021) proposes Generalizable Episodic Memory, which extends this idea to the continuous
domain, and proposes updating formula with a parametric memory QEM .

3    METHOD

In this section, we describe our novel offline method, value-based episodic memory, as depicted in
Figure 1. VEM uses expectile V -learning (EVL) to learn V -functions while confines value learning
within the dataset to reduce extrapolation error.  EVL uses an expectile operator that interpolates
between Bellman expectation operator and optimality operator to balance behavior cloning and op-
timal value learning.  Further, VEM integrates memory-based planning to improve the advantage
estimation and accelerate the convergence of EVL. Finally, generalized advantage-weighted learn-
ing is used for policy learning with enhanced advantage estimation.  A formal description for the
VEM algorithm is shown in Algorithm 1 in Appendix A.1.

3.1    EXPECTILE V-LEARNING

To achieve a balance between behavior cloning and optimal value learning, we consider the Bellman
expectile operator defined as follows:

((T µ)V )(s) := arg min Eₐ∼µ₍·|s₎Στ [δ(s, a)]²  + (1 − τ )[δ(s, a)]² Σ                 (2)

where µ is the behavior policy, δ(s, a)  =  Es'∼P (·|s,a)[r(s, a) + γV (s′)     v] is the expected 
one-
step TD error, [ ]₊  = max( , 0) and [ ]    = min( , 0).  This operator resembles the expectile 
statis-
tics (Newey & Powell, 1987; Rowland et al., 2019) and hence its name.   We can see that when
τ = 1/2, this operator is reduced to Bellman expectation operator, while when τ      1, this 
operator
approaches Bellman optimality operator, as depicted in Lemma 3.

We use the following toy example to further illustrate


the trade-offs achieved by EVL. Consider a random
generated  MDP.  When  the  operator  can  be  applied
exactly, the Bellman optimality operator is sufficient
to  learn  the  optimal  value  V ∗.   However,  applying
operators with an offline dataset raises a noise on the

actual operator due to the estimation error with finite
and biased data.  We simulate this effect by adding
random Gaussian noise to the operator. Applying the
optimality operator on offline datasets can lead to se-
vere overestimation due to the maximization bias and
bootstrapping. The value estimation learned by EVL,
on               the contrary, achieves a trade-off between learning
optimal policy and behavior cloning and can be close
to  the optimal value with proper chosen τ , as depicted

300

200

100

0

−100

noise

           0.0

0.1

0.2

0.3

0.4

0.3         0.4         0.5         0.6         0.7         0.8         0.9

τ


in Figure 2.  The noise upon the operator largely de-
pends on the size of the dataset. Estimation error can
be significant with insufficent data.  In this case, we
need a small τ to be conservative and be close to be-
havior cloning. When the dataset is large and we are
able to have an accurate estimation for the operator,

Figure 2:  Trade-offs of EVL between gen-
eralization  and  conservatism  in  a  random
MDP.  The  green  line  shows  the  optimal
value and the blue line shows the value of
behavior policy.  The curve is averaged over
20 MDPs.

we can use a larger τ to recover the optimal policy.  By adjusting τ , the expectile operator can 
ac-
commodate variant types of datasets. However, the expectile operator in Equation 2 does not have a

3


Published as a conference paper at ICLR 2022

closed-form solution. In practice, we consider the one-step gradient expectile operator

((Tg)µV )(s) = V (s) + 2αEₐ∼µ₍·|s₎ Στ [δ(s, a)]    + (1 − τ )[δ(s, a)]−Σ ,              (3)

where α is the step-size.  Please refer to Appendix B.1 for the detailed derivation.  For notational
convenience, we use T µ to denote the one-step gradient expectile operator (Tg)µ hereafter.

We consider the case where the dynamics are nearly-deterministic like robotic applications,  and
we remove the expectation over the next states in the operator.  This leads to a practical 
algorithm,
Expectile V -Learning, where we train the value network to minimize the following loss:


JV (θ) = E(s,a,s')∼D

Σ.Vˆ(s) − Vθ (s)

Σ2Σ ,

(4)


where Vˆ

Vˆ(s) = Vθ' (s) + 2α  τ [δ(s, a, s′)]+  + (1 − τ )[δ(s, a, s′)]−  ,

is the target value after applying one-step gradient expectile operator and δ(s, a, s′)  =

r(s, a) + γVθ' (s′) − Vθ' (s).  V -function and the target Vˆ-function are parameterized by θ and 
θ′,
respectively. EVL is guaranteed to converge with concentration rate γτ = 1 − 2(1 − γ)α max{τ, 1 −
τ }. Please refer to Section 4 for a detailed analysis.

3.2    IMPLICIT MEMORY-BASED PLANNING

Although EVL reduces the extrapolation error, it is still a challenging problem to bootstrap over
long time horizons due to estimation errors with a fixed dataset. Therefore, we propose using value-
based  planning  to  conduct  bootstrapping  more  efficiently.   We  adopt  an  implicit  
memory-based
planning scheme that strictly plans within offline trajectories to avoid over-optimistic 
estimations in
the planning phase.  This is aligned with recent advances in episodic memory-based methods (Hu
et al., 2021), but we conduct this planning on expectile V -values rather than Q-values. 
Specifically,


we compare the best return so far along the trajectory with the value estimates Vˆ
maximum between them to get the augmented return Rˆt:

and takes the


Rˆt

=    rt + γ max(Rˆt+1, Vˆ(st₊₁)),   if    t < T,
rt,                                       if    t = T,

(5)

where t denotes steps along the trajectory, T is the episode length, and Vˆ is generalized from 
similar
experiences.  This procedure is conducted recursively from the last step to the first step along the
trajectory, forming an implicit planning scheme within the dataset to aggregate experiences along
and across trajectories.  Further, the back-propagation process in Equation 5 can be unrolled and
rewritten as follows:


Rˆt

=     max

0<n≤nmax

Vˆt,n,

Vˆt,n

rt + γVˆt+1,n−1      if    n > 0,
Vˆ(st)                    if    n = 0,

(6)

where n denotes different length of rollout steps and Vˆt,n = 0 for n > T .

3.3    GENERALIZED ADVANTAGE-WEIGHTED LEARNING

Based on Rˆt  calculated in Section 3.2,  we can conduct policy learning in a regression form,  as
adopted in return-based offline RL methods (Nair et al., 2020; Siegel et al., 2020; Peng et al., 
2019):
max Jπ(φ) = E₍st,at)∼D Σlog πφ(at | st) · f .Aˆ(st, at)ΣΣ ,                      (7)

where Aˆ(st, at) = Rˆt     Vˆ(st) and f is an increasing, non-negative function.  Please refer to 
Ap-
pendix C.1 for the detailed implementation of Equation 7. Note that Rˆt is not the vanilla returns 
in
the dataset, but the enhanced estimation calculated by implicit planning from Vˆt, as opposed with
other return based methods.  Please refer to Algorithm 1 and Section 4 for implementation details
and theoretical analysis.

4


Published as a conference paper at ICLR 2022

4    THEORETICAL  ANALYSIS

In this section, we first derive the convergence property of expectile V -Learning. Then, we demon-
strate that memory-based planning accelerates the convergence of the EVL. Finally, we design a toy
example to demonstrate these theoretical analyses empirically.  Please refer to Appendix B for the
detailed proofs of the following analysis.

4.1    CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING

In this section, we assume the environment is deterministic.  We derive the contraction property of

T µ as the following statement:

Lemma 1.  For any τ ∈ (0, 1), T µ is a γτ -contraction, where γτ = 1 − 2α(1 − γ) min{τ, 1 − τ }.
Proof.  We introduce two more operators to simplify the analysis:

(T µV )(s) = V (s) + Eₐ∼µ[δ(s, a)]₊, (T µV )(s) = V (s) + Eₐ∼µ[δ(s, a)]−.           (8)

Next we show that both operators are non-expansion (e.g., ǁT µV₁ − T µV₂ǁ∞  ≤ ǁV₁ − V₂ǁ∞).

Finally, we rewrite T µ based on T µ and T µ and we prove that T µ is a γτ -contraction. Please 
refer

to Appendix B.2 for the complete proof.

Based on Lemma 1, we give a discussion about the step-size α and the fraction τ :

About  the  step-size  α.    Generally,  we  always  want  a  larger  α.   However,  α must  
satisfy  that

V (s) + 2ατδ(s, a) ≤ max{r(s, a) + γV (s′), V (s)} and V (s) + 2α(1 − τ )δ(s, a) ≥ min{r(s, a) +

γV (s′), V (s)},  otherwise  the  V -value  will  be  overestimated.    Thus,  we  must  have  2ατ  
≤  1

and  2α(1 − τ )  ≤  1,  which  infers  that  α  ≤               1              .   When  α  =       
     ¹          ,  we  have


2 max{τ,1−τ }

γ   = 1     2α min  τ, 1     τ  (1     γ) = 1      ᵐⁱⁿ{τ,¹−τ} (1     γ).

max{τ,1−τ }

2 max{τ,1−τ }

About the fraction τ .    It is easy to verify that γτ approaches to 1 when τ      0 or τ      1, 
which
means that with a larger τ the contractive property is getting weaker. The choice of τ makes a 
trade-
off between the learning stability and the optimality of values. We further point out that when τ = 
1,
the Expectile V -learning degrades as a special case of the generalized self-imitation learning 
(Tang,
2020), which losses the contractive property.

Next, we prove that T µ is monotonous improving with respect to τ :

Lemma 2.  For any τ, τ ′ ∈ (0, 1), if τ ′ ≥ τ , we have T µV (s) ≥ T µV (s), ∀s ∈ S.

Based on the Lemma 2, we derive that Vτ∗ is monotonous improving with respect to τ :

Proposition 1.  Let Vτ∗  denote the fixed point of T µ.  For any τ, τ ′  ∈ (0, 1), if τ ′  ≥ τ , we 
have

Vτ∗' (s) ≥ Vτ∗(s), ∀s ∈ S.

Further, we derive that Vτ∗ gradually approaches V ∗ with respect to τ :

Lemma 3.  Let V ∗ denote the fixed point of Bellman optimality operator T ∗.  In the deterministic
MDP, we have limτ→₁ Vτ∗ = V ∗.

Based on the above analysis, we have the following conclusion:

Remark 1.  By choosing a suitable τ , we can achieve the trade-off between the contraction rate and
the fixed point bias.  Particularly, a larger τ introduces a smaller fixed point bias between Vτ∗  
and
V ∗, and produces a larger contraction rate γτ simultaneously.

4.2    VALUE-BASED EPISODIC MEMORY

In this part, we demonstrate that the memory-based planning effectively accelerates the convergence
of the EVL. We first define the VEM operator as:


(  vₑmV )(s) =     max

n≤nmax

5

{(T µ)ⁿ−¹T µV (s)},                                (9)


Published as a conference paper at ICLR 2022


0.90

0.85

0.80

0.75

0.70

0.65

2.5                3.0                3.5

Bias

0.20

0.15

0.10

0.7                0.8                0.9

Contraction Rate

0.50

0.45

0.40

0.35

0                         1                         2

Bias

0.4

0.3

0.2

0.1

0.0

0.35            0.40            0.45            0.50

Contraction Rate

(a) The maximal rollout step nmax.                              (b) The different behavior 
policies.

Figure 3:  A toy example in the random MDP. In both figures,  the color darkens with a larger τ
(τ        0.6, 0.7, 0.8, 0.9  ).   The size of the spots is proportional to the relative scale of 
the third
variable: (a) Change nmₐₓ. From magenta to blue, nmₐₓ is set as 1, 2, 3, 4 in order. (b) Change the
behavior polices µ, where µ(s) = softmax(Q∗(s, )/α). From light yellow to dark red, the α is set
as 0.1, 0.3, 1, 3 in order.

where nmₐₓ is the maximal rollout step for memory control.  Then, we derive that multi-step esti-
mation operator Tvₑm does not change the fixed point and contraction property of T µ:

Lemma 4.  Given τ  ∈ (0, 1) and nmₐₓ  ∈ N+, Tvₑm is a γτ -contraction.  If τ  >  ¹ , Tvₑm has the

2

same fixed point as T µ.

Next, we derive that the contraction rate of Tvₑm depends on the dataset quality. Further, we demon-
strate that the convergence rate of Tvₑm is quicker than T µ even the behavior policy µ is random:

Lemma 5.  When the current value estimates V (s) are much lower than the value of behavior policy,

Tvₑm provides an optimistic update. Formally, we have


|TvₑmV (s) − V ∗(s)| ≤ γⁿ∗ (s)−1γτ ǁV − V µ

ǁ∞ + ǁV

− V ∗ǁ∞, ∀s ∈ S,        (10)


where n∗(s)  =  arg max

{(T µ)ⁿ−¹T µV (s)}, V µ

is the fixed point of (T µ)ⁿ∗ (s)−1T µ

and it is the optimal rollout value starting from s.

This lemma demonstrates that   vₑm can provide an optimistic update for pessimistic value estimates.
Specifically, the scale of the update depends on the quality of the datasets. If the behavior 
policy µ

is expert, which means V µ   is close to V ∗. Then, following the lemma, the contraction rate will 
be

τ

near to γⁿ∗ (s)−1γτ . Moreover, if the initial value estimates are pessimistic (e.g., the 
initialized value
function with zeros), we will have n∗(s)     nmₐₓ, indicating that the value update will be 
extremely
fast towards a lower bound of Vτ∗. On the contrary, if µ is random, we have n∗(s)     1 and the 
value
update will be slow towards Vτ∗.

Remark 2.  By choosing a suitable nmₐₓ,  we can achieve the trade-off between the contraction
rate and the estimation variance, i.e., a larger nmₐₓ yields a fast update towards a lower bound of
fixed point and tolerable variances empirically.  Meanwhile, the choice of nmₐₓ does not introduce
additional bias, and the fixed point bias is totally controlled by τ .

4.3    TOY EXAMPLE

We design a toy example in the random deterministic MDP to empirically demonstrate the above
analysis.  Following (Rowland et al., 2020), we adopt three indicators, including update variance,
fixed-point bias, and contraction rate, which is shown in Figure 3. Specifically, the contraction 
rate


is supV

Σ

V ' ǁTvₑmV  − TvₑmV ′ǁ∞/ǁV  − V ′ǁ∞,  the bias is ǁVv∗em  − V ∗ǁ∞ and the variance is

Σ 1

 	   

pointed of    vₑm.   First,  the experimental results in Figure 3(a) demonstrate that the 
relationship

of n-step estimation and τ .  Formally, the contraction rate decreases as n becomes larger, and the
fixed-point bias increases as τ becomes smaller, which are consistent with Lemma 1 and Lemma 2.
Figure 3(a) also shows that the variance is positively correlated with n.  Second, the experimental
results in Figure 3(b) demonstrate that the relationship of dataset quality and τ .  The higher 
dataset
quality corresponds to the lower contraction rate and variance, which is consistent with Lemma 5.

6


Published as a conference paper at ICLR 2022

(a) Large                                         (b) Medium                                     
(c) Umaze

Figure 4:  Visualization of the value estimation in various AntMaze tasks. Darker colors correspond
to the higher value estimation.  Each map has several terminals (golden stars) and one of which is
reached by the agent (the light red star). The red line is the trajectory of the ant.

5    RELATED  WORK

Offline Reinforcement Learning.    Offline RL methods (Kumar et al., 2019; Siegel et al., 2020;
Argenson & Dulac-Arnold, 2020; Wu et al., 2021; Dadashi et al., 2021; Kostrikov et al., 2021; Jin
et al., 2021; Rashidinejad et al., 2021) can be roughly divided into policy constraint, pessimistic
value estimation, and model-based methods. Policy constraint methods aim to keep the policy to be
close to the behavior under a probabilistic distance (Fujimoto et al., 2019; Peng et al., 2019; Nair
et al., 2020).  Pessimistic value estimation methods like CQL (Kumar et al., 2020) enforces a regu-
larization constraint on the critic loss to penalize overgeneralization. Model-based methods attempt
to learn a model from offline data, with minimal modification to the policy learning  (Kidambi et 
al.,
2020; Yu et al., 2020; Janner et al., 2019). However, these methods have to introduce additional be-
havioral policy models, dynamics models, or regularization terms (Zhang et al., 2020b;a; Lee et al.,
2021).  Another line of methods uses empirical return as the signal for policy learning, which con-
fines learning within the dataset but leads to limited performance (Levine et al., 2020; Geist et 
al.,
2019; Wang et al., 2021).

Episodic Control.    Episodic control aims to store good past experiences in a non-parametric mem-
ory and rapidly latch into past successful policies when encountering similar states instead of 
waiting
for many optimization steps (Blundell et al., 2016b).  Pritzel et al. (2017) and Lin et al. (2018) 
intro-
duce a parametric memory, which enables better generalization through neural networks. Our work
is closely related to recent advances in  Hu et al. (2021), which adopts an implicit planning 
scheme to
enable episodic memory updates in continuous domains. Our method follows this implicit scheme,
but conducts planning with expectile V -values to avoid overgeneralization on actions out of dataset
support.

6    EXPERIMENTS

In our experiments, we aim to answer the following questions:  1) How does our method performe
compared to state-of-the-art offline RL algorithms on the D4RL benchmark dataset?  2) How does
implicit  planning  affect  the  performance  on  sparse  reward  tasks?   3)  Can  expectile  V 
-Learning
effectively reduces the extrapolation error compared with other offline methods?  4) How does the
critical parameter τ affect the performance of our method?

6.1    EVALUATION ENVIRONMENTS

We ran VEM on AntMaze, Adroit, and MuJoCo environments to evaluate its performance on var-
ious types of tasks.  Precisely, the AntMaze navigation tasks control an 8-DoF quadruped robot to
reach a specific or randomly sampled goal in three types of maps.   The reward in the AntMaze
domain is highly sparse.  The Adroit domain involves controlling a 24-DoF simulated hand tasked
with hammering a nail, opening a door, twirling a pen, or picking up and moving a ball.  On the
adroit tasks, these datasets are the following, “human”:  transitions collected by a human 
operator,

7


Published as a conference paper at ICLR 2022


Type
fixed
play
play
diverse
diverse
diverse
human
human
human
human
cloned
cloned
cloned
expert
expert
expert
expert
random
random
random
medium
medium
medium

Env
umaze
medium
large
umaze
medium
large
door
hammer
relocate
pen
door
hammer
pen
door
hammer
relocate
pen

walker2d
hopper
halfcheetah
walker2d
hopper
halfcheetah

VEM(Ours)
87.5±1.1

78.0±3.1

57.0±5.0

78.0 ± 1.1

77.0±2.2

58.0 ± 2.1

11.2±4.2

3.6±1.0

1.3±0.2

65.0±2.1

3.6±0.3

2.7±1.5

48.7±3.2

105.5±0.2

128.3±1.1

109.8±0.2

111.7±2.6

6.2±4.7

11.1±1.0

16.4±3.6

74.0±1.2

56.6±2.3

47.4±0.2

VEM(τ =0.5)

85.0±1.5

71.0±2.5

45.0±2.5

75.0±5.0

60.0±5.0

48.0±2.7

6.9±1.1

2.5±1.0

0.0±0.0

55.2±3.1

0.0±0.0

0.5±0.1

27.8±2.2

104.8±0.2

102.3±5.6

101.0±1.5

115.2±1.3

6.2±4.7

10.8±1.2

2.6±2.1

16.6±0.1

56.6±2.3

45.3±0.2

BAIL
62.5 ± 2.3

40.0 ± 15.0

23.0±5.0

75.0±1.0

50.0±10.0

30.0±5.0

0.0±0.1

0.0±0.1

0.0±0.1

32.5±1.5

0.0±0.1

0.1±0.1

46.5±3.5

104.7±0.3

123.5±3.1

94.4±2.7

126.7±0.3

3.9±2.5

9.8±0.1

0.0±0.1

73.0±1.0

58.2±1.0

42.6±1.2

BCQ
78.9

0.0

6.7

55.0

0.0

2.2

-0.0

0.5

0.5

68.9

0.0

0.4

44.0

99.0

114.9

41.6

114.9

4.9

10.6

2.2

53.1

54.5

40.7

CQL
74.0

61.2

11.8

84.0

53.7

14.9

9.1

2.1

2.1

55.8

3.5

5.7

40.3

-

-

-

-

7.0

10.8

35.4

79.2

58.0

44.4

AWR
56.0

0.0

0.0

70.3

0.0

0.0

0.4

1.2

-0.0

12.3

0.0

0.4

28.0

102.9

39.0

91.5

111.0

1.5

10.2

2.5

17.4

35.9

37.4

Table 1: Performance of VEM with four offline RL baselines on the AntMaze, Adroit, and MuJoCo
domains with the normalized score metric proposed by D4RL benchmark, averaged over three ran-
dom seeds with     standard deviation. Scores range from 0 to 100, where 0 corresponds to a random
policy performance, and 100 indicates an expert.  We use the results in Fu et al. (2020) for AWR
and BCQ, and use the results in Kumar et al. (2020) for CQL. The results of BAIL come from our
implementation according to the official code  (https://github.com/lanyavik/BAIL).

“cloned”: transitions collected by a policy trained with behavioral cloning interacting in the 
environ-
ment + initial demonstrations, “expert”:  transitions collected by a fine-tuned RL policy 
interacting
in the environment.  As for the MuJoCo tasks, the datasets are “random”:  transitions collected by
a random policy,“medium”:  transitions collected by a policy with suboptimal performance.  The
complete implementation details are presented in Appendix C.

6.2    PERFORMANCE ON D4RL TASKS

As shown in Table 1, VEM achieves state-of-the-art performance on most AntMaze tasks and has
a significant improvement over other methods on most Adroit tasks.  VEM also achieves good per-
formances in MuJoCo domains.   We find that VEM has low value estimation errors in all tasks,
which promotes its superior performance. However, as a similar training framework, BAIL only has
reasonable performances on simple offline tasks, such as MuJoCo. Please refer to Appendix D.2 for
the complete training curves and value estimation error on D4RL.

To further analyze the superior performance of VEM in the sparse reward tasks, we visualize the
learned value estimation in AntMaze tasks, which is shown in Figure 4. Experimental results show
that VEM has the higher value estimates on the critical place of the map (e.g., corners) since 
various
trajectories  in  the  datasets  are  connected.   The  accurate  value  estimation  leads  to  its 
 success  on
complex sparse reward tasks.

6.3    ANALYSIS OF VALUE ESTIMATION

As both Expectile V -Learning (EVL) and Batch Constrained Q-Learning (BCQ) (Fujimoto et al.,
2019) aim to avoid using the unseen state-action pairs to eliminate the extrapolation error, we re-
place EVL in VEM with BCQ (named BCQ-EM) to evaluate the effectiveness of the EVL module.

8


Published as a conference paper at ICLR 2022

The experimental results in Figure 9 in Appendix D.1 indicate that the performance of BCQ-EM is
mediocre, and BCQ reaches performance significantly below VEM. We observe a strong correla-
tion between the training instability and the explosion of the value estimation. This result should 
not
come as a surprise since the Adroit tasks have a larger action space compared with MuJoCo domains
and narrow human demonstrations. Therefore, the generative model in BCQ cannot guarantee com-
pletely the unseen actions are avoided.  In contrast, VEM avoids fundamentally unseen actions by
keeping the learning procedure within the support of an offline dataset, indicating the necessity of
the EVL module. Please refer to Appendix C for the implementation details.

We evaluate τ      0.1, 0.2, ..., 0.9   to investigate the effect of the critical hyper-parameter 
in EVL,
which is shown in Figure 7 in Appendix D.1.  The experimental results demonstrate that the esti-
mated value increases with a larger τ , which is consistent with the analysis in Section 4.1. 
Moreover,
we observe that τ is set at a low value in some complex high-dimensional robotic tasks or narrow
human demonstrations, such as Adroit-cloned/human, to get the conservative value estimates. How-
ever, if τ is set too high (e.g., τ = 0.9 in the pen-human task), the estimated value will explode 
and
poor performance. This is as expected since the over-large τ leads to the overestimation error 
caused
by neural networks. The experimental results demonstrate that we can balance behavior cloning and
optimal value learning by choosing τ in terms of different tasks.

6.4    ABLATIONS

Episodic Memory Module.    Our first study aims to answer the impact of memory-based planning
on performance. We replace the episodic memory module in VEM with standard n-step value esti-
mation (named VEM-1step or VEM-nstep).  The experimental results in Figure 8 in Appendix D.1
indicate that implicit planning along offline trajectories effectively accelerates the convergence 
of
EVL.

Expectile Loss.    In addition to the Expectile loss, we explored other forms of loss.  Formally, we
compare the Expectile loss and quantile loss, a popular form in Distributional RL algorithms (Dab-
ney et al., 2018), which is shown in Figure 5 in Appendix D.1.  The experimental results indicate
that the Expectile loss is better since it is more stable when dealing with extreme values.

7    CONCLUSION

In this paper, we propose a novel offline RL method, VEM, based on a new V -learning algorithm,
EVL. EVL naturally avoids actions outside the dataset and provides a smooth tradeoff between gen-
eralization and conversation for offline learning.  Further, VEM enables effective implicit planning
along offline trajectories to accelerate the convergence of EVL and achieve better advantage estima-
tion.  Unlike most existing offline RL methods, we keep the learning procedure totally within the
dataset’s support without any auxiliary modular, such as environment model or behavior policy. The
experimental results demonstrate that VEM achieves superior performance in most D4RL tasks and
learns    the accurate values to guide policy learning, especially in sparse reward tasks.  We hope 
that
VEM will inspire more works on offline RL and promote practical RL methods in the future.

8    REPRODUCIBILITY

To ensure our work is reproducible, we provide our code in the supplementary materials. In the fu-
ture, we will publish all source code on Github. The detailed implementation of our algorithm is 
pre-
sented as follows. The value network is trained according to Equation 4. The actor-network is 
trained
according to Equation 7.  The hyper-parameters and network structure used in VEM are shown in
Appendix C.3. All experiments are run on the standard offline tasks, D4RL (https://github.com/rail-
berkeley/d4rl/tree/master/d4rl).

9


Published as a conference paper at ICLR 2022

REFERENCES

Arthur  Argenson  and  Gabriel  Dulac-Arnold.     Model-based  offline  planning.     arXiv  
preprint
arXiv:2008.05556, 2020.

Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack  Rae,  Daan  Wierstra,  and  Demis  Hassabis.   Model-free  episodic  control.   arXiv  
preprint
arXiv:1606.04460, 2016a.

Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack  Rae,  Daan  Wierstra,  and  Demis  Hassabis.   Model-free  episodic  control.   arXiv  
preprint
arXiv:1606.04460, 2016b.

Xinyue Chen,  Zijian Zhou,  Zheng Wang,  Che Wang,  Yanqiu Wu,  and Keith Ross.   BAIL: Best-
action imitation learning for batch deep reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020.

Will Dabney, Mark Rowland, Marc G Bellemare, and Re´mi Munos.  Distributional reinforcement
learning with quantile regression.  In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.

Robert  Dadashi,  Shideh  Rezaeifar,  Nino  Vieillard,  Le´onard  Hussenot,  Olivier  Pietquin,  and
Matthieu  Geist.    Offline  reinforcement  learning  with  pseudometric  learning.    arXiv  
preprint
arXiv:2103.01948, 2021.

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

Scott Fujimoto, David Meger, and Doina Precup.  Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.

Matthieu Geist,  Bruno Scherrer,  and Olivier Pietquin.   A theory of regularized markov decision
processes. In International Conference on Machine Learning, pp. 2160–2169. PMLR, 2019.

Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu.  EMaQ: Expected-
max Q-learning operator for simple yet effective offline and online RL.  In International Confer-
ence on Machine Learning, pp. 3682–3691. PMLR, 2021.

Hao Hu, Jianing Ye, Zhizhou Ren, Guangxiang Zhu, and Chongjie Zhang.  Generalizable episodic
memory for deep reinforcement learning. arXiv preprint arXiv:2103.06469, 2021.

Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.  When to trust your model:  Model-
based  policy  optimization.    Advances  in  Neural  Information  Processing  Systems,  32:12519–
12530, 2019.

Ying Jin, Zhuoran Yang, and Zhaoran Wang.   Is pessimism provably efficient for offline RL?   In

International Conference on Machine Learning, pp. 5084–5096. PMLR, 2021.

Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.  Morel: Model-
based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.

Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum.  Offline reinforcement learning
with fisher divergence critic regularization.  In International Conference on Machine Learning,
pp. 5774–5783. PMLR, 2021.

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-
learning via bootstrapping error reduction.  Advances in Neural Information Processing Systems,
32:11784–11794, 2019.

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.

Jongmin  Lee,  Wonseok  Jeon,  Byung-Jun  Lee,  Joelle  Pineau,  and  Kee-Eung  Kim.    OptiDICE:
Offline  policy  optimization  via  stationary  distribution  correction  estimation.    arXiv  
preprint
arXiv:2106.10783, 2021.

10


Published as a conference paper at ICLR 2022

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.  Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang. Episodic memory deep Q-networks.
In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 2433–
2439, 2018.

Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

Whitney K Newey and James L Powell.  Asymmetric least squares estimation and testing.  Econo-
metrica: Journal of the Econometric Society, pp. 819–847, 1987.

Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.  Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.

Alexander  Pritzel,  Benigno  Uria,  Sriram  Srinivasan,  Adria  Puigdomenech  Badia,  Oriol  
Vinyals,
Demis Hassabis, Daan Wierstra, and Charles Blundell.  Neural episodic control.  In International
Conference on Machine Learning, pp. 2827–2836. PMLR, 2017.

Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.

Mark Rowland, Robert Dadashi, Saurabh Kumar, Re´mi Munos, Marc G Bellemare, and Will Dab-
ney.  Statistics and samples in distributional reinforcement learning.  In International Conference
on Machine Learning, pp. 5528–5536. PMLR, 2019.

Mark  Rowland,  Will  Dabney,  and  Re´mi  Munos.   Adaptive  trade-offs  in  off-policy  learning. 
  In

International Conference on Artificial Intelligence and Statistics, pp. 34–44. PMLR, 2020.

Noah Y Siegel,  Jost Tobias Springenberg,  Felix Berkenkamp,  Abbas Abdolmaleki,  Michael Ne-
unert,  Thomas  Lampe,  Roland  Hafner,  Nicolas  Heess,  and  Martin  Riedmiller.    Keep  doing
what  worked:  Behavioral  modelling  priors  for  offline  reinforcement  learning.   arXiv  
preprint
arXiv:2002.08396, 2020.

Yunhao Tang.  Self-imitation learning via generalized lower bound Q-learning.  Advances in Neural
Information Processing Systems, 33, 2020.

Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu, and Tong Zhang. Exponentially weighted
imitation learning for batched historical data.  Advances in Neural Information Processing Sys-
tems, 31:6288, 2018.

Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M Kakade.  Instabilities of offline rl
with pre-trained neural representation. arXiv preprint arXiv:2103.04947, 2021.

Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov,
and Hanlin Goh.   Uncertainty weighted Actor-Critic for offline reinforcement learning.   arXiv
preprint arXiv:2105.08140, 2021.

Yiqin Yang, Xiaoteng Ma, Li Chenghao, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and
Qianchuan  Zhao.   Believe  what  you  see:  Implicit  constraint  approach  for  offline  
multi-agent
reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.

Tianhe  Yu,  Garrett  Thomas,  Lantao  Yu,  Stefano  Ermon,  James  Y  Zou,  Sergey  Levine,  
Chelsea
Finn, and Tengyu Ma.   MOPO: Model-based offline policy optimization.   Advances in Neural
Information Processing Systems, 33:14129–14142, 2020.

Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans.  GenDICE: Generalized offline estimation
of stationary values. arXiv preprint arXiv:2002.09072, 2020a.

Shangtong Zhang, Bo Liu, and Shimon Whiteson.  GradientDICE: Rethinking generalized offline
estimation of stationary values.  In International Conference on Machine Learning, pp. 11194–
11203. PMLR, 2020b.

11


Published as a conference paper at ICLR 2022

A    ALGORITHM

A.1    VALUE-BASED EPISODIC MEMORY CONTROL

Algorithm 1 Value-based Episodic Memory Control

Initialize critic networks Vθ1 , Vθ2  and actor network πφ with random parameters θ₁, θ₂, φ

Initialize target networks θ1′        θ₁, θ2′        θ₂

Initialize episodic memory

for t = 1 to T do


for i ∈ {1, 2} do

Sample N transitions

.st, at, rt, st, Rˆ(i)Σ

from M

Update θi ← minθi N −        R    − Vθ (s )Σ

₁ Σ .   (i)                           2

t                              i

end for

if t mod u then

θi′       κθi + (1     κ)θi′

Update Memory

end if
end for

Algorithm 2 Update Memory

for trajectories τ in buffer       do

for st, at, rt, st₊₁ in reversed(τ ) do
for i ∈ {1, 2} do

Compute Rˆ(i)  with Equation 6 and save into buffer M

end for
end for

end for

A.2    AN APPROACH FOR AUTO-TUNING τ

When we have a good estimation of V ∗, for example, when there is some expert data in the dataset,
we can auto-tune τ such that the value learned by EVL is close to the estimation of V ∗. This can be
done by calculating the Monte-Carlo return estimates of each state and selecting good return values
as the estimation of optimal value V˜∗. Based on this target, we develop a method for auto-tuning τ 
.

By parameterizing τ  = sigmoid(ξ) with a differentiable parameter ξ ∈ R, we can auto-tune τ by
minimizing the following loss J (ξ) = ξ(EVˆ(s) − V˜∗).  If (EVˆ(s) − V˜∗) < 0, the differentiable
parameter ξ will become larger and the value estimation EVˆ(s) will become larger accordingly.
Similarly, ξ and EVˆ(s) will become smaller if (EVˆ(s)     V˜∗)  > 0.  The experimental results in
Figure 10 in Appendix D.1 show that auto-tuning can lead to similar performance compared with
manual selection.

12


Published as a conference paper at ICLR 2022

B    THEORETICAL  ANALYSIS

B.1    COMPLETE DERIVATION.

The expectile regression loss (Rowland et al., 2019) is defined as

ER(q; Q, τ ) = EZ∼q  [τ I(Z > q) + (1 − τ )I(Z ≤ q)] (Z − q)²   ,                (11)

where Q is the target distribution and the minimiser of this loss is called the τ -expectile of Q.  
the
corresponding loss in reinforcement learning is


= Eµ Στ (y − Vθ(s))²  + (1 − τ )(y − Vθ(s))² Σ .

−     (12)

Then, taking the gradient of the value objective with respect to Vθ(s), we have

∇JV (θ) = Σ µ(a | s) [−2τ (y − Vθ(s))₊I(y > Vθ(s)) − 2(1 − τ )(y − Vθ(s))₊I(y ≤ Vθ(s))]

= Σ µ(a | s) [−2τ (y − Vθ(s))₊ − 2(1 − τ )(y − Vθ(s))−]

= Σ µ(a | s) [−2τ (δ)₊ − 2(1 − τ )(δ)−] .


Therefore,

Vˆ(s) = Vθ(s) − α∇JV (θ)

= Vθ(s) + 2αEₐ∼µ [τ [δ(s, a)]₊ + (1 − τ )[δ(s, a)]−]

(13)

(14)

B.2    PROOF OF LEMMA 1

Lemma 1.  For any τ ∈ [0, 1), T µ is a γτ -contraction, where γτ = 1 − 2α(1 − γ) min{τ, 1 − τ }.

Proof.  Note that T µ   is the standard policy evaluation Bellman operator for µ, whose fixed point 
is

V µ. We see that for any V₁, V₂,

µ                   µ


T₁/₂V₁(s) − T₁/₂V₂(s)

= V₁(s) + αEₐ∼µ[δ₁(s, a)] − (V₂(s) + αEₐ∼µ[δ₂(s, a)])

= (1 − α)(V₁(s) − V₂(s)) + αEₐ∼µ[r(s, a) + γV₁(s′) − r(s, a) − γV₂(s′)]

≤ (1 − α)ǁV₁ − V₂ǁ∞ + αγǁV₁ − V₂ǁ∞

= (1 − α(1 − γ))ǁV₁ − V₂ǁ∞.

We introduce two more operators to simplify the analysis:

T₊ V (s) = V (s) + Eₐ∼µ[δ(s, a)]₊,

T− V (s) = V (s) + Eₐ∼µ[δ(s, a)]−.

(15)

(16)

Next we show that both operators are non-expansion (i.e., ǁT µV₁ − T µV₂ǁ∞ ≤ ǁV₁ − V₂ǁ∞). For

+                 +

any V₁, V₂, we have

µ                  µ


T₊ V₁(s) − T₊ V₂(s) = V₁(s) − V₂(s) + Eₐ∼µ[[δ₁(s, a)]₊ − [δ₂(s, a)]₊]

= Eₐ∼µ[[δ₁(s, a)]₊ + V₁(s) − ([δ₂(s, a)]₊ + V₂(s))].

(17)

The relationship between [δ₁(s, a)]₊ + V₁(s) and [δ₂(s, a)]₊ + V₂(s) exists in four cases, which 
are

•  δ₁ ≥ 0, δ₂ ≥ 0, then [δ₁(s, a)]₊ + V₁(s) − ([δ₂(s, a)]₊ + V₂(s)) = γ(V₁(s′) − V₂(s′)).

•  δ₁ < 0, δ₂ < 0, then [δ₁(s, a)]₊ + V₁(s) − ([δ₂(s, a)]₊ + V₂(s)) = V₁(s) − V₂(s).

•  δ₁ ≥ 0, δ₂ < 0, then


[δ₁(s, a)]₊ + V₁(s) − ([δ₂(s, a)]₊ + V₂(s))

= (r(s, a) + γV₁(s′)) − V₂(s)

< (r(s, a) + γV₁(s′)) − (r(s, a) + γV₂(s′))

= γ(V₁(s′) − V₂(s′)),

where the inequality comes from r(s, a) + γV₂(s′) < V₂(s).

13

(18)


Published as a conference paper at ICLR 2022


•  δ₁ < 0, δ₂ ≥ 0, then

[δ₁(s, a)]₊ + V₁(s) − ([δ₂(s, a)]₊ + V₂(s))

= V₁(s) − (r(s, a) + γV₂(s′))

≤ V₁(s) − V₂(s),

(19)

where the inequality comes from r(s, a) + γV₂(s′) ≥ V₂(s).

Therefore, we have T µV₁(s) − T µV₂(s) ≤ ǁV₁ − V₂ǁ∞. With the T µ, T µ, we rewrite T µ as

+                         +                                                                         
        +       −                          τ

µ

Tτ V (s) = V (s) + 2αEₐ∼µ[τ [δ(s, a)]₊ + (1 − τ )[δ(s, a)]−]

= (1 − 2α)V (s) + 2ατ (V (s) + Eₐ∼µ[δ(s, a)]₊) + 2α(1 − τ )(V (s) + Eₐ∼µ[δ(s, a)]−)

= (1 − 2α)V (s) + 2ατ T µV (s) + 2α(1 − τ )T µV (s).


+                                                −

And

T₁/₂V (s) = V (s) + αEₐ∼µ[δ(s, a)]

= V (s) + α(T  V (s) + T  V (s) − 2V (s))

(20)

(21)

= (1 − 2α)V (s) + α(T µV (s) + T µV (s)).

+                        −

We first focus on τ < ¹ . For any V₁, V₂, we have

µ                  µ

Tτ V₁(s) − Tτ V₂(s)

= (1 − 2α)(V₁(s) − V₂(s)) + 2ατ (T µV₁(s) − T µV₂(s)) + 2α(1 − τ )(T µV₁(s) − T µV₂(s))

= (1 − 2α − 2τ (1 − 2α))(V₁(s) − V₂(s)) + 2τ .T µ  V₁(s) − T µ  V₂(s)Σ +

2α(1 − 2τ ) .T µV₁(s) − T µV₂(s)Σ

≤ (1 − 2α − 2τ (1 − 2α))ǁV₁ − V₂ǁ∞ + 2τ (1 − α(1 − γ))ǁV₁ − V₂ǁ∞ + 2α(1 − 2τ )ǁV₁ − V₂ǁ∞

= (1 − 2ατ (1 − γ))ǁV₁ − V₂ǁ∞                                                                       
      (22)

Similarly, when τ > 1/2, we have T µV₁(s) − T µV₂(s) ≤ (1 − 2α(1 − τ )(1 − γ))ǁV₁ − V₂ǁ∞.

τ                 τ

B.3    PROOF OF LEMMA 2

Lemma 2.  For any τ, τ ′ ∈ (0, 1), if τ ′ ≥ τ , we have T µ ≥ T µ, ∀s ∈ S.


Proof.  Based on Equation 20, we have

τ'         τ

µ                 µ

Tτ' V (s) − Tτ V (s)

= (1 − 2α)V (s) + 2ατ ′T µV (s) + 2α(1 − τ ′)T µV (s)

+

µ                                  µ


− ((1 − 2α)V (s) + 2ατ T₊ V (s) + 2α(1 − τ )T− V (s))

= 2α(τ ′ − τ )(T µV (s) − T µV (s))

(23)

= 2α(τ ′ − τ )Eₐ∼µ[[δ(s, a)]₊ − [δ(s, a)]−] ≥ 0.

B.4    PROOF OF LEMMA 3

Lemma 3.  Let V ∗ denote the fixed point of Bellman optimality operator T ∗.  In the deterministic
MDP, we have limτ→₁ Vτ∗ = V ∗.

Proof.  We first show that V ∗ is also a fixed point for T µ.  Based on the definition of T ∗, we 
have
V ∗(s) = maxₐ[r(s, a) + γV ∗(s′)], which infers that δ(s, a) ≤ 0, ∀s ∈ S, a ∈ A.  Thus, we have
T µV ∗(s) = V ∗(s) + Eₐ∼µ[δ(s, a)]₊  = V ∗(s).  By setting (1 − τ ) → 0, we eliminate the effect

of T µ.  Further by the contractive property of T µ, we obtain the uniqueness of V ∗.  The proof is

com−pleted.                                                            τ                            
                          τ

14


Published as a conference paper at ICLR 2022

B.5    PROOF OF LEMMA 4

Lemma 4.  Given τ ∈ (0, 1) and T  ∈ N+, Tvₑm is a γτ -contraction.  If τ > ¹ , Tvₑm has the same

2

fixed point as T µ.

Proof.  We prove the contraction first. For any V₁, V₂, we have


TvₑmV₁(s) − TvₑmV₂(s) =     max

{(T µ)ⁿ−¹T µV₁(s)} −  max  {(T µ)ⁿ−¹T µV₂(s)}


1≤n≤nmax

τ                                                τ

1≤n≤T


≤    max

|(T µ)ⁿ−¹T µV₁(s) − (T µ)ⁿ−¹T µV₂(s)|


1≤n≤nmax

max

n≤nmax

τ                                 τ

γn−1γτ ǁV1  − V2ǁ∞

(24)

≤ γτ ǁV₁ − V₂ǁ∞.

Next we show that Vτ∗,  the fixed point of T µ,  is also the fixed point of Tvₑm  when τ  >  ¹ .  
By


τ

definition, we have    ∗       µ  ∗

∗          µ   ∗

2

µ     ∗          µ   ∗.

Vτ  = Tτ Vτ . Following Lemma 2, we have Vτ  = Tτ Vτ  ≥ T₁/₂Vτ  = T  Vτ

Repeatedly applying T µ and using its monotonicity, we have T µVτ∗ ≥ (T µ)ⁿ−¹Vτ∗, 1 ≤ n ≤ nmₐₓ.
Thus, we have TvₑmVτ∗(s) = max₁≤n≤T {(T µ)ⁿ−¹T µVτ∗(s)} = Vτ∗(s).

B.6    PROOF OF LEMMA 5

Lemma 5.  When the current value estimates V (s) are much lower than the value of behavior policy,

Tvₑm provides an optimistic update. Formally, we have

|TvₑmV (s) − V ∗(s)| ≤ γⁿ∗ (s)−1γτ ǁV − V µ   ǁ∞ + ǁV      − V ∗ǁ∞, ∀s ∈ S,        (25)

where n∗(s) = arg max₁≤n≤T {(T µ)ⁿ−¹T µV (s)} and V µ   is the fixed point of (T µ)ⁿ∗ (s)−1T µ.


Proof.  The lemma is a direct result of the triangle inequality. We have

TvₑmV (s) − Vτ∗(s) = (T µ)ⁿ∗ (s)−1T µV (s) − Vτ∗(s)

= (T µ)n∗(s)−1T µV (s) − (T µ)n∗(s)−1T µV µ

(s) + V µ

(s) − V ∗(s)


≤ γⁿ∗ (s)−1γτ ǁV − V µ   ǁ∞ + ǁV  ∗     − V ∗ǁ.

n  ,τ        τ

(26)

B.7    PROOF OF PROPOSITION 1

Proposition 1.  Let Vτ∗  denote the fixed point of T µ.  For any τ, τ ′  ∈ (0, 1), if τ ′  ≥ τ , we 
have

Vτ∗' (s) ≥ Vτ∗(s), ∀s ∈ S.

Proof.  With the Lemma 2, we have T µV ∗ ≥ T µV ∗.  Since V ∗ is the fixed point of T µ, we have


T µV ∗ = V ∗

τ'   τ

τ    τ                 τ

V ∗ = T

τ

∗  ≤ T µV ∗

T µ and using its monotonicity, we have V ∗ ≤ T µV ∗ ≤ (T µ)∞ V ∗ = V ∗' .

C    DETAILED  IMPLEMENTATION

C.1    GENERALIZED ADVANTAGE-WEIGHTED LEARNING

In practice, we adopt Leaky-ReLU or Softmax functions.
Leaky-ReLU:

max Jπ(φ) = E₍s,ₐ₎∼D Σlog πφ(a | s) · f .Aˆ(s, a)ΣΣ ,


ˆ                .Aˆ(s, a)        if    Aˆ(s, a) > 0

(27)


where    f (A(s, a)) =

Aˆ(s,a)
α

if    Aˆ(s, a) ≤ 0

15


Published as a conference paper at ICLR 2022


Softmax:

max Jπ(φ) = E₍s,ₐ₎∼D

Σlog πφ(a | s) · Σ

exp( ¹ Aˆ(s, a))

(s ,a )∼Batch exp( 1 Aˆ(si, ai))

.          (28)


C.2    BCQ-EM

i     i                                α

The value network of BCQ-EM is trained by minimizing the following loss:

min JQ(θ) = E₍s ,ₐ ,s   ₎∼D Σ(Rt − Qθ(st, at))²Σ                                                   
(29)


Rt =     max

0<n≤nmax

Qt,n

,    Qt,n

=    rt + γQt₊₁,n−₁(st₊₁, aˆt₊₁)    if    n > 0,
Q(st, aˆt)                                   if    n = 0,

(30)

where aˆt corresponds to the perturbed actions, sampled from the generative model Gw(st).
The perturbation network of BCQ-EM is trained by minimizing the following loss:


min Jξ(φ) = −Es∼D [Qθ(s, ai + ξφ(s, ai, Φ))] ,    {ai ∼ Gw(s)}n

,             (31)

where ξφ(s, ai, Φ) is a perturbation model, which outputs an adjustment to an action a in the range
[   Φ, Φ].  We adopt conditional variational auto-encoder to represent the generative model Gw(s)
and it is trained to match the state-action pairs sampled from      by minimizing the cross-entropy
loss-function.

C.3    HYPER-PARAMETER AND NETWORK STRUCTURE

Table 2: Hyper-parameter Sheet
Hyper-Parameter                      Value

Critic Learning Rate                    1e-3

Actor Learning Rate                    1e-3
Optimizer                           Adam

Target Update Rate (κ)                  0.005

Memory Update Period                   100

Batch Size                             128

Discount Factor                        0.99

Gradient Steps per Update                200

Maximum Length d        Episode Length T

Table 3: Hyper-Parameter τ used in VEM across different tasks
AntMaze-fixed         umaze          medium         large

AntMaze-diverse       umaze          medium         large
Adroit-human           door           hammer          pen
Adroit-cloned           door           hammer          pen
Adroit-expert            door           hammer          pen
MuJoCo-medium     walker2d     halfcheetah     hopper
MuJoCo-random     walker2d     halfcheetah     hopper

We   use   a   fully   connected   neural   network   as   a   function   approximation   with   
256   hid-
den   units   and   ReLU   as   an   activation   function.       The   structure   of   the   
actor   network
is   [(state dim, 256), (256, 256), (256, action dim)].       The   structure   of   the   value   
network   is
[(state dim, 256), (256, 256), (256, 1)].

16


Published as a conference paper at ICLR 2022

D    ADDITIONAL  EXPERIMENTS  ON  D4RL

D.1    ABLATION STUDY


400

200

0

VEM

VEM (abs)

600

400

200

0

−200

VEM

VEM (abs)

100

50

0

VEM

VEM (abs)

2000

1000

0

VEM

VEM (abs)


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(a) door-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(b) hammer-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(c) relocate-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(d) pen-human

Figure 5: Comparison results between expectile loss and quantile loss on Adroit tasks.  We respec-
tively name our algorithm with expectile loss and quantile loss as VEM and VEM (abs).


3000

2000

1000

0

0.0         0.2         0.4         0.6         0.8         1.0

Million Steps

(a) pen-human

            VEM (0.1)

VEM (0.3)

VEM (0.5)

VEM (0.7)

VEM (0.8)

600

400

200

0

0.0         0.2         0.4         0.6         0.8         1.0

Million Steps

(b) door-human

750

500

250

0

250

0.0         0.2         0.4         0.6         0.8         1.0

Million Steps

(c) hammer-human


10000

8000

6000

4000

2000

VEM (0.1)

VEM (0.3)

VEM (0.5)

VEM (0.7)

VEM (0.8)

800

600

400

10000

5000


0

0.0         0.2         0.4         0.6         0.8         1.0

Million Steps

(d) pen-human

200

0.0         0.2         0.4         0.6         0.8         1.0

Million Steps

(e) door-human

0

0.0         0.2         0.4         0.6         0.8         1.0

Million Steps

(f) hammer-human

Figure 6:  The results of VEM (τ ) with various τ in Adroit tasks.  The results in the upper row are
the performance. The results in the bottom row are the estimation value.


2000

1000

0

VEM
TD3+BC(0.5)
TD3+BC(2.5)
TD3+BC(4.5)

400

200

0

VEM
TD3+BC(0.5)
TD3+BC(2.5)
TD3+BC(4.5)

600

400

200

0

−200

VEM
TD3+BC(0.5)
TD3+BC(2.5)
TD3+BC(4.5)

100

50

0

VEM
TD3+BC(0.5)

TD3+BC(2.5)

        

TD3+BC(4.5)


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(a) pen-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(b) door-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(c) hammer-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(d) relocate-human


×1012

×1012

×1012

×1012


6                                          VEM

TD3+BC(0.5)                 4

4                                          TD3+BC(2.5)

TD3+BC(4.5)

2                                                                              2

           VEM                                6

TD3+BC(0.5)

TD3+BC(2.5)                 4

TD3+BC(4.5)

2

          VEM                                4

TD3+BC(0.5)

TD3+BC(2.5)
TD3+BC(4.5)

2

VEM

          TD3+BC(0.5)

TD3+BC(2.5)
TD3+BC(4.5)


0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(e) pen-human

0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(f) door-human

0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(g) hammer-human

0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(h) relocate-human

Figure 7:  Comparison results between VEM with TD3+BC. We adopt different hyper-parameters
α      0.5, 2.5, 4.5   in TD3+BC to test its performance.  The upper row are the performance.  The
results in the bottom row are the estimation error (the unit is 10¹²).

17


Published as a conference paper at ICLR 2022


0.8

0.6

0.4

0.2

0.0

VEM

VEM-1step
VEM-nstep

0.75

0.50

0.25

0.00

VEM

VEM-1step
VEM-nstep

0.6

0.4

0.2

0.0

          VEM

VEM-1step
VEM-nstep

0.6

0.4

0.2

0.0

VEM

VEM-1step
VEM-nstep


0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(a) medium-play

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(b) medium-diverse

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(c) large-play

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(d) large-diverse

Figure 8: The comparison between episodic memory and n-step value estimation on AntMaze tasks.


400

200

0

VEM

           BCQ-EM
BCQ

1000

500

0

VEM

           BCQ-EM
BCQ

100

75

50

25

0

VEM

          BCQ-EM
BCQ

2000

1000

0

VEM
BCQ-EM
BCQ


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(a) door-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(b) hammer-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(c) relocate-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(d) pen-human


×1013

4

2

VEM
BCQ-EM
BCQ

1.00

0.75

0.50

0.25

×1014

VEM
BCQ-EM
BCQ

×1013

6

4

2

VEM
BCQ-EM
BCQ

×1013

8

6

4

2

VEM
BCQ-EM
BCQ


0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(e) door-human

0.00

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(f) hammer-human

0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(g) relocate-human

0

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(h) pen-human

Figure 9: The comparison between VEM, BCQ-EM and BCQ on Adroit-human tasks.  The results
in the upper row are the performance. The results in the bottom row are the estimation error, where
the unit is 10¹³.

D.2    COMPLETE TRAINING CURVES AND VALUE ESTIMATION ERROR


400

200

VEM

VEM(auto)

0.5

0.4

0.3

VEM(auto)

0                                                                              0.2


0.0            0.1            0.2            0.3            0.4

Million Steps

(a) Episode return

0.0            0.1            0.2            0.3            0.4

Million Steps

(b) τ value

Figure 10: Comparison between fixed τ (VEM) and auto-tuning τ (VEM(auto)) in the door-human
task.

18


Published as a conference paper at ICLR 2022


600

400

200

0

        

VEM(1)
VEM(1000)

6000

4000

2000

0

        

VEM(1)
VEM(1000)

2000

1500

1000

500

0

VEM(1)
VEM(1000)

4000

2000

VEM(1)
VEM(1000)


0.0            0.1            0.2            0.3            0.4

Million Steps

(a) door-human

0.0            0.1            0.2            0.3            0.4

Million Steps

(b) hammer-human

0.0            0.1            0.2            0.3            0.4

Million Steps

(c) relocate-human

0.0            0.1            0.2            0.3            0.4

Million Steps

(d) pen-human

Figure 11:  Value estimation of VEM (nmₐₓ) in adroit-human tasks,  where nmₐₓ  is the maximal
rollout step for memory control (see Equation 11). We set τ = 0.5 in all tasks.


0.75

0.50

0.25

0.00

VEM
BAIL

0.8

0.6

0.4

0.2

0.0

VEM
BAIL

0.8

0.6

0.4

0.2

0.0

VEM
BAIL

0.75

0.50

0.25

0.00

VEM
BAIL


0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(a) antmaze-umaze       (b)           antmaze-umaze- (c)        antmaze-medium- (d)         
antmaze-medium-


diverse

play

diverse


0.6

0.4

0.2

0.0

          VEM

BAIL

0.6

0.4

0.2

0.0

          VEM

BAIL

400

200

0

VEM
BAIL

600

400

200

0

−200

VEM
BAIL


0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(e) antmaze-large-play

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(f) antmaze-large-diverse

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(g) door-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(h) hammer-human


100

50

0

VEM
BAIL

2000

1000

0

VEM
BAIL

150

100

50

0

−50

VEM
BAIL

400

200

0

−200

VEM
BAIL


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(i) relocate-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(j) pen-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(k) door-cloned

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(l) hammer-cloned


2500

2000

1500

1000

VEM
BAIL

2000

1500

1000

500

        

VEM

BAIL

3000

2000

1000

VEM
BAIL

4000

2000

0

VEM
BAIL


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(m) pen-cloned

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(n) hopper-medium

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(o) walker2d-medium

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(p) halfcheetah-medium


400

300

200

100

0

VEM
BAIL

600

400

200

VEM
BAIL

2000

1000

0

VEM
BAIL


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(q) hopper-random

0.0            0.1            0.2            0.3            0.4

Million Steps

(r) walker2d-random

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(s) halfcheetah-random

Figure 12: The training curves of VEM and BAIL on D4RL tasks.

19


Published as a conference paper at ICLR 2022


0.6

0.5

0.4

0.3

VEM

0.4

0.3

0.2

0.1

0.0

VEM

0.4

0.3

0.2

0.1

0.0

VEM

0.4

0.3

0.2

0.1

VEM


0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(a) antmaze-umaze      (b)           antmaze-umaze- (c)        antmaze-medium- (d)        
antmaze-medium-


diverse

play

diverse


0.15

0.10

0.05

VEM

0.100

0.075

0.050

0.025

VEM

800

600

400

200

VEM

4000

2000

VEM


0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(e) antmaze-large-play

0.0        0.1        0.2        0.3        0.4        0.5

Million Steps

(f) antmaze-large-diverse

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(g) door-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(h) hammer-human


2000

1500

1000

VEM

20000

15000

10000

5000

VEM

400

300

200

100

0

VEM

2000

1500

1000

500

VEM


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(i) relocate-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(j) pen-human

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(k) door-cloned

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(l) hammer-cloned


800

600

400

VEM

150

100

50

VEM

100

80

60

40

VEM

600

400

200

VEM


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(m) pen-cloned

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(n) hopper-medium

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(o) walker2d-medium

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(p) halfcheetah-medium


400

300

200

100

0

VEM

6000

4000

2000

0

VEM

100

80

60

40

VEM


0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(q) hopper-random

0.0            0.1            0.2            0.3            0.4

Million Steps

(r) walker2d-random

0.0        0.2        0.4        0.6        0.8        1.0

Million Steps

(s) halfcheetah-random

Figure 13:  The value estimation error of VEM on D4RL tasks.  The estimation error refers to the
average estimated state values minus the average returns.

20

