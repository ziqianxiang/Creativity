Published as a conference paper at ICLR 2022
Federated Learning from Only Unlabeled Data
with Class-Conditional-Sharing Clients
NanLuI ZhaoWang2 Xiaoxiao Li3 Gang Niu4* QiDou2 Masashi Sugiyama4,1
1The University of Tokyo 2The Chinese University of Hong Kong
3The University of British Columbia 4RIKEN
{lu@edu.,sugi@}k.u-tokyo.ac.jp, {zwang21@cse.,qidou@}cuhk.edu.hk
xiaoxiao.li@ece.ubc.ca, gang.niu.ml@gmail.com
Ab stract
Supervised federated learning (FL) enables multiple clients to share the trained
model without sharing their labeled data. However, potential clients might even
be reluctant to label their own data, which could limit the applicability of FL in
practice. In this paper, we show the possibility of unsupervised FL whose model is
still a classifier for predicting class labels, if the class-prior probabilities are shifted
while the class-conditional distributions are shared among the unlabeled data
owned by the clients. We propose federation of unsupervised learning (FedUL),
where the unlabeled data are transformed into surrogate labeled data for each of
the clients, a modified model is trained by supervised FL, and the wanted model is
recovered from the modified model. FedUL is a very general solution to unsuper-
vised FL: it is compatible with many supervised FL methods, and the recovery of
the wanted model can be theoretically guaranteed as if the data have been labeled.
Experiments on benchmark and real-world datasets demonstrate the effectiveness
of FedUL. Code is available at https://github.com/lunanbit/FedUL.
1	Introduction
Federated learning (FL) has received significant attention from both academic and industrial perspec-
tives in that it can bring together separate data sources and allow multiple clients to train a central
model in a collaborative but private manner (McMahan et al., 2017; Kairouz et al., 2019; Yang et al.,
2019). So far, the majority of FL researches focused on the supervised setting, requiring collected
data at each client to be fully labeled. In practice, this may hinder the applicability of FL since
manually labeling large-scale training data can be extremely costly, and sometimes may not even be
possible for privacy concerns in for example medical diagnosis (Ng et al., 2021).
To promote the applicability of FL, we are interested in a challenging unsupervised FL setting where
only unlabeled (U) data are available at the clients under the condition described blow. Our goal is
still to train a classification model that can predict accurate class labels. It is often observed that the
clients collect their own data with different temporal and/or spatial patterns, e.g., a hospital (data
center) may store patient data every month or a particular user (mobile device) may take photos at
different places. Therefore, we consider a realistic setting that the U data at a client come in the form
of separate data sets, and the data distribution of each U set and the number of U sets available at
each client may vary. Without any labels, it is unclear whether FL can be performed effectively.
In this paper, we show the possibility: the aforementioned unsupervised FL problem can be solved if
the class-prior probabilities are shifted while the class-conditional distributions are shared between the
available U sets at the clients, and these class priors are known to the clients. Such a learning scenario
is conceivable in many real-world problems. For example, the hospital may not release the diagnostic
labels of patients due to privacy concerns, but the morbidity rates of diseases (corresponding to the
class priors) may change over time and can be accessible from public medical reports (Croft et al.,
2018); moreover, in many cases it is possible to estimate the class priors much more cheaply than to
collect ground-truth labels (Quadrianto et al., 2009a; Sugiyama et al., 2022).
* Correspondence to: Gang Niu <gang.niu.ml@gmail.com>.
1
Published as a conference paper at ICLR 2022
Server
Server update
Local update
Server
Qi
f2
True label
Client 2
Client 2
Client C
Client 1
Client C
Client 1
In the left panel, each client fc (c ∈ [C]) has access to fully labeled data, and a global model f can be trained
using a supervised FL scheme, e.g., Federated Averaging (FedAvg) (McMahan et al., 2017). In the right panel,
each client has access to only unlabeled (U) data coming in the form of separate sets, where supervised FL
methods cannot be directly applied. We propose FedUL that treats the indexes of the U sets as surrogate labels,
transforms the local U datasets to (surrogate) labeled datasets, and formulates a surrogate supervised FL task.
Then we modify each client model to be compatible with the surrogate task by adding a fixed transition layer Qc
to the original model output fc . FedUL can incorporate existing supervised FL methods as its base method for
surrogate training, and the wanted model f can be retrieved from the surrogate model.
Figure 1: Illustration of standard supervised FL scheme vs. proposed FedUL scheme.
True label
True label
fl
Q2
fc
QC
Surrogate label
Surrogate label
Surrogate label
Based on this finding, we propose the federation of unsupervised learning (FedUL) (see Figure 1).
In FedUL, the original clients with U data are transformed to surrogate clients with (surrogate)
labeled data and then supervised FL can be applied, which we call the surrogate task. Here, the
difficulty is how to infer the wanted model for the original classification task from the model learned
by the surrogate task. We solve this problem by bridging the original and surrogate class-posterior
probabilities with certain injective transition functions. This can be implemented by adding a
specifically designed transition layer to the output of each client model, so that the learned model is
guaranteed to be a good approximation of the original class-posterior probability.
FedUL has many key advantages. On the one hand, FedUL is a very general and flexible solution: it
works as a wrapper that transforms the original clients to the surrogate ones and therefore is compatible
with many supervised FL methods, e.g., FedAvg. On the other hand, FedUL is computationally
efficient and easy-to-implement: since the added transformation layer is fixed and determined by
the class priors, FedUL adds no more burden on hyper-parameter tuning and/or optimization. Our
contributions are summarized as follows:
•	Methodologically, we propose a novel FedUL method that solves a certain unsupervised FL
problem, expanding the applicability of FL.
•	Theoretically, we prove the optimal global model learned by FedUL from only U data converges to
the optimal global model learned by supervised FL from labeled data under mild conditions.
•	Empirically, we demonstrate the effectiveness of FedUL on benchmark and real-world datasets.
Related Work: FL has been extensively studied in the supervised setting. One of the most popular
supervised FL paradigms is FedAvg (McMahan et al., 2017) which aggregates the local updates at
the server and transmits the averaged model back to local clients. In contrast, FL in the unsupervised
setting is less explored. Recent studies have shown the possibility of federated clustering with U
data (Ghosh et al., 2020; Dennis et al., 2021). However, these clustering based methods rely on
geometric or information-theoretic assumptions to build their learning objectives (Chapelle et al.,
2006), and are suboptimal for classification goals. Our work is intrinsically different from them in
the sense that we show the possibility of federated classification with U data based on empirical risk
minimization (ERM) (Vapnik, 1998), and therefore the optimal model recovery can be guaranteed.
Learning from multiple U sets has also been studied in classical centralized learning. Mainstream
methods include learning with label proportions (LLP) (Quadrianto et al., 2009b) and Um classi-
fication (Lu et al., 2021). The former learns a multiclass classifier from multiple U sets based on
2
Published as a conference paper at ICLR 2022
empirical proportion risk minimization (EPRM) (Yu et al., 2014), while the latter learns a binary
classifier from multiple U sets based on ERM. EPRM is inferior to ERM since its learning is not
consistent. Yet, it is still unexplored how to learn a multiclass classification model from multiple
U sets based on ERM. To the best of our knowledge, this work is the first attempt to tackle this
problem in the FL setup, which is built upon the binary Um classification in centralized learning. A
full discussion of related work can be found at Appendix B.
2	Standard Federated Learning
We begin by reviewing the standard FL. Let us consider a K-class classification problem with
the feature space X ⊂ Rd and the label space Y = [K], where d is the input dimension and
[K] := {1, . . . , K}. Let x ∈ X and y ∈ Y be the input and output random variables following
an underlying joint distribution with density p(x, y), which can be identified via the class priors
{πk = p(y = k)}kK=1 and the class-conditional densities {p(x | y = k)}kK=1. Let f : X → RK be a
classification model that assigns a score to each of the K classes for a given input x and then outputs
the predicted label by ypred = argmaxk∈[K] (f (x))k, where (f (x))k is the k-th element of f (x).
In the standard FL setup with C clients, for c ∈ [C], the c-th client has access to a labeled training set
Dc = {(xC, yc)}n=cι〜Pc(x, y) of sample size n and learns its local model f by minimizing the risk
Rc(fc) := E(x,y)~pc(x,y) ['(fc(X),y)].	(I)
Here, E denotes the expectation, ` : RK × Y → R+ is a proper loss function, e.g., the
softmax cross-entropy loss 'ce(fc(x),y) = -PK=I 1(y = k)log (PKxLPexf(f))X)%) =
log (PK=I exp((fc(x))i)) - (fc(x))y, where 1(∙) is the indicator function. In practice, ERM
is commonly used to compute an approximation of Rc(fc) based on the client’s local data Dc by
Rc(fc; Dc) =《 pn=c 1 '(fc(χc),yC).
The goal of standard FL (McMahan et al., 2017) is that the C clients collaboratively train a global
classification model f that generalizes well with respect to p(x, y), without sharing their local data
Dc . The problem can be formalized as minimizing the aggregated risk:
R(f) = ɪ XC 1 Rc(f).	⑵
C c=1
Typically, we employ a server to coordinate the iterative distributed training as follows:
•	in each global round of training, the server broadcasts its current model f to all the clients {fc}cC=1;
•	each client c copies the current server model fc = f, performs L local step updates
fc J fc - αι ∙ VRbc(fc; Dc),	(3)
where αl is the local step-size, and sends fc - f back to the server;
•	the server aggregates the updates {fc - f}cC=1 to form a new server model using FedAvg:
C
f J f-αg ∙ y^c ι(fc - f),	(4)
where αg is the global step-size.
3	Federated Learning: No Label No Cry
Next, we formulate the problem setting of FL with only U training sets, propose our method called
federation of unsupervised learning (FedUL), and provide the theoretical analysis of FedUL. All
proofs are given in Appendix C.
3.1	Problem Formulation
In this paper, we consider a challenging setting: instead of fully labeled training data, each client
c ∈ [C] observes Mc (Mc ≥ K, ∀c)1 sets of unlabeled samples, Uc = {Ucm}mM=c 1, where Ucm =
1Note that we do not require each client has the same number of U sets. We only assume the condition
Mc ≥ K, ∀c to ensure the full column rank matrix Πc, which is essential for FL with U training sets.
3
Published as a conference paper at ICLR 2022
{xic,m}in=c,1m denotes the collection of the m-th U set for client c with sample size nc,m. Each U set
can be seen as a set of data points drawn from a mixture of the original class-conditional densities:
K
Um 〜pm(x)= E	∏m,kp(x ∣ y = k),	(5)
k=1
where πcm,k = pcm(y = k) denotes the k-th class prior of the m-th U set at client c. These class priors
form a full column rank matrix Πc∈ RMc×K with the constraint that each row sums up to 1. Note
that we do not require any labels to be known, only assume the class priors within each of the involved
datasets are available, i.e., the training class priors πcm,k and the test class prior πk = p(y = k).
Our goal is the same as standard FL: the C clients are interested in collaboratively training a global
classification model f that generalizes well with respect to p(x, y), but using only U training sets (5)
on each local client.
3.2	Proposed Method
As discussed, FedAvg is a representative approach for aggregating clients in the FL framework. It
also serves as the basis for many advanced federated aggregation techniques, e.g., Fedprox (Li et al.,
2020), FedNova (Wang et al., 2020), and SCAFFOLD (Karimireddy et al., 2020). However, these
methods cannot handle clients without labels. To tackle this problem, we consider the construction of
surrogate clients by transforming the local U datasets into labeled datasets and modifying the local
model such that it is compatible with the transformed data for training.
Local Data Transformation First, we transform the data for the clients. For each client c ∈ [C],
let y be the index of given U sets, and pc(x, y) be an underlying joint distribution for the random
variables X ∈ X and y ∈ [M], where M = maxc∈[c] M，By treating y as a surrogate label, We
may transform the original U training sets Uc= {Ucm}mM=c 1 to a labeled training set for client c,
Uc = {xi,yi}n= 1 〜PC(X,历，	⑹
where nc = PmM=c 1 nc,m . Obviously, the surrogate class-conditional density PC(X | y = m)
corresponds to Pm(X) in (5), for m ∈ [MJ If ∃Mc < m ≤ M, we padPC(X | y = m) = 0. The
surrogate class prior ∏m = pc(y = m) can be estimated by ncm/n0.
Based on the data transformation, we then formulate a surrogate supervised FL task: the C clients
aim to collaboratively train a global classification model g : X → RM that predicts the surrogate
label y for the given input x, without sharing their local data Uc. This task can be easily solved by
standard supervised FL methods. More specifically, we minimize
1 LC
J(g) = C Xc=I JCs)= E(χ,y)〜Pc(x,y)['(g(X),y)]	⑺
by employing a server g that iteratively broadcasts its model to all local clients {gc}cC=1 and aggregates
local updates from all clients using e.g., FedAvg. The local and global update procedures exactly
follow (3) and (4) in standard FL.
Now a natural question arises: can we infer our desired model f from the surrogate model g ? To
answer it, we study the relationship between the original and surrogate class-posterior probabilities.
Theorem 1. For each client C ∈ [C], let ηc : X → Δm-i and η : X → ∆κ-ι be the surrogate
and original ClaSS-posteriorprobability functions, where (η∕X))m = p/y = m | x) for m ∈ [M ],
(η(X))k = P(y = k | X) for k ∈ [K], ∆M-1 and ∆K-1 are the M- and K -dimensional simplexes.
Let ∏c = [∏c, ∙∙∙ ,∏M ]> and π = [∏1, ∙∙∙ , ∏K ]> be vector forms of the surrogate and original
class priors, and Πc ∈ RM×K be the matrix form of πcm,k defined in (5). Then we have
ηc(X) = Qc(η(X); ∏,∏c,∏c),	⑻
where the unnormalized version of vector-valued function Qc is given by
ιx	1	11
Qc(η(X); π, ∏c,∏c) = Dnc ∙∏c ∙ Dn ∙ η(X).
Da denotes the diagonal matrix with diagonal terms being vector a and ∙ denotes matrix multiplica-
tion. Qc is normalized by the sum of all entries, i.e., (Qc)i
(Qc)i
Pj (Qec) j'
2To make the problem well-defined, for client C with Mc < M, we set Um = 0, ∀Mc < m ≤ M.
4
Published as a conference paper at ICLR 2022
Algorithm 1 Federation of unsupervised learning (FedUL)
Server Input: initial f, global step-size ɑg, and global communication round R
Client c’s Input: local model fc, unlabeled training sets Uc = {Ucm}mM=c 1, class priors Πc and π,
local step-size αl, and local updating iterations L
1:	We start with initializing clients with Procedure A.
2:	For r = 1 → R rounds, we run Procedure B and Procedure C iteratively .
3:	procedure A. ClientInit(c)	_
4:	transform Uc to a surrogate labeled dataset UC according to (6)
5:	modify f to gc = Qc (f), where Qc is computed according to Theorem 1
6:	end procedure
7:	procedure B . CLIENTUPDATE(c)
8:	fc J f	. Receive updated model from SERVEREXECUTE
9:	gc J Qc(fc)
10:	for l = 1 → L do
11:	gc J gc 一 αι ∙ VJc(gc; Uc)	. SGD update based on objective (7)
12:	fc J fc 一 αι ∙ VJc(Qc(fc); Uc)	. The update on gc induces an update on fc
13:	end for
14:	send fc 一 f to SERVEREXECUTE
15:	end procedure
16:	procedure C. SERVEREXECUTE(r)
17:	receive local models’ updates {fc 一 f}cC=1 from CLIENTUPDATE
18:	f J f ― αg ∙ PC=ι(fc - f)	. FL aggregation
19:	broadcast f to CLIENTUPDATE
20:	end procedure
Note that π, πc, and Πc are all fixed, Qc is deterministic and can be identified with the knowledge of
π, Πc, and an estimate of πc. In addition, we note that if Mc < M, the last M 一 Mc entries of Qc
are padded with zero by construction. We further study the property of Qc in the following lemma.
Lemma 2. For each client c ∈ [C], the transition function Qc : (X → ∆K-1) → (X → ∆M-1)
defined in Theorem 1 is an injective function.
In this sense, the transition function Qc naturally bridges the class-posterior probability of our desired
task η(x) and that of the learnable surrogate task ηc(x) given only U training sets. This motivates
us to embed the estimation of η(x) into the estimation of ηc(x) at the client side.
Local Model Modification Second, we modify the model for the clients. Let f(x) be the original
global model output that estimates η(x). At each client c ∈ [C], we implement Qc by adding a
transition layer following fc and then we have gc(x) = Qc(fc(x)). Based on the local model
modification, (7) can be reformulated as follows:
J(f) = CC Xc=1 Jc(Qc(f)) ：= E(χ,y)〜Pc(χ,y)['(Qc(f (x)),y)].	(9)
Therefore, local updates on the surrogate model gc naturally yield
updates on the underlying model f. Now we can aggregate the
surrogate clients by a standard FL scheme. A detailed algorithm,
using FedAvg as an example, is given in Algorithm 1.
Remarks The relationship elucidating the supervised and sur-
rogate FL paradigms (for a single client) is visually shown in
Figure 2. The blue part illustrates a supervised FL scheme: the
global model is f(x) parameterized by θ; the loss is R(f; y) in
(2) where labels y are observed. However, we cannot observe y
and thus employ a surrogate FedUL scheme which is shown in
green: the global model is g (x) directly modified from f ; the
loss is J(g; y) in (7) where y are surrogate labels.
Figure 2: Graphical representa-
tion of supervised FL (in blue)
and FedUL (in green).
5
Published as a conference paper at ICLR 2022
3.3 Theoretical Analysis
In what follows, we provide theoretical analysis on the convergence and optimal model recovery for
the proposed method.
Convergence of the Algorithm In supervised FL, convergence analysis has been studied with
regularity conditions (see Section 3.2.2 in Kairouz et al. (2019) for a comprehensive survey), e.g.,
the bounded gradient dissimilarity (Karimireddy et al., 2020) that associates the client model with
the server model for non-IID data distributions.3 In our analysis, we focus on the behaviour of our
surrogate learning objective J with the effect from transformation Qc .
Proposition 3 (Theorem V in Karimireddy et al. (2020)). Assume that J(g) in (7) satisfies
*	1C
(A1)-(A3) in Appendix C.3. Denote g = argmιng∈g J(g), where J(g) = C Ec=ι Jc(g)：=
十 ∑n= ι '(g(xi),Vi). Let the global step-size αg ≥ 1 and local step-size ɑι ≤ 8(1+62).二二,
FedUL given by Algorithm 1 is expected to have contracting gradient: with the initialized model g0,
F := J(g0) — J(g*) and constant M = σ
, in R rounds, the learned model gR satisfies
βM√F	β1/3(FG)2/3	βB2F
E[kvJ(gR)k2]≤o ‰ 十 /% 十 %-
Note that the proposition statement is w.r.t. the convergence ofg learned from the surrogate task. By
model construction, g is built from Qc(f). By Lemma 2, the convergence of trained model g to the
optimal model g* in the function class G implies the convergence of the retrieved model f to the
optimal model f * = argmmf∈f J(f) In the corresponding function class F (induced by G), where
J(f) = C PC=I Jc(Qc(f)) := n1c Pi= 1 '(Qc(f (Xi)),yi) is the empirical version of (9).
Optimal Model Recovery In our context, the model is optimal in the sense that the minimizer of
the surrogate task J(f; y) coincide with the minimizer of the supervised task R(f; y), as if all the
data have been labeled. Based on the convergence of surrogate training, we show that FedUL can
recover this optimal model.
Theorem 4. Assume that the cross-entropy loss is chosen for `, and the model used for g
is very flexible, e.g., deep neural networks, so that g** ∈ G where g** = argmin。J(g; y).
Let f* = argminf ∈F J(f) be the optimal classifier learned with FedUL by Algorithm 1, and
f? = argminf R(f; y) be the optimal classifier learned with supervised FL. By assumptions in
Proposition 3, we have f* = f?.
4	Experiments
In this section, we conduct experiments to validate the effectiveness of the proposed FedUL method
under various testing scenarios. Compared with the baseline methods, FedUL consistently achieves
superior results on both benchmark and real-world datasets. The detailed experimental settings and
additional supportive results are presented in Appendix D & E.
4.1	Benchmark Experiments
Setup: As proof-of-concepts, we first perform experiments on widely adopted benchmarks MNIST
(LeCun et al., 1998) and CIFAR10 (Krizhevsky et al., 2009). We generate M sets of U training data
according to (5) for each client C ∈ [C]. The total number of training samples nc on each client is
fixed, and the number of instances contained in each set indexed by m is also fixed as nc/M. In order
to simulate the real world cases, we uniformly select class priors πcm,k from range [0.1, 0.9] and then
regularize them to formulate a valid Πc ∈ RM×K as discussed in Section 3.1.
For model training, we use the cross-entropy loss and Adam (Kingma & Ba, 2015) optimizer with
a learning rate of 1e—4 and train 100 rounds. If not specified, our default setting for local update
3The IID case is equivalent to batch sampling from the same distribution, where the analysis is less interesting
associated with our FL setting and we omit details here.
6
Published as a conference paper at ICLR 2022
Table 1: Quantitative comparison of our method with the baseline methods on benchmark datasets
under IID and Non-IID setting (mean error (std)). The best method (paired t-test at significance level
5%) is highlighted in boldface.
IID Task with 5 Clients
Dataset	Sets	FedPL	FedLLP	FedLLP-VAT	FedUL	FedAvg 10%
MNIST	-10- 20 40	2.12 (0.10)~19.00(15.90)~20.53 (5.39) 2.98 (0.15)	4.38 (3.95)	4.40 (5.11) 3.90 (0.22)	10.11 (15.74)	11.96 (9.56)	0.78 (0.06) 1.12 (0.09) 1.00 (0.29)	1.79 (0.09)
CIFAR10	-10- 20 40	34.60 (0.30)~77.01 (5.02)	77.56 (8.71) 42.06 (1.63)	78.74 (6.96)	84.18 (5.95) 46.20 (0.67)	73.75 (4.78)	78.65 (6.91)	18.56 (0.04) 19.66 (0.30) 20.31 (0.69)	32.85 (1.03)
Non-IID Task with 5 Clients				
Dataset	Sets	-FedPL	FedLLP	FedLLP-VAT	-FedUL-	FedAvg 10%
MNIST	-10- 20 40	15.54(3.48)~20.52(15.00)~23.39 (15.34) 7.54 (2.77)	6.63 (7.94)	11.76 (6.60) 5.76 (0.90)	5.53 (2.51)	7.82 (10.73)	2.98 (1.72) 1.77 (0.49) 1.64 (0.18)	3.82 (1.56)
CIFAR10	-10- 20 40	70.65 (2.38)~81.12(5.41)	81.55 (7.09) 63.53 (1.24)	65.51 (13.16)	70.51 (7.66) 53.78 (0.13)	65.72 (3.93)	60.23 (5.71)	42.92 (3.02) 35.24 (0.98) 31.43 (1.72)	58.62 (8.63)
epochs (E) is 1 and batch size (B) is 128 with 5 clients (C) in our FL system. We use LeNet (LeCun
et al., 1998) and ResNet18 (He et al., 2016) as backbone for MNIST and CIFAR10, respectively.
Here, we investigate two different FL settings: IID and Non-IID, where the overall label distribution
across clients is the same in the IID setting, whereas clients have different label distributions in the
non-IID setting. Specially, for Non-IID setting, we impose class-prior shift as follows:
•	the classes are divided into the majority and minority classes, where the fraction of each minority
class is less than 0.08 while the fraction of each majority class is in the range [0.15, 0.25];
•	the training data for every client are sampled from 2 majority classes and 8 minority classes;
•	the test data are uniformly sampled from all classes.
Baselines: (1) Federated Pseudo Labeling (FedPL) (Diao et al., 2021): at each client of a FedAvg
system, we choose the label with the largest class prior in a U set as the pseudo label for all the
data in this set. During local training, these pseudo labels are boosted by Mixup (Zhang et al., 2017)
high-confidence data and low-confidence data, where the confidence is given by model predictions.
(2) Federated LLP (FedLLP) (Dulac-Arnold et al., 2019): each client model is updated by minimizing
the distance between the ground truth label proportion and the predicted label proportion, and we
aggregate them using FedAvg. (3) FedLLP with Virtual Adversarial Training (FedLLP-VAT) (Tsai
& Lin, 2020): a consistency term is added to FedLLP as regularization for optimizing client model
in FedAvg. (4) FedAvg with 10% labeled data (FedAvg 10%): following Chen et al. (2020), we
also compare with supervised learning based on FedAvg using 10% fully labeled data. The detailed
baseline settings are depicted in Appendix D.
Results: The experimental results of our method and baselines on two benchmark datasets under
IID and Non-IID settings are reported in Table 1, where the mean error (std) over clients based on
3 independent runs are shown. Given the limited number of total data available in the benchmark
datasets, we compare our method with baselines in an FL system with five clients and vary the number
of U sets M ∈ {10, 20, 40} at each client. In this regard, each set contains a sufficient number of
data samples to train the model.
Our observations are as follows. First, the proposed FedUL method outperforms other baseline
methods by a large margin under both IID and Non-IID settings. Second, when varying the number of
sets M ∈ {10, 20, 40}, FedUL not only achieves lower classification error but also shows more stable
performance compared to baselines. Third, as we use FedAvg as the default aggregation strategy for
all the methods, the performances of FedUL, FedPL, and FedAvg all degrade under the inter-client
Non-IID setting, as expected. The error rates of FedLLP and FedLLP-VAT are high and unstable,
which demonstrates that the EPRM based methods are inferior as discussed in Section 1.
7
Published as a conference paper at ICLR 2022
0.2	0.4	0.8	1.6
Noise ratio
(a)
1
10	15	20	25
Accessible Non-IID data (k)
(b)
IT
(c)
16128 4
(％)」。」」① UO=a⅛=SSeO
-64	128	51Γ
Training batch size
(d)
⅛ ⅛
Figure 3:	Ablation studies on MNIST under a Non-IID FL system. (a) Study on the robustness of our
method with noisy class priors. (b) Effects of the number of accessible Non-IID U data. (c) Analysis
of local updating epochs. (d) Effects of training batch size.
4.2	Ablation Studies
In the following, we present a comprehensive investigation on the ablation studies of the proposed
FedUL method on MNIST benchmark.
Robustness with Noisy Class Priors: To investigate the robustness of our method, we further apply
FedUL in a noisy environment. Specifically, we perturb the class priors πcm,k by a random noise
Y, and let the method treat the noisy class prior ∏m,k = ∏m,k ∙ ((2e - 1)γ + 1), where Y uniform
randomly take values in [0, 1] and ∈ {0, 0.2, 0.4, 0.8, 1.6}, as the true πcm,k during the whole
learning process. Note that we tailor the noisy πecm,k if it surpasses the range. We conduct experiments
with five clients and ten U sets at each client. The experimental setup is the same as before except for
the replacement of πcm,k . From Figure 3(a), we can see that FedUL performs stably even with some
challenging noise perturbations (see when noise ratio > 0).
Effects of the Number of Accessible Non-IID U Data: The purpose of FL is utilizing more data to
empower model performance. While how unlabeled data, especially unlabeled Non-IID data, can
contribute to FL model performance needs investigation. To this end, we analyze the effects of the
number of accessible Non-IID U data. We set each client with nc = 500 samples ∀c ∈ [C] and
conduct experiments by changing the amount of Non-IID U data by increasing the number of clients
C ∈ {10, 20, 30, 40, 50}. As shown in Figure 3(b), the model performance improves by a large
margin as the number of accessible Non-IID U data increases by using our proposed FedUL method.
Analysis of Local Updating Epochs: The frequency of aggregation may sometimes influence the
model performance in an FL system. We would like to explore if FedUL is stable when the local
clients are updated with different local epochs E. In Figure 3(c), we explore E ∈ {1, 2, 4, 8, 16}
with fixed B = 128 and C = 5. The results demonstrate that with different E, the performances of
FedUL are reasonably stable.
Effects of Batch Size: The training batch size is always an important factor in machine learning tasks,
especially in an FL system. Hence here, we study how batch size affects our model performance.
Specifically, we train the model using different batch sizes B ∈ {16, 64, 128, 512, ∞} with fixed
E = 1 and C = 5. Here B = ∞ stands for loading all of the training samples into a single batch.
The results in Figure 3(d) indicate that FedUL can effectively work under a wide range of batch sizes.
4.3	Case Study on Real-world Problem
To demonstrate the feasibility of our proposed FedUL scheme to the real problem, we test FedUL on
a medical application for the diagnosis of brain disease. In the rest of this section, we introduce the
problem formulation, data and setting, and then discuss the results.
Real Problem Formulation: For the real-world scenario, we consider a clinical setting for disease
classification in C different healthcare systems. Suppose each healthcare system has M hospitals
located in different regions, each associated with unique class priors due to population diversities.
Medical data are not shareable between the healthcare systems for privacy reasons. We assume
patient-wise diagnosis information is unavailable, but each hospital provides the prevalence of these
patients (i.e., class priors of the disease classes).
8
Published as a conference paper at ICLR 2022
Table 2: Quantitative comparison of our method with the baseline methods for the five healthcare
systems (HS) using the real-world intracranial hemorrhage detection dataset. Results of three runs
are reported in mean error (std) format. The best methods are highlighted in boldface.
Hospitals	Method	HS 1	HS 2	HS3	HS4	HS5
12	FedPL FedUL (ours)	44.69 (4.28)~46.42 (4.35)~46.29 (1.83)~47.55 (2.84)~46.53 (2.40) 40.26 (1.39) 40.35 (1.10) 39.31 (1.10) 39.72 (0.67) 39.43 (1.42)
24	FedPL FedUL (ours)	42.71 (2.97)~42.18 (3.00)~42.30(1.28)~41.79 (1.98)~41.46 (0.53) 32.27 (2.71) 30.99 (2.03) 31.29 (1.05) 31.72 (1.23) 31.37 (1.58)
48	FedPL FedUL (ours)	41.44 (3.04)~40.38 (1.38)~41.06 (2.88)~41.44(1.78)~40.26 (2.35) 31.48 (2.25) 30.92 (0.88) 31.16 (0.84) 31.55 (0.58) 30.57 (1.37)
-	FedAVg 10%	46.79 (1.91)~46.44 (0.78)~47.19 (1.05)~47.29 (1.93)~46.20 (1.61)
(f) SUbdUral
(e) Subarachnoid
Figure 4:	Example samples of each class in the distributed medical dataset (Flanders et al., 2020).
Dataset and Settings: We use the RSNA Intracranial Hemorrhage Detection dataset (Flanders et al.,
2020), which contains large-scale Magnetic Response Image (MRI) data from several hospitals (see
Figure 4), to simulate this real-world scenario. There are 106k samples in the dataset belonging to six
classes, including five types of intracranial hemorrhage (Epidural, Intraparenchymal, Intraventricular,
Subarachnoid, and Subdural) and healthy control type. We model an FL system with five healthcare
systems (i.e., clients). Each healthcare system contains 21.2k data samples. To show the effect on the
number of samples in each U set, we vary the number of hospitals (i.e., sets) M = 12, 24, and 48 in
each system. Here we generate each ∏m,k for the k-th class in m-th U set at c-th client from range
[0.1,0.9] and check that the generated class priors are not all identical. For model training, we use
the cross-entropy loss and Adam (Kingma & Ba, 2015) optimizer with a learning rate of 1e - 4 and
train 100 rounds. We use ResNet18 (He et al., 2016) as backbone for this experiment.
Results and Analysis: As indicated in our proof-of-concept (Table 1), FedPL consistently outper-
forms the other two LLP baselines, i.e., FedLLP and FedLLP-VAT. Here we focus on comparing
FedUL with FedPL. The supervised baseline FedAvg with 10% fully labeled data is also included
for reference. We report the results of each hospital in mean error (std) format over three runs in
Table 2. We can see that our proposed FedUL method consistently outperforms FedPL and FedAvg
10% for all the hospitals and under different set number settings. The superiority of FedUL in this
challenging real-world FL setting further indicates the effectiveness and robustness of our algorithm.
These results are inspiring and bring the hope of deploying FedUL to the healthcare field, where data
are often very costly to be labeled, heterogeneous, and distributed.
5 Conclusion
In this paper, we propose FedUL, a novel FL algorithm using only U data. FedUL works as a
wrapper that transforms the unlabeled client data and modifies the client model to form a surrogate
supervised FL task, which can be easily solved using existing FL paradigms. Theoretically, we prove
that the recovery of the optimal model by using the FedUL method can be guaranteed under mild
conditions. Through extensive experiments on benchmark and real-world datasets, we show FedUL
can significantly outperform the baseline methods under different settings. As a wrapper, we expect
FedUL can be further improved by incorporating advanced FL aggregation or optimization schemes
for the non-IID setting (Wang et al., 2020; Li et al., 2020; 2021; Karimireddy et al., 2020).
9
Published as a conference paper at ICLR 2022
Ethics S tatement
Collecting sample-wise labels for training can be costly and may violate data privacy. Considering
data in the real world are often distributed at a large scale, we provide a promising solution to perform
multi-class classification on distributed U data in the FL system. Our method FedUL can be widely
applied to data-sensitive fields, such as healthcare, politics, and finance, where the sample-wise label
is not available, but populational prevalence can be accessed. With FedUL, we hope the future FL
can enjoy significant labeling cost reduction and improve its privacy. The real datasets used in this
work are publicly available with appropriate prepossessing to de-identify patients’ identities. We
provide proper references to the datasets, libraries, and tools used in our study. This study does not
have any legal compliance, conflict of interest, or research integrity issues.
Reproducibility S tatement
We provide the codes to reproduce the main experimental results at https://github.com/
lunanbit/FedUL. We further provide experimental details and additional information of the
datasets in the supplementary materials.
Acknowledgments
NL was supported by JST AIP Challenge Program and the Institute for AI and Beyond, UTokyo.
ZW and QD were supported by project of Shenzhen-HK Collaborative Development Zone. XL
was thankful for the support by NVIDIA. GN was supported by JST AIP Acceleration Research
Grant Number JPMJCR20U. MS was supported by JST AIP Acceleration Research Grant Number
JPMJCR20U3 and the Institute for AI and Beyond, UTokyo.
References
Abdullatif Albaseer, Bekir Sait Ciftler, Mohamed Abdallah, and Ala Al-Fuqaha. Exploiting unlabeled
data in smart cities using federated edge learning. In 2020 International Wireless Communications
andMobile Computing (IWCMC) ,pp.1666-1671. IEEE, 2020.
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder-
ated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
334-334, 1997.
O. Chapelle, B. Scholkopf, and A. Zien (eds.). Semi-Supervised Learning. MIT Press, 2006.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
pp. 1597-1607. PMLR, 2020.
Janet B Croft, Anne G Wheaton, Yong Liu, Fang Xu, Hua Lu, Kevin A Matthews, Timothy J
Cunningham, Yan Wang, and James B Holt. Urban-rural county and state differences in chronic
obstructive pulmonary disease—united states, 2015. Morbidity and Mortality Weekly Report, 67
(7):205, 2018.
Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the
cross-entropy method. Annals of operations research, 134(1):19-67, 2005.
Don Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated clus-
tering. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 2611-2620.
PMLR, 18-24 Jul 2021.
Enmao Diao, Jie Ding, and Vahid Tarokh. Semifl: Communication efficient semi-supervised federated
learning with unlabeled clients. arXiv preprint arXiv:2106.01432, 2021.
10
Published as a conference paper at ICLR 2022
Gabriel Dulac-Arnold, Neil Zeghidour, Marco Cuturi, Lucas Beyer, and Jean-Philippe Vert. Deep
multi-class learning from label proportions. arXiv preprint arXiv:1905.12909, 2019.
Alireza Fallah, Aryan Mokhtari, and Asuman E Ozdaglar. Personalized federated learning with
theoretical guarantees: A model-agnostic meta-learning approach. In NeurIPS, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning. PMLR, 2017.
Adam E Flanders, Luciano M Prevedello, George Shih, Safwan S Halabi, Jayashree Kalpathy-
Cramer, Robyn Ball, John T Mongan, Anouk Stein, Felipe C Kitamura, Matthew P Lungren,
et al. Construction of a machine learning dataset through collaboration: the rsna 2019 brain ct
hemorrhage challenge. Radiology: Artificial Intelligence, 2(3):e190211, 2020.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework
for clustered federated learning. In Proceedings of the 34th Conference on Neural Information
Processing Systems, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura, and Koji Yamamoto. Distillation-
based semi-supervised federated learning for communication-efficient collaborative training with
non-iid private data. arXiv preprint arXiv:2008.06180, 2020.
Wonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised
learning with inter-client consistency & disjoint learning. In International Conference on Learning
Representations (ICLR), 2021.
Yihan Jiang, Jakub Konecny, Keith Rush, and Sreeram Kannan. Improving federated learning
personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representation, pp. 1—-13, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private meta-
learning. arXiv preprint arXiv:1909.05830, 2019.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Conference on Machine Learning and
Systems, 2020a, 2020.
Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning
on non-IID features via local batch normalization. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=6YEQUn0QICG.
Nan Lu, Shida Lei, Gang Niu, Issei Sato, and Masashi Sugiyama. Binary classification from multiple
unlabeled datasets via surrogate set classification. In International Conference on Machine
Learning, pp. 7134-7144. PMLR, 2021.
11
Published as a conference paper at ICLR 2022
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics ,pp.1273-1282. PMLR, 2017.
Dianwen Ng, Xiang Lan, Melissa Min-Szu Yao, Wing P Chan, and Mengling Feng. Federated
learning: a collaborative effort to achieve better medical imaging models for individual sites that
have small labelled datasets. Quantitative Imaging in Medicine and Surgery, 11(2):852, 2021.
N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le. Estimating labels from label proportions.
Journal of Machine Learning Research, 10:2349-2374, 2009a.
Novi Quadrianto, Alex J Smola, Tiberio S Caetano, and Quoc V Le. Estimating labels from label
proportions. Journal of Machine Learning Research, 10(10), 2009b.
Mark D Reid and Robert C Williamson. Composite binary losses. The Journal of Machine Learning
Research, 11:2387-2422, 2010.
Felix Sattler, Klaus-Robert Muller, and Wojciech Samek. Clustered federated learning: Model-
agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural
networks and learning systems, 2020.
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron Mor-Yosef,
and Itai Zeitak. Overcoming forgetting in federated learning on non-iid data. arXiv preprint
arXiv:1910.07796, 2019.
M. Sugiyama, H. Bao, T. Ishida, N. Lu, T. Sakai, and G. Niu. Machine Learning from Weak
Supervision: An Empirical Risk Minimization Approach. MIT Press, Cambridge, Massachusetts,
USA, 2022.
Kuen-Han Tsai and Hsuan-Tien Lin. Learning from label proportions with consistency regularization.
In Asian Conference on Machine Learning, pp. 513-528. PMLR, 2020.
V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19, 2019.
Xin Yao and Lifeng Sun. Continual local training for better initialization of federated models. In
2020 IEEE International Conference on Image Processing (ICIP), pp. 1736-1740. IEEE, 2020.
Felix X Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, and Shih-Fu Chang. On learning
from label proportions. arXiv preprint arXiv:1402.5902, 2014.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Xiaojin Jerry Zhu. Semi-supervised learning literature survey. 2005.
12
Published as a conference paper at ICLR 2022
Supplementary Material
Roadmap The appendix is organized as follows. We summarize notations used in our paper in
Section A and give a full discussion of related work in Section B. Our proofs are presented in
Section C. We provide detailed experimental settings in Section D and supplementary experimental
results on the benchmark datasets in Section E.
A Notation Table
Table A.1: Notations used in the paper.
Notations Description
dκk%ylry pπfc7π4a / C C 九 / 底 a 仪因 RZMm "⅛⅛ & M 广πc歹pczτrmC7ΓCgυcυQJ
input dimension
number of classes for the original classification task
index of the class k ∈ [K]
input space X ⊂ Rd
label space Y = [K] where [K] := {1, . . . , K}
input x ∈ X
true label y ∈ Y
test data distribution
k-th class prior πk := p(y = k) of the test set
vector form of πk where π = [π1, ∙一,πκ]>
loss function for a classification task
risk for the original FL task
global model predicting label y for input x
number of clients in the FL setup
index of the client c ∈ [C]
c-th client model predicting label y for input x
local data distribution at the c-th client
number of training data at the c-th client
labeled training set Dc = {(xic, yic)}in=c 1 at the c-th client
local step size
global step size
global communication round
local updating iterations
number of available U sets at the c-th client
index of the U set m ∈ [Mc]
number of training data in the m-th U set at the c-th client
m-th U data distribution at the c-th client
m-th U set at the c-th client Ucm = {xic,m}in=c,1m
all available U sets at the c-th client Uc = {Ucm}mM=c 1
the maximum number of available U sets among all clients M = maxc∈[C] Mc
k-th class prior πcm,k = pcm(y = k) of the m-th U set at the c-th client
matrix form of πcm,k where Πc ∈ RMc×K
surrogate label y ∈ [M]
surrogate local data distribution at the c-th client
surrogate labeled training set UC = {xi, y,}n=C 1 at the c-th client
m-th surrogate class prior ∏m = p/y = m) at the c-th client
vector form of ∏m where ∏ = [∏1,…，ΠM]>
surrogate global model predicting surrogate label y for input X
surrogate class-posterior probability at the c-th client where (n/x))m = ρc (y = m | x)
true class-posterior probability at the c-th client where(η(x))k = p(y = k | x)
vector-valued transition function Qc : (X → ∆K) → (X → ∆M)
surrogate risk for the surrogate FL task
13
Published as a conference paper at ICLR 2022
B Related Work
B.1	Personalized Federated Learning
Considering the heterogeneity of data, clients, and systems, personalization is a natural and trending
solution to improve FL performance. Many efforts have been made in personalized FL. FedProx(Li
et al., 2020), FedCurv (Shoham et al., 2019), and FedCL (Yao & Sun, 2020) propose regularization-
based methods that include special regularization terms to improve FL model generalizing to various
clients. Meta-learning methods, such as MAML (Finn et al., 2017), are also used to learn personalized
models for each client (Jiang et al., 2019; Fallah et al., 2020; Li et al., 2019). Unlike the methods
mentioned above that yield a single generalizable model, multiple FL models can be trained using
clustering strategies (Sattler et al., 2020; Ghosh et al., 2020). Different local FL models can also be
achieved by changing the last personalized layers (Li et al., 2021; Arivazhagan et al., 2019).
At first glance, FedUL looks similar to personalized FL since each client learns a "personalized"
model gc which contains a client-specific transition layer Qc. However, our learning setting and
method are very different from existing personalized FL approaches, as explained below. First, our
framework focuses on FL with only unlabeled data. The personalized model gc only serves as a
proxy, in order to be compatible with the transformed unlabeled data, and our ultimate goal is to learn
f that is not exactly personalized. Specifically, 1) if the inter-clients are IID, FedUL yields the same
proxy model gc and federated model f for all the clients; 2) if the inter-clients are Non-IID, although
gc is different (as Qc is different), the underlying model f (what we want) is still the same. Second,
our method differs from the previous personalized FL methods by its surrogate training nature: it
works as a wrapper that transforms unlabeled clients to (surrogate) labeled ones and is compatible
with many supervised FL methods.
B.2	Semi-supervised Federated Learning
Another line of research working on utilizing unlabeled data in FL follows the semi-supervised
learning framework (Zhu, 2005; Chapelle et al., 2006), which assumes a small amount of labeled
data available at the client side (labels-at-client) or at the server side (labels-at-server). For example,
FedMatch (Jeong et al., 2021) introduces a new inter-client consistency loss that maximizes the
agreement between multiple clients and decomposes the model parameters into one for labeled data
and the other for unlabeled data for disjoint learning. FedSem (Albaseer et al., 2020) generates pseudo
labels for unlabeled data based on the trained FedAvg model with labeled data, and the generated
(pseudo) labeled data are further used to retrain FedAvg to obtain the global model. Moreover,
distillation-based semi-supervised FL (Itahara et al., 2020) considers using shared unlabeled data for
distillation-based message exchanging.
Although the motivation of FedUL can be related to semi-supervised FL since both work on utilizing
unlabeled data in FL, our learning setting is different since we only have unlabeled data at both
the client and the server sides, and our method is different since FedUL follows the empirical risk
minimization framework.
14
Published as a conference paper at ICLR 2022
C Proofs
In this appendix, we prove all theorems.
C.1 Proof of Theorem 1
By the Bayes’ rule, ∀m ∈ [M] we have:
(ηc(χ))m = Pc(Ua= m
Pc(x)
=	Pc(X,y = m
PM= ι Pc (x,y = m)
= Pc(X I y = m)ρc(y = m)
PM= 1 Pc(X |y = m)Pc(y = m)
一Pm(X)πm
PM= IPm(X)∏m
πm PK=I πm,kP(X | y = k)
PM= 1 ∏m PK=I ∏m,kP(X I y = k
∏m PK 1 ∏m,k (η(X))k
PMC	m PK	m,k (n(X))k .
mm=l π c 乙k = 1 πc ∏k
By transforming (10) to the matrix form, we complete the proof.
(10)
□
C.2 Proof of Lemma 2
Given our data generation process in Section 3.1, we have that Mc ≥ kand Πc ∈ RMC×K is a full
column rank matrix. With DnC and D-1 being non-degenerate, We obtain that Qc(η(X)) presents
an injective mapping from "(x) to ηc(X).	□
C.3 Proof of Proposition 3
The assumptions (A1)-(A3) are listed as folloWs.
(A1) Bounded gradient dissimilarity: ∃ constants G ≥ 0 and B ≥ 1 s.t.
1 XC 1 kVJc(g)k2 ≤ G2 + B2kVJ(g)k2,∀g.
C c=1
(A2) β-smooth function Jc satisfies
kVJc(θ) - VJc(θ0)k ≤βkθ-θ0k,∀c,θ,θ0.
(A3) Unbiased stochastic gradient With bounded variance:
E[kvJc(g;Uc)- VJc(g)k2] ≤ σ2,∀c,g,
where Jc(g;Uc) = n1c Pn= 1 '(虱Xi),a)
Note that this proposition statement is W.r.t. the convergence of model g learned from the surrogate
supervised learning task. The proof follows directly from Theorem V in Karimireddy et al. (2020).
□
C.4 Proof of Theorem 4
When a proper loss (Reid & Williamson, 2010) is used for ', the mapping of g** is the unique
minimizer of J(g). We show an example of the a widely used proper loss function: the cross-entropy
loss (De Boer et al., 2005) defined as
'ce(g(X),y)
—
m)lθg(g(X))m = - lθg(g(X))y.
15
Published as a conference paper at ICLR 2022
We can see that the cross-entropy loss is non-negative by its definition. Therefore, minimizing J(g)
can be obtained by minimizing the conditional risk Ep(y∣χ) ['(g(x),y) | x] for every X ∈ X .So We
are noW optimizing
MM
φ(g) = - £ p(y = m | x) ∙ log(g(x))m,	s.t. E (g(x))m = 1.
m=1	m=1
By using the Lagrange multiplier method (Bertsekas, 1997), we have
MM
L = -E p(y = m | x) ∙ log(g(x))m - λ ∙ ( £ (g(x))m - 1).
m=1
The derivative of L with respect to g is
m=1
∂ L
∂g
—
p(y = 11 x)	、	p(y = M | x) ʌ'
(g(x))ι	-λ,…,-(g(x))M	- λ
>
By setting this derivative to 0 we obtain
(g(x))m = -ɪ ∙ p(y = m | x),	∀m = 1,...,M and ∀x ∈ X.
Since g ∈ Δm-i is the M-dimensional simplex, we have PM=ι(g**(x))m = 1 and PM=Ip(y
j | x) = 1. Then
MM
X (g** (X))m = - λ ∙ X p(y = m | X) = L
m=1	m=1
Therefore we obtain λ = -1 and (g**(x))m = p(y = m | x) = (n(x))m,∀m = 1,...,M and
∀x ∈ X, which is equivalent to g**(x) = η(x). Similarly, we obtain
f?(x) = η(x).
(11)
Given Proposition 3 and the assumption that the model used for g is sufficiently flexible, we obtain
that the classification model learned by FedUL (see Algorithm 1) satisfies g* = g**. By our model
construction, g is built from Qc(f). Then we have
f* = f**	(12)
where f** = argminf J(f) is the model induced by g**. Given Theorem 1 and Lemma 2, we
obtain that g**(x) = η(x) if and only if
f**(x)= η(x).
By summarizing (11), (12), and (13) We obtain f * = f ?.
(13)
□
16
Published as a conference paper at ICLR 2022
D	Experimental details
We illustrate our model architectures and training details of the benchmark and real-world experiments
in this section.
D.1 Model Architecture
For our benchmark experiment on MNIST, we use a six-layer convolutional neural network. The
network details are listed in Table D.2. For our benchmark experiment on CIFAR10 and real-world
experiment, we use ResNet18 as our backbone. The network details are listed in Table D.3.
Layer	Details
-1-	Conv2D(3, 64, 5,1, 2), BN(64), ReLU, MaXPool2D(2, 2)
-2-	Conv2D(64, 64,5,1, 2), BN(64), ReLU, MaXPool2D(2, 2)
-3-	Conv2D(64,128,5,1, 2), BN(128), ReLU
-4-	FC(6272, 2048), BN(2048), ReLU
-5-	FC(2048, 512), BN(512), ReLU
6	FC(512, 10)
Table D.2: Model architecture of the benchmark experiment on MNIST. For convolutional layer
(Conv2D), we list parameters with sequence of input and output dimension, kernel size, stride and
padding. For max pooling layer (MaxPool2D), we list kernel and stride. For fully connected layer
(FC), we list input and output dimension. For BatchNormalization layer (BN), we list the channel
dimension.
D.2 Training Details
Benchmark Experiments: We implement all the methods by PyTorch and conduct all the experi-
ments on an NVIDIA TITAN X GPU. Please note for all experiments, we split 20% original data
for validation and model selection. During training process, we use Adam optimizer with learning
rate 1e - 4 and the cross-entropy loss. We set batch size to 128 and training rounds to 100. To avoid
overfitting, we use L1 regularization. The detailed weight of L1 regularization used in benchmark
experiments is shown in Table D.4. The detailed number of samples at each client for MNIST and
CIFAR10 under IID and Non-IID setting is given in Table D.5 and Table D.6, respectively.
We describe details of the baseline methods and corresponding hyper-parameters as follows.
• FedPL (Diao et al., 2021): this method is based on empirical classification risk minimization.
The learning objective is
1 C Mc
R(f) = CC XXLfix + λLmix,	(14)
c=1 m=1
where
Lfix = ' (f(x+) ,y+)	(15)
is the "fix" loss. (xb+, yb+) represents for a sample xb+ with high confidence pseudo label yb+
and
Lmix = λmiχ ∙ ' f (Xmix) , y+) +(1 - λmiχ) ∙ ' f (/mix) , y-))	(16)
is the "mix" loss, in which
λmix 〜Beta(a, a), XmiX《-λmixxb + (1 ― λmiX) Xb .	(17)
a is the mixup hyper-parameter and (xb-, yb-) represents for a sample xb- with low confidence
pseudo label yb-. Following the default implementation in Diao et al. (2021), we set hyper-
parameter a = 0.75. Moreover, we set the weight of "mix" loss λ = 0.3 and the confidence
threshold τ = 0.4.
17
Published as a conference paper at ICLR 2022
Layer	Details
1	Conv2D(3, 64, 7, 2, 3), BN(64), ReLU
2	Conv2D(64, 64, 3,1,1), BN(64), ReLU
3	Conv2D(64, 64, 3,1,1), BN(64)
4	Conv2D(64, 64, 3,1,1), BN(64), ReLU
5	Conv2D(64, 64, 3,1,1), BN(64)
6	Conv2D(64,128, 3, 2,1), BN(128), ReLU
7	Conv2D(128,128, 3,1,1), BN(64)
8	Conv2D(64, 128, 1, 2, 0), BN(128)
9	Conv2D(128,128, 3,1,1), BN(128), ReLU
10	Conv2D(128,128, 3,1,1), BN(64)
11	Conv2D(128, 256, 3, 2,1), BN(128), ReLU
12	Conv2D(256, 256, 3,1,1), BN(64)
13	Conv2D(128, 256,1, 2, 0), BN(128)
14	Conv2D(256, 256, 3,1,1), BN(128), ReLU
15	Conv2D(256, 256, 3,1,1), BN(64)
16	Conv2D(256, 512, 3, 2, 1), BN(512), ReLU
17	Conv2D(512, 512, 3,1,1), BN(512)
18	Conv2D(256, 512,1, 2, 0), BN(512)
19	Conv2D(512,512, 3,1,1), BN(512), ReLU
20	Conv2D(512, 512, 3,1,1), BN(512)
21	AvgPool2D
22	FC(512, 10)
Table D.3: Model architecture of the benchmark experiment on CIFAR10. For convolutional layer
(Conv2D), we list parameters with sequence of input and output dimension, kernel size, stride and
padding. For max pooling layer (MaxPool2D), we list kernel and stride. For fully connected layer
(FC), we list input and output dimension. For BatchNormalization layer (BN), we list the channel
dimension.
Table D.4: Detailed weight of L1 regularization used in benchmark experiments.
Dataset	MNIST			CIFAR10		
Sets	10	20	30	10	20	30
Weight	1e-5	5e-5	^2e-6^	"^e-5	1e-5	4e-6
• FedLLP (Dulac-Arnold et al., 2019): this baseline method is based on empirical proportion
risk minimization. The learning objective is the aggregated proportion risk, i.e.,
1 C Mc
Rb(f) = C XX dprop (∏m,∏m)	(⑻
c=1 m=1
where πcm and πbcm are the true and predicted label proportions for the m-th U set Ucm at the
c-th client, b： := J Px〜Um f (x). For each client c, dprop is defined as
cc
K
dprop(∏m,∏m) ：= - χ ∏m log 键.
k=1
• FedLLP-VAT (Tsai & Lin, 2020): this method is based on empirical proportion risk mini-
mization, integrated with a consistency regularization term. The learning objective is the
18
Published as a conference paper at ICLR 2022
Table D.5: Detailed number of samples at each client for MNIST benchmark
	Class	0	1	2	3	4	5	6	7	8	9
IID	ClientI	920	1098	970	972	945	893	898	1007	971	926
	Client2	920	1098	970	972	945	893	898	1007	971	926
	Client3	920	1098	970	972	945	893	898	1007	971	926
	Client4	920	1098	970	972	945	893	898	1007	971	926
	Client5	920	1098	970	972	945	893	898	1007	971	926
Non-IID	ClientI	4498	4646	56	56	54	57	54	60	60	54
	Client2	57	54	4425	4737	49	50	49	58	57	58
	Client3	64	-61-	57	-61 -	4316	4217	54	53	58	57
	Client4	53	58	55	35	50	-0-	4572	4576	60	57
	Client5	57	58	^^61 ~	-0~	60	-0-	19	54	4418	4576
Table D.6: Detailed number of samples at each client for CIFAR10 benchmark
	Class	airplane	automobile	bird	cat	deer	dog	frog	horse	ship	truck
IID	ClientI	-755-	788	794	796	791	842	827	788	820	799
	Client2	-755-	788	794	796	791	842	827	788	820	799
	Client3	-755-	788	794	796	791	842	827	788	820	799
	Client4	-755-	788	794	796	791	842	827	788	820	799
	Client5	-755-	788	794	796	791	842	827	788	820	799
Non-IID	ClientI	3748~	3872	47	46	45	47	45	50	50	45
	Client2	~48	45	3688	3938	^^1-	^^1-	^^1-	^^8	48	^^8
	Client3	-53-	51	48	-0-	3597	3887	45	^^3	48	^^7
	Client4	~44	18	46	-0-	42	-0-	3810	3813	50	^^8
	Client5	~47	0	51	-0-	50	-0-	46	^^5-	3788	3790
aggregated proportion risk with consistency regularization, i.e.,
1 C	Mc
R(f) = CE E dprop (∏m, ∏m) +。展说(f),	(⑼
c=1 m=1
where πcm and πbcm are the true and predicted label proportions for the m-th U set Ucm at the
c-th client, and dprop is a distance function.
'cons (f) = dcons (f(x),f(X))	(20)
is the consistency loss, where dcons is a distance function, and xb is a perturbed input from
the original x. Following the default implementation in their paper (Tsai & Lin, 2020), we
set the hyper-parameter, the weight of consistency loss α = 0.05 and the perturbation weight
μ = 6.0 for FedLLP-VAT on CIFAR10 benchmark experiment. For MNIST benchmark,
we set the hyper-parameter, the weight of consistency loss α = 5e - 4 and the perturbation
weight μ = 1e - 2.
Real-world Experiments: We perform a case study on the clinical task — intracranial hemorrhage
diagnosis. We use the RSNA Intracranial Hemorrhage Detection dataset (Flanders et al., 2020)
(Figure 4), which contains large-scale Magnetic Response Image (MRI) data from several hospi-
tals.There are 106k samples in the dataset belonging to six classes, including five types of intracranial
hemorrhage (Epidural, Intraparenchymal, Intraventricular, Subarachnoid, and Subdural) and healthy
control type. In our experiments, we simulate a real-world scenario with five healthcare systems
(a.k.a. clients) and assume the number of hospitals (a.k.a sets) in each healthcare systems varies
M = 12, 24, and 48. At first, we randomly split the whole dataset for five healthcare systems equally,
so each healthcare systems is assigned 21.2k samples. Then we generate πcm for m-th set in c-th
client from range [0.1, 0.9] for all the hospitals with U data only. We check that the generated class
priors are not all identical. Finally, we load samples for each the hospitals following the generated π.
Detailed dataset statistics is given in Table D.7.
During training process, we use Adam optimizer with learning rate 1e - 4 and cross-entropy loss. We
set batch size to 128 and training rounds to 100. To avoid overfitting, we use L1 regularization. We
19
Published as a conference paper at ICLR 2022
Table D.7: Detailed number of samples in each healthcare system (HS) for real-world RSNA
Intracranial Hemorrhage Detection dataset (Flanders et al., 2020).
Type	Healthy	Epidural	Intraparenchymal	Intraventricular	Subarachnoid	Subdural
HS 1	3726-	-221-	2023	1198	2027	-4005-
HS 2	3756-	-226~	1923	1190	2063	-4042-
HS 3	3804-	-177~	1932	1266	2040	-3981-
HS 4	3805~	-224-	1939	1187	2074	-3971~
HS 5	3711 ~	-198~	1929	1299	2080	-3983~
set the weight of L1 regularization as 2e - 6, 8e - 7 and 4e - 7 for 10, 20 and 40 sets, respectively.
In real-world experiments, we use the same hyper-parameters as benchmark experiments for all
methods except FedLLP-VAT. For FedLLP-VAT, we set the weight of consistency loss α = 0.05 and
the perturbation weight μ = 6.0.
Training Efficiency: We further evaluate the training efficiency of our method with comparison
to baseline methods. The detailed one epoch training time (mean) and forward FLOPs are given
in Table D.8. The computation time is evaluated on a PC with one Nvidia TITAN X GPU. We can
see that our method slightly increase the forward FLOPs and can achieve higher training efficiency
compared with FedPL and FedLLP-VAT.
Table D.8: Quantitative comparison of our method with the baseline methods training epoch time and
forward flops on benchmark and real-world datasets under Non-IID and IID setting.
Dataset	Method	Non-IID Task with 5 Clients		IID Task with 5 Clients	
		Computation time (s)	FLOPs (G)	Computation time (s)	FLOPs (G)
MNIST	FedPL	3.29	0.048125312	478	0.048125312
	-FedLLP-	167		245	
	FedLLP-VAT	2Γ0		287	
	FedUL	180	0.048125422	235	0.048125422
CIFAR10	FedPL	3.37	0.141595648	570	0.141595648
	-FedLLP-	195		244	
	FedLLP-VAT	3.43		486	
	FedUL	2.63	0.141595758	380	0.141595758
Real-world	FedPL	32.47	6.937938944	-	-
	FedUL	32.34	6.937939028	-	-
20
Published as a conference paper at ICLR 2022
E More Experimental Results on the Benchmark Datasets
E.1 Detailed Statistics of Figure 3(a)
We show detailed statistics of Figure 3(a) in Table E.9. With different noisy class priors, even with
challenging = 0.8 and = 1.6, our method still achieves low error rate, which verify the robustness
and effectiveness of our method.
Table E.9: Inaccurate class priors experiments with different noise ratio on MNIST under Non-IID
setting with 5 clients and 10 sets for every client (mean error (Std)).
e = 0 (Clean) e = 0.2	e = 0.4	e = 0.8	e = 1.6
-2.98 (1.72)^^ 3.02 (1.30)^^3.10(0.96)^^3.02 (1.36)^^3.49 (1.74)
E.2 Detailed Statistics of Figure 3(b)
We show detailed statistics of Figure 3(b) in Table E.10. With more Non-IID data, our method
performs better, which validates that more unlabeled Non-IID data can be helpful for FL.
Table E.10: Experiments on accessible data amount under the Non-IID setting (mean error (std)).
5k	10k	15k	20k	25k
3.56 (0.09)^^2.62 (0.07)^^1.97 (0.38)^^1.75 (0.11)^^1.72 (0.13)
E.3 DIFFERENT COMBINATIONS OF E AND B
We compare our method with different combinations of E and B on benchmark dataset MNIST in
the Non-IID setting. The classification error rate (mean and std) is consistently small as presented (cf.
Table E.11), which indicates our method can work under a wide range combinations of E and B.
Table E.11: Experiments using different combinations of training batch size B and local update epoch
E on benchmark dataset MNIST in the Non-IID setting (mean error (std)).
E	1			4			16		
B	16	128	∞	16	128	∞	16	128	∞
Error	7.99	2,93	3.69	2.54	4.16	2.81	2.92	3.51	2.88
	(9.83)	(1.20)	(0.88)	(1.09)	(3.44)	(0.37)	(1.12)	(1.88)	(1.06)
E.4 Benchmark Experiments on Clients with Different Sets
Here we conduct experiments on benchmark datasets under IID and Non-IID setting with 5 clients
and different sets for each client (10, 20, 30, 40, 50). The experimental results are shown in Table
E.12, which further indicates the effectiveness of our method even under this challenging setting.
E.5 Benchmark Experiments with More Clients
To study the influence of the number of clients, here we further conduct benchmark experiments
on both MNIST and CIFAR10 with 10 clients. The detailed results are shown in Table E.13. The
observations are similar to Table 1.
21
Published as a conference paper at ICLR 2022
Table E.12: Quantitative comparison of our method with the baseline methods on benchmark datasets
under Non-IID and IID setting (mean error (std)) with different sets for each client. The best method
(paired t-test at significance level 5%) is highlighted in boldface.
IID Task with 5 Clients
Dataset	FedPL	FedLLP	FedLLP-VAT	FedUL	FedAvg 10%
MNIST	3.74 (0.78)	5.22 (5.38)	2.24 (0.05)	1.19 (0.11)	2.21 (0.07)
CIFAR10	45.49 (2.74)	84.16(0.71)	86.01 (2.54)	31.17 (2.80)	42.27 (1.06)
Non-IID Task with 5 Clients					
Dataset	-FedPL	FedLLP	FedLLP-VAT	-FedUL-	FedAVg 10%
MNIST	12.03 (2.89)	4.26 (3.20)	3.88 (2.89)	2.31 (1.14)	3.42 (0.20)
CIFAR10	59.06 (3.73)	62.44(16.99)	59.72 (18.73)	35.46 (1.40)	53.13 (0.22)
Table E.13: Quantitative comparison of our method with the baseline methods on benchmark datasets
under Non-IID and IID setting (mean error (std)). The best method (paired t-test at significance level
5%) is highlighted in boldface.
IID Task with 10 Clients
Dataset	Sets	FedPL	FedLLP	FedLLP-VAT	FedUL	FedAVg 10%
MNIST	10 20 40	2.19(0.04)	10.11 (7.12)~25.83 (22.64) 2.55 (0.30)	24.23 (17.52) 28.40 (23.20) 3.13 (0.25)	32.53 (16.93)	34.46(15.46)	0.74 (0.14) 1.05 (0.05) 1.47 (0.11)	1.58 (0.23)
CIFAR10	10 20 40	40.47 (0.58)~76.23 (1.47)	76.25 (1.65) 46.44 (0.71)	78.10 (3.58)	76.26 (3.83) 51.58 (1.18)	79.10 (2.34)	80.44(1.60)	20.40 (0.63) 21.61 (0.23) 22.71 (0.17)	33.57 (1.03)
Non-IID Task with 10 Clients
Dataset	Sets	-FedPL	-FedLLP-	FedLLP-VAT	-FedUL-	FedAvg 10%
	10	12.24(1.36)	20.81 (14.72)	17.26 (10.39)	1.91 (0.16)	
MNIST	20	5.73 (0.89)	17.67 (11.48)	18.45 (4.30)	1.45 (0.27)	3.03 (0.24)
	40	4.16(0.56)	14.98 (10.11)	13.62 (5.47)	1.27 (0.12)	
	10	68.53 (1.14)	84.11 (2.63)	82.17 (2.58)	39.20 (0.79)	
CIFAR10	20	61.49 (1.42)	71.69 (7.59)	72.34 (7.39)	34.12 (0.57)	52.48 (1.55)
	40	55.89 (2.19)	58.21 (4.95)	59.42 (9.59)	32.21 (1.02)	
22