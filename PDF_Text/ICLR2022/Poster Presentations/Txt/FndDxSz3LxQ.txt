Published as a conference paper at ICLR 2022
Learn Locally, Correct Globally:
A Distributed Algorithm for Training Graph
Neural Networks
Morteza Ramezani∖ Weilin Cong∖ Mehrdad Mahdavi
Mahmut T. Kandemir, Anand Sivasubramaniam
Pennsylvania State University, University Park, PA 16802, USA
morteza@cse.psu.edu, wxc272@psu.edu, mzm616@psu.edu
anand@cse.psu.edu, kandemir@cse.psu.edu
Ab stract
Despite the recent success of Graph Neural Networks (GNNs), training GNNs on
large graphs remains challenging. The limited resource capacities of the existing
servers, the dependency between nodes in a graph, and the privacy concern due
to the centralized storage and model learning have spurred the need to design an
effective distributed algorithm for GNN training. However, existing distributed
GNN training methods impose either excessive communication costs or large
memory overheads that hinders their scalability. To overcome these issues, we
propose a communication-efficient distributed GNN training technique named
Learn Locally, Correct Globally (LLCG). To reduce the communication and mem-
ory overhead, each local machine in LLCG first trains a GNN on its local data by
ignoring the dependency between nodes among different machines, then sends the
locally trained model to the server for periodic model averaging. However, ignoring
node dependency could result in significant performance degradation. To solve the
performance degradation, we propose to apply Global Server Corrections on the
server to refine the locally learned models. We rigorously analyze the convergence
of distributed methods with periodic model averaging for training GNNs and show
that naively applying periodic model averaging but ignoring the dependency be-
tween nodes will suffer from an irreducible residual error. However, this residual
error can be eliminated by utilizing the proposed global corrections to entail fast
convergence rate. Extensive experiments on real-world datasets show that LLCG
can significantly improve the efficiency without hurting the performance.
1 Introduction
In recent years, Graph Neural Networks (GNNs) have achieved impressive results across numerous
graph-based applications, including social networks (Hamilton et al., 2017; Deng et al., 2019),
recommendation systems (Ying et al., 2018; Wang et al., 2018),
and drug discovery (Fout et al., 2017; Do et al., 2019; Ghorbani
et al., 2022; Faez et al., 2021). Despite their recent success, effective
training of GNNs on large-scale real-world graphs, such as Face-
book social network (Boldi & Vigna, 2004), remains challenging.
Although several attempts have been made to scale GNN training by
sampling techniques (Hamilton et al., 2017; Zou et al., 2019; Zeng
et al., 2020; Chiang et al., 2019; Chen et al., 2018; Zhang et al.,
2021; Ramezani et al., 2020), they are still inefficient for training
on extremely large graphs, due to the unique structure of GNNs
and the limited memory capacity/bandwidth of current servers. One
potential solution to tackle these limitations is employing distributed
training with data parallelism, which have become almost a de facto
No. Machines
Figure 1: Comparison of the
speedup and the memory con-
sumption of distributed multi-
machine training and central-
ized single machine training
on the Reddit dataset.
standard for fast and accurate training for natural language processing (Lin et al., 2021; Hard et al.,
* Equal Contribution.
1
Published as a conference paper at ICLR 2022
2018) and computer vision (Bonawitz et al., 2019; KOneCny et al., 2018). For example, as shown in
Figure 1, moving from single machine to multiple machines reduces the training time and alleviates
the memory burden on each machine. Besides, scaling the training of GNNs with sampling techniques
can result in privacy concerns: existing sampling-based methods require centralized data storage and
model learning, which could result in privacy concerns in real-world scenarios (Shin et al., 2018; Wu
et al., 2021). Fortunately, the privacy in distributed learning can be preserved by avoiding mutual
access to data between different local machines, and using only a trusted third party server to access
the entire data.
Nonetheless, generalizing the existing data parallelism techniques of classical distributed train-
ing to the graph domain is non-trivial, which is mainly due to the dependency between nodes
in a graph. For example, unlike solving image classification problems where images are mu-
tually independent, such that we can divide the image dataset into several partitions without
worrying about the dependency between images; GNNs are heavily relying on the information
inherent to a node and its neighboring nodes. As a result, partitioning the graph leads to sub-
graphs with edges spanning subgraphs (cut-edges), which will cause information loss and hin-
der the performance of the model (Angerd et al., 2020). To cope with this problem, (Md et al.,
2021; Jiang & Rumi, 2021; Angerd et al., 2020) propose to transfer node features and (Zheng
et al., 2020; Tripathy et al., 2020; Scardapane et al., 2020) propose to transfer both the node
feature and its hidden embeddings between local machines, both of which can cause signifi-
cant storage/communication overhead and privacy concerns (Shin et al., 2018; Wu et al., 2021).
To better understand the challenge of distributed
GNN training, we compare the validation F1-
score in Figure 2 (a) and the average data com-
municated per round in Figure 2 (b) for two dif-
ferent distributed GNN training methods on the
Reddit dataset. On the one hand, we can ob-
serve that when ignoring the cut-edges, Parallel
SGD with Periodic Averaging (PSGD-PA (Dean
et al., 2012; Li et al., 2020b)) suffers from sig-
nificant accuracy drop and cannot achieve the
same accuracy as the single machine training,
even by increasing the number of communica-
tion. However, Global Graph Sampling (GGS)
can successfully reach the baseline by consider-
ing the cut-edges and allowing feature transfer, at
the cost of significant communication overhead,
and potential violation of privacy.
0.00
Number of Communication
0
pəlgpmɑIaloɔ sa^H
¾ JeqnmNa^βA<
(a)
Figure 2: Comparison of (a) the
score and (b) the average data communicated
per round (in bytes and log-scale) for two differ-
ent distributed GNN training settings, including
Parallel SGD with Periodic Averaging (PSGD-
PA) where the cut-edges are ignored and only
the model parameters are transferred and Global
Graph Sampling (GGS), where the cut-edges are
considered and the node features of the cut-edges
are transferred to the corresponding local ma-
chine, on the Reddit dataset using 8 machines.
In this paper, We propose a communication-efficient distributed GNN training method, called Learn
Locally, Correct Globally (LLCG). To reduce the communication overhead, inspired by the recent
success of the distributed optimization with periodic averaging (Stich, 2019; Yu et al., 2019), we
propose Local Training with Periodic Averaging: where each local machine first locally trains a
GNN model by ignoring the cut-edges, then sends the trained model to the server for periodic model
averaging, and receive the averaged model from server to continue the training. By doing so we
eliminate the features exchange phase between server and local machines, but it can result in a
significant performance degradation due to the lack of the global graph structure and the dependency
between nodes among different machines. To compensate for this error, We propose a Global Server
Correction scheme to take advantage of the available global graph structure on the server and refine
the averaged locally learned models before sending it back to each local machine. Notice that without
Global Server Correction, LLCG is similar to PSGD-PA as introduced in Figure 2.
To get a deeper understanding on the necessity of Global Server Correction, we provide the first
theoretical analysis on the convergence of distributed training for GNNs with periodic averaging. In
particular, we show that solely averaging the local machine models and ignoring the global graph
structure will suffer from an irreducible residual error, which provides sufficient explanation on
why Parallel SGD with Periodic Averaging can never achieve the same performance as the model
trained on a single machine in Figure 2 (a). Then, we theoretically analyze the convergence of
our proposal LLCG . We show that by carefully choosing the number of global correction steps,
2
Published as a conference paper at ICLR 2022
LLCG can overcome the aforementioned residual error and enjoys O(1/ √PT) convergence rate
with P local machines and T iterations of gradient updates, which matches the rate of (Yu et al.,
2019) on a general (not specific for GNN training) non-convex optimization setting. Finally, we
conduct comprehensive evaluations on real-world graph datasets with ablation study to validate the
effectiveness of LLCG and its improvements over the existing distributed methods.
Related works. Recently, several attempts have been made on distributed GNN training. According
to how they deal with the input/hidden feature of nodes that are associated with the cut-edges (i.e.,
the edges spanning subgraphs of each local machine), existing methods can be classified into two
main categories: (1) Input feature only communication-based methods: In these methods, each local
machine receives the input features of all nodes required for the gradient computation from other
machines, and trains individually. However, since the number of required nodes grows exponentially
with the number of layers, these methods suffer from a significant communication and storage
overhead. To alleviate these issues, (Md et al., 2021) proposes to split the original graph using a
min-cut graph partition algorithm that can minimize the number of cut-edges. (Jiang & Rumi, 2021)
proposes to use importance sampling to assign nodes on the local machine with a higher probability.
(Angerd et al., 2020) proposes to sample and save a small subgraph from other local machines as
an approximation of the original graph structure. Nonetheless, these methods are limited to a very
shallow GNN structure and suffer from significant performance degradation when the original graph
is dense. (2) Input and hidden feature communication-based methods: These methods propose to
communicate hidden features in addition to the input node features. Although these methods reduce
the number of transferred bytes during each communication round (due to the smaller size of hidden
embedding and less required nodes features), the number of communication rounds grows linearly
as the number of layers, and are prone to more communication delay. To address these issues, in
addition to optimal partitioning of the graph, (Zheng et al., 2020) proposes to use sparse embedding
to reduce the number of bytes to communicate and (Tripathy et al., 2020) proposes several graph
partitioning techniques to diminish the communication overhead.
2 Background and problem formulation
In this section, we start by describing Graph Convolutional Network (GCN) and its training algorithm
on a single machine, then formulate the problem of distributed GCN training. Note that we use
GCN with mean aggregation for simplicity, however, our discussion is also applicable to other GNN
architectures, such as SAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), ResGCN (Li
et al., 2019) and APPNP (Klicpera et al., 2019).
Training GCN on a single machine. Here, we consider the semi-supervised node classification in
an undirected graph G(V, E) with N = |V| nodes and |E| edges. Each node vi ∈ V is associated with
a pair (xi, yi), where xi ∈ Rd is the input feature vector, yi ∈ R|C| is the ground truth label, and C is
the candidate labels in the multi-class classifications. Besides, let X = [x1, . . . , xN] ∈ RN×d denote
the input node feature matrix. Our goal is to find a set of parameters θ = {W(`)}却 by minimizing
the empirical loss L(θ) over all nodes in the training set, i.e.,
L(θ) = ɪ X	Φ(h(L),yi),	h(') = σ(l.1 ʌ, X	h('-1)W⑶)，	(1)
' '	N j∈v*i i , 1, i	|N(vi)| 0j∈N(Vi) j	,一
where φ(∙, ∙) is the loss function (e.g., cross entropy loss), σ(∙) is the activation function (e.g., ReLU),
and N(vi ) is the neighborhood of node vi . In practice, we can update the model parameters by the
stochastic gradient computed on a sampled mini-batch (using full-neighbors) by
vL(θ,ξ) = ⅛ X.p Vφ(h(L), yi),	⑵
B	i∈ξ
where ξ denotes an i.i.d. sampled mini-batch of size B and we have E[VL(θ, ξ)] = VL(θ).
Distributed GCN training with periodic averaging. In this paper, we consider the distributed
learning setting with P local machines and a single parameter server. The original input graph G
is partitioned into P subgraphs, where Gp(Vp, Ep) denotes the subgraph on the p-th local machine
with Np = |Vp| nodes, and Xp ∈ RNp×d as the input feature of all nodes in Vp located on the p-th
machine. Then, the full-batch local gradient VLlpocal(θp) is computed as
VLr(Op)=NXE3,Vyi, h(')=σ(Eχj∈Np3)hj'τ)wp')),⑶
3
Published as a conference paper at ICLR 2022
Algorithm 1 Distributed GCN training with “Parallel SGD with Periodic Averaging”
Input: Global parameters θ0, local parameters θp = (90, time-step t = 0, learning rate η.
1:	for r ― 1 to R do
2:	for P — 1 to P do in parallel	. Parallel training on local machines
3:	Local machine P receives the global parameters θp — θt.	. Communication
4:	for k - 1 to K do
5:	t — t + 1.
6:	Local machine P constructs the mini-batch ξpt with neighbor sampling.
7:	Local machine P computes the stochastic gradients 十Lpocal (θp, ξp).
8:	Local machineP updates the local parameter by θp+1 = θp — η+Lpocal(θp,ξp).
9:	end for
10:	Local machine P sends the local parameters θpt+1 to the server. . Communication
11:	end for
12:	Server updates the global parameters by parameter averaging θ)t+1 = P Pp=ι θp+1.
13:	end for
Output: Server returns trained GCN model with mint E[∣NL(0t) k2].
where θp = {wP')}L=ι is the model parameters on the p-th local machine, Np(vi) = {vj |(vi, vj) ∈
Ep } is the local neighbors of node vi on the p-th local machine. When the graph is large, the
computational complexity of forward and backward propagation could be very high. One practical
solution is to compute the stochastic gradient on a sampled mini-batch with neighbor sampling, i.e.,
十L 产伤,5 B Xi∈ξFh (L), yi),h* = σ( ∣N⅛∣ Xjj)h尸 wQ (4)
where ξp is an i.i.d. sampled mini-batch ofBp nodes, Np(vi) ⊂ N(vi) is the sampled neighbors.
An illustration of distributed GCN training with Parallel SGD with Periodic Averaging (PSGD-PA) is
summarized in Algorithm 1. Before training, the server maintains a global model 8° and each local
machine keeps a local copy of the same model θp0 . During training, the local machine first updates
the local model θp using the stochastic gradient 十Lp)Cal(θp, ξp) computed by Eq. 4 for K iterations
(line 8), then sends the local model θpt to the server (line 10). At each communication step, the server
collects and averages the model parameters from the local machines (line 12) and send the averaged
model θpt+1 back to each local machine.
Limitations. Although PSGD-PA can significantly reduce the communication overhead by transfer-
ring the locally trained models instead of node feature/embeddings (refer to Figure 2 (b)), it suffers
from performance degeneration due to ignorance of the cut-edges (refer to Figure 2 (a)). In the next
section, we introduce a communication-efficient algorithm LLCG that does not suffer from this
issue, and can achieve almost the same performance as training the model on a single machine.
3	Proposed Algorithm: Learn Locally Correct Globally
In this section, we describe Learn Locally, Correct Globally (LLCG) for distributed GNN training.
LLCG includes two main phases, local training with periodic model averaging and global server
correction, to help reduce both the number of required communications and size of transferred data,
without compromising the predictive accuracy. We summarize the details of LLCG in Algorithm 2.
3.1	local training with periodic model averaging
At the beginning of a local epoch, each local machine receives the latest global model parameters
from the server (line 3). Next, each local machine runs Kρr iterations to update the local model
(line 4 to 9), where K and ρ are the hyper-parameters that control the local epoch size. Note that
instead of using a fixed local epoch size as Algorithm 1, we choose to use exponentially increasing
local epoch size in LLCG with ρ > 1. The reasons are as follows.
At the beginning of the training phase, all local models θpt are far from the optimal solution and
will receive a gradient 十Lp)Cal(θp, ξp) computed by Eq. 4. Using a smaller local update step at
the early stage guarantees each local model does not diverge too much from each other before the
model averaging step at the server side (line 12). However, towards the end of the training, all local
4
Published as a conference paper at ICLR 2022
Algorithm 2 Distributed GCN training by “Learn Locally, Correct Globally”
Input: Global parameters θ0, local parameters θp, time-step t = 0, local step size hyper-
parameters K, ρ, and learning rate γ, η
1:	for r _ 1 to R do
2:	for P — 1 to P do in parallel	. Parallel training on local machine
3:	Local machine P receives the global parameters θp — θt	. Communication
4:	for k - 1 to KPr do
5:	t — t + 1
6:	Local machine P constructs the mini-batch ξpt with neighbor sampling
7:	Local machine P computes stochastic gradients ▽ Lpocal(θp, ξp)
8:	Local machine P updates model parameter by θp+1 = θp — η57LpOCal (θp, ξp)
9:	end for
10:	Local machine P sends the local parameters θpt+1 to the server . Communication
11:	end for
12:	Server updates the global parameters using parameter averaging θt+1 = P Pp=ι θp+1
13:	for S _ 1 to S do	. Server Correction
14:	t — t + 1
15:	Server constructs a mini-batch ξt with full-neighbors
16:	Server computes the stochastic gradient ▽ L(θt,ξt)
17:	Server updates the global parameters by θ)t+^1 = θt — Y十L(θt, ξt)
18:	end for
19:	end for
Output: Server return GCN model with trained mint E[∣NL(8t) k2]
models θp will receive relatively smaller gradient ▽ Lp)Cal(θp, ξp), such that we can chose a larger
local epoch size to reduce the number of communications, without worrying about the divergence of
local models. By doing so, after total number of T = PrR=1 Kρr iterations, LLCG only requires
R = logρ T rounds of communications. Therefore, compared to the fully-synchronous method, we
can significantly reduce the total number of communications from O(T) to O(logρ K).
3.2	Global server correction
The design of the global server correction is to ensure that the trained model not only learns from
the data on each local machine, but also learns the global structure of the graph, thus reducing
the information loss caused by graph partitioning and avoiding cut-edges. Before the correction,
the server receives the locally trained models from all local machines (line 10) and applies model
parameter averaging (line 12). Next, S server correction steps are applied on top of the averaged
model (line 13 to 18). During the correction, the server first constructs a mini-batch ξt using full-
neighbors1(line 15), compute the stochastic gradient VL(θt, ξt) on the constructed mini-batch by
Eq. 2 (line 16) and update the averaged model θt for S iterations (line 17). The number of correction
steps S 2 depends on the heterogeneity among the subgraphs on each local machine: the more
heterogeneous the subgraphs are, the more correction steps are required to better refine the averaged
model and reduce the divergence across the local models. Note that, the heterogeneity is minimized
when employing GGS (Figure 2) with the local machines having access to the full graph, as a result.
However, GGS requires sampling from the global graph and communication at every iteration, which
results in additional overhead and lower efficiency. Instead, in LLCG we are trading computation on
the server for the costly feature communication, and only requires periodic communication.
4	Theoretical Analysis
In this section, we provide the convergence analysis on the distributed training of GCN under two
different settings, i.e., with and without server correction. We first introduce the notations and
assumptions for the analysis (Section 4.1). Then, we show that periodic averaging of local machine
models alone and ignoring the global graph structure will suffer from an irreducible residual error
(Section 4.2). Finally, we show that this residual error can be eliminated by running server correction
steps after each periodic averaging step on the server (Section 4.3).
1Note that using full neighbors is required for the server correction but not the local machines
2In practice, we found S = 1 or S = 2 works well on most datasets.
5
Published as a conference paper at ICLR 2022
(a)
With cross-device communication
Without neighbor sampling
l=2
l=1
(C)
Without cross-device communication
With neighbor sampling
十Llιocal(θ, ξι)十L2ocal(θ, ξ2)
Ew Llocal(θ, ξ1)] = WLl产(θ),
E⑹ L1ocal(θ, ξ2)] = WLlocal(θ)
Figure 3: Comparison of notations VLpOcal(θ),十Lpocal(θ, ξp), and VLp1ll(θ) on two local machines,
where the blue node and green circles represent nodes on different local machines.
4.1	Notations and assumptions
Let us first recall the notations defined in Section 2, where L(θ) denotes the global objective function
computed using the all node features X and the original graph G, Lp(θ) denotes the local objective
function computed using the local node features XP and local graph Gp, θp denotes the model
parameters on the p-th local machine at the t-th step, and θt = P Pp=1 θp denotes the virtual
averaged model at the t-th step. In the non-convex optimization, our goal is to show the expected
gradient of the global objective on the virtual averaged model parameters E[∣∣VL(θt)k2] decreases as
the number of local machines P and the number of training steps T increase. Besides, we introduce
VLfull(θ) as the gradient computed on the p-th local machine but have access the full node features
X and the original graph structure G as
VLPUll⑹=i⅛ SH),	h(') = σ(WbI Xj∈N(Vi) hj'T)wp'))∙⑸
Please refer to Figure 3 for an illustration of different gradient computations. Besides, we introduce
local-global gradient discrepancy as κ2 = KA + &, where KA = maχp∈[P ]{kVLp)Cal (e)-
VLfull(θ) k2} is the maximum difference between the gradient computed on the local machine with
and without having access to the global graph structure, which is mainly due to fact that the local
machines are oblivious to the full graph information; and K2X =maχp∈[P]{kVLfpull(θ)-VL(θ)k2}
is the maximum difference between the gradient computed using the local node and all nodes, which
is mainly due to the heterogeneity of the node features on each local machine, and we have K2X = 0
if the nodes are i.i.d. sampled to each local machine. Notice that local-global gradient discrepancy
K2 plays an important role in our theoretical results.
For the convergence analysis, we make the following standard assumptions.
Assumption 1 The stochastic gradient on the p-th local machine (with neighbor sampling) has
stochastic gradient variance bounded by σv2ar and stochastic gradient bias bounded by σb2ias, i.e.,
E[kV L pocal (θ; ξ) - E[V L pocal (θ; ξ)]k2 ] ≤ σ Var, E[kE[V L local (θ; ξ)] - VL local (θ)k2] ≤ 九、.
Assumption 2 The stochastic gradient for global server correction (with full neighbors) has stochas-
tic gradient variance bounded by σ 2ι0b)ai, i.e., E[∣∣V Lful (θ; ξ) 一 VLful (θ)]∣∣2] ≤ σgl.
The existence of stochastic gradient bias and variance in sampling-based GNN training have been
studied in (Cong et al., 2020; 2021), where (Cong et al., 2021) further quantify the stochastic gradient
bias and variance as a function of the number of GCN layers. in particular, they show that the
existence of σb2ias is due to neighbor sampling and non-linear activation, and we have σb2ias = 0 if all
neighbors are used or the non-linear activation is removed. The existence of σv2ar is because we are
sampling mini-batches to compute the stochastic gradient on each local machine during training. As
the mini-batch size increases, σv2ar will be decreasing, and we have σv2ar = 0 when using full-batch.
4.2	Distributed GNN via Parameter Averaging
in the following, we provide the first convergence analysis on distributed training of GCN. We show
that solely periodic averaging of the local machine models and ignoring the global graph structure
suffers from an upper bound that is irreducible with the number of training steps. Comparing to the
traditional distributed training (e.g., distributed training Convolutional Neural Network for image
classification (Dean et al., 2012; Li et al., 2020b)), the key challenges in the distributed GCN training
is the two different types of gradient bias: (1) The expectation of the local full-batch gradient is
6
Published as a conference paper at ICLR 2022
a biased estimation of the global full-batch gradient, i.e., P Pp=1 VLpOcal(θ) = VL(θ). This is
because each local machine does not have access to the original input graph and full node feature
matrix. Note that the aforementioned equivalence is important for the classifcal distributed training
analysis Dean et al. (2012); Yu et al. (2019). (2) The expectation of the local stochastic gradient is a
biased estimation of the local full-batch gradient i.e., E[VLpoCal(θ, ξp)] = VLpoCal(θ). This is because
the stochastic gradient on each local machine is computed by using neighbor sampling, which has
been studied in (Cong et al., 2021).
Theorem 1 (Distributed GCN via Parameter Averaging) Consider applying model averagingfor
GNN training under Assumption 1 and 2. Ifwe choose learning rate η = √p and the local step size
K ≤ √P3/4, then for any T ≥ L2P steps ofgradient updates we have
1	T -1	1
T Et=0 E[kVL(θt)k2] = O √p=) + O(κ2 + σIlaSY
Theorem 1 implies that, by carefully choosing the learning rate η and the local step size K, the gradient
norm computed on the virtual averaged model is bounded by O (1 / √PT) after R = T/K = O (PP^4)
communication rounds, but suffers from an irreducible residual error upper bound O(κ2 + σb2iasT). In
the next section, we show that this residual error can be eliminated by applying server correction.
4.3 Distributed GCN via Server Correction
Before proceeding to our result, in order to simplify the presentation, let us first define the nota-
tion GrIobaI = mint∈Tlobw(r) E[kVL(θt)k2] and GroCaI = mint∈To=al(r) E[∣∣P EP=I VLoCal(θ,)∣∣ ]
as the minimum gradient computed at the r-th round global and local step, where Tglobal (r) and
Tlocal(r) are the number of iteration run after the r-th communication round on server and local
machine, respectively. Please refer to Eq. 42 in Appendix C.2 for a formal definition.
Theorem 2 Consider applying model averaging for GCN training under Assumption 1 and 2. Ifwe
choose learning rate Y = η = √p, the local step size K, P such that P
server correction step size S =
ofgradient updates we have: T
max ( κ2 + 2σ⅛j _ Gr ∖ KP
maxr∈[R] ( 1-l(√P∕t)	GoCaa) GMal，
PT=1 E[kVL(θt)k2] = O( √PT ∖.
R=I K2P2r ≤ 32LT 1/3/2, and
then for any T ≥ L2P steps
Theorem 2 implies that, by carefully choosing the learning rates γ and η, the local step size hyper-
parameters K, P, and the number of global correction steps S, after T steps (R rounds of communica-
tion), employing parameter averaging with Global Server Correction, We have the norm of gradient
bounded by O(1∕√PT), without suffering the residual error that exists in the naive parameter
averaging (in Theorem 1). Besides, the server correction step size is proportional to the scale of κ2
and local stochastic gradient bias σb2ias. The larger κ2 and σb2ias, the more corrections are required to
eliminate the residual error. However, in practice, we observe that a very small number of correction
steps (e.g., S = 1) performs well, which minimizes the computation overhead on the server.
5	Experiments
Real-world simulation. In a real-world distributed setting, the server and local machines are located
on different machines, connected through the network (Li et al., 2020a). However, for our experiments,
we only have access to a single machine with multiple GPUs. As a result, we simulate a real-world
distributed learning scenario, such that each GPU is responsible for the computation of two local
machines (8 in total) and the CPU acts as the server. For these reasons, in our evaluations, we opted
to report the communication size and number of communication rounds, instead of the wall-clock
time, which can show the benefit of distributed training. We argue that these are acceptable measures
in real-world scenarios as well since the two main factors in distributed training are initializing
connection overhead and bandwidth (Tripathy et al., 2020).
Baselines. To illustrate the effectiveness of LLCG, we setup two general synchronized distributed
training techniques as the our baseline methods, namely “Parallel SGD with Parameter Averaging”
(PSGD-PA) and “Global Graph Sampling” (GGS), as introduced in Figure 2, where the cut-edges
in PSGD-PA are ignored and only the model parameters are transferred, but the cut-edges in GGS
are considered and the node features of the cut-edges are transferred to the corresponding machine.
7
Published as a conference paper at ICLR 2022
FHckr
Proteins
mɔs IH .TeATeqoE> SsoqlIJWIrBqOI0
10
No. Communications
(a)
Arxiv
——PSGD-PA
—GGS
——IJJcG
20	：
Bouw IHgATBqOI0
20
No. Communications
(b)
Reddit
——PSGD-PA
—GGS
——LLCG
40
(d)
---PSGD-PA
——IiCG
SSol IBqOlD
20	40
No, Communications
20
40
No, Communications
60
PSGD-PA
GGS
LLCG
Size of Comm, in Bytes (Log Scale)
Size of Comm, in Bytes (Log Scale)
1
0
(e)	(f)	(g)	(h)
Figure 4: Comparing LLCG against PSGD-PA and GGS on real-world datasets. We show the global
validation score in terms of the number of communications in (a,b,c,d), the training loss per round of
communications in (e,f ), and the global validation score per bytes of exchanged data in (g,h).
Table 1: Comparison of performance and the average Megabytes of node representation/feature
communicated per round on various datasets.
	Method	No. Comm.	GCN/SAGE		GAT		APPNP	
			Performance	Avg. MB	Performance	Avg. MB	Performance	Avg. MB
Flickr (F1-score)	PSGD-PA		49.08±0.27	12.57	51.56±0.28	4.24	50.81±0.48	8.40
	GGS	50	51.22±0.13	1849.32	52.41±0.29	1895.61	51.33±0.33	1897.82
	LLCG		50.38±0.20	12.57	52.01±0.33	4.24	51.15±0.25	8.40
OGB-Proteins	PSGD-PA		72.85±0.70	6.20	64.95±1.01	3.14	71.10±0.79	7.31
	GGS	100	74.78±0.36	922.42	68.11±0.60	912.79	71.29±0.31	917.20
(ROC-AUC)	LLCG		73.92±0.45	6.20	67.62±0.58	3.14	71.18±0.43	7.31
OGB-Arxiv (F1-score)	PSGD-PA		69.43±0.21	3.55	69.88±0.18	3.59	68.48±0.17	7.71
	GGS	100	70.51±0.26	3391.03	70.82±0.23	3396.79	69.01±0.10	3394.33
	LLCG		70.21±0.13	3.55	70.58±0.37	3.59	68.73±0.29	7.71
Reddit (F1-score)	PSGD-PA		71.17±1.06	14.83	70.57±1.24	7.48	83.48±0.81	11.63
	GGS	75	94.77±0.20	3798.81	95.03±0.48	3805.28	95.23±0.22	3770.46
	LLCG		94.67±0.15	14.83	94.73±0.23	7.48	94.64±0.17	11.63
Note that we choose GGS as a reasonable representation for most existing proposals (Md et al., 2021;
Zheng et al., 2020; Tripathy et al., 2020) for distributed GNN training, since these methods have very
close communication cost and also require a large cluster of machines to truly show their performance
improvement. We also use PSGD-PA as a lower bound for communication size, which is widely
used in traditional distributed training and similar to the one used in (Angerd et al., 2020; Jiang &
Rumi, 2021). However, we did not specifically include these methods in our results since we could
not reproduce their results in our settings. Please refer to Appendix A for a detailed description of
implementation, hardware specification and link to our source code.
Datasets and evaluation metric. We compare LLCG and other baselines on real-world semi-
supervised node classification datasets, details of which are summarized in Table 2 in the Appendix.
The input graphs are splitted into multiple subgraphs using METIS before training, then the same
set of subgraphs are used for all baselines. For training, we use neighborhood sampling (Hamilton
et al., 2017) with 10 neighbors sampled per node and ρ = 1.1 for LLCG. For a fair comparison, we
chose the base local update step K such that LLCG has the same number of local update steps as
PSGD-PA. During evaluation, we use full-batch without sampling, and report the performance on
the full graph using AUC ROC and F1 Micro as the evaluation metric. Unless otherwise stated, we
conduct each experiment five times and report the mean and standard deviation.
5.1 Primary Results
In this section, we compare our proposed LLCG algorithm with baselines on four datasets. Due to
space limitations we defer the detailed discussion on additional datasets to the Appendix A.4.
LLCG requires same number of communications. Figure 4 (a) through 4 (d) illustrate the
validation accuracy per communication rounds on four different datasets. We run a fixed number of
communication rounds and plot the global validation score (the validation score computed using the
full-graph on the server) at the end of each communication step. For PSGD-PA and GGS, the score is
calculated on the averaged model, whereas for LLCG the validation is calculated after the correction
8
Published as a conference paper at ICLR 2022
Figure 5: Effect of local
epoch size (K)
Arxiv
eO。S EIBA15
Reddit
----- w/ saπψling 20%,S=I
----- w/ saπψling 5%, S=I
w/ saπψling 5%, S=2
0.0-,
20	40	60
Number of Communications
eO。S EIBA15
-----w/o sanψling ,S=1
-----w/ saπψling 20%,S=I
----- w/ saπψling 5%, S=I
w/ saπψling 5%, S=4
20	⅛	60
Number of Communications
Figure 6: Effect of sampling on local machine and num-
ber of correction steps on the server
step. It can be seen that PSGD-PA suffers from performance drop compared to other two methods,
due to the residual error we discussed in Section 4, while both GGS and LLCG perform well and
can achieve the expected accuracy. Note that the performance drop of PSGD-PA can vary across
different datasets; in some cases such as Reddit, PSGD-PA can significantly hurt the accuracy,
while on other datasets the gap is smaller. Nevertheless, LLCG can always close the gap between
PSGD-PA and GGS with minimal overhead.
LLCG convergences as fast as GGS. To represent the effect of communication on the real-time
convergence, in Figure 4 (e) and 4 (f), we plot the global training loss (training loss computed on the
full-graph on the server) after each communication round. Similar to the accuracy score, the training
loss is also computed on the server averaged (and corrected, in case of LLCG) global model. These
results clearly indicate that LLCG can improve the convergence over PSGD-PA, while it shows a
similar convergence speed to GGS.
LLCG exchanges data as little as PSGD-PA. Figure 4 (g) and 4 (h) show the relation between
global validation accuracy with the average size (volume) of communication in bytes. As expected,
this figure clearly shows the effectiveness of LLCG . On the one hand, LLCG has a similar amount
of communication volume as PSGD-PA but can achieve a higher accuracy. On the other hand,
LLCG requires significantly less amount of communication volume than GGS to achieve the same
accuracy, which leads to slower training time in real world settings.
LLCG works with various GNN models and aggregations. We evaluate four popular GNN
models, used in recent graph learning literature: GCN Kipf & Welling (2017), SAGE Hamilton et al.
(2017), GAT Velickovic et al. (2018) and APPNP Klicpera et al. (2019). In Table 1, we summarize
the test score and average communication size (in MB) on different datasets for a fixed number of
communication rounds. Note that we only include the results for the aggregation methods (GCN
or SAGE) that have higher accuracy for the specific datasets, details of which can be found in
Appendix A.2. As shown here, LLCG can consistently improve the test accuracy for all different
models compared to PSGD-PA, while the communication size is significantly lower than GGS, since
LLCG only needs to exchange the model parameters.
Effect of local epoch size. Figure 5 compares the effect of various values of local epoch size
K ∈ {1, 4, 16, 64, 128} for fixed ρ and S on the OGB-Arxiv dataset. When using fully synchronous
with K = 1, the model suffers from very slow convergence and needs more communications. Further
increasing the K to larger values can speed up the training; however, we found a diminishing return
point for K > 128 in this dataset and extremely large K in general.
Effect of sampling in local machines. In Figure 6, we report the validation scores per round of
communication to compare the effect of neighborhood sampling at local machines. We can observe
that when the neighborhood sampling size is reasonably large (i.e., 20%), the performance is very
similar to full neighborhood training. However, reducing the neighbor sampling ratio to 5% could
result in a larger local stochastic gradient bias σb2ias, which requires using more correction steps (S).
6 Concluding Remarks
In this paper, we propose a novel distributed algorithm for training Graph Neural Networks (GNNs).
We theoretically analyze various GNN models and discover that, unlike the traditional deep neural
networks, due to inherent data samples dependency in GNNs, naively applying periodic parameter
averaging leads to a residual error and current solutions to this issue impose huge communication
overheads. Instead, our proposal tackles these problems by applying correction on top of locally
learned models, to infuse the global structure of the graph back into the network and avoid any costly
communication. In addition, through extensive empirical analysis, we support our theoretical findings
and demonstrate that LLCG can achieve high accuracy without additional communication costs.
9
Published as a conference paper at ICLR 2022
Acknowledgements
This work was supported in part by CRISP, one of six centers in JUMP, a Semiconductor Research
Corporation (SRC) program sponsored by DARPA and NSF grants 1909004, 1714389, 1912495,
1629915, 1629129, 1763681, 2008398.
Reproducibility S tatement
We provide a GitHub repository in Appendix A including all code and scripts used in our ex-
perimental studies. This repository includes a README.md file, explaining how to install and
prepare the code and required packages. Detailed instruction on how to use the partitioning
scripts is provided for various datasets. In addition, we provide several configuration files (un-
der scripts/configs) folder for different hyper-parameters on each individual dataset, and a
general script (scripts/run-config.py) to run and reproduce the results with these configura-
tions. Details of various models and parameters used in our evaluation studies can also be found in
Appendix A.
References
Alexandra Angerd, Keshav Balasubramanian, and Murali Annavaram. Distributed training of graph
convolutional networks using subgraph approximation. arXiv preprint arXiv:2012.04930, 2020.
Paolo Boldi and Sebastiano Vigna. The webgraph framework I: compression techniques. In Proceed-
ings of the 13th international conference on World Wide Web, WWW 2004, New York, NY, USA,
May 17-20, 2004, pp. 595-602. ACM, 2004. doi: 10.1145/988672.988752.
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
Ivanov, Chlo6 Kiddon, Jakub Konecny, StefanO Mazzocchi, Brendan McMahan, Timon Van
Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning
at scale: System design. In Proceedings of Machine Learning and Systems 2019, MLSys 2019,
Stanford, CA, USA, March 31 - April 2, 2019. mlsys.org, 2019.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmdssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 941-949. PMLR, 2018.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019,
Anchorage, AK, USA, August 4-8, 2019, pp. 257-266. ACM, 2019. doi: 10.1145/3292500.3330925.
Weilin Cong, Rana Forsati, Mahmut T. Kandemir, and Mehrdad Mahdavi. Minimal variance sampling
with provable guarantees for fast training of graph neural networks. In KDD ’20: The 26th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August
23-27, 2020, pp. 1393-1403. ACM, 2020.
Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On the importance of sampling in learning
graph convolutional networks. arXiv preprint arXiv:2103.02696, 2021.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. Large
scale distributed deep networks. In Advances in Neural Information Processing Systems 25: 26th
Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting
held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1232-1240, 2012.
Songgaojun Deng, Huzefa Rangwala, and Yue Ning. Learning dynamic context graphs for predicting
social events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 1007-1016.
ACM, 2019. doi: 10.1145/3292500.3330919.
10
Published as a conference paper at ICLR 2022
Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemical
reaction prediction. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp.
750-760. ACM, 2019. doi: 10.1145/3292500.3330958.
Faezeh Faez, Yassaman Ommi, Mahdieh Soleymani Baghshah, and Hamid R Rabiee. Deep graph
generators: A survey. IEEE Access, 9:106675-106702, 2021.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA, pp. 6530-6539, 2017.
Mahsa Ghorbani, Anees Kazi, Mahdieh Soleymani Baghshah, Hamid R Rabiee, and Nassir Navab.
Ra-gcn: Graph convolutional network for disease prediction problems with imbalanced data.
Medical Image Analysis, 75:102272, 2022.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024-1034,
2017.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, FrangOise Beaufays, Sean
Augenstein, Hubert Eichner, Chlo6 Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In
Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A
large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and
Conference Proceedings, pp. 448-456. JMLR.org, 2015.
Peng Jiang and Masuma Akter Rumi. Communication-efficient sampling for distributed training of
graph convolutional networks. arXiv preprint arXiv:2101.07706, 2021.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Jakub Konecny, H Brendan McMahan, X Yu Felix, Ananda Theertha Suresh, Dave Bacon, and Peter
Richtdrik. Federated learning: Strategies for improving communication efficiency. 2018.
Guohao Li, Matthias Muller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep
as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
Korea (South), October 27 - November 2, 2019, pp. 9266-9275. IEEE, 2019. doi: 10.1109/ICCV.
2019.00936.
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff
Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating
data parallel training. arXiv preprint arXiv:2006.15704, 2020a.
11
Published as a conference paper at ICLR 2022
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020b.
Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Mahdi Soltanolkotabi,
Xiang Ren, and Salman Avestimehr. Fednlp: A research platform for federated learning in natural
language processing. arXiv preprint arXiv:2104.08815, 2021.
Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty, Evangelos Georganas, Alexan-
der Heinecke, Dhiraj Kalamkar, Nesreen K Ahmed, and Sasikanth Avancha. Distgnn: Scalable
distributed training for large-scale graph neural networks. arXiv preprint arXiv:2104.06700, 2021.
Morteza Ramezani, Weilin Cong, Mehrdad Mahdavi, Anand Sivasubramaniam, and Mahmut Kan-
demir. Gcn meets gpu: Decoupling “when to sample” from “how to sample”. Advances in Neural
Information Processing Systems, 33, 2020.
Simone Scardapane, Indro Spinelli, and Paolo Di Lorenzo. Distributed graph convolutional networks.
arXiv preprint arXiv:2007.06281, 2020.
Hyejin Shin, Sungwook Kim, Junbum Shin, and Xiaokui Xiao. Privacy enhanced matrix factorization
for recommendation with local differential privacy. IEEE Transactions on Knowledge and Data
Engineering, 30(9):1770-1782, 2018.
Sebastian U. Stich. Local SGD converges fast and communicates little. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net, 2019.
Chuxiong Sun and Guoshi Wu. Adaptive graph diffusion networks with hop-wise attention. arXiv
preprint arXiv:2012.15024, 2020.
Chuxiong Sun and Guoshi Wu. Scalable and adaptive graph neural networks with self-label-enhanced
training. arXiv preprint arXiv:2104.09376, 2021.
Alok Tripathy, Katherine Yelick, and Aydin Buluc. Reducing communication in graph neural network
training. arXiv preprint arXiv:2005.03300, 2020.
Petar Velickovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lid, and YoshUa
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018.
Jizhe Wang, Pipei HUang, HUan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik LUn Lee. Billion-scale
commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,
London, UK, August 19-23, 2018, pp. 839-848. ACM, 2018. doi: 10.1145/3219819.3219869.
ChUhan WU, Fangzhao WU, Yang Cao, Yongfeng HUang, and Xing Xie. Fedgnn: Federated graph
neUral network for privacy-preserving recommendation. arXiv preprint arXiv:2102.04925, 2021.
Rex Ying, RUining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and JUre Leskovec.
Graph convolUtional neUral networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,
London, UK, August 19-23, 2018, pp. 974-983. ACM, 2018. doi: 10.1145/3219819.3219890.
Hao YU, Sen Yang, and ShenghUo ZhU. Parallel restarted sgd with faster convergence and less
commUnication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volUme 33, pp. 5693-5700, 2019.
Hanqing Zeng, HongkUan ZhoU, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna.
Graphsaint: Graph sampling based indUctive learning method. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020.
12
Published as a conference paper at ICLR 2022
Qingru Zhang, David Wipf, Quan Gan, and Le Song. A biased graph neural network sampler with
near-optimal regret. arXiv preprint arXiv:2103.01089, 2021.
Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang,
and George Karypis. Distdgl: Distributed graph neural network training for billion-scale graphs.
arXiv preprint arXiv:2010.05337, 2020.
Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent
importance sampling for training deep and large graph convolutional networks. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2θ19, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,pp. 11247-11256,
2019.
13