Published as a conference paper at ICLR 2022
S Quant: On-the-Fly Data-Free Quantization
via Diagonal Hessian Approximation
Cong Guo* 1,2, Yuxian Qiu1,2, JingWen Leng1,2, *, Xiaotian Gao3, Chen Zhang4, Yunxin Liu5,
Fan Yang3, Yuhao Zhu6 & Minyi Guo1,2, *
1 Shanghai Jiao Tong University, 2 Shanghai Qi Zhi Institute
3 Microsoft Research, 4 DAMO Academy, Alibaba Group
5 Institute for AI Industry Research (AIR), Tsinghua University, 6 University of Rochester
{guocong, qiuyuxian, leng-jw}@sjtu.edu.cn
{xiaotian.gao, fanyang}@microsoft.com
mingchong.zc@alibaba-inc.com, liuyunxin@air.tsinghua.edu.cn
yzhu@rochester.edu, guo-my@cs.sjtu.edu.cn
Ab stract
Quantization of deep neural networks (DNN) has been proven effective for com-
pressing and accelerating DNN models. Data-free quantization (DFQ) is a promis-
ing approach without the original datasets under privacy-sensitive and confidential
scenarios. However, current DFQ solutions degrade accuracy, need synthetic data
to calibrate networks, and are time-consuming and costly. This paper proposes
an on-the-fly DFQ framework with sub-second quantization time, called SQuant,
which can quantize networks on inference-only devices with low computation
and memory requirements. With the theoretical analysis of the second-order in-
formation of DNN task loss, we decompose and approximate the Hessian-based
optimization objective into three diagonal sub-items, which have different areas
corresponding to three dimensions of weight tensor: element-wise, kernel-wise,
and output channel-wise. Then, we progressively compose sub-items and pro-
pose a novel data-free optimization objective in the discrete domain, minimizing
Constrained Absolute Sum of Error (or CASE in short), which surprisingly does
not need any dataset and is even not aware of network architecture. We also design
an efficient algorithm without back-propagation to further reduce the computa-
tion complexity of the objective solver. Finally, without fine-tuning and synthetic
datasets, SQuant accelerates the data-free quantization process to a sub-second
level with > 30% accuracy improvement over the existing data-free post-training
quantization works, with the evaluated models under 4-bit quantization. We have
open-sourced the SQuant framework1.
1	Introduction
With the widespread application of DNN, more and more DNN models are deployed on both
computation-constrained and memory-constrained environments, e.g., smartphones, IoT devices,
and self-driving cars. The desire for lightweight and energy-efficient DNN deployment solutions is
increasing. Quantization is one of the most promising techniques to convert weights and activations
to lower bit formats and simultaneously reduce computational time and memory consumption. There
are two kinds of quantization: Post-training quantization (PTQ) (Banner et al., 2018; Choukroun
et al., 2019; Zhao et al., 2019; Nagel et al., 2020) and Quantization-aware training (QAT) (Gupta
et al., 2015; Jacob et al., 2018; Wang et al., 2019; Zhuang et al., 2021). QAT requires to simulate
quantization in the training process, which invokes time-consuming retraining and hyper-parameter
tuning. In contrast, PTQ directly quantizes well-trained models without retraining. However, they
still need training datasets to calibrate (Nagel et al., 2020) quantized models but are often unavailable
due to privacy and security issues, such as medical and confidential scenarios.
* Jingwen Leng and Minyi Guo are corresponding authors of this paper.
1https://github.com/clevercool/SQuant
1
Published as a conference paper at ICLR 2022
In contrast, data-free quantization (DFQ) has recently been presented as a promising way to quantize
models without original datasets (Nagel et al., 2019; Cai et al., 2020; Zhang et al., 2021; Xu et al.,
2020; Liu et al., 2021; Qin et al., 2021; Choi et al., 2020). From a deployment perspective, DFQ is the
most attractive quantization method since we can apply it to any trained models as a black box post-
processing step. However, current DFQ methods cannot achieve high accuracy and fast processing
time simultaneously. Traditionally, DFQ (Nagel et al., 2019) uses rounding quantization, leading
to the rounding-to-nearest strategy. Such a strategy causes significant accuracy loss, especially
in low-bit settings. To bridge the accuracy gap between data-free and data-driven quantization,
researchers propose a series of data-generative DFQ methods. They use gradient-based methods to
generate fake datasets for trained models. With the synthetic data, they can employ a data-driven
calibration and fine-tuning strategy to improve accuracy. However, data generation typically adopts
the time-consuming gradient-based methods, which require multiple iterations to generate each input.
For example, prior works often spend hours generating a calibration dataset and fine-tuning the
network (Xu et al., 2020; Liu et al., 2021; Zhang et al., 2021).
To solve this dilemma, we propose SQuant, a fast and accurate data-free quantization framework for
convolutional neural networks, employing the constrained absolute sum of error (CASE) of weights
as the rounding metric. By leveraging Hessian information of network loss due to quantization,
we propose a novel diagonal Hessian approximation, which decomposes the optimization objective
into three data-free sub-items: element-wise, kernel-wise, and output channel-wise, each of which
corresponds to a single or a set of dimensions of the weight tensor. We progressively compose and
optimize these three sub-items in the discrete space. The final approximate objective eliminates
the requirement of data generation. We propose a progressive algorithm with linear complexity to
solve the optimization objective, further accelerating DFQ time to a sub-second level. For example,
SQuant only needs an average of 4 ms and 84 ms for quantizing a layer and the overall network of
ResNet18, respectively. As it does not require back-propagation nor fine-tuning, SQuant can run on
inference-only devices with limited computation and memory resources on the fly. That opens up
new opportunities and scenarios for adopting quantization.
Compared with state-of-the-art DFQ methods, SQuant achieves higher accuracy on all evaluated
models under the 4/6/8-bit settings. SQuant only introduces 0.1% accuracy loss on average under
the 8-bit setting. Under fewer bit precisions, the advantage of SQuant further expands. SQuant only
introduces 1.8% accuracy loss on average under the 6-bit setting. Under the 4-bit setting, SQuant can
achieve more than 30% accuracy improvement compared with data-free PTQ methods. In a word,
SQuant pushes the accuracy and processing time of DFQ to a new frontier.
2	Preliminaries
2.1	Notations
We specifically use x, y and w to denote the input, output, and weight variables, respectively. Constant
and scalar are denoted by italic letters, e.g., c, M. Column vector and flattened matrix are denoted
by bold lowercase letters, e.g., w, and matrices (or tensors) are represented by uppercase letters,
e.g., W. The subscript and superscript can further represent the element indices and the layer of a
network, respectively, e.g., W'j. E[∙] denotes the expectation operator, and the network loss function
is represented by L(∙). For convenience in this paper, we call the row of FC (fully connected layer)
weight as the output channel and the column of FC weight as the input channel, which are the
counterparts to Conv (convolution layer) weight. We use M, N, and K to denote output channel size,
input channel size, and kernel height × kernel width, respectively. Specifically, FC has the shape of
(M,N, 1).
2.2	Quantization
Most previous works adopt the rounding-to-nearest approach for quantizing deep neural networks by
rounding elements w to the nearest quantization grid values with a fixed-point data type. The quanti-
zation and dequantization for a quantized element 0 can be described as 命=S ∙ clip( [W], min, max),
where s denotes the quantization scale parameter and, min and max are the lower and upper thresholds
for the clipping function clip(∙). The operator [•] represents the rounding-to-nearest, i.e., minimizing
the mean squared error (MSE) between the quantized and the original value.
2
Published as a conference paper at ICLR 2022
2.3	Hessian-Based Optimization for Neural Networks
The Hessian-based approach is one of the most promising optimizations to further improve the
quantization (Dong et al., 2019b;a; Nagel et al., 2020; Shen et al., 2020; Qian et al., 2020; Wu et al.,
2020; Hubara et al., 2020; Li et al., 2021; Yao et al., 2021) and pruning (Yu et al., 2021) performance
for DNN models. Some of those works exploit the Hessian matrix to approximate loss degradation
due to the quantization perturbation of weight, ∆W, by
E[L(X, Y, W + ∆W) - L(X, Y, W)]≈ E[∆W ∙ gw + 1 ∆W ∙ HW ∙ ∆WT],	(1)
where the equation comes from second-order Taylor series expansion, gW is the gradient and HW is
the full network Hessian matrix w.r.t. original weight, W. Since a well-trained model has already
converged, the gradient term will be close to 0 and thus can be safely ignored. However, computing
HW is infeasible because of the large memory overhead and computation complexity. To tackle
this problem, We approximate HW as a layer-wise Hessian matrix Hw' under the assumption of
cross-layer independence (Dong et al., 2017; Nagel et al., 2020), i.e., Hw' = x'x'T 0 弋'L, where
0 denotes Kronecker product of two matrices, V2'L is the Hessian of the task loss w.r.t. y'.
For the m-th output channel of Conv or FC, HW' can be approximatively simplified into output
channel-wise (Nagel et al., 2020; Yu et al., 2021; Wu et al., 2020; Qian et al., 2020),
HWm ≈ V2st Lm,m ∙ x'x' T = lm ∙ x'x' T,	(2)
where V22' L is approximately a diagonal matrix. Then the final optimization objective is
∆cm: = argmin ∆Wm: E[HWm]∆Wmm T
,	∆W'm,:	,	,
≈ arg min ∆W'm : E[x'x'T]∆W'm :T = arg min E[(∆W'm :x')2],
∆W'm,:	,	,	∆W'm,:	,
(3)
(4)
which is the MSE between the output activation produced from original and quantized weights. Each
sub-problem deals with a single output channel ∆W'm,:. We will further approximate Eq. (4) to remove
any input data dependency from the optimization objective in Sec. 3.2.
3	Methodology
3.1	Overview
Although we can obtain a good quantization strategy by minimizing MSE for each output channel, it
is an NP-hard combinatorial optimization problem. Even approaching an acceptable local minimum
requires significant effort and involves input activations without the data-free promise.
To avoid the combinatorial optimization problem and eliminate the requirement of data, we propose
the SQuant framework. First, SQuant approximates Eq. (4) with three diagonal Hessian matrices
corresponding to the dimensions of weight, in Sec. 3.2. Due to the quantization with a fixed-point data
type, SQuant transforms the problem into a data-free optimization problem in the discrete domain.
SQuant dedicates to optimizing each layer’s weight employing a flipping approach (Nagel et al.,
2020) without any input activation. To achieve our proposed optimization objective, minimizing
CASE (Constrained Absolute Sum of Error), SQuant progressively composes three approximate
sub-items under constraint relaxation, introduced in Sec. 3.3. Finally, SQuant needs to work out
a flipping set f to minimize the CASE of each kernel and output channel. We design an efficient
algorithm with a linear computation complexity to find a proper f based on Eq. (8), in Sec. 3.4.
3.2	Diagonal Hessian Approximation
In this work, we propose a new approximation of the Hessian matrix to cover non-diagonal elements
and decompose Eq. (4) into three sub-items that correspond to the three dimensions of the weight
3
Published as a conference paper at ICLR 2022
N X K
ΔW m,:
H-E: SQUant-E
H-K: SQUant-K
■ ΔW
T
m,:
E[xx T ]
H-C: SQuant-C
K
Figure 1: AWm,： E[xxT]∆Wm,：T. SQaunt-E, SQaUnt-K, and SQUant-C are three approximate SUb-
items, which cover H-E, H-K and H-C, respectively.
tensor as illustrated in Fig. 1: SQuant-E for element-wise optimization covers the diagonal elements
of Hwm (H-E); SQuant-K for kernel-wise optimization covers the diagonal blocks of Hwm (H-K);
SQuant-C for output channel-wise optimization covers the whole Hwm (H-C).
The E[x'x'T] can be approximated by the following equation:
E[x'x'T] ≈ E + K + C,	(5)
where C = cmJNK ,
k1 JK
K =
e1,1
,and E =
kNJK
In the above equations, JNK is an all-one matrix with the dimension of N × K (denoted as NK), and
cm is a constant value for m-th output channel. K is a diagonal block matrix, where JK represents an
all-one matrix with the dimension of K × K. The n-th diagonal block corresponds to n-th kernel in
convolution and has its own constant value kn . E is a diagonal matrix with the diagonal elements of
en,i, each of which is a constant value corresponding to i-th element of n-th kernel.
Eq. (5) provides an approximation that preserves as much information from three different levels
of E[x'x' ] as possible, which we explain in Appendix A.1. The matrix C catches the common
component of the Hessian matrix, while the matrix E reserves the individual components in the
diagonal line of the Hessian matrix. In addition, we consider kernel-wise approximation for convo-
lution layers by using matrix K. For each inference, the weights of a kernel, Wm,n二，scan the same
feature map. As a result, the corresponding x has nearly the same expectation values in the center
area, with a small perturbation in the marginal area due to padding. Therefore, knJK as a kernel-wise
approximation achieves a low approximate error for convolution. For any E[x'x' ], we can always
find a decomposition that satisfies en,i, kn , cm > 0, for which we present the decomposition method in
Appendix A.2. Substituting Eq. (5) into Eq. (4) yields the following equation.
AWm,: E[x'x'T]∆Wm,: T ≈ ∑ en ,i∆Wm ,“； + ∑ kn∆W ,nJ K ∆Wmj ,n,: T + Cm AWm ,:JNK ∆Wm,: T .(6)
n,i	n
3.3	Data-free Optimization
To achieve the data-free optimization objective, we omit the coefficients (en,i, kn and cm) in Eq. (6),
which leads to the approximate objective in Eq. (8) optimized by our fast SQuant framework. We
present the omitting process and empirically verify that the approximation does almost not influence
the performance in Appendix A.2 and Appendix A.3. It can be easily found that there are no training
samples needed to minimize,
argmin ∑∆Wmj ,“； + ∑∆Wmj n ,:JK AWm, n,: T + AWm ,JNK AWm:	⑺
AWm,:	n, i	n
=argmin ∑AWm ,如：+ ∑ (∑AWm ,n,i) + 心Wm, n ,i)2∙	(8)
4
Published as a conference paper at ICLR 2022
Next, we transform the overall objective Eq. (8) in the discrete space and explain how to compose
and optimize the three approximated sub-items in order. Without loss of generality, we assume all
weights have been scaled with the scale parameter Sm for Wm,：.
Sub-item Analysis For the element-wise item, i.e., the first item in Eq. (8), the problem is reduced
to the following objective, which we call SQuant-E.
函Cm,: = argmin ∑AWm ,n, i 2 = argmin IAWm ,n,"令M为2,n, i, £cmm, n, i | ≤ re = 0.5,	⑼
AWm,: n, i	∆Wm, n, i
SQuant-E is essentially the rounding method when re = 0.5. Rounding does not introduce any
approximate error and has O(1) complexity for each weight element. However, as many previous
works pointed out (Nagel et al., 2020), rounding-to-nearest is not optimal because it only considers
the diagonal elements the matrix E[x'x'T] while ignores the rest majority elements.
For kernel-wise item (the second item in Eq. (8)), we have the following objective called SQuant-K,
ACm,:= argmin∑ (EAWm,n,i)2 = argmin £AWm,n,i1 ⇔∀Acm,n,:, ∑Acm,n,i1 ≤ rk = 0.5>	(Io)
∆Wm,: n i	AWm,n,: i	i
where ∣ ∑i- AWm n i I is the Absolute Sum of Error (ASE) of each kernel-wise weight matrix in the
convolution and rk equals 0.5 because of the discrete quantization. In other words, SQuant is based
on the insight of Sum of (Signed) error instead of the accumulation of absolute (unsigned) error.
Similarly, for the output channel-wise item (the third item in Eq. (8)), we have SQuant-C,
Acm,:=argmin (∑AWm, n, i)2 ⇔∀AWm,:, |EAcm n, i∙ | ≤ rc=0.5.	(II)
AWm : n, i	n, i
Relaxation Obviously, re = 0.5 is against rk = 0.5 because rounding (re = 0.5) only guarantees
the upper-bound rk = 0.5K for SQUant-K. Some elements need to relax the constraint re to a larger
number, such as 1.0, to satisfy rk = 0.5. Similarly, SQuant-C also needs to relax rk = 1.0.
CASE Flipping We adopt the flipping approach (Nagel et al., 2020) to minimize the ASE. Due to
the discrete quantization, rounded elements can be flipped (from rounding up to rounding down and
vice versa) with ±1 integer mutation. Formally, we need to work out a flipping set fm to satisfy the
overall objective Eq. (8) by composing these three sub-items in order (SQuant-E → SQuant-K →
SQuant-C) with constraints relaxation. After optimization, the fm will be
∀(m, n, i) ∈ fm, IAWm,n,iI ≤ 0.5;	∀(m, n, j) ∈ fm, 0.5 ≤∣AWm,n,j < 1.0,	(12)
where fm is the index set of flipped elements for m-th output channel. Specifically, we need to flip k =
[ASEe elements, whose perturbation has the same sign as ∑i- AWm n i ∙ We prove the equivalence for
Eq. (10) and Eq. (11) by illustrating the transformation process to a discrete problem in Appendix B.1.
However, any k elements can satisfy Eq. (10) leading to large search space. Fortunately, based on
Eq. (9), SQuant-K can select specific k elements with the top-k largest perturbation because they will
have the smallest perturbation after flipping under the constraint of SQuant-E. Therefore, we adopt
the Constrained ASE (CASE) to optimize the SQuant-E&K composition via the top-k perturbation
algorithm, which is the only solution for minimizing the CASE proven in Appendix B.2. Obviously,
SQuant-E&K&C needs to “flip” the “SQuanted” kernel after SQuant-E&K. Notice that we can only
flip one element for a kernel to satisfy the constraint rk = 1.0.
The following section will introduce an efficient SQuant algorithm with a linear computation com-
plexity for CASE flipping.
3.4	On-the-Fly S Quant
Progressive Algorithm We design a progressive algorithm illustrated in algorithm 1 to meet our
stated optimization objective, i.e., minimizing the CASE of weight perturbation. The critical insight
of the progressive algorithm is to gradually calibrate the deviation from the optimal global solution
introduced by the fine-grained diagonal sub-item. To calibrate the SQuant-E, SQuant-K flips certain
rounded elements. After the SQuant-K calibration, SQuant-C then further flips SQuanted kernels.
5
Published as a conference paper at ICLR 2022
We start by rounding the weight and updating its perturbation to satisfy re = 0.5 (Line 4-5). Then
we run the SQuant-K (Line 6) to flip specific elements under re = 1.0, satisfy rk = 0.5, and update
kernel perturbation (Line 7). The follow-up SQuant-C (Line 8) further flips specific kernels under
rk = 1.0 and satisfy rc = 0.5. Finally, we derive the quantized weights (Line 9).
1
2
3
4
5
6
7
8
9
1
2
3
4
5
6
7
Algorithm 1: Progressive SQuant Algorithm.
Input: Weight tensor W of layer ', scale factor S of layer '.
Output: Quantized weight tensor C of layer `.
foreach m ∈ [1, ..,M] do // SQuant-C: SQuant M output channels.
foreach n ∈ [1, ..,N] do // SQuant-K: SQuant N kernels.
foreach i ∈ [1, ..., K] do // SQuant-E: Round K elements.
Em,n,i = bWm,n,i/sme // Scale and SQuant-E (Rounding).
∆E m, n, i = E m, n, i - W m, n, i // Element perturbation.
Km,n,: = SQuantFlip(Em,n,:, ∆Em,n,:)// SQuant-K.
AKm,n,: =UPdatePertUrbation(AEm,n,：) // Kernel perturbation.
_ Cm,:= SQUantFliP(Km,:, AKm,:)• Sm// SQuant-C.
return C
Flip Algorithm SQUant-K and SQUant-C can Utilize the same fliP fUnction. The goal of the fliP
algorithm is to find a ProPer element set f to fliP and minimize the CASE dePicted in algorithm 2.
First, we need to comPUte the accUmUlated PertUrbation (e) (Line 2). We select weights with Positive
PertUrbation to decrease the Positive e and vice versa for negative e. Therefore, we set 0 for the
elements with a different sign (Line 3) to disable them. ObvioUsly, we need only k = b|e|e elements
and redUce |e| < rk = 0.5 (Line 4). Finally, we fliP k weights with the largest |p| (Line 5-6). For now,
we have SQUanted the kernel and tUned kernel CASE to |e| ≤ rk = 0.5. SPecifically, for FC and Conv
with a kernel size of (1, 1), we can skiP the SQUant-K. As mentioned in Section 3.3, SQUant-C fliPs
only one element in each kernel. Therefore, we UPdate the kernel PertUrbation (Line 7 of algorithm 1)
for SQUant-C to fliP kernel illUstrated in APPendix B.3. As a resUlt, SQUant sUccessfUlly identifies
the oPtimUm combination f Under a low comPUtation comPlexity, which we analyze in APPendix B.4.
Algorithm 2: SQUant FliP Algorithm.
Input: RoUnded/SQuanted Weight w;
Weight PertUrbation p.
Output: UPdated QUantized Weight w.
def SQuantFlip(w, p):
e=∑ipi// Accumulated perturbation.
p[e• P < 0] = 0 // Disable Elements/kernels with different sign from e.
k= b|e|e // Flip k elements/kernels based on the CASE.
f = ToPK(|p|, k).indices// Indices of k largest perturbation.
w[f] = FliP(w[f])// Flip k elements/kernels with same sign as e.
return w
On-the-Fly Framework From the overall PersPective of the oPtimization, SQUant-K has MN sUb-
Problems, while SQUant-C has M sUb-Problems. BecaUse of the indePendence of sUb-Problems,
SQUant is friendly for DNN accelerators, e.g., GPU, allowing each sUb-Problem to be accelerated in
Parallel. WithoUt the reqUirement of back-ProPagation nor fine-tUning, SQUant can rUn on inference-
only devices with constrained comPUtation and memory resoUrces on the fly. That Provides new
oPPortUnities for oPtimizing weight qUantization. In the next section, we demonstrate the imPressive
efficiency and accUracy of SQUant.
4	Experiments
For demonstrating the strength of SQUant, we evalUate the SQUant as well as foUr SOTA methods,
DFQ (Nagel et al., 2019), ZeroQ (Cai et al., 2020), DSG (Zhang et al., 2021; Qin et al., 2021), and
6
Published as a conference paper at ICLR 2022
ArCh	Method No BP No FT W-bit A-bit Top-1					
Baseline	一	一	32	32	71.47
DFQ	X	X	4	4	0.10
ZeroQ	X	X	4	4	19.09
DSG	X	X	4	4	34.53
GDFQ	X	X	4	4	60.60
SQuant	X	X	4	4	66.14
ResNet18 DFQ	X	X	6	6	67.30
ZeroQ	X	X	6	6	69.84
DSG	X	X	6	6	70.46
GDFQ	X	X	6	6	70.13
SQuant	X	X	6	6	70.74
DFQ	X	X	8	8	69.70
ZeroQ	X	X	8	8	71.43
GDFQ	X	X	8	8	70.68
SQuant	X	X	8	8	71.47
Baseline	一	一	32	32	77.74
ZeroQ	X	X	4	4	7.75
DSG	X	X	4	4	23.10
GDFQ	X	X	4	4	55.65
SQuant	X	X	4	4	70.80
ResNet50 ZeroQ	X	X	6	6	72.93
DSG	X	X	6	6	76.07
GDFQ	X	X	6	6	76.59
SQuant		X	6	6	77.05
ZeroQ	X	X	8	8	77.65
DSG	X	X	8	8	77.68
GDFQ	X	X	8	8	77.51
SQuant	X		8	8	77.71
Table 1: Results of data-free methods with
ResNet18 and ResNet50. “No BP” means that no
back-propagation algorithm is used to generate
data, “No FT” means no fine-tuning (retraining)
for weight quantization.
ArCh	Method No BP No FT W-bit A-bit Top-1					
	Baseline	—	—	32	32	78.81
	ZeroQ	X	X	4	4	18.20
	GDFQ	X	X	4	4	70.39
	SQuant	X	X	4	4	73.26
InCeption	ZeroQ	X	X	6	6	74.94
V3	GDFQ	X	X	6	6	77.20
	SQuant	X	X	6	6	78.30
	ZeroQ	X	X	8	8	78.78
	GDFQ	X	X	8	8	78.62
	SQuant	X	X	8	8	78.79
	Baseline	—	—	32	32	69.38
	ZeroQ	X		4	4	0.09
	GDFQ	X	X	4	4	28.93
	SQuant	X		4	4	43.45
Squeeze	ZeroQ	X	X	6	6	16.54
Next	GDFQ	X	X	6	6	65.46
	SQuant	X	X	6	6	67.34
	ZeroQ	X	X	8	8	68.18
	GDFQ	X	X	8	8	68.22
	SQuant	X	X	8	8	69.22
	Baseline	—	—	32	32	65.07
	ZeroQ	X	X	6	6	35.21
Shuffle Net	GDFQ	X	X	6	6	60.12
	SQuant	X	X	6	6	60.25
	ZeroQ	X		8	8	64.34
	GDFQ	X	X	8	8	64.03
	SQuant			8	8	64.68
Table 2: Results of data-free methods with Incep-
tionV3, SqueezeNext, and ShuffleNet.
GDFQ (Xu et al., 2020), with 5 different CNN models including ResNet-18 &50(Heetal., 2016),
Inception V3 (Szegedy et al., 2016), SqueezeNext (Gholami et al., 2018) and ShuffleNet (Zhang
et al., 2018) on the golden standard dataset ImageNet (Krizhevsky et al., 2012).
In our experiments, SQuant is dedicated to weight quantization, including setting quantization range
and selecting the grid point with per-channel quantization, which is friendly for hardware accelerators.
With the BN-based approach, we adopt a simple rounding method and a wide quantization range
for activation suggested by DFQ (Nagel et al., 2019) without breaking the data-free premise. We
clip activation tensors in a layerwise manner (per-tensor). We utilize a uniform distribution as
the initialization for the activation quantization range. All DFQ algorithms are implemented with
PyTorch (Paszke et al., 2019) and evaluated on Nvidia GPU A100-40GB. Unless otherwise stated, we
employ both weight and activation quantization in all experiments. Also, uniform quantization grids
are used in all experiments, and hyper-parameters, e.g., re = rk = 1.0 and rc = 0.5, for all SQuant
experiments are the same.
4.1	Comparison to SOTA Methods
Table 1 and Table 2 show the results on the ImageNet datasets for various bit-width choices, comparing
our SQuant against other data-free methods. Among these methods, ZeroQ, DSG, and GDFQ are
data-generative approaches with back-propagation. The former two are PTQ methods, while the last
is a QAT method, which retrains the network with the synthetic data. DFQ is the only true data-free
method with weight equalization and bias correction.
Experiments show that SQuant significantly outperforms all other SOTA DFQ methods, even with
synthetic dataset calibrating their networks. The 8-bit quantization preserves better network accuracy
7
Published as a conference paper at ICLR 2022
ArCh	ResNet18	ResNet50	InCeptionV3	SqueezeNext	ShuffleNet
Layers	21	54	95	112	50
SQuant Time (ms)	84	188	298	272	121
ZeroQ Time (s)	38	92	136	109	38
GDFQ Time (hour)	1.7	3.1	5.7	4.8	1.9
Table 3: SQuant, ZeroQ and GDFQ 4-bit quantization time on GPU A100
than the lower-bit quantization does because of higher precision. The benefit of SQuant becomes more
prominent as the bit-width decreases. SQuant outperforms the PTQ methods, i.e., DFQ, ZeroQ, and
DSG, more than 30% on all models with 4-bit quantization. It is noteworthy that SQuant surpasses
GDFQ in all cases and even surpasses more than 15% in ResNet50 under 4-bit quantization, although
GDFQ is a quantization-aware training method.
Table 1 and Table 2 also show that GDFQ significantly outperforms ZeroQ and DSG under lower-bit
settings (e.g., 4-bit). Since we use the same activation quantization method for evaluating these
methods, the results indicate that the weight quantization plays a critical role in the overall model
quantization. However, GDFQ requires fine-tuning (FT) with back-propagation (BP). In contrast,
SQuant adopts a direct optimization objective of weight perturbation, which does not require fine-
tuning nor BP, and still outperforms GDFQ in the 4-bit setting. These results clearly illustrate the
advantages of SQuant, a CASE-based optimization framework, which is to minimize the CASE of
weight perturbation.
4.2	S Quant Efficiency
The trade-off between efficiency and accuracy is challenging for previous DFQ methods. Before
SQuant, DFQ is the fastest one since it does not require back-propagation and fine-tuning, but it
performs poorly, especially in low-bit cases. GDFQ performs relatively well but takes hours to
complete 400 epochs that produce synthetic data from weights and fine-tune the network. SQuant
employs the direct optimization objective, minimizing the CASE of weight perturbation, pushes
the quantization procedure to a sub-second level. Table 3 shows the 4-bit quantization time of the
five models using SQuant, ZeroQ, and GDFQ. The efficient algorithm design also contributes to the
surprising results. Note that the SQuant results in Table 3 are the sum of all layer quantization time,
and it will be faster if we quantize layers in parallel. A single layer takes SQuant just 3 milliseconds
on average because SQuant does not involve complex algorithms, such as back-propagation and
fine-tuning. That means we can implement the SQuant algorithm on inference-only devices such as
smartphones and IoT devices and quantize the network on the fly.
4.3	Ablation study
SQuant Granularity We decouple the effect of SQuant-K and SQuant-C, which have different
granularities to optimize CASE. As shown in Table 4, their accuracies both outperform SQuant-E
Method	W-bit	A-bit	Top-1	Method	No BP	No SD	W-bit	A-bit	Top-1
Baseline	32	32	71.47	Baseline	一	—	32	32	71.47
SQuant-E	3	32	2.05	ZeroQ + AdaRound	X	X	3	32	49.86
				DSG + AdaRound	X	X	3	32	56.09
SQuant-E&C SQuant-E&K	3 3	32 32	40.87 52.07	SQuant	X	X	3	32	60.78
									
SQuant-E&K&C	3	32	60.78	ZeroQ + AdaRound	X	X	4	32	63.86
				DSG + AdaRound	X X	X X	4	32	66.87
SQuant-E	4	32	48.15	SQuant			4	32	69.75
SQuant-E&C	4	32	67.14	ZeroQ + AdaRound	X	X	5	32	68.39
SQuant-E&K	4	32	68.07	DSG + AdaRound	X	X	5	32	68.97
SQuant-E&K&C	4	32	69.75	SQuant	X	X	5	32	71.19
Table 4: SQuant ablation results with				Table 5: ResNet18 results of SQuant, ZeroQ and DSG					
ResNet18.				with AdaRound. “No SD” means no synthetiC data. 8					
Published as a conference paper at ICLR 2022
(i.e., rounding), and combining them leads to higher accuracy for ResNet18. SQuant-E&C has a
lower accuracy than SQuant-E&K because SQuant-C has a more significant approximation error than
SQuant-K. On the other hand, SQuant-E alone is not optimal because it uses a smaller granularity
and ignores a large amount of Hessian information as we analyze in Section 3. This ablation study
shows that SQuant-E&K&C achieves the best accuracy by exploiting the most Hessian information
(H-C), and SQuant-E&K also achieves a higher accuracy with H-K than SQuant-E with H-E.
Comparison to Data-free AdaRound AdaRound (Nagel et al., 2020) is a novel data-driven PTQ
method, which also utilizes the Hessian-based approach to round-up or round-down the weights
with an approximation assumption. Under the data-free premise, we augment ZeroQ and DSG
with the AdaRound by feeding their generated synthetic data to AdaRound. Results in Table 5
show that SQuant has better accuracy than data-free AdaRound because SQuant directly optimizes
the CASE objective instead of the MSE of the output activation adopted by AdaRound. It is hard
for DSG+AdaRound to find the optimal solution with an excessively long optimization path and
gradient-based approaches. Even though AdaRound tries a shorter way to fine-tune the weights in the
layer-wise fashion, SQuant still outperforms AdaRound in quantization time and accuracy.
5	Related Work
Compression is a promising method to reduce the DNN model’s memory and computation cost.
Pruning (Han et al., 2015b;a) is one of the effective approaches to exploit the inherent redundancy
of DNN. However, pruning will cause sparse irregular memory accesses. Therefore, pruning needs
software (Gale et al., 2020; Guan et al., 2020; Qiu et al., 2019; Guo et al., 2020a; Guan et al., 2021;
Fedus et al., 2021) and hardware (Gondimalla et al., 2019; Guo et al., 2020b; Zhang et al., 2020;
Wang et al., 2021) optimization to accelerate.
Quantization is more practical because it can be supported directly by existing accelerators.
Quantization-aware training (QAT) (Gupta et al., 2015; Jacob et al., 2018; Wang et al., 2019;
Zhuang et al., 2021) is one of the most promising techniques to retrain networks and mitigate the
accuracy drop introduced by quantization. However, the training procedure is time-consuming and
costly. Therefore, post-training quantization (PTQ) (Banner et al., 2018; Choukroun et al., 2019;
Zhao et al., 2019; Nagel et al., 2020) has earned lots of attention due to the absence of any fine-tuning
or retraining process, at the expense of accuracy.
Recently, several methods for CNN quantization without the original training datasets have been
proposed. These methods are known as data-free quantization (DFQ), including PTQ (Nagel et al.,
2019; Cai et al., 2020; Zhang et al., 2021) and QAT (Xu et al., 2020; Liu et al., 2021; Qin et al.,
2021; Choi et al., 2020). DFQ (Nagel et al., 2019) and ACIQ (Nagel et al., 2019) rely on weight
equalization or bias correction without requiring synthetic data. Other works synthesize the data to
calibrate or fine-tune the network based on the batch normalization statistics (Cai et al., 2020) or
adversarial knowledge distillation techniques (Liu et al., 2021; Choi et al., 2020).
6	Conclusion
This paper approximates and composes the original Hessian optimization objective into the CASE
of weight perturbation with a data-free premise. Surprisingly, CASE only involves the weight
perturbation and requires no knowledge of any datasets or network architecture. Based on that, we
proposed the on-the-fly SQuant framework. We used a progressive algorithm to minimize CASE
directly and significantly improve accuracy than other DFQ methods. SQuant considerably reduces
optimization complexity and accelerates the data-free quantization procedure, which previously
requires back-propagation with massive computation and memory resources consumption seen in
other works. In summary, SQuant outperforms other data-free quantization approaches in terms of
accuracy and pushes the quantization processing time to a sub-second level.
Acknowledgments
We would like to thank the anonymous reviewers for their constructive feedback. This work was
supported by the National Key R&D Program of China under Grant 2021ZD0110104, and the
National Natural Science Foundation of China (NSFC) grant (U21B2017, 62072297, and 61832006).
9
Published as a conference paper at ICLR 2022
References
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of
convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018.
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq:
A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 13169-13178, 2020.
Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, and Jungwon Lee. Data-free network quantization
with adversarial knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, pp. 710-711, 2020.
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks
for efficient inference. In ICCV Workshops, pp. 3009-3018, 2019.
Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via
layer-wise optimal brain surgeon. arXiv preprint arXiv:1705.07565, 2017.
Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and
Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv
preprint arXiv:1911.03852, 2019a.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian
aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 293-302, 2019b.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning.
In SC20: International Conference for High Performance Computing, Networking, Storage and
Analysis, pp. 1-14. IEEE, 2020.
Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and
Kurt Keutzer. Squeezenext: Hardware-aware neural network design. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops, pp. 1638-1647, 2018.
Ashish Gondimalla, Noah Chesnut, Mithuna Thottethodi, and TN Vijaykumar. Sparten: A sparse
tensor accelerator for convolutional neural networks. In Proceedings of the 52nd Annual IEEE/ACM
International Symposium on Microarchitecture, pp. 151-165, 2019.
Yue Guan, Jingwen Leng, Chao Li, Quan Chen, and Minyi Guo. How far does bert look at:
Distance-based clustering and analysis of bert’s attention. In Proceedings of the 28th International
Conference on Computational Linguistics, pp. 3853-3860, 2020.
Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, Minyi Guo, and Yuhao Zhu. Block-skim:
Efficient question answering for transformer. arXiv preprint arXiv:2112.08560, 2021.
Cong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang, Xiaoying Jia,
Xipeng Li, Minyi Guo, and Yuhao Zhu. Accelerating sparse dnn models without hardware-support
via tile-wise sparsity. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pp. 1-15, 2020a.
Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen, Chao Li, Bin Yao,
and Minyi Guo. Balancing efficiency and flexibility for dnn acceleration via temporal gpu-systolic
array integration. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pp. 1-6. IEEE,
2020b.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International conference on machine learning, pp. 1737-1746.
PMLR, 2015.
10
Published as a conference paper at ICLR 2022
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for
efficient neural networks. arXiv preprint arXiv:1506.02626, 2015b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post
training neural quantization: Layer-wise calibration and integer programming. arXiv preprint
arXiv:2006.10518, 2020.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient
integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2704-2713, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi
Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint
arXiv:2102.05426, 2021.
Yuang Liu, Wei Zhang, and Jun Wang. Zero-shot adversarial quantization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1512-1521, 2021.
Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization
through weight equalization and bias correction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 1325-1334, 2019.
Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or
down? adaptive rounding for post-training quantization. In International Conference on Machine
Learning, pp. 7197-7206. PMLR, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Xu Qian, Victor Li, and Crews Darren. Channel-wise hessian aware trace-weighted quantization of
neural networks. arXiv preprint arXiv:2008.08284, 2020.
Haotong Qin, Yifu Ding, Xiangguo Zhang, Aoyu Li, Jiakai Wang, Xianglong Liu, and Jiwen
Lu. Diverse sample generation: Pushing the limit of data-free quantization. arXiv preprint
arXiv:2109.00212, 2021.
Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, and Yuhao Zhu. Adver-
sarial defense through network profiling based path extraction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 4777-4786, 2019.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney,
and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8815-8821, 2020.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
11
Published as a conference paper at ICLR 2022
Yang Wang, Chen Zhang, Zhiqiang Xie, Cong Guo, Yunxin Liu, and Jingwen Leng. Dual-side sparse
tensor core. In Proceedings of the 48th Annual International Symposium on Computer Architecture,
ISCA '21,pp.1083-1095, 2021.
Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. Learning channel-wise interactions for
binary convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 568-577, 2019.
Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Understanding
common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261, 2020.
Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan.
Generative low-bitwidth data free quantization. In European Conference on Computer Vision, pp.
1-17. Springer, 2020.
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang,
Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization.
In International Conference on Machine Learning, pp. 11875-11886. PMLR, 2021.
Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Michael W Mahoney, and Kurt Keutzer.
Hessian-aware pruning and optimal neural implant. arXiv preprint arXiv:2101.08940, 2021.
Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li,
Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantization.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
15658-15667, 2021.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient
convolutional neural network for mobile devices. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 6848-6856, 2018.
Zhekai Zhang, Hanrui Wang, Song Han, and William J Dally. Sparch: Efficient architecture for sparse
matrix multiplication. In 2020 IEEE International Symposium on High Performance Computer
Architecture (HPCA), pp. 261-274. IEEE, 2020.
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network
quantization without retraining using outlier channel splitting. In International conference on
machine learning, pp. 7543-7552. PMLR, 2019.
Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, and Chunhua Shen. Effective training
of convolutional neural networks with low-bitwidth weights and activations. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2021.
A Approximation and Decomposition
A. 1 Approximated Hessian Matrix for data-free quantization
The quantization loss function for the entire network is
L (∆W) = ∆WE[H]∆WT.	(13)
Consider a convolution layer defined as
Ym,h,w = ∑Wm,n,i,jXn,h-i,w-j.	(14)
n,i, j
Here, Y has three dimensions, output channel, output feature map height, and output feature map
width, i.e., (M × OH × OW) indexing by (m, h, w), W has four dimensions, output channel, input
channel, kernel height, kernel width, i.e., (M × N × KH × KW) indexing by (m, n, i, j), and X
has three dimensions, input channel, input feature map height, and input feature map width, i.e.,
(N × IH × IW) indexing by (n,h - i,w - j). Ignoring the interaction between layers and output
12
Published as a conference paper at ICLR 2022
channels following Nagel et al. (2020), for a specific convolution layer l and output channel m, the
elements of corresponding output channel-wise Hessian HWa is
HWa	. . =	d2L
n, i, j, n 0, i0, j 0	d W a, n, i, j d W m ,n0, i 0,j0
=	∂ y	∂ L	∂ Y m, h ,w
dWm,n,i,j h,w dYm,h,w dWm,n,i,j
∂	yL	∂ L
dWm,n,i,j h,w dYm,h,w
∂ L
∂ Ym,h0,w0
Xn,h-i,w- j
=∑
h,W	m
=∑∑
h,w h0,w0
___ y d L X	ʌ X
j ∑ ∂Y	IlXn0,h0-i0,W0-j0	Xn,h-i,w-j
,h,W h0,W0 Ym,h0,W0
∂ 2L
∂ Y d一∂Y	Xn, h - i, W-j Xn0, h 0-i0, W0-j 0
m,h,w	m,h0,w0
(15)
(16)
(17)
(18)
(19)
(20)
Assuming V；°L is a diagonal matrix yields Eq. (30) in (Nagel et al., 2020)
HWa	≈y
n,i, j,n0,i0, j0	∑
h,W
∂ 2L
OV2	X n, h—i, w—j X n, h — i0, w—j0.
m,h,W
(21)
To make Eq. (13) get irrelevant to training samples, we assume that input feature maps auto-correlate
with each other in a similar way, resulting in
w'	∂ 2L
E[Hn,i,j,n0,i0,j0] = E[Σ ∂y2	Xn,h-i,w-jXn0,h-i0,w- j0] ≈ cm	(22)
h,W Ym,h,W
for all n, i, j, n0 , i0 and j0 , where cm is a constant. It should be noted that Eq. (22) is a strong assumption.
For more accurate approximation, we further look into each input channel (i.e., n = n0, i 6= i0, and
j 6= j0), and find
「一w£” ∙， 「口 ∂2L _	_	η	一、
E[H i, j, i, j 0 ] = E[∑ ∂y2^^ Xn,h-i,w- j Xn,h-i0,w- j0] ≈ km,n ,	(23)
h,w Ym,h,w
for all i, j, i0, and j0, where km,n is a constant. It is generally true because the kernel size is usually
much smaller than the size of feature maps, so the shift introduced by different i, j, i0 , and j0 is a
„	1 .	wWmn i	[•…	i	.	>	. „
small perturbation of E[Hi, j,mi0,n, j0] compared with the summation over the entire feature map. Finally,
we focus on each diagonal element of Hessian matrix (i.e., n = n0, i = i0, and j = j0) and denote
`	∂2L
E[HW m,n,i,j] = E[∑ IY- X n, h -i, w -j] = em, n ,i,j,	(24)
h,w Ym,h,w
where em,n,i, j is a constant. Please note that the output channel-wise expected Hessian matrix
w`
E[Hn,im, j,n0,i0, j0] is a principle submatrix of E[H], so it must positive semi-define. Therefore, we set
em n i j > km n > cm > 0 to ensure the approximation to E[HWim 0 i0 0] is also positive semi-define
, , ,	,	n,i, j,n ,i ,j
and nontrivial. Considering Eq. (22), Eq. (23), and Eq. (24) at the same time, we can get the
approximation to expected Hessian shown in Eq. (5). Extending the discussion to fully connected
layer is straightforward thus omitted here.
13
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
A.2 Decomposition
In this section, We present the decomposition method for HWa = lmE[x'x'T] illustrated in the
algorithm 3. First, we construct three matrices with the shape of (NK, NK), C0 = JNK,
J K	1	1 一
K0 =	...	,and E0 =	...	.
Here, JNK is an all-one matrix With dimension NK = N × K and JK represents an all-one matrix With
dimension K. The n-th diagonal block corresponds to n-th kernel in convolution and has the same
constant kn. E is a diagonal matrix Whose diagonal elements are 1.
Algorithm 3: E[x'x'T] Decomposition.
Input: E[x'x'T], H;
Channel-Wise matrix, C0 .
Kernel-Wise matrix, K0 .
Element-Wise matrix, E0 .
Output: Matrix E,K, and C.
H0 = |H|
Cm = (1 — ε) ∙min(H0) // Output channel-wise.
C=cmC0
foreach n ∈ [1, ...,N] do // Kernel-wise.
kn = (I - εn ) ∙ min(Hn:n+K, n:n+K - Cm )
Kn,: = kn K0n,:
foreach i ∈ [1, ..., K] do // Element-wise.
en, i = Hn × K+i,n×K+i - Cm - kn
_ E n, i = en, iEn, i
return E, K, C
In algorithm 3, 0 < ε,ε0 < 1, and We can get the matrices E,K, and C. Evidently, algorithm 3 can
make Cm > 0, kn > 0, and en,i > 0 for any E[x'x'T] ≈ E + K + C.
A.3 Approximation Error Analysis
To achieve the data-free optimization objective, We omit the coefficients (en,i, kn and Cm) in Eq. (6),
Which leads to the approximate objective in Eq. (8) optimized by our fast SQuant frameWork. We
approximate Eq. (6) to Eq. (8) to enable fast data-free quantization. The approximation error is
insignificant as our comprehensive results have shoWn the high accuracy of the final quantized model
in Table 1 and Table 2 of the manuscript. The intuition behind the approximation is that We use an
iterative process Which progressively reduces each term of Eq. (6). Because each term’s coefficient
(en,i, kn, and Cm) is positive, the reduction of each term Would generally lead to the reduction of the
precise objective in Eq. (6). In this section, We provide an empirical analysis of the approximation
error betWeen Eq. (6) andEq. (8).
In this empirical experiment, We use the real dataset to generate the precise coefficients of en,i, kn , and
Cm in Eq. (6). To quantify the approximation error in our SQuant frameWork, We evaluate a metric
called approximation precision and shoW that We achieve a nearly 95% approximation precision.
Since SQuant uses the flipping-based iterative optimization frameWork to minimize Eq. (8), We define
an element as correctly flipped if its flipping leads to the decrease of the precise objective Eq. (6) and
approximate objective Eq. (8). The approximation precision (AP) is the ratio of the correct element
based on data-free Eq. (8) compared to data-driven Eq. (6), i.e.,
Number of correct elements
AP =---------------------------.
Number of flipped elements
14
Published as a conference paper at ICLR 2022
Layers	SqUant-E&K			SqUant-E&K&C		
	Flipped Correct		AP	Flipped Correct		AP
1	2346	2346	100.00 %	123	123	100.0 %
2	2683	2594	96.68 %	97	97	100.0 %
3	2662	2653	99.66 %	100	100	100.0 %
4	2698	2630	97.48 %	107	107	100.0 %
5	-	-	-	230	220	95.7 %
6	5349	5339	99.81 %	223	223	100.0 %
7	10782	10676	99.02 %	332	332	100.0 %
8	10777	10633	98.66 %	342	342	100.0 %
9	10655	10424	97.83 %	348	348	100.0 %
10	-	-	-	649	619	95.4 %
11	21371	21173	99.07 %	663	663	100.0 %
12	43116	41561	96.39 %	906	906	100.0 %
13	42976	41402	96.34 %	932	932	100.0 %
14	43321	41315	95.37 %	918	918	100.0 %
15	-	-	-	1988	1639	82.4 %
16	86010	83070	96.58 %	1846	1846	100.0 %
17	171344	161358	94.17%	2629	2629	100.0 %
18	172071	158066	91.86%	2602	2602	100.0 %
19	172623	154536	89.52 %	2504	2504	100.0 %
Total	800784	749776	93.6%	17539	17150	97.8 %
Acc.	68.07			69.75		
Table 6: ResNet18 results under 4-bit weight-only quantization. “Flipped” is the number of the
Flipped elements after SQuant optimization. “Correct” is the number of elements has the same
optimization direction as the precise objective. AP is the approximation precision.
We perform the above approximation error analysis on ResNet18 with ImageNet under 4-bit weight-
only quantization. We evaluate the SQuanted weight on the inference datasets. We compute the
coefficients en,i, kn, and cm with 1000 samples. Table 6 shows the results, which clearly show that
SQuant-E&K&C achieves nearly 100% approximation precision. In other words, nearly all flipped
elements can indeed reduce the precise objective in Eq. (6). Based on this empirical study, we show
that the approximation from Eq. (6) to Eq. (8) is effective for our data-free quantization.
B S quant Algorithm
B.1 Discrete Optimization Problem
We introduce the transformation of the discrete optimization problem. We know that the quantization
is to round the scaled elements to the integer grid. Each quantization step has two rounding directions,
rounding up and down, with a step size of 1. We have the quantization example within [0, 1] shown in
Fig. 2.
Clearly, the element 0.7 (0.4) can be rounded up (down) to 1.0 (0.0) with +0.3 (-0.4) orignal
perturbation and flipped to 0.0 (1.0) with -0.7 (+0.6) flipped perturbation with -1 (+1) mutation.
Each flipping operation leads to a ±1 integer mutation and increases the perturbation to [0.5, 1.0].
We prove that We can always find k = [| ∑i AWm n ill elements to flip and reduce | ∑i ACm 冗 J ≤ 0.5.
15
Published as a conference paper at ICLR 2022
0	0.5		
Δw = - 0.4 Flipped Δw = - 0.7	0.4 <	e-		► Down ∖ UP 	∣--φ	► I 0.7	Flipped Δw = 0.6 Δw = 0.3
Quantization Step
Figure 2: The flipping approach.
Proof. Assume n-th kernel has K elements, a rounded up elements with index set fa have positive
perturbation, b rounded down elements with index set fb have negative perturbation, and K = a + b.
Then, we have
I ∑∆Wm,n∕∣=∣∑∆Wm,n,t- ∑ ∣∆wm,”/,	t ∈ fa,	j ∈ fb	(25)
≤max(∑∆W1,n,t, ∑ ∣∆W1,n,j∣),	t ∈ fa,	j ∈ fb	(26)
Without loss of generality, let ∑t ∆W2,n,t > ∑j ∣∆W*,n,j∣,
max(∑∆wm,n,t,	∑ ∖∆Wm,n,j∣)= ∑∆Wm,n,t t ∈ fa, j ∈ fb	(27)
≤0.5 ∙ a.	(28)
Therefore, we can always find k = [∣ ∑i∆Wιm,n,』]≤ [0.5 ∙ a] elements in fa with size of a to flip
down and make ∣ ∑i∆cm n i∣ ≤ 0.5. For example, if ∑i∆Wm n i = 3.2, we need to flip 3 elements
with positive perturbation (rounding up) to negative (rounding down) with -3.0 mutation. Then, We
have
∆cm,n: = ɪn (∑∆Wm, n, i )2 ⇒ ∣∑∆cm ,n, i∣ = ∣k-∣∑∆Wm,n , i ∣ I .	(29)
For each kernel ∣ ∑i∆Wm,n,i∣ ≥ 0, ∑i∆cm,n,i has the minimum value ∣ k - ∣ ∑i∆Wm,n,i∣∣ ≤ 0.5. The
sufficiency of Eq. (10) has been proven:
∆cm,: = argmin(∑∆Wm,n,i∙)2 ⇒∀∆cm,”,:，∣∑∆cm,n,i∣ ≤ rk = 0.5	(30)
∆W2
m, n,: 1	l
When a = K, b = 0 and all perturbation = 0.5, original ∑i∆Wm,n,i have the upper bound 0.5 ∙ K. □
Proof. If we flip k - 1 or k + 1 elements for n-th kernel, the ∑i ∆cm n i can reduce to k - 1 -
∣∑i∆wm,n,i∣ 1 and 1 k + 1 -∣∑i∆wm,n,i∣ I ,respectively. Obviously,
∣k - i -∣∑∆Wm ,n,,∙∣ ∣ = I [∣∑∆Wm ,n/ - i -∣∑∆Wm,“ ,i∙∣ i> 0.5,	(3i)
1	i	'	'	i	i	'
I k+1 -∣ ∑∆Wm ,n,,∙∣ ∣ = ∣l∣ ∑∆Wm, n,川+1 -∣ ∑∆Wm,”, i∙∣ ∣> 0.5.	仃幻
1	i	'	'	i	i	1
With other numbers = k, we can also draw the same conclusions. Therefore, when 殊 ≤ 0.5, there is
only one value, i.e., the minimum value, with k flipped elements satisfy the Eq. (10). The necessity
of Eq. (10) has been proven:
argmin(∑∆Wm,n,iɔ2 ⇔^Wm,n,:, ∣∑∆Cm,n,i∣ ≤ rk = 0∙5	(33)
∆W2
m,n,:
Similarly, we can extend all conclusions to SQuant-C.	□
16
Published as a conference paper at ICLR 2022
For SQuant, we only consider the flipping operation in one quantization step and select the elements
whose sign is the same as ∑i AWm n i because flipping with more quantization steps (e.g., flip 0.7 to
-1.0) and the elements with different perturbation signs will cause a more significant perturbation
and will violate the Eq. (8). We explain that in the next section.
B.2 PROOF OF TOK-k PERTURBATION ALGORITHM
Proof. We will prove the SQuant-E&K will lead to the top-k algorithm. We have the composition
SQuant-E and SQuant-K optimization objective for n-th kernel,
argmin ∑ (AWm,n,i)2 + (EAWm,n,i)2,	(34)
AWm, n,:	i	i
which is the first two items of Eq. (8). Without loss of generality, we assume e = ∑i- AWm n i > 0, then
SQuant needs to flip k = [e] elements with perturbation > 0 to transform ∑i- AWm n i to e - k and is
still constant e - k regardless of which k elements are. Therefore, the k elements are only determined
by the first item of Eq. (34), ∑i- (AWm,n,i)2. We denote f as the index set of the k flipped elements and
the original perturbation O = AWm,n,: for n-th kernel. Therefore, Oj > 0, j ∈ f. Substituting f and O
in Eq. (34), we have the optimization objective for f after flipping k elements,
argmin ∑(∣O∕)2 + ∑(1 -|Oj|)2 + (e -k)2, t / f, j ∈ f, Oj > 0	(35)
ft	j
=argmin ∑(∣Oi∙∣)2 -∑(|Oj|)2 + ∑(1 -|Oj|)2, j / f, Oj > 0	(36)
fi	j	j
=argmin ∑ [(1 -|Oj|)2 -|Oj|2],	j / f, Oj > 0	(37)
fj
=argmin ∑(1 -2∣Oj∣),	j / f, Oj > 0	(38)
fj
=argmaχ ∑(∣Oj|), j / f, Oj > 0.	(39)
Therefore, the Eq. (39) is essentially the top-k perturbation algorithm. We can easily extend the top-k
algorithm in SQuant-C and design the perturbation update algorithm in B.3.
□
B.3	Perturbation Update Algorithm
SQuant-K initializes all rounded elements as flip candidates. After SQuant-K, we update the flip
candidates for SQuant-C as shown in algorithm 4 based on the insight of top-k perturbation algorithm
( B.2).
Over SQuant First we define the situation of k > |e| as “Over SQuant” (line 6). For example, if
we have a kernel with e = +1.6, we need to SQuant it to -0.4 to satisfy in (-0.5, 0.5] with flipping
k = 2 elements {2.6, 2.7} to {2, 2}. Obviously, when SQuant-C needs this kernel to calibrate, the last
element 2.7 should be the first and the only candidate (line 7,8) to flip back to the original rounded
number 3 to make the e = +0.6, due to it has the largest element perturbation in the k fliped elements
and the smallest element perturbation (| - 0.3| < 0.5) after it flips back. It is vice versa for e < 0.
Under SQuant For “Under SQuant” (line 9), we need to make the first un-flipped element as the
flip candidate (line 10, 11) for SQuant-C, and will lead the kernel to ”Over SQuant” with absolute
kernel perturbation in (0.5, 1.0) when SQuant-C flips this element of the kernel.
Finally, each kernel has only one candidate flip element for SQuant-C to satisfy Eq. (8). In practice,
it is easy to fuse the perturbation update algorithm with the flip algorithm without extra overhead.
B.4	Complexity Analysis
The original optimization problem described by Eq. (4) is NP-hard with O(M ∙ 2NK). Based on the
SQuant approximation, the new optimization objective is to minimize CASE whose complexity is
17
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 4: Perturbation Update Algorithm.
Input: Weight perturbation p;
Output: Updated weight perturbation p;
def UpdatePerturbation(p):
e=∑ipi// Accumulated perturbation (signed CASE).
p[e ∙ P < 0] = 0 / / Disable Elements/kernels with different sign from e.
k= b|e|e // Flip k elements/kernels based on the CASE.
f = TopK(|p|, k).indices// Indices of k largest perturbation.
if k > |e| then // Over SQuant.
i = fk // The k-th (last) element of f.
V = Pi // 0.5 ≤ |v| < 1.0
else // Under SQuant.
i = TopK(|P|, k + 1).indices[k+1] // The (k + 1)-th largest element of P.
_ V = Pi // |V| ≤ 0.5
P=0// Disable all elements.
Pi = V // The only flip element candidate of the kernel for SQuant-C.
return P
O(M ∙ 2N) for SQuant-C, O(MN ∙ 2K) for SQuant-K and O(MNK) for SQuant-E. SQuant is optimized
to a top-k algorithm with a significant complexity reduction to O(n ∙ log(k)) for each sub-problem.
Our experiments show that after SQuant-K pre-optimization, SQuant-C only requires a tiny top-k
number, such as k = 32, to satisfy all cases. For a 3 × 3 kernel with 9 elements, SQuant-K only needs
k = 4 because the kernel CASE is always ≤ 0.5K = 4.5. Finally, their complexity can reduce to
linear, O(M ∙ N ∙ 5) for SQuant-C and O(MN ∙ 9 ∙ 2) for SQuant-K.
18