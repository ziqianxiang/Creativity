Published as a conference paper at ICLR 2022
Multi-Mode Deep Matrix and Tensor Factor-
IZATION
Jicong Fan1,2
1 School of Data Science, The Chinese University of Hong Kong (Shenzhen), China
2Shenzhen Research Institute of Big Data, China
fanjicong@cuhk.edu.cn
Ab stract
Recently, deep linear and nonlinear matrix factorizations gain increasing atten-
tion in the area of machine learning. Existing deep nonlinear matrix factorization
methods can only exploit partial nonlinearity of the data and are not effective in
handling matrices of which the number of rows is comparable to the number of
columns. On the other hand, there is still a gap between deep learning and ten-
sor decomposition. This paper presents a framework of multi-mode deep matrix
and tensor factorizations to explore and exploit the full nonlinearity of the data in
matrices and tensors. We use the factorization methods to solve matrix and tensor
completion problems and prove that our methods have tighter generalization error
bounds than conventional matrix and tensor factorization methods. The experi-
ments on synthetic data and real datasets showed that the proposed methods have
much higher recovery accuracy than many baselines.
1 Introduction
Low-rank matrices and tensors are pervasive in sciences and engineering. Low-rank matrix com-
Pletion (LRMC)(Srebro & Shraibman, 2005; Candes & Recht, 2009; Recht, 2011; HU et al., 2013;
Hardt, 2014; Chen et al., 2014; Shamir & Shalev-Shwartz, 2014; Sun & Luo, 2016; Fan & Chow,
2017; Fan et al., 2020; KUmmerle & Sigl, 2018; Fan et al., 2019) and low-rank tensor completion
(LRTC) (Gandy et al., 2011; Acar et al., 2011; LiU et al., 2012; Kressner et al., 2014; YUan & Zhang,
2016; Foster & Risteski, 2019), as shown in FigUre 1, aim to recover the missing entries of a low-
rank matrix or tensor. LRMC and LRTC are very UsefUlly in data preprocessing, image and video
inpainting, and collaborative filtering (Fan & Cheng, 2018; LiU et al., 2012). For instance, in collab-
orative filtering (e.g. the recommendation problem of Netflix), the rating matrix given by Users on
items is often highly incomplete becaUse each User can only rate a few items and each item is often
rated by a few Users. If the Unknown entries of the rating matrix are predicted, recommendation can
then be made, accordingly, for Users or items. Since rating matrices often have potentially low-rank
strUctUres, we can Use LRMC or even LRTC to recover the missing entries for recommendation.
Matrix Completion
Tensor Completion
FigUre 1: IntUitive examples of matrix and tensor completion (black sqUare indicates missing valUe).
Notation ThroUghoUt the paper, ‘x’, ‘x’, ‘X’, and ‘X’ denote scalar, colUmn vector, matrix, and
tensor respectively. ‘X(j)’ (or ‘Xj’) denotes a matrix with index j.k ∙ kF denotes the FrobeniUs
norm of matrix or tensor. k」* denotes the nuclear norm of matrix, i.e. sum of the singular values.
k ∙ ∣∣∞ denotes the maximum absolute element in a matrix or tensor. 'C Xj A' denotes the j-
mode product of a tensor C with a matrix A. Pω(X) denotes the projection onto the set Ω, i.e.,
[Pω(X)]ij = [X]ij if (i,j) ∈ Ω and [Pω(X)]j = 0 otherwise. ∣Ω∣ denotes the cardinality of Ω.
g(∙) denotes an activation function and hi denotes the number of units in layer l of a neural network.
1
Published as a conference paper at ICLR 2022
LRMC In the past decade, a lot of algorithms with theoretical guarantee or/and empirical success
have been proposed for LRMC. For example, Candes & Recht (2009) proved that a low-rank matrix
can be recovered exactly with high probability from a few entries sampled uniformly at random,
via nuclear norm minimization. Nie et al. (2012) proposed to minimize the Schatten-p quasi-norm
for LRMC. Nuclear norm and Schatten-p quasi-norm minimizations are based on singular value
decomposition and hence have high computational cost when the size of the matrix is very large. In
addition, they do not incorporate the rank prior that are possibly available in practice, though could
be inaccurate. In contrast, low-rank factorization based methods (Srebro & Shraibman, 2005; Srebro
& Salakhutdinov, 2010; Wen et al., 2012; Hardt, 2014; Sun & Luo, 2016; Jin et al., 2016; Shang
et al., 2016) are scalable to big matrices and can provide high recovery accuracy if the factorization
size is properly determined. In fact, regularized matrix factorizations are closely related to nuclear
norm and Schatten-p quasi-norm minimizations. For example, it is known (Srebro et al., 2005;
Rennie & Srebro, 2005) that ∣∣X∣∣* = minAB=χ IlAkFIlBkF = minAB=χ 2(IlAIlF + IlBIlF).
For LRMC, we can solve
miAmize 2∣∣Pω(Y - AB)IF + |(⑷除 + IBlF),	(1)
where A ∈ Rm×d, B ∈ Rd×n, and 0 < d ≤ min{m, n}. Interestingly, Gunasekar et al. (2017)
found and proved that gradient descent for (1) with λ = 0 and d = m = n converges to the minimum
nuclear norm solution, provided that the learning rate is small enough and the initializations are close
enough to the origin. Fan et al. (2019) proposed a class of rank regularizers called factor group sparse
regularizer (FGSR) as the variational forms for Schatten-p quasi-norms, e.g.,
XmAB q IAI2,q + αIBI2,l =(1 + 1/q)aq/(q+1) IXI^),	⑵
where α > 0 and q ∈ {1,21, ɪ,...}. When q = 1, one gets Schatten-1/2 quasi-norm. Later,
Giampouras et al. (2020) extended the variational form to arbitrary p ∈ (0, 1].
LRTC A few LRMC methods have been directly extended to LRTC (Acar et al., 2011; Liu et al.,
2012; Kressner et al., 2014). One may categorize LRTC algorithms according to the different tensor
factorization models such as CP (CANDECOMP/PARAFAC) decomposition (Kolda & Bader, 2009;
Jain & Oh, 2014), Tucker decomposition (Xu et al., 2013; Xie et al., 2018), tensor singular value
decomposition (Kilmer et al., 2013; Zhang & Aeron, 2016), and tensor ring decomposition (Zhao
et al., 2016; Huang et al., 2020). Take the CP decomposition based LRTC as an example, one can just
learn a CP decomposition from the partial observations (Acar et al., 2011). Yuan & Zhang (2016)
provided the sample complexity for the exact recovery of LRTC via nuclear norm minimization,
though the optimization is intractable because computing the nuclear norm of a tensor is NP-hard
(Hillar & Lim, 2013). Barak & Moitra (2016) and Foster & Risteski (2019) exploited the sum-of-
squares hierarchy for LRTC and provided theoretical guarantees for the recovery. Razin et al. (2021)
studied the implicit regularization in tensor factorization and proved that gradient descent with small
learning rate and near-zero initialization gives rise to a bias towards low tensor rank solutions. Liu
et al. (2019) introduced convolutional neural network to tensor completion. The method can model
the possibly complex interactions inside tensors while preserving the desired low-rank structure.
To complete a third-order (w.l.g) tensor based on the Tucker decomposition, we may solve
minimize 1 ∣∣X — C ×1 A1 ×2 A2 ×3 A3∣∣F, subject to Pω(X) = Pω(Y),	(3)
X,A1,A2,A3,C 2
where we can also consider regularization or constraint on the factors A1, A2, A3, and C. For
instance, Xu et al. (2013) reformulated (3) to all-mode matricization and solved the problem by
alternating minimization. Xie et al. (2018) proposed to minimize the number of nonzero elements
of C and the ranks of the matricizations of X, in addition to (3).
Deep Matrix Factorization Recently, deep matrix factorization (DMF) (Fan & Cheng, 2018;
Arora et al., 2019) gains increasing attention in machine learning. There are two types of DMF
methods: linear DMF (Trigeorgis et al., 2016; Zhao et al., 2017; Arora et al., 2019) and nonlinear
DMF (Xue et al., 2017; Wang et al., 2017; Fan & Cheng, 2018). A general formulation is
X ≈ g1(A1g2(A2 …gL-1 (AL-IAL)…)),	(4)
2
Published as a conference paper at ICLR 2022
where {gi}lL=-11 are activation functions. When all gi are linear, (4) is linear DMF. Otherwise, (4) is
nonlinear DMF. Arora et al. (2019) found that adding depth to a linear matrix factorization can en-
hance an implicit tendency towards low-rank and can provide higher recovery accuracy than depth-2
factorization, though the implicit regularization in linear DMF may not be captured using simple
mathematical norms. In (Fan & Cheng, 2018), the numerical results showed that nonlinear DMF is
able to recover (with high accuracy) the missing entries of a full-rank matrix of which the columns
are generated by a nonlinear low-dimensional latent variable model.
Contributions of this paper This paper first provides theoretical analysis for why and when non-
linear DMF outperforms linear DMF in matrix completion. Second, this paper proposes a new
matrix factorization method called two-mode nonlinear DMF. The method can exploit the full non-
linearity of the data while classical nonlinear DMF uses only partial nonlinearity of the data. Third,
the paper presents a new tensor decomposition method called multi-mode nonlinear deep tensor fac-
torization, which can be regarded as a high-order generalization of the two-mode nonlinear DMF.
Finally, the paper shows that the multi-mode deep matrix and tensor factorization methods have
tighter generalization error bounds than conventional factorization methods in matrix and tensor
completion. Extensive numerical results verified the effectiveness of the proposed methods.
2 Why and when does nonlinear DMF outperform MF?
Nonlinear DMF methods can outperform linear MF methods empirically in many real tasks such
as collaborative filtering (Xue et al., 2017; Fan & Cheng, 2018), image inpainting (Fan & Cheng,
2018), and disease modeling (Wang et al., 2017). The fundamental reason is that the data matrices
in these tasks indeed have some nonlinear structures that cannot be exploited by linear MF methods.
Nevertheless, in nonlinear DMF, the theoretical guarantee is very limited. In contrast, linear MF or
even linear DMF have been well studied and guaranteed with sample complexity (Hardt, 2014; Sun
& Luo, 2016; Jin et al., 2016), generalization bound (Srebro & Shraibman, 2005; Fan et al., 2019),
or convergence (Gunasekar et al., 2017; Arora et al., 2019). In this section, we will provide some
theoretical guarantee for nonlinear DMF in matrix completion.
Assumption 1. Suppose Y = X + E, where Y ∈ Rn1 ×n2. The columns of X are generated by
x = f (z),
where f : Rd → Rn1 denotes a nonlinear mapping, d < min(n1, n2), and z ∈ Rd denotes a latent
variable. The entries of the noise matrix E are drawn from N (0, σ2).
Suppose we observed a few entries of Y randomly (sampling without replacement) and want to
recover X from the incomplete Y . Consider the following method (Fan & Cheng, 2018):
1	λL	λ
minimize -kPΩ(Y - WLg(WL-1 …g(WIZ)…))kF + -- kW IlWIllF + 亨IlZIlF, ⑸
W1 ,...,WL ,Z 2	2	2
where Z ∈ Rd×n2, Wl ∈ Rhl ×hl-1 , l = 1, . . . , L, hL = n1, h0 = d, and h-1 = n2. λw and λz
are regularization parameters. Here we have let all activation functions be the same without loss of
generality and omitted the bias terms of the neural network for simplicity. According to the universal
approximation theorems (Cybenko, 1989; Pinkus, 1999; Lu et al., 2017; Hanin & Sellke, 2017), f
can be well approximated by the neural network provided that L is sufficiently large or at least one
of h1, . . . , hL-1 is sufficiently large. The following theorem provides the excess risk bound for (5).
Theorem 1. Suppose Z and {Wι}L=1 are given by ⑸.Let X = WLg(WL-1 …g(W1Z)…).
L
Suppose IZIF l=1	IWl IF ≤	κ,	max(IY I∞,	IX I∞)	≤	ξ,	the Lipschitz constant of g is η. Let
N = n1n2. Then with probability at least 1 - 2N-1, there exists a numerical constant c such that
√1N kX-X kF
≤ PhkPQ(Y - X)kF + √1N kEkF + Cc
PPL=O hlhl-1 logSLTc-%)
[	∣Ω∣
In Theorem 1, the last term of the RHS of the inequality is equivalent to
L-1
ψι = cξ(∣Ω∣-l(η2d + hL-1 nι + X hlhi-i)log(ηLTξTκ))1/4.	(6)
l=1
3
Published as a conference paper at ICLR 2022
Most major activation functions (e.g. ReLU and sigmoid) are, at worst, 1-Lipschitz with respect to
`2 norm, which indicates that ηL-1 is not large. When n2 is large enough, ψ1 is linear with the
latent dimension d of the columns of X when using (5). In practice, it is difficult to know the true
d in advance. But the Frobenius norm regularizations on Z and W1 are able to reduce the nuclear
norm or even the rank of W1 Z theoretically, which means the d in formulation (5) and Theorem 1
can be larger than the d (ground truth) in Assumption 1.
Note that classical MF (depth-1) is a special case of (5) when L = 1 and g is linear. However,
because of the nonlinearity in the data generating model of Assumption 1, the rank of X can be
much higher than the intrinsic dimension of the data, i.e. r := rank(X) d. For example, if f is a
q-order polynomial function, r can be as large as d+qq . For classical MF, the last term of the RHS
of the inequality in Theorem 1 reduces to
Ψ2 = cξ(∣Ω∣-1(nιr + rn2)log(ξ-lκ0)) 1/4.	(7)
Here κ0 may be less than the κ in (6) but they are in the log operator. Suppose all the Frobenius norms
are the same (the worst case, may not happen because the norms of the factors in a deep factorization
are usually smaller than the norms of the factors in a shallow factorization), denoted by β . Then
κ = βL+1 and κ0 = β2, but r d. In the experiments, we observed that κ ≈ (κ0)1.3 when L = 2
and κ ≈ (κ0)1.7 when L = 3, which are much better than the worst cases. Now comparing (7) with
(6), we conclude that the nonlinear DMF provides a tighter generalization bound than the classical
MF when n2 is much larger than n1 and PlL=-11 hlhl-1 is not too large. The second condition is
realistic because we usually let h1, . . . , hL-1 < n1 or we can just assume h1, . . . , hL-1 < r.
Consider the case that n2 is not large compared to n1, e.g. n2 = n1. In (6), suppose d = r/2,
L = 3, h1 = 2d, h2 = 3d, and η = 1, we have ψ1 = cξ(∣Ω∣-1(2n2r + 2r2)log(κ)) 1/4 and
ψ2 = cξ(∣Ω∣-1(2n2r)log(κ0))1/3 4, Where ψ1 > ψ2. It means the upper bound given by the classical
MF is tighter than that given by the nonlinear DMF. In general, the superiority of the nonlinear
DMF over the classical MF increases when n2 /n1 increases, which is consistent with the numerical
results shown in Figure 5 of the appendix.
3 Two-mode nonlinear deep matrix factorization
Data matrices in many problems are square or nearly square, which restricts the capability of nonlin-
ear DMF. For instance, the difference between the width and height of an image is often small. The
covariance matrices and graph adjacency matrices are square matrices. In recommendation system,
the number of users and the number of items may be of the same order of magnitude. A covariance
matrix or graph adjacency matrix may be constructed naturally by data with nonlinear latent struc-
tures. Similarly, one may imagine that the rating given by a user on an item is an interaction of the
user’s features and item’s features and the features may have some nonlinear low-dimensional latent
structures. The nonlinear DMF in (5) exploits only partial nonlinearity of the data.
Formally, we make the following assumption.
Assumption 2. Suppose Y = X + E, where Y ∈ Rn1 ×n2 and X = U>QV . The columns of
U ∈ Rm1 ×n1 and V ∈ Rm2×n2 are generated respectively by
u = f1 (z) and v = f2(s),
where f1 : Rd1 → Rm1 and f2 : Rd2 → Rm2 are nonlinear mappings, d1 < min(m1, n1),
d2 < min(m2, n2), and z ∈ Rd1 and s ∈ Rd2 denote latent variables. The entries of Q ∈ Rm1×m2
and E ∈ Rn1 ×n2 are drawn from N(0, σQ2 ) and N (0, σE2 ) respectively.
Suppose we have a matrix Y given by Assumption 2, we can factorize it as
Y ≈	UTQV,	Uii	=	fθι (Zi1),	Vi2	= fθ2(Si2),	i1	= 1,...,n1,	i2	=	1,...,n2.	(8)
In (8), fθ1 : Rd1 → Rm1 and fθ2 : Rd2 → Rm2 are multi-layer neural networks with parameter sets
θ1 = {W1(1),. ..,WL(1)} and θ2 = {W1(2),. ..,WL(2)}, where Wi(j) denotes the weight matrix in
the i-th layer of neural network j, and Lj denotes the number of layers of neural network j, j = 1, 2.
4
Published as a conference paper at ICLR 2022
Figure 2: Two-mode nonlinear deep matrix factorization
We call (8) two-mode or two-way nonlinear deep matrix factorization. Figure 2 shows an intuitive
example of (8) with depth-three factorizations in both directions.
Suppose we observed a few entries of Y randomly (sampling without replacement). Based on
Assumption 2, we propose the following matrix completion method
minimize 1∣∣Pω(Y - /% (Z)>Qfθ2(S))kF + R(Z, Q, S,θ1,θ2),
Z,Q,S,θ1,θ2 2
(9)
where
fθι (Z) = g(wL1)g(wL3 …g(W(1Z)…)),
fθ2 (S) = g(wL2)g(wL2-ι …g(Wι(2)S)…)),
and Z ∈ Rd1×n1, S ∈ Rd2×n2, Wl(j) ∈ Rhl(j)×hl(-j)1, l = 1, . . .,Lj, h(Lj) = mj, h(0j ) = dj ,
h(-j1) = nj, j = 1, 2, and Q ∈ Rm1 ×m2. We have let all activation functions be the same with-
out loss of generality. In (9), R(∙) is a regularization operator to reduce overfitting. For instance,
R(Z, Q, S,θ1,θ2) = λ21 (kZ kF + kSkF) + λ (kQkF + Pw ∈θ1∪θ2 k W kF). Compared to (5),
(9) exploits the full nonlinearity of the data. The following theorem provides the excess risk for (9).
Theorem 2. Suppose Z, S, {Wl(1)}lL=11, {Wl(2)}lL=21, and Q are given by (9). Let
X = fθι (Z)>Qfθ2(S). Suppose kQkFkZHfISIf Q；=i QL=ι 口叫⑶口尸 ≤ K,
max(kYk∞, ∣∣Xk∞) ≤ ξ, and the Lipschitz constant of g is η. Let N = n1n2. Then with proba-
bility at least 1 - 2N-1, there exists a numerical constant c such that
11	1
乐 kX - X kF ≤ pΩkPΩ(γ 一 X)kF + √N kE kF
+ cξ
"1m2 + Pj=I p2o h(j)h(j)J log(ηL1+L2ξ-1κ)ʌ /
∣Ω∣
In Theorem 2, the last term of the RHS of the inequality can be written as
2 Lj
ψ3 = cξ(∣Ω∣-1(nι di + njdj + m`m2 + XX h(j)h(j)i)log(nLTξ-1κ))"	(10)
When the hL-1 in (6) is much larger than the d1 in (10), we have ψ3 < ψ1. It means the two-mode
nonlinear DMF (9) provides a tighter generalization bound than classical nonlinear DMF (5).
It is worth mentioning that one may formulate DMF in a two-sided form, e.g., X ≈
g(wL2)g(wL1)ZwR1))wR2)) or X ≈ g(wL1)g(ZLZR)wRi)) equivalently. However, this
model has two shortcomings compared to the one-sided DMF (4) and two-mode DMF (8). First,
(4) and (8) are more compact (in terms of the number of parameters) than the two-sided DMF. For
example, assume the sizes of X, ZL, and ZR are 500×500, 100×10, and 10×100 respectively,
then the number of parameters in the two-sided DMF is larger than 100,000. In the one-sided DMF
(4), let X = g(W2g(W1Z)) and let the sizes of W1, W2, and Z be 500×100, 100×10, and
10×500 respectively, then the number of parameters is 56,000, which is much smaller than that of
the two-sided DMF. The other shortcoming is that the two-sided DMF is difficult to interpret in real
applications. In contrast, (4) and (8) are much easier to interpret and more realistic.
5
Published as a conference paper at ICLR 2022
4 Multi-mode nonlinear deep tensor factorization
In this section, we extend the two-mode deep matrix factorization to multi-mode nonlinear deep
tensor factorization. First, we make the following assumption.
Assumption 3. Suppose Y = X + E, where Y ∈ Rn1×n2 …×nk and X = C ×ι U ×2 U …Xk Uk.
For j = 1, . . . , k, the rows of Uj ∈ Rnj ×mj are generated by u(j) = fj(z(j)), where fj : Rdj →
Rmj is a nonlinear mapping, dj < mj, and z (j) ∈ Rdj are latent variables. The entries of core
tensor C ∈ Rm1×m2 …×mk and noise tensor E ∈ Rn1 ×n2…×nk are drawn from N(0,σC) and
N(0, σE2 ) respectively.
Although {fj}jk=1, C, and {Z(j)}jk=1 are unknown, we can take advantage of neural networks and
tensor decomposition to model the data, i.e.,
Y ≈ C ×1 Ui ×2 U2 …×k Uk, Uj=fθj (ZS)), j = 1,...,k,	(11)
where fθj : Rdj → Rdj ×mj is a neural network with parameter set θj. Figure 3 shows an intuitive
example of the proposed factorization method for a third-order tensor. Different from Tucker de-
composition, we perform nonlinear DMF on the factor matrices and do not constrain the norms of
the columns of the factor matrices. This factorization method can take advantage of the nonlinear
structures of the tensor fully.
Figure 3: Multi-mode nonlinear deep tensor factorization
j =1, 2, 3	g()…渐二二二g()三三三三;
E=∏ p 旦 ≡3
To recover X from a few entries of Y, we propose to solve the following problem
minimize 1 k。。(Y-C ×ιfθι(Z⑴)×2fθ2(Z⑵)…Xkfθ%(Z(k))kF + R({θj, Zj)}k=ι, C),
{θj,Z(j)}jk=1,C 2	1	2	k	F	j=1
(12)
where fθ3 (Z(j)) = g(W^g(W^ ∙ ∙ ∙ g(W「Z(j))…))and θj = {W9,…，WLj)}, j =
1,...,k. R(∙) denotes the regularization on the parameters, e.g.,
k	k Lj
R({θj, Z (j)}k=ι, C) = λ1 χ kZ (j)kF + λ (∣∣CkF + XX kwj kF)	(13)
j=1	j=1 l=1
The following theorem shows the excess risk bound for (12).
Theorem 3. Suppose C and {θj, Z(j)}k=ι are g^ven by (12). Let X = C ×ι fe` (Z(I)) x2
fθ2(Z⑵)…Xk fθk(Z(k)). Suppose ∣∣C∣∣f ≤ βc, ||叱(叫|尸 ≤ β(j), l = 1,...,Lj, ∣∣Z叫尸 ≤
(j)
β0 , j = 1, . . . , k, max(kYk∞, kXk∞) ≤ ξ, and the Lipschitz constant of g is η. Let
N = Qjk=1 nj. Then with probability at least 1 - 2N-1, there exists a numerical constant c
such that
√√N kX - XkF ≤ PhkPQ(Y - X)kF + √√N kE kF
((Qj=I mj+ Pk=i P2oh(j)h(j)i) log
+cξ ι	可
Remark 1. One can obtain slightly tighter bounds in Theorems 1, 2, and 3 when using spectral
norms instead of Frobenius norms. But in (5), (9), and (12), the regularizations only control the
Frobenius norms explicitly. In addition, (12) and Theorem 3 are also applicable to CP-like decom-
position based tensor completion if C is super-diagonal.
6
Published as a conference paper at ICLR 2022
We see that Theorem 2 is a special case of Theorem 3 when k = 2. Denote ψ4 the last term of the
RHS of the inequality in Theorem 3. Since h(0j) = dj and h(-j1) = nj , we have
(k	k	k Lj	k	Lj
∣Ω∣-1(Y mj+ Xnjdj +XXhl(j)hl(-j)1 log ηPjk=1Ljξ-1βCYYβl(j)
j=	j=	j= l=1	j=	l=0	(14)
If g is linear and L = 0, (12) degenerates into the Tucker decomposition based LRTC, e.g. (3).
Accordingly, ψ4 becomes
ψ5 = cξ (网-1 (Y mj + Xnjmj) log(ξ-1 βc ∏ β(j)))	.	(15)
j=1	j=1	j=1
Since mj dj, we have ψ5 > ψ4. It means the nonlinear method (12) has a tighter generalization
error bound than linear tensor completion methods. Therefore, we expect that (12) outperforms
linear LRTC methods when the data have nonlinear structures. For convenience, in this paper, we
call (9) and (12) multi-mode deep matrix and tensor factorizations (M2DMTF).
Optimization The optimization for (9) and (12) are non-trivial because there are high-order ten-
sors and neural networks with unknown inputs and incomplete outputs. We tried alternating mini-
mization, nonlinear conjugate descent, L-BFGS (LiU & Nocedal, 1989), iRprop+ (Igel & Husken,
2000), and Adam (Kingma & Ba, 2014), where all variables are randomly initialized from Gaussian
distribution. It is found that iRProp+ outperforms other algorithms especially on real datasets. The
details of optimization and the analysis of time and space complexity are presented in Appendix A.
5 Numerical results
5.1	Synthetic data
We use the following nonlinear model to generate matrices:
U = W3(1)σ(W2(1) sin(W1(1)Z)), V = W3(2) exp(W2(2)σ(W1(2)S)), X = U>V, (16)
where Z ∈ R2×100, W1(1) ∈ R10×2, W2(1) ∈ R20×10, W3(1) ∈ R30×20, S ∈ R2×100, W1(2) ∈
R10×2, W2(2) ∈ R20×10, and W3(2) ∈ R30×20 are drawn from U (-1, 1). σ denotes the sigmoid
fucntion. The exact rank of X is 20 but the smallest 10 nonzero singular values are close to zero.
Such a synthetic matrix is realistic because the nonlinearity in real data may not be very strong
and the matrices are often low-rank. Similarly, the following nonlinear model is used to generate
third-order tensors:
U1 = σ(W2(1) sin(W1(1)Z(1))),	U2 = exp(W2(2)σ(W1(2)Z(2))),
U3 =cos(W2(3)exp(W1(3)Z(3))),	X =C ×1 U1 ×2 U2 ×3U3,
(17)
where Z(1) ∈ R2×50, W1(1) ∈ R10×2, W2(1) ∈ R20×10, Z(2) ∈ R2×50, W1(2) ∈ R10×2, W2(2) ∈
R20×10, Z(3) ∈ R2×50, W1(3) ∈ R10×2, and W2(3) ∈ R20×10 are drawn from U(-1, 1) and
C ∈ R20×20×20 are drawn from N(0, 1). The exact n-rank of X is (20, 20, 20) but it can be well
approximated by a tensor with n-rank (10, 10, 10) because here the nonlinearity is not very strong.
First, we compare the proposed method with linear matrix factorization (MF (1)), nuclear norm
minimization (NNM) (Candes & Recht, 2009), TNNR (HU et al., 2013), HM-IRLS (KUmmerle &
Sigl, 2018), FGSR (Fan et al., 2019), (stacked) Autoencoders (SAE) (Sedhain et al., 2015), and
DMF (Fan & Cheng, 2018). We use the tanh activation function in SAE, DMF, and M2DMTF. We
determine the hyper parameters of all methods via cross-validation to ensure they perform as well
as possible. Details about the optimizations and parameter settings are in Appendix B. We use the
relative recovery error (Fan etal., 2019) kPΩ (X 一 X) k f /1 ∣Pω (X) 11 f to evaluate the performance
of matrix completion, where Ω consists of the locations of the missing entries. Figure 4(a) shows
the average performance of 20 repeated trials when the missing rate increases from 0.8 to 0.99. MF
7
Published as a conference paper at ICLR 2022
and NNM have much higher recovery error than FGSR. This result is consistent with the results of
(Fan et al., 2019; Arora et al., 2019). Although SAE is a nonlinear method, it still has high recovery
error when the missing rate is less than 0.9. The reason is that filling the missing entries of the input
of the encoder with zero introduces relatively large bias, which may not be eliminated effectively
by the hidden layers. DMF and FGSR have quite similar performance, which is consistent with the
analysis at the end of Section 2. Our M2DMTF outperformed other methods significantly especially
when the missing rate is high. The reason is that M2DMTF is able to exploit the full nonlinearity of
the data and has tighter generalization bound than other methods.
Figure 4(b) shows the performance (average of 20 trials) of FaLRTC (Liu et al., 2012), TenALS
(Jain & Oh, 2014), TMac (Xu et al., 2013), KBR-TC (Xie et al., 2018), TRLRF (Yuan et al., 2019),
CoSTCo (Liu et al., 2019), OITNN-O (Wang et al., 2020), and M2DMTF. Since the rank of the
tensor is quite low, TMac, KBR-TC, and M2DMTF have high recovery accuracy when the missing
rate is less than 0.96. Note that TMac and KBR-TC slightly outperformed M2DMTF when the
missing rate is less than 0.96. The reason is that the optimization of M2DMTF is more difficult.
In Figure 4(b), when the missing rate is higher than 0.97, the proposed M2DMTF outperformed all
methods largely. The optimization curves of M2DMTF are shown in Figure 7 of Appendix B.
Missing rate
0.051
01
0.9	0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98
Missing rate
(a) Matrix completion on data given by (16)	(b) Tensor completion on data given by (17)
Figure 4: Performance evaluation of matrix completion and tensor completion on synthetic data.
5.2	Real matrices
We consider two benchmark datasets: MovieLens-100k and MovieLens-1M. They consist of the
ratings (1 to 5) for movies given by users. The sizes (movies×users) of the rating matrices in
the datasets are 1682 × 943 and 3706 × 6040 respectively. For each matrix (highly incomplete),
we randomly split the known entries to a training set and a test set. The relative recovery errors
(average of 10 repeated trials) on the two datasets are reported in Tables 1 and 2 respectively. FGSR,
SAE, and M2DMTF outperformed MF and DMF in all cases. When the training-test ratio is 9/1
or 7/3, SAE has less recovery error than M2DMTF. This is quite different from the results on the
synthetic data in Section 5.1. Currently, it is unknown why SAE has such a good performance on
the MovieLens data. One possible reason is that the MovieLens data are quite noisy and SAE can
successfully reduce the influence of the noise when the missing rate is relatively low. When the
training-test ratio is smaller, i.e., 5/5, 3/7, or 1/9, M2DMTF outperformed SAE.
Table 1: Relative recovery error of matrix completion on MovieLens-100k
Train/Test	MF	FGSR	SAE	DMF	M2DMTF
9/1	0.2584±0.0019	0.2483±0.0020	0.2412±0.0017	0.2518±0.0020	0.2445±0.0020
7/3	0.2666±0.0011	0.2514±0.0009	0.2456±0.0012	0.2551±0.0011	0.2469±0.0014
5/5	0.2795±0.0010	0.2575±0.0011	0.2527±0.0008	0.2603±0.0014	0.2519±0.0011
3/7	0.3039±0.0009	0.2679±0.0009	0.2656±0.0004	0.2738±0.0007	0.2580±0.0007
1/9	0.3940±0.0025	0.3233±0.0026	0.3028±0.0026	0.3157±0.0018	0.2731±0.0008
8
Published as a conference paper at ICLR 2022
Table 2: Relative recovery error of matrix completion on on MovieLens-1M
Train/Test	MF	FGSR	SAE	DMF	M2DMTF
9/1	0.2278±0.0003	0.2248±0.0002	0.2239±0.0003	0.2266±0.0002	0.2261 ±0.0001
7/3	0.2315±0.0003	0.2271±0.0001	0.2278±0.0002	0.2291 ±0.0002	0.2276±0.0001
5/5	0.2381±0.0003	0.2319±0.0002	0.2346±0.0003	0.2333±0.0002	0.2314±0.0001
3/7	0.2520±0.0003	0.2379±0.0003	0.2458±0.0002	0.2419±0.0003	0.2364±0.0002
1/9	0.3226±0.0008	0.2697±0.0017	0.2796±0.0006	0.2573±0.0006	0.2510±0.0002
5.3	Real tensors
We compare the proposed method M2DMTF with the baselines on the following datasets: Amino
acid fluorescence (Bro, 1997) (5 X 201 X 61), Flow injection (N0rgaard & Ridder, 1994) (12 X
100 × 89), and SW-NIR kinetic data (Bijlsma & Smilde, 2000) (301 × 241 × 8). More details about
the experimental settings are in Appendix B.2. The relative recovery error (average of 10 trials) are
reported in Table 3. The results of FaLRTC and CoSTCo are not reported because they have much
higher recovery error than other methods in most cases and we want to fit the table to the width of
the page. On Amino, our M2DMTF outperformed other methods significantly in all cases. On Flow,
when the missing rate is 0.95 or 0.97, M2DMTF has much lower recovery error than other methods.
On SW-NIR, M2DMTF has low recovery error consistently while most baselines failed totally.
Table 3: Relative recovery error on three real tensor datasets (MR denotes missing rate)
Data	MR	TenALS	TMac	KBR-TC	TRLRF	OITNN-O	M2DMTF
ouτxπγ	^08^	0.0252±o.ooi9	0.0225±o.oo21	0.0318±o.ooii	0.0486±O,O18O	0.0348±o.ooi6	0.0189±o.ooo7
	0.90	0.1317±0.0208	0.0451±o,oo63	0.0358±o,oo22	0.1004±o,O284	0.0684±o,oo96	0.0213±o.ooio
	0.95	0.4727±o,O213	0.3520±o.1632	0.1370±o,oi31	0.3941±o,io29	0.2803±o,O246	0.0354±o.ooo9
	0.97	> 0.95	0.5209±o,O926	0.3192±o,O453	0.5810±o.1136	0.5651±O,O3O9	0.1387±o.o346
	^08^	0.0344±o,ooo4	0.0128±o.oooi	0.0028±o.oooi	0.0471±o,oo79	0.0253±o.ooi6	0.0206±o.oo45
ɪ- 工	0.90	0.0359±o.oo44	0.0132±o,ooo2	0.0041±o.ooo2	0.0619±o,oii7	0.0451±o,oo46	0.0198±o,oo54
	0.95	0.3495±o.1838	0.0716±O,O329	0.0695±o,O322	0.3817±o,O964	0.1369±o,oo88	0.0396±o.oi27
	0.97	0.4141±O,1678	0.3488±o,O4O5	0.3627±O.O5O9	0.5985±o,io81	0.2617±o.oi76	0.0503±O.O185
SW-NIR	^08^	0.2894±o.oi9O	0.5409±o.oi6O	0.6546±o,oi74	0.2425±o.O281	0.1464±o,O3O9	0.0593±o.oii4
	0.90	0.3427±o,oi44	0.8760±o.1298	0.6845±o,o327	0.3607±O.O36O	0.4683±o,o617	0.0601±O.OO78
	0.95	0.4175±o,O246	> 0.95	0.8334±o,o291	0.9146±o,O446	0.8881±o,o231	0.0758±o,oo23
	0.97	> 0.95	0.8823	> 0.95	> 0.95	> 0.95	0.0862±o.oo57
Comparative studies were also conducted on three larger datasets. The results are reported in Ap-
pendix B.2.4. The experiments of tensor completion based recommendation system are in Section
B.2.5. The visualization of the learned tensor and factors of M2DMTF are in Appendix B.2.6.
6 Conclusion
This paper has proposed a framework of multi-mode deep matrix and tensor factorizations1. The
proposed factorization methods can explore and exploit the full nonlinearity of the data effectively.
We applied the factorization methods to matrix and tensor completion and provided the generaliza-
tion error bounds, which verified the superiority of the proposed methods over conventional linear
and nonlinear matrix factorization methods and linear tensor completion methods. The proposed
framework reduced the gap between tensor decomposition and deep learning and showed that their
combinations can provide better performance in tensor recovery. On some datasets, when the miss-
ing rates were low, our methods did not achieve the best performance. The reason is that the non-
linearity in the data is not very strong and the problem is not difficult for the baselines methods
of which the optimizations are easier than our methods. Future work may focus on improving the
optimization efficiency of M2DMTF.
1Codes link: https://github.com/jicongfan/Multi- Mode- Deep- Matrix- and- Tensor- Factorization
9
Published as a conference paper at ICLR 2022
Acknowledgements
The work was supported by the research funding T00120210002 of Shenzhen Research Institute of
Big Data and the Youth program 62106211 of National Natural Science Foundation of China.
References
Evrim Acar, Rasmus Bro, and Bonnie Schmidt. New exploratory clustering tool. Journal of Chemo-
metrics: A Journal ofthe Chemometrics Society, 22(1):91-100, 2008.
Evrim Acar, Daniel M Dunlavy, Tamara G Kolda, and Morten M0rup. Scalable tensor factorizations
for incomplete data. Chemometrics and Intelligent Laboratory Systems, 106(1):41-56, 2011.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix fac-
torization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019.
Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In
Conference on Learning Theory, pp. 417-445, 2016.
Sabina Bijlsma and Age K Smilde. Estimating reaction rate constants from a two-step reaction: a
comparison between two-way and three-way methods. Journal of Chemometrics, 14(5-6):541-
560, 2000.
Rasmus Bro. Parafac. tutorial and applications. Chemometrics and intelligent laboratory systems,
38(2):149-171, 1997.
Rasmus Bro. Exploratory study of sugar production using fluorescence spectroscopy and multi-
way analysis. Chemometrics and Intelligent Laboratory Systems, 46(2):133-147, 1999. ISSN
0169-7439. doi: https://doi.org/10.1016/S0169-7439(98)00181-6.
Emmanuel J. Candes and Benjamin Recht. Exact matrix completion via convex optimization.
Foundations of Computational Mathematics, 9(6):717-772, 2009. ISSN 1615-3383. doi:
10.1007/s10208-009-9045-5.
Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, and Rachel Ward. Coherent matrix comple-
tion. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp.
674-682. JMLR Workshop and Conference Proceedings, 2014.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Jicong Fan and Jieyu Cheng. Matrix completion by deep matrix factorization. Neural Networks, 98:
34-41, 2018.
Jicong Fan and Tommy W.S. Chow. Matrix completion by least-square, low-rank, and sparse self-
representations. Pattern Recognition, 71:290 - 305, 2017. ISSN 0031-3203.
Jicong Fan, Lijun Ding, Yudong Chen, and Madeleine Udell. Factor group-sparse regularization for
efficient low-rank matrix recovery. In Advances in Neural Information Processing Systems 32,
pp. 5104-5114. Curran Associates, Inc., 2019.
Jicong Fan, Mingbo Zhao, and Tommy W.S. Chow. Matrix completion via sparse factorization
solved by accelerated proximal alternating linearized minimization. IEEE Transactions on Big
Data, 6(1):119-130, 2020. doi: 10.1109/TBDATA.2018.2871476.
Dylan J Foster and Andrej Risteski. Sum-of-squares meets square loss: Fast rates for agnostic tensor
completion. In Conference on Learning Theory, pp. 1280-1318. PMLR, 2019.
Silvia Gandy, Benjamin Recht, and Isao Yamada. Tensor completion and low-n-rank tensor recovery
via convex optimization. Inverse Problems, 27(2):025010, 2011.
10
Published as a conference paper at ICLR 2022
Paris Giampouras, Rene Vidal, Athanasios Rontogiannis, and Benjamin Haeffele. A novel varia-
tional form of the schatten-p quasi-norm. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21453-
21463. Curran Associates, Inc., 2020.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017.
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th
Annual Symposium on Foundations of Computer Science (FOCS),, pp. 651-660. IEEE, 2014.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM
(JACM), 60(6):1-39, 2013.
Yao Hu, Debing Zhang, Jieping Ye, Xuelong Li, and Xiaofei He. Fast and accurate matrix comple-
tion via truncated nuclear norm regularization. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 35(9):2117-2130, 2013. ISSN 0162-8828. doi: http://doi.ieeecomputersociety.
org/10.1109/TPAMI.2012.271.
Huyan Huang, Yipeng Liu, Jiani Liu, and Ce Zhu. Provable tensor ring completion. Signal Process-
ing, 171:107486, 2020.
Christian Igel and Michael Husken. Improving the rprop learning algorithm. In Proceedings of
the second international ICSC symposium on neural computation (NC 2000), volume 2000, pp.
115-121. Citeseer, 2000.
Prateek Jain and Sewoong Oh. Provable tensor factorization with missing data. In Advances in
Neural Information Processing Systems, pp. 1431-1439, 2014.
Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable efficient online matrix completion via
non-convex stochastic gradient descent. In Proceedings of the 30th International Conference on
Neural Information Processing Systems, pp. 4527-4535, 2016.
Misha E Kilmer, Karen Braman, Ning Hao, and Randy C Hoover. Third-order tensors as operators
on matrices: A theoretical and computational framework with applications in imaging. SIAM
Journal on Matrix Analysis and Applications, 34(1):148-172, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Daniel Kressner, Michael Steinlechner, and Bart Vandereycken. Low-rank tensor completion by
riemannian optimization. BIT Numerical Mathematics, 54(2):447-468, 2014.
Christian Kummerle and Juliane SigL Harmonic mean iteratively reweighted least squares for low-
rank matrix recovery. The Journal of Machine Learning Research, 19(1):1815-1863, 2018.
Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical Programming, 45(1):503-528, 1989. ISSN 1436-4646. doi: 10.1007/BF01589116.
Hanpeng Liu, Yaguang Li, Michael Tsang, and Yan Liu. Costco: A neural tensor completion model
for sparse tensors. In Proceedings of the 25th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining, pp. 324-334, 2019.
Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor completion for estimating
missing values in visual data. IEEE transactions on pattern analysis and machine intelligence,
35(1):208-220, 2012.
11
Published as a conference paper at ICLR 2022
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: a view from the width. In Proceedings of the 31st International Conference on
Neural Information Processing Systems,pp. 6232-6240, 2017.
Feiping Nie, Heng Huang, and Chris Ding. Low-rank matrix recovery via efficient Schatten p-norm
minimization. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,
AAAI’12, pp. 655-661. AAAI Press, 2012.
Lars N0rgaard and Carsten Ridder. Rank annihilation factor analysis applied to flow injection anal-
ysis with photodiode-array detection. Chemometrics and Intelligent Laboratory Systems, 23(1):
107-114, 1994.
Allan Pinkus. Approximation theory of the mlp model. Acta Numerica 1999: Volume 8, 8:143-195,
1999.
Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in tensor factorization. In
International Conference on Machine Learning, pp. 8913-8924. PMLR, 2021.
Benjamin Recht. A simpler approach to matrix completion. J. Mach. Learn. Res., 12:3413-3430,
December 2011. ISSN 1532-4435.
Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd international conference on Machine learning, pp. 713-
719. ACM, 2005.
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders
meet collaborative filtering. In Proceedings of the 24th international conference on World Wide
Web, pp. 111-112, 2015.
Robert J Serfling. Probability inequalities for the sum in sampling without replacement. The Annals
of Statistics, pp. 39-48, 1974.
Ohad Shamir and Shai Shalev-Shwartz. Matrix completion with the trace norm: learning, bounding,
and transducing. The Journal of Machine Learning Research, 15(1):3401-3423, 2014.
Fanhua Shang, Yuanyuan Liu, and James Cheng. Tractable and scalable Schatten quasi-norm ap-
proximations for rank minimization. In Artificial Intelligence and Statistics, pp. 620-629, 2016.
Nathan Srebro and Ruslan R Salakhutdinov. Collaborative filtering in a non-uniform world: Learn-
ing with the weighted trace norm. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S.
Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 2056-
2064. Curran Associates, Inc., 2010.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference
on Computational Learning Theory, pp. 545-560. Springer, 2005.
Nathan Srebro, Jason Rennie, and Tommi S. Jaakkola. Maximum-margin matrix factorization. In
Advances in neural information processing systems, pp. 1329-1336, 2005.
Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. IEEE
Transactions on Information Theory, 62(11):6535-6579, 2016.
George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, and Bjorn W Schuller. A deep
matrix factorization method for learning attribute representations. IEEE transactions on pattern
analysis and machine intelligence, 39(3):417-429, 2016.
Andong Wang, Chao Li, Zhong Jin, and Qibin Zhao. Robust tensor decomposition via orientation
invariant tubal nuclear norms. In AAAI, pp. 6102-6109, 2020.
Qi Wang, Mengying Sun, Liang Zhan, Paul Thompson, Shuiwang Ji, and Jiayu Zhou. Multi-
modality disease modeling via collective deep matrix factorization. In Proceedings of the 23rd
ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1155-
1164, 2017.
12
Published as a conference paper at ICLR 2022
Zaiwen Wen, Wotao Yin, and Yin Zhang. Solving a low-rank factorization model for matrix com-
pletion by a nonlinear successive over-relaxation algorithm. Mathematical Programming Com-
Putation,4(4):333-361,2012. ISSN 1867-2957. doi: 10.1007∕s12532-012-0044-1.
JP Wold, Rasmus Bro, A Veberg, F Lundby, AN Nilsen, and J Moan. Active photosensitizers
in butter detected by fluorescence spectroscopy and multivariate curve resolution. Journal of
agricultural and food chemistry, 54(26):10197-10204, 2006.
Q. Xie, Q. Zhao, D. Meng, and Z. Xu. Kronecker-basis-representation based tensor sparsity and its
applications to tensor recovery. IEEE Transactions on Pattern Analysis and Machine Intelligence,
40(8):1888-1902, 2018.
Yangyang Xu, Ruru Hao, Wotao Yin, and Zhixun Su. Parallel matrix factorization for low-rank
tensor completion. arXiv PrePrint arXiv:1312.1254, 2013.
Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. Deep matrix fac-
torization models for recommender systems. In IJCAI, volume 17, pp. 3203-3209. Melbourne,
Australia, 2017.
Longhao Yuan, Chao Li, Danilo Mandic, Jianting Cao, and Qibin Zhao. Tensor ring decomposi-
tion with rank minimization on latent space: An efficient approach for tensor completion. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 9151-9158, 2019.
Ming Yuan and Cun-Hui Zhang. On tensor completion via nuclear norm minimization. Foundations
of ComPutational Mathematics, 16(4):1031-1068, 2016.
Zemin Zhang and Shuchin Aeron. Exact tensor completion using t-svd. IEEE Transactions on
Signal Processing, 65(6):1511-1526, 2016.
Handong Zhao, Zhengming Ding, and Yun Fu. Multi-view clustering via deep matrix factorization.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.
Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring decom-
position. arXiv PrePrint arXiv:1606.05535, 2016.
A Optimization details and computational complexity
In our M2DMTF, the matrix model is a special case of the tensor model. Therefore, we only show
the optimization detail and the computational complexity of the tensor case. The gradient-based
optimization is shown in Algorithm 1. We suggest using iRprop+ as the optimizer in the algorithm
because we found empirically that iRprop+ is more stable and efficient than other optimizers such
as Adam in solving our M2DMTF.
Algorithm 1 Gradient-based optimization for M2DMTF (12)
Input: Y ∈ Rn1×…×nk, Ω, λι, λ2, g, network structures {mj,Lj, {h(力}3}：=1, T
1: Initialization: Co, {ZOj),{W"}3，6#)}：5 {UOj)= f«j)(ZOj))}：1.
2: for t = 1, 2, . . . , T do
3:	for j = 1, 2, . . . , k do
{Vθj), VZj)} ÷ba- Lj, where Lj = 2 忤⑸(Y(j)- U(j)[Ct](j)匕>)||：
R({θ(j), Z (j)}k=1, Ct) and Vt = UyT)㊈…Uf)㊈ U(k) …㊈ Ut(j+1).
4:	end for
+
5
6
7
VC 一 ∂Lk∕∂C
{{θ" Z(j)}k=ι, Ct} <— gradient-based optimizer ({▽&(?), Vz(j)}：=「VC)
end for
Output: Y = CT ×1 UT* I) 2 3 4 ×2 uT2 ∙∙∙×k UTk)
13
Published as a conference paper at ICLR 2022
Without loss of generality, We here assume that in (12), nι = •…n = n, mi = •…mk = m,
Li = •…=Lk = L, and h(1) = •…=h(k) = hi, l = 0,1,...,L. The time cost in each iteration
of the gradient-based optimization is
O k min{nmk + nk mk-i, nkm + nk-imk} + kn X hi-ihi ,
in Which the first term is from the Tucker-like decomposition While the second term is from the
forWard and backWard propagations of the neural netWorks. Since the second term is much smaller
than the first one and m < n, the time complexity per iteration is about O knkm + knk-i mk .
As We need to store the observed elements of Y, core tensor C, latent factors Z(j), netWork pa-
rameter, their gradients, and the values of hidden units, the space complexity is O(∣Ω∣ + mk +
k PL=I hi-ihi + kn PL=I hi), which is further simplified to O(∣Ω∣ + mk + kn P3 hi) because
n is larger than all hi. NoW We see that the time (per iteration in the optimization) and space com-
plexity of our M2DMTF are slightly higher than the Tucker decomposition based tensor completion
methods. However, M2DMTF requires much more iterations than the baseline methods because it
is composed of tensor decomposition and neural networks.
B	More ab out the experiments
B.1 Matrix completion
In the proposed method (9), the regularization is
2 Lj
R(Z, Q, S ,θ1,θ2) = λ2r (kz kF + kskF) + λ22 (kQkF + XX kWi(j) kF )∙
j=i i=i
We let λ2 = λ2√n1n2 and determine λι and λ2 instead. In the optimization, S, Z, and {Wi(j)}
are initialized randomly, Q is initialized to an identity matrix.
B.1.1	Parameter setting on the synthetic data
•	In MF (problem (1) in the main paper), the factorization dimension d is 5 because it out-
performs other choices. The λ is chosen from {0.01, 0.1, 1} and the optimizer is iRprop+.
The maximum iteration is 2000.
•	In FGSR (Fan et al., 2019), we use the variational form of Schatten-1/2 norm, set d = 20,
and choose λ from {0.01, 0.1}. The maximum iteration is 2000.
•	In SAE (Sedhain et al., 2015), the network structure is [100, 20, 10, 5, 10, 20, 100]. The
weight decay parameter is chosen from {0.1, 0.5}.
•	In DMF (Fan & Cheng, 2018), the network structure is [3, 10, 20, 100]. The weight decay
parameters are chosen from {0.01, 0.1}.
•	InM2DMTF, L =2, di = d2 = 3, h(ii) = h(i2) = 10, mi = m2 =20,andλi = λ02 = 1.
In SAE, DMF, and M2DMTF, the activation function is the hyperbolic tangent function. The opti-
mizer is iRprop+ and the maximum iteration is 3000.
B.1.2	More results on the synthetic data
Figure 5 compares DMF with linear MF (FGSR) in the cases of different n2/ni. We see that when
n2/ni is too small, FGSR outperforms DMF. When n2/ni increases, the superiority of DMF over
FGSR becomes more significant. These empirical results are consistent with our theoretical analysis
in Section 2 of the main paper.
Figure 6 shows the performance of M2 DMTF with different number of hidden layers and different
activation function. it can be found that tanh and sigmoid functions lead to lower recovery error than
ReLU in all cases. The main reason is that the data were generated by smooth functions that can be
14
Published as a conference paper at ICLR 2022
. Missing rate=0.9
ʌ Missing rate=0.93
Figure 5: The influence of n2/n1 on FGSR and DMF
」OL① A」① >00 8」ω>lBo(r
Figure 6: The performance of M2DMTF with different number of hidden layers and different acti-
vation function. The sizes of added hidden layers are 10.
O Tanh
O Sigmoid
ReLU
well approximated by tanh and sigmoid functions. When the tanh function or sigmoid function is
used, increasing the depth of the neural networks can reduce the recovery error. But when L is too
large, i.e. L = 4, the recovery error increased slightly, which might be caused by overfitting.
Figure 7 shows the values of the objective functions and relative recovery errors against the iterations
in matrix completion and tensor completion when the missing rate is 0.95. The recovery error
decreases slightly when the number of iterations is very large. In general, the optimization efficiency
is not very high because M2DMTF is composed of multiple neural networks with unknown inputs
and incomplete outputs.
10.5
10OOOI-------------1------------1--------- I	I	二
-----Objective function
9000	∣ ReiatiVe recoveIy error ∣
8000
0.5
0.45
0.4
UO⅛ua①>loaqo
0.45
I-----Objective function
----ReIatiVe recovery error ∣
(a) Iterative performance of M2DMTF in matrix com- (b) Iterative performance of M2DMTF in tensor com-
pletion on data given by (16)	pletion on data given by (17)
Figure 7: Iterative performance of M2DMTF (iRprop+) on synthetic data with missing rate 0.95.
15
Published as a conference paper at ICLR 2022
We replace the sin, cos, exp, and σ in (16) and (17) with tanh such that the nonlinearity in the
data generating models are consistent with the activation functions used in our matrix and tensor
completion method M2DMTF. The average results of 10 trials are reported in Figure 8. We see that
M2DMTF outperformed other methods significantly when the missing rate is high enough.
(a) Matrix completion on data given by modified (16)
Figure 8: Performance evaluation of matrix completion and tensor completion on synthetic data
generated by (16) and (17) in which the nonlinear functions replaced by tanh.
Missing rate
(b) Tensor completion on data given by modified (17)
B.1.3	Parameter setting on the MovieLens datasets
•	In MF (problem (1) in the main paper), we set d = 10 and λ = 10. In stead of iRprop+, we
use alternating minimization because it outperformed iRprop+ in this case. The maximum
iteration is 2000.
•	In FGSR Fan et al. (2019), we use the variational form of Schatten-1/2 norm, set d =
100, and set λ = 0.015 or 0.007 (for MovieLens-100k or MovieLens-1M). The maximum
iteration is 2000.
•	In SAE Sedhain et al. (2015), we use a single hidden layer of size 500. The weight decay
parameter is 0.5.
•	In DMF, Fan & Cheng (2018), the network structure is [10 or 20, 50, 100, 1682]. The
weight decay parameters are chosen from {0.05, 0.1}.
•	In M2DMTF, , we set L = 2, d = 10 or 20, h(11) = h(12) = 50, m1 = m2 = 100, and
λ1 = λ02 = 1.
B.1.4	More results on the MovieLens datasets
The RMSE (average of 10 trials) on the MovieLens-100k are reported in Table 4. The proposed
M2DMTF outperformed MF, FGSR and DMF in all cases and outperformed SAE when the missing
rate was high. Note that RMSE is a monotonic transformation of the relative recovery error used in
the main paper.
Table 4: RMSE on MovieLens-100k
Train-Test	MF	FGSR	SAE	DMF	M2DMTF
9-1	0.9565±0.0071	0.9194±0.0073	0.8928±0.0063	0.9320±0.0070	0.9048±0.0071
7-3	0.9880±0.0037	0.9318±0.0028	0.9105±0.0036	0.9454±0.0038	0.9159±0.0045
5-5	1.0356±0.0038	0.9541±0.0038	0.9364±0.0022	0.9692±0.0048	0.9341±0.0040
3-7	1.1259±0.0032	0.9924±0.0037	0.9840±0.0016	1.0155±0.0025	0.9563±0.0025
1-9	1.4597±0.0093	1.1978±0.0094	1.1217±0.0097	1.1697±0.0064	1.0119±0.0025
16
Published as a conference paper at ICLR 2022
B.2 Tensor completion
In the proposed method (12), the regularization is
k	k Lj
R({θj, Z (j)}k=ι, C) = λl X kZ (j)kF + λ QckF + XX kWι(j) kF).	(18)
j=1	j=1 l=1
We let λ2 = λ02QQjk=1 nj) and determine λ1 and λ02 instead. In the optimization, Uj is initial-
ized by the singular value decomposition of the j-mode matricization of Y, C is initialized to zero,
and {Z(j)}, {Wl(j)} are initialized randomly.
B.2.	1 Parameter setting on the synthetic data
•	In FaLRTC (LiU et al., 2012), We set αι = α2 = α3 = 3 choose μ from {0.5,1,2, 5,10}.
•	In TenALS (Jain & Oh, 2014), the initial rank is 5 or 10 and the max iteration is 200.
•	In TMac (XU et al., 2013), the rank is initialized to [10, 10, 10] or [5, 5, 5] and adjUsted
adaptively. The maximUm iteration is 500.
•	In KBR-TC (Xie et al., 2018), ρ = 1.01 andλ = 0.01. The maximUm iteration is 300.
•	TRLRF (YUan et al., 2019), the rank is set to [5, 5, 5] or [10, 10, 10]. We choose λ from
{0.01, 0.1, 1, 10, 50}.
•	In CoSTCo (LiU et al., 2019), the learning rate is chosen from {0.01, 0.001, 0.001}, the
rank is chosen from {2, 3, 5, 10, 20}, the epoch nUmber is 50 or 200, and the batch size is
256 × 2 or 256 × 5.
•	In OITNN-O (Wang et al., 2020), We choose α from {0.0001, 0.001, 0.01, 0.1}. The max
iteration is 500.
•	In M2DMTF, L = 2, d1 = d2 = d3 = 3, h(11) = h(12) = h(13) = 10, m1 = m2 = m3 = 20,
and λ1 = λ02 = 1. The optimizer is iRprop+ and the maximUm iteration is 3000.
B.2.2	Time cost comparison on the synthetic data
We report the average time costs (second) and the recovery errors of all methods in Table 5, Which
also inclUdes the performance of M2DMTF With different iteration nUmbers. We see that oUr method
With 3000 iterations is sloWer than other methods. HoWever, When We Use less iterations (e.g. 1000
or 200), the time cost of oUr method is comparable With or even loWer than the baselines and its
recovery error is still mUch loWer than the baselines.
Table 5: Average time cost (second) on the synthetic tensor data (CoSTCo Was performed in Python
While all other methods Were performed in MATLAB.)
	FaLRTC	TenALS	TMac	KBR-TC	TRLRF	CoSTCo	OITNN-O
recovery error	0.944	0.973	0.323	0.169	0.621	0.617	-0.328-
time cost	3.5s	2.1s	1.4s	4.7s	4.3s	72s	6.8s
	M2DMTF						
iteration	100	200	300	500	1000	2000	3000
recovery error	0.098	0.068	0.057	0.043	0.037	0.033	0.031
time cost	1.1s	2.0s	3.1s	4.9s	9.9s	20.6s	32.3s
B.2.3	Experimental settings on real tensors
We normalize each tensor by X J X/∣∣X∣∣∞. In M2DMTF, We set L = 1 and λι = λ2 = 0.001.
For i = 1, 2, 3, we set m^ ≈ ni, d% ≈ ni if n > 10 and set m^ ≈ 0.8%, d% ≈ 0.7% if n ≤ 10.
The parameters of other methods are carefUlly determined to provide their best performance.
17
Published as a conference paper at ICLR 2022
B.2.4	More results on tensor datasets
In addition to the three datasets considered in the main paper, we test the proposed method on three
more datasets: Porphyrin (Wold et al., 2006) 170 × 274 × 35, Sugar (Bro, 1999) 256 × 571 × 7,
and Bonnie (Acar et al., 2008) 89 × 97 × 549. To reduce the computational costs of all methods on
Bonnie, we only consider a subset of size 89 × 97 × 100. The results are reported in Table 6. We
see that M2DMTF outperforms other methods when the missing rate is high enough.
Table 6: Relative recovery error on more tensor datasets (MR denotes missing rate)
Data	MR	TenALS	TMac	KBR-TC	TRLRF	OITNN-O	M2DMTF
Porphyrin	^08^	0.0931±0.0013	0.0105±0.0021	0.0304±0.0011	0.0319±0.0180	0.0118±0.0016	0.0184±0.0007
	0.90	0.0968±0.0383	0.0281±0.0023	0.0330±0.0017	0.0451±0.0053	0.0225±0.0023	0.0211±0.0018
	0.95	0.1091±0.0133	0.0493±0.0169	0.0458±0.0030	0.0701±0.0035	0.0522±0.0036	0.0376±0.0005
	0.97	0.1199±0.0108	0.1128±0.0138	0.0569±0.0154	0.1606±0.0178	0.1066±0.0052	0.0450±0.0009
	^08^	0.1438±0.0549	0.0456±0.0041	0.0406±0.0027	0.1075±0.0045	0.0719±0.0046	0.0504±0.0114
E 8 n S	0.90	0.1584±0.0019	0.0639±0.0034	0.0563±0.0039	0.1214±0.0076	0.0915±0.0059	0.0568±0.0038
	0.95	0.1749±0.0208	0.1173±0.0265	0.1090±0.0048	0.2083±0.0196	0.1573±0.0045	0.0921±0.0043
	0.97	> 0.95	0.3129±0.0801	0.2791±0.0306	0.6515±0.0364	0.2443±0.0218	0.1689±0.0316
Bonnie	"-0.90-	0.0538±0.0032	0.0084±0.0005	0.0049±0.0001	0.0407±0.0004	0.0128±0.0007	0.0110±0.0002
	0.95	0.0598±0.0126	0.0209±0.0016	0.0204±0.0022	0.0633±0.0045	0.0385±0.0014	0.0167±0.0013
	0.97	0.3561±0.0649	0.1451±0.0386	0.1005±0.0211	0.1187±0.0167	0.0785±0.0018	0.0328±0.0042
	0.99	> 0.95	0.7354±0.0657	> 0.95	> 0.95	0.3835±0.0032	0.1174±0.0127
B.2.5	Tensor-completion based recommendation as a downstream task
We split the MovieLens-100k dataset by time to form a tensor of size 943 × 1682 × 31, where
31 denotes the number of weeks as a dimension of time. Since the baselines KBR-TC, TRLRF,
OITNN-O, and the proposed M2DMTF have high time cost on large tensors, we reduce the size of
the tensor by removing the users giving 49 or less ratings and the movies rated by 49 or less users.
Then we get a tensor of size 563 × 593 × 31. The relative recovery error (average of 5 trials) are
reported in Table 7. Our M2DMTF outperformed other methods. In addition, comparing Table 7
with Table 1, we found that tensor completion methods are more effective than matrix completion
methods in recommendation.
Table 7: Relative recovery errors of tensor completion methods on MovieLens-100k (MR denotes
missing rate)
MR	FaLRTC	TenALS	TMac	KBR-TC	TRLRF	CoSTCo	OITNN-O	M2DMTF
0.5	0.2627	0.2813	0.3065	0.2528	0.2634	0.2696	0.2830	0.2429-
0.7	0.2692	0.3097	0.3470	0.2685	0.2719	0.2724	0.2951	0.2483
0.9	0.2881	0.3356	0.3782	0.2739	0.2804	0.2763	0.3207	0.2614
B.2.6	Visualization of factors learned by M2DMTF
Here We take the 'Flow injection, tensor (N0rgaard & Ridder, 1994) as an example. The data is
from measuring samples with three different analytes on a flow injection analysis (FIA) system
where a pH-gradient is imposed. The three analytes in twelve measured samples are 2-, 3-, and
4-hydroxy-benzaldehyde. They have different absorption spectra depending on whether they are in
their acidic or basic form. The data set is a 12 (samples) × 100 (wavelengths) × 89 (times) array.
Figure 9 presents the original data, incomplete data, and recovered data for the fifth horizontal slice
(sample-5) of the ‘Flow’ tensor, where the missing rate is 0.97. We see that FaLRTC, TRLRF, and
CoSTCo failed totally. TenALS, TMac, and KBR-TC have high recovery error visually. In contrast,
the data recovered by our M2DMTF is very close to the original data. More importantly, the result
of M2DMTF indicates that there are three analytes, while other methods failed to reveal the fact.
We analyze the characteristics of the low-dimensional factors learned by our M2DMTF. Figure 10
presents the values of U2 ∈ R100×20 and Z(2) ∈ R100×10 as well as the correlation coefficients
18
Published as a conference paper at ICLR 2022
KBR-TC
CoSTCo
Figure 9: Completion result visualization of the fifth horizontal slice of the ‘Flow’ tensor (missing
rate=0.97).
20 40 60 80
20 40 60 80
Figure 10: Latent factors U2 and Z(2) (corresponding to the wavelengths dimension of ‘Flow’
tensor data) given by the proposed method M2DMTF. Column 1: values of U2 and Z(2). Column
2: correlation coefficients between the rows of U2 and Z(2) . Column 3: correlation coefficients
between the columns of U2 and Z(2) . Column 4: mutual information between the columns of U2
and Z(2).
20 40 60 80 100
Corrcoef(Z)
and mutual information. In the first column, we see that the values of U2 are more chaotic than the
values Z(2). The reason is that the 20 variables in U2 are correlated mutually. In the second column
of Figure 10, we see that the autocorrelation of the samples (rows) of U2 is similar to that of Z(2) ,
because U2 relies on Z(2) . The third column of the figure indicates that the correlation between
the variables in U2 are higher than that in Z(2) . The last column compares the mutual information
that can quantify nonlinear relationship. Now comparing column 4 with column 3, we conclude
that there exist significant nonlinear relationship between the variables in U2 and it is necessary to
represent U2 by a lower-dimensional latent factor Z (2) via deep factorization. These results verified
that the nonlinear latent factor assumption of our M2DMTF is practical and useful.
C Proof of theorems
First of all, we provide the following lemma.
Lemma 1. Let S be a set defined over tensors of size n1 X n2 × ∙ ∙ ∙ × nk. Let |S| be the e-
covering number ofS with respect to the Frobenius norm. Let N = Qik=1 ni. Suppose X ∈ S and
max{kY k∞, kX k∞} ≤ ξ. Then with probability at least 1 - 2N-1, there exists a constant c such
19
Published as a conference paper at ICLR 2022
that
X∈pS √1NkY - XkF - PhkPQ(Y - X)kF
≤ 上 + (8ξ4 log(∣S∣N)y4
^ P∣Ω∣	k	∣Ω∣	)
The proof of the lemma is in Appendix D.1.
C.1 Proof for Theorem 1
The following lemma (proved in Appendix 2) shows an upper bound for the covering number of the
deep factorization matrix set.
Lemma2. Let S = {X ∈ Rm×n : X = WLg(WL-ι ∙∙∙ g(W1Z)),∣∣Z IIF ≤ β°,∣∣ Wi IIF ≤
βl,1 ≤ l ≤ L}, where Z ∈ Rh0×n, Wl ∈ Rhl×hl-1, l = 1, . . .,L, hL = m, h0 = d, and
h-1 = n. Suppose the Lipschitz constant of g is η. Then the covering numbers of S with respect to
the Frobenius norm satisfy
N(S,k∙kF O ≤ ( QUE Ie 厂0 S
Now using Lemma 1 and Lemma 2, we have
√1N kX - X kF = √1N kY - E - X kF
≤√1N kY - X kF + √1N kEkF
1	1	2
≤ PWkPQ(Y - X)kF + √N kE kF + pW +
1	1	2
≤ PWkPQ(Y - X)kF + √N kE kF + pW +
(8ξ4 log(∣S∣N) y4
I ∣Ω∣	)
(8ξ4 (log N + PL=o hihi-1 log 3(L + 1)ηLTQ3 βl)、
Ω	'
∖ /
≤ PwkPQ(Y - X)kF+ √N kE kF+ 6ξp+τ +
8ξ4 (log N + PL=O hi hi-i log(ηLTξ-1 QL=o βι ))∖ 1/4
∣Ω∣
≤ pwk%(Y- X)kF+√N kE kF+Cc (PL=O hlhl-1 log∣(ΩL-1ξ-1Q3βl)
In the fourth inequality, we have set = 3ξ(L + 1). The last inequality holds because N is much
smaller than ∣S ∣ and c can be a constant large enough. This finished the proof.
C.2 Proof for Theorem 2
The following lemma (proved in Appendix D.3) provides an upper bound for the covering number
of the two-mode deep factorization matrix set.
Lemma 3. Let	S = {X ∈ Rn1×n2	: X	= fθ1(Z)>Qfθ2(S),	kQkF	≤ βQ, kZkF	≤
βO(1), kSkF ≤	βO(2), kWi(j)kF ≤	βi(j), 1	≤ l ≤ Lj,j =	1,2; fθ1(Z)	=
g(wL1)g(wL1-1 …g(W1(1)Z))),fθ2(S) = g(wL2)g(wL2-1 …g(W1(2)S))),}, where Z ∈
Rd1×n1, S ∈ Rd2×n2, Wi(j) ∈ Rhl(j)×hl(-j)1, l = 1, . . . ,Lj, h(Lj) = mj, h(Oj) = dj, h(-j1) = nj,
j = 1, 2, and Q ∈ Rm1 ×m2. Suppose the Lipschitz constant of g is η. Then the covering numbers
of S with respect to the Frobenius norm satisfy
N (S, k ∙ kF, e) ≤ ( 3(k +1)(max(L1,L2)+ 1)s )p ,
where p = m1m2 + Pj2=1 PiL=jO hi(j)hi(-j)1 and s = βQηL1 +L2(QL=O β(1))(QL=0 β(2)).
20
Published as a conference paper at ICLR 2022
Similar to the proof of Theorem 1, using Lemma 1, we have
√1N kX-X kF
≤ PhkPQ(Y - X)kF + √1N kEkF + p⅛ +
(8ξ4 log(∣S∣N) Y"
I ∣ω∣	)
where N = n1n2. Let = 3ξ(k + 1)(max(L1, L2) +1). According to Lemma 3 and we obtain
2 Lj	L1	L2
log(ISI) = (m1m2 + XX h(j)h(-)i) log (βQηLl+L2ξ-1( Y β(I))(Y β(2)))∙
j=1 l=0	l=0	l=0
Now we have
√1NkX - XkF ≤PhkPQ(Y - X)kF + √1NkEkF
((E E	_LL2	LLj	Z1(j) Z1(j)、]cn∙ L L gL1+L2e-l/ΓTL1 zς ⑴ΓTL2	/O(2)λλ ∖ 1/4
(m1m2	+ Tj=I	Tl=O	hl hl-1) log (βQη 1+ 2ξ (IIl=O βl 乂	lll=0	βl )) ∖
Ω	)
This finished the proof.
C.3 Proof for Theorem 3
The following lemma (proved in Appendix D.4) provides an upper bound for the covering number
of the multi-mode deep factorization tensor set.
Lemma 4. Let S = {X ∈ Rnι×n2…×nk ： X = C ×1 fθ1 (Z(I)) ×2 fθ2(Z⑵)∙∙∙ ×k
fθk (Z(k))； fθj (Z)	= g(WL)g(WLj)-I …g(Wι(j)Z(j)))), j =	1,...,k; kCkF ≤
βC,kZ(j)kF ≤ βO(j), kWl(j)kF ≤ βl(j),1	≤	l ≤	Lj,j = 1,...,k},	where Z(j) ∈	Rdj×nj,
Wl(j) ∈ Rhl(j) ×hl(-j)1, l = 1, . . . , Lj, h(Lj)	=	mj,	h(Oj) = dj, h(-j1) =	nj, j = 1, . .	. , k, and
C ∈ Rm1×m2…×mk. Suppose the LiPschitz constant of g is η. Then the covering numbers of S
with respect to the Frobenius norm satisfy
N (S, k∙kF, e) ≤ ( 3(k + 1)(Lmax + 1)S )p,
where p = Qjk=1 mj + Pjk=1 PlL=jO hl(j)hl(-j)1 ands = βCηPjk=1 Lj Qjk=1 QlL=jO βl(j).
The proof is similar to that for Theorem 2. Using Lemma 1 and Lemma 4 , we arrive at
√√N kX - XkF ≤ PhkPQ(Y - X)kF + √1N kE kF
((Qk=I mj+ Pk=I P2O h(j)h(j)ι)log (βc ηPk=1 Lj ξ-1
+ Cc [ '----------------------------"--------------------
where c is a constant.
1/4
D Proof for lemmas
D.1 Proof for Lemma 1
We give the following lemma.
21
Published as a conference paper at ICLR 2022
Lemma 5 (Hoeffding inequality for sampling without replacement (Serfling, 1974)). Let
X1, X2,..., Xs be a SetOfSamPIeS taken without replacementfrom a distribution (xι, x2,... ,xn}
of mean u and variance σ2. Denote a = min^ Xi and b
P
1 s
I - X Xi- UI ≥ t ≤ 2exp
i=1	_
—
)=maxi Xi. Then
2st2
(1 - (S - 1)/N)(b - a)2
We define
L(X)：=焉∣Pω(Y - X)kF,
∣ω∣
L(X) := NN kY - XkF,
where N = ∏3 n Suppose max{kYk∞, kXk∞} ≤ ξ. According to Lemma 5, we have
P hL(X) - L(X)∣ ≥ t] ≤ 2exp (-(1 - (∣Ω∣∣ - 1)/N)δ2),
where δ = 4ξ2. Using union bound for all X ∈ S yields
P sup IL(X) -Z(X)∣≥ t
X ∈S
≤2∣s∣ exp (-(1 - (∣Ω∣ω∣1)∕n 州2).
Or equivalently, with probability at least 1 - 2N-1,
..—. .—..
sup ∣Z(X)-L(X)I ≤
X ∈S




δ2 log(∣S∣N)
2
ɪ - 1 + ɪ
∣Ω∣ N + N ∣Ω∣
∕δ2 log(∣S∣N) δ
≤V	2∣Ω∣	= Α
Since ∣√u - √v∣ ≤ ,∣u - v∣ holds for any non-negative U and v, We have
sup I qzχ1 - √≡m I ≤√δ.
Recall that € ≥ ∣∣X - X∣∣f ≥ ∣∣P(X - X)Ilf, we have
I √z(x) - qzmI
=√N IkY - XkF -kY- XkF i≤√n
and
I qLX - q(X ∣
llpΩ(Y - X 州F - kPΩ(Y - X)kF∣ ≤ √=Ω∣.
It follows that
sup
X ∈S
≤ sup
X ∈S
€
IqZ^ - √Z(X) I
I qzx - qzχ)+∣ qzx - √z(X)∣+∣ qzχ - √zx
≤ PW
+√δ+ √n
≤工 + (δ2 log(∣S∣N) Y/4
^√∣Ω∣	I 2∣Ω∣	)
This finished the proof.
22
Published as a conference paper at ICLR 2022
D.2 Proof for Lemma 2
Let X = WLg(WL-1 ∙ ∙ ∙ g(W1Z)), where Z ∈ Rd×n, Wi ∈ Rhl×hl-1, l = 1,...,L, and
h0 = d. Let Sab := {W ∈ Rα×b : ∣∣ W∣∣f ≤ β}. Then there exists an e-net Sab obeying
N(SabJ ∙kF,e) ≤
such that IlW - W∣∣f ≤ e. Now replace e with e∕γ and let ∣∣Wi - WIkF ≤ t, l = 1,... ,L,
γι
IIz - ZIIf ≤ —.LetYi = (L + 1)ηL-IQi=iβ, l = o,...,L.
∣X - X kF
= ∣Wl9(Wl-i …W2g(W1Z)) - WLg(Wl-i ∙∙∙ W2g(W1Z))∣f
=k Wl9(Wl-i …W2g(W1Z)) ± WLg(WL-I …W2g(W1Z))
± WLg(WL-1 ∙∙∙ W2g(W1Z)) ∙∙∙± WLg(WL-1 ∙∙∙ W2g(W1Z))
± WLg(WL-1 ∙∙∙ W2g(W1Z)) - WLg(Wl-1 …g(W1Z))∣f
≤∣Wl9(Wl-1 ∙∙∙ W2g(W1Z)) - WLg(WL-1 ∙∙∙ W2g(W1Z))∣f
+ k WLg(WL-1 ∙∙∙ W2 g(W1Z)) - WLg(Wl-1 ∙∙∙ W2g(W1Z ))∣f
+ ∙∙∙ + k WLg(WL-1∙∙∙ W2g(W1Z)) - WLg(WL-1∙∙∙ W2g(W1Z ))∣f
+ k WLg(Wl-1 ∙∙∙ W2 g(W1Z)) - WLg(Wl-1 ∙∙∙ W2g(W1Z))∣f
L-1
≤ηLTkWL - WLkFkZkF ∏ IWikF
国	(19)
L-2
+ ηL-1kWL-1 - WL-IkF kZ kF kWLkF ∏ ||用||尸
i=1
L-1
+ …+ ηL-1kW1 - W1kFkZkF ∏ kWlkF
i=2
L
+ ηL-1kZ - Z kF ∏ kW∣∣F
i = 1
L-1
≤ηL-1 工 ∏ βi + nL-1 — ∏ βi + ∙∙∙ + nL-1 - ∏ βi + nL-1 - ∏ βi
YL 昌	YL-T-I	Y1 量	γo ⅛
The second inequality utilized the Lipschitz continuity of g and the submultiplicativity of the Frobe-
nius norm. Therefore, S is an e-cover of S. We have
N(S,k∙kF ,e)
≤ ∏ (3β∏iAhihi-1
i=0 ∖ e J
=∏ (3(L + 1)ηL-1Q3 βi Jj
=(3(L + 1)ηL-1Q3 βi「。hihi-1.
D.3 PROOF FOR LEMMA 3
Lemma 3 is a special case of Lemma 4.
23
Published as a conference paper at ICLR 2022
D.4 Proof for Lemma 4
Let ρ0 = kC kF . For j = 1, . . . , k, we have
Lj	Lj
kUjkF ≤ηLjkZ(j)kFYkWl(j)kF = ηLj Y βl(j) ,ρj.
l=1	l=0
(20)
Denote T = "：=0 Pj. Similar values can be defined for C and Uj.
Let Sjl= {Wι(j) ∈ Ra×b : k Wι(j)kF ≤ β(j)}. Then there exists an e-net Sjl obeying
N(Sjl,k∙kF,e) ≤
such that k Wl(j) - W(j)∣∣F ≤ e. Let Γ(j) = (Lj + 1)ηLj QQi=l βi, l = 0,..., Lj, and Yj =
(k + 1) ∏t=j Pt, j =0,1,...,k. Replace e with jF, We have IIWlej)- W(j)∣∣F ≤ jF and
Γl Υj	Γl Υj
have
hl(j)hl(-j)1
N (Sjl,k∙kF,
Υj βlej)
It follows that
IUj- UjkF
=kg(wLj)g(wLj)-ι …w2(j)g(Wι(j)z(j))))- g(WLj)g(WLj)-I …Wej)g(Wej)z (j))))∣∣F
=kg(wLj)g(wLj-ι …W(j)g(Wej)Z(j)))) ± g(W jg(WLj3 …Wy)g(W(j)Z(j))))
± g(WLj)g(WLj-ι …W2j)g(W*z(j))))…士 g(WLj)g(WLj-ι …Wp)g(Wp')z(j))))
± g(WLg(WL-1 …W2g(W1 Z)) - g(WLj)g(Wj-1 …g(W(j)Z(j))))kF
≤kg(WLjg(WLj-ι …Wy)g(W*Z(j)))) - g(WLj)g(WLj∙-ι …W2j)g(W*Z(j))))kF
+ kg(WLJ)g(WLj-ι …W^g(Wp)z(j)))) - g(WLj)g(WLj)-I …W2j)g(Wj Zj )))∣f
+ …+ kg(WLjj) g(WLj)-I …W2(j)g(Wi(j)z(j)))) - g(WLj)g(WLj)-I …W(j)gWIjZj )))∣f
+ kg(WLj) g(WLj)-I …W2(j)g(WIj)Z(j)))) - g(WLj)g(WLj)-I …W2j)g(W(j) Z (j))))kF
Lj-1
≤ηLjkWL)- WLj)If∣Z(j)kF Y kWl(j)kF
l=1
Lj-2
+ ηLjkWLj,-ι - WLj)-IkFkZ(j)kF∣WLj)If Y kWl(j)∣∣F
l=1
L-1
+ …+ ηLjkWej)- W(j)kF kZ(j) kF Y kWl(j)kF
l=2
Lj
+ ηLjkZ(j) - Z(j)kF YkWl(j)kF
l=1
≤η
Lj e
ΓL')Y.
Lj-1
Y βl(j) +ηLj
j l=0
Y	βj)+ …+ ηLj EjY Y βlj)+ ηLj a〒 Y βj)
Lj-1	Γ1 Υj l6=1	Γ0 Υj l6=0
γj.
(21)
24
Published as a conference paper at ICLR 2022
Now we have
IIX - X IIf
= IiC	×1	UI	×2	∙∙∙ ×k	Uk- C ×1 UI ×2 ∙∙∙ ×k	Uk IlF
=IC	×1	U1	×2	∙∙∙×k	Uk	± C ×1 U1 ×2 ∙∙∙×k	Uk ± C	×1 U1	×2 ∙∙∙ ×k-1	Uk-1	×k	U k
± ∙ ∙ ∙ ± C	×1	U1 ×2	∙ ∙ ∙	×k-1 Uk-1 ×k Uk -	C ×1 U1 ×2 ∙ ∙	∙ ×k Uk∣∣F
≤∣∣C	×1	U1	×2	∙∙∙ ×k	(Uk - Uk )If + IlC ×1 U1 ×2 ∙∙∙	×k-1	(Uk-1 - Uk-I)	×k	U k	If
+ ∙ ∙ ∙ + ||C ×1 (UI- UI) ×2 U2 ∙ ∙ ∙ ×k Uk ∣∣F + Il(C - C) ×1 UI ×2 U2 ∙ ∙ ∙ ×k U k ∣∣F
≤∣C∣F∣U1∣F …∣Uk-1∣F∣Uk - UkIf + ∣C∣F 115” …∣Uι - U一||尸∣Uk∖∣F
+ …+ ∣C∣F∣U1 - U1∣FIUIIf ∙∙∙ ∣UkIIF + IC - C∣∣F∣U1∣F∣∣U2∣∣F ∙∙∙ ∣Uk∣F
≤∣C∣FIU1If ∙∙∙ ∣∣Uk-1∣∣FY^ + ICIf⑼"∙∙∙ R电||尸
+ ∙∙∙ + ICIf ɪ IU2If ∙∙∙ IUk If + / IUIf IU⅛ ∙∙∙ IUk If
Y 1	Y 0
≤e(Yk∏ρj	+ ∕ ∏	ρj	+ ∙∙∙ +	Y;∏ρj	+	Yo∏Pj)
∖ kj=k	k-1j=k-1	1 j = 1	0 j=0	)
=€
Therefore S is an E-COver of S. Then the covering number of Sdnk can be bounded as
N(Sdnk , I∙If ,€)
≤ (S )∏k=1 mj ∏ ∏ (
j	J	j=11 = 0 ∖
=(a )∏k=1 mj ∏ ∏ (
∖	/	j=11 = 0 ∖
3Γ(j)γjβ(j) ∖ "C
€
3(Lj + 1)ρjYj ∖j3
E
+ Isnk=I mj
< (3(k + 1)τ)∏k=1 mj
k	Lj
∏∏
j=1l=0
∏ (3
j=1 '
+ 1)(LmaX + 1)τ )PL=0 hjhj1
+ 1)(Lj + 1)τ V(j)h(-)1
€
+ 1)τ ∖∏k=ι mj/3(k + 1)(LmaX + 1)τ ʌpk=1 P之0 hjhj1
€
≤ ( 3(k + I)(LmaX +
€
∏k=ι m +Pk=I Pa hjhj
3(k + 1)(LmaX + 1)P0 H" SLjQM βj ∖∏k=1 H& hjhj1
€
25