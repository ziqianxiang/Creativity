Published as a conference paper at ICLR 2022
Adversarial Unlearning of Backdoors via Im-
plicit Hypergradient
Yi Zeng *1, Si Chen1, Won Park2, Z. Morley Mao2, Ming Jin1 and RUoxi Jia1
1 Virginia Tech, Blacksburg, VA 24061, USA
2University of Michigan, Ann Arbor, MI 48109, USA
Ab stract
We propose a minimax formUlation for removing backdoors from a given poi-
soned model based on a small set of clean data. This formUlation encompasses
mUch of prior work on backdoor removal. We propose the Implicit Backdoor
Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Unlike previoUs
work, which breaks down the minimax into separate inner and oUter problems, oUr
algorithm Utilizes the implicit hypergradient to accoUnt for the interdependence
between inner and oUter optimization. We theoretically analyze its convergence
and the generalizability of the robUstness gained by solving minimax on clean
data to Unseen test data. In oUr evalUation, we compare I-BAU with six state-of-
art backdoor defenses on eleven backdoor attacks over two datasets and varioUs
attack settings, inclUding the common setting where the attacker targets one class
as well as important bUt Underexplored settings where mUltiple classes are tar-
geted. I-BAU’s performance is comparable to and most often significantly better
than the best baseline. ParticUlarly, its performance is more robUst to the variation
on triggers, attack settings, poison ratio, and clean data size. Moreover, I-BAU
reqUires less compUtation to take effect; particUlarly, it is more than 13× faster
than the most efficient baseline in the single-target attack setting. FUrthermore, it
can remain effective in the extreme case where the defender can only access 100
clean samples—a setting where all the baselines fail to prodUce acceptable resUlts.
1	Introduction
In backdoor attacks, adversaries aim to embed predefined triggers into a model dUring training time
sUch that a test example, when patched with the trigger, is misclassified into a target class. For
instance, it has been shown that one coUld Use a sticker as the trigger to mislead a road sign classifier
to identify STOP signs to speed limit signs (GU et al., 2017). SUch attacks pose a great challenge to
deploy machine learning in mission-critical applications (Li et al., 2020c; 2021a).
VarioUs approaches have been proposed to remove the effect of backdoor attacks from a poisoned
model. One popUlar class of approaches (Wang et al., 2019; Chen et al., 2019; GUo et al., 2019)
is to first synthesize the trigger patterns from the model and then Unlearn the triggers. However,
these approaches presUme that backdoor triggers only target a small portion of classes, thereby
becoming ineffective when many classes are targeted by the attacker. Moreover, they sUffer from
high compUtational costs, as they either reqUire synthesizing the trigger for each class independently
or training additional models to synthesize the triggers all at once. Another line of works does not
rely on trigger synthesis; instead, it directly mitigates the triggers’ effects via, for example, fine-
tUning and prUning the model parameters (LiU et al., 2018a) and preprocessing the model inpUt (QiU
et al., 2021). While these approaches are more efficient than the trigger-synthesis-based approaches,
they cannot maintain a good balance between robUstness and model accUracy dUe to the Unawareness
of potential triggers. Recent work (Li et al., 2020b) proposes to leverage a teacher model to gUide
fine-tUning. However, oUr empirical stUdies find that the effectiveness of this approach is particUlarly
sensitive to the Underlying attack and data aUgmentation techniqUes Utilized to enrich the fine-tUning
dataset. Overall, it remains a challenge to design a defense that achieves a good balance between
model accUracy, robUstness, and compUtational efficiency.
To address the challenge, we propose a minimax formUlation to remove backdoor triggers from
a poisoned model. The formUlation encompasses prior works on trigger synthesis-based defense,
which solves the inner and oUter problems independently. Moreover, the formUlation does not make
* Correspondence to yizeng@vt.edu. Codes of implementations is OPensoUrCed on Github: I-BAU
1
Published as a conference paper at ICLR 2022
any assumption about the backdoor trigger beyond a bounded norm constraint, thus remaining ef-
fective across various attack settings. To solve the minimax, we propose an Implicit Bacdoor Ad-
versarial Unlearning (I-BAU) algorithm based on implicit hypergradients. Unlike previous work,
which breaks down the minimax into separate inner and outer optimization problems, our algorithm
derives the implicit hypergradient to account for the interdependence between inner and outer op-
timization and uses it to update the poisoned model. We theoretically analyze the convergence of
the proposed algorithm. Moreover, we investigate the generalization error of the minimax formula-
tion, i.e., to what extent the robustness acquired by solving the minimax generalizes to unseen data
in the test time. We present the generalization bounds for both linear models and neural networks.
We conduct a thorough empirical evaluation of I-BAU by comparing it with six state-of-art backdoor
defenses on seven backdoor attacks over two datasets and various attack settings, including the com-
mon setting where the attacker targets one class and an important but underexplored setting where
multiple classes are targeted. I-BAU’s performance is comparable to and most often significantly
better than the best baseline. Particularly, its performance is more robust to the variation on triggers,
attack settings, poison ratio, and clean data size. Moreover, I-BAU requires less computation to take
effect; particularly, it is more than 13× faster than the most efficient baseline in the single-target
attack setting. It can still remain effective in the extreme case where the defender can only access
100 clean samples—a setting where all the baselines fail to produce acceptable results.
2	Related Work
Backdoor Attacks. Backdoor attacks evolve through three stages. 1) Visble triggers using unrelated
patterns (Gu et al., 2017; Chen et al., 2017), or optimized triggers (Liu et al., 2018b; Bagdasaryan
& Shmatikov, 2021; Zhao et al., 2020; Garg et al., 2020) for higher attack efficacy. While this line’s
attack methods achieve high attack success rates and clean accuracy, the triggers are visible and
thus easily detected by human eyes. 2) Visually invisible triggers via solving bilevil optimizations
regarding the lp norm (Li et al., 2020a), or adopting nature optical effects (Liu et al., 2020; Nguyen
& Tran, 2021), and Clean label attacks via feature embeeding (Turner et al., 2019; Saha et al., 2020)
for better stealthiness. For most work from this line, the triggers or the poisons can evade the detec-
tion from manual efforts. 3) Invisible triggers considering other latent spaces, e.g., the frequency
domian (Zeng et al., 2021; Hammoud & Ghanem, 2021) or auto-encoder feature embedding (Li
et al., 2021b), which improved the stealthiness by breaking the fundamental assumptions of a list
of defenses. This paper incorporates successful attacks from these major groups and we show our
method provides an effective, and generalizable defense.
Backdoor Defenses. Backdoor defenses can be divided into four categories: 1) Poison detection via
outlier detection regarding functionalities or artifacts (Gao et al., 2019; Chen et al., 2018; Tran et al.,
2018; Koh & Liang, 2017; Chou et al., 2020; Zeng et al., 2021), which rely on the modeling of clean
samples’ distribution. 2) Poisoned model identification identifies if a given model is backdoored
or not (Xu et al., 2019; Wang et al., 2020). 3) Robust training via differential privacy (Du et al.,
2019; Weber et al., 2020) or ensembled/decoupled pipeline (Levine & Feizi, 2020; Jia et al., 2020;
2021; Huang et al., 2022). This line of work tries to achieve general robustness to withstand outlier’s
impact, but may suffer from low clean accuracy. 4) Backdoor removal via trigger synthesising (Wang
et al., 2019; Chen et al., 2019; Guo et al., 2019), or preprocessing & finetuning (Li et al., 2020b;
Borgnia et al., 2020; Qiu et al., 2021). This line of work serves as the fundamental solution given
a poisoned model, but there is still no satisfying solution to attaining robust results across different
datasets and triggers. In this work, we propose a minimax formulation and a solver for backdoor
removal. We attempt to include all cutting-edge methods under this category for comparison.
3	Problem Formulation
Attack model. We assume that an adversary carries out a backdoor attack against a clean training
set generated from the distribution D. The adversary has pre-determined triggers and target classes.
The adversarial goal is to poison the training dataset such that, given a clean test sample x, adding a
backdoor pattern δ to X (i.e., X + δ) will alter the trained classifier output to be a target class y. The
norm of δ represents the adversary,s manipulative power. We assume that ∣∣δ∣∣ ≤ Cδ. In general,
the attack can add r backdoored examples into the training set to induce the association between
the trigger δ and the target class y. The ratio of the total backdoored examples over the size of the
training set is defined as the poison ratio. For better stealthiness, attacks often aim not to affect the
prediction of a test example when the trigger is absent. Note that in our attack model, we explicitly
acknowledge the possibility that multiple trigger patterns and multiple target classes are present. We
will refer to the model trained on the poisoned training set as a poisoned model.
2
Published as a conference paper at ICLR 2022
Defense goal. We consdier that the defender is given a poisoned classifier fθpoi and an extra set of
clean data D = {(χi,yi)}n=ι from D. With fθp0i and D, the defender aims to build a model fθ*
immunne to backdoors, i.e., fθ* (x + δ) = fθ* (x). Note that the size of available clean examples is
assumed to be much smaller than the size necessary to retrain a high-accuracy model from scratch.
Minimax formulation for defense. To achieve the defense goal, we want the resulting classifier
to maintain the correct label even if the attacker patches the backdoor trigger to a given input. This
intuition naturally leads to the following minimax optimization formulation of backdoor removal:
1n
θ* = arg min max H(δ,θ):= —	L(fθ (Xi + δ),yi),	(1)
θ	kδk≤Cδ	n
i=1
where L is the loss function. Note that this formulation looks similar to adversarial training for
evasion attacks (a.k.a. adversarial examples) (Madry et al., 2017). The main distinction is that in
evasion attacks, the adversarial perturbation is specific to an individual data point, while in backdoor
attacks, the same perturbation (i.e., the backdoor trigger) is expected to cause misclassifications for
any examples upon being patched. Hence, in the formulation for backdoor attacks, the same trigger,
δ, is applied to every clean sample xi , whereas, in the minimax formulation for evasion attacks, the
perturbation is optimized independently for each sample (see Eq. (2.1), Madry et al. (2017)).
The minimax formulation for backdoor removal has many advantages. 1), it gives us a unifying
perspective that encompasses much prior work on backdoor removal. The formulation comprises
an inner maximization problem and an outer minimization problem. Both of these problems have
a natural interpretation in the security context. The inner maximization problem aims to find a
trigger that causes a high loss for predicting the correct label. This aligns with the problem of
a backdoor attack that misleads the model to predict a predefined incorrect target label. In our
formulation, we choose to maximize the prediction loss for the correct label instead of using the
exact backdoor attack objective of minimizing the prediction loss associated with the target label.
We make this design choice because, in reality, the defender does not know the target labels selected
by the attacker. The outer minimization problem is to find model parameters so that the “adversarial
loss” given by the inner attack problem is minimized. Existing backdoor removal approaches based
on trigger synthesis and unlearning the synthesized trigger (Wang et al., 2019; Chen et al., 2019;
Guo et al., 2019) can be considered as a special instance of this minimax formulation, where they
neglect the interdependence between the inner and outer optimization and solve them separately. 2),
the formulation does not make any assumption on the backdoor trigger beyond the bounded norm
constraint. By contrast, much of the existing work makes additional assumptions about backdoor
triggers in order to be effective. For instance, synthesis-based approaches often assume that the
classes coinciding with the target classes of the triggers account for only a tiny portion of the total,
making those approaches ineffective in countering multiple target cases. 3), the formulation provides
a quantitative measure of backdoor robustness. In particular, when the parameters θ yield a (nearly)
vanishing loss, the corresponding model is perfectly robust to attacks specified by our attack model
on D. We will discuss the generalization properties of this formulation in Section 5. Using the
results developed there, we can further reason about the robustness on the data distribution D.
4	Algorithm
A natural (but problematic) algorithm design. Given the empirical success of adversarial train-
ing for defending against evasion attacks (Li et al., 2022), one may wonder whether we can solve
the minimax for the backdoor attack via adversarial training. Adversarial training keeps alternating
between two steps until the loss converges: (1) solving the inner maximization fora fixed outer mini-
mization variable; and (2) solving the outer minimization by taking a gradient of the loss evaluated at
the maximum. Practically, adversarial training proceeds by first generating adversarial perturbations
and then updating the model using the gradient calculated from the perturbed data. For backdoor at-
tacks, the adversarial perturbation needs to fool the model universally on all inputs. Hence, a natural
algorithm to solve the backdoor minimax problem is to tune the model with universal adversarial
perturbations. Universal adversarial perturbations have been studied in the past, and there exists an
off-the-shelf technique to create such perturbations (Moosavi-Dezfooli et al., 2017). However, we
found that this simple algorithm is highly unstable. Figure 1 illustrates the change of the model ac-
curacy and robustness (measured in terms of the attack success rate (ASR)) for 20 runs as adversarial
training proceeds. It can be seen that the ASR varies significantly across different runs. Within a
single run, while the ASR decays as a whole, the decrement is mostly erratic and slow.
3
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
There are two issues with naive adversarial training with universal perturbations. First, the perfor-
mance of the existing universal perturbation algorithm is unstable. At its core, it generates adver-
sarial perturbations for each example and adds the perturbations together. The addition may not
always lead to a perturbation that can fool all examples; instead, the addition operation may cancel
the effect of individual perturbations. More importantly, the decoupling between solving the inner
maximization and the outer minimization in adversarial training is often justified by Danskin’s Theo-
rem (Danskin, 2012), which states that the gradient of the inner function involving the maximization
term is simply given by the gradient of the function evaluated at this maximum. In other words,
letting δ* = argmax∣∣δ∣∣≤Cδ H(δ,θ), wehave Vθmax∣∣δ∣∣≤Cδ H(δ,θ) = VθH(δ*,θ). However,
in practice, due to stochastic gradient descent, it is impossible to solve the inner maximization op-
timally. Worse yet, the underperformance of the existing universal perturbation algorithm makes it
even harder to approach the maximum. Moreover, Danskin’s Theorem only holds for convex loss
functions. Due to the violation of maximum condition and convexity, it is unsuitable for utilizing
Danskin’s Theorem, and hence, adversarial training is not justified.
Algorithm 1: Implicit Backdoor Adversarial Unlearning (I-BAU)
Input: θ1 (poisoned model); D (clean set accessible for backdoor unlearning);
Output: θK (sanitized model)
Parameters: Cδ (`2 norm bound); K, T (outer and inner iteration numbers); α, β > 0 (step sizes)
for each iteration i ∈ (1, K - 1) do
δi — 01×d;
for t ∈ (1, T ) do
Update δit+1 = δt + α^ιH(δ*θi),θi);
L δi = δi X min (1，kCfc)；
Compute Vδ> (Hessian products) via an iterative solver with reverse mode differentiation;
Vψ(θi) = V2H (δi,θi) + Vδ>Vι H (δi,θi);
_ Update θi+ι = θi — βVψ(θi);
return θK
Proposed algorithm. We propose an algorithm to solve the minimax problem in (1) that does
not require the loss function to be convex and is more robust to the approximation error caused
by not being able to solve the inner maximization problem to global or even local optimality. Let
δ(θ) be an arbitrary suboptimal solution to max∣∣δ∣∣≤Cδ H(δ,θ). Let ψ(θ) := H(δ(θ),θ) be the
objective function evaluated at the suboptimal point; if δ(θ) is a stationary point, we make an dis-
tinction by defining the corresponding function as ψ*(θ). Denote V∖H(δ, θ) and V2H(δ, θ) as the
partial derivatives with respect to the first and second variable, respectively, and V21H(δ, θ) and
V21,2H(δ, θ) as the second-order derivatives of H with respect to the first variable and the mixed
variables, respectively. The gradient of ψ with respect to θ is given by
response Jacobian directgrad.of δ
z------------------λ------{
Vψ(θ)	=V2H(δ(θ),θ)+ (Vδ(θ))> V1H (δ(θ), θ).	(2)
1―{z-}	|	、{/ J 1	{z	}
hypergrad. of θ direct grad. of θ	indirect grad. of θ
The direct gradients are easy to compute. For instance, suppose H is the loss of a neural network,
then V1H(δ(θ), θ) and V2H(δ(θ), θ) are the gradients with respect to the network input and the
model parameters, respectively. However, the response Jacobian is intractable to obtain because we
must compute the change rate of the suboptimal solution to the inner maximization problem with
respect to θ. When δ(θ) satsifies the first-order stationarity condition, i.e., V1H(δ(θ), θ) = 0, and
assuming that V12H(δ(θ), θ) is invertible, the response Jacobian is given by
Vδ(θ) = -WIH (δ(θ),θ))-1 V2,2H (δ(θ),θ),	(3)
which follows from the implicit function theorem. Note that Vδ(θ) plays a role in the Hessian in (2),
in the sense that it adjusts each dimension of the gradient V1H(δ(θ), θ) to the importance of that di-
mension. Hessian expresses the importance via curvature, whereas Vδ(θ) measures the importance
based on the sensitivity to the change of θ. A widely known fact from the optimization literature
is that second-order optimization algorithms are tolerant to the inaccuracy of Hessian (Byrd et al.,
2011). Based on this intuition, we propose to approximate Vδ(θ) with a suboptimal solution by
(3). In practice, the approximation is addressed with an iterative solver of limited rounds (e.g., con-
jugated gradient algorithm (Rajeswaran et al., 2019) or fixed-point algorithm (Grazzi et al., 2020))
along with the reverse mode of automatic differentiation (Griewank & Walther, 2008).
4
Published as a conference paper at ICLR 2022
τr-<∙ ii	i /1 ∖	i ∙ , 1	∙	ɪ 1 ∙	∙ . ∕c∖ El	χ-7 T / n∖ /
Finally, to solve (1),we plug m the approximate response Jacobian into (2). Then, We use Vψ(θ) (re-
ferred to as implicit hypergradient hereinafter) to perform gradient descent and update the poisoned
model. Note that the implicit hypergradient only depends on the value of δ(θ) instead of its opti-
mization path; hence, it can be implemented in a memory-efficient manner compared to the explicit
way of solving bi-level optimizations (Grazzi et al., 2020). The overall algorithm, dubbed Implicit
Backdoor Adversarial Unlearning, for `2 -norm bounded attacks is presented in Algorithm 1. Gener-
alizing the algorithm to other norms is straightforward. We found that imposing a large norm bound
in I-BAU has no discernible effect on clean accuracy (Appendix A.3.1). This empirical observation
enables the proposed I-BAU to be flexible enough to deal with different scales of backdoor triggers
in practice. Further details on the the iterative solver with the suboptimum solution and memory
complexity analysis are provided in Appendix A.3.
5 Theoretical Analysis
In this section, we analyze the convergence of the I-BAU. We also study the generalization properties
of the minimax backdoor removal formulation. Assuming that by solving the minimax, we obtain a
model such that all points in the clean set D are robust to triggers ofa specific norm, we would like to
reason about to what extent an unseen point from the underlying data distribution D is trigger-robust.
Convergence Bound: Suppose that H(∙,θ) is μH(θ)-strongly convex and
smooth, where μH(∙) and LH(∙) are continuously differentiable. Define α(δ)
LH (θ)-Lipschitz
=LH (θ)+μH (θ),
and let δ(θ) be the unique fixed point of δ + α(θ)V1H (δ, θ). Then, there exists LH,θ s.t.
supkδk≤2Cδ kV1H(δ, θ)k ≤ LH,θ (Grazzi et al., 2020). We make the following assumptions:
•	Lipschitz continuity ofdirect gradients: ViH(∙, θ) and V2H(∙, θ) are Lipschitz continuous func-
tions of θ (with Lipschitz constants η1,θ and η2,θ, repectively).
•	Lipschitz continuity and norm boundedness of second-order terms: V2H (∙, θ) is ρι,θ- Lipschitz
continuous, and V2,iH(∙, θ) is p2,θ-Lipschitz continuous. Also, the cross derivative term is norm
bounded by Lh,θ ≥ V221H(δ(θ), θ).
•	Asymptotic convergence of δt(θ) to δ(θ): kδt(θ) - δ(θ)k ≤ ρθ (t) kδ(θ)k, where ρθ (t) is such
that ρθ(t) ≤ 1, and ρθ(t) → 0 as t → +∞.
Theorem 1 Consider the approximate hypergradient Vψ(θi) computed by Algorithm 1 (line 7). For
every inner and outer iterate t, i ∈ N, we have:
]	Vψ(θi) - Vψ(θi)∣∣ ≤ 卜 i(θi) + C2(θi)1-qi) Pθi ⑴ + C3 (θi)qθi,	(4)
where qθi := max {1 — α(θi)μH(θi),α(θi)LaR) — 1}, ei(θi):=(小—+ ηι,θ* iLhqiα(θi)) Cδ,
c2(θi):= LH,θi Cδ (LH (θi)kVα(θi)k + P2,θi α(θi)) + p1,θi"二Cδ E, and finally c3(θi ):=
qθi
LH,θi Lh,θi α(θi)
1-qθi	'
As the inner epoch number t increases, the estimated gradient becomes more accurate. Under the
assumption that H(∙, θ) is convex, the solution converges. We show the full proof in Appendix A.1.
Generalization Bound for Linear Models: Consider a class of linear classifiers θ ∈ Θ
where the weights are norm-bounded by Cθ . Define χ to be the upper bound of any in-
put sample x ∈ D, i.e.,
n-1 PjI hθ (Xj + δ)yj ≤ Y +
kxk2 ≤ χ. Let the empirical risk defined as
maxj6=yj θ (xj + δ)j with a margin γ > 0.
RY (θ)
≤
Theorem 2 For any linear model θ and ξ ∈ (0, 1), the following holds with a probability ofat least
1 — ξ over ((xi, yi))in=1:
Pr arg max [θ(x + δ)j] 6= y
j
≤ RY (θ) + 2Cθ (χ+-Cδ) + 3Jin^ξ),
n	2n
(5)
When the number of clean samples, n, gets larger, the generalizability of the adversarial unlearning
on linear models becomes better. It is also interesting to compare the adversarial generalization
bound for backdoor attacks with the bound for evasion attacks in (Yin et al., 2019), which has an
additional √d factor in the second term of the bound. Hence, for inputs of large dimensions, the
adversarial training for backdoor attacks is expected to be more generalizable than that for evasion
attacks. The detailed proof is shown in the Appendix A.2.1.
5
Published as a conference paper at ICLR 2022
Generalization Bound for Neural Networks: Consider a neural network with L fixed nonlineari-
ties (σ1 , . . . , σL) , where σi is %i-Lipschitz (e.g., coordinate-wise ReLU, max-pooling as discussed
in (Bartlett et al., 2017)) and σi(0) = 0. Let L weight matrices be denoted by (A1, . . . , AL)
with dimensions (d0 , . . . , dL), respectively, where d0 = d is the number of input dimensions,
dL = C is the number of classes, and W = max(d0, . . . , dL) is the highest dimension layer.
Assume that the weight matrices satisfy kAi k2,1 ≤ ai, kA1k2,1 ≤ a0 , and Ai ∈ Rdi×mi. Let
aι = ao(1 + mιn^2Cδ). The whole network's output regarding a backdoored sample is denoted
as: σL (ALσL-1 (AL-1 . . . σ1 (A1 (x+ δ)) . . .)). Let kAikσ be bounded by the spectral norms si.
Theorem 3 For any neural network model θ and ξ ∈ (0, 1), the following holds with a probability
of at least 1 - ξ over ((xi, yi))in=1:
Pr arg max [θ(x +	δ)j] 6= y ≤
j
^	C
RY ⑹ + n +
48 ln(n)(∣∣X kp + √n) v¼(2W(W+n))
γn
QiL=1 si %i
2/3、3/2	/-⑹
A +3".
When the number of clean samples, n, gets larger, the generalizability of the unlearning on neural
networks becomes better. Compared with the generalization bound for regular training in (Bartlett
et al., 2017), our bound has an additional n term in vzln(2W(W + n)) and (IlXkp + √n), which
indicates harder generalizability than regular learning problem. However, empirically, we find our
solution, I-BAU, is still of excellent generalizability even if only 100 clean samples are accessible.
Our proof is enabled by recognizing that the backdoor perturbation, δ, is required to be the same
across all the poison samples and therefore can be treated as additional dimensions of the model
parameters. The proof implements Dudley entropy integral to bound the Rademacher complexity.
The full details are deferred to Appendix A.2.2. We also empirically validate Theorem 3 against W
and n in Appendix A.2.3.
6	Evaluation
Our evaluation aims to answer the following questions: (1) Efficacy: Can I-BAU effectively remove
various backdoor triggers? (2) Stability: Can I-BAU be consistently effective across different runs?
(3) Sensitivity: How sensitive is I-BAU to the poison ratio and the size of the available clean set? (4)
Efficiency: How efficient is I-BAU? We usea simplified VGG model (Simonyan & Zisserman, 2014)
as the target model for all experiments and set Cδ = 10 as the norm constraint for implementing
I-BAU. The details of experimental settings and model architecture can be found in Appendix A.4.
6.1	Efficacy
We evaluate I-BAU’s efficacy against three attack settings: 1) One-trigger-one-target attack, in which
the attacker uses only one trigger, and there is only one target label. This setting is most commonly
considered in existing defense works. 2) One-trigger-all-to-all attack, in which the attacker uses
only one trigger but aims to mislead predictions from i to i + 1, where i is the ground truth label (Gu
et al., 2017). 3) Multi-trigger-multi-target attack, where the attacker uses multiple distinct triggers,
each targeting a different label. We will refer to the first two settings as “one-trigger setting” and the
last setting as “multi-trigger setting.” For each setting, we study seven different backdoor triggers
in the main text, namely, BadNets white square trigger (BadNets) (Gu et al., 2017), Hello Kitty
blending trigger (Blend) (Chen et al., 2017), l0 norm constraint invisible trigger (l0 inv) (Li et al.,
2020a), l2 norm constraint invisible trigger (l2 inv) (Li et al., 2020a), Smooth trigger (frequency
invisble trigger) (Smooth) (Zeng et al., 2021), Trojan square (Troj SQ) (Liu et al., 2018b), and
Trojan watermark (Troj WM) (Liu et al., 2018b). These trigger patterns are illustrated in Figure
3 in the Appendix. Given that I-BAU’s fundamental formulation takes backdoor noise as additive
noise, one may be curious about how I-BAU’s performance mitigates non-additive backdoor attacks.
We have included case studies on using I-BAU mitigating four non-additive triggers (semantical
replacement, WaNet (Nguyen & Tran, 2021), IAB attack (Nguyen & Tran, 2020)) and hidden trigger
attack (Saha et al., 2020) (non-additive poisoning) to illustrate the effectiveness (Appendix A.5).
For all experiments in the mian text, we adopt a large poison ratio of 20%, a severe attack case for
defenders. To acquire a poisoned model, we train on the poisoned dataset for 50 epochs.
We compare I-BAU with six state-of-art defenses: Neural Cleanse (NC) (Wang et al., 2019), Deepin-
spect (DI) (Chen et al., 2019), TABOR (Guo et al., 2019), Fine-pruning (FP) (Liu et al., 2018a),
6
Published as a conference paper at ICLR 2022
Neural Attention Distillation (NAD) (Li et al., 2020b), and Differential Privacy training (DP) (Du
et al., 2019). Note that DP requires access to the poisoned data; hence, its attack model is different
from the attack model of the other baselines and our method. In the following tables, We use [ ASR ] to
mark the results that fail to reduce the attack success rate (ASR) below 20%; we use [ ACC ] to mark
the results where the accuracy (ACC) on clean data drops by more than 10%; we use [ ACC OrASR ]
to mark the best result among the six baselines; finally, we use [ acc or asr ] to mark the results from
I-BAU that is comparable to or significantly better than the best result among the baselines. We con-
sider I-BAU’s result as comparable if 1) the ACC gap is less than 1%, 2) the ASR gap is less than
4%, or the ASR of I-BAU is close to the label percental (i.e., the probability of random guessing for
outputting the target label, 10% for the CIFAR-10, 2.3 % for the GTSRB).
One-trigger Setting: Table 1 presents the defense results on the CIFAR-10 dataset. CIFAR-10
contains 60,000 samples. We use 50,000 samples as training data, among which 10,000 samples are
poisoned. We used 5,000 separate samples as the clean set accessible to the defender for conducting
each defense (e.g., via unlearning, finetuning, trigger synthesis) except DP. The remaining 5,000
samples are used to assess the defense result. The left column depicts the attack cases. The first
seven cases each contain only one target label. The all-to-all case represents the case in which we
use the BadNets trigger to carry out the one-trigger-all-to-all attack. As shown in the table, all single-
target attacks are capable of achieving an ASR close to 100% with no defenses. It is empirically
observed that the ASR of the all-to-all attack is upper-bounded by the ACC. Intuitively, the model
needs to classify the clean samples correctly to classify the corresponding backdoored samples with
triggers to the next class. Thus, we tune the model to produce an ASR that is close to the ACC.
Attack	No Defense		NC		DI		TABOR		FP		NAD		DP		I-BAU(Ours)	
	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR
BadNets	84.94	98.28	83.42	8.76	82.12	48.50	83.78	8.12	82.22	96.66	78.72	11.66	11.08	21.77	83.35	12.30
Blend	84.82	99.78	83.08	33.96	81.84	52.62	83.42	21.26	81.24	89.78	77.48	13.02	11.68	13.72	82.30	12.96
l0 inv	85.36	100	83.02	8.78	83.40	29.1	82.27	8.16	82.18	100	65.08	7.22	12.48	25.22	84.08	9.54
l2 inv	85.26	100	80.68	8.08	82.46	7.82	80.30	11.64	81.50	98.94	43.18	12.56	11.58	20.57	83.48	7.48
Smooth	85.34	99.24	83.72	46.88	83.32	61.82	84.14	45.94	82.66	9.44	77.22	54.38	10.70	28.14	83.46	18.30
Trojan SQ	84.76	99.66	81.30	8.02	83.14	6.94	81.38	7.06	82.34	99.50	51.86	7.84	10.70	18.26	83.18	9.82
Trojan WM	84.92	99.96	81.76	6.02	82.88	7.24	82.60	49.26	81.64	99.88	56.84	0.82	15.21	32.89	83.58	3.42
All to all	86.38	85.02	85.38	82.88	84.74	56.38	×	×	84.48	66.46	75.70	2.34	14.80	10.93	80.34	10.46
Table 1: Results on CIFAR-10, one-trigger cases. CIFAR-10’s ACC is sensitive to fine-tuning and I-BAU; we
compare I-BAU when it drops similar ACCs amount to the most effective method. × - no detected trigger.
Model performance on CIFAR-10 is particularly sensitive to finetuning and unlearning, which will
result in a decrease in the ACC in general. Thus, for a fair comparison, we show the I-BAU results
in Table 1 when the ACC falls to the same level as the most effective baseline. For instance, for
BadNets attacks, we present the ASR result of I-BAU when the ACC of I-BAU decreases to a value
similar to the ACC of TABOR, which is the best defense baseline against BadNets.
Attack	No Defense		TABOR		FP		NAD		DP		I-BAU(Ours)	
	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR	ACC	ASR
BadNets	97.69	99.18	98.99	4.16	99.34	60.19	15.19	9.08	-6.46	-Tqq-	99.38	3.32
Blend	97.44	99.91	99.09	33.32	99.52	-69.66	47.71	18.48	5.70	100	98.89	5.01
l0 inv	97.72	100	98.80	0.47	99.41	-74.86	17.41	1.20	7.40	88.23	99.24	0.42
l2 inv	97.57	99.91	98.51	0.41	99.53	-40.46	15.50	1.16	5.46	100	97.75	0.45
Smooth	97.87	99.89	98.62	0.47	99.55	-47.75	10.06	0.70	5.94	95.58	98.96	0.22
Trojan SQ	98.12	99.98	99.06	5.70	99.48	-75.96	23.31	14.68	5.51	100	99.04	5.11
Trojan WM	97.84	100	98.63	5.40	99.45	-69.82	11.16	13.62	5.70	100	99.44	2.55
All to all	97.10	95.42	98.63	47.07	99.45	-67.34	25.53	0.42	5.87	5.70	99.13	0.04
Table 2: Results on GTSRB, one-trigger cases. I-BAU’s results shown here were obtained after 100 rounds of
I-BAU. For that, Neural Cleanse, Deppinspect, and TABOR are from the same line of work, so we here only
compare the result with the most state-of-art method in this category, TABOR.
As shown in Table 1, the performance of the baselines exhibits large variance across different trig-
gers. Specifically, each baseline underperforms in at least three attack cases (marked by ([ ASR ] or
[acc ])) The defenses based on trigger synthesis (including NC, DL and TABOR) failed to confront
the all-to-all attack case, where allthe labels were targeted. This is because this setting violates their
assumption that target classes only account for the minority of training data. Particularly, TABOR
fails to detect any backdoor triggers in this case). For DP, we fine-tune a noise multiplier effective
to mitigate all attacks, yet also leads to bad ACC. On the other hand, I-BAU robustly mitigates all
triggers without significantly affecting the ACC. Compared to the best result among the state-of-art
baselines ([ acc or ASR ]), I-BAU,s is comparable to or much better ([ acc or ASR ]) under most of the
settings. The only setting where I-BAU underperforms the best baseline is Smooth triggers. FP is
the only effective baseline in this setting. But interestingly, this setting is also the only setting where
FP is effective; in other words, its performance is highly dependent on the underlying trigger.
7
Published as a conference paper at ICLR 2022
	No Def.	NC	DI	TABOR	FP	NAD	DP	Ours*
ACC	85.96-'	X	83.74	84.04	83.42	77.38	11.18	,	'-77.44
avg. ASR	98.37-	X	30.37	40.68	73.18	10.93	-12.83-	12.96
TrojanWM:^9"	99.92-	X	-968^^	28.02	99.7	18.38	-13.40-	11.48
Trojan SQ: ⇒ 2	99.42	X	13.78	19.56	99.36	11.94	2.58	10.80
B BadNets: ⇒ ð~~	94.5	X	72.6	9.16	9.56	13.32	6.94	18.64
S Smooth: ⇒ 1	97.08	X	30.58	89.70	9.32	8.38	6.28	15.22
V Blend: ⇒ 3	98.12	X	48.84	50.84	96.14	8.46	27.98	18.24
lo inv: ⇒ 4	100	X	23.78	80.96	99.88	9.88	18.02	9.46
l2 inv: ⇒ 5	99.54	X	13.34	6.52	98.32	6.16	14.62	6.92
ACC	97.18	:	:68.72	99.33	76.38	99.36	10.83	-^6T7-:	:99.09
avg. ASR	99.37-	-10.87^^	-7.21 ^^	-11.89	-938-	11.59	42.86	4.18
Trojan WM: ⇒ 9	-99.49-	400^-	-360^-	-386-	-156-	-661-	100	1.78
Trojan SQ: ⇒ 2-	99.59	2.79	6.26	2.33	0.12	4.14	0	5.68
B BadNets: ⇒ 0	98.19	6.61	0.47	10.29	14.09	4.79	0	0.72
汽 Smooth: ⇒ 1	99.89	8.56	6.92	15.28	10.05	2.83	100	5.65
V Blend: ⇒ 3	99.15	46.89	22.66	45.91	29.69	41.54	100	3.76
lo inv: ⇒ 4	100	1.31	5.15	1.06	0.59	17.70	0	5.25
l2 inv: ⇒ 5	99.33	5.94	5.43	4.52	9.60	3.52	0	6.46
Table 3: Results for 7-trigger-7-target cases. × marks no trigger was detected. *Here, ASR results on CIFAR-
10 are provided when the model attained an ACC similar to that of NAD (the only effective one on CIFAR-10).
Table 2 shows the evaluation on the GTSRB dataset, which contains 39,209 training data and 12,630
test data of43 different classes. The experimental setting is the same as the one on CIFAR, except for
the data split, which is discussed in the Appendix A.4.1. We find that model performance on GTSRB
is not sensitive to the tuning procedure. As 5,000 clean samples are available to the defender, the
ACC of the model increases after incorporating the defense. NAD’s performance highly depends
on the pre-designed preprocessing procedure adopted during fine-tuning. While it produces the
best defense results for some settings on CIFAR-10, it suffers from a large degradation of ACC on
GTSRB. Once again, I-BAU is the only defense method that remains effective for all attack settings.
Multi-trigger Setting: Table 3 shows the results on CIFAR and GTSRB in a 7-trigger-7-target
setting, in which each trigger targets a different label. We adopt the same poison ratio for each (20%)
and then combine the seven distinct poisoned datasets to obtain the final datasets (size 350,000 for
CIFAR-10, and 274,463 for the GTSRB). We show the average ASR and the specific ASR.
On CIFAR-10, since the target classes are no longer the minor-
ity (i.e., 7/10 labels are targeted), the baselines based on trig-
ger synthesis, which make the minority assumption, are ineffec-
tive. Particularly, NC fails to detect any triggers in this setting.
NAD is the only baseline able to evaluate well on CIFAR-10, and
our methods achieve comparable performance. However, sim-
ilar to the one-trigger setting, NAD’s performance requires the
customization of preprocessing to different datasets. The default
preprocessing cannot maintain the same performance on GTSRB;
e.g., the random flipping—a beneficial preprocessing step for
CIFAR—completely alters the semantics of images in GTSRB.
On the other hand, I-BAU effectively reduces ASR while main-
taining the ACC for both datasets with no changes needed. More-
over, in the Appendix, we visualize the distribution of poisoned
and clean examples in the feature space before and after applying
I-BAU, which shows that I-BAU can place the poisoned examples
back in the cluster with correct labels.
Figure 1: Comparison of Naive
and I-BAU. I-BAU’s perfor-
mance is more stable.
6.2	Stability
In Section 4, we mentioned that the naive heuristic based on adversarial learning with existing
universal perturbation suffers from erratic performance. Here, we aim to evaluate whether I-BAU
can overcome this problem. We adopt the same setting for the two on the CIFAR-10 using BadNets
and observe the change in ACC and ASR over different iterations for each run. As shown in Figure
1, the ASR decreases very quickly; indeed, the attack can be mitigated by just one single iteration
of outer minimization. Also, within every single run, the ASR decreases more smoothly compared
to the naive heuristic. Notably, I-BAU attains a slightly higher ACC than the naive heuristic.
8
Published as a conference paper at ICLR 2022
6.3 Sensitivity
We evaluate the sensitivity of each defense to the poison ratio and the size of clean samples. The ex-
periments were performed on Trojan WM on CIFAR-10 to exemplify the results. Table 4 shows the
sensitivity to the poison ratio. Observe that as the poison ratio drops, TABOR becomes ineffective at
synthesizing triggers and ultimately fails to remove the triggers; NC, however, becomes the most ef-
fective baseline. While DP’s ASR is significantly worse than the effective baselines, its performance
gets slightly better as the poison ratio drops. DP attempts to restrict the impact of each training data
point on the learning outcome via noising the gradient. As a side effect, it hinders learning from
good data, thereby leading to poor ACC in general. Compared to the baselines, I-BAU exhibits the
least sensitivity to the poison ratio, maintaining good ACC and ASR across different ratios.
poison ratio	Results	No Def.	NC	DI	TABOR	FP	NAD	DP	Ours
5.0%	-ACC	86.58	83.14	78.63—	×	83.16—	79.74	36.80	84.76-
	-ASR-	99.88	5.58	10.40	×	99.72	6.34	96.84	9.78
0.5%	-ACC	86.42	-84.16-	83.56-	×	84.72	80.92	39.92	83.22-
	ASR	98.58	12.9	20.22	×	93.78	28.6	61.27	13.08
Table 4: Results on CIFAR-10 (Trojan WM) with different poison ratios. × marks no trigger was detected.
Table 5 shows sensitivity to available clean data’s size. As DP is independent from the clean set
(which trains a general robust model against all perturbations from scratch), we drop it from the
comparison. We see that as the number of clean samples drops, the performance of all defenses
declines. I-BAU is least sensitive to the size of clean samples; even in extreme case with only 100
clean samples available, I-BAU still maintains an acceptable performance of removing backdoors.
# Clean Data	Results	No Def.	NC	DI	TABOR	FP	NAD	OUrS
2,500	ACC-	84.92	78.39-	80.63-	80.23-	81.36	~46.8-	82.21—
	ASR	99.96	6.53	10.07	33.40	99.58	7.12	6.96
500	ACC-	84.92	78.24	80.17	77.03-	78.1	38.5	80.07
	ASR	99.96	25.66	1.14	21.92	85.68	9.08	5.20
100	ACC-	84.92	84.101	69.51-	83.495	73.00	-36.14	-769-
	ASR	99.96	99.92	1.12	99.687	97.80	5.76	4.00
Table 5: Results with different # of clean data on CIFAR-10 (Trojan WM).
6.4 Efficiency
Finally, we compare the efficiency of defenses. We use the one-
trigger-one-target attack setting to exemplify the result. Table 6
shows the average runtime for each defense to take effect (i.e.,
mitigating the ASR to < 20%). As I-BAU can mitigate the attacks
in one iteration, it only takes 6.82 s on average on CIFAR-10 and
7.84 s on GTSRB. Note that NC and TABOR need to go through
each label independently for trigger synthesis; thus, the total run-
time is proportional to the number of labels. Especially, it takes
much more time for the two to take effect on GTSRB than on
CIFAR-10. In difficult attack cases, such as all-to-all attacks and
	CIFAR-10 (S)	GTSRB (s)
NC	384.92	―1864.96-
DI	394.38	472.21
TABOR	1123.31	3529.70
FP	45.33	83.78
NAD	79.90	79.14
Ours	6.82	7.84
Table 6: Average time for de-
fenses to be effective on one-
trigger-one-target cases.
multi-trigger-multi-target attacks, I-BAU requires more rounds to be operative but remains the only
effective one across all settings with high efficiency and efficacy. The Appendix shows the per-
formance of I-BAU over different rounds under multi-target attack cases. Theoretical analysis and
comparison of the time complexity of I-BAU are presented in Appendix A.3.3.
7	Conclusion
In this work, we proposed a minimax formulation of the backdoor removal problem. This formula-
tion encompassed the objective of the existing unlearning work without making assumptions about
attack strategies or trigger patterns. To solve the proposed minimax, we proposed I-BAU using
implicit hypergradients. A theoretical analysis verified the convergence and generalizability of the
proposed I-BAU or minimax formulation. A comprehensive empirical study established that I-BAU
is the only generalizable defense across all evaluated eleven backdoor attacks. The defense results
are comparable to or exceed the best results obtained by combining six existing state-of-the-art tech-
niques. Meanwhile, I-BAU is less sensitive to poison rate and is effective in extreme cases where the
defender has access to only 100 clean samples. Finally, under the standard one-trigger-one-target
circumstances, I-BAU can achieve an effective defense in an average of 7.35 s.
9
Published as a conference paper at ICLR 2022
Acknowledgements
This work was supported by the Commonwealth Cyber Initiative, an investment in the advancement
of cyber R&D, innovation, and workforce development. For more information about CCI, visit
www.cyberinitiative.org
References
Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models, 2021.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. Advances in Neural Information Processing Systems, 30:6240-6249, 2017.
Walter Baur and Volker Strassen. The complexity of partial derivatives. Theoretical computer
science, 22(3):317-330, 1983.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. Journal of machine learning research,
18, 2018.
Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum,
Tom Goldstein, and Arjun Gupta. Strong data augmentation sanitizes poisoning and backdoor
attacks without an accuracy tradeoff, 2020.
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21
(3):977-995, 2011.
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728, 2018.
Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan
detection and mitigation framework for deep neural networks. In IJCAI, pp. 4658-4664, 2019.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning, 2017.
Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal
attacks against deep learning systems. In Deep Learning and Security Workshop, 2020. URL
https://arxiv.org/abs/1812.00292.
John M Danskin. The theory of max-min and its application to weapons allocation problems, vol-
ume 5. Springer Science & Business Media, 2012.
Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via
differential privacy. arXiv preprint arXiv:1911.07116, 2019.
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, and Surya Nepal.
Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual
Computer Security Applications Conference, ACSAC ’19, pp. 113-125, New York, NY, USA,
2019. Association for Computing Machinery. ISBN 9781450376280. doi: 10.1145/3359789.
3359790. URL https://doi.org/10.1145/3359789.3359790.
Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang. Can adversarial weight pertur-
bations inject neural backdoors. Proceedings of the 29th ACM International Conference on
Information & Knowledge Management, Oct 2020. doi: 10.1145/3340531.3412130. URL
http://dx.doi.org/10.1145/3340531.3412130.
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration com-
plexity of hypergradient computation. In International Conference on Machine Learning, pp.
3748-3758. PMLR, 2020.
10
Published as a conference paper at ICLR 2022
Andreas Griewank. Some bounds on the complexity of gradients, jacobians, and hessians. In
Complexity in numerical optimization, pp. 128-162. World Scientific, 1993.
Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algo-
rithmic differentiation. SIAM, 2008.
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.
Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach
to inspecting and restoring trojan backdoors in ai systems. arXiv preprint arXiv:1908.01763,
2019.
Hasan Abed Al Kader Hammoud and Bernard Ghanem. Check your other door! establishing back-
door attacks in the frequency domain. arXiv preprint arXiv:2109.05507, 2021.
Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling
the training process. In ICLR, 2022.
Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. Intrinsic certified robustness of bagging against
data poisoning attacks, 2020.
Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of nearest neighbors
against data poisoning attacks, 2021.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1885-1894,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL http://
proceedings.mlr.press/v70/koh17a.html.
Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defense against general
poisoning attacks, 2020.
Shaofeng Li, Minhui Xue, Benjamin Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoor
attacks on deep neural networks via steganography and regularization. IEEE Transactions on
Dependable and Secure Computing, 2020a.
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention
distillation: Erasing backdoor triggers from deep neural networks. In International Conference
on Learning Representations, 2020b.
Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey.
arXiv preprint arXiv:2007.08745, 2020c.
Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor attack in the
physical world. In ICLR Workshop, 2021a.
Yiming Li, Baoyuan Wu, Yan Feng, Yanbo Fan, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Semi-
supervised robust training with generalized perturbed neighborhood. Pattern Recognition, 124:
108472, 2022.
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor
attack with sample-specific triggers. In ICCV, 2021b.
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses, pp. 273-294. Springer, 2018a.
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu
Zhang. Trojaning attack on neural networks. In 25nd Annual Network and Distributed System
Security Symposium, NDSS. The Internet Society, 2018b.
11
Published as a conference paper at ICLR 2022
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor
attack on deep neural networks. In European Conference on Computer Vision, pp. 182-199.
Springer, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1765-1773, 2017.
Anh Nguyen and Anh Tran. Wanet-imperceptible warping-based backdoor attack. arXiv preprint
arXiv:2102.10369, 2021.
Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. Advances in Neural
Information Processing Systems, 33:3454-3464, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Gilles Pisier. RemarqUes SUr Un resultat non publie de b. maurey. Seminaire Analysefonctionnelle
(dit, pp. 1-12, 1981.
Han QiU, Yi Zeng, Shangwei GUo, Tianwei Zhang, Meikang QiU, and Bhavani ThUraisingham.
Deepsweep: An evalUation framework for mitigating dnn backdoor attacks Using data aUgmen-
tation. In Proceedings of the 2021 ACM Asia Conference on Computer and Communications
Security, pp. 363-377, 2021.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. Advances in neural information processing systems, 32, 2019.
AnirUddha Saha, AkshayvarUn SUbramanya, and Hamed Pirsiavash. Hidden trigger backdoor at-
tacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volUme 34, pp. 11957-
11965, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolUtional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatUres in backdoor attacks. In Advances
in Neural Information Processing Systems, pp. 8000-8010, 2018.
Alexander TUrner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks.
arXiv preprint arXiv:1912.02771, 2019.
BolUn Wang, YUanshUn Yao, Shawn Shan, HUiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. NeUral cleanse: Identifying and mitigating backdoor attacks in neUral networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707-723. IEEE, 2019.
Ren Wang, GaoyUan Zhang, Sijia LiU, Pin-YU Chen, JinjUn Xiong, and Meng Wang. Practical
detection of trojan neural networks: Data-limited and data-free cases. In Computer Vision-ECCV
2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIII
16, pp. 222-238. Springer, 2020.
Maurice Weber, XiaCjUn Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.
Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans
using meta neural analysis. arXiv preprint arXiv:1910.03137, 2019.
Dong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially
robust generalization. In International conference on machine learning, pp. 7085-7094. PMLR,
2019.
12
Published as a conference paper at ICLR 2022
Yi Zeng, Won Park, Z Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks’ triggers: A
frequency perspective. In ICCV, 2021.
Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of
Machine Learning Research, 2(Mar):527-550, 20θ2.
Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-
label backdoor attacks on video recognition models, 2020.
13
Published as a conference paper at ICLR 2022
A Appendix
A.1 Convergence Bound
Consider the minimax formulation of backdoor unlearning defined in (1). To simply the nota-
tion, we will use θ instead of θi, unless otherwise specified. We start by defining Φ(δ, θ) = δ +
α(θ)V1H (δ, θ), where H (SUm of cross-entropies) is twice continuously differentiable w.r.t. both δ
and θ. Recall that H(∙, θ) is μH(θ)-strongly convex and LH(θ)-Lipschitz smooth, where μH(∙) and
LH (∙) are continuously differentiable. By setting the step size α(θ) = LH⑻；..⑻,it can be shown
that Φ(∙, θ) is a contraction for some coefficient qg := max {1 一 α(θ)μH(θ),α(θ)La(θ) — 1}. The
optimal choice of the step-size leads to qg = K(θ)+1, where κ(θ) = LH卷.Note that, for every
i∈ (1,K),θ∈Θ,
μH(θ)I 4 V2H (δi(θ),θ) 4 Lh(Θ)I.	(7)
Hence, the condition number of V21H (δi (θ), θ) is smaller than κ(θ) (Grazzi et al., 2020).
Let ∂1Φ(δ, θ) and ∂2Φ(δ, θ) denote the partial Jacobians of Φ at (δ, θ) w.r.t. the first and the second
variables, respectively. We can write the derivatives of Φ as:
∂2Φ(δ,θ) = -VιH(δ, θ)Vα(θ)> 一 α(θ)V2ιH(δ,θ),
∂1Φ(δ, θ) = I 一 α(θ)V21H (δ, θ).	(8)
When evaluated at (δ(θ), θ), the partial Jacobian w.r.t. the second variable, ∂2Φ(δ(θ), θ), can be
simplified as:
∂2Φ(δ,θ) = -α(θ)V2ι H (δ(θ),θ).	(9)
Thus, k∂2Φ(δ(θ), θ)k = α(θ)kV221H (δ(θ), θ)k. Following the notations from (Grazzi et al., 2020),
we create the bound of the partial Jacobian of Φ to be LΦ,g ≥ k∂2Φ(δ(θ), θ)k. By our assumption,
the bound for the cross derivative term δ is Lh,g ≥ V221H (δ(θ), θ). Hence, we can choose:
LΦ,g = α(θ)Lh,g.	(10)
Let ∆∂1Φ := k∂1Φ (δ1, θ) 一 ∂1Φ (δ2, θ)k and ∆∂2Φ := k∂2Φ (δ1, θ) 一 ∂2Φ (δ2, θ)k. By assump-
tion on the Lipschitz continuity and norm boundedness of second-order terms, we have :
∆∂1Φ = ∣∣α(θ) (V2H (δι,θ) -V2H (δ2,θ))∣∣ ≤ α(θ)Pι,θ Μ - δ2k,	(11)
and:
∆∂2Φ =k (VιH (δι,θ) — ViH (δ2,θ)) Vα(θ)> + α(θ) EIH (δ1,θ) - VhH (δ2,θ)) k
≤ (LH(θ)kVα(θ)k + α(θ)ρ2,θ) ||比 一 δ2k .	(’
Thus, ∂ιΦ(∙, θ) is Lipschitz continuous with constant
Vi,θ := α(θ)pi,θ,	(13)
and ∂2Φ(∙,θ) is Lipschitz continuous with constant
v2,θ := LH ⑹ kVa⑹ k + α⑹p2,θ.	(14)
Recall the result from (Grazzi et al., 2020):
Lemma 1 The approximate hypergradient has an error bounded by
∣∣Vψ(θ)-Vψ*(θ)∣∣ ≤ 卜i(θ)+ C2(θ)=qt) Pθ(t) + c3(θ)qθ,
(15)
where
c1 (θ) =	Sθ+η⅛θ	Cδ,
c2 (θ) =	卜,θ+ν⅛⅛θ,	LH,gCδ,
c3(θ) =	.LH,θ LΦ,θ	
	i-qθ *	
(16)
The proof of Theorem 1 uses the inequality from Lemma 1 with the constants specified in (10), (13),
and (14).
14
Published as a conference paper at ICLR 2022
A.2 Generalization B ounds
Following the notations from (Bartlett et al., 2017), we define a general margin operator M(v, y) :=
Vy - maxj=y Vj, and the ramp loss 'γ with a given margin γ:
(0	r < -γ
1 + r∕γ r ∈ [-γ, 0] .	(17)
1	r>0
According to (Yin et al., 2019), the population risk against a perturbation δ is given by:
RY(θ) := E ('γ(-M(θ(x + δ),y))),	(18)
and the empirical risk is given by:
RY(θ) ：= n-1 X 'γ (-M (θ (xj + δ) ,yj)).	(19)
j
Note that RY (θ) and RY(θ) are the upper bounds of the fraction of errors on the source distribution
and the dataset used for unlearning, Dsan , respectively. Finally, given a set of real-valued functions
H, recall the Rademacher complexity as:
n
< (H∣s) := n-1Eς SUp XSjh (Xj,yj),	(20)
h∈Hj=1
where ςj ∈ {±1} is the Rademacher random variable. The following bound of the adversarial
unlearning can be derived using standard tools in Rademacher complexity.
Lemma 2 Given unlearned models Θ with θ ∈ Θ and some margin γ > 0, define:
Θγ := {(x, y) → 'γ(-M(θ(x + δ),y)) : θ ∈ Θ}.
(21)
The following holds with the probability of at least 1 -ξ over the clean dataset, Dsan (see Algorithm
1) for every θ ∈ Θ:
Pr arg max[θ(x + δ)j] 6= y
j
≤ RY (θ) + 2< ((Θγ )∣Dsan ) + 3心户.
(22)
To instantiate this bound, We only need to control the Rademacher complexity, < ((Θγ)∣Dsan), for
linear models and neural networks.
A.2. 1 Proof for linear models
Define the set of linear, poisoned models under perturbation δ:
ΘlYin := {(x, y) 7→ hθ,x+δi : θ∈ Θ},	(23)
where θ can be regarded as a weight matrix kθk bounded by Cθ . Recall that the perturbation norm
kδ k is bounded by Cδ, and the input l2 norm kxk2 is bounded by χ. We can proceed to evaluate the
15
Published as a conference paper at ICLR 2022
upper bound of the Rademacher complexity:
1n
sup — T ςj hθ,x + δ
kθk≤Cθ n j=1
≤ Eς
1
SUP —
kθk≤Cθ n
nI
Xςj(xj + δ)II kθk
j=1	I
(Cauchy-Schwarz inequality)
1
n
1
n
1
n
/
Xςj(Xj + δ)II I (given that kθk ≤ Cθ)
n
n
j=1
ςjxj + δ	ςj
j=1
(24)
n
ςjxj
j=1
n
δ	ςj	(Triangle inequality)
j=1
1
+---
n
∖
n
n
E〈 一
n
nn
X ς Xj +Eς 1 δ X
ςj
j=1
^{^―
A
I j=1
|------{---
B
}
}
In particular, A can be bounded by:
unn
Eςn、∑∑ςjςk hxj , xki
j
1 k=1
n
Eς	ςjςk hxj , xki (Jensen’s inequality)
j,k
(25)
Similarly, B can be bounded by:
n
Xkxik2
j=1
A
≤
≤
≤
n
n
1
n \
1
n \
χ
B ≤ Eς sup —
kδk≤Cδ n
n
δXςj
j=1
≤ CδEς I∣Xςj (given that kδk ≤ Cs)
≤ C
n
n
Eς	ςj ςk (Jensen’s inequality)
j,k
√n.
Thus, we can bound the Rademacher complexity as following:
< ((Θγ )∣Dsan) ≤ C^√+C)
Substituting (27) to the (22) completes the proof of Theorem 2.
(26)
(27)
16
Published as a conference paper at ICLR 2022
A.2.2 Proof for neural networks
To instantiate the bound with Rademacher complexity for neural networks, we will follow the
idea from (Bartlett et al., 2017) and use covering numbers to bound the Rademacher complexity,
< ((Θγ)∣Dsan). Let N(U, e, k∙k) denotes the least cardinality of any subset V ⊆ U that covers U
at a scale e with norm ∣∣∙k, i.e., supa∈u minB∈v ∣∣A - Bk ≤ e. Recall the Dudley entropy integral
below.
Lemma 3 Assume that the input values are norm bounded by 1,
<(9yIDsan) ≤ α>fo (√αn+√ Z	qlogN((巴力，,。.，e，k∙心)d).
(28)
Thus, the derivation of the generalization bound of (1) with neural networks is reduced to
the problem of finding the bound for the covering number of the set of all neural networks,
N ((Θγ)∣Dsan, e, ∣ ∙ ∣∣2). The full problem can be divided into four steps: (I) finding a matrix-
covering bound for the affine transformation of the input layer of poisoned models, conducted in
this section; (II) finding a matrix-covering bound for the affine transformation of the following lay-
ers after the first layer, provided in (Bartlett et al., 2017); (III) obtaining a covering number bound
for entire networks using induction on layers; (IV) accomplishing the complete generalization bound
for neural networks by applying the covering number to (22) and (28).
Step (I): Input Layer Matrix Covering. The covering number of the input layer considers the
matrix product, Z=(X + ∆)Aι, where Ai ∈ Rd×m is the weight matrix of the input layer,
X ∈ Rn×d is the input data, and ∆ = [δ, δ, . . . , δ]> is the UNO perturbation vector repeated n
、----{----}
n
times to get the same shape as X. Note that Z can be rewritten as:
ʌ . ..
Z=(X + ∆)Aι
[ X	In ]
A1
∆A1
(29)
^ ^
X Al,
where In is the n × n identity matrix, X is an n × (d + n) matrix, and A1 is a (d + n) × m matrix.
Now, the goal of step (I) is to find the covering number for the matrix product, X A1 .
Given X ∈ Rn×d, we can obtain the normalized matrix Y ∈ Rn×d by rescaling the columns of X
to have unit p-norm: K,j := X：,j/ ||X：,j ∣p. Let No := 2(n + d)m, and define
{ V1, ... , VN0 } := {g [ Y In ] eie> : g ∈ {-1, +1}, i ∈ {1,...,d + n}, j ∈ {1,..∙, m}}.
(30)
Forp ≤ 2, the results from (Bartlett et al., 2017) implies:
max IlViIl ≤ max k [ Y	In ] ei∣∣0 = max
i	2	i∈{1...N0}	2 i
^
X ei
^
X ei
2 ≤ 1.
(31)
p
Define α0 ∈ R(d+n)×m to be a “rescaling matrix”:
Γ kx：,ikp
kX：,2kp
.kX：,lkp
.kX,2kp
α0
kX：,dkp
1
.kX：,d kp
.1
(32)
1
1
17
Published as a conference paper at ICLR 2022
where the purpose of α0 is to annul the rescaling of X introduced by Y = [ Y In ], i.e., XA1
Y (α0 A1), and denotes the element-wise product. Let B := α0 A1. Then, we have:
. .. ^ ^
(X + ∆)Aι = X Al
^ ^
=Y B
n+d m
Y XXBij eiej>
i=1 j=1
n+d m
YkB kι X X ⅛ eiej>
(33)
n+d m
kBkιXX
二
Bij
W7
[Y
In ] eiej
∈ kB kι ∙ conv ({fl，...，VN0})，
where ConV ({忆，...，VNO}) is the convex hull of {亿，.•.，VNO}. Given conjugate exponents
(p，q) and (r，S) WithP ≤ 2, by the conjugacy of ∣∣∙kp,r and ∣∣∙kq,s:
..^ ..	.	. ^ .. .. .. .. ^ ..
IIBk1 ≤ hα0, ∣A1∣i ≤ ∣∣α0kp,r k^A-1kq,s.	(34)
-rʌ r∙	C ,11+1	1' ,1 Γ∙ . 1	令 Λ
Defining C as the desired cover of the first layer XA1 :
C:
∣ι⅛
N	N	kB k 一	—
EkiK ： ki ≥ 0，Eki = k> = JLkM EK, ： (ii，...，ik) ∈ [No]k
i=1	i=1	1	j=1
(35)
where, k := a02(1+mnιC∣*kp+nι)2mr . By ConstrUCtion, |C| ≤ [M]k. Now, we prove that
C is the desired cover set. The technique is generalized from (Bartlett et al., 2017) to backdoor
unlearning.
Consider the case of kA1 k2,1, i.e., (q， S) = (2， 1), and let kA1 k2,1 ≤ a0, then we get:
llɑ0∣p,r = (J (la：,1 Ip，…，g:,mlp)(
=	kX:,1kp，...， kXi,dkp，1，...，1	，...，	kX:,1kp，...，kX:,dkp，1，...，1
=i[x"1kp，…，kX]kp，E!lp
=m1/r (X kχj∣P + nj
≤m1" (∣∣Xkp + n1∕p).
l l	(36)
Subsequently, the bound of ∣ Ai ∣	can be derived from:
=
1
,
2
A1A
A∆
1:
A:A
A∆

= KkA：ik2 + ∣∣∆4ιk2)
1/2	2	2 1/2
，…，(kA：mk2 + k∆4mk2)	ɪ
(37)
18
Published as a conference paper at ICLR 2022
where we have:
k∆A”k2
(38)
Therefore, substituting (38) to (37) gives us the bound:
2 ] = KkA:1 k2 + n (δ>A：1)2) 1/2 ,..., (kA：mk2 + n (δ>Am)2)”
≤ ||kA：1k2 + n1/2 ∣∣δ>Al∣∣ , . . . , kA:mk2 + n1/2 ∣∣δ>4m∣∣ ]
= IlA:1k2 + n1/2 llδ>A:[ 11 + . . . + kA:mk2 + n1/2 ∣∣δ>4ml1
m
=kA1k2,1 + n1/2 X∣∣δ>Aj∣∣
j=1
1
m
≤kA1k2,1 + n1/2 X (kδk2 ∙kAj k2)
j=1
=kA1k2,1 (1 + mn1∕2kδk2)
≤ a0 1 + mn1/2 Cδ .
Let a1 = ao(1 + mn1/2Cδ). Given the case of ∣∣A1∣∣21, We can obtain the bound for ∣∣B ∣∣1:
kBk1 ≤ kα0kp,rkA1k2,1 ≤ m1/r (kXkp + n1∕p) a1.
(39)
(40)
Thus, combining (31) and folloWing the Maurey lemma (Pisier (1981), Zhang (2002), Lemma 1),
We have:
(X + ∆)A1 - ⅛ XkiV	≤ kBk- max ∣∣Vi∣∣2
k	k i=1...N0	2
i=1	2
≤ a12(kXkp + n2)2m2	(41)
≤	k
≤ 12,
Which shoWs that the desired cover element is in C .
Theorem 4 In the case of the input layer of a poisoned model, θ, consider ∆ as the UNO perturba-
tion matrix with n rows of δ, and A1 ∈ Rd×m to be the weight metrix of the first layer. The covering
resolution e〔 is given. Defining a] := a0(1 + mn1/1Cδ), where a° is the bound of ∣∣A1k2 1 ,forany
input X ∈ Rn×d, the convering number is bounded as follows:
lnN ({(X + ∆)A1 : A1 ∈ Rd×m} , e1, k∙ k2) ≤
a12(kX kp + n1/2)=2"
e12
ln(2(d + n)m).
(42)
Step (II): Other Layer’s Matrix Covering. Let’s define Z ∈ Rni×di as the input of a specific
layer of the neural netWork, and Ai ∈ Rdi×mi as the Weight matrix of that layer. Given conjugate
exponents (p, q) and (r, s) With p ≤ 2, letkAi kq,s ≤ ai . Lastly, the covering resolution of each
19
Published as a conference paper at ICLR 2022
layer, i , is given. Since no perturbation is considered in the following layers, we can directly adopt
the covering studied in (Bartlett et al., 2017) as follows:
lnN ({ZAi : Ai ∈ Rdi×mi, kAikq,s ≤ 电} , G, k∙ k2) ≤
ai2kZkp2mi2∕r
ln(2dimi). (43)
Step (III): The Whole Network Covering Bound. Recall that the whole neural network is struc-
tured as follows: Fθ(x) := σL (ALσL-1 (AL-1 . . . σ1 (A1x) . . .)), where σi is %i-Lipschitz.
Define two sequences of vector spaces Vι,..., VL and W2,..., Wl+i, where Vi has a norm | ∙ |i
and Wi has a norm 川∙ |||i. The linear operators, Ai : Vi → Wi+ι, are associated with some
operator norm lA∕i→i+ι ≤ Si, ie,〔A/if+i := kAikσ ≤ sup∣z∣i≤ι 川AiZMi+1 = Si Then,
letting τ := Pj≤L j%j QlL=j+1 %lsl, with given convering resolutions, (1, . . . , L), the neural net
images, HX := {Fθ(X + ∆)}, have the covering number bound (Bartlett et al., 2017):
L
N (HX,τ,1• |L+1) ≤ Y	SUp	N ({AiF(Aι,...,Ai-I)(X + δK ,ei, ∣∣∣∙ I∣∣i+l) .	(44)
i=1 (A1 ,...,Ai-1)
=	∀j<i
Step (IV): Proof of Theorem 3. The key technique in the remainder of this proof is
•	1) to substitute covering number estimates from (42) and (43) into (44) but
•	2) centering the covers at 0 (meaning the cover at layer i ∈ (2, L) satisfies IAi I2,1 ≤ ai,
IlAIIl2,1 ≤ aο, and ∣∣A1∣∣2,1
∆AA11	≤ a1), and
•	3) collecting (x1, . . . , xn) as rows of matrix X ∈ Rn×d
To start, the covering number estimate of the whole network from (44) when combined with (42)
and (43) (specifically with p = 2, S = 1, and W = max(d0, . . . , dL)) results in:
ln N (HX, e, k ∙ k2)
≤ PL=ISUp (AM....") lnN
∀j<i
"iP(Aι,A2...,Ai-ι )
=SUpAIlnN (《Ai
X>
In
,e1,k ∙ k2
X>
In
+ PL=2 sup(A2,∀.<ij) ln N (卜 iF(Ai,...,Ai-ι) ( ] ɪn
a21
Ai
,ei, k ∙ k2
≤supA1
XIn> #!>
J 卜i,k∙k2
(45)
2 ln (2W(W + n))
+ PiL=2 sup(A2,...,Ai-1)
∀j<i
a F(A
1,A2,...,Ai-1)	XIn>	>
2 ln 2W2 ,

2
2
T2
i
where (*) equality holds since 1) l2 coverings of a matrix and its transpose are the same, and 2) the
X>
In
cover can be translated by F(AI A2	Al)
>
Ai> without changing its cardinality. We
can further simplify (45), by evaluating the following norm for any Ai ∈ (A2, . . . , AL-1):
F	X>
F(Aι,A2,...,Ai-ι), [ In
2
2
(46)
20
Published as a conference paper at ICLR 2022
which by induction gives
X>
max F(Aι,A2,...,Ai-ι) (	In
i-1
≤ (kXkp+n1/2)Y%jsj.
2	j=1
Combining (46) and (47), the cover is bounded by:
ln N (HX ,3∣H∣2)
aι2(∣χkP + √nT
1
PL ai
i=1
ln(2W(W+n))+PiL=2
a2(kχkP + √n)2%2s2 Q2
s2
sj ln(2W (W + n)).
<j<i
%j2sj2 ln(2W2)
Let
αi	1 ai 2/3
,where α := 一
%i j>i %jSj	a Vi√
α:= X (aj r
then,
ln N (HX, e, k∙k2) ≤ dχkp+√n)2 ln(2W(W+n))QL=i j (a3).
(47)
(48)
(49)
(50)
Consider the class of networks, Θ∖N, obtained by affixing the ramp loss, '7, and the negated margin
operator, -M, to the output of the provided network class:
θNN := {(x, y) → 'γ(-M(θ(χ), y)) ： θ ∈ Θ}.
(51)
Since (z, y) → '7(-M(z, y)) is 2∕γ-Lipschitz w.r.t. ∣∣ ∙ ∣∣2 and definition of '7, the function class
ΘγNN falls under the setting of (50), the covering number ofa set of all neural networks is bounded
as follows:
ln N ((ΘNN )∣χ ,e,k∙∣∣2)
一 (kXkp + √n)2 ln(2W(W+n))
≤	^2
: (kXkp + √n)2 ln(2W(W+n))R
(52)
Using the above covering number bound (52) in the Dudley entropy integral (28) with α := 1∕√n,
we achieve the bound for the Rademacher complexity as follows:
R ((FY)|J ≤ ɪnf √α + ln(√n∕ɑ)1^≡ + √n≡≡+B≡
≤ 4 + 24ln(n)(∣X∣p + √n),ln(2W(W + n))
n	γn
We complete the proof of Theorem 3 by substituting (53) in (22).
3/2
.
(53)
A.2.3 Emperical validation of theorem 3
In this section, we empirically verify Theorem 3 regarding two variables: the neural network’s width,
W, and the number of clean samples. The experiment is conducted with poisoned models trained
over Trojan WM poisoned CIFAR-10 (poison rate: 20%, target label: 2).
Model Name	# Parameters	W	Average Error Gap (%)
ResNet-18	-11689512-	512	035
GoogLeNet	-6624904-	832	027
DenseNet-121	7978856 ―	1024	0.24	-
Table 7: Empirical Error Gap with different widths (W) of the neuron networks. Each network is adopted and
trained from scratch for 50 epochs and achieves the same level of ASR (99.48 ± 0.5%). We obtained the Error
Gap using the original poisoning trigger (Trojan WM) after the model was defended by I-BAU and reported the
results from an average of 5 runs.
≤
≤
2


21
Published as a conference paper at ICLR 2022
Table 7 shows the empirical results of adopting different models with different widths. All the
poisoned models are poisoned with Trojan WM attack using a poison rate of 20%. We sorted the
models according to their maximum width (W) in Table 7. The Error Gap is obtained as the absolute
value of the test error subtracted by the training error after conducting the defense. Based on the
observation, the error gap is smaller as the model width grows, indicating better generalizability.
Aligning with Theorem 3, the generalizability has a positive correlation with W .
# Clean Samples	Average Error Gap (%)
500	1.67
2500	0.71
5000	0.35	-
Table 8: Empirical Error Gap with different numbers of available clean samples. We adopted the poisoned
ResNet-18 from Table 7 for this experiment. We obtained the Error Gaps using I-BAU defended models with
different clean samples and reported the results from the average of 5 runs.
Table 8 shows the empirical results of adopting different numbers of clean samples during the I-
BAU. As indicated from the results, a larger number of clean samples would lead to a smaller value
of the Error Gap, which indicates a better generalization of unlearning effect from training to unseen
data. Such results aligned with Theorem 3.
A.3 Implementation Details and Complexity Analysis
I-BAU does not need to compute the second-order derivative directly. Instead, it is computed via
implementing an approximation of the response Jacobian via an iterative solver (e.g., conjugated
gradient algorithm (Rajeswaran et al., 2019) or fixed-point algorithm (Grazzi et al., 2020)) in limited
rounds along with the reverse mode of automatic differentiation (Baur & Strassen, 1983; Griewank
& Walther, 2008) by treating the problem as a linear system. Automatic differentiation in reverse
mode is a widely used technique in modern deep learning packages such as Tensorflow and PyTorch
(Baydin et al., 2018). This section gave the ablation study over the norm bound given in Algorithm
1 and the memory and time complexity analysis and comparisons.
A.3.1 ABLATION STUDY ON THE l2 NORM BOUND
This section studies the impact of the preset l2 bound’s influence in the I-BAU unlearning scheme.
We tested five different bounds to illustrate the effects, i.e., 0.5, 5, 10, 20, and Best Efforts, as shown
in Figure 2. The settings of Best Efforts norm bound is that we do not include a norm constrained
of the synthesized trigger as long as the trigger’s value is within the image value range (from 0 to 1
in our case with float type images).
Figure 2: Evaluation of different l2 norm bound would impact the ACC/ASR on mitigating Trojan WM attack
on the GTSRB dataset. Each of the results listed is averaged from 5 independent runs using different random
seeds.
The l2 norm of launching the Trojan WM attack is 8.739 (measured by comparing with a zero matrix
of the same size). As shown in Figure 2 a larger norm bound leads to a more robust and accurate
synthesis of the potential trigger on the GTSRB. Especially the l2 norm bounds that are greater than
22
Published as a conference paper at ICLR 2022
the attack trigger’s l2 bound would lead to an effective defense in terms of low ASR and low impacts
over the clean ACC. Based on Figure 2, we find that a large norm bound does not significantly impact
over the clean ACC, namely the tread-off between the l2 bound and the ACC drop is not substantial.
In practice, when adopting I-BAU for backdoor defense, one is encouraged to adopt a large norm
bound, thus encompassing more potential attacks.
A.3.2 Memory complexity analysis
Following Griewank (1993), we assume
that the space complexity of computing
Vδ(θ) = -M2H (δ(θ),θ))-1 V2,2H (δ(θ),θ)
via automatic differentiation is no more
than twice the memory used when com-
puting VH (δ, θ), which making our space
complexity as M em(VH (δ, θ)). Recalling
another popular class of methods to solve
bilevel optimization—explicit gradient methods
(Grazzi et al., 2020), whose memory complexity
is Mem(K ∙ T ∙ VH(δ,θ)) as they need to
save the full computational graph during back-
propagation, where K is the number of rounds
for adversarial unlearning, T is the number
of computations for the inner. In comparison,
I-BAU is more memory efficient by adopting
the implicit gradient via the iterative solver to
approximate the computational graph without
saving the whole graph.
Input (32 × 32 × 3)
-	Conv2d 3 × 3(32 × 32 × 32)-
Conv2d 3 × 3(32 × 32 × 32)
Max-Pooling 2 × 2(16 × 16 × 32)
-	Dropout (0.3) (16 × 16 × 32)-
-	Conv2d 3 × 3(16 × 16 × 64)-
-	Conv2d 3 × 3(16 × 16 × 64)-
Max-Pooling 2 × 2 (8 × 8 × 64)
Dropout (0.4) (16 × 16 × 32)
-	Conv2d 3 × 3(8 × 8 × 128)-
-	Conv2d 3 × 3(8 × 8 × 128)-
Max-Pooling 2 × 2 (4 × 4 × 128)
Dropout (0.4) (16 × 16 × 32)
Flatten (2048)
Dense (C)
Table 9: The target model details. The simplified
VGG model contains three simplified VGG blocks,
of which each contains two convolutional layers in
each block. Here, we report the size of each layer.
A.3.3 Time complexity analysis
Following our design of Algorithm 1 (total K rounds), assuming using the fixed-point algorithm
Grazzi et al. (2020) as the iterative solver with H iterations for line 7, Algorithm 1, the time com-
plexity would be O(K ∙ H ∙ O(θ)), where O(θ) is the time complexity of training a neuronal network,
θ, via backpropagation for one epoch on the clean images used for unlearning (for most of the ex-
periments, we used 5000 samples). In practice, we adopted H = 5, and for most of the one-target
attack cases, K = 1 is enough to provide effective defenses (ASRs drop to random guessing rate).
Below are some theoretical analyses and comparison of I-BAU with other state-of-art defenses listed
in Table 6 regarding the time complexities:
•	NC and TABOR require to go through all classes (C = 10 for the CIFAR-10 and C = 43
for the GTSRB), and each label requires a large number of steps (K1 steps) of optimization to
synthesize the trigger. Roughly their time complexity under the settings of limited iterations is
O(Kι ∙ C ∙O(θ)). In practice, Ki ∙ C is much larger than K ∙ H.
•	DI incorporated an additional GAN to synthesis the trigger, assuming training and implementing
the GAN is of the time complexity O(θGAN), thus making the total time complexity roughly
equals to O(max(O(θGAN), O(θ))). In practice, the overhead of training a GAN trigger in-
spector is much expensive (estimated 300× longer GPU time on the CIFAR-10) than training
θ.
•	FP mitigates backdoor attacks via multi rounds (K2 rounds) of pruning the network; in practice,
FP requires more than 100 rounds of pruning (used half the number of samples for pruning, and
the rest is for fine-tuning) to meet the stop requirements.
•	NAD’s time complexity is proportional to the number of epochs used to fine-tune the student and
teacher models. As those two models share the same structure, we assume the time complexity
of training them over the unlearning dataset is O(θ). Assuming teacher model training phase
takes K3 epochs, and tuning student model based on the teacher model takes K4 epochs, then
the total time complexity is O((K3 + 2 × K4)O(θ)). In practice, we adopted K3 = K4 = 20
according to the original work.
23
Published as a conference paper at ICLR 2022
In conclusion, we find that theoretically, I-BAU is more efficient than other state-of-art defenses,
and the theoretical results are aligned with the empirical observations over the average time taken
effect over one-target attacks (see Table 6).
A.4 Experimental Detailed Settings
The details of the simplified VGG model adopted in our paper are explained in Table 9. For each
convolutional layer, we used batch normalization, and ELU is adopted as the activation function for
each. We use Adam with a learning rate of 0.05 as the optimizer for poisoned models. The models
are trained with 50 epochs over each poisoned dataset to converge and attain the results shown in
the main text. Our experiment adopted ten NVIDIA TITAN Xp GPUs as the computing units with
four servers equipped with AMD Ryzen Threadripper 1920X 12-Core Processors. Interestingly,
the same experiments showed slower convergence (it takes more rounds to mitigate the backdoors)
using GTX TITAN X and GTX 2080 TI. To reproduce the exact experimental results, we suggest
considering adopting NVIDIA TITAN Xp GPUs for the experiments. PyTorch (Paszke et al., 2019)
is adopted as the deep learning framework for implementations. For the settings of implementing
the I-BAU, the inner and outer is conducted with iterative optimizers (SGD or Adam) with a learning
rate of 0.1.
A.4. 1 Attacks details
We list the examples of the adopted backdoor attacks in this section. We incorporated eleven dif-
ferent backdoor attacks in this work. Figure 3 shows the examples from CIFAR-10 and the GTSRB
before and after patched with different backdoor triggers. We adopted the same target label on the
two datasets under one-trigger-one-target settings, as listed in Figure 3: BadNets white square trig-
ger targeting at label 8 (BadNets) (Gu et al., 2017), Hello Kitty blending trigger targeting at label
1 (Blend) (Chen et al., 2017), `0 norm constraint invisible trigger targeting at 0 (`0 inv) (Li et al.,
2020a), `2 norm constraint invisible trigger targeting at 0 (`2 inv) (Li et al., 2020a), Smooth trigger
(frequency invisble trigger) targeting at 6 (Smooth) (Zeng et al., 2021), Trojan square targeting at
2 (Troj SQ) (Liu et al., 2018b), Trojan watermark targeting at 2 (Troj WM) (Liu et al., 2018b). For
both CIFAR-10 and the GTSRB poisoned models, we train the models with the entire training set
(50000 samples for CIFAR-10, 39209 for the GTSRB) with the fixed 20% poison rate across all the
experiments. For unlearning, the available clean data is sampled from both datasets’ test set with a
fixed size of 5000, where the remaining data (5000 for the CIAFR-10 and 7630 for the GTSRB) will
be used to evaluate the unlearning efficacy (ACC and ASR).
Figure 3: Datasets and examples of backdoor attacks that considered in the main text. We consider two different
datasets in this work, namely, the CIFAR-10 dataset and the GTSRB dataset. Nine different backdoor attack
triggers are included in the experimental part with one-trigger or multi-trigger attack patterns. Above, we show
the target label used during the one-trigger attacks (e.g., badnets targeting at label 8) of each backdoor attack.
A.4.2 Baseline defenses details
We compared I-BAU with six state-of-art backdoor unlearning defenses: Neural Cleanse
(NC) (Wang et al., 2019), Deepinspect (DI) (Chen et al., 2019), TABOR (Guo et al., 2019), Fine-
pruning (FP) (Liu et al., 2018a), Neural Attention Distillation (NAD) (Li et al., 2020b), and Differ-
ential Privacy training as a general robustness defense (DP) (Du et al., 2019). The detailed settings
of comparison are provided as follows:
24
Published as a conference paper at ICLR 2022
•	NC is conducted following the same settings as the original work but only use the same 5000
samples as ours; for the outlier detection, we marked and unlearned all the detected triggers
(Median Absolute Deviation (MAD) based on the generated trigger’s norm, marked all the triggers
whose mask MAD loss larger than 2 as detected).
•	DI adopted model inversion technique for agnostic to the clean samples, yet made the defense’s
efficacy highly depend on the inversion technique. For a fair comparison, we feed the same 5000
samples, which are available to the other methods, to the GAN synthesizer in DI and obtains the
final results; for the outlier detection, we marked and unlearned all the detected triggers (MAD
based on the average loss for generating a trigger, marked all the triggers whose MAD loss larger
than 2 as detected).
•	TABOR’s settings follow the original work but with only 5000 clean samples being provided.
(MAD based on the norm computation proposed in the original work to get rid of false alarms,
marked all the triggers whose MAD loss larger than 2 as detected.)
•	FP follows the suggestions of the original work, where we prune the network by supervising the
ACC to drop to a certain percentage, i.e., 20%. This part’s ACC is done by using 1000 samples
from the 5000, and the rest 4000 clean samples are used to fine-tune the model to recover the
ACC.
•	NAD’s implementation follows the exact settings as the original work. The original work did
not emphasize much over the preprocessing, and we used the same preprocessing following their
open-sourced codes 1.
•	DP follows the settings in the work that first mentioned use DP as a backdoor defense (Du et al.,
2019), where we tuned the noise multiplier to attain universal effectiveness across all the consid-
ered attacks(50 for the CIFAR-10, 1.5 for the GTSRB).
A.5 Case S tudy on Non-additive Backdoor Attacks
As our fundamental formulation takes backdoor triggers as additive noise, in this specific section,
we would like to evaluate the effectiveness of I-BAU towards non-additive backdoor attacks empir-
ically. We selected four unique backdoor attacks/ settings to evaluate I-BAU towards non-additive
attacks, which will be introduced as follows. 1) Semantical replacement (SR), which directly re-
places the poison image with a piece of different semantical information. We designed this attack
by directly changing all the poisoned images to an out-of-distributed ‘Hallo Kitty’ image with the
poisoned label. SR should be considered as one of the worst-case attack scenario in practice, as its
trigger is additional semantical information with a large norm bound; 2) WaNet (Nguyen & Tran,
2021) adopts a universal wrapping augmentation as the backdoor trigger. Under such a case, the
backdoor trigger becomes a specific augmentation technique but not direct information insertion or
addition. And WaNet has shown its ability to bypass some existing defense methods; 3) IAB attack
(Nguyen & Tran, 2020) adopts autoencoder to learn and assign sample-specific noise to inputs to
launch sample-specific backdoor attacks; 4) Hidden trigger (HT) attack (Saha et al., 2020) adopts
a unique poisoning procedure using projected gradient descent to compute adversarial noise, which
we consider as another example of a non-additive attack. We evaluate the effectiveness of I-BAU
against the above four non-additive attacks on the CIFAR-10 dataset. We will illustrate their specific
settings and results in the following parts of this section.
A.5.1 Towards mitigating SR
We first evaluate an extreme case where we replace the poisoned CIFAR-10 images with an out-
of-domain ’Hallo Kitty’ image. Such a procedure directly changes the semantic information of the
poisoned data. We set the target label as ’0’. Like the other evaluated attack, we replaced 20%
of the non-target-class samples with the trigger kitty and set the label as ’0’. As for launching the
attack during test time, we evaluate the same ’Hallo Kitty’ image exposed to the poisoned model and
measure the attack success rate (in this case it becomes either 100% or 0%). The target model here
adopted the small VGG16 introduced in our experiment. As for I-BAU, we adopted Adam optimizer
and a learning rate of 0.1 and an unlimited l2 norm bound (instead, we crop the perturbation’s value
and restricted it to 0-1 as discussed in Appendix A.3.1). The results before and after are listed below
in Table 10.
1https://github.com/bboylyg/NAD
25
Published as a conference paper at ICLR 2022
Clean ACC - Before	ASR - Before	Clean ACC - After	ASR - After
83.82%	100.00% —	82.12%	0.00% —
Table 10: Empirical evaluation on I-BAU’s effectiveness towards semantical replacement as backdoor attack.
As demonstrated in Table 10, within three rounds of I-BAU, we obtained a clean model with an
acceptable ACC drop compared to the baseline. Interestingly, the extreme case directly inserts an
additional semantical link between the poison kitty and the class ’0’, which can be interpreted as
direct exposure of a specific training sample to the test set and interfere with the model general-
izability (aka, overfit to a rare feature). Surprisingly, I-BAU can effectively mitigate such attack
patterns. To our best knowledge, the above attack setting is never considered before. I-BAU also
shows a potential path towards resolving model overfitting issues. We will leave such discussion to
future work.
A.5.2 Towards mitigating WaNet
WaNet is one of the famous invisible attack, instead of adopting an additive trigger, it adopts the
same elastic transformation as the trigger of the attack. We directly downloaded the pre-trained
poisoned PreActResNet18 on the CIFAR-10 from their work as the poison model to be adversarially
unlearnt 2. As for I-BAU, we adopted Adam optimizer and a learning rate of 0.0001 and an unlimited
l2 norm bound. The results before and after are listed in Table 11, which indicates an effective
defense with acceptable influence in the ACC.
Clean ACC - Before	ASR - Before	Clean ACC - After	ASR - After
94.36%	99.62% —	92.86%	10.80% —
Table 11: Empirical evaluation on I-BAU’s effectiveness towards WaNet.
A.5.3 Towards mitigating the IAB attack
An emerging line of attack focuses on sample-specific attacks; here, we evaluate against IAB attack,
which utilizes an autoencoder to learn and insert sample-specific triggers. We followed the same
implementation as provided in the original work 3 , with the following specific settings: dataset:
CIFAR-10; target label:0; ρb = ρc = 0.1; model: PreActResNet18. One difference is that we
loaded the CIFAR-10 dataset in a customized dataset format instead of the default (i.e., we used
“torch.Tensor” loaded from “NumPy.array” with range [0, 1], instead of “torch.Tensor” loaded from
“PILimages” with range [-1.99, 2.13]). We did this to facilitate the implementation of I-BAU
(which targeting at noises ranging from [0, 1] for the current implementation). The results prior
to and during the intervention are summarized in Table 12, indicating an effective defense with a
low influence in the ACC.
Clean ACC - Before	ASR - Before	Clean ACC - After	ASR - After
87.28%	99.20% —	86.46%	9.58% —
Table 12: Empirical evaluation on I-BAU’s effectiveness towards the IAB attack.
A.5.4 Towards mitigating HT
Finally, we evaluated the effectiveness of Hidden trigger (HT) backdoors (non-additive during poi-
soning), whose trigger inserting process is by directly resolving adversarial noise generated via
projected gradient descent. We evaluated I-BAU with the CIFAR-10 random pairs attack settings
from the original work (trigger_ 10, target: 8, source: 5, number of samples to generate PGD noise:
1500, number of poison in target class: 800, = 16, optimization for generating poison: 0.01 with
a decay rate of 0.95 every 2000 iterations). However, we found the original settings suffer from
limitations in targeted ASR in our experiment, which is only “18.30%” (i.e., only drops the ACC
after patching the trigger but have a relatively low chance leading to the target label). To enforce
2https://github.com/VinAIResearch/Warping-based_Backdoor_Attack-release
3 https://github.com/VinAIResearch/input- aware- backdoor- attack- release
26
Published as a conference paper at ICLR 2022
a successful targeted attack, we enlarged = 50. During the fine-tuning process of [1], we only
fine-tuned the clean model (ACC:“84.60”) over the poison data, which resulted in a poisoned model
with an ACC/ASR of “73.41/89.00”. After adopting I-BAU for 20 rounds, the poisoned model’s
performance becomes “84.58/11.10”, and with larger rounds (90 rounds), the performance can fur-
ther be improved to “84.06/0.23”, which indicates a robust and effective defense and an extra effect
on recovering ACC.
A.5.5 Highlights on the case studies towards non-additive attacks
The above results from the case study on using I-BAU to mitigate non-additive attacks highlight that
although our formulation targets a universal pattern that most misled misclassifications in an additive
way, I-BAU is empirically effective towards mitigating non-additive attacks. These emperical results
have a great chance to lead to some exciting future works on theoretical analysis of the effectiveness
of our proposed minimax formulation. We will open-source all incorporated attacks (at the moment,
eleven attacks are incorporated, seven in the main text, and four non-additive case studies in the
Appendix) and the pre-trained poisoned models. We will constantly check out the emerging attacks
and look forward to seeing the first attack can evade our defense!
Figure 4: TSNE analysis of the output features (flatten layer output of the NN, with a size of (N × 2048))
before and after backdoor unlearning using CIFAR-10 BadNets (targeting at 8) poisoned model. Each color
in the color bar represents a different class from (0, . . . , 9). We mark the sample with triggers and without
triggers as listed in the Figure.
A.6 TSNE Analysis on Unlearning Effects
The TSNE analysis of the feature extracted by the poisoned model and the unlearned model is shown
in Figure 4. The model considered here is a BadNets poisoned model on the CIFAR-10 dataset
(target label is 8), and the results listed in Figure 4 are before and after one round of I-BAU. Before
the I-BAU, the poison model’s extracted features for samples with/without triggers are disparate,
even for samples originating from the same class. After the I-BAU, we can see that the unlearned
model will map the samples patched with triggers back to their original classes (same color). Such
results demonstrate the effectiveness of the backdoor unlearning from another perspective.
A.7 Iterative Illustration on Multi-target Cases
We show the iterative records of I-BAU mitigating more complicated attack cases, i.e., the all-to-all
attacks and 7-trigger-7-target cases on the two evaluated datasets. We listed them here as they take
27
Published as a conference paper at ICLR 2022
more rounds than one-trigger-one-target cases, which can usually be mitigated in a one-shot-kill
manner.
Figure 5: I-BAU iterative records under mitigating BadNets in all-to-all attack cases. (a). the results on the
CIFAR-10 where took more rounds for the I-BAU to mitigate the attack successfully. (b). the results on the
GTSRB manage to mitigate the triggers with fewer rounds and less impact over the ACC.
Figure 5 shows the results on countering the BadNets all-to-all attacks, where (a) is the results on
the CIAFR-10 dataset, and (b) is the results on the GTSRB dataset. As shown in Figure 5 (a),
although it takes more rounds than one-trigger-one-target cases to mitigate the attack, thanks to the
accurate computing of the hyper gradient, the I-BAU did not impact much over the ACC. When the
ASR drops below 10%, the unlearned model can still maintain an ACC above 80% (original ACC:
86.38). Meanwhile, in Figure 5 (b), we see that we can unlearn the BadNets trigger under all-to-all
cases with even fewer rounds of I-BAU, and the unlearned model can maintain an ACC of around
99% during the entire unlearning procedure.
Figure 6 demonstrates the mitigation records of each iteration of I-BAU over the 7-trigger-7-target
cases over the two evaluated datasets. Figure 6 (a) draws the details on the CIFAR-10 dataset, which
took more than 200 rounds of I-BAU to mitigate all seven attacks. On the GTSRB, the mitigation
of all seven triggers takes less time. And the model can maintain an ACC close to 99% during the
entire unlearning procedure.
Figure 6: I-BAU iterative records under mitigating 7-trigger-7-target attack cases. (a). the results on the CIFAR-
10 where took more rounds for the I-BAU to mitigate the attacks successfully. (b). the results on the GTSRB
manage to mitigate the triggers with fewer rounds and less impact over the ACC.
Upon observation, there are triggers easier to be found by the I-BAU, e.g., Trojan WM and Trojan
SQ, as they are optimized triggers, and thus easier to be found by the I-BAU. Such interesting
observation might lead to new logic to consider while designing backdoor triggers (more optimized
triggers might be easier to remove, as demonstrated).
28