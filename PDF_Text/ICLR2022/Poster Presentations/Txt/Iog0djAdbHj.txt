Published as a conference paper at ICLR 2022
Better Supervisory Signals
by Observing Learning Paths
Yi Ren
UBC
renyi.joshua@gmail.com
Shangmin Guo
University of Edinburgh
s.guo@ed.ac.uk
Danica J. Sutherland
UBC and Amii
dsuth@cs.ubc.ca
Ab stract
Better-supervised models might have better performance. In this paper, we first
clarify what makes for good supervision for a classification problem, and then
explain two existing label refining methods, label smoothing and knowledge dis-
tillation, in terms of our proposed criterion. To further answer why and how bet-
ter supervision emerges, we observe the learning path, i.e., the trajectory of the
model’s predictions during training, for each training sample. We find that the
model can spontaneously refine “bad” labels through a “zig-zag” learning path,
which occurs on both toy and real datasets. Observing the learning path not only
provides a new perspective for understanding knowledge distillation, overfitting,
and learning dynamics, but also reveals that the supervisory signal of a teacher
network can be very unstable near the best points in training on real tasks. In-
spired by this, we propose a new knowledge distillation scheme, Filter-KD, which
improves downstream classification performance in various settings.
1	Introduction
In multi-class classification problems, we usually supervise our model with “one-hot” labels: label
vectors y which have yi = 1 for one i, and 0 for all other dimensions. Over time, however, it
has gradually become clear that this “default” setup is not always the best choice in practice, in
that other schemes can yield better performance on held-out test sets. One such alternative is to
summarize a distribution of human annotations, as Peterson et al. (2019) did for CIFAR10. An
alternative approach is label smoothing (e.g. Szegedy et al., 2016), mixing between a one-hot label
and the uniform distribution. Knowledge distillation (KD), first training a teacher network on the
training set and then a student network on the teacher’s output probabilities, was originally proposed
for model compression (Hinton et al., 2015) but can also be thought of as refining the supervision
signal: it provides “soft” teacher outputs rather than hard labels to the student.
Knowledge distillation is promising because it requires no additional annotation effort, but - unlike
label smoothing - can still provide sample-specific refinement. Perhaps surprisingly, knowledge
distillation can improve student performance even when the teacher is of exactly the same form as
the student and trained on the same data; this is known as self-distillation (Furlanello et al., 2018;
Zhang et al., 2019). There have been many recent attempts to explain knowledge distillation and
specifically self-distillation (e.g. Menon et al., 2021; Allen-Zhu & Li, 2020; Tang et al., 2020), from
both optimization and supervision perspectives. We focus on the latter area, where it is usually
claimed that the teacher provides useful “dark knowledge” to the student through its labels.
Inspired by this line of work, we further explore why and how this improved supervisory signal
emerges during the teacher’s one-hot training. Specifically, we first clarify that given any input
sample x, good supervision signals should be close (in L2 distance) to the ground truth categorical
distribution, i.e., p* (y | x). We then show that a neural network (NN) can automatically refine “bad
labels”, those where p* (y | x) is far from the training set,s one-hot vector.1 During one-hot training,
the model prediction on such a sample first moves towards p* (y | x), and then slowly converges to
its supervisory label, following a “zig-zag” pattern. A well-trained teacher, one that does not overfit
1This might be because X is ambiguous (perhaps p* (y | x) is flat, or We simply got a sample from a less-
likely class), or because the one-hot label has been corrupted through label noise or otherwise is “wrong.”
1
Published as a conference paper at ICLR 2022
to particular training labels, can thus provide supervisory signals closer to p* (y | x). We justify
analytically (Section 3.3) that this pattern is common in gradient descent training. Our explanations
cause us to recognize that this signal can be better identified by taking a moving average of the
teacher’s prediction, an algorithm we term Filter-KD. This approach yields better supervision and
hence better downstream performance, especially when there are many bad labels.
After completing this work, we became aware of an earlier paper (Liu et al., 2020) studying almost
the same problem in a similar way. We discuss differences between the papers throughout.
2	Supervision influences generalization
We begin by clarifying how the choice of supervisory signal affects the learned model.
2.1	Choices of supervision signal
In K-way classification, our goal is to learn a mapping f : X → ∆K that can minimize the risk
R(f) , E [L(y, f(x))] = p(x) p(y |x) L(y, f (x)) dx dy,	(1)
(x,y)〜P
Here the label y is an integer ranging from 1 to K, and f gives predictions in the probability simplex
∆K (a nonnegative vector of length K summing to one); L(y, f (x)) is the loss function, e.g. cross-
entropy or square loss. The input signal xis usually high dimensional, e.g., an image or a sequence of
word embeddings. The joint distribution of (x, y) is P, whose density2 can be written as p(x)p(y|x).
In practice, as P is unknown, we instead (approximately) minimize the empirical risk
NK	N
Remp (f, D) , XX N Hn= = k) L(k,f (Xn)) = X NN e[L(f(Xn)),	(2)
n=1 k=1	n=1
where l(yn = k) is an indicator function which equals 1 if y= = k or 0 otherwise, and eyn ∈
{0, 1}K is its one-hot vector form. L(f (xn )) = (L(1, f(xn )), . . . , L(K, f(xn ))) ∈ RK is the loss
for each possible label. In Remp, the N training pairs D , {(xn , yn )}nN=1 are sampled i.i.d. from P.
Comparing (2) to (1), we can see p(x) is approximated by an uniform distribution over the samples,
which is reasonable. However, using an indicator function (i.e., one-hot distribution) to approximate
p(y | x) bears more consideration. For example, if a data point x is quite vague and its true p(y|x)
is flat or multimodal, we might hope to see x multiple times with different label y during training.
But actually, most datasets have only one copy of each x, so we only ever see one corresponding ey.
Although Remp is an unbiased estimator for R, if we used a better (e.g. lower-variance) estimate of
p(y | x), we could get a better estimate for R and thus, hopefully, better generalization.
Specifically, suppose we were provided a “target” distribution ptar(y | x) (written in vector form as
ptar(x)) for each training point x, as D0 = {(xn , ptar(xn ))}nN=1. Then we could use
NK	N
Rtar(f, D0)，XX N Ptar(y= = k | Xn) L(k,f (x= )) = X N Ptor(Xn)TL(f(x=)).	(3)
Standard training with Remp is a special case of Rtar, using ptar(xn ) = eyn. The CIFAR10H dataset
(Peterson et al., 2019) is one attempt at a different Ptar, using multiple human annotators to estimate
Ptar. Label smoothing (e.g. Szegedy et al., 2016) sets Ptar to a convex combination of ey and the
constant vector 玄 L In knowledge distillation (KD; Hinton et al., 2015), a teacher is first trained on
D, then a student learns from D0 with Ptar based on the teacher’s outputs. All three approaches yield
improvements over standard training with Remp.
2Because we are working with classification problems, we use densities with respect to a product of some
arbitrary measure on X (probably Lebesgue) with counting measure on y, and assume that these densities exist
for notational convenience. None of our arguments will depend on the choice of base measure.
2
Published as a conference paper at ICLR 2022
-60	-40	-20	0	20	40 CO
(a) tSNE of toy dataset.
(b)ACC VS ∣∣ptar - p*∣∣2
OXS
OM
003
oæ
OΛ1
oxn .	.	.	.	.
O	10	20	30	40	50
L2-dIstance of p_tar and p*
(C)ECEV.S. ∣Ptar — p*∣∣2
Figure 1: ExperimentS on a toy dataSet when learning from different ptar. In (b-c), the horizontal
axis represents |巾面一p*∣∣2, and the vertical axis is the generalization performance.OHT means
one-hot training (on Remp), LS means label smoothing, GT means ground truth training with p*, KD
is knowledge distillation, and ESKD is early-stopped KD. The Spearman correlation coefficient for
results in (b) is -0.930 with p-Value 1.9 × 10-53; for (c) is 0.895 with p-Value 2.7 × 10-43.
2.2	Measuring the Quality of Supervision
Choosing a different ptar, then, can lead to a better final model. Can we characterize which ptar will
do well? We propose the following, as a general trend.
Hypothesis 1. Suppose we train a model supervised by ptar, that is, we minimize Rtar(f, D0). Then,
smaller average L? distance between P$仃 and the ground truth p* on these samples, i.e. small
EX [kP tar (x) — P*(x)k2], will in general lead to better generalization performance.
This hypothesis is suggested by Proposition 3 of Menon et al. (2021), which shows (tracking con-
stants omitted in their proof) that for any predictor f and loss bounded as L(y, y) ≤ ',
E0 [(Rtar(f, D0) — R(f))2] ≤ N Var [pTrL(f (x))] + '2K (EkPtar(X)-P*(x)||2)2.	(4)
D0	N x	x
When N is large, the second term will dominate the right-hand side, implying smaller aVerage
kptar - p* k will lead to Rtar being a better approximation of the true risk R; minimizing it should
then lead to a better learned model. This suggests that the quality of the superVision signal can be
roughly measured by its L2 distance to the ground truth p*. Appendix B slightly generalizes the
result of Menon et al. with bounds based on total Variation (L1) and KL diVergences; we focus on
the L2 Version here for simplicity.
To further support this hypothesis, we conduct experiments on a synthetic Gaussian problem (Fig-
ure 1 (a); details in Appendix C), where we can easily calculate p* (y | x) for each sample. We
first generate seVeral different ptar by adding noise3 to the ground truth p*, then train simple 3-layer
NNs under that superVision. We also show fiVe baselines: one-hot training (OHT), label smoothing
(LS), KD, early-stopped KD (ESKD), and ground truth (GT) superVision (using p*). KD refers to a
teacher trained to conVergence, while ESKD uses a teacher stopped early based on Validation accu-
racy. We early-stop the student’s training in all settings. From Figure 1 (b-c), it is clear that smaller
kptar - p* k2 leads to better generalization performance, as measured either by accuracy (ACC) or
expected calibration error (ECE)4 on a held-out test set. Appendix C has more detailed results.
3	Insights from the learning path
In the toy example of Section 2, we see that ESKD outperforms other baselines in accuracy by
a substantial margin (and all baselines are roughly tied in ECE). We expect that superVision with
smaller kptar - p* k2 leads to better generalization performance, but it is not clear how better ptar
emerges from when the teacher in ESKD is trained using one-hot labels. This section will answer
this, by obserVing the learning paths of training samples.
3Whenever We mention adding noise to p* ,we mean We add independent noise to each dimension, and then
re-normalize it to be a distribution. Large noise can thus flip the “correct” label.
4ECE measures the calibration of a model (Guo et al., 2017). Briefly, lower ECE means the model’s confi-
dence in its predictions is more accurate. See Appendix A for details.
3
Published as a conference paper at ICLR 2022
Figure 2: Normalized (divided by ∖∕2) distance between output distribution q and p* during the
one-hot training in different stages (left to right: initial, early stop, convergence). In these figures,
Ex kq(x) - p* (x)k2 of Hypothesis 1 is the mean height of all points in the figure. We provide more
results about the NNs trained under different supervisions using this fashion in Appendix F.
3.1	Pay more attention to harder samples
For a finer-grained understanding of early stopping the teacher, we would like to better under-
stand how the teacher’s predictions evolve in training. Assuming Hypothesis 1, the main factor
is Exkq(x) - p*(x)k2, where q is the teacher’s output probability distribution. We expect, though,
that this term will vary for different x, in part because some samples are simply more difficult to
learn. As a proxy for this, we define base difficulty as key - p*(x)k2, which is large if:
•	x is ambiguous: p* has several large components, so there is no one-hot label near p* .
•	x is not very ambiguous (there is a one-hot label near p*), but the sample was “unlucky” and
drew y from a low-probability class.
Figure 2 shows these two quantities at three points in training: initialization, the point where ESKD’s
teacher stops, and convergence. At initialization, most points5 have large kq(x) - p*(x)k2. By the
point of early stopping, most q(x) values are roughly near p*. At convergence, however, q(x) ≈ ey,
as the classifier has nearly memorized the training inputs, leading to a diagonal line in the plot.
It is clear the biggest contributors to Exkq(x) - p*(x)k2 when training a model to convergence are
the points with high base difficulty. Per Hypothesis 1, these are the points we should most focus on.
3.2	Learning Path of Different Samples
To better understand how q changes for each sample, we track the model’s outputs on all training
samples at each training step. Figure 3 shows four samples with different base difficulty, with the
vectors of three probabilities plotted as points on the simplex (details in Appendix A).
The two easy samples very quickly move to the correct location near ey (as indicated by the light
color until reaching the corner). The medium sample takes a less direct route, drifting off slightly to-
wards p*, but still directly approaches ey. The hard sample, however, does something very different:
it first approaches p*, but then veers off towards ey, giving a “zig-zag” path. In both the medium
and hard cases, there seems to be some “unknown force” dragging the learning path towards p* ; in
both cases, the early stopping point is not quite perfect, but is noticeably closer to p* than the final
converged point near ey . In other words, during one-hot training, the NNs can spontaneously refine
the “bad labels.” Under Hypothesis 1, this partly explains the superior performance of ESKD to
KD with a converged teacher.6
These four points are quite representative of all samples under different toy-dataset settings. In
Appendix E, we also define a “zig-zagness score” to numerically summarize the learning path shape,
and show it is closely correlated to base difficulty.
5The curve structure is expected: points with p* ≈ (1, 1, 3) are near the middle of the base difficulty
range, and all points are initialized with fairly ambiguous predictions q.
6KD’s practical success is also related to the temperature and optimization effects, among others; better
supervision is not the whole story. We discuss the effect of various hyperparameters in Appendix D.
4
Published as a conference paper at ICLR 2022
Figure 3: Learning path of samples with different base difficulty. Corners correspond to one-hot
vectors. Colors represent training time: transparent at initialization, dark blue at the end of training.
3.3	Explanation of patterns in the learning path
The following decomposition Win help explain the “unknown force” pushing q towards p*.
Proposition 1. Let zt (x) , f (wt, x) denote the network output logits with parameters wt, and
qt(x) = SoftmaX(Zt(x)) the probabilities. Let wt+1，Wt - η Vw (Ptar(Xu)tL(qt(Xu))) be the
result of applying one step of SGD to wt using the data point (xu , ptar(xu)) with learning rate η.
Then the change in network predictions for a particular sample Xo is
qt+1(	) -	qt(	) = ηAt(	) Kt(	,xu)	(ptar(Xu)	-	qt(Xu》+ O(n2kVWZ(Xu)k2p),
where At(Xo) = VZqt(Xo) and Kt(Xo, Xu) = (Vw Z(Xo)|wt) (Vw Z(Xu)|wt)T are K × K matrices.
The matrix Kt is the empirical neural tangent kernel, which we can think of roughly as a notion of
“similarity” between Xo and Xu based on the network’s representation at time t, which can change
during training in potentially complex ways. In very wide networks, though, Kt is nearly invariant
throughout training and nearly independent of the initialization (Jacot et al., 2018; Arora et al.,
2019). The gradient norm can be controlled by gradient clipping, or bounded with standard SGD
analyses when optimization doesn’t diverge. Appendix G has more details and the proof.
Figure 4 shows the learning path of a hard sample during one-hot training, say Xo, where the label
eyo is far from p* ( ). In each epoch, W will receive N — 1 updates based on the loss of Xu = Xo
(the small blue path), and one update based on Xo (the big red path). At any time t, “dissimilar”
samples will have small Kt (Xo, Xu) (as measured, e.g., by its trace), and hence only slightly affect
the predicted label for Xo. Similar samples will have large Kt (Xo, Xu), and hence affect its updates
much more; because p* is hopefully similar for similar X values, it is reasonable to expect that the
mean of ey for data points with similar X will be close to p*(Xo). Thus the net effect of updates for
Xu 6= Xo should be to drag q(Xo) towards p*(Xo). This is the “unknown force” we observed earlier.
The other force affecting q(Xo) during training
is, of course, the update based on Xo , driving
q(Xo) towards eyo . How do these two forces
balance over the course training? Early on,
keyu - qt (Xu)k2 is relatively large for nearly
any Xu, because near initialization qt(Xu) will
be relatively flat. Hence the size of the updates
for “similar” Xu and the update for Xo should
be comparable, meaning that, if there are at
least a few “similar” training points, qt (Xo)
will move towards p* (Xo). Throughout train-
ing, some of these similar samples will become
well-classified, so that keyu - qt(Xu)k2 be-
comes small, and their updates will no longer
exert much force on q(Xo). Thus, the Xo up-
dates begin to dominate, causing the zig-zag
pattern as the learning path turns towards eyo .
For easy samples, where p* and eyo are in the same direction, these forces agree and lead to fast
convergence. On samples like the “medium” point in Figure 3, the two forces broadly agree early
on, but the learning path deviates slightly towards p* en route to eyo .
Figure 4: Updates of q(Xo) over training.
5
Published as a conference paper at ICLR 2022
Figure 5: Learning path on CIFAR10. In the first two panels we record qt for each batch, while in
the last panel we record it for each epoch. See Appendix E for the learning paths of more samples.
Liu et al. (2020) prove that a similar pattern occurs while training a particular linear model on data
with some mislabeled points, and specifically that stopping training at an appropriate early point
will give better predictions than continuing training.
4	Learning paths on real tasks
Our analysis in Sections 2 and 3 demonstrate that the model can spontaneously refine the bad labels
during one-hot training: the zig-zag learning path first moves towards the unknown p*. But is this
also true on real data, for more complex network architectures? We will now show that they do,
although seeing the patterns requires a little more subtlety.
We visualize the learning path of data points while training a ResNet18 (He et al., 2016) on CIFAR10
for 200 epochs as an example. The first panel of Figure 5 shows the learning path of an easy sample.7
We can see that qt converges quickly towards the left corner, the one-hot distribution for the correct
class, because the color of the scattered points in that figure are quite light. At the early stopping
epoch, qt has already converged to ey . However, for a hard sample, it is very difficult to observe
any patterns from the raw path (blue points): there are points almost everywhere in this plot. This
is likely caused by the more complex network and dataset, as well as a high learning rate in early
training. To find the hidden pattern in this high-variance path, we treat qt as a time series signal with
many high-frequency components. Thus we can collect its low-frequency components via a low-
pass filter and then draw that, i.e., taking a exponential moving average (EMA) on qt (red points).
This makes it clear that, overall, the pattern zig-zags, first moving towards the unknown true label
before eventually turning to memorize the wrong label.
This filtering method not only helps us observe the zig-zag pattern, but can also refine the labels.
We use the following two experiments to verify this. First, we train the model on CIFAR10H using
one-hot labels. As CIFAR10H provides phum, which can be considered as an approximation of p*,
we track the mean distance between q and phum during training. In the first panel of Figure 6, which
averages the distance of all 10k training samples, the difference between the blue qkd curve and red
qfilt curve is quite small. However, we can still observe that the red curve is lower than the blue
one before overfitting: filtering can indeed refine the labels. This trend is more significant when
considering the most difficulty samples, as in the second panel: the gap between curves is larger.
To further verify the label refinement ability of filtering, we randomly choose 1,000 of the 50,000
CIFAR10 training samples, and flip their labels to a different y0 6= y. The last panel in Figure 6 tracks
how often the most likely prediction of the network, argmax qt, recovers the true original label,
rather than the provided random label, for the corrupted data points. At the beginning of the model’s
training, the initialized model randomly guesses labels, getting about 10% of the 1,000 flipped labels
correct. During training, the model spontaneously corrects some predictions, as the learning path
first moves towards p*. Eventually, though, the model memorizes all its training labels. Training
with the predictions from the 400th epoch corresponds to standard KD (no corrected points). Early
stopping as in ESKD would choose a point with around 70% of labels corrected. The filtered path,
though, performs best, with over 80% corrected.
7Without knowing p*, we instead use zig-zag score — see Appendix E — to measure the difficulty.
6
Published as a conference paper at ICLR 2022
AIl IOk SamPleS in CIFARloH
----Qkd tθ Phum
Qkd tθ θy
----Qfilt to Phum
―qfi∣t to ey
0 25 50 75 100 125 150 175
Epochs
0.0-
0
Most ambiguous Ik samples in CIFAR10H
1.0
飞Q:27x P ① Z=BE」ON
25 50 75 100 125 150 175 200
Epochs
IkWrOng Iabeled SamPleS in ClFARIO
Vooooooom
Vooooooo
U 7654321
XeEPetj20(JJO⅛
0 50 100 150 200 250 300 350 400
Epochs
Original Path
Filtered Path
Random Gue
KD/OHT
ESKD
FiIterKD
Figure 6: Filtering can refine the labels in both clean and noisy label case.
0.0	0.2	0.4	0.6	0.8
Noise ratio
0.0	0.2	0.4	0.6	0.8
Noise ratio
Cifarioo
Figure 7: Test accuracy under different noise ratio σ. Solid lines are the means while shade region
are the standard errors for 3 runs with different random seeds (shaded range is the standard error).
Last panel compare the influence of different temperatures. Each thin rectangle plot represents a
different σ = {0, 0.1, ..., 0.8}, in which we plot the results with different τ = {0.5, 1, 2, 4, 10}.
5	Filtering as a method for knowledge distillation
Figure 6 gives a clear motivation for a new knowledge distillation method, which we call Filter-KD
(Algorithm 1): train the student from the smoothed predictions of the teacher network. Specifically,
we maintain a look-up table qsmooth ∈ RN ×K to store a moving average of qt for each training
sample. Note that in one epoch, each qsmooth(xn) will be updated only once. We check the early
stopping criterion with the help of a validation set. Afterwards, the teaching supervision qsmooth is
ready, and we can train a student network under its supervision. This corresponds to using a moving
average of the teacher model “in function space,” i.e. averaging the outputs of the function over time.
Compared to ESKD, Filter-KD can avoid the extremely high variance of qt during training. Unlike
Figure 5, which plots qt after every iteration of training, most practical implementations only con-
sider early-stopping at the end of each epoch. This is equivalent to down-sampling the noisy learning
path (as in the last panel of Figure 5), further exacerbating the variance of qt . Thus ESKD will likely
select a bad ptar for many data points. Filter-KD, by contrast, has much more stable predictions.
We will show the effectiveness of this algorithm shortly, but first we discuss its limitations. Com-
pared to ESKD, the running time of Filter-KD might be slightly increased. Furthermore, compared
to the teaching model in ESKD, the Filter-KD requires a teaching table qsmooth, which will require
substantial memory when the dataset is large. One alternative avoiding the need for this table would
be to instead take an average “in parameter space,” like e.g. momentum parameter updating as in
Szegedy et al. (2015). We empirically find that, although this helps the model converge faster, it does
not lead to a better teacher network; see Appendix I. Thus, although Filter-KD has clear drawbacks,
we hope that our explanations here may lead to better practical algorithms in the future.
Similarly inspired by this spontaneous label refining mechanism (or early stopping regularization),
Liu et al. (2020) and Huang et al. (2020) each propose algorithms aiming at the noisy label problem.
We discuss the relationship between these three methods in Appendix H.
5.1	Quantitative results of Filter-KD
We now compare the performance of Filter-KD and other baselines on a real dataset. We focus on
self-distillation and a fixed temperature τ = 1 (except Table 2 and last panel in Figure 7), as we
7
Published as a conference paper at ICLR 2022
Noise σ	0%	5%	10%	20%
OHT	56.95	53.02	52.02	30.52
ESKD	58.61	53.53	52.99	36.55
FilterKD	59.32	56.43	55.51	40.81
Table 1: Results on TinyImageNet dataset.
	Eff→Res	Res→Mob	Res→VGG
Teacher	86.83	77.23^^	77.98
Student	78.09	72.58	71.04
ESKD	81.16	73.13	72.64
FilterKD	83.03	75.79	74.49
Table 2: Teacher→student, on CIFAR100.
want the only difference among these methods to be ptar. Thus we can conclude the improvement
we achieved comes purely from the refined supervision. See Appendix D for other temperatures.
	Accuracy				OHT	ECE		FilterKD
	OHT	KD	ESKD	FilterKD		KD	ESKD	
CIFAR10	95.34	95.39	95.42	95.63	0.026	0.027	0.027	0.007
CIFAR100	78.07	78.40	78.83	80.09	0.053	0.061	0.067	0.029
Table 3: Quantitative comparison of generalization performance for ResNet18 self-distillation
(mean value of 5 runs). Refer to Table 6 for detailed results.
Input: Dataset {(xn, yn)}nN=1, where In = {1, 2, ..., N} is the index for each pair
#	Train the teacher
Initialize a network model, initialize an N × K matrix called qsmooth
Go through the entire dataset, calculate qsmooth [n] = Softmax(model(xn))
for epoch = 1, 2, ..., T do
for n ∈ {1, 2, . . . , N} in random order do
P = Softmax(model(Xn))
qsmooth[n] = (I ― α) ∙ qsmooth[n] + α ' P
Update parameters based on loss = CroSSEntroPy(p, ynn),
end for
Check the early stopping criterion; stop training if satisfied
end for
#	Train the student
For each input xn, set ptar = qsmooth[n]; train the network under the supervision of ptar
Algorithm 1: Filter-KD. α controls the cut-off frequency of low-pass filter (0.05 here).
The first task we consider is noisy-label classification, where we train and validate the model on a
dataset with some labels randomly flipped. After training, all the models are evaluated on a clean
held-out test set. The experiments are conducted on CIFAR (Figure 7) and TinyImageNet (Table 1),
under different noise ratios σ: σ = 0.1 means 10% of the labels are flipped. In Figure 7, an
interesting trend can be observed: the enhancement brought by Filter-KD is not significant when
σ is too small or too large. That is reasonable because for small σ, few samples have bad labels,
thus the possible enhancement might not be large. When σ is too high, the labels of similar points
become less reliable, and the learning path will no longer head as reliably towards p*. Thus, for
very high noise ratios, the performance of Filter-KD decays back towards that of OHT.
Filter-KD also outperforms other methods in both accuracy and calibration on the clean dataset, as
illustrated in Table 3. This remains true in the more common KD case when the teacher network is
bigger than the student; see results in Table 2. Note that in Table 2, we no longer keep τ = 1, as the
baselines are more sensitive to τ when the teacher is large (see Appendix D for more discussion);
instead we optimize τ for each setting. Here “Eff” is short for EfficientNet-B1 (Tan & Le, 2019),
“Res” is short for ResNet18, “Mobi” is short for MobileNetV2 (Sandler et al., 2018), “VGG” is
short for VGG-11 (Simonyan & Zisserman, 2015).
In summary, the explanation in Section 4 and experimental results in this section demonstrate that
better supervisory signal, which can be obtained by Filter-KD, can enhance the prediction perfor-
mance in both clean and noisy-label case. (It is also possible that the algorithm improves perfor-
8
Published as a conference paper at ICLR 2022
mance for other reasons as well, especially in the clean label case; the influence of temperature may
be particularly relevant.)
6	Related work
Human label refinement Peterson et al. (2019) use a distribution of labels obtained from multiple
annotators to replace the one-hot label in training, and find that doing so enhances both the general-
ization performance of trained models and their robustness under adversarial attacks. However, this
approach is relatively expensive, as it requires more annotation effort than typical dataset creation
techniques; several experienced annotators are required to get a good estimate of p*.
Label smoothing Another popular method is label smoothing (Szegedy et al., 2016), discussed
in the previous sections. We believe that suitable smoothing can decrease |巾面一p*∣∣2 to some
extent, but the lower bound of this gap should be large, because label smoothing treats each class
and each sample in the same way. Much prior research (e.g. Muller et al., 2019) shows that KD
usually outperforms label smoothing as well.
KD and ESKD Knowledge distillation, by contrast, can provide a different ptar for each training
sample. KD was first proposed by Hinton et al. (2015) for model compression. Later, Furlanello
et al. (2018); Cho & Hariharan (2019); Zhang et al. (2019); Yuan et al. (2020) found that distillation
can help even in “self-distillation,” when the teacher and student have identical structure. Our work
applies to both self-distillation and larger-teacher cases.
Another active direction in research about KD is finding explanations for its success, which is often
still considered somewhat mysterious. Tang et al. (2020) decompose the “dark knowledge” provided
by the teacher into three parts: uniform, intra-class and inter-class. This explanation also overlaps
with ours: if we observe the samples with different difficulty in the same class, we are discussing
intra-class knowledge; if we observe samples with two semantically similar classes, we then have
inter-class knowledge. Among previous work, the most relevant study is that of Menon et al. (2021).
Our Hypothesis 1 is suggested by their main claim, which focuses on the question of what is a good
ptar. Our work builds off of theirs by helping explain why this good ptar emerges, and how to find
a better one. Besides, by looking deeper into the learning path of samples with different difficulty,
our work might shed more light on KD, anti-noisy learning, supervised learning, overfitting, etc.
Noisy label task Learning useful information from noisy labels is a long-standing task (Angluin
& Laird, 1988; Natarajan et al., 2013). There are various ways to cope with it; for instance, Menon
et al. (2019) use gradient clipping, Patrini et al. (2017) use loss correction, Huang et al. (2020)
change the supervision during training, and Zhang et al. (2020) employ extra information. It is
possible to combine KD-based methods with traditional ones to further enhance performance; our
perspective of learning paths also provides further insight into why KD can help in this setting.
7	Discussion
In this paper, we first claim that better supervision signal, i.e., Ptar that is closer to p*, leads to
better generalization performance; this is supported by results of Menon et al. (2021) and further
empirical suggestions given here. Based on this hypothesis, we explain why LS and KD outperform
one-hot training using experiments on a toy Gaussian dataset. To further understand how such
better supervision emerges, we directly observe the behavior of samples with different difficulty by
projecting them on a 2-D plane. The observed zig-zag pattern is well-explained by considering the
tradeoff between two forces, one pushing the prediction to be near that of similar inputs, the other
pushing the prediction towards its training label; we give an intuitive account based on Proposition 1
of why this leads to the observed zig-zag pattern.
To apply these findings on real tasks, in which the data and network are more complex, we conduct
low-pass filter on the learning path, and propose Filter-KD to further enhance ptar. Experimental
results on various settings (datasets, network structures, and label noise) not only show the advantage
of the proposed method as a method for a teacher in knowledge distillation methods, but also help
verify our analysis of how generalization and supervision work in training neural network classifiers.
9
Published as a conference paper at ICLR 2022
Acknowledgements
This research was enabled in part by support provided by the Canada CIFAR AI Chairs program,
WestGrid, and Compute Canada.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.
Dana AnglUin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343-370,
1988.
Sanjeev Arora, Simon S. DU, Wei HU, ZhiyUan Li, RUslan SalakhUtdinov, and RUosong Wang.
On exact compUtation with an infinitely wide neUral net. In Advances in Neural Information
Processing Systems, 2019.
Jang HyUn Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings
ofthe IEEE/CVF International Conference on Computer Vision,pp. 4794U802, 2019.
Tommaso FUrlanello, Zachary Lipton, Michael Tschannen, LaUrent Itti, and Anima AnandkUmar.
Born again neUral networks. In Proceedings of International Conference on Machine Learning,
pp.1607-1616. PMLR, 2018.
Mengya Gao, YUjUn Shen, QUanqUan Li, JUnjie Yan, Liang Wan, DahUa Lin, Chen Change Loy,
and XiaooU Tang. An embarrassingly simple approach for knowledge distillation. arXiv preprint
arXiv:1812.01819, 2018.
ChUan GUo, Geoff Pleiss, YU SUn, and Kilian Q Weinberger. On calibration of modern neUral
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Kaiming He, Haoqi Fan, YUxin WU, Saining Xie, and Ross Girshick. MomentUm contrast for
UnsUpervised visUal representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neUral network. arXiv
preprint arXiv:1503.02531, 2015.
Lang HUang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk
minimization. Advances in Neural Information Processing Systems, 33, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neUral networks. In Advances in Neural Information Processing Systems, 2018.
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regu-
larities of labeled data in overparameterized models. Proceedings of International Conference on
Machine Learning, 2021.
Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sangwook Cho, and Se-Young Yun. Understanding
knowledge distillation. https://openreview.net/forum?id=tcjMxpMJc95, 2019.
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in Neural Information Processing
Systems, 2020.
Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient
clipping mitigate label noise? In International Conference on Learning Representations, 2019.
10
Published as a conference paper at ICLR 2022
Aditya Krishna Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar.
A statistical perspective on distillation. In International Conference on Machine Learning, pp.
7632-7642. PMLR, 2021.
Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems, 2019.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. Advances in Neural Information Processing Systems, 26:1196-1204, 2013.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human
uncertainty makes classification more robust. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 9617-9626, 2019.
Karsten Roth, Timo Milbich, Bjorn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. S2sd:
Simultaneous similarity-based self-distillation for deep metric learning. Proceedings of Interna-
tional Conference on Machine Learning, 2021.
David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical
report, Cornell University Operations Research and Industrial Engineering, 1988.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bileNetV2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4510-4520, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. Proceedings of International Conference on Learning Representations, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, 2015.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the Inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105-6114. PMLR, 2019.
Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. Under-
standing and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation
via label smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 3903-3911, 2020.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the per-
formance of convolutional neural networks via attention transfer. Proceedings of International
Conference on Learning Representations, 2017.
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
ICCV, 2019.
Zhilu Zhang and Mert R Sabuncu. Self-distillation as instance-specific label smoothing. Advances
in Neural Information Processing Systems, 2020.
Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pfister. Distilling effective
supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9294-9303, 2020.
11
Published as a conference paper at ICLR 2022
Code, including the experiments producing the figures and a Filter-KD implementation, is available
at https://github.com/Joshua-Ren/better_supervisory_signal.
A Miscellaneous background
Calculation of ECE Expected calibration error is a measurement about how well the predicted
confidence represent the true correctness likelihood (Guo et al., 2017). For example, if our model
gives 100 predictions, each with confidence, say, q(y = k|x) ∈ [0.7, 0.8]). Then we might expect
there are 70 〜80 correct predictions among those 100 ones. To calculate this, We first uniformly
divide [0,1] into M bins, with each bin represented by Im ∈ (m-1, m] and Bm is all the X samples
Whose confidence falls into Im . Then, ECE is calculated as:
ECE =
M
X
m=1
IBm
n
|acc(Bm) - conf(Bm)| ,
(5)
where acc(Bm) = ∣Bm∣ Pi∈Bm Myi = ki), Conf(Bm) = TBmT Pi∈Bm 4® = kiM, k is the
true label and yi = argmax[q(y∣Xi)] is the model’s prediction. All the ECE mentioned in this paper
is calculated by setting M = 10.
Barycentric coordinate system When visualizing the learning path, it is problematic to directly
choose two dimensions and draw them in the Cartesian coordinate system, as illustrated in the first
panel in Figure 8. In geometry, a suitable way to project a 3-simplex vector onto a 2-D plane is
converting it to a point in a barycentric coordinate system. Specifically, we have three basis vectors:
vo = [0, 0]t, v1 = [1, 0]t, and V2 = [2, √3] , which are the corner and two edges of an equilateral
triangle respectively. Then the 2D-coordinate is calculated as (x,y) = [vo； Vi； v2] ∙ q. So every
points in the left corner of a Cartesian system plane can be converted to the triangle in a barycentric
system, as illustrated in the last panel in Figure 8.
Figure 8: How to project a probability vector on to the plane of Barycentric coordinate system.
B	Risk es timate variance bound
The following result is a generalization of Proposition 3 of Menon et al. (2021), whose proof we
replicate and extend here:
Proposition 2. Let L be any bounded loss, L(y, y) ≤ ' < ∞, and consider Rtar of (3). For any
predictor f : X → ∆K, we have that
E」(Rtα(f, D0) - R(f ))2] ≤ " Var [pL∙L(f (x))] + ξ,
12
Published as a conference paper at ICLR 2022
where ξ can be any of the following seven quantities:
'2 K (E kp tar (x)-p* (x)k2 )2
♦ (I Ilp tar (X) - P* (x)kl)2
2'2 (EE	PKL(ptar(x) k P*(x)))2	2'2	EE KL(Pa(x)	k	P*(x))
2'2 (EE	PKL(p*(x) kp tar (x)) )2	2'2	EE KL(P a (x)	k	P* (x))
'2Ex KL(Ptar(x) I P*(x)) + KL(P*(x) I Ptar(x)).
Proof. To begin,
DE0 (Rtar(f,D0)-R(f))2 =VDa0r[Rtar(f,D0)-R(f)]+ (DE0[Rtar(f,D0)-R(f)])2.
For the variance term, since R(f) is a constant and Rtar is an average of N i.i.d. terms, we get
VDar[Rtar (f, DO)- Rf )] = Nn Var[Ptar (X)TLf(X))].
The other term, as Rtar is an average of i.i.d. terms and R(f) = Ex P*(x)>L(f(x)), is
(K [Rtar (f, D)- R(f)] )2 = (EE (Ptar (x) - P* (x))> L(f (x))) 2 .
For the first bound, we apply the Cauchy-Schwarz inequality,
(Ptar (x) - P* (x))> L(f (x)) ≤ kPtar (x) - P* (x)∣2 ∣L(f (x))∣2 ；
since the elements of L(f (x)) are each at most', the term ∣∣L(f (x))∣∣2 is at most y∕K'.
For the other bounds, We instead apply Holder,s inequality, yielding
(Ptar(x)-P*(x))>L(f(x)) ≤ kPtar(x)-P*(x)k1kL(f(x))k∞ ≤ 'kPtar(x) - P*(x)k1.
The KL bounds follow by Pinsker’s inequality and then Jensen’s inequality. The last bound, for the
Jeffreys divergence, combines the two KL bounds.	□
C Toy Gaussian dataset
delta_mu=4	delta_mu=2	delta_mu=l	delta_mu=0.5
-.-	1.0	1.5	2.0	2.5	3.0	0	2	4	6	8	10	__	,	_	__	__	_	.	一	一	一
L2-distance of p_tar and p*	L2-distance of p tar and p*	L2-distance of p_tar and p*	L2-distance of p_tar and p*
Figure 9: Correlation between test accuracy and Iptar - p* I2 for different settings.
L2-distance of p_tar and p*
delta mu=2
5	10	15	20	25
L2-distance of p_tar and p*
Figure 10: Correlation between test ECE and Ilptar - p* k2 for different settings.
delta_mu=l
delta_mu=0.5
10	20	30	40
L2-distance of p_tar and p*
In Sections 2 and 3, we apply a toy Gaussian dataset to verify two facts derived from Hypothesis 1,
so as the learning path of specific samples. Here we provide more details about this dataset.
13
Published as a conference paper at ICLR 2022
	Run1				Run2				Run3			
	τ=0.5	τ=1	τ=2	τ=10	τ=0.5	T=1	T=2	T=10	τ=0.5	T=1	T=2	T=10
OHT	94.87	-	-	-	95.15	-	-	-	94.48	-	-	-
ESKD	95.02	95.27	95.34	95.05	95.41	95.41	95.39	94.76	94.11	94.65	94.88	94.75
FilterKD	95.18	95.87	95.66	94.81	95.11	95.56	95.71	94.81	95.09	95.31	95.22	95.16
OHT	77.85	-	-	-	78.23	-	-	-	77.99	-	-	-
ESKD	78.28	78.56	79.28	79.09	78.50	78.20	79.3	78.81	78.12	78.31	78.84	78.36
FilterKD	78.43	79.57	79.72	79.65	79.23	79.61	79.45	79.01	79.75	80.32	80.41	79.99
Table 4: The influence of temperature in self-distillation on CIFAR10/100 dataset.
Generate the dataset Here, We have N samples, each sample a 3-tuple (x,y, p*). To get one
sample, we first select the label y = k following an uniform distribution over all K classes. After
that, we sample the input signal x∣y=k 〜N3 ,σ2I), where σ is the noisy level for allthe samples.
μk is the mean vector for all the samples in class k. Each 模卜 is a 30-dim vector, in which each
dimension is randomly selected from {-δμ, 0,δμ}. Suchaprocess is similar to selecting 30 different
features for each class. Finally, we calculate the true Bayesian probability of this sample, i.e.,
p*(y∣χ).
Calculate the ground truth probability We use the fact thatp* (y|x) a p(x∣y)p(y). As y follows
an uniform distribution, we havep*(y∣x)
jkXP(=k=j). FolloWing P(XIy = k) 〜N(μk,σ2I),
we findp*(y∣x) should have a Softmax form, i.e., P
esk
jej.
Specifically, we have:
esk
p*(y=k1x)=P^
si = - 2σ2 kx - μik2∙
(6)
Setup of experiments in Figure 1 in this experiment, we generate a toy Gaussian dataset with
K = 3, σ = 2 and N = 105. To reduce the variance of test error, we make a train/valid/test
split with ratio [0.05 0.05, 0.9]. We train an MLP with 3 hidden layers, each with 128 hidden
units and ReLU activations. We first conduct experiments on some baseline settings, i.e., learning
from one-hot supervision (OHT for short), from smoothed label supervision (LS for short), from
a converged teacher’s prediction (KD for short), and from an early-stopped teacher’s prediction
(ESKD for short). In OHT case, an NN is trained under the supervision of ey . If we train this
NN until the convergence of training accuracy, we obtain the ptar for the KD case. If we select the
snapshot of that NN based on the best validation accuracy, we obtain the ptar for the ESKD case. If
we directly set ptar = 0.9ey + 0.1u, where u is an uniform distribution over K classes, we obtain the
supervision for LS case. With different supervisions, we train a new network with identical structure
until the validation accuracy no longer increase. Furthermore, to see a trend between generalization
ability and ∣∣ptar - p* k2, we run the experiment 200 times under different noisy supervisions.
D Temperature in different KD methods
Temperature is an important hyper-parameter in different kinds of KD methods, which might influ-
ence the performance a lot. In this appendix, we will discuss why we prefer τ = 1.
The role played by τ In general, the loss function in KD has the form:
L = β ∙ (1) ∙ H(qτ，PTr) + (1- β) ∙ H(q, ey),	(7)
where β ∈ [0, 1] is another hyper-parameter to trade-off the importance between one-hot label and
teacher’s predictions. Furthermore, for this loss, the gradient of L to logits z is:
∂- = β ∙ (1) YqT - PTri) + (I - β) Yqi - eyJ.
∂zi	τ
(8)
14
Published as a conference paper at ICLR 2022
LO 15 20 25 30 35 40
Distance to p*
Figure 11: The influence of label smoothing factor on LS (first row) and different τ on KD (other
rows) under two different settings of the toy dataset. It is clear that these hyper-parameters won’t
influence the performance too much as long as they are in a good region.
Why we use τ = 1 The most important reason for us to choose τ = 1 (as well as β = 1), even with
the risk of providing sub-optimal performance, is that we want to observe how much enhancement
is brought by refining the label. In our mind, KD’s success comes from the following two aspects:
(a)	better label (i.e., ptar is better than ey);
(b)	better learning dynamics (soften q to qτ make the training easier).
The first one provides the student with more useful knowledge, and the second one helps the student
to extract it. The focus of our paper is the improvement in labels. When the temperature is not 1,
both (a) and (b) are influenced, as we are using qτ (i.e., the smoothed student output) to match ptτar
(i.e., the smoothed target) during training and using q to inference during testing. If we fix τ = 1,
the only difference between Filter-KD, ESKD, KD, LS, and OHT training is ptar. Under such a
condition, we believe the fact that Filter-KD/ESKD outperforms OHT (strictly better in each run) is
enough to conclude KD methods can provide better labels. Furthermore, as Filter-KD is proposed
to mitigate the high-variance issue in ESKD, and we can directly observe the zig-zag learning path
of samples with bad labels, we believe it is reasonable to make the conclusion in the paper.
Furthermore, using τ 6= 1 doesn’t substantially
change our analysis. As illustrated in Equa-
tion (8), the gradient shows that qτ moves to-
wards ptτar in each update. Hence our analysis
in Proposition 1 still holds, which means the
trade-off between the two forces still exists.
0.90
Q0∙85
iŋ 0.80
∪0.75
2 0.70
ωθ.65
0.60
0.55 .
O
τ = 1 is reasonably good in our settings
In fact, the optimal τ depends on many set-
tings, e.g., teacher’s size, optimizer, learning
rate (scheduler), and etc: there is still no con-
sensus on what is the best choice of τ for all
settings. So for the experiments in the main
context, we fix τ and fine-tune other hyper-
parameters to achieve good results. However,
to verify this choice of τ is not terrible, we per-
formed a coarse grid search on both CIFAR and
toy examples. As illustrated in Table 4 and Fig-
5	10	15
Temperature
0.903
⅛ 0.902
3 0.901
tt。・9。。
F 0.899
0.898
20
Figure 12: First row: large teacher to small stu-
dent; second row: self-distill. Left column: τ in a
larger range; right column: τ in a small range.
0.902
A
2 0.901
u 0.900
W 0.899
H
0.898
Temperature
ure 11, choosing τ ∈ [1, 2] seems to be good choice: the performance doesn’t significantly decay
until τ is too large.
Some readers might also curious about why our results don’t match the common notion that the
optimal τ should be 4, which is first provided in Hinton et al. (2015) and followed by much later
15
Published as a conference paper at ICLR 2022
1.0
0.8
in
in
30.6
CT
(O
⅛θ4
N
0.2
0.0
Wrong labels Correct labels
Figure 13: The learning path of samples with correct and wrong labels.
work (e.g. Gao et al., 2018; Zagoruyko & Komodakis, 2017; Cho & Hariharan, 2019). Some works
also use a grid search to conclude that a temperature as high as 20 should be the best choice (Yuan
et al., 2020; Kim et al., 2019). We find this mismatch comes from the relative difference between
the network size of teacher and student. In short, when distilling from a large teacher to a small
student (as most of the cases in aforementioned works), high τ is preferred. When conducting self-
distillation, τ need not be that high. For example, Zhang et al. (2019) and Roth et al. (2021) claim
τ = 1 is the best choice, while Zhang & Sabuncu (2020) claim τ ≈ 2 is the best. To further
verify this, we compare the temperature trend between two cases on the toy dataset. In Figure 12,
the first row is distilling a 10-layer, 256-width MLP to a 3-layer, 32-width MLP; the second row is
self-distillation between 3-layer, 32-width MLPs.
At the same time, we also run a grid search the smoothing factor α of our Filter-KD in Table 5.
Smoothing α	0.01	0.05	0.1	0.2	0.5	1 (ESKD)
FilterKD	79.50	80.00	79.83	79.59	78.48	78.39
Table 5: The influence of the smoothing factor in FilterKD on CIFAR100 (mean of 3 runs). For
comparison, OHT obtained 77.64.
E How representative is the zig-zag pattern ?
In the main paper, we only visualize the zig-zag learning path of a few samples in Figures 3 and 5.
The readers might then wonder whether this pattern is representative across the whole dataset. To
verify this, we define a quantitative metric called zig-zag score. Specifically, we first calculate the
integral of each dimension of the prediction:
T
Qi∈{1,...,K} , Xqit(xn).	(9)
t=1
We then use the highest Qi among i ∈ {1, ..., K} \ {y}, where y is the label’s class, as the zig-zag
score. In other words, we focus on the behavior of q on those dimensions that are not the training
label. If this score is large, we might expect the neighbouring samples exert high influence on the
path, and vice versa. Note that as we have Pi Qi = constant for any sample (as qt is a K-simplex),
this zig-zag score might correlated with C-score of Jiang et al. (2021). However, their focuses are
different: C-score is similar to -Qi=y and focuses more on how fast the prediction converge to the
label; zig-zag score, on the other hand, is more about how much the path deviates towards a different
class, possibly the label close to p*.
With this score, we test the correlation between base difficulty and zig-zag score on toy datasets
under different settings. From Figure 14, it is safe to conclude that samples with higher base dif-
ficulty will have a more zig-zagging path during training. We also compare the expected zig-zag
score of the 1000 samples with flipped label in CIFAR10 (recall the experiment in Figure 6). In
Figure 13, it is clear that the zig-zagness of these wrong label samples (which we are sure have high
base difficulty) is significantly higher than the average.
Finally, we also randomly select some data samples in each class of CIFAR10 and visualize their
learning paths. Figure 15 shows random samples for the noisy label experiment. It is clear that the
16
Published as a conference paper at ICLR 2022
Figure 14: Correlation between base difficulty and zig-zag score in four different toy datasets.
learning path of easy samples with correct labels converge fast while that of samples with wrong
labels is zig-zag. In Figure 16, which shows the learning path with high zig-zag score when the
training set is clean, we can still observe some samples with zig-zag path. These samples might be
ambiguous, With a quite flat p*. However, as We do not know the true p* of these samples, it is
impossible to provide a result like Figure 13.
F	Distance gap under different supervisions
Figure 2 provides the distance gap between q and corresponding p* for each training sample in
different training stages, under the supervision of one-hot label ey. From the results in this figure, we
notice that the behavior of hard samples contributes more to the success of ESKD. Here we further
visualize how these gaps changes when the model is trained under different types of supervision,
i.e., ground truth, LS and ESKD.
In the first row, which is the result of label smoothing, we see the kptar - p* k2 (i.e., red dots) has
a “V”-shape. The vertex location depends on the smoothing parameter we choose. However, as
discussed previously, label smoothing has only one parameter to control ptar, which is like using a
linear model to fit a high-order function. So, although a proper smoothing value can bring better su-
pervision than one-hot label, its upper bound might be limited. Regarding the training dynamics, we
can see a similar trend as the results shown in the one-hot case, i.e., all the samples first move down
and then converge to ptar. From the middle panel, we might expect label smoothing to outperform
the one-hot case, because the scatters are closer to the x-axis, which represents the ground truth p* .
The second and the third rows demonstrate the KD and ESKD case. Results in the KD case are
quite similar to the one-hot case in Figure 2, because the converged ptar is close to ey . However, we
can still expect KD to outperform one-hot training, because the ptar is closer to p* than ey is. The
last panel in this row also demonstrates that kq - p* k2 might be smaller than kptar - p* k2, which
can be considered as an explanation for why iterated self-distillation, e.g., Born Again Networks
(Furlanello et al., 2018), can improve the performance. For the ESKD case, we see the overfitting
almost disappear: the distribution of the blue points do not change too much after the early stopping
criterion is satisfied.
In the last row, which illustrates the training under the supervision of p*, we find all the blue points
move toward the x-axis, i.e., their ptar = p*, and finally converge to it. There is also no overfitting
in this case. From the last two panels, we see the disperse of blue points is significantly smaller than
all other settings, which means the network’s prediction is quite close to p* . Hence the performance
of this case is the best.
17
Published as a conference paper at ICLR 2022
∙IΓ
(a) Samples with clean labels.
(b) Samples with wrong labels.
Figure 15: Random selection of samples in CIFAR10 with 1000 flipped labels.
18
Published as a conference paper at ICLR 2022
Figure 16: Random selection of samples with high zig-zag score in clean CIFAR10.
19
Published as a conference paper at ICLR 2022
T=O (InitiaI)
(Early stop)
T=200 (Converge)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
D(X)II 2
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Iley(X) -p*(x川2
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
∣]ey(x)-p*(x)∣∣2
T=200 (Converge)
(Early stop)
T=O (Initial)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
IIey(X) - P*(x 川 2
T=O (InitiaI)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Iley(X) - p*(x)∣∣2
(Early stop)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Iley(X)-p*(x)∣∣2
T=200 (ConVerge)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Iley(X)-p*(x)∣∣2
T=O (Initial)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Iley(X)— p*(X 川 2
(EarIy StoP)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Iley(X)- p *(M∣∣2
T=200 (ConVerge)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4	0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4	0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
IIey(X) - p*(x川2	Iley(X) - p*(x川2	∣∣ey(x) - p*(x)∣∣2
Figure 17: Distance gap of each sample under different supervision: label smoothing, KD, ESKD,
and ground-truth training.
Z=(X) *d—Mb-一
20
Published as a conference paper at ICLR 2022
G More about the decomposition and NTK model
Proof of Proposition 1. Recall z(x) = f (w, x) is the vector of output logits, and q = Softmax(z)
is the output probability. We are taking a step of SGD observing xu , and observing the change in
predictions on xo . We begin with a Taylor expansion,
qt+1(xo) - qt(xo) = Vwqt(xo)∣wt ∙ (wt+1 - Wt) + O(∣∣wt+1 - wt∣∣2).
'---V--} '1*{}	1---{-------} |---V-----}
K×1	K×1	K×d	d×1
To evaluate the leading term, we plug in the definition of SGD and repeatedly use the chain rule:
Vwqt(Xo)∣wt∙ (wt+1 - Wt) = (VZqt(Xo)∣zt ∙VwZt(Xo)∣wt) . ( - η VwL(Xu)Iwt)T
|-----{---} |-----7-----，	|---{---} |----{-----}	|---{------}
K×d	d×1	K×K	K×d	1×d
=V Zq (Xo) |Zt ∙v WZt (Xo) lwt ∙ ( -nV Z L(Xu) |zt，V WZt (Xu) |wt )
1--{z------} 1-,z----} 1---------{---} 1------{-----}
K×K	K×d	1×K	K×d
=-η VZqt(Xo )∣Zt ∙[ VwZt(Xo)Iwt ∙ (VwZt(Xu)Iwt )T ] ∙ (VZL(Xu)IZt)T
|---------{------} |---V-----} |-------{------}	|-----{-}
K ×K	K ×d	d×K	K ×1
=η ∙ At(Xo) ∙ κt(Xo,Xu) ∙ (Ptar(Xu) - qt(Xu)).
For the higher-order term, using as above that
wt+1 - wt = -ηVwZt(Xu)∣w, ∙ (Ptar(Xu) - qt(Xu))
and noting that, since the vectors are probabilities, kPtar(Xu) - qt(Xu)k is bounded, we have that
ο(kwt+1-wtk2) = O(η2 k (VWZ(Xu)IWt )Tk2p kPtar(Xu)-qt(Xu) k2) = O(η2kVWZ(Xu )%).□
In the decomposition,
-qιq2	.- 一qιqκ -
q2(l 一 q2 …	-q2qκ
.	.	.	,	(10)
..	.
-qκ q2	…qκ (1 - qκ).
q1(1 - q1)
-q2 q1
At ( ) = Vz qt(Xo )Iz, =	.
.
.
-qK q1
which is a symmetric positive semi-definite (PSD) matrix8 with trace tr(At (Xo)) = 1 - PiK=1 qi2 .
As we have Pi qi = 1, itis easy to check the trace of this matrix is larger at the beginning of training
(when q tends to be flat) than that at the end of training (q tends to be peaky), as illustrated by most
panels in Figure 18. Given that the trace of a matrix is the sum of its eigenvalues and At(Xo) is PSD,
smaller tr(At(Xo)) = 1 - PiK=1 qi2 means this matrix will tend to shrink its inputs more. Hence the
change in predictions tends to decrease when qt becomes more peaky.
The second term in that expression, Kt (Xo, Xu), is the outer product of gradients at Xo and Xu.
Intuitively, if their gradients have similar directions, this matrix is large, and vice versa. This matrix
is known as the empirical neural tangent kernel, and it can change through the course of training
as the network’s notion of “similarity” evolves. For appropriately initialized very wide networks
trained with very small learning rates, Kt remains almost constant during the course of training, and
is almost independent of the initialization of the network parameters; the kernel it converges to is
known as the neural tangent kernel (Jacot et al., 2018; Arora et al., 2019).
Learning Dynamics ofq(Xo) In Proposition 1, we decompose qt+1(Xo) - qt(Xo) into three parts;
we use this to analyze what the learning path of a training sample might be like in Section 3.3. Here
we will provide more detailed illustration of the two groups of force imposed on q(Xo).
Specifically, qt+1(Xo) - qt(Xo) will be influenced by two variables, i.e., updating sample Xu and
time t. We discuss their effects separately. For Xu, only the last two terms, i.e., Kt (Xo, Xu) and
(Ptar(Xu) - qt(Xu)) depends on it.
8The matrix A can be observed to be the covariance matrix of a categorical distribution with item probabil-
ities q, and hence PSD.
21
Published as a conference paper at ICLR 2022
Figure 18: Upper panels: how tr(At(xn)) changes during training. Each panel represents a specific
xn . The panels are ordered by their integral difficulty, from left-to-right and up-to-down. The x-
axis is the number of epochs, and the y-axis is tr(At (xn)). Lower panels: the correlation between
cos(xo, xu) and tr(K0). Panels are ordered by the integral difficulty of xo. The subtitle of the panel
gives the sample ID, the class it belongs to (i.e., A, B, or C) and the color of their corresponding
class (i.e., A is blue, B is orange and C is green).
In Section 3.3, we claim that ifxo and xu are similar, the norm ofK0(xo, xu) might be large, and vice
versa. Here we empirically demonstrate this using a toy Gaussian dataset, as illustrated in Figure 18.
The figure shows how the similarity between xo and xu correlates with tr(K0 (xo, xu)). Each panel
represents one xo , which is claimed in the title of the subfigures. The x-axis represents the rank of
the cosine similarity between observed xo and all N training samples (including itself). The y-axis
is the trace of K0(xo, xu). The color of each scatter point is the true label of xu. From the figure, we
can observe a clear decreasing trend, which means smaller similarity leads to larger tr(K0 (xo, xu)).
Additionally, the term (ptar(xu) - qt(xu)) provides a direction that qt(xo) should move towards.
We claim in Section 3.3 that at any point in the input space, the labels of these input samples might
follow the ground truth distribution, i.e., p*(y∣x). Hence most of the neighbouring X might PUn
q(xo) towards its ground truth p* (y∣xo). We Win discuss the norm of this term when discussing the
influence of t. In short, at any time, the neighboring xu will imPose stronger force on xo and the
direction of the force roughly Points to the ground truth p* (y|xo).
22
Published as a conference paper at ICLR 2022
As discussed, Kt is roughly constant over t in the very-wide limit. For finite width networks, how-
ever, it adapts to reflect the network’s new “understanding” of similarities. For instance, it might
learn that certain types of images are more semantically similar than the randomly-initialized net-
work thought. This does not fundamentally change our intuitions as long as it doesn’t happen too
often, but could potentially lead to more complicated zig-zag patterns as the network’s estimate of
p* from neighboring points perhaps improves over time.
Over the course of training, At (xo) and (ptar(xu) - qt(xu)) will also change. In practical regimes,
none of these terms have an easy analytical expression w.r.t. t: qt is quite complicated. Thus, we pro-
vide some intuition, with experimental verification. In Figure 18, we show how tr(At (xo)) depends
on qt: flat qt leads to larger trace. A similar trend also exists in the norm of (ptar(xu) - qt(xu)).
As the initialized q tend to be flat, updates of any samples will influence network’s parameters a
lot. When the training progresses, those easy samples converge fast, so their kptar(xu) - qt(xu)k2
and tr(At) become small. However, as the qt for the hard sample is still far away from its ey, the
large kptar(xu) - qt(xu)k2 and tr(At) will finally drag qt back towards the one-hot distribution, as
illustrated in Figure 3.
H Comparison to Liu et al. (2020) and Huang et al. (2020)
The main claim of this paper is that better supervisory signals can enhance the generalization ability
of the trained model. Inspired by the success of KD, we find that the neural network can sponta-
neously refine those “bad” labels during training by observing their learning path. The learning path
of those hard samples will first move towards their true p(y|x) and then converge to their label ptar
or ey. We explain why this phenomenon occurs by expanding the gradients of each training sample.
This phenomenon is also explained in Proposition 1, and formally proved for a particular softmax
regression model by Liu et al. (2020). As a complement, we propose an explanation using an NTK
model, and experimentally verify it by observing the learning path and distance gap during training.
Another difference between these two works is that we consider the problem of refining supervisory
signals, while Liu et al. (2020) consider correcting wrong labels (a special case of “bad” supervi-
sion). Our work provides additional emphasis and empirical study for the clean-label case.
Regarding the algorithm, Liu et al. (2020) design an effective regularization term inspired by early
stopping regularization. They apply exponential moving average (EMA) on the model’s output when
calculating this regularization term to further enhance the performance. This method is similar to
that proposed by Huang et al. (2020), who switch between optimizing the training loss and an
objective based on the EMA of the model over the course of training.
Although it bears significant similarity to these methods, Filter-KD does not change the course of
training the teacher model. Rather, we propose (based on the high variance of the zig-zag learning
paths) to simply use the the EMA of that model’s outputs as a teacher for later distillation.
We suspect that these three algorithms work because of essentially the same underlying principle,
whether we think of this as being based on the zig-zag learning path or as early-stopping regulariza-
tion. We expect that this principle will be helpful moving forward in the field’s understanding of the
learning dynamics of SGD methods for neural networks.
I	Low pass filter on network parameters
In Section 4, we point out the high variance issue of the traditional KD methods after observing
the learning path of those hard samples. We then propose a Filter-KD algorithm to smooth the
output of each training sample during training. Such a low pass filtering method is quite similar
to the momentum mechanism used in self-supervised learning, e.g., from the classic method of
Ruppert (1988) to MOCO (He et al., 2020), which conduct low pass filtering on each parameter of
the network. As mentioned before, the proposed Filter-KD algorithm might require more memory
when the dataset becomes larger, because qsmooth would record every training sample’s prediction.
Conducting low pass filtering on network parameters might be a good way to solve this. To verify
whether this method works, we train a ResNet18 on CIFAR100, using one-hot supervision. At the
beginning of training, we copy the training model and call it tracking model. At the end of each
23
Published as a conference paper at ICLR 2022
Figure 19: Behavior of the tracking model (with low pass filter on each parameters). ResNet18
trained for 150 epochs on CIFAR100.
update (i.e., each batch), the parameters of training model is updated as usual while the tracking
model’s parameters are updated with momentum. Specifically, for the training model, wttr+ai1n =
Wtrain + ηVL, for the tracking model, wt+k = (1 - α)wtrack + αwt+i[ We train the model for
150 epochs, and show the learning curve of test accuracy in the left panel of Figure 19. We see the
performance of the tracking model converges faster than the training model, which is reasonable
because filtering parameters is regularizing the training process. However, the converging accuracy
of this two models are the same. At the same time, we find this tracking model is not as good as
qsmooth when teaching a new model, as illustrated in the right panel of Figure 19.
As this paper mainly focuses on the behavior of the model’s prediction, the discussion and ex-
periments of filtering in parameter space is limited. However, as many papers demonstrates the
effectiveness of this momentum mechanism, we think it is important to explore the relationship
further.
		TeSt ACC							TeSt ECE					
	Run1	Run2	Run3	Run4	Run5	Run1	Run2	Run3	Run4	Run5
OHT	95.35	95.30	95.42	95.23	95.42	0.027	0.027	0.026	0.025	0.025
KD	95.30	95.38	95.42	95.44	95.42	0.027	0.027	0.027	0.026	0.026
ESKD	95.29	95.41	95.39	95.58	95.42	0.026	0.029	0.025	0.028	0.027
FilterKD	95.66	95.68	95.49	95.76	95.58	0.005	0.006	0.008	0.011	0.006
OHT	78.27	78.31	77.97	77.78	78.02	0.053	0.053	0.052	0.053	0.054
KD	78.64	78.55	78.03	78.18	78.49	0.060	0.057	0.062	0.066	0.059
ESKD	78.84	78.73	78.85	78.97	78.74	0.065	0.066	0.067	0.070	0.066
FilterKD	79.87	79.93	80.19	80.22	80.23	0.028	0.026	0.034	0.028	0.031
Table 6: Each run of results in Table 3. In each run, the initialization of student networks for different
methods are controlled to be the same.
24