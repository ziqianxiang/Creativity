Published as a conference paper at ICLR 2022
Solve Minimax Optimization by Anderson Ac-
CELERATION
Huan He, Shifan Zhao, Yuanzhe Xi, Joyce C Ho * Yousef Saad t
Department of Computer Science	Department of Computer Science and Engineering
Emory University	University of Minnesota
Atlanta, GA 30329, USA	Minneapolis, MN 55455, USA
Ab stract
Many modern machine learning algorithms such as generative adversarial networks
(GANs) and adversarial training can be formulated as minimax optimization. Gradi-
ent descent ascent (GDA) is the most commonly used algorithm due to its simplicity.
However, GDA can converge to non-optimal minimax points. We propose a new
minimax optimization framework, GDA-AM, that views the GDA dynamics as a
fixed-point iteration and solves it using Anderson Mixing to converge to the local
minimax. It addresses the diverging issue of simultaneous GDA and accelerates
the convergence of alternating GDA. We show theoretically that the algorithm can
achieve global convergence for bilinear problems under mild conditions. We also
empirically show that GDA-AM solves a variety of minimax problems and improves
adversarial training on several datasets. Codes are available on Github * 1.
1	Introduction
Minimax optimization has received a surge of interest due to its wide range of applications in modern
machine learning, such as generative adversarial networks (GAN), adversarial training and multi-
agent reinforcement learning (Goodfellow et al., 2014; Madry et al., 2018; Li et al., 2019). Formally,
given a bivariate function f (x, y), the objective is to find a stable solution where the players cannot
improve their objective, i.e., to find the Nash equilibrium of the underlying game (von Neumann &
Morgenstern, 1944):
arg min arg max f (x, y).
x∈X	y∈Y
(1)
It is commonplace to use simple algorithms such as gradient descent ascent (GDA) to solve such
problems, where both players take a gradient update simultaneously or alternatively. Despite its
simplicity, GDA is known to suffer from a generic issue for minimax optimization: it may cycle
around a stable point, exhibit divergent behavior, or converge very slowly since it requires very
small learning rates (Gidel et al., 2019a; Mertikopoulos et al., 2019). Given the widespread usage of
gradient-based methods for solving machine learning problems, first-order optimization algorithms to
solve minimax problems have gained considerable popularity in the last few years. Algorithms such
as optimistic Gradient Descent Ascent (OG) (Daskalakis et al., 2018; Mertikopoulos et al., 2019) and
extra-gradient (EG) (Gidel et al., 2019a) can alleviate the issue of GDA for some problems. Yet, it
has been shown that these methods can still diverge or cycle around a stable point (Adolphs et al.;
Mazumdar et al., 2019; Parker-Holder et al., 2020). For example, these algorithms even fail to find
a local minimax (the set of local minimax is a superset of local Nash (Jin et al., 2020; Wang et al.,
2020)) as shown in Figure 1. This leads to the following question: Can we design better algorithms
for minimax problems? We answer this in the affirmative, by introducing GDA-AM. We cast the GDA
dynamics as a fixed-point iteration problem and compute the iterates effectively using an advanced
nonlinear extrapolation method. We show that indeed our algorithm has theoretical and empirical
guarantees across a broad range of minimax problems, including GANs.
*hhe37, szhao89, yxi26, jho31@emory.edu
^ saad@umn.edu
1https://github.com/hehuannb/GDA-AM
1
Published as a conference paper at ICLR 2022
(a) Cycling Behavoir (b) Diverging Behavoir
Figure 1: Left:f (x, y) = (4x2 - (y-3x+0.05x3)2-0.1y4)e-0.01(x +y ). Middle: -3x2-y2+4xy.
Right: f (x, y) = 2x2 + y2 + 4xy + 4y3 - 4y4. We can observe that baseline methods fail to
converge to a local minimax, whereas GDA-AM with table size p = 3 always exhibits desirable
behaviors.
(c) Converging to a non-optima
Our contributions: In this paper, we propose a different approach to solve minimax optimization.
Our starting point is to cast the GDA dynamics as a fixed-point iteration. We then highlight that the
fixed-point iteration can be solved effectively by using advanced non-linear extrapolation methods
such as Anderson Mixing (Anderson, 1965), which we name as GDA-AM. redAlthough first mentioned
in Azizian et al. (2020), to our best knowledge, this is still the first work to investigate and improve
the GDA dynamics by tapping into advanced fixed-point algorithms.
We demonstrate that GDA dynamics can benefit from Anderson Mixing. In particular, we study
bilinear games and give a systematic analysis of GDA-AM for both simultaneous and alternating
versions of GDA. We theoretically show that GDA-AM can achieve global convergence guarantees
under mild conditions.
We complement our theoretical results with numerical simulations across a variety of minimax
problems. We show that for some convex-concave and non-convex-concave functions, GDA-AM can
converge to the optimal point with little hyper-parameter tuning whereas existing first-order methods
are prone to divergence and cycling behaviors.
We also provide empirical results for GAN training across two different datasets, CIFAR10 and
CelebA. Given the limited computational overhead of our method, the results suggest that an extrapo-
lation add-on to GDA can lead to significant performance gains. Moreover, the convergence behavior
across a variety of problems and the ease-of-use demonstrate the potential of GDA-AM to become
the minimax optimization workhorse.
2	Preliminaries and background
2.1	Minimax optimization
Definition 1. Point (x*, y*) is a local Nash equilibrium of f if there exists δ > 0 such that for any
(x, y) satisfying ∣∣x - x* ∣∣ ≤ δ and ∣∣y - y* ∣∣ ≤ δ Wehave: f (x*, y) ≤ f (x*, y*) ≤ f (x, y*).
To find the Nash equilibria, common algorithms including GDA, EG and OG, can be formulated as
follows. For the two variants of GDA, simultaneous GDA (SimGDA) and alternating GDA (AltGDA),
the updates have the following forms:
Simultaneous : xt+ι = Xt - ηNXf(xt, Yt), yt+1 = Yt + ηNyf (xt, Yt)
Alternating : xt+ι = Xt - ηNxf(xt, y), yt+ι = y + ηNyf (xt+ι, y).
(2)
The EG update has the following form:
xt+2 = xt- ηNxf (Xt, yt),	yt+1 = Yt + η Ny f (Xt, Yt)
xt+ι = xt - ηNxf(xt+ 1, Yt+ 1), Yt+ι = yt + ηNyf (Xt+1, Yt+ 2).
(3)
2
Published as a conference paper at ICLR 2022
The OG update has the following form:
ηη
xt+1 = Xt-ηVχf (xt, yt) + 2Vxf (xt-1, yt-ι), yt+1 = Nt+ηVy f(xt, Yt)-~2^f(xt-ι, yt-ι).
(4)
Vxf(X, y)
-Vyf(X, y)
2.2	Fixed-Point Iteration and Anderson Mixing (AM)
Definition 2. w? is a fixed point of the mapping g if w? = g (w?) .
Consider the simple fixed-point iteration wt+1 = g(wt) which produces a sequence of iterates
{wo, wι,…，WN}. In most cases, this converges to the fixed-point, w* = g(w*). Take gradient
descent as an example, it can be viewed as iteratively applying the operation: wt+1 = g (wt) ,
wt-αtVf (wt) , where the limit is the fixed-point w? = g (w?) (i.e.Vf (wt = 0) . SimGDA updates
can be defined as the repeated application of a nonlinear operator:
wt+1 = G(ηsim) (wt) , wt - ηV (wt) with w = yX , V (w)
Similarly, we can write AltGDA updates as wt+1 = G(ηalt) (wt). An issue with fixed-point iteration
is that it does not always converge, and even in the cases where it does converge, it might do so very
slowly. GDA is one example that it could result in the possibility of the operator converging to a limit
cycle instead of a single point for the GDA dynamic. A way of dealing with these problems is to use
acceleration methods, which can potentially speed up the convergence process and in some cases
even decrease the likelihood for divergence.
There are many different acceleration methods, but we will put our focus on an algorithm which we
refer to as Anderson Mixing (or Anderson Acceleration). In short, Anderson Mixing (AM) shares the
same idea as Nesterov’s acceleration. Given a fixed-point iteration wt = g (wt-1), Anderson Mixing
argues that a good approximation to the final solution w* can be obtained as a linear combination
of the previous p iterates wt+1 = Pip=0 βig (wt-pt+i). Since obtaining the proper coefficients βi is
a nonlinear procedure, Anderson Mixing is also known as a nonlinear extrapolation method. The
general form of Anderson Mixing is shown in Algorithm 1. For efficiency, we prefer a ‘restarted’
version with a small table size p that cleans up the table F every p iterations because it avoids solving
a linear system of increasing size.
Algorithm 1: Anderson Mixing Prototype (truncated version)
Input: Initial point wo, Anderson restart dimension p, fixed-point mapping g : Rn → Rn.
Output: wt+1
for t = 0, 1, . . . do
Set pt = min{t, p}.
Set Ft = [ft-pt, . . . , ft], where fi = g(wi) - wi for each i ∈ [t -pt . . t].
Determine weights β = (β0, . . . , βpt)T that solves minβ kFtβk2 , s. t. Pip=t 0 βi = 1.
Set wt+1 = Pi=0 βig (wt-pt+i ).
end
2.3	AM and Generalized Minimal Residual (GMRES)
Developed by Saad & Schultz (1986), Generalized Minimal Residual method (GMRES) is a Krylov
subspace method for solving linear system equations. The method approximates the solution by the
vector in a Krylov subspace with minimal residual, which is described below.
Definition 3. Assume we have the linear system of equations X = b with ∈ Rn×n , b ∈ Rn and
an initial guess X0. Then we denote the initial residual by r0 = b - X0 and define the tth Krylov
subspace as Kt = span{r0, r0, ∙∙∙ ,t-1 r0}.
The tth iterate Xt of GMRES minimizes the norm of the residual rt = b - Xt in Kt, that is, Xt solves
min kb - Xtk2 .
xt∈x0+Kt
3
Published as a conference paper at ICLR 2022
The following formulation is equivalent to GMRES minimization problem and more convenient for
implementation. It computes xbt such that
xbt = arg min kb - (x0 + xbt)k2 = arg min kr0 - xbtk2 .
xbt ∈Kt	xbt ∈Kt
Using a larger Krylov dimension will improve the convergence of the method, but will require more
memory. For this reason, a smaller Krylov subspace dimension t and ‘restarted’ versions of the
method are used in practice Saad (2003).
The convergence of GMRES can be studied through the magnitude of the residual polynomial.
Theorem 2.1 (Lemma 6.31 of Saad (2003)). Let xbt be the approximate solution obtained at the t-th
iteration of GMRES being applied to solve x = b, and denote the residual as rt = b - xbt. Then, rt
is of the form
rt = ft()r0,	(5)
where
krtk2 = kft()r0k2 = min kft()r0k2,	(6)
ft ∈Pt
where Pp is the family of polynomials with degree p such that fp (0) = 1, ∀fp ∈ Pp, which are
usually called residual polynomials.
Although GMRES is applied to a system of linear equations not a fixed-point problem, there is a
strong connection between Anderson Mixing and GMRES. In AM we are looking for a fixed-point x
such that Gx - b - x = 0 and by rearranging this equation we get
b+(G-I)x=0⇔ (I-G)x=b.
Theorem 2.2 shows that if GMRES is applied to the system (I - G)x = b and AM is applied to
g(x) = Gx + b with the same initial guess and I - G is non-singular, then these are equivalent in
the sense that the iterates of each algorithm can be obtained directly from the iterates of the other
algorithm.
Theorem 2.2 (Equivalence between AM with restart and GMRES (Walker & Ni, 2011a)). Consider
the fixed point iteration x = g(x) where g(x) = Gx + b for G ∈ Rn×n and b ∈ Rn. IfI-G
is non-singular, Algorithm 1 produces exactly the same iterates as GMRES being applied to solve
(I - G)x = b when both algorithms start with the same initial guess.
Theorem 2.2 can also be generalized to the restart version of AM an GMRES as well.
3	GDA-AM : GDA with Anderson Mixing
We propose a novel minimax optimizer, called GDA-AM, that is inspired by recent advances in
parameter (or weight) averaging (Wu et al., 2020; Yazici et al., 2019). We argue that a nonlinear
adaptive average (combination) is a more appropriate choice for minimax optimization.
3.1	GDA with Naive Anderson Mixing
We propose to exploit the dynamic information present in the GDA iterates to “smartly” combine the
past iterates. This is in contrast to the classical averaging methods (moving averaging and exponential
moving averaging) (Yang et al., 2019) that “blindly" combine past iterates. A naive adoption of
Anderson Mixing using the past p GDA iterates for both simGDA and altGDA has the following
form:
pp
Anderson mixing :	xt+1 =	βixt-p+i, yt+1 =	βiyt-p+i.
i=0	i=0
Since Zhang et al. (2021); Gidel et al. (2019b) show the AltGDA is superior to SimGDA in many
aspects, we briefly summarized both Simultaneous and Alternating GDA-AM in Algorithms 2 and 3
with the truncated Anderson Mixing Algorithm 1 using a table size p.
4
Published as a conference paper at ICLR 2022
Algorithm 2	Simultaneous GDA-AM		Algorithm 3: Alternating GDA-AM			
Input: x0, y size p Output: xt, Set w0 = [x0 for t = 0, 1, . xt, yt = xt+1 = x yt+1 = y wt+1 = Use Ande to extrap end xt, yt = wt+ return xt, yt	, step yt , y0], . . do wt[0 : t-η t-η xt+1 yt+1 rson olate 1[0 : s	size η, Anderson table sx = length(x0) sx - 1], wt [sx : end] %f(χt, yt) ▽yf (χt, yt) ixing with table size p wt+1 x - 1], wt+1 [sx : end]	I O S f e χ r	nput: χ0, y size p utput: χt, et w0 = [χ0 r t = 0, 1, . χt, yt = χt+1 = χ yt+1 = y wt+1 = Use Ande to extrap nd t, yt = wt+ eturn χt, yt	, step yt , y0], . . do wt[0 : t-η t-η χt+1 yt+1 rson olate 1[0 : s	size η, Anderson table sx = length(x0) sx - 1], wt [sx : end] ▽xf (χt, yt) ▽yf (Xt+1, yt) ixing with table size p wt+1 x - 1], wt+1 [sx : end]
It is important to note that the Anderson Mixing form shown in Algorithm 1 is for illustrative purpose
and not computationally efficient. For example, only one column of Ft needs to be updated at each
iteration. In addition, the solution of the least-square problem in Algorithm 1 can also be solved by
a quick QR update scheme which costs (2n + 1)p2 (Walker & Ni, 2011a). Thus, from Algorithms
2 and 3, we can see that the major cost of GDA-AM arises from solving the additional linear least
squares problem compared to regular GDA at each iteration. Additional implementation details are
provided in the Appendix.
4 Convergence results for GDA-AM
In this section, we show that both simultaneous and alternating version GDA-AM converge to
the equilibrium for bilinear problems. First, we do not require the learning rate to be sufficiently
small. Second, we explicitly provide a linear convergence rate that is faster than EG and OG. More
importantly, we derive nonasymptotic rates from the spectrum analysis perspective because existing
theoretical results can not help us derive a convergent rate (see C.1).
4.1	Bilinear Games
Bilinear games are often regarded as an important simple example for theoretically analyzing and
understanding new algorithms and techniques for solving general minimax problems (Gidel et al.,
2019a; Mertikopoulos et al., 2019; Schaefer & Anandkumar, 2019). In this section, we analyze the
convergence property of simultaneous GDA-AM and alternating GDA-AM schemes on the following
zero-sum bilinear games:
min max f(x, y) = xTAy + bTx + cTy, A is full rank.	(7)
x∈Rn y∈Rn
The Nash equilibrium to the above problem is given by (x*, y*) = (--tc, -Tb).
We also investigate bilinear-quadratic games from a spectrum analysis perspective. In addition, we
show that analysis based on the numerical range (Bollapragada et al., 2018) can be also extended to
such games, although it can not help derive a convergent bound for equation 7. Detailed discussion
can be found in Appendix C.1 and C.4.1.
4.2	Simultaneous GDA-AM
Suppose xo and yo are the initial guesses for x* and y*, respectively. Then each iteration of
simultaneous GDA can be written in the following matrix form:
xt+1
yt+1
I	-ηA
ηAT I
'-----{----;
G(Sim)
胤-η M
} l{z}	l{z}
w(Sim)	b(Sim)
(8)
5
Published as a conference paper at ICLR 2022
It has been shown that the iteration in equation 8 often cycles and fails to converge for the bilinear
problem due to the poor spectrum/numerical range of the fixed point operator G(Sim) (Gidel et al.,
2019a; Azizian et al., 2020; Mokhtari et al., 2020a). Next we show that the convergence can be
improved with Algorithm 2.
Theorem 4.1. [Global convergence for simultaneous GDA-AM on bilinear problem] Denote the
distance between the stationary point w* and current iterate w(k+1)p of Algorithm 2 with table size
P as N(k+1)p = ∣∣w* - w(k+1)pk∙ Then we have the following bound for Nt
N(k+1)p ≤ ρ(A)Nk2p	(9)
where P(A) = ( (1 2——ʌ )2. Here, Tp is the ChebyShev polynomial of first kind of degree p and
Tp (1+ K(T )-1)
Tp (1+ K(T2)--) < 1 since 1 + κ(T))-1 > 1.
It is worthy emphasizing that the convergence rate of Algorithm 2 is independent of learning rate η
while the convergence results of other methods like EG and OG depend on the learning rate.
Remark 4.1.1. Both EG and OG have the following form of convergence rate (Mokhtari et al.,
2020a) for bilinear problem
Nt+1 ≤ (I - K(T))N2,
where c is a positive constant independent of the problem parameters.
(b) Different condition number
(a) Eigenvalues of iteration matrix of
SimGDA and GDA-AM
(c) Different table size, condition
number κ = 100
Figure 2: Figure 2a: The blue line is the spectrum of matrix G(Sim) while the red line is spectrum
of matrix I - G(Sim). Our method transforms the divergent problem to a convergent problem due
to the transformed spectrum. Figure 2b: Convergence rate comparison between SimGDA-AM and
EG for different condition numbers of and fixed table size p = 10, 20, 50. Figure 2c: Convergence
rate comparison between SimGDA-AM and EG for increasing table size on a matrix with condition
number 100.
4.3	Alternating GDA-AM
The underlying fixed point iteration in Algorithm 3 can be written in the following matrix form:
xytt++11	= ηAIT I --ηη2AAT yxtt	-η bc
S--------{z-------}l{z}	l{z}
G(Alt)	w(Alt) b(Alt)
According to the equivalence between truncated Anderson acceleration and GMRES with restart, we
can analyze the convergence of Algorithm 3 through the convergence analysis of applying GMRES
to solve linear systems associated with G = I - G(Alt) :
G=
0
-ηAT
6
Published as a conference paper at ICLR 2022
Theorem 4.2. [Global convergence for alternating GDA-AM on bi-
linear problem] Denote the distance between the stationary point
w* and current iterate w(k+1)p of Algorithm 3 with table size P as
N(k+1)p = kw* - w(k+1)p k. Assume is normalized such that its
largest singular value is equal to 1. Then when the learning rate η is
less than 2, we have the following bound for Nt
N(2k+1)p ≤	1+
2η
Figure 3: An illustration of the
where c and rare the center and radius ofa disk D(c, r) which includesspectrum of G (red) and the clos-
all the eigenvalues of G. Especially, C < 1.	ing circle (blue)in Theorem 42
Theorem 4.2 shows that when p >
log q2+η
Ioil
, alternating GDA-AM will converge globally.
2 - η(C Ep
4.4	Discussion of obtained rates
We would like to first explain on why taking Chebyshev polynomial of degree P at the point 1 + κ-ι.
We evaluate the Chebyshev polynomial at this specific point because the reciprocal of this value
gives the minimal value of infinite norm of the all polynomials of degree p defined on the interval
I = [η2。宗in(), η2°max()] based on Theorem 6.25 (page 209) (Saad, 2003). In other words, taking
the function value at this point leads to the tight bound.
When comparing between existing bounds, we would like to point our our derived bounds are hard to
compare directly. The numerical experiments in figure 2b numerically verify that our bound is smaller
than EG. We wanted to numerically compare our rate with EG with positive momentum. However the
bound of EG with positive momentum is asymptotic. Moreover, it does not specify the constants so
we can not numerically compare them. We do provide empirical comparison between GDA-AM and
EG with positive momentum for bilinear problems in Appendix D.1. It shows GDA-AM outperforms
EG with positive momentum. Regarding alternating GDA-AM , we would like to note that the bound
in Theorem 4.2 depends on the eigenvalue distribution of the matrix G. Condition number is not
directly related to the distribution of eigenvalues of a nonsymmetric matrix G. Thus, the condition
number is not a precise metric to characterize the convergence. If these eigenvalues are clustered,
then our bound can be small. On the other hand, if these eigenvalues are evenly distributed in the
complex plane, then the bound can very close to 1.
More importantly, we would like to stress several technical contributions.
1	: OUr obtained Theorem 4.1 and 4.2 provide nonasymptotic guarantees, while most other work are
asymptotic. For example, EG with positive momentum can achieve a asymptotic rate of 1 - O(1/√∕κ)
under strong assumptions (Azizian et al., 2020).
2	: Our contribution is not just about fix the convergence issue of GDA by applying Anderson Mixing;
another contribution is that we arrive at a convergent and tight bound on the original work and not
just adopting existing analyses. We developed Theorem 4.1 and 4.2 from a new perspective because
applying existing theoretical results fail to give us neither convergent nor tight bounds.
3	: Theorem 4.1 and 4.2 only requires mild conditions and reflects how the table size p controls the
convergence rate. Theorem 4.1 is independent of the learning rate η. However, the convergence
results of other methods like EG and OG depend on the learning rate, which may yield less than
desirable results for ill-specified learning rates.
5	Experiments
In this section, we conduct experiments to see whether GDA-AM improves GDA for minimax
optimization from simple to practical problems. We first investigate performance of GDA-AM on
bilinear games. In addition, we evaluate the efficacy of our approach on GANs.
7
Published as a conference paper at ICLR 2022
5.1	Bilinear Problems
In this section, we answer following questions: Q1: How is GDA-AM perform in terms of iteration
number and running time? Q2: How is the scalability of GDA-AM ? Q3: How is the performance of
GDA-AM using different table size p? Q4: Does GDA-AM converge for large step size η?
We compare the performance with SimGDA, AltGDA, EG, and OG, and EG with Negative Momen-
tum(Azizian et al. (2020)) on bilinear minimax games shown in equation 7 without any constraint.
, b, c, and initial points are generated using normally distributed random number. We set the maximum
iteration number as 1 × 106, stopping criteria 1 × 10-5 and depict convergence by use of the norm
of distance to optima, which is defined as ∣∣w* - wt∣∣. Similar to Azizian et al. (2020); Wei et al.
(2021a), the step size is set as 1 after rescaling to have 2-norm 1. We present results of different
settings in Figures 4, 5, and 6.
We first generate different problem size (n = 100, 1000, 5000) and present results of convergence in
terms of iteration number in Figure 4. It can be observed that GDA-AM converges in much fewer
iterations for different problem sizes. Note that EG, EG-NM, and OG converge in the end but requires
many iterations, thus we plot only a portion for illustrative purposes. Figure 5 depicts the convergence
for all methods in terms of time. It can be observed that the running time of GDA-AM is faster
than EG. Although slower than OG, we can observe GDA-AM converges in much less time for all
problems. Figure 4 and Figure 5 answer Q1 and Q2; although there is additional computation for
GDA-AM , it does not hinder the benefits of adopting Anderson Mixing. Even for a large problem
size, GDA-AM still converges in much less time than the baselines.
Next, we run GDA-AM using different table size p and show the results in Figure 6a and Figure
6b. Figure 6a indicates an increasing of table size results in faster convergence in terms of iteration
number, which also verifies our claim in Theorem 4.1. However, we also observe an increased running
time when using a larger table size in Figure 6b. Further, we can see that p = 50 converges in a
comparable time and iterations to p = 100. Similar results are found in repeated experiments as well.
As a result, our answer to Q3 is that although a larger p means less iterations, a medium p is sufficient
and a small p still outperforms the baselines. The optimal choice of p is related to the condition
number and step size, which is another interesting topic in the Anderson Mixing community.
Next, we answer Q4 on convergence under different step sizes. Although GDA-AM usually converges
with suitable step size, our theorem suggests it requires a larger table size when combined with a
extremely aggressive step size. Figure 6c shows the convergence under such circumstance. We can
observe that although a very large step size goes the wrong way in the beginning, Anderson Mixing
can still make it back on track except when η > 1. It answers the question and confirms our claim
that GDA-AM can achieve global convergence for bilinear problems for a large step size η > 0.
6	8	10
Iteration	×ιo5
(b) n = 500	(c) n = 1000
Iteration
Figure 4: Comparison in terms of iteration: minx maxy f(x, y) = xTAy + bTx + cTy. We use
different problem size and fix p = 10, η = 1 for all experiments.
(a) n = 100
5.2	GAN Experiments: Image Generation
We apply our method to the CIFAR10 dataset (Krizhevsky, 2009) and use the ResNet architecture with
WGAN-GP (Gulrajani et al., 2017) and SNGAN (Miyato et al., 2018) objective. We also compared
8
Published as a conference paper at ICLR 2022
20	40	60	80
Time (Seconds)
(b) Time comparison for Figure 4b
40	60	80	100	120
Time (Seconds)
(c) Time comparison for Figure 4c
Time (Seconds)
(a) Time comparison for Figure 4a
(a) Effects of p in terms of iteration
(b) Time compasion for Figure 6a
(c) Effect of step size η, p = 10
Figure 6: Effects of table size p and step size η, n = 500
10210010-210-4
(θ-eωs⅛o-) E-Iou ①□UBωα
R7OG
-A EG
-S-SimAM p=10
∖-G- AItAM p=10
-B-SimAM p=50
-G- AItAM p=50
÷ SimAM p=100
/G AltAM P=IOO I
Figure 5: Comparison between methods in terms of time.
-B-SimAM p=10
卜Θ∙ AItAM p=10
-B-SimAM p=50
→ AltAM p=50
-B-SimAM p=100
-Q AItAM p=100
,	-0-OG：77=2
R ASimGDA-AMR=OJ
、、 +SimGDA-AMR=I
\ 口 -B-SimGDA-AMR=2
s -O AItGDA-AMgOJ
\ 卜十 AItGDA-AMiTZ=I
-Q- AItGDA-AM:%2
the performance of GDA-AM using cropped CelebA (64×64) (Liu et al., 2015) on WGAN-GP. We
compare with Adam and extra-gradient with Adam (EG) as it offers significant improvement over
OG. Models are evaluated using the inception score (IS) (Salimans et al., 2016) and FID (Heusel
et al., 2017) computed on 50,000 samples. For fair comparison, we fixed the same hyperparamters
of Adam for all methods after an extensive search. Experiments were run with 5 random seeds. We
show results in Table 1. Table 1 reports the best IS and FID (averaged over 5 runs) achieved on these
datasets by each method. We see that GDA-AM yields improvements over the baselines in terms of
generation quality.
Table 1: Best inception scores and FID for Cifar10 and FID for CelebA (IS is a less informative
metric for celebA).
WGAN-GP(ResNet)	SNGAN(ResNet)
	CIFAR10		CelebA		CIFAR10		
Method	IS ↑	FID 1	FID	IS	FID
Adam	7.76 ±.11	22.45 ±.65	8.43 ±.05	8.21 ±.05	20.81 ±.16
EG	7.83 ±.08	20.73 ±.22	8.15 ±.06	8.15 ±.07	21.12 ±.19
Ours (GDA-AM )	8.05 ±.06	19.32 ±.16	7.82 ±.06	8.38 ±.04	18.84 ±.13
6	Conclusion
We prove the convergence property of GDA-AM and obtain a faster convergence rate than EG and
OG on the bilinear problem. Empirically, we verify our claim for such a problem and show the
efficacy of GDA-AM in a deep learning setting as well. We believe our work is different from previous
approaches and takes an important step towards understanding and improving minimax optimization
by exploiting the GDA dynamic and reforming it with numerical techniques.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work was funded in part by the NSF grant OAC 2003720, IIS 1838200 and NIH grant
5R01LM013323-03,5K01LM012924-03.
References
Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. Proceedings of Machine Learning Research.
PMLR.
Donald G. Anderson. Iterative procedures for nonlinear integral equations. 1965.
Walss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-JUlien, and Gauthier GideL
Accelerating smooth games by manipulating spectral shapes. In The 23rd International Conference
on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily,
Italy], volume 108 of Proceedings ofMachine Learning Research, pp. 1705-1715. PMLR, 2020.
Raghu Bollapragada, Damien Scieur, and Alexandre d’Aspremont. Nonlinear acceleration of mo-
mentum and primal-dual algorithms. arXiv preprint arXiv:1810.04539, 2018.
Ronald E. Bruck. On the weak convergence of an ergodic iteration for the solution of variational
inequalities for monotone operators in hilbert space. Journal of Mathematical Analysis and
Applications, 1977.
Daniela Calvetti, Bryan Lewis, and Lothar Reichel. On the regularizing properties of the gmres
method. Numerische Mathematik, 91(4):605-625, 2002.
Michel Crouzeix and Cesar Palencia. The numerical range is a (1+2)-SPeCtral set. SIAMJournal on
Matrix Analysis and Applications, 38(2):649-655, 2017.
C. Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with oPtimism.
ArXiv, abs/1711.00141, 2018.
Howard C Elman. Iterative methods for large, sparse, nonsymmetric systems of linear equations.
PhD thesis, Yale University New Haven, Conn, 1982.
Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceleration.
Numerical Linear Algebra with Applications, 16(3):197-221, 2009.
Bernd Fischer and Roland Freund. Chebyshev Polynomials are not always oPtimal. Journal of
Approximation Theory, 65(3):261-272, 1991.
Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A varia-
tional inequality PersPective on generative adversarial networks. In 7th International Conference
on Learning Representations, ICLR, 2019a.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le Priol, Gabriel Huang,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for imProved game dynamics.
In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and
Statistics, Proceedings of Machine Learning Research. PMLR, 2019b.
I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, S. Ozair, Aaron C.
Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. ExPlaining and harnessing adversarial
examPles, 2015.
Anne Greenbaum. Iterative methods for solving linear systems. SIAM, 1997.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
ImProved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, 2017.
10
Published as a conference paper at ICLR 2022
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, 2017.
Yu-Guan Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. On the convergence of single-call
stochastic extra-gradient methods. In NeurIPS, 2019.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In Proceedings of the 37th International Conference on Machine Learning,
Proceedings of Machine Learning Research, pp. 4880-4889. PMLR, 2020.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. ArXiv,
abs/1611.01236, 2017.
Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, and Xiao Wang. Last iterate convergence in
no-regret learning: constrained min-max optimization for convex-concave landscapes. In AISTATS,
2021.
S. Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart J. Russell. Robust multi-agent
reinforcement learning via minimax deep deterministic policy gradient. In AAAI, 2019.
Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. In ICML, pp. 6083-6093, 2020. URL http://proceedings.mlr.
press/v119/lin20a.html.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent
for stochastic nonconvex-strongly-concave minimax problems. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR 2018,, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks, 2019.
Eric V. Mazumdar, Michael I. Jordan, and S. Sastry. On finding local nash equilibria (and only local
nash equilibria) in zero-sum games. ArXiv, abs/1901.00838, 2019.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
gradient mile. In 7th International Conference on Learning Representations, ICLR, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Proceedings of
the 31st International Conference on Neural Information Processing Systems, NIPS’17, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Y. Yoshida. Spectral normalization for
generative adversarial networks. ArXiv, abs/1802.05957, 2018.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artificial Intelligence and Statistics, pp. 1497-1507. PMLR, 2020a.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In Proceedings
of the Twenty Third International Conference on Artificial Intelligence and Statistics, Proceedings
of Machine Learning Research. PMLR, 2020b.
11
Published as a conference paper at ICLR 2022
A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM J.
Optim., 2004.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, and Meisam Razaviyayn. Solving a
Class of Non-Convex Min-Max Games Using Iterative First Order Methods. Curran Associates
Inc., Red Hook, NY, USA, 2019.
Dmitrii M. Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efficient search of first-order nash
equilibria in nonconvex-concave smooth min-max problems, 2021.
Jack Parker-Holder, Luke Metz, Cinjon Resnick, Hengyuan Hu, Adam Lerer, Alistair Letcher,
Alexander Peysakhovich, Aldo Pacchiano, and Jakob Foerster. Ridge rider: Finding diverse
solutions by following eigenvectors of the hessian. In Advances in Neural Information Processing
Systems, 2020.
L. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical
notes of the Academy of Sciences of the USSR, 1980.
Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing, 1986.
Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing
Systems, 2016.
Florian Schaefer and Anima Anandkumar. Competitive gradient descent. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, 2019.
Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algorithms
for smooth minimax optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 2019.
John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton
University Press, 1944.
Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on
NumericalAnalysis, 49(4):1715-1735, 2011a.
Homer F. Walker and Peng Ni. Anderson acceleration for fixed-point iterations. 2011b.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence
in constrained saddle-point optimization. In International Conference on Learning Representations,
2021a. URL https://openreview.net/forum?id=dx11_7vm5_r.
Fuchao Wei, Chenglong Bao, and Yang Liu. Stochastic anderson mixing for nonconvex stochastic
optimization. arXiv preprint arXiv:2110.01543, 2021b.
Yue Wu, Pan Zhou, A. Wilson, E. Xing, and Zhiting Hu. Improving gan training with probability
ratio clipping and sample reweighting. ArXiv, abs/2006.06900, 2020.
Zi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A unified single-loop alternating gradient
projection algorithm for nonconvex-concave and convex-nonconcave minimax problems, 2021.
Minghan Yang, A. Milzarek, Z. Wen, and T. Zhang. A stochastic extra-step quasi-newton method for
nonsmooth nonconvex optimization. arXiv: Optimization and Control, 2019.
12
Published as a conference paper at ICLR 2022
Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay
Chandrasekhar. The unusual effectiveness of averaging in GAN training. In 7th International
Conference on Learning Representations, ICLR , 2019, 2019.
Guodong Zhang and Yuanhao Wang. On the suboptimality of negative momentum for minimax
optimization. In AISTATS, 2021.
Guodong Zhang, Yuanhao Wang, Laurent Lessard, and Roger B. Grosse. Don’t fix what ain’t broke:
Near-optimal local convergence of alternating gradient descent-ascent for minimax optimization.
CoRR, 2021.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I.
Jordan. Theoretically principled trade-off between robustness and accuracy. CoRR, 2019. URL
http://arxiv.org/abs/1901.08573.
Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo. A single-loop smoothed gradient descent-
ascent algorithm for nonconvex-concave min-max problems. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
2020.
13
Published as a conference paper at ICLR 2022
A Related work
There is a rich literature on different strategies to alleviate the issue of minimax optimization. A useful
add-on technique, Momentum, has been shown to be effective for bilinear games and strongly-convex-
strongly-concave settings (Zhang & Wang, 2021; Gidel et al., 2019b; Azizian et al., 2020). Several
second-order methods (Adolphs et al.; Mescheder et al., 2017; Mazumdar et al., 2019; Parker-Holder
et al., 2020) show that their stable fixed points are exactly either Nash equilibria or local minimax
by incorporating second-order information. However, such methods are computationally expensive
and thus unsuitable for large applications such as image generation. Focusing on variants of GDA,
EG and OG are two widely studied algorithms on improving the GDA dynamics. EG proposed to
apply extra-gradient to overcome the cycling behaviour of GDA. OG, originally proposed in Popov
(1980) and rediscovered in Daskalakis et al. (2018); Mertikopoulos et al. (2019), is more efficient by
storing and re-using the extrapolated gradient for the extrapolation step. Without projection, OG is
equivalent to extrapolation from past. Mokhtari et al. (2020b) shows that both of these algorithms can
be interpreted as approximations of the classical proximal point method and did a unified analysis
for bilinear games. These approaches mentioned the GDA dynamics can be viewed as a fixed-point
iteration, but none of them further provides a solution to improve it. In this work, we fill this gap
by proposing the application of the extrapolation method directly on the entire GDA dynamics.
Unlike OG, EG and their variants (Hsieh et al., 2019; Lei et al., 2021; Thekumparampil et al.,
2019; Yang et al., 2019), which regard minimax problems as variational inequality problems (Bruck,
1977; Nemirovski, 2004), our work is from a new perspective and thus orthogonal to these previous
approaches.
In addition, several recent works consider nonconvex-concave minimax problems. Zhang et al.
(2020) introduced a “smoothing” scheme combined with GDA to stabilize the dynamic of GDA. Luo
et al. (2020) proposed a method called Stochastic Recursive gradiEnt Descent Ascent (SREDA) for
stochastic nonconvex-strongly-concave minimax problems, by estimating gradients recursively and
reducing its variance. Lin et al. (2020) showed that the two-timescale GDA can find a stationary point
of nonconvex-concave minimax problems effectively. Ostrovskii et al. (2021) proposed a variant of
Nesterov’s accelerated algorithm to find -first-order Nash equilibrium that is a stronger criterion than
the commonly used proximal gradient norm. Nouiehed et al. (2019) proposed a iterative method that
finds -first-order Nash equilibrium in O(-2) iterations under Polyak-Lojasiewicz (PL) condition.
Focusing on nonconvex minimax problems, they studied an interesting and difficult problem. Since
our work cast insight on the effectiveness of solving minimax optimization via Anderson Mixing, we
expect the extension of this algorithm to general nonconvex problems can be further investigated in
the future.
B	Anderson Mixing Implementation Details
In this section, we discuss the efficient implementation of Anderson Mixing. We start with generic
Anderson Mixing prototype (Algorithm 4) and then present the idea of Quick QR-update Anderson
Mixing implementation as described in Walker & Ni (2011b), which is commonly used in practice.
For each iteration t ≥ 0 , AM prototype solves a least squares problem with a normalization
constraint. The intuition is to minimize the norm of the weighted residuals of the previous m iterates.
Algorithm 4: Anderson Mixing Prototype (truncated version)
Input: Initial point w°, Anderson restart dimension p, fixed-point mapping g : Rn → Rn.
Output: wt+1
for t = 0, 1, . . . do
Set pt = min{t, p}.
Set Ft = [ft-pt, . . . , ft], where fi = g(wi) - wi for each i ∈ [t -pt . . t].
Determine weights β = (β0, . . . , βpt)T that solves minβ kFtβk2 , s. t. Pip=t 0 βi = 1.
Set wt+1 = Pi=t 0 βig (wt-pt+i).
end
The constrained linear least-squares problem in Algorithm AA can be solved in a number of ways.
Our preference is to recast it in an unconstrained form suggested in Fang & Saad (2009); Walker &
Ni (2011b) that is straightforward to solve and convenient for implementing efficient updating of QR.
14
Published as a conference paper at ICLR 2022
Define fi = g(wi) - wi, 4fi = fi+1 - fi for each i and set Ft = [ft-pt, . . . , ft], Ft =
[4ft-pt , . . . , 4ft]. Then solving the least-squares problem (minβ kFtβk2 , s. t. Pip=t 0 βi = 1)
is equivalent to
min T kft - Ftγk2	(10)
γ=(γ0,...,γpt-1)
where α and γ are related by α0 = γ0, αi = γi - γi-1 for 1 ≤ i ≤ pt - 1, and αpt = 1 - γpt-1.
Now the inner minimization subproblem can be efficiently solved as an unconstrained least squares
problem by a simple variable elimination. This unconstrained least-squares problem leads to a
modified form of Anderson Mixing
pt -1
wt+1 = g (wt) -	γi(t) [g (wt-pt+i+1) - g (wt-pt+i)] = g (wt) - Gtγ(t)
i=0
where Gt = [4gt-pt, . . . , 4gt-1] with 4gi = g(wi+1) - g(wi) for each i.
To obtain γ(t) = γ0(t), . . . , γp(t)-1 by solving equation 10 efficiently, we show how the successive
least-squares problems can be solved efficiently by updating the factors in the QR decomposition
Ft = Qt Rt as the algorithm proceeds. We assume a think QR decomposition, for which the solution
of the least-squares problem is obtained by solving the Pt X Pt linear system RY = Q0 * f. Each Ft
is n × pt and is obtained from Ft-1 by adding a column on the right and, if the resulting number of
columns is greater than P, also cleaning up (re-initialize) the table. That is,we never need to delete
the left column because cleaning up the table stands for a restarted version of AM. As a result, we
only need to handle two cases; 1 the table is empty(cleaned). 2 the table is not full. When the table
is empty, we initialize F1 = Q1R1 with Q1 = 4f0/ k4f0k2 and R = k4f0k2. If the table size is
smaller than P, we add a column on the right of Ft-1. Have Ft-1 = QR, we update Q and R so that
Ft = [Ft-ι, ∆ft-ι] = QR. It is a single modified Gram-Schmidt sweep that is described as follows:
Algorithm 5: QR-updating procedures
for i = 1, . . . , Pt-1 do
SetR(i,Pt) = Q(：,i)0 *4ft-ι.
Update 4ft-ι — ∆ft-ι — R (i,Pt) * Q(:,i)
end
Set Q (:, Pt) = 4ft-1/ k4ft-1k2 and R (Pt, Pt) = k∆ft-1k2
Note that we do not explicitly conduct QR decomposition in each iteration, instead we update the
factors (O(P2n)) and then solve a linear system using back substitution which has a complexity of
O(P2). Based on this complexity analysis, we can find Anderson Mixing with QR-updating scheme
has limited computational overhead than GDA (or OG). This explains why GDA-AM is faster than
EG but slower than OG in terms of running time of each iteration.
C Theoretical Results
C.1 Difficulty of analysis on GDA with Anderson Mixing
In the analysis, we study the inherent structures of the dynamics of the fixed point iteration and provide
the convergence analysis for both simultaneous and alternating schemes. We want to emphasize that
the direct application of existing convergence results of GMRES can not lead to convergent results.
A recent paper Bollapragada et al. (2018) study the convergence acceleration schemes for multi-step
optimization algorithms using Regularized Nonlinear Acceleration. We also want to point out that a
naive application of Crouzeix,s bound to the minimax optimization problem can not be used to derive
the convergent result.
Theorem C.1 (Fischer & Freund (1991)). Let n ≥ 5 be an integer, r > 1, and c ∈ R. Consider the
following constrained polynomial minmax problem
min max |P(z)|
p∈Pn=p(c) = 1 Z∈Er
(11)
where
Er := [z ∈ C J |z — 1| + |z + 1∣≤ r + ~∖	(12)
15
Published as a conference paper at ICLR 2022
and c ∈ C \ Er. Then this problem can be solved uniquely by
tn(z; c) :
Tn(Z)
Tn^,
where
Tn(z)=2 {n+Vn), Z=2 (V+V)
if
(a)	|c| ≥ 2 卜√2 + r-√2) or
(b)	|c| ≥ (1∕2ar) Ca — 1 + p2a4 - a* + 1) , where a『：=1 (r + 1).
(13)
(14)
This is because the point 0 where all the residual polynomials take the fixed value of 1 is included
in the numerical range of the iteration matrix, which violates the assumption of Theorem C.1. As
a result, it can not be used to prove that the residual norm is decreasing based on this approach.
Instead, we show that although the coefficient matrix is non-normal, it is diagonalizable. We then give
the convergence results based on the eigenvalues instead of the numerical range. More specifically,
Anderson mixing is equivalent to GMRES being applied to solve the following linear system:
(I - G(Alt))w = b(Alt), with w0 = w0(Alt).	(15)
Writing this linear system in the block form:
-η0AT η2ηAAT w = b(Alt).	(16)
The residual norm bound for GMRES reads:
krtk2=minkp(I-G(Alt))r0k2.	(17)
p∈Pt1
Notice that the matrix (I - G(Alt) ) is non-normal. If we apply Crouzeix’s bound in Crouzeix &
Palencia (2017) to our problem as Bollapragada et al. (2018) did, then we have the following bound
krtk2 ≤ min kp(I- G(Alt))k ≤ (1 + √2) min SUp	∣∣p(z)∣∣	(18)
kr0k2	p∈Pt1	p∈Pt1 z∈W (I-G(Alt) )
where W (I — G(Alt)) = {z*(I — G(Alt) )z, ∀z ∈ C2n \ {0}, ∣∣z∣∣ = 1} is the numerical range for
I - G(Alt). In order to simplify the upper bound in the previous theorem, we study the numerical
range of I - G(Alt) similar to Bollapragada et al. (2018). Writing z = z1 and computing the
z2
numerical range of I - G(Alt) explicitly yields:
[zι,z2] _；AT ηηAτ	Z1 = η2z2ATz2+ ηz1z2 - ηz2Tzι.	(19)
For a general matrix A, there is no special structure about the numerical range of I - G(Alt). However,
when is symmetric, we can decompose as = Pin=1 λiviviT where {λi }in=1 are eigenvalues of in
decreasing order and {vi}in=1 are associated eigenvectors, and write AT = Pin=1 λi2viviT. Then we
can compute the numerical range of G(Alt) as follows:
n
X [z12,z22]
i
0
-ηλiviviT
ηλiviviT
η2λi2viviT
ηλi
η2λi2
T
viT z1
T
viT z2
(20)
Following the techniques proposed in Bollapragada et al. (2018) to analyze the numerical range of
general 2 × 2 matrices, we can show that the numerical range of I - G(Alt) is equal to the convex
hull of the union of the numerical range of
Gi
0
-ηλi
ηλi
η2λi2
, i = 1, . . . , n.
(21)
16
Published as a conference paper at ICLR 2022
And the boundary of numerical range of Gi is an ellipse whose axes are the line segments joining the
points x to y and w to z, respectively, with
η2λ2	η2λ2
X = 0, y = η2λ2,	,w =	- √-1ηl%1, Z = -^L + 7-1^".	(22)
Thus, the numerical range of I- G(Alt) can be spanned by convex hull of the union of the numerical
range of a set of 2-by-2 matrices and the numerical range of each such a 2-by-2 matrix is an ellipse.
We can compute the center o and focal distance d of the ellipse generated by numerical range of
I - G(Alt) explicitly. Then a linear transformation enables us to use Theorem C.1 to show that the
near-best polynomial for the minimax problem on the numerical range of I - G(Alt) is given by
tn(z; C) ：= Tn(~—) if 0 is excluded from the numerical range of I - G(Alt). However, according
Tn( ~d~)
to equation 22 the numerical range includes the point 0 where the residual polynomial takes value
1, thus the analysis based on numerical range can not help derive the convergent result as the upper
bound is not guaranteed to be less than 1.
C.2 Proofs of theorem
We first provide proof of Theorem 4.1.
Theorem C.2 (Global convergence for simultaneous GDA-AM on bilinear problem). Denote the
distance between the stationary point w* and current iterate w(k+i)p ofAlgorithm 2 with Anderson
restart dimension P as N(k+i)p = dist(w*, w(k+i)p). Then we have the following bound for Nt
Algorithm 2 is unconditionally convergent
N(k+1)p ≤ T 门 2	7Nkp
Tp(I + K(T)-1 )
(23)
where TP is the Chebyshev polynomial of first kind of degree P and 臼 1 2——；< 1 since 1 +
lp(1+ K(T )-1 )
—2— > 1
K(T )-1 > 1.
Proof of Theorem 4.1. Note that I - G(SLm) is a normal matrix which will be denoted as G for
notational simplicity. Thus it admits the following eigendecomposition:
G = UΛUl , UUl = I, Λ = diag(λ1,. ..,λ2n).	(24)
Based on the equivalence between GMRES and Anderson Mixing, we know that the convergence
rate of simultaneous GDA-AM can be estimated by the spectrum of G. Especially, it holds that
r(k+1)p = Ufp(Λ)Ul rkp. fp ∈ Pp	(25)
where Pp is the family of residual polynomials with degree p such that fp(0) = 1, ∀fp ∈ Pp.
According to Lemma 2.1, we have the following estimation
Ilr(k + 1)pk2 = min kfp(G)rkpk2 ≤ min max Ifp(X)lkrkpk2.	(26)
fp∈Pp	fp∈Pp L
Due to the block structure of G, the eigenvalues of G can be computed explicitly as
士 ησi √-1, i = 1,...,n,	(27)
where σLis the ith largest singular value of matrix . This shows that the eigenvalues of G are n pairs
of purely imaginary numbers excluding 0 since has full rank.
Since the eigenvalues of G are distributed in two intervals excluding the origin
I = [-ησmax () √ 1, -ησmin () √ 1] ∪ [ησmin () √ 1, ησmax () √ 1],
it can be shown that the following p-th degree polynomial with value 1 at the origin that has the
minimal maximum deviation from 0 on I is given by:
T t t _ τl(q(√-1z))	√ √~T ∖ _ 2	2(√-1z - ησmin)(√-1z + ησmin)	SQ∖
fp(Z)=	Tl(q(0))	,	q(VTz) = 1	(ησmaχ())2 - (ησmin())2 —	(28)
17
Published as a conference paper at ICLR 2022
where l = [P] and Tl is the Chebyshev polynomial of first kind of degree l. The function q(√-1z)
maps I to [-1, 1]. Thus the numerator of the polynomial fp is bounded by 1 on I. The size of
denominator can be determined by the method discussed in Chapter 3 of Greenbaum (1997). Assume
q(0) = 1 (y + y-1),then Tl(q(0)) = 11 (yl + y-l). Then y can be determined by solving
(O) = (ησmax O)2 + (ησmin O)2
q( ) = (ησmaχ())2 - (ησmin())2 .
The solutions to this equation are
=ησmax() + ησmin() 0r = ησmax() - ησmin()
ησmax () - ησmin()	ησmax () + ησmin()
Then plugging the value of q(0) into the polynomial fp yields
kr(k + 1)pk
IIrkPk
≤ 2 ( ,η2σmax() 一 ,η2σmin() Y
― l √η2σmax() + Vη2σ2nin() J
=2 ( σmax () - σmin () λ l = 2( κ() - 1 Y
σmax ()+ σmin()J	lκ() + 1√
(29)
(30)
(31)
Note that Nt and rt is related through G(Wt - w*) = rt. Therefore,
N(k+1)p = kw(k+1)p - w*k2 = ∣∣G-1r(k+1)pk2 = min kGTfp(G)G(Wkp- W*)k2
fp ∈Pp
/	2	、号	(32)
≤ .min max lfp(λi)lkwkp - w*k2 ≤ 2(1 - ∩	-. ) Nkp.
fp∈Pp i	κ() + 1
Actually a tighter bound can be proved after noting that the problem is essentially equivalent to
polynomial minmax problem on the interval:
I = [η2σm in(), η%m ax()],
Then it is well known that,
N(k+1)p ≤ fmiPp λi∈[η2σmimaxη2σmaχ()] lfp(λi)lkwkp - w*k
1
72
Tp(1 + 2 Jh
max min
NkP
)
2
≤
≤ Tp(1 + κ⅜ι) Nkp
where TP Chebyshev polynomial of degree p of the first kind and
(33)
T 门上1 2~~V < 1. Explicitly,
Tp(1+ K(T )-1 )
Tp(I + E—r) = 5 h(1+ Tτ∖—+ + ι∕(1+ Tf∖—))2 - 1)
κ(T) - 1	2	κ(T) - 1	κ(T) - 1
+ (1 + κ(Ty-ι+ S(1 + κT2-1 )2二)pi
□
Next, we give the proof of Theorem 4.2.
Theorem C.3 (Global convergence for alternating GDA-AM on bilinear problem). Denote the
distance between the stationary point w* and current iterate w(k+1)p ofAlgorithm 3 with Anderson
restart dimension P as N(k+1)p = dist(w*, w(k+1)p). Assume is normalized such that its largest
singular value is equal to 1. Then when the learning rate η is less than 2, we have the following
boundfor Nt
N(2k+i)p ≤ J1 + 2¾(C)pN2p
where cand r are the center and radius of a disk D(c, r) which includes all the eigenvalues of G in
equation 4.3. Especially, C < 1.
18
Published as a conference paper at ICLR 2022
Proof. Since the residual rp of AA at p-th iteration has the form of
p
rp = (I-	Gi)r0,
i=1
and AA minimizes the residual, we have
kr(k+1)pk22 ≤ min krkp -βGirkpk22 ≤ min kfp(G)rkpk22,
β	fp ∈Pp
where Pp is the family of polynomials with degree p such that fp(0) = 1, ∀fp ∈ Pp . It’s easy to see
that G is unitarily similar to a block diagonal matrix Λ with 2 × 2 blocks as follows:
0	ησi
-ησi (ησi)2
∀ i ∈ [n].
Thus the eigenvalues of G can be easily identified as
λ±i
(ησi(ησi ± ∙√(ησi)2 - 4))
2
i ∈ [n].
where σι ≥ σ? ≥∙∙∙≥ Qn are the singular values of. Furthermore, the eigenvector and eigenvalue
associated with each 2 × 2 diagonal block are
0	ησi	1	1
-ησi	(ησi )2	λ±i	=	λ±i	λ±i
η i	η i	ησi	ησi
Thus G is diagonalizable and denote the matrix with the columns of eigenvectors of G by X. The
real part of the eigenvalues of G are at least
R(λ±i) ≥ (ησiγ, i ∈ H	(34)
And since ∣ησ∕ ≥ |，(ησi)2 - 4)|, all the eigenvalues will be included in a disk D(c, r) which is
included in the right half plane. Moreover, both C and r being greater than zero indicates that C < 1.
Start from the following inequality:
N(k+i)p = ||w(k+i)p- w*∣∣2 = IIGTr(k+i)p∣∣2 ≤ min IlGTfp(G)rkp∣∣2
fp ∈Pp
=fmin 11GTfp(G)G(Wkp- w*)||2 = min IIG-1(G)(Wkp- w*)∣l2	(35)
fp ∈Pp	fp ∈Pp
=fmin kfp(G)(Wkp- w*)k2
fp ∈Pp
We will use the eigendeomposition of G and the special polynomial (c-t )p to derive the inequality
in Theorem 3. Now we know C < 1. If we choose gp(t) = (c-t )p, we can obtain
min kfp(G)(Wkp- w*)k2 ≤kgp(G)(Wkp-w*)∣∣2
fp ∈Pp
which implies
.min kfp(G)(Wkp- w*)k2 ≤ kgp(XAXT)kk(wkp- w*)k2
fp ∈Pp
Since G is diagonalizable (which has been shown above), we assume the eigendecomposition of G is
G = XΛX-1. Then
.min kgp(G)(Wkp-w*)k2 ≤ kxkkXTk rma2η kgp(A)kk(Wkp-W*)k2 ≤ KG(Ir)pkwkp-w*k2
fp ∈Pp	{λi } n	c
where κG is the condition number of X. The last inequality comes from Lemma 6.26 and Proposition
6.32 in Saad (2003).. Since G and Λ are unitarily similar, κG is equal to the condition number of the
eigenvector matrix of Λ. The eigenvector matrix of Λ is a block diagonal matrix with the ith block
11
as λ+i	λ-i . Thus the singluar values of the eigenvector matrix of Λ is equal to the union of the
-ησi	ησi _l
singular values of these 2-by-2 blocks. Under the assumption that the largest singular value of are
equal to 1 and the learning rate is less than 2, it is easy to find the singular values of the eigenvector
matrix of λ are √2 土 ησ. Thus, kg = √2+ησmax = √2++η = J1+言.
□
19
Published as a conference paper at ICLR 2022
C.3 Discussion of obtained rates
We would like to first explain on Why taking Chebyshev polynomial of degree P at the point 1 + κ⅛ι.
We evaluate the Chebyshev polynomial at this specific point because the reciprocal of this value
gives the minimal value of infinite norm of the all polynomials of degree p defined on the interval
I = [η2σmin(), η2σm2 ax()] based on Theorem 6.25 (page 209) (Saad, 2003). In other words, taking
the function value at this point leads to the tight bound.
When comparing between existing bounds, we would like to point our our derived bounds are hard to
compare directly. Alternatively, we can derive another bound for comparison with existing bounds
for simultaneous GDA-AM.If we use the inequality that Tp(t) ≥ 1 ((t + √t2 - 1)p), we can obtain
the bound P(A) = 4( ^k(atA)-1 )2 = 4(1 - O(	1	)), which is in a form that is comparable
κ(AT A)+1	κ(AT A)
with EG and can compete with EG + positive momentum. The numerical experiments in figure 2b
numerically verify that our bound is smaller than EG. We wanted to numerically compare our rate
with EG with positive momentum. However the bound of EG with positive momentum is asymptotic.
Moreover, it does not specify the constants so we can not numerically compare them. We do provide
empirical comparison between GDA-AM and EG with positive momentum for bilinear problems in
Appendix D.1. It shows GDA-AM outperforms EG with positive momentum. Regarding alternating
GDA-AM , we would like to note that the bound in Theorem 4.2 depends on the eigenvalue distribution
of the matrix G. Condition number is not directly related to the distribution of eigenvalues of a
nonsymmetric matrix G. Thus, the condition number is not a precise metric to characterize the
convergence. If these eigenvalues are clustered, then our bound can be small. On the other hand, if
these eigenvalues are evenly distributed in the complex plane, then the bound can very close to 1.
More importantly, we would like to stress several technical contributions.
1.	Our obtained Theorem 4.1 and 4.2 provide nonasymptotic guarantees, while most other
work are asymptotic. For example, EG with positive momentum can achieve a asymptotic
rate of 1 - O(1/√κ) under strong assumptions (Azizian et al., 2020).
2.	Our contribution is not just about fix the convergence issue of GDA by applying Anderson
Mixing; another contribution is that we arrive at a convergent and tight bound on the original
work and not just adopting existing analyses. We developed Theorem 4.1 and 4.2 from a new
perspective because applying existing theoretical results fail to give us neither convergent
nor tight bounds.
3.	Theorem 4.1 and 4.2 only requires mild conditions and reflects how the table size p controls
the convergence rate. Theorem 4.1 is independent of the learning rate η. However, the
convergence results of other methods like EG and OG depend on the learning rate, which
may yield less than desirable results for ill-specified learning rates.
C.4 Convex-concave and general case
Given the widespread usage of minimax problems in applications of machine learning, it is natural
to ask about its properties when being applied to general nonconvex-nonconcave settings. If f is
a nonconvex-nonconcave function, the problem of finding global Nash equilibrium is NP-hard in
general. Recently, Jin et al. (2020) show that local or global Nash equilibrium may not exist in
nonconvex-nonconcave settings and propose a new notation local minimax as defined below:
Definition 4. A point (x?, y?) is said to be a local minimax point of f, if there exists δ0 > 0 and
a function h satisfying h(δ) → 0 as δ → 0, such that for any δ ∈ (0, δ0], and any (x, y) satisfying
kx - x? k ≤ δ and ky - y? k ≤ δ, we have
f(x?,y) ≤ f (x?, y?) ≤ max f(x,y0).
y0"∣y0-y*k≤h(δ)
Jin et al. (2020) also establishes the following first- and second-order conditions to characterize local
minimax:
Proposition 1 (First-order Condition). Any local minimax point (x*, y*) satisfies Vf (x*, y*) = 0.
Proposition 2 (Second-order Necessary Condition). Any local minimax point (x*, y*) satisfies
Vyyf(x*, y*) 4 0 and Vxxf (x*, y*) - Vxyf (x*, y*)(Vyyf(x*, y*))-1Vyχf (x*, y*) < 0.
20
Published as a conference paper at ICLR 2022
Proposition 3 (Second-order Sufficient Condition). Any stationary point (x*,y*) satisfies
Vyyf(x*, y*) Y 0 and Vxxf (x*, y*) —'Vxyf(X*, y*)(Vyyf(x*, y*))-1Vyχf(x*, y*) a 0 is a
local minimax point.
Given the second-order conditions of local minimax, it turns out that above question is extremely
challenging—GDA-AM is a first-order method. But we can prove the following result for GDA-AM:
Theorem C.4 (Local minimax as subset of limiting points of GDA-AM). Consider a general objective
function f(x, y). The set of limiting points of GDA-AM for minimax problem
min max f(x, y)
x∈Rn y∈Rn
includes the local minimax points of this function.
The definition of local minimax is stronger than that of first order point. The convergence analysis
for complexity of finding stationary point is included in the next section. The proof of Theorem C.4
needs the result from the following theorem.
Theorem C.5 (Calvetti et al. (2002)). Let δ satisfy 0 < δ ≤ δ0 for some constant δ0 > 0 (refer to
Calvetti et al. (2002) for details), and let bδ ∈ X satisfy b - bδ ≤ δ. Let k ≤ ` and let xδk denote
the kth iterate determined by the GMRES method applied to equation Ax = bδ , with initial guess
xδ0 = 0. Similarly, let xk denote the kth iterate determined by the GMRES method applied to equation
Ax = b with initial guess x0 = 0. Then, there are constants σk independent of δ, such that
xk - xδk ≤ σkδ, 1 ≤ k ≤ `
Then, we give the proof of Theorem C.4.
Proof of Theorem C.4. For notational simplicity, We will denote Vxxf (x*, y*), Vxyf (x*, y*) and
Vyy f(x*, y*) by Hx*x* , Hx*y* and Hy* y* , respectively. Simultaneous GDA can be written as
xt - ηVxf(xt, yt)
yt + ηVyf(xt, yt) .
xt+1
yt+1
wt+1
Since the function is differentiable, Taylor expansion holds for Vxf(xt, yt) and Vyf(xt, yt) at a
local minimx point w* = (x*, y*),
Vxf(Xt,yt) = Vxf (x*, y*) + Hx*x*(xt — x*) + Hx*y* (yt - y*) + o(∣∣wt — w*k2)
Vyf(xt, yt) = Vyf(x*,y*) + Hy*y* (yt - y*) + Hy*x*(xt - x*) + o(kwt - w* k2).
Use the fact that Vf(x*, y*) = 0 to simplify the above equations and obtain
Vxf(xt,yt) = Hx*x*(xt —x*) + Hx*y* (yt — y*) + o(kwt —w*k2)
Vyf(xt, yt) = Hy*y* (yt —y*) + Hy*x* (xt —x*) + o(kwt — w* k2).
Inserting the above formulas into the iteration scheme, it yields
wt+1
xt+1
yt+1
I — ηHx* x*
ηHy*x*
—ηHx*y*
I+ηHy*y*
xt +	ηHx*x* x* + ηHx*y* y* +
yt	—ηHy*y* y* — ηHx*y* x* + C
where denotes the higher order error o(kwt — w* k2). According to Theorem 2.2, we know that
simultaneous GDA-AM is equivalent to applying GMRES to solve the following linear system
(I—
(1 — α)I — ηHx*x*
ηHy*x*
—ηHx*y*	、 _ aI + ηHx*x*
(1 — α)I + ηHy*y*	—ηHy*x*
ηHx* y*
αI — ηHy*y*
w = b+C
where b
ηHx*x*x* +ηHx*y*y*
— ηHy*y* y* — ηHx*y* X*
. We now know that GDA-AM
is equivalent to GMRES
being applied to solve the following linear system
αI + ηHx*x*
—nHy* x*
ηHx* y*
αI — ηHy*y*
W = b
21
Published as a conference paper at ICLR 2022
The symmetric part of the coefficient matrix of the above linear system is
aI + ηHx*x*	0
0	αI - ηHy*y* .
According to Proposition 2, αI - ηHy*y* is positive definite since Hy*y* 4 0. If Hχ*χ* is
positive semidefinite, then αI + ηHx*x* is positive definite and we’re done. Otherwise, assume
λmin(Hχ*χ*) < 0. Then for fixed a, when η < -，,(H * *), αI + ηHχ*χ* will be positive definite.
Then according to Theorem 2.2, we know GDA-AM indeed converges. Let’s create a new companion
linear system as follows
αI + ηHx*x*	ηHx*y*
-ηHy* x*	αI - ηHy*y*
W = b + αw*
Note that W = W and GMRES on this companion linear system is convergent under suitable
choice of learning rate η. Let the iterates of GMRES for W, W, W be denoted by Wt, Wt, wh Then
IlWt - Wtk ≤ ∣∣Wt - Wtk + kWt - Wtk. According to Theorem C.5, We also have ∣∣Wt - Wt∣∣ ≤
σke, 1 ≤ k ≤ t. Further more, again according to Theorem C.5, we know ∣Wt - Wt∣ ≤ σk(αW* + e).
Starting from an initial point very close to w* and let t → ∞ and α, e → 0, Wt will converge to
W* = (x*, y*), which means the local minimax w* = (x*, y*) is a limiting point of GDA-RAM. □
Theorem C.6. For strongly-convex-strongly-concave function f (x, y), GDA-AM will converge to
the Nash equilibrium of this function.
Proof: Since strongly-convex-strongly-concave function f(x, y) has unique Nash equilibrium which
is also the unique minimax point, this minimax point must be the limiting point of GDA-AM according
to Theorem C.4.	□
C.4. 1 Bilinear-quadratic games
Moreover, we can further show that the GDA-AM converges on bilinear-quadratic games. Consider a
quadratic problem as follows,
min max f (x, y) = xT Ay + xTBx-yTCy+bTx+cTy,	(36)
x∈Rn y∈Rn
where A is full rank, B and C are both positive definite.
Theorem C.7. [Global convergence for simultaneous GDA-AM on bilinear-quadratic problem] Let
rt(Sim) be the residual of Algorithm 2 being applied to problem equation 36. For some constant
ρ< 1,
U (Sim)U	八	(λmin(Jτ + J))2 ∖ / IIr Ii
krt k2 ≤ C-	4λmaχ(JTJ) )	kr0k2,
'-----------------------}
(37)
{z^
Pt/2
where J
tively.
ηB	ηA
-ηAT	ηC
and λmin and λmax denote the smallest and largest eigenvalue, respec-
The convergence property of GMRES has been studied in the next theorem. We use this theorem to
show the convergence rate of GDA-AM for bilinear-quadratic games.
Theorem C.8 (Elman (1982)). Consider solving a linear system Ex = b using GMRES. Let
rt = b - Ext be the residual at tth iteration. If the Hermitian part of E is positive definite, then for
some positive constant ρ < 1, it holds that
krtk2 ≤
(λmin(EH + E))2 丫/2
4λmaχ (EHE))
{z^^
Pt/2
kr0k2.
}
(38)
1--
22
Published as a conference paper at ICLR 2022
Proof of Theorem C.7. Applying simultaneous GDA-AM to solve the above problem is equivalent
to applying Anderson Mixing on the following fixed point iteration:
xt+1
yt+1
I - ηB	-ηA
ηAT	I - ηC
G(Quad-sim)
w(Quad-sim)
+	]-ηC
^~{^≡}
b(Quad-sim)
(39)
We know that we need to study the convergence properties of GMRES for solving the following
linear system
ηB
-ηAT
ηA
ηC
w = b.
(40)
For notational simplicity, the superscripts has been dropped.
Denote the coefficient matrix
ηB
-ηAT
ηA
ηC
by J. The symmetric part of J is
J + JT _ R(B + BT)	0
^^=[	0	2 (C + CT)
which is positive definite. Then immediately by Theorem C.8, the following convergence rate holds
For some constant 0 < ρ < 1,
krtk2 = min ∣∣P(J)r0 ∣∣2 ≤ (1 - (λmin(j +J) !	kr0k2
p∈Pt1	(4λmax (JT J))
S----------{-----------Z	(41)
Pt/
=夕〃2肛012
□
Note that the convergence of GDA-AM for bilinear-quadratic games can also be analyzed by numerical
range as shown in (Bollapragada et al., 2018). Although we previously show that analysis based on
the numerical range can not help us derive a convergent bound for bilinear games, we show analysis in
Bollapragada et al. (2018) can be extended to bilinear-quadratic games. When B and C are positive
definite, 1 is outside of the numerical range of matrix G(Quad-sim) as shown in 7a. When B or C is
not positive definite, 1 can be included in the numerical range of matrix G(Quad-sim) as shown in 7b.
That is saying analysis based on the numerical range (Crouzeix & Palencia, 2017; Bollapragada et al.,
2018) to the bilinear-quadratic problem can lead to a convergent result when B and C are positive
definite. And analysis based on the numerical range can not help us derive convergent results when
B or C is not positive definite.
C.5 Stochastic convex-nonconvace case
In this section, we study the convergence of GDA-AM for convex-noncovace problem in the stochastic
setting with the same assumptions in Wei et al. (2021b); Xu et al. (2021). The recent work Wei et al.
(2021b) proves the convergence of the stochastic gradient descent with Anderson Mixing for min
optimization. The convergence of GDA-AM for minimax optimization builds on top of it with several
modifications. The minimax problem is equivalent to minimizing a function Φ(∙) = maxy∈γ f (∙, y)
(Lin et al., 2020). And we are interested in complexity of a pair of -stationary point (x, y) instead of
analysis of a point x.
Definition 5. (Lin et al., 2020) A pair of points (x, y) is an -stationary point ( ≥ 0) of a differen-
tiable function Φ if
l∣Vχf(X, Y)Il ≤ E
kPγ (y + (1∕')Vy f(x, y))- y∣∣≤ e/'
Assumption 1. f : Rd 7→ R is continuously differentiable. f(x) ≥ flow > -∞ for any x ∈ Rd.
Vf is globally L-Lipschitz continuous; namely ∣∣Vf (x) — Vf (y)∣2 ≤ Lkx 一 y∣2 forany x,y ∈ Rd.
23
Published as a conference paper at ICLR 2022
(a) Positive definite B and C
(b) Random generated B and C
Figure 7:
I-ηB
ηAT
Numerical range of fixed-point operator (Simultaneous GDA-AM )
G=
-ηA
I-ηC
for bilinear-quadratic games.
Assumption 2. For any iteration k, the stochastic gradient Vfξk (Xk) satisfies Eξk [Vfξk (Xk)]=
▽f (Xk), Eξk ʊ∣Vfξk (xk) - Nf(Xk )|同 ≤ σ2, where σ > 0, and ξk, k = 0,1,..., are indepen-
dent samples that are independent of {Xi }k
Theorem C.9. For a general convex-nonconcave function f, suppose that Assumptions 1 and 2
hold. Batch size nt = n for t = 0,... ,N — 1. C > 0 is a constant. βt = 4L(1/C-1).δt ≥
Cβ-2, 0 ≤ αt ≤ min {l, βt2 } and at is chosen to make sure the positive definiteness of Ht. Let
R be a random variable following PR (t) d=f Prob{R = t} = 1/N, and N be the total number
of stochastic GDA-AM calls needed to calculate stochastic gradients ▽fSt (wt) in our algorithm.
To ensure E [(▽ f (wR )∣∣ ] ≤ e, total number of stochastic GDA-AM calls needed to calculate
stochastic gradients ▽fSt (wt) is O(-4).
Recall that we can recast GDA scheme as the following fixed point iteration.
wt+1 = G(ηsim) (wt) , wt + ηV (wt) with w =
-▽xf(x, y)
▽yf(x, y)
Ignoring the stepsize η and let Wt and Rt record the first and second order diffrence of recent m
iterates:
Wt = [∆wt-m, ∆wt-m+ι ,…，Δwt-1 ] , Rt = [∆Vt-m, ∆Vt-m+1,…, ∆Vt-1]
Similarly as Wei et al. (2021b),the Anderson mixing can be decoupled into
Wt+1 = Wt - WtΓt,	(Projection step)
Wt+1 = Wt + βt Vt,	(Mixing step)
where βt is the mixing parameter, and Vt = Vt — WtΓt and Γt is solved by
Γt=argmin ∣Vt - RtΓ∣2 + δt∣Γ∣2
Γ∈Rm
We want to argue that similar arguments in Wei et al. (2021b) can be applied to the problem here. To
see why Anderson mixing works for minimax optimization, we assume function f is smooth. Then
the hessian matrix for G(ηsim) is
H	-▽2xxf -▽2xyf
H =	▽2yx f	▽2yy f
24
Published as a conference paper at ICLR 2022
Notice that in a small neighborhood of wt+1, we have
Rt=-HWt=(-品-Mf) Wt
Thus kVt - RtΓk2 ≈ kVt + HWtΓk2, which is equivalent to solving for a vector pt such that
Hpk = Vt . This is exactly the second order method for the fixed point iteration problem. Also at
each step the AM is minimizing the residual, the reason that AM is equivalent to GMRES for linear
problem is that this quadratic approximation is exact. Finally, we rewrite AM as the quasi-newton
framework as Wei et al. (2021b) did. wt+1 = wt + HtVt where
min kHt - βtIkF subject to HtRt = -Xt
Ht
Finally, with damping parameter, Anderson mixing has the following form
Wt+1 = Wt + βtVt - αt (Wt + βtRt) Γt	(42)
we can also apply the very similar arguments to prove key results in lemma 1, lemma 2 in Wei
et al. (2021b). There is also a key difference with Wei et al. (2021b). Here we are considering
minimax optimization problem. Thus our gradient is actually V (w)
x ,y
rather than
Vy f (X, y)
Vxf(X, y)
Vyf(X, y)
Vf (W)
This will introduce some difficulty to the dynamics of the fixed pointe
iteration. However, noticing that kV k = kVf (W)k and
f (wt+1) ≤ f (Wt)	+	Vf (Wt)T (wt+1	— Wt)	+ 2 ∣∣Wt+1	— Wtk2
≤ f (Wt)	+	Vf (Wt)T (Wt+1	— Wt)	+ L ∣∣Wt+ι	— Wtk2	(43)
=f (Wt) + Vf (Wt)T HtVt + L ∣HtVtk2
where
Vf(W ʌ _ -Vxf (x, y)	(44
Vf (Wt) =	Vyf(X, y)	(44)
we call this the ascent-descent gradient (ADG) which is the gradient for minimax optimization
problem
min max f(x, y).
x∈Rd y∈Rd
TT
To see why Vf (Wt)T (Wt+1 — Wt) ≤ Vf (Wt)T (Wt+1 — Wt), we consider their difference
(V — V)f (Wt)T (Wt+ι - Wt) = -2Vχf (xt, yt)T(xt+ι - xt).
For fixed yt, f(xt, yt+1) has the Talyor expansion:
f(xt+1,yt) = f(xt, yt)+Vxf(xt, yt)T (xt+1-xt)+(xt+1-xt)T Vxxf(xt+θ(xt+1-xt), yt)(xt+1-xt)
Assuming f is convex w.r.t x and apply safeguard to ensure f(xt+1, yt) ≤ f(xt, yt) can guarantee
T
(V - V)f (Wt) (Wt+1 - Wt) ≥ 0. Now applying lemmas in Wei et al. (2021b), we can derive the
convergence of our method for general convex-nonconcave function similarly.
D Additional Experiments
D.1 Comparison with EG with Positive Momentum
In this section, we include additional comparison between GDA-AM and EG with positive momentum.
GDA-AM has two big theoretical advantages over EG with positive momentum. First, convergence
of GDA-AM does not require strong assumptions on choices of hyperparamters. Second, 4.1 and 4.2
provide nonasymptotic guarantees while convergence of EG with positive mometum is asymptotic.
Experimental results are shown in 8. It indicates GDA-AM outperforms EG with positive momentum.
Finding a good choice of the inner and outer step size of EG and momentum term is hard. For EG
with positive momentum, we set the step size of extrapolation step as 1, the step size of update as 0.5,
and the positive momentum term as 0.3 after grid search as shown in 8b and 8c. On the other hand,
GDA-AM converges fast for different step size without hyperparameter tuning.
25
Published as a conference paper at ICLR 2022
Φ-EOS⅛O-) IJLUoU əouElSQ
I-∣-SimGDA
-* AItGDA
→-OG
-»・EG
-。・ EG-NM
-V1 EG-PM
→-SimGDA-AM
∣-B-AltGDA∙AM
-> EG-PM:片0.2
-■»- EG-PMF=0.5
-0- EG-PMF=I
SimGDA-AM: η=0.2
-4-SimGDA-AM: rj=0.5
-H-SimGDA-AMp=I
-A EGTM: >0,1
-+- EG-PM: /3=0.3
EG-PM: >0.5
-÷-SimGDA-AM: 7;=1
(b) Effects of step size η . β is fixed (c) Effects of momentum term β . η
as 0.3.	is fixed as 1.
Figure 8: Additional Comparison between GDA-AM and EG with positive momentum
D.2	1d Minimax functions
We begin with investigating the empirical performance of GDA-AM for 6 non-trivial 1d bivariate
functions. We set initial points as (3, 3) and m as 20 or 5 for all functions. We use optimal learning
rates for all methods on each problem. Results are shown in Figure 9, 10, 11, 12, 13 and 14. We
observe GDA-AM consistently outperforms all baselines and improves convergence. It is worthwhile
to mention that the difference between GDA-AM and traditional averaging is twofold. First, traditional
averaging does not involve an adaptive averaging scheme and thus blindly converge to (0, 0) for all
1d bivariate functions. In contrast, GDA-AM obtains optimal weights by solving a small linear system
on past iterates. Using different weights for each iteration, GDA-AM is able to minimize the residual
of past iterates and thus find the solution of a fixed-point iteration. More importantly, averaging
does not change the GDA dynamic because averaging generates a new sequence of parameters based
on GDA iterates. This means averaging is independent with base training algorithm (GDA here).
However, GDA-AM changes the dynamic directly by overwriting the latest iterate. It means Anderson
Mixing interacts with GDA, which is another major difference from averaging.
10-21
10^25
O 20	40	60	80 IOO
Iteration
---SimGDA
--AItGDA
-Averaging
→- EG
→- OMD
→- SimGDA-AM
+ AItGDA-AM
Figure 9: f(x, y) = (x - 1 )(y - 1) + 1 e-(x-0.25) -(y-0.75) ). The optima for this function is not
(0, 0). Because averaging blindly converges to (0, 0), it can never find the correct solution.
D.3 Density estimation
To test our proposed method, we evaluate our method on two low-dimension density estimation
problems, mixture of 25 Gaussians and Swiss roll. For both generator and discriminator, we use fully
connected neural networks with 3 hidden layers and 128 hidden units in each layer. Except for the
output layer of discriminator that uses a sigmoid activation, we use tanh-activation for all other layers.
We run Adam and GDA-AM for 50000 steps. The learning rate is set as 2× 10-4 and β1 = 0, β2 = 0.9
after an extensive grid search, which is close to the maximal possible stepsize under which the methods
rarely diverge. Figure 15 and 16 show the output after {1K, 10K, 30K, 50K} iterations. It can be
seen that our method converges faster to the target distribution offers a improvement over Adam. In
addition, we can observe that the generated samples using our method gather around the circle and
are less connected with other circles.
26
Published as a conference paper at ICLR 2022
Figure 10: f(x, y) = (4x2 - (y - 3x + 0.05x3)2 - 0.1y4)e-0.01(x2+y2) . All baselines except
averaging are cyclying around the optima. Averaging is converging slowly.
Figure 11: f(x, y) = -3x2 - y2 + 4xy. Baselines tend to diverge. Averaging is converging slowly
again because averaging can only blindly converge to (0, 0) and the optima for this function is (0, 0).
D.4 Robust Neural Network Training
In this section, we test the effectiveness of GDA-AM by training a robust neural network on MNIST
data set against adversarial attacks (Madry et al., 2019; Goodfellow et al., 2015; Kurakin et al., 2017)
. The optimization formulation is
N
min max	' (f (Xi + δ； W) ,yi)	(45)
W i=1 δi, st∣δi∣∞≤ε
where w is the parameter of the neural network, the pair (xi, yi) denotes the i-th data point, and δi is
the perturbation added to data point i. The accuracy of our formulation against popular attacks, FGSM
(Goodfellow et al., 2015) and PGD (Kurakin et al., 2017), are summarized in Table 2.. Since solving
such problem is computationally challenging, Nouiehed et al. (2019) proposed an approximation of
the above optimization problem with a new objective function as the following nonconvex-concave
problem:
N9	m
mWn X max X tj' (f (xK； W) ,Ui) , T = (tι,∙∙∙ ,tm) | X t = 1,t ≥ 0	(46)
i=1	j=0	i=1
where K is a parameter in the approximation, and xiKj is an approximated attack on sample xi by
changing the output of the network to label j . We use the public available implementation (Nouiehed
et al., 2019) 2. We apply our algorithm on top of (Nouiehed et al., 2019) and compare our results
(p = 50) with (Madry et al., 2019; Zhang et al., 2019; 2020; Nouiehed et al., 2019). Results are
summarized in table 2. We can observe that GDA-AM leads to a comparable or slightly better
performance to the other methods. In addition, GDA-AM does not exhibit a significant drop in
accuracy when is larger and this suggests the learned model is more robust.
2https://github.com/optimization-for-data-driven-science/Robust-NN-Training
27
Published as a conference paper at ICLR 2022
★
O-L
1 1
① UU£SQ
Start
Optima
Figure 12: f(x, y) = 1 x3 + y2 + 2xy - 6x - 3y + 4.
Figure 13: f (x, y) = x3 - y3 - 2xy + 6.
---SiinGDA
——AItGDA
-Averaging
→- EG
→- OMD
→- SimGDA-AM
T- AItGDA-AM
50 IOO 150	200
Iteration
D.5 Image Generation
In this section, we provide additional experimental results that are not given in Section 5. Figure
18a and 18b show the Inception Score for CIFAR10 using WGAN-GP and SNGAN. It can be
observed that our method consistently performs better than Adam and EG during training. Further, on
CIFAR-10 using WGAN-GP and SNGAN, GDA-AM is slightly slower than Adam (about 110-115%
computational time), but significantly faster than EG (about 65-75% computational time).
D.6 Details on the experiments
For our experiments, we used the PyTorch 3 deep learning framework. Experiments were run one
NVIDIA V100 GPU. The residual network architecture for generator and discriminator are sum-
marized in Table 3 and 4. We use a WGAN-GP loss, with gradient penalty λ = 10. When using
the gradient penalty (WGAN-GP), we remove the batch normalization layers in the discriminator.
When using SNGAN, we replace the batch normalization layers with spectral normalization. Hyper-
paramters of Adam are selected after grid search. We use a learning rate of 2 × 10-4 and batch size
of 64. For table size of GDA-AM , we set it as 120 for CIFAR10 and 150 for CelebA. We set β1 = 0.0
and β2 = 0.9 as we find it gives us better models than default settings.
3https://pytorch.org/
	Natural	FGSM L∞	一				PGD40L∞	
		ε=0.2	ε = 0.3	ε=0.4	ε=0.2	ε=0.3	ε=0.4
Madry et al. (2019)	98.58%	96.09%	94.82%	89.84%	94.64%	91.41%	78.67%
Trade: ε = 0.35	97.37%	95.47%	94.86%	79.04%	94.41%	92.69%	85.74%
Trade: ε = 0.40	97.21%	96.19%	96.17%	96.14%	95.01%	94.36%	94.11%
Nouiehed et al. (2019)	98.20%	97.04%	96.66%	96.23%	96.00%	95.17%	94.22%
Zhang et al. (2020)	98.89%	97.87%	97.23%	95.81%	96.71%	95.62%	94.51%
GDA-AM	98.61%	97.75%	97.74%	97.75%	96.47%	95.91%	95.41%
Table 2: Test accuracies under FGSM and PGD attack. Trade refers to Zhang et al. (2019).
28
Published as a conference paper at ICLR 2022
-4	-2	O	2	4	O 20	40	60	80 IOO
Iteration
Figure 14： f (x, y) = 2x2 + y2 + 4xy + 3y3 - 1 y4.
(a) Adam
(b) GDA-AM
(a) Adam
Figure 16： Swiss roll： Evolution plot of Adam and GDA-AM . Green dots are observed points and
red dots are generated points.
Figure 15： 25 Gaussians： Evolution plot of Adam and GDA-AM . Green dots are observed points
and red dots are generated points.
(b) GDA-AM
29
Published as a conference paper at ICLR 2022
Figure 17: FID (lower or ] is better) for CIFAR 10
Figure 18: Left: IS for CIFAR10 using WGANGP. Middle: IS for CIFAR10 using SNGAN. Right:
FID for CelebA using WGANGP.
Figure 19: Generated Images for CIFAR10 and CelebA using WGAN-GP(ResNet)
30
Published as a conference paper at ICLR 2022
Table 3:	ResNet architecture used for our CIFAR-10 experiments.
Generator
Input: Z ∈ R128 〜N(0,I)
Linear 128 → 256 × 4 × 4
ResBlock 128 → 128
ResBlock 256 → 256
ResBlock 256 → 256
Batch Normalization
ReLu
transposed conv. (256, kernel:3 × 3, stride:1, pad: 1
tanh(∙)
Discriminator
Input: X ∈ R3 × 32 × 32
Linear 128 → 128 × 4 × 4
ResBlock 128 → 128
ResBlock 128 → 128
ResBlock 128 → 128
Linear 128 → 1
Table 4:	ResNet architecture used for our CelebA (64 × 64) experiments.
Generator
Input: z ∈ R128 〜N(0, I)
Linear 128 → 512 × 8 × 8
ResBlock 512 → 256
ResBlock 256 → 128
ResBlock 128 → 64
Batch Normalization
ReLu
transposed conv. (64, kernel:3 × 3, stride:1, pad: 1
tanh(∙)
Discriminator
Input: x ∈ R3 × 64 × 64
Linear 128 → 128 × 4 × 4
ResBlock 128 → 128
ResBlock 128 → 256
ResBlock 256 → 512
Linear 512 → 1
31