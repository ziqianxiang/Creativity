Published as a conference paper at ICLR 2022
DictFormer: Tiny Transformer with Shared
Dictionary
Qian Lou, Ting Hua, Yen-Chang Hsu, Yilin Shen, Hongxia Jin
Samsung Research America
{qian.lou, ting.hua, yenchang.hsu, yilin.shen, hongxia.jin}@samsung.com
Ab stract
We introduce DictFormer with efficient shared dictionary to provide a compact,
fast, and accurate transformer model. DictFormer significantly reduces the redun-
dancy in the transformer’s parameters by replacing the prior transformer’s parame-
ters with compact, shared dictionary, few unshared coefficients and indices. Also,
DictFormer enables faster computations since expensive weights multiplications
are converted into cheap shared look-ups on dictionary and few linear projections.
Training dictionary and coefficients are not trivial since indices used for looking
up dictionary are not differentiable. We adopt a sparse-constraint training with
l1 norm relaxation to learn coefficients and indices in DictFormer. DictFormer is
flexible to support different model sizes by dynamically changing dictionary size.
Compared to existing lightweight Transformers, DictFormer consistently reduces
model size over Transformer on multiple tasks, e.g., machine translation, abstrac-
tive summarization, and language modeling. Extensive experiments show that
DictFormer reduces 3.6× to 8.9× model size with similar accuracy over multiple
tasks, compared to Transformer.
1	Introduction
Transformer (Vaswani et al., 2017) has been widely used in natural language processing (NLP) for
its superior capability in capturing long-distance dependencies. However, its good performance
comes with the giant model size. For example, T5 (Raffel et al., 2019) with a hidden dimension
of 65K and GPT-3 (Brown et al., 2020) with 96 transformer blocks have 11 billion and 175 bil-
lion parameters, respectively. These large Transformers suffer from multiple severe issues, such as
complicated learning and difficult deployment on mobile/IoT devices. First, during the training of
a big transformer model, large training corpora (Raffel et al., 2019; Brown et al., 2020) or careful
regularization (Merity et al., 2017; Mehta et al., 2021) are required. Furthermore, the trained model
is over-parameterized (Reid et al., 2021; Zhao et al., 2021). The large model size with 11 billion and
175 billion parameters is beyond the capabilities of many edge devices including mobile devices and
IoTs. Therefore, there is an urgent need to design parameter-efficient and fast transformer model
that eliminates redundant parameters and enables real-time NLP applications on the edge.
Weights sharing is a choice proved by ALBERT (Lan et al., 2020) for designing compact and ef-
ficient pre-trained transformer encoders, like BERT, on self-supervised learning task. However,
directly sharing all weights in an encoder-decoder transformer for a sequence-to-sequence task like
machine translation will dramatically decrease performance and accuracy (Reid et al., 2021). Al-
though a recent framework Universal Transformer (UT) (Dehghani et al., 2019) shows that a vanilla
transformer’s accuracy can be improved by using recursive weight sharing, UT’s high accuracy
comes at the cost of deeper blocks or wider hidden dimensions, which significantly enlarges the
computational cost and does not necessarily reduce the model size.
This paper introduces a new compact, fast, and accurate transformer architecture, DictFormer, that
can be easily trained and deployed on edge devices. DictFormer depends on dictionary sharing
and unshared linear projection coefficients instead of weights sharing. Specifically, a shared dic-
tionary among all encoder/decoder blocks can significantly reduce the parameter redundancy and
therefore compresses the model size. Few unshared linear projection with coefficients on the shared
dictionary enable each encoder/decoder block to have distinct layer-wise feature representations,
1
Published as a conference paper at ICLR 2022
thus improving the representation abilities compared to prior weights sharing. Also, DictFormer
provides a method to dynamically control the layer-wise representation abilities by using the group-
wise shared dictionary for layers with large-dimension features, e.g., Feed-forward Network (FFN).
Last but not least, we show that training dictionary and coefficients are not trivial since indices used
for looking up dictionary are not differentiable. We convert coefficients and indices into a sparse
matrix and train it with l1 norm relaxation and convert this sparse matrix into dense coefficients
and indices during inference.
Extensive experiments demonstrate that our DictFormer provides significant improvements over
existing transformers on three sequence-to-sequence tasks, (i) machine translation, (ii) abstractive
summarization, and (iii) language modeling. For machine translation, on IWSLT 2014 German-
English, DictFormer attains transformer performance with 8.9× fewer parameters and 2× fewer
Multi-Adds; on WMT 2014 German-English, DictFormer brings about 4.9× model compression
and 1.9× computation reduction; on WMT 2014 English-French, DictFormer obtains consistent
performance improvements: 4.9× model compression and 1.9× less computation with a similar
score. For abstractive summarization, DictFormer reduces the model size by more than 4.7× on
CNN-DailyMail dataset. For language modeling, DictFormer matches the performance of trans-
former with 3.6× to 5.7× fewer parameters than transformer on WikiText-103 benchmark.
2	Related Work and Motivation
Related lightweight Transformers. Several methods have been proposed to design lightweight
transformers. The first line of research is to reduce the transformer computation complexities by
redesigning self-attention mechanism including (Katharopoulos et al., 2020; Zhou et al., 2021; Ra-
ganato et al., 2020; You et al., 2020; Correia et al., 2019; Kaiser et al., 2018). These methods cannot
reduce model size. The second line of research is model compression, e.g., quantization (Prato
et al., 2020; Lou et al., 2020b), pruning (Behnke & Heafield, 2020), low-rank factorization (Ma
et al., 2019), and knowledge distillation (Wang et al., 2020). These two directions of research can
be combined with our Dictformer. The third line of research is efficient architecture design (So
et al., 2019; Wu et al., 2020; Mehta et al., 2021) by improving the expressiveness of transformers.
The forth line of research is weights sharing (Xia et al., 2019; Ma et al., 2019; Dehghani et al.,
2019; Reid et al., 2021; Takase & Kiyono, 2021) by reusing parameters across transformer blocks.
Weights sharing cannot reduce computations. Our Dictformer falls into the third and forth category.
We show that our Dictformer with Dictionary sharing can reduce both model size and computations.
Transformers with Weights Sharing. Weight sharing is surprisingly effective to compress model
size for discriminate NLP models based on Transformer encoders, e.g., BERT. For example, prior
work ALBERT (Lan et al., 2020) shows that even sharing all parameters across layers does not
introduce any accuracy reduction. However, for generative sequence-to-sequence models based on
transformer’s encoders and decoders, sharing all parameters will significantly decrease accuracy
on multiple standard machine translation or language modelling tasks (Reid et al., 2021; Takase &
Kiyono, 2021). To match vanilla Transformer’s accuracy, multiple works (Reid et al., 2021; Takase
& Kiyono, 2021; Xia et al., 2019; Ma et al., 2019) only share weights across partial layers instead
of all layers. However, partial weights sharing remarkably brings down the model size compression
effect of weights sharing. Also, how to decide which layers should be shared in partial weights
sharing is difficult due to the large and dynamic search space that is dependent on the specific tasks.
Transformer with all-parameters sharing such as Universal Transformer (Dehghani et al., 2019)
matches or improves transformer’s performance at the cost of a wider or deeper transformer archi-
tecture. A wider transformer with a larger embedding dimension enlarges the model size and brings
larger computations (Mult-Adds). A deeper transformer with more encoder/decoder blocks does
not only increase model size, but also introduces more computations. Importantly, weights sharing
techniques cannot reduce Mult-Adds numbers and training/inference time. Figure 6(a) in Appendix
shows the comparisons of Transformers with weights sharing and our dictionary sharing. Weights
sharing techniques cannot solve the deployment challenges of transformers on resource-limited de-
vices for real-time NLP applications.
2
Published as a conference paper at ICLR 2022
DiCtFormer
model size/
Universal
Transformer
De DeLighT
DiCtFormer-tiny	Subformer Tied Trasnformer
C)Lite TranSformer
O Evolved Transformer
■
Transformer
ZSmaIIer CirCIe means fewer OperatiOnS
WMUIt-Adds: 500M 1500M	4500M
29
28
n 27
LU
2 26
25
24
5	10	15	20	25	30	35	40	45	50
# ParamS (in million)
Figure 1: DictFormer achieves similar BLEU score with prior works using fewer parameters and
MUlt-Adds on WMT 2014 De-En translation. DictFormer is flexible and scalable, e.g., for mo-
bile devices, DiCtFOrmer-tiny can reduce 2.2× model size of lite transformer (WU et al., 2020)
that achieves state-of-the-art performance under mobile settings. Results of existing works come
from their own implementations and #ParamS does not include embedding parameters, e.g., Trans-
former (Vaswani et al., 2017) has 44M #Params.
Motivation of DictFormer with Dictionary Sharing.
Figure 1 shows that although transformers with weights sharing including Universal Trans-
former (Dehghani et al., 2019), Tied Transformer (Xia et al., 2019), and Subformer (Reid et al.,
2021) significantly reduce the model size over vanilla transformer and lightweight transformer De-
LighT (Mehta et al., 2021), they cannot reduce the #Mult-Adds and even suffer from larger #Mult-
Adds. Particularly, Tied Transformer (Xia et al., 2019) and Subformer (Reid et al., 2021) compress
transformer model by 1.7× 〜 2.6× but cannot reduce #MUlt-Adds. Universal Transformer (De-
hghani et al., 2019) achieves 〜 1 BLEU score improvement with 1.4 × less parameters, but the
#MUlt-AddS is increased by 〜4.3×, thereby significantly prolonging the NLP inference latency
and restraining the deployment of real-time NLP applications on edge devices. To enable the de-
ployment of transformer models on mobile devices, recent work Evolved Transformer (So et al.,
2019) and Lite Transformer (Wu et al., 2020) try to design new lightweight transformer architecture
to meet the defined mobile settings, e.g., 10 million parameters, but their tiny architectures suffer
from a large accuracy decrease. As Figure 1 shows, Evolved Transformer and Lite Transformer lose
2.9 and 2.4 BLEU score compared to base transformer, respectively.
3 DictFormer
Overview. We propose DictFormer with dictionary sharing to enable a fast, compact, and accurate
transformer. When matching transformer’s accuracy, DictFormer can reduce more than 3.6× param-
eters and 〜3× Mult-Adds shown in Figure 1, which outperforms prior transformers with a higher
BLEU score. Also, DictFormer is flexible to compress model size given an accuracy threshold. For
instance, when DictFormer matches the accuracy of Lite Transformer, it could further compress the
model of lite transformer by 〜2.2×. Given a N-layer transformer model, we can easily transform
it into DictFormer by converting all the weights in N blocks into one shared dictionary and few
unshared look-up coefficients, shown in Figure 2. For example, N -layer weights attention WiA and
FFN WiF, where i ∈ [0, N - 1], in Transformer (Figure 2(a)) are represented by smaller dictio-
naries DA, DF that are shared by N blocks, and N -layer unshared coefficients CiA and CiF for
DictFormer (Figure 2(b)). Meanwhile, the attention and FFN operations in vanilla Transformer are
replaced with shared-dictionary attention and group-wise shared-dictionary FFN whose details are
introduced in our following contents. Dictformer reduces Transformer’s #Params from O(d2N) to
O(d(m + tN)). This is a O(dN/(m + tN))× model size reduction since dictionary size m < d,
coefficient size t << d, where the embedding size is d. Dictformer also reduces #Mult-Adds from
O(d2 N n) to O(dN n(m + t)), where n is input sequence length. The details on how to define and
calculate #Params and #Mult-Adds are shown in Appendix A.1.
3
Published as a conference paper at ICLR 2022
N
x
#Params: O(d2N)
#MUlt-Adds: O(d2Nn)
(a) Transformer : Weights WiA & WiF are not shared
Figure 2: Our DictFormer replaces N -layer unshared, large attention and FFN weights WiA , WiF
(i ∈ [0, N - 1]) in transformer with smaller, shared dictionaries DA, DF and coefficients CiA,
CiF . The Mult-Adds operations between weights and inputs in Transformer are also reduced by our
dictionary look-ups and few linear projections with coefficients.
DiCt-Attention
i
Embed ]
Embed
DiCt-Attention
d
4d
Dict-FFN
i
DeCoder BloCk
EnCoder BloCk
DF
m
v____DiCt-FFN
Dict-Attention ■
J
N x
#Params: O(d(m+tN)) #Mult-Adds: O(dNn(m+t))
(b) Our DiCtFormer: DiCtionaries (DA& DF) are shared by N bloCks

Shared-dictionary Attention. Given a N -layer transformer model, we define that Qi , Ki , and Vi
are the i-th layer query, key, and values. Same to Transformer (Vaswani et al., 2017), we also use
equation 1 to calculate the attention scores once we have query, key, and values.
AttentiOn(Q i, Ki,Vi) = softmax(Qi fi ) ∙ V	(1)
d
Our DictFormer utilizes equation 2 instead of MuItiHead(Qi,Ki,Vi) = MHi ∙ WO Used in
Transformer (Vaswani et al., 2017) to compute multi-head values, where MHi is derived according
to equation 3, and DA, CiO, and IiO are used to linearly project MHi, instead ofWiO. The attention
value head for each head j in layer i is computed by equation 4 instead of head = Atention(Q%∙
WQj, Ki ∙ WKj, Vi ∙ WVj) used in existing transformers.
M ultiH ead(Qi , Ki , Vi) = SD(M Hi , DA , CiO , IiO)
MHi = COncat(headi1 , ..., headih)
(2)
(3)
headij = AttentiOn(SD(Qi, DA, CiQj, IiQj), SD(Ki, DA, CiKj, IiKj), SD(Vi, DA, CiVj, IiVj)) (4)
The reason why we use the shared attention dictionary DA , indices Ii and coefficients Ci to replace
the larger and unshared weights WiQj , WiKj, WiVj and WiOj used in previous transformers is that
the linear projections of lookup ofDA with coefficients CiX can have similar representation abilities
as WiXj, where X represents inputs type, e.g., X = Q means input is query. To be specific, DA, Ii,
and Ci have size ofd × mA, tA × d, and tA × d, respectively. mA and tA are the dictionary size and
coefficient size in attention. As equation 5 shows, the linear projections in transformer’s attention,
e.g., WQj ∙ Qj, are replaced by our lightweight shared dictionary projection function (SD), e.g.,
SD(Qj, DA, CiQj , IiQj ), derived by equation 6 and equation 7. SD(Qj, DA, CiQj , IiQj ) replaces
WQj ∙ Qj since WQj can be replaced by looking up DA and few linear projections with coefficients
CiQj without accuracy decrease. The following paragraphs and Figure 3(a) introduce the details of
look-up and scaling using equation 6 and equation 7.
Qj ∙ WQj ⇒ SD(Qj, DA,cQj ,lQj)
(5)
4
Published as a conference paper at ICLR 2022
In equation 6, the lookup of DA by indices IiQj is defined by DA[:, IiQj [t, id]] that can fetch the
IiQj [t, id]]-column vector from DA; Unshared linear projection that is used to enlarge the represen-
A
tation ability of shared dictionary is depicted by Wi j = Pt=1 Ci j [t, id] DA [:, Ii j [t, id]], where
represents scaling a fetched vector from a dictionary with a scalar in coefficients. Therefore, lin-
Q	]Q
early projecting Qj With Wi according to WQj ∙ Qjis replaced by W^j ∙ Qjm equation 6 when
Q Q	]Q	Q
we find proper DA, Ii j ,Ci j to meet that Wi j and WiQj have similar representation abilities,
]Q
e.g., they have matched accuracy. Directly generating WiQj and multiplying it with Qj potentially
increases the computations. To tackle this problem, we compute the multiplications between Qi
and dictionary DA as Oi first according to equation 7, and reuse Oi for the following look-ups and
scaling with CiQj .
tA
SD(Qi, DA ,CQj, IQ) = Qi ∙ X CQj [t,id] © DA [:, IQj [t,id]]
t=1
tA
=(X Qi ∙ DA [:, IQj [t,id]]) © CQj [t,id]	(6)
t=1
tA
=X Oi[:, IQj [t,id]] © CQj [t,id],id ∈ [0, d]
t=1
θi[:,b] = Qi ∙ DA[：,b],b ∈ [1,mA]	(7)
We use Figure 3(a) to show an example of equation 5 about how to lookup dictionary with in-
Index IiF2
Coeffi. CiF2=1
Coeffi. CiFg=2
[1,7, 27]
[0.3, 0.1, 0.6]
[0.1, 0.6, 0.3]
Coefficients CiQ
Index IiQ
tF2|
2∣ -dɪn;^	tF2∣ ~d~^√lj tF2∣ -dʌr^
[0.2, 0.3, 0.5]
0.3
+
N-layer Weight WF2
GroUP-2 Dictionary DF2
(a) Replacing weight with shared-dictionary, index, coefficients (b) Replacing weight with group-wise dictionary and coefficients
d
N-layer Weight WQ
[1,13, 35]
L-E二口二二二二二二二
1 13 35	0.2
d/
~mA^^
Shared Dictionary DA
Figure 3: (a) An example of looking up dictionary with indices and scaling it with coefficients. This
lookup and scaling is able to reconstruct N -layer weight. (b) An example of looking up group-wise
dictionary and its scaling for large-dimension representation projection.
dices and scaling it with coefficients. This process can create a N -layer weight WQ . For example,
given the shared dictionary DA, to generate the first column of weight WiQ , the first column of
index matrix IiQ is taken out, e.g., [1, 13, 35], and is used as indices to fetch the the corresponding
columns from DA, e.g., DA[:][1], DA [:][13], DA [:][35]. Then the first column of coefficients CiQ,
e.g., [0.2, 0.3, 0.5], are multiplied with DA[:][1], DA [:][13], DA [:][35] respectively, and the sum of
multiplication works as the first column of WiQ . In this way, weights W in attention with 4d2N pa-
rameters are compressed into mA, IQ, and CQ with size dmA + 8tAdN. For example, DictFormer
can reduce 8.7× model size when d = 512, mA = 256, tA = 24, N = 6. More calculation details
can be found at appendix A.1.
Group-wise Shared-dictionary FFN. The FFN in prior transformer includes two-layer compu-
tations: (i) Fi = max(0,Xi ∙ WF1 + bi), (ii) F2 = Fi ∙ WF2 + b2. Instead of the regular
linear projections Xi ∙ WF1 + bi and Fi ∙ WF2 + b2, DictFormer uses a new lightweight pro-
jection called group-wise shared dictionary projection (GSD), i.e., GSD(Xi, D, CiF1 , IiF1 , G) and
5
Published as a conference paper at ICLR 2022
tA ~d~τ>^hl	tAι~d~X>^1
5] IndeX Ii	[0.2, 0.3, 0.5]COeffiCientS G
Shared Oi
n[l I
mA
Figure 4: Since index I in DictFormer is not differentiable, we train sparse coefficients Z instead of
jointly training I and C. After training, sparse Z is converted to I and C for deployment.
[0.2, 0.3, 0.5止
Attention Results S
Sparse Coefficients Z

≈
GSD(F1, D, CiF2, IiF2 , G), in equation 8 and equation 9 to compute the FFN. Here GSD projection
is same as SD in shared-dictionary attention when G = 1. In SD projection, it works well to replace
a N weights of d × d with a d × m dictionary where each column of dictionary is multiplied with
a scalar in coefficients. However, when the column of dictionary is large, e.g., 4 × d in the second
layer of FFN, SD projection is difficult to achieve an accurate model since an vector of dictionary
with 4 × d, e.g. 2048, multiplied by a same scalar is not flexible enough. To increase the flexibility
of SD projection and improve performance, GSD firstly divides each column of dictionary into G
groups equally and assign different scalars to multiply the numbers in each group.
F1 = max(0, GSD(Xi, D, CiF1, IiF1, G) +b1)
F2 = GSD(F1, D, CiF2, IiF2, G) +b2
(8)
(9)
Equation 10 and equation 11 show the computations of GSD. Dictionary DF2 [:, :] is divided
equally into G groups and the g-th group is defined as DF2 [I ndexg, :], where Indexg = [(g -
1) 4Gd ,g 4Gd) and g ∈ [1,G]. Also, the multiplication result between g-th group dictionary and input
is defined as Og[:, b] shown in equation 11, where b ∈ [1, mF2]. Different groups of dictionary
share the same look-up indices IiF2 . The tF2 queried vectors in each group will be scaled by its
corresponding coefficients CiF2g and the scaled results are then accumulated to derive the g-th GSD
result shown in equation 10. In addition, G group-wise GSD are further summed to compute the
final GSD result.
G tF2	g
GSD(X, DF2, CiF2, IiF2, G) = X X Og[:, IiF2[t, id]]	CiF2 [t, id]	(10)
g=1 t=1
Og[:, b] = X[:, Indexg] ∙ DF2 [Indexg,b],g ∈ [1,G],b ∈ [1,mF2],In,d^eXg = [(g - 1)* g^l)	(11)
GG
Figure 3(b) shows an example of group-2 shared dictionary for the the second layer of FFN. The
dictionary DF2 is equally split into two parts that share the same indices IiF2 but each group has
Fg=1	Fg=2
unique coefficients, e.g., Ci 2 and Ci 2 . The first group and the second group use the corre-
sponding coefficients, e.g., [0.3, 0.1, 0.6] and [0.1, 0.6, 0.3], to scale the queried dictionary vectors,
respectively. The accumulation of scaled vectors can be used to represent the N -layer weight WF2 .
Training DictFormer via Constraints and Relaxation. DictFormer replaces transformer weights
with linear projections of a dictionary, thereby having three-step computations including small pro-
jections, lookup and scale. DictFormer computes a small multiplication between input and dictio-
nary, generating intermediate variable O, looking up O and scaling it with coefficients. To train the
DictFormer, one should jointly optimize dictionary, index I and coefficients C . Directly training
the Dictformer leads to a combinatorial optimization problem since index I is non-continuous. Al-
though one could use AutoML, such as evolutionary method and Reinforcement learning to jointly
learn dictionary, I and C, these methods suffer from a large training time with low performance. To
workaround the training of index I, we reformulate the method to a regular linear projection with
sparse constraints, which can efficiently train the DictFormer.
6
Published as a conference paper at ICLR 2022
We convert the index Ii and coefficients Ci into sparse coefficients Z, so that the training of index
Ii and coefficients Ci is replaced with training sparse coefficients Z. During the deployment phase,
the sparse coefficients Z are reversely transformed into index Ii and coefficients Ci shown in Figure
4. In particular, the shape of coefficients C and index I is tA × d × N . The shape of sparse Z
is mA × d × N . There are two steps to derive Z: Initializing all elements as 0 and copying C
elements to Z according to the index values in Ii. For example, since the first column of I1 and C1
is [1, 13, 35] and [0.2, 0.3, 0.5], all entries of the first column in Z are zeros except the 1st entry, 13th
entry, and the 35th entry are 0.2, 0.3, and 0.5. Therefore, the lookup and scale ofOi can be converted
into the matrix multiplication between Oi and Zi shown in equation 12. The new coefficient C has a
sparsity constraint that requires that the non-zero elements (l0 norm) of each column is tA << mA
shown in equation 13. Now we can train Z with ||ZiQj [:, id]||l0 = tA, instead of training I and C.
tA
X Oi[.,I[t,id]] Θ CQj [t,id] = Oi ∙ ZQj	(12)
t=1
||ZiQj[:,id]||l0 =tA	(13)
The l0 norm sparsity constraint in equation 13 is non-differentiable and we cannot directly train Z.
To achieve the training with sparsity, we are inspired by existing sparse training methods Tsuruoka
et al. (2009); Li et al. (2016) to firstly loose this constraint to a l1 norm constraint shown in equa-
tion 14 to penalize the non-zero parameters and clip the unimportant values that are near to zeros
using equation 16. To be specific, the gradients of coefficients are calculated by equation 15, where
parameter λ is used to control the trade-offbetween the gradient ofloss L, δδL, and lι norm sparsity
constraint.
d
||Z||l1 = X ||ZiQj [:, id]||l1
id
δ(L+λ⅛) = δL +λsign(Z)
δZ	δZ
(14)
(15)
Equation 16 is a threshold function during training. During the backpropagation, the derivative of
threshold function is 1 if |x| > value(ρ), 0 otherwise. During the forward phase, Equation 16 can
be used to globally clip the near-zero values to zero given a ratio ρ, where value(ρ) derives the value
at ratio ρ in the ascending order. One also can force that at most t of the elements in each column of
Z are non-zero while the rest are set to 0 by letting value(ρ) is the t-th largest value of each column.
μ(x) = (x,
if |x| > value(ρ).
otherwise.
(16)
4	Experimental Methodology
Machine Translation Dataset. Three machine translation benchmarks are tested: IWSLT’14
German-English (De-En), WMT’14 English to German (En-De), and WMT’14 English to France
(En-Fr). For IWSLT’14 De-En, we adopt the same settings in Wu et al. (2020) with 160K training
sentence pairs and 10K joint byte pair encoding (BPE) vocabulary in lower case. For WMT’14
En-De, our models are trained with 4.5M sentence pairs, validated on newstest2013, and tested on
newstest2014. Also, a 32K joint source and target BPE is used in the vocabulary. For WMT’14 En-
Fr, we follow the setup in Wu et al. (2020) by training models on 36M training sentence pairs from
WMT’14, validating on newstest2012 and 2013, and testing on newstest2014. The vocabulary has a
size of 40K and is based on a joint source and target BPE factorization. The evaluation setting used
in Vaswani et al. (2017) is utilized with a beam size of 4 and a length penalty of 0.6. We replicate
the same BLEU calculations in Wu et al. (2020) with case-sensitive tokenization. The last 10 model
checkpoints are averaged for testing and the lowest-perplexity model is picked for validation.
Abstractive Summarization Dataset. We evaluate DictFormer on CNN-DailyMail dataset (Chen
et al., 2016) that has 280K news articles with multi-sentence summaries. We follow the settings
7
Published as a conference paper at ICLR 2022
in (Wu et al., 2020), truncate the articles to 3000 tokens, use a 30K BPE vocabulary and F1-Rouge
as the metric.
Language Modeling Dataset. We also evaluate DictFormer on WIKITEXT-103 (Merity et al.,
2016) that has a 260K BPE vocabulary and contains 103M/217K/245K tokens for training, valida-
tion, and testing. Language modeling performance about perplexity (ppl) is reported.
Architecture and Evaluation. For machine translation tasks, we follow the sequence-to-sequence
transformer (Vaswani et al., 2017) with encoder and decoder to develop our model. For IWSLT
dataset, the general settings are same as (So et al., 2019). For WMT dataset, the setting is based
on (Wu et al., 2020). Also, the same model for machine translation is used for summarization task.
For language modelling, we use the transformer-based decoder architecture of (Baevski & Auli,
2019) but with 512 model dimensions and 12 blocks that is same to (Wu et al., 2020). Fairseq’s
transformer implementation (Ott et al., 2019) is used as the backbone for the baseline model.
In our DictFormer architecture, encoder and decoder have three dictionaries, respectively, i.e., en-
coder attention dictionary DeAnc, encoder F1 dictionary DeFn1c, encoder F2 dictionary DeFn2c, and
decoder attention dictionary DdAec, decoder F1 dictionary DdFe1c, decoder F2 dictionary DdFe2c. Each
of them is shared by all blocks in encoder or decoder. For example, DeAnc is shared by all attention
blocks in encoder. We study the effects of DictFormer with various dictionary size from 80 to 240
and different coefficient size from 12 to 84 shown in Figure 8. One can pick up a dictionary size and
coefficient size to control the trade-off between accuracy and model size according to the require-
ments. For example, one could get the results in our tables when dictionary size is set as mA = 2d5,
mF1 = 2d5, mF2 = 2 ,and tA = 10, tF1 = tF2 = -d.
In this paper, #Params means parameter numbers of transformer without including embedding lay-
ers; #TParams shows parameter numbers of model and embedding layers. The Mult-Adds calcu-
lation is tested when a model is used to translating two sentences with the length of 30 in default
(same to previous work Wu et al. (2020)).
Table 1: Comparison with state-of-the-art transformers on machine translation tasks. For a fair com-
parison with existing tiny transformer, We follow Lite Transformer (Wu et al., 2020) to scaling down
the hidden size to meet the deployment requirements on mobile settings. For example, Transformer
uses a 128 hidden size and 2.8M #Params; #Ops represents Mult-Adds and Entries with ± shows
the average across three independent runs.
IWSLT’14 De-En	WMT’14 En-De	WMT’14 En-Fr
#ParamSRatiOS BLEU ABLEU#OPS性ParamSRatiOS BLEU ABLEU#OPSl BLEU ∆BLEU
Transformer (Vaswani et al., 2017)	2.8M	1.0×	27.8	-	63M	2.8M	1.0×	21.3	-	87M	33.6	-
Universal Trans (Dehghani et al., 2019)	2.1 M	1.5×	30.3	+2.5	152M	2.6M	1.1 ×	22.4	+1.1	193M	35.8	+2.2
Tensorized. Trans (Ma et al., 2019)	1.4M	2×	28.0	0.2	-	1.7M	1.6×	21.2	-0.1	-	33.4	-0.2
Lite Transformer (Wu et al., 2020)	2.3 M	1.2×	29.6±0.4	+1.8	46M	1.8M	1.6×	21.8±0.3	+0.6	47M	35.3±0.4	+1.7
DeLighT (Mehta et al., 2021)	1.1 M	2.5×	28.9±0.2	+1.1	49M	1.6M	1.8×	21.8±0.1	+0.5	64 M	34.2±0.2	+0.6
Subformer (Reid et al., 2021)	1.3M	2.2×	27.6±0.3	-0.2	61M	1.9M	1.5 ×	21.0±0.2	-0.3	85M	33.5±0.4	-0.1
DictFormer	0.3 M	8.9×	30.0±0.3	+2.2	32M	0.6M	4.9×	22.0±0.2	+0.7	46M	35.6±0.3	+2.0
5	Results
Machine Translation. We first rePort the machine translation results and comPare them with Prior
works. Our baseline transformer model settings are in line with Lite Trans. (Wu et al., 2020) which
Provides the best results under mobile settings. Our DictFormer generally outPerforms the state-of-
the-art transformers on IWSLT’14 De-En, WMT’14 En-De, and WMT’14 En-Fr. Table 1 rePresents
the quantitative results of our DictFormer. ComPared to Transformer (Vaswani et al., 2017), Dict-
Former comPresses 8.9× model size, reduces 1.9× #Mult-Adds, and imProves 2.2 BLEU score on
IWSLT’14 De-En, which achieves better trade-off between Performance and model size than Pre-
vious works. Similar to IWSLT’14 De-En, our exPeriments on larger corPora including WMT’14
En-De and En-Fr also show DictFormer obtains more comPact and accurate model than existing
Transformers. For instance, only our DictFormer can comPress more than 6× model size over
Transformer without decreasing any model Performance under mobile settings.
8
Published as a conference paper at ICLR 2022
Table 2: Comparison with state-of-the-art transformers on abstractive summarization and Language
modeling tasks. #Ops is calculated by longer sequence with the input length of 100. Entries with ±
represents the average across three independent runs.
CNN-DailyMail for abstractive summarization R1	R2	RL	#Ops #ParamS Ratios	WIKITEXT103 for language modeling Valid ppl. Test ppl. #Ops #ParamS Ratios
TranS./Adaptive	41.4	18.9	38.3	3.6G	41.4M	1,0 × Lite Transformer	41.3	18.8	38.3	1.5G	17.3M	2.4 × DelighT	-	-	-	-	-	- Subformer	41.6	19.2	38.4	-	41M	1.01	× DictFormer	41.3±0.2 18.9±0.1 38.3±0.1 0.9G	8.9M	4.7×	23,2	24.0	50.3G	37.8M	1,0	× 21,4	22.2	48.7G	37.2M	1.01 × 23,5	24.1	-	〜33M	1.1	× 20,8	21.1	〜50G	〜18M	2.1	× 21.3±0.2	22.2±0.2	20.3G	10.0M	3.8	×
Abstractive Summarization and Language Modeling. We then report the abstractive summariza-
tion and language modeling results and compare them with prior works. Abstractive summarization
model follows the transformer model, but language modeling model follows the adaptive inputs
model (Baevski & Auli, 2019). Our DictFormer consistently outperforms prior transformers on
both summarization and language modeling tasks. Table 2 represents the quantitative results of our
DictFormer. For abstractive summarization on CNN-DailyMail, DictFormer achieves similar F1-
Rouge score with Transformer but requires 4.7× less model size and 〜4× less computations. For
language modeling on WIKITEXT103, compared to adaptive inputs model (Baevski & Auli, 2019),
DictFormer compresses 3.8× model size with less computations and matched testing perplexity.
Sensitive Study and Ablation Study. We study the DictFormer’s performance under different
model size and hyper-parameters in Appendix. We also use table 3 in Appendix to show the ablation
study of DictFormer’s techniques. The performance of DictFormer improves with an increase in the
number of model parameters, across different corpora. DictFormer achieves similar BLEU score
with Transformer using fewer model parameters. Although DictFormer’s performance improves
with an increase in the size of dictionary and coefficients, the model size is also enlarged. So one
should pick up a proper dictionary or coefficient size according to accuracy requirements.
6	Conclusion
We propose DictFormer with dictionary sharing to enable a fast, compact, and accurate transformer.
DictFormer significantly reduces both model size and computation parameters of prior transformers
with similar performance. Also, DictFormer can consistently improve the performance on multiple
tasks, such as machine translation, abstractive summarization, and language modeling with similar
model size. DictFormer code is available at https://github.com/SamNLP/DictFormer.
9
Published as a conference paper at ICLR 2022
References
Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=ByxZX20qFQ.
Maximiliana Behnke and Kenneth Heafield. Losing heads in the lottery: Pruning transformer atten-
tion in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 2664-2674, 2020.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,
abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.
Danqi Chen, Jason Bolton, and Christopher D. Manning. A thorough examination of the cnn/daily
mail reading comprehension task. CoRR, abs/1606.02858, 2016. URL http://arxiv.org/
abs/1606.02858.
Goncalo M Correia, Vlad Niculae, and Andre FT Martins. Adaptively sparse transformers. arXiv
preprint arXiv:1909.00015, 2019.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=HyzdRiR9Y7.
Bo Feng, Qian Lou, Lei Jiang, and Geoffrey Fox. CRYPTOGRU: Low latency privacy-preserving
text analysis with GRU. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing, pp. 2052-2057, Online and Punta Cana, Dominican Republic, November
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.156.
Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam
Shazeer. Fast decoding in sequence models using discrete latent variables. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 2390-2399. PMLR, 10-15 Jul
2018. URL https://proceedings.mlr.press/v80/kaiser18a.html.
AngeIoS Katharopoulos, APoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International Conference on Ma-
chine Learning, pp. 5156-5165. PMLR, 2020.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=H1eA7AEtvS.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Qian Lou and Lei Jiang. She: A fast and accurate deep neural network for encrypted data. In
Advances in Neural Information Processing Systems, pp. 10035-10043, 2019.
Qian Lou, Bo Feng, Geoffrey C. Fox, and Lei Jiang. Glyph: Fast and accurately training deep neural
networks on encrypted data. CoRR, abs/1911.07101, 2019.
Qian Lou, Song Bian, and Lei Jiang. Autoprivacy: Automated layer-wise parameter selection for
secure neural network inference. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8638-8647.
Curran Associates, Inc., 2020a.
10
Published as a conference paper at ICLR 2022
Qian Lou, Feng Guo, Minje Kim, Lantao Liu, and Lei Jiang. Autoq: Automated kernel-wise neural
network quantization. In International Conference on Learning Representations, 2020b.
Qian Lou, Yilin Shen, Hongxia Jin, and Lei Jiang. Safenet: A secure, accurate and fast neural
network inference. In International Conference on Learning Representations, 2021.
Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song.
A tensorized transformer for language modeling. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf.
Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
Delight: Deep and light-weight transformer. In International Conference on Learning Represen-
tations, 2021. URL https://openreview.net/forum?id=ujmgfuxSLrO.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-
guage models. arXiv preprint arXiv:1708.02182, 2017.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint
arXiv:1904.01038, 2019.
Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fully quantized transformer for machine
translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1-
14, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
findings-emnlp.1. URL https://aclanthology.org/2020.findings-emnlp.1.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.
Alessandro Raganato, Yves Scherrer, and Jorg Tiedemann. Fixed encoder self-attention patterns in
transformer-based machine translation. CoRR, abs/2002.10260, 2020. URL https://arxiv.
org/abs/2002.10260.
Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo. Subformer: Exploring Weight Sharing
for Parameter Efficiency in Generative Transformers. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021, Punta Cana, Dominican Republic, November 2021. Association
for Computational Linguistics.
David R. So, Chen Liang, and Quoc V. Le. The evolved transformer. CoRR, abs/1901.11117, 2019.
URL http://arxiv.org/abs/1901.11117.
Sho Takase and Shun Kiyono. Lessons on parameter sharing across layers in transformers. CoRR,
abs/2104.06022, 2021. URL https://arxiv.org/abs/2104.06022.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. Stochastic gradient descent training
for L1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Confer-
ence ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pp. 477-485, Suntec, Singapore, August 2009. Association
for Computational Linguistics. URL https://aclanthology.org/P09-1054.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, E UkaSz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa- Paper.pdf.
11
Published as a conference paper at ICLR 2022
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-
attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint
arXiv:2002.10957, 2020.
Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short
range attention. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ByeMPlHKPH.
Yingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and Tao Qin. Tied transformers: Neural machine
translation with shared encoder and decoder. In The Thirty-Third AAAI Conference on Artifi-
cial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial In-
telligence, EAAI2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 5466-5473.
AAAI Press, 2019. ISBN 978-1-57735-809-1. URL https://aaai.org/ojs/index.
php/AAAI/article/view/4487.
Weiqiu You, Simeng Sun, and Mohit Iyyer. Hard-coded gaussian attention for neural machine
translation. arXiv preprint arXiv:2005.00742, 2020.
Changsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and Hongxia Jin. Automatic mixed-precision
quantization search of bert. Proceedings of the Thirtieth International Joint Conference on Artifi-
cial Intelligence, Aug 2021. doi: 10.24963/ijcai.2021/472. URL http://dx.doi.org/10.
24963/ijcai.2021/472.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings
of AAAI, 2021.
12
Published as a conference paper at ICLR 2022
A APPENDIX
A.1 DictFormer benefits WITH dictionary sharing
The block comparison of T ansformer and our DictFormer.
u。AU 考 Pe ①'Al NZH
noitnettA DS
Ops:
dn2
SD (Eq.
Paras #:
8NdtA+mAd
Ops:
Nnd(mA+tA)
Paras #:
3dmF+32NdtF
Nnd(3mF+32tF+
(b) Our DictFormer block
Figure 5: The block comparison of (a) transformer and (b) our DictFormer.
Figure 5 further depicts the comparison of transformer encoder block and DictFormer encoder block.
The i-th Transformer block including attention and FFN has four weights WiA and WiF . WiA has
four weights including query WiQj, key WiKj, value WiVj, and output WiOj, thereby having 4Nd2
parameters and 4N nd2 Mult-Adds given a sequence of size n. WiF has two parts: WiF1 and WiF2 .
N -layer FFN blocks have 8Nd2 parameters and 8N nd2 Mult-Adds operations. In contrast, Dict-
Former significantly reduces the parameters and Mult-Adds of Transformer. Specifically, each Dict-
Former block replaces the attention and FFN with shared dictionary (SD) attention and group-wise
shared dictionary (GSD) FFN. Linear projection is performed by looking up dictionary DA using
Eq.5. SD Attention has 8N dtA + mAd parameters and N nd(mA + tA) operations. GSD FFN
has 3dmF + 32N dtF parameters and N nd(3mF + 32tF ) operations, where t, m are the size of
coefficient and dictionary. To simplify the calculations of #params and #Mult-Adds, we can let
tA = tF, mA = mF. Dictformer reduces Transformer’s #Params from O(d2N) to O(d(m + tN)).
This is a O(dN/(m + tN))× model size reduction since dictionary size m < d, coefficient size
t << d, where the embedding size is d. Dictformer also reduces #Mult-Adds from O(d2 N n) to
O(dN n(m + t)).
A.2 The comparison between weight sharing and dictionary sharing
Weight sharing is surprisingly effective to compress model size for discriminate NLP models based
on Transformer encoders, e.g., BERT. For example, prior work ALBERT (Lan et al., 2020) shows
that even sharing all parameters across layers does not introduce any accuracy reduction. However,
for generative sequence-to-sequence models based on transformer’s encoders and decoders, sharing
all parameters will significantly decrease accuracy on multiple standard machine translation or lan-
guage modelling tasks (Reid et al., 2021; Takase & Kiyono, 2021). To match vanilla Transformer’s
accuracy, multiple works (Reid et al., 2021; Takase & Kiyono, 2021; Xia et al., 2019; Ma et al.,
2019) only share weights across partial layers instead of all layers. However, partial weights sharing
remarkably brings down the model size compression effect of weights sharing. Also, how to decide
which layers should be shared in partial weights sharing is difficult due to the large and dynamic
search space that is dependent on the specific tasks.
13
Published as a conference paper at ICLR 2022
• Wider: ds > d f large model size & Mult-Adds#
• Deeper: Ns > N 玲 Mult-Adds#
(a) Transformer with weight sharing: wider or deeper (inefficient)
Figure 6: (a) Transformer with weights sharing. It contains three parts including embedding, Ns
encoder blocks and Ns decoder blocks. Each encoder/decoder block contains attention and Feed-
Forward Network (FFN). The embedding size is ds and FFN feature size is 4 × ds . Weights in the
i-th Attention and FFN are denoted as WiA and WiF. To match or improve accuracy of transformer
w/o weights sharing, transformer w/ wights sharing should be wider (ds > d) or deeper (Ns > N),
where d and N are the embedding size and blocks number of transformer w/o wight sharing. (b)
Our DictFormer with dictionary sharing. WiA and WiF are represented by smaller dictionaries DA ,
DF, and coefficients CiA and CiF, where dictionary size m < d, coefficient size t << d.
N x
• m < d & tiny t & N blocks f Tiny model & Mult-Adds#
(b) DictFormer: efficient transformer with dictionary sharing
Transformer with all-parameters sharing such as Universal Transformer (Dehghani et al., 2019)
matches or improves transformer’s performance at the cost of a wider or deeper transformer archi-
tecture. A wider transformer with a larger embedding dimension enlarges the model size and brings
larger computations (Mult-Adds). A deeper transformer with more encoder/decoder blocks does
not only increase model size, but also introduces more computations. Importantly, weights shar-
ing techniques cannot reduce Mult-Adds numbers and training/inference time. Figure 6(a) shows
the comparisons of Transformers with weights sharing and our DictFormer with dictionary sharing.
Weights sharing techniques cannot solve the deployment challenges of transformers on resource-
limited devices and obtain real-time NLP applications. To reduce both model size and computations
of Transformers, we introduce DictFormer with dictionary sharing instead of previous weights shar-
ing. In particular, DictFormer shares dictionary across all layers so there is no need to decide which
layers should be shared. Also, DictFormer with few unshared look-up coefficients does not require
a wider embedding size or deeper encoder/decoder to improve accuracy. Rather than compressing
model size by parameters sharing, DictFormer with dictionary sharing can also enable computations
sharing to reduce running latency. Therefore, our DictFormer provides a compact, fast, and accurate
transformer model for sequence-to-sequence NLP tasks.
A.3 Training settings
To fairly compare with our baselines, all of our training settings are in line with (Wu et al., 2020).
For machine translation tasks, a dropout of 0.3 is used and the dropout ratio is linearly scaled down
when we shrink the dimension of the embeddings for WMT datasets. The learning rate linearly
warms up from 10-7 to 10-3 followed by a cosine annealing with a single cycle and Adam optimizer
same as (So et al., 2019) and (Wu et al., 2020). Inverse linear square root learning rate scheduling
is used for IWSLT De-En. The summarization training settings are the same as machine translation.
To train models on language modeling task, we follow the training setting in (Baevski & Auli,
2019), but decrease the dropout rate by half in FFN layers. The training experiments of WMT,
summarization, and language modeling are conducted on 8 NVIDIA Tesla V100 GPUs. IWSLT
De-En is trained on two GPUs. Machine translations tasks are trained for 50K steps, but language
modelling tasks are trained for 286K steps. We further describe deployment settings in Appendix.
14
Published as a conference paper at ICLR 2022
35
34
33
ɔ 32
% 31
CQ 30
29
28
27
0 2 4 6 8 10 12 14 16
Parameters (in million)
ɔ
LU
-I
CQ
42
40
38
36
34
32
30
Parameters (in million)
(b) WMT/14 En-Fr
35
30
25
20
0 10 20 30 40 50 60 70
Parameters (in million)
(a) IWSLT’14 De-En
(c) WIKITEXT-103
Figure 7: The performance of DictFormer improves with an increase in the number of model pa-
rameters, across different corpora.
A.4 DEPLOYMENT SETTINGS.
Deploying deep learning models including transformer on powerful cloud-based servers as a ser-
vice suffers from data privacy issues when users, data is private or confidential (Lou & Jiang, 2019;
Lou et al., 2019). In addition, the inference cost of privacy-sensitive applications that performed
over encrypted data using fully homomorphic encryption is prohibitively expensive (Feng et al.,
2021; Lou et al., 2021; 2020a). (i) Mobile settings. Scaling up transformer models leads to higher
machine translation performance, but those large architectures are not suitable for real-world appli-
cations, especially when we should deploy them on edge devices where computation and memory
size are highly constrained. Multiple prior works like (Wu et al., 2020) formalize the deployment
of lightweight transformers on mobile devices by defining the mobile settings regarding the amount
of computation and parameter numbers. Particularly, given the ARM Cortex-A72 mobile CPU with
computation performance of 48G FLOPS, the computation constraint for machine translation should
be under 500M Mult-Adds and the parameter number should be around 10M with a sequence of 30
tokens that are the general length for machine translation. (ii) System-independent settings. Other
than deployed on mobile devices, DictFormer also supports system-independent deployment. In
this setting, we directly choose prior transformer architectures like DelighT (Mehta et al., 2021) and
Subformer (Reid et al., 2021) as baselines and compare our DictFormer with them under the the
same performance constraint.
A.5 Sensitivity S tudy
. We study the DictFormer’s performance under different model size as Figure 7 shows. This is
important since system-independent settings require scalable DictFormer. The performance of Dict-
Former improves with an increase in the number of model parameters, across different corpora.
DictFormer achieves similar BLEU score with Transformer using 3× to 10× less model parame-
ters. In Figure 8, we also report the hyper-parameter’s effects on DictFormer’s performance with
WMT’14 En-Fr dataset with embedding size d = 512. Figure 8(a) shows the dictionary size study
when the coefficient size is fixed to 60. All dictionaries share the same size, and all coefficients also
share same size. The Figure 8(b) shows the coefficient size study when the dictionary size is fixed
240. Although DictFormer’s performance improves with an increase in the size of dictionary and
coefficients, the model size is also enlarged. So one should pick up a proper dictionary or coefficient
size according to accuracy requirements.
A.6 Ablation S tudy
We also use table 3 to show the ablation study of DictFormer’s techniques. Transformer Vaswani
et al. (2017) adopts the base architecture with hidden size 512. Transformer-Flatten flattens the
feed-forward hidden size of transformer from 2048 to 512, thus reducing the parameters from 44
millions to 25 millions without hurting the accuracy. Only-Dictionary is based on Transformer-
Flatten and uses unshared dictionary architecture. This method achieves 33.2 BLEU with 9.7 mil-
lion model parameters. The #TParams is 14.3 million and 27.4 million when adding embedding
parameters on IWSLT’14 De-En and WMT’14 En-Fr, respectively. When sharing dictionaries in at-
tention, Shared-Attention achieves similar BLEU score with 4.2 million model parameters. Mean-
15
Published as a conference paper at ICLR 2022
4 3 2 1
(Uo三一UJ W) s」①一①UJB」Bd
(a) Dictionary study
∩-m
O 9
4 3 3 3
6 5 4 3 2 1
(UO= = UJ U一) s」①一①UJB」Bd
12	24	36	48	60	72	84
Coefficient size (t)
(b) Coefficient study
Figure 8: Increasing the size of dictionary (a) and coefficients (b) improves DictFormer's perfor-
mance. (a) and (b) are tested on WMT'14 En-Fr dataset for machine translation task.
while, Shared-FFN only shares FFN using group-wise shared dictionaries (G is group number).
DictFormer that shares both FFN and attention with G=2, denoted by Shared-both, requires 2.6M
model parameters and matches the accuracy of unshared DictFormer. Improved-Embed represents
that the embedding layers of Shared-both are further compressed by using existing method Mehta
et al. (2021) so that the #TParamS is reduced to 5.1M from 7.1M.
Table 3: Ablation study of DictFormer techniques on IWLS’14 De-En and WMT’14 En-Fr. Entries
with ± represents the average across three independent runs.
	IWSLT’14 De-En			WMT’14 En-Fr		
	#Params	#TParamS	BLEU	#Params	#TParams	BLEU
Transformer (Vaswani et al., 2017)	44M	48.6M	34.4	44M	64M	40.7
Transformer-Flatten	25M	29.6M	34.4	25M	45M	40.7
Only-Dictionary	9.7M	143M^^	33.5 ±0.3	10.6M	27.4M	39.5 ±0.3
Shared-Attention	4.2M	8.7M	33.3 ±0.4	4.5M	21.3M	39.5 ±0.4
Shared-FFN (G=1)	7.8M	12.2M	33.2 ±0.5	8.6M	25.4M	39.2 ±0.5
Shared-FFN (G=2)	8.1M	12.5M	33.2 ±0.3	8.9M	25.7M	39.6 ±0.4
Shared-FFN (G=3)	8.4M	12.8M	33.3 ±0.3	9.1M	25.9M	39.5 ±0.3
Shared-both (G=2)	2.6M	7.1M	33.2 ±0.4	2.5M	19.3M	39.6 ±0.4
Improved-Embed	2.6M	5.1M	33.1 ±0.4	2.5M	9.8M	39.5 ±0.3
16