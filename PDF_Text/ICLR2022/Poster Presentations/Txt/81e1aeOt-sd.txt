Published as a conference paper at ICLR 2022
On-Policy Model Errors
in Reinforcement Learning
Lukas P. Frohlich*
Institute for Dynamic Systems and Control
ETH Zurich
Zurich, Switzerland
lukasfro@ethz.ch
Maksym Lefarov
Bosch Center for Artificial Intelligence
Renningen, Germany
Maksym.Lefarov@de.bosch.com
Melanie N. Zeilinger
Institute for Dynamic Systems and Control
ETH Zurich
Zurich, Switzerland
mzeilinger@ethz.ch
Felix Berkenkamp
Bosch Center for Artificial Intelligence
Renningen, Germany
Felix.Berkenkamp@de.bosch.com
Ab stract
Model-free reinforcement learning algorithms can compute policy gradients given
sampled environment transitions, but require large amounts of data. In contrast,
model-based methods can use the learned model to generate new data, but model
errors and bias can render learning unstable or suboptimal. In this paper, we present
a novel method that combines real-world data and a learned model in order to get
the best of both worlds. The core idea is to exploit the real-world data for on-
policy predictions and use the learned model only to generalize to different actions.
Specifically, we use the data as time-dependent on-policy correction terms on top of
a learned model, to retain the ability to generate data without accumulating errors
over long prediction horizons. We motivate this method theoretically and show that
it counteracts an error term for model-based policy improvement. Experiments on
MuJoCo- and PyBullet-benchmarks show that our method can drastically improve
existing model-based approaches without introducing additional tuning parameters.
1 Introduction
Model-free reinforcement learning (RL) has made great advancements in diverse domains such as
single- and multi-agent game playing (Mnih et al., 2015; Silver et al., 2016; Vinyals et al., 2019),
robotics (Kalashnikov et al., 2018), and neural architecture search (Zoph & Le, 2017). All of these
model-free approaches rely on large numbers of interactions with the environment to ensure successful
learning. While this issue is less severe for environments that can easily be simulated, it limits the
applicability of model-free RL to (real-world) domains where data is scarce.
Model-based RL (MBRL) reduces the amount of data required for policy optimization by approximat-
ing the environment with a learned model, which we can use to generate simulated state transitions
(Sutton, 1990; RaCaniere et al., 2017; Moerland et al., 2020). While early approaches on low-
dimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models
with closed-form posteriors, recent methods rely on neural networks to scale to complex tasks on
discrete (Kaiser et al., 2020) and continuous (Chua et al., 2018; Kurutach et al., 2018) action spaces.
However, the learned representation of the true environment always remains imperfect, which intro-
duces approximation errors to the RL problem (Atkeson & Santamaria, 1997; Abbeel et al., 2006).
Hence, a key challenge in MBRL is model-bias; small errors in the learned models that compound
over multi-step predictions and lead to lower asymptotic performance than model-free methods.
To address these challenges with both model-free and model-based RL, Levine & Koltun (2013);
Chebotar et al. (2017) propose to combine the merits of both. While there are multiple possibilities to
*Work done partially while at the Bosch Center for Artificial Intelligence.
1
Published as a conference paper at ICLR 2022
combine the two methodologies, in this work we focus on improving the model’s predictive state
distribution such that it more closely resembles the data distribution of the true environment.
Contributions The main contribution of this paper is on-policy corrections (OPC), a novel
hyperparameter-free methodology that uses on-policy transition data on top of a separately learned
model to enable accurate long-term predictions for MBRL. A key strength of our approach is that it
does not introduce any new parameters that need to be hand-tuned for specific tasks. We theoretically
motivate our approach by means of a policy improvement bound and show that we can recover the
true state distribution when generating trajectories on-policy with the model. We illustrate how opc
improves the quality of policy gradient estimates in a simple toy example and evaluate it on various
continuous control tasks from the MuJoCo control suite and their PyBullet variants. There, we
demonstrate that opc improves current state-of-the-art MBRL algorithms in terms of data-efficiency,
especially for the more difficult PyBullet environments.
Related Work To counteract model-bias, several approaches combine ideas from model-free and
model-based RL. For example, Levine & Koltun (2013) guide a model-free algorithm via model-
based planning towards promising regions in the state space, Kalweit & Boedecker (2017) augment
the training data by an adaptive ratio of simulated transitions, Talvitie (2017) use ‘hallucinated’
transition tuples from simulated to observed states to self-correct the model, and Feinberg et al.
(2018); Buckman et al. (2018) use a learned model to improve the value function estimate. Janner
et al. (2019) mitigate the issue of compounding errors for long-term predictions by simulating short
trajectories that start from real states. Cheng et al. (2019) extend first-order model-free algorithms
via adversarial online learning to leverage prediction models in a regret-optimal manner. Clavera
et al. (2020) employ a model to augment an actor-critic objective and adapt the planning horizon to
interpolate between a purely model-based and a model-free approach. Morgan et al. (2021) combine
actor-critic methods with model-predictive rollouts to guarantee near-optimal simulated data and
retain exploration on the real environment. A downside of most existing approaches is that they
introduce additional hyperparameters that are critical to the learning performance (Zhang et al., 2021).
In addition to empirical performance, recent work builds on the theoretical guarantees for model-free
approaches by Kakade & Langford (2002); Schulman et al. (2015) to provide guarantees for MBRL.
Luo et al. (2019) provide a general framework to show monotonic improvement towards a local
optimum of the value function, while Janner et al. (2019) present a lower-bound on performance
for different rollout schemes and horizon lengths. Yu et al. (2020) show guaranteed improvement in
the offline MBRL setting by augmenting the reward with an uncertainty penalty, while Clavera et al.
(2020) present improvement guarantees in terms of the model’s and value function’s gradient errors.
Moreover, Harutyunyan et al. (2016) propose a similar correction term as the one introduced in this
paper in the context of off-policy policy evaluation and correct the state-action value function instead
of the transition dynamics. Similarly, Fonteneau et al. (2013) consider the problem of off-policy
policy evaluation but in the batch RL setting and propose to generate ‘artificial’ trajectories from
observed transitions instead of using an explicit model for the dynamics.
A related field to MBRL that also combines models with data is iterative learning control (ILC)
(Bristow et al., 2006). While RL typically focuses on finding parametric feedback policies for general
reward functions, ILC instead seeks an open-loop sequence of actions with fixed length to improve
state tracking performance. Moreover, the model in ILC is often derived from first principles and then
kept fixed, whereas in MBRL the model is continuously improved upon observing new data. The
method most closely related to RL and our approach is optimization-based ILC (Owens & Hatonen,
2005; Schollig & D’Andrea, 2009), in which a linear dynamics model is used to guide the search
for optimal actions. Recently, Baumgartner & Diehl (2020) extended the ILC setting to nonlinear
dynamics and more general reward signals. Little work is available that draws connections between
RL and ILC (Zhang et al., 2019) with one notable exception: Abbeel et al. (2006) use the observed
data from the last rollout to account for a mismatch in the dynamics model. The limitations of this
approach are that deterministic dynamics are assumed, the policy optimization itself requires a line
search procedure with rollouts on the true environment and that it was not combined with model
learning. We build on this idea and extend it to the stochastic setting of MBRL by making use of
recent advances in RL and model learning.
2
Published as a conference paper at ICLR 2022
Algorithm 1 General Model-based Reinforcement Learning
1:	for n = 1, . . . do
2:	for b = 1, . . . , B do
3:	Rollout policy πn on environment and store transitions Dn = {(sn,b, an,b,❷鹏)}^-)1
4:	Pn(St+ι | st, at): Learn a global dynamics model given all data D∖nl = Un=ι Ub=ι Db
5:	θn+ι = θn + QRnn= Optimize the policy based on the model P with any RL algorithm
2 Problem Statement and Background
We consider the Markov decision process (MDP) (S, A, P, r, γ, ρ), where S ⊆ RdS and A ⊆ RdA
are the continuous state and action spaces, respectively. The unknown environment dynamics are
described by the transition probability P(st+1 | st, at), an initial state distribution ρ(s0) and the
reward signal r(s, a). The goal in RL is to find a policy πθ(at | st) parameterized by θ that maximizes
the expected return discounted by γ ∈ [0, 1] over episodes of length T,
η = E T CYtr(st, at)	,	St+1	〜p(st+ι	|	St,	at),	so	〜P,	at	〜∏θ(at	| St). (1)
s0:T ,a0:T	t=0
The expectation is taken with respect to the trajectory under the stochastic policy πθ starting from a
stochastic initial state s0. Direct maximization of Eq. (1) is challenging, since we do not know the
environment’s transition model P. In MBRL, we learn a model for the transitions and reward function
from data, p(st+ι | st, at) ≈ p(st+ι | st, at) and r(st, at) ≈ r(st, at), respectively. Subsequently,
we maximize the model-based expected return η as a surrogate problem for the true RL setting, where
η is defined as in Eq. (1) but with P and r instead. For ease of exposition, we assume a known reward
function r = r, even though we learn itjointly with P in our experiments.
We let ηn denote the return under the policy ∏n, = ∏θn at iteration n and use S and a for states and
actions that are observed on the true environment. Algorithm 1 summarizes the overall procedure
for MBRL: At each iteration n we store B on-policy trajectories Dn = {(sn,b, a；,b, snbι)}T-ɔ1
obtained by rolling out the current policy πn on the real environment in Line 3. Afterwards, we
approximate the environment with a learned modelPn(St+ι | st, at) based on the data Di：n in Line 4,
and optimize the policy based on the proxy objective η in Line 5. Note that the policy optimization
algorithm can be off-policy and employ its own, separate replay buffer.
Model Choices The choice of model P plays a key role, since it is used to predict sequences T
of states transitions and thus defines the surrogate problem in MBRL. We assume that the model
comes from a distribution family P, which for each state-action pair (st, at) models a distribution
over the next state st+1. The model is then trained to summarize all past data D1:n = ∪in=1 ∪bB=1 Dib
by maximizing the marginal log-likelihood L,
Pmodel(St+ι | st,at) = argmax X	L(St+i；St,a0.	(2)
p∈P (St,at,St+l)∈Dlm
For a sampled trajectory index b 〜U({1,...,B}), sequences T start from the initial state Sn,b and are
distributed according to Pmodel(T | b) = δ(so — Sn,b) QT-j1 Pmodel(St+ι | st, at)∏θ(at | st), where
δ(∙) denotes the Dirac-delta distribution. Using model-data for policy optimization is in contrast to
model-free methods, which only use observed environment data by replaying past transitions from
a recent on-policy trajectory b ∈ {1, . . . , B}. In our model-based framework, this replay buffer is
equivalent to the non-parametric model
T-1
Pnata(T | b) = δ(so — sn,b) Y Pnata(St+1 11, b), wherePnata(st+1 | t,b) = δ(st+ι — SnbJ,⑶
t=0
where we only replay observed transitions instead of sampling new actions from πθ .
3	On-policy Corrections
In this section, we analyze how the choice of model impacts policy improvement, develop opc as
a model that can eliminate one term in the improvement bound, and analyze its properties. In the
following, we drop the n sub- and superscript when the iteration is clear from context.
3
Published as a conference paper at ICLR 2022
3.1	Policy Improvement
Independent of whether We use the data directly in Pdata or summarize it in a world model Pmodel,
our goal is to find an optimal policy that maximizes Eq. (1) via the corresponding model-based proxy
objective. To this end, We would like to know how a policy improvement 狼+1 -狼 ≥ 0 based on
the model p, which is what we optimize in MBRL, relates to the true gain in performance ηn+ι — ηn
on the environment with unknown transitions P. While the two are equal without model errors, in
general the larger the model error, the worse we expect the proxy objective to be (Lambert et al.,
2020). Specifically, we show in Appendix B.1 that the policy improvement can be decomposed as
≥
ηn+ι
- ηn
一 -J
Sz
ηn+ι
True policy improvement
- ηn
一 -J
Sz
Model policy improvement
∣ηn+1 - ηn+1∣	-	∣ηn - ηn |
1-----7------'	1—7—'
Off-policy model error On-policy model error
(4)
	
where a performance improvement on our model-based objective η only translates to a gain in Eq. (1)
if two error terms are sufficiently small. These terms depend on how well the performance estimate
based on our model, η, matches the true performance, η. If the reward function is known, this term
only depends on the model quality of P relative to p. Note that in contrast to the result by Janner et al.
(2019), Eq. (4) is a bound on the policy improvement instead of a lower bound on ηn+1 .
The first error term compares ηn+ι and ηn+ι, the performance estimation gap under the optimized
policy πn+1 that we obtain in Line 5 of Algorithm 1. Since at this point we have only collected data
with πn in Line 3, this term depends on the generalization properties of our model to new data; what
we call the off-policy model error. For our data-based model Pdata that just replays data under ∏n
independently of the action, this term can be bounded for stochastic policies. For example, Schulman
et al. (2015) bound it by the average KL-divergence between πn and πn+1 . For learned models
Pmodel, it depends on the generalization properties of the model (Luo et al., 2019; Yu et al., 2020).
While understanding model generalization better is an interesting research direction, we will assume
that our learned model is able to generalize to new actions in the following sections.
While the first term hinges on model-generalization, the second term is the on-policy model error, i.e.,
the deviation between ηn and ηn under the current policy ∏n,. This error term goes to zero for Pdata as
we use more on-policy data B → ∞, since the transition data are sampled from the true environment,
c.f., Appendix B.2. While the learned model is also trained with on-policy data, small errors in our
model compound as we iteratively predict ahead in time. Consequently, the on-policy error term
grows as O(min(γ∕(1 - γ)2, H/(1 - γ),H2)), c.f., (Janner et al., 2019) and Appendix B.3.
3.2	Combining Learned Models and Replay Buffer
The key insight of this paper is that the learned model in Eq. (2) and the replay buffer in Eq. (3)
have opposing strengths: The replay buffer has low error on-policy, but high error off-policy since
it replays transitions from past data, i.e., they are independent of the actions chosen under the new
policy. In contrast, the learned model can generalize to new actions by extrapolating from the data
and thus has lower error off-policy, but errors compound over multi-step predictions.
An ideal model would combine the model-free and model-based approaches in a way such that it
retains the unbiasedness of on-policy generated data, but also generalizes to new policies via the model.
To this end, we propose to use the model to predict how observed transitions would change for a new
state-action pair. In particular, we use the model,s mean prediction fn(s, a) = E[Pmodel( ∙ ∣ s, a)] to
construct the joint model
Pnpc(St+1 ∣	st, at, b)	=	δ(st+1 - Sn,bi)	*	δ(st+1	-	[fn(St, at)	-	fn(Sn,b,	an*]),	(5)
1------{-------}	1------------------V-------------------}
Pdata(St+1 । t,b)	Model mean correction to generalize to st, at
where * denotes the convolution of the two distributions and b refers to a specific rollout stored
in the replay buffer that was observed in the true environment. Given a trajectory-index b, PnPc
in Eq. (5) transitions deterministically according to st+ι = Sn+l + fn(st, at) 一 fn(^t,b, an,b),
resembling the equations in ILC (c.f., Baumgartner & Diehl (2020) and Appendix E). If we roll
out PnPc along a trajectory, starting from a state ^n,b and apply the recorded actions from the
replay buffer, an,b, the correction term on the right of Eq. (5) cancels out and we have PnPc (St+ι ∣
^n*, an,b,b) = Pnata(St+ι ∣ t,b) = δ(St+ι - Sn+l). ThUS OPC retrieves the true on-policy data
4
Published as a conference paper at ICLR 2022
Figure 1: Illustration to compare predictions of the three models Eqs. (2), (3) and (5) starting from the
same state s0. In Fig. 1a, We see that on-policy, i.e., using actions (a0, al), Pdata returns environment
data, while ρmodel (blue) is biased. We correct this on-policy bias in expectation to obtain PoPc. This
alloWs us to retain the true state distribution When predicting With these models recursively (c.f.,
bottom three lines in Fig. 1b). When using OPC for off-policy actions (a0, a1), POPc does not recover
the true off-policy state distribution since it relies on the biased model. However, the corrections
generalize locally and reduce prediction errors in Fig. 1b (top three lines).
distribution independent of the prediction quality of the model, which is why we refer to this method
as on-policy corrections (opc). This behavior is illustrated in Fig. 1a, where the model (blue) is
biased on-policy, but opc corrects the model’s prediction to match the true data. In Fig. 1b, we show
how this affects predicted rollouts on a simple stochastic double-integrator environment. Although
small on-policy errors in PmOdel (blue) compound over time, the corresponding POPc matches the
ground-truth environment data closely. Note that even though the model in Eq. (5) is deterministic,
we retain the environment,s stochasticity from the data in the transitions to St+ι, so that we recover
the on-policy aleatoric uncertainty (noise) from sampling different reference trajectories via indexes b.
When our actions at are different from a，，Popc still uses the data from the environment,s transitions,
but the correction term in Eq. (5) uses the learned model to predict how the next state changes in
expectation relative to the prediction under ab. That is, in Fig. 1a for a new at the model predicts the
state distribution shown in red. Correspondingly, we shift the static prediction St+ by the difference
in means (gray arrow) between the two predictions; i.e., the change in trajectory by changing from
ab to at. Since we shift the model predictions by a time-dependent but constant offset, this does not
recover the true state distribution unless the model has zero error. However, empirically it can still
help with long-term predictions in Fig. 1b by shifting the model off-policy predictions (red) to the
opc predictions (green), which are closer to the environment,s state distribution under the new policy.
3.3	Theoretical Analysis
In the previous sections, we have introduced opc to decrease the on-policy model error in Eq. (4) and
tighten the improvement bound. In this section, we analyze the on-policy performance gap from a
theoretical perspective and show that with opc this error can be reduced independently of the learned
model,s error. To this end, we assume infinitely many on-policy reference trajectories, B → ∞,
which is equivalent to a variant of PoPc that considers Sb+1 as a random variable that follows the true
environment,s transition dynamics. While impossible to implement in practice, this formulation is
useful to understand our method. We define the generalized opc-model as
P?pc(st+1 | st, at,b)=	pM | Sb, a，)	*
、-------{z------}
Environment on-policy transition
δ (st+ι - [.f(st, at) - f(Sb, ab)]),
、-----------------------{z-----------------------}
opc correction term
(6)
which highlights that it transitions according to the true on-policy dynamics conditioned on data
from the replay buffer, combined with a correction term. We provide a detailed derivation for the
generalized model in Appendix B, Lemma 4. With Eq. (6), we have the following result:
Theorem 1. Let η?Pc and η be the expected return under the generaIized OPC-model Eq. (6) and
the true environment, respectively. Assume that the learned model’s mean transition function
f(st, at) = E[Pmodel(st+ι | St, at)] is Lf-LiPSChitzandthe rewardr(st, at) is Lr-Lipschitz. Further,
5
Published as a conference paper at ICLR 2022
if the policy π(at | st) is Lπ -Lipschitz with respect to st under the Wasserstein distance and
its (co-)variance Var[π(at | st)] = Σπ (st) ∈ Sd+A is finite over the complete state space, i.e.,
maxst∈s trace{Σ∏ ⑸)} ≤ σ2 ,then with Ci = /2(1 + L) Lf Lr and C2 = JLf + L
lη - n?pcl ≤ 产dA1CICT√T.	(7)
1-γ
We provide a proof in Appendix B.4. From Theorem 1, we can observe the key property of opc:
for deterministic policies, the on-policy model error from Eq. (4) is zero and independent of the
learned models' predictive distribution Pmodel, So that η = n?pc. For policies with non-zero variance,
the bound scales exponentially with T, highlighting the problem of compounding errors. In this
case, as in the off-policy case, the model quality determines how well we can generalize to different
actions. We show in Appendix B.5 that, for one-step predictions, opc’s prediction error scales as the
minimum of policy variance and model error. To further alleviate the issue of compounding errors,
one could extend Theorem 1 with a branched rollout scheme similarly to the results by Janner et al.
(2019), such that the rollouts are only of length H T.
In practice, Popc cannot be realized as it requires the true (unknown) state transition model p. However,
as We use more on-policy reference trajectories for PoPc in Eq. (5), it also converges to zero on-policy
error in probability for deterministic policies.
∙-v
Lemma 1. Let M be aMDP with dynamics P(st+1 | st, at) and reward r < rmax. Let M be another
MDP with dynamics Pmodel = P. Assume a deterministic policy π : S → A and a set Oftrajectories
D = Ub=ι{(Sb, ab, Sb+ι)}T=o1 collected from M under π. Ifwe use OPC Eq. (5) with data D, then
lim Pr(∣η 一 ηopc∣ > ε) = 0 ∀ε > 0 WithCOnVergenCerate O(1∕B).	(8)
B→∞
We provide a proof in Appendix B.4. Lemma 1 states that given sufficient reference on-policy data,
the performance gap due to model errors becomes arbitrarily small for any model Pmodel When
using opc. While the assumption of infinite on-policy data in Lemma 1 is unrealistic for practical
applications, we found empirically that opc drastically reduces the on-policy model error even when
the assumptions are violated. In our implementation, we use stochastic policies as well as trajectories
from previous policies, i.e., off-policy data, for the corrections in opc (see also Section 4.2).
3.4	Discussion
Epistemic uncertainty So far, we have only considered aleatoric uncertainty (noise) in our
transition models. In practice, modern methods additionally distinguish epistemic uncertainty that
arises from having seen limited data (Deisenroth & Rasmussen, 2011; Chua et al., 2018). This leads
to a distribution (or an ensemble) of models, where each sample could explain the data. In this setting,
we apply opc by correcting each sample indiVidually. This allows us to retain epistemic uncertainty
estimates after applying opc, while the epistemic uncertainty is zero on-policy.
Limitations Since OPC uses on-policy data, it is inherently limited to local policy optimization
where the policy changes slowly over time. As a consequence, it is not suitable for global exploration
schemes like the one proposed by Curi et al. (2020). Similarly, since opc uses the observed data
and corrects it only with the expected learned model, PoPc always uses the on-policy transition
noise (aleatoric uncertainty) from the data, even if the model has learned to represent it. While not
having to learn a representation for aleatoric uncertainty can be a strength, it limits our approach to
environments where the aleatoric uncertainty does not vary significantly with states/actions. It is
possible to extend the method to the heteroscedastic noise setting under additional assumptions that
enable distinguishing model error from transition noise (Schollig & D,Andrea, 2009). Lastly, our
method applies directly only to MDPs, since we rely on state-observations. Extending the ideas to
partially observed environments is an interesting direction for future research.
4	Experimental Results
We begin the experimental section with a motivating example on a toy problem to highlight the
impact of opc on the policy gradient estimates in the presence of model errors. The remainder of the
section focuses on comparative evaluations and ablation studies on complex continuous control tasks.
6
Published as a conference paper at ICLR 2022
Figure 2: Signed gradient error when using inaccurate models Eq. (9) to estimate the policy gradient
without (left) and with (right) opc. The background’s opacity depicts the error’s magnitude, whereas
color denotes if the sign of estimated and true gradient differ (red) or coincide (blue). opc improves
the gradient estimate in the presence of model errors. Note that the optimal policy is θ* = -1.0.
4.1	Illustrative Example
In Section 3.3, we investigate the influence of the model error directly on the expected return
from a theoretical perspective. From a practical standpoint, another relevant question is how the
policy optimization and the respective policy gradients are influenced by model errors. For general
environments and reward signals, this question is difficult to answer, due to the typically high-
dimensional state/action spaces and large number of parameters governing the dynamics model as
well as the policy. To shed light on this open question, we resort to a simple low-dimensional example
and investigate how opc improves the gradient estimates under a misspecified dynamics model.
In particular, we assume a one-dimensional deterministic environment with linear transitions and
a linear policy. The benefits of this example are that we can 1) compute the true policy gradient
based on a single rollout (determinism), 2) determine the environment’s closed-loop stability under
the policy (linearity), and 3) visualize the gradient error as a function of all relevant parameters
(low dimensionality). The dynamics and initial state distribution are specified by p(st+1 | st, at) =
δ(Ast + Bat | st, at) with ρ(so) = δ(so) where A,B ∈ R and δ(∙) denotes the Dirac-delta
distribution. We define a deterministic linear policy πθ (at | st) = δ(θst | st) that is parameterized
by the scalar θ ∈ R. The objective is to drive the state to zero, which we encode with an exponential
reward r(st, at) = exp { —(st∕σr)2}. Further, we assume an approximate dynamics model
p(st+ι |	st,at)	=	δ((A + ∆A)st	+	(B	+ ∆B)a,	|	st,at),	(9)
where ∆A, ∆B quantify the mismatch between the approximate model and the true environment. In
practice, the mismatch can arise due to noise-corrupted observations of the true state or, in the case of
stochastic environments, due to a finite amount of training data.
With the setting outlined above, we investigate how model errors influence the estimation of policy
gradients. To this end, we roll out different policies under models with varying degrees of error ∆B.
For each policy/model combination, we compute the model-based policy gradient and compare it to
the true gradient. The results are summarized in Fig. 2, where the background’s opacity depicts the
gradient error’s magnitude and its color indicates whether the respective signs of the gradients are
the same (blue, ≥ 0) or differ (red, < 0). For policy optimization, the sign of the gradient estimate
is paramount. However, we see in the left-hand image that even small model errors can lead to
the wrong sign of the gradient. opc significantly reduces the magnitude of the gradient error and
increases the robustness towards model errors. See also Appendix C for a more in-depth analysis.
4.2 Evaluation on Continuous Control Tasks
In the following section, we investigate the impact of opc on a range of continuous control tasks.
To this end, we build upon the current state-of-the-art model-based RL algorithm mbpo (Janner
et al., 2019). Further, we aim to answer the important question about how data diversity affects
MBRL, and opc in particular. While having a model that can generate (theoretically) an infinite
amount of data is intriguing, the benefit of having more data is limited by the model quality in terms
7
Published as a conference paper at ICLR 2022
HalfCheetah	Hopper
Ouofnw
telluByP
Walker2d	AntTruncatedObs
0	100	200	0	100	0	200	0	200
# Steps ×103	# Steps ×103	# Steps ×103	# Steps ×103
Figure 3: Comparison of OPC (----), mbpo(?) (-----) and SAC (……)on four environments from the
MuJoCo control suite (top row) and their respective PyBullet implementations (bottom row).
of being representative of the true environment. For opc, the following questions arise from this
consideration: Do longer rollouts help to generate better data? And is there a limit to the value of
simulated transition data, i.e., is more always better?
For the dynamics model pmodel, We follow ChUa et al. (2018); Janner et al. (2019) and use a Proba-
bilistic ensemble of neural networks, where each head predicts a Gaussian distribution over the next
state and reward. For policy optimization, we employ the soft actor critic (sac) algorithm by Haarnoja
et al. (2018). All learning curves are presented in terms of the median (lines) and interquartile range
(shaded region) across ten independent experiments, where we smooth the evaluation return with a
moving average filter to accentuate the results of particularly noisy environments. Apart from small
variations in the hyperparameters, the only difference between opc and mbpo is that our method
USeS popc, while MBPO uses pmodel. We provide pseudo-code for the model rollouts of MBPO and
opc in Algorithm 2 in Appendix A.1. Generally, we found that opc was more robust to the choice of
hyperparameters. The rollout horizon to generate training data is set to H = 10 for all experiments.
Note that when using Pdata to generate data, we retain the standard (model-free) SAC algorithm.
Our implementation is based upon the code from Janner et al. (2019). We made the following changes
to the original implementation: 1) The policy is only updated at the end of an epoch, not during
rollouts on the true environment. 2) The replay buffer retains data for a fixed number of episodes, to
clearly distinguish on- and off-policy data. 3) For policy optimization, mbpo uses a small number of
environment transitions in addition to those from the model. We found that this design choice did
not consistently improve performance and added another level of complexity. Therefore, we refrain
from mixing environment and model transitions and only use simulated data for policy optimization.
While we stay true to the key ideas of mbpo under these changes, we denote our variant as mbpo(?)
to avoid ambiguity. See Appendices D.6 and D.7 for a comparison to the original mbpo algorithm.
Comparative Evaluation We begin our analysis with a comparison of our method to MBPO(?)
and sac on four continuous control benchmark tasks from the MuJoCo control suite (Todorov et al.,
2012) and their respective PyBullet variants (Ellenberger, 2018-2019). The results are presented
in Fig. 3. We see that the difference in performance between both methods is only marginal when
evaluated on the MuJoCo environments (Fig. 3, top row). Notably, the situation changes drastically
for the PyBullet environments (Fig. 3, bottom row). Here, mbpo(?) exhibits little to no learning
progress, whereas opc succeeds at learning a good policy with few interactions in the environment.
One of the main differences between the environments (apart from the physics engine itself) is that
the PyBullet variants have initial state distributions with significantly larger variance.
Influence of State Representation In general, the success of RL algorithms should be agnostic to
the way an environment represents its state. In robotics, joint angles H are often re-parameterized by
a sine/cosine transformation, H → [sin(H), cos(H)]. We show that even for the simple CartPole envi-
8
Published as a conference paper at ICLR 2022
RoboSchool (original)
[sin(小),cos(小)]
S 1.00
×
0.75
I
0.50
0.25
Λ
口 0	2	4	6
# Steps ×103
RoboSchool (transf.)
[sin(小),cos(小)]→ 外
PyBullet (original)
d
Pybullet (transf.)
外 → [sin(小),cos(小)]
0246
# Steps ×103
0246
# Steps ×103
0246
# Steps ×103
Figure 4:	Comparison of OPC (-----) and mbpo(?) (----) on different variants of the CartPole
environment. When the pole,s angle Id is observed directly (center plots), both algorithms successfully
learn a policy. With the sine/cosine transformations (outer plots), mbpo(?) fails to solve the task.
3
N=20 ×103	N=40 ×103	N= 100 ×103	N=200 ×103
12
9
6
3
0
0	100	200 0	100	200 0	100	200 0	100	200
# Steps ×103	# Steps ×103	# Steps ×103	# Steps ×103
Figure 5:	Ablation study for opc on the HalfCheetah environment. In each plot, we fix the number
of simulated transitions N and vary the rollout lengths H = {1(.....), 5(----), 10(---)}.
ronment, the parameterization of the pole’s angle has a large influence on the performance of MBRL.
In particular, we compare opc and mbpo(?) on the RoboSchool and PyBullet variants of the CartPole
environment, which represent the pole’s angle with and without the sine/cosine transformation. The
results are shown in Fig. 4. To rule out other effects than the angle’s representation, we repeat the
experiment for each implementation but transform the state to the other representation, respectively.
Notably, opc successfully learns a policy irrespective of the state’s representation, whereas mbpo(?)
fails if the angle of the pole is represented by the sine/cosine transformation.
Influence of Data Diversity Here, we investigate whether multi-step predictions are in fact
beneficial compared to single-step predictions during the data generation process. To this end,
we keep the total number of simulated transitions N for training constant, but choose different
horizon lengths H = {1, 5, 10}. The corresponding numbers of simulated rollouts are then given by
nrollout = N/H. The results for N = {20, 40, 100, 200} × 103 on the HalfCheetah environment are
shown in Fig. 5. First, note that more data leads to a higher asymptotic return, but after a certain point
more data only leads to diminishing returns. Further, the results indicate that one-step predictions
are not enough to generate sufficiently diverse data. Note that this result contradicts the findings by
Janner et al. (2019) that one-step predictions are often sufficient for mbpo.
5 Conclusion
In this paper, we have introduced on-policy corrections (opc), a novel method that combines observed
transition data with model-based predictions to mitigate model-bias in MBRL. In particular, we extend
a replay buffer with a learned model to account for state-action pairs that have not been observed on
the real environment. This approach enables the generation of more realistic transition data to more
closely match the true state distribution, which was further motivated theoretically by a tightened
improvement bound on the expected return. Empirically, we demonstrated superior performance on
high-dimensional continuous control tasks as well as robustness towards state representations.
9
Published as a conference paper at ICLR 2022
References
Pieter Abbeel, Morgan Quigley, and Andrew Y. Ng. Using Inaccurate Models in Reinforcement
Learning. In Proceedings ofthe International Conference on Machine Learning (ICML), pp. 1-8,
2006.
Christopher G. Atkeson and Juan Carlos Santamaria. A Comparison of Direct and Model-Based
Reinforcement Learning. In Proceedings of the IEEE International Conference on Robotics and
Automation (ICRA), pp. 3557-3564, 1997.
K. Baumgartner and M. Diehl. Zero-Order Optimization-Based Iterative Learning Control. In
Proceedings of the IEEE Conference on Decision and Control, pp. 3751-3757, 2020.
Joseph K. Blitzstein and Jessica Hwang. Introduction to Probability. CRC Press, 2019.
Douglas A. Bristow, Marina Tharayil, and Andrew G. Alleyne. A Survey of Iterative Learning
Control. IEEE Control Systems Magazine, 26(3):96-114, 2006.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efficient
Reinforcement Learning with Stochastic Ensemble Value Expansion. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 8224-8234, 2018.
Frank M. Callier and Charles A. Desoer. Linear System Theory. Springer, 1991.
Yevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, and Sergey
Levine. Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement
Learning. In Proceedings of the International Conference on Machine Learning (ICML), pp.
703-711, 2017.
Ching-An Cheng, Xinyan Yan, Nathan Ratliff, and Byron Boots. Predictor-Corrector Policy Op-
timization. In Proceedings of the International Conference on Machine Learning (ICML), pp.
1151-1161, 2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep Reinforcement
Learning in a Handful of Trials Using Probabilistic Dynamics Models. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 4754-4765, 2018.
Ignasi Clavera, Yao Fu, and Pieter Abbeel. Model-Augmented Actor-Critic: Backpropagating
Through Paths. In Proceedings of the International Conference on Learning Representations
(ICLR), 2020.
Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efficient Model-Based Reinforcement
Learning Through Optimistic Policy Search and Planning. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 14156-14170, 2020.
Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A Model-Based and Data-Efficient
Approach to Policy Search . In Proceedings of the International Conference on Machine Learning
(ICML), pp. 465-472, 2011.
Benjamin Ellenberger. PyBullet Gymperium. https://github.com/benelot/
pybullet-gym, 2018-2019.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning.
arXiv:1803.00101 [cs.LG], 2018.
Raphael Fonteneau, Susan A Murphy, Louis Wehenkel, and Damien Ernst. Batch Mode Reinforce-
ment Learning Based on the Synthesis of Artificial Trajectories. Annals of Operations Research,
208(1):383-416, 2013.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Proceedings of the
International Conference on Learning Representations (ICLR), pp. 1861-1870, 2018.
10
Published as a conference paper at ICLR 2022
Anna Harutyunyan, Marc G. Bellemare, Tom StePleton, and Remi Munos. Q(λ) with Off-Policy
Corrections. In International Conference on Algorithmic Learning Theory, pp. 305-320, 2016.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to Trust Your Model: Model-
Based Policy Optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
12519-12530, 2019.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-Based Reinforcement Learning
for Atari. In Proceedings of the International Conference on Learning Representations (ICLR),
2020.
Sham Kakade and John Langford. Approximately Optimal Approximate Reinforcement Learning. In
Proceedings of the International Conference on Machine Learning (ICML), pp. 267-274, 2002.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Scalable
Deep Reinforcement Learning for Vision-Based Robotic Manipulation. In Proceedings of the
Conference on Robot Learning (CoRL), pp. 651-673, 2018.
Gabriel Kalweit and Joschka Boedecker. Uncertainty-Driven Imagination for Continuous Deep
Reinforcement Learning. In Proceedings of the Conference on Robot Learning (CoRL), pp.
195-206, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of
the International Conference on Learning Representations (ICLR), 2015.
Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning.
In Operations Research & Management Science in the Age of Analytics, pp. 130-166. INFORMS,
2019.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-Ensemble
Trust-Region Policy Optimization. In Proceedings of the International Conference on Learning
Representations (ICLR), 2018.
Nathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra. Objective mismatch in
model-based reinforcement learning. In Proceedings of the Annual Learning for Dynamics and
Control Conference (L4DC), pp. 761-770, 2020.
Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 1-9, 2013.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
Framework for Model-Based Deep Reinforcement Learning with Theoretical Guarantees. In
Proceedings of the International Conference on Learning Representations (ICLR), 2019.
Ester Mariucci and Markus Reiβ. Wasserstein and Total Variation Distance Between Marginals of
Levy Processes. Elecronic Journal of Statistics, 12(2):2482-2514, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement Learning.
Nature, 518(7540):529-533, 2015.
Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-Based Reinforcement Learning:
A Survey. arXiv:2006.16712 [cs.LG], 2020.
Andrew Morgan, Daljeet Nandha, Georgia Chalvatzaki, Carlo D’Eramo, Aaron Dollar, and Jan Peters.
Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement
Learning. In Proceedings of the IEEE International Conference on Robotics and Automation
(ICRA), pp. 6672-6678, 2021.
11
Published as a conference paper at ICLR 2022
David H. Owens and Jari Hatonen. Iterative Learning Control - An Optimization Paradigm. Annual
Reviews in Control, 29(1):57-70, 2005.
Victor M. Panaretos and Yoav Zemel. Statistical Aspects of Wasserstein Distances. Annual Review of
Statistics and Its Application, 6:405-431, 2019.
SebaStien RacaniEe, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo
Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pas-
canu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-Augmented
Agents for Deep Reinforcement Learning. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 5694-5705, 2017.
R. Tyrrell Rockafellar. Integral Functionals, Normal Integrands and Measurable Selections. In
Nonlinear operators and the calculus of Variations, pp. 157-207. Springer, 1976.
Jeff G. Schneider. Exploiting Model Uncertainty Estimates for Safe Dynamic Control Learning.
Advances in Neural Information Processing Systems (NeurIPS), pp. 1047-1053, 1997.
Angela P. Schollig and Raffaello D’Andrea. Optimization-Based Iterative Learning Control for
Trajectory Tracking. In Proceedings of the European Control Conference (ECC), pp. 1505-1510,
2009.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust Region
Policy Optimization. In Proceedings of the International Conference on Machine Learning (ICML),
pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. arXiv:1707.06347 [cs.LG], 2017.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game of Go with
Deep Neural Networks and Tree Search. Nature, 529:484-489, 2016.
Richard S. Sutton. Integrated Architectures for Learning, Planning, and Reacting Based on Approx-
imating Dynamic Programming. In Proceedings of the International Conference on Machine
Learning (ICML), pp. 216-224, 1990.
Erik Talvitie. Self-Correcting Models for Model-Based Reinforcement Learning. In Proceedings of
the AAAI National Conference on Artificial Intelligence, pp. 2597-2603, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics Engine for Model-Based Control.
In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp. 5026-5033, 2012.
Oriol Vinyals, Igor Babuschkin, WojciechM. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander S. Vezhnevets, Remi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina McKinney, Oliver Smith, Tom
Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.
Grandmaster Level in StarCraft II Using Multi-Agent Reinforcement Learning. Nature, 575(7782):
350-354, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: Model-Based Offline Policy Optimization. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 14129-14142, 2020.
Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andre Biedenkapp, Kurtland Chua, Frank
Hutter, and Roberto Calandra. On the Importance of Hyperparameter Optimization for Model-
Based Reinforcement Learning. In Proceedings of the International Conference on Artificial
Intelligence and Statistics (AISTATS), pp. 4015-4023, 2021.
12
Published as a conference paper at ICLR 2022
Yueqing Zhang, Bing Chu, and Zhan Shu. A Preliminary Study on the Relationship Between Iterative
Learning Control and Reinforcement Learning. IFAC-PaPersOnLine, 52(29):314-319, 2019.
Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. In Proceed-
ings of the International Conference on Learning RePresentations (ICLR), 2017.
13
Published as a conference paper at ICLR 2022
Supplementary Material
In the appendix we provide additional details on our method, ablation studies, and the detailed
hyperparameter configurations used in the paper. An overview is shown below.
Table of Contents
A Implementation Details and Computational Resources	15
A.1 Detailed Algorithm for Rollouts with opc .................................. 15
A.2 Hyperparameter Settings ................................................... 16
A.3 Implementation and Computational Resources ................................ 16
B Theoretical Analysis of On-Policy Corrections	17
B.1	General Policy Improvement Bound ......................................... 17
B.2	Properties of the Replay Buffer .......................................... 17
B.3	Properties of the Learned Model .......................................... 18
B.4	Properties of opc ........................................................ 18
B.5 Model Errors in opc ....................................................... 26
C Motivating Example - In-depth Analysis	27
C.1	Setup .................................................................... 27
C.2	Reward Landscapes ........................................................ 28
C.3	Influence of Model Error ................................................. 28
C.4 Influence of Off-Policy Error ............................................. 29
C.5	Additional Information ................................................... 30
D Additional Experimental Results	32
D.1	Comparison with Other Baseline Algorithms ................................ 32
D.2	Ablation - Retain Epochs ................................................. 33
D.3	Influence of State Representation: In-Depth Analysis ..................... 33
D.4	Improvement Bound: Empirical Investigation ............................... 35
D.5	Off-Policy Analysis ...................................................... 36
D.6	Implementation Changes to Original mbpo	................................. 37
D.7	Results for Original mbpo Implementation	................................. 38
E Connection between MBRL and ILC	41
E.1	Norm-optimal ILC ......................................................... 41
E.2	Model-based RL to Norm-optimal ILC ....................................... 42
E.3	Extensions ............................................................... 43
14
Published as a conference paper at ICLR 2022
A Implementation Details and Computational Res ources
A.1 Detailed Algorithm for Rollouts with opc
In Section 3.2 and Fig. 1a, we have introduced the opc transition model and how to roll out trajectories
with the model. Here, we will give more details on the algorithmic implementation for the generation
of simulated data. Algorithm 2 follows the branched rollout scheme from mbpo (Janner et al., 2019).
Differences to mbpo are highlighted in blue.
∣⅛≠
Generally, opc only requires a deterministic transition function f to compute the corrective term
in Line 8 in Algorithm 2. For models that include aleatoric uncertainty, we choose f (st , at) =
Est0[p(st0 | st, at)]. If, in addition, the model includes epistemic uncertainty, We refer to the
comment in Section 3.4 in the main part of the paper.
In practice, rollouts on the true environment are terminated early if, for instance, a particular state
exceeds a user-defined boundary. Consequently, not all trajectories in the replay buffer are necessarily
of length T . Since the prediction in Line 8 requires valid transition tuples for the correction term, We
additionally check in Line 10 Whether the next reference state is a terminal state. Thus, in contrast to
mbpo, for opc We terminate the inner loop in Algorithm 2 early if either a simulated or reference
state is a terminal state.
Algorithm 2 Branched rollout scheme With OPC model (differences to MBPO highlighted in blue)
Input: Required parameters:
•	Set of trajectories Dn = {(Sn,b, an,b, Sn++bι)}T- for b ∈ {1,...,B}
•	Environment model P(StO | st, at). Define f (s, a) = Es”[p(sto | s, a)].
•	Policy πθ
•	Prediction horizon H
•	Number of simulated transitions N
1:	Dsim J 0: Initialize empty buffer for simulated transitions
2:	while |Dsim | < N do
3:	b 〜U{1, B}: Sample random reference trajectory
4:	t 〜U{0, T 一 H}: Sample random starting state
5:	h J 0, St J Sb: Initialize starting state
6:	while h < H do
7:	at+h 〜 ∏θ(a | st+h): Sample action from policy
8:	st+h+ι J St+h+ι + f(st+h, at+h) ― f(S>h, ab+h): Do One-SteP prediction with OPC
9:	Dsim J Dsim ∪ (St+h, at+h, St+h+1): Store neW transition tuple
10:	if (st+h+ι is terminal) or (Sb+h+ι is terminal) then
11:	break
12:	h J h + 1: Increase counter
return Dsim
15
Published as a conference paper at ICLR 2022
A.2 Hyperparameter Settings
Below, we list the most important hyperparameter settings that were used to generate the results in
the main paper.
Table 1: Hyperparameter settings for opc (blue) and mbpo(?) (red) for results shown in Fig. 3.
Note that the respective hyperparameters for each environment are shared across the different
implementations, i.e., MuJoCo and PyBullet.
	HalfCheetah	Hopper	Walker2D	AntTruncatedObs
epochs	200	150	300	300
env steps per	1000
epoch	
retain epochs	50/5	50	50	5
policy updates	40	20	20	20
per epoch	
model horizon	10
model rollouts	100’000
per epoch	
mix-in ratio	0.0
model network	ensemble of 7 with 5 elites
policy network	MLP with 2 hidden layers of size 64
A.3 Implementation and Computational Resources
Our implementation is based on the code from mbpo (Janner et al., 2019), which is open-sourced
under the MIT license. All experiments were run on an HPC cluster, where each individual experiment
used one Nvidia V100 GPU and four Intel Xeon CPUs. All experiments (including early debugging
and evaluations) amounted to a total of 84‘713 hours, which corresponds to roughly 9.7 years if
the jobs ran sequentially. Most of this compute was required to ensure reproducibility (ten random
seeds per job and ablation studies over the effects of parameters). The Bosch Group is carbon-neutral.
Administration, manufacturing and research activities do no longer leave a carbon footprint. This
also includes GPU clusters on which the experiments have been performed.
16
Published as a conference paper at ICLR 2022
B	Theoretical Analysis of On-Policy Corrections
In this section, we analyze opc from a theoretical perspective and how it affects policy improvement.
Notation In the following, we drop the n superscript for states and actions for ease of exposition.
That is, Sn，b = sb and an，b = ab.
B.1	General Policy Improvement B ound
We begin by deriving inequality Eq. (4), which serves as motivation for opc and is the foundation for
the theoretical analysis. Our goal is to bound the difference in expected return for the policies before
and after the policy optimization step, i.e., ηn+1 - ηn . Since we are considering the MBRL setting, it
is natural to express the improvement bound in terms of the expected return under the model η and
thus obtain the following
ηn+1 - ηn
Kn+1 - Kn
True policy
improvement
ηn+1 - ηn	+ ηn+1	- 0n+1	+ @n	-	@n
rηn+1 - ηn	+ ηn+1	- f∏n+1	+ ηln	-	Kn
ηn+1 - ηn - |Kn+1 - ηn+1 | - |nn - Kn |-
'------{z-----} ×--------------------} X--------{-----}
Model policy	Off-policy	On-policy
improvement	model error	model error
≥
According to this bound, the improvement of the policy under the true environment is governed by
the three terms on the LHS:
•	Model policy improvement: This term is what we are directly optimizing in MBRL offset
by the return of the previous iteration ηn, which is constant given the current policy ∏
n.
Assuming that we are not at an optimum, standard policy optimization algorithms guarantee
that this term is non-negative.
•	Off-policy model error: The last term is the difference in return for the true environment
and model under the improved policy πn+1. This depends largely on the generalization
properties of our model, since it is not trained on data under πn+1.
•	On-policy model error: This term compares the on-policy return under πn between the true
environment and the model and it is zero for any model P= p. Since We have access to
transitions from the true environment under the πn , the replay buffer Eq. (3) fulfills this
condition under certain circumstances and the on-policy model error vanishes, see Lemma 2.
Note that the learned model Eq. (2) is able to generalize to unseen state-action pairs better than the
replay buffer Eq. (3) and accordingly will achieve a lower off-policy model error. The motivation
behind opc is therefore to combine the best of the learned model and the replay buffer to reduce both
on- and off-policy model errors.
B.2	Properties of the Replay Buffer
The benefit of the replay buffer Eq. (3) is that it can never introduce any model-bias such that any
trajectory sampled from this model is guaranteed to come from the true state distribution. Accordingly,
if we have collected sufficient data under the same policy, the on-policy model error vanishes.
Lemma 2. Let M be the true MDP with (stochastic) dynamics p, bounded reward r < rmax and
let Pdata be the transition modelfor the replay buffer Eq. (3). Further, consider a Set OftrajeCtOrieS
D = Sb=ι{(sb, ab, sb+ι )}T=o1 collected from M under policy π. If we collect more and more
on-policy training data under the same policy, then
lim Pr (∖η 一 ηreplay | > ε) = 0 ∀ε > 0
B→∞
where ηreplay is the expected return under the replay buffer using the collected trajectories D.
Proof. First, note that the corresponding expected return for the replay-buffer model is given by
B T-1
ηreplay = B XX『(sb, ab),
b=1 t=0
17
Published as a conference paper at ICLR 2022
which is a sample-based approximation of the true reward η . By the weak law of large numbers (see
e.g., Blitzstein & HWang (2019, Theorem 10.2.2)), the Lemma then holds.	□
B.3	Properties of the Learned Model
FolloWing Janner et al. (2019, Lemma B.3), a general bound on the performance gap betWeen tWo
MDPs With different dynamics can be given by
H
Ini - η2∣≤ 2rmax∈m EtYt∙	(10)
where Em ≥ maxt Es〜p；(s)KL(p1(s0, a)∣∣p2(s0, a)) bounds the mismatch between the respective
transition models. NoW, the final form of Eq. (10) depends on the horizon length H. For H → ∞, We
obtain the original result form Janner et al. (2019) with Pt≥ι tγt = γ∕(1 - Y)2. For the finite horizon
case one can obtain tigther bounds when H is smaller than the effective horizon, H < γ∕(1 - γ),
encoded in the discount factor:
Xtγt ≤ min n),„o,	(11)
t=1	2	1 - Y (1 - Y)2
which can be verified by upper bounding t ≤ H to obtain H∕(1 - Y) or by bounding Y ≤ 1 to obtain
O(H2).
Note that this bound is vacuous for deterministic policies, since the KL divergence between two
distributions with non-overlapping support is infinite. In the following we focus on the Wasserstein
metric under the Euclidean distance.
B.4	Properties of opc
In this section, we analyze the properties of opc relative to the true, unknown environment’s transition
distribution P and a learned representation p. In general, the OPC-model mixes observed transitions
from the environment with the learned model. The resulting transitions are then a combination of the
mean transitions from the learned model, the aleatoric noise from the data (the environment), and the
mean-error between our learned model and the environment.
B.4.1	Proof of Lemma 1
In this section, we prove Lemma 1 by showing that the opc-model coincides with the replay-buffer
Eq. (3) in the case of a deterministic policy and thus lead to the same expected return n.
∙-v
Lemma 3. Let M be the true MDP with (stochastic) dynamics p and let M be a MDP with the same
reward function r and initial state distribution ρo, but different dynamics Pmodel, respectively. Further,
assume a deterministic policy π : S → A and a set of trajectories D = Ub=1{(Sb, ab, Sb+ι)}T=01
collectedfrom M under π. Ifwe extend the approximate dynamics Pmodel by OPC with data D, then
nreplay = nopc
where nreplay and nopc are the model-based returnsfollowing models Eqs. (3) and (5), respectively.
Proof. For the proof, it suffices to show that the resulting state distributions of the two transition
models Pdata andPoPc under the deterministic policy ∏ are the same for all b with 1 ≤ b ≤ B. We
show this by induction:
18
Published as a conference paper at ICLR 2022
(Theorem 1)
〔Lemma 8)	[Lemma 10)	[Lemma 4)	(Lemma 9)	(Lemma 11)
(Lemma 7〕	〔Corollary 1]	[Lemma 13]
(Theorem 2〕	〔Lemma ②
Figure 6: Overview about the supporting Lemmas for the proof of Theorem 1.
For t = 0 the states are sampled from the initial state distribution, thus We have so = S0 by definition.
Assume that St = Sb as an induction hypothesis. Then
pοpc(st+ι | St, π(st), b) = δ(st+ι - Sb+, * δ卜t+ι - [f (st, ∏(st)) - f (Sb, ab)])
=6即1 - [Sb+ι + f(St,π(St)) - f(Sb,aa)
=δ(st+1 - hSb+1 + f (Sb, ab) - f (Sb, ab)i)
=δ(st+1 - Sb+l)
=Pdata(St+1 11 +1,b)
Where the second step folloWs by the induction hypothesis and due to the deterministic policy. Thus,
for any index b we have Topc = Tb and the result follows.	□
NoW, combining Lemmas 2 and 3 proofs the result in Lemma 1.
B.4.2	Proof of Theorem 1
In this section we prove our main result. An overview of the lemma dependencies is shown in Fig. 6.
Remark on Notation In the main paper, we unify the notation for state sequence probabilities of
the different models Eqs. (2), (3) and (5) as F(Tt：t+H | t, b) with X ∈ {replay, model, opc}. This
allows for a consistent description of the respective rollouts independent of the model being a learned
representation, a replay buffer or the opc-model. For that notation, the index b denotes the sampled
trajectory from the collected data on the real environment. Implicitly, we therefore condition the state
sequence probability on the observed transition tuples [(Sb+ι, Sb, ab),..., (Sb+H, Sb+H-ι, ab+H-ι)],
i.e.,
Px(τt.t+H It,b) = PX(Tt:t+H | (Sb+1,Sb,ab),...,(Sb+H,Sb+H-1,ab+H-ι)),	(12)
where we omit the explicit conditioning for the sake of brevity in the main paper. Similarly, we can
write the one-step model Eq. (5) for opc in an explicit form as
Popc(St+“St, at, b) = Popc(St+“St, at, Sb+ι, Sb, ab).	(13)
Note that with the explicit notation, the relation between the opc-model Eq. (5) and generalized opc-
model Eq. (6) becomes clear:
P?Pc(St+ι ∣ St, at, b) = Popc(St+ι ∣ St, at, Sb, ab) = /Popc(St+ι ∣ St, at, Sb+唐,ab)dSb+「(14)
19
Published as a conference paper at ICLR 2022
For the following proofs, we stay with the explicit notation for sake of clarity and instead omit the
conditioning on b.
Generalized OPC-Model In this section, we have a closer look at the generalized OPC-model
Eq. (6). The main difference between Eqs. (5) and (6) is that the former is in fact transitioning
deterministically (the stochasticity arises from the environment’s aleatoric uncertainty which manifests
itself in the observed transitions). The two models can be related via marginalization of sb+ι, see
Eq. (14). The resulting generalized opc-model can then be related to the true transition distribution
according to the following Lemma.
Lemma 4. For all sb, ab ∈ S ×A with sb+ι 〜p(St+ι | sb, ab) it holds that
popc(st+ι
| st, at, sb, ab) = P (st+ι — Iy(St, at) - f(sb, ab)] | st, at,
∖	'------------------------}
(15)
{^^^^^^^—
Mean correction
where f(st, at) = E[pmodel(st+ι | st, at)] is the mean transition of the learned model Eq. (2) and
popc(st+ι | st, at, sb, ab) denotes the OPC-model if we marginalize over the distribution for sb+ι
instead of using its observed value.
Proof. Using the explicit notation Eq. (13), the OPC-model from Eq. (5) is defined as
Popc(st+ι | st, at,b) = Popc(st+1 | St, at, sb, ab, sb+ι)	(16)
=δ(st+ι — sb+ι) * δ(st+ι - [/(st, at)- f(sb, ab)]),	(17)
=δ(st+ι - [sb+ι + f(st, at)-『(sb, ab)]).	(18)
With sb+ι 〜p(st+ι | sb, ab), marginalizing st+ι yields (note that we use st+ι instead of st+ι to
denote the random variable for the next state in order to distinguish it from the random variable for
popc(st+ι | st, at, b) under the integral)
Popc(st+11 st, at, sb, ab) = / δ(st+ι- [st+1 + f®, at) - f(sb, ab)] %肉+11 sb, ab) dst+1,
=P (st+1 - [.f(st, at) - f(sb, ab)] | sb, ab).
□
Remark 1. An alternative way of writing the general OPC-model is the following,
POPc(St+1 | st,at,
sb, ab)= P(st+11 sb, ab) * δ
'--------------{------} |
On-policy transition
(st+1 - [f(st, at) - f(sb, ab)i),
_ - /
{z
Mean correction term
(19)
which highlights that the POPc transitions according to the true on-policy dynamics conditioned on
data from the replay buffer, combined with a correction term. We can further explicitly see why
an implementation of this model wouldn’t be possible due to its dependency on the true transition
probabilities. Thus, in practice, we’re limited to the sample-based approximation shown in the paper.
The fundamental idea for the proof of Theorem 1 lies in the following Lemma 5, which is the
foundation for bounding the on-policy error. The Wasserstein distance naturally arises in bounding
this type of error model as it depends on the expected return under two different distributions. The
final result is then summarized in Theorem 1.
Lemma 5. Let PQPC be the generalized OPC-model (Cf Lemma 4) and ηopc be its corresponding
expected return. Assume that the return r(st, at) is Lr -Lipschitz and the policy π(at | st) is
Lπ -Lipschitz with respect to st under the Wasserstein distance, then
∣η -ηopc∣ ≤ LrP1+L∏XγtW2(P(st),Popc(st))	(20)
t≥0
20
Published as a conference paper at ICLR 2022
Proof.
∣η -nopci TTEphX Ytr (st, at)i - τ JEopc hX YtrM at)il	(21)
t≥0	t≥0
=∣XYt(. E	E.	.	Jr(st,at)]- 舄	Jr(St,at)])l	(22)
t≥0	St,at〜p(st,at)	st,at〜poPc(St,at)
≤XγtL . E. .jr(st,at)]- 舄	Jr(St,at)]|	(23)
t≥0	I st,^t 〜p(st,at)	St ,at 〜poPc(St,at)	∣
Applying Lemma 8:
≤ X YtLrW2(p(St, at),popc(st, at))	(24)
t≥0
Writing the joint distributions for state/action in terms of their conditional (i.e., policy) and marginal
distributions p(St, at) = π(at | St)p(St):
=XYtLrW2(∏(at I St)P(St),∏(at I St)Popc(St))	(25)
t≥0
Under the assumption that the policies π are Lπ-Lipschitz under the Wasserstein distance, application
of Lemma 10 concludes the proof:
≤ XγtLrP1+L∏W2(p(St),Popc(St)).	(26)
t≥0
□
Lemma 6 (Wasserstein Distance between Marginal State Distributions). Let PoPc(St) and P(St ) be
the marginal state distributions at time t when rolling out from the same initial state S0 under the
same policy with the OPC-model and the true environment, respectively. Assume that the underlying
learned dynamics model is Lf -Lipschitz continuous with respect to both its arguments and the policy
π(a I S) is Lπ -Lipschitz with respect to S under the Wasserstein distance. If it further holds that
the policy’s (co-)variance Var[π(a I S)] = Σπ (S) ∈ Sd+A is finite over the complete state space, i.e.,
maxs∈s trace{Σ∏(s)} ≤ σ∏, then the discrepancy between the marginal state distributions ofthe
two models is bounded
t-1
W2(Popc(St),P(St)) ≤ 2pdAσ∏Lf X(Lf + Ln)t0	(27)
t0=0
Proof. We proof the Lemma by induction.
Base Case: t = 1 For the base case, we need to show that starting from the same initial state So
the following condition holds:
W2(Popc(Sι),P(Sι)) ≤ 2√dAσ2Lf
For ease of readability, we define z = (S, a) and use the notation dP(x, y) = P(x, y) dxdy whenever
no explicit assumptions are made about the distributions.
W2(Popc (si),p(Si))
W2 (〃PoPc(S1 I Zo, zo)dP(Zo, zo),/p(Si ∣ Zo)dP(Zo)
(28)
(29)
Recall that we can write both PoPc and P as convolution between Pe and a Dirac delta (see Lemma 4).
Together with the identity Eq. (54), the Wasserstein distance for sums of random variables Eq. (53)
and noting Wp(Pe (), Pe ()) = 0:
≤ W2	(〃 δ(sι	-	[f (Z0) +『(zo)	- f(Zo)])	dP(Z0,	zo), / δ(S1	-	f (Zo)) dP(Z0))	(30)
21
Published as a conference paper at ICLR 2022
Squaring and using Lemma 9:
r
∙∙w	∙∙w	，
-[f (Zo) + f(zo) — f(Zo)])dp(Zo)p(zo), J δ(Sι - f(Zo)) dp(Zo)
≤ Il kf(Zo) + f(zo) - f(Zo) - f (Zo)k2 dp(Z0,z0)
〃 kf(Zo)- f(Z0)k2 dp(Zo, Zo)
≤ Lf / kso - Sok2
+ ∣∣ao - aok2 dp(So, ao, So, ao)
(31)
(32)
(33)
We are assuming that the initial states of the trajectory rollouts coincide. The joint state/action distri-
bution can then be written as p(So, ao, so, ao) = p(So)π(ao | So)δ(so — So)π(ao | so). Integrating
with respect to So leads to:
=Lf / ∣∣ao — ao∣∣2p(So)∏(ao | So)∏(ao | So)dSo dao dao	(34)
This term describes the mean squared distance between two random actions. Since we condition π on
the same state So, the policy distributions coincide. Define ∆a = ao 一 ao,
=L2fEE[∣∆a∣2]	(35)
J so ∆a
= Lf2 E trace{Var[∆a]}	(36)
^ so
Now Var[∆a] = Var[π(ao | So)] + Var[π(ao | So)] = 2Var[π(ao | So)]
=2Lf E [trace{Var[π(ao | So)]}]	(37)
^ so
This term is in fact less than Eq. (27) for t = 0, thus proofing the base case.
≤ 2pdAσ∏Lf	(38)
Inductive Step We will show that if the hypothesis holds for t then it holds for t + 1 as well.
We explicitly write the following intermediate bound such that its application in the proof is more
apparent, i.e.,
W2(p(^t ),popC(St)) ≤ f kf(Zt-ι) - f(Zt-1)k2 dp(Zt-ι, Zt-ι)	(39)
t-1
≤ 2pdAσ∏Lf X(Lf + L∏)t0,	(40)
t0=o
where the first inequality immediately follows from the same reasoning as in the base case Eq. (28)-
Eq. (32).
W2(p(St+ι,p(St+ι))	(41)
≤ ∕kf(Zt)- f(Zt)∣2 dp(Zt,Zt)	(42)
≤ Lf / ∣∣St - Stk2 dp(Zt, Zt) + Lf / ∣∣at - at∣2 dp(Zt,Zt)	(43)
Applying Lemma 11 to the second integral
≤ (Lf + L∏) / l∣St - St∣2 dp(Zt, Zt) + 2√dA^Lfσ∏	(44)
∙-v	∙-v
We predict along a consistent trajectory, i.e., St = St + f (s1, a1) - f (St-ι, a1)
≤ (Lf + L∏) / ∣f(Zt-i)- f(Zt-i)∣2 dp(Zt-i, Zt-i) + 2√dALfσ∏	(45)
22
Published as a conference paper at ICLR 2022
Assume that the hypothesis Eq. (39) holds for t
t-1
≤ (Lf + Ln) X 2PdAσ∏L2 ^X(Lf + Ln)t + 2PdALfσ∏	(46)
t0=0
t-1
= 2pdAσ∏Lf [1 + X(L2f + L∏)t0]	(47)
t0=0
t
= 2pdAσ∏Lf X(Lf + L∏)t0	(48)
t0=0
□
Theorem 1. Let n?pc and η be the expected return under the generalized OPC-model Eq. (6) and
the true environment, respectively. Assume that the learned model’s mean transition function
f(st, at) = E[pmodel (st+ι | st, at)] is Lf-LiPsChitzandthereward r(st, at) is Lr -Lipschitz. Further,
if the policy π(at | st) is Lπ -Lipschitz with respect to st under the Wasserstein distance and
its (co-)variance Var[π(at | st)] = Σπ (st) ∈ Sd+A is finite over the complete state space, i.e.,
maxst∈s trace{Σ∏(st)} ≤ σ∏, then with Ci = /2(1 + Ln)Lf Lr and C2 = JLf + L∏
lη - n?pcl ≤ 产dA1CICT√T.	(7)
1-γ
Proof. From combining Lemmas 5 and 6 it follows that
lη - n?pc| ≤ ,2pA(1 + Ln)σ∏LfLr X Ytt X(Lf + L∏Yt
t≥0	t0=0
with the shorthand notations Ci = vz2(1 + L∏)Lf Lr and C2 = Lf + L∏
t	It^t
=Ci|A|1 σ∏XγttIXC2t0
t=0	t0=0
Since t ≤ T, we have that Ptt0=0 C2t0 ≤ TC2T
T
≤ Ci|A|4 σ∏ C22 √T X γt
t=0
(49)
(50)
(51)
Since T ≤ ∞, we have with the geometric series PtT=0 γt ≤ 1/(1 - γ)
(52)
□
B.4.3	Definitions, Helpful Identities and Supporting Lemmas
Here we briefly summarize some basic definitions and properties that will be used throughout the
following.
• The Wasserstein distance fulfills the properties of a metric: Wp(pi,p3) ≤ Wp(pi,p2) +
Wp(p2,p3).
• Wasserstein distance of sums of random variables (see, e.g., Mariucci & Reiβ (2018,
Corollary 1) for a proof):
n
Wp(pι * …*pn,qι * …* qn) ≤ EWp(Pi,qi)	(53)
i=i
23
Published as a conference paper at ICLR 2022
•	For any function g(z, Z) We have
p(st+1
一g(z, Z))ν(z, Z)dz dZ = P *
δ(st+1
一g(z, Z))V(z, Z) dz dz
(54)
•	For any multivariate random variable z1 and z2 With probability distributions p(z1) =
p1 (x)q(y) and p(z2) = p2 (x)q(y), respectively, We have that (Panaretos & Zemel, 2019)
W22(p1(x)q(y), p2(x)q(y)) = W22(p1(x),p2(x)).	(55)
Further, the folloWing Lemmas are helpful for the proof of Theorem 1.
Lemma 7 (Kantorovich-Rubinstein (cf. Mariucci & Reiβ (2018) Proposition 1.3)). Let X and
Y be integrable real random variables. Denote by μ and μ their laws [...]. Then the following
characterization of the Wasserstein distance of order 1 holds:
Wι(ν,μ) =	sup E [φ(x)] — E [φ(y)],	(56)
kΦkLip≤ι x~ VG)	y~μG)
where the supremum is being taken over all φ satisfying the Lipschitz condition ∣φ(x)-φ(y)∣ ≤ |x—y|,
for all x, y ∈ R.
Lemma 8. Let f be Lf -Lipschitz with respect to a metric d. Then
I E, f(x)] — E, f(y)]∣≤ LfWι(ν,μ) ≤ LfW2(ν,μ)	(57)
X 〜ν(∙)	y 〜μ(∙)
Proof. The first inequality is a direct consequence of Lemma 7 and the second inequality comes
from the well-known fact that if 1 ≤ P ≤ q, then Wp(μ, V) ≤ Wq(μ, V) (cf. Mariucci & Reiβ (2018,
Lemma 1.2)).	□
Lemma 9. For any two functions f(s) and g(s) and probability density P(s) that govern the distri-
butions defined by
P1(x1) =	δ(x1 — f (s))P(s) ds and P2(x2) =	δ(x2 — g(s))P(s) ds,	(58)
it holds for any q ≥ 1 that
Wqq (P1,P2) ≤	kf (s) — g(s)kqP(s)ds.	(59)
Proof. We have
Wq(P1(x1),p2(x2))=	inf、〃陶—ξ2∣∣q Y(ξ1,ξ2)dξ1 dξ2	(60)
q	γ∈Γ(p1,p2)
Enforcing the following structure on γ(ξ1,ξ2) reduces the space of possible distributions: γ(ξ1,ξ2) =
R δ(ξ1 — f (s))δ(ξ2 — g(s))P(s) ds, sothatγ(ξ1,ξ2) ∈ Γ(P1,P2) and thus
≤ / kξι — ξ2kq∕ δ(ξι — f (s))δ(ξ2 — g(s))p(s)dsdξι dξ2	(61)
Integrating over ξ1 and ξ2 yields
= kf(s) — g(s)kqP(s) ds	(62)
□
Lemma 10. If the policy π(at | st) is Lπ -Lipschitz with respect to st under the Wasserstein distance,
then with p(s, a) = π(a ∣ S)P(S) andp(s, a) = π(a ∣ S)P(S),
W2(p(S,a),P(s,a)) ≤ (1 + L∏)W2(p(S),P(s))	(63)
24
Published as a conference paper at ICLR 2022
Proof.
W2(p(s, a),p(s, a))
= inf [ [ka — ak2 + ks — sk2] γ(a, a, s, s) dadads ds
γ∈Γ(p(s,a),p(s,a)) J L	j
(64)
(65)
Enforcing the following structure on Y reduces the space of possible distributions: γ(s, s, a, a)
γ(s, s)γ(a, a | s, s) with γ(s, s) ∈ Γ(p(s),p(s)) and γ(a, a|s, s) ∈ Γ(π(a | s), π(a | s)).
≤ ,ʌ. inf	n i h hka - ak2γ(a, a |s, s) dadao+ks - sk2i γ(s, s)dsds
γ(s,s)∈Γ(p(s),p(s))	J k J ∖.	J	」
Ygas,s)er(n(a|s),n(a|s))
(66)
Interchange infimum and the integral: Rockafellar (1976, Theorem 3A)
inf inf
Y(s,s) J γ Y(a,a|s,s)
J ka — a∣∣2γ(a, a | S, s) dada + ∣∣S — s∣∣2 }γ(S, s)dSds
(∏(a | S),π(a | s)) + ∣∣S - s∣∣2}γ(S, s) dsds
inf
γ(S,s)∈Γ(p(S),p(s))
(67)
(68)
Using the assumption that the action distribution is Lπ-Lipschitz continuous under the Wasserstein
metric with respect to the state s:
≤ …rnf(s),p(sj(1+Ln )kS-sk2]γ(s,s)dS ds
= (1 + L∏ )W2(p(S),p(s))
(69)
(70)
Lemma 11 (Average Squared Euclidean Distance Between Actions). If the policy π(a | s) is Lπ-
Lipschitz with respect to s under the Wasserstein distance and the policy’s (co-)variance Var[π(a |
s)] = Σ∏ (S) ∈ S++A is finite over the complete state space, i.e., maxs∈s trace{Σ∏ (s)} ≤ σ∏, then
ʌ E	[kat- atk2] ≤ l∏kst- stk2 + 2PdAσ∏
at 〜∏(st)
at 〜∏(st)
(71)
Proof. Straightforward application of Corollary 1 and Lemma 13
Lemma 12 (Average Squared Euclidean Distance). Consider two random variables x, y with
distributions Pχ,Py, mean vectors μχ,μy ∈ Rm and covariance matrices Σχ, Σy ∈ Sm, respectively.
Then the average squared Euclidean distance between the two is
XEy [kx - yk2] = kμχ - μyk2 + trace {∑χ + Σy}.
(72)
Proof. Define Z = X - y with mean μz = μχ - μy and variance ∑z = Σχ + ∑y.
E kzk2 = E	zi2
i
= XE zi2
i
= X E[zi]2 + Var[zi]
i
=μz μz + trace {ςz }
=k μχ - μy k + trace { ςx + Σy }.
□
□
□
25
Published as a conference paper at ICLR 2022
Theorem 2 (Gelbrich Bound (from KUhn et al. (2019))). If ∣∣ ∙ k is the Euclidean norm, and the
distributions Px and Py have mean vectors μχ,μy ∈ Rm and Covariance matrices Σχ, Σy ∈ Sm,
respectively, then
W2(px,Py) ≥ ∕kμx - μyk2 + trace {∑x + Σy - 2(∑xgy £x/2)1/2}.	(73)
The bound is exact ifPx and Py are elliptical distributions with the same density generator.
Corollary 1. Consider the same setting as in Lemma 12, then the average squared Euclidean distance
is bounded by
E [kx - yk2] ≤W2(px,Py) + 2trace{(£x/2£y£x/2)1/2}.	(74)
x,y
Proof. Straightforward application of the results from Lemma 12 and Theorem 2.	□
Lemma 13. If the policy’s (co-)variance Var[π(a | s)] = Σπ (s) ∈ Sd+A is finite over the complete
state space, i.e., maxs∈s trace{Σ∏(s)} ≤ σ∏, then
trace {(∑∏⑶ 1〃Σ∏(s)Σ∏⑶ 1/2)1/2} ≤ pdA,σ2π	(75)
Proof.
trace {(∑∏ ⑶1/2 Σ∏ (s)Σ∏ ⑶1/2)1/2 }	(76)
The trace of a matrix is the same as the sum of its eigenvalues, and the square root of a ma-
trix has eigenvalues that are square root of its eigenvalues. From Jensen,s inequality we know
that PdAI √λi ≤ JdA PdAi λi and consequently it holds for a matrix M ∈ RdA ×dA that
trace{M1∕2} ≤ PdA trace{M} , so that
≤ Jd/ trace{Σ∏(S)1∕2Σ∏(s)Σ∏(S)1∕2}	(77)
The trace is invariant under cyclic permutation
= PdA trace{ Σ∏ (S)Σ∏ (s)}	(78)
Since both matrices are positive semi-definite, it follows from the Cauchy-Schwartz inequality that
≤ Pd/ trace{Σ∏ (S)} trace{Σ∏ (s)}	(79)
By assumption, the covariance matrices’ traces are bounded
≤ pAσ2	(80)
□
B.5 Model Errors in opc
While Theorem 1 highlights that OPC counteracts the on-policy error in predicted performance, for
stochastic policies we use the Lipschitz continuity of the model to upper-bound errors. In this section,
we look at the impact of model errors in combination with opc. Specifically we focus on the one-step
prediction case from a known initial state S°. There, while for the model Pmodel without OPC the
prediction error only depends on the quality of the model, with opc it is instead the minimum of the
model error and the policy variance. This is advantageous, since typical environments tend to have
more states than actions, so that the trace of the policy variance can be significantly smaller than the
full-state model error.
Lemma 14. Under the assumptions of Theorem 1, starting from an initial state So the following
condition holds:
W2(p?Pc(SI),P(Si))
≤ min (θ 卜race{Var[π(∙ | So)]}),O(J
kf(So, a) —『(So, a)k2 d∏(a |
{^^^^^^^^^^^^^^^^^^^™
One-step model error
26
Published as a conference paper at ICLR 2022
Proof. The first term in the minimum follows directly from the base-case of Lemma 6. For the second
term, follow the same derivation, but note that under the distribution p(So, ao, so, a0) = p(S0)∏(a0 |
S0)δ(s0 - S0)π(a0 | so) We have Jp(Sι | Zo) dp(Zo) = Jp(sι | zo) dp(zo). Inserting this into the
r.h.s. of Eq. (28) and following the same steps we obtain
w2(P?pc(si),p(Si))
≤ W2 (Ij δ(sι - [f (Zo) + f(zo)-『(Zo)]) dp(Zo)p(zo), / δ(sι - f (zo)) dp(z°))	(81)
≤ 〃 kf(zo) + f(zo) - f(zo) - f(zo)k2 dp(zo, zo) =ZZ kf(zo)- f(zo)k2 + kf(zo)- f(zo)k2 dp(zo, zo) =2 Z kf(zo)- f(zo)k2 dp(zo) =2/ kf(So,a)-『(So,a)k2dp(a | So)	(82) (83) (84) (85)
□
Note the additional factor of two in front of the upper bound on the model error, which comes from
using the model ‘twice,: once with Z and once with z. In practice we do not see any adverse effects
of this error, presumably because either the variance of the policy is sufficiently small, or due to the
upper bound being lose in practice.
C Motivating Example — In-depth Analysis
In this section, we re-visit the motivating example presented in Section 4.1 of the main paper. For
completeness, we re-state all assumptions that lead to the simplified system at hand. We continue
with an analysis of the reward landscape and how opc influences its shape. Next, we investigate how
an increasing mismatch of the dynamics model impacts the gradient error. In addition to the result
presented in the main paper, we here show the influence of different model errors, i.e., ∆A as well as
∆B. While opc is motivated for the use case of on-policy RL algorithms, we further show that the
resulting gradients are robust with respect to differences in data-generating and evaluation policy,
i.e., the off-policy setting. Lastly, we state the the signed gradient distance that we use for evaluation
of the gradient errors, state the relevant theorem for determining the closed-loop stability of linear
systems, as well as all numerical values used for the motivating example.
C.1 Setup
Here, we assume a linear system with deterministic dynamics
p(st+1 | st,at) = δ(Ast + Bat | st,at), ρo(so) = δ(so)
(86)
with A, B ∈ R and δ(∙) denoting the Dirac-delta distribution. The linear policy and bell-shaped
reward are given by the following equations
πθ(at | st) = δ(θst | st) with θ ∈ R
and r(st, *=exp [-陵)]∙
Further, we assume to have access to an approximate dynamics model P
P(St+1 | st, at) = δ((A + δA)St + (B + ∆B)at 1 st, at),
(87)
(88)
where ∆A, ∆B quantify the mismatch between the approximate model and the true system. For
completeness, the (deterministic) policy gradient is defined as
1 T-1
口 τ ∑r(st, at),
T t=o
(89)
27
Published as a conference paper at ICLR 2022
—— True system …∙ ∙ Model
---Model + OPC
----Reference policy πn
Figure 7: Cumulative reward for different systems as a function of the policy parameter. The reference
trajectory that is used for OPC was generated by πθn (denoted by the black dashed line). The model
mismatch between the true system and the approximated model is ∆A = 0.5, ∆B = 0.0.
where the state/action pairs are obtained by simulating any of the two above models for T time-steps
and following policy ∏n resulting in the trajectory Tn = {(s7, an)}T=o1. BecaUSe both the model
and policy are deterministic, we can compute the analytical policy gradient from only one rollout.
C.2 Reward Landscapes
In a first step, we will look at the cumulative rewards as a function of the policy parameter for the
different systems at hand: 1) the true system, 2) the approximate model without opc and 3) the
approximate model with opc. Further, let’s assume that the model mismatch is fixed to some arbitrary
value. The resulting reward landscapes are shown in Fig. 7. We would like to emphasize several
key aspects in the plots: First, as one would expect the model mismatch leads to different optimal
policies as well as misleading policy gradients for large parts of the policy parameter space. Second,
the reward landscape for the model with OPC depends on the respective reference policy πn that
was used to generate the data for the corrections. Consequently, the correct reward is recovered at
θ = θn. More importantly, the OPC reshape the reward landscape such that the policy gradients point
towards the correct optimum (left plot). Lastly, even when using opc the policy gradient’s sign is
not guaranteed to have the correct sign (right plot). The extent of this effect strongly depends on the
model mismatch, which we will investigate in the next section.
C.3 Influence of Model Error
As shown in the previous section, the estimated policy gradient depends on the current policy as well
as the mismatch between the true system and the approximate model. Fig. 2 depicts the (signed)
differences between the true policy gradient as well as the approximated gradient as a function of
model mismatch and the reference policy. Here, the opacity of the background denotes the magnitude
of the error and the color denotes if the true and estimated gradient have the same (blue) or oppposite
(red) sign. In the context of policy learning, the sign of the gradient is more relevant than the
actual magnitude due to internal re-scaling of the gradients in modern implementations of stochastic
optimizers such as Adam (Kingma & Ba, 2015). In our example, even for negligible model errors
(either in ∆A or ∆B), the model-based approach can lead to gradient estimates with the opposite
sign, indicated by the large red areas for the left figures in Fig. 2. On the other hand, applying opc to
the model, we gradient estimates are significantly more robust with respect to errors in the dynamics.x
28
Published as a conference paper at ICLR 2022
(b) Gradient error when varying model error ∆B
Figure 8: Signed gradient error (see Equation equation 90) when using the approximate model to
estimate the policy gradient without (left) and with (right) on-policy corrections (OPCs). Using OPCs
increases the robustness of the gradient estimate with respect to the model error.
C.4 Influence of Off-Policy Error
Until now we have considered the case in which the reference trajectory used for opc is generated
with the same policy as the one used for gradient estimation, i.e., the on-policy setting. In this case,
we have observed that the true return could be recovered (see Fig. 7) when using opc and that the
gradient estimates are less sensitive to model errors (see Fig. 8). The off-policy case corresponds
to the policy gains in Fig. 7 that are different from the reference policy πn indicated by the dashed
line. Fig. 9 summarizes the results for the off-policy setting. Here, we varied the policy error and the
reference policy itself for varying model errors. Note that for the correct model, we always recover
the true gradient. But also for inaccurate models, the gradient estimates retain a good quality in most
cases, with the exception for some model/policy combinations that are close to unstable.
29
Published as a conference paper at ICLR 2022
(b) Model error ∆B.
Figure 9: Signed gradient error due to off-policy data when using OPC. Note that we retain the true
gradient in case of no model error.
C.5 Additional Information
Next, we provide some additional information about how we compute gradient distances, properties
of linear systems, and exact numerical values used.
C.5. 1 Computing the Signed Gradient Distance
Figure 10: Sketch depicting the signed gradient distance Eq. (90). In this particular case, gradient g1
is positive and g2 is negative.
In order to compare two (1-dimensional) gradients in terms of sign and magnitude, we use the
following formula
1 sign(g2) ∙ ∆g,	if	gι	=0
d(gι,g2) = sign(gι) ∙ ∆g,	if	g2	= 0	,
π
sign(gι ∙ g2) ∙ ∆g, otherwise
with ∆g = |arctan g1 - arctan g2 | . (90)
The magnitude of this quantity depends on the normalized difference between the tangent’s angles ∆g
and is positive for gradients with the same sign and vice versa it is negative for gradients with opposing
signs. See also Fig. 10 for a sketch.
30
Published as a conference paper at ICLR 2022
C.5.2 Determining the Closed-loop Stability for Linear Systems
For linear and deterministic systems, we can easily check if the system is (asymptotically) stable for
a particular linear policy using the following standard result from linear system theory:
Theorem 3 (Exponential stability for linear time-invariant systems (Callier & Desoer, 1991)). The
solution of xt+1 = Fxt is exponentially stable if and only if σ(F) ⊂ D(0, 1), i.e., every eigenvalue
of F has magnitude strictly less than one.
In our setting, this means that the closed-loop systems fulfilling the following are unstable,
|A + ∆A + (B + ∆B)θ∣ > 1,	(91)
i	.e., the state and input grow exponentially. We therefore refrain from including unstable system in
the results to avoid numerical issues for the gradients’ computation. The respective areas in the plots
are not colored, see e.g., bottom left corner in Fig. 8b.
C.5.3 Numerical Values
The numerical values for all parameters used in the motivating example are given as follows:
•	True system dynamics: A = 1.0, B = 1.0
•	Initial condition: s0 = 1.0
•	Reward width parameter: σr = 0.05
•	Optimal policy gain: θ* = -1.0
•	Rollout horizon: T = 60
31
Published as a conference paper at ICLR 2022
D Additional Experimental Results
In this section, we provide additional experimental results that did not fit into the main body of the
paper.
D.1 Comparison with Other Baseline Algorithms
Fig. 11 shows our method compared to a range of baseline algorithms. The results for all baselines
were obtained from Janner et al. (2019) via personal communication. Note that all results are
presented in terms of mean and standard deviation. The comparison includes the following methods:
•	mbpo (Janner et al., 2019),
•	pets (Chua et al., 2018),
•	sac (Haarnoja et al., 2018),
•	ppo (Schulman et al., 2017),
•	steve (Buckman et al., 2018),
•	slbo (Luo et al., 2019).
HalfCheetah	Hopper
0	50	100	150	200
# Steps ×103
Walker2d
0	20	40	60	80	100	120
# Steps ×103
AntTruncatedObs
0	50	100	150	200	250	300
# Steps ×103
----OPC	----MBPO(?)
----STEVE ----------- SAC
0	50	100	150	200	250	300
# Steps ×103
----MBPO ------------ PETS
----PPO	---- SLBO
Figure 11:	Comparison of opc against a range of baseline methods on three MuJoCo environments.
We present the mean and standard deviation across 5 independent experiments (10 for opc and
mbpo(?)). The original data for mbpo and the other baselines were provided by Janner et al. (2019).
Solid lines represent the mean and the shaded areas correspond to mean ± one standard deviation.
32
Published as a conference paper at ICLR 2022
D.2 Ablation - Retain Epochs
One of the hyperparameters that we found is critical to both opc and mbpo(?), is retain epochs,
i.e., the number of epochs that are kept in the data buffer for the simulated data generated with the Px
with x = {OPC, model}. The results for a comparison are shown in Fig. 12. For mbpo(?), we found
that for some environments (HalfCheetah, AntTruncatedObs) smaller values for retain epochs
are helpful, i.e., simulated data is almost only on-policy, and for other environments larger values are
beneficial (Hopper, Walker2d). For opc on the other hand, we found that retain epochs = 50
almost always leads to better results.
10 5 0
301× nruter noitaulavE
oCoJuM
HalfCheetah
Hopper
Walker2d
3
telluByP
1.5
1.0
0.5
0.0
0	200
# Steps ×103
----MBPO(?) (50)
AntTruncatedObs
4
3
2
1
0	200
# Steps ×103
....MBPO(?) (5)
# Steps ×103	# Steps ×103
-----OPC (50)
H OPC (5)
Figure 12:	Ablation study for opc and mbpo(?) on four environments from the MuJoCo con-
trol suite (top row) and their respective PyBullet implementations (bottom row). We vary the
retain epochs hyperparameter (indicated by the number in the parentheses behind the legend
entries), i.e., the number of epochs that are kept in the data buffer for the simulated data.
D.3 Influence of State Representation: In-Depth Analysis
In the following, we will have a closer look at the surprising result from Fig. 4 (left). In there, the
results indicate that mbpo(?) is not able to learn a stabilizing policy within the first 7’500 steps on
the RoboSchool variant of the CartPole environment. We hypothesize that the failure of mbpo(?) is
due to a mismatch of the simulated data and the true state distribution. As a result, the policy that is
optimized with the simulated data cannot stabilize the pole in the true environment.
To validate our hypothesis, We perform the following experiment: First, We train a policy ∏* that
we know performs well on the true environment, leading to the maximum evaluation return of 1000.
With this policy, we roll out a reference trajectory on the true environment Tref = {(St, a/}1。.
To perform branched rollouts With the respective methods, opc and mbpo(?), We use the learned
transition models after 20 epochs (5’000 time steps) that were logged during a full learning process
for each method. We then perform 100 branched rollouts of length H = 20 starting from randomly
sampled initial states of the reference trajectories.
Fig.	13 shows the difference between the true and predicted state trajectories (median and 95th
*
percentiles) for each state in [x, Cos(H), Sin(H), X,例.Since we start each branched rollout from
a state on the real environment, the initial error is always zero. Across all states, the errors are
drastically reduced when using opc. Fig. 14 shows the predicted trajectories for the cosine of the
pole’s angle, cos(H). For mbpo(?), we observe that the trajectories often diverge and attain values
that are clearly out of distribution, i.e., cos(H) > 1.
33
Published as a conference paper at ICLR 2022
- eurts:ecnereffid etatS
Lateral Position
cos(Angle)	sin(Angle)
10	15	20
time steps
5	10	15	20
time steps
5	10	15	20
time steps
5 derp
-ɔuə-lwp əl-s
Lateral Velocity
Angular Velocity
0.5
0.0
-0.5
5	10	15	20	5	10	15	20
time steps	time steps
Figure 13: Difference in state trajectories sttrue - stpred from branched rollouts using a fixed policy of
.
length H = 20 on the RoboSchool environment (cf. Fig. 4 (left)) with S = [x, Cos(H), Sin(H), X, £].
The solid lines show the median across 100 rollouts and the shaded areas represent the 95th percentiles.
With OPC (------), the simulated rollouts follows the true state trajectories much closer, whereas with
mbpo(?) ( ) the prediction errors quickly accumulate over time.
4208
.................................
1110
(⅛ )soɔ-^uvpəm.lojsue-lH
321098
000099
...........................................
111100
5	10	15	20	5	10	15	20
Figure 14: Trajectories of the second state (cos(H)) from 100 branched rollouts using a fixed policy
of length H = 20 on the RoboSchool environment (cf. Fig. 4 (left)). Both plots present the same
data but the differ in terms of scaling of the ordinate. With OPC (----------), the respective trajectories
remain around values close to one, which corresponds to the upright position of the pendulum. When
using the standard predictive model from mbpo(?) ( ), the state trajectories often diverge and the
rollouts are terminated prematurely.
34
Published as a conference paper at ICLR 2022
-IoJJ 巴 əpon
yciloP-nO
əpon
yciloP-ffO
nη
nruter eurT
200
100
0
300
200
I
100
0
6	7	8	9	10	11	12	13
Iteration n of Policy Optimization
Figure 15: Empirical evaluation of the error terms in the policy improvement bound in Eq. (4) for the
CartPole environment (PyBullet). We evaluate the respective terms for a sequence of policies that
were obtained during different iterations n from a full policy optimization run. The respective returns
ηn+ι, ηn, ηn+1,ηn+1 are approximated as the mean from 100 rollouts on the true environment and
the respective models, PnPc (------) and Pmodel (-----). For the return on the true environment ηn (top),
we additionally show the sample distribution of the rollouts’ returns. This nicely demonstrates how
the policy smoothly transitions from failing consistently (n ≤ 8) to successfully stabilizing the
pole (n ≥ 12). Additionally, note that the on-policy model error is almost always smaller for opc
compared to mbpo(?), which supports the theoretical motivation that our method is build upon.
D.4 Improvement Bound: Empirical Investigation
In this section, we empirically investigate to what extent opc is able to tighten the policy improvement
bound Eq. (4) compared to pure model-based approaches. As mentioned in the main paper, the
motivation behind opc is to reduce the on-policy error and we assume that the off-policy error is
not affected too badly by the corrected transitions. Generally speaking, it is difficult to quantify
a-priori how opc compares to a purely model-based approach as this depends on the generalization
capabilities of the learned dynamics model, the environment itself and the reward signal.
Here, we analyze the CartPole environment (PyBullet implementation) and estimate the respective
error terms that appear in the policy improvement bound Eq. (4). To this end, we roll out a sequence
of policies πn on the true environment (to estimate ηn and ηn+1 ) and on the learned model with and
without OPC (to estimate ηn and ηn+ι). The sequence of policies was obtained during a full policy
optimization run (the policy was logged after each update) and we roll out each policy 100 times
on the respective environment/models. The corresponding learned transition models were similarly
logged during a full policy optimization. For OPC, we estimate the off-policy return τjn+ι using
the learned model from iteration n and reference trajectories from the true environment that were
collected under πn, but then roll out the model with πn+1. The results are shown in Fig. 15.
35
Published as a conference paper at ICLR 2022
UmsXA。二 odgo IUməXA-Iodgo
OPC
1000
750
500
250
0
mbpo(*)
1000
750
500
250
0
1.1	1.3	1.5	1.7	1.9	2.1	2.3	2.5	2.7	2.9	3.1
Policy Stochasticity Multiplier β
Figure 16: Sample distributions of the return on the CartPole environment (PyBullet) with in-
creasing Stochasticity of the behaviour policy ∏roiiout When rolling out with PnPc/OPC (top) and
PmOdel/mbpo(?) (bottom). The multiplier β quantifies the Stochasticity of ∏roiiout relative to the
reference policy πn (Eq. (92)) such that higher values lead to more ‘off-policy-ness’.
D.5 Off-Policy Analysis
In this section, we investigate the robustness of opc towards ‘off-policy-ness’ of the reference
trajectories that are simulated under the data-generating policy πn . To this end, we manually increase
the stochasticity of the behavior policy πrOllOut by a factor β such that
V[∏rollout(∙ | S)] = β2V[∏n(∙ | S)],	(92)
and we keep the mean the same for both policies. Fig. 16 shows the distributions of the returns from
100 rollouts with varying degree of ‘off-policy-ness’ on the CartPole environment (PyBullet). Note
that the data-generating policy consistently leads to the maximum return of 1000. For β close to
one, both opc and mbpo(?) lead to almost ideal behavior and correctly predict the return in more
than 95% of the cases. As we increase the policy’s stochasticity (from left to right), the respective
performance of the policy decreases until all rollouts terminate prematurely (β ≥ 2.3). Notably,
the extent of this effect is almost identical between opc and mbpo(?). We conclude that opc is, at
least empirically, robust towards ‘off-policy-ness’ of the reference trajectories. Otherwise, we should
observe a more pronounced degradation of the policy’s performance with increased stochasticity of
the behavior policy, since this leads to observing more off-policy state/actions.
36
Published as a conference paper at ICLR 2022
Algorithm 3 Original MBPO algorithm
1:	Initialize policy πφ, predictive model pθ, environment dataset Denv, model dataset Dmodel
2:	for N epochs do
3:	Train model pθ on Denv via maximum likelihood
4:	for E steps do
5:	Take action in environment according to πφ ; add to Denv
6:	for M model rollouts do
7:	Sample st uniformly from Denv
8:	Perform k-step model rollout starting from st using policy πφ ; add to Dmodel
9:	for G gradient updates do
ʌ
10:	Update policy parameters on model data: φ J φ - λ∏VφJ∏ (φ, Dmodel)
Algorithm 4 Our version of MBPO algorithm, denoted as MBPO(?)
1:	Initialize policy πφ, predictive model pθ, environment dataset Denv, model dataset Dmodel
2:	for N epochs do
3:	Train model pθ on Denv via maximum likelihood
4:	for E steps do
5:	Take action in environment according to πφ ; add to Denv
6:	for M model rollouts do
7:	Sample st uniformly from Denv
8:	Perform k-step model rollout starting from st using policy πφ ; add to Dmodel
9:	for G gradient updates do
ʌ
10:	Update policy parameters on model data: φ J φ - λπVφJπ (φ, Dmodel)
D.6 Implementation Changes to Original mbpo
Here, we provide details to the changes we made to the original implementation of mbpo. We denote
our variant as mbpo(?).
Episodic Setting MBPO updates the policy during rollouts on the real environment. Algorithms 3
and 4 show the original and our version of mbpo, respectively. While the original version might be
more sample efficient because it allows for more policy gradient updates based on more recent data,
it is not realistic to update the policy during a rollout on the real environment.
Mix-in of Real Data As mentioned in the main paper, one of the key problems with MBRL is
that the generated data might exhibit so-called model-bias. Biased data can be problematic for
policy optimization as the transition tuples possibly do not come from the true state distribution
of the real environment, thus misleading the RL algorithm. In the original implementation of
mbpo, the authors use a mix of simulated data from the model as well as observed data from the
true environment for policy optimization (https://github.com/jannerm/mbpo/blob/
22cab517c1be7412ec33fbe5c510e018d5813ebf/mbpo/algorithms/mbpo.py#
L430) to alleviate the issue of model bias. While this design choice might help in practice, it adds
another hyperparameter and the exact influence of this parameter is difficult to interpret. We therefore
refrain from mixing in data from the true environment, but instead only use simulated transition
tuples - staying true to the spirit of MBRL.
Fix Replay Buffer The replay buffer that stores the simulated data Dmodel for policy optimization
is allocated to a fixed size of rollout length×rollout batch size×retain epochs,
meaning that per epoch rollout length × rollout batch size transition tuples are simu-
lated and only the data from the last retain epochs are kept in the buffer (https://github.
com/jannerm/mbpo/blob/22cab517c1be7412ec33fbe5c510e018d5813ebf/
mbpo/algorithms/mbpo.py#L351). As the buffer is implemented as a FIFO queue and
rollouts might terminate early such that the actual number of simulated transitions is less than
rollout length, data from older episodes as specified by retain epochs are retained in the
buffer. We assume that this is not the intended behavior and correct for that in our implementation.
37
Published as a conference paper at ICLR 2022
InVertedPendulum	Hopper	HalfCheetah
O
■
O
O 5
■ ■
1 O
.01 X UJmg UolBA
2.5	5.0	7.5
# Steps × 103
opc (ours)
3 2 1
X UJmg Uola-BAM
0	50	100
# Steps × 103
MBPO β = 5%
O
0 5 0
X UJmg UolBA
100	200
# Steps × 103
....SAC
MBPO β = 0%
Figure 17: Results based on original implementation Appendix D.7: Comparison of opc to the
model-based mbpo and model-free sac algorithms. The two mbpo variants differ in terms of the
mix-in ratio β of real off-policy transitions in the training data - a critical hyperparameter. All
model-based approaches outperform sac in terms of convergence for the high-dimensional tasks.
Moreover, on the highly stochastic Hopper environment, opc outperforms both mbpo variants and
does not require additional real off-policy data.
D.7 Results for Original mbpo Implementation
The following results were obtained with the original mbpo implementation without the changes
described in Appendix D.6. Consequently, the results for opc in this section also do not include these
changes.
Comparative Evaluation
We evaluate the original implementation on three continuous control benchmark tasks from the
MuJoCo control suite (Todorov et al., 2012). The results for opc, two variants of mbpo and sac are
presented in Fig. 3. Both opc and mbpo use a rollout horizon of H = 10 to generate the training data.
The difference between the two mbpo variants lies in the mix-in ratio β of real off-policy transitions
to the simulated training data. Especially for highly stochastic environments such as the Hopper,
this mix-in ratio is a critical hyperparameter that requires careful tuning (see also Fig. 18). Fig. 3
indicates that opc is on par with mbpo on both the InvertedPendulum and HalfCheetah environments
that exhibit little stochasticity. On the Hopper environment, opc outperforms both mbpo variants.
Note that the mix-in ratio for mbpo is critical for successful learning (the original implementation
uses β = 5%). opc on the other hand, does not require any mixed-in real data. sac learns slower on
the more complex Hopper and HalfCheetah environments, re-iterating that model-based approaches
are significantly more data-efficient than model-free methods.
Large Ablation Study - Hopper
The full study investigates the influence of the following hyperparameters and design choices:
•	Rollout length H and total number of simulated transitions N .
•	Mix-in ratio of real transitions into the training data β.
•	Deterministic or stochastic rollouts (for mbpo): Current state-of-the-art methods in MBRL
rely on probabilistic dynamic models to capture both aleatoric and epistemic uncertainty.
Accordingly, when rolling out the model, these two sources of uncertainty are accounted for.
However, we show that in terms of evaluation return, the stochastic rollouts do not always
lead to the best outcome.
•	Re-setting the buffer of simulated data after a policy optimization step: We found that
re-setting the replay buffer for simulated data after each iteration of Algorithm 3 can have
a large influence. In particular, the replay buffer is implemented as a FIFO queue with a
fixed size. Hence, if the buffer is not emptied after each iteration, it still contains (simulated)
off-policy transitions.
Fig. 18 presents the results of the ablation study. We want to highlight a few core insights:
38
Published as a conference paper at ICLR 2022
1.	When choosing the best setting for each method, opc improves mbpo by a large margin
(bottom row, right). Generally, for long rollouts H = 20 (bottom row), opc improves
mbpo.
2.	Across all settings, opc performs well and is more robust with respect to the choices of
hyperparameters (e.g., bottom row left and center, third row left). Only for few exceptions,
re-setting the buffer can have detrimental effects (e.g., top right, third row right).
3.	Mixing in real transition data can be highly beneficial for mbpo (second row left and center)
but it can also have the opposite effect (bottom row).
4.	Using deterministic rollouts can be beneficial (third row right and left), detrimental (third
row center) or have no influence (bottom row left) for mbpo.
5.	It is not clear, if re-setting the buffer after each iteration should overall be recommended or
not. This remains an open question left for future research.
39
Published as a conference paper at ICLR 2022
0	50	100	150
# Steps × 103
3 2 1
0	50	100	150
# Steps × 103
SoIX UJməj UΟIanIBAω
3 2 1
0	50	100	150
# Steps ×103
0	50	100	150
# Steps × 103
3 2 1
SOI × UinsJ UOIsnIBAω
0	50	100	150
# Steps × 103
SoIX UinsJ UOIsnIBAω
3 2 1
0	50	100	150
# Steps ×103
0	50	100	150
# Steps × 103
0	50	100	150
# Steps × 103
0	50	100	150
# Steps ×103
0	50	100	150
# Steps × 103
----- OPC + reset buffer
3 2 1
SoIX UinsJ UΟIsnIBAω
0	50	100	150
# Steps × 103
0	50	100
# Steps ×103
150
--- opc + not reset buffer
----- MBPO + reset buffer + deterministic
----MBPO + not reset buffer + deterministic
----- MBPO + reset buffer + stochastic
--- mbpo + not reset buffer + stochastic
Figure 18: Results based on original implementation Appendix D.7: Large ablation study on the
Hopper environment, investigating the influence various hyperparameters and design choices. Mix-in
ratio of real data β (columns): 0%, 5%, 10% from left to right. Rollout length H (rows): 1, 5, 10, 20
from top to bottom.
40
Published as a conference paper at ICLR 2022
E Connection between MBRL and ILC
In this section we compare optimization-based or so-called norm-optimal ILC (NO-ILC) with MBRL.
In particular, we show that under certain assumptions we can reduce the MBRL setting to NO-ILC.
This comparison is structured as follows: First, we review the basic assumptions and notations for
NO-ILC. While there are many variations on NO-ILC, we will only consider the very basic setting,
i.e., linear dynamics, fully observed state and deterministic state evolution. Then, based on the lifted
state representation of the problem, we derive the solution to the optimization problem that leads to
the input sequence of the next iteration / rollout. Next, we will state the general MBRL problem and
pose the simplifications that we need to make in order to be equivalent to the NO-ILC problem. Last,
we show that the solution to the reduced MBRL problem is equivalent to the one of NO-ILC.
E.1 Norm-optimal ILC
The goal of NO-ILC is to find a sequence of inputs a = [a0>, . . . , aT>-1]> with length T + 1 such
that the outputs y = [y>,..., y>]> follow a desired output trajectory y = [y>,..., y>]τ. In the
simplest setting we assume that the system evolves according to the following linear dynamics
st+1 = Ast + Bat + dt
yt = Cst,
(93)
(94)
with state st ∈ RdS , action at ∈ RdA, output yt ∈ Rp and disturbance dt ∈ RdS . One of
the major assumption in ILC is that the disturbances dt are repetitive, meaning that the sequence
d = [d0τ, . . . , dTτ]τ does not change (or only varies slightly) across multiple rollouts. While these
disturbances can be considered to come from some exogenous error source, one can also interpret
these as unmodeled effects of the dynamics, e.g., nonlinearities stemming from friction, aerodynamic
effects, etc.
For the derivation, let’s assume C = I so that we’re operating on an MDP. Consequently, the goal
of ILC is to track a sequence of desired states S instead of outputs y. In order to find an optimal
input sequence, we minimize the squared 2-norm of a state’s deviation from the reference for each
time-step t of the sequence, i.e., et = St - st. As is common in the ILC literature, we make use of
the so-called lifted system formulation such that we can conveniently write the state evolution for one
rollout using a single matrix/vector multiplication. Assuming zero initial state (which can always
be done in the linear setting by just shifting the state by a constant offset), we obtain the following
formulation
S = Fa + d, with F
0
B
AB
A2B
∈ RdS (T +1)×dA T
AB
(95)
0
B
0
B
Thus, we can write the state-error at the j -th iteration of ILC in the lifted representation as (recall that
the disturbance d does not change across iterations)
e⑴=S - S⑴=S - Fa(i) - d,	(96)
e(i+1) = e(i) - F(a(i+1) - a(i)).	(97)
Now, the resulting optimization problem then becomes
a*i+1) = arg min J (a(i+1)) with J (a(i+1)) = 2 ∣∣e(i+1)k2.
In order to be less sensitive to noise and the inherent stochasticity of real-world problems, one
typically also adds a regularizing term that penalizes the changes in the input sequence. While this
additional penalization term can slow down the learning process it makes it more robust by avoiding
overcompensation to the disturbances. The full objective then becomes
J (a(i+1)) = 2ke(i+1)kM + 1 ∣a(M - a⑴∣W,	(98)
41
Published as a conference paper at ICLR 2022
where we additionally added positive semi-definite cost matrices M, W for the respective norms
in order to facilitate tuning of the corresponding terms in the objective. Given the regularized cost
function we obtain the optimal sequence of inputs at the next iteration as
a[i+1) = a，+ (F>MF + W )-1 F>Me⑴.	(99)
E.2 Model-based RL to Norm-optimal ILC
Assumptions In order to show the equivalence of MBRL and NO-ILC we need to make some
assumptions to the above stated optimization problem.
•	No aleatoric uncertainty in the model, i.e., no transition noise ωt = 0 ∀t.
•	Typically in RL we assume the policy to be stationary, however, in this setting we will allow
for non-stationary policies that are indexed by time t (the result will not necessarily depend
on the state such that we essentially just obtain a feedforward control sequence. To include
feedback one can always just combine the feedforward signal with a local controller that
tracks the desired state/action trajectory).
•	The reward is given as the negative quadratic error of the state w.r.t. a desired state trajectory,
r(st, at) = -2∣∣St - st∣∣2
•	Typically in RL we have constraints on the policy such that it does not change too much
after every iteration, see e.g. TRPO, REPS, etc. While in the mentioned approaches, the
policy are often constrained in terms of their parameterization vectors, we constrain it as
∣πt(i+1) - πt(i)∣2 ≤ π ∀t.
∙∙w
•	Assume that the model is given by ft (s, a) = As + Ba + dt, where A and B are fixed
system matrices and dt is a time-dependent offset that we learn.
The learned dynamics model Given state/input pairs of the i-th trajectory τ(i) = {(st(i), at(i))}tT=0
from the true system, we can now improve our dynamics model. In particular, if we minimize the
prediction error over τ(i), we obtain
dt(i) = st(+i)1 - Ast(i) +Bat(i)	(100)
The resulting dynamics for the optimal control problem in Eq. (1) become
ft(i(s, a) = s(+1+ A(S-S(i)) + B(a - a(i))	(IOI)
which are the error dynamics around the trajectory. Now in the noisy case, taking the last trajectory is
not necessarily the best thing one can do. E.g., (SchOllig & D’Andrea, 2009) instead integrate the
information of all past trajectories via Kalman-filtering. In the fully observed case, one way to think
of this is as low-pass filtering dt in order to account for the transition noise ω .
The resulting MBRL problem Now, let’s plug in all assumptions into the MBRL problem and
have a look at how to solve it.
T1
∏(i+1) = arg min	2 ∣∣St - St∣2
π={π0,...,πT} t=02
St+1 = St(i+)1 + A(S - St(i)) + B(a -a(ti))	(102)
at = πt
∣πt(i) - πt ∣2 ≤ π ∀t
where we have flipped the reward’s sign to transform it into a minimization problem. In the presented
form, this optimization problem has a well-defined unique solution, however, it is a-priori not clear
if the trust-region constraint is active for some time-steps. To facilitate an analytical solution to
equation 102, we incorporate the constrain on the policy’s stepsize by a soft-constraint such that
T
π(i+1) =	argmin	X 2list - stk2 + 于kπ(i) - πtk2
π={π0,...,πT} t=02	2
St+1 = S(ti+)1 + A(S - S(ti)) + B(a -a(ti))
(103)
at = πt,
42
Published as a conference paper at ICLR 2022
with Ctπ being constants that weight the respective trust-region terms.
Generally, the MBRL problem cannot be solved analytically due to the (possibly highly non-linear,
non-differentiable, etc.) reward signal and the need for propagating the (uncertain) dynamics model
forwards in time. However, using the simplifying assumptions above, the reduced MBRL problem
equation 103 can in fact be solved analytically. We can circumvent the equality constraints by
predicting the state trajectory using the error corrected dynamics such that we obtain a lifted dynamics
formulation similar to the analysis for ILC,
s(i) = Fa(i) + d(i),	with d(i) = s(i) - Fa(i),	(104)
with s, a denoting the stacked states and actions of the recorded trajectory. Using this notation, the
optimization problem reduces to
π(i+1) = argmin1 ∣∣s - s(i)k2 + 1 ∣∣π(i) - ∏kC∏,	(105)
π2	2
with Cπ = diag[C0π, . . . , CHπ ]. By inserting equation 104 into equation 105 we obtain the closed-
form solution as
π(i+1) = (F>F + Cn) — 1 F> (S - d(i)) = π(i) + (F>F + Cn) — 1 F> (S - S⑴),(106)
which, clearly, is equivalent to equation 99 for M = I and W = Cπ .
E.3 Extensions
In the previous section, we analyzed the most basic setting for NO-ILC, i.e., linear dynamics, no
transition noise in the dynamics and no state/input constraints. Some of these assumptions can easily
be liftd to genearlized the presented framework.
Nonlinear Dynamics Instead of the system being defined by fixed matrices A, B, we can just
linearize a non-linear dynamics model f(S, a) around the last state/input trajectory such that
Ati)= df	, B" = f
∂S	(i)	(i)	∂a	(i)	(i)
st =st ,at=at	st =st ,at=at
(107)
and the corresponding dynamics matrix for the lifted representation becomes (dropping the superscript
notation indicating the iteration for clarity)
-	0	…	-
Bo	0
A0B0	B1	0	∈ RdS(T +1)×dAT
AoA1Bo	AoB1 B2	'..
(108)
State/Input Constraints Recall that we could solve the quadratic problems equation 98 and
equation 103 analytically because we assumed no explicit inequality constraints on the state
and inputs. In practice, however, both states and actions are limited by physical or safety con-
straints typically given by polytopes. While a closed-form solution is not readily available for
this case, one can easily employ any numerical solver to deal with such constraints. See, e.g.,
https://en.wikipedia.org/w/index.php?title=Quadratic_programming for
a comprehensive list of available solvers.
Stochastic Dynamics Schollig & D’Andrea (2009) generalize the ILC setting by considering two
separate sources of noise: 1) transition noise in the dynamics, i.e., St+1 = f(St, at) + ωtf as well
as 2) varying disturbances, i.e., d(i+1) = d(i) + ωd. Based on this model, a Kalman-Filter in the
iteration-domain is developed that estimates the respective random variables and the ILC scheme is
adapted to account for the estimated quantities.
43