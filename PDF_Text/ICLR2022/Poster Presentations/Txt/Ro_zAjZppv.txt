Published as a conference paper at ICLR 2022
Tracking the risk of a deployed model
AND DETECTING HARMFUL DISTRIBUTION SHIFTS
Aleksandr Podkopaev1,2, Aaditya Ramdas1,2
Department of Statistics & Data Science1
Machine Learning Department2
Carnegie Mellon University
{podkopaev,aramdas}@cmu.edu
Ab stract
When deployed in the real world, machine learning models inevitably encounter
changes in the data distribution, and certain—but not all—distribution shifts could
result in significant performance degradation. In practice, it may make sense to
ignore benign shifts, under which the performance of a deployed model does not
degrade substantially, making interventions by a human expert (or model retrain-
ing) unnecessary. While several works have developed tests for distribution shifts,
these typically either use non-sequential methods, or detect arbitrary shifts (benign
or harmful), or both. We argue that a sensible method for firing off a warning has
to both (a) detect harmful shifts while ignoring benign ones, and (b) allow contin-
uous monitoring of model performance without increasing the false alarm rate. In
this work, we design simple sequential tools for testing if the difference between
source (training) and target (test) distributions leads to a significant increase in
a risk function of interest, like accuracy or calibration. Recent advances in con-
structing time-uniform confidence sequences allow efficient aggregation of statis-
tical evidence accumulated during the tracking process. The designed framework
is applicable in settings where (some) true labels are revealed after the prediction
is performed, or when batches of labels become available in a delayed fashion.
We demonstrate the efficacy of the proposed framework through an extensive em-
pirical study on a collection of simulated and real datasets.
1 Introduction
Developing a machine learning system usually involves data splitting where one of the labeled folds
is used to assess its generalization properties. Under the assumption that the incoming test in-
stances (target) are sampled independently from the same underlying distribution as the training data
(source), estimators of various performance metrics, such as accuracy or calibration, are accurate.
However, a model deployed in the real world inevitably encounters variability in the input distri-
bution, a phenomenon referred to as dataset shift; see the book by Quionero-Candela et al. (2009).
Commonly studied settings include covariate shift (Shimodaira, 2000) and label shift (Saerens et al.,
2002). While testing whether a distribution shift is present has been studied both in offline (Rabanser
et al., 2019; Gretton et al., 2012; Hu & Lei, 2020) and online (Vovk et al., 2005; Vovk, 2020; 2021)
settings, a natural question is whether an intervention is required once there is evidence that a shift
has occurred.
A trustworthy machine learning system has to be supplemented with a set of tools designed to raise
alarms whenever critical changes to the environment take place. Vovk et al. (2021) propose retrain-
ing once an i.i.d. assumption becomes violated and design corresponding online testing protocols.
However, naively testing for the presence of distribution shift is not fully practical since it does not
take into account the malignancy of a shift (Rabanser et al., 2019). To elaborate, users are typically
interested in how a model performs according to certain prespecified metrics. In benign scenarios,
distribution shifts could be present but may not significantly affect model performance. Raising
unnecessary alarms might then lead to delays and a substantial increase in the cost of model de-
ployment. The recent approach by Vovk et al. (2021) based on conformal test martingales is highly
dependent on the choice of conformity score. In general, the methodology raises an alarm whenever
1
Published as a conference paper at ICLR 2022
a deviation from i.i.d. is detected, which does not necessarily imply that the deviation is harmful
(see Appendix A.2).
1.0
0.8
0.6
0.4
0.2
0.0
0.0	0.2	0.4	0.6	0.8	1.0
(a)
(b)
Figure 1: Samples from the source and the target (hatched) distributions under benign (a) covariate
and (b) label shifts. (a) X 〜Unif([0,1] X [0,1]) on the source and Xi 〜Beta(1, 2), i = 1,2 on
the target. Labels satisfy: P (Y = 1 | X = x) = P (χ2 + x2 + ε ≥ 1/4)where ε 〜N(0,0.01).
(b) Marginal probability of class 1 changes from π1S = 0.7 on the source to π1T = 0.3 on the target.
Covariates satisfy: X | Y = y 〜N(μy, I2), where μo = (-2,0)>, μι = (2,0)>. In both cases, a
model which separates well data from the source will generalize well to the target.
In some cases, it is possible to handle structured shifts in a post-hoc fashion without performing
expensive actions, such as model retraining. One example arises within the context of distribution-
free uncertainty quantification where the goal is to supplement predictions of a model with a measure
of uncertainty valid under minimal assumptions. Recent works (Tibshirani et al., 2019; Gupta et al.,
2020; Podkopaev & Ramdas, 2021) show how to adapt related procedures for handling covariate
and label shifts without labeled data from the target. However, both aforementioned shifts impose
restrictive assumptions on the possible changes in the underlying probability distribution, assuming
either that P(X) changes but P (Y|X) stays unchanged (covariate shift assumption), or that P(Y)
changes but P(X|Y) stays unchanged (label shift assumption).
Thinking of distribution shifts only in terms of covariate or label shifts has two drawbacks: such
(unverifiable) assumptions often may be unrealistic, and even if they were plausible, such shifts
may be benign and thus could be ignored. To elaborate on the first point, it is evident that while
distribution shifts constantly occur in practice, they may generally have a more complex nature. In
medical diagnosis, P(Y) and P(X|Y = y) could describe the prevalence of certain diseases in the
population and symptoms corresponding to disease y . One might reasonably expect not only the
former to change over time (say during flu season or epidemics) but also the latter (due to potential
mutations and partially-effective drugs/vaccines), thus violating both the covariate and label shift
assumptions. Regarding the second point, a model capable of separating classes sufficiently well
on the source distribution can sometimes generalize well to the target. We illustrate such benign
covariate and label shifts on Figures 1a and 1b respectively. We argue that the critical distinction—
from the point of view on raising alarms—should be built between harmful and benign shifts, and
not between covariate and label shifts.
A related question is whether labeled data from one distribution can be used for training a model
in a way that it generalizes well to another distribution where it is hard to obtain labeled examples.
Importance-weighted risk minimization can yield models with good generalization properties on the
target domain, but the corresponding statistical guarantees typically become vacuous if the impor-
tance weights are unbounded, which happens when the source distribution’s support fails to cover
the target support. Adversarial training schemes (Ganin et al., 2016; Wu et al., 2019) for deep learn-
ing models often yield models with reasonable performance on some types of distribution shifts, but
some unanticipated shifts could still degrade performance. This paper does not deal with how to
train a model if a particular type of shift is anticipated; it answers the question of when one should
consider retraining (or re-evaluating) a currently deployed model.
We argue for triggering a warning once the non-regularities in the data generating distribution lead to
a statistically significant increase in a user-specified risk metric. We design tools for nonparametric
sequential testing for an unfavorable change in a chosen risk function of any black-box model. The
2
Published as a conference paper at ICLR 2022
procedure can be deployed in settings where (some) true target labels can be obtained, immediately
after prediction or in a delayed fashion.
During the preparation of our paper, we noticed a very recent preprint (Kamulete, 2021) on broadly
the same topic. While they also advise against testing naively for the presence of a shift, their ap-
proach is different from ours as (a) it is based on measuring the malignancy of a shift based on
outlier scores (while we suggest measuring malignancy via a drop in test accuracy or another pre-
specified loss), and arguably more importantly (b) their procedure is non-sequential (it is designed
to be performed once, e.g, at the end of the year, and cannot be continuously monitored, but ours is
designed to flag alarms at any moment when a harmful shift is detected). Adapting fixed-time test-
ing to the sequential settings requires performing corrections for multiple testing: if the correction
is not performed, the procedure is no longer valid, but if naively performed, the procedure becomes
too conservative, due to the dependence among the tests being ignored (see Appendix A.1). We re-
duce the testing problem to performing sequential estimation that allows us to accumulate evidence
over time, without throwing away any data or the necessity of performing explicit corrections for
multiple testing; these are implicitly handled efficiently by the martingale methods that underpin the
sequential estimation procedures (Howard et al., 2021; Waudby-Smith & Ramdas, 2021).
In summary, the main contributions of this work are:
1.	We deviate from the literature on detecting covariate or label shifts and instead focus on
differentiating harmful and benign shifts. We pose the latter problem as a nonparametric
sequential hypothesis test, and we differentiate between malignant and benign shifts by
measuring changes in a user-specified risk metric.
2.	We utilize recent progress in sequential estimation to develop tests that provably control the
false alarm rate despite the multiple testing issues caused by continuously monitoring the
deployed model (Section 2.2), and without constraining the form of allowed distribution
shifts. For example, we do not require the target data to itself be i.i.d.; our methods are
provably valid even if the target distribution is itself shifting or drifting over time.
3.	We evaluate the framework on both simulated (Section 3.1) and real data (Section 3.2),
illustrating its promising empirical performance. In addition to traditional losses, we also
study several generalizations of the Brier score (Brier, 1950) to multiclass classification.
2 Sequential testing for a significant risk increase
Let X and Y denote the covariate and label spaces respectively. Consider predictors f : X → Y .
Let '(∙, ∙) be the loss function chosen to be monitored, with R(f) := E ['(f (X), Y)] denoting the
corresponding expected loss, called the risk of f.
We assume from here onwards that ` is bounded, which is the only restriction made. This is needed to
quantify how far the empirical target risk is from the true target risk without making assumptions on
P(X, Y). This is not a restriction of the current paper only, but appears broadly in statistical learning
theory (or the study of concentration inequalities): if one picks an unbounded loss function—say the
logarithmic loss—which can take on infinite values, then it is impossible to say how far the true and
empirical risks are without further assumptions, for example that the distribution of P (Y|X) is light
tailed. We prefer not to make such assumptions in this paper, keeping it as assumption-light as
possible and thus generally applicable in various domains. The restriction to bounded losses is not
heavy since examples abound in the machine learning literature; see some examples in Appendix B.
Remark 1. For classification, sometimes one does not predict a single label, but a distribution over
labels. In that case the range of f would be ∆lYl. This poses no issue, and it is common to use
bounded loss functions and risks such as the Brier score, as exemplified in Section 3. On a different
note, for regression, the loss (like squared error) is bounded only if the observations are. This is
reasonable in some contexts (predicting rain or snow) but possibly not in others (financial losses).
2.1	Casting the detection of risk increase as a sequential hypothesis test
We aim to trigger a warning whenever the risk on the target domain exceeds the risk on the source
by a non-negligible amount specified in advance. For example, alerting could happen once it is
possible to conclude with certain confidence that the accuracy has decreased by 10%. Shifts that
3
Published as a conference paper at ICLR 2022
lead to a decrease or an insignificant increase in the risk are then treated as benign. Formally, we
aim to construct a sequential test for the following pair of hypotheses:
H0 :	RT(f) ≤ RS(f) + εtol, vs.	H1 :	RT(f) > RS(f) + εtol,	(1)
where εtol	≥ 0 is an	acceptable tolerance level, and	RS (f)	and RT (f) stand for the risk	of f on
the source and target domains respectively. Assume that one observes a sequence of data points
Z1 , Z2 ,   At each time point t, a sequential test takes the first t elements of this sequence and
output either a 0 (continue) or 1 (reject the null and stop). The resulting sequence of 0s and 1s
satisfies the property that if the null H0 is true, then the probability that the test ever outputs a 1
and stops (false alarm) is at most δ. In our context, this means that if a distribution shift is benign,
then with high probability, the test will never output a 1 and stop, and thus runs forever. Formally,
a level-δ sequential test Φ defined as a mapping Sn∞=1 Zn → {0, 1} must satisfy: PH0 (∃t ≥ 1 :
Φ(Z1, ..., Zt) = 1) ≤ δ. Note that the sequential nature of a test is critical here as we aim to
develop a framework capable of continuously updating inference as data from the target is collected,
making it suitable for many practical scenarios. We distinguish between two important settings when
we assume that the target data either satisfies (a) an i.i.d. assumption (under the same or different
distribution as the source) or (b) only an independence assumption. While the i.i.d. assumption
may be arguably reasonable on the source, it is usually less realistic on the target, since in practice,
one may expect the distribution to drift slowly in a non-i.i.d. fashion instead of shifting sharply but
staying i.i.d.. Under setting (b), the quantity of interest on the target domain is the running risk:
1t
R(t)(f) = t∑E['(f(x0),Yi0)], t ≥ 1,
where the expected value is taken with respect to the joint distribution of (Xi0, Yi0), possibly different
for each test point i. The goal transforms into designing a test for the following pair of hypotheses:
H0 :	R(Tt)(f)	≤ RS(f)+εtol,	∀t≥ 1, vs.	H1	:	∃t?	≥ 1 :	R(Tt?)(f) >	RS(f)+εtol. (2)
When considering other notions of risk beyond the misclassification error, one could also be inter-
ested in relative changes in the risk, and thus a sequential test for the following pair of hypotheses:
H00 : RT (f) ≤ (1 + εtol)RS(f), vs. H10 : RT (f) > (1 + εtol)RS(f).	(3)
The proposed framework handles all of the aforementioned settings as we discuss next. The most
classical approach for sequential testing is the sequential probability ratio test (SPRT) due to Wald
(1945). However, it can only be applied, even for a point null and a point alternative, when the rel-
evant underlying distributions are known. While extensions of the SPRT exist to the composite null
and alternative (our setting above), these also require knowledge of the distributions of the test statis-
tics (e.g., empirical risk) under the null and alternative, and being able to maximize the likelihood.
Clearly, we make no distributional assumptions and so we require a nonparametric approach. We
perform sequential testing via the dual problem of nonparametric sequential estimation, a problem
for which there has been much recent progress to draw from.
2.2	Sequential testing via sequential estimation
When addressing a particular prediction problem, the true risk on neither the source nor the target
domains is known. Performance of a model on the source domain is usually assessed through a
labeled holdout source sample of a fixed size nS: {(Xi, Yi)}in=S1. We can write:
RS(f) + εtol = RbS(f) + RS(f) - RbS (f) + εtol,
where RbS(f) := (Pin=S1 `(f (Xi), Yi)) /nS. For any fixed tolerance level δS ∈ (0, 1), clas-
sic concentration results can be used to obtain an upper confidence bound εappr on the difference
RS(f) - RS(f), and thus to conclude that with probability at least 1 - δS:
.-. .-------------------. .------.
RS(f) + εtol ≤ UbS(f) +εtol, where UbS(f) = RbS(f) +εappr.	(4)
For example, by Hoeffding's inequality, ns = O(1∕ε2ppj points suffice for the above guarantee, but
that bound can be quite loose when the individual losses `(f (Xi), Yi) have low variance. In such set-
tings, recent variance-adaptive confidence bounds (Waudby-Smith & Ramdas, 2021; Howard et al.,
4
Published as a conference paper at ICLR 2022
2021) are tighter. It translates to an increase in the power of the framework, allowing for detecting
harmful shifts much earlier, while still controlling the false alarm rate at a prespecified level.
In contrast, the estimator of the target risk has to be updated as losses on test instances are observed.
While the classic concentration results require specifying in advance the size of a sample used for es-
timation, time-uniform confidence sequences retain validity under adaptive data collection settings.
For any chosen δT ∈ (0, 1), those yield a time-uniform lower confidence bound on RT (f):
P ∃t ≥ 1 : RT (f) < Lb(Tt) (f) ≤ δT ,
(t)
where LT (f) is the bound constructed after processing t test points. We typically set δS = δT =
δ∕2, where δ refers to the desired type I error. Under the independence assumption (equation 2), the
form of the drift in the distribution of the data is allowed to change with time. From the technical per-
spective, the difference with the i.i.d. setting is given by the applicability of particular concentration
results. While the betting-based approach of Waudby-Smith & Ramdas (2021) necessitates assum-
ing that random variables share a common mean, proceeding with the conjugate-mixture empirical-
Bernstein bounds (Howard et al., 2021) allows us to lift the common-mean assumption and handle
a time-varying mean. We summarize the testing protocol in Algorithm 1 and refer the reader to
Appendix E for a review of the concentration results used in this work. One can easily adapt the
framework to proceed with a fixed, absolute threshold on the risk, rather than a relative threshold
RS(f) + εtol, e.g., we can raise a warning once accuracy drops below 80%, rather than 5% below
the training accuracy.
Algorithm 1 Sequential testing for an absolute increase in the risk.
Input: Predictor f, loss `, tolerance level εtol, sample from the source {(Xi, Yi)}in=S1.
1:	procedure
2:	Compute the upper confidence bound on the source risk US(f);
3:	for t = 1, 2, . . . do
4:	Compute the lower confidence bound on the target risk Lb(Tt) (f);
5
6
ifLb(Tt)(f) > UbS(f)+εtolthen
Reject H0 (equation 1) and fire off a warning.
Testing for a relative increase in the risk is performed by replacing the line 5 in the Algorithm 1 by
(t)
the condition LT (f) > (1 + εtol)US(f). For both cases, the proposed test controls type I error as
formally stated next. The proof is presented in Appendix D.
Proposition 1. Fix any δ ∈ (0, 1). Let δS, δT ∈ (0, 1) be chosen in a way such that δS + δT = δ.
Let Lb(Tt) (f) define a time-uniform lower confidence bound on RT(t) (f) at level δT after processing t
data points (t ≥ 1), and let US(f) define an upper confidence bound on RS(f) at level δS. Then:
PH0	∃t	≥ 1	:	Lb(Tt)	(f) >	UbS(f) + εtol ≤ δ,
PH00	∃t≥ 1	:Lb(Tt)(f) >	(1+εtol)UbS(f) ≤δ,
(5)
that is, the procedure described in Algorithm 1 controls the type I error for testing the hypotheses
H0 (equation 1 and equation 2) and H00 (equation 3).
Remark 2. Both the testing protocol (Algorithm 1) and the corresponding guarantee (Proposition 1)
are stated in a form which requires the lower bound on the target risk to be recomputed after pro-
cessing each test point. More generally, test data could be processed in minibatches of size m ≥ 1.
Remark 3. Type I error guarantee in equation 5 holds under continuous monitoring. This goes
beyond standard fixed-time guarantees, for which type I error is controlled only when the sample
size is fixed in advance, and not under continuous monitoring. Define a stopping time ofa sequential
test in Algorithm 1:
N(δ) := inf t≥ 1 : Lb(Tt)(f) > UbS(f)+εtol .
Then the guarantee in equation 5 can be restated as: PH0 (N (δ) < ∞) ≤ δ, that is the probability
of ever raising a false alarm is at most δ.
5
Published as a conference paper at ICLR 2022
From sequential testing to changepoint detection. A valid sequential test can be transformed
into a changepoint detection procedure with certain guarantees (Lorden, 1971). The key character-
istics of changepoint detection procedures are average run length (ARL), or average time to a false
alarm, and average detection delay (ADD). One way to convert a sequential test into a detection
procedure is by running a separate test starting at each time point t = 1, 2, . . . , and claiming a
change whenever the first one of the tests rejects the null. Subsequently, these tests yield a sequence
of stopping variables N1(δ), N2(δ), . . . The corresponding stopping time is defined as:
N?3)：= JInf (Nk(δ) + (k — 1)).
k=1,2,...
Lorden (1971) established a lower bound on the (worst-case) ARL of such changepoint detection
procedure of the form: E* [N?(δ)] ≥ 1∕δ. The (worst-case) average detection delay is defined as:
EιN(δ) = SUp ess SUP Em [(N(δ) - (m - 1))+ | (Xj,%'),..., (Xm-I,工—好,
m≥1
where Em denotes expectation under Pm, the distribution of a sequence (X10 , Y10 ), (X10 , Y10 ), . . .
under which (Xm0 , Ym0 ) is the first term from a shifted distribution.
3	Experiments
In Section 3.1, we analyze the performance of the testing procedure on a collection of simulated
datasets. First, we consider settings where the i.i.d. assumption on the target holds, and then relax
it to the independence assumption. In Section 3.2, we evaluate the framework on real data. We
consider classification problems with different metrics of interest including misclassification loss,
several versions of the Brier score and miscoverage loss for set-valued predictors. Due to space
limitations, we refer the reader to Appendix B for a detailed review of the considered loss functions.
3.1	Simulated data
Tracking the risk under the i.i.d. assumption. Here we induce label shift on the target domain
and emulate a setting where it noticeably harms the accuracy of a predictor by modifying the setup
from Section 1 through updating the class centers to μo = (-1,0)> and μι = (1,0)>, making the
classes largely overlap. The (oracle) Bayes-optimal predictor on the source domain is:
f?(x) =πS ∙ Ex μι,I2)
∏S ∙ θ(x; μ0,I2) + ∏S ∙ θ(x; μ1,I2)，
(6)
where 夕(x; μi,I2) denotes the probability density function of a Gaussian random vector with mean
μi, i ∈ {0,1} and an identity covariate matrix. Let' be the 0-1 loss, and thus the misclassification
risk RT(f *) on the target is:
PT (f?(X)	= Y) = P	卜>(μι - μo)	< log (11)	+ 2	(kμι∣∣2 -∣∣μo∣∣2) | Y =" ∙ ∏T
+ P	(X>(μ1 - μ0)	≥ log (∏S)	+ 2	(kμ1k2 -kμ0k2) | Y = 0) ∙ πT.
For three values ofπ1S, the source marginal probability of class 1, we illustrate how label shift affects
the misclassification risk of the Bayes-optimal predictor on Figure 2a, noting that it is linear in π1T .
Importantly, whether label shift hurts or helps depends on the value of π1S .
We fix π1S = 0.25 and use the corresponding Bayes-optimal rule. On Figure 2b, we compare
upper confidence bounds on the source risk due to different concentration results, against the size of
the source holdout set. Variance-adaptive upper confidence bounds—predictably-mixed empirical-
Bernstein (PM-EB) and betting-based (see Appendix E)—are much tighter than the non-adaptive
Hoeffding’s bound. Going forward, we use a source holdout set of 1000 points to compute upper
confidence bound on the source risk, where εappr from equation 4 is around 0.025.
Next, we fix εtol = 0.05, i.e., we treat a 5% drop in accuracy as significant. For 20 values of
π1T, evenly spaced in the interval [0.1, 0.9], we sample 40 batches of 50 data points from the target
6
Published as a conference paper at ICLR 2022
(a)	(b)
Figure 2:	(a) The misclassification risk on the target of the Bayes-optimal predictors for three values
ofπ1S. Notice that label shift does not necessarily lead to an increase in the risk. (b) Upper confidence
bounds US(f) on the misclassification risk on the source obtained via several possible concentration
results. For each sample size, the results are aggregated over 1000 random data draws. The variance-
adaptive confidence bounds (predictably-mixed empirical-Bernstein and the betting-based one) are
much tighter when compared against the non-adaptive one.
distribution. On Figure 3a, we track the proportion of null rejections after repeating the process
250 times. Note that here stronger label shift hurts accuracy more. On Figure 3b, we illustrate
average size of a sample from the target needed to reject the null. The results confirm that tighter
bounds yield better detection procedures, with the most powerful test utilizing the betting-based
bounds (Waudby-Smith & Ramdas, 2021). A similar analysis for the Brier score (Brier, 1950) as a
target metric is presented in Appendix F. We further study the performance of the framework under
the covariate shift setting (Appendix H).
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	0.5	0.6	0.7	0.8	0.9
Marginal probability of class 1 on the target, ∏T	Marginal probability of class 1 on the target, ∏T
(a)	(b)
Figure 3:	(a) Proportion of null rejections when testing for an increase in the misclassification risk
after processing 2000 samples from a shifted distribution. The vertical dashed yellow line separates
null (benign) and alternative (harmful) settings. Testing procedures that rely on variance-adaptive
confidence bounds (CBs) have more power. (b) Average sample size from the target that was needed
to reject the null. Tighter concentration results allow to raise an alarm after processing less samples.
Tracking beyond the i.i.d. setting (distribution drift). Here we consider testing for an increase
in the running risk (equation 2). First, we fix π1T = 0.75 and keep the data generation pipeline as
before, that is, the target data are still sampled in an i.i.d. fashion. We compare lower confidence
bounds on the target risk studied before against the conjugate-mixture empirical-Bernstein (CM-EB)
bound (Howard et al., 2021). We note that this bound on the running mean of the random variables
does not have a closed-form and has to be computed numerically. On Figure 4a, we illustrate that the
lower confidence bounds based on betting are generally tighter only for a small number of samples.
Similar results hold for the Brier score as a target metric (see Appendix F.1).
Next, we lift the i.i.d. assumption by modifying the data generation pipeline: starting with π1T =
0.25, we increase π1T by 0.1 after sampling each 200 instances, until it reaches the value 0.85. It
makes CM-EB the only valid lower bound on the running risk on the target domain. The results of
running the framework for this setting are presented on Figure 4b.
7
Published as a conference paper at ICLR 2022
Number of samples from the target domain
(a)	(b)
Figure 4:	(a) Different lower confidence bounds (LCB) on the target risk under the i.i.d. assumption.
Betting-based LCB is only tighter than conjugate-mixture empirical-Bernstein (CM-EB) for a small
number of samples. (b) Under distribution drift, only CM-EB performs estimation of the running
risk. The resulting test consistently detects a harmful increase in the running risk.
3.2 Real data
In deep learning, out-of-distribution robustness is often assessed based on a model performance gap
between the original data (used for training) and data to which various perturbations are applied.
We focus on two image classification datasets with induced corruptions: MNIST-C (Mu & Gilmer,
2019) and CIFAR-10-C (Krizhevsky, 2009; Hendrycks & Dietterich, 2019). We illustrate an exam-
ple of a clean MNIST image on Figure 5a along with its corrupted versions after applying motion
blur, blur along a random direction (Figure 5b), translate, affine transformation along a random
direction (Figure 5c), and zigzag, randomly oriented zigzag over an image (Figure 5d). For CIFAR-
10-C, we consider corrupting original images by applying the fog effect with 3 levels of severity as
illustrated on the bottom row of Figure 5. For both cases, clean or corrupted test samples are passed
as input to networks trained on clean data. While corruptions are visible to the human eye, one might
still hope that they will not significantly hurt classification performance. We use the betting-based
confidence bounds on the source and conjugate-mixture empirical-Bernstein confidence bounds on
the target domain.
(a) Clean	(b) Motion blur	(c) Translate	(d) Zigzag
(e) Clean	(f) Fog, severity: 1	(g) Fog, severity: 3	(h) Fog, severity: 5
Figure 5: Examples of MNIST-C ((a)-(d)) and CIFAR-IO-C ((e)-(h)) images.
1
Tracking the risk of a point predictor on MNIST-C dataset. We train a shallow CNN on clean
MNIST data and run the framework testing whether the misclassification risk increases by 10%,
feeding the network with data in batches of 50 points either from the original or shifted distributions.
Details regarding the network architecture and the training process are given in Appendix G. On
Figure 6a, we illustrate the results after running the procedure 50 times for each of the settings.
8
Published as a conference paper at ICLR 2022
The horizontal dashed line defines the rejection threshold that has been computed using the source
data: once the lower bound on the target risk (solid lines) exceeds this value, the null hypothesis
is rejected. When passing clean MNIST data as input, we do not observe degrading performance.
Further, applying different corruptions leads to both benign and harmful shifts. While to the human
eye the translate effect is arguably the least harmful one, it is the most harmful to the performance
of a network. Such observation is also consistent with findings ofMu & Gilmer (2019) who observe
that if trained on clean MNIST data without any data augmentation, CNNs tend to fail under this
corruption. We validate this observation by retraining the network several times in Appendix G.
Tracking the risk of a set-valued predictor on CIFAR-10-C dataset. For high-consequence
settings, building accurate models only can be insufficient as it is crucial to quantify uncertainty in
predictions. One way to proceed is to output a set of candidate labels for each point as a prediction.
The goal could be to cover the correct label of a test point with high probability (Vovk et al., 2005)
or control other notions of risk (Bates et al., 2021). We follow Bates et al. (2021) who design a
procedure that uses a holdout set for tuning the parameters of a wrapper built on top of the original
model which, under the i.i.d. assumption, is guaranteed to have low risk with high probability (see
Appendix G for details). Below, we build a wrapper around a ResNet-32 model that controls the
miscoverage risk (equation 14) at level 0.1 with probability at least 0.95. For each run, CIFAR-
10 test set is split at random into three folds used for: (a) learning a wrapper (1000 points), (b)
estimating upper confidence bound on the miscoverage risk on the source (1000 points), and (c)
evaluation purposes on either clean or corrupted images. We take εtol = 0.05, that is, 5% drop in
coverage is treated as significant. Figure 6b illustrates that only the most intense level of fog is
consistently harmful to coverage. We also consider setting a lower prescribed miscoverage level
(0.05) for the set-valued predictor (see Appendix G). When larger prediction sets are produced,
adding fog to images becomes less harmful.
(a)	(b)
Figure 6: (a) Performance of the testing framework on MNIST-C dataset. Only the translation
effect is consistently harmful to the classification performance of a CNN trained on clean data. (b)
Performance of the testing framework on CIFAR-10-C dataset. Only the most severe version of
the fog lead to a significant degradation in performance measured by a decrease in coverage of a
set-valued predictor trained on top of a model trained on clean data.
4 Conclusion
An important component of building reliable machine learning systems is making them alarm a user
when potentially unsafe behavior is observed, instead of allowing them to fail silently. Ideally, a
warning should be displayed when critical changes affecting model performance are present, e.g.,
a significant degradation of the target performance metrics, like accuracy or calibration. In this
work, we considered one particular failure scenario of deployed models—presence of distribution
shifts. Relying solely on point estimators of the performance metrics ignores uncertainty in the
evaluation, and thus fails to represent a theoretically grounded approach. We developed a set of
tools for deciding whether the performance of a model on the test data becomes significantly worse
than the performance on the training data in a data-adaptive way. The proposed framework based
on performing sequential estimation requires observing true labels for test data (possibly, in a de-
layed fashion). Across various types of distribution shifts considered in this work, it demonstrated
promising empirical performance for differentiating between harmful and benign ones.
9
Published as a conference paper at ICLR 2022
Acknowledgements The authors thank Ian Waudby-Smith for fruitful discussions and the anony-
mous ICLR 2022 reviewers for comments on an early version of this paper. The authors acknowl-
edge support from NSF DMS 1916320 and an ARL IoBT CRA grant. Research reported in this
paper was sponsored in part by the DEVCOM Army Research Laboratory under Cooperative Agree-
ment W911NF-17-2-0196 (ARL IoBT CRA). The views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the official policies, either
expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for Government purposes notwithstanding
any copyright notation herein.
Ethics Statement. We do not foresee any negative impacts of this work in and of itself; there may
be societal impacts of the underlying classification or regression task but these are extraneous to the
aims and contributions of this paper.
Reproducibility statement. In order to ensure reproducibility of the results in this paper, we in-
clude the following to the supplementary materials: (a) relevant source code for all simulations that
have been performed, (b) clear explanations of all data generation pipelines and preprocessing steps
that have been performed, (c) complete proofs for all established theoretical results.
References
Stephen Bates, Anastasios Nikolas Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I. Jordan.
Distribution-free, risk-controlling prediction sets. Journal of the ACM, 2021.
Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,
1950.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks.
Journal of Machine Learning Research, 2016.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 2012.
Chirag Gupta and Aaditya Ramdas. Top-label calibration and multiclass-to-binary reductions. In
International Conference on Learning Representations, 2022.
Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. Distribution-free binary classification:
prediction sets, confidence intervals and calibration. In Advances in Neural Information Process-
ing Systems, 2020.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2019.
Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform, nonpara-
metric, nonasymptotic confidence sequences. The Annals of Statistics, 2021.
Xiaoyu Hu and Jing Lei. A distribution-free test of covariate shift using conformal prediction. arXiv
preprint: 2010.07147, 2020.
Vathy M. Kamulete. Test for non-negligible adverse shifts. arXiv preprint: 2107.02990, 2021.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
G. Lorden. Procedures for reacting to a change in distribution. The Annals of Mathematical Statis-
tics, 1971.
Norman Mu and Justin Gilmer. MNIST-C: A robustness benchmark for computer vision. arXiv
preprint:1906.02337, 2019.
10
Published as a conference paper at ICLR 2022
Aleksandr Podkopaev and Aaditya Ramdas. Distribution free uncertainty quantification for classifi-
cation under label shift. Conference on Uncertainty in Artificial Intelligence, 2021.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence.
Dataset Shift in Machine Learning. The MIT Press, 2009.
StePhan Rabanser, StePhan Gunnemann, and Zachary Lipton. Failing loudly: An empirical study
of methods for detecting dataset shift. In Advances in Neural Information Processing Systems,
2019.
Yaniv Romano, Matteo Sesia, and Emmanuel J. Candes. Classification with valid and adaptive
coverage. In Advances in Neural Information Processing Systems, 2020.
Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to
new a priori probabilities: A simple procedure. Neural Computation, 2002.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning and Inference, 2000.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal pre-
diction under covariate shift. In Advances in Neural Information Processing Systems, 2019.
Vladimir Vovk. Testing for concept shift online. arXiv preprint: 2012.14246, 2020.
Vladimir Vovk. Testing randomness online. Statistical Science, 2021.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer, 2005.
Vladimir Vovk, Ivan Petej, Ilia Nouretdinov, Ernst Ahlberg, Lars Carlsson, and Alex Gammerman.
Retrain or not retrain: conformal test martingales for change-point detection. In Symposium on
Conformal and Probabilistic Prediction and Applications, 2021.
Abraham Wald. Sequential tests of statistical hypotheses. The Annals of Mathematical Statistics,
1945.
Ian Waudby-Smith and Aaditya Ramdas. Estimating means of bounded random variables by betting.
arXiv preprint: 2010:09686, 2021.
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International Conference on Machine Learn-
ing, 2019.
11
Published as a conference paper at ICLR 2022
A Issues with existing tests for distribution shifts/drifts
A. 1 Non-sequential tests have highly inflated false alarm rates when
DEPLOYED IN SEQUENTIAL SETTINGS WITH CONTINUOUS MONITORING
In this work, we propose a framework that utilizes confidence sequences (CSs), and thus allows
for continuous monitoring of model performance. On the other hand, traditional (fixed-time) test-
ing procedures are not valid under sequential settings, unless corrections for multiple testing are
performed. First, we illustrate that deploying fixed-time detection procedures under sequential set-
tings necessarily leads to raising false alarms. Then, we illustrate that naive corrections for multiple
testing—without taking advantage of the dependence between the tests—lead to losses of power of
the resulting procedure.
Deploying fixed-time tests without corrections for multiple testing. Under the i.i.d. setting, our
framework reduces to testing whether the means corresponding to two unknown distributions are
significantly different. Here we consider a simplified setting: assume that one observes a sequence
Z1, Z2, . . . of bounded i.i.d. random variables, and the goal is to construct a lower confidence bound
for the corresponding mean μ. In this case, a natural alternative to confidence sequences is a lower
bound obtained by invoking the Central Limit Theorem:
万	bt
Zt - zδ ∙√t,
where zδ is (1 - δ)-quantile of the standard Gaussian random variable and Zt, bt denote the sample
average and sample standard deviation respectively computed using first t instances Z1, . . . , Zt. For
the study below, We use the critical (significance) level δ = 0.1. We sample the data as: Zt 〜
Ber(0.6), t = 1, 2, . . . , and consider 100 possible sample sizes, evenly spaced between 20 and 1000
on a logarithmic scale. Next, we compare the CLT lower bound with the betting-based one (which
underlies the framework proposed in this work) under the following settings:
1.	Fixed-time monitoring. For a given sample size t, we sample the sequence Z1, . . . , Zt,
compute the lower bounds and check whether the true mean is covered only once. For each
value of the sample size, we resample data 100 times and record the miscoverage rate, that
is, the fraction of times the true mean is miscovered.
2.	Continuous monitoring. Here, the lower bound is recomputed once new data become
available. We resample the whole data sequence Z1, . . . , Z1000 1000 times, and for each
value of the sample size, we track the cumulative miscoverage rate, that is, the fraction of
times the true mean has been miscovered at some time up to t.
Under fixed-time monitoring (Figure 7a), the false alarm rate is controlled at prespecified level δ
by both procedures. However, under continuous monitoring (Figure 7b), deploying the CLT lower
bound leads to raising false alarms. At the same time, the betting-based lower bound controls the
false alarm rate under both types of monitoring.
Deploying fixed-time tests with corrections for multiple testing. Next, we illustrate that adapt-
ing fixed-time tests to sequential settings via performing corrections for multiple testing comes at
the price of significant power losses. Performing the Bonferroni correction requires splitting the
available error budget δ among the times when testing is performed. In particular, we consider:
Power correction:
∞
X 2i=δ,
Polynomial correction:
6 X三
∏2	i2
i=1
δ.
(7)
Note that the second option is preferable as the terms in the sum decrease at a slower rate, thus al-
lowing fora narrower sequence of intervals. Proceeding under the setup considered in the beginning
of this section (data points are sampled from Ber(0.6)), we consider two scenarios:
12
Published as a conference paper at ICLR 2022
4 3 2 1
. . . .
Uooo
as」E」e-e ə-eL
102
Sample SiZe
(a)
Figure 7: False alarm rate for the CLT and betting-based lower confidence bound (LCB) under:
(a) fixed-time monitoring and (b) continuous monitoring. Note that both bounds control the false
alarm rate at a prespecified level δ = 0.1 under fixed-time monitoring. However under continuous
monitoring, the false alarm rate of the CLT bound quickly exceeds the critical level δ = 0.1. At the
same time, the betting LCB successfully controls the false alarm rate.
102
Sample SiZe
(b)
103
1.	We recompute the CLT lower bound each time a batch of 25 samples is received and per-
form the Bonferroni correction (utilizing both ways of splitting the error budget described
in equation 7). We present the lower bounds on Figure 8a (the results have been aggregated
over 100 data draws). Observe that:
•	While the sequence of intervals shrinks in size with growing number of samples under
the polynomial correction, this is not the case under the power correction.
•	Not only the betting confidence sequence is uniformly tighter than the CLT-based over
all considered sample sizes, it also allows for monitoring at arbitrary stopping times.
Note that the CLT lower bound allows for monitoring only at certain times (marked
with stars on Figure 8a).
2.	For a fairer comparison with the betting-based bound, we also consider recomputing the
CLT bound each time a new sample is received. Since utilizing the power correction
quickly leads to numerical overflows, we utilize only the polynomial correction. We present
the lower bounds on Figure 8b (the results have been aggregated over 100 data draws).
While now the CLT lower bound can be monitored at arbitrary stopping times, it is sub-
stantially lower (thus more conservative) than the betting-based.
3 2 1
. . .
Ooo
3n">ω0-l
0	200	400	600	800	1000	0	200	400	600	800	1000
Sample size	Sample size
(a)	(b)
Figure 8: Adapting the CLT lower bound to continuous monitoring via performing corrections for
multiple testing: (a) each time a batch of 25 samples is received, (b) each time a new sample is
received. Under both settings, the CLT-based lower bound is more conservative than the betting-
based, which, in testing terminology, means that the resulting testing framework has less power.
A.2 Conformal test martingales do not in general differentiate between
HARMFUL AND BENIGN DISTRIBUTION SHIFTS
Testing the i.i.d. assumption online can be performed using conformal test martingales (Vovk et al.,
2021; Vovk, 2021; 2020). Below, we review building blocks underlying a conformal test martingale.
13
Published as a conference paper at ICLR 2022
1. First, one has to pick a conformity score. Assigning a lower score to a sample indicates
abnormal behavior. Vovk et al. (2021) consider the regression setting and, like us, use
scores that depends on true labels: for a point (xi, yi), let ybi denote the output of a predictor
on input xi . The authors propose a score of the form:
αi = - |yi - ybi| .	(8)
Note that lower scores defined in equation 8 clearly reflect degrading performance of a pre-
dictor (possibly due to the presence of distribution drift). Under the classification setting,
we propose to consider the following conformity score:
KK
αi = Xfk(xi) ∙ l{fk(Xi) ≤ fy<Xi)} = 1 - Xfk(Xi) ∙ l{fk(xi) > fyi(xi)} , (9)
i=1	i=1
which is a rescaled estimated probability mass of all the labels that are more likely than
the true one (here, We assume that predictor f outputs an element of ∆lYl). Rescaling
in equation 9 is used to ensure that this score represent the conformity score, that is, the
higher the value is, the better a given data point conforms. Note that if for a given data
point, the true label happens to be top-ranked by a predictor f, then such point receives the
largest conformity score equal to one. This score is inspired by recent Works in conformal
classification (Romano et al., 2020; Podkopaev & Ramdas, 2021).
2.
3.
After processing n data points, a transducer transforms a collection of conformity scores
into a conformal p-value:
Pn = p ({(Xi, yi)}in=1 , u) :
|i ∈ {1,...,n} : αi < αn∣ + u ∙ |i ∈ {1,...,n} : αi = αn∖
n
where U 〜 Unif([0,1]). Conformal p-values are i.i.d. uniform on [0,1] when the data
points are i.i.d. (or more generally, exchangeable; see (Vovk et al., 2021)). Note that the
design of conformal p-value Pn ensures it takes small value when the conformity score αn
is small, that is, when abnormal behavior is being observed in a sequence.
A betting martingale is used to gamble again the null hypothesis that a sequence of random
variables is distributed uniformly and independently on [0, 1]. Formally, a betting martin-
gale is a measurable function F : [0, 1]? → [0, ∞] such that F() = 1 ( defines an
empty sequence and Z? stands for the set of all finite sequences of elements of Z) and for
each sequence (u1, . . . , un-1) ∈ [0, 1]n-1 and any n ≥ 1:
F(u1, . . . , un-1, u)du = F(u1, . .
0
. , un-1 ).
The simplest example is given by the product of simple bets:
F(u1, . . . , un) = εn
ε > 0,
(10)
but more sophisticated options are available (Vovk et al., 2005). For the simulations that
follow, we use simple mixture martingale which is obtained by integrating equation 10 over
ε ∈ [0, 1].
Conformal test martingale Sn is obtained by plugging in the sequence of conformal p-
values P1 , . . . , Pn into the betting martingale. The test starts with S0 = 1 and it rejects
at the first time n when Sn exceeds 1∕α. They type I error control for this test is justified
by Ville’s inequality which states that for any nonnegative martingale (which Sn is, un-
der the i.i.d. null), the entire process Sn stays below 1∕a with probability at least 1 一 ɑ.
Mathematically:
P (∃n : Sn ≥ 1∕α) ≤ α.
To study conformal test martingales, we consider the label shift setting described in Section 3.1.
Recall that for this setting we know exactly when a shift in label proportions becomes harmful to
misclassification risk of the Bayes-optimal rule on the source distribution (see Figures 2a and 3a).
For the simulations that follow, we assume that the marginal probability of class 1 on the source is
π1S = 0.25, and use the corresponding optimal rule.
We analyze conformal test martingales under several distribution drift scenarios differing in their
severity and rate, and start with the settings where a sharp shift is present.
14
Published as a conference paper at ICLR 2022
1.	Harmful distribution shift with cold start. Here, the data are sampled i.i.d. from a shifted
distribution corresponding to π1T = 0.75. We illustrate 50 runs of the procedure on Fig-
ure 9a. Recall that when the data are sampled i.i.d. the conformal p-values are i.i.d. uniform
on [0, 1]. Under the (exchangeability) null, conformal test martingales are not growing, and
thus are not able to detect that a present shift, even though it corresponds to a harmful set-
ting.
2.	Harmful distribution shift with warm start. For a fairer comparison, we also consider a
warm start setting when the first 100 points are sampled i.i.d. from the source distribution
(π1T = 0.25), followed by the data sampled i.i.d. from a shifted distribution (π1T = 0.75).
We illustrate 50 runs of the procedure on Figure 9b. In this case, conformal test martin-
gales demonstrate better detection properties. However, a large fraction of conformal test
martingales still is incapable of detecting a shift.
3-----------------------------------------------------------------------------------------------------------------
0	200	400	600	800	1000	1200
Time / number of processed samples
0	200	400	600	800	1000	1200
Time / number of processed samples
(a)	(b)
Figure 9: 50 runs of conformal test martingales (blue dotted lines) under harmful distribution shift
with: (a) cold start (shift happens in the beginning), (b) warm start (shift happens in an early stage
of a model deployment). The horizontal red dashed line outlines to the rejection threshold due to
Ville’s inequality. Even though warm start improves detection properties, only a small fraction of
conformal test martingales detects a shift that leads to more than 10% drop in classification accuracy.
The simulations above illustrate that conformal test martingales are inferior to the framework pro-
posed in this work whenever a sharp distribution shift happens in the early stage of a model deploy-
ment, even when such shift is harmful. Next, we consider several settings where instead of changing
sharply, the distribution drifts gradually.
3.	Slow and benign distribution drift. Starting with the marginal probability of class 1,
π1T = 0.1, we keep increasing π1T by 0.05 each time a batch of 75 data points is sampled
until it reaches the value 0.45. Recall from Section 3.1 that those values of π1T correspond
to a benign setting where the risk of the predictor on the target domain does not exceed
substantially the source risk. We illustrate 50 runs of the procedure on Figure 10a.
4.	Slow and harmful distribution drift. Starting with the marginal probability of class 1
π1T = 0.5, we keep increasing π1T by 0.05 each time a batch of 75 data points is sampled
until it reaches the value 0.85. Recall from Section 3.1 that those values of π1T correspond
to a harmful setting where the risk of the predictor on the target domain is substantially
larger than the source risk. We illustrate 50 runs of the procedure on Figure 10b.
5.	Sharp and harmful distribution drift. Starting with the marginal probability of class 1,
π1T = 0.1, we keep increasing π1T by 0.2 each time a batch of 150 data points is sampled
until it reaches the value 0.9. We illustrate 50 runs of the procedure on Figure 10c.
The settings where the distribution drifts gradually illustrate several shortcomings of conformal test
martingales:
•	Conformal test martingales consistently detect only sharp distribution drifts. Recall from
Section 3.1 that increasing π1T from 0.1 to 0.9 results in more than 20% accuracy drop.
When a drift is slow (Figures 10a and 10b), conformal test martingales demonstrate much
less power.
•	Inspired by the ideas of Vovk et al. (2021) who assumed, like us, that (some) true data
labels are observed, we designed a conformity score that reflects decrease in performance.
15
Published as a conference paper at ICLR 2022
-4 -I-------------1------------1----------1-----------1-----------1-----------1—
O	200	400	600	800	1000	1200
Time / number of processed samples
0	200	400	600	800	1000	1200
Time / number of processed samples
(c)
Figure 10: 50 runs of conformal test martingales (blue dotted lines) under gradual distribution drifts:
(a) slow and benign, (b) slow and harmful, (c) sharp and harmful. The horizontal red dashed line
outlines to the rejection threshold due to Ville’s inequality. Note that conformal test martingales
consistently detect only sharp distribution drifts. Moreover, conformal test martingales illustrate
similar behavior under (a) and (b) but the corresponding settings are drastically different.
On Figures 10a and 10b, conformal test martingales illustrate similar behavior but the cor-
responding settings are drastically different. Only one corresponds to a benign drift when
the risk does not become significantly worse than the source risk. Thus, even though it is
possible to make conformal test martingale reflect degrading performance, it is hard to in-
corporate evaluation of the malignancy of a change in an interpretable way, like a decrease
of an important metric.
•	Another problem of using conformal test martingales to be aware of is that after some
time the corresponding values of the test martingale (larger implies more evidence for a
shift) could start to decrease as the shifted distribution becomes the ‘new normal’ (Vovk
et al., 2021). This is because they measure deviations from iid data, not degradations in
performance from some benchmark (like source accuracy).
B	Loss functions
For our simulations, we consider the following bounded losses. Below, let yb(x; f) :=
arg maxk∈Y fk(x) denote the label prediction of a model f on a given input x ∈ X.
Multiclass losses. The most common example is arguably the misclassification loss and its gener-
alization that allows for a label-dependent cost of a mistake:
'mis(f (χ),y)	：= 1 {b(χ;	f)	= y}∈	{0,1},	'w-mis(f (χ),y)	：= 'y	∙1 {y(χ; f) =	y}	∈ [0, L],
where {'k}k∈γ is a collection of per-class costs and L = maxk∈γ 'k. The loss 'w-mis is more
relevant to high-stakes decision settings and imbalanced classification. However, high accuracy
alone can often be insufficient. The Brier score (squared error), introduced initially for the binary
setting (Brier, 1950), is commonly employed to encourage calibration of probabilistic classifiers.
For multiclass problems, one could consider the mean-squared error of the whole output vector:
'brier(f(χ),y) ：= 2kf(χ)-h(y)k2 ∈ [0,1],	(11)
16
Published as a conference paper at ICLR 2022
where h : Y → {0,1}|Y| is a one-hot label encoder: hy (y) = 1 {y0 = y} for y, y0 ∈ Y. Top-label
calibration (Gupta & Ramdas, 2022) restricts attention to the entry corresponding to the top-ranked
label. A closely related loss function, which we call the top-label Brier score, is the following:
'brier-top(f(x),y) := (fb(χf)(χ) -1 {b(χ; f) = y})2 = (fy(x；f)(χ) - hyy,f)(y))2 ∈ [0, D(12)
Alternatively, instead of the top-ranked label, one could focus only on the entry corresponding to
the true class. It gives rise to another loss function which we call the true-class Brier score:
`brier-true(f (x), y) := (fy(x) - 1)2 ∈ [0, 1].	(13)
The loss functions `brier, `brier-top, `brier-true trivially reduce to the same loss function in the binary
setting. In Appendix C, we present a more detailed study of the Brier score in the multiclass setting
with several illustrative examples.
Set-valued predictors. The proposed framework can be used for set-valued predictors that output
a subset ofY as a prediction. Such predictors naturally arise in multilabel classification, where more
than a single label can be the correct one, or as a result of post-processing point predictors. Post-
processing could target covering the correct label of a test point with high probability (Vovk et al.,
2005) or controlling other notions of risk (Bates et al., 2021) like the miscoverage loss:
'miscov(y,S(x)) = l{y ∈S(x)} ,	(14)
where S(x) denotes the output of a set-valued predictor on any given input x ∈ X. When consid-
ering multilabel classification, relevant loss functions include the symmetric difference between the
output and the set of true labels, false negative rate and false discovery rate.
C B rier score in the multiclas s setting
This section contains derivations of decompositions stated in Section 2 and comparisons between
introduced versions of the Brier score.
Brier score decompositions. Define a function C : X → ∆lYl, with entries Ck(X):=
P (Y = k | f(X)). In words, coordinates of c(X) represent the true conditional probabilities of
belonging to the corresponding classes given an output vector f(X). Recall that h : Y → {0, 1}|Y|
is a one-hot label encoder. The expected Brier score for the case when the whole output vector is
considered (that is, the expected value of the loss defined in equation 11) satisfies the following
decomposition:
2 ∙ Rbrier(f) = Ekf(X) - h(Y)k2
=Ekf(X)-C(X)+C(X)-h(Y)k2
(=a) Ekf(X) - C(X)k2 + EkC(X) - h(Y)k2
= Ekf(X) - C(X)k2 + EkC(X) - E [C(X)] + E [C(X)] - h(Y)k2
(=b)Ekf(X)-C(X)k2-EkC(X)-E[C(X)]k2+Ekh(Y)-E[h(Y)]k2.
|--------V--------} |------------V-----------} |-----------V----------'
calibration error	sharpness	intrinsic uncertainty
Above, (a) follows by conditioning on f(X) for the cross-term and recalling that E [h(Y) | f(X)] =
C(X), (b) also follows by conditioning on f(X) and noticing that E [h(Y)] = E [C(X)]. Now, recall
that a predictor is (canonically) calibrated iff(X) a=.s. C(X), in which case the calibration error term
is simply zero.
Next, we consider the top-label Brier score `brier-top. Define Ctop : X → [0, 1], as:
Ctop(X) := P (Y = b(X; f) I fy(x；f)(X),b(x;f)),
or the fraction of correctly classified points among those that are predicted to belong to the same
class and share the same confidence score as X. Following essentially the same argument as for the
17
Published as a conference paper at ICLR 2022
standard Brier score, we get that:
E [`brier-top(f (χ),γ)]
=E (fb(x；f)(X) - hb(x；f)(Y))2
=E (fb(x；f)(X) - ctop(X) + Ctop(X) - hb(x；f)(Y))2
=E (fb(x；f)(X) - ctop(X))2 + E (ctop(X) - hy(x；f)(Y))2
=E fb(Xf )(X) -ctop(X))2 -E (ctop(X) - E [ctop(X)])2 +Var(hb(Xf )(Y)) ∙
|--------------------V--------------------}	|--------------------V----------
top-label calibration error	top-label sharpness
, |---------------V-----------}
variance of the
misclassification loss
Note that in contrast to the classic Brier score decomposition, the last term in this decomposition
depends only on the top-class prediction of the underlying predictor, and thus on its accuracy.
Comparison of the scores in multiclass setting. Recall that the difference between three versions
of the Brier score arises when one moves beyond the binary classification setting. We illustrate the
difference by considering a 4-class classification problem where the data represent a balanced (that
is all classes are equally likely) mixture of 4 Gaussians with identity covariance matrix and mean
vectors being the vertices ofa 2-dimensional unit cube. One such sample is presented on Figure 11a.
Next, we analyze locally the Brier scores when the Bayes-optimal rule is used as an underlying
predictor, that is we split the area into small rectangles and estimate the mean score within each
rectangle by a sample average. The results are presented on Figures 11b, 11c and 11d. Note that the
difference between the assigned scores is mostly observed for the points that lie at the intersection
of 4 classes where the support of the corresponding output vectors is large.
(a)
(c)
(b)
I- 1.0
:
■
4 .她瑰
(d)
Figure 11:	(a) Visualization of 4-class classification problem with all classes being equally likely;
(b) localized classic Brier score `brier (equation 11); (c) localized top-label Brier score `brier-top (equa-
tion 12); (d) localized true-class Brier score `brier-true (equation 13).
Brier scores under label shift. Here we consider the case when label shift on the target domain
is present. First, introduce the label likelihood ratios, also known as the importance weights, wy :=
18
Published as a conference paper at ICLR 2022
πyT /πyS, y ∈ Y. For measuring the strength of the shift, we introduce the condition number: κ =
supy wy/ infy:wy 6=0 wy. Note that when the shift is not present, the condition number κ = 1. To
evaluate the sensitivity of the losses to the presence of label shift, we proceed as follows: first, the
class proportions for both source and target domains are sampled from the Dirichlet distribution
(to avoid extreme class proportion, we perform truncation at levels 0.15 and 0.85 and subsequent
renormalization). Then we use the Bayes-optimal rule for the source domain to perform predictions
on the target and compute the corresponding losses. On Figure 12, we illustrate relative increase in
the average Brier scores plotted against the corresponding condition number when all data points are
considered (Figure 12a) and when attention is restricted to the area where classes highly intersect
(Figure 12b). In general, all three versions of the Brier score suffer similarly on average, but in the
localized area where classes highly intersect, the top-label Brier score does not increase significantly
under label shift.
S'"
I — 卦一务0」。U 一 0>sRH
-0.2
0	5	10	15	20	25
Condition number
-0.25
1 1
S'
I — Tf5 一务0」。U 一 0>RH
Classic Brier score
Top-label Brier score
True-class Brier score
10	15	20	25
Condition number
(a)	(b)
Figure 12:	(a) Relative increase for different versions of the Brier score in the multiclass setting
under label shift; (b) Relative increase for different versions of the Brier score in the multiclass
setting under label shift when attention is restricted to the area where classes highly intersect (cube
with vertices at (±1/2, ±1/2)). While in general, all three versions of the Brier score suffer similarly
on average, in the localized area where classes highly intersect, the top-label Brier score does not
increase significantly under label shift.
D Proofs
Proof of Proposition 1. For brevity, we omit writing f for the source/target risks and the corre-
sponding bound upper/lower confidence bounds. Starting with the absolute change and the null
H0 : RT - RS ≤ εtol, we have:
PH0 ∃t ≥ 1 : Lb(Tt) > UbS + εtol
= PH0	∃t	≥ 1 :	LbT	- RT	-	UbS	- RS	> εtol -	(RT	- RS)
≤PH0	∃t≥ 1 :	Lb(Tt)	-RT	-	UbS	- RS	>0 .
Note that ∃t ≥ 1 : (Lb(Tt) - RT) - (UbS - RS) > 0 implies that either ∃t ≥ 1 : Lb(Tt) - RT > 0 or
UbS - RS < 0. Thus, invoking union bound yields:
PH0 ∃t≥ 1 : Lb(Tt) -RT - UbS - RS > 0
≤ P ∃t ≥ 1 : Lb(Tt) - RT > 0 + P UbS - RS < 0
≤ δT + δS ,
by construction and validity guarantees for Lb(Tt) and UbS . Similarly, considering the relative change,
i.e., the null: H00 : RT ≤ (1 + εt0ol)RS, we have:
PH00 ∃t≥ 1: Lb(Tt) > (1+εt0ol)UbS
= PH00	∃t	≥ 1 :	Lb(Tt)	-	RT	- (1 +	εt0ol)	UbS	- RS	> (1 + εt0ol)RS	- RT
≤ PH00	∃t	≥ 1 :	Lb(T)	-	RT	- (1 +	εt0ol)	UbS	- RS	> 0 .
19
Published as a conference paper at ICLR 2022
Similarly, note that ∃t ≥ 1 : (Lb(Tt) - RT ) - (1 + εt0ol)(UbS - RS) > 0 implies that either ∃t ≥ 1 :
Lb(Tt) - RT > 0 or UbS - RS < 0. Thus, invoking union bound yields the desired result.
□
E Primer on the upper and lower confidence bounds
This section contains the details for the concentration results used in this work. Results presented in
this section are not new were developed in a series of recent works (Waudby-Smith & Ramdas, 2021;
Howard et al., 2021). We follow the notation from Waudby-Smith & Ramdas (2021) for consistency
and use the superscript (t) when referring to confidence sequences (CS) and (n) when referring to
confidence intervals (CI).
Predictably-mixed Hoeffding’s (PM-H) confidence sequence. Then the upper and lower end-
points of the predictably-mixed Hoeffding’s (PM-H) confidence sequence are given by:
(t)
PM-H :
UP(Mt)-H :
(Pt=ι λiZi - log(1∕δ) + Pt=I Ψh(λi)
I Pt=ι λi	Pt=ι λi
(P=IλZ + log(1∕δ) + Pt=I Ψh(λi)
I Pt=ι λi	Pt=ι λi
where ψH (λ) := λ2∕8 and λ1, λ2, . . . is a predictable mixture. We use a particular predictable
mixture given by:
λPM-H ：= S 8log(1∕δ) ∧ ι,
t	Vt log(t +1)
When approximating the risk on the source domain, one would typically have a holdout sample of
a fixed size n, and so one could either use the classic upper limit of the Hoeffding's confidence
interval, which is recovered by taking equal λ% = λ =，8 log(1∕δ)∕n, i = 1,...,n, in which case
the upper and lower limits simplify to:
L
(n)
H:
(7 - Y

or by considering running intersection of the predictably mixed Hoeffding’s confidence sequence:
(mint≤n UPM-H, maxt≤n LPM-H).
Predictably-mixed empirical-Bernstein (PM-EB) confidence sequence. The upper and lower
endpoints of the predictably-mixed empirical-Bernstein (PM-EB) confidence sequence are given
by: L(t)	Pi=1 λiZi LPM-EB := ~Pt λ~ U (t)	Pt=I λiZi UPM-EB := ^^t λ~ where	log(1∕δ)+ Pi=ι ViψE(%) 	 	 Pt=1 λi	' log(1∕δ)+ Pi=I ViψE(λi) + PLa ,
Vi := 4(Xi- bi-1)2 , and ψE (X)= (-IOg(I - X)- R 4 for λ ∈ [0, 1).
One particular choice of a predictable mixture (λtPM-EB)t∞=1 is given by:
λPM-EB	I_2log(I∕δ)- A-不2	1 + Pt=I(Zi- bi)2	介1 + Pt=1 Zi
λt	:= Vb2-it log(1+1) ∧ Cc	bt := m , μt := t + 1
for some C ∈ (0,1). We use C = 1∕2 and also set bo = 1∕2, bo = 1∕4. If given a sample of a fixed
size n, we consider running intersection along with the predictable sequence given by:
ʌ PM-EB _ 12log(1∕6	1
ʌt — ʌ	-2i2	∧ c, t — 1,. .., n.
nσbt2-1
20
Published as a conference paper at ICLR 2022
Betting-based confidence sequence. Tighter confidence intervals/sequences can be obtained by
invoking tools from martingale analysis and deploying betting strategies for confidence inter-
vals/sequences construction proposed in (Waudby-Smith & Ramdas, 2021). While those can not
be computed in closed-form, empirically they tend to outperform previously considered confidence
intervals/sequences. Recall that we are primarily interested in one-sided results, and for simplicity
we discuss. For any m ∈ [0, 1], introduce a capital (wealth) process:
t
K±(m) := Y (1 ± λ±(m) ∙ (Zi- m)),
i=1
where {λ+(m)}∞=1 and {λ-(m)}∞ 1 are [0, 1/m]-valued and [0, 1/(1 - m)]-valued predictable
sequences respectively. A particular example of such predictable sequences we use is given by:
λ+ (m) :=卜 +∣ ∧m	λ-(m) :=卜-∣ ∧ i-m，
where, for example, C = 1/2 or 3/4 and λ± do not depend on m. Such choice guarantees, in
particular, that the resulting martingale is nonnegative. For example, the wealth process Kt+ (m)
incorporates a belief that the true mean μ is larger than m and the converse belief is incorporated
in Kt- (m), that is, the wealth is expected to be accumulated under the corresponding belief (e.g.,
consider m = 0 and the corresponding Kt+(0) with a high value, and m = 1 and the corresponding
Kt+(1) with a low value). Using that Kt+(m) is non-increasing in m, i.e., m2 ≥ m1, then Kt+(m2) ≤
Kt+(m1), we thus can use grid search (up to specified approximation error ∆grid) to efficiently
approximate L(Bte)t = inf Bt+, where
B+ := {m ∈ [0, 1] : K+(m) < 1∕δ},
that is, the collection of all m for which that the capital wasn’t accumulated. Then we can consider
LBne) = maxt≤η lB?. When m = μ is considered, none of K±(μ) is expected to be large, since by
Ville’s inequality:
P (∃t ≥ 1 : K+(μ) ≥ 1∕δ) ≤ δ,
and thus we know that with high probability the true mean is larger than L(Bnet) . That is,
P (μ < LBn)) = P (μ < max LBe)) = P (∃t ≥ 1: μ < inf b+)
=P (∃t ≥ 1 : K+(μ) ≥ 1∕δ) ≤ δ.
By a similar argument, we get that with high probability, the true mean is less than UB(net) =
mint≤n sup Bt-:
P (μ > UBnnt)) = P (μ > m∫nsup BJ = P (∃t ≥ 1 : μ > sup B-)
=P (∃t ≥ 1 : K- (μ) ≥ 1∕δ) ≤ δ.
Conjugate-mixture empirical-Bernstein (CM-EB) confidence sequence. Below, we present a
shortened description of CM-EB and refer the reader to Howard et al. (2021) for more details.
Assume that one observes a sequence of random variables Zt, bounded in [a, b] almost surely for
all t, and the goal is to construct a confidence sequence for μt := t-1 Pt=1 Ei-ιZi, the average
conditional expectation. Theorem 4 in Howard et al. (2021) states that for any (Zt), [a, b]-valued
predictable sequence, and any u, the sub-exponential uniform boundary with crossing probability α
for scale C = b - a, it holds that:
≥ 1 - 2α,
where a reasonable choice for the predictable sequence (Zbt) is given by Zbt = (t - 1)-1 Pit=-11 Zi.
21
Published as a conference paper at ICLR 2022
The key challenge which is addressed by conjugate mixtures is obtaining sublinear uniform bound-
aries that allows the radius of the confidence sequences to shrink to zero asymptotically. When a
closed form of the confidence sequence is not required, the gamma-exponential mixture generally
yields the tightest bounds. The procedure relies on the following mixing result (Lemma 2, Howard
et al. (2021)) which states that for any α ∈ (0, 1) and any chosen probability distribution F on
[0, λmax):
UCM(V)= sup ∖s ：∈ R ： J exp (λs — ψ(λ)v) dF(λ) < — > ,
|
^^^^z^
=:m(s,v)
}
yields a sub-ψ uniform boundary with crossing probability α. When the gamma distribution is used
for mixing, m(s, v) has a closed form given in [Proposition 9, Howard et al. (2021)]. Subsequently,
the resulting gamma-exponential mixture boundary uαCM (v) is computed by numerically solving
the equation m(s, v) = 1/— in s. Howard et al. (2021) provide the packages for computing the
corresponding confidence sequence.
F Experiments on simulated data
Figure 13 illustrates data samples for two settings where the presence of label shift is not expected to
cause degradation in model performance (measured in terms of absolute increase in misclassification
risk) for the first one (μo = (-2,0)>, μι = (2,0)>), but label shift may potentially degrade
performance for the second (μo = (-1,0)>, μι = (1,0)>). For both cases, samples follow the
same data generation pipeline as in Section 1 with only changes in class centers.
(a)	(b)
Figure 13:	(a) Simulated dataset with well-separated classes. Presence of label shift presumably will
not lead to a high absolute increase in the misclassification risk. (b) In contrast, when the classes are
not well-separated, presence of label shift presumably might hurt the misclassification risk.
F.1 Brier score as a target metric
Here we replicate the empirical study from Section 3.1 but use the Brier score as a target metric.
Recall that all three multiclass versions of the Brier score discussed in this work reduce to the
same loss in the binary setting. First, we compare upper confidence bounds for the Brier score
computed by invoking different concentration results on Figure 14. Similar to the misclassification
risk, variance-adaptive confidence bounds exploit the low-variance structure and are tighter when
compared against the non-adaptive one.
Next, we perform empirical analysis of the power of the testing framework. We take εtol = 0.1
which corresponds to testing for a 10% relative increase in the Brier score. We take nS = 1000 data
points from the source distribution to compute upper confidence bound on the source risk US(f).
Subsequently, we sample the data from the target distribution in batches of 50, with maximum
number of samples from the target set to be 2000. On Figure 14b, we present the proportion of
cases when the null hypothesis is rejected out of 250 simulations performed for each candidate class
1 probability. On Figure 14c, we illustrate average sample size from the target domain that was
needed to reject the null hypothesis. When a stronger and more harmful label shift is present, less
samples are required to reject the null, and moreover, the most powerful tests utilize upper/lower
22
Published as a conference paper at ICLR 2022
confidence bounds obtained via the betting approach. On Figure 14d, we present the comparison of
different time-uniform lower confidence bounds.
0	200	400	600	800	1000
Number of samples from the source
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9
Marginal probability of class 1 on the target, πT
(b)
0.16
0.14
0.12
凯.10
T 0.08
目 0.06
0.04
0.02
0.00
(a)
=1600
J 1400
[1200
∂
比 1000
800
600
400
200
Ol 0z-s 0-dEes
0.4	0.5	0.6	0.7	0.8	0.9
Marginal probability of class 1 on the target, πT
0	250	500	750	1000	1250	1500	1750	2000
Number of samples from the target domain
(c)	(d)
Figure 14:	(a) Upper confidence bounds US (f) on the Brier score for the source domain. Sim-
ilar to the misclassification risk, variance-adaptive confidence bounds are tighter when compared
against the Hoeffding’s one. For each fixed number of data points from the source domain used to
compute US(f), presented results are aggregated over 1000 random data draws. (b) Proportion of
null rejections made by the procedure when testing for 10% relative increase of the Brier score. (c)
Average sample size from the target distribution that was needed to reject the null. Invoking tighter
concentration results allows to raise an alarm after processing less samples from the target domain.
(d) Different lower/upper confidence bounds on the target/source domain for the Brier score.
G	Experiments on real datasets
G.1 MNIST-C SIMULATION
Architecture and training. For MNIST-C dataset, we train a shallow CNN with two convolu-
tional layers (each with 3 × 3 kernel matrices), each followed by max-pooling layers. Subsequently,
the result is flattened and followed by a dropout layer (p = 0.5), a fully-connected layer with 128
neurons and an output layer. Note that the network is trained on original (clean) MNIST data, which
is split split into two folds with 10% of data used for validation purposes. All images are scaled to
[0, 1] range before the training is performed.
Training multiple networks. To validate observations regarding shift malignancy from Sec-
tion 3.2, we train 5 different networks (following the same training protocol) and report aggregated
(over 25 random ordering of the data from the target) results on Figure 15. The observation that ap-
plying translation to the MNIST images represents a harmful shift is consistent across all networks.
G.2 CIFAR-10-C simulation
Architecture and training. The model underlying a set-valued predictor is a standard ResNet-32.
It is trained for 50 epochs on the original (clean) CIFAR-10 dataset, without data augmentation,
using 10% of data for validation purposes. All images are scaled to [0, 1] range before the training
is performed. The accuracy of the resulting network is ≈ 80.5%.
23
Published as a conference paper at ICLR 2022
Figure 15: Lines of the same color correspond to 5 different CNNs. For each network, the results
aggregated over 25 random runs of the testing framework for randomly permuted test data. Applying
translate effect is consistently harmful to the performance of CNNs trained on clean MNIST data.
The bar around the yellow dashed line corresponds to 2 standard deviations.
Transforming a point predictor into a set-valued one. To transform a point predictor into a set-
valued one, we consider a sequence of candidate prediction sets Sλ(x), parameterized by univariate
parameter λ, with larger λ leading to larger prediction sets. Under the considered setting, the un-
derlying predictor is an estimator of the true conditional probabilities πy(x) = P (Y = y | X = x).
Given a learnt predictor f, we can define
K
Py(x； f) ：= X fk(x) ∙ l{fk(x) > fy(x)}
k=1
to represent estimated probability mass of the labels that are more likely than y. Subsequently, we
can consider the following sequence of set-valued predictors:
Sλ(x) = {y ∈ Y : Py(x； f) ≤ λ} , λ ∈ Λ := [0, 1],
that is the sequence is based on the estimated density level sets, starting by including the most likely
labels according to the predictions of f. To tune the parameter λ, we follow Bates et al. (2021): we
keep a labeled holdout calibration set, and use it to pick:
λb = inf λ ∈ Λ : Rb+ (λ0) < β, ∀λ0 > λ ,
where Rb+(λ0) is an upper confidence bound for the risk function at level β. The resulting set-valued
predictor is then (β, γ)-RCPS, that is,
P (R(Sb) ≤ β) ≥ 1 - Y
under the i.i.d. assumption. More details and validity guarantees can be found in Bates et al. (2021).
Set-valued predictor when β = 0.05 is used as a prescribed error level. In contrast to β =
0.1 used in the main paper, we also consider decreasing β to 0.05, which in words, corresponds
to increasing a desired coverage level of the resulting set-valued predictor. Figure 16a compares
average sizes of the prediction sets for two candidate values: β1 = 0.05 and β2 = 0.1, when the
set-valued predictor is passes either clean CIFAR-10 data, or images to which fog corruption has
been applied. As expected, decreasing β leads to larger prediction sets on average, with the size
increasing when corrupted images are passes as input, that the size reflects uncertainty in prediction.
In Figure 16b, we observe that when we run the testing framework for the set-valued predictor
corresponding to β = 0.05, only the most severe version of corruptions by adding fog is consistently
marked as harmful, and thus raising an alarm. Similar to Section 3.2, we also use εtol = 0.05.
H Testing for harmful covariate shift
In this section, we consider a case when covariate shift is present on the target domain, that is, when
the marginal distribution P(X) changes but P(Y |X) does not. Consider the binary classification
24
Published as a conference paper at ICLR 2022
(a)	(b)
Figure 16: (a) Average size of prediction sets for β1 = 0.05 and β2 = 0.1 and different types of
input data. First, lower β, corresponding to higher desired coverage, leads to larger prediction sets
on average. Second, average size of the prediction sets increases when more corrupted images are
passed as input, thus reflecting uncertainty in prediction. (b) Results of running the framework when
β1 = 0.05 is used to construct a wrapper. Observe that setting a lower prescribed error level β and
thus enlarging resulting prediction sets partially mitigates the impact of corrupting images with the
fog effect. However, the most severe form of such corruption still consistently leads to rejecting the
null. The bars around dashed and solid lines correspond to 2 standard deviations.
setting with accuracy being a target metric. It is known that the optimal decision rule for this case is
the Bayes-optimal rule:
广(X) = 1{P (Y = 11 X = x) ≥ 1/2},
which minimizes the probability of misclassifying a new data point. Then one might expect that
a change in the marginal distribution of X does not necessarily call for retraining if a learnt pre-
dictor f is ‘close’ to f? (which itself could be a serious assumption to make). However, a change
in the marginal distribution of X could indicate, in particular, that one should reconsider certain
design choices made during training a model. To illustrate when it could be useful, we consider the
following data generation pipeline:
1.	Initially, each point is assigned either to the origin or to a circle of radius 1 centered at the
origin with probability 1/2 .
2.	For points assigned to the origin, the coordinates are sampled from multivariate Gaussian
distribution with zero mean and rescaled identity covariance matrix: 3612.
3.	For points assigned to the circle, the coordinates are sampled from multivariate Gaussian
distribution with the same covariance matrix but with the mean vector:
[χx = cos(0,
Vy =Sin(P),
where 夕S 〜 Unif([-π∕3,π∕3]) on the source domain and 夕T 〜 Unif([0, 2π]) on the
target domain (see Figure 17 for a visualization).
4.	Then points are assigned the corresponding labels according to:
P (Y = 1 | X = X) = I {x； + x2 ≥ 2
Itis easy to see that a linear predictor, e.g., logistic regression, can achieve high accuracy if deployed
on the data sampled from the source distribution. However, it will clearly fail to recover the true
relationship between the features and responses. In this case, a change in the marginal distribution
of X might indicate that updating a functional class could be necessary. On this data, we also run
the proposed framework testing for a 10% increase in misclassification risk (εtol = 0.1). At each
run, we use 200 points to train a logistic regression and 100 points to estimate the betting-based
upper confidence bound on the source risk. On the target domain, we use the lower confidence
bound due to conjugate-mixture empirical-Bernstein (CM-EB). The results presented on Figure 17c
(which have been aggregated over 100 random data draws) illustrate that the framework successfully
detects a harmful shift, requiring only a small number of samples to do so.
25
Published as a conference paper at ICLR 2022
(a)	(b)
(c)
Figure 17: (a) Data samples from the source (red) and target (blue) distributions for the covariate
shift simulation. (b) Logistic regression predictor learnt on the source distribution plotted along with
a data sample from the target distribution. While learnt predictor clearly has high accuracy on the
source domain, it fails to approximate the true underlying data generating distribution. (c) Results of
running the framework when testing for a 10% increase in the misclassification risk. The framework
detects a harmful shift after processing only a small number of samples.
26