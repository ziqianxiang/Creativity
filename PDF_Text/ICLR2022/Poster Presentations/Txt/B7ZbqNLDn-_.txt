Published as a conference paper at ICLR 2022
Recycling Model Updates in Federated
Learning: Are Gradient Subspaces Low-Rank?
Sheikh Shams Azam, Seyyedali Hosseinalipour, Qiang Qiu, Christopher Brinton
School of ECE, Purdue University
{azam1, hosseina, qqiu, cgb}@purdue.edu
Ab stract
In this paper, we question the rationale behind propagating large numbers of
parameters through a distributed system during federated learning. We start by
examining the rank characteristics of the subspace spanned by gradientsacross
epochs(i.e., the gradient-space) in centralized model training, and observe thatthis
gradient-space often consists of a few leading principal components accounting for
an overwhelming majority (95 - 99%) of the explained variance. Motivated by this,
we propose the "Look-back Gradient Multiplier" (LBGM) algorithm, whichexploits
this low-rank propertyto enable gradient recyclingbetween model update rounds
of federated learning, reducing transmissions of large parameters to single scalars
for aggregation. We analytically characterize the convergence behavior of LBGM,
revealing the nature of the trade-off between communication savings and model
performance. Our subsequent experimental results demonstrate the improvement
LBGM obtains in communication overhead compared toconventionalfederated
learningon several datasets and deep learning models. Additionally, we show that
LBGM is a general plug-and-play algorithm that can be used standalone or stacked
on top of existing sparsification techniques for distributed model training.
1	Introduction
Federated Learning (FL) (Konecny et al., 2016) IlsS emrgged ss a PoPUaar dStUibueed mccldne
learning (ML) paradigm. By having each device conduct local model updates, FL substitutes raw data
transeissions with eodel paraeeter transeissions, proeoting data privacy (Shokri & Sheatikov,
2015;Azae et al.,2021) and coeeunication savings (Wang et al.,2020a). At the saee tiee,
overparaeeterized neural networks (NN) are becoeing ubiquitous in the ML eodels trained by
FL, e.g., in coeputer vision (Liu et al.,2016;Huang et al.,2017) and natural language processing
(Brown et al.,2020;Liu et al.,2019a). While NNs can have paraeeters in the range of a few
eillion (VGG (Sieonyan & Zisserean,2014), ResNet (He et al.,2016)) to several billions (GPT-3
(Brown et al.,2020), Turing NLG (tur,2020)), prior works have deeonstrated that a eajority of
these paraeeters are often irrelevant (Frankle & Carbin,2019;Liu et al.,2019b;Han et al.,2015;Li
et al.,2017) in optieization and inference. This presents an opportunity to reduce coeeunication
overhead by transeitting lighter representations of the eodel, conventionally achieved through
coepression/sparsification techniques (Wang et al.,2018;Alistarh et al.,2017;Vogels et al.,2019).
In this work, we investigate the “overparaeeterization” of NN training, through the lens of rank
characteristics of the subspace spanned by the gradients (i.e., the gradient-space) generated during
stochastic gradient descent (SGD).We start with thefundaeental question: can we observe the
effect of overparameterization in NN optimization directly through the principal components analysis
(PCA) of the gradients generated during SGD-based training? And if so: can this property be used
to reduce communication overhead in FL? Our eain hypothesis is that
the subspaces spanned by gradients generatedacross SGD epochsare low-rank. (H1)
This leads us to propose a technique that instead of propagating a eillion/billion-dieensional vector
(i.e., gradient) over the systee in each iteration of FL only requires propagating a single scalar in the
eajority of the iterations. Our algorithe introduces a new class of techniques based on the concept of
reusing/recycling device gradients over tiee. Our eain contributions can be sueearized as follows:
•	We deeonstrate the low-rank characteristics of the gradient-space by directly studying its prin-
cipal coeponents for several NN eodels trained on various real-world datasets.We show that
1
Published as a conference paper at ICLR 2022
Figure 1: PCA components progression. The top row shows the number of components that account for 99%
(N99-PCA in blue) and 95% (N95-PCA in red) explained variance of all the gradients generated during gradient
descent epochs. The bottom row shows the performance of the model on the test data. The results are presented
for: (i) CIFAR-10 classification (left 4 columns), and (ii) CelebA regression (right 4 columns).
principal gradient directions (i.e., directions of principal components of the gradient-space)can
be approximated in terms of actual gradients generated during the model training process.
•	Our insights lead us to develop the “Look-back Gradient Multiplier” (LBGM) algorithm tosig-
nificantly reduce the communication overhead in FL. LBGM recyclespreviously transmitted
gradientsto represent the newly-generated gradients at each device with a single scalar. We
further analytically investigate the convergence characteristics of this algorithm.
•	Our experiments show the communication savings obtained via LBGM in FL both as a standalone
solution and a plug-and-play method used with other gradient compression techniques, e.g., top-K.
We further reveal that LBGM can be extended to distributed training, e.g., LBGM with SignSGD
(Bernstein et al.,2018) substantially reduces communication overhead in multi-GPU systems.
2	A Gradient- S pace Odyssey
We firststart by directly studying the principal component analysis (PCA) ofthe gradient-space of
overparameterized NNs. Given a centralized ML training task (e.g., classification, regression, seg-
mentation), we exploit principal component analysis (PCA) (Pearson,1901) to answer the following:
how many principal components explain the 99% and 95% variance (termed N99-PCA and N95-PCA,
respectively; together N-PCA) of all the gradients generated during model training?
We start with 4 different NN architectures: (i) fully-connected neural network (FCN), (ii) convo-
lutional neural network (CNN), (iii) ResNet18 (He et al.,2016), and (iv) VGG19 (Simonyan &
Zisserman,2014); trained on 2 datasets: CIFAR-10 (Krizhevsky et al.,2009) and CelebA (Liu
et al.,2015), with classification and regression tasks, respectively. We then compute the n-pca for
each epoch by applying PCA on the set of gradients accumulated until that epoch (pseudo-code in
Algorithm2in AppendixD.1 ). The results depicted in Fig.1agree with our hypothesis (H1): both
n99-pca and n95-pca are significantly lower than that the total number of gradients generated
during model training, e.g., in Fig.1 * the number of principal components (red and blue lines in top
plots) are substantially lower (often as low as 10% of number of epochs, i.e., gradients generated)
for both datasets. In AppendixE.1 , we further find that (H1) holds in our experiments using several
additionaldatasets: CIFAR-100 (Krizhevsky et al.,2009), MNIST (LeCun & Cortes,2010), FMNIST
(Xiao et al.,2017), CelebA (Liu et al.,2015), PascalVOC (Everingham et al.,2010), COCO (Lin et al.,
2014); models: U-Net (Ronneberger et al.,2015), SVM (Cortes & Vapnik,1995); and tasks: segmen-
tation and regression. Note that (especially on CIFAR-10) variations in n-pca across models are not
necessarily related to the model performance (CNN performs almost as well as ResNet18 but has
much lower n-pca; Fig. 1 —ooUmmsS 2 &3) OTOomPeeXyy(NNVhaSmOTePrammeeesShannCCN UUt
has lower n-pca; Fig. 1 - columns 1 &2). Thf PcCthPatPnnkdficCienCyffheegPddient-SPceeiSnot a
consequence of model complexity or performance suggests that the gradient-space of state-of-the-art
large-scale ML models could be represented using a fewprincipal gradient directions (PGD3.
N-PCA and FL. In an “ideal” FL frameworn, if both the server and the worners/devices have
thePGDs, then the newly generated gradients can be transmitted by sharing their projections on
thePGDs, i.e., thePGDmultipliers (PGM3. PGMs andPGDscan together be used to reconstruct the
device generated gradients at the server,dramatically reducing communication costs.However, this
setting is impractical since: (i3 it is infeasible to obtain thePGDsprior to the training, and (ii3 PCA is
computationally intensive. We thus loon for an efficient online approximation of thePGDs.
*The addition of a learning rate scheduler (e.g., cosine annealing scheduler (Loshchilov & Hutter,201633 has
an effect on the PCA of the gradient-space. Careful investigation of this phenomenon is left to future worn.
2
Published as a conference paper at ICLR 2022
Figure 2: Overlap of actual and principal gradients. The heatmap shows the pairwise cosine similaritybetween
actual (epoch) gradients andprincipal gradient directions (PCA gradients). xpoch gradients have a substantial
overlap with one or more PCA gradients and consecutive epoch gradients show a gradual variation.This suggest
that there may exist a high overlap between the gradients generated during the NN model training.The results
are shown for a CNN classifier trained on CIFAR-10 (left 4 columns) and CelebA (right 4 columns) datasets.
xach subplot is marked with #L, the layer number of the CNN and #elem, the number of elements in each layer.
CIFAR-10, L#1
#elem: 1500
o #elem: 500
ST
So 50 IOO 150 200
epoch gradients
CIFAR-10, L#3
#elem: 25000
SWqPeJb U
Figure 3: Similarity among consecutive gradients. The cosine similarity of consecutive gradients reveals a
gradual change in directions of gradients over epochs. Thus, the newly generated gradients can be representedin
terms of the previously generated gradientswith low approximation error.Reusing/recycling gradients can thus
lead to significant communication savings during SGD-based federated optimization.
Overlap of Actual Gradients and N-PCA. To approximatePGDs, we exploit an observation made
in Fig.1(and further in AppendixE.1): n-pca mostly remains constant over time, suggesting that
the gradients change gradually across SGD epochs. To further investigate this, in Fig.2(and further
in Appendix E.2) We POot the COSnne Simiaartty t)etween GGDaanaaCaUaggdaenentaaa hhemtmap., We
observe that (i) the similarity of actual gradients toPGDsvaries gradually over time, and (ii) actual
gradientn have a high conine nimilarity with one or morePGDn. Thin leadn to oCr necond hypothenin:
PGDscan be approximated using a subset of gradientsgenerated across SGD epochs . (H2)
Look-back Gradients. OCr obnervationn above nCggent a nignificant overlap among connecCtive
gradients generated during SGD epochs. Thin in further verified in Fig. 3(andin Appendix E.3*),
where we plot the pairwise cosine similarity of consecutive gradients generated during SGD epochs.t
For example, consider the boxes marked B1, B2, and BE in layer 1 (L#1 in Fig.E). Gradients in each
box can be used to approximate other gradients within the box. Also, interestingly, the number of
such boxes that can be drawn is correlated with the corresponding number ofPGDs. Based on (H2),
we next propose our Look-back Gradient Multiplier (LBGM) algorithm that utilizes a subset of actual
gradients, termed “look-back gradients”, to reuse/recycle gradients transmitted in FL.
E Look-back Gradient Multiplier Methodology
Federated Learning (FL) considers a system of K workers/devices indexed 1, ..., K, as shown in
Fig.4. xach worker k possesses a local dataset Dk with nk = |Dk| datapoints. The goal of the
system is to minimize the global loss function F(∙) expressed through the following problem:
K
θm∈RinMF(θ) ,XωkFk(θ),	(1)
k=1
tRefer to Algorithm 2nn AppeddixD』forthedeteiledpceudonpde.
*2 of 24 experiments (Fig.52&5Ein Appendixx.E) show inconsistent gradient overlaps. However, our
algorithm discussed in Sec.Estill performs well on those datasets and models (see Fig.60in Appendixx.E).
E
Published as a conference paper at ICLR 2022
Figure 4: Look-back gradient multiplier. (a) The Look-back Coefficients (LBCs) are the projection of
accumulated stochastic gradients at the workers on their Look-back Gradients (LBGs). (b) Scalar LBCs, i.e., the
ρkt),', are transmitted to the server. (C) LBG-based gradient approximations are reconstructed at the server.
where M is the dimension of the model θ , ωk = nk /N, N = PkK=1 nk , and Fk (θ) =
Pd∈D fk (θ ; d)/nk is the local loss at worker k, with fk (θ ; d) denoting the loss function for
data sample d given parameter vector θ. FL tackles (1) via engaging the workers in local SGD model
training on their own datasets. The local models are periodically transferred to and aggregated at the
main server after τ local updates, forming a global model that is used to synchronize the workers
before starting the next round of local model training.
At the start of round t, each model parameter θ(kt,0) is initialized with the global model θ(t). Thereafter,
worker k updates its parameters θkt,b) as: θkt,b+1) - θkt,b) - ηgk(θa%, where gg(θkt,b)) is the
stochastic gradient at local step b, and η is the step size. During a vanilla FL aggregation, the global
model parameters are updated as θ(t+1) J θ(t) - ηPkK=1 ωkg(kt), where g(kt) = Pτb=-01 gk(θ(kt,b))
is the accumulated stochastic gradient (ASG) at worker k.More generally, this aggregation may
be conducted over a subset of workers at time t. Wddfinn e VFk(θf)) = PT-l1 % Fk (θf,") and
戏)=gkt)∕τ as the corresponding accumulated true gradient and normalized ASG, respectively.
Indexing and Notations. In superscripts with parenthesis expressed as tuples, the first element
denotes the global aggregation round while the second element denotes the local update round, e.g.,
g(kt,b) is the gradient at worker k at global aggregation round t at local update b. Superscripts without
parenthesis denote the index for look-back gradients, e.g., ' in Pkt),' defined below.
LBGM Algorithm. LBGM (see Fig.4) consists of three main steps: (i) workers initialize and propagate
their look-back gradients (LBGs) to the server; (ii) workers estimate their look-back coefficients
(LBCs), i.e., the scalar projection of subsequent ASGs on their LBGs, and the look-back phase (LBP),
i.e., the angle between the ASG and the LBG; and (iii) workers update their LBGs and propagate them
to the server if the LBP passes a threshold, otherwise they only transmit the scalar LBC. Thus, LBGM
propagates only a subset of actual gradients generated at the devices to the server. The intermediate
global aggregation steps between two LBG propagation rounds only involve transfer of a single
scalar, i.e., the LBC, from each worker, instead of the entire ASG vector.
In LBGM, the local model training is conducted in the same way as vanilla FL, while model aggrega-
tions at the server are conducted via the following rule:
K
θ(t+1) = θ(t) - η Xωkge(kt),	(2)
k=1
where ge(kt) is the approximation of worker k’s accumulated stochastic gradient, given by Definition1.
Definition 1. (Gradient Approximation in LBGM) Given the ASG g(kt), and the LBG g`k, the gradient
approximation ge(kt) recovered by the server is given by:
忒，=Pkt,egi for ∣∣ρkt),'dk∣∣ = ∣∣dkt)CoSmkt),'“,	(DI)
where LBC PF' = hgkt), gki∕∣∣gk ∣∣2 is the projection ofthe accumulated gradient gkt) on the LBG
gk, and LBP Okt, denotes the angle between gkt) and g£ (see Fig/(ɑ)).
4
Published as a conference paper at ICLR 2022
Algorithm 1 LBGM: Look-back Gradient Multiplier
Notation:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
θ(t): global model parameter at global aggregation round t.
θ(kt,b): model parameter at worker k, at global aggregation round t and local update b.
g (kt) : accumulated gradient at worker k at global aggregation round t.
g`k : last full gradient transmitted to server, termed look-back gradient (LBG).
αkt),': phase between the accumulated gradient gkt) and LBG gk, termed look-back phase (LBP).
Training at worker k:
Upaateloaalaaaameters: θkt,0) 一 θ⑴，and initialize gradient accumulator: gkt) — 0.
for b = 0 to (τ -1) do
SemPleaminibotCh OadataPOints Bk from Dk and compute gkt,b) = Pd∈Bk Vfk(θkt,b); d)∕∣Bk|.
Update local parameters: θkt,b+* 1) J θkt,b) 一 η ∙ gkt,b), and accumulate gradient: g：t J g：t + gkt,b).
end for
CaICUIatetheLBPerror: sin2 * *(α”) = 1 -(hgkt),gki/(kgkt) k X kgkk))?
if sin2(akt),') ≤ δkhreshold then	. checking the LLP error
SCndeCaBCLBCtOtheeerVer: μ? J Py/=〈g2,gɪi/kgkk2.
else	. updating the LBG
SendaCtUaI gtadienttothe rerver: μ? J gkt.
pdate worker-copy ot LBG:	g`k J g(kt) .
end if
Global update at the aggregation server:
nitialize global parameter θ(0) and broadcast it across workers.
for t = 0 to (T — 1) do
ReCCiVeUPdateS from WPrkers {μkt) }K=ι∙
UPdategIeaalPeremeters: θ"+1) J θ⑴—η PK=I ωk ∣¾ ∙ μ? ∙ gkk + (1 — Sk) ∙ μk[,
where sk is an indicator tunction gieen by, sk
ʃ 1,	if μkt) is a scalar
[0, otherwise, i.e., if μkt is a vector
UPdateeeCVer-CfPyOfLBGs: gk J (1 — sk)μ(kt + (Sk)gk, ∀k.
end for
LBGM initializes the LBGs g`k with the first actual gradients propagated by the deeices at t = 1. For
the subsequent aggregations, each worker k shares only its LLC Pkt),' with the server if the value Vf
LLP erevr sin2(akt),') is below a threshold, i.e., sin2(akt),') ≤ δkhreshold, where δkhreshold ∈ [0,1] is a
tunable parameter; otherwise it updates the LLG via transmitting the entire gradient to the server.
The details of LBGM are given in Algorithm1. We next conduct convergence analysis for LBGM.
Assumptions: It is presumed that the local loss functions are bounded below: minθ∈RM Fk (θ) >
-∞, ∀k. Let k ∙ k denote the 2-norm. C)UrnIayyiiS UserihefoVrWiggandIddadauSUmPnPnS(Wang
et al.,2020a;Friedlander & Schmidt,2012;Hosseinalipour et al.,2020;Li et al.;Stich,2019):
1. Smoothness of Local Loss Functions: Local loss function Fk : RM → R, ∀k, is β-smooth:
IIVFk (θχ ) -VFk (θy )|| ≤ β∣∣θχ - θyH , ∀θχ, θy € RM .	(A-
2. SGD Characteristics: Local gradients gk (θ), ∀k, estimated by SGD are unbiased estimators of
the true gradients VFk(θ), and have a bounded variance σ2 ≥ 0; mathematically:
E[gk(θ)] =VFk(θ),andE kgk(θ) -VFk(θ)k2 ≤σ2, ∀θ ∈RM.	(A2)
3. Bounded Dissimilarity of Local Loss Functions: For any realization of weights {ωk ≥
where PkK=1 ωk = 1, there exist non-negative constants Υ2 ≥ 1 and Γ2 ≥ 0 such that
0}kK=1,
K
XωkkVFk(θ)k2 ≤Υ2
k=1
K
X ωkVFk (θ)
k=1
2
+ Γ2 , ∀θ ∈ RM.
(A3)
5
Published as a conference paper at ICLR 2022
Theorem 1. (General Convergence Characteristic of LBGM) Assume A1, A2, A3, and that ηβ ≤
min {l∕(2τ), 1∕(τ,2(1 + 4Υ2))}. Ifthe threshold value in step 7oAAgoOritmIa SfifiiS班es the
condition δkthreshold ≤ ∆2 /kd(kt) k2, ∀k , where ∆2 ≥ 0 is a constant, then after T rounds of global
aggregations, the performance of LBGM is characterized by the following upper bound:
T-1
T XE ∣∣VF(θ(t))∣∣
t=0
<8∣F(θ(0)) - F?]
ητT
+16∆2 +8ηβσ2+5η2β2σ2(τ - 1)+20η2β2Γ2τ(τ - 1). (3)
2

Proof. The proof is provided in AppendixA.
The condition on δkhreshold and LBP error sin2 (ɑf') in the above theorem implies that to have a fixed
bound in (3), for a fixed ∆2, a larger gradient norm kd(kt) k2 is associated with a tighter condition on
the LBP error sin2(αkt),') and δkhreshold. This is intuitive because a larger gradient norm corresponds
to a larger estimation error when the gradient is recovered at the server for a given LBP (see Fig.4).
In practice, since the gradient norm kd(kt) k2 does not grow to infinity during model training, the
condition on LBP error in Theorem1, i.e., sin2(α即)≤ ∆2∕kdkt)k2, can always be satisfied for
any ∆2 ≥ 0, since transmitting actual gradients of worker k makes Okt, = 0. Given the general
convergence behavior in Theorem1, we next obtain a specific choice of step size and an upper bound
on ∆2 for which LBGM approaches a stationary point of the global loss function (1).
Corollary 1. (Convergence of LBGM to a Stationary Point) Assuming the conditions of Theorem ,,
if ∆2 ≤ η, where η = 1∕√τT, then LBGM converges to a stationary point of the global loss function,
with the convergence bound characterized below:
T X0E [«vF (Mt))∣∣2] ≤O(√Tτ)+O(√⅛)+O(√⅛)+O(⅛1)+O(H
(4)
Proof. The proof is provided in Appendix B.	■
Considering the definition of ∆2 in Theorem1and the condition imposed on it in Corollary1,
the LBP error (i.e., sin2(Qkt),')) should satisfy IIdkt)k2 sin2(Qkt),') ≤ η = 1∕√τT for LBGM to
reach a stationary point of the global loss. Since kd(kt) k2 is bounded during the model training,
this condition on the LBP error can always be satisfied by tuning the frequency of the full (actual)
gradient transmission, i.e., updating the LBG as in lines10&11of Algorithm1. Specifically, since
the correlation across consecutive gradients is high and drops as the gradients are sampled from
distant epochs (see Fig.3), a low value of the LBP error can be obtained via more frequent LBG
transmissions to the server.
Main Takeaways from Theorem1, Corollary1, and Algorithm1:
1.	Recovering Vanilla-FL Bound: In (3), if the LBGs are always propagated by all the devices, we
have αkt),' = 0, ∀k, and thus ∆2 = 0 satisfies the condition on the LBP error. Then, (3) recovers
the bound for vanilla FL (Wang et al.,2020a;Stich,2019;Wang & Joshi,2018).
2.	Recovering Centralized SGD Bound: In (3), if the LBGs are always propagated by all the devices,
i.e., ∆2 = 0, the local dataset sizes are equal, i.e., wk = 1∕K, ∀k, and τ = 1, then (3) recovers
the bound for centralized SGD, e.g., seeFriedlander & Schmidt(2012).
3.	Unifying Algorithm1and Theorem1: The value of ∆2 in (3) is determined by the value of the
LBP error sin2(αkt),'), which is also reflected in step 7of AggOrithm LThissuggestsatateie
performance improves when the allowable threshold on sin2(&£),') is decreased (i.e., smaller
∆2), which is the motivation behind introducing the tunable threshold δkthreshold in our algorithm.
4.	Effect of LPB Error on Convergence: As the value of sin2(&£),') increases, the term in (3)
containing ∆2 will start diverging (it can become the same order as the gradient Id(kt) I2). The
condition in Corollary1on ∆2 avoids this scenario, achieving convergence to a stationary point.
5.	Performance vs. Communication Overhead Trade-off: Considering step7of Algorithm1,
increasing the tolerable threshold on the LBP error increases the chance of transmitting a scalar
(i.e., LBC) instead of the entire gradient to the server from each worker, leading to communication
savings. However, as seen in Theorem1and the condition on ∆2 in Corollary1, the threshold on
the LBP error cannot be increased arbitrarily since the LBGM may show diverging behavior.
6
Published as a conference paper at ICLR 2022
—— Vanilla FL —— 6^reshold=0.8 —— 6^reshold=0.4
——ethreshold=Q 9	---- ðthresholdɪɑ θ ——ðthresholdɪɑ 2
Vanilla FL
LBGM
MNIST
shared(×106)
FMNlST
Non-IlD
#params
shared(×106)
CIFAR-IO	CeIebA
#params #ParamS
shared(×106)	shared(×106)
Figure 5: LBGM as a Standalone Algorithm. Irrespec-
tive of the dataset/data configuration across workers,
LBGM consistently outperforms vanilla FL in terms of
the total parameters shared (middle row) while achiev-
ing comparable accuracy (top row). The bottom row
shows accuracy vs. # parameters shared.
MNIST
Shared(XlOe)
FMNlSr
Non-IID
Shared(XlOe)
CIFAR-IO	CeIebA
Non-IID	Regression
shared(×106) shared(×106)
Figure 6: Effect of δkthreshold on LBGM. As δkthreshold
decreases, the training may become unstable. For
larger values of δkthreshold , LBGM achieves communica-
tion benefits (middle row) while maintaining a perfor-
mance identical to vanilla FL (top row). The bottom
row shows accuracy vs. # parameters shared.
4 Experiments
Model Settings. We run experiments on several NN models and datasets. Specifically, we consider:
S1: CNN on FMNIST, MNIST, CelebA, and CIFAR-10, S2: FCN on FMNIST and MNIST, and
S3: ResNet18 on FMNIST, MNIST, CelebA, CIFAR-10 and CIFAR-100 for both independently and
identically distributed (iid) and non-iid data distributions. We present results of S1 (on non-iid data)
in this section and defer the rest (including S1 on iid data and U-Net on PascalVOC) to AppendixF.
Properties Studied. We specifically focus on four properties of LBGM: P1: the benefits of gradient
recycling by LBGM as a standalone algorithm, P2: the effect of δkthreshold on LBGM from Theorem1,
P3: practical capabilities of LBGM as a general plug-and-play algorithm that can be stacked on top
of other gradient compression techniques in FL training, and finally P4: generalizability of LBGM to
distributed learning frameworks, e.g., multi-processor or multi-GPU ML systems.
Baselines. For P1 and P2, we compare LBGM with vanilla FL. For P3, we stack LBGM on top of top-K
and ATOMO (Wang et al.,2018), two state-of-the-art techniques for sparsification and low-rank
approximation-based gradient compression, respectively. For P4, we stack LBGM on top of SignSGD
(Bernstein et al.,2018), a state-of-the-art method in gradient compression for distributed learning.
Implementation Details. We consider an FL system consisting of 100 workers. We consider both the
iid and non-iid data distributions among the workers. Under the iid setting, each worker has training
data from all the labels, while under the non-iid setting each worker has training data only from a
subset of all labels (e.g., from 3 of 10 classes in MNIST/FMNIST). The workers train with mini-batch
sizes ranging from 128 to 512 based on the choice of dataset. We implement LBGM with uniform
δkthreshold across workers. We also use error feedback (Karimireddy et al.,2019) as standard only if top-
K sparsification is used in the training. The FL system is simulated using PyTorch (Paszke et al.,2019)
and PySyft (Ryffel et al.,2018) and trained on a 48GB Tesla-P100 GPU with 128GB RAM. All of our
code and hyperparameters are available at https://github.com/shams-sam/FedOptim.
AppendixC.2details the process of hyperparameter selection for the baselines.
Complexity. Compared to other gradient compression techniques, the processing overhead introduced
by LBGM is negligible. Considering Algorithm1, the calculation of LBCs and LBP errors involves
inner products and division of scalars, while reconstruction of LBG-based gradient approximations
at the server is no more expensive than the global aggregation step: since the global aggregation
step requires averaging of local model parameters, it can be combined with gradient reconstruction.
This also holds for LBGM as a plug-and-play algorithm, as top-K and ATOMO (Wang et al.,2018)
introduce considerable computation overhead. In particular, LBGM has O(M) complexity, where M
is the dimension of the NN parameter, which is inexpensive to plug on top of top-K (O(M log M)),
ATOMO (Wang et al.,2018) ( O(M2)), and SignSGD (Bernstein et al.,2018) ( O(M)) methods.The
corresponding space complexity of LBGM for the server and devices is discussed in AppendixC.1.
LBGM as a Standalone Algorithm. We first evaluate the effect of gradient recycling by LBGM in
FL. Fig.5depicts the accuracy/loss values (top row) and total floating point parameters transferred
7
Published as a conference paper at ICLR 2022
---w/o LBGM ——w/ LBGM
MNIST	FMNIST
NOn-IID/Top-K	Non-IID/Top-K
I-OO-
CIFAR-IO	CeIebA
Non-IID/ATOMO	ATOMO
(90Is
0∙,5050°,5
25 50 75 IOO
t
0.；
O.J
o.:
50 IOO 150
t
25 50 75 IOO
t
10.0
7.5
50 IOO 150 200
t
CIFAR-IO	CeIebA
MNIST	FMNIST
Non-IIDZTop-K
Top-K Non-IIDZATOMO Non-IIDZATOMO
,2.0-
0.6
0.4 ,
2 5 50 7 5 IOO
t
75 150 225 300
t
125 250 375 500
t
50 IOO 150 200
t
*■LLaC
(°IsSs-
胃
0'00 125 250 375
t
125 250 375 500
1.00 p
E
0-00⅛-
#params # params
shared{×106) shared{×106)
0.2
00O
# pa rams	#ParamS	#ParamS	# pa rams
shared{×106) shared(×106) shared(×106) shared{×106)
75 l⅞0 225
t
75 150 225 300
1.0
#params # params
shared{×106) shared(×106)
Figure 7: LBGM as a Plug-and-Play Algorithm. LBGM obtains substantial communication benefits when
implemented on top of existing gradient compression techniques by exploiting the rank-characteristics of the
gradient-space. Top-K and ATOMO are known to achieve state-of-the-art performance of their respective
domains of sparsification and low-rank approximation respectively.
over the system (middle row) across training epochs for δkthreshold = 0.2, ∀k on diferent datasets. The
parameters transferred indicates the communication overhead, leading to a corresponding performance
vs. efficiency tradeoff (bottom row). For each dataset, we observe that LBGM reduces communication
overhead on the order of 107 floating point parameters per worker.Similar results on other datasets
and NN models are deferred to AppendixF.1. We also consider LBGM under device sampling (see
Algorithm3in AppendixD.2) and present the results in AppendixF.5, which are qualitatively similar.
Effect of δkthreshold on Accuracy vs. Communication Savings. In Fig.5, the drops in accuracy for
the corresponding communication savings are small except for on CIFAR-10. The 14% reduction
in accuracy here is a result of the hyperparameter setting δkthreshold = 0.2. As noted in takeaway3in
Sec.3, a decrease in the allowable threshold on the LBP error improves the accuracy; the effect of
threshold value is controlled by changing δkthreshold values in Algorithm1. Thus, we can improve
the accuracy by lowering δkthreshold: for δkthreshold = 0.05, the accuracy drops by 4% only while still
retaining a communication saving of 55%, and for δkthreshold = 0.01, we get a 22% communication
saving for a negligible drop in accuracy (by only 0.01%).In Fig.6, we analyze LBGM under different
δkthreshold values for different datasets. A drop in model performance can be observed as we increase
δkthreshold , which is accompanied by an increase in communication savings. This is consistent with
takeaway5from Sec.3, i.e., while a higher threshold requires less frequent updates of the LBGs, it
reduces the convergence speed. Refer to AppendixF.2for additional results.
LBGM as a Plug-and-Play Algorithm. For the
plug-and-play setup, LBGM follows the same
steps as in Algorithm1, with the slight modifica-
tion that the output of gradient compression tech-
niques, top-K and ATOMO, are used in place
of accumulated gradients g(kt) and LBGs g`k , ∀k.
In Fig.7, we see that LBGM adds on top of exist-
ing communication benefits of both top-K and
ATOMO, on the order of 106 and 105 floating
point parameters shared per worker, respectively
(30 - 70% savings across the datasets).The bot-
tom row shows the accuracy/loss improvements
that can be obtained for the same number of pa-
rameters transferred.While top-K and ATOMO
compress gradients through approximation, they
do not alter the underlying low-rank characteris-
tics of the gradient-space. LBGM exploits this property to obtain substantial communication savings
on top of these algorithms. Refer to AppendixF.3for additional experiments.
Generalizability of LBGM to Distributed Training. LBGM can be applied to more general dis-
tributed gradient computation settings, e.g., multi-core systems. Whileheterogeneous (non-iid)data
distributions are not as much of a consideration in these settings as they are in FL (since data can
---w∕0 LBGM ——w/ LBGM
MNIST
FMNIST
Non-IIDZSignSGD Non-IIDZSignSGD
1.8]
£
go
o
75
50
∣.25
t
00O 25 50 75 1∞
0.75
0.50
0.25
0 00O~25 50 75 ICO
t
40
⅞?20
1.∞
£
%
O
1.25
75
50
% 25 50 75 1∞
t
0.75
0.50
00O	20	40
#bits
shared(×106)
20
0O 25 50 7 5 IOO
t
0.25
00⅛
)	20	40
#bits
shared(×106)
CIFAR-IO	CeIebA
Non-IIDZSignSGD	SignSGD
2.01----------
0.4
0.2
L5∣
L。
。可
0 0O~1>5 250 3 75 500
400τ
0.4
0.2
3∞
2∞
1∞
0O 125 250 375 500

,2.0ι
。咕
ɔ 200	400
#bits
shared(×106)
28]
1刈
IoOl
00O 75 150 225 300
0O 75 1 50 225 300
0.5
200
shared(×106)
Figure 8: Application of LBGM as a plug-and-play al-
gorithm on top of SignSGD in distributed training.
8
Published as a conference paper at ICLR 2022
be transferred/allocated across nodes), there is research interest in minimizing parameter exchange
among nodes to reduce communication latency. SignSGD (Bernstein et al.,2018) is known to
reduce the communication requirements by several order of magnitude by converting floating-point
parameters to sign bit communication. In Fig.8, we apply LBGM as a plug-and-play addition on top
of SignSGD and find that LBGM further reduces the overall bits transferred by SignSGD on the order
of 107 bits( 60 - 80% savings across the datasets). Refer to AppendixF.4for additional experiments.
5	Related Work
NN Overparameterization Analysis. Several prior works on NN overparameterization have focused
on Hessian-based analysis.Sagun et al.(2016;2017) divide the eigenspace of the Hessians into two
parts: bulk and edges, and show that increasing network complexity only affects the bulk component.
Ghorbani et al.(2019) argues that the existence of large isolated eigenvalues in the Hessian eigenspace
is correlated with slow convergence.Gur-Ari et al.(2018) studies the overlap of gradients with
Hessians and shows the Hessian edge space remains invariant during training, and thus that SGD
occurs in low-rank subspaces.Gur-Ari et al.(2018) also suggest that the edge space cardinality is
equal to the number of classification classes, which does not align with our observations in Sec.2. In
contrast to these, our work explores the low-rank property by studying the PCA of the gradient-space
directly.Li et al.(2021), a contemporary of ours, employs the spectral decomposition of the NN
gradient space to improve centralized SGD training time. Our methodology based on Hypothesis
(H2) is more suitable for FL since having the resource-constrained workers/devices execute spectral
decomposition as a component of the training process would add significant computational burden.
The partitioning of the gradient subspace has also been observed in the domain of continual learn-
ing (Chaudhry et al.,2020;Saha et al.,2020). However, the subspace addressed in continual learning
is the one spanned by gradient with respect to data samples for the final model, which is different
than the subspace we consider, i.e., the subspace of gradient updates generated during SGD epochs.
Gradient Compression. Gradient compression techniques can be broadly categorized into (i)
sparsification (Wangni et al.,2018;Sattler et al.,2019), (ii) quantization (Seide et al.,2014;Alistarh
et al.,2017), and (iii) low-rank approximations (Wang et al.,2018;Vogels et al.,2019;Albasyoni et al.,
2020;Haddadpour et al.,2021). Our work falls under the third category, where prior works have aimed
to decompose large gradient matrices as an outer product of smaller matrices to reduce communication
cost. This idea was also proposed in Konecny et al. QO16), one Of hie PiOneering WorkS in FL. WMe
these prior works study the low-rank property in the context of gradient compression during a single
gradient transfer step, our work explores the low rank property of the gradients generated across
successive gradient epochs during FL. Existing techniques for gradient compression can also benefit
from employing LBGM during FL, as we show in our experiments for top-K, ATOMO, and SignSGD.
Model Compression. Model compression techniques have also been proposed to reduce NN
complexity, e.g., model distillation (Ba & Caruana,2014;Hinton et al.,2015), model pruning (LeCun
et al.,1990;Hinton et al.,2015), and parameter clustering (Son et al.,2018;Cho et al.,2021). (Li
et al.,2020) extends the lottery ticket hypothesis to the FL setting.These methods have the potential
to be employed in conjunction with LBGM to reduce the size of the LBGs stored at the server.
FL Communication Efficiency. Other techniques have focused on reducing the aggregation fre-
quency in FL.Hosseinalipour et al.(2020);Lin et al.(2021) use peer-to-peer local network communi-
cation, while SloMo (Wang et al.,2020b) uses momentum to delay the global aggregations.
6	Discussion & Conclusions
In this paper, we explored the effect of overparameterization in NN optimization through the PCA of
the gradient-space, and employed this to optimize the accuracy vs. communication tradeoff in FL.
We proposed the LBGM algorithm, which usesour hypothesis that PGDs can be approximated using a
subset of gradients generated across SGD epochs, and recycles previously generated gradients at the
devices to represent the newly generated gradients. LBGM reduces communication overhead in FL
by several orders of magnitude by replacing the transmission of gradient parameter vectors with a
single scalars from each device. We theoretically characterized the convergence behavior of LBGM
algorithm and experimentally substantiated our claimson several datasets and models. Furthermore,
we showed that LBGM can be extended to further reduce latency of communication in large distributed
training systems by plugging LBGM on top of other gradient compression techniques. More generally,
our work gives a novel insight to designing a class of techniques based on “Look-back Gradients”
that can be used in distributed machine learning systemsto enhance communication savings.
9
Published as a conference paper at ICLR 2022
References
Turing-NLG: A 17-billion-parameter language model by Microsoft. https://www.microsoft.com/en-
us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/, 2020.
Alyazeed Albasyoni, Mher Safaryan, Laurent CondaL and Peter Rich饬rik. Optimal Gradient Compression for
Distributed and Federated Learning. arXiv preprint arXiv:2010.03246, 2020.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-Efficient
SGD via Gradient Quantization and Encoding. In Neural Information Processing Systems (NeurIPS), 2017.
Sheikh Shams Azam, Taejin Kim, Seyyedali Hosseinalipour, Christopher Brinton, Carlee Joe-Wong, and Saurabh
Bagchi. Towards Generalized and Distributed Privacy-preserving Representation Learning. arXiv preprint
arXiv:2010.01792, 2021.
Lei Jimmy Ba and Rich Caruana. Do Deep Nets really need to be Deep? In Neural Information Processing
Systems (NeurIPS), 2014.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. SignSGD: Com-
pressed Optimisation for Non-convex Problems. In International Conference on Machine Learning (ICML),
2018.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.
In Neural Information Processing Systems (NeurIPS), 2020.
Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual Learning in Low-rank
Orthogonal Subspaces. In Neural Information Processing Systems (NeurIPS), 2020.
Minsik Cho, Keivan A Vahid, Saurabh Adya, and Mohammad Rastegari. DKM: Differentiable K-Means
Clustering Layer for Neural Network Compression. arXiv preprint arXiv:2108.12659, 2021.
Corinna Cortes and Vladimir Vapnik. Support-vector Networks. Machine Learning, 20(3):273-297,1995.
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal
Visual Object Classes (VOC) Challenge. International Journal of Computer Vision (IJCV), 88(2):303-338,
2010.
Jonathan Frankle and Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural
Networks. In International Conference on Learning Representations (ICLR), 2019.
Michael P Friedlander and Mark Schmidt. Hybrid Deterministic-stochastic Methods for Data Fitting. SIAM
Journal on Scientific Computing, 34(3):A1380-A1405, 2012.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An Investigation into Neural Net Optimization via
Hessian Eigenvalue Density. In International Conference on Machine Learning (ICML), 2019.
Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient Descent Happens in a Tiny Subspace. arXiv preprint
arXiv:1812.04754, 2018.
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated Learning
with Compression: Unified Analysis and Sharp Guarantees. In International Conference on Artificial
Intelligence and Statistics (AISTATS), 2021.
Song Han, Jeff Pool, John Tran, and William J Dally. Learning both Weights and Connections for Efficient
Neural Networks. In Neural Information Processing Systems (NeurIPS), 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. arXiv preprint
arXiv:1503.02531, 2015.
Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G Brinton, Nicolo Michelusi, Vaneet Aggarwal,
David J Love, and Huaiyu Dai. Multi-stage Hybrid Federated Learning over Large-scale Wireless Fog
Networks. arXiv preprint arXiv:2007.09511, 2020.
10
Published as a conference paper at ICLR 2022
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely Connected Convolutional
Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4700-4708, 2017.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error Feedback fixes SignSGD
and other Gradient Compression Schemes. In International Conference on Machine Learning (ICML), 2019.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and Dave Bacon.
Federated Learning: Strategies for Improving Communication Efficiency. In Neural Information Processing
Systems (NeurIPS), 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning Multiple Layers of Features from Tiny Images. 2009.
Yann LeCun and Corinna Cortes. MNIST Handwritten Digit Database, 2010. URL http://yann.lecun.
com/exdb/mnist/.
Yann LeCun, John S Denker, and Sara A Solla. Optimal Brain Damage. In Neural Information Processing
Systems (NeurIPS), pp. 598-605, 1990.
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. LotteryFL: Personalized
and Communication-efficient Federated Learning with Lottery Ticket Hypothesis on Non-iid Datasets. arXiv
preprint arXiv:2008.03371, 2020.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning Filters for Efficient Convnets.
In International Conference on Learning Representations (ICLR), 2017.
Tao Li, Lei Tan, Qinghua Tao, Yipeng Liu, and Xiaolin Huang. Low Dimensional Landscape Hypothesis is
True: DNNs can be Trained in Tiny Subspaces, 2021.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of FedAvg on
Non-IID Data. In International Conference on Learning Representations (ICLR).
Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G Brinton, and Nicolo
Michelusi. Two Timescale Hybrid Federated Learning with Cooperative D2D Local Model Aggregations.
arXiv preprint arXiv:2103.10481, 2021.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolldr, and
C Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In European Conference on Computer
Vision (ECCV), pp. 740-755, 2014.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. SSD: Single Shot Multibox Detector. In European Conference on Computer Vision (ECCV), pp. 21-37.
Springer, 2016.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv
preprint arXiv:1907.11692, 2019a.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the Value of Network
Pruning. In International Conference on Learning Representations (ICLR), 2019b.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In IEEE
International Conference on Computer Vision (ICCV), December 2015.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts. In International
Conference on Learning Representations (ICLR), 2016.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Neural
Information Processing Systems (NeurIPS). 2019.
Karl Pearson. LIII. On Lines and Planes of Closest Fit to Systems of Points in Space. The London, Edinburgh,
and Dublin Philosophical Magazine and Journal of Science, 2(11):559-572, 1901.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image
Segmentation. In International Conference on Medical Image Computing and Computer-assisted Intervention,
pp. 234-241, 2015.
11
Published as a conference paper at ICLR 2022
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, and Jonathan
Passerat-Palmbach. A Generic Framework for Privacy Preserving Deep Learning. arXiv preprint
arXiv:1811.04017, 2018.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the Hessian in Deep Learning: Singularity and
Beyond. arXiv preprint arXiv:1611.07476, 2016.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical Analysis of the Hessian
of Over-parametrized Neural Networks. arXiv preprint arXiv:1706.04454, 2017.
Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient Projection Memory for Continual Learning. In Interna-
tional Conference on Learning Representations (ICLR), 2020.
Felix Sattler, Simon Wiedemann, Klaus-Robert Muller, and Wojciech Samek. Robust and Communication-
efficient Federated Learning from Non-iid Data. IEEE Transactions on Neural Networks and Learning
Systems, 31(9):3400-3413, 2019.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit Stochastic Gradient Descent and its
Application to Data-parallel Distributed Training of Speech DNNs. In Conference of the International Speech
Communication Association (INTERSPEECH), 2014.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving Deep Learning. In ACM SIGSAC Conference on
Computer and Communications Security (CCS), pp. 1310-1321, 2015.
Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition.
arXiv preprint arXiv:1409.1556, 2014.
Sanghyun Son, Seungjun Nah, and Kyoung Mu Lee. Clustering Convolutional Kernels to Compress Deep
Neural Networks. In European Conference on Computer Vision (ECCV), pp. 216-232, 2018.
Sebastian U Stich. Local SGD Converges Fast and Communicates Little. In International Conference on
Learning Representations (ICLR), 2019.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical Low-rank Gradient Compression
for Distributed Optimization. In Neural Information Processing Systems (NeurIPS), 2019.
Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright, and Dimitris Papailiopoulos.
ATOMO: Communication-efficient Learning via Atomic Sparsification. In Neural Information Processing
Systems (NeurIPS), 2018.
Jianyu Wang and Gauri Joshi. Cooperative SGD: A Unified Framework for the Design and Analysis of
Communication-efficient SGD Algorithms. arXiv preprint arXiv:1808.07576, 2018.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the Objective Inconsistency
Problem in Heterogeneous Federated Optimization. In Neural Information Processing Systems (NeurIPS),
2020a.
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. SlowMo: Improving Communication-
efficient Distributed SGD with Slow Momentum. In International Conference on Learning Representations
(ICLR), 2020b.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient Sparsification for Communication-efficient
Distributed Optimization. In Neural Information Processing Systems (NeurIPS), 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking
Machine Learning Algorithms, 2017.
12
Published as a conference paper at ICLR 2022
Appendix
Table of Contents
A Proof of Theorem 1	14
A.1 Condition on Learning Rate....................................................20
B Proof of Corollary 1	21
C Additional Discussions22
C.1	LBGM Storage Considerations.................................................22
C.2	Hyperparameter Tuning.......................................................22
D Additional Pseudocodes22
D.1	Pseudocode for Preliminary Experiments......................................22
D.2	Pseudocode for Device Sampling Experiments..................................24
E Additional Preliminary Experiments25
E.1	PCA Component Progression...................................................25
E.2	Overlap of Actual and Principal Gradient....................................25
E.3 Similarity among Consecutive Generated Gradients..............................26
F Additional LBGM Experiments27
F.1 LBGM as a Standalone Algorithm................................................27
F.2 Effect of δkthreshold on LBGM..................................................27
F.3 LBGM as a Plug-and-Play Algorithm.............................................27
F.4 Generalizability of LBGM to Distributed Training..............................27
F.5 LBGM under Client Sampling....................................................28
13
Published as a conference paper at ICLR 2022
A Proof of Theorem 1
We start by introducing a lemma, that is used prominently throughout the analysis that follows.
Lemma 1. ForaSeqUenCeofveCtorS {aJN=? such that E [a∕ai-ι, αi-2, ∙∙∙ , aj = 0, ∀i,
E
N II2I N
Xai∣∣ I = XE hkaik2i.
Proof.
N II2I N	N N
XaiIII	I = X E hkaik2i +XXEai>aj.
i=1 I	i=1	i=1 j=1
j6=i
E
Using the law of total expectation, assuming i < j , we get
E [a[aj] = E [a>E Iaj团,…，a1]=0,
which completes the proof.
(L2)
(5)
(6)
Next, we define a few auxiliary variables that would be referenced in the proof later: as defined in
the main text, g(kt) = Pbτ=-01 gk(θ(kt,b)) is the accumulated stochastic gradient at worker k, where
b ranging from 0 to τ - 1 denotes the rounds of local updates. Using Assumption (A2), error in
stochastic gradient approximation g《(θkt,b)) can be defined as e：M = gk(θkt,b)) - ▽「《(θf,b)).
Consequently, We can write the stochastic gradient as gk(θf) = PFk(θkt,b)) + e£,b) where
VFk (θkt,b)) is the true gradient. From Assumption (A2) it follows that
E e(kt,b) = 0 andE
e(kt,b)III2	≤σ2.
(7)
We also introduce the normalized stochastic gradient d(kt) given by
(t)
gk
τ
τ-1	τ-1	τ-1
一 X gk (θkt,b)) = — X VFk (θkt,b)) + - X ekt,b) = hkt) + ekt),
τ	k	τ	k	τ k	kk
(8)
b=0
b=0
b=0
where we define
1 τ-1
Cumulative Average of the true gradient: hkt) = - EVFk(θkt,b)), and
(9)
Cumulative Average of the SGD error: e(kt)
b=0
τ-1
1 X」t，b)
T I^ek
b=0
(10)
Next, we evaluate the first and second moment of normalized SGD error, e(kt) as follows
E [ekt)i= E "- X ekt,b)# = - X E [日产]=0,
b=0	b=0
(11)
E
e(kt)III2
1 τ-1
τ2 X E
b=0
铲 II2] ≤ στ2,
(12)
E HX Tl
where (11) uses Assumption (A2) and (12) uses Lemma1.
Now we can proceed to the proof of Theorem1. We start with the update rule of LBGM, where a
round of global update is given by
KK
θ(t+1) = θ(t) - η X ωkge(kt) = θ(t) - τ η X ωkde(kt),	(13)
k=1	k=1
14
Published as a conference paper at ICLR 2022
where ge(kt) is the approximate gradient shared by worker k with the server and de(kt) is given by
dkt) = gkt)/T = ip"" T = ρkt'edfk	(14)
where d = gk/τ is the normalized stochastic gradient w.r.t. the last LBG shared by worker k. Also,
from the trigonometric relationship shown in Fig.4, we have,
∣∣ρkt),'dk∣∣ = ∣∣dkt) CoSmkt),[∣	(15)
Similar to d(kt) from (8), d`k can be split into the cumulative average of true look-back gradient h`k
and the corresponding cumulative average of SGD error `k, given by
d`k = h`k + `k.	(16)
From Assumption (A1), the global loss function is β smooth, which implies:
F(θ(t+1)) - F(θ㈤)≤ DVF(θ㈤),θ(t+1) - θ㈤E + β ∣∣θ(t+1) - θ(t)∣∣2
Taking expectation over WkIbb, k ∈ {1,2,…,K}, b ∈ {0,1,…，τ 一 1},
E hF(θ (t+1))i - F (θ(t))
(17)
≤-τηE KVF(θ㈤)，XX 3卜•
1---------7-----
Z1
+
τ2η2β E
2
XK 3kde(kt)∣∣∣∣∣2
Z2
(18)
}
|
}
We evaluate Z1 as follows
Z1 = E
VF(θ(t)),XKωk
"*VF(θ(t)),XKωkd(kt)
{	，
Z1,1
E	VF(θ(t)),XKωk	d(kt) - d(kt)
-E
KVF(θ(t)), XX3k
{^^
Z1,2
(19)
E
|
/ 1
}
where Z1,1 is given by,
Z1,1 (=i) E "*VF (θ (t)),XK 3kh(kt)+# +E "*VF (θ(t)), XK 3kW(kt)
2
(=)2∣VF(θ(t))∣∣2 + 1 E	XX3khkt)
k=1
I- 2 E
VF(θ(t))-X ωkh(kt)
k=1
(20)
2
K
K
where (i) follows from (16) and (ii) uses 2 ha, bi = kak2 + kbk2 - ka - bk2 for any two real
vectors a and b. We next upper bound the term Z1,2 (although Z1,2 has a negative sign in (19), Z1
also appears with a negative sign in (18) which allows us to do the upper bound) as follows
(i) 1
≤ —
-4
Z1,2 = E
(21)
where (i) follows from ha, bi ≤ (1/4)kak2 + kbk2 (result of Cauchy-Schwartz and Young’s
inequalities). Substituting (20) and (21) back in (19), we get
15
Published as a conference paper at ICLR 2022
+E
K
X ωk
k=1
-Zi ≤-1 卜F(θ(t))1-1E
+ 1E
2
VF (θ㈤)-Xq 3卜 h“
(22)
We next bound the term Z2 in (18) as follows
2
(23)
where Z2,1 is given by,
Xe 3k hkt) 口 + E
≤) E I X 3khkt)∣ I + X 3kE M『(% E
k=1	k=1
kXK=13k(kt)2I
" ωk h11十三，
(24)
where (i) follows from (8), (ii) uses Jensen’s inequality: k PkK=1 3kak k2 ≤ PkK=1 3k kak k2, s.t.
PkK=1 3k = 1, and (iii) uses (12). Plugging (24) back into (23), we get
2
Z2 ≤ 2E Wi ωk hkt)l I+学
K
+ 2E	∣∣X 3k
∣k=1
(25)
Substituting (22) and (25) back in (18), we get
e [f W+1))] - F (θ(t)) ≤ - τη∣vF (θ(t))∣2-τη e ]∣i 3k hkt)∣
K
2
K
2
+τη e
VF (θ(t))-X 3kh(kt)	+τηE X
k1
+τ2η2βE IllllX 3kh(kt)llll I + τ η2βσ2 +τ2η2
k1
3k
K
k1
-τη∣vF (θ(t))∣2—τη(i—2τηβ)E [∣χq 3khkt)∣ 1+τ2η E
+ τη(1+ τ ηβ)E
K
3k
k=1
k1
2
+ τ η2βσ2.
VF(θ(t))-XK 3kh(kt)∣∣∣∣∣
(26)
2
16
Published as a conference paper at ICLR 2022
Choosing τηβ ≤ 1/2, implies that 一 τ2η (1 一 2τηβ) ≤ 0 and 1 + τηβ ≤ 3/2 < 2, which results in
simplification of (26) as:
E F (θ(t+1)) - F(θ(t))
ητ
≤- 1∣∣vf (θ ㈤)∣∣2+2 E
K
VF(θ(t)) -Xωkh(kt)
k=1
2
K
2
+ ηβσ2
k=1
K
(i)
≤-
2
2
4∣∣VF (B")" +1 X ωkE IlVFk M)) - hk"
2
k=1
K
2
+ 2 X ωk E	∣∣d(kt) - de(kt)
k=1
|
+ηβσ2,
(27)
"{z
Z3
}
where (i) follows from Jensen’s inequality k PkK=1 ωkak k2 ≤ PkK=1 ωk kakk2, s.t. PkK=1 ωk = 1
and θ(kt) = θ(t), ∀k due to local synchronization. Now Z3 can be bounded as follows:
2
Z3 = E
d(kt),d`k
2
THgk∣∣2
T12	gkt), gk)
d`k
d`k
(i=ii) E
∆2,
(28)
where (i) uses (14), (ii) uses LBGM definition from (D1), (iii) uses the fact that d£) = g£/τ, (iv)
uses ha, bi = kakkbk cos(α), and (v) follows from the condition in the theorem. Substituting (28)
back in (27), we get
E F (θ(t+1)) - F(θ(t))
ητ
≤-
K
1∣∣VF(θ(t))∣2 + 1 Xωk E ∣∣VFk(θkt)) -hkt)∣∣2 +2∆2 + ηβσ2,
(29)
where Z4 is given by,
k=1	|
—
—
囱
E
K
2
}
Z4 = E	∣∣∣VFk(θ(kt))-h(kt) ∣∣∣2
-12 E
τ2
τ-1
X VFk(θ(kt,0)) -VFk(θ(kt,b))
b=0
2
τ-1
τ-1
≤ 1 XE	∣∣VFk(θkt,0)) -VFk(θkt,b))∣∣
b=0
2 τ-1
≤ β XE I∣θkt,0)
τ-1
b=0
-θ(kt,b)∣∣∣2
(30)
2
2
17
Published as a conference paper at ICLR 2022
Also, using the local update rule θftb J θf'0) — η PS=0 gk (θf's)), where θf'b) is the model
parameter obtained at the b-th local iteration of the global round t at device k, we get:
Z5 = E
- θ(kt,b)2
b-1
η2E X gk(θ(kt,s))
(i)
b-1
(ii)
s=0
b-1
≤ 2η2E	X VFk(θkt,s))
2I +
s=0
b-1
2η2E	IXekt,s)∣2
(≤ii) 2η2bXE	VFk(θ(kt,s))2	+2η2XE
(kt,s)2
s=0
τ-1
≤ 2η2bXE	VFk(θ(kt,s))
s=0
2 + 2η2σ2b,
(31)
s=0
2
2
where (i) uses Cauchy-Schwartz inequality and (ii) uses Lemma1and Cauchy-Schwartz inequality.
Also note that
τ-1
Xb
b=0
T(T - I)
-2-
(32)
Taking the cumulative sum of both hand sides of Z5 from (31) over all batches, i.e., 1 PT-1, and
using (32), we get:
1 τ-1
I XE
b=0
- θ(kt,b)∣∣∣2
τ-1
≤ σ2η2 (T - 1) + η2(T - 1) XE
b=0 J
∣∣∣VFk(θ(kt,b))∣∣∣2
________ - /
{z
Z6
(33)
Furthermore, term Z6 can be bounded as follows:
Z6=E ∣∣∣VFk(θ(kt,b))∣∣∣2 (≤i) 2E ∣∣∣VFk(θ(kt,b)) -VFk(θ(kt,0))∣∣∣2 +2E ∣∣∣VFk(θ(kt,0))∣∣∣2
(≤ii) 2β2E ∣∣∣θ(kt,b) -θ(kt,0)∣∣∣2 +2E ∣∣∣VFk(θ(kt,0))∣∣∣2 ,	(34)
where (i) uses Cauchy-Schwartz inequality and (ii) uses (A1). Replacing Z6 in (33) using (34), we
get:
1 τ-1
1 X E
b=0 J
∣∣∣θ(kt,0) -θ(kt,b)∣∣∣2
'∙^^^^^^^^^^^{^^^^^^^^^^^^
Z5
≤ η2σ2(T -1) + 2η2β2(T -1)τXE
b=0 J
∣∣∣θ(kt,b) -θ(kt,0)∣∣∣2 +2η2T(T - 1)E
Z5
(35)
}
Note that Z5, which is originally defined in (31), appears both in the left hand side (LHS) and right
hand side (RHS) of the above expression. Rearranging the terms in the above inequality yields:
1 τ-1
1 X E
b=0
∣∣θ(t,o) _ θ(t,b)∣∣21 ≤	η2σ2(T -1)	+	2n2τ(T - 1)
I k k ∣ _| - 1 — 2η2β2τ(τ — 1)	1 — 2η2β2τ(τ — 1)
E ∣∣∣VFk(θ(kt,0))∣∣∣2
(36)
18
Published as a conference paper at ICLR 2022
Defining H = 2η2β2τ (τ - 1), the above inequality can be re-written to evaluate Z4,
Z4
2 τ-1
β XE
b=0
≤	*β2σ2(τ - 1)	+
- 1 — 2η2β2τ (τ — 1)
2η2β 2τ (T — 1) E
1 — 2η2β2τ (τ — 1)
片(τ - 1) + 1⅛ E
卜 Fk (θkt%∣∣2
(37)
Taking a weighted sum from the both hand sides of the above inequality across all the workers and
using (A3), we get:
K
2 X ωkE 卜Fk(θkt))-明『
k=1
≤ 倘eW(，- 1) Xω + H Xω e Γhvf	(t,0))∣∣2]
≤	2(1 — H)之 k + 2(1 - H) ʌ; kEIJlVFk( k)|口
η2β2 σ2(τ — 1)
2(1— H)
K
+2T-Hy X 3卜E [lvFk (θktM
(i) η2β2σ2(τ — 1)
≤	2(1 - H)
+ 2(h⅛E [Bvf(*)『
H Γ2
+ 2(1 - H),
(38)
where (i) follows from (A3) and θ(kt) = θ(t), ∀k since computation occurs at the instance of global
aggregation. Next, plugging (38) back in (29), we get:
E F (θ(t+1)) — F(θ(t))
ητ
≤- 4∣∣vf (θ㈤)
k=1
≤- 4 I∣vF-"I2+η¾⅛)
+ 2∆2 + ηβσ2
l2	1	l
Il +2 X 3k E	∣∣VFk (θkt))—
2∆2 + ηβσ2
HΥ2
+ 2(1 - H)
E	llvF (θ(t))ll
HΓ2
+ 2(1 - H)
≤- 4(1-
2HΥ2
1—H
VF「"I2 + η¾1)
HΓ2
+ 2(1 - H)
+ 2∆2 + ηβσ2 .	(39)
2
K
2
If H ≤ 1十2)丫2 for some constant α > 1, then it follows that Y-H ≤ 1 + 2θfɪ and 2HΥ2 ≤ 1.
Choosing α = 2 we can simplify the above expression as follows:
E F(θ(t+1)) - F(θ(t))
ητ
≤- 1∣∣VF(θ(t))∣∣2 +2∆2 + ηβσ2 + η2β2σ2(τ - 1) (2 + 8Yr) + 2η2β2Γ2τ(τ - 1)(1 +
≤-8H
VF(θ(t))∣∣ +2∆2 + ηβσ2 + ]η2β2σ2(τ - 1) + 5η2β2Γ2T(τ - 1).	(40)
19
Published as a conference paper at ICLR 2022
Rearranging the terms in the above inequality and taking the average across all aggregation rounds
from the both hand sides, yields:
T-1
T-1
T X E 卜F(θ(t,0))
t=0
8 IPT-II E [F(θ㈤)]-F(θ(t+1))i
ητT
8 hF(θ(0)) - F(θ(T))i
+ 16∆2 + 8ηβσ2 + 5η2β2σ2 (τ - 1) + 20η2β2Γ2τ (τ - 1)
ητT
8 [F(θ(O)) - F?i
+ 16∆2 + 8ηβσ2 + 5η2β2σ2 (τ - 1) + 20η2β2Γ2τ (τ - 1)
ητT
+ 16∆2 + 8ηβσ2 + 5η2β2σ2 (τ - 1) + 20η2β2Γ2τ (τ - 1),
(41)
2
where we used the fact that F is bounded below, since Fk-s are presumed to be bounded below, and
F? ≤ F(θ), ∀θ ∈ RM. This completes the proof of Theorem1.
A. 1 Condition on Learning Rate
From the two conditions on the learning rate used in the analysis above, we have
ηβ ≤ ɪ	(42)
2τ
2η2β2τ(T - 1) ≤ 1+14γ2	(43)
We can further tighten the second constraint as,
2η2β2τ(τ - 1) ≤ 2η2β2T2 ≤	1-2	(44)
1+ 4Υ2
Combining the two we have,
ηβ ≤ min ,——I	1	= > .	(45)
2τ,τP2(1 + 4Υ2) ∫
20
Published as a conference paper at ICLR 2022
B Proof of Corollary 1
Using (41) we have:
T-1
T X E 卜F(θ(t,0))
T t=0
≤
8 F(θ(0)) - F?
ητT
+ 6∆2 + 8ηβσ2 + 5η2β2σ2(τ - ) +20η2β2Γ2τ(τ-).
(46)
Next, using the assumption in the corollary statement, we have ∆2 ≤ η. We then can upper bound
the RHS of (46) to get:
T-1
T X E	IlVF(θ(t,0))
T t=0
≤
8 F(θ(0)) - F?
ητT
+ 6η + 8ηβσ2 + 5η2β2σ2 (τ - ) + 20η2β2Γ2τ (τ - ).
(47)
Choosing η
TT, We get,
T-1
T X E	Il VF(θ(t,0))
T t=0
8 [F(θ(0)) - F?]	8βσ2	16	5β2σ2(τ- 1)
√TT	+ √τT + √TT + TT
20β2Γ2τ (T — 1)
+ —TT-一
(48)
We can Write the above expression as,
≤
1 T-1
T X E
t=0
IIIVF (θ(t,0))III2
≤O (%)+0 ( √Tt )+ O (%)+ O ( ；))+ O (三).
(49)
This completes the proof for Corollary1.
21
Published as a conference paper at ICLR 2022
C Additional Discussions
C.1 LBGM Storage Considerations
As discussed in Section3, LBGM requires both server and workers to store the last copy of the
workers’ look-back gradient (LBG). While storing a single LBG (same size as the model) at each
device might be trivial since the space complexity increases by a constant factor (i.e., the space
complexity increases from O (M) to O (2M) = O (M) where M is the model size), storage of
LBGs at the server might require more careful considerations since it scales linearly with the number
of devices (i.e., space complexity increases from O (M) to O (KM) where M is the model size and
K is the number of workers). Thus, the storage requirements can scale beyond memory capabilities
of an aggregation server for a very large scale federated learning systems. We, therefore propose the
following solutions for addressing the storage challenge:
•	Storage Offloading. In a large scale federated learning system, it is realistic to assume network
hierarchyHosseinalipour et al.(2020), e.g., base stations, edge servers, cloud servers, etc. In such
cases the storage burden can be offloaded and distributed across the network hierarchy where the
LBGs of the devices are stored.
•	LBG Compression. If the LBGM is applied on top of existing compression techniques such as
Top-K, ATOMO, etc., the size of LBGs to be stored at the server also gets compressed which
reduces the storage burden. Alternatively, we could use parameter clustering techniques (Son
et al.,2018;Cho et al.,2021) to reduce LBG size at the server.
•	LBG Clustering. In a very large scale federated learning system, say with a billion workers, it’s
unrealistic to assume that all the billion clients have very different LBGs given the low rank
hypothesis (H1) and possible local data similarities across the workers. It should therefore be
possible to cluster the LBGs at the server into a smaller number of centroids and only saving the
centroids of the clusters instead of saving all the LBGs of the devices individually. The centroids
can be broadcast across the devices to update the local version of the LBGs.
C.2 Hyperparameter Tuning
Most of the compression/communication savings baselines operate on a tradeoff between communi-
cation savings and accuracy. For hyperparameter selection, we first optimize the hyperparameters
for the base algorithm such that we achieve the best possible communication saving subject to
constraint that accuracy does not fall off much below the corresponding vanilla federated learning
approach. For example, we optimize the value of K in top-K sparsification by changing K in
orders of 10, i.e. K = 10%, K = 1%, K = 0.1%, etc. and choose the value that gives the best
tradeoff between the final model accuracy and communication savings (this value is generally around
K = 10%). Similarly, for ATOMO we consider rank-1, rank-2, and rank-3 approximations. While
rank-1 approximation gives better communication savings, the corresponding accuracy falls off
sharply. Rank-3 approximation gives only a marginal accuracy benefit over rank-2 approximation but
adds considerably more communication burden. Thus we use rank-2 approximations in ATOMO.
In the plug-and-play evaluations, the LBGM algorithm is applied on top of the base algorithms
once their hyperparameters are tuned as a final step to show the additional benefits we can attain by
exploiting the low-rank characteristic of the gradient subspace. Our chosen hyperparameters can be
found in our code repository: https://github.com/shams-sam/FedOptim.
D Additional Pseudocodes
D. 1 Pseudocode for Preliminary Experiments
In this Appendix, we provide a psuedocode for generating the preliminary experimental results in
Sec.2. The actual implementation of the following function calls used in the pseudocode can be found
in the listed files of our code repository: https://github.com/shams-sam/FedOptim:
•	get_num_PCA_components: implemented in function estimate_optimal_ncomponents, file:
src/common/nb_utils.py of the repository.In summary, we stack the accumulated gradients over
the epochs an perform singular value decomposition, after which we do the standard analysis
22
Published as a conference paper at ICLR 2022
for estimating the number of components explaining a given amount of variance in the datasets.
Specifically, we count the number of singular values that account for the 99% and 95% of the
aggregated singular values.
•	get_PCA_components: implemented in function pca_transform, file: src/common/nb_utils.py
of the repository.In summary, we stack the accumulated gradients over the epochs an perform
singular value decomposition, after which we do the standard analysis for recovering the principal
components explaining a given amount of variance in the datasets. Specifically, we recover the
left singular vectors corresponding the singular values that account for the 99% and 95% of the
aggregated singular values.
•	cosine_similarity: implemented using functions sklearn.preprocessing.normalize and
numpy.dot, such that cosine_similarity(a, b) = normalize(a).dot(normalize(b)) where
a and b are numpy.array,where normalize performs the vector normalization and dot is the
standard vector dot product.
•	plot_1, plot_2, and plot_3: implemented in files src/viz/prelim_1.py, src/viz/prelim_2.py,
and src/viz/prelim_3.py respectively.
Algorithm 2 Pseudocode for Preliminary Experiments in Section2
1:Initialize model parameter θ(0) .
2:Initialize actual_grads = {}	. store gradients for PCA
3:Initialize pca95_store = {}	. store #components accounting for 95% variance
4:Initialize pca99_store = {}	. store #components accounting for 99% variance
5: for t = 0 to T - 1 do
6:	InieiaIi ze g(t - 0.
7:	Setθ(t,0) 一 θ(t).
. training for T epochs
. initialiie gradient accumulator
8:	for b = 0 to B - 1 do	. B minibatches per epoch
9:Sample a minibatch of datapoints	B from dataset D.
10：	ComPUtegG* b)= p』Vf (θ(t,b); d)∕∣B∣.
11:	UPdateParameter: θ(t,b+1) 一 θ(t,b) — η ∙ g(t，b).
12:	ACCgmdiete gradient: g(t) ― g(t) + g(t,b).
13:	end for
14:	Set θ(t+1) 一 θ(t,B).
15:	actual_grads.append(g(t))	. Uppetd UccumulUted grUdiett to store
16:	pca95_store.append(get_num_PCA_components(actual_grads, variance = 0.95))
17:	pca99_store.append(get_num_PCA_components(actual_grads, variance = 0.99))
18: end for
19: plot_1(pca95_store, pca99_store)
. plot of PCA compotett progressiot
20:	principal_grads = get_PCA_components(actual_grads, variance = 0.99)
21:	heatmap = zeros(len(actual_grads), len(principal_grads))
22:	for i = 0 to len(actual_grads) - 1 do
23:	for j = 0 to len(principal_grads) - 1 do
24:	heatmap[i, j] = cosine_similarity(actual_grads[i], principal_grads[j])
25:	end for
26:	end for
27: plot_2(heatmap)
. plot of overlUp of UctuUl Utd pritcipUl grUdietts
28:	heatmap = zeros(len(actual_grads), len(actual_grads))
29:	for i = 0 to len(actual_grads) - 1 do
30:	for j = i to len(actual_grads) - 1 do
31:	heatmap[i, j] = heatmap[j, i] = cosine_similarity(actual_grads[i], actual_grads[j])
32:	end for
33:	end for
34: plot_3(heatmap)
. plot of similUrity Umotg UctuUl grUdietts
23
Published as a conference paper at ICLR 2022
D.2 Pseudocode for Device Sampling Experiments
The pseudocode for LBGM with device sampling is given below. The process is largely similar to
Algorithm1, except modifications in the global aggregation strategy. During global aggregation the
server samples a subset K0 of all the available clients and receives updates from only those clients for
aggregation as shown in line15-17of the Algorithm3.
In terms of the effect of sampling on the unsampled devices, as long as an unsampled device that
joins at a later step of the training have a fairly good look-back gradient (i.e., its newly generated
gradient are close to its look-back gradient (LBG)), there is no need for transmission of the entire
parameter vector. This would often happen in practice unless the device engages in model training
after a very long period of inactivity, in which case it would need to transmit its updated LBG before
engaging in LBGM communication savings.
Algorithm 3 LBGM with Device Sampling
Notation:
θ(t): global model parameter at global aggregation round t.
θ(kt,b): model parameter at worker k, at global aggregation round t and local update b.
g (kt) : accumulated gradient at worker k at global aggregation round t.
g`k : last full gradient transmitted to server, termed look-back gradient (LBG).
αkt),': phase between the accumulated gradient gkt) and LBG gk, termed look-back phase (LBP).
Training at worker k:
1: UPpaatelcaalaaaameters: θkt,0) 一 θ⑴，and initialize gradient accumulator: gkt) — 0.
2: for b = 0 to (τ -1) do
3:	SemPleaminibotCh OadataPOints Bk from Dk and compute gkt,b) = Pd∈βfc Vfk(θkt,b); d)∕∣Bk|.
4:	Update local parameters: θkt,b+1) — θkt,b) — η ∙ gkt,b), and accumulate gradient: g? — g? + gkt,b)∙
5: end for
6: CalculatetheLBPerror: sin2(akt),') = 1 - (hg',gki/(kgkt) k X kgkk))
7: if sin2 3 4 5 6 7 8 9 10 11 12(akt),') ≤ δkhreshold then	. checking the LLP error
8:	SCndeCaE LBCtOtheeerVer: μkt) ― Pkt耙=(gkt),gki/kgkk2.
9: else	. updating the LBG
10:	SendaCtUaI gtadienttothe SerVer: μkt — gkt.
11:	UPdate WOCker-CfPyofLBG: gkk — gkt.
12: end if
Global update at the aggregation server:
13:
14:
15:
16:
17:
nitialize global parameter θ(0) and broadcast it across workers.
for t = 0 to (T — 1) do
ample set of indices	K0, a random subset from the pool of deeices {1, 2, ..., K}.
eceiee updates from workers	{μ(kt) }k∈K0 .
UPdategIPaaIPeremeters: θ(t+1) 一 θ(t) —号 Pk∈κo ωk [sk ∙ μktt ∙ gk + (1 — Sk) ∙ μk[,
where sk is an indicator function gieen by, sk
1, if μ(kt) is a scalar
0, otherwise, i.e., if μ(kt) is a eector
18:
19:
UPdateheCVer-CfPyofk)Ok-backgraPientS(LBGs): gk — (1 — sk)μkt + (Sk)gk, ∀k ∈ K0.
end for
24
Published as a conference paper at ICLR 2022
E Additional Preliminary Experiments
We next present the additional experiments performed to test hypotheses, (H1) & (H2). As mentioned
in Section2, we study the rank-characteristics of centralized training using SGD on multiple datasets:
FMNIST, MNIST, CIFAR-10, CelebA, COCO, and PascalVOC, and model architectures: CNN, FCN,
Resnet18, VGG19, and U-Net. SectionE.1presents experiments complementary to the one presented
in Fig.1, while SectionE.2&E.3present experiments complementary to those presented in Fig.2, &
3, respectively. Please follow the hyperlinks for the ease of navigating through the figures.
E.1	PCA Component Progression
Together with Fig.1, Figs.10-13show that both N99-PCA and N95-PCA are significantly lower than
that the total number of gradients calculated during model training irrespective of model/dataset/learn-
ing task for multiple datasets and models which all agree with (H1). Specifically, the principal
gradients (i.e., red and blue lines in the top row of the plots) are substantially lower (often as low as
10% of number of epochs, i.e., gradients generated) in these experiments. Refer below for the details
of the figures.
1.	Fig.9repeats the experiment conducted in Fig.1on CIFAR-100 using FCN, CNN, Resnet18, &
VGG19.
2.	Fig.10repeats the experiment conducted in Fig.1on MNIST using FCN, CNN, Resnet18, &
VGG19.
3.	Fig.11repeats the experiment conducted in Fig.1on FMNIST using FCN, CNN, Resnet18, &
VGG19.
4.	Fig.12repeats the experiment conducted in Fig.1on CIFAR-10, FMNIST, and MNIST using
SVM, suggesting that we can use LBGM for classic classifiers that are not necessary neural
networks.
5.	Fig.13repeats the experiment conducted in Fig.1on COCO, and PascalVOC using U-Net.
E.2 Overlap of Actual and Principal Gradient
Next, we perform experiments summarized in Fig.14-35to further validate our observation in Fig.2:
(i) cosine similarity of actual gradients with principal gradients varies gradually over time, and (ii)
actual gradients have a high cosine similarity with one or more of the principal gradients. Refer
below for the details of the figures. Each subplot is marked with #L, the layer number of the neural
network, and #elem, the number of elements in each layer.
In the plots with dense heatmaps (a large number of gradients in along x and y axis) for larger models
such as Fig.14, it is harder to observe the overlap among the actual gradient and the PCA gradients.
However, we can still notice the number of prinicipal components (along y axis) is substantially
lower than the total number of epochs gradients (along y axis). A better picture of gradient overlap
with other gradients can still be seen in corresponding inter-gradient overlap plot in Fig.36, which
is consistent with the corresponding PCA progression shown in Fig.1. Note that the lesser number
of prinicipal gradient directions (e.g., CNN in Fig.1) implies a higher overlap among the generated
gradients (e.g., CNN in Fig.3), while a larger number of PGDs (e.g., VGG19 in Fig.1) implies a
lower overlap among generated gradients (e.g., VGG19 in Fig.40).
1	.Fig.14repeats the experiment conducted in Fig.2on CelebA using VGG19.
2	.Fig.15repeats the experiment conducted in Fig.2on CelebA using Resnet18.
3	.Fig.16repeats the experiment conducted in Fig.2on CelebA using FCN.
4	.Fig.17repeats the experiment conducted in Fig.2on CelebA using CNN.
5	.Fig.18repeats the experiment conducted in Fig.2on CIFAR-10 using VGG19.
6	.Fig.19repeats the experiment conducted in Fig.2on CIFAR-10 using Resnet18.
7	.Fig.20repeats the experiment conducted in Fig.2on CIFAR-10 using FCN.
8	.Fig.21repeats the experiment conducted in Fig.2on CIFAR-10 using CNN.
25
Published as a conference paper at ICLR 2022
9	.Fig.22repeats the experiment conducted in Fig.2on CIFAR-100 using VGG19.
10	.Fig.23repeats the experiment conducted in Fig.2on CIFAR-100 using Resnet18.
11	.Fig.24repeats the experiment conducted in Fig.2on CIFAR-100 using FCN.
12	.Fig.25repeats the experiment conducted in Fig.2on CIFAR-100 using CNN.
13	.Fig.26repeats the experiment conducted in Fig.2on FMNIST using VGG19.
14	.Fig.27repeats the experiment conducted in Fig.2on FMNIST using Resnet18.
15	.Fig.28repeats the experiment conducted in Fig.2on FMNIST using FCN.
16	.Fig.29repeats the experiment conducted in Fig.2on FMNIST using CNN.
17	.Fig.30repeats the experiment conducted in Fig.2on MNIST using VGG19.
18	.Fig.31repeats the experiment conducted in Fig.2on MNIST using Resnet18.
19	.Fig.32repeats the experiment conducted in Fig.2on MNIST using FCN.
20	.Fig.33repeats the experiment conducted in Fig.2on MNIST using CNN.
21	.Fig.34repeats the experiment conducted in Fig.2on PascalVOC using U-Net.
22	.Fig.35repeats the experiment conducted in Fig.2on COCO using U-Net.
E.3 Similarity among Consecutive Generated Gradients
Furthermore, we perform experiments summarized in Fig.36-57. Together with Fig.3, these
experiments show that there is a significant overlap of consecutive gradients generated during SGD
iterations, which further substantiates (H2) and bolsters our main idea that “gradients transmitted in
FL can be recycled/reused to represent the gradients generated in the subsequent iterations”. Refer
below for the details of the figures.
1	.Fig.36repeats the experiment conducted in Fig.3on CelebA using VGG19.
2	.Fig.37repeats the experiment conducted in Fig.3on CelebA using Resnet18.
3	.Fig.38repeats the experiment conducted in Fig.3on CelebA using FCN.
4	.Fig.39repeats the experiment conducted in Fig.3on CelebA using CNN.
5	.Fig.40repeats the experiment conducted in Fig.3on CIFAR-10 using VGG19.
6	.Fig.41repeats the experiment conducted in Fig.3on CIFAR-10 using Resnet18.
7	.Fig.42repeats the experiment conducted in Fig.3on CIFAR-10 using FCN.
8	.Fig.43repeats the experiment conducted in Fig.3on CIFAR-10 using CNN.
9	.Fig.44repeats the experiment conducted in Fig.3on CIFAR-100 using VGG19.
10	.Fig.45repeats the experiment conducted in Fig.3on CIFAR-100 using Resnet18.
11	.Fig.46repeats the experiment conducted in Fig.3on CIFAR-100 using FCN.
12	.Fig.47repeats the experiment conducted in Fig.3on CIFAR-100 using CNN.
13	.Fig.48repeats the experiment conducted in Fig.3on FMNIST using VGG19.
14	.Fig.49repeats the experiment conducted in Fig.3on FMNIST using Resnet18.
15	.Fig.50repeats the experiment conducted in Fig.3on FMNIST using FCN.
16	.Fig.51repeats the experiment conducted in Fig.3on FMNIST using CNN.
17	.Fig.52repeats the experiment conducted in Fig.3on MNIST using VGG19.
18	.Fig.53repeats the experiment conducted in Fig.3on MNIST using Resnet18.
19	.Fig.54repeats the experiment conducted in Fig.3on MNIST using FCN.
20	.Fig.55repeats the experiment conducted in Fig.3on MNIST using CNN.
21	.Fig.56repeats the experiment conducted in Fig.3on PascalVOC using U-Net.
22	.Fig.57repeats the experiment conducted in Fig.3on COCO using U-Net.
26
Published as a conference paper at ICLR 2022
F	Additional LBGM Experiments
In this section, we present complimentary experiments to the properties studies in Section4of the
main text. In particular, we show that our observations hold for datasets: CIFAR-10, CIFAR-100,
CelebA, FMNIST, and MNIST. We also present results when using shallower models FCN or deeper
models Resnet18 different from CNNs. SectionF.1gives further evidence of utility of LBGM as
a standalone solution. SectionF.2lists figures that summarize the effect of changing δkthreshold on
model performance for mentioned datasets and models. In SectionF.3we list figures that summarize
additional experiments to support the observations made in Fig.7and SectionF.4lists figures for
additional experiments corresponding to the observations made in Fig.8. Finally, SectionF.5presents
results on LBGM algorithm corresponding to the case wherein 50% of clients are randomly sampled
during global aggregation. Since the two layer FCN considered is a simple classifier, it does not
perform well on complex datasets such as CIFAR-10, CIFAR-100, and CelebA. Thus, the respective
results are omitted for FCN on these complicated datasets and the performance of FCN is only
studied for MNIST and FMNIST datasets. Similarly, the 4-layer CNN architecture does not perform
well on CIFAR-100 dataset and hence the corresponding results are ommited. We also present results
using U-Net architecture for semantic segmentation on PascalVOC dataset.
F.1 LBGM as a S tandalone Algorithm.
1.	Fig.58shows the result of repeating the experiment conducted in Fig.5(CNNs on non-iid data
distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST, and
U-Net for segmentation for dataset PascalVOC.
2.	Fig.59shows the result of repeating the experiment conducted in Fig.5(CNNs on non-iid data
distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
3.	Fig.60shows the result of repeating the experiment conducted in Fig.5(CNNs on non-iid
data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10,
CIFAR-100, FMNIST, and MNIST using a setup similar to that ofWang et al.(2018).
F.2 EFFECT OF δkthreshold ON LBGM.
1.	Fig.61shows the result of repeating the experiment conducted in Fig.6(CNNs on non-iid data
distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST, and
U-Net for segmentation for dataset PascalVOC.
2.	Fig.62shows the result of repeating the experiment conducted in Fig.6(CNNs on non-iid data
distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
3.	Fig.63shows the result of repeating the experiment conducted in Fig.6(CNNs on non-iid
data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10 and
CIFAR-100 using a setup similar to that ofWang et al.(2018).
F.3 LBGM as a Plug-and-Play Algorithm.
1.	Fig.64shows the result of repeating the experiment conducted in Fig.7(CNNs on non-iid data
distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST.
2.	Fig.65shows the result of repeating the experiment conducted in Fig.7(CNNs on non-iid data
distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
3.	Fig.66shows the result of repeating the experiment conducted in Fig.7(CNNs on non-iid
data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10 and
CIFAR-100 using a setup similar to that ofWang et al.(2018).
F.4 Generalizability of LBGM to Distributed Training.
1.	Fig.67shows the result of repeating the experiment conducted in Fig.8(CNNs on non-iid data
distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST.
2.	Fig.68shows the result of repeating the experiment conducted in Fig.8(CNNs on non-iid data
distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
27
Published as a conference paper at ICLR 2022
3.	Fig.69shows the result of repeating the experiment conducted in Fig.8(CNNs on non-iid
data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10 and
CIFAR-100 using a setup similar to that ofWang et al.(2018).
F.5 LBGM under Client Sampling.
We present results with LBGM under client sampling in this subsection. The results are qualitatively
similar to those presented in Sec.4under “ LBGM as Standalone Algorithm”. For example, our
results on the MNIST dataset for 50% client participation shows a 35% and 55% improvement in
communication efficiency for only 0.2% and 4% drop in accuracy for the corresponding i.i.d and
non-i.i.d cases (see column 1 of Fig.71&70respectively).
1.	Fig.70shows the result of repeating the experiment conducted in Fig.5(CNNs on non-iid data
distribution) under 50% client sampling using CNNs for non-iid data distribution for datasets
CIFAR-10, FMNIST, and MNIST, and regression for dataset CelebA.
2.	Fig.71shows the result of repeating the experiment conducted in Fig.5(CNNs on non-iid
data distribution) under 50% client sampling using CNNs for iid data distribution for datasets
CIFAR-10, FMNIST, and MNIST.
CIFAR-100
CNN
CIFAR-100	CIFAR-100	CIFAR-100
Figure 9: PCA Components Progression. Repeat of Fig.1on CIFAR-100 dataset.
MNIST
FCN
"°°
C
①75
Q. 50
O 25
⅛ 0.
100
75
50
25
MNIST	MNIST	MNIST
epoch	epoch	epoch
Figure 10: PCA Components Progression. Repeat of Fig.1on MNIST dataset.
28
Published as a conference paper at ICLR 2022
FMNIST
FMNIST
ResNet 18
200
FMNIST
CNN
N99-PCA
150
100
FMNIST
VGG19
00	75 150 225 300
5 Q 5
7 5 2
Ooo
>UE⊃uuro
5 0 5
,∙,∙,∙
50
75 150 225 300
O 75 150 225 300 " 0	75 150 225 300 " 0	75 150 225 300 " 0	75 150 225 300
epoch	epoch	epoch	epoch
Figure 11: PCA Components Progression. Repeat of Fig.1on FMNIST dataset.
epoch
epoch
epoch
Figure 12: PCA Components Progression. Repeat of Fig.1on CIFAR-10, F-MNIST, and MNIST datasets but
using squared SVM classifier.
COCO
Segmentation
O O
4 2
50
5
2
75
epoch
PascaIVOC
Segmentation
Figure 13:	PCA Components Progression. Repeat of Fig.1on COCO, and PascalVOC datasets but using
U-Net classifier.
29
Published as a conference paper at ICLR 2022
#elem: 512
CeIebA, L#4
#elem: 64
VIIaIa
O 25 50 75 IOO O 25 50 75 IOO O 25 50 75 IOO
epoch gradients epoch gradients epoch gradients
Figure 14:	PCA Components Overlap with Gradient. Repeat of Fig.2on VGG19 trained on CelebA dataset.
30
Published as a conference paper at ICLR 2022
CeIebA, L#3
#elem: 64
CeIebA, L#5
#elem: 64
CeIebA, L#6
#elem: 64
CeIebA, L#4
#elem: 36864
CeIebAr L#7
#elem: 36864
EUd S=U ①-pe」6 eud EUd eud SlUBUd(n4u 9-pe」6 EUd
SlUqpe-6 Bud S=U①-pe」6 eud EUBud
#e em: 64
#e em: 64
#elem: 64
#elem: 128
#elem: 128
#e em: 128
#e em: 128
#e em: 128
#elem: 128
#elem: 128
#e em: 256
#e em: 256
Felem: 256
#elem: 256
Felem: 256
#e em: 256
#elem: 256
#elem: 256
#elem: 512
#elem: 512
#elem: 512
#elem: 512
em: 512
em: 512
5120
epoch gradients
epoch gradients
epoch gradients
epoch gradients
epoch gradients
epoch gradients
Figure 15: PCA Components Overlap with Gradient. Repeat of Fig.2on ResNet18 trained on CelebA dataset.
CeIebA, L#2
#elem: 128
#elem: 128
#elem: 256
#elem: 512
#elem: 256
31

,]ose]ep vgəɪəɔ uo pəureɪi NND uo Z 母IH ^ιu∂ipviQ 中?伙 dvy^o siuəuodiuoj γjj :ʌɪ əjustj
s4U9∣pejb IPoda
ooτ ςk
0 0
ZO
VO
S4uaιpejb IPoda
OOT GI
OT :山叫3#
8#1
OOOq ：山叫3#
£#1
s4Uθ∣pejβ IPod3
ooɪ WL OS ςz o
S4U9∣pejb IPod3
OOT SL OG SZ 0
g「ad_enGr
OoG ：山叫3#
9#1
OOoq乙9 :山叫a#
S#1
OOT GL OG 52	0
0乙：UJ9|9#
乙 #1'VqΘ∣Θ3
ooςτ ：uig|9#
I#n 4∀qθlθ□
PCa g「ad一①nts
3 2 Io
,]ose]ep VQ0P□ uo PoUIU耳 N□H uo Z ?IH Jo ]eodoX ∙ιu∂ipviQ ”具恢 dvy^o siuəuodmoj γjj :ɑɪ QJ∏υTj
s4uəipejb qz>ods
00
VO
910
810
OT
S4Uθ∣pejβ 4ɔodə
OOT Gl OS 9 乙	0
PCa g「ad不nts
2 IO
OT :山叫3#
7#1 vnələɔ
OZLOE :山叫3#
τ≠-∣ vnəiəɔ
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
££
,【os引叩 0T-HVII□ Uo POUle4 6TOOA Uo C '⅛ jo WOdoX ∙ιu∂ipmQ 甲?m dvpə^o siuəuodmoj yjj :gɪ əjn^ɪj
ooz osɪ OOl os o
S4∪9∣pej6 IPodm
ZTS :山叫Q#
£9#1
OOZ OST OOl OS
乙1G :山叫3#
四#1
OOf 051 OOT 05
s4U9∣pejβ IPOde
ooz oςτ ooτ OG o
s)U9∣pej6 IPoda
OOZ osτ ooi os
003 * 常 °S
ooɛ 0^W0ζ
96乙60£乙：山3|3#
T9#1
。。己 OSt 00T 05
；；
器.
乙IG :山叫3#
96乙6G£乙:
乙IG :山叫3#
ɔθ? 05
8乙1:luəiə*
;;
尸
己Ig :山叫a#
工£#1
DOC OST OOT OS
乙IG :LUala#
OOZ OSTQ就 OS
κ史
91 d
ZT不■
o 2.
V2868S :山叫e# JZ
6 乙 #1	Ti
OOZ OST ooi os o
⅛f
L
2iς :ma|a# °
oo? osτθ⅛o?-' os o
「
∣T
8乙£££ ：山叩井 ɔ
6#1
002 0¾τ OOT OG
≡τs 3可己#
OOZ OGl OOT OS O
Zt
9E
。；-
VZ
81
91
谓
腔
3追
/φ'
乙IG ：uj9|e# 0 7
S4∪∂∣pej6 IPode
oo? oςr ooτ os
乙1G :山叫3#
£。#1
OOJ OST OOT 05	0
VZ
:「
21
8t796Z.TI :iɪɪə
££#"1
OOf OSt 00T 05
9G乙：山叫3#
9£#-|
oo? osτ ooτ CK
9G乙：山叫3#
61#-1
oo? osτ ooτ OS
8乙1：山叫3#
乙1#~1
002 Ogl ooτ OS
：l r
加
。989£ :山叫m# 0
ς*^l lOI-HVdID
S4uθ∣pejβ IPodm
96乙60£乙 Wa2#
£0#1
OOZ OST OOT OS 0
乙IG :山己|3#
乙S#1
OoZ OSl OOl OS
OH :山叫a#
G9#1
ooz oςτ ooτ os
乙Iq :山己1己#
179#1
002 OGT 001 05
乙1S :山叫a#
8G#1
ooɛ oςτ ooτ os
£G#1
002 OGl 001 OS
乙IG :山
OOZ。019辘1os
sτ
STS :山己|己#
05 #1
OOZ OGI 001 05

96乙6GE乙：山3|3#
S17≠-I
ɔθ? OSl 001 05	0
9，乙业叫。#
乙£#1
OoZ OST 00T OS
9S2 :可叫3#
IE#1
OOZ OSl 001 OS
0
94
n
cΛ-
Il
ɪɪ
VZ868S ：山叫。#
G乙#1
oo? osi ooi Og o
952 :山叩#
力乙#1
002 OSl 001 OG 0
9G乙：山叫a#
0£#1
ooɛ OST OOT OS
ggɛ ：uiep#
£2#-l
002 OSl OOT OG
:山叫3#
乙乙#^l
ooz osτ ooτ 05
99口:川叫a#
81 #1
002 OST OOI OS
8乙1：LU叫3#
τι*^ι
OOZ OGi ooτ (K
。9 :luəiə^
17#1 iOT-HVJD
乙16廿6乙：山叫曰#
Ll#l
αo? osi ooτ os o
831 :山叫a#
01#1
002 OSl 001 05

V9 :山叫己#
E#i ,oι-avdo
8乙■[:川叫a#
9T#~I
002 OSl OOT Og
179 :以ja|a#
z#i ,oτ-yv=∣D
P9 :山叫a#
8#1
002 OST 001 05


8乙£工：山叩#
l#~\ jOT-HVdD
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
P£
1。S叩 P
Ol-HVHK) UO pəurei: 8lJθZS刑 uo C ?旧 JO 卯式利∙ιuaιpvΛQ ψι^ dvμaxQ siuəuodiuoj yjj :ðɪ əjngɪj[
siuəipejo IPOd3
OOZ OST OOT OS
s4U9∣pejb LPode
OOZ oςτ ooτ OG
S4∪9∣pejb IPode
OOf OSl OOT 05
S4∪aιpej6 q□oda
ooz oςι ooτ oς
s4uəipejo IPoda
00∑ 051 OOl os o
S4U9!pejo ʧɔodə
002 OST OOt Og o
8t796∆τi ：山3国#
9f#1
OOZ OST OOI OS 0
翳
Ilt-
96W6GE乙:山叫
(W OqT ooτ oς
9弓乙；LUal己#
6£#1
(mz nςτ ooτ ∏ς
UdLS ：3呐#
19#1
ɔo? oςτ OOi oς
9bdb9" UFlT#
8S#1
ɔo? Ogl OOi oς
952 ：UJ9|9#
ooz Ogi ooτ 。。
6Lb W叩#
6G#1
ooe oςι ooτ oς
8ZI ：UJ9|B#
ɔor。不倍Lg
乙IG :山己2#
∈c⅛η
OoZ oςτ ooɪ ∩ς
CAOIEI :小叫己#
：小叩喜
09 #1
OOz OqT ooτ Og
96Z65EZ ；UJal3#
6。#1
OOZ OSI OOT 05 Q
UL ：川呐拜
乙9#1
co? oςτ ooτ oς
乙IG :小叫己#
VS#1
OOZ O<l OOT Oq
6t?
Ztr
g£
82
杵
ZTS :山叫m#
9G#1
ooz o⅛τ nnf
95δ ；小叩#
117#1
nnz ∏ςτ nor nς
95
M-
Q∣
此
9Γ
V乙8689 W叩#
0。#1
n□z oςτ onτ Qq π
乙IG :山己2#
IG#1
ooɛ Ogl oot OS o
t7989E :山叩#
iOT-WVJD
V989E :山叩#
L#1 'OT-HVJD
步9 :山叫a#
9⅛^I /oι-yvjo
V9 :山叫a#
,0l-HVJD
t?9 :uj己2#
,0l-yVJD
t79 ：山叫己#
乙#^ι foτ-yvdiD
^989E ：山己|己#
£1#1
nnz ∩ςτ □□ι
8乙1：山叫a#
=OZ ＞需 g
9WZ.VI :山叫己#
。"回唧OS
82
IZ
F
9切£林；山叫己#0
61#1
ooz Oql ooτ oς
91
0
BCI :ωa∣θ≠
00? osτ Obτ
ZP
9E
OE
Pi
8T
31
9S3 :山叫3# °
乙射#~1
OOZ OgI ooτ
Z16。6乙：山己目#
IE≠^I
OOZ osτ ooτ os o
『9
95
Cj 7
*
VZ
∣τ
PCa grad-ents
2R9电 Ra2 8 40
嚣
匿
TZ
H
zτς叫叫3# 0
8V≠-I
OOZ OGT OOI OS 0
9；
6b
gfr
Il
τε
y
乙TG ：W叫3# °
oo≡ OSi ooτ os o
t7Z868ζ ：山叫3#
952 :山叫s#
9£#-1
OOZ OST OOl OS
81工：山己|己#
IZ#1
ooz OqT ∩∩ι Oq
1z9 =UJθlθ≠
。1#1
ooz oqτ ππι
的
9G乙：IU3目#
Gb#1
ooz osi ooτ oς
嚣
£*#1
ooz osτ ooɪ
9文：山叫a#
9£#1
OOZ Oql ooτ os o
。乙868G U叫己#
VE#1
002 OST OOl 0⅞	0
8CL 3叩#
0乙#1
coz ∩ςτ not Oq n
乱
Ii
Il
9G乙：LU叫a#
002 OST OOl OS
90乙：山叫a#
乙£#1
QoZ OST OOI (K
9S2 ；小引3#
8E#1
Onz OqI ooτ
89Z3E :山叫3#
2£#1
nnz OGI onɪ nς o
PCa grad-enES PCa grachd PCa grad不nfs PCa grad一ents
J∙Hg 4n Qh ” > frvεεzIT StVETUI Svfr,Ezzt
zfemyσ“ U 8Z9O V839o 98O7~ V980 96TVS8I⅛tzo
95
8fr
Ot
9SfrZ.tI ：UJ9|9#
8乙工；lUal己#
81 #1
OoZ OqT ∩∩τ ∩ς
8乙【：WSI3#
8CT ；山己|己#
£1≠1
007 Oql ooɪ Oq
t79 ®己@#
21#1
nnr OqI onτ
179 ：山己目井
ll#1
(mz ∏ςτ nnɪ (Iq
fr989E ：山叩#
01#1
nnz nqι □oι
8乙I ：山叫2#
OE#^I
002 osτ ooτ os
8乙£E£ :山己|己#
91 #1
nnz Oql ∩∩τ ∩ς (
在9 :山己回#
6#1
πnτ: OqI onτ
8乙I ：山叫9#
6乙#1
oo2 osτ ooτ os
2618 ：UJ9|9#
OOZ/赫%
V9 :山叫3#
，1#1
OOZ Ost ooτ Oq
。9 ：山叫己#
8#1
ooɛ OSi ooτ as o
80fr6 ：山叫3#
T#-l iOT-HVdD
PCa grad不nts PCa gradients PCa gradenfs PCa gradients
/-9→⅛b-εzT εεzεττ ££ SZTI √⅛τ≡ 5 J O i >
Zεkr□9z96o C-OC-OGOGO SO5OC-O5O Kfis-S.K a T C
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
££
IoSe弗P 01-HVHlɔ uo pəʊreɪl NND uo (领H -ιu∂ipviQ 中?m dvμ∂^Q siuəuodmoj γjj ：[( am既更
S4UΘ∣pej6 IPOd3
s】U3!pe」6 LPoda	SiUa牌」6 IPodS
OOZ Odl OOT OG 0 θθɛ OST OOT OG 0
OT :山叫3#	OOOG :小叫3#
002 OGl OOT Oq 0
OOG ：UJ叫3#
9#1
00? OST OOT OG 0
0乙：UJ9|9#
iOT-WdD
S4U9∣pejβ q□odθ
OOZ OST 001 OG 0
OOOq乙9 :山叫a#
S#1
co? OgT ooτ Og o
-o
OoG工：ui3H#
,oι-y∀d∣D
PCa g「ad 不 nσPCa gradgts
']ose]ep 0[-HVII□ Uo pəureɪi NɔH uo C 韦IH Jo ]eodoX ∙ιu∂ipviQ 中叫 dvμ∂^Q siuəuodmoj γjj ：Q7 əjustj
siuəipejo IPodə
s；uəipejb q□odθ
00 乙 OGT 001	09
OSAOE :33|3#
I#1 zOT-WdD
PCa gradients
T X
TCO68Δ9fr*ε2T
OOo9 0乙 O890 NJo
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
9£
1。S叩 P
00IWHI□ u。pəurej^ 6TOOA uo C J。邨式利-iuəipmo ψι^ d叩a然）siuəuodiuoj γjj ：77 əjngɪj[
s4U9∣pejD IPod3
OOC OST ooτ 05 0
s4U9∣pejβ IPod3
SlU叫 pe」b IPOd3
OOZ OgT ooi os o
zτs ：山令国#
88
为
∣fr
；；'
n
ZTS ：山叩#
£15 ：山叫3#
9V#~I
ZTS ：山叫a#
9SC ：iU己
OOZ OSI OOT OG 0
V9 :3叫a#
9#1 ,ooι-avj∣□
s⅛∪9∣pejb qɔodə
oor oςτ OOT 05
ɪ
OOl :山司a#
00aIq :山己|己#
乙B ：UJ9|9#
OOZ。丽（S
OOC OST
CG ：IU9P#
乙G#1
ooe OSi oot oς
96乙6G£乙：山3|3
Gt7#1
ooz: osɪ ooi os ∣
oo≈，牌（K
952 ：山川3#
OOE OST OOI OS 0
952 ：W3|3#
。乙#1
OOZ OgI OOT OG 0
乙16176乙：山叫3#
szτ ：iU叫3#
01#1
ooc osτ OOi os o
V9 :3叫a#
£#i 1ooτ-yvj∣3
乙IS :山叫a#
8§#1
doc Ost σoτ Og
zχς ：W9R#
IG#~I
do? oςτ ooτ Og
乙IG M叫3#
班#1
ooε osτ ooɪ Og
BiyrPVEZTL
8/5 SfrEZtO
9G乙：山刊小#
DOZ OSI OOT 。占
9G乙：山己[己#
0£#1
DOf O5T OOT 05
8CT ：山叫己井
91 #1
szz.ɛz. ：LU9|9#
^#1 1OOT-MVJD
δτs :山硒3#
OOC OSt OOI OS 0
zτs ：ujsib#
OOC On OOI OS 0
乙1G二山叩拜
乙IG ：UJ9|9#
9£#1
OOE 啊 OOT OS 0
frZ8685 ：UJ9|9#
61#1
OOE OST OOI OS P
952 ：IU8|3#
—
0。。OOI QG q
hr EeCZZTT
Oraoc-Osorao
WFXtZTT
8z-yo⅛-β7xyo
gra≡-enGrPCa
tl⅛4⅛
TSOMLfrZO
radlmnts PCa gradients PCa grad不nts
乙IG ：wm|3#
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
L£
1。S叩 P
OOT-HViID u。pəurei: 8ΠθNsθH uθ C '⅛ Je)此式利∙ιuaιpvΛQ 凝以M dvμ∂AQ siuəuodiuoj yjj : £( əin
ooc OqT OOT Og o
OOI ：川3国#
9S
国，
：＞
徒
91
O
0081I ：W叫3#
OOZ / 虢%
∣z
⅛T
ZE :山叫a#
9Z#1
Oql ooi o⅞
■磋
9IZ6 :山叫a# °
61#1
OOZ OSI OOI Og 01
91 : UJ 叫 a#
。。1 /康发
9T ；山叫s#
9#1 iOOT-WdD
L#1 ,ooτ-w=∣D
iOOT-WdD
ZlT
8ZT ：Luap#
劲#1
OOf OST OOT 0≡ C
^££7?n
freð SQSOSO
V0E2 ：山叫a#
乙£ ：UJ3|a#
-CZTTT
-bl8tnc
oβΔθsEΓT
M857⅛69En
1210 8 6 4 2 0
9τ ：iuap#
'OOT-WdD 2#1 ,ooτ-wj□
φmt'衣 E 乙1 9Ln⅛* LUKJl-- 6θ∆9⅛bvt
t-9-tf Ozfr 980 ħv9RO→chr98o 9h>7-Qαu9t7x
理a
需等
9T不，
8≡τ ：山引3# Gr
。n ooτ Og o
oo≡ /弑喘
Et⅛^l
003 OSl OOl OS 0
09 ：山叫8#
9£#1
OOC OSl OOl 0，	0
V9 3。#
PCa grad-ents
,ooτ-y∀d0
£#1

HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
epoch gradients epoch gradients
Figure 24: PCA Components Overlap with Gradient. Repeat of Fig.2on FCN trained on CIFAR-100 dataset.
CIFAR-100, L#1
#elem: 1500
CIFAR-IOOr L#2
#elem: 20
CIFAR-IOOi L#4
#elem: 50
CIFAR-IOOi L#3
25000
S+Ju ①-PSa
L#5
8U①eud
0	50 100 150 200
epoch gradients
L#6	L#7
#elem: 500 #elem: 50000
0	50 100 150 200	0	50 100 150 200
epoch gradients epoch gradients
Figure 25: PCA Components Overlap with Gradient. Fig.2on CNN trained on CIFAR-100 dataset.
L#8
#elem: 100
0	50 100 150 200
epoch gradients
38
6£
[əs典HP JLSlNWH Uo PoUlUn 6If>f>A Uo Z *⅛ Jo IeodəX ∙ιu∂ipvΛQ ψiΛ∖ dvp^Q sjuəuodmoj yjj ：G7 əjusij
PCa graddnt:s
179#1
OOl Si OS %
96Z6SE2 :山引己#
siuəipejo IPod己
siuəipejb IPOde
S4uaιpej6 IPOde
STS :山叫白#
乙9#1
OOT 0L 09 SZ
s)uəipejð IPodm
OS #1
OOl SL OS %
2TS :山叫3#
ZTS :山叫a#
9626ζE2 :口叫3#
£0#1
ZTS :山叫3#
2TS :山叫3#
乙IG :山叫3#
PCa grad不nts PCa grad不nts
r££ZiT Etutct-Tt
TC9OV9TC9O 7-8 V 0928 Vo
。。「广铸飞
VZ868S :可叫3#
9宾：山&付#
乙乙#~1
OOT “ OS 52 0
OOT ££“铸' Q
ggɛ :uiap#
0£#1
OOT S£ OS SC 0
9SZ ：山叫3井
gςg :ui9|a#
OOT Si OS SZ 0
9SZ ：山叫3#
1738689 :山叫3#
PCa grad不nts PCa gradients
8乙「山叫a#
OOT 0L OS " 0
9S2 ：uja|a#
家#^l
OOl 鼠 OS SZ 0
8乙I ：小叫a#
OOl SL OS SZ 0
8乙£££ :山叫3#
力乙#1
乙16V6乙:山可己#
PCa grad不nts
V9 ：山叫&#
8#1
OOT “ OS 52 0
9£G ：iuap#
,1SINlΛld
OOl SL OS SZ 0
179 :山叫3#
乙#^l USINW=I
9SZ ：UIB|3#
81 #1
831 ：UI9|9#
τι*^ι
。9 ：山叫9#
jISINhId
9#1 'ISINlAld
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
FMNISTr L#1
#elem: 3136
FMNIST, L#3
#elem: 64
FMNIST, L#4
#elem: 36864
FMNISTj L#6
#elem: 64
FMNISTr L#7
#elem: 36864
FMNIST, L#2
#elem: 64
FMNIST, L#5
#elem: 64
#elem: 64
#e em: 64
64
#e em: 64
#e em: 36864
#e em: 64
#e em: 64
#e em: 128
#e em: 128
#e em: 128
#elem： 128
#elem: 8192
#elem： 128
#eiem： 12S
#e em: 128
#e en∩: 128
em: 256
#elem: 256
em: 256
#e em: 256
#elem: 32768
#e em: 256
#e em: 256
#e em: 256
#e em: 256
L#49
#e em: 256
#elem: 256
em: 1179648
#elem： 512
#e em: 512
#e em: 512
#e em: 512
#e en∩: 512
Am: 512
L#56
#elem: 512
Figure 27: PCA Components Overlap with Gradient. Repeat of Fig.2on ResNet18 trained on FMNIST
dataset.
40

']ose]ep ISINWJ Uo PoUlU耳 NND Uo Z 由IH -ιu∂ipviQ 甲?m dvμ∂^Q siuəuodmoj γjj ：A7 əjustj
90
S4uaιpejb IPod3
ooɪ GI
S4U□ιpejβ IPod3
DOT
siuəipejb IPoda
OOOq :山叫a#
£#1
OOl SZ
OOG ：we|3#
9#1
OOT WL
PCa g「ad_ents
8#1
OOi ς/
80
OT
O
Pea g「ad一ents
OS ：山叫3#
。#1 'OTUSINhH
OOoG乙：UJ叫3#
iOI-ISINNd
O乙：山叫a#
iOT-ISINlAlzI
OoS ：UJ3|3#
T#1 zOI-ISINlAld
IoSe弗P ISINWJ Uo PoUle图 N□H Uo C 韦IH jo ]uodoχ ∙ιu∂ipviQ 中叫 dvμ∂^Q siuəuodiuoj γjj ：Q7 əjustj
s4uaipejB qz>ods
OOT Gl OS G 乙 0
00
乙’0
S4UΘ∣pejβ ^ɔodə
θθɪ Gl OS G 乙	0
VO
910
810
OT
OT :山叫3#
乙 #"TOT-ISINIAH
PCa
g「ad_enrS
0
0根8£ ：山叫3#
T#"I 'OriSINlAH
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
oo

to

80

∙]osu]up ISINW Uo POUlU4 6TOOA Uo (韦IH jo IeOdoX ∙ιu∂ipvΛQ 豆算伙 dvpə^o siuəuodiuoj yjj ：Q£ əm既更
s⅞∪3∣pej6 IPOde
ooτ A os GZ o
s)uəipejg IPOda
OOi A os 文 o
s4uəipejð IPOda
ooτ & OG 文 o
s⅞usιpej6 IPOde
OOT & os sz o
S4uaιpejβ IPods
OOI 5L Og SZ
OI Mei3#
99#1
OOi QL Og sz
o
s4uəipejb ιpode
ooτ & Oq Sz o
OLIG ：UJ9|9#
£9#1
00T 5£ 05 SZ
s；uθ∣pejβ q□od∂
OOI WL OS SZ o
乙[G ：UJ9|9#
。9#1
ooτ GL oς ςz
CTS ；山叫8#
£9#1
ooτ H os ςz
STS ；LU叫a#
乙9#1
OOT GL 05 ςz
96乙6GE7 MmI3#
T9#^l
001 SL OS SZ 0
乙1G ：LU9|9#
09*1
OOI 9L OS SZ
2TS :山叫s#
65#1
OOT QL OS SZ
乙IG ;山引3#
8G#1
OOT QL Og SZ
96乙6G£Z ；山叫3#
LG4Π
00T QL OS SZ 0
zτς :山可己#
。。…噌%。
96Z6ζE2 :山叫a#
6。#1
OOi 弘 OS “	0
ZTS M引a#
乙 1z#1
001 " OS S3 0
ZTS ：山司。#
ooɪ Si 5-ls≡
95Z ：UJ9|9#
8乙#1
ooτ “ os GZ o
172868S ：山叫3#
TZ#1
oot ζt oς % o
ZlS :山叫&#
GG #1
OOt ；£ OG SZ
ZlS :山叫&#
t?G#1
ooi SL Og ςz
9636SεZ :可叫3#
。。…E整%。
ZTS :ɑjθlθ*
。。一/储%
乙Tg :山引&#
IG#1
O(H SL 05 ⅞Z
乙TS :W叫3#
。。…啜％
乙IG ：iue|e#
St#1
ooτ “ os se
JTS ：川3m#
)力#1
ooτ 0L os sε
Ziς :山叫3#
9力#1
OOT SL OS 夕
9626S£Z ：山叫R#
效#1
OOI Si OS S2 0
乙IG :luə2#
OOI S£ OS S?
乙TG ：ujap#
Efr#1
OOl S£ OS
9626SE2 :山叫a#
τi7⅛η
ooi “ os sz o
乙IG :3叫3#
。3 /睛%
99乙：小叫3#
£乙#1
001 OS GZ
9G乙：山叫3#
0Z#1
001 “ Og 冗
乙IG :山叫。#	乙TG :山叫a#	nς :ɑjələ# 96265EZ :山叫3# 乙1G :山叫3#
00T Siθ O^^*Sδ 0	00T “丐咨工? o ooτ 必吟咨"¼z o ooτ GJS番'sz o oot gJ*F'sz 0
8W6z,ττ ：ujai9#
OOT sze⅞Γ≡≡ 0
9G乙：UJ9|B#
ooɪjðtɪŋ
9G 乙;ωB∣θ⅛
OOT ≡JW∖≡
9S3 ：38国#
。。…啜%
t73868ζ ：UJ9|9#
。。…6替上。
952 ：川3国#
9乙#1
ooi 0L os ς≡
fr286Bζ ：iUSIs#
OOT 0L
遨4
952 ：山叫3#
。乙#1
OOI SI OS SZ
99乙:iuəɪə*
99乙：山叫3#
OOT SL

9G乙：山叫e#
9G乙：山叫3#
OOX
ς≡ o
。。…8储1
2161762 :山叫m#
oo! SArI
82τ ：iuep#
91#1
OoT GL 05 ςz 0
8乙1:山砌3#
001 a oς 52
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
£P
"ləSB}Bp ISINW Uo pəureij STPNs9H Uo C ,副H Jo]eodoX ∙ιu∂ipmQ ψiM. dvμ∂^Q siuəuoduioj yjj ：T£ OJ∏5IJ
SIU台!pm」B q^ods
。。…像,。
82X :lus|S#
s4uəipejo IPOda
s^uəipejb Ipodm
0乙IG ：山叫己#
19#1
s4uəipejo IPoda
OOT EL OS SZ 0
S4uθιpejb IPOda
ZIG :山叫3#
09#~1
DOI " 05 g 乙
65#1
OOI 0L OG SZ
OOI 5£
ZTS :岬9#
乙£0]£[：山引a#
乙IG :山叫a#
vς*^ι
OOl ZL
ZT5 :iuəiə*
2。#1
ooτ QL
1728685 :山砌s#
0力#1
OOl S£ OS SZ 0
90乙：CU9|9#
EE#1
OOI SZ OS SZ
821 ：UJS|S#
9乙#1
OCl ς/
QQPLVT :山合2#
61#1
OOT 9£ 05 SJ 0.
t79 :"3目#
乙1#1
ooτ ς/
V9 :山叫3#
g#i ,isin∣λ∣
00 ɪ ζL
gςz ：川叫3#
8t796Zττ ：UJ9|S#
9。#1
DOI 5Z
Vl
1
9G乙：山叫a#
9任；UJ叫a#
乙1606乙：山己|己#
s
a
9G乙：山叫3#
6£#口
DOT SL OS
8 乙I :LUmI3#
。乙#1
OOT W/
9G廿£。1 ：山 aim#
5Z#1
82T :iuəiə^
82T :山司a#
9626SE2	6#
8乙I :山叫3#
8zzεz :山叫s#
91#1
OOT SL OS ς≡ Om
但
179 ：UJ9|9#
6#1
DOT OG 5Z
t79 ；山叫3#
乙 #1'_LSINW
PCa gradients PCa gradients PCa grad-enfs PCa gradants
PCa grachenyr
PCa gradients
PCa gradients PCa grad-enfs PCa gradients
242QleL2 8 4 0 181512 9 6 3 0

HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022

MNIST, L#1
#elem: 7840
MNIST, L#2
#elem: 10
epoch gradients
epoch gradients
Figure 32:	PCA Components Overlap with Gradient. Repeat of Fig.2on FCN trained on MNIST dataset.
epoch gradients
epoch gradients epoch gradients epoch gradients
Figure 33:	PCA Components Overlap with Gradient. Fig.2on CNN trained on MNIST dataset.
44

QOSewP ɔθʌɪvɔsuj uo PoUle耳 p↑∖[-∩ UoC 留IHJO ]eodoX -ιu∂ipvΛQ ψiM. dvpə^o siuəuodiuoj yjj əjn?ɪj
s⅞∪θ∣pej6 IPOda
t?9 ；山叫e#
09*1
ooτ SL os S?
t79 ；UJmI3#
8G#1
ooi SL os ςz
t79 ；山叫e#
乙9#1
ooτ GL os ςz
V9 ；山叫e#
T9#^l
ooτ SL os ςz
乙618 ；山叫9#
£9#1
OOI 0L OS SZ
t?9 M叫3#
6G#1
OOT QL 0£ SZ
：LU3|B#
f989E :可叫a#
t79 :山叫a#
8乙2££ :山S|3#
V9 ：山叫a#
J6T8 :山可3#
09 ：UU9|S#
皿6蚪Z
vz
s⅛
Il
95Z :山叫8# 0
。。一，啜％ .
■
9ςt7zi7τ :uj9p#
。。…嘴％。

ZE :川叫a#	ZE :山叫a#
£#1 '30 八 IeJSed 9#1 '30/MenSed
9126 M叫a#
G#1 (□OΛ∣e3sed
ZE ：LU9|9#
P#1 noAlenSed
≡4U9∣pej6 IPOde
。。…喋上

2T6t76? U叫a#
£乙#1
OOI OS GZ C
831 ：山叫3#
0Z#1
OOt GL Og 完
V989E ：山叫R#

s4U9∣pejδ IPode
。。-J.%
8ZT ：w叫&#
9乙#1
ooi 0L os ς≡
51
8乙£££ :山叫a#
61#1
OOT GL OS 完 0
T∙⅛f∙η ' 'η⅛Λjβ 二
b9 Um|m#
Zt#l
Ogi OG ，乙
0
SE
OE
Q。
ςι
oɪ
SlUm!pe」6 IPods
国小错% 外
82T :山叫&#
G乙#1
OOT 0L OS SZ O1

V9 :山叫3#
81#1
OOT GL 05 ζz 0
期81 ：川叫3#
IT#T
ooτ QL oς ⅛z c
S4uaιpejβ IPods
°。7 铺%
8sτ ；山叫3#
。乙#1
OOI ζL 0≡ SZ
仃
OT
V9 ：山叫3#
LT#1
OOT 7 05 SZ 0
乙E ：lU叫3#
OT#T
001 QL Og ςz
ZZ
ΞΞ
OZ
OT
zɪ
V98 ：川叫。#
E⅛^l ,□OΛ∣e□sed
s4uəipejb IPoda
ooɪ 5i°⅞r∖z
821 ：LU3|S#
E乙#1
OOT SL OG SZ
SZ :山叫a#
Z.#~\ i□OΛ∣e□sed
s⅛uθ∣pejβ q□odθ
步9 :山叫3#
LG#1
ooτ QL os ςz
…6替上
A n
建
6 ^n
821 ：LLJ9|9# 0
在#1
00T SL OS SZ 0
t79 ：iuap#
Wl
ooi a oς ςz
ZE :山可3#
8#1
OOT 0L OS 52
。0£ :uue|B#
T#~l ,DOΛ∣e□sed
HOZ mɔl IB JodBd əɔuəiəjuoɔ B sb poqs∏qnj
处
,]ose]ep θɔθɔ u。pəureɪ: WN-ʃl Uo(?IHJo ]eodoX ∙ιu∂ipvΛQ 磔?血 dvy∂^o siuəuodiuoj yjj ：g£ əju?tj
8ZI ：LU9|a#
乙 17#1
OOl " OS sɛ
s⅞uθ∣pej6 IPOda
s4uəipejθ IPOde
54U9∣pejδ i(ɔodə
s⅛uθ∣pejβ IPod3
ooτ &
S4uaιpej6 IPod3
OOT EL
s4uəipejb IPoda
ooτ WL
SlUm!pe」6 IPod3
ooτ Q
O
久：w叩#
99#1
OOi QL Og sz
：LUd|B并
。9#1
OOT GL DS SZ
“：UJ邓寿
siuəi^igʌiɔodə
OOI ££ Og = 0
久V8 L W叩#
£9#1
OOT 0L OS SZ C
8V06 二UJ叩#
59#1
OOT S£ OS SZ
9 LCb :川叫d#
69*1
OOI £L Oq SE
6E :3川"#
89#1
OOT GL OG 文
乙618 ；山&8#
V9 :LU叫3#
"9 ；山叫a#
t79 ：LU9|9#
V9 M叫3#
t79 ；山叫3#
力9 ：川叫a#
OOTSt
OOT SwSZ
6。#1
ooτ $£ OS S2
。。工/鬻％
WG密%

m”叫r%

O
τi7⅛η
OOl “ OS SZ 0
呢ɪ :山叫。#
OOT Siθ O^^lSδ 0
2τ6fr6δ :可叫3#
OOi % 6 储"ɪ Sz 0
8aM 叫 3#
OoTG£ 8 储% 0
89Z2E M叫3#
OoTGJ 铺% 0
8ZT ：Luap#
OOT srθ⅞lt^lsz 0
乙201£］：山引。#
9G乙：OJ9|9#
9G乙：OJ9|9#
9SZ⑼叩#
9G乙：UJB|9#
953 ：山8国#
OOI ⅞z.
t79 ；山叫己#
Wl
OOI a M 52
…6塞LZ
821 ：LUml 3#
在#1
OOT SL OS SZ
。。一 J 褚%
i7T#"l
ZE :川叫2#
1ODO□



乙£ ：LU9|S#
9#1 ,θɔθɔ
9126 U叫a#
S#-l *O□OD
SE ：LUB|3#
t7#~l ,O□O□
V98 :山叫3#
£#~l 'O□O□
≡ ：UJ9|B#
乙#^ΓO3O3
tz3868ς :山叫。#
皿“6替上。
Ze ：山引a#
8#1
OOT 0L OS 52
V9 ：山3|3#
1#1803
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
CeIebA, L#1	CeIebA, L#2
#elem: 1728 #elem: 64
#elem: 64 #elem: 73728
CeIebA, L#6
#elem: 64
CeIebAf L#3
#elem: 64
CeIebA, L#4
#elem: 64
CeIebA, L#5
#elem: 36864
CeIebAr L#7
#elem: 64
#e em: 128
#e em: 128
50 75 ιoo
L#10
#elAm: 128
20	25 50 75 100 °O 25 50 75 100
L#15	L#16
#elem: 128 #elem: 128
#elem: 256
#elem: 256
ffelem: 256
#elem
epoch gradients
epoch gradients
epoch gradients
epoch gradients
epoch gradients
L#39
#elem: 5
L#60
#elem： 5
L#40
#elem: 5
L#54
#elem: 5
L#34
#elem: 5
L#62
#eIem: E
L#36
#elem: 5
L#50
#elem： 5
L#66
#elem:
L#51
#elem: E
L#58
#elem: 5
L#56
#elem： E
L#52
#elem： 5
L#55
#elem： E
L#38
#elem: 5
L#46
#elem: 5
L#48
#elem: Ξ
L#63
#elem: E
L#43
#elem: 5
L#32
#elem: 7
L#47
#elem: 5
L#35
#elem: E
L#44
#elem: 5
L#64
#elem: 5
L#27
#elem: 2
L#30
#elem: 256
L#42
#elem: 512
T L#53
#elem: 2359296
L#41
#elem: 2359296
T L#37
#elem: 2359296
L#29
#elem: 589824
T L#49
#elem: 2359296
epoch gradients epoch gradients
L#22
#elem: 1
T L#45
.#elem: 2359296
L#31
#elem^256
T L#61
#elem: 2359296
L#59
#elem: 512
T L#57
#elem: 2359296
L#24
#elem: 256
T'	L#33
#elem: 1179648
L#65
#elem: 5120
L#25
#elem: 589824
25 50 75
L#23
#elem: 25ι
Figure 36: PCA Components Overlap with Gradient. Repeat of Fig.3on VGG19 trained on CelebA dataset.
47
817
[əse卯P VQ9P□ uo POUIU4 8TPMsθ^[ Uo £ 唱IH Jo 卯ədəX ∙ιu∂ipmQ 豆"M dvμ∂^Q sjuəuodmoj yjj :ʌe əjnsɪj
s⅞uθ∣pej6 IPoda
96Z6SE2 :川叫a#
9SZ :33国#
os。。一平#1 °M。。…
Og OOZ 〃常入 理
o§
o5
OS
9SW「LU 叩#
6I*^I
ooτ SL os 文 o
8乙££2 :3己目#
91#1
OOT £L OS SZ C
8乙I ：UJal己#
12#1
OOT 5£ OS 完
8乙工：LUa2#
LT#1
001 £L OS 0Z
8乙T ：LU叫a#
0乙#1
001 " Oq ”
。。一产#1诟。。…


P9 ;山己目#
SI#1
ooτ GL os ςz
8乙工；山叫己#
8I#T
OOI 0L OS SZ
■二 Wfl- ・二. i ■:卜 wa：	wς
H0I： h^: ^i： bS： h^： 13l： ∣3∣
∣M∣^k





a

V9 ：山研3#
t7T#~l
V989ε ：CU引W#
t?9 ：uj利己#
IX#1
0989f MH#
Ol#l
t79 ：W3同#
。9 ：IAJ9|B#
o ooτ "
。9 :山别e#
-	2l*∏
o OOI GL
V9 ;lu叫3#
9#1 "qm|”
t79 3叩#
£#i ,vqθιθ□
D989E ；山叩#
L#1 4∀qθlθD
t79 ;山叫3#
S#1 ʃvgəiəɔ
D989E ；川己|3#
p#-\ ʃvɑəiəɔ
80V6 ：UJ9|9#
t#i '∀qθιθ□
t79 U叩#
z*i i∀qθιθ□
-Ch grad-ents
75 50 25 Q
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
Figure 38: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on CelebA dataset.
CeIebA, L#1
#elem: 1500
CeIebA, L#2
#elem: 20
CeIebAf L#3
#elem: 25000
CeIebA, L#4
#elem: 50
#e em: 625000
epoch gradients
epoch gradients
Figure 39: PCA Components Overlap with Gradient. Fig.3on CNN trained on CelebA dataset.
49
0ζ
,【os引叩 0T-HVII□ Uo POUle4 6TOOA Uo £ '⅛ jo WOdoX ∙ιu∂ipmQ 甲?m dvpə^o siuəuodmoj yjj ：Q^ əjn^ɪj
54uaιp6j6 LPOda s^uəipej^ IPOda	s；uəipej^ IPOCI3
OOZ OST OOl M … ..     一	„
■ I1 I
3口________(: 一λ
g
O
OOZ Ogl ooτ
oς
Oo m
op
L 0

乙IG ：山司&#
皿#1	2
ooz Ogl oot oso o
K
OI ：3|9#
s⅞uaιpej6 q□odθ s^uəipejð q□oda s4u叫pg」6 LPod。
00Z 0⅛T OOl 05
zτς ：山叫a#
乙9#1
osτ ooτ 05
s⅞uaιpejβ IPodR
ooɛ osτ ooτ Og
zτs ：UJ9|9#
£9#1
OOZ OGl OOI OS
OSlS :可叫3#
09#1
OC)Z ∩ςτ ooτ (K
9626SEZ ：uJB|e#"
Oo OOZ OSl OOl Og Oo OOZ OSl OOT 0，	0
.” O	■	O	'
ζlS ：iU己|3#
96C6SEζ :山叫e#
乙;UJeld#
乙IG :山己|d#
乙1G ;山己2#
96乙6GEW ；川己|己#
■g
n
h
g
乙IG ;川引m#	0
cτs ；UJ叩#
9G#1
oor OST ooτ OG
ζlS ；小己|己拜
GG#1
oςτ ooτ Oq
ζ1S ：山己|己拜
t?G#1
do? oςτ ooτ oς

215 :山型#
皿回嗑儿，晒
乙LG :w叫T#
6§#1
QQC OSl OOl OS
乙工自：UJd|T#
20#1
002 OSl OOT OS
Oo
9btb"8乙：uid|0#
LS#~l
ok osτ ooτ os


96乙6GE乙：山己|3#
£S#1
002 OgT ooτ oς
21S ：山叩#
乙S#1
ooε osτ ooτ Oq
乙IG ;uj叫m#
乙IG ：川己|2#
0G#1
ɔoe osr 00τ Oq



6包#1
£。#1
9。#1
Gt#~l
PP#1
£。#1
8t#1
三：山叫a#
Zt7#1
三：山叫e#
6£#t
G ：山引己#
9£#\
乙IG :小弓目#
0>#1
96乙6G£乙：山叫己#
___L肘./
9626SEZ :山叫己#、
一」即一…
zτς :山叫a#
zτs ：山叫a#
8096UI：：山引3#
952 ：wap#
9SZ ：wap#
9SZ :山叫a#
tzZ868ς ：lub|3#
fr£#1
0€#l
62#T
LUd|B#
LU3|3#
LZ.#~\
LU3|3#
9乙#1
t?28685 :山ap#
：LU 叫 a#
LU 叫 m#
UJals#
§-3
sɔ
o⅛
tz乙868G ：山己|己#

IUJ 叩#
:山叩#
L1#l
8乙1：UJ己|己#
02 #1
6T#1
81#1
91*1
SI#1
LU9|3#
UJ 叫 a#
UJ 叫 3#
:iua|e#
：LUS|D#
9GMt7l 3叫a#
8ZZ££ ：UJB|3#
OOZ /域北
001 * 域九
m105t¾V OS
OOZ /墟叫
m O=T0Jr06
OOt⑹制O5
OOC
由需g
V9 ：UJ3|3#
l#~\ 'oτ-yvd∣□
i?9 :山叫a#
9⅛-ι foτ-yvj∣3
V989E :山叫a#
ς⅛^l 1OT-HVdD
V9 :山叫a#
t?#~i joτ-yvj∣□
V9 :山叫a#
e≠^i ,oτ-yvd∣□
t?9 ：山叫a#
乙ioτ-yv=∣D
8≡Z,τ ：UUS|B#
i#i -oτ-av∑∣D
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
#elem
#elem
epoch gradients
epoch gradients
epoch gradients
epoch gradients
epoch gradients
epoch gradients
epoch gradients
CIFAR-10,
#elem:
L#ll
#elem: I
L#15
L#12
L#14
#elem: ∣
L#62
#elem：
L#17
#elem: 1
L#24
#elem: 1
L#26
#elem: 3
L#50
#elem: 5
L#27
#elem: 3
L#54
#elem: 5
L#56
L#59
#elem： 5
L#18
#elem: 1
L#60
#elem： 5
L#23
L#33
#elem: 2
L#29
#elem： 1
L#30
#elem： 1
L#53
#elem: 5
L#20
#elem; 3
L#51
#elem: Σ
L#32
#elem: N
L#35
L#41
#elem: 256
L#36
#eIem: 256
L#39
#elem: 256
L#42
#elem: 256
L#21
#elem; 1
L#38
#elem: 256
L#57
#elem： E
L#55
#elem: 2359296
L#44
#elem: 256
L#10 一
36864
7	L#46
#Hem: 1179648
L#40
#elem: 589824
L#47
#elerrκ512
L#37
#elem: 32768
L#45
#elem: 256
L#25
147456
L#48
#elem: 512
L#28
147456
L#61
#elem： 5120
L#22
#elem: 8192
L#19
#elem; 147456
L#34
#elem; 589824
L#31
#elem: 29493.2
L#43
#elemi589824
L#52
#elem: 131072
L#13
#elem: 36864
L#16
#elem; 73728
CIFAR-IOf L#1
#elem: 9408
7	L#58
#elem： 2359296
CIFAR-10, L#2
#elem: 64
CIFAR-10, L#7
36864
CIFAR-10, L#6
#elem: 64
CIFAR-10, L#4
#elem: 36864
CIFAR-IOr L#5
#elem: 64
Figure 41: PCA Components Overlap with Gradient. Repeat of Fig.3on ResNet18 trained on CIFAR-10
dataset.
"	L#49
#elem： 2359296
51
Published as a conference paper at ICLR 2022
CIFAR-10, L#1
CIFAR-IOf L#2
epoch gradients
epoch gradients
Figure 42: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on CIFAR-10 dataset.
ClFAR-IO, L#1
CIFAR-IOr L#2
#elem: 20
CIFAR-10, L#3
#elem: 25000
CIFAR-10, L#4
#elem: 50
*o
0	50 100 150 200
epoch gradients
epoch gradients
epoch gradients
epoch gradients
Figure 43: PCA Components Overlap with Gradient. Fig.3on CNN trained on CIFAR-10 dataset.
52
Published as a conference paper at ICLR 2022
#elem:
#elem
#elem：
epoch gradients
epoch gradients
epoch gradients
epoch gradients
#elem;
L#10
#elem: 1
L#39
#elem: 5
L#60
⅛elem: 5
L#12
#elem: 1
L#40
#elem: 5
L#54
#elem: ≡
L#36
#eIelTi: 5
L#50
#elem： 5
L#19
#elem: 7
#elem: 51200
L#51
#elem: E
L#58
#elem: 5
L#34
#elem: 5
L#56
#elem： 5
L#52
#elem：
L#38
#elem: 5
CIFAR-100,
#elem:
CIFAR-IOOi
#elem:
CIFAR-IOOi
L#46
#elem: 5
L#55
#elem： E
L#62
#elem: 5
L#63
#elem: 5
L#43
#elem: 5
L#47
#elem: 5
#elem: 589824
L#ll
并回朝：1
L#48
#elem: E
L#35
#elem: Σ
#elem: 73728
L#44
L#32
#elem: 2
L#26
#elem: 2
L#64
#elem: 5
L#15
#elem: 128
L#30
#elem: 256
L#14
#elem: 128
L#42
#elem: 512
L#53
#elem： 2359296
7	L#41
#elem: 2359296
7	L#37
⅜elem: 2359296
L#18
#elem: 256
L#29
#elem: 589824
L#16
#elem: 128
epoch gradients epoch gradients epoch gradients
7	L#45
,#elem: 2359296
L#21
#elem: 589824
L#22
#elem: N
L#17
#elem: 294912
L#31
#elem: 256
L#20
#elem: 256
L#59
#elem: 512
L#27
#elem: 2
7	L#57
#elem: 2359296
7	L#61
⅜elem: 2359296
CIFAR-100, L#5
#elem: 36864
L#13
#elem: 147456
7	L#33
#elem: 1179648
L#24
#elem: 256
CIFAR-IOOf L#1
#elem: 1728
CIFAR-100, L#6
⅜elem: 64
CIFAR-IOOr L#7
#elem: 64
Figure 44: PCA Components Overlap with Gradient. Repeat of Fig.3on VGG19 trained on CIFAR-100
dataset.
"	L#49
,#elem: 2359296
53
Published as a conference paper at ICLR 2022
1	L#49
#elem： 147456
CIFAR-IOOf L#1
#elem: 432
0	50 100 150 200
L#43
#elem: 147456
0	50 100 150 200
L#36
#elem: 64
#elem: 64
9216
50 100 150 200
L#29
50 100 150 200
L#22
#elem
#elem：
L#24
L#38
#elem:
L#39
#elem: ∣
L#ll
#elem:
L#26
L#27
#elem:
L#12
L#14
#elem::
L#17
#elem;
L#18
L#23
#elem:
L#30
#elem; ∣
L#21
#elem;.
L#32
CIFAR-IOOi
#elem:
CIFAR-100,
.#elem:
CIFAR-IOOi
#elem:
L#33
#elem; ∣
L#35
#elem; ∣
L#41
#elem: 1
L#48
#elem： 1
L#42
#elem: 1
#elem: 2304
L#10 一
#elem: 2304
L#25
#elerr>: 9216
L#44
#elem： 128
L#45
#elem: 128
L#37
#elem: 36864
L#16
#elem; 4608
L#46
#elem： 147456
L#40
#elem: 73728
L#19
#elem; 9216
L#47
#elem： 128
L#34
#elem; 36864
L#28
#elem: 18432
L#31
36864
CIFAR-100, L#4
#elem: 2304
CIFAR-IOOr L#7
#elem: 2304
CIFAR-100, L#6
#elem: 16
L#51
0 OSoOI
EU@pe」cn
00	50 100 150 200
L#8
O OS OOI olnɪ Ooz
S4u&pe」6 IPOd①
⅜elem: 16
o OSgIOgIooZ
EUquodαj
O OS OOI olnE OOZ
1/>LPB」6 IPOd ①
O OSOol
EU①-pe」6
50 100 150 200
L#50
#elem: 128
IPodφEUIPod(υ
OSlOOZ 0 ou-l。OlOSl OoZ O OSOoI
IPOdφi/>4_>u3pb」6 IPOd ①(Λ4-∙U①-Prt3」6
Figure 45:	PCA Components Overlap with Gradient. Repeat of Fig.3on ResNet18 trained on CIFAR-100
dataset.
54
Published as a conference paper at ICLR 2022
CIFAR-100, L#1
CIFAR-100, L#2
#elem: 100
ɔθ 50 IOO 150 200
epoch gradients
ɔθ 50 IOO 150 200
epoch gradients
ι,o
0.8
0.6
0.4
0.2
0.0
#elem: 307200
Figure 46:	PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on CIFAR-100 dataset.
C1FAR-1OO, L#1
Iem: 1500
CIFAR-100, L#2
#elem: 20
CIFAR-IOOf L#3
#elem: 25000
CIFAR-IOOj L#4
#elem: 50
ɔθ 50 100 150 200
epoch gradients
L#6
#elem: 500
L#8
#elem: 100
50 100 150 200
epoch gradients
epoch gradients
epoch gradients
#elem: 625000
#elem: 50000
Figure 47:	PCA Components Overlap with Gradient. Fig.3on CNN trained on CIFAR-100 dataset.
55
9£
[əs典HP JLSlNWH Uo PoUlUn 6If>f>A Uo £ ,⅛ jo ieədəX ∙ιu∂ipvΛQ ι[箕M dvy^Q siuəuodmoj yjj ：Qh əin^ɪj
s^uəipejb IPOd3
zτς ：we|s#
乙9#1
SL OS S?
s4uəipejb IPOdo
，【R Og 文 。1



Oo

S

W
96Z65∈ζ :山叫3#
6。#1 一
θθɪɪɪɪjg
zτs ：山叫3#
Zt7⅛"l
OOI GL OS SZ
zτς :山叫a#
S£#1
ooi U os ςz
S


0≡
Vi

S4U3∣pejb LPodO
ogι >z. Og 昊 9 §

9626SEZ :ujəɪə^ɔ
g-J精%底
⅛
乙Iq :川叫a#
≡4uaιpejb uɔodə




°°m
o≡


ZIS ：山叫8#
6S#1
OOT GL OS %
Og



Sq :uiəiə^
SS #1
ooτ si os sc
sτs ：w叫s#
VS#1
OOT Si OS SJ
96乙6££乙：山3目/
SI

K
乙IG :iu叫3# 0
。。上工甚


W
乙1G :山叫3#
31S ：山引3#
乙s#i
OOI 9£ OS SZ
og
a


ζXS :山叫3#
£。#1
ooi st Og sε
。。…喈6Z
0§
96乙6G= ；33付#
一
ootɪɪɪog

9626SEZ :euəɪə^tɔ
。。一 J镇“伯
zτς ：山。付#
fr£#1
OOI “ OG =


o

a


a




-IG ：UJ9|9#
0>#1
ooτ SL os sz
ZIS :山叫3#
O o
S

8i796£TI ：LU3|B#
££#1	1
Qgl “ Og GC 应
952 M叫m#
uɪ


953 :山叫a#
9G乙：w叫a#
9⅛d :tu叩#
OZ #1
；93#
£T#1
v68by⅛ ∙uj叩群
T?#l
ooτ sr os sz
8ZI ：UJ9|9#
"#1
V9 ：LU3|3#


•山02并
61#1
SL os ς≡
8zτ ：Lua|a# o

Z#~l 'ISINlNd
。。一螺％
Oo



乙IG :山叩#
8£#1
OOI SL OS SZ
Oo


K
9SZ ：wa|e#
乙£#1
1
O
V2868S :山叫3#
9G乙：山引己#
Gl
8uτ ：山叫己#
TI#1
9S2 ：wap# 0
X£#l -
ooτ QL qς ςz o≡
s4uəipejb LPoda
s;u9ipejb Ipoda
q :山叫s#
衣9#1
q ：UJ9|3#
OW
5 UmIs#
£P#1
q u 叫a#
9£#1
1728685 ：lus[B#
OSlS ：W3|3#
S9#1
uɪ


9S2 M叫3#
。乙#^l
乙16。6乙
L1#l
⅛
¥
8ZT ：uJ9|e#
V9 :山叫a#
9#1 USINWd
V989E :山叫a# o V9 :山叫a#
"ISINIaI=I	'_LSIN网=I
S


8乙I ：uJdfd#
qn#i
e
P
n
珀
KJ d
LΠ _
8乙I ：LU己目#
9l#l

。9 :山司&#
三 ad-enrs
Og
ooɪ si0^∖z


：IU3|3#
iISINWd
8Z£££ :山叫3# 0
V9 U叫a#
乙#^l dXSINlAlJ


W :w叫a#
I#1 ,1SINIλIJ
epoch
IOO 75
grad-enfs
50 25 o
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Lζ
IoS 叩 P
ISINWJ u。pəurej^ 8【灯2。丑 uθ £ ,⅛ JO ieodoχ -iuəipmo 凝以M dvpiə^o siuəuodiuoj γjj əjn^ɪj
96Z6SE2 M叩#
6衣#1 J
。。…。鬻％
Oo
S4u∂∣pej6 LPoda sjumιpm6 IPOda s;uaipej6 IPoda
乙IG ：UJB|9#	96乙69£乙：UJ9|9#	乙1G ：UJ9|9#
0§
IMjt is；
0§
£G#1
ooτ " os ςz
^8∙S
aS-
乙1G :山叫&#
乙£01£工：山叫3#
乙TG ：ujsp#
乙TG :iLImI3#
□ ：UJ 叫 3#
8b#1
□ ：we目#
LT#A
乙：UJ9|9#
他#1
952 ：33同#
l7fr⅛-l
8t796IlT ：UJ9|9#
9。#1	-
V乙868G ：山叫3#
£在#1
：UJS|3#
：LU3|3#
LU3|3#
UJ 叫 e#
:iUala#
：iuap#
I ：UJ叫a#
IT#1
:iusp#
ςτ⅛^ι
II ：LU3|a#
VT #1
:uj 叫 e#
zτ*-ι
[:山叫己#
，乙#1
［：CLIa同#
ZI#-1
I ILUΘ∣9⅛
"#1
：:山叫a#
Zt7#1
r :山叫s#
Tt7#1
［:山3H#
9Z*^I
乙：UJ叫a#
6E# 1
L ：3叫3#
9£#1
L ：山回z#
8T#1
I ：uua|a#
61#1
I ：Lua|S#
。乙#1
I ：LU叫3#
0£#"1
［：山引e#
E2#1
V989E ：山叫a#
L •山叫3#
S£#1
[：山叫己#
IZ#1
V989E :山叫a#
乙618 :山叫m#
乙乙#1
t7989E :山司3#
(H #1
892在:ujəɪə*
£f#L
V989E :山叫&#
ET #1
fr2868S ：川叫m#
or#i
gςt7Zt7ι ：Lusp#
6I#1
1728685 :山叫3#
射£#1
9Gt72tq;:山叫a#
83#1 l
2T6fr62 :山叫3#
工£#1
8Z£££ :山叫白#
91#1
9#1 ,±SINWd
"ISINWd
dXSINlAlJ
I#1 iISINlAlJ
L#1 iISININ=I
P#1 "ISiNlAl =I
E#1 iISINWd
…6错％
ooɪ d替％
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
FMNlST, L#1
#elem: 7840
FMNIST, L#2
#elem: 10
Lnz 0LrlM
nu ①一 IPOd ①
0.8
0.6
0.4
0.2
≡o
25	50	75	100
epoch gradients
°0	25	50	75 100
epoch gradients
Figure 50: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on FMNIST dataset.
FMNISTr L#1
#elem: 500
FMNIST, L#2
#elem: 20
FMNlST, L#3
#elem: 25000
FMNISTt L#4
#elem: 50
#e em: 400000
epoch gradients
epoch gradients
Figure 51: PCA Components Overlap with Gradient. Fig.3on CNN trained on FMNIST dataset.
58
6£
4次即叩 ISINW Uo POUlU4 6TOOA Uo £ 韦IH jo IeodoX 'iuəipvio 豆〃佻 dvpə^o siuəuodiuoj yjj ：7C əm既H
8ZT ：iU9|s# 0
工苔

ZTS ：山引3#
S⅞U3∣peJD IPod3
OOI % OS 我 Oo
STS :山叫&#	96^6sεz ：UJ9|9#
ZTS ；山叫3#	乙IG :山叫m#
乙1SI ;山回己#
乙IG ：UJ叩#
9G乙：山引己#
—
⅛







9加。1 3叫3#0
8乙工：山叫3# 0
gi窄％
8” ：UJ叫3#
TT#1
OOT QL oς QZ
Cn


Ui
⅛




V9 ：UJ3|3#
'ISINW
V9 ：山叫a#
9#1 1XSINIaI
V989E :山叫a#
iISINlAl
V9 :山叫a#
fr⅛^l ,1SIN∣Λ∣

zτς ：小叫s# 0
315 ：山引3#
≡f∙ !—»一

乙ɪg ：IU9|9# 0
乙I6t76乙：川日2#
8乙I :山己目#
8乙【：山己目#


⅛



u!
V9 :山叫a#
£#1 ,±SINW
V9 :山叫3#
乙#^l 'ISINH
。9 :山司a#
8#1
8Z£E£ ：lub|3#
6#1
ooτ GL oς ςz c
8乙1：山叫e#
。。…噌，说一	一一
■0
I: IH
9£G :山3付#
I⅛^l 'ISINW
grad-enrs epoch grachenfs
50 25 0 IOo 75 50 25 0
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
09
QOSe卯P ISINW Uo pəureij STPNs9H Uo £ ∙⅛ jo ]eodoX ∙ιu∂ipmQ ψiM. dvμ∂^Q siuəuoduioj yjj ：cc əinsɪj
S4U9∣pejo uɔodə	sjusipe」B IPOd3
O乙Tq :uj叫3#	乙ɪg :山叫&#
siuəipejb LPod3 s;uəipejb IPod3
S4uaιp6Jb IPC)ds
ooτ WL Og “ o<
2TS :小叫a#
。“6源
a





zz,oιετ ：山引己#
OOT GL
96乙6GE7 ：山叫a#、
SG#1
ooi ς/
≡≡,-
OOT QL os sz
乙IG :uj叫s#
£G#1
OOI QL 0ς SZ
Oo (D
.op
[晦
hi
∣a⅛
I班
∖ M Q.
乙【G ：UJ9|9#
9G乙：山叫3#	9G乙：山叫3#
。。…嚓上
皿/睛％症
悴
e
P
t^Z868ς :山叫己#0
£17#1
9⅛d心小#
OOT SL
”和 α 0≡.
8乙1：山引3#
8乙1：山叫3#	0
gɛɪ :iuap#	2618 :山叫3#
9加。］；山引己#
。9 ：LU3|9#
⅛
IT#1
V989E ：tu引3#
V#~\ '1SINW
。。…嚓%佑
8乙1：IU叫a#
ooτ√¾T‰o≡



■g
:
t7989ε ：W9g#


。。…甯%佑
ooɪ S^f∖z OS
8乙££ :山叫3#
91#1
OOT £L OS SZ C




。9 :山合目#
SI#1
ooτ GL os ςz
grad-ents
50 25 o
epoch
Ioo 75
⅛
grad-enUΓe-
50 25 0 IOa
t79 ；33|3#	fr9 U叫3#
^#1 1ISINW	乙 #1 'ISINId
9ετε ；山叫m#
I #1 'JSlNW
-Ch grad不nts
75 50 25 Q
HOZ mɔl IB JQdBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
epoch gradients epoch gradients
Figure 54: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on MNIST dataset.
MNISTz L#1
#elem: 500
MNlST, L#2
#elem: 20
MNlST, L#3
#elem: 25000
#elem: 400000 #elem: 500 #elem: 5000
epoch gradients epoch gradients epoch gradients
Figure 55: PCA Components Overlap with Gradient. Fig.3on CNN trained on MNIST dataset.
epoch gradients
61
Published as a conference paper at ICLR 2022
PascaIVOC, L#1 PascaIVOC, L#2
#elem: 704 #elem: 22
PascaIVOC, L#3
#elem: 864
PascaIVOC, L#4
#elem: 32
PascaIVOC, L#5
.#elem: 9216
PascaIVOC, L#6
#elem: 32
PascaIVOC, L#7
#elem: 32







sθ
25 50
L#8
75 IOO
S0
S
S
S

S




O1 吧 em:_32
25 50 75
L#9
#elem: 32
100
≡θ
25 50 75
L#10
#elem: 32
100
≡0	25 50 75 100
L#ll
#elem: 18432
SO
25 50 75 100
L#12
#elem: 64
®0	25 50 75 100
L#13
#elem: 36364
sɔ
25 50 75 IOO
L#14
#elem: 64

ɪn
ɪn

K

S
S
S
S
S
S



P
∣Λ
°0	2 5 50 75 100
L#15
_ #elem: 64
25 50 75
L#16
#elem: 64
IOO
Sθ
25 50 75
L#17
#elem: 64
160
°0	25 50 75 100
L#18
_ #elem: 64
O	.
00	25 50 75 100
L#19
#elem: 73728
sɔ
25 50 75 IOO
L#20
#elem: 128
°0	25 50 75 100
L#21
#elem: 147456
~ in
P Z
⅛S

S
S




S
S
S
S



2θ
25 50 75 100
L#22
#elem: 128
≡0	25 50 75 100
I L#23
#elem: 128
≡θ
25 50 75 100
L#24
#elem: 128

25 50 75 100
L#25
#elem: 128
§0 25 50 75 16θ
I L#26
#elem: 128
§0	25 50 75 100
L#27
#elem: 294912
§0	25 50 75 100
L#28
o #elem^56








=0	2 5 50 75 100
L#29
#elem: 589824
⅛S

00	25 50 75 100
L#36
o ^^Ienτ1^8
Jz m
P Z
⅛s
§0	25 50 75 100
L#43
-#elem: 128



2θ
25 50 -
L#50
75 IOO
#elem： 64

S
SO 25 50 75 100
L#57
#elem: 64

o	,
Φ ≡0	25 50 75
L#64
ω C #elem: 32
100



∣0 25 5。 75
100
epoch gradients
S
S
S
S
S
S




SO
灯
*
Ul
25 50 75 ιbo
L#30
#elem: 256
≡0	25 50 75 100
L#37
o *el^τ^2768
S
§0	25 5C 75 100
L#44
λ #elem: 128
≡o

S
25 50 75 100
L#51
#elem: 8192
SO
8

L#58
#elem: 64
75 IOO
O
≡0	2 5 50 75 100
I L#65
C #elem: 2048

S

∣o
75 100
≡0	25 50 75 100
L#31
#elem: 256
Sθ
25 50 75 100
L#32
#elem: 256
Sθ
25 50 75 IbO
L#33
#elem: 256
§o
25 50 75 IOO
L#34
#elem: 256
25 50 75 IOO
L#35
131072
灯1

Kl

8
ɪn
Sθ
S

S
S
S
S



世
25 50 75 100
L#38
#elem: 128
00	25 50 75 100
L#39
#elem: 294912
SO
25 50 75 IOo
L#40
#elem: 128
00	25 50 75 100
L#41
#elem: 147456
®0	25 50 75 100
L#42
< ⅜glem: 128



⅛Q

S
S
8


20	25 50 75 100
L#45
λ #elem: 128
20	25 50 75 IOD
L#46
λ #elem: 128
§0	25 50 75 100
L#47
o #elem^^8
§0	25 50 75 100
L#48
o ⅛elem^28
§0	25 50 7⅛ 100
L#49
#elem: 32768



S
S


S





§0	25 50 75 100
L#52
#elem： 64
§0 25 SO 75 100
L#53
⅛eler∏L73728
O -	--
§0	25 50 75 100 §0	25 50 75 100
L#54	L#55
#elem： 64 #elem: 36864
sɔ
25 50 75 IOO
L#56
#elem： 64


S
IA
Sθ 25
:50 75
L#59
IOO
8

«0	25 50 75 100
-	L#66
C #elem: 32

gl
25 50 75 100
epoch gradients epoch gradients
S
S
K
LΛ
00 25 50 75
L#60
#elem: 64
IOO
sɑ
25 50 75
L#61
#elem: 64
IOO
S 6 25 50 7⅛
L#62
#elem: 64
IbO
SO 25 50
L#63
75 IOO
#elem: 8192

S
S
S
S




≡0	25 50 75 100
L#67
C #elem: 18432
O
25 50 75
L#68
#elem: 32
100
O
25 50 7⅛ 100
L#69
#elem: 9216
≡0	25 50 75 IOO
epoch∙营 ⅛ftlients
C #elem: 32

S

=0 25 50 75 100
epoch gradients



∣0 25 so 7⅛ Ibo
epoch gradients




∣0	25 50 7⅛ 100
epoch gradients
≡0	25 50 7⅛ IOO
epoch gradients
Figure 56: PCA Components Overlap with Gradient. Repeat of Fig.3on U-Net trained on PascalVOC dataset.
62
Published as a conference paper at ICLR 2022
COCOr L#2
#elerri: 2
COCO, L#5
#elem: 9216
#e em: 64
50 75 ιoo
L#10
#elem: 32
ρm: 64
#e em: 256
#elem: 256
#e em: 256
#e em: 128
#elem: 128
em: 128
Am: 128
em: 128
#e em: 64
em: 64
#Hem: 64
pm: 64
Am: 64
#e em: 32
#e em: 32
#e em: 32
Figure 57: PCA Components Overlap with Gradient. Repeat of Fig.3on U-Net trained on COCO dataset.
50 75 100
50 75 100
63
Published as a conference paper at ICLR 2022
---Vanilla FL
——LBGM
MNIST	FMNIST

75
5
2
0d
5
O 20	40
≠params
shared(×106)
O 20	40
≠params
shared(×106)
CIFAR-IO
IID
00
O 200	400
≠params
shared(×106)
PascaIVOC
Segmentation
Sso-
I 200 400	600
≠params
shared(×106)
U 5 O
V 7 5
a
Figure 58:	LBGM as a Standalone Algorithm. Experimental results in Fig.5repeated on datasets: CIFAR-10,
FMNIST, and MNIST (iid data distribution) using classifier: CNN, and dataset: PascalVOC using U-Net
architecture.
---Vanilla FL
——LBGM
SUJB」Bd#
MNIST	MNIST
IID	Non-IID
FMNIST
IID
FMNIST
Non-IID
色 1.00
×0-75
P 0-50
⅛ 0.25
(D
ω o oo⅛

.0	0.5	1.0	0.0	0.5	1.0
#ParamS	#ParamS
shared(×106)	shared(×106)
U 5 O 5
V 7 5 2
-i
.0	0.5	1.0	0.0	0.5	1.0
#ParamS	#ParamS
shared(×106)	shared(×106)
Figure 59:	LBGM as a Standalone Algorithm. Experimental results in Fig.5repeated for datasets: FMNIST
and MNIST (both iid and non-iid data distribution) using classifier: FCN.
64
Published as a conference paper at ICLR 2022
---Vanilla FL
——LBGM
SEra」rad#
MNIST
Non-IID
∞31.00
V 0.75
2so.5o
⅛ 0∙25
W 0.00l
0.25
0.00,
25 50 75 IOO
t
t
0.75
0.50
FMNIST
Non-IID
CIFAR-IO
Non-IID
0.4
CIFAR-100
Non-IID
-IlO-
OOIX)SSq
125 250 375
t
CeIebA
Regression
1.00
0.75
shared(×108) shared(×108) shared(×108)
Figure 60: LBGM as a Standalone Algorithm. Experimental results in Fig.5repeated for datasets: CIFAR-10,
CIFAR-100, FMNIST, MNIST (non-iid data distribution), and CelebA (face landmark regression task) using
classifier: Resnet18.
shared(×108)
4 Iloo
shared(×108)
-----Vanilla FL
δ Jhreshold=O.8
-----ðthreshold-ŋ g
MNIST
IID
FMNIST
IID
Sjhreshold=0.6
CIFAR-IO
IID
0 5 0 5 0
0 7 5 2 0
L
AJn
Ooo
4 2
(9s-PaJeqS
SiUeJd#
20	40
#pa rams
shared(×106)
0.75
0.50
0.25
0.00
75 IOO
40
20
0O 25 50 75 IOO
t
0.75
0.50
0.25
O 20	40
#params
shared(×106)
O 200	400
# pa rams
shared(×106)
6£hreShold=O/
δ threshold = Q2
PascaIVOC
Segmentation
75 150 225
t
75 150 225
t
200	400	600
# pa rams
shared(×106)
5 0 5 0
7 5∙∙

Figure 61:	Effect of δkthreshold on LBGM. Experimental results in Fig.6repeated for datasets: CIFAR-10,
FMNIST, and MNIST (iid data distribution) using classifier: CNN, and dataset: PascalVOC using U-Net
architecture.
65
Published as a conference paper at ICLR 2022
-----Vanilla FL
-----(5FreShold = p g
MNIST
IID
—ð?1
—^h'
MNIST
Non-IID
ireshold-Q ɛ
∣reshold-Q θ
FMNIST
IID
threshold=。/
6threshold = 0.2
FMNIST
Non-IID
U 0.75
∏3
⅛ 0.50
U 0.25
(ŋ
0.00,
Q5Q5
107 5 2
(goTX) ptυ-l BUS
SUJ B」Bd#
t
0 5 0 5
∙∙∙2
Iooo
O 5	10
# pa rams
shared(×105)
0.25
t
0.75
0.50
O 5	10
#params
shared(×105)
O 5 io
#ParamS
shared(×105)
0.75
0.50
0.25
10.0
7.5
5.0
2.5
0 0O 25 50 75 IOO
t
O 5 io
#ParamS
shared(×105)
Figure 62:	Effect of δkthreshold on LBGM. Experimental results in Fig.6repeated for datasets: FMNIST and
MNIST (both iid and non-iid data distribution) using classifier: FCN.
----Vanilla FL
---- ðthresholdɪθ g
CIFAR-IO
Non-IID
—— 6FreShold = of
----- ðthreshold = q 6
CIFAR-100
Non-IID
6FreShold = o 4
—— ðthreshold = Q 2
CeIebA
Regression
H-6-4
ʊ O O
125 250 375
~-5Q'5"
-IlO-
FoIX) SSO-
SEro」rod#
125 250 375
0.2
0.6
0.4
0.4
J 390一
0	12
≠params
shared(×108)
0	2	4
≠params
shared(×108)
≠params
shared(×108)

Figure 63:	Effect of δkthreshold on LBGM. Experimental results in Fig.6repeated for datasets: CIFAR-10,
CIFAR-100 (non-iid data distribution), and CelebA (face landmark regression task) using classifier: Resnet18.
66
Published as a conference paper at ICLR 2022
# pa rams
shared(×106)
w/o LBGM ----- w/	LBGM
MNIST	FMNIST
IMop-K	IMop-K
CIFAR-IO	MNIST
IID/Top-K	IID/ATOMO
FMNIST	CIFAR-IO
IID/ATOMO	IID/ATOMO
# pa rams
shared(×106)
#params
shared(×106)
#params
shared(×106)
# pa rams
shared(×106)
#params
shared(×106)
Figure 64: LBGM as a Plug-and-Play Algorithm.. Experimental results in Fig.7repeated for dataset: CIFAR-10,
FMNIST, and MNIST (iid data distribution) using classifier: CNN.
——w/o LBGM ——w/ LBGM
# pa rams
accuracy shared (×105) accuracy
MNIST
shared(×105)
MNIST
shared(×105)
FMNIST
shared(×105)
FMNIST	MNIST
Non-IIDfTop-K	IID/ATOMO
shared(×105) shared(×105)
MNIST
shared(×105)
FMNIST	FMNIST
IID/ATOMO Non-Il D/ATO MO
# pa rams	# pa rams
shared(×105) shared(×105)
Figure 65:	LBGM as a Plug-and-Play Algorithm.. Experimental results in Fig.7repeated for datasets: FMNIST
and MNIST (both iid and non-iid data distribution) using classifier: FCN.
67
Published as a conference paper at ICLR 2022
CIFAR-IO
Non-IIDZTop-K
Auejnuue
- - - -ft . . . .
5 O 5 O 8 6 4 2
1 1 a α α α
(90Ipa∣eι∣s >ura⅛8ra
SE
)	5	10	15
# pa rams
shared(×106)
w/o LBGM --- w/ LBGM
CIFAR-100
Non-IIDZTop-K
shared(×106)
# pa rams
CeIebA
Top-K
t 1 1 O Λ
OOI-Sso-
OOI-Ss-
O
O
O
O
10
7
5
2
O
O
O
O
O
O
CeIebA
ATOMO
CIFAR-100
Non-IIDZATOMO
CIFAR-IO
Non-IIDZATOMO
125 250 375 500
5	10
#params
shared(×106)
25 50 75 IOO
ι
# pa rams
shared(×106)
Figure 66:	Effect of δkthreshold on LBGM. Experimental results in Fig.7repeated for datasets: CIFAR-10,
CIFAR-100 (non-iid data distribution), and CelebA (face landmark regression task) using classifier: Resnet18.
w/o LBGM
MNIST
IID/SignSGD
----w/ LBGM
FMNIST
IID/SignSGD
CIFAR-IO
IID/SignSGD
00%,0	0.2	0.4
#bits
shared(×108)
Figure 67: Experimental results in Fig.8repeated for dataset:	CIFAR-10, FMNIST, FMNIST (iid data
distribution) using classifier: CNN.
68
Published as a conference paper at ICLR 2022
w/o LBGM ---- w/ LBGM
s--q#
MNIST
IID/SignSGD
'05 O 5
Q∙5 2
Iooo
MNIST
Non-IIDZSignSGD
0.8∏-------------
0.6
0.4
0.2
00O	25 50 75 IOO
FMNIST
IID/SignSGD
FMNIST
Non-IIDZSignSGD
00O	25 50 75 IOO
0.00(
W loo
7 0.75
0.50
0.25
0.00j
L-
tŋ
in
25 50 75 IOO
000O	25 50 75 IOO
t
25 50 75 IOO
'05 O 5
Q∙5 2
Iooo
0°%,0	0.5	1.0
#bits
shared(×106)
0 %,o	0.5 ι,o o o%.o 0.5 ι,o
#bits	#bits
shared(×106)	shared(×106)
#bits
shared(×106)
Figure 68:	Experimental results in Fig.8repeated for dataset: FMNIST and MNIST (both iid and non-iid data
distribution) using classifier: FCN.
---w/o LBGM ----- w/ LBGM
s--q#
CIFAR-IO
Non-IIDZSignSGD
0.8ι-----------1
CIFAR-100	CeIebA
Non-IIDZSignSGD	SignSGD
6 4 2 0 O O O O 8 6 4 2
000015105 æ æ æ æ
AUeJnUUe (901X) PaJeIlS AUeJmUe
20
o.o^---——―~~
O 50 IOO 150
#bits
shared(×106)
(°IX) Ss-
shared(×106)
50	100
#bits
shared(×106)
Figure 69:	Effect of δkthreshold on LBGM. Experimental results in Fig.8repeated for datasets: CIFAR-10,
CIFAR-100 (non-iid data distribution), and CelebA (face landmark regression task) using classifier: Resnet18.
69
Published as a conference paper at ICLR 2022
---Vanilla FL
——LBGM
MNIST
Non-IID
FMNIST
Non-IID
CIFAR-IO	CeIebA
Non-IID	Regression
(90IrPaJroqs
SE」d#
L0-0-0-
O 20	40
≠params
shared(×106)
O 20	40
≠params
shared(×106)
O 200	400 O 100	200
≠params	≠params
shared(×106)
shared(×106)
Figure 70:	Experimental results in Fig.5repeated for dataset: CIFAR-10, FMNIST, FMNIST (non-iid data
distribution) and CelebA (face landmark regression taks) using classifier: CNN under 50% client sampling for
both Vanilla FL and LBGM.
---Vanilla FL
——LBGM
MNIST
FMNIST
0 5 0 5
。7∙2
Iooo
AUeJrDU
oooo
I
00
Ooo
4 2
(90IrpajqS
SEald⅜
0 5 0 5
Iooo
20
40
5 0 5
7∙∙
Ooo
≠params
shared(×106)
I
5 0 5 0
7∙∙∙
Oooo
75
0 00⅛
≠params
shared(×106)
CIFAR-IO
IID
00
20	40
≠params
shared(×106)
D
D
5
2
Figure 71:	Experimental results in Fig.5repeated for dataset: CIFAR-10, FMNIST, FMNIST (iid data
distribution) using classifier: CNN under 50% client sampling for both Vanilla FL and LBGM.
70