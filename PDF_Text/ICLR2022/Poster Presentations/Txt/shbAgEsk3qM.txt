Published as a conference paper at ICLR 2022
Understanding and Leveraging Overparame-
terization in Recursive Value Estimation
Chenjun Xiao1,2,*, Bo Dai1, JinCheng Mei1, Oscar Ramirez1, Ramki Gummadi1,
Chris Harris1 & Dale Schuurmans1,2
1 Google
2Department of Computing Science, University of Alberta
Ab stract
The theory of function approximation in reinforcement learning (RL) typically
considers low capacity representations that incur a tradeoff between approximation
error, stability and generalization. Current deep architectures, however, operate in
an overparameterized regime where approximation error is not necessarily a bottle-
neck. To better understand the utility of deep models in RL we present an analysis
of recursive value estimation using overparameterized linear representations that
provides useful, transferable findings. First, we show that classical updates such
as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to
different fixed points than residual minimization (RM) in the overparameterized
linear case. We then develop a unified interpretation of overparameterized lin-
ear value estimation as minimizing the Euclidean norm of the weights subject to
alternative constraints. A practical consequence is that RM can be modified by
a simple alteration of the backup targets to obtain the same fixed points as FVI
and TD (when they converge), while universally ensuring stability. Further, we
provide an analysis of the generalization error of these methods, demonstrating
per iterate bounds on the value prediction error of FVI, and fixed point bounds for
TD and RM. Given this understanding, we then develop new algorithmic tools for
improving recursive value estimation with deep models. In particular, we extract
two regularizers that penalize out-of-span top-layer weights and co-linearity in
top-layer features respectively. Empirically we find that these regularizers dramati-
cally improve the stability of TD and FVI, while allowing RM to match and even
sometimes surpass their generalization performance with assured stability.
1	Introduction
Model-free value estimation remains a core method of reinforcement learning (RL), lying at the
heart of some of the most prominent achievements in this area (Mnih et al., 2015; Bellemare et al.,
2020). Such success appears paradoxical however, given that value estimation is subject to the deadly
triad: any value update that combines off-policy estimation with Bellman-bootstrapping and function
approximation diverges in the worst case (Sutton and Barto, 2018). Without additional assumptions it
is impossible to ensure the viability of iterative value estimation schemes, yet this remains a dominant
method in RL—its popularity supported by empirical success in many applications. Such a sizable
gap between theory and practice reflects limited understanding of such methods, how they behave in
practice, and what accounts for their empirical success (van Hasselt et al., 2018; Achiam et al., 2019).
Decomposing the deadly triad indicates that off-policy estimation and bootstrapping are difficult to
forego: off-policy estimation is supported by the empirical effectiveness of action value maximization
and replay buffers, while Bellman-bootstrapping provides significant advantages over Monte Carlo
estimation (Sutton, 1988). On the other hand, our understanding of the third factor, the relationship
between function representation and generalization, has evolved dramatically in recent years. Al-
though it was once thought that restrictive function approximation—representations that lack capacity
to fit all data constraints—might be essential for generalization, we now know that this view is
oversimplified (Belkin et al., 2019). The empirical success of deep learning (Krizhevsky et al., 2012),
*Work performed while an intern at Google Brain. Email: {chenjun,daes}@Ualberta.ca
1
Published as a conference paper at ICLR 2022
extremely large models (Brown et al., 2020) and associated theoretical advances (Jacot et al., 2018)
have made it clear that gradient-based training of overparameterized models embodies implicit biases
that encourage generalization even after all data constraints are fit exactly. This success suggests a
new opportunity for breaking the deadly triad: by leveraging overparameterized value representations
one can avoid some of the most difficult tradeoffs in value-based RL (Lu et al., 2018).
The use of overparameterized deep models in value-based RL, however, still exhibits mysteries in
stability and performance. Although one might expect larger capacity models to improve the stability
of Bellman-bootstrapping, in fact the opposite appears to occur (van Hasselt et al., 2018). Our own
empirical experience indicates that classical value estimation with deep models eventually diverges
in non-toy problems. It has also been shown that value updating leads to premature rank-collapse
in deep models (Kumar et al., 2021), coinciding with instability and degrading generalization. In
practice, some form of early-stopping is usually necessary to obtaining successful results, a fact
that is not often emphasized in the literature (Agarwal et al., 2021). Meanwhile, there is a long
history of convergent methods being proposed in the RL literature—starting from residual gradient
(Baird, 1995), to gradient-TD (Sutton et al., 2008; Maei et al., 2009), prox gradient TD (Liu et al.,
2015; 2016), and emphatic TD (Yu, 2015; Sutton et al., 2016)—yet none of these has demonstrated
sufficient generalization quality to supplant unstable methods. The current state of development
leaves an awkward tradeoff between stability and generalization. A stable recursive value estimation
method that ensures generalization quality with overparametrization remains elusive.
In this paper we investigate whether overparameterized value representations might allow the stability-
generalization tradeoff to be better managed, enabling stable estimation methods that break the deadly
triad and generalize well. We first consider policy evaluation with overparameterized linear value
representations, a simplified setting that still imposes the deadly triad (Zhang et al., 2021). Here
we find that alternative updates, such as temporal differencing (TD), fitted value iteration (FVI) and
residual minimization (RM) converge to different fixed points in the overparameterized case (when
they converge), even though these updates share a common fixed point when the approximation
error is zero and there are no extra degrees of freedom (Dann et al., 2014). That is, these algorithms
embody implicit biases that only become distinguishable in the overparameterized regime. From this
result, we observe that the fixed points lie in different bases, which we use to develop a unified view
of iterative value estimation as minimizing the Euclidean norm of the weights subject to alternative
constraint sets. This unification allows us to formulate alternative updates that share fixed points
with TD and FVI but guarantee stability without requiring regularization or prox constraints (Zhang
et al., 2021). Next, we analyze the generalization performance of these algorithms and provide a
per-iterate bound on the value estimation error of FVI, and fixed point bounds on the value estimation
error of TD. From these results, we identify two novel regularizers, one that closes the gap between
RM and TD and another that quantifies the effect of the feature representation on the generalization
bound. We deploy these regularizers in a realistic study of deep model training for optimal value
estimation and observe systematic stability and generalization improvements. We also observe that
the performance gap between RM and TD can be closed and in some cases eliminated.
2	Related work
Value estimation has a lengthy history throughout RL research. Our main focus is on off-policy value
estimation with parametric function representations and iterative (i.e., gradient based) updates. We
do not consider exploration nor full planning problems (i.e., approximately solving an entire Markov
decision process (MDP)) in the theoretical development, but instead focus on offline value estimation;
however, we do apply the findings to policy improvement experiments in the empirical investigation.
Dann et al. (2014) provide a comprehensive survey of value estimation with parametric function
representations. Significant attention has been focused on underparameterized representations where
backed up values are not necessarily expressible in the function class, however we focus on the
overparameterized case where any backed up values can be assumed to be exactly representable with
respect to finite data. This change fundamentally alters the conclusions one can draw about algorithm
behavior, as we see below. One of the key consequences is that classical distinctions (Scherrer, 2010;
Dann et al., 2014) between objectives—e.g., mean squared Bellman error (MSBE), mean squared
projected Bellman error (MSPBE), mean squared temporal difference error (MSTDE), and the norm
of the expected TD update (NEU)—all collapse when the Bellman errors can all be driven to zero.
Despite this collapse, we find that algorithms targetting the different objectives—TD and LSTD for
MSPBE (Sutton, 1988; Bradtke and Barto, 1996) and RM without double sampling (DS) for MSTDE
2
Published as a conference paper at ICLR 2022
(Maei et al., 2009; Dann et al., 2014)—converge to different fixed points given overparameterization,
even when they ultimately satisfy the same set of temporal consistency constraints.
It is well known that classical value updates can diverge given off-policy data and parametric function
representations (Baird, 1995; Tsitsiklis and Van Roy, 1996; 1997). The stability of these methods has
therefore been studied extensively with many mitigations proposed, including restricting the function
representation (Gordon, 1995; Szepesvari and Smart, 2004) or adjusting the representation to ensure
contraction (Kolter, 2011; Ghosh and Bellemare, 2020; Wang et al., 2021b), or modifying the updates
to achieve convergent variations, such as LSTD (Bradtke and Barto, 1996; Yu, 2010), FVI (Ernst et al.,
2005; Munos and Szepesvari, 2005; SzePeSvari and Munos, 2008; Lizotte, 2011) or the introduction
of target networks (Mnih et al., 2015; Lillicrap et al., 2016; Zhang et al., 2021; Carvalho et al., 2020).
Others have considered modified the updates to combat various statistical inefficiencies (van Hasselt,
2010; Weng et al., 2020; Konidaris et al., 2011). Another long running trend has been to consider
two time-scale algorithms and analyses, reflected in gradient-TD methods (Sutton et al., 2008; Maei
et al., 2009), prox gradient TD (Liu et al., 2015; 2016), primal-dual TD (Dai et al., 2017; Du et al.,
2017), and emphatic TD (Yu, 2015; Sutton et al., 2016). Beyond mere convergence, however, we
discover a greater diversity in fixed points among algorithms in the overparameterized case, which
play a critical but previously unacknowledged role in generalization quality.
The fact that minimizing MSPBE via TD methods still dominates practice appears surprising given the
theoretical superiority of other objectives. It has been argued, for example, that direct policy gradient
methods (Sutton et al., 1999) dominate minimizing Bellman error objectives (Geist et al., 2017). Even
among Bellman based approaches, it is known that MSBE can upper bound the value estimation error
(MSE) whereas MSPBE cannot (Kolter, 2011; Dann et al., 2014), yet MSPBE minimization (via TD
based methods) empirically dominates minimizing MSBE (via residual methods). This dominance
has been thought to be due to the double sampling bias of residual methods (Baird, 1995; Dann et al.,
2014), but we uncover a more interesting finding that their fixed points lie in different bases in the
overparameterized setting, and that reducing this difference closes the performance gap.
We analyze the convergence of classical updates given offline data and provide associated generaliza-
tion bounds, with the primary goal of understanding the discrepancy between previous theory and the
empirical success of TD/FVI versus RM. Although this theory sheds new light in exploitable ways, it
cannot overcome theoretical limits on offline value estimation, such as lower bounds on worst case
error that are exponential in horizon length (Wang et al., 2021a;b; Zanette, 2021; Xiao et al., 2021).
We analyze the convergence of the expected updates, extendible to the stochastic case using known
techniques (Yu, 2010; Bhandari et al., 2018; Dalal et al., 2018; Prashanth et al., 2021; Patil et al.,
2021). We expand the coverage of these earlier works by including alternative updates and focusing
on the overparameterized case, uncovering previously unobserved differences in the fixed points.
There is a growing body of work on linear value estimation and planning that leverages the insight
of (Parr et al., 2008; Taylor and Parr, 2009) that linear value estimation is equivalent to linear
model approximation. A number of works have strived to obtain provably efficient algorithms for
approximating the optimal policy values in this setting, but these generally rely on exploration or
strong assumptions about data coverage (Song et al., 2016; Yang and Wang, 2019; Duan et al., 2020;
Agarwal et al., 2020; Jin et al., 2020; Yang et al., 2020; Hao et al., 2021) that we do not make. Instead
we study linear value estimation to gain insight, but rather than focus on linear planning we leverage
the findings to improve the empirical performance of value estimation with deep models.
3	Preliminaries
Notation We let R denote the set of real numbers, In an n × n identity matrix, and I the indicator
function. For a finite set X, we use ∆(X) to denote the set of probability distributions over X. For
a vector μ We let ∣supp(μ) | denote the size of the support of μ (i.e., the number of nonzero entries
in μ). For a matrix A ∈ Rn×m, we let At be the Moore-Penrose pseudoinverse of A, k Ak be its
spectral norm, and λmax(A) and λmin(A) be its maximum and minimum non-zero eigenvalues. We
also use ∏a = AtA to denote the projection matrix to the row space of A. For a vector X ∈ Rd, we
let ∣∣xk be its l2 norm and IlxkA = XxτAx be the associated norm for a positive definite matrix A.
We also use diag(x) ∈ Rd×d to denote a diagonal matrix whose diagonal elements are x.
Markov reward processes We consider the problem of predicting the value of a given stationary
policy in a Markov Decision Process (MDP). For a stationary policy, this problem can be formulated
3
Published as a conference paper at ICLR 2022
in terms of a Markov reward process M = {S, P, r, γ}, such that S is a finite set of states, r : S → R
and P : S → ∆(S) are the reward and transition functions respectively, and γ ∈ [0, 1) is the discount
factor. Let S = |S | be the number of states. For a given state s ∈ S, the function r(s) gives the
immediate reward incurred at s, while P(∙∣s) gives the next-state transition probability of s. The
value function specifies the future discounted total reward obtained from each state, defined as
∞
v(s) = E X γtr(st)s0 = s .	(1)
To simplify the presentation we identify functions as vectors to allow vector-space operations: the
value function v and reward function r are identified as vectors v, r ∈ RS, the transition P is
identified as an S × S transition matrix, where the s-th row Ps specifies the transition probability
P(∙∣s) of state s. These definitions allow the value function to be expressed using Bellman,s equation
v = r + γPv .	(2)
Linear Function Approximation Itis usually not possible to consider tabular value representations
in practice, since the state set is usually combinatorial or infinite. In our theoretical development we
focus on linear function approximations, where v is approximated by a linear combination of features
describing states; i.e., v(s) ≈ φ(s)>θ, where θ ∈ Rd is a parameter vector and φ : S → Rd maps a
given state S ∈ S toa d-dimensional feature vector φ(s) ∈ Rd. We let Φ ∈ RlSl×d denote the feature
matrix, with the s-th row corresponding to the feature vector φ(s), so that the value approximation
can be written as v ≈ Φθ. We assume kφ(s)k ≤ 1 for any s ∈ S, and for simplicity we also assume
that there is no redundant or irrelevant features in the feature map; that is, Φ is full rank.
3.1	Batch Value Estimation
We consider batch mode (“offline”) estimation of the value function. Let μ ∈ ∆(S) be an arbitrary
probability distribution over states and Dμ = diag(μ). The data set consists of {si,ri, si}n=ι
transition tuples, which are generated by S 〜 μ,r = r(si), Si ~ P(∙∣Si). Let n(s) = ∑n=ι I(Si =
s) be the number of counts of state s. We define the empirical data distribution matrix D = diag(μ),
where μ(s) = n(s)/n is the empirical data distribution over states. The goal is to estimate the value
function by finding a weight vector θ ∈ Rd that minimizes the value prediction error,
E(θ) = kΦθ - vkDμ = Xs∈S μ(s)(φ(s)>θ -V(S))2 .	(3)
Let P be the empirical transition matrix, where the S-th row represents the estimated transition of
state s: if n(S) > 0, Ps(SO) = P3 I(Si = s, Si = s')/h(s); if n(S) = 0, Ps(SO) = 0 for all s0 ∈ S.
The empirical mean squared Bellman error on the batch data can be defined as
MSBE(θ) = 1∣∣r + YPΦθ - Φθ∣∣D .	(4)
Over vs Underparameterized Features In this paper we are particularly interested in the over-
parameterized regime d > ∣supp(μ)∣ where one can exactly satisfy the temporal consistencies on
all transitions in the batch data set, achieving zero Bellman error. (Obviously this would also be
possible if d = ∣supp(∕^) | but the strictly overparameterized case is more interesting, as we will see
below.) By contrast, in the underparameterized regime d < ∣supp(μ)∣, one can only expect to find an
approximate solution that in general has nonzero Bellman error.
We consider three core algorithms in our analysis, covering major classical approaches.
Residual Minimization (RM) RM directly minimizes the empirical mean squared Bellman error
Eq. (4) (MSBE) (Baird, 1995). The gradient update (Dann et al., 2014) can be expressed as
Θt+1 = θt- η(Φ - YPΦ)>D(Φθt - (r + YPΦθt)),	(5)
where θt is the estimated weight at step t, and η is the learning rate. As a gradient descent method, the
convergence of this update is robust, and applies to both linear and nonlinear function approximation.
Temporal Difference (TD) Learning The simplest variant of TD (Sutton, 1988), known as TD(0),
also updates weights iteratively using transition data to approximate the value function. Let θt be the
weight vector at step t. Then the so-called “semi-gradient” of Eq. (4) is used to compute the update,
Θt+1 = θt - ηΦ>D(Φθt - (r + YPΦθt)) ,	(6)
4
Published as a conference paper at ICLR 2022
where η is the learning rate. From Eq. (6), it is clear that in the underparameterized (d < lsupp⑷I)
regime, if the system converges, it must converge to parameters θD such that
Φ>Dr - Φ>D(Φ - YPΦ)θD = 0 ⇒ θD = (Φ>D(Φ - YPΦ))-1Φ>Dr , (7)
where θD is the TDfixed point. That is, given limited representational power, the TD fixed point
minimizes the squared projected Bellman error (MSPBE) by solving the projected Bellman equation:
ΦθD =∏D (r + YPΦθD) ,	(8)
such that ΠΦD = Φ(Φ>DΦ)-1Φ>D is a weighted projection matrix. It is well-known that TD(0)
can diverge if the data sampling distribution μ is not the stationary distribution of the Markov process.
One can still compute the TD fixed point directly using batch data, for example using the LSTD
algorithm (Bradtke and Barto, 1996), but this requires computation on the order of O(d2) compared
to O(d) of the iterative update algorithm Eq. (6). The value prediction error of TD is discussed in
(Tsitsiklis and Van Roy, 1997; Kolter, 2011; Dann et al., 2014; Bhandari et al., 2018).
Fitted Value Iteration (FVI) FVI iteratively updates the weight vector by solving a regression
problem where the target is constructed from the current estimate (Ernst et al., 2005; Dann et al.,
2014), which is also known as approximate dynamic programming (Sutton and Barto, 2018). In
particular, given the current weight θt at iteration t, the objective Eq. (9) is minimized to obtain θt+1 ,
FVIt(θ) = 2∣∣r + YPΦθt- Φθ∣∣D .	(9)
A simple calculation shows the TD fixed point matches the fixed point of FVI whenever θ0 is in
the row-span of D Φ. Although convergence of FVI can be established under strong conditions
(Szepesvari and Munos, 2008), the algorithm can be quite unstable in the general batch setting (Chen
and Jiang, 2019; Wang et al., 2021b).
4	Over-Parameterized Linear Value Function Approximation
In this section, we study the convergence properties of the value estimation algorithms introduced
in Section 3.1 in the overparameterized regime where d > ∣supp(μ^)∣. To faciliate analysis, We
first introduce additional notation to simplify the derivations. Let {xi }ik=1 denote the states in the
support of μ, such that n(xi) > 0 for all i = {1,..., k} and k = ∣supp(μz)∣. Define a mask matrix
H ∈ Rk×lSl and a truncated empirical data distribution matrix Dk ∈ Rk×k according to
-1>ι]	PXxI)	-
H =	.	,	Dk =	...	,	(10)
Λ>kJ	l_	μ(Xk L
where 1xi ∈ {0, 1}|S| is an indicator vector such that φ(xi) = Φ>1xi. We can then translate between
the full distribution and its support via the following.
Proposition 1. The empirical data distribution matrix D can be decomposed as D = H>DkH.
Let M = HΦ, N = HPΦ and R = Hr denote the state features, the expected next state features
under the empirical transitions, and the rewards on the support of the data distribution respectively.
Overparameterized Residual Minimization We first study the convergence of RM given a fixed
D. First note that the update Eq. (5) can be re-written as
θt+1 = (Id - η(M - YN)>Dk(M - YN))θt + η(M - YN)>DkR.	(11)
In the overparameterized regime, one can easily verify that there are infinitely many solutions θ ∈ Rd
satisfying (M - YN)θ = R. The gradient of Eq. (11) is zero at any of these solutions, which
implies that RM can have infinitely many fixed points. However, given that RM minimizes the MSBE
objective via gradient descent, as we show in the following theorem, the RM update initialized from
θ0 = 0 will converge to a unique fixed point.
Theorem 1. With η ≤ .+)产 and Startingfrom θo = 0, RM converges to θRM = (M 一 YN)* R.
Remark 1. For simplicity we present the fixed points ofRM and TD starting from θ0 = 0. The fixed
points given an arbitrary initial weight vector θ0 ∈ Rd are shown in Appendices A.1 and A.2.
This result parallels similar findings in the supervised learning literature, that training overparam-
eterized deep models with gradient descent (or related algorithms) encodes implicit regularization
5
Published as a conference paper at ICLR 2022
Figure 1: An illustrative example showing
the spectrum of W with k = 2 and d = 3.
M = [φ1, φ2]>. Without loss of generality,
let φ1 = (cos τ, sinτ, 0) and φ2 = (1, 0, 0).
N = [Φ1,Φ2]>, where Φ1 = Φ2, Φ2 =
(—cos τ, √22 Sin τ, √22 Sin T). Then W 二
[[0,1]>, [√22, — (1 + √22 cos τ)]>]. Clearly,
the spectral norm of W increases as the
angle τ between φ1 and φ2 decreases.
that drives the model solution to particular outcomes in the overparameterized regime (Soudry et al.,
2018; Gunasekar et al., 2018; Neyshabur et al., 2019). Moreover, this implicit regularization is
often associated with generalization benefits. However, unlike the case for supervised learning, RM
solutions do not often generalize well. Below we uncover a key difference between the RM fixed
point and those of TD and FVI that sheds new light on the source of generalization differences.
Overparameterized TD Learning We next consider the convergence properties of the TD(0)
update in the overparameterized setting. First, rewrite the TD(0) update formula Eq. (6) as
θt+1 = (Id — ηM>Dk(M — γN))θt + ηM>DkR.	(12)
Similar to RM, in the overparameterized regime any solutions θ ∈ Rd that satisfy (M — γN)θ = R
are the fixed points of Eq. (12), which implies an infinite set of fixed points. This is quite unlike
the underparameterized case where there is a unique TD fixed point Eq. (7) given by the solution of
projected Bellman equation. However, we now show that in the overparameterized setting, similar to
solving RM using gradient descent, TD also encodes an implicit bias toward a particular fixed point.
This of course requires TD to converge, which can be assured by a simple condition. Let W = NM1,
which has a geometric interpretation that we will exploit later in Section 5. Observe that
N = N∏m + N(Id — ∏m) = NMtM + N(Id — ∏m) = WM + N(Id — ∏m), (13)
i.e., N can be decomposed into its projection onto the row-span ofM plus a perpendicular component.
Eq. (13) shows that W projects N onto the row space of M ; see Fig. 1 for an illustration. We refer
to W as the core matrix since its norm determines the convergence of TD.
Theorem 2.	Choosing η <(i+，)k>k and Startingfrom θo = 0 ,if IlW ∣∣ < ɪ, TD(0) converges to
θTD = Mt(Ik — YW)-1R. If Il WIl ≥ 1 there is an initial θo for which TD(0) does not converge.
A few key observations. First, note that the RM fixed point in Theorem 1 and the TD fixed point in
Theorem 2 are not identical. That is, the different value estimation algorithms continue to demonstrate
different preferences for fixed points, but in the overparameterized setting these differences are implicit
in the algorithms and cannot be captured by the MSBE versus MSPBE objectives, since both are zero
for any θ that satisfies (M — γN)θ = R. Second, the fixed point of TD lies in different basis than
RM. That is, θTD lies in the row space of the state features M, whereas θRM lies in the row space
of the residual features M — γN, and these two spaces are not identical in general. We revisit the
significance of this difference below, but intuitively, the parameter vector θ is being trained to predict
values rather than temporal differences, and the future test states from which value predictions are
made will tend to be closer to the space spanned by M than M — γN.
Overparameterized Fitted Value Iteration Finally, we consider the convergence of FVI. Recall
that at iteration t, FVI solves the least squares problem Eq. (9) to compute the next weight vector.
Using the notation established above, the normal equations for this problem can be expressed as
M>DkMθ = M>Dk(R+γNθt), but this system cannot be directly used to compute the solution
since M>DkM is not invertible. Furthermore, just like RM and TD, any θ ∈ Rd that satisfies
(M — γN)θ = R is a fixed point of FVI. If one solves the least squares problem Eq. (9) using
gradient descent, it is known (Bartlett et al., 2021; Soudry et al., 2018) that the optimization converges
to the minimum norm solution
θt+1 = Mt(R + γNθt) .	(14)
Interestingly, by choosing this solution, each iteration of FVI corresponds to applying a linear backup
on the current value estimate, where the backup operator is defined by the core matrix.
6
Published as a conference paper at ICLR 2022
Definition 1. Define the core matrix linear operator TW by TWν = R + γWν for any ν ∈ RS.
Similar results for the case with underparameterized linear model have been discussed in (Parr et al.,
2008). Using this operator we can characterize the convergence condition of FVI, reaching the
conclusion that whenever TW is a non-expansion, FVI converges to the same fixed point as TD.
Theorem 3.	Let θ0 be the initial weight and θt ∈ Rd be the output of FVI at iteration t. We have
θt+ι = M*TW(R + γNθo).	(15)
Furthermore, given that ∣∣ W∣∣ < ∖/γ, the algorithm converges to θTD = M*(Ik - YW)-1 R. If
IlWk ≥ γ there is an initial θo for which FVI does not converge.
4.1	Unified View of Overparameterized Value Estimators
We now show that the convergence points above can be characterized as solutions to related con-
strained optimization problems, providing a unified perspective on the respective algorithm biases.
Theorem 4.	θRM is the solution of the following constrained optimization,
inf 2 ∣∣θk2 s.t. Mθ = R + γNθ,	(16)
θ∈Rd 2
and θTD is the solution of the following constrained optimization,
inf 1 ∣∣θk2 s.t. Mθ = R + γNθ, null(M )θ = 0.	(17)
θ∈Rd 2
That is, the convergence points of RM, TD and FVI in the overparameterized case can all be seen
as minimizing the Euclidean norm of the weights θ subject to satisfying the Bellman constraints
Mθ = R + γNθ, where TD and FVI implicitly add the additional constraint that θ must lie in
the row span of M; moreover, this is the only constraint that differentiates TD from RM. From this
perspective, the algorithms can all be seen as iterative procedures for solving a particular form of
quadratic program, when they converge. Of course, proper constrained optimization techniques would
be able to stably compute solutions in scenarios where TD or FVI diverge (Boyd and Vandenberghe,
2004), but a more direct way to ensure convergence is implied by the following corollary.
Corollary 1. θTD is also the solution of the following constrained optimization,
inf 2∣∣θk2 s.t. Mθ = R + YN∏mθ .	(18)
θ∈Rd 2
Note that the right hand side of the constraint simply pre-projects next state value predictions onto
the row space of M before determining the Bellman backed up value. This allows a novel objective
to be formulated whose minimizer recovers the same fixed point as TD,
MSCBE(θ) = 2 ∣∣R + YN∏mθ - Mθ∣D ,	(19)
which stands for mean squared corrected Bellman error. Note that MSCBE is not identical to MSPBE
because the projection is applied before not after the Bellman backup. Gradient descent minimization
of MSCBE yields the same fixed point as θTD, which is essentially equivalent to applying RM to
corrected target values while ensuring stability. Note also that in the linear case the projection matrix
ΠM only needs to be precomputed once.
4.2 Value Prediction Error B ounds
One can also establish generalization bounds on the value estimation error of these methods in the
overparameterized regime. We first provide a finite time analysis of the value prediction error of FVI.
Theorem 5. Let Σ = M> Dk M be the empirical covariance matrix, and θt be the output of FVI
StartingfrOm θo as defined in Theorem 3. Thenfor any θ* ∈ arg mi□e∈Rd E(θ),
E (θt)-E (θ*)
t-1	2
≤ kλm⅛) ((ε2 + σ2)∣∣X(YW )i∣∣ +∣I(YW 尸『阿2|仇-θ*k2) + 2 ⑹丘-口. ,	(20)
i=0
where ε = ∣∣N(Id - ∏m)θ*∣ andσ = ∣∣H(P - P)Vk.
Intuitively, ε measures the length of next-state features along the direction θ*, and σ is the expected
value prediction error under the empirical transition model, which can be bounded using standard
7
Published as a conference paper at ICLR 2022
concentration inequalities. The proof of this theorem is given in Appendix A.5. Observe that for
any step t ≥ 1, the output of FVI θt is within the row-span of M. This allows one to decompose
the prediction error into a component within the row-span, controlled by leveraging the core matrix
linear operator TW, and an orthogonal component that can be bounded by ∣∣θ*kId -∏^.
Under the convergence conditions of Theorems 2 and 3, we also have the following generalization
bound for the value prediction error of θTD .
Corollary 2. Suppose that ∣W ∣ ≤ 1, and the value of any s ∈ S is bounded by v(s) ∈ [0, vmax].
For any θ* ∈ argmi□θ∈Rd E(θ),
E[E(θTD)] ≤
Ylog(∣S∣∕δ)	I 47E[kθ*kId-∏M] I δι,
ʌ	—+-	ʌ	-+- δ V1
nminE[λmm(Σ)](1 - Y )4	E%m (Σ)](1 — Yy
(21)
where nmin = mins:n(s)>0 n(s) is the minimum counts given the data set.
This result automatically implies the requirements for ensuring offline generalization, accounting
both for distribution shift (Wang et al., 2021b) and policy completeness (Munos and Szepesvari,
2005; Duan et al., 2020) in feature space. In particular, for Eq. (20) and Eq. (21), we characterize
the distribution shift using well known concentration bounds in Appendix A.6, which leads to the
denominators kλmin(Σ) and nminE[λmin(Σ)] respectively. In addition, we explicitly characterize
the misalignment between the features of current states and next states using the core matrix, which
can be used to bound misalignment between values, replacing the feature completeness assumption.
We note that if the convergence condition cannot be satisfied, that is when ∣W ∣ ≥ 1∕Y, the estimation
error could be arbitrarily large. The sources of value estimation error are explicit in Corollary 2.
The first term measures the error due to sampling (statistical error), while the second term considers
out-of-span components of the optimal weight vector θ* with respect to M (approximation error).
The smallest eigenvalue of the empirical covariance matrix E[λmin(Σ)], as well as the length of
the orthogonal components E[∣θ*∣Id-∏^], can both be controlled using classical techniques for
concentration properties of random matrix. In Appendix A.7 we present the exact approach for
bounding these two terms. Furthermore, by Corollary 1, one can also apply Corollary 2 to an
algorithm that directly optimizes MSCBE. Although a solution of Eq. (18) must exist, its value
prediction error can be arbitrarily large given that ∣W ∣ ≥ 1∕Y. This also connects to a similar result
for the TD fixed point that minimizes MSPBE in the underparameterized regime (Kolter, 2011).
5	Regularizers for Deep Reinforcement Learning Algorithms
For tractability, the theory in prior sections assumes fixed representations with a linear parameteriza-
tion on only the final layer parameters of the value function. However, in practice, deep RL algorithms
also learn the representations in an end-to-end fashion. Inspired by the linear case, we now identify
two novel regularizers that are applicable more generally—one that closes the gap between RM and
TD inspired by the unified view of different fixed points, and another that quantifies the effect of
feature representation on the generalization bound.
Two-Part Approximation Most deep RL algorithms rely on approximating values with a deep
neural network Qω that predicts the future outcome of given state-action pair (Mnih et al., 2015;
Kalashnikov et al., 2018; Lillicrap et al., 2016). In practice, Qω is trained by TD learning that
minimizes the objective Ps 0(r(s,a)+YQ ω (s,a)-Qω (s,a)),where Q ω (s,a) is known as the target
network to increase the learning stability. We view Qω as a two part-approximation with ω = (φ, θ),
where the output of the penultimate layer is referred as the feature mapping φ, the weight of last fully
connected layer is referred as θ, and the Q-function is approximated by Qω (s, a) = φ(s, a)>θ. Our
goal is to define regularizers on φ and θ that can be effectively applied to practical algorithms.
The first regularizer directly takes inspiration from Theorem 4: by restricting the linear weight θ
within the row space of M (now defined by exited (s, a) pairs in the data), RM finds the same fixed
point as TD. We implement this idea by penalizing the norm of the perpendicular component of θ,
Rθ = ∣θ - ΠMθ∣ ,	(22)
In practice we compute this regularizer for each minibatch of data. The projection step is computed
by a least squares algorithm with an additional l2 regularization for numerical stability.
8
Published as a conference paper at ICLR 2022
Figure 2: We show the results with proposed regularization compared to the baseline algorithms.
The algorithms are trained using a fixed offline data set collected by random initialized policies. The
x-axis shows the training iterations (in thousands) and y-axis shows the performance. All plots are
averaged over 100 runs. The shaded area shows the standard error.
The second regularizer is designed to address the effect of the feature representation on convergence
and value prediction error. In particular, Theorems 2 and 3 show that TD and FVI converge if the
spectral norm of W be upper bounded by 1∕γ, which by Theorem 5 will also reduce the bound on
generalization error. Hence, it is natural to penalize the norm of this matrix using standard automatic
differentiation tools. However, such an approach is prone to numerical difficulty, as it involves
differentiation through a matrix pseudo inverse. We instead propose an alternative regularizer inspired
by the geometric interpretation of the core matrix Eq. (13): recall from Fig. 1 that W can be viewed
as the weights that project N onto the row space of M . To ensure that an arbitrary feature vector
can be well approximated using W, it would be ideal if M was orthonomral, which would imply an
ideally-behaved basis to represent N . This intuition justifies the following regularization:
Rφ = βId - M>DkM ,	(23)
where β is a scale parameter designed to approximate the column norm. That is, the regularizer forces
the neural network to learn an orthogonal feature embedding by normalizing the empirical feature
covariance matrix. The gradient of Rφ can also approximated using mini-batches. We augment the
original learning objectives by adding both Rθ and Rφ weighted by hyper-parameters.
5.1	Empirical Justification of Regularizers
The goal of our experiments is to assess the applicability of the proposed regularization schemes
based on orthogonality and projection operations to practical deep RL algorithms. To avoid the
confounding effects of exploration, we restrict our study to learning from a frozen batch of data
with a fixed number of transitions collected prior to learning. We use a randomly initialized policy
in this initial collection step. We consider both discrete and continuous control benchmarks in this
analysis. For the discrete action environments, we use DQN (Mnih et al., 2015) as the baseline
algorithm to add our regularizers. For continuous control environments, we use QT-Opt (Kalashnikov
et al., 2018) as the baseline algorithm, which is an actor-critic method that applies the cross-entropy
method to perform policy optimization. Our modifications add Rφ and Rθ to the standard MSBE
objective on the critic Q-network. Additional details describing the complete experiment setup for
each environment are provided in Appendix B. Experimental results contrasting vanilla TD and RM
with their regularized variants are summarized in Fig. 2. These findings demonstrate that the proposed
regularization schemes can be used to improve the performance of both vanilla TD learning and RM.
Note that RM is typically less popular than TD due to its worse empirical performance. On Acrobot
and Reacher, the modification was able to fully close the gap between RM and TD. On Cartpole,
(where vanilla RM dominates vanilla TD), and on Pendulum, the regularizers also deliver significant
improvements to the TD learning baseline and modest improvements to the RM baseline.
6	Conclusion
We have investigated the fixed points of classical updates for value estimation in the overparameterized
setting, where there is sufficient capacity to fit all the Bellman constraints in a given data set. We
find that TD and FVI have different fixed points than RM, but in the linear case the difference can
be entirely attributed to a constraint missing from RM that the solution lie in the row space of the
predecessor state features. We devised two novel regularizers based on these findings, which stabilized
the performance of TD without sacrificing generalization, while improving the generalization of RM,
in the setting of estimating optimal values with a deep model. Characterizing the implicit bias of other
algorithms, such as gradient or emphatic TD variants remains open. Identifying other regularizers
that further close the gap between TD and RM is also an interesting direction for future investigation.
9
Published as a conference paper at ICLR 2022
7	Acknowledgement
The authors would like to thank Mengjiao Yang, George Tucker, Ofir Nachum and Aviral Kumar
for insightful discussions and providing feedback on a draft of this manuscript. Dale Schuurmans
gratefully acknowledges funding from the Canada CIFAR AI Chairs Program, Amii and NSERC.
References
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep
Q-learning. arXiv preprint arXiv:1903.08894, 2019.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: Structural complex-
ity and representation learning of low rank MDPs. In Advances in Neural Information Processing
Systems (NeurIPS), volume 33, 2020.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G. Belle-
mare. Deep reinforcement learning at the edge of the statistical precipice. arXiv preprint
arXiv:2108.13264, 2021.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
International Conference on Machine Learning (ICML), pages 30-37,1995.
Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
arXiv preprint arXiv:2103.09177, 2021.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849-15854, 2019.
Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado,
Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric
balloons using reinforcement learning. Nature, 588:77-82, 2020.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference on Learning Theory (COLT), pages
1691-1692, 2018.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge U Press, 2004.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine Learning, 22(1):33-57, 1996.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information
Processing Systems (NeurIPS), volume 33, 2020.
Diogo Carvalho, Francisco S Melo, and Pedro Santos. A new convergent variant of Q-learning with
linear function approximation. In Advances in Neural Information Processing Systems (NeurIPS),
volume 33, 2020.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning (ICML), pages 1042-1051, 2019.
Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions
via dual embeddings. In Artificial Intelligence and Statistics, pages 1458-1467. PMLR, 2017.
Gal Dalal, Balazs SZoreni, Gugan Thoppe, and Shie Mannor. Finite sample analysis for TD(0) with
function approximation. In AAAI Conference on Artificial Intelligence (AAAI), pages 6144-6160,
2018.
10
Published as a conference paper at ICLR 2022
Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A
survey and comparison. Journal of Machine Learning Research, 15:809-883, 2014.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In International Conference on Machine Learning, pages 1049-
1058. PMLR, 2017.
Yaqi Duan, Zeyu Jia, , and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning (ICML), pages 2701-2709,
2020.
Damien Ernst, P. Guerts, and L. Whenkel. Tree-based batch mode reinforcement learning. Journal of
Machine Learning Research, 6:503-556, 2005.
Mattheiu Geist, Bilal Piot, and Olivier Pietquin. Is the Bellman residual a bad proxy? In Advances in
Neural Information Processing Systems (NeurIPS), volume 31, pages 3208-3217, 2017.
Dibya Ghosh and Marc G. Bellemare. Representations for stable off-policy reinforcement learning.
In International Conference on Machine Learning (ICML), pages 3556-3565, 2020.
Geoffrey J. Gordon. Stable function approximation in dynamic programming. In International
Conference on Machine Learning (ICML), pages 261-268, 1995.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning (ICML), 2018.
Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesvari, and Mengdi Wang. Sparse feature selection
makes batch reinforcement learning more sample efficient. In International Conference on Machine
Learning (ICML), 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
alization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
volume 31, pages 8571-8580, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement
learning with linear function approximation. Journal of Machine Learning Research, 125:1-7,
2020.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018.
J. Zico Kolter. The fixed points of off-policy TD. In Advances in Neural Information Processing
Systems (NeurIPS), volume 24, pages 2169-2177, 2011.
George Konidaris, Scott Niekum, and Philip S. Thomas. TDγ : Re-evaluating complex backups in
temporal difference learning. In Advances in Neural Information Processing Systems (NeurIPS),
volume 24, pages 2402-2410, 2011.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep neural
networks. In Advances in Neural Information Processing Systems (NeurIPS), volume 25, 2012.
Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization
inhibits data-efficient deep reinforcement learning. In International Conference on Learning
Representations (ICLR), 2021.
Ilja Kuzborskij, Csaba Szepesvari, Omar Rivasplata, Amal Rannen-Triki, and Razvan Pascanu. On
the role of optimization in double descent: A least squares study. arXiv preprint arXiv:2107.12685,
2021.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016.
11
Published as a conference paper at ICLR 2022
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient TD. In Conference on Uncertainty in Artificial Intelligence (UAI),
pages 504-513,2015.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Proximal gradient
temporal difference learning algorithms. In International Joint Conference on Artificial Intelligence
(IJCAI), pages 4195-4199, 2016.
Daniel J. Lizotte. Convergent fitted value iteration with linear function approximation. In Advances
in Neural Information Processing Systems (NeurIPS), volume 24, pages 2537-2545, 2011.
Tyler Lu, Dale Schuurmans, and Craig Boutilier. Non-delusional Q-learning and value iteration. In
Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 9971-9981,
2018.
Hamid R. Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, and David Silver. Convergent
temporal-difference learning with arbitrary smooth function approximation. In Advances in Neural
Information Processing Systems (NeurIPS), volume 22, pages 1204-1212, 2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Remi Munos and Csaba Szepesvari. Finite time bounds for sampling based fitted value iteration. In
International Conference on Machine Learning (ICML), pages 880-887, 2005.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parameterization in generalization of neural networks. In International Conference on
Learning Representations (ICLR), 2019.
Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael L. Littman.
An analysis of linear models, linear value-function approximation, and feature selction for rein-
forcement learning. In International Conference on Machine Learning (ICML), pages 752-759,
2008.
Gandharv Patil, L. A. Prashanth, and Doina Precup. Finite time analysis of temporal difference
learning with linear function approximation: the tail averaged case. 2021.
L. A. Prashanth, Nathaniel Korda, and Remi Munos. Concentration bounds for temporal difference
learning with linear function approximation: The case of batch data and uniform sampling. Machine
Learning, 110(3):559-618, 2021.
Bruno Scherrer. Should one compute the temporal difference fix point or minimize the Bellman
residual? In International Conference on Machine Learning (ICML), 2010.
John Shawe-Taylor, Christopher KI Williams, Nello Cristianini, and Jaz Kandola. On the eigen-
spectrum of the Gram matrix and the generalization error of kernel-PCA. IEEE Transactions on
Information Theory, 51(7):2510-2522, 2005.
Zhao Song, Ronald Parr, Xuejun Liao, and Lawrence Carin. Linear feature encoding for reinforcement
learning. In Advances in Neural Information Processing Systems (NeurIPS), 2016.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. Journal of Machine Learning Research, 19:1-57, 2018.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3
(1):9-44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems (NeurIPS), volume 12, 1999.
12
Published as a conference paper at ICLR 2022
Richard S Sutton, Csaba Szepesvari, and Hamid Reza Maei. A convergent O(n) algorithm for
off-policy temporal-difference learning with linear function approximation. In Advances in Neural
Information Processing Systems (NeurIPS), volume 21, pages 1609-1616, 2008.
Richard S. Sutton, A. Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. Journal of Machine Learning Research, 17:1-29, 2016.
Csaba Szepesvari and Remi Munos. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(5), 2008.
Csaba Szepesvari and W. D. Smart. Interpolation-based Q-learning. In International Conference on
Machine Learning (ICML), pages 100-107, 2004.
Gavin Taylor and Ronald Parr. Kernelized value function approximation for reinforcement learning.
In International Conference on Machine Learning (ICML), pages 4214-4226, 2009.
John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic program-
ming. Machine Learning, 22(1):59-94, 1996.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE Transactions on Automatic Control, 42(5):674-690, 1997.
Hado van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems
(NeurIPS), volume 23, 2010.
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil.
Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.
Ruosong Wang, Dean Foster, and Sham M Kakade. What are the statistical limits of offline RL with
linear function approximation? In International Conference on Learning Representations (ICLR),
2021a.
Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M. Kakade. Instabilities of offline RL
with pre-trained neural representation. In International Conference on Machine Learning (ICML),
pages 10948-10960, 2021b.
Wentao Weng, Harsh Gupta, Niao He, and Lei Ying. The mean-squared error of double Q-learning.
In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Chenjun Xiao, Ilbin Lee, Bo Dai, Dale Schuurmans, and Csaba Szepesvari. On the sample complexity
of batch reinforcement learning with policy-induced data. arXiv preprint arXiv:2106.09973, 2021.
Lin F. Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning (ICML), 2019.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I. Jordan. On function approx-
imation in reinforcement learning: Optimism in the face of large state spaces. In Advances in
Neural Information Processing Systems (NeurIPS), 2020.
Huizhen Yu. Convergence of least squares temporal difference methods under general conditions. In
International Conference on Machine Learning (ICML), pages 1207-1214, 2010.
Huizhen Yu. On convergence of emphatic temporal-difference learning. In Conference on Learning
Theory (COLT), pages 1-28, 2015.
Andrea Zanette. Exponential lower bounds for batch reinforcement learning. In International
Conference on Machine Learning (ICML), pages 12287-12297, 2021.
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target
network. In International Conference on Machine Learning (ICML), pages 12621-12631, 2021.
13
Published as a conference paper at ICLR 2022
Appendix
A Proofs
A.1 Proof of Theorem 1
Theorem 6 (Restatement of Theorem 1). Let θo ∈ Rd be the initial weight vector. With η ≤(/7尸
RM converges to Θrm = (M - YN)* R + (Id - ∏m-YN)θo.
Proof. Let A = M - γN for simplicity. First recall the residual minimization update,
θt+ι = (Id - ηA> Dk A) θt + ηA> Dk R.	(24)
Let θ* = A1R. It can be verified θ* is one of the feasible solution as Aθ* = R. Then We use
induction to show that for any θ0 ∈ Rd and t ≥ 0
θt+ι - θ* = (Id - ηA>DkA)t+1(θ0 - θ*).	(25)
The base case holds by the update rule Eq. (24). Suppose that the statement holds for t, then We have
θt+ι -	θ* =	(Id -	ηA>DkA)	θt	+ ηA>DkR - θ*	(26)
=(Id -	ηAτDkA)	θt	- (Id - ηAτDk A)θ*	(27)
=(Id - ηAτDkA) (θt - θ*)	(28)
=(Id - ηAτDkA)t+1 (θo - θ*).	(29)
Thus,
θt+ι = (Id - ηAτDk A)t+1θ0 + (Id - (Id - ηAτDk A)t+1)θ*.	(30)
We let V ΛV τ be its eigendecomposition of AτDkA, Which is the empirical covariance matrix of
residual features. Let V- be the null space of V . Then
Id - (Id - ηAτDkA)t+1	(31)
=Id - (VVτ - ηVΛVτ + V-V-τ)t+1	(32)
=Id-(V(Ik - ηΛ)V τ + V-V-τ)t+1	(33)
=Id-(V-V-τ)t+1 -V(Ik -ηΛ)t+1Vτ	(34)
=VVτ - V (Ik - ηΛ)t+1Vτ	(35)
=V (Ik -(Ik - ηΛ)t+1) VT	(36)
Let λmax be the largest eigenvalue of AτDkA. We noW shoW that λmax ≤ 1 + γ.
k
λmax (ATDkA) ≤ X μ(Si)λmax ((φ(si) - Yφ(Si))(O(Si)- Yφ(Si))T) ≤ (I + Yy,(37)
i=1
Where We use the fact that λmax is a convex function and We assume kφ(S)k ≤ 1 for all S ∈ S. Thus,
given that η ≤ ɪ+^, η ≤ λ^ .Then Id - (Id - ηAτDk A)t+1 = VVT as t → ∞. Thus
lim θt = lim (Id - ηAτDk A)t+1θ0 + VVτθ* = lim (Id - ηAτDkA)t+1θ0 + θ*,	(38)
t→∞	t→∞	t→∞
where the last equality follows by that θ* is in the row space of A by definition. When θ0 = 0, we
have the algorithm converges to θ*.
14
Published as a conference paper at ICLR 2022
We next show the result for general θ0. Let θ0 = θ01 + θ02, where θ01 = ΠM-γNθ0 is the component
of θ0 that is in the row space of A, θ02 = (Id - ΠM-γN)θ0 is the perpendicular residual. Then,
lim (Id - ηA>DkA)t+1θ0	(39)
t→∞
= lim (V-V-> + V (Ik - ηΛ)t+1V >)(θ01 + θ02)	(40)
t→∞
=θ02 + lim V (Ik - ηΛ)t+1V >θ01 =θ02,	(41)
t→∞
where the last step follows by the choice of η. This finishes the proof.
□
A.2 Proof of Theorem 2
We will need the matrix binomial theorem in the proof.
Lemma 1 (Matrix Binomial Theorem). For n ≥ 0 and two matrices X , Y
(I+XY)nX = X(I+YX)n.	(42)
Proof.
(I+XY)nX = Xn	n (XY)kX = XXn	n (Y X)k = X(I+YX)n.	(43)
k=0	k=0
□
Theorem 7 (Restatement of Theorem 2). Assuming that M>Dk (M - γN) is diagonalizable
Let θo ∈ Rd be the initial weight vector. With η <(“，)吗,if k Wk < Y1, TD(0) converges to
θTD = M*(Ik - YW)-1R + β, where β = Q0Q-1θ0, Qo are eigenvectors of M>Dk(M 一 YN)
with zero eigenvalues. If ∣∣ W∣∣ ≥ 1 there is an initial θo for which TD(0) does not converge.
Proof. We first rewrite the TD update formulate as
θt+1 = (Id - ηM>Dk (M - YN))θt + ηM >DkR	(44)
A simple recursive argument shows that for any θ0 ∈ Rd,
t
θt+1 = (Id-ηM>Dk(M-YN))t-1θ0+ηX(Id-ηM>Dk(M-YN))iM>DkR.
i=0
(45)
By the matrix binomial theorem (Lemma 1),
(Id-ηM>Dk(M-YN))iM>Dk =M>Dk(Ik -η(M-YN)M>Dk)i.	(46)
By writing N as the projection to the row-span of M and the perpendicular component, we have
(M - YN)M>	(47)
=(M - γNMtM - γN(Id - M↑M))M>	(48)
=(Ik - YW)MM> ,	(49)
where the last step follows by (Id - MtM)M> = 0. Thus we can rewrite θt+1 as
t
θt+1 = (Id - ηM>Dk(M - YN))t-1θ0 + ηM>Dk X(Ik - η(M - YN)M>Dk)iR (50)
i=0
t
= (Id - ηM>Dk(M - YN))t-1θ0 + ηM>Dk X(Ik - η(Ik -YW)MM>Dk)iR,
i=0
(51)
15
Published as a conference paper at ICLR 2022
Given ∣∣ W∣∣ < 1∕γ, We have that all eigenvalues of Ik - YW are positive. Let η <([+，*,口, then
kη(Ik - γW)MM>Dk k < ηk(Ik -γW)kkMM>Dkk < 1,	(52)
otherWise the matrix poWer series diverges. Thus
t
ηM>Dk X(Ik - η(Ik - YW)MM>DkYR = Mt(Ik - YW)-1R .	(53)
i=0
Therefore, given that θ0 = 0, We have the algorithm converge to Mt (Ik - YW)-1R.
We noW shoW the convergence point for an arbitrary θ0. Let QΛQ-1 be the eigen decomposition
of M>Dk (M - YN). By the loW rank structure of this matrix, it has at most h ≤ k non-zero
eigenvalues. Let Q0 be the eigenvectors With eigenvalue zero. Then
lim (Id - ηM >Dk(M - YN))tθ0	(54)
t→∞
= lim Q(Id - ηΛ)tQ-1θ0	(55)
t→∞
=Q0Q0-1θ0 ,	(56)
Where the last step folloWs by the choice ofη.
A.2.1 Characterization for Non-diagonalizable Case
In the above analysis, We assume that the matrix M>Dk (M - YN) is diagonalizable. We noW
characterize the convergent point for the general case using Jordan normal form of the matrix. Let
Z = M>Dk(M - YN) and Z = QJ Q-1 be the jordan normal form of Z. We still denote Q0
the eigenvectors With eigenvalue zero. Then there is
lim (I - ηZ)t = lim Q(I - ηJ)tQ-1
t→∞	t→∞
(57)
Since I - ηJ has a block diagonal structure, its poWer can be obtained by first computing the poWer
of each block. Let Ji be the jordan block With eigenvalue λi . We Write Ji = λi I + L, Where L is a
matrix such that the only non-zero entries of L are on the first off-diagonal. Then We can Write the
i-th block of J as (1 - ηλi)I - ηL. Using the binomial theorem We get
t
((I- ηλi)I - ηL)t = X (t) (1 - ηλi)t-s(-ηL)s.	(58)
s=0
Note that Ls is the matrix With ones on the s-th diagonal aWay from the main diagonal, and Ls = 0
for s larger than the size of L. Therefore, ((1 -ηλi)I-ηL)t is a triangular matrix With (1 -ηλi)t on
the main diagonal, -ηt(1 - ηλi)t-1 on the first off-diagonal, and so on. Therefore, the eigenvalues
of this matrix are all (1 - ηλi)t. Then given a learning rate that η < 1∕λmax, for any jordan block
With λi > 0, We have that the matrix poWer converges. For λi = 0, the jordan block corresponds
to eigenvectors that are in the kernel space of Z. Thus, suppose that all eigenvalues of Z are
non-negative, We have
lim Q(I - ηJ)tQ-1θ0 =Q0Q0-1θ0.	(59)
t→∞
Note that if a negative λi exists, the above derivations can still be used to characterize the convergent
sub-component of θ0 . The non-convergent sub-component of θ0 Will diverge With an exponential rate
as shoWn above.
□
A.3 Proof of Theorem 3
Proof. We first prove the update formula. Recall the FVI update,
θt=Mt(R+YNθt-1).
16
Published as a conference paper at ICLR 2022
For t = 1 the result holds by definition. Suppose that
t-2
θt = Mt I X(YNM)iR + (YNMt)t-1(R + YNθ0)
i=0
We now prove the result for t + 1 by induction.
θt+ι = M t(R + YN θt)
Mt R+YNMt X(YNMt)iR+(YNMt)t-1(R+YNθ0)
Mt R+ X- (YNMt)iR + (YNMt)t(R + YNθ0)
Mt (X(YNMt)iR + (γNMt)t(R + γNθ0)
(60)
(61)
(62)
(63)
(64)
Clearly the convergence of this algorithm depends on the spectral norm of NMt. In particular, given
that kNMt k < 1/Y, we have the algorithm converges to
Mt(Ik - YW)-1R	(65)
as t → ∞. This finishes the proof.
□
A.4 Proof of Theorem 4 and Corollary 1
Proof. We first prove the result for residual minimization fixed point θRM. The proof is adopted from
characterizing the minimum norm solution of solving least square (Boyd and Vandenberghe, 2004).
Let A = M - Y N for simplicity. We write the Lagrange of the optimization problem,
L(θ, α) = inf sup4||。||2 + ɑ>(R — Aθ)	(66)
θ∈Rd α∈Rk 2
=sup ɪkA>αk2 + a>R — α>AA>α	(67)
α∈Rk 2
=sup a>R — 1α>AA>α.	(68)
Solving for a* and add it to θ* = A>α* gives that θ* = AtR.
We next prove Corollary 1, which characterizes the TD and FVI fixed point θTD. Let W = NMt.
We write the Lagrange of the optimization problem,
L(θ,α) = inf sup 1kθ∣∣2 + α>(R — (Ik — YW)Mθ)	(69)
θ∈Rd α∈Rk 2
=sup 1 ∣∣M >(Ik — Y W )>αk2 + a>R — α>(Ik — Y W )MM > (Ik — YW )>α (70)
α∈Rk 2
=sup a>R — 1 a>(Ik — YW)MM>(Ik — YW)>α .	(71)
Solving for a* and add it to θ* = M>(Ik — YW)>α* gives that θ* = Mt(Ik — YW)-1R. The
second part of Theorem 4 is immediately followed by this.
□
A.5 Proof of Theorem 5
Lemma 2. Let θt be the output of FVI at iteration t with θ0 as the initial parameter. We have that
MtMθt = θt for any t ≥ 1.
17
Published as a conference paper at ICLR 2022
Proof. The claim is implied by the fact that θt is in the row space of M . In particular, by Theorem 3,
θt = M*α for some α ∈ Rn. Thus,
MtMθt = MtMMtα = Mtα = θt.	(72)
This finishes the proof.	□
Lemma 3. E(θ) is 1-smoothness.
Proof. Recall the prediction error of θ ∈ Rd
1	21	2
E(θ) = 2 kΦθ - vkDμ = 2 kθ -θ*k∑ ,	(73)
where Σ = Φ>D*Φ. The gradient of θ is E0(θ) = Σ(θ - θ*). Then
kE0(θ1)-E0(θ2)k = kΣ(θ1 - θ2)k ≤ λmax(Σ) k(θ1 - θ2)k ≤ kθ1 - θ2k ,	(74)
where we use ∣∣φ(s)k ≤ 1 for all s ∈ S and λmaχ(Σ) ≤ Ps μ(s)λmaχ(φ(s)φ(s)>) ≤ 1.	□
Lemma 4. Let εapp = N∏Mθ* and σStat = H(P — P)Φθ*. We have
Mθ* = R + γ(εapp + εStaJ + YWMθ* .	(75)
Proof. Using the definitions we have,
M θ*	=	R + γ H P Φθ*	(76)
=R + γNθ* + YH(P - P)Φθ*	(77)
=R + Y(WM + N ∏M )θ* + γ H (P - P)Φθ*	(78)
=R + γ(εapp + εstat) + γWMθ* .	(79)
□
Proof. By Theorem 3, θt = MtT t-1 (R + YNθ0) is the output of FVI at iteration t. By noting that
E(θ*) = 0 and Lemma 3, for any θ ∈ Rd.
E(θ) ≤ 2 kθ - θ*k2 = 2 (kθ - θ*kMIM + kθ - θ*kId-MIM) .	(80)
We first consider the second term. By Lemma 2,
kθt - θ*kId-MIM = (θt - θ*)> (Id - MtM) (θt - θ*) = kθ*kId-MIM .	(81)
We now consider the first term. By Lemma 4,
M(θ* -θt) = Mθ* -Tt-1(R+YNθ0)	(82)
t-2	t-2
= X(YW)i (R + Y(εapp + εstat)) + (YW)t-1(M θ*) - X(YW)iR - (YW)t-1(R + YNθ0)
i=0	i=0
(83)
t-2	t-1
X(YW)iεapp + X(YW)iεstat + (YW)t-1N(θ* -θ)	(84)
i=0	i=0
Let Σ = M 1 DkM be the empirical covariance matrix. Note that λmin(M 1 M)/k ≥ λmin(Σ). To
show this, let D = diag( 1,..., 1), and M = USV> be the SVD of M. Then
λ1L(M>DM) = min x>M>DMX	(85)
kxk=1
=min a>SU>DUSa	(86)
kαk=1
≥ min α>SU>DkUSα	(87)
kαk=1
= λ+min(M>DkM)	(88)
18
Published as a conference paper at ICLR 2022
where We replace X = Va since V are orthonormal bases, and min髭[k] μ% ≤ 1/k. Therefore,
kMtk = '/gjM-) ≤ 1∕√kλmm(∑).
Combining the results above we have,
kθt - θ* kMtM
≤	∣∣M tM (θt-θ*)∣∣2 kMtk2 kM(θt-θ*)k2				2	(89) (90)
≤	Y kλmin(Σ)	t-1 X(YW)i(εapp + εstat) + (γW)t-1N(θ* - θ) i=0				(91)
≤	4Y kλmin(Σ)	l(ε2 + σ2)	t-1 X(YW)i i=0	2 + ∣∣(YW)t-1∣∣2 kΦk2kθ0		-θ*k).
(92)
Combine this with Eq. (81) finishes the proof.
□
A.6 Proof of Corollary 2
Proof. Recall that in the proof of Theorem 5 we have
kθt-θ*kMIM ≤ C 1 + * X(YW)i
+ ∣∣(YW) j∕kΦk2kθo-θ*k2j .
(93)
Given that kWk < 1, we have for the fixed point θ∞,
kθt - θ*kMIM
/	4γ(ε2 + σ2)
`≤■	ʌ
一kλmin(Σ)(1 - γ)2
(94)
We first consider σ2 = ∣∣H(P - P)v∣∣2. By Hoeffding,s inequality and a union bound we have with
probability at least 1 - δ, for any s ∈ supp(D),
l(Ps- Ps)>v∣ ≤ R∕0lSF.	(95)
Thus, let nmin = mins:n(s)>0 n(s), we have
σ2 ≤ iog(∣s∣∕δ)
k — 2(1 - γ)2nmin
(96)
Now we consider ε2 = ∣∣N∏Mθ*k2. Since N∏M is perpendicular to M, and all features have
norm bounded by one,
ε2
飞 ≤kθ*kId-MIM .	(97)
Combine the above we have,
E(θ) ≤ 1 kθ - θ*kMIM + 1 kθ*kId-MIM	(98)
≤ ʌ~^Y[~^不(2(1 -√2^∖ + kθ*kId-M iM ) + 2 kθ*kId-M iM	(99)
λmin (Σ)(1 - Y)2 2(1 - Y) nmin	2
γ log(∣s∣0	4γ
- TΞ-~ ^7	+ ^	TΞ~^^1	-7^
λmin(∑)(1 - Y)4nmin λmin(∑)(1 - Y)2
kθ*kId-M1M
(100)
Finally, using the tower rule gives the desired result.
19
Published as a conference paper at ICLR 2022
□
A.7 Concentration of Eigenvalues and Bounding the Orthogonal Complement
We will need the following result (Kuzborskij et al., 2021, Theorem 6), which is concerned with
the magnitude of projection onto the eigenspace of a covariance matrix. The result is based on
(Shawe-Taylor et al., 2005, Theorem 1)
Lemma 5. Let Σ = * Pi xgx> be the covariance matrix ofi.i.d. data Xi ∈ Rd. Denote the h- "tail”
of eigenvalues of a covariance matrix Σ = as
n
λ>h= X λi.
i=h+1
(101)
Let Ur be the first r eigenbasis for r ∈ [n]. Then for any z ∈ Rd, with probability at least 1 - δ,
E [k∏Urzk2] ≤ min ∖1 λ>h + 1+√h∖ 2XXkχik2∖ + kzk2ι∕18in72nI.	(102)
h∈[r] I n	n∏ ∖ n	Vn δ /
The next lemma gives a non-asymptotic result to understand the behaviour of λmin (Kuzborskij et al.,
2021, Lemma 1).
Lemma 6. Let X = [X1, . . . , Xn ] ∈ Rd×n be a random matrix with i.i.d. columns, such that
maxi ∣∣Xi∣∣2 ≤ K ,and let Σ = XX >/n, and Σ = E[XιX>]. Then, for every α ≥ 0, with
probability at least 1 - 2e-α, we have
λ[in(Σ) ≥λ1L(∑) I- K 2
for n ≥ d ,
(103)
and furthermore, assuming that ∣∣Xik∑t = √d,for all i ∈ [n], we have
Yin® ≥Yin⑶"-K 2 (C +6r∣))	for n<d,
(104)
where we have an absolute constant c = 23.5√In9.
B Experiment setup
In this section, we provide additional details about the experimental setup and hyper-parameters
used for each of the environments. For all of these environments the regularization weights were
considered as tunable hyper-parameters. In addition, for Rφ (see Eq 23), the scale factor β was also
considered as a parameter to be tuned in order to approximate the feature matrix norm.
B.1	Acrobot
•	Replay buffer with 10k tuples sampled using a random policy across trajectories with
maximum episode length of 64.
•	A DQN with hidden units consisting of fully connected layers with (100, 100) units.
•	Batch size 64.
•	Learning rate of 1e-3.
•	Regularized RM with weight of 2e-2 on Rφ and 1e-4 on Rw .
•	Regularized TD with weight of 0 on Rφ and 1e-4 on Rw .
B.2	Reacher
•	Replay buffer with 10k tuples sampled from a random policy across trajectories with
maximum steps per episode of length 50.
20
Published as a conference paper at ICLR 2022
•	Learning rate 1e-4.
•	A value network for the continuous action inputs with a fc observation layer with params
(50,), a fc action layer with params (50,) and a joint fc layer with params (100,).
•	Batch size 64.
•	Gradient clipping with a norm of 10.0
•	Regularized RM with weight of 0.15 on Rw and 0 on Rφ.
•	Regularized TD with weight of 2e-2 on Rw and 7e-3 on Rφ .
B.3	Cartpole
•	Replay buffer with 10k tuples sampled using a random policy across trajectories with
maximum steps per episode of length 50.
•	A DQN with hidden units consisting of fully connected layers with (100, 100) units.
•	Batch size 64.
•	Learning rate 1e-3.
•	Regularized RM with weight of 0.29 on Rw and 0 on Rφ .
•	Regularized TD with weight of 1.5e-3 on Rw and 5e-3 on Rφ.
B.4	Pendulum
•	Replay buffer with 1k tuples obtained by sampling directly from a fixed initial state distribu-
tion using a random policy.
•	A value network for the continuous action inputs with a fc observation layer with params
(50,), a fc action layer with params (50,) and a joint fc layer with params (100,).
•	Batch size 64.
•	Learning rate 1e-3.
•	Regularized RM with weight of 1.0 on Rw and 5.4e-4 on Rφ.
•	Regularized TD with weight of 0 on Rw and 1.0 on Rφ .
B.5	Extra Experiment Results
We provide extra experiment results on four Mujoco control problems to assess the applicability of
the proposed regularization Rφ : HalfCheetah, Hopper, Ant, and Walker2d. The results are provided
in Fig. 3. All results are averaged over 100 runs with different random seeds. The hyper-parameters
are provided below.
•	The Q-function is approximated by two hidden layer fully neural networks, where the hidden
layer size is 256.
•	Batch size 256.
•	Learning rate 3e-4.
•	Regularized TD weight are tuned from {1e - 4, 1e - 3, 1e - 2, 1e - 1, 1}
21
Published as a conference paper at ICLR 2022
180	√⅛×
a
«4	02	«4	««	03
Training st»s (IeC)

4O∞ — *«
-reg-dφg
XfX
AntS
5oαo — *pα
reg-ddpg
4000
ιooα I
a I
Ga 02 OA «« Oa 14
Training st≡>s ］，幽
(a) HalfCheetah	(b) Hopper
(c) Ant
(d) Walker2d
Figure 3: We show the results with proposed regularization compared to the baseline algorithms.
The algorithms are trained using a fixed offline data set collected by random initialized policies. The
x-axis shows the training iterations (in thousands) and y-axis shows the performance. All plots are
averaged over 100 runs. The shaded area shows the standard error.
22