Published as a conference paper at ICLR 2022
A Class of Short-term Recurrence Anderson
Mixing Methods and Their Applications
Fuchao Wei1, Chenglong Bao 3,4*, Yang Liu 1,2
1	Department of Computer Science and Technology, Tsinghua University
2	Institute for AI Industry Research, Tsinghua University
3	Yau Mathematical Sciences Center, Tsinghua University
4	Yanqi Lake Beijing Institute of Mathematical Sciences and Applications
wfc16@mails.tsinghua.edu.cn, {clbao,liuyang2011}@tsinghua.edu.cn
Ab stract
Anderson mixing (AM) is a powerful acceleration method for fixed-point itera-
tions, but its computation requires storing many historical iterations. The extra
memory footprint can be prohibitive when solving high-dimensional problems in
a resource-limited machine. To reduce the memory overhead, we propose a novel
class of short-term recurrence AM methods (ST-AM). The ST-AM methods only
store two previous iterations with cheap corrections. We prove that the basic ver-
sion of ST-AM is equivalent to the full-memory AM in strongly convex quadratic
optimization, and with minor changes it has local linear convergence for solving
general nonlinear fixed-point problems. We further analyze the convergence prop-
erties of the regularized ST-AM for nonconvex (stochastic) optimization. Finally,
we apply ST-AM to several applications including solving root-finding problems
and training neural networks. Experimental results show that ST-AM is competi-
tive with the long-memory AM and outperforms many existing optimizers.
1	Introduction
Anderson mixing (AM) (Anderson, 1965) is a powerful sequence acceleration method (Brezinski
et al., 2018) for fixed-point iterations and has been widely used in scientific computing (Lin et al.,
2019; Fu et al., 2020; An et al., 2017), e.g., the self-consistent field iterations in electronic structure
computations (Garza & Scuseria, 2012; Arora et al., 2017). Specifically, we consider a fixed-point
iteration xk+1 = g(xk), k = 0, 1, . . . , where g : Rd 7→ Rd is the fixed-point map. By using
m historical iterations, AM(m) aims to extrapolate a new iterate that satisfies certain optimality
property. When the function evaluation is costly, the reduction of the number of iterations brought
by AM can save a large amount of computation (Fang & Saad, 2009).
AM can be used as a method for solving nonlinear equations (Kelley, 2018) as the fixed-point
problem x = g(x) is equivalent to h(x) := x - g(x) = 0. In practice, since computing the
Jacobian of h(x) is commonly difficult or even unavailable (Nocedal & Wright, 2006), AM can be
seen as a practical alternate for Newton’s method (An et al., 2017). Also, compared with the classical
iterative methods such as the nonlinear conjugate gradient (CG) method (Hager & Zhang, 2006), no
line-search or trust-region technique is used in AM, which is preferable for large-scale unconstrained
optimization. Empirically, it is observable that AM can largely accelerate convergence, though its
theoretical analysis is still under-explored. It turns out that in the linear case (Walker & Ni, 2011;
Potra & Engler, 2013), the full-memory AM (m = k) is essentially equivalent to GMRES (Saad
& Schultz, 1986), a powerful Krylov subspace method that can exhibit superlinear convergence
behaviour in solving linear systems (Van der Vorst & Vuik, 1993). For general nonlinear problems,
AM is recognized as a multisecant quasi-Newton method (Fang & Saad, 2009; Brezinski et al.,
2018). As far as we know, only local linear convergence has been obtained for the limited-memory
AM(m < k) in general (Toth & Kelley, 2015; Evans et al., 2020; De Sterck & He, 2021).
For the application of AM, one of the major concerns is the historical length m, a critical factor
related to the efficiency of AM (Walker & Ni, 2011). A larger m can incorporate more historical
* Corresponding author.
1
Published as a conference paper at ICLR 2022
information into one extrapolation, but it incurs heavier memory overhead since 2m vectors of
dimension d need to be stored in AM(m). The additional memory footprint can be prohibitive for
solving high-dimensional problems in a resource-limited machine (Deng, 2019). Using small m
can alleviate the memory overhead but may deteriorate the efficacy of AM since much historical
information is omitted in the extrapolation (Walker & Ni, 2011; Evans et al., 2020).
To address the memory issue of AM, we deeply investigate the properties of the historical iterations
produced by AM and leverage them to develop the short-term recurrence variant, namely ST-AM.
The basic version of ST-AM imposes some orthogonality property on the historical sequence, which
is inspired by the CG method (Hestenes & Stiefel, 1952) that enjoys a three-term recurrence. Fur-
thermore, to better suit the more difficult nonconvex optimization, a regularized short-term form is
introduced. We highlight the main contributions of our work as follows.
1.	We develop a novel class of short-term recurrence AM methods (ST-AM), including the
basic ST-AM, the modified ST-AM (MST-AM), and the regularized ST-AM (RST-AM).
The basic ST-AM is applicable for linear systems; MST-AM can solve general fixed-point
problems; RST-AM aims for solving stochastic optimization. An important feature of ST-
AM is that all methods only need to store two previous iterations with cheap corrections,
which significantly reduces the memory requirement compared with the classical AM.
2.	A complete theoretical analysis of the ST-AM methods is given. When solving strongly
convex quadratic optimization, we prove that the basic ST-AM is equivalent to the full-
memory AM and the convergence rate is similar to that of the CG method. We also prove
that MST-AM has improved local linear convergence for solving fixed-point problems.
Besides, we establish the global convergence property and complexity analysis for RST-
AM when solving stochastic optimization problems.
3.	The numerical results on solving (non)linear equations and cubic-regularized quadratic op-
timization are consistent with the theoretical results for the basic ST-AM and MST-AM.
Furthermore, extensive experiments on training neural networks for image classification
and language modeling show that RST-AM is competitive with the long-memory AM and
outperforms many existing optimizers such as SGD and Adam.
2	Related work
AM is also known as an extrapolation algorithm in scientific computing (Anderson, 2019). A paral-
lel method is Shanks transformation (Shanks, 1955) which transforms an existing sequence to a new
sequence for faster convergence. Related classical algorithms include Minimal Polynomial Extrap-
olation (Cabay & Jackson, 1976) and Reduced Rank Extrapolation (Eddy, 1979), and a framework
of these extrapolation algorithms including AM is given in (Brezinski et al., 2018). Note that an
elegant recursive algorithm named -algorithm had been discovered for Shanks transformation for
scalar sequence (Wynn, 1956), and was later generalized as the vector -algorithm (Wynn, 1962) to
handle vector sequences, but this short-term recurrence form is not equivalent to the original Shanks
transformation in general (Brezinski & Redivo-Zaglia, 2017). Since AM is closely related to quasi-
Newton methods (Fang & Saad, 2009), there are also some works trying to derive equivalent forms
of the full-memory quasi-Newton methods using limited memory (Kolda et al., 1998; Berahas et al.,
2021), while no short-term recurrence is available. To the best of our knowledge, ST-AM is the first
attempt to short-term recurrence quasi-Newton methods.
Recently, there have been growing demands for solving large-scale and high-dimensional fixed-
point problems in scientific computing (Lin et al., 2019) and machine learning (Bottou et al., 2018).
For these applications, Newton-like methods (Byrd et al., 2016; Wang et al., 2017; Mokhtari et al.,
2018) are less appealing due to the heavy memory and computational cost, especially in nonconvex
stochastic optimization, where only sublinear convergence can be expected if only stochastic gradi-
ents can be accessed (Nemirovski & Yudin, 1983). On the other side, first-order methods (Necoara
et al., 2019) stand out for their low per-iteration cost, though the convergence can be slow in prac-
tice. When training neural networks, SGD with momentum (SGDM) (Qian, 1999), and adaptive
learning rate methods, e.g. AdaGrad (Duchi et al., 2011), RMSprop (Tieleman & Hinton, 2012),
Adam (Kingma & Ba, 2014), are very popular optimizers. Our methods have the nature of quasi-
Newton methods while the memory footprint is largely reduced to be close to first-order methods.
Thus, ST-AM can be a competitive optimizer from both theoretical and practical perspectives.
2
Published as a conference paper at ICLR 2022
3	Methodology
In this section, we give the details of the proposed ST-AM. We always assume the objective function
as f : Rd → R, the fixed-point map g : Rd → Rd. Moreover, We do not distinguish rk = -Vf (Xk)
and rk = g(xk) 一 Xk in our discussion as Vf (x) = 0 is equivalent to g(x) = X — Vf (x) = x.
3.1	Anderson mixing
The AM finds the fixed point of g via maintaining tWo sequences of length m (m ≤ k):
Xk = [∆xk-m, ∆xk-m+ι,…，∆xk-i],Rk = [∆rk-m, ∆r-m+ι,…，∆rk-i] ∈ Rd×m, (1)
Where the operator ∆ denotes the forWard difference, e.g. ∆Xk = Xk+1 - Xk. Each update of AM
can be decoupled into tWo steps, namely the projection step and the mixing step:
Xk = Xk — XkΓk, (Projection step), Xk+ι = Xk + βkrk, (Mixing step),	(2)
where rk := rk — RkΓk and βk > 0 is the mixing parameter. The Γk is determined by
Γk = arg min krk - RkΓk2.	(3)
Γ∈Rm
Thus, the full form of AM (Fang & Saad, 2009; Walker & Ni, 2011) is
Xk+1 =Xk+βkrk — (Xk+βkRk)Γk.	(4)
Remark 1. To see the rationality of AM, assume g is continuously differentiable, then we have
h(Xj) —h(Xj-1) ≈ h0 (Xk)(Xj —Xj-1) aroundXk, where h0(Xk) is the Jacobian of h(X) := X—g(X).
So, it is reasonable to assume Rk ≈ —h0 (Xk)Xk, and we see krk — Rk Γk2 ≈ krk + h0(Xk)XkΓk2.
Thus, we can recognize (3) as solving h0 (Xk)dk = h(Xk) in a least-squares sense, where dk =
XkΓk. The mixing step incorporates rk into the new update Xk+1 if βk > 0. Otherwise, if βk = 0,
then Xk+1 = Xrk is an interpolation of the previous iterates, leading to a stagnation.
3.2	The basic short-term recurrence Anderson mixing
The basic ST-AM is to solve the strongly convex quadratic optimization:
min f (x) := ；XTAX — bTX,	(5)
where A 0. Let p-1 = q-1 = p0 = q0 = 0 ∈ Rd. At the k-th iteration, given the two matrices
Pk-1 = (pk-2, pk-1) ∈ Rd×2, Qk-1 = (qk-2, qk-1) ∈ Rd×2 and defining p = Xk — Xk-1 and
q = rk — rk-1, the basic ST-AM constructs
P = P — Pk-I(QT-ιq),	q = q — Qk-I(QT-ιq),	(6a)
Pk = P/同∣2,	qk = q/Iqk2.	(6b)
Then, we update Pk = (Pk-1, Pk), Qk = (qk-1, qk) ∈ Rd×2. Such construction ensures QkTQk =
I2 for k ≥ 2 and the storage of Pk and Qk is equal to AM(2). With the corrected Pk and Qk, the
ST-AM method modifies the projection step and the mixing step accordingly, that is,
Xrk = Xk — PkΓk, (Projection step), Xk+1 = Xrk + βkrrk, (Mixing step), (7)
where Γk = arg min krk — QkΓk2 = QkTrk and rrk = rk — QkΓk. Thus, the ST-AM replaces Xk
and Rk in (1) by Pk and Qk respectively and imposes the orthogonality condition on Qk. The details
of basic ST-AM are given in Algorithm 2 in Appendix C.1. Define Prk = (P1,P2, . . . ,Pk), Qrk =
(q1, q2, . . . , qk), the Krylov subspace Km(A, v) ≡ span{v, Av, A2v, . . . , Am-1v}, the range of X
as range(X). We give the properties of the basic ST-AM in Theorem 1.
Theorem 1. Let {Xk} be the sequence generated by the basic ST-AM. The following relations hold:
(i) ∣∣q∣∣2 > 0, range(Prk) = range(Xk) = Kk(A,ro), range(Qk) = range(Rk) = AKk(A,ro)；
(ii)Qrk = —APrk,QrkTQrk =Ik;
(iii) rrk ⊥ range(Qrk) and Xrk = X0 + zk, where zk = arg minz∈Kk(A,r0) ∣r0 — Az∣2.
If ∣rrk ∣2 = 0, then Xk+1 is the exact solution.
3
Published as a conference paper at ICLR 2022
2 ( √LTμ-ι
2 'k√⅛!
The proof is in Appendix C.1. Note that the property (iii) in Theorem 1 exactly describes the relation
Xk = xG，where XG is the output of the k-th iteration of GMRES (Saad & SchUltz,1986). Moreover,
let XAM be the k-th intermediate iterate in the full-memory AM. It holds that XAM = XG (See
Proposition 1 in Appendix C.1.), which induces that Xk = XAM = xG. This equivalence indicates
that ST-AM is more efficient than AM and GMRES since only two historical iterations need to
be stored. Moreover, by directly applying the convergence analysis of GMRES (Corollary 6.33 in
(Saad, 2003)), we obtain the convergence rate of the basic ST-AM for solving (5):
Corollary 1. Suppose the eigenvalues of A lie in [μ,L] with μ > 0, and let {xk} be the se-
quence generated by the basic ST-AM, then the k-th intermediate residual 尸k satisfies ∣∣fk∣∣2 ≤
k
kr0k2. Moreover, the algorithm finds the exact solution in at most (d+ 1) iterations.
Remark 2. The GMRES can be simplified to an elegant three-term recurrence algorithm called the
conjugate residual (CR) method (Algorithm 6.20 in (Saad, 2003)) when solving (5). Thus, a similar
simplification for AM is expected to exist. Like CG and Chebyshev acceleration (Algorithm 12.1 in
(Saad, 2003)), the convergence rate of ST-AM has the optimal dependence on the condition number,
while ST-AM does not form the Hessian-vector products explicitly.
3.3 The modified short-term recurrence Anderson mixing
For general nonlinear fixed-point problems, global convergence may be unavailable for the basic ST-
AM, as a counter-example exists for AM (Mai & Johansson, 2020). Thus, we propose a modified
version of the basic ST-AM (MST-AM) and prove the local linear convergence rate under similar
conditions used in (Toth & Kelley, 2015; Evans et al., 2020). Concretely, the MST-AM makes three
main changes to the basic ST-AM.
Change 1: Instead of applying the normalization (6b), the MST-AM constructs pk and qk via
Zk = (QT-IQk-1 户QT-ιq, Pk = P - Pk-IZk, qk = q - Qk-C,	⑻
where “ j ” is the Moore-Penrose inverse. Accordingly, We choose Γk = arg min ∣∣rk 一 QkΓ∣2 =
(QTQk)'QTrk. This change relaxes the orthonormality for Qk (k ≥ 2), but keeps the orthogonality
condition: Qrk-Iqk = 0. In fact, Q'qk = 0 in the case of solving (5).
Change 2: MST-AM imposes the boundedness constraints on Pk-1Zk and Qk-1Zk: If ∣Pk-1Zk ∣2 >
cp∣P∣2 or ∣Qk-1Zk∣2 > cq∣q∣2, then Pk = Pk-1,Qk = Qk-1, where cp > 0,cq ∈ (0, 1) are
predefined constants. It is worth mentioning that adding some boundedness condition is common in
the analysis ofAM (Toth & Kelley, 2015; Evans et al., 2020).
Change 3: MST-AM restarts, i.e. setting Pk = Qk = 0 ∈ Rd×2 every m iterations. This restart
operation is to limit the number of higher-order terms appeared in the residual expansion in our
analysis and we can set m to be a large number in practice.
The detailed description of MST-AM is given in Appendix C.2. In the next theorem, we establish
the convergence rate analysis for the MST-AM.
Theorem 2. Let {xk } be the SeqUence generated by MST-AM, x* ∈ Rd be a fixed point of g and m
be the restarting periodforMST-AM. Suppose that in the ball B(ρ) := {x ∈ Rd∣∣x — x*∣2 < ρ} for
some ρ > 0, g is Lipschitz continuously differentiable and there are constants K ∈ (0,1) and K > 0
with (i) kg(y) — g(x)∣2 ≤ Kky — x∣2 for every x,y ∈ B(ρ), and (ii) ∣∣g0(y) — g0(x)∣∣2 ≤ ^∣∣y - x∣∣2
for every X, y ∈ B(ρ), where g0 is the Jacobian ofg. Assume |1 一 βk | + Kβk ≤ K0 for a constant
K0 ∈ (0, 1). IfX0 is sufficiently close to X*, then for rk := g(Xk) — Xk, the following bound holds:
mk
krk+1∣∣2 ≤ θk (II- Bk | + Kek ) Ilrk ∣∣2 + K ^X O (krk-j ∣∣2) ,	(9)
j=0
where θk = ∣∣尸k ∣∣2∕∣∣rk ∣∣2 ≤ 1 and mk = k mod m. Thus, the residuals {rk} converge Q-linearly,
and the errors {∣Xk — X* ∣2} converge R-linearly.
Remark 3. In a local region around X*, the convergence rate is determined by the first-order term
θk (I1 — βkI + Kβk)∣rk ∣2. We can choose βk = 1 such that I1 — βk I + Kβk = K < 1. Since
尸k is the orthogonal projection of rk onto the subspace range(Qk)⊥, θk has the interpretation of
4
Published as a conference paper at ICLR 2022
Algorithm 1 RST-AM for stochastic programming
Input： xo ∈ Rd, βk > 0, αk ∈ [0,1], δk1) > 0, δk2) > 0.
Output: x ∈ Rd
1:	P0,Q0 =0∈Rd×2,p0,q0 =0 ∈Rd
2:	for k = 0, 1, . . . , until convergence, do
3：	rk = -NfSk (Xk )
4: if k > 0 then
5： p = xk - xk-1, q = rk - rk-1
6:	Zk = (QT-1Qk-1 + δk1)pT-ιPk-ι)tQT-ιq
7：	qk = q - Qk-1ζk, pk = p - Pk-1ζk
8：	Pk = [pk-1 , pk ], Qk = [qk-1 , qk ]
9： end if
10： Check Condition (13) and use smaller αk if (13) is violated
11:	Γk = (QT Qk + δk2) PTPk)^ QTrk
12： Xk = Xk - αkPkγk,rk = rk - αkQkγk
13： Xk+1 = Xk + β∕k
14:	Apply learning rate schedule of αk , βk
15： end for
16： return Xk
the direction-sine between rk and the subspace range(Qk). When θk is small, e.g., rk nearly lies in
range(Qk), the acceleration by MST-AM is significant. Compared to AM(m), MST-AM incorporates
historical information with orthogonalization. In the SPD linear case and without restart, the global
orthogonality property holds, i.e. rk ⊥ range(Qk), which means there is no loss of historical
information, while AM(m) (Evans et al., 2020) does not have such property in this ideal case.
3.4 The regularized short-term recurrence Anderson mixing
Inspired by the recent work on stochastic Anderson mixing (SAM) method (Wei et al., 2021), we
develop a regularized ST-AM (RST-AM) for solving nonconvex stochastic optimization problems.
Consider the nonconvex optimization problem minχ∈Rd f (x) := 1 PT=I fξi (x), where fξi : Rd →
R is the loss function corresponding to i-th data sample and T is the number of data samples. In
mini-batch training, the gradient is evaluated for fsk (Xk):= 六 Pii∈sk fξi (Xk), where Sk ⊆ [T]:=
{1, 2, . . . ,T} is the sampled mini-batch, and nk := |Sk| is the batch size. In this case, we set
rk = -NfSk (Xk) (Line 3 in Algorithm 1), which is an unbiased estimate of the negative gradient.
Recalling from (8), Zk = (QT-1Qk-ι)tQT-1∆rk-ι = arg min ∣∣∆rk-ι — Qk-ιZ∣∣2 as q = ∆rk-ι
by definition. Since ST-AM is based on a local quadratic approximation (5) in a small region around
Xk, a large magnitude of ∣Pk-1Zk∣2 tends to make the change from ∆Xk-1 to pk = ∆Xk-1 -
Pk-1Zk too aggressively, which may lead to instability. Consequently, we add a penalty term in the
above least squares problem, i.e.
Zk = arg min ∣∆rk-1 - Qk-1Z∣22 + δk(1) ∣Pk-1Z∣22,	(10)
where δk(1) > 0. The same as SAM (Wei et al., 2021), we also add a regularization term for comput-
ing Γk via
Γk = arg min ∣rk - QkΓ∣22 + δk(2) ∣PkΓ∣22,	(11)
where δk(2) > 0, and a damping term αk is used as shown in Line 12 in Algorithm 1. In practice, we
choose the two regularization parameters as
.⑴=	cιkrkk2
k = ∣∆Xk-ik2 + e。'
max
c2 ∣rk ∣22
kpk k2 + e0
, Cβk-2
(12)
where c1 , c2 , C > 0 are constants, and e0 > 0 is a small constant to bound the denominators
away from zero. Assuming ∣pk-1 ∣2 ≈ ∣pk ∣2 = O(∣∆Xk-1 ∣2), the choices of (12) make
∣δk(1)PkT-1Pk-1∣2 ≈ O(∣rk∣22)and∣δk(2)PkTPk∣2 ≈ O(∣rk∣22)aware of the change of the local
curvature： large (small) ∣rk ∣2 tends to lead to a large (small) regularization.
5
Published as a conference paper at ICLR 2022
Remark 4. One update of xk given by Line 11-13 in Algorithm 1 can be formulated as xk+1 =
Xk +	Hkrk,	where	Hk	=	βkI	-	a®YkZ[qT,K =	Pk	+	βkQk,Zk	=	QTQk +	δk2)pTPk.	To
guarantee the positive definiteness of Hk, we follow the same procedure in SAM. Let λk be the
largest eigenvalue of YkZkQrT + QkZkYk. If ak satisfies
αkλk ≤ 2βk(1 — μ),	(13)
then SrTHkSk ≥ βkμ∣∣skk2, NSk ∈ Rd, where μ ∈ (0,1) is a constant. Note that λk can be Cheaply
obtained by computing the largest eigenvalue ofa matrix of R4×4 (see Appendix C.3.1).
We summarize the RST-AM in Algorithm 1 and establish its convergence properties here. First, we
impose the same assumptions on the objective function f as those in (Wei et al., 2021).
Assumption 1. f : Rd → R is continuously differentiable. f(x) ≥ flow > -∞ for any x ∈ Rd.
Vf is globally L-Lipschitz continuous; namely kVf (x)-Vf (y)k2 ≤ L∣∣x-y∣∣2 forany x,y ∈ Rd.
Assumption 2. For any iteration k, the stochastic gradient Vfξk (Xk) satisfies Eξfc [Vfξfc (Xk)]=
Vf (xk), Eξk [kVfξk (xk) - Vf (xk)k22] ≤ σ2, where σ > 0, and ξk, k = 0, 1, . . . are independent
samples that are independent of {Xj }jk=0.
The diminishing condition about βk is
+∞
βk = +∞,
k=0
+∞
X βk2 < +∞.
k=0
(14)
We give the convergence properties of RST-AM in nonconvex (stochastic) optimization and proofs
are deferred to Appendix C.3.2.
Theorem 3.	Suppose Assumption 1 hold and {Xk} is the sequence generated by full-batch RST-AM,
i.e. nk = T. Let βk = β ∈ (0, 2l(i/c—i)] be a constant, ak ∈ [0,1] and satisfies (13), then
1 N-1
N E kVf(Xk)k2 ≤
k=0
2(f(xo)- flow)
Nμβ
(15)
in the N iterations. To ensure N PNOIIlVf (Xk )k2 < G the number of iterations is O(1∕e).
Theorem 4.	Suppose Assumptions 1 and 2 hold and {Xk} is the sequence generated by RST-AM
1
with batch size n = n ≤ T. If βk ∈ (0, 4l(i/c—i)] and satisfies (14), ak ∈ [0, min{1, βk }] and
satisfies (13), then
lim inf kVf(Xk)k2 = 0 with probability 1 and ∃Mf > 0 → E[f (Xk)] ≤ Mf, ∀k.	(16)
k→∞
If Eξk [kVfξk (Xk)k22] ≤ Mg, ∀k, where Mg > 0 is a constant, we have
lim kVf(Xk)k2 = 0 with probability 1.	(17)
k→∞
Theorem 5.	Suppose Assumptions 1 and 2 hold and {Xk}kN=-01 is the first N iterations generated by
~
-   C.. . . . . <	ʧ I	i	-C	. Γ	I I	D-I	<	1-、 .	" y
RST-AM with fixed batch size n = n. Let βk = min{ 4L(i+c-i), σ√DN}, where D ISa problem-
1
independent constant; ak ∈ [0, min{1, βk }] and satisfies (13). Let R be a random Variablefollow-
ing PR(k) := P rob{R = k} = 1/N, then
E[kVf(XR)k22] ≤
16Df L(1 + C T)	σ (4Df	4(L + μ-1 )(1 + C T )D
Nμ2	+ μ√N I D +	n
(18)
where Df := f(X0) - f low and the expectation is taken with respect to R and {Sj}jN=-01. To ensure
E[kVf (XR)k22] ≤ , the number of iterations is O(1/2).
Remark 5. The proofs of Theorem 4 and 5 are based on the analysis of SAM (Wei et al., 2021).
The theorems show that the convergence of RST-AM is no worse than SGD (Robbins & Monro,
1951). There are two key differences between RST-AM and SAM: RST-AM is based on short-term
recurrences while SAM usually maintains longer historical sequences to ensure effectiveness; RST-
AM uses additional correction and regularization terms (Line 5-8 in Algorithm 1) to incorporate
historical information while SAM simply discards the oldest iteration to make space for ∆Xk-1
and ∆rk-1. The reduced memory requirement in RST-AM makes it applicable for solving more
challenging problems in machine learning.
6
Published as a conference paper at ICLR 2022
4 Experiments
We validated the effeCtiveness of our proposed ST-AM methods in various appliCations in fixed-point
iterations and nonConvex optimization, inCluding linear and nonlinear problems, deterministiC and
stoChastiC optimization. SpeCifiCally, we first tested ST-AM in linear problems, CubiC-regularized
quadratiC minimization (Carmon & DuChi, 2020) and a multisCale deep equilibrium (MDEQ) model
(Bai et al., 2020). Then we applied RST-AM to train neural networks and Compared them with
several first-order and seCond-order optimizers. Experimental details are in Appendix D.
4.1	Experiments about the basic ST-AM and MST-AM
We verified the properties of ST-AM deClared in Theorem 1 and 2 by solving four problems (details
are in Appendix D.1): (I) strongly Convex quadratiC optimization (Corresponding to Theorem 1);
(II) solving a nonsymmetric linear system Ax = b (corresponding to K = 0 in Theorem 2); (III)
cubic-regularized quadratic minimization minχ∈Rd f (x) := ∣∣Ax - b∣∣2 + Mf ∣∣xk2 (corresponding to
K > 0 in Theorem 2); (IV) root-finding problems in MDEQ on CIFAR-10 (KrizheVSky et al., 2009).
The compared methods were gradient descent (GD), fixed-point iteration (FP), conjugate residual
method (CR) (Saad, 2003), BFGS (Nocedal & Wright, 2006), Broyden’s method (Broyden, 1965),
and the full-memory AM (AM). We used the basic ST-AM to solve Problem I and II, and MST-AM
(cp = cq = 1) to solve Problem III and IV.
(a) Problem I
IO-13
TY-,-w
Ooo 厂
Illw
p==丈一
2 5 8 14
- - ---
10101010-10^
y===
O IO 20	30	40	50
iteration
(C) ProblemnI(M = 0.1)
Iiooooo
=Z="--(Z --- :p.IeM>peg
QS 6,2 Q
SSoIB31
IOO
小
5 0 5 0 5 0 5
8 8 7 7 6 6 5
% Λ3」n。OVəl
(d) Forward process	(e) Backward process	(f) Test accuracy and loss
Figure 1: (a) ∣∣rk ∣∣2∕∣∣r0∣∣2 of GD and CR, and ∣∣fk k2∕∣∣r0k2 of AM and ST-AM for solving Prob-
lem I; (b) Ilrkk2∕i∣r01∣2 for solving Problem II; (c) ∣∣rk∣∣2∕∣∣r0∣∣2 for solving Problem III (M = 0.1);
(d)(e) relative residuals of the forward and backward root-finding processes in MDEQ, and shaded
areas correspond to the standard deviations; (f) test accuracy and loss in MDEQ/CIFAR-10.
The numerical results shown in Figure 1 demonstrate the power of ST-AM as a variant of Krylov
subspace methods. It significantly accelerates the slow convergence of the GD or FP method, and
can outperform AM. Figure 1(a) clearly verifies the correctness of Theorem 1: within the machine
precision, the intermediate residual rk of ST-AM coincides with the residual rk of CR. Note that
AM fails to coincide with CR and ST-AM due to the intrinsic numerical weakness to solve (3), as
also pointed out in (Walker & Ni, 2011). Figure 1(b) shows that ST-AM can outperform CR, though
both methods enjoy short-term recurrences and are equivalent for solving SPD linear systems. Fig-
ure 1(c) also shows MST-AM surpasses BFGS in solving cubic-regularized problems. The tests in
MDEQ/CIFAR-10 indicate that MST-AM is comparable to the full-memory methods in the forward
root-finding process and converges faster in the backward process. The accuracy is also comparable.
7
Published as a conference paper at ICLR 2022
IO2
10-1
10^4
IO-7
IOTO .
au~pe∙J6≡UUOU Peenbs
IL
——5GD
Adam
——SAM(2)
——SAM(5)
——SAM(IO)
——RSTAM
0	50	100	150	200
epoch
0	50	100	150	200
epoch
(a) Train loss (w/o preconditioning) (b) SNG (w/o preconditioning) (c) Train loss (w/ preconditioning)
Figure 2:	Experiments on MNIST. (a)(b) Training loss and the squared norm of gradient (SNG) (w/o
preconditioning for SAM, RST-AM); (c) Training loss (w/ preconditioning for SAM, RST-AM).
4.2 Experiments about RST-AM
We applied RST-AM to train neural networks, with full-batch training on MNIST (LeCun et al.,
1998), and mini-batch training on CIFAR-10/CIFAR-100 and Penn Treebank (Marcus et al., 1993).
Experiments on MNIST. We trained a convolutional neural network (CNN) on MNIST to see the
convergence behaviour of RST-AM in nonconvex optimization (cf. Theorem 3), for which we were
only concerned about the training loss. Figure 2(a)(b) show that the short-memory SAMs (m = 2, 5)
hardly show any improvement over the first-order optimizers SGD and Adam, while RST-AM can
close the gap of the long-memory (m = 10) and the short-memory methods. We also considered
the effect of preconditioning on RST-AM (See Appendix A.3). The notation “A_B” means B method
preconditioned by A method. Figure 2(c) indicates that preconditioning also works much better for
RST-AM than sAm(2), and RMSProP-RST-AM can outperform the non-preconditioned RST-AM.
Table 1: Experiments on CIFAR10/CIFAR100. “-” means failing to complete the test in our device
due to memory limit. “*” indicates numbers published in (Wei et al., 2021).
(a) Final TOP1 test accuracy (mean ± standard deviation) (%) for training 160 epochs.
Method	Test accuracy on CIFAR10						Test accuracy on CIFAR100		
	ResNet18	ResNet20	ResNet32	ResNet44	ResNet56	WRN16-4	ResNet18	ResNeXt	DenseNet
SGDM*	94.82±.15	92.03±.16	92.86±.15	93.10±.23	93.47±.28	94.90±.09	77.27±.09	78.41±.54	78.49±.12
Adam*	93.03±.07	91.17±.13	92.03±.28	92.28±.62	92.39±.23	92.45±.11	72.41±.17	73.57±.17	70.80±.23
AdaBound	94.25±.31	90.77±.08	91.73±.06	92.00±.18	92.44±.04	93.50±.12	75.07±.14	75.74±.20	76.06±.13
AdaBelief*	94.65±.13	91.15±.21	92.15±.17	92.79±.24	93.30±.07	94.46±.13	76.25±.06	78.27±.16	78.83±.15
Lookahead*	94.92±.33	92.07±.04	92.86±.15	93.26±.24	93.36±.13	94.90±.15	77.63±.35	78.93±.12	79.37±.16
AdaHessian*	94.36±.09	91.92±.32	92.18±.18	92.74±.11	92.40±.06	94.04±.12	76.59±.42	-	-
SAM(2)	95.07±.04	92.14±.33	93.04±.23	93.46±.09	93.66±.06	95.07±.16	77.51±.24	79.02±.21	80.00±.23
SAM(10)*	95.17±.10	92.43±.19	93.22±.32	93.57±.14	93.77±.12	95.23±.07	78.13±.14	79.31±.27	80.09±.52
RST-AM	95.27±.04	92.39±.11	93.24±.36	93.52±.02	93.69±.18	95.21±.09	77.91±.22	79.53±.34	80.36±.25
(b) The memory and computation cost compared with SGDM. The notations “m”,“t/e” and “t” are abbrevia- tions of memory, per-epoch time and total running time, respectively.												
Cost	CIFAR10/ResNet18			CIFAR10/WRN16-4			CIFAR100/ResNeXt50			CIFAR100/DenseNet121		
(× SGDM)	m	t/e	t	m	t/e	t	m	t/e	t	m	t/e	t
SGDM*	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00
SAM(10)*	1.73	1.78	1.00	1.26	1.28	0.80	1.30	1.16	0.58	1.16	1.19	0.60
RST-AM	1.05	1.46	0.82	1.03	1.14	0.71	1.04	1.07	0.54	1.01	1.11	0.55
Experiments on CIFAR. We trained ResNet18/20/32/44/56 (He et al., 2016), WideResNet16-4
(Zagoruyko & Komodakis, 2016) (abbr. WRN16-4) on CIFAR-10, and ResNet18, ResNeXt50 (Xie
et al., 2017), DenseNet121 (Huang et al., 2017) on CIFAR-100. The baseline optimizers were
SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang et al., 2020), Lookahead (Zhang
et al., 2019), AdaHessian (Yao et al., 2021) and SAM. Here, some results of the baselines in (Wei
et al., 2021) were used for reference since the experimental settings were the same. Table 1(a) shows
RST-AM improves SAM(2) and has comparable test accuracy to SAM(10). RST-AM also outper-
forms other baseline optimizers. Table 1(b) reports the memory and computation cost, where we
8
Published as a conference paper at ICLR 2022
used SGDM as the baseline and other optimizers were terminated when achieving a comparable or
better test accuracy than SGDM. It indicates that RST-AM introduces ≤ 5% extra memory over-
head compared with SGDM, and significantly reduces the memory footprint of AM. Since RST-AM
needs fewer training epochs, the total running time is less than SGDM.
epoch
(a) 1-Layer LSTM
(b) 2-Layer LSTM	(c) 3-Layer LSTM
Figure 3:	Validation perplexity of training 1,2,3-layer LSTM on Penn Treebank.
Experiments on Penn Treebank. We trained
LSTMs with 1-3 layer(s) on Penn Treebank and
report the validation perplexity in Figure 3 and
test perplexity in Table 2 (lower is better). The
results suggest that RST-AM is comparable to
or even better than SAM(10). The improvement
of RST-AM over other optimizers is also sig-
nificant. We report the computation and mem-
ory cost in Appendix D.2.4. RST-AM can still
surpass Adam while using much fewer epochs,
thus reducing the total running time.
Table 2: Test perplexity of training 1,2,3-layer
LSTM on Penn Treebank. Lower is better.
Method	1-Layer	2-Layer	3-Layer
SGDM	83.48±.03	65.89±.18	61.88±.23
Adam	80.33±.15	64.32±.06	59.72±.13
AdaBelief	81.29±.35	64.68±.10	60.46±.07
SAM(2)	80.79±.19	65.52±.29	61.13±.12
SAM(10)	78.78±.14	62.46±.11	58.93±.09
RST-AM	78.41±.18	62.46±.08	58.31±.23
Table 3: Test accuracy (%) for adversarial training.
Optimizer	CIFAR10∕ResNet18				CIFAR100/DenseNet121			
	Clean	FGSM	PGD-20	C&Wg	Clean	FGSM	PGD-20	C&Wg
SGD	82.16	63.23	51.91	50.22	59.45	39.76	30.92	29.00
RST-AM	82.53	63.78	52.43	50.52	60.48	40.41	31.20	29.52
Table 4: FID score for SN-GAN.
Method	Adam	AdaBelief	RST-AM
Best FID	13.07±.18	12.80±.09	12.05±.15
Final FID	13.34±.14	13.59±.21	12.50±.29
Adversarial training. We applied RST-AM to adversarial training (Madry et al., 2018) as the outer-
optimizer and compared it with SGD by the clean test accuracy and robust test accuracy. The results
on CIFAR10/ResNet18 and CIFAR100/DenseNet121 are reported in Table 3. It can be seen that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy. More results
can be found in Appendix D.2.5.
Generative adversarial network (GAN). We tested RST-AM by training a GAN equipped with
spectral normalization (SN-GAN) (Miyato et al., 2018), where the generator and discriminator net-
works were ResNets and the dataset was CIFAR-10. Table 4 shows that RST-AM can achieve lower
FID score (better accuracy) than Adam and AdaBelief.
5 Conclusion
In this paper, to address the memory issue of Anderson mixing (AM), we develop a novel class
of short-term recurrence AM methods (ST-AM) and test it in various applications, including solv-
ing linear and nonlinear problems, deterministic and stochastic optimization. We give a complete
theoretical analysis of the proposed methods. We prove that the basic ST-AM is equivalent to the
full-memory AM in strongly convex quadratic optimization. With some minor changes, it has local
linear convergence for solving general fixed-point problems under some common assumptions. We
also introduce the regularized form of ST-AM and analyze its convergence properties. The numerical
results show that the ST-AM methods are comparable to or even better than the long-memory AM
while consuming less memory. The regularized ST-AM also outperforms many existing optimizers
in training neural networks in various tasks.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work was supported by the National Key R&D Program of China (No. 2021YFA1001300),
National Natural Science Foundation of China (No.61925601), Tsinghua University Initiative Scien-
tific Research Program, National Natural Science Foundation of China (No.11901338), and Huawei
Noah’s Ark Lab. We thank all anonymous reviewers for their valuable comments and suggestions
on this work.
References
Hengbin An, Xiaowei Jia, and Homer F Walker. Anderson acceleration and application to the three-
temperature energy equations. Journal OfComputational Physics, 347:1-19, 2017.
Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM
(JACM), 12(4):547-560, 1965.
Donald G Anderson. Comments on “Anderson acceleration, mixing and extrapolation”. Numerical
Algorithms, 80(1):135-234, 2019.
Marcin Andrychowicz, Misha DeniL Sergio Gomez Colmenarejo, Matthew W Hoffman, David
Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient
descent by gradient descent. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, pp. 3988-3996, 2016.
Akash Arora, David C Morse, Frank S Bates, and Kevin D Dorfman. Accelerating self-consistent
field theory of block polymers in a variable unit cell. The Journal of Chemical Physics, 146(24):
244902, 2017.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural
Information Processing Systems, 32:690-701, 2019.
Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale deep equilibrium models. In Advances
in Neural Information Processing Systems (NeurIPS), 2020.
Albert S Berahas, Frank E Curtis, and Baoyu Zhou. Limited-memory BFGS with displacement
aggregation. Mathematical Programming, pp. 1-37, 2021.
Lon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Claude Brezinski and Michela Redivo-Zaglia. Shanks function transformations in a vector space.
Applied Numerical Mathematics, 116:57-63, 2017.
Claude Brezinski, Michela Redivo-Zaglia, and Yousef Saad. Shanks sequence transformations and
Anderson acceleration. SIAM Review, 60(3):646-669, 2018.
Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics
of Computation, 19(92):577-593, 1965.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
Stan Cabay and LW Jackson. A polynomial extrapolation method for finding limits and antilimits
of vector sequences. SIAM Journal on Numerical Analysis, 13(5):734-752, 1976.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Yair Carmon and John C Duchi. First-order methods for nonconvex quadratic minimization. SIAM
Review, 62(2):395-436, 2020.
Mauro Cettolo, Jan Niehues, Sebastian Stuker, LUisa Bentivogli, and Marcello Federico. Report
on the 11th IWSLT evaluation campaign, IWSLT 2014. In Proceedings of the International
Workshop on Spoken Language Translation, Hanoi, Vietnam, volume 57, 2014.
10
Published as a conference paper at ICLR 2022
Hans De Sterck and Yunhui He. On the asymptotic linear convergence speed of Anderson accelera-
tion, Nesterov acceleration, and nonlinear GMRES. SIAM Journal on Scientific Computing, (0):
S21-S46, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248-255. IEEE, 2009.
Yunbin Deng. Deep learning on mobile devices: A review. In Mobile Multimedia/Image Processing,
Security, and Applications 2019, volume 10993, pp. 109930A. International Society for Optics
and Photonics, 2019.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.
Iain S Duff, Albert Maurice Erisman, and John Ker Reid. Direct methods for sparse matrices.
Oxford University Press, 2017.
Rick Durrett. Probability: Theory and examples, volume 49. Cambridge University Press, 2019.
R P Eddy. Extrapolating to the limit of a vector sequence. In Peter C.C. Wang, Arthur L. Schoenstadt,
Bert I. Russak, and Craig Comstock (eds.), Information Linkage Between Applied Mathematics
and Industry, pp. 387-396. Academic Press, 1979.
Claire Evans, Sara Pollock, Leo G Rebholz, and Mengying Xiao. A proof that Anderson acceler-
ation improves the convergence rate in linearly converging fixed-point methods (but not in those
converging quadratically). SIAM Journal on Numerical Analysis, 58(1):788-810, 2020.
Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceleration.
Numerical Linear Algebra with Applications, 16(3):197-221, 2009.
Anqi Fu, Junzi Zhang, and Stephen Boyd. Anderson accelerated Douglas-Rachford splitting. SIAM
Journal on Scientific Computing, 42(6):A3560-A3583, 2020.
Alejandro J Garza and Gustavo E Scuseria. Comparison of self-consistent field convergence accel-
eration techniques. The Journal of Chemical Physics, 137(5):054110, 2012.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Gene H Golub and Charles F Van Loan. Matrix computations, 4th. Johns Hopkins, 2013.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
William W Hager and Hongchao Zhang. A survey of nonlinear conjugate gradient methods. Pacific
Journal of Optimization, 2(1):35-58, 2006.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Magnus R. Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear systems.
Journal of Research of the National Bureau of Standards, 49:409-435, 1952.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
Neural Information Processing Systems, 30, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700-4708, 2017.
Carl T Kelley. Numerical methods for nonlinear equations. Acta Numerica, 27:207-287, 2018.
11
Published as a conference paper at ICLR 2022
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tamara G Kolda, Dianne P O’leary, and Larry Nazareth. BFGS with update skipping and varying
memory. SIAM Journal on OPtimization, 8(4):1060-1083, 1998.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Lin Lin, Jianfeng Lu, and Lexing Ying. Numerical methods for Kohn-Sham density functional
theory. Acta Numerica, 28:405-539, 2019.
Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming, 45(1):503-528, 1989.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning RePresentations, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning RePresentations, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning RePresentations, 2018.
Vien Mai and Mikael Johansson. Anderson acceleration of proximal gradient methods. In Interna-
tional Conference on Machine Learning, pp. 6620-6629. PMLR, 2020.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. 1993.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning RePresentations,
2018.
Aryan Mokhtari, Mark Eisen, and Alejandro Ribeiro. IQN: An incremental quasi-Newton method
with local superlinear convergence rate. SIAM Journal on OPtimization, 28(2):1670-1698, 2018.
Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Mathematical Programming, 175(1):69-107, 2019.
Arkadij Semenovic Nemirovski and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Jorge Nocedal and Stephen Wright. Numerical oPtimization. Springer Science & Business Media,
2006.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for ComPutational Linguistics, pp. 311-318, 2002.
Florian A Potra and Hans Engler. A characterization of the behavior of the Anderson acceleration
on linear problems. Linear Algebra and Its APPlications, 438(3):1002-1011, 2013.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12
(1):145-151, 1999.
Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
International Conference on Machine Learning, pp. 8093-8104. PMLR, 2020.
12
Published as a conference paper at ICLR 2022
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 400-407, 1951.
Youcef Saad and Martin H Schultz. GMRES: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing, 7(3):856-
869, 1986.
Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.
Damien Scieur, Alexandre dAspremont, and Francis Bach. Regularized nonlinear acceleration.
Mathematical Programming, 179(1):47-83, 2020.
Daniel Shanks. Non-linear transformations of divergent and slowly convergent sequences. Journal
of Mathematics and Physics, 34(1-4):1-42, 1955.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Alex Toth and CT Kelley. Convergence analysis for Anderson acceleration. SIAM Journal on
Numerical Analysis, 53(2):805-819, 2015.
Henk A Van der Vorst and C Vuik. The superlinear convergence behaviour of GMRES. Journal of
Computational and Applied Mathematics, 48(3):327-341, 1993.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on
Numerical Analysis, 49(4):1715-1735, 2011.
Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927-956, 2017.
Fuchao Wei, Chenglong Bao, and Yang Liu. Stochastic Anderson mixing for nonconvex stochastic
optimization. Advances in Neural Information Processing Systems, 34, 2021.
Peter Wynn. On a device for computing the em(Sn) transformation. Mathematical Tables and Other
Aids to Computation, pp. 91-96, 1956.
Peter Wynn. Acceleration techniques for iterated vector and matrix problems. Mathematics of
Computation, 16(79):301-322, 1962.
Saining Xie, RoSS Girshick, Piotr Dollar, ZhuoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1492-1500, 2017.
Yunan Yang. Anderson acceleration for seismic inversion. Geophysics, 86(1):R99-R108, 2021.
ZheWei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
AdaHessian: An adaptive second order optimizer for machine learning. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 35, pp. 10665-10673, 2021.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. In British Machine Vision
Conference 2016. British Machine Vision Association, 2016.
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forWard, 1 step back. In Advances in Neural Information Processing Systems, pp. 9597-9608,
2019.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris, and James Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in observed
gradients. Advances in Neural Information Processing Systems, 33, 2020.
13
Published as a conference paper at ICLR 2022
A	Additional preliminaries
We provide some additional preliminaries in this section for readers that are not familiar with An-
derson mixing, fixed-point iterations and some techniques mentioned in the main paper.
A. 1 Fixed-point iteration
The fixed-point problem and the optimization problem are the main application scenarios of our
methods. It is worth pointing out that there are some minor differences between these two prob-
lems that make algorithm designs different. The key difference is that for a fixed-point problem,
the Jacobian (if exists) is generally not symmetric while for optimization, the Hessian is naturally
symmetric. In principle, a fixed-point solver can also be applicable for an optimization problem
since the first-order necessary condition of minχ∈Rd f (x), where f : Rd → R, is Vf (x) = 0.
Consider a contraction mapping g : Rd 7→ Rd, i.e. for some κ < 1, kg(x) - g(y)k2 ≤ κkx -
y∣∣2, ∀x,y ∈ Rd. According to the contraction mapping theorem, a unique fixed point x* exists
for g and the iterates generated by the iteration Xk+ι = g(xk) converge to x*, starting from any
x0 ∈ Rd. In practice, the damped fixed-point iteration is also commonly used: given the βk ∈ (0, 2),
the update is xk+1 = (1 - βk)xk + βkg(xk) = xk + βkrk, where rk := g(xk) - xk is called the
residual.
The fixed-point iteration, also known as Picard iteration in some areas, can converge very slowly in
practice. Anderson mixing is a method to improve the convergence.
A.2 Another form of Anderson mixing
The derivation of Anderson mixing (AM) in Section 3.1 explicitly interprets AM as a two-step
procedure. In the literature, there is another equivalent form of AM.
Let the projection coefficients Γk = (γk(1), . . . , γk(m) )T ∈ Rm. Define the auxiliary coefficients
{θk(j)}jm=0 as θk(0) = γk(1),θk(j) = ∆γk(j)(j = 1,...,m - 1), θk(m) = 1 - γk(m), then Pjm=0 θk(j) =
1 and 尸k = Pj=O θj)rk-m+j. Hence the least squares problem (3) can be reformulated as a
constrained problem min{θ(j)}m k Pjm=0 θk(j)rk-m+jk2 s.t. Pjm=0 θk(j) = 1, which indicates that
as a linear combination of the historical residuals {rj- }m=k-m,尸k is minimal in terms of the L2-norm.
Also, the projection step and the mixing step (2) can be reformulated as Xk = Pjm=0 θk(j)xk-m+j
and xk+1 = (1-βk) Pjm=0 θk(j)xk-m+j +βk Pjm=0 θk(j)g(xk-m+j),respectively. Such formulation
is also adopted in the literature (Toth & Kelley, 2015; Mai & Johansson, 2020; Scieur et al., 2020).
Let Hk be the solution to the constrained optimization problem (Fang & Saad, 2009)
min kHk - βkIkF subject to HkRk = -Xk,	(19)
Hk
then the update (4) is xk+1 = xk +Hkrk. It suggests that AM is a multisecant quasi-Newton method
(Fang & Saad, 2009).
A.3 Preconditioned Anderson mixing
Like preconditioning for Krylov subspace methods (Saad, 2003), preconditioning can also be incor-
porated into Anderson mixing to mitigate the ill-conditioning of the original problem (Wei et al.,
2021). The idea is to replace the mixing step in (2) via a preconditioned mixing.
Suppose that there is a basic solverpreconditioner(xk, sk), which works as a black-box procedure
that updates xk given the residual sk, i.e. xk+1 = preconditioner (xk, sk), then the preconditioned
mixing of RST-AM is
Xk+1 = Preconditioner(Xk ,尸k).
which substitutes for the Line 13 in Algorithm 1. The simple mixing (Line 13 in Algorithm 1
can be seen as a special case by defining preconditioner(xk, sk) = xk + βksk, i.e. precondi-
tioned by a damped fixed-point iteration. Moreover, if we write the preconditioning operation as
14
Published as a conference paper at ICLR 2022
preconditioner (xk, sk) := xk + Gksk, where Gk is the matrix to approximate the inverse Jaco-
bian, then the preconditioned AM is
xk+1 = Xk + Gkrk -(Xk + GkRk)(RTRk 户RTrk	QO)
(cf. the definitions of Xk, Rk in Section 3.1). The matrix Hk = Gk - (Xk + GkRk)(RTRk)^RT
forms a low-rank updated approximation to the inverse Jacobian: it solves
min kHk - GkkF subject to HkRk = -Xk,
Hk
which is a direct extension of (19).
For the stochastic Anderson mixing, using damped projection can be helpful, i.e. Xk = (1 - ɑk )xk +
αk (xk - XkΓk) = Xk - akXkΓk and rk = rk - αkRkΓk correspondingly. Then the preconditioned
AM with damped projection is
Xk+1 = Xk + Gk rk - αk (Xk + Gk Rk )Γk .	(21)
B Additional discussion
B.1	The memory and computational efficiency
Since the ST-AM methods only need to store two previous iterations, the memory and computational
cost can be reduced to be close to first-order methods.
Memory cost. Assume that the iteration number is k and the model parameter size is d. The full-
memory AM stores all previous iterations, thus the additional memory is 2kd. To reduce the memory
overhead, the limited-memory AM(m) only maintains the most recent m iterations while discarding
the older historical information (cf. (1)). Hence the additional memory of AM(m) is 2md. Choosing
a suitable m can be problem-dependent (Walker & Ni, 2011), and it is often suggested that m ≥ 5
(Mai & Johansson, 2020; Fu et al., 2020) to avoid too much historical information being discarded.
There is no equivalence between AM(m) and the full-memory AM. On the other side, our ST-AM
methods have the same memory footprint as that of AM(2), i.e. only introducing 4d additional
memory. For a large model that contains millions or even billions of parameters, such reduction in
memory requirement can be significant.
Computational cost. For ST-AM, besides the gradient evaluations, the main additional computa-
tional cost comes from vector corrections, the projection and mixing step. These operations are
very cheap: matrix multiplications of R2×d × Rd×2, R2×d × Rd×1, and Rd×2 × R2×1; the pseudo-
inverse can be exactly solved as the size of the matrix is R2×2. Also, the QkT-1Qk-1 and PkT-1Pk-1
in Line 6 in Algorithm 1 can reuse the results in the previous iteration (cf. Line 11). So the total
additional computational cost of RST-AM is O(d).
B.2	Applicability
In the main paper, we develop a class of short-term recurrence Anderson mixing. Among them, the
basic ST-AM can serve as a new linear solver for linear systems; MST-AM can be used as a new
fixed-point solver for nonlinear equations; RST-AM is a new method for optimization. These meth-
ods are based on Anderson mixing and have close relationship between each other. The theoretical
analysis and numerical results show the great potential of ST-AM methods for various applications.
Here, we give some comparisons between the ST-AM methods and some other classical methods.
ST-AM versus current solvers for linear systems. ST-AM is an iterative solver. It is equivalent
to the full-memory AM and GMRES in strongly convex quadratic optimization (or SPD linear sys-
tems), and can also have linear convergence rate for general nonsymmetric linear systems. Since itis
based on short-term recurrences, like the CG method, the memory and per-iteration cost of ST-AM
is economical. On the other side, the LU factorization based methods (Golub & Van Loan, 2013)
are direct solvers that can incur overwhelming memory and computation cost for large sparse linear
systems. Although sparse direct solvers (Duff et al., 2017) can alleviate overhead, they are often
more complicated to implement and difficult for parallel computing. Moreover, iterative solvers can
benefit from the preconditioning technique that improves convergence (Saad, 2003). Also, ST-AM
15
Published as a conference paper at ICLR 2022
has advantages over other iterative methods since it does not need to directly access the matrix and
only the residual is required. In the case that explicitly accessing the matrix is difficult, this property
is appealing. Hence, unlike nonlinear CG that relies on line search, it is direct to extend ST-AM
to unconstrained optimization where the gradient is commonly available while the Hessian can be
too costly to obtain. Moreover, ST-AM is very flexible and any iterative solver can be viewed as a
black-box iterative process to be accelerated by ST-AM.
MST-AM versus other quasi-Newton methods for nonlinear equations. MST-AM has the na-
ture of quasi-Newton methods since it is built upon AM that is recognized as a multisecant quasi-
Newton method (Fang & Saad, 2009). In scientific computing, AM has been successfully applied to
solve many difficult nonlinear problems arising from many areas (An et al., 2017; Lin et al., 2019;
Fu et al., 2020; Yang, 2021). Since AM only manipulates residuals and does not use line-search
or trust-region technique, it is efficient to apply AM to accelerate a slowly convergent black-box
iterative process. A comprehensive discussion about the applicability of AM for nonlinear problems
and the relation between AM and Broyden’s methods can be found in (Fang & Saad, 2009).
One of the biggest issues of AM and other quasi-Newton methods is the additional memory over-
head, because they need to store historical iterations to form the secant equations. To make a com-
promise, limited-memory quasi-Newton methods such as L-BFGS (Liu & Nocedal, 1989) are pro-
posed in which only limited number of historical iterations are stored. In each update, the oldest
iteration is discarded to make space for the up-to-date secant pair. As a result, the limited-memory
quasi-Newton methods can lose the local superlinear convergence properties achieved by the full-
memory schemes (Berahas et al., 2021). Also, as stated by Berahas et al. (2021), the choice of the
number of historical iterations (i.e. historical length) is problem-dependent, and one does not know
a priori the best choice when solving a particular problem.
Unlike the limited-memory quasi-Newton methods whose performance can be sensitive to the his-
torical length, our MST-AM method only needs to store two corrected historical iterations. MST-
AM carefully incorporates historical information through orthogonalization. In the ideal case, i.e.
strongly convex quadratic optimization (or SPD linear systems), it is equivalent to the full-memory
AM which means there is no loss of historical information. Our experiments verify the theoretical
properties of MST-AM and indicate MST-AM can be a competitive method for nonlinear equations.
RST-AM versus first-order methods for stochastic optimization. For stochastic optimization
such as training deep neural networks in machine learning, the first-order methods have come to
dominate the field due to the low memory and per-iteration cost. The RST-AM is an extension of
ST-AM and MST-AM to tackle this challenging problem. RST-AM has theoretical guarantees in
deterministic/stochastic optimization and also inherits several advantages of AM and ST-AM:
•	Fast convergence in quadratic optimization. It is important to possess such property for an
optimizer since a smooth function can be approximated by a quadratic function in the local
region around the optima and many techniques such as trust region (Nocedal & Wright,
2006) rely on this local approximation. Adaptive learning rate methods such as AdaGrad
(Duchi et al., 2011), Adam (Kingma & Ba, 2014) use a diagonal approximation of the
Fisher information matrix that is an approximation of the Hessian. However, in the sim-
ple quadratic case, these methods can only roughly match the performance of the Jacobi
method for solving linear systems (Saad, 2003), which is better than gradient descent but
far inferior to the powerful Krylov subspace methods. The momentum method mimics
CG method by incorporating a historical iteration into the search direction. However, the
choice of momentum and stepsize can be an art (Sutskever et al., 2013). For RST-AM,
there is no need to determine the stepsize and the fast convergence rate of ST-AM can be
recovered by simply setting αk = 1 and δk(1) = δk(2) = 0. For more difficult functions, the
damping and regularization in RST-AM can be enabled to improve stability.
•	Theoretical guarantee in stochastic optimization. If only first-order information can be ac-
cessed, the SGD (Ghadimi & Lan, 2013) achieves an optimal convergence rate O(1/2) to
obtain an -accurate solution (Nemirovski & Yudin, 1983). In such case, it seems to be abig
mismatch for current second-order methods, because there is no theoretical improvement
albeit with more memory and computation resource. Such mismatch may also account for
the popularity of first-order methods. Since RST-AM has very limited additional memory
16
Published as a conference paper at ICLR 2022
overhead, and also achieves the O(1/2) complexity, it can be applied to many applications
that are dominated by first-order methods.
•	Flexibility in use. The application of RST-AM can be very flexible. In principle, RST-AM
can be applied to improve any slowly convergent black-box iterative process by viewing
the latter as a fixed-point iteration. For example, consider accelerating a solver of a com-
mercial software, where we have no access to the underlying codes to provide our custom
implementation, or some case rewriting the codes is too cumbersome. Moreover, RST-
AM can efficiently incorporate the preconditioning technique. Hence any optimizer, even
an optimizer built upon neural networks (Andrychowicz et al., 2016), can be used as a
preconditioner for RST-AM. Preconditioning largely enhances the applicability of RST-
AM for various applications. For example, RST-AM can be preconditioned by Adam for
the language task and SGDM for image classification. So any fine-tuned first-order opti-
mizer can be combined with RST-AM to achieve an overall improvement. As RST-AM is
light-weight, this additional cost is marginal and can be largely counteracted by the actual
improvement. So in some sense, the purpose of RST-AM is not to totally replace current
off-the-shelf optimizers but to achieve collaborative effectiveness: RST-AM is aware of the
second-order information while the first-order method can mitigate the ill-conditioning of
the problem.
Overall, the ST-AM methods have wide applicability and can be competent methods from both
theoretical and practical perspectives.
C Proofs
We give more details about ST-AM and the proofs of the theorems in the main paper.
C.1 The basic ST-AM for strongly convex quadratic optimization
Recall that the strongly convex quadratic optimization is formulated as
min f (x) := gxTAx 一 bTx,	(22)
where A ∈ Rd×d is SPD, b ∈ Rd. Solving (22) is equivalent to solving the SPD linear system
Ax = b.	(23)
The detail of the basic ST-AM is given in Algorithm 2.
We first state the relationship of AM with GMRES in the following proposition. Similar results can
also be found in (Walker & Ni, 2011; Wei et al., 2021).
Let xkG, rkG := b- AxkG denote the k-th GMRES iterate and residual, respectively, and Kk(A, v) :=
span{v, Av, . . . , Ak-1v} denotes the k-th Krylov subspace generated by A and v. Define ej :=
(1, 1,. . . , 1)T ∈ Rj for j ≥ 1. Let range(X ) denote the linear space spanned by the columns
of X. The main results of the full-memory AM are stated in Proposition 1 and Proposition 2.
The Proposition 1 is the same as the Proposition 2 in (Wei et al., 2021), and we restate it here for
completeness. The Proposition 2 is new as far as we know.
Proposition 1 (General linear system). For solving a general linear system Ax = b with the full-
memory AM (m = k), suppose that βk > 0 and the fixed-point map is g(x) = (I- A)x + b. If
the initial point of AM is xo = XG and rank(Rk) = m, then the intermediate iterate Xk satisfies
Xk = xG ∙
Proof. The definition of the fixed-point map suggests that the residual rk = g(Xk)- Xk = b- AXk.
Since Rk = -AXk and A is nonsingular, we have rank(Xk) = m. We first show
range(Xk) = Kk(A,r0G)	(24)
by induction. We abbreviate Kk (A, r0G) as Kk in this proof.
17
Published as a conference paper at ICLR 2022
Algorithm 2 ST-AM for strongly convex quadratic optimization		
Input: X0 ∈ Rd, βk > 0, 0 < maxjiter ≤ d.		
Output: X ∈ Rd		
1	: P0,Q0 =0∈Rd×2,p0,q0 =0 ∈Rd	
2	for k = 0,1,..., max_iter do	
3	rk = -▽/(Xk )	
4	: if k > 0 then	
5	:	p = Xk - Xk-1, q = rk - rk-1	(Compute ∆Xk-1, ∆rk-1)
6	q = q - Qk-I(QT-ιq),p = p — Pk-I(QkT-Iq)	(q ⊥ Qk-1)
7	Pk = P/I引 ∣2,qk = q/kqk2	(∣qk∣2 = 1)
8	:	Pk = [pk-1 , pk ], Qk = [qk-1 , qk ]	(QkTQk = I2)
9	: end if	
10	:	Γk = Qk rk	
11	xk = xk - Pkγk, rk = rk - Qkγk	(Projection step:尸k ⊥ Qk)
12	Xk+1 = Xk + βk 尸 k	(Mixing step)
13	if 忻k ∣∣2 = 0 then	
14	:	break	
15	: end if	
16	: end for	
17	: return Xk	
First, ∆x0 = β0r0 = β0r0G since x1 = x0 + β0r0. If k = 1, then the proof is complete. Then,
suppose that k > 1 and, as an inductive hypothesis, that range(Xk-1) = Kk-1. With (4) we have
∆xk-1 = xk - xk-1
= βk-1 rk-1 - (Xk-1 + βk-1 Rk-1 )Γk-1
= βk-1 (b - Axk-1) - (Xk-1 - βk-1AXk-1)Γk-1
=βk-1b - βk-1A(XO + δx0 + •…+ ∆xk-2) - (Xk-I - βk-1AXk-1)rk-1
= βk-1r0 - βk-1AXk-1ek-1 - (Xk-1 - βk-1AXk-1)Γk-1.	(25)
Since r0 ∈ Kk-1, and by the inductive hypothesis range(Xk-1) ⊆ Kk-1 which also implies
range(AXk-1) ⊆ Kk, we know ∆xk-1 ∈ Kk, which implies range(Xk) ⊆ Kk. Since we assume
rank(Xk) = m = k which implies dim(range(Xk)) = dim(Kk), we have range(Xk) = Kk, thus
completing the induction. As a result, we also have
range(Rk) = range(AXk) = AKk(A,r0G).	(26)
Recalling that to determine Γk, we solve the least squares problem (3) and Rk = -AXk. We have
Γk = arg min krk + AXkΓk2.	(27)
Γ∈Rm
Since rank(AXk) = rank(Xk) = m, (27) has a unique solution. Also, since rk = b - Axk =
b — A(xo + Xkek) = ro — AXkek, We have Irk + AXkΓ = r0 — AXkek + AXkΓ = r° — AXkΓ,
where Γ = ek - Γ. So Γk solves (27) if and only if Γk = ek - Γk solves
.. .~..
min ∣∣ro - AXkΓ∣∣2,	(28)
Γ∈Rm
According to (24), (28) is equal to minz∈Km (A,rG) kr0 - Azk2 which is the GMRES minimization
problem. Since the solution of (28) is also unique, we have
kG
xk = xk - Xkγk = xk - Xk (e - γk) = x0 + Xkrk = Xk .
□
In Proposition 1, the assumption that Rk has full column rank is critical to ensure no stagnation
occurs in AM for solving a general linear system. In fact, for SPD linear systems (23) or strongly
convex quadratic optimization (22), when AM breaks down, i.e. Rk is rank deficient, AM obtains
the exact solution, as shown in the next proposition.
18
Published as a conference paper at ICLR 2022
Proposition 2 (SPD). For applying the full-memory AM to minimize a strongly convex quadratic
problem (22), or equivalently, solve a SPD linear system (23), suppose that βk > 0 and the fixed-
point map is g(x) = (I - A)x + b. If rank(Rk) = k holds for 1 ≤ k < s while failing to hold for
k = S, where S ≥ 1, then the residual of AM satisfies r = fs-ι = 0.
Proof. The definition ofg suggests that the residual rk = g(xk) -xk = b-Axk. The relation Rk =
-AXk holds during the iterations and the nonsingularity of A implies rank(Xk) = rank(Rk).
For S = 1, since the first step of AM is x1 = x0 + β0r0, the assumption rank(R1) = 0 implies that
rank(r0) = rank(X1) = 0, i.e. r1 = rf0 := 0.
For S > 1, because ∆xs-1 = xs - xs-1 = -Xs-1Γs-1 + βs-1rfs-1, the rank deficiency of
Xs implies ∆xs-1 ∈ range(Xs-1), which further implies rfs-1 ∈ range(Xs-1). So there exists
ζ ∈ Rs-1, such that rfs-1 = Xs-1ζ. Note that according to (3), rfs-1 ⊥ Rs-1 = -AXs-1, so we
have
0 = rfsT-1AXs-1 = (Xs-1ζ)TAXs-1 =ζTXsT-1AXs-1.	(29)
Because rank(Xs-1) = S - 1 and A is SPD, we know XsT-1AXs-1 is also SPD. So ζ = 0, which
implies fs-ι = 0. Hence Xs = Xs-ι and r§ = fs-ι = 0.	□
Now we give the proof of Theorem 1.
Proof of Theorem 1. Besides relations (i)-(iii), we add an auxiliary relation here:
(iv) rk = r0 + QfkΓfk ∈ Kk+1(A, r0), where Γfk ∈ Rk.
We prove the relations (i)-(iv) by induction.
For k = 1, since rf0 6= 0, according to Proposition 2, rank(∆x0) = rank(X1) = 1, rank(∆r0) =
rank(Rι) = 1, so q = 0, which implies Line 7 in Algorithm 2 is well-defined. Therelation (i) holds.
Since q = q = ∆xo,p = p = ∆r0,qnd ∆r0 = -A∆x0, the equality Qι = -API also holds. Due
to the normalization in Line 7, Qf 1T Qf1 = 1. Since r1 = r0 - β0Ar0 and range(Qf1) = range(Ar0),
itis clear that r1 = r0 - Qf 1Γf1 ∈ K2(A, r0), namely relation (iv). Due to the projection step Line 11,
rf1 ⊥ range(Q1) = range(Qf1). Also, rf1 = r1 - Q1Γ1 = r0 - β0Ar0 - Q1Γ1 = r0 - Qf1η1, where
the last equality is due to span{Ar0} = range(Q1) = range(Qf 1). For r1G = r0 - Az1, where
z1 = arg minz∈K1(A,r0) kr0 - Azk2, it holds r1G ⊥ AK1(A, r0) = range(Qf 1). As a result, both rf1
and r1G are the orthogonal projections of r0 onto the subspace range(Qf1)⊥, which implies rf1 = r1G.
So xf1 = x1G = x0 + z1 because their residuals are equal and A is nonsingular. Hence relation (iii)
holds.
Suppose that k > 1, and as an inductive hypothesis, the relations (i)-(iv) hold for j = 1, . . . , k - 1.
Consider the k-th iteration. From Line 6 in Algorithm 2, q ∈ range(∆rk-ι,Qk-ι), and P ∈
range(∆xk-ι,Pk-ι). We first prove that q = 0 by contradiction.
If q = 0, then from Line 6 in Algorithm 2, ∆rk-ι ∈ range(Qk-I) ⊆ range(Qk-ι), which implies
∆xk-1 ∈ range(Pk-1) ⊆ range(Pfk-1) as ∆rk-1 = -A∆xk-1 and Qfk-1 = -APfk-1 and A is
nonsingular. From Line 11 and Line 12, we have
∆xk-1 = xk - xk-1 = -Pk-1Γk-1 + βk-1rfk-1 .	(30)
So rfk-1 ∈ range(Pk-1) ⊆ range(Pfk-1) since ∆xk-1 ∈ range(Pk-1). Hence there exists
ζ ∈ Rk-1, such that rfk-1 = Pfk-1ζ. From the inductive hypothesis, we know rfk-1 ⊥ Qfk-1 =
-APfk-1 , so we have
0 = rfkT-1APfk-1 = (Pfk-1ζ)TAPfk-1 = ζTPfkT-1APfk-1.
Since QfkT-1Qfk-1 = Ik-1, we know rank(Qfk-1) = k - 1, which implies rank(Pfk-1) = k - 1 due
to Qfk-1 = -APfk-1. Hence PfkT-1APfk-1 is also SPD. Then ζ = 0 which implies rfk-1 = 0. It is
impossible otherwise Algorithm 2 has terminated in the (k - 1)-th iteration. So q = 0 and Line 7 is
well-defined.
Since rfk-1 = rk-1 - Qk-1Γk-1, and rk-1 ∈ Kk(A, r0), range(Qk-1) ⊆ range(Qfk-1) =
AKk-1 (A, r0) as the inductive hypothesis, we have rfk-1 ∈ Kk (A, r0), which together with (30)
19
Published as a conference paper at ICLR 2022
and range(Pk-I) ⊆ range(B-I) = Kk-I(A,r0) infers ∆xk-1 ∈ Kk(A, r0). Hence ∆rk-1 =
-A∆xk-ι ∈ AKk(A, r0). As a result, qk = q/||引∣2 ∈ range(∆rk-ι,Qk-ι1 ⊆ AKk(A,r0).
So range(Qk) = range(Qk-ι,qk) ⊆ AKk(A,r0). Moreover, qk ∈ range(Qk-1), otherwise
q ∈ range(Qk-I) that implies ∆rk-ι = q ∈ range(Qk-ι), which is impossible following the
former proof of 7 = 0. So We have range(Qk) = AKk (A, r0).
Because ∆rk-ι = -A∆xk-ι and Qk-1 = -APk-I due to Qk-L = -APk-1, Line 6 in Algo-
rithm 2 infers q = -Ap, which implies qk = -APk. So Qk = -APk. Since A is nonsingular and
range(Qk) = AKk(A,r0), we have range(Pk) = Kk(A,r0).
As range(Xk) = Kk(A, r0) and range(Rk) = AKk(A, r0) has been proved in Proposition 1, the
relation (i) holds for the k-th iteration.
To prove QTQk = Ik, it suffices to show qk ⊥ Qk-ι, as the equalities QT_1Qk-ι = Ik-1 and
∣∣qk∣∣2 = 1 has already held. It is equivalent to prove q ⊥ Qk-ι. From the construction of q in
Line 6 in Algorithm 2, we know QT-i7 = 0, so q ⊥ span(qk-2,qk-1) (for k = 2, q ⊥ q0 = 0
clearly holds). To further prove q ⊥ range(Qk-3)(k ≥ 4), note that
∆rk-1 = -A∆χk-1 = APk-Irk-1 - βk-1A尸k-1 = -Qk-Irk-1 - βk-1A尸k-1,
where the second equality is a direct substitution with (30). Therefore,
QT-3^一 = -QT-3Qk-1Γk-1 - βk-1QT-3Afk-1 = 0 - βk-1(AQk-3)τrk-1 = 0,	(31)
where the second equality is due to range(Qk-1)_= span(qk-2, qk-1) ⊥ range(Qk-3) and A is
SPD, the third equality is due to 尸k-1 ⊥ range(Qk-I) = AKk-I(A,r0) and range(AQk-3)=
A2Kk-3(A, r0) ⊆ AKk-1 (A, r0). As a result, noting that q = ∆rk-1, we obtain
QT-3q = QT-3q - QT-3Qk-I(QT-1q) = 0,
which is due to (31) and range(Qk-I) = span{qk-2,qk-1} ⊥ range(Qk-3). Therefore, we show
that QTQ= Ik, which along with Qk = -APk proves relation (ii) in the k-th iteration.
Next, we prove the relation (iv). We have
rk =尸k-1 - βk-1A尸k-1
=rk-1 - Qk-Irk-1 - βk-1A(rk-1 - Qk-Irk-I)
=rk-1 - Qk-Irk-1 - βk-1Ark-I + βk-1 AQk-Irk-1
=r0 + Qk-Irk-1 - Qk-Irk-1 - ek-1 (Ar0 + AQk-Irk-I) + ek-1 AQk-Irk-1,
where the last equality is due to rk-1 = r0 + Qk-Irk-I by the inductive hypothesis. Since
range(Qk-I) = AKk-1 (A,r0) ⊆ AKk(A,r0), range(Qk-1) ⊆ range(Qk), span{Ar0} ⊆
AKk(A,r0), range(AQk-1) ⊆ range(AQk-I) ⊆ _ A2Kk-I(A,r°) ⊆ AKk(A,j0), and
range(Qk) = AKk(A,r0), it is clear that rk = r0 + Qkr ∈ Kk+1(A,r0) for some rk ∈ Rk.
The relation (iv) is proved.
Finally, we prove the relation (iii). For proving fk ⊥ range(Qk), note that fk ⊥ span(qk-1,qk}=
range(Qk) already holds due to the projection step (Line 10 and Line 11in Algorithm 2). It suffices
to prove fk ⊥ range(Qk-2). In fact, since we have fk = rk - Qk「k, we can prove that rk ⊥
range(Qk-2) and Qk『k ⊥ range(Qk-2):
Since range(Qk) = span{qk-1, qk} ⊥ range(Qk-2) as induced from QTQk = Ik, it is clear
that Qkrk ⊥ range(Qk-2). For rk, according to Line 12 in Algorithm 2, we have rk = fk-1 -
βk-1Afk-1. We have fk-1 ⊥ range(Qk-I) ⊇ range(Qk-2) by the inductive hypothesis. Also,
QT-2Afk-1 = (AQk-2)Tfk-I = 0 due to range(AQk-2) = A2Kk-2(A,r0) ⊆ AKk-I(A,r0)=
range(Qk-1).
Therefore, we obtain fk ⊥ range(Qk-2), which along with fk ⊥ span{qk-1,qk} implies fk ⊥
range(Qk).
To prove Xk = xG := x0 + zk, where Zk = argminz∈κfc(A；ro)∣∣r0 - Az∣2, first we have r =
r0 + Q k r k, where r k ∈ _R^. HenCe rk = rk - Q k r k = r0 + Q k r k - Q k r k = r0 - Q k nk, Where
nk ∈ Rk. Since fk ⊥ Qk, fk is the orthogonal projection of r° onto the subspace range(Qk)⊥.
On the other side, for GMRES, rG = r0 - Azk ⊥ AKk(A,r0) = range(Qk), so rG is also the
20
Published as a conference paper at ICLR 2022
orthogonal projection of ro onto the subspace range(Qk)⊥. So 尸k = r£, which further indicates
Xk = x£. Hence, the relation (iii) holds.
With relations (i)-(iv) being proved in the k-th iteration, We complete the induction.	□
C.2 Modified ST-AM for general fixed-point iterations
Algorithm 2 is suitable for analysis and implementation in the linear case. For general nonlinear
fixed-point iterations, we adopt an alternative form as described in Algorithm 3 which discards the
normalization of q in each iteration (Line 7 in Algorithm 2). In Line 7 in Algorithm 3, the orthogonal
projection of ∆rk-1 is checked to ensure ∆rk-1 is “less linearly dependent” on range(Qk), which
ensures kqk k2 is bounded away from zero; the check of kPk-1ζk k2 ensures that kpk - ∆xk-1 k2 ≤
cpk∆xk-1 k2, which is also important since a large deviation from ∆xk-1 can make kpk k2 > ρ
(ρ is the radius introduced in Theorem 2). When this condition cannot be satisfied, the algorithm
simply reuses the old Pk-1, Qk-1. The main procedure of MST-AM restarts every m iterations, i.e.
Pk , Qk = 0. Such restart mechanism is to restrict the higher-order terms in the residual expansion,
as shown in (9). Also, restart can flush out the outdated historical information that may weaken the
quality of Pk and Qk that are used to pursue a local first-order approximation of g in MST-AM.
Algorithm 3 MST-AM for nonlinear fixed-point problems
Input: x0 ∈ Rd, βk ∈ (0, 1], cp > 0, cq ∈ (0, 1), m > 0.
Output: x ∈ Rd
1	: P0, Q0 = 0 ∈Rd×2,p0,q0 =0 ∈ Rd
2	: for k = 0, 1, . . . , until convergence do
3	:	rk = g(Xk) - Xk
4	: if k mod m 6= 0 then
5	:	p = Xk - Xk-1, q = rk - rk-1
6	Zk = (QT-IQk-1)，QT-Iq
7	:	if ∣Pk-1ζk∣2 ≤ cp∣p∣2 and ∣Qk-1ζk∣2 ≤ cq∣q∣2 then
8	:	pk = p - Pk-1 ζk, qk = q - Qk-1ζk
9	:	Pk = [pk-1 , pk], Qk = [qk-1 , qk]	(qk ⊥ Qk-1)
10	:	else
11	:	Pk = Pk-1, Qk = Qk-1
12	:	end if
13	: else
14	:	Pk, Qk = 0 ∈ Rd×2, pk, qk = 0 ∈ Rd
15	: end if
16	:	Γk = (QkT Qk)， QkTrk
17	Xk = Xk — PkΓk,尸k = Irk — QkΓk	(Projection step:尸k ⊥ Qk)
18	xk+ι = Xk + βk尸k	(Mixing step)
19	: end for
20	: return Xk
	(Thenotation “『'is theMoore-Penrosepseudoinverse.)
In the linear case, Algorithm 2 and Algorithm 3 (with m = ∞) are equivalent. Similar to Algo-
rithm 2, we have the following properties held for MST-AM:
Claim 1. In the k-th iteration (k > 0) of Algorithm 1 applied to minimize a strongly convex
quadratic problem (5), assuming cp = ∞, cq = 1, m = ∞, the following relations hold:
(i) Ilqk∣∣2 > 0, range(Pk) = range(Xk) = Kk(A,ro), range(Qk) = range(Rk) = AKk(A,ro)；
(ii) Qk = -APk, qi ⊥ qj (1 ≤ i = j ≤ k);
(iii)尸k ⊥ range(Qk) and Xk = xo + zk, where Zk = arg minz∈Kk(A,r0)∣∣ro — Az∣2.
If ∣∣rk ∣∣2 =0, then Xk+1 is the exact solution.
The proof of Claim 1 is essentially the same as the proof of Theorem 1, with a special care that
QTQk = Ik is replaced by the relation that columns of Qk are orthogonal to each other, in other
words, QTQk= diag{∣∣qι∣∣2,..., ∣Iqk∣∣2}.
21
Published as a conference paper at ICLR 2022
For minimizing nonlinear functions or accelerating nonlinear fixed-point iterations, the long-term
relation that QTQk = diag{kqιk2,..., |屡∣∣2} generally cannot hold, while the orthogonalization
procedure in Line 6 still leads to a short-term orthogonality relation: qi ⊥ qj for |i - j | ≤ 2.
Hence, QTQk = diag{∣∣qk-ι∣∣2, ∣∣qk∣∣2} for k ≥ 1. Thus the pseudoinverse (QrTQk)t =
diag{I(∣qk-ι∣2 = 0)1∕∣qk-ιk2,I(∣qr∣2 = 0)1∕∣qkk2}, where I(∙) is the indicator function
that
I(x) =	10
x is true,
x is false.
Now, we give the proof of Theorem 2.
Proof of Theorem 2. For convenience, we restate the main assumptions of g here:
(i)	∣g(y) - g(x)∣2 ≤ κ∣y - x∣2, κ ∈ (0, 1), for ∀x, y ∈ B(ρ),	(32a)
(ii)	kg0(y) - g0(x)k2 ≤ ^ky 一 x∣2, K > 0, for ∀x,y ∈ B(ρ).	(32b)
Also, since |1 - βk | +κβk < 1, we know βk > 0 is bounded, i.e. βk ≤ β where β > 0 is a constant.
The proof is based on the two lemmas given in Lemma 1 and Lemma 2. Besides (9), we also prove
that ∣rk∣2 ≤ ∣r0∣2 by induction.
For k = 0, xo = xo ∈ B(ρ), and due to (48b), ∣E — x*∣∣2 ≤ ∣∣xo — x*∣∣2 + β0∣∣r0∣∣2 ≤ ∣∣xo —
x*∣2 + β(1 + κ)∣xo - x*∣2 ≤ Pprovided ∣∣xo — x*∣2 ≤ ρ∕(1 + β(1 + κ)). Since
r1 = g(x1) — x1 = g(x1) — (x0 + β0r0) = g(x1) — (x0 + r0) + (1 — β0)r0
= g(x1) — g(x0) + (1 — β0)r0,
it follows that
Ilr1∣∣2 ≤ ∣g(XI)- g(XO)∣2 + |1 -优|什0||2
≤ 左e0什0||2 + |1 - βoI∣∣r0∣∣2 = (κβo + |1 - β0∣)∣∣r0∣∣2.
Also note that θo =忻0||2/什0||2 = 1. Thus (9) holds. Because κβo + |1 - βo∣ ≤ κo < 1, we have
∣r1∣2 < ∣r0∣2.
Now, suppose that (9) and ∣rk∣2 ≤ ∣r0∣2 hold for k ≥ 0. We establish the results for k + 1.
Let Γk = (γk1),γk2))τ ∈ R2. Since Γ = (QTQk户Qrkrk, QTQk = diag{∣∣qk-ι∣∣2, ∣∣qr ∣∣2}, it
follows that
γk1) = I(qk-1 = 0) Tk qk-1 , γ(2) = Kqk = 0)rTqk.	(33)
qkT-1qk-1	qkTqk
Therefore,
∣γk1)I ≤I(qk-1 = 0), |1 - γk1)l ≤ max(I(qk-1 = 0),I(qk-1 = 0)⅛-jk-^l,
∣qk-1∣2	∣qk-1∣2
Iγk2)I≤I(qk = 0)性，∣i-γk2)l≤ max [z(qk = 0),工(qk = 0)ɪɪk^1 .	(34)
∣qk∣2	∣qk∣2
Define C = (1-1+1-Cq ) . We have
∣PkΓk∣2 = ∣pkγk(2) + pk-1γk(1-)1∣2 ≤ ∣pkγk(2)∣2 + ∣pk-1γk(1-)1∣2
≤I(qk=0)∣Pk∣2⅛ +1(qk-ι=0)∣Pk-ι∣2≤ 2c∣rk∣2,	(35)
∣qk ∣2	∣qk-1∣2
where the second inequality is due to (34) and the third inequality is due to (49). Then
I∣xk - x*∣∣2 = IIXk - Pkrk - x*∣∣2 ≤ ∣∣xk - x*∣∣2 + IIPkγk ∣∣2
≤ 1-K ∣rk ∣2 +2Ckrk 112 = (I-K + 2叫隆 ∣2,
22
Published as a conference paper at ICLR 2022
and due to ∣∣ffc∣∣2 ≤ krk ∣∣2,it holds that
∣∣χk+ι - χ*∣∣2 = Ilxk + βkrk - χ"∣∣2 ≤ Ilxk - χ*∣∣2 + βk∣∣r⅛∣∣2
≤ Ilxk - x*∣∣2 + β∣∣rk∣∣2 = Gj-—+ 2c + β)∣∣rk∣∣2.
1 — K
By the inductive hypothesis that ∣rk∣2 ≤ ∣∣r0∣∣2, and (48b), it has
IIxk - x*∣∣2 ≤ (γ-1-+ 2c)∣r0∣2 ≤ (7^--------+ 2c) (1 + κ)∣xo - x*∣∣2,	(36)
1 — K	11 — K )
and
∣∣xk+ι - x*l∣2 ≤ (1—K + 2c + β) l∣r0∣2 ≤ (1—K + 2c + β) (1 + K) Ilxo - x*l∣2.	(37)
As a result, We can choose ∣∣xo - x*∣∣2 sufficiently small to ensure xk ∈ B(P) and xk+1 ∈ B(ρ),
which ensure the g(xk) and g(xk+ι) are well defined.
At the end of the k-th iteration of Algorithm 3, we have
rk+1 = g(xk+ι) - xk+1
=g(xk+ι) - g(xk) + g(xk) - (xk + βkr)
=(g(xk+ι) - g(xk)) + (g(xk) - xk - r) + (1 - βk)rk.	(38)
Let Lk ：= g(xk+ι) - g(xk) + (1 一 βk)r的 Hk ：= g(xk) 一 xk 一 rk, then
IlLk ∣∣2 ≤ KIlxk+1 - xk ∣∣2 + ∣1 - βk IIlrk ∣∣2
=Kβk∣rk ∣∣2 + ∣1 - βk ||鼻 ∣∣2
=θk(Kβk + 11 - βk I)Ilrk ∣∣2,	(39)
which bounds the linear part of the residual rk+1.
For the higher-order terms Hk, we have
Hk = g(xk) - (xk - PkΓk + rk - Qkrk )
=g(xk) - g(xk ) + (Pk + Qk )rk.
=g(xk) - g(xk)+ (pk+ qk)y(2) + (Pk-I + qk-I)YkI).	(4O)
According to the formula f1 g0(x + t(y - x))(y - x)dt = g(y) - g(x), we have
∖	∖	(2)、	(2)、
g(xk) - g(xk) =	g(xk) - g(xk	- PkYk )	+ g(xk	-PkYk )	- g(xk)
1
=/ -g0(xk -Pkf - tPk-iYkI))Pk-IYkI)必
0
1
+ / -g'(xk - tPkYk2))PkYk2dt.
0
Also, by Lemma 2, we have
(41)
1	k-1
Pk + qk =	g0(xk - tPk)Pkdt + ^∣∣∆r∏(k)-i∣∣2 E O(∣∆r7-1∣2),
0	j=k-mk
1	k-2
Pk-i + qk-i = / g'(xk-i - tPk-i)Pk-idt + ^∣∣∆rυ(k)-i∣∣2	E O(∣∆rj∣∣2),
0	j = k-mk
where π(k) denotes that the latest update of qk by Line 8 occurred in the π(k)-th iteration and
υ(k) = π(π(k) -1) marks that qk-i records qv(k) that is the penultimate update by Line 8 occurring
in the υ(k)-th iteration.
23
Published as a conference paper at ICLR 2022
By substituting these relations to (40), it follows that
Hk
(Z g0(Xk-tPk)Pkdt + ^ern(k)-lI∣2 X	O(k∆rjk2) j γk2)
0	j=k-mk
/	/1	k-2	∖
+	I	I	g0(xk-i	-	tpk-i)pk-idt	+	^∣∣∆rυ(k)-i∣∣2	E O(k∆rjl∣2) j	γk1)
0	j=k-mk
11
-	g0(xk - pkγk(2) - tpk-1γk(1))pk-1γk(1)dt -	g0(xk - tpkγk(2))pkγk(2)dt
00
1
(g0(xk - tpk) - g0(xk - tpkγk(2)))pkγk(2)dt
0
1
+	(g0(xk-1 - tpk-1) - g0(xk - pkγk(2) - tpk-1γk(1)))pk-1γk(1)dt
0
k-1
+ ^k∆r∏(k)-i∣2	X	O(k∆rj ∣2)γk2)
j=k-mk
k-2
+ ^∣∆rυ(k)-i∣2 X O(k∆rj ∣2)γk1)
j=k-mk
= Ak + Bk + Ck + Dk .	(42)
Then we can bound each terms of Hk as follows (Here we assume qk 6= 0 and qk-1 6= 0 as qk = 0
leads to
0 and qk-1 = 0 leads to
1
0 where the result is trivial.):
IAkI2
(g0(xk - tpk) - g0(xk - tpkγk(2)))pkγk(2)dt
2
1
≤ Ig0(xk - tpk) - g0(xk - tpkγk(2))I2 Ipkγk(2) I2dt
0
1
≤ Kl ktPk - tPkγk2)k2kPkk2 lγk2)ldt
0
=2 l∣Pkk2l1 - γk2) llYk2) |
K	C
2kPkk2
O(IrkI2Irk - qkI2)
k
K X	O(krj k2),
j=k-mk
(43)
≡
where the third equality is due to (34), and the last equality is due to (49) and (50b);
1
IBkI2	(g0(xk-1 - tpk-1) - g0(xk - pkγk(2) - tpk-1γk(1)))pk-1γk(1)dt
2
1
≤ lg0(xk-1 - tPk-1) - g0(xk - Pkγk(2) - tPk-1γk(1))l2 lPk-1γk(1) l2dt
0
1
≤ K	∣∣∆xk-ι -Pkγk2) - tPk-1 YkI) + tPk-1∣∣2∣∣Pk-1∣∣2∣γkI)Idt
0
≤ K (ll4/k-illz + IlPkII2∣γk2)| + 1 l∣Pk-1∣∣2∣1 - γk1)“ l∣Pk-1∣∣2IYkI)I
≤ K fk∆χk-1k2 + kPk∣2眯 +1 kPk-1k2 krk,- qk-lk2) kPk-1k2∕⅛
IqkI2	2	Iqk-1I2	Iqk-1I2
=^∣rk∣2 (O(Il∆rk-i∣2) + O(∣rkk2) + 0(1” - qk-1 k2)
k
=K X O(∣rj k2),
j=k-mk
(44)
24
Published as a conference paper at ICLR 2022
where the last inequality is due to (34), and the second equality is due to (48a), (49), (50b);
k-1
kCkk2 = ^∣∣∆r∏(k)-i∣∣2 X O(k∆rj k2)γk2)
j=k-mk
≤ ^∣∣∆r∏(k)-i∣∣2
^k∆r∏(k)-1k2
≡ X O(Qjk2)
j=k-mk
产亍 X	O(k∆rjk2)
kqπ(k)k2 j=k-mk
k
K X O(krj k2),
j=k-mk
(45)
where the first inequality is from (34), and the second equality is due to (50b);
k-2
kDkk2 = ^k∆rυ(k)-ik2 X	O(k∆rjk2)Y(I)
j=k-mk
≤ 创4ru(k)-ig
krk k2
kqk-1k2
k-2
X O(k∆rjk2)
j=k-mk
创 AMk)Tk2 Wk
k-2
X O(k∆rjk2)
j=k-mk
k
K X	O(krj k2),
j=k-mk
(46)
where the first inequality is from (34), and the second equality is due to (50b). Then with the bounds
(43), (44), (45) and (46), we obtain
k
kHkk2 = K X O(krj k2).	(47)
j=k-mk
Combining (39) and (47) to (38), we obtain
k
krk+1k≤kLkk2 + kHkk2 ≤ θk(κβk + |1 - βk∣)krkk2 + K X O(krjk2),
j=k-mk
as desired. Since mk ≤ m, the higher-order terms are limited. Note that
κβk + |1 - βk | ≤ κ0 < 1
by assumption. Then, for ∣∣χo-χ*∣∣2 SUfficiently small, the residuals {rk } are Q-Iinearlyconvergent,
which infers ∣∣rk ∣∣2 ≤ ∣∣r0k2. Therefore, We complete the induction.	□
Remark 6. There may be some concern about whether K can be counteracted by the Constant hidden
in the Big-O notation. Infact, since K is the Lipschitz constant of g0 and the constants in O(∙) are
composed of κ, Cp, Cq, it follows that K is unrelated to O(∙). Hence a small K can lead to a small
uniform boundedness ofthe higher-order terms. In the extreme case where ^ = 0, e.g. g is a linear
map, the residual only consists of the first-order term Lk.
Lemma 1. Under the same assumptions of Theorem 2, for k ≥ 1, we have the following bounds:
(1 - κ)k∆xk-1k2 ≤ k∆rk-1k2 ≤ (1 + κ)k∆xk-1k2,	(48a)
(1 - κ)kxk - x*∣∣2 ≤ I∣rkk2 ≤ (1 + κ)kxk - x*∣∣2,	(48b)
If qk 6= 0, then
kp⅛ ≤	1 + Cp	(49)
kqkk2 -(1-K)(1-Cq)	( )
25
Published as a conference paper at ICLR 2022
If the condition in Line 7 is true, then
kpkk2 ≤ (1 + cp)k∆xk-1k2,
qk 6= 0, (1 - cq)k∆rk-1k ≤ kqkk2 ≤ k∆rk-1k2
(50a)
(50b)
Proof. From the assumption (32a) of g, we have
(1 - κ)k∆xk-1k2 ≤ kxk - xk-1 k2 - kg(xk) - g(xk-1)k2
≤ kg(xk) - g(xk-1) - (xk - xk-1)k2
= krk - rk-1 k2 = k∆rk-1 k2
≤ kg(xk) - g(xk-1)k2 + kxk - xk-1 k
≤ (1 + κ)kxk - xk-1k2,
(1 - κ)kxk - x"∣2 ≤ ∣∣Xk - x*∣∣2 - ∣∣g(xk) - g(x*)∣∣2
≤ ∣∣g(χk) - g(χ*) - (Xk - χ*)k2 = ∣∣rk∣∣2
≤ ∣∣g(χk) - g(χ*)∣2 + ∣∣χk - χ*∣∣2
≤ (1 + κ)∣Xk - X*∣∣2∙
If the condition in Line 7 is true, i.e., ∣Pk-1ζk∣2 ≤ cp∣∆xk-1∣2, ∣Qk-1ζk∣2 ≤ cq∣∆rk-1∣2, we
have
∣pk∣2 = ∣∆xk-1 -	Pk-1ζk ∣2 ≤ ∣∆xk-1∣2 + ∣Pk-1ζk∣2 ≤ (1 + cp)∣∆xk-1 ∣2,
∣qk ∣2 = ∣∆rk-1 -	Qk-1ζk∣2 ≥ ∣∆rk-1 ∣2 - ∣Qk-1ζk∣2 ≥ (1 - cq)∣∆rk-1 ∣2.
The inequality ∣qk∣2 ≤ ∣∆rk-1 ∣ is due to the fact that qk is the orthogonal projection of ∆rk-1
onto range(Qk-1)⊥. Also, qk 6= 0 must hold otherwise q = Qk-1ζk which violates the condition
∣Qk-1ζk∣2 ≤ cq∣∆rk-1∣2 as cq ∈ (0, 1).
If qk 6= 0, then qk must be updated by Line 8 in some previous iteration. We assume the latest
update by Line 8 occurred in the j-th iteration, i.e., qk = qj , pk = pj , hence
∣Pk ∣2 = ∣Pj ∣2 ≤ (I + Cp)心叼-112 ≤	1+ Cp
Ilqk l∣2 Ilqj ∣2 — (I-Cq )^rj-lg — (I-Cq)(I- K)
where the first inequality is due to (50a) and (50b) and the second inequality is due to (48a).	□
Lemma 2. Under the same assumptions of Theorem 2, in the k-th iteration (k ≥ 0) of the restarted
MST-AM (Algorithm 3), we have
1	k-1
Pk + qk =	g0(xk - tpk)pkdt + ^∣∣∆r∏(k)-i∣∣2	O(∣∆rj∣2),	(51)
0	j=k-mk
where mk = k mod m, ∆rk-mk -1 := rk-mk, π(k) denotes that the latest update of qk by Line 8
occurred in the π(k)-th iteration andπ(k) = k-mk if Line 8 is never executed up to the k-iteration.
Proof. We prove (51) by induction. Denote ζk = (ζk(1), ζk(2))T.
For k = 0, the relation trivially holds.
For k = 1, p1 = ∆x0 , q1 = ∆r0 . It follows that
p1 + q1
g(x1) - g(x0) =	g0(x1 - tp1)p1dt.
0
So (51) holds for k = 1.
Suppose that (51) holds for 0 ≤ j ≤ k - 1, where k ≥ 2. For k ≥ 2, if mk := k mod m = 0 or 1,
(51) holds because it is the same as the case of k = 0 or k = 1. Now consider the nontrivial cases.
26
Published as a conference paper at ICLR 2022
For mk = 0 and m = 1, if the condition in Line 7 in Algorithm 3 is true, then π(k) = k. With the
convention that ∆rk-mk-i = rk-mk, We have
Pk + qk = ʌʃk -1 - Pk-IZk + ʌrk -1 - Qk-1ζk
g(xk) — g(xk-ι) — (Pk-I + qk-iX(2) — (Pk-2 + *-2乂(I)
/ g(xk
Jo
—tʌXk-I)∆xk-∖dt
—/	g0(χk-ι
Jo
—/	g0(χk-2
Jo
k-2
―tpk-1)Pk-1Cf2dt + 同lʌrn(k-l)-lg	X	O(l4j l∣2)<k2)
j=k-1-mk-1
k-3
—tPk-2)pk-2ζ(I)dt + 同∣Arυ(k-1)-1l∣2	X o(llʌrj l∣2)ck1),
j=k-2-mfc-2
(52)
where υ(k — 1) = π(π(k — 1) — 1) denotes that qk-2 records qυ(k-1) that is the penultimate update
by Line 8 occurring in the υ(k — 1)-th iteration. Considering the terms in Pk + qk, we know
g 0(χk — dxk-1Dxk-1 — g (χk-1 — tPk-1)Pk-1Z(2) — g 0(χk-2 — tPk-2)Pk-2Z(I)
=g' (Xk — tʌχk-1)(ʌχk-1 — Pk-1Zk2) — Pk-2Z(I))
+ (g/ (Xk — dχk-1) — g' (χk-1 — tPk-1))Pk-1 Z(2)
+ (g /(Xk — tʌχk-1) — g (χk-2 — tPk-2))Pk-2Z(I).	(53)
For ζk = (QT-IQk-1)tQT-1Ark-1,because QT-IQk-1 = diag{∣∣qk-2∣∣2 Jqk-1∣∣2}, it follows
that
=Tiqk-2 = 0)
△rT-1qk-2
qTr-2qk-2
=Z(qk-1 = 0)
Arτ-1 qk-1
qTr-1qk-1
Therefore,
IZkI)l≤i(qk-2 = 0)『-112 ,底2)∣≤i(qk-1 =0)『-112.	(54)
lqk-2l2	lqk-1l2
Now, to bound the terms in (53), we have
∣∣(g /(χk — tʌχk-1) — g '(χk-1 — tPk-1))Pk-1Z(2)l2
≤ ^∣∣Χk — Χk-1 — t(ʌXk-1 — Pk-I)Il2||Pk-1Z(2)||2
≤ ^((1 — t)kʌXk-Ik2 + t|Pk-1k2)kPk-112|Z(2)|
≤ κ((1 —
t)kʌXk-Ik2 + t∣∣Pk-1k2)kPk-1k2I(qk-1 = 0)
kArk-1k2
Ilqk-1k2
k-1	k-1
≤ 创 Ark-1k2 X θ(kʌrj∣2) =创 Ark-1∣2 X θ(kʌrj∣∣2),
j=k-1-mk-ι	j = k-mfc
(55)
where the third inequality is due to (54), and the last inequality is due to (48a), (50a) and (49), where
the constants are absorbed into the big-O notation. Similarly, it follows that
Il (g ' (χk — dχk-1) — g' (χk-2 — tPk-2))Pk-2Z(I)∣2
≤ ^||ʌXk-1 +ʌXk-2 — dXk-1 + tPk-2∣2∣Pk-2Z(I) ∣2
≤ a((1 — "1八4-11∣2 + llʌ^-zg + t∣∣Pk-2∣∣2)∣∣Pk-2∣∣2lZkI)I
≤ ^((1 —
t)kʌxk-11∣2 + IiaXk-Zkz + t∣∣Pk-2k2)l∣Pk-2k2Z(qk-2 = 0)
3k-1k2
kqk-2∣∣2
k-1	k-1
≤ ^kArk-1k2 X	θ(kʌrj∣2) = ^kArk-1k2 X	θ(kʌrjk2).
j=k-2-mk-2	j=k-mk
(56)
27
Published as a conference paper at ICLR 2022
For the remaining terms in (52), we have
k-2	k-3
^k∆r∏(k-i)-ik2	X	O(k∆r7- k2)Z(2) + Kk∆rυ(k-i)-ik2	X O(k∆rj∣∣2)Z(1)
j=k-1-mk-1	k=k-2-mk-2
≤ 砧")」2 I (qk-ι=0) I
k-2
X
j=k-1-mk-1
O(k∆rjk2)
+ κk"(I)-1k2I(qk-2 =O) k⅛⅛
k-3
O(k∆rjk2)
k=k-2-mk-2
k-2
≤ Kk∆rk-1k2 X O(k∆j∣2),	(57)
k=k-mk
where the last inequality is due to (50b) and that qk-1 = qπ(k-1), qk-2 = qυ(k-1).
With relations (52), (53), (55), (56) and (57), and noting that pk = ∆xk-1 - pk-1ζk(2) - pk-2ζk(1),
and
1
(g0(xk - t∆xk-1)∆xk-1 - g0(xk-1 - tpk-1)pk-1ζk(2) - g0(xk-2 - tpk-2)pk-2ζk(1))dt
0
2
≤ Z	kg0(xk
0
- t∆xk-1)∆xk-1 - g0(xk-1 - tpk-1)pk-1ζk(2) - g0(xk-2 - tpk-2)pk-2ζk(1)k2dt,
we can estimate pk + qk as
1	k-1
Pk + qk = g g0(xk - t∆xk-i)pkdt + κk∆rk-ik2 E	O(k∆rjk2).
0
j=k-mk
(58)
To further obtain (51), notice that the difference
g0(xk - tpk)pkdt -	g0(xk - t∆xk-1)pkdt
1
(g0(xk - tpk) - g0(xk - t∆xk-1))pkdt
2
2
≤ kg0(xk - tpk) - g0(xk - t∆xk-1)k2kpkk2dt
0
1
≤ Kl tkPk-1 ζ(2) + Pk-2ζ(1)k2kPk k2dt
0
^ .. .	...... .. .	.. _.	...
≤ 2 Cpk∆Xk-1∣∣2∣∣Pk I∣2 = Kk∆rk-i∣∣2 O(k∆rk-ik2),
where the last inequality is due to the condition kPk-1ζkk2 ≤ cpkpk2 and inequalities (48a) and
(50a). Hence the difference can be absorbed in the Big-O notation in (58). Thus (51) holds when
the condition in Line 7 is true for the k-th iteration.
We consider the case where the condition in Line 7 is false for the k-th iteration. Then Pk =
Pk-1, Qk = Qk-1. In other words, the memory recording pk, qk keeps unchanged from the (π(k) +
1)-th to k-th iteration. Since π(k) < k and π(π(k)) = π(k), with the inductive hypothesis, we have
pk + qk = pπ(k) + qπ(k)
= g	(xπ(k)
0
g	(xπ(k)
0
π(k)-1
-tPπ(k))Pπ(k)dt + ^Crn(k)-1k2	E	O(k^rj k2)
j=π(k)-mπ(k)
k-1
-tPk)Pkdt + ^skʌrn(k)-l k	X	O(Brj k2).
j=k-mk
(59)
28
Published as a conference paper at ICLR 2022
To further obtain (51), note that the difference
g0 (xk - tpk)pkdt -	g0 (xπ(k) - tpk)pkdt
0	0
≤	kg0(xk - tpk) - g0 (xπ(k) - tpk)k2kpkk2dt
0
2
≤ ^kχk — χ∏(k)∣∣2∣∣Pk∣∣2
k-1	k-1
=K X	ICxj l∣2kPk k2 = ^Crn(k)-1k2 X OUCrj^
j=π(k)	j=π(k)
can be absorbed into the Big-O notation in (59). Thus (51) also holds when the condition in Line 7
is false for the k-th iteration.
As a result, we complete the induction in the k-th iteration.
□
C.3 Regularized short-term recurrence Anderson mixing
C.3.1 Check of positive definitenes s
We describe the check of positive definiteness in RST-AM, which follows the same procedure as
that of SAM (Wei et al., 2021). From Line 11-13 in Algorithm 1, one update of xk RST-AM is
xk+ι = xk + Hkrk, where Hk = Bk I 一 αkγk ZkQT, Yk = Pk + Bk Qk, Zk = QTQk + δkPT Pk.
Hk is generally not symmetric. For the convergence analysis of RST-AM, a critical condition is the
positive definiteness of Hk, i.e.
STHkSk ≥ Bkμ∣Sk k2, ∀Sk ∈ Rd,	(60)
where μ ∈ (0,1) is a constant. Next, we show how to guarantee it.
Let λmin(∙) denote the smallest eigenvalue, and λmaχ(∙) denote the largest eigenvalue. Since
STHkSk = 1 ST(Hk + HT)Sk, Condition (60) is equivalent to λmin (2 (Hk + HT)) ≥ Bkμ. By
some simple algebraic operations, We obtain λmin (2 (Hk + HT)) =Bk - 2 αk λmax(Yk ZkQT +
QkZlYT). Let λk := λmaχ(YkZkQT + QkZlYT), then Condition (60) is equivalent to
αkλk ≤ 2Bk(1 — μ),	(61)
namely, (13) in Remark 4. To check Condition (61), note that
Since (QT) (Yk Qk), (Zt θkJ ∈ R4×4, λk can be computed cheaply. This cost is negligible
compared with those to form PkTPk, QkTQk, which need O(d) flops. Then, to guarantee the positive
definiteness of Hk, we check whether αk satisfies (61) and use a smaller αk if necessary, e.g. αk =
2Bk (1 — μ)∕λk .
C.3.2 Proofs of the theorems
We first give the proof of the boundedness of lPk-1ζkl2 and lQk-1ζkl2.
Lemma 3. For P, Q ∈ Rd×m(d ≥ m), δ > 0, and Z = QTQ + δPTP, we have
kPZtQTk2 ≤ δ-2,	(63a)
lQZtQTl2≤ 1.	(63b)
Proof. We first consider the case that Z is nonsingular, then PZtQT = PZ-1QT, QZtQT =
QZ-1QT. It can be seen that PTP and QTQ are symmetric positive semidefinite, and Z is SPD as
it is assumed to be nonsingular. Also, we have δPTP Z and QTQ Z, where the notation “”
29
Published as a conference paper at ICLR 2022
denotes the Loewner partial order, i.e., A	B with A, B ∈ Rm×m means that B - A is positive
semidefinite. Hence, we have
Z- 1 δPTPZ-1 W I,Z-2QTQZ-2 W I,
which implies
kZ-1 (PTP) Z-1 k2 ≤ δ-1,
kZ-2 (QtQ) Z-1 k2 ≤ 1.
Let λmax(∙) denote the largest eigenvalue, We have
kQZ-IQTk2 = λmaχ (QZ-IQT) = λmaχ (QTQZ-1) = λmaχ (Z - 2 QTQZ- 1 ) ≤ 1,
kPZ-IQTk2 = λmaχ (PZ-1qtqz-1pt)
=λmaχ (PTPZ-1QTQZ-1)
=λmaχ (z-2 (PtP) Z-2 ∙ Z-2 (QtQ) Z-1)
≤ kZ-2 (PtP) Z-1 ∙ Z-1 (QtQ) Z-2 k2
≤ kZ-2 (PtP) Z-1 k2kZ-2 (QtQ) Z-1 k2 ≤ δ-1.
Therefore, kPZ-IQTk ≤ δ-2 and ∣∣QZ-IQTk2 ≤ 1.
For the case that Z is singular, it can be proved that
ker(Z) := {x ∈ Rm|Zx = 0} = {x ∈ Rm|Px = 0andQx = 0}.	(64)
In fact, if Px = 0 and Qx = 0, it is obvious that Zx = 0. On the other side, if Zx = 0, then
xTZx = 0, i.e. xTQTQx + δxTP TP x = 0. Since 0 W PTP, 0 W QTQ, δ > 0, it folloWs that
xTQTQx = 0 and δxTP TP x = 0, Which further implies Qx = 0 and Px = 0. Hence (64) holds.
Let U1 satisfy U1TU1 = I and range(U1) = ker(Z), i.e. the orthonormal basis of ker(Z), and U2
satisfy U2TU2 = I and U2TU1 = 0, i.e. the orthonormal basis of ker(Z)⊥. With the equality (64),
We knoW PU1 = 0, QU1 = 0. Define U = (U1, U2) ∈ Rm×m, then UTU = Im and by direct
computation, We have
UTZU =	00
U2T0ZU2	,
Where U2TZU2 = (QU2 )TQU2 + δ(P U2 )TPU2 is nonsingular according to the definition of U2 .
So
Zt = U (O (UTZ%)-1) UT.
As a result, we can further compute PZt Qt and QZtQt as
PZtQT = (PU2)(U2TZU2)-1(QU2)T, QZtQT = (QU2)(U2TZU2)-1(QU2)T.
EI t . τ∖	T-tT T z^ι	T	t Γ7	ATA , C AtA	t . ∙	.1	. Γ7 ∙	♦	1
Then let P = PU2 , Q = QU2 , and Z = QTQ + δPTP, and noting that Z is nonsingular, we can
obtain IlPZtQτ∣∣2 = ∣∣PZ-1Qτk2 ≤ δ-2, kQZtQτ∣∣2 = ∣∣QZTQTk2 ≤ 1.	□
As a result of Lemma 3, kPk-1ζkk2, kQk-1ζkk2 in Algorithm 1 are bounded by O(k∆rk-1k2), as
shown in the following corollary.
Corollary 2. kPk-1ζkk2, kQk-1ζk2 in Algorithm 1 are bounded, i.e.
kPk-1 Zkk2 ≤ (δk1))-2 k∆rk-1k2, kQk-1 Zkk2 ≤ k∆rk-1k2,	(65)
where ζk = (QkT-1Qk-1 + δk(1)Pk-1Pk-1)tQkT-1∆rk-1.
30
Published as a conference paper at ICLR 2022
Now, we turn to the proofs of the theorems about RST-AM in Section 3.4. For brevity, we use δk to
denote δk(2), i.e. δk ≡ δk(2). The proofs follow those of SAM (Wei et al., 2021). Nonetheless, since
RST-AM uses different historical sequences compared with SAM, we give the detailed proofs for
RST-AM for completeness.
From Assumption 2, for the mini-batch gradient fs% (Xk)=十 Pi∈isk fξi(Xk), where n = |Sk|,
the following properties hold:
E [VfSk (X)IXk ] = Vf (xk),	(66a)
2
E[kVfSk (Xk ) -Vf (Xk )k2|xk] ≤ -.	(66b)
k	nk
Consider the update of RST-AM. From Line 11-13 in Algorithm 1, it can be written as Xk+1 =
Xk + Hkrk, where rk = -VfSk (Xk), and for k ≥ 0,
Hk = βkI - αk (Pk + βkQk) (QTQk + δkPTPky QT.	(67)
To prove the theorems, the critical points are (i) the positive definiteness of the approximate Hessian
Hk and (ii) an adequate suppression of the noise from the gradient estimates in the stochastic case.
We first give a lemma related to the projection step.
Lemma 4. Suppose that {Xk} is generated by RST-AM. If αk ≥ 0, βk > 0, then for any vk ∈ Rd,
we have
kHkVkk2 ≤ 2 (β2 (1 + 2αk - 2αk) + αkδ-1) M(68)
Proof. The result holds when k = 0 as H0 = β0I. For k ≥ 1,
Hkvk = βkvk - (αk Pk + αkβk Qk )Γk ,	(69)
where Γk = (QTQk + δkPkTPkyQTvk solves
min kvk - QkΓkkk + δkkPkΓkkk.	(70)
By direct computation, kVk - QkΓk ∣∣2 + δk∣∣PkΓkkk = ∣∣vkk2 - vTQk(QTQk + δkPTPk)t ∙ QTvk,
thus
∣vk - QkΓk ∣kk + δk ∣PkΓk∣kk ≤ ∣vk∣kk.	(71)
Therefore,
∣Hkvk ∣kk
=	∣βk vk - (αk Pk + αk βk Qk )Γk ∣k
=	∣βk (vk - αk QkΓk ) - αkPkΓk ∣k
—11	k
=kβk(1 - αk)Vk + βkαk(vk - QkFk)- αkδk 2 δk Pk「k kk
≤ (βk(1 - αk)k + β2 αk + α2δ-1) ∙ (Ilvk kk + Ilvk- Qk rk k2 + δk IlPk rk Ilk)
≤ (βk (1 + 2αk - 2αk) + αkδ-1) (kvkkk + kvk kk)
=2 (βk (1 + 2αk - 2αk) + /δ-1) kvkkk.	(72)
In the above, the first inequality uses the inequality
n
k Xaixikkk ≤
i=1
where ai ∈ R, Xi ∈ Rd. The second inequality is based on inequality (71).	□
With Lemma 4, we can prove the deterministic case of RST-AM.
Xn |ai|kxikk!k ≤	Xn aik!Xn kxikkk!
(73)
31
Published as a conference paper at ICLR 2022
Proof of Theorem 3. Since 1 + 2α2k - 2αk ≤ 1 for 0 ≤ αk ≤ 1, and δk-1 ≤ C-1β2, with Lemma 4,
we have
kHkrkk22 ≤2β2(1+C-1)krkk22.
Since αk satisfies (61), we have
TiTHkTk ≥ βμkrkk2.
Then, under Assumption 1, we have
f (Xk+1) ≤ f (Xk ) + ▽/(Xk)T(Xk + 1 - Xk ) + 上 ∣∣xk+1 - xkk2
=f (Xk ) - rTHkrk + 2 IIHk rk ∣∣2
≤ f (Xk)-βμ∣Tk∣2 + Lβ2(1 + C -1)kτk k2
=f (Xk)-β (μ - βL(1 + CT)) ∣Tk k2
≤ f (Xk)-2 βμ ∙∣Vf (Xk )k2,	(74)
where the last inequality is due to 0 < β ≤ 2L(i+c-i). Thus, f(Xk+ι) — f(Xk) ≤
-1 βμkvf (Xk )∣2.
Summing both sides of this inequality for k ∈ {0, . . . , N - 1} and recalling f(X) > flow in
Assumption 1 gives
1	N-1
flow - f(X0) ≤ f(xn) - f(X0) ≤ -2βμ E ∣Vf(Xk)k2.
k=0
Rearranging and dividing further by N yields (15).	口
The next lemmas and proofs are about the stochastic case.
Lemma 5. Suppose that Assumption 2 holds for {Xk} generated by RST-AM. In addition, if βk > 0,
and αk ≥ 0 and satisfies (13), then
ESk [k Hkrk k2] ≤ 2 卜 2 (1 + 2αk - 2αk) + δk) ∙ (kVf(Xk) ∣∣2 + 晟)，	(75a)
Vf(Xk)τESk[HkTk] ≤ -1 βkμ∣Vf(Xk)k2 + 1 αk(δk； + βk)2 ∙ σ2,	(75b)
2	2	βk μ	n
where μ > 0 is the constant introduced in Remark 4.
Proof. (i) From Lemma 4, we have
ESk [kHkrk ∣∣2] ≤ 2 (β2 (1 +2α2 - 2αk) + ^k) ESk [Irk k2].	(76)
From Assumption 2, we have
ESk [∣Tkk2]= ESk [∣Tk - ESk [Tk]∣2] + IESk [Tk]∣2 ≤ IVf(Xk)k2 + σ2∕nk.	(77)
With (76), (77), we obtain (75a).
(ii) The result holds for k = 0 since H0 = β0I. Consider k ≥ 1. Define k = VfSk (Xk) -
Vf(Xk) = -Tk - Vf (Xk). Then HkTk = Hk (-k - Vf (Xk)) . Since αk satisfies (13), it has
λmin (1 (Hk + HT)) ≥ βkμ. Thus
Vf (Xk)THkVf (Xk) = 1 Vf(Xk)τ (Hk + Hi) Vf (Xk) ≥ βkμ∣Vf (Xk)∣2,
which implies
ESkVf(Xk)τHkVf(Xk)] ≥ βkμ∣Vf(Xk)∣∣2∙	(78)
32
Published as a conference paper at ICLR 2022
Let Mk = αk (Pk + βkQk) (QTQk + δkPTPky QT, then Hk = βkI - Mk. With the assumption
(66a), i.e. ESk [k] = 0, we have
EsJVf(xk)THkek] = EsJVf(xk)t ® e® - Mkek)]
=βkVf (xk)τEsJek] - EsJVf (XkVMkek] = -EsJVf(xkVMkek].
Using the Cauchy-Schwarz inequality with expectations, we obtain
∣Esk[Vf(xk)THkek]| = ∣EsJVf(xk)TMkek]| ≤ ,Es%[∣∣Vf(xk)k2],EsJIIMkekk2]
≤ kVf(xk)k2,Esk[kMkekk2].	(79)
We now bound ∣∣Mkek∣∣2∙ For brevity, let Zk = QTQk + δkP1TPk, and N = PkzkQT, N =
βk Qk ZkQTJhen
_ 1
kMk∣2 = ∣αk (Ni + N2) ∣∣2 ≤ αk (IIN1∣∣2 + ∣∣N2∣∣2) ≤ αk(δ-2 + βk),	(80)
where the last inequality is from Lemma 3.
With (80), we have ∣∣Mkek∣2 ≤ αk(δ-- + βk)|际12, which implies
2
ESk [kMk ek k2] ≤ α2 (δk 2 + βk )2ESk [kek I∣2] ≤ αk (δk 2 + βk )2 -,	(81)
nk
where the last inequality is due to (66b). Now we bound |ESk [Vf(xk)THkek]| as follows (cf. (79)):
|ESk [Vf(xk)THkek]|
≤ kVf(Xk)∣2√Esk[∣Mkek∣2]
≤ αk(δ-2 + βk)∣Vf(xk)k2,Esk[kek∣2]
,__1	. . σ ____. ...
≤ αk(δk	+ βk)	,——IlVf(Xk)∣2
√nk
=Pβkkμ ∣Vf (χk)∣2 ∙αk T βk) -‰
√βk μ	Jn
C _1	C
≤ 2 βk μ∣Vf(χk )k2 + 2 αk(δk2 + βk)
βk μ
2 σ2
------
nk
(82)
With the inequality (78) and (82), we obtain
Vf (Xk)T ESk [Hkrk]
= -Vf (Xk)T ESk [Hk (ek + Vf (Xk))]
= -ESk[Vf(Xk)THkVf(Xk)] -ESk[Vf(Xk)THkek]
≤ -ESk[Vf(Xk)THkVf(Xk)] + |ESk[Vf(Xk)THkek]|
≤ -ekM|lVf(xk)k2 + ^ekμkVf(xk)k2 + 5 k ji + 'O • 一
2	2 βk μ	n
=-1 βkμ∣Vf(Xk)∣2 + 1 αk(I + βk)2 •匕.	(83)
2	2	βkμ	n
□
By imposing one more restriction on αk, we can obtain a convenient corollary:
Corollary 3. Suppose that Assumption 2 holds for {Xk} generated by RST-AM. C > 0 is the
1
constant in (12). If βk > 0,0 ≤ αk ≤ min{1, βk } and satisfies (61), then
ESk[∣Hkrk∣2] ≤ 2β2 (1 + C-1) ∙ (∣∣Vf(xk)∣2 + σ2] ,	(84a)
nk
Vf(Xk)TESk[Hkrk] ≤ -WekM〔lVf(xk)k2 + β2 ∙ μ-1 (1 + CT) —.	(84b)
k	2	nk
33
Published as a conference paper at ICLR 2022
Proof. The first result (84a) is clear by considering (75a) and noticing that 1 + 2α2k - 2αk ≤ 1 when
αk ∈ [0,1] and δ-1 ≤ CTek. Since a® ≤ βk1 ,δk ≥ Cβ-2 and (1 + C-1 )2 ≤ 2(1 + CT) We
have
1 a (δ-2 + βk)2 σ2 ≤ 1 βk (C-1 βk + βk)2 σ2
2	βkμ	nk 一 2	βk μ	nk
=∣μ-1(C-1 + I)2β2 ∙--
2	nk
2
≤ β2μ-1(1 + CT) 一 ∙
nk
Substituting it into (75b), we obtain (84b).	口
With Corollary 3, We establish the descent property of RST-AM:
Lemma 6. Suppose that Assumptions 1 and 2 hold for {xk} generated by RST-AM. C > 0 is the
1
constant in (12). If 0 < βk ≤ 4l(i/c-i), 0 ≤ ak ≤ min{1, βj2 } and satisfies (61), then
ESk [f (Xk+1)] ≤ f (Xk)- IekμINf (Xk)Il2 + Iek ((L + μT)(I + CT)) —∙	(85)
4	nk
Proof. According to Assumption 1, we have
f (xk+l) ≤ f (xk ) + ▽/(Xk)T(Xk + 1 - xk ) + 2 ∣∣xk⅛1 - xk∣∣2
=f (Xk) + Vf(Xk)THk Tk + L IlHk rk∣2.	(86)
Taking expectation with respect to the mini-batch Sk on both sides of (86) and using Corollary 3 we
obtain
ESk[f(Xk+1)]
≤ f (Xk)+ Vf(Xk)τESk[HkTk] + LESk IIHkrk∣2
≤ f(Xk)- 5Bkμ∣vf(Xk)II2 + βkμ-1(1 + CT)	+ Lek(I + CT) (IlVf(Xk)∣2 H	)
2	nk	nk
=f (Xk)- βk (μ- - BkL(I + CT))IlVf(Xk)∣∣2 + β2(L + μ-1)(1 + CT) — .	(87)
2	nk
Then (87) combined with the assumption βk ≤d二个。-i)implies (85).	口
Lemma 6 suggests that the term related to the noise from gradient estimates is bounded as a second-
order term (i.e. O(ek2)). Thus, with the diminishing stepsize, the effect of noise also diminishes.
To establish the global convergence, we introduce the definition of a supermartingale following the
proofs in (Wang et al., 2017; Wei et al., 2021).
Definition 1. Let {Fk} be an increasing sequence of σ-algebras. If {Xk} is a stochastic process
satisfying (i) E[|Xk|] < ∞, (ii) Xk ∈ Fk for all k, and (iii) E[Xk+1 |Fk] ≤ Xk for all k, then {Xk}
is called a supermartingale.
Proposition 3 (Supermartingale convergence theorem, see, e.g., Theorem 4.2.12 in (Durrett, 2019)).
If {Xk} is a nonnegative supermartingale, then limk→∞ Xk → X almost surely and E[X] ≤
E[X0].
Now, we prove the convergence theory of RST-AM in the nonconvex stochastic case.
ProofofTheorem 4.⑴ Define φk ：= β4μIlVf(Xk)∣∣2 and L = (L + μ-1)(1 + C-1), Yk =
f (Xk) + Lσ2 P∞=k β2. Let Fk be the σ-algebra measuring φk,Yk, and xk. From (85) we know that
34
Published as a conference paper at ICLR 2022
for any k,
2∞
E[Yk+i|Fk] = E[f(Xk+I)IFk] + L— X β2
n
i=k+1
1	σ2 ∞
≤ f (Xk)- 4 Bk μkVf (xk)k2 + L — ^X β2 = Yk- φk,	(88)
i=k
which implies that E[γk+1 - flowIFk] ≤ γk - flow - φk. Since φk ≥ 0, we have 0 ≤ E[γk -
flow] ≤ γ0 - flow < +∞. As the diminishing condition (14) holds, we obtain E[f (Xk)] ≤ Mf for
some constant Mf > 0. According to Definition 1, {γk - flow } is a supermartingale. Therefore,
Proposition 3 indicates that there exists a γ such that limk→∞ γk = γ with probability 1, and
E[γ] ≤ E[γ0]. Note that from (88) we have E[φk] ≤ E[γk] - E[γk+1]. Thus,
∞∞
E X φk ≤ X(E[γk] - E[γk+1]) < +∞,
which further yields that
∞∞
X φk = 4 X BkkVf (Xk)k2 < +∞ with probability 1.	(89)
Since Pk∞=0 βk = +∞, it follows that (16) holds.
(ii) If the noisy gradient is bounded, i.e.,
Eξk[kVfξk(Xk)k22] ≤Mg,	(90)
where Mg > 0 is a constant, then a stronger result can be obtained.
For any give > 0, according to (16), there exist infinitely many iterates Xk such that kVf (Xk)k2 ≤
. Then if (17) does not hold, there must exist two infinite sequences of indices {si }, {ti } with
ti > si, such that for i = 0, 1, . . ., k = si + 1, . . . , ti - 1,
kVf(Xsi)k2 ≥2,kVf(Xti)k2 <,kVf(Xk)k2 ≥.	(91)
Then from (89) it follows that
∞	+∞ ti-1	+∞ ti-1
+∞ > XBkkVf(Xk)k22 ≥XXBk kVf (Xk)k22 ≥ 2 X X Bk with probability 1,
k=0	i=0 k=si	i=0 k=si
which implies that
ti -1
Bk → 0 with probability 1, as i → +∞.	(92)
k=si
According to (77) and (72), we have
E[kXk+1 - Xkk2IXk]
= E[kHkrkk2IXk]
≤ 个2 Cβk (I + 202 — 2。&) + αkδ-1)E[krkk2|xk]
≤ βkP2(1 + CT)E[krkk2∣xk]
≤ βkP2(1 + C-1)(E[krkk2∣xk])1
≤ βkP2(1 + CT)Mg,	(93)
where the last inequalities are due to Cauchy-Schwarz inequality and (90). Then it follows from (93)
that
1 ti -l
E[kxti — xsik2] ≤ P2(1 + CT)Mg E Bk,
k=si
which together with (92) implies that kXti - Xsi k2 → 0 with probability 1, as i → +∞. Hence,
from the Lipschitz continuity of Vf, it follows that kVf (Xti) - Vf (Xsi)k2 → 0 with probability
1 as i → +∞. However, this contradicts (91). Therefore, the assumption that (17) does not hold is
not true.	□
35
Published as a conference paper at ICLR 2022
Proof of Theorem 5. According to (87) in Lemma 6, we have
N-1	1
∑βk 2 μ — βk L(1 + C T) EkVf(Xk )k2
k=0
N-1	2
≤ f (XO)- flow + ^X βk(L + MT)(I + CT)一,
k=0	nk
(94)
where the expectation is taken with respect to {Sj}jN=-01. Define
PR(k) d=ef P rob{R = k}
Bk (1 μ - BkL(I + C-I))
PN=01 βj (2μ -BjL(I + CT))
k = 0, . . . , N -1,
(95)
then
E kVf(XR)k22
PN-I Bk (1 μ - BkL(1 + CT)) E [kVf(Xk)k2]
PN=O1 Bj (2μ -BjL(I + CT))
Df + σ2(L + 〃-1)(1 + CT) PNo1 B2∕nk
PN=OI Bj(Iμ -BjL(I + CT))
(96)
▼ 一 G-	-	.F	一 —	.	C	C	Γ	，，	K、	F
Let D be a problem-independent constant. If We choose Bk = B := mm{4工./。7), ^√^}, and
nk = n, then the definition ofPR simplifies to PR(k) =1∕N. From (96) we have
E[kVf(XR)k22] ≤
Df+ σ2(L + μ-1)(1 + CT) ∙字
NB ∙ 44
4Df	σ2(L + μ 1 )(1 + C 1) ∙ B
--- +----------；-------------
N Bμ	4nμ
≤
4Df	4 4L(1 + CT) σ√Nl	4σ2(L + μ-1)(1 + CT)
Nμ I μ ， JJ f	nμ
〜
D
σ√N
V 4Df 44L(1 + CT) σ√N∖	4σ(L + μ-1)(1 + CT)D
一nμ (M	D)	nμ√N
_ 16Df L(1 + C T)	σ 4 4Df 4(L + μ-1)(1 + C T)D
= Nμ + 在(Ir +	n
Therefore, to ensure E[kVf(XR)k22] ≤ , the number of iterations is O(1∕2).
□
D Experimental details
Our main codes Were Written based on the PyTorch frameWork 1 and one GeForce RTX 2080 Ti
GPU Was used for the tests in training neural netWorks. Our methods are ST-AM (MST-AM) and
the regularized version RST-AM.
D.1 Experimental details about ST-AM/MST-AM
The experiments about ST-AM Were conducted to verify the main theorems, i.e. Theorem 1, Corol-
lary 1 and Theorem 2. Four types of problems Were used for the experiments. The conjugate residual
(CR) method is a short-term recurrence version of the full-memory GMRES and needs matrix-vector
products to fulfill the algorithm. We give the pseudocode of the CR method (Algorithm 6.20 in
(Saad, 2003)) here for readers Who are not familiar With this numerical algorithm.
1 Information about this frameWork can be found in https://pytorch.org.
36
Published as a conference paper at ICLR 2022
Algorithm 4 CR Algorithm for solving a SPD linear system Ax = b.
Input: x0 ∈ Rd .
Output: x ∈ Rd
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
r0 = b - Ax0 , p0 = r0
for k = 0, 1, . . . , until convergence, do
αk = rTArk
ak = (Apk )TApk
xk+1 = xk + αkpk
rk+1 = rk - αkApk
R — rT+1 Ark + 1
βk = rTArk
pk+1 = rk+1 + βkpk
Apk+1 = Ark+1 + βkApk
end for
return xk
According to Theorem 1, the CR method is essentially equivalent to ST-AM for solving strongly
convex quadratic optimization. However, it should be pointed out that the CR method needs to
directly assess the matrix A, and the residual update (Line 5) is based on the linear assumption,
which makes it inapplicable for nonlinear problems or the case that A is unavailable. Also, even
though finite difference technique can be used to construct the matrix-vector products, the number
of gradient evaluations is twice of that of ST-AM.
D.1.1 Strongly convex quadratic optimization
To construct a case of (5), we considered the least squares problem:
min f(x) ：= 1 l∣Ax — bk2,	(97)
x∈Rd	2
where A ∈ R'×d,b ∈ R', which can be reformulated as a form of (5).
We generated a dense random matrix A ∈ R500×100 and a dense random vector b ∈ R500 for the
test. The gradient descent used a fixed stepsize η ≤ kAF that can guarantee convergence, and the
same η was used as the βk for the full-memory AM and ST-AM.
We also conducted additional tests about solving the problem (97) with different condition numbers,
where the eigenvalues of A were set to be uniformly distributed. The results with different condition
numbers (cond(ATA) = 102 , 104, 106) are shown in Figure 4. It can be observed that the curves
of CR, AM and ST-AM nearly coincide, which verify the correctness of Theorem 1. Also, since
the eigenvalues are uniformly distributed, the superlinear convergence behaviour may not happen.
Nonetheless, CR, AM and ST-AM are still much faster then the GD method.
Figure 4: Solving (97) with different condition numbers: Cond(ATA) = λmaχ(AτA)∕λmin(AτA).
37
Published as a conference paper at ICLR 2022
D.1.2 Solving a nonsymmetric linear system
FOr the sOlutiOn Of a nOnsymmetriC linear system
Ax = b,	(98)
where A ∈ Rd×d, b ∈ Rd, the fixed-pOint iteratiOn (FP) is xk+1 = g(xk) := xk + η(b - Axk).
TheOrem 2 requires g tO be a COntraCtive map, i.e. II - ηAI2 < 1. We used a test matrix “fidap029”
frOm the Matrix Market 2. This matrix is a banded matrix and nOt symmetriC. We used JaCObi
preCOnditiOner fOr all the tested iterative methOds. SinCe sOlving linear systems is nOt the main
fOCus Of this wOrk, we did nOt dO a thOrOugh test Of applying ST-AM tO sOlve variOus nOnsymmetriC
linear systems.
D.1.3 Cubic-Regularized quadratic minimization
The COnCerned CubiC-regularized quadratiC minimizatiOn is
min f (X) := IkAx - bk2 + M Ilxk2,	(99)
x∈Rd	2	3
where A ∈ R'×d,b ∈ R', and M ≥ 0 is the regularization parameter. It can be computed that
Vf (x) = AT(Ax — b) + M kx∣∣2x = (ATA + M ∣∣x∣21 )x — AT b.
Hence, for the gradient descent xk+1 = g(xk) := xk - ηVf (xk), the Jacobian of g is I 一 nATA 一
ηM (IxI2-1xxT + IxI2 I), which hasK > 0 in the local region B(P).
We generated a dense random matrix A ∈ R500×100 and a dense vector b ∈ R500 for the test. The
official implementation of L-BFGS in PyTorch was used for comparison. The historical length m of
L-BFGS was 50, i.e., BFGS was actually used in the test.
10	20	30	40	50
iteration
(a) MST-AM
10β
10-9
ι≤ IOYi
e
≥ IOf
W
=Kr吐
IerIi
10^u∙
10	20	30	40	50
iteration
(b) ST-AM
——AM: M=100
——AM: M=I
...AM: M=OΛ>1
——STAM: M=100
——STAM: M=I
...STAM: M=0∙01
Iteration
(C) MST-AM
40	50
io
M-I
M-0.01
M-IOO
M-I
M-0.01
10	20	30	40	50
Iteration
(d) ST-AM

Figure 5: Solving (99) with different M. (a) krkk2/kr0k2 of MST-AM; (b) krkk2/kr0k2 of ST-AM;
(C) IlPk-ιZk∣∣2∕∣∣∆χk-ι∣∣2 and kQk-iZk∣∣2∕∣∣∆rk-ι∣∣2 ofMST-AM;(d) ∣∣Pk-ιZk∣∣2∕∣∣∆χk-ι∣∣2
and IlQk-ιZk∣∣2∕∣∣∆rk-ι∣∣2 OfST-AM.
We also ConduCted the tests related to different regularization parameters M, and the Cases that
M = 0.01, 1, 100 are shOwn in Figure 5. Figure 5 shOws that with the large M = 100, bOth AM and
MST-AM COnverge faster. An ablatiOn study was alsO COnduCted abOut the bOundedness restriCtiOn
Of IPk-1ζk I2, IQk-1ζk I2 in TheOrem 2. In Figure 5(b), we shOw the COnvergenCe behaviOur Of
ST-AM withOut the bOundedness CheCk. In the Case Of the rather large regularizatiOn M = 100, ST-
AM dOes nOt shOw a mOnOtOne deCrease Of the residual. TO further investigate the Cause, we plOt the
magnitude Of IPk-1ζkI2∕I∆xk-1I2, IQk-1ζkI2∕I∆rk-1I2 in Figure5(C) and Figure 5(d). It Can
be Observed that the evOlutiOns Of IPk-1ζkI2∕I∆xk-1I2, IQk-1ζkI2∕I∆rk-1I2 are quite OsCilla-
tOry in ST-AM, while being rOughly bOunded belOw 1 in MST-AM. This phenOmenOn may aCCOunts
fOr the mOre stable COnvergenCe behaviOur Of MST-AM and verifies the neCessity Of the Changes Of
MST-AM COmpared with ST-AM. In Our experiments, we alsO fOund the restarting periOd Can be set
quite large and has little effeCt On ST-AM.
2 https://math.nist.gOv/MatrixMarket/.
38
Published as a conference paper at ICLR 2022
D.1.4 Root-finding problems in the multiscale deep equilibrium model
The multiscale deep equilibrium (MDEQ) model is a recent extension of the deep equilibrium (DEQ)
model (Bai et al., 2019) for computer vision. One of the central engines for these DEQ models is
the root-finding problem:
fθ (z; x) = gθ (z; x) — Z ⇒ find z* s.t. fθ (z*; x) = 0,	(100)
where θ and x are parameters and the input representation, respectively. In (Bai et al., 2020), the
Broyden’s method is employed to solve this problem. Since MST-AM is suitable for solving non-
linear systems of equations, we can apply MST-AM to solve (100).
We implemented the MST-AM method and integrated it into the MDEQ framework 3 . The task was
image classification on CIFAR-10 and we used the small model for test. We followed the suggested
experimental setting of the framework. The optimizer was Adam with learning rate of 0.001 and the
weight-decay was 2.5 × 10-6. The batch size was 128 and the number of epochs was 50. The tested
fixed-point solver was used for the forward root-finding process and for the backward root-finding
process. The threshold of the steps for the forward process was 18 and the threshold of the steps for
the backward process was 20.
We used the built-in AM method and Broyden’s method as the baseline methods. The m for AM
was 20, i.e. using the full-memory AM.
D.2 Experimental details about RST-AM
Our experiments on RST-AM focused on training neural networks. Since ST-AM can be regarded
as a special case of RST-AM with δk(1) = δk(2) = 0, the basic ST-AM is also covered. In the
Line 10 in Algorithm 1, αk should be adjusted to meet the positive definiteness check (13), which
can be simplified to (∆xk)Trk ≥ βkμ∣∣rk∣∣2 in practice. The adjustment of αk can be (i) ɑk =
min{ak, 2βk(1 — μ)∕λk}, or (ii) ɑk = 0. We used the option (ii) since the violation of (13) seldom
happened in our tests and option (ii) is more simple to apply.
In the experiments on MNIST, Penn Treebank, we incorporated preconditioning (described in Ap-
pendix A.3) into the baseline method SAM and our method. Preconditioning is found to be effective
for difficult problems, e.g. mini-batch training with very small batch sizes and the scaling of the
model’s parameters being important.
D.2. 1 Hyperparameter setting of RST-AM
The hyperparameters of RST-AM are easy to tune. The only hyperparameters that need to be care-
fully tuned are the regularization parameters c1 , c2 in δk(1) and δk(2) . We found RST-AM is more
sensitive to c1, possibly due to the fact that it influences the construction of secant equations. c2 can
be quite small in our
so as to ensure δk(2)
tests. The hyperparameter C in δk(2) was set to be very small (C = 1 × 10-16)
c2krkk2
kpk k2 + eo
almost all the time. 0 is introduced to prevent the denominators
from being zero. We found ∣pk ∣2 > 0 and ∣∆xk-1 ∣2 > 0 always held in the tests, so 0 can be
omitted. In the experiments except for deterministic optimization on MNIST, we kept the setting
c1 = 1, c2 = 1 × 10-7 unchanged and found such setting is quite robust.
The other hyperparameters are αk, βk, which can be always initially set as 1. So RST-AM has the
same number of hyperparameters to tune as SGDM, since SGDM needs to tune learning rate and
the momentum.
D.2.2 Experiments on MNIST
The experiments on MNIST 4 aimed to validate the effectiveness of RST-AM in deterministic opti-
mization, so we were only concerned about the training loss by regarding it as a nonlinear function
to be optimized. To facilitate the full-batch training, we used a subset of the training dataset by
randomly selecting 10k images from the total 60k images.
3 https://github.com/locuslab/mdeq.
4Based on the official PyTorch implementation https://github.com/pytorch/examples/blob/master/mnist.
39
Published as a conference paper at ICLR 2022
The baselines were SGDM, Adam, Adagrad, RMSprop and SAM. We tried our best to ensure that
the baselines had the best performance in the tests. We tuned the learning rates by log-scale grid-
searches from 10-3 to 100. For SGDM, Adam, Adagrad, and RMSprop, the learning rates were
0.2, 0.001, 0.01, 0.001, respectively. For SAM, we used the hyperparameter setting recommended
in (Wei et al., 2021).
For RST-AM, we set c1 = 0.05, c2 = 1 × 10-7, αk = βk = 1. When (∆xk)Trk ≤ 0 occurs,
xk+1 = xk + 0.2rk was used as the new update.
For the preconditioned RST-AM, we set c1 = 1 × 10-2, c2 = 1 × 10-7 for the Adagrad-
preconditioned RST-AM and c1 = 1 × 10-2, c2 = 1 × 10-8 for the RMSprop-preconditioned
RST-AM.
For all these tests, we trained the model for 200 epochs.
(a) Train loss
(b) Squared norm of gradient
Figure 6: Training on MNIST with the Adam/RMSprop preconditioner.
In Figure 2 in the main paper, we report the RMSprop/Adagrad preconditioned SAM/RST-AM. We
give the result about using Adam as the preconditioner in Figure 6. It suggests that Adam does not
perform as well as the RMSprop method to serve as a preconditioner for SAM/RST-AM in this task.
Nevertheless, Adam_RST-AM is still better than Adam and Adam_SAM(2), which demonstrates the
effect of our proposed short-term recurrence scheme.
D.2.3 Experiments on CIFAR
This group of experiments were the same as those in (Wei et al., 2021) so that we can have a direct
comparison between RST-AM and SAM. We still give the details here for completeness.
We followed the same way of training ResNet (He et al., 2016). The batchsize was set to be 128.
For N iterations of training, the learning rate of each optimizer was decayed atthe (［与 C)-th and the
(b3NC)-th iterations. Here, for SAM and RST-AM, the αk and βk serve as the learning rates, so
the learning rate decay denotes decaying αk , βk simultaneously. The experiments were run with 3
random seeds.
The baseline optimizers were SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang
et al., 2020), Lookahead (Zhang et al., 2019) and AdaHessian (Yao et al., 2021). Adam and the
recently proposed optimizers AdaBound and AdaBelief are adaptive learning rate methods which
use different learning rates for different model parameters. Lookahead is a k-step method and has
an inner optimizer. In each cycle of Lookahead, the inner-optimizer optim is used to iterate for k
steps and then the first iterate and the last iterate are interpolated to obtain the starting point of the
next cycle. Compared to Adam, AdaHessian uses Hessian-vector products to construct a diagonal
approximation of the Hessian.
For fair comparison, the hyperparameters of all the optimizers (including RST-AM) were tuned
through experiments on CIFAR-10/ResNet20. For each optimizer, the hyperparameter setting that
attained the highest final test accuracy on CIFAR-10/ResNet20 was kept unchanged for training
the other networks on CIFAR. We found the results of hyperparameter tuning were consistent with
those reported in (Wei et al., 2021). For completeness, we list the settings of hyperparameters here
(learning rate is abbreviated as lr, and “*” indicates the same setting as that in (Wei et al., 2021)).
• SGDM*: lr = 0.1, momentum = 0.9, weight-decay = 5 X 10-4, lr-decay = 0.1.
40
Published as a conference paper at ICLR 2022
5□
a□
7 Q
60
0	20	40	60	80	100	120	140	160
epochs
80
70
60
洪
δ,50
ro
8 40
<
I 30
20
10
O
O 20	40	60	80 IOO 120	140	160
(a) CIFAR-10/ResNet18
80
70
求6。
›
υ
ro 50
n
u
u
< 40
⅛
3。
20
10
(b) CIFAR-10/WRN16-4
O 20	40	60	80 IOO 120	140	160
epochs
(d) CIFAR-100/DenseNet121
epochs
(c) CIFAR-100/ResNeXt50
Figure 7: Test accuracy of training ResNet18 and WideResNet16-4 on CIFAR-10 and training
ResNeXt50 and DenseNet121 on CIFAR-100.
•	Adam*: lr = 0.001, (β1,β2) = (0.9,0.999), Weight-decay = 5 X 10-4,lr-decay = 0.1.
•	AdaBound: lr = 0.001, (β1,β2) = (0.9,0.999), final_lr = 0.1, gamma = 0.001, weight-
decay = 5 × 10-4, lr-decay = 0.1.
•	AdaBelief*: lr = 0.001, (β1, β2) = (0.9, 0.999), eps = 1 × 10-8, weight-decay = 5 × 10-4,
lr-decay = 0.1.
•	Lookahead*: optim: SGDM (lr = 0.1, momentum = 0.9, weight-decay = 1 × 10-3),
α = 0.8, steps = 10, lr-decay = 0.1.
•	AdaHessian*: lr = 0.15, (β1, β2) = (0.9, 0.999), eps=1 × 10-4, hessian-power: 1, weight-
decay = 5 × 10-4/0.15, lr-decay = 0.1.
•	SAM(2): optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 × 10-3), αk =
1.0, βk = 1.0, c1 = 0.01, p = 1, m = 2, weight-decay = 1.5 × 10-3, lr-decay = 0.06.
•	SAM(10)*: optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 × 10-3), αk =
1.0, βk = 1.0, c1 = 0.01, p = 1, m = 10, weight-decay = 1.5 × 10-3, lr-decay = 0.06.
•	RST-AM: c1 = 1, c2 = 1 × 10-7, α0 = β0 = 1, weight-decay = 1 × 10-3, lr-decay = 0.1.
For the tests of SGDM, Adam, AdaBelief, Lookahead, AdaHessian and SAM(10), we also had
consistent numerical results with those reported in (Wei et al., 2021), so we reported their results of
these methods in the main paper for reference.
Figure 7 shows the curves of test accuracy for training ResNet18 and WRN16-4 on CIFAR-10 and
training ResNeXt50 and DenseNet121 on CIFAR-100. The numerical results of final test accuracy
can be found in Table 1(a) in the main paper. It can be found in Figure 7 that the convergence
behaviour of RST-AM is less erratic than SAM(10) and SAM(2). In the first 80 epochs, RST-AM
41
Published as a conference paper at ICLR 2022
converges faster than SAM, partly due to using a smaller weight decay. The learning rate schedule
has a large impact on the convergence of each optimizer. Similar to SAM(10), RST-AM can always
climb up and stabilize to a higher test accuracy in the final 40 epochs. It is found that RST-AM is
comparable to SAM(10), while improving the short-memory SAM(2).
°10。 OOoo
0 9 8 7 6 5 4
% ʌɔe-lnuɔv U-E-IJ.
0	20	40	60	80	100 120 140 160
epochs
(a) Train accuracy
Figure 8: Train accuracy and test accuracy of RST-AM with different c2 .
0	20	40	60	80 100 120 140 160
epochs
(b) Test accuracy
SGDM
RSTAM: Cl=IO
RSTAM: Cl=I
RSTAM: Cl=0.1
RSTAM: Cl=0.01
0	20	40	60	80	100 120 140 160
epochs
(b) Test accuracy
Figure 9:	Train accuracy and test accuracy of RST-AM with different c1.
OOOOOOO
19 8 7 6 5 4
% Aue-Jnyxx U@1
90
夕80
U
白70
n
Q
U
< 60
+j
S
①
I-	50
40
20	40	60	80	100	120	140	160	0	20	40	60	80	100	120	140	160
epochs	epochs
(a) Train accuracy	(b) Test accuracy
Figure 10:	Train accuracy and test accuracy of RST-AM with different weight-decays.
Since our hyperparameter tuning was conducted on CIFAR10/ResNet20, we give some results about
the hyperparameters on this model.
Effect of c2 in RST-AM. Figure 8 shows the effect of c2, where we kept c1 = 1, weight-decay=5 ×
10-4 fixed, and c2 = 10-1, 10-3, 10-5, 10-7, 10-9. It indicates that RST-AM is not sensitive to
c2.
Effect of c1 in RST-AM. Figure 9 shows the effect of c1, where we kept c2 = 10-7, weight-
decay=5 × 10-4 fixed, and c1 = 0.01, 0.1, 1, 10. It suggests that with smaller c1, RST-AM converges
faster in terms of train accuracy, but the test accuracy is worse.
■ O
42
Published as a conference paper at ICLR 2022
Effect of weight decay in RST-AM. Weight-decay is a common hyperparameter that can affect the
generalization of each optimizer. In Figure 10, we show the convergence behaviour of RST-AM
with different weight-decays. It suggests that with a too small weight-decay, RST-AM tends to be
overfitting in the test dataset.
90
---SAM: epochal 60
...RST-AM: epoch-80
——RST-AM: epoch-100
——RST-AM: epoch-160
0	20	40	60	80	100	120	140	160
epochs
■R 60
160
洪
›
U
2 7□
IZU
epochs
(b) Test accuracy on CIFAR10/WideResNet16-4
(a) Test accuracy on CIFAR10/ResNet18
70
20
60
洪
δ,50
2
§ 40
<
⅛J 30
10
O-
79
78
77
76
100 120 140 160
...SGDM: epoch-80
——SGDM: epoch-100
——SGDM: epoch-160
...SAM: epoch-80
---SAMiepochTOO
---SAM: epoch>160
RST-AM: epoch-80
RST-AM: epoch-100
RST-AM: epoch-160
0	20	40	60	80	100	120	140	160
epochs
(c) Test accuracy on CIFAR100/ResNeXt50
求60
›
⅛5°
u
XU 40
30
79
78
77
epochs
140	160
(d) Test accuracy on CIFAR100/DenseNet121
Figure 11: Training deep neural networks for 80,100,160 epochs. The results of final test accuracy
of RST-AM for training 80,100,160 epochs and the final test accuracy of SGDM for training 160
epochs are shown in the nested figures for comparison.
100 120 140 160
…- SGDM: epoch-80
——SGDM: epoch-100
——SGDM: epoch-160
---SAM: epoch-80
---SAMiepochTOO
---SAM: epoch-160
R歼AM: epoch-80
RCT-AM: epoch-100
RCT-AM: epoch-160

Table 5: The cost and final test accuracy compared with SGDM. The notations “m”,“t/e”, “e”,
“t” and “a” are abbreviations of memory, per-epoch time, training epochs, total running time, and
accuracy, respectively. “*” indicates numbers published in (Wei et al., 2021).
Cost(X SGDM)	CIFAR10/ResNet18	CIFAR10/WRN16-4
& accuracy	m	t/e	e	t	a(%)	m	t/e	e	t	a(%)
SGDM*	1.00	1.00	1.00	1.00	94.82	1.00	1.00	1.00	1.00	94.90
SAM(10)*	1.73	1.78	0.56	1.00	94.81	1.26	1.28	0.63	0.80	94.94
RST-AM	1.05	1.46	0.56	0.82	94.84	1.03	1.14	0.63	0.71	94.95
Cost (X SGDM)		CIFAR100/ResNeXt50					CIFAR100/DenseNet121			
& accuracy	m	t/e	e	t	a(%)	m	t/e	e	t	a(%)
SGDM*	1.00	1.00	1.00	1.00	78.41	1.00	1.00	1.00	1.00	78.49
SAM(10)*	1.30	1.16	0.50	0.58	78.37	1.16	1.19	0.50	0.60	78.84
RST-AM	1.04	1.07	0.50	0.54	78.39	1.01	1.11	0.50	0.55	78.90
Table 1(a) in the main paper reports the test accuracy of each optimizer when training for the same
epochs. In fact, as shown in Figure 11, within 100 epochs, RST-AM can achieve a better test
43
Published as a conference paper at ICLR 2022
accuracy than SGD. So if the running time of the training process matters, it is expected that RST-
AM can use less total running time due to the large number of reduction in training epochs. In
Table 1(b), we set SGD as the baseline, and compare the computation and memory cost with SGD.
In Table 5, we give more details about the saving in training epochs and the final test accuracy. It
can be seen that RST-AM can achieve a comparable or better test accuracy than SGDM with less
computation time, while reducing the memory footprint of SAM.
We also tried using Adam as a preconditioner for RST-AM but found the final test accuracy was
often worse. For example, the test accuracy of Adam-RST-AM for CIFAR-10∕ResNet20 is only
90.79%. We suppose it is the worse generalization ability of Adam (Luo et al., 2018) that makes
Adam not suitable as a preconditioner for RST-AM in the image classification task.
D.2.4 Experiments on Penn Treebank
This group of experiments were the same as those in (Wei et al., 2021) for direct comparison. Results
in Table 2 were measured with 3 random seeds. The parameter settings of the LSTM models were
the same as those in (Zhuang et al., 2020; Wei et al., 2021). The baseline optimizers were SGDM,
Adam, AdaBelief, and SAM. The validation dataset was used for tuning hyperparameters.
For SGDM, the learning rate (abbr. lr) was tuned via grid-search in {1, 10, 30, 100}. We set lr = 10,
momentum = 0.9 for the 2-layer/3-layer LSTM, and lr = 30, momentum = 0 for the 1-layer LSTM.
For Adam, the learning rate was tuned via grid-search in {1 × 10-3, 2 × 10-3, 5 × 10-3, 8×10-3, 1 ×
10-2, 2 × 10-2}. We found the setting that lr = 5 × 10-3 performs best.
For AdaBelief, we set lr = 5 × 10-3 which is better than the recommended settings of the official
implementation5.
For SAM, we used the recommended setting in (Wei et al., 2021) and used the baseline Adam as the
preconditioner. The cases m = 2 and m = 20 are denoted as SAM(2) and SAM(10), respectively.
For our method RST-AM, we kept c1 = 1, c2 = 1 × 10-7 unchanged and used the same precondi-
tioner (Adam) as SAM.
The batch size was 20. We trained the model for 500 epochs and the learning rate was decayed by
0.1 at the 250th epoch and the 375th epoch.
Table 6: The cost to achieve comparable results of Adam. The notations “m”,“t/e” and “t” are
abbreviations of memory, per-epoch time and total running time, respectively.
Cost (× Adam)	m	1-Layer t/e	t	m	2-Layer t/e	t	m	3-Layer t/e	t
Adam	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00
SAM(10)	1.20	1.84	0.74	1.36	1.88	0.75	1.90	1.67	0.67
RST-AM	1.06	1.73	0.69	1.15	1.78	0.71	1.11	1.53	0.61
Table 7: Test perplexity of training 1,2,3-layer LSTM on Penn Treebank for 200 epochs. Lower is
better. “*” indicates numbers published in (Wei et al., 2021).
Method	1-Layer	2-Layer	3-Layer
Adam*	80.88±.15	64.54±.18	60.34±.22
SAM(2)	81.82±.09	66.62±.26	61.55±.11
SAM(10)	79.30±.12	63.21±.02	59.47±.07
RST-AM	79.49±.11	63.61±.26	59.34±.23
In Table 6, we report the memory footprint and per-epoch running time of SAM(10) and RST-AM
compared with Adam. It indicates that RST-AM also largely reduces the memory overhead of the
5https://github.com/juntang-zhuang/Adabelief-OPtimizer/tree/update_0.2.0ZPyTorCh_Experiments/LSTM.
44
Published as a conference paper at ICLR 2022
long-memory SAM(10) in the language modeling task. Since the batch size is very small, the cost of
stochastic gradient evaluation is quite cheap, which makes the additional cost of matrix computation
in RST-AM and SAM(10) become considerable. However, if we consider achieving a comparable
validation/test perplexity to that of Adam, RST-AM does not need to train for the same number of
epochs as Adam. Table 7 shows the test perplexity of Adam, SAM(2), SAM(10), and RST-AM for
training 200 epochs. By comparing the results of Table 2 and Table 7, we see RST-AM is better than
Adam with much fewer training epochs, thus RST-AM can save a large amount of training time, as
shown in Table 6.
D.2.5 Experiments on adversarial training
The adversarial training considers the empirical adversarial risk minimization (EARM) problem:
1T
min — max	f¢. (x),
x∈Rd T i=1 kξiYik2≤e ξi
(101)
where ξi is the i-th adversarial data within the e-ball centered at ξi. We followed the standard PGD
adversarial training in (Madry et al., 2018), using projection gradient descent (PGD) to solve the
inner maximization problem and the tested optimizers (SGD and RST-AM) to solve the outer mini-
mization problem. The experiments were conducted on CIFAR10/ResNet18, CIFAR10/WRN16-4,
CIFAR100/ResNet18, and CIFAR100/DenseNet121. We trained the neural networks for 200 epochs
and decayed the learning rate at the 100th and 150th epoch.
Since adversarial training is much more time consuming than the ordinary training in Section D.2.3,
we tuned the hyperparameters in CIFAR10/ResNet20, and applied the same hyperparameters to
other models. We found the setting that c1 = 1, c2 = 1 × 10-7 and weight-decay = 0.001 is still
suitable for this task.
The CIFAR-10 (CIFAR-100) dataset contains 50K images for training and 10K images for testing.
Since it is found that the phenomenon of overfitting is severer (Rice et al., 2020) in adversarial
training, we randomly selected 5K images from the total 50K training dataset as the validation
dataset (the other 45K images remained as the training dataset), and chose the best checkpoint
model in the validation set to evaluate on the test dataset. We consider two types of test accuracy:
(i) the clean test accuracy, where clean data was used for model evaluation;
(ii)	the robust test accuracy, where corrupted data was used for model evaluation. The attacking
methods are FGSM (Goodfellow et al., 2014), PGD-20 (Madry et al., 2018), and C&Wg attack
(Carlini & Wagner, 2017).
Table 8: Test accuracy (%) for adversarial training.
Optimizer	Clean	CIFAR10/ResNet18			CIFAR100/DenseNet121			
		FGSM	PGD-20	Ct⅛W∞	Clean	FGSM	PGD-20	C&Wg
SGD	82.16	63.23	51.91	50.22	59.45	39.76	30.92	29.00
RST-AM	82.53	63.78	52.43	50.52	60.48	40.41	31.20	29.52
Optimizer		CIFAR10/WRN16-4				CIFAR100/ResNet18		
	Clean	FGSM	PGD-20	C&Wg	Clean	FGSM	PGD-20	C&Wg
SGD	80.84	60.97	49.29	47.62	55.42	36.17	28.18	26.31
RST-AM	81.36	61.38	49.93	47.95	56.49	37.00	28.50	26.66
In Tabel 8, we report the average results of tests with three different random seeds. It shows that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy across various
models on CIFAR10/CIFAR100. To see the convergence behaviour of SGD and RST-AM, we plot
the curves of the clean validation accuracy and the PGD-10 attacked validation accuracy in Fig-
ure 12. We can observe the phenomenon of robust overfitting (Rice et al., 2020) from these curves,
which justifies our experimental setting with validation set for the checkpoint selection. Nonetheless,
the numerical results suggest that RST-AM can still be better than SGDM in adversarial training.
It is also found that due to the heavy cost of gradient evaluations in PGD adversarial training, the
additional computational cost incurred by RST-AM is negligible and the per-epoch running time of
SGD and RST-AM is roughly the same. So we do not report it here.
45
Published as a conference paper at ICLR 2022
(a) CIFAR10/ResNet18
(c) CIFAR10/WRN16-4
(e) CIFAR100/ResNet18
(g) CIFAR100/DenseNet121
Figure 12: Clean accuracy and PGD-10 attacked accuracy on the validation set in training different
neural networks.
epochs
(b) CIFAR10/ResNet18
epochs
(d) CIFAR10/WRN16-4
epochs
(f) CIFAR100/ResNet18
(h) CIFAR100/DenseNet121
46
Published as a conference paper at ICLR 2022
D.2.6 Experiments on training a generative adversarial network
We describe our setting of training a generative adversarial network (GAN) here. Like the adver-
sarial training, the GAN training process is also a min-max problem. The stability of an optimizer
is critical for the training process. To demonstrate the applicability of RST-AM, we conducted
experiments on a GAN which was equipped with spectral normalization (Miyato et al., 2018) (SN-
GAN). The experimental setting was the same as that of AdaBelief (Zhuang et al., 2020): the dataset
was CIFAR-10; ResNets were used as the generator and the discriminator networks; the steps for
optimization in the discriminator and the generator per iteration were 5 and 1, respectively; the min-
batch size was 64 and the total iteration number was 100000. The Frechet Inception Distance (FID)
(Heusel et al., 2017) was used as the evaluation metric: lower FID score means better accuracy.
The baseline optimizer were Adam and AdaBelief as they perform well in this task (Zhuang et al.,
2020). We also used the recommended hyperparameter settings for the two optimizers. For our
RST-AM method, due to the ill-conditioning of the min-max problem, we used the AdaBelief as the
preconditioner and set the damping parameter αk = 0.6. The regularization parameters c1 = 1 and
c2 = 1 × 10-7 were still kept unchanged just as the previous experiments.
Figure 13: FID score for training SN-GAN on CIFAR-10.
In Figure 13, we show the curve of FID score for each optimizer, which is the average of three
independent runs. It indicates that the RST-AM method is stable for this min-max optimization
problem.
Table 9:	The effect of αk for RST-AM.
Method Adam AdaBelief α = 0.8 α = 0.6	α = 0.5 α = 0.4	α = 0.2 α = 0.1
FID score 13.07	12.80	12.48	12.05	12.75	13.13	13.07	12.59
Since we only tuned the damping parameter αk for RST-AM, we report the FID scores of other
choices of αk in Table 9 during our experiment. It shows that even with the suboptimal choices of
αk, e.g. αk = 0.1, 0.5, 0.8, RST-AM can still outperform the baselines.
D.3 Additional experiments
To further test the performance of our method in training neural networks on larger datasets or
different models, we conducted additional experiments of the image classification task in ImageNet
(Deng et al., 2009) and the Transformer (Vaswani et al., 2017) based neural machine translation task
in the IWSTL14 DE-EN (Cettolo et al., 2014) dataset.
D.3.1 Experiments on ImageNet
We trained ResNet50 on ImageNet with SGDM and RST-AM. We used the built-in ResNet50 model
in PyTorch. We ran the tests of each optimizer with three random seeds and four GeForce RTX 2080
47
Published as a conference paper at ICLR 2022
Ti GPUs were used for each test. The hyperparameters of SGDM were set as the recommended
setting in PyTorch. For RST-AM, the hyperparameters were kept the same as those in the CIFAR
experiments. The weight-decay was 1 × 10-4. The number of the total training epochs is 90. The
learning rate decay of SGD was at the 30th and 60th epochs. For RST-AM, since the experiments
on CIFAR show it can often converge faster to an acceptable solution than SGDM, we adopted the
early learning rate decay strategy recommended in (Zhang et al., 2019): decay the αk and βk for
RST-AM at the 30th, 50th and 70th epochs.
Table 10:	TOP 1 test accuracy (%) w.r.t. epoch, the best TOP1 test accuracy (%), and the cost. The
memory, per-epoch time and total time of SGDM are set as the units. The total time is the time to
first achieve the accuracy ≥ 75.90%.
Method epoch = 72 epoch = 88 epoch = 90 best memory per-epoch time total time
SGDM	75.75	75.90	75.81	75.93±.15	1.00	1.00	1.00
RST-AM	75.90	75.95	75.98	76.04±.06	1.06	1.01	0.83
In Table 10, we report the TOP1 accuracy in the validation dataset during training. Note that we also
report the epoch number for each optimizer to achieve the accuracy equal or exceeding 75.90%. It
shows RST-AM needs 72 epochs while SGDM needs 88 epochs. The curves of the training accuracy
and test accuracy are shown in Figure 14. The results suggest that RST-AM is still a competitive
optimizer in training a larger model in a larger dataset.
(a) Train accuracy
Figure 14: Train and test accuracy for training ImageNet/ResNet50.
(b) Test accuracy
D.3.2 Training Transformer
We conducted the neural machine translation task with Transformer. We implemented our RST-AM
method and integrated it into the fairseq framework 6. The basic experimental setting was set as
the recommended setting in (Yao et al., 2021). We trained the model for 50 epochs and the BLEU
(Papineni et al., 2002) score was calculated using the average model of the last five checkpoints. We
added a baseline optimizer RAdam (Liu et al., 2019) which was inspired by the warmup procedure
in training Transformer.
Table 11: The BLEU score of training Transformer on IWSLT14.
Method	SGD	Adam	AdaBelief	RAdam	RST-AM
BLEU score	28.14±.08	35.71±.03	35.15±.14	35.60±.06	35.89±.02
6 https://github.com/pytorch/fairseq.
48
Published as a conference paper at ICLR 2022
The numerical results reported in Table 11 show that Adam is well-suited for this neural machine
translation task, though it does not perform well in the image classification task. Also, the results
demonstrate that RST-AM can still outperform Adam in this task.
Table 12: BLEU score evaluated at the 40/45/50-th epoch, and the cost. The memory, per-epoch
time and total time of Adam are set as the units. The total time is the time of RST-AM to achieve
a BLEU score matching the final BLEU of Adam: | BLEU(RST-AM) - BLEU(Adam) | ≤ 0.03,
where 0.03 is the standard deviation of the results of Adam.
Method	epoch = 40	epoch = 45	epoch = 50	memory	per-epoch time	total time
Adam	35.42±.10	35.54±.08	35.71±.03	1.00	1.00	1.00
RST-AM	35.59±.14	35.69±.06	35.89±.02	1.16	1.00	0.90
In Table 12, we report the BLEU scores evaluated in the test dataset at the 40th, 45th and 50th epochs.
The results show the better performance of RST-AM over Adam and the per-epoch computational
cost is nearly the same. To achieve a comparable solution to Adam, RST-AM can save 10% training
time.
49