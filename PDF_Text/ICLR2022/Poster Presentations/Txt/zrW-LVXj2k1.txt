Published as a conference paper at ICLR 2022
On the benefits of maximum likelihood
estimation for Regression and Forecasting
Pranjal Awasthi, Abhimanyu Das, Rajat Sen & Ananda Theertha Suresh
Google Research
{pranjalawasthi, abhidas, senrajat, theertha}@google.com
Ab stract
We advocate for a practical Maximum Likelihood Estimation (MLE) approach
towards designing loss functions for regression and forecasting, as an alternative
to the typical approach of direct empirical risk minimization on a specific target
metric. The MLE approach is better suited to capture inductive biases such as prior
domain knowledge in datasets, and can output post-hoc estimators at inference time
that can optimize different types of target metrics. We present theoretical results
to demonstrate that our approach is competitive with any estimator for the target
metric under some general conditions. In two example practical settings, Poisson
and Pareto regression, we show that our competitive results can be used to prove
that the MLE approach has better excess risk bounds than directly minimizing
the target metric. We also demonstrate empirically that our method instantiated
with a well-designed general purpose mixture likelihood family can obtain superior
performance for a variety of tasks across time-series forecasting and regression
datasets with different data distributions.
1	Introduction
The task of fitting a regression model for a response variable y against a covariate vector x ∈ Rd
is ubiquitous in supervised learning in both linear and non-linear settings (LathUiliere et al., 2020;
Mohri et al., 2018) as well as non-i.i.d settings like multi-variate forecasting (Salinas et al., 2020;
Wang et al., 2019). The end goal in regression and forecasting problems is often to use the resulting
model to obtain good performance in terms of some target metric of interest on the population level
(usually measured on a previously unseen test set). The mean-squared error or the mean absolute
deviation are examples of common target metrics.
In this paper, our focus is on the choice of loss function that is used to train such models, which is an
important question that is often overlooked, especially in the deep neural networks context where the
emphasis is more on the choice of network architecture (Lathuiliere et al., 2020).
Perhaps the most common method used by practitioners for choosing the loss function for a particular
regression model is to directly use the target metric of interest as the loss function for empirical risk
minimization (ERM) over a function class on the training set. We denote this approach for choosing a
loss function as Target Metric Optimization (TMO). This is especially more common with the advent
of powerful general purpose function optimizers like deep networks and has also been rigorously
analyzed for simpler function classes (Mohri et al., 2018).
Target Metric Optimization seems like a reasonable approach - if the practitioner knows about the
target metric of interest for prediction using the model, it seems intuitive to optimize for the same
objective on the training data. Prior work (both theoretical and applied) has both advocated for and
argued against TMO for regression problems. Many prominent works on regression (Goldberger
et al., 1964; Lecue & Mendelson, 2016) use the TMO approach, though most of them assume that
the data is well behaved (e.g. sub-Gaussian noise). In terms of applications, many recent works on
time-series forecasting (Wu et al., 2020; Oreshkin et al., 2019; Sen et al., 2019) also use the TMO
approach directly on the target metric. On the other hand, the robust regression literature has long
advocated for not using the target metric directly for ERM in the case of contamination or heavy
tailed response/covariate behaviour (Huber, 1992; Hsu & Sabato, 2016; Zhu & Zhou, 2021; Lugosi
& Mendelson, 2019a; Audibert et al., 2011; Brownlees et al., 2015) on account of its suboptimal
1
Published as a conference paper at ICLR 2022
high-probability risk bounds. However, as noted in (Prasad et al., 2020), many of these methods
are either not practical (Lugosi & Mendelson, 2019a; Brownlees et al., 2015) or have sub-optimal
empirical performance (Hsu & Sabato, 2016). Even more practical methods such as (Prasad et al.,
2020) would lead to sufficiently more computational overhead over standard TMO.
Another well known approach for designing a loss function is Maximum Likelihood Estimation (MLE).
Here one assumes that the conditional distribution of y given x belongs to a family of distributions
p(y|x; θ) parameterized by θ ∈ Θ (McCullagh & Nelder, 2019). Then one can choose the negative
log likelihood as the loss function to optimize using the training set, to obtain an estimate θmie. This
approach is sometimes used in the forecasting literature (Salinas et al., 2020; Davis & Wu, 2009)
where the choice of a likelihood can encode prior knowledge about the data. For instance a negative
binomial distribution can be used to model count data. During inference, given a new instance x0,
one can output the statistic from p(y|x0; θmle) that optimizes the target metric, as the prediction
value (Gneiting, 2011). MLE also seems like a reasonable approach for loss function design - it is
folklore that the MLE is asymptotically optimal for parameter estimation, in terms of having the
smallest asymptotic variance among all estimators (Heyde, 1978; Rao, 1963), when the likelihood is
well-specified. However, much less is known about finite-sample, fixed-dimension analysis of MLE,
which is the typical regime of interest for the regression problems we consider in this paper. An
important practical advantage for MLE is that model training is agnostic to the choice of the target
metric - the same trained model can output estimators for different target metrics at inference time.
Perhaps the biggest argument against the MLE approach is the requirement of knowing the likelihood
distribution family. We address both these topics in Section 5.
Both TMO and MLE can be viewed as offering different approaches to selecting the loss function
for a given regression model. In this paper, we argue that in several settings, both from a practical
and theoretical perspective, MLE might be a better approach than TMO. This result might not be
immediately obvious apriori - while MLE does benefit from prior knowledge of the distribution,
TMO also benefits from prior knowledge of the target metric at training time.
Our main contributions are as follows:
Competitiveness of MLE: In Section 3, we prove that under some general conditions on the family
of distributions and a property of interest, the MLE approach is competitive with any estimator for
the property. We show that this result can be applied to fixed design regression problems in order to
prove that MLE can be competitive (up to logarithmic terms) with any estimator in terms of excess
square loss risk, under some assumptions.
Example Applications: In Section 4.1, we apply our general theorem to prove an excess square loss
bound for the the MLE estimator for Poisson regression with the identity link (Nelder & Wedderburn,
1972; Lawless, 1987). We show that these bounds can be better than those of the TMO estimator,
which in this case is least-squares regression. Then in Section 4.2, we show a similar application
in the context of Pareto regression i.e y|x follows a Pareto distribution. We show that MLE can be
competitive with robust estimators like the one in (Hsu & Sabato, 2016) and therefore can be better
than TMO (least-squares).
Empirical Results: We propose the use of a general purpose mixture likelihood family (see
Section 5) that can capture a wide variety of prior knowledge across datasets, including zero-inflated
or bursty data, count data, sub-Gaussian continuous data as well as heavy tailed data, through different
choices of (learnt) mixture weights. Then we empirically show that the MLE approach with this
likelihood can outperform ERM for many different commonly used target metrics like WAPE, MAPE
and RMSE (Hyndman & Koehler, 2006) for two popular forecasting and two regression datasets.
Moreover the MLE approach is also shown to have better probabilistic forecasts (measured by
quantile losses (Wen et al., 2017)) than quantile regression (Koenker & Bassett Jr, 1978; Gasthaus
et al., 2019; Wen et al., 2017) which is the TMO approach in this case.
2	Prior Work on MLE
Maximum likelihood estimators (MLE) have been studied extensively in statistics starting with
the work of Wald (1949); Redner (1981), who showed the maximum likelihood estimates are
asymptotically consistent for parametric families. Fahrmeir & Kaufmann (1985) showed asymptotic
2
Published as a conference paper at ICLR 2022
normality for MLE for generalized linear models. It is also known that under some regularity
assumptions, MLE achieves the Cramer-Rao lower bound asymptotically (Lehmann & Casella, 2006).
However, we note that none of these asymptotic results directly yield finite sample guarantees.
Finite sample guarantees have been shown for certain problem scenarios. Geer & van de Geer (2000);
Zhang (2006) provided uniform convergence bounds in Hellinger distance for maximum likelihood
estimation. These ideas were recently used by Foster & Krishnamurthy (2021) to provide algorithms
for contextual bandits. There are other works which study MLE for non-parametric distributions e.g.,
Dumbgen & RUfibach (2009) showed convergence rates for log-concave distributions. There has
been some works (SUr & Candis, 2019; Bean et al., 2013; Donoho & Montanari, 2016; El Karoui,
2018) that show that MLE can be sub-optimal for high dimensional regression i.e when the dimension
grows with the number of samples. In this work we focus on the setting where the dimension does
not scale with the number of samples.
Our MLE results differ from the above work as we provide finite sample competitiveness guarantees.
Instead of showing that the maximum likelihood estimator converges in some distance metric, we
show that under some mild assumptions it can work as well as other estimators. Hence, our methods
are orthogonal to known well established results in statistics.
Perhaps the closest to our work is the competitiveness result of Acharya et al. (2017), who showed
that MLE is competitive when the size of the output alphabet is bounded and applied to profile
maximum likelihood estimation. In contrast, our work applies for unbounded output alphabets and
can provide stronger guarantees in many scenarios.
3	Competitiveness of MLE
In this section, we will show that under some reasonable assumptions on the likelihood family, the
MLE is competitive with any estimator in terms of estimating any property of a distribution from the
family. We will then show that this result can be applied to derive bounds on the MLE in some fixed
design regression settings that can be better than that of TMO. We will first setup some notation.
Notation: Given a positive semi-definite symmetric matrix M, kxkM := xT Mx is the matrix
norm of the vector x. λ(M) denotes an eigenvalue of a symmetric square matrix M; specifically
λmax (M) and λmin (M) denote the maximum and minimum eigenvalues respectively. The letter f
is used to denote general probability densities. We use p to denote the conditional probability density
of the response given the covariate.小％ Will be overloaded to denote the 'ι norm between two
probability distributions i.e kp - p0k1 :=	|p(z) - p0(z)|dz. DKL(p1; p2) will be used to denote the
KL-divergence between the two distributions. If Z is a set equipped with a norm ∣∣∙k, then N(e, Z)
will denote an -net i.e any point z ∈ Z has a corresponding point z0 ∈ N(, Z) s.t kz - z0k ≤ .
Brd denotes the ball centered at the origin with radius r and Srd-1 denotes its surface. We define
[n] := {1, 2,…，n}. ∣∙∣ denotes the size of the enclosed set.
General Competitiveness: We first consider a general family of distributions F over the space
Z. For a sample Z 〜 f (for Z ∈ Z and f ∈ F), the MLE distribution is defined as fz =
argmaxf∈F f(z). We are interested in estimating a property π : F → W of these distributions from
an observed sample. The following definition will be required to impose some joint conditions on the
distribution family and the property being estimated, that are needed for our result.
Definition 1. The tuple (F, π), where F is a set of distributions and π : F → W a property
of those distributions, is said to be (T, e, δ1, δ2)-approximable, if there exists a set of distribu-
tions F ⊆ F s.t |F| ≤ T and for every f ∈ F, there exists a f such that ∣f - f∣1 ≤ δ1 and
Prz〜f (1(fz) — ∏(fz)∣∣ ≥ e) ≤ δ2, where fz = argmax∕∈f f(z) and W has a norm ∣∣∙∣.
The above definition states that the set of distributions F has a finite δ-cover, F in terms of the '1
distance. Moreover the cover is such that solving MLE on the cover and applying the property π on
the result of the MLE is not too far from π applied on the MLE over the whole set F. This property
is satisfied trivially if F is finite. We note that it is also satisfied by some commonly used set of
distributions and corresponding properties. Now we state our main result.
Theorem 1.	Let π be an estimator such that for any f ∈ F and Z 〜f, Pr(k∏(f) — ∏(z)k ≥e) ≤ δ.
Let Ff be a subset ofF that contains f such that with probability at least 1 - δ, fz ∈ Ff and (Ff, π)
3
Published as a conference paper at ICLR 2022
is (T, , δ1, δ2)-approximable. Then the MLE satisfies the following bound,
Pr(kπ(f)-π(fz)k ≥3) ≤ (T+3)δ+δ1+δ2.
We provide the proof of Theorem 1 in Appendix A. We also provide a simpler version of this result
for finite distribution families as Theorem 3 in Appendix A for the benefit of the reader.
Competitiveness in Fixed Design Regression: Theorem 1 can be used to show that MLE is
competitive with respect to any estimator for square loss minimization in fixed design regression. We
will first formally introduce the setting. Consider a fixed design matrix X ∈ Rn×d where n is the
number of samples and d the feature dimension. We will work in a setting where n d. The target
vector is a random vector given by yn ∈ Rn . Let yi be the i-th coordinate of yn and xi denote the
i-th row of the design matrix. We assume that the target is generated from the conditional distribution
given xi such that,
y 〜p(∙Xθ*), θ ∈ Θ.
We are interested in optimizing a target metric '(∙, ∙) given an instance of the random vector yn. The
final objective is to optimize,
1n
miH Eyi 〜P(∙∣X0Θ*) nE'(yi,h(Xi))
∈	n i=1
where H is a class of functions. In this context, we are interested in comparing two approaches.
TMO (see (Mohri et al., 2018)). This is standard empirical risk minimization on the tar-
get metric where given an instance of the random vector yn one outputs the estimator h =
minh∈H n Pn=I '(yi, h(xi)).
MLE and post-hoc inference (see (Gneiting, 2011)). In this method one first solves for the parameter
in the distribution family that best explains the empirical data by MLE i.e.,
n
θmie := argmin L(yn; θ), where L(yn; θ) := X - logp(y/xi； θ)
θ∈Θ	i=i
〜
Then during inference given a sample xi the predictor is defined as, h(xi) :=
argminy Ey∈p(.∣χ ^ ɪ)['(y,y)] or in other words We output the statistic from the MLE distribu-
tion that optimizes the loss function of interest. For instance if ' is the square loss, then h(xi) will be
the mean of the conditional distribution p(y|xi; θmle).
We will prove a general result using Theorem 1 when the target metric ` is the square loss and H is a
linear function class. Moreover, the true distribution p(∙∣Xi; θ*) is such that E[y∕ = (θ*, Xi〉for all
i ∈ [n] i.e we are in the linear realizable setting.
In this case the quantity of interest is the excess square loss risk given by,
nn
E(θ) := - XEynkyn - Xθk2 - - XEynkyn - Xθ*k2 = kθ -叫∑,
n i=i	n i=i
(1)
where Σ := (Pi XiXT)/n is the normalized covariance matrix, and θ* is the population minimizer
of the target metric over the linear function class. Now we are ready to state the main result.
Theorem 2.	Consider a fixed design regression setting where the likelihood family is parameterized
by θ ∈ Θ ⊆ Bdw and |N (, Θ ∩ Bdw)| ≤ |N (, Bdw)| for a small enough . Further the following
conditions hold,
1.	DKL(P(yi； θ);p(yi； θ0)) ≤ Lkθ - θ0k2.
2.	The negative log-likelihood L(yn; θ) as a function of θ is α-strongly convex and β-smooth, w.p.
at least 1 - δ.
Further suppose there exists an estimator θest such that E(θest) ≤ (ci + c? log(1∕δ))η/n, where
c1 , c2 are problem dependent quantities and η > 0. Then the MLE estimator also satisfies,
4
Published as a conference paper at ICLR 2022
E (θmle) = O
(ci + C2d(log n + log(wLλmaχ(Σ)) + log(β∕α) + log 1))“、
n
w.p at least 1 - δ.
We provide the proof in Appendix C. The proof involves proving the conditions in Theorem 1 and
bounding the size of the cover T .
In order to better understand Theorem 2, let us consider a typical case where there exists a possibly
complicated estimator such that E(θest) = O((d + log(1∕δ))∕n). In this case the above theorem
implies that MLE will be competitive with this estimator up to a log n factor. In many cases the MLE
might be much simpler to implement than the original estimator but would essentially match the
same error bound. We now provide specific examples in subsequent sections.
4	Applications of Competitiveness Result
In this section we will specialize to two examples, Poisson regression and Pareto regression, where
we show that MLE can be better than TMO through the use of our competitive result in Theorem 2.
4.1	Poisson Regression
We work in the fixed design setting in Section 3 and assume that the conditional distribution of y |x is
Poisson i.e,
μk e-μi
p(yi = k|xi； θ*) =	-iyj—	where,	μi	=	<θ",	Xii	>	0,	(2)
k!
for all i ∈ [n]. Poisson regression is a popular model for studying count data regression which
naturally appears in many applications like demand forecasting (Lawless, 1987). Note that here we
study the version of Poisson regression with the identity link function (Nelder & Wedderburn, 1972),
while another popular variant is the one with exponential link function (McCullagh & Nelder, 2019).
We choose the identity link function for a fair comparison of the two approaches as it is realizable
for both the approaches under the linear function class i.e the globally optimal estimator in terms
of population can be obtained by both approaches. The exponential link function would make the
problem non-realizable under a linear function class for the TMO approach.
We make the following natural assumptions. Let Σ = (Pin=1 xixiT )∕n be the design covariance
matrix as before and M = (Pn=I μiUiUiT)∕n, where Ui = Xi∕∣∣Xik2. Let X and Z be the condition
numbers of the matrices M and Σ respectively.
Assumption 1. The parameter space Θ and the design matrix X satisfy the following,
•	(A1) The parameter space Θ = {θ ∈ Rd : kθk2 ≤ w, min(kθk22, hθ, xii) ≥ γ > 0, ∀i ∈ [n]}.
•	(A2) The design matrix is such that λmin(Σ) > 0 and kxik2 ≤ R for all i ∈ [n].
•	(A3) Let λmin(M) ≥ 4R22 (dlog(24χ) + log(1∕δ)) and ,λmaχ(M)(dlog(24χ) + log(1∕δ)) ≤
√nλmin(M )∕16 ,for a small δ ∈ (0,1)1.
The above assumptions are fairly mild. For instance λma is Ω(1∕d) for random Gaussian covariance
matrices (Bai & Yin, 2008). The other part merely requires that λmin(M) = Ω(ddλmaχ(M)∕n).
We are interested in comparing MLE with TMO for the square loss which is just the least-squares
2
estimator i.e θis := argmme∈θ n ∣∣yn - Xθ∣∣2∙ Note that it is apnon unclear as to which approach
would be better in terms of the target metric because on one hand the MLE method knows the
distribution family but on the other hand TMO is explicitly geared towards minimizing the square
loss during training.
Least squares analysis is typically provided for regression under sub-Gaussian noise. By adapting
existing techniques (Hsu et al., 2012), we show the following guarantee for Poisson regression with
least square loss. We provide a proof in Appendix D for completeness. 1
1 Note that the constants can be further tightened in our analysis.
5
Published as a conference paper at ICLR 2022
Lemma 1. Let μmaχ = maxi μi. The least squares estimator θg satisfies the following loss bounds
w.p. at least 1 - δ,
^	_ ∫O(μmnax(log 1 + d))	if μmaχ ≥ (log(1∕δ)+ dlog6)∕2
(Is)	(θ( 1 (log 1 + d)2)	otherwise
Now we present our main result in this section which uses the competitiveness bound in Theorem 2
coupled with the existence of a superior estimator compared to TMO, to show that the MLE estimator
can have a better bound than TMO.
In Theorem 4 (in Appendix F), under some mild assumptions on the covariates, we construct an
estimator θest with the following bound for the Poisson regression setting,
E (θest) ≤ C ∙ kθ* k2 ∙ λmaχ(Σ) ( d + ：g( 1)).	⑶
The construction of the estimator is median-of-means tournament based along the lines of (Lugosi &
Mendelson, 2019a) and therefore the estimator might not be practical. However, this immediately
gives the following bound on the MLE as a corollary of Theorem 2.
Corollary 1. Under assumption 1 and the conditions of Theorem 4, the MLE estimator for the
Poisson regression setting satisfies w.p. at least 1 - δ,
E (θmle) = O(k 叫 2 ∙ λmaχ(Σ)	n + logCwR'"⑶)+lθg X + log δ )).
The bound in Corollary 1 can be better than the bound for θls in Lemma 1. In the sub-Gaussian
region, the bound in Lemma 1 scales linearly with μmaχ which can be prohibitively large even when
a few covariates have large norms. The bound for the MLE estimator in Corollary 1 has no such
dependence. Further, in the sub-Exponential region the bound in Lemma 1 scales as O(d2∕n) while
the bound in Corollary 1 has a O(d∕n) dependency, up to log-factors. In Appendix G, we also show
that when the covariates are one-dimensional, an even sharper analysis is possible, that shows that
the MLE estimator is always better than least squares in terms of excess risk. In Appendix I.8, we
perform a simulated experiment that shows a linear growth of the test error w.r.t λmax(Σ), further
showing the efficacy of our theoretical bounds.
4.2 Pareto Regression
Now we will provide an example of a heavy tailed regression setting where it is well-known that
TMO for the square loss does not perform well (Lugosi & Mendelson, 2019a). We will consider the
Pareto regression setting given by,
bmb	- b - 1 ,八* 一
p(yi|xi) = b+1 , mi = h hθ , Xii	for yi ≥ mi
yi
(4)
provided <θ*, Xii > γ for all i ∈ [n]. Thus y% is Pareto given Xi and E[y∕xi] = μ% :=⑥,x/. We
will assume that b > 4 such that 4 + -moment exists for > 0. As in the Poisson setting, we choose
this parametrization for a fair comparison between TMO and MLE i.e in the limit of infinite samples
θ* lies in the linear solution space for both TMO (least squares) and MLE.
As before, to apply Theorem 2 we need an estimator with a good risk bound. We use the estimator in
Theorem 4 of (Hsu & Sabato, 2016), which in the fixed design pareto regression setting yields,
E (θest) = (1+ O( d⅛lj1
θ*kΣ
b(b-2).
Note that the above estimator might not be easily implementable, however this yields the following
corollary of Theorem 2, which is a bound on the performance of the MLE estimator.
Corollary 2. Under assumptions of our Pareto regression setting, the MLE estimator satisfies w.p.
at least 1 - δ,
-	/d2 (log n + log Z + log bwRλmax⑸ + log j)
E (θmle) = 1 + O -δ---------------------γ------------L
n
kθ*kΣ
b(b-2).
6
Published as a conference paper at ICLR 2022
The proof is provided in Appendix H. It involves verifying the two conditions in Theorem 2 in the
Pareto regression setting.
The above MLE guarantee is expected to be much better than what can be achieved by TMO which
is least-squares. It is well established in the literature (Hsu & Sabato, 2016; Lugosi & Mendelson,
2019a) that ERM on square loss cannot achieve a O(log(1∕δ)) dependency in a heavy tailed regime;
instead it can achieve only a O(,1∕δ) rate.
5	Choice of Likelihood and Inference Methods
In this section we discuss some practical considerations for MLE, such as adapting to a target metric
of interest at inference time, and the choice of the likelihood family.
Inference for different target metrics: In most practical settings, the trained regression model
is used at inference time to predict the response variable on some test set to minimize some target
metric. For the MLE based approach, once the distribution parameters are learnt, this involves using
an appropriate statistic of the learnt distribution at inference time (see Section 3). For mean squared
error and mean absolute error, the estimator corresponds to the mean and median of the distribution,
but for several other commonly used loss metrics in the forecasting domain such as Mean Absolute
Percentage Error (MAPE) and Relative Error (RE) (Gneiting, 2011; Hyndman & Koehler, 2006),
this estimator corresponds to a median of a transformed distribution (Gneiting, 2011). Please see
Appendix I for more details. This ability to optimize the estimator at inference time for different
target metrics using a single trained model is another advantage that MLE based approaches have
over TMO models that are trained individually for specific target metrics.
Mixture Likelihood: An important practical question when performing MLE-based regression
is to decide which distribution family to use for the response variable. The goal is to pick a
distribution family that can capture the inductive biases present in the data. It is well known that a
misspecified distribution family for MLE might adversely affect generalization error of regression
models (Heagerty & Kurland, 2001). At the same time, it is also desirable for the distribution family
to be generic enough to cater to diverse datasets with potentially different types of inductive biases,
or even datasets for which no distributional assumptions can be made in advance.
A simple approach that we observe to work particularly well in practice with regression models using
deep neural networks is to assume the response variable comes from a mixture distribution, where
each mixture component belongs to a different distribution family and the mixture weights are learnt
along with the parameters of the distribution. More specifically, we consider a mixture distribution of
k components p(y|x; θ1, . . . , θk, w1, . . . , wk) = Pjk=1 wj pj (y|x; θj), where each pj characterizes
a different distribution family, and the mixture weights wj and distribution parameters θj are learnt
together. We would like to have a mixture distribution that can handle different situations like sparse
data, sub-Exponential and sub-Gaussian tails, count data as well as heavy tailed data. Moreover it
should be applicable to continuous valued datasets in general.
We use a three component mixture of (i) the constant 0 (zero-inflation for dealing with bi-modal
sparse data), (ii) a continuous version of negative binomial where n and p are learnt and (iii) a
Pareto distribution where the scale parameter is learnt. We provide more details about the continuous
version of negative binomial distribution in Appendix I.2. Our experiments in Section 6 show that
this mixture shows promising performance on a variety of datasets.
This will increase the number of parameters and the resulting likelihood might require non-convex
optimization. However, we empirically observed that with sufficiently over-parameterized networks
and gradient-based optimizers, this is not a problem in practice (Fig. 4 shows a convergence curve).
6	Empirical Results
We present empirical results on two time-series forecasting problems and two regression problems
using neural networks. We will first describe our models and baselines. Our goal is to compare the
MLE approach with the TMO approach for three target metrics per dataset.
7
Published as a conference paper at ICLR 2022
Model	Favorita			M5		
	MAPE	WAPE	RMSE	MAPE	WAPE	RMSE
TMO(MSE)	0.6121±0.0075	0.2891±0.0023	175.3782±0.8235	0.5045±0.004	0.2839±0.0008	7.507±0.023
TMO(MAE)	0.3983±0.0012	0.2258±0.0006	161.4919±0.4748	0.4452±0.0005	0.266±0.0001	7.0503±0.0094
TMO(MAPE)	0.3199±0.0011	0.2528±0.0016	192.3823±1.3871	0.3892±0.0001	0.3143±0.0007	11.3799±0.1965
TMO(Huber)	0.432±0.0033	0.2366±0.0018	164.7006±0.7178	0.4722±0.0007	0.269±0.0002	7.093±0.0133
MLE(ZNBP)	0.3139±0.0011	0.2238±0.0009	164.6521±1.5185	0.3864±0.0001	0.2677±0.0002	7.2133±0.0152
Table 1: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two time-series datasets. The confidence
intervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,
we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, we
output the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean.
Model	Bicycle Share			Gas Turbine		
	MAPE	WAPE	RMSE	MAPE	WAPE	RMSE
TMO(MSE)	0.2503±0.0008	0.1421±0.0003	878.5815±1.3059	0.8884±0.0118	0.3496±0.0041	1.5628±0.0071
TMO(MAE)	0.2594±0.0011	0.1436±0.0003	901.1357±1.4943	0.774±0.0054	0.3389±0.0019	1.5789±0.0067
TMO(MAPE)	0.2382±0.0012	0.1469±0.0012	899.9163±4.8219	0.8108±0.0009	0.8189±0.001	3.0573±0.0019
TMO(Huber)	0.2536±0.0011	0.1414±0.0004	889.1173±1.9654	0.902±0.0128	0.3598±0.0049	1.5992±0.0082
MLE(ZNBP)	0.1969±0.0018	0.1235±0.001	767.4368±7.1274	0.9877±0.0019	0.3379±0.0004	1.4547±0.0054
Table 2: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two regression datasets. The confidence
intervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,
we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, we
output the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean.
Common Experimental Protocol: Now we describe the common experimental protocol on all the
datasets (we get into dataset related specifics and architectures subsequently). For a fair comparison
the architecture is kept the same for TMO and MLE approaches. For each dataset, we tune the
hyper-parameters for the TMO(MSE) objective. Then these hyper-parameters are held fixed for all
models for that dataset i.e only the output layer and the loss function is modified. We provide all the
details in Appendix I. Our code will be available here.
For the MLE approach, the output layer of the models map to the MLE parameters of the mixture
distribution introduced in Section 5, through link functions. The MLE output has 6 parameters, three
for mixture weights, two for negative binomial component and one for the scale parameter in Pareto.
The choice of the link functions and more details are specified in Appendix I.3. The loss function
used is the negative log-likelihood implemented in Tensorflow Probability (Dillon et al., 2017). Note
that for the MLE approach only one model is trained per dataset and during inference we output
the statistic that optimizes the target metric in question. We refer to our MLE based models that
employ the mixture likelihood from Section 5 as MLE(ZNBP)loss, where ZNBP refers to the mixture
components Zero, Negative-Binomial and Pareto.
For TMO, the output layer of the models map to y and We directly minimize the target metric in
question. Note that this means we need to train a separate model for every target metric. Thus we
have one model each for target metrics in {’MSE’, ’MAE’, ’MAPE’}. Further We also train a model
using the Huber loss 2. In order to keep the number of parameters the same as that of MLE, We add
an additional 6 neurons to the TMO models.
6.1	Experiments on Forecasting Datasets
We perform our experiments on tWo Well-knoWn forecasting datasets used in Kaggle competitions.
1.	The M5 dataset (M5, 2020) consists of time series data of product sales from 10 Walmart stores in
three US states. The data consists of tWo different hierarchies: the product hierarchy and store
location hierarchy. For simplicity, in our experiments We use only the product hierarchy consisting
of 3K individual time-series and 1.8K time steps.
2.	The Favorita dataset (Favorita, 2017) is a similar dataset, consisting of time series data from
Corporacidn Favorita, a South-American grocery store chain. As above, we use the product
hierarchy, consisting of 4.5k individual time-series and 1.7k time steps.
The task is to predict the values for the last 14 days all at once. The preceding 14 days are used for
validation. We provide more details about the dataset generation for reproducibility in Appendix I.
2The Huber loss is commonly used in robust regression (Huber, 1992; Lugosi & Mendelson, 2019a)
8
Published as a conference paper at ICLR 2022
Model	p10QL	p90QL
TMO (Quantile)	0.0973±0.0002	0.0628±0.0019
MLE(ZNBP)	0.0788±0.0008	0.0536±0.0007
Table 3: The MLE model predicts the empirical quantile of inter-
est during inference. It is compared with Quantile regression (TMO
based). The results, averaged over 50 runs along with the correspond-
ing confidence intervals are presented.
Model	MAPE	WAPE	RMSE
MLE(NB)	0.3314+/-0.0016	0.2521+/-0.002	175.501+/-1.1928
MLE(ZNB)	0.3186+/-0.0011	0.2453+/-0.002	170.0075+/-1.282
MLE(ZNBP)	0.3139±0.0011	0.2238±0.0009	164.6521+/-1.5185
Table 4: We perform an ablation study on the Favorita dataset,
where we progressively add the components of our mixture distribu-
tion. There are three MLE models in the progression: Negative Bino-
mial (NB), Zero-Inflated Negative Binomial (ZNB) and finally ZNBP.
The base architecture for the baselines as well as our model is a seq-2-seq model (Sutskever et al.,
2014). The encoders and decoders both are LSTM cells (Hochreiter & Schmidhuber, 1997). The
architecture is illustrated in Figure 1 and described in more detail in Appendix I.
We present our experimental results in Table 1. On both the datasets the MLE model with the
appropriate inference-time estimator for a metric is always better than TMO trained on the same
target metric, except for WAPE in M5 where MLE’s performance is only marginally worse. Note
that the MLE model is always the best or second best performing model on all metrics, among all
TMO models. For TMO the best performance is not always achieved for the same target metric. For
instance, TMO(MAE) performs better than TMO(MSE) for the RMSE metric on the Favorita dataset.
In Table 4 we perform an ablation study on the Favorita dataset, where we progressively add mixture
components resulting in three MLE models: Negative Binomial, Zero-Inflated Negative Binomial
and finally ZNBP. This shows that each of the components add value in this dataset.
6.2	Experiments on Regression Datasets
We perform our experiments on two standard regression datasets,
1.	The Bicyle Share dataset (Bicycle, 2017) has daily counts of the total number of rental bikes.
The features include time features as well as weather conditions such as temperature, humidity,
windspeed etc. A random 10% of the dataset is used as test and the rest for training and validation.
The dataset has a total of 730 samples.
2.	The Gas Turbine dataset (Kaya et al., 2019) has 11 sensor measurements per example (hourly)
from a gas turbine in Turkey. We consider the level of NOx as our target variable and the rest
as predictors. There are 36733 samples in total. We use the official train/test split. A randomly
chosen 20% of the training set is used for validation. The response variable is continuous.
For all our models, the model architecture is a fully connected DNN with one hidden layer that has
32 neurons. Note that for categorical variables, the input is first passed through an embedding layer
(one embedding layer per feature), that is jointly trained. We provide further details like the shape of
the embedding layers in Appendix I. The architecture is illustrated in Figure 2.
We present our results in Table 2. On the Bicycle Share dataset, the MLE(ZNBP)based model
performs optimally in all metrics and often outperforms the TMO models by a large margin even
though TMO is a separate model per target metric. On the Gas Turbine dataset, the MLE based model
is optimal for WAPE and RMSE, however it does not perform that well for the MAPE metric.
In Table 3, we compare the MLE based approach versus quantile regression (TMO based) on the
Bicycle Share dataset, where the metric presented is the normalized quantile loss (Wang et al., 2019).
We train the TMO model for the corresponding quantile loss directly and the predictions are evaluated
on normalized quantile losses as shown in the table. The MLE based model is trained by minimizing
the negative log-likelihood and then during inference we output the corresponding empirical quantile
from the predicted distribution. MLE(ZNBP)outperforms TMO(Quantile) significantly.
Discussion: We compare the approaches of direct ERM on the target metric (TMO) and MLE
followed by post-hoc inference time optimization for regression and forecasting problems. We prove
a general competitiveness result for the MLE approach and also show theoretically that it can be
better than TMO in the Poisson and Pareto regression settings. Our empirical results show that our
proposed general purpose likelihood function employed in the MLE approach can uniformly perform
well on several tasks across four datasets. Even though this addresses some of the concerns about
choosing the correct likelihood for a dataset, some limitations still remain for example concerns about
the non-convexity of the log-likelihood. We provide a more in-depth discussion in Appendix J.
9
Published as a conference paper at ICLR 2022
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In 12th USENIX symposium on operating systems design and implementation
(OSD116),pp. 265-283, 2016.
Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified maximum
likelihood approach for estimating symmetric properties of discrete distributions. In International
Conference on Machine Learning, pp. 11-21. PMLR, 2017.
Jean-Yves Audibert, Olivier Catoni, et al. Robust linear least squares regression. The Annals of
Statistics, 39(5):2766-2794, 2011.
Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108-127. World Scientific, 2008.
Derek Bean, Peter J Bickel, Noureddine El Karoui, and Bin Yu. Optimal m-estimation in high-
dimensional regression. Proceedings of the National Academy of Sciences, 110(36):14563-14568,
2013.
Bicycle. Bicycle share dataset. https://www.kaggle.com/contactprad/
bike-share-daily-data/, 2017.
Christian Brownlees, Emilien Joly, and Ggbor Lugosi. Empirical risk minimization for heavy-tailed
losses. The Annals of Statistics, 43(6), Dec 2015. ISSN 0090-5364. doi: 10.1214/15-aos1350.
URL http://dx.doi.org/10.1214/15-AOS1350.
Yunshun Chen, Aaron TL Lun, and Gordon K Smyth. From reads to genes to pathways: differential
expression analysis of rna-seq experiments using rsubread and the edger quasi-likelihood pipeline.
F1000Research, 5, 2016.
Richard A Davis and Rongning Wu. A negative binomial model for time series of counts. Biometrika,
96(3):735-749, 2009.
Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. arXiv
preprint arXiv:1711.10604, 2017.
David Donoho and Andrea Montanari. High dimensional robust m-estimation: Asymptotic variance
via approximate message passing. Probability Theory and Related Fields, 166(3):935-969, 2016.
Lutz Dumbgen and Kaspar Rufibach. Maximum likelihood estimation of a log-concave density and
its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):40-68, 2009.
Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators. Probability Theory and Related Fields,
170(1):95-175, 2018.
Ludwig Fahrmeir and Heinz Kaufmann. Consistency and asymptotic normality of the maximum
likelihood estimator in generalized linear models. The Annals of Statistics, 13(1):342-368, 1985.
Favorita.	Favorita forecasting dataset.	https://www.kaggle.com/c/
favorita-grocery-sales-forecasting/, 2017.
Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction,
allocation, and triangular discrimination. arXiv preprint arXiv:2107.02237, 2021.
Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas,
Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile function
rnns. In The 22nd international conference on artificial intelligence and statistics, pp. 1901-1910.
PMLR, 2019.
Sara A Geer and Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge
university press, 2000.
10
Published as a conference paper at ICLR 2022
Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical
Association,106(494):746-762, 2011.
Arthur Stanley Goldberger et al. Econometric theory. Econometric theory., 1964.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Patrick J Heagerty and Brenda F Kurland. Misspecified maximum likelihood estimates and gener-
alised linear mixed models. Biometrika, 88(4):973-985, 2001.
C.C. Heyde. On an optimal asymptotic property of the maximum likelihood estimator of
a parameter from a stochastic process. Stochastic Processes and their Applications, 8(1):
1-9, 1978. ISSN 0304-4149. URL https://www.sciencedirect.com/science/
article/pii/0304414978900649.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Daniel Hsu and Sivan Sabato. Loss minimization and Parameter estimation with heavy tails. Journal
of Machine Learning Research, 17(18):1-40, 2016. URL http://jmlr.org/papers/v17/
14-273.html.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, PP. 9-1. JMLR WorkshoP and Conference Proceedings, 2012.
Peter J Huber. Robust estimation of a location Parameter. In Breakthroughs in statistics, PP. 492-518.
SPringer, 1992.
Rob John Hyndman and Ann B Koehler. Another look at measures of forecast accuracy. International
Journal of Forecasting, 22(4):679-688, 2006.
Heysem Kaya, PINAR TUFEKCL and Erdinc Uzun. Predicting co and no X emissions from gas
turbines: novel data and a benchmark Pems. Turkish Journal of Electrical Engineering & Computer
Sciences, 27(6):4783-4796, 2019.
Bernhard Klar. Bounds on tail Probabilities of discrete distributions. Probability in the Engineering
and Informational Sciences, 14(2):161-171, 2000.
Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Economet-
ric Society, PP. 33-50, 1978.
StePhane Lathuiliere, Pablo Mesejo, Xavier Alameda-Pineda, and Radu Horaud. A comprehensive
analysis of deeP regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42
(9):2065-2081, 2020. doi: 10.1109/TPAMI.2019.2910523.
Jerald F Lawless. Negative binomial and mixed Poisson regression. The Canadian Journal of
Statistics/La Revue Canadienne de Statistique, PP. 209-225, 1987.
G. Lecue and S. Mendelson. Learning subgaussian classes: UPPer and minimax bounds. In S.
Boucheron and N. Vayatis, editors, Topics in Learning Theory. Societe Mathematique de France,
2016.
Erich L Lehmann and George Casella. Theory of point estimation. SPringer Science & Business
Media, 2006.
Ggbor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distribu-
tions: A survey. Foundations of Computational Mathematics, 19(5):1145-1190, 2019a.
Ggbor Lugosi and Shahar Mendelson. Sub-gaussian estimators of the mean of a random vector. The
annals of statistics, 47(2):783-794, 2019b.
M5.	M5 forecasting dataset.	https://www.kaggle.com/c/
m5-forecasting-accuracy/, 2020.
11
Published as a conference paper at ICLR 2022
Davis J McCarthy, Yunshun Chen, and Gordon K Smyth. Differential expression analysis of
multifactor rna-seq experiments with respect to biological variation. Nucleic acids research, 40
(10):4288-4297, 2012.
Peter McCullagh and John A Nelder. Generalized linear models. Routledge, 2019.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the
Royal Statistical Society: Series A (General), 135(3):370-384, 1972.
Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis
expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437,
2019.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estima-
tion via robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 82(3):601-627, 2020.
C.R. Rao. Criteria of estimation in large samples. Sankhya, 25, SerA, 1963.
Richard Redner. Note on the consistency of the maximum likelihood estimate for nonidentifiable
distributions. The Annals of Statistics, pp. 225-228, 1981.
Philippe Rigollet. High-dimensional statistics. https://ocw.mit.edu/courses/
mathematics/18-s997-high-dimensional-statistics-spring-2015/
lecture-notes/MIT18_S997S15_Chapter2.pdf, 2015.
Alesandro Rinaldo. Sub-exponential concentration. http://www.stat.cmu.edu/
~arinaldo/Teaching/36709/S19/Scribed_Lectures/Feb5_Aleksandr.pdf,
2019.
Mark D Robinson and Gordon K Smyth. Small-sample estimation of negative binomial dispersion,
with applications to sage data. Biostatistics, 9(2):321-332, 2008.
David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic
forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):
1181-1191, 2020.
Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: A deep neural network
approach to high-dimensional time series forecasting. arXiv preprint arXiv:1905.03806, 2019.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995,
2009.
Pragya Sur and Emmanuel J Candes. A modern maximum-likelihood theory for high-dimensional
logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516-14525,
2019.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
arXiv preprint arXiv:1409.3215, 2014.
Abraham Wald. Note on the consistency of the maximum likelihood estimate. The Annals of
Mathematical Statistics, 20(4):595-601, 1949.
Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.
Deep factors for forecasting. In International Conference on Machine Learning, pp. 6607-6617.
PMLR, 2019.
Ruofeng Wen Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile
recurrent forecaster. In NIPS Time Series Workshop, 2017.
12
Published as a conference paper at ICLR 2022
Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting
the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
753-763, 2020.
Tong Zhang. From epsilon-entropy to kl-entropy: Analysis of minimum information complexity
density estimation. TheAnnalsofStatistics, 34(5):2180-2210, 2006.
Ziwei Zhu and Wenjing Zhou. Taming heavy-tailed features by shrinkage. In International Conference
OnArtificial Intelligence and Statistics, pp. 3268-3276. PMLR, 2021.
13
Published as a conference paper at ICLR 2022
A Proof for competitivenes s of MLE
Theorem 3. Let π be an estimator for a property π, such thatfor any f ∈ F and Z 〜 f, Pr(kπ(f) -
π(z)k ≥ e) ≤ δ. Then the MLE based estimator satisfies the following bound,
Pr(kπ(f)-π(fz)k ≥2) ≤ (|F| + 1)δ.
Proof of Theorem 3. By triangle inequality and by the properties,
k∏(f) - ∏(fz)k ≤ k∏(f) - ∏(z)k + k∏(z) - ∏(fz)k.
Hence,
1k∏(f )-∏(fz)k≥2e ≤ 1k∏(f )-∏(z)k≥e + 1k∏(z)-∏(fz )k≥e∙
We now take expectation of both LHS and RHS of the above equation with respect to the distribution
f. For the LHS, observe that
E[1kπ(f)-π(fz)k≥2] =Pr(kπ(f)-π(fz)k ≥ 2e).
For the first term in the RHS
E[1k∏(f)-∏(z)k≥J ≤ Pr(k∏(f) — ∏(z)k ≥ e) ≤ δ,
by the assumption in the theorem. Combining the above three equations yield that
Pr(k∏(f) - π(fz)k ≥ 2e) ≤ E[1k∏(z)-∏f 心]+ δ.
WenOWPrOVethat E[1k∏(z)-∏(fz)k≥e] ≤ |F|S.
E[1kπ(z)-π(fz)k≥e] = / f(Z)Ikn(Z)-n(fz)k≥e
z
(a)
≤ / fz(Z)Ikn(z)-n(fz)k≥e
z
≤ / Ef(Z)Ikn(z)-n(f)k≥e
zf
-XLf(Z)Ikn(z)-n(f)k≥e
=X Pr(k∏(Z) - π(f)k≥e)
f z~f
≤ Xδ = ∣F∣δ,
f
Where (a) uses the fact that fz is the MLE estimate Over set F and hence fz(Z) ≥ f(Z). (b) fOllOWs
frOm Fubini’s theOrem.
□
NOW We PrOve an extensiOn Of Our last theOrem that can deal With infinite likelihOOd families.
Proof of Theorem 1. Let F be the cOver Of Ff that satisfies assumPtiOns in DefinitiOn 1. By triangle
inequality and by the PrOPerties Of F, With PrObability at least 1 - δ2 - δ,
~ ~
k∏(f) - ∏(fz)k ≤ k∏(f) - ∏(Z)k + k∏(Z) - ∏(fz)k + k∏(fz) - ∏(fz)k
~
≤k∏(f) - π(Z)k + k∏(Z)-∏(fz)k + e
We have used the fact that fz ∈ Ff W.P at least 1 - δ. Hence With PrObability at least 1 - δ2 - δ,
1kn(f )-n(fz)k≥3e ≤ 1kn(f )-n(z)k≥e + 1k∏(z)-n(fz )k≥e'
14
Published as a conference paper at ICLR 2022
We now take expectation of both LHS and RHS of the above equation with respect to the distribution
f . For the LHS, observe that
E[1kπ(f)-π(fz)k≥3] = Pr(kπ(f) - π(fz)k ≥ 3).
For the first term in the RHS
E[1k∏(f)-∏(z)k≥J ≤ Pr(k∏(f) - ∏(z)k ≥ e) ≤ δ,
by the assumption in the theorem. Combining the above three equations yield that
Pr(IIπ(f) - ∏(fz)k ≥ 3e) ≤ 旧[1博3—冗田)卷』+ 2δ + δ?.
We now prove that 旧[1Μ(Ν)_冗(&*≥/ ≤ Tδ + δι. Let f be the distribution m Ff that is at most δι
away from f . Then,
E[lkn(z)—π(fz )k≥e] = / *
Z
(a)
≤Z
(b)
≤Z
(c)
≤Z
f (z)1kπ(z)-π(fz )k≥e
f(z)1k∏(z)-∏(fz )k≥e + kf - fkl
f (z)1k∏(z)-π(fz)k≥e + δ1
W ∕~∖1	~	I X
fz (z)1k∏(z)-π(fz )k≥e + δ1
≤ / X f(z)1k∏(z)-∏(fz )k≥e + δ1
jz∈Ff
(=)X Zf(z)1k^(z)-∏(f)k≥e + δ1
f∈F JZ
=X Pr.(k∏(f)- ∏(z)k ≥ e)+ δι
-- Z〜f
f∈F
≤ (X δ) + δ1 = Tδ + δ1,
~ ~
f∈F
where the (a) follows from the definition of `1 distance between distributions, (b) follows from
the properties of the cover, (c) uses the fact that fZ is the MLE estimate over set F and hence
fWZ(z) ≥ fW(z). (d) follows from Fubini’s Theorem.
□
B Other general results for the MLE
In this section, we prove that the log likelihood of the MLE distribution at the observed data point is
is close to the log-likelihood of the ground truth at the data point, upto an additive factor that depends
on the Shtarkov sum of the family.
Lemma 2. Let Z 〜f. The maximum likelihood estimator satisfies the following inequality with
probability at least 1 - δ,
log f (z) ≤ log fz(z) ≤ log f (z) + log (X fz(z)) + log 1 ,
where	Z∈Z fZ(z) is the Shtarkov sum of the family F. Furthermore, if F is finite, then
logf (z) ≤ logfz(z) ≤ log f (z) + log |F| + logδ,
Further is the distribution family is finite, then
logf (z) ≤ logfz(z) ≤ logf(z)+log∣F∣ +log1 ∙
δ
15
Published as a conference paper at ICLR 2022
Proof. Let K = 1 Pz∈Z fz(Z).
PMf(Z) ≤ fz(Z)/K) = Pr( f ≥ 1
H Kfi
=K X fz(Z)= δ,
z∈Z
where (a) follows from Markov’s inequality. The first part of the lemma follows by taking the
logarithm and substituting the value of K . For the second part, observe that if F is finite then,
fz(Z)≤	f(Z) =	f(Z) =	1=|F|.
z∈Z	z∈Z f∈F	f∈F z∈Z	f∈F
□
We now prove a result of Shtarkov’s sum, which will be useful in other scenarios.
Lemma 3. Let w = π(Z) for some property π.
Xfw(w) ≤Xfz(Z).
Proof.
Xfz(Z) ≥ X pg(z)(Z) = X X fw(Z) = Xfw(w).
z	z	w z:g(z)=w	w
□
C General Fixed Design Result
In this section, we will prove Theorem 2 which is an application of Theorem 1 to the fixed design
regression setting with the square loss as the target metric. The theorem holds under some reasonable
assumptions. We will fist prove some intermediate lemmas.
Lemma 4. Let f (∙; θ) = ∩n=1 p(∙∣Xi; θ) in the fixed design setting. If ∣∣θ 一。1卜 ≤ δ then,
kf(.； θ) - f(∙; θ0)∣ι ≤√2nLδ.
under the assumptions of Theorem 2.
Proof. By Pinskers’ inequality we have the following chain,
kf(∙; θ) - f(∙; θ)∣1 ≤ P2DκL(f(∙; θ); f(∙; θ))
≤
n
2 X DκL(p(∙∣Xi; θ);p(∙∣Xi; θ0))
i=1
□
Lemma 5. Assume the ConditionS of Theorem 2 holds. Let θmɪθ := argmaXθ∈N 但㊀)L(y , θ), in
the Poisson regression setting. Then with probability at least 1 - δ we have,
|E(θmɪe) — E(θmle)∣ ≤ 4Dl5
n
16
Published as a conference paper at ICLR 2022
Proof. We assume that the event in (2) of Theorem 2 holds.
∣L(yn;θmie)-L(yn; θ°)∣ ≤ 2言,
where θc is the closest point in N(e, Θ) to θmie. Therefore, by definition
L(yn; θmle) ≤L(yn; θmle) + (β∕2)12.
By virtue of the strong-convexity we have,
— θmlel∣2 ≤ β≡2.
2α
Now by triangle inequality we have,
WE(θmle) - JE(θmle)∣ = |^mle — θ*1 — ^mle — θ[j
≤ ∣∣θmle — θmle ∣∣	≤
λmax3V1
α
□
Now We can use the fact |a - b| ≤ 2 max(√α, √b) |√ɑ 一 √6∣ and that Θ ⊆ Bw to conclude,
IE(θmle) — E(θmle)1 ≤ 4wλmax0Nl∕~∙
α
Proof of Theorem 2. Consider the netN(φ, θ). If φ = δ2∕(2nL), then by Lemma 4 the net forms a
δ cover on F. Note that under the conditions of Theorem 2 IN (φ, θ)I ≤ (3w∕φ)d.
Next note that in the application of Theorem 1, we should set
_ (ci + C2 log(1∕δ))η
e —	.
n
Thus if we set φ in place of e in Lemma 5 we would need the following to apply Theorem 1,
4wλmaχ(∑)φJβ — (CI + Cc log(1∕δ))η .
αn
Thus, φ can be such that,
log φ ≤ max(log(2nL), 0.5log β + log(4wλmaχ(Σ)) + log(n)).
From above we can apply Theorem 1 with the above e, δ1 — δ2 — δ and T — (3w∕φ)d. Therefore,
we get
P(E(θmie) ≥ 3(CI + c2lng(1∕δ))η) ≤ (T + 5)δ.	(5)
We can set δ — δ∕(T + 5) to then conclude that w.p at least 1 — δ,
n
E(θ ɪ)= 0 ((ci + c2d(log(w) + log(1∕φ)))η
Substituting the bound on log φ yields the result.
□
17
Published as a conference paper at ICLR 2022
D Least Squares for Poisson
We begin by restating the following general statement about OLS.
Lemma 6 (see Theorem 2.2 in (Rigollet, 2015)). Suppose r = rank(XT X) and φ ∈ Rn×r be an
orthonormal basis for the column-span of X. Then we have the following result,
E(θis) ≤ 4 sup (UTZ)2,
n u∈Sr1-1
(6)
where W = φτe. Here, E = yn - Xθ* for the given response sample yn.
Now are at a position to prove Lemma 1.
Proof of Lemma 1. We are interested in tail bounds on the RHS of (6). We will first analyze tail
bounds for a fixed u ∈ Sr1-1. We have the following chain,
E[exp{shu, Wi}] = E[exp{shφu, Ei}] := E[exp{shv, Ei}]
nn
=∩E[exp{sviei}] = "E[exp(svi(yi - μi))]
i=1	i=1
where μ% = (θ*, x/. Now bounding each term separately We have,
E[exp(svi(yi - μi))] =exp(-sv.μi)exp(μi(exp(svj - 1))
We have,
nn
Rexp(μi(exp(svi) - 1)) = expl fμi(exp(svi) - 1)
i=1	i=1
n
∞
n∞
X(X j (sVi)j
≤ exp E μiSVi + E μmax
i=1
j=2
,(svi)j
≤ exp (Xμisvi + μmax(exp(s) - S - 1)),
(7)
where we have used the fact that ||v||p ≤ ||v ||2 = 1 for all p ≥ 2. Thus we have,
E[exp{s<u, W)}] ≤ exp(μmaχ(exp(s) - S - 1)).
This means that the RV hu, W) is sub-exponential with parameter ν2 = 2μmaχ and α = 0.56. Thus if
t ≤ 4μmaχ then we have that for a fixed u,
P(hu,w)} ≥ t) ≤ exp(-4-t—).	(8)
∖	4μmax )
Thus by a union bound over the -net with = 2, we have that wp 1 - δ,
sup(u, Wi ≤ √8μmaχ (log(1∕δ) + r log 6).
u
Thus we get the risk bound wp 1 - δ,
E(θis) ≤ 32μmax (log(1∕δ) + r log6).	(9)
n
For the sub-exponential region when t ≥ μmaχ we get the bound,
E (θls) ≤ 16max((log(1∕δ)+ r log6)2,μmax
We get the bound by setting r = d.
□
18
Published as a conference paper at ICLR 2022
E Competitiveness for Poisson Regression
Lemma 7. Let f (∙; θ) = Qn= 1 p(∙∣Xi; θ) where P is defined in Eq. (2). If ∣∣θ 一 θ0k2 ≤ δ then,
kf(∙;θ)-f(∙;θ0)kι ≤ R∖隹W
Proof. By Pinskers’ inequality we have the following chain,
kf(∙; θ) - f (∙; θ0)kι ≤ P2DκL(f(∙; θ); f(∙; θ))
≤t
≤t
n
2 X DκL(p(∙∣Xi; θ);p(∙∣Xi; θ0))
i=1
n
2 X(hθ, xii(log(hθ, xii) 一 log(hθ0, xii)) 一 (hθ,xii 一 hθ0, xii))
i=1
Notice that the absolute value of the derivative of a log X — X Wrt x, is upper bounded by a∕γ — 1 if
x ≥ γ > 0 and also a ≥ γ . Therefore, following from above we have,
12nwR2 ll λ	八 U
≤ {-γ~kθ - θ0∣2,
thus concluding the proof.
□
NoW We prove high probability bounds on the strong convexity and smoothness of the likelihood in
the context of Poisson regression.
Lemma 8. Let X be the condition number of the matrix M, where Ui = Xi∕∣Xi∣2. Then with
probability at least 1 一 2δ, we have
λmin(VθL(yn; θ)) ≥	n-2 λmin(M),	(10)
2∣θ∣
provided nλmin(M) ≥ 卷(dlog(24χ) + log(1∕δ)) and Pλmaχ(M)(dlog(24χ) + log(1∕δ)) ≤
√nλmin(M )/16.
Proof. We start With the expression of the Hessian of L(yn ; θ),
n
V* 2θ L = X
i=1
T
yiXixT
hθ Xii
nT
< ^X yiuiui
LM lθF
L(θ),
where Ui = Xi∕∣Xiki.
For a fixed unit vector u ∈ S1d-1 let us define,
n TT	n
l(u, θ):= X yiu* ：= Xy,g,
i=1	∣ ∣	i=1
where Vi =〈ui, u〉2/k0『.
19
Published as a conference paper at ICLR 2022
Following the definition of sub-exponential RV in (Rinaldo, 2019),
X yivi ∈ SE
i
where Vi = 2μi *. This implies that wp atleast 1 - 2δ∕C We have,
ν2 =	νi2, α = 0.56 max vi
(11)
|	yivi
i
-Xμivi∖ ≤ 2 jχμiv2l°g((C
if,
jχμ心 log(C) ≤ 2(Xμiv2)/(maxVi).
Note that the above condition is mild and is satisfied when λmin (Pi μiUiuf) ≥ log(C∕δ)∕4γ2,as
minu Pi μiVi ≥ λmin (Pi μiUUT). Here, C is a problem dependent constant that will be chosen
later.
Now consider an -net in `2 norm over the surface unit sphere denoted by N(, d). It is well known
that ∖N (, d)∖ ≤ (3∕)d. Now any z ∈ S1d-1 can be written as z = x + u where x ∈ N (, d) and
u ∈ Bd-1. Therefore, we have that
L(z, θ) ≥ L(x, θ) - λmax(L(θ))
A similar argument as above gives us,
λmaχ(L(θ)) = max L(u, θ) ≤ max L(x, θ) + ɪ max L(u, θ)
u∈B1d-1	x∈N (,d)	2 u∈B1d-1
=⇒ λmax(L(θ)) ≤ 2 max L(x, θ)
x∈N (,d)
Thus by an union bound over the net we have wp 1 - 2δ and other conditions,
zmdnι L(z, θ) ≥kθk2 λmin
i
2(1 + 2e)tE ”
而―log
max
u∈S1d-1
—
if the conditions above hold with C = (3∕e)d. We can now set e = 1/(8 * χ) where X is the condition
number of the matrix Pi μiUiuT. Now by virtue of that fact that ,λmaχ(P μiUUT)log(C∕δ) ≤
λmin (Pi μiUiUT)/16 we have the result.	□
Now we will prove a result on the smoothness of the negative log likelihood for the Poisson regression
setting.
Lemma 9. With probability at least 1 - 2δ, we have
2nR2
λmaχ(Vθ L(yn; θ)) ≤	λmaχ(M),	(12)
ifλmin(M) ≥ R2(dlog 6 + log(1∕δ))∕4nγ2.
Proof. We start by writing out the Hessian again but this time upper bounding it in semi-definite
ordering,
n
V2θ L = X
i=1
T
yiXiχT
hθ, Xii
nT
/ yixixi
4 2 - Y2-
i=1 γ
n
4X
i=1
yiR2UiUiT
γ2
L(θ)
20
Published as a conference paper at ICLR 2022
For a fixed unit vector u ∈ S1d-1 let us define,
L(u, θ) := XX yiR2uTuiuTU := XX y,v,,
i=1	γ	i=1
where Vi = R hui, u>2∕γ2. Proceeding as in Lemma 8 We conclude that,
X yivi ∈ SE ν2 = X νi2 , α = 0.56 max vi
where Vi = 2μ% v2. This implies that wp atleast 1 - 2δ∕C we have,
(13)
|	yivi
i
-Xμivi∖ ≤ 2 jXμiv2l°g((C
if,
jXμiv2 log(C) ≤ 2(Xμivi)/(maxvi).
Note that the above condition is mild and is satisfied when λmin (Pi μiuiUT) ≥ R2 log(C∕δ)∕4γ2,
as min。Pi μ%Vi ≥ λmin (Pi μiuiuT). Here, C is a problem dependent constant that will be chosen
later.
Now consider an 1∕2-net in `2 norm over the surface unit sphere denoted by N (0.5, d). It is well
known that ∖N (, d)∖ ≤ (3∕)d. Now any z ∈ S1d-1 can be written as z = x+u where x ∈ N (0.5, d)
and u ∈ Bd0.-51. Therefore, we have that,
λmax (L(θ)) = max L(u, θ)
u∈B1d-1
≤ max L(x, θ) + - max L(u, θ)
x∈N (,d)	2 u∈B1d-1
=⇒ λmax(L(θ))
≤ 2 x∈mNa(x,d) L(x, θ)
Thus we set C = 6d and obtain wp at least 1 - 2δ,
λmax(L(O)) ≤ 2 -2λmax(M),
γ2
provided 4√λmaχ(M)lθg(C∕δ) ≤ λmaχ(M)∕√n.
□
Proof of Corollary 1. We show that the conditions of Theorem 2 hold.
Creating Nets: First we need to create an -net over the parameter space Θ. We start by creating an
-net over the sphere with radius w. Now suppose, < γ∕2R. Then we remove all centers θc if ∃i
s.t hθc, xii< γ∕2. This is a valid -net over Θ as all net partitions that are removed do not have any
points lying in Θ. In subsequent section, we will always follow this strategy to create -nets over
subsets of Θ.
Strong Convexity and Smoothness: From Lemma 8 and 9 we have that,
β	w2R2
=	2~χ.
α γ2
with probability 1 - O(δ).
KL Divergence: The L in Theorem 2 is bounded by 2wR2∕γ according to Lemma 7.
Combining the above into Theorem 2 and using the estimator in Section F we get our result.
□
21
Published as a conference paper at ICLR 2022
F Median of Means Estimator for Poisson
In this section we will design a median of means estimator for the Poisson regression model based
on the estimator proposed in the work of Lugosi & Mendelson (2019b). Recall that we have a fixed
design matrix X ∈ Rn×d with rows x1, . . . , xn, and for each i ∈ [n], yi is drawn from a Poisson
distribution with mean μi = θ* ∙ Xi. We will further assume that the design matrix is chosen from an
(L, 4) hyper-contractive distribution. Mathematically this implies that for any unit vector u
E[(∑- 1 x ∙ u)4] ≤ L ∙ E[(∑-1 x ∙ u)2]2.
For simplicity we will assume that L = O(1). In the above definition note that E is the empirical
expectation over teh fixed design. This is a benign assumption and for instance would be satisfied if
the design matrix is drawn from a sub-Gaussian distribution. In the general case, the obtained bounds
will scale with L. We have the following guarantee associated with Algorithm 1.
Algorithm 1: Median of Means Estimator
Input: Samples S = {(x1, y1), . . . , (xn, yn)}, confidence parameter δ.
Step 1: Compute Σ = E[xx>]. Form S0 = {x01 = y1Σ- 1 x1,..., Xn = yn∑--1 Xn}.
Step 2: Randomly partition S0 into k blocks of size n/k each where k = 20「log( 1)].
Step 3: Feed in the k blocks to the median-of-means estimator of Lugosi & Mendelson (2019b)
to get v.
1
Step 4: Return θ = Σ- 1 v.
Theorem 4.	There is an absolute constant c > 0 such that with probability at least 1 - δ, Algorithm 1
outputs θ such that
kθ - θ*kΣ ≤ C ∙ kθ*k2 ∙ λ max (Σ)( d + lθg(1 ) ).	(14)
n
Proof. The proof is exactly along the lines of the proof of Theorem 1 in the work of (Lugosi &
Mendelson, 2019b) that we repeat here for the sake of completeness since the original theorem is not
explicitly stated for a fixed design setting. To begin with notice that
E[yΣ-1 x] = E[(θ* ∙ x)Σ-1 x)]
=E[(θ* ∙ x)Σ-2x]
=Σ 2 θ*.
(15)
(16)
(17)
Hence if V is the output of the median of the means estimator in Step 3 of Algorithm 1, then
11
the least squares error of θ is exactly ∣∣v 一 Σ 1 θ*k2. For convenience define μ = Σ 1 θ* and
Σ0 = E[x0x0>] 一 Σ 1 θ*θ*>Σ 1. Exactly as in Lugosi & Mendelson (2019b) our goal is to show that
μ0 beats any other vector V in the median of means tournament if V is far away from μ0. To quantify
this define	___ ______
r = max(400 J Tr(∑), 4√1θJ "»)1).
For a fixed vector V of length r, and block Bj, μ0 beats V if
一2k X (xi 一 μ0)
n
i∈Bj
V + r > 0.
Let us denote by σi,j ∈ {0, 1} a random variable representing whether data point i is in block j or
not. By Chebychev’s inequality we get that with probability at least 9/10,
Ik X(Xi - μ0) ∙ v∣ ≤ n√1°∣XEσj((Xi — "0) ∙v)2]
i∈Bj	i
=k√101 Inp1 X E[((xi 一 μ0) ∙ v)2].
nn
(18)
(19)
22
Published as a conference paper at ICLR 2022
Here P is the probability of a point belonging to block k. Noting that np = θ(n/k) We get that with
probability at least 9/10,
In X(Xi- μ0) ∙ v∣ ≤ √0rempɪɪ.	QO)
i∈Bj
Applying binomial tail estimates we get that with probability at least 1 一 e-k/180, μ0 beats V on at
least 8/10 of the blocks. By applying the covering argument verbatim as in the proof of Theorem 1
in Lugosi & Mendelson (2019b) we get that with probability at least 1 一 δ, V will satisfy
kv 一 ∑ 2 θ*k2 ≤ C ∙ λmax(∑0)( d +lng(1) ).
Finally, it remains to bound the spectrum of ∑0 . We have
Σ0 = E[y2∑-1 xx>∑-2] 一 Σ2θ*θ*>∑1	(21)
W E[((θ ∙ x)2 + θ ∙ x)∑-1 xx>∑-1 ]	(22)
T1 + T2 ,	(23)
where
T1 = E[(θ* ∙ x)2∑-2xx>∑-2],	(24)
T2 = E[(θ* ∙ x)∑-1 xx>∑-1 ].	(25)
To bound T1 , T2 we note that for any function m(x) we have
E[m(x)∑-1 xx>∑-1 ] W pE[m2(x)]I.	(26)
Using the above inequality and the fact that the design matrix is (O(1), 4) hyper-contractive we get
that
me* ∙ x)2] = o(∣∣θ* k √χmax(∑))	(27)
PE[(θ* ∙ X)4] = O(kθ*k2λmax(∑)).	(28)
Combining the above we get that
∑0 W T1 + T2	(29)
W O(λmax(∑))I.	(30)
□
G 1-D Poisson Regression
When the covariates are one dimensional, a sharper analysis can actually be performed to show that
the MLE dominates TMO in all regimes in the Poisson regression setting considered above.
Lemma 10. There exists an absolute ConStant C ≥ 2 such that for any δ ∈ [ nC, 1), it holds with
probability at least 1 一 δ that,
1n
.^	1 X-ʌ .	^	. r,
E(θls) = — y^(yi 一 θls) ≤
n i=1
4 ∙∣θ*l Pi=ι∣Xil
---. 	-
n--Pi=ι x2
(31)
B W1).
The bound above is also tight i.e with constant probability it holds that
E (θis) = 1 X 3 — θis)2 = Ω(妇
nn
i=1
Pi=I IXiI3 ʌ
X------V------}
■ ʌ
= B(θis)
(32)
We provide the proofs in later in the section. Having established the bound for least squares estimator,
we next prove the following upper bound on the mean squared error achieved by the MLE.
23
Published as a conference paper at ICLR 2022
Theorem 5.	There exists an absolute constant C ≥ 2 such that for any δ ∈ [ n1c, 1) ,it holds with
probability at least 1 - δ that,
E(θmle) = - XT3 — θmle)2 ≤ 4^ ∙ ( Ps=^ >°g ([
n 仁	n	i=ι Ixilj V
S----{-----}
,.ʌ .
：=B(8mle)
(33)
It is easy to see that the covariate dependent term in the bound on the mean squared error achieved by
θmle (defined in Eq. 32) is always better the corresponding term in the bound achieved by θls (defined
in Eq. 33). To see this notice that,
B(θis)	(P= Il)(Pn=ι Mi)
.^ .
B(θmle)
(P 乙 X )2
≥ 1. (from Cauchy-Schwarz inequality.)
Furthermore, in many cases the bound achieved by θmle can be significantly better than the one
achieved by θis. As an example consider a skewed data distribution where √n of the Xi S take a large
value of √n, while the remaining data points take a value of ne, where e is a small constant. In this
case we have that,
.^ .
B(θis)
.^ .
B(θmle)
(P乙 W)(P乙 Xi)
-(P 乙 W)2-
Ω(ne).
Proof of Lemma 10. Notice that
1n
E (θis) = n(θis-θ* )2(E X2).	(34)
i=1
ɪ ɪ	∙ , ∙	1 , i	1 ,ι	,	1 ∙ ,	∙	/% n⅛ ∖ 9 ɪ ι , ι ,ι , c .
Hence, it is enough to bound the parameter distance, i.e., (% - θ*)2. In order to do that we first
notice that yiXi is a sub-exponential random variable with parameters V2,a where where V = 2μiX2,
μi = θ*Xi, and α% = 0.56Xi (Rinaldo, 2019). In other words,
yiXi ∈ SE(νi2, α),	(35)
Thus from the bound on a sum of independent sub-exponential variables we have that,
yiXi ∈ SE(ν2 =	νi2, α = 0.56 maxXi).
(36)
Thus we have that,
PIi
2exp (- 2tV2
2exΡ(-2α)
if t ≤ ν2
α
otherwise
This means, that w.p at least 1 - 2δ,
I EyiXi - J^μiXi∣ ≤ 2
if
J(X μiX2)log(-) ≤	2P μ*
i δ	0.56 maxi Xi
The condition above is satisfied under our assumptions on Xi and δ, thereby leading to the bound
(θis - θ*) ≤ 2√wlog(1∕δ)
√PX3
PXT.
i
i
The bound on the MSE claimed in the lemma then follows.
24
Published as a conference paper at ICLR 2022
Now We prove the lower bound. Again it is enough to show a lower bound on |% - θ*∣. Notice that
θls - θ*
Pn=Kyi - μi)xi
pn=1 x2
(37)
Define the random variable Z = Pn=∖(y% - μi)x〃 We will show anti-concentration for Z by
computing the hyper-contractivity of the random variable. Recall that a random varibale Z is η-HC
(hyper-contractive) if E[Z4] ≤ η4E[Z2]2 . Next we have
nn
E[Z2] = E[X(yi - μi)xi]2 = X μiχ2.	(38)
n
E[Z4] = E[X(yi - μi)xi]2
i=1
n
=)：μi (I + 3μi )xi + 2): μiμj Xi Xj
i=1	i6=j
:= ∆.	(39)
Pn=1 "Y
Hence Z is η-HC with η4 = ʌ----δ----J.
From anti-concentration of hyper-contractive random variables (O’Donnell, 2014) we have
P(∣Z∣≥ 2PEZi) ≥ Ω(η4).
(40)
Hence we get that
P(IX(yi-μi)χil ≥ 2utX
≥ Ω(η4)
> (P乙 ”,χ"
(41)
(42)
∆
Next notice that since μi ≥ Y for all i (Assumption 1), we have that 1 + 3μi ≤ (3 + Y)μi. This
implies that ∆ ≤ (3 + Y)( Pi=ι μiX2) . Hence if Y is a constant then with probability at least
ιγτ = Ω ⑴ we have
3+γ
E(…(n ∙—
2
Xi2
(43)
□
T-> C OEt	Lc 11,1	, A ∙ FC F
Proof of Theorem 5. Recall that θmle is defined as
θmie = argmin	θxi - yi log(θXi).	(44)
θ∈Θ i=1
Setting the gradient of the objective to zero, we get the following closed form expression for θmle .
θmle
n
i=j=l yi
n∖n	.
i=1 Xi
(45)
□
25
Published as a conference paper at ICLR 2022
Next, We note that Z = Pn=ι yi is a poission random random variable with parameter μ =
pn=ι θ*Xi. From tail bounds for Poisson random variables (Klar, 2000) We have that for any e > 0,
2
P[|Z - μ∣ > e] ≤ 2e-不.	(46)
Taking e = c√μ log( 1), we get that with probability at least 1 - δ,
| X yi - μl ∈ (2,2) {μ log(δ ),	(47)
i=1
provided that log( 1) < μ (that holds for our choice δ, once n is large enough). Hence we conclude
that with probability at least 1 - δ, the mean squared error of θmle is bounded by
l^ n*, ∣Pn=1 θ*Xi	Pn=I yi∣
lθmle - θ | = I-Pn-Pn----∣
i=1 xi	i=1 xi
=o(q^)
、Ei=I Xi j
=O(Jlθ*1 詈 δδ))
i PPn=I Xi J
The bound on the mean squared error follows from the above.
(48)
H Competitiveness for Pareto Regression
We verify the conditions of Theorem 2 for the Pareto regression setting.
Lemma 11. Let f(∙; θ) = Qn= ι p(∙∣Xi; θ) where P is defined in Eq. (4). If ∣∣θ —。[b ≤ δ then,
kf(∙; θ) - f(∙; θ0)kι ≤ S2nR
Proof. By Pinskers’ inequality we have the following chain,
kf(∙; θ) - f(∙; θ0)kι ≤ d2Dkl(∕(∙; θ); f(∙; θ0))
n
∖
2f DκL(p(∙∣xn; θ); p(∙∣xn; θ0))
i=1
un
≤ t	2b| log mi - logm0i|
≤ X 2b∣hθ, Xii-hθ0, Xiil
~∖h Y
≤ s2bn∣θ - θ0k2R
The second inequality follows from the fact that log X is Lipschitz with parameter L if X > 1/L. The
last inequality follows from Cauchy-Schwarz and the norm bound on x∕s.	□
Now we prove the rest of the conditions.
Lemma 12. We have the following smoothness bound,
λmax(VθL) ≤
(b-1)
Y 2
λmax (Σ).
26
Published as a conference paper at ICLR 2022
Proof. We start by writing out the Hessian,
VθL = X -(b_1)2XixT := L(B)
i=1 hθ, xii
Using the fact that hθ, xii ≥ γ gives us the result.
Lemma 13. We have the following strong convexity bound,
λmin(VθL) ≥ (b2--2)λmin(Σ).
w2R2
□
Proof. It follows from the expression of the Hessian in Lemma 12 and using the fact hθ, xii ≤
kθk2kxik2	□
Proof of Corollary 2. We show that the conditions of Theorem 2 hold.
Creating Nets: First we need to create an -net over the parameter space Θ. We start by creating an
e-net over the sphere with radius w. Now suppose, e < γ∕2R. Then We remove all centers θc if ∃i
s.t hθc, xii < γ∕2. This is a valid e-net over Θ as all net partitions that are removed do not have any
points lying in Θ. In subsequent section, we will always follow this strategy to create e-nets over
subsets of Θ.
Strong Convexity and Smoothness: From Lemma 13 and 12 we have that,
β	w2R2
=-ʃ ζ.
α γ2
KL Divergence: The L in Theorem 2 is bounded by 2bR∕γ according to Lemma 11.
Combining the above into Theorem 2 and using the estimator in (Hsu & Sabato, 2016) we get our
result.
□
I More on Experiments
We provide more experimental details in this section.
I.1 Metrics
The metrics and loss functions used are as follows:
MSE: The metric is
1n
n X(yi - yi)2∙
i=1
RMSE is just the square-root of this metric.
MAE: The metric is
1n
n X|yi - yi∖.
i=1
WAPE: The metric is
Pn=IIyi - yi∖
Pitι∖yi∖	.
MAPE: The metric is
1	X 1 -色.
n	yi
i:yi 6=0
27
Published as a conference paper at ICLR 2022
Quantile Loss: The reported metrics in Table 3 are the normalized quantile losses defined as,
n
X
i=1
2p(yi - yi)1yi≥yi + 2p(yi - yDIyi<yi
pn=ι≡
(49)
During training the unnormalized version is used for quantile regression.
Huber Loss: The loss is given by,
Lδ (y,仍
22(y - y)2,
Hy - yI - δ2
if |y - y| ≤ δ
otherwise.
I.2	More details about the mixture distribution
We use a mixture distribution between zero, a continuous extension of negative binomial (NB)
distribution and Pareto. The continuous extension of negative binomial is such that the p.d.f at y is
proportional to,
p(y) Y
Γ(n + k)
Γ(k +1)Γ(n)
(1 -p)npk
given parameters n and p. That is, the p.m.f of a regular discrete NB distribution is written in terms
of Gamma functions and then we extend that to non-integral points up to proportionality, such that
the measure sums to 1. This definition of NB is the standard implementation in Tensorflow (Abadi
et al., 2016; Dillon et al., 2017), in order to support both discrete and continuous data . It has been
used in many regression datasets before, especially in the field of genomics where the collected data
is a discrete continuous mixture (Robinson & Smyth, 2008; Chen et al., 2016; McCarthy et al., 2012).
Why not use a discrete-continuous mixture of zero, discrete NB and Pareto?
A mixture of these three distributions is a discrete continuous mixture, whose CDF is well-defined. It
is possible to define a likelihood in a standard manner such that it has a component proportional to
the pdf of Pareto at all points and to this we add dirac delta functions proportional to magnitude of
the negative-binomial pmf at non-negative integral points. We have an extra mass at zero to account
for the zero component. The sum of the dirac masses along with the integral of the Pareto density
sums to one. However, such a likelihood is not very useful in practice. For instance, if the data is
mostly continuous but not heavy tailed, the log-likelihood would mostly have the Pareto component
(because non-integral points have no contribution from the other components) which is not a desirable
outcome, as a NB distribution can better model sub-Gaussian and sub-Exponential data in terms of
moments.
I.3	Mapping of outputs for ZNBP
For the ZNBP model we require an output dimension size of 6. The first three dimensions are mapped
through a softmax layer (Goodfellow et al., 2016) to mixture weights. The fourth dimension is
mapped to the ’n’ in negative-binomial likelihood through the link function,
φ(x) =	1x +11,
1/(1 - x)
if x > 0
otherwise
The fifth dimension is mapped to ’p’ of the negative-binomial though the sigmoid function. The last
dimension is mapped to the scale parameter for the Pareto component using the φ(x) link function
above.
I.4	Hardware
We use the Tesla V100 architecture GPU for our experiments. We use Intel Xeon Silver 4216 16-Core,
32-Thread, 2.1 GHz (3.2 GHz Turbo) CPU and our post-hoc inference for MLE is parallelized over
all the cores.
28
Published as a conference paper at ICLR 2022
I.5	More details about Inference for MLE
We follow the approach of monte-carlo sampling. For each inference sample x0, we generate 10k
samples from the learnt distributionp(∙∣x0; θmie). Then We compute the correct statistics. For MSE,
RMSE the statistic is just the mean and for WAPE, MAE it is the median. For a quantile, it is the
corresponding quantile from the empirical distribution. For any loss of the form,
'(y,y)
1-( ^)β
the optimal statistic is the median from the distribution proportional to yβp(y∣x0; θmie) (Gneiting,
2011). Note that the MAPE falls under the above With β = -1 and relative error corresponds to
β = 1. The statistic can be computed by importance Weighing the empirical samples.
I.6	Models, Hyperparameters and Tuning
Encoder
Decoder
Figure 1: Time-Series Seq-2-Seq models. The MLE config is shoWn on the left and the TMO config
is shoWn at the right. The main difference is the output dimension and the loss function. In order to
keep the number of parameters the same, in the TMO model We add an extra layer of size 6 With
Relu activation (shoWn as Adj. (adjustment))
For the time-series datasets, the model is a seq-2-seq model With one hidden layer LSTMs for both
the encoder and the decoder. The hidden layer size is h = 256. The output layer of the LSTM is
connected to the final output layer through one hidden layer also having h neurons. Note that h Was
tuned in the set [8, 16, 32, 64, 128, 256] and for all datasets and models 256 Was chosen. In order to
be keep the number of parameters exactly the same, in the TMO models We add an extra layer With
ReLU With 6 neurons before the output.
We tuned the learning rate for Adam optimizer in the range [1e-5, 1e-1] in log-scale. The batch-size
Was also tuned in [64, 128, 256, 512] and the Huber-δ in [2i for i in range(-8, 8)]. The learning rate
Was eventually chosen as 2.77e-3 for both datasets, as it Was close to the optimal values selected for
all baseline models. The batch-size Was chosen to be 512 and the Huber-δ Was 32 and 64 for M5 and
Favorita respectively.
For the regression datasets the model is a DNN With one hidden layer of size 32. For the Bicycle
dataset the categorical features had there oWn embedding layer. The features [season, month, Weekday,
Weathersit] had embedding layer sizes [2,4,4,2].
We tuned the learning rate for Adam optimizer in the range [1e-5, 1e-1] in log-scale. The batch-size
Was also tuned in [64, 128, 256, 512] and the Huber-δ in [2i for i in range(-8, 8)]. The learning rate
Was eventually chosen as 3e-3 for the gas turbine dataset based on the best perforamce of the baseline
models and 1.98e-3 for the gas turbine dataset. The batch-size Was chosen to be 512 and the Huber-δ
Was 128 and 32 for Bicyle Share and Gas turbine respectively.
For the ZNBP model We also tune the α parameter in the pareto component betWeen [3, 4, 5]. The
value of 3 Was selected for all datasets, except for gas turbine Where We used α = 5.
29
Published as a conference paper at ICLR 2022
MLE	TMO
mle_params	y
↑	- T
Link FUnCtion-I
(Linear
L ɪ 、
Relu
Figure 2: Fully connected network for regression models. The MLE config is shown on the left and
the TMO config is shown at the right. The main difference is output dimension and the loss function.
Dataset	n	d
M5	1879	256
Favorita	1653	256
Bicycle	584	32
Gas Turbine	22039	32
Table 5: Note that here d refers to the dimension of the last layer of the architecture used in the
respective datasets.
We used a batched version of GP-UCB (Srinivas et al., 2009) to tune the hyper-parameters. We used
Tensorflow (Abadi et al., 2016) to train our models. In Table 5, we provide the number of samples
and the dimension of the last layer in each of our datasets.
I.7	Datasets
For the Favorita and M5 dataset we used the product hierarchy over the item-level time series. Along
with the item-level (leaf) time-series, we also add all the higher-level (parent) time-series from the
product hierarchy (i.e. we add family and class level time-series for Favorita, and department and
category level time series for M5). The time-series for a parent time-series is obtained as the mean
of the time-series of its children. This is closer to a real forecasting setting in practice where one is
interested in all levels of the hierarchy. The metrics reported are over all the time-series (both parents
and leaves) treated equally. The history length for our predictions is set to 28.
For the M5 dataset the validation scores are computed using the predictions from time steps 1886
to 1899, and test scores on steps 1900 to 1913. For the Favorita dataset the validation scores are
computed using the predictions from time steps 1660 to 1673, and test scores on steps 1674 to 1687.
The train test splits are as mentioned in the main paper. For the Gas turbine dataset we use the official
train test split. For the Bicycle share data there is no official split, but we use a randomly chosen fixed
10% as the test set for all our experiments.
30
Published as a conference paper at ICLR 2022
I.8	Additional Experiments and Figures
In order to show the dependency of λmax (Σ) in the bound in Corollary 1 we perform a simulated
experiment. We generate a dataset with n = 5000 and d = 10 such that each coordinate of x
is distributed i.i.d from a uniform distribution between [0, U]. In out experiment, we vary U in
{1,2, •一，9}. Then y is genearted from a Poisson distribution with rate (θ*, Xi for a fixed θ*. This
varies λmax(Σ) which is equal to U2/12. We train and validate on 2500 samples with early stopping
and plot the squared loss achieved on the test by the θmie based estimator in Figure 3 versus λmaχ(Σ).
We can clearly see a linear relationship which further validates our theoretical results.
In Figure 4, we plot the average training loss as a function of training iterations for the
MLE(ZNBP)model. We can see that the loss converges to a minima.
Figure 3: Test squared error versus λmax(Σ). We can clearly see a linear relationship. Each point in
the plot is averaged over 10 runs and we plot the standard error bars.
6
SSol
5 4
6u⊂rau
Figure 4: We plot the average training loss for the MLE(ZNBP)model as a function of training
iterations. We can observe that the training curve converges.
J Extended Discussion and Limitations
We advocate for MLE with a suitably chosen likelihood family followed by post-hoc inference tuned
to the target metric of interest, in favor of ERM directly with the target metric. On the theory side,
we prove a general result that shows competitiveness of MLE with any estimator for a target metric
under fairly general conditions. Application of the bound in the case of MSE for Poisson regression
and Pareto regression is shown. We believe that our general result is of independent interest and can
be used as a tool to prove competitiveness of MLE for a wide variety of problems. Such applications
can be an interesting direction of future work.
On the empirical side we show that a well designed mixture likelihood like the one from Section 5 can
adapt quite well to different datasets as the mixture weights are trained. As we have mentioned before,
31
Published as a conference paper at ICLR 2022
the MLE log-likelihood loss in such cases can be non-convex which might lead to some limitations in
terms of optimization. However, we observed that this is usually not a problem in practice and the
solutions that can be reached by mini-batch SGD can be quite good in terms of performance.
In conclusion we would recommend the following protocol for a practitioner based on our theoretical
and empirical observations:
If the overall problem is convex for TMO but introducing a MLE loss makes the problem non-
convex, then the gains from the MLE approach might be neutralized by the added hardness of the
non-convexity introduced. An example of such a situation is TMO for minimizing square loss on
a linear function class, which is just least-squares linear regression, but introduction of a mixture
likelihood like the one in Section 4 makes the problem non-convex. In this case it might be better
to stick with TMO or at least proceed with caution with the MLE approach. Note that if the chosen
MLE retains the convexity of the problem, for example Poisson MLE in Section 4.1, then we would
still recommend going with the MLE approach.
However, in many practical scenarios when training using a deep network, the TMO approach is
non-convex to begin with, even when the target metric itself is something simple and convex like the
square loss. In such a case we would recommend the MLE approach with a likelihood class that can
capture inductive biases about the dataset. This is because both TMO and MLE are non-convex and it
is better to capitalize on the potential gains from the MLE approach.
Finally, note that the user can always choose between TMO and even between different likelihood
classes through cross-validation in a practical setting. If the practitioner would like to forgo the
decision making in choosing the likelihood class, we recommend using a versatile likelihood like the
mixture likelihood in Sections 5.
We do not anticipate this work to have any negative social or ethical impact.
32