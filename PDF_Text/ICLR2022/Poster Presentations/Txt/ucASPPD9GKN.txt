Published as a conference paper at ICLR 2022
Is Homophily a Necessity for Graph Neural
Networks ?
Yao Ma
New Jersey Institute of Technology
yao.ma@njit.edu
Neil Shah
Snap Inc.
nshah@snap.com
Xiaorui Liu
Michigan State University
xiaorui@msu.edu
Jiliang Tang
Michigan State University
tangjili@msu.edu
Ab stract
Graph neural networks (GNNs) have shown great prowess in learning representa-
tions suitable for numerous graph-based machine learning tasks. When applied to
semi-supervised node classification, GNNs are widely believed to work well due
to the homophily assumption (“like attracts like”), and fail to generalize to het-
erophilous graphs where dissimilar nodes connect. Recent works have designed
new architectures to overcome such heterophily-related limitations. However, we
empirically find that standard graph convolutional networks (GCNs) can actually
achieve strong performance on some commonly used heterophilous graphs. This
motivates us to reconsider whether homophily is truly necessary for good GNN
performance. We find that this claim is not quite accurate, and certain types of
“good” heterophily exist, under which GCNs can achieve strong performance.
Our work carefully characterizes the implications of different heterophily condi-
tions, and provides supporting theoretical understanding and empirical observa-
tions. Finally, we examine existing heterophilous graphs benchmarks and recon-
cile how the GCN (under)performs on them based on this understanding.
1 Introduction
Graph neural networks (GNNs) are a prominent approach for learning represen-
tations for graph structured data. Thanks to their great capacity in jointly lever-
aging attribute and graph structure information, they have been widely adopted
to promote improvements for numerous graph-related learning tasks (Kipf and
Welling, 2016; Hamilton et al., 2017; Ying et al., 2018; Fan et al., 2019; Zit-
nik et al., 2018), especially centered around node representation learning and
semi-supervised node classification (SSNC). GNNs learn node representations
by a recursive neighborhood aggregation process, where each node aggregates
and transforms features from its neighbors. The node representations can then
be utilized for downstream node classification or regression tasks. Due to this
neighborhood aggregation mechanism, several existing works posit that many
GNNs implicitly assume strong homophily and homophily is critical for GNNs
to achieve strong performance on SSNC (Zhu et al., 2020b;a; Chien et al., 2021;
3
Figure 1: A het-
erophilous graph
on which GCN
achieves perfect
performance.
Maurya et al., 2021; Halcrow et al., 2020; Lim et al., 2021). In general, homophily describes the
phenomenon that nodes tend to connect with “similar” or “alike” others. Homophily is observed
in a wide range of real-world graphs including friendship networks (McPherson et al., 2001), po-
litical networks (Gerber et al., 2013; Newman, 2018), citation networks (Ciotti et al., 2016) and
more. Under the homophily assumption, through the aggregation process, a node’s representation
is “smoothed” via its neighbors’ representations, since each node is able to receive additional infor-
mation from neighboring nodes, which are likely to share the same label. Several recent works (Zhu
et al., 2020b;a) claim that GNNs are implicitly (or explicitly) designed with homophily in mind, are
not suitable for graphs exhibiting heterophily, where connected nodes are prone to have different
properties or labels, e.g dating networks or molecular networks (Zhu et al., 2020b). Such works
accordingly design and modify new architectures and demonstrate outperformance over other GNN
models on several heterophilous graphs.
1
Published as a conference paper at ICLR 2022
Present work. In our work, we empirically find that the graph convolutional network (GCN) Kipf
and Welling (2016), a fundamental, representative GNN model (which we focus on in this work)
is actually able to outperform such heterophily-specific models on some heterophilous graphs after
careful hyperparameter tuning. This motivates us to reconsider the popular notion in the literature
that GNNs exhibit a homophilous inductive bias, and more specifically that strong homophily is cru-
cial to strong GNN performance. Counter to this idea, we find that GCN model has the potential to
work well for heterophilous graphs under suitable conditions. We demonstrate intuition with the fol-
lowing toy example: Consider the perfectly heterophilous graph (with all inter-class edges) shown in
Figure 1, where the color indicates the node label. Blue-labeled and orange-labeled nodes are asso-
ciated with the scalar feature 0 and 1, respectively. Ifwe consider a single-layer GCN by performing
an averaging feature aggregation over all neighboring nodes, it is clear that all blue nodes will have
a representation of 1, while the orange nodes will have that of 0. Additional layers/aggregations
will continue to alternate the features between the two types of nodes. Regardless of the number of
layers, the two classes can still be perfectly separated. In this toy example, each blue (orange) node
only connects orange (blue) nodes, and all blue (orange) nodes share similar neighborhood patterns
in terms of their neighbors’ label/feature distributions.
Our work elucidates this intuition and extends it to a more general case: put simply, given a (ho-
mophilous or heterophilous) graph, GCN has the potential to achieve good performance if nodes
with the same label share similar neighborhood patterns. We theoretically support this argument
by investigating the learned node embeddings from the GCN model. We find that homophilous
graphs always satisfy such assumptions, which explains why GCN typically works well for them.
On other hand, there exist both “good” and “bad” heterophily, and GCNs can actually achieve strong
performance for “good” heterophily settings while they usually fail on “bad” heterophily settings.
Our work characterizes these settings, and provides a new perspective and solid step towards deeper
understanding for heterophilous graphs. In short:
Our contributions. (1) We reveal that strong homophily is not a necessary assumption for the GCN
model. The GCN model can perform well over some heterophilous graphs under certain condi-
tions. (2) We carefully characterize these conditions and provide theoretical understandings on how
GCNs can achieve good SSNC performance under these conditions by investigating their embed-
ding learning process. (3) We carefully investigate commonly used homophilous and heterophilous
benchmarks and reason about GCN’s performs on them utilizing our theoretical understanding.
2	Preliminaries
Let G = {V, E} denote a graph, where V and E are the sets of nodes and edges, respectively. The
graph connection information can also be represented as an adjacency matrix A ∈ {0,1}lVl×lVl,
where |V| is the number of nodes in the graph. The i, j-th element of the adjacency matrix A[i, j] is
equal to 1 if and only if nodes i and j are adjacent to each other, otherwise A[i, j] = 0. Each node
i is associated with a l-dimensional vector of node features xi ∈ Rl ; the features for all nodes can
be summarized as a matrix X ∈ RlVl×l. Furthermore, each node i is associated with a label yi ∈ C,
where C denotes the set of labels. We also denote the set of nodes with a given label c ∈ C as Vc .
We assume that labels are only given for a subset of nodes Vlabel ⊂ V. The goal of semi-supervised
node classification (SSNC) is to learn a mapping f : V → C utilizing the graph G, the node features
X and the labels for nodes in Vlabel .
2.1	Homophily in Graphs
In this work, we focus on investigating performance in the context of graph homophily and het-
erophily properties. Homophily in graphs is typically defined based on similarity between con-
nected node pairs, where two nodes are considered similar if they share the same node label. The
homophily ratio is defined based on this intuition following Zhu et al. (2020b).
Definition 1 (Homophily). Given a graph G = {V, E} and node label vector y, the edge homophily
ratio is defined as the fraction of edges that connect nodes with the same labels. Formally, we have:
h(G, {yi； i ∈ V}) = |E1| X Myj = yk),
(j,k)∈E
where |E| is the number of edges in the graph and 1(∙) is the indicator function.
(1)
A graph is typically considered to be highly homophilous when h(∙) is large (typically, 0.5 ≤ h(∙) ≤
1), given suitable label context. On the other hand, a graph with a low edge homophily ratio is
considered to be heterophilous. In future discourse, We write h(∙) as h when discussing given a
fixed graph and label context.
2
Published as a conference paper at ICLR 2022
2.2	Graph Neural Networks
Graph neural networks learn node representations by aggregating and transforming information over
the graph structure. There are different designs and architectures for the aggregation and transforma-
tion, which leads to different graph neural network models (Scarselli et al., 2008; Kipf and Welling,
2016; Hamilton et al., 2017; VeliCkovic et al., 2017; Gilmer et al., 2017; ZhoU et al., 2020).
One of the most popular and widely adopted GNN models is the graph convolutional network
(GCN). A single GCN operation takes the following form H0 = D-1AHW, where H and H0
denote the inpUt and oUtpUt featUres of layer, W(k) ∈ Rl×l is a parameter matrix to transform the
featUres, and D is a diagonal matrix and D[i, i] = deg(i) with deg(i) denoting the degree of node
i. From a local perspective for node i, the process can be written as a featUre averaging process
hi = de1(i)Pj∈N(i) WXj, where N(i) denotes the neighbors of node i. The neighborhood N(i)
may contain the node i itself. UsUally, when bUilding GCN model Upon GCN operations, nonlinear
activation fUnctions are added between consecUtive GCN operations.
3	Graph Convolutional Networks under Heterophily
Considerable prior literatUre posits that graph neUral networks (sUch as GCN) work by assUming
and exploiting homophily assUmptions in the Underlying graph (MaUrya et al., 2021; Halcrow et al.,
2020; WU et al., 2018). To this end, researchers have determined that sUch models are considered
to be ill-sUited for heterophiloUs graphs, where the homophily ratio is low (ZhU et al., 2020b;a;
Chien et al., 2021). To deal with this limitation, researchers proposed several methods inclUding
H2GNN (ZhU et al., 2020b), CPGNN (ZhU et al., 2020a) and GPRGNN (Chien et al., 2021), which
are explicitly designed to handle heterophiloUs graphs via architectUral choices (e.g. adding skip-
connections, carefUlly choosing aggregators, etc.)
In this section, we revisit the claim that GCNs have
fUndamental homophily assUmptions and are not sUited
for heterophiloUs graphs. To this end, we first ob-
serve empirically that the GCN model achieves fairly
good performance on some of the commonly Used het-
erophiloUs graphs; specifically, we present SSNC per-
formance on two commonly Used heterophiloUs graph
datasets, Chameleon and Squirrel in Table 1 (see
Appendix D for fUrther details aboUt the datasets and
models). Both Chameleon and Squirrel are highly
Table 1: SSNC accUracy on two het-
erophiloUs datasets.
Method	Chameleon (h = 0.23)	Squirrel (h = 0.22)
GCN	67.96 ± 1.82	54.47 ± 1.17
H2GCN-1	57.11 ± 1.58	36.42 ± 1.89
H2GCN-2	59.39 ± 1.98	37.90 ± 2.02
CPGNN-MLP	54.53 ± 2.37	29.13 ± 1.57
CPGNN-Cheby	65.17 ± 3.17	29.25 ± 4.17
GPRGNN	66.31 ± 2.05	50.56 ± 1.51
MLP	48.11 ± 2.23	31.68 ± 1.90
heterophiloUs (h≈0.2). We find that with some hyperparameter tUning, GCN can outperform al-
ternative methods uniquely designed to operate on some certain heterophilous graphs. This obser-
vation sUggests that GCN does not always “Underperform” on heterophiloUs graphs, and it leads
Us to reconsider the prevalent assUmption in literatUre. Hence, we next examine how GCNs learn
representations, and how this information is Used in downstream SSNC tasks.
3.1	When does GCN learn similar embeddings for nodes with the same label?
GCN is considered to be Unable to tackle heterophiloUs graphs dUe to its featUre averaging pro-
cess (ZhU et al., 2020b; Chien et al., 2021). Namely, a node’s newly aggregated featUres are con-
sidered “corrUpted” by those neighbors that do not share the same label, leading to the intUition
that GCN embeddings are noisy and Un-ideal for SSNC. However, we find that crUcially, for some
heterophiloUs graphs, the featUres of nodes with the same label are “corrUpted in the same way.”
Hence, the obtained embeddings still contain informative characteristics and thUs facilitate SSNC.
We next illUstrate when GCN learns similar embeddings for nodes with the same label, beginning
with a toy example and generalizing to more practical cases.
GCNs have been shown to be able to captUre the local
graph topological and strUctUral information (XU et al.,
2019; Morris et al., 2019). Specifically, the aggregation
step in the GCN model is able to captUre and discriminate
neighborhood distribUtion information, e.g. the mean of
the neighborhood featUres (XU et al., 2019). Let Us con-
FigUre 2: Two nodes share the same
neighborhood distribUtion; GCN learns
eqUivalent embeddings for a and b.
sider the two nodes a and b shown in FigUre 2, where we Use color to indicate the label of each
node. If we fUrther assUme that all nodes sharing the same label are associated with exactly the
same featUres, then clearly, after 1-step aggregation, the GCN operation will oUtpUt exactly the
same embedding for nodes a and b. Accordingly, XU et al. (2019) reasons that the GCN model lacks
expressiveness dUe to its inability to differentiate the two nodes in the embedding space. However, in
3
Published as a conference paper at ICLR 2022
the SSNC task, mapping a and b to the same location in the embedding space is explicitly desirable.
Intuitively, if all nodes with the same label are mapped to the same embedding and embeddings for
different labels are distinct, SSNC is effortless (Zhao et al., 2020).
Such assumptions are hard to meet in practice. Thus, to consider a more practical scenario, we
assume that both features and neighborhood patterns for nodes with a certain label are sampled from
some fixed distributions. Under these conditions, same-label nodes may not share fixed embeddings,
but we can aim to characterize their closeness. Intuitively, if the learned embeddings for same-label
nodes are close and embeddings for other-label nodes are far, we expect strong SSNC performance
to be good, given class separability (low intra-class variance and high inter-class variance) (Fisher,
1936). We prove that, for graphs meeting suitable conditions the distance between GCN-learned
embeddings of any same-label node pair is bounded by a small quantity with high probability.
Assumptions on Graphs. We consider a graph G , where each node i has features xi ∈ Rl and
label yi. We assume that (1) The features of node i are sampled from feature distribution Fyi, i.e,
Xi 〜Fyi, with μ(FyJ denoting its mean; (2) Dimensions of Xi are independent to each other; (3)
The features in X are bounded by a positive scalar B, i.e, maxi,j |X[i, j]| ≤ B; (4) For node i,
its neighbor’s labels are independently sampled from neighbor distribution Dyi . The sampling is
repeated for deg(i) times to sample the labels for deg(i) neighbors.
We denote a graph following these assumptions (1)-(4) as G = {V, E, {Fc, c ∈ C}, {Dc, c ∈ C}}.
Note that we use the subscripts in Fyi and Dyi to indicate that these two distributions are shared
by all nodes with the same label as node i. Next, we analyze the embeddings obtained after a GCN
operation. Following previous works (Li et al., 2018; Chen et al., 2020; Baranwal et al., 2021), we
drop the non-linearity in the analysis.
Theorem 1. Consider a graph G = {V, E, {Fc, c ∈ C}, {Dc, c ∈ C}}, which follows Assump-
tions (1)-(4). For any node i ∈ V, the expectation of the pre-activation output of a single GCN
operation is given by
E[hi] = W (Ec〜Dyi ,x〜Fc[x]) .	(2)
and for any t > 0, the probability that the distance between the observation hi and its expectation
is larger than t is bounded by
P (khi - E[hi ]k2 ≥ t) ≤ 2 ∙l ∙ exΡ (- 2P：WB2l) ,	(3)
where l denotes the feature dimensionality and ρ(W) denotes the largest singular value of W.
The detailed proof can be found in Appendix A. Theorem 1 demonstrates two key ideas. First, in
expectation, all nodes with the same label have the same embedding (Eq. (2)). Second, the distance
between the output embedding of a node and its expectation is small with a high probability. Specif-
ically, this probability is related to the node degree and higher degree nodes have higher probability
to be close to the expectation. Together, these results show that the GCN model is able to map nodes
with the same label to an area centered around the expectation in the embedding space under given
assumptions. Then, the downstream classifier in the GCN model is able to assign these nodes to
the same class with high probability. To ensure that the classifier achieves strong performance, the
centers (or the expectations) of different classes must be distant from each other; if we assume that
μ(Fyj are distinct from each other (as is common), then the neighbor distributions {Dc, C ∈ C}
must be distinguishable to ensure good SSNC performance. Based on these understandings and
discussions, we have the following key (informal) observations on GCN’s performance for graphs
with homophily and heterophily.
Observation 1 (GCN under Homophily). In homophilous graphs, the neighborhood distribution of
nodes with the same label (w.l.o.g c) can be approximately regarded as a highly skewed discrete Dc,
with most of the mass concentrated on the category c. Thus, different labels clearly have distinct
distributions. Hence, the GCN model typically in SSNC on such graph, with high degree nodes
benefiting more, which is consistent with previous work (Tang et al., 2020b).
Observation 2 (GCN under Heterophily). In heterophilous graphs, if the neighborhood distribution
of nodes with the same label (w.l.o.g. c) is (approximately) sampled from a fixed distribution Dc,
and different labels have distinguishable distributions, then GCN can excel at SSNC, especially
when node degrees are large. Otherwise, GCNs may fail for heterophilous graphs.
Notably, our findings illustrate that disruptions of certain conditions inhibit GCN performance on
heterophilous graphs, but heterophily is not a sufficient condition for poor GCN performance. GCNs
are able to achieve reasonable performance for both homophilous and heterophilous graphs if they
follow certain assumptions as discussed in the two observations. In Section 3.2, we theoretically
4
Published as a conference paper at ICLR 2022
demonstrate these observations for graphs sampled from the Contextual Stochastic Block Model
(CSBM) (Deshpande et al., 2018) with two classes, whose distinguishablilty of neighborhood dis-
tributions can be explicitly characterized. Furthermore, in Section 3.3, we empirically demonstrate
these observations on graphs with multiple classes. We note that although our derivations are for
GCN, a similar line of analysis can be used for more general message-passing neural networks.
3.2	Analysis based on CSBM model with two classes
The CSBM model. To clearly control assumptions, we study the contextual stochastic block
model(CSBM), a generative model for random graphs; such models have been previously adopted
for benchmarking graph clustering (Fortunato and Hric, 2016) and GNNs (Tsitsulin et al., 2021).
Specifically, we consider a CSBM model consisting of two classes c1 and c2 . In this case, the nodes
in the generated graphs consist of two disjoint sets C1 and C2 corresponding to the two classes,
respectively. Edges are generated according to an intra-class probability p and an inter-class prob-
ability q. Specifically, any two nodes in the graph, are connected by an edge with probability p,
if they are from the same class, otherwise, the probability is q. For each node i, its initial fea-
tures Xi ∈ Rl are sampled from a Gaussian distribution Xi 〜N(μ, I), where μ = μk ∈ Rl for
i ∈ Ck with k ∈ {1, 2} and μι = μ2. We denote a graph generated from such an CSBM model as
G 〜CSBM(μι, μ2,p,q). We denote the features for node i obtained after a GCN operation as hi.
Linear separability under GCN. To better evaluate the effectiveness of GCN operation, we study
the linear classifiers with the largest margin based on {Xi, i ∈ V} and {hi, i ∈ V} and compare
their performance. Since the analysis is based on linear classifiers, we do not consider the linear
transformation in the GCN operation as it can be absorbed in the linear model, i.e, we only consider
the process hi = ^^ Pj∈N⑴ Xj. For a graph G ~ CSBM(μι, μ2,p, q), We can approximately
regard that for each node i, its neighbor’s labels are independently sampled from a neighborhood
distribution Dyi, where yi denotes the label of node i. Specifically, the neighborhood distributions
corresponding to ci and c are Dcγ =[册,p++q ] and Dc2 = [p++q,/],respectively.
Based on the neighborhood distributions, the features obtained from GCN operation follow Gaussian
distributions:
〜N ( pμ1+ qμ
p+q
, for i ∈ C1 ; and hi
〜N ( qμ1+ pμ2
p+ q
for i ∈ C2 .
hi
(4)
Based on the properties of Gaussian distributions, it is easy to see that Theorem 1 holds. We denote
the expectation of the original features for nodes in the two classes as Ec1 [Xi] and Ec2 [Xi]. Similarly,
we denote the expectation of the features obtained from GCN operation as Ec1 [hi] and Ec2 [hi]. The
following proposition describes their relations.
Proposition 1. (Ec1 [Xi], Ec2 [Xi]) and (Ec1 [hi], Ec2 [hi]) share the same middle point. Ec1 [Xi] -
Ec2 [Xi] and Ec1 [hi] - Ec2 [hi] share the same direction. Specifically, the middle point m and the
shared direction W are as follows: m = (μι + μ2 )/2, and W = (μι - μ2)∕kμι - μ2∣∣2.
This proposition follows from direct calculations. Given that the feature distributions of these two
classes are systematic to each other (for both Xi and hi), the hyperplane that is orthogonal to W
and goes through m defines the decision boundary of the optimal linear classifier for both types of
features. We denote this decision boundary as P = {x∣w>x - w>(μι + μ2)∕2}.
Next, to evaluate how GCN operation affects the classification performance, we compare the proba-
bility that this linear classifier misclassifies a certain node based on the features before and after the
GCN operation. We summarize the results in the following theorem.
Theorem 2. Consider a graph G 〜CSBM(μι, μ2,p, q). For any node i in this graph, the linear
classifier defined by the decision boundary P has a lower probability to misclassify hi than Xi when
deg(i) > (p + q)2/(p - q)2.
The detailed proof can be found in Appendix B. Note that the Euclidean distance between the two
discrete neighborhood distributions Dcq and Dcγ is √2 (P-q). Hence, Theorem 2 demonstrates
that the node degree deg(i) and the distinguishability (measured by the Euclidean distance) of the
neighborhood distributions both affect GCN’s performance. Specifically, we can make the following
conclusions: (1) When p and q are fixed, the GCN operation is more likely to improve the linear
separability of the high-degree nodes than low-degree nodes, which is consistent with observations
in (Tang et al., 2020b). (2) The more distinguishable the neighborhood distributions are (or the
larger the Euclidean distance is), the more nodes can be benefited from the GCN operation. For
5
Published as a conference paper at ICLR 2022
∙87∙65
Oooo
0.4....................................................
0.81 0.74 0.68 0.63 0.59 0.52 0.46 0.42 0.38 0.32 0.28 0.25
Homophily Ratio
(a) Synthetic graphs generated from Cora
0.3
∙87∙63
Oooo
0.74 0.65 0.58 0.53 0.48 0.41 0.36 0.32 0.28 0.24 0.2 0.18
Homophily Ratio
(b) Synthetic graphs generated from Citeseer
Figure 3: SSNC accuracy of GCN on synthetic graphs with various homophily ratios.
example, when p = 9q or 9p = q, (p + q)2/(p - q)2 ≈ 1.23, thus nodes with degree larger than 1
can benefit from the GCN operation. These two cases correspond to extremely homophily (h = 0.9)
and extremely heterophily (h = 0.1), respectively. However, GCN model behaves similarly on these
two cases and is able to improve the performance for most nodes. This clearly demonstrates that
heterophily is not a sufficient condition for poor GCN performance. Likewise, when p≈q, the two
neighborhood distributions are hardly distinguishable, and only nodes with extremely large degrees
can benefit. In the extreme case, where p = q, the GCN operation cannot help any nodes at all. Note
that Theorem 2 and the followed analysis can be extended to a multi-class CSBM scenario as well -
see Appendix F for an intuitive explanation and proof sketch.
3.3 Empirical investigations on graphs with multiple classes
We conduct experiments to substantiate our claims in Ob-
servations 1 and 2 in graphs with multiple classes. We
evaluate how SSNC performance changes as we make a
homophilous graph more and more heterophilous under
two settings: (1) different labels have distinct distribu-
tions, and (2) different labels’ distributions are muddled.
3.3.1	Targeted Heterophilous Edge Addition
Graph generation strategy. We start with common, real-
world benchmark graphs, and modify their topology by
adding synthetic, cross-label edges that connect nodes
with different labels. Following our discussion in Obser-
vation 2, we construct synthetic graphs that have similar
neighborhood distributions for same-label nodes. Specif-
Alg. 1: Hetero. Edge Addition
input : G = {V, E}, K, {Dc}C=-1
and {Vc}|cC=|0-1
output: G0 = {V, E0}
Initialize G0 = {V, E}, k = 1 ;
while 1 ≤ k ≤ K do
Sample node i 〜Uniform(V);
Obtain the label, yi of node i;
Sample a label c 〜Dyi;
Sample node j 〜Uniform(Vc);
Update edge set E0 = E0 ∪ {(i, j)};
k — k + 1;
return G = {V, E0}
ically, given a real-world graph G , we first define a discrete neighborhood target distribution Dc for
each label c ∈ C . We then follow these target distributions to add cross-label edges. The process of
generating new graphs by adding edges to G is shown in Algorithm 1. Specifically, we add a total K
edges to the given graph G : to add each edge, we first uniformly sample a node i from V with label
yi, then we sample a label c from C according to Dyi, and finally, we uniformly sample anodej from
Vc and add the edge (i, j ) to the graph. We generate synthetic graphs based on several real-world
graphs. We present the results based on Cora and Citeseer (Sen et al., 2008). The results for
other datasets can be found in Appendix C. Both Cora and Citeseer exhibit strong homophily.
For both datasets, we fix Dc for all labels. Although many suitable Dc could be specified in line
with Observation 2, we fix one set for illustration and brevity. For both datasets, we vary K over
11 values and thus generate 11 graphs. Notably, as K increases, the homophily h decreases. More
detailed information about the {Dc, c ∈ C} and K for both datasets is included in Appendix C.
Observed results. Figure 3(a-b) show SSNC results (accuracy) on graphs generated based on Cora
and Citeseer, respectively. The black line in both figures shows results for the presented setting
(we introduce γ in next subsection). Without loss of generality, we use Cora (a) to discuss our find-
ings, since observations are similar over these datasets. Each point on the black line in Figure 3(a)
represents the performance of GCN model on a certain generated graph and the corresponding value
in x-axis denotes the homophily ratio of this graph. The point with homophily ratio h = 0.81 de-
notes the original Cora graph, i.e, K = 0. We observe that as K increases, h decreases, and while
the classification performance first decreases, it eventually begins to increase, showing a V -shape
pattern. For instance, when h = 0.25 (a rather heterophilous graph), the GCN model achieves an
impressive 86% accuracy, even higher than that achieved on the original Cora graph. We note that
6
Published as a conference paper at ICLR 2022
O
1
2
3
4
5
6
0.04 0.04
0.37 0.4 0.02 0.02
0.29
0.04 0.02 041 0.49	0.36。45
0.42 0.37	0.5 0.41 0.01 0.01
0.04 OA 0.5 ^≡0-49 0.39 0.03
0	1	2	3	4	5	6
(a) γ = 0, Acc: 86%
1.0
0.8
0.6
0.4
0.2
」QQ
0.85
0.54
0.58
0.36
0.39
0.36 0-39
Q.54
。58
Q.91
0.581
0-58
0^5
08
0.39
0.39 0.37
0.6
0.36
0.39^^
国 0.37 0.36
^2^
0.39
0.2
^3^ 4	5	6
1.0
0.4
illO-O
∣Ξ	0.76	0.74	0.64	0.7	0.77 0.76	1.0 0.8 0.6
¾0∙76	035	0.77	0.68	0.73	0.76 0.83	
［竺	0.77	。£	0.71	0.72	0.72 0.74	
狂	0.68	0.71	0.72	0.71	0.67 0.67	
						0.4
∣±L	0.73	0.72	0.71	0.79	0.76 0.77	
I 0.77 L	0.76 033 n	0.72 0.74 n	0.67 0.67 n	0.76 0.77	0.83 0.76 0.76 0.89	0.2 ⅛,0
(c) γ = 0.8, Acc: 44%
08	082	0.78	0.7	0.77	031	0.83	
0.82	085	0.8	0.73	。£	033	0.85	
0.78	0.8	0.79	0.67	0.75	0.8	0.82	
0.7	0.73	0.67	0.73	0.69	0.72	0.74	
0.77	0.8	0.7S	0.69	0.77	0.78	0.81	
0.81	033	0.8	0.72	0.78	033	0.84	
0.83	085	082	0.74	0.81	0∙84	0.88	
01^!34^i6	00
(d) γ = 1, Acc: 41%
(b) γ = 0.4, Acc: 66%
O
2
3
4
5
Figure 4: Cross-class neighborhood similarity on synthetic graphs generated from Cora; all graphs
have h = 0.25, but with varying neighborhood distributions as per the noise parameter γ.
performance continues to increase as K increases further to the right (we censor due to space limi-
tations; see Appendix C for details). This clearly demonstrates that the GCN model can work well
on heterophilous graphs under certain conditions. Intuitively, the V -shape arises due to a “phase
transition”, where the initial topology is overridden by added edges according to the associated Dc
target neighbor distributions. In the original graph, the homophily ratio is quite high (h = 0.81),
and classification behavior is akin to that discussed in Observation 1, where same-label nodes have
similar neighborhood patterns. As we add edges to the graph, the originally evident neighborhood
patterns are perturbed by added edges and gradually become less informative, which leads to the
performance decrease in the decreasing segment of the V -shape in Figure 3(a). Then, as we keep
adding more edges, the neighborhood pattern gradually approaches Dc for all c, corresponding to
the increasing segment of the V -shape.
3.3.2 Introducing noise to neighborhood distributions
Graph generation strategy. In Section 3.3.1, we showed that the GCN model can achieve reason-
able performance on heterophilous graphs constructed following distinct, pre-defined neighborhood
patterns. As per Observation 2, our theoretical understanding suggests that performance should de-
grade under heterophily if the distributions of different labels get more and more indistinguishable.
Hence, we next demonstrate this empirically by introducing controllable noise levels into our edge
addition strategy. We adopt a strategy similar to that described in Algorithm 1, but with the key
difference being that we introduce an additional parameter γ , which controls the probability that
we add cross-label edges randomly rather than following the pre-defined distributions. A detailed
description of this approach is demonstrated in Algorithm 2 in Appendix C. For nodes of a given
class c (w.l.o.g), compared to the edges added according to Dc, the randomly added edges can be
regarded as noise. Specifically, by increasing the noise parameter γ, we increase the similarity be-
tween Dc, Dc0 for any pair of labels c, c0. If γ = 1, then all neighborhood distributions will be
indistinguishable (they will all be approximately Uniform(|C|). By fixing K and varying γ, we can
generate graph variants with the same homophily ratio but different similarities between Dc and
Dc0. As in Section 3.3.1, we create graphs by adding edges at various K, but also vary γ ∈ [0, 1] in
increments of 0.2 on both Cora and Citeseer.
Observed results. We report the SSNC performance on these graphs in Figure 3. Firstly, we observe
that noise affects the performance significantly when the homophily ratio is low. For example,
observing Figure 3(a) vertically at homophily ratio h = 0.25, higher γ clearly results in worse
performance. This indicates that not only the fixed-ness of the neighborhood distributions, but their
similarities are important for the SSNC task (aligned with Observation 2. It also indicates that
there are “good” and “bad” kinds of heterophily. On the other hand, high γ does not too-negatively
impact when K is small, since noise is minimal and the original graph topology is yet largely
homophilous. At this stage, both “good” (fixed and disparate patterns) and “bad” (randomly added
edges) heterophilous edges introduce noise to the dominant homophilous patterns. When the noise
level γ is not too large, we can still observe the V -shape: e.g. γ = 0.4 in Figure 3(a) and γ = 0.2
in Figure 3 (b); this is because the designed pattern is not totally dominated by the noise. However,
when γ is too high, adding edges will constantly decrease the performance, as nodes of different
classes have indistinguishably similar neighborhoods.
To further demonstrate how γ affects the neighborhood distributions in the generated graph, we
examine the cross-class neighborhood similarity, which we define as follows:
Definition 2 (Cross-Class Neighborhood Similarity (CCNS)). Given graph G and labels y for all
nodes, the CCNS between classes c,c0 ∈ C is s(c, c0)= 认|,” £跖心 j∈v , Cos (d(i),d(j)) where
7
Published as a conference paper at ICLR 2022
Vc indicates the set of nodes in class c and d(i) denotes the empirical histogram (over |C| classes)
of node i S neighbors' labels, and thefunCtion cos(∙, ∙) measures the cosine similarity.
When c = c0, s(c, c0) calculates the intra-class similarity, otherwise, it calculates the inter-class
similarity from a neighborhood label distribution perspective. Intuitively, if nodes with the same
label share the same neighborhood distributions, the intra-class similarity should be high. Likewise,
to ensure that the neighborhood patterns for nodes with different labels are distinguishable, the inter-
class similarity should be low. To illustrate how various γ values affect the neighborhood patterns,
we illustrate the intra-class and inter-class similarities in Figure 4 for γ = 0, 0.4, 0.8, 1 on graphs
generated from Cora with homophily ratio h = 0.25. The diagonal cells in each heatmap indicate
the intra-class similarity while off-diagonal cells indicate inter-class similarity. Clearly, when γ is
small, the intra-class similarity is high while the inter-class similarity is low, which demonstrates
the existence of strongly discriminative neighborhood patterns in the graph. As γ increases, the
intra-class and inter-class similarity get closer, becoming more and more indistinguishable, leading
to bad performance due to indistinguishable distributions as referenced in Observation 2.
4 Revisiting GCN’s Performance on Real-world Graphs
In this section, we first give more details on the experiments we run to compare GCN and MLP. We
next investigate why the GCN model does or does not work well on certain datasets utilizing the
understanding developed in earlier sections.
4.1	Quantitative Analysis
Following previous work (Pei et al., 2020; Zhu et al., 2020b), we evaluate the performance of
the GCN model on several real-world graphs with different levels of homophily. We include the
citation networks Cora, Citeseer and Pubmed (Kipf and Welling, 2016), which are highly
homophilous. We also adopt several heterophilous benchmark datasets including Chameleon,
Squirrel, Actor, Cornell, Wisconsin and Texas (Rozemberczki et al., 2021; Pei et al.,
2020). Appendix D.1 gives descriptions and summary statistics of these datasets. For all datasets,
we follow the experimental setting provided in (Pei et al., 2020), which consists of 10 random splits
with proportions 48/32/20% corresponding to training/validation/test for each graph. For each
split, we use 10 random seeds, and report the average performance and standard deviation across
100 runs. We compare the GCN model with the MLP model, which does not utilize the graph
structure. With this comparison, we aim to check whether the GCN model always fails for for
heterophilous graphs (perform even worse than MLP). We also compare GCN with state-of-the-art
methods and their descriptions and performance are included in the Appendix D. The node clas-
sification performance (accuracy) of these models is reported in Table 2. Notably, GCN achieves
better performance than MLP on graphs with high homophily (Cora, Citeseer, and Pubmed),
as expected. For the heterophilous graphs, the results are comparatively mixed. The GCN model
outperforms MLP on Squirrel and Chameleon (it even outperforms methods specifically de-
signed for heterophilous graphs as shown in Appendix D.5.), while underperforming on the other
datasets (Actor, Cornell, Wisconsin, and Texas). In the next section, we provide explana-
tions for GCN’s distinct behaviors on these graphs based on the understanding developed in earlier
sections.
4.2	Qualitative Analysis
Our work so far illustrates that the popular notion of GCNs not being suitable for heterophily, or
homophily being a mandate for good GCN performance is not accurate. In this subsection, we
aim to use the understanding we developed in Section 3 to explain why GCN does (not) work
well on real-world graphs. As in Section 3.3.2, we inspect cross-class neighborhood similarity
(Definition 2) for each dataset; due to the space limit, we only include representative ones here
(Cora, Chameleon, Actor and Cornell); see Figure 5). Heatmaps for the other datasets
can be found in Appendix E. From Figure 5(a), it is clear that the intra-class similarity is much
higher than the inter-similarity ones, hence Cora contains distinct neighborhood patterns, consistent
with Observation 1. In Figure 5(b), we can observe that in Chameleon, intra-class similarity is
generally higher than inter-class similarity, though not as strong as in Figure 5(a). Additionally,
there is an apparent gap between labels 0, 1 and 2, 3, 4, which contributes to separating nodes of the
former 2 from the latter 3 classes, but potentially increasing misclassification within each of the two
groupings. These observations also help substantiate why GCN can achieve reasonable performance
(much higher than MLP) on Chameleon. The GCN model underperforms MLP in Actor and
we suspect that the graph does not provide useful information. The heatmap for Actor in Figure 5
shows that the intra-class and inter-class similarities are almost equivalent, making the neighborhood
8
Published as a conference paper at ICLR 2022
Table 2: Node classification performance (accuracy) on homophilous and heterophilous graphs.
	Cora	Citeseer	Pubmed	Chameleon	Squirrel	Actor	Cornell	Wisconsin	Texas
GCN	87.12 ± 1.38	76.50 ± 1.61	88.52 ± 0.41	67.96 ± 1.82	54.47 ± 1.17	30.31 ± 0.98	59.35 ± 4.19	61.76 ± 6.15	63.81 ± 5.27
MLP	75.04 ± 1.97	72.40 ± 1.97	87.84 ± 0.30	48.11 ± 2.23	31.68 ± 1.90	36.17 ± 1.09	84.86 ± 6.04	86.29 ± 4.50	83.30 ± 4.54
O
1
2
3
4
5
6
1.0
0.03
0.05 0.01 0.05 0.01
0.15
0.1 0.05
0.11 0.07 0.04
0.12
0.04 0.01
0.04 0.02
0.11
0.06 0.05 0.07 0.04 «■»：
0.07 0.03 0.15 0.12 0.11 0.15
0.8
0.07
0.6
0.4
0.12
0.2
0.16
0.01 0.01 0.04 0.02 0.12
O
2
3	4	5	6
(a) Cora
0.09 0.1 0.04 0.06 0.01
0.09
0.11
llO-O
(b) Chameleon
(d) Cornell
Figure 5: Cross-class neighborhood similarity on homophilous graphs and heterophilous graphs.
distributions for different classes hard to distinguish and leading to bad GCN performance. Similar
observations are made for Cornell. Note that Cornell only consists of 183 nodes and 280
edges, hence, the similarities shown in Figure 5 are impacted significantly (e.g. there is a single
node with label 1, leading to perfect intra-class similarity for label 1).
5	Related Work
Graph neural networks (GNNs) are powerful models for graph representation learning. They have
been widely adopted to tackle numerous applications from various domains (Kipf and Welling,
2016; Fan et al., 2019; Bastings et al., 2017; Shi et al., 2019). (Scarselli et al., 2008) proposed
the first GNN model, to tackle both node and graph level tasks. Subsequently, Bruna et al. (2013)
and Defferrard et al. (2016) generalized convolutional neural networks to graphs from the graph
spectral perspective. Kipf and Welling (2016) simplified the spectral GNN model and proposed
graph convolutional networks (GCNs). Since then, numerous GNN variants, which follow specific
forms of feature transformation (linear layers) and aggregation have been proposed (Velickovic et al.,
2017; Hamilton et al., 2017; Gilmer et al., 2017; Klicpera et al., 2019). The aggregation process can
be usually understood as feature smoothing (Li et al., 2018; Ma et al., 2020; Jia and Benson, 2021;
Zhu et al., 2021). Hence, several recent works claim (Zhu et al., 2020b;a; Chien et al., 2021), assume
(Halcrow et al., 2020; Wu et al., 2018; Zhao et al., 2020) or remark upon (Abu-El-Haija et al., 2019;
Maurya et al., 2021; Hou et al., 2020) GNN models homophily-reliance or unsuitability in capturing
heterophily. Several recent works specifically develop GNN models choices to tackle heterophilous
graphs by carefully designing or modifying model architectures such as Geom-GCN (Pei et al.,
2020), H2GCN (Zhu et al., 2020b), GPR-GNN (Chien et al., 2021), and CPGNN (Zhu et al., 2020a).
Some other works aim to modify/construct graphs to be more homophilous (Suresh et al., 2021).
There is concurrent work (Luan et al., 2021) also pointing out that GCN can potentially achieve
strong performance on heterophilous graphs. A major focus of this paper is to propose a new model
to handle heterophilous graphs. However, our work aims to empirically and theoretically understand
whether homophily is a necessity for GNNs and how GNNs work for heterophilous graphs.
6	Conclusion
It is widely believed that GNN models inherently assume strong homophily and hence fail to gener-
alize to graphs with heterophily. In this paper, we revisit this popular notion and show it is not quite
accurate. We investigate one representative model, GCN, and show empirically that it can achieve
good performance on some heterophilous graphs under certain conditions. We analyze theoretically
the conditions required for GCNs to learn similar embeddings for same-label nodes, facilitating the
SSNC task; put simply, when nodes with the same label share similar neighborhood patterns, and
different classes have distinguishable patterns, GCN can achieve strong class separation, regard-
less of homophily or heterophily properties. Empirical analysis supports our theoretical findings.
Finally, we revisit several existing homophilous and heterophilous SSNC benchmark graphs, and
investigate GCN’s empirical performance in light of our understanding. Note that while there exist
graphs with “good heterophily”, “bad heterophily” still poses challenges to GNN models, which
calls for dedicated efforts. We discuss the limitation of the current work in Appendix I.
9
Published as a conference paper at ICLR 2022
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. MixHop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. In Kamalika Chaudhuri and Ruslan Salakhutdi-
nov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pages 21-29. PMLR, 09-15 JUn 2019. URL
http://proceedings.mlr.press/v97/abu-el-haija19a.html.
Aseem Baranwal, Kimon FoUntoUlakis, and AUkosh Jagannath. Graph convolUtion for semi-
sUpervised classification: Improved linear separability and oUt-of-distribUtion generalization.
arXiv preprint arXiv:2102.06966, 2021.
Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. Graph convolU-
tional encoders for syntax-aware neUral machine translation. arXiv preprint arXiv:1704.04675,
2017.
Joan BrUna, Wojciech Zaremba, ArthUr Szlam, and Yann LeCUn. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Ming Chen, Zhewei Wei, Zengfeng HUang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolUtional networks. In International Conference on Machine Learning, pages 1725-1735.
PMLR, 2020.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive Universal generalized pagerank
graph neUral network. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=n6jl7fLxrP.
Valerio Ciotti, Moreno BonaventUra, Vincenzo Nicosia, Pietro Panzarasa, and Vito Latora. Ho-
mophily and missing links in citation networks. EPJ Data Science, 5:1-14, 2016.
Enyan Dai and SUhang Wang. Say no to the discrimination: Learning fair graph neUral networks with
limited sensitive attribUte information. In Proceedings of the 14th ACM International Conference
on Web Search and Data Mining, pages 680-688, 2021.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. arXiv preprint arXiv:1606.09375, 2016.
Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic
block models. In NeurIPS, 2018.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pages 417-426, 2019.
Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7
(2):179-188, 1936.
Santo Fortunato and Darko Hric. Community detection in networks: A user guide. Physics reports,
659:1-44, 2016.
Elisabeth R Gerber, Adam Douglas Henry, and Mark Lubell. Political homophily and collaboration
in regional planning networks. American Journal of Political Science, 57(3):598-610, 2013.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pages
1263-1272. PMLR, 2017.
Jonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan Perozzi. Grale: Designing networks for
graph learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining, pages 2523-2532, 2020.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. arXiv preprint arXiv:1706.02216, 2017.
10
Published as a conference paper at ICLR 2022
Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard T. B. Ma, Hongzhi Chen, and Ming-Chang
Yang. Measuring and improving the use of graph information in graph neural networks. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=rkeIIkHKvS.
Ankit Jain and Piero Molino. Enhancing recommendations on uber eats with graph convolutional
networks.
Junteng Jia and Austin R Benson. A unifying generative model for graph learning algorithms: Label
propagation, graph convolutions, and combinations. arXiv preprint arXiv:2101.07730, 2021.
Weiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. arXiv preprint
arXiv:2101.11174, 2021.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations (ICLR), 2019.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and
Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong
simple methods. Advances in Neural Information Processing Systems, 34, 2021.
Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen
Chang, and Doina Precup. Is heterophily a real nightmare for graph neural networks to do node
classification? arXiv preprint arXiv:2109.05641, 2021.
Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph
neural networks as graph signal denoising. arXiv preprint arXiv:2010.01777, 2020.
Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving graph neural networks with simple
architecture design. arXiv preprint arXiv:2105.07634, 2021.
Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social
networks. Annual review of sociology, 27(1):415-444, 2001.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4602-4609,
2019.
Mark Newman. Networks. Oxford university press, 2018.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=S1e2agrFvS.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2):cnab014, 2021.
Aravind Sankar, Yozen Liu, Jun Yu, and Neil Shah. Graph neural networks for friend ranking in
large-scale social platforms. 2021.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2008.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
11
Published as a conference paper at ICLR 2022
Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Skeleton-based action recognition with directed
graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 7912-7921, 2019.
Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the limit of
graph neural networks by improving the assortativity of graphs with local mixing patterns. arXiv
preprint arXiv:2106.06586, 2021.
Xianfeng Tang, Yozen Liu, Neil Shah, Xiaolin Shi, Prasenjit Mitra, and Suhang Wang. Knowing
your fate: Friendship, action and temporal explanations for user engagement prediction on so-
cial apps. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 2269-2279, 2020a.
Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, Prasenjit Mitra,
and Suhang Wang. Investigating and mitigating degree-related biases in graph convoltuional net-
works. In Proceedings of the 29th ACM International Conference on Information & Knowledge
Management, pages 1435-1444, 2020b.
Anton Tsitsulin, Benedek Rozemberczki, John Palowitch, and Bryan Perozzi. Synthetic graph gen-
eration to benchmark graph learning. 2021.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
XUan WU, Lingxiao Zhao, and Leman AkoglU. A qUest for strUctUre: jointly learning the graph strUc-
tUre and semi-sUpervised classification. In Proceedings of the 27th ACM international conference
on information and knowledge management, pages 87-96, 2018.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km.
Rex Ying, RUining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and JUre Leskovec.
Graph convolUtional neUral networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
974-983, 2018.
Tong Zhao, Yozen LiU, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data aUg-
mentation for graph neUral networks. In AAAI, 2020.
Jie ZhoU, GanqU CUi, Shengding HU, Zhengyan Zhang, Cheng Yang, ZhiyUan LiU, Lifeng Wang,
Changcheng Li, and Maosong SUn. Graph neUral networks: A review of methods and applica-
tions. AI Open, 1:57-81, 2020.
Jiong ZhU, Ryan A Rossi, AnUp Rao, TUng Mai, Nedim Lipka, Nesreen K Ahmed, and Danai
KoUtra. Graph neUral networks with heterophily. arXiv preprint arXiv:2009.13566, 2020a.
Jiong ZhU, YUjUn Yan, Lingxiao Zhao, Mark Heimann, Leman AkoglU, and Danai KoUtra. Beyond
homophily in graph neUral networks: CUrrent limitations and effective designs. Advances in
Neural Information Processing Systems, 33, 2020b.
Meiqi ZhU, Xiao Wang, ChUan Shi, HoUye Ji, and Peng CUi. Interpreting and Unifying graph neUral
networks with an optimization framework. arXiv preprint arXiv:2101.11859, 2021.
Marinka Zitnik, Monica Agrawal, and JUre Leskovec. Modeling polypharmacy side effects with
graph convolUtional networks. Bioinformatics, 34(13):i457-i466, 2018.
12
Published as a conference paper at ICLR 2022
A Proof of Theorem 1
To prove Theorem 1, we first introduce the celebrated Hoeffding inequality below.
Lemma 1 (Hoeffding’s Inequality). Let Z1, . . . , Zn be independent bounded random variables with
Zi ∈ [a, b] for all i, where -∞ < a ≤ b < ∞. Then
P (n X(Zi- E [Zi]) ≥ t! ≤ exp (- (Tntap)
and
P (1 X(Zi- E [Zi]) ≤ -t! ≤ exp(—(Γn⅛)
for all t ≥ 0.
Theorem 1.	Consider a graph G = {V, E, {Fc, c ∈ C}, {Dc, c ∈ C}}, which follows Assump-
tions (1)-(4). For any node i ∈ V, the expectation of the pre-activation output of a single GCN
operation is given by
E[hi] = W (Ec~Dyi,χ~Fc[x]) .	(5)
and for any t > 0, the probability that the distance between the observation hi and its expectation
is larger than t is bounded by
P (khi - E[hi]k2 ≥ t) ≤ 2 ∙l ∙ exp (-2pdeWB2l) ,	(6)
where l is the feature dimensionality and ρ(W) denotes the largest singular value of W.
Proof. The expectation of hi can be derived as follows.
E [hi] = E
j∈N(i)
Ei WXj
=J^ X WEc~Dy∙,:
deg(i)	yi
eg	j∈N(i)
=W (Ec-Dyi ,x~Fc[x]).
,x~Fc[x]
We utilize Hoeffding’s Inequality to prove the bound in Eq. (6). Let xi [k], k = 1, . . . , l denote the
i-th element of x. Then, for any dimension k, {xj [k], j ∈ N (i)} is a set of independent bounded
random variables. Hence, directly applying Hoeffding’s inequality, for any t1 ≥ 0, we have the
following bound:
P (IX (Xj问 - E[χj网])≥ t) ≤ 2exp (- (de2B2*1)
j∈N(i)	2B
If
(xj-E[xj])
j∈N(i)
≥ √7t1, then at least for one k ∈ {1, ...,l}, the inequality
2
(Xj [k] - E [Xj [k]])I ≥ t1 holds. Hence, we have
j∈N(i)	I
P	(xj -E[xj])
j∈N(i)
≥ √7t1	≤ P
2
[∖ X (Xj [k] - E[Xj[k]]) ≥ t1
k=1
j∈N(i)
≤ X P (I X (Xj [k] - E[Xj[k]]) ≥ tι
k=1	Ij∈N(i)	I
2 ∙ l∙exp l 一
(deg(i))t21
2B2
13
Published as a conference paper at ICLR 2022
Let tι = √2ι, then We have
P	(xj -E[xj])
j∈N(i)
≥ t2 I ≤ 2 ∙ l ∙ exp (-
2
(deg(i))t22
2B2l
Furthermore, we have
khi -E[hi]k2
W	(xj -E[xj])
j∈N (i)	2
≤ kWk2	(xj - E [xj])
j∈N (i)
2
= ρ(W)	(xj - E [xj])	,
j∈N(i)
2
Where kWk2 is the matrix 2-norm of W. Note that the last line uses the identity kWk2 = ρ(W).
Then, for any t > 0, We have
P (khi-E[hi]k2 ≥ t) ≤ P (P(W)	X (Xj-E[xj])	≥ t
j∈N(i)	2
Which completes the proof.
P (	(xj -E[xj])
j∈N(i)
≤ 2 ∙ l∙exp
((deg(i7)F ∖
-2ρ2(W)B2l)
t
P(W)
≥
2
□
B	Proof of Theorem 2
Theorem 2.	Consider a graph G 〜CSBM(μι, μ2,p, q). For any node i in this graph, the linear
classifier defined by the decision boundary P has a lower probability to mis-classify hi than xi
when deg(i) > (p + q)2/(p - q)2.
Proof. We only prove for nodes from classes c0 since the case for nodes from classes c1 is symmetric
and the proof is exactly the same. For a node i ∈ C0, We have the folloWs
P(xi is mis-classified) = P(w>xi + b ≤ 0) for i ∈ C0	(7)
P(hi is mis-classified) = P(w>hi + b ≤ 0) for i ∈ C0,	(8)
where W and b = -w> (μι + μι) /2 is the parameters of the decision boundary P. we have that
P(w>hi + b ≤ 0)= P(w>Pdeg(i)hi + Pdeg(i)b ≤ 0).	(9)
We denote the scaled version of hi as hi = Vdeg(i)hi. Then, hi follows
hi = √deg(i)hi 〜N
VZdeg(i)(pμo + qμι)工
P + q	,
for i ∈ C0 .
(10)
Because of the scale in Eq. (9), the decision boundary for h0i is correspondingly moved to w>h0 +
y∕deg(i)b = 0. Now, since Xi and hi share the same variance, to compare the mis-classification
probabilities, we only need to compare the distance from their expected value to their corresponding
decision boundary. Specifically, the two distances are as follows:
diSxi =幽"
dish0
VZdeg(i)∣p - q∣ ∣∣μo - μ1k2
(p+q)
(11)
2
14
Published as a conference paper at ICLR 2022
The larger the distance is the smaller the mis-classification probability is. Hence, when dish0 <
disxi , h0i has a lower probability to be mis-classified than xi. Comparing the two distances, we
conclude that When deg(i) > (p+q)2, hi has a lower probability to be mis-classified than x〃
Together with Eq. 9, we have that
P(hi is mis-classified) < P(Xi is mis-classified) if deg(i) > (-----)
p-q
which completes the proof.
(12)
□
C Additional Details and Results for Section 3.3.1
C.1 Details on the generated graphs
In this subsection, we present the details of the graphs that we generate in Section 3.3.1. Specifically,
we detail the distributions {Dc, c ∈ C} used in the examples, the number of added edges K, and the
homophily ratio h. We provide the details for Cora and Citeseer in the following subsections.
Note that the choices of distributions shown here are for illustrative purposes, to coincide with
Observations 1 and 2. We adapted circulant matrix-like designs due to their simplicity.
C.1.1 CORA
There are 7 labels, which we denote as {0, 1, 2, 3, 4, 5, 6}. The distributions {Dc, c ∈ C} are listed
as follows. The values of K and the homophily ratio of their corresponding generated graphs are
shown in Table 3.
D0 : Categorical([0, 0.5, 0, 0, 0, 0, 0.5]),
D1 : Categorical([0.5, 0, 0.5, 0, 0, 0, 0]),
D2 : Categorical([0, 0.5, 0, 0.5, 0, 0, 0]),
D3 : Categorical([0, 0, 0.5, 0, 0.5, 0, 0]),
D4 : Categorical([0, 0, 0, 0.5, 0, 0.5, 0]),
D5 : Categorical([0, 0, 0, 0, 0.5, 0, 0.5]),
D6 : Categorical([0.5, 0, 0, 0, 0, 0.5, 0]).
Table 3: # of added edges (K) and homophily ratio (h) values for generated graphs based on Cora.
K 1003	2006	3009	4012	6018	8024	10030	12036	16048	20060	24072
^^h^^0.740^^0.681 ^^0.630^^0.587^^0.516^^0.460^^0.415^^0.378^^0.321 ^^0.279^^0.247
C.1.2 CI TE SEER
There are 6 labels, which we denote as {0, 1, 2, 3, 4, 5}. The distributions {Dc, c ∈ C} are listed
as follows. The values of K and the homophily ratio of their corresponding generated graphs are
shown in Table 4.
D0 : Categorical([0, 0.5, 0, 0, 0, 0.5]),
D1 : Categorical([0.5, 0, 0.5, 0, 0, 0]),
D2 : Categorical([0, 0.5, 0, 0.5, 0, 0]),
D3 : Categorical([0, 0, 0.5, 0, 0.5, 0]),
D4 : Categorical([0, 0, 0, 0.5, 0, 0.5]),
D5 : Categorical([0.5, 0, 0, 0, 0.5, 0]).
15
Published as a conference paper at ICLR 2022
Table 4: # of added edges (K) and homophily ratio (h) values for generated graphs based on
Citeseer
K	1204	2408	3612	4816	7224	9632	12040	14448	19264	24080	28896
h	0.650	0.581	0.527	0.481	0.410	0.357	0.317	0.284	0.236	0.202	0.176
C.2 Results on more datasets: CHAMELEON and SQUIRREL
We conduct similar experiments as those in Section 3.3.1 based on Chameleon and Squirrel.
Note that both Squirrel and Chameleon have 5 labels, which we denote as {0, 1, 2, 3, 4}. We
pre-define the same distributions for them as listed as follows. The values of K and the homophily
ratio of their corresponding generated graphs based on Squirrel and Chameleon are shown in
Table 5 and Table 6, respectively.
D0 : Categorical([0, 0.5, 0, 0, 0.5]),
D1 : Categorical([0.5, 0, 0.5, 0, 0]),
D2 : Categorical([0, 0.5, 0, 0.5, 0]),
D3 : Categorical([0, 0, 0.5, 0, 0.5]),
D4 : Categorical([0.5, 0, 0, 0.5, 0]).
Table 5: # of added edges (K) and homophily ratio (h) values for generated graphs based on
Squirrel.
K	12343	24686	37030	49374	61716	74060	86404	98746	111090	12434	135776
h	0.215	0.209	0.203	0.197	0.192	0.187	0.182	0.178	0.173	0.169	0.165
Table 6: # of added edges (K) and homophily ratio (h) values for generated graphs based on
Chameleon.
K	1932	3866	5798	7730	9964	11596	13528	15462	17394	19326	21260
h	0.223	0.217	0.210	0.205	0.199	0.194	0.189	0.184	0.180	0.176	0.172
∙87∙6
Ooo
0.5
0.222 0.215 0.209 0.203 0.190 0.192 0.1B7 0.1B2 0.17B 0.174 0.169 0.165
Homophily Ratio
(a) Synthetic graphs generated from Squirrel
5 0 5
7 76
Ooo
0.60-
0.23 0.223 0.217 0.21 0.205 0.199 0.194 O.1Θ9 0.184 0.1β 0.176 0.172
Homophily Ratio
(b) Synthetic graphs generated from Chameleon
Figure 6:	Performance of GCN on synthetic graphs with various homophily ratio.
The performance of the GCN model on these two sets of graphs (generated from Squirrel and
Chameleon) is shown in Figure 6. The observations are similar to what we found for those gen-
erated graphs based on Cora and Citeseer in Section 3.3.1. Note that the original Squirrel
and Chameleon graphs already have very low homophily, but we still observe a V -shape from
the figures. This is because there are some neighborhood patterns in the original graphs, which are
distinct from those that we designed for addition. Hence, when we add edges in the early stage,
the performance decreases. As we add more edges, the designed pattern starts to mask the original
patterns and the performance starts to increase.
16
Published as a conference paper at ICLR 2022
C.3 GCN’S PERFORMANCE IN THE LIMIT (AS K → ∞)
In this subsection, we illustrate that as K → ∞, the accuracy of the GCN model approaches 100%.
Specifically, we set K to a set of larger numbers as listed in Table 7. Ideally, when K → ∞,
the homophily ratio will approach 0 and the model performance will approach 100% (for diverse-
enough Dc). The performance of the GCN model on the graphs described in Table 3 and Table 7
are shown in Figure 7. Clearly, the performance of the GCN model approaches the maximum as we
continue to increase K.
Table 7: Extended # of added edges (K) and homophily ratio (h) values for generated graphs based
on Cora.
K	28084	32096	36108	40120	44132	48144	52156	56168	80240
h	0.272	0.248	0.228	0.211	0.196	0.183	0.172	0.162	0.120
Figure 7:	As the number of edges approaches K → ∞, the homophily ratio h → 0, and GCN’s
performance approaches 100%.
C.4 Details of Algorithm 2
The pseudo code to describe the process to generate graphs with a noise level γ is shown in Al-
gorithm 2. The only difference from Algorithm 1 is in Line 6-7, we randomly add edges if the
generated random number r is smaller than the pre-defined γ (with probability γ).
Alg. 2: HeteroPhiloUs Edge Addition with Noise
input ： G = {V, E}, K, {Dc}C=-1 and {Vc}C=-1
output: G0 = {V, E0}
Initialize G0 = {V, E}, k = 1 ;
while 1 ≤ k ≤ K do
Sample node i 〜Uniform(V);
Obtain the label, yi of node i;
Sample a number r 〜Uniform(0,1) ;	// Uniform (0,1) denotes the continuous
standard uniform distribution
if r ≤ γ then
L Sample a label c 〜Uniform (C \ {yi});
else
L Sample a label C 〜Dyi;
Sample node j 〜Uniform(Vc);
Update edge set E0 = E0 ∪ {(i, j)};
k — k + 1;
return G = {V, E0}
17
Published as a conference paper at ICLR 2022
D Experimental Details: Datasets, Models, and Results
We compare the standard GCN model (Kipf and Welling, 2016) with several recently proposed
methods specifically designed for heterophilous graphs including H2GCN (Zhu et al., 2020b), GPR-
GNN (Chien et al., 2021), and CPGNN (Zhu et al., 2020a). A brief introduction of these methods
can be found in Appendix D.2.
D. 1 Datasets
We give the number of nodes, edges, homophily ratios and distinct classes of datasets we used in
this paper in Table 8.
Table 8: Benchmark dataset summary statistics.
	Cora	Citeseer	Pubmed	Chameleon	Squirrel	Actor	Cornell	Wisconsin	Texas
# Nodes (|V |)	2708	3327	19717	2277	5201	7600	183	251	183
# Edges (|E |)	5278	4676	44327	31421	198493	26752	280	466	295
Homophily Ratio (h)	0.81	0.74	0.80	0.23	0.22	0.22	0.3	0.21	0.11
# Classes (|C|)	7	6	3	5	5	5	5	5	5
D.2 Models
•	H2GCN (Zhu et al., 2020b) specifically designed several architectures to deal with heterophilous
graphs, which include ego- and neighbor-embedding separation (skip connection), aggregation
from higher-order neighborhoods, and combination of intermediate representations. We include
two variants H2GCN-1 and H2GCN-2 with 1 or 2 steps of aggregations, respectively. We adopt
the code published by the authors at https://github.com/GemsLab/H2GCN.
•	GPR-GNN (Chien et al., 2021) performs feature aggregation for multiple steps and then lin-
early combines the features aggregated with different steps. The weights of the linear com-
bination are learned during the model training. Note that it also includes the original fea-
tures before aggregation in the combination. We adopt the code published by the authors at
https://github.com/jianhao2016/GPRGNN.
•	CPGNN (Zhu et al., 2020a) incorporates the label compatibility matrix to capture the connec-
tion information between classes. We adopted two variants of CPGNN that utilize MLP and
ChebyNet (Defferrard et al., 2016) as base models to pre-calculate the compatibility matrix, re-
spectively. We use two aggregation layers for both variants. We adopt the code published by the
authors at https://github.com/GemsLab/CPGNN.
D.3 MLP+GCN
We implement a simple method to linearly combine the learned features from the GCN model and
an MLP model. Let H^C N ∈ RlVl×lCl denote the output features from a 2-layer GCN model, where
|V | and |C | denote the number of nodes and the number of classes, respectively. Similarly, we use
HMLP ∈ RlVl×lCl to denote the features output from a 2-layer MLP model. We then combine them
for classification. The process can be described as follows.
H = α ∙ H(2CN + (1- α) ∙ HMLp,	(⑶
where α is a hyperparameter balancing the two components. We then apply a row-wise softmax to
each row of H to perform the classification.
D.4 Parameter Tuning and Resources Used
We tune parameters for GCN, GPR-GCN, CPGNN, and MLP+GCN from the following options:
•	learning rate: {0.002, 0.005, 0.01, 0.05}
•	weight decay {5e-04, 5e-05, 5e-06, 5e-07, 5e-08, 1e-05, 0}
•	dropout rate: {0, 0.2, 0.5, 0.8}.
For GPR-GNN, we use the “PPR” as the initialization for the coefficients. For MLP+GCN, we tune
α from {0.2, 0.4, 0.6, 0.8, 1}. Note that the parameter search range encompasses the range adopted
in the original papers to avoid unfairness issues.
18
Published as a conference paper at ICLR 2022
All experiments are run on a cluster equipped with Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
CPUs and NVIDIA Tesla K80 GPUs.
D.5 More Results
	Cora	Citeseer	Pubmed	Chameleon	Squirrel	Actor	Cornell	Wisconsin	Texas
GCN	87.12 ± 1.38	76.50 ± 1.61	88.52 ± 0.41	67.96 ± 1.82	54.47 ± 1.17	30.31 ± 0.98	59.35 ± 4.19	61.76 ± 6.15	63.81 ± 5.27
MLP	75.04 ± 1.97	72.40 ± 1.97	87.84 ± 0.30	48.11 ± 2.23	31.68 ± 1.90	36.17 ± 1.09	84.86 ± 6.04	86.29 ± 4.50	83.30 ± 4.54
MLP + GCN	87.01 ± 1.35	76.35 ± 1.85	89.77 ± 0.39	68.04 ± 1.86	54.48 ± 1.11	36.24 ± 1.09	84.82 ± 4.87	86.43 ± 4.00	83.60 ± 6.04
H2GCN-1	86.92 ± 1.37	77.07 ±1.64	89.40 ± 0.34	57.11 ± 1.58	36.42 ± 1.89	35.86 ±1.03	82.16 ± 6.00	86.67 ± 4.69	84.86 ± 6.77
H2GCN-2	87.81 ± 1.35	76.88 ± 1.77	89.59 ± 0.33	59.39 ± 1.98	37.90 ± 2.02	35.62 ± 1.30	82.16 ± 6.00	85.88 ± 4.22	82.16 ± 5.28
CPGNN-MLP	85.84 ± 1.20	74.80 ± 0.92	86.58 ± 0.37	54.53 ± 2.37	29.13 ± 1.57	35.76 ± 0.92	79.93 ± 6.12	84.58 ± 2.72	82.62 ± 6.88
CPGNN-Cheby	87.23 ± 1.31	76.64 ± 1.43	88.41 ± 0.33	65.17 ± 3.17	29.25 ± 4.17	34.28 ± 0.77	75.08 ± 7.51	79.19 ± 2.80	75.96 ± 5.66
GPR-GNN	86.79 ± 1.27	75.55 ± 1.56	86.79 ± 0.55	66.31 ± 2.05	50.56 ± 1.51	33.94 ± 0.95	79.27 ± 6.03	83.73 ± 4.02	84.43 ± 4.10
E Heatmaps for Other Benchmarks
We provide the heatmaps for Citeseer and Pubmed in Figure 8 and those for Squirrel,
Texas, and Wisconsin in Figure 9. For the Citeseer and Pubmed, which have high ho-
mophily, the observations are similar to those of Cora as we described in Section 4.2. For
Squirrel, there are some patterns; the intra-class similarity is generally higher than inter-class
similarities. However, these patterns are not very strong, i.e, the differences between them are not
very large, which means that the neighborhood patterns of different labels are not very distinguish-
able from each other. This substantiates the middling performance of GCN on Squirrel. Both
Texas and Wisconsin are very small, with 183 nodes, 295 edges and 251 nodes, 466 edges,
respectively. The average degree is extremely small (< 2). Hence, the similarities presented in
the heatmap may present strong bias. Especially, in Texas, there is only 1 node with label 1. In
Wisconsin, there are only 10 nodes with label 0.
(a) Citeseer
(b) Pubmed
Figure 8: Cross-class neighborhood similarity on Citeseer and Pubmed. On both graphs, the
intra-class similarity is clearly higher than the inter-class ones.
F Extending Theorem 2 to Multiple Classes
Below, we provide a proof sketch and illustration for an extension to Theorem 2’s main results in a
multi-class context. Specifically, we consider a special case for more tractable analysis.
Consider a K -class CSBM. The nodes in the generated graphs consist of K disjoint sets of the same
size C1 , . . . , CK corresponding to the K classes, respectively. Edges are generated according to an
intra-class probability p and an inter-class probability q. Specifically, for any two nodes in the graph,
if they are from the same class, then an edge is generated to connect them with probability p, other-
wise, the probability is q. For each node i, its initial associated features xi ∈ Rl are sampled from
a GaUssian distribution Xi 〜N(μ, I), where μ = μk ∈ Rl for i ∈ Ck with k ∈ {1,..., K} and
μz = μw ∀z, W ∈ {1,...,K}. We further assume that the distance between the mean of distribu-
tions corresponding to any two classes is equivalent, i.e, kuz - uw k2 = D, ∀z, w ∈ {1, . . . , K},
where D is a positive constant. We illustrate the 3-classes case in Figure 10, where we use the dashed
circles to demonstrate the standard deviation. Note that the K classes of the CSBM model are sym-
metric to each other. Hence, the optimal decision boundary of the K classes are defined by a set
19
Published as a conference paper at ICLR 2022

O
1
4
(a) Squirrel
(b) Texas
Figure 9: Cross-class neighborhood similarity on Squirrel, Texas and Wisconsin. The inter-
class similarity on Squirrel is slightly higher than intra-class similarity for most classes, which
substantiates the middling performance of GCN. Both Texas and Wisconsin are quite small,
hence the cross-class similarity in these two graphs present severe bias and may not provide precise
information about these graphs.
(c) Wisconsin
of K2 hyperplanes (see Figure 10 for an example), where each hyperplane equivalently separates
two classes. Similar to the analysis in the binary case (see the description below Proposition 1),
for any given two classes Cz and Cw, the hyperplane is P = {x∣w>X - w>(μz + μw)/2} with
W = (Uz - Uw)/kuz - Uw)∣∣2, which is orthogonal to (μz - μw) and going through (μz + μw)/2.
For example, in Figure 10, the decision boundaries separate the entire space to 3 areas correspond-
ing to the 3 classes. Each decision boundary is defined by a hyperplane equally separating two
classes. For example, the descision bounadry between c1 and c2 is orthogonal to (μ1 — μ2) and
going through the middle point (μ1 + μ2)/2. Clearly, the linear separability is dependent on the
distance D between the classes and also the standard deviations of each class’ node features. More
specifically, linear separability is favored by a larger distance D and smaller standard deviation.
Figure 10: 3-class CSBM in 2-dimensional space. The dashed circles demonstrate the standard
deviation for each class. The entire space is split to three areas (indicated by different colors) corre-
sponding to the three classes for optimal decision making. The equilateral triangle indicates that the
distance between the means of any two classes is the same.
Next, we discuss how the GCN operation affects the linear separability. Overall, we want to demon-
strate the following: (i) after the GCN operation, the classes are still symmetric to each other; (ii)
the GCN operation will reduce the distance between the classes (in terms of the expectation of the
output embeddings) and reduce the standard deviation of the classes; and (iii) reducing the distance
impairs the separability while reducing the standard deviation improves the separability. Hence, we
20
Published as a conference paper at ICLR 2022
analyze the two effect and provide a threshold. We describe these three items in a more detailed way
as follows.
Preservation of Symmetry. For any class cz, z ∈ {1, . . . , K}, its neighborhood label distribution
Dcz can be described by a vector where only the z-th element equals .+(——耳 and all other
elements equal to +(Kq-I) . We consider the aggression process hi = > 1(i)P Xj. Then,
p	q	g	j∈N(i)
for a node i with label cz , its features obtained after this process follow the following Gaussian
distribution.
hi ~ N
∕pμζ +	Σ	qμk	∖
_____k∈□,…Khk=Z	I ] F . r
P +(K -I)q	，pdeg(i)y，	z
(14)
Specifically, we denote the expectation of class cz after the GCN operation as Ecz [h] =
pμz +	P	qμk
------kp+1(,κK:,q=z . We next show that the distance between any two classes is the same.
Specifically, for any two classes cz and cw , after the GCN operation, the distance between their
expectation is as follows.
kEcz[h]-Eck[h]k2=
pμz +	∑	qμk	pμw +	∑	qμk
k∈{1,...K },k6=z	k∈{1,...K },k6=w
—
p+(K-1)q
p+(K-1)q
2
=|(p -q)K-μw)	= TKMr kμz-μw k2	(15)
p+ (K - 1)q 2 p+ (K - 1)q
Note that We have ∣∣μz - μw∣b = D for any pair of classes Cz and Cw. Hence, after the GCN
operation, the distance between any two classes is still the same. Thus, the classes are symmetric
two each other.
Reducing inter-class distance and intra-class standard deviation. According to Eq (15), after
the graph convolution operation, the distance between any two classes is reduced by a factor of
p+lρ--1)q. On the other hand, according to Eq. (14), the standard deviation depends on the degree
of nodes. Specifically, for node i with degree deg(i), its standard deviation is reduced by a factor
of y/deg(i).
Implications for separability. For a node i, the probability of being mis-classified is the prob-
ability of its embedding falling out of its corresponding decision area. This probability depends
on both the inter-class distance between classes (in terms of means) and the intra-class standard
deviation. To compare the linear separability before and after the graph convolution operation, we
scale the distance between classes before and after the graph convolution operation to be the same.
Specifically, We scale hi as hi = p+∣lpK-∣1)q hi. Note that classifying hi is equivalent to classifying
hi. Based on h0, the distance between any two classes Cz, Cw is scaled to ∣∣μz - μw ∣∣2 = D, which
is equivalent to the class distance before the graph convolution operation. Correspondingly, the
standard deviation for hi equals to p+(κ-1)q ∙ / I / .、. Now, to compare the mis-classification
i	|p-q|	deg(i)
probability before and after the graph convolution, we only need to compare the standard devia-
tions as the distances between classes in these two scenarios has been scaled to the same. More
specifically, when p+∣P--∣ )q ∙ √；(,)< 1, the mis-classification probability for node i is re-
duced after the GCN model, otherwise, the mis-classification probability is increased after the
GCN model. In other words, for nodes with degree larger than (p+(p--)?' , the mis-classification
rate can be reduced after the graph convolution operation. This thereold is similar to the one we
developed in Theorem 2 and similar analysis/discussions as those for Theorem 2 follows for the
this multiple-class case.
G Other Metrics Than Cosine Similarity
The choice of similarity measure would not affect the results and conclusions significantly. We
empirically demonstrate this argument by investigating two other metrics: Euclidean distance and
Hellinger distance. The heatmaps based on these two metrics for Chamelon dataset is shown in
Figure 11 in Appendix G. The patterns demonstrated in these two heatmaps are similar to those
21
Published as a conference paper at ICLR 2022
observed in Figure 5(b), where cosine similarity is adopted. Note that larger distance means lower
similarity. Hence, the numbers in Figure 11 should be interpreted in the opposite way as those in
Figure 5.
(a) Euclidean Distance
Figure 11: Cross-class neighborhood patterns on Chameleon based on different metrics than co-
sine similarity metric.
O
1
2
3
1.25
1.20
1.15
1.10
1.05
0.85
0.89
0.93
1.00
0.95
0.89
0.8
0.88
0.90
0.85
0.93
0.88
0.86
1.05
1.25
O '	1	'	2	'	3	'	4
(b) Hellinger distance
1.25
ll0.80
H Additional Experiments with Different Settings
In this section, we discuss additional experiments for Section 3.3.1.
H.1 Generating Graphs with Other Neighborhood Distributions
In this section, we extend the experiments in Section 3.3.1 by including more patterns for neighbor-
hood distributions. We aim to illustrate if the neighborhood distribution for different classes (labels)
are sufficiently distinguishable from each other, we should generally observe similar V -shape curves
as in Figure 3 in Section 3.3.1. However, it is impractical to enumerate all possible neighborhood
distributions. Hence, in this section, we include two additional neighborhood distribution patterns
in Section H.1.1 and two extreme patterns in Section H.1.2.
H.1.1 Additional Patterns
Here, for both Cora and Citeseer, we adopt two additional sets of neighborhood distributions
for adding new edges. For convenience, for Cora, we name the two neighborhood distribution
patterns as Cora Neighborhood Distribution Pattern 1 and Cora Neighborhood Distribution Pat-
tern 2. Similarly, for Citeseer, we name the two neighborhood distribution patterns as Citeseer
Neighborhood Distribution Pattern 1 and Citeseer Neighborhood Distribution Pattern 2. We follow
|C|-1
Algorithm 1 to generate graphs while utilizing these neighborhood distributions as the {Dc}c=0
for Algorithm 1.
The two neighborhood distribution patterns for Cora are listed as bellow. Figure 12 and Figure 13
show GCN’s performance on graphs generated from Cora following Cora Neighborhood Distribu-
tion Pattern 1 and Cora Neighborhood Distribution Pattern 2, respectively.
Cora Neighborhood Distribution Pattern 1:
22
Published as a conference paper at ICLR 2022
Do: CategOriCal([0,3, 3,3,0,0,0]),
DI： CategOrical([3,0,0,0,3, 3,0]),
D2 ： CategOriCal([1,0,0,0,0, 1, 1]),
D3 ： Categorical([1,0,0,0, 1,0, 3]),
D4 ： CategOrical([o, 3,0,3,0,3,0]),
D5 ： CategOHeal。0,3, 3,0,3,0,0]),
D6 ： CategOriCal([0,0, 2,2,0,0,0]).
Figure 12: Performance of GCN on synthetic
graphs from Cora. Graphs are generated follow-
ing Cora Neighborhood Distribution Pattern 1
Cora Neighborhood Distribution Pattern 2:
Do ： CategOriCal([0, 2,0, 2,0,0,0]),
Di ： CategOriCal([2,0,0,2,0,0,0]),
D2 ： CategOriCal([0, 0, 0, 1, 0, 0, 0]),
D3 ： CategOriCal([1, 1, 1,0, 1, 1], 0),
555	55
D4 ： CategOriCal([0,0,0,1,0, 1,0]),
Figure 13: Performance of GCN on synthetic
graphs from Cora. Graphs are generated follow-
ing Cora Neighborhood Distribution Pattern 2
D5 ： CategOrieal([0,3,3,0,3,0,0]),
D6 ： CategOriCal([0, 0, 0, 0, 0, 0, 1]).
Clearly, the results demonstrated in Figure 12 and Figure 13 are similar to the black curve (γ=0)
in 3a. More specifically, they present V -shape curves.
The two neighborhood distribution patterns for Citeseer are listed as bellow. Figure 14 and Fig-
ure 15 show GCN’s performance on graphs generated from Citeseer following Citeseer Neigh-
borhood Distribution Pattern 1 and Citeseer Neighborhood Distribution Pattern 2, respectively.
Citeseer Neighborhood Distribution Pattern 1:
Do ： CategOriCal([0, 2,1,0,0,0]),
D1 ： CategOriCal([1,0,0,0, 1, 31),
D2 ： CategOriCal([2,0,0,0,0, 2]),
D3 ： CategOriCal([0, 0, 0, 0, 1, 0),
D4 ： CategOriCal([0,3,0,3,0,3]),
D5 ： CategOriCal([0, 1,1,0, 3,0]),
Figure 14: Performance of GCN on synthetic
graphs from Citeseer. Graphs are gener-
ated following Citeseer Neighborhood Distribu-
tion Pattern 1
Citeseer Neighborhood Distribution Pattern 2:
23
Published as a conference paper at ICLR 2022
Do : Categorical([0, 2,0, 2,0,0]),
DI： Categorical。/, 0,0,32,0]),
D2 : Categorical([0, 0, 0, 1, 0, 0]),
D3 ： CategoriCal([1,1,1,0, 5,1),
D4 ： CategoriCal([0,0,0, 1,0, 2]),
D5 ： Categorical([0,0,0, 1,2,0]),
Figure 15: Performance of GCN on synthetic
graphs from Citeseer. Graphs are gener-
ated following Citeseer Neighborhood Distribu-
tion Pattern 2
Clearly, the results demonstrated in Figure 14 and Figure 15 are similar to the black curve (γ=0)
in 3b. More specifically, they present V -shape curves.
H.1.2 Extreme Neighborhood Distribution Patterns
In this subsection, we further extend the experiments in Section 3.3.1 by investigating “extreme”
neighborhood distribution patterns suggested by WXGg. More specifically, these two patterns are
“a given label are connected to a single different label” and “a given label are connected to all
other labels other excluding its own label”. For convenience, we denote these two types of neigh-
borhood distributions as single and all, respectively. We utilize these neighborhood distributions
as {Dc}|cC=|-0 1 for generating synthetic graphs. Next, we first present the results for Cora with
analysis for both the single and all neighborhood distribution patterns. Then, we present the
results for Citeseer but omit analysis and discussion since the observations are similar to
those we make for Cora.
Note that to ensure “a label is only connected to a single different label”, we need to group the labels
into pairs and “connect them”. Since there are different ways to group labels into pairs, there exists
various ways to formulate the single neighborhood distributions. We demonstrate one of the single
neighborhood distributions as all the other possible are symmetric to each other. For Cora, the
single neighborhood distribution we adopted is as follows. Note that there are 7 labels in Cora, and
we could not pair all the labels. Thus, we leave one of labels (label 4 in our setting) untouched, i.e,
it is not connect to other labels during the edge addition process.
D0 ： Categorical([0, 1, 0, 0, 0, 0, 0]),
D1 ： Categorical([1, 0, 0, 0, 0, 0, 0]),
D2 ： Categorical([0, 0, 0, 1, 0, 0, 0]),
D3 ： Categorical([0, 0, 1, 0, 0, 0, 0]),
D4 ： Categorical([0, 0, 0, 0, 0, 0, 0]),
D5 ： Categorical([0, 0, 0, 0, 0, 0, 1]),
D6 ： Categorical([0, 0, 0, 0, 0, 1, 0]).
The GCN’s performance on these graphs generated from Cora following the single neighborhood
distribution pattern is shown in Figure 16a. It clearly presents a V -shape curve. Almost perfect
performance can be achieved when the homophily ratio gets close to 0. This is because the single
neighborhood distributions for different labels are clearly distinguishable from each other. We fur-
ther demonstrate the heatmap of cross-class similarity for the generated graph with homophily ratio
0.07 (the right most point in Figure 16a) in Figure 16b. Clearly, this graph has very high intra-class
similarity and very low inter-class similarity, which explains the good performance.
For Cora, the all neighborhood distribution patterns can be described as follows.
Do ： Categorical([0, 1, 1, 1, 1,1,1]),
666666
Di ： Categorical([1,0, 1, 1, 1,1,1]),
6	66666
TD ： Categorical([1,1,0, 1, 1,1,1]),
66	6666
24
Published as a conference paper at ICLR 2022
(a) Performance of GCN on synthetic graphs from Cora. Graphs are gen-
erated following single neighborhood distribution.
(b) Cross-class neighborhood
similarity for the graph generated
from Cora following single with
homophily ratio 0.07 (the right
most point in Figure 16a).
Figure 16: Cora: single neighborhood distribution pattern
D3 : Categorical([-, -, -, 0,-,-,-]),
666	666
D4: CategOricalaI, 1, 6,6，0,6, 1]),
D5: CategOrical([6, 1,6, 1,6,0,1]),
D : CategOriCal([1, 1, 1, 1, 1, 1,0]).
666666
The GCN’s performance on these graphs generated from Cora following the all neighborhood
distribution pattern is shown in Figure 17a. Again, it clearly presents a V -shape curve. However, the
performance is not perfectly good even when we add extremely large number of edges. For example,
for the right most point in Figure 17a, we add almost as 50 times many as edges into the graph and
the homophily ratio is reduced to 0.03 while GCN’s performance for it is only around 70%. This
is because the all neighborhood distributions for different labels are not easily distinguishable from
each other. More specifically, any two neighborhood distributions for different labels in all are very
similar each other (they share “4 labels”). We further empirically demonstrate this by providing the
heatmap of cross-class similarity for the generated graph with homophily ratio 0.03 (the right most
point in Figure 17a) in Figure 17b. As per our discussion in Observation 2, this graph is with not
such “good” heterophily and GCNs cannot produce perfect performance for it. This observation
further demonstrates our key argument that the distinguishability of the distributions for different
labels are important for performance.
For Citeseer, the single neighborhood distribution we adopted is as follows. The GCN’s perfor-
mance on these graphs generated from Citeseer following the single neighborhood distribution
pattern is shown in Figure 18a. The heatmap of cross-class similarity for the generated graph with
homophily ratio 0.06 (the right most point in Figure 18a) in Figure 18b.
D0 : CategOrical([0, 1, 0, 0, 0, 0]),
D1 : CategOrical([1, 0, 0, 0, 0, 0]),
D2 : CategOrical([0, 0, 0, 1, 0, 0]),
D3 : CategOrical([0, 0, 1, 0, 0, 0]),
D4 : CategOrical([0, 0, 0, 0, 0, 1]),
D5 : CategOrical([0, 0, 0, 0, 1, 0]).
For Citeseer, the all neighborhood distribution patterns can be described as follows. The GCN’s
performance on these graphs generated from Citeseer following the all neighborhood distribu-
tion pattern is shown in Figure 19a. The heatmap of cross-class similarity for the generated graph
with homophily ratio 0.01 (the right most point in Figure 19a) in Figure 19b.
D0 : CategOrical([0, 1/5, 1/5, 1/5, 1/5, 1/5]),
25
Published as a conference paper at ICLR 2022
1.0
0.4
ɑ6 6
Aejn4φ
0.81 0.68 0.59 0.46 0.38 0.28 0.22 0.16 0.15 0.11 0.09 0.08 0.07 0.06 0.05 0.044 0.04 0.037 0.03
Homophily Ratio
(a) Performance of GCN on synthetic graphs from Cora. Graphs are gen-
erated following all neighborhood distribution.
6 4 2
■ ■ ■
Ooo
0.0
(b) Cross-class neighborhood
similarity for the graph generated
from Cora following all with
homophily ratio 0.03 (the right
most point in Figure 17a).
Figure 17: Cora: all neighborhood distribution pattern
(a) Performance of GCN on synthetic graphs from Citeseer. Graphs
are generated following single neighborhood distribution.
(b) Cross-class neighborhood
similarity for the graph generated
from Citeseer following
single with homophily ratio 0.06.
2	3	4	5	6
Figure 18:	Citeseer: single neighborhood distribution pattern
D1 : Categorical([1/5, 0, 1/5, 1/5, 1/5, 1/5]),
D2 : Categorical([1/5, 1/5, 0, 1/5, 1/5, 1/5]),
D3 : Categorical([1/5, 1/5, 1/5, 0, 1/5, 1/5]),
D4 : Categorical([1/5, 1/5, 1/5, 1/5, 0, 1/5]),
D5 : Categorical([1/5, 1/5, 1/5, 1/5, 1/5, 0]).
Similar observations as those we made for Cora can be made for Citeseer, hence we do not
repeat the analysis here.
I Limitation
Though we provide new perspectives and understandings of GCN’s performance on heterophilous
graphs, our work has some limitations. To make the theoretical analysis more feasible, we make a
few assumptions. We dropped the non-linearity in the analysis since the main focus of this paper is
the aggregation part of GCN. While the experiment results empirically demonstrate that our analysis
seems to hold with non-linearity, more formal investigation for GCN with non-linearity is valuable.
Our analysis in Theorem 2 assumes the independence between features, which limits the generality
of the analysis and we would like to conduct further instigation to more general case. We provide
theoretical understanding on GCN’s performance based on CSBM, which stands for a type of graphs
26
Published as a conference paper at ICLR 2022
(a) Performance of GCN on synthetic graphs from Citeseer. Graphs
are generated following all neighborhood distribution.
d 1.0	0.81	0.82	0.81	0.82	ɪl
N 0.81	0.99	0.81	0.8	0.8	■
I 0.82	0.81	0.99	0.8	0.81	0.81 售
I 0.81	0.8	0.8	0.99	0.81	08 .g
H 0.82	0.8	0.81	0.81	0.99	2111
L	0.8 BH	0.81	0.8	0.8 KS	
(b) Cross-class neighborhood
similarity for the graph generated
from Citeseer following all
with homophily ratio 0.01.
Figure 19:	Citeseer: all neighborhood distribution pattern
attracting increasing attention in the research community. However, it is not ideal for modeling
sparse graphs, which are commonly observed in the real-world. Hence, itis important to devote more
efforts to analyzing more general graphs. We believe our results established a solid initial study for
further investigation. Finally, our current theoretical analysis majorly focuses on the GCN model;
we hope to extend this analysis in the future to more general message-passing neural networks.
J	Broader Impact
Graph neural networks (GNNs) are a prominent architecture for modeling and understanding graph-
structured data in a variety of practical applications. Most GNNs have a natural inductive bias
towards leveraging graph neighborhood information to make inferences, which can exacerbate unfair
or biased outcomes during inference, especially when such neighborhoods are formed according
to inherently biased upstream processes, e.g. rich-get-richer phenomena and other disparities in
the opportunities to “connect” to other nodes: For example, older papers garner more citations
than newer ones, and are hence likely to have a higher in-degree in citation networks and hence
benefit more from neighborhood information; similar analogs can be drawn for more established
webpages attracting more attention in search results. Professional networking (“ability to connect”)
may be easier for those individuals (nodes) who are at top-tier, well-funded universities compared
to those who are not. Such factors influence network formation, sparsity, and thus GNN inference
quality simply due to network topology (Tang et al., 2020b). Given these acknowledged issues,
GNNs are still used in applications including ranking (Sankar et al., 2021), recommendation(Jain
and Molino), engagement prediction (Tang et al., 2020a), traffic modeling(Jiang and Luo, 2021),
search and discovery (Ying et al., 2018) and more, and when unchecked, suffer traditional machine
learning unfairness issues (Dai and Wang, 2021).
Despite these practical impacts, the prominent notion in prior literature in this space has been that
such methods are inapplicable or perform poorly on heterophilous graphs, and this may have miti-
gated practitioners’ interests in applying such methods for ML problems in those domains conven-
tionally considered heterophily-dominant (e.g. dating networks). Our work shows that this notion is
misleading, and that heterophily and homophily are not themselves responsible for good or bad in-
ference performance. We anticipate this finding to be helpful in furthering research into the capacity
of GNN models to work in diverse data settings, and emphasize that our work provides an under-
standing, rather than a new methodology or approach, and thus do not anticipate negative broader
impacts from our findings.
Acknowledgements
This research is supported by the National Science Foundation (NSF) under grant numbers
IIS1714741, CNS1815636, IIS1845081, IIS1907704, IIS1928278, IIS1955285, IOS2107215, and
27
Published as a conference paper at ICLR 2022
IOS2035472, the Army Research Office (ARO) under grant number W911NF-21-1-0198, the Home
Depot, Cisco Systems Inc and Snap Inc.
28