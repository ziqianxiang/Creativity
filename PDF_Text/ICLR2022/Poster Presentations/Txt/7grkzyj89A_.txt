Published as a conference paper at ICLR 2022
Generalization through the Lens of Leave-
one-out Error
Gregor Bachmann a, Thomas Hofmanna, and AUrelien LUcchib
aDepartment of Computer Science, ETH Zurich, Switzerland
bDepartment of Mathematics and CompUter Science, University of Basel ,
{gregor.bachmann, thomas.hofmann}@inf.ethz.ch
aurelien.lucchi@unibas.ch
Ab stract
Despite the tremendous empirical success of deep learning models to solve vari-
ous learning tasks, our theoretical understanding of their generalization ability is
very limited. Classical generalization bounds based on tools such as the VC di-
mension or Rademacher complexity, are so far unsuitable for deep models and it
is doubtful that these techniques can yield tight bounds even in the most idealistic
settings (Nagarajan & Kolter, 2019). In this work, we instead revisit the concept
of leave-one-out (LOO) error to measure the generalization ability of deep models
in the so-called kernel regime. While popular in statistics, the LOO error has been
largely overlooked in the context of deep learning. By building upon the recently
established connection between neural networks and kernel learning, we leverage
the closed-form expression for the leave-one-out error, giving us access to an effi-
cient proxy for the test error. We show both theoretically and empirically that the
leave-one-out error is capable of capturing various phenomena in generalization
theory, such as double descent, random labels or transfer learning. Our work there-
fore demonstrates that the leave-one-out error provides a tractable way to estimate
the generalization ability of deep neural networks in the kernel regime, opening
the door to potential, new research directions in the field of generalization.
1	Introduction
Neural networks have achieved astonishing performance across many learning tasks such as in com-
puter vision (He et al., 2016), natural language processing (Devlin et al., 2019) and graph learning
(Kipf & Welling, 2017) among many others. Despite the large overparametrized nature of these
models, they have shown a surprising ability to generalize well to unseen data. The theoretical un-
derstanding of this generalization ability has been an active area of research where contributions have
been made using diverse analytical tools (Arora et al., 2018; Bartlett et al., 2019; 2017; Neyshabur
et al., 2015; 2018). Yet, the theoretical bounds derived by these approaches typically suffer from one
or several of the following limitations: i) they are known to be loose or even vacuous (Jiang et al.,
2019), ii) they only apply to randomized networks (Dziugaite & Roy, 2018; 2017), and iii) they
rely on the concept of uniform convergence which was shown to be non-robust against adversarial
perturbations (Nagarajan & Kolter, 2019).
In this work, we revisit the concept of leave-one-out error (LOO) and demonstrate its surprising
ability to predict generalization for deep neural networks in the so-called kernel regime. This object
is an important statistical estimator of the generalization performance of an algorithm that is the-
oretically motivated by a connection to the concept of uniform stability (Pontil, 2002). It has also
recently been advocated by Nagarajan & Kolter (2019) as a potential substitute for uniform conver-
gence bounds. Despite being popular in classical statistics (Cawley & Talbot, 2003; Vehtari et al.,
2016; Fukunaga & Hummels, 1989; Zhang, 2003), LOO has largely been overlooked in the context
of deep learning, likely due to its high computational cost. However, recent advances from the neural
tangent kernel perspective (Jacot et al., 2018) render the LOO error all of a sudden tractable thanks
to the availability of a closed-form expression due to Stone (1974, Eq. 3.13). While the role of the
LOO error as an estimator of the generalization performance is debated in the statistics community
1
Published as a conference paper at ICLR 2022
(Zhang & Yang, 2015; Bengio & Grandvalet, 2004; Breiman, 1996; Kearns & Ron, 1999), the recent
work by Patil et al. (2021) established a consistency result in the case of ridge regression, ensuring
that LOO converges to the generalization error in the large sample limit, under mild assumptions on
the data distribution.
Inspired by these recent advances, in this work, we investigate the use of LOO as a generalization
measure for deep neural networks in the kernel regime. We find that LOO is a surprisingly rich
descriptor of the generalization error, capturing a wide range of phenomena such as random label
fitting, double descent and transfer learning. Specifically, we make the following contributions:
•	We extend the LOO expression for the multi-class setting to the case of zero regularization
and derive a new closed-form formula for the resulting LOO accuracy.
•	We investigate both the LOO loss and accuracy in the context of deep learning through
the lens of the NTK, and demonstrate empirically that they both capture the generalization
ability of networks in the kernel regime in a variety of settings.
•	We showcase the utility of LOO for practical networks by accurately predicting their trans-
fer learning performance.
•	We build on the mathematically convenient form of the LOO loss to derive some novel
insights into double descent and the role of regularization.
Our work is structured as follows. We give an overview of related works in Section 2. In Section 3
we introduce the setting along with the LOO error and showcase our extensions to the multi-class
case and LOO accuracy. Then, in Section 4 we analyze the predictive power of LOO in various
settings and present our theoretical results on double descent. We discuss our findings and future
work in Section 5. We release the code for our numerical experiments on Github1.
2	Related Work
Originally introduced by Lachenbruch (1967); Mosteller & Tukey (1968), the leave-one-out error is
a standard tool in the field of statistics that has been used to study and improve a variety of mod-
els, ranging from support vector machines (Weston, 1999), Fisher discriminant analysis (Cawley
& Talbot, 2003), linear regression (Hastie et al., 2020; Patil et al., 2021) to non-parametric density
estimation (Fukunaga & Hummels, 1989). Various theoretical perspectives have justified the use
of the LOO error, including for instance the framework of uniform stability (Bousquet & Elisseeff,
2002) or generalization bounds for KNN models (Devroye & Wagner, 1979). Elisseeff et al. (2003)
focused on the stability of a learning algorithm to demonstrate a formal link between LOO error
and generalization. The vanilla definition of the LOO requires access to a large number of trained
models which is typically computationally expensive. Crucially, Stone (1974) derived an elegant
closed-form formula for kernels, completely removing the need for training multiple models. In the
context of deep learning, the LOO error remains largely unexplored. Shekkizhar & Ortega (2020)
propose to fit polytope interpolators to a given neural network and estimate its resulting LOO error.
Alaa & van der Schaar (2020) rely on a Jackknife estimator, applied in a LOO fashion to obtain
better confidence intervals for Bayesian neural networks. Finally, we want to highlight the concur-
rent work of Zhang & Zhang (2021) who analyze the related concept of influence functions in the
context of deep models.
Recent works have established a connection between kernel regression and deep learning. Sur-
prisingly, it is shown that an infinitely-wide, fully-connected network behaves like a kernel both at
initialization (Neal, 1996; Lee et al., 2018; de G. Matthews et al., 2018) as well as during gradient
flow training (Jacot et al., 2018). Follow-up works have extended this result to the non-asymptotic
setting (Arora et al., 2019a; Lee et al., 2019), proving that wide enough networks trained with small
learning rates are in the so-called kernel regime, i.e. essentially evolving like their corresponding
tangent kernel. Moreover, the analysis has also been adapted to various architectures (Arora et al.,
2019a; Huang et al., 2020; Du et al., 2019; Hron et al., 2020; Yang, 2020) as well as discrete gradient
descent (Lee et al., 2019). The direct connection to the field of kernel regression enables the usage
of the closed-form expression for LOO error in the context of deep learning, which to the best of our
knowledge has not been explored yet.
1https://github.com/gregorbachmann/LeaveOneOut
2
Published as a conference paper at ICLR 2022
3	Setting and Background
In the following, we establish the notations required to describe the problem of interest and the
setting we consider. We study the standard learning setting, where we are given a dataset of input-
i.i.d.
target pairs S = {(xι, yι),..., (xn, yn)} where each (xi, yi) 〜 D for i = 1,...,n is distributed
according to some probability measure D. We refer to xi ∈ X ⊂ Rd as the input and to y ∈ Y ⊂
RC as the target. We will often use matrix notations to obtain more compact formulations, thus
summarizing all inputs and targets as matrices X ∈ Rn×d and Y ∈ Rn×C . We will mainly be
concerned with the task of multiclass classification, where targets yi are encoded as one-hot vectors.
We consider a function space F and an associated loss function Lf : X × Y -→ R that measures
how well a given function f ∈ F performs at predicting the targets. More concretely, we define the
regularized empirical error as
1n
LS : F → R, f → -	Lf(Xi, yi) + λQ(f),
n i=1
where λ > 0 is a regularization parameter and Ω : F → R is a functional. For any vector V ∈ RC,
let v* = argmaXk≤c Vk. Using this notation, We let af (x, y) := 1{f*伽)="*}. We then define the
empirical accuracy AS as the average number of instances for which f ∈ F provides the correct
prediction, i.e.
1n
AS : F → [0, 1], f 1 一 ɪ2 af (Xi, yi)
n i=1
Finally, we introduce a learning algorithm QF : (X × Y)n -→ F, that given a function class F
and a training set S ∈ (X × Y)n, chooses a function f ∈ F. We will exclusively be concerned with
learning through empirical risk minimization, i.e.
QF(S) := fS := argminf∈fLS(f),
and use the shortcut fs := fλ=0∙ In practice however, the most important measures are given by
the generalization loss and accuracy of the model, defined as
L : F→ R, f → E(x,y)〜D [Lf(X, y)] , A : F→ [0,1], f → P(x,y)〜D (f *(X)=心.
A central goal in machine learning is to control the difference between the empirical error LS (f)
and the generalization error L(f). In this work, we mainly focus on the mean squared loss,
1C
Lf(X, y) = 2 Efk(X) - yk)2,
k=1
and also treat classification as a regression task, as commonly done in the literature (Lee et al.,
2018; Chen et al., 2020; Arora et al., 2019a; Hui & Belkin, 2021; Bachmann et al., 2021). Finally,
we consider the function spaces induced by kernels and neural networks, presented in the following.
3.1	Kernel Learning
A kernel K : X × X -→ R is a symmetric, positive semi-definite function, i.e. K(X, X0) =
K(X0, X) ∀ X, X0 ∈ X and for any set {X1, . . . , Xn} ⊂ X, the matrix K ∈ Rn×n with entries
Kij = K(Xi, Xj) is positive semi-definite. It can be shown that a kernel induces a reproducing
kernel Hilbert space, a function space equipped with an inner product h∙, )h, containing elements
n
H = Cl ({f ： fk(∙) = X NkK(∙, Xi) for α ∈ Rn×C, Xi ∈ X})
i=1
where cl is the closure operator. Although H is an infinite dimensional space, it turns out that the
minimizer fS can be obtained in closed form for mean squared loss, given by
fS(x) = KT (K + λ1n)-1 Y -→→ fs(X) := KTKtY
3
Published as a conference paper at ICLR 2022
where the regularization functional is induced by the inner product of the space, Ω(f) = ||f ∣∣H,
K ∈ Rn×n has entries Kij = K(xi, Xj) and Kx ∈ Rn has entries (Kχ) = K(x, Xi). At denotes
the pseudo-inverse of A ∈ Rn×n. We will analyze both the regularized (fλ) and unregularized
model (fs) and their associated LOO errors in the following. For a more in-depth discussion of
kernels, we refer the reader to Hofmann et al. (2008); Paulsen & Raghupathi (2016).
3.2	Neural Networks and Associated Kernels
Another function class that has seen a rise in popularity is given by the family of fully-connected
neural networks of depth L ∈ N,
FNN
fθ : fθ(x) = W(L)σ W (L-1) . . .σ W(1)x . . . , for W (l) ∈ Rdl×dl-1 ,
for dl ∈ N, d0 = d, dL = C and σ : R -→ R is an element-wise non-linearity. We denote
by θ ∈ Rm the concatenation of all parameters (W(1),..., W(L)), where m = PL-o1 ddi+ι
is the number of parameters of the model. The parameters are initialized randomly according to
W(I) 〜 N(0, σW) for σw > 0 and then adjusted to optimize the empirical error LS, usually through
an iterative procedure using a variant of gradient descent. Typically, the entire vector θ is optimized
via gradient descent, leading to a highly non-convex problem which does not admit a closed-form
solution f, in contrast to kernel learning. Next, we will review settings under which the output of a
deep neural network can be approximated using a kernel formulation.
Random Features. Instead of training all the parameters θ, we restrict the function space by only
optimizing over the top layer W(L) and freezing the other variables W(1), . . . , W (L-1) to their
value at initialization. This restriction makes the model linear in its parameters and induces a finite-
dimensional feature map
φRF : Rd -→ RdL-1, x 7→ σ W (L-1) . . .σ W(1)x ... .
Originally introduced by Rahimi & Recht (2008) to enable more efficient kernel computations, the
random feature model has recently seen a strong spike in popularity and has been the topic of many
theoretical works (Jacot et al., 2020; Mei & Montanari, 2021).
Infinite Width. Another, only recently established correspondence emerges in the infinite-width
scenario. By choosing σw = √d- for each layer, Lee et al. (2018) showed that at initialization, the
network exhibits a Gaussian process behaviour in the infinite-width limit. More precisely, it holds
that fk i.ii.d. GP(0, Σ(L)), for k = 1,...,C, where Σ(L) : Rd X Rd → R is a kernel, coined NNGP,
obeying the recursive relation
ς(I)(X, x0) = Ez~N(0,∑(i-ι)∣x,xo) [σ(z1 )σ(Z2)]
with base Σ(1)(x, x0) = √1dXTx0 and Σ(l-1)∣χ,χo ∈ R2×2 is Σ(l-1) evaluated on the set {x, x0}.
To reason about networks trained with gradient descent, Jacot et al. (2018) extended this framework
and proved that the empirical neural tangent kernel converges to a constant kernel Θ. The so-called
neural tangent kernel Θ : Rd × Rd -→ R is also available through a recursive relation involving the
NNGP:
Θ(l+1)(x, x0) = Θ(l)(x, x0)Σ(l+1) (x, x0) + Σ(l+1)(x, x0),
with base Θ(1)(x, x0) = Σ(1)(x, x0) and Σ(l)(x, x0) = Ez~n(o,∑(i-i)∣. ,0 [σ(zjσ(z2)]. Training
infinitely-wide networks with gradient descent thus reduces to kernel learning with the NTK Θ(L).
Arora et al. (2019a); Lee et al. (2019) extended this result to a non-asymptotic setting, showing that
very wide networks trained with small learning rates become indistinguishable from kernel learning.
3.3 Leave-one-out Error
The goal of learning is to choose a model f ∈ F with minimal generalization error L(f). Since
the data distribution D is usually not known, practitioners obtain an estimate for L(f) by using an
4
Published as a conference paper at ICLR 2022
additional test set Stest = {(xt1, y1t) , . . . , (xtb, ybt)} not used as input to the learning algorithm QF,
and calculate
1b tt	1b tt
Ltest = b	Lf(Xi，yi )， Atest =3 af (Xi，yi )
(1)
i=1
i=1
While this is a simple approach, practitioners cannot always afford to put a reasonably sized test set
aside. This observation led the field of statistics to develop the idea of a leave-one-out error (LOO),
allowing one to obtain an estimate of L(f) without having to rely on additional data (Lachenbruch,
1967; Mosteller & Tukey, 1968). We give a formal definition below.
Definition 3.1. Consider a learning algorithm QF anda training setS = {(xi, yi)}in=1. Let S-i =
S \ {(xi, yi)} be the training set without the i-th data point (xi, yi) and define f-i := QF (S-i),
the corresponding model obtained by training on S-i. The leave-one-out loss/accuracy of QF on S
is then defined as the average loss/accuracy of f-i incurred on the left out point (xi, yi), given by
1n	1n
LLOO(QF; S) = — L L Lf-i(xi, yi),	ALOO(QF; S) = —〉: af-i (xi, yi)
n i=1	n i=1
Notice that calculating LLOO can indeed be done solely based on the training set. On the other hand,
using the training set to estimate L(f) comes at a cost. Computing f-i for i = 1, . . . , n requires
fitting the given model class n times. While this might be feasible for smaller models such as random
forests, the method clearly reaches its limits for larger models and datasets used in deep learning.
For instance, evaluating LLOO for ResNet-152 on ImageNet requires fitting a model with 6 × 107
parameters 107 times, which clearly becomes extremely inefficient. At first glance, the concept of
leave-one-out error therefore seems to be useless in the context of deep learning. However, we will
now show that the recently established connection between deep networks and kernel learning gives
us an efficient way to compute LLOO. Another common technique is K-fold validation, where the
training set is split into K equally sized folds and the model is evaluated in a leave-one-out fashion
on folds. While more efficient than the vanilla leave-one-out error, K-fold does not admit a closed-
form solution, making the leave-one-out error more attractive in the setting we study.
In the following, We will use the shortcut fS := fλ. Surprisingly, it turns out that the space of
kernels using QλF with mean-squared loss admits a closed-form expression for the leave-one-out
error, only relying on a single model obtained on the full training set (X, Y ):
Theorem 3.2. Consider a kernel K : RdXRd → R and the associated objective with regularization
parameter λ > 0 under mean Squared loss. Define A = K (K + λ1n)-1. Then it holds that
n C
Lloo (QF) = n XX (δQ ,	Aloo (QF)
i=1 k = 1
1 n
n ∑1{(yi-∆λ∙)*=y*}
i=1
(2)
where the residual ∆λk ∈ R for i = 1,...,n, k = 1,...,C is g^ven by ∆λk
Yik-fλ(Xi)
I-Aii
For the remainder of this text, we will use the shortcut LLλOO := LLOO QλF . While the expression
for LLλOO for C = 1 has been known in the statistics community for decades (Stone, 1974) and more
recently for general C (Tacchetti et al., 2012; Pahikkala & Airola, 2016), the formula for ALλOO is
to the best of our knowledge a novel result. We provide a proof of Theorem 3.2 in the Appendix
A.1. Notice that Theorem 3.2 not only allows for an efficient computation of LLλOO and ALλOO but
also provides us with a simple formula to assess the generalization behaviour of the model. It turns
out that LLλOO captures a great variety of generalization phenomena, typically encountered in deep
learning. Leveraging the neural tangent kernel together with Theorem 3.2 allows us to investigate
their origin.
As a first step, we consider the zero regularization limit λ -→ 0, i.e. L0LOO since this is the standard
setting considered in deep learning. A quick look at the formula reveals however that care has to be
taken; as soon as the kernel is invertible, we have a 0 situation.
5
Published as a conference paper at ICLR 2022
Corollary 3.3. Considertheeigendecomposition K = V diag(ω)VT for V ∈ O(n) andω ∈ Rn.
Denote its rank by r = rank( K). Then it holds that the residuals ∆λk ∈ R can be expressed as
n
∆λk(r) = X Ylk
l=1
Pn=r+1 Vij Vlj + Pr = 1 λ+λj Vij VIj
P' 底+ Pj=1 λ⅛ V
Moreover in the zero regularization limit, i.e. λ → 0, it holds that
n
X YIk
∆λk(r) → ∆ik(r) =	l=1
X YIk
l=1
if r < n
if r = n
Remark. At first glance for λ = 0, a distinct phase transition appears to take place when moving
from r = n - 1 to r = n. Notice however that a small eigenvalue ωn will allow for a smooth
interpolation, i.e. ∆ik(n) -ω-n--→-→0 ∆ik(n - 1) for i = 1, . . . , n, k = 1, . . . , C.
A similar result for the special case of linear regression and r = n has been derived in Hastie et al.
(2020). We now explore how well these formulas describe the complex behaviour encountered in
deep models in the kernel regime.
4	Leave-One-Out as a Generalization Proxy
In this section, we study the derived formulas in the context of deep learning, through the lens of
the neural tangent kernel and random feature models. Through empirical studies, we investigate
LOO for varying sample sizes, different amount of noise in the targets as well as different regimes
of overparametrization, in the case of infinitely-wide networks. We evaluate the models on the
benchmark vision datasets MNIST (LeCun & Cortes, 2010) and CIFAR10 (Krizhevsky & Hinton,
2009). To estimate the true generalization error, we employ the test sets as detailed in equation 1.
We provide theoretical insights into double descent, establishing the explosion in loss around the
interpolation threshold. Finally, we demonstrate the utility of LOO for state-of-the-art networks by
predicting their transferability to new tasks. All experiments are performed in Jax (Bradbury et al.,
2018) using the neural-tangents library (Novak et al., 2020).
4.1	Function of Sample Size
While known to exhibit only a small amount of bias, the LOO error may have a high variance
for small sample sizes (Varoquaux, 2018). Motivated by the consistency results obtained for ridge
regression in Patil et al. (2021), we study how the LOO error (and its variance) of an infinitely-wide
network behaves as a function of the sample size n. In the following we thus interpret LOO as a
function of n. In order to quantify how well LOO captures the generalization error for different
sample regimes, we evaluate a 5-layer fully-connected NTK model for varying sample sizes on
MNIST and CIFAR10. We display the resulting test and LOO losses in Figure 1. We plot the
average result of 5 runs along with confidence intervals. We observe that both LLOO and ALOO
closely follow their corresponding test quantity, even for very small sample sizes of around n = 500.
As we increase the sample size, the variance of the estimator decreases and the quality of both LOO
loss and accuracy improves even further, offering a very precise estimate at n = 20000.
4.2	Random Labels
Next we study the behaviour of the generalization error when a portion of the labels is randomized.
In Zhang et al. (2017), the authors investigated how the performance of deep neural networks is
affected when the functional relationship between inputs x and targets y is destroyed by replacing
y by a random label. More specifically, given a dataset X ∈ Rn×d and one-hot targets y = ek for
k ∈ {1, . . . , C}, we fix a noise level p ∈ [0, 1] and replace a subset of size pn of the targets with
6
Published as a conference paper at ICLR 2022
Figure 1: Test and LOO losses (a,c) and accuracies (b, d) as a function of sample size n. We use a
5-layer fully-connected NTK model on MNIST and CIFAR10.
random ones, i.e. y = e^ for k 〜U({1,..., C}). We then train the model using the noisy targets y
while measuring the generalization error with respect to the clean target distribution. As observed in
Zhang et al. (2017); Rolnick et al. (2018), neural networks manage to achieve very small empirical
error even for p = 1 whereas their generalization consistently worsens with increasing p. Random
labels has become a very popular experiment to assess the quality of generalization bounds (Arora
et al., 2019b; 2018; Bartlett et al., 2017). We now show how the leave-one-out error exhibits the
same behaviour as the test error under label noise, thus serving as an ideal test bed to analyze this
phenomenon. In the following, we consider the leave-one-out error as a function of the labels, i.e.
LLOO = LLOO (y). The phenomenon of noisy labels is of particular interest when the model has
enough capacity to achieve zero empirical error. For kernels, this is equivalent to rank(K) = n and
we will thus assume in this section that the kernel matrix has full rank. However, a direct application
of Theorem 3.2 is not appropriate, since randomizing some of the training labels also randomizes
i
the implicit test set. In other words, our aim is not to evaluate f -i against yi since yi might be
S
i
one of the randomized labels. Instead, we want to compare f-i with the true label yi , in order to
S
preserve a clean target distribution. To fix this issue, we derive a formula for the leave-one-out error
for a model trained on labels Y but evaluated on Y:
Proposition 4.1. Consider a kernel with spectral decomposition K = V diag(ω)VT for V ∈
RnXn orthogonal and ω ∈ Rn. Assume that rank(K) = n. Then it holds that the leave-one-out
error LLOO(Y; Y) for a model trained on S = {(xi, yi)}n=1 but evaluated on S = {(xi, yi)}n=1
is given by
n K	2	n
LLOO(Y;Y) = nXX(Aik+Yik-Yik)，	ALOO(Y;Y) = nX"(yi--)*=y*}
where Aik = ∆ik (Y) ∈ R is defined as in Corollary 3.3.
We defer the proof to the Appendix A.4. Attentive readers will notice that we indeed recover The-
orem 3.2 when Y = Y. Using this generalization of LOO, We can study the setup in Zhang et al.
(2017) to test whether LOO indeed correctly reflects the behaviour of the generalization error. To
this end, we perform a random label experiment on MNIST and CIFAR10 with n = 20000 for a NTK
model of depth 5, where we track test and LOO losses while increasingly perturbing the targets with
noise. We display the results in Figure 2. We see an almost perfect match between test and LOO
loss as well test and LOO accuracy, highlighting how precisely LOO reflects the true error.
4.3	Double Descent
Originally introduced by Belkin et al. (2019), the double descent curve describes the peculiar shape
of the generalization error as a function of the model complexity. The error first follows the classical
U -shape, where an increase in complexity is beneficial until a specific threshold. Then, increas-
ing the model complexity induces overfitting, which leads to worse performance and eventually a
strong spike around the interpolation threshold. However, as found in Belkin et al. (2019), further
increasing the complexity again reduces the error, often leading to the overall best performance. A
large body of works provide insights into the double descent curve but most of them rely on very
7
Published as a conference paper at ICLR 2022
(a) LLOO, MNIST
(b) ALOO, MNIST	(c) LLOO, CIFAR10 (d) ALOO, CIFAR10
Figure 2: Test and LOO losses (a, c) and accuracies (b, d) as a function of noise. We use a 5-layer
fully-connected NTK model on MNIST and CIFAR10 with n = 20000.
restrictive assumptions such as Gaussian input distributions (Harzli et al., 2021; Adlam & Penning-
ton, 2020; d’Ascoli et al., 2020) or teacher models (Bartlett et al., 2020; Harzli et al., 2021; Adlam
& Pennington, 2020; Mei & Montanari, 2021; Hastie et al., 2020). Here we show under arguably
weaker assumptions that the LOO loss also exhibits a spike at the interpolation threshold.
In order to reason about the effect of complexity, we have to consider models with a finite di-
mensional feature map φm : Rd -→ Rm . Increasing the dimensionality of the feature space Rm
naturally overparametrizes the model and induces a dynamic in the rank r(m) := rank(K) =
rank (φm(X)). We restrict our attention to models that admit an interpolation point, i.e. ∃m* ∈ N
such that r(m*) = n. While the developed theory holds for any kernel, We focus the empirical
evaluations largely to the random feature model due to its aforementioned connection to neural net-
Works. In the folloWing, We Will interpret the leave-one-out-error as a function of m and n, i.e.
LLOO = LLnOO(m). Notice that LLnOO(m) measures the generalization error of a model trained on
n - 1 points, instead of n. The underlying interpolation point thus shifts, i.e. we denote m* ∈ N
such that r(m*) = n — 1. In the following we will show that LnOO(m* (n)) n-→∞> ∞.
Intuition. We give a high level explanation for our theoretical insights in this paragraph. For
m < m* the dominating term in LLnOO(m) is
n
g(r, λ) = X
i=1
1
Pn=r+1 V2 + Pr=I λ+ωV2
We observe a blow-up for small λ due to the unit vector nature of both Vi∙ and V∙j. For instance,
considering the interpolation point where r = n — 1 and λ = 0, we get g(n — 1, 0) = En=I V2-,
in
the sum over the elements of V∙n, which likely has a small entry due to its unit norm nature.On
the other hand, reducing r or increasing λ has a dampening effect as we combine multiple positive
terms together, increasing the likelihood to move away from the singularity. A similar dampening
behaviour in LLnOO(m) can also be observed for m > m* when ωn >> 0. In order to formalize this
argument, we need the following assumption:
Assumption 4.2. For K = V diag (ω) VT we impose that the orthogonal matrix V follows
V〜U (On)
where U (On) is the Haar measure on the orthogonal group On.
Under Assumption 4.2, we can prove that at the interpolation point, the leave-one-out error exhibits
a spike, which worsens as we increase the amount of data:
Theorem 4.3. For large enough n ∈ N and λ → 0, it holds that
LLO。(m*) ' 2nA
where A 〜Γ(2, 1) is independent of n. LLOO(m*) hence diverges a.s. With n → ∞.
It is surprising that Assumption 4.2 suffices to prove Theorem 4.3, highlighting the universal nature
of double descent. We provide empirical evidence for the double descent behaviour of LOO in
Figure 3. We fit a 1-layer random feature model of varying width to the binary (labeled) MNIST
dataset with a sample size of n = 5000. We observe that the LOO error indeed closely follows the
8
Published as a conference paper at ICLR 2022
Figure 3: (a) Train, Test and LOO losses (a) and accuracies (b) as a function of width. We use a
random feature model on binary MNIST with n = 5000.
test error, exhibiting the spike as predicted by Theorem 4.3. The location of the spike indeed exactly
matches the interpolation threshold which is situated around m* ≈ n.
4.4 Transfer Learning
Finally, we study more realistic networks often employed for large-scale computer vision tasks.
We study the task of transfer learning (Yosinski et al., 2014) by considering classifiers trained on
ImageNet (Krizhevsky et al., 2012) and fine-tuning their top-layer to the CIFAR10 dataset. Notice
that this corresponds to training a kernel with a data-dependent feature map φdata induced by the non-
output layers. Our experiments on ResNet18 (He et al., 2016), AlexNet (Krizhevsky et al., 2012),
VGG (Simonyan & Zisserman, 2015) and DenseNet (Huang et al., 2017) demonstrate that also for
transferring between tasks we have a very precise estimation of the test accuracy. We display the
results in Table 1. We consider a small data regime where n = 10000, as often encountered in
practice for transfer learning tasks. In order to highlight the role of pre-training, we also evaluate
random feature maps φrand where we use standard initialization for all the parameters in the network.
Indeed we clearly observe the benefits of pre-training, leading to a high increase in performance
for all considered models. For both settings, pre-trained and randomly initialized, we observe an
excellent agreement between test and leave-one-out accuracies across all architectures.
Model	ATEST (0DATA )	ALOO (Φdata )	ATEST (0RAND )	ALOO (Φrand)
ResNet 1 8	67.2 ± 0.1	67.5 ± 0.4	37.7 ± 0.2	37.1 ± 0.1
AlexNet	57.6 ± 0.2	58.2 ± 0.2	24.2 ± 0.2	23.7 ± 0.2
VGG16	56.6 ± 0.2	56.8 ± 0.5	29.3 ± 0.5	29.2 ± 0.8
DenseNet 1 6 1	72.5 ± 0.2	71.3 ± 0.3	51.7 ± 0.3	49.9 ± 0.3
Table 1: Test and LOO accuracies for models pre-trained on ImageNet and transferred to CIFAR10
by re-training the top layer. We use 5 runs, each with a different training set of size n = 10000. We
compare the pre-trained networks with random networks to show the benefits of transfer learning.
5 Conclusion
We investigated the concept of leave-one-out error as a measure for generalization in the context of
deep learning. Through the use of NTK, we derive new expressions for the LOO loss and accuracy
of deep models in the kernel regime and observe that they capture the generalization behaviour
for a variety of settings. Moreover, we mathematically prove that LOO exhibits a spike at the
interpolation threshold, needing only a minimal set of theoretical tools. Finally, we demonstrated
that LOO accurately predicts the performance of deep models in the setting of transfer learning.
Notably, the simple form of LOO might open the door to new types of theoretical analyses to better
understand generalization, allowing for a disentanglement of the roles of various factors at play such
as overparametrization, noise and architectural design.
9
Published as a conference paper at ICLR 2022
References
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variance decomposition. 34th Conference on Neural Information Processing Systems (NeurIPS
2020), 2020.
Ahmed M. Alaa and Mihaela van der Schaar. Discriminative jackknife: Quantifying uncertainty in
deep learning via higher-order influence functions. Proceedings of the 37th International Confer-
ence on Machine Learning (ICML), 2020.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. Proceedings of the 35th International Conference on
Machine Learning (ICML), 2018.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. 33rd Conference on Neural Information
Processing Systems (NeurIPS), 2019a.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. Proceedings
of the 36th International Conference on Machine Learning (ICML), 2019b.
Gregor Bachmann, Seyed-Mohsen Moosavi-Dezfooli, and Thomas Hofmann. Uniform conver-
gence, adversarial spheres and a simple remedy. Proceedings of the 38th International Conference
on Machine Learning (ICML), 2021.
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. 31st Conference on Neural Information Processing Systems (Neurips), 2017.
Peter L. Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
Research 20, pp. 1-17, 2019.
Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020. ISSN
0027-8424.
Mikhail Belkin, Daniel J. Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116:15849 - 15854, 2019.
Yoshua Bengio and Yves Grandvalet. No unbiased estimator of the variance of k-fold cross-
validation. J. Mach. Learn. Res., 5:1089-1105, December 2004. ISSN 1532-4435.
Olivier BoUsqUet and Andre Elisseeff. Stability and generalization. The Journal ofMachine Learn-
ing Research, 2:499-526, 2002.
K. O. Bowman, L. R. Shenton, and PaUl C. Gailey. DistribUtion of the ratio of gamma variates.
Communications in Statistics - Simulation and Computation, 27(1):1-19, 1998.
James BradbUry, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, DoUgal
MaclaUrin, George NecUla, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NUmPy programs, 2018. URL http:
//github.com/google/jax.
Leo Breiman. HeUristics of instability and stabilization in model selection. The Annals of Statistics,
24(6):2350 - 2383, 1996.
Gavin C. Cawley and Nicola L.C. Talbot. Efficient leave-one-oUt cross-validation of kernel fisher
discriminant classifiers. Pattern Recognition, 36(11):2585-2592, 2003. ISSN 0031-3203.
ShUxiao Chen, Hangfeng He, and Weijie J. SU. Label-aware neUral tangent kernel: Toward better
generalization and local elasticity. 34th Conference on Neural Information Processing Systems
(NeurIPS), 2020.
10
Published as a conference paper at ICLR 2022
StePhane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double
descent : Bias and variance(s) in the lazy regime. Proceedings of the 37th International Confer-
ence on Machine Learning (ICML), 2020.
Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian Process behaviour in wide deeP neural networks. International Conference on
Learning Representations (ICLR), 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deeP
bidirectional transformers for language understanding. Proceedings of NAACL-HLT, 2019.
Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202-207,1979.
Simon S. Du, Kangcheng Hou, Barnabas Poczos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. GraPh neural tangent kernel: Fusing graPh neural networks with graPh kernels. 34rd Confer-
ence on Neural Information Processing Systems (NeurIPS), 2019.
Gintare Karolina Dziugaite and Daniel M. Roy. ComPuting nonvacuous generalization bounds for
deeP (stochastic) neural networks with many more Parameters than training data. Proceedings of
the Thirty-Third Conference on Uncertainty in Artificial Intelligence, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dePendent Pac-bayes Priors via differential
Privacy. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), volume 31.
Curran Associates, Inc., 2018.
Andre Elisseeff, Massimiliano Pontil, et al. Leave-one-out error and stability of learning algorithms
with aPPlications. NATO science series sub series iii computer and systems sciences, 190:111-
130, 2003.
K. Fukunaga and D.M. Hummels. Leave-one-out Procedures for nonParametric error estimates.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(4):421-423, 1989.
Ouns El Harzli, Guillermo Valle-Perez, and Ard A. Louis. Double-descent curves in neural net-
works: a new PersPective using gaussian Processes, 2021.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. SurPrises in high-
dimensional ridgeless least squares interPolation, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J. Smola. Kernel methods in machine learn-
ing. The Annals of Statistics, 36(3), Jun 2008. ISSN 0090-5364.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and
ntk for deep attention networks. Proceedings of the 37th International Conference on Machine
Learning (ICML), 2020.
Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks.
2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261-2269,
2017.
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize
better than deep feedforward networks? - a neural tangent kernel perspective. 34rd Conference
on Neural Information Processing Systems (NeurIPS), 2020.
Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-
entropy in classification tasks, 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. 32rd Conference on Neural Information Processing Systems
(NeurIPS), 2018.
11
Published as a conference paper at ICLR 2022
Arthur Jacot, Befin Simyek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. Proceedings of the International Conference on Ma-
chine Learning (ICML), 2020.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. International Conference on Learning Repre-
sentations (ICLR), 2019.
Michael Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-out
cross-validation. Neural COmPutatiOn, 11(6):1427-1453, 1999.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. PrOceedings Of the 5th InternatiOnal COnference On Learning RePresentatiOns, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical Report 0, University of Toronto, Toronto, Ontario, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural InfOrmatiOn PrOcessing Systems, volume 25. Curran Associates, Inc., 2012.
PA Lachenbruch. An almost unbiased method of obtaining confidence intervals for the probability
of misclassification in discriminant analysis. BiOmetrics, 1967.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database, 2010.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. InternatiOnal COnference On Learn-
ing RePresentatiOns (ICLR), 2018.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. 33rd COnference On Neural InfOrmatiOn PrOcessing Systems (NeurIPS),
2019.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. COmmunicatiOns On Pure and APPlied Mathematics,
06 2021. doi: 10.1002/cpa.22008.
F. Mosteller and J. Tukey. Data analysis, including statistics. HandbOOk Of SOcial PsychOlOgy, 2,
1968.
Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain general-
ization in deep learning. 33rd COnference On Neural InfOrmatiOn PrOcessing Systems (NeurIPS),
2019.
Radford M. Neal. PriOrs fOr Infinite NetwOrks, pp. 29-53. Springer New York, New York, NY,
1996. ISBN 978-1-4612-0745-0. doi: 10.1007/978-1-4612-0745-02
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. PrOceedings Of The 28th COnference On Learning TheOry (PMLR), 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. InternatiOnal COnference On Learning
RePresentatiOns (ICLR), 2018.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In
InternatiOnal COnference On Learning RePresentatiOns, 2020. URL https://github.com/
google/neural-tangents.
Tapio Pahikkala and Antti Airola. Rlscore: Regularized least-squares learners. JOurnal Of Machine
Learning Research, 17(220):1-5, 2016. URL http://jmlr.org/papers/v17/16-470.
html.
12
Published as a conference paper at ICLR 2022
Pratik Patil, Yuting Wei, Alessandro Rinaldo, and Ryan Tibshirani. Uniform consistency of cross-
validation estimators for high-dimensional ridge regression. In Arindam Banerjee and Kenji
Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and
Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 3178-3186. PMLR,
13-15Apr2021.
Vern I. Paulsen and Mrinal Raghupathi. An Introduction to the Theory of Reproducing Kernel Hilbert
Spaces, pp. i-iv. Cambridge Studies in Advanced Mathematics. Cambridge University Press,
2016.
M. Pontil. Leave-one-out error and stability of learning algorithms with applications. International
Journal of Systems Science, 2002.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise, 2018.
Sarath Shekkizhar and Antonio Ortega. Deepnnk: Explaining deep models and their generalization
using polytope interpolation, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. International Conference on Learning Representations (ICLR), 2015.
M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal
Statistical Society, 36(2):111-147, 1974.
Andrea Tacchetti, Pavan Mallapragada, Matteo Santoro, and Lorenzo Rosasco. Gurls: a toolbox for
regularized least squares learning. 02 2012.
Gael Varoquaux. Cross-validation failure: Small sample sizes lead to large error bars. NeuroImage,
180:68-77, 2018. ISSN 1053-8119. New advances in encoding and decoding of brain signals.
Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-
one-out cross-validation and waic. Statistics and Computing, 27(5):1413-1432, Aug 2016. ISSN
1573-1375.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,
2018.
Ch. Walck. Hand-book on statistical distributions for experimentalists. 1996.
J. Weston. Leave-one-out support vector machines. In IJCAI, 1999.
Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture, 2020.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? Advances in Neural Information Processing Systems, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. International Conference on Learning Repre-
sentations (ICLR), 2017.
Rui Zhang and Shihua Zhang. Rethinking influence functions of neural networks in the over-
parameterized regime, 2021.
Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397-1437,
2003.
Yongli Zhang and Yuhong Yang. Cross-validation for selecting a model selection procedure. Journal
of Econometrics, 187(1):95-112, 2015. ISSN 0304-4076.
13
Published as a conference paper at ICLR 2022
A Omitted Proofs
We list all the omitted proofs in the following section.
A.1 Proof of Theorem 3.2
Theorem 3.2. Consider a kernel K : Rd ×Rd -→ R and the associated objective with regularization
parameter λ > 0 under mean squared loss. Define A = K (K + λ1n)-1. Then it holds that
nK	n
L LOO (QERM) = n XX (∆λk)2 ,	A loo (QERM) = n X
i=1 k=1
i=1
1
{(ya-δ" ) =y↑ }
where the residuals ∆λk ∈ R for i = 1,...,n, k = 1,...,K is given by ∆λk = YikjA (Xi)
Proof. Recall that fS solves the optimization problem
nK
fs = argminf∈f LS (f) ：= argminʃ∈f XX
(fk (Xi)- Yik)) + λUf ||H}
i=1 k=1
and predicting on the training data takes the form fS(X) = AY for some A ∈ Rn×n. NoW
consider the model f—i := fS_. obtained from training on S-i. W.L.O.G. assume that i = n. We
n
Want to understand the quantity fλ-,kn(xn), i.e. the k-th component of the prediction on xi. To that
end, consider the dataset Z := S-n ∪ { (xn, f—n(xn)) }. Notice that for any f ∈ F, it holds
C	n-1	2	2
LZ(f-n) = X{Xf-n(Xi)-Kk) + f-n(Xn)-f—n(Xn))	}+λf-nι∣H
k=1	i=1
C n—1	2
=XX (f-n(χi)- Kk) + λ∣∣f-nι∣H
k=1 i=1
=LS-nf—n)
≤ LλS-n (f)
C
≤ LS-n (f)+ X (fk (Xn)-f-n(Xn
k=1
2
LλZ(f)
n
where the first inequality follows because f n minimizes LS 九 by definition. Thus fx also mini-
mizes LλZ and hence also takes the form
~

f-n(χ) = Ay
(
_	ʃ_	一 一 、，尸、	_ _	二
Where Y ∈ Rn×C such that Yik
Yik
△,
if i 6=
which is
... .
f-k(xn)	else
n
. Now we care about the n-th prediction
〜
〜
n
n-1
f-n(Xn) = Σ>njY,k = £ Anj% + Annf-k(Xn)
j=1
n
j=1
=EAnjYjk — AnnYnk + Annf-k(Xn)
j=1
λn
= fS,k (xn) - AnnYnk + Annfλ,k (xn )
14
Published as a conference paper at ICLR 2022
、S '	.
Solving for f-,k(xn) gives
ʌ ...
f-n(Xn)
1 - Ann
(3)
Then, subtracting Ynk leads to
__ 、	S '	.	___
Ynk - fλ,k (Xn) = Ynk -
1 - Ann
Ynk - Ynk Ann - fS ,k(Xn ) + An
n Ynk
Ynk- fS,k (Xn)
1 - Ann
Squaring and summing the expression over n and k results in the formula.
n
For the accuracy, we know that we correctly predict if the maximal coordinate of fλ-n (Xn) agrees
with the maximal coordinate of yn, i.e.
n
argmaxk fλ-,kn(Xn) = argmaxk Ynk
From equation 3, we notice that
-n	fSλ,k (Xn ) - AnnYnk
argmaxk f-,k (Xn) = argmaxk ——γ-A--------
= argmaxk -∆λnk + Ynk
argmaxk
fS,k (Xn ) - Ynk + Ynk - Ann Ynk
1 - Ann
(yn - AnJ
We thus have to check the indicator 1{(yn-∆n∙)*=y*} and SUm it over n to obtain the result. □
A.2 B inary Classification
Here we state the corresponding results in the case of binary classification. The formulation for the
accuracy changes slightly as now the sign of the classifier serves as the prediction.
Proposition A.1. Consider a kernel K : Rd × Rd -→ R and the associated objective with regular-
ization parameter λ > 0 under mean squared loss. Define A = K (K + λ1n)-1. Then it holds
that
nn
LLOO (QERM) = 1XA)2,	ALOO (QERM) = 1X1{yi∆iλ<1}
i=1	i=1
where the residuals ∆iλ ∈ R for i = 1, . . . , n is given by ∆iλ =
yi-fλ (Xi)
I-Aii
Proof. Notice that the result for LLOO is analogous to the proof for Theorem 3.2 by setting K = 1.
For binary classification we use the sign of the classifier as a decision rule, i.e. the classifier predicts
correctly if yfλ(x) > 0. We can thus calculate that
、S '	.
ynfλ	(Xn)
1 - Ann
1 - Ann
yn2 - yn
yn
1 - yn
1 - Ann
1 - yn∆λn
Thus, the n-th sample is correctly classified if and only if
1 - yn∆λ > 0 Q⇒ yn∆λ < 1
We now just count the correct predictions for the accuracy, i.e.
1n
ALOO (QERM) = - X 1{yi∆iλ<1}
i=1
□
15
Published as a conference paper at ICLR 2022
A.3 Proof of Corollary 3.3
Corollary 3.3. Consider the eigendecomposition K = V diag(ω)V T for V ∈ O(n) and ω ∈ Rn.
Denote its rank by r = rank(K). Then it holds that the residuals ∆iλk ∈ R can be expressed as
Pn=r+1 VikVLk + Pk = ι λ+⅛VikVLk
n
∆iλk(r) =XYlk
l=1
Pn=r+1 Vi2k + Pk=I λ+⅛ Vik
Moreover for zero regularization, i.e. λ -→ 0, it holds that
∆iλk(r) -→∆ik(r)
n
X Mk
l=1
n
X Ylk
l=1
Pj=r+1 VijVIj
Pn=r + 1 琮
Pn=I ωjVijVlj
if r < n
if r = n
Proof. Define A = K (K + λ1n)-1
Pjn=1 AijYjk. Let us first simplify A:
A=K(K+λ1n)-1=
=V diag (—ω~r) 1
ω+λ
Rn×n. Recall that fS(X) = Ay and thus f (Xi)
=V diag (ω) VT (V diag (ω) VT + λ1n)-1
VT
We can characterize the off-diagonal elements for i 6= j as follows:
∈
r
r
nr
Aij = X Vik ωi+λVjk = X Vik Vjk ωi+λ
k=1 i	k=1	i
nr
=-X VikVjk - X ω + λ KkVjk
k=r+1	k=1 k
ωi
ωi
k=1
VikVjk-
k=1
----^Vjk Vjk
ωk + λ
where we made use of the fact that V‰ ⊥ Vj∙. The diagonal elements i = j on the other hand can
be written as
r
r
nr
Aii = X ⅝k= X V2 *τ
k=1 ik ω, + λ w ik ωk + λ
nr
=1 - X Vk - X Vik -ɪ-
k=r+1 ik k=1 ikωk+λ
ωi
Vi2k -	Vi2k
λ
ωk + λ
where We have made use of the fact that Vi∙ is a unit vector. Plugging-in the quantities into LLOO
results in
∆iλk
Yik	-	fk(Xi)	_	Yik	- Pn=I	AiiYik	_ Yik - Aiiyi	- P=	AiIYLk
1 - Aii
Yik Pjn=r+1
1 - Aii
1 - Aii
吟 + Pk=1 Vijω+^) + PQi YLk (Pn=r+1 Vij Vlj + Pj=I ωλ+λ Vij %j)
Pn=r+ι V2 + Pj=ι Vi2ωι+λ
Pn=I Ylk (Pn=r+1 Vij Vlj + Pj=1 ωjλ+λ VijVij)
Pn=r+1 V2+ Pj=1 Vi2ωk+λ
Now crucially, in the full rank case r = n, we have empty sums, i.e. Pln=r+1
we obtain
Pln=n+1 =0and
∆iλk
Pn=ι YLk Pn=I ωjλ+λVijVlj _ Pn=ι Ylk Pn=I ωj+λVijVlj
一 Pn 1 V2 λ
j=1 ij ωj+λ
λ→0∖ X Y Pi=I ωjVijVlj
> 》 Ylk Pn	ɪV2
l=1	j=1 ωj ij
P=j+λ
16
Published as a conference paper at ICLR 2022
On the other hand, in the rank deficient case r < n we can cancel the regularization term:
∆iλk -λ--→-→0
n
XYlk
l=1
Pj=r+1 VijVlj
Pn=r+1 Vij
Plugging this into the formulas for LLλOO and ALλOO concludes the proof.
□
A.4 Proof of Proposition 4.1
Proposition 4.1. Consider a kernel with spectral decomposition K = V diag(ω)V T for V ∈
Rn×n orthogonal and ω ∈ Rn. Assume that rank(K) = n. Then it holds that the leave-one-out
error LLOO(Y； Y) for a model trained on S = {(xi, yi)}n=ι but evaluated on S = {(xi, yi)}n=ι
is given by
LLOO(Y; Y) = 1 XX XX (∆ik + Yik - Yik)2,
n i=1 k=1
1n
ALOO (Y; Y ) = n X 1{(yi-∆i∙j}
i=1
where ∆ik = ∆ik(Y) ∈ R is defined as in Corollary 3.3.
T-> f ɪʌ , 1 ∕~∙ . 1 1 . . C /	ɪʌ , 1 3—i . 1	11, ∙ 1 i~∙ T~1 11
Proof. Denote by S-i the dataset {(xj ,yj)}n=i∙ Denote by f- i the model trained on S-i. Recall
from the proof of Theorem 3.2 that
~ ...
f-n(Xn )
fk (xn ) - Ann Ynk
1 - Ann
Instead of subtracting the same label Ynk, we now subtract the evaluation label Ynk:
~ . .
Ynk- f-n(Xn)
λλ
Ynk - AnnYnk - fk (xn) + AnnYnk	(1 - Ann )(Ynk - Ynk ) + Ynk - fk (xn )
1 - Ann
1 - Ann
(Ynk - Ynk ) +
Ynk - fk(Xn)
1 - Ann
=(Ynk - Ynk) + \k
The second term ∆λ is now the summand of the standard leave-one-out error where We evaluate on
∙τV -c-r τ	1	El	CC ,1	∙ , n	∙	F	∙	11	, 1
Y. We can hence re-use Theorem 3.3 to decompose it. Squaring and summing over n concludes the
LOO loss result. For the accuracy, we notice that a similar derivation as for Theorem 3.2 applies:
n
argmaxk fλ-,kn(xn)
argmaxk
fk (xn ) - Ann Ynk
1 - Ann
argmaxk
λ
fk (xn ) - Ynk + Ynk - AnnYnk
1 - Ann
λ
argmaxk -∆λnk + Ynk
(yn - ∆n∙)*
We thus have to check the indicator against the true label yn, i.e. 1{(y _入.)*=y*} and SUm it over
n to obtain the result.	□
A.5 Proof of Theorem 4.3
Theorem 4.3. For large enough n ∈ N, we can estimate as
LLO。(m*) ' 2nA
where A 〜Γ(2,1) is independent of n. LLOO (m*) hence diverges a.s. with n → ∞.
Proof. First we notice that for m = m*, by definition it holds that r = n 一 1, which simplifies the
LOO expression to
Xn yiVin
LLOOm -→→ 1
17
Published as a conference paper at ICLR 2022
For notational convenience, we will introduce v ∈ Rn such that vi := Vin. We will now bound the
both factors one-by-one. The first part is a simple application of Proposition B.1 and Proposition
B.5:
LLOO W) = ) (X Vj X V ≥ n2 n (X vj =
Now for large enough n, we can use Lemma B.6 to make the following approximation in distribu-
tion:
nB ≈ 2 n-ɪ B —→ 2A
2
where A 〜Γ(2,1). Thus for large enough n, it holds that
LrLOO (m*) ' 2nA
As the approximation becomes exact for larger and larger n, we conclude that
LLOO(m*) ——→ ∞ a.s.
□
B Additional Lemmas
In this section we present the additional technical Lemmas needed for the proofs of the main claims
in A.
Lemma B.1. Consider a unit vector v ∈ Sr-1. Then it holds that
n1
X 4 ≥ n2
白熄一
Proof. Let’s parametrize each vi as
zi
Vi =,	=
PPm2
for i = 1, . . . , n and z ∈ Rn. One can easily check that ||v||2 = 1 and hence v ∈ Sn-1. Plugging
this in, we arrive at
n1
X " = E
i=1
r
Pn	2 n n 2	n n 2
『=XX 孑=n + XX j
i	i=1 j=1 i	i=1 j 6=i i
We can re-arrange the sum into pairs
zj2	zi2	2	1
—q +----2 = a +------2 ≥ 2
zi2	zj2	a2
2
for a2 = z2 > 0 and using the fact that X + ɪ ≥ 2 for X ≥ 0. We can find n(r2")such summands,
and thus
X ɪ ≥ n + 2n(n — I) = n2
n	n
J V2 —	2
i=1 i
□
Lemma B.2. Consider V 〜U (SnT) and any fixed orthogonal matrix U ∈ O(n). Then it holds
that
Uv (=d) v
Proof. This is a standard result and can for instance be found in Vershynin (2018).	□
18
Published as a conference paper at ICLR 2022
Lemma B.3. Consider W 〜N(0,1n). Then it holds that
v
w
||w||2
U (SnT)
〜
Proof. This is a standard result and can for instance be found in Vershynin (2018).
Lemma B.4. Consider two independent Gamma variables X 〜 Gamma(α, V) and Y
Gamma(β, ν). Then it holds that
□
〜
X
V , V 〜Beta (α, β)
X+Y
Proof. This is a standard result and can for instance be found in Bowman et al. (1998).	口
Lemma B.5. Consider V 〜 U (SnT). Then it holds that
Proof. First realize that we can write
n
√n X yivi = IT (V © y) = In V
i=1
where in = (√n,..., √n) with ∣∣in∣∣2 = 1 and the fact that V © y =) V for fixed y ∈ {-1,1}n.
The idea is now to choose U ∈ O(n) such that UT1n = e1 = (1, 0, . . . , 0). Then by using Lemma
B.2, it holds
T (d) T	(d)	T	(d) T (d)
1TnV = 1Tn UV = U1n	V = e1TV = v1
Thus, surprisingly, it suffices to understand the distribution ofv1. By Lemma B.3, we know that
(d)	z1
Vi =—,
PZ + •一+Zn
where Z ~ N(0,1n). We are interested in the square of this expression,
where we define w = Pin=2zi2, clearly independent of z2. Moreover, it holds that Z2 〜
Gamma (ɪ, ɪ) and W 〜Gamma (n-1, ɪ). Thus, by Lemma B.4 We can conclude that
□
Lemma B.6. Consider the Sequence ofBeta distributions Xn 〜Beta(k, n). Then it holds that
nXn -(-d→) Gamma(k, 1)
Proof. This is a standard result and can for instance be found in WalCk (1996).	口
C	Further Experiments
In this section we present additional experimental results on the leave-one-out error.
19
Published as a conference paper at ICLR 2022
(a) Depth 1
Figure 4: Kernel rank rank(K) as a function of complexity. For (a) we use a depth 1 random feature
model σ(Wx), W ∈ Rm×d, where complexity is measured through width m. In (b) and (c) we
use a random feature model of depth 2, σ(V σ(W x)), W ∈ Rm1 ×d and V ∈ Rm2×m1 . For (b) we
visualize the rank as a function of m1, where m2 = 120 fixed and in (c) as a function of m2, where
m1 = 10 fixed. We use MNIST with n = 100 samples.
(c) Depth 2, m2
(b) Depth 2, m1
C.1 Rank Dynamics for Different Kernels
In this section, we illustrate how the rank rank(K) of the kernel evolves as a function of the un-
derlying complexity. As observed in various works, the spike in the double descent curve for neural
networks usually occurs around the interpolation threshold, i.e. the point in complexity where the
model is able to achieve zero training loss. Naturally, for the kernel formulation, interpolation is
achieved when the kernel matrix K has full rank. We illustrate in the following how the dynamics
of the rank change for different architectures, displaying a similar behaviour as finite width neural
networks. In Fig. 4 (a) we show a depth 1 random feature model with feature maps σ(Wx) where
W ∈ Rm×d and σ is the ReLU non-linearity. We use MNIST with n = 100. We measure the
complexity through the width m of the model. We observe that in this special case, the critical com-
plexity exactly coincides with the number of samples, i.e. rank(K) = n as soon as m = n. This
seems counter-intuitive at first as an exact match of the complexity with the sample size is usually
not observed for standard neural networks. We show however in Fig. 4 (b) and (c) that the rank dy-
namics indeed become more involved for larger depth. In Fig. 4 (b) and (c), we use a 2-layer random
feature model with feature map σ(V σ(W x)) for weights W ∈ Rm1 ×d and V ∈ Rm2×m1 . In (b)
we show the change in rank as m1 increases, while m2 = 120 is fixed. We observe that indeed the
dynamics change and the critical complexity is not at m1 = n. Similarly in (c) we fix m1 = 10
and vary m2 . Also there we observe that the critical threshold is not located at n but rather a bigger
width is needed to reach interpolation.
Finally we also study the linearizations of networks that also give rise to kernels. More concretely,
we consider feature maps of the form
φ(x) = Vθ fθ (x)
where fθ is a fully-connected network with parameters θ ∈ Rp , as introduced before. Our double-
descent framework also captures this scenario. In Fig. 5, we display the rank dynamics in case of a
2 layer network fθ (x) = wTσ(Uσ(V x)) where w ∈ Rm2, U ∈ Rm2×m1 , V ∈ Rm1 ×d and σ is
the ReLU non-linearity. Again we observe that the rank dynamics change and do not coincide with
the number of samples n.
C.2 LOO AS FUNCTION OF DEPTH L
We study how the depth L of the NTK kernel Θ(L) affects the performance of LOO loss and ac-
curacy. We use the datasets MNIST and CIFAR10 with n = 5000 and evaluate NTK models with
depth ranging from 3 to 20. We present our findings in Figure 6. Again we see a very close match
between LOO and the corresponding test quantity for CIFAR10. Interestingly the performance is
slightly worse for very shallow models. For MNIST we see a gap between LOO loss and test loss,
which is due to the very zoomed-in nature of the plot (the gap is actually only 0.015) as the loss
values are very small in general. Indeed we observe an excellent match between the test and LOO
accuracy.
20
Published as a conference paper at ICLR 2022
Figure 5: Rank dynamics of K for the linearization kernel of fθ (x) = wT σ(Uσ(V x)) where
w ∈ Rm2, U ∈ Rm2 ×m1, V ∈ Rm1 ×d and σ is the ReLU non-linearity.
(a) LLOO, MNIST	(b) ALOO, MNIST	(c) LLOO, CIFAR10	(d) ALOO, CIFAR10
Figure 6: Test and LOO losses (a, c) and accuracies (b, d) as a function of depth L. We use fully-
connected NTK model on MNIST and CIFAR10.
C.3 Double Descent with Random Labels
Here we demonstrate how the spike in double descent is a very universal phenomenon as demon-
strated by Theorem 4.3. We consider a random feature model of varying width m on binary MNIST
with n = 2000, where the labels are fully randomized (p = 1), destroying thus any relationship
between the inputs and targets. Of course, there will be no double descent behaviour in the test ac-
curacy as the network has to perform random guessing at any width. We display this in Figure 7. We
observe that indeed the model is randomly guessing throughout all the regimes of overparametriza-
tion. Both the test and LOO loss however, exhibit a strong spike around the interpolation threshold.
This underlines the universal nature of the phenomenon, connecting with the fact that Theorem 4.3
does not need any assumptions on the targets.
C.4 Transfer Learning Loss
We report the corresponding test and leave-one-out losses, moved to the appendix due to space
constraints. We display the scores in Table 2. Again we observe a very good match between the test
and LOO losses. Moreover, we again find that pre-training on ImageNet is beneficial in terms of the
achieved loss values.
21
Published as a conference paper at ICLR 2022
Figure 7: Test and LOO losses (a) and accuracies (b) as a function of sample width m. We use a
random feature model on binary MNIST with random labels.
MODEL	LTEST (Φdata)	LLOO (Φdata)	Ltest (Φrand)	LLOO (Φrand)
ResNet 1 8	0.602 ± 0.0053	0.619 ± 0.009	0.77 ± 0.003	0.758 ± 0.0051
AlexNet	0.638 ± 0.0021	0.639 ± 0.0036	0.837 ± 0.002	0.835 ± 0.004
VGG16	0.657 ± 0.0062	0.66 ± 0.0091	0.852 ± 0.0026	0.834 ± 0.0049
DenseNet 1 6 1	0.599 ± 0.0044	0.613 ± 0.0097	0.718 ± 0.0043	0.731 ± 0.0081
Table 2: Test and LOO losses for models pre-trained on ImageNet and transferred to CIFAR10 by
re-training the top layer. We use 5 runs, each with a different training set of size n = 10000. We
compare the pre-trained networks with random networks to illustrate the benefits of transfer learning.
22