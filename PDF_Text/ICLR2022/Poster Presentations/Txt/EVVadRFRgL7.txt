Published as a conference paper at ICLR 2022
Bayesian Modeling and Uncertainty Quantifi-
cation for Learning to Optimize: What, Why,
and How
Yuning You1, Yue Cao1, Tianlong Chen3, Zhangyang Wang3, Yang Shen1,2
1 Department of Electrical and Computer Engineering, Texas A&M University
2Department of Computer Science and Engineering, Texas A&M University
3Department of Electrical and Computer Engineering, University of Texas at Austin
{yuning.you,cyppsp,yshen}@tamu.edu, {tianlong.chen,atlaswang}@utexas.edu
Ab stract
Optimizing an objective function with uncertainty awareness is well-known to
improve the accuracy and confidence of optimization solutions. Meanwhile,
another relevant but very different question remains yet open: how to model
and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) it-
self? To close such a gap, the prerequisite is to consider the optimizers as
sampled from a distribution, rather than a few prefabricated and fixed update
rules. We first take the novel angle to consider the algorithmic space of opti-
mizers, and provide definitions for the optimizer prior and likelihood, that in-
trinsically determine the posterior and therefore uncertainty. We then leverage
the recent advance of learning to optimize (L2O) for the space parameteriza-
tion, with the end-to-end training pipeline built via variational inference, referred
to as uncertainty-aware L2O (UA-L2O). Our study represents the first effort to
recognize and quantify the uncertainty of the optimization algorithm. The ex-
tensive numerical results show that, UA-L2O achieves superior uncertainty cal-
ibration with accurate confidence estimation and tight confidence intervals, sug-
gesting the improved posterior estimation thanks to considering optimizer uncer-
tainty. Intriguingly, UA-L2O even improves optimization performances for two
out of three test functions, the loss function in data privacy attack, and four of
five cases of the energy function in protein docking. Our codes are released at
https://github.com/Shen-Lab/Bayesian-L2O.
1	Introduction
Computational models of many real-world applications involve optimizing non-convex objective
functions. As the non-convex optimization problem is NP-hard, no optimization algorithm (or opti-
mizer) could guarantee the global optima in general. Instead their solutions’ usefulness (sometimes
based on their proximity to the optima), when the optima are unknown, can be very uncertain. Be-
ing able to quantify solution uncertainty directly provides calibration with ensured awareness of
the solution quality and usefulness (and another potential benefit is in optimization performance by
enhancing the search efficiency). For instance, reliable and trustworthy machine learning models
demand uncertainty awareness and quantification (UQ) during training (optimizing) such models,
whereas in reality deep neural networks without proper modelling of uncertainty suffer from over-
confidence and miscalibration (Guo et al., 2017). In another example of 3D prediction of protein-
protein interactions, even though there exists the model uncertainty of the objective function and
the data uncertainty of the protein structure data (Cao & Shen, 2020), state-of-the-art methods only
predict several ranked solutions (Porter et al., 2019) without any associated uncertainty, which is
hard for biologists to interpret.
Despite progress in optimization with uncertainty-awareness, fundamental open questions remain:
existing methods consider uncertainty either within the data or the model (including objective func-
tions) (Kendall & Gal, 2017; Ortega et al., 2012; Lyu et al., 2021; Cao & Shen, 2020), whereas
inconspicuous attention was paid to the uncertainty arising from the optimizer that is directly re-
1
Published as a conference paper at ICLR 2022
sponsible for deriving the end solutions. The optimizer is usually prefabricated and fixed. For
instance, there are several popular update rules in Bayesian optimization, such as expected improve-
ment (Vazquez & Bect, 2010) and upper confidence bound (Srinivas et al., 2009), that are chosen
and unaltered for the entire process. For Bayesian neural networks training, the update rule of the
iterative optimizer is usually chosen off-the-shelf, such as Adam, SGD, and RMSDrop. In principle,
it is important to recognize the existence of an optimization algorithm space where a specific opti-
mizer lies as well as the importance of the optimizer uncertainty, intrinsically defined in the space
to the optimization and UQ solutions. In practice, such uncertainty is unwittingly omitted in the
current context where an optimizer is treated as a de facto specified sample in the space.
A naive characterization for a hand-built optimizer is to rely on a few of its hyper-parameters (Wen-
zel et al., 2020; Lorraine & Duvenaud, 2018), whereas such a parameterization space could be too
biased and restricted to span a broader and more complicated optimizer manifold, potentially result-
ing in inaccurate UQ results (see Section 4, confidence estimation for test functions). Fortunately,
the recent surge of learning to optimize (L2O) (Andrychowicz et al., 2016a; Li & Malik, 2016; Cao
et al., 2019a; You et al., 2020; Li et al., 2020; Chen et al., 2020b;a; Shen et al., 2021; Chen et al.,
2021) which is another prominent optimization paradigm, parameterized by a neural network using
high-dimensional weights and following the data-driven ethos to learn the update rule adaptively,
therefore introducing the less inductive bias and accompanied with the universal approximation
ability, indicates a plausible solution to depict the sophisticated and more complete optimizer space.
Contributions. The aforementioned introduction reveals and discusses the fundamental question of
why modelling the optimizer uncertainty, and our contingent questions would be, What defines the
optimizer uncertainty and how to enable UQ during optimization. We give the following answers.
(i) We define the prior and likelihood of the optimizer (Neal, 2012), which routinely determine the
optimizer posterior with the general product rule and Bayes’ theorem. The optimizer uncertainty is
therefore well-defined. (ii) To unseal UQ within optimization, we treat an optimizer as a random
sample from an algorithmic space of iterative optimizers. We leverage the surging L2O technique
(Andrychowicz et al., 2016a; Li & Malik, 2016) with high-dimensional weights for the optimizer
space parameterization. (iii) We further construct the end-to-end training pipeline via variational
inference (Kingma & Welling, 2013; Higgins et al., 2016) to avoid the expensive computational
cost of Markov chain Monte Carlo (MCMC) and degenerated posterior. The newly trained L2O
is referred to as uncertainty-aware L2O (UA-L2O). In summary, the core intellectual value of this
work is, for the first time, we recognize and quantify a novel form of uncertainty that lies in the
optimization algorithm space parameterized by L2O, apart from the classical data- or model-based
uncertainties (also known as epistemic and aleatoric uncertainties (Fox & Ulkuimen, 2011)).
Extensive experiments show that UA-L2O had superior capability in uncertainty calibration thanks
to the accurate estimation of solution posteriors. It confidence levels well matched the chance for the
global optimum falling in the corresponding intervals. Intriguingly, although not directly targeted,
UA-L2O also outperformed competing methods in optimization performance, as seen in optimizing
two out of three test functions in high dimensions, the loss function in data privacy attack, energy
functions in four out of five protein-docking cases.
2	Preliminaries
2.1	Learning to Optimize
Let us consider an optimization problem: minx f (x) where x ∈ Rd. A classic optimizer often
iteratively updates x based on a handcrafted rule. For example, the first-order gradient descent
algorithm takes an update at iteration t based on the local gradient at the instantaneous point xt :
xt+1 = Xt - αVf (xt), where α is the step size.
Learning to Optimize (L2O) has lots of freedom to use the available information. We define zt as
optimization trajectories’ historical information up to time t, e.g., the existing iterates X0 , . . . , Xt,
and/or their gradients Vf (X0), . . . , Vf(Xt). L2O models an update rule by a predictor function g
of zt: Xt+1 = Xt - g(zt; φ), where the mapping of g is parameterized by φ. Finding an optimal
update rule can be formulated mathematically as searching for a good φ over the parameter space
of g. Practically, g is often a neural network. Since neural networks are universal approximators,
L2O has the potential to discover completely new update rules without relying on existing rules. In
order to find a desired φ associated with a fast optimizer, (Andrychowicz et al., 2016b) proposed to
2
Published as a conference paper at ICLR 2022
minimize the weighted sum of the objective function f(xt) over a time span T:
T
min wtf(xt), with xt+1 = xt - g(zt; φ), t = 0, . . . , T - 1,	(1)
where w1 , . . . , wT are the weights whose choices vary from case to case and depend on empirical
settings. Note that φ determines the objective value through determining the iterates xt . L2O solves
the problem (1) for a desirable φ and correspondingly the update rule g(zt; φ).
A typical L2O workflow is divided into two stages (Chen et al., 2021): a meta-training stage that
learns the optimizer with a set of similar optimizees from the task distribution; and a meta-testing
stage that applies the learned optimizer to new unseen optimizees. The meta-training process often
occurs offline and is time consuming. However, the online application of the method, meta testing,
is (aimed to be) time saving.
2.2	The Uncertainty of Optimization
The choice of optimizers is recognized to remarkably impact the solution quality, especially for
non-convex and rugged objectives such as various loss functions for training deep networks. While
non-convex optimization is NP-hard and the global optimum is never guaranteed, domain users
usually can only blindly take the solutions given by a particular optimizer, or choose from several
well-known algorithms based on human experiences or on an ad hoc basis. Such trial-and-error se-
lections are biased and restricted to span a more complete optimizer manifold, and computationally
expensive, even intractable when new algorithms are to be discovered by L2O, whose “fitness” or
confidence on certain problem instances (especially instances with shifts from the target task distri-
butions) is never known. Given a specific problem instance, if we can provide a confidence score
or quantify the uncertainty of each candidate optimizer, it will certainly improve the selection and
calibration of optimization algorithms, in particular the learned optimizers since they are essentially
all “new”, and hence enhancing the reliability of L2O.
As for non-convex objectives, different choices of optimizers, even each algorithm’s hyper-
parameter or initialization variations, can lead to vastly different solutions, implying a new type of
uncertainty arising from choosing optimizers. Stochastic optimization methods like random search
(Zhigljavsky, 2012), simulated annealing (Kirkpatrick et al., 1983) and genetic algorithms (Gold-
enberg, 1989) inject the randomness into algorithms to reduce solution uncertainty. However, they
cannot reliably quantify its effect on the final solutions. Several works explored uncertainty quan-
tification during optimization, by Bayesian optimization (Hennig & Schuler, 2012; Wang & Jegelka,
2017) or Monte Carlo sampling (Ortega et al., 2012). Several works in Bayesian optimization ex-
plored uncertainty quantification during optimization, by modeling the distributions over the func-
tional space (Hennig & Schuler, 2012; Wang & Jegelka, 2017) or directly over the optimum (Ortega
et al., 2012; Cao & Shen, 2020). However, those methods are often computationally too expensive,
not to mention applied within the already costly L2O process. More importantly, they neither ex-
plicitly consider the uncertainty within the optimizer itself, nor concern the automatic learning and
discovery of the most suitable optimizer.
3	Technical Approach
To quantify and address such “fitness” especially for the L2O parameterization in a principled way,
we will introduce this new optimizer uncertainty arising from the choice of optimization algo-
rithms, which is different from the classical data- or model-based uncertainties (also known as epis-
temic and aleatoric uncertainties (Fox & Ulkumen, 2011)). We will explore: how to quantify opti-
mizer uncertainty? how to design a learned optimizer so that its uncertainty can be more accurately
quantified? what is the best calibrated optimizer one can choose or learn for a given problem?
The core innovation is to treat an optimizer as a random sample from a continuous algorithmic space,
rather than one of a few hand-crafted update rules, so as to model the intrinsic uncertainty within
the optimizer. This novel view of algorithmic space is particularly enabled by L2O. Note that an
L2O update rule is typically parameterized by a neural network g , with its inputs zt and learnable
parameters φ. Its learning capacity allows us to “sample” optimizers by taking g(∙; φ) with different
weights φ. That is sufficiently versatile and flexible in practice.
3
Published as a conference paper at ICLR 2022
3.1	Concepts for Optimizer Uncertainty
Without loss of generality, we define an iterative algorithmic space G , where each point g ∈ G maps
to an update g(zt) dependent on the current/past zero-th order and/or first-order information zt. We
treat g as a random vector from G , that leads to defining the optimizer uncertainty as follows:
Definition 1 (Optimizer Uncertainty) Let G be the algorithmic space, where each point g ∈ G is
an optimizer (omitting φ parameterization). We assume that
1.	g has a prior distribution p(g);
2.	Its likelihood can be interpreted as p(zt|zt0 , g) = Qit=t +1 p(xi|zi-1, g), ∀t0 < t.
The likelihood factorization is based on the causality of iterative optimizers, i.e., (xi , zi) depends
solely on {(xi0 , zi0)}i0 <i given g. Thereby, we define the optimizer uncertainty at step t as the
posterior of g, that is a conditional distribution on the t steps of the trajectory, using Bayes’ theorem:
t
p(g∣zt) ∞ p(g) ∩p(xi∣Zi-i, g).	(2)
i=1
Intuitively, the prior p(g) represents the belief about well-performing optimizers, the likelihood
p(zt |zt0, g) represents the probability of observing an optimization trajectory (data) under a given
optimizer g, and the posterior p(g|zt) represents the probability for g being the optimizer generating
the observed data.
Prior work on hyper-parameter optimization (HPO) for classical algorithms (Feurer & Hutter, 2019)
could be viewed as a special case of L2O (e.g., only a few hyper-parameters are “learnable”, which
results in a much biased and restricted prior p(g)). The seminal work Bergstra et al. (2011) optimized
hyper-parameters using posterior-based fitness modeling. Recently, Wenzel et al. (2020) proposed a
non-Bayesian approach of hyper-parameter ensembling that can estimate the predictive uncertainty
of an optimization algorithm by varying hyper-parameters. Our work generalizes from tuning a few
hyper-parameters to learning the entire optimizer space, and extends a Bayesian treatment. Since
L2O involves much higher-dimensional parameters compared to HPO, its uncertainty quantification
calls for computationally tractable and scalable approaches, which is detailed next.
3.2	L2O with Bayesian Uncertainty Quantification
Now that we have the optimizer space G parameterized in φ, an optimizer g is modeled as a random
vector in the space. That allows for reducing the modeling of optimizer uncertainty p(g|zt) to mod-
eling the posterior p(φ∣zj For simplicity, We use a Gaussian prior of φ: p(φ) 8 exp(-我 kΦk2)
where λ is a constant. We also follow the widely-adopted idea in (Gal & Ghahramani, 2016; Bishop
& Tipping, 2003; Ortega et al., 2012; Cao & Shen, 2020), to vieW the loss function value as propor-
tional to the negative logarithmic likelihood, i.e., p(xt∣zt-ι, g) = p(xt∣zt-ι, φ) 8 exp(-f (xφ)).
We can then calculate the posterior of the optimizer based upon equation (2):
T1
eχP (- Xf (xφ) - 2λkφk2)∙	⑶
t=1	t=1
Training. Maximum a posteriori (MAP) estimation of the optimizer parameters leads to a de-
generate distribution and point estimation Without uncertainty. Moreover, directly maximizing the
optimizer posterior in Eq. 3 via Markov chain Monte Carlo (MCMC) Would encounter computa-
tional intractability considering the high-dimensional space of the optimizer space parameterized in
L2O by neural netWork Weights. Instead, We introduce a variational distribution q(φ; θ) (Kingma
& Welling, 2013) With the variational parameter θ and accordingly We introduce the folloWing ob-
jective function Whose minimization is equivalent to maximizing the evidence loWer bound (ELBO)
(see Appendix D for the derivation):
T
-KL[q(Φ; θ)l∣p(Φ∣zτ)] = -Eφ〜q(φa X f (xφ) - KL[q(Φ; θ)∣∣p(Φ)]∙	(4)
t=1
We refer to the L2O under the neW meta-training loss (4) W.r.t θ as uncertainty-aWare L2O (UA-
L2O), Which can be trained end-to-end With the Gaussian parametrization and the reparametrization
p(φlzτ) H eχp(-2λkφk2) Y eχp(-f (χφ))
4
Published as a conference paper at ICLR 2022
Figure 1: Feed-forward computational chart for training UA-L2O. Notations are as in the main text.
trick (Kingma & Welling, 2013; Higgins et al., 2016). The feed-forward computational chart for
training UA-L2O is depicted in Figure 1.
Deployment. With the well-trained UA-L2O and the found θ*, We ideally assume the function
values of the trajectory are submartingales that E«~q@e*)f (xφ+ι) ≤ f(xt), Ee~q@e*) ∣f(xφ)∣ <
∞. Then based upon martingale convergence theorem (Doob, 1953) limt→∞ f(xt) = f(x∞)
exists almost surely, i.e. it converges with infinite optimization iterations. Within the finite steps,
UA-L2O has a well-defined approximated posterior q(φ; θ*), and hence the baked-in ability to
quantify optimizer uncertainty at the solution. Therefore, the posterior of solutions is calculated by
the integral with the variational distribution q(φ; θ*) as:
P(x*∣zτ)
/ p(χ*∣zτ, φ)q(φ; θ*)dφ,
(5)
which can be estimated by Monte Carlo sampling. Specifically, for estimating the solution posterior,
the learned model is run 10,000 times with random initialization and different trajectories zt accord-
ingly. The optimizer parameter θ is sampled from q(θ∖φ*) once per iteration, following variational
inference as in Kingma & Welling (2013) and Higgins et al. (2016).
4 Experiments
We examine our proposed UA-L2O on optimizing: (i) non-convex test functions, (ii) loss functions
in data privacy attack, and (iii) energy functions in predicting 3D protein-protein interactions (pro-
tein docking). We compare UA-L2O with three non-Bayesian methods: manually-designed Adam
(Kingma & Ba, 2014) and particle swarm optimization (PSO) (Kennedy & Eberhart, 1995) as well
as DM-LSTM (Andrychowicz et al., 2016a), an L2O method. We also compare to Adam and PSO
with hyper-parameter Bayesian optimization (Adam-BO and PSO-BO), Adam with learning rate en-
sembles (Adam_Ir_EnSembIe) and Adam with stochastic gradient MCMC (Adam-Noisy_Gradient).
Lastly we compare to Bayesian active learning (BAL) (Cao & Shen, 2020).
For Adam and PSO, we make their hyper-parameters probabilistic to calculate the posterior as shown
in Table 6 in Appendix C. All algorithms are run for 10,000 times with random initialization to obtain
the empirical posterior distributions p(x* ∣zτ), following Eq. 5 for UA-L2O and its counterparts for
others. We access the optimization uncertainty with the following expression (Ortega et al., 2012;
Cao & Shen, 2020):
P(lbσ ≤ I∣x* - x∣∣2 ≤ ubσ) = 1 - σ,	(6)
Solution sampling
Posterior estimation	and estimation	Calculating bounds Assaying posterior
p(x*)	x*~p(x*),	τσ = ubσ - lbσ count = count +1
χ = argmin /(x*)	if ∙ in confidence
interval of X
count = O	I RUnning for 10,000 times ∣	eσ = COUnt/10, OOO
• Ground truth XtrUe X Estimation x
Figure 2: Computational procedure for uncertainty quantification: given a designated confidence score σ,
calculating the length of confidence interval rσ and its corresponding estimated confidence σ .
5
Published as a conference paper at ICLR 2022
where lbσ and ubσ are the lower and upper bounds at the confidence level of σ, and we choose
the one with the lowest function value (we also study the mean value in Appendix E) to be the
estimated solution x. Ablation on different runs and comparison with competitors in wall-clock
time are presented in Appendix E. For the choice of assessing the proximity of solutions rather than
function values |f (x*) - f (X)|, the reason is that in many optimization applications (e.g. privacy
attack and protein docking), the objective function actually plays the role of the “tool” instead of the
“goal”, thus the function value is not the key metric to assess the quality of solutions in particular
when the objective function is non-convex and noisy.
Evaluation metrics. UA-L2O is designed for uncertainty awareness in the optimizer. To assess
the calibration of uncertainty quantification (UQ), we use the accuracy of confidence, following
(Ortega et al., 2012; Cao & Shen, 2020). In other words, we compare expected confidence levels
σ as in Eq. 6 (0.8 and 0.9 in this paper) and the empirical probability εσ of “success” when the
corresponding confidence intervals indeed contain the global optima. When there is a tie in the
major assessment metric σ - εσ, the tightness of the confidence interval for the solution’s proximity
to the global optimum (Eq. 6), rσ = ubσ - lbσ, serves as a tie-breaker. A schematic illustration of
the uncertainty assessment is shown in Figure 2.
Although UA-L2O is designed for the uncertainty awareness rather than the quality of optimization
solutions, we still assess potential benefit in optimization performance, using the expected distance
between the optimized solutions and the global optimum Eχ*~p(χ*∣zτ)∣∣x* - XtrUeI∣2∙ The lower
distances indicate the better solution quality (optimization performance).
Two-stage training. Due to the extremely high-dimensional optimizer space and the rugged poste-
rior landscape, it is over-challenging to directly train the model through the equivalency of ELBO
optimization in Equation (4). We therefore perform two-stage training where we first train our model
in a non-Bayesian way and then use the resulting θ as the warm start for fine-tuning the mean vari-
ational parameters in the second stage. We examine the performance of the warm-start model in
Appendix E to show the benefit of the second-stage training.
Implementations. The model is implemented in Tensorflow 1.13 (Abadi et al., 2016) and optimized
by Adam (Kingma & Ba, 2014). For the L2O architecture, we use the coordinate-wise LSTM from
(Andrychowicz et al., 2016a) containing 10,282 free parameters. The meta-training and test data are
described in Appendix B. For all experiments, the length of LSTM is set to be 20, with 5,000 training
epochs for both training stages. We tune the λ values in equations (3) and (4) from {0.1, 1, 10},
validated by validation performance in test functions and by the tightness of confidence intervals in
privacy attack and protein docking. The ablation study on λ is presented in Appendix C showing
that UA-L2O is not sensitive to the value of λ.
4.1	Non-Convex Test Functions
Setup. We first evaluate the performance on the test functions in the global optimization bench-
mark (Jamil & Yang, 2013). We choose three extremely rugged, non-convex functions, Rastrigin,
Ackley and Griewank with analytical forms shown in Appendix B, paired with dimensionalities in
{3, 6,..., 30}.
Results. We compare UA-L2O with competitors in optimization and UQ performances shown in
Figure 3. In the most relevant assessment metric, the accuracy of confidence levels (bottom two
rows), UA-L2O performs the best in all but few cases for all the three functions, outperforming
hand-engineered optimizers, non-Bayesian or Bayesian, and L2O without uncertainty awareness
(DM_LSTM). The few exceptions, such as the Griewank function in 27 dimensions, could not be
attributed to the higher dimensionality (as that was not the case for UA-L2O for Rastrigin and Ack-
ley functions). DM-LSTM also showed overconfidence in such exceptions but the few exceptions
cannot be attributed to L2O either.
Given that UA-L2O consistently outperforms competitors in confidence accuracy, comparing the
tightness of confidence intervals alone (Figure 9) is not meaningful. Note that other methods, when
having tighter confidence intervals, were often too overconfident to realize that the true global optima
were inside the intervals much less than their confidence levels suggested.
We also examined potential but not intended benefits of UA-L2O in optimization performance (top
row of Figure 3). Interestingly, UA-L2O even improved optimization performances for Ackley and
Griewank functions when the dimensionality is above 12, while being slightly worse in Rastrigin.
6
Published as a conference paper at ICLR 2022
=anhx X=川
Rastngin
Adam
*■ ■ AdamBO
→- PSO
PS0_B0
-*- BAL
→- DM_I_STM
→- UA-L20
6.016.03 8.018.03
3	6	9 12 15 18 21 24 27 30	3	6	9 12 15 18 21 24 27 30	3 6	9 12 15 18 21 24 27 30
Dimension	Dimension	Dimension
Figure 3: Optimization and uncertainty performance of different methods in three non-convex test functions.
Different column represents different functions, and each row stands for: (i) 1st row is for the non-intended
optimization performance, the lower the better; (ii) 2nd & 3rd rows, the most important metric for the intended
uncertainty calibration, are for the precision of the estimated confidence, lower values indicating more accurate
posterior estimation. The corresponding confidence intervals are shown in Appendix C.
These results echo our conjecture that, although our central goal is not for better optimization so-
lutions but for their better uncertainty awareness, better uncertainty calibration could sometimes
improve optimization performances.
We further plot the confidence for solution regions segmented in ∣∣x* - Xk 2 versus the corresponding
optimization performance ∣∣x* - XtrUeI∣2 (see Appendix A for detailed procedure) in Figure 10. We
find that when UA-L2O achieves better optimization performance (Ackley and Griewank as opposed
to Rastrigin), it generates solutions with higher confidence in the regions closer to the global optima
and with narrower distributions (more certain). In contrast the competitors do not show such trends.
4.2	Data Privacy Attack
Setup. We next apply our model to an application that critically needs UQ. As many machine learn-
ing models are deployed publicly, it is important to avoid leaking private and sensitive information,
such as financial data and health data. Data privacy attack (Nasr et al., 2018) studies this problem
by playing the role of hackers and attacking machine learning models to quantify the risk of privacy
leakage. Better attacks would help models to be better prepared for privacy defense.
We use the model and dataset in (Cao et al., 2019b), where each input has 9 features involving
patient genetic information shown in Appendix B, and the output is the probability of the clinical
significance (pathogenicity) for genetic variants in patients. We study the following model inversion
attack (Fredrikson et al., 2015): given 5 features X0 ∈ [0, 1]5 out of9 and the label y of each patient,
the privacy attack aims to recover the rest 4 features Xtrue ∈ [0, 1]4 (potentially sensitive patient
information). Therefore, the optimization is minx∈[0,1]4 P(g(X0, X) - y)2 for all patients where g
is a trained predictor. The closeness between the optimized and real input features can quantify the
risk of information leakage and the quality of the attack. We evaluate our method for all test cases
in (Cao et al., 2019b).
Results. We report the UQ and optimization performances of UA-L2O in Table 1. In UQ, UA-L2O’s
performance (the accuracy of estimated confidence) stands out against competitors, although not as
dominant as it does to competitors in test functions. Even in optimization, UA-L2O outperforms all
competitors, which again demonstrate the potential benefit of uncertainty-awareness to optimization.
7
Published as a conference paper at ICLR 2022
Table 1: Optimization and uncertainty performance of different methods in genetic data privacy attack.
Method	Ekx - XtrUek2	际.9 -。9|	|0.8 - 0.8|	ro.9	r0.8
Adam	0.39	0.90	0.80	0.08	0.06
Adam-BO	0.38	0.90	0.80	0.08	0.05
Adam」JEnsemble	0.35	0.90	0.80	0.01	0.009
Adam_Noisy-Gradient	0.57	0.90	0.80	0.01	0.01
PSO	0.54	0.08	0.10	0.54	0.34
PSOBO	0.53	0.06	0.15	0.48	0.42
BAL	0.52	0.90	0.80	0.01	0.01
DM-LSTM	0.34	0.09	0.77	0.09	0.02
UA-L2O	0.30	0.25	0.27	0.06	0.04
Note that PSO had slightly more accurate confidence but much looser intervals, suggesting flat
posteriors and leading to worse optimization.
We argue that, unlike pure optimization problems in test functions, the optimization for privacy
attack involves the generalization issue in the machine learning paradigm: less generalizable samples
pose challenges to not only optimization but also UQ. To verify this argument, we plot optimization
performance for individual samples versus their estimated posterior precision in Figure 4 showing
a positive correlation: samples easier to generalize are closer to the optima (more leftward in the
x-axis), and more precise in estimated confidence (more downward in the y-axis). The tightness of
confidence intervals is also well correlated with solution quality.
Figure 4: Optimization performance versus UQ results (0.9, 0.8, r0.9 and r0.8) of UA-L2O for 318 test sam-
ples in data privacy attack.
Similar to that in Section 4.1, we also plot confidence versus optimization performance in Figure
11, to further demonstrate the calibration capability of UA-L2O. To demonstrate the practical use
of developed UQ, we conduct an experiment on “failure” case detection, that we use assessed un-
certainties to probe the optimization performance of individual runs. Results in Appendix F show
UA-L2O provides the best detection.
4.3	Energy Functions for Protein Docking
Setup. We lastly examine UA-L2O using a bioinformat-
ics application: predicting the 3D structures of protein-
complexes (Smith & Sternberg, 2002) or protein dock-
ing. Ab initio protein docking can be recast as optimiz-
ing a noisy and expensive energy function in a high-
dimensional conformational space (Cao & Shen, 2020).
While solving such optimization problems still remains
difficult, quantifying the uncertainty of resulting solutions
is even more challenging. The compared state-of-the-art
method is BAL (Cao & Shen, 2020).
We calculate the energy function (objective function
f (x)) in a CHARMM19 force field as in (Moal & Bates,
2010). 25 protein-protein complexes are chosen from the
protein docking benchmark set 4.0 (Hwang et al., 2010)
as the training set, which is shown in Appendix B. For
Figure 5: Scatter plot of optimization per-
formance versus the length of confidence in-
terval for five cases in protein docking. The
Pearson correlation coefficient achieved by
BAL is -0.80, and that by UA-L2O is 0.59.
each complex, we choose 5 starting points (top-5 structure predictions from ZDOCK (Pierce et al.,
2014)). In total, our training set includes 125 samples. Moreover, we parameterize the search space
for flexible docking as R12 as in BAL. The resulting f(x) is fully differentiable in the search space.
We only consider 100 interface atoms due to the computational concern. The number of iterations
for one training epoch is 600 and in total we have 5,000 training epochs. Both BAL and UA-L2O
have 600 iterations during the testing stage.
8
Published as a conference paper at ICLR 2022
1.00
Figure 6: Estimated posterior distributions, confidence intervals and ground truth solutions for cases 1AK4_7,
3CPH_7 and 1HE8_3 in protein docking.
Table 2: Optimization and uncertainty performance of BAL and UA-L2O in protein docking.
Target PDB model 叫x* - Xtruek2 (A) 叫x* - XtrUek2 ∈ [lb0.9, ub0.9]? ub0.9 - lb0.9 (A)
BAL in PDB 1HE8_3
0.75
0.50
0.25
0.00
UA-L20 in PDB 1HE8 3
0.
0.50
0.25
2
3
4
5
l∣χ*-制2
0.00
0
(docking difficulty)	BAL	UA-L2O	BAL	UA-L2O	BAL	UA-L2O
1AHW_3 (easy)	1.89	1.11	No	Yes	2.20	0.79
1AK4_7 (easy)	2.45	1.13	Yes	Yes	1.93	1.11
3CPH_7 (medium)	3.89	3.11	No	No	1.70	1.62
1HE8_3 (medium)	3.05	1.42	Yes	Yes	2.24	1.32
1JMO_4 (difficult)	1.45	1.87	No	No	2.90	0.67
Results. We compare UA-L2O and BAL in Table 2. For UQ, UA-L2O not only achieves a more
accurate confidence as shown in Figure 6 and Appendix C (three of five cases had ground truths
located in 90% confidence intervals), but also has a clearer trend that the better solutions correspond
to tighter intervals (more certain) as in Figure 5 (with Pearson correlation coefficient of 0.59 versus
-0.8 for BAL). Even for optimization, UA-L2O outperforms other methods in four out of five cases.
The results show significant advantages of UA-L2O compared with the state-of-the-art BAL in both
UQ and optimization for protein docking.
4.4	Result Summary
We briefly summarize the results as follows.
•	In general, UA-L2O achieves better optimization performance with a few exceptions. This
echos the hypothesis that the assessment of uncertainty is able to enhance the search efficiency
and effectiveness during optimization.
•	UA-L2O provides the most accurate posterior estimation most of the time. This empirical evi-
dence verifies the effectiveness of our uncertainty-aware L2O inspired from theory.
•	Accurate posterior estimation is a key for UA-L2O’s superior calibration capability, such that
UA-L2O generates solutions closer to optima with higher confidence and tighter confidence inter-
vals, whereas competitors suffer from overconfidence or miscalibration. Such calibration capabil-
ity is critical to detect the failure cases in real-world applications.
5 Conclusions
Current optimization algorithms, even with uncertainty-awareness, do not address the uncertainty
arising within the optimizer itself. To close the gap, in this paper we attempt to ask and address
three fundamental questions, why modelling the optimizer uncertainty, what defines the optimizer
uncertainty, and how to enabling UQ during optimization. We first emphasize the uncertainty arising
from the optimizer is a crucial source that directly responses for the end solutions deriving, except
for data- and model-uncertainty. Next, we define the prior and likelihood of the optimizer which
determines the optimizer posterior and the optimizer uncertainty. We further leverage learning to
optimize (L2O) for the optimizer parameterization, treating an optimizer as a random sample from an
algorithmic space of iterative optimizers, with the end-to-end training pipeline built via variational
inference. Extensive experiments show UA-L2O achieves the optimization performance, superior
in two out of three test functions with the high variable dimension, best in the loss function in
data privacy attack, and exceeded in the energy function for protein docking in four out of five
cases. Besides, it delivers the most accurate posterior estimation and calibration capability, with the
solutions closer to the ground truth of a larger population and tighter confidence intervals. Our study
represents the first effort to recognize and quantify the uncertainty of the optimization algorithm,
with extensive support from numerical results and analysis.
9
Published as a conference paper at ICLR 2022
Acknowledgment
This project was in part supported by NSF (CCF-1943008 to YS, CCSS-2113904 to ZW). Portions
of this research were conducted with the advanced computing resources provided by Texas A&M
High Performance Research Computing.
Reproducibility S tatement
To ensure reproducibility, we use the Machine Learning Reproducibility Checklist v2.0, Apr. 7
2020 (Pineau et al., 2021).
• For all models and algorithms presented,
-	A clear description of the mathematical settings, algorithm, and/or model. We
clearly describe all of the settings, formulations, and algorithms in Section 3.
-	A clear explanation of any assumptions. We state our assumptions clearly in Sec-
tion 3.
-	An analysis of the complexity (time, space, sample size) of any algorithm. We do
not make the analysis.
•	For any theoretical claim,
-	A clear statement of the claim. We provide a clear statement in Section 3.
-	A complete proof of the claim. We do not make theoretical proofs.
• For all datasets used, check if you include:
-	The relevant statistics, such as number of examples. We provide all the related
statistics in Section 4 and Appendix B.
-	The details of train/validation/test splits. We give this information in our repository.
-	An explanation of any data that were excluded, and all pre-processing step. We
give this information in our repository.
-	A link to a downloadable version of the dataset or simulation environment. Our
repository contains all of the instructions to run experiments on the datasets in this
work. We put it in supplementary materials.
-	For new data collected, a complete description of the data collection process, such
as instructions to annotators and methods for quality control. We do not collect
or release new datasets.
• For all shared code related to this work, check if you include:
-	Specification of dependencies. We give installation instructions in the README of
our repository.
-	Training code. The training code is available in our repository.
-	Evaluation code. The evaluation code is available in our repository.
-	(Pre-)trained model(s). We do not release trained models.
-	README file includes table of results accompanied by precise command to run
to produce those results. We include a README with detailed instructions to repro-
duce all of our experimental results.
• For all reported experimental results, check if you include:
-	The range of hyper-parameters considered, method to select the best hyper-
parameter configuration, and specification of all hyper-parameters used to gen-
erate results. We provide all details of hyper-parameter tuning in Section 4.
-	The exact number of training and evaluation runs. Ten thousand independent rep-
etitions are conducted for each experiments.
-	A clear definition of the specific measure or statistics used to report results. We
provide all details of evaluation metrics in Section 4.
-	A description of results with central tendency (e.g. mean) & variation (e.g. error
bars). We report confidence intervals for all experiments in Section 4.
10
Published as a conference paper at ICLR 2022
-	The average runtime for each result, or estimated energy cost. We do not report
the running time or energy cost.
-	A description of the computing infrastructure used. A clear description is pre-
sented in Section 3.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSD116),pp. 265-283, 2016.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016a.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016b.
James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs KegL Algorithms for hyper-parameter
optimization. Advances in neural information processing systems, 24, 2011.
Christopher M Bishop and Michael E Tipping. Bayesian regression and classification. Nato Science
Series sub Series III Computer And Systems Sciences, 190:267-288, 2003.
Yue Cao and Yang Shen. Bayesian active learning for optimization and uncertainty quantification in
protein docking. Journal of chemical theory and computation, 16(8):5334-5347, 2020.
Yue Cao, Tianlong Chen, Zhangyang Wang, and Yang Shen. Learning to optimize in swarms. In
Advances in Neural Information Processing Systems, pp. 15018-15028, 2019a.
Yue Cao, Yuanfei Sun, Mostafa Karimi, Haoran Chen, Oluwaseyi Moronfoye, and Yang Shen. Pre-
dicting pathogenicity of missense variants with weakly supervised regression. Human mutation,
40(9):1579-1592, 2019b.
Tianlong Chen, Weiyi Zhang, Jingyang Zhou, Shiyu Chang, Sijia Liu, Lisa Amini, and Zhangyang
Wang. Training stronger baselines for learning to optimize. arXiv preprint arXiv:2010.09089,
2020a.
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and
Wotao Yin. Learning to optimize: A primer and a benchmark. arXiv preprint arXiv:2103.12828,
2021.
Xuxi Chen, Wuyang Chen, Tianlong Chen, Ye Yuan, Chen Gong, Kewei Chen, and Zhangyang
Wang. Self-pu: Self boosted and calibrated positive-unlabeled training. In International Confer-
ence on Machine Learning, pp. 1510-1519. PMLR, 2020b.
Joseph Leo Doob. Stochastic processes, volume 10. New York Wiley, 1953.
Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated Machine Learning,
pp. 3-33. Springer, Cham, 2019.
CraIg R Fox and Gulden Ulkumen. Distinguishing two dimensions of uncertainty. Perspectives on
thinking, judging, and decision making, 2011.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322-1333, 2015.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
11
Published as a conference paper at ICLR 2022
David E Goldenberg. Genetic algorithms in search, optimization and machine learning, 1989.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp.1321-1330.JMLR. org, 2017.
Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global optimiza-
tion. The Journal ofMachine Learning Research, 13(1):1809-1837, 2012.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
HoWook Hwang, Thom Vreven, Joel Janin, and ZhiPing Weng. Protein-Protein Docking Benchmark
Version 4.0. Proteins, 78(15):3111-3114, November 2010. ISSN 0887-3585. doi: 10.1002/prot.
22830. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2958056/.
Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation
problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2):
150-194, 2013.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision?, 2017.
J Kennedy and R Eberhart. Particle swarm optimization, proceedings of ieee international confer-
ence on neural networks (icnn’95) in, 1995.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.
science, 220(4598):671-680, 1983.
Chaojian Li, Tianlong Chen, Haoran You, Zhangyang Wang, and Yingyan Lin. Halo: Hardware-
aware learning to optimize. In European Conference on Computer Vision, pp. 500-518. Springer,
2020.
Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernet-
works. arXiv preprint arXiv:1802.09419, 2018.
Gangming Lyu, Yuning You, Ang Li, Xuewen Liao, and Christos Masouros. Probabilistic construc-
tive interference precoding for imperfect csit. IEEE Transactions on Vehicular Technology, 70(4):
3932-3937, 2021.
Iain H. Moal and Paul A. Bates. SwarmDock and the Use of Normal Modes in Protein-Protein Dock-
ing. International Journal of Molecular Sciences, 11(10):3623-3648, September 2010. ISSN
1422-0067. doi: 10.3390/ijms11103623. URL https://www.ncbi.nlm.nih.gov/pmc/
articles/PMC2996808/.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning:
Stand-alone and federated learning under passive and active white-box inference attacks. arXiv
preprint arXiv:1812.00910, 2018.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Pedro Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, and Daniel Braun. A nonparamet-
ric conjugate prior distribution for the maximizing argument of a noisy function. In Advances in
Neural Information Processing Systems, pp. 3005-3013, 2012.
12
Published as a conference paper at ICLR 2022
Vikas Pejaver, Jorge Urresti, Jose Lugo-Martinez, Kymberleigh A Pagel, Guan Ning Lin, Hyun-
Jun Nam, Matthew Mort, David N Cooper, Jonathan Sebat, Lilia M Iakoucheva, et al. Mutpred2:
inferring the molecular and phenotypic impact of amino acid variants. BioRxiv, pp. 134981, 2017.
Brian G. Pierce, Kevin Wiehe, Howook Hwang, Bong-Hyun Kim, Thom Vreven, and Zhip-
ing Weng. ZDOCK server: interactive docking prediction of protein-protein complexes and
symmetric multimers. Bioinformatics, 30(12):1771-1773, 02 2014. ISSN 1367-4803. doi:
10.1093/bioinformatics/btu097. URL https://doi.org/10.1093/bioinformatics/
btu097.
Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer,
Florence d'Alche Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine
learning research. Journal of Machine Learning Research, 22:1-20, 2021.
K. A. Porter, I. Desta, D. Kozakov, and S. Vajda. What method to use for protein-protein docking?
Curr. Opin. Struct. Biol., 55:1-7, 04 2019.
Jiayi Shen, Xiaohan Chen, Howard Heaton, Tianlong Chen, Jialin Liu, Wotao Yin, and Zhangyang
Wang. Learning a minimax optimizer: A pilot study. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=nkIDwI6oO4_.
Graham R Smith and Michael JE Sternberg. Prediction of protein-protein interactions by docking
methods. Current opinion in structural biology, 12(1):28-35, 2002.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian pro-
cess optimization in the bandit setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995, 2009.
Emmanuel Vazquez and Julien Bect. Convergence properties of the expected improvement algo-
rithm with fixed mean and covariance functions. Journal of Statistical Planning and inference,
140(11):3088-3095, 2010.
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. In
International Conference on Machine Learning, pp. 3627-3635, 2017.
Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for
robustness and uncertainty quantification. arXiv preprint arXiv:2006.13570, 2020.
Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In International Confer-
ence on Machine Learning, pp. 5660-5669. PMLR, 2018.
Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. L2-gcn: Layer-wise and learned
efficient training of graph convolutional networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 2127-2135, 2020.
Anatoly A Zhigljavsky. Theory of global random search, volume 65. Springer Science & Business
Media, 2012.
13
Published as a conference paper at ICLR 2022
Appendix
A Confidence Estimation Procedures
In evaluation, for the confidence versus optimization performance scatter plot, we first perform
Monte Carlo sampling on the solution posterior, and then bin the solutions to estimate their fraction
and corresponding average distance to the optimum. Detailed procedures can be found in Algorithms
1.
Algorithm 1: Confidence estimation with corresponding optimization performance
Input: Sampled solutions x* with size N, estimated solution X, known optimum xtrue, bin width
w.
Initialize: S = {}.
1.	Calculate ∣∣x* - X∣∣2 for all sampled solutions, and bin them with w.
for each bin b do
2.	Calculate the population fraction |b|/N .
3.	Calculate average distance to XtrUe: Mean({∣x* - XtrUeI∣2∣x* ∈ b}).
4.	S - S ∪ (|b|/N, Mean({kχ* - χtrue∣∣2 |x* ∈ b})).
end for
Return: Scatter point set S.
B	Data Description
The analytical forms of three test functions are shown in Table 3. The meta-training data are a
broad family of similar functions (Cao et al., 2019a). For Rastrigin, it is fD(x) = ∣Ax - b∣22 -
c PiD=1 10 cos(2πxi) + 10D where A, b, c are sampled from i.i.d normal distributions; for Ackley,
it is /d(x) = —20exp(-0.2p0.51Ax — b∣2) — CPD=I exp(cos(2πxi)/D) + exp(1) + 20; for
Griewank, it is fp(x) = 1 + PD=IIlAX - b∣2∕4000 - C QD=I Cos(Xi).
For privacy attack, we use 9-dimension features of privacy attack data generated from MutPred2
(Pejaver et al., 2017), which captures mutational impacts on protein structure, dynamics, and func-
tion, and grouped hierarchically into a custom oncology based on their inherent relationships. The
list of features can be found in Table 4. The meta-training is performed on 1,593 patients and we
evaluate in 318 patients (Cao et al., 2019b).
For protein docking, the PDB IDs of meta-training samples are shown in Table 5 and we test in
proteins with PDB in Table 2.
Table 3: The analytical forms of test functions fD (x) with the dimension D, and their corresponding flatness
around global optima, estimated with the norm of Hessain matrix.
Function	Analytic form
Rastrigin	fD(x) = IlXk2 — PD=I 10cos(2πxi) + 10D
Ackley	fD(x) = -20exp(-0.2p0.5∣xk2) — PD=I exp(cos(2πXi)∕D) + exp(1) + 20
Griewank	fD(x) = 1 + PiD=1IxI22∕4000 - QiD=1 cos(xi)
C More Evaluation Results
We provide more evaluation results as follows. (i) The ablation of λ values on three test functions is
shown in Figure 7. (ii) The UQ results for test cases 1AHW_3 and 1JMO_4 in protein docking are
plotted in Figure 8. (iii) Confidence intervals corresponding to results in Figure 3.
14
Published as a conference paper at ICLR 2022
Table 4: 9-dimension feature description for data privacy attack experiment.
Feature index
1
2
3
4
5
6
7
8
9
PrOperty/molecular impact
Relative solvent accessibility
Allosteric site
Catalytic site
Secondary structure
Stability and conformation flexibility
Special structural signatures
Macromolecular binding
Metal binding
PTM site
Table 5: 4-letter ID of proteins used in protein docking training set.
Difficulty level
Protein Data Bank (PDB) code
Rigid
--I—⅛ 4 4-一≡34xl3i2uj2 --I—<⅛
Medium
Flexible
1N8O, 7CEI, 1DFJ, 1AVX, 1BVN, 1IQD, 1CGI, 1MAH, 1EZU,
1JPS,1PPE,1R0R, 2I25, 2B42,1EAW, 2JEL,1BJ1,1KXQ,1EWY
1XQS,1M10,1IJK,1GRN
1IBR,1ATN
Figure 7: Optimization and uncertainty performance of UA-L2O with different λ in three non-convex test
functions. Different row represents different functions, and each column stands for: (i) 1st column is OPtimiza-
tion performance, lower the better; (ii) 2nd column is precision of the estimated confidence, lower denoting
more accurate posterior estimation; (iii) 3rd is the length of the confidence interval: only when with similar
precision of the estimated confidence would a tighter interval indicating more certain solutions.
9 12 15 18 21 24 27 30
9 12 15 18 21 24 27 30
9 12 15 18 21 24 27 30
9 12 15 18 21 24 27 30
1.7
3 6 9 12 15 18 21 24 27 30
Dimension
9 12 15 18 21 24 27 30
Table 6:	The optimizer distributions over hyper-parameters in Adam and PSO.
Methods	Optimizer Distribution Settings
Adam	log10(lr)〜U[-2,-1], βι ~U[0.9,1.0], β2 〜U[0.999, 1.0]
PSO W 〜U[0.5, 1.5], C1 〜U[1.5, 2.5], C2 〜U[1.5, 2.5]
15
Published as a conference paper at ICLR 2022
O
2
5 Q 5
LL
Λ4-suα
BAL in PDB 1AHW_3
I
90% confidence interval
5% tails
EilX - ×truel∣2
2.0
1.5
1.0
BAL in PDB 1JMO_4
UA-L2O in PDB IAHW 3
2.0η	1	-
UA-L2O in PDB 1JMO_4
Λ4-suluα
Q
0.5
0.0^-TB---------1 0.0^-——LI-------1
0	1	2	3	4	50123456
l∣χ*-⅜	l∣χ*-χ∣∣2
Rastriqin
Ackley
Griewank
2.4
2.2
∣2.0
1.8
1.6
1.8
1.6
1.4
1.2
12 15 18 21 24 27 30
12 15 18 21 24 27 30
15
10
12 15 18 21 24 27 30
Dimension
12 15 18 21 24 27 30
Dimension
Figure 9: The corresponding confidence intervals to results in Figure 3, a secondary metric and tie-breaker for
uncertainty calibration accuracy and a measure of the length of the confidence interval: only when the estimated
confidence levels are similarly precise, tighter intervals indicates higher certainty about solutions
Figure 8: Estimated posterior distributions, confidence intervals and ground truth solutions for cases 1AHW_3
and 1JMO_4 in protein docking.
Dimension
D Computation of UA-L2O Objective Function
We first rewrite the evidence as:
log p(zT |zt0)
/ q(φ; θ)logP(ZT∣Zto )dφ
∕q(φ; θ)iog-dφ
/ q(φ; θ)logP(ZT, Φ∣Zto )dφ - / q(φ; θ)logp(Φ∣zτ)dφ
-/ q(φ;θ)log Pznhdφ+/ q@θ)log Mdφ
-KL[q(Φ; θ)l∣p(zτ, Φ∣zto)] + KL[q(Φ; Θ)∣∣p(Φ∣zt )],
where -KL[q(φ; θ)∣∣p(zτ, φ∣zt0)] is ELBO and KL[q(φ; θ)∣∣p(φ∣zτ)] is our objective. Therefore,
minimizing KL[q(φ; θ) ∣∣p(φ∣zτ)] is equivalent to maximizing ELBO (Yin & Zhou, 2018).
16
Published as a conference paper at ICLR 2022
Rastrigin, dimension=15
2	4	6	8
Ackley, dimension=15
Figure 10: Scatter plot of confidence for solution regions segmented by ∣∣x* — X∣∣2, versus the corresponding
optimization performance ∣∣x* -XtrUe k 2 in three test functions. The peak closer to left side indicates the solution
populations are in high confidence if they are closer to the optima.
Figure 11: Scatter plot of confidence for solution regions segmented by ∣∣x* — X∣2, versus the corresponding
optimization performance ∣∣x* — XtrUek2 in data privacy attack.
We rewrite our objective as:
KL[q(Φ; θ)l∣p(Φ∣zτ)] = Eφ〜qae){ log q(φ; θ) - logp(Φ∣zτ)}
=Eφ〜q(φa {log q(φ; θ) - logp(Φ)p(zt 应。,φ)} 十 Z
=Eφ〜q(φ[θ){ log qP^θ) - logP(ZT ∣Zto , φ)} + Z
T
=Eφ〜q(φa X f(χφ) + KL[q(Φ;θ)l∣p(Φ)] + Z
t=t0+1
E	Ablation Study
We perform ablation studies on sample size in uncertainty quantification of UA-L2O in Table 7,
mean versus minimum of X computation and wall-clock time comparison in Table 8, on privacy
attack experiment on gene BRCA1/2 in Table 9. Results show UA-L2O is not sensitive to sample
size or mean/minimum calculation of X, and it is comparably efficient with competitor optimizers.
Moreover, we include a study of the wall-time versus dimensionality ranging from 10 to 2,000, using
the optimization of Rastrigin function in Figure 10. We report UA-L2O’s wall-time for both meta-
training and meta-testing. The results above show that the meta-training time grows almost linearly
with the dimensionality when dimensionality >200 (which is partly attributed to the fact that the
number of samples is now adopted as a value correlated with the dimension). So meta-training even
17
Published as a conference paper at ICLR 2022
in millions of dimensions for deep-learning model parameters is expensive yet still manageable,
especially using multiple GPU cores. Most importantly, meta-testing was much faster and the time
remained flat with regards to the dimensions, that once UA-L2O is trained, its deployment remains
fast and scales well to the dimensionality. In fact, L2O usually takes the setting of offline training
(which could be time-consuming) and online deployment (which is fast enough and scales well)
Chen et al. (2021).
We further compare with baselines as learning rate ensemble and stochastic gradient MCMC of
Adam, shown in Table 11. We also make comparison with warm-start UA-L2O to show the im-
provement of UA-L2O training.
Table 7:	Ablation of sample size in posterior estimation on BRCA1/2 privacy attack.
Samplesize	E∣∣x*	- Xtruek2	I	怕0.9	-。⑼	际.8	- 0∙8∣ I	r0.9	r0.8
100	0.30	0.25	0.25	0.06	0.06
1,000	0.30	0.27	0.31	0.06	0.04
10,000	0.30	0.25	0.27	0.06	0.04
Table 8: Ablation of mean/minimum of X computation in posterior estimation on BRCA1/2 privacy attack.
E∣∣X* - Xtruek2 | 岛.9 - 0∙9∣	k0.8 - 0∙8∣ ∣ r0.9	r0.8
Mean	0.31	I	0.27	0.26	I 0.07	0.04
Min	0.30	I	0.25	0.27	| 0.06	0.04
Table 9:	Comparison of wall-clock time on BRCA1/2 privacy attack.
Adam	Adam-BO	PSO	PSO-BO	BAL	DM-LSTM	UA-L2O
Wall-Clock Time (s)	30	33	22	24	43	35	41
Table 10:	Comparison of wall-clock time on Rastrigin function with different dimensionality.
Dimension Meta-training (s) Meta-test (s)
10	607	36
100	610	38
200	662	37
500	1,059	38
1,000	1,998	38
2,000	3,904	38
Table 11: More baseline comparison of UA-L2O on privacy attack of gene BRCA1/2.
E∣∣X* - XtrUek2 | 同.9 -。刈	∣S.8 - 0∙8∣ |『0.9	『0.8
UA-L2O Warm-Start Only	0.76	I	0.55	0.57	I 0.40	0.30
UA-L2O	0.30	I	0.25	0.27	∣ 0.06	0.04
F	Utilization of Optimizer Uncertainty
We demonstrate the “usefulness” of the proposed optimizer uncertainty via performing the experi-
ment of “failure detection” as a basic downstream task.
Specifically, an optimization solution is defined as “failure” if its distance to the global optimum
is beyond a threshold 0.5√d (where d is the dimension and the threshold was chosen for approxi-
mately balanced success/failure cases). Instead of fixing the confidence level σ and estimating the
confidence interval r (as observed in Figure 4 that better optimization correlated better with tighter
confidence intervals), here We let each method estimate the confidence level σ at fixed r < 0.5√d
(“success”) for the optimization solution in each trajectory and use σ to classify the quality of such
solutions over 1000 trajectories. The classification performance was assessed using the area under
18
Published as a conference paper at ICLR 2022
the receiver-operator characteristic curve (AUROC) and the area under the precision-recall curve
(AUPRC). The results on privacy attack are shown in Table 12, indicating the best posterior estima-
tion of UA-L2O.
Table 12: “Failure” case detection with optimizer uncertainties on privacy attack.
	Adam	AdamJBO	PSO	PSO_BO	BAL	DM-LSTM	UA-L2O
AUROC	0.53	0.59	0.74	0.78	0.00	0.37	0.99
AUPRC	0.97	0.98	0.73	0.90	0.00	0.95	0.99
19