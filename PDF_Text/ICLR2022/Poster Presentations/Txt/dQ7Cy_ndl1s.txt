Published as a conference paper at ICLR 2022
Controlling the Complexity and Lipschitz
Constant improves Polynomial Nets
Zhenyu Zhu, Fabian Latorre, Grigorios G Chrysos, Volkan Cevher
EPFL, Switzerland
{[first name].[surname]}@epfl.ch
Ab stract
While the class of Polynomial Nets demonstrates comparable performance to neural
networks (NN), it currently has neither theoretical generalization characterization
nor robustness guarantees. To this end, we derive new complexity bounds for the
set of Coupled CP-Decomposition (CCP) and Nested Coupled CP-decomposition
(NCP) models of Polynomial Nets in terms of the '∞-operator-norm and the '2-
operator norm. In addition, we derive bounds on the Lipschitz constant for both
models to establish a theoretical certificate for their robustness. The theoretical
results enable us to propose a principled regularization scheme that we also evaluate
experimentally in six datasets and show that it improves the accuracy as well as
the robustness of the models to adversarial perturbations. We showcase how
this regularization can be combined with adversarial training, resulting in further
improvements.
1 Introduction
Recently, high-degree Polynomial Nets (PNs) have been demonstrating state-of-the-art performance
in a range of challenging tasks like image generation (Karras et al., 2019; Chrysos and Panagakis,
2020), image classification (Wang et al., 2018), reinforcement learning (Jayakumar et al., 2020),
non-euclidean representation learning (Chrysos et al., 2020) and sequence models (Su et al., 2020).
In particular, in public benchmarks like the Face verification on MegaFace task1 (Kemelmacher-
Shlizerman et al., 2016), Polynomial Nets are currently the top performing model.
A major advantage of Polynomial Nets over traditional Neural Networks2 is that they are compatible
with efficient Leveled Fully Homomorphic Encryption (LFHE) protocols (Brakerski et al., 2014).
Such protocols allow efficient computation on encrypted data, but they only support addition or
multiplication operations i.e., polynomials. This has prompted an effort to adapt neural networks by
replacing typical activation functions with polynomial approximations (Gilad-Bachrach et al., 2016;
Hesamifard et al., 2018). Polynomial Nets do not need any adaptation to work with LFHE.
Without doubt, these arguments motivate further investigation about the inner-workings of Polynomial
Nets. Surprisingly, little is known about the theoretical properties of such high-degree polynomial
expansions, despite their success. Previous work on PNs (Chrysos et al., 2020; Chrysos and Panagakis,
2020) have focused on developing the foundational structure of the models as well as their training,
but do not provide an analysis of their generalization ability or robustness to adversarial perturbations.
In contrast, such type of results are readily available for traditional feed-forward Deep Neural
Networks, in the form of high-probability generalization error bounds (Neyshabur et al., 2015;
Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al., 2018) or upper bounds on their Lipschitz
constant (Scaman and Virmaux, 2018; Fazlyab et al., 2019; Latorre et al., 2020). Despite their
similarity in the compositional structure, theoretical results for Deep Neural Networks2 do not apply
to Polynomial Nets, as they are essentialy two non-overlapping classes of functions.
Why are such results important? First, they provide key theoretical quantities like the sample
complexity of a hypothesis class: how many samples are required to succeed at learning in the
PAC-framework. Second, they provide certified performance guarantees to adversarial perturbations
1 https://paperswithcode.com/sota/face- verification- on- megaface
2with non-polynomial activation functions.
1
Published as a conference paper at ICLR 2022
(Szegedy et al., 2014; Goodfellow et al., 2015) via a worst-case analysis c.f. Scaman and Virmaux
(2018). Most importantly, the bounds themselves provide a principled way to regularize the hypothesis
class and improve their accuracy or robustness.
For example, Generalization and Lipschitz constant bounds of Deep Neural Networks that depend
on the operator-norm of their weight matrices (Bartlett et al., 2017; Neyshabur et al., 2017) have
layed out the path for regularization schemes like spectral regularization (Yoshida and Miyato, 2017;
Miyato et al., 2018), Lipschitz-margin training (Tsuzuku et al., 2018) and Parseval Networks (Cisse
et al., 2017), to name a few.
Indeed, such schemes have been observed in practice to improve the performance of Deep Neural
Networks. Unfortunately, similar regularization schemes for Polynomial Nets do not exist due to
the lack of analogous bounds. Hence, it is possible that PNs are not yet being used to their fullest
potential. We believe that theoretical advances in their understanding might lead to more resilient and
accurate models. In this work, we aim to fill the gap in the theoretical understanding of PNs. We
summarize our main contributions as follows:
Rademacher Complexity Bounds. We derive bounds on the Rademacher Complexity of the Cou-
pled CP-decomposition model (CCP) and Nested Coupled CP-decomposition model (NCP) of PNs,
under the assumption of a unit '∞-norm bound on the input (Theorems 1 and 3), a natural assumption
in image-based applications. Analogous bounds for the '2-norm are also provided (Appendices E.3
and F.3). Such bounds lead to the first known generalization error bounds for this class of models.
Lipschitz constant Bounds. To complement our understanding of the CCP and NCP models, we
derive upper bounds on their '∞-Lipschitz constants (Theorems 2 and 4), which are directly related
to their robustness against '∞-bounded adversarial perturbations, and provide formal guarantees.
Analogous results hold for any `p-norm (Appendices E.4 and F.4).
Regularization schemes. We identify key quantities that simultaneously control both Rademacher
Complexity and Lipschitz constant bounds that we previously derived, i.e., the operator norms of
the weight matrices in the Polynomial Nets. Hence, we propose to regularize the CCP and NCP
models by constraining such operator norms. In doing so, our theoretical results indicate that both the
generalization and the robustness to adversarial perturbations should improve. We propose a Projected
Stochastic Gradient Descent scheme (Algorithm 1), enjoying the same per-iteration complexity as
vanilla back-propagation in the '∞-norm case, and a variant that augments the base algorithm with
adversarial traning (Algorithm 2).
Experiments. We conduct experimentation in five widely-used datasets on image recognition and on
dataset in audio recognition. The experimentation illustrates how the aforementioned regularization
schemes impact the accuracy (and the robust accuracy) of both CCP and NCP models, outperforming
alternative schemes such as Jacobian regularization and the L2 weight decay. Indeed, for a grid of
regularization parameters we observe that there exists a sweet-spot for the regularization parameter
which not only increases the test-accuracy of the model, but also its resilience to adversarial pertur-
bations. Larger values of the regularization parameter also allow a trade-off between accuracy and
robustness. The observation is consistent across all datasets and all adversarial attacks demonstrating
the efficacy of the proposed regularization scheme.
2 Rademacher Complexity and Lipschitz constant bounds for
Polynomial Nets
Notation. The symbol ◦ denotes the Hadamard (element-wise) product, the symbol • is the face-
splitting product, while the symbol ? denotes a convolutional operator. Matrices are denoted by
uppercase letters e.g., V . Due to the space constraints, a detailed notation is deferred to Appendix C.
Assumption on the input distribution. Unless explicitly mentioned otherwise, we assume an
'∞-norm unit bound on the input data i.e., kz∣∣∞ ≤ 1 for any input z. This is the most common
assumption in image-domain applications in contemporary deep learning, i.e., each pixel takes values
in [-1,1] interval. Nevertheless, analogous results for '2-norm unit bound assumptions are presented
in Appendices E.3, E.4, F.3 and F.4.
We now introduce the basic concepts that will be developed throughout the paper i.e., the Rademacher
Complexity of a class of functions (Bartlett and Mendelson, 2002) and the Lipschitz constant.
2
Published as a conference paper at ICLR 2022
Figure 1: Schematic of CCP model (left) and NCP model (right), where ◦ denotes the Hadamard
product. Blue boxes correspond to learnable parameters. Green and red boxes denote input and
output, respectively. Yellow boxes denote operations.
Definition 1 (Empirical Rademacher Complexity). Let Z = {z1, . . . , zn} ⊆ Rd and let
{σj : j = 1, . . . , n} be independent Rademacher random variables i.e., taking values uniformly
in {-1, +1}. Let F be a class of real-valued functions over Rd. The Empirical Rademacher
complexity of F with respect to Z is defined as follows:
n
RZ(F) := Eσ sup
f∈F
1n
n X σf(Zj) .
j=1
Definition 2 (Lipschitz constant). Given two normed spaces (X, k ∙ ∣∣χ) and (Y, k ∙ ∣∣γ), afunction
f : X → Y is called Lipschitz continuous with Lipschitz constant K ≥ 0 if for all x1, x2 in X:
∣f(x1) -f(x2)∣Y ≤ K∣x1 - x2∣X .
2.1	Coupled CP-Decomposition of Polynomial Nets (CCP model)
The Coupled CP-Decomposition (CCP) model of PNs (Chrysos et al., 2020) leverages a coupled CP
Tensor decomposition (Kolda and Bader, 2009) to vastly reduce the parameters required to describe a
high-degree polynomial, and allows its computation in a compositional fashion, much similar to a
feed-forward pass through a traditional neural network. The CCP model was used in Chrysos and
Panagakis (2020) to construct a generative model. CCP can be succintly defined as follows:
f(z) = C 吊=1 Uiz ,	(CCP)
where z ∈ Rd is the input data, f(z) ∈ Ro is the output of the model and Un ∈ Rm×d, C ∈ Ro×m
are the learnable parameters, where m ∈ N is the hidden rank. In Fig. 1 we provide a schematic of
the architecture, while in Appendix D.1 we include further details on the original CCP formulation
(and how to obtain our equivalent re-parametrization) for the interested reader.
In Theorem 1 We derive an upper bound on the complexity of CCP models with bounded '∞-operator-
norms of the face-splitting product of the weight matrices. Its proof can be found in Appendix E.1.
For a given CCP model, we derive an upper bound on its '∞-Lipschitz constant in Theorem 2 and its
proof is given in Appendix E.4.1.
Theorem 1.	Let Z = {z1, . . . , zn} ⊆ Rd and suppose that ∣zj ∣∞ ≤ 1 for all j = 1, . . . , n. Let
FCCP := {f(Z) =〈c,OhUiz) : kckι ≤ μ, ∣∣∙k=ιui∣∣∞ ≤ λ}.
The Empirical Rademacher Complexity of CCPk (k-degree CCP polynomials) with respect to Z is
bounded as:	__________
RZ(FCCP) ≤ 2μλ J2klog(d) .
n
Proof sketch of Theorem 1 We now describe the core steps of the proof. For the interested reader,
the complete and detailed proof steps are presented in Appendix E.1. First, Holder,s inequality is
used to bound the Rademacher complexity as:
RZ(FCCP) = E SUp 1 (G X[σjθk=ι(Uizj)]) ≤ E SUp IkckI
f∈FCkCP n	j=1	f∈FCkCP n
n
X[σjθC=ι(Uizj)]
j=1
∞
(1)
3
Published as a conference paper at ICLR 2022
This shows Why the factor ∣∣ckι ≤ μ appears in the final bound. Then, using the mixed product
property (Slyusar, 1999) and its extension to repeated Hadamard products (Lemma 7 in Appendix C.3),
we can rewrite the summation in the right-hand-side of (1) as follows:
nn	n
Xσj。忆ι(Uizj) = Xσj ∙k=ι (Ui) ^k=ι (Zj) = ∙k=ι(5) Xσj *k=1 (Zj).
j=1	j=1	j=1
This step can be seen as a linearization of the polynomial by lifting the problem to a higher dimensional
space. We use this fact and the definition of the operator norm to further bound the term inside
the '∞-norm in the right-hand-side of (1). Such term is bounded as the product of the '∞-operator
norm of ∙k=ι(Ui), and the '∞-norm of an expression involving the Rademacher variables σj and
the vectors *k=ι (zj). Finally, an application of Massart,s Lemma (Shalev-Shwartz and Ben-David
(2014), Lemma 26.8) leads to the final result.
Theorem 2.	The LiPschitz Constant(With respect to the '∞-norm) ofthefUnction defined in Eq. (CCP),
restricted to the set {Z ∈ Rd : ∣Z∣∞ ≤ 1} is bounded as:
k
Lip∞(f) ≤ k∣C∣∞ Y ∣Ui∣∞.
i=1
2.2 Nested Coupled CP-Decomposition (NCP model)
The Nested Coupled CP-Decomposition (NCP) model leverages a joint hierarchical decomposition,
which provided strong results in both generative and discriminative tasks in Chrysos et al. (2020). A
slight re-parametrization of the model (Appendix D.2) can be expressed with the following recursive
relation:
x1 = (A1Z) 。 (s1 ),	xn = (AnZ) 。 (Snxn-1 ),	f (Z) = Cxk , (NCP)
where Z ∈ Rd is the input vector and C ∈ Ro×m , An ∈ Rm×d, Sn ∈ Rm×m and s1 ∈ Rm are the
learnable parameters. In Fig. 1 we provide a schematic of the architecture.
In Theorem 3 we derive an upper bound on the complexity ofNCP models with bounded '∞-operator-
norm of a matrix function of its parameters. Its proof can be found in Appendix F.1. For a given NCP
model, we derive an upper bound on its '∞-Lipschitz constant in Theorem 4 and its proof is given in
Appendix F.4.1.
Theorem 3.	Let Z = {Z1, . . . , Zn} ⊆ Rd and suppose that ∣Zj ∣∞ ≤ 1for all j = 1, . . . , n. Define
the matrix Φ(A1, S1, . . . , An, Sn) := (Ak ∙Sk) ik=-11 I0 Ai ∙ Si. Consider the class Offunctions:
FCP := {f (z) as in (NCP) : ∣C∣∞ ≤ μ, ∣Φ(Aι, Si,…，Ak, Sk)∣∞ ≤ λ},
where C ∈ R1×m (single output), thus, we will write it as c, and the corresponding bound also
becomes ∣∣c∣ι ≤ μ. The Empirical Rademacher Complexity ofNCP k (k-degree NCP polynomials)
with respect to Z is bounded as:
RZ(FNCP) ≤ 2μλ{2k lng(d) .
Theorem 4.	The Lipschitz constant (with respect to the '∞-norm) ofthefunction defined in Eq. (NCP),
restricted to the set {Z ∈ Rd : ∣Z∣∞ ≤ 1} is bounded as:
k
Lip∞(f) ≤ k∣C∣∞ Y(∣Ai∣∞∣Si∣∞).
i=1
3 Algorithms
By constraining the quantities in the upper bounds on the Rademacher complexity (Theorems 1
and 3), we can regularize the empirical loss minimization objective (Mohri et al., 2018, Theorem 3.3).
Such method would prevent overfitting and can lead to an improved accuracy. However, one issue
with the quantities involved in Theorems 1 and 3, namely
k-1
∙ik=1Ui∞,	(Ak∙Sk)YI0Ai∙Si	,
i=1	∞
4
Published as a conference paper at ICLR 2022
is that projecting onto their level sets correspond to a difficult non-convex problem. Nevertheless, we
can control an upper bound that depends on the '∞-operator norm of each weight matrix:
Lemma 1. It holds that •ik=1Ui ∞ ≤ Qik=1 kUik∞.
Lemma 2. It holds that I(Ak • Sk) Qk∑1 I ③ Ai • S』≤ Qk=III Aik∞IlSill∞∙
The proofs of Lemmas 1 and 2 can be found in Appendix E.2 and Appendix F.2. These results mean
that by constraining the operator norms of each weight matrix, we can control the overall complexity
of the CCP and NCP models.
Projecting a matrix onto an '∞-operator norm ball is a simple task that can be achieved by projecting
each row of the matrix onto an `1 -norm ball, for example, using the well-known algorithm from
Duchi et al. (2008). The final optimization objective for training a regularized CCP is the following:
1n
min	— TL(G Ui,…,Uk ； Xi,yi)	subject to IlCI∣∞ ≤ μ, ∣Uik∞ ≤ λ, (2)
C,U1,...,Uk n
i=1
where (xi, yi)n=ι is the training dataset, L is the loss function (e.g., cross-entropy) and μ, λ are the
regularization parameters. We notice that the constraints on the learnable parameters Ui andC	have
the effect of simultaneously controlling the Rademacher Complexity and the Lipschitz constant of
the CCP model. For the NCP model, an analogous objective function is used.
To solve the optimization problem in Eq. (2) we will use a Projected Stochastic Gradient Descent
method Algorithm 1. We also propose a variant that combines Adversarial Training with the projection
step (Algorithm 2) with the goal of increasing robustness to adversarial examples.
Algorithm 1: Projected SGD	Algorithm 2: Projected SGD + Adversarial Training
Input: dataset Z, learning rate {γt > 0}tT=-01, iterations T, hyper-parameters R, f , Loss L. Output: model with parameters θ. Initialize θ. for t = 0 to T - 1 do Sample (x, y) from Z θ = θ - γtOθL(θ; x, y). if t mod f = 0 then θ = Q{θ=kθk∞≤R}(θ)	Input: dataset Z, learning rate {γt > 0}tT=-01, iterations T and n, hyper-parameters R, f , and α, Loss L Output: model with parameters θ. Initialize θ. for t = 0 to T - 1 do Sample (x, y) from Z for i = 0 to n - 1 do XadV = Q{χ0：||x0-x∣∣∞≤e} {x + aVχL(θ; x, y)} θ = θ - YtOθL(θ; xadv,y) if t mod f = 0 then θ = Q{θd∣θk∞≤R}(θ)
In Algorithms 1 and 2 the parameter f is set in practice to a positive value, so that the projection
(denoted by Π) is made only every few iterations. The variable θ represents the weight matrices of
the model, and the projection in the last line should be understood as applied independently for every
weight matrix. The regularization parameter R corresponds to the variables μ, λ in Eq. (2).
Convolutional layers Frequently, convolutions are employed in the literature, especially in the
image-domain. It is important to understand how our previous results extend to this case, and
how the proposed algorithms work in that case. Below, we show that the '∞-operator norm of the
convolutional layer (as a linear operator) is related to the '∞-operator norm of the kernel after a
reshaping operation. For simplicity, we consider only convolutions with zero padding.
We study the cases of 1D, 2D and 3D convolutions. For clarity, we mention below the result for the
3D convolution, since this is relevant to our experimental validation, and we defer the other two cases
to Appendix G.
Theorem 5.	Let A ∈ Rn×m×r be an input image and let K ∈ Rh×h×r×o be a convolutional kernel
with o output channels. For simplicity assume that k ≤ min(n, m) is odd. Denote by B = K?A the
output of the convolutional layer. LetU ∈ Rnmo×nmr be the matrix such that vec(B) =U vec(A)
i.e., U is the matrix representation of the convolution. Let M(K) ∈ Ro×hhr be the matricization of
5
Published as a conference paper at ICLR 2022
K, where each row contains the parameters of a single output channel of the convolution. It holds
that: kUk∞ = kM(K)k∞.
Thus, We can control the '∞ -operator-norm of a convolutional layer during training by controlling
that of the reshaping of the kernel, which is done with the same code as for fully connected layers. It
can be seen that When the padding is non-zero, the result still holds.
4	Numerical Evidence
The generalization properties and the robustness of PNs are numerically verified in this section.
We evaluate the robustness to three Widely-used adversarial attacks in sec. 4.2. We assess Whether
the compared regularization schemes can also help in the case of adversarial training in sec. 4.3.
Experiments With additional datasets, models (NCP models), adversarial attacks (APGDT, PGDT)
and layer-Wise bound (instead of a single bound for all matrices) are conducted in Appendix H
due to the restricted space. The results exhibit a consistent behavior across different adversarial
attacks, different datasets and different models. Whenever the results differ, We explicitly mention the
differences in the main body beloW.
4.1	Experimental Setup
The accuracy is reported as as the evaluation metric for every experiment, Where a higher accuracy
translates to better performance.
Datasets and Benchmark Models: We conduct experiments on the popular datasets of Fashion-
MNIST (Xiao et al., 2017), E-MNIST (Cohen et al., 2017) and CIFAR-10 (Krizhevsky et al.,
2014). The first tWo datasets include grayscale images of resolution 28 × 28, While CIFAR-10
includes 60, 000 RGB images of resolution 32 × 32. Each image is annotated With one out of the
ten categories. We use tWo popular regularization methods from the literature for comparison, i.e.,
Jacobian regularization (Hoffman et al., 2019) and L2 regularization (Weight decay).
Models: We report results using the folloWing three models: 1) a 4th-degree CCP model named
"PN-4", 2) a 10th-degree CCP model referenced as "PN-10" and 3) a 4th-degree Convolutional CCP
model called "PN-Conv". In the PN-Conv, We have replaced all the Ui matrices With convolutional
kernels. None of the variants contains any activation functions.
Hyper-parameters: Unless mentioned otherWise, all models are trained for 100 epochs With a batch
size of 64. The initial value of the learning rate is 0.001. After the first 25 epochs, the learning rate is
multiplied by a factor of 0.2 every 50 epochs. The SGD is used to optimize all the models, While the
cross-entropy loss is used. In the experiments that include projection or adversarial training, the first
50 epochs are pre-training, i.e., training only With the cross-entropy loss. The projection is performed
every ten iterations.
Adversarial Attack Settings: We utilize tWo Widely used attacks: a) Fast Gradient Sign Method
(FGSM) and b) Projected Gradient Descent (PGD). In FGSM the hyper-parameter represents the
step size of the adversarial attack. In PGD there is a triple of parameters (total, niters, iter), Which
represent the maximum step size of the total adversarial attack, the number of steps to perform for a
single attack, and the step size of each adversarial attack step respectively. We consider the folloWing
hyper-parameters for the attacks: a) FGSM With = 0.1, b) PGD With parameters (0.1, 20, 0.01), c)
PGD With parameters (0.3, 20, 0.03).
4.2	Evaluation of the robustness of PNs
In the next experiment, We assess the robustness of PNs under adversarial noise. That is, the method
is trained on the train set of the respective dataset and the evaluation is performed on the test set
perturbed by additive adversarial noise. That is, each image is individually perturbed based on the
respective adversarial attack. The proposed method implements Algorithm 1.
The quantitative results in both Fashion-MNIST and E-MNIST using PN-4, PN-10 and PN-Conv
under the three attacks are reported in Table 1. The column ‘No-proj’ exhibits the plain SGD training
(i.e., Without regularization), While the remaining columns include the proposed regularization,
Jacobian and the L2 regularization respectively. The results Without regularization exhibit a substantial
decrease in accuracy for stronger adversarial attacks. The proposed regularization outperforms all
methods consistently across different adversarial attacks. Interestingly, the stronger the adversarial
6
Published as a conference paper at ICLR 2022
Method	No proj.	OUr method Fashion	Jacobian -MNIST	L2
Clean	87.28 ± 0.18%	87.32 ± 0.14%	86.24 ± 0.14%	87.31 ± 0.13%
PN4 FGSM-0.1	12.92 ± 2.74%	46.43 ± 0.95%	17.90 ± 6.51%	13.80 ± 3.65%
PN-4 PGD-(0.1, 20, 0.01)	5.64 ± 1.76%	49.58 ± 0.59%	12.23 ± 5.63%	5.01 ± 2.44%
PGD-(0.3, 20, 0.03)	0.18 ± 0.16%	28.96 ± 2.31%	1.27 ± 1.29%	0.28 ± 0.18%
Clean	88.48 ± 0.17%	88.72 ± 0.12%	88.12 ± 0.11%	88.46 ± 0.19%
PN10 FGSM-0.1	15.96 ± 1.00%	44.71 ± 1.24%	19.52 ± 1.14%	16.51 ± 2.33%
PN-10	PGD-(0.1, 20, 0.01)	1.94 ± 0.82%	47.94 ± 2.29%	5.44 ± 0.81%	2.16 ± 0.95%
PGD-(0.3, 20, 0.03)	0.02 ± 0.03%	30.51 ± 1.22%	0.05 ± 0.02%	0.01 ± 0.02%
Clean	86.36 ± 0.21%	86.38 ± 0.26%	84.69 ± 0.44%	86.45 ± 0.21%
PNC FGSM-0.1	10.80 ± 1.82%	48.15 ± 1.23%	10.62 ± 0.77%	10.73 ± 1.58%
PN-Conv PGD-(0.1, 20, 0.01)	9.37 ± 1.00%	46.63 ± 3.68%	10.20 ± 0.32%	8.96 ± 0.83%
PGD-(0.3, 20, 0.03)	1.75 ± 0.83%	28.94 ± 1.20%	8.26 ± 1.05%	2.03 ± 0.99%
E-MNIST				
Clean	84.27 ± 0.26%	84.34 ± 0.31%	81.99 ± 0.33%	84.22 ± 0.33%
PN4 FGSM-0.1	8.92 ± 1.99%	27.56 ± 3.32%	14.96 ± 1.32%	8.18 ± 3.48%
PN-4 PGD-(0.1, 20, 0.01)	6.24 ± 1.43%	29.46 ± 2.73%	6.75 ± 2.92%	5.93 ± 1.97%
PGD-(0.3, 20, 0.03)	1.22 ± 0.85%	19.07 ± 0.98%	3.06 ± 0.53%	1.00 ± 0.76%
Clean	89.31 ± 0.09%	90.56 ± 0.10%	89.19 ± 0.07%	89.23 ± 0.13%
PN10 FGSM-0.1	15.56 ± 1.16%	37.11 ± 2.81%	24.21 ± 1.89%	16.30 ± 1.82%
PN-10	PGD-(0.1, 20, 0.01)	2.63 ± 0.65%	37.89 ± 2.91%	9.18 ± 1.09%	2.33 ± 0.43%
PGD-(0.3, 20, 0.03)	0.00 ± 0.00%	20.47 ± 0.96%	0.11 ± 0.08%	0.02 ± 0.03%
Clean	91.49 ± 0.29%	91.57 ± 0.19%	90.38 ± 0.13%	91.41 ± 0.18%
PNC FGSM-0.1	4.28 ± 0.55%	35.39 ± 7.51%	3.88 ± 0.04%	4.13 ± 0.41%
PN-Conv PGD-(0.1, 20, 0.01)	3.98 ± 0.82%	33.75 ± 7.17%	3.86 ± 0.01%	4.83 ± 0.87%
PGD-(0.3, 20, 0.03)	3.24 ± 0.76%	28.10 ± 3.27%	3.84 ± 0.01%	2.76 ± 0.65%
Table 1: Comparison of regularization techniques on Fashion-MNIST (top) and E-MNIST (bottom).
In each dataset, the base networks are PN-4, i.e., a 4th degree polynomial, on the top four rows, PN-10,
i.e., a 10th degree polynomial, on the middle four rows and PN-Conv, i.e., a 4th degree polynomial
with convolutions, on the bottom four rows. Our projection method exhibits the best performance in
all three attacks, with the difference on accuracy to stronger attacks being substantial.
attack, the bigger the difference of the proposed regularization scheme with the alternatives of
Jacobian and L2 regularizations.
Next, we learn the networks with varying projection bounds. The results on Fashion-MNIST and
E-MNIST are visualized in Fig. 2, where the x-axis is plotted in log-scale. As a reference point, we
include the clean accuracy curves, i.e., when there is no adversarial noise. Projection bounds larger
than 2 (in the log-axis) leave the accuracy unchanged. As the bounds decrease, the results gradually
improve. This can be attributed to the constraints the projection bounds impose into the Ui matrices.
Similar observations can be made when evaluating the clean accuracy (i.e., no adversarial noise in
the test set). However, in the case of adversarial attacks a tighter bound performs better, i.e., the best
accuracy is exhibited in the region of 0 in the log-axis. The projection bounds can have a substantial
improvement on the performance, especially in the case of stronger adversarial attacks, i.e., PGD.
Notice that all in the aforementioned cases, the intermediate values of the projection bounds yield an
increased performance in terms of the test-accuracy and the adversarial perturbations.
Beyond the aforementioned datasets, we also validate the proposed method on CIFAR-10 dataset.
The results in Fig. 3 and Table 2 exhibit similar patterns as the aforementioned experiments. Although
the improvement is smaller than the case of Fashion-MNIST and E-MNIST, we can still obtain about
10% accuracy improvement under three different adversarial attacks.
4.3	Adversarial training (AT) on PNs
Adversarial training has been used as a strong defence against adversarial attacks. In this experiment
we evaluate whether different regularization methods can work in conjunction with adversarial training
that is widely used as a defence method. Since multi-step adversarial attacks are computationally
intensive, we utilize the FGSM attack during training, while we evaluate the trained model in all three
7
Published as a conference paper at ICLR 2022
-→- Clean --- FGSM 0.1	-*- PGD 0.1 0.01 20	PGD 0.3 0.03 20
(b) E-MNIST
Figure 2: Adversarial attacks during testing on (a) Fashion-MNIST (top), (b) E-MNIST (bottom)
with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds yield the
highest accuracy. The patterns are consistent in both datasets and across adversarial attacks.
Clean
FGSM-0.1
PGD 0.1 0.01 20	→- PGD 0.3 0.03 20
_ ___ ____ _______________ ___ ____
Figure 3: Adversarial attacks during testing on CIFAR-10.
8
Published as a conference paper at ICLR 2022
Model Projection	PN-Conv			
	No-proj	Our method	Jacobian	L2
Clean accuracy	65.09 ± 0.14%	65.22 ± 0.13%	64.43 ± 0.19%	65.11 ± 0.08%
FGSM-0.1	6.00 ± 0.53%	15.13 ± 0.81%	3.34 ± 0.40%	1.27 ± 0.10%
PGD-(0.1, 20, 0.01)	7.08 ± 0.68%	15.17 ± 0.88%	1.74 ± 0.14%	1.05 ± 0.05%
PGD-(0.3, 20, 0.03)	0.41 ± 0.09%	11.71 ± 1.11%	0.04 ± 0.02%	0.51 ± 0.04%
Table 2: Evaluation of the robustness of PN models on CIFAR-10. Each line refers to a different
adversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacks
the projection improves the accuracy by a significant margin.
adversarial attacks. For this experiment we select PN-10 as the base model. The proposed model
implements Algorithm 2.
The accuracy is reported in Table 3 with Fashion-MNIST on the top and E-MNIST on the bottom. In
the FGSM attack, the difference of the compared methods is smaller, which is expected since similar
attack is used for the training. However, for stronger attacks the difference becomes pronounced
with the proposed regularization method outperforming both the Jacobian and the L2 regularization
methods.
Method	AT	Our method + AT	Jacobian + AT	L2 + AT
	Adversarial training (AT) with PN-10 on Fashion-MNIST			
FGSM-0.1	65.33 ± 0.46%	65.64 ± 0.35%	62.04 ± 0.22%	65.62 ± 0.15%
PGD-(0.1, 20, 0.01)	57.45 ± 0.35%	59.89 ± 0.22%	57.42 ± 0.24%	57.40 ± 0.36%
PGD-(0.3, 20, 0.03)	24.46 ± 0.45%	39.79 ± 1.40%	25.59 ± 0.20%	24.99 ± 0.57%
Adversarial training (AT) With PN-10 on E-MNIST
FGSM-0.1	78.30 ± 0.18%	78.61 ± 0.58%	70.11 ± 0.18%	78.31 ± 0.32%
PGD-(0.1, 20, 0.01)	68.40 ± 0.32%	68.51 ± 0.19%	64.61 ± 0.16%	68.41 ± 0.37%
PGD-(0.3, 20, 0.03)	35.58 ± 0.33%	42.22 ± 0.60%	39.83 ± 0.24%	35.17 ± 0.46%
Table 3: Comparison of regularization techniques on (a) Fashion-MNIST (top) and (b) E-MNIST (bot-
tom) along With adversarial training (AT). The base netWork is a PN-10, i.e., 10th degree polynomial.
Our projection method exhibits the best performance in all three attacks.
The limitations of the proposed work are threefold. Firstly, Theorem 1 relies on the '∞ -operator
norm of the face-splitting product of the Weight matrices, Which in practice We relax in Lemma 1 for
performing the projection. In the future, we aim to study if it is feasible to compute the non-convex
projection onto the set of PNs with bounded '∞-norm of the face-splitting product of the weight
matrices. This would allow us to let go off the relaxation argument and directly optimize the original
tighter Rademacher Complexity bound (Theorem 1).
Secondly, the regularization effect of the projection differs across datasets and adversarial attacks, a
topic that is worth investigating in the future.
Thirdly, our bounds do not take into account the algorithm used, which corresponds to a variant of
the Stochastic Projected Gradient Descent, and hence any improved generalization properties due to
possible uniform stability (Bousquet and Elisseeff, 2002) of the algorithm or implicit regularization
properties (Neyshabur, 2017), do not play a role in our analysis.
5	Conclusion
In this work, we explore the generalization properties of the Coupled CP-decomposition (CCP) and
nested coupled CP-decomposition (NCP) models that belong in the class of Polynomial Nets (PNs).
We derive bounds for the Rademacher complexity and the Lipschitz constant of the CCP and the
NCP models. We utilize the computed bounds as a regularization during training. The regularization
terms have also a substantial effect on the robustness of the model, i.e., when adversarial noise is
added to the test set. A future direction of research is to obtain generalization bounds for this class of
functions using stability notions. Along with the recent empirical results on PNs, our derived bounds
can further explain the benefits and drawbacks of using PNs.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We are thankful to Igor Krawczuk and Andreas Loukas for their comments on the paper. We are
also thankful to the reviewers for providing constructive feedback. Research was sponsored by the
Army Research Office and was accomplished under Grant Number W911NF-19-1-0404. This work is
funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint venture between
EPFL and ETH Zurich. This project has received funding from the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation programme (grant agreement
number 725594 - time-data).
References
P. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks,
2017.
P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal ofMachine Learning Research, 3(Nov):463-482, 2002.
O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,
2:499-526, 2002.
Z. Brakerski, C. Gentry, and V. Vaikuntanathan. (leveled) fully homomorphic encryption without
bootstrapping. ACM Transactions on Computation Theory (TOCT), 6(3):1-36, 2014.
G. Chrysos, S. Moschoglou, G. Bouritsas, Y. Panagakis, J. Deng, and S. Zafeiriou. π-nets: Deep
polynomial neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR),
2020.
G. G. Chrysos and Y. Panagakis. NAPS: Non-adversarial polynomial synthesis. Pattern Recognit.
Lett., 140:318-324, 2020.
M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In International Conference on Machine Learning, pages
854-863. PMLR, 2017.
T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for
classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.
G. Cohen, S. Afshar, J. Tapson, and A. van Schaik. Emnist: an extension of mnist to handwritten
letters. arXiv preprint arXiv:1908.06571, 2017.
F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning,
2020.
Z. Cvetkovski. Holder's Inequality, Minkowski's Inequality and Their Variants, pages 95-105.
Springer Berlin Heidelberg, 2012. doi: 10.1007/978-3-642-23792-8_9.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projections onto the l 1-ball for
learning in high dimensions. In Proceedings of the 25th international conference on Machine
learning, pages 272-279, 2008.
J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Simonyan, and M. Norouzi. Neural audio
synthesis of musical notes with wavenet autoencoders, 2017.
M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas. Efficient and accurate estimation
of lipschitz constants for deep neural networks. In Advances in neural information processing
systems (NeurIPS), 2019.
H.	Federer. Geometric measure theory. Springer, 2014.
10
Published as a conference paper at ICLR 2022
R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing. Cryptonets:
Applying neural networks to encrypted data with high throughput and accuracy. In M. F. Balcan
and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine Learning Research, pages 201-210, New York,
New York, USA, 20-22 Jun 2016. PMLR.
N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
In COLT, 2018.
I.	J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations (ICLR), 2015.
E. Hesamifard, H. Takabi, M. Ghasemi, and R. N. Wright. Privacy-preserving machine learning as a
service. Proc. Priv. Enhancing Technol., 2018(3):123-142, 2018.
J.	Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization, 2019.
S.	M. Jayakumar, W. M. Czarnecki, J. Menick, J. Schwarz, J. Rae, S. Osindero, Y. W. Teh, T. Harley,
and R. Pascanu. Multiplicative interactions and where to find them. In International Conference
on Learning Representations (ICLR), 2020.
T.	Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial
networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard. The megaface benchmark: 1
million faces for recognition at scale. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4873-4882, 2016.
T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51(3):455-500,
2009.
A.	Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset. online: http://www. cs. toronto.
edu/kriz/cifar. html, 55, 2014.
F. Latorre, P. Rolland, and V. Cevher. Lipschitz constant estimation of neural networks via sparse
polynomial optimization. In International Conference on Learning Representations (ICLR), 2020.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu. Defense against adversarial attacks using
high-level representation guided denoiser. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018.
T. Lyche. Numerical Linear Algebra and Matrix Factorizations. Springer, Cham, 2020.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. In International Conference on Learning Representations, 2018.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. Adaptive
Computation and Machine Learning. MIT Press, Cambridge, MA, 2 edition, 2018. ISBN 978-0-
262-03940-6.
B.	Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017.
B.	Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In
Conference on Learning Theory, pages 1376-1401. PMLR, 2015.
B.	Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized
margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.
C.	R. Rao. Estimation of heteroscedastic variances in linear models. Journal of the American
Statistical Association, 65(329):161-172, 1970. doi: 10.1080/01621459.1970.10481070.
11
Published as a conference paper at ICLR 2022
K. Scaman and A. Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient
estimation. In Advances in neural information processing systems (NeurIPS), 2018.
S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014. ISBN 1107057132.
V. Slyusar. A family of face products of matrices and its properties. Cybernetics and Systems Analysis,
35(3):379-384,1999.
J. Su, W. Byeon, J. Kossaifi, F. Huang, J. Kautz, and A. Anandkumar. Convolutional tensor-train lstm
for spatio-temporal learning. Advances in neural information processing systems (NeurIPS), 2020.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR),
2014.
Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certification of perturba-
tion invariance for deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018.
X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 7794-7803, 2018.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He. Feature denoising for improving adver-
sarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2019.
Y. Yoshida and T. Miyato. Spectral Norm Regularization for Improving the Generalizability of Deep
Learning. arXiv e-prints, art. arXiv:1705.10941, May 2017.
H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui, and M. Jordan. Theoretically principled trade-off
between robustness and accuracy. In Proceedings of the 36th International Conference on Machine
Learning, 2019.
12
Published as a conference paper at ICLR 2022
A	Appendix introduction
The Appendix is organized as follows:
•	The related work is summarized in Appendix B.
•	In Appendix C the notation and the core Lemmas from the literature are described.
•	Further details on the Polynomial Nets are provided in Appendix D.
•	The proofs on the CCP and the NCP models are added in Appendix E and Appendix F
respectively.
•	The extensions of the theorems for convolutional layers and their proofs are detailed in
Appendix G.
•	Additional experiments are included in Appendix H.
B Related Work
Rademacher Complexity: Known bounds for the class of polynomials are a consequence of more
general result for kernel methods (Mohri et al., 2018, Theorem 6.12). Support Vector Machines
(SVMs) with a polynomial kernel of degree k effectively correspond to a general polynomial with the
same degree. In contrast, our bound is tailored to the parametric definition of the CCP and the NCP
models, which are a subset of the class of all polynomials. Hence, they are tighter than the general
kernel complexity bounds.
Bounds for the class of neural networks were stablished in (Bartlett et al., 2017; Neyshabur et al.,
2017), but they require a long and technical proof, and in particular it assumes an '2-bound on the
input, which is incompatible with image-based applications. This bound also depend on the product of
spectral norms of each layer. In contrast, our bounds are more similar in spirit to the path-norm-based
complexity bounds (Neyshabur et al., 2015), as they depend on interactions between neurons at
different layers. This interaction precisely corresponds to the face-splitting product between weight
matrices that appears in Theorem 1.
Lipschitz constant: A variety of methods have been proposed for estimating the Lipschitz constant
of neural networks. For example, Scaman and Virmaux (2018) (SVD), Fazlyab et al. (2019) (Semidef-
inite programming) and Latorre et al. (2020) (Polynomial Optimization) are expensive optimization
methods to compute tighter bounds on such constant. These methods are unusable in our case as they
would require a non-trivial adaptation to work with Polynomial Nets. In contrast we find an upper
bound that applies to such family of models, and it can be controlled with efficient '∞-operator-norm
projections. However, our bounds might not be the tightest. Developing tighter methods to bound
and control the Lipschitz constant for Polynomial Nets is a promising direction of future research.
C Background
Below, we develop a detailed notation in Appendix C.1, we include related definitions in Appendix C.2
and Lemmas required for our proofs in Appendix C.3. The goal of this section is to cover many of
the required information for following the proofs and the notation we follow in this work. Readers
familiar with the different matrix/vector products, e.g., Khatri-Rao or face-splitting product, and with
basic inequalities, e.g., Holder,s inequality, can skip to the next section.
C.1 Notation
Different matrix products and their associated symbols are referenced in Table 4, while matrix
operations on a matrix A are defined on Table 5. Every matrix product, e.g., Hadamard product,
can be used in two ways: a) A ◦ B, which translates to Hadamard product of matrices A and B, b)
◦iN=1Ai abbreviates the Hadamard products A1 ◦ A2 ◦ . . . AN.
`-------{z------}
N products
The symbol xij refers to jth element of vector xi .
13
Published as a conference paper at ICLR 2022
Table 4: Symbols for various matrix products. The precise definitions of the products are included in
Appendix C.2 for completion.
Symbol	Definition
◦ * • 0 ?	Hadamard (element-wise) product. Column-wise Khatri-Rao product. Face-splitting product. Kronecker product. Convolution.
Table 5: Operations and symbols on a matrix A.
Symbol	Definition
k Ak∞	'∞-operator-norm; corresponds to the maximum 'ι-norm of its rows.
Ai	ith row of A.
ai,j	(i, j)th element of A.
Ai,j	(i, j)th block of a block-matrix A
Ai	The i-th matrix in a set of matrices {Aι, ∙∙∙ , AN}.
C.2 Definitions
For thoroughness, we include the definitions of the core products that we use in this work. Specifically,
the definitions of the Hadamard product (Definition 3), Kronecker product (Definition 4), the Khatri-
Rao product (Definition 5), column-wise Khatri-Rao product (Definition 6) and the face-splitting
product (Definition 7) are included.
Definition 3 (Hadamard product). For two matrices A and B of the same dimension m × n, the
Hadamard product A ◦ B is a matrix of the same dimension as the operands, with elements given by
(a ◦ b)i,j = ai,j bi,j .
Definition 4 (Kronecker product). If A is an m × n matrix and B is a p × q matrix, then the
Kronecker product A 0 B is the Pm × qn block matrix, given asfollows:
a1,1B
a1,nB
A0B
am,1B
am,n B
Example: the Kronecker product of the matrices A ∈ R2×2 and B ∈ R2×2 is computed below:
a1,1	a1,2
a2,1	a2,2
'------7-----:
A
b1,1	b1,2
b2,1	b2,2
,	'----V-----'
B|
a1,1b1,1
a1,1b2,1
a2,1b1,1
a2,1b2,1
a1,1b1,2
a1,1b2,2
a2,1b1,2
a2,1b2,2
a1,2b1,1
a1,2b2,1
a2,2b1,1
a2,2b2,1
a1,2b1,2 -
a1,2b2,2
a2,2b1,2
a2,2b2,2
{z
A0B
Definition 5 (Khatri-Rao product). The Khatri-Rao product is defined as:
0
}
A * B = (Aij 0 Bij)i,j ,
in which the (i, j)-th block is the mipi × njqj sized Kronecker product of the corresponding blocks
of A and B, assuming the number of row and column partitions of both matrices is equal. The size
of the product is then ( i mipi) × ( i njqj).
Example: if A and B both are 2 × 2 partitioned matrices e.g.:
A1,1	A1,2
A2,1	A2,2
a1,1
a2,1
a3,1
A
α1,2	a1,3
a2,2	a2,3
a3,2	a3,3
14
Published as a conference paper at ICLR 2022
B
B1,1 B1,2
b2,1 B2,2
b1,1
b2,1
b3,1
b1,2	b1,3
b2,2	b2,3
b3,2	b3,3
then we obtain the following:
a1,1b1,1	a1,2b1,1
a2,1b1,1	a2,2b1,1
A1,1 X B1,1 J A1,2 X B1,2
A2,1 X B2,1	A2,2 X B2,2
A * B
a1,3b1,2	a1,3b1,3
a2,3b1,2	a2,3b1,3
a3,1b2,1	a3,2b2,1	a3,3b2,2	a3,3b2,3
a3,1b3,1	a3,2b3,1	a3,3b3,2	a3,3b3,3
Definition 6 (Column-wise Khatri-Rao product). A column-wise Kronecker product oftwo matrices
may also be called the Khatri-Rao product. This product assumes the partitions Ofthe matrices are
their columns. In this case m1 = m, p1 = p, n = q and for each j: nj = pj = 1. The resulting
product is a mp × n matrix of which each column is the Kronecker product of the corresponding
columns of A and B.
Example: the Column-wise Khatri-Rao product of the matrices A ∈ R2×3 and B ∈ R3×3 is
computed below:
a1,1	a1,2	a1,3 a2,1	a2,2	a2,3 { A	* |	b1,1	b1,2 b1,3 b2,1	b2,2	b2,3 b3,1	b3,2	b3,3 "{^^^^^^^^^ B	|	a1,1b1,1 a1,2b1,2 a1,3 b1,3 a1,1b2,1 a1,2b2,2 a1,3 b2,3 a1,1b3,1 a1,2b3,2 a1,3 b3,3 a2,1b1,1 a2,2b1,2 a2,3 b1,3 a2,1b2,1 a2,2b2,2 a2,3 b2,3 a2,1b3,1 a2,2b3,2 a2,3 b3,3 		
{z^^^
A*B
From here on, all * refer to Column-wise Khatri-Rao product.
Definition 7 (Face-splitting product). The alternative concept of the matrix product, which uses
row-wise splitting of matrices with a given quantity of rows. This matrix operation was named the
face-splitting product of matrices or the transposed Khatri-Rao product. This type of operation is
based on row-by-row Kronecker products of two matrices.
Example: the Face-splitting product of the matrices A ∈ R3×2 and B ∈ R3×2 is computed below:
a1,1	a1,2		b1,1 b1,2		a1,1b1,1	a1,2b1,1	a1,1b1,2	a1,2b1,2
a2,1	a3,2	•	b2,1	b2,2	=	a2,1b2,1	a2,2b2,1	a2,1b2,2	a2,2b2,2
a3,1	a3,2 I - _	}|	b3,1	b3,2	}|	a3,1b3,1	a3,2b3,1	a3,1b3,2	a3,2b3,2 _ - /
{z A			{	 B		{z A∙B
C.3 Well-known Lemmas
In this section, we provide the details on the Lemmas required for our proofs along with their proofs
or the corresponding citations where the Lemmas can be found as well.
Lemma 3. (Federer, 2014) Let g, h be two composable Lipschitz functions. Then g ◦ h is also
Lipschitz with Lip(g ◦ h) ≤ Lip(g)Lip(h). Here and only here ◦ represents function composition.
Lemma 4. (Federer, 2014) Let f : X ⊆ Rn → Rm be differentiable and Lipschitz continuous.
Let Jf (x) denote its total derivative (Jacobian) at x. Then Lipp (f) = supx∈X kJf (x)kp where
kJf (x)kp is the induced operator norm on Jf (x).
Lemma 5 (Holder,s inequality). (Cvetkovski, 2012) Let (S, Σ,μ) be a measure space and let
p,q ∈ [1, ∞] with 1 + ɪ = 1. Then, for all measurable real-valued functions f and g on S, it
holds that:
kfgk1 ≤ kfkp kgkq .
15
Published as a conference paper at ICLR 2022
Lemma 6 (Mixed Product Property 1). (Slyusar, 1999) The following holds:
(AIBI) ◦ (A2B2) = (A1 • Az)(BI * B2) .
Lemma 7 (Mixed Product Property 2). The following holds:
◦i=i(AiBi) = ∙i=ι(Ai) *i=ι (Bi).
Proof. We prove this lemma by induction on N .
Base case (N = 1): A1B1 = A1B1.
Inductive step: Assume that the induction hypothesis holds for a particular k, i.e., the case N = k
holds. That can be expressed as:
◦k=i(AiBi) = ∙k=ι(Ai) *k=ι (Bi).
(3)
Then we will prove that it holds for N = k + 1:
◦ik=+11 (AiBi)
= [◦ik=1(AiBi)] ◦ (Ak+1Bk+1)
= [•ik=1(Ai) *ik=1 (Bi)] ◦ (Ak+1Bk+1) use inductive hypothesis (Eq. (3))
= [•ik=1(Ai) • Ak+1][*ik=1(Bi) * Bk+1] Lemma 6 [Mixed product property 1]
=∙k+1(Ai) *k+ (Bi).
That is, the case N = k + 1 also holds true, establishing the inductive step.	□
Lemma 8 (Massart Lemma. Lemma 26.8 in Shalev-Shwartz and Ben-David (2014)). Let A
二{aι,…，aN} be a finite Set OfveCtOrs in Rm. Define a = N PN=I ai. Then：
R(A) ≤ max ∣∣α — a∣∣ —」Iog N .
a∈A	m
Definition 8 (Consistency of a matrix norm). A matrix nOrm is Called COnsistent On Cn,n, if
∣AB∣ ≤ ∣A∣ ∣B∣ .
hOlds fOr A, B ∈ Cn,n.
Lemma 9 (Consistency of the operator norm). (LyChe, 2020) The OperatOr nOrm is COnsistent if the
vector norm ∣∣∙∣α is defined fOrall m ∈ N and ∣∣∙∣β = ∣∣∙∣ɑ
PrOOf.
∣AB∣
max
Bx6=0
kABx∣ɑ
kxkɑ
kABxka kBX∣α
Bx=0 ∣Bxka kxka
≤ max
y6=0
∣Ay∣a
kyka
max
x6=0
∣Bxka
kxka
∣A∣ ∣B∣ .
□
Lemma 10. (Rao, 1970)
(AC) * (BD) = (A ③ B)(C * D).
D Details on polynomial networks
In this section, we provide further details on the two most prominent parametrizations proposed in
Chrysos et al. (2020). This re-parametrization creates equivalent models, but enables us to absorb
the bias terms into the input terms. Firstly, we provide the re-parametrization of the CCP model in
Appendix D.1 and then we create the re-parametrization of the NCP model in Appendix D.2.
16
Published as a conference paper at ICLR 2022
D. 1 Re-parametrization of CCP model
The Coupled CP-Decomposition (CCP) model of PNs (Chrysos et al., 2020) leverages a coupled CP
Tensor decomposition. A k-degree CCP model f(ζ) can be succinctly described by the following
recursive relations:
y1 = V1ζ,	yn = (Vnζ) ◦ yn-1 +yn-1,	f(ζ) = Qyk +β,	(4)
where Z ∈ Rδ is the input data with δ ∈ N, f(Z) ∈ Ro is the output of the model and Vn ∈ Rμ×δ,
Q ∈ Ro ×μ and β ∈ Ro are the learnable parameters, where μ ∈ N is the hidden rank. In order to
simplify the bias terms in the model, we will introduce a minor re-parametrization in Lemma 11 that
we will use to present our results in the subsequent sections.
Lemma 11. Letz = [ζ>,1]> ∈ Rd, xn = [yn>,1]> ∈ Rm, C = [Q,β] ∈ Ro×m, d = δ+ 1,m =
μ + 1. Define:
U1 = 0V>1	01 ∈ Rm×d,	Ui = 0V>i	11 ∈ Rm×d (i > 1).
where the boldface numbers 0 and 1 denote all-zeros and all-ones column vectors of appropriate
size, respectively. The CCP model in Eq. (4) can be rewritten as f(z) = C ◦ik=1 Uiz, which is the
one used in Eq. (CCP).
As a reminder before providing the proof, the core symbols for this proof are summarized in Table 6.
Table 6: Core symbols in the proof of Lemma 11.
Symbol	Dimensions	Definition
◦	-	Hadamard (element-wise) product.
Z	Rδ	Input of the polynomial expansion.
f(Z)	Ro	Output of the polynomial expansion.
k	N	Degree of polynomial expansion.
m	N	Hidden rank of the expansion.
Vn	Rμ×δ	Learnable matrices of the expansion.
Q	Ro×μ	Learnable matrix of the expansion.
β	Ro	Bias of the expansion.
z	Rd	Re-Parametrization of the input.
C	Ro×m	C = (Q, β).	
Proof. By definition, we have:
>	1 >	y1	V1 ζ V1	0 ζ	V1 0	>	1 >	U
x1	=	[y1 ,	1]	=	1	=	1	= 0>	1	1	=	0> 1 [ζ ,	1] =	U1z .
x	=	[y> ,	1]>	=	yn	=	(Vnζ) ◦ yn-1	+ yn-1	=	Vnζ + 1	◦	yn-1
=	(Vn	1]	[1] ◦	[yn1-1 ] =	[Vn 1]	[Z>, 1]> ◦	[y>-1, 1]>	=	Unz ◦	Xn-1 .
Hence, it holds that:
f(z) =Qyk+β= (Q, β) y1k =Cxk
= C Uk z ◦ xk-1
= C Ukz ◦ (Uk-1z) ◦ xk-2
C Ukz ◦ (Uk-Iz) ◦…。(Uz) ◦ Xi
C Ukz ◦ (Uk-Iz) ◦…。(Uz) ◦ (Uiz)
C ◦ik=1 (Uiz) .
17
Published as a conference paper at ICLR 2022
□
D.2 Reparametrization of the NCP model
The nested coupled CP decomposition (NCP) model of PNs (Chrysos et al., 2020) leverages a joint
hierarchical decomposition. A k-degree NCP model f(ζ) is expressed with the following recursive
relations:
y1 = (V1ζ) ◦ (b1), yn = (Vnζ) ◦ (Unyn-1 + bn), f(ζ) = Qyk + β .	(5)
where Z ∈ Rδ is the input data with δ ∈ N, f(Z) ∈ Ro is the output of the model and Vn ∈ Rμ×δ,
bn ∈ Rμ, Un ∈ Rμ×μ, Q ∈ Ro×μ and β ∈ Ro are the learnable parameters, where μ ∈ N
is the hidden rank. In order to simplify the bias terms in the model, we will introduce a minor
re-parametrization in Lemma 12 that we will use to present our results in the subsequent sections.
Lemma 12. Letz = [ζ>,1]> ∈ Rd, xn = [yn>,1]> ∈ Rm, C = [Q,β] ∈ Ro×m, d = δ+ 1,m =
μ + 1. Let:
s1 =	[b1>,1]> ∈Rm,	Si	=	0U>i	b1i	∈	Rm×m(i	> 1),	Ai	=	0V>i	01	∈ Rm×d
where the boldface numbers 0 and 1 denote all-zeros and all-ones column vectors of appropriate
size, respectively. The NCP model in Eq. (5) can be rewritten as
x1 = (A1z) ◦ (s1),	xn = (Anz) ◦ (Sn xn-1),	f(z) = Cxk .	(6)
In the aforementioned Eq. (6), we have written Sn even for n = 1, when s1 is technically a vector,
but this is done for convenience only and does not change the end result.
E Result of the CCP model
E.1 PROOF OF THEOREM 1： RADEMACHER COMPLEXITY BOUND OF CCP UNDER '∞ NORM
To facilitate the proof below, we include the related symbols in Table 7. Below, to avoid cluttering
the notation, we consider that the expectation is over σ and omit the brackets as well.
Table 7: Core symbols for proof of Theorem 1.
Symbol	Dimensions	Definition
◦	-	Hadamard (element-wise) product.
•	-	Face-splitting product.
*	-	Column-wise Khatri-Rao product.
z	Rd	Input of the polynomial expansion.
f(z)	R	Output of the polynomial expansion.
k	N	Degree of polynomial expansion.
m	N	Hidden rank of the expansion.
Ui	Rm×d	Learnable matrices.
c	R1×m	Learnable matrix.
μ	R	Ilcki ≤ μ.
λ	R	||・k=i(Ui)L ≤ λ∙
18
Published as a conference paper at ICLR 2022
Proof.
1n
RZ(FCCP) = E Jup n ∑Sσjf(Zj)
1n
E sup — Ε(σj<c,。忆I(Uizj)〉)
f ∈FCkCP	j=1
E sup 1 (G X[σj9k=ι(Uizj)])
f ∈FCkCP	j=1
≤ E sup 11lcIlI
n
X[σ/k=ι(Uizj)]
j=1
Lemma 5 [Holder,s inequality]
∞
E sup 1 l∣ckι
n
[σj∙ik=1(Ui)*ik=1(zj)]
j=1
Lemma 7 [Mixed product property]
E sup 1 l∣ckι
n
∙k=ι(Ui) X[σj*k=ι (zj)]
∞
∞
≤ E sup 11lcIlI
n
∑h *k=ι Izj)]
j=1
∙ik=1(Ui)∞
∞
≤ μλ E
n
[σj *ik=1
j=1
(zj)]
∞
(7)
Next, we compute the bound of E Pjn=1 [σj *ik=1 (zj)]	.
Let Zj = *k=ι(zj) ∈ Rdk. For each l ∈ [dk], let Vl = (Zj,..., Zn) ∈ Rn. Note that ∣∣v1∣∣2 ≤
√n maxj∙ ∣Zj ∣∣∞. Let V = {vι,..., Vdk}. Then, it is true that:
E
n
[σj*ik=1(zj)]
j=1
=E
∞
n
σjZj
j=1
= E max
l=1
∞
n
σj(Vl)j
j=1
nR(V) .
(8)
Using Lemma 8 [Massart Lemma] we have that:
R(V) ≤ 2max ∣Zj ∣∞ ,2log(dk)∕n.
19
(9)
Published as a conference paper at ICLR 2022
Then, it holds that:
1n
RZ(FCCP) = E Jup n∑Sσjf(Zj)
≤ μ^ E χ[σj *k=1 (Zj )]∣	Eq. (7)
=μλnR(V)	Eq. (8)
n	(10)
≤ 2μλ max 11 Zj k∞ q log(dk)/n	Eq. (9)
=2μλmax ∣∣*k=ι(zj)∣∣∞ q2log(dk)/n
≤ 2μλ(max ∣∣zj∣∣∞)kq2log(dk)/n
≤ 2μλp2k log (d)/n.
□
E.2 Proof of Lemma 1
Table 8: Core symbols in the proof of Lemma 1.
Symbol	Dimensions	Definition
区	-	Kronecker product.
•	-	Face-splitting product.
Z	Rd	Input of the polynomial expansion.
f(Z)	R	Output of the polynomial expansion.
k	N	Degree of polynomial expansion.
m	N	Hidden rank of the expansion.
Ui	Rm×d	Learnable matrices.
Uij	Rd	jth row of Ui.
λi	R	∣Uik∞ ≤ λi for i = 1, 2,..."~~
Proof.
IU(Ui)L = max∣∣[∙k=ι(Ui )]j∣∣ι
=ImaxI Ik=JUj] ∣∣ι
Definition of Face-splitting product
Multiplicativity of absolute value
□
E.3 RADEMACHER COMPLEXITY BOUND UNDER `2 NORM
Theorem 6.	Let Z = {Z1, . . . , Zn} ⊆ Rd and suppose that ∣Zj ∣∞ ≤ 1 for all j = 1, . . . , n. Let
FCCP := {f (Z) =〈c，OhUiZ) ： kck2 ≤ μ, ∣∣∙k=1Ui∣∣2 ≤ λ}.
20
Published as a conference paper at ICLR 2022
The Empirical Rademacher Complexity of CCPk (k-degree CCP polynomials) with respect to Z is
bounded as:
RZ(FCCP) ≤ μ=
To facilitate the proof below, we include the related symbols in Table 9. Below, to avoid cluttering
the notation, we consider that the expectation is over σ and omit the brackets as well.
Table 9: Core symbols for proof of Theorem 6.
Symbol	Dimensions	Definition
。	-	Hadamard (element-wise) product.
∙	-	Face-splitting product.
*	-	Column-wise Khatri-Rao product.
z	Rd	Input of the polynomial expansion.
f(z)	R	Output of the polynomial expansion.
k	N	Degree of polynomial expansion.
m	N	Hidden rank of the expansion.
Ui	Rm×d	Learnable matrices.
c	R1×m	Learnable matrix.
μ	R	kck2 ≤ μ.
λ	R	gk=ι(Ui)Il2 ≤ λ.
Proof.
1n
RZ(FCCP) = E JUp n ∑Sσjf(Zj)
1n
E sup nE(σj〈c,。3 (Uizj )〉)
f ∈FCkCP	j=1
E sup 1 1c, X[σ*k=ι(Uizj)])
f ∈FCkCP	j=1
≤ E sup 1 IlcIl2
n
X[σj ^k=ι(Uizj)]
j=1
Lemma 5 [Holder,s inequality]
2
E sup ɪ ∣∣c∣2
n
∑[σj ∙k=ι (Ui) *k=ι (zj)]
j=1
Lemma 7 [Mixed product property]
2
E sup ɪ ∣∣c∣2
n
∙k=ι(Ui) ∑[σj *k=ι (zj)]
≤ E sup 1 IlcIl2
n
[σj *ik=1
j=1
(zj)]	∙ik=1(Ui)2
2
2
21
Published as a conference paper at ICLR 2022
2
So:
n
E Xσ∙ *k=ι (Zj)]
j=1
n
X[σj *k=1 (Zj )]
j=1
n
X[σj *k=1 (Zj )]
j=1
Jensen’s inequality
n
E X[σsσj (*k=1 (Zs), *k=l(zj »]
s,j
un
utX[*ik=1(Zj)22]
j=1
unk
utX(YkZjk22)
j=1 i=1
≤ √n.
RZ(FCCP) ≤ E” ： kck2
n
X[σj*ik=1(Zj)]
j=1
W=I(Ui)II2
2
(11)
E
2
≤
∖
E
t
2
2
2
SUpf∈FkCp kck2 ∣∣∙k=1(Ui)Il2
√n
μλ
□
E.4 Lipschitz constant bound of the CCP model
We will first prove a more general result about the `p -Lipschitz constant of the CCP model.
Theorem 7.	The Lipschitz constant (with respect to the `p-norm) of the function defined in Eq. (CCP),
restricted to the set {Z ∈ Rd : kZkp ≤ 1} is bounded as:
k
Lipp(f) ≤ kkCkp Y kUikp .
i=1
Proof. Let g(x) = Cx and h(Z) = ◦ik=1(UiZ). Then it holds that f(Z) = g(h(Z)). By Lemma 3,
we have: Lipp(f) ≤ Lipp(g)Lipp(h). We will compute an upper bound of each function individually.
Let us first consider the function g(x) = Cx. By Lemma 4, because g is a linear map represented by
a matrix C, its Jacobian is Jg (x) = C. So:
Lipp(g) = kCkp := sUp kCxkp .
kxkp=1
where kC kp is the operator norm on matrices induced by the vector p-norm.
Now, let us consider the function h(Z) = ◦ik=1UiZ. Its Jacobian is given by:
dh k
dZ = Ediag9j=iUjz)Ui.
i=1
22
Published as a conference paper at ICLR 2022
Using Lemma 4 we have:
Lipp(h) ≤ sup
Z：kZkpS1
k
X[diag(oj=i(Uj z))Ui]
i=1
k
X kdiag(Oj=i(Ujz))Uikp
p
≤ sup
Z"∣zkp≤1
≤ sup
Z "∣zkp≤1
≤ sup
Z "∣zkp≤1
≤ sup
Z "∣zkp≤1
≤ sup
Z：kZkpS1
≤ sup
Z "∣zkp≤1
k
i=1
k
X kdiag(Oj=i (Ujz)) kp kUi Ilp
i=1
k
X >=i(Ujz)kp kUikp
i=1
k
XY(kUjzkp) kUikp
i=1 j 6=i
k
XY(kUjkp kzkp) kUikp
i=1 j 6=i
kk
X Y(kUjkp)
i=1 j=1
Triangle inequality
Lemma 9 [consistency]
k	kUjkp .
j=1
So:
Lipp(FL) ≤ Lipp(g)Lipp(h)
k
≤ kkCkp Y kUikp .
i=1
□
E.4. 1 Proof of Theorem 2
Proof. This is a particular case of Theorem 7 when p = ∞.
23
Published as a conference paper at ICLR 2022
F Res ult of the NCP model
F.1 PROOF OF THEOREM 3： RADEMACHER COMPLEXITY OF NCP UNDER '∞ NORM
Proof.
1n
Rz (FNcp) = E JUp n52σjf(Zj)
1n
=E sup n	(σj hc, Xk(Zj)i)
=E sup 1 (c, χ∣σjXk(Zj)]:
≤ E JUp	n llckι ∑S[σjXk(Zj)]
Lemma 5 [Holder,s inequality]
E sup I l∣ckι
n
[σj((AkZj) ◦ (SkXk-1(Zj)))]
E sup I l∣ckι
n
E[σj((Ak • Sk)(zj * Xk-I(Zj)))]
j=1
Lemma 7 [Mixed product property]
∞
E sup I l∣ckι
n
(k •Sk)	[σj(Zj * Xk-1(Zj))]
j=1
∞
(12)
Now, because of the recursive definition of the Eq. (NCP), we obtain：
nn
σj(Zj*Xk-1(Zj)) =	σj (Zj * (Ak-1Zj) ◦ (Sk-1Xk-2(Zj)))
j=1	j=1
n
=	σj (Zj * ((Ak-1 • Sk-1)(Zj * Xk-2(Zj))))	Lemma 7
j=1
n
=fGj (I ③(Ak-1 • Sk-I))(Zj * (Zj * Xk-2(Zj)))	Lemma 10
j=1
n
=I 0 (Ak-1 • Sk-ι) £Wj(Zj * (Zj * Xk-2(Zj))).
j=1
(13)
recursively applying this argument we have：
n	k-1	n
X	σj(Zj	*	Xk-1(Zj)) =	Y I0Ai •	Si	X	σj	*ik=1	(Zj)	.	(14)
j=1	i=1	j=1
Combining the two previous equations (Eqs. (13) and (14)) inside Eq. (12) we finally obtain
24
Published as a conference paper at ICLR 2022
1	k-1	n
RZ(FNCP) ≤ E Jup - kckι (Ak • Sk)(Y I 乳 Ai • Si) Xσj *k=ι (Zj)
≤ E sup - kckι
k-1
(Ak • Sk) (Y I ③ Ai • Si
∞
n
σj *ik=1 (Zj)
≤ μ-E X σj *k=ι (Zj)
j=1
=μλnR(V) = μλR(V).
n
Eq. (8) .
following the same arguments as in Eq. (10) it follows that:
RZ(FNCP) ≤ 2μλ『k log(d).
□
F.2 Proof of Lemma 2
Proof.
k-1
(Ak • Sk) Y I 乳 Ai • Si
i=1	∞
k-1
≤ kAk • Skk∞ Y kI 乳 Ai • Sik∞
i=1
Lemma 9 [consistent]
k
YkAi • Sik∞
i=1
k
≤ Y kAik∞kSik∞
i=1
Lemma 1 .
□
F.3 RADEMACHER COMPLEXITY UNDER `2 NORM
Theorem 8. Let Z = {Z1, . . . , Zn} ⊆ Rd and suppose that kZj k∞ ≤ -for all j = -, . . . ,n. Define
the matrix Φ(A1, S1, . . . , An, Sn) := (Ak •Sk) ik=-11 I0 Ai • Si. Consider the class offunCtions:
FN CP := {f (z) αsin (NCP) : ∣∣C∣∣2 ≤ μ, ∣∣Φ(A1, Si,..., Ak, Sk)Il2 ≤ λ},
where C ∈ R1×m (single output case). The Empirical Rademacher Complexity of NCPk (k-degree
NCP polynomials) with respect to Z is bounded as:
RZIFNcp) ≤ √√n.
25
Published as a conference paper at ICLR 2022
Proof.
1n
RZ (FNCP) = E JUp n∑Sσjf (Zj )
1n
=E sup n	(σj hc, Xk(Zj)i)
=E sup 1 (c, χ[σjXk(Zj)]:
≤ E JUp n kck2 '∑^σjxk(Zj)]
Lemma 5 [Holder,s inequality]
E sup n kck2 '∑^∖σj((AkZj) O(Skxk-I(Zj)))]
E sup ɪ ∣∣ck2
n
Ε[σj((Ak • Sk)(Zj * xk-1(Zj)))]
Lemma 7 [Mixed product property]
E sup ɪ ∣∣ck2
n
(Ak •Sk)	[σj(Zj * xk-1(Zj))]
j=1
2
E sup ɪ ∣∣ck2
k-1	n
Y I 0 Ai • Si X σj *k=ι (Zj)]
i=1	j=1
Eq. (14)
≤ E sup n ∣∣c∣∣2
(Ak • Sk) (YI 乳 Ai • Si)
i=1	2
n
σj *ik=1 (Zj )]
j=1
2
≤ μnλE X σj∙*k=ι (Zj)
j=1	2
≤
μλ
Eq. (11) .
□
F.4 Lipschitz constant bound of the NCP model
Theorem 9.	Let FL be the class of functions defined as
FL := x1 = (A1Z) ◦ (S1),xn = (AnZ) ◦ (Snxn-1), f(Z) = Cxk :
kCkp ≤ μ, kAikp ≤ λi, kSikp ≤ PiJzkp ≤ 1}.
The Lipschitz Constant of FL (k-degree NCP polynomial) under `p norm restrictions is bounded as:
k
LiPP(FL) ≤ kμ Y(λiPi).
i=1
Proof. Let g(x) = Cx, h(Z) = (AnZ) ◦ (Snxn-1(Z)). Then it holds that f(Z) = g(h(Z)).
26
Published as a conference paper at ICLR 2022
By Lemma 3, we have: Lip(f) ≤ Lip(g)Lip(h). This enables us to compute an upper bound of each
function (i.e., g, h) individually.
Let us first consider the function g(x) = Cx. By Lemma 4, because g is a linear map represented by
a matrix C, its Jacobian is Jg (x) = C. So:
Lipp(g) = kCkp := kxskupp=1kCxkp= (σmmaaxxi(PC)jC(i,j)	iiffpp==2∞.
where kCkp is the operator norm on matrices induced by the vector p-norm, and σmax(C) is the
largest singular value of C .
Now, let us consider the function xn(z) = h(z) = (Anz) ◦ (Snxn-1(z)). Its Jacobian is given by:
Jxn = diag(Anz)SnJxn-1 + diag(Snxn-1)An,	Jx1 = diag(S1)A1 .
Lipp(h) =	sup
z"∣zkp≤1
= sup
z"∣zkp≤1
≤ sup
z"∣zkp≤1
≤ sup
z"∣zkp≤1
≤ sup
z"∣zkp≤1
≤ sup
z"∣zkp≤1
= sup
z"∣zkp≤1
≤ sup
z"∣zkp≤1
≤ sup
z"∣zkp≤1
= sup
z"∣zkp≤1
Jxn kp
diag(Anz)SnJxn-1 + diag(Snxn-1)Ankp
diag(Anz)SnJxn-1 kp + k diag(Snxn-1)Ankp	Triangle inequality
diag(Anz)kpkSnkpkJxn-1kp + k diag(Snxn-1)kpkAnkp	Lemma 9 [consistent]
AnzkpkSnkpkJxn-1 kp + kSnxn-1 kp kAn kp
Ankpk
Ankpk
Ankpk
Ankpk
Ankpk
z
z
z
z
z
kp kSn kp kJx
n-1
kp + kSn kp kxn-1 kp kAn kp
kpkSnkpkJxn-1 kp + kSnkpk(An-1z) ◦ (Sn-1 xn-2)kp kAn kp
kpkSnkpkJxn-1 kp + kSnkpkAn-1zkpkSn-1xn-2 kpkAnkp
kpkSnkpkJxn-1 kp + kSnkpkAn-1kpkzkpkSn-1kpkxn-2kpkAnkp
kpkSnkp(kJxn-1 kp + kAn-1kpkSn-1kpkxn-2kp)
≤ sup
z"∣zkp≤1
AnkpkzkpkSnkp(kJxn-1
n-1
kp+ Y(kSikpkAikp)kzkpn-2).
i=1
Then we proof the result by induction.
Inductive hypothesis:
n
,*S ≤ @(国斜Aikp).
Case k = 1:
Lipp (h) = sup kJx1 kp
Z "∣zkp≤1
= k diag(S1)A1 kp
≤ k diag(S1)kpkA1kp
≤ kS1kpkA1kp .
27
Published as a conference paper at ICLR 2022
Case k = n:
Lipp(h) =	sup kJxn kp
Z"lzkp≤1
n-1
≤ sup kAnkpkzkpkSnkp(kJxn-1kp+Y(kSikpkAikp)kzkpn-2)
z"∣zkp≤1	=
n-1	n-1
≤ sup kAnkpkzkpkSnkp((n-1)Y(kSikpkAikp)+Y(kSikpkAikp)kzkpn-2)
"kzkp≤1	i=1	i=1
n
≤nY(kSikpkAikp).
i=1
So:
Lipp(FL) ≤ Lipp(g)Lipp(h)
k
≤ kkCkp Y(kSikpkAikp).
i=1
□
F.4.1 proof of Theorem 4
Proof. This is particular case of Theorem 9 with P = ∞.	□
G Relationship between a Convolutional layer and a Fully
Connected layer
In this section we discuss various cases of input/output types depending on the dimensionality of the
input tensor and the output tensor. We also provide the proof of Theorem 5.
Theorem 10.	Let A ∈ Rn. Let K ∈ Rh be a 1-D convolutional kernel. For simplicity, we assume
h is odd and h ≤ n. Let B ∈ Rn, B = K ? A be the output of the convolution. Let U be the
convolutional operator i.e., the linear operator (matrix) U ∈ Rn×n such that B = K ? A = UA.
It holds that kU k∞ = kKk1.
Theorem 11.	Let A ∈ Rn×m, and let K ∈ Rh×h be a 2-D convolutional kernel. For simplicity
assume h is odd number and h ≤ min(n, m). Let B ∈ Rn×m, B = K ? A be the output of the
convolution. Let U be the convolutional operator i.e., the linear operator (matrix) U ∈ Rnm×nm
such that vec(B) = U vec(A). It holds that kUk∞ = kvec(K)k1.
G.1 Proof of Theorem 10
Proof. From, B = K ? A = UA we can obtain the following:
∕ui,1	U1,2
U2,1	U2,2
..
..
..
un,1	un,2
u1,n
u2,n
.(A1,…，An)> = (K1,…,Kh)> ? (A1,…,An)>.
.
.
unn
n,n
We observe that:
Khh+1+j-i if ∣i-j∣≤ 怨;
[0	if |i-j| > h-1.
28
Published as a conference paper at ICLR 2022
Then, it holds that:
nh
ku k∞=ii=χ X ∖ui,j ∖≤ m=X XKj∣ = kκ kι.
= j=1	= j=1
□
G.2 Proof of Theorem 11
Proof. We partition U into n × n partition matrices of shape m × m. Then the (i, j)th partition
matrix U(i,j) describes the relationship between the Bi
Toeplitz matrix in the previous result.
and the Aj. So, U(i,j) is also similar to the
Mh+j∙ ∙
U(i,j) = [0 τ+j-i
Meanwhile, the matrix M satisfies:
ml 八=∕k(i, h+1 +l-s)
i(s,l)	0
if
if
|i-j| ≤ h-1;
|i-j| > h-1.
if ∖s - l∖ ≤
if ∖s-l∖ >
h-1
~7Γ ；
h-1
2
Then, we have the following:
n×m	n
n×m	n
kUk∞ = mi=a1x	∖ui,j∖ ≤ mi=a1x	kUi,jk
j=1	j=1
In addition, by h ≤ n, h ≤ m we have:
h
≤mi=na1xXiiKjii1
j=1
h
XKi1= kvec(K)k1.
i=1
∞
h
u h+1+M (h-1 %=XUK Ii
i=1
1.
(15)
Then, it holds that kUk∞ = Pih=1 iiKi ii1.
□
G.3 Proof of Theorem 5
Proof. We partition U into o × r partition matrices of shape nm × nm. Then the (i, j)th partition
of the matrix U(i,j) describes the relationship between the ith channel of B and the jth channel of
A. Then, the following holds: iU(i,j) i∞ = Pih=1 iKiij i1, where Kij means the two-dimensional
tensor obtained by the third dimension of K takes j and the fourth dimension of K takes i.
n×m×r
kUk∞=maxin=×1m×o X ∣∣u(i,j)∣∣
j=1
o-1 n×m
max max
l=0 i=1
r-1 n×m
s=0 j=1
∣∣u(i
+nml,j+nms)
r-1 n×m
o-1	n×m	∣
≤ max	(mi=a1x	∣u(i+nml,j+nms)
s=0
j=1
r-1
mol=-a01xXiiU(l+1,s+1)ii∞
s=0
r
ml=oa1x X iiU(l,s) ii∞
s=1
rh
ml=oa1x X X iiKlisii1
s=1 i=1
≤ moax
l=1
=l∣κ ii
29
Published as a conference paper at ICLR 2022
Similar to Eq.(15): for every nm rows, We choose k++1 th row. Then its 1-norm is equal to this nm
rows of the KK,s ∞-norm. So the equation holds.	□
H	Auxiliary numerical evidence
A number of additional experiments are conducted in this section. Unless explicitly mentioned
otherwise, the experimental setup remains similar to the one in the main paper. The following
experiments are conducted below:
1.	The difference between the theoretical and the algorithmic bound and their evolution during
training is studied in Appendix H.1.
2.	An ablation study on the hidden size is conducted in Appendix H.2.
3.	An ablation study is conducted on the effect of adversarial steps in Appendix H.3.
4.	We evaluate the effect of the proposed projection into the testset performance in Ap-
pendix H.4.
5.	We conduct experiments on four new datasets, i.e., MNIST, K-MNIST, E-MNIST-BY,
NSYNTH in Appendix H.5. These experiments are conducted in addition to the datasets
already presented in the main paper.
6.	In Appendix H.6 experiments on three additional adversarial attacks, i.e., FGSM-0.01,
APGDT and TPGD, are performed.
7.	We conduct an experiment using the NCP model in Appendix H.7.
8.	The layer-wise bound (instead of a single bound for all matrices) is explored in Ap-
pendix H.8.
9.	The comparison with adversarial defense methods is conducted in Appendix H.9.
H.1 Theoretical and algorithmic bound
As mentioned in Section 3, projecting the quantity θ = •ik=1Ui ∞ onto their level set corresponds
to a difficult non-convex problem. Given that we have an upper bound
θ = •ik=1Ui ∞ ≤ Πik=1 kUi k∞ =: γ .
we want to understand in practice how tight is this bound. In Fig. 4 we compute the ratio θ for PN-4.
In Fig. 5 the ratio is illustrated for randomly initialized matrices (i.e., untrained networks).
H.2 Ablation study on the hidden size
Initially, we explore the effect of the hidden rank of PN-4 and PN-10 on Fashion-MNIST. Fig. 6
exhibits the accuracy on both the training and the test-set for both models. We observe that PN-10
has a better accuracy on the training set, however the accuracy on the test set is the same in the two
models. We also note that increasing the hidden rank improves the accuracy on the training set, but
not on the test set.
H.3 Ablation study on the effect of adversarial steps
Our next experiment scrutinizes the effect of the number of adversarial steps on the robust accuracy.
We consider in all cases a projection bound of 1, which provides the best empirical results. We
vary the number of adversarial steps and report the accuracy in Fig. 7. The results exhibit a similar
performance both in terms of the dataset (i.e., Fashion-MNIST and K-MNIST) and in terms of
the network (PN-4 and PN-Conv). Notice that when the adversarial attack has more than 10 steps
the performance does not vary significantly from the performance at 10 steps, indicating that the
projection bound is effective for stronger adversarial attacks.
30
Published as a conference paper at ICLR 2022
PN-4 Fashion-MNIST
∙12∙10∙08∙06g∙02∙0°
1111111
sA Ol
-2	0	2	4	6	8	10	12
Log of bound for each matrix in regularization
(a)
PN-4 K-MNIST
1.200
1.175
1.150
1.125
O 1.100
⅛
cc 1.075
1.050
1.025
1.000
-2	0	2	4	6	8	10	12
Log of bound for each matrix in regularization
(c)
PN-4 Fashion-MNIST
Punog
10	20	30	40	50
Epochs
50000-
(b)
PN-4 K-MNIST
(d)
Figure 4: Visualization of the difference between the bound results on Fashion-MNIST (top row) and
on K-MNIST (bottom row). Specifically, in (a) and (C) We visualize the ratio Y =
different log bound values for PN-4. In (b), (d) the exact values of the two bounds are
Qk=IkUi k∞
k∙k=ιUi k∞
computed
for
over
the course of the unregularized training. Notice that there is a gap between the two bounds, however
importantly the two bounds are increasing at the same rate, while their ratio is close to 1.
PN-IO
6-5
1 1
A Ol
1.2
2	3	4	5	6	7	8
Log of hidden rank
(a)
Figure 5: Visualization of the ratio
hidden rank = 16
6 5 4 3∙2λ
A Ol
1.0
2	4	6	8	10	12	14
Depth
(b)
Qk=I kUik∞
k∙k=1Uik∞
in a randomly initialized network (i.e., using normal
distribution random matrices). Specifically, in (a) we visualize the ratio for different log hidden rank
values for PN-10. In (b) we visualize the ratio for different depth values for hidden rank = 16. Neither
of two plots contain any regularization.
31
Published as a conference paper at ICLR 2022
50
0
QQOQ
9 8 7 6
() AUeJn84
2	3	4	5	6
Log OfHidden size
Figure 6: Accuracy of PN-4 and PN-10 when the hidden rank varies (plotted in log-scale).
∙→…PN-4	PN-Conv
Ablation study on the effect of adversarial steps
PGD attack steps
(a) Fashion-MNIST
Figure 7: Ablation study on the effect of adversarial steps in Fashion-MNIST and K-MNIST. All
methods are run by considering a projection bound of 1.
Ablation study on the effect of adversarial steps
PGD attack steps
(b) K-MNIST
32
Published as a conference paper at ICLR 2022
H.4 Evaluation of the accuracy of PNs
In this experiment, we evaluate the accuracy of PNs. We consider three networks, i.e., PN-4, PN-10
and PN-Conv, and train them under varying projection bounds using Algorithm 1. Each model is
evaluated on the test set of (a) Fashion-MNIST and (b) E-MNIST.
The accuracy of each method is reported in Fig. 8, where the x-axis is plotted in log-scale (natural
logarithm). The accuracy is better for bounds larger than 2 (in the log-axis) when compared to tighter
bounds (i.e., values less than 0). Very tight bounds stifle the ability of the network to learn, which
explains the decreased accuracy. Interestingly, PN-4 reaches similar accuracy to PN-10 and PN-Conv
in Fashion-MNIST as the bound increases, while in E-MNIST it cannot reach the same performance
as the bound increases. The best bounds for all three models are observed in the intermediate values,
i.e., in the region of 1 in the log-axis for PN-4 and PN-10.
—PN-4	PN-IO	—÷- PN-Conv
Figure 8: Accuracy of PN-4, PN-10 and PN-Conv under varying projection bounds (x-axis in
log-scale) learned on (a) Fashion-MNIST, (b) E-MNIST. Notice that the performance increases for
intermediate values, while it deteriorates when the bound is very tight.
We scrutinize further the projection bounds by training the same models only with cross-entropy loss
(i.e., no bound regularization). In Table 10, we include the accuracy of the three networks with and
without projection. Note that projection consistently improves the accuracy, particularly in the case
of larger networks, i.e., PN-10.
Method	PN-4	PN-10 Fashion-MNIST	PN-Conv
No projection	87.28 ± 0.18%	88.48 ± 0.17%	86.36 ± 0.21%
Projection	87.32 ± 0.14%	88.72 ± 0.12%	86.38 ± 0.26%
E-MNIST			
No projection	84.27 ± 0.26%	89.31 ± 0.09%	91.49 ± 0.29%
Projection	84.34 ± 0.31%	90.56 ± 0.10%	91.57 ± 0.19%
Table 10: The accuracy of different PN models on Fashion-MNIST (top) and E-MNIST (bottom)
when trained only with SGD (first row) and when trained with projection (last row).
33
Published as a conference paper at ICLR 2022
H.5 Experimental results on additional datasets
To validate even further we experiment with additional datasets. We describe the datasets below
and then present the robust accuracy in each case. The experimental setup remains the same as in
Section 4.2 in the main paper. As a reminder, we are evaluating the robustness of the different models
under adversarial noise.
Dataset details: There are six datasets used in this work:
1.	Fashion-MNIST (Xiao et al., 2017) includes grayscale images of clothing. The training set
consists of 60, 000 examples, and the test set of 10, 000 examples. The resolution of each
image is 28 × 28, with each image belonging to one of the 10 classes.
2.	E-MNIST (Cohen et al., 2017) includes handwritten character and digit images with a
training set of 124, 800 examples, and a test set of 20, 800 examples. The resolution of each
image is 28 × 28. E-MNIST includes 26 classes. We also use the variant EMNIST-BY that
includes 62 classes with 697, 932 examples for training and 116, 323 examples for testing.
3.	K-MNIST (Clanuwat et al., 2018) depicts grayscale images of Hiragana characters with a
training set of 60, 000examples, and a test set of 10, 000 examples. The resolution of each
image is 28 × 28. K-MNIST has 10 classes.
4.	MNIST (Lecun et al., 1998) includes handwritten digits images. MNIST has a training set
of 60, 000 examples, and a test set of 10, 000 examples. The resolution of each image is
28 × 28.
5.	CIFAR-10 (Krizhevsky et al., 2014) depicts images of natural scenes. CIFAR-10 has a
training set of 50, 000 examples, and a test set of 10, 000 examples. The resolution of each
RGB image is 32 × 32.
6.	NSYNTH (Engel et al., 2017) is an audio dataset containing 305, 979 musical notes, each
with a unique pitch, timbre, and envelope.
We provide a visualization3 of indicative samples from MNIST, Fashion-MNIST, K-MNIST and
E-MNIST in Fig. 9.
We originally train PN-4, PN-10 and PN-Conv without projection bounds. The results are reported
in Table 11 (columns titled ‘No proj’) for MNIST and K-MNIST, Table 13 (columns titled ‘No
proj’) for E-MNIST-BY and Table 14 (columns titled ‘No proj’) for NSYNTH. Next, we consider
the performance under varying projection bounds; the accuracy in each case is depicted in Fig. 10
for K-MNIST, MNIST and E-MNIST-BY and Fig. 11 for NSYNTH. The figures (and the tables)
depict the same patterns that emerged in the two main experiments, i.e., the performance can be vastly
improved for intermediate values of the projection bound. Similarly, we validate the performance
when using adversarial training. The results in Table 12 demonstrate the benefits of using projection
bounds even in the case of adversarial training.
H.6 Experimental results of more types of attacks
To further verify the results of the main paper, we conduct experiments with three additional adver-
sarial attacks: a) FGSM with = 0.01, b) Projected Gradient Descent in Trades (TPGD) (Zhang et al.,
2019), c) Targeted Auto-Projected Gradient Descent (APGDT) (Croce and Hein, 2020). In TPGD
and APGDT, we use the default parameters for a one-step attack.
The quantitative results are reported in Table 15 for four datasets and the curves of Fashion-MNIST
and E-MNIST are visualized in Fig. 12 and the curves of K-MNIST and MNIST are visualized in
Fig. 13. The results in both cases remain similar to the attacks in the main paper, i.e., the proposed
projection improves the performance consistently across attacks, types of networks and adversarial
attacks.
H.7 Experimental results in NCP model
To complement, the results of the CCP model, we conduct an experiment using the NCP model. That
is, we use a 4th degree polynomial expansion, called NCP-4, for our experiment. We conduct an
3The samples were found in https://www.tensorflow.org/datasets/catalog.
34
Published as a conference paper at ICLR 2022
(a) MNIST
(b) Fashion-MNIST
(c) K-MNIST
(d) E-MNIST
Figure 9: Samples from the datasets used for the numerical evidence. Below each image, the class
name and the class number are denoted.
35
Published as a conference paper at ICLR 2022
-→- Clean --- FGSM 0.1	-*- PGD 0.1 0.01 20	PGD 0.3 0.03 20
(b) MNIST
(c) E-MNIST-BY
Figure 10:	Adversarial attacks during testing on (a) K-MNIST (top), (b) MNIST (middle), (c)
E-MNIST-BY (bottom) with the x-axis is plotted in log-scale. Note that intermediate values of
projection bounds yield the highest accuracy. The patterns are consistent in all datasets and across
adversarial attacks.
36
Published as a conference paper at ICLR 2022
-→- Clean --- FGSM 0.1	-*- PGD 0.1 0.01 20	PGD 0.3 0.03 20
Accuracy(%)
Log of Bound
Figure 11:	Adversarial attacks during testing on NSYNTH.
37
Published as a conference paper at ICLR 2022
-→- Clean -÷- FGSMO.Ol	-→- APGDT -*- TPGD
(a) Fashion-MNIST
(b) E-MNIST
Figure 12:	Three new adversarial attacks during testing on (a) Fashion-MNIST (top), (b) E-MNIST
(bottom ) with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds
yield the highest accuracy. The patterns are consistent in both datasets and across adversarial attacks.
38
Published as a conference paper at ICLR 2022
-→- Clean -÷- FGSMO.Ol	-→- APGDT -*- TPGD
(a) K-MNIST
(b) MNIST
Figure 13:	Three new adversarial attacks during testing on (a) K-MNIST (top), (b) MNIST (bottom)
with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds yield the
highest accuracy. The patterns are consistent in both datasets and across adversarial attacks.
39
Published as a conference paper at ICLR 2022
Method		No proj.	OUr method K-M.	Jacobian NIST	L2
	Clean	84.04 ± 0.30%	84.23 ± 0.30%	83.16 ± 0.31%	84.18 ± 0.44%
DNT Λ	FGSM-0.1	18.86 ± 2.61%	35.84 ± 1.67%	22.61 ± 1.30%	22.05 ± 2.76%
PN-4	PGD-(0.1, 20, 0.01) 11.20 ± 4.27%		40.26 ± 1.36%	16.00 ± 5.63%	10.93 ± 2.42%
	PGD-(0.3, 20, 0.03)	1.94 ± 1.11%	24.75 ± 1.32%	4.46 ± 2.59%	2.70 ± 1.26%
	Clean	87.90 ± 0.24%	88.80 ± 0.19%	88.73 ± 0.16%	87.93 ± 0.18%
DNT 1 ∏	FGSM-0.1	24.52 ± 1.44%	41.83 ± 2.00%	26.90 ± 1.02%	26.62 ± 1.59%
PN-10	PGD-(0.1, 20, 0.01)	7.54 ± 0.79%	39.55 ± 0.64%	11.50 ± 1.35%	5.09 ± 0.68%
	PGD-(0.3, 20, 0.03)	0.05 ± 0.04%	25.24 ± 0.93%	1.24 ± 0.64%	0.19 ± 0.12%
	Clean	88.41 ± 0.37%	88.48 ± 0.42%	86.57 ± 0.46%	88.56 ± 0.62%
DNT f~1∕ΛΠX7	FGSM-0.1	13.34 ± 2.01%	47.75 ± 2.03%	14.16 ± 3.05%	12.43 ± 2.58%
PN-Conv	PGD-(0.1, 20, 0.01) 10.81 ± 1.25%		45.68 ± 3.11%	12.05 ± 0.82%	11.05 ± 0.85%
	PGD-(0.3, 20, 0.03)	6.91 ± 2.04%	31.68 ± 1.43%	7.54 ± 1.39%	6.28 ± 2.37%
MNIST					
	Clean	96.52 ± 0.13%	96.62 ± 0.17%	95.88 ± 0.16%	96.44 ± 0.18%
DNT Λ	FGSM-0.1	20.96 ± 5.16%	64.09 ± 2.41%	33.59 ± 8.46%	26.07 ± 5.64%
PN-4	PGD-(0.1, 20, 0.01) 14.23 ± 5.39%		66.05 ± 7.06%	20.83 ± 5.64%	16.06 ± 5.84%
	PGD-(0.3, 20, 0.03)	2.59 ± 2.01%	51.47 ± 3.17%	4.92 ± 1.18%	4.26 ± 2.44%
	Clean	97.46 ± 0.11%	97.63 ± 0.06%	97.36 ± 0.05%	97.53 ± 0.10%
Γ)XT 1 Λ	FGSM-0.1	30.12 ± 4.58%	70.02 ± 1.28%	40.22 ± 2.31%	28.77 ± 2.41%
PN-10	PGD-(0.1, 20, 0.01)	9.70 ± 2.11%	73.57 ± 1.17%	18.74 ± 5.39%	10.91 ± 2.32%
	PGD-(0.3, 20, 0.03)	0.47 ± 0.53%	55.36 ± 2.32%	2.49 ± 1.46%	0.44 ± 0.29%
	Clean	98.32 ± 0.12%	98.40 ± 0.12%	97.88 ± 0.12%	98.32 ± 0.11%
Γ)XT Γ'f^τyχ7	FGSM-0.1	18.98 ± 2.99%	67.50 ± 6.22%	27.02 ± 9.88%	23.77 ± 5.58%
PN-Conv	PGD-(0.1, 20, 0.01) 12.57 ± 2.81%		72.85 ± 12.23%	13.96 ± 2.57%	13.84 ± 3.18%
	PGD-(0.3, 20, 0.03) 10.57 ± 4.08%		55.56 ± 8.48%	10.22 ± 0.52%	9.10 ± 3.62%
Table 11: Comparison of regularization techniques on K-MNIST (top) and MNIST (bottom). In each
dataset, the base networks are PN-4, i.e., a 4th degree polynomial, on the top four rows, PN-10, i.e., a
10th degree polynomial, on the middle four rows and PN-Conv, i.e., a 4th degree polynomial with
convolutions, on the bottom four rows. Our projection method exhibits the best performance in all
three attacks, with the difference on accuracy to stronger attacks being substantial.
	AT	Our method + AT	Jacobian + AT	L2 + AT
Method	Adversarial training (AT) with PN-10 on K-MNIST			
FGSM-0.1	70.93 ± 0.46%	71.14 ± 0.30%	64.48 ± 0.51%	70.90 ± 0.57%
PGD-(0.1, 20, 0.01)	60.94 ± 0.71%	61.20 ± 0.39%	57.89 ± 0.31%	61.47 ± 0.44%
PGD-(0.3, 20, 0.03)	30.77 ± 0.26%	33.07 ± 0.58%	29.96 ± 0.21%	30.35 ± 0.42%
Adversarial training (AT) with PN-10on MNIST
FGSM-0.1	91.89 ± 0.30%	91.94 ± 0.17%	87.85 ± 0.27%	92.22 ± 0.30%
PGD-(0.1, 20, 0.01)	87.36 ± 0.29%	87.38 ± 0.37%	84.96 ± 0.25%	87.26 ± 0.49%
PGD-(0.3, 20, 0.03)	61.96 ± 0.92%	63.96 ± 1.02%	62.24 ± 0.24%	62.44 ± 0.76%
Table 12: Comparison of regularization techniques on (a) K-MNIST (top) and (b) MNIST (bottom)
along with adversarial training (AT). The base network is a PN-10, i.e., 10th degree polynomial. Our
projection method exhibits the best performance in all three attacks.
experiment in the K-MNIST dataset and present the result with varying bound in Fig. 14. Notice that
the patterns remain similar to the CCP model, i.e., intermediate values of the projection bound can
increase the performance significantly.
H.8 Layer-wise bound
To assess the flexibility of the proposed method, we assess the performance of the layer-wise bound. In
the previous sections, we have considered using a single bound for all the matrices, i.e., kUik∞ ≤ λ,
because the projection for a single matrix has efficient projection algorithms. However, Lemma 1
enables each matrix Ui to have a different bound λi . We assess the performance of having different
bounds for each matrix Ui .
40
Published as a conference paper at ICLR 2022
Method		PN-4, PN-10 and PN-Conv on E-MNIST-BY	
		No proj.	Our method
	Clean	-80.18 ± 0.19%	80.26 ± 0.17%
PN-4	FGSM-0.1	3.65 ± 0.76%	16.58 ± 3.87%
	PGD-(0.1, 20, 0.01)	4.57 ± 1.98%	19.77 ± 4.42%
	PGD-(0.3, 20, 0.03)	0.59 ± 0.40%	10.13 ± 2.08%
	Clean	-84.17 ± 0.06%	85.32 ± 0.04%
PN-10	FGSM-0.1	11.67 ± 1.21%	32.37 ± 2.58%
	PGD-(0.1, 20, 0.01)	2.48 ± 0.66%	31.22 ± 2.32%
	PGD-(0.3, 20, 0.03)	0.03 ± 0.05%	13.74 ± 0.77%
	Clean	-85.92 ± 0.08%	86.03 ± 0.08%
PN-Conv	FGSM-0.1	0.65 ± 0.17%	29.07 ± 2.72%
	PGD-(0.1, 20, 0.01)	1.57 ± 1.40%	31.06 ± 4.70%
	PGD-(0.3, 20, 0.03)	0.33 ± 0.06%	23.93 ± 6.32%
Table 13: Comparison of regularization techniques on E-MNIST-BY. The base network are PN-4,
i.e., 4th degree polynomial, on the top four rows, PN-10, i.e., 10th degree polynomial, on the middle
four rows and PN-Conv, i.e., a 4th degree polynomial with convolution, on the bottom four rows. Our
projection method exhibits the best performance in all three attacks, with the difference on accuracy
to stronger attacks being substantial.
Model Projection	PN-4	
	No-proj	Proj
Clean accuracy	80.25 ± 0.27%	80.33 ± 0.26%
FGSM-0.1	0.91 ± 0.14%	22.25 ± 0.04%
PGD-(0.1, 20, 0.01)	0.31 ± 0.11%	22.27 ± 0.00%
PGD-(0.3, 20, 0.03)	0.46 ± 0.29%	20.38 ± 2.30%
Table 14: Evaluation of the robustness of PN models on NSYNTH. Each line refers to a different
adversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacks
projection improves the accuracy by a remarkable margin.
41
Published as a conference paper at ICLR 2022
-→- Clean --- FGSM 0.1	-*- PGD 0.1 0.01 20	PGD 0.3 0.03 20
Accuracy(%)
Log of Bound
Figure 14:	Experimental result of K-MNIST in NCP model.
42
Published as a conference paper at ICLR 2022
	Method	No proj. Fashi	Our method on-MNIST
	FGSM-0.01	26.49 ± 3.13%	58.09 ± 1.63%
PN-4	APGDT	16.59 ± 4.35%	50.83 ± 1.55%
	TPGD	26.88 ± 6.78%	59.03 ± 1.45%
	FGSM-0.01	18.59 ± 1.82%	60.56 ± 1.06%
PN-10	APGDT	8.76 ± 1.14%	51.93 ± 1.91%
	TPGD	14.53 ± 1.49%	63.33 ± 0.51%
	FGSM-0.01	15.30 ± 3.10%	55.90 ± 2.60%
PN-Conv APGDT		11.88 ± 1.33%	53.49 ± 0.72%
	TPGD	14.50 ± 1.59%	58.72 ± 1.87%
E-MNIST			
	FGSM-0.01	13.40 ± 5.16%	32.83 ± 2.08%
PN-4	APGDT	9.33 ± 4.00%	26.38 ± 2.70%
	TPGD	17.40 ± 3.11%	34.68 ± 1.92%
	FGSM-0.01	14.47 ± 1.80%	48.28 ± 3.06%
PN-10	APGDT	10.13 ± 0.93%	41.72 ± 4.05%
	TPGD	13.97 ± 0.88%	47.44 ± 3.62%
	FGSM-0.01	4.71 ± 1.10%	39.37 ± 5.43%
PN-Conv APGDT		3.58 ± 0.66%	30.43 ± 4.87%
	TPGD	4.08 ± 0.33%	35.85 ± 10.20%
K-MNIST			
	FGSM-0.01	23.31 ± 5.34%	43.74 ± 5.97%
PN-4	APGDT	17.02 ± 6.97%	39.43 ± 1.89%
	TPGD	23.45 ± 7.67%	48.46 ± 3.84%
	FGSM-0.01	26.87 ± 2.14%	50.99 ± 3.52%
PN-10	APGDT	16.23 ± 1.32%	41.46 ± 3.85%
	TPGD	22.63 ± 0.99%	49.91 ± 1.37%
	FGSM-0.01	12.31 ± 2.03%	52.58 ± 6.80%
PN-Conv APGDT		13.47 ± 2.19%	42.94 ± 1.68%
	TPGD	14.25 ± 2.51%	48.19 ± 3.02%
MNIST			
	FGSM-0.01	34.14 ± 7.63%	73.95 ± 5.18%
PN-4	APGDT	29.88 ± 9.47%	71.26 ± 4.88%
	TPGD	27.01 ± 9.77%	76.88 ± 1.98%
	FGSM-0.01	32.34 ± 4.67%	78.83 ± 1.63%
PN-10	APGDT	19.55 ± 1.72%	75.22 ± 2.05%
	TPGD	28.11 ± 3.87%	79.74 ± 2.07%
	FGSM-0.01	22.73 ± 3.10%	69.83 ± 8.91%
PN-Conv APGDT		17.95 ± 3.39%	64.94 ± 8.96%
	TPGD	21.82 ± 3.07%	66.47 ± 11.83%
Table 15: Evaluation of the robustness of PN models on four datasets with three new types of attacks.
Each line refers to a different adversarial attack. The projection offers an improvement in the accuracy
in each case.
We experiment on PN-4 that we set a different projection bound for each matrix Ui . Specifically,
we use five different candidate values for each λi and then perform the grid search on the Fashion-
MNIST FGSM-0.01 attack. The results on Fashion-MNIST in Table 16 exhibit how the layer-wise
bounds outperform the previously used single bound4. The best performing values for PN-4 are
λι = 1.5, λ2 = 2,λ3 = 1.5, λ4 = 2,μ = 0.8. The values of λ% in the first few layers are larger,
while the value in the output matrix C is tighter.
To scrutinize the results even further, we evaluate whether the best performing λi can improve the
performance in different datasets and the FGSM-0.1 attack. In both cases, the best performing λi can
improve the performance of the single bound.
4The single bound is mentioned as ‘Our method’ in the previous tables. In this experiment both ‘single
bound’ and ‘layer-wise bound’ are proposed.
43
Published as a conference paper at ICLR 2022
				
Method	No proj.	Jacobian	L2 Fashion-MNIST	Single bound	Layer-wise bound
FGSM-0.01 FGSM-0.1	26.49 ± 3.13% 39.88 ± 4.59% 12.92 ± 2.74% 17.90 ± 6.51%	24.36 ± 1.95% 13.80 ± 3.65%	58.09 ± 1.63% 46.43 ± 0.95%	63.95 ± 1.26% 55.14 ± 3.65%
K-MNIST				
FGSM-0.01 FGSM-0.1	23.31 ± 5.34% 25.46 ± 3.51% 18.86 ± 2.61% 22.61 ± 1.30%	27.85 ± 7.62% 22.05 ± 2.76%	43.74 ± 5.97% 35.84 ± 1.67%	49.61 ± 1.44% 47.54 ± 3.74%
MNIST				
FGSM-0.01 FGSM-0.1	34.14 ± 7.63% 32.78 ± 6.94% 20.96 ± 5.16% 33.59 ± 8.46%	29.31 ± 3.95% 26.07 ± 5.64%	73.95 ± 5.18% 64.09 ± 2.41%	79.23 ± 3.65% 74.97 ± 5.60%
Table 16: Evaluation of our layer-wise bound versus our single bound. To avoid confusion with
previous results, note that ’single bound’ corresponds to ’Our method’ in the rest of the tables in this
work. The different λi values are optimized on Fashion-MNIST FGSM-0.01 attack. Then, the same
λi values are used for training the rest of the methods. The proposed layer-wise bound outperforms
the single bound by a large margin, improving even further by baseline regularization schemes.
Method
Noproj. Single bound Layer-wise bound GaUSSiandenoiSing Median denoising Guided denoising
Fashion-MNIST
FGSM-0.01 26.49 ± 3.13% 58.09 ± 1.63% 63.95 ± 1.26%	18.80 ± 3.08%	19.68 ± 3.20%	29.69 ± 5.37%
FGSM-0.1 12.92 ± 2.74% 46.43 ± 0.95% 55.14 ± 3.65%	14.14 ± 2.77%	14.02 ± 1.95%	22.94 ± 5.65%
Table 17: Comparison of the proposed method against adversarial defense methods on feature
denoising (Xie et al., 2019) and guided denoising (Liao et al., 2018). Notice that the single bound (cf.
Appendix H.8 for details) already outperforms the proposed defense methods, while the layer-wise
bounds further improves upon our single bound case.
H.9 Adversarial defense method
One frequent method used against adversarial perturbations are the so called adversarial defense
methods. We assess the performance of adversarial defense methods on the PNs when compared with
the proposed method.
We experiment on PN-4 in Fashion-MNIST. We chose three different methods: gaussian denoising,
median denoising and guided denoising (Liao et al., 2018). Gaussian denoising and median denoising
are the methods of using gaussian filter and median filter for feature denoising (Xie et al., 2019).
The results in Table 17 show that in both attacks our method performs favourably to the adversarial
defense methods.
44