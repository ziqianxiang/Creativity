Published as a conference paper at ICLR 2022
Do We Need Anisotropic Graph Neural
Networks ?
Shyam A. Tailor** 1 Felix L. Opolka 1，2	Pietro Lio1 Nicholas D. Lane 1，3
1	Department of Computer Science and Technology, University of Cambridge
2	Invenia Labs, Cambridge, UK
3	Samsung AI Center, Cambridge, UK
Ab stract
Common wisdom in the graph neural network (GNN) community dictates that
anisotropic models—in which messages sent between nodes are a function of
both the source and target node—are required to achieve state-of-the-art perfor-
mance. Benchmarks to date have demonstrated that these models perform better
than comparable isotropic models—where messages are a function of the source
node only. In this work we provide empirical evidence challenging this narra-
tive: we propose an isotropic GNN, which we call Efficient Graph Convolution
(EGC), that consistently outperforms comparable anisotropic models, including
the popular GAT or PNA architectures by using spatially-varying adaptive filters.
In addition to raising important questions for the GNN community, our work has
significant real-world implications for efficiency. EGC achieves higher model ac-
curacy, with lower memory consumption and latency, along with characteristics
suited to accelerator implementation, while being a drop-in replacement for exist-
ing architectures. As an isotropic model, it requires memory proportional to the
number of vertices in the graph (O(V )); in contrast, anisotropic models require
memory proportional to the number of edges (O(E)). We demonstrate that EGC
outperforms existing approaches across 6 large and diverse benchmark datasets,
and conclude by discussing questions that our work raise for the community go-
ing forward. Code and pretrained models for our experiments are provided at
https://github.com/shyam196/egc.
1	Introduction
Graph Neural Networks (GNNs) have emerged as an effective way to build models over arbitrarily
structured data. For example, they have successfully been applied to computer vision tasks: GNNs
can deliver high performance on point cloud data (Qi et al., 2017) and for feature matching across
images (Sarlin et al., 2020). Recent work has also shown that they can be applied to physical
simulations (Pfaff et al., 2020; Sanchez-Gonzalez et al., 2020). Code analysis is another application
domain where GNNs have found success (Guo et al., 2020; Allamanis et al., 2017).
In recent years, the research community has devoted significant attention to building more expres-
sive, and better performing, models to process graphs. Efforts to benchmark GNN models, such as
Open Graph Benchmark (Hu et al., 2020), or the work by Dwivedi et al. (2020), have attempted to
more rigorously quantify the relative performance of different proposed architectures. One common
conclusion—explicitly stated by Dwivedi et al. (2020)—is that anisotropic1 models, in which mes-
sages sent between nodes are a function of both the source and target node, are the best performing
models. By comparison, isotropic models, where messages are a function of the source node only,
achieve lower accuracy, even if they have efficiency benefits over comparable anisotropic models.
Intuitively, this conclusion is satisfying: anisotropic models are inherently more expressive, hence
we would expect them to perform better in most situations. Our work provides a surprising chal-
lenge to this wisdom by providing an isotropic model, called Efficient Graph Convolution (EGC),
* Corresponding author. Contact at sat62@cam.ac.uk
1Definition from Dwivedi et al. (2020). Equal to attentional & message passing from Bronstein et al. (2021).
1
Published as a conference paper at ICLR 2022
Messages are functions of source and target
nodes and hence must be materialized
Isotropic
Messages are functions of source only; propagation can
be implemented using matrix multiplication-style approaches
Figure 1: Many GNN architectures (e.g. GAT (VelickoVic et al., 2018), PNA (Corso et al., 2020))
incorporate sophisticated message functions to improve accuracy (left). This is problematic as we
must materialize messages, leading to O(E) memory consumption and OPs to calculate messages;
these dataflow patterns are also difficult to optimize for at the hardware leVel. This work demon-
strates that we can use simple message functions, requiring only O(V ) memory consumption
(right) and improve performance over existing GNNs.
that outperforms comparable anisotropic approaches, including the popular GAT (VelickoVic et al.,
2018) and PNA (Corso et al., 2020) architectures.
In addition to proViding a surprising empirical result for the community, our work has significant
practical implications for efficiency, as shown in Figure 1. As EGC is an isotropic model achieVing
high accuracy, we can take adVantage of the efficiency benefits offered by isotropic models without
haVing to compromise on model accuracy. We haVe seen memory consumption and latency for
state-of-the-art GNN architectures increase to O(E) in recent years, due to state-of-the-art models
incorporating anisotropic mechanisms to boost accuracy. EGC reduces the complexity to O(V ),
deliVering substantial real-world benefits, albeit with the precise benefit being dependent on the
topology of the graphs the model is applied to. The reader should note that our approach can also
be combined with other approaches for improVing the efficiency of GNNs. For example, common
hardware-software co-design techniques include quantization and pruning (Sze et al., 2020) could
be combined with this work, which proposes an orthogonal approach of improVing model efficiency
by improVing the underlying architecture design. We also note that our approach can be combined
with graph sampling techniques (Zeng et al., 2019; Hamilton et al., 2017; Chen et al., 2018a) to
improVe scalability further when training on graphs with millions, or billions, of nodes.
Contributions (1) We propose a new GNN architecture, Efficient Graph ConVolution (EGC),
and proVide both spatial and spectral interpretations for it. (2) We proVide a rigorous eValuation of
our architecture across 6 large graph datasets coVering both transductiVe and inductiVe use-cases,
and demonstrate that EGC consistently achieVes better results than strong baselines. (3) We pro-
Vide seVeral ablation studies to motiVate the selection of the hyperparameters in our model. (4) We
demonstrate that our model simultaneously achieVes better parameter efficiency, latency and mem-
ory consumption than competing approaches. Code and pre-trained models for our experiments
(including baselines) can be found at https://github.com/shyam196/egc. At time of
publication, EGC has also been upstreamed to PyTorch Geometric (Fey & Lenssen, 2019).
2	Background
2.1	Hardware - S oftware Co-Design for Deep Learning
SeVeral of the popular approaches for co-design haVe already been described in the introduction:
quantization, pruning, and careful architecture design are all common for CNNs and Transform-
ers (Vaswani et al., 2017). In addition to enabling better performance to be obtained from general
purpose processors such as CPUs and GPUs, these techniques are also essential for maximizing the
return from specialized accelerators; while it may be possible to improVe performance oVer time
due to improVements in CMOS technology, further improVements plateau without innoVation at the
algorithmic leVel (Fuchs & Wentzlaff, 2019). As neural network architecture designers, we cannot
simply rely on improVements in hardware to make our proposals Viable for real-world deployment.
2
Published as a conference paper at ICLR 2022
2.2	Graph Neural Networks
Many GNN architectures can be viewed as a generalization of CNN architectures to the irregular
domain: as in CNNs, representations at each node are built based on the local neighborhood using
parameters that are shared across the graph. GNNs differ as we cannot make assumptions about
the the size of the neighborhood, or the ordering. One common framework used to define GNNs
is the message passing neural network (MPNN) paradigm (Gilmer et al., 2017). A graph G =
(V, E) has node features X ∈ RN×F, adjacency matrix A ∈ RN×N and optionally D-dimensional
edge features E ∈ RE ×D . We define a function φ that calculates messages from node u to node
v, a differentiable and permutation-invariant aggregator ㊉，and an update function Y to calculate
representations at layer l + 1: h(+)ι = Yw,㊉j∈N(i)[φ(h(i), h(j), eij)]). Propagation rules for
baseline architecture are provided in Table 1, with further details supplied in Table 5 in the Appendix.
Relative Expressivity of GNNs Common wisdom in the research community states that isotropic
GNNs are less expressive than anisotropic GNNs; empirically this is well supported by benchmarks.
Brody et al. (2022) prove that GAT models can be strictly more expressive than isotropic models.
Bronstein et al. (2021) also discuss the relative expressivity of different classes of GNN layer, and
argue that convolutional (also known as isotropic) models are well suited to problems leveraging
homophily2 in the input graph. They further argue that attentional, or full message passing, models
are suited to handling heterophilous problems, but they acknowledge the resource consumption and
trainability of these architectures may be prohibitive—especially in the case of full message passing.
Scaling and Deploying GNNs While GNNs have seen success across a range of domains, there
remain challenges associated with scaling and deploying them. Graph sampling is one approach
to scaling training for large graphs or models which will not fit in memory. Rather than training
over the full graph, each iteration is run over a sampled sub-graph; approaches vary in whether they
sample node-wise (Hamilton et al., 2017), layer-wise (Chen et al., 2018a; Huang et al., 2018), or sub-
graphs (Zeng et al., 2019; Chiang et al., 2019). Alternatively, systems for distributed GNN training
have been proposed (Jia et al., 2020) to scale training beyond the limits ofa single accelerator. Some
works have proposed architectures that are designed to accommodate scaling: graph-augmented
MLPs, such as SIGN (Rossi et al., 2020), are explicitly designed as a shallow architecture, as all
the graph operations are done as a pre-processing step. Other work includes applying neural archi-
tecture search (NAS) to arrange existing GNN layers (Zhao et al., 2020), or building quantization
techniques for GNNs (Tailor et al., 2021). Finally, a recent work has shown that using memory-
efficient reversible residuals (Gomez et al., 2017) for GNNs (Li et al., 2021) enables us to train far
deeper and larger GNN models than before, thereby progressing the state-of-the-art accuracy.
Why Are Existing Approaches Not Sufficient? It is worth noting that many of these approaches
have significant limitations that we aim to address with our work. Sampling methods are often
ineffective when applied to many problems which involve model generalization to unseen graphs—a
common use-case for GNNs. We evaluated a variety of sampling approaches and observed that even
modest sampling levels, which provide little benefit to memory or latency, cause model performance
to decline noticeably. In addition, these methods do not accelerate the underlying GNN, hence they
may not provide any overall benefit to inference latency. There is also no evidence that we are aware
of that graph-augmented MLPs perform adequately when generalizing to unseen graphs; indeed,
they are known to be theoretically less expressive than standard GNNs (Chen et al., 2021). We
also investigated this setup, and found that these approaches do not offer competitive accuracy with
state-of-the-art approaches. Experiment details and results, along with further discussion of the
limitations of existing work, is provided in Appendix B.
In summary, our work on efficient GNN architecture design is of interest to the community for two
reasons: firstly, it raises questions about common assumptions, and how we design and evaluate
GNN models; secondly, our work may enable us to scale our models further, potentially yielding
improvements in accuracy. In addition, for tasks where we need to generalize to unseen graphs, such
as code analysis or point cloud processing, we reduce memory consumption and latency, thereby
enabling us to deploy our models to more resource-constrained devices than before. We note that
efficient architecture design can be usefully combined with other approaches including sampling,
quantization, and pruning, where appropriate.
2Homophily means that if two nodes are connected, then they have high similarity
3
Published as a conference paper at ICLR 2022
Run separate graph filters
parameterised by Θb over graph
w(i) = Φx(i) + b
×
Apply per-node weightings
w(i) to each basis filter
Figure 2: Visual representation of our EGC-S layer. In this visualization we have 3 basis filters (i.e.
B = 3), which are combined using per-node weightings w. This simplified figure does not show
the usage of heads, or multiple aggregators, as used by EGC-M.
›Σ
3	Our Architecture: Efficient Graph Convolution (EGC)
In this section we describe our approach, and delay theoretical analysis to the next section. We
present two versions: EGC-S(ingle), using a single aggregator, and EGC-M(ulti) which generalizes
our approach by incorporating multiple aggregators. Our approach is visualized in Figure 2.
3.1	Architecture Description
For a layer with in-dimension of F and out-dimension of F0 we use B basis weights Θb ∈ RF0×F.
We compute the output for node i by calculating combination weighting coefficients w(i) ∈ RB
per node, and weighting the results of each aggregation using the different basis weights Θb. The
output for node i is computed in three steps. First, we perform the aggregation with each set of basis
weights Θb. Second, we compute the weighting coefficients w(i) = Φx(i) + b ∈ RB for each node
i, where Φ ∈ RB×F and b ∈ RB are weight and bias parameters for calculating the combination
weighting coefficients. Third, the layer output for node i is the weighted combination of aggregation
outputs:
B
y(i) = X wb(i) X α(i, j)Θbx(j)	(1)
b=1	j∈N (i)
where α(i, j) is some function of nodes i and j, andN (i) denotes the in-neighbours ofi. A popular
method pioneered by GAT (VelickoVic et al., 2θ18) to boost representational power is to represent ɑ
using a learned function of the two nodes’ representations. While this enables anisotropic treatment
of neighbors, and can boost performance, it necessarily results in memory consumption of O(E)
due to messages needing to be explicitly materialized, and complicates hardware implementation for
accelerators. If we choose a representation for α that is not a function of the node representations—
such as α(i,j) = 1 to recover the add aggregator used by GIN (XU et al., 2019), or ɑ(i,j)=
1/yzdeg(i)deg(j) to recover symmetric normalization used by GCN (KiPf & Welling, 2017)一
then we can implement our message propagation phase using sparse matrix multiplication (SpMM),
and avoid explicitly materializing each message, even for the backwards pass. In this work, we
assume α(i, j) to be symmetric normalization as used by GCN unless otherwise stated; we use this
normalization as itis known to offer strong results across a variety of tasks; more formal justification
is provided in section 4.2.
Adding Heads as a Regularizer We can extend our layer through the addition of heads, as
used in architectures such as GAT or Transformers (Vaswani et al., 2017). These heads share the
basis weights, but apply different weighting coefficients per head. We find that adding this degree of
freedom aids regularization when the number of heads (H) is larger than B, as bases are discouraged
from specializing (see section 5.3), without requiring the integration of additional loss terms into the
optimization—hence requiring no changes to code for downstream users. To normalize the output
4
Published as a conference paper at ICLR 2022
dimension, We change the basis weight matrices dimensions to H X F. Using ∣∣ as the concatenation
operator, and making the use of symmetric normalization explicit, we obtain the EGC-S layer:
HB
y(i)=h=ι X—p⅛ M	⑵
EGC works by combining basis matrices. This idea was proposed in R-GCN (Schlichtkrull et al.,
2018) to handle multiple edge types; Xu et al. (2021) can be viewed as a generalization of this
approach to point cloud analysis. In this work we are solving a different problem to these works: we
are interested in designing efficient architectures, rather than new ways to handle edge information.
3.2 Boosting Repres entational Capacity
Recent work by Corso et al. (2020) has shown that using only a single aggregator is sub-optimal:
instead, it is better to combine several different aggregators. In Equation (2) we defined our layer to
use only symmetric normalization. To improve performance, we propose applying different aggre-
gators to the representations calculated by Θbx(j). The choice of aggregators could include different
variants of summation aggregators e.g. mean or unweighted addition, as opposed to symmetric nor-
malization that was proposed in the previous section. Alternatively, we can use aggregators such as
stddev, min or max which are not based on summation. It is also possible to use directional aggre-
gators proposed by Beaini et al. (2021), however this enhancement is orthogonal to this work. If we
have a set of aggregators A, we can extend Equation (2) to obtain our EGC-M layer:
HB
y(i) = ∣ X X
Whii,b	M	Θbx(j)	(3)
h=1 ㊉∈A b=1	j∈N(i)∪{i}
where ㊉ is an aggregator. With this formulation, we are reusing the same messages we have calcu-
lated as before—but we are applying several aggregation functions to them at the same time.
Aggregator Fusion It would appear that adding more aggregators would cause latency and mem-
ory consumption to grow linearly. However, this is not true in practice. Firstly, since sparse op-
erations are typically memory bound in practice, we can apply extra aggregators to data that has
already arrived from memory with little latency penalty. EGC can also efficiently inline the node-
wise weighting operation at inference time, thereby resulting in relatively little memory consump-
tion overhead. The equivalent optimization is more difficult to apply successfully to PNA due to the
larger number of operations per-node that must be performed during aggregation, caused by scaling
functions being applied to each aggregation, before all results are concatenated and a transformation
applied. More details, including profiling and latency measurements, can be found in Appendix D.
4	Interpretation and B enefits
This section will explain our design choices, and why they are better suited to the hardware. We
emphasize that our approach does not directly correspond to attention.
4.1	Spatial Interpretation: Node-Wise Weight Matrices
In our approach, each node effectively has its own weight matrix. We can derive this by re-arranging
eq. (2) by factorizing the Θb terms out of inner sum:
y ⑺=H	Θh)	X y /	1 Xj)I	(4)
h=ι	l∈N⅛{i} PegWdegj))
l{z}	|-------------{--------------'
Varying per Node	Computable via SpMM
In contrast, GAT shares weights, and pushes complexity into the message calculation phase by
calculating per-message weightings. MPNN (Gilmer et al., 2017) and PNA (Corso et al., 2020)
further increase complexity by explicitly calculating each message—resulting in substantial latency
|E|
overhead due to the number of dense operations increasing by roughly |^|. Specifically, we have:
5
Published as a conference paper at ICLR 2022
yGAτ= H Θ (X	αh,ijXj))	yPi)A = U(x(i), M M(x?Xj)))
h = 1	∖j∈N (i)∪{i}	)	j∈N (i) Explicit M essage
^z)	।-------------{------------'	Calculation
Shared Weights Calculated Message Weighting
From an efficiency perspective we observe that our approach of using SpMM has better character-
istics due to it requiring only O(V ) memory consumption—no messages must be explicitly mate-
rialized to use SpMM. We note that although it is possible to propagate messages for GAT with
SpMM, there is no way to avoid storing the weightings during training as they are needed for back-
propagation, resulting in O(E) memory consumption. We also note that fusing the message and
aggregation steps for certain architectures may be possible at inference time, but this is a difficult
pattern for hardware accelerators to optimize for.
Relation To Attention Our method is not directly related to attention, which relies upon pairwise
similarity mechanisms, and hence results in a O(E) cost when using the common formulations.
Alternatives to attention-based Transformers proposed by Wu et al. (2019a) are a closer analogue to
our technique, but rely upon explicit prediction of the per-timestep weight matrix. This approach is
not viable for graphs, as the neighborhood size is not constant.
4.2	Spectral Interpretation: Localised Spectral Filtering
We can also interpret our EGC-S layer through the lens of graph signal processing (Sandryhaila &
Moura, 2013). Many modern graph neural networks build on the observation that the convolution
operation for the Euclidean domain when generalised to the graph domain has strong inductive
biases: it respects the structure of the domain and preserves the locality of features by being an
operation localised in space. Our method can be viewed as a method of building adaptive filters for
the graph domain. Adaptive filters are a common approach when signal or noise characteristics vary
with time or space; for example, they are commonly applied in adaptive noise cancellation. Our
approach can be viewed as constructing adaptive filters by linearly combining learnable filter banks
with spatially varying coefficients.
The graph convolution operation is typically defined on the spectral domain as filtering the input
signal x ∈ RN on a graph with N nodes with a filter gθ parameterized by θ. This requires translating
between the spectral and spatial domain using the Fourier transform. As on the Euclidean domain,
the Fourier transform on the graph-domain is defined as the basis decomposition with the orthogonal
eigenbasis of the Laplace operator, which for a graph with adjacency matrix A ∈ RN ×N is defined
as L = D-A, where D is the diagonal degree matrix with Dii = PjN=1 Aij. The Fourier transform
of a signal x ∈ RN then is F(x) = U>x, where L = UΛU>, with orthogonal eigenvector-matrix
U ∈ RN×N and diagonal eigenvalue-matrix Λ ∈ RN×N. The result of a signal x filtered by gθ is
y = gθ(L)x = Ugθ(Λ)U>x where the second equality holds if the Taylor expansion of gθ exists.
Our approach corresponds to learning multiple filters and computing a linear combination of the
resulting filters with weights depending on the attributes of each node locally. The model therefore
allows applying multiple filters for each node, enabling us to obtain a spatially-varying frequency
response, while staying far below O(E) in computational complexity. Using a linear combination
of filters, the filtered signal becomes y = PbB=1 wb gθb (L)x, where wb ∈ RN are the weights of
filter b for each of the N nodes in the graph. If we parameterize our filter using first-order Chebyshev
polynomials as used by Kipf & Welling (2017) our final expression for the filtered signal becomes
B B B'B	1 1	1	1
Y = Eb=I Wb ® (D 2 AD 2 )XΘb, where A = A + IN is the adjacency matrix With added
self-loops and D is the diagonal degree matrix of A as defined earlier. This justifies the symmetric
normalization aggregator we chose in Equation (2).
Cheng et al. (2021) proposed an approach for localized filtering. However, their approach does not
generalize to unseen topologies or scale to large graphs as it requires learning the coefficients of
several filter matrices Sk of size N × N. Our approach does not suffer from these constraints.
6
Published as a conference paper at ICLR 2022
Architecture	Propagation Rule	Memory	ZINC (MAE D Unseen Graph Regression	CIFAR (Acc. ↑) Unseen Graph Classification	MolHIV (ROC-AUC ↑) Unseen Graph Classification	Code-V2 (F1 ↑) Unseen Graph Classification	Arxiv (Acc. ↑) Transductive Node Classification
GCN	-y ⑴=Θ P「	1	Xj)- j √deg(i)deg(j)	O(V)	0.459 ± 0.006	55.71 ± 0.38	76.14 ± 1.29	0.1480 ± 0.0018	71.92 ± 0.21
GIN	y(i) = fΘ[(1 + )x(i) + Pj x(j)]	O(V)	0.387 ± 0.015	55.26 ± 1.53	76.02 ± 1.35	0.1481 ± 0.0027	67.33 ± 1.47
GraphSAGE	y(i) = Θ1x(i) +Lj Θ2x(j)	O(V)	0.468 ± 0.003	65.77 ± 0.31	75.97 ± 1.69	0.1453 ± 0.0028	71.73 ± 0.26
GAT	y(i) = αi,iΘx(i) + Pj αi,j Θx(j)	O(E)	0.475 ± 0.007	64.22 ± 0.46	77.17 ± 1.37	0.1513 ± 0.0011	* 71.81 ± 0.23
GATv2	y(i) = αi,iΘx(i) + Pj αi,j Θx(j)	O(E)	0.447 ± 0.015	67.48 ± 0.53	77.15 ± 1.55	0.1537 ± 0.0022	* 71.87 ± 0.43
MPNN-Sum	y(i) = U(x(i),PjM(x(i),x(j)))	O(E)	0.381 ± 0.005	65.39 ± 0.47	75.19 ± 3.57	0.1470 ± 0.0017	* 66.11 ± 0.56
MPNN-Max	y(i) = U (x(i), maxjM(x(i), x(j)))	O(E)	0.468 ± 0.002	69.70 ± 0.55	77.07 ± 1.37	0.1552 ± 0.0022	* 71.02 ± 0.21
PNA	y(i) = U(x(i), Lj M(x(i), x(j)))	O(E)	0.320 ± 0.032	70.21 ± 0.15	79.05 ± 1.32	* 0.1570 ± 0.0032	* 71.21 ± 0.30
EGC-S (Ours)	Equation 2	O(V)	0.364 ± 0.020	66.92 ± 0.37	77.44 ± 1.08	0.1528 ± 0.0025	72.21 ± 0.17
EGC-M (Ours)	Equation 3	O(V)	0.281 ± 0.007	71.03 ± 0.42	78.18 ± 1.53	0.1595 ± 0.0019	71.96 ± 0.23
Table 1: Results (mean ± standard deviation) for parameter-normalized models run on 5 datasets.
Details of the specific aggregators chosen per dataset and further experimental details can be found
in the supplementary material. Results marked with * ran out of memory on 11GB 1080Ti and
2080Ti GPUs. EGC obtains best performance on 4 of the tasks, with consistently wide margins.
5	Evaluation
5.1	Protocol
We primarily evaluate our approach on 5 datasets taken from recent works on GNN benchmarking.
We use ZINC and CIFAR-10 Superpixels from Dwivedi et al. (2020) and Arxiv, MolHIV and Code
from Open Graph Benchmark (Hu et al., 2020). These datasets cover a wide range of domains,
cover both transductive and inductive tasks, and are larger than datasets which are typically used in
GNN works. We use evaluation metrics and splits specified by these papers. Baseline architectures
chosen reflect popular general-purpose choices (Kipf & Welling, 2017; Xu et al., 2019; Hamilton
et al., 2017; Velickovic et al., 2018; Gilmer et al., 2017), along with the state-of-the-art PNA (Corso
et al., 2020) and GATv2 (Brody et al., 2022) architectures.
In order to provide a fair comparison we standardize all parameter counts, architectures and opti-
mizers in our experiments. All experiments were run using Adam (Kingma & Ba, 2014). Further
details on how we ensured a fair evaluation can be found in the appendix.
We do not use edge features in our experiments as for most baseline architectures there exist no
standard method to incorporate them. We do not use sampling, which, as explained in Section 2.2,
is ineffective for 4 datasets; for the remaining dataset, Arxiv, we believe it is not in the scientific
interest to introduce an additional variable. This also applies to GraphSAGE, where we do not use
the commonly applied neighborhood sampling. All experiments were run 10 times.
5.2	Main Results
Our results across the 5 tasks are shown in Table 1. We draw attention to the following observations:
•	EGC-S is competitive with anisotropic approaches. We outperform GAT(v1) and
MPNN-Sum on all benchmarks, despite our resource efficiency. The clearest exception
is MPNN-Max on CIFAR & Code, where the max aggregator provides a stronger inductive
bias. We observe that GATv2 improves upon GAT, but does not clearly outperform EGC.
•	EGC-M outperforms PNA. The addition of multiple aggregator functions improves per-
formance of EGC to beyond that obtained by PNA. We hypothesize that our improved
performance over PNA is related to PNA’s reliance on multiple degree-scaling transforms.
While this approach can boost the representational power of the architecture, we believe
that it can result in a tendency to overfit to the training set.
•	EGC performs strongly without running out of memory. We observe that EGC is one of
only three architectures that did not exhaust the VRAM of the popular Nvidia 1080/2080Ti
GPUs, with 11GB VRAM, when applied to Arxiv: wehadtouseanRTX 8000 GPU with
48GB VRAM to run these experiments. PNA, our closest competing technique accuracy-
wise, exhausted memory on the Code benchmark as well. Detailed memory consumption
figures are provided in Table 4.
7
Published as a conference paper at ICLR 2022
(a) Constant parameter count (100k)
(b) Constant hidden dimension (128)
Figure 3: Study over the number of heads (H) and bases (B). Study run on ZINC dataset with
EGC-S. Metric is MAE (mean ± standard deviation): lower is better. We study keeping the total
parameter count constant, and fixing the hidden dimension. Each experiment was tuned individually.
Setting B > H does not necessarily improve performance due to the risk of overfitting, and forces
the usage of a smaller hidden dimension to retain a constant parameter count.
Overall, EGC obtains the best performance on 4 out of the 5 main datasets; on the remaining dataset
(MolHIV), EGC is the second best architecture. This represents a significant achievement: our
architecture demonstrates that We do not need to choose between efficiency and accuracy.
5.3	Additional Studies
HoW Should Heads and Bases be Chosen? To understand the trade-off between the number of
heads (H) and bases (B), We ran an ablation study on ZINC using EGC-S; this in shown in Figure 3.
The relationship between these parameters is non-trivial. There are several aspects to consider: (1)
increasing H and B means that we spend more of our parameter budget to create the combinations,
which reduces hidden dimension—as shown in Figure 3. This is exacerbated if we use multiple
aggregators: our combination dimension must be HB|A|. (2) Increasing B means we must reduce
the hidden size substantially, since it corresponds to adding more weights of size H X F. (3)
Increasing H allows us to increase hidden size, since each basis weight becomes smaller. We see
in Figure 3 that increasing B beyond H does not yield significant performance improvements: we
conjecture that bases begin specializing for individual heads; by sharing, there is a regularizing
effect, like observed in Schlichtkrull et al. (2018). This regularization stabilizes the optimization
and we observe lower trial variance for smaller B .
We advise B = H or B =冬 We find H = 8 tobe effective with EGC-S; for EGC-M, where more
parameters are spent on combination weights, we advise setting H = 4. This convention is applied
consistently for Table 1; full details are supplied in the appendix and code.
Should The Combination Weightings (w) Be
Activated? Any activation function will
shrink the space the per-node weights Θ(hi) can
lie in, hence we would expect it to harm perfor-
mance; this is verified in Table 2. Activating
w may improve training stability, but we did
not observe to be an issue in our experiments.
Another issue is that different aggregators re-
sult in outputs with different means and vari-
ances (Tailor et al., 2021), hence they need to
be scaled by different factors to be combined.
Activation EGC-S	EGC-M
Identity	0.364 ± 0.020	0.281 ± 0.007
Hardtanh	0.435 ± 0.010	0.293 ± 0.013
Sigmoid	0.366 ± 0.008	0.303 ± 0.016
Softmax	0.404 ± 0.010	0.307 ± 0.013
Table 2: Activating the combination weightings w
harms performance. Run on ZINC; lower is better.
Applying EGC to Large-Scale Heterogeneous Graphs We evaluated EGC on the heteroge-
neous OGB-MAG dataset, containing 2M nodes and 21M edges. On a homogeneous version of the
graph, we exceed the baselines’ performance by 1.5-2%; we experiment with both symmetric nor-
malization (EGC-S) and the mean aggregators to demonstrate that the mechanism utilized by EGC
8
Published as a conference paper at ICLR 2022
is effective, regardless of which aggregator provides the stronger inductive bias for the dataset. Our
architecture can be expanded to handle different edge types, yielding the R-EGC architecture which
improves performance over R-GCN by 0.9%. We expect that accuracy can be further improved by
using sampling techniques to regularize the optimization, or using pretrained embeddings; however,
adding these techniques makes comparing the results more difficult as it is known that sampling
techniques can affect each architecture’s performance differently (Liu et al., 2020).
5.4	Memory and Latency Benchmarks
We now assess our model’s resource efficiency. For
CPU measurements we used an Intel Xeon Gold
5218 and for GPU we used an Nvidia RTX 8000.
Aggregator Fusion We evaluated aggregator fu-
sion across several topologies on both CPU and
GPU. For space reasons, we leave full details to
Appendix D. In summary, we observe that using 3
aggregators over standard SpMM incurs an addi-
tional overhead of only 14%, enabling us to im-
prove model performance without excessive com-
putational overheads at inference time.
End-to-End Latency We provide latency and
memory statistics for parameter-normalized mod-
els in Table 4. We draw attention to how slow
and memory-intensive the O(E) models are for
training and inference. The reader should note
that the inference latency and memory consump-
tion for MPNN and PNA rises by 6-7× rela-
|E|
tive to EGC, corresponding to the large 吊 ra-
tio for Arxiv. EGC-M offers substantially lower
latency and memory consumption than its near-
est competitor, PNA, however the precise bene-
fit will be dataset-dependent. Extended results
can be found in Appendix E, including results for
accuracy-normalized models on Arxiv, which fur-
ther demonstrate EGC’s resource efficiency.
6 Discussion and Conclusion
Method	Test Accuracy % ↑
MLP	26.92 ± 0.26
GCN	30.43 ± 0.25
GraphSAGE-Mean	31.53 ± 0.15
EGC-S	32.13 ± 0.73
EGC (㊉=Mean)	33.22 ± 0.50
R-GCN (Full Batch)	46.29 ± 0.45
R-EGC (Full Batch)	47.21 ± 0.40
Table 3: EGC can be applied applied to large
scale heterogeneous graphs, outperforming R-
GCN by 0.9%.
Model	Peak Training Memory (MB)	GPUTraining Latency (ms)	GPU Inference Latency (ms)
GCN	1905	159.8 ± 4.6	35.2 ± 0.1
GIN	1756	155.2 ± 3.9	35.2 ± 0.1
GraphSAGE	1352	113.9 ± 5.4	25.1 ± 0.4
GAT	10841	324.3 ± 1.2	84.7 ± 0.3
GATv2	14124	341.8 ± 0.5	129.2 ± 0.2
MPNN-Sum	14323	768.2 ± 0.8	230.7 ± 0.3
MPNN-Max	14623	797.8 ± 0.9	258.2 ± 0.4
PNA	14533	892.7 ± 1.1	305.7 ± 0.5
EGC-S	2430	177.7 ± 2.2	37.3 ± 0.1
EGC-M	4068	220.9 ± 0.6	42.2 ± 0.3
Table 4: Memory and latency statistics for
parameter-normalized models (used in ta-
ble 1) on Arxiv. Note that EGC-M memory
consumption and latency can be reduced with
aggregator fusion at inference time.
How Surprising Are Our Results? We observed that it was possible to design an isotropic GNN
that is competitive with state-of-the-art anisotropic GNN models on 6 benchmarks. This result con-
tradicts common wisdom in the GNN community. However, our results may be viewed as part of
a pattern visible across the ML community: both the NLP and computer vision communities have
seen works indicating that anisotropy offered by Transformers may not be necessary (Tay et al.,
2021; Liu et al., 2022), consistent with our observations. It is worth asking why we observe our re-
sults, given that they contradict properties that have been theoretically proven. We believe that there
are a variety of reasons, but the most important is that most real world datasets do not require the
theoretical power these more expressive models provide to achieve good results. In particular, many
real world datasets are homophilous: therefore simplistic approaches, such as EGC, can achieve
high performance. This implies that the community should consider adding more difficult datasets
to standard benchmarks, such as those presented in Lim et al. (2021) and Velickovic et al. (2021).
Our proposed layer, EGC, can be used as a drop-in replacement for existing GNN layers, and
achieves better results across 6 benchmark datasets compared to strong baselines, with substantially
lower resource consumption. Our work raises important questions for the research community,
while offering significant practical benefits with regard to resource consumption. We believe the
next step for our work is incorporation of edge features e.g. through line graphs (Chen et al., 2020b)
or topological message passing (Bodnar et al., 2021).
9
Published as a conference paper at ICLR 2022
Acknowledgements
This work was supported by the UK’s Engineering and Physical Sciences Research Council (EP-
SRC) with grant EP/S001530/1 (the MOA project) and the European Research Council (ERC) via
the REDIAL project (Grant Agreement ID: 805194). FLO acknowledges funding from the Huawei
Studentship at the Department of Computer Science and Technology of the University of Cambridge.
The authors would like to thank Ben Day, Javier Fernandez-Marques, Chaitanya Joshi, Titouan
Parcollet, Petar VeliCkovic, and the anonymous reviewers Who have provided comments on earlier
versions of this work.
Ethics S tatement
The method described in this paper is generic enough that it can be applied to any problem GNNs
are applied to. The ethical concerns associated with this work are related to enabling more efficient
training and deployment of GNNs. This may be positive (drug discovery) or negative (surveillance),
but these concerns are inherent to any work investigating efficiency for machine learning systems.
Reproducibility S tatement
We have supplied the code required to regenerate our results, along with the hyperparameters re-
quired. In addition, we supply pre-trained models. Resources associated with this paper can be
found at https://github.com/shyam196/egc.
References
Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs
with graphs. arXiv preprint arXiv:1711.00740, 2017.
Gene M Amdahl. Validity of the single processor approach to achieving large scale computing
capabilities. In Proceedings ofthe April 18-20,1967, spring joint computer conference, pp. 483—
485, 1967.
Dominique Beaini, Saro Passaro, Vincent Letourneau, William L. Hamilton, Gabriele Corso, and
Pietro Lio. Directional graph networks, 2021.
Cristian Bodnar, Fabrizio Frasca, Yu Guang Wang, Nina Otter, Guido Montufar, Pietro Lio, and
Michael Bronstein. Weisfeiler and lehman go topological: Message passing simplicial networks,
2021.
Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In Interna-
tional Conference on Learning Representations, 2022. URL https://openreview.net/
forum?id=F72ximsx7C1.
Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges, 2021.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. arXiv preprint arXiv:1801.10247, 2018a.
Lei Chen, Zhengdao Chen, and Joan Bruna. On graph neural networks versus graph-augmented
{mlp}s. In International Conference on Learning Representations, 2021. URL https://
openreview.net/forum?id=tiqI7w64JG2.
Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen
Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Tvm:
An automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th
USENIX Conference on Operating Systems Design and Implementation,OSDr18, pp. 579-594,
USA, 2018b. USENIX Association. ISBN 9781931971478.
10
Published as a conference paper at ICLR 2022
Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak, Ling Liang, Mingyu Yan, Lei
Deng, Yufei Ding, Zidong Du, Yunji Chen, and Yuan Xie. Rubik: A Hierarchical Architecture for
Efficient Graph Learning. arXiv:2009.12495 [cs], September 2020a. URL http://arxiv.
org/abs/2009.12495. arXiv: 2009.12495.
Zhengdao Chen, Xiang Li, and Joan Bruna. Supervised community detection with line graph neural
networks, 2020b.
Xiuyuan Cheng, Zichen Miao, and Qiang Qiu. Graph convolution with low-rank learnable local
filters. In International Conference on Learning Representations, 2021.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257-266, 2019.
Intel Corporation. Intel® 64 and ia-32 architectures optimization reference manual.
Gabriele Corso, LUca Cavalleri, DominiqUe Beaini, Pietro Lio, and Petar VeliCkovic. Principal
Neighbourhood Aggregation for Graph Nets. arXiv:2004.05718 [cs, stat], June 2020. URL
http://arxiv.org/abs/2004.05718. arXiv: 2004.05718.
Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas LaUrent, YoshUa Bengio, and Xavier Bresson.
Benchmarking Graph NeUral Networks. arXiv:2003.00982 [cs, stat], JUly 2020. URL http:
//arxiv.org/abs/2003.00982. arXiv: 2003.00982.
Matthias Fey and Jan Eric Lenssen. Fast Graph Representation Learning with PyTorch Geometric,
5 2019. URL https://github.com/pyg-team/pytorch_geometric.
Adi FUchs and David Wentzlaff. The accelerator wall: Limits of chip specialization. In 2019 IEEE
International Symposium on High Performance Computer Architecture (HPCA), pp. 1-14. IEEE,
2019.
JUstin Gilmer, SamUel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. NeUral
message passing for qUantUm chemistry. In International Conference on Machine Learning, pp.
1263-1272. PMLR, 2017.
Aidan N. Gomez, Mengye Ren, RaqUel UrtasUn, and Roger B. Grosse. The reversible residUal
network: Backpropagation withoUt storing activations, 2017.
Daya GUo, ShUo Ren, ShUai LU, Zhangyin Feng, DUyU Tang, ShUjie LiU, Long ZhoU, Nan DUan,
Jian Yin, Daxin Jiang, et al. Graphcodebert: Pre-training code representations with data flow.
arXiv preprint arXiv:2009.08366, 2020.
William L Hamilton, Rex Ying, and JUre Leskovec. IndUctive representation learning on large
graphs. arXiv preprint arXiv:1706.02216, 2017.
WeihUa HU, Matthias Fey, Marinka Zitnik, YUxiao Dong, HongyU Ren, Bowen LiU, Michele Catasta,
and JUre Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
Wenbing HUang, Tong Zhang, YU Rong, and JUnzhoU HUang. Adaptive sampling towards fast graph
representation learning. arXiv preprint arXiv:1809.05343, 2018.
Zhihao Jia, Sina Lin, MingyU Gao, Matei Zaharia, and Alex Aiken. Improving the accUracy, scala-
bility, and performance of graph neUral networks with roc. Proceedings of Machine Learning and
Systems, 2:187-198, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N. Kipf and Max Welling. Semi-SUpervised Classification with Graph ConvolUtional Net-
works. In International Conference on Learning Representations, 2017.
11
Published as a conference paper at ICLR 2022
GUohao Li, Matthias Muller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks
with 1000 layers, 2021.
Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-
homophilous graphs, 2021.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s, 2022.
Ziqi Liu, Zhengwei Wu, Zhiqiang Zhang, Jun Zhou, Shuang Yang, Le Song, and Yuan Qi. Bandit
samplers for training graph neural networks, 2020.
Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-
based simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020.
Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning
on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification, 2020.
Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Fed-
erico Monti. Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198,
2020.
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Pe-
ter W Battaglia. Learning to simulate complex physics with graph networks. arXiv preprint
arXiv:2002.09405, 2020.
Aliaksei Sandryhaila and Jose MF Moura. Discrete signal processing on graphs. IEEE transactions
on signal processing, 61(7):1644-1656, 2013.
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:
Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 4938-4947, 2020.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference, pp. 593-607. Springer, 2018.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural
networks. Synthesis Lectures on Computer Architecture, 15(2):1-341, 2020.
Shyam A. Tailor, Javier Fernandez-Marques, and Nicholas D. Lane. Degree-Quant: Quantization-
Aware Training for Graph Neural Networks. In International Conference on Learning Represen-
tations, 2021.
Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler.
Are pre-trained convolutions better than pre-trained transformers?, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neu-
ral Information Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181- attention- is- all- you- need.pdf.
Petar Velickovic, Adria PUigdomenech Badia, David Budden, Razvan Pascanu, Andrea Banino,
Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning bench-
mark. 2021.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
12
Published as a conference paper at ICLR 2022
Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions, 2019a.
Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr. au2, Christopher Fifty, Tao Yu, and Kilian Q.
Weinberger. Simplifying graph convolutional networks, 2019b.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural
Networks? arXiv:1810.00826 [cs, stat], February 2019. URL http://arxiv.org/abs/
1810.00826. arXiv: 1810.00826.
Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan Qi. Paconv: Position adaptive convolu-
tion with dynamic kernel assembling on point clouds, 2021.
Mingyu Yan, Lei Deng, Xing Hu, Ling Liang, Yujing Feng, Xiaochun Ye, Zhimin Zhang, Dongrui
Fan, and Yuan Xie. HyGCN: A GCN Accelerator with Hybrid Architecture. In 2020 IEEE Inter-
national Symposium on High Performance Computer Architecture (HPCA), pp. 15-29, February
2020. doi: 10.1109/HPCA47549.2020.00012. ISSN: 2378-203X.
Carl Yang, Aydin Buluc, and John D. Owens. Design principles for sparse matrix multiplication on
the gpu, 2018.
Haoran You, Tong Geng, Yongan Zhang, Ang Li, and Yingyan Lin. Gcod: Graph convolutional net-
work acceleration via dedicated algorithm and accelerator co-design. In 28th IEEE International
Symposium on High-Performance Computer Architecture (HPCA 2022), 2021.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019.
Yongan Zhang, Haoran You, Yonggan Fu, Tong Geng, Ang Li, and Yingyan Lin. G-cos: Gnn-
accelerator co-search towards both better accuracy and efficiency, 2021.
Yiren Zhao, Duo Wang, Xitong Gao, Robert Mullins, Pietro Lio, and Mateja Jamnik. Probabilistic
dual network architecture search on graphs. arXiv preprint arXiv:2003.09676, 2020.
13
Published as a conference paper at ICLR 2022
Method	Propagation Rule	Memory	Notes
GCN (Kipf & Welling, 2017)	y(i) = θ Pj∈N(i)∪{i} √deg(1)deg(j) Xs	O(V)	Formally defined for undirected graphs with self-loops; moti- vated by graph signal processing.
GIN (Xu et al., 2019)	y(i) = fΘ [(1 + )x(i) + Pj∈N(i) x(j)]	O(V)	f is a learnable function, typically parameterized as an MLP or linear layer; may be fixed or learned.
GraphSAGE (Hamil- y(i) = Θ1x(i) + Lj∈N(i) Θ2x(j) ton et al., 2017)		O(V)	L typically parameterized as mean or max.
GAT (Velickovic et al., 2018)	y(i) = αi,iΘX(i) + Pj∈N(i) αi,j ΘX(j)	O(E)	Attention coefficients calculated using:	αi,j	= exp(LeakyReLU(a> [Θx(i) ∣∣ Θx(j) ])) Pk∈N(i)∪{i} eχp(LeakyReLU(a>[θχ(i) Il θχ(k)])).	ommon to define multiple attention heads and concatenate.
GATv2 (Brody et al., 2022)	y(i) = αi,iΘX(i) + Pj∈N (i) αi,j ΘX(j)	O(E)	Similar to GAT, but with attention calcula- tion redefined to improve expressivity	αi,j	= exp(a>LeakyReLU(Θ[xi ∣ xj])) Pk∈N(i)∪{i} eχp(a>LeakyReLU(6[xi k xk])).
MPNN (Gilmer et al., 2017)	y(i) = U(X(i),Lj∈N(i) M(X(i),X(j),eij))	O(E)	U, M typically defined as linear layers acting on concatenated features; L may be any valid aggregator, typically sum or max.
PNA (Corso et al., 2020)	y(i) = U(X(i),Lj∈N(i) M(X(i),X(j),eij))	O(E)	Similar to MPNN, but with defined to use 4 aggregators (mean, standard deviation, max, and min) scaled by 3 different functions of node degree, resulting in 12 different aggregations by default.
Table 5: Propagation rules for general-purpose GNN architectures we compare against in this work;
rules are provided using node-wise formulations. We evaluate against popular architectures, and a
recent proposal that has achieved state-of-the-art performance, PNA.
A Further Experiment Details
A. 1 Ensuring Fairness
We expand on our experimental protocol, with a particular focus on describing measures that we
took to ensure that the results we report are not unfairly biased towards EGC.
For EGC-S, we use H = 8 and B = 4, as implied by our ablation for all experiments, with the
single exception of OGB-Code, where we use H = B = 8. The benefit of using a smaller set of
bases is that we can increase the hidden dimension, but this is not viable in this case since most of
the 11M parameters in the model correspond to the token read-out layers, which quickly increases
as the model hidden dimension grows. As shown by section 5.3, if we cannot increase the hidden
dimension, it is better to increase the bases. This is the only exception we make.
For EGC-M, we use H = B = 4 for all experiments. The main challenge is aggregator selection,
and this remains a major challenge for our work. We were unable to find a satisfactory technique
for automated discovery of aggregator choices, hence we rely on heuristics to find them. We restrict
ourselves to use 3 aggregators for each model (yielding 35 possible choices). In order to determine
the aggregators, we use two heuristics: (1) aggregators should be “diverse” and (2) aggregators
should be chosen based on inductive bias for task. Using these two rules, we try up to 3 possible
choices of aggregators; all choices considered are shown in Table 6. We note that while some choices
do improve performance, our conclusions are not invalidated; it is also worth noting that it is likely
that better aggregator choices can be found.
A.2 Cluster Details
Most of our experiments were run on several machines in our SLURM cluster using Intel CPUs and
NVIDIA GPUs. Each machine was running Ubuntu 18.04. The GPU models in our cluster were
RTX 2080Ti and GTX 1080Ti. High-memory experiments were run on V100s in our cluster and an
RTX 8000 virtual machine we had access to.
14
Published as a conference paper at ICLR 2022
Dataset	Add	Mean	Symmetric Normalization	Max	Min	Std	Var	Result
Zinc	X			X		X		0.281 ± 0.007
Zinc			X	X	X			0.284 ± 0.045
CIFAR	X			X		X		71.03 ± 0.42
CIFAR			X	X	X			70.05 ± 1.14
MolHIV	X	X		X				78.19 ± 1.54
MolHIV	X			X			X	77.40 ± 1.02
MolHIV	X			X		X		77.98 ± 1.65
Arxiv		X	X	X				71.96 ± 0.23
Arxiv	X	X	X					70.59 ± 0.67
Arxiv	X	X		X				70.38 ± 0.76
Code-V2			X	X	X			0.1595 ± 0.0019
Code-V2				X	X	X		0.1572 ± 0.0029
Code-V2	X			X		X		0.1578 ± 0.0021
Table 6: Possible aggregators tried for EGC-M. Up to 3 combinations (from a possible 35) were
tried, as we limited ourselves to always using 3 aggregators. Our conclusions do not change, and it
is likely that better results can be found with more optimal aggregator choices.
Model	ZINC (MAE D	CIFAR (Acc. ↑)	MolHIV (ROC-AUC ↑)	Code-V2 (F1 ↑)
GA-MLP	0.510 ± 0.037	58.13 ± 0.65	75.50 ± 1.32	0.1485 ± 0.0027
GCN	0.459 ± 0.006	55.71 ± 0.38	76.14 ± 1.29	0.1480 ± 0.0018
EGC-S	0.364 ± 0.020	66.92 ± 0.37	77.44 ± 1.08	0.1528 ± 0.0025
Table 7: Results of applying graph-augmented MLPs (GA-MLPs) to tasks requiring generalization
to unseen graphs. We see that the performance is broadly similar to the corresponding GCN model—
and far weaker than EGC-S, which uses the same aggregator. We also considered using the add
aggregation on ZINC, and achieved 0.444 ± 0.019. By comparison, the equivalent GIN model
(which uses the add aggregation) achieved 0.387 ± 0.015.
B	Limitations of Existing Approaches
As explained in the related work, existing approaches to improving GNN efficiency have severe
limitations. In this section we elaborate upon them, and provide experimental evidence where nec-
essary.
We first examine graph-augmented MLPs (GA-MLPs); to our knowledge there are few experimental
results assessing the performance of these models when they are applied to problems requiring
generalization to unseen graphs. In the literature they are generally applied to large scale node
classification benchmarks, such as those found in the OGB benchmark suite (Wu et al., 2019b;
Rossi et al., 2020). It is known that these models are theoretically less expressive than standard
GNNs, however their performance when applied to node classification datasets has been acceptable.
We consider models of the form:
4
Y = Readout(MLP( k SkXW))	(5)
k=0
We use up to the 4th power of the diffusion operator S to emulate the depth of the corresponding
GNNs, which all use 4 layers. We set S to use symmetric normalization, as used by GCN and EGC-
S; we also consider setting S to the adjacency matrix on ZINC, to emulate the operations used by
GIN. The results are provided in Table 7. We see that the GA-MLP models offer similar (but often
worse) performance than the corresponding GNN baselines. The models are not competitive with
approaches such as GAT or MPNN, and is outperformed by a wide margin by EGC-S. Achieving
15
Published as a conference paper at ICLR 2022
Experiment	ZINC (MAE J)	CIFAR (Acc. ↑)	MolHIV (ROC-AUC ↑)	Code-V2 (F1 ↑)
EGC-S	0.364 ± 0.020	66.92 ± 0.37	77.44 ± 1.08	0.1528 ± 0.0025
EGC-S + DropEdge (p = 0.1)	0.468 ± 0.007	66.37 ± 0.28	77.41 ± 1.32	0.1553 ± 0.0021
EGC-S + DropEdge (p = 0.5)	0.629 ± 0.023	64.60 ± 0.58	75.33 ± 0.82	0.1527 ± 0.0019
EGC-S + GraphSAINT Node Sampler (p = 0.1)	0.631 ± 0.012	61.37 ± 0.75	73.65 ± 1.41	0.1461 ± 0.0027
Table 8: Results of applying sampling approaches to EGC-S. We do not enable sampling at test
time—hence these approaches do not offer any test time reductions to computation. Sampling, in
principle, is applicable to any underlying GNN: our conclusions will transfer to other underlying
GNNs. We see that DropEdge (Rong et al., 2020) with a low drop probability (p = 0.1) can aid
model performance; however, setting p this low does not significantly reduce memory consumption
or computation. Increasing p to 0.5 does reduce resource consumption noticeably, but results in
noticeable degradation to model performance.
state-of-the-art performance with GA-MLPs does not appear to be possible, at least with our current
understanding of these models.
We now proceed to investigate sampling, in which each training step does not run over the entire
graph, but over some sampled subgraph. These methods have seen great popularity when applied to
tasks such as node classification, and they are able to deliver sampling ratios in excess of 20×, hence
yielding noticeable improvements to memory consumption (the primary limitation with large scale
training). However, we note that the benefit of these methods has not been examined carefully for
many graph topologies; while they have been shown to be effective for many “small-world” graph
topologies—which arise in many graphs—it is not the case that all graphs fall into this category. For
example, molecule graphs would not fit this category.
We first assess the sampling strategy from GraphSAINT (Zeng et al., 2019) by applying it to EGC-S
models. For our experiments, we use the node-centric sampler. The results are presented in Table 8;
we disable sampling at test time. Even with a relatively low dropping probability of 10% (i.e. 90%
of nodes are retained), the model performance degradation is severe. We note that the computational
savings achieved are modest when using this drop probablility. We also observed similar results
when using the edge-centric sampler from GraphSAINT.
We also attempted a different approach for sampling proposed by DropEdge (Rong et al., 2020);
as before, we apply it to EGC-S models. In this simple scheme, elements of the adjacency matrix
are dropped; this scheme was shown to be effective for training deep graph networks, but it is also
useful for reducing the computational footprint of models, since it effectively reduces the number of
messages that have to be computed. We also provide results in Table 8. The results are significantly
better than we observe when using GraphSAINT’s sampling strategies: at 10% drop probabilities
we even see an improvement in some cases, due to the regularization effect. However, once we
increase the drop probability to levels where we would observe a noticeable reduction in compu-
tational demand, we observe that model performance declines. In summary, while DropEdge is a
more effective strategy for sampling on many inductive tasks, it is not beneficial as a method to
reduce the computational burden.
Finally, we discuss the limitations of other approaches proposed in the literature. Quantization is
an approach that is typically applied at inference time; mixed-precision approaches can be applied
at training time, however care must be taken for GNNs to avoid biasing the gradients (Tailor et al.,
2021). Additionally, while neural architecture search (NAS) may be useful to find memory-efficient
models if the search objective is set appropriately, they suffer from some limitations—primarily
search time and memory consumption. Finally, approaches such as reversible residuals (Li et al.,
2021) are useful to architecture design, they do not tackle issues such as high peak memory usage
induced by the message passing step.
C Approaches for Hardware Acceleration of GNNs
The reader should note that most existing work for GNN hardware acceleration focuses on support-
ing only a subset of GNNs: specifically, they tend to only support models that can be implemented
16
Published as a conference paper at ICLR 2022
using SpMM. Approaches in the literature falling into this area include Chen et al. (2020a), Yan
et al. (2020), You et al. (2021) and Zhang et al. (2021). It is possible to add greater flexibility to
the accelerator to support more expressive message passing schemes, however this necessarily im-
plies greater complexity. As Amdahl’s law (Amdahl, 1967) implies, increasing flexibility is likely
to reduce peak performance, while increasing silicon area requirements. Therefore, aiming for the
simplest primitive (as we do with EGC) is the most sensible approach to obtain hardware accelera-
tion.
D Aggregator Fusion
Algorithm 1 Aggregator Fusion with aggregators A. This method is a modification of the Com-
pressed Sparse Row (CSR) SpMM algorithm, where we maximize re-use of matrix B. Maximizing
re-use enables us to obtain significantly better accuracy with minimal impact on memory and la-
tency. For simplicity, pseudocode assumes H = B = 1. This version demonstrates how we can
remove memory overheads at inference time.
Input: CSR A ∈ RN×N, Dense B ∈ RN×F, Combination weightings W ∈ RN×lAl
Output: Dense C ∈ RN ×F
for i = 0 to A.rows - 1 do
for jj = A.row_pointer[i] to A.row_pointer[i + 1] do
j = A. column_index [jj ]
Init temp arrays of length F per aggregator
aij = A.values[jj]
{May be faster to interleave these calls:}
for ㊉ ∈ A do
ProceSs_row㊉(aj, B[i,:], temp㊉)
end for
end for
{Can be generalized to H, B > 1:}
C[i, :] = Pφ∈A w[i,㊉]∙ temP出[：]
end for
The naive approach of performing each aggregation sequentially would cause a linear increase in
latency with respect to |A|. However, a key observation to note is that we are memory-bound: the
bottleneck with sparse operations is waiting for the data to arrive from memory. This observation
applies to both GPUs and CPUs, and justified through profiling. Using a profiler on a GTX 1080Ti
we observed that SpMM using the Reddit graph Hamilton et al. (2017) with feature sizes of 256
achieved just 1.2% of the GPU’s peak FLOPS, with 88.5% of stalls being caused by unmet memory
dependencies. The fastest processing order performs as much work as possible with data that has
already been fetched from memory, rather than fetching it multiple times. This concept is illustrated
in Algorithm 1 in the appendix. We can perform all aggregations as a lightweight modification to
the standard compressed sparse row (CSR) SpMM algorithm.
The second observation we make is that storing the results of all aggregations is unnecessary at in-
ference time. Note that the CSR SpMM algorithm processes each row in the output matrix sequen-
tially: rather than storing the aggregations for every row, instead we should store only the weighted
results. This approach not only reduces memory consumption, but also latency as we improve the
effectiveness of our cache and reduce memory system contention. In practice, this optimization is
especially important when performing inference on topologies which have more frequent periods
where the processing has become compute-bound, since we reduce contention between load and
store units Corporation. This is also demonstrated in Algorithm 1.
We evaluated aggregator fusion across four different topologies, on both CPU and GPU; our results
can be found in Table 9. We assumed all operations are 32-bit floating point, and that we were
using three aggregators: summation-based, max, and min; these aggregators match those used for
EGC-M Code. Our benchmarks were conducted on a batch of 10k graphs from the ZINC and Code
datasets, Arxiv, and the popular Reddit dataset (Hamilton et al., 2017), which is one of the largest
graph datasets commonly evaluated on in the GNN literature. Our SpMM implementation on GPU
is based on Yang et al. (2018). Code for the kernels are provided in our repo.
17
Published as a conference paper at ICLR 2022
CPU (Xeon Gold 5218)
GPU (RTX 8000)
Method
Reddit / s Code / s
Arxiv / s
ZINC / s Reddit / ms Code / ms Arxiv /ms ZINC / ms
Weight Matmul	0.07	±	0.00	0.423 ± 0.023	0.055	± 0.013	0.074 ±	0.010
CSR SpMM	29.08	±	0.12	1.943 ± 0.040	0.760	± 0.021	0.315 ±	0.006
Naive Fusion	88.25	±	0.20	8.680 ± 0.094	2,631	± 0.016	1,482 ±	0.010
+ Faster Ordering	40.05	±	0.10	4.592 ± 0.054	1.303	± 0.037	0.772 ±	0.008
+ Store Weighted Result Only 38.63 ±	0.22	1.752 ± 0.018	0.952	± 0.026	0.278 ±	0.003
2.36 ± 0.00	13.66 ± 0.12	1.74 ± 0.01	2.36 ± 0.02
186.44 ± 0.05	19.88 ± 0.10	5.56 ± 0.02	3.39 ± 0.01
595.91 ±	0.13	112.52	± 0.22	23.81 ± 0.06	19.09 ± 0.05
214.60 ±	0.10	29.38	± 0.12	7.63 ± 0.03	4.78 ± 0.02
208.22 ±	0.13	26.64	± 0.09	6.75 ± 0.03	4.38 ± 0.02
Table 9: Inference latency (mean and standard deviation) for CSR SpMM, used by GCN/GIN,
and aggregator fusion. Assuming a feature dimension of 256 and H = B = 1 per Algorithm 1.
We observe that aggregator fusion results in an increase of 34% in the worse case; in contrast,
the naive implementation has a worst case increase of 466%. Also included are timings for dense
multiplication with a square weight matrix; we observe that sparse operations dominate latency
measurements.
Model	Train Epoch Time (s)	Test Epoch Time (s)	Peak Train Memory (MB)
GCN	144.7 ± 0.5	6.3 ± 0.3	1337
GIN	134.5 ± 0.1	5.7 ± 0.3	1331
GraphSAGE	140.3 ± 0.3	5.6 ± 0.3	1226
GAT	176.0 ± 0.7	6.3 ± 0.3	2885
MPNN-Sum	305.3 ± 2.7	6.8 ± 0.3	3448
MPNN-Max	319.0 ± 1.3	7.6 ± 0.4	3901
PNA	575.4 ± 2.3	12.0 ± 0.2	9399
EGC-S	166.4 ± 2.7	6.2 ± 0.3	1842
EGC-M	225.7 ± 0.6	7.9 ± 0.3	3470
Table 10: Latency and memory results for parameter-normalized models on OGB Code-V2. Despite
|E|
having a lower 胃 ratio of 2.75 relative to Arxiv (13.67), We see that the trends We observed m for
Arxiv broadly remain. It is worth noting again that EGC-M is far more efficient both latency and
memory-Wise than PNA.
As expected, our technique optimizing for input re-use achieves significantly loWer inference latency
than the naive approach to applying multiple aggregators. While the naive approach results in a mean
increase in latency of 331%, our approach incurs a mean increase of only 14% relative to ordinary
SpMM, used by GCN and GIN. The increase is topology dependent, With larger increases in latency
being observed for topologies Which are less memory-bound. We also provide timings for dense
matrix multiplication (i.e. XΘ) to justify our focus on optimizing sparse operations in this Work:
CSR SpMM operation is 4.7× sloWer (geomean) than the corresponding Weight multiplication. We
believe further optimizations of the operations used by architecture are achievable through the use of
auto-tuning frameWorks e.g. TVM (Chen et al., 2018b), but this lies beyond the scope of this Work.
E Latency and Memory Consumption on Other Datasets
In the evaluation, We assessed the memory consumption and latency for the parameter-normalized
models on Arxiv. In this section, We consider a similar exercise for OGB Code models. The results
are provided in Table 10. Our conclusions remain broadly similar, With EGC-M offering clear
improvements to memory consumption, latency, and parameter efficiency relative to PNA. EGC-S
is superior to GAT, With similar inference latency, better model performance, and noticeably loWer
memory consumption. We train models using batch size 128; the reader should note that the memory
consumption figures can vary betWeen runs (even for the same model), since graphs vary in the
number of nodes and edges.
So far We have not demonstrated that our approach is more efficient than the baselines on Arxiv,
Which is the only dataset Where EGC-M and PNA are not the best performing. To demonstrate that
We are more efficient We must shoW that We achieve loWer memory consumption and latency for
a given accuracy-level—i.e. We must consider models that are accuracy-normalized. We evaluate
increasing the parameter count for baseline models until they achieve the same accuracy as EGC-S;
18
Published as a conference paper at ICLR 2022
Accuracy-Normalized Model	Parameters	GPU Training Latency (ms)	GPU Inference Latency (ms)	Peak Memory (MB)
GCN	184k	208.6 ± 2.2	44.9 ± 0.4	2549
GIN	FAIL	FAIL	FAIL	FAIL
GraphSAGE	593k	335.8 ± 2.4	60.2 ± 0.2	3208
EGC-S	100k	177.7 ± 2.2	37.3 ± 0.1	2430
Table 11: Latency and memory statistics for accuracy-normalized models on Arxiv. To achieve the
same accuracy as EGC-S, we must boost the size of the baseline models. We observe that EGC-S
offers noticeable reductions to both memory consumption and latency for a given accuracy level.
we also gave these models an extra advantage over our method by increasing the hyperparameter
search budget. The results are shown in Table 11, where we observe that EGC-S is more efficient
once we are comparing models achieving the same accuracy.
The reader should note that GAT (but not GATv2) can be implemented to reduce memory consump-
tion by noting that the left and right halves of the attention vector can be computed separately, and
added together as appropriate. We use an optimized implementation of GAT for our experiments
(from PyTorch Geometric); we refer the reader to the implementation for further details.
F Generalizing to Heterogeneous Graphs
Our R-EGC model is similar to the baseline R-GCN model included in the OGB repository. The
OGB model deviates from the standard definition of R-GCN (Schlichtkrull et al., 2018) since it han-
dles different node types, not just edge types. The baseline model has weights to generate messages
for each relation-type, and a weight matrix to update each individual node type. This corresponds
to:
y(i)
W + rXR ≡ JX Jx(j)
(6)
where η corresponds to the node type of node i, and R represents the set of relation types. Note that
the mean aggregator is used.
We deviate from the baseline by using a single set of basis weights. Instead, we use a different
weighting calculation layers (w(i) = Φx + b) per node and relation type.
HB	HB
y(i)= k X WihbΘbχ(j)+X w⅛ k X Wrih,b X Θbx(j)	⑺
h=1 b=1	r∈R r h=1 b=1	j∈N (i)
19