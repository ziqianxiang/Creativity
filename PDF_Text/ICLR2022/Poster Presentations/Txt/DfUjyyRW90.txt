Published as a conference paper at ICLR 2022
INFOrmation Prioritization through
EmPOWERment in Visual Model-Based RL
Homanga BharadhWaj * Mohammad Babaeizadeh	Dumitru Erhan
Carnegie Mellon University Google Research, Brain Team Google Research, Brain Team
Sergey Levine
Google Research, Brain Team
University of California Berkeley
Ab stract
Model-based reinforcement learning (RL) algorithms designed for handling com-
plex visual observations typically learn some sort of latent state representation,
either explicitly or implicitly. Standard methods of this sort do not distinguish be-
tween functionally relevant aspects of the state and irrelevant distractors, instead
aiming to represent all available information equally. We propose a modified ob-
jective for model-based RL that, in combination with mutual information maxi-
mization, allows us to learn representations and dynamics for visual model-based
RL without reconstruction in a way that explicitly prioritizes functionally relevant
factors. The key principle behind our design is to integrate a term inspired by
variational empowerment into a state-space model based on mutual information.
This term prioritizes information that is correlated with action, thus ensuring that
functionally relevant factors are captured first. Furthermore, the same empower-
ment term also promotes faster exploration during the RL process, especially for
sparse-reward tasks where the reward signal is insufficient to drive exploration in
the early stages of learning. We evaluate the approach on a suite of vision-based
robot control tasks with natural video backgrounds, and show that the proposed
prioritized information objective outperforms state-of-the-art model based RL ap-
proaches with higher sample efficiency and episodic returns.
1	Introduction
Model-based reinforcement learning (RL) provides a promising approach to accelerating skill learn-
ing: by acquiring a predictive model that represents how the world works, an agent can quickly
derive effective strategies, either by planning orby simulating synthetic experience under the model.
However, in complex environments with high-dimensional observations (e.g., images), modeling the
full observation space can present major challenges. While large neural network models have made
progress on this problem (Finn & Levine, 2017; Ha & Schmidhuber, 2018; Hafner et al., 2019a;
Watter et al., 2015; Babaeizadeh et al., 2017), effective learning in visually complex environments
necessitates some mechanism for learning representations that prioritize functionally relevant fac-
tors for the current task. This needs to be done without wasting effort and capacity on irrelevant
distractors, and without detailed reconstruction. Several recent works have proposed contrastive
objectives that maximize mutual information between observations and latent states (Hjelm et al.,
2018; Ma et al., 2020; Oord et al., 2018; Srinivas et al., 2020). While such objectives avoid recon-
struction, they still do not distinguish between relevant and irrelevant factors of variation. We thus
pose the question: can we devise non-reconstructive representation learning methods that explicitly
prioritize information that is most likely to be functionally relevant to the agent?
In this work, we derive a model-based RL algorithm from a combination of representation learning
via mutual information maximization (Poole et al., 2019) and empowerment (Mohamed & Rezende,
2015). The latter serves to drive both the representation and the policy toward exploring and repre-
senting functionally relevant factors of variation. By integrating an empowerment-based term into a
*Work done during Homanga's research internship at Google. hbharadh@cs.cmu.edu
1
Published as a conference paper at ICLR 2022
Representation learning
Figure 1: Overview of InfoPower. I(Ot ; Zt) is the contrastive learning objective for learning an encoder
to map from image O to latent Z. I(At-1; Zt|Zt-1) is the empowerment objective that prioritizes encoding
controllable representations in Z. -I (it+1; Zt+1 |Zt, At) helps learn a latent forward dynamics model so that
future Zt+k can be predicted from current Zt. I(Rt ; Zt) helps learn a reward prediction model, such that the
agent can learn a plan At, ..At+k, .. through latent rollouts. Together, this combination of terms produces a
latent state space model for MBRL that captures all necessary information at convergence, while prioritizing
the most functionally relevant factors via the empowerment term.
Ot
Planning
mutual information framework for learning state representations, we effectively prioritize informa-
tion that is most likely to have functional relevance, which mitigates distractions due to irrelevant
factors of variation in the observations. By integrating this same term into policy learning, we fur-
ther improve exploration, particularly in the early stages of learning in sparse-reward environments,
where the reward signal provides comparatively little guidance.
Our main contribution is InfoPower, a model-based RL algorithm for control from image ob-
servations that integrates empowerment into a mutual information based, non-reconstructive frame-
work for learning state space models. Our approach explicitly prioritizes information that is most
likely to be functionally relevant, which significantly improves performance in the presence of time-
correlated distractors (e.g., background videos), and also accelerates exploration in environments
when the reward signal is weak. We evaluate the proposed objectives on a suite of simulated robotic
control tasks with explicit video distractors, and demonstrate up to 20% better performance in terms
of cumulative rewards at 1M environment interactions with 30% higher sample efficiency at 100k
interactions.
2	Problem Statement and Notation
A partially observed Markov decision process (POMDP) is a tuple (S, A, T, R, O) that consists of
states s ∈ S, actions a ∈ A, rewards r ∈ R, observations o ∈ O, and a state-transition distribu-
tion T(s0|s, a). In most practical settings, the agent interacting with the environment doesn’t have
access to the actual states in S, but to some partial information in the form of observations O. The
underlying state-transition distribution T and reward distribution R are also unknown to the agent.
In this paper, we consider the observations o ∈ O to be high-dimensional images, and so the agent
should learn a compact representation space Z for the latent state-space model. The problem state-
ment is to learn effective representations from observations O when there are visual distractors
present in the scene, and plan using the learned representations to maximize the cumulative sum
of discounted rewards, J = E[Pt γt-1rt]. The value of a state V (Zt) is defined as the expected
cumulative sum of discounted rewards starting at state Zt .
We use q(∙) to denote parameterized variational approximations to learned distributions. We
denote random variables with capital letters and use lowercase letters to denote particular re-
alizations (e.g., zt denotes the value of Zt). Since the underlying distributions are unknown,
we evaluate all expectations through Monte-Carlo sampling with observed state-transition tuples
(ot,at-1,ot-1,zt,zt-1,rt).
2
Published as a conference paper at ICLR 2022
3	Information Prioritization for the Latent State-Space Model
Our goal is to learn a latent state-space model with a representation Z that prioritizes capturing
functionally relevant parts of observations O, and devise a planning objective that explores with
the learned representation. To achieve this, our key insight is integration of empowerment in the
visual model-based RL pipeline. For representation learning we maximize MI maxZ I(O, Z) sub-
ject to a prioritization of the empowerment objective maxZ I(At-1; Zt|Zt-1). For planning, we
maximize the empowerment objective along with reward-based value with respect to the policy
maxA I (At-1; Zt |Zt-1) + I(Rt; Zt).
In the subsequent sections, we elaborate on our approach, InfoPower, and describe lower bounds
to MI that yield a tractable algorithm.
3.1	Learning Controllable Factors and Planning through Empowerment
Controllable representations are features
of the observation that correspond to enti-
ties which the agent can influence through
its actions. For example, in quadrupedal
locomotion, this could include the joint
positions, velocities, motor torques, and
the configurations of any object in the en-
vironment that the robot can interact with.
For robotic manipulation, it could include
the joint actuators of the robot arm, and the
configurations of objects in the scene that
it can interact with. Such representations
are denoted by S+ in Fig. 2, which we can
formally define through conditional inde-
pendence as the smallest subspace of S,
S+ ≤ S, such that I(At-1;St|St+) =
0. This conditional independence relation
can be seen in Fig. 2. We explicitly priori-
Figure 2: PGM showing decomposition of state S into con-
trollable parts S+ (directly influenced by actions A), parts
not influenced by actions that still influence the reward, S - ,
and distractors DS- . St-+1 may be influenced by St- (arrow
+
not shown to reduce clutter) but not by St+ .
τ,

tize the learning of such representations in the latent space by drawing inspiration from variational
empowerment (Mohamed & Rezende, 2015).
The empowerment objective can be cast as maximizing a conditional information term
I(At-1; Zt|Zt-1) = H(At-1|Zt-1) - H(At-1|Zt, Zt-1). The first term H(At-1|Zt-1) encour-
ages the chosen actions to be as diverse as possible, while the second term -H(At-1 |Zt, Zt-1) en-
courages the representations Zt and Zt+1 to be such that the action At for transition is predictable.
While prior approaches have used empowerment in the model-free setting to learn policies by explo-
ration through intrinsic motivation (Mohamed & Rezende, 2015), we specifically use this objective
in combination with MI maximization for prioritizing the learning of controllable representations
from distracting images in the latent state-space model.
We include the same empowerment objective in both representation learning and policy learning.
For this, we augment the maximization of the latent value function that is standard for policy learn-
ing in visual model-based RL (Sutton, 1991), with maxA I(At-1; Zt|Zt-1). This objectives com-
plements value based-learning and further improves exploration by seeking controllable states. We
empirically analyze the benefits of this in sections 4.3 and 4.5.
In Appendix A.1 we next describe two theorems regarding learning controllable representations. We
observe that the max Pt I(At-1; Zt|Zt-1) objective for learning latent representations Z, when
used along with the planning objective, provably recovers controllable parts of the observation O,
namely S+ . This result in Theorem 1 is important because in practice, we may not be able to
represent every possible factor of variation in a complex environment. In this situation, we would
expect that when |Z|	|O|, learning Z under the objective max Pt I(At-1; Zt|Zt-1) would
encode S+.
We further show through Theorem 2 that the inverse information objective alone can be used to train
a latent-state space model and a policy through an alternating optimization algorithm that converges
to a local minimum of the objective max Pt I(At-1; Zt|Zt-1) at a rate inversely proportional to
3
Published as a conference paper at ICLR 2022
the number of iterations. In Section 4.3 we empirically show how this objective helps achieve higher
sample efficiency compared to pure value-based policy learning.
3.2	Mutual Information maximization for Representation Learning
For visual model-based RL, we
need to learn a representation space
Z , such that a forward dynamics
model defining the probability of the
next state in terms of the current
state and the current action can be
learned. The objective for this is
Pt -I(it; Zt |Zt-1, At-1). Here, it
denotes the dataset indices that de-
termine the observations p(ot|it) =
δ(ot - ot0 ). In addition to the for-
ward dynamics model, we need to
learn a reward predictor by maximiz-
ing Pt I(Rt; Zt), such that the agent
can plan ahead in the future by rolling
forward latent states, without having
to execute actions and observe re-
wards in the real environment.
Finally, we need to learn an en-
coder for encoding observations O
to latents Z . Most successful
prior works have used reconstruction-
loss as a natural objective for
learning this encoder (Babaeizadeh
et al., 2017; Hafner et al., 2019b;a).
A reconstruction-loss can be moti-
vated by considering the objective end
Algorithm 1: Information Prioritization in Visual
Model-based RL (InfoPower)
Initialize dataset D with random episodes.
Initialize model parameters φ, χ, ψ, η.
Initialize dual variable λ.
while not converged do
for update step c = 1..C do
// Model learning
Sample data {(at ,θt,rt)}k=L -D.
Compute latents Zt 〜pψ(zt∖zt-ι, at-ι,ot).
Calculate L based on section 3.4.
(φ, χ, ψ, η) - (φ, χ, ψ, η) + Vφ,χ,ψ,η L
λ 一 λ — VλL
// Behavior learning
Rollout latent plan, S ^ S ∪{zt ,a,rt}
V (zt) ≈ Eπ[lnqη(rt∖zt) +lnqψ(at-1∖zt, zt-1)]
Update policy π and value model
end
// Environment interaction
for time step t = 0..T - 1 do
Zt 〜Pφ(zt∖zt-I,at-1 ,ot)； at 〜∏(at∖zt)
rt, ot+1 - env.step(at).
end
Add data D-D∪{(ot,at,rt)T=J.
I (O, Z) and computing its BA lower
bound (Agakov, 2004). I(ot; Zt) ≥ Ep(ot,zt) [log qφ0 (ot∖Zt)] + H(p(ot)). The first term here is the
reconstruction objective, with qφ0 (ot∖Zt) being the decoder, and the second term can be ignored as
it doesn’t depend on Z. However, this reconstruction objective explicitly encourages encoding the
information from every pixel in the latent space (such that reconstructing the image is possible) and
hence is prone to not ignoring distractors.
In contrast, if we consider other lower bounds to I(O, Z), we can obtain tractable objectives
that do not involve reconstructing high-dimensional images. We can obtain an NCE-based lower
bound (HjeImetaL,2018): I(ot; Zt) ≥ 旧勺力—心血心)[log fθ(zt,ot) - log pt^t fθ(zt,oto)], where
qφ(Zt∖ot) is the learned encoder, ot is the observation at timestep t (positive sample), and all observa-
tions in the replay buffer ot0 are negative samples. fθ(Zt, ot0) = exp (ZtT WθZt0) The lower-bound is
a form of contrastive learning as it maximizes compatibility of Zt with the corresponding observation
ot while minimizing compatibility with all other observations across time and batch.
Although prior work has explored NCE-based bounds for contrastive learning in RL (Srinivas et al.,
2020), to the best of our knowledge, prior work has not used this in conjunction with empowerment
for prioritizing information in visual model-based RL. Similarly, the Nguyen-Wainwright-Jordan
(NWJ) bound (Nguyen et al., 2010), which to the best our knowledge has not been used by prior
works in visual model-based RL, can be obtained as,
I (Ot； Zt) ≥ Eq4)(zt∣θt)p(θt)[.fθ (Zt,Ot)] - e Eqφ(zt；∣θt；)p(θt)eZt t',
where fθ is a critic. There exists an optimal critic function for which the bound is tightest and
equality holds.
We refer to the InfoNCE and NWJ lower bound based objectives as contrastive learning, in order
to distinguish them from a reconstruction-loss based objective, though both are bounds on mutual
4
Published as a conference paper at ICLR 2022
information. We denote a lower bound to MI by I(ot, zt). We empirically find the NWJ-bound to
perform slightly better than the NCE-bound for our approach, explained in section 4.5.
3.3	Overall Objective
We now motivate the overall objective, which consists of maximizing mutual information while pri-
oritizing the learning of controllable representations through empowerment in a latent state-space
model. Based on the discussions in Sections 3.1 and 3.2, we define the overall objective for repre-
sentation learning as
H-1
max I(Ot; Zt) s.t.
Z0:H-1 t=0
Ct
}1
∑z	{
(-I(it; Zt |Zt-1, At-1)+ I (At-1 ; Zt|Zt-1)+ I(Rt; Zt)) ≥ c0.
t=0
H-1
The objective is to maximize a MI term I(Ot; Zt)through contrastive learning such that a constraint
on Ct holds for prioritizing the encoding of forward-predictive, reward-predictive and controllable
representations. We define the overall planning objective as
H-1
max	X	I(At-1；	Zt ∣Zt-ι) + V(Zt)	；	At	=	∏(Zt)	； V(Zt)	≈ X R
0:H-1 t=0	t
The planning objective is to learn a policy as a function of the latent state Z such that the empower-
ment term and a reward-based value term are maximized over the horizon H .
We can perform the constrained optimization for representation learning through the method of
Lagrange Multipliers, by the primal and dual updates shown in Section A.2. In order to analyze this
objective, let |O| = n and |Z | = d. Since, O corresponds to images and Z is a bottlenecked latent
representation, d n.
I(O, Z) is maximized when Z contains all the information present in O such that Z is a suffi-
cient statistic of O. However, in practice, this is not possible because |Z|	|O|. When c0 is
sufficiently large, and the constraint t Ct ≥ c0 is satisfied, Z = [S+, S-]. Hence, the objective
max Pt I(Ot, Zt) s.t. Pt Ct ≥ c0 cannot encode anything else in Z, in particular it cannot
encode distractors DS-.
To understand the importance of prioritization through Ct ≥ c0, consider max Pt I(Ot, Zt) without
the constraint. This objective would try to make Z a sufficient statistic of O, but since |Z| |O|,
there are no guarantees about which parts of O are getting encoded in Z. This is because both
distractors S- and non-distractors S+, DS- are equally important with respect to I(O, Z). Hence,
the constraint helps in prioritizing the type of information to be encoded in Z.
3.4	Practical Algorithm and Implementation Details
To arrive at a practical algorithm, we optimize the overall objective in section 3.3 through lower
bounds on each of the MI terms. For I(O, Z) we consider two variants, corresponding to the NCE
and NWJ lower bounds described in Section 3.2. We can obtain a variational lower bound on each
of the terms in Ct as follows, with detailed derivations in Appendix A.3:
-I(it; zt |at-1) ≥ -	E[DKL(p(zt∣zt-i, at-i,θt)∣∣qχ(zt∣zt-i, at-i))]
t
I(rt； Zt) ≥ Ep(ft)[logqη(r |zt)] + H(P(Tt)
I(at-1； Zt∣zt-i) ≥ Ep(ot∣zt-1,at-1)qφ(zt∣ot)[logqψ(at-i∣zt,zt-i)] + E[H(∏(at-i∣zt-i))]
Here, qφ(zt∖ot) is the observation encoder, qψ(at—jz-) is the inverse dynamics model,
qη(rt∣zt) is the reward decoder, and qχ(zt∖zt-ι, at-1)) is the forward dynamics model. The inverse
model helps in learning representations such that the chosen action is predictable from successive
latent states. The forward dynamics model helps in predicting the next latent state given the current
latent state and the current action, without having the next observation. The reward decoder predicts
the reward (a scalar) at a time-step given the corresponding latent state. We use dense networks for
all the models, with details in Appendix A.7. We denote by Ct, the lower bound to Ct based on the
sum of the terms above. We construct a Lagrangian L = Pt I(ot; Zt) + λ (Ct - c°) and optimize it
by primal-dual gradient descent. An outline of this is shown in Algorithm 1.
5
Published as a conference paper at ICLR 2022
Planning Objective. For planning to choose actions at every time-step, We learn a policy π(a∣z)
through value estimates of task reward and the empowerment objective. We learn value estimates
with V(Zt) ≈ En[lnqη(rt∣zt) + lnqψ(at-ι∣zt, zt-ι)]. We estimate V(Zt) similar to Equation 6
of (Hafner et al., 2019a). The empowerment term qψ(at-ι∣zt,zt-ι) in policy learning incentivizes
choosing actions at-1 such that they can be predicted from consecutive latent states Zt-1, Zt. This
biases the policy to explore controllable regions of the state-space. The policy is trained to maximize
the estimate of the value, while the value model is trained to fit the estimate of the value that changes
as the policy is updated.
Finally, we note that the difference in value function of the underlying MDP Qπ (o, a) and the latent
MDP Qn(z, a), where Z 〜qφ(z∣o) is bounded, under some regularity assumptions. We provide
this result in Theorem 3 of the Appendix Section A.4. The overall procedure for model learning,
planning, and interacting with the environment is outlined in Algorithm 1.
4	Experiments
Through our experiments, we aim to understand the following research questions:
1.	How does InfoPower compare with the baselines in terms of episodic returns in environments
with explicit background video distractors?
2.	How sample efficient is InfoPower when the reward signal is weak (< 100k env steps when
the learned policy typically doesn’t achieve very high rewards)?
3.	How does InfoPower compare with baselines in terms of behavioral similarity of latent states?
Please refer to the website for a summary and qualitative visualization results
https://sites.google.com/view/information-empowerment
4.1	Setup
We perform experiments with modified DeepMind Control Suite environments (Tassa et al., 2018),
with natural video distractors from ILSVRC dataset (Russakovsky et al., 2015) in the background.
The agent receives only image observations at each time-step and does not receive the ground-
truth simulator state. This is a very challenging setting, because the agents must learn to ignore
the distractors and abstract out representations necessary for control. While natural videos that are
unrelated to the task might be easy to ignore, realistic scenes might have other elements that resemble
the controllable elements, but are not actually controllable (e.g., other cars in a driving scenario). To
emulate this, we also add distractors that resemble other potentially controllable robots, as shown
for example in Fig. 3 (1st and 6th), but are not actually influenced by actions.
In addition to this setting, we also perform exper-
iments with gray-scale distractors based on videos
from the Kinetics dataset (Kay et al., 2017). This
setting is adapted exactly based on prior works (Fu
et al., 2021; Zhang et al., 2020) and we compare di-
rectly to the published results.
We compare InfoPower with state-of-the art base-
lines that also learn world models for control:
Dreamer (Hafner et al., 2019a), C-Dreamer that is a
contrastive version of Dreamer similar to Ma et al.
(2020), TIA (Fu et al., 2021) that learns explicit
reconstructions for both the distracting background
and agent separately, DBC (Zhang et al., 2020), and
DeepMDP (Gelada et al., 2019). We also compare
variants of InfoPower with different MI bounds
(NCE and NWJ), and ablations of it that remove the
empowerment objective from policy learning.
4.2	A behavioral similarity metric based
Figure 3: Some of the natural video background
distractors in our experiments. The videos change
every 50 time-steps. Some backgrounds (for e.g.
the top left and the bottom right) have more com-
plex distractors in the form of agent-behind-agent
i.e. the background has pre-recorded motion of a
similar agent that is being controlled.
ON GRAPH KERNELS
In order to measure how good the learned latent representations are at capturing the functionally
relevant elements in the scene, we introduce a behavioral similarity metric with details in A.5. Given
6
Published as a conference paper at ICLR 2022
walker walk
cheetah run
Oooo
Ooo
3 2 1
Slma mpss-
—InfoPower ----- dreamer	cd reamer ---- tia
finger spin
quadruped walk
dbc ----- deepmdp
Figure 4: Evaluation of InfoPower and baselines in a suite of DeepMind Control tasks with natural video
distractors in the background. The x-axis denotes the number of environment interactions and the y-axis shows
the episodic returns. The S.D is over 4 random seeds. Higher is better.
the sets Sz = {zi}in=1 and Szgt = {zgit}in=1, we construct complete graphs by connecting every vertex
with every other vertex through an edge. The weight of an edge is the Euclidean distance between
the respective pairs of vertices. The label of each vertex is an unique integer and corresponding
vertices in the two graphs are labelled similarly.
Let the resulting graphs be denoted as Gz = (Vz , Ez) and Gzgt = (Vzgt , Ezgt) respectively. Note that
both these graphs are shortest path graphs, by construction. Let, ei = {ui , vi } and ej = {uj , vj }.
We define a kernel k that measures similarity between the edges ei , ej and the labels on respective
vertices. k(ei, ej ) =
kv(l(ui), l(uj))ke(l(ei), l(ej))kv(l(vi), l(vj)) + kv(l(ui), l(vj))ke(l(ei), l(ej))kv(l(vi), l(uj))
Here, the function l(∙) denotes the labels of vertices and the weights of edges. k is a Dirac delta
kernel i.e. ke(x, y) = 1 - δ(x, y) (Note that δ(x, y) = 1 iff x = y, else δ(x, y) = 0) and ke
is a BroWnian ridge kernel, ke(x, y) = C max(0, C - |x - y|), where C is a large number. We
define the shortest path kernel to measure similarity between the two graphs, as, k(Gz, Gzgt) =
∣⅛7 Hei∈ez ∑ej ∈ez k" ej). The value of k(Gz, Gzgt) is low when a large number of pairs
of corresponding vertices in both the graphs have the same edge length. We expect methods that
recover latent state representations which better reflect the true underlying simulator state (i.e., the
positions of the joints) to have higher values according to this metric.
4.3	Sample efficiency analysis
In Fig. 4, we compare InfoPower and the baselines in terms of episodic returns. This version
of InfoPower corresponds to an NWJ bound on MI which we find works slightly better than the
NCE bound variant analyzed in section 4.5. It is evident that InfoPower achieves higher returns
before 1M steps of training quickly compared to the baselines, indicating higher sample efficiency.
This suggests the effectiveness of the empowerment model in helping capture controllable represen-
tations early on during training, when the agent doesn’t take on actions that yield very high rewards.
4.4	Behavioral similarity of latent states
In this section, we analyze how similar are the learned latent representations with respect to the
underlying simulator states. The intuition for this comparison is that the proprioceptive features
in the simulator state abstract out distractors in the image, and so we want the latent states to be
behaviorally similar to the simulator states.
Quantitative results with the defined metric. In Table 1, we show results for behavioral simi-
larity of latent states (Sim), based on the metric in section 4.2. We see that the value of Sim for
InfoPower is around 20% higher than the most competitive baseline, indicating high behavioral
similarity of the latent states with respect to the corresponding ground-truth simulator states.
Qualitative visualizations with t-sne. Fig. 5 shows a t-SNE plot of latent states Z 〜qφ(z∖o) for
InfoPower and the baselines with visualizations of 3 nearest neighbors for two randomly chosen
latent states. We see that the state of the agent is similar is each group for InfoPower, although the
7
Published as a conference paper at ICLR 2022
Figure 5: t-SNE plot of latent states Z 〜qφ(z∣o) With visualizations of three nearest neighbors for two ran-
domly sampled points (in red frame). We see that the state of the agent is similar is each set for InfoPower,
whereas for Dreamer, and the most competitive baseline C-Dreamer, the nearest neighbor frames have signifi-
cantly different agent configurations.
0.5	1.0	1.5	2.0
Environment Step lee
0.0	0.5	1.0	1.5	2.0
Environment Step le6
InfoPower NWj - Policy
InfoPower NCE - Policy
Figure 6: Evaluation of InfoPower and ablated variants in a suite of DeepMind Control tasks with natural
video distractors in the background. The x-axis denotes the number of environment interactions and the y-
axis shows the episodic returns. InfoPower-NWJ and InfoPower-NCE are full versions of our method
differing only in the MI lower bound. The versions with - Policy do not include the empowerment objective in
policy learning, but only use it from representation learning. The S.D is over 4 random seeds. Higher is better.
background scenes are significantly different. However, for the baselines, the nearest neighbor states
are significantly different in terms of the pose of the agent, indicating that the latent representations
encode significant background information.
4.5	Ablation Studies
In Fig. 6, we compare different ablations of InfoPower. Keeping everything else the same, and
changing only the MI lower bound to NCE, we see that the performance is almost similar or slightly
worse. However, when we remove the empowerment objective from policy optimization (the ver-
sions with ‘-Policy’ in the plot), we see that performance drops. The drop is significant in the regions
< 200k environment interactions, particularly in the sparse reward environments - cartpole balance
and ball in a cup, indicating the necessity of the empowerment objective in exploration for learning
controllable representations, when the reward signal is weak.
8
Published as a conference paper at ICLR 2022
5	Related Works
Visual model-based RL. Recent developments in video prediction and contrastive learning have
enabled learning of world-models from images (Watter et al., 2015; Babaeizadeh et al., 2017; Finn
& Levine, 2017; Hafner et al., 2019a; Ha & Schmidhuber, 2018; Hafner et al., 2019b; Xie et al.,
2020). All of these approaches learn latent representations through reconstruction objectives that
are amenable for planning. Other approaches have used similar reconstruction based objectives for
control, but not for MBRL (Lee et al., 2019; Gregor et al., 2019).
MI for representation learning. Mutual Information measures the dependence between two ran-
dom variables. The task of learning latent representations Z from images O for downstream appli-
cations, has been very successful with MI objectives of the form maxf1,f2 I(f1 (O), f2(Z)) (Hjelm
& Bachman, 2020; Tian et al., 2020; Oord et al., 2018; Tschannen et al., 2019; Nachum & Yang,
2021). Since calculating MI exactly is intractable optimizing MI based objectives, it is important
to construct appropriate MI estimators that lower-bound the true MI objective (Hjelm et al., 2018;
Nguyen et al., 2010; Belghazi et al., 2018; Agakov, 2004). The choice of the estimator is crucial,
as shown by recent works (Poole et al., 2019), and different estimators yield very different behav-
iors of the algorithm. We incorporate MI maximization through the NCE (Hjelm et al., 2018) and
NWJ (Nguyen et al., 2010) lower bounds, such that typical reconstruction objectives for representa-
tion learning which do not perform well with visual distractors, can be avoided.
Inverse models and empowerment. Prior approaches have used inverse dynamics models for reg-
ularization in representation learning (Agrawal et al., 2016; Zhang et al., 2018) and as bonuses
for improving policy gradient updates in RL (Shelhamer et al., 2016; Pathak et al., 2017). The
importance of information theoretic approaches for learning representations that maximize predic-
tive power has been discussed in prior work (Still, 2009) and more recently in Lee et al. (2020).
Empowerment (Mohamed & Rezende, 2015) has been used as exploration bonuses for policy learn-
ing (Leibfried et al., 2019; Klyubin et al., 2008), and for learning skills in RL (Gregor et al., 2016;
Sharma et al., 2019; Eysenbach et al., 2018). In contrast to prior work, we incorporate empower-
ment both for state space representation learning and policy learning, in a visual model-based RL
framework, with the aim of prioritizing the most functionally relevant information in the scene.
RL with environment distractors. Some recent RL frameworks have studied the problem of ab-
stracting out only the task relevant information from the environment when there are explicit dis-
tractors (Hansen & Wang, 2020; Zhang et al., 2020; Fu et al., 2021; Ma et al., 2020). Zhang et al.
(2020) constrain the latent states by enforcing a bisimulation metric, without a reconstruction ob-
jective. Our approach is different from this primarily because of the empowerment objective that
provides useful signal for dealiasing controllable vs. uncontrollable representations independent of
how strong is the observed reward signal. Fu et al. (2021) model both the relevant and irrelevant
aspects of the environment separately, and differ from our approach that prioritizes learning only the
relevant aspects. (Shu et al., 2020) was and earlier approach that used contrastive representations
for control. More recently, (Nguyen et al., 2021) used temporal predictive coding and (Ma et al.,
2020) used InfoNCE based contrastive loss for learning representations while ignoring distractors in
visual MBRL. Incorporating data augmentations for improving robustness with respect to environ-
ment variations is an orthogonal line of work (Laskin et al., 2020; Hansen & Wang, 2020; Raileanu
et al., 2020; Srinivas et al., 2020; Kostrikov et al., 2020), complementary to our approach.
6	Conclusion
In this paper we derived an approach for visual model-based RL such that an agent can learn a latent
state-space model and a policy by explicitly prioritizing the encoding of functionally relevant factors.
Our prioritized information objective integrates a term inspired by variational empowerment into a
non-reconstructive objective for learning state space models. We evaluate our approach on a suite
of vision-based robot control tasks with two sets of challenging video distractor backgrounds. In
comparison to state-of-the-art visual model-based RL methods, we observe higher sample efficiency
and episodic returns across different environments and distractor settings.
9
Published as a conference paper at ICLR 2022
References
David Barber Felix Agakov. The im algorithm: a variational approach to information maximization.
Advances in neural information processing systems, 16(320):201, 2004.
Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke
by poking: Experiential learning of intuitive physics. arXiv preprint arXiv:1606.07419, 2016.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.
Stochastic variational video prediction. arXiv preprint arXiv:1710.11252, 2017.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning,pp. 531-540. PMLR, 2018.
Richard Blahut. Computation of channel capacity and rate-distortion functions. IEEE transactions
on Information Theory, 18(4):460-473, 1972.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory 2nd Edition (Wiley Series in
Telecommunications and Signal Processing). Wiley-Interscience, July 2006. ISBN 0471241954.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE
International Conference on Robotics and Automation (ICRA), pp. 2786-2793. IEEE, 2017.
Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola. Learning task informed abstractions. In
International Conference on Machine Learning, pp. 3480-3491. PMLR, 2021.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170-2179. PMLR, 2019.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron
van den Oord. Shaping belief states with generative environment models for rl. arXiv preprint
arXiv:1906.09237, 2019.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565. PMLR, 2019b.
Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmen-
tation. arXiv preprint arXiv:2011.13389, 2020.
R Devon Hjelm and Philip Bachman. Representation learning with video deep infomax. arXiv
preprint arXiv:2007.13278, 2020.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action
video dataset. arXiv preprint arXiv:1705.06950, 2017.
10
Published as a conference paper at ICLR 2022
Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Keep your options open: an
information-based driving principle for sensorimotor systems. PloS one, 3(12):e4018, 2008.
Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.
Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953,
2019.
Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio
Guadarrama. Predictive information accelerates learning in rl. arXiv preprint arXiv:2007.12401,
2020.
Felix Leibfried, Sergio Pascual-Diaz, and Jordi Grau-Moya. A unified bellman optimality principle
combining reward maximization and empowerment. arXiv preprint arXiv:1907.12392, 2019.
Xiao Ma, Siwei Chen, David Hsu, and Wee Sun Lee. Contrastive variational model-based reinforce-
ment learning for complex observations. arXiv e-prints, pp. arXiv-2008, 2020.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-
cally motivated reinforcement learning. arXiv preprint arXiv:1509.08731, 2015.
Ofir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive
fourier features. arXiv preprint arXiv:2105.12272, 2021.
Kenji Nakagawa, Yoshinori Takei, Shin-ichiro Hara, and Kohei Watabe. Analysis of the convergence
speed of the arimoto-blahut algorithm by the second-order recurrence formula. IEEE Transactions
on Information Theory, 2021.
Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for
model-based planning in latent space. arXiv preprint arXiv:2106.07156, 2021.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International conference on machine learning, pp. 2778-2787.
PMLR, 2017.
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, pp. 5171-
5180. PMLR, 2019.
Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic
data augmentation for generalization in reinforcement learning. 2020.
Kate Rakelly, Abhishek Gupta, Carlos Florensa, and Sergey Levine. Which mutual-information
representation learning objectives are sufficient for control? arXiv preprint arXiv:2106.07278,
2021.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019.
11
Published as a conference paper at ICLR 2022
Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-
supervision for reinforcement learning. arXiv preprint arXiv:1612.07307, 2016.
Rui Shu, Tung Nguyen, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano
Ermon, and Hung Bui. Predictive coding for locally-linear control. In International Conference
on Machine Learning,pp. 8862-8871. PMLR, 2020.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
Susanne Still. Information-theoretic approach to interactive learning. EPL (Europhysics Letters),
85(2):28005, 2009.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin, 2(4):160-163, 1991.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer
Vision-ECCV2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
Part XI 16, pp. 776-794. Springer, 2020.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to
control: A locally linear latent dynamics model for control from raw images. arXiv preprint
arXiv:1506.07365, 2015.
Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, and Florian Shkurti. Latent skill
planning for exploration and transfer. In International Conference on Learning Representations,
2020.
Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning.
arXiv preprint arXiv:1804.10689, 2018.
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. arXiv preprint
arXiv:2006.10742, 2020.
12
Published as a conference paper at ICLR 2022
A Appendix
A. 1 Theoretical results on controllable representations
Let Φ(∙) denote the encoder such that Z = Φ(O). O is the observation seen by the agent and S is
the underlying state. S+ is defined as that part of underlying state S which is directly influenced by
actions A i.e. S+ ≤ S s.t. I(At-1; St|St+) = 0.
We next describe two theorems regarding learning controllable representations, with proofs in the
Appendix. We observe that the max Pt I(At-1; Zt|Zt-1) objective alone for learning latent repre-
sentations Z, along with the planning objective provably recovers controllable parts of the observa-
tion O, namely S+ .
Theorem 1.	The objective max t I (At-1; Zt|Zt-1) provably recovers controllable parts S+ of
the observation O. S+ is defined as that part of underlying state S which is directly influenced by
actions A i.e. S+ ⊂ S s.t. I(St; At-1|St+) = 0.
Proof. Based on Fig. 2 and Fig. 7, we have the following
max I (At-1; Zt) ≤ max I (At-1; Ot) Data-Processing Inequality
≤ I (At-1 ; St) Data-Processing Inequality
≤ I(At-1;St,St+)
≤ I (At-1 ; St |St+) + I (At-1; St+) Chain Rule of MI
≤ I (At-1 ; St+) Conditional independence
So, the maximum above can be obtained when the encoder Φ is an identity function over S+ and
a zero function over [S-, S-]. So, Zt = Sj. Hence, given Zt-ι, I(At-ι; ZtIZt-1) is maximized
when Φ is an identity function over S+. So, max Pt I(At-1; Zt|Zt-1) learns the encoder Φ such
that controllable representations are recovered.
□
This result is important because in practice, we may not be able to represent every possible factor
of variation in a complex environment. In this situation, we would expect that when IZ I	IOI,
learning Z under the objective max Pt I(At-1; ZtIZt-1) would encode S+. We next show that
the inverse information objective alone can be used to train a latent-state space model and a policy
through an alternating optimization algorithm that converges to a local minimum of the objective
max Pt I(At-1; ZtIZt-1) at a rate inversely proportional to the number of iterations.
Theorem 2.	max∏,ψ Pt 1 (At-I; ZtIZt-I) = Pt Eπ(at-ι∣zt-ι)p(zt∣zt-ι,at-ι,ot) log “雪；」1：：；-； )1，
can be optimized through an alternating optimization scheme that has a convergence rate of O(1/N)
to a local minima of the objective, where N is the number of iterations.
Proof. Let Z 〜qφ(z∖o) denote a latent state sampled from the encoder distribution. Learning a
world model such that reward prediction and inverse information are maximized can be summarized
as:
max ^X E∏(at ∣zt),qφ(zt∣θt) Ep(rt∣zt,at) log qη (rt|zt, at) + Ep(zt+1 |zt,at) log	∏(∕古[古)
The policy π(atIzt) is learned by
max X EnStIzt),qφ(ZtIot)
t
Model-based RL in this case involves alternating optimization between policy learning and learning
the world model. In the case where there is no reward signal from the environment, i.e. p(rtIzt), we
have the following objective
Ep(rt∣zt,at) log qη (rt|zt, at) + Ep(zt+1 ∣Zt,at) log "∏() ∣])
mπax mφ,aψx X Eπ(at Izt),qφ(zt Ist)p(zt+1 Izt,at) log
qψ (at∣Zt+1,Zt)
∏(at ∣Zt)
13
Published as a conference paper at ICLR 2022
Figure 7: PGM of the MDP with distractor states. The state observed by the agent O consists of three parts
S+, S- and DS-. S+ is the controllable part of the state i.e. it is affected by the actions of the agent, and in
turn affects the reward R; S- is not controllable by the agent but affects the reward R and future S+; DS- is
not controllable by the agent and doesn’t affect the reward R and future S+.
The optimal qψ give a fixed π and φ can be derived as in page 335 of Cover & Thomas (2006).
qψ (at |zt+i ,zt) =
qφ (Zt |st )p(zt+ι∣zt ,at )∏(at∣zt)
Ra qφ (Zt |st )p(zt+ι∣zt, at )∏(at∣zt)
The optimal π* given a fixed ψ and φ can be derived as
π* (a®)=	eχp (Eqe3 |st )p(zt + ι 包一)[log qψ (at|zt+1,zt 用
t t	Ra exP(Eqφ (zt ∣st )p(zt + ι |zt ,at )q [log qψ (at|zt+1,zt)])
We note that these alternating iterations correspond to an instantiation of the Blahut-Arimoto al-
gorithm (Blahut, 1972) for determining the information theoretic capacity of a channel. Based
on (Nakagawa et al., 2021) we see that this iterative optimization procedure between qψ* and π*
converges, and the worst-case rate of convergence is O(1/N) where N is the number of iterations.
□
This result is useful because it shows that even in the absence of rewards from the environment
(rt = 0 ∀t), when planning to minimize regret is not possible, the inverse information objective can
be used to train a policy that explores to seek out controllable parts of the state-space. When rewards
from the environment are present, we can train the policy with this objective and the value estimates
obtained from the cumulative rewards during planning. In Section 4.3 we empirically show how this
objective helps achieve higher sample efficiency compared to pure value-based policy learning.
A.2 Primal Dual updates
∂ PtH=-01 I(Ot; Zt) + λ (-I(it; Zt|Zt-1, At-1) + I(At-1; Zt|Zt-1) + I(Rt; Zt) - c0)
Z0:H-1 = Z0:H-1 +--------------------------------"---------------------------------------
dZo：H-1
14
Published as a conference paper at ICLR 2022
λ=λ-
-I(it; Zt∖Zt-1 ,At-1) + I(At-I; ZtlZt-I) + I(Rt; Zt) -c0
、--------V---------} 、------V-------}	、—V—}
fwd dynamics model	inv dynamics model rew model
A.3 Forward, Inverse, and Reward Models
-I(it; Zt|Zt-1, At-1) = -
=-Z
≥-Z
p(zt∖zt-1, at-1, ot)
P(zt, Zt-I,at-1, it,0t)log —7~~,-----------
p(Zt∖Zt-1, at-1)
p(zt, zt-1, at-1, ot)
p(zt, zt-1, at-1, ot) log
g p(zt∖zt-i,at-i,θt) + log qχ(zt∖zt-i,at-i)
g qχ(zt∖zt-i,at-i)	g p(zt∖zt-i,at-l)
p(zt∖zt-i,at-i,θt)
qχ(zt∖zt-i,at-i)
=-DKL(P(zt∣zt-i,at-i, θt)∣∣qχ(zt∣zt-i, at-i))
We can obtain a variational lower bound on the empowerment
I(at; zt+k |zt) =
=Z
≥Z
=Z
p(at, zt+k, zt) log
p(at, zt+k, zt)
p(at|zt+k, zt)
p(at|zt)
q(at|zt+k, zt)
p(at, zt+k, zt) log
g p(at|zt)
q(at|zt+k,zt)
p(at|zt)
+ log P(at\zt+k, Zt)
m q(at∖zt+k,Zt)
p(at, Zt+k, Zt) log q(at ∖Zt+k, Zt) -	p(Zt)p(Zt+k∖at, Zt)p(at∖Zt) logp(at∖Zt)
p(at ,zt+k ,zt)[log q(at∖zt+k ,zt)] + Ep(Zt)p(zt+k∣at ,Zt)[H(p(at∖zt))]
Here, q(at∖Zt+k, Zt) is the inverse dynamics model, and H(p(at∖Zt)) is the entropy of the policy.
We can obtain a variational lower bound on the reward model as
I(rt;zt)=
=Z
=Z
≥Z
=Z
p(rt, zt) log
p(rt, zt)
p(rt |zt)
p(rt)
q(rt|zt)
p(rt, zt) log
p(rt, zt) log
p(rt)
q(rt∖zt)
p(rt)
q(rt|Zt)
p(rt)
p(rt∖Zt)
+ log M
+ KL[p(rt,zt)||q(rt,zt)]
p(rt,zt) log q(rt|zt) -	p(rt, zt) logp(rt)
= Ep(rt,zt) [log q(rt∖Zt)] + Constant
Here, q(rt∖Zt) is the reward decoder. Since the reward is a scalar, reconstructing it is computation-
ally simpler compared to reconstructing high dimensional observations as in the BA bound for the
observation model.
A.4 Value Difference Result
We show that the difference in value function of the underlying MDP Qπ(o, a) and the latent MDP
Qn (z, a), where Z 〜qφ(z∣o) is bounded, under some regularity assumptions. SimilartotheassUmP-
tion in (Gelada et al., 2019), let the policy π have bounded semi-norm value functions under the total
variation distance DTV i.e. |Vπ(z)∣Dtv ≤ K and ∣Ezι〜PV(zι) - Ez?〜QV(z2)∣ ≤ KDTV(P,Q).
Define the forward dynamics learning objective as minχ LT = minχ Dkl(p(o0∣o, a)∣∣qχ(z0∣z, a))
and the reward model objective as mi% LR = mi% Dkl (p(r∖o)∖∖qη (r∣z))
15
Published as a conference paper at ICLR 2022
Theorem 3.	The difference between the Q-function in the latent MDP and that in the original MDP
is bounded by the loss in reward predictor and forward dynamics model.
ππ
E(o,a)~dπ (o,a),z~qφ (z| o)[Q (O, a) ― Q (Z, a)] ≤
√LR + YK √LT
1-Y
Proof.
ππ
E(o,a)~dπ (o,a),z~qφ(z∣o)[Q (o, a)	Q (Z, a)]
≤ E(o,a)~dπ (o,a),z~qφ(z∖o) ( |r (o, a) ― Ir(Z, a) 1 + γlEo0~p(o0∣o,a)V(o ) ― Ez0~qχ(z01z,a)V(Z ) | )
≤ JDKL(P(r。)1。(r|Z)) + E(o,a)~dπ (o,a),z~qφ(z∣o) (YlEo0~p(o0∣o,a),z0~qφ(z0∣o0)[V (o/)- V(Z0)]∣
+ γlEo0~p(o0∣o,a),z0~qχ(z0∣z,a) [V(o ) - V(Z )]|)
≤ λ/LR + E(o,a)~dπ(o,a),z~qφ(z∣o)(YIEo0~p(o0∣o,a),z0~qφ(z0∣o0) [V(o0) - V(Z0川)
+ E(o,a)~dπ (o,a),z~qφ (z| o) (YKDTV(P(O |o, a)||qX(Z |Z, a)))
≤ LRjR + YE(O,a)~dπ (o,a),z~qφ (z| o)Eo0~p(o01 o,a),z0~qφ (z01 o0) [V(O) - V(Z )])
+ YKJDKL(P(01o, a)||qx(ZlZ,a))
≤ a∕rR + YE(o,a)~d∏(o,α),z~qφ(z∣o)[V(O)- V(Z)])+ YKAZLT
≤ λ/LR + YE(O,a)~dπ (o,a),z~qφ(z∣o)[Q(o, a) - Q(Z, a)]) + YKʌ/LT
So, we have shown that
ππ
E(o,a)~dπ (o,a),z~qφ(z∣o)[Q (o, a)	Q (Z,a)] ≤
√LR + YK √LT
1-Y
□
We can see that, as Lr, Lt → 0, the Q-function in the representation space of the MDP, Q, becomes
increasingly closer to that of the original MDP, Q. Since this result holds for all Q-functions, it also
holds for the optimal Q* and Q*. This result extends sufficiency results in prior works Rakelly et al.
(2021) to a setting with stochastic encoders and KL-divergence losses on forward dynamics and
reward, as opposed to Wasserstein metrics (Gelada et al., 2019), and bisimulation metrics (Zhang
et al., 2020).
A.5 B ehavioral Similarity metric details
Given the sets Sz = {Zi}in=1 and Szgt = {Zgit}in=1, we construct complete graphs by connecting
every vertex with every other vertex through an edge. The weight of an edge is the Euclidean
distance between the respective pairs of vertices. The label of each vertex is an unique integer and
corresponding vertices in the two graphs are labelled similarly. Let the resulting graphs be denoted
as Gz = (Vz, Ez) and Gzgt = (Vzgt, Ezgt) respectively. Note that both these graphs are shortest path
graphs, by construction.
Since the Euclidean distances in both the graphs cannot be directly compared, we scale the distances
such that the shortest edge weight among all edge weights are equal in both the graphs. Scaling
every edge weight by the same number doesn’t change the structure of the graph. Now, we define
the shortest path kernel to measure similarity between the two graphs, as follows
k(Gz,GZgt) = iE1-∣	X X k(ei,ej)
z ei∈Ez ej∈Ezgt
ɪ	r	ι i	r	ι ɪ τ ,ι i i P	∙	∙i ∙, i	.i F
Let, ei = {ui , vi} and ej = {uj , vj }. Here, the kernel k measures similarity between the edges
ei, ej and the labels on respective vertices. Let the function l(∙) denote the labels of vertices and the
weights of edges.
f/	∖ 7 /7/ ∖ 7/ ∖∖ 1 /7/ ∖ 7/ ∖∖ 1 /7/ ∖ 7/ ∖ ∖ . 7 /7/ ∖ 7/ ∖∖ 1 /7/ ∖ 7/ ∖∖ 1 /7/ ∖ 7/ ∖∖
k(ei , ej ) = kv (l(ui), l(uj ))ke (l(ei ), l(ej ))kv (l(vi ), l(vj ))+kv (l(ui ), l(vj ))ke (l(ei ), l(ej ))kv (l(vi ), l(uj ))
16
Published as a conference paper at ICLR 2022
Here, kv is a Dirac delta kernel i.e. ke(x, y) = 1 - δ(x, y) (Note that δ(x, y) = 1 iff x = y, else
δ(x, y) = 0) and ke is a BroWnian ridge kernel, ke(x, y) = C max(0, C - |x - y|), where C is a large
number.
So, in summary, the value of k(ei, ej ) is low when the edge lengths between corresponding pairs of
vertices in both the graphs are close. Hence, the value of k(Gz, Gzgt) is low when a large number of
pairs of corresponding vertices in both the graphs have the same edge length.
We expect methods that recover latent state representations which better reflect the true underlying
simulator state (i.e., the positions of the joints) to have higher values according to this metric.
A.6 Description of the distractors
ILSVRC dataset (Russakovsky et al., 2015) The distractors used for the main experiments are
natural video backgrounds with RGB images from the ILSVRC dataset. We use 200 videos during
training, and reserve 50 videos for testing. An illustration of this for different environments are
shown in Fig. 8. These images are snapshots of what is received by the algorithm (64x64x3 images)
and no information about proprioceptive features is provided. While natural videos that are unrelated
to the task might be easy to ignore, realistic scenes might have other elements that resemble the
controllable elements, but are not actually controllable (e.g., other cars in a driving scenario).
To emulate this, we also add distractors that resemble other potentially controllable robots, as shown
for example in Fig. 8 (top left and bottom right frames), but are not actually influenced by the
actions of the algorithm. These are particularly challenging because the background video has a pre-
recorded motion of the same (or similar) agent that is being controlled. We include such challenging
agent-behind-agent background videos with a frequency of 1 in very 3 videos.
For the experiments in Table 1, examples of different levels of distractors are shown in Fig. 9. The
three different levels have distractor windows of size 32x32, 40x40, and 64x64 respectively.
Kinetics dataset (Kay et al., 2017) For comparing directly with results in prior works DBC (Zhang
et al., 2020), and TIA (Fu et al., 2021), we evaluate on the setting where the random videos are
grayscale images from the Kinetics dataset driving car class. The settings are same as in the prior
works and we compare directly to the results in the respective papers.
A.7 Training and Network details
The agent observations have shape 64 x 64 x 3, action dimensions range from 1 to 12, rewards per
time-step are scalars between 0 and 1. All episodes have randomized initial states, and the initial
distractor video background is chosen randomly.
We implement our approach with TensorFlow 2 and use a single Nvidia V100 GPU and 10 CPU
cores for each training run. The training time for 1e6 environment steps is 4 hours on average. In
comparison, the average training time for 1e6 steps for Dreamer is 3 hours, for CDreamer is 4 hours,
for TIA is 4 hours, for DBC is 4.5 hours, and for DeepMDP is 5 hours.
The encoder consists of 4 convolutional layers with kernel size 4 and channel numbers 32, 65, 128,
256. The forward model consists of deterministic and stochastic parts, as in standard in visual model-
based RL (Hafner et al., 2019b;a). The stochastic states have size 30 and deterministic state has size
200. The compatibility function fθ(zt, ot0) = exp (ztT Wθ zt0) for contrastive learning is learned
with a 200 x 200 Wθ matrix. Here, zt0 is the encoding of ot0. All other models are implemented as
three dense layers of size 300 with ELU activations.
We use ADAM optimizer, with learning rate of 6e-4 for the latent-state space model, and 8e-5 for
the value function and policy optimization. The hyper-parameter C0 for the prioritized information
constraint is set to 1000, after doing a grid-search over the range [100, 10000] and observing similar
performance for 1000 and 10000. 100 training steps are followed by 1 episode of interacting with the
environment with the currently trained policy, for observing real transitions. The dataset is initialized
with 7 episodes collected with random actions in the environment. We kept the hyper-parameters of
InfoPower same across all environments and distractor types.
For fair comparisons, we kept all the common hyper-parameter values same as Dreamer (Hafner
et al., 2019a). This was the protocol followed in prior works TIA (Fu et al., 2021) and Ma et al.
17
Published as a conference paper at ICLR 2022
Figure 8: Illustration of the natural video background distractors used in our experiments. The videos change
after every 50 time-steps. Some video backgrounds (for example the top left and the bottom right) have more
complex distractors in the form of agent-behind-agent i.e. an agent of similar morphology moves in the back-
ground of the agent being controlled.
Figure 9: Illustration of the different levels of distractors. L1, L2, and L3 respectively have distractor windows
of size 32x32, 40x40, and 64x64.
(2020). For the baseline TIA, we tuned the environment-specific parameters λRadv and λOs as
mentioned in Table 2 of (Fu et al., 2021). For λRadv, we performed a gridsearch over the range
10k - 50k and for λOs we performed a gridsearch over the range 1 - 3 to obtain the parame-
ters for best performance, which we used for the plots in Fig. 4 and Fig. 11. Respectively for
walker-walk, cheetah-run, finger-spin, quadruped-walk, hopper-stand, ball-in-a-cup-catch, cartpole-
balance-sparse, quadruped-run, the values of λRadv are 30k,30k,20k,40k,30k,40k,20k,30k. The val-
ues of λOs are 2,2,1.5,2.5,2.5,2,2,2.
For baseline DBC (Zhang et al., 2020), we kept all the parameters same as in Table 2 of the paper,
because DBC does not have any environment-specific parameters and the same values were used for
all environments in the DBC paper. The DeepMDP agent and its hyperparameters are adapted from
the implementation provided in the github repo of DBC (Zhang et al., 2020).
A.8 Contrastive Learning
Since the distracting backgrounds are changing videos of a certain duration, for contrastive learning,
we contrast both across time and across batches. Contrasting across time helps model invariance
against temporally contiguous distractors (for example in the same video) while contrasting across
batches helps in learning invariance across different videos. Concretely, we sample batches from the
dataset (i){(at, ot, rt)}tH=1, where i denotes a batch index, and for each observation ot in batch i, all
observations ot0 both in batch i and in other batches j 6= i are considered negative samples.
A.9 Result on varying distractor levels
We consider different levels of distractors by varying the size of the window where distractors in
the background are active. Fig. 9 in the Appendix illustrates this visually. Table 1 shows results for
18
Published as a conference paper at ICLR 2022
Table 1: DM Control Walker Stand with natural video distractors at different levels. We tabulate the rewards
(Rew) and behavioral similarity (Sim) at 500k and 1M steps of training. Higher is better for both Rew and Sim.
Name	Levels	Rew@500k	ReW@1M	Sim@500k	Sim@1M
Dreamer	L1	197±31	240 ± 27	0.73±0.02	0.74±0.01
DBC	L1	261 ± 25	390 ± 34	0.75±0.03	0.74±0.02
C-Dreamer	L1	291 ± 34	590 ± 26	0.73±0.02	0.79±0.01
DeepMDP	L1	263 ± 31	340 ± 22	0.74±0.02	0.75±0.03
InfoPower	L1	397 ± 22	650 ± 100	0.82 ± 0.01	0.84 ± 0.03
Dreamer	L2	180 ± 36	197 ± 20	0.56±0.03	0.59±0.02
DBC	L2	221 ± 28	320 ± 33	0.65±0.02	0.66±0.02
C-Dreamer	L2	282 ± 30	550 ± 66	0.63±0.01	0.71±0.02
DeepMDP	L2	213±34	300 ± 25	0.59±0.02	0.61±0.04
InfoPower	L2	394 ± 30	644 ± 101	0.77 ± 0.03	0.77 ± 0.03
Dreamer	L3	140 ± 52	157 ± 10	0.32±0.02	0.33±0.03
DBC	L3	165 ± 20	221 ± 24	0.45±0.01	0.48±0.01
C-Dreamer	L3	231 ± 39	485 ± 86	0.58±0.02	0.64±0.01
DeepMDP	L3	153 ± 23	212±28	0.44±0.02	0.49±0.01
InfoPower	L3	389 ± 20	624 ± 80	0.71 ± 0.01	0.74 ± 0.03
Table 2: Comparison of baselines TIA and DBC with InfoPower on environments with video distractors
from the Kinetics dataset Driving Car class. The results for TIA and DBC are from the respective papers.
TIA			DBC		InfoPower	
	500k	800k	500k	800k	500k	800k
Walker-Run	389±45	602±40	165±92	210±32	417±22	630±33
Cheetah-Run	384±149	579±172	261±64	277±81	425±97	572±158
Hopper-Stand	338±221	581±214	110±92	207±120	395±140	615±176
Ball-in-a-Cup	201±197	115±110	—	—	255±104	263±116
Walker-Walk	878±37	948±28	545±57	630±68	925±18	972±26
Finger-Spin	371±96	487±107	801±10	832±4	795±34	787±58
Hopper-Hop	47±44	72±68	5±12	0±0	85±26	77±74
the different approaches on varying distractor levels. The size of the frame for distractors at levels
L1, L2, and L3 respectively are 32x32, 40x40, and 64x64. We observe that the baselines perform
worse as the distractor window size increases. While, for InfoPower, the performance decrease
with increasing level is minimal. This indicates the effectiveness of InfoPower in filtering out
background distractors from the observations while learning latent representations. Note that the
distractors in Figs. 4 and 6 are of level 3 i.e. same size as the observations 64x64.
A.10 Results on distractors from the Kinetics dataset
In this section, we evaluate InfoPower on the same distractors used in prior works, DBC and
TIA. The distractor videos are from the Kinetics dataset (Kay et al., 2017) Driving Car class, and
converted to grayscale. From Table 2 we see that InfoPower slightly outperforms the baselines.
We believe the performance gap is not as significant as Fig. 4 because the distractors being grayscale
images, and not RGB might be easier to ignore by the baselines as well as InfoPower.
A.11 Setting with only shifted agent behind agent distractors
In this section we evaluate InfoPower with the base-
lines only for the challenging setting of agent-behind-
agent distrators where the background contains the same
agent being controlled (a walker or a cheetah). An il-
lustration of this is shown in Fig. 10. From the results
in Table 3, we see that InfoPower significantly out-
19
Figure 10: Ilustration of the distractor set-
ting for results in Table 3.
Published as a conference paper at ICLR 2022
performs the competitive baselines both at 500k and 1M
environment interactions. This suggests the utility of
InfoPower in separating out informative elements of
the observation from uninformative ones even in chal-
lenging settings.
Table 3: Evaluation of InfoPower and the most competitive baselines in a suite of challenging distractors
where the background contains a shifted version of the agent being controlled. Results are averaged over 4
random seeds. Fig. 10 shows a visualization of the distractors for this setting.
	Walker-Walk-shifted		Cheetah-Run-shifted	
	500k	1M	500k	1M
DBC	35±15	104±28	41±10	78±38
CDreamer	108±36	164±45	73±44	110±52
TIA	79±42	152±33	65±24	115±28
InfoPower	168±48	278±42	130±46	207±35
20
Published as a conference paper at ICLR 2022
Figure 11: Evaluation of InfoPower and baselines with empowerment in policy learning. The setting cor-
responds to DeepMind Control tasks with RGB natural video distractors in the background (same as that in
Figure 5 of the main paper). The x-axis denotes the number of environment interactions and the y-axis shows
the episodic returns.
A.12	Comparison of baselines with empowerment in policy learning
In this section, we compare against modified versions of the baselines, such that we include empow-
erment in the policy learning of the baselines. This is the only modification we make to the baselines,
and keep everything else unchanged. We described in section 3.3 that inclusion of empowerment
in the objective for visual model-based RL (by modifying both the representation learning objective
and the policy learning objective) is an important contribution of our paper. The aim of this exper-
iment is to disentangle the benefits of empowerment in representation learning, for InfoPower
with that in policy learning.
We have plotted the results in Fig. 11. We can see that the performance of the baselines is slightly
better than Fig. 4 because of the inclusion of empowerment in the policy learning objective which as
we motivated previously in section 3.4 helps with exploring controllable regions of the state-space.
Note that by adding empowerment, we have effectively modified the baselines.
A.13 Comparison on environments without distractors
We compare the performance of InfoPower with a state-of-the-art visual model-based RL method,
Dreamer in the default DM Control environments without distractors. We see from Table 4 that
InfoPower performs slightly better or similar to Dreamer in all the environments consistently.
This result complements our main paper result on environments with distractors and confirms that
the benefit of InfoPower in challenging distracting environments does not come at a cost of per-
formance in simpler non-distracting environments.
Table 4: Comparison of InfoPower with state-of-the-art visual model-based RL method, Dreamer in the
default DM Control environments without distractors. We tabulate reward at 500k and 1M environment inter-
actions. Results are averaged over 4 random seeds. Higher is better.
Dreamer			InfoPower	
	Rew @ 500k	Rew @ 1M	Rew @ 500k	1M
Walker Walk	890±52	985± 10	910±41	980± 17
Cheetah Run	580±190	811±95	586±142	810±102
Finger Spin	570±187	573±96	590±172	598±85
Quadruped Walk	592±45	646±50	623±65	666±47
Hopper Stand	701±68	906±33	715±60	930±39
Ball in a Cup	871±102	910±48	913±67	939±52
Cart-Pole Sparse	742±58	849±88	776±40	866±80
Quadruped Run	450±47	515±65	468±42	510±61
21
Published as a conference paper at ICLR 2022
A.14 Additional ablations
In this section, we consider additional ablations, namely a version of of InfoPower that does not
have the empowerment objective, a version of InfoPower that does not have the MI maximization
objective. We see from Table 5 that removing either of these terms drops performance.
Table 5: DM Walker Stand with natural video distractors
Name	Obs bound	ReW@500k	ReW@1M	Sim@500k	Sim@1M
Dreamer	BA	140 ± 52	157±10	0.32±0.02	0.33±0.03
Contrastive-Dreamer	NCE	231 ± 39	485 ± 86	0.58±0.02	0.64±0.01
NonGenerative-Dreamer	NWJ	280 ± 25	480 ± 93	0.59±0.01	0.62±0.05
InfoPower	NCE	254 ± 27	481 ± 30	0.69±0.03	0.72±0.02
InfoPower	NWJ	389 ± 20	624±80	0.71±0.01	0.74±0.03
Inverse only	None	282 ± 70	367 ± 24	0.63±0.04	0.66±0.03
22