Published as a conference paper at ICLR 2022
Fast Regression for Structured Inputs
Raphael A. Meyer
New York University
ram900@nyu.edu
Cameron Musco
University of Massachusetts Amherst
cmusco@cs.umass.edu
Christopher Musco
New York University
cmusco@nyu.edu
David P. Woodruff
Carnegie Mellon University
dwoodruf@andrew.cmu.edu
Samson Zhou
Carnegie Mellon University
samsonzhou@gmail.com
Ab stract
We study the `p regression problem, which requires finding x ∈ Rd that mini-
mizes kAx - bkp for a matrix A ∈ Rn×d and response vector b ∈ Rn . There
has been recent interest in developing subsampling methods for this problem that
can outperform standard techniques when n is very large. However, all known
subsampling approaches have run time that depends exponentially on p, typically,
dO(p), which can be prohibitively expensive. We improve on this work by show-
ing that for a large class of common structured matrices, such as combinations
of low-rank matrices, sparse matrices, and Vandermonde matrices, there are sub-
sampling based methods for `p regression that depend polynomially on p. For ex-
ample, we give an algorithm for `p regression on Vandermonde matrices that runs
in time O(nlog3 n + (dp2)0.5+ω ∙ polylog n), where ω is the exponent of matrix
multiplication. The polynomial dependence on p crucially allows our algorithms
to extend naturally to efficient algorithms for '∞ regression, via approximation
of '∞ by 'o(iog n). Of practical interest, We also develop a new subsampling al-
gorithm for `p regression for arbitrary matrices, which is simpler than previous
approaches for p ≥ 4.
1	Introduction
Given a matrix A ∈ Rn×d and a vector b ∈ Rn, the goal of linear regression is to find a vector
x ∈ Rd such that Ax is as close as possible to b. In approximate `p linear regression in particular,
we seek to find X ∈ Rd such that, for some approximation parameter ε > 0,
kAX — bkp ≤ (1 + ε) min ∣∣Ax — b∣∣p.
x∈Rd
Here for a vector y ∈ Rn, ∣y∣p = (Pin=1 |yi|p)1/p. `p regression is central in statistical data
analysis, and has numerous applications in machine learning, data science, and applied mathemat-
ics (Friedman et al., 2001; Chatterjee & Hadi, 2006). There are a number of algorithmic approaches
to solving `p regression. For example, we can directly apply iterative methods like gradient descent
or stochastic gradient descent. Alternatively, we can use iteratively reweighted least squares, which
reduces the regression problem to solving poly(d) linear systems (Adil et al., 2019b;a).
Both the above approaches require repeated passes over the matrix A, so while their runtimes are
typically linear in nd, or more generally on the time to multiply the matrix A by a vector, that factor
is multiplied by other parameters, such as the number of iterations to convergence. An alternative
approach, which can lead to faster running time when n is large, is to apply “sketch-and-solve”
methods. This approaches begins with an inexpensive subsampling step, which selects a subset of
rows in A to produce a smaller matrix M with Poly (d, ε, log n《n rows. M can be written as
M = SA where S is a row sampling and rescaling matrix. The goal is for ∣SAX — Sb∣p to be a
good approximation to ∣AX — b∣p for all X ∈ Rd . If this the case, then an approximate solution to
the original `p regression problem can be obtained by solving the subsampled problem, which has
smaller size, thus allowing for more efficient computation.
1
Published as a conference paper at ICLR 2022
The standard approach to subsampling for `p regression is to sample rows with probability propor-
tional to their so-called `p Lewis weights (Cohen & Peng, 2015). Unfortunately, for general inputs
A, `p Lewis weight sampling requires O dmax(1,p/2) rows, and it can be shown that no subsam-
pling method can take fewer than dO(p) (Li et al., 2021). This means that sampling is only helpful
in the limited regime where n dp/2 . However, there are many applications in which the matrix
A has additional structure, which can be leveraged to design more efficient algorithms. For exam-
ple, Vandermonde matrices are used in the polynomial regression problem, which has been studied
for over 200 years (Gergonne, 1974) and has applications to machine learning (Kalai et al., 2008),
applied statistics (MacNeill, 1978), and computer graphics (Pratt, 1987). The goal is to fit a signal,
which is measured at time points t1, . . . , tn using a degree d polynomial. This problem can be for-
mulated as `p regression with a Vandermonde feature matrix A, whose ith row ai is of the form
[1, ti, (ti)2, . . . , (ti)d-1]. Regression problems with Vandermonde matrices also arise in the settings
of Fourier-constrained function fitting (Avron et al., 2019) and Toeplitz covariance estimation (El-
dar et al., 2020). Notably, (Shi & Woodruff, 2019) leverages the structure of Vandermonde matrices
to more quickly build a subsampled matrix M given any Vandermonde matrix A than would be
possible for a general input. Their method does not change how many rows are in the matrix M,
so the overall algorithm still incurs an exponential dependence in p. So while the approach is a
helpful improvement for Vandermonde regression where p is small, this leaves an infeasible run-
time for important problems like '∞ regression, which can be approximated by 'p regression for
p = O (log n).
1.1	Our Contributions
We first show that for `p regression on Vandermonde matrices, it is possible to reduce the size of the
subsampled matrix M to depend polynomially instead of exponentially on p.
Theorem 1.1. Given ε ∈ (0, 1) andp ≥ 1, a Vandermonde matrix A ∈ Rn×d, and b ∈ Rn, there
exists an algorithm that uses O (n log3 n) + d0.5+ω Poly (± ,P, log n) time to compute a sampling
matrix S ∈ Rm×n with m = O log2 n) rows So that With high probability, for all X ∈ Rd,
(1 - ε)kAx - bkp ≤ kSAx - Sbkp ≤ (1 + ε)kAx - bkp,
and then to return a vector Xb ∈ Rd such that kAXb - bkp ≤ (1 + ε) minx∈Rd kAX - bkp.
The best known previous work required at least Ω(nlog2 n + dp/2 ∙ Poly(I/ε)) time (Shi &
Woodruff, 2019). This has an exponential dependence in p, while our algorithm has just a poly-
nomial dependence.
Building on Theorem 1.1, we observe that to obtain a (1 + ε)-approximation to the fundamental
problem of '∞ polynomial regression, it suffices to consider 'p regression for P = O (Iogn).
Since our results have polynomial dependence on p rather than exponential, we thus obtain the first
subsampling guarantees for Vandermonde '∞ regression.
Theorem 1.2. Given ε ∈ (0, 1), a Vandermonde matrix A ∈ Rn×d, and b ∈ Rd, there exists
an algorithm that uses O (n log3 n) + d0'5+ω poly (±, log n time to compute a sampling matrix
S ∈ Rm×n with m = O (遣 log4 n SUch that with high probability, for all X ∈ Rd,
(1 - ε)kAX - bk∞ ≤ kSAX - Sbk∞ ≤ (1 + ε)kAX - bk∞,
and then to return a vector Xb ∈ Rd such that kAXb - bk∞ ≤ (1 + ε) minx∈Rd kAX - bk∞.
To the best of our knowledge, this is the first known dimensionality reduction for '∞ regression with
provable guarantees for any (nontrivial) input matrix. We summarize these results in Table 1.
Our second contribution is to show that improved sampling bounds for `p regression can be extended
to a broad class of inputs, beyond Vandemonde matrices. We introduce the following definition to
capture the “true dimension” of regression problems for structured input matrices.
Definition 1.3 (Rank of Regression Problem). Given an integer p ≥ 1 and a matrix A ∈ Rn×d,
suppose there exists a matrix M ∈ Rn×t and a fixed function f : Rd → Rt so that for all X ∈ Rd,
|hai,Xi|p = |hmi,f(X)i|.
Then we call the minimal such t the rank of the `p regression problem.
2
Published as a conference paper at ICLR 2022
Rows Sampled, `p Regression	Rows Sampled, '∞ Regression	Reference
dp Poly (log n, ε) dp Poly (log n, ε) dp2 poly (log n, 1) (Theorem 1.1)	n n d poly (log n, ,) (Theorem 1.2)	(Avron et al., 2013) (Shi&Woodruff,2019) Our Results
Table 1: Sample complexity for regression on Vandermonde matrices
Theorems 1.1 and 1.2 rely on the following key structural property that we prove about the p-fold
tensor product of rows of the Vandermonde matrix.
Lemma 1.4. For integer p ≥ 1, the rank of the `p regression problem on a Vandermonde matrix
A ∈ Rn×d is O (dp).
Lemma 1.4 implies that the `p loss function for a row of a Vandermonde matrix can potentially be
expressed as a linear combination of O (dp) variables even though the entries of the measurement
vector b can be arbitrary. By comparison, the `p loss function for a row ai of a general matrix A
is |hai, xi - bi|pp can only be expressed as a linear combination of O (pdp) variables, correspond-
ing to each of the dk k-wise products of coordinates of x, for each k ∈ [p]. As a corollary of
Lemma 1.4, Theorems 1.1 and 1.2 obtain a small coreset for 'p regression (as well as '∞ regression)
on a Vandermonde matrix, which can thus be used as a preconditioner for `p regression.
We generalize Lemma 1.4 to similar guarantees for `p regression on a matrix A that is the sum of
a low-rank matrix K and an s-sparse matrix S that has at most s non-zero entries per row. Thus
using the notion of the rank of the regression problem for such a matrix A, we obtain the following
guarantee:
Theorem 1.5. Given ε ∈ (0, 1) and p ≥ 1, a rank k matrix K ∈ Rn×d and a s-sparse matrix
S ∈ Rn×d so that A := K + S, and b ∈ Rn, there exists an algorithm that uses O nkω-1 +
n Poly (2p, ds, kp, sp, ε, logn) time to compute a sampling matrix T ∈ Rm×n containing m =
O pdss (k+S)P(s+p) log2 (pn)) rows So that With high probability, for all X ∈ Rd,
(1 - ε)kAx - bkp ≤ kTAx - Tbkp ≤ (1 + ε)kAx - bkp,
and then to return a vector Xb ∈ Rd such that kAXb - bkp ≤ (1 + ε) minx∈Rd kAX - bkp. Fur-
ther, if the low-rank factorization of K is given explicitly, then this runtime can be improved to
n Poly (2p, ds, kp, sp, ε, logn).
Similarly, we obtain efficient guarantees for `p regression on a matrix A that is the sum of a Van-
dermonde matrix V and a sparse matrix S that has at most s non-zero entries per row.
Theorem 1.6. Given ε ∈ (0, 1) and p ≥ 1, a Vandermonde matrix V ∈ Rn×d and an s-sparse
matrix S ∈ Rn×d such that A := V + S, and b ∈ Rn, there exists an algorithm that uses
O (n log3n + poly (p2d,ds,sp, logn,1)) time to compute a sampling matrix T ∈ Rm×n with
m = O ^p-dΔj3P(s+p) log2(pn)) rows, so that with high Probabilityfor all X ∈ Rd,
(1-ε)kAX-bkp ≤ kTAX - Tbkp ≤ (1 + ε)kAX - bkp,
and then to return a vector Xb ∈ Rd such that kAXb - bkp ≤ (1 + ε) minx∈Rd kAX - bkp.
Surprisingly, our methods even yield practical algorithms for general matrices with absolutely no
structure. Although the rank of the `p regression problem for general matrices is O (dp), we still
obtain the optimal sample complexity (see (Li et al., 2021) for a lower bound) of roughly O dp/2 .
Furthermore, an advantage of our approach is that We only need to perform 'q Lewis weight sam-
pling for q ∈ [1,4] and there are known efficient iterative methods for computing the 'q Lewis
weights for q ∈ [1, 4], e.g., see Section 2.1 of this paper or Section 3 in (Cohen & Peng, 2015). By
contrast, previous methods relied on computing general `p weights forp > 4, which, prior to the re-
cent work of (Fazel et al., 2021), required solving a large convex program, e.g., through semidefinite
programming, see Section 4 in (Cohen & Peng, 2015).
Finally, we experimentally validate our theory on synthetic data. In particular, we consider matri-
ces and response vectors that are motivated by existing lower bounds for subsampling and sketch-
ing methods. For Vandermonde regression, our experiments demonstrate that the number of rows
3
Published as a conference paper at ICLR 2022
needed is polynomial in p, reinforcing how structured matrices outperform the worst-case bound.
For unstructured matrix 'p regression, We demonstrate that our 'q Lewis Weight subsampling
scheme is effective and accurate.
1.2 Overview of our Techniques
Our algorithmic contributions rely on two key observations, which we describe below. For the sake
of presentation, we assume p is an integer in this overview, though we handle arbitrary real values
of p in the subsequent algorithms and analyses.
Reduced rank of the `p regression problem on structured inputs. The first main ingredient is
the simple yet powerful observation that the rank of the `p regression problem on structured inputs
such as Vandermonde matrices A ∈ Rn×d does not need to be the O (dp) rank of the `p regression
problem on general matrices. For a general matrix, hai, xip for an integer p can be rewritten as a
linear combination P αiyi of O (dp) terms, where each coefficient αi is a p-wise product of entries
of the row vector ai ∈ Rd and similarly each yi is a p-wise product of entries of the vector x ∈ Rd .
However, when A is a Vandermonde matrix, then the j-th entry of ai is simply Aij,-21. Thus each
coefficient in hai, xip can be written as a linear combination of 1, Ai,2, (Ai,2)2, . . . , (Ai,2)p(d-1) and
so we can express hai, xip as a linear combination of O (dp) variables rather than O (dp) variables.
We can similarly show that the rank of the `p regression problem on rank k-matrix A is O (kp) by
writing each row ai as a linear combination of basis vectors v1, . . . , vk. Then, using the Hadamard
Product-Kronecker Product mixed-product property, we can rewrite hai, xip as a linear combination
of p-wise products of the variables hv1, xi, . . ., hvk, xi, i.e., a linear combination of O (kp) variables
rather than O (dp ) variables. It follows that the rank of the `p regression problem on a matrix A
whose rows have at most s non-zero entries is O (dssp) by noting that (1) there are ds = O (ds)
sparsity patterns and that (2) for a fixed sparsity pattern, the rank of the induced matrix is at most
s, which induces a linear combination of O (sp) variables for the `p regression problem. These
decomposition techniques can also be generalized to show that the rank of the `p regression problem
is low on matrices A such that A = K + V, A = K + S or A = V + S, where K is a low-rank
matrix, V is a Vandermonde matrix, and S is a sparse matrix.
However, we remark that although the observation that the rank of the `p regression problem on
structured inputs can be low, this itself does not yield an algorithm for `p regression. This is because
the loss function is |hai, xi - bi|p rather than hai, xip and there can be n possible different values
of bi across all i ∈ [n].
Rounding and truncating the measurement vector: a novel algorithmic technique. Thus, the
second main ingredient is manipulating the measurement vector b ∈ Rn so that the loss function can
utilize the low-rank property of the `p regression problem on structured inputs. A natural approach
would be to round the entries of b, say to the nearest power of (1 + ε). Unfortunately, such a
rounding approach would roughly preserve kbkp up to a multiplicative (1 + ε) factor, but it would
not preserve kAx-bkp. For example, suppose hai, xi = bi = N for some arbitrarily large value N.
Then |hai, xi - bi|p = 0, but if bi were rounded to (1 +ε)N, we would have |hai, xi - bi|p = εpNp,
which can be arbitrarily large.
The lesson from this counterexample is that when bi is significantly larger than hai, xi - bi, any
rounding technique can be arbitrarily bad because kbkp can be significantly larger than OPT :=
minx∈Rd kAx - bkp. On the other hand, if kbkp is a constant factor approximation to OPT, then
the previous counterexample cannot happen because either (1) bi is large relative to kbkp and hai, xi
cannot be too close to bi so that rounding bi will not significantly affect the difference hai , xi - bi
or (2) bi is small relative to kbkp and so any rounding of bi will not affect the contribution of
the i-th row of A in the overall loss. Thus our task is reduced to manipulating the input so that
kbkp = O (OPT).
To that end, we note that if k Ax 一 b∣∣p = O (OPT) for a vector X ∈ Rd, then by a triangle inequality
argument, we have that the residual vector b0 = b 一 Ax ∈ Rd satisfies ∣∣b0∣∣p = O (OPT). Namely,
we show that to find such a vector X, it suffices by triangle inequality to find a subspace embedding
for A, i.e., a matrix M ∈ Rm×d with m n such that
(1 一 C)kAxkp ≤ kMxkp ≤ (1 + C)kAxkp
4
Published as a conference paper at ICLR 2022
for some constant C ∈ (0, 1). Typically, such a matrix M can be found by sampling the rows of A
according to their `p Lewis weights to generate a matrix with m = O dmax(1,p/2) rows. However,
using our observation that the rank of the `p regression problem on structured inputs can be low,
We can sample O (dp) rows of A with probabilities proportional to their 'q Lewis weights, where
q = 2pr for an integer r such that 2r ≤ p < 2r+1. Crucially, we can find such a vector X without
reading the coordinates of b since we only require to read the rows of A to perform Lewis weight
sampling in this phase. Thus we can efficiently find a residual vector b0 such that kb0kp = O (OPT).
Partitioning the matrix into groups. We then round the entries of b0 to obtain a vector b00 with
at most' := O (Iogn) unique values, truncating all entries that are less in magnitude than poj(n)
to zero instead. We now solve the `p regression problem minx∈Rd kAx - b00kp by partitioning
the rows of A into ' groups Gι,...,G' based on their corresponding values of b00. The main
point is that all rows in each group Gk have the same entry tk in b00. Thus we can again observe that
∣hai, Xi-tk ∣p can be written as a linear combination of O (p2 * * * &d) variables and therefore use 'q Lewis
weight sampling (for the same q) to reduce each group Gk down to O (p2d log d) rows. Since there
are ' = O (Iogn) groups, then there are roughly O (p2d) total rows that have been sampled across
all the groups. It follows from the decomposition of the `p loss function across each group that the
matrix T formed by these rows is a coreset for the `p regression problem. Therefore, by solving
the `p regression problem on T, which has significantly smaller dimension than the original input
matrix A, we obtain a vector Xb with the desired property that
kAXb - bkp ≤(1+O(ε))xm∈iRndkAX-bkp.
Practical `p regression for arbitrary matrices. We now describe a practical procedure for `p
regression on general matrices that avoids the necessity for convex programming to approximate
the 'p Lewis weights for p > 4. We first pick an integer r such that 芸 ∈ [2,4). By the same
structural argument as in Lemma 1.4, we can tensor product eachr row ai of A with itself 2r times,
thus obtaining an extended matrix of size n × d0, where d0 = d2r, independent of the entries in b.
We then 'p/2r Lewis weight sample on the extended matrix, using the iterative method in Figure 1
since 2pr < 4 rather than solving a convex program for 'p Lewis weight sampling for P > 4. Since
2pr ∈ [2,4), it follows from Theorem 2.3 that Lewis weight sampling requires roughly (d0)p/2 rows.
Thus we obtain a matrix with O ((d2r )p/2r+1) = O (dp/2) rows, from which we can compute a
residual vector b0 such that kb0kp = O (OPT), where OPT := minx∈Rd kAX - bkp.
We then round and truncate the entries of b0 using the subroutine RoundTrunc to obtain a vector
b00, which allows us to partition the rows of A and the entries of b00 into O (Iofn) groups. Due to
the constant values of b00 in each group, we can again 'p/2r Lewis weight sample on an extended
matrix using an iterative method and finally solve the 'p regression problem on the subsequent rows
that have been sampled. We give our algorithm in full in Algorithm 3.
2 `p REGRESSION FOR VANDERMONDE MATRICES
2.1 Preliminaries on Lewis Weight Sampling
There are a number of known sampling distributions for dimensionality reduction for the 'p regres-
sion problem, such as the 'p leverage scores, e.g., (Cormode et al., 2018; Dasgupta et al., 2008) and
the 'p sensitivities, e.g., (Clarkson et al., 2019; Braverman et al., 2020; 2021; Musco et al., 2021); in
this paper we focus on the 'p Lewis weights, e.g., (Cohen & Peng, 2015; Durfee et al., 2018; Chen
& Derezinski, 2021; Parulekar et al., 2021).
Definition 2.1 ('p Lewis Weights, (Cohen & Peng, 2015)). Given a matrix A ∈ Rn×d andp ≥ 1,
the 'p Lewis weights w1(A), . . . , wn(A) are the unique quantities that satisfy
(wi(A))2/p = ai>(A>W1-2/pA)-1ai
for all i ∈ [n], where W ∈ Rn×n is the diagonal matrix with Wi,i = wi(A) for all i ∈ [n].
5
Published as a conference paper at ICLR 2022
Definition 2.2 (Lewis Weight Sampling). For an input matrix A ∈ Rn×d and m samples, let the
sampling matrix S ∈ Rm×n be generated by independently setting each row of S to be the i-th
d	1/p	wp (A)
standard basis vector multiplied by (二/⑷)	With probability i ； .
Theorem 2.3 (`p Subspace Embedding from Lewis Weight Sampling, (Cohen & Peng, 2015)).
For any A ∈ Rn×d , let the matrix S ∈ Rm×n be generated from Lewis weight sampling with
m = O (-——(>P/)log(d∕δ)log(1∕ε)), where Y = 2 for P ∈ [1, 2] and Y = 5 for P > 2. Then with
probability at least 1 - δ, we have that simultaneously for all x ∈ Rd ,
(1 - ε)kAxkp ≤ kSAxkp ≤ (1 + ε)kAxkp.
Since the Lewis weights are implicitly defined, it may not be clear how to compute them exactly. In
fact, it suffices to compute constant factor approximations to the Lewis weights. (Cohen & Peng,
2015) show that for P ≤ 4, there exists a simple iterative approach to compute a constant factor
approximation to the `p Lewis weights in input sparsity time, which we present in Figure 1.
W = LeWisIterate(A,p,β, W)
(1)	For i = 1,...,n
(a)	Let Ti be a constant factor approximation to a> (A> W1-2∕pA)-1ai
(b)	Wi — (Ti)P/2
(c)	Return W
w = ApproXLeWisWeights(A,p, β, T)
(1)	Intialize Wi = 1 for all i ∈ [n].
(2)	Fort = 1,...,T
(a) w — LeWisIterate(A,p,β, w)
(3)	Return W
Fig. 1: Iterative algorithm for approximate `p Lewis weights with P < 4.
At a high level, the correctness of Figure 1 follows from Banach’s fixed point theorem and the fact
that the subroutine LeWisIterate is a contraction mapping, because |2/P - 1| < 1 forP < 4 (Cohen
& Peng, 2015). However, for P ≥ 4, Figure 1 no longer works. Instead, prior to the recent work of
(Fazel et al., 2021), approximating `p Lewis weights forP > 4 seems to require solving the convex
program
Q = argmax det(M), subject to	(ai>Ma)p/2 ≤ d, M	0,
Mi
and setting Wi = (ai> Qa)p/2 . Unfortunately, this is often infeasible in practice, and we could not
obtain empirical results using it. Therefore, a nice advantage of our algorithms, both for structured
and unstructured matrices, is that we only use 'q Lewis weight sampling for q ≤ [1,4), even if
P ≥ 4, whereas previous algorithms required using `p Lewis weight sampling forP > 4.
2.2 Algorithm and Analysis
We first describe the general framework of our algorithm for efficient `p regression, so that given
A ∈ Rn×- and b ∈ R-, the goal is to approximately compute OPT := minx∈Rd kAx - bkp.
Recall that in order to apply our structural results, we first require a measurement vector b00 with
a small number of distinct entries. We obtain b00 by first finding a constant factor approximation,
i.e., X ∈ R- SUch that k Ax — b∣∣p ≤ O (OPT). We can 'p Lewis weight sample the rows of A to
do this, but the time to solve the subsequent polynomial regression problem would have exponential
dependence on P, due to Theorem 2.3. Instead, we use our structural properties to implicitly create
a matrix M from A with fewer than dp columns and then "2「Lewis weight sample the rows of
6
Published as a conference paper at ICLR 2022
A, where r is the integer that satisfies 2r ≤ p < 2r+1, and then we solve the `p regression problem
on the sampled rows of A and entries of b to obtain x. We set b0 to be the residual vector b - Ax
so that kb0kp = O (OPT). We then use the procedure RoundTrunc, i.e., as in Algorithm 1, which
sets the entries of b0 that are the smallest in magnitude to zero, and rounds the remaining entries of
b0 to the nearest power of (1 + ε).
Algorithm 1 RoundTrunc: Round and truncate coordinates of input vector b
Input: Vector b ∈ Rn , accuracy parameter ε > 0
Output: Vector x ∈ Rn that is a rounded and truncated version of b
1:	M — maxi∈n |bi|
2:	for i = 1 to i = n do
3:	Pi — blogι+ε |bi|c
4:	Xi — (1 + ε)pi ∙ sign(bi)
5:	if |xi| ≤ M then
6:	xi — 0
7:	return x
Since kb0kp = O (OPT), it then follows by triangle inequality that it suffices to approximately
solve the `p regression problem on the vector b00 instead. We partition the rows of A into groups
Gι, ...,g` based on the values of b00 and perform "2「Lewis weight sampling again on an implicit
matrix that we create from the rows of each group. Here we leverage our theory about the rank of
the `p regression problem for structured inputs. It then suffices to solve the `p regression problem
on the matrix formed by the sampled rows across all the groups.
Algorithm 2 Faster regression for Vandermonde matrices
Input: Vandermonde matrix A ∈ Rn×d, measurement vector b, accuracy parameter ε > 0
Output: xb ∈ R with kAxb - bkp ≤ (1 + O (ε)) minx∈Rd kAx - bkp
1:	r — blog pc	. 2r ≤ p < 2r+1 .
2:	Extend A to a Vandermonde matrix M with dimensions n × d0, where d0 = (2r (d - 1) + 1).
3:	Use 'p∕2r-Lewis weight sampling on M to find a set S of O (d0 log d0) indices in [n] =
{1, 2, . . . , n}, and corresponding rescaling factors.
4:	Let A0 be the corresponding submatrix of A with indices in S, and scaled accordingly.
5:	Compute X ≤ 5 minχ∈Rd ∣∣ A0x - b∣∣p.
6:	b0 — b - AX, b00 ― ROUndTrUnC(b0), ' — O (Iofn)
7:	Partition the rows of A into groups Gi,..., g`, each containing all rows with the same value
of b00
8:	Let Gk be the submatrix of Gk extended to d00 = 22r(d - 1) + 1 columns.
9:	Let tk be the coordinate of b00 corresponding to Gk for each k ∈ [`].
10:	Use 'p∕2r -Lewis weight sampling on [Gk; tk] to find a set Sk of O (∣20 log d00) indices in [n]
and rescaling factors.
11:	Let Tk be the corresponding sampling and rescaling matrix for Sk0 .
12:	T— [T1;...;Tk]>
13:	Compute xb ≤ (1 + ε) minx∈Rd ∣TAx - Tb∣p.
14:	return xb
We show in the supplementary material that Algorithm 2 satisfies the guarantees of Theorem 1.1.
Moreover, because Theorem 1.1 has polynomial dependence on p rather than exponential depen-
dence, we obtain the first known sublinear size coreset for the important problem of '∞ regression.
We use the following structural property.
Lemma 2.4. Let X ∈ Rn andP = Ω ( ^in I. Then ∣∣x∣∞
≤ ∣X∣p ≤ (1 + ε)∣X∣∞
Lemma 2.4 implies that to solve '∞ regression, we can instead solve 'p regression for P =
Ω (Iofn). Then Theorem 1.2 follows from the fact that even for P = Ω (lo∣n), the matrix T
7
Published as a conference paper at ICLR 2022
in Algorithm 2 satisfies
(1 - ε)kTAx - Tb00kp ≤ kAx - b00kp ≤ (1 + ε)kTAx - Tb00kp
for all x ∈ Rd . Hence, we can solve the `p regression problem on the smaller matrix T to solve the
'∞ regression on A.
The results of Theorem 1.1 can be further extended to matrices with block Vandermonde structure.
Corollary 2.5. Given ε ∈ (0, 1) andp ≥ 1, A ∈ Rn×dq, and b ∈ Rd, suppose A = [A1 | . . . |Aq]
for Vandermonde matrices A1, . . . , Aq ∈ Rn×d. Then there exists an algorithm that, with high
probability, returns a vector xb ∈ Rd such that
kAxb - bkp ≤ (1 + ε) min kAx - bkp,
x∈Rd
using O (T(A)(dp)q-1 logn + poly((dp)q logn, 1∕ε)) time, where T(A) is the runtime ofmul-
tiplying the matrix A by an arbitrary vector. For q = 2, this can be further optimized to
O (ndω2/2-1 + poly((dp)2, log n, 1∕ε)) time, where O (nω2) is the time to multiply an n X n matrix
with an n × n2 matrix, so that ω2 ∈ [3, 4].
We obtain similar algorithms for noisy low-rank matrices and noisy Vandermonde matrices, i.e.,
Theorem 1.5 and Theorem 1.6. We defer presentation and justification for these results to the sup-
plementary material.
Algorithm 3 Faster 'p regression for general matrices
Input: Matrix A ∈ Rn×d, measurement vector b, accuracy parameter ε > 0
Output: xb ∈ R with kAxb - bkp ≤ (1 + ε) minx∈Rd kAx - bkp, parameter p ≥ 4
1:	r — blogPc _ 1	.2r+1 ≤ p < 2r+2.
2:	Extend A to a matrix M with dimension n × O d2r so that each row in Mi is the 2r -fold
tensor product of ai reshaped into a d2r length vector.
3:	Use "2r -Lewis weight sampling on M to find a set S of O (dp/2 log d) indices in [n] and
rescaling factors.
4:	Let A0 be the corresponding submatrix of A with indices in S and scaled accordingly.
5:	Compute X ≤ 5 minχ∈Rd ∣∣ A0x 一 b∣∣p.
6:	b0 一 b 一 AX, b00 - ROUndTrUnC(b0), ' 一 O (loεn)
7:	Partition the rows of A into groups Gi,..., g`, each containing all rows with the same value
of b00
8:	Let Gk be the corresponding submatrix and tk be the coordinate of b00 corresponding to Gk for
each k ∈ [`].
9:	Use 'p∕2r -Lewis weight sampling on [Gk; tk] to find a set Sk of O (ε5dp/2 log d indices in [n]
and rescaling factors.
10:	Let Tk be the corresponding rows with indices in Sk0 and scaled accordingly.
11:	T - [Ti; …；Tk]>
12:	Compute Xb ≤ (1 + ε) minx∈Rd ∣TX 一 Tb∣p.
13:	return Xb
Finally, we describe in Algorithm 3 a practical approach for `p regression for arbitrary matrices
without requiring any structural assumptions.
Theorem 2.6. Algorithm 3 outputs a vector Xb such that ∣AXb 一 b∣p ≤ (1 + ε) min ∣AX 一 b∣p.
The runtime of Algorithm 3 is T(n, dp/2) + poly (dp/2, log n, ɪ), where T(n, dp/2) is the time to
multiply a matrix of size n × dp/2 by a vector of length dp/2 .
Theorem 2.6 achieves the same optimal sample complexity as previous `p Lewis weight algorithms
of roughly dp/2. However, the main advantage of Algorithm 3 is that it performs 'q Lewis weight
sampling for some q ∈ [2, 4), which is quite efficient because we can use an iterative method rather
than solving a convex program.
8
Published as a conference paper at ICLR 2022
53113
0 0 0 - -
1110
1
)laciripmeε( rorrE evitaleR
5	10	15	20
Norm Parameter (p)
(a) Relative Error versus p
Fig. 2: Comparison of relative error (εempirical) versus `p parameter p and sample complexity m on
Vandermonde data. We ran both experiments 30 times and plot the median, 25th quartile, and 75th
quartile for each value of p and m.
)laciripmeε( rorrE evitaleR
25	500	1,000 1,500 2,000 2,500 3,000
Subsample Complexity (m)
----LpN Lewis Weights
Uniform
(b) Relative Error versus m
3 Empirical Verification
We provide empirical evidence to validate our core statistical claim about Vandermonde regression:
that the relative error ε achieved by 'p∕2r Lewis Weight subsampling is polynomial in p, and not
exponential in p. More precisely, we compute the gap between the error achieved from exact `p
Vandermonde regression and the error achieved from the subsampled regression:
εempirical
IlVx - b∣∣p-k Vx*- b∣∣p
IIVx* - bkp
Our theory tells us that 院仃好汨：1 )3 ≤ O (%), where m is the total number of subsampled
rows. The prior work on unstructured matrices instead suggests (εempirical) ≤ O (dl(p)) (Shi &
Woodruff, 2019). So, to visually distinguish these two settings, we look at the logarithm of both
sides:
log(εempirical) ≤ O (ln(p) + ln(m))	OR	log(εempirical) ≤ O(Pln(d) + ln(m))
In particular, our work suggests a logarithmic dependence on p, while the prior work suggests a
linear dependence on p.
To validate our theory, we plot log(εempirical) versus p and m on synthetic data. Specifically, we
generate n = 25, 000 i.i.d. N(0, 1) times samples to form a Vandermonde matrix V with d = 20
columns, then compute the polynomial q(t) = t10 at each time sample, add N(0, 1010) additive
noise, and save the corresponding values in b. We then compute εempirical for this `p regression
problem. Notably, in order to compute xb, we omit the rounding procedure in our code, since the
rounding is designed for worst-case inputs. Instead, we simply compute xb by solving minx IVb x -
b∣p where V and b are computed by sampling and rescaling V and b with the Lewis Weights.
Figure 2 shows the result of these experiments, which were run in Julia 1.6.1, on Windows 10 with
an Intel i7-7700K CPU and 16Gb RAM. In Figure 2a, wefix m = 1000 and vary p ∈ [2, 25]. The
trendline of Lewis Weight sampling clearly better fits a logarithmic model, as opposed to a linear
model. This reinforces our analysis by showing that the dependence onp is notably sub-exponential,
beating the known bounds for `p subsampling on unstructured matrices.
As a benchmark, we compare our Lewis Weight sampling method to uniform sampling. The noise
in b is large enough that most rows of A have little information about the underlying polynomial
q(t). Lewis weight sampling takes avoids these rows, while uniform sampling does not, explaining
why uniform sampling is much weaker in Figure 2a. Further, in Figure 2b, we fix p = 6 and
vary m ∈ [100, 3000], showing that Lewis Weight sampling outperforms uniform sampling across
both m and p. In Appendix C, we demonstrate similar results for unstructured matrix regression,
validating the analysis of Theorem 2.6.
9
Published as a conference paper at ICLR 2022
Acknowledgements
Cameron Musco was supported by NSF grants 2046235 and 1763618, and an Adobe Research grant.
David P. Woodruff and Samson Zhou were supported by National Institute of Health grant 5401 HG
10798-2 and a Simons Investigator Award. Christopher Musco and Raphael Meyer were supported
by NSF grant 2045590 and DOE Award DE-SC0022266.
References
Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative refinement for lp-
norm regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ,pp.1405-1424, 2019a. 1,15
Deeksha Adil, Richard Peng, and Sushant Sachdeva. Fast, provably convergent IRLS algorithm for
p-norm linear regression. In Advances in Neural Information Processing Systems 32, NeurIPS,
pp. 14166-14177, 2019b. 1, 15
Haim Avron, Vikas Sindhwani, and David P. Woodruff. Sketching structured matrices for faster
nonlinear regression. In 27th Annual Conference on Neural Information Processing Systems.
Proceedings, pp. 2994-3002, 2013. 3
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. A universal sampling method for reconstructing signals with simple fourier transforms.
In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC, pp.
1051-1063, 2019. 2
Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay,
David P. Woodruff, and Samson Zhou. Near optimal linear algebra in the online and sliding
window models. In 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS,
pp. 517-528, 2020. 5, 22
Vladimir Braverman, Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal, and Sam-
son Zhou. Adversarial robustness of streaming algorithms through importance sampling. CoRR,
abs/2106.14952, 2021. 5, 22
Samprit Chatterjee and Ali S Hadi. Regression analysis by example, volume 607. John Wiley and
Sons, 2006. 1
Xue Chen and Michal Derezinski. Query complexity of least absolute deviation regression via robust
uniform convergence. In Conference on Learning Theory, COLT, pp. 1144-1179, 2021. 5, 22
Rachit Chhaya, Jayesh Choudhari, Anirban Dasgupta, and Supratim Shit. Streaming coresets for
symmetric tensor factorization. In Proceedings of the 37th International Conference on Machine
Learning, ICML, pp. 1855-1865, 2020. 22
Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference, STOC, pp. 81-90, 2013. 22
Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui Meng,
and David P. Woodruff. The fast cauchy transform and faster robust linear regression. SIAM J.
Comput., 45(3):763-810, 2016. 22
Kenneth L. Clarkson, Ruosong Wang, and David P. Woodruff. Dimensionality reduction for tukey
regression. In Proceedings of the 36th International Conference on Machine Learning, ICML, pp.
1262-1271, 2019. 5, 22
Michael B. Cohen and Richard Peng. lp row sampling by lewis weights. In Proceedings of the
Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC, pp. 183-192, 2015.
2, 3, 5, 6, 15, 21, 22
Graham Cormode, Charlie Dickens, and David P. Woodruff. Leveraging well-conditioned bases:
Streaming and distributed summaries in minkowski p-norms. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, ICML, pp. 1048-1056, 2018. 5, 22
10
Published as a conference paper at ICLR 2022
Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W. Mahoney. Sampling
algorithms and coresets for lp regression. In Proceedings of the Nineteenth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA, pp. 932-941, 2008. 5, 22
David Durfee, Kevin A. Lai, and Saurabh Sawlani. l1 regression using lewis weights preconditioning
and stochastic gradient descent. In Conference On Learning Theory, COLT, pp. 1626-1656, 2018.
5, 22
Yonina C. Eldar, Jerry Li, Cameron Musco, and Christopher Musco. Sample efficient toeplitz co-
variance estimation. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms,
SODA, pp. 378-397, 2020. 2
Maryam Fazel, Yin Tat Lee, Swati Padmanabhan, and Aaron Sidford. Computing lewis weights to
high precision. CoRR, abs/2110.15563, 2021. 3, 6
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning.
Springer series in statistics New York, 2001. 1
JD Gergonne. The application of the method of least squares to the interpolation of sequences.
Historia Mathematica, 1(4):439-447, 1974. 2
I Gohberg and V Olshevsky. Complexity of multiplication with vectors for structured matrices.
Linear Algebra and Its Applications, 202:163-192, 1994. 15
Jerrad Hampton and Alireza Doostan. Coherence motivated sampling and convergence analysis
of least squares polynomial chaos regression. Computer Methods in Applied Mechanics and
Engineering, 290:73-97, 2015. 20
Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically
learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805, 2008. 2, 20
Daniel Kane, Sushrut Karmalkar, and Eric Price. Robust polynomial regression up to the information
theoretic limit. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS,
pp. 391-402, 2017. 20, 21
Kiran S. Kedlaya and Christopher Umans. Fast polynomial factorization and modular composition.
SIAM J. Comput., 40(6):1767-1802, 2011. 17
Yi Li, Ruosong Wang, and David P. Woodruff. Tight bounds for the subspace sketch problem with
applications. SIAM J. Comput., 50(4):1287-1335, 2021. 2, 3,22
Ian B MacNeill. Properties of sequences of partial sums of polynomial regression residuals with
applications to tests for change of regression at unknown times. The Annals of Statistics, pp.
422-433, 1978. 2
Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Symposium on Theory of Computing Confer-
ence, STOC, pp. 91-100. ACM, 2013. 22
Raphael Meyer, Cameron Musco, Christopher Musco, David Woodruff, and Samson Zhou. Cheyb-
shev sampling is universal for lp polynomial regression, 2021. 20, 21, 22
Cameron Musco, Christopher Musco, David P. Woodruff, and Taisuke Yasuda. Active sampling for
linear regression beyond the l2 norm. CoRR, abs/2111.04888, 2021. 5, 21, 22
Jelani Nelson and Huy L. Nguyen. OSNAP: faster numerical linear algebra algorithms via sparser
subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer Science,
FOCS, pp. 117-126, 2013. 22
Michael Nusken and Martin Ziegler. Fast multipoint evaluation of bivariate polynomials. In Algo-
rithms - ESA, 12th Annual European Symposium, Proceedings, pp. 544-555, 2004. 17
Aditya Parulekar, Advait Parulekar, and Eric Price. l1 regression with lewis weights subsampling.
In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,
APPROX/RANDOM, pp. 49:1-49:21, 2021. 5, 22
11
Published as a conference paper at ICLR 2022
Vaughan Pratt. Direct least-squares fitting of algebraic surfaces. ACM SIGGRAPH computer graph-
ics, 21(4):145-152,1987. 2, 20
Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Re, and Atri Rudra. A two-pronged
progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pp. 1060-1079, 2018. 17
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), Proceedings, pp.
143-152. IEEE Computer Society, 2006. 22
Xiaofei Shi and David P. Woodruff. Sublinear time numerical linear algebra for structured matrices.
In The Thirty-Third AAAI Conference on Artificial Intelligence, pp. 4918-4925, 2019. 2, 3, 9, 13
Christian Sohler and David P. Woodruff. Subspace embeddings for the l1-norm with applications.
In Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC, pp. 755-764, 2011.
22
Ruosong Wang and David P. Woodruff. Tight bounds for lp oblivious subspace embeddings. In
Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pp.
1825-1843, 2019. 22
David P. Woodruff and Qin Zhang. Subspace embeddings and lp-regression using exponential ran-
dom variables. In COLT 2013 - The 26th Annual Conference on Learning Theory, pp. 546-567,
2013. 22
B roader Impact
This work is focused on improving the runtime of algorithms for `p regression on structured inputs,
and can allow users to more efficiently solve regression problems on large datasets. The problem we
study is abstracted away from specific ethical and unethical applications of `p regression. So, with
the assumption that the research and data science communities will focus on ethical applications
while avoiding unethical ones, we believe that the benefits of our algorithmic results outweigh the
negative risks.
A Missing Proofs from Section 2
In this section, we give the missing proofs from Section 2.
A. 1	`p REGESSION ON VANDERMONDE MATRICES
We first prove a simple statement that shows a “good” solution to the `p regression problem on a
subspace embedding SA is also a “good” solution to the original input matrix A.
Lemma A.1. Let S be a sampling and rescaling matrix such that
11	13
12HSAXllp ≤ IlAXllp ≤ 12IISAXl∣p, E [∣∣SAx∣∣p] = IlAXllp
for all X ∈ Rd. Let OPT = minx∈Rd lAX - blp and let X ∈ Rd be any vector for which lSAX -
Sblp ≤ 5OPT. Then with probability at least 0.79,
lAX - blp ≤ 12OPT.
Proof. Let x* be a minimizer of ∣∣Ax 一 b∣p so that OPT = ∣∣ Ax* 一 b∣p. We will prove the claim
by contrapositive, so we first suppose that lAX - blp ≤ 12OPT. Then by the triangle inequality,
IISAX - Sbllp ≥ IlSA(X 一 x*)Ilp -∣∣SA* 一 Sb∣∣p.
Since 12∣∣SAxIp ≤ IlAXIp ≤ 12∣∣SAxIp for all X ∈ Rd, then
IlSAX - SbIlp ≥ 11 IlA(X 一 x*)∣∣p - ∣∣SA* - Sb∣∣p.
12
Published as a conference paper at ICLR 2022
By the triangle inequality,
kSAX - Sbkp ≥ 12 (IlAX - bkp - kAx* - bkP)-IISA* - sbkp.
Note that by Jensen's inequality, E[∣∣SAx* - Sb∣∣p] ≤ OPT, So that by Markov's inequality,
Pr [I SAx* - SbkP ≥ 5OPT] ≤ 1.
5
Thus with probability at least 0.8,
IISAX - Sbkp ≥ 12 (IlAX - bkp -IlAχ* - bkP)- 5OPT∙
Thus if kAx - bkp ≤ 12OPT, then
∣∣SAx - Sbkp ≥ 11 (12OPT - OPT) - 5OPT > 5OPT,
as desired.
□
Before justifying the correctness of Algorithm 2, we first recall the following algorithm for efficient
`p subspace embeddings.
Theorem A.2. (Shi & Woodruff, 2019) Given ε ∈ (0, 1) and p ∈ [1, 2], a Vandermonde ma-
trix A ∈ Rn×d, and b ∈ Rd, let T (A) be the time it takes to perform matrix-vector mul-
tiplication, i.e., compute Av for an arbitrary v ∈ Rd. There exists an algorithm that uses
O (T(A) log n + dq ∙ Poly(I/ε)) time, where q = ω for P ∈ [1,4) and q = p/2 + C for some
fixed constant C > 0 for P > 4, and with high probability, returns X ∈ Rd such that
kAX — bkp ≤ (1 + ε) min IIAx — bkp.
x∈Rd
We now justify the correctness of Algorithm 2.
Lemma A.3. Given ε ∈ (0, 1) and P ≥ 1, a Vandermonde matrix A ∈ Rn×d, and b ∈ Rd, then
with high probability, Algorithm 2 returns a vector Xb ∈ Rd such that
kAXb - bkp ≤ (1 + O (ε)) min kAX - bkp.
x∈Rd
Proof. Consider Algorithm 2 and let r be an integer so that 2r ≤ P < 2r+1 . Note that
X|hai,Xi|p=X|(hai,Xi)2r|p/2r.
i∈[n]	i∈[n]
Since A is Vandermonde, then
rr
d	2	d	2	2r(d-1)+1
(hai, Xf= (X ai,jXj)	= (X aj,ιχj I = X	aj,-1yj,
j=1	j=1	j=1
where each yj is a fixed function of the coordinates of X. Notably, the fixed function is the same
across all i ∈ [n]. Hence, the '?r subspace embedding problem on an input Vandermonde matrix
A ∈ Rn×d can be reshaped as a constrained `1 subspace embedding problem on an input Van-
dermonde matrix of size n × (2r (d - 1) + 1). Thus, for `p regression with P ∈ [2r, 2r+1), we
have
2r (d-1)+1	p/2r
X|hai,Xi|p=X X aij,-21yj	,
i∈[n]	i∈[n]	j=1
which is a constrained 'p∕2r regression problem on an input Vandermonde matrix M of size n X
(2r (d - 1) + 1).
By Theorem A.2, We can use 的叱 Lewis weight sampling to find a matrix M0 such that
5kMykp/2： ≤kM0ykp∕2r ≤ 5kMykp∕2r
13
Published as a conference paper at ICLR 2022
for all y ∈ R2r(d-1)+1 with high probability. Note that by the above argument, ifwe take the matrix
A0 corresponding to the scaled rows of A that are sampled by M0 , then we also have
11	13
12kAxkP ≤kA0x∣∣p ≤ 12kAx∣∣p
and thus
12kAxkp ≤ kA0xkp ≤ 12kAxkp
for all X ∈ Rd+1 with high probability. Thus by Lemma A.1, we can find a vector X such that
IIAx 一 bkp ≤ 12 min k Ax — bkp.
x∈Rd
We use the subroutine RoundTrunc to create the vector b00 which is the vector with all entries ofb0
rounded to the nearest power of (1 + ε), starting at the maximum entry ofb0 in absolute value, and
stopping after We are po］；(n)times that, and replacing all remaining entries with 0. By the triangle
inequality, we have
IAx — b00Ip ≤ IAx — b0Ip + Ib0 — b00Ip ≤ IAx — b0Ip + 12εOPT ≤ (1 + 12ε)IAx — b00Ip,
for any x ∈ Rd+1 .
Note that b00 has discretized the values of b0 into ' = O (ɪogp) possible values. We partition the
rows of A into ' groups Gι,..., g`, based on the corresponding values of b00. Suppose that for a
group Gk, the corresponding values of b00 are all tk. Then we have
IAx—b00Ipp=XX |hai,xi — tk|p .
k i∈Gk
Since A is Vandermonde, for each i ∈ Gk,
rr
/	d	∖2	/	d	∖ 2	22r(d-1)+1
(hai, xi — tk )2 = I —tk + Σai,j Xj)	= I —tk +∑>j,-1χj)	= E	ai,2 tk,j yj ,
j=1	j=1	j=1
for some fixed values tk,1, tk,2, . . . that can be computed from tk, where again each yj can be a
different function of the coordinates of x. In particular, there can be 2r choices for the exponent of
tk. For a fixed choice of the exponent oftk, the exponent of ai,2 can range from 0 to 2r(d — 1).
Hence, the '2r regression problem on a submatrix of an input Vandermonde matrix A ∈ Rn×d with
the same fixed measurement values, i.e., the corresponding coordinates of b00 are all the same, can be
reshaped to a constrained `1 regression problem on an input Vandermonde matrix with22r(d— 1)+1
columns times a (22r(d — 1)+ 1)× (22r(d — 1)+ 1) diagonal matrix.
Hence, by invoking Theorem A.2 to sample rows of M corresponding to their 'p∕2r Lewis weights
in the submatrix induced by the rows of Gk, we obtain a sampling matrix Tk such that with high
probability,
∕2r(d-1) + 1	ʌ P/2r
(1—ε)∣TkMy-TkVk|喘：≤ X I X	ajTkjyj )	≤ (1+ε)∣TkMy-T®Vk∣∣p∕2r,
i∈Gk j=1
where Vk corresponds to the vector b00 that is set to zero outside of coordinates whose values are tk.
Note that by the above argument, then we also have
(1 — ε)∣TkAx - TkVkkp ≤ X ∣hai,x〉— tk∣p ≤ (1 + ε)∣TkAx - TkVk∣p,
i∈Gk
with high probability. Summing over all k, we have
(1 — ε) X kTk Ax - Tk Vkkp ≤ XX Ihai, x〉— tk Ip ≤ (1 + ε) X ∣Tk Ax - Tk Vk |微
k	k i∈Gk	k
with high probability.
14
Published as a conference paper at ICLR 2022
Let T be the sampling matrix so that T = Ti ◦ ... ◦ t`, so that Ek ∣∣TkAx - TkVk∣∣p =
kTAx - Tb00kpp. Observe that kAx - b00kpp = Pk Pi∈Gk |hai,xi - tk|p. Hence,
(1 - ε)∣TAx - Tb00∣pp ≤ ∣Ax - b00∣pp ≤ (1 + ε)∣TAx - Tb00∣pp
and thus
(1 - ε)∣TAx - Tb00∣p ≤ ∣Ax-b00∣p ≤ (1 + ε)∣TAx - Tb00∣p.
Thus we can compute a vector xb ∈ Rd such that
∣Axb - b∣p ≤ (1 + O (ε)) min ∣Ax - b∣p.
x∈Rd
□
Before analyzing the time complexity of Algorithm 2, we first recall the following algorithms for
Vandermonde matrix-vector multiplication and approximate `p regression.
Theorem A.4 (Vandermonde Matrix-Vector Multiplication Runtime, e.g., Table 1 in (Gohberg &
Olshevsky, 1994)). The runtime of computing Ax for a Vandermonde matrix A ∈ Rn×d and a
vector x ∈ Rd for d ≤ n is O n log2 n .
Theorem A.5 (Approximate `p Regression Runtime). (Adil et al., 2019b) Given A ∈ Rn×d and
P ≥ [2, ∞), there exists an algorithm that makes O (√nlog nε) calls to a linear System SolVer and
computes a vector X such that
kAX — bkp ≤ (1 + ε) min kAx — bkp.
x∈Rd
We now analyze the runtime of Algorithm 2.
Lemma A.6. Algorithm 2 runs in O (nlog3 n) + d0.5+ω Poly (ɪ,p, logn) time.
Proof. Observe that Algorithm 2 has three main bottlenecks for runtime. Since we only need to
Lewis weight sample from the extended matrices, we do not need to explicitly form them, which
would otherwise require Ω(ndp2) timejust to list to entries. Hence the first bottleneck is performing
the Lewis weight sampling procedure on the extended matrices. The second bottleneck is solving
the `p regression problem on the final subsampled matrix. The only remaining procedure is rounding
and truncating the coordinates ofb ∈ Rn to form a vector b00 ∈ Rn using the procedure RoundTrunc
and then forming the groups Gi,..., g`, which clearly takes O (n) arithmetic operations combined.
We thus analyze each of the three main runtime bottlenecks.
First observe that the extended matrix M is a Vandermonde matrix with O dp2 columns. (Cohen
& Peng, 2015) show that O (log n) matrix-vector multiplication operations can be done to compute
approximate Lewis weights for the purposes of `p Lewis weight sampling. By Theorem A.4, each
matrix-vector multiplication uses time O n log2 n . Hence computing the extended matrix M uses
O n log3 n time. Similarly, the extended matrix for each group Gk is the product of a Vander-
monde matrix with O dp2 columns and a diagonal matrix. Thus by Theorem A.4, the extended
matrices for all the groups Gk can be formed using O n log3 n time in total.
Since each Vandermonde matrix has O dp2 rows, then observe that each group Gk samples
O (dp2 log d) rows and there are ' = O (ε log n) such groups k ∈ [']. Thus the resulting sub-
sampled matrix has O (用 log2 n) rows for d ≤ n. To approximately solve the 'p regression
problem, Theorem A.5 notes that for P ≥ 2 and a subsampled matrix of size O (用 log2 d),
We require only O (p3/d log n log，) calls to a linear system solver. Moreover, on an itera-
tion t of the `p regression algorithm of (Adil et al., 2019a) used in Theorem A.5, the linear
system solves the equation Xt J (A>RtA)-IA>Rtb for a diagonal matrix RT. Each lin-
ear system solve can be done in dω poly (∣,p, logn) time. Hence, the total time to approxi-
mately solve the 'p regression problem is d°∙5+ω poly (∣,p, logn). Therefore, the total runtime
is O (n log3 n) + d0∙5+ω poly (ε ,p, log n).	□
15
Published as a conference paper at ICLR 2022
Lemma 2.4. Let X ∈ Rn and P = Ω
. Then kxk∞ ≤ kxkp ≤ (1 + ε)kxk∞.
Proof. For any vector X ∈ Rd, we have
∣∣xk∞ = max |xi| ≤ T∣Xi∣p	≤ n1/p ∙ max |xi|.
i∈n	i∈n
Since (1 + ε)3"ps > e for all ε > 0, then (1 + ε)p > n for P = Ω (logεn). Therefore,
n1/p ∙ max ∣x∕ ≤ (1 + ε)max ∣x∕ = (1 + ε)∣x∣∞.
i∈n	i∈n
□
Corollary 2.5. Given ε ∈ (0, 1) andP ≥ 1, A ∈ Rn×dq, and b ∈ Rd, suppose A = [A1| . . . |Aq]
for Vandermonde matrices A1, . . . , Aq ∈ Rn×d. Then there exists an algorithm that, with high
probability, returns a vector Xb ∈ Rd such that
∣AXb - b∣p ≤ (1 + ε) min ∣AX - b∣p,
x∈Rd
using O (T(A)(dp)q-1 logn + poly((dp)q logn, 1∕ε)) time, where T(A) is the runtime ofmul-
tiplying the matrix A by an arbitrary vector. For q = 2, this can be further optimized to
O (ndω2/2-1 + poly((dp)2, log n, 1∕ε)) time, where O (nω2) is the time to multiply an n X n matrix
with an n × n2 matrix, so that ω2 ∈ [3, 4].
Proof. Recall that a key part in the proof of Theorem 1.1 was to first the '2r regression on a Van-
dermonde matrix with dimension Rn×d to `1 regression on a Vandermonde matrix with dimension
Rn×(2r(d-1)+1), where 2r ≤ P < 2r+1. For a matrix A with block Vandermonde structure, we can
similarly write
/ dq
(hai , Xi)	= X ai,j xj
j=1
2r
qd
ΣΣaj,-+(k-1)dxj+(k-1)d I
k=1 j=1
(2r(d-1))q+1	q
X	Y	aip,j2,k+(k-1)dyj ,
j=1	k∈[q],P pj,k =2r
where again each y7- is a fixed function of the coordinates of x. Thus We can reshape the '2r
regression problem on a matrix A with dimension Rn×dq with block Vandermonde structure to an
'1 regression problem on a matrix A with dimension Rn×(2r(d-I))q+1. Moreover, we can further
reshape A into the concatenation of (dP)q-1 Vandermonde matrices, where each Vandermonde
matrix has columns that are geometrically growing in ai,2 but are multiplied by all (dP)q-1 products
Qqk-=11 aip,k2+kd,where Pk ∈ [dP].
We can now use matrix-vector multiplication on each of the (dP)q-1 Vandermonde matrices.
Thus by Theorem A.2, we can `1 Lewis weight sample from the rows of ther reshaped A, using
O T (A)(dP)q-1 log n + (dP)ωq time. We can similarly write (hai, x - tki)2r for each tk among
the discretized values of the updated b vector as a sum of (2r(d - 1))q + 1 terms that are all
products of powers of the bases ai,2 , ai,2+d, . . . and a variables yj, as in the proof of Theorem 1.1.
Thus we can partition the '2r regression problem into ' = O (Iofn) instances of a constrained
`1 regression problem on (dP)q-1 Vandermonde matrices, each with at most 2r(d - 1))q + 1
columns. To approximately solve the `p regression problem, we can thus sample rows by their
'p∕2r Lewis weights, as in Theorem 1.1. Since there are up to (dp)q-1 Vandermonde matrices,
each with at most 2r(d - 1))q + 1 columns, then by Theorem A.2, the total time required is
O (T(A)(dρ)q-1 logn + (dp)ωq ∙ poly(logn, 1∕ε)).	□
16
Published as a conference paper at ICLR 2022
We remark that for the special case of q = 2, (Sa et al., 2018) noted an efficient bivariate matrix
multiplication algorithm of (Nusken & Ziegler, 2004; Kedlaya & Umans, 2011).
Theorem A.7. (Nusken & Ziegler, 2004; Kedlaya & Umans,2011; Sa et al., 2018) Given a q-variate
polynomial f(X1, . . . , Xq) such that each variable has degree at most d - 1 and N = dq distinct
points x(i) = (x(i)ι,...,x(i)q) for i ∈ [N], there exists an algorithm that uses O (dω2(q-1)/2+1)
time to output the vector (f (x(1)), . . . , f (x(N))), where O (nω2 ) is the time to multiply an n × n
matrix with an n × n2 matrix, so that ω2 ∈ [3, 4].
The case of a matrix-vector product for q = 2 corresponds to the evaluation ofd2 points in The-
orem A.7. Thus We need to repeat the algorithm in Theorem A.7 a total of d⅛ times to handle all
n rows in the input matrix. Since each instance of the algorithm uses O dw2/2+1 time, the total
time for the matrix-vector product is O (ndω2/2-1), rather than the naive O (nd) time (recall that
ω2 ∈ [3, 4]).
Corollary A.8. Given ε ∈ (0, 1) andp ≥ 1, A ∈ Rn×2d, and b ∈ Rd, suppose A = [A1 |A2] for
Vandermonde matrices A1, A2 ∈ Rn×d. Then there exists an algorithm that with high probability
returns a vector xb ∈ Rd such that
kAxb - bkp ≤ (1 + ε) min kAx - bkp,
x∈Rd
using O (ndω"2-1 +poly((dp)2, logn, 1∕ε)), where O (nω2) is the time to multiply an n X n
matrix with an n × n2 matrix.
A.2 `p REGESSION ON OTHER STRUCTURED INPUT
Algorithm 4 Faster regression for noisy loW-rank matrices
Input: Rank k matrix K ∈ Rn×d , matrix S ∈ Rn×d With at most s non-zero entries per roW, such
that A = K + S, measurement vector b, accuracy parameter ε > 0
Output: xb ∈ R With kAxb - bkp ≤ (1 + O (ε)) minx∈Rd kAx - bkp
1:	r — blog Pc	. 2r ≤ p < 2r+1.
2:	Extend A to a matrix M With dimension n × O ds (k + s)2r so that each entry Mi,j is the
coefficient of the j-th term in the tensor decomposition of af2r.
3:	Use 'p∕2r -Lewis weight sampling on M to find a set S of O (d0 log d0) indices in [n] and rescal-
ing factors, for d0 = O (ds(k + s)2r).
4:	Let A0 be the corresponding submatrix of A with indices in S and scaled accordingly.
5:	Compute X ≤ 5 minχ∈Rd ∣∣A0x — b∣∣p.
6:	b0 - b — AX, b00 - ROUndTrUnC(b0), ' 一 O (修)
7:	Partition the rows of A into groups Gi,..., g`, each containing all rows with the same value
of b00
8:	Let Gk be the corresponding submatrix and tk be the coordinate of b00 corresponding to Gk for
each k ∈ [`].
9:	Use 'p∕2r -Lewis weight sampling on [Gk; tk] to find a set Sk of O (∣20 log d00) indices in [n]
and rescaling factors, where d00 = O pds (k + s)2r .
10:	Let Tk be the corresponding sampling and rescaling matrix for Sk0 .
11:	T - [Ti; …;Tk]>
12:	Compute Xb ≤ (1 + ε) minx∈Rd kTAX - Tbkp.
13:	return Xb
Lemma A.9. Given ε ∈ (0, 1) and p ≥ 1, a matrix A ∈ Rn×d such that A = K + S for a
rank k matrix K and an s-sparse matrix S, and b ∈ Rd, there exists an algorithm that with high
probability, returns a vector Xb ∈ Rd such that
kAXb - bkp ≤ (1 + ε) min kAX - bkp .
x∈Rd
17
Published as a conference paper at ICLR 2022
Proof. The proof is similar to Lemma A.3. We once again let r be an integer so that 2r ≤ p < 2r+1
and observe that
X|hai,xi|p=X|(hai,xi)2r|p/2r=X|(hki+si,xi)2r|p/2r.
i∈[n]	i∈[n]	i∈[n]
Since K is a low-rank matrix, then for all ki, we can write
k
ki =	αi,jvj ,
j=1
for a fixed set of basis vectors v1, . . . , vk ∈ Rd. Hence we have
|hai,xi|p
i∈[n]
X *
i∈[n]
k
si +	αi,j vj
j=1
2r p/2r
x
Since S has sparsity s, then we can further write
|hai,xi|p=
i∈[n]	i∈[n]
k
βi,jeij +
j=1
2r
αi,j vj, x
p/2r
where e1 , . . . , ed denote the elementary vectors. By the Hadamard Product-Kronecker Product
mixed-product property, we have
X Ihai, x)|p = X ∣(yι « ... VX) © x0(2r )∣P/	,
i∈[n]	i∈[n]
where each yk ∈ {αi,1v1, . . . , αi,kvk,βi,1ei1, . . . , βi,sei,s} for i ∈ [2r]. Thus for a fixed set of
elementary vectors e^,..., eis, there are (k + s)2r possible values for the tensor product (yι 0
. . . y2r). Since there are ds choices for the elementary vectors ei1 , . . . , eis, then there are at most
(Cds(k + s)2r) possible values for the tensor product for an absolute constant C > 0. Therefore,
the 'p subspace embedding problem on A ∈ Rn×d can be reshaped as a constrained "2「subspace
embedding problem on an input matrix of size n × (Cds(k + s)2r). Hence for `p regression with
p ∈ [2r, 2r+1), we have
2r (d-1) + 1	p∕2r
X Ihai, xi∣P = X X	Mijy	,
i∈[n]	i∈[n]	j=1
which is a constrained 'p∕2r regression problem on a matrix M of size n X (Cds (k + s)2r) whose
entries can be determined from the decomposition of each row of A.
Using 'p∕2r Lewis weight sampling, Theorem A.2 implies that We can find a matrix M0 such that
11 kMykp/2： ≤ kM0ykp∕2r ≤ .kMykp/2：
for all y ∈ R(Cds(k+S)2 ) with high probability. By Lemma A.1, We can thus compute a vector X
such that
k AX - bkp ≤ 12 min ∣∣ Ax - b∣∣p.
We again set b0 = b - AX and define OPT = minχ∈Rd ∣∣ Ax - b∣p, so that
∣b0kp = ∣Ax - b∣p ≤ 12OPT.
Let b0 = b - AX and OPT = minχ∈Rd ∣∣ Ax - b∣p, so that
∣b0kp = ∣Ax - b∣p ≤ 12OPT.
18
Published as a conference paper at ICLR 2022
Let b00 be the vector with all entries of b0 rounded to the nearest power of (1 + ε), starting at the
maximum entry of b0 in absolute value, and stopping after We are p。,.) of that and replacing all
remaining entries with 0. By the triangle inequality, we have
kAx - b00kp ≤ kAx - b0kp + kb0 - b00kp ≤ kAx - b0kp + 12εOPT ≤ (1 + 12ε)kAx - b00kp,
for any x ∈ Rd+1.
Since the coordinates of b00 can have ' = O (⅛n) possible distinct values, we can partition the
rows of A into ' groups, Gi,..., g`, based on the corresponding values of b00. Let tm be the
corresponding value of b00 for all rows in a group Gm , so that
kAx - b00kpp =	|hai,xi - tm|p.
m i∈Gk
By the above argument, we have for each i ∈ Gk,
(hai, xi - tm)2r
-tm +
k	2r
βi,jeij + Xj=1αi,jvj,x
2rCds (k+s)2r
Bi,jtm,jyj,
j=1
where (1) Bi,j are entries of a matrix B with 2rCds(k+s)2 columns that can be computed from A,
(2) tm,1, tm,2, . . . are fixed values that can be computed from mk, and (3) each yj is a fixed function
of the coordinates of x. Notably, B is the matrix formed by the concatenation of the coefficients of
the decomposition of the α-fold tensor product of the row into the α-fold tensor products of the low-
rank and sparse basis elements, for each α = 0, . . . , p. By comparison, the matrix M previously
defined in this proof is only the decomposition for α = p. Hence, the '?r regression problem on a
submatrix of A ∈ Rn×d the same coordinr ate of b00, can be reshaped as a constrained `1 regression
problem on a matrix with 2rCds (k + s)2r columns.
The remainder of the proof follows from the same grouping argument as Theorem 1.1. We apply
Theorem A.2 by sampling rows of B corresponding to their 'p∕2r Lewis weights in the submatrix
induced by the rows of Gm , we obtain a matrix Tm such that with high probability,
(2rcds(k+s)2	∖p/2
(I-E)kTmBy-Tmvmkp/2r ≤ ^X I	^X	Bi,j tm,j yj I ≤ (I+D k TmBy-Tmvmllp/2r ,
i∈Gm	j=1
where vm is the vector restricted to the coordinates of b00 that are equal to tm. Conditioning on the
above inequality holding, it follows that
(1 - ε)kTmAx - Tmvmkpp ≤ X |hai,xi -tm|p ≤ (1 + ε)kTmAx - Tmvmkpp.
i∈Gm
Therefore by summing over all m ∈ [`], we have that with high probability,
(1-ε) XkTmAx-Tmvmkpp ≤ X X |hai, xi - tm|p ≤ (1 + ε) X kTmAx - Tmvmkpp.
m	m i∈Gm	m
For T = Ti ◦ ... ◦ t`, we have Em ∣∣TmAx - TmVmkp = ∣∣Tx - b00kp. Since ∣∣Ax - b00kp
Pm Pi∈Gm |hai,xi -tm|p,then
(1 - ε)∣TAx - Tb00∣pp ≤ ∣Ax-b00∣pp≤(1+ε)∣TAx-Tb00∣pp.
Therefore for p ≥ 1,
(1 - ε)∣TAx - Tb00∣p ≤ ∣Ax-b00∣p ≤ (1 + ε)∣TAx - Tb00∣p.
Because ' = O (⅛n) and P O (T(Gk)) = O (T(A)), we can compute a vector b ∈ Rd such
that
∣Axb - b∣p ≤ (1 + O (ε)) min ∣Ax - b∣p.
x∈Rd
□
19
Published as a conference paper at ICLR 2022
Lemma A.10. Given the low-rank factorization of K,	Algorithm 4 uses
n Poly (2p,ds,kp,sp, 1, logn) time.
Proof. We analyze the runtime of Algorithm 4. First note that we can compute the extended matrix
in time O (n((k + 1)pdssp)) to perform the 'p/?r Lewis weight sampling, where We recall that r is
the unique integer such that 2r ≤ p < 2r+1. To perform Lewis weight sampling on the extended
matrix, we require matrix-vector multiplication, which requires time O (nk) for a low-rank matrix
and time O (ns) for a matrix whose rows have at most s nonzero entries.
After the first iteration of Lewis weight sampling, we can round and truncate the coordinates of
b ∈ Rn to form a vector b00 ∈ Rn using the procedure RoundTrunc and then forming the
groups Gι,..., g`, which clearly takes O (n) arithmetic operations combined. Once the groups
are formed, We can compute the extended matrix in time O (n((k + 1)pdssp)) and perform 的出
Lewis weight sampling on each group, which takes O (nk + ns) total time across all groups. To
approximately solve the resulting `p regression problem formed by the subsampled rows, we require
Poly (ds, kp, sp, ε) time. Therefore, the total runtime is n Poly (2p, ds,kp,sp, ε, log n.	□
Theorem 1.5 then follows from Lemma A.9 and Lemma A.10. Theorem 1.6 is achieved through
similar analysis for a noisy Vandermonde matrix. In particular, it follows from the same proof
structure as Lemma A.3 by showing for 2r ≤ p < 2r+1, the '?r subspace embedding problem on
A = V + S for a given Vandermonde matrix V ∈ Rn×d and a sparse matrix S ∈ Rn×d can be
reshaped as a constrained `1 subspace embedding problem on an input Vandermonde matrix of size
n × d0, where d0 = (ds)(sp)(pd).
B Applications to Polynomial Regression
In the polynomial regression problem, the goal is to find a degree d polynomial q such that
kb(t)-f(t)kP ≤ (1 + ε) ∙
min
qdeg(q)≤d
kq(t) -f(t)kpp,
where ε > 0 is an accuracy parameter given as input and |卜|, is the 'p norm to the pth power,
kgkpp = R-11 |g(t)|pdt. The polynomial regression problem is a fundamental problem in statistics,
computational mathematics, machine learning, and more. The problem has been studied as early as
the 19th century with the work of Legendre and Gauss on least squares polynomial regression and
has applications in learning half-spaces (Kalai et al., 2008), solving parametric PDEs (Hampton &
Doostan, 2015), and surface reconstruction (Pratt, 1987).
Given the flexibility to choose query locations x1, . . . , xs, we can consider the polynomial regression
problem as an active learning or experimental design problem. Thus we would like to minimize the
number of queries s, as a function of the approximation degree d, the norm p, and the accuracy
parameter ε, to find q. Observe that d +1 queries are obviously necessary, but also that d +1 queries
suffice when f can be exactly fit by a degree d polynomial, by using direct interpolation. In general,
however, in the case when minq^eg(q)≤d ∣∣q(t) - f (t)kp = 0 we require s > d + 1 queries. Our
Vandermonde `p regression results can be used to give the first result showing that for all p ≥ 1,
d exp(O(p)) ∙ Poly (∣) queries suffice to obtain a (1 + ε)-approximation to the best polynomial fit.
We require the following structural theorem reducing the `p polynomial regression problem to a
problem of solving `p regression on Vandermonde matrices:
Theorem B.1. (Kane et al., 2017; Meyer et al., 2021) Suppose s1, . . . , sn0 are drawn uniformly
from [-1, 1]. Let A ∈ Rn0×(d+1) be the associated Vandermonde matrix, so that Ai,j = sij-1. Let
no = exp(O(p))O (^⅛p d5) and let b ∈ Rn0 be the evaluations of f, so that bi = f(sj. Then
with probabilitym,the Sketched solution X = argminχ ∣ Ax — b∣p satisfies
∣Px-f∣p≤(1+ε)x∈mRidn+1∣Px-f∣p.
Theorem B.1 states that we can uniformly sample exp(O(p)) ∙ Poly (d, ɪ) points from [-1,1].
We can then form an `p regression problem by using the evaluation each of the polynomial bases
20
Published as a conference paper at ICLR 2022
at the sampled points to form the design matrix A and querying the underlying signal f at the
sampled points to form the measurement vector b. Theorem B.1 Says that the optimal solution X =
argmi□x ∣∣ Ax - b∣∣p is a (1 + ε)-approximation to the best fit degree d polynomial. We can naively
approximately solve the `p regression on the Vandermonde matrix A and the measurement vector
b by standard `p regression techniques such as Lewis weight sampling, which would result in total
query complexity exp(O(p))O (ε⅛P d5). However, We can instead note that Lemma 1.4 implies
We can instead solve an 'q regression problem on a Vandermonde matrix with O (dp) columns for
q ∈ [1,2]. Crucially for q ∈ [1, 2], there exist active 'q regression algorithms that only require
reading O(d) ∙ Poly (∣) entries of b:
Theorem B.2. (Musco et al., 2021) Given p ∈ [1, 2] and an input matrix A ∈ Rn×d, there exists an
algorithm that reads O(d) ∙ Poly (∣) entries of b and with probability at least 0.99, outputs X such
that
∣AX - b∣p ≤ (1 + ε) min ∣AX - b∣p .
Hence by Theorem B.2, we can approximately solve the 'q regression on a Vandermonde matrix
with dp rows by reading O(dp) ∙ Poly (∣) entries of b. By Lemma 1.4, the approximate solution
will also be a (1 + ε)-approximation to the optimal 'p regression on the Vandermonde matrix A. By
Theorem B.1, the approximate solution will also form the coefficient vector of a polynomial that is a
(1 + ε)-approximation to the polynomial 'p regression problem. Therefore, we obtain the following
guarantees for the polynomial regression problem:
Theorem B.3. For any degree d and norm p ≥ 1, there exists an algorithm that queries f at
S = dp Poly (log(dp), ,) points and outputs a degree dpolynomial b(t) such that
kb(t)-f(t)kP ≤ (1 + ε) ∙
min
qdeg(q)≤d
∣q(t)-f(t)∣pp,
with probability at least 3.
The previous-best algorithm (Kane et al., 2017; Meyer et al., 2021) for 'p polynomial regression
sampled O(d5) points uniform from [-1, 1] and then used standard 'p regression algorithms that
required reading the signal at all O(d5) sampled points, for a total query complexity of O(d5).
By comparison, Theorem B.3 only has linear dependency in d due to the structural property of
Lemma 1.4. Since Ω(d) queries are clearly necessary, our result settles the dependency of d in the
query complexity for 'p polynomial regression for all d and allp ≥ 1.
C Additional Experiments
In this section, we experimentally verify that 'p/2r Lewis Weight sampling works for unstructured
matrices. We take a similar approach as in Section 3 to verify that 'p/2r Lewis Weight sampling
is correct for 'p regression on unstructured matrices. For this test, we fix n = 25, 000, d = 10,
and p = 6, while varying m ∈ [1, 1000]. We let A = G01 G0 where G1 ∈ R100×6 and G2 ∈
R24,900×4 are i.i.d. N(0, 1) matrices. To generate b, we sample a vector X ∈ R10 whose first 6
entries are N(0, 1002) and remaining 4 entries are N(0, 1), and let b = AX + z where z ∈ R25,000
is a N(0, 1) iid vector.
We generate this matrix A and response vector b just once and run 'p/2r Lewis Weight sampling
many times, so the variance in the plot comes only from the random sampling algorithms. Note that
we again omit the rounding procedure on b. Figure 3 shows the result of this test, and we clearly
see that the error shrinks quickly in m for our algorithm. This approach is much more practical
than the prior Lewis Weight approximation method for unstructured matrices when p > 4. That
approach required solving a non-linearly constrained SDP O (log n) times (Cohen & Peng, 2015),
while our method requires only a Gaussian sketch matrix and the standard Lewis Weight Iteration,
which converges very quickly.
Since b is so large on its first 100 entries, it is important for any subsampling algorithm to sample
at least 6 of the first 100 rows. Uniform sampling picks none of these rows until m ≈ 25000 = 250,
which is why uniform sampling fails to converge to a good solution for small m. Lewis weight
sampling instead gives much higher priority to the first 100 rows, avoiding any issue. This is why
the the gap between Lewis Weight sampling and uniform sampling is so large for this experiment.
21
Published as a conference paper at ICLR 2022
Fig. 3: Empirical Relative Error (εempirical) versus subsample complexity m. We ran the experiment
50 times and plot the median, 25th quartile, and 75th quartile for each value of m.
D Additional Related Work
As previously mentioned, subspace embeddings are common tools used to approximately solve `p
regression. Given an input matrix A ∈ Rn×d, a subspace embedding is a matrix M ∈ Rm×d with
m n such that
(1 - ε)kAxkp ≤ kMxkp ≤ (1 + ε)kAxkp,
for all x ∈ Rd. Thus given an instance of `p regression, where the goal is to minimize kAx - bkp,
we can set B = [A; b], compute a subspace embedding for B, and then solve a constrained `p
regression problem on the smaller matrix B.
A subspace embedding of a matrix A ∈ Rn×d can be formed by sampling rows of A with proba-
bilities proportional to their `p leverage scores, e.g., (Cormode et al., 2018; Dasgupta et al., 2008)
their `p sensitivities, e.g., (Clarkson et al., 2019; Braverman et al., 2020; 2021; Musco et al., 2021);
or their `p Lewis weights, e.g., (Cohen & Peng, 2015; Durfee et al., 2018; Chhaya et al., 2020; Chen
& Derezinski, 2021; Parulekar et al., 2021; Meyer et al., 2021). In any of these cases, the result-
ing matrix M will contain a subset of rows of A that are rescaled by a function of their sampling
probability, as to give an unbiased estimate of the actual `p mass.
In addition to sampling methods, sketching is a common approach for subspace embeddings. In
these cases, the subspace embedding M is formed by setting M = RA for some (often random)
matrix R ∈ Rm×n. The advantage of sketching over sampling is that sometimes Rcan be computed
oblivious to the structure ofA, whereas the sampling probabilities for each of the above distributions
(`p leverage scores, `p sensitivities, and `p Lewis weights) are data dependent and thus require a
pass over the matrix A. The sketching matrix can be generated from a family of random matrices
whose entries are Cauchy random variables for p = 1, e.g., (Sohler & Woodruff, 2011; Meng &
Mahoney, 2013; Clarkson et al., 2016), or SUb-GaUssian random variables for P = 2, e.g., (Sarlos,
2006; Nelson & Nguyen, 2013; Clarkson & Woodruff, 2013). More generally, exponential random
variables can be Used for p ≥ 2, thoUgh the nUmber of rows m in the resUlting sketch matrix now
has a polynomial dependency in n (WoodrUff & Zhang, 2013). A line of recent works has stUdied
the tradeoffs between oblivioUs linear sketches, sampling-based algorithms, and other sketches for
`p sUbspace embeddings (Wang & WoodrUff, 2019; Li et al., 2021). In this paper, we focUs on
sampling-based algorithms for `p regression dUe to preservation of strUctUre when the inpUt matrix
A itself has strUctUre.
22