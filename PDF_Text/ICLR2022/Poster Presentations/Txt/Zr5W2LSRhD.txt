Published as a conference paper at ICLR 2022
Constructing Orthogonal Convolutions in
AN EXPLICIT MANNER
Tan Yu, Jun Li, Yunfeng Cai, Ping Li
Cognitive Computing Lab
Baidu Research
10900 NE 8th St. Bellevue, Washington 98004, USA
{tanyu01,lijun12,caiyunfeng,liping11}@baidu.com
Ab stract
Convolutions with orthogonal input-output Jacobian matrix, i.e., orthogonal con-
volution, have recently attracted substantial attention. A convolution layer with
an orthogonal Jacobian matrix is 1-Lipschitz in the 2-norm, making the output
robust to the perturbation in input. Meanwhile, an orthogonal Jacobian matrix
preserves the gradient norm in back-propagation, which is critical for stable train-
ing deep networks. Nevertheless, existing orthogonal convolutions are burdened
by high computational costs for preserving orthogonality. In this work, we exploit
the relation between the singular values of the convolution layer’s Jacobian and
the structure of the convolution kernel. To achieve orthogonality, we explicitly
construct the convolution kernel for enforcing all singular values of the convolu-
tion layer’s Jacobian to be 1s. After training, the explicitly constructed orthogonal
(ECO) convolutions are constructed only once, and their weights are stored. Then,
in evaluation, we only need to load the stored weights of the trained ECO convo-
lution, and the computational cost of ECO convolution is the same as the standard
dilated convolution. It is more efficient than the recent state-of-the-art approach,
skew orthogonal convolution (SOC) in evaluation. Experiments on CIFAR-10 and
CIFAR-100 demonstrate that the proposed ECO convolution is faster than SOC in
evaluation while leading to competitive standard and certified robust accuracies.
1	Introduction
A layer with an orthogonal input-output Jacobian matrix is 1-Lipschitz in the 2-norm, robust to the
perturbation in input. Meanwhile, it preserves the gradient norm in back-propagating the gradient,
which effectively overcomes the gradient explosion and attenuation issues in training deep neural
networks. In the past years, many studies have shown that exploiting orthogonality of the Jaco-
bian matrix of layers in neural networks can achieve provable robustness to adversarial attacks (Li
et al., 2019), stabler and faster training (Arjovsky et al., 2016; Xiao et al., 2018), and improved
generalization (Cogswell et al., 2016; Bansal et al., 2018; Sedghi et al., 2019).
In a fully-connected layer y = Wx where W ∈ Rcout ×cin is the weight matrix, the layer’s input-
output Jacobian matrix J =祭 is just W. Thus, preserving the orthogonality of J can be accom-
plished through imposing the orthogonal constraint on W, which has been extensively studied in
PreVioUS works (Mhammedi et al., 2017; CiSSe et al., 2017; Anil et al., 2019). In contrast, in a con-
volution layer, the Jacobian matrix is no longer the weight matrix (convolution kernel). Instead, it
is the circulant matrix composed of convolution kernel (Karami et al., 2019; Sedghi et al., 2019).
Thus, generally, simply constructing an orthogonal convolution kernel cannot achieve an orthogonal
convolution. Achieving an orthogonal Jacobian matrix in the convolution layer is more challenging
than that in a fully-connected layer. It is plausibly straightforward to expand the convolution kernel
to the doubly block circulant Jacobian matrix and impose the orthogonal constraint. But it is difficult
to construct a Jacobian matrix that is both doubly block-circulant and orthogonal.
Block Convolutional Orthogonal parameterization (BCOP) (Li et al., 2019) is one of the pioneering
works for constructing the orthogonal convolution neural networks. It adopts the construction algo-
rithm (Xiao et al., 2018) which decomposes a 2-D convolution into a stack of 1-D convolutions and
a channel-wise orthogonal transformation. Trockman & Kolter (2021) maps the convolution kernel
1
Published as a conference paper at ICLR 2022
and the feature tensor into the frequency domain using Fast Fourier Transform (FFT), and achieves
the orthogonality of the Jacobian matrix through Cayley transform on the weight matrix in the fre-
quency domain. They devise the convolution kernel of the same size as the input feature map, which
takes more parameters than standard convolution layers with a local reception field. Meanwhile,
FFT maps the real-value feature map and the weight matrix into matrices of complex values, in-
creasing the computational cost to 4 times its counterpart in the real-value domain. Meanwhile, the
Cayley transform requires computing the matrix inverse, which is not friendly for GPU computation.
Recently, Skew Orthogonal Convolutions (SOC) (Singla & Feizi, 2021b) devises skew-symmetric
filter and exploits matrix exponential (Hoogeboom et al., 2020) to attain the orthogonality of the
Jacobian matrix. But SOC is slow in evaluation since it needs to apply a convolution filter multi-
ple times sequentially on the feature map to obtain the Taylor expansion. Observing the efficiency
limitations of the existing methods, in this work, we propose an explicitly constructed orthogonal
(ECO) convolution, which is fast in both training and evaluation.
It is well known that the Jacobian matrix of a layer is orthogonal if and only if each of its singular
values is 1. Thus we convert the problem of ensuring the orthogonality of the Jacobian matrix
into making every singular value as 1. Based on the relation between the singular values of the
Jacobian matrix for the convolution layer and the weight matrix of the convolution layer discovered
by Sedghi et al. (2019), we construct the convolution kernel so that it ensures every singular value
of the Jacobian matrix to be 1. Compared with the recent state-of-the-art method, SOC (Singla
& Feizi, 2021b) implicitly approximating orthogonal convolution by multiple times convolution
operations, ours explicitly builds the orthogonal convolution. Thus, we can directly deploy the
constructed orthogonal convolution in evaluation, taking the same computational cost as the standard
convolution. It is more efficient than SOC with multiple times convolution operations in evaluation.
Experiments on CIFAR10 and CIFAR100 show that, in evaluation, taking less time, ours achieves
competitive standard and robust accuracy compared with SOC.
2	Related Work
Weight orthogonalization. In a fully-connected layer, the Jacobian matrix is the weight matrix
itself. Thus many efforts are devoted to orthogonalizing the weights. Early works exploit an or-
thogonal weight initialization to speed up the training (Saxe et al., 2014; Pennington et al., 2017)
and build deep neural networks (Xiao et al., 2018). Recently, more efforts are devoted to exploiting
the orthogonality in training. Some approaches propose “soft” constraints on the weights. For ex-
ample, Bansal et al. (2018); Xiong et al. (2016) introduce mutual coherence and spectral restricted
isometry as a regularization on the weight matrix. Parseval Networks (Cisse et al., 2017) adapts a
regularizer to encourage the orthogonality of the weight matrices. However, these methods cannot
guarantee the exact orthogonality of the weight matrix. Some approaches orthogonalize features
during the forward pass. For example, Huang et al. (2018b) extends Batch Normalization (Ioffe
& Szegedy, 2015) with ZCA; Huang et al. (2018a) solves a suboptimization problem to decorre-
late features. These methods cannot avoid computationally expensive operations like SVD. Other
approaches enforce orthogonality by the Riemannian optimization on the Stiefel manifold. For
example, Casado & Martinez-Rubio (2019) uses Pade approximation as an alternative to matrix
exponential mapping to update the weight matrix on the Stiefel manifold. Li et al. (2020) uses an
iterative Cayley transform to enforce weight matrix orthonormal. Anil et al. (2019) also attains
orthogonal weights by orthogonal parameterization based on Bjorck orthonormalization (BjOrck &
Bowie, 1971). Lastly, some approaches incorporate orthogonal constraint into the network archi-
tecture. For example, Mhammedi et al. (2017) builds orthogonal layers in RNNs with householder
reflections.
Orthogonal convolution. In a convolution layer, the Jacobian matrix is no longer the weight matrix.
Thus, the above-mentioned weight orthogonalization methods cannot be trivially applied in convo-
lution layers. Wang et al. (2020); Qi et al. (2020) encourage the orthogonality of the Jacobian matrix
of convolution layers through a regularizer, but they cannot achieve the strict orthogonality and can-
not attain the provable robustness. Block Convolutional Orthogonal parameterization (BCOP) (Li
et al., 2019) is a pioneering work for enforcing the orthogonality of the Jacobian matrix of a convo-
lution layer. It conducts parameterization of orthogonal convolutions by adapting the construction
algorithm proposed by Xiao et al. (2018). But BCOP is slow in training. Trockman & Kolter (2021)
applies the Cayley transform to a skew-symmetric convolution weight in the Fourier domain so that
2
Published as a conference paper at ICLR 2022
the convolution recovered from the Fourier domain has an orthogonal Jacobian matrix. Though it
achieves higher efficiency in some applications than BCOP, it is still significantly more costly than
standard convolutional layers. Skew Orthogonal Convolution (Singla & Feizi, 2021b) achieves or-
thogonal convolution through Taylor expansion of the matrix exponential. It is faster than BCOP in
training but slower in evaluation. In contrast, our method is efficient in both training and evaluation.
Recently, Su et al. (2021) achieves orthogonal convolutions via paraunitary systems.
3	Preliminary
Notation. For a matrix M, M[i, j] denotes the element in the i-th row and the j-th column, M[i, :]
denotes the i-th row and M[:, j] denotes the j-th column. We use the similar notation for indexing
tensors. For n ∈ N, [n] denotes the set of n numbers from 0 to n - 1, that is, {0,1,…，n - 1}.
We denote the function to obtain the singular values by σ(∙). Let vec(∙) denotes the function which
unfolds a matrix or higher-order tensor into a vector. Let ωk = exp(2πi∕k) where i = √-1. Let
Fk ∈ Rk×k be matrix for the discrete Fourier transform (DFT) for k-element vectors and each entry
of Fk is defined as Fk [p, q] = ωkpq.
3.1	Relation between the Jacobian matrix and the weight matrix
We denote the input tensor of a convolution layer by X ∈ Rc×n×n, where n is the spatial size, and c
is the number of input channels. We denote the convolution operation on X by ConvW (X), where
W ∈ Rk×k×c×c size where k is the size of the reception field and k is normally smaller than size of
the input image, n. In this definition, we set the number of output channels identical to that of input
channels. Meanwhile, we set the convolution stride as 1 and adapt the cyclic padding to make the
output of the same size as the input.
We denote the output tensor by Y ∈ Rc×n×n and each element of Y is obtained by
∀l, s, t ∈ [c] × [n] × [n], Y [l, s, t] =XXXX[l
+ r,s + p,t + q]WW[p, q,l,r].	(1)
r∈[c] p∈[k] q∈[k]
Since the convolution operation is a linear transformation, there exists a linear transformation M ∈
Rn2c×n2c which satisfies:
Y = ConvW(X) ⇔ vec(Y) = Mvec(X).	(2)
To simplify illustration, W ∈ Rk×k×c×c is normally expanded into W ∈ Rn×n×c×c through zero-
padding. We term W as the expanded convolution kernel. In W, only k2c2 elements are non-zero.
Based on the above definition, we review the following theorem in Sedghi et al. (2019):
Theorem 1 (see Sedghi et al. (2019) Section 2.2). For any expanded convolution kernel W ∈
Rn×n×c×c, let M is the matrix encoding the linear transformation of a convolution with W. For
each p, q ∈ [n] × [n], let P(p,q) be the c × c matrix computed by
∀s,t∈ [c] × [c], P(p,q)[s, t] = (Fn>W [:,:, s, t]Fn)[p, q].	(3)
Then
σ(M) =	[	σ(P(p,q) ).	(4)
p∈[n],q∈[n]
The above theorem gives the relation between the singular values of the transformation matrix M
and the expanded convolution W .
Theorem 2. For any W ∈ Rn×n×c×c, the Jacobian matrix of the convolution layer, J, is orthogonal
if and only if ∀p, q ∈ [n] × [n], each singular value of the matrix P(p,q) is 1.
Proof. A real matrix is orthogonal if and only if each singular value is 1. Meanwhile, the Jacobian
matrix J =
∀p, q ∈ [n] ×
∂νec(Y)
∂vec(X)
M. Based on Theorem 1, each singular value of M is 1 if and only if
[n], each singular value of the matrix P(p,q) is 1, thus we complete the proof.
3
Published as a conference paper at ICLR 2022
3.2	Matrix Orthogonalization
Carley transform. Orthogonal parameterization has been extensively studied in the previous
works. Many works (Absil et al., 2008; Helfrich et al., 2018; Maduranga et al., 2019) utilize Cayley
transform on skew-symmetric matrices (A>= -A) to obtain the orthogonal weight matrices. Cay-
ley transform is a bijection between skew-symmetric matrices and orthogonal matrices without -1
eigenvalues. Specifically, it maps a skew-symmetric matrix A to an orthogonal matrix Q by
Q=(I-A)(I+A)-1.	(5)
Li et al. (2020) proposes an iterative approximation of the Cayley transform for orthogonally-
constrained optimizers and achieves higher speed for training CNNs and RNNs.
Matrix exponential. Casado & Martinez-Rubio (2019) exploits the matrix exponential of the
skew-symmetric matrix for orthogonality. Specifically, ifA is a skew-symmetric matrix, exp(A) is
orthogonal. SOC (Singla & Feizi, 2021b) approximates exp(A) through Taylor series exp(A) ≈
ST (A) = PT=O %,and derives a bound on the approximation error:
kexp(A)- St(A)∣∣2 ≤ 卑T,	(6)
where ∣∣ ∙ k2 is the spectral norm, the maximum singular value of a matrix. Since the approximation
error is bounded by kAk2, SOC adopts the spectral normalization on A to make kAk2 small.
Orthogonality of convolution. In a fully-connected layer, its Jacobian matrix is the transpose of
the weight matrix. Thus we can achieve orthogonality of the Jacobian matrix through orthogonal
parameterization on the weight matrix through common orthogonal parameterization methods such
as Cayley transform or matrix exponential of the skew-symmetric matrix. However, in a convolution
layer, the Jacobian matrix is no longer the transpose of the weight matrix; hence we cannot just apply
orthogonal parameterization on the weight matrix to achieve orthogonality of the Jacobian matrix.
3.3	Lipschitzness and Provab ly Robustness
Lipschitz constant. A function f : Rm → Rn is L-Lipschitz under the `2 norm iff ∣f (x) -
f (y)∣2 ≤ L∣x - y∣2, ∀(x, y) ∈ Rm × Rm, where L is a positive constant. We define the smallest
value of L satisfying L-Lipschitz for f as the Lipschitz constant of f, denoted by Lip(f). Given
two Lipschitz continuous functions f and g, the Lipschitz constant of the a composition of them,
Lip(f ◦ g), is upper-bounded by the product of the Lipschitz constants of f and g (Weaver, 2018):
Lip(f ◦ g) ≤ Lip(f)Lip(g).	(7)
Thus, given a network n(∙) consisting of L layers, if the Lipschitz constant of each layer is 1, the
Lipschitz constant of the whole network, Lip(n) is less than 1.
Provably robustness. Considering a neural network n(∙) for c-class classification. Given an input
x with the ground-truth label y ∈ [c], the network generates a c-dimensional prediction vector
n(x) = [si,... ,sc] where the i-th item in n(x) denotes the confidence score for the class i. The
margin of prediction for X by n(∙) is defined as
Mn(x) = max(0, sy - maxi6=y si ).	(8)
As proved by Tsuzuku et al. (2018) and Li et al. (2019), for a network n(∙) with a Lipschitz constant
of L, X is provably robustly classified by n(∙) under perturbation with a '2-norm of M√2Lχ), that is,
arg max n(x + δ) = y, ∀∣δ∣2 < Mn(X).	(9)
i	2L
4 Method
Based on Theorem 2, to achieve an orthogonal Jacobian matrix J for a convolution layer, we need
to ensure that, ∀p, q ∈ [n] × [n], each singular value of the matrix P(p,q) is 1. When P(p,q) is a
real matrix, this condition is equivalent to the orthogonality of P(p,q). To simplify the notation, we
assemble the matrices P(p,q) (p, q ∈ [n] × [n]) into a tensor P ∈ Rn×n×c×c such that P[p, q, :, :] =
P(p,q). Based on the definition, we rewrite the transformation from W to P(p,q) in Eq. (3) into
∀s, t ∈ [c] × [c], P[:, :, s,t] = Fn>W[:, :, s, t]Fn.	(10)
4
Published as a conference paper at ICLR 2022
Since Fn is the full-rank square matrix, the mapping from W to P is invertible. Thus, to achieve an
orthogonal Jacobian matrix, we can directly construct orthogonal P [p, q, :, :] (p, q ∈ [n] × [n]) using
Carley transform or matrix exponential of skew-symmetric matrices (introduced in Section 3.2) to
make each singular value of them identical to 1. Then the convolution kernel weight W can be
determined uniquely from the P through the inverse transformation in Eq. (10). Since Fn is the
Discrete Fourier Transform (DFT) matrix, Fn>W[:, :, s, t]Fn in Eq. (10) is equivalent to conduct-
ing a 2-dimensional DFT on W [:, :, s, t]. Thus, we can recover W [:, :, s, t] through the inverse
2-dimensional DFT (IDFT2D) on the constructed orthogonal matrix P[:, :, s, t]. That is,
W[:,:,s,t] =IDFT2D(P[:,:,s,t]), ∀s,t∈ [c] × [c].	(11)
Based on Theorem 2, the Jacobian matrix of the convolution layer with the kernel W is orthogonal.
Nevertheless, the above approach has many drawbacks. First of all, for a generally constructed
P, the recovered expanded convolution kernel W computed based on the inverse 2-dimensional
DFT might contain n2c2 non-zeros elements. It significantly increases the number of parameters in
the convolution layer from k2c2 to n2c2. Meanwhile, the convolution with kernel W has a global
reception field, which is also slower than the original convolution kernel with a local reception
field. Besides, the above approach needs to compute n2 times matrix orthogonalization (Carley
transform or matrix exponential) on P[p, q, :, :] (p, q ∈ [n] × [n]). Considering the spatial size
of the input feature tensor, n and the input/out channel number c are not a small value, n2 times
orthogonalization on c × c matrices might be computationally expensive.
Thus, to achieve a high efficient orthogonal convolution, we devise a 2-dimensional periodic P . We
set the period as k and assume n is divisible by k. Specifically, P satisfies
P[p,q, :, :] = P[p + i * k,q + j * k,:,:],	∀[p,q]	∈	[k]	X	[k],	[i,j] ∈	[n/k]	X	[n/k].	(12)
We define P0 = P[0:k, 0:k, :, :] ∈ Rk×k×c×c. That is, P can be obtained by repeating P0 by
n/k X n/k times in the first two dimensions. It has many benefits by the 2-dimensional periodic
configuration. First of all, based on the property of DFT, the convolution kernel W[:, :, s, t] obtained
from an inverse 2D DFT on the 2-dimensional periodic P[:, :, s, t] on contains only k2 non-zero
elements. That is, the number of non-zero elements in the recovered W is only k2c2 , which is
the same as the number of elements in original convolution kernel W with a local reception field
of k X k size. Besides, the non-zero elements in W can be efficiently obtained. To be specific,
we define W0 ∈ Rk×k×c×c obtained from W0 [:, :, s, t] = IDFT2D(P0[:, :, s, t]). Based on the
property of DFT (Nussbaumer, 1981), W and W0 have the following relation:
W[i,j, :, :]
Wo[ink,嗒,:,:],if i%k = 0 and j%k = 0,
0c×c ,	otherwise.
(13)
That is, we can obtain the non-zero elements of W ∈ Rn×n through computing W0 ∈ Rk×k,
taking only k2 times 2D FFT on c X c matrices. Since P is periodic along the first two dimensions, to
achieve an orthogonal Jacobian matrix based on Theorem 2, we only need to explicitly construct k2
orthogonal matrices P0[p, q, :, :] ((p, q) ∈ [k] X [k]) to ensure that n2 matrices P[p, q, :, :] ((p, q) ∈
[n] x [n]) are orthogonal. As the recovered W[i,j,:,:] is non-zero only if i%k = 0 and j% k = 0.
This property corresponds to the dilated convolution (Yu & Koltun, 2016). Specifically, using the
relation between W and W0 in Eq. (13), the following equation holds:
ConvW (X)
ConvWO (X, dilation = n).
0k
(14)
That is, we can achieve the orthogonal convolution through a dilated convolution with the kernel W0
and the dilation n/k. Based on above analysis, the process of achieving the orthogonal convolution
is decomposed into three steps:
1.	Construct the orthogonal matrices P0 [p, q, :, :], ∀(p, q) ∈ [k] X [k].
2.	Recover W0[:, :, s, t] from P0[:, :, s, t] using inverse 2D DFT, ∀(s, t) ∈ [c] X [c].
3.	Conduct the dilated convolution using the recovered kernel W0 and the dilation n/k.
Below we introduce the details in these three steps individually.
5
Published as a conference paper at ICLR 2022
Orthogonal parameterization for P0 [p, q, :, :]. This step can be implemented by any matrix or-
thogonalization approach such as Carley Transform on skew-symmetric matrix or matrix exponen-
tial of the skew-symmetric matrix. By default, we adopt the Taylor expansion for the exponential
of the skew-symmetric matrix proposed in SOC (Singla & Feizi, 2021b) in this step considering
its simplicity and efficiency. To be specific, we start from a parameter tensor P0 ∈ Rk×k×c×c .
∀ (p, q) ∈ [k] × [k], we compute
P 0[p, q, :, :] J P0[p, q, :, :] - P0[p, q, :, :]>,
P0[p,q,:,:] J XXX P0[p,.q,:, :]i.	(15)
i!
0=0
Recall from Eq. (6) that the approximation error between exp(A) and the Taylor expansion Sk(A),
∣∣exp(A) 一 Sk(A)∣∣2, is bounded by kAkk-. That is, When ∣∣Ak2 < 1, it can achieve a good ap-
proximation. Thus, folloWing SOC (Singla & Feizi, 2021b), We conduct spectral normalization on
P0[p, q,:,:] so that kexp(P0[p, q,:,:]) 一 PT=O P0[p^]i ∣2 ≤ kP⅛^k =吉.
Recover W0 from P0. ∀(s, t) ∈ [c] × [c], inverse 2D DFT is on P0[:, :, s, t] to obtain W0[:, :, s, t]:
W0[:,:,s,t] = (Fk>)-1P0[:,:, s,t]Fk-1 = IDFT2D(P0[:,:, s,t]).	(16)
As W 0 is the convolution kernel for the further convolution, to make the convolution compatible
With mainstream real-value convolutional neural netWorks, We should keep W0 real. Based on the
property of DFT, for inverse 2D discrete Fourier transform, the output Y ∈ Rk×k if purely real if
and only if the input X is conjugate-symmetry, that is, X[i, j] = X* [(k-i)%k, (k-j)%k] for each
(i,j) ∈ [k] × [k]. Thus, to make W0 real, we need set Po[i,j,:,:] := P0[(k-i)%k, (k-j)%k,:
, :] for each (i, j) ∈ [k] × [k]. Since P0 is real-value, We just need to set P0 [i, j, :, :] and
Po[(k-i)%k, (k-j)%k,:,:] identical. Due to the conjugate-symmetry settings, among these k2
matrices {P0 [(i, j, :, :]}0k=,k1,j=1, there are only L different matrices. L is (k2 + 1)/2 when k is odd
and (k2 + 4)/2 when k is even. In the implementation, we encode all the information ofP0 through
a L × C × C tensor Po. Then we build P0 from P0 by a mapping matrix I ∈ Rk ×k:
P 0[i,j,:,:] = P 0[I[i,j ],:,:], ∀(i,j) ∈ [k] X [k],	(17)
Algorithm 1: The proposed efficient orthogonal convolution. In training, the operations in lines
1-14 are involved in orthogonal convolution. In contrast, in evaluation, only the operation in
line 14 is conducted, the operations in lines 1-13 have been conducted before evaluation and the
obtained W has been stored in the model for further evaluation.
Input: Input tensor X ∈ Rn×n×c where c is the number of input channels, the spatial size of the
convolution, k, the parameter tensor P0 ∈ RL×c×c (if k is odd, L is k-2+1 else ʒ++4)
00
<P<P<P
Output: Output tensor Y ∈ Rn×n×c from conducting orthogonal convolution on X.
1 for all i ∈ [L] do
τ	dr. -, ʌ r .	-,~Γ
,：,：]J P0[i, ：, ：] - P0[i,：,：]
1	. C	j IAT	/∕T> Γ "	1 ∖
, :, :] J SpectralNorm(P 0[i, :, :])
- ∙] J PT . Po[i，:，:]i
,:，:]J 2=0=0	i!
5	end
6	I = ConstructMap(k) % construct the mapping matrix I ∈ Rk×k
7	P 0 = zeros(k, k, c, c) ; W0 = zeros(k, k, c, c)
8	for all (i, j) ∈ [k] × [k] do
9	I Po[p, q, ：, ：] = Po[I[p, q],：,：]
10	end
11	for all (s, t) ∈ [c] × [c] do
12	W0[:,:, s,t] = IDFT2d(Po[:, :, s,t])
13	end
14	Y = ConvW0 (X, dilation = n/k)
15	return Y
6
Published as a conference paper at ICLR 2022
where I is a mapping matrix, which maps a pair (i, j) ∈ [k] × [k] to a scalar l ∈ [1, L] and it satisfies
I[i,j] = I[(k-i)%k, (k-j)%k], ∀(i, j) ∈ [k] × [k].	(18)
The details of constructing the mapping matrix I is provided in Algorithm 2 of Appendix A.
Dilation convolution. In this step, we conduct the standard dilated convolution using the recovered
kernel W0 obtained in Eq. (16) with a dilation n/k on the input X and obtain the output Y.
We summarize the proposed model in Algorithm 1. In each training iteration, given a batch of B
samples, the lines 1-13 are only conducted once, and the obtained W0 is shared across B samples.
After training, we store the computed W0 in line 12. In evaluation, we load the stored W0 and only
compute the operation in line 14, which is just vanilla dilated convolution and thus is very efficient.
In the current configuration of Algorithm 1, we set P0[p, q, :, :] ∈ Rc×c to be real matrix. In fact,
P0[p, q, :, :] can also be complex matrix. To make the output of the proposed orthogonal convolu-
tion compatible with existing mainstream real-value neural network, W0 [:, :, i, j] = IDFT2DP0 [:
, :, i, j] should be a real matrix. That is, we should set P0[:, :, i, j] to be conjugate symmetric and
P0[:, :, i,j] is unnecessarily real. When considering P0[p, q, :, :] as a complex matrix, it makes the
constructed orthogonal convolution kernel W0 more complete. Meanwhile, it brings more compu-
tational cost in training. In Appendix B, we compare with using a complex P0 [p, q, :, :].
5 Implementation
Spectral normalization. For a small approximation error from Taylor expansion, we conduct
spectral normalization on P0[p, q, :, :] to make kP0[p, q, :, :]k2 ≤ 1. Following Miyato et al. (2018);
Singla & Feizi (2021a;b), we use the power method to estimate the approximated spectral norm and
then divide P0 [p, q, :, :] by the estimated spectral norm.
Padding. Following SOC (Singla & Feizi, 2021b), we use cyclic padding to substitute zero-
padding since zero-padded orthogonal convolutions must be size 1 (see proof in Appendix D.2 of
SOC (Singla & Feizi, 2021b)). To be specific, for a dilated convolution of k × k size with a dilation
d, we add d(k - 1)/2 paddings on each side in a cyclic manner.
Input and output channels. The proposed orthogonal convolution is devised under the condition
that the number of channels in input is equal to that in output, that is, cin = cout = c. When cin 6=
cout, following SOC (Singla & Feizi, 2021b), we additionally conduct the following operations:
1.	if cin > cout, we apply our orthogonal convolution with a kernel size of k × k × cin × cin,
generating the output with cin channels. We only select the first cout channels in the output.
2.	if cin < cout , we use zero pad the input with cout - cin channels. Then we apply our
orthogonal convolution with a kernel size of k × k × cout × cout .
Strided convolution. Given an input X ∈ Rn×n×cin , a convolution with stride s generates the
output Y ∈ R S × S ×cout. Our convolution is designed for 1-stride convolution. For a more general
s-stride convolution, we decompose it into a down-sampling module and an orthogonal convolution,
following BCOP (Li et al., 2019) and SOC (Singla & Feizi, 2021b). Specifically, the down-sampling
module adapts the invertible down-sampling (Maduranga et al., 2019), which simply reshapes the
input feature map X from Rn×n×cin to RS ×S×cins . The invertible down-sampling decreases the
spatial size of the feature map from k × k to k/s × k/s and meanwhile strictly preserves the gradient
norm. Then we conduct the proposed 1-stride orthogonal convolution on the reshaped X.
1-Lipschitz architecture. Following SOC (Singla & Feizi, 2021b), we build a provably 1-Lipschitz
network. The details of the architecture are given in Table 1. It consists of L convolution layers
in 5 blocks, and each block contains L/5 layers. By default, we only deploy the proposed ECO
convolution in the last three blocks and keep the first two blocks as SOC. Each block consists of two
groups of convolutions layers. The first group repeats L/5 - 1 convolutions layers with stride 1,
and the second group contains a single convolution layer with stride 2. Following SOC, a MaxMin
activation function (Anil et al., 2019) is added after each convolution layer. It has been implemented
in the PaddlePaddle deep learning platform https://www.paddlepaddle.org.cn.
7
Published as a conference paper at ICLR 2022
	Input Size	Output Size	Group 1		Group 2	
			Convolution	Repeats	Convolution	Repeats
Block 1	24 X 24	12 × 12	conv[3, 32,1]	(L/5 - 1)	conv[3, 64, 2]-	1
Block 2	12 × 12	6 × 6	conv[3, 64, 1]	(L/5 - 1)	conv[3, 128, 2]	1
Block 3	6 × 6	3 × 3	conv[3, 128, 1]	(L/5 - 1)	conv[3, 256, 2]	1
Block 4	3 × 3	1 × 1	conv[3, 256, 1]	(L/5 - 1)	conv[3, 512, 3]	1
Block 5	1 × 1	1 × 1	conv[1, 512, 1]	(L/5 - 1)	conv[1,1024, 1]	1
Table 1: The configurations of the provably 1-Lipschitz architecture with L convolution layers. It
consists of 5 blocks. Each block contains two groups of convolution layers. The first group contains
(L/5 - 1) convolution layers of stride 1, and the second group contains a single convolution layer
of stride 2. The output size decreases to 1/2 after each block. conv [k, c, s] denotes the convolution
with the kernel of spatial size k × k, output channels c, stride s. The dilation of a convolution layer
is n/k where n is the spatial size of the input, and k is the spatial size of the convolution kernel.
6 Experiments
Settings. The training is conducted on a single NVIDIA V100 GPU with 32G memory. Follow-
ing Singla & Feizi (2021b), the training takes 200 epochs. The initial learning rate is 0.1 when the
number of convolution layers is larger than 25 and 0.05 otherwise, and it decreases by a factor of
0.1 at the 50th and the 150th epoch. We set the weight decay as 5e-4.
Evaluation metrics. We evaluate the performance proposed orthogonal convolution by two met-
rics: standard recognition accuracy and the robust recognition accuracy. The standard recognition
accuracy is the same as that used in classic image classification. In contrast, the robust recognition
accuracy is the accuracy under an l2 perturbation. As shown in Section 3.3, L-LiPschitz network
has the robustness certificate when the '2-norm perturbation e is less than Mn(x)/√2L. Thus,
for a 1-Lipschitz we built based on the proposed ECO convolution has the robustness certificate as
Model	Conv. TyPe	CIFAR10		CIFAR100		Time per Epoch (s)
		Standard Accuracy	Robust Accuracy	Standard Accuracy	Robust Accuracy	
	BCOP	70.48%	51.11%	39.19%	24.54%	49.6
LipConvnet-5	SOC	71.31%	52.11%	38.42%	23.68%	18.1
	Ours	71.69%	52.36%	38.68%	23.86%	18.8
	BCOP	70.79%	52.68%	39.29%	24.99%	73.0
LipConvnet-10	SOC	72.12%	54.72%	39.13%	25.01%	27.9
	Ours	72.15%	54.57%	39.64%	24.83%	29.3
	BCOP	70.48%	51.87%	37.93%	23.94%	86.2
LipConvnet-15	SOC	72.30%	55.34%	39.26%	24.76%	33.9
	Ours	72.87%	55.20%	39.43%	24.74%	40.3
	BCOP	69.99%	51.96%	36.37%	22.90%	116.8
LipConvnet-20	SOC	72.23%	55.45%	39.24%	25.29%	39.2
	Ours	72.52%	55.41%	40.03%	25.36%	44.9
	BCOP	68.84%	50.41%	33.04%	19.72%	138.5
LipConvnet-25	SOC	72.30%	55.34%	38.77%	24.05%	46.3
	Ours	72.42%	55.32%	38.32%	24.04%	49.2
	BCOP	69.77%	51.26%	30.95%	18.56%	150.2
LipConvnet-30	SOC	72.80%	55.71%	39.55%	25.42%	54.9
	Ours	72.48%	55.46%	39.18%	24.41%	57.7
	BCOP	68.61%	49.56%	27.75%	13.73%	166.6
LipConvnet-35	SOC	72.89%	55.71%	38.12%	24.32%	60.6
	Ours	71.59%	54.76%	38.60%	24.14%	64.0
	BCOP	67.64%	48.49%	27.09%	13.71%	180.3
LipConvnet-40	SOC	72.10%	55.33%	37.98%	23.63%	67.3
	Ours	72.43%	56.08%	38.24%	24.35%	70.9
Table 2: Comparisons among BCOP (Li et al., 2019), SOC (Singla & Feizi, 2021b) and our ECO
convolution on standard accuracy, robust accuracy, the time cost per epoch in training.
8
Published as a conference paper at ICLR 2022
Mn(x)∕√2 ≥ e. Following Li et al. (2019) and Singla & Feizi (2021b), when testing the robust
accuracy, we set the default norm of perturbation, , as 36/255. In this case, the robust accuracy is
the ratio of samples satisfying Mn(x)/√2 ≥ 36/255.
Standard and robust recognition accuracy. Table 2 compares the recognition accuracy of the pro-
posed ECO convolution with BCOP (Li et al., 2019) and SOC (Singla & Feizi, 2021b) on CIFAR10
and CIFAR100 datasets. As we can observe from the table that our ECO convolution achieves
competitive performance compared with BCOP and SOC in both standard and robust recognition
accuracy. Specifically, on CIFAR-10 and CIFAR-100, when the number of convolution layers, L, is
5 or 40, ours outperforms both SOC and BCOP in standard and robust recognition accuracy.
Ablation study on the dilated convolution. The dilation of the proposed ECO convolution is n/k
where n is the spatial size of the feature map and k is the spatial size of the convolution kernel. In
an early convolution block, the feature map is of large size, and thus the dilation of the convolution
is large. We discover that the large dilation in early blocks deteriorate the recognition performance.
We conduct an ablation study on the dilated convolution by fixing the first l blocks as SOC and only
use our SOC in the last 5-l blocks in LipConvnet-5. As shown in Table 3, when we deploy our ECO
convolution is the first one or two convolution blocks, the performance becomes worse considerably.
Thus, we only apply our ECO convolution in the last three convolution blocks, by default.
l	0	1	2	3	4	5
CIFAR10 Standard	67.88%	70.97%	71.69%	71.64%	71.54%	71.31%
CIFAR10 Robust	48.83%	51.66%	52.36%	52.55%	52.08%	52.11%
CIFAR100 Standard	31.35%	38.34%	38.68%	38.81%	38.82%	38.42%
CIFAR100 Robust	21.60%	23.40%	23.86%	24.02%	38.85%	23.68%
Table 3: Ablation on implementing the first l blocks by SOC and the last 5 - l blocks by our ECO
convolution. We use LipConvnet-5 model.
Training and evaluation time. As shown in the last column of Table 2, SOC is slightly more
efficient than our ECO convolution in training. It is because the dilation convolution in our ECO
adds more padding, making the input feature map larger than that in SOC. Meanwhile, as shown
in Table 4, our ECO convolution is faster than SOC in evaluation. It is due to that we have already
constructed the ECO convolution before evaluation. Thus, in evaluation, the computational cost
of our ECO convolution is the same as standard convolution. In contrast, SOC needs to take T
times convolution operations per convolution layer, taking T times computational cost as standard
convolution. By default, SOC sets T = 5 in training and T = 10 in evaluation.
Model	LC-5	LC-10	LC-15	LC-20	LC-25	LC-30	LC-35	LC-40
SOC	0.162s	0.276s	0.373s	0.480s	0.587s	0.693s	0.774s	0.890s
ECO	0.106s	0.108s	0.116s	0.124s	0.130s	0.135s	0.142s	0.147s
Table 4: Comparisons between SOC (Singla & Feizi, 2021b) and our ECO convolution on the
evaluation time on the third convolution block. LC denotes LipConvnet. In evaluation, we set the
batch size as 128 and the evaluation is conducted on a single V100 GPU.
7 Conclusion
This work investigates building the convolution layer with an orthogonal Jacobian matrix, which
is 1-Lipschitz in the 2-norm and preserves the gradient. By exploiting the relation between the
convolution kernel and the Jacobian matrix, we explicitly construct the convolution kernel to achieve
the orthogonal convolution. The explicitly constructed orthogonal convolution (ECO) takes the exact
computational cost as a standard dilated convolution in evaluation, which is more efficient than the
existing state-of-the-art method, SOC, approximating the orthogonal convolution through multiple
times convolution operations. Our experiments on CIFAR-10 and CIFAR-100 show that our ECO
achieves competitive standard and robust accuracy as SOC using much less evaluation time.
9
Published as a conference paper at ICLR 2022
References
Pierre-Antoine Absil, Robert E. Mahony, and Rodolphe Sepulchre. Optimization Algorithms on
Matrix Manifolds. Princeton University Press, 2008.
Cem Anil, James Lucas, and Roger B. Grosse. Sorting out lipschitz function approximation. In
Proceedings of the 36th International Conference on Machine Learning (ICML), pp. 291-301,
Long Beach, CA, 2019.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
Proceedings of the 33nd International Conference on Machine Learning (ICML), pp. 1120-1128,
New York City, NY, 2016.
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regular-
izations in training deep cnns? In Advances in Neural Information Processing Systems (NeurIPS),
pp. 4266T276, Montreal, Canada, 2018.
Ake Bjorck and Clazett Bowie. An iterative algorithm for computing the best estimate of an orthog-
onal matrix. SIAM Journal on Numerical Analysis, 8(2):358-364, 1971.
Mario Lezcano Casado and David Martinez-Rubio. Cheap orthogonal constraints in neural net-
works: A simple parametrization of the orthogonal and unitary group. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning (ICML), pp. 3794-3803, Long Beach, CA, 2019.
Moustapha Cissei, Piotr Bojanowski, Edouard Grave, Yann N. Dauphin, and Nicolas Usunier. Par-
seval networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning (ICML), pp.
854-863, Sydney, Australia, 2017.
Michael Cogswell, Faruk Ahmed, Ross B. Girshick, Larry Zitnick, and Dhruv Batra. Reducing over-
fitting in deep networks by decorrelating representations. In Proceedings of the 4th International
Conference on Learning Representations (ICLR), San Juan, Puerto Rico, 2016.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In Proceedings of the 35th International Conference on Machine Learning
(iCmL), pp. 1974-1983, Stockholmsmassan, Stockholm, Sweden, 2018.
Emiel Hoogeboom, Victor Garcia Satorras, Jakub M. Tomczak, and Max Welling. The convolu-
tion exponential and generalized sylvester flows. In Advances in Neural Information Processing
Systems (NeurIPS), virtual, 2020.
Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li. Orthogonal
weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep
neural networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence
(AAAI), pp. 3271-3278, New Orleans, LA, 2018a.
Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. In Proceedings
of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 791-800,
Salt Lake City, UT, 2018b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning (ICML), pp. 448-456, Lille, France, 2015.
Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, and Daniel Duckworth.
Invertible convolutional flow. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 5636-5646, Vancouver, Canada, 2019.
Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold
via the cayley transform. In Proceedings of the 8th International Conference on Learning Repre-
sentations (ICLR), Addis Ababa, Ethiopia, 2020.
10
Published as a conference paper at ICLR 2022
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B. Grosse, and Jorn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. In Advances in
Neural Information Processing Systems (NeurIPS),pp. 15364-15376, Vancouver, Canada, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In Proceedings of the 6th Interna-
tional Conference on Learning Representations (ICLR), Vancouver, Canada, 2018.
Kehelwala D. G. Maduranga, Kyle E. Helfrich, and Qiang Ye. Complex unitary recurrent neural
networks using scaled cayley transform. In Proceedings of the Thirty-Third AAAI Conference on
Artificial Intelligence (AAAI), pp. 4528-4535, Honolulu, HI, 2019.
Zakaria Mhammedi, Andrew D. Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal
parametrisation of recurrent neural networks using householder reflections. In Proceedings of the
34th International Conference on Machine Learning (ICML), pp. 2401-2409, Sydney, Australia,
2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In Proceedings of the 6th International Conference on Learning
Representations (ICLR), Vancouver, Canada, 2018.
Henri J Nussbaumer. The fast fourier transform. In Fast Fourier Transform and Convolution Algo-
rithms, pp. 80-111. Springer, 1981.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in Neural Information
Processing Systems (NIPS), pp. 4785-4795, Long Beach, CA, 2017.
Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for
visual recognition. In Proceedings of the 37th International Conference on Machine Learning
(ICML), pp. 7824-7835, Virtual Event, 2020.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. In Proceedings of the 2nd International Conference
on Learning Representations (ICLR), Banff, Canada, 2014.
Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers.
In Proceedings of the 7th International Conference on Learning Representations (ICLR), New
Orleans, LA, 2019.
Sahil Singla and Soheil Feizi. Fantastic four: Differentiable and efficient bounds on singular values
of convolution layers. In Proceedings of the 9th International Conference on Learning Represen-
tations (ICLR), Virtual Event, 2021a.
Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. In Proceedings of the 38th Interna-
tional Conference on Machine Learning (ICML), pp. 9756-9766, Virtual Event, 2021b.
Jiahao Su, Wonmin Byeon, and Furong Huang. Scaling-up diverse orthogonal convolutional net-
works with a paraunitary framework. arXiv preprint arXiv:2106.09121, 2021.
Asher Trockman and J. Zico Kolter. Orthogonalizing convolutional layers with the cayley transform.
In Proceedings of the 9th International Conference on Learning Representations (ICLR), Virtual
Event, Austria, 2021.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 6542-6551, Montreal, Canada, 2018.
Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X. Yu. Orthogonal convolutional neural
networks. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 11502-11512, Seattle, WA, 2020.
Nik Weaver. Lipschitz algebras. World Scientific, 2018.
11
Published as a conference paper at ICLR 2022
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10, 000-layer vanilla
convolutional neural networks. In Proceedings of the 35th International Conference on Machine
Learning (ICML), pp. 5389-5398, Stockholmsmassan, Stockholm, Sweden, 2018.
Wei Xiong, Bo Du, Lefei Zhang, Ruimin Hu, and Dacheng Tao. Regularizing deep convolutional
neural networks with a structured decorrelation constraint. In Proceedings of the IEEE 16th
International Conference on Data Mining (ICDM), pp. 519-528, Barcelona, Spain, 2016.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Proceed-
ings of the 4th International Conference on Learning Representations (ICLR), San Juan, Puerto
Rico, 2016.
12
Published as a conference paper at ICLR 2022
A CONSTRUCTING THE MAPPING MATRIX I
In Algorithm 2, we give the details of constructing the mapping matrix I.
Algorithm 2: Constructing the mapping matrix I
Input: The spatial size of the convolution kernel, k.
Output: The mapping matrix I
1
2
3
4
5
6
7
8
9
10
11
12
I = zeros(k, k);
count = 0;
for all (i, j) ∈ [k] × [k] do
if I[(k - i)%k, (k - j)%k] == 0 then
I[i,j] = count；
count = count + 1;
end
else
I I[i,j]= I[(k -i)%k, (k - j)%k];
end
end
return I
B	Comparisons with the complex settings
As we mentioned in Section 4, to obtain a real-value convolution kernel W0, the entries in P0 are
unnecessarily real. We compare the performance when P0 is constrained to be real as Algorithm 1
and that when P0 is a complex tensor. As shown in Table 5, on CIFAR-10 benchmark, using
complex P0, generally, it performs slightly better than its real counterpart on both standard accuracy
and robust accuracy. The better performance is attributed to the fact that the complex setting can
construct more complete orthogonal convolution. It is worth noting that the computational cost of
multiplication between complex matrices is four times as that between real matrices of the same
sizes. But the complex P0 is only used for constructing the real orthogonal kernel W0 . After
kernel construction, the computation cost of convolution is the same for the layer with complex P0
and that with real P0 . Thus, as shown in Table 5, the training time per epoch when using complex
P0 is around 2 times as that using real P0 . In evaluation, since we have constructed the real W0
from a real or complex P0 before evaluation, the computational cost is the same no matter P0 is
complex and real.
Standard Accuracy			Robust Accuracy		Training Time	
Model	LC-10	LC-15	LC-10	LC-15	LC-10	LC-15
Real	72.15%	72.87%	54.57%	55.20%	40.3	44.9
Complex	72.40%	72.98%	54.60%	55.53%	78.0	81.8
Table 5: Comparisons between the real and complex P0 on CIFAR-10 dataset. LC is the abbrevia-
tion for LipConvnet.
C Comparisons with the standard convolution
Here, we compare the proposed orthogonal convolution with the standard convolution without or-
thogonal constraint. To make a fair comparison, we only replace the orthogonal convolution layers
of LipConvnet-5 by the standard convolution and keep the other layers unfixed. Since the network
built upon the standard convolution cannot preserve the 1-Lipschitz property, we cannot directly
obtain the robust accuracy based on Mn(x) as the original LipConvnet-5 network. Thus, we utilize
Projected Gradient Descent (PGD) attack (Madry et al., 2018) to obtain the robust accuracy upper
13
Published as a conference paper at ICLR 2022
	CIFAR10			CIFAR100	
	Standard Accuracy	Robust Accuracy	Standard Accuracy	Robust Accuracy
Standard ECO	75.21% 71.69%	963%- 52.36%	45.62% 38.42%	7.31% 23.68%
Table 6: Comparisons with the standard convolution.
bound under the distortion with the '2 norm less than 熊.The results are shown in Table 6. As
shown in the table, on both CIFAR10 and CIFAR10 datasets, the network based on the standard con-
volution achieves higher standard accuracy than the network based on our ECO convolution. In the
meanwhile, ours achieves higher robust accuracy on both datasets under the same scale of distortion.
D Truncated Taylor Expansion
Note that, the exact matrix exponential (Casado & Martlnez-Rubio, 2019) exp(A) relies on singular
value decomposition (SVD), which is not friendly for GPU computing platform. Thus, we adopt the
truncated Taylor Expansion (Singla & Feizi, 2021b), ST(A) = PT=O A, to approximate the exact
matrix exponential exp(A). The truncation error R = exp(A) - ST (A) satisfies
kRk2 = kexp(A)- St (A)、≤ MT.	(19)
Below we first compare the time cost in GPU platform using the exact matrix exponential exp(A)
and that based on the truncated Taylor Expansion ST (A). To be specific, we implement exp(A)
simply based on the torch.matrix_exp. We measure the time cost on GPU using the function
torch.cuda.Event and we set T = 10 in truncated Taylor Expansion ST (A). As shown in Table 7,
the latency based on the truncated Taylor Expansion is significantly less than that based on the exact
matrix exponential.
Exact	Truncated
12.8ms	2.5ms
Table 7: The latency of the exact matrix exponential and the truncated Taylor Expansion in a layer.
Then we show the scale of the average factual truncated error R with respect to T. It is based
on the trained LipConvnet-5 network built upon the proposed ECO convolution. By default, we
set T = 5 in training and T = 10 in testing. As shown in Table 8, the scale of the truncation
error is relatively small when T is 5 or 10. Meanwhile, we also show the influence of the truncation
error on the orthogonality of the convolution. Following Trockman & Kolter (2021), we measure the
orthogonality of a convolution layer through kve：(Conv(X ))k2 -1, and a strict orthogonal convolution
kvec(X)k2
layer should satisfy
kvec(COnV(X ))k2
---------------1 — 0
kvec(X )k2
(20)
We estimate the deviation of the proposed ECO convolution from the strict orthogonal convolution
through this measurement. As shown in Table 8, when T is 5 or 10, the deviation of our ECO
convolution from the strict orthogonal convolution is relative marginal.
T	1	2	3	4	5	10
kRk2	0.48	0.16	0.04	0.008	0.001	0.0001
kvec(Conv(X ))k2	丁 kνec(X )k2	- 1	0.12	0.019	0.0038	0.0004	0.0000	0.0000
Table 8: The scale of the average factual truncated error.
14