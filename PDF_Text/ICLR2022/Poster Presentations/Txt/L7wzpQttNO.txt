Published as a conference paper at ICLR 2022
BDDM: B ilateral Denoising Diffusion Models
for Fast and High-Quality Speech Synthesis
Max W. Y. Lam, Jun Wang, Dan Su
Tencent AI Lab
Shenzhen, China
{maxwylam, joinerwang, dansu}@tencent.com
Dong Yu
Tencent AI Lab
Bellevue WA, USA
dyu@tencent.com
Ab stract
Diffusion probabilistic models (DPMs) and their extensions have emerged as com-
petitive generative models yet confront challenges of efficient sampling. We
propose a new bilateral denoising diffusion model (BDDM) that parameterizes
both the forward and reverse processes with a schedule network and a score
network, which can train with a novel bilateral modeling objective. We show
that the new surrogate objective can achieve a lower bound of the log marginal
likelihood tighter than a conventional surrogate. We also find that BDDM al-
lows inheriting pre-trained score network parameters from any DPMs and con-
sequently enables speedy and stable learning of the schedule network and op-
timization of a noise schedule for sampling. Our experiments demonstrate that
BDDMs can generate high-fidelity audio samples with as few as three sam-
pling steps. Moreover, compared to other state-of-the-art diffusion-based neu-
ral vocoders, BDDMs produce comparable or higher quality samples indistin-
guishable from human speech, notably with only seven sampling steps (143x
faster than WaveGrad and 28.6x faster than DiffWave). We release our code at
https://github.com/tencent-ailab/bddm.
1	Introduction
Deep generative models have shown a tremendous advancement in speech synthesis (van den Oord
et al., 2016; Kalchbrenner et al., 2018; Prenger et al., 2019; Kumar et al., 2019; Kong et al., 2020b;
Chen et al., 2020; Kong et al., 2021). Successful generative models can be mainly divided into two
categories: generative adversarial network (GAN) (Goodfellow et al., 2014) based and likelihood-
based. The former is based on adversarial learning, where the objective is to generate data indistin-
guishable from the training data. Yet, the training GANs can be very unstable, and the relevant train-
ing objectives are not suitable to compare against different GANs. The latter uses log-likelihood or
surrogate objectives for training, but they also have intrinsic limitations regarding generation speed
or quality. For example, the autoregressive models (van den Oord et al., 2016; Kalchbrenner et al.,
2018), while being capable of generating high-fidelity data, are limited by their inherently slow
sampling process and the poor scaling properties on high-dimensional data. Likewise, the flow-
based models (Dinh et al., 2016; Kingma & Dhariwal, 2018; Chen et al., 2018; Papamakarios et al.,
2021) rely on specialized architectures to build a normalized probability model, whose training is
less parameter-efficient. Other prior works use surrogate objectives, such as the evidence lower
bound in variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; Maal0e et al.,
2019) and the contrastive divergence in energy-based models (Hinton, 2002; Carreira-Perpinan &
Hinton, 2005). These models, despite showing improved speed, typically only work well for low-
dimensional data, and, in general, the sample qualities are not competitive to the GAN-based and
the autoregressive models (Bond-Taylor et al., 2021).
An up-and-coming class of likelihood-based models is the diffusion probabilistic models (DPMs)
(Sohl-Dickstein et al., 2015), which introduces the idea of using a forward diffusion process to
sequentially corrupt a given distribution and learning the reversal of such diffusion process to restore
the data distribution for sampling. From a similar perspective, Song & Ermon (2019) proposed
the score-based generative models by applying the score matching technique (Hyvarinen & Dayan,
1
Published as a conference paper at ICLR 2022
2005) to train a neural network such that samples can be generated via Langevin dynamics. Along
these two lines of research, Ho et al. (2020) proposed the denoising diffusion probabilistic models
(DDPMs) for high-quality image syntheses. Dhariwal & Nichol (2021) demonstrated that improved
DDPMs Nichol & Dhariwal (2021) are capable of generating high-quality images of comparable
or even superior quality to the state-of-the-art (SOTA) GAN-based models. For speech syntheses,
DDPMs were also applied in Wavegrad (Chen et al., 2020) and DiffWave (Kong et al., 2021) to
produce higher-fidelity audio samples than the conventional non-autoregressive models (Yamamoto
et al., 2020; KUmar et al., 2019; Yang et al., 2021; BinkoWski et al., 2020) and matched the quality
of the SOTA autoregressive methods (Chen et al., 2020).
Despite the compelling results, the diffusion generative models are tWo to three orders of magnitude
sloWer than other generative models such as GANs and VAEs. Their primary limitation is that they
require up to thousands of diffusion steps during training to learn the target distribution. Therefore a
large number of reverse steps are often required at sampling time. Recently, extensive investigations
have been conducted to reduce the sampling steps for efficiently generating high-quality samples,
Which We Will discuss in the related Work in Section 2. Distinctively, We conceived that We might
train a neural netWork to efficiently and adaptively estimate a much shorter noise schedule for sam-
pling While achieving generation performances comparable or superior to the conventional DPMs.
With such an incentive, after introducing the conventional DPMs as our background in Section 3, We
propose in Section 4 bilateral denoising diffusion models (BDDMs), named after a bilateral mod-
eling perspective - parameterizing the forward and reverse processes with a schedule network and
a score netWork, respectively. We theoretically derive that the schedule netWork should be trained
after the score network is optimized. For training the schedule network, we propose a novel objec-
tive to minimize the gap between a newly derived lower bound and the log marginal likelihood. We
describe the training algorithm as well as the fast and high-quality sampling algorithm in Section 5.
The training of the schedule network converges very fast using our newly derived objective, and its
training only adds negligible overhead to DDPM’s. In Section 6, our neural vocoding experiments
demonstrated that BDDMs could generate high-fidelity samples with as few as three sampling steps.
Moreover, our method can produce speech samples indistinguishable from human speech with only
seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave).
2	Related Work
Prior works showed that noise scheduling is crucial for efficient and high-fidelity data generation in
DPMs. DDPMs (Ho et al., 2020) used a shared linear noise schedule for both training and sampling,
which, however, requires thousands of sampling iterations to obtain competitive results. To speed
up the sampling process, one class of related work, including (Chen et al., 2020; Kong et al., 2021;
Nichol & Dhariwal, 2021), attempts to use a different, shorter noise schedule for sampling. For
clarity, we thereafter denote the training noise schedule as β ∈ RT and the sampling noise schedule
as β ∈ RN with N < T. In particular, Chen et al. (2020) applied a grid search (GS) algorithm to
select β. Unfortunately, GS becomes prohibitively slow when N grows large, e.g., N = 6 took more
than a day on a single NVIDIA Tesla P40 GPU. This is because the time costs ofGS algorithm grow
exponentially with N, i.e., O(9N) with 9 bins as the default setting in Chen et al. (2020). Instead
of searching, Kong et al. (2021) devised a fast sampling (FS) algorithm based on an expert-defined
6-step noise schedule for their score network. However, this specifically tuned noise schedule is
hard to generalize to other score networks, tasks, or datasets.
Another class of noise scheduling methods searches fora subsequence of time indices of the training
noise schedule, which we call the time schedule. DDIMs (Song et al., 2021) introduced an accel-
erated reverse process that relies on a pre-specified time schedule. A linear and a quadratic time
schedule were used in DDIMs and showed superior generation quality over DDPMs within 10 to
100 sampling steps. Nichol & Dhariwal (2021) proposed a re-scaled noise schedule for fast sam-
pling, but this also requires pre-specifying the time schedule and the training noise schedule. Nichol
& Dhariwal (2021) also proposed learning variances for the reverse processes, whereas the variances
of the forward processes, i.e., the noise schedule, which affected both the means and variances of
the reverse processes, were not learnable. According to the results of (Song et al., 2021; Nichol &
Dhariwal, 2021), using a linear or quadratic time schedule resulted in quite different performances
in different datasets, implying that the optimal choice of schedule varies with the datasets. So, there
remains a challenge in finding a short and effective schedule for fast sampling on different datasets.
2
Published as a conference paper at ICLR 2022
Notably, Kong & Ping (2021) proposed a method to map a noise schedule to a time schedule for
fast sampling. In this sense, searching for a time schedule becomes a sub-set of the noise scheduling
problem, which resembles the above category of methods.
Although DPMs (Sohl-Dickstein et al., 2015) and DDPMs (Ho et al., 2019) mentioned that the noise
schedule could be learned by re-parameterization, the approach was not investigated in their works.
Closely related works that learn a noise schedule emerged until very recently. San-Roman et al.
(2021) proposed a noise estimation (NE) method, which trained a neural net with a regression loss
to estimate the noise scale from the noisy sample at each time point, and then predicted the next
noise scale. However, NE requires a prior assumption of the noise schedule following a linear or
Fibonacci rule. Most recently, a concurrent work to ours by Kingma et al. (2021) jointly trained a
neural net to predict the signal-to-noise ratio (SNR) by maximizing the variational lower bound. The
SNR was then used for noise scheduling. Different from ours, this scheduling neural net only took
t as input and is independent of the noisy sample generated during the loop of sampling process.
Intrinsically, with limited information about the sampled data, the predicted SNR could deviate from
the actual SNR of the noisy data during sampling.
3	Background
3.1	Diffusion probabilistic models (DPMs)
Given i.i.d. samples {x0 ∈ RD} from an unknown data distribution pdata(x0), diffusion prob-
abilistic models (DPMs) (Sohl-Dickstein et al., 2015) define a forward process q(x1:T |x0) =
QtT=1 q(xt|xt-1) that converts any complex data distribution into a simple, tractable distribution af-
ter T steps of diffusion. A reverse process pθ (xt-ι∣xt) parameterized by θ is used to model the data
distribution: pθ(x0) = R π(xT) QtT=1pθ(xt-1 |xt)dx1:T, where π(xT) is the prior distribution for
starting the reverse process. Then, the variational parameters θ can be learned by maximizing the
standard log evidence lower bound (ELBO):
T
Felbo ：= Eq logPθ(X0∣X1) - EDKL (q(xt-i∣Xt, X0)∣∣Pθ(xt-i∣Xt)) - DKL (q(xτ∣xo)∣∣∏(xτ))
t=2
(1)
3.2	Denoising Diffusion Probabilistic Models (DDPMs)
As an extension to DPMs, denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020)
applied the score matching technique (Hyvarinen & Dayan, 2005; Song & Ermon, 2019) to define
the reverse process. In particular, DDPMs considered a Gaussian diffusion process parameterized
by a noise schedule β ∈ RT with 0 < β1, . . . , βT < 1:
T
qβ(xi:T|xo) ：= ∏qβt(xt∣xt-ι),	where qβt(xt∣xt-ι) := N(√1 - βtxt-ι,βtI).	(2)
t=1
Based on the nice property of isotropic Gaussians, one can express xt directly conditioned on x0 :
qβ(xt∣xo) = N(αtxo, (1 - α2)I), where a = ɪɪ √1 - βi.
i=1
To revert this forward process, DDPMs employ a score network1 θ (xt, αt) to define
Pθ(xt-1 |xt) ：= N	M Q	Xt-----/	2 % (xt,αt) , ∑t
(3)
(4)
t
1Here, θ (xt , αt) is conditioned on the continuous noise scale αt, as in (Song et al., 2020b; Chen et al.,
2020). Alternatively, the score network can also be conditioned on a discrete time index θ (xt , t), as in (Song
et al., 2021; Ho et al., 2020). An approximate mapping of a noise schedule to a time schedule (Kong & Ping,
2021) exists, therefore we consider conditioning on noise scales as the general case.
3
Published as a conference paper at ICLR 2022
Figure 1: A bilateral denoising diffusion model (BDDM) introduces a junctional variable xt and a
schedule network φ. The schedule network can optimize the shortened noise schedule βn(φ) if we
know the score of the distribution at the junctional step, using the KL divergence to directly compare
Pθ* (Xn-ι |Xn = Xt) against the re-parameterized forward process posteriors.
where Σt is the co-variance matrix defined for the reverse process. Ho et al. (2020) showed that
setting ∑t = βtI =、―：-1 βtI is optimal for a deterministic x°, while setting ∑t = βj is optimal
for a white noise x0 〜N(0, I). Alternatively, Nichol & Dhariwal (2021) proposed learnable
variances by interpolating the two optimals with a jointly trained neural network, i.e., Σt,θ(x) :=
diag(exp(vθ(x) log βt + (1 - vθ(x)) log βt)), where vθ(x) ∈ RD is a trainable network.
Note that the calculation of the complete ELBO in Eq. (1) requires T forward passes of the score
network, which would make the training computationally prohibitive for a large T . To feasibly
train the score network, instead of computing the complete ELBO, Ho et al. (2020) proposed an
efficient training mechanism by sampling from a discrete uniform distribution: t 〜 U{1,...,T},
xo 〜Pdata(xo), Et 〜N(0, I) at each training iteration to compute the training loss:
2
L(dtd)pm(θ) :
Et —Eθ (αtχo + 1- - α2 Et, αt J U
(5)
which is a re-weighted form of DKL (qβ (xt-ι |xt, xo) ∣∣pθ(xt-ι ∣xt)). Ho et al. (2020) reported that
the re-weighting worked effectively for learning θ. Yet, we demonstrate it is deficient for learning
the noise schedule β in our ablation experiment in Section 6.2. 4
4 B ilateral denoising diffusion models (BDDMs)
4.1 Problem formulation
For fast sampling with DPMs, we strive for a noise schedule β for sampling that is much shorter
than the noise schedule β for training. As shown in Fig. 1, we define two separate diffusion
processes corresponding to the noise schedules, β and β, respectively. The upper diffusion pro-
cess parameterized by β is the same as in Eq. (2), whereas the lower process is defined as
q^(Xi:N|Xo) = QN=I q^ (Xn∣Xn-ι) with much fewer diffusion steps (N《 T). In our prob-
lem formulation, β is given, but β is unknown. The goal is to find a β for the reverse process
Pθ(xn-ι∣Xn; βn) SUCh that x° Can be effectively recovered from XN with N reverse steps.
4.2 Model description
Although many prior arts (Ho et al., 2020; Chen et al., 2020; Song et al., 2021; San-Roman et al.,
2021) directly applied a shortened linear or Fibonacci noise schedule to the reverse process, we
argue that these are sub-optimal solutions. Theoretically, the diffusion process specified by a new
shortened noise schedule is essentially different from the one used to train the score network θ .
Therefore, θ is not guaranteed suitable for reverting the shortened diffusion process. This issue
motivated a novel modeling perspective to establish a link between the shortened schedule β and
the score network θ, i.e., to have β optimized according to θ.
4
Published as a conference paper at ICLR 2022
As a starting point, We consider an N = ∖T∕τC, where 1 ≤ τ < T is a hyperparameter controlling
the step size such that each diffusion step between two consecutive variables in the shorter diffusion
process corresponds to τ diffusion steps in the longer one. Based on Eq. (2), we define the following:
qβn+ι (Xn+1|xn = Xt)= qβ(Xt+τ Ixt) = N
(6)
where Xt is an intermediate diffused variable we introduced to link the two differently indexed
diffusion sequences. We call it a junctional variable, which can be easily generated given xo and β
during training: Xt = αtx0 +	- α2Wn.
Unfortunately, for the reverse process when X0 is not given, the junctional variable is intractable.
However, our key observation is that while using the score by a score network θ* trained for the
long β-parameterized diffusion process, a short noise schedule β(φ) can be optimized accordingly
by introducing a schedule network φ. We provide its mathematical derivations in Appendix A.3.
Next, we present a formal definition of BDDM and derive its training objectives, Ls(cno)re(θ) and
£篇(φ; θ*), for the score network and the schedule network, respectively, in more detail.
4.3	Score Network
Recall that a DDPM starts the reverse process with a white noise XT ~ N(0, I) and takes T steps
to recover the data distribution:
Pθ(XO) dd=m EN(0,I) [epθ(xlt-i∣xt) [Pθ(X0|X1:T)]].	⑺
A BDDM, in contrast, starts from the junctional variable Xt, and reverts a shorter sequence of
diffusion random variables with only n steps:
pθ (XO) := Ece^Xn-i；^Xt^in^) [Ep@ (X i：n-2 1 Xn -1 ) [pθ (XO |X1：n-1)]] ,	2 ≤ n ≤ N,	⑻
where q^(:^n-ι; Xt, Wn) is defined as a re-parameterization on the posterior:
qβ(Xn—1; Xt, Wn) :=qe I Xn—1 IXn = Xt, X0
(9)
=N
ʌ
βn
Wn, ⅛⅛βnI I ,	(10)
1
where α^n = Qn=ι ʌ/1 一 βi, Xt = αtXo + pl — 02Wn is the junctional variable that maps Xt
to Xn given an approximate index t 〜U{(n 一 1)τ,...,nτ — 1,nτ} and a sampled white noise
Wn ~ N(0, I). Detailed derivation from Eq. (9) to (10) is provided in Appendix A.2.
4.3.1	Training objective for score network
With the above definition, a new form of lower bound to the log marginal likelihood can be derived
such that logpθ(Xo) ≥ Fscn)e(θ) := -L(c⅛e(θ) 一 Rθ(Xo, Xt), where
L(c⅛e(θ): = DKL (Pθ(Xn-1∣Xn = Xt)∣g(Xn-1 ； Xt, Wn)) ,	(11)
Rθ(Xo, Xt) := — Epθ(Xi∣Xn=xt) [logPθ(X0∣X1)] .	(12)
See detailed derivation in Proposition 1 in Appendix A.2. In the following Proposition 2, we prove
that via the junctional variable Xt, the solution θ* for optimizing the objective LddPm(θ),∀t ∈
{1, ..., T} is also the solution for optimizing Ls(cno)re(θ), ∀n ∈ {2, ..., N}. Thereby, we show that the
score network θ can be trained with L(dtd)pm(θ) and re-used for reverting the short diffusion process
over XN：o. Although the newly derived lower bound result in the same objective as the conventional
score network, it for the first time establishes a link between the score network θ and XN：o. The
connection is essential for learning β, which we will describe next.
5
Published as a conference paper at ICLR 2022
4.4	schedule network
T l ʌ l ʌ l ʌ » r	1 11	.	1 ∙ ∙ .	1	1 . .1 i'	1	F	. ∙ ∙	A
In BDDMs, a schedule netWork is introduced to the forWard process by re-parameterizing βn as
ʌ
ʌ
βn(φ) = fφ (xt； βn+ι), and recall that during training, We can use Xt
atxo + √1 - α2En and
αt2+τ
βn+ι = 1--------t+2τ. Through the re-parameterization, the task of noise scheduling, i.e., searching
αt
for β, can now be reformulated as training a schedule network fφ that ancestrally estimates data-
dependent variances. The schedule network learns to predict βn based on the current noisy sample
Xt - this makes our method fundamentally different from existing and concurrent work, includ-
ing Kingma et al. (2021) — as we reveal that, aside from βn+ι, t, or n that reflects diffusion step
information, Xt is also essential for noise scheduling from a reverse direction at inference time.
Specifically, we adopt the ancestral step information (βn+1) to derive an upper bound for the current
step while leaving the schedule network only to take the current noisy sample Xt as input to predict
1	1	i'	1	∙	. .Λ	.	1	T-1∙	.	1	F	1 i' r>
a relative change of noise scales against the ancestral step. First, we derive an upper bound of βn
an+1
by proving 0 < βn < min {1 -
1-βn + 1
,βn+ι} in Appendix A.1. Then, by multiplying the upper
bound by a ratio estimated by a neural network σφ : RD 7→ (0, 1), we define
fφ(xt; βn+ι)=min[l-----αn+1 ,βn+ι∖ σφ(xt),
1 - βn+1
(13)
Where the netWork parameter set φ is learned to estimate the ratio betWeen tWo consecutive noise
1 Z A	1 A ∖ i'	. 1	.	.
scales (βn and βn+1) from the current noisy input xt.
Finally, at inference time for noise scheduling, starting from a maximum reverse steps (N) and two
hyperparameters (α^N, Bn), we ancestrally predict the noise scale βn(φ) = fφ (xn； βn+ι), for n
from N to 1, and cumulatively update the product &n = 〃7+1 .
4.4. 1 Training objective for schedule network
Here we describe how to learn the network parameters φ effectively. First, we demonstrated that
φ should be trained after θ is well-optimized, referring to Proposition 3 in Appendix A.3. The
Proposition also shows that we are minimizing the gap between the lower bound F(n)e(θ*) and
logpθ* (Xo), i.e., logpθ* (Xo) - F(n)e(θ*), by minimizing the following objective
Lsnp (Φ; θ*) =DKL (Pθ* (Xn-1∣Xn = Xt) || qβn (φ) (Xn-1 ； X。,。/) ,	(14)
which is defined as a KL divergence to directly compare pθ* (Xn-ι∣Xn = Xt) against the re-
parameterized forward process posteriors, which are tractable when conditioned on the junctional
noise scale αt and X0 .
The detailed derivation of Eq. (14) is also provided in the proof of Proposition 3 to get its concrete
formulas as shown in Step (8-10) in Alg. 2.
5	Algorithms: training, noise scheduling, and sampling
5.1	Training score and schedule networks
Following the theoretical result in Appendix A.3, θ should be optimized before learning φ. Thereby
first, to train the score network Eθ, we refer to the settings in (Ho et al., 2020; Chen et al., 2020; Song
et al., 2021) to define β as a linear noise schedule: βt = βstart + T (βend - βstart), for 1 ≤ t ≤ T,
where βstart and βend are two hyperparameter that specifies the start value and the end value. This
results in Algorithm 1, which resembles the training algorithm in (Ho et al., 2020).
Next, based on the converged score network θ*, we train the schedule network φ. We draw an
n 〜 U{2,..., N} at each training step, and then draw a t 〜 U{(n - 1)τ,..., nτ}. These together
can be re-formulated as directly drawing t 〜 U{τ,…,T - T} for a finer-scale time step. Then, we
6
Published as a conference paper at ICLR 2022
Algorithm 1 Training Score Network (θ)	Algorithm 2 Training Schedule Network (φ)
1: Given T, {βt}T=ι 		1: Given θ*,τ, T, {αt,βt}T=ι
2: {at}T=ι = {Qt=ι √1 - βt}T=I	2: repeat
3: repeat	3:	X0 〜Pdata(X0)
4: Xo 〜Pdata(X0)	4: t 〜U{τ,...,T — T }
5: t 〜U{1,...,T}	5:	δt = 1 - ct
6: Et 〜N(0, I)			6: En 〜N(0, I)
7: Xt = αtxo + 1- - α2Et	7:	Xt = CtXo + √δtEn
8: L(dtd)pm = kEt - Eθ(xt, αt)k22	8: βn = min [δt, 1 — α0+τ ( σφ(Xt)
9: Take a gradient descent step on VθLddPm	αt
10: until converged	9: C = 4 1 log(δt∕βn) + 2 1D(βn∕δt- 1)
Algorithm 3 Noise Scheduling	2 _ 10:	Lst≡p = 2(δt-βn) ||En - δn~eθ* (Xt,Ct)||2 + C
1: Given θ , c^n,Bn, XN 〜N(0, I)	11: Take a gradient descent step on VφLs(tnep)
2: for n = N to 2 do	12: until converged
3: xn —1 〜pθ* (Xn-1|xn; αn , ∕βn) .	λ,	Algorithm 4 Sampling
αn	
4:	Cn —1 - -/	八	
V 1-βn	1: Given θ*, {βn}N= 1, XNs 〜N(0,I)
2 5: βn-1 = min{l — C^n-i,βn}σφ (Xn-l)	Ar	N	/	Ns
6: if βn-ι < βι then	2: {Cn}N= 1 = Qn=1 √1 — βn n=1
—1	j	Λ	Λ 7:	return βn , . . . , βN	3: for n = Ns to 1 do
8: end if	4: X3n-1 〜Pθ*(Xn-1∣Xn; ^n,βn)
9: end for	5: end for
re,	Λ	Λ 10: return β1 , . . . , βN	6: return Xo
sequentially compute the variables needed for calculating L(nP(φ; θ*), as presented in Algorithm 2.
We observed that, although a linear schedule is used to define β, the noise schedule of β predicted
by fφ is not limited to but rather different from a linear one.
5.2	Noise scheduling for fast and high-quality sampling
After the score network and the schedule network are trained, the inference procedure can divide
into two phases: (1) the noise scheduling phase and (2) the sampling phase.
First, we run the noise scheduling process similarly to a sampling process with N iterations maxi-
mum. Different from training, where at is forward-computed, αn is instead a backward-computed
variable (from N to 1) that may deviate from the forward one because {βi}n-1 are unknown in the
noise scheduling phase during inference. To start noise scheduling, we first set two hyperparame-
ters: c^n and βN. We use βι, the smallest noise scale seen in training, as a threshold to early stop
the noise scheduling process so that we can ignore small noise scales (< β1) that were never seen
by the score network. Overall, the noise scheduling process presents in Algorithm 3.
In practice, we apply a grid search algorithm ofM bins to Algorithm 3, which takes O(M 2) time, to
find proper values for (c^n,8n). We used M = 9 as in (Chen et al., 2020). The grid search for our
noise scheduling algorithm can be evaluated on a small subset of the training samples. Empirically,
even as few as 1 sample for evaluation works well in our algorithm. Finally, given the predicted
noise schedule β ∈ RNs, We generate samples with Ns sampling steps, as shown in Algorithm 4.
6	Experiments
We conducted a series of experiments on neural vocoding tasks to evaluate the proposed BDDMs.
First, we compared BDDMs against several strongest models that have been published: the mixture
of logistics (MoL) WaveNet (Oord et al., 2018) implemented in (Yamamoto, 2020), the WaveGlow
(Prenger et al., 2019) implemented in (Valle, 2020), the MelGAN (Kumar et al., 2019) implemented
in (Kumar, 2019), the HiFi-GAN (Kong et al., 2020b) implemented in (Kong et al., 2020a) and
the two most recently proposed diffusion-based vocoders, i.e., WaveGrad (Chen et al., 2020) and
DiffWave (Kong et al., 2021), both re-implemented in our code. The hyperparameter settings of
BDDMs and all these models are detailed in Appendix B.
In addition, we also compared BDDMs to a variety of scheduling and acceleration techniques ap-
plicable to DDPMs, including the grid search (GS) approach in WaveGrad, the fast sampling (FS)
7
Published as a conference paper at ICLR 2022
Table 1: Comparison of neural vocoders in terms of MOS with 95% confidence intervals, real-time
factor (RTF) and model size in megabytes (MB) for inference. The highest score and the scores that
are not significantly different from the highest score (p-values ≥ 0.05) are bold-faced.
Neural Vocoder	MOS	RTF	Size
GroUnd-trUth	4.64 ± 0.08	—	—
WaveNet (MoL)(Oord et al., 2018)	3.52 ± 0.16	318.6	282MB
WaveGloW (Prenger et al., 2019)	3.03 ± 0.15	0.0198	645MB
MelGAN (KUmar et al., 2019)	3.48 ± 0.14	0.00396	17MB
HiFi-GAN (Kong et al., 2020b)	4.33 ± 0.12	0.0134	54MB
WaveGrad - 1000 steps (Chen et al., 2020)	4.36 ± 0.13	38.2	183MB
DiffWave - 200 steps (Kong et al., 2021)	4.49 ± 0.13	7.30	27MB
BDDM - 3 steps (&n = 0.68, Bn = 0.53)	3.64 ± 0.13	0.110	27MB
.. _ _. ʌ ... BDDM - 7 steps (&n = 0.62, Bn = 0.42)	4.43 ± 0.11	0.256	27MB
._ .. _ _. ʌ _ ... BDDM - 12 steps (&n = 0.67, Bn = 0.12)	4.48 ± 0.12	0.438	27MB
Table 2: Comparison of sampling acceleration methods with the same score network and the same
number of steps. The highest score and the scores that are not significantly different from the highest
score (p-values ≥ 0.05) are bold-faced.
Steps	Acceleration Method	STOI	PESQ	MOS
3	GS (Chen et al., 2020) FS (Kong et al., 2021) DDIM (Song et al., 2021) NE (San-Roman et al., 2021) BDDM	0.965 ± 0.009 0.939 ± 0.023 0.943 ± 0.015 0.966 ± 0.010 0.966 ± 0.011	3.66 ± 0.20 3.09 ± 0.23 3.42 ± 0.27 3.62 ± 0.18 3.63 ± 0.24	3.61 ± 0.12 3.10 ± 0.12 3.25 ± 0.13 3.55 ± 0.12 3.64 ± 0.13
7	FS (Kong et al., 2021) DDIM (Song et al., 2021) NE (San-Roman et al., 2021) BDDM	0.981 ± 0.006 0.974 ± 0.008 0.978 ± 0.007 0.983 ± 0.006	3.68 ± 0.24 3.85 ± 0.12 3.75 ± 0.18 3.96 ± 0.09	3.70 ± 0.14 3.94 ± 0.12 4.02 ± 0.11 4.43 ± 0.11
12	DDIM (Song et al., 2021) NE (San-Roman et al., 2021) BDDM	0.979 ± 0.006 0.981 ± 0.007 0.987 ± 0.006	3.90 ± 0.10 3.82 ± 0.13 3.98 ± 0.12	4.16 ± 0.12 3.98 ± 0.14 4.48 ± 0.12
approach based on a user-defined 6-step schedule in DiffWave, the DDIMs (Song et al., 2021) and
a noise estimation (NE) approach (San-Roman et al., 2021). For fair and reproducible comparison
with other models and approaches, we used the LJSpeech dataset (Ito & Johnson, 2017), which
consists of 13,100 22kHz audio clips of a female speaker. All diffusion models were trained on the
same training split as in (Chen et al., 2020). We also replicated the comparative experiment of neural
vocoding using a multi-speaker VCTK dataset (Yamagishi et al., 2019) as presented in Appendix C
and obtained a result consistent with that obtained from the LJSpeech dataset.
6.1	Sampling quality in objective and subjective metrics
To assess the quality of each generated audio sample, we used both objective and subjective mea-
sures for comparing different neural vocoders given the same ground-truth spectrogram s as the
condition, i.e., θ (x, s, αt). Specifically, we used two scale-invariant metrics: the perceptual evalu-
ation of speech quality (PESQ) (Rix et al., 2001) and the short-time objective intelligibility (STOI)
(Taal et al., 2010) to measure the noisiness and the distortion of the generated speech relative to the
reference speech. Mean opinion score (MOS) was also used as a subjective metric for evaluating the
naturalness of the generated speech. The assessment scheme of MOS is included in Appendix B.
In Table 1, we compared BDDMs against the state-of-the-art (SOTA) vocoders. To predict noise
SchedUles With different sampling steps (3, 7, and 12), We set three pairs of {ɑN, Bn} for BDDMs
by running on Algorithm 3 a quick hyperparameter grid search, which is detailed in Appendix B.
Among the 9 evalUated vocoders, only oUr proposed BDDMs With 7 and 12 steps and DiffWave With
200 steps shoWed no statistic-significant difference from the groUnd-trUth in terms of MOS. More-
over, BDDMs significantly oUtspeeded DiffWave in terms of RTFs. Notably, previoUs diffUsion-
8
Published as a conference paper at ICLR 2022
----L_step
----L elbo
O 20	40	60	80 IOO
Training Step
25	50	75 IOO 125	150	175
t
Figure 3: Different lower bounds to log pθ (x0)

―I- Fbd dm
-I- F_elbo
Figure 2: Different training losses for σφ
based vocoders achieved high MOS scores at the cost of an unacceptable RTF for industrial deploy-
ment. In contrast, BDDMs managed to achieve a high standard of generation quality with only 7
sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave).
In Table 2, we evaluated BDDMs and alternative accelerated sampling methods, which used the
same score network for a pair-to-pair comparison. The GS method performed stably when the
step number was small (i.e., N ≤ 6) but not scalable to more step numbers, which were therefore
bypassed in the comparisons of 7 and 12 steps. The FS method by Song et al. (2021) was linearly
interpolated to 3 and 7 steps for a fair comparison. Comparing its 7-step and 3-step results, we
observed that the FS performance degraded drastically. Both the DDIM and the NE methods were
stable across all the steps but were not performing competitively enough. In comparison, BDDMs
consistently attained the leading scores across all the steps. This evaluation confirmed that BDDM
was superior to other acceleration methods for DPMs in terms of both stability and quality.
6.2 Ab lation S tudy and Analysis
We attribute the primary advantage of BDDMs to the newly derived objective Ls(tnep) for learning φ.
To better reason about this, we performed an ablation study, where we substituted the proposed loss
with the standard negative ELBO for learning φ as mentioned by Sohl-Dickstein et al. (2015). We
plotted the network outputs with different training losses in Fig. 2. It turned out that, when using
Le(lnb)o to learn φ, the network output rapidly collapsed to zero within several training steps; whereas,
the network trained with Ls(tnep) produced fluctuating outputs. The fluctuation is a desirable property
showing the network properly predicts t-dependent noise scales, as t is a random time step drawn
from a uniform distribution in training.
By setting β = β,we empirically validated that F(tdm := FS(tθre+L(t)P ≥ Fto with their respective
values at t ∈ [20,180] using the same optimized θ*. Each value is provided with 95% confidence
intervals, as shown in Fig. 3. In this experiment, we used the LJ speech dataset and set T = 200 and
T = 20. Notably, we dropped their common entropy term Rθ(Xo, Xt) < 0 to mainly compare their
KL divergences. This explains those positive lower bound values in the plot. The graph shows that
our proposed bound Fb(dt)dm is always a tighter lower bound than the standard one across all examined
t. Moreover, we found that Fb(dtd)m attained low values with a relatively much lower variance for
t ≤ 50, where Fe(ltb)o was highly volatile. This implies that Fb(dtd)m better tackles the difficult training
part, i.e., when the score becomes more challenging to estimate as t → 0.
7	Conclusions
BDDMs parameterize the forward and reverse processes with a schedule network and a score net-
work, of which the former’s optimization is tied with the latter by introducing a junctional variable.
We derived anew lower bound that leads to the same training loss for the score network as in DDPMs
(Ho et al., 2020), which thus enables inheriting any pre-trained score networks in DDPMs. We also
showed that training the schedule network after a well-optimized score network can be viewed as
tightening the lower bound. Followed from the theoretical results, an efficient training algorithm and
a noise scheduling algorithm were respectively designed for BDDMs. Finally, in our experiments,
BDDMs showed a clear edge over the previous diffusion-based vocoders.
9
Published as a conference paper at ICLR 2022
References
Mikolaj BinkoWski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman
Casagrande, Luis C. Cobo, and Karen Simonyan. High fidelity speech synthesis with adversarial
netWorks. International conference on learning representations, 2020.
S Bond-Taylor, A Leach, Y Long, and CG Willcocks. Deep generative modelling: A comparative
revieW of vaes, gans, normalizing floWs, energy-based and autoregressive models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2021.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. AISTATS,
pp. 33-40, 2005.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-
grad: Estimating gradients for Waveform generation. In International conference on learning
representations, 2020.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. Advances in neural information processing systems, pp. 6571-6583, 2018.
Prafulla DhariWal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in neural information processing systems, 34, 2021.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Brendan J Frey. Local probability propagation for factor analysis. Advances in neural information
processing systems, 12:442-448, 1999.
Ian GoodfelloW, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
G. E. Hinton. Training products of experts by minimizing contrastive divergence. neural computa-
tion. Neural computation, pp. 14(8):1771-1800, 2002.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. FloW++: Improving floW-
based generative models With variational dequantization and architecture design. ICML, 2019.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840-6851, 2020.
Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score match-
ing. Journal of Machine Learning Research, pp. 6(4), 2005.
Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, EdWard Lock-
hart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural
audio synthesis. In International Conference on Machine Learning, pp. 2410-2419. PMLR, 2018.
Diederik P Kingma and Prafulla DhariWal. GloW: Generative floW With invertible 1x1 convolutions.
Advances in neural information processing systems, pp. 10215-10224, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:1, 2014.
Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In
Advances in neural information processing systems, 2021.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial netWorks for
efficient and high fidelity speech synthesis. https://github.com/jik876/hifi-gan,
2020a.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial netWorks for
efficient and high fidelity speech synthesis. Advances in neural information processing systems,
33, 2020b.
10
Published as a conference paper at ICLR 2022
Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop
on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. International conference on learning representations, 2021.
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Brebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. Advances in neural information processing systems,
32, 2019.
Rithesh Kumar. Official repository for the paper melgan: Generative adversarial networks for condi-
tional waveform synthesis. https://github.com/descriptinc/melgan-neurips,
2019.
Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Effective low-cost time-domain audio separation
using globally attentive locally recurrent networks. In 2021 IEEE Spoken Language Technology
Workshop (SLT),pp. 801-808. IEEE, 2021.
Lars Maal0e, Marco Fraccaro, Valentin Lievin, and Ole Winther. Biva: A very deep hierarchy of
latent variables for generative modeling. Advances in neural information processing systems, pp.
6548-6558, 2019.
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021.
Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast
high-fidelity speech synthesis. In International conference on machine learning, pp. 3918-3926.
PMLR, 2018.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing flows for probabilistic modeling and inference. JMLR, pp. 22(57):1-
64, 2021.
Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network
for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 3617-3621. IEEE, 2019.
Flavio Protasio Ribeiro, Dinei Florencio, Cha Zhang, and Mike Seltzer. CROWDMOS: An approach
for crowdsourcing mean opinion score studies. In ICASSP. IEEE, 2011. Edition: ICASSP.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, pp. 1278-1286, 2014.
Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual evaluation
of speech quality (pesq)-a new method for speech quality assessment of telephone networks and
codecs. In 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing.
Proceedings (Cat. No. 01CH37221), volume 2, pp. 749-752. IEEE, 2001.
Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion mod-
els. arXiv preprint arXiv:2104.02600, 2021.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn-
ing, pp. 2256-2265, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. Interna-
tional conference on learning representations, 2021.
11
Published as a conference paper at ICLR 2022
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
Advances in neural information processing systems, 33:12438-12448, 2020.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. UAI, pp. 574-584, 2020a.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional conference on learning representations, 2020b.
Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. A short-time objective
intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE international
conference on acoustics, speech and signal processing, pp. 4214-4217. IEEE, 2010.
Rafael Valle. Waveglow: a flow-based generative network for speech synthesis. https://
github.com/NVIDIA/waveglow, 2020.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. Proc. 9th ISCA Speech Synthesis Workshop, pp. 125-125, 2016.
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sam-
ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021.
Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English
multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019.
Ryuichi Yamamoto. Wavenet vocoder. https://github.com/r9y9/wavenet_vocoder,
2020.
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel. Wavegan: A fast waveform genera-
tion model based on generative adversarial networks with multi-resolution spectrogram. ICASSP,
2020.
Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, and Lei Xie. Multi-band melgan: Faster
waveform generation for high-quality text-to-speech. In 2021 IEEE Spoken Language Technology
Workshop (SLT), pp. 492-498. IEEE, 2021.
12
Published as a conference paper at ICLR 2022
A Theoretical derivations for BDDMs
In this section, we provide the theoretical supports for the following:
EI 1	i'	Λ	1∙	A Z	A	1∙ k ∖
•	The derivation for upper bounding βn (see Appendix A.1).
•	The score network θ trained with Lddpm(θ) for the reverse process pθ(xt-i|xt) can be
re-used for the reverse process pθ(Xn-1 |Xn) (see Appendix A.2).
•	The schedule network φ can be trained with L(np(φ; θ*) after the score network θ is opti-
mized. (see Appendix A.3).
A.1 Deriving an upper bound for noise scale
Since monotonic noise schedules have been successfully applied to in many prior arts including
DPMs (Ho et al., 2020; Kingma et al., 2021) and score-based methods (Song et al., 2020a; Song
C 1 -	CCCC∖	ICIl	,1	.	. ∙	Λ Λ	F	Λ Γ∙ A
& Ermon, 2020), we also follow the monotonic assumption and derive an upper bound for βn as
below:
ʌ
ʌ
Remark 1. Suppose the noise schedule for sampling is monotonic, i.e., 0 < β1 < . . . < βN < 1,
then, for 1 ≤ n < N, βn satisfies the following inequality:
0 <βn < min 11---cn+1 ,βn+J .
1 - βn+1
ʌ
ʌ
(15)
Proof. By the general definition of noise schedule, We know that 0 ‹ βι,...,βN < 1 (Note: no
inequality sign in between). Given that αη = QZi ∖∣' - βi, We also have 0 < αι,...,αt < 1.
τ-,∙	.	1	.1	. A -T
First, we show that βn < 1
αn— 1
αn+ι
ʌ
1-βn + 1
(^-n
c	Cn+1
1 - ----7---
1 - βn+1
(16)
TL T	1	.1	. A	-T
Next, we show that βn < 1
-αn+1:
_ αn J1 - 8n _ Cn+i
ʌ
1 - β)n
ʌ
1 - β)n
,λ . . n - r ʌ
< 1 < ⇒ 8n < 1 - αn+1.
(17)
—
r	A	r
< 1 < ⇒ Bn < 1 -
Now, we have βn < min {1
that βn+1 < 1 — Cn+1:
&n+i	1
1-βn+1 ,
-αn+1 }. When 1 - αn+1 <
1 — αn+1 , we can show
1—βη+1
—
1 - ɑn+1 < 1 -
cn+1
ι n
1 - βn+1
1 — On Q⇒ α^n+ι > C^n Q⇒
2
αn+1、R
> 2	> αn+1
C2
n
(18)
1-
α211	...
八 2	< 1 -(y-n+1 ^⇒ βn+1	< 1 - (y-n+1.
C2
n
(19)
By the assumption of monotonic sequence, we also have βn < βn+1 . Knowing that βn+1 <
α α	α2
1 - c^n+1 is always true, we obtain a tighter bound for βn 0 < βn < min 1 1 -	n+1 , βn+1 》. □
I	1-βn+1	J
A.2 Deriving the training objective for score network
First, followed from the data distribution modeling of BDDMs as proposed in Eq. (8):
pθ (XO) : = EXn-I 〜qg(Xn-1 ；Xt,€n) [EXlm-2~Pθ (X 1:n-2 | Xn-1 ) [Pθ (Xo|Xl:n—1)]],
we can derive a new lower bound to the log marginal likelihood as follows:
(20)
^⇒
ʌ
ʌ ʌ
ʌ
13
Published as a conference paper at ICLR 2022
Proposition 1. Given Xt 〜qβ(xt∣xo),thefollowing lower bound holdsfor n ∈ {2,..., N}:
logpθ(Xo) ≥Fnr-e(θ) ：= -Lne⑼-Rθ(Xo, χt),	Qi)
where
L(Core⑼=DKL Pθθ (Xn-1∣Xn = Xt) ||9台(Xn-1 ； Xt, Wn)) ,	(22)
Rθ(Xo, Xt):= -Epθ(Xι∣Xn =Xt) [logPθ(Xo∣Xι)].	(23)
Proof.
log Pθ (Xo) =log
J pθ (X0:n —2 |Xn—1) qβ (Xn—1; Xt, Wn)dX^1:n—1
(24)
= log
P pθ (X0:n — 2 IXn-I) q^ (Xn—1
; Xt, Wn)
P6(Xl:n—1∣Xn
P6(Xl:n—1∣Xn
Xt)
Xt)
dXl:n-1
(25)
Pθ (Xo|X1)qe(Xn—1； Xt, Wn )
g Epe(Xl，n-l|Xn=Xt)	pθ (Xn-1% = Xt)
(26)
Γ	Pθ (Xo|X1)qe(Xn-1； Xt, Wn)]
[Jensen's Inequality] ≥Epθ⑶环一尾二⑦力 log	Pθ (Xn-β∣Xn = Xt)	(27)
=Epθ (Xi ∣Xn=χt) [logpθ (X0 |X1)] - DKL (pθ (Xn-1 |Xn = Xt) ||qe(Xn-1 ； Xt, wn
(28)
=-L(2(θ)-Rθ (Xo, Xt)	(29)
□
Next, we show that the score network θ trained with L(dtd)pm(θ) can be re-used in BDDMs. We first
provide the derivation for Eq. (9- 10). We have
qz^(Xn-1; Xt, Wn) := q^ (Xn-1|Xn = Xt, Xθ = Xt_V一 ^n
N I l^n-1 ∣βn Xt - P1 -然Wn +
∖ 1 - αn	αn
1- αn
Xt, ⅛τ βnI
N (( αn-1βn	J1 - BnQ - an-1)
N an(1 - αn) +	T-鬲
αn-1∕βn	1 - ^xn-1
βnI
N ( I 1 Xt -
ʌ
βn
1 - an-1 βnl).
1 - ɑ2	/
(30)
(31)
(32)
(33)
Proposition 2. Suppose Xt 〜qβ (x∕xo) , then any solution satisfying θ* = argmin § Ldt)^pnι (θ), ∀t ∈
{1,...,T}, also satisfies θ* = argmin § L (Cnlre (θ),∀n ∈ {2,...,N}.
Proof. By the definition in Eq. (4), we have
pθ (Xn-I |Xn
wΘ (xt, an)
(34)
Here, from the training objective in Eq. (5), since Xt = αtXo+yz1 - 02 Wn, the noise scale argument
for the score network is known to be αt. Therefore, We can use w§ (xt, at) instead of w§ (Xt, 0n) for
14
Published as a conference paper at ICLR 2022
expanding L(nre(θ). Since pθ(Xn-i|Xn = Xt) and q^(-Xn-ι; Xt, Wn) are two isotropic Gaussians
with the same variance, the KL divergence is a scaled '2-norm of their means, difference:
Lscore(θ) ：=DKL (pθ (Xn—1|Xn = xt) ||% (Xn—1 ； xt, Wn ))
(35)
1 - an
2(1— On-Jβn
^
==≡ wθ (Xt,at)
(36)
—
—
ʌ
βn
2
(37)
2
(1 一 βn)(1 - an)
ʌ
ʌ
ʌ
βn
2
2(1 - βn - αn)Bn
(1 — βn)(1 — ɑn)
βn
ʌ
ʌ
ʌ
2(1- βn - ɑn)βn (1 - αn)(1- βn)
(n - θ (xt, αt))
kn - θ (xt, αt)k22
2
(38)
(39)
ʌ
βn
ʌ
2(1 — βn — αn)
which is proportional to L(dtd)pm ：=
Thus,
n - θ αt x0 +	1 - αt2 n , αt
2
(40)
2
as defined in Eq.
(5).
argminθ L(dtd)pm (θ) ≡ argminθ Ls(cno)re (θ).
(41)
□
Next, We can simplify Rθ(Xo, Xt) to a reconstruction loss for X°:
Rθ(Xo,xt)：= -Epθ(Xi∣Xn=xt) [logPθ(x0∣Xι)]
(42)
=EPθ (Xl∣Xn=Xt)
=EPθ (XlIXn=Xt)
D1
=TT log2πβ1 + 丁EPθ(XiIXn=Xt)
2	2β1
log N
x0 -
^,
β1
Xi----产Wθ(xι, αι)
Ti
1 — √‰θ (Xι,αι)
D1
τlog2πβi + 函
βi Wθ (Xι,αι),βιl
(43)
2
2
(44)
2
2
(45)
where pθ(Xi |Xn = Xt) can be efficiently sampled using the reverse process in (Song et al., 2021).
Yet, in practice, similar to the training in (Song et al., 2021; Chen et al., 2020; Kong et al., 2021),
we dropped Rθ(Xo, Xt) when training θ. In theory, we know that Rθ(Xo, Xt) achieves its optimal
value at θ* = argmin6 ∣∣€θ(X1,α1) — wι∣2, which shares a similar objective as Lddpm. By minimizing
Lddpm，we train a score network θ* that best minimizes ∆q :
IIq -^θ* (αtxo + 1- - α26t, αt)k2
for all 1 ≤ t ≤ T. Since the first diffusion step has the smallest effect on corrupting Xo (i.e.,
βι ≈ 0), it suffices to consider a αι = √1 一 βι = αι, in which case we can jointly minimize
Rθ (Xo, Xt) by minimizing Lddpm.
In this sense, during training, given Xt 〜qβ(x"xo), we can train the score network with the same
training objective as in DDPMs and DDIMs. Practically, it is beneficial for BDDMs as we can re-use
the score network θ of any well-trained DDPM or DDIM.
15
Published as a conference paper at ICLR 2022
A.3 Deriving the training objective for schedule network
Given that θ can be trained to maximize the log evidence with the pre-specified noise schedule β for
training, the consequent question of interest in BDDMs is how to find a fast and good enough noise
schedule β ∈ RN for sampling given an optimized θ*. In BDDMs, this problem is reduced to how
to effectively learn the network parameters φ .
Proposition 3. Suppose θ has been optimized and hypothetically converged to the optimal θ*,
where by optimal it means that with θ* we have pθ* (Xn-ι∣Xn = Xt) = q^(x^n-ι; xt, Wn) given
Xt 〜qβ(xt∣xo)∙ When β Is unknown but we have x° = x° and α^n = at, we can minimize the
gap between the optimal lower bound Fsscnr(θ*) and logpθ* (Xo), i.e, logpθ* (Xo) — F((Or(θ*), by
minimizing the following objective with respect to βn :
LsteP (βn; θ ) =DKL (pθ* (Xn—1|Xn = Xt)IIqen (Xn-1|x0; at)
δt
._ ʌ ,
2(δt — Bn)
(46)
(47)
wn — δn-wθ* (0tχ0 + ∙∖∕δten, αt) J] + C,
where
δt = 1 — αt2 ,
(48)
Proof. Note that Xo = xo, α- = at, Xt = αtXo + √1 — a2Wn and Pθ*(X--ι∣X- = Xt)
q^(3^n-1; Xt, Wn). When x° is given to pθ*, we can express the probability as follows:
p6*(Xn—1|Xn = Xt(XO), X0 = XO)
(49)
/ N(Z； O, I)pθ*(Xn-1∣Xn
/N(z； O, I)q^(Xn-i; Xt
atXO +
atXO +
1 — at2 z, Wn = z)dz
(50)
(51)
/ N(z; 0, I)N I Xn—1
.at Xo + √1 — a2 Z
ʌ
βn
1 — a2 1 ʌ ∖
z, --n-1 βnI	dz
1-an	/
;
—
(52)
[See Eq. (2) in (Frey, 1999)]
/
=N	Xn —1；
∖
βn
at2)) +
1 — a2/(1 — ∕βn)
1 — α22
βn I I
(53)
=N	Xn —1；
atXO
1 — a22 — β?
q - βn,	1—βn
—I I =: qen(Xn—1； x0, at),
(54)
β

—
n
where, different from pθ* (Xn-1 |Xn = Xt), from Eq. (49) to Eq. (50), instead of conditioning on a
specific Xt, when Xo is given Xt can be generated using any Z 〜N(0, I).
16
Published as a conference paper at ICLR 2022
From this, We can express the gap between logpθ* (Xo) and F(n)e(θ*) in the following form:
logPθ*(Xo = X0)- F(In)e(θ*)
,	B	[ Pθ (XOIxI)qβ(Xn-1； xt, Cn)
= log pθ*(x0 = XO)- Epθ* (X1,n-ιlχn=χt) [log —pg (xn-i∣xn = xt)—
pθ*	pθ	:n -	pg* pθ* (X0：n-1|xn = Xt)
= log pθ*(x0 = XO)- Epθ* (XLn-ιlxn=χt)呼 pθ*(Xm-i∣Xn = Xt)
_JP	]	pθ* (X1:n—1 |Xn = Xt)
=Epθ*(XLn-llxn=xt) [logpg*(Xi：n-i|Xn = Xt, Xo = Xo)
_]	pθ* (Xn-1|Xn = Xt)
=Epθ*(Xn-l|Xn=Xt)Fg qβn(Xn-i; Xo,αt)
= DKL (pθ* (Xn-IIXn = Xt)||qen (Xn-1； X0,at))
Next, we evaluate the above KL divergence term. By definition, we have
Cθ* (Xt,&n)
(61)
(55)
(56)
(57)
(58)
(59)
(60)
Together with Eq. (54), we have
ʌ
1 - β)n
2(1 - iβn - α2)
1 - α2
2(1 - jβn - α2)
δt
,_ ʌ ,
2(δt - Bn)
L(nP (βn; θ*) ：= DKL (pθ*(Xn-1∣Xn = Xt)∣Wβn (Xn-1； Xθ,αt
ʌ
1 - βn
2(1 - iβn - α2)
ʌ
1 - β)n
2(1 - iβn - α2)
2
1 - at	βn	(、
1 i---K-Wn-----∕/	° /	acθ* (Xt,αt)
V 1 - βn	p(I - βn )(I - αt )
βn
2
Cθ* (Xt, αt)	+ C
2
ʌ
βn
Wn----lcθ*
δt
+ C,
+C
2
(62)
(63)
+C
(64)
(65)
(66)
(67)
—
where
(68)
□
A	1 11	.	1 I .	. ∙	. Λ i'	/ ʌ	A	∖	1 f' 1 ∙ 1 -	Z 1 <⅛∖	1 .
As we use a schedule network φ to estimate βn from (αn+ι, βn+ι) as defined m Eq. (13), we obtain
the final step loss for learning φ:
L(nP (Φ; θ*)
δt
ʌ ...
2(δt - βn(φ))
Cn- βn≡Wθ* (Xt,αt)
δt
2
+1 上+D β T
2	4	βn(φ)	2 ∖ δt
-1
(69)
17
Published as a conference paper at ICLR 2022
This proposed objective for training the schedule network can be interpreted as to better model
the data distribution (i.e., maximizing logpθ(xo)) by correcting the gradient scale βn for the next
reverse step (from Xn to Xn-ι) given the gradient vector ∈θ* estimated by the score network θ*.
B	Experimental details
B.1	Conventional grid search algorithm for DDPMs
We reproduced the grid search algorithm in (Chen et al., 2020), in which a 6-step noise schedule was
searched. In our paper, we generalized the grid search algorithm by similarly sweeping the N -step
noise schedule over the following possibilities with a bin width M = 9:
{1, 2, 3,4, 5, 6, 7, 8, 9}乳{10-&N/N, 10-6,(N-1)/N,…，10-6,1/N},	(70)
where 0 denotes the cartesian product applied on two sets. LS-MSE was used as a metric to select
the solution during the search. When N = 6, we resemble the GS algorithm in (Chen et al., 2020).
Note that above searching method normally does not scale up to N > 6 steps for its exponential
computational cost O(9N).
B.2	Hyperparameter setting in BDDMs
Algorithm 2 took a skip factor τ to control the stride for training the schedule network. The value
of τ would affect the coverage of step sizes when training the schedule network, hence affecting
the predicted number of steps N for inference - the higher T is, the shorter the predicted inference
schedule tends to be. We set τ = 66 for training the BDDM vocoders in this paper.
For initializing Algorithm 3 for noise scheduling, we could take as few as 1 training sample for
validation, perform a grid search on the hyperparameters {(αN = 0.1ατ%,Bn = 0.1j)} for i,j =
1, ..., 9, i.e., 81 possibilities in total, and use the PESQ measure as the selection metric. Then, the
predicted noise schedule corresponding to the maximum PESQ was stored and applied to the online
inference afterward, as shown in Algorithm 4. Note that this searching has a complexity of only
O(M 2) (e.g., M = 9 in this case), which is much more efficient than O(MN) in the conventional
grid search algorithm in (Chen et al., 2020), as discussed in Section B.1.
B.3	Implementation details
Our proposed BDDMs and the baseline methods were all implemented with the Pytorch library. The
score networks for the LJ and VCTK speech datasets were trained from scratch on a single NVIDIA
Tesla P40 GPU with batch size 32 for about 1M steps, which took about 3 days.
For the model architecture, we used the same architecture as in DiffWave (Kong et al., 2021) for
the score network with 128 residual channels; we adopted a lightweight GALR network (Lam et al.,
2021) for the schedule network. GALR was originally proposed for speech enhancement, so we con-
sidered it well suited for predicting the noise scales. For the configuration of the GALR network, we
used a window length of8 samples for encoding, a segment size of64 for segmentation and only two
GALR blocks of 128 hidden dimensions, and other settings were inherited from (Lam et al., 2021).
To make the schedule network output with a proper range and dimension, we applied a sigmoid func-
tion to the last block’s output of the GALR network. Then the result was averaged over the segments
and the feature dimensions to obtain the predicted ratio: σφ(x) = AvgPool2D(σ(GALR(x))),
where GALR(∙) denotes the GALR network, AvgPool2D(∙) denotes the average pooling operation
applied to the segments and the feature dimensions, and σ(x) := 1/(1 + e-x). The same net-
work architecture was used for the NE approach for estimating αt2 and was shown better than the
ConvTASNet used in the original paper (San-Roman et al., 2021). It is also notable that the com-
putational cost of a schedule network is indeed fractional compared to the cost of a score network,
as predicting a noise scalar variable is intrinsically a relatively much easier task. Our GALR-based
schedule network, while being able to produce stable and reliable results, was about 3.6 times faster
than the score network. The training of schedule networks for BDDMs took only 10k steps to
converge, which consumed no more than an hour on a single GPU.
18
Published as a conference paper at ICLR 2022
Table 3: Ratings that have been used in evaluation of speech naturalness of synthetic samples.
Rating	Naturalness	Definition
1	Unsatisfactory	Very annoying, distortion is objectionable.
2	Poor	Annoying distortion, but not objectionable.
3	Fair	Perceptible distortion, slightly annoying.
4	Good	Slight perceptible level of distortion, but not annoying.
5	Excellent	Imperceptible level of distortion.
Table 4: Performances of different noise schedules on the multi-speaker VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) 6θ(∙) that was trained on VCTK for about
1M iterations.
Noise schedule	LS-MSE (1)		MCD (；)	STOI (↑)	PESQ (↑)	MOS (↑)
DDPM (Ho et al.,2020; Chen et al., 2020) 8 steps (Grid Search)	101	2.09			0.787	3.31	4.22 ± 0.04
1,000 steps (Linear)	85.0	2.02	0.798	3.39	4.40 ± 0.05
DDIM (Song et al., 2021)					
8 steps (Linear)	553	3.20	0.701	2.81	3.83 ± 0.04
16 steps (Linear)	412	2.90	0.724	3.04	3.88 ± 0.05
21 steps (Linear)	355	2.79	0.739	3.12	4.12 ± 0.05
100 steps (Linear)	259	2.58	0.759	3.30	4.27 ± 0.04
NE (San-Roman et al., 2021)					
8 steps (Linear)	208	2.54	0.740	3.10	4.18 ± 0.04
16 steps (Linear)	183	2.53	0.742	3.20	4.26 ± 0.04
21 steps (Linear)	852	3.57	0.699	2.66	3.70 ± 0.03
BDDM (c^n, Sn)					
8 steps (0.2, 0.9)	98.4	2.11	0.774	3.18	4.20 ± 0.04
16 steps (0.5, 0.5)	73.6	1.93	0.813	3.39	4.35 ± 0.05
21 steps (0.5, 0.1)	76.5	1.83	0.827	3.43	4.48 ± 0.06
Regarding the image generation task, to demonstrate the generalizability of our method, we directly
adopted a Score network pre-trained on the CIFAR-10 dataset implemented by a third-party open-
source repository. Regarding the schedule network, to demonstrate that it does not have to use
specialized architecture, we replaced GALR by the VGG11 (Simonyan & Zisserman, 2014), which
was also used by as a noise estimator in (San-Roman et al., 2021). The output dimension (number
of classes) of VGG11 was set to 1. Similar to the setting for GALR in speech synthesis, we added
a sigmoid activation to the last layer to ensure a [0, 1] output. Similar to the training in speech
domain, we trained the VGG11-based schedule networks while freezing the score networks for 10k
steps, which normally can be finished in about two hours.
Our code for the speech vocoding and the image generation experiments will be uploaded to Github
after the final decision of ICLR is released.
B.4	Crowd-sourced subjective evaluation
All our Mean Opinion Score (MOS) tests were crowd-sourced. We refer to the MOS scores in (Pro-
tasio Ribeiro et al., 2011), and the scoring criteria have been included in Table 3 for completeness.
The samples were presented and rated one at a time by the testers.
C Additional experiments
A demonstration page at https://bilateral- denoising- diffusion- model.
github.io shows some samples generated by BDDMs trained on LJ speech and VCTK datasets.
19
Published as a conference paper at ICLR 2022
C.1 Multi-speaker speech synthesis
In addition to the single-speaker speech synthesis, we evaluated BDDMs on the multi-speaker
speech synthesis benchmark VCTK (Yamagishi et al., 2019). VCTK consists of utterances sam-
pled at 48 KHz by 108 native English speakers with various accents. We split the VCTK dataset for
training and testing: 100 speakers were used for training the multi-speaker model and 8 speakers
for testing. We trained on a 44257-utterance subset (40 hours) and evaluated on a held-out 100-
utterance subset. For the score network, we used the Wavegrad architecture (Chen et al., 2020) so
as to examine whether the superiority of BDDMs remains in a different dataset and with a different
score network architecture.
Results are presented in Table 4. For this multi-speaker VCTK dataset, we obtained consistent
observations with that for the single-speaker LJ dataset presented in the main paper. Again, the
proposed BDDM with only 16 or 21 steps outperformed the DDPM with 1,000 steps. To the best
of our knowledge, ours was the first work that reported this degree of superior. When reducing
to 8 steps, BDDM obtained performance on par with (except for a worse PESQ) the costly grid-
searched 8 steps (which were unscalable to more steps) in DDPM. For NE, we could again observe
a degradation from its 16 steps to 21 steps, indicating the instability of NE for the VCTK dataset
likewise. In contrast, BDDM gave continuously improved performance while increasing the step
number.
C.2 Comparing different reverse processes for BDDMs
This section demonstrates that BDDMs do not restrict the sampling procedure to a specialized re-
verse process in Algorithm 4. In particular, we evaluated different reverse processes, including that
of DDPMs as shown in Eq. (4) and DDIMs (Song et al., 2021), for BDDMs and compared the ob-
jective scores on the generated samples. DDIMs (Song et al., 2021) formulate a non-Markovian gen-
erative process that accelerates the inference while keeping the same training procedure as DDPMs.
The original generative process in Eq. (4) in DDPMs is modified into
S
Pθτ)(X0:T) := π(XT) YPθγi)(xYi-ι |xYi) × Ypθt)(X0|xt),	(71)
i=1	t∈γ
where Y is a sub-sequence of length N of [1,...,T] with YN = T, and Y := {1,…，T}\ Y is defined
as its complement; Therefore, only part of the models are used in the sampling process.
To achieve the above, DDIMs defined a prediction function fθ(t)(Xt) that depends on θ to predict
the observation X0 given Xt directly:
fθ(t) (Xt) :
1
αt
(Xt - q、- α eθ (χt, a/).
(72)
By leveraging this prediction function, the conditionals in Eq. (71) are formulated as
pθγi) (XYi-i|XYi) = N (αγi-1 (XYi -ςCθ(XYi ,aYi)) ,σYiI )	ifi ∈ [N],i > 1
αγi
pθt)(X0∣Xt) = N (fθt)(Xt),σ2I) otherwise,
(73)
(74)
where the detailed derivation of σt and ς can be referred to (Song et al., 2021). In the original
DDIMs, the accelerated reverse process produces samples over the subsequence of β indexed by Y :
β = {βn∣n ∈ γ}. In BDDMs, to apply the DDIM reverse process, We use the β predicted by the
schedule network in place of a subsequence of the training schedule β.
Finally. the objective scores are given in Table 5. Note that the subjective evaluation (MOS) is
omitted here since the other assessments above have shown that the MOS scores are highly corre-
lated with the objective measures, including STOI and PESQ. They indicate that applying BDDMs
to either DDPM or DDIM reverse process leads to comparable and competitive results. Meanwhile,
the results show some subtle differences: BDDMs over a DDPM reverse process gave slightly better
samples in terms of signal error and consistency metrics (i.e., LS-MSE and MCD), while BDDM
over a DDIM reverse process tended to generate better samples in terms of intelligibility and per-
ceptual metrics (i.e., STOI and PESQ).
20
Published as a conference paper at ICLR 2022
Table 5: Performances of different reverse processes for BDDMs on the VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) e§(∙) and the same noise schedule.
Noise schedule	LS-MSE Q)	MCD (；)	STOI (↑)	PESQ (↑)
BDDM (DDPM reverse process)				
8 steps (0.3, 0.9, 1e-5)	91.3	2.19	0.936	3.22
16 steps (0.7, 0.1, 1e-6)	73.3	1.88	0.949	3.32
21 steps (0.5, 0.1, 1e-6)	72.2	1.91	0.950	3.33
BDDM (DDIM reverse process)				
8 steps (0.3, 0.9, 1e-5)	91.8	2.19	0.938	3.26
16 steps (0.7, 0.1, 1e-6)	77.7	1.96	0.953	3.37
21 steps (0.5, 0.1, 1e-6)	77.6	1.96	0.954	3.39
Table 6: Comparing sampling methods for DDPM with different number of sampling steps in terms
of FIDs in CIFAR10.
Sampling method	Sampling steps	FID
DDPM (baseline) (Ho et al., 2020)	1000	3.17
DDPM (sub-VP) (Song et al., 2020b)	〜100	3.69
DDPM (DP + reweighting) (Watson et al., 2021)	128 64	5.24 6.74
DDIM (quadratic) (Song et al., 2021)	100 50	4.16 4.67
FastDPM (approx. STEP) (Kong & Ping, 2021)	100 50	2.86 3.20
2Improved DDPM (hybrid) (Nichol & Dhariwal, 2021)	100 50	4.63 5.09
VDM (augmented) (Kingma et al., 2021)	1000	7.413
Ours BDDM	100 50	2.38 2.93
C.3 Unconditional image generation
For the unconditional image generation task, we evaluated the proposed BDDMs on the benchmark
CIFAR-10 (32 × 32) dataset. The score functions, including those initially proposed in DDPMs
(Ho et al., 2020) or DDIMs (Song et al., 2021) and those pre-trained in the above third-party im-
plementations, are all conditioned on a discrete step-index. We estimated the noise schedule β in
continuous space using the VGG11 schedule network and then mapped it to discrete time schedule
using the approximation method in (Kong & Ping, 2021).
Table 6 shows the performances of different sampling methods for DDPMs in CIFAR-10. By set-
ting the maximum number of sampling steps (N) for noise scheduling, we can fairly compare the
improvements achieved by BDDMs against related methods in the literature in terms of FID. Re-
markably, BDDMs with 100 sampling steps not only surpassed the 1000-step DDPM baseline, but
also produced the SOTA FID performance amongst all generative models using less than or equal to
100 sampling steps.
2Our implementation was based on https://github.com/openai/improved-diffusion
3The authors of VDM claimed that they tuned the hyperparameters only for minimizing the likelihood and
did not pursue further tuning of the model to improve FID.
21
Published as a conference paper at ICLR 2022
n = 3
n = 2
n = 1
n = 0
Figure 4: Spectrum plots of the speech samples produced by BDDM within 3 sampling steps. The
first row shows the spectrum of a random signal for starting the reverse process. Then, from the top
to the bottom, we show the spectrum of the resultant signal after each step of the reverse process
performed by the BDDM. We also provide the corresponding WAV files on our demo page.
22