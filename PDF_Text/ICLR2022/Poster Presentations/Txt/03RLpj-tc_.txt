Published as a conference paper at ICLR 2022
Crystal Diffusion Variational Autoencoder
for Periodic Material Generation
Tian Xie； Xiang Fu； Octavian-Eugen Ganea； Regina Barzilay, Tommi Jaakkola
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{txie,xiangfu,oct,regina,tommi}@csail.mit.edu
Abstract
Generating the periodic structure of stable materials is a long-standing challenge
for the material design community. This task is difficult because stable materials
only exist in a low-dimensional subspace of all possible periodic arrangements of
atoms: 1) the coordinates must lie in the local energy minimum defined by quan-
tum mechanics, and 2) global stability also requires the structure to follow the
complex, yet specific bonding preferences between different atom types. Existing
methods fail to incorporate these factors and often lack proper invariances. We
propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the
physical inductive bias of material stability. By learning from the data distribu-
tion of stable materials, the decoder generates materials in a diffusion process that
moves atomic coordinates towards a lower energy state and updates atom types to
satisfy bonding preferences between neighbors. Our model also explicitly encodes
interactions across periodic boundaries and respects permutation, translation, rota-
tion, and periodic invariances. We significantly outperform past methods in three
tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and real-
istic materials, and 3) generating materials that optimize a specific property. We
also provide several standard datasets and evaluation metrics for the broader ma-
chine learning community. * 1
1 Introduction
Solid state materials, represented by the periodic arrangement of atoms in the 3D space, are the
foundation of many key technologies including solar cells, batteries, and catalysis (Butler et al.,
2018). Despite the rapid progress of molecular generative models and their significant impact on
drug discovery, the problem of material generation has many unique challenges. Compared with
small molecules, materials have more complex periodic 3D structures and cannot be adequately
represented by a simple graph like molecular graphs (Figure 1). In addition, materials can be made
up of more than 100 elements in the periodic table, while molecules are generally only made up
of a small subset of atoms such as carbon, oxygen, and hydrogen. Finally, the data for training
ML models for material design is limited. There are only -200k experimentally known inorganic
materials, collected by the ICSD (Belsky et al., 2002), in contrast to close to a billion molecules in
ZINC (Irwin & Shoichet, 2005).
The key challenge of this task
is in generating stable materials.
Such materials only exist in a low-
dimensional subspace of all possi-
ble periodic arrangements of atoms:
1) the atom coordinates must lie in
the local energy minimum defined by
quantum mechanics (QM); 2) global
stability also requires the structure
to follow the complex, yet specific
Cθ≡BC1
Figure 1: The periodic structure of diamond. The left shows
the infinite periodic structure, the middle shows a unit cell
representing the periodic structure, and the right shows a
multi-graph (Xie & Grossman, 2018) representation.
(0,o,1
(07θ)
* Equal contribution. Correspondence to: Tian Xie at txie@csail.mit.edu
1Code and data are available at https://github.com/txie-93/cdvae
1
Published as a conference paper at ICLR 2022
bonding preferences between different atom types (section 3.2). The issue of stability is unique
to material generation because valency checkers assessing molecular stability are not applicable to
materials. Moreover, we also have to encode the interactions crossing periodic boundaries (Figure 1,
middle), and satisfy permutation, translation, rotation, and periodic invariances (section 3.1). Our
goal is to learn representations that can learn features of stable materials from data, while adhering
to the above invariance properties.
We address these challenges by learning a variational autoencoder (VAE) (Kingma & Welling,
2014) to generate stable 3D materials directly from a latent representation without intermediates
like graphs. The key insight is to exploit the fact that all materials in the data distribution are stable,
therefore if noise is added to the ground truth structure, denoising it back to its original structure
will likely increase stability. We capture this insight by designing a noise conditional score network
(NCSN) (Song & Ermon, 2019) as our decoder: 1) the decoder outputs gradients that drive the
atom coordinates to the energy local minimum; 2) it also updates atom types based on the neigh-
bors to capture the specific local bonding preferences (e.g., Si-O is preferred over Si-Si and O-O in
SiO2). During generation, materials are generated using Langevin dynamics that gradually deforms
an initial random structure to a stable structure. To capture the necessary invariances and encode the
interactions crossing periodic boundaries, we use SE(3) equivariant graph neural networks adapted
with periodicity (PGNNs) for both the encoder and decoder of our VAE.
Our theoretical analysis further reveals an intriguing connection between the gradient field learned
by our decoder and an harmonic force field. De facto, the decoder utilizes the latter to estimate the
forces on atoms when their coordinates deviate from the equilibrium positions. Consequently, this
formulation provides an important physical inductive bias for generating stable materials.
In this work, we propose Crystal Diffusion Variational AutoEncoder (CDVAE) to generate stable
materials by learning from the data distribution of known materials. Our main contributions include:
•	We curate 3 standard datasets from QM simulations and create a set of physically mean-
ingful tasks and metrics for the problem of material generation.
•	We incorporate stability as an inductive bias by designing a noise conditional score net-
work as the decoder of our VAE, which allows us to generate significantly more realistic
materials.
•	We encode permutation, translation, rotation, and periodic invariances, as well as interac-
tions crossing periodic boundaries with SE(3) equivariant GNNs adapted with periodicity.
•	Empirically, our model significantly outperforms past methods in tasks including recon-
structing an input structure, generating valid, diverse, and realistic materials, and generat-
ing materials that optimize specific properties.
2	Related Work
Material graph representation learning. Graph neural networks have made major impacts in ma-
terial property prediction. They were first applied to the representation learning of periodic materials
by Xie & Grossman (2018) and later enhanced by many studies including Schutt et al. (2018); Chen
et al. (2019). The Open Catalyst Project (OCP) provides a platform for comparing different architec-
tures by predicting energies and forces from the periodic structure of catalytic surfaces (Chanussot
et al., 2021). Our encoder and decoder PGNNs directly use GNN architectures developed for the
OCP (Klicpera et al., 2020b; 2021; Shuaibi et al., 2021; Godwin et al., 2021), which are also closely
related to SE(3) equivariant networks (Thomas et al., 2018; Fuchs et al., 2020).
Quantum mechanical search of stable materials. Predicting the structure of unknown materials
requires very expensive random search and QM simulations, and is considered a grand challenge
in materials discovery (Oganov et al., 2019). State-of-the-art methods include random sampling
(Pickard & Needs, 2011), evolutionary algorithms (Wang et al., 2012; Glass et al., 2006), substi-
tuting elements in known materials (Hautier et al., 2011), etc., but they generally have low success
rates and require extensive computation even on relatively small problems.
Material generative models. Past material generative models mainly focus on two different ap-
proaches, and neither incorporate stability as an inductive bias. The first approach treats materials
as 3D voxel images, but the process of decoding images back to atom types and coordinates often
results in low validity, and the models are not rotationally invariant (Hoffmann et al., 2019; Noh
et al., 2019; Court et al., 2020; Long et al., 2021). The second directly encodes atom coordinates,
2
Published as a conference paper at ICLR 2022
types, and lattices as vectors (Ren et al., 2020; Kim et al., 2020; Zhao et al., 2021), but the models
are generally not invariant to any Euclidean transformations. Another related method is to train a
force field from QM forces and then apply the learned force field to generate stable materials by
minimizing energy (Deringer et al., 2018; Chen & Ong, 2022). This method is conceptually similar
to our decoder, but it requires additional force data which is expensive to obtain. Remotely related
works include generating contact maps from chemical compositions (Hu et al., 2021; Yang et al.,
2021) and building generative models only for chemical compositions (Sawada et al., 2019; Pathak
et al., 2020; Dan et al., 2020).
Molecular conformer generation and protein folding . Our decoder that generates the 3D atomic
structures via a diffusion process is closely related to the diffusion models used for molecular con-
former generation (Shi et al., 2021; Xu et al., 2021b). The key difference is that our model does not
rely on intermediate representations like molecular graphs. G-SchNet (Gebauer et al., 2019) is more
closely related to our method because it directly generates 3D molecules atom-by-atom without re-
lying on a graph. Another closely related work is E-NFs (Satorras et al., 2021) that use a flow model
to generate 3D molecules. In addition, score-based and energy-based models have also been used
for molecular graph generation (Liu et al., 2021) and protein folding (Wu et al., 2021). Flow models
have also been used for molecular graph generation (Shi et al., 2020; Luo et al., 2021). However,
these generative models do not incorporate periodicity , which makes them unsuitable for materials.
3	Preliminaries
3.1	Periodic structure of materials
Any material structure can be represented as the periodic arrangement of atoms in the 3D space. As
illustrated in Figure 1, we can always find a repeating unit, i.e. a unit cell, to describe the infinite
periodic structure of a material. A unit cell that includes N atoms can be fully described by 3 lists:
1) atom types A = (a0, ..., aN) ∈ AN, where A denotes the set of all chemical elements; 2) atom
coordinates X = (x0, ..., xN) ∈ RN×3; and 3) periodic lattice L = (l1, l2, l3) ∈ R3×3. The
periodic lattice defines the periodic translation symmetry of the material. Given M = (A, X, L),
the infinite periodic structure can be represented as,
{(a0i,x0i)|a0i = ai, x0i =xi+k1l1 +k2l2+k3l3,k1,k2,k3 ∈Z},	(1)
where k1, k2, k3 are any integers that translate the unit cell using L to tile the entire 3D space.
The chemical composition of a material denotes the ratio of different elements that the material is
composed of. Given the atom types of a material with N atoms A ∈ AN , the composition can be
represented as c ∈ R|A| , where ci > 0 denotes the percentage of atom type i and Pi ci = 1. For
example, the composition of diamond in Figure 1 has c6 = 1 and ci = 0 for i 6= 6 because 6 is the
atomic number of carbon.
Invariances for materials. The structure of a material does not change under several invariances. 1)
Permutation invariance. Exchanging the indices of any pair of atoms will not change the material. 2)
Translation invariance. Translating the atom coordinates X by an arbitrary vector will not change
the material. 3) Rotation invariance. Rotating X and L together by an arbitrary rotation matrix
will not change the material. 4) Periodic invariance. There are infinite different ways of choosing
unit cells with different shapes and sizes, e.g., obtaining a bigger unit cell as an integer multiplier
of a smaller unit cell using integer translations. The material will again not change given different
choices of unit cells.
Multi-graph representation for materials. Materials can be represented as a directed multi-graph
G = {V, E} to encode the periodic structures following (Wells et al., 1977; O’Keeffe & Hyde, 1980;
Xie & Grossman, 2018), where V = {v1, ..., vN} is the set of nodes representing atoms and E =
{eij,(k1,k2,k3) |i,j ∈ {1, ..., N}, k1, k2,k3 ∈ Z} is the set of edges representing bonds. eij,(k1,k2,k3)
denotes a directed edge from node i at the original unit cell to node j at the cell translated by
k1l1 + k2l2 + k3l3 (in Figure 1 right, (k1, k2, k3) are labeled on top of edges). For materials, there
is no unique way to define edges (bonds) and the edges are often computed using k-nearest neighbor
(KNN) approaches under periodicity or more advanced methods such as CrystalNN (Pan et al.,
2021). Given this directed multi-graph, message-passing neural networks and SE(3)-equivariant
networks can be used for the representation learning of materials.
3
Published as a conference paper at ICLR 2022
Add noises A,X→A,X	「	r - r
------r- = ∙=∙A∣ ■ ■ I	Denoise A, X	∣ ∙	∣
M=(A,X,L) I	i ---------ς-----► I	I
, Conditional , I ∙ι PGNNDEC(河Z) ∣∙ ∙∣
E	E Encode
I	I -------------►
1	∙ 1 PGNNenc(M)
--------► Training
--------►	Generation
Predia	. 0.5
-------------0.5
MLPAGG(Z)	CompositionC
Ra = (A,x,L)	_Rant init
I-I-I
+ I I +	4
L-J
Lattice L # of atoms N
£AGG
Conditional
Langevin dynamics	!"- 
盛一1：£ 一 ；： ɪ
PGNNDEC(MZ)	[_7_ _
Figure 2:	Overview of the proposed CDVAE approach.
3	.2 Problem definition and its physical origin
Our goal is to generate novel, stable materials M = (A, X, L) ∈ AN × RN×3 × R3×3. The space
of stable materials is a subspace in AN × RN ×3 × R3×3 that satisfies the following constraints. 1)
The materials lie in the local minimum of the energy landscape defined by quantum mechanics, with
respect to the atom coordinates and lattice, i.e. ∂E∕∂X = 0 and ∂E∕∂L = 0. 2) The material is
globally stable and thus cannot decompose into nearby phases. Global stability is strongly related
to bonding preferences between neighboring atoms. For example, in SiO2, each Si is surrounded
by 4 O and each O is surrounded by 2 Si. This configuration is caused by the stronger bonding
preferences between Si-O than Si-Si and O-O.
Generally, finding novel, stable materials requires very expensive random search and quantum me-
chanical simulations. To bypass this challenge, we aim to learn a generative model p(M) from the
empirical distribution of experimentally observed stable materials. A successful generative model
will be able to generate novel materials that satisfy the above constraints, which can then be verified
using quantum mechanical simulations.
3	.3 Diffusion models
Diffusion models are a new class of generative models that have recently shown great success in
generating high-quality images (Dhariwal & Nichol, 2021), point clouds (Cai et al., 2020; Luo
& Hu, 2021), and molecular conformations (Shi et al., 2021). There are several different types
of diffusion models including diffusion probabilistic models (Sohl-Dickstein et al., 2015), noise-
conditioned score networks (NCSN) (Song & Ermon, 2019), and denoising diffusion probabilistic
models (DDPM) (Ho et al., 2020). We follow ideas from the NCSN (Song & Ermon, 2019) and learn
a score network sθ (x) to approximate the gradient of a probability density Vχp(x) at different noise
levels. Let {σi}M be a sequence of positive scalars that satisfies σ1∕σ2 = ... = ol-i/ol > 1. We
define the data distribution perturbed by Gaussian noise σ as qσ(x) = pdata (t)N (x|t, σ2I) dt.
The goal of NCSN is to learn a score network to jointly estimate the scores of all perturbed data
distributions, i.e. ∀o ∈ {oi}iL=1 : sθ(x, o) ≈ Vxqσ(x). During generation, NCSN uses an
annealed Langevin dynamics algorithm to produce samples following the gradient estimated by the
score network with a gradually reduced noise level.
4	Proposed Method
Our approach generates new materials via a two-step process: 1) We sample a z from the latent space
and use it to predict 3 aggregated properties ofa material: composition (c), lattice (L), and number
of atoms (N), which are then used to randomly initialize a material structure M = (A, X, L). 2)
We perform Langevin dynamics to simultaneously denoise X and A conditioned on z to improve
both the local and global stability of M and generate the final structure of the new material.
To train our model, we optimize 3 networks concurrently using stable materials M = (A, X, L)
sampled from the data distribution. 1) A periodic GNN encoder PGNNENC (M ) that encodes M
into a latent representation z. 2) A property predictor MLPAGG(z) that predicts the c, L, and N of
M from z. 3) A periodic GNN decoder PGNNDEC(M |z) that denoises both X and A conditioned
4
Published as a conference paper at ICLR 2022
TΓ∏ C' . 1	∙	,	Λ^Λ-	/ A ɪz- T ∖ ∙	1 ,	FK	1 T	T l'i'	. 1	1 C ∙
on z. For 3), the noisy structure M = (A, X , L) is obtained by adding different levels of noise
to X and A. The noise schedules are defined by the predicted aggregated properties, with the
motivation of simplifying the task for our decoder from denoising an arbitrary random structure
from over 〜100 elements to a constrained random structure from predicted properties. We train
all three networks together by minimizing a combined loss including the aggregated property loss
LAGG, decoder denoising loss LDEC, and a KL divergence loss LKL for the VAE.
To capture the interactions across periodic boundaries, we employ a multi-graph representation (sec-
tion 3.1) for both M and M. We also use SE(3) equivariant GNNs adapted with periodicity as both
the encoder and the decoder to ensure the permutation, translation, rotation, and periodic invariances
of our model. The CDVAE is summarized in Figure 2 and we explain the individual components of
our method below. The implementation details can be found in Appendix B.
Periodic material encoder. PGNNENC (M) encodes a material M as a latent representation z ∈
RD following the reparameterization trick in VAE (Kingma & Welling, 2014). We use the multi-
graph representation (refer to section 3.1) to encode M, and PGNNENC can be parameterized with
an SE(3) invariant graph neural network.
Prediction of aggregated properties. MLPAGG (z) predicts 3 aggregated properties of the encoded
material from its latent representation z. It is parameterized by 3 separate multilayer perceptrons
(MLPs). 1) Composition c ∈ R|A| is predicted by minimizing the cross entropy between the ground
truth composition and predicted composition, i.e. - Pi pi log ci. 2) Lattice L ∈ R3×3 is reduced
to 6 unique, rotation invariant parameters with the Niggli algorithm (Grosse-Kunstleve et al., 2004),
i.e., the lengths of the 3 lattice vectors, the angles between them, and the values are predicted with
an MLP after being normalized to the same scale (Appendix B.1) with an L2 loss. 3) Number of
atoms N ∈ {1, 2, ...} is predicted with a softmax classification loss from the set of possible number
of atoms. LAGG is a weighted sum of the above 3 losses.
Conditional score matching decoder. PGNNDEC (MM|z) is a PGNN that inputs a noisy mate-
rial MM with type noises σA, coordinate noises σχ, as well as a latent z, and outputs 1) a score
SX (MM|z; σA, σχ) ∈ RN×3 to denoise the coordinate for each atom towards its ground truth value,
and 2) a probability distribution of the true atom types PA(MM|z;σA,σχ) ∈ RN×lAl. We use a
SE(3) graph network to ensure the equivariance of sX with respect to the rotation of M . To obtain
the noisy structures M , we sample σA and σX from two geometric sequences of the same length:
{σA,j}jL=1, {σX,j}jL=1, and add the noises with the following methods. For type noises, we use the
type distribution defined by the predicted composition c to linearly perturb true type distribution
A ~ (J+：. Pa + [+A. Pc) , where PA,ij = 1 if atom i has the true atom type j and PA,ij = 0 for
all other j s, and pc is the predicted composition. For coordinate noises, we add Gaussian noises to
the true coordinates X 〜N(X,σX2 I).
PGNNDEC is parameterized by a SE(3) equivariant PGNN that inputs a multi-graph representation
(section 3.1) of the noisy material structure and the latent representation. The node embedding for
node i is obtained by the concatenation of the element embedding of <⅛ and the latent representation
z, followed by a MLP, h0 = MLP(ea@) ∣∣ z), where ∣∣ denotes concatenation of two vectors
and ea is a learned embedding for elements. After K message-passing layers, PGNNDEC outputs
a vector per node that is equivariant to the rotation of M . These vectors are used to predict the
scores, and we follow Song & Ermon (2019); Shi et al. (2021) to parameterize the score network
with noise scaling: SX (M|z; σA,σχ) = SX (M∣z)∕σχ. The node representations hK are used
to predict the distribution of true atom types, and the type predictor is the same at all noise levels:
PA(M|z; σA, σχ) = PA(M|z), PA(M|z)i = Softmax(MLP(hK)).
Periodicity influences denoising target. Due to periodicity, a specific atom i may move out of the
unit cell defined by L when the noise is sufficiently large. This leads to two different ways to define
the scores for node i. 1) Ignore periodicity and define the target score as Xi — X/ or 2) Define the
target score as the shortest possible displacement between Xi and Xi considering periodicity, i.e.
dmin(xi, Xi) = min® ,k2,k3 (xi — Xi + k[l[ + k2l2 + k3l3). We choose 2) because the scores are
the same given two different X that are periodically equivalent, which is mathematically grounded
for periodic structures, and empirically results in much more stable training.
5
Published as a conference paper at ICLR 2022
The training loss for the decoder LDEC can be written as,
2L XL： [Eqdata(M )j,σXj (MM)(辰(Mz) -	+ ^j La (PA(MM⑶，PA N
(2)
where λa denotes a coefficient for balancing the coordinate and type losses, La denotes the cross
entropy loss over atom types, pA denotes the true atom type distribution. Note that to simplify the
equation, we follow the loss coefficients in Song & Ermon (2019) for different σX,j and σA,j and
factor them into Equation 2.
Material generation with Langevin dy-
namics. After training the model, we can
generate the periodic structure of mate-
rial given a latent representation z . First,
we use z to predict the aggregated prop-
erties: 1) composition c, 2) lattice L,
and 3) the number of atoms N . Then,
we randomly initialize an initial peri-
odic structure (A0 , X0 , L) with the ag-
gregated properties and perform an an-
nealed Langevin dynamics (Song & Er-
mon, 2019) using the decoder, simulta-
neously updating the atom types and co-
ordinates. During the coordinate update,
we map the coordinates back to the unit
cell at each step if atoms move out of the
cell. The algorithm is summarized in Al-
gorithm 1.
Algorithm 1 Material Generation via Annealed
Langevin Dynamics
1:	Input: latent representation z, type and coordinate noise
levels {σA}, {σX }, step size , number of sampling
steps T
2:	Predict aggregated properties c,L,N from z.
3:	Uniformly initialize X0 within the unit cell by L.
4:	Randomly initialize A0 with c.
5:	for j - 1 to L do
6：	αj . € ∙ σX,j∕σX,L
7:	for t - 1 to T do
8：	sχ,t — SX (At-1, Xt-1, L∣z; σA,j, σχ,j)
9：	PA,t — PA(At-ι, Xt-ι, L|z； σA,j,σχ,j)
10:	Draw Xt 〜N(0, I)
11：	Xt ― Xt-ι + αj sχ,t + √20iXf
12:	Xt - back_to_Cell(Xt, L)
13:	At = argmax PA,t
14:	Xo - XT, Ao - AT
Connection between the gradient field and a harmonic force field. The gradient field sX(M|z)
is used to update atom coordinates in Langevin dynamics via the force term, αjsX,t. In Appendix A,
We show that a§sχ,t is mathematically equivalent to2 a harmonic force field F(X) = -k(X -
X) when the noises are small, where X is the equilibrium position of the atoms and k is a force
constant. Harmonic force field, i.e. spring-like force field, is a simple yet general physical model
that approximates the forces on atoms when they are close to their equilibrium locations. This
indicates that our learned gradient field utilizes the harmonic approximation to approximate QM
forces without any explicit force data and generates stable materials with this physically motivated
inductive bias.
5	Experiments
We evaluate multiple aspects of material generation that are related to real-world material discovery
process. Past studies in this field used very different tasks and metrics, making it difficult to compare
different methods. Building upon past studies (Court et al., 2020; Ren et al., 2020), we create a set
of standard tasks, datasets, and metrics to evaluate and compare models for material generation.
Experiment details can be found in Appendix D.
Tasks. We focus on 3 tasks for material generation. 1) Reconstruction evaluates the ability of the
model to reconstruct the original material from its latent representation z. 2) Generation evaluates
the validity, property statistics, and diversity of material structures generated by the model. 3)
Property optimization evaluates the model’s ability to generate materials that are optimized for a
specific property.
Datasets. We curated 3 datasets representing different types of material distributions. 1) Perov-
5 (Castelli et al., 2012a;b) includes 18928 perovskite materials that share the same structure but
differ in composition. There are 56 elements and all materials have 5 atoms in the unit cell. 2)
Carbon-24 (Pickard, 2020) includes 10153 materials that are all made up of carbon atoms but differ
in structures. There is 1 element and the materials have 6 - 24 atoms in the unit cells. 3) MP-20
(Jain et al., 2013) includes 45231 materials that differ in both structure and composition. There are
2In fact, this is also true for the original formulation of NCSN (Song & Ermon, 2019)
6
Published as a conference paper at ICLR 2022
Perov-5	Carbon-24	MP-20
Ground Truth
FTCP
Cond-DFC-VAE
CDVAE
Figure 3:	Reconstructed structures of randomly selected materials in the test set. Note our model
reconstructs rotated (translated) version of the original material due to the SE(3) invariance.
Method	Table 1: Reconstruction performance. Matchrate(%) ↑	RMSE ] Perov-5 Carbon-24 MP-20 Perov-5 Carbon-24 MP-20
FTCP Cond-DFC-VAE CDVAE	99.34	6228	69.89	0.0259	0.2563	0.1593 51.65	-	-	0.0217	-	- 97.52	55.22	45.43	0.0156	0.1251	0.0356
89 elements and the materials have 1 - 20 atoms in the unit cells. We use a 60-20-20 random split
for all of our experiments. Details regarding dataset curation can be found at Appendix C.
Stability of materials in datasets. Structures in all 3 datasets are obtained from QM simulations and
all structures are at local energy minima. Most materials in Perov-5 and Carbon-24 are hypothetical,
i.e. they may not have global stability (section 3.2) and likely cannot be synthesized. MP-20 is a
realistic dataset that includes most experimentally known inorganic materials with at most 20 atoms
in the unit cell, most of which are globally stable. A model achieving good performance in MP-20
has the potential to generate novel materials that can be experimentally synthesized.
Baselines. We compare CDVAE with the following 4 baselines, which include the latest coordinate-
based, voxel-based, and 3D molecule generation methods. FTCP (Ren et al., 2020) is a crystal
representation that concatenates real-space properties (atom positions, atom types, etc.) and Fourier-
transformed momentum-space properties (diffraction pattern). A 1D CNN-VAE is trained over this
representation for crystal generation. Cond-DFC-VAE (Court et al., 2020) encodes and generates
crystals with 3D density maps, while employing several modifications over the previous Voxel-VAE
(Hoffmann et al., 2019) method. However, the effectiveness is only demonstrated for cubic systems,
limiting its usage to the Perov-5 dataset. G-SchNet (Gebauer et al., 2019) is an auto-regressive
model that generates 3D molecules by performing atom-by-atom completion using SchNet (Schutt
et al., 2018). Since G-SchNet is unaware of periodicity and cannot generate the lattice L. We
adapt G-SchNet to our material generation tasks by constructing the smallest oriented bounding box
with PCA such that the introduced periodicity does not cause structural invalidity. P-G-SchNet is
our modified G-SchNet that incorporates periodicity. During training, the SchNet encoder inputs
the partial periodic structure to predict next atoms. During generation, we first randomly sample a
lattice L from training data and autoregressively generate the periodic structure.
5.1	Material reconstruction
Setup. The first task is to reconstruct the material from its latent representation. We evaluate recon-
struction performance by matching the generated structure and the input structure for all materials
in the test set. We use StructureMatcher from pymatgen (Ong et al., 2013), which finds the
best match between two structures considering all invariances of materials. The match rate is the
percentage of materials satisfying the criteria stol=0.5, angle_tol=10, ltol=0.3. The
RMSE is averaged over all matched materials. Because the inter-atomic distances can vary signifi-
cantly for different materials, the RMSE is normalized by TV/N, roughly the average atom radius
per material. Note G-SchNet is not a VAE so we do not evaluate its reconstruction performance.
Results. The reconstructed structures are shown in Figure 3 and the metrics are in Table 1. Since
our model is SE(3) invariant, the generated structures may be a translated (or rotated) version of the
ground truth structure. Our model has a lower RMSE than all other models, indicating its stronger
capability to reconstruct the original stable structures. FTCP has a higher match rate than our model.
7
Published as a conference paper at ICLR 2022
Perov-5	Carbon-24	MP-20
FTCP
Cond-DFC-VAE
G-SchNet
G-SchNet (PeriOdic)
CDVAE
-*≡H>*-	-≡O- -Oa~
Figure 4: Structures sampled from N(0, 1) and filtered by the validity test.
Table 2: Generation performance3.
Method	Data	Validity(%) 3 4 ↑		COV (%) ↑		Property Statistics，		
		Struc.	Comp.	R.	P.	ρ	E	# elem.
FTCP 5	Perov-5	-024~~	54.24	-0.00~~	0.00	10.27	156.0	0.6297
	Carbon-24	0.08	-	0.00	0.00	5.206	19.05	-
	MP-20	1.55	48.37	4.72	0.09	23.71	160.9	0.7363
Cond-DFC-VAE	Perov-5	73.60	82.95	73.92	10.13	2.268	4.111	0.8373
G-SchNet	Perov-5	99.92	98.79	0.18	0.23	1.625	4.746	0.03684
	Carbon-24	99.94	-	0.00	0.00	0.9427	1.320	-
	MP-20	99.65	75.96	38.33	99.57	3.034	42.09	0.6411
P-G-SchNet	Perov-5	79.63	99.13	0.37	0.25	0.2755	1.388	0.4552
	Carbon-24	48.39	-	0.00	0.00	1.533	134.7	-
	MP-20	77.51	76.40	41.93	99.74	4.04	2.448	0.6234
CDVAE	Perov-5	100.0	98.59	99.45	98.46	0.1258	0.0264	0.0628
	Carbon-24	100.0	-	99.80	83.08	0.1407	0.2850	-
	MP-20	100.0	86.70	99.15	99.49	0.6875	0.2778	1.432
This can be explained by the fact that the same set of local structures can be assembled into different
stable materials globally (e.g., two different crystal forms of ZnS). Our model is SE(3) invariant
and only encodes local structures, while FTCP directly encodes the absolute coordinates and types
of each atom. In Figure 5, we show that CDVAE can generate different plausible arrangements of
atoms by sampling 3 Langevin dynamics with different random seeds from the same z. We note
that this capability could be an advantage since it generates more diverse structures than simply
reconstructing the original ones.
5.2	Material generation
Setup. The second task is to generate novel, stable materials that are distributionally similar to the
test materials. The only high-fidelity evaluation of stability of generated materials is to perform
QM calculations, but it is computationally prohibitive to use QM for computing evaluation metrics.
We developed several physically meaningful metrics to evaluate the validity, property statistics, and
diversity of generated materials. 1) Validity. Following Court et al. (2020), a structure is valid as
long as the shortest distance between any pair of atoms is larger than 0.5 A, which is a relative weak
criterion. The composition is valid if the overall charge is neutral as computed by SMACT (Davies
et al., 2019). 2) Coverage (COV). Inspired by Xu et al. (2021a); Ganea et al. (2021), we define two
coverage metrics, COV-R (Recall) and COV-P (Precision), to measure the similarity between ensem-
bles of generated materials and ground truth materials in test set. Intuitively, COV-R measures the
percentage of ground truth materials being correctly predicted, and COV-P measures the percentage
of predicted materials having high quality (details in Appendix G). 3) Property statistics. We com-
pute the earth mover’s distance (EMD) between the property distribution of generated materials and
test materials. We use density (ρ, unit g/cm3), energy predicted by an independent GNN (E, unit
eV/atom), and number of unique elements (# elem.) as our properties. Validity and coverage are
computed over 10,000 materials randomly sampled from N (0, 1). Property statistics is computed
over 1,000 valid materials randomly sampled from those that pass the validity test.
3Some metrics unsuitable for specific datasets have "-" values in the table (explained in Appendix D.1).
4For comparison, the ground truth structure validity is 100.0 % for all datasets, and ground truth composition
validity is 98.60 %, 100.0 %, 91.13 % for Perov-5, Carbon-24, and MP-20.
5Due to the low validity of FTCP, we instead randomly generate 100,000 materials from N (0, 1) and use
1,000 materials from those valid ones to compute diversity and property statistics metrics.
8
Published as a conference paper at ICLR 2022
Table 3: Property optimization performance.
Method	SR5	Perov-5 SR10	SR15	SR5	Carbon-24 SR10	SR15	SR5	MP-20 SR10	SR15
FTCP	0.06	0.11	0.16	0.0	0.0	0.0	0.02	0.04	0.05
Cond-DFC-VAE	0.55	0.64	0.69	-	-	-	-	-	-
CDVAE	0.52	0.65	0.79	0.0	0.06	0.06	0.78	0.86	0.90
Results. The generated structures are shown in Figure 4 and the metrics are in Table 2. Our model
achieves a higher validity than FTCP, Cond-DFC-VAE, and P-G-SchNet, while G-SchNet achieves
a similar validity as ours. The lower structural validity in P-G-SchNet than G-SchNet is likely due to
the difficulty of avoiding atom collisions during the autoregressive generation inside a finite periodic
box. On the contrary, our G-SchNet baseline constructs the lattice box after the 3D positions of all
atoms are generated, and the construction explicitly avoids introducing invalidity. Furthermore, our
model also achieves higher COV-R and COV-P than all other models, except in MP-20 our COV-P is
similar to G-SchNet and P-G-SchNet. These results indicate that our model generates both diverse
(COV-R) and high quality (COV-P) materials. More detailed results on the choice of thresholds for
COV-R and COV-P, as well as additional metrics can be found in Appendix G. Finally, our model
also significantly outperforms all other models in the property statistics of density and energy, further
confirming the high quality of generated materials. We observe that our method tends to generate
more elements in a material than ground truth, which explains the lower performance in the statistics
of # of elems. than G-SchNet. We hypothesize this is due to the non-Gaussian statistical structure
of ground truth materials (details in Appendix D.3), and using a more complex prior, e.g., a flow-
model-transformed Gaussian (Yang et al., 2019), might resolve this issue.
5.3	Property optimization
Setup. The third task is to generate materials that optimize a specific property. Following Jin et al.
(2018), we jointly train a property predictor F parameterized by an MLP to predict properties of
training materials from latent z . To optimize properties, we start with the latent representations
of testing materials and apply gradient ascent in the latent space to improve the predicted property
F(∙). After applying 5000 gradient steps with step sizes of 1 X 10-3, 10 materials are decoded
from the latent trajectories every 500 steps. We use an independently trained property predictor
to select the best one from the 10 decoded materials. Cond-DFC-VAE is a conditional VAE so
we directly condition on the target property, sample 10 materials, and select the best one using the
property predictor. For all methods, we generate 100 materials following the protocol above. We
use the independent property predictor to predict the properties for evaluation. We report the success
rate (SR) as the percentage of materials achieving 5, 10, and 15 percentiles of the target property
distribution. Our task is to minimize formation energy per atom for all 3 datasets.
Results. The performance is shown in Table 3. We significantly outperform FTCP, while having
a similar performance as Cond-DFC-VAE in Perov-5 (Cond-DFC-VAE cannot work for Carbon-24
and MP-20). Both G-SchNet and P-G-SchNet are incapable of property optimization 6. We note
that all models perform poorly on the Carbon-24 dataset, which might be explained by the complex
and diverse 3D structures of carbon.
6 Conclusions and Outlook
We have introduced a Crystal Diffusion Variational Autoencoder (CDVAE) to generate the periodic
structure of stable materials and demonstrated that it significantly outperforms past methods on the
tasks of reconstruction, generation, and property optimization. We note that the last two tasks are
far more important for material design than reconstruction because they can be directly used to
generate new materials whose properties can then be verified by QM simulations and experiments.
We believe CDVAE opens up exciting opportunities for the inverse design of materials for various
important applications. Meanwhile, our model is just a first step towards the grand challenge of
material design. We provide our datasets and evaluation metrics to the broader machine learning
community to collectively develop better methods for the task of material generation.
6Very recently the authors published an improved version for conditional generation (Gebauer et al., 2021)
but the code has not been released yet.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
We have made the following efforts to ensure reproducibility: 1) We provide our code at https://
github.com/txie-93/cdvae; 2)We provide our data and corresponding train/validation/test
splits at https://github.com/txie-93/cdvae/tree/main/data; 3) We provide de-
tails on experimental configurations in Appendix D.
Acknowledgments
We thank Peter Mikhael, Jason Yim, Rachel Wu, Bracha Laufer, Gabriele Corso, Felix Faltings,
Bowen Jing, and the rest of the RB and TJ group members for their helpful comments and sugges-
tions. The authors gratefully thank DARPA (HR00111920025), the consortium Machine Learning
for Pharmaceutical Discovery and Synthesis (mlpds.mit.edu), and MIT-GIST collaboration for sup-
port.
References
Alec Belsky, Mariette Hellenbrandt, Vicky Lynn Karen, and Peter Luksch. New developments in
the inorganic crystal structure database (icsd): accessibility in support of materials research and
design. AccOi CrystaUographiCa Section B: Structural Science, 58(3):364-369, 2002. 1, 16
Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine
learning for molecular and materials science. Nature, 559(7715):547-555, 2018. 1
Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and
Bharath Hariharan. Learning gradient fields for shape generation. In Computer Vision-ECCV
2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II116,
pp. 364-381. Springer, 2020. 4
Ivano E Castelli, David D Landis, Kristian S Thygesen, S0ren Dahl, Ib Chorkendorff, Thomas F
Jaramillo, and Karsten W Jacobsen. New cubic perovskites for one-and two-photon water splitting
using the computational materials repository. Energy & Environmental Science, 5(10):9034-
9043, 2012a. 6, 16
Ivano E Castelli, Thomas Olsen, Soumendu Datta, David D Landis, S0ren Dahl, Kristian S Thyge-
sen, and Karsten W Jacobsen. Computational screening of perovskite metal oxides for optimal
solar light capture. Energy & Environmental Science, 5(2):5814-5819, 2012b. 6, 16
Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane
Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020
(oc20) dataset and community challenges. ACS Catalysis, 11(10):6059-6072, 2021. 2, 15
Chi Chen and Shyue Ping Ong. A universal graph deep learning interatomic potential for the periodic
table. arXiv preprint arXiv:2202.02450, 2022. 3
Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal
machine learning framework for molecules and crystals. Chemistry of Materials, 31(9):3564-
3572, 2019. 2
Callum J Court, Batuhan Yildirim, Apoorv Jain, and Jacqueline M Cole. 3-d inorganic crystal
structure generation and property prediction via representation learning. Journal of chemical
information and modeling, 60(10):4518-4535, 2020. 2, 6, 7, 8
Yabo Dan, Yong Zhao, Xiang Li, Shaobo Li, Ming Hu, and Jianjun Hu. Generative adversarial
networks (gan) based efficient sampling of chemical composition space for inverse design of
inorganic materials. npj Computational Materials, 6(1):1-7, 2020. 3
Daniel W Davies, Keith T Butler, Adam J Jackson, Jonathan M Skelton, Kazuki Morita, and Aron
Walsh. Smact: Semiconducting materials by analogy and chemical theory. Journal of Open
Source Software, 4(38):1361, 2019. 8, 16
Volker L Deringer, Chris J Pickard, and Gabor Csanyi. Data-driven learning of total and local
energies in elemental boron. Physical review letters, 120(15):156001, 2018. 3
10
Published as a conference paper at ICLR 2022
Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint
arXiv:2105.05233, 2021. 4
Fabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-
translation equivariant attention networks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Had-
sell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Process-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html. 2
Octavian-Eugen Ganea, Lagnajit Pattanaik, Connor W Coley, Regina Barzilay, Klavs F Jensen,
William H Green, and Tommi S Jaakkola. Geomol: Torsional geometric generation of molecular
3d conformer ensembles. arXiv preprint arXiv:2106.07802, 2021. 8, 18
Niklas Gebauer, Michael Gastegger, and Kristof Schutt. Symmetry-adapted generation of 3d point
sets for the targeted discovery of molecules. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. 3, 7
Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Muller, and Kristof T
Schutt. Inverse design of 3d molecular structures with conditional generative neural networks.
arXiv preprint arXiv:2109.04824, 2021. 9
Colin W Glass, Artem R Oganov, and Nikolaus Hansen. Uspex—evolutionary crystal structure
prediction. Computer physics communications,175(11-12):713-720,2006. 2
Jonathan Godwin, Michael Schaarschmidt, Alexander Gaunt, Alvaro Sanchez-Gonzalez, Yulia
Rubanova, Petar Velickovic, James Kirkpatrick, and Peter Battaglia. Very deep graph neural
networks via noise regularisation. arXiv preprint arXiv:2106.07971, 2021. 2
Ralf W Grosse-Kunstleve, Nicholas K Sauter, and Paul D Adams. Numerically stable algorithms
for the computation of reduced unit cells. Acta Crystallographica Section A: Foundations of
Crystallography, 60(1):1-6, 2004. 5, 15
Geoffroy Hautier, Chris Fischer, Virginie Ehrlacher, Anubhav Jain, and Gerbrand Ceder. Data mined
ionic substitutions for the discovery of new compounds. Inorganic chemistry, 50(2):656-663,
2011. 2
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
4c5bcfec8584af0d967f1ab10179ca4b- Abstract.html. 4
Jordan Hoffmann, Louis Maestrati, Yoshihide Sawada, Jian Tang, Jean Michel Sellier, and Yoshua
Bengio. Data-driven approach to encoding and decoding 3-d crystal structures. arXiv preprint
arXiv:1909.00949, 2019. 2,7
Jianjun Hu, Wenhui Yang, Rongzhi Dong, Yuxin Li, Xiang Li, Shaobo Li, and Edirisuriya MD
Siriwardane. Contact map based crystal structure prediction using global optimization. Crys-
tEngComm, 23(8):1765-1776, 2021. 3
John J Irwin and Brian K Shoichet. Zinc- a free database of commercially available compounds for
virtual screening. Journal of chemical information and modeling, 45(1):177-182, 2005. 1
Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen
Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The ma-
terials project: A materials genome approach to accelerating materials innovation. APL materials,
1(1):011002, 2013. 6, 16
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In International conference on machine learning, pp. 2323-2332.
PMLR, 2018. 9
11
Published as a conference paper at ICLR 2022
Sungwon Kim, Juhwan Noh, Geun Ho Gu, Alan Aspuru-Guzik, and Yousung Jung. Generative
adversarial networks for crystal structure prediction. ACS central Science, 6(8):1412-1420, 2020.
3
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/
abs/1312.6114. 2, 5
Johannes Klicpera, Shankari Giri, Johannes T Margraf, and Stephan Gunnemann. Fast and
uncertainty-aware directional message passing for non-equilibrium molecules. arXiv preprint
arXiv:2011.14115, 2020a. 15
Johannes Klicpera, Janek Groβ, and Stephan GUnnemann. Directional message passing for molec-
ular graphs. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020b. URL https://openreview.
net/forum?id=B1eWbxStPH. 2, 15
Johannes Klicpera, Florian Becker, and Stephan GUnnemann. Gemnet: Universal directional graph
neural networks for molecules. arXiv preprint arXiv:2106.08903, 2021. 2, 15
Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint
arXiv:2106.00132, 2021. 17
Meng Liu, Keqiang Yan, Bora Oztekin, and Shuiwang Ji. Graphebm: Molecular graph generation
with energy-based models. arXiv preprint arXiv:2102.00546, 2021. 3
Teng Long, Nuno M Fortunato, Ingo Opahle, Yixuan Zhang, Ilias Samathrakis, Chen Shen, Oliver
Gutfleisch, and Hongbin Zhang. Constrained crystals deep convolutional generative adversarial
network for the inverse design of crystal structures. npj Computational Materials, 7(1):1-7, 2021.
2
Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837-2845,
2021. 4
Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph
generation. arXiv preprint arXiv:2102.01189, 2021. 3
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021. 17
Juhwan Noh, Jaehoon Kim, Helge S Stein, Benjamin Sanchez-Lengeling, John M Gregoire, Alan
Aspuru-Guzik, and Yousung Jung. Inverse design of solid-state materials via a continuous repre-
sentation. Matter, 1(5):1370-1384, 2019. 2
Artem R Oganov, Chris J Pickard, Qiang Zhu, and Richard J Needs. Structure prediction drives
materials discovery. Nature Reviews Materials, 4(5):331-348, 2019. 2
M. O’Keeffe and B. G. Hyde. Plane nets in crystal chemistry. Philosophical Transactions of the
Royal Society of London. Series A, Mathematical and Physical Sciences, 295(1417):553-618,
1980. ISSN 00804614. URL http://www.jstor.org/stable/36648. 3
Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher,
Shreyas Cholia, Dan Gunter, Vincent L Chevrier, Kristin A Persson, and Gerbrand Ceder. Python
materials genomics (pymatgen): A robust, open-source python library for materials analysis.
Computational Materials Science, 68:314-319, 2013. 7, 15, 18
Hillary Pan, Alex M Ganose, Matthew Horton, Muratahan Aykol, Kristin A Persson, Nils ER Zim-
mermann, and Anubhav Jain. Benchmarking coordination number prediction algorithms on inor-
ganic crystal structures. Inorganic chemistry, 60(3):1590-1603, 2021. 3, 15
Yashaswi Pathak, Karandeep Singh Juneja, Girish Varma, Masahiro Ehara, and U Deva Priyakumar.
Deep learning enabled inorganic material generator. Physical Chemistry Chemical Physics, 22
(46):26935-26943, 2020. 3
12
Published as a conference paper at ICLR 2022
Chris J. Pickard. Airss data for carbon at 10gpa and the c+n+h+o system at 1gpa, 2020. URL
https://archive.materialscloud.org/record/2020.0026/v1. 6
Chris J Pickard and RJ Needs. High-pressure phases of silane. Physical review letters, 97(4):045504,
2006. 16
Chris J Pickard and RJ Needs. Ab initio random structure searching. Journal of Physics: Condensed
Matter, 23(5):053201, 2011. 2, 16
Zekun Ren, Juhwan Noh, Siyu Tian, Felipe Oviedo, Guangzong Xing, Qiaohao Liang, Armin
Aberle, Yi Liu, Qianxiao Li, Senthilnath Jayavelu, et al. Inverse design of crystals using gen-
eralized invertible crystallographic representation. arXiv preprint arXiv:2005.07609, 2020. 3, 6,
7, 16
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv
preprint arXiv:2202.00512, 2022. 17
Victor Garcia Satorras, Emiel Hoogeboom, Fabian B Fuchs, Ingmar Posner, and Max Welling. E (n)
equivariant normalizing flows for molecule generation in 3d. arXiv preprint arXiv:2105.09016,
2021. 3
Yoshihide Sawada, Koji Morikawa, and Mikiya Fujii. Study of deep generative models for inorganic
chemical compositions. arXiv preprint arXiv:1910.11499, 2019. 3
Kristof T Schutt, HUziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller.
Schnet-a deep learning architecture for molecules and materials. The Journal of Chemical
Physics, 148(24):241722, 2018. 2, 7
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang.
Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint
arXiv:2001.09382, 2020. 3
Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular con-
formation generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th Inter-
national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol-
ume 139 of Proceedings of Machine Learning Research, pp. 9558-9568. PMLR, 2021. URL
http://proceedings.mlr.press/v139/shi21b.html. 3, 4, 5, 17
Muhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary
Ulissi, andC Lawrence Zitnick. Rotation invariant graph neural networks using spin convolutions.
arXiv preprint arXiv:2106.09575, 2021. 2
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn-
ing, pp. 2256-2265. PMLR, 2015. 4
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the
data distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d'Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Informa-
tion Processing Systems 32: Annual Conference on Neural Information Processing Sys-
tems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11895-
11907, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
3001ef257407d5a371a96dcd947c7d93- Abstract.html. 2, 4, 5, 6
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018. 2
Yanchao Wang, Jian Lv, Li Zhu, and Yanming Ma. Calypso: A method for crystal structure predic-
tion. Computer Physics Communications, 183(10):2063-2070, 2012. 2
Logan Ward, Ankit Agrawal, Alok Choudhary, and Christopher Wolverton. A general-purpose
machine learning framework for predicting properties of inorganic materials. npj Computational
Materials, 2(1):1-7, 2016. 18
13
Published as a conference paper at ICLR 2022
Alexander Frank Wells et al. Three dimensional nets and polyhedra. Wiley, 1977. 3
Jiaxiang Wu, Tao Shen, Haidong Lan, Yatao Bian, and Junzhou Huang. Se (3)-equivariant energy-
based models for end-to-end protein folding. bioRxiv, 2021. 3
Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and
interpretable prediction of material properties. Physical review letters, 120(14):145301, 2018. 1,
2, 3
Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative
dynamics for molecular conformation generation. In International Conference on Learning Rep-
resentations, 2021a. URL https://openreview.net/forum?id=pAbm1qfheGk. 8,
18
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geo-
metric diffusion model for molecular conformation generation. In International Conference on
Learning Representations, 2021b. 3
Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan.
Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 4541-4550, 2019. 9
Wenhui Yang, Edirisuriya M Dilanga Siriwardane, Rongzhi Dong, Yuxin Li, and Jianjun Hu. Crystal
structure prediction of materials with high symmetry using differential evolution. arXiv preprint
arXiv:2104.09764, 2021. 3
Yong Zhao, Mohammed Al-Fahdi, Ming Hu, Edirisuriya Siriwardane, Yuqi Song, Alireza Nasiri,
and Jianjun Hu. High-throughput discovery of novel cubic crystal materials using deep generative
neural networks. arXiv preprint arXiv:2102.01880, 2021. 3
Nils ER Zimmermann and Anubhav Jain. Local structure order parameters and site fingerprints for
quantification of coordination environment and crystal structure similarity. RSC Advances, 10
(10):6063-6081, 2020. 18
14
Published as a conference paper at ICLR 2022
A	Proof for the connection to a harmonic force field
We assume the loss in Equation 2 can be minimized to zero when the noises are small, meaning that
SX(A, X, L|z) = dmm(X, X), Vj > J,	⑶
σX,j
where σX,j ∈ {σX,j}jL=1 and any noise smaller than σX,J is considered as small.
The force term in the Langevin dynamics αjsX,t can then be written as
αj SX (A, X, Llz; σA,j, σX,j ) = e ∙ σX ,j/σX ,L ∙ SX (A, X, L|z)/σX ,j	⑷
σX,j dmin (X, X) ∖
=e ∙ ɪʌ--------2-----, ∀j > J	(5)
σX,L	σX,j
e
=----2--dmin(X, X), ∀j > J	⑹
σX2 ,L
If We write e∕σX L = k,then,
αj SX (A, X, L|z; σA,j , σX,j ) = -kdmin (X, X), ∀j > J	(7)
If the noises are small enough that atoms do not cross the periodic boundaries, then we have
_ ，____~ , _____ ~ .
dmin (X, X) = X 一 X .Therefore,
αjSX(A, X, L∣z; σA,j, σX,j) = —k(X — X), Vj > J.	(8)
B	Implementation details
B.1	Prediction of lattice parameters
There are infinitely many different ways of choosing the lattice for the same material. We compute
the Niggli reduced lattice (Grosse-Kunstleve et al., 2004) with pymatgen (Ong et al., 2013), which is
a unique lattice for any given material. Since the lattice matrix L is not rotation invariant, we instead
predict the 6 lattice parameters, i.e. the lengths of the 3 lattice vectors and the angles between them.
We normalize the lengths of lattice vectors with 3/N, where N is the number of atoms, to ensure
that the lengths for materials of different sizes are at the same scale.
B.2	Multi-graph construction
For the encoder, we use CrystalNN (Pan et al., 2021) to determine edges between atoms and build
a multi-graph representation. For the decoder, since it inputs a noisy structure generated on the fly,
the multi-graph must also be built on the fly for both training and generation, and CrystalNN is too
slow for that purpose. We use a KNN algorithm that considers periodicity to build the decoder graph
where K = 20 in all of our experiments.
B.3	GNN architecture
We use DimeNet++ adapted for periodicity (Klicpera et al., 2020a;b) as the encoder, which is SE(3)
invariant to the input structure. The decoder needs to output an vector per node that is SE(3) equiv-
ariant to the input structure. We use GemNet-dQ (Klicpera et al., 2021) as the decoder. We used
implementations from the Open Catalysis Project (OCP) (Chanussot et al., 2021), but we reduced
the size of hidden dimensions to 128 for faster training. The encoder has 2.2 million parameters and
the decoder has 2.3 million parameters.
C Dataset curation
C.1 Perov- 5
Perovskite is a class of materials that share a similar structure and have the general chemical formula
ABX3. The ideal perovskites have a cubic structure, where the site A atom sits at a corner position,
15
Published as a conference paper at ICLR 2022
the site B atom sits at a body centered position and site X atoms sit at face centered positions.
Perovskite materials are known for their wide applications. We curate the Perov-5 dataset from an
open database that was originally developed for water splitting (Castelli et al., 2012a;b).
All 18928 materials in the original database are included. In the database, A, B can be any non-
radioactive metal and X can be one or several elements from O, N, S, and F. Note that there can
be multiple different X atoms in the same material. All materials in Perov-5 are relaxed using
density functional theory (DFT), and their relaxed structure can deviate significantly from the ideal
structures. A significant portion of the materials are not thermodynamically stable, i.e., they will
decompose to nearby phases and cannot be synthesized.
C.2 CARBON-24
Carbon-24 includes various carbon structures obtained via ab initio random structure searching
(AIRSS) (Pickard & Needs, 2006; 2011) performed at 10 GPa.
The original dataset includes 101529 carbon structures, and we selected the 10% of the carbon
structure with the lowest energy per atom to create Carbon-24. All 10153 structures in Carbon-24
are relaxed using DFT. The most stable structure is diamond at 10 GPa. All remaining structures
are thermodynamically unstable but may be kinetically stable. Most of the structures cannot be
synthesized.
C.3 MP-20
MP-20 includes almost all experimentally stable materials from the Materials Project (Jain et al.,
2013) with unit cells including at most 20 atoms. We only include materials that are originally from
ICSD (Belsky et al., 2002) to ensure the experimental stability, and these materials represent the
majority of experimentally known materials with at most 20 atoms in unit cells.
To ensure stability, we only select materials with energy above the hull smaller than 0.08 eV/atom
and formation energy smaller than 2 eV/atom, following Ren et al. (2020). Differing from Ren et al.
(2020), we do not constrain the number of unique elements per material. All materials in MP-20 are
relaxed using DFT. Most materials are thermodynamcially stable and have been synthesized.
D	Experiment details
D. 1 Reasons for the unsuitability of some metrics for specific datasets
In Table 2, property statistics are computed by comparing the earth mover’s distance between the
property distribution of generated materials and ground truth materials. So, they are not meaningful
for ground truth data.
Materials in Perov-5 have the same structure, so it is not meaningful to require higher structure
diversity.
Materials in Carbon-24 have the same composition (carbon), so it is not meaningful to require higher
composition diversity. In addition, all models have 〜100% composition validity, so it is not Com-
pared in the table.
D.2 Composition validity checker
We modified the charge neutrality checker from SMACT (Davies et al., 2019) because the original
checker is not suitable for alloys. The checker is based on a list of possible charges for each element
and it checks if the material can be charge neutral by enumerating all possible charge combinations.
However, it does not consider that metal alloys can be mixed with almost any combination. As a
result, for materials composed of all metal elements, we always assume the composition is valid in
our validity checker.
For the ground truth materials in MP-20, the original checker gives a composition validity of 〜50%,
which significantly underestimates the validity of MP-20 materials (because most of them are exper-
imentally synthesizable and thus valid). Our checker gives a composition validity of 〜90%, which
is far more reasonable. We note again that these checkers are all empirical and the only high-fidelity
evaluation of material stability requires QM simulations.
16
Published as a conference paper at ICLR 2022
D.3 Non-Gaussian statistical structure of materials
The material datasets are usually biased towards certain material groups. For example, there are lots
of lithium-containing materials in MP-20 because it started with battery research. We also find that
our decoder tends to underfit the data distribution with a larger β in Equation 9. We believe these
observations indicate that the statistical structure of the ground truth materials are far from Gaussian.
As a result, sampling from N(0, 1) may lead to out-of-distribution materials, which explains why
our method tends to generate more elements per material than the ground truth.
D.4 Hyperparameters and training details
The total loss can be written as,
L = LAGG + LDEC + LKL = λcLc + λLLL + λNLN + λXLX + λALA + βLKL.	(9)
We aim to keep each loss term at a similar scale. For all three datasets, we use λc = 1, λL =
10, λN = 1, λX = 10, LA = 1.
We tune β between 0.01, 0.03, 0.1 for all three datasets and select the model with best validation
loss. For Perov-5, MP-20, we use β = 0.01, and for Carbon-24, we use β = 0.03.
For the noise levels in {σA,j}jL=1, {σX,j}jL=1, we follow Shi et al. (2021) and set L = 50. For all
three datasets, we use σA,max = 5, σA,min = 0.01, σX,max = 10, σX,min = 0.01.
During the training, we use an initial learning rate of 0.001 and reduce the learning rate by a factor
of 0.6 if the validation loss does not improve after 30 epochs. The minimum learning rate is 0.0001.
During the generation, we use = 0.0001 and run Langevin dynamics for 100 steps at each noise
level.
E Visualization of multiple reconstructed structures

Perov-5
Carbon-24
MP-20
Ground Truth
Sample 1
Sample 2
Sample 3

Figure 5: Different reconstructed structures from CDVAE from the same z, following 3 Langevin
dynamics sampling with different random seeds.
F Sampling speed for material generation
We summarize the speed for generating 10,000 materials for all models in Table 4. FTCP is sig-
nificantly faster, but the quality of generated materials is very poor as shown in Table 2. Cond-
DFC-VAE is faster than our method in Perov-5, but has a lower quality than our method and only
works for cubic systems. It is also unclear how it will perform on larger materials in Carbon-24
and MP-20, because the compute increases cubicily with the increased size of the density map. G-
SchNet/P-G-SchNet have a comparable sampling time as our method, but have a lower quality. We
also note that we did not optimize sampling speed in current work. It is possible to reduce sampling
time by using fewer sampling steps without significantly influencing generation quality. There are
also many recent works that aim to speed up the sampling process for diffusion models (Nichol &
Dhariwal, 2021; Kong & Ping, 2021; Salimans &Ho, 2022).
17
Published as a conference paper at ICLR 2022
Table 4: Time used for generating 10,000 materials on a single RTX 2080 Ti GPU.
	FTCP	Cond-DFC-VAE	G-SchNet	P-G-SchNet	CDVAE
Perov-5	< 1 min	0:5h	2.0h	2.0h	3.1 h
Carbon-24	< 1 min	—	6.2 h	6.3 h	5.3 h
MP-20	< 1 min	—	6.3 h	6.3 h	5.8 h
G Coverage metrics for material generation
Inspired by Xu et al. (2021a); Ganea et al. (2021), we define six metrics to compare two ensembles
of materials: materials generated by a method {Mk}k∈[1..K], and ground truth materials in test data
{Mι}∈[i..L].	”
We use the Euclidean distance of the CrystalNN fingerprint (Zimmermann & Jain, 2020) and
normalized Magpie fingerprint (Ward et al., 2016) to define the structure distance and composi-
tion distance between generated and ground truth materials, respectively. They can be written as
DStrUC.(Mk, M；) and DComP.(Mk, Mf). We further define the thresholds for the structure and
composition distance as δstruc. and δcomp. , respectively.
Following the established classification metrics of Precision and Recall, we define the coverage
metrics as:
COV-R(ReCall) = 1 |{l ∈ [1..L] : ∃k ∈ [1..K],Dstruc. (Mk, M*) < δstruc.,
DComP.(Mk, Ml*) < δcomp. }|
(10)
AMSD-R (Recall)
AMCD-R (Recall)
1 x
l∈[1..L]
LX
l∈[1..L]
k∈m[1i.n.K] DStruC.(Mk,Ml*)
min DComP.(Mk, M*),
k∈[1..K]
(11)
(12)
where COV is ”Coverage”, AMSD is ”Average Minimum Structure Distance”, AMCD is ”Average
Minimum Composition Distance”, and COV-P (precision), AMSD-P (precision), AMCD-P (pre-
cision) are defined as in above equations, but with the generated and ground truth material sets
swapped. The recall metrics measure how many ground truth materials are correctly predicted,
while the precision metrics measure how many generated materials are of high quality (more dis-
cussions can be found in Ganea et al. (2021)).
We note several points on why we define the metrics in their current forms. 1) COV requires both
structure and composition distances to be within the thresholds, because generating materials that
are structurally close to one ground truth material and compositionally close to another is not mean-
ingful. As a result, AMSD and AMCD are less useful than COV. 2) We use fingerprint distance,
rather than RMSE from StructureMatcher (Ong et al., 2013), because the material space is
too large for the models to generate enough materials to exactly match the ground truth materials.
StructureMatcher first requires the compositions of two materials to exactly match, which will
cause all models to have close-to-zero coverage.
For Perov-5 and Carbon-24, we choose δstruC. = 0.2, δComP. = 4. For MP-20, we choose δstruC. =
0.4, δComP. = 10. In Figure 6, Figure 7, Figure 8, we show how both COV-R and COV-P change by
varying δstruC. and δComP. in all three datasets.
18
Published as a conference paper at ICLR 2022
Table 5: Full coverage metrics for the generation task.
Method	Data	COV-R ↑	AMSD-R，	AMCD-R，
FTCP	Perov-5	"-0.00	0.7447	7.212
	Carbon-24	0.00	1.181	0.00
	MP-20	4.72	0.6542	9.271
Cond-DFC-VAE	Perov-5	73.92	0.1508	2.773
G-SchNet	Perov-5	0.18	0.5962	1.006
	Carbon-24	0.00	0.5887	0.00
	MP-20	38.33	0.5365	3.233
P-G-SchNet	Perov-5	0.37	0.5510	1.0264
	Carbon-24	0.00	0.6308	0.00
	MP-20	41.93	0.5327	3.274
CDVAE	Perov-5	99.45	0.0482	0.6969
	Carbon-24	99.80	0.0489	0.00
	MP-20	99.15	0.1549	3.621
COV-P ↑ AMSD-P， AMCD-P，
0.00	0.3582	3.390
0.00	0.8822	24.16
0.09	0.1954	4.378
10.13	0.3162	4.257
0.23	0.4259	1.3163
0.00	0.5970	0.00
99.57	0.2026	3.601
0.25	0.3967	1.316
0.00	0.8166	0.00
99.74	0.1985	3.567
98.46	0.0593	1.272
83.08	0.1343	0.00
99.49	0.1883	4.014
o.o-
。：2	。：4	。：6	。：8 LO
Struc. precision thresholds
5	10	15	20
Comp, precision thresholds
——FTCP ——Cond-DFC-VAE ——G-SchNet G-SchNet (periodic) CDVAE
Figure 6: Change of COV-R and COV-P by varying δstruc. and δcomp. for Perov-5. Dashed line
denotes the current chosen thresholds.
19
Published as a conference paper at ICLR 2022
0 8 6 4 2 0
■ ■■■■■
Iooooo
H,Λ0
0.2	0.4	0.6	0.8	1.0
Struc. recall thresholds
5	10	15	20
Comp, recall thresholds
0 8 6 4 2 0
■ ■■■■■
Iooooo
d,ΛO
5	10	15	20
Comp, precision thresholds
0.2	0.4	0.6	0.8	1.0
Struc. precision thresholds
——FTCP ——G-SchNet - G-SchNet (periodic) CDVAE
Figure 7:	Change of COV-R and COV-P by varying δstruc. and δcomp. for Carbon-24. Dashed line
denotes the current chosen thresholds.
5	10	15	20
Comp, recall thresholds
0.2	0.4	0.6	0.8	1.0
Struc. precision thresholds
5	10	15	20
Comp, precision thresholds
——FTCP ——G-SchNet - G-SchNet (periodic) CDVAE
Figure 8:	Change of COV-R and COV-P by varying δstruc. and δcomp.
denotes the current chosen thresholds.
for MP-20. Dashed line
20