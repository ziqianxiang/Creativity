Published as a conference paper at ICLR 2022
Lossy Compression with Distribution Shift as
Entropy Constrained Optimal Transport
Huan Liu1*, George Zhang2* ,Jun Chen1, Ashish Khisti2
1 McMaster University, 2University of Toronto
{liuh127, chenjun}@mcmaster.ca
gq.zhang@mail.utoronto.ca, akhisti@ece.utoronto.ca
Ab stract
We study an extension of lossy compression where the reconstruction distribution
is different from the source distribution in order to account for distributional shift
due to processing. We formulate this as a generalization of optimal transport with
an entropy bottleneck to account for the rate constraint due to compression. We
provide expressions for the tradeoff between compression rate and the achievable
distortion with and without shared common randomness between the encoder and
decoder. We study the examples of binary, uniform and Gaussian sources (in an
asymptotic setting) in detail and demonstrate that shared randomness can strictly
improve the tradeoff. For the case without common randomness and squared-
Euclidean distortion, we show that the optimal solution partially decouples into the
problem of optimal compression and transport and also characterize the penalty
associated with fully decoupling them. We provide experimental results by training
deep learning end-to-end compression systems for performing denoising on SVHN
and super-resolution on MNIST suggesting consistency with our theoretical results.
1 Introduction
Using deep neural networks for lossy image compression has proven to be effective, with rate-
distortion performance capable of dominating general-purpose image codecs like JPEG, WebP or
BPG (Rippel & Bourdev, 2017; Agustsson et al., 2017; Mentzer et al., 2018). More recently, many
of these works have included generative aspects within the compression to synthesize realistic
elements when the rate is otherwise too low to represent fine-grained details (Tschannen et al.,
2018; Agustsson et al., 2019; Mentzer et al., 2020). Though this has been found to deteriorate rate-
distortion performance, it has generally resulted in more perceptually-pleasing image reconstruction
by reducing artifacts such as pixelation and blur. Using a distributional constraint as a proxy for
perceptual measure, several works have subsequently formalized this in a mathematical framework
known as the rate-distortion-perception tradeoff (Blau & Michaeli, 2018; 2019; Matsumoto, 2018;
2019; Theis & Wagner, 2021; Yan et al., 2021; Zhang et al., 2021). As is conventional in lossy
compression, these works address the scenario in which both low distortion, whereby each individual
image reconstruction resembles the ground truth image, and closeness in distribution in which it is not
easy to discriminate between image samples from the data-generating distribution and reconstruction
distribution, are desirable.
The underlying ideal in conventional compression systems is to have perfect reconstruction with
respect to some ground truth input. However this is not the case in applications such as denoising,
deblurring, or super-resolution (SR), which require restoration from a degraded input image. In fact, in
these cases a ground truth may not even be available. In such applications naturally the reconstruction
distribution must match the original source rather than the degraded input distribution. A large body
of literature has been devoted to various image restoration tasks, including several methods based on
deep learning including both supervised (e.g., (Blau & Michaeli, 2018)) and unsupervised learning
methods (e.g., (Wang et al., 2021b)). Although most of the literature exclusively treats compression
* Equal Contribution
1
Published as a conference paper at ICLR 2022
and restoration separately, in many application they can co-occur. For example, the encoder which
records a degraded image may not be co-located with the decoder, but must transmit a compressed
version of the image over a digital network. In turn, the decoder must perform both decompression
and restoration simultaneously.
To that end, we study an extension of lossy compression in which the reconstruction distribution
is different than the source distribution to account for distributional shift due to processing. The
problem can be described as a transformation from some source domain to a new target domain under
a rate constraint, which generalizes optimal transport. This readily extends other works which view
image restoration under the perception-distortion tradeoff (Blau & Michaeli, 2018) or under optimal
transport (Wang et al., 2021b). It also provides a generalization of the rate-distortion-perception
problem (Blau & Michaeli, 2019) where the reconstruction distribution must be close to the input
distribution. Following (Theis & Agustsson, 2021; Theis & Wagner, 2021), we also utilize common
randomness as a tool for compression in our setting. Our results are summarized as follows:
•	We provide a formulation of lossy compression with distribution shift as a generalization of
optimal transport with an entropy constraint and identify the tradeoff between the compression rate
and minimum achievable distortion both with and without common randomness at the encoder
and decoder. We identify conditions under which the structure of the optimal solution partially
decouples the problems of compression and transport, and discuss their architectural implications.
We study the examples of binary, uniform and Gaussian sources (in asymptotic regime) in detail
and demonstrate the utility of our theoretical bounds.
•	We train deep learning end-to-end compression systems for performing super-resolution on MNIST
and denoising on SVHN. Our setup is unsupervised and to the best of our knowledge the first to
integrate both compression and restoration at once using deep learning. We first demonstrate that by
having common randomness at the encoder and decoder the achievable distortion-rate tradeoffs are
lower than when such randomness is not present. Furthermore, we provide experimental validation
of the architectural principle suggested by our theoretical analysis.
2 Theoretical Formulation
We consider a setting where an input X 〜PX is observed at the
encoder, which is a degraded (e.g., noisy, lower resolution, etc)
version of the original source. It must be restored to an output
Y 〜PY at the decoder, where PY denotes the target distribution
of interest. For example, if X denotes a noise-corrupted image
and Y denotes the associated clear reconstruction, then PY can be
selected to match the distribution of the original source. We will
assume PX and PY are probability distributions over X , Y ⊆ Rn
and require X and Y to be close with respect to some fidelity
metric, which will be measured using a non-negative cost function
d(x, y) over XXY. We will refer to d(∙, ∙) as the distortion
measure and assume that it satisfies d(x, y) = 0 if and only if
x = y. We further assume that X cannot be directly revealed to
the decoder, but instead must be transmitted over a bit interface
with an average rate constraint. Such a scenario occurs naturally
in many practical systems when the encoder and decoder are not
co-located such as communication systems or storage systems. As
one potential application, when aerial photographs are produced
for remote sensing purposes, blurs are introduced by atmospheric
Figure 1: Illustration of Theo-
rem 1 (no common randomness).
Given source distribution PX , tar-
get reconstruction distribution PY
and rate R, we can find quantiza-
tions X of X and Y of Y and
consider transport between them.
turbulence, aberrations in the optical system and relative motion between camera and ground. In
such scenarios unsupervised restoration is preferred as it is often intractable to accurately model
such degradation processes and collection of paired training data can be time consuming or require
significant human intervention. Unsupervised image restoration has been studied recently in Zhang
et al. (2017); Pan et al. (2021); Wang et al. (2021b); Menon et al. (2020). These works also fix the
reconstruction distribution Y 〜PY and propose to minimize a distortion metric between the output
and the degraded input as in our present work, but do not consider compression.
2
Published as a conference paper at ICLR 2022
2.1 Optimal Transport and Extensions
Definition 1 (Optimal Transport). Let Γ(pX,pY ) be the set of all joint distributions pX,Y with
marginals pX and pY . The classical optimal transport problem is defined as
D(pX,pY) =	inf	E[d(X, Y)],	(1)
pX,Y ∈Γ(pX,pY )
where we refer to each pX,Y ∈ Γ(pX,pY ) as a transport plan.
Operationally the optimal transport plan in (1) minimizes the average distortion between the input
and output while keeping the output distribution fixed to pY . This may generate a transport plan
with potentially unbounded entropy, which may not be amenable in a rate-constrained setting. We
therefore suggest a generalization to Definition 1 which constrains the entropy of the transport plan.
It turns out that having common randomness at the encoder and decoder can help in this setting, so
we will distinguish between when it is available and unavailable.
Definition 2 (Optimal Transport with Entropy Bottleneck — no Common Randomness). Let
Mncr (pX, pY ) denote the set of joint distributions pX,Z,Y compatible with the given marginal
distributions pX, pY satisfying pX,Z,Y = pXpZ |XpY |Z. The optimal transport from pX to pY with
an entropy bottleneck of R and without common randomness is defined as
Dncr(pX,pY,R) ,	inf	E[d(X, Y)]
pX,Z,Y ∈Mncr (pX,pY )
s.t. H(Z) ≤ R,
where H(∙) denotes the Shannon entropy of a random variable.
(2)
We note that when the rate constraint R is sufficiently large such that one can select Z = X or Z = Y
in (2), then Dncr (pX, pY , R) = D(pX, pY ) in (1). More generally, D(pX, pY ) serves as a lower
bound for Dncr (pX, pY , R) for any R > 0. Definition 2 also has a natural operational interpretation
in our setting. We can view the encoder as implementing the conditional distribution pZ |X to output
a representation Z given the input X , and the decoder as implementing the conditional distribution
pY |Z to output the reconstruction Y given the representation Z. The entropy constraint H(Z) ≤ R
essentially guarantees that the representation Z can be losslessly transmitted at a rate close to R1.
Of particular interest to us is the squared Euclidean distance d(X, Y ) = ||X - Y ||2. As it turns out,
when we specialize to this distance function we can without loss of optimality also impose a more
structured architecture for implementing the encoder and the decoder. Let W] (∙, ∙) be the squared
quadratic Wasserstein distance, by using the squared loss in Definition 1.
Theorem 1. Let
Dmse(PX,pγ,R) ,	inf E[kX - Xk2]+ E[kY - Yk2] + W∣(pχ,Pγ)
PX |X ,pY | Y
ʌ ʌ ʌ ʌ ʌ ʌ
s.t. E[X|X] = X,	E[Y|Y] = Y,	H(X) ≤ R,	H(Y) ≤ R,
and
Dmse(PX ,R) , inf E[kX - X k2]
pX |X
s.t. H(X) ≤ R.
Moreover, let
Dncr (PX ,PY ,R)，Dmse(PX , R) + Dmse (PY, R) + W2 (PX* ,Pγ* ),
Dncr(PX ,PY, R)，Dmse(PX ,R) + Dmse(PY, R),
(3)
(4)
(5)
(6)
1The source coding theorem guarantees that any discrete random variable Z can be losslessly compressed
using a variable length code with average length of no more than H(Z) + 1 bits. We also note some differences
between this formulation and the distortion-rate function from information theory - here the target distribution
pY is fixed, and we impose a constraint on entropy rather than mutual information to reflect that we perform
compression on a per-sample basis (i.e. the one-shot scenario) rather than on asymptotically long blocks.
3
Published as a conference paper at ICLR 2022
where PX * and PY * are the marginal distributions induced by the minimizers PX *∣χ and PY *∣γ that
attain Dmse (pX, R) and Dmse (pY, R), respectively (assuming the existence of such minimizers).
Then under the squared Eucledian distortion measure,
Dncr (PX , PY , R) = Dmse (PX , PY , R).	(7)
In addition, we have
Dncr(PX ,PY, R) ≥ Dncr(PX ,P Y, R) ≥ Dncr(PX ,P Y, R),	(8)
and both inequalities are tight when PX = PY.
Theorem 1 deconstructs Z into
ʌ ʌ
the quantizations X of X and Y of Y
, and decomposes the overall
distortion in (2) in terms of the losses due to quantization, transport, and dequantization in (3). It
also suggests a natural architecture that partially decouples compression and transport without loss of
optimality. First, the sender uses the distribution PX∣x to produce the compressed representation X
from X. This is then passed through a “converter” PY∣X^ to transform X to an optimal representation
Y of Y. Finally, the receiver maps Y back to Y using the conditional distribution Py∣ y . This is
illustrated in Figure 1. The entropy constraint H(X) ≤ R in (2) essentially guarantees that X can
be losslessly transmitted to the decoder where the converter can be applied to map X to Y before
outputting Y. Alternately the constraint H(Y) ≤ R guarantees that the converter could also be
implemented at the encoder and then Y can be compressed and transmitted to the decoder. Finally
note that our proposed architecture is symmetric2 with respect to the encoder and the decoder and
in particular the procedure to transport Y to X would simply be the inverse of transporting X to Y,
and indeed the distortion incurred by dequantizing PyY is the same as the distortion incurred by
quantizing PY 丫.
For the special case of same source and target distribution, we have Dmse(PX , PX , R) =
2Dmse(PX, R), implying that the rate required to achieve distortion D under no output distribution
constraint (and with the output alphabet relaxed to Rn) achieves distortion 2D under the constraint
that Y equals X in distribution. This recovers the result of Theorem 2 in Yan et al. (2021) for
the one-shot setting. More generally, (8) shows that we may lower bound Dmse (PX, PY, R) by the
distortion incurred when compressing X and Y individually, each at rate R, through ignoring the cost
of transport. On the other hand, the upper bound corresponds to choosing the optimal rate-distortion
representations X*, Y* for X, Y, then considering transport between them. The advantage of this
approach is that knowledge of the other respective distribution is not necessary for design. Although
not optimal in general, we will, in fact, provide an example where this is optimal in Section 2.2.
Finally, the following result implies that under mild regularity conditions, the optimal converter PY∣j^
can be realized as a (deterministic) bijection, and in the scalar case it can basically only take the form
as illustrated in Figure 1.
Theorem 2.	Assume that Dncr(PX,PY, R) is a strictly decreasing function in a neighborhood of
R = R* and Dncr(PX ,py ,R*) is attained by px,z,y .Let X，E[X |Z ] and Y，E[Y |Z ]. Then
ʌ ʌ
H (X) = H (Y) = R*,	(9)
E[∣∣X - Yk2] = w2(px,PY),	(10)
and there is a bijection between X and Y.
We remark that in general computing the optimal transport map is not straightforward. For the case
of binary sources we can compute an exact characterization for Dncr as discussed in Section 2.2.
Furthermore as discussed in Appendix A.6, WI(PX ,PY>) can be computed in closed form when X
and Y are scalar valued, which can be used to obtain upper bounds on Dncr. In our experimental
results in Section 3 we use deep learning based methods to learn approximately optimal mappings.
2We say that the problem is symmetric if it is invariant under reversing pX , pY with a new distortion measure
defined by reversing the arguments of d(∙, ∙).
4
Published as a conference paper at ICLR 2022
No common randomness	With common randomness
X	PZX	Z	PY |Z	Y	X	Pz∣x,u	Z	pγ∣z,u	Y
T U Γ
X Px X	X	PY X	Y	Pγ∣Y	Y	X	Py∣x,u	Y
T
U
Figure 2: Architectures. ToP left: Definition 2. Bottom left: Theorem 1. ToP right: Definition 3.
ʌ ʌ
Bottom right: Theorem 3. EntroPy coding of intermediate rePresentations Z, X , Y is not shown. For
Theorem 3, the division between sender and receiver is across an encoder C = f (X, U) and decoder
Y = g(C, U) Performing entroPy coding along pY |X,U.
So far we have focused on the setting when there is no shared common randomness between the
encoder and the decoder. We will now consider the setting when a shared random variable denoted
by U is Present at the encoder and decoder. We assume that the variable U is indePendent of the
inPut X so that the decoder has no aPriori information of the inPut. In Practice the sender and
receiver can agree on a Pseudo-random number generator ahead of time and some kind of seed could
be transmitted, after which both sides can generate the same U . We further discuss how shared
randomness is used in Practice in the exPerimental section.
Definition 3 (OPtimal TransPort with EntroPy Bottleneck — with Common Randomness). Let
Mcr(pX,pY ) denote the set of joint distributions pU,X,Z,Y comPatible with the given marginal
distributions pX, pY and satisfying pU,X,Z,Y = pU pX pZ|X,U pY |Z,U, where pU rePresents the
distribution of shared randomness. The oPtimal transPort from pX to pY with entroPy bottleneck R
and common randomness is defined as
Dcr(pX,pY,R) ,	inf	E[d(X, Y)]
pU,X,Z,Y ∈Mcr (pX,pY )
s.t. H(Z|U) ≤ R.
(11)
Note that we oPtimize over pU (the distribution associated with shared randomness), in addition
to pZ|X,U and pY |Z,U in (11). Furthermore, Dcr(pX,pY,R) ≤ Dncr(pX,pY,R) in general, as we
do not have access to shared randomness in Definition 2. Also from the same argument that was
made following Definition 2, we have that Dcr(pX,pY , R) ≥ D(pX,pY ) in Definition 1. As with
Definition 2, we can also Provide a natural oPerational interPretation. In Particular, given the inPut
X and common randomness U the encoder can outPut a comPressed rePresentation Z using the
conditional distribution pZ |X,U. The rePresentation Z can be losslessly comPressed aPProximately
to an average rate of R again by exPloiting the shared randomness U. Finally the decoder, given
Z and U can outPut the reconstruction Y using the conditional distribution pY |Z,U. An interesting
difference with Definition 2 is that the setuP is no longer symmetric between encoder and decoder, as
X is indePendent of U but Y is not. The following result Provides a simPlification to the architecture
in Definition 3.
Theorem 3.	Let Qcr(pX,pY ) denote the set of joint distributions pU,X,Y compatible with the given
marginals pX, pY satisfying pU,X,Y = pUpXpY |U,X as well as H(Y |U, X) = 0. Then
Dcr(pX,pY,R) = inf	E[d(X, Y)]
pU,X,Y ∈Qcr (pX,pY )
s.t. H(Y |U) ≤ R.
(12)
Before discussing the imPlications of Theorem 3 we remark on a technical Point. Because the
Shannon entroPy is defined only for discrete random variables, U must be chosen in a way such that
Y |U = u is discrete for each u, even for continuous (X, Y ). This is known to be Possible, e.g., Li
& El Gamal (2018) have Provided a general construction for a U with this ProPerty, with additional
quantitative guarantees to ensure that U is informative of Y . In the finite alPhabet case we show in
APPendix A.3 that oPtimization of U can be formulated as a linear Program.
We next discuss the imPlication of Theorem 3. First note that the Problem can be modelled with only
pY |U,X Producing a reconstruction Y without the need for the intermediate rePresentation Z, much
like the conventional oPtimal transPort in Definition 1. The condition H(Y |U, X) = 0 also imPlies
that the transPort Plan is deterministic when conditioned on the shared randomness, which Plays the
role of stochasticity. Furthermore in this architecture the encoder should comPute the rePresentation
5
Published as a conference paper at ICLR 2022
(a)
(b)	(c)
Figure 3: Binary case distortion-rate tradeoffs. (a) qx = qγ = 0.3, where DnCr(B(qX), B(qY), R)
and Dncr(B(qx), B(qγ), R) coincide with DnCr(B(qx), B(qγ), R); (b) qx = 0.3, qγ = 0.5, where
DnCr(B (qx), B(qγ ),r) is tight but D 0仃(B(qx), B(qγ ),R) is loose; (c) qχ = 0.3, qγ = 0.6, where
both bounds are loose. Moreover, it can be seen from all these examples that common randomness
can indeed help improve the distortion-rate tradeoff.
Y given the source X and the shared random-variable U (which corresponds to the transport problem)
and then compress it losslessly at a rate close to H(Y |U) (which corresponds to the compression
problem). The receiver only needs to decompress and reconstruct Y . This is in contrast to the case
without common randomness in Theorem 1 where the Y must be generated at the decoder.
2.2 Numerical Examples
We present how the results in Theorem 1 & 3 can be evaluated for some specific source models. We
first consider the example of Binary sources. Let X 〜B(qx) and Y 〜B(qγ) be two Bernoulli
random variables with qx,qγ ∈ (0,1), and let d(∙, ∙) be the Hamming distortion measure dH(∙, ∙)
(i.e., dH (x, y) = 0 if x = y and dH (x, y) = 1 otherwise), which coincides with the squared error
distortion in Theorem 1 for binary variables. The explicit expressions of DCr(B(qx), B(qγ), R),
Dncr(B(qx), B(qγ),R) as well as DnCr(B(qx), B(qγ),R) and DnCr(B(qx), B(qγ),R) are pro-
vided by Theorem 4 in Appendix A.4, from which the following observations can be made. In
general, we have DnCr(B(qx), B(qγ), R) > DCr(B(qx), B(qγ), R), i.e., common randomness
strictly improves the distortion-rate tradeoff (except at some extreme point(s)). Moreover, as long as
B(qx) and B(qγ) are biased toward the same symbol (namely, qx, qγ ≤ 1/2 or qx, qγ ≥ 1/2), the
upper bound DnCr(B(qx), B(qγ), R) is tight, which implies that blindly using optimal quantizer and
dequantizer in the conventional rate-distortion sense incurs no penalty. Some illustrative examples
are shown in Figure 3.
In Appendix A.6 we consider the case when X and Y are continuous valued sources from a uniform
distribution and establish an upper bound on DnCr(∙) that is shown to be tight as the rate R → ∞.
For Gaussian distributions in the asymptotic optimal transport setting (see Appendix A.7 for relevant
definitions and results) we present results qualitatively similar to the binary case in Appendix A.8.
3	Experimental Results
We use two sets of results to illustrate that the principles derived from our theoretical results are
applicable to practical compression with deep learning. Importantly, we assume an unsupervised
setting in which we have only unpaired noisy and clean images available to us, as in Wang et al.
(2021b). Our first experiment is, to the best of our knowledge, the first in which restoration and
compression are performed jointly using deep learning. We will furthermore demonstrate the utility
of common randomness in this setting. The second set of experiments are designed on the principle
of Theorem 1. In addition to the generator trained from our first experiment, we will construct a
helper network to allow us to estimate the decomposition (3). This is then compared with the direct
loss between the noisy image and rate-constrained denoising reconstruction. If the losses are close,
this would suggest that the decomposition is not only without loss of optimality but also effective.
3.1	Rate-Distortion Comparison with Common Randomness
Let px be a degraded source distribution that we wish to restore and pγ be the target distribution. Our
goal is to compress X so that the reconstruction semantically resembles X within target distribution
pγ . For our application, we will use MSE loss as a fidelity criterion. Let f be an encoder, Q a
quantizer, and g a decoder. For a given rate R with common randomness U available, we have a
6
Published as a conference paper at ICLR 2022
Figure 4: Illustration of our experimental setup. (a) shows the end-to-end learning system with
common randomness, where the encoder and decoder have access to the same randomness u. (b)
presents the network setup for verifying the architecture principle given in Theorem 1.
problem of the form
min
f,g,Q
s.t.
kX-g(Q(f(X,U)))k22
pg(Q(f (X,U))) = pY ,	H (Q(f (X, U))|U) ≤ R,
(13)
which uses parameterized neural networks to implement (11). We also fix Q such that a hard
constraint on the rate is satisfied and assume f and g are sufficiently expressive to map to these fixed
∆
quantization points. Let Y = g(Q(f (X, U))). We will use a penalty on the Wasserstein-1 distance
between PY and PY in accordance with the Wasserstein GAN (Arjovsky et al., 2017) framework, so
that our system is a stochastic rate-constrained autoencoder with GAN regularization. Specifically,
we follow the network shown in Figure 4(a) which in addition to f, Q, and g contains critic h.
For the realization of common randomness in Definition 3, we adopt the universal quantization
scheme of Ziv (1985); Theis & Agustsson (2021). Given trained f and g and degraded image X, we
generate restored image Y through
Y = g(Q(f(X) + U) - U),	(14)
where U is the stochastic noise shared by the sender and receiver. Details about the quantization are
provided in Appendix B.2. To find an appropriate f and g, we use the relaxed objective
Li= E[kX — Y k2] + λWι(pγ ,pγ),	(15)
which is the sum of the MSE and Wasserstein-1 losses weighted by λ. By optimizing our network
using this objective, we see two favorable properties. First, the Wasserstein-1 loss ensures the
distribution of output is close to that of target images, i.e. pγ ≈ PY for sufficiently large λ. Moreover,
the MSE loss that pushes the output Y to input X ensures that the output structurally resembles X.
Consequently, the training objective allows the output Y to be clear and preserves content from input.
To generate a rate-distortion trade-off curve, we modify the encoder to produce a different number of
symbols ranging from low bit rate to high bit rate and record the MSE distortion loss between noisy
inputs and denoised outputs. Figure 5(a) and Figure 5(c) show the curves for image super-resolution
and image denoising. We also show some qualitative results in Figure 5(b) and 5(d). As the rate
increases, the generated high-quality images are clearer.
As exemplified by the numerical results in Section 2.2, common randomness can help reduce the rate
that is needed for reconstruction given a specific distortion. Equivalently, given a fixed rate, a system
with common randomness can perform better than one without common randomness. To demonstrate
this in practice, we conduct the following experiment. We remove the common randomness setup
from the framework in Section 3.1 and alternatively add two independent noises U1 and U2 to the
encoder and decoder sides. Concretely, under the new setting, (14) becomes
Y = g(Q(f(X) + Ui) — U2)	(16)
Then we conduct training using the objective (15) as in the common randomness. The tradeoff curve
without common randomness for both tasks are shown in Figure 5(a) and 5(c) with orange dots.
Performance of the framework is better when there is common randomness.
7
Published as a conference paper at ICLR 2022
(a)
，XyrkA
8 7-31。
VlSZ 3<2
OGOr-售
q G I 6 1
Low-Res
GT
q
6
R = 4	R = 8	R= 12
5 g “ 7J⅛
Oo，U/ O
8 〃7户
7b I O
S / Q 4
GOp-OO
£ g 〃 7 2
r： 178 16
318 3 2 4
■HBIHB
9R2W1
W3
ZH 178 临
BHBI
四 161IaoR5
aβ2lB21
'、7
(c)
Noisy
Clear
2 4
R = 16	R =20	R = 24
(b)
R = 60	R = 78	R = 96
(d)
Figure 5: (a)(b) The experimental results of 4 times image super-resolution. (c)(d) The experimental
results of image denoising. The noise pattern is synthesized by additive Gaussian noise with standard
deviation set to 20. (a)(c) Rate-distortion trade-offs. Blue points are the MSE distortion loss for a
particular rate under the setting of using common randomness, while orange points illustrate the
same trade-off without using common randomness. For both tasks, at any rate, the performance of
using common randomness is better than the case without common randomness. (b)(d) Examples for
outputs from several models with different rates. As the rate increases, the outputs become clearer.
3.2	Architectural Principle
In the case without common randomness, Theorem 1 implies that (under the rate constraint) the
overall distortion E[kX - Y k2] can be decomposed to the summation of the three distortion terms
E[kX - Yk2 ] = E[kX - X k2 ]+ E[kY - Y k2 ]+ W2 (PX ,pγ),	(17)
where X and Y are some representations of X and Y under MSE distortion. The chosen rate-
distortion representations X for X and Y for Y must not only be representative of X and Y, but
also enjoy low cost of transport between one another. We now seek to estimate the overhead of this
decomposition in practice.
However, due to the nature of the deep learning framework, the distortion measure between images
and compressed representations cannot be explicitly measured. Thus, we alternatively develop
a two-branch network to compare the summation of the three distortion components (17) to the
overall distortion. First, we take trained f and g from the previous experiment and freeze their
weights. Given noisy input X, we encode it through f , then decoder g1 is trained to minimize the
distortion with X, and decoder g2 is trained to minimize the distortion with Y = f (g(X)) which
distributionally approximates a clean restoration (we use Y instead of ground truth because we
assume an unsupervised setting). Let
Yl= gι (f (X)), Y = g2(f(X)).
The idea here is that YL is a rate-constrained reconstruction of X and YY2 is a rate-constrained
reconstruction of Y, both of which are produced from compressing X using f. We assume this
is reasonable as in light of Theorem 2, there is no loss of optimality in doing so given sufficiently
expressive neural networks. The overall decomposed loss is then given by
L2 = E[kX - Yi k2 ]+ E[kY — Y k2 ]+ E[kYι - Y k2 ],	(18)
|-----{z-----} |-------{z----} |------{z-----}
(a)	(b)	(c)
8
Published as a conference paper at ICLR 2022
Super-resolution
Denoising
Table 1: Results of architecture principle in Theorem 1. The end-to-end loss and decomposed loss
are very Close for different rates._____________________________________________________
Rate	4	10	20	30	12	30	60	90
End-to-end Loss	0.0558	0.0435	0.0351	0.0308	0.0230	0.0175	0.0140	0.0123
Decomposed Loss	0.0586	0.0453	0.0349	0.0309	0.0243	0.0192	0.0158	0.0139
in which Y^1 approximates X and Y⅛ approximates Y. Training is Performedjointly over g1 and g2.
One additional point is that g1 and g2 can also be trained separately, although in this case we can
no longer assume that f can be reused without loss of optimality, as this objective would not be
equivalent to minimizing (18) (there is no control over (c)). We develop an additional experiment in
which we optimize encoder-decoder pairs f1 , g1 to minimize (a) and f2 , g2 to minimize (b), where
now Yl is produced using fι, gι and % using f2,g2. In this setting, we aim to approximate the
rate-distortion optimal X * and Y * corresponding to Dncr(PX ,pγ, ∙) from Theorem 1 using YI and
Y2 , and in doing this separate optimization it is clear that we will drive down (a) and (b) but increase
(c). As it turns out, the resultant values (a) and (b) obtained during joint optimization are not much
worse than the values from separate optimization. This provides evidence that in practice, the optimal
rate-distortion representations (i.e. under objective (4)) can be leveraged for the general objective
(18) without much loss of optimality, which further suggests that the encoder f1 can be potentially
trained without knowledge of pY without much performance loss. These can be viewed in Table 2. 4 5
Table 2: Comparison of separate vs. joint training for (18). See the above paragraph for explanation.
Super-resolution								
Rate	4		10		20		30	
Method	Joint	Separate	Joint	Separate	Joint	Separate	Joint	Separate
E[kX - Yik2]	0.0355	0.0356	0.0223	~0.0214	0.0136	0.0113	0.0092	0.0083
EfY - Kk2]	0.0227	0.0216	0.0222	0.0206	0.0191	0.0191	0.0172	0.0155
Denoising								
Rate	12		30		60		90	
Method	Joint	Separate	Joint	Separate	Joint	Separate	Joint	Separate
E[kX - Yik2]	0.0191	0.0190	0.0146	~0.0145	0.0117	0.0123	0.0104	0.0107
ElY - Y2k2]	0.0050	0.0046	0.0044	0.0040	0.0038	0.0035	0.0032	0.0030
4 Related Works
Sinkhorn distances (Cuturi, 2013) are a formulation of optimal transport with a penalty term cor-
responding to the mutual information between the source and target distributions. This has been
studied in information theory literature (e.g. (Bai et al., 2020; Wang et al., 2021a)). For source
coding in particular, Saldi et al. (2015a;b) consider common randomness with constrained output
distribution. Blau & Michaeli (2018) evaluated a number of deep image restoration techniques and
somewhat counter-intuitively demonstrated a tradeoff between optimizing for distortion and “percep-
tual quality”, i.e. realism. Wang et al. (2021b) model the shift in distribution due to degradation as an
optimal transport problem. However this work does not consider compression and their results are
qualitatively different from ours. Meanwhile, output-constrained lossy compression has also been
shown to improve perceptual quality (Tschannen et al., 2018), leading to the rate-distortion-perception
framework (Blau & Michaeli, 2019). An analysis of the one-shot distortion-rate function was studied
recently by (Elkayam & Feder, 2020). Our problem formulation is different as we assume the output
distribution is fixed.
5 Conclusion and Future Work
We consider the setting of lossy compression in which we compress across different source and target
distributions. We formulate this as an entropy-constrained optimal transport problem and provide
expressions for characterizing the tradeoff between compression rate and the minimum achievable
distortion with and without shared common randomness. We also develop a number of architectural
principles through our theoretical results and provide experimental validations by training deep
learning models for super-resolution and denoising tasks over compressed representations. On the
theory side it will be interesting to consider the case where there are either rate constraints on
the amount of shared common randomness between the encoder and decoder or consider the case
when the shared randomness is correlated with the source input, which can arise in many practical
applications. On the practical side it will be interesting to experimentally study the a broader set of
tasks under distribution shift where our theory could be applicable.
9
Published as a conference paper at ICLR 2022
References
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca
Benini, and Luc Van Gool. Soft-to-hard vector quantization for end-to-end learning compressible
representations. pp. 1142-1152, 2017.
Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Gener-
ative adversarial networks for extreme learned image compression. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 221-231, 2019.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
YikUn Bai, Xiugang Wu, and Ayfer Ozgur. Information constrained optimal transport: From talagrand,
to marton, to cover. In 2020 IEEE International Symposium on Information Theory (ISIT), pp.
2210-2215. IEEE, 2020.
Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 6228-6237, 2018.
Yochai Blau and Tomer Michaeli. Rethinking lossy compression: The rate-distortion-perception
tradeoff. In International Conference on Machine Learning, pp. 675-685, 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems, 26:2292-2300, 2013.
Abbas El Gamal and Young-Han Kim. Network information theory. Cambridge university press,
2011.
Nir Elkayam and Meir Feder. One shot approach to lossy source coding under average distortion
constraints. In 2020 IEEE International Symposium on Information Theory (ISIT), pp. 2389-2393.
IEEE, 2020.
Robert M Gray and Thomas G Stockham. Dithered quantizers. IEEE Transactions on Information
Theory, 39(3):805-812, 1993.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Andras Gyorgy and TamaS Linder. Optimal entropy-constrained scalar quantization of a uniform
source. IEEE Transactions on Information Theory, 46(7):2704-2711, 2000.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Cheuk Ting Li and Abbas El Gamal. Strong functional representation lemma and applications to
coding theorems. IEEE Transactions on Information Theory, 64(11):6967-6978, 2018.
Ryutaroh Matsumoto. Introducing the perception-distortion tradeoff into the rate-distortion theory of
general information sources. IEICE Communications Express, 7(11):427-431, 2018.
Ryutaroh Matsumoto. Rate-distortion-perception tradeoff of variable-length source coding for general
information sources. IEICE Communications Express, 8(2):38-42, 2019.
Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised
photo upsampling via latent space exploration of generative models. In Proceedings of the ieee/cvf
conference on computer vision and pattern recognition, pp. 2437-2445, 2020.
Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Condi-
tional probability models for deep image compression. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4394-4402, 2018.
10
Published as a conference paper at ICLR 2022
Fabian Mentzer, George D Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity
generative image compression. In Advances in Neural Information Processing Systems, volume 33,
2020.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep
generative prior for versatile image restoration and manipulation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2021.
Gabriel Peyre and Marco CUtUrL Computational optimal transport. Foundations and Trends in
Machine Learning, 11(5-6), 2019.
Oren Rippel and LUbomir BoUrdev. Real-time adaptive image compression. In International
Conference on Machine Learning, pp. 2922-2930, 2017.
Naci Saldi, Tamas Linder, and Serdar YUkseL Randomized quantization and source coding with
constrained oUtpUt distribUtion. IEEE Transactions on Information Theory, 61(1):91-106, 2015a.
Naci Saldi, Tamas Linder, and Serdar YUksel. Output constrained lossy source coding with limited
common randomness. IEEE Transactions on Information Theory, 61(9):4984-4998, 2015b.
Shibani Santurkar, David Budden, and Nir Shavit. Generative compression. In 2018 Picture Coding
Symposium (PCS), pp. 258-262. IEEE, 2018.
Lucas Theis and Eirikur Agustsson. On the advantages of stochastic encoders. arXiv preprint
arXiv:2102.09270, 2021.
Lucas Theis and Aaron B Wagner. A coding theorem for the rate-distortion-perception function.
arXiv preprint arXiv:2104.13662, 2021.
Michael Tschannen, Eirikur Agustsson, and Mario Lucic. Deep generative models for distribution-
preserving lossy compression. In Advances in Neural Information Processing Systems, pp. 5929-
5940, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer, 2009.
Shuchan Wang, Photios A Stavrou, and Mikael Skoglund. Generalized talagrand inequality for
sinkhorn distance using entropy power inequality. arXiv preprint arXiv:2109.08430, 2021a.
Wei Wang, Fei Wen, Zeyu Yan, Rendong Ying, and Peilin Liu. Optimal transport for unsupervised
restoration learning. arXiv preprint arXiv:2108.02574, 2021b.
Zeyu Yan, Fei Wen, Rendong Ying, Chao Ma, and Peilin Liu. On perceptual lossy compression: The
cost of perceptual reconstruction and an optimal training framework. In International Conference
on Machine Learning, 2021.
George Zhang, Jingjing Qian, Jun Chen, and Ashish Khisti. Universal rate-distortion-perception
representations for lossy compression. arXiv preprint arXiv:2106.10311, 2021.
Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image
restoration. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3929-3938, 2017.
Jacob Ziv. On universal quantization. IEEE Transactions on Information Theory, 31(3):344-347,
1985.
11
Published as a conference paper at ICLR 2022
A Theoretical Results
A. 1 Distortion-Rate vs Rate-Distortion Formulation
In addition to Definitions 2 and 3, we can equivalently define
Rncr(pX,pY,D) , inf	H(Z)
pX,Z,Y ∈Mncr (pX,pY)
s.t. E[d(X, Y)] ≤ D,
Rcr(pX,pY,D) , inf	H(Z|U)
pU,X,Z,Y ∈Mcr(pX ,pY )
s.t.	E[d(X, Y)] ≤ D.
(19)
(20)
Dncr/cr (pX, pY , R) and Rncr/cr (pX, pY , D) are monotonically decreasing in R and D, respectively,
so they are the inverse of each other. Sometimes it is more convenient to work with this rate-distortion
formulation.
A.2 Proofs of Theoretical Results
Proof of Theorem 1. For any pX,Z,Y ∈ Mncr (pX, pY ) with H(Z) ≤ R,
E[kX-Yk2] =E[kX-E[X|Z]k2]+E[kY -E[Y|Z]k2]+E[kE[X|Z] -E[Y|Z]k2]
≥ Dmse (pX, pY , R),
where the last inequality follows from the definition of Dmse (pX , pY , R) and the fact that
max{H(E[X|Z]), H(E[Y |Z]} ≤ H(Z) ≤ R.
As a consequence, we must have Dncr (pX, pY , R) ≥ Dmse (pX, pY , R). On the other hand, for any
PX∣χ, pγ∣γ with EX|X] = X, E[Y|Y] = Y, H(X) ≤ R, and H(Y) ≤ R, We can construct a
joint distribution PXXYY SUch that X - X - Y - Y form a Markov chain, PXX = PXPX∣χ,
PYY = PYPγ∣γ, andPX,γ satisfying E[∣∣X - Y∣∣2] = W2(PX,Pγ). Note that
E[kX - Yk2] = E[∣X - Xk2] + E[∣Y - Yk2] + E[∣X - Yk2]
=E[∣X - Xk2] + E[kY - Yk2] + W2(PX,PY).	(21)
Let Z，X. It can be verified that px,z,y ∈ MnCr(PX,Py) and H(Z) = H(X) ≤ R, which,
together with (21), implies Dncr(PX,PY, R) ≤ Dmse(PX,PY, R). This completes the proof of (7).
Dropping the term Wm(PX ,Pγ) in (3) yields
Dncr(PX,PY,R) ≥ Dmse(PX, R) + Dmse(PY, R),
where
D mse(PX ,R) , inf E[∣X - X k2 ]
pX |X
ʌ ʌ ʌ
s.t. E[X|X] = X, H(X) ≤ R.
and Dmse(PY, R) is definely analogously. On the other hand, choosing PX∣χ = PX,χ andPY∣γ =
PY0∣γ in(3) gives
Dncr (PX,Py,R) ≤ Dmse(PX , R) + Dmse(PY,R) + W2 (PX0 ,Pγ0 ),
wherePX,χ andPγ,∣γ are the minimizers that attain Dmse(PX,R) and Dmse(PY, R) respectively
while Pχo andPγ, are their induced marginal distributions. It is clear thatPX,∣χ andPY,∣γ coincide
12
Published as a conference paper at ICLR 2022
__ ʌ ʌ _______________ ʌ ʌ
with PX*∣χ and PY*丫 respectively as the constraints E[X|X] = X and E[Y|Y] = Y are auto-
matically satisfied by PX*∣χ andPY*∣y. This proves (8). For the special casePX = PY, We have
PX *∣χ = Pγ *∣γ and consequently the upper bound and the lower bound in (8) coincide.
Note that due to the involvement of conditional expectation, X is not necessarily defined over X if X
is a strict subset of Rn (for the same reason, Y is not necessarily defined over Y). In other words, the
output of the quantizer is not consrained to the input alphabet and needs to be relaxed to Rn . As such,
Dmse (PX, R) should be interpreted as the one-shot distortion-rate function with the reconstruction
alphabet being Rn , which is in general strictly below its counterpart with the reconstruction alphabet
being X (also known as the distortion-rate-perception function with an inactive perception constraint)
if X is a strictly subset of Rn . This subtle issue, which is often overlooked in the literature, arises
when one deals with discrete X and Y (see the binary example in Section 2.2 and Appendix A.4). □
ʌ ʌ
Proofof Theorem 2. We have that max{H (X), H (Y)} ≤ R*. If one of them, say H (Y), is less
than R*, this will lead to a contradiction by the following argument. Note that
D* = E[kX - Yk2] = E[kX - Xk2] + E[kY - Yk2] + E[kX - Yk2],
which depends on PXXYY only through PXX, PXY, and PYY. We can construct a new joint
distribution PX X, Y, Y such that PX X, = PX X, PX, Y, = PX Y, PYY, = PYY, and X — X0 —
Y0 — Y form a Markov chain. Denote X0 by Z0. It is clear that the induced px,z,,y belongs to
Mncr(PX,Py), preserves E[∣∣X - Y∣∣2], and H(Z0) < R*, which is contradictory with the fact that
Dncr (px ,Pχ ,R) is a strictly decreasing function in a neighborhood of R = R* since R can be set
slightly below R* without violating the constraint H(Z0) ≤ R. This proves (9), which futher implies
the existence of a bijection between X and Y.
It remains to prove (10). If (10) does not hold, then we can find some PX,, γ,, with pχ,, = pχ and
Pγ,, = pγ such that E[∣∣X00 - Y00∣∣2] < E[∣∣X 一 Y∣∣2]. Leveragethispχ,, γ,, to construct a new joint
distribution PX X,, Y,, Y such that PX X,, = PXX, PYY,, = PYY, and X - X00 - Y00 - Y form
a Markov chain. Denote X00 by Z00. It is clear that the induced pχ z„ Y belongs to Mncr(PX,Py),
H (Z00) = R*,and	，，
E[∣∣X - Y『]=E[∣∣X - X00k2] + E[∣∣Y - Y00∣2] + E[∣∣X00 - Y00∣2]
< e[∣∣x - X k2] + e[∣∣y - Y k2] + e[∣∣X - Y k2]
=D,
which is contradictory with the fact that Rncr(PX , PY, D) is a strictly decreasing function in a
neighborhood of D = D* ，Dncr(PX,Pχ, R*) since D can be set slightly below D* without
violating the constraint E[kX - Yk2] ≤ D. So in conclusion, the converter is a one-to-one mapping,
which induces an optimal coupling that attains W2 (PX,pγ).	□
Proof of Theorem 3. Choosing PU,X,Y from W22(PX,PY) and setting Z = Y shows that
Dcr(PX,PY,R) ≤	inf	E[d(X, Y)]
pU,X,Y ∈W (pX,pY )
s.t. H(Y|U) ≤ R.
So it remains to prove that this upper bound is tight. In the light of the functional representation
lemma, for any (U, X, Z, Y) with PU,X,z,Y ∈ Mcr(PX,PY), there exist V1, independent of (U, X),
and V2, independent of (U, X, V1), as well as determintic mappings φ1 and φ2 such that Z =
φ2(U, X, V1) and Y = φ2(Z, V2). Let U0 , (U, V1, V2). Clearly, PU,,X,Y = PU,PXPY|U,,X and
H(Y|U0, X) = 0. Moreover, we have
H(Z|U) ≥ H(Z|U0)
≥ H(Y|U0),
13
Published as a conference paper at ICLR 2022
where the last inequality is due to the fact that Y is determined by (Z, U0). Therefore,
Dcr(pX,pY,R) ≥	inf	E[d(X, Y)]
pU0,X,Y ∈W(pX,pY)
s.t. H(Y |U0) ≤ R.
This completes the proof of (12).
Note that each realization of U is associated with a deterministic function from X to Y . As a
consequence, the problem boils down to optimizing the probablity distribution defined over this
collection of functions. For the finite alphabet case, there are totally |Y||X| such functions. In fact, a
simple application of the support lemma shows that only |Y | + 1 functions need to be assigned with
a positive probability.	口
A.3 Linear Program Formulation for Common Randomness
In the finite alphabet case, we can formulate Theorem 3 as follows:
Dcr(pX,pY,R) =minXpU(u)E[d(X,fu(X))]
pU u∈U
s.t. X pU (u)H(fu(X)) ≤ R,
u∈U
XpU(u)P(fu(X) = y) = pY (y),	y ∈ Y,
u∈U
where PU is defined over U，{1,2, ∙∙∙ , ∣Y∣lXl}, and {f : U ∈ U} is the set of all distinct functions
from X to Y. By the support lemma (Appendix C on page 631 of El Gamal & Kim (2011)), only
|Y | + 1 functions need to be assigned with a positive probability.
A.4 B inary Case
Let Dm(Bin) , |qX -qY | and Dm(Ba)x , qX + qY - 2qX qY . Note that Dm(Bin) is the total variation distance
between B(qX) and B(qY ), which is the minimum E[dH (X, Y )] achievable by coupling X and Y .
On the other hand, we have E[dH (X, Y )] = Dm(Ba)x for X, Y independent. It is clear that Dm(Bin) and
Dm(Ba)x are the infimum and supremum of Dncr(B(qX), B(qY ), R) (as well as Dcr(B(qX), B(qY ), R)),
respectively.
Theorem 4. Assume Hamming distortion measure. Under no common randomness, we have
2(I-qx )(1-qy)
Dncr(B(qX), B(qY), R)
1-H-1(R)
+ 2 - qX - qY ,
2qx qγ
I-H-I(R)
+ qX + qY ,
qX + qY ≤ 1,
qX + qY > 1,
(22)
—
—
for R ∈ [0,min{Hb(qX), Hb(qY)}], and = Dm(Bin) forR > min{Hb(qX), Hb(qY)}. Moreover,
Dncr(B(qx), B(qγ),R)
-2(I-qX)(1-qY) + 2 -OV- ”
I-H-I (R)	+ 2	qx	qY,
- I-H-YR) + qx + qY，
(i-qχ )2+qY-(qγ-qx+h-1(R))2
----------------τ~.-：--------
I-H-I(R)
qX +(1-qγ )2-(qx-qγ + H-1(R))2
----------------πτ—:---------
1-H-1(R)
+ Hb-1(R)
+ Hb-1(R)
2qγ (1-qx )H-1(R)
(1-H-I(R))2
2qx (1-qγ )H-1(R)
(1-H-I(R))2
qx ,qγ ≤ 2,
qx ,qγ ≥ 2,
qx < 1 ,qγ > 1,
qx > 1 ,qγ < 1,
(23)
14
Published as a conference paper at ICLR 2022
for R ∈ [0,min{Hb(qX), Hb(qY)}], and
Dncr(B(qχ), B(qγ)E =
(1-qχ )2 + (1-qγ )2
1-H-1 (R)
qX+qY
+ 2 - qX - qY,
1-H-1(R)
(1-qX )2+qY
+ qX + qY ,
1-H-1(R)
qX +(I-qY 猿
I-H-I(R)
+ 1 - qX + qY,
+ 1 + qX - qY,
qχ ,qγ ≤ 2,
qχ ,qγ ≥ 2,
qχ < 1 ,qγ > 1,
qχ > 1 ,qγ < 1,
(24)
—
—
—
—
for R ∈ [0, min{Hb (qX), Hb(qY)}]. Here, Hb-1(R) denotes the inverse of the binary entropy
function on [0, 1/2]. With common randomness,
Dcr(B(qX), B(qY), R)
2(1 - qX)qXR
Hb(qX)
+ D(B )
max
(25)
—
forR ∈ [0,ρHb(qX)], and= Dm(Bin) forR > ρHb(qX). Here, ρ , min{qY/qX, (1 - qY)/(1 -qX)}.
Proof of (25). There are totally 4 distinct functions from {0, 1} to {0, 1}:
f1(x) = x,	f2(x) = 1 - x, f3(x) = 0,	f4(x) = 1, x ∈ {0, 1}.
Therefore, we have
XpU(u)H(fu(X)) = Hb(qX)(pU(1) +pU(2)),
u∈U
X pU (u)E[dH (X, fu(X))] = pU (2) + qX pU (3) + (1 - qX)pU (4),
u∈U
X pU (u)P(fu (X) = 1) = pU (1)qX + (1 - qX)pU (2) + pU (4).
u∈U
In light of Theorem 3,
Rcr(B(qX), B(qY), D) =	min	Hb (qX)(pU (1) +pU(2))
PU⑴,…,PU⑷
s.t. pU (2) + qX pU (3) + (1 - qX)pU (4) ≤ D,	(26)
qX pU (1) + (1 - qX)pU (2) + pU (4) = qY,	(27)
pU(1)+pU(2)+pU(3)+pU(4) = 1,	(28)
pU (1), pU (2), pU (3), pU (4) ≥ 0.	(29)
Note that
pU (2) + qX pU (3) + (1 - qX)pU (4)
= pU (2) + qX pU (3) + (1 - qX)(1 - pU (1) - pU (2) - pU (3))	(30)
= -(1 - qX)pU (1) + qX pU (2) + (2qX - 1)pU (3) + 1 - qX,	(31)
where (30) is due to (28). Moreover, it follows by (27) and (28) that
pU (3) = -(1 - qX)pU (1) - qX pU (2) + 1 - qY.	(32)
Substituting (32) into (31) and invoking the fact that pU (2) ≥ 0 gives
pU (2) + qX pU (3) + (1 - qX)pU (4)
= -2(1 - qX )qX (pU (1) + pU (2)) + 4(1 - qX )qX pU (2) + Dm(Ba)x
≥ -2(1 - qX)qX (pU (1) + pU (2)) + Dm(Ba)x,
which, together with (26), implies
PU ⑴ + PU ⑵ ≥ 布--------;一(D(Bx - D).
2(1 - qX)qX
15
Published as a conference paper at ICLR 2022
As a consequence, we must have
Rcr(B(qx), B(qγ),D) ≥ Dd)	(DmBIx- D)∙
2(1 - qX)qX
One can readily verify that this lower bound is tight as it is attained by PU with
PU⑴=2(1-∖χ)qχ (DmBx- D),
PU ⑵=0,
pu ⑶=-5— (DmBx -D)+1 - qγ,
2qX
pU ⑷ = -布-------7 (D(Bx - D) + qY，
2(1 - qX)
which satisfies (26)-(29) for D ∈ [DmB，, DmBx]. The expression of Dcr(B(qχ), B(qγ), R) can be
obtained by taking the inverse of Rcr(B(qχ), B(qγ), D).	□
Proof of (22). We will rely on some results which will come after this proof.
Note that Hamming distortion coincides with squared error distortion when X = Y = {0, 1}. So
Theorem 1, Lemma 1, and Lemma 2 are applicable here. In particular, in light of Lemmas 1 and
2, for any R ≥ 0 and e > 0, there exists a joint distribution PXXYY compatible with the given
marginal distributions PX and PY such that X and Y are deterministically related finite-support
ʌ ʌ ʌ ʌ
random variables with H (X) = H (Y) ≤ R and X - X - Y - Y form a Markov chain;
ʌ ʌ ʌ ʌ
moreover, X = E[X[X], Y = E[Y[Y], and
E[kX - Xk2] + E[k Y - Yk]+ E[kX - Yk2] ≤ Dncr(B(qχ), B(qY), R) + e∙ (33)
Without loss of generality, we assume X and Y take value from {Xi}N=ι and {y)i}N=ι, respectively,
and Y = ψ(X), where ψ is a bijection from {Xi}N=ι to {0i}N=ι with yi = ψ(Xi), i = 1,…，N. Let
θi，PX(Xi), or equivalently, θi，PY(yi), i = 1,…，N. Note that X = E[X[X] and Y = E[Y[Y]
if and only if PXX (1|Xi) = Xi and PY∣y(1y) = r^i for θi > 0, i = 1, ∙∙∙ , N. So the constraints
PN=I PX(Xi)PXIX(11Xi) = qX and PN=IPY(Oi)PYIY(1|yi) = qγ can be written equivalently as
PN=I θiXi = qX and PN=I θiyi = qγ. Moreover, it is easy to verify that
E[kX - X k2 ] + E[kY - Y k2]+ E[kX - Y k2]
NNn
=X θi(1 - Xi)Xi + X θi(1 - yi)yi + X θi (Xi- yi)2
i=1	i=1	i=1
N
=X θi(Xi + yi - 2XiOi).
i=1
In light of Theorem 1 and (33), the following optimization problem (P) yields an upper bound on
Dncr(PX,PY, R) with a gap at most e:
N
min X θi(Xi + yi
(θi,χi,yi,)N=ι M
N1
s.t.	Vθi log - ≤ R,
i=1	θi
-2XiOi)
N
Xθi = 1,
i=1
(P)
NN
E OiXi = qX, E Oiyi = qγ,
i=1	i=1
θi ≥ 0, Xi ∈ [0, 1], yi ∈ [0,1], i = 1,…，N.
Given (θi, yi)NN, (P) degenerates to a linear programming problem with respect (Xi)N=I over hyper-
rectangle [0, 1]N subject to the constraint PN=I θiXi = qX, for which the minimum is attained at
16
Published as a conference paper at ICLR 2022
a point on an edge of [0,1]N. Therefore, it suffices to consider (Xi)N=I with at most one element
different from 0 and 1. By a similar argument, it can be shown that there is no loss of optimality in
assuming that at most one of yi, i = 1,…，N, takes value other than 0 or 1. Due to the merge of
different elements, the one-to-one relationship might not be preserved. Nevertheless, by Lemma 2,
We just need to consider deterministically related X and Y with support size at most 3. Applying the
linear programming argument to (P) with N = 3 shows that, at the cost of potentially compromising
the one-to-one relationship, at most one element in the support of X as well as the support of Y need
to take value different from 0 and 1. In the case that the bijection is lost, X or Y must have a reduced
support size. One can restore the bijection by invoking Lemma 2, then use the linear programming
argument to assign extreme values to all but at most one element in the support. Following this line of
reasoning, we can conclude that the attention can be restricted to deterministically related X and Y
with support size at most 3 and at most one element in the support different from 0 and 1. Moreover,
the following configurations can be excluded.
1.	Support size = 3 and the existence of pairs (X, y) and (X0, y0) for some X > X0 and y < y0
((X, y) is said to be a pair if X = X ⇔ Y = y): Since
(X - y)2 + (X0 - y0)2 - (X - y0)2 - (X0 - y)2
=-2Xy - 2X0y0 + 2Xy0 + 2X0y
= 2(X - X0)(y0-仍
> 0,
it follows that E[∣∣X -Y ∣∣2] can be strictly reduced by moving the same amount of probability
from {X = X,Y = y} to {X = X,Y = y0} and from {X = X0,Y = y0} to {X =
ʌ ʌ ʌ
X0, Y = y}. This modification does not affect PX and PY, and consequently H (X), H (Y),
E[∣X - X k2], E[∣∣Y - Y k2] remain the same. So the distortion-rate performance of this
configuration is strictly suboptimal.
2.	Support size = 2 and the existence of pairs (X, y) and (X0, y0) for some X > X0 and y < y0:
Same as configuration 1).
3.	Support size = 2 and existence of pairs (X, 1) and (0, y) for some X ∈ (0,1) and y ∈ (0,1):
ʌ ʌ ʌ ʌ
It follows by E[X |X] = X and E[Y|Y] = Y that
Px ,Y(X, 1) = qχX,
px ,y (0,仍=1 - qχ,
1 - qY
y = 1 - T-^X.
X
Clearly, H(X) = H(Y) = Hb(qX). Since X ∈ (0,1) and y ∈ (0,1), we must have
qx < qX < qγ, which implies H(X) = H(Y) > min{H(X), H(Y)}. Furthermore, it
can be verified that
E[∣X - Xk2] + E[∣Y - Yk2] + E[∣X - Yk2]
qxx x(1 - x) + (1- qXx) y(1
qY - qx .
However, this end-to-end distortion is obviously achievable when R = min{H(X), H(Y)}.
So the rate-distortion performance of this configuration is strictly suboptimal. 4
4. Support size = 2 and the existence of pairs (X, 0) and (1, y) for some X ∈ (0,1) and
y ∈ (0,1): Same as configuration 3).
17
Published as a conference paper at ICLR 2022
In view of the excluded configurations, We are left with the case where PXY assigns all probabilities
to {X = 0, Y = 0}, {X = 1, Y = 1}, and {X = X, Y = y} for some X ∈ [0,1] and y ∈ [0,1]. So
it suffices to consider the N = 3 version of (P) with Xi = yι = 0, X3 = y3 = 1, X2 = X, and y2 = y.
The constraints P3=1 θi = 1, P3=1 θiXi = qχ, and P3=1 θiy = qγ imply
θ1 = 1 - qX - (I - X)θ,
θ3 = qχ - Xθ,
qγ - qX
y = ~Γ~
+ X.
In this way, we get a simplified optimization problem (P’):
min 2X(1 - X)θ + (1 - 2X)(qγ - qχ)	(P’)
θ,^
s.t. (1 - qχ - (1 - X)θ) log  -1-~+θ + θ log 1 + (qχ - Xθ) log -1-7τ ≤ R,
1 — qχ — (1 — X)θ	θ	qχ — Xθ
X ∈	[0,	1], θ ∈ [0, 1], (I - X)θ	∈	[qY	-	qχ,	1 - qχ],	xθ ∈	[qχ	-	qY, qχ].
Note that (P’) does not depend on and consequently yields the exact characterization of
Dncr (B(qχ), B(qY), R). Therefore, Rncr(B(qχ), B(qY), D) is characterized by the following opti-
mization problem (P”):
min (1 - qχ - (1 - X)θ) log  --171~+a + θ log 1 + (qχ - Xθ) log —1~^	(P”)
θ,x	1 — qχ — (1 — X)θ	θ	qχ — Xθ
s.t. 2X(1 — X)θ + (1 — 2X)(qγ — qχ) ≤ D,
X ∈ [0, 1],	θ ∈ [0, 1],	(1 — X)θ ∈ [qY - qχ, 1 - qχ],	xθ ∈ [qχ - qY, qχ].
Given X, the objective function of (P")is concave in θ and consequently its minimum is attained at
an endpoint of [θ, θ], where
θ，max 10,厂X,qχ二ɪ],
[	1 — X	X J
万 ʌ . ʃ,	1 - qχ	qχ	D - (1 -	2X)(qY	- qχ)[
θ , mint1, T-T ,ɪ,------------2X(T-1)----ʃ.
Without loss of generality, we assume qY ≥ qχ and qχ + qY ≤ 1. The following statements can be
easily verified.
1.
For X ∈ [0, D+qX-γqr],
θ
qY - qχ
1 — X
θ
1 - qχ
1 — X .
2 FCr 4 ∈ (D+qχ—qγ qχ+qγ—D ]
2. For X ∈ ( 2(1 —qγ) ,	2qγ ],
θ
qY - qχ
1 — X
θ = D - (I - 2X)(qY - qχ)
=	2X(1 — X)
3. For X ∈ (qx+qγ≡d, qγ],
θ
qY - qχ
1 — X
θ
qχ
X
18
Published as a conference paper at ICLR 2022
4. For X > qX, [θ,θ] is empty.
Note that When X ∈ [0, qχ] and θ = θ,
1
(1 - qχ - (1 - x)θ)log
1 - qχ - (1 - x)θ
+ θ log 1 + (qx - χθ)log qχ⅛
= (I-qY) log τ⅛+θlog 1 + (qx - xθ) log qχ⅛
≥ Hb (qY )
= H(Y).
So it suffices to consider the case X ∈ [0, qχ] and θ = θ, for which (P")is reduced to the following
form:
min η(X),
x∈[0,含]
where
1-qχ ]c(T i-^ I qχ-' ice
ι-^ log ι-qχ + ι-xτ log
2X-qχ + (1-2^)qγ-D
η(x)， +
2
D-(I-2x)(qy-qχ)
2X(1-^)
qχ + (1-2^)qY-D
2(1-X)
log
log
log
1-x
qχ-x ,
2X
[0, D+≡qYqγ ],
2^-qχ + (1-2^)qY -D
2^(1-^)
D-(1-2X)(qY -qχ )
2(1-^)
仑—qx log U____+ qχ log ɪ
X iθg ^-qχ + X iog qχ ,
qχ +(1-2X)qγ-D ,
D+qx-qY qχ +qY-D1
(2(1-qY) , -2qY-],
(qχ +qY-D qχ ]
I	2qY	, qγ 卜
+
X
X
X
∈
∈
∈
Note that η(X) is a continuous function over [0, qχ]. Moreover, η(X) decreases monotonically from
Hb(qx) to Hb( 2-DIm-X-I-D) as X varies from 0 to D++qX-qY. Since η(X) is a concave function of
qX for X ∈ [qX +qY-D, qX], it follows that
min	η(X) = min < η(
x∈[ qX+Y-D, >]	1
qX + qY - D
2qY
)，n( qX)
qY
min ʃHb(	2qXqY	),Hb(qY)].
qX + qY - D
So we have
min
X∈[0, D+qχqY)Y ]∪[
qχ +qγ -D
2qγ
qx ]
qγ」
η(χ)
.ʃ
min
(
min
Hb(
2 Dmax- D	),Hb(
2 - qX - qY - D
2qX qY
qX + qY - D
), Hb(qY)
D(B)	D
Hb U- -Y-D ),Hb(
2qX qY
qX + qY - D
(34)
min η(
D + qX - qY
≥
^∈[
2(1 - qY)
min
),η(
qX + qY - D
2qY
D+qχ -qY qχ+qγ -D
2(1-qy)	,	2qγ
η(x),
]
)
)
where (34) is due to the fact that Hb(qγ) ≥ Hb(qχ) ≥ Hb( ?二m-x-D-D) (with the first inequality
being a consequence of qX ≤ qY and qX + qY ≤ 1). So the problem boils down to solving
min	η(X).
a∈[ D+qχ-qγ qχ +qγ-D ]
x∈[ 2(1-qY) , 2qY ]
It can be verified that the minimum is attained at X = D+qXq-qY. This completes the proof of (22).
A graphical illustration of the entropy-constrained optimal transport plan for the binary case can be
found in Figure 6.
19
Published as a conference paper at ICLR 2022
Proof of (23). Note that
	Dmse(B(qX ),R) = 2 Dncr(B(qx ), B(qX ),R) f - I(IH-XC)R) + 1 - qX, qX ≤ 1, =	1-HqbX2 (R)	12	R∈ [0,Hb(qx)], I- 1-H-1(R) + qx,	qx > 2 , Dmse(B(qγ ),R) = 2 Dncr(B(qγ ), B(qγ ),R) f - I(IH-YI)R) + 1 - qγ, qγ ≤ 1, =	1-Hqb2 (R)	12	R∈ [0,Hb(qγ)]. l- 1-H-1(R) + qγ,	qγ > 2,
Moreover, we have
	卜X*( THbI(RR)) = 1 - H-I(R), R ∈ [0,Hb(qx)],qx ≤ 1, IpX* (1)= H-"),	2 ʃpX* (0)= Hu(R)	R	J PX* (I-HXI(R) ) = 1 - H-I(R), R ∈ [0,Hb(qx)],qx ≥ 2, 1- b ( ) ʃpY * (qY -H⅞R) ) = 1 - H-I(R),	1 I Y (I)-HiH(RI)(R)	b	R ∈ [0,Hb(qγ)],qγ ≤ 2, ʃpY*(0) = H-I(R),	Ru「nH, V >1 ∣pγ* (1-H¾(Ry) = 1 - H-i(R),	R ∈ [0, HKqY)],qγ ≥ 2.
So
W2 (PX * ,pγ> * )
(qx -qγ)2
1-H-1(R),
(qy-qx+H-1(R))2
I-H-I(R)
(qχ TY + H-1(R))2
I-H-I(R)
+ Hb-1(R) -
+ Hb-1 (R) -
2qγ (1-qχ)H-I (R)
(1-H-I(R))2
2qx (1-qγ )H-1(R)
(1-H-I(R))2
qx ,qγ ≤ 1 or qx ,qγ ≥ 1,
qx < 1 ,qγ > 1,
qx > 1 ,q γ < 1.
Based on the above expressions, one can easily verify (23) and (24). In particular, it is worth noting
that when qx ,qγ ≤ 2 or qx ,qγ ≥ 2,
Dncr(B(qx), B(qγ),R) = Dncr(B(qx), B(qγ), R), R ∈ [0, min{Hb(qx), Hb(qγ)}],
i.e., there is no penalty for using optimal quantizer and dequantizer in the conventional rate-distortion
sense.
Remark: It is easy to verify that
Dcr(B(qx), R) , min Dcr(B(qx), B(qγ), R)
qY ∈[0,1]
=1qx (1 - HbRlX)) ,	R ∈ [0, HbIqX)],qx ≤ 2,
[(I - qX) (1 - HbRqX)) , R ∈ [0, Hb(qX )],qX > 2,
Dncr(B(qx),R), min Dncr(B(qx), B(qγ), R)
qY ∈[0,1]
=[qx - H-I(R)	R ∈ [0, Hb(qx)],qx ≤ 2,
11 - qx - Hb 1(R), R ∈ [0, Hb(qx)], qx > 1,
which are respectively the conventional one-shot distortion-distortion function (or equivalently, one-
shot distortion-rate-perception function with an inactive perception constraint) for B(qx) with and
without common randomness. In general, Dncr (B(qx), R) is different from Dmse(B(qx), R) (the
former is strictly above the latter). The reason is as follows: even though the output distribution
constraint is removed in the definition of Dncr(B(qx), R), the output alphabet remains to be {0, 1};
in contrast, for Dmse(B(qx), R), the output alphabet is relaxed to R.	口
20
Published as a conference paper at ICLR 2022
A.5 Auxiliary Results
Lemma 1 (Finite Support Approximation). For any R ≥ 0 and > 0, there exists a joint distribution
PXX Y Y compatible with the given marginal distributions PX and PY such that X - X - Y — Y
ʌ ʌ ʌ
form a Markov chain and X is a finite-support random variable with H (X) ≤ R (or Y is a finite-
ʌ __ ʌ ʌ __ ʌ ʌ
support random variable with H(Y) ≤ R); moreover, E[X|X] = X, E[Y|Y] = Y, and
E[kX - Xk2]+ E[kY - Yk]+ E[kX - Yk2] ≤ Dncr(PX,PY, R) + C.
Proof. In light of Theorem 1, We can find PXX Y Y such that X - X - Y - Y form a Markov
ʌ ʌ ʌ ʌ
chain, H(X) ≤ R, E[X∣X] = X, E[Y∣Y], and
E[kX - X k2]+ E[kY - Y k]+ E[kX - Y k2] ≤ Dncr(PX ,PY, R) + 2	(35)
The proof is complete if X isa finite-support random variable. So it suffices to consider the case where
X takes value from some countably infinite set {Xi}∞=ι. Since E[∣∣X∣∣2] < ∞ and E[∣∣Y∣∣2] < ∞, it
follows that there exists a positive integer N such that
P{X ∈ {Xi}∞=N}E[∣Xk2 + kYk2∣X ∈ {Xi}∞=N] ≤ j∙
Let X0 , X if X ∈ {Xi}N=11 and X0，E[X|X ∈ {Xi}∞:N] if X ∈ {Xi}∞=N. Note that
E[∣X - X0k2]+ E[∣X0- Yk2]
=P{X ∈ {Xi}N-1}E[kX - X0k2 + kX0 - Yk2∣X ∈ {Xi}N-1]
+ p{X ∈ {Xi}∞=N}e[∣x - X0k2 + kX0 - Yk2X ∈ {Xi}∞=N]
=p{X ∈ {Xi}N-1}E[kχ - Xk2 + kX - Yk2X ∈ {Xi}N-1]
+p{X ∈ {Xi}∞=N}E[kχ - X0k2 + kX0 - Yk2X ∈ {Xi}∞=N]
≤ P{X ∈ {Xi}N-1}E[kχ - Xk2 + kX - Yk2X ∈ {Xi}N-1]
+p{X ∈ {Xi}∞=N }E[kχ - X 0k2 + 2∣x 0k2 + 2∣y k2∣x ∈ {a}∞=N ]
≤ p{X ∈ {Xi}N-1}E[kχ - Xk2 + kX - Yk2X ∈ -I
+p{X ∈ {Xi}∞=N }E[kχ k2 + kX0k2 + 2∣y k2 X ∈ {Xi}∞=N ]
≤ E[kχ - Xk2 + kX - Yk2] + 2P{X ∈ {Xi}∞=N}E[kχk2 + kYk2X ∈ {Xi}∞=N]
≤ E[kX - X k2]+ E[kX - Y k2] + ∣∙	(36)
Now define a new joint distribution PX X〃 Y〃 Y such that PX X〃 = PXX,, PX〃 Y〃 = PX, y,
PYYo’ = PyY, and X - X00 - Y00 - Y form a Markov chain. It is clear that X00 is a finite-
ʌ ʌ ʌ ʌ ʌ ʌ ʌ
support random variable with H(X00) = H(X0) ≤ H(X) ≤ R, E[X|X00] = X00, E[Y|Y00] = Y00,
and
E[kX - X00k2] + E[kY - Y00k] + E[kX00 - Y00k2]
≤ e[∣x - X0k2] + e[∣y - Yk] + e[∣X0 - Yk2]
≤ E[kX - X k2]+ E[kY - Y k]+ E[kX - Y k2] + C	(37)
≤ Dncr(PX , PY, R) + C,	(38)
where (37) and (38) are due to (35) and (36), respectively. This completes the proof of Lemma 1. □
Lemma 2 (Deterministic on Finite Support). Let X - X
- Y - Y be a Markov chain with
ʌ	ʌ __ ʌ ʌ ʌ ʌ
E[X|X] =	X, E[Y|Y]	= Y, and	assume that X	(or Y) is a	finite-support random variable. There
ʌ ʌ
exist deterministically related random variables X 0 and Y0, with the support size no greater than that of
21
Published as a conference paper at ICLR 2022
Figure 6: Illustration of the entropy-constrained optimal transport plan for the binary case (assuming
qx + qγ ≤ 1), where PX (X)= PY (y) = 1 - H- (R) With X =亭：-1(；) and y = VY-H-IR ∙
It is interesting to note that the quantizer PX∣χ does not depend on PY while the dequantizer PY∣ γ
does not depend on PX. So they are decoupled in a certain sense. Moreover, PX∣χ andPγ∣γ coincide
respectively with optimal quantizer PX*∣x and dequantizer Pγ∣γ* in the conventional rate-distortion
sense when qX , qY ≤ 1/2.
ʌ ʌ ʌ ʌ ʌ ʌ ʌ ʌ ʌ
X and H (X 0) = H (Y 0) ≤ H (X) (or H (X 0) = H (Y 0) ≤ H (Y)), such that X 仆 X 0 仆 Y 0 仆 Y
ʌ ʌ ʌ ʌ
form a Markov chain, E[X∣X0] = X0, E[Y| Y0] = Y0, and
E[kX - X0k2] + E[kY - Y0k] + E[kX0 - Y0k2]
=E[kX - Xk2]+ E[kY - Yk]+ E[kX - Yk2].
Proof. Let Y，E[Y[X]. Since Y 什 Y 什 Y form a Markov chain and E[Y[Y] = Y, we have
Y = E[Y[X]. Constructa new joint distribution PX Xo Yo Y such that PXXo = PXX, PXo γo =
PX Y, PγYo = PYY, and X 什 X0 什 Y0 什 Y form a Markov chain. It is clear that E[X |X0] = X0,
ʌ ʌ
E[Y|Y0] = Y0, and
E[kX - X 0k2] + E[kY - Y 0 k2]+ E[kX 0 - Y 0k2]
E[kX - X k2]+ E[kY - Y k2]+ E[kX - Y k2]
E[kX
E[kX - X k2]+ E[kY - Y k2]+ E[kX - Y k2],
~	入	入	ʌ	ʌ
where the last equality is due to Y = E[Y|X]. If the function that maps X0 to Y0 (or equivalently,
r∖	~	ʌ ʌ ʌ	ʌ
maps X to Y) is invertible, then X, X0, Y0 have the same support size and H(X) = H(X0) =
ʌ
H(Y0), which completes the proof. Otherwise, the support size of Y0 must be strictly smaller than
ʌ ʌ ʌ ʌ ʌ
that of X0 (which is the same as that of X) and H(Y0) < H(X0) = H(X). We can alternately
reduce the support sizes of X 0 and Y0 using this argument until they are deterministically related
(and consequently have the same support size and the same entropy). This can be accomplished in a
finite number of steps because the reduction in support size cannot continue forever.	□
□
A.6 Uniform Distribution
Let X 〜Unif[0, a] and Y 〜Unif[0, b] be uniformly distributed random variables, where a,b > 0.
Note that the density functions are given as: PX (x) = ɪ, 0 ≤ X ≤ a and PY(y) = b, 0 ≤ y ≤ b and
PX (X) and PY (y) are zero outside these intervals. The cumulative density functions of X and Y are
22
Published as a conference paper at ICLR 2022
given as follows:
0,	x ≤ 0,	0,	y ≤ 0,
Cx (X)=	a, 0 ≤ X ≤ a,	Cy (y) = y, 0 ≤ y ≤ b,
[1,	X ≥ a,	11,	y ≥ b.
Following Peyre & CUtUri (2019, Remark 2.30) We have that the optimal transport (without rate
constraint) is given by:
W22(px,pγ) = ∕1(Cχ1(r) - CY1(r))2dr = (-3a)2,	(39)
where Cχ1(∙) and C-1(∙) are the pseudo-inverse of the CDF functions for X and Y as defined in
Peyre & Cuturi (2019, Remark 2.30).
We will next develop an upper bound on Dncr(pX,pY, R) using Theorem 1 when the rate is of the
form R = log2 (N) for any N ∈ {1, 2, . . .}, by considering the following choice for X and Y:
Y ∈Y
a	3a 5a
2N, 2N, 2N,..
Ib 3b 5b
∣2N, 2N, 2N
(2N - 1)a1
-2N- ʃ
(2N - 1)b 1
-2N- ʃ 1
(40)
(41)
X ∈ X
To compute the upper bound we select PX∣χ to correspond to scalar quantization of X i.e., given X
we select X as a point in X closest to X. The distribution pγ∣ γ is defined in an analogous manner3 .
Our upper bound can be computed as:
D+cr(px,PY, R)= E[(X - X)2] + E[(Y - Y)2] + W2(PX,PY).	(42)
Note that with ∆ = N we have that E[(X - X)2] = a2∆2 and E[(Y - Y)2] = b2∆~ .Thus we only
need to compute the third term. Following Peyre & Cuturi (2019, Remark 2.28) we have that:
W22(PX,PY) = X X(2i - 1)2 ==(1-∆2) .	(43)
i=1
Thus using ∆2 = 2-2R, we have that
Dncr(PX,PY, R) = (b -a)2 + α∙b2-2R.	(44)
ncr	3	6
Note that the upper bound approaches the lower bound in (39) as R → ∞, with exponential rate of
convergence.
For the case of general R, we let N = d2Re and following Gyorgy & Linder (2000, Theorem 1), we
select
X ∈X
ac
c0
C+ 3^2) ,...,a (c +(2N - 3)^2
叫	X^2
--------------/	1
x^3
{z^
XN
(45)
Y∈Y = ∖ y ,b(c + 2),b 卜 + 3^2),...,b(c +(2N-3)^2^
I l{z} I-------' I--------' I-----------------
y1
{z
y2
{z
y3
{z^
^N
}
}
3Please note that we do not claim that the proposed choice is optimal with respect to Dmse in (4) although it
is known to be optimal solution for a related problem - the entropy constrained scalar quantization (Gyorgy &
Linder (2000)). AS a result we cannot claim to compute the upper bound Dncr stated Theorem 1 but provide
another upper bound.
23
Published as a conference paper at ICLR 2022
Figure 7: Example of Uniform sources with a = 2 and b = 5. The left plot shows the lower bound
W22(pX,pY ) in (39) and the upper bound Dn+cr (pX, pY , R) which is the sum of of the right hand
side in (46), (47) and (50). For comparison We also show the value of Wi(PX ,pγ). The right plot
shows the distortions associated with the quantization and dequantization steps.
where c is the unique solution in the interval (0, 1/N] to the equation:
-C log C - (1 - C) log (Ir C) = R,
N-1
and c0 = N-C) holds. Note that the length of the first interval is C and the length of all other intervals
is c0. In the special case where R = log? N we will have that C = C = NN and our construction for
X and Y is consistent with the previous case.
As before we usePX∣x andPY∣κ to be the distributions associated with scalar quantization. Thus we
have that:
E[(X - X)2] = C-ɪ2~ + (I - C)^2^,	(46)
b2C2	b2C02
E[(Y - Y)2] = c7γ + (1 - c) b1r.	(47)
Furthermore using the result stated in Peyre & Cuturi (2019, Remark 2.30) we have that
N
W22(Pχ,PY) = c(xι - yι)2 + C0 X(Xj- y )2	(48)
j=2
=(b -	a)2c4	+	(b	-	a)2c0	X	(C +	2j 2	1 C)	(49)
j=1
=(b - a)2 (J + c2c0(N - 1) + CC02 (N - 1)2 +	c03(2N - 1)(2N - 3)(N - 1)
(50)
Finally, the upper bound Dn+cr (PX, PY, R) can be obtained by summing the right hand side
of (46), (47) and (50). We provide a numerical evaluation of this upper bound in Fig. 7.
A.7 Asymptotic Optimal Transport
Let X1,X2,…and Yι,Y2,…be i.i.d. processes with marginal distributionsPX andPY, respec-
tively.
24
Published as a conference paper at ICLR 2022
Definition 4 (Asymptotic Optimal Transport with Entropy Bottleneck — no common randomness).
The asymptotic optimal transport from pX to pY with an entropy bottleneck of R and without
common randomness is defined as
Dn(∞cr)(pX, pY, R) , infDn(ncr)(pX,pY,R),
n≥1
where
1n
Dnn)(PX,PY,R) ,	inf	-	E[d(Xi,Yi)]
Pχn,Z,γn ∈Mncr(0n=ιPX M=IpY ) " ~^
s.t. - H(Z) ≤ R.
Remark: It is clear that Dn(1c)r(PX, PY, R) = Dncr (PX, PY, R). Moreover, one can readily show
that {nDn(ncr) (PX, PY, R)}n∞=1 is a subadditive sequence and consequently Dn(∞cr) (PX, PY, R) =
limn→∞ Dncr (PX , PY , R).
Theorem 5.	We have
Dn(∞cr)(PX, PY, R) = inf	E[d(X, Y)]
pX,Z,Y∈Mncr(pX,pY)
s.t.	max{I(X; Z), I(Y; Z)} ≤ R.
Proof. This result can be specialized from Theorem 1 in Saldi et al.(2015b).	□
The following result is the counterpart of Theorem 1 in the asymptotic setting.
Theorem 6.	Let
D(mse)(px,PY,R) ,	inf	E[kX - Xk2]+ E[kY - Yk2]+ W2(PX,Pγ)
pX ∣x ,pY | Y	(51)
ʌ ʌ ʌ ʌ ʌ ʌ
s.t. EX |X] = X,	E[Y∣Y] = Y, I(X; X) ≤ R,	I (Y; Y) ≤ R,
and
Dmse(PX,R) , inf E[kX - Xk2]
PXIX	(52)
ʌ
s.t. I(X; X) ≤ R.
Moreover, let
Dn∞r) (px ,py ,r)，DmSe) (PX ,r)+Dme(PY ㈤+w2(pχ * ,py *),	⑴)
Dn∞r)(pχ ,py , r)，DmSe)(PX, R)+DmSe)(PY, r),	。由
where pχ * and pγ * are the marginal distributions induced by the minimizers pχ *∣χ and pγ *∣γ that
attain Dm(Sse) (PX, R) and Dm(Sse)(PY, R), respectively (assuming the existence and uniqueness of such
minimizers). Then under the squared Eucledian distortion measure,
Dn(Scr)(pX, pY, R) =Dm(Sse)(pX,pY,R).	(55)
In addition, we have
Dn∞)(pχ ,py ,r) ≥ Dn∞)(pχ ,py ,r) ≥ Dn∞,(pχ ,py , R),	(56)
and both inequalities are tight when pX = pY.
Proof. For any pX,Z,Y ∈ Mncr(pX,pY) with max{I (X; Z), I(Y; Z)} ≤ R,
E[kX - Yk2] =E[kX-E[X|Z]k2]+E[kY-E[Y|Z]k2]+E[kE[X|Z]-E[Y|Z]k2]
25
Published as a conference paper at ICLR 2022
≥ Dm(∞se)(pX,pY , R),
where the last inequality follows from the definition of Dm(∞se) (pX, pY , R) and the fact that
max{I(X;E[X|Z]),I(Y;E[Y|Z]} ≤ max{I(X; Z), I(Y; Z)} ≤ R.
In view of Theorem 5, we must have Dn(∞cr) (pX, pY , R) ≥ Dm(∞se) (pX , pY , R). On the other hand,
ʌ ʌ ʌ ʌ ʌ ʌ
for any PX |X, PYIY with E[X |X] = X, E[Y∣Y] = Y, I(X; X) ≤ R, and I (Y; Y) ≤ R, We
—Y form a Markov chain,
=W?(PX,py). Note that
can construct a joint distribution PXXYY SUch that X - X - Y
PXX = pXpX∣χ, PYY = PYpY| Y, and pX,Y Satisfying e[∣∣x ― YlI B
E[kX - Yk2] = E[kX - Xk2] + E[kY - Yk2] + E[kX - Yk2]
=E[kX - Xk2] + E[kY - Yk2] + W2(PX,PY).	(57)
Let Z ，X. It can be verified that px,z,y ∈ Mncr(px,Py) and max{I(X; Z),I(Y; Z)}=
max{I(X; X),I(Y;X)} ≤ max{I(X; X),I(Y; Y)} ≤ R, which, together with (57), implies
Dn(∞cr) (PX,PY, R) ≤ Dm(∞se) (PX,PY, R). This completes the proof of (55).
Dropping the term Wl(PX ,pγ) in (51) yields
Dn∞r)(PX,PY,R) ≥ D(∞)(PX,R) + D(∞)(PY,R),
where
Dm∞)(px,R) , inf E[kX - Xk2]
pX |X
ʌ ʌ ʌ
s.t. E[X∣X] = X, I(X; X) ≤ R.
∣⅛≠
and Dmse(pY, R) is definely analogously. On the other hand, choosing pχ∣χ = pχ,χ andpγ∣γ =
Pγo∣γ in (51) gives
Dn∞r)(PX,pγ,r) ≤ Dm∞)(PX,R) + Dm∞e)(PY, R) + Wf(PXO,pγ0),
where pχ,∣χ andpγ,∣γ are the minimizers that attain D(∞∞)(PX ,R) and Dm∞)(PY , R) respectively
while pχo andpγ, are their induced marginal distributions. It is clear thatpχ,∣χ andpγ,∣γ coincide
___________________________________________________ ʌ ʌ ___________ ʌ ʌ
withpχ*∣χ andpγ*∣γ respectively as the constraints E[X|X] = X and E[Y|Y] = Y are automat-
ically satisfied by pχ*∣χ and pγ*∣γ. This proves (56). For the special case pχ = PY, we have
pχ *∣χ = pγ *∣γ and consequently the upper bound and the lower bound in (56) coincide. □
Definition 5 (Asymptotic Optimal Transport with Entropy Bottleneck — with Common Randomness).
The asymptotic optimal transport from pX to pγ with entropy bottleneck R and common randomness
is defined as
Dc(r∞)(pX,pγ,R), infDc(rn)(pX,pγ,R),
n≥1
where
1n
D(n)(pχ,PY,R)，	inf	— VE[d(Xi,Yi)]
PU,χn,Z,γn ∈Mcr(®i=ιPX 念々=\PY ) " ^^
s.t. 1H(Z|U) ≤ R.
n
Remark: It is clear that Dc(1r) (pχ, pγ, R) = Dcr(pχ, pγ, R). Moreover, one can readily show
that {nDc(nr ) (pχ, pγ, R)}n∞=1 is a subadditive sequence and consequently Dc(∞r ) (pχ, pγ, R) =
limn→∞ Dncr (pχ , pγ , R).
26
Published as a conference paper at ICLR 2022
Theorem 7.	We have
Dc(∞r )(pX,pY,R) =	inf	E[d(X, Y)]
pX,Y ∈Γ(pX,pY)
s.t. I(X; Y ) ≤ R.
Proof. This result is known (see Theorem 7 in Saldi et al. (2015a)). It is possible to give a simpler
proof of the achievability part by leveraging Theorem 3 and the strong data processing inequality Li
& El Gamal (2018). The converse is based on standard information-theoretic arguments. □
A.8 Gaussian Case
Let X 〜N(μχ,σχ) and Y 〜N(μγ, σY) be two Gaussian random variables, and let d(∙, ∙) be
the squared distortion measure (i.e., d(x, y) = (x - y)2). Let DmGGn，(μx - μγ)2 + (σχ - σγ)2
and Dm(Ga)x , (μX - μY )2 + σX2 + σY2 . Note that Dm(Gin) is the squared Wasserstein-2 distance
between N(μX, σX2 ) andN(μY, σY), which is the minimum E[(X - Y)2] achievable by coupling
X and Y. On the other hand, we have E[(X - Y)2] = Dm(Ga)x for X, Y independent. It is clear that
Dm(Gin) and Dm(Ga)x are the infimum and supremum of Dn(∞cr) (N(μX, σX2 ),N(μY, σY2 ), R) (as well as
Dc(∞r ) (N(μX, σX2 ),N(μY, σY2 ), R)), respectively.
Theorem 8.	Assume squared distortion measure. Under no common randomness, we have
Dn(∞cr)(N (μX, σX2 ),N(μY,σY2),R) = Dm(Gin) + 2σXσY2-2R,	R∈ [0, ∞).	(58)
Moreover,
Dn∞r)(N(μχ,σX),N(μγ,σ1),R) = DnMN(μχ,σX),N(μγ,σ1),R), R ∈ [0, ∞), (59)
Dn∞)(N(μχ,σX),N(μγ用),R) = (σX + σ2)2-2R	R ∈ [0, ∞).	(60)
With common randomness,
D(∞)(N(μχ,σX),N(μγ,σY),R)= DmGax - 2σχσγPr-I-R, R ∈ [0, ∞).	(61)
Proof. Consider PX∣χ and PY∣γ such that E[X[X], E[Y|Y], I(X; X) ≤ R, and I(Y; Y) ≤ R.
Denote the mean and the variance of X by μχ and σX, respectively. Similarly, denote the mean and
the variance of Y by μγ and σY , respectively. Clearly, μχ = μχ, μγ = μγ, σX = σX - E[(X -
X)2], and σY = σγ - E[(Y - Y)2]. Moreover,
W22(pχ,pγ) ≥ W22(N(μχ,σX),N(μγ,σY))
=(μχ - μγ)2 + (σχ - σγ)2
=(μx - μγ)2 + (σχ - QY)2.
So we have
E[(X - X)2]+ E[kY - Yk2]+ W22(PX,PY)
≥ σX - σX + σY - σY + (μχ - μγ)2 + (σχ - QY)2
=DmGax - 2σχ QY.	(62)
It can be verified that
R ≥ I(X; X)
1
=2 log(2πeσX) - h(X∣X)
≥ 2 log(2πeσX)- h(X - X)
27
Published as a conference paper at ICLR 2022
≥ 2 log(2πeσX) - 2 log(2πeE[(X - XY])
σ
2
X
2 log
σ
2
X
which implies
QX ≤ σχ √1 - 2-2R.
Similarly,
σγ ≤ Qy √1 - 2-2R.
Substituting (63) and (64) into (62) and invoking (55) in Theorem 6 shows
Dn∞r)(N(μχ,σX),Ng,σY),R) ≥ Dmn + 2σχQy2-2R.
To see that this lower bound is tight, we can let
X = X + N,
Y = Y + N,
(63)
(64)
(65)
(66)
where X 〜N(μx,σX(1 - 2-2R)) is independent of N 〜N(0,σX2-2R) while Y 〜
N(μy, Qy (1 - 2-2R)) is independent of N 〜N(0, Qy2-2r). This completes the proof of (58).
To prove (59) and (60), it suffices to note the well-known fact that PX∣χ andPy∣y associated with
(65) and (66) attain D(∞Se)(N(μχ, QX),R) and D(∞Se)(N(μy, Qy ),R), respectively.
Now we proceed to prove (61). Consider pχ,y ∈ Γ(N(μχ,qX), N(μy ,q?)) with I(X; Y) ≤ R.
Let ξ , E[(X - μχ)(Y - μy)]. We have '
E[(X - Y)2] = Dm(Ga)x - 2ξ.
(67)
Moreover,
R ≥ I(X; Y)
=1 log(2πeQX) + 1 log(2πeQy) - h(X, Y)
≥ 1 log(2πeQX) + 1 log(2πeQy) - 1 log((2πe)2(QXQy - ξ))
=2 log
which implies
QX Qy
QX Qy- ξ2,
ξ ≤ QXQy ʌ/1 - 2-2R.
Substituting (68) into (67) and invoking Theorem 7 shows
D(∞)(N(〃x,QX),N(μy,qY),R) ≥ Dmax - 2qxQy p1 - 2-2R.
(68)
To see that this lower bound is tight, we can let X and Y be jointly Gaussian with ξ =
QXQy √1 - 2-2r. This completes the proof of Theorem 8. We acknowledge that the expres-
sion of DcG)(N(μX,qX),N(μy, Qy), R) was established in (Saldi et al., 2015b, Section IV-B)
for the special case when the Gaussian distributions have zero mean. However, to the best of our
understanding, they only provided an upper bound for the case of no common randomness.	□
In Figure 8, We plot DnGr)(Ng,qX),N(μy,qY),R), Dn∞0(N(μX,qX),N(μy,qY),R),
Dncr(N(μX,qX),N(μy, Qγ), R), and D(∞)(N(μX,qX), N(μy ,qY),R) for two illustrative ex-
amples.
28
Published as a conference paper at ICLR 2022
1-
0-
0.50 -
0.25 -
0.00 -
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
Rate
(a)	(b)
Figure 8: GauSSian case distortion-rate tradeoffs. (a) μx = μγ = 0, σx = σγ = 1,
where D：；) (N(μχ, σX),N(μγ, σY),R) and Dn∞^ (N(μχ, σX),N(μγ, σY),R) coincide with
Dn∞r) (N(μχ,σχ),N(μγ,σY),R); (b) μχ = 0, σχ = 1, μγ = 1, σγ = 2, where
Dn∞r) (N(μχ,σχ), N(μγ, σY), R) is tight but D∞r) (N(μχ,σ1χ), N(μγ, σYγ), R) is loose. More-
over, it can be seen from both examples that common randomness can indeed help improve the
distortion-rate tradeoff.
B	Experimental Results
B.1	Dataset
Image super-resolution is conducted on MNIST (LeCun et al., 1998). For synthesizing low-resolution
images, we perform bilinear downsampling on the original image from 28 × 28 to 7 × 7. The samples
in Figure 5(b) show that the low resolution digits are blurry and some of them are hard to recognize.
Image denoising is conducted on SVHN (Netzer et al., 2011). In our experiments, we synthesize the
noisy image with additive Gaussian noise. The standard deviation is set to 20.
B.2	Universal Quantization
Let C be our codebook for quantization. Recall that the encoder uses a tanh activation so its output
lies in (-1, 1)d. Given dimension d and L quantization levels per dimension as parameters, C will
consist of L uniformly spaced intervals across all d dimensions. The upper bound of model rate is
given by d log(L). With this codebook, universal quantization (Ziv, 1985; Gray & Stockham, 1993;
Theis & Agustsson, 2021) is implemented as follows. We assume the sender and receiver have access
to the same U 〜 U[-1/(L - 1), +1∕(L - 1)]d. The sender computes
z = arg min kf (x) + u - ck
c∈C
and sends z to the receiver. The receiver decodes the image by passing z - u through the decoder.
This is also known as a subtractive dither in literature (Gray & Stockham, 1993). For super-resolution
and image denoising, the interval L is respectively fixed at 4 and 8.
B.3	Training Details
To induce distributional shift, we use the Wasserstein GAN for our experiments. We alternate between
training the encoder/decoder f, g, and the critic h. By Kantorovich-Rubinstein duality (Villani, 2009),
the critic is used to approximate
Wi (PY ,pγ )= SUP E[h(Y)] - E[h(Y)],	(69)
kVhk≤1
29
Published as a conference paper at ICLR 2022
where Y = g(Q(f (X) + U) - U) for U as in Appendix B.2. The Lipschitz constraint is implemented
with a gradient penalty (Gulrajani et al., 2017) in practice.
Super-resolution. The training for end-to-end network lasts for 50 epochs. λ in (15) is fixed at
1e - 3 across all rates. The learning rate initialized to be 0.0001 and is decayed by a factor of
5 after 30 epochs. The Adam (Kingma & Ba, 2014) optimizer is used. Table 3 illustrates the
detailed training setting. For the helper two-branch network at a specific rate constraint, we load
the pre-trained encoder weight of the corresponding end-to-end network, as well as two randomly
initialized decoders g1 , g2 . Note that only theese two decoders are trainable. During training, we use
the Adam optimizer with the learning rate initialized at 0.0001. There are a total of 100 epochs until
the convergence of the two decoders. The learning rate is decayed once at 50 epochs by a factor of 5.
Detailed training settings are shown in Table 4.
Image Denoising. The experiments for image denoising share many settings with image super-
resolution. Tables 3 and 4 can be reused to reproduce the experiments on image denoising. Here, we
list the difference between them. For denoising, the end-to-end model is trained for 100 epochs with
λ fixed at 3e - 3 across all rates. The learning rate is decayed by a factor of 5 after 40 epochs. The
two-branch model is trained for total 200 epochs and we decay the learning rate at 100 epochs by a
factor of 5.
Table 3: Hyperparameters used for training end-to-end model in Fig. 4(a). α is the learning rate,
(β1, β2) are the parameters for Adam, and λGP is the gradient penalty coefficient.
	α	βi	β2	λGP
Encoder	10-4	0.5	0.999	-
Decoder	10-4	0.5	0.999	-
Critic	10-4	0.5	0.999	10
Table 4: Hyperparameters used for training two-branch model in Fig. 4(b). α is the learning rate,
(β1, β2) are the parameters for Adam, and λGP is the gradient penalty coefficient.
	α	βi	β2	λGP
Encoder	0	-	-	-
Decoder-1	10-4	0.5	0.999	-
Decoder-2	10-4	0.5	0.999	-
Critic	10-4	0.5	0.999	10
B.4	Detailed Results in Figure. 5
In Fig. 5(a)(c), we have provided a comparison between the case with or without common randomness
in the form of a scatter chart. Here, we present detailed quantities of each point in Fig. 5(a)(c). Table
5 shows the number of each dot for super-resolution experiments, and Table 6 present the value
of each dot for image denoising. From the two tables, we can further see the utility of common
randomness quantitatively.
B.5	Comparison with Baseline
To illustrate the effectiveness of our system, we compare with a baseline method that separately deal
with the tasks of image restoration and compression. For the restoration (image super-resolution and
denoising), we respectively build two U-Nets with skip connections and train them in the unsupervised
manner by adopting the Eq. 15 as objective i.e.,
Li = E[kX — Y k2] + λι Wι(pγ ,pγ).	(70)
After the restoration networks are trained to converge, we fix their weights and use them to pro-
duce restored images Y given degraded one X. Afterwards, We adopt our end-to-end network as
compression network by minimizing the following loss at different rates:
30
Published as a conference paper at ICLR 2022
Table 5: The detailed number of MSE distortion loss in Fig. 5(a).
Super-resolution with Common Randomness
Rate	4	6	8	10	12	14	16	18
MSE	0.0515	0.0457	0.0420	00394	0.0372	0.0353	0.0339	0.0324
Rate	20	22	24	26	28	30	32	-
MSE	0.0313	0.0300	0.0297	0.0285	0.0280	0.0277	0.0269	-
Super-resolution without Common Randomness
Rate	4	6	8	10	12	14	16	18
MSE	0.0558	0.0506	0.0463	00435	0.0415	0.0396	0.0383	0.0367
Rate	20	22	24	26	28	30	32	-
MSE	0.0351	0.0337	0.0331	0.0328	0.0315	0.0308	0.0300	-
Table 6: The detailed number of MSE distortion loss in Fig. 5(c).
Image Denoising with Common Randomness
Rate	12	18	24	30	36	42	48	54	60
MSE	0.0219	0.0195	0.0175	0.01634	00Γ54	0.0147	0.0138	0.0135	0.0129
Rate	66	72	78	84	90	96	102	108	114
MSE	0.0126	0.0123	0.0118	0.0117	0.0115	0.0112	0.0109	0.0107	0.0106
Image Denoising without Common Randomness
Rate	12	18	24	30	36	42	48	54	60
MSE	0.0230	0.0208	0.0189	0.0175	0.0165	0.0157	0.0151	0.0145	0.014
Rate	66	72	78	84	90	96	102	108	114
MSE	0.0137	0.0133	0.0130	0.0127	0.0123	0.0120	0.0118	0.0116	0.0114
Li = E[∣P — Y +k2] + λWι(pγ ,pγ +),	(71)
where Y + is the outputs of compression network. Note that, to guarantee the distribution of
reconstructed Y + is close to that of target images, we implement a penalty on the Wasserstein-1
distance in (70) and (71). For the image super-resolution, we experimentally selected λ1 = 0.05 and
λ2 = 0.01. For the image denoising, we experimentally selected λ1 = 0.03 and λ2 = 0.005. Once
the compression network is converged, we report the final MSE distortion between Y + and X using
E[kX - Y +k2].
The detailed results for entropy-constrained image super-resolution and denoising are respectively
shown in Table 7 and Table 8. It can easily check throughout the tables that our end-to-end systems
outperform the baselines.
B.6	Comparison with Ground Truth
In order to illustrate the rate-distortion trade-offs, we report the MSE distortion that is measured
between degraded input images and decoder outputs in Figure 5. Since the input and output distribu-
tions are different, we do not expect MSE → 0 as the rate increases. The MSE distortion between
degraded input and restored output is still able to reveal how much content information of the input is
preserved in output (lower is better).
We now additionally show the MSE distortion between ground truth and decoder outputs in Tables 9
and 10. Concretely, We measure MSE distortion E[∣∣Y - Y∣∣2], where Y is ground truth and Y is
the network output. Note that for training, the ground truth is only used in an unsupervised fashion
31
Published as a conference paper at ICLR 2022
Table 7: Comparison between our end-to-end system with the baseline method for image super-
resolution. Numbers are the MSE distortion loss for a particular rate. Best results are in bold.
Super-resolution with Common Randomness
Rate	4	6	8	10	12	14	16	18
Baseline	0.0603	0.0568	0.0544	0.0530	0.0523	0.0511	0.0503	0.0498
Ours	0.0515	0.0457	0.0420	0.0394	0.0372	0.0353	0.0339	0.0342
Rate	20	22	24	26	28	30	32	-
Baseline	0.0489	0.0485	0.0484	0.0482	0.0478	0.0476	0.0471	-
Ours	0.0313	0.0300	0.0297	0.0285	0.0280	0.0277	0.0269	-
Super-resolution without Common Randomness
Rate	4	6	8	10	12	14	16	18
Baseline	0.0620	0.0585	0.0573	0.0555	0.0543	0.0535	0.0532	0.0520
Ours	0.0558	0.0506	0.0463	0.0435	0.0415	0.0396	0.0383	0.0367
Rate	20	22	24	26	28	30	32	-
Baseline	0.0517	0.0512	0.0506	0.0505	0.0497	0.0495	0.0490	-
Ours	0.0351	0.0337	0.0331	0.0328	0.0315	0.0308	0.0300	-
Table 8: Comparison between our end-to-end system with the baseline method for image denoising.
Numbers are the MSE distortion loss for a particular rate. Best results are in bold.
Image Denoising with Common Randomness
Rate	12	18	24	30	36	42	48	54	60
Baseline	0.0242	0.0213	0.0189	0.0173	0.0162	0.0154	0.0148	0.0142	0.0136
Ours	0.0219	0.0195	0.0175	0.0163	0.0154	0.0147	0.0138	0.0135	0.0129
Rate	66	72	78	~84^^	90	96	102	108	114
Baseline	0.0134	0.0130	0.0127	0.0124	0.0120	0.0118	0.0116	0.0111	0.0110
Ours	0.0126	0.0123	0.0118	0.0117	0.0115	0.0112	0.0109	0.0107	0.0106
Image Denoising without Common Randomness
Rate	12	18	24	30	36	42	48	54	60
Baseline	0.0252	0.0216	0.0202	0.0185	0.0178	0.0164	0.0158	0.0154	0.0149
Ours	0.0230	0.0208	0.0189	0.0175	0.0165	0.0157	0.0151	0.0145	0.014
Rate	66	72	78	84	90	96	102	108	114
Baseline	0.0147	0.0144	0.0140	0.0135	0.0133	0.0129	0.0126	0.0120	0.0117
Ours	0.0137	0.0133	0.0130	0.0127	0.0123	0.0120	0.0118	0.0116	0.0114
with unpaired noisy images, and here the ground truth-noisy image pairs are only used for test time
evaluation. Note also that the MSE distortion is correspondingly lower if common randomness is
adopted.
Table 9: Illustration of MSE distortion between network outputs and ground truth for super-resolution.
Super-resolution with Common Randomness
Rate	4	8	12	16	20	24	28	32
MSE	0.0582	0.0464	0.0408	0.0380	0.0360	0.0353	0.0348	0.0343
Super-resolution without Common Randomness								
Rate	4	8	12	16	20	24	28	32
MSE	0.0646	0.0492	0.0426	0.0390	0.0375	0.0362	0.0353	0.0345
32
Published as a conference paper at ICLR 2022
Table 10: Illustration of MSE distortion between network outputs and ground truth for image
denoising.
Image Denoising with Common Randomness
Rate	12	24	36	48	60	72	84	96	108
MSE	0.0157	0.0114	0.0092	0.0077	0.0069	0.0062	0.0056	0.0052	0.0047
Image Denoising without Common Randomness									
Rate	12	24	36	48	60	72	84	96	108
MSE	0.0169	0.0128	0.0104	0.0090	0.0080	0.0072	0.0066	0.0060	0.0057
B.7	Breakdown of the Table 1
It is worth reminding that the each number in Table 1 is the total distortion 18. Here, we provide
∕~∙J
a breakdown of total distortion in each of the three term for joint training, i.e, E[kX - Y1 k2],
E[kY - Y2k2] andE[kY -句2].
B.8	Network Architecture
Super-resolution. The detailed network structure for end-to-end model and two-branch model are
respectively presented in Table 12 and Table 13. The last linear layer of the encoder controls the
number of output symbols.
Denoising. The detailed network structure for end-to-end model and two-branch model are respec-
tively presented in Table 14 and Table 15. The last linear layer of the encoder controls the number of
output symbols.
Table 11: Breakdown of the Table 1. At any rate, it can be observed that the total losses of our
approximation system (L2 ) are very close to that of end-to-end learning system under the setting
∕~∙J
without common randomness (E[kX - Y k2 ]).
Image Super-resolution Using Joint Training
Rate	∕~∙j E[kX -	Yik2]	E[kY -讪	EIYLZ %k2]	L2	∕~∙j E[kX - Yk2]
4	0.0355	0.0227	0.0004	0.0586	0.0558
10	0.0223	0.0222	0.0008	0.0453	0.0435
20	0.0136	0.0191	0.00013	0.0349	0.0351
30	0.00122	0.0172	0.0015	0.0309	0.0308
Image Denoising Using Joint Training
Rate	E[kX 二 Yik2]	E[kY - Y2k2]	E[kYi- Y2k2]	L2	E[kX 二 Y k2]
12	0.01911	0.00497	0.00020	0.02428	0.02302
30	0.01458	0.00435	0.00026	0.01919	0.01746
60	0.01168	0.00378	0.00032	0.01578	0.01401
90	0.01035	0.00323	0.00028	0.01386	0.01229
33
Published as a conference paper at ICLR 2022
Table 12: Model architectures of end-to-end network used in super-resolution.
Encoder
Input
Conv2D,l-ReLU
Conv2D,l-ReLU
Flatten
Linear, I-ReLU
Linear, I-ReLU
Linear, Tanh
Quantizer
Decoder
Input
Linear, BatchNorm1D, l-ReLU
Linear, BatchNorm1D, l-ReLU
Unflatten
ConvT2D, BatchNorm2D,l-ReLU
ConvT2D, BatchNorm2D,l-ReLU
ConvT2D, BatchNorm2D, Sigmoid
Critic
Input
Conv2D,l-ReLU
Conv2D,l-ReLU
Conv2D,l-ReLU
Linear
Table 13: Model architectures of two-branch network used in super-resolution.
Encoder
Input
Conv2D,l-ReLU
Conv2D,l-ReLU
Flatten
Linear, l-ReLU
Linear, l-ReLU
Linear, Tanh
Quantizer
Decoder1 and 2
Input
Linear, BatchNorm1D, l-ReLU
Linear, BatchNorm1D, l-ReLU
Unflatten
ConvT2D, BatchNorm2D,l-ReLU
ConvT2D, BatchNorm2D,l-ReLU
ConvT2D, Sigmoid
Table 14: Model architectures of end-to-end network used in image denoising.
Encoder
Input
Conv2D, l-ReLU
Conv2D, l-ReLU
Conv2D, l-ReLU
Flatten
Linear, Tanh
Quantizer
Decoder
Input
Linear, BatchNorm1D, l-ReLU
Linear, BatchNorm1D, l-ReLU
Unflatten
ConvT2D, BatchNorm2D, l-ReLU
ConvT2D, BatchNorm2D,l-ReLU
ConvT2D, BatchNorm2D, l-ReLU
ConvT2D, BatchNorm2D, Sigmoid
Critic
Input
Conv2D, l-ReLU
Conv2D, l-ReLU
Conv2D, l-ReLU
Linear
Table 15: Model architectures of two-branch network used in image denoising. ResBlock is formed
using two Conv2D and skip connection.
Encoder
Input
Conv2D,l-ReLU
Conv2D,l-ReLU
Conv2D,l-ReLU
Flatten
Linear, Tanh
Quantizer
Decoder1 and 2
Input
Linear, BatchNorm1D, l-ReLU
Linear, BatchNorm1D, l-ReLU
Unflatten
ConvT2D, l-ReLU
ResBlock, ConvT2D,l-ReLU
ResBlock, ConvT2D,l-ReLU
ResBlock, ConvT2D, Sigmoid
34