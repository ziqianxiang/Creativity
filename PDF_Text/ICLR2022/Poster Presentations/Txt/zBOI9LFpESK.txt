Published as a conference paper at ICLR 2022
Learning Generalizable Representations for
Reinforcement Learning via Adaptive Meta-
learner of B ehavioral Similarities
Jianda Chen & Sinno Jialin Pan
Nanyang Technological University, Singapore
jianda001@e.ntu.edu.sg, sinnopan@ntu.edu.sg
Ab stract
How to learn an effective reinforcement learning-based model for control tasks
from high-level visual observations is a practical and challenging problem. A key to
solving this problem is to learn low-dimensional state representations from observa-
tions, from which an effective policy can be learned. In order to boost the learning
of state encoding, recent works are focused on capturing behavioral similarities
between state representations or applying data augmentation on visual observations.
In this paper, we propose a novel meta-learner-based framework for representation
learning regarding behavioral similarities for reinforcement learning. Specifically,
our framework encodes the high-dimensional observations into two decomposed
embeddings regarding reward and dynamics in a Markov Decision Process (MDP).
A pair of meta-learners are developed, one of which quantifies the reward similarity
and the other quantifies dynamics similarity over the correspondingly decomposed
embeddings. The meta-learners are self-learned to update the state embeddings by
approximating two disjoint terms in on-policy bisimulation metric. To incorporate
the reward and dynamics terms, we further develop a strategy to adaptively balance
their impacts based on different tasks or environments. We empirically demonstrate
that our proposed framework outperforms state-of-the-art baselines on several
benchmarks, including conventional DM Control Suite, Distracting DM Control
Suite and a self-driving task CARLA.
1	Introduction
Designing effective reinforcement learning algorithms for learning to control from high-dimensional
visual observations is crucial and has attracted more and more attention (Yarats et al., 2021; Laskin
et al., 2020a; Schwarzer et al., 2021; Lesort et al., 2018). To learn a policy efficiently from high-
dimensional observations, prior approaches first learn an encoder to map high-dimensional obser-
vations, e.g., images, to low-dimensional representations, and subsequently train a policy from
low-dimensional representations to actions based on various RL algorithms. Therefore, how to learn
low-dimensional representations, which are able to provide semantic abstraction for high-dimensional
observations, plays a key role.
Early works on deep reinforcement learning train encoders based on various reconstruction
losses (Lange & Riedmiller, 2010; Watter et al., 2015; Wahlstrom et al., 2015; Higgins et al.,
2017), which aim to enforce the learned low-dimensional representations to reconstruct the original
high-dimensional observations after decoding. Promising results have been achieved in some ap-
plication domains, such as playing video games, simulated tasks, etc. However, the policy learned
on state representations trained with a reconstruction loss may not generalize well to complex en-
vironments, which are even though semantically similar to the source environment. The reason
is that a reconstruction loss is computed over all the pixels, which results in all the details of the
high-dimensional images tending to be preserved in the low-dimensional representations. However,
some of the observed details, such as complex background objects in an image, are task-irrelevant
and highly environment-dependent. Encoding such details in representation learning makes the
learned representation less effective for a downstream reinforcement learning task of interest and less
generalizable to new environments. To address the aforementioned issue, some data augmentation
1
Published as a conference paper at ICLR 2022
techniques have been proposed to make the learned representations more robust (Yarats et al., 2021;
Lee et al., 2020b; Laskin et al., 2020b). However, these approaches rarely consider the properties of a
Markov Decision Process (MDP), such as conditional transition probabilities between states given an
action, in the representation learning procedure.
Recent research (Zhang et al., 2021; Agarwal et al., 2021; Castro, 2020; Ferns et al., 2004) has
shown that the bisimulation metric and its variants are potentially effective to be exploited to learn
a more generalizable reinforcement learning agent across semantically similar environments. The
bisimulation metric measures the “behavioral similarity” between two states based on two terms: 1)
reward difference that considers the difference in immediate task-specific reward signals between two
states, and 2) dynamics difference that considers the similarity of the long-term behaviors between
two states. A general idea of these approaches is to learn an encoder to map observations to a latent
space such that the distance or similarity between two states in the latent space approximates their
bisimulation metric. In this way, the learned representations (in the latent space) are task-relevant
and invariant to environmental details. However, manually specifying a form of distance, e.g., the L1
norm as used in (Zhang et al., 2021), in the latent space may limit the approximation precision for the
bisimulation metric and potentially discard some state information that is useful for policy learning.
Moreover, existing approaches rarely explore how to learn an adaptive combination of reward and
dynamics differences in the bisimulation metric, which may vary in different tasks or environments.
We propose a novel framework for learning generalizable state representations for RL, named Adap-
tive Meta-learner of Behavioral Similarities (AMBS). In this framework, we design a network with
two encoders that map the high-dimensional observations to two decomposed representations regard-
ing rewards and dynamics. For the purpose of learning behavioral similarity on state representations,
we introduce a pair of meta-learners that learn similarities in order to measure the reward and the
dynamics similarity between two states over the corresponding decomposed state representations,
respectively. The meta-learners are self-learned by approximating the reward difference and the
dynamics difference in the bisimulation metric. Then the meta-learners update the state representa-
tions according to their behavioral distance to the other state representations. Previous approaches
with a hand-crafted form of distance/similarity evaluating state encoding in the L1 space are diffi-
cult to minimize the approximation error for the bisimulation metric, which may lead to important
side information being discarded, e.g., information regarding to Q-value but not relevant to states
distance/similarity. Instead, our learned similarities measure two states representations via a neural ar-
chitecture, where side information can be preserved for policy learning. Our experiments also showed
that a smaller approximation loss for similarity learning can be obtained by using the meta-learners.
This demonstrates that the proposed meta-learners can overcome the approximation precision issue
introduced by the L1 distance in previous approaches and provide more stable gradients for robust
learning of state representations for deep RL. Moreover, we explore the impact between the reward
and the dynamics terms in the bisimulation metric. We propose a learning-based adaptive strategy to
balance the effect between reward and dynamics in different tasks or environments by introducing a
learnable importance parameter, which is jointly learned with the state-action value function. Finally,
we use a simple but effective data augmentation strategy to accelerate the RL procedure and learn
more robust state representations.
The main contributions of our work are 3-fold: 1) we propose a meta-learner-based framework to
learn task-relevant and environment-details-invariant state representations; 2) we propose a network
architecture that decomposes each state into two different types of representations for measuring the
similarities in terms of reward and dynamics, respectively, and design a learnable adaptive strategy
to balance them to estimate the bisimulation metric between states; 3) we verify our proposed
framework on extensive experiments and demonstrate new state-of-the-art results on background-
distraction DeepMind control suite (Tassa et al., 2018; Zhang et al., 2018; Stone et al., 2021) and
other visual-based RL tasks.
2	Related Work
Representation learning for reinforcement learning from pixels Various existing deep RL
methods have been proposed to address the sample-efficiency and the generalization problems for
conventional RL from pixel observation. In end-to-end deep RL, neural networks learn representations
implicitly by optimizing some RL objective (Mnih et al., 2015; Espeholt et al., 2018). Watter et al.
2
Published as a conference paper at ICLR 2022
(2015) and Wahlstrom et al. (2015) proposed to learn an encoder with training a dynamics model
jointly to produce observation representations. Lee et al. (2020a), Hafner et al. (2019), Hafner et al.
(2020) and Zhang et al. (2019) aimed to learn an environment dynamics model with the reconstruction
loss to compact pixels to latent representations. Gelada et al. (2019) proposed to learn representations
by predicting the dynamics model along with the reward, and analyzed its theoretical connection to
the bisimulation metric. Agarwal et al. (2021) proposed to learn representations by state distances
based on policy distribution. Kemertas & Aumentado-Armstrong (2021) modified the learning of
bisimulation metric with intrinsic rewards and inverse dynamics regularization. Castro et al. (2021)
replaced the Wasserstein distance in bisimulation metric with a parametrized metric.
Data Augmentation in reinforcement learning Laskin et al. (2020b) and Ye et al. (2020b) explored
various data augmentation techniques for deep RL, which are limited to transformations on input
images. Cobbe et al. (2019) applied data augmentation to domain transfer. Ye et al. (2020a) studied
data augmentation on game environments for zero-shot generalization. RandConv (Lee et al., 2020b)
proposed a randomized convolutional neural network to generate randomized observations in order to
perform data augmentation. A recent work DrQ (Yarats et al., 2021) performs random crop on image
observations and provides regularized formulation for updating Q-function. Raileanu et al. (2021)
proposed to use data augmentation for regularizing on-policy actor-critic RL objectives. Pitis et al.
(2020) proposed a counterfactual data augmentation technique by swapping observed trajectory pairs.
3	Preliminaries
We define an environment as a Markov Decision Process (MDP) described by a tuple M =
(S, A, P, R, γ), where S is the high-dimensional state space (e.g., images), A is the action space,
P(s0|s, a) is the transition dynamics model that captures the probability of transitioning to next
state s0 ∈ S given current state s ∈ S and action a ∈ A, R is the reward function yielding
a reward signal r = R(s, a) ∈ R, and γ ∈ [0, 1) is the discounting factor. The goal of rein-
forcement learning is to learn a policy ∏(a∣s) that maximizes the expected cumulative rewards:
max∏ E[Pt Ytr(st, at)∣at 〜∏(∙∣st), St 〜P (s0∣s, a)]. Inthe scope of this paper, We do not consider
partial observability, and use stacked consecutive frames as the fully observed states.
Soft Actor-Critic (SAC) is a widely used off-policy model-free reinforcement learning al-
gorithm (Haarnoja et al., 2018). SAC aims to maximize the entropy objective (Ziebart,
2010) which is the reinforcement learning objective augmented with an entropy term J(π) =
Pt E [r(st, at) + αH (∏(∙∣St))]. In order to maximize the objective, SAC learns a state-value func-
tion Qθ (s, a), a stochastic policy ∏ψ (a|s) and the temperature a, where θ and ψ are the parameters of
Qθ and πψ , respectively. The Q-function is trained by minimizing the squared soft Bellman residual
JQ(G= E(st,at)〜D 2 (Qθ (St, at) - (r(st, at) + YEst+ 1 〜P(∙∣st,at) [V⅛ (St+1)] )) ?,	⑴
where Dis the dataset or replay buffer storing the transitions, and Vq is the value function parameter-
ized by θ. The parameters ψ of policy is learned by maximizing the following objective
J∏(ψ)= Est〜D [Eat〜∏ψ [αlog(∏ψ(a∣s)) - Qθ(st, at)]] .	(2)
The Bisimulation Metric defines a pseudometric d : S × S → R,1 where d quantifies the
behavioral similarity of two discrete states (Ferns et al., 2004). An extension to both continuous
and discrete state spaces has also been developed (Ferns et al., 2011). A variant of the bisimulation
metric proposed in Castro (2020) defines a metric w.r.t. a policy π, which is known as the on-policy
bisimulation metric. It removes the requirement of matching actions in the dynamics model but
focuses on the policy π, which is able to better capture behavioral similarity for a specific task.
Because of this property, in this paper, we focus on the π-bisimulation metric (Castro, 2020):
Fπ(d)(Si, Sj) = (1 - c)Rni- R∏j I + cWι(d)(Psi, Psj-).	(3)
where Rn := Pa π(a∣s)R(s, a) and Pn := Pa∏(a∣s)P(∙∣s, a). Themapping Fπ : met → met,
where met denotes the set of all pseudometrics in state space S and Wι(d)(∙, ∙) denotes the 1-
Wasserstein distance given the pseudometric d. Then Fn has a least fixed point denoted by d*. Deep
1If the pseudometric d of two states is 0, then the two states belong to an equivalence relation.
3
Published as a conference paper at ICLR 2022
Encoder
Encoder
Φ
ISf 节V
°d(s)→∣ Q(Net ∣→Q(s,a)
办(s)/
篇f 间
Figure 1: Architecture of our AMBS framework. The dotted arrow represents the regression target
and the dash arrow means stop gradient. Left: the learning process of meta-learner. Right: the model
architecture for SAC with adaptive weight c which is jointly learned with SAC objective.
Bisimulation for Control (DBC) (Zhang et al., 2021) is to learn latent representations such that the L1
distances in the latent space are equal to the π-bisimulation metric in the state space:
argmin (kΦS) - Φ(sj )kι — 火言 - R∏j I- γW2 (d)P∏ Rj),
where φ is the state encoder, and W2(d)(∙, ∙) denotes the 2-Wasserstein distance. DBC is combined
with the reinforcement learning algorithm SAC, where φ(s) is the input for SAC.
4	Adaptive Meta-learner of Behavioral Similarities
In this section, we propose a framework named Adaptive Meta-learner of Behavioral Similarities
(AMBS) to learn generalizable states representation regarding the π-bisimulation metric. The learning
procedure is demonstrated in Figure 1. Observe that the π-bisimulation metric is composed of two
terms: Rni — R∏j ∣, which computes the difference of rewards between states, and W2(d)(P∏i, P∏j),
which computes the difference of the outputs of dynamics model between states. We propose a
network architecture which contains encoders to transform the high-dimensional visual observations
to two decomposed encodings regarding rewards and dynamics. We develop two meta-learners,
one of which quantifies the reward difference on reward representations and the other captures
dynamics distance (Section 4.1). Each meta-learner is self-learned to update the corresponding state
representations by learning to approximate a term in the π-bisimulation metric, respectively. Rather
than enforcing the L1 distance between embedded states to be equal to the π-bisimulation metric,
our meta-learners are able to use a more flexible form of similarity, i.e., a well-designed non-linear
neural network, with each similarity evaluates the reward difference or dynamics difference between
states beyond the original Euclidean space. Moreover, we propose a strategy for learning to combine
the outputs of the two learned similarities in a specific environment (Section 4.2). We introduce a
learnable weight for the combination and such weight is adaptively learned together with the policy
learning procedure (in this work we use SAC as the base reinforcement learning algorithm). In
addition, we also use a data augmentation technique to make the learned representations more robust
(Section 4.3). The whole learning procedure is trained in an end-to-end manner.
4.1	Meta-learners for Decomposed Representations
We design a network architecture with two encoders, φr and φd , to encode decomposed features from
visual observations as shown in the left side of Figure 1. Each encoder maps a high-dimensional
observation to a low-dimensional representation, i.e., φr : S → Zr capturing reward-relevant features
and φd : S → Zd capturing dynamics-relevant features. We design two meta-learners, fr and fd,
where fr learns to measure reward difference and fd aims to measure dynamics difference between
two state representations. Specifically, each meta-learner takes two embedded states as input and
outputs the corresponding similarity. The procedure is summarized as follows.
Si,	Sj → φ*(si),φ*(sj) → f*(φ*(si),φ*(sj)), where * ∈ {r,d}.
Note that the aggregation of the outputs of the two encoders φr and φd of each observation (i.e.,
state)is also fed into SAC to learn a policy (the right side of Figure 1). Each self-learned meta-
learner learns to capture similarity by approximating a distance term in the π-bisimulation metric,
respectively, and provides stable gradients for updating φr or φd according to the distance to the other
state representations. The details of network architecture can be found in Appendix B.2.
4
Published as a conference paper at ICLR 2022
As discussed in Section 1, restricting the form of distance to be the L1/L2 norm in the latent space
may limit the approximation precision. As shown in Figure 2, the L1 and the L2 norm for measuring
the distance of two latent representations φ(si) and φ(sj ) lead to large regression losses during RL
training, which destabilize the representation learning and consequently decrease final performance.
Besides, such hand-crafted distances make semantically similar states encoding close to each other in
terms of the L1 norm in Euclidean space, which however may lose part of useful information, e.g.,
policy-relevant information but not immediately regarding to the distance.
To overcome the aforementioned limitations, we propose to exploit meta-learners fr and fd to learn
similarities. By learning With a regression target, the similarity learned by f* is easier to converge to
the target and leads to faster descent tendency of regression loss (as shown in Figure 2 where y-axis
is log-scale). Such a property provides more stable gradients for state representation comparing
to the L1/L2 norm, and therefore the meta-learner f* is able to guide the process of updating the
state encoder. Besides, f* is a non-linear transformation that evaluates the state similarity in a more
flexible space instead of the original Euclidean space. Consequently, it is able to preserve more
task-relevant information in state representation that is required for further SAC policy learning.
As the goal of meta-learners is to approximate different types of simi-
larities in the π-bisimulation metric, We design the loss functions for the
meta-learners using the mean squared error:
'(fr ,Φr) = fr(Φr(Si ),Φr(Sj)) -火以 -花^ |) 2 ,	(4)
'(fd,φd) = (fd(Φd(Si),Φd(Sj))- W2(P3P∏∙))2.	(5)
To make the resultant optimization problems compatible With SGD up- Figure 2: The regres-
dates With transitions sampled from replay buffer, We replace the reWard sion loss among different
metric term ∣R∏. - R∏. | by the distance of rewards in the sampled transi- forms of distances.
tions in (4), and use a learned parametric dynamics model P to approximate the true dynamics model
Psπ in (5). We also use 2-Wasserstein distance W2 in (5) (as in DBC) because it has a closed form for
Gaussian distributions. Denote by (Si, ai, ri, S0i) and (Sj, aj, rj, S0j) two transitions sampled from the
replay buffer. The loss functions (4) and (5) can be revised as follows,
'(fr ,Φr) = (fr(Φr(Si ),Φr (Sj)) Tri-町 |)2 ,	(6)
'(fd,φd) = (fd(φd(Si),φd(Sj)) - W2(P(∙∣Φd(Si), ai), P(∙∖φd(Sj), aj)))2,	(J)
where P(∙∣φd(s*), a*) is a learned probabilistic dynamics model, which is implemented as a neural
network that takes the dynamics representation φd(S*) and an action a* as input, and predicts the
distribution of dynamics representation of next state S0*. The details of P can be found Appendix A.
4.2	Balancing Impact of Reward and Dynamics
The π-bisimulation metric (3) is defined as a linear combination of the reward and the dynamics
difference. Rather than considering the combination weight as a hyper-parameter, which needs to be
tuned in advance, we introduce a learnable parameter c ∈ (0, 1) to adaptively balance the impact of
the reward and the dynamics in different environments and tasks.
As the impact factor c should be automatically determined when learning in a specific task or
environment, we integrate the learning of c into the update of the Q-function in SAC such that
the value of c is learned from the state-action value which is highly relevant to a specific task and
environment. To be specific, we concatenate the low-dimensional representations φr (S) and φd(S)
weighted by 1 - c and c, where 1 - c and c are output of a softmax to ensure c ∈ (0, 1). The loss for
the Q-function is revised as follows, which takes φr (S), φd (S), weight c and action a as input:
Jq(θ,c,φr,φd) = E(s,a,s0)〜D 1 (Qθ(φr(s),φd(s),c,a) - (r(S,a) + YV(s0)))2 ,	(8)
where Qθ is the Q-function parameterized by θ and V(∙) is the target value function.
Moreover, to balance the learning of the two meta-learners fr and fd , we jointly minimize the
regression losses of fr and fd ((6) and (7), respectively) with the balance factor c. Thus, the loss
5
Published as a conference paper at ICLR 2022
function is modified as follows,
'(Θ) = (1 - c)'(fr, φr) + c'(fd, φd), where Θ = {fr, fd, φr, φd}.
(9)
Note that c is already learned along with the Q-function, we stop gradient of c when minimizing
the loss (9). This is because if we perform gradient descent w.r.t. c, it may only capture which loss
(`(fr, φr) or `(fd, φd)) is larger/smaller and fail to learn the quantities of their importance. We also
stop gradient for the dynamics model P since P only predicts one-step dynamics.
4.3	Overall Objective with Data Augmentation
In this section, we propose to integrate a data augmentation strategy into our proposed framework
to learn more robust state representations. We follow the notations of an existing work on data
augmentation for reinforcement learning, DrQ (Yarats et al., 2021), and define a state transformation
h : S × T → S that maps a state to a data-augmented state, where T is the space of parameters
of h. In practice, we use a random crop as the transformation h in this work. Then T contains all
possible crop positions and v ∈ T is one crop position drawn from T. We apply DrQ to regularize
the objective of the Q-function (see Appendix for the full objective of Q-function).
Besides, by using the augmentation transformation h, we rewrite the loss of representation learning
and similarity approximation in (9) as follows,
'(θ) =(1 - c) (fr (φr (S(I)), φr (SjI))) - |ri - rj |)
+ C (fd (φd(SiI)),φd(sjI)))- W2 (P(∙∣φd(s(1)), ai), P(∙∣φd(sj1)), aj)))2
+ (1 - c) fr φr(S(j2)),φr(Si(2))) - |ri - rj|)2
+ c (fd (φd(sj1 2 3 4 5 6 7)),φd(s(2))) - W2 (P(∙∣Φd(S(I)), ai), P(∙∣Φd(sj1)), aj)))2
(10)
where SF) = h(s*, VF)) is the transformed
state with parameters VF). Specifically, s(1),
S(j1), Si(2) and S(j2) are transformed states with
(1)	(1)	(2)	(2)
parameters of Vi , Vj , Vi and Vj respec-
tively, which are all drawn from T indepen-
dently. The first two terms in (10) are similar
to the two terms in (9), while the observation
S* in (9) is transformed to S*1). For the last two
Algorithm 1 AMBS+SAC
1: Input: Replay Buffer D, initialized Θ = {fr,φr,fd,φd} , Q
network Qθ , actor piψ, target Q network Qg,
2: Sample a batch with size B: {(si, ai,ri, si)}B=ι 〜D.
3: Shuffle batch {(si, ai, ri, s0i)}bB=1 to {(sj,aj, rj, s0j)}bB=1.
4: update q network by (8) with drq augmentation.
5: update actor network by (11).
6: update alpha α.
7: update encoder Θ by (10)
8: update dynamics model P.
terms in (10), the observation is transformed by
a different parameter V*(2) except for the last term, where the parameter V*(1) does not change. The
reason why the parameter V*(1) is used in the last term is that we expect the meta-learner fd is able to
make a consensus prediction on S(*1) and S(*2) as they are transformed from a same state S*. Another
major difference between the first two terms and the last two terms is the order of the subscripts i and
j : i is followed by j in first two terms while j is followed by i in last two terms. The reason behind
this is that we aim to make fr and fd to be symmetric.
The loss of the actor of SAC, which is based on the output of the encoder, is
Jn (ψ) = E(s,a)〜DIa log(∏ψ (a∣φ(S))) - Qθ (Φ(s), a)],	(11)
where φ denotes the convolutional layers of φ and Qθ(φ(S), a) is a convenient form of the Q-function
with c-weighted state representations input. We also stop gradients for φ and φ (φr and φd) regarding
the actor loss. In other words, the loss in (11) only calculates the gradients w.r.t. ψ.
The overall learning algorithm is summarized in Algorithm 1. Note that due to limit of space, more
details can be found in Appendix A. In summary, as illustrated in Figure 1, in AMBS, the convnet
is shared by the encoders φr and φd, which consists of 4 convolutional layers. The final output of
6
Published as a conference paper at ICLR 2022
the convnet is fed into two branches: 1) one is a fully-connected layer to form reward representation
φr (s), and 2) the other is also a fully-connected layer but forms dynamics representation φd(s). The
meta-learners fr and fd are both MLPs with two layers. More implementation details are provided at
Appendix B. Source code is available at https://github.com/jianda-chen/AMBS.
5	Experiments
The major goal of our proposed AMBS is to learn a generalizable representation for RL from
high-dimensional raw data. We evaluate AMBS on two environments: 1) the DeepMind Control
(DMC) suite (Tassa et al., 2018) with background distraction; and 2) autonomous driving task on
CARLA (Dosovitskiy et al., 2017) simulator. Several baselines methods are compared with AMBS: 1)
Deep Bisimulation Control (DBC) (Zhang et al., 2021) which is recent research on RL representation
learning by using the L1 distance to approximate bisimulation metric; 2) PSEs (Agarwal et al., 2021)
which proposes a new state-pair metric called policy similarity metric for representation learning and
learns state embedding by incorporating a contrastive learning method SimCLR (Chen et al., 2020);
3) DrQ (Yarats et al., 2021), a state-of-the-art data augmentation method for deep RL; 4) Stochastic
Latent Actor-Critic (SLAC) (Lee et al., 2020a), a state-of-the-art approach for partial observability
on controlling by learning a sequential latent model with a reconstruction loss; 5) Soft Actor-Critic
(SAC) (Haarnoja et al., 2018), a commonly used off-policy deep RL method, upon which above
baselines and our approach are built.
5.1	DMC suite with background distraction
DeepMind Control (DMC) suite (Tassa et al., 2018) is an environment that provides high dimensional
pixel observations for RL tasks. DMC suite with background distraction (Zhang et al., 2018;
Stone et al., 2021) replaces the background with natural videos which are task-irrelevant to the RL
tasks. We perform the comparison experiments in two settings: original background and nature
video background. For each setting we evaluate AMBS and baselines in 4 environments, cartpole-
swingup, finger-spin, cheetah-run and walker-walk. Results of additional environments are shown in
Appendix D.
Figure 3: (a) Pixel observations of DMC suite with original background. (b) Pixel observations of
DMC suite with natural video background. Videos are sampled from Kinetics (Kay et al., 2017).
Original Background. Figure 3a shows the DMC observation of original background which is
static and clean. The experiment result is shown in Figure 4. Our method AMBS has comparable
learning efficiency and converge scores comparing to state-of-the-art pixel-level RL methods DrQ
and SLAC. Note that DBC performs worse in original background setting.
Figure 4: Training curves of AMBS and comparison methods. Each curve is average on 5 runs.
7
Published as a conference paper at ICLR 2022
Figure 5: Training curves of AMBS and comparison methods on natural video background setting.
Videos are sampled from Kinetics (Kay et al., 2017) dataset. Each curve is average on 5 runs.
Natural Video Background. Figure 3b shows the DMC observation with natural video background.
The background video is sampled from the Kinetics (Kay et al., 2017) dataset under label “driving
car”. By following the experimental configuration of DBC, we sample 1,000 continuous frames as
background video for training, and sample another 1,000 continuous frames for evaluation. The
experimental result is shown in Figure 5. Our method AMBS outperforms other baselines in terms
of both efficiency and scores. It converges to the scores that are similar to the scores on original
background setting within 1 million steps. DrQ can only converge to the same scores on cartpole-
swingup and finger-spin but the learning is slightly slower than AMBS. The sequential latent model
method SLAC performs badly on this setting. This experiment demonstrates that AMBS is robust to
the background task-irrelevant information and the AMBS objectives improve the performance when
comparing to other RL representation learning methods, e.g., DBC.
5.2	Ablation Study
Our approach AMBS consists of several major components: meta-learners, a factor to balance the
impact between reward and dynamics, and data augmentation on representation learning. To evaluate
the importance of each component, we eliminate or replace each component by baseline methods.
We conduct experiments on cheetah-run and walker-walk under the natural video background setting.
Figure 6 shows the performance of AMBS, variants of AMBS and baseline methods. AMBS w/o
data aug is constructed by removing data augmentation in AMBS. AMBS w/o f is referred to as
replacing the meta-learners fr and fd by the L1 distance, as in DBC. AMBS w/o c removes the
component of balancing impact between reward and dynamics but encodes the observation into
a single embedding. It uses single meta-learner to approximate the sum of reward and dynamics
distance. DBC + DrQ is DBC with data augmentation applied on representation learning and SAC.
AMBS c = 0.5 denotes AMBS with a fixed weight c = 0.5. AMBS w/ (1, γ) replaces c in AMBS
by freezing the reward weight to 1 and the dynamics weight to γ. Such a setting of weights is used
in DBC. Figure 6 demonstrates that AMBS performs better than any of its variant. Comparing
to DBC that does not utilize data augmentation, our variant AMBS w/o data aug still performs
better. This comparison shows that using meta-learners fr and fd to learn reward- and dynamics-
relevant representations is able to improve RL performance compared with using the L1 distance to
approximate the whole π-bisimulation metric.
5.3	Transfer over Reward Functions
The environments walker-walk, walker-run and walker-stand share the same dynamics but have
different reward functions according to the moving speed. To evaluate the transfer ability of AMBS
over reward functions, we transfer the learned model from walker-walk to walker-run and walker-
stand. We train agents on walker-run and walker-stand with frozen convolutional layers of encoders
that are well trained in walker-walk. The experiments are done under the natural video setting
Figure 6: Ablation Study on cheetah-run (left)
and walker-walk (right) in natural video setting.
Figure 7:	Transfer from walker-walk to (left)
walker-run and (right) walker-stand.
8
Published as a conference paper at ICLR 2022
compared with DBC as shown in Figure 7. It shows that the transferring encoder converges faster
than training from scratch. Besides our approach has a better transfer ability than DBC.
5.4 Generalization over Background Video
We evaluate the generalization ability of our method when background video is changed. We follow
the experiment setting of PSE (Agarwal et al., 2021)n. We pick 2 videos, ‘bear’ and ‘bmx-bumps’,
from the training set of DAVIS 2017 (Pont-Tuset et al., 2018) for training. We randomly sample
one video and a frame at each episode start, and then play the video forward and backward until
episode ends. The RL agent is evaluated on validation environment where the background video is
sampled from validation set of DAVIS 2017. Those validation videos are unseen during the training.
The evaluation scores at 500K environment interaction steps is shown in Table 1. The comparison
method DrQ+PSEs is data augmented by DrQ. AMBS outperforms DrQ+PSEs on all the 4 tasks. It
demonstrates that AMBS is generalizable on unseen background videos.
Methods	C-SwinguP	F-SPin	C-run	W-walk
DrQ + PSEs	749 土 19	779 ± 49	308 ± 12	789 ± 28
AMBS	807 ± 41	933 ± 96	332 ± 27	893 ± 85
Table 1: Generalization to unseen background videos sampled from DAVIS 2017 (Pont-Tuset et al.,
2018). Scores of DrQ+PSEs are reported from Agarwal et al. (2021).
(a)	(b)
Figure 8:	(a) Illustration of a third-person view in “Town4” scenario of CARLA. (b) A first-person
observation for RL agent. It concatenates five cameras for 300 degrees view .
5.5 Autonomous Driving Task on CARLA
CARLA is an autonomous driving simulator that provides a 3D
simulation for realistic on-road scenario. In real world cases
or in realistic simulation, the learning of RL agents may suffer
from complex background that contains task-irrelevant details.
To argue that AMBS can address this issue, we perform exper-
iments with CARLA. We follow the setting of DBC to create
an autonomous driving task on map “Town04” for controlling a
car to drive as far as possible in 1,000 frames. The environment
Figure 9: CARLA simulation.
contains a highway with 20 moving vehicles. The reward function prompts the driving distance
and penalizes collisions. The observation shape is 84 × 420 pixels which consists of five 84 × 84
cameras. Each camera has 60 degrees view, together they produce a 300 degrees first-person view, as
shown in Figure 8. The weather, clouds and brightness may change slowly during the simulation.
Figure 9 shows the training curves of AMBS and other comparison methods. AMBS performs the
best, which provides empirical evidence that AMBS can potentially generalize over task-irrelevant
details, e.g., weather and clouds, in real world scenarios.
6 Conclusion
This paper presents a novel framework AMBS with meta-learners for learning task-relevant and
environment-detail-invariant state representation for deep RL. In AMBS, we propose state encoders
that decompose an observation into rewards and dynamics representations for measuring behavioral
similarities by two self-learned meta-learners, and design a learnable strategy to balance the two
types of representations. AMBS archives new state-of-the-art results in various environments and
demonstrates its transfer ability on RL tasks. Experimental results verify that our framework is able
to effectively learn a generalizable state representation.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work is supported by Microsoft Research Asia collaborative research grant 2020.
References
Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=qda7-sVg84.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov
decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10069-
10076, 2020.
Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. Mico: Improved rep-
resentations via sampling-based state similarity for markov decision processes. In A. Beygelzimer,
Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing
Systems, 2021. URL https://openreview.net/forum?id=wFp6kmQELgu.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daume In and Aarti Singh (eds.), Proceedings
of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 1597-1607. PMLR, 13-18 Jul 2020. URL http://proceedings.
mlr.press/v119/chen20j.html.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization
in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 1282-1289. PMLR, 09-15 Jun 2019. URL http://proceedings.
mlr.press/v97/cobbe19a.html.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:
An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning,
pp. 1-16, 2017.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron,
Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable
distributed deep-RL with importance weighted actor-learner architectures. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 1407-1416. PMLR, 10-15 Jul 2018.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In
Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI ’04, pp. 162-169,
Arlington, Virginia, USA, 2004. AUAI Press. ISBN 0974903906.
Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov
decision processes. SIAM J. Comput., 40(6):1662-1714, December 2011. ISSN 0097-5397. doi:
10.1137/10080484X. URL https://doi.org/10.1137/10080484X.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. DeepMDP:
Learning continuous latent space models for representation learning. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2170-2179. PMLR,
09-15 Jun 2019. URL http://proceedings.mlr.press/v97/gelada19a.html.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1861-1870. PMLR, 10-15 Jul 2018. URL
http://proceedings.mlr.press/v80/haarnoja18b.html.
10
Published as a conference paper at ICLR 2022
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 2555-2565. PMLR, 09-15 JUn
2019. URL http://proceedings.mlr.press/v97/hafner19a.html.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=S1lOTC4tDS.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving zero-shot transfer
in reinforcement learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1480-1490. PMLR, 06-11 Aug 2017. URL http://proceedings.mlr.
press/v70/higgins17a.html.
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,
Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman.
The kinetics human action video dataset. CoRR, abs/1705.06950, 2017.
Mete Kemertas and Tristan Ty Aumentado-Armstrong. Towards robust bisimulation metric learning.
In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems, 2021. URL https://openreview.net/forum?id=
ySFGlFjgIfN.
Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning.
In The 2010 International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2010.
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised repre-
Sentations for reinforcement learning. In Hal Daume In and Aarti Singh (eds.), Proceed-
ings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 5639-5650. PMLR, 13-18 Jul 2020a. URL http:
//proceedings.mlr.press/v119/laskin20a.html.
Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19884-19895. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.
cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf.
Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 741-752. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/
paper/2020/file/08058bf500242562c0d031ff830ad094-Paper.pdf.
Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique
for generalization in deep reinforcement learning. In International Conference on Learning
Representations, 2020b. URL https://openreview.net/forum?id=HJgcvJBFvB.
Timothee Lesort, Natalia Dlaz-Rodriguez, Jean-Franois Goudou, and David Filliat. State representa-
tion learning for control: An overview. Neural Networks, 108:379-392, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.
11
Published as a conference paper at ICLR 2022
Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a
lower discount factor. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou
(eds.), Advances in Neural Information Processing Systems, volume 21. Curran Asso-
ciates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2008/file/
08c5433a60135c32e34f46a71175850c- Paper.pdf.
Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally
factored dynamics. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 3976-3990. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
294e09f267683c7ddc6cc5134a7e68a8- Paper.pdf.
Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and
Luc Van Gool. The 2017 davis challenge on video object segmentation. CoRR, abs/1704.00675,
2018.
Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data
augmentation for generalization in deep reinforcement learning, 2021.
Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach-
man. Data-efficient reinforcement learning with self-predictive representations. In International
Conference on Learning Representations, 2021.
Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite -
a challenging benchmark for reinforcement learning from pixels. CoRR, abs/2101.02722, 2021.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller.
Deepmind control suite. CoRR, abs/1801.00690, 2018.
Niklas Wahlstrom, Thomas B. Schon, and Marc Peter Deisenroth. From pixels to torques: Policy
learning with deep dynamical models. CoRR, abs/1502.02251, 2015.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/
paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf.
Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=GY6-6sTvGaf.
Chang Ye, Ahmed Khalifa, Philip Bontrager, and Julian Togelius. Rotation, translation, and cropping
for zero-shot generalization. In 2020 IEEE Conference on Games (CoG), pp. 57-64, 2020a. doi:
10.1109/CoG47356.2020.9231907.
Chang Ye, Ahmed Khalifa, Philip Bontrager, and Julian Togelius. Rotation, translation, and cropping
for zero-shot generalization. In 2020 IEEE Conference on Games (CoG), pp. 57-64, 2020b. doi:
10.1109/CoG47356.2020.9231907.
Amy Zhang, Yuxin Wu, and Joelle Pineau. Natural environment benchmarks for reinforcement
learning. CoRR, abs/1811.06032, 2018.
Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=-2FCwDKRREu.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine. SO-
LAR: Deep structured representations for model-based reinforcement learning. In Kamalika Chaud-
huri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7444-7453. PMLR, 09-15
Jun 2019. URL http://proceedings.mlr.press/v97/zhang19m.html.
12
Published as a conference paper at ICLR 2022
Brian D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
Entropy. PhD thesis, USA, 2010. AAI3438449.
13
Published as a conference paper at ICLR 2022
A Objectives of Adaptive Meta-learner of B ehavioral
Similarities (AMBS)
We denote two encoders φr and φd, where each encoder maps a high-dimensional observation to a
low-dimensional representation. The encoder φr : S → Zr captures reward-relevant features and
the encoder φd : S → Zd captures dynamics-relevant features, where Zr and Zd are representation
spaces. We design two meta-learners fr : Zr × Zr → R and fd : Zd × Zd → R to output reward
difference and dynamics difference, respectively. We follow DrQ (Yarats et al., 2021) to define the
state transformation h : S × T → S that maps a state to a data-augmented state, where T is the
space of parameters of h. In practice, we use random crop as transformation h.
With augmentation transformation h, the loss of representation learning is,
以⑼=(I-C) (fr(Or(S(I)),φr(SjI)))Tri -rj|)
+ C fd(φd(s(1)),φd(sj1)))- W2(P(∙∣Φd(si1)), ai), P(∙∣φd(s(1)), aj)))2
+ (1 - c) fr(φr(S(j2)),φr(Si(2))) - |ri - rj|2
+ c fd(Φd(sj2)), Φd(s(2))) — W2(P(∙∣φd(si1)), ai), P(∙∣φd(s”, aj )))2 ,
where Θ = {fr,fd, φr, φd}, S(I) = h(s*, VT)) is the transformed state with parameters v，l). SPecif-
ically, Si(1), Sj(1), Si(2) and S(j2) are transformed states with parameters of vi(1), vj(1), vi(2) and v(j2)
respectively, which are all drawn from T independently. P(∙∣φd(s(), a() is a learned probabilistic
dynamics model, which outputs a Gaussian distribution over Zd for predicting dynamics represen-
tation of next state S0( . W2 denotes the 2-Wasserstein distance which has closed-form for Gaussian
distributions: W2(N(μi,∑i),N(μj,∑j))2 = kμi - μj∙∣∣2 + k∑i2 - Σj IlF, where μ* ∈ R1Zd1 are
the mean vectors of Gaussian distribution, Σ( ∈ R|Zd| are the diagonals of covariance matrices, and
k ∙ ∣∣f is Frobenius norm. Note that we stop gradients for C as mentioned in Section 4.2.
The adaptive weight C and (1 - C) are the softmax output of corresponding learnable parameter
ηc ∈ R2 . It can be formulated as
[C : (1 - C)] = sof tmax(ηc).	(13)
where the operation : denotes concatenation of two vectors/scalars,
We integrate the learning of C into the update of Q-function of SAC. The objective of the Q-function
with DrQ augmentation and clipped double trick is
12
jQ(θk,ηc) = E(s,a,s0)〜D 2 (Qθk([(1 - c)φr(s⑴)：cφd(s⑴)],a) — (r(s, a) + YV(S )))
+2 (Qθk ([(1-c)φr (S⑵)：cφd(s ⑵)],a) - (r(s, a)+ YV (s0))) ], (14)
where S(l) = h(S, v(l)), v(l) are all drawn from T independently, and {Qθk }2k=1 are the double Q
networks. The target value function with DrQ augmentation is
V(s0) = 1 (min Qθk f[(1 - e)φr(S0(I)) : cφd(S0(1))], ao(1)) + α log∏ψ(a0(1) |so(1))
2 k=1,2
+ min Qθk ([(1 - C)Φr(s0(2))：由d(S0⑵)],a0⑵)+ αlog∏ψ(a0(2)|so(2))
k=1,2	k
(15)
where SO(I) = h(S, vo(l)), a0(I)〜∏ψ (a0(l)∣S0(l)), Gk is the set of parameters of the target Q network,
φr and φd are target encoders specifically for the target Q network and G is the adaptive weight for
target encoders. The parameters θGk, φGr, φGd and ηGc are softly updated.
The loss of actor of SAC is,
Jπ(ψ)= E(S)〜D
Ea〜∏ψ[αlog(∏ψ(a∣φ(S(I))))- min Qθk(Φ(s⑴)，a)]
k=1,2
(16)
14
Published as a conference paper at ICLR 2022
The loss of α of SAC is,
J(α)
E(S)〜D 忸a〜∏ψ [αlog(∏ψ(a∣φ(s⑴)))-αH]]
(17)
where H ∈ R is the target entropy hyper-parameter which in practice is H= -|A|.
The loss for updating of dynamics model P is
'(P) = E(s,a,s0)〜D
l^ φφd(s0⑴)-μ(P(∙∣Φd(s⑴),a))
N -2σ(P(∙∣φd(s(%, a))
(18)
where μ(P(∙)) and σ(P(∙)) are mean and standard deviation of P output GaUSSian distribution,
respectively.
Algorithm 2 shows the algorithm at each learning step.
Algorithm 2 AMBS+SAC
1:	Input: Replay Buffer D, initialized Θ = {fr, φr, fd, φd} , Q network Qθk, actor piψ, target Q
network Q%,
2:	Sample a batch with size B: {(si, ai,ri, si)}B=ι 〜D.
3:	Shuffle batch {(si,ai,ri,s0i)}bB=1 to {(sj,aj,rj,s0j)}bB=1.
4:	Update Q network by equation 14 with DrQ augmentation.
5:	Update actor network by equation 16 .
6:	Update alpha α by equation 17 .
7:	Update encoder Θ by equation 12
8:	Update dynamics model P by equation 18.
9:	Softly update target Q network: θk = τQθk + (1 - τQ)θk .
10:	Softly update target encoder: φ = τφφ +(1 - τφ)φ .
11:	Softly update target E η = τφη + (1 - τφ)η .
B	Implementation Details
B.1	Pixels Processing
We stack 3 consecutive frames as observation, where each frame is 84 × 84 RGB images in DeepMind
control suite. Each pixel is divided by 255 to down scale to [0, 1]. We consider the stacked frames as
fully observed state.
B.2	Network Architecture
We share the convnet for encoder φr and encoder φd. The shared convnet consists of four convolu-
tional layers with 3 × 3 kernels and 32 output channels. We set stride to 1 everywhere, except for the
first convolutional layer, which has stride 2. We use ReLU activation for each convolutional layer
output. The final output of convnet is fed into two branches: 1) one is a fully-connected layer with 50
dimensions to form reward representation φr(s), and 2) the other one is also a fully-connected layer
with 50 dimensions but form dynamics representation φd(s). The meta-learners fr and fd are both
MLPs with two layers with 50 hidden dimensions.
The critic network takes φr (s), φd(s) and action as input, and feeds them into three stacked fully-
connected layers with 1024 hidden dimensions. The actor network takes the shared convnet output as
input, and feeds it into four fully-connected layers where the first layer has 100 hidden dimensions
and other layers have 1024 hidden dimensions. The dynamics model is MLPs with two layers with
512 hidden dimensions. ReLU activations are used in every hidden layer.
B.3	Hyperparameters
The hyperparameters used in our algorithm are listed in Table 2.
15
Published as a conference paper at ICLR 2022
Parameter name	Value
Replay buffer capacity	106
Discount factor γ	0.99
Minibatch size	128
Optimizer	Adam
Learning rate for α	10-4
Learning rate except α	5 X 10-4
Target update frequency	2
Actor update frequency	2
Actor log stddev bounds	[-10, 2]
τQ	0.01
τφ	0.05
Init temperature	0.1
Table 2: Hyperparameters used in our algorithm.
B.4	Action Repeat for DMC
We use different action repeat hyper-parameters for each task in DeepMind control suite, which are
listed in Table 3.
Task name	Action repeat
Cartpole Swingup	8
Finger Spin	2
Cheetah Run	4
Walker Walk	2
Reacher Easy	4
Ball In CUP Catch	4
Table 3: Action repeat used for each task in DeepMind control suite.
B.5	Source Codes
Source code of AMBS is available at https://github.com/jianda-chen/AMBS.
C Analysis
C.1 Regression Losses of Approximating Bisimulation Metric
We compare the regression losses of approximating the bisimulation metric over RL environment
steps among DBC and our AMBS. DBC uses the L1 norm to calculate the distances between two
states embeddings. AMBS performs meta-learners on state representations to measure the distance
with learned similarities. Figure 10 demonstrates that AMBS has smaller regression loss and also
faster descent tendency compared to DBC. Meta-learners in AMBS are able to provide more stable
gradients to update the state representations.
16
Published as a conference paper at ICLR 2022
Figure 10: The regression losses over RL environment steps among DBC and AMBS.
C.2 Visualizations of Combination Weight c
Figure 11 shows the values of combination weight c in AMBS trained on DMC with origin background
and Figure 11 is on DMC with video background setting. The values of c at 1M steps vary in different
tasks. The common trend is that they increase at the beginning of training. In original background
setting c changes slowly after the beginning. In natural video background setting , c has a fast drop
after the beginning. In the beginning, agent learns a large weight c for dynamics feature that may
boost the learning of RL agent. Then it drops for natural video background setting because it learns
that the video background is distracting the agent. Lower weight is learned for the dynamics features.
Figure 11: The value of combination weight c
over environment steps. AMBS is trained on
DMC on Original background.
Figure 12: The value of combination weight c
over environment steps. AMBS is trained on
DMC on video background.
C.3 Norms of State Embeddings
We record the L1 norms of reward representation φr (s) and dynamics representation φd(s) during
training. We record n1- ∣∣φr (s)|| ι for reward representation where nr is the number of dimensions of
φr (s), and nɪ- ∣∣φd (s)|| ι for dynamics representation where nr is the number of dimensions of φd (s).
Figure 13 shows the values averaged on each sampled training batch. Although the L1 norm of φd(s)
decreases during training, it converges around 0.4-0.5. The L1 norm of φr (s) also decreases in the
training procedure. It can verify that dynamics representation φd(s) will not converge to all zeros.
Figure 13: Norms of state embeddings: n- ∣∣φr (s)∣∣ι and nɪ- ∣∣φd (s)∣∣ι∙
17
Published as a conference paper at ICLR 2022
C.4 Sharing Encoders Between Actor and Critic
In AMBS, we share only the convolutional layers φ of encoders between actor and critic network.
We compare to sharing the whole encoders φr and φd in this experiment. Figure 14 shows the raining
curves on Walker-Walk with natural video background. Sharing full encoder φr and φd is worse than
sharing only CNN φ.
Figure 14: Training curves of different ways of sharing encoder. Experiments on Walker-Walk with
natural video background.
D Additional Experimental Results
D.1 Additional Result of DMC Suite with and w/o Background Distraction
Figure 15 shows the training curves on original background setting. Figure 16 shows the training
curves on Kinetics (Kay et al., 2017) video background setting.
environment steps	le6
environment steps	le6
Figure 15: Training curves of AMBS and comparison methods. Each curve is average on 3 runs.
environment steps	le6
18
Published as a conference paper at ICLR 2022

AMBS
Figure 16: Training curves of AMBS and comparison methods on natural video background setting.
Videos are sampled from Kinetics (Kay et al., 2017) dataset. Each curve is average on 3 runs.
walker walk
environment steps
reacher easy
environment steps




























D.2 Additional Result of Generalization over Background Video
Table 4 shows the scores on DAVIS video background setting comparing to PSEs (Agarwal et al.,
2021).
Methods ∣ C-swingup F-spin C-run W-Walk R-easy BiC-CatCh
DrQ + PSEs 749 ± 19
AMBS
807 ± 41
779±49 308± 12 789±28 955± 10 821 ± 17
933 ± 96 332 ± 27 893 ± 85 980 ± 11 944 ± 59
Table 4: Generalization with unseen background videos sampled from DAVIS 2017 (Pont-Tuset et al.,
2018) dataset. We evaluate the agents at 500K environment steps. Scores of DrQ + PSEs are reported
from Agarwal et al. (2021).
D.3 Additional Result on CARLA
The CARLA experiment is to learn an autonomous driving RL agent to avoid collisions with other
moving vehicles and drive as far as possible on a highway within 1000 frames. Those vehicles are
randomly generated, including vehicles’ types, colors and initial positions. We raise the number of
generated vehicles from 20 to 40 so that the road becomes more crowded for RL agent. Besides, we
also extend the training steps to 2e5 for AMBS and DBC. Figure 17 shows the training curve on
CARLA. AMBS and DBC run on the precious setting of CARLA, while AMBS40 and DBC40 run
on the highway with 40 generated vehicles. When increasing the number of vehicles from 20 to 40,
DBC becomes more difficult to learn but AMBS is just slightly slower. DBC still doesn’t achieve
high scores after 1e5 steps.
19
Published as a conference paper at ICLR 2022
Figure 17: Training curve on CARLA environment.
E Proofs
E.1 Value-function Bound
Theorem E.1 (Value function difference bound for different discounting factors (Petrik & Scherrer,
2009; Kemertas & Aumentado-Armstrong, 2021)). Consider two identical MDPs except for different
discounting factors γ1 and γ2, where γ1 ≤ γ2 and reward r ∈ [0, 1]. Let Vγπ denotes the value
function for MDP with discounting factor γ given policy π.
阳⑻ - 嗦⑻ l≤ (1-YlM- Y2)	(19)
Definition E.1 (c-weighted on policy bisimulation metric). Given a policy π, and c ∈ (0, 1), the
on-policy bisimulation metric exists,
d(Si, Sj) = (1 -c)∣R∏i -R∏j∣ + cWι(d)(P∏, P∏).	(20)
Theorem E.2 (Value difference bound). For any c ∈ [γ, 1), given two state si and sj,
(1 - C) |Vπ(Si) - Vπ(Sj)| ≤ d(Si, Sj)	(21)
proof. Lemma 6 of Kemertas & Aumentado-Armstrong (2021).
Theorem E.3 (Generalized value difference bound). For any c ∈ (0, 1), given two states Si and Sj,
(1 - c)|Vπ(Si) - Vπ(Sj)I ≤ d(Si, Sj) + 2(I-IC-；)(mnCc,γ))	(22)
proof. We follow the proof of Theorem 1 in Kemertas & Aumentado-Armstrong (2021). Suppose
another MDP with discounting factor γ0 = C exists. From Theorem E.2 we have
(1 - c)∣vγ∏(Si) - V(Sj)| ≤ d(Si,Sj).	(23)
Then,
(1-c)∣Vπ(Si) - Vπ(Sj)|
=(1 - C)|V π (Si) - Vγπ0(Si) + Vγπ0 (Si) - Vπ (Sj) + Vγπ0 (Sj) - Vγπ0 (Sj)|
≤(1 - c)(∣vγo (Si) - V (Sj )1 + ∣Vπ (Si)- vγo 回)| + ∣vπ (Sj)- V (Sj )∣)
≤d(Si, Sj) + (1 - C)(∣Vπ(Si) - V∏(Si)∣ + ∣Vπ(Sj) - V∏(Sj)∣)
(24)
≤d(Si,Sj) +
2(1 - C)(γ-min(γ0,γ))
(1 - γ)(1 - γ0)
Replace γ0 by C, finally we have
(1 - c)∣Vπ(Si) - Vπ(Sj)∣ ≤ d(Si, Sj) + 2(I -IC-γγ)(mnCc,γ)).	(25)
20