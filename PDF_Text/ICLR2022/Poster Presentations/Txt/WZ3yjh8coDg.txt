Published as a conference paper at ICLR 2022
An unconstrained layer-peeled perspective
ON NEURAL COLLAPSE
Wenlong Ji	Yiping Lu
Peking University	Stanford University
Yiliang Zhang
Zhun Deng
University of Pennsylvania Harvard University
Weijie J. Su
University of Pennsylvania
Ab stract
Neural collapse is a highly symmetric geometry of neural networks that emerges
during the terminal phase of training, with profound implications on the general-
ization performance and robustness of the trained networks. To understand how
the last-layer features and classifiers exhibit this recently discovered implicit bias,
in this paper, we introduce a surrogate model called the unconstrained layer-peeled
model (ULPM). We prove that gradient flow on this model converges to critical
points of a minimum-norm separation problem exhibiting neural collapse in its
global minimizer. Moreover, we show that the ULPM with the cross-entropy loss
has a benign global landscape for its loss function, which allows us to prove that
all the critical points are strict saddle points except the global minimizers that
exhibit the neural collapse phenomenon. Empirically, we show that our results
also hold during the training of neural networks in real-world tasks when explicit
regularization or weight decay is not used.
1 Introduction
Deep learning has achieved state-of-the-art performance in various applications (LeCun et al., 2015),
such as computer vision (Krizhevsky et al., 2012), natural language processing (Brown et al., 2020),
and scientific discovery (Long et al., 2018; Zhang et al., 2018). Despite the empirical success of
deep learning, how gradient descent or its variants lead deep neural networks to be biased towards
solutions with good generalization performance on the test set is still a major open question. To
develop a theoretical foundation for deep learning, many studies have investigated the implicit bias of
gradient descent in different settings (Li et al., 2018; Amari et al., 2020; Vaswani et al., 2020; Soudry
et al., 2018; Lyu & Li, 2019; Arora et al., 2019).
It is well acknowledged that well-trained end-to-end deep architectures can effectively extract features
relevant to a given label. Although theoretical analysis of deep learning has been successful in recent
years (Arora et al.; Goldblum et al., 2019), most of the studies that aim to analyze the properties of the
final output function fail to understand the features learned by neural networks. Recently, in Papyan
et al. (2020), the authors observed that the features in the same class will collapse to their mean and
the mean will converge to an equiangular tight frame (ETF) during the terminal phase of training, that
is, the stage after achieving zero training error. This phenomenon, namely, neural collapse (Papyan
et al., 2020), provides a clear view of how the last-layer features in the neural network evolve after
interpolation and enables us to understand the benefit of training after achieving zero training error
to achieve better performance in terms of generalization and robustness. To theoretically analyze
the neural collapse phenomenon, Fang et al. (2021) proposed the layer-peeled model (LPM) as a
simple surrogate for neural networks, where the last-layer features are modeled as free optimization
variables. In particular, in a balanced K-class classification problem using a neural network with d
neurons in the last hidden layer, the LPM takes the following form:
1n	1	1
min- EL(Whi,yi), s.t. 2||W||F ≤ Ci, -||H||F ≤ C2,
W,H n	2	2
i=1
(1.1)
1
Published as a conference paper at ICLR 2022
where C1,C2 are positive constants. Here, W = [wι, w2,…，WK]> ∈ RK×d is the weight of the
final linear classifier, H = [hi, h2,…，hn] ∈ Rd×n is the feature of the last layer and yi is the
corresponding label. The intuition behind the LPM is that modern deep networks are often highly
over-parameterized, with the capacity to learn any representations of the input data. It has been shown
that an equiangular tight frame (ETF), i.e., feature with neural collapse, is the only global optimum
of the LPM objective (1.1) (Fang et al., 2021; Lu & Steinerberger, 2020; Wojtowytsch & E, 2020;
Zhu et al., 2021).
However, feature constraints in LPMs are not equivalent to weight decay used in practice. In this
study, we directly deal with the unconstrained model and show that gradient flow can find those neural
collapse solutions without the help of explicit constraints and regularization. To do this, we build a
connection between the neural collapse and recent theories on max-margin implicit regularization
(Lyu & Li, 2019; Wei et al., 2018), and use it to provide a convergence result to the first-order
stationary point of a minimum-norm separation problem. Furthermore, we illustrate that the cross-
entropy loss enjoys a benign global landscape where all the critical points are strict saddles in the
tangent space, except for the only global minimizers that exhibit the neural collapse phenomenon.
Finally, we verify our insights via empirical experiments.
In contrast to previous theoretical works on neural collapse, our analysis does not incorporate any
explicit regularization or constraint on features. A comparison with other results can be found in Table
1 and we defer a detailed discussion to Section 5.2. The reasons we investigate the unregularized
objective are summarized as follows:
1.	Feature regularization or constrain is still not equivalent to weight decay used in practice.
However, previous studies have justified that neural networks continue to perform well
without any regularization or constraint (Zhang et al., 2021). Moreover, it is proved that
SGD with exponential learning rate on unconstrained objective is equivalent to SGD with
weight decay. (Li & Arora, 2019).
2.	As shown in this study, neural collapse exists even under an unconstrained setting, which
implies the emergence of neural collapse should be attributed to gradient descent and
cross-entropy loss rather than explicit regularization.
3.	Regularization or constraint feature constraint can be barriers for existing theories of neural
networks (Jacot et al., 2018; Lyu & Li, 2019). By allowing features to be totally free, we
hope our results can inspire further analysis to plug in a realistic neural network.
Reference	Contribution	Feature Norm Constraint	Feature Norm Regularization	Loss Function
(Papyan et al., 2020)	Empirical Results	%	%	CE Loss
(Wojtowytsch & E, 2020) (Lu & Steinerberger, 2020) (Fang et al., 2021)	Global Optimum	"	%	CE Loss
(Mixon et al., 2020) (Poggio & Liao, 2020) (Han et al., 2021)	Modified Training Dynamics	%	%	`2 Loss
(Zhu et al., 2021) concurrent	Landscape Analysis	%	"	CE Loss
This paper	Training Dynamics+ Landscape Analysis	%	%	CE Loss
Table 1: Comparison of recent analysis for neural collapse. We provide theoretical results with
the minimum modification of the training objective function. Here we use CE loss to refer to the
cross-entropy loss.
1.1	Contributions
The contributions of the present study can be summarized as follows.
2
Published as a conference paper at ICLR 2022
•	We build a relationship between the max-margin analysis (Soudry et al., 2018; Nacson et al.,
2019b; Lyu & Li, 2019) and the neural collapse and provide the implicit bias analysis to the
feature rather than the output function. Although both parameters and features diverge to
infinity, we prove that the convergent direction is along the direction of the minimum-norm
separation problem.
•	Previous works (Lyu & Li, 2019; Ji et al., 2020) only prove that gradient flow on homoge-
neous neural networks will converge to the KKT point of the corresponding minimum-norm
separation problem. However, the minimum-norm separation problem remains a highly
non-convex problem and a local KKT point may not be the neural collapse solution. In
this study, we perform a more detailed characterization of the convergence direction via
landscape analysis.
•	Previous analysis about neural collapse relies on the explicit regularization or constraint. In
this study, we show that the implicit regularization effect of gradient flow is sufficient to
lead to a neural collapse solution. The emergence of neural collapse should be attributed to
gradient descent and loss function, rather than explicit regularization or constraint. We put
detailed discussion in Section 5.2.
1.2	Related Works
Implicit Bias of Gradient Descent: To understand how gradient descent or its variants helps deep
learning to find solutions with good generalization performance on the test set, a recent line of
research have studied the implicit bias of gradient descent in different settings. For example, gradient
descent is biased toward solutions with smaller weights under `2 loss (Li et al., 2018; Amari et al.,
2020; Vaswani et al., 2020) and will converge to large margin solution while using logistic loss
(Soudry et al., 2018; Nacson et al., 2019b; Lyu & Li, 2019; Chizat & Bach, 2020; Ji et al., 2020).
For linear networks, Arora et al. (2019); Razin & Cohen (2020); Gidel et al. (2019) have shown that
gradient descent determines a low-rank approximation.
Loss Landscape Analysis: Although the practical optimization problems encountered in machine
learning are often nonconvex, recent works have shown the landscape can enjoy benign properties
which allow further analysis. In particular, these landscapes do not exhibit spurious local minimizers
or flat saddles and can be easily optimized via gradient-based methods (Ge et al., 2015). Examples
include phase retrieval (Sun et al., 2018), low-rank matrix recovery (Ge et al., 2016; 2015), dictionary
learning (Sun et al., 2016; Qu et al., 2019; Laurent & Brecht, 2018) and blind deconvolution (Lau
et al., 2019).
2 Preliminaries and Problem Setup
In this PaPer, || ∙ ||f denotes the FrobeniUs norm, k ∙ k2 denotes the matrix spectral norm, k ∙ k*
denotes the nuclear norm, k ∙ k denotes the vector l2 norm and tr(∙) is the trace of matrices. We use
[K] := {1,2,…，K} to denote the set of indices up to K.
2.1	Preliminaries
We consider a balanced dataset with K classes SkK=1{xk,i}in=1. A standard fully connected neural
network can be represented as:
f (x; Wfull) = bL + Wlσ ①—+ WL-iσ (…σ (bi + Wιx)…)).	(2.1)
Here Wfull = (Wi, W2,…,WL) denote the weight matrices in each layer, (bi, b2,…，bL
denote the bias terms, and σ(∙) denotes the nonlinear activation function, for example, ReLU or
sigmoid. Let hk,i = σ (bL-i + WL-ισ (…σ (bi + WιXk,i))) ∈ Rd denote the last layer feature
for data xk,i and hk = n P2i hk,i denotes the feature mean within the k-th class. To provide a
formal definition of neural collapse, we first introduce the concept of a simplex equiangular tight
frame (ETF):
Definition 2.1 (Simplex ETF). A symmetric matrix M ∈ RK ×K is said to be a simplex equiangular
tight frame (ETF) if
M = 1 IQ(IK - V71K1K).	(2.2)
3
Published as a conference paper at ICLR 2022
Where Q ∈ Rd×K is a matrix with orthogonal columns.
Let W ∈ RK×d = WL = [wι, w2, •一，WK]> be the weight of the final layer classifier, the four
criteria of neural collapse can be formulated precisely as:
•	(NC1) Variability collapse: As training progresses, the within-class variation of the activa-
tion becomes negligible as these activation collapse to their class mean hk = ɪ PN1 hk,i.
∖∖hk,i — hk|| =0,	1 ≤ k ≤ K.
•	(NC2) Convergence to Simplex ETF: The vectors of the class-means (after centering by
their global-mean) converge to having equal length, forming equal-size angles between any
given pair, and being the maximally pairwise-distanced configuration constrained to the
previous two properties.
cos(h k, h j) = — K1-1,	∖∖h k∖∖ = ∖∖h j ∖∖,	k = j.
•	(NC3) Convergence to self-duality: The linear classifiers and class-means will converge
to align with each other, up to appropriate rescaling, that is, there exist a universal constant
C > 0 such that
Wk = Chk,	k ∈ [K].
•	(NC4) Simplification to Nearest Class-Center For a given deepnet activation:
h = σ (bL-1 + WL-iσ (…σ (bi + Wιx)…))∈ Rd,
the network classifier converges to choose whichever class has the nearest train class-mean
arg min hwk, h → arg min ∣∣h — h k ∣∣ ,
kk
In this paper, we say that a point W ∈ RK ×d , H ∈ Rd×nK satisfies the neural collapse conditions
or is a neural collapse solution if the above four criteria are all satisfied for (W, H).
2.2 Problem Setup
We mainly focus on the neural collapse phenomenon, which is only related to the classifiers and
features in the last layer. Since the general analysis of the highly non-smooth and non-convex neural
network is difficult, we peel down the last layer of the neural network and propose the following
unconstrained layer-peeled model (ULPM) as a simplification to capture the main characters
related to neural collapse during training dynamics. A similar simplification is commonly used in
previous theoretical works (Lu & Steinerberger, 2020; Fang et al., 2021; Wojtowytsch & E, 2020;
Zhu et al., 2021), but ours does not have any constraint or regularization on features. It should be
mentioned that although Mixon et al. (2020); Han et al. (2021) also studied the unconstrained model,
their analysis adopted an approximation to real dynamics and was highly dependent on the `2 loss
function, which is rarely used in classification tasks. Compared with their works, we directly deal
with the real training dynamics and cover the most popular cross-entropy loss in classification tasks.
Let W = [wi, W2,…，WK]> ∈ RK×d and H = [hi,i,…，hi,„, h2,1,…，hκ,n] ∈ Rd×Kn be
the matrices of classifiers and features in the last layer, where K is the number of classes and n is the
number of data points in each class. The ULPM is defined as follows:
min
W,H
Kn
L(W, H) = WmiHn —XXlog
, k=1 i=1
exp(w>hk,i)
PjK=1 exp(Wj>hk,i)
(2.3)
Here, we do not have any constraint or regularization on features, which corresponds to the absence
of weight decay in deep learning training. The objective function (2.3) is generally non-convex on
(W, H) and we aim to study the landscape of the objective function (2.3). Furthermore, we consider
the gradient flow of the objective function:
dW(t) _	∂L(W(t), H(t)) dH	∂L(W(t), H(t))
dt =	∂W	, ^dT =	∂H	.
4
Published as a conference paper at ICLR 2022
3 Main Results
In this section, we present our main results regarding the training dynamics and landscape analysis of
(2.3). This section is organized as follows: First, in Section 3.1, we show the relationship between
the margin and neural collapse in our surrogate model. Inspired by this relationship, we propose a
minimum-norm separation problem (3.1) and show the connection between the convergence direction
of the gradient flow and the KKT point of (3.1). In addition, we explicitly solve the global optimum
of (3.1) and show that it must satisfy the neural collapse conditions. However, owing to the non-
convexity, we find an Example 3.1 in Section 3.2, which shows that there exist some bad KKT points
such that a simple gradient flow will get stuck in them and does not converge to the neural collapse
solution, which is proved to be optimal in Theorem 3.3. Then, We present our second-order analysis
result in Theorem 3.4 to show that those bad points will exhibit decreasing directions in the tangent
space thus gradient descent and its variants can avoid those bad directions and only converge to the
neural collapse solutions (Lee et al., 2016; Ge et al., 2015).
3.1	Convergence To The First–Order Stationary Point
Heuristically speaking, the simplex ETF (Definition 2.1) gives a set of vectors with the maximum
average angle between them. As a result, neural collapse implies that the neural networks tend to
maximize the angles between each class and the corresponding classifiers. At a high level, such
behavior is quite similar to margin maximization which is known to be an implicit regularization
effect of gradient descent and has been extensively studied in Soudry et al. (2018); Nacson et al.
(2019b); Lyu & Li (2019); Ji et al. (2020). First, we illustrate the connection between the margin and
neural collapse. Recall that the margin of a single data point xk,i and the associated feature hk,i is
qk,i (W, H) := wk>hk,i - maxj6=k wj>hk,i. Then, the margin of the entire dataset can be defined as
qmin(W,H) :=	min	qk,i(W, H).
k∈[K],i∈[n]
The following theorem demonstrates that neural collapse yields the maximum margin solution of our
ULPM model:
Theorem 3.1	(Neural collapse as max-margin solution). For the ULPM model (2.3), the margin of
the entire dataset always satisfies
qmin(W,H) ≤
kW kF + kHkF
2(k - i)√n
and the equality holds if and only if (W, H) satisfies the neural collapse conditions with kW kF
kHkF.
Based on this finding, we present an analysis of the convergence of the gradient flow on the ULPM
(2.3). Following Lyu & Li (2019), we link the gradient flow on cross-entropy loss to a minimum-norm
separation problem.
Theorem 3.2.	For problem (2.3), let (W (t), H (t)) be the path of gradient flow at time t, if there
exist a time t0 such that L(W(t0), H(t0)) < log 2, then any limit point of
.. ^ . . ^
{(H(t), W(t)) = (
H (t)	W (t)	)}
PkW(t)kF + kH(t)kF, PkW(t)kF + kH(t)kF 力
is along the direction (i.e., a constant multiple) ofa Karush-Kuhn-Tucker (KKT) point of the following
minimum-norm separation problem:
m,H2lWllF + 1|1H||F
s.t.wk>hk,i - wj>hk,i ≥ 1, k 6= j ∈ [K], i ∈ [n].
(3.1)
Remark 3.1. Here we write (3.1) as a constraint problem, but the constraint is introduced by the
implicit regularization effect of gradient flow on our ULPM objective (2.3). Since the training
dynamics will diverge to infinity, we hope to justify that the diverge direction is highly related
to neural collapse and an appropriate normalization is needed, which is why it appears to be a
5
Published as a conference paper at ICLR 2022
constraint optimization form. Our goal is to justify that the neural collapse phenomenon is caused
by the properties of the loss function and training dynamics rather than an explicit regularization or
constraint, which seems to be necessary for previous studies (Fang et al., 2021; Lu & Steinerberger,
2020; Wojtowytsch & E, 2020).
Remark 3.2. In Theorem 3.2, we assume that there exists a time t0 such that L(W (t0), H(t0)) <
log 2. Note that the loss function can be rewritten as:
Kn
L(W,H)=XXlog(1 + X exp(wj hk,i - wkhk,i))	(3.2)
the requirement L(W, H) < log 2 implies that wkhk,i - wjhk,i ≥ 0 for any k, j ∈ [K], i ∈ [n],
which is equivalent to qmin(W, H) > 0; that is, every feature is separated perfectly by the classifier.
This assumption is common in the study of implicit bias in the nonlinear setting (Nacson et al., 2019a;
Lyu & Li, 2019; Ji et al., 2020) and its validity can be justified by the fact that neural collapse is found
only in the terminal phase of training in the deep neural network, where the training accuracy has
achieved 100%. It is also an interesting direction to remove this assumption and study the early-stage
dynamics of training, which is beyond the scope of this study and we leave it to future exploration.
Theorem 3.2 indicates that the convergent direction of the gradient flow is restricted to the max-margin
directions, which usually have good robustness and generalization performance. In general, the
KKT conditions are not sufficient to obtain global optimality because the minimum-norm separation
problem (3.1) is non-convex. On the other hand, we can precisely characterize its global optimum in
the ULPM case based on Theorem 3.1:
Corollary 3.1. Every global optimum of the minimum-norm separation problem (3.1) is also a KKT
point that satisfies the neural collapse conditions.
With Theorem 3.2 bridging dynamics with KKT points of (3.1) and Corollary 3.1 associating global
optimum of (3.1) with neural collapse, the remaining work is to close the gap between the KKT point
and the global optimum.
3.2 Second-Order Landscape Analysis
In the convex optimization problem, the KKT conditions are usually equivalent to global optimality.
Unfortunately, owing to the non-convex nature of the objective (2.3), the KKT points can also be
saddle points or local optimum other than the global optimum. In this section, we aim to show
that this non-convex optimization problem is actually not scary via landscape analysis. To be more
specific, we prove that except for the global optimum given by neural collapse, all the other KKT
points are actually saddle points that can be avoided by gradient flow.
In contrast to previous landscape analysis of non-convex problems, where people aim to show that
the objective has a negative directional curvature around any stationary point (sun et al., 2015; Zhang
et al., 2020), our analysis is slightly different. Note that since the model is unconstrained, once
features can be perfectly separated, the ULPM objective (2.3) will always decrease along the direction
of the current point and the optimum is attained only in infinity. Although growing along all of those
perfectly separating directions can let the loss function decrease to 0, the speed of decrease is quite
different and there exists an optimal direction with the fastest decreasing speed. As shown in section
3.1, first-order analysis of training dynamics fails to distinguish such an optimal direction from KKT
points, and we need second-order analysis to help us fully characterize the realistic training dynamics.
First, we provide an example to illustrate the motivation and necessity of second-order landscape
analysis.
Example 3.1 (A Motivating Example). consider the case where K = 4, n = 1, let (W, H) be the
following point:
-10	0 -
100
0	1	-1	.
0	-1	1
One can easily verify that (W, H) enables our model to classify all features perfectly. Furthermore,
we can show that it is along the direction of a KKT point of the minimum-norm separation problem
1
-1
0
0
W=H
6
Published as a conference paper at ICLR 2022
(3.1)	by constructing the Lagrangian multiplier Λ = (λij)iK,j=1 as follows:
1-21-20 O
1-21-20 O
O O 1-21-2
O O 1-21-2
=
Λ
2 + 2e-2
2 + 2e-2 + 2e2
VWL(W, H) = VHL(W, H)
—
1 + α
r 1 I-1+α
V 1 + 2a2 I -α
-α
W0 = H0>
To see this, simply write down the corresponding Lagrangian (note that we aim to justify (W , H)
is along the direction of a KKT point of (3.1), to make it a true KKT point, one needs to multiple
1∕√2 on W, H):
1	1	4	11
L(W,H, A) = 4kwIIf + 4IIhIIf - E£%j(2Wihi- 2Wjhi- 1).
i=1j 6=i
Simply take derivatives for W, H and Λ we find that it satisfies the KKT conditions. However, the
gradient of (W, H) is:
-1	-10	0 -
-1	1	0	0
0	0	1	-1	.
0	0	-1	1
We can see that the directions of the gradient and the parameter align with each other (i.e., W is
parallel to VwL(W, H), and H is parallel to NHL(W, H)), which implies that simple gradient
descent may get stuck in this direction and only grow the parameter norm. However, if we construct:
—1 + α	α	α
1+α	α	α
-α 1 -α -1 -α ,
-α -1 -α 1 -α
By simple calculation, we find that f(α) := L(W0, H0) satisfies f0(α) = 0, f00(α) < 0. Since
IW0IF = IWIF, IH0IF = IHIF and ||W0 - W ||2F + ||H0 - H||2F → 0asα → 0, this result
implies that for any > 0, we can choose appropriate α such that:
||W0||2F= ||W||2F,||H0||2F= ||H||2F,
||W0 - W||2F+ ||H0 -H||2F < , L(W0, H0) < L(W,H).
In Example 3.1, it is shown that there are some KKT points of the minimum-norm separation problem
(3.1) that are not globally optimal, but there exist some better points close to it; thus, the gradient-
based method can easily avoid them (see Lee et al. (2016); Ge et al. (2015) for a detailed discussion).
In the following theorem, we will show that the best directions are neural collapse solutions in the
sense that the loss function is the lowest among all the growing directions.
Theorem 3.3. The optimal value of the loss function (2.3) on a sphere is obtained (i.e., L(W, H) ≤
L(W0, H0) for any ||W0||2F + ||H0||2F = ||W ||2F + ||H||2F) if only if (W, H) satisfies neural
collapse conditions and ||W ||F = ||H||F.
Remark 3.3. Note that the second condition is necessary because neural collapse conditions do not
specify the norm ratio of W and H. That is, if (W, H) satisfies the neural collapse conditions, then
for any α, β ∈ R, (αW, βH) will also satisfy them, but only some certain α, β are optimal.
Now, we turn to those points that are not globally optimal. To formalize our discussion in the
motivating example 3.1, we first introduce the tangent space:
Definition 3.1 (tangent space). The tangent space of (W, H) is defined as a set of directions that
are orthogonal to (W, H) :
T(W,H) = {∆W ∈ RK×d, ∆H ∈ Rd×nK) : tr(W>∆W) + tr(H>∆H) = 0}
Our next result justifies our observation in Example 3.1 that for those non-optimal points, there exists
a direction in the tangent space such that moving along this direction will lead to a lower objective
value.
7
Published as a conference paper at ICLR 2022
Theorem 3.4. If (W, H) is not the optimal solution in Theorem 3.3 (i.e., (W, H) is not a neural
collapse solution or it is a neural collapse solution but ∣W ∣F 6= ∣H∣F), then there exists a direction
(∆W, ∆H) ∈ T(W, H) and constant M > 0 such that for any 0 < δ < M,
L(W +δ∆W,H+δ∆H) < L(W, H).	(3.3)
Further more, it implies that for any > 0 there exists a point (W0, H0) such that:
||W0||2F +||H0||2F = ||W||2F+ ||H||2F,	(34)
||W0 - W ||2F + ||H0 -H||2F < ,L(W0,H0) < L(W,H).	.
Remark 3.4. The result in (3.3) gives us a decreasing direction orthogonal to the direction of (W, H).
As shown in Example 3.1, the gradient on these non-optimal points might be parallel to (W, H); thus,
the first-order analysis fails to explain the prevalence of neural collapse. Here the decreasing direction
is obtained by analyzing the Riemannian Hessian matrix and finding its eigenvector corresponding
to a negative eigenvalue, which further indicates that these points are first-order saddle points in
the tangent space. That is why we name it second-order landscape analysis. A formal statement
and proof are presented in the appendix C. Previous works have shown that for a large family of
gradient-based methods, they can avoid saddle points and only converge to minimizers (Lee et al.,
2016; Ge et al., 2015; Panageas et al., 2019), thus our landscape analysis indicates that the gradient
flow dynamics only find neural collapse directions.
4 Empirical Results
6>wys
8uaJ£Psln-OSqe ?4
Last Iayerfeatune (Cifar-IO)
Last layer feature (MNiST)
Ue-SQ pβu-s
Psln-OSqe
1	10	100	500	1	10	100	500	1	10	100	500	100 ' ' ' IO1 IO2
Epoch	Epoch	Epoch	Epoch
(a) Variation of the norms	(b) Within-class variation	(c) Cosines between pairs	(d) Self-duality
Figure 1: Experiments on real datasets without weight decay. We trained a ResNet18 on both MNIST and
CIFAR10 datasets. The x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are
set to have log scales.
To evaluate our theory, we trained the ResNet18 (He et al., 2016) on both MNIST (LeCun et al.,
1998) and CIFAR-10 (Krizhevsky et al., 2009) datasets without weight decay, and tracked how the
last layer features and classifiers converge to neural collapse solutions. The results are plotted in
Figure 1. Here we reported the following metrics to measure the level of neural collapse:
1.	The variation of both feature norm (i.e., Std(∣∣hk - hk)∕Avg(∣∣hk - h∣∣)) and classifier
norm in the last layer (i.e., Std(∣wkk)∕Avg(∣Wk∣∣)).
2.	Within-class variation for last layer features (i.e., Avg(∣hk,i - hk∣)/Avg(∣hk,i - h∣)).
3.	Average cosine between last layer features (i.e., Avg(∣ cos(hk, hk，) + 1/(K - 1)∣)) and that
of last layer classifiers (i.e., Avg(∣ Cos(Wk, W k，) + 1∕(K - 1)∣)).
4.	Self-duality distance between features and classifiers corresponding to the same class in the
last layer. (i.e., Avg(∣(h k - h)∕∣h k - h∣∣- W k∕∣∣Wk k∣))
Here a smaller value of each metric indicates a closer state to a neural collapse solution. Specifically,
the within-class variation measures (NC1) ,the variation of norms measures and cosine between pairs
measure (NC2), the self-duality measures (NC3), and (NC4) has been shown to be a corollary of
(NC1)-(NC3) (Papyan et al., 2020). More experiment results under various settings and training
details can be found in Appendix D. As shown in the Figure 1, all of these metric decreases along
the training epochs. These results reveal strong evidence for the emergence of neural collapse in the
unconstrained setting and provide sound support for our theory.
8
Published as a conference paper at ICLR 2022
5 Conclusion and Discussion
5.1	Conclusion
To understand the implicit bias of neural features from gradient descent training, we built a connection
between max-margin implicit bias and the neural collapse phenomenon and studied the ULPM in this
study. We proved that the gradient flow of the ULPM converges to the KKT point of a minimum-norm
separation problem, where the global optimum satisfies the neural collapse conditions. Although the
ULPM is non-convex, we show that ULPM has a benign landscape where all the stationary points are
strict saddle points in the tangent space, except for the global neural collapse solution. Our study
helps to demystify the neural collapse phenomenon, which sheds light on the generalization and
robustness during the terminal phase of training deep networks in classification problems.
5.2	Relationship with Other Results on Neural Collapse
Theoretical analysis of neural collapse was first provided by Lu & Steinerberger (2020); Wojtowytsch
& E (2020); Fang et al. (2021), who showed that the neural collapse solution is the only global
minimum of the simplified non-convex objective function. In particular, Wojtowytsch & E (2020); Lu
& Steinerberger (2020) studied a continuous integral form of the loss function and showed that the
features learned should have a uniform distribution on the sphere. A more realistic discrete setting
was studied in Fang et al. (2021), where the constraint is on the entire feature matrix rather than
individual features. Our result utilizes the implicit bias of the cross-entropy loss function to remove
the feature norm constraint, which is not practical in real applications.
Although the global optimum can be fully characterized by neural collapse conditions, the ULPM
objective is still highly non-convex. Regarding optimization, Mixon et al. (2020); Poggio & Liao
(2020); Han et al. (2021) analyzed the unconstrained feature model with `2 loss and established
convergence results for collapsed features for gradient descent. However, they fail to generalize to
the more practical cross-entropy loss functions used in classification tasks. The analysis relies highly
on the `2 loss to obtain a closed-form gradient flow and still requires some additional approximation
to guarantee global convergence.
The most relevant study is a concurrent work (Zhu et al., 2021), which provides a landscape
analysis of the regularized unconstrained feature model. Zhu et al. (2021) turns the feature norm
constraint in Fang et al. (2021) into feature norm regularization and still preserves the neural collapse
global optimum. At the same time, it shows that the modified regularized objective shares a benign
landscape, where all the critical points are strict saddles except for the global one. Although our study
and Zhu et al. (2021) discover similar landscape results, we believe our characterization remains
closer to the real algorithms since we do not introduce any constraints or regularization on the
feature norm following the conventional setting in realist training. The regularization of features
introduced in Zhu et al. (2021) is still different from weight decay regularization (Krogh & Hertz,
1992). However, weight decay on homogeneous neural networks is equivalent to gradient descent
with scaling step size on an unregularized objective (Li & Arora, 2019; Zhang et al., 2018). Moreover,
neural networks are found to perform well in the absence of weight decay, which highlights the
importance of implicit regularization (Zhang et al., 2021). As a result, instead of explicit feature norm
constraint/regularization, in this study, we consider implicit regularization brought by the gradient
flow on cross-entropy. We show that implicit regularization is sufficient to lead the dynamics to
converge to the neural collapse solution without the explicit regularization.
Acknowledgments
We are grateful to Qing Qu and X.Y. Han for helpful discussions and feedback on an early version
of the manuscript. Wenlong Ji is partially supported by the elite undergraduate training program of
the School of Mathematical Sciences at Peking University. Yiping Lu is supported by the Stanford
Interdisciplinary Graduate Fellowship (SIGF).
9
Published as a conference paper at ICLR 2022
References
Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu. When does preconditioning help or hurt generalization? arXiv PrePrint
arXiv:2006.10732, 2020.
Raman Arora, Sanjeev Arora, Joan Bruna, Nadav Cohen, Rong Ge, Suriya Gunasekar, Chi
Jin, Jason Lee, Tengyu Ma, Behnam Neyshabua, and Zhao Song. Theory of deep learn-
ing. https://www.cs.princeton.edu/courses/archive/fall19/cos597B/
lecnotes/bookdraft.pdf/.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv PrePrint arXiv:1905.13655, 2019.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv PrePrint arXiv:2005.14165, 2020.
Lenaic Chizat and Francis Bach. ImPlicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In CanferenCe on Learning Theory, pp. 1305-1338. PMLR, 2020.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for Classicaljapanese literature. arXiv PrePrint arXiv:1812.01718, 2018.
J. Dutta, K. Deb, RuPesh Tulshyan, and Ramnik Arora. APProximate kkt Points and a Proximity
measure for termination. JOUrnaI of Global Optimization, 56:1463-1499, 2013.
C. Fang, H. He, Q. Long, and W. Su. ExPloring deeP neural networks via layer-Peeled model:
Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences (in
press), 2021.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In COnference on Iearning theory, pp. 797-842. PMLR, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv
PrePrint arXiv:1605.07272, 2016.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. arXiv PrePrint arXiv:1904.13262, 2019.
Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom Goldstein.
Truth or backpropaganda? an empirical investigation of deep learning theory. arXiv PrePrint
arXiv:1910.00359, 2019.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv PrePrint arXiv:1506.07540, 2015.
X. Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under mse loss: Proximity to and
dynamics on the central path, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In PrOCeedingS of the IEEE COnference on COmPUter ViSiOn and Pattern recognition,
pp. 770-778, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and
generalization in neural networks. In PrOCeedingS of the 32nd International COnference on Neural
InfOrmatiOn PrOCeSSing Systems, pp. 8580-8589, 2018.
Ziwei Ji, Miroslav Dud^k, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the
regularization path for general losses. In COnferenCe on Learning Theory, pp. 2109-2136. PMLR,
2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
10
Published as a conference paper at ICLR 2022
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
VolUtionalneUral networks. AdVanceS in neural information processing systems, 25:1097-1105,
2012.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In AdVanceS in
neural information processing systems, pp. 950-957, 1992.
Yenson Lau, Qing Qu, Han-Wen Kuo, Pengcheng Zhou, Yuqian Zhang, and John Wright. Short-and-
sparse deconvolution-a geometric approach. arXiv PrePrint arXiv:1908.10959, 2019.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In International conference on machine Iearning, pp. 2902-2907. PMLR, 2018.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436^44,
2015.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In ConferenCe on Iearning theory, pp. 1246-1257. PMLR, 2016.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In ConferenCe On Learning Theory,
pp. 2U7. PMLR, 2018.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. arXiv
PrePrint arXiv:1910.07454, 2019.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International ConferenCe on MaChine Learning, pp. 3208-3216. PMLR, 2018.
Jianfeng Lu and Stefan Steinerberger. Neural collapse with cross-entropy loss. arXiv PrePrint
arXiv:2012.08465, 2020.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv PrePrint arXiv:1906.05890, 2019.
Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv
PrePrintarXiV:2011.11619, 2020.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic
and depth-sensitive margins in homogeneous and non-homogeneous deep models. In International
Conference on MaChine Learning, pp. 4683U692. PMLR, 2019a.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd
International ConferenCe on ArtifiCiaI Intelligence and StatiStics, pp. 3420-3428. PMLR, 2019b.
Ioannis Panageas, Georgios Piliouras, and Xiao Wang. First-order methods almost always avoid
saddle points: The case of vanishing step-sizes. arXiv PrePrint arXiv:1906.07772, 2019.
Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal
phase of deep learning training. ProCeedingS of the NationaI ACademy of Sciences, 117(40):
24652-24663, 2020.
Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers
trained with the square loss. arXiv PrePrint arXiv:2101.00072, 2020.
Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Analysis of the optimization
landscapes for overcomplete representation learning. arXiv PrePrint arXiv:1912.02427, 2019.
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by
norms. arXiv PrePrint arXiv:2005.06398, 2020.
11
Published as a conference paper at ICLR 2022
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv Preprint arXiv:1409.1556, 2014.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The JoUrnaI of Machine Learning ReSearch, 19(1):
2822-2878, 2018.
JU Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint
arXiv:1510.06096, 2015.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and
the geometric picture. IEEE TranSaCtiOnS on InfOrmatiOn Theory, 63(2):853-884, 2016.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. FOUndatiOnS of
COmpUtatiOnal Mathematics, 18(5):1131-1198, 2018.
Sharan Vaswani, Reza Babanezhad, Jose Gallego, Aaron Mishkin, Simon Lacoste-Julien, and
Nicolas Le Roux. To each optimizer a norm, to each norm its generalization. arXiv preprint
arXiv:2006.06821, 2020.
G Alistair Watson. Characterization of the subdifferential of some matrix norms. Linear algebra and
its applications, 170:33^5, 1992.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. 2018.
Stephan Wojtowytsch and Weinan E. On the emergence of tetrahedral symmetry in the final and
penultimate layers of neural network classifiers. arXiv preprint arXiv:2012.05420, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. COmmUniCatiOnS of the ACM, 64(3):107-115,
2021.
Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep potential molecular
dynamics: a scalable model with the accuracy of quantum mechanics. PhySiCal ReVieW Letters,
120(14):143001,2018.
Yuqian Zhang, Qing Qu, and John Wright. From symmetry to geometry: Tractable nonconvex
problems. arXiv preprint arXiv:2007.06753, 2020.
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geo-
metric analysis of neural collapse with unconstrained features. arXiv preprint arXiv:2105.02375,
2021.
12
Published as a conference paper at ICLR 2022
A Elements of optimization
In this section, we introduce some basic definitions and theory about optimization. In the following
discussion, we consider a standard form inequality constrained optimization problem:
min f (x)
x∈Rd
s.t. gi (x) ≤ 0,	i ∈ [n].
(A.1)
In addition, we assume all of those functions f and gi are twice differentiable. A point x ∈ Rd is
said to be feasible if and only if it satisfies all of the constraints in (A.1), i.e., gi(x) ≤ 0, i ∈ [n].
And the Lagrangian of problem (A.1) is defined as following:
n
L(x, λ) = f(x) +	λigi(x).
i=1
A.1 Karush Kuhn Tucker Conditions
Now let’s first introduce the definition of Karush Kuhn Tucker (KKT) point and approximate KKT
point. Here we follows the definition of (, δ)-KKT point as in Lyu & Li (2019).
Definition A.1 (Definition of KKT point). A feasible point is said to be KKT point of problem (A.1)
if there exist λι, λ2, ∙∙∙ , λn ≥ 0 such that the following Karush KUhn Tucker (KKT) conditions
hold:
L ▽/(X) + Pi=ι λiVgi(x) = 0
2.	λigi (x) = 0, i ∈ [n]
Definition A.2 (Definition of (, δ)-KKT point). For any , δ > 0, a feasible point of (A.1) is said to
be (e, δ)-KKT point of problem (A.1) if there exist λι, λ2,…，λn ≥ 0 such that:
1.	”▽/(X) + Pi=ι %Wi(X)U≤ e
2.	λigi (x) ≥ -δ, i ∈ [n]
Generally speaking, KKT conditions might not be necessary for global optimality. We need some
additional regular conditions to make it necessary. For example, as shown in Dutta et al. (2013)
we can require the problem to satisfy the following Mangasarian-Fromovitz constraint qualification
(MFCQ):
Definition A.3 (Mangasarian-Fromovitz constraint qualification (MFCQ) ). For a feasible point X of
(A.1), problem (A.1) is said to satisfy (MFCQ) at X if there exist a vector v ∈ Rd such that:
hVχgi(x), vi > 0, i ∈ [n].	(A.2)
Moreover, when MFCQ holds we can build a connection between approximate KKT point and KKT
point, see detailed proof in Dutta et al. (2013):
Theorem A.1 (Relationship between Approximate KKT point and KKT point). Let
Xk ∈ Rd : k ∈ N be a sequence of feasible points of (A.1) , {ek > 0 : k ∈ N} and
{δk > 0 : k ∈ N} be two sequences of real numbers such that Xk is an (ek, δk)-KKT point for
every k, and ek → 0, δk → 0. If Xk → X as k → +∞. If MFCQ (A.3) holds at X, then X is a KKT
point of (P).
B Omitted proofs from Section 3.1
Recall our ULPM problem:
miH L(W，H)= - XL XX log (PKxP(W>"、
W,H	k=1 i=1	j=1 exp(wj> hk,i )
13
Published as a conference paper at ICLR 2022
Let’s revieW some basic notations defined in the main body. Let sk,i,j = Wk>hk,i - Wj>hk,i, ∀k ∈
[K], i ∈ [n], j ∈ [K], the margin of a single feature hk,i is defined to be qk,i(W, H) :=
minj6=k sk,i,j = Wk> hk,i - maxj6=k Wj> hk,i . We define the margin of entire dataset as qmin =
qmin(W, H) = mink∈[1,K ],i∈[1,n] qk,i(W, H). We first prove Theorem 3.1 in the mainbody.
Proof of Theorem 3.1. First we can find that the margin will not change if we minus a vector a
for all Wj, so if we denote the mean of classifier Wi = Wi - K PK=I Wi and then we have
w>hk,i - maxj=k w>hk,i = W>hk,i - maxj=k W>hk,i ≥ qmin(W, H), that is:
W>hk,i- W>hk,i ≥ qmin(W, H),∀j = k ∈ [K],i ∈ [n].
(B.1)
Note that PK=I W>hk,i = 0 then SUm this inequality over j We have:
(K - 1)w>hk,i - X W>hk,i = KW>hk,i ≥ (K - 1)qmin(W, H),∀k ∈ [K],i ∈ [n]∙
j6=k
By Cauchy inequality, We have:
ɔ ( ~~F^ H Wk ||2 + √nllhk,il12) ≥ w>hk,i ≥ -N-qmin (W, H)
2n	K
Sum (B.2) over k and i We have:
1 √n(l∣W∣lF + ||H ||F) ≥ n(K - 1)qmin (W, H).
On the other hand, We knoW that:
K	1K	K
||W||F = Xl|Wi - KXWill2 ≤ X∣∣w,∣∣2 = ∣∣w∣∣F.
(B.2)
(B.3)
i=1
Then We can conclude that:
i=1
i=1
q . (W H) ≤kWkF + kHkF
qmin(W,H) ≤	2(k- i)√n
as desired. When the equality holds, first We have ∣∣W∣∣F = ||W ||2F Which is equivalent to
-K PK=ι Wi = 0, Wi = wi. Take it back into (B.3), then we must have all of the equality holds in
(B.2) and (B.1), Which give us:
Wk = √nhk,i, ||Wk ll2 = n||hk,i H2
Take this into (B.1) we have:
||W||2F+||H||2F
2K
hk,i = hk,i0 , hk>,ihj,i0 = Wk>Wj
Which implies neural collapse conditions.
∣∣w∣∣F + ∣∣h∣∣F
2K(κ - ι)√n ,
NoW let’s turn to the training dynamics and prove Theorem 3.2. Before starting the proof, We need
to introduce some additional notations. Since the W and H are all optimization variables here, We
denote θ = vec(W, H) as the Whole parameter for simplicity and all of previous function can be
defined on θ by matching the corresponding parameters. Denote ρ = kθk as the norm of θ and
-log(eL⑻-1)
T2
. Now we can state our first lemma to show how training dynamics of gradient
floW on ULPM objective (B) is related to a KKT point of (3.1).
Lemma B.1. If there exist a time t0 such that L(θ(t0)) < log 2, then for any t > t0 θ :=
θ∕qmin(θ)1/2 is an (e, δ) - approximate KKT point of the following minimum-norm separation
problem. More precisely, We have
=22(1 - β(t))	K2(K - 1)n
e =V	γ(t)	, = 2eγ(t)qmin(t),
where:
θ dθ dθ
β = ⅛∣2，瓦川 dt ||2i
is the angle betWeen θ and its corresponding gradient.
γ
—
□
14
Published as a conference paper at ICLR 2022
Proof. The training dynamics is given by gradient flow:
dθ _	∂L(θ)
dt ∂θ
Then by the chain rule we have:
dL(θ) _	∂L dθ
dt ∂θ dt
dθ
dt
(B.4)
It indicates that the loss function L is monotonically decreasing. If L(θ(t0)) < log 2, we have
L(θ(t)) < log 2, ∀t > t0. On the other hand, note that
Kn
L(θ(t)) = X X log(1 + X e-sk,i,j(t)) ≥ log(1 +exp(-qmin(t))),	(B.5)
k=1 i=1	j 6=k
which gives us qmin(t) > 0, ∀t > t0.
Let g = dθt, note that we can rewrite the ULPM objective function (B) as L(θ) = PK=I Pn=Ilog(1 +
Pj6=k e-sk,i,j ). By the chain rule and the gradient flow equation we have
g
k=1 i=1 j6=k
e-sk,i,j
1 + Pι=k e-sk,i,l gk,i,j,
where gk,i,j is the gradient of sk,i,j, i.e. gk,i,j = Vθsk,i,j(θ). Now let gk,i,j
Vθ sk,i,j (θ) and construct λk,i,j
P________e-sk,i,j
l≡2 1+Pι=k e-sk,i,l
, we only need to show:
gk,i,j /qmin
Kn	1
∣B-E ∑∑λk,i,j gk,i,j ι∣2 ≤ 三,	(B.6)
k=1 i=1 j6=k	γ
XXX λk,i,j (Skdj (θ) -I) ≤ k 2eK. ~)n.	(B.7)
k=1 i=1 j6=k	eqminγ
To prove (B.6), We only need to compute (Recall that θ = θ∕qmin(θ)1/2):
Kn
∣∣θ-XXXλk,i,jgk,i,j 112
k=1 i=1 j 6=yn
22
qmn 11W-≡ ||2 = qmn (2 - 2β).
Note that:
log(eL(θ)	1)	N
Y =---------2--------, L(θ) = Elog(I + Ee nj) ≥ log(1+exp(-qmin)).	(B.8)
ρ	n=1	j 6=yn
Then we have the following inequality:
qmin
7 ≤ ~~2~.
ρ2
(B.9)
Take this back into (B.8) we have (B.6) as desired.
To prove (B.7), first by our construction:
Kn
XXXλk,i,j(sk,i,j(θ7)-1)
k=1 i=1 j6=k
P
qmin IlgII 2
Kn
XXX
k=1 i=1 j6=k
e-sk,i,j
1 + pι=k e-sk,i,ι (SkK- qmin)
(B.10)
15
Published as a conference paper at ICLR 2022
Note that ∣∣g∣∣2 ≥(g,舟)=P hg,θ and hgk,i,j 阳=2sk,i,j since sk,i,j = w>hk,i - w> hk,i,
we have:
1	1Kn
l∣g∣∣2 ≥ 1 hg,θi =1∑∑∑
ρ	ρ k=1 i=1 j6=k
Kn
=ρ X X X
e-sk,i,j
1 + P= e-sk,i,l hgk,i,j,θi
e-sk,i,j
1 + P= e-sk"，1 sk,i,j
Kn
≥ PqKnE ɪɪefj	(since sk,i,j ≥ qmin > 0 and e-sk,i,l ≤ 1)
ρ	k=1 i=1 j6=k
≥ 2 qmin e-qmin
Take this inequality back into the (B.10) we have:
(B.11)
Kn
XXX λk,i,j (S"⑶-1) ≤
k=1 i=1 j6=k
Kρ2 X XX	eqmin-sk,ij	/	ʌ
2qmn A Z j=k1+P/ e-sk,i,l (SsklLin
K2 K n
≤ REEEeqmin-sk,ij Mi,--.)
2qmin k=1 i=1 j6=k
≤
≤
K Kn
_______ eqmin-sk,i,j ( S	)
OC 入乙乙乙 e	(sk,ij	qmin)
2qminγ k=1 i=1 j6=k
K2(K - 1)n
2eqminγ
Where the last inequality is obtained from the fact xe-x ≤ 1, ∀χ > 0, which can be proved by some
elementary calculus.	□
Based on Lemma B.1, we have shown that the (W, H) will be a (, δ)-KKT point, if we can show
(, δ) converge to zero, then by Theorem A.1 we know the limit point will be along the direction of
a KKT point. Ignoring the constant term, we only have to show how 7(t),β(t) and qmin(t) evolve
along time. Now we provide the following lemmas to illustrate their dynamics. The first lemma aims
at proving that the norms of parameter ρ(t) and YO are monotonically increasing.
Lemma B.2. If there exist t0 such that L(θ(t0)) < log 2, then for anyt >t0 we have:
dρ2
dt
> 0,
dγY
-Y ≥ 0.
dt
(B.12)
Proof. We can disentangle the whole training dynamics into the following two parts:
•	the radial part: V := θθ> dt,
•	the tangent part: U = (I - θθ>) ɪ.
First analyze the radial part, by the chain rule: ∣∣v∣∣2 = ∣θ> d | = | P <θ, d)| = | P1 dp21. For dp2-,
we have the following equation:
1 dP2 _ /θ dθ∖ _ 2 X X X	e-sk,ij
2 瓦= Vdt/ = ⅛ A j⅛ 1+P-e-sk,i，1 k,i,j,
(B.13)
16
Published as a conference paper at ICLR 2022
where the last equality holds by equation (B.11).Then when t > t0, we have shown that qmin(t) ≥ 0,
combine this with the fact sk,i,j ≥ qmin we obtain the first inequality in (B.12)
1 dρ2
2 ~tt^
Kn
=2XXX
k=1 i=1 j6=k
e-sk,i,j
1 + P∙S sk,i,j
Kn
≥2XXX
k=1 i=1 j6=k
e-sk,i,j
1 + Pι=kL2 qmin ≥ 0
(B.14)
Next, We aim to prove the monotonicity of γ(t), compute the derivative of γ(t) We have:
- log(eL(θ) - 1)
Y =----------2-------
ρ2
ddt logY = ddt(Iog(Tog断⑻-I))- 2logP),
(B.15)
d log(-log(eL(θ)-1))
Recall that We have:
1	eL⑻	dL(θ)
log(eL⑻-1) eL⑻-1 dt
〉dL(θ)	1	eL(θ
一 dt qmin eL⑻-1
(B.16)
1 dρ2
2 ~tt
Kn
=2XXX
k=1 i=1 j 6=k
e-sk,i,j
1 + Pl= e-sk",l sk,i,j
K n P	e-sk,i,j
≥2 XX	Wk ----------qmin
-k= i=1 1 + P= efj
Kn
XXlog(1+Xe-sk
k=1 i=1	j6=k
.ij)_______1_________fj=e-sk"j
Ilog(I + j=kek e-sk"j)1 + j=kek e-sk"j
qmin
(B.17)
K n	eL(θ)	1
≥2 X X log(1+X) -qmin
eL(θ) - 1
=2 —ZL7θ) — qmin.
Then second last line is because the definition of the loss function L(θ) = PkK=1 Pin=1 log(1 +
Pj=k e-sk,ij) ≥ log(1 + Pj=k e-sk,ij) and the monotonicity of ex-1 (in fact, dXex- =
e-x(xχ2ex+1) ≤ 0,∀x> 0).
As a result, We notice that
1 dt log Y(t)
-1 dL	d
正-dt logρ.
At the same time, we notice that ||v『=P (2 务
chain rule:
lθ = l (P 艺-dρθ∖ = 1
dt	P2 Pdt	dt	ρ2
2	1 dρ2 d
=2 £ ∙ dt log ρ on the one hand, and by the
1 dρ2
2 -dT
Combine this With the radial term:
dL
dt
dθ
dt
kvk2 + kuk2
—
2
Dividing 2 ddt- on both sides, we have
dL (1 dρ2 ʌ 1 d	d d ʌ 1 dθ
-加12-dΓ) = d logρ + (原 logρ) dt
(B.18)
17
Published as a conference paper at ICLR 2022
2
d	d	-1
dtlog ρ + Idtlog ρ
^
dθ
dt
≤-
dL(θ)	1	eL(θ)
dt qmin 2(eL(θ) - 1).
Now by equation (B.15) and (B.16) we obtain:
2
1 d- log 7 ≥ -
2 dt
dL(θ)	1
eL(θ)
dt qmin 2 (eL⑻-1)
d	d	-1
-dt log P ≥ Id log P
dθ
dt
(B.19)
By (B.14) We know the P is monotonically increasing, We have d log ρ > 0 and then We get the
second inequality in (B.12).	□
Lemma B.2 gives us the monotonicity of γ7, note that since the loss function L(θ(t0)) < log 2, we
have γ7(t) ≥ γ7(t0) > 0, then we can treat γ7(t) in Lemma B.1 as a positive constant. The remaining
work is to show qmin(t) grows to infinity and β(t) → 1. To show qmin(t) → ∞, it’s equivalent to
show L(t) → 0 and we have the following lemma:
Lemma B.3. If there exist t0 such that L(θ(t0)) < log 2, then L(θ(t)) → 0 and qmin(θ(t)) → ∞
as t → ∞, moreover we have the convergence rate L(θ(t)) = O(1/t)
Proof. By (B.4) and (B.13), the evolution of loss function L(θ) can be written as:
dL(θ)
dt
dθ
dt
2 ≤-(dt
2	Kn
=-(2XXX
k=1 i=1 j 6=k
e-sk,i,j
1 +	l6=k e-sk,i,l
sk,i,j)2.
—
Combine it with (B.5),(B.14) and (B.17) we have:
Kn
2XXX
k=1 i=1 j 6=k
Which indicates:
e-sk,i,j
eL(θ) - 1
1 + P= e-sk,i,ι sk,i,j ≥ 2KθΓ
eL(θ) - 1
qmin ≥ -2-----L7θ- log(e4(")- 1),
eL(θ)
dL(θ(t))	eL(θ(t)) - 1
-(dΓ1 ≤-4( -^L≡- log-®'))- 1))2.
Since 0 < L(θ(t)) < L(θ(t0)) < log 2, note that:
(B.20)
which implies:
d e
dx
x-1
) > 0, lim
x→0
ex - 1
-----=1,
x
eL(θ(t)) - 1
<	L(θ(t))	<
1
lθg(2),
(B.21)
x
1
On the other hand, We can find that:
log(L(θ(t)) - 1) < log(L(θ(t0)) - 1) <0,	1 < L(θ(t)) <2
(B.22)
Combine equation (B.21) and (B.22) together, one can conclude that there exist a constant C > 0
such that for t > t0 :
dL(θ≡ ≤ -C(L(θ(t)))2
It further implies that:
c<0 (ɪ),
dt(L(θ(t))),
then integral on both side we have:
1
1
C(t - t0) <
-------- ------------.
L(θ(t))	L(θ(to)),
and
L(θ(t)) = O(1/t).
Thus we must have L(θ(t)) → 0 and combine this with qmin ≥ - log(eL(θ(t)) - 1) we know
qmin(θ(t)) → ∞ as desired.	□
18
Published as a conference paper at ICLR 2022
To bound β(t), we first need a useful lemma to bound the changes of the direction of θ.
Lemma B.4. If there exist t0 such that L(θ(t0)) < log 2, then for any t > t0
^
dθ
dt
1 d
≤ Y(t0) dt
log ρ.
Proof. First we know that:
^
dθ
dt
dθ
dt
e-sk,i,j
1 + Pι=k e-sk,i,l gk,i,j
dθ
dt
Kn
XXX
k=1 i=1 j 6=k
Kn
≤XXX
k=1 i=1 j 6=k
e-sk,i,j
1 + Pl=k e-sk,"
kgk,i,j k .
(B.23)
Recall our gk,i,j = ds∂θj and sk,i,j = w>hk,i - w>hk,i, We can find that kgk,i,jk ≤ 2ρ. On the
other hand, Combine it with (B.13) and (B.9) we have:
^
dθ
dt
dθ
dt
1
≤ -
ρ
1 dρ2 ρ2	d	1 d	1	d
≤ ʒ------= =-------而 log P ≤ -不 log P ≤ 不 log P
2qmin dt	qmin	dt	γ dt	γ(t0)	dt
as desired. Where the second inequality holds by multiple q-ρ- on the right hand of (B.23) and the
formula of 埠 in equation (B.13), and the last inequality holds since we have shown that 7(t) is
monotonically increasing in (B.2) and Y(t0) > 0 since L(t0) < log 2.
□
Let’s turn back to β, though We can not shoW directly that it increase to one, We can find a sequence
of time {tm} for each limit point such that β(tm) → 1
Lemma B.5. If there exist t0 such that L(θ(t0)) < log 2, then for every limit point θ of {θ(t) :
t ≥ 0}, there exists a sequence of time {tm > 0 : m ∈ N} such that tm → ∞, θ(tm) → θ, and
β (tm ) → 1.
Proof. Recall that in equation (B.19) we have shown that:
2
: log 7 ≥ 2 (d- log P) 1
dt	dt
^
dθ
dt
(B.24)
Since dlog P = 1 ddρ = 2p2 dρt^ = ρ12 @ d〉and:
dθ _ d θ
—=————
dt dt ∣∣θk
:(Pdθ -1 θθ>dθ) = 1(I-θθ>)dθ
P2 dt P dt P	dt
Plug them into (B.24) we have:
dt logγ ≥2
Iidtι∣2-qdtE d
中,给2成
log P=2(β-2 - 1)ddt log p∙
For any t2 > t1 > t0, integrate both sides from time t2 to t1 we have: logγ7(t2) - logγ7(t1) ≥
2 Rtt2 (β(t)-2 - 1) dt log Pdt. By the continuity of β we know there exist a time t such that:
log γ7(t2) - log γ7(t1) ≥ 2
t2
(β(t)-2
t1
d
-1) dt log Pd
2(β(t*)-2
t2 d
-1 Jti dt log Pdt
2(β(t*)-2 - 1)(logP(t2) - logP(tι)).
(B.25)
19
Published as a conference paper at ICLR 2022
By (B.9) We know that Y ≤ qm2n and the right hand is bounded, and the Y is monotonically
increasing, then there exist γ∞ such that γ(t) ↑ γ∞.
Now we are ready to construct the sequence oftm, first take a sequence of {m > 0, m ∈ N} such
that ∈m → 0. We construct tm, by induction, suppose we have already find tι < t2 < •一< tm-ι
satisfy our requirement, since θ is a limit point of {θ(t) : t > 0}, then we can find a time sm such
that:
kθ(sm) - 0∣ ≤ Cm, log 〜Y∞、≤ ^m.
γY(sm)	m
By the monotonicity and continuity ofρ we can find a time s0m such that logρ(s0m) -log ρ(sm) ≤ Cm.
Take t2 = s0m, t1 = sm in (B.25), there exist a time tm such that:
2(β(tm)-2 - 1) ≤
log Y(t2)- log Y(tl) ≤ 2
logρ(t2) - logρ(tι) — em
(B.26)
on the other hand, by Lemma B.4 we have:
..^ . 、 一.. .. ^ . 、 一.. .. ^ . 、 ^ .
∣∣θ(tm) - θk ≤∣∣θ(sm) - θk + ∣∣θ(sm) - θ(tm)∣∣
≤Cm
+ γ(to) (log ρ(tm) - log P(Sm)) ≤ (1 + γ(t0)"
m.
(B.27)
Note that <θ,华>> 0, then by definition we know β > 0. Combine equations (B.26) and (B.27) we
have β(tm) → 1 and θ(tmr) → θ as desired.	□
Now we are ready to prove Theorem 3.2:
ProofofTheorem3.2. By Lemma B.1, we know that once t > to,
(W(t), H(t))∕qmin(W(t), H(t)) is an Q2¾β(t)), KKin(n) -approximate KKT
point. We have shown that γY(t) > γY(t0) > 0 in Lemma B.2, qmin → ∞ in Lemma
B.3 and from Lemma B.5 we know for any limit point (W, H) of {(H(t), W(t)):=
(√W(t) H(TEt) kF, √W⅞⅛)}, there exists a sequence of time {tm > 0: m ∈ N}
such that tm → ∞,β(tm) → 1 and (H(tm), W(tm)) → (W, H). Then (W, H) is along the
direction of a limit point of a sequence of (C, δ)-approximate KKT point with C, δ → 0. On the other
hand, we can verify that the problem (3.1) satisfies MCFQ (A.3) by simply setting v = θ, then:
hVsk,i,j, θi = 2sk,i,j ≥ O.
Now by Theorem A.1 we know (W, H) is along the direction of a KKT point of problem (3.1) □
Theorem 3.2 characterize the convergent behaviour of gradient flow, under separable conditions the
limit point is along the direction of a KKT point of (3.1), next we show that the global minimum of
(3.1) must satisfy neural collapse conditions by proving Corollary 3.1:
Proof of Corollary 3.1. Since we have shown that the problem (3.1) satisfy MCFQ, then the KKT
conditions are necessary for global optimality, we only need to show the global optimum satisfies
neural collapse conditions. First the constraints in (3.1) can be transformed to be a single constraint
by the definition of margin:
∀k 6= j ∈ [K],i ∈ [n],	wk>hk,i - wj>hk,i ≥ 1. ⇔ qmin(W,H) ≥ 1.	(B.28)
Note that the margin is homogeneous:
qmin(αW, αH) =α2qmin(W,H),∀α∈R.
Then for any point (W, H) satisfies qmin(W, H) > 0, after an appropriate scaling α,
(αW, αH), ∀α2 ≥ 1/qmin(W, H) is feasible for (3.1). Take optimum among all scaling fac-
tor α we know the minimum norm is attained if and only if α2 = 1/qmin(W, H). And the optimum
norm is:
2IIaWIIF +1 IIaH||F =	W H (||w||F + ||H||F).
2	2	2qmin(W, H)
20
Published as a conference paper at ICLR 2022
Then by Theorem 3.1 we have:
2〃」W H(l∣wIlF + IHIlF) ≥ 2(K- i)√n.
2qmin (W , H)
And the global optimum is attained only when (W, H) satisfies neural collapse conditions □
C Omitted proofs from Section 3.2
To begin with, let’s finish the computation in the motivating example (Example 3.1)
Proof in Example 3.1. Consider the case where K = 4, n = 1, let (W, H) be the following point:
		1	-1	0	0
W=		-1	1	0	0
	H=	0	0	1	-1
		0	0	-1	1
One can easily verify that this (W, H) enables our model to classify all of the features perfectly.
Further more, we can show it is along the direction of a KKT point of the minimum-norm separation
problem (3.1) by construct the Lagrangian multiplier Λ = (λij)iK,j=1 as following:
1-21-20 O
1-21-20 O
O O 1-21-2
O O 1-21-2
=
Λ
To see this, just write down the corresponding Lagrangian (note that to make it to be a true KKT
point of (3.1), one needs to multiple 1/√2 on W, H):
4
1	1	4	11
L(W, H, Λ) = 4∣IWkF + 4 kH kF - E Eλij(2 Wihi- 2 Wjhi-I).
i=1j 6=i
Simply take derivatives for W, H and Λ we can find it satisfies KKT conditions. On the other hand,
the gradient of (W, H) is
VWL(W, H) = VHL(W, H) = - 2 + 2e-2 + 2e2
1	-1	0	0
-1	1	0	0
0	0	1	-1
0	0	-1	1
We can find that the directions of gradient and the parameter align with each other (i.e., W is parallel
to VWL(W, H), and H is parallel to VHL(W, H)), which implies that simple gradient descent
may gets stuck in this direction and only grows the parameters’ norm. However, ifwe construct:
		1+α	-1+α	α	α -
W0	一，	-1+α	1+α	α	α
	V 1 + 2ɑ2	-α	-α	1-α	-1 -α
		-α	-α	-1 -α	1-α
		1+α	-1+α	-α	-a -
H0	,	 -ʃɪ^	-1+α	1+α	-α	-α
	V 1 + 2ɑ2	α	α	1-α	-1 -α
		α	α	-1 -α	1-α
Note that kW0kF = kWkF, kH0kF = kHkF and IIW0 - W II2F + IIH0 -HII2F → 0asα → 0.
First we can compute:
		-2 + 4α2	4α2 - 2	-4α2	-4α2
W0H0	1	4α2 - 2	2+4α2	-4α2	-4α2
	― 1 + 2α2	-4α2	-4α2	2+4α2	4ɑ2 - 2
		-4α2	-4α2	4α2 - 2	2+4α2
21
Published as a conference paper at ICLR 2022
and:
L(W0, H0)
- 4 log
e
2
2α2 - 1	2α2
e2 + e 1+2α2 + 2e 1+2α2
Our aim is to show that for any e > 0, there exist α such that ∣ɑ∣ < e and L(W0, H0) < L(W, H).
2α2 - 1	_ 2 2α2
By the formulation of L(W0, H0), it,s sufficient to show that f (α)，e 1+2ɑ2 + 2e~ 1+2α2 < f (0).
Then take the derivative of f(α) we have:
4a2-2
f 0(α) = e 1+2α2
8α 8α (2α2 — 1)
---------------------―
1 + 2α2	(1 + 2α2)2
+ 2e
16α3
(1 + 2ɑ2)2
4a2
+ 2e 1+2α2
8α
1 + 2ɑ2
f00(α)
4a2 -2
e1+2a2
8α
8α(2a2 — 1)
16α3
8α
4a2-2
+ e1+2a2
-4a2
+ 2e- 1+2α2
1 + 2α2
(1 + 2a2)2
(1 + 2a2)2
1 + 2α2
64α2
-------2+ +
(1 + 2α2)2
64 2α2 — 1 α2
(1 + 2a2 )3
+ 1 + 2a2
8 2α2 — 1
(1 + 2a2)2
80α2
128α4
(1 + 2a2)2
1 + 2α2
(1 + 2a2)3
—
—
—
2
8
—
—
2
8
—
Now we can find that f0(0) = 0 and f0θ(0) = 16( * — 1) < 0. Since thefunction f (α) is continuously
twice differentiable, we can conclude that for any e > 0, we can choose appropriate α such that:
||W0||2F = ||W ||2F, ||H 0 ||2F = ||H||2F,
||W0—W||2F +||H0—H||2F < e, L(W0, H0) < L(W,H).
□
Now we prove Theorem 3.3 by some similar strategies as used in proving Theorem 3.1.
Proof of Theorem 3.3. Again we rewrite the ULPM objective by introducing sk,i,j = wk>hk,i —
wj>hk,i:
Kn
L(W,H)=XXlog(1+Xexp(—sk,i,j)).
k=1 i=1	j6=k
In addition, we can find that centralizing Wi does not change the value of sk,i,j. Let Wi = Wi —
K PPk=ι Wk, ∀i ∈ [K], then sk,i,j = Wkhk,i - wj hk,i = Wkhk,i - Wjhk,i and PPi=I Wi = 0∙
First by the strict convexity of ex and Jensen Inequality:
Kn
L(W,H) ≥ XXlog(1 + (K - 1)exp(K-I X—sk,i,j))
k=1 i=1	j6=k
=XX log(1 + (K - 1)exP(-Kwk Ti)).
K—1
k=1 i=1
(C.1)
Where the last equality is obtained from:
£ sk,i,j = W> WkhkL W>hk,i = (K — 1)W>hk,i - EWjrhk,i = KWkhk,i
j 6=k	j 6=k	j 6=k
22
Published as a conference paper at ICLR 2022
Now again by the strict convexity of log(1 + (K - 1) exp(-x)) and Jensen inequality, we have:
L(W,H)≥XK Xn log(1 + (K - 1)exp(-KK h：, ))
k=1 i=1	-
Kn
≥ nKIog(I +(K- 1)eχp(-n(K - 1)XXw>hk,i))
n	k=1 i=1
I Kn I
≥ nK Iog(I + (K - 1) exp(- 2n(K - 1)52 √nw kwkk2 + √nkhk,ik2))
≥ nK Iog(I +(K - 1)eχp(- 2√n(K - 1)(HW IlF + IIH IlF))).
(C.2)
Where the last inequality holds since ∣∣W ∣∣F = PK=I ∣∣wfc ∣2 ≥ PK=I Iw ∣∣2 - K ∣∣ PK=I Wk ∣∣2
PK=1 ∣Wk ∣2.
When all of the above inequality reduce to equality, we must have:
1.	PK=ι Wi = 0, Wi = Wi (the last inequality in (C.2))
2.	Wk = √nhk,i, ∀i ∈ [n] (the third inequality in (C.2))
3.	∣Wk∣ = ∣Wk0 ∣, ∣hk,i∣ = ∣hk0,j∣, ∀k, k0 ∈ [K],i,j ∈ [n] (the second inequality in (C.2))
4.	sk,i,j = W>hk,i - W>hk,i = KKTW>hk,i,∀k,j ∈ [K],i ∈ [n] (the first inequality in
(C.1))
These four conditions are exactly equivalent to neural collapse conditions and ∣W ∣F = ∣H∣F □
The global optimality is not enough to illustrate how does gradient flow converges to neural collapse
since there may exist some bad local minimum. We will provide the following second-order analysis
to eliminate spurious local minimum. First, define the cross-entropy loss on a matrix Z ∈ RK ×nK :
Kn
L(Z)=XX-log
k=1 i=1
ezk,i,j
PK=I ezk,i,l
where zk,i,j denote the j-th row and [(k - 1)K + i]-th column elements of Z. Then we have
L(W, H) = L(WH) . Now compute the gradient of L(Z) to each element:
∂L(Z) = -1+ zk,i,k
∂zk,i,k —	PK=I ezk,i,ι
臀=Pj, ∀k=j.
∂zk,i,j	lK=1 ezk,i,l
If U ∈ Rk satisfies u> RL(Z) = 0, denote Up as the maximum element of u, then We have:
0 = Up 臀 + X Uq 臀=up(-1 + PKp") + X Uq 六匕
∂zp,i,p	q6=p	∂zp,i,q	l=1 ezp,i,l	q6=p	l=1 ezp,i,l
=-X(Up-Uq) PKpYI ≤ O.
q6=p	l=1 ezp,i,l
(C.3)
Where the last inequality holds if and only if Uq = Up , ∀q ∈ [K]. Which indicates that the rank of
RL(Z) is K - 1 and U>RL(Z) = 0 ⇔ U = 1. NoW We are ready to prove Theorem 3.4 in the main
body.
Proof of Theorem 3.4. First compute the gradient of ULPM objective (B), by the chain rule We have:
RWL(W, H) =RL(WH)H>,RHL(W,H) = W>RL(WH).
23
Published as a conference paper at ICLR 2022
If there exist a vector (∆W, ∆H) ∈ T(W, H) such that hVwL(W, H), ∆Wi+
〈▽hL(W, H), ∆Hi = 0, moreover We can assume RWL(W, H), ∆Wi+
hVHL(W, H), ∆Hi < 0 since We can take the negative direction if the formula is greater than
zero, then by Taylor expansion:
L(W+δ∆W,H+δ∆H) = L(W, H) + δhVWL(W, H), ∆Wi + δhVHL(W, H), ∆Hi + O(δ2),
We knoW that (∆W , ∆H) satisfies our requirement.
NoW let’s discuss the case When:
hVWL(W,H),∆Wi + hVHL(W, H), ∆Hi = 0, ∀(∆W, ∆H) ∈ T(W,H),
by definition of T(W, H), it contains all vectors that are orthogonal to (W, H), so
(VW L(W, H), VHL(W, H)) is parallel to (W, H), that is, there exist λ such that:
VL(WH)H> = λW, W>VL(WH) = λH.	(C.4)
If there does not exist (∆W, ∆H) satisfying the requirement, We knoW that for any feasible curve
φ(t) = (W(t), H(t)) With φ(0) = (W, H) on the sphere S = {(W0,H0) : kW 0k2F + kH0k2F =
kW k2F + kH k2F}, t = 0 admits the local minimum of L(φ(t)) and thus:
d2
0 ≤ 2LL(Φ(t))∣t=o = φ0(0)TV2L(W, H)φo(0) + VL(W, H)φ00(O).	(C.5)
dt2
On the other hand, since the curve lies on the sphere S, denote h(W, H) = kW k2F + kH k2F , then
h(φ(t)) must stay as a constant, take tWice derivative We have:
d2
0 = 2h(h(φ(t))∣t=o = φo(0)τV2h(W, H)φo(0) + Vh(W, H)φ00(O).	(C.6)
dt2
Then sum these tWo conditions together, We have:
0 ≤ 条(L(°⑴)+P 据⑴))〜=φ0(O)T v2(L+τh)(W,H )φ0(O)+V(L+12|h)(W,H )φ00(o).
(C.7)
By equation (C.4) we know that V(L + 乎h)(W, H) = O Note that φ0(O) ∈ T(W, H) since
the curve lies on S and for any (∆W, ∆H) ∈ T(W, H) We can construct a curve φ(t) such that
φ0(O) = (∆W, ∆H). Then (C.7) indicates that ∀(∆W, ∆H) ∈ T(W, H) we have:
O ≤(∆W, ∆H)tV2L(W, H)(∆W, ∆H) + 与(∆W, ∆H)τV2h(W, H)(∆W, ∆H)
=(∆W, ∆H)tV2L(W, H)(∆W, ∆H) + ∣λ∣(k∆WkF + ∣∣∆H∣∣F).
(C.8)
When λ = O, by equation (C.3) we know that ∣∣VL(WH)k2 > O, which gives us ∣λ∣ ≤
kVL(WH)k2 When λ 6= O, combine the two equations in (C.4) we know:
λWτW = WτVL(WH)Hτ = λHHτ ⇒ WτW = HHτ ,	(C.9)
which further implies:
||W||F=||H||F,	||W||2 = ||H||2.
Thus we also have (Note that when W = H = O we can take λ to be zero):
VL(WH)HT = λW ⇒ ∣λ∣∣∣W∣∣2 ≤ ∣∣VL(WH)∣∣2∣∣H∣∣2
⇒ ∣λ∣ ≤ ∣∣VL(WH)∣∣2.
Now when ∣λ∣ < ∣∣VL(WH)∣∣2,we can show that it will contradict with (C.8): We have shown that
the rank of VL(Z) is K - 1, so by (C.4) and (C.9) there exist a vector a such that Wa = Hτa = O,
24
Published as a conference paper at ICLR 2022
let u and v are the left and right singular vectors corresponding to the largest singular value of
▽L(WH), construct ∆W = ua>, ∆H = -av>, then (∆W, ∆H) ∈ T(W, H) and:
(∆W, ∆H)>V2L(W, H)(∆W, ∆H) - λ(k∆WkF + k∆HkF)
=(W∆H + ∆WH)V2L (WH) (W∆H + ∆WH) + 2 {VL(WH), ∆W∆H〉+ ∣λ∣(∣∣∆W∣∣F + ∣∣∆H∣∣F)
≤2∣∣a∣∣2(∣λ∣ - u>VL(WH)v) < 0.
Then it only remains to analyze the ∣λ∣ = ||VL(WH)∣∣2 cases, construct another convex optimiza-
tion problem:
min L(Z) + ∣λ∣∣∣Z∣∣*,	(C.10)
suppose Z has SVD Z = UΣV>, as We know that the subgradient of ∣∣Z∣∣* can be written as (see
Watson (1992) for a proof):
∂∣Z∣* = {UV> + W, W ∈ RK×nK | U> W = 0, WV = 0, k W∣∣2 ≤ 1} .	(C.11)
On the other hand, we know that:
H>HH>H = H>W>WH = VΣ2V>
WW>WW> = WHH> W> = UΣ2U>,
which indicates that H>H = VΣV> and WW> = UΣU>. Combine them with (C.4) we have:
VL(WH)H>H = λWH ⇔ VL(WH)VΣV> = λUΣV>
⇔ VL(W H)V = λU
WW>VL(WH) = λWH ⇔ UΣU>VL(WH) = λUΣV>
⇔ U>VL(WH) = λV>.
Note that ∣λ∣ = ||VL(WH) ∣∣2, then by (C.11) we know that-VL(WH) ∈ ∣λ∣∂∣WH ∣∣*.Then
by the strict convexity of (C.10) we know WH is the global minimum of it. In addition, we have
k W kF + ∣H kF = 2trΣ2) = 2∣ WH ∣*. In addition, previous works (Haeffele & Vidal, 2015) have
shown that:
kZ k* = ,m^1 (kW kF + kH kF),
Z=WH 2
which is equivalent to:
kWHk* ≤ 2(kWkF + kHkF).
Now for any (W0, H0), we know that:
L(W, H) + 券(kWkF + kHkF) = L(WH) + ∣λ∣kWHk* ≤ L(W0H0) + N∣W0H0k*
≤L(W0, H0) + ≡(kW0kF + ∣h0kF),
which indicates (W, H) must attain global minimum of the following optimization problem:
min L(W, H) + ≡(∣∣WkF + kH∣F).
W,H	2
If (W, H) does not satisfy neural collapse conditions, by the optimality of neural collapse solution
(Theorem 3.3) we know there exists another point (W0, H0) such that L(W0, H0) < L(W, H) and
kWk2F+kHk2F= kW0k2F+kH0k2Fthus:
L(W, H) + 券(kWkF + kHkF) > L(W0, H0) + ≡(kW0kF + kHo∣F),
which contradicts with the global optimality of (W, H), thus (W, H) must satisfy all of the neural
collapse conditions and we finish the proof.	□
25
Published as a conference paper at ICLR 2022
D	Additional Empirical Results
Gradient Descent on the ULPM Objective. We conduct experiments on the ULPM objective (2.3)
to support the results of convergence toward neural collapse in our theories. We set N = 10, K = 5,
and d = 20, and used gradient descent with a learning rate of 5 to run 105 epochs. We characterize
the dynamics of the training procedure in Figure 2 based on four aspects: (1) Relative variation of the
centered class-mean feature norms (i.e., Std(∣∣hk - hk)∕Avg(∣∣hk - h∣∣)) and the variation of the
classifier,s norms (i.e., Std(∣∣w k k)∕Avg(∣∣W k ∣∣)). (2) Within-class variation of the last layer features
(i.e., Avg(∣∣hk,i - hkk)∕Avg(∣hk,i - h∣∣)). (3) The cosines between pairs of last layer features (i.e.,
Avg(∣ cos(hk, hk') + 1∕(K -1)∣)) and that ofthe classifiers (i.e., Avg(∣ Cos(Wk, Wk，) + 1/(K -1)∣)).
(4) The distance between the normalized centered classifier and the normalized last layer feature (i.e.,
Avg(∣(hk - h)∕∣∣hk - hk - Wk∕∣Iwkk∣)). Empirically, We observe that all four quantities decrease
at approximately the rate O(1/(log(t))).
E - pb
4 × IO-Z
3 X lQ-z
IO1 IO2 IO3IOlO5
Epoch
(a) Variation of the norms
VUCVkVt=Vsn 一 OSqe
IO1 IO2 10310¾05
Epoch
(b) Within-class variation
VC-OTOU pθe≡s
6 × IoT
4 × IO-2
IO1 IO2 IO3IOlO5
Epoch
(c) Cosines between pairs
VUCVkVt=Vsn 一 OSqe
× IOT
Io-I
1 IO1 IO2 IO3IOlO5
Epoch
(d) Self-duality
Figure 2:	Training dynamics in ULPM. The x-axis in the figures is set to have log(log(t)) scales,
and the y-axis in the figures are set to have log scales. (a) The dynamics of the variation of the
centered class-mean features’ norms (shown in blue) and the variation of the classifier’s norms
(shown in red). We observe that the logarithm of both terms decreases at a rate O(1/(log(t))). (b)
Dynamics of within-class variation of the last layer features. The logarithm of the variation converges
at approximately the rate O(1/ log(t))). (c) The dynamics of the cosines between pairs of last layer
features (shown in blue) and those of the classifiers (shown in red). The logarithm of both terms
converge approximately at rate O(1/ log(t))). (d) Dynamics of the distance between the normalized
centered classifier and normalized last layer feature. The logarithm of the quantity converges at
approximately the rate O(1/ log(t))) to the point of self-duality.
Details of Realistic Training. In the real data experiments, we trained the VGG-13 (Simonyan
& Zisserman, 2014) and ResNet18 (He et al., 2016) on MNIST (LeCun et al., 1998), KMNIST
(Clanuwat et al., 2018), FashionMNIST (Xiao et al., 2017) and CIFAR-10 datasets (Krizhevsky et al.,
2009) without weight decay, and with a learning rate of 0.01, momentum of 0.3, and batch size of
128. The metrics are defined similarly to the ULPM case and the experiment results are reported in
Figure 1, 3, 4, 5, 6, 7, and 8. All experiments were run in Python (version 3.6.9) on Google Colab. It
8uaJ£PSln-OSqe ?4
Ue-SQ pβu-s
8uaJ£PSln-OSqe
(d) Self-duality
io® ιo1 ιoj
Epoch
(a) Variation of the norms
(b) Within-class variation
(c) Cosines between pairs

Figure 3:	Experiments on real datasets without weight decay. We trained a VGG13 on CIFAR10 dataset. The
x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are set to have log scales.
26
Published as a conference paper at ICLR 2022
1 6λwbs
8uaJ£PSln-OSqe ?4
ΘUIS8 pau-s?<
8uaJ£PSln-OSqe ?4
ιo1 ιoj
Epoch
(d) Self-duality
(a) Variation of the norms
(b) Within-class variation
(c) Cosines between pairs
Figure 4:	Experiments on real datasets without weight decay. We trained a VGG13 on the MNIST dataset. The
x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are set to have log scales.
8uaJ£Psln-OSqe ?4
Ue-SQ pβu-s?<
8uaJ£Psln-OSqe ?4
io® ιo1 ιoj
Epoch
(d) Self-duality
(a) Variation of the norms
(b) Within-class variation
(c) Cosines between pairs
Figure 5:	Experiments on real datasets without weight decay. We trained a VGG18 on the KMNIST dataset.
The x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are set to have log scales.
6>wys
Classlfler(KMNlST)
Last layer feature (KMNlST)
8uaJ姮一Psln-OSqe ?4
Ue-Hou pav-s?<
8uaJ姮一Psln-OSqe ?4
io® ιo1 ιoj
Epoch
(d) Self-duality
(a) Variation of the norms
(b) Within-class variation
(c) Cosines between pairs
Figure 6:	Experiments on real datasets without weight decay. We trained a ResNet18 on the KMNIST dataset.
The x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are set to have log scales.
6>wys
əɔuaj姮一Psln-OSqe ?4
CIassIfler(FMNlST)
Last layer feature (FMNlST)
(a) Variation of the norms
(b) Within-class variation
əu-soə pavzs
(c) Cosines between pairs
əɔuaj姮一Psln-OSqe
ιoo ιo1 ιoj
Epoch
(d) Self-duality
Figure 7:	Experiments on real datasets without weight decay. We trained a VGG13 on the Fashion-MNIST
dataset. The x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are set to have
log scales.
27
Published as a conference paper at ICLR 2022
6>wys
α3uajα=psmosqe ?4
αu-s∞ P9uzs ?<
(a) Variation of the norms
(b) Within-class variation
(c) Cosines between pairs
α3uajα=psmosqe ?4
IO1
Epoch
10j
(d) Self-duality
Figure 8:	Experiments on real datasets without weight decay. We trained a ResNet18 on the Fashion-MNIST
dataset. The x-axis in the figures are set to have log(log(t)) scales and the y-axis in the figures are set to have
log scales.
should be mentioned that we can observe that the variation of classifier norm stays at a low value
and do not decrease in some settings (e.g., Figure 7a and Figure 8a), this phenomenon is also found
in Figure 2 of (Papyan et al., 2020), which might attribute to the network architecture (e.g., batch
normalization) and characteristics of real-world datasets. Overall, we can find that neural collapse
occurs for various network architectures and datasets under an unconstrained setting, which provides
sound support for our theory.
E Limitation and future directions
Currently, our analysis is still limited in the terminal phase of training and only studies the training
behavior after data is perfectly separated. Such assumption is necessary for all implicit regularization
analysis (Lyu & Li, 2019; Ji et al., 2020; Nacson et al., 2019a) under a nonlinear setting, and how
to fully characterize the early time training dynamics is still an open problem. Moreover, we have
provided a loss landscape analysis in the paper showing that there is no spurious minimum in the
tangent space. To guarantee the training dynamics do not get stuck in saddle points and converge
to neural collapse solution, we need to introduce randomness in the training dynamics (e.g. use
stochastic gradient descent (Ge et al., 2015)), otherwise simple gradient flow may get stuck in saddle
points as we have shown in Example 3.1. How to characterize the training dynamics of stochastic
gradient descent in the nonlinear setting is also an open problem. It is an interesting direction to
further build a complete characterization of convergence to neural collapse solutions by addressing
the early time training dynamics and studying the stochastic gradient descent.
28