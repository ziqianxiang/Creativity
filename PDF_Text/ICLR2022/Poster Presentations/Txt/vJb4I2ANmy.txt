Published as a conference paper at ICLR 2022
Noisy Feature Mixup
Soon Hoe Lim*	N. Benjamin Erichson*
Nordita,	University of Pittsburgh
KTH and Stockholm University	erichson@pitt.edu
soon.hoe.lim@su.edu
Francisco Utrera
University of Pittsburgh
and ICSI
utrerf@berkeley.edu
Winnie Xu
University of Toronto
winniexu@cs.toronto.edu
Michael W. Mahoney
ICSI and UC Berkeley
mmahoney@stat.berkeley.edu
Ab stract
We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method
for data augmentation that combines the best of interpolation based training and
noise injection schemes. Rather than training with convex combinations of pairs of
examples and their labels, we use noise-perturbed convex combinations of pairs
of data points in both input and feature space. This method includes mixup and
manifold mixup as special cases, but it has additional advantages, including better
smoothing of decision boundaries and enabling improved model robustness. We
provide theory to understand this as well as the implicit regularization effects of
NFM. Our theory is supported by empirical results, demonstrating the advantage
of NFM, as compared to mixup and manifold mixup. We show that residual
networks and vision transformers trained with NFM have favorable trade-offs
between predictive accuracy on clean data and robustness with respect to various
types of data perturbation across a range of computer vision benchmark datasets.
1 Introduction
Mitigating over-fitting and improving generalization on test data are central goals in machine learning.
One approach to accomplish this is regularization, which can be either data-agnostic or data-dependent
(e.g., explicitly requiring the use of domain knowledge or data). Noise injection is a typical example
of data-agnostic regularization (Bishop, 1995), where noise can be injected into the input data
(An, 1996), or the activation functions (Gulcehre et al., 2016), or the hidden layers of deep neural
networks (Camuto et al., 2020; Lim et al., 2021). Data augmentation constitutes a different class
of regularization methods (Baird,1992; ChaPelle et al., 2001; DeCoste & Scholkopf, 2002), which
can also be either data-agnostic or data-dependent. Data augmentation involves training a model
with not just the original data, but also with additional data that is properly transformed, and it has
led to state-of-the-art results in image recognition (CireSan et al., 2010; Krizhevsky et al., 2012).
The recently-proposed data-agnostic method, mixup (Zhang et al., 2017), trains a model on linear
interpolations of a random pair of examples and their corresponding labels, thereby encouraging
the model to behave linearly in-between training examples. Both noise injection and mixup have
been shown to impose smoothness and increase model robustness to data perturbations (Zhang
et al., 2020; Carratino et al., 2020; Lim et al., 2021), which is critical for many safety and sensitive
applications (Goodfellow et al., 2018; Madry et al., 2017).
In this paper, we propose and study a simple yet effective data augmentation method, which we call
Noisy Feature Mixup (NFM). This method combines mixup and noise injection, thereby inheriting
the benefits of both methods, and it can be seen as a generalization of input mixup (Zhang et al.,
2017) and manifold mixup (Verma et al., 2019). When compared to noise injection and mixup, NFM
imposes regularization on the largest natural region surrounding the dataset (see Fig. 1), which may
help improve robustness and generalization when predicting on out of distribution data. Conveniently,
NFM can be implemented on top of manifold mixup, introducing minimal computation overhead.
* Equal contribution
1
Published as a conference paper at ICLR 2022
Contributions. Our main contributions are as follows.
•	We study NFM via the lens of implicit regulariza-
tion, showing that NFM amplifies the regularizing
effects of manifold mixup and noise injection, im-
plicitly reducing the feature-output Jacobians and
Hessians according to the mixing level and noise
levels (see Theorem 1).
•	We provide mathematical analysis to show that
NFM can improve model robustness when com-
pared to manifold mixup and noise injection. In
particular, we show that, under appropriate assump-
tions, NFM training approximately minimizes an
upper bound on the sum of an adversarial loss and
feature-dependent regularizers (see Theorem 2).
λx1 + (1 - λ)x2
♦---------------•---------•
Xi	X2
λx01 + (1 - λ)x02
Figure 1: An illustration of how two
data points, x1 and x2, are transformed
in mixup (top) and noisy feature mixup
(NFM) with S := {0} (bottom).
•	We provide empirical results in support of our theoretical findings, showing that NFM improves
robustness with respect to various forms of data perturbation across a wide range of state-of-the-
art architectures on computer vision benchmark tasks.
In the Supplementary Materials (SM), we provide proofs for our theorems along with additional
theoretical and empirical results to gain more insights into NFM. In particular, we show that NFM
can implicitly increase classification margin (see Proposition 1 in SM C) and the noise injection
procedure in NFM can robustify manifold mixup in a probabilistic sense (see Theorem 5 in SM D).
We also provide and discuss generalization bounds for NFM (see Theorem 6 and 7 in SM E).
Notation. I denotes identity matrix, [K] := {1, . . . , K}, the superscript T denotes transposition, ◦
denotes composition, denotes Hadamard product, 1 denotes the vector with all components equal
one. For a vector v, vk denotes its kth component and kv kp denotes its lp norm for p > 0. conv(X)
denote the convex hull of X. Mλ(a, b) := λa + (1 - λ)b, for random variables a, b, λ. δz denotes the
Dirac delta function, defined as δz(x) = 1 if x = z and δz(x) = 0 otherwise. 1A denotes indicator
function of the set A. For α,β > 0, Dλ ：= α¾Beta(α + 1,β) + αββBeta(β + 1, α) denotes
a uniform mixture of two Beta distributions. For two vectors a, b, cos(a, b) := ha, bi/kak2 kbk2
denotes their cosine similarity. N(a, b) is a Gaussian distribution with mean a and covariance b.
2	Related Work
Regularization. Regularization refers to any technique that reduces overfitting in machine learning;
see (Mahoney & Orecchia, 2011; Mahoney, 2012) and references therein, in particular for a discussion
of implicit regularization, a topic that has received attention recently in the context of stochastic
gradient optimization applied to neural network models. Traditional regularization techniques such
as ridge regression, weight decay and dropout do not make use of the training data to reduce the
model capacity. A powerful class of techniques is data augmentation, which constructs additional
examples from the training set, e.g., by applying geometric transformations to the original data
(Shorten & Khoshgoftaar, 2019). A recently proposed technique is mixup (Zhang et al., 2017), where
the examples are created by taking convex combinations of pairs of inputs and their labels. Verma
et al. (2019) extends mixup to hidden representations in deep neural networks. Subsequent works
by Greenewald et al. (2021); Yin et al. (2021); Engstrom et al. (2019); Kim et al. (2020a); Yun et al.
(2019); Hendrycks et al. (2019) introduce different variants and extensions of mixup. Regularization
is also intimately connected to robustness (Hoffman et al., 2019; Sokolic et al., 2017; Novak et al.,
2018; Elsayed et al., 2018; Moosavi-Dezfooli et al., 2019). Adding to the list is NFM, a powerful
regularization method that we propose to improve model robustness.
Robustness. Model robustness is an increasingly important issue in modern machine learning.
Robustness with respect to adversarial examples (Kurakin et al., 2016) can be achieved by adversarial
training (Goodfellow et al., 2014; Madry et al., 2017; Utrera et al., 2020). Several works present
theoretical justifications to observed robustness and how data augmentation can improve it (Hein &
Andriushchenko, 2017; Yang et al., 2020b; Couellan, 2021; Pinot et al., 2019a; 2021; Zhang et al.,
2020; 2021; Carratino et al., 2020; Kimura, 2020; Dao et al., 2019; Wu et al., 2020; Gong et al.,
2020; Chen et al., 2020). Relatedly, Fawzi et al. (2016); Franceschi et al. (2018); Lim et al. (2021)
2
Published as a conference paper at ICLR 2022
investigate how noise injection can be used to improve robustness. Parallel to this line of work, we
provide theory to understand how NFM can improve robustness. Also related is the study of the
trade-offs between robustness and accuracy (Min et al., 2020; Zhang et al., 2019; Tsipras et al., 2018;
Schmidt et al., 2018; Su et al., 2018; Raghunathan et al., 2020; Yang et al., 2020a).
3	Noisy Feature Mixup
Noisy Feature Mixup is a generalization of input mixup (Zhang et al., 2017) and manifold mixup
(Verma et al., 2019). The main novelty of NFM against manifold mixup lies in the injection of noise
when taking convex combinations of pairs of input and hidden layer features. Fig. 1 illustrates, at
a high level, how this modification alters the region in which the resulting augmented data resides.
Fig. 2 shows that NFM is most effective at smoothing the decision boundary of the trained classifiers;
compared to noise injection and mixup alone, it imposes the strongest smoothness on this dataset.
Formally, we consider multi-class classification with K labels. Denote the input space by X ⊂ Rd
and the output space by Y = RK. The classifier, g, is constructed from a learnable map f : X → RK,
mapping an input x to its label, g(x) = arg maxk fk (x) ∈ [K]. We are given a training set,
Zn := {(xi, yi)}in=1, consisting of n pairs of input and one-hot label, with each training pair
zi := (xi, yi) ∈ X × Y drawn i.i.d. from a ground-truth distribution D. We consider training a deep
neural network f := fk ◦ gk, where gk : X → gk (X) maps an input to a hidden representation at
layer k, and fk : gk(X) → gL(X) := Y maps the hidden representation to a one-hot label at layer L.
Here, gk(X) ⊂ Rdk for k ∈ [L], dL := K, g0(x) = x and f0(x) = f (x).
Training f using NFM consists of the following steps:
1.	Select a random layer k from a set, S ⊂ {0} ∪ [L], of eligible layers in the neural network.
2.	Process two random data minibatches (x, y) and (x0, y0) as usual, until reaching layer k. This
gives us two immediate minibatches (gk (x), y) and (gk (x0), y0).
3.	Perform mixup on these intermediate minibatches, producing the mixed minibatch:
(gk, y) := (Mλ(gk(X), gk(XO)), Mλ(y, yO)),	⑴
where the mixing level λ 〜 Beta(α, β), with the hyper-parameters α,β > 0.
4.	Produce noisy mixed minibatch by injecting additive and multiplicative noise:
(gk,y) :=((1 + σmuitξmult) Θ Mλ(gk(x),gk(x0)) + σaddξkdd,Mλ(y,y0)),	⑵
where the ξkadd and ξkmult are Rdk -valued independent random variables modeling the additive
and multiplicative noise respectively, and σadd , σmult ≥ 0 are pre-specified noise levels.
5.	Continue the forward pass from layer k until the output using the noisy mixed minibatch (ggk, yg).
6.	Compute the loss and gradients that update all the parameters of the network.
Baseline (85.5%).	Dropout (87.0%). Weight decay (88.0%). Noise injections (87.0%).
Mixup (84.5%). ManifoldmixuP (88.5%). NoiSymixuP (89.0%). NFM (90.0%).
Figure 2: The decision boundaries and test accuracy (in parenthesis) for different training schemes on
a toy dataset in binary classification (see Subsection F.2 for details).
3
Published as a conference paper at ICLR 2022
At the level of implementation, following (Verma et al., 2019), we backpropagate gradients through
the entire computational graph, including those layers before the mixup layer k.
In the case where σadd = σmult = 0, NFM reduces to manifold mixup (Verma et al., 2019). If in
addition S = {0}, it reduces to the original mixup method (Zhang et al., 2017). The main difference
between NFM and manifold mixup lies in the noise injection of the fourth step above. Note that
NFM is equivalent to injecting noise into gk(x), gk(x0) first, then performing mixup on the resulting
pair, i.e., the order that the third and fourth steps occur does not change the resulting noisy mixed
minibatch. For simplicity, we have used the same mixing level, noise distribution, and noise levels
for all layers in S in our formulation.
Within the above setting, we consider the expected NFM loss:
LNFM (f ) = E(χ,y),(χ0,y0)〜D Ek 〜S E λ 〜石亡右。(仪展 ) E ξ 七〜Qlfk (Mλ壬k	(Xhgk 3^,^3^9,
where l : RK × RK → [0, ∞) is a loss function (note that here we have suppressed the dependence
of both l and f on the learnable parameter θ in the notation), ξk := (ξkadd, ξkmult) are drawn from
some probability distribution Q with finite first two moments, and
Mλ,ξk(gk(x),gk(x0)) := (1 + σmultξkmult)	Mλ(gk(x), gk(x0)) + σaddξkadd.
NFM seeks to minimize a stochastic approximation of LNFM (f) by sampling a finite number of
k, λ, ξk values and using minibatch gradient descent to minimize this loss approximation.
4	Theory
In this section, we provide mathematical analysis to understand NFM. We begin with formulating
NFM in the framework of vicinal risk minimization and interpreting NFM as a stochastic learning
strategy in Subsection 4.1. Next, we study NFM via the lens of implicit regularization in Subsection
4.2. Our key contribution is Theorem 1, which shows that minimizing the NFM loss function is
approximately equivalent to minimizing a sum of the original loss and feature-dependent regularizers,
amplifying the regularizing effects of manifold mixup and noise injection according to the mixing and
noise levels. In Subsection 4.3, we focus on demonstrating how NFM can enhance model robustness
via the lens of distributionally robust optimization. The key result of Theorem 2 shows that NFM
loss is approximately the upper bound on a regularized version of an adversarial loss, and thus
training with NFM not only improves robustness but can also mitigate robust over-fitting, a dominant
phenomenon where the robust test accuracy starts to decrease during training (Rice et al., 2020).
4.1	NFM: Beyond Empirical Risk Minimization
The standard approach in statistical learning theory (Bousquet et al., 2003) is to select a hypothesis
function f : X → Y from a pre-defined hypothesis class F to minimize the expected risk with
respect to D and to solve the risk minimization problem: inff ∈f R(f) := E(χ,y)〜D [l(f (x),y)], for
a suitable choice of loss function l. In practice, we do not have access to the ground-truth distribution.
Instead, we find an approximate solution by solving the empirical risk minimization (ERM) problem,
in which case D is approximated by the empirical distribution Pn = 1 pn=1 δzi. In other words, in
ERM we solve the problem: inff ∈f Rn(f ):= n Pn=1 l(f(χi),yi).
However, when the training set is small or the model capacity is large (as is the case for deep neural
networks), ERM may suffer from overfitting. Vicinal risk minimization (VRM) is a data augmentation
principle introduced in (Vapnik, 2013) that goes beyond ERM, aiming to better estimate expected
risk and reduce overfitting. In VRM, a model is trained not simply on the training set, but on samples
drawn from a vicinal distribution, that smears the training data to their vicinity. With appropriate
choices for this distribution, the VRM approach has resulted in several effective regularization
schemes (Chapelle et al., 2001). Input mixup (Zhang et al., 2017) can be viewed as an example of
VRM, and it turns out that NFM can be constructed within a VRM framework at the feature level (see
Section A in SM). On a high level, NFM can be interpreted as a random procedure that introduces
feature-dependent noise into the layers of the deep neural network. Since the noise injections are
applied only during training and not inference, NFM is an instance of a stochastic learning strategy.
Note that the injection strategy of NFM differs from those of An (1996); Camuto et al. (2020); Lim
4
Published as a conference paper at ICLR 2022
et al. (2021). Here, the structure of the injected noise differs from iteration to iteration (based on the
layer chosen) and depends on the training data in a different way. We expect NFM to amplify the
benefits of training using either noise injection or mixup alone, as will be shown next.
4.2	Implicit Regularization of NFM
We consider loss functions of the form l(f (x), y) := h(f (x)) - yf (x), which includes standard
choices such as the logistic loss and the cross-entropy loss, and recall that f := fk ◦ gk . Denote
Lntd ：= n Pn=ι l(f (xi), yi) and let Dx be the empirical distribution of training samples {xi }i∈[n].
We shall show that NFM exhibits a natural form of implicit regularization, i.e., regularization imposed
implicitly by the stochastic learning strategy, without explicitly modifying the loss.
Let > 0 be a small parameter. In the sequel, we rescale 1 - λ 7→ (1 - λ), σadd 7→ σadd,
σmuit → eσmuit, and denote Nk f and Vj- f as the first and second directional derivative of fk with
respect to gk respectively, for k ∈ S . By working in the small parameter regime, we can relate the
NFM empirical loss LnNFM to the original loss Lsntd and identify the regularizing effects of NFM.
Theorem 1. Let > 0 be a small parameter, and assume that h and f are twice differentiable. Then,
LNFM = Ek〜SLNFM(k), where
LNFM (k) = Lntd + eRik) + e2R* + e2R^') + e”(0,	(3)
with Rk = Rk+σluRddM+ 丸UltRiand R* = R*+ 藁Rdd⑹ + %UltRmM⑻
where
1n
Radd(k) =	E h，,(f (Xi))Vkf(gk(xi))T Egk [ξadd(ξadd)T]vk f 做(引)，	⑷
2n
i=1
1n
RmUlItkk =百 ∑>00(f (Xi))Vkf (gk(χi))T (Eξk [ξmuit(ξmult)T ] θ gk (Xi) gk (Xi)T)vk 〃gk ⑷),
n i=1
(5)
1n
Raddtk) = 2n E(h0(f (χi)) - yig% [(ξadd)Tvkf (gk(χi))ξadd],	⑹
n i=1
1n
RmUlttk) = 2n∑(h/(f(χi)) - yi)Eξk[(ξmult θgk(χi))TVkf(gk(χi))(ξmult θgk(xi))].⑺
n i=1
Here, R1k , Rjk and R3k are the regularizers associated with the loss of manifold mixup (see Theorem 3
in SM for their explicit expression), and 夕 is SomefUnction such that lime→o 夕(e) = 0.
Theorem 1 implies that, when compared to manifold mixup, NFM introduces additional smoothness,
regularizing the directional derivatives, Vkf(gk(Xi)) and Vjk f (gk (Xi)), with respect to gk (Xi),
according to the noise levels σadd and σmult, and amplifying the regularizing effects of manifold
mixup and noise injection. In particular, making Vj f(Xi) small can lead to smooth decision
boundaries (at the input level), while reducing the confidence of model predictions. On the other hand,
making the Vkf(gk(Xi)) small can lead to improvement in model robustness, which we discuss next.
4.3	Robustness of NFM
We show that NFM improves model robustness. We do this by considering the following three lenses:
(1) implicit regularization and classification margin; (2) distributionally robust optimization; and (3)
a probabilistic notion of robustness. We focus on (2) in the main paper. See Section C-D in SM and
the last paragraph in this subsection for details on (1) and (3).
We now demonstrate how NFM helps adversarial robustness. By extending the analysis of Zhang
et al. (2017); Lamb et al. (2019), we can relate the NFM loss function to the one used for adversarial
training, which can be viewed as an instance of distributionally robust optimization (DRO) (Kwon
et al., 2020; Kuhn et al., 2019; Rahimian & Mehrotra, 2019) (see also Proposition 3.1 in (Staib &
Jegelka, 2017)). DRO provides a framework for local worst-case risk minimization, minimizing
supremum of the risk in an ambiguity set, such as in the vicinity of the empirical data distribution.
5
Published as a conference paper at ICLR 2022
Following (Lamb et al., 2019), we consider the binary cross-entropy loss, setting h(z) = log(1 + ez),
with the labels y taking value in {0, 1} and the classifier model f : Rd → R. In the following, we
assume that the model parameter θ ∈ Θ := {θ : yif(xi) + (yi - 1)f(xi) ≥ 0 for all i ∈ [n]}. Note
that this set contains the set of all parameters with correct classifications of training samples (before
applying NFM), since {θ : 1{f(xi)≥0} = yi for all i ∈ [n]} ⊂ Θ. Therefore, the condition of θ ∈ Θ
is satisfied when the model classifies all labels correctly for the training data before applying NFM.
Since, in practice, the training error often becomes zero in finite time, we study the effect of NFM on
model robustness in the regime of θ ∈ Θ.
Working in the data-dependent parameter space Θ, we have the following result.
Theorem 2. Let θ ∈ Θ := {θ : yif (Xi) + (yi - 1)f(xi) ≥ 0 for all i ∈ [n]} such that Rk f(gk (Xi))
and Vk f(gk (Xi)) exist for all i ∈ [n], k ∈ S. Assume that fk (gk (Xi)) = Rkf (gk (Xi))T gk (Xi),
R2kf(gk(Xi)) = 0 for all i ∈ [n], k ∈ S. In addition, suppose that kRf (Xi)k2 > 0 for all i ∈ [n],
Er〜Dx[gk(r)] = 0 and Ilgk(Xi)Il2 ≥ CXk√dkforall i ∈ [n], k ∈ S. Then,
1n
LnFM ≥ - V max - l(f (Xi + δi), yi) + Lneg + e2φ(e),	(8)
n	n i=1 kδik2≤imix	n
where FmiX ：= FE - [1 _ λ] ∙ Ej Q hr(k)c(k) k^kf Sk(Xi))k2 ʌ/ɪi and Lreg ：=
Where	Fi	：=	feX-Di	[1	λ]	Ek 〜S	r i	cx	IHf(Xi)k2	Vdk and Ln	:一
2n Pn=1 ∣h00(f(Xi))l(Freg)2, with Tiikk = | Cos(Vkf (gk (Xi)), gk (Xi)) | and
(Fireg)2 := F2IRkf(gk(Xi))I22 Eλ[(--λ)]2EXr[Igk(Xr)I22cos(Rkf(gk(Xi)),gk(Xr))2]
+σa2ddEξk[IξkaddI22cos(Vkf(gk(Xi)),ξkadd)2]
+σm2ultEξk[Iξkmultgk(Xi)I22cos(Vkf(gk(Xi)),ξkmultgk(Xi))2] ,	(9)
and φ is some function such that lim→0 φ(F) = 0.
The second assumption stated in Theorem 2 is similar to the one made in Lamb et al. (2019); Zhang
et al. (2020), and is satisfied by linear models and deep neural networks with ReLU activation function
and max-pooling. Theorem 2 shows that the NFM loss is approximately an upper bound of the
adversarial loss with l2 attack of size FmiX = mini∈[n] FimiX, plus a feature-dependent regularization
term Lrneg (see SM for further discussions). Therefore, we see that minimizing the NFM loss not
only results in a small adversarial loss, while retaining the robustness benefits of manifold mixup, but
it also imposes additional smoothness, due to noise injection, on the adversarial loss. The latter can
help mitigate robust overfitting and improve test performance (Rice et al., 2020; Rebuffi et al., 2021).
NFM can also implicitly increase the classification margin (see Section C of SM). Moreover, since
the main novelty of NFM lies in the introduction of noise injection, it would be insightful to isolate
the robustness boosting benefits of injecting noise on top of manifold mixup. We demonstrate these
advantages via the lens of probabilistic robustness in Section D of SM.
5	Empirical Results
In this section, we study the test performance of models trained with NFM, and examine to what
extent NFM can improve robustness to input perturbations. We demonstrate the tradeoff between
predictive accuracy on clean and perturbed test sets. We consider input perturbations that are common
in the literature: (a) white noise; (b) salt and pepper; and (c) adversarial perturbations (see Section F).
We evaluate the average performance of NFM with different model architectures on CIFAR-
10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), ImageNet (Deng et al., 2009), and CIFAR-
10c (Hendrycks & Dietterich, 2019). We use a pre-activated residual network (ResNet) with depth
18 (He et al., 2016) on small scale tasks. For more challenging tasks, we consider the performance of
wide ResNet-18 (Zagoruyko & Komodakis, 2016) and ResNet-50 architectures, respectively.
Baselines. We evaluate against related data augmentation schemes that have shown performance
improvements in recent years: mixup (Zhang et al., 2017); manifold mixup (Verma et al., 2019);
6
Published as a conference paper at ICLR 2022
cutmix (Yun et al., 2019); puzzle mixup (Kim et al., 2020b); and noisy mixup (Yang et al., 2020b).
Further, we compare to vanilla models trained without data augmentation (baseline), models trained
with label smoothing, and those trained on white noise perturbed inputs.
Experimental details. All hyperparameters are consistent with those of the baseline model across
the ablation experiments. In the models trained on the different data augmentation schemes, we keep
α fixed, i.e., the parameter defining Beta(α, α), from which the λ parameter controlling the convex
combination between data point pairs is sampled. Across all models trained with NFM, we control
the level of noise injections by fixing the additive noise level to σadd = 0.4 and multiplicative noise
to σmult = 0.2. To demonstrate the significant improvements on robustness upon the introduction of
these small input perturbations, we show a second model (‘*’) that was injected with higher noise
levels (i.e., σadd = 1.0, σmult = 0.5). See SM (Section F.5) for further details and comparisons
against NFM models trained on various other levels of noise injections.
5.1	CIFAR 1 0
Pre-activated ResNet-18. Table 1 summarizes the performance improvements and indicates a
consistent robustness across different α values. The model trained with NFM outperforms the
baseline model on the clean test set, while being more robust to input perturbations (Fig. 3; left). This
advantage is also displayed in the models trained with mixup and manifold mixup, though in a less
pronounced way. Notably, the NFM model is also robust to salt and pepper perturbations and could
be significantly more so by further increasing the noise levels (Fig. 3; right).
5.2	CIFAR- 1 00
Wide ResNet-18. Previous work indicates that data augmentation has a positive effect on performance
for this dataset (Zhang et al., 2017). Fig. 4 (left) confirms that mixup and manifold mixup improve
the generalization performance on clean data and highlights the advantage of data augmentation.
The NFM training scheme is also capable of further improving the generalization performance. In
5
9
0 5 0
9 8 8
EJn。。VjSəj
75 0.00
0.05 0.10 0.15 0.20 0.25 0.30 0.35
White Noise (σ)
-+- Baseline、
Mixup
-∙- CutMix
-∙- PuzzIeMix
Noisy Mixup
Manifold Mixup ∖
NFM (*)
-+- NFM
0.02
0.04	0.06	0.08
Salt and Pepper Noise (Y)
-+- Baseline
Mixup
-∙- CutMix
-∙- PuzzIeMix
Noisy Mixup
Manifold Mixup
-∙- NFM (*)
-*- NFM
Figure 3: Pre-actived ResNet-18 evaluated on CIFAR-10 with different training schemes. Shaded
regions indicate one standard deviation about the mean. Averaged across 5 random seeds.
Table 1: Robustness of ResNet-18 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on CIFAR-10. The results are averaged over 5 models trained with different seed values.
Scheme	Clean (%)	σ (%)	Y (%)
0.1	0.2	0.3 I 0.02 0.04	0.1
Baseline	94.6	90.4 76.7	56.3	86.3 76.1	55.2
Baseline + Noise	94.4	94.0 87.5 71.2	89.3	82.5 64.9
Baseline + Label Smoothing	95.0	91.3 77.5	56.9	87.7 79.2 60.0
Mixup (α = 1.0) Zhang et al. (2017)	95.6	93.2 85.4 71.8	87.1	76.1	55.2
CutMix Yun et al. (2019)	96.3	86.7 60.8 32.4	90.9 81.7	54.7
PuzzleMix Kim et al. (2020b)	96.3	91.7 78.1	59.9	91.4 81.8 54.4
Manifold Mixup (α = 1.0) Verma et al. (2019)	95.7	92.7	82.7 67.6	88.9 80.2 57.6
Noisy Mixup (α = 1.0) Yang et al. (2020b)	78.9	78.6 66.6 46.7	66.6 53.4 25.9
Noisy Feature Mixup (α = 1.0)	I 95.4 I 95.0 91.6 83.0 I 91.9 87.4 73.3
7
Published as a conference paper at ICLR 2022
Ooo
7 6 5
EJn。。VjSəj
40 「 ʌ」
0.00	0.05
0.10	0.15	0.20	0.25	0.30
White Noise (σ)
Figure 4: Wide ResNets evaluated on CIFAR-100. Averaged across 5 random seeds.
-+- Baseline、
Mixup
-∙- CutMix
PuzzIeMix
Noisy Mixup
Manifold Mixup
-∙- NFM (*)
一*- NFM
Ooo
7 6 5
40 0.00
0.02	0.04	0.06	0.08
Salt and Pepper Noise (Y)
Table 2: Robustness of Wide-ResNet-18 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on CIFAR-100. The results are averaged over 5 models trained with different seed values.
Scheme	Clean (%)	0.1	σ (%) 0.2	0.3	0.02	γ(%) 0.04	0.1
Baseline	76.9	64.6	42.0	23.5	58.1	39.8	15.1
Baseline + Noise	76.1	75.2	60.5	37.6	64.9	51.3	23.0
Mixup (α = 1.0) Zhang et al. (2017)	80.3	72.5	54.0	33.4	62.5	43.8	16.2
CutMix Yun et al. (2019)	77.8	58.3	28.1	13.8	70.3	58.	24.8
PuzzleMix (200 epochs) Kim et al. (2020b)	78.6	66.2	41.1	22.6	69.4	56.3	23.3
PuzzleMix (1200 epochs) Kim et al. (2020b)	80.3	53.0	19.1	6.2	69.3	51.9	15.7
Manifold Mixup (α = 1.0) Verma et al. (2019)	79.7	70.5	45.0	23.8	62.1	42.8	14.8
Noisy Mixup (α = 1.0) Yang et al. (2020b)	78.9	78.6	66.6	46.7	66.6	53.4	25.9
Noisy Feature Mixup (α = 1.0)	-809~~	80.1	72.1	55.3	72.8	62.1	34.4
Table 3: Robustness of ResNet-50 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on ImageNet. Here, the NFM training scheme improves both the predictive accuracy on
clean data and robustness with respect to data perturbations.
Scheme	Clean (%)	σ (%) 0.1	0.25	0.5	γ (%) 0.06	0.1	0.15
Baseline Manifold Mixup (α = 0.2) Verma et al. (2019)	76.0 76.7	73.5	67.0	50.1 74.9	70.3	57.5	53.2	50.4 45.0 58.1	54.6 49.5
Noisy Feature Mixup (α = 0.2) Noisy Feature Mixup (α = 1.0)	770 76.8	76.5^^720~~60.1 76.2 71.7 60.0	58.3	56.0	52.3 60.9	58.8 54.4
addition, we see that the model trained with NFM is less sensitive to both white noise and salt and
pepper perturbations. These results are surprising, as robustness is often thought to be at odds with
accuracy (Tsipras et al., 2018). However, we demonstrate NFM has the ability to improve both
accuracy and robustness. Table 2 indicates that for the same α, NFM can achieve an average test
accuracy of 80.9% compared to only 80.3% in the mixup setting.
5.3	ImageNet
ResNet-50. Table 3 similarly shows that NFM improves both the generalization and robustness
capacities with respect to data perturbations. Although less pronounced in comparison to previous
datasets, NFM shows a favorable trade-off without requiring additional computational resources.
Note that due to computational costs, we do not average across multiple seeds and only compare
NFM to the baseline and manifold mixup models.
5.4	CIFAR- 1 0C
In Figure 6 we use the CIFAR-10C dataset (Hendrycks & Dietterich, 2019) to demonstrate that models
trained with NFM are more robust to a range of perturbations on natural images. Figure 6 (left) shows
8
Published as a conference paper at ICLR 2022
"EJn。。VjSəj
50 0.00	0.02
0.04	0.06	0.08	0.10	0.12
Adverserial Noise (e)
Figure 5: Pre-actived ResNet-18 evaluated on CIFAR-10 (left) and Wide ResNet-18 evaluated on
20,
0.000
0.025
0.050 0.075 0.100 0.125 0.150
Adverserial Noise (e)
CIFAR-100 (right) with respect to adversarially perturbed inputs.
"EJn8v jsəj
90
80
70
60
50
40
1	2	3	4	5
Severities
80
60
40
20	..................... ..
gaussιan jpeg impulse shot snow speckle
Noise type
Figure 6: Pre-actived ResNet-18 evaluated on CIFAR-10c.
the average test accuracy across six selected perturbations and demonstrates the advantage of NFM
being particularly pronounced with the progression of severity levels. The right figure shows the
performance on the same set of six perturbations for the median severity level 3. NFM excels on
Gaussian, impulse, speckle and shot noise, and is competitive with the rest on the snow perturbation.
5.5	Robustness to Adversarial Examples
So far we have only considered white noise and salt and pepper perturbations. We further consider
adversarial perturbations. Here, we use projected gradient decent (Madry et al., 2017) with 7 iterations
and various levels to construct the adversarial perturbations. Fig. 5 highlights the improved resilience
of ResNets trained with NFM to adversarial input perturbations and shows this consistently on both
CIFAR-10 (left) and CIFAR-100 (right). Models trained with both mixup and manifold mixup do not
show a substantially increased resilience to adversarial perturbations.
In Section F.6, we compare NFM to models that are adversarially trained. There, we see that
adversarially trained models are indeed more robust to adversarial attacks, while at the same time
being less accurate on clean data. However, models trained with NFM show an advantage compared
to adversarially trained models when faced with salt and pepper perturbations.
6 Conclusion
We introduce Noisy Feature Mixup, an effective data augmentation method that combines mixup and
noise injection. We identify the implicit regularization effects of NFM, showing that the effects are
amplifications of those of manifold mixup and noise injection. Moreover, we demonstrate the benefits
of NFM in terms of superior model robustness, both theoretically and experimentally. Our work
inspires a range of interesting future directions, including theoretical investigations of the trade-offs
between accuracy and robustness for NFM and applications of NFM beyond computer vision tasks.
Further, it will be interesting to study whether NFM may also lead to better model calibration by
extending the analysis of Thulasidasan et al. (2019); Zhang et al. (2021).
9
Published as a conference paper at ICLR 2022
Code of Ethics
We acknowledge that we have read and commit to adhering to the ICLR Code of Ethics.
Reproducibility
The codes that can be used to reproduce the empirical results, as well as description of the data
processing steps, presented in this paper are available as a zip file in Supplementary Material at
OpenReview.net. The codes are also available at https://github.com/erichson/NFM. For
the theoretical results, all assumptions, proofs and the related discussions are provided in SM.
Acknowledgments
S. H. Lim would like to acknowledge the WINQ Fellowship and the Knut and Alice Wallenberg
Foundation for providing support of this work. N. B. Erichson and M. W. Mahoney would like to
acknowledge IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of
this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no
official endorsement should be inferred. We are also grateful for the generous support from Amazon
AWS.
References
Guozhong An. The effects of adding noise during backpropagation training on a generalization
performance. Neural Computation, 8(3):643-674,1996.
Henry S Baird. Document image defect models. In Structured Document Image Analysis, pp.
546-556. Springer, 1992.
Chris M Bishop. Training with noise is equivalent to Tikhonov regularization. Neural Computation,
7(1):108-116, 1995.
Olivier Bousquet, Stephane Boucheron, and Gabor Lugosi. Introduction to statistical learning theory.
In Summer school on machine learning, pp. 169-207. Springer, 2003.
Alexander Camuto, Matthew Willetts, UmUt Simsekli, Stephen Roberts, and Chris Holmes. Explicit
regularisation in Gaussian noise injections. arXiv preprint arXiv:2007.07368, 2020.
Luigi Carratino, Moustapha Cisse, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regulariza-
tion. arXiv preprint arXiv:2006.06049, 2020.
Olivier Chapelle, Jason Weston, Leon Bottou, and Vladimir Vapnik. Vicinal risk minimization.
Advances in Neural Information Processing Systems, pp. 416-422, 2001.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation.
Journal of Machine Learning Research, 21(245):1-71, 2020.
Dan Claudiu Cireyan, Ueli Meier, Luca Maria Gambardella, and Jurgen Schmidhuber. Deep, big,
simple neural nets for handwritten digit recognition. Neural Computation, 22(12):3207-3220,
2010.
Nicolas Couellan. Probabilistic robustness estimates for feed-forward neural networks. Neural
Networks, 142:138-147, 2021.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A kernel
theory of modern data augmentation. In International Conference on Machine Learning, pp.
1528-1537. PMLR, 2019.
Dennis DeCoste and Bernhard Scholkopf. Training invariant support vector machines. Machine
Learning, 46(1):161-190, 2002.
10
Published as a conference paper at ICLR 2022
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248-255. Ieee, 2009.
Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-
dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3-4):211-407, 2014.
Gamaleldin F Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. arXiv preprint arXiv:1803.05598, 2018.
Logan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry,
Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric
Wallace. A discussion of ’adversarial examples are not bugs, they are features’. Distill, 2019.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. ArXiv preprint
arXiv:1906.00945, 2020.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers:
from adversarial to random noise. arXiv preprint arXiv:1608.08967, 2016.
Jean-Yves Franceschi, Alhussein Fawzi, and Omar Fawzi. Robustness of classifiers to uniform
lp and Gaussian noise. In International Conference on Artificial Intelligence and Statistics, pp.
1280-1288. PMLR, 2018.
Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International
Statistical Review, 70(3):419-435, 2002.
Chengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: A simple way to improve
generalization of neural network training. arXiv preprint arXiv:2002.09024, 2020.
Ian Goodfellow, Patrick McDaniel, and Nicolas Papernot. Making machine learning robust against
adversarial inputs. Communications of the ACM, 61(7):56-66, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kristjan Greenewald, Anming Gu, Mikhail Yurochkin, Justin Solomon, and Edward Chien. k-mixup
regularization for deep learning via optimal transport. arXiv preprint arXiv:2106.02933, 2021.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.
In International Conference on Machine Learning, pp. 3059-3068. PMLR, 2016.
Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi.
Escaping the big data paradigm with compact transformers. arXiv preprint arXiv:2104.05704,
2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. arXiv preprint arXiv:1705.08475, 2017.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corrup-
tions and perturbations. Proceedings of the International Conference on Learning Representations,
2019.
Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
arXiv preprint arXiv:1912.02781, 2019.
11
Published as a conference paper at ICLR 2022
Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with Jacobian regularization. arXiv
preprint arXiv:1908.02729, 2019.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning, pp. 5275-5285.
PMLR, 2020a.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning, 2020b.
Masanari Kimura. Mixup training as the complexity reduction. arXiv preprint arXiv:2006.06231,
2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. Advances in Neural Information Processing Systems, 25:1097-1105,
2012.
Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein distributionally robust optimization: Theory and applications in machine learning. In
Operations Research & Management Science in the Age of Analytics, pp. 130-166. INFORMS,
2019.
Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world,
2016.
Yongchan Kwon, Wonyoung Kim, Joong-Ho Won, and Myunghee Cho Paik. Principled learning
method for Wasserstein distributionally robust optimization with local perturbations. In Interna-
tional Conference on Machine Learning, pp. 5567-5576. PMLR, 2020.
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training:
Achieving robust neural networks without sacrificing too much accuracy. In Proceedings of the
12th ACM Workshop on Artificial Intelligence and Security, pp. 95-103, 2019.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisy recurrent
neural networks. arXiv preprint arXiv:2102.04877, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
M. W. Mahoney. Approximate computation and implicit regularization for very large-scale data
analysis. In Proceedings of the 31st ACM Symposium on Principles of Database Systems, pp.
143-154, 2012.
M. W. Mahoney and L. Orecchia. Implementing regularization implicitly via approximate eigenvector
computation. In International Conference on Machine Learning, pp. 121-128, 2011.
Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data
can help, double descend, or hurt generalization. arXiv preprint arXiv:2002.11080, 2020.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robust-
ness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9078-9086, 2019.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018.
12
Published as a conference paper at ICLR 2022
Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arXiv preprint
arXiv:2105.07581, 2021.
Rafael Pinot, LaUrent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cedric GoUy-
Pailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization.
arXiv preprint arXiv:1902.01148, 2019a.
Rafael Pinot, Florian Yger, Cedric Gouy-Pailler, and Jamal Atif. A unified view on differential
privacy and robustness to adversarial examples. arXiv preprint arXiv:1906.07982, 2019b.
Rafael Pinot, Laurent Meunier, Florian Yger, Cedric Gouy-Pailler, Yann Chevaleyre, and Jamal
Atif. On the robustness of randomized classifiers to adversarial examples. arXiv preprint
arXiv:2102.10875, 2021.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716,
2020.
Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
preprint arXiv:1908.05659, 2019.
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann. Fixing data augmentation to improve adversarial robustness. arXiv preprint
arXiv:2103.01946, 2021.
Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
International Conference on Machine Learning,pp. 8093-8104. PMLR, 2020.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.
Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness
of visual transformers. arXiv preprint arXiv:2103.15670, 2021.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):1-48, 2019.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep
neural networks. IEEE Transactions on Signal Processing, 65(16):4265-4280, 2017.
Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of
adversarial training. In NIPS workshop on Machine Learning and Computer Security, volume 3,
pp. 4, 2017.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the
cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification models.
In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631-648, 2018.
Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.
On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
arXiv preprint arXiv:1905.11001, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
Francisco Utrera, Evan Kravitz, N Benjamin Erichson, Rajiv Khanna, and Michael W Mahoney.
Adversarially-trained deep nets transfer better. arXiv preprint arXiv:2007.05869, 2020.
Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media,
2013.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz,
and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In
International Conference on Machine Learning, pp. 6438-6447. PMLR, 2019.
13
Published as a conference paper at ICLR 2022
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via Lipschitz
augmentation. arXiv preprint arXiv:1905.03684, 2019a.
Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classification
via an all-layer margin. arXiv preprint arXiv:1910.04284, 2019b.
Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher Re. On the generalization effects of
linear transformations in data augmentation. In International Conference on Machine Learning,
pp.10410-10420. PMLR, 2020.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. arXiv preprint arXiv:2003.02460, 2020a.
Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez, Kannan
Ramchandran, and Michael W Mahoney. Boundary thickness and robustness in learning models.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 6223-6234, 2020b.
Wenpeng Yin, Huan Wang, Jin Qu, and Caiming Xiong. BatchMixup: Improving training by interpo-
lating hidden states of the entire mini-batch. In Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021, pp. 4908-4912, 2021.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In International
Conference on Computer Vision, pp. 6023-6032, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482. PMLR, 2019.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020.
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves
calibration. arXiv preprint arXiv:2102.06289, 2021.
14
Published as a conference paper at ICLR 2022
Supplementary Material (SM) for “Noisy Feature Mixup”
Organizational Details. This SM is organized as follows.
•	In Section A, we study the regularizing effects of NFM within the vicinal risk minimization
framework, relating the effects to those of mixup and noise injection.
•	In Section B, we restate the results presented in the main paper and provide their proof.
•	In Section C, we study robutsness of NFM through the lens of implicit regularization,
showing that NFM can implicitly increase the classification margin.
•	In Section D, we study robustness of NFM via the lens of probabilistic robustness, showing
that noise injection can improve robustness on top of manifold mixup while keeping track
of maximal loss in accuracy incurred under attack by tuning the noise levels.
•	In Section E, we provide results on generalization bounds for NFM and their proofs,
identifying the mechanisms by which NFM can lead to improved generalization bound.
•	In Section F, we provide additional experimental results and their details.
We recall the notation that we use in the main paper as well as this SM.
Notation. I denotes identity matrix, [K] := {1, . . . , K}, the superscript T denotes transposition, ◦
denotes composition, denotes Hadamard product, 1 denotes the vector with all components equal
one. For a vector v, vk denotes its kth component and kv kp denotes its lp norm for p > 0. conv(X)
denote the convex hull of X. Mλ(a, b) := λa + (1 - λ)b, for random variables a, b, λ. δz denotes the
Dirac delta function, defined as δz(x) = 1 if x = z and δz(x) = 0 otherwise. 1A denotes indicator
function of the set A. For a,β> 0, DDλ ：= α¾Beta(α +1,β) + OeeBeta(β + 1, α), a uniform
mixture of two Beta distributions. For two vectors a, b, cos(a, b) := ha, bi/kak2kbk2 denotes their
cosine similarity. N(a, b) denotes the Gaussian distribution with mean a and covariance b.
A NFM Through the Lens of Vicinal Risk Minimization
In this section, we shall show that NFM can be constructed within a vicinal risk minimization (VRM)
framework at the level of both input and hidden layer representations.
To begin with, we define a class of vicinal distributions and then relate NFM to such distributions.
Definition 1 (Randomly perturbed feature distribution). Let Zn = {z1, . . . , zn} be a feature set. We
say that P0n is an ei-randomly perturbed feature distribution if there exists a set {z10 , . . . , zn0 } such
that Pn = 1 Pn=1 δz0, with z0 = Zi + <¾ ,for some random variable e% (possibly dependent on Zn)
drawn from a probability distribution.
Note that the support of an ei -randomly perturbed feature distribution may be larger than that of Z.
If Zn is an input dataset and the ei are bounded variables such that keik ≤ β for some β ≥ 0, then P0n
is a β-locally perturbed data distribution according to Definition 2 in (Kwon et al., 2020). Examples
of β-locally perturbed data distribution include that associated with denoising autoencoder, input
mixup, and adversarial training (see Example 1-3 in (Kwon et al., 2020)). Definition 1 can be viewed
as an extension of the definition in (Kwon et al., 2020), relaxing the boundedness condition on the
ei to cover a wide families of perturbed feature distribution. One simple example is the Gaussian
distribution, i.e., when ei 〜N(0, σ2), which models Gaussian noise injection into the features.
Another example is the distribution associated with NFM, which we now discuss.
To keep the randomly perturbed distribution close to the original distribution, the amplitude of
the perturbation should be small. In the sequel, we let > 0 be a small parameter and rescale
1 - λ 7→ (1 - λ), σadd 7→ σadd and σmult 7→ σmult.
Let Fk be the family of mappings from gk(X) to Y and consider the VRM:
inf Rn(fk) := E(gk(x),y0)〜Pnk) [l(fk(gk (χ))),y0)],	(10)
where Pnk) = n PNI 6(§①)忒),with gk (Xi) = gk (Xi) + EeNFM(k) and yi = y + Cey ,for some
random variables eiNFM(k) and eiy .
15
Published as a conference paper at ICLR 2022
In NFM, we approximate the ground-truth distribution D using the family of distributions {P(nk) }k∈S,
with a particular choice of (eiNFM(k), eiy). In the sequel, we denote NFM at the level of kth layer as
NFM(k) (i.e., the particular case when S := {k}).
The following lemma identifies the (eiNFM(k), eiy) associated with NFM(k) and relates the effects
of NFM(k) to those of mixup and noise injection, for any perturbation level > 0.
Lemma 1. Let > 0 and denote zi (k) := gk (xi). Learning the neural network map f using
NFM(k) is a VRM with the (eiN F M (k) , eiy)-randomly perturbed feature distribution, P(nk) =
n1 Pn=ι δ(z0(k),y0), with z0(k) := zi(k) + EeNFM(k), yi := y% + Eey, as the vicinal distribution. Here,
ey = (I — λ)(yi — yi)，
eiNFM(k) = (1 + Eσmultξmult)	eimixup(k) + einoise(k),	(11)
where emixup(k) = (1 — λ)(Zi(k) — zi(k)), and enθise(k) = σmuιtξmuιt Θ zi(k) + σ°ddξadd, with
zi(k), ≡i(k) ∈ gk (X), λ 〜Beta(α, β) and yi, yi ∈ Y. Here, (zi(k), yi) are drawn randomly from
the training set.
Therefore, the random perturbation associated to NFM is data-dependent, and it consists of a
randomly weighted sum of that from injecting noise into the feature and that from mixing pairs
of feature samples. As a simple example, one can take ξadd, ξmult to be independent standard
Gaussian random variables, in which case We have VOoSelkk 〜N(o, @大&&1 + 焉Ultdiag(Zi(k))2),
and ei 〜N(0, σ2dd + σmUltMλ(zi(k),zi(k))2) in Lemma L
We now prove Lemma 1.
Proof of Lemma 1. Let k be given and set E = 1 without loss of generality. For every i ∈ [n],
NFM(k) injects noise on top of a mixed sample zi0(k) and outputs:
zi (k) =	(1	+ σmultξmult)	Θ zi (k) + σaddξadd	(12)
=(I	+ σmultξmult)	θ (λzi(k) + (1 — λ)zi(k))	+ σaddξadd	(13)
= zi(k) + eiNFM(k),	(14)
where eNFM(k) = (1 — X)(Zi(k) — Zi(k)) + σmultξmuit Θ (λzi(k) + (1 — X)Zi(k) + σθddξadd.
Now, note that applying mixup to the pair (zi(k), Zi(k)) results in zi(k) = Zi(k) + emm-xupkk, With
emiχup(k) = (ι — χ)(Zi(k) — Zi(k)), where Zi(k), Zi(k) ∈ gk(X) and λ 〜Beta(α, β), whereas
applying noise injection to Zi(k) results in (1+σmultξmult)ΘZi(k)+σaddξadd = Zi(k) + einoise(k),
noise(k)	NFM(k)	mixup(k)
with ei	= σmultξmult Θ Zi (k) + σaddξadd. Rewriting ei	in terms of ei	and
noise(k)
ei	gives
eiNFM(k) = (1 + σmultξmult) Θ eimixup(k) + einoise(k).	(15)
Similarly, we can derive the expression for eiy using the same argument. The results in the lemma
follow upon applying the rescaling 1 — X 7→ E(1 — X), σadd 7→ Eσadd and σmult 7→ Eσmult, for
E > 0.	□
B Statements and Proof of the Results in the Main Paper
B.1 Complete Statement of Theorem 1 in the Main Paper and the Proof
We first state the complete statement of Theorem 1 in the main paper.
Theorem 3 (Theorem 1 in the main paper). Let E > 0 be a small parameter, and assume that h and
f are twice differentiable. Then, LnFM = Ek〜SLNFM, where
LNFM (k) = Lntd + ERlkk + E2R2kk + E2R3kk + e%(e),	(16)
16
Published as a conference paper at ICLR 2022
with
D(k) — D(k)	2	Dadd(k)	2	RmulMk)
R2 = R2 + σadd R2	+ σmultR2	,	(17)
遨k = R3k)+σaddRudkk+σm ult 咫"lltkk,	(18)
where
RIk) = b~Dλ[1_ʌ] X(h0(f(xi) — yi)Vkf(gk(Xi))TExr〜Dx[gk(Xr) — gk(xi)],	(19)
n	rx
i=1
Rk= Eλ〜Dλ2n - 22] X h"(f(xi))Vk f (gk(Xi))T
i=1
X Exr 〜Dχ [(gk (Xr ) ― gk (Xi))(gk(xr ) ― gk (Xi)) ]Vkf(gk (Xi)),	(20)
殴)=Eλ~Dλ 2n -λ)2] X(h0(f(Xi))-yi)
n	i=1
× Exr 〜Dχ [(gk (Xr ) ― gk (Xi )) Vk f (gk (Xi ))(gk (Xr ) ― gk (Xi ))],	QI)
1n
Rdddk)= 2nfh0 (f (x∕v kf(gk (Xi))T Eξk[ξadd(ξadd)T]Vkf(gk(Xi)),	(22)
i=1
1n
Rmultdk) = Tr EhO (f(g))Vkf(gk (χi))T (Eξk[ξmult(ξmult)T] © gk (Xi 项(Xi)T»f® (Xiy),
2n
i=1
(23)
rn
Radddk) = 2n ∑(h0(f (χi)) - yi)Eξk [(ξadd)T Vkf & (χi))ξadd],	(24)
n i=1
rn
Rmultdk) = 2n E(h'(f(χi)) -yi)Eξk[(ξmult © gk(Xi))TVkf(gk(χi))(ξmult © gk(“力,(25)
n i=1
and 夕(e) = E、〜D入 Exr〜Dx Eξk〜。[夕(e)], With 夕 SOmefUnCtiOn SUCh that lime→o 夕(e) = 0.
Following the setup of Zhang et al. (2020), we provide empirical results to show that the second order
Taylor approximation for the NFM loss function is generally accurate (see Figure 7).
Recall from the main paper that the NFM loss function to be minimized is LNFM = Ek〜SLNFM(k),
where
r n n
LNFMdk) = __ χχ 葭〜Betada,β)Eξk〜Ql(fk(Mλξ(gk(Xi), gk(Xj))),Mλ(yi,yj)),	(26)
n i=1 j=1
----with NFM
--- with approximated loss
----with NFM	0 7
--- with approximated loss
0.6
o 0.5
Figure 7: Comparison of the original NFM loss with the approximate loss function during training
and testing for a two layer ReLU neural network trained on the toy dataset of Subsection F.2.
0	20	40	60	80	100
epoch
0	20	40	60	80	100
epoch
17
Published as a conference paper at ICLR 2022
where l : RK × RK → [0, ∞) is a loss function of the form l(f (x), y) = h(f (x)) - yf(x),
ξk := (ξkadd , ξkmult) are drawn from some probability distribution Q with finite first two moments
(with zero mean), and
Mλ,ξk(gk(x),gk(x0)) := (1+σmultξkmult)Mλ(gk(x),gk(x0))+σaddξkadd.	(27)
Before proving Theorem 3, we note that, following the argument of the proof of Lemma 3.1 in Zhang
et al. (2020), the loss function minimized by NFM can be written as follows. For completeness, we
provide all details of the proof.
Lemma 2. The NFM loss (26) can be equivalently written as LNFM = Ek〜SLnFM(k), where
n
LNFM (k) = 1 X Eλ 〜Dλ E“"x Eξk 〜Q[hf (gk ⑵)+ EeNFM (k)))-ifk (gk E) + EeNFM(k))],
n
i=1
(28)
with
eiNFM(k) = (1 + Eσmultξkmult) eimixup(k) + einoise(k).	(29)
Here eimixup(k) = (1- λ)(gk(xr) -gk(xi)) and einoise(k) = σmultξkmult gk(xi) + σaddξkadd, with
gk(xi),gk (Xr) ∈ gk(X) and λ 〜Beta(α, β).
Proof of Lemma 2. From (26), we have:
nn
LNFM ⑻=n12 xxEλ 〜Beta(α,β)Eξk 〜Q l(fk (Μλ,ξk (gk (Xi),gk (Xj ))),Mλ3,yj )).	(30)
n i=1 j=1
We can rewrite:
Eλ~Beta(α,β) l(fk (Mλ,ξk (gk (Xi), gk (Xj ))), Mλ(yi, yj ))
=Eλ~Beta(α,β) [h(fk (Mλ,ξk (gk (xi), gk (Xj )))) - Mλ(yi, yj )fk (Mλ,ξk (gk (xi), gk (Xj )))] (31)
=Eλ^Beta(α,β)[λ(h(fk (Mλ,ξk (gk (Xi),gk (Xj )))) - Uifk (Mλ,ξk (gk (Xi),gk(Xj ))))
+ (1- λ)(h(fk (Mλ,ξk (gk (Xi), gk (Xj)))) - yj fk (Mλ,ξk (gk (Xi), gk (Xj))))]	(32)
=Eλ 〜Beta(α,β) EB 〜Bern(λ) [B(h(fk (Mλ,ξk (gk (Xi), gk (Xj )))) - yifk (Mλ,ξk (gk (Xi), gk (Xj ))))
+ (1- B)(h(fk(Mλ,ξk(gk(Xi),gk(Xj)))) - yjfk(Mλ,ξk(gk(Xi),gk(Xj))))],	(33)
where Bern(λ) denotes the Bernoulli distribution with parameter λ (i.e., P[B = 1] = λ and
P[B = 0] = 1 - λ).
Note that λ 〜 Beta(α, β) and B∣λ 〜 Bern(λ). By conjugacy, We can switch their order:
B 〜Bern ( ——— ) , λ∣B 〜Beta(α + B, β + 1 — B),	(34)
α+β
and arrive at:
Eλ 〜Beta(α,β)lfk (Mλ,ξk (gk (Xi), gk (Xj ))), Mλ(yi, yj ))
=EB 〜Bern( -ɪ ) Eλ 〜Beta(α+B,β+1-B) [B (h(fk(Mλ,ξk (gk (Xi),gk (Xj ))))
— yifk (Mλ,ξk (gk (Xi ), gk (Xj ))))
+ (1 — B)(h(fk (Mλ,ξk (gk (Xi), gk (Xj)))) — yj fk (Mλ,ξk (gk (Xi), gk (Xj))))]	(35)
α
a + β
Eλ^^Betja,(α+1,β) [h]fk(Mλ,ξk (gk (Xi), gk(Xj )))) — yifk (Mλ,ξk (gk (Xi), gk (Xj )))]
β
a + β
+
Eλ^Beta(α,β+1) [h(fk (Mλ,ξk (gk (Xi), gk (Xj )))) — yj fk (Mλ,ξk (gk (Xi), gk (Xj )))] ∙
(36)
Using the facts that Beta(β + 1, α) and 1 — Beta(α, β + 1) are of the same distribution and
M1-λ (Xi, Xj) = Mλ(Xj, Xi), we have:
):Eλ,^Beta,(o,,β+1) [h(fk (Mλ,ξk (gk (Xi), gk (Xj )))) — yj fk (Mλ,ξk (gk (Xi), gk (Xj )))]
i,j
=EEλ 〜Beta(β+1,α)[h(fk (Mλ,ξk (gk (Xi),gk(Xj )))) - Uifk (Mλ,ξk (gk(Xi),gk (Xj )))]∙	(37)
i,j
18
Published as a conference paper at ICLR 2022
Therefore, denoting D ：= α⅛Be^ta(α + 1,β) + 舟Beta(β + 1, α) and Dx := 1 Pn=I δχj the
empirical distribution induced by the training samples {xj }j ∈[n], we have:
1n
LlnFM (kk = n	〜Dλ Exr 〜Dx Eξk^Q[h(fk (Mλ,ξk (gk (xi),gk (Xr A))
n i=1
- yifk (Mλ,ξk (gk(xi), gk (xr)))].	(38)
The statement of the lemma follows upon substituting the fact that Mλ,ξk (gk(xi), gk(xr)) = gk(xi) +
CeNFM(k) into the above equation.	□
With this lemma in hand, we now prove Theorem 3.
Proof of Theorem 3. Denote ψi(C) := h(fk(gk(xi) + CeiNF M(k))) - yifk(gk(xi) + CeiNFM(k)),
where eiNFM(k) is given in (29). Since h and fk are twice differentiable by assumption, ψi is twice
differentiable in C, and
C2
ψi(E) = ψi(O) + cψi(O) + ~2 ψ00(O) + e2^i(e),	(39)
where % is some function such that lime→o 夕i(c) = 0. Therefore, by Lemma 2, LNFM =
Ek〜SLNFM(k), where
1n
LNFM(k) = - Σ>λ〜Dλ Exr〜Dx Eξk〜Q[ψi(e)]	(40)
n i=1
1 n	E2
=n EEλ~Dλ Exr-Dx Eξk~Q ψi(0) + cψi(0) + ^2ψ00(0) + e23i(e)	(41)
i=1
1 n	E2
=n EEλ~Dλ Exr -Dx Eξk~Q ψi(0) + cψi (0) + ^2 ψ00(0) + e2θ(e)	(42)
i=1
=：Lntd + ER1k) + E2(R2k) + R八 + E2%(C),	(43)
where 夕(E) = 1 Pi=1 旧入〜队Exr-DxEξk-QS(c)].
It remains to compute ψ0(0) and ψi0(O) in order to arrive at the expression for the RIk), Rk and
Rk presented in Theorem 3.
Denoting gk(xi) := gk(Xi) + EeNFM(k), we compute, applying chain rule:
ψi(E) = h0 (fk(gk (Xi)))Vk fk (gk (Xi))T dg∂(χ2 - yiVkfk (gk (Xi))T dg∂χ2	(44)
=(h'(fk(gk(Xi)))- yi)Vkfk(gk(Xi))Td⅛^	(45)
∂E
= (h0(fk(ggk(Xi))) - yi)Vkfk(ggk(Xi))T eiNF M(k)	(46)
= (h0(fk(ggk(Xi))) - yi)Vk fk (ggk (Xi))T [(1 - λ)(gk(Xr) - gk(Xi)) + σaddξkadd
+ σmultξkmult gk(Xi) + E(1 - λ)σmultξkmult (gk (Xr) - gk (Xi))],	(47)
where we have used dg∂(xi) = eNFM(k) in the second last line and substituted the expression for
eiNFM(k) from (29) in the last line above.
Therefore,
ψi0(0) = (h0(fk(gk(Xi))) - yi)Vkfk(gk(Xi))T[(1 - λ)(gk(Xr) -gk(Xi))+σaddξkadd
+ σmultξkmult gk (Xi)],
(48)
19
Published as a conference paper at ICLR 2022
and
Eξk〜Qψi(0) = (h0(fk(gk(Xi)))- yi)Vkfk(gk(xi))T[(1 - λ)(gk(xr) - gk(Xi))],	(49)
where We have used the assumptions that Eξk〜Qξadd = 0 and Eξk〜qξmult = 0. The expression for
the R(1k) in the theorem then follows from substituting (49) into (42).
Next, using chain rule, we have:
ψ00(e) = ∂∂ ((h'(fk(gk(Xi))) - yi)Vkfk(gk(Xi))Tdg∂X))	(50)
=(d~ (h0(fk (gk (Xi))) - yi)[ Vk fk(gk (Xi))T dgk(Xi)
∂	∂
+ (h/(fk (gk (Xi))) - yi Kr (Vkfk (gk (Xi))T "	[ ∙	(51)
∂	∂
Note that, applying chain rule,
∂∂ (Vkfk(gk(Xi))Tdg∂Xl[ = ∂∂- (Vkfk(gk(Xi))TeNFM⑻)	(52)
=	% ((eNFM(k))TVkfk(gk(Xi)))	(53)
=	(eNFM Ik))T Vk fk (gk (Xi)) dgdX)	(54)
=	(eiNFM(k))TV2kfk (ggk(Xi))eiNFM(k) .	(55)
Also, using chain rule again,
^d(h,(fk(gk(Xi))) - yi)) = h00(fk(gk(Xi)))Vkfk(gk(Xi))Tdg∂⅛^	(56)
= h00(fk(ggk(Xi)))Vkfk(ggk (Xi))T eiNFM(k) .	(57)
Therefore, we have:
ψi00() = h00(fk(ggk(Xi)))Vkfk(ggk(Xi))TeiNFM(k)(eiNFM(k))TVkfk(ggk(Xi))
+ (h0(fk(ggk(Xi))) - yi)(eiNFM(k))TV2kfk(ggk(Xi))eiNFM(k)	(58)
= h00 (fk (ggk (Xi)))Vk fk (ggk (Xi))T [(1 - λ)(gk(Xr) - gk(Xi)) + σaddξkadd
+	σmultξkmult	gk (Xi) + (1 - λ)σmultξkmult	(gk (Xr) - gk (Xi))]
×	[(1 - λ)(gk(Xr) - gk (Xi)) + σaddξkadd + σmultξkmult	gk (Xi)
+	(1 - λ)σmultξkmult	(gk(Xr) - gk (Xi))]T Vk fk (ggk (Xi))
+	(h0 (fk (ggk (Xi))) - yi)[(1 - λ)(gk (Xr) - gk (Xi)) + σaddξkadd + σmultξkmult gk (Xi)
+	(1 - λ)σmultξkmult (gk(Xr) - gk (Xi))]T V2k fk (ggk (Xi))[(1 - λ)(gk(Xr) - gk(Xi))
+ σaddξkadd + σmultξkmult	gk (Xi) + (1 - λ)σmultξkmult	(gk (Xr) - gk (Xi))] (59)
=	: h00(fk(ggk(Xi)))Vkfk(ggk(Xi))TP1()Vkfk(ggk(Xi))+(h0(fk(ggk(Xi)))-yi)P2(),
(60)
where we have substituted the expression for the eiNFM(k) into the first line to arrive at the last line
above.
20
Published as a conference paper at ICLR 2022
Note that,
Eξk ~QPι(e)
=Eξk~Q∣(I- λ)(gk(xr ) - gk (Xi)) + σaddξkdd + σmultξmult Θ gk (Xi)
+ (1 - λ)σmult ξkmult	(gk (xr) - gk(xi))] × [(1 - λ)(gk (xr) - gk (xi)) + σaddξkadd
+ σmultξkmult	gk(Xi) + (1 - λ)σmult ξkmult	(gk (Xr) - gk(Xi))]T	(61)
= (I- λ)2(gk (Xr ) - gk (Xiy)(gk(Xr ) - gk (Xi))T + σ2ddEξk^Q[ξkdd(ξkdd)τ ]
+ σmuitEξk~Q[(ξmult Θ gk(Xi))(ξmult Θ gk(Xi))τ] +。㈢	(62)
= (I- λ)2(gk (Xr ) - gk (Xi)Mgk(Xr ) - gk (Xi))T + σaddEξk~Q[ξadd(ξadd)τ]
+σm uitEξk ~Q[(ξmult(ξmult)τ) θ gk (Xi 访(Xi)T]+。⑵，	◎)
as E → 0, where We have used the assumption that Eξk~Qξkdd = 0 and Eξk~Qξmult = 0 in the
second last line above.
Similarly,
Eξk ~QP2(e)
=Eξk~Q[(1 - λ)(gk (Xr ) - gk (Xi)) + σaddξadd + σmultξmult θ gk (Xi)
+ e(1 - λ)σmuitξmult Θ (gk(Xr) - gk(Xi))]τVkfk(gk(Xi))[(1 - λ)(gk(Xr) - gk(Xi))
+ σaddξkadd + σmultξkmult Θ gk(Xi) + E(1 - λ)σmultξkmult Θ (gk(Xr) -gk(Xi))]	(64)
=(1 - λ) (gk (Xr ) - gk (Xi)) Vk fk (gk (Xi)) (gk (Xr ) - gk (Xi))
+σ2ddEξk ~Q[(ξadd)τ Vk fk(gk (Xi ))ξadd]
+ σmuitEξk~Q[(ξmult Θ gk(Xi))τVkfk(gk(Xi))(ξmult Θ gk(Xi))] +。(匕	(65)
as E → 0.
Now, recall from Eq. (42) that we have
1 n	E2
LNFM(k) = n EEλ~DDλ ExrSx Eξk~Q ψi(0) + Eψi(0) + Eψi0(0) + e"	(66)
i=1
=:Lntd + eR(k) + e2(R2k) + R3k)) + e%(e),	(67)
where ψi(0) = h(fk(gk(Xi))) - yifk(gk(Xi)). Also, we have:
Eξk ~Q[ψi0(e)]
=h00(fk (gk (Xi)))Vk fk (gk (Xi))τ Eξk ~Q[Pι(e)]Vk fk(gk (Xi))
+ (h0(fk(gk(Xi))) - yi)Eξk~q[P2(e)]	(68)
= h00(fk (ggk(Xi)))Vkfk (ggk(Xi))τ[(1 - λ)2 (gk (Xr) - gk(Xi))(gk (Xr) - gk (Xi))τ
+σ2ddEξk ~Q[ξadd(ξadd)τ]+σm “阪7健”1' (ξmUlt)T) θ gk ⑵加(Xi)T]+。(划
× Vkfk (ggk(Xi))
+ (h (fk (ggk (Xi ))) - yi )[(1 - λ) (gk (Xr ) - gk (Xi )) Vkfk (ggk (Xi ))(gk (Xr ) - gk (Xi ))
+ σ2ddEξk 〜Q[(ξkadd)τV2kfk(ggk(Xi))ξkadd]
+σm ultEξk~Q[(ξmult θ gk (Xi))τ Vkfk (gk (Xi))(ξmult θ gk ⑵川+。(划.	@)
21
Published as a conference paper at ICLR 2022
Therefore, setting = 0,
Eξk 〜Q[ψ00(0)]
=h00(fk(gk(xi)))Vkfk(gk(Xi))T[(1 - λ)2(gk(xr) - gk (xi))(gk (xr) - gk (xi))
+σaddEξk 〜Q[ξadd(ξadd)T ]+σm “阪上〜Q[(ξmuit(ξmuit)τ)。g® (Xi)Igk (Xi)T ]]
× Vk fk (gk (xi ))
+ (h (fk (gk (Xi ))) - yi)[(1 - λ) (gk (Xr ) - gk (Xi )) Vkfk (gk (Xi ))(gk (Xr ) - gk (Xi ))
+	σ2ddEξk^Q[(ξkadd)TV2kfk(gk(Xi))ξkadd]
+	σmuitEξk〜Q[(ξmult © gk(Xi))TVkfk(gk(xi))(ξmult θ gk(Xi))]].	(70)
The expression for the 殴k and 殴)in the theorem follows upon substituting (70) into (66).	□
B.2 Theorem 2 in the Main Paper and the Proof
We first restate Theorem 2 in the main paper and then provide the proof. Recall that we consider the
binary cross-entropy loss, setting h(z) = log(1 + ez), with the labels y taking value in {0, 1} and
the classifier model f : Rd → R.
Theorem 4 (Theorem 2 in the main paper). Letθ ∈ Θ := {θ : yif(Xi)+(yi-1)f(Xi) ≥ 0 for all i ∈
[n]} be a point such that Vkf(gk(Xi)) and V2kf(gk(Xi)) exist for all i ∈ [n], k ∈ S. Assume that
fk(gk(Xi)) = Vkf(gk(Xi))Tgk(Xi), V2kf(gk(Xi)) = 0 for all i ∈ [n], k ∈ S. In addition, suppose
that IlVf(Xi)∣∣2 > 0 for all i ∈ [n], Er〜Dx[gk(r)] = 0 and Ilgk(Xi)∣∣2 ≥ CXkk√dk for all i ∈ [n],
k ∈ S. Then,
1n
LNFM ≥ — £ max	l(f (Xi + δi), yi) + Lneg + e2φ(e),	(71)
n	n i=1 kδik2≤imix	n
where
mix	∏7 r1 ʌ] ∏7	](k) (k) kVk f(gk (Xi))k2 Γτ∖	m
Ei =	EEλ 〜Dλ [1	一 λ]	∙ Ek 〜S	ri cX ) —∣vf(X∙ )∣2Mdk 卜	(72)
ri(k) = | cos(Vkf(gk(Xi)),gk(Xi))|,	(73)
1n
Lneg = 2n X ∣h00(f (Xi))∣(≡refl )2,	(74)
n i=1
with
(Eireg)2 =E2IVkf(gk(Xi))I22 Eλ[(1-λ)]2Exr[Igk(Xr)I22cos(Vkf(gk(Xi)),gk(Xr))2]
+σa2ddEξ[IξaddI22cos(Vkf(gk(Xi)),ξadd)2]
+ σmult Eξ [Iξmult © gk (Xi)I2 cos(Vk f(gk (Xi )), ξmult © gk (Xi)) ] ,	(75)
and φ is some function such that lim→0 φ(E) = 0.
Theorem 4 says that LnNFM is approximately an upper bound of sum of an adversarial loss with l2-
attack of size Emix = mini Eimix and a feature-dependent regularizer with the strength of mini (Eireg)2 .
Therefore, minimizing the NFM loss would result in a small regularized adversarial loss. We note
that both Eimix and Eireg depend on the cosine similarities between the directional derivatives and the
features at which the derivatives are evaluated at, whereas the Eireg additionally depend on the cosine
similarities between the directional derivatives and the injected noise.
Before proving Theorem 4, we remark that the assumption that fk (gk(Xi)) = Vkf(gk (Xi))Tgk(Xi),
V2kf(gk(Xi)) = 0 for all i ∈ [n], k ∈ S is satisfied by fully connected neural networks with ReLU
activation function or max-pooling. For a proof of this, we refer to Section B.2 in Zhang et al. (2020).
The assumption that Er〜dx [gk (r)] = 0 could be relaxed at the cost of obtaining a more complicated
formula (see Remark 1 for the formula) for the Eireg in the bound, which could be derived in a
straightforward manner.
22
Published as a conference paper at ICLR 2022
Proofof Theorem 4. For h(z) = log(1 + ez), We have h0(z) = ι^ez =: S(Z) ≥ 0 and h00(z)=
(i+Zz)2 = S(z)(1 - S(Z)) ≥ 0. Substituting these expressions into the equation of Theorem 3 and
using the assumptions that fk (gk(Xi)) = Vkf (gk (Xi))Tgk (Xi) and Er〜d, [gk (r)] = 0, we have, for
k ∈S,
RIk) = Eλ~Dλ [1 — λ] Xyi- S(f(Xi)))fk(gk(χi)),	(76)
n	i=1
and we compute:
Rgk) = Eλ〜Dλ [(1 - λ)2] X S(f (Xi))(1 — S(f(χi)))Vkf(gk(Xi))T
2	2n	i=1
X Exr 〜Dχ [(gk (Xr ) - gk(Xi))(gk (Xr ) - gk (Xi )) ]Vk f (gk (Xi ))
Eλ D [(1 — λ)]2 a	T
≥	λ 2n-- E |S (f (Xi))(1 - S(f (Xi)))∣Vk f (gk (Xi ))t
n	i=1
× Exr 〜Dχ[(gk (Xr ) - gk(Xi))(gk (Xr ) - gk (Xi))T]Vk f (gk (Xi))
EX 力[(1 - λ)]2 "	T
=^Dλ2——E |S(f (Xi))(1 - S(f (Xi)))∣Vkf(gk(Xi))t
2n
i=1
× (Exr ~Dx [(gk (Xr )gk (Xr ) ] + gk (Xi )gk (Xi) ])Vk f (gk (Xi))
Eλ D [(1 - λ)]2 "	T 0
=	λ 2^-- E |S (f (Xi))(1 - S(f (Xi)))l(Vk f (gk (Xi))T gk(Xi))2
n	i=1
(77)
(78)
(79)
+
Eλ 力[(1 - λ)]2 "	T „
-2^Dλ2——-E∣S(f (Xi))(1 - S(f (Xi)))IExr ∈Dχ[(Vkf(gk (Xi))T gk (Xr ))2]
2n
i=1
(80)
EX 力[(1 - λ)]2 "	„	„
=-2^Dλ2——-E∣S (f (Xi))(1 - S(f (Xi)))IkVkf (gk (Xi))k2kgk (Xi)k2
2n
i=1
1n
× (cos(Vkf (gk(Xi)),gk(Xi)))2 + 2^ E |S(f (Xi))(1 - S(f (Xi)))∣kVkf(gk(Xi))k2
n i=1
× Eλ[(1 - λ)]2Exr[kgk(Xr)k22cos(Vkf(gk(Xi)),gk(Xr))2]	(81)
1n
≥ 2n E |S(f (Xi))(1 - S(f (Xi)))IkVkf (gk(Xi))k2Eλ〜Dλ [(1 - λ)]2dk(r(k)cxk))2
i=1
1n
+ 2n ∑ |S(f(Xi))(1 - S(f(Xi)))IkVkf(gk(Xi))k2 ∙ Eλ[(1 - λ)]2Exr[kgk(Xr)k2
× cos(Vk f (gk (Xi)), gk (Xr))2]	(82)
1n
=2n E |S(f(Xi))(1 - S(f(Xi)))IkVf(Xi)k2
n i=1
X(E1 〜Dλ[(1 - λ)]2 kVkV(齐；¾)k2dk(r(k)cxk))2)
kVf(Xi)k2
1n
+ 2n ∑ |S(f(Xi))(1 - S(f(Xi)))IkVkf(gk(Xi))k2 ∙ Eλ[(1 - λ)]2Exr[kgk(Xr)k2
X cos(Vk f (gk (Xi)), gk (Xr))2].	(83)
In the above, we have used the facts that E[Z2] = E[Z]2 + V ar(Z) ≥ E[Z]2 and S, S(1 - S) ≥ 0 to
obtain (78), the assumption that Er〜Dx [gk(r)] = 0 to arrive at (79), the assumption that kgk(Xi)Il2 ≥
cxk)√dk for all i ∈ [n], k ∈ S to arrive at (82), and the assumption that IlVf(Xi)k2 > 0 for all
i ∈ [n] to justify the last equation above.
23
Published as a conference paper at ICLR 2022
Next, we bound R(1k), using the assumption that θ ∈ Θ. Note that from our assumption on θ, we
have yif (xi) + (yi - 1)f(xi) ≥ 0, which implies that f (xi) ≥ 0 if yi = 1 and f (xi) ≤ 0 if
yi = 0. Thus, if yi = 1, then (yi - S(f(xi)))fk(gk(xi)) = (1 - S(f(xi)))fk(gk(xi)) ≥ 0, since
f(xi) ≥ 0 and (1 - S(f(xi))) ≥ 0 due to the fact that S(f(xi)) ∈ (0, 1). A similar argument leads
to (yi - S(f(xi)))fk(gk(xi)) ≥ 0 if yi = 0. So, we have (yi - S(f(xi)))fk(gk(xi)) ≥ 0 for all
i ∈ [n].
Therefore, noting that Eλ〜D入[1 一 λ] ≥ 0, We compute:
3	Eλ D [1 一 λ]匚
RIk) = ^Dn——-E |yi - S(f(χi))llfk(gk(χi))l	(84)
i=1
Eλ D [1 一 λ] A
=--------λ------ £ ∣S(f(Xi)) — yilkvk f(gk(xi))k2kgk (Xi)k2∣ COs(Vk f(gk(xi)),gk (Xi))∣
n	i=1
(85)
1 n
≥ — £ |Sf(Xi)) — y/INk f(gk (Xi))k2(Eλ 〜Dλ [1 一 λ]r(k)cXk) VZdk)	(86)
nλ
i=1
=1 XX ∣S(f(xi)) —yilkVf (Xi)k2(E1 〜Dλ [1 — λ] k"f f[χi])k2r(k)cXk)pdk) . (87)
Note that R3(k) = 0 as a consequence of our assumption that V1 2kf(gk(Xi)) = 0 for all i ∈ [n], k ∈ S,
and similar argument leads to:
1n
Radd⑹=2n £|s(〃xi))(1 一 sf(χi)))Vk/&(Xi))TEξk[ξadd(ξadd)T]Vk于®(Xi))侬)
n i=1
1n
= 2nn E |S(f (Xi))(1 - S(f (χi)))∣kVkf(gk(Xi))k2
n i=1
× Eξk[kξkaddk22cos(Vkf(gk(Xi)),ξkadd)2]	(89)
1n
Rmtk = 2nn E∣s(f(χi))(1 - s(f(χi)))∣Vk f(gk(Xi))T (Eξk[ξadd (ξadd)τ] θ gk ("加(Xi)T)
n i=1
× Vkf(gk(Xi))
1n
=厂 E ∣s(f (Xi))(1 - s(f (Xi)))lkVk f(gk (Xi))k2
2n i=1	2
× Eξk [kξkmult θ gk(Xi)k22cos(Vkf(gk(Xi)),ξmult θ gk(Xi))2].	(90)
24
Published as a conference paper at ICLR 2022
Using Theorem 3 and the above results, we obtain:
1n
LN - - X i(f (Xi),yi)
≥ Ek[eRik) + e2R2k) + e2Radd(k) + e2Rmult(k) + e2^(e)]	(91)
-n
≥ - E∣s(f (χi)) — yi∣kvf (χi)k2emix	(92)
-n
+ 2nn ∑ |S(f (xi))(l - S(f (xi)))∣kVf (xi)k2(emix)2
-n
+ 2nn ∑ |S (f (xi))(l - S(f (Xi)))lkvk f (gk (Xi))k2 ∙ Eλ[(l - λ)]2Eχr [kgk(xr )k2
n i=1
× cos(Vkf(gk(xi)), gk(xr))2]	(93)
-n
+ 2nn ∑ |S(f (xi))(l - S(f (xi)))∣(enoise)2 + e%(e),	(94)
where eɪmi" := eE"〜Dλ [l - λ]Ek [ k*f fX(Xk2)k2r(k)cXk)√dk∣ and
(einoise)2 =e2kVkf(gk(xi))k22σa2ddEξk[kξkaddk22cos(Vkf(gk(xi)),ξkadd)2]
+σm2 ultEξk[kξkmultgk(xi)k22cos(Vkf(gk(xi)),ξkmultgk(xi))2] .	(95)
On the other hand, for any small parameters ei > 0 and any inputs z1, . . . , zn, we can, using a
second-order Taylor expansion and then applying our assumptions, compute:
ln	ln
n X kδmaχjf (Zi+殆加-n Xl(f (zi),yi)
nn
≤ -X ∣s(f(Zi))- yi∣kVf(Zi)k2ei + 2nnX ∣s(f(Zi))(i - S(f(Zi)))IkVf(Zi)k2≡2
i=1
i=1
ln
+nX ksmaxJiM")	(96)
nn
≤ - X ∣S(f(Zi))- yiIkVf(Zi)k2ei + 2nn X ∣S(f(Zi))(l - S(f (Zi)))IkVf (Zi)k2e2
n i=1	n i=1
ln
+ -之导i0(ei),	(97)
where the φi are functions such that limz→o 夕：(z) = 0,夕i0(ei) := maxkδik2≤% 夕i(δi) and
limz→0 2i0(Z) = 0.
Combining (94) and (97), we see that
nn
LInFM ≥ — Xιιχ maχ . l(f(g + 6产),m)+ Lneg + e2/) — — £(留工)2/号力
n	n	kδimix k2 ≤imix	n	n
(98)
ln
=：-∑ max . l(f(xi + δimix),yi)+ Lnneg + e2φ(e),	(99)
n	kδimix k2 ≤imix
where Lneg is defined in the theorem. Noting that lime→o φ(e) = 0, the proof is done.	□
25
Published as a conference paper at ICLR 2022
Remark 1. Had we assumed that Er~Dx [gk(r)] = 0, then the statements of Theorem 4 remain
unchanged, but with (ireg)2 replaced by
He )2= e2kVk f(gk (Xi))k2 (El[(1- λ)]2Eχr [kgk (Xr )k2 Cos(Vk f (gk (Xi )),gk (Xr ))2]
+ σ2ddEξ[kξaddk2 Cos(Vk f(gk (Xi)),ξadd)2
+σm2 ultEξ[kξmult	gk(Xi)k22 cos(Vk f (gk (Xi)), ξmult	gk(Xi))2]
- 2Eλ[(1 - λ)]2Vkf(gk(Xi))T [Ergk(r)gk(Xi)T + gk(Xi)Ergk(r)T]Vkf(gk(Xi)).
(100)
C NFM Through the Lens of Implicit Regularization and
Clas sification Margin
First, we define classification margin at the input level. We shall show that minimizing the NFM
loss can lead to an increase in the classification margin, and therefore improve model robustness in
this sense.
Definition 2 (Classification Margin). The classification margin of a training input-label sample
si := (Xi, ci) measured by the Euclidean metric d is defined as the radius of the largest d-metric ball
in X centered at Xi that is contained in the decision region associated with the class label ci, i.e., it
is: γd(si) = sup{a : d(Xi, X) ≤ a ⇒ g(X) = ci ∀X}.
Intuitively, a larger classification margin allows a classifier to associate a larger region centered
on a point Xi in the input space to the same class. This makes the classifier less sensitive to input
perturbations, and a perturbation of Xi is still likely to fall within this region, keeping the classifier
prediction. In this sense, the classifier becomes more robust. In the typical case, the networks are
trained by a loss (cross-entropy) that promotes separation of different classes in the network output.
This, in turn, maximizes a certain notion of score of each training sample (Sokolic et al., 2017).
Definition 3 (Score). For an input-label training sample si = (Xi, ci), we define its score as
o(si) = minj=ɑ √2(eca — ej)tf (Xi) ≥ 0, where ei ∈ RK is the Kronecker delta vector (one-hot
vector) with eii = 1 and eij = 0 for i 6= j .
A positive score implies that at the network output, classes are separated by a margin that corresponds
to the score. A large score may not imply a large classification margin, but score can be related to
classification margin via the following bound.
Proposition 1. Assume that the score o(si) > 0 and let k ∈ S. Then, the classification margin for
the training sample si can be lower bounded as:
γd(si) ≥
___________C(si)___________
suPχ∈conv(X) kVk f(gk (X))k2
(101)
whereC(si) = o(si)/ supx∈conv(X) kVgk(X)k2.
Since NFM implicitly reduces the feature-output Jacobians Vkf (including the input-output Jacobian)
according to the mixup level and noise levels (see Proposition 3), this, together with Theorem 1,
suggests that applying NFM implicitly increases the classification margin, thereby making the model
more robust to input perturbations. We note that a similar, albeit more involved, bound can also be
obtained for the all-layer margin, a more refined version of classification margin introduced in (Wei
& Ma, 2019b), and the conclusion that applying NFM implicitly increases the margin also holds.
We now prove the proposition.
Proof of Proposition 1. Note that, for any k ∈ S, Vf (X) = Vkf(gk(X))Vgk(X) by the chain rule,
and so
kVf(X)k2 ≤ kVkf(gk(X))k2kVgk(X)k2
≤
sup
x∈conv(X)
kVk f (gk (X))k2	sup	kVgk(X)k2 .
x∈conv(X)
(102)
(103)
26
Published as a conference paper at ICLR 2022
The statement in the proposition follows from a straightforward application of Theorem 4 in (Sokolic
et al., 2017) together with the above bound.	□
D NFM Through the Lens of Probabilistic Robustness
Since the main novelty of NFM lies in the introduction of noise injection, it would be insightful
to isolate the robustness boosting benefits of injecting noise on top of manifold mixup. We shall
demonstrate the isolated benefit in this section.
The key idea is based on the observation that manifold mixup produces minibatch outputs that lie in
the convex hull of the feature space at each iteration. Therefore, for k ∈ S, NFM(k) can be viewed
as injecting noise to the layer k features sampled from some distribution over conv(gk(X)), and so
the NFM(k) neural network Fk can be viewed as a probabilistic mapping from conv(gk(X)) to
P(Y), the space of probability distributions on Y.
To isolate the benefit of noise injection, we adapt the approach of (Pinot et al., 2019a; 2021) to our
setting to show that the Gaussian noise injection procedure in NFM robustifies manifold mixup in a
probabilistic sense. At its core, this probabilistic notion of robustness amounts to making the model
locally Lipschitz with respect to some distance on the input and output space, ensuring that a small
perturbation in the input will not lead to large changes (as measured by some probability metric) in
the output. Interestingly, it is related to a notion of differential privacy (Lecuyer et al., 2019; Dwork
et al., 2014), as formalized in (Pinot et al., 2019b).
We now formalize this probabilistic notion of robustness.
Let P > 0. We say that a standard model f : X → Y is a?-robust if for any (x, y)〜D such that
f(x) = y, one has, for any data perturbation τ ∈ X,
kτkp ≤ αp =⇒ f(x) = f(x + τ).	(104)
Analogous definition can be formulated when output of the model is distribution-valued.
Definition 4 (Probabilistic robustness). A probabilistic model F : X → P(Y) is called (αp, )-robust
with respect to D if, for any x, τ ∈ X, one has
kτkp ≤ αp =⇒ D(F (x), F(x + τ)) ≤ ,	(105)
where D is a metric or divergence between two probability distributions.
We refer to the probabilistic model (built on top of a manifold mixup classifier) that injects Gaus-
sian noise to the layer k features as probabilistic FM model, and we denote it by F noisy(k) :
conv(gk (X)) → P(Y). We denote G as the classifier constructed from F noisy(k), i.e., G : x 7→
arg maxj∈[K] [F noisy(k)]j (x).
In the sequel, we take D to be the total variation distance DT V , defined as:
DTV (P, Q) := sup |P(S) - Q(S)|,	(106)
S⊂X
for any two distributions P and Q over X. Recall that ifP and Q have densities ρp and ρq respectively,
then the total variation distance is half ofthe L1 distance, i.e., DTV (P, Q) = 2 JX ∣Pp(x) - Pq(x)|dx.
The choice of the distance depends on the problem on hand and will give rise to different notions of
robustness. One could also consider other statistical distances such as the Wasserstein distance and
Renyi divergence, which can be related to total variation (see (Pinot et al., 2021; Gibbs & Su, 2002)
for details).
Before presenting our main result in this section, we need the following notation. Let Σ(x) :=
σa2ddI + σm2 ultxxT . For x, τ ∈ X, let Πx be a dk by dk - 1 matrix whose columns form a basis
for the subspace orthogonal to gk(x + τ) - gk(x), and {ρi(gk(x), τ)}i∈[dk-1] be the eigenvalues of
(ΠxTΣ(gk(x))Πx)-1ΠxTΣ(gk(x + τ))Πx - I. Also, let [F]topk (x) denote the kth highest value of
the entries in the vector F (x).
Viewing an NFM(k) classifier as a probabilistic FM classifier, we have the following result.
27
Published as a conference paper at ICLR 2022
Theorem 5 (Gaussian noise injection robustifies FM classifiers). Let k ∈ S, dk > 1, and assume
that gk (x)gk (x)T ≥ βk2I > 0 for all x ∈ conv(X ) for some constant βk. Then, F noisy(k) is
(αp, k (p, d, αp, σadd, σmult))-robust with respect to DTV against lp adversaries, with
Ek(p,d,αp,σadd,σmuit) = 2min{l, max{A, B}},	(107)
where
A = Ap(αp)
σmult
σ2 + σ2 β2
σadd + σmultβk
Z1
0
Vgk(x + tτ )dt
2kgk(x)k2
α α ɑ καp(1p∈(0,2] + d 1/2 1∕P1p∈(2,∞) + √d1p=∞)
B = Bk (T)	Padd+σm Ultek
with
and
Z1
0
Vgk (x + tτ)dt
(αp1ɑp<1 + αp1αp≥1,	ifP ∈ (0, 2],
d1/2T/p(ap1ap<1 + αp1ɑp≥1),	if P ∈ (2, ∞),
√d(αp1αp <1 + αp 1ɑp≥ι),	f P = ∞,
2
(108)
(109)
(110)
Bk (τ) =	sup
x∈conv(X)
Z0
1
Vgk (x + tτ )dt
dk-1
ρi2 (gk (x), τ)
i=1
(111)
2
2
+
2 Λ
Moreover, if x ∈ X is such that [F noisy(k)]top1(x) ≥ [F noisy(k)]top2(x) + 2E(P, d, αp, σadd, σmult),
then for any τ ∈ X, we have
kτ kp ≤ α =⇒ G(x) = G(x + τ ),	(112)
for any P > 0.
Theorem 5 implies that we can inject Gaussian noise into the feature mixup representation to improve
robustness of FM classifiers in the sense of Definition 4, while keeping track of maximal loss in
accuracy incurred under attack, by tuning the noise levels σadd and σmult . To illustrate this, suppose
that σmult = 0 and consider the case of P = 2, in which case A = 0, B 〜 ɑ2 /σ&&& and so injecting
additive Gaussian noise can help controlling the change in the model output, keeping the classifier’s
prediction, when the data perturbation is of size α2 .
We now prove Theorem 5. Before this, we need the following lemma.
Lemma 3. Let x1 := z ∈ Rdk and x2 := z + τ ∈ Rdk, with τ > 0 and dk > 1, and Σ(x) :=
σa2ddI + σm2 ultxxT ≥ (σa2dd + σm2 ultβ2)I > 0, for some constant β, for all x. Let Π be a dk by dk - 1
matrix whose columns form a basis for the subspace orthogonal to τ, and let ρ1 (z, τ), . . . , ρdk-1(z, τ)
denote the eigenvalues of(ΠTΣ(x1)Π)-1ΠTΣ(x2)Π - I.
Define the function C(x1, x2, Σ) := max{A, B}, where
2
σmult
σ2 + σ e
σadd + σmultβ
(kτk22+2τTz),
B =	kτk2
Pσ2dd+σm ultβ2 ∖
dk-1
X ρi2 (z, τ).
i=1
(113)
(114)
A
Then, the total variation distance between N(x1, Σ(x1)) andN(x2, Σ(x2)) admits the following
bounds:
1 ≤ DTV(N(X1, Σ(xι)),N(X2, Σ(x2))) ≤ 9
200 —	min{ 1, C(xι, x2, Σ)}	一 2
(115)
Proof of Lemma 3. The result follows from a straightforward application of Theorem 1.2 in (Devroye
et al., 2018), which provides bounds on the total variation distance between Gaussians with different
means and covariances.	□
28
Published as a conference paper at ICLR 2022
With this lemma in hand, we now prove Theorem 5.
Proof of Theorem 5. We denote the noise injection procedure by the map I : x → N(x, Σ(x)),
where Σ(x) = σa2ddI + σm2 ultxxT .
Let x ∈ X be a test datapoint and τ ∈ X be a data perturbation such that kτ kp ≤ αp for p > 0.
Note that
DTV (Fk(I(gk(x))), Fk(I(gk(x + τ)))) ≤ DTV (I(gk(x)),I(gk(x + τ)))	(116)
≤ DTV(I(gk(x)),I(gk(x) + gk(x + τ) - gk(x)))
(117)
= DTV (I(gk(x)),I (gk(x) + τk))	(118)
≤ 9min{1, Φ(gk(x),τk,σadd,σmuit,βk)},	(119)
where τk := gk(x + τ) - gk(x)
of calculus, and
Vgk (X + tτ)dt I T by the generalized fundamental theorem
Φ(gk(x), τk, σadd, σmult, βk)
maχ I Ck +mult—β2 (kτk Hi+2g ,gk (Xyi)
I add + multβk
kτk k2
Pσ2dd+σm Ultek ∖
dk-1
X ρi2(gk(x),τ)	,
i=1	
(120)
where the ρi(gk(X), τ) are the eigenvalues given in the theorem.
In the first line above, we have used the data preprocessing inequality (Theorem 6 in (Pinot
et al., 2021)), and the last line follows from applying Lemma 3 together with the assumption
that gk(X)gk(X)T ≥ βkk > 0 for all X.
Using the bounds
and
kτkkk ≤
Z1
0
Vgk(X +tτ)dt	kτkk
k
KTk,gk (x)i| ≤
kgk(X)kk
0
Vgk (X + tT)dt
kTkk,
k
(121)
(122)
we have
Φ(gk(X), Tk, σadd, σmult,βk) ≤ maχ {A, B} ,	(123)
where
k	1	k	1		
A = Kk Mkt Rk	Vgk(x+tτ)dt		kTkkk + 2kgk(X)kk ∣	Vgk(X+tT)dt∣ kTkk
σadd	σmult k	0 and	k	0k (124)	
∣∣Ro1 Vgk(X+tτ )dt∣∣2 kτ kk Padd + σm Ultek	∖	udk -1 ρik(gk(X), T)	(125) i=1	
≤ sup (	/ Vgk (x + tτ)dt ∙ ʌ x∈conv(X)	∣ 0	∣k =: B (T)	kτkk : k ( ) ∕σ ~^+^σk	β. σadd + σmUlt ek		XIPk(gk (x),τ J JTk	βk	(126) i=1	σadd + σmUltek (127)
29
Published as a conference paper at ICLR 2022
The first statement of the theorem then follows from the facts that kτ k2 ≤ kτ kp ≤ αp for p ∈ (0, 2],
∣∣τ∣∣2 ≤ d1∕2-1∕qkτkq ≤ d1∕2-1∕qαq for q > 2, and ∣∣τ∣∣2 ≤ √d∣∣τ∣∣∞ ≤ √dα∞ for any T ∈ Rd.
In particular, these imply that A ≤ CAp , where
(αp1αp<1 + αP1αp≥1,	ifp∈(0,2],
Ap = d d1∕2-1∕p(αp 1αp<ι + αp 1ɑp≥ι), if P ∈ (2, ∞),
I √d(αp1αp<ι + αp1ɑp ≥ι),	if P = ∞,
(128)
and
C:
σmult
σ2 + σ2 β2
σadd + σmultβk
Z01
Vgk (x + tτ )dt
2
+2∣gk(x)∣2
2
Z Vgk(x +tτ)dt
0
(129)
2
The last statement in the theorem essentially follows from Proposition 3 in (Pinot et al., 2021).	□
E On Generalization Bounds for NFM
Let F be the family of mappings x 7→ f(x) and Zn := ((xi, yi))i∈[n]. Given a loss function l, the
Rademacher complexity of the set l ◦ F := {(x, y) 7→ l(f (x), y) : f ∈ F} is defined as:
1n
Rn(l ◦F ):= EZn,σ sup - σ^(f(Xif (xi),yi) ,	(130)
f∈F ni=1
where σ := (σ1 , . . . , σn), with the σi independent uniform random variables taking values in
{-1, 1}.
Following (Lamb et al., 2019), we can derive the following generalization bound for the NFM
loss function, i.e., the upper bound on the difference between the expected error on unseen data
and the NFM loss. This bound shows that NFM can reduce overfitting and give rise to improved
generalization.
Theorem 6 (Generalization bound for the NFM loss). Assume that the loss function l satisfies
|l(x, y) - l(x0, y)| ≤ M for all x, x0 and y. Then, for every δ > 0, with probability at least 1 - δ
over a draw of n i.i.d. samples {(xi, yi)}in=1, we have the following generalization bound: for all
mapsf ∈ F,
Ex,y[l(f (x), y)] - LNFM ≤ 2Rn(l ◦ F) + 2Mʌ/ln^ - Qe(f),	(131)
where
Qe(f) = E[eR,) + e2 R2k) + e2R3k)] + e%(e),	(132)
for SomefUnction 夕 such that limχ→∞ 夕(x) = 0.
To compare the generalization behavior of NFM with that without using NFM, we also need the
following generalization bound for the standard loss function.
Theorem 7 (Generalization bound for the standard loss). Assume that the loss function l satisfies
|l(x, y) - l(x0, y)| ≤ M for all x, x0 and y. Then, for every δ> 0, with probability at least 1 - δ
over a draw of n i.i.d. samples {(xi, yi)}in=1, we have the following generalization bound: for all
maps f ∈ F,
Ex,y [l(f (x), y)] - Lnd ≤ 2Rn(l ◦ F) + 2Mʌ/ln(^.	(133)
By comparing the above two theorems and following the argument of (Lamb et al., 2019), we see
that the generalization benefit of NFM comes from two mechanisms. The first mechanism is based
on the term Q(f ). Assuming that the Rademacher complexity term is the same for both methods,
then NFM has a better generalization bound than that of standard method if Q(f ) > 0. The second
mechanism is based on the Rademacher complexity term Rn(l ◦ F). For certain families of neural
networks, this term can be bounded by the norms of the hidden layers of the network and the norms
of the Jacobians of each layer with respect to all previous layers (Wei & Ma, 2019a;b). Therefore,
30
Published as a conference paper at ICLR 2022
this term differs for the case of training using NFM and the case of standard training. Since NFM
implicitly reduces the feature-output Jacobians (see Theorem 3), we can argue that NFM leads to a
smaller Rademacher complexity term and hence a better generalization bound.
We now prove Theorem 6. The proof of Theorem 7 follows the same argument as that of Theorem 6.
Proof of Theorem 6. Let Zn := {(xi, yi)}i∈[n] and Zn0 := {(x0i, yi0)}i∈[n] be two test datasets, where
Zn0 differs from Zn by exactly one point of an arbitrary index i0 .
Denote GE(Zn) := supf∈F Ex,y [l(f (x), y)] - LnNFM, where LnNFM is computed using the dataset
Zn, and likewise for GE(Zn0 ). Then,
GE(Zn) - GE(Zn) ≤ M(2n2- 1) ≤ 2M,	(134)
n	n2	n
where we have used the fact that LnNFM has n2 terms and there are 2n- 1 different terms for Zn
and Zn. Similarly, We have GE (Zn) - GE(Zn) ≤ 2M.
Therefore, by McDiarmid’s inequality, for any δ > 0, with probability at least 1 - δ,
GE(Zn) ≤ EZn [GE(Zn)] + 2Mʌ/ln^.	(135)
Applying Theorem 3, We have
GE(Zn) ≤ EZn
sup EZn0
f∈F n
sup EZn0
f∈F n
1XX i(f(χi),yi)l -LNFM +2M/n(ɪɪɪɪ
n	n	2n
i=1
nn
—X l(f (χi),yi) — - X l(f(xi),yi) - Qe(f)
nn
i=1	i=1
+ 2M ∖
≤ Ezn,zn sup1 XX(i(f (χi),y0) - I(f(χi),yi)) - Q") + 2M∖/ln(ɪ/ɪɪ
n f∈F ni=1	2n
(136)
(137)
(138)
≤ Ezn,zn,σ supɪ XXσi(l(f (χi),yi) - l(f(xi),yi) - Qe(f ) + 2M∖∕ln(^
n n f∈F n	2n
i=1
(139)
≤ 2EZn,σ SuP — ^X σil(f (xi),yi) - Qe(f) + 2M∖ ―)
f∈F ni=1	2n
=2Rn (l ◦ F) - Qe(f) + 2M∖/ɪn(ɪ/ɪ) ,
2n
(140)
(141)
where (136) uses the definition of GE(Zn),(137) uses ±n Pn=ι l(f (xi), yi) inside the expectation
and the linearity of expectation, (138) folloWs from the Jensen’s inequality and the convexity of
the supremum, (139) follows from the fact that σi(l(f (x0i), yi0) - l(f (xi), yi)) and l(f(x0i), yi0) -
l(f (xi), yi) have the same distribution for each σi ∈ {-1, 1} (since Zn, Zn0 are drawn i.i.d. with the
same distribution), and (140) follows from the subadditivity of supremum.
The bound in the theorem then follows from the above bound.
□
31
Published as a conference paper at ICLR 2022
F	Additional Experiments and Details
F.1 Input Perturbations
We consider the following three types of data perturbations during inference time:
•	White noise perturbations are constructed as X = X + ∆x, where the additive noise is sampled
from a GaUSSian distribution ∆x 〜N(0, σ). This perturbation strategy emulates measurement
errors that can result from data acquisition with poor sensors (where σ corresponds to the severity
of these errors).
•	Salt and pepper perturbations emulate defective pixels that result from converting analog signals to
digital signals. The noise model takes the form P(X = X) = 1 - γ, and P(X = max) = P(X =
mm) = γ∕2, where X(i,j) denotes the corrupted image and mm, max denote the minimum and
maximum pixel values, respectively. γ parameterizes the proportion of defective pixels.
•	Adversarial perturbations are “worst-case” non-random perturbations that maximize the loss
'(gδ(X + ∆X),y) subject to the constraint ∣∣∆X∣∣ ≤ r on the norm of the perturbation. We
consider the projected gradient decent for constructing these perturbations (Madry et al., 2017).
F.2 Illustration of the Effects of NFM on Toy Datasets
We consider a binary classification task for the noise corrupted 2D dataset whose data points form
two concentric circles. Points on the same circle corresponds to the same label class. We generate
500 samples, setting the scale factor between inner and outer circle to be 0.05 and adding Gaussian
noise with zero mean and standard deviation of 0.3 to the samples. Fig. 8 shows the training and
test data points. We train a fully connected feedforward neural network that has four layers with the
ReLU activation functions on these data, using 300 points for training and 200 for testing. All models
are trained with Adam and learning rate 0.1, and the seed is fixed across all experiments. Note that
the learning rate can be considered as a temperature parameter which introduces some amount of
regularization itself. Hence, we choose a learning rate that is large for this problem to better illustrate
the regularization effects imposed by the different schemes that we consider.
Fig. 2 illustrates how different regularization strategies affect the decision boundaries of the neural
network classifier. The decision boundaries and the test accuracy indicate that white noise injections
and dropout (we explore dropout rates in the range [0.0, 0.9] and we finds that 0.2 yields the best
performance) introduce a favorable amount of regularization. Most notably is the effect of weight
decay (we use 9e-3), i.e., the decision boundary is nicely smoothed and the test accuracy is improved.
In contrast, the simple mixup data augmentation scheme shows no benefits here, whereas manifold
mixup is improving the predictive accuracy considerably. Combining mixup (manifold mixup) with
noise injections yields the best performance in terms of both smoothness of the decision boundary
and predictive accuracy. Indeed, NFM is outperforming all other methods here.
The performance could be further improved by combining NFM with weight decay or dropout. This
shows that there are interaction effects between different regularization schemes. In practice, when
(a) Data points for training.	(b) Data points for testing.
Figure 8: The toy dataset in R2 that we use for binary classification.
32
Published as a conference paper at ICLR 2022
Test Accuracy
--Baseline
——Mixup (a= 0.1]
Noisy Mixup (cr = 0.1)
—— Manifold Mixup (σ = 0.1)
—— Noisy Feature Mixup (*)
--Noisy Feature Mixup (α = 0.1)
'° --------------------
--∙-- Baseline	~~J	--
—	Mixup (α = 0.1)
Noisy Mixup (α = 0.1)、、
Manifold Mixup (cr = 0.1) 、
-	*- Noisy Feature Mixup (*)	∖
—	— Noisy Feature Mixup (α = 0.1)
90
80
70
60
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
White Noise (σ)
90
80
70
60
0.00	0.05	0.10	0.15	0.20
Salt and Pepper Noise (γ)
Figure 9: Vision transformers evaluated on CIFAR-10 with different training schemes.
Table 4: Robustness of Wide-ResNet-18 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on CIFAR-100. The results are averaged over 5 models trained with different seed values.
Scheme	Clean (%)	0.1	σ (%) 0.2	0.3	γ(%) 0.08	0.12	0.2
Baseline	91.3	89.4	77.0	56.7	83.2 74.6	48.6
Mixup (α = 0.1) Zhang et al. (2017)	91.2	89.5	77.6	57.7	82.9 74.6	48.6
Mixup (α = 0.2) Zhang et al. (2017)	91.2	89.2	77.8	58.9	82.6 74.5	47.9
Noisy Mixup (α = 0.1) Yang et al. (2020b)	90.9	90.4	87.5	80.2	84.0 79.4	63.8
Noisy Mixup (α = 0.2) Yang et al. (2020b)	90.9	90.4	87.4	79.8	83.8 79.3	63.4
Manifold Mixup (α = 0.1) Verma et al. (2019)	91.2	89.2	77.2	56.9	83.0 74.3	47.1
Manifold Mixup (α = 1.0) Verma et al. (2019)	90.2	88.4	76.0	55.1	81.3 71.4	42.7
Manifold Mixup (α = 2.0) Verma et al. (2019)	89.0	87.0	74.3	53.7	79.8 70.3	41.9
Noisy Feature Mixup (α = 0.1)	91.4	90.2	88.2	84.8	84.4 81.2	74.4
Noisy Feature Mixup (α = 1.0)	89.8	89.1	86.6	82.7	82.5 79.0	71.4
Noisy Feature Mixup (α = 2.0)	88.4	87.6	84.6	80.1	80.4 76.5	68.6
one trains deep neural networks, different regularization strategies are considered as knobs that are
fine-tuned. From this perspective, NFM provides additional knobs to further improve a model.
F.3 Additional Results for Vision Transformers
Here we consider compact vision transformer (ViT-lite) with 7 attention layers and 4 heads (Hassani
et al., 2021). Fig. 9 (left) compares vision transformers trained with different data augmentation
strategies. Again, NFM improves the robustness of the models while achieving state-of-the-art
accuracy when evaluated on clean data. However, mixup and manifold mixup do not boost the
robustness. Further, Fig. 9 (right) shows that that the vision transformer is less sensitive to salt and
pepper perturbations as compared to the ResNet model. These results are consistent with the high
robustness properties of transformers recently reported in Shao et al. (2021); Paul & Chen (2021).
Table 4 provides additional results for different α values.
Table 4 shows results for vision transformers trained with different data augmentation schemes and
different values of α. It can be seen that NFM with α = 0.1 helps to improve the predictive accuracy
on clean data while also improving the robustness of the models. For example, the model trained
with NFM shows about a 25% improvement compared to the baseline model when faced with salt
and paper perturbations (γ = 0.2). Further, our results indicate that larger values of α have a negative
effect on the generalization performance of vision transformer.
F.4 Ablation S tudy
In Table 5 we provide a detailed ablation study where we vary several knobs. First, we can see that
just injecting noise helps to improve robustness, but the test accuracy is only marginally improving.
On the other hand, just mixing inputs and hidden features improves the testing performance of the
model, but it does not significantly improve the robustness of a model. In contrast, the NFM scheme
combines best of both worlds and shows that both accuracy and robustness can be increased. Varying
the noise levels indicate that there is a trade-off between test accuracy on clean data and robustness to
33
Published as a conference paper at ICLR 2022
perturbations. We also vary the mixup parameter α to show that the good performance is consistent
across a range of different values.
Table 5: Ablation study using Wide-ResNet-18 trained and evaluated on CIFAR-100.
Mixup Manifold Noise Injections α Noise Levels Clean (%)	σ (%)	Y (%)
				σadd	σmult		0.1	0.25	0.5	0.06	0.1	0.15
X	X	X	-	0	0	76.9	64.6	42.0	23.5	58.1	39.8	15.1
X	X	✓	-	0.4	0.2	78.1	76.2	65.7	46.6	70.0	58.8	28.4
✓	X	X	1	0	0	80.3	72.5	54.0	33.4	62.5	43.8	16.2
✓	X	✓	1	0.4	0.2	78.9	78.6	66.6	46.7	66.6	53.4	25.9
✓	✓	X	0.2	0	0	79.7	70.6	46.6	25.3	62.1	43.0	15.2
✓	✓	X	1	0	0	79.7	70.5	45.0	23.8	62.1	42.8	14.8
✓	✓	X	2	0	0	79.2	69.3	43.8	23.0	62.8	44.2	16.0
✓	✓	✓	1	0.1	0.1	-810~~	76.2	56.6	36.4	66.8	49.7	21.4
✓	✓	✓	0.2	0.4	0.2	80.6	79.2	70.2	51.7	71.5	60.4	30.3
✓	✓	✓	1	0.4	0.2	80.9	80.1	72.1	55.3	72.8	62.1	34.4
✓	✓	✓	2	0.4	0.2	80.7	80.0	71.5	53.9	72.7	62.7	36.6
✓	✓	✓	1	0.8	0.4	80.3	80.1	75.5	66.4	74.3	66.5	44.6
F.5 Additional Results for ResNets with Higher Levels of Noise Injections
In the experiments in Section 5, we considered models trained with NFM that use noise injection
levels σadd = 0.4 and σmult = 0.2, whereas the ablation model uses σadd = 1.0 and σmult = 0.5.
Here, we want to better illustrate the trade-off between accuracy and robustness. We saw that there
exists a potential sweet-spot where we are able to improve both the predictive accuracy and the
robustness of the model. However, if the primary aim is to push the robustness of the model, then we
need to sacrifice some amount of accuracy.
Fig. 10 is illustrating this trade-off for pre-actived ResNet-18s trained on CIFAR-10. We can see
that increased levels of noise injections considerably improve the robustness, while the accuracy on
clean data points drops. In practice, the amount of noise injection that the user chooses depend on the
situation. If robustness is critical, than higher noise levels can be used. If adversarial examples are the
main concern, than other training strategies such as adversarial training might be favorable. However,
the advantage of NFM over adversarial training is that (a) we have a more favorable trade-off between
robustness and accuracy in the small noise regime, and (b) NFM is computationally inexpensive,
when compared to most adversarial training schemes. This is further illustrated in the next section.
F.6 Comparison with Adversarial Trained Models
Here, we compare NFM to adversarial training in the small noise regime, i.e., the situation where
models do not show a significant drop on the clean test set. Specifically, we consider the projected
gradient decent (PGD) method (Madry et al., 2017) using 7 attack iterations and varying l2 per-
5 0 5 0 5
9 9 8 8 7
ycaruccA tseT
'、	、、、7
- ɑadd = 0-4, ‰⅛= 0.2	∖	'、、、、
- σadd = 0-6f‰⅛= θ-3	'∖	'、、
-j,- Oadd = °∙8, σmu∣t = 0.4
Oadd = 1-0» OmUit = θ∙5
―一 Oadd = L2, σmυ∣t = 0.6
70
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
WhiteNoise (σ)
70
0.000 0.025 0.050 0.075 0.100 0.125 0.150
Salt and Pepper Noise (γ)
Figure 10: Pre-actived ResNet-18 evaluated on CIFAR-10 trained with NFM and varying levels of
additive (σadd) and multiplicative (σmult) noise injections. Shaded regions indicate one standard
deviation about the mean. Averaged across 5 random seeds.
34
Published as a conference paper at ICLR 2022
J -------- 、3、、、、、、
-→- ε = 0.01	、\
-- NFM (σadd = 0.4,σmoft = 0.2)	、、、、「、、、、
一一 NFM (σadd= 1.2,σmoft = 0.6)	"飞、
ycaruccA tse
95
90
85
80
75 J ———	W、
70 - £ = 001	W：
-T- NFM (σadd = 0A,σmυ∣t≈Q2) 、、、、、
65 —— NFM (σadd=1.2,σmoft = 0.6)	、二、
60 J'------------------------1------r----
0.000 0.025 0.050 0.075 0.100 0.125 0.150
Adverserial Noise (e)
95
90
85
80
75
70
65
600.00
0.02	0.04	0.06	0.08	0.10
AdverseriaI Noise (e)
0.12
Figure 11: Pre-actived ResNet-18 evaluated on CIFAR-10 (left) and Wide ResNet-18 evaluated on
CIFAR-100 (right) with respect to adversarial perturbed inputs. Shaded regions indicate one standard
deviation about the mean. Averaged across 5 random seeds.
turbation levels e to train adversarial robust models. First, we compare how resilient the different
models are with respect to adversarial input perturbations during inference time (Fig. 11; left). Again
the adversarial examples are constructed using the PGD method with 7 attack iterations. Not very
surprisingly, the adversarial trained model with e = 0.01 features the best resilience while sacrificing
about 0.5% accuracy as compared to the baseline model (here not shown). In contrast, the models
trained with NFM are less robust, while being about 1 - 1.5% more accurate on clean data.
Next, we compare in (Fig. 11; right) the robustness with respect to salt and pepper perturbations, i.e.,
perturbations that both models have not seen before. Interestingly, here we see an advantage of the
NFM scheme with high noise injection levels as compared to the adversarial trained models.
F.7 Feature Visualization Comparison
In this subsection, we concern ourselves with comparing the features learned by three ResNet-
50 models trained on Restricted Imagenet (Tsipras et al., 2018): without mixup, manifold mixup
(Verma et al., 2019), and NFM. We can compare features by maximizing randomly chosen pre-logit
activations of each model with respect to the input, as described by Engstrom et al. (2020). We do so
for all models with Projected Gradient Ascent over 200 iterations, a step size of 16, and an `2 norm
constraint of 2,000. Both the models trained with manifold mixup and NFM use an α = 0.2, and the
NFM model uses in addition σadd = 2.4 and σmult = 1.2. The result, as shown in Fig. 12, is that the
features learned by the model trained with NFM are slightly stronger (i.e., different from random
noise) than the clean model.
F.8 Train and Test Error for CIFAR- 1 00
Figure 13 shows models trained with different training schemes on CIFAR-100. Compared to the
baseline model, the models trained with manifold mixup and NFM have a similar convergence
behavior. However, they are able to achieve a smaller test error. This shows that both manifold mixup
and NFM have a favorable implicit regularization effect, where the effect is more pronounced for the
NFM scheme.
35
Published as a conference paper at ICLR 2022
Manifold Mixup	NFM (Ours)
Seed
No Mixup
Figure 12: The features learned by the NFM classifier are slightly stronger (i.e., different from
random noise) than the clean model. See Subsection F.7 for more details.
O
10
Oooo
8 6 4 2
,lota,E-rati
200
(a) Train error.
50	100	150
Number of epochs
0
O
O
10
Oooo
8 6 4 2
,lo己
50	100	150
Number of epochs
200
(b) Test error.
0
O
Figure 13: Train (a) and test (b) error for a pre-actived Wide-ReSNet-18 trained on CIFAR-100.
36