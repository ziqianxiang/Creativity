Published as a conference paper at ICLR 2022
Self-ensemble Adversarial Training for Im-
proved Robustness
Hongjun Wang1 Yisen Wang1,2* *
1	Key Lab. of Machine Perception (MoE), School of Artificial Intelligence, Peking University
2	Institute for Artificial Intelligence, Peking University
Ab stract
Due to numerous breakthroughs in real-world applications brought by machine
intelligence, deep neural networks (DNNs) are widely employed in critical applica-
tions. However, predictions of DNNs are easily manipulated with imperceptible
adversarial perturbations, which impedes the further deployment of DNNs and may
result in profound security and privacy implications. By incorporating adversarial
samples into the training data pool, adversarial training is the strongest principled
strategy against various adversarial attacks among all sorts of defense methods.
Recent works mainly focus on developing new loss functions or regularizers, at-
tempting to find the unique optimal point in the weight space. But none of them taps
the potentials of classifiers obtained from standard adversarial training, especially
states on the searching trajectory of training. In this work, we are dedicated to
the weight states of models through the training process and devise a simple but
powerful Self-Ensemble Adversarial Training (SEAT) method for yielding a robust
classifier by averaging weights of history models. This considerably improves
the robustness of the target model against several well known adversarial attacks,
even merely utilizing the naive cross-entropy loss to supervise. We also discuss
the relationship between the ensemble of predictions from different adversarially
trained models and the prediction of weight-ensembled models, as well as provide
theoretical and empirical evidence that the proposed self-ensemble method pro-
vides a smoother loss landscape and better robustness than both individual models
and the ensemble of predictions from different classifiers. We further analyze a
subtle but fatal issue in the general settings for the self-ensemble model, which
causes the deterioration of the weight-ensembled method in the late phases*.
1	Introduction
Deep learning techniques have showed promise in disciplines such as computer vision (Krizhevsky
et al., 2012; He et al., 2016), natural language processing (Vaswani et al., 2017; Devlin et al., 2019),
speech recognition (Sak et al., 2015; Wang et al., 2017) and even in protein structural modeling (Si &
Yan, 2021; Jumper et al., 2021). However, even for those efforts surpassing human-level performance
in several fields, deep learning based methods are vulnerable to adversarial examples generated by
adding small perturbations to natural examples (Szegedy et al., 2014; Goodfellow et al., 2015).
Following the discovery of this adversarial vulnerability (Ma et al., 2020; Wang et al., 2020b; Niu
et al., 2021), numerous defense approaches for protecting DNNs from adversarial attacks have been
proposed (Papernot et al., 2016; Xu et al., 2018; Liao et al., 2018; Ross & Doshi-Velez, 2018; Bai
et al., 2019; Qin et al., 2020; Wang et al., 2020a; Bai et al., 2021; Huang et al., 2021). Because it
is a straightforward and effective data-driven strategy, among these methods, adversarial training
(Madry et al., 2018) has become the de-facto way for strengthening the robustness of neural networks.
The core of adversarial training is proactively inducing a robust model by forcing DNNs to learn
adversarial examples generated from the current model, which helps DNNs gradually make up for
their deficiencies. Therefore, it is a practical learning framework to alleviate the impact of adversarial
attacks.
As a saying goes, a person may go faster but a team can go farther. An ensemble of classifiers
fused by output aggregates the collective wisdom of all participated classifiers, which outperform the
,Correspondence to: YisenWang (yisen.wang@pku.edu.cn)
*Code is available at https://github.com/whj363636/Self-Ensemble-Adversarial-Training
1
Published as a conference paper at ICLR 2022
decision made by a single classifier. Many ensemble methods (Tramer et al., 2018; Pang et al., 2019)
have shown their abilities to boost adversarial robustness. Despite averaged predictions of different
models enables the aggregation against a large number of attacks, it is still a heavy computing
burden to adversarially train all the member models. What is worse, the architectures and parameters
of every member model, as well as their related contribution factors, should be safely saved and
need performing forward propagation for every member repeatedly, which is a nightmare when
dealing with massive components. Despite the fact that the ensemble technique adopts a variety of
architectures and parameters, these models may have similar decision-making boundaries. Therefore,
it is natural to wonder:
Could we develop the potential of the classifier itself to build a robust model rather
than rely on multiple seemingly disparate classifiers? Could we make use of states
on the searching trajectory of optimization when performing adversarial training?
Motivated by these questions, we discuss the relationship between an ensemble of different classifiers
fused by predictions and by parameters, and present a computation-friendly self-ensemble method
named SEAT. In summary, this paper has the following contributions:
•	By rethinking the standard adversarial training and its variants, instead of developing a
well-designed loss to find a single optimal in the weight space, we design an efficient
algorithm called Self-Ensemble Adversarial Training (SEAT) by uniting states of every
history model on the optimization trajectory through the process of adversarial training.
Unlike the traditional ensemble technique, which is extremely time-consuming and repeating
adversarial training procedures for individuals, SEAT simply requires training once.
•	We present a theoretical explanation of the differences between the two ensemble approaches
discussed above and visualize the change of loss landscape caused by SEAT. Furthermore,
while using data augmentation of external data can experimentally fix the deterioration
during the late stages (Gowal et al., 2020; Rebuffi et al., 2021), we further investigate a
subtle but fatal intrinsic issue in the general settings for the self-ensemble model, which
causes the deterioration of the weight-ensembled method.
•	We thoroughly compare adversarial robustness of SEAT with other models trained by the
state-of-the-art techniques against several attack methods on CIFAR-10 and CIFAR-100
datasets. Results have shown that the SEAT method itself can efficiently search in the weight
space and significantly improve adversarial robustness.
2	Review: S tandard Adversarial Training and its Successors
In this section, we first introduce basic concepts of adversarial training. Then we briefly review some
variants and enhancements of standard adversarial training.
2.1	Preliminaries
Consider a standard image task which involves classifying an input image into one of C classes, let
x ∈ [0, 1]n denote the input natural image and y ∈ {1, 2, . . . , C} be the ground truth label, sampled
from a distribution D. For adversarial attack, the goal of the adversary is to seek a malignant example
in the vicinity of x, by adding a human imperceptible perturbation ε ∈ Rn . To exactly describe the
meaning of “imperceptible”, this neighbor region Bε(x) anchored at x with apothem ε can be defined
as Bε(x) = {(x0, y) ∈ D | kx - x0k ≤ ε}. We also define a loss function: ` d=ef Θ × D → [0, ∞),
which is usually positive, bounded, and upper-semi continuous (Blanchet & Murthy, 2019; Villani,
2003; Bartlett & Mendelson, 2001) for all θ ∈ Θ. Instead of the original formulation of adversarial
training (Szegedy et al., 2014; Goodfellow et al., 2015), which mixes natural examples x and
adversarial examples x0 together to train models, the adversarial risk minimization problem can be
formulated as a two-player game that writes as below:
inf E(x,y)〜D	sup ' (θ; χ0,y).	(1)
θ∈θ________} X0∈Bε(x)
{z	、	一寸―	J
OUter minimization Inner maximization
In general, the adversary player computes more and more harmful perturbations step by step, where
the PGD method (Madry et al., 2018) is commonly applied:
xt+1 = ∏Bε(χ) (xt + K sign(Vχ'(x,y))) ,	(2)
2
Published as a conference paper at ICLR 2022
where κ is the step size and the generated samples at each iteration will be projected back to the
Bε(x). When finding x0 at their current optimal in the inner maximization, the model parameters will
be updated according to these fixed samples only in the outer minimization.
2.2	Related Work
Emerging adversarial training methods are primarily concerned with strengthening their robustness
through empirical defense. Here we roughly separate these improvements into two categories: (1)
sample-oriented (i.e. for x0 in the inner maximization) and (2) loss-oriented (i.e. for ` in the outer
minimization) improvement.
From the side of the former, Baluja & Fischer (2017) utilize a GAN framework to create adversarial
examples via the generative model. Wang et al. (2019) proposes a criterion to control the strength of
the generated adversarial examples while FAT (Zhang et al., 2020) control the length of searching
trajectories of PGD through the lens of geometry.
For the latter, a stream of works have been carried out with the goal of establishing a new super-
vised loss for better optimization. Usually, the loss ` in the both inner maximization and outer
minimization* * are cross-entropy defined as: 'ce = -yT log q when given the probability vector
q = [efθ(x0)1,…，efθ(XO)c]/Pk=I efθ(x0)k. The addition of a regularizer to the vanilla loss is one
branch of the modification. Representative works like ALP (Kannan et al., 2018), TRADES (Zhang
et al., 2019), and VAT (Miyato et al., 2019) introduce a regularization term to smooth the gap between
the probability output of natural examples and adversarial ones, which can be denoted as:
'reg = 'ce + η E R(P(θ, χ'i,yi),p(θ; XMyy),	(3)
(xi,yi)~B
where η is the regularization hyper-parameter and R(∙, ∙) is l2 distance for ALP and VAT*, and
Kullback-Leibler divergence for TRADES. B stands for the sampled minibatch. Another branch of
the loss modification is to reweight the contributions of each instance in the minibatch based on their
intrinsic characteristics. This can be formulated by:
'rew =	E w(θ; Xi, * yi)'ce ⑹ Xi, *, Ji) ,	(4)
(xi,yi)~B
where w(∙) is a real-valued function mapping the input instances to an importance score. MMA (Ding
et al., 2020) separately considers the correct and incorrect classification on natural cases and switches
between them by learning a hard importance weight:
'mma = E	wι(θ; Xi,yi)'ce (θ; Xi,yi) + w2(θ; Xi,yi)'ce (θ; χi,yi),	(5)
(xi,yi)~B
where wι and w? are the indicator function 1 (fθ (Xi) = y%) and 1 (fθ (Xi) = y%), respectively.
Inspired by the geometry concern of FAT, GAIRAT (Zhang et al., 2021) evaluates w(θ; X, y) =
(1 + tanh(c1 - 10c2 + 5))/2 to adaptively control the contribution of different instances by their
corresponding geometric distance of PGD, where c1 is a hyper-parameter and c2 is the ratio of the
minimum successful iteration numbers of PGD to the maximum iteration step. Embracing both two
schools of improvement, MART (Wang et al., 2020c) adds a new regularization term apart from KL
divergence and explicitly assigns weights within regularizers:
'MART = 'ce + E	w(θ; Xi,yi)RκL(p(θ; Xi ,Ji),p(θ; Xi,Ji)) + Rmag (p(。； ^i,ji)), (6)
(χi,ya)~B
where RKL is Kullback-Leibler divergence same as TRADES and wi = (1 - p(θ; Xi, yi)) is a softer
scheme when compared with MMA. The margin term Rmag = - log (1 - maxk6=yi pk (θ; X0i, yi))
aims at improving the decision boundary.
The above methods endeavor to solve the highly non-convex and non-concave optimization in Eqn 1.
However, they assume that the optimal weights of a classifier appear in its prime and abandon all the
states along the route of optimization, which are beneficial to approach the optimal.
*For simplification, here we ignore the implemented difference of loss in the inner maximization and the
outer minimization (e.g. TRADES and MMA). The reference of ` can be contextually inferred easily.
*For VAT, θ in the first term and the second term are different.
3
Published as a conference paper at ICLR 2022
Algorithm 1 Self-Ensemble Adversarial Training (SEAT)
Input: A DNN classifier fθ (∙) With initial learnable parameters θo and loss function '; data distribution
D; number of iterations N ; number of adversarial attack steps K; magnitude of perturbation ε; step size κ;
learning rate τ ; exponential decay rates for ensembling α; constant factor c.
Initialize θ 一 θo, θ 一 θ.
for t - 1, 2,…，N do
Sample a minibatch (x, y) from data distribution D
x0 - X + ε, ε 〜 Uniform(一ε, ε).
for k - 1, 2,…，K do
Xk 一 ∏χk∈Bε(χ) (Ksign (xk-1 + Rxk-I'(θ; (xk,y))))
end for
gθt — E(x,y) [Vθt'(θt; (Xk,y))] in every minibatch
Calculate τt according to the current iterations
θt — θt 一 τtgθt
α0 — min ( α, -ɪ )
∖ , t+c)
θ — a0θ + (1 一 α0)θt
end for
Return A self-ensemble adversarial training model fθ(∙)
3 Methodology
In this section, We first discuss the traditional ensemble methods based on predictions of several
individual classifiers. Then, We propose a Self-ensemble Adversarial Training (SEAT) strategy fusing
Weights of individual models during different periods, Which is both intrinsically and computationally
convenient. FolloWing that, We further provide theoretical and empirical analysis on Why the
prediction of such an algorithm can make a great difference from that of simple ensembling classifiers.
3.1 Prediction-oriented Ensemble
Ensemble methods are usually effective to enhance the performance (Caruana et al., 2004; Garipov
et al., 2018) and improve the robustness (Tramer et al., 2018; Pang et al., 2019). For clarity, we denote
F as a Pool of candidate models where F = {fθι,…，fθn }. To represent the scalar output of the
averaged prediction of candidate classifiers over C categories, we define the averaged prediction f
involving all candidates in the pool F as:
n
fF (χ,y) = Eeifei(X,y)
n
(7)
s.t.	βi = 1,
i=1
where β represents the normalized contribution score allotted to the candidates. It is logical to assume
that ∀βi > 0 since each candidate model must be well-trained, otherwise it will be expelled from the
list. Note that here we only discuss whether the participated model performs well on its own, not
whether it has a positive impact on the collective decision making, which is a belated action.
3.2 Self-ensemble Adversarial Training
In order to average weights of a model, it is natural to choose an analogous form as Eqn 7 to obtain
TT
the predicted weights: θ = t=1 βtθt, s.t. t=1 βt = 1. However, such a simple moving average
cannot keenly capture the latest change, lagging behind the latest states by half the sample width.
To address this problem, we calculate the optimal weights by characterizing the trajectory of the
weights state as an exponential moving average (EMA) for measuring trend directions over a period
of time. Intuitively, EMA not only utilizes recent proposals from the current SGD optimization but
also maintains some influence from previous weight states:
θT = αθT -1 + (1 - α)θT
T
t=1
(1 - α)1-δ(t-1)αT-tθt,
(8)
4
Published as a conference paper at ICLR 2022
where δ(∙) is the unit impulse function (i.e., δ(0) = 1, otherwise it is 0). AlgorithmI summarizes the
full algorithm.
In consideration of both using the moving average technique, we wonder whether the averaged
prediction has a link to the prediction of a weight-averaged model. In fact, they are indeed related to
each other to some extent:
Proposition 1. (Proof in Appendix B) Let fθ (∙) denote the predictions of a neural network
parametrized by weights θ. Assuming that ∀θ ∈ Θ, fθ (∙) is continuous and ∀(x,y) ∈ D, fθ(x,y) is
at least twice differentiable. Consider two points θt, θ ∈ Θ in the weight space and letξ = θt -θ, for
t ∈ {1, 2, ∙∙∙ ,T}, the difference between Jf(x,y) and fg(x,y) is ofthe second order ofsmallness
if and only if PtT=1 (βtξ>) = 0.
Based on Proposition 1, we could immediately obtain the below conclusions:
Remark 1. Note that it always constructs ensembles of the well-matched enhanced networks to obtain
stronger defense, so it evenly assigns βι = β2 = •一=βn = 1/n. However, things change in the self-
ensemble setting since models obtained at relatively late stages will be much robust than the beginning.
Based on this assumption, it has a non-decreasing sequence βι ≤ β2 ≤ ∙∙∙ ≤ βn,s.t. En=I βi = 1.
The inequality is tight only when the initial weight reaches its fixed point.
In this case, the averaged prediction will predispose to the models obtained at the end of optimization
and such a predisposition loses the superiority of ensemble due to the phenomenon of homogenization
in the late phases. To provide an empirical evidence of homogenization of models at latter periods,
we visualize the effect of homogenization along the training process in Figure 1a. We define the
homogenization of a model as: 4e =击∣ P mini∈[1,m] ∣fθe(χ,y) - fee-i (x, y)|, used for calculat-
ing the difference of the output of history models over a period of time m. Note that 4e becomes
smaller and smaller along with epochs passing, which proves the existence of homogenization.
(a) The effect of homogenization of models.	(b) Robust accuracy of parameter-averaged
ResNet18 against various attacks on CIFAR-10.
Figure 1:	The overall analyse of AT and SEAT.
Here we formally state the gap between such an averaged prediction and the prediction given by
SEAT in the following theorem:
TheOrem 1. (Proof in AppendixB) Assuming thatfor i,j ∈ {1, ∙∙∙ ,T}, θi = θj ifand only if i = j.
The difference between the averaged prediction of multiple networks and the prediction of SEAT is of
the second order ofsmallness ifand only if βi = (1 — α)1-δ(i-1)ατ-i for i ∈ {1, 2,…,T}.
Theorem 1 tells us that it is much harder for SEAT to approximate the averaged prediction of history
networks than the EMA method. So the prediction of SEAT keeps away from the averaged prediction
of models, which suffers from homogenization. To ensure whether the difference in Theorem 1 is
benign, we provide empirical evidence of the improvement of the self-ensemble technique on the loss
landscape.
It is difficult for the traditional 1D linear interpolation method to visualize non-convexities (Goodfel-
low & Vinyals, 2015) and biases caused by invariance symmetries (e.g. batch normalization) in the
DNN. Li et al. (2018); Wu et al. (2020) address these challenges, providing a scheme called filter
normalization to explore the sharpness/flatness of DNNs on the loss landscape. Let v1 and v2 are
two random direction vectors sampled from a random Gaussian distribution. So we plot the value of
5
Published as a conference paper at ICLR 2022
(a) The best individual
(b) SEAT
(c) Deteriorated SEAT
Figure 2:	The loss surfaces of ResNet18 on CIFAR-10 test data. The loss surface of the standard
adversarially trained models transitions up and down, which means the loss of such a model changes
dramatically as moving along some directions. In contrast, SEAT has a fairly smooth landscape.
loss function around θ when inputting data samples:
L(θ; v1, v2) = `(θ +mv1 +nv2;x,y) ,	(9)
where m = ɪkθkk, n = ^^, and ∣∣∙k denotes the Frobenius norm. Specifically, We apply the method
to plot the surface of DNN within the 3D projected space. We visualize the loss values of ResNet18
on the testing dataset of CIFAR-10 in Figure 2. Each grid point in the x-axis represents a sampled
gradient direction tuple (v1, v2). It is clear that SEAT has a smooth and slowly varying loss function
while the standard adversarially trained model visually has a sharper minimum with higher test error.
Safeguard for the Initial Stage: During the initial training period, the history models are less
instrumental since they are not adequately learned. Therefore, it is not wise to directly make use of
the ensembled weights, which may accumulate some naive weight states and do harm to approach the
oracle. Therefore, we apply the protection mechanism for novice models at the beginning, denoted
as α0 = min (α, .), where C is a constant factor controlling the length of warm-up periods for
history models.
3.3 Deterioration: The Devil in the Learning Rate
Having provided both theoretical and experimental support as well as a clear procedure of SEAT
algorithm, we may take it for granted that this self-ensemble method could be integrated into the
routine working flow as an off-shelf technique. But it is not the case.
As shown in Figure 2b and 2c, while the two self-ensemble models have almost nearly identical
settings, their loss landscapes are completely different (quantitative results can be referred to Appendix
A.4). Despite the fact that the landscape of deteriorated SEAT is a little mild than the standard
adversarially trained model, it is still extremely steep when compared with SEAT. This phenomenon
about deterioration is also discovered by Rebuffi et al. (2021). They combine model weight averaging
with data augmentation and external data to handle the deterioration. The intention behind such a
solution is robust overfitting (Rice et al., 2020) of individuals. However, it is unclear why an ensemble
model assembled from overfitting individuals performs poorly, while inferior models can still be
aggregated to produce a superior one in the middle of training stages. In the rest of this section, we
provide another solution from a quite new perspective: the strategy of learning rate.
Proposition 2. (Proof in Appendix B) Assuming that every candidate classifier is updated by SGD-
like strategy, meaning θt+ι = θt 一 Tt V®, fet (x0, y) with τι ≥ τ? ≥ ∙∙∙ ≥ TT > 0, the performance
of self-ensemble model depends on learning rate schedules.
As illustrated in Figure 1a, we note that the phenomenon of homogenization exacerbates after each
tipping point (e.g. Epoch 76, 91 and 111) when using the staircase strategy of learning rate. Combined
with the trend of robust accuracy in Figure 1b, we find that the self-ensemble model cannot gain any
benefits when the learning rate decays below a certain value. More results on different learning rate
schedules can be found in Figure 3a.
4	Experimental Results
In this section, we first conduct a set of experiments to verify the advantages of the proposed method.
Then, we investigate how each component of SEAT functions and how it affects robust accuracy.
6
Published as a conference paper at ICLR 2022
4.1	Experimental setup
We mainly use ResNet18 and WRN-32-10 (Zagoruyko & Komodakis, 2016) for the experiments
on CIFAR-10/CIFAR-100 and all images are normalized into [0, 1]. For simplicity, we only report
the results based on L∞ norm for the non-targeted attack. We train ResNet18 using SGD with 0.9
momentum for 120 epochs and the weight decay factor is set to 3.5e-3 for ResNet18 and 7e-4 for
WRN-32-10. For SEAT, we use the piecewise linear learning rate schedule instead of the staircase one
based on Proposition 2. The initial learning rate for ResNet18 is set to 0.01 and 0.1 for WRN-32-10
till Epoch 40 and then linearly reduced to 0.001, 0.0001 and 0.01, 0.001 at Epoch 60 and 120,
respectively. The magnitude of maximum perturbation at each pixel is ε = 8/255 with step size
κ = 2/255 and the PGD steps number in the inner maximization is 10. To evaluate adversarial
defense methods, we apply several adversarial attacks including PGD (Madry et al., 2018), MIM
(Dong et al., 2018), CW (Carlini & Wagner, 2017) and AutoAttack (AA) (Croce & Hein, 2020). We
mainly compare the following defense methods in our experiments:
•	TRADES, MART, FAT and GAIRAT: For TRADES and MART, we follow the official
implementation of MART§ to train both two models for better robustness. The hyper-
parameter η of both TRADES and MART is set to 6.0. The learning rate is divided by 10 at
Epoch 75, 90, 100 for ResNet18 and at 75, 90, 110 for WRN-32-10, respectively. For FAT
and GAIRAT, We completely comply with the official implementation" to report results.
•	PoE: We also perform the experiments on the Prediction-oriented Ensemble method (PoE).
We select the above four advanced models as the candidate models. When averaging the
output of these models, we assign different weights to individuals according to their robust
accuracy. Specifically we respectively fix β1,2,3,4 = 0.1, 0.2, 0.3, 0.4 for FAT, GAIRAT,
TRADES, and MART, named PoE. Considering the poor robust performance of FAT and
GAIRAT, we also provide the PoE result by averaging the outputs of TRADES trained with
λ = 1, 2, 4, 6, 8 and assign the same β to them, named PoE (TRADES).
•	CutMix (with WA): Based on the experimental results of Rebuffi et al. (2021), Cutmix with
a fixed window size achieves the best robust accuracy. We follow this setting and set the
window size to 20. Since we do not have matched computational resources with Deepmind,
we only train models for 120 epochs with a batch size of 128 rather than 400 epochs with a
batch size of 512. Besides, we neither use Swish/SiLU activation functions (Hendrycks &
Gimpel, 2016) nor introduce external data and samples crafted by generative models (Brock
et al., 2019; Ho et al., 2020; Child, 2021) for fair comparison.
4.2	Robustness Evaluation of SEAT
In this section, we fully verify the effectiveness of the SEAT method. We report both average accuracy
rates and standard deviations. All results in Tables 1 and 2 are computed with 5 individual trials.
Results on ResNet18 are summarized in Table 1. Here, we mainly report the results on the CIFAR-10
dataset due to the space limitation. For the results on CIFAR-100, please refer to Appendix A.2. From
the table, we can see that the superiority of SEAT is apparent especially considering we only use
candidate models supervised by the traditional cross-entropy loss without external data. Compared
with two advanced loss-oriented methods (TRADES and MART), the results of SEAT are at least
〜2% better than TRADES and MART against CW and AA. We further emphasize that although
the boost of SEAT regarding PGD attacks is slight when compared with GAIRAT, SEAT achieves
startling robust accuracy results when facing CW and AA attacks. Note that AA is an ensemble of
various advanced attacks (the performance under each component of AA can be found in Appendix
A.1) and thus the robust accuracy of AA reliably reflects the adversarial robustness, which indicates
that the robustness improvement of SEAT is not biased. It’s worth noting that PoE is unexpectedly
much weaker in defending AA attack even than its members. We guess the poor performance of FAT
and GAIRAT encumber the collective though we appropriately lower the contribution score of them.
If we switch to an ensemble of the outputs of TRADES with different λs, the performance will be
slightly improved. That demonstrates that the robustness of every candidate model will affect the
performance of the ensemble. The negligible computational burden estimated in Appendix A.5 also
demonstrates the superiority of our on-the-fly parameter-averaged method of SEAT.
We also evaluate the SEAT method on WRN-32-10. Results are shown in Table 2. We notice that
the superiority of SEAT appears to be enhanced when using WRN-32-10. The gap between SEAT
§https://github.com/YisenWang/MART
"https://github.com/zjfheart/Geometry-aware-Instance-reweighted-Adversarial-Training
7
Published as a conference paper at ICLR 2022
Table 1: Comparison of our algorithm with different defense methods using ResNet18 on CIFAR-10.
The maximum perturbation is ε = 8/255. Average accuracy rates (in %) and standard deviations
have shown that the proposed SEAT method greatly improves the robustness of the model.
Method	NAT	PGD20	PGD100	MIM	CW	AA
AT	84.32±0.23=	48.29±0.11	48.12±0F	47.95±0.04	49.57±0.15	44.37±0.37
TRADES	83.91±0.33	54.25±0.11	52.21±0.09	55.65±0.1	52.22±0.05	48.2±0.2
FAT	87.72±0.14	46.69±0.31	46.81±0.3	47.03±0.17	49.66±0.38	43.14±0.43
MART	83.12±0.23	55.43±0.16	53.46±0.24	57.06±0.2	51.45±0.29	48.13±0.31
GAIRAT	83.4±0.21	54.76±0.42	54.81±0.63	53.57±0.31	38.71±0.26	31.25±0.44
PoE	85.41±0.29	55.2±0.37	55.07±0.24	54.33±0.32	49.25±0.16	46.17±0.35
PoE (TRADES)	83.57±0.31	53.88±0.45	53.82±0.27	55.01±0.18	52.72±0.65	49.2±0.24
CutMix (with WA)	81.26±0.44	52.77±0.33	52.55±0.25	53.01±0.25	50.01±0.55	47.38±0.36
SEAT	83.7±O13=	56.02±0.11	55.97±0.0T=	57.13±0.12	54.38±0.1	51.3±0.26
SEAT+CutMix	81.53±0.31	55.3±0.27	54.82±0.18	56.41±0.17	53.83±0.31	49.1±0.44
Table 2: Comparison of our algorithm with different defense methods using WRN-32-10 on CIFAR-
10. The maximum perturbation is ε = 8/255. Average accuracy rates (in %) and standard deviations
have shown that SEAT also shows a great improvement on robustness.
Method	NAT	PGD20	PGD100	MIM	CW	AA
AT	87.32±O21=	49.01±0.33	48.83±0.2T=	48.25±0.17	52.8±0.25	48.17±0.48
TRADES	85.11 ±0.77	54.58±0.49	54.82±0.38	55.67±0.31	54.91±0.21	52.19±0.44
FAT	89.65±0.04	48.74±0.23	48.69±0.18	48.24±0.16	52.11±0.71	46.7±0.4
MART	84.26±0.28	54.11±0.58	54.13±0.3	55.2±0.22	53.41±0.17	50.2±0.36
GAIRAT	85.92±0.69	58.51±0.42	58.48±0.34	58.37±0.27	44.31±0.22	39.64±1.01
PoE	87.1±0.25	55.75±0.2	55.47±0.19	56.04±0.31	53.66±0.18	49.44±0.35
PoE (TRADES)	86.03±0.37	54.26±0.47	54.73±0.21	55.01±0.22	55.52±0.18	53.2±0.4
CutMix (with WA)	82.79±0.44	58.43±1.21	58.2±0.83	58.95±0.57	58.32±0.43	54.1±0.82
SEAT	86.44±0.12=	59.84±0.2	59.8±0.16=	60.87±0.1	58.95±0.34	55.67±0.22
SEAT+CutMix	84.81±0.18	60.2±0.16	60.31±0.12	60.53±0.21	59.46±0.24	56.03±0.36
and advanced defense methods enlarges to at least 〜4%. In the setting with data augmentation,
when compared with the results of ResNet18, SEAT combined with CutMix gains much higher
performance on robust accuracy against several attacks as the model becomes larger. The observation
given by Rebuffi et al. (2021) shows that additional generated data would overload the model with
low capacity. When it comes to ours, we can further infer from a marginal and even a negative effect
shown in Table 1 that the model with low capacity cannot benefit from both external and internal
generated data. This phenomenon springs from the fact that CutMix techniques tend to produce
augmented views that are far away from the original image they augment, which means that they are
a little hard for small neural networks to learn.
4.3	Ablation Study
In this section, we perform several ablation experiments to investigate how different aspects of SEAT
influence its effectiveness. If not specified otherwise, the experiments are conducted on CIFAR-10
using ResNet18.
(a) Different learning rate strategies (b) Epoch v.s. Iteration
100
90
80
70
60
50
40
30
20
10
0
-■-NAT
-■-PGD20
-⅛-PGD100
^MIM
^CW
-*≡AA
10	100	1000	10000
0	5
(c) Length of initial safeguard
Figure 3: The overall ablation study of SEAT. The dashed line represents the natural accuracy while
the solid line represents the robust accuracy.
Different Learning Rate Strategies. The staircase learning rate technique is always applied to
update weights in commonly used neural networks, which τ remains unchanged within the specific
8
Published as a conference paper at ICLR 2022
epochs and suddenly decreases to a certain value at some points. We also have the cosine, cyclic, and
warming up learning rate schedules to compare with, in addition to the staircase and piecewise linear
one deployed to SEAT. Figure 3a depicts the results of this experiment using various learning rate
methodologies. The natural accuracy under various learning rate schedules is reported by the top
dashed lines, which all grow gradually over time. After reaching their maxima, the relative bottom
solid lines fall more or less. Quantitative Results of above five strategies from best checkpoints are
shown in Appendix A.3. However, it can be observed that the red line (piecewise linear) and the blue
line (cosine) decrease more slowly than the orange line (staircase) and they achieve higher robust
accuracy at much later epochs than the staircase. Apparently, the piecewise linear strategy makes τ
approach the threshold τt0 at a slow pace while the cosine one increases the valid candidate models
that can make contributions. Likewise, the cyclic strategy gets better robust accuracy and further
improves the performance in the late stage. But the effect of warming up learning rate is marginal
since the homogenization of the candidate models we mentioned in Sec. 3.2 cannot be fixed by the
warmup strategy. These facts bear out Proposition 2 from the side.
Epoch-based v.s. Iteration-based Ensembling. Next, we investigate whether the frequency of
ensembling has a great effect on the robustness of the collective. Specifically, we select two scenarios
to compare with. One is an epoch-based ensemble that only gathers the updated models after
traversing the whole training dataset. The other is an iteration-based ensemble collecting models
whenever SGD functions. Comparison between the epoch-based and the iteration-based ensembling
is shown in Figure 3b. It can be seen that the curves of both two schemes are nearly the same in their
prime except that the iteration-based method has a slightly greater natural accuracy and degrades
more slowly.
Influence of the Initial Safeguard. We also study the effect of the length of the initial safeguard. We
plot all results against different kinds of attacks with c ranging from 0 to 10000 (i.e. Epoch 32). The
curves in Figure 3c show that the model ensembled with large c, which ignores more history models,
results in weaker memorization and poor robustness. Actually, from the perspective of Proposition 2,
t0 is fixed if the scheme of learning rate updating and the total training epochs are unchanged. So
using a large c is equivalent to squeeze the number of benign candidates before t0 . Similarly, a small
c can manually filter the early models without being well trained. However, such a drop is minor
because EMA has a built-in mechanism for adjusting.
Mixed Strategies with SEAT for Improved Robustness. Finally, we report the results under differ-
ent mixed strategies on CIFAR-10 dataset with ResNet18 in Table 3. Overall, SEAT+TRADES and
SEAT+TRADES+CutMix do not perform well. We believe the reason is that TRADES cannot well
adapt to learning strategies other than the staircase one for its history snapshots of TRADES trained
under other strategies are much inferior than the ones trained under the staircase learning rate strategy.
However, MART is in harmony with other strategies. The combination of SEAT+MART+CutMix
achieves the best or the second-best robustness against almost all types of attacks.
Table 3: Average robust accuracy (%) and standard deviation under different mixed strategies on
CIFAR-10 dataset with ResNet18.
Method	NAT	PGD20	PGD100	MIM	CW	APGDCE	APGDDLR	APGDT	FABT	Square	AA
SEAT	-8377-	56.02	55.97	57.13	54.38	53.87	53.35	50.88	51.41	57.77	51.3
	±0.13	±0.11	±0.07	±0.12	±0.1	±0.17	±0.24	±0.27	±0.37	±0.22	±0.26
SEAT	81.21	57.05	57.0	57.92	52.75	50.75	50.36	48.56	49.45	54.45	49.91
+TRADES	±0.44	±0.28	±0.15	±0.12	±0.09	±0.25	±0.29	±0.37	±0.65	±0.58	±0.17
SEAT	78.94	56.92	56.88	57.24	52.09	54.06	53.77	51.03	51.11	57.76	51.7
+MART	±0.14	±0.46	±0.29	±0.48	±0.66	±0.56	±0.25	±0.44	±0.41	±0.08	±0.35
SEAT	78.22	57.14	57.09	57.11	53.17	52.67	51.86	50.91	50.23	55.12	50.01
+TRADES+CutMix	±0.33	±0.21	±0.45	±0.39	±0.41	±0.82	±0.5	±0.17	±0.29	±0.46	±0.44
SEAT	75.87	57.3	57.29	57.47	53.13	54.33	53.98	51.2	51.42	58.01	52.1
+MART+CutMix	±0.24	±0.16	±0.43	±0.18	±0.2	±0.33	±0.61	±0.73	±0.17	±0.08	±0.22
5 Conclusion
In this paper, we propose a simple but powerful method called Self-Ensemble Adversarial Training
(SEAT), which unites states of every history model on the optimization trajectory through the process
of adversarial training. Compared with the standard ensemble method, SEAT only needs training
once and has a better reusability. Besides, we give a theoretical explanation of the difference
between the above two ensemble methods and visualize the change of loss landscape caused by
SEAT. Furthermore, we analyze a subtle but fatal intrinsic issue in the learning rate strategy for the
self-ensemble model, which causes the deterioration of the weight-ensembled method. Extensive
experiments validate the effectiveness of the proposed SEAT method.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Yisen Wang is partially supported by the National Natural Science Foundation of China under Grant
62006153, Project 2020BD006 supported by PKU-Baidu Fund, and Open Research Projects of
Zhejiang Lab (No. 2022RC0AB05).
References
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efficient black-box adversarial attack via random search. In ECCV, 2020.
Yang Bai, Yan Feng, Yisen Wang, Tao Dai, Shu-Tao Xia, and Yong Jiang. Hilbert-based generative
defense for adversarial examples. In ICCV, 2019.
Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving
adversarial robustness via channel-wise activation suppressing. In ICLR, 2021.
Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adver-
sarial examples. arXiv preprint arXiv:1703.09387, 2017.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. In COLT, 2001.
Jose H. Blanchet and Karthyek R. A. Murthy. Quantifying distributional model risk via optimal
transport. Math. Oper. Res., 2019.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In ICLR, 2019.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
S&P, 2017.
Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from
libraries of models. In ICML, 2004.
Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images.
In ICLR, 2021.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. MMA training: Direct
input space margin maximization through adversarial training. In ICLR, 2020.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In CVPR, 2018.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, and Andrew Gordon Wilson.
Loss surfaces, mode connectivity, and fast ensembling of dnns. In NeurIPS, 2018.
Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization
problems. In ICLR, 2015.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
10
Published as a conference paper at ICLR 2022
Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian
error linear units. arXiv preprint arXiv:1606.08415, 2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,
2020.
Hanxun Huang, Yisen Wang, Sarah Monazam Erfani, Quanquan Gu, James Bailey, and Xingjun
Ma. Exploring architectural ingredients of adversarially robust deep neural networks. In NeurIPS,
2021.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn TUnyasUvUnakooL Russ Bates, AUgUstm Zidek, Anna Potapenko, Alex Bridgland,
Clemens Meyer, Simon A A Kohl, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes,
Stanislav Nikolov, RishUb Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen
Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian
Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, Koray KavUkcUoglU, PUshmeet Kohli,
and Demis Hassabis. Highly accUrate protein strUctUre prediction with AlphaFold. Nature, 2021.
Harini Kannan, Alexey KUrakin, and Ian J. Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
Alex Krizhevsky, Ilya SUtskever, and Geoffrey E. Hinton. Imagenet classification with deep convolU-
tional neUral networks. In NeurIPS, 2012.
Hao Li, Zheng XU, Gavin Taylor, Christoph StUder, and Tom Goldstein. VisUalizing the loss landscape
of neUral nets. In NeurIPS, 2018.
FangzhoU Liao, Ming Liang, Yinpeng Dong, TianyU Pang, Xiaolin HU, and JUn ZhU. Defense against
adversarial attacks Using high-level representation gUided denoiser. In CVPR, 2018.
XingjUn Ma, YUhao NiU, Lin GU, Yisen Wang, Yitian Zhao, James Bailey, and Feng LU. Understanding
adversarial attacks on deep learning based medical image analysis systems. Pattern Recognition,
2020.
Aleksander Madry, Aleksandar Makelov, LUdwig Schmidt, Dimitris Tsipras, and Adrian VladU.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
TakerU Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. VirtUal adversarial training: A
regUlarization method for sUpervised and semi-sUpervised learning. IEEE Trans. Pattern Anal.
Mach. Intell., 2019.
Dantong NiU, Ruohao Guo, and YiSen Wang. Moire attack (ma): A new potential risk of screen
photos. In NeurIPS, 2021.
TianyU Pang, KUn XU, Chao DU, Ning Chen, and JUn ZhU. Improving adversarial robUstness via
promoting ensemble diversity. In ICML, 2019.
Nicolas Papernot, Patrick D. McDaniel, Xi WU, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial pertUrbations against deep neUral networks. In S&P, 2016.
Yao Qin, Nicholas Frosst, Sara SaboUr, Colin Raffel, Garrison W. Cottrell, and Geoffrey E. Hinton.
Detecting and diagnosing adversarial images with class-conditional capsUle reconstrUctions. In
ICLR, 2020.
Sylvestre-Alvise RebUffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann. Fixing data aUgmentation to improve adversarial robUstness. arXiv preprint
arXiv:2103.01946, 2021.
Leslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robUst deep learning. In
ICML, 2020.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robUstness and interpretability
of deep neUral networks by regUlarizing their inpUt gradients. In AAAI, 2018.
Hasim Sak, Andrew W. Senior, Kanishka Rao, and FrancoiSe BeaUfays. Fast and accUrate recUrrent
neUral network acoUstic models for speech recognition. In INTERSPEECH, 2015.
11
Published as a conference paper at ICLR 2022
Yunda Si and Chengfei Yan. Improved protein contact prediction using dimensional hybrid residual
networks and singularity enhanced loss function. bioRxiv, 2021.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D.
McDaniel. Ensemble adversarial training: Attacks and defenses. In ICLR, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
Cedric Villani. Topics in optimal transportation.(books). OR/MS Today, 2003.
Hongjun Wang, Guanbin Li, Xiaobai Liu, and Liang Lin. A hamiltonian monte carlo method for
probabilistic adversarial attack and learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2020a.
Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, and Liang Lin. Transferable, controllable,
and inconspicuous adversarial attacks on person re-identification with deep mis-ranking. In CVPR,
2020b.
Yisen Wang, Xuejiao Deng, Songbai Pu, and Zhiheng Huang. Residual convolutional ctc networks
for automatic speech recognition. arXiv preprint arXiv:1702.07793, 2017.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. In ICLR, 2020c.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. In NeurIPS, 2020.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In NDSS, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan S.
Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020.
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan S. Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In ICLR, 2021.
12
Published as a conference paper at ICLR 2022
A Further Experiments
Here we adopt ResNet18 and / or WRN-32-10 as the backbone model with the same experimental
setup as in Sec. 4.1, where we reported the natural accuracy (NAT), PGD-20 and PGD-100 attack
(PGD), MIM (PGD with a momentum term), CW attack and each component of AutoAttack. All the
experiments are conducted for 5 individual trials and we also report their standard deviations. All the
methods were realized by Pytorch 1.5, where we used a single NVIDIA GeForce RTX 3090 GPU.
A. 1 Robustness against Components of AutoAttack
To broadly demonstrate the robustness of our proposal, we conducted experiments against each
component of AutoAttack. We perform each component of AA on CIFAR-10 dataset with both
ResNet18 and WRN-32-10, including three parameter-free versions of PGD with the CE, DLR,
targeted-CE loss with 9 target classes loss (APGDCE, APGDDLR, APGDT), the targeted version
of FAB (FABT) and an existing complementary Square (Andriushchenko et al., 2020). Results are
shown in the following Table 4. And it is obvious that our SEAT outperforms other methods against
all components of AA.
Table 4: Average robust accuracy (%) and standard deviation against each component of AA on
CIFAR-10 dataset with ResNet18 and WRN-32-10.
ReSNet18	WRN-32-10
	APGDCE	APGDDLR	APGDT	FABT	Square	APGDCE	APGDDLR	APGDT	FABT	Square
AT	47.47	48.57	45.14	46.17	-54.21—	49.17	50.09	47.34	48.00	56.5
	士0.35	±0.18	±0.31	±0.11	±0.15	士0.26	±0.36	±0.33	±0.43	±0.18
TRADES	53.47	50.89	47.93	48.53	55.75	55.38	55.55	52.2	53.11	59.47
	±0.21	±0.26	±0.36	±0.43	±0.21	士0.43	±0.42	±0.13	±0.72	±0.17
MART	52.98	50.36	48.17	49.39	55.73	55.2	55.41	51.99	52.88	59.01
	±0.13	±0.3	±0.72	±0.28	士0.51	±0.32	±0.4	±0.3	±0.63	±0.38
SEAT	53.87	53.35	50.88	51.41	-57.77-	57.57	57.74	55.06	55.53	62.26
	±0.17	±0.24	±0.27	±0.37	±0.22	±0.18	±0.29	±0.27	±0.36	±0.23
A.2 Performance on CIFAR- 1 00
To further demonStrate the robuStneSS of our propoSal againSt adverSarial attackS, we benchmark the
State-of-the-art robuStneSS with ReSNet18 on CIFAR-100. We widely inveStigate the performance
of SEAT againSt the PGD methodS (PGD20 and PGD100), MIM, CW, AA and itS all componentS.
ReSultS Shown in Table 5 demonStrate the effectiveneSS of SEAT for building a robuSt claSSifier.
Table 5: CompariSon of our algorithm with different defenSe methodS uSing ReSNet18 on CIFAR10.
The maximum perturbation iS ε = 8/255. Average accuracy rateS (in %) and Standard deviationS
have Shown that the propoSed SEAT method greatly improveS the robuStneSS of the model.
Method	NAT	PGD20	PGD100	MIM	CW	APGDCE	APGDDLR	APGDT	FABT	Square	AA
AT	-6071-	28.22	28.27	28.31	24.87	26.63	24.13	21.98	23.87	27.93	23.91
	±0.35	±0.3	±0.12	±0.41	±0.51	±0.29	±0.22	±0.3	±0.21	±0.12	±0.41
TRADES	59.93	29.9	29.88	29.55	26.14	27.93	25.43	23.72	25.16	30.03	24.72
	±0.46	±0.41	±0.11	±0.25	±0.21	±0.44	±0.29	±0.45	±0.15	±0.32	±0.37
MART	57.24	30.62	30.62	30.83	26.3	29.91	26.32	24.28	24.86	28.28	24.27
	±0.64	±0.37	±0.17	±0.28	±0.29	±0.07	±0.24	±0.49	±0.66	±0.39	±0.21
SEAT	56.28-	32.15	32.12	32.62	29.68	30797	29.62	26.88	27.71	32.35	27.87
	±0.33	±0.17	±0.26	±0.15	±0.26	±0.18	±0.22	±0.23	±0.24	±0.34	±0.24
A.3 Different Learning Rate Strategies
Apart from Showing the curve of different learning rate Schedule in Figure 3 (a) in Sec. 4.3, we alSo
report final reSultS in Table 6. The effect of warming up learning rate iS marginal. When compared
with the StaircaSe one, the warmup Strategy cannot generate diverSe modelS in the later StageS So the
homogenization of the candidate modelS we mentioned in Sec. 3.2 cannot be fixed by the warmup
Strategy. On the contrary, thoSe methodS like coSine / linear / cyclic that provide relatively diverSe
modelS in the later StageS can mitigate the iSSue, accounting for more robuSt enSemble modelS.
A.4 Deterioration of vanilla EMA
AS Shown in Sec. 3.3, the deteriorated SEAT underperformS SEAT a lot from the perSpective of
optimization. We alSo report quantitative reSultS on both ReSNet18 and WRN-32-10 Shown in the
13
Published as a conference paper at ICLR 2022
Table 6: Average robust accuracy (%) under different learning strategies on CIFAR-10 dataset with
ResNet18.
Method	NAT	PGD20	PGD100	MIM	CW	APGDCE	APGDDLR	APGDT	FABT	Square	AA
SEAT (Staircase)	80.91	54.58	54.56	54.47	49.71	52:39	48.01	45.83	45.11	53.64	45.85
SEAT (Cosine)	83.0	55.09	55.16	56.39	53.43	52.34	52.1	49.51	50.15	56.48	50.48
SEAT (Linear)	83.7	56.02	55.97	57.13	54.38	53.87	53.35	50.88	51.41	57.77	51.3
SEAT (Warmup)	82.74	55.31	55.35	56.39	53.26	53.55	48.94	45.89	46.6	54.94	45.82
SEAT (Cyclic)	83.14	56.03	55.79	56.99	54.01	53.72	53.1	50.66	51.02	57.75	51.44
Tables 7 and 8. The deteriorated one does not bring too much boost when compared to the vanilla
adversarial training (except for PGD methods). A plausible explanation for the exception of PGD
is that the SEAT technique produces an ensemble of individuals that are adversarially trained by
PGD with the cross-entropy loss, which means that they are intrinsically good at defending the PGD
attack with the cross-entropy loss and its variants even though they suffer from the deterioration.
Considering results have greatly improved after using the piecewise linear learning rate strategy, it
is fair to say that adjusting learning rate is effective. As we claimed in Proposition 2 and its proof,
the staircase will inevitably make the self-ensemble model worsen since PT=ι(βtξ›) Will gradually
approach to zero, meaning the difference between fy(x, y) and fθ(χ, y) achieves the second order
of smallness.
Table 7: Average robust accuracy (%) and standard deviation on CIFAR-10 dataset with ResNet18.
Method	NAT	PGD20	PGD100	MIM	CW	APGDCE	APGDDLR	APGDT	FABT	Square	AA
AT	84.32-	48.29	48.12	47.95	49.57	47:47	48.57	45.14	46.17	54.21	44.37
	士0.23	士0.11	士0.13	士0.04	士0.15	士0.35	士0.18	士0.31	士0.11	士0.25	士0.37
SEAT	80.91-	54.58	54.56	54.47	49.71	52:39	48.01	45.83	45.11	53.64	45.85
(deteriorated)	士0.38	士0.71	士0.29	士0.39	士0.41	士0.26	士0.18	士0.52	士0.23	士0.44	士0.19
SEAT	83.7	56.02	55.97	57.13	54.38	53.87	53.35	50.88	51.41	57.77	51.3
	士0.13	士0.11	士0.07	士0.12	士0.1	士0.17	士0.24	士0.27	士0.37	士0.22	士0.26
Table 8: Average robust accuracy (%) and standard deviation on CIFAR-10 dataset with WRN-32-10.
Method	NAT	PGD20	PGD100	MIM	CW	APGDCE	APGDDLR	APGDT	FABT	Square	AA
AT	87.32-	49.01	48.83	48.25	52.8	54ΓT7	53.09	48.34	49.00	57.5	48.17
	±0.21	士0.33	士0.27	士0.17	士0.25	±0.26	士0.36	士0.33	士0.43	士0.18	士0.48
SEAT	85.28-	55.68	55.57	55.6	53.01	54:12	53.54	49.95	50.02	57.81	49.96
(deteriorated)	士0.42	士0.42	士0.19	士0.23	士0.41	士0.54	士0.28	士0.67	士0.75	士0.33	士0.31
SEAT	86.44	59.84	59.8	60.87	58.95	57.57	57.74	55.06	55.53	62.26	55.67
	±0.12	士0.2	士0.16	士0.1	士0.34	士0.18	士0.29	士0.27	士0.36	士0.23	士0.22
A.5 Computational Complexity for SEAT
To demonstrate the efficiency of the SEAT method, we use the number of Multiply-Accumulate
operations (MACs) in Giga (G) to compute the theoretical amount of multiply-add operations in
DNNs, roughly GMACs = 0.5 * GFLOPs. Besides, we also provide the actual running time. As
shown in Table 9, the SEAT method takes negligible MACs and training time when compared with
standard adversarial training.
Table 9: Evaluation of time complexity of SEAT. Here we use the number of Multiply-Accumulate
operations (MACs) in Giga (G) to measure the running time complexity. And we also compute the
actual training time with or without the SEAT method using ResNet18 and WRN-32-10 on a single
NVIDIA GeForce RTX 3090 GPU.
Method	MACs (G)	Training Time (mins)
ReSNet18 (AT)	0.56	272
ResNet18 (SEAT)	0.59	273
WRN-32-10 (AT)	6.67	1534
WRN-32-10 (SEAT)	6.81	1544
14
Published as a conference paper at ICLR 2022
B Proofs of theoretical results
B.1	Proof of Proposition 1
Proposition 1. (Restated) Let fθ (∙) denote the predictions of a neural network parametrized by
weights θ. Assuming that ∀θ ∈ Θ, fθ (∙) is continuous and ∀(x,y) ∈ D, fθ(x,y) is at least
twice differentiable. Consider two points θt , θ ∈ Θ in the weight space and let ξ = θt - θ, for
t ∈ {1, 2, ∙∙∙ , T}, the difference between Jf(x, y) and fθ(x, y) is ofthe second order ofsmallness
if and only if PtT=1 (βtξ>) = 0.
Proof. For the sake of the twice differentiability of fθ (x, y), based on the Taylor expansion, we can
fit a quadratic polynomial of fθ(χ, y) to approximate the value of fθt (x, y):
fθt (x, y) = fθ(x, y) + ξ>Vξfθ(x, y) + 1 ξ> Vξfθ(x, y)ξ + O (∆n),	(10)
where O (∆n ) represents the higher-order remainder term. Note that the subscript ξ here stands for
a neighborhood where the Taylor expansion approximates a function by polynomials of any point
(i.e. θ) in terms of its value and derivatives. So the difference between the averaged prediction of
candidate classifiers and the prediction of the ensembled weight classifier can be formulated as:
T
fF(x,y) - fθ(x,y) = Eetfθt (χ,y) - fθ(X,y)
t=1
^T：	T	T
=Xefxy) + X βtξ>vξ fθ(X, y) + X BtO O2)'-f(x4d. (II)
t=1	：： t=1	t=1
T
=X(Btξ>)vξfθ(X,y) + O。2).
t=1
Therefore, We can claim that the difference between fθt (x, y) and fθ(χ, y) is ”almost” at least of the
first order of smallness except for some special cases. And we will immediately declare under which
condition this difference can achieve the second order of smallness in the following proof of Theorem
1.	□
B.2	Proof of Theorem 1
Theorem 1. (Restated) Assuming that for i,j ∈ {1, ∙∙∙ ,T}, θi = θj if and only if i = j. The
difference between the averaged prediction of multiple networks and the prediction of SEAT is of the
second order ofsmallness if and only if βi = (1 一 α)1-δ(iτ)ατ-i for i ∈ {1, 2,…，T}.
Proof. According to Eqn 11, we know that the second order of smallness will achieve when
PiT=1(Biξ>) = 0. Thus, we continue deducing from Eqn 11 as:
T
X(Biξ> ) = 0
i=1
T
X βi(θi- θ) = 0
i=1
T
X βiθi = θ
i=1
TT
XBiθi = X(1 一 α)1-δ(i-1)αT-iθi.
i=1	i=1
(12)
To get a further conclusion, we next use Mathematical Induction (MI) to prove only when Bi =
(1 一 α)1-δ(iτ)ατ-i for i ∈ {1, 2,…，T} will lead to PT=I βiθi = PT=ι(1 - α)1-δ(i-1)ατ-iθi.
15
Published as a conference paper at ICLR 2022
Base case: Let i = 1, it is clearly true that β1 = αT-1 if and only if β1θ1 = αT-1θ1, hence the
base case holds.
Inductive step: Assume the induction hypothesis that for a particular k, the single case T = k holds,
meaning the sequence of (β1,β2,…，βk) is equal to the sequence of ((1 - α)1-δ(0)αT-1, (1 -
α)1-δ⑴αT-2,…，(1 - α)1-δ(k-1)ɑτ-k) if Pk=I 民仇=Pk=1(1 - α)1-δ(i-1)αT-iθi.
For T = k + 1, it follows that:
k+1	k+1
Xβiθi=X(1-α)1-δ(i-1)αT-iθi
i=1	i=1
Xeθi + βk+ιθk+ι = χ2(1'-ηαθ4-^-(i--⅛τ-iθi + (1 - α)1-δ((k+1)-1)ɑT-(k+1)θk+ι (13)
i=1 ^y	i=1	^^^^-`^
βk+1θk+1 = (1 - α)1-δ((k+1)-1)αT-(k+1)θk+1
βk+1 = (1 - α)1-δ((k+1)-1)αT-(k+1).
The sequence of normalized scores at the (k + 1)-th ensembling at left hand is (β1,β2,…，βk, βk+ι)
after adding the new term βk+ι. Likewise, the sequence of the right hand is (β1,β2, ∙∙∙ ,βk) is equal
to the sequence of ((1 — a)1-δ(0)ɑτ-1, (1 — α)1-δ(1)ɑτ-2,…，(1 — a)1-δ(k-1)ɑτ-k), Because
every fθt ∈ F is different from others and the sequence is ordered, We have (βι ,β2, ∙∙∙ ,βk, βk+ι)=
((1 - α)1-δ(0)ɑτ-1, (1 - α)1-δ(DaT-2,…，(1 - α)1-δ((k+1)-1)ατ-(k+1)).
Conclusion: Since both the base case and the inductive step have been proved as true, by mathematical
induction the statement βi = (1 - a)1-δ(i-1)aτ-i for i ∈ {1,2,…，T} holds for every positive
integer T. Following Eqn 11, the difference between SEAT and the averaged prediction of history
networks is controlled by the first order term which reaches 0 only at βi = (1 - α)1-δ(i-1)αT -i
for i ∈ {1,2,…，T}. Thus, SEAT is hardly be approximate to the averaged prediction of history
networks indeed.	□
16
Published as a conference paper at ICLR 2022
B.3 Proof OF Proposition 2
Proposition 2. (Restated) Assuming that every candidate classifier is updated by SGD-like strategy,
meaning θt+ι =& 一 τNθt fθt (χ∖y) With τι ≥ τ? ≥ ∙∙∙ ≥ TT > 0, the performance of self-
ensemble model depends on learning rate schedules.
Proof. First We discuss a special case - the change at the t-th iteration. Reconsidering the first order
term in Eqn 11, we have:
T
X(βtξ>)Vξfθ(x,y)
t=1
T
=X[βt(θt - θ)]Vξfθ(x,y)
t=1
T
=X[βt((1 -(1 一 ①I(T)OT-t)θt - θF∖t)]Vξ%(x,y)
t=1
T
=X[βt((1 -(1 一 α)1-δ(t-1)ατ-t)θt - (1 - (1 - α)1-"tT)ατ-t)θt-i + θt-i
t=1
-(I- α)1 δ(t I)OT tθt-1 - θF∖t)] vξfθ(X, y)
T
=X[βt((1 - (1 - α)1-δ(t-1)0τ-t)θt - (1 - (1 - a)1-δ(t-1)OT-t)θt-i + θt-i
t=1
-(I- α)1-δ(tT)QT-tθt-1 - (1 - α)1-δ(tT)QT-t+1θt-i - θF∖t,t-1)]vξfθ(X, y)
T
=X[βt((1 - (1 - Q)1-δ(tT)QT-t)θt - (1 - (1 - α)1-δ(tT)QT-t)θt-1
t=1
+ (I-(I- Q>-δ(t-I)QT-t - (1 - Q)1-δ"-I)QT-ta)θt-i - θF∖t,t-i)]vξ f (X, y),
(14)
so we can deduce by combining:
T
X[βt((1 - (1 - Q)1-δ(tT)QT-t)(θt - θt-1) + C)]Vξf (x, y),	(15)
t=1
where C = (1 - (1 - Q)1-δ(tT)QT-t - (1 - Q)1-δ(tT)QT-tQ)θt-ι - θj∙∖t,t-ι. By using SGD
to update θt, we have:
T
X[βt((1 - (1 - Q)1-*tT)QT-t)(τtE(χ,y)(Vθt,(θt; (Xk,y)) + C))]Vξf (x, y).	(16)
t=1
Considering C is a constant for the t-th update, without changing samples in the t-th minibatch, we
can conclude that the output of SEAT depends on the learning rate τt .
17
Published as a conference paper at ICLR 2022
ECj	1	.1	1	1	.	∙	∙	. 1	1 ∖~^T 八 1 ɪ Λ Λ .	1
To further analyse the whole training process, We construct θ = T ∑t=i & and ξ = θ - θ to unpack
θF∖t for the averaged prediction of history networks, and then reformulate Eqn 11:
T
X(底>)Vξ%(x,y)
t=1
T
=XLMθ - θ)]vξfθ(x, y)
t=1
T 1
=Σ>t((T - (1 - α)i(tτ)ατ-t)θt - θF∖t)]Vξfθ(x,y)
t=1
T
=X[et((T - (1 - Q>-δ(t-1)QT-t)θt - (T - (1 - α)1-δ(t-DQT-t)θt-i +■-T1
t=1
-(1 - q)1-6(tT)QT-tθt-l - θF∖t)] Vξfθ(x, y
T
=X[et((T - (1 - α)1-δ(tT)QT-t)θt - (T - (1 - Q)1-6(t-1)QT-t)θt-ι + -ɪɪ
t=1
-(1 - Q)1-δ(tT)QT-tθt-1 - (1 - Q)1-"tT)QT-t+1θt-i - θF∖t,t-l)]Vξfθ(x, y)
T1	1	1
=X[et((T - (1 - Q) - (J )q T)θt - (T - (1 - Q) - (J )q T)θt-ι + (T
t=1
-(1 - Q)1-δ(tT)QT-t - (1 - Q)1-δ(tT)QT-tQ)θt-1 - θF∖t,t-l)]Vξfθ(x, y)
T1
=∑>t((T - (1 -Q)KtT)QT-t)(θt - θt-1) + C0)]Vξfθ(x,y),
t=1
〜	(17)
where C0 = (T - (1 - Q)1-δ(t-1)qt-t - (1 - Q)1-δ(tT)QT-tQ)θt-ι - θʃʌt,t-i. Likewise, the
above equation can be further deduced:
T1
∑>t(( T - (1 -Q)KtT)QT-t)(τtE(χ,y) (Vθt /(θt;(说,y)) + C0))]Vξf(x,y),	(18)
t=1
which means the difference between 方(x, y) and %(x, y) depends on learning rate schedules. □
18