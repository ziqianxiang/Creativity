Published as a conference paper at ICLR 2022
Interacting	Contour Stochastic	Gradient
Langevin Dynamics
Wei Deng1, 2, Siqi Liang1, Botao Hao3, Guang Lin1, Faming Liang1
1 Purdue University 2Morgan Stanley 3DeepMind
fmliang@purdue.edu; weideng056@gmail.com
Ab stract
We propose an interacting contour stochastic gradient Langevin dynamics (IC-
SGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic
gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We
show that ICSGLD can be theoretically more efficient than a single-chain CSGLD
with an equivalent computational budget. We also present a novel random-field
function, which facilitates the estimation of self-adapting parameters in big data and
obtains free mode explorations. Empirically, we compare the proposed algorithm
with popular benchmark methods for posterior sampling. The numerical results
show a great potential of ICSGLD for large-scale uncertainty estimation tasks.
1	Introduction
Stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) has achieved great successes
in simulations of high-dimensional systems for big data problems. It, however, yields only a fast
mixing rate when the energy landscape is simple, e.g., local energy wells are shallow and not well
separated. To improve its convergence for the problems with complex energy landscapes, various
strategies have been proposed, such as momentum augmentation (Chen et al., 2014; Ding et al., 2014),
Hessian approximation (Ahn et al., 2012; Li et al., 2016), high-order numerical schemes (Chen et al.,
2015; Li et al., 2019b), and cyclical learning rates (Izmailov et al., 2018; Maddox et al., 2019; Zhang
et al., 2020b). In spite of their asymptotic properties in Bayesian inference (Vollmer et al., 2016)
and non-convex optimization (Zhang et al., 2017), it is still difficult to achieve compelling empirical
results for pathologically complex deep neural networks (DNNs).
To simulate from distributions with complex energy landscapes, e.g., those with a multitude of modes
well separated by high energy barriers, an emerging trend is to run multiple chains, where interactions
between different chains can potentially accelerate the convergence of the simulation. For example,
Song et al. (2014) and Futami et al. (2020) showed theoretical advantages of appropriate interactions
in ensemble/population simulations. Other multiple chain methods include particle-based nonlinear
Markov (Vlasov) processes (Liu & Wang, 2016; Zhang et al., 2020a) and replica exchange methods
(also known as parallel tempering) (Deng et al., 2021a). However, the particle-based methods
result in an expensive kernel matrix computation given a large number of particles (Liu & Wang,
2016); similarly, naively extending replica exchange methods to population chains leads to a long
waiting time to swap between non-neighboring chains (Syed et al., 2021). Therefore, how to conduct
interactions between different chains, while maintaining the scalability of the algorithm, is the key to
the success of the parallel stochastic gradient MCMC algorithms.
In this paper, we propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD)
sampler, a pleasingly parallel extension of contour stochastic gradient Langevin dynamics (CSGLD)
(Deng et al., 2020b) with efficient interactions. The proposed algorithm requires minimal communi-
cation cost in that each chain shares with others the marginal energy likelihood estimate only. As a
result, the interacting mechanism improves the convergence of the simulation, while the minimal
communication mode between different chains enables the proposed algorithm to be naturally adapted
to distributed computing with little overhead. For the single-chain CSGLD algorithm, despite its
theoretical advantages as shown in Deng et al. (2020b), estimation of the marginal energy likelihood
remains challenging for big data problems with a wide energy range, jeopardizing the empirical
performance of the class of importance sampling methods (Gordon et al., 1993; Doucet et al., 2001;
1
Published as a conference paper at ICLR 2022
Wang & Landau, 2001; Liang et al., 2007; Andrieu et al., 2010; Deng et al., 2020b) in big data
applications. To resolve this issue, we resort to a novel interacting random-field function based
on multiple chains for an ideal variance reduction and a more robust estimation. As such, we can
greatly facilitate the estimation of the marginal energy likelihood so as to accelerate the simulations
of notoriously complex distributions. To summarize, the algorithm has three main contributions:
•	We propose a scalable interacting importance sampling method for big data problems with the min-
imal communication cost. A novel random-field function is derived to tackle the incompatibility
issue of the class of importance sampling methods in big data problems.
•	Theoretically, we study the local stability of a non-linear mean-field system and justify regularity
properties of the solution of Poisson’s equation. We also prove the asymptotic normality for the
stochastic approximation process in mini-batch settings and show that ICSGLD is asymptotically
more efficient than the single-chain CSGLD with an equivalent computational budget.
•	Our proposed algorithm achieves appealing mode explorations using a fixed learning rate on the
MNIST dataset and obtains remarkable performance in large-scale uncertainty estimation tasks.
2	Preliminaries
2.1	Stochastic gradient Langevin dynamics
A standard sampling algorithm for big data problems is SGLD (Welling & Teh, 2011), which is a
numerical scheme of a stochastic differential equation in mini-batch settings:
NL 高，、 y------
Xk+1 = Xk - €k —VχU(Xk) + √2τekWk,
(1)
n
where Xk ∈ X ∈ Rd , ek is the learning rate at iteration k, N denotes the number of total data points,
T is the temperature, and Wk is a standard Gaussian vector of dimension d. In particular, Nn VxU(x)
is an unbiased stochastic gradient estimator based on a mini-batch data B of size n and nnU(x) is the
unbiased energy estimator for the exact energy function U (X). Under mild conditions on U, Xk+1 is
known to converge weakly to a unique invariant distribution π(x) a e-Ux) as e《→ 0.
2.2	Contour stochastic gradient Langevin dynamics
Despite its theoretical guarantees, SGLD can converge exponentially slow when U(X) is non-convex
and exhibits high energy barriers. To remedy this issue, CSGLD (Deng et al., 2020b) exploits the flat
histogram idea and proposes to simulate from a flattened density with much lower energy barriers
$Wo(x) a ∏(x)∕Ψθ(U(x)),	(2)
u-ui-1
where Z is a hyperparameter, Ψθ(U) = Pii=ι ( θ(i - 1)e(log Wi) 2TOg θ(iT))~δ - ) 1ui-ι<u≤ui.
In particular, {ui }im=0 determines the partition {Xi }im=1 of X such that Xi = {x : ui-1 < U(x) ≤
Ui}, where -∞ = uo < uι < …< Um-ι < um, = ∞. For practical purposes, we assume
Ui+1 — Ui = ∆u for i = 1,... ,m - 2. In addition, θ = (θ⑴,θ(2),...,θ(m)) is the self-adapting
parameter in the space Θ = { (θ(1),…，θ(m)) ∣0 < θ(1),…，θ(m) < 1& Pm=I θ(i) = 1 j.
Ideally, setting Z = 1 and θ(i) = θ∞(i), where θ∞(i) = jχ. π(x)dx for i ∈ {1,2,…，m}, enables
CSGLD to achieve a “random walk” in the space of energy and to penalize the over-visited partition
(Wang & Landau, 2001; Liang et al., 2007; Fort et al., 2011; 2015). However, the optimal values of
θ∞ is unknown a priori. To tackle this issue, CSGLD proposes the following procedure to adaptively
estimate θ via stochastic approximation (SA) (Robbins & Monro, 1951; Benveniste et al., 1990):
(1) Sample xk+ι = Xk + ekNVxUJψfkk (xk) + √2τekWk,
小、.八	八，	八	、
(2) Optimize θk+1 = θk + ωk+1H(θk, xk+1),
2
Published as a conference paper at ICLR 2022
Algorithm 1 Interacting contour stochastic gradient Langevin dynamics algorithm (ICSGLD).
{Xi }im=1 is pre-defined partition and ζ is a hyperparameter. The update rule in distributed-memory
settings and discussions of hyperparameters is detailed in section B.1.1 in the supplementary material.
[1.] (Data subsampling) Draw a mini-batch data Bk from D, and compute stochastic gradients
VχUe (Xkp)) and energies U (Xkp)) foreach x(p), where P ∈ {1,2,…，P}, |Bk | = n, and |D| = N.
[2.] (Parallel simulation) Sample xN+P := (xk+「xk?「…，xk+)1)> based on SGLD and θk
xk+P = XN P + ek N VχUψθk (XN P) + Mwk P,	(4)
NP
where k is the learning rate, τ is the temperature, wk denotes P independent standard
Gaussian vectors, VxU田@ (XN P) = (VχUψg (X(I)), VχUeΨθ (X⑵),∙∙∙ , VχUeΨθ (X(P )))>, and
VχUψθ (x) = [l + ∆∆τu (log θ(JU (x)) - log θ((JU (x) - 1) ∨ 1))i VxU (x) for any X ∈ X.
[3.] (Stochastic approximation) Update the self-adapting parameter θ(i) for i ∈ {1, 2, ∙∙∙ ,m}
1P
θk+l(i) = θk(i) + ωk + 1 P X θk (JU (Xkp)I)) (Ii=Je (X(P)I) -θk (i)),	⑸
p=1
where 1a is an indicator function that takes value 1 if the event A appears and equals 0 otherwise.
where VxUψθ (∙) isa stochastic gradient function of $田@ (∙) to be detailed in Algorithm 1. H(θ, x):=
(Hι(θ, x), ... , Hm(θ, x)) is random-field function where each entry follows
m
Hi(θ, X) = θζ(JU(X))(Ii=JU(X) - θ(i)) , where ju(x) = X i1Ui-ι<nU(x)≤Ui.	⑶
i=1
Theoretically, CSGLD converges to a sampling-optimization equilibrium in the sense that θk ap-
proaches to a fixed point θ∞ and the samples are drawn from the flattened density $38- (x).
Notably, the mean-field system is globally stable with a unique stable equilibrium point in a small
neighborhood of θ∞. Moreover, such an appealing property holds even when U(X) is non-convex.
3	Interacting contour stochastic gradient Langevin dynamics
The major goal of interacting CSGLD (ICSGLD) is to improve the efficiency of CSGLD. In particular,
the self-adapting parameter θ is crucial for ensuring the sampler to escape from the local traps and
traverse the whole energy landscape, and how to reduce the variability of θk,s is the key to the success
of such a dynamic importance sampling algorithm. To this end, we propose an efficient variance
reduction scheme via interacting parallel systems to improve the accuracy of θk's.
3.1	Interactions in parallelism
Now we first consider a naive parallel sampling scheme with P chains as follows
xk+P = XN P + ek N VχUψθk (XN P) + EWN P,
where XN P = (X(I), x(2),…，x(P))>, wk P denotes P independent standard Gaussian vectors,
and Uψθ(xN P) = (Uψθ (X⑴),Uψθ(X⑵)，…，Uψθ (x(P)))>.
Stochastic approximation aims to find the solution θ of the mean-field system h(θ) such that
h(θ) = H He(θ, x)$e(dx)
X
0,
where $e is the invariant measure simulated via SGLD that approximates $田8 in (2) and H(θ, x)
is the novel random-field function to be defined later in (8). Since h(θ) is observable only up
to large random perturbations (in the form of H(θ, X)), the optimization of θ based on isolated
3
Published as a conference paper at ICLR 2022
random-field functions may not be efficient enough. However, due to the conditional independence
of X(I), x(2), •…,X(P) in parallel sampling, it is very natural to consider a Monte Carlo average
h(θ) = P XX Z He(θ, X(P))$e(dx(P)) = 0.	(6)
p=1 X
Namely, we are considering the following stochastic approximation scheme
θk+1 = θk + ωk+1Hf(θk, XkN+P1),	(7)
where H(θk, xN+P) is an interacting random-field function H(θk, xN+P) = P PP=I H(θk, xk+ι).
Note that the Monte Carlo average is very effective to reduce the variance of the interacting random-
NP
field function H(θ, x P) based on the conditionally independent random field functions. Moreover,
each chain shares with others only a very short message during each iteration. Therefore, the
interacting parallel system is well suited for distributed computing, where the implementations
and communication costs are further detailed in section B.1.2 in the supplementary material. By
contrast, each chain of the non-interacting parallel CSGLD algorithm deals with the parameter θ and
a large-variance random-field function H (θ, x) individually, leading to coarse estimates in the end.
Formally, for the population/ensemble interaction scheme (7), we define a novel random-field function
H(θ, x) = (Hι(θ, x),H2(θ, x),… ,Hm(θ, x)), where each component satisfies
Hei(θ,x) =θ(JUe(x)) 1i=JUe (x) - θ(i) .	(8)
As shown in Lemma 1, the corresponding mean-field function proposes to converge to a different
fixed point θ?, s.t.
θ*(i) æ (/ e- T, dx) - æ θ∞(i).	(9)
A large data set often renders the task of estimating θ∞ numerically challenging. By contrast, we
resort to a different solution by estimating θ? instead based on a large value of ζ. The proposed
algorithm is summarized in Algorithm 1. For more study on the scalablity of the new scheme, we
leave the discussion in section B.1.3.
3.2	Related works
Replica exchange SGLD (Deng et al., 2020a; 2021a) has successfully extended the traditional
replica exchange (Swendsen & Wang, 1986; Geyer, 1991; Earl & Deem, 2005) to big data problems.
However, it works with two chains only and has a low swapping rate. As shown in Figure 1(a), a
naive extension of multi-chain replica exchange SGLD yields low communication efficiency. Despite
some recipe in the literature (Katzgraber et al., 2008; Bittner et al., 2008; Syed et al., 2021), how to
conduct multi-chain replica exchange with low-frequency swaps is still an open question.
(a) Replica Exchange (parallel tempering)
(b) Interacting contour SGLD (ICSGLD)
Figure 1: A comparison of communication costs between replica exchange (RE) and ICSGLD. We
see RE takes many iterations to swap with all the other chains; by contrast, ICSGLD possesses a
pleasingly parallel mechanism where the only cost comes from sharing a light message.
Stein variational gradient descent (SVGD) (Liu & Wang, 2016) is a popular approximate inference
method to drive a set of particles for posterior approximation. In particular, repulsive forces are
proposed to prevent particles to collapse together into neighboring regions, which resembles our
strategy of penalizing over-visited partition. However, SVGD tends to underestimate the uncertainty
given a limited number of particles. Moreover, the quadratic cost in kernel matrix computation further
raises the scalability concerns as more particles are proposed.
4
Published as a conference paper at ICLR 2022
Admittedly, ICSGLD is not the first interacting importance sampling algorithm. For example, a
population stochastic approximation Monte Carlo (pop-SAMC) algorithm has been proposed in Song
et al. (2014), and an interacting particle Markov chain Monte Carlo (IPMCMC) algorithm has been
proposed in Rainforth et al. (2016). A key difference between our algorithm and others is that our
algorithm is mainly devised for big data problems. The IPMCMC and pop-SAMC are gradient-free
samplers, which are hard to be adapted to high-dimensional big data problems.
Other parallel SGLD methods (Ahn et al., 2014; Chen et al., 2016) aim to reduce the computational
cost of gradient estimations in distributed computing, which, however, does not consider interactions
for accelerating the convergence. Li et al. (2019a) proposed asynchronous protocols to reduce
communication costs when the master aggregates model parameters from all workers. Instead, we
don’t communicate the parameter x ∈ Rd but only share θ ∈ Rm and the indices, where m d.
Our work also highly resembles the well-known Federated Averaging (FedAvg) algorithm (Li et al.,
2020; Deng et al., 2021b), except that the stochastic gradient U(x) is replaced with the random field
function H(θ, x) and we only share the low-dimensional latent vector θ. Since privacy concerns
and communication cost are not major bottlenecks of our problem, we leave the study of taking the
Monte Carlo average in Eq.(6) every K > 1 iterations for future works.
4	Convergence properties
To study theoretical properties of ICSGLD, we first show a local stability property that is well-suited
to big data problems, and then we present the asymptotic normality for the stochastic approxi-
mation process in mini-batch settings, which eventually yields the desired result that ICSGLD is
asymptotically more efficient than a single-chain CSGLD with an equivalent computational cost.
4.1	Local stability for non-linear mean-field systems in big data
The first obstacle for the theoretical study is to approximate the components of θ∞ corresponding to
the high energy region. To get around this issue, the random field function H(θ, x) in (8) is adopted
1
to estimate a different target θ? α θ∞∞. As detailed in Lemma 3 in the supplementary material, the
mean-field equation is now formulated as follows
hi(θ) H θ?(i) 一(θ(i)Cθ)ζ + perturbations,	(10)
where Ce = (]ZK^) and Zeζ,θ = Pm=I，；%：(，：. We see that (10) may not be linearly stable
as in Deng et al. (2020b). Although the solution of the mean-field system h(θ) = 0 is still unique,
there may exist unstable invariant subspaces, leading us to consider the local properties. For a proper
initialization of θ, which can be achieved by pre-training the model long enough time through SGLD,
the mean value theorem implies a linear property in a local region
hi(θ) H θ*(i) 一 θ(i) + perturbations.
Combining the perturbation theory (Vanden-Eijnden, 2001), we present the following stability result:
Lemma 1 (Local stability, informal version of Lemma 3) Assume Assumptions A1-A4 (given in
the supplementary material) hold. For any properly initialized θ, we have <h(θ), θ 一 θ*i ≤
—φ∣∣θ — θ*∣∣2, where θ? = θ? + O (SuPx Var(ξn(x)) + e + 2),θ? H θ∞∞ , φ > 0, e denotes a
learning rate, and ξn(x) denotes the noise in the stochastic energy estimator of batch size n and
Var(∙) denotes the variance.
By justifying the drift conditions of the adaptive transition kernel and relevant smoothness properties,
we can prove the existence and regularity properties of the solution of the Poisson’s equation in
Lemma 6 in the supplementary material. In what follows, we can control the fluctuations in stochastic
approximation and eventually yields the L2 convergence.
Lemma 2 (L2 convergence rate, informal version of Lemma 7) Given standard Assumptions A1-
A5. θk converges to θ?, where θ? = θ? + O (SuPx Var(ξn(x)) + e + 2), such that
E[∣∣θk — b?k2i = O(ωk).
5
Published as a conference paper at ICLR 2022
The result differs from Theorem
1 of Deng et al. (2020b) in that the biased fixed point θb? instead of
θ? is treated as the equilibrium of the continuous system, which provides us a user-friendly proof.
Similar techniques have been adopted by Durmus & Eric Moulines (2017); Xu et al. (2018). Although
1
the global stability (Deng et al., 2020b) may be sacrificed when Z = 1 based on Eq.(8), θ? α θ∞∞ is
much easier to estimate numerically for any i that yields 0 < θ∞ (i)	1 based on a large ζ > 1.
4.2	Asymptotic normality
-1 /
To study the asymptotic behavior of 3卜 2 (θk 一 θ?), where θ? is the equilibrium point s.t. θ?=
θ? + O (Var(ξn(x)) + e + ml), we consider a fixed step size ω in the SA step for ease of explanation.
Let θt denote the solution of the mean-field system in continuous time (尻=θo), and rewrite the
single-chain SA step (7) as follows
θk+1 一θ(k + 1)ω = θk 一 θkω + ω (H(Ok, xk+1)- H(θkω, xk+1))
+ ω (H (θ kω, xk+1)- h(θkω) 一 ('(k+1)ω - θkω 一 ωh(θ^kω )).
Further, we set θkω := ω- 1 (θk 一 θkω). Then the stochastic approximation differs from the mean
field system in that
kk
θ(k + 1)ω = ω 2 E (H(θi, xi+1) - H(θiω, xi+1)) +ω 2 E (H (θiω, xi+ι) - h(θiω)) -ω2 ∙ remainder
i=0	i=0 |----{z----}
×	{z	II: martingale Mi
I: perturbations
kk
≈ ω2 X hθ(θiω)(θi - θiω) +ω2 X Mi ≈
i=0
^^">^^^"
1 ~
≈ω 2 θiω
i=0
ZsW hθ(θs)θsds + ZsW R1 (θs)dWs,
00
where h® (θ):=岛h(θ) is a matrix, W ∈ Rm is a standard Brownian motion, the last term follows
from a certain central limit theorem (Benveniste et al., 1990) and R denotes the covariance matrix of
the random-field function s.t. R(θ) := Pk∞=-∞ Covθ(H(θ, xk), H(θ, x0)).
We expect the weak convergence of Uk to the stationary distribution of a diffusion
dUt = hθ (θt)Utdt + R1∕2(θt)dWt,
(11)
where Ut = ω-1/2 (θt 一 θ?). Given that θt converges to θ? sufficiently fast and the local linearity of
hθ, the diffusion (11) resembles the Ornstein-Uhlenbeck process and yields the following solution
Ut ≈ e-th"θ*)U0 +
Zt
0
e-(t-s)hθ (θb* )
.^ .
◦ R(Ob?)dWs.
Then we have the following theorem, whose formal proof is given in section C.3.
Theorem 1 (Asymptotic Normality) Assume Assumptions A1-A5 (given in the supplementary ma-
terial) hold. We have the following weak convergence
∞
0
9-1/2回- θ?) ⇒N(o, ∑)
where Σ
ethθ* ◦ R ◦ ethθ>* dt, hθ
一 ，^ 、
hθ(θb?).
4.3 Interacting parallel chains are more efficient
For clarity, we first denote an estimate of θ based on ICSGLD with P interacting parallel chains by
θkP and denote the estimate based on a single-long-chain CSGLD by θkP .
Note that Theorem 1 holds for any step size ωk = O(k-α), where α ∈ (0.5, 1]. If we simply run a
single-chain CSGLD algorithm with P times of iterations, by Theorem 1,
ω/2(θkP - θ?) ⇒N(0, Σ).
As to ICSGLD, since the covariance Σ relies on R, which depends on the covariance of the
martingale {Mi}i≥ι, the conditional independence of x(1), x(2),…，X(P) naturally results in an
efficient variance reduction such that
6
Published as a conference paper at ICLR 2022
Corollary 1 (Asymptotic Normality for ICSGLD) Assume the same assumptions. For ICSGLD
with P interacting chains, we have the following weak convergence
ω-"2(θP - b?) ⇒N(0, Σ∕P).
b
That is, under a similar computational budget, We have k	( ：JP b?)kF =瓷P ≈ P1-α.
Il Var(θk -θ* 川F	k/
Corollary 2 (Efficiency) Given a decreasing step size ωk= O(k-α), where 0.5 < α < 1, ICSGLD
is asymptotically more efficient than the single-chain CSGLD With an equivalent training cost.
In practice, sloWly decreasing step sizes are often preferred in stochastic algorithms for a better
non-asymptotic performance (Benveniste et al., 1990).
5 Experiments
5.1	Landscape exploration on MNIST via the scalable random-field function
This section shoWs hoW the novel random-field function (8) facilitates the exploration of multiple
modes on the MNIST dataset§, While the standard methods, such as stochastic gradient descent (SGD)
and SGLD, only get stuck in few local modes. To simplify the experiments, We choose a large batch
size of 2500 and only pick the first five classes, namely digits from 0 to 4. The learning rate is fixed
to 1e-6 and the temperature is set to 0.1t. We see from Figure 2(a) that both SGD and SGLD lead to
fast decreasing losses. By contrast, ICSGLD yields fluctuating losses that traverse freely betWeen
high energy and loW energy regions. As the particles stick in local regions, the penalty of re-visiting
these zones keeps increasing until a negative learning rate is injected to encourage explorations.
(a) Training Loss	(b) SGD
(c) SGLD	(d) ICSGLD
Figure 2: Visualization of mode exploration on a MNIST example based on different algorithms.
We conducted a singular value decomposition (SVD) based on the first tWo coordinates to visualize
the trajectories: We first choose a domain that includes all the coordinates, then We recover the
parameter based on the grid point and truncated values in other dimensions, and finally We fine-tune
the parameters and present the approximate losses of the trajectories in Figure 2(b-d). We see SGD
trajectories get stuck in a local region; SGLD exploits a larger region but is still quite limited in
the exploration; ICSGLD, instead, first converges to a local region and then escapes it once it over-
visits this region. This shoWs the strength of ICSGLD in the simulations of complex multi-modal
distributions. More experimental details are presented in section D.1 of the supplementary material.
5.2	Simulations of multi-modal distributions
This section shoWs the acceleration effect of ICSGLD via a group of simulation experiments for
a multi-modal distribution. The baselines include popular Monte Carlo methods such as CSGLD,
SGLD, cyclical SGLD (cycSGLD), replica exchange SGLD (reSGLD), and the particle-based SVGD.
The target multi-modal density is presented in Figure 3(a). Figure 3(b-g) displays the empirical
performance of all the testing methods: the vanilla SGLD With 5 parallel chains (×P5) undoubtedly
§The random-field function (Deng et al., 2020b) requires an extra perturbation term as discussed in section
D4 in the supplementary material (Deng et al., 2020b); therefore it is not practically appealing in big data.
tData augmentation implicitly leads to a more concentrated posterior (Wenzel et al., 2020; Aitchison, 2021).
7
Published as a conference paper at ICLR 2022
performs the worst in this example and fails to quantify the weights of each mode correctly; the
single-chain cycSGLD with 5 times of iterations (×T5) improves the performance but is still not
accurate enough; reSGLD (×P5) and SVGD (×P5) have good performances, while the latter is
quite costly in computations; ICSGLD (×P5) does not only traverse freely over the rugged energy
landscape, but also yields the most accurate approximation to the ground truth distribution. By
contrast, CSGLD (×T5) performs worse than ICSGLD and overestimates the weights on the left side.
For the detailed setups, the study of convergence speed, and runtime analysis, we refer interested
readers to section D.2 in the supplementary material.
(a) Truth (b) SGLD (c) cycSGLD (d) SVGD (e) reSGLD (f) CSGLD (g) ICSGLD
Figure 3: Empirical behavior on a simulation dataset. Figure 3(c) and 3(f) show the simulation based
on a single chain with 5 times of iterations (×T5) and the others run 5 parallel chains (×P5).
5.3	Deep contextual bandits on mushroom tasks
This section evaluates ICSGLD on the contextual bandit problem based on the UCI Mushroom
data set as in Riquelme et al. (2018). The mushrooms are assumed to arrive sequentially and the
agent needs to take an action at each time step based on past feedbacks. Our goal is to minimize
the cumulative regret that measures the difference between the cumulative reward obtained by the
proposed policy and optimal policy. We evaluate Thompson Sampling (TS) based on a variety of
approximate inference methods for posterior sampling. We choose one -greedy policy (EpsGreedy)
based on the RMSProp optimizer with a decaying learning rate (Riquelme et al., 2018) as a baseline.
Two variational methods, namely stochastic gradient descent with a constant learning rate (ConstSGD)
(Mandt et al., 2017) and Monte Carlo Dropout (Dropout) (Gal & Ghahramani, 2016) are compared
to approximate the posterior distribution. For the sampling algorithms, we include preconditioned
SGLD (pSGLD) (Li et al., 2016), preconditioned CSGLD (pCSGLD) (Deng et al., 2020b), and
preconditioned ICSGLD (pICSGLD). Note that all the algorithms run 4 parallel chains with average
outputs (×P4) except that pCSGLD runs a single-chain with 4 times of computational budget (×T4).
For more details, we refer readers to section D.3 in the supplementary material.
Figure 4 shows that EpsGreedy ×P4 tends to explore too much for a long horizon as expected;
ConstSGD×P4 and Dropout×P4 perform poorly in the beginning but eventually outperform Eps-
Greedy ×P4 due to the inclusion of uncertainty for exploration, whereas the uncertainty seems to
be inadequate due to the nature of variational inference. By contrast, pSGLD×P4 significantly
outperforms the variational methods by considering preconditioners
within an exact sampling framework (SGLD). As a unique algo-
rithm that runs in a single-chain manner, pCSGLD×T4 leads to
the worst performance due to the inefficiency in learning the self-
adapting parameters, fortunately, pCSGLD×T4 slightly outperform
pSGLD×P4 in the later phase with the help of the well-estimated
self-adapting parameters. Nevertheless, pICSGLD×P4 propose to
optimize the shared self-adapting parameters at the same time, which
in turn greatly contributes to the simulation of the posterior. As a
result, pICSGLD×P4 consistently shows the lowest regret exclud-
ing the very early period. This shows the great superiority of the
interaction mechanism in learning the self-adapting parameters for
accelerating the simulations.
Figure 4: Cumulative regret
on the mushroom task.
5.4 Uncertainty estimation
This section evaluates the qualify of our algorithm in uncertainty quantification. For model architec-
tures, we use residual networks (ResNet) (He et al., 2016) and a wide ResNet (WRN) (Zagoruyko
& Komodakis, 2016); we choose 20, 32, and 56-layer ResNets (denoted by ResNet20, et al.) and a
WRN-16-8 network, a 16-layer WRN that is 8 times wider than ResNet16. We train the models on
8
Published as a conference paper at ICLR 2022
CIFAR100, and report the test accuracy (ACC) and test negative log-likelihood (NLL) based on 5
trials with standard error. For the out-of-distribution prediction performance, we test the well-trained
models in Brier scores (Brier) * on the Street View House Numbers dataset (SVHN).
Due to the wide adoption of momentum stochastic gradient descent (M-SGD), we use stochastic
gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) as the baseline sampling algorithm
and denote the interacting contour SGHMC by ICSHMC. In addition, we include several high
performing baselines, such as SGHMC with cyclical learning rates (cycSGHMC) (Zhang et al.,
2020b), SWAG based on cyclic learning rates of 10 cycles (cycSWAG) (Maddox et al., 2019) and
variance-reduced replica exchange SGHMC (reSGHMC) (Deng et al., 2021a). For a fair comparison,
ICSGLD also conducts variance reduction on the energy function to alleviate the bias. Moreover, a
large ζ = 3 × 106 is selected, which only induces mild gradient multipliers ranging from -1 to 2 to
penalize over-visited partitions. We don’t include SVGD (Liu & Wang, 2016) and SPOS (Zhang et al.,
2020a) for scalability reasons. A batch size of 256 is selected. We run 4 parallel processes (×P4)
with 500 epochs for M-SGD, reSGHMC and ICSGHMC and run cycSGHMC and cycSWAG 2000
epochs (×T4) based on a single process with 10 cycles. Refer to section D.4 of the supplementary
material for the detailed settings.
Table 1: Uncertainty estimations on CIFAR 1 00 and SVHN.
Model	ReSNet20			ResNet32		
	ACC (%)	NLL	Brier (‰)	ACC(%)	NLL	Brier (‰)
cycSGHMC×T4	75.41±0.1O=	8437±30	2.91±0.13=	77.93±0.17Z	7658±19	3.29±0.13
cycSWAG×T4	75.46±0.11	8419±26	2.78±0.12	77.91±0.15	7656±22	3.19±0.14
M-SGD×P4	76.01±0.l2=Z	8175±25	2.58±0.08=	78.41±0.1Γ	7501±23	2.77±0. 15
reSGHMC×P4	76.15±0.16	8196±27	2.73±0.10	78.57±0.07	7454±15	3.04±0.09
ICSGHMC×P4	76.34±0.15	8076±3 1	2.54±0.14	78.72±0.16	7406±29	2.76±0.15
Model	ReSNet56			WRN-16-8		
	ACC (%)	NLL	Brier (‰)	ACC(%)	NLL	Brier (‰)
cycSGHMC×T4	81.23±0.l9z	6770±59	3.18±0.08=	82.98±0.0Γ	6384±1 1	2.17±0.05
cycSWAG×T4	81.14±0.11	6744±55	3.06±0.09	83.05±0.04	6359±14	2.04±0.07
M-SGD×P4	81.03±0.lF	6847±22	2.86±0.08 二	82.57±0.07Z	6821±21	1.77±0.06
reSGHMC×P4	81.11±0.16	691 5±40	2.92±0.12	82.72±0.08	6452±19	1.92±0.04
ICSGHMC×P4	81.51±0.18	6630±38	2.88±0.09	83.12±0.10	6338±36	1.83±0.06
Table 1 shows that the vanilla ensemble results via M-SGD×P4 surprisingly outperform
cycSGHMC×T4 and cycSWAG×T4 on medium models, such as ResNet20 and ResNet32, and
show very good performance on the out-of-distribution samples in Brier scores. We suspect that
the parallel implementation (×P4) provides isolated initializations with less correlated samples; by
contrast, cycSGHMC×T4 and cycSWAG×T4 explore the energy landscape contiguously, implying a
risk to stay near the original region. reSGHMC×P4 shows a remarkable performance overall, but
demonstrates a large variance occasionally; this indicates the insufficiency of the swaps when multiple
processes are included. When it comes to testing WRN-16-8, cycSWAG×T4 shows a marvelous
result and a large improvement compared to the other baselines. We conjecture that cycSWAG is
more independent of hyperparameter tuning, thus leading to better performance in larger models.
We don’t report CSGHMC×P4 since it becomes quite unstable during the training of ResNet56 and
WRN-16-8 models and causes mediocre results. As to ICSGHMC×P4, it consistently performs
remarkable in both ACC and NLL and performs comparable to M-SGD×P4 in Brier scores.
Code is available at github.com/WayneDW/Interacting-Contour-Stochastic-Gradient-Langevin-Dynamics.
6 Conclusion
We have proposed the ICSGLD as an efficient algorithm for sampling from distributions with a
complex energy landscape, and shown theoretically that ICSGLD is indeed more efficient than
the single-chain CSGLD for a slowly decreasing step size. To our best knowledge, this is the
first interacting importance sampling algorithm that adapts to big data problems without scalability
concerns. ICSGLD has been compared with numerous state-of-the-art baselines for various tasks,
whose remarkable results indicate its promising future in big data applications.
*The Brier score measures the mean squared error between the predictive and actual probabilities.
9
Published as a conference paper at ICLR 2022
Acknowledgment
Liang’s research was supported in part by the grants DMS-2015498, R01-GM117597 and R01-
GM126089. Lin acknowledges the support from NSF (DMS-1555072, DMS-2053746, and DMS-
2134209), BNL Subcontract 382247, and DE-SC0021142.
References
Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian Posterior Sampling via Stochastic
Gradient Fisher Scoring. In Proc. of the International Conference on Machine Learning (ICML),
2012.
Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed Stochastic Gradient MCMC. In Proc.
of the International Conference on Machine Learning (ICML), 2014.
Laurence Aitchison. A Statistical Theory of Cold Posteriors in Deep Neural Networks. In Proc. of
the International Conference on Learning Representation (ICLR), 2021.
C. Andrieu, E. Moulines, and P. Priouret. Stability of Stochastic Approximation under Verifiable
Conditions. SIAMJ. Control Optim., 44(1):283-312, 2005.
Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov Chain Monte Carlo
Methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3), 2010.
Albert Benveniste, Michael Metivier, and Pierre Priouret. Adaptive Algorithms and Stochastic
Approximations. Berlin: Springer, 1990.
Elmar Bittner, Andreas Nussbaumer, and Wolfhard Janke. Make Life Simple: Unleash the Full Power
of the Parallel Tempering Algorithm. Physical Review Letters, 101:130603-130603, 2008.
Changyou Chen, Nan Ding, and Lawrence Carin. On the Convergence of Stochastic Gradient MCMC
Algorithms with High-order Integrators. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 2278-2286, 2015.
Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, and Lawrence Carin. Stochastic Gradient
MCMC with Stale Gradients. In Advances in Neural Information Processing Systems (NeurIPS),
2016.
Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. In
Proc. of the International Conference on Machine Learning (ICML), 2014.
Wei Deng, Qi Feng, Liyao Gao, Faming Liang, and Guang Lin. Non-Convex Learning via Replica
Exchange Stochastic Gradient MCMC. In Proc. of the International Conference on Machine
Learning (ICML), 2020a.
Wei Deng, Guang Lin, and Faming Liang. A Contour Stochastic Gradient Langevin Dynamics
Algorithm for Simulations of Multi-modal Distributions. In Advances in Neural Information
Processing Systems (NeurIPS), 2020b.
Wei Deng, Qi Feng, Georgios Karagiannis, Guang Lin, and Faming Liang. Accelerating Conver-
gence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction. In Proc. of the
International Conference on Learning Representation (ICLR), 2021a.
Wei Deng, Yi-An Ma, Zhao Song, Qian Zhang, and Guang Lin. On Convergence of Federated
Averaging Langevin Dynamics. arXiv:2112.05120v1, 2021b.
Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D. Skeel, and Hartmut Neven.
Bayesian Sampling using Stochastic Gradient Thermostats. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 3203-3211, 2014.
Arnaud Doucet, Nando de Freitas, and Neil Gordon. Sequential Monte Carlo Methods in Practice.
Springer Science & Business Media, 2001.
10
Published as a conference paper at ICLR 2022
* 1 ♦ 1 ʌ	1 TA ∙ -nr I ∙	、τ	.	f~Λ	* 1 ♦ Γ∙ . 1 τ T 1 ∙ . 1
Alain Durmus and Eric Moulines. Non-asymptotic Convergence Analysis for the Unadjusted
Langevin Algorithm. Annals ofApplied Probability, 27:1551-1587, 2017.
David J. Earl and Michael W. Deem. Parallel Tempering: Theory, Applications, and New Perspectives.
Phys. Chem. Chem. Phys., 7:3910-3916, 2005.
Murat A Erdogdu, Lester Mackey, and Ohad Shamir. Global Non-convex Optimization with Dis-
cretized Diffusions. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
G. Fort, E. Moulines, and P. Priouret. Convergence of Adaptive and Interacting Markov Chain Monte
Carlo Algorithms. Annals of Statistics, 39:3262-3289, 2011.
G. Fort, B. Jourdain, E. Kuhn, T. Lelievre, and G. Stoltz. Convergence of the Wang-Landau Algorithm.
Math. Comput., 84(295):2297-2327, 2015.
Futoshi Futami, Issei Sato, and Masashi Sugiyama. Accelerating the Diffusion-based Ensemble
Sampling by Non-reversible Dynamics. In Proc. of the International Conference on Machine
Learning (ICML), 2020.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning. In Proc. of the International Conference on Machine Learning
(ICML), 2016.
Charles J. Geyer. Markov Chain Monte Carlo Maximum Likelihood. Computing Science and
Statistics: Proceedings of the 23rd Symposium on the Interfac, pp. 156-163, 1991.
Neil J Gordon, David J Salmond, and Adrian FM Smith. Novel Approach to Nonlinear/Non-Gaussian
Bayesian State Estimation. IEE Proceedings F (Radar and Signal Processing), 140(2), 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging Weights Leads to Wider Optima and Better Generalization. In Proc. of the Conference
on Uncertainty in Artificial Intelligence (UAI), 2018.
K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the Best Multi-stage Architecture
for Object Recognition? In Proc. of the International Conference on Computer Vision (ICCV), pp.
2146-2153, September 2009.
Helmut G Katzgraber, Simon Trebst, David A Huse, and Matthias Troyer. Feedback-Optimized
Parallel Tempering Monte Carlo. Journal of Statistical Mechanics: Theory and Experiment, pp. p.
P03018, 2008.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned Stochastic
Gradient Langevin Dynamics for Deep Neural Networks. In Proc. of the National Conference on
Artificial Intelligence (AAAI), pp. 1788-1794, 2016.
Chunyuan Li, Changyou Chen, Yunchen Pu, Ricardo Henao, and Lawrence Carin. Communication-
Efficient Stochastic Gradient MCMC for Neural Networks. In Proc. of the National Conference on
Artificial Intelligence (AAAI), 2019a.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of
FedAvg on Non-IID Data. In Proc. of the International Conference on Learning Representation
(ICLR), 2020.
Xuechen Li, Denny Wu, Lester Mackey, and Murat A. Erdogdu. Stochastic Runge-Kutta Accelerates
Langevin Monte Carlo and Beyond. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 7746-7758, 2019b.
Faming Liang, Chuanhai Liu, and Raymond J. Carroll. Stochastic Approximation in Monte Carlo
Computation. Journal of the American Statistical Association, 102:305-320, 2007.
Qiang Liu and Dilin Wang. Stein Variational Gradient Descent: A General Purpose Bayesian
Inference Algorithm. In Advances in Neural Information Processing Systems (NeurIPS), 2016.
11
Published as a conference paper at ICLR 2022
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
Simple Baseline for Bayesian Uncertainty in Deep Learning. In Advances in Neural Information
Processing Systems (NeurIPS), 2019.
Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic Gradient Descent as Approxi-
mate Bayesian Inference. Journal ofMachine Learning Research, 18:1-35, 2017.
J.C. Mattingly, A.M. Stuartb, and D.J. Highamc. Ergodicity for SDEs and Approximations: Locally
Lipschitz Vector Fields and Degenerate Noise. Stochastic Processes and their Applications, 101:
185-232, 2002.
Jonathan C. Mattingly, Andrew M. Stuart, and M.V. Tretyakov. Convergence of Numerical Time-
Averaging and Stationary Measures via Poisson Equations. SIAM Journal on Numerical Analysis,
48:552-577, 2010.
Mariane Pelletier. Weak Convergence Rates for Stochastic Approximation with Application to
Multiple Targets and Simulated Annealing. Annals of Applied Probability, 8:10-44, 1998.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex Learning via Stochastic
Gradient Langevin Dynamics: a Nonasymptotic Analysis. In Proc. of Conference on Learning
Theory (COLT), June 2017.
Tom Rainforth, Christian A. Naesseth, Fredrik Lindsten, Brooks Paige, Jan-Willem van de Meent,
Arnaud Doucet, and Frank Wood. Interacting Particle Markov Chain Monte Carlo. In Proc. of the
International Conference on Machine Learning (ICML), 2016.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian Bandits Showdown. In Proc. of
the International Conference on Learning Representation (ICLR), 2018.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. Annals of Mathematical
Statistics, 22:400-407, 1951.
Gareth O. Roberts and Richard L. Tweedie. Exponential Convergence of Langevin Distributions and
Their Discrete Approximations. Bernoulli, 2(4):341-363, 1996.
Issei Sato and Hiroshi Nakagawa. Approximation Analysis of Stochastic Gradient Langevin Dynamics
by Using Fokker-Planck Equation and Ito Process. In Proc. of the International Conference on
Machine Learning (ICML), 2014.
Qifan Song, Mingqi Wu, and Faming Liang. Weak Convergence Rates of Population versus Single-
Chain Stochastic Approximation MCMC Algorithms. Advances in Applied Probability, 46:
1059-1083, 2014.
Robert H. Swendsen and Jian-Sheng Wang. Replica Monte Carlo Simulation of Spin-Glasses.
Physical Review Letters, 57:2607-2609, 1986.
Saifuddin Syed, Alexandre Bouchard-COte, George Deligiannidis, and Arnaud Doucet. Non-
Reversible Parallel Tempering: a Scalable Highly Parallel MCMC scheme. Journal of Royal
Statistical Society, Series B, 2021.
Yee Whye Teh, Alexandre Thiery, and Sebastian Vollmer. Consistency and Fluctuations for Stochastic
Gradient Langevin Dynamics. Journal of Machine Learning Research, 17:1-33, 2016.
Eric Vanden-Eijnden. Introduction to Regular Perturbation Theory. Slides, 2001. URL https:
//cims.nyu.edu/~eve2/reg_pert.pdf.
Sebastian J. Vollmer, Konstantinos C. Zygalakis, and Yee Whye Teh. Exploration of the (Non-)
Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics. Journal of Machine
Learning Research, 17(159):1-48, 2016.
Fugao Wang and David P. Landau. Efficient, Multiple-range Random Walk Algorithm to Calculate
the Density of States. Physical Review Letters, 86:2050-3, 2001.
T. Weinhart, A. Singh, and A.R. Thornton. Perturbation Theory & Stability Analysis. Slides, 2010.
12
Published as a conference paper at ICLR 2022
Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In
Proc. ofthe International Conference on Machine Learning (ICML),pp. 681-688, 2011.
Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How Good is the Bayes
Posterior in Deep Neural Networks Really? In Proc. of the International Conference on Machine
Learning (ICML), 2020.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global Convergence of Langevin Dynamics
Based Algorithms for Nonconvex Optimization. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. In Proceedings of the British
Machine Vision Conference (BMVC), pp. 87.1-87.12, September 2016.
Jianyi Zhang, Ruiyi Zhang, Lawrence Carin, and Changyou Chen. Stochastic Particle-Optimization
Sampling and the Non-Asymptotic Convergence Theory. In Proceedings of the International
Workshop on Artificial Intelligence and Statistics, 2020a.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical
Stochastic Gradient MCMC for Bayesian Deep Learning. In Proc. of the International Conference
on Learning Representation (ICLR), 2020b.
Yuchen Zhang, Percy Liang, and Moses Charikar. A Hitting Time Analysis of Stochastic Gradient
Langevin Dynamics. In Proc. of Conference on Learning Theory (COLT), pp. 1980-2022, 2017.
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random Erasing Data Augmen-
tation. ArXiv e-prints, 2017.
13
Published as a conference paper at ICLR 2022
We summarize the supplementary material as follows: Section A provides the preliminary knowledge
for stochastic approximation; Section B shows a local stability condition that adapts to high losses;
Section C proves the main asymptotic normality for the stochastic approximation process, which
naturally yields the conclusion that interacting contour stochastic gradient Langevin dynamics
(ICSGLD) is more efficient than the analogous single chain based on slowly decreasing step sizes;
Section D details the experimental settings.
A Preliminaries
A. 1 Stochastic approximation
Given a random-field function H(θ, x), the stochastic approximation algorithm (Benveniste et al.,
1990) proposes to solve the mean-field equation h(θ) = 0 in the analysis of adaptive algorithms
h(θ) = H He(θ, x)$e(dx) = 0,
X
where X ∈ X ⊂ Rd, θ ∈ Θ ⊂ Rm, $e (x) is a distribution that depends on the self-adapting
parameter θ. Given the transition kernel Πθ(x, A) for any Borel subset A ⊂ X, the algorithm can be
written as follows
(1)	Simulate Xk+ι 〜 ∏θ% (Xk, ∙), which yields the invariant distribution $ek (∙),
(2)	Optimize θk+1 = θk + ωk+1H	(θk, Xk+1).
Compared with the standard Robbins-Monro algorithm (Robbins & Monro, 1951), the algorithm
proposes to simulate X from a transition kernel ∏θ(∙, ∙) instead of the distribution $e(∙) directly. In
.1	F	TT ∕Z⅛	∖	7 / Λ ∖ ∙	. -K r	11.	.1	ɪ r 1	,,1	1	♦
other words, ,H	(θk, Xk+1) - h(θk) is not a Martingale but rather a Markov state-dependent noise.
A.2 Poisson’ s equation
In the stochastic approximation algorithm, the sequence of {(Xk, θk)}k∞=1 on the product space
X × Θ is generated, which is an inhomogeneous Markov chain and requires the tool of the Poisson’s
equation to study the convergence
， 、 一 ， 、 二，一 、 一，一、
μθ(X)- ∏θμθ(x) = H(θ, X)- h(θ),
where μe(∙) is a function on X. The solution μe(x) to the Poisson,s equation exists and is formulated
in the following form when the above series converges:
μθ(x) ：= X ∏θ(H(θ, x) — h(θ)),
k≥0
where Πkθ(H (θ, X) - h(θ)) = (H (θ, y) - h(θ))Πkθ(X, dy). To ensure such a convergence,
Benveniste et al. (1990) made the following regularity conditions on the solution μe(∙) of the
Poisson’s equation:
There exist a Lyapunov function V : X → [1, ∞) and a positive constant C > 0 such that ∀θ, θ0 ∈ Θ,
we have
k∏θμθ(x)k ≤ CV(x),	k∏θμθ(X)- ∏θ*Me，(x)k ≤ C∣∣θ - θ0∣∣V(x),	E[V(x)] ≤ ∞, (12)
where a common choice for the Lyapunov function is to set V(X) = 1 + kXk2 (Teh et al., 2016;
Vollmer et al., 2016).
A.3 Gaussian diffusions
Consider a stochastic linear differential equation
dUt = he (θt)Utdt + R1r2(θt)dWt,	(13)
where U is a m-dimensional random vector,	he	:=	焉 h(θ),	R(θ)	:=
P∞=-∞ Cove(H(θ, Xk), H(θ, xo)) is a positive definite matrix that depends on θ(∙), W ∈ Rm
14
Published as a conference paper at ICLR 2022
is a standard Brownian motion. Given a large enough t such that θt converges
sufficiently fast, we may write the diffusion associated with Eq.(13) as follow
to a fixed point θb?
Ut ≈ e-th"θ*α +
Zt
0
e-(t-s)hθ (θb* )
.^ .
◦ R(θb?)dWs,
(14)
C	.ι ..ι	. ∙ 7 ∕* ∖ ∙	J	-T τ	∙ τ , ∙ι	, zɔ	∙
Suppose that the matrix hθ(θ?) is negative definite, then Ut converges in distribution to a Gaussian
variable
,7 /C \
E[Ut] = e-thθ(θ*)U0
Var(Ut) =	te
0
O
O
. ，公、	- ，公 、
thθ (θ*) ◦ R ◦ ethθ (θ*)du.
The main goal of this supplementary file is to study the Gaussian approximation of the process
-1/2
ωk	(θk - θ?) to the solution Eq.(14) for a proper step size ωk. Thereafter, the advantage of
interacting mechanisms can be naturally derived.
B S tab ility and convergence analysis
As required by the algorithm, we update P contour stochastic gradient Langevin dynamics (CSGLD)
simultaneously. For the notations, we denote the particle of the p-th chain at iteration k by x(kp) ∈ X ⊂
Rd and thejoint state of the P parallel particles at iteration k by XN P := (Xk1), xk2),…，XkP)) ∈
X P ⊂ RdP. We also denote the learning rate and step size at iteration k by k and ωk, respectively.
We denote by N (0, IdP) a standard dP -dimensional Gaussian vector and denote by ζ a positive
hyperparameter.
B.1 ICSGLD algorithm
First, we introduce the interacting contour stochastic gradient Langevin dynamics (ICSGLD) with P
parallel chains:
(1)	Simulate xN+P = XN P - ekVxL(XN P, θfc) + N(0, 2ekTIdP),
(2)	Optimize θk+1 = θk + ωk+1Hf(θk, XkN+P1 ),
(S1)
(S2)
where VxL(xn P, θ) := (VxL(X⑴,θ), VxL(X⑵,θ),…，VxL(X(P), θ))>, VxL(x, θ) is the
stochastic adaptive gradient given by
N ζτ
VxL(χ, θ) = — 1 + ∆u (log 队JU(X))Togθ((Ju(X)T) ∨ 1)) VxU(x).	(15)
'---------------------{z--------------------}
gradient multiplier
In particular, the interacting random-field function is written as
1P
H(θk, X鲁 P ) = P EH (θk, xkp+1),	(16)
p=1
where each random-field function H(θ, X) = (H1 (θ, X), . . . , Hm(θ, X)) follows
Hei(θ,X) =θ(JUe(X)) (1i=JUe(x) - θ(i) , i= 1,2,...,m.	(17)
Here JU(x) denotes the index i ∈ {1,2,3,…，m} such that Ui_1 < N U(x) ≤ Ui for a set of
energy partitions {ui}im=0 and U(X) = i∈B Ui(X) where Ui denotes the negative log of a posterior
based on a single data point i and B denotes a mini-batch of data of size n. Note that the stochastic
15
Published as a conference paper at ICLR 2022
J . T^T / ∖	1 .	1 ∙	1	「 ，1	∙ 1 T / ∖ 1	1 ∙
energy estimator U (X) results in a biased estimation for the partition index JUe (X) due to a non-linear
transformation. To avoid such a bias asymptotically with respect to the learning rate k, we may
consider a variance-reduced energy estimator UVR(X) following Deng et al. (2021a)
NUVR(X) = N X (Ui(X)- Ui (Xqbq」))+ XUi (Xqbqc),
(18)
where the control variate Xqb k C is updated every q iterations.
Compared with the naive parallelism of CSGLD, a key feature of the ICSGLD algorithm lies in
NP
the joint estimation of the interacting random-field function H(θ, X P) in Eq.(16) for the same
mean-field function h(θ).
B.1.1	Discussions on the hyperparameters
The most important hyperparameter is ζ . A fine-tuned ζ usually leads to a small or even slightly
negative learning rate in low energy regions to avoid local-trap problems. Theoretically, ζ affects the
L2 convergence rate hidden in the big-O notation in Lemma 3.
The other hyperparameters can be easily tuned. For example, the ResNet models yields the full loss
ranging from 10,000 to 60,000 after warm-ups, we thus partition the sample space according to the
energy into 200 subregions equally without tuning; since the optimization of SA is nearly convex,
tuning {ωk} is much easier than tuning {k} for non-convex learning.
B.1.2	Discussions on distributed computing and communication cost
In shared-memory settings, the implementation is trivial and the details are omitted.
In distributed-memory settings: θk+1 is updated by the central node as follows:
•	The p-th worker conducts the sampling step (S1) and sends the indices JUe(x(p) )’s to the central
node;
•	The central node aggregates the indices from all worker and updates θk based on (S2);
•	The central node sends θk+1 back to each worker.
We emphasize that we don’t communicate the model parameters X ∈ Rd, but rather share the self-
adapting parameter θ ∈ Rm, where m d. For example, WRN-16-8 has 11 M parameters (40 MB),
while θ can be set to dimension 200 of size 4 KB; hence, the communication cost is not a big issue.
Moreover, the theoretical advantage still holds if the communication frequency is slightly reduced.
B.1.3 Scalability to big data
Recall that the adaptive sampler follows that
Xk+1 = Xk - ek+1 N [l + ζτlogθk(JU(Xkk) - lo∆^(JU(Xk) - 1 ∨ 1 ] VxU(Xk) + P2T1k+7wk+ι,
S-------------------------------------------------{z------------------------}
gradient multiplier
The key to the success of (I)CSGLD is to generate sufficiently strong bouncy moves (negative gradient
multiplier) to escape local traps. To this end, ζ can be tuned to generate proper bouncy moves.
Take the CIFAR100 experiments for example:
•	the self-adjusting mechanism fails if the gradient multiplier uniformly “equals” to 1 and a too
small value of ζ = 1 could lead to this issue;
•	the self-adjusting mechanism works only if we choose a large enough ζ such as 3e6 to generate
(desired) negative gradient multiplier in over-visited regions.
16
Published as a conference paper at ICLR 2022
However, when we set ζ =3e6, the original stochastic approximation (SA) update proposed in (Deng
et al., 2020b) follows that
θk+1(i) = θk (i) + ωk + 1 θk (JU (Xk+1)) (Ii=JU (xk+ι) - θk ⑴).
{^^^^^^^∙
∙•^^^^^^{^^^^^^≡"^
essentially 0 for ζ1
Since θ(i) < 1 for any i ∈ {1,…，m}, θ(i)ζ is essentially 0 for such a large Z, which means that
the original SA fails to optimize when ζ is large. Therefore, the limited choices of ζ inevitably
limits the scalability to big data problems. Our newly proposed SA scheme
θk+1 (i) = θk(i) + ωk+1 θk (JU (xk +1)) (Ii=JU (xk+ι) - θk ⑴)
independent of ζ
is more independent of Z and proposes to converge to a much smoother equilibrium θ∞∞ζ instead
U(X)
of θ∞, where θ∞(i) = JX π(x)dx α JX e--丁dx is the energy PDF. As such, despite the linear
stability is sacrificed, the resulting algorithm is more scalable. For example, estimating e-10,000× 1 is
numerically much easier than e-10,000 for a large Z such as 10, 000, where 10, 000 can be induced
by the high losses in training deep neural networks in big data.
B.2	Assumptions
A long-standing problem for stochastic approximation is the difficulty in establishing the stability
property and a practical remedy for this problem is to study Θ on a fixed compact set.
Assumption A1 (Compactness) The space Θ is compact and infΘ θ(i) > 0 for any i ∈
{1, 2, . . . , m}.
For weaker assumptions, we refer readers to Theorem 3.2 (Fort et al., 2015), where a recurrence prop-
erty can be proved for the Metropolis-based Wang-Landau algorithm, which eventually established
that the estimates return to a desired compact set often enough.
Next, we lay out the smoothness assumption, which is standard in the convergence analysis of SGLD,
see e.g. Mattingly et al. (2010), Raginsky et al. (2017) and Xu et al. (2018).
Assumption A2 (Smoothness) U (x) is M -smooth when there exists a positive constant M that
satisfies ∀X, X0 ∈ X,
INxU(x) - VxU(x0)k ≤ MIlX - x0k.	(19)
In addition, we assume the dissipativity condition to ensure that the geometric ergodicity of the
dynamical system holds. This assumption is also crucial for verifying the solution properties of
the solution of Poisson’s equation. Similar assumptions have been made in Mattingly et al. (2010);
Raginsky et al. (2017) and Xu et al. (2018).
Assumption A3 (DissipatiVity) There exist constants m > 0 and b ≥ 0 that satisfies ∀x ∈ X and
any θ ∈ Θ,
(VχL(x, θ), Xi ≤ b — m∣∣x∣∣2.	(20)
To further establish a bounded second moment on X ∈ X with respect to a proper Lyapunov function
V (X), we impose the following conditions on the gradient noise:
Assumption A4 (Gradient noise) The stochastic gradient based on mini-batch settings is an unbi-
ased estimator such that
E[VxUe(Xk) - VxU(Xk)] = 0;
furthermore, for some positive constants M and B, we have
E[IVxUe(Xk) - VxU(Xk)I2] ≤ M2IXI2 +B2,
where E[∙] acts on the distribution ofthe noise in the stochastic gradient VxU(Xk).
17
Published as a conference paper at ICLR 2022
B.3	Local stability via the scalable random-field function
Now, we are ready to present our first result. Lemma 3 establishes a local stability condition for the
non-linear mean-field system of ICSGLD, which implies a potential convergence of θk to a unique
fixed point that adapts to a wide energy range under mild assumptions.
Lemma 3 (Local stability, restatement of Lemma 1) Assume Assumptions A1-A4 hold. Given any
〜
〜
small enough learning rate , a large enough m and batch size n, and any θ ∈ Θ, where Θ is a small
2
neighborhood of θ? that contains θ?, we have hh(θ), θ 一 θ？ ≤ —φ∣∣θ — θ*∣∣2, where θ? = θ?+O(ε),
ε = O (SuPX Var(ξn(x)) + C + m1) and θ?
1
(RXI π(x)dx) Z
(RXm π(X)dX) 1
Pm=I (RXk π(x)dx) ζ	Pm=I (RXk π(x)dx) ζ
φ = infθ mini Z-1(i)(1 — O(ε)) > 0, Zbζ,θ(i) is defined below Eq.(28), and ξn(x) denotes the noise
in the energy estimator U(x) ofbatch Size n and Var(∙) denotes the variance.
Proof The random-field function Hi(θ, x) = θ(JUe (x)) 1i=J e (X) — θ(i) based on the stochastic
energy estimator U(x) yields a biased estimator of Hi(θ, x) = θ(J(x)) (li=j(X)— θ(i)) for any
i ∈ {1,2,..., m} based on the exact energy partition function J(∙). By Lemma.4, We know that the
bias caused by the stochastic energy is of order O(Var(ξn(x))).
Now we compute the mean-field function h(θ) based on the measure $e(x) simulated from SGLD:
hi(θ)= H Hi(θ, x)$e(x)dx = H Hi(θ, x)$e(x)dx + O (Var(ξn(x)))
XX
/
、
Hi(θ, x)
X
$Ψ θ
V----
I1
(x) -$
(x) + $田8 (x) -$Ψ8 (x) + $8 (x)
}--------------V------------
I2 :piece-wise approximation
------------------V---------------'
I3 :numerical discretization
dx + O (Var(ξn (x))) ,
(21)
where $e is the invariant measure simulated via SGLD that approximates $田® (x). $田@ (x) and
$ψθ (x) are two invariant measures that follow $田@ (x) H 田《；盘))and $ψθ (x) H 市《；氏));
Ψθ(u) and Ψθ(u) are piecewise continuous and constant functions, respectively
m
Ψθ(U) = X (θ(k — 1)e(logθ(k)igθ(kT))U-UU-) 1uk-ι<u≤uk
k=1
m
Ψeθ(u)=	θ(k)1uk-
1 <u≤uk .
k=1
(22)
(i) For the first term I1, we have
LHg χ)wψ θ (χ)dχ
Z+1θ Xx θ(J (X))(Ii=J(X)—θ(i)) θζ∏JχX))dx
Z⅛ X ZXkQiz θζ¾dx
1
Zζ+1,θ
mm
X L θζτ(k)ιk=idx - θ(i) X fXk
k=1 xk	k=1 xk
π(X)
θζ-1(k)
(23)
dx
1
Zζ+1,θ
JXin(X)dx
θζ-1(i)
— θ(i)Zeζ,θ
where Zeζ+ι,θ = Pk=I 与：((X)。乂 denotes the normalizing constant of $市@ (x).
18
Published as a conference paper at ICLR 2022
X π(x)dx
The solution θ? that solves 缸乂卜)-θ(k)Z<,θ = 0 for any k ∈ {1,2,…，m} satisfies θ*(k)
1
RXZn(X)dx) . Combining the definition of Zζ,θ? = Pm=I RXrx)dx, We have
Z = Xm JXk π(X)dx = XX	JXk π(X)dx
k=1	θZ 1(k)	k = 1 R RXfc π(x)dx∖ Z
Ze	Zeζ,θ?)
Z⅛1 Xm	JXk π(χ)dx
ZZ,θ* 工 -	√-1
k=1 (RXkn(X)dx)
1
ζ-1 Xm
Zζ,θ* X (I π
k=1	Xk
1 ∙	1	1	1	^Γ7
Which leads to Zζ,θ?
π(x)dx) Z )
. In other Words, the mean-field system Without
perturbations yields a unique solution θ*(i)
/	、ι
(('π(x)dx) Z
——-i-------1——1 for any i ∈ {1, 2,…，m}.
Pm=I (RXk π(X)dx)ζ
(ii)	For the second term I2, We have
(Hi(θ, x)(-$eeθ(x) + $We(X))dx = O ()
(24)
Where the result folloWs from the boundedness ofH (θ, X) in (A1) and Lemma B4 (Deng et al.,
2020b).
(iii)	For the last term I3, folloWing Theorem 6 of Sato & NakagaWa (2014), We have for any fixed θ,
H Hi(θ, x) (-$Ψθ (x) + Be(x)) dx = O(e).
X
(25)
Plugging Eq.(23), Eq.(24) and Eq.(25) into Eq.(21), We have
,	，一、 Ο≤ -i
hi(θ) = Zζ+1,θ
π(x)dx
εβi(θ) +	θζ-i(i)-----θ(i)Zeζ,θ
Z-1	Zζ,θ*
ζ+1,θθZ-1(i)
εβi(θ)…
Zζ,θ*
(26)
+广-θζ⑶Ze
Z-1	Zζ,θ*
ζ+1,θ θZ-1(i)
θζ-1 (i)
εβi(θ) ɪɪ2+ θ? (i)-(θ(i)Cθ )Z
Zζ,θ*
where βi(θ) is a bounded term such that Z-+ι,θεβi(θ) = O (Var(ξn(x)) + e + mm), ε =
O(SUpx Var(ξn(x)) + e + mm) and Cθ = (ZZζθ-) Z. By the definition of Zeζ,θ = Pm=I RXzlxkdx,
ZZ,θ*
when ζ = 1, Cθ ≡ 1 for any θ ∈ Θ, which suggests that the stability condition doesn’t
rely on the initialization of θ; however, when ζ 6= 1, Cθ 6= 1 when θ 6= θ?, we see that
hi(θ) α θ*(i)ζ - (θ(i)Cθ)ζ + perturbations is a non-linear mean-field system and requires a
proper initialization of θ ∈ Θ.
For any θ ∈ Θ ⊂ Θ being close enough to θ? , there exists a Lipschitz constant Lθe =
supi≤m θ三θ ∣θCθj-C*∣ < ∞. By Ce* = 1, θ(i) ≤ 1, and mean value theorem for some
θ(i) ∈ [θ(i),θ*(i)], wehave
∣θZ(i) - (θ(i)Cθ)ζ I = Z(Z(i)Ce)ζ-M(i) - θ(i)Cθ|
=Z(Z(i)Ce)ζ-1∣θ*(i) - θ(i) + θ(i)Ce* - θ(i)CeI
≤ Z (Z(i)Ce)ζ-1∣θ? (i) - θ(i)I + θ(i)∣Ce* - Ce I
≤ Z (Z(i)Ce)ζ-1(i + Le)∣θ*(i) - θ(i)∣,	(27)
19
Published as a conference paper at ICLR 2022
Combining Eq.(26) and Eq.(27), we have
hi(θ) = Z-+ι,θθZ-¾ 卜βi(θ)
θζ-1(i)
Zeζ,θ?
+ θ? (i)-(θ(i)Cθ )ζ
(28)
^	1	-	,,,,	,,	,,,,
Z-θ(i) [εβi(θ)+ θ*(i)-θ(i)],
1
where Zζ-,θ1(i)
~	1	~
ZZ+ι,θ Zζ,θ?
Z(MCe)zT(1+L∕θζτ⑴:βi(θ) is some bounded term such that 氏(θ) ≤
βi(θ)θζ-1(i)
ζ(θe(i)Cθe)ζ-1(1+Lθe)Zeζ,θ?
; Cθe
ZZθ?) ; Lθ = supi≤m,θ∈Θ ∣θCθ"C⅛ < ∞.
Next, we apply the perturbation theory to solve the ODE system with small disturbances (Weinhart
et al., 2010) and obtain the equilibrium θ?,
1	C，公、，八	a	, ,1	C	」	7 /八、	一.
where εβ(θ?) + θ? - θ? = 0, to the mean-field equation hi(θ) such that
^
hi(θ) = Z
-1
ζ,θ(i)
-1
ζ,θ(i)
[εβi(θ) + θ*(i) - θ(i)]
卜βi(θ) - εβi(θ?) + εβi(θ?) + θ*(i)-。⑶]
-1
ζ,θ(i)
O(ε)(θ(i)
-θ?H)) + θ?
(i) - θ(i)
(29)
-1
ζ,θ(i)
where a smoothness condition clearly holds for the β(∙) function t. Given a positive definite
Lyapunov function V(θ) = 11 ∣∣θ? - θ∣∣2, the mean-field system h(θ) = Z-θ(i)(εβ(θ) + θ? - θ)=
-1
ζ,θ(i)
/r z/ʌ /	∖ ∖ / Λ^	Λ∖	∙	_ Γ-ι rʌ	1	,1	11	,
(1 - O(ε))(θ? - θ) for i ∈ {1, 2,…，m} enjoys the following property
... ... . ^.
hh(θ), VV(θ)i = hh(θ), θ - θ*i
≤-miin Z-1(i)(l -O(ε))kθ - b?『
≤ -φkθ - b*k2,
where φ = infθ min Z-1(i)(1 - O(ε)) > 0 given the compactness assumption A1 and a small
enough ε = O (SuPx Var(ξn(x)) + e + /).	■
Remark 1 The newly proposed random-field function Eq.(17) may sacrifice the global stability
by including an approximately linear mean-field system Eq.(28) instead of a linear stable system
(see formula (15) in Deng et al. (2020b)). The advantage, however, is that such a mechanism
facilitates the estimation of θ?. We emphasize that the original energy probability in each partition
X π(x)dx km=1 (Deng et al., 2020b) may be very difficult to estimate for big data problems. By
contrast, the estimation of {(JXk π(x)dx)Z }m==1 becomes much easier given a proper ζ > 0.
Technical lemmas
Lemma 4 The stochastic energy estimator U(x) leads to a controllable bias in the random-field
function.
. 一~. 、一 . .. . . ...
∣E[Hi(θ, x)] - Hi(θ, x)| = O(Var(ξn(x))),
where the expectation E[∙] is taken with respect to the random noise in the stochastic energy estimator
of U (∙).
Proof Denote the noise in the stochastic energy estimator by ξ(x), such that U(∙) = U(∙) +
.. / . . ∖ . .
ξ(∙). Recall that Hi(θ, x) = θ(JU (X))(Ii=JU 的-
and JU (x) ∈ {1, 2,…，m} satisfies
，A small change of θ won't significantly affect the perturbations caused by Eq.(24), Eq.(25) and Var(ξn(∙)).
^
Z
^
Z
^
Z
^
^
^
Z
20
Published as a conference paper at ICLR 2022
N
UJe(χ)-ι < NnU(x) ≤ UJe3)for a set of energy partitions {ui}m=0. We can interpret Hi(θ, x) as a
non-linear transformation Φ that maps U(x) to (0, 1). Similarly, Hi(θ, x) maps U(x) to (0, 1). In
what follows, the bias of random-field function is upper bounded as follows
∣E[Hi(θ, x)] - Hi(θ, x)| = I / Φ(U(x) + ξ(x)) - Φ(U(x))dμ(ξ(x))
/ ξ(x)Φ0(U (x)) + 亨 Φ00(u)dμ(ξ(x))
≤
/ ξn (x)Φ0(U(x))dμ(ξn(x)) +
φ-(u)- / ξn(X)2dμKn(X))
≤ O(Var(ξn(x))) ,
where μ(ξ(x)) is the probability measure associated with ξ(x); the second equality follows from
Taylor expansion for some energy U and the third equality follows because the stochastic energy
estimator is unbiased; Φ0(U(x)) = O( θ(j(x))-θUJ(X'T)) is clearly bounded due to the definition of
θ; a similar conclusion also applies to Φ00(∙). The last inequality easily follows by applying Cauchy
Schwarz inequality.
B.4 Convergence of the self-adapting parameters
The following is a restatement of Lemma 3.2 of Raginsky et al. (2017), which holds for any θ in the
compact space Θ.
Lemma 5 (Uniform L2 bounds) Assume Assumptions A1, A3 and A4 hold. We have a bounded
second moment supk≥1 E[kxk k2] < ∞ given a small enough learning rate.
The following lemma justifies the regularity properties of Poisson’s equation, which is crucial in
controlling the perturbations through the stochastic approximation process. The first version was
proposed in Lemma B2 of Deng et al. (2020b). Now we give a more detailed proof by utilizing a
Lyapunov function V (x) = 1 + x2 and Lemma 5.
Lemma 6 (Solution of Poisson’s equation) Assume that Assumptions A1-A4 hold. There is a solu-
tion μθ(∙) on X to the Poisson's equation
μθ(X)- ∏θμθ(x) = H(θ, X)- h(θ)∙	(30)
Furthermore, there exists a constant C such that for all θ, θ0 ∈ Θ
E[k∏θμθ(x)k] ≤ c,
E[k∏θμθ(x) - ∏θ0μθ0(x)k] ≤ C∣∣θ - θ0k∙	()
Proof The existence and the regularity property of Poisson’s equation can be used to control the
perturbations. The key of the proof lies in verifying drift conditions proposed in Section 6 of Andrieu
et al. (2005).
(DRI) By the smoothness assumption A2, we have that U(x) is continuously differentiable almost
everywhere. By the dissipative assumption A3 and Theorem 2.1 (Roberts & Tweedie, 1996), we
can show that the discrete dynamics system is irreducible and aperiodic. Now consider a Lyapunov
function V = 1 + kxk2 and any compact subset K ⊂ Θ, the drift conditions are verified as follows:
(DRI1) Given small enough learning rates {k}k≥1, the smoothness assumption A2, and the dissipa-
tive assumption A3, applying Corollary 7.5 (Mattingly et al., 2002) yields the minorization condition
for the CSGLD algorithm, i.e. there exists η > 0, a measure ν, and a set C such that ν(C) = 1.
Moreover, we have
Pθ∈K(x, A) ≥ ην(A) ∀A ∈ X,x ∈ C∙	(I)
1	∣∣y — x+e Vχ Le (x,θ)k2
where Pθ(x, y) := .√上尸泣E[e	4≡	|x denotes the transition kernel based on CS-
GLD with the parameter θ ∈ K and a learning rate , in addition, the expectation is taken over the
21
Published as a conference paper at ICLR 2022
adaptive gradient VχL(x, θ) in Eq.(15). Using Assumption A1-A4, We can prove the uniform L2
upper bound by following Lemma 3.2 (Raginsky et al., 2017). Further, by Theorem 7.2 (Mattingly
et al., 2002), there exist α ∈ (0,1) and β ≥ 0 such that
.. .. ~	..
Pθ∈κV(χ) ≤ αv(x) + β.	(II)
Consider a Lyapunov function V = 1 + ∣∣xk2 and a constant K = α + β, it yields that
Pθ∈KV(x) ≤ κV (x).	(III)
NoW We have verified the first condition (DRI1) by checking conditions (I),(II), and (III),
(DRI2) In What folloWs, We check the boundedness and Lipshitz conditions on the random-field
function He(θ, x), Where each subcomponent is defiend as Hei(θ, x) = θ(JUe (x)) 1i=J e (x) - θ(i) .
Recall that V = 1 + ∣x∣2 , the compactness assumption A1 directly leads to
sup ∣H (θ, x)∣ ≤ mV (x).	(IV)
θ∈K⊂[0,1]m
For any θ1, θ2 ∈ K and a fixed x ∈ X, it suffices for us to solely verify the i-th index, Which is the
index that maximizes ∣θι(i) 一 θ2(i)∣, then
也(θι, x) - Hi(θ2, x)| = θι(Ju(x)) (Ii=JU(x) -θι(i)) - θ2(Ju(x)) (Ii=JU(x) -θ2(i))
≤ 仇(JU(x)) - θ2(Ju(x))l + 仇(JU(x))θι(i) - θ2(Ju(x))θ2(i)∣
≤ max (优(j) - θ2(j)| + θι(j)∣θι(i) - θ2(i)l + 仇(j) - θ2(j)∣θ2(i))
≤ 3∣θι(i) - θ2(i)∣,
Where the last inequality holds since θ(i) ∈ (0, 1] for any i ≤ m.
(DRI3) We proceed to verify the smoothness of the transitional kernel Pθ(x, y) With respect to θ.
For any θ1 , θ2 ∈ K and fixed x and y, We have
∣Pθι (x, y) - Pθ2 (x, y)|
1
1
2∖∕ (4πe)d/2
E e-
_ ~ . ...
∣∣y-x + e^χL(x,θι) k
2
|x -
2∖∕ (4πe)d/2
E e-
_ ~ . ...
∣∣y-x + e^χL(x,θ2) k
2
|x
.Iky - X + eVχL(x, θ1)k2 - ky - X + eVχL(x, θ2)∣∣2∣
.. ~ . ~ . ...
kVxLe(x,θ1) - VxLe(x, θ2)k
kθ1 - θ2k,
Where the first inequality (up to a finite constant) folloWs by kex - ey k . kx - yk for any x, y in a
compact space; the last inequality folloWs by the definition of the adaptive gradient in Eq.(15) and
k log(x) - log(y)k . kx - yk by the compactness assumption A1.
For f : X → Rd, define the norm ∣f ∣ v = suPχ∈χ V((X))I. Following the same technique proposed
in Liang et al. (2007) (page 319), We can verify the last drift condition
∣Pθιf - Pθ2f kv ≤ CkfkVkθι - θ2k, Vf e LV := {f : X → Rd, kf kv < ∞}.	(VI)
Having conditions (I), (II), ∙一 and (VI) verified, we are now able to prove the drift conditions
proposed in Section 6 of Andrieu et al. (2005).	■
Before we present the L2 convergence of θk, we make some extra assumptions on the step size.
Assumption A5 (Learning rate and step size) The learning rate {ek}k∈N is a positive non-
increasing sequence of real numbers satisfying the conditions
lim ek = 0,
k
∞
ek = ∞.
k=1
22
Published as a conference paper at ICLR 2022
The step size {ωk}k∈N is a positive non-increasing sequence of real numbers such that
∞∞
lim ωk = 0, ωk = +∞,	ωk2 < +∞.	(32)
k→∞
k=1	k=1
A practical strategy is to set ωk := O(k-α) to satisfy the above conditions for any α ∈ (0.5, 1].
The following is an application of Theorem 24 (page 246) (Benveniste et al., 1990) given stability
conditions (Lemma 3).
Lemma 7 (L2 convergence rate, restatement of Lemma 2) Assume Assumptions A1-A5 hold.
For any θ0 ∈ Θ ⊂ Θ, a large m, small learning rates {k}k∞=1, and step sizes {ωk}k∞=1, {θk}k∞=0
converges to θ?, where θ? = θ? + O (SuPx Var(ξn(x)) + supk≥k° e《+ ml) for some ko, such that
E [kθk - b?k2i = O (ωk).
The theoretical novelty is that we treat the biased θ? as the equilibrium of the continuous system
instead of analyzing how far we are away from θ? in all aspects as in Theorem 1 (Deng et al., 2020b).
This enables us to directly apply Theorem 24 (page 246). Nevertheless, it can be interpreted as a
special case of Theorem 1 (Deng et al., 2020b) except that there are no perturbation terms and the
equilibrium is θb? instead of θ
?.
C Gaus sian approximation
C.1 Preliminary: sufficient conditions for weak convergence
-1/2
To formally prove the asymptotic normality of the stochastic approximation process ωk (θk - θ?),
we first lay out a preliminary result (Theorem 1 of Pelletier (1998)) that provides sufficient conditions
to guarantee the weak convergence.
Lemma 8 (Sufficient Conditions) Consider a stochastic algorithm as follows
θk+1 = θk + ωk+1h(θk) + ωk+1νek+1 + ωk+1ek+1,
where νek+1 denotes a perturbation and ek+1 is a random noise. Given three conditions (C1), (C2),
and (C3) defined below, we have the desired weak convergence result
(33)
where Σ
0∞ ethθ? ◦ R ◦ ethθ>? dt, R denotes the limiting covariance of the martingale
limk→∞
Im Γ	T I -τ- 1	7 k ∙ ,τ	i ι r∙ ,ι	,	,	∙,	,∙	7 τ	7	∕* ∖ , GT
E[ek+1e>+1 |Fk] and Fk is the σ-algebra ofthe events up to iteration k, he? = he (θ*)+ξI,
ξ = limk→∞
0.5	0.5
ωk -ωk+1 t
T7T.5	∙
ωk
×
(C1) There exists an equilibrium point θ? and a stable matrix he? := he(θ?) ∈ Rm×m such that for
any θ ∈ {θ : ∣∣θ — θ*k ≤ M} for some M > 0, the mean-fieIdfunctiOn h : Rm → Rm satisfies
.^ .
h(θb?) = 0
kh(θ) — hθ? (θ — b?)k . kθ — b?k2,
(C2) The step size ωk decays with an order α ∈ (0, 1] such that ωk = O(k-α).
(C3) Assumptions on the disturbances . There exists constants M > 0 and αe > 2 such that
E [ek+ι∣Fk ] 1{ke-b*k≤f} = 0,
SuPE [kek+ι∣∣α|Fk] 1{∣∣e-b*k≤f} < ∞,
E ωk- kνek+1k 1{ke-eb? k≤Mf} → 0,
E [ek+iek+i|Fk] 1{∣∣e-b*k≤f} → R.
tFor example, ξ = 0 if ωk = O(k-α), where α ∈ (0.5,1] and ξ = k0 if ωk = k0-.
(I1)
(I2)
(II)
(III)
23
Published as a conference paper at ICLR 2022
Remark 2 By the definition of the mean-field function h(θ) in Eq.(26), it is easy to verify the
condition C1. Moreover, Assumption A5 also fulfills the condition C2. Then, the proof hinges on the
verification of the condition C3.
C.2 Preliminary: convergence of the covariance estimators
In particular, to verify the condition E ek+1ek>+1|Fk 1{kθ-θb k≤Mf} → R, , we study the con-
vergence of the empirical sample mean E[f (xk)] for a test function f to the posterior expectation
f = JX f (x)$b* (x)(dx). Poisson's equation is often used to characterize the fluctuation between
f(x) and f:
Lg(X) = f(x) - f,	(34)
where L refers to an infinitesimal generator and g(x) denotes the solution of the Poisson’s equation.
Similar to the proof of Lemma 6, the existence of the solution of the Poisson’s equation has been
established in (Mattingly et al., 2002; Vollmer et al., 2016). Moreover, the perturbations of E[f(xk)] -
f are properly bounded given regularity properties for g(x), where the 0-th, 1st, and 2nd order of the
regularity properties has been established in Erdogdu et al. (2018).
The following result helps us to identify the convergence of the covariance estimators, which is
adapted from Theorem 5 (Chen et al., 2015) with decreasing learning rates {k}k≥1. The gradient
biases from Theorem 2 (Chen et al., 2015) are also included to handle the adaptive biases.
Lemma 9 (Convergence of the Covariance Estimators) Suppose Assumptions A1-A5 hold. For
any θ0 ∈ Θ ⊂ Θ, a large m, small learning rates {k}k∞=1, step sizes {ωk}k∞=1 and any bounded
function f, we have
[f(xk)] -
X
f (x)$b (x)dx
→ 0,
where $b* (x) is the invariant measure simulated via SGLD that approximates $ψff (x) H
π(X)
θ? (J(χ)),
Proof We study the single-chain CSGLD and reformulate the adaptive algorithm as follows:
xk+1 = Xk ― Ck VxL(Xk, θk ) + N(0, 2tkTI)
=Xk - Ck (VxL(Xk, b?) + Y(Xk, θk)) + N(0, 2ckτI),
where VxL(X, θ) = Nn [l + 怎(log θ(J(x)) - log θ((J(x) - 1) ∨ 1))] VxU(x) *, VxL(X, θ) is
defined in Section B.1 and the bias term is given by Υ(Xk, θk) = VxL(Xk, θk) - VxL(Xk, θ?).
Then, by Jensen’s inequality and Lemma 7, we have
.. . , ... ~, ~. ^ ...,
kE[Y(xk, θk)]k ≤ E[kVxL(χk, θk) - VxL(xk, θ*)k]
.E[kθk - θ*k] ≤ √E[kθk - θ*k2] ≤ O (√ωk).
(35)
Combining Eq.(35) and Theorem 5 (Chen et al., 2015), we have
[f(Xk)] -
X
f (x)$b (x)dx
1	. Pk=I ωikE[Y(Xi, θi)]k
∑k	k
i Ci	i ωi
+
→ 0, as k → ∞,
where the last argument directly follows from the conditions on learning rates and step sizes in
Assumption A5.	■
J(X) = Pi=1 i1Ui-i<U (x)≤Ui , where the exact energy function U (x) is selected.
E
E
O
24
Published as a conference paper at ICLR 2022
C.3 Proof of Theorem 1
Recall that the stochastic approximation based on a single process follows from
θk+1
θk + ωk+1H(θk, xk+1)
二 θk + ωk+1h(θk) + ωk+1 (μθk (Xk+1) - □θkMθk (Xk+1))
θk + ωk+1h(θk)
+ ωk+1 ( 口。七+1 μθk+ι (xk + l) - πθkμθk (Xk+ 1)+ —k+2-k+1 口。k+1 小。七+1 (Xk+ 1)
ωk+1
'--------------------------------7-----------------------------
νk+1
+ ωk+1 \ —— fωk+1πθk μθk (Xk ) - ωk+2πθk+ι μθk+ι (Xk+1)) + Mθk (Xk+1)- 口。七 Mθk (Xk )
ωk+1	S---------------------}
|	{z	/	ek+1
ςk+1
θk + ωk+1h(θk )+ ωk+1 (Vk+1 + ςk+1) +ωk+1 ek + 1 ,
1---------7-----------}
perturbation
martingale
(36)
where the second equality holds from the solution of Poisson’s equation in Eq.(30).
We denote θk = θk + —k+1∏θkμθk (xQ. Adding —k+2∏θk+1 祖。卜+\ (Xk+1) on both sides ofEq.(36),
we have
θk+1
=θk + ωk+1h(θk ) + ωk+1 (Vk+1 + ek + 1 + ςk+1) + —左+2口。忆十1 Mθk+ι (Xk+1) - ωk + 1πθk Mθk (Xk )
=θk + ωk+1h(θk ) + ωk+1 (Vk+1 + ek + 1)
=θk + ωk+1h(θk) + ωk+1 (Vk+1 + ek + 1) ,
(37)
where Vk+1 = Vk+1 + h(θk) - h(θk). Next, We proceed to verify the conditions in C3.
(I)	By the martingale difference property of {ek} and the compactness assumption A1, we know that
for any αe > 2
一 r	, 一 r	.	一 r..	, , ≈, , 一 r	，、
E[ek+1∣Fk] = 0,	SUpE[kek+1k |Fk] < ∞.	(I)
k≥0
(II)	By the definition of h(θk) in Eq.(26), we can easily check that h(θk) is Lipschitz continuous
in a neighborhood of θ?. Combining Eq.(31), we have ∣∣h(θk) - h(θk)k = O(∣∣θk - θkk) =
，，，	—	，	、 ，，、	,一，	、一	一 …	，，r	…，，_	"	..	,一，	、	,一，	、一
O(k—k+1∏θkμθk (Xk)k) = O(ωk+1). Then E[∣Vk+1∣] ≤ C∣θk - θk ∣∣ + O(—k+2) = O(—k+1) by
the step size condition Eq.(32). In what follows, we can verify
E [kνk+1k21 ≤ 2E [kνk+1k21 +2E "kh(θk)-h(θk)k2# = O(—k) → 0.	(II)
k	k	k
(In) For the martingale difference noise ek+1 = μθk (Xk+1) 一 ∏θk μθk (Xk) with mean 0, we have
E[ek+1e>+1|Fk] = E[μθk(Xk+1)〃ek(Xk+1)>lFk] - πθ%μe%(Xk)πθ%μe%(Xk)>.
We denote E[ek+1ek>+1 |Fk] by a function f(Xk). Applying Lemma 9, we have
E[ek+1e>+1∣Fk ] = E[f(Xk)] → f f (X)$b dX = lim E[ek+1e>+1 |Fk] ：= R,	(III)
?	k→∞
where R := R(θb?) and R(θ) is also equivalent to Pk∞=-∞ Covθ(H(θ, Xk), H(θ, X0)).
Having the conditions C1, C2 and C3 verified, we apply Lemma 8 and have the following weak
convergence for θk
25
Published as a conference paper at ICLR 2022
where Σ = R0∞ ethθ? ◦ R ◦ ethθ>? dt and hθ?
, z^、	O'. ^	,.
hθ(θ?) + ξI, ξ = limk→∞
0.5	0.5
ωk -ωk + 1
T7τ^5
ωk
Considering the definition that θk = θk + ωk+ι∏θkμθk (Xk) and E[k∏θhμθk (xk)k] is uniformly
bounded by Eq.(31), we have
ω1∕2∏θkμθk (xk) → 0 in probability.
By Slutsky’s theorem, we eventually have the desired result
ω-=2(θk- θ?) ⇒N(0, Σ).
where the step size ωk decays with an order α ∈ (0.5, 1] such that ωk = O(k-α).
26
Published as a conference paper at ICLR 2022
D More on experiments
D.1 Mode exploration on MNIST via the scalable random-field function
For the network structure, we follow Jarrett et al. (2009) and choose a standard convolutional neural
network (CNN). Such a CNN has two convolutional (conv) layers and two fully-connected (FC) layers.
The two conv layers has 32 and 64 feature maps, respectively. The FC layers both have 50 hidden
nodes and the network has 5 outputs. A large batch size of 2500 is selected to reduce the gradient noise
and reduce the stochastic approximation bias. We fix ζ = 3e4 and weight decay 25. For simplicity,
We choose 100,000 partitions and ∆u = 10. The step size follows ωk = min{0.01,k0.6+100 }.
D.2 S imulations of multi-modal distributions
The target density function is given by π(x)	α
(x1, x2) and U(x) follows U (x)	=	0.2(x21 + x22)
We also include a regularization term L(x) = I(x2+x2)>20 ×
((x12 + x22) - 20). This design leads to a highly multi-modal
distribution with 25 isolated modes. Figure 5 shows the con-
tour and the 3-D plot of the target density. The ICSGLD
and baseline algorithms are applied to this example. For IC-
SGLD, we set k = 3e-3, τ = 1, ζ = 0.75 and total number
of iterations= 8e4 . Besides, we partition the sample space
into 100 subregions with bandwidth ∆u = 0.125 and set
3k = min(3e-3, k0.6+100).
exp(-U (x)), where x =
- 2(cos(2πx1) + cos(2πx2)).
Figure 5: Target density.
For comparison, we run the baseline algorithms under similar
settings. For CSGLD, we run a single process 5 times of the
time budget and all the settings are the same as those used by ICSGLD. For reSGLD, we run five
parallel chains with learning rates 0.001,0.002,…，0.005 and temperatures 1, 2,…，5, respectively.
We estimate the correction every 100 iterations. We fix the initial correction 30 and choose the same
step size for the stochastic approximation as in ICSGLD. For SGLD, we run five chains in parallel
with the learning rate 3e-3 and a temperature of 1. For cycSGLD, we run a single-chain with 5
times of the time budget. We set the initial learning rate as 1e-2 and choose 10 cycles. For the
particle-based SVGD, we run five chains in parallel. For each chain, we initialize 100 particles as
being drawn from a uniform distribution over a rectangle. The Iearning rate is set to 3e-3.
əuuəoj①>-p T>∣
2.5
2.0
1.5-
ι,o
0.5
5000	10000	15000	20000
Steps
Figure 6: Estimation KL divergence versus time steps for ICSGLD and baseline methods. We repeat
experiments 20 times.
CSGLD xT5
reSGLD xP5
ICSGLD xP5
cycSGLD xT5
SGLD xP5
6	20	40	60	80	100
Time (s)
To compare the convergence rates in terms of running steps and time between ICSGLD and other
algorithms, we repeat each algorithm 20 times and calculate the mean and standard error over 20
trials. Note that we run all the algorithms based on 5 parallel chains (×P5) except that cycSGLD
and CSGLD are run in a single-chain with 5 times of time budget (×T5) and the steps and running
27
Published as a conference paper at ICLR 2022
time are also scaled accordingly. Figure 6 shows that the vanilla SGLD×P5 converges the slowest
among the five algorithms due to the lack of mechanism to escape local traps; cycSGLD×T5 slightly
alleviates that problem by adopting cyclical learning rates; reSGLD×P5 greatly accelerates the
computations by utilizing high-temperature chains for exploration and low-temperature chains for
exploitation, but the large correction term inevitably slows down the convergence; ICSGLD×P5
converges faster than all the others and the noisy energy estimators only induce a bias for the latent
variables and don’t affect the convergence rate significantly.
For the particle-based SVGD method, since more particles require expensive computations while
fewer particles lead to a crude approximation. Therefore, we don’t show the convergence of SVGD
and only compare the Monte Carlo methods.
D.3 Deep contextual bandits on mushroom tasks
For the UCI Mushroom data set, each mushroom is either edible or poisonous. Eating an edible
mushroom yields a reward of 5, but eating a poisonous mushroom has a 50% chance to result in a
reward of -35 and a reward of 5 otherwise. Eating nothing results in 0 reward. All the agents use the
same architecture. In particular, we fit a two-layer neural network with 100 neurons each and ReLU
activation functions. The input of the network is a feature vector with dimension 22 (context) and
there are 2 outputs, representing the predicted reward for eating or not eating a mushroom. The mean
squared loss is adopted for training the models. We initialize 1024 data points and keep a data buffer
of size 4096 as the training proceeds. The size of the mini-batch data is set to 512. To adapt to online
scenarios, we train models after every 20 new observations.
We choose one -greedy policy (EpsGreedy) based on the RMSProp optimizer with a decaying learn-
ing rate (Riquelme et al., 2018) as a baseline. Two variational methods, namely stochastic gradient
descent with a constant learning rate (ConstSGD) (Mandt et al., 2017) and Monte Carlo Dropout
(Dropout) (Gal & Ghahramani, 2016) are compared to approximate the posterior distribution. For the
sampling algorithms, we include preconditioned SGLD (pSGLD) (Li et al., 2016), preconditioned
CSGLD (pCSGLD) (Deng et al., 2020b), and preconditioned ICSGLD (pICSGLD). Note that all the
algorithms run 4 parallel chains with average outputs (×P4) except that pCSGLD runs a single-chain
with 4 times of computational budget (×T4). In particular for the two contour algorithms, we set
ζ = 20 and choose a constant step size for the stochastic approximation to fit for the time-varying
posterior distributions. For more details on the experimental setups, we refer readers to section D in
the supplementary material.
We report the experimental setups for each algorithm. Similar to Table 2 of Riquelme et al. (2018),
the inclusion of advanced techniques may change the optimal settings of the hyperparameters.
Nevertheless, we try to report the best setups for each individual algorithm. We train each algorithm
2000 steps. We initialize 1024 mushrooms and keep a data buffer of size 4096 as the training proceeds.
For each step, we are given 20 random mushrooms and train the model 16 iterations every step for
the parallel algorithms (×P4); we train pCSGLD×T4 64 iterations every step.
EpsGreedy decays the learning rate by a factor of 0.999 every step; by contrast, all the others choose
a fixed learning rate. RMSprop adopts a regularizer of 0.001 and a learning rate of 0.01 to learn the
preconditioners. Dropout proposes a 50% dropout rate and each subprocess simulates 5 models for
predictions. For the two importance sampling (IS) algorithms, we partition the energy space into
m = 100 subregions and set the energy depth ∆u as 10. We fix the hyperrameter ζ = 20. The step
sizes for pICSGLD×P4 and pCSGLD×T4 are chosen as 0.03 and 0.006, respectively. A proper
regularizer is adopted for the low importance weights. See Table 2 for details.
Table 2: Details of the experimental setups.
Algorithm	Learning rate	Temperature	RMSprop	IS	Train	Dropout	-Greedy
EPSGREEDY×P4	5e-7 (0.999)	0	-~YEs~~	NO	16	NO	0.3%
CONSTSGD×P4	1e-6	0	NO	NO	16	NO	NO
DROPOUT×P4	1e-6	0	NO	NO	16	YEs (50%)	NO
PCSGLD×T4	5e-8	0.3	Yes	Yes	64	NO	NO
PSGLD×P4	3e-7	0.3	Yes	NO	16	NO	NO
PICS GLD×P4	3e-7	0.3	Yes	Yes	16	NO	NO
28
Published as a conference paper at ICLR 2022
D.4 Uncertainty estimation
All the algorithms, excluding M-SGD×P4, choose a temperature of 0.0003 t. We run the parallel
algorithms 500 epochs (×P4) and run the single-chain algorithms 2000 epochs (×T4). The initial
learning rate is 2e-6 (Bayesian settings), which corresponds to the standard 0.1 for averaged data
likelihood.
We train cycSGHMC×T4 and MultiSWAG×T4 based on the cosine learning rates with 10 cycles.
The learning rate in the last 15% of each cycle is fixed at a constant value. MultiSWAG simulates 10
random models at the end of each cycle. M-SGD×P4 follows the same cosine learning rate strategy
with one cycle.
reSGHMC×P4 proposes swaps between neighboring chains and requires a fixed correction of 4000
for ResNet20, 32, and 56 and a correction of 1000 for WRN-16-8. The learning rate is annealed at
250 and 375 epochs with a factor of 0.2. ICSGHMC×P4 also applies the same learning rate. We
choose m = 200 and ∆u = 200 for ResNet20, 32, and 56 and ∆u = 60 for WRN-16-8. Proper
regularizations may be applied to the importance weights and gradient multipliers for training deep
neural networks.
Variance reduction (Deng et al., 2021a) only applies to reSGHMC×P4 and ICSGHMC×P4 because
they are the only two algorithms that require accurate estimations of the energy. We only update
control variates every 2 epochs in the last 100 epochs, which maintain a reasonable training time and a
higher reduction of variance due to a small learning rate. Other algorithms yield a worse performance
when variance reduction is applied to the gradients.
D.5 Empirical Validation of Reduced Variance
To compare the θ's learned from ICSGLD and CSGLD, we try to simulate from a Gaussian mixture
distribution 0.4N (-6, 1) + 0.6N (4, 1), where N(u, v) denotes a Gaussian distribution with mean u
and standard deviation v. We fix ζ = 0.9 and ∆u = 1. We run ICSGLD with 1,000,000 iterations
based on 10 interacting parallel chains and run CSGLD with
10,000,000 iterations using a single chain. We refer to them
as ICSGLD×P10 and CSGLD×T10, respectively. The rest
of the settings follows from the experimental setup in section
4.1 (Deng et al., 2020a).
To measure the variance of the estimates, we repeated the
experiments 10 times and present the mean and two stan-
dard deviations for both CSGLD×T10 and ICSGLD×P10
in Figure 7. The results indicate that both estimates of θζ
(by CSGLD and ICSGLD) converge to the equilibrium that
approximates the ground truth of the density of states. No-
tably, ICSGLD×P10 yields a significantly smaller variance
than CSGLD×T10, but with the same computational budget.
This shows the clear advantage of ICSGLD (many interact-
ing short runs) over CSGLD (a single long run) in tackling
the large variance issue for importance sampling.
Figure 7: ICSGLD v.s. CSGLD.
tWe use various data augmentation techniques, such as random flipping, cropping, and random erasing
(Zhong et al., 2017). This leads to a much more concentrated posterior and requires a very low temperature.
29