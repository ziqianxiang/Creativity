Published as a conference paper at ICLR 2022
Constrained Physical-Statistics Models
for Dynamical System Identification
and Prediction
Jeremie Dona*1, Marie Dechelle*1, Marina Levy2, Patrick Gallinari13
1Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France
2Sorbonne Universita CNRS,LOCEAN-IPSL, F-75005 Paris, France
3 Criteo AI Labs, Paris, France
firstname.lastname@isir.upmc.fr except marina.levy@locean.ipsl.fr
Ab stract
Modeling dynamical systems combining prior physical knowledge and machine
learning (ML) is promising in scientific problems when the underlying processes
are not fully understood, e.g. when the dynamics is partially known. A com-
mon practice to identify the respective parameters of the physical and ML compo-
nents is to formulate the problem as supervised learning on observed trajectories.
However, this formulation leads to an infinite number of possible decompositions.
To solve this ill-posedness, we reformulate the learning problem by introducing
an upper bound on the prediction error of a physical-statistical model. This al-
lows us to control the contribution of both the physical and statistical compo-
nents to the overall prediction. This framework generalizes several existing hybrid
schemes proposed in the literature. We provide theoretical guarantees on the well-
posedness of our formulation along with a proof of convergence in a simple affine
setting. For more complex dynamics, we validate our framework experimentally.
1	Introduction
Dynamical systems prediction and identification find crucial applications ranging from medicine
and the study of tumors (Hanahan & Weinberg, 2011; Lu & Fei, 2014) to oceanic and climate fore-
casting (Oreskes et al., 1994; Caers, 2011). The modeling of such systems traditionally rely on
ordinary or partial differential equations (ODE/PDE) (Madec, 2008; Marti et al., 2010), and their
resolution via numerical solvers and data assimilation (Ghil & Malanotte-Rizzoli, 1991). In real
world applications, two main pitfalls occur: first the dynamics may only be partially known and thus
do not fully represent the studied phenomena (Rio & Santoleri, 2018); second, the system state may
only be partially observed as in ocean models (Gaultier et al., 2013). Machine learning (ML) has be-
come a complementary approach to traditional physics based models (denoted MB for model based)
(Reichstein et al., 2019; Dueben & Bauer, 2018). Both offer advantages: whereas MB approaches
generalize and extrapolate better, ML high expressivity approaches benefit from the ongoing growth
of available data such as satellite observations, with reduced costs compared to data assimilation.
In that perspective, recent lines of work tackle the learning of hybrid models relying on prior physi-
cal knowledge and machine learning (Yin et al., 2021; Mehta et al., 2020). Efficiently learning such
decompositions actually means solving two different tasks: system identification, i.e. estimating the
parameters of the physical model, and prediction, i.e. recovering the trajectories associated to the
dynamics. Both are essential for hybrid MB/ML models of dynamical systems. Whereas predic-
tion aims at robust extrapolation, identification accounts for physical interpretability of the MB/ML
model. While solving both problems using model-based formulation admits well-known numeri-
cal solutions, for example using the adjoint method (Le Dimet & Talagrand, 1986; Courtier et al.,
1994), the combination of physical models and deep learning is still an open area of research. In this
context, ML applications mainly focus on the prediction task, at the expense of the system identifica-
tion: Willard et al. (2020) underlines the lack of generalizability of black-box ML models and their
* Equal contribution
1
Published as a conference paper at ICLR 2022
inability to produce physically sound results. Indeed, Ayed et al. (2020) show that without any prior
knowledge, the recovered estimates ofa dynamical system states are not physically plausible despite
accurate predictions. Moreover, as noted by Yin et al. (2021), learning a linear MB/ML decompo-
sition with the sole supervision on the system trajectories is ill-posed and admits an infinite number
of decompositions. Such observations highlight the need to incorporate physically motivated con-
straints in the learning of hybrid models, e.g. through regularization penalties, and several works
propose additional constraints to guide the model towards physical solutions (Jia et al., 2019; Yin
et al., 2021; Linial et al., 2021). Finally, to complete prior dynamical knowledge with a data-driven
component and ensure interpretability of the decomposition, we work out a principled framework
that generalizes previous attempts in the regularization of hybrid models. Our contributions are :
•	In section 3.1, we introduce a novel way to recover well-posedness and interpretability in
the learning of hybrid MB/ML models via the control of an upper bound. We extend our
framework to incorporate auxiliary data when available to handle complex real-world data.
•	In section 3.2, we propose a novel alternate-optimization algorithm to learn hybrid models.
•	In section 3.3, we provide an analysis of the convergence of the proposed algorithm on a
simplified case and experimentally evidence the soundness of our approach on more com-
plex settings of increasing difficulty including challenging real world problems (section 4).
2	Background and Problem Setup
We consider a dynamical system with state at time t denoted Zt = Z(t). Zt might be fully or
only partially observed: we write Zt = (Xt, Yt), where Xt is the observed component and Yt the
unobserved one. The evolution of Z is governed by a differential equation with dynamics :
dZt _ d (Xt∖ _ (fx(Zt)∖
dt = dt VYt)= IfY(Zt)J
The objective is to predict trajectories of X, i.e. to model the evolution of the observable part fol-
lowing dχtt = fχ(Zt). For simplicity, We omit the index X in fχ and write f (.) = fχ(.).
Dynamical Hypothesis We assume partial knowledge of the dynamics of the observed Xt :
~dtt = f (Zt)= fk(Zt) + fu(Zt)	(2)
where fk ∈ Hk is a known operator with unknown parameters θ*, and fu ∈ Hu is the unknown
residual dynamics. Hk and Hu denote function spaces, see discussion in appendix B.
Learning Problem Our objective is to approximate f with a function h learned from the observed
data. According to eq. (2), we assume h = hk + hu. hk ∈ Hk, i.e. belongs to the same hy-
pothesis space as fk: it has the same parametric form. Its parameters are denoted θk. Note that
hk(., θ*) = fk. hu ∈ Hu is represented by a free form functional with parameters θu, e.g. a neural
network. The learning problem is to estimate from data the parameters of hk so that they match
the true physical ones and hu to approximate at best the unknown dynamics f. In this regard, an
intuitive training objective is to minimize a distance d between h = hk + hu and f :
d(h,f) = EZ〜PZkh(Z)- f(Z)∣∣2,	(3)
where pZ is the distribution of the state Z that accounts for varying initial states. Each Z defines
a training sample. Minimizing eq. (3) with h = hk + hu enables to predict accurate trajectories
but may have an infinite number of solutions and hu may bypass the physical hypothesis hk. Thus,
interpretability is not guaranteed. We now develop our proposition to overcome this ill-posedness.
3	Method
In hybrid modeling, two criteria are essentials: 1. identifiability, i.e. the estimated parameters of hk
should correspond to the true physical ones; 2. prediction power, i.e. the statistical component hu
should complete hk so that h = hk + hu performs accurate prediction over the system states. To
2
Published as a conference paper at ICLR 2022
control the contribution of each term hk and hu, we work upper bounds out of eq. (3) (section 3.1).
We then propose to minimize d(h, f) while constraining the upper bounds, which provide us with
a well-posed learning framework (section 3.2). Besides, we show that several previous works that
introduced constrained optimization to solve related problems are specific cases of our formulation
(Yin et al., 2021; Jia et al., 2019; Linial et al., 2021). Finally, we introduce an alternate optimization
algorithm which convergence is shown in section 3.3 for a linear approximation of f.
3.1	Structural Constraints for Dynamical Systems
To ensure identifiability, we derive regularizations on hk and hu flowing from the control ofan upper
bound of d(h, f). In particular, to minimize d(hk, fk) would enable us to accurately interpret hk as
the true fk , and hu as the residual dynamics fu . However, since we do not access the parameters of
fk, computing d(hk, fk) is not tractable. We then consider two possible situations. In the first one,
the only available information on the physical system is the parametric form of fk (or equivalently
of hk), training thus only relies on observed trajectories (section 3.1.1). In the second one, we
consider available auxiliary information about fk that will be used to minimize the distance between
hk and fk (section 3.1.2). While the first setting is the more general, the physical prior it relies on is
often insufficient to effectively handle real world situations. The second setting makes use of more
informative priors and better corresponds to real cases as shown in the experiments (section 4.2).
3.1.1	Controlling the ML Component and the MB Hypothesis
We propose a general approach to constrain the learning of hybrid models when one solely access
the functional form of hk . In this case, to make hk accountable in our observed phenomena, a
solution is to minimize d(hk, f). Following the triangle inequality we link up both errors d(h, f)
and d(hk, f) (computations available in appendix C.1):
d(h, f) ≤d(h,hk)+d(hk,f)=d(hu,0)+d(hk,f)	(4)
We want the physical-statistical model h = hk + hu to provide high quality forecasts. Minimizing
the sole upper bound does not ensure such aim, as hu is only penalized through d(hu , 0) and is
not optimized to contribute to predictions. We thus propose to minimize d(h, f) while controlling
both d(hu , 0) and d(hk , f). Such a control of the upper bound of eq. (4) amounts to balancing the
contribution of the ML and the MB components. This will be formally introduced in section 3.2.
Link to the Literature The least action principle on the ML component i.e. constraining d(hu , 0)
is invoked for a geometric argument in (Yin et al., 2021), and appears as a co-product of the intro-
duction of d(hk, f) in eq. (4). Optimizing d(hk, f) to match the physical model with observations
is investigated in (Forssell & Lindskog, 1997).
The general approach of eq. (4) allows us to perform prediction (via h) and system identification
(via hk) on simple problems (see section 4.1). The learning of real-world complex dynamics, via
data-driven hybrid models, often fails at yielding a physically sound estimation, as illustrated in
section 4.2. This suggests that learning complex dynamics requires additional information. In many
real-world cases, auxiliary information is available in the form of measurements providing comple-
mentary information on fk. Indeed, a common issue in physics is to infer an unobserved variable of
interest (in our case fk parameters θ?) from indirect or noisy measurements that we refer to as proxy
data. For instance, one can access a physical quantity but only at a coarse resolution, as in (Um et al.,
2020; Belbute-Peres et al., 2020) and in the real world example detailed in section 4.2. We show in
the next subsection how to incorporate such an information in order to approximate d(hk , fk).
3.1.2	Matching the Physical Hypotheses: Introducing Auxiliary Data
We here assume one accesses a proxy of fk, denoted fkpr ∈ Hk. Our goal is to adapt our framework
to incorporate such auxiliary information, bringing the regularization induced by fkpr within the
scope of the control of an upper bound. This enables us to extend our proposition towards the
solving of real world physical problems, still largely unexplored by the ML community. We have:
d(h, f) ≤d(h,hk)+d(hk,fkpr)+Γ=d(hu,0)+d(hk,fkpr)+Γ	(5)
where Γ is a constant of the problem that cannot be optimized (see appendix C.2). In that context,
we can benefit from auxiliary information providing us with coarse estimates of θ?, denoted θpr,
3
Published as a conference paper at ICLR 2022
such that fkpr = hk(., θpr) ≈ fk. To use the available θpr to guide our estimation towards the
true parameters θ? of fk, a simple solution is to directly enforce the minimization of d(hk , fkpr)
in the parameter space by minimizing kθk - θpr k2 , where θk are the parameters of hk . Indeed,
because fk and fkpr have identical parametric forms (as both belong to the same functional space
Hk), minimizing kθk - θpr k2 will bring hk closer to fkpr and thus to fk. As above, we propose to
minimize d(h, f) while controlling both d(hu, 0) and d(hk, fkpr), as described in section 3.2.
Link to the Literature In (Linial et al., 2021) fkpr stands for true observations used to constrain a
learned latent space, minimizing d(hk, fkpr). Jia et al. (2019) uses synthetic data as fkpr to pre-train
their model which amounts to the control an upper bound, see appendix C.3. Finally, this setting
finds an extension, when the model fkpr is a learned model, for example trained using eq. (4), leading
to a self-supervision approach described in appendix C.4.
3.2	Learning Algorithm and Optimization Problem
From the upper bounds, we first recover the well-posedness of the optimization and derive a theo-
retical learning scheme (section 3.2.1). We then discuss its practical implementation (section 3.2.2).
3.2.1	Well-Posedness and Alternate Optimization Algorithm
Recovering Well-Posedness We reformulate the ill-posed learning of minhk,hu∈Hk×Hu d(h, f),
by instead optimizing d(h, f) while constraining the upper bounds. Let us define Sk and Su as
Sk = { hk	∈Hk |	'(hk) ≤ μk }	Su	= {	hu	∈Hu	|	d(hu, 0) ≤ μu	}	(6)
where μk,μu are two positive scalars and '(hk) = d(hk, f) in the case of section 3.1.1 and
`(hk) = d(hk, fkpr) in the case of section 3.1.2. Our proposition then amounts to optimizing d(h, f)
over the Minkowski-sum Sk + Su = { h = hk + hu | hk ∈ Sk, hu ∈ Su } :
min d(h, f),	(7)
h∈Sk+Su
This constrained optimization setting enables us to recover the well-posedness of the optimization
problem under the relative compactness of the family of function Hk (proof in appendix D.3).
Proposition 1 (Well-posedness). Under the relative compactness of Sk, eq. (7) finds a solution h
that writes as h = hk + hu ∈ Sk + Su. Moreover, this solution is unique.
Alternate Optimization Algorithm As the terms in both upper bounds of eqs. (4) and (5) specif-
ically address either hk or hu , we isolate losses relative to hk and hu and alternate projections of
hk on Sk and hu on Su , as described in Algorithm 1. Said otherwise, we learn h by alternately
optimizing hk (hu being fixed) and hu (hk being fixed). In practice, we rely on a dual formulation
(see section 3.2.2 and the SGD version of Algorithm 1 in Appendix F).
Algorithm 1 Alternate estimation: General Setting
Result: Converged hk and hu
Set h0u = 0, h0k = minhk∈Hk d(hk, f), tol ∈ R+
while d(h, f) > tol do
hkn+1 = arg min d(hk + hun, f);	hun+1 = arg min d(hkn+1 + hu,f)
hk ∈Sk	hu ∈Su
n - n + 1
end
(8)
The convergence of the alternate projections is well studied for the intersection of convex sets or
smooth manifolds (von Neumann, 1950; Lewis & Malick, 2008) and has been extended in our
setting of Minkowski-sum of convex sets (Lange et al., 2019). Because d as defined in eq. (3) is
convex, Su and Sk are convex sets as soon as Hk and Hu are convex (Appendix A). Thus, if d(., f)
is strongly convex, eq. (8) finds one and only one solution (Boyd et al., 2004). However, neither the
convexity of Hu nor of Hk is practically ensured. Nonetheless, we recover the well-posedness of
eq. (7) and show the convergence of Algorithm 1 in the simplified case where h is an affine function
4
Published as a conference paper at ICLR 2022
of Xt (see section 3.3). For complex PDE where convexity may not hold, we validate our approach
experimentally and we evidence in section 4 that this formulation enables us to recover both an
interpretable decomposition h = hk + hu and improved prediction and identification performances.
3.2.2	Practical Optimization
Equation (6) involves the choice of μk and μu. In practice, We implement the projection algorithm
by descending gradients on the parameters of hk and hu , with respect to the following losses:
Lk(hk) = λhd(h, f) + λhk'(hk)	Lu(hu) = λhd(h,f) + λhud(hu, 0)
(9)
Where λh, λhk , λhu are positive real values, dynamically increased/decreased during training. In-
deed, d(hu , 0) can be interpreted as a stability loss, preventing the neural netWorks to trump the
physical component. On the other hand, d(hk, f) can be interpreted has an initialization loss yield
a first estimate of θk explaining the dynamics.
Yet, f being unknoWn: d(h, f) is not tractable. To estimate d(h, f), We rely on the trajectories
associated to the dynamics. We minimize the distance betWeen the ODE floWs φh and φf defined
by h and f, dφ(φh, φf), over all initial conditions X0:
dφ(φh, φf) = EX0	kφh(τ,X0) - φf(τ,X0)k2dτ
t0
(10)
We have: dφ(φh, φf) = 0 ⇔ d(h, f) = 0. Definitions of flows for ODE and in depth considera-
tion on these distances are available in appendix A. The gradients ofdφ(φh, φf) with respect to the
parameters of hk or hu can be either estimated analytically using the adjoint method (Chen et al.,
2018) or using explicit solvers, e.g. Rk45, and computing the gradients thanks to the backpropaga-
tion, see (Onken & Ruthotto, 2020). To compute eq. (10), we rely on a temporal sampling of X:
our datasets are composed ofn sequences of observations of length N, Xi = (Xti0, . . . , Xti +N∆t),
where each sequence Xi follows eq. (2) and corresponds to one initial condition Xti0. We then sam-
ple the space of initial conditions Xti0 to compute a Monte-Carlo approximation to dφ (φh, φf). Let
ODESolve be the function integrating any arbitrary initial state xt0 up to time t with dynamics h,
so that xt = ODESolve(xt0, h, t). The estimate ofdφ(φh, φf) then writes as:
nN
dφ(φh,φf) ≈ n χχ∣∣ODEsOlve(Xi0 ,h,t)- Xj ∣∣2
Note that the way to compute ODEsolve differs across the experiments (see section 4).
3.3	Theoretical Analysis for a Linear Approximation
We investigate the validity of our proposition When approximating an unknoWn derivative With an
affine function (interpretable first guess approximators). We here consider hk as a linear function.
We do not assume any information on f, thus relieving this section from the need of an accurate
prior knoWledge fk. In this context, We shoW the convergence of the learning scheme introduced
in Algorithm 1 With ` = d(hk, f), demonstrating the validity of our frameWork in this simplified
setting. For more complex cases, for Which theoretical analysis cannot be conducted, our frameWork
is validated experimentally in section 4. All proofs of this section are conducted using the distance
dφ . Let Xs be the unique solution to the initial value problem:
-tt^- = f (Xt) with Xt=o = Xo
With hk(X) = AX and hu(X) = DA, the affine approximation of f Writes as:
j t = AXt + DA with Xt=o = Xo
dt
(11)
(12)
Where A ∈ Mp,p(R), DA ∈ Rp. We Write XD the solution to eq. (12) and XA the solution to
eq. (12) When DA = 0. The alternate projection algorithm With the distance dφ Writes as:
A = arg min 八IXs(τ) - XD(τ儿 dτ + λa 八IXs(τ) - XA(τ儿 dτ
A	t0	t0
DA = arg min [ ∣∣Xs(τ) - XD(τ儿 dτ + λD ∣∣Da∣∣2
DA	t0	2
(13)
(14)
5
Published as a conference paper at ICLR 2022
where λD, λA > 0. As the optimization of eq. (13) is not convex on A, the solution existence and
uniqueness is not ensured. The well-posedness w.r.t A can be recovered by instead considering a
simple discretization scheme, e.g. Xt+1 ≈ (AXt + DA)∆t + Xt and solving the associated least
square regression, which well-posedness is guaranteed, see details in appendix D.2. Such strategy is
common practice in system identification. Theoretical considerations on existence and uniqueness
of solutions to eqs. (13) and (14) are hard to retrieve. If A is an invertible matrix:
Proposition 2 (Existence and Uniqueness). If A is invertible, There exists a unique DA, hence a
unique XD, solving eq. (14). (proof in appendix D.4)
Finally, formulating Algorithm 1 as a least square problem in an affine setting (see appendix D.5),
we prove the convergence of the alternate projection algorithm (appendix D.6) :
Proposition 3. For λD and λA sufficiently high, the algorithm that alternates between the estimation
of A and the estimation of DA following eqs. (13) and (14) converges.
4	Experiments
We validate Algorithm 1 on datasets of increasing difficulty (see appendix E), where the system state
is either fully or partially observed (resp. section 4.1 and section 4.2). We no longer rely on an affine
prior and explicit hk and hu for each dataset. Performances are evaluated via standard metrics: MSE
(lower is better) and relative Mean Absolute Error (rMAE, lower is better). We assess the relevance
of our proposition based on eqs. (4) and (5), against NeuralODE (Chen et al., 2018), Aphynity (Yin
et al., 2021) and ablation studies. We denote Ours eq. (4) (resp. Ours eq. (5)) the results when
` = d(hk, f) i.e eq. (4), (resp. ` = d(hk, fkpr) i.e. eq. (5)) When d(hk, f) (resp. d(hu, 0)) is not
considered in the optimization, we refer to the results as d(h, f)+d(hu, 0) (resp. d(h, f)+d(hk, f)).
When h is trained by only minimizing the discrepancy between actual and predicted trajectories the
results are denoted «Only d(h, f)». We report between brackets the standard deviation of the metrics
over 5 runs and refer to Appendices F and G for training information and additional results.
4.1	Fully Observable Dynamics
To illustrate the learning scheme induced by eq. (4), we focus on fully observed low dimensional dy-
namics: a simple example emerging from Newtonian mechanics and a population dynamics model.
Damped Pendulum (DPL) Now a standard benchmark for hybrid models, we consider the motion
of a pendulum of length L damped due to viscous friction (Greydanus et al., 2019; Yin et al., 2021).
Newtonian mechanics provide an ODE describing the evolution of the angle x of the pendulum:
X — g/L Sin(X) + kX = 0
(15)
We suppose access to observations of the system state Z = (x, X). We consider as physical motion
hypothesis hk (x, θk) = θk sin(χ). The true pulsation θ* = g/L of the pendulum has to be estimated
with θk. The viscous friction term kX remains to be estimated by hu.
Population Dynamics (LV) Lotka-Volterra ODE system models a prey/predator population dy-
namics describing the growth of the preys (X) without predators (y), and the extinction of predators
without preys, the non linear terms expressing the encounters between both species:
X = ɑx — βxy, and y = —γy + δxy
(16)
We observe the system state Z = (χ,y) and set as prior knowledge: hk(χ,y) = (θ1χ, -θ22y).
θ? = (α, γ) has to be estimated by θk = (θk1, θk2). hu accounts for the non linear terms (βXy, δXy).
Experimental Setting For both DPL and LV experiments, we consider the following setting: we
sample the space of initial conditions building 100/50/50 trajectories for the train, validation and test
sets. The sequences share the same parameters; respectively (L,k), for DPL, and (α, β, γ, δ) for
LV. The parameter θk is set to a neuron (of dimension 1 in the pendulum and 2 for LV) and hu is a
2-layer MLP. Further experimental details are available in appendices E.1, E.2 and F.
6
Published as a conference paper at ICLR 2022
Table 1: Experimental Results for PDL and LV data. The presented metric for parameter evaluation is the
rMAE reported in %. Pred. columns report the prediction log MSE on trajectories on test set.
Model	rMAE(θk, θ?)	PDL Pred. logMSE	LV rMAE(θk, θ?)	Pred. logMSE
Ours eq. (4)	1.56 (0.009)	-13.7 (0.84)	7.80 (0.011)	-9.28 (0.75)
Only d(h,f)	9.35 (0.04)	-13.3 (0.65)	24.5 (0.017)	-9.21 (0.91)
d(h, f) +d(hk,f)	1.82 (0.01)	-13.4 (0.56)	7.91 (0.02)	-9.01 (0.99)
d(h, f) + d(hu, 0)	11.1 (0.03)	-12.9 (0.29)	9.80 (0.098)	-9.45 (0.55)
Aphynity	6.15 (0.009)	-12.2 (0.13)	21.1 (0.016)	-9.89 (0.53)
NeuralODE	一	-10.1 (0.32)	—	-9.11 (1.1)
-*- Il^-All?
-*- ∣∣D-D∣∣?
Identification and Prediction Results Table 1
shows that despite accurate trajectory forecasting, the
unconstrained setting «Only d(h, f)» fails at estimating
the models parameters, showing the need for regular-
ization for identification. Constraining the norm of the
ML component can be insufficient: for LV data, both
Aphynity and d(h, f) + d(hu, 0) do not accurately es-
timate the model parameters. However, the control of
d(hk , f), following eq. (4), significantly improves the
parameter identification for both datasets. Indeed, in
4.0]
3.5-
3.0
2.5-
2.0-
1.5-
1.0-
0.5
0.0
gradient iterations (×200)
20	25	30
the PDL case, hk and f are (pseudo)-periodic of the
same period, hence the gain in the performances. Fi-
nally, our proposition based on eq. (4) is able to identify
the parameters of DPL and LV equation with a preci-
sion of respectively 1.56% and 7.8% beating all consid-
Figure 1: Affine Case : Evolution of the MSE
between estimated dynamics (A, D) and the
true one (A, D) with the number of gradients
steps for linearized DPL.
ered baselines. Regarding prediction performances, in under-constrained settings ( «Only d(h, f)»
in Table 1), hu learns to corrects the inaccurate hk. Table 1 and figs. 4 and 5 (appendix G.1) show
that our proposition provides more consistent prediction performances. These experiments confirm
that the constraints on hk and hu arising from the control of the upper bound of eq. (4) increase
interpretability and maintain prediction performances.
Throwback to the Affine Case We verify the convergence proved in section 3.3 using the damped
pendulum (eq. (15)) linearized in the small oscillations regime (see appendix E.1). Making an affine
hypothesis following eq. (12), we apply our alternate projection algorithm and optimize A and DA
alternately using SGD. Figure 1 shows that we are able to accurately estimate A and D using our
proposition, recovering both the oscillation pulsation and the damping coefficient.
4.2	High Dimensional Dynamics
We now address the learning of transport equations, describing a wide range of physical phenomena
such as chemical concentration, fluid dynamics or material properties. We evaluate the learning
setting induced by eq. (4) and (5) on two physical datasets depicting the evolution of the temperature
T advected by a time-dependent velocity field U and subject to forcing S, following:
∂T
与t + ▽.(TU ) = S(U)
(17)
The system state Z = (T, U, S) is partially observed, we only access T. Every quantities, observed
or to estimate, are regularly sampled on a spatiotemporal grid: at each timestep t, the time varying
velocity field Ut writes as Ut = (ut, vt) and ut, vt, Tt and the forcing term St are all of size 64 × 64.
Experimental Setting We consider as physical prior the advection i.e hk(T, θk) = -V.(Tθk).
Thus, θk is time-dependent, as we learn it to approximate θ? = U. We identify the velocity field θk
from observations of T, learning a mapping between T and U parameterized by a neural network
Gψ, so that θk = Gψ(Tt-ι,..., Tt) ≈ Ut, which is common practice in oceanography (Bereziat &
Herlin, 2015). Gψ is optimized following eq. (9). S remains to be learned by hu . hk implements
7
Published as a conference paper at ICLR 2022
Table 2: Results for Adv+S and Natl data. We report the MSE (× 100) on the predicted observations T , the
velocity fields U and the source term S over 6 time steps on test set.
Models	Adv+S				Natl	
	T	U	S	T	U	S
Ours eq. (4)	0.74 (0.05)	1.99 (0.13)	0.17 (0.01)	8.27 (0.06)	11.72 (0.07)	6.01 (0.08)
Ours eq. (5)	—	—	—	6.86 (0.12)	6.81 (0.07)	4.35 (0.11)
Aphynity	0.85 (0.35)	3.07 (0.74)	0.18 (0.05)	8.18 (0.16)	11.75 (0.49)	6.02 (0.02)
NeuralODE	1.35 (0.02)	—	—	8.83 (0.98)	一	一
a differentiable semi-Lagrangian scheme (Jaderberg et al., 2015) (see appendix E.3) and hu is a
ResNet. Gψ is a UNet. Training details and a schema of our model are to be found in appendix F.
Synthetic Advection and Source (Adv+S) To
test the applicability of the learning setting in-
duced by eq. (4) on partially observed settings,
we first study a synthetic setting (denoted Adv+S)
of eq. (17) by generating velocity fields U, simu-
lated following (Boffetta et al., 2001) and adding
a source term S inspired by (Frankignoul, 1985).
The simulation details are given in appendix E.3.
Real Ocean Dynamics (Natl) We consider a
dataset emulating real world observations of the
North ATLantic ocean (denoted Natl) (Ajayi et al.,
2019). Modeling the evolution of T in Natl is
challenging as its dynamics is chaotic and highly
non-linear. This simulation is representative of the
complexity encountered in real world data. The
principled approach of eq. (4) is insufficient here
and one must resort to additional physical informa-
tion. We illustrate section 4.2 and make use of aux-
iliary data: satellite observations provide a coarse
estimate of surface velocity fields (appendix E).
V truth
v estimated
U truth
u estimated
T truth
T estimated
S truth
S estimated
Figure 2: Best viewed in color. Estimations of S, T
and U = (u, v) on Adv+S. Prediction ranges from
1 to 20 half-days.
The goal is to refine the approximated velocity fields to fit the ocean dynamics. We proceed as
described in eq. (5) and enforce d(hk , fkpr) supervising Gψ with the proxy data (appendix E.3).
Identification and Prediction Results Table 2 indicates that for Adv+S dataset, we estimate ac-
curately the unobserved velocity fields. Qualitatively, Figure 2 shows that controlling our proposed
upper bound eq. (4) facilitates the recovery of truthful velocity fields U along with an accurate pre-
diction of T . For the highly complex Natl, Table 2 shows that the introduction of auxiliary data
following the formulation in eq. (5) significantly helps identification, as the dynamics is too com-
plex to be able to recover physically interpretable velocity fields using the bound of eq. (4).
Regarding prediction performances on the Adv+S data, Table 2 shows that thanks to our truthful
estimates of U, our model provides more precise prediction than NODE and Aphynity. For real
world data, thanks to the proxy data our model recovers better velocity fields terms while providing
a better estimate for T . Besides, adding prior knowledge in the prediction systems improves pre-
diction performances: appendix G shows that NODE minimizes d(h, f) by predicting average and
blurred frames. This shows the need for regularization when learning on structured physical data.
Ablation Study We present in Table 3 an ablation study on the Adv+S dataset evidencing the
influence of our learning choices on the resolution of both identification and prediction tasks (see
appendix G for detailed results). “Joint” rows of Table 3 indicate that the learning of hu and hk is
done simultaneously. Table 3 shows that the sole optimization of d(h, f) fails at estimating phys-
ically sounded U. This evidences the ill-posedness in such unconstrained optimization. Table 3
indicates that all introduced regularizations improve the recovery of U w.r.t. the «Only d(h, f)»
8
Published as a conference paper at ICLR 2022
baseline, while adding d(hu , 0) significantly improves both prediction performances and velocity
fields estimation. We highlight that the alternate optimization performs better compared to optimiz-
ing jointly all parameters of hk and hu . Notably, our proposition to optimize hk and hu alternately
beats all baselines on both T prediction and U identification (Table 3, Joint rows). Finally, jointly
trained models fail at estimating U in Table 3, forcing hu to capture the whole dynamics.
Table 3: Ablation Study on Adv+S. We report the MSE (× 100) on the predicted observations T , the velocity
fields U and the source term S over 6 time steps. “Joint” rows refer to the simultaneous optim. of hk and hu.
Training	Models	T	U	S
	OUrs (U known)	0.52	n/a	0.19
	OUrs eq. (4)	0.74 (0.05)	1.99 (0.13)	0.17 (0.01)
Alternate	Only d(h, f)	1.02 (0.16)	4.08 (0.23)	0.19 (0.06)
	d(h, f) + d(hk, f)	1.02 (0.09)	3.66 (0.15)	0.19 (0.03)
	d(h, f) + d(hu, 0)	0.77 (0.06)	2.38 (0.17)	0.19 (0.01)
Joint	OUrs eq. (4)	1.44 (0.08)	3.30 (0.18)	0.30 (0.03)
	Only d(h, f)	1.38 (0.19)	6.96 (0.21)	0.39 (0.08)
5	Related Work
Grey-box or hybrid modeling, combining ODE/PDE and data based models, has received an increas-
ing focus in the machine learning community (Rico-Martinez et al., 1994; Thompson & Kramer,
1994; Raissi et al., 2020b). Hybrid approaches allow for alleviated computational costs for fluid
simulation (Tompson et al., 2017; De Avila Belbute-Peres et al., 2020; Wandel et al., 2021), and
show better prediction performances through data specific constraints that preserve physics (Raissi
et al., 2020a; Jia et al., 2019). They offer increased interpretability via constraints on convolutional
filters (Long et al., 2018; 2019) or on learned residual (Geneva & Zabaras, 2020). Physical knowl-
edge, introduced through ODE/PDE regularization (Psichogios & Ungar, 1992; Bongard & Lipson,
2007; de Bezenac et al., 2018) or Hamiltonian priors (GreydanUs et al., 2019; Lee et al., 2021),
increases generalization power w.r.t pure ML approaches. Closer to our work, (Mehta et al., 2020;
San & MaUlik, 2018; YoUng et al., 2017; Saha et al., 2020) stUdy the learning of a physical model
aUgmented with a statistical component. Yin et al. (2021) tackle the same task, ensUring the UniqUe-
ness in the decomposition by constraining the norm of the ML component. We generalize latter
approaches and address the well-posedness in the learning of hybrid ML/MB models throUgh ad-
ditional regUlarization on the estimated parameters of the physical part. Indeed, to describe natUral
phenomena relying on hybrid MB/ML models, one major task lies in the estimation of the MB part
parameters. This can be done Using neUral networks (Raissi et al., 2019; Mehta et al., 2020). How-
ever, identification tasks being intrinsically ill-posed (Sabatier, 2000), imposing prior knowledge or
regUlarization is necessary to ensUre soUnd estimations (Stewart & Ermon, 2017). Yet, Using only
prediction as sUpervision, the recovered parameters are not physically interpretable (de Bezenac
et al., 2018; Ayed et al., 2020). To ensUre UniqUeness of the estimation solUtion, Ardizzone et al.
(2018) Use invertible neUral networks. Linial et al. (2021); Tait & DamoUlas (2020); SaemUndsson
et al. (2020) combine variational encoding (Kingma & Welling, 2013) and a PDE model, sampling
the space of initial conditions and parameters to solve both identification and prediction. However,
sUch methods only deal with low-dimensional dynamics. Besides low dimensional systems, we also
show the soUndness of oUr approach on complex high dimensional and partially observed dynamics.
6	Discussion
We propose in this work an algorithm to learn hybrid MB/ML models. For interpretability pUrposes,
we impose constraints flowing from an Upper boUnd of the prediction error and derive a learning
algorithm in a general setting. We prove its well posedness and its convergence in a linear ap-
proximation setting. Empirically, we evidence the soUndness of oUr approach thanks to ablation
stUdies and comparison with recent baselines on several low and high dimensional datasets. This
work can see several extensions: considering non Uniform 2 or 3-D grid for climate models, fUrther
considerations on the investigated Upper boUnds, or different decomposition hypothesis.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We would like to thank all members of the MLIA team from the ISIR laboratory of Sorbonne Uni-
Versite for helpful discussions and comments. We acknowledge financial support from ANR AI
Chairs program via the DL4CLIM ANR-19-CHIA- 0018-01 project, the LOCUST ANR project
(ANR-15-CE23-0027) and the European Union’s Horizon 2020 research and innovation program
under grant agreement 825619 (AI4EU). The Natl60 data were provided by MEOM research team,
from the IGE laboratory from the Universite Grenoble Alpes.
References
Adekunle Ajayi, Julien Le Sommer, Eric Chassignet, Jean-Marc Molines, Xiaobiao Xu, Aurelie
Albert, and Emmanuel Cosme. Spatial and temporal variability of north atlantic eddy field at
scale less than 100km. Earth and Space Science Open Archive, pp. 28, 2019. doi: 10.1002/
essoar.10501076.1.
Lynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich Kothe. Analyzing inverse problems
with invertible neural networks. In International Conference on Learning Representations, 2018.
Ibrahim Ayed, Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari. Learning the spatio-
temporal dynamics of physical processes from partial observations. In ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3232-3236,
2020.
Filipe de Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable pde
solvers and graph neural networks for fluid flow prediction. In International Conference on Ma-
chine Learning, pp. 2402-2411. PMLR, 2020.
Dominique Bereziat and Isabelle Herlin. Coupling dynamic equations and satellite images for mod-
elling ocean surface circulation. Communications in Computer and Information Science, 550:
191-205, 2015. doi: 10.1007/978-3-319-25117-2\_12. URL https://hal.inria.fr/hal-01245369.
Guido Boffetta, G Lacorata, G Redaelli, and A Vulpiani. Detecting barriers to transport: a review of
different techniques. Physica D: Nonlinear Phenomena, 159(1-2):58-70, 2001.
Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems.
Proceedings of the National Academy of Sciences, 104(24):9943-9948, 2007.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Jef Caers. Modeling uncertainty in the earth sciences. John Wiley & Sons, 2011.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-
ferential equations. In Samy Bengio, Hanna Wallach, Hugo Larochelle, Kristen Grauman, Nicold
Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
31, pp. 6571-6583. Curran Associates, Inc., 2018.
PHILIPPE Courtier, J-N Thepaut, and Anthony Hollingsworth. A strategy for operational imple-
mentation of 4d-var, using an incremental approach. Quarterly Journal of the Royal Meteorolog-
ical Society, 120(519):1367-1387, 1994.
Filipe De Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable PDE
solvers and graph neural networks for fluid flow prediction. In Hal Daume III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 2402-2411. PMLR, 13-18 Jul 2020.
Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes:
Incorporating prior scientific knowledge. In International Conference on Learning Representa-
tions, 2018.
10
Published as a conference paper at ICLR 2022
Michail Diamantakis. The semi-lagrangian technique in atmospheric modelling: current status and
future challenges. In Seminar on Recent Developments in Numerical Methods for Atmosphere and
Ocean Modelling, 2-5 September 2013, pp.183-200, Shinfield Park, Reading, 2014. ECMWF,
ECMWF.
J6r6me Droniou. Integration et EsPaces de Sobolev a Valeurs Vectorielies. working paper or
preprint, April 2001. URL https://hal.archives-ouvertes.fr/hal-01382368.
Peter D Dueben and Peter Bauer. Challenges and design choices for global weather and climate
models based on machine learning. Geoscientific Model Development, 11(10):3999-4009, 2018.
U. Forssell and P. Lindskog. Combining semi-physical and neural network modeling: An ex-
ample ofits usefulness. IFAC Proceedings Volumes, 30(11):767-770, 1997. ISSN 1474-6670.
doi: https://doi.org/10.1016/S1474-6670(17)42938-7. IFAC Symposium on System Identifica-
tion (SYSID’97), Kitakyushu, Fukuoka, Japan, 8-11 July 1997.
Claude Frankignoul. Sea surface temperature anomalies, planetary waves, and air-sea feedback in
the middle latitudes. Reviews of geophysics, 23(4):357-390, 1985.
Lucile Gaultier, Jacques Verron, Jean-Michel Brankart, Olivier Titaud, and Pierre Brasseur. On
the inversion of submesoscale tracer fields to estimate the surface ocean circulation. Journal of
Marine Systems, 126:33-42, 2013.
Nicholas Geneva and Nicholas Zabaras. Modeling the dynamics of pde systems with physics-
constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020.
ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2019.109056.
Michael Ghil and Paola Malanotte-Rizzoli. Data assimilation in meteorology and oceanography.
Advances in geophysics, 33:141-266, 1991.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Hanna
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alche Buc, Emily Fox, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 15379-15389. Curran
Associates, Inc., 2019.
Douglas Hanahan and Robert A Weinberg. Hallmarks of cancer: the next generation. cell, 144(5):
646-674, 2011.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. CVPR, 2017.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. Spatial transformer
networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https:
//proceedings.neurips.cc/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf.
Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob Zwart, Michael S Steinbach, and
Vipin Kumar. Physics guided rnns for modeling dynamical systems: A case study in simulating
lake temperature profiles. In SIAM International Conference on Data Mining, SDM 2019, SIAM
International Conference on Data Mining, SDM 2019, pp. 558-566. Society for Industrial and
Applied Mathematics Publications, 2019. doi: 10.1137/1.9781611975673.63.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Kenneth Lange, Joong-Ho Won, and Jason Xu. Projection onto Minkowski sums with application
to constrained learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 3642-3651. PMLR, 09-15 Jun 2019.
Frangois-Xavier Le Dimet and Olivier Talagrand. Variational algorithms for analysis and assimi-
lation of meteorological observations: theoretical aspects. Tellus A: Dynamic Meteorology and
Oceanography, 38(2):97-110, 1986.
11
Published as a conference paper at ICLR 2022
Seungjun Lee, Haesang Yang, and Woojae Seong. Identifying physical law of hamiltonian systems
via meta-learning. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=45NZvF1UHam.
Adrian S. Lewis and J6r6me Malick. Alternating projections on manifolds. Mathematics of Opera-
tions Research, 33(1):216-234,2008. doi:10.1287/moor.1070.0291.
Ori Linial, Neta Ravid, Danny Eytan, and Uri Shalit. Generative ode modeling with known un-
knowns. In Proceedings of the Conference on Health, Inference, and Learning, pp. 79-94, 2021.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-Net: Learning PDEs from data.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3208-3216,
Stockholmsmassan, Stockholm Sweden, July 2018. PMLR.
Zichao Long, Yiping Lu, and Bi Dong. PDE-Net 2.0: Learning PDEs from data with a numeric-
symbolic hybrid deep network. Journal of Computational Physics, 399:108925, 2019.
Guolan Lu and Baowei Fei. Medical hyperspectral imaging: a review. Journal of biomedical optics,
19(1):010901, 2014.
Gurvan Madec. NEMO ocean engine. Note du P6le de mod6lisation, Institut Pierre-Simon Laplace
(IPSL), France, No 27, 2008.
Olivier Marti, Pascale Braconnot, J-L Dufresne, Jacques Bellier, Rachid Benshila, Sandrine Bony,
Patrick Brockmann, Patricia Cadule, Arnaud Caubel, Francis Codron, et al. Key features of the
ipsl ocean atmosphere model and its sensitivity to atmospheric resolution. Climate Dynamics, 34
(1):1-26, 2010.
Viraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, Andrew Oakleigh Nelson, Mark D
Boyer, Egemen Kolemen, and Jeff Schneider. Neural dynamical systems: Balancing structure and
flexibility in physical prediction. arXiv preprint arXiv:2006.12682, 2020.
Derek Onken and Lars Ruthotto. Discretize-optimize vs. optimize-discretize for time-series regres-
sion and continuous normalizing flows. arXiv preprint arXiv:2005.13420, 2020.
Naomi Oreskes, Kristin Shrader-Frechette, and Kenneth Belitz. Verification, validation, and confir-
mation of numerical models in the earth sciences. Science, 263(5147):641-646, 1994.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In Hanna Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch6
Buc, Emily Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 8026-8037. Curran Associates, Inc., 2019.
Dimitris C. Psichogios and Lyle H. Ungar. A hybrid neural network-first principles approach to
process modeling. AIChE Journal, 38(10):1499-1511, 1992. doi: https://doi.org/10.1002/aic.
690381003. URL https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690381003.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning
velocity and pressure fields from flow visualizations. Science, 367(6481):1026-1030, 2020a.
ISSN 0036-8075. doi: 10.1126/science.aaw4741.
Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning
velocity and pressure fields from flow visualizations. Science, 367(6481):1026-1030, 2020b.
12
Published as a conference paper at ICLR 2022
Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno Car-
valhais, et al. Deep learning and process understanding for data-driven earth system science.
Nature, 566(7743):195-204, 2019.
R Rico-Martinez, JS Anderson, and IG Kevrekidis. Continuous-time nonlinear signal processing: a
neural network based approach for gray box identification. In Proceedings of IEEE Workshop on
Neural Networks for Signal Processing, pp. 596-605. IEEE, 1994.
M-H Rio and R Santoleri. Improved global surface currents from the merging of altimetry and sea
surface temperature data. Remote sensing of Environment, 216:770-785, 2018.
Pierre C Sabatier. Past and future of inverse problems. Journal of Mathematical Physics, 41(6):
4082-4124, 2000.
Steindor Saemundsson, Alexander Terenin, Katja Hofmann, and Marc Deisenroth. Variational inte-
grator networks for physically structured embeddings. In International Conference on Artificial
Intelligence and Statistics, pp. 3078-3087. PMLR, 2020.
Priyabrata Saha, Saurabh Dash, and Saibal Mukhopadhyay. Phicnet: Physics-incorporated
convolutional recurrent neural networks for modeling dynamical systems. arXiv preprint
arXiv:2004.06243, 2020.
Omer San and Romit Maulik. Machine learning closures for model order reduction of thermal fluids.
Applied Mathematical Modelling, 60, 04 2018. doi: 10.1016/j.apm.2018.03.037.
Russell Stewart and Stefano Ermon. Label-free supervision of neural networks with physics and
domain knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31,
2017.
Daniel J Tait and Theodoros Damoulas. Variational autoencoding of pde inverse problems. arXiv
preprint arXiv:2006.15641, 2020.
Michael L Thompson and Mark A Kramer. Modeling chemical processes using prior knowledge
and neural networks. AIChE Journal, 40(8):1328-1340, 1994.
Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Accelerating Eulerian
fluid simulation with convolutional networks. In Doina Precup and Yee Whye Teh (eds.), Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 3424-3433. PMLR, 06-11 Aug 2017.
Kiwon Um, Robert Brand, Fei Yun, Philipp Holl, and Nils Thuerey. Solver-in-the-loop: Learning
from differentiable physics to interact with iterative pde-solvers. In Neural Information Process-
ing Systems (NeurIPS), volume 33, pp. 6111-6122, 2020.
John von Neumann. Functional Operators (AM-22), Volume 2: The Geometry of Orthogonal
Spaces. Princeton University Press, 1950. ISBN 978-1-4008-8189-5. doi: https://doi.org/10.
1515/9781400881895.
Nils Wandel, Michael Weinmann, and Reinhard Klein. Learning incompressible fluid dynamics
from scratch - towards fast, differentiable fluid models that generalize. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=KUDUoRsEphu.
Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating
physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919,
2020.
Yuan Yin, Vincent Le Guen, Jeremie Dona, Emmanuel de Bezenac, Ibrahim Ayed, Nicolas Thome,
and Patrick Gallinari. Augmenting Physical Models with Deep Networks for Complex Dynamics
Forecasting. In ICLR, 2021.
Chih-Chieh Young, Wen-Cheng Liu, and Ming-Chang Wu. A physically based and machine learning
hybrid approach for accurate rainfall-runoff modeling during extreme typhoon events. Applied
Soft Computing, 53:205-216, 2017.
13
Published as a conference paper at ICLR 2022
A Distance
A. 1 Distance B etween Dynamics
We here give the definition of the distance d. Let u and v be two functions of L2 (Rp, Rp). We
consider the distance:
d(u,v) = EX〜PX ∣∣u(X) - V(X)k2	(18)
Naturally, eq. (18) verifies the triangle inequality, the symmetry and the positiveness. Moreover, in
this case, for all functions f, d(., f) is convex. Indeed, for u, v two functions, and λ ∈ [0, 1]:
d(λu +(1 - λ)v, f) = EX〜PX ∣∣λu(X) + (1 - λ)v(X) - f(X)∣∣2
=EX〜PX kλu(X) - λf (X) -(I- λf (X) + (I- λ)v(X)∣2
≤ λEX〜PX ku(X) - f (X)k2 + (I- λ)EX-PX Ilv(X) - f (X)∣2
Hence the convexity of d(., f). This consideration suffices to ensure the convexity of Sk and Su
defined in section 3.
A.2 Distance B etween Flows
Consider the ODE with X(t), X0 ∈ RP:
dXF=f(XX…=Xo
(19)
Equation (19) admits a unique solution as soon as f is Lipschitz. We note X? this solution. Then,
we can defined the flow φf of such ODE as :
[0,T]×RP → RP
t, X0 →φf(t,X0) =X?(t)
(20)
With the definition of eq. (20), we can define the distance between two flows of ODE as:
dφ(φu, φf) = EX0-PX0
τ∣φu(t,X0)
t0
- φf(t,X0)∣ dt
(21)
dφ is positive and symmetric. Let φu , φv be two flows, we have the triangle inequality:
dφ(φu, φf) = EX0-PX0
Zτ
t0
∣φu(t,X0)
EX0-PX0
t0
∣φu(t,X0)
≤ EX0-PX0
Zτ
t0
∣φu(t,X0)
φf (t, X0)∣ dt
φv (t, X0) + φv (t, X0) + φf (t, X0)∣ dt
φv(t,X0)∣ + ∣φv(t,X0)+φf(t,X0)∣ dt
—
—
—
≤ dφ(φv, φv) + dφ(φv, φf)
Let φf be fixed, we also have the convexity of dφ (., φf) with respect to the first argument. Indeed
for λ ∈ [0, 1]:
dφ(λφu + (1 - λ)φv, f) = EX0-PX0
EX0-PX0
∣λφu (t, X0) + (1 - λ)φv - φf (t, X0)∣ dt
t0
∣λφu (t, X0) + (1 - λ)φv - λφf (t, X0) - (1 - λ)φf (t, X0)∣ dt
t0
≤ λ dφ(φu, φv) + (1 - λ)dφ (φv, φf)
However, in this case the convexity is not ensured with respect to u and vThis is the reason why for
theoretical investigations, we consider the distance d instead of dφ .
Nonetheless, dφ(φu, φf) = 0 =⇒ φu = φf =⇒ u = f.
14
Published as a conference paper at ICLR 2022
B	Remark on Additive Decomposition
First, note that in the case of a metric space the decomposition as defined in eq. (2) always exists.
We now detail an intuition for the well-posedness of such decomposition.
Let Hk be a closed convex subset of functions of an Hilbert space (E, <, >), and f the function
we want to approximate with partial knowledge (represented by the space of hypothesis Hk). Then,
thanks to Hilbert projection lemma, we have the uniqueness of the minimizer of ming∈Hk kf - gk,
i.e it exists one unique hk ∈ Hk such that: ∀g ∈ Hk, kf - hk k ≤ kf - gk.
Finally, the additive decomposition hypothesis presents a remarkable advantage in the case of a
vector space. Indeed, if Hk is a (closed) vector space, let Hk⊥ be its supplementary in E, then we
have the uniqueness in the decomposition: f = fHk + fH⊥ , where fH⊥ ∈ Hk⊥ and fHk ∈ Hk .
The existence and uniqueness flowing directly from the additive decomposition hypothesis, this can
explain why such assumption is common when bridging ML and MB hypothesis.
C Upper Bounds
C.1 Derivation of Equation (4)
The first upper bound is a simple use of the triangle inequality:
d(h, f) = d(h, f) +d(hk,f) - d(hk, f)
≤ d(hk,f) + |d(h, f) -d(hk,f)|
≤ d(hk, f) + d(h, hk)
C.2 Derivation of Equation (5)
To derive the second upper bound, we assume that fkpr comes from an overall dynamics fpr obeying
the additive decomposition hypothesis of eq. (2) so that fpr and fkpr verifies: fpr = fkpr + fupr. First,
with computations similar to eq. (4), we have:
d(h, f) ≤ d(h, fpr) + d(fpr, f)	(22)
Then:
d(h,fpr)=d(h,fpr)+d(hk,fkpr)-d(hk,fkpr)
≤ d(hk,fpr) + ∣d(h,fpr) - d(hk,fpr)|
≤ d(hk,fpr) + ∣d(h,fpr) - d(h,fpr) - d(h,fpr) + d(hk,fpr)|
≤ d(hk,fpr) + ∣d(h,fpr) - d(h,fpr)| + ∣d(hk,fpr) - d(h, hk)|
≤ d(hk, fkpr) + d(fpr, fkpr) + d(h, hk)	(23)
Combining Equations (22) and (23), we retrieve eq. (5):
d(h, f) ≤d(hk,fkpr)+d(h,hk)+d(fpr,fkpr)+d(fpr,f)	(24)
and we have: Γ = d(fpr, fkpr) +d(fpr, f). Γ is a constant of the problem that cannot be optimized.
C.3 UPPER-BOUND USING AUXILIARY DYNAMICS fpr
Let fpr be the dynamics of model data, we can link up the error made by h on true data (following
dynamics f) and the error made by h on model data (with dynamics fpr) via:
d(h, f) ≤ d(h, fpr) + d(fpr, f)	(25)
Thus a pre-training on auxiliary data of dynamics fpr amounts to control the term d(h, fpr) in the
upper-bound of eq. (25).
15
Published as a conference paper at ICLR 2022
C.4 Self-Supervision
Let h = hk + hu be the function to learn and Gψ the recognition network providing an estimate θki
of the parameters from an initial sequence (Xti , . . . , Xti +k∆t). This learning setting corresponds to
how velocity fields are learned from consecutive measurements of the tracer fields T in section 4.2.
To compute d(hk, fkpr) in the case where fpr = h?, where h? = h?k +h?u is a learned model, we rely
on the computed θk associated to h?k (thanks for example to the algorithm of section 3.2 associated
to eq. (4)) to generate a synthetic dataset with achievable supervision in the space of the parameters
θk.
From a real initial sequence (Xti , . . . , Xti +k∆t), we can estimate the unknown parame-
ter θk associated to sequence i with the recognition network Gψ learned with h?, i.e
θk = Gψ (Xio,..., Xζ+k∆t). Then, integrating from the initial condition Xζ, We generate a tra-
jectory of known parameters θk with dynamics h? denoted by: Xi = (Xio,..., Xj). Sampling
the space of initial conditions, We obtain a synthetic dataset: ((X 1,θ1),..., (Xm, θm)) enabling us
to perform self-supervision for Gψ . Let θki be the parameters estimated by Gψ from the simulated
Xti, . . ., Xti+k∆t , we make the following approximation:
d(hk, fk) ≈
—
2
(26)
D Proofs
D. 1 NOTE ON THE CONVEXITY OF Sk AND Su
Convexity of Sk
Proof. Let u, v ∈ Sk :
d(tu + (1 - t)v, f) = ktu + (1 - t)v - fk = ktu - tf + (1 - t)v - (1 - t)fk
≤ tμι + (I - t)μι = μι
Hence the convexity of Sk.	□
Convexity of Su
Proof. Let t ∈ [0, 1] and u, v ∈ Su.
d(hk, hk + tu + (1 - t)v) = d(0, tu + (1 - t)v)
≤ td(u, 0) + (1 - t)d(v, 0)
≤ μ2
Hence the convexity of Su.	□
D.2 ODE Identification
Consider the following set: SA = {X (t) ∈ C1([0, T], Rp) such that: ∃A ∈ Mp,p(R), X 0 = AX },
where T > 0.
SA is not a convex set. Consider u and v in SA, and consider Au and Av so that u0(t) = Auu(t)
and v0(t) = Avv(t). For λ ∈]0, 1[: we have:
[λu + (1 - λ)v]0 = λu0 + (1 - λ)v0
= λAuu + (1 - λ)Avv
In general the last term is not equal to Aλu+(1-λ)v (λu + (1 - λ)v), for some matrix Aλu+(1-λ)v.
Thus SA is not a convex set. However, discretizing the trajectories and employing a simple integra-
tion scheme leads to considering the following cost function:
L(A) = X k (Xs(t + 1) — (A∆t + Id)XA(t))k2,	(27)
t
16
Published as a conference paper at ICLR 2022
As a least square regression problem, L(A) is convex with respect to A. A least square regres-
sion setting can also be recovered using more complex integration schemes, or several time steps
integration.
D.3 Proof for Well-posedness of Equation (7)
We set ourselves in the Hilbert space of squared integrable functions with the canonical scalar prod-
uct L2(Rp, Rp), <, > . For further consideration on such functional space we refer to (Droniou,
2001).
We assume that Hk hence Sk is convex and a relatively compact family of functions.
Convexity of Sk + Su Let S = Sk + Su = {f ∣∃fk ∈Sk,fu ∈Su,f = fk + fu}.
Let f, g ∈ S and λ ∈]0, 1[:
λf + (1 - λ)g = λfk + (1 - λ)gk + λfu + (1 - λ)gu ∈ Sk + Su
Hence the convexity of S .
Closeness of Su We show that Su is a closed set. Indeed, Su = g-1 ([0,μu]), where
g(u) = kuk, Because g is 1-Lipschitz (using the triangle inequality), g is continuous. Therefore
Su is closed set as the inverse image of a closed set by continuous function.
Sequential Limit We now show that S is a closed set thanks to the sequential characterisation: let
fn a converging sequence of elements of S and denote f its limit. We prove that fn converges in S .
Because ∀n, fn ∈ S, we have: fn = fkn + fun, where fun ∈ Su and fkn ∈ Sk.
Thanks to the relative compactness of Sk, we can extract a converging sub-sequence, of indexes nj,
from fkn so that fknj → fk ∈ Sk.
Because fn → f, the sub-sequence fnj converges: fnj → f.
By definition,	funj	is a sequence of	Su	and we also have that:	funj	=	fnj	-	fknj .	Because the
right member of the equation converges (as a sum of converging functions), the left member of the
equation converges i.e. funj converges.
since Su	is a closed set	funj	converges in	Su .	We write	fu	its limit. Therefore,	funj	=
fnj - fknj →f-fk =fu ∈Su.Hence,f=fu+fkwithfu ∈Suandfk ∈Sk.
Therefore S is a closed set.
Finally, we can apply Hilbert projection lemma on the closed convex set S and retrieve the unique-
ness of the minimizer of eq. (7).
Remark The relative compactness of a family of functions is a common assumption in functional
analysis. For example, in the study of differential equation Cauchy-Peano theorem provides the
existence to the solution of an oDE under the assumption of relative compactness.
Also, Ascoli theorem provides the relative compactness of a family of function F under
the hypothesis of the equi-continuity of F and the relative compactness of the image space
A(x) = {f(x)|f ∈ F}.
D.4 Proof of Proposition 2
We now set ourselves in the Hilbert space L2([0, T], Rp), <, > of squared integrable functions,
where <, > is the canonical scalar product of L2([0, T], Rp).
Proof. Let A be a given invertible matrix. We consider the following space SD = {X ∈
C1([0, T], Rp) such that: ∃D ∈ Rp, X0 = AX + D and X(t = 0) = X0}, where T > 0. We
show that SD is a closed convex set.
17
Published as a conference paper at ICLR 2022
Convexity Indeed, let λ ∈]0, 1[ and u, v ∈ SD. λu + (1 - λ)v is differentiable and:
[λu + (1 - λ)v)]0 = λu0 + (1 - λ)v0 = A(λu + (1 - λ)v) + D,
Where D = λDu + (1 - λ)Dv. Hence λu + (1 - λ)v ∈ SD.
Closeness via Affine-Space To prove the closeness of SD , we prove that it is an affine space of
finite dimension.
Let g the application that to any vector D ∈ Rd associate the solution XD.
Let D0 ∈ RD, we show that gD0 : D → g(D0 + D) - g(D0) is a linear application.
Naturally, for gD0 (0Rp) = 0L2. Then for D 6= 0Rp we have:
gD0 (D) = eAt (X0 + A-1 (D0 + D)) - A-1 (D0 + D) - eAt (X0 + A-1 (D0) + A-1D0
= eAtA-1D
Therefore gD0 is a linear function and g is an affine function.
Moreover, g is an injection. Indeed, if two functions are equals, then they have at most one inverse
image by g thanks to Cauchy-Lipschitz theorem. Therefore it defines a bijection of Rd in g(Rd).
Since, SD = g(Rd), SD is an affine space of dimension p and g is continuous in particular for the
canonical norm induced on L2([0, T], Rp). Therefore SD is an affine space of finite dimension and
is a closed set.
Finding a Unique Minimizer We conclude by applying Hilbert projection lemma: our problem
of minimizing R0T Xs(τ) - XD(τ), amounts to an orthogonal projection problem. Because SD
is a closed convex set, we have existence and uniqueness of such projection. Therefore, it exists a
unique function XD ∈ Sd and a unique vector D minimizing its distance to the function Xs.	□
D.5 Algorithm in Linear Setting
We detail in Algorithm 2 the alternate projection algorithm in a linear setting. We denote
Y = (Xti0+∆t,Xti0+n∆t) andX = (Xti0,Xti0+(n-1).∆t). For readability purposes we set ∆t = 1.
Algorithm 2 Alternate estimation: Linear Setting
Result: A ∈Mp,p(R), D ∈ Rp
k = 0, D0 = 0, A0-1 = 0A00 = minAkY -XAk
while kDk - Dk-1 k > and kAk - Ak-1k > do
Dk+1 = minD kY -XAk - Dk22 + λkDk22
Ak+1 = minA ∣∣Y - XA - Dk+1k2 + YkY - XA∣∣2
k - k + 1
end
D.6 Proof to Proposition 3
Naturally, one could estimate jointly D and A using least square regression. However, the idea is to
verify the convergence of such alternate algorithm in a simple case. We conduct the proof for the
first dimension of Y to lighten notations, meaning that we are regressing the first dimension of Y
against the X .
A similar reasoning for the other dimension completes the proof.
Proof. We first give the analytical solution for D. Let An be fixed.
Estimation of D Consider:
LD = kY-XAn-Dk22+λkDk22	(28)
where D = (d,..., d) ∈ RQ. For Q samples, We find d so that ∂d = 0:
18
Published as a conference paper at ICLR 2022
∂L	Q
∂d = O ⇔ -2 *〉：(yi — XiA — d) + 2λd = 0
i=1
Q
⇔Qd+λd=X(yi-XiAn)
i=1
Q
⇔ d(Q + λ) = X(yi - XiAn)
i=1
j Y - XA
⇔d =EQ
where Y - XA =吉 PQ=1(yi - XiAn).
Estimation of A Let D be fixed and consider:
LA= kY-XA-Dk22+YkY-XAk22
(29)
Similarly, we aim to cancel the first derivative of LA with respect to all parameters of A
(a1,.., ap):
∂La
daj
Q
0 ⇔ - 2 * Exij (yi - aoXi,o +-+ apXi,p - d)
i=1
Q
-2γ * Exij (yi - aoxi,o +-+ apxi,p) = 0
i=1
⇔-2Xt(Y-XA-D) - 2YXt(Y - XA) =0
⇔(1+Y)XtXA-Xt(Y-D) -YXtY=0
⇔(1 + γ)XtXA = Xt (YY +(Y - D))
B-1Xt
⇔a = 1-+ψ ((1 + Y)Y - D)
(30)
where B = XtX. Equation (30) indicates that as soon a D converges, An converges. Thus, we now
prove the convergence of (Dn). Then, for n > 1 consider:
UDn+1- Dnh = T⅛τd
1 + λ∕Q
1 + λ∕Q I
1
Y - XAn - Y - XAnT
X (An - An-1)
(1+λ∕Q)(1+Y)
1
XB-1Xt([(1 + Y)Y - Dn] - [(1 + Y)Y - Dn-1)]
(1+λ∕Q)(1+Y)
K
IXB-IXiDn-T
- Dn]
≤ (1 + λ∕Q)(1+ Y)
Dn-1 - Dn
where K = kXB-1Xtk.
Therefore, for λ,Y, sufficiently large,([+工/*.+))< 1.
kDn - Dn-1 k converges as a positive
decreasing sequence. Finally, the sequence of (Dn) converge and so the sequence of (An).
In conclusion, the proposed algorithm converges.	□
19
Published as a conference paper at ICLR 2022
E	Datasets
In this section, we provide exhaustive simulation details for the damped pendulum, Lotka-Volterra,
and both geophysical datasets.
E.1 Damped-Pendulum
For the damped pendulum data, eq. (15) is integrated with ∆t = 0.2s using a Runge-Kutta 4-
5 scheme from t = 0 up to t = 10s. Both the pulsation ω0 and the damping coefficient k are
fixed across the dataset. We generate 100/50/50 sequences respectively for train, validation and test
sampling over the initial conditions so that (θ, θ)〜U([-π∕2, π∕2] X [-0.1,0.1]).
Small Oscillations To linearize the pendulum, we consider the small oscillations regime and take
the initial conditions so that: (θ,θ)〜U([-π∕6,π∕6] × [-0.1,0.1]). In that case eq. (15) writes
as:
色仅｝ = (-λ
dt Θθ = I 1
(31)
-λ g
and following notations of section 3.3, we have: DA = 0 and A = ( ι L)
E.2 Lotka-Volterra
For Lotka-Volterra data, eq. (16) is integrated with ∆t = 0.05 using a Runge-Kutta 4-5 scheme
from t = 0 up to t = 20. All parameters α, β , γ, δ are set to 1 across the dataset. We generate
100/50/50 sequences respectively for train, validation and test sampling over the initial prey and
predators populations so that (x, y)〜 U [0, 2]2 .
Practical Issues and Adaptation Assuming that α and γ have positive values makes the following
problem arises: the trajectories defined by hk for the prey are unbounded, whereas the trajectories
defined by eq. (16) are. Minimizing d(hk, f) over long term horizon will lead in an underestimation
of α to match the bounded behaviour of true data. Therefore, we enforce d(hk, f) on the prey
component as soon as the number of predator is small. In practice, we set this threshold to 0.15.
E.3 Geophysical Datasets
We present in this section introductory tools for the understanding of the fluid dynamics data pre-
sented in section 4.2. We first introduce the physical modeling of ocean dynamics. Then, we outline
the Adv+S dataset simulation which draws from ocean modeling. Finally, we introduce the Natl
dataset and the proxy data used in the experiments.
Introduction To Ocean Modeling The increase in ocean observations thanks to satellites and
floats enabled a great development in Earth modeling over the last decades. The ocean circula-
tion, that is the current velocity fields dynamics, are now realistically modeled in tri-dimensional
structured models such as NEMO (Madec, 2008).
Such models rely on in-depth physical knowledge of the studied system and its representation
through partial differential equations. Integrated over depth, the equations associated to the transport
of the Sea Surface Temperature T by a time-varying horizontal velocity field U can be written as:
∂T
∂t
∂U
∂t
-V.(TU) + DT + F T
-(U.V)U +f ∧ U -g0Vh+ DU + FU
(32)
(33)
where f is the Coriolis parameter, h the depth of the surface layer obtained from sea surface height
(SSH) observations, g0 the reduced gravity which takes the stratification in density of the ocean into
account such that g0 ≈ g.10-3. In a two-dimensional setting, V(TU) refers to the advection of a
20
Published as a conference paper at ICLR 2022
scalar quantity T by a velocity field U = (u, V) and writes as: V(TU) = ∂TU + ∂Tv. The mixing
terms, referred to as DT/U and the forcings FT/U, are not known.
In the context of the presented work, the physical state Zt = (Tt, Ut), fX and fY from eq. (1) can
be interpreted as follows: fX represents the dynamics of the observed T, i.e. fX(T) = -V.(T U) +
DT + FT in eq. (32). fY represents the dynamics of U in eq. (33), i.e. fY (U, h) = -(U.V)U +
f∧U-g0Vh+DU+FU.
Whereas T is observed by satellites, U is not known. However, the Sea Surface Height (SSH) could
be used to compute coarse estimates of U. Indeed, under hypothesis such as stationarity (d∂U = 0),
incompressibility ((U.V)U = 0)), forcings can be omited. In this case, eq. (33) can be rewritten
into
f ∧U = -g0Vh	(34)
When projected onto x and y axis, eq. (34) becomes
0 __	0dh	f __	0dh	CG
-fv = -g -77-,	fu = -g -77~,	(35)
∂x	∂y
Note that eq. (34) and eq. (35) do not hold at fine scales as the stationarity and incompressibility
assumptions only hold at large scale. In this case, the SSH h can be regarded as a stream function.
Both datasets considered in the paper follow the same equations approximating the tracer equation
(eq. (17)) inspired by eq. (32):
∂T
由=-V.(TU)+ S
(36)
We study the equations 32 and 33 in an incremental approach. In the following parts, we describe
how T, U and S are computed in both datasets Adv+S and Natl.
E.3.1 ADV+S
We first investigate a dataset generated following simplifying assumptions (Adv+S). We don’t rely
on true U and S, we instead build them so that they correspond to our hypothesis.
Building a Velocity Field U Under stationarity and incompressibility hypothesis, U can be ap-
proximated from a stream function H. Note that, in this dataset, H is not equal to the SSH h, it is
simulated following (Boffetta et al., 2001):
H(x, y, t)
y - B(t) × cos kx
tanh[^^⊂ k /	] + Cy
ʌ/l + k2B(t)2 × sin2kx
(37)
As introduced precedently (see eq. (34)), eq. (33) can be simplified and we compute U = (u, v) so
that it follows:
U = -dy,	V = ∂H
(38)
Note that B varies periodically with time according to B = B0 + cos(ωt+ φ). We compute 10
different velocity fields sampling random parameters B0 , k, C, ω, , φ.
Building a Source Term S In eq. (32), the diffusion term DT is omitted. We generate the source
term S so that it represents the forcing term FT in eq. (36). To illustrate heat exchanges, we draw
from Frankignoul (1985). This source term is a non linear transformation of U = (U, V) multiplied
by the difference between the ocean temperature and a reference temperature:
―一	,一 一、	(0 if dH < 10—4
S(U,T) = We X (T — Te) where We = 4 0 ifId <
, e	e	e 1	otherwise.
where Te is the sequence mean image (computed without source).
Dataset Generation Using computed U and S, we integrate eq. (36) with ∆t = 8640s over
30 days, using a Semi-Lagrangian scheme (see explanations below). We generate 800/100/200
sequences respectively for train, validation and test sampling over the initial conditions, which are
images of size 64 × 64 sampled from Natl dataset. Finally, for integration, we impose East-West
periodic conditions, implying that what comes out the left part re-enters at the right, and reciprocally.
We also impose velocity to be null on both top and bottom parts of the image.
21
Published as a conference paper at ICLR 2022
Semi-Lagrangian Integration Unlike Eulerian scheme, relying on time discretization of the
derivative, the semi Lagrangian scheme relies on the constancy of the solution of a PDE along a
characteristic curve. Consider a solution to the advection equation, i.e. eq. (36) with S = 0. The
method of characteristics consists in exhibiting curves (x(s), t(s)) along which the derivative of the
solution T is simple, i.e ∂∂T(x(s),t(s)) = 0. For a 1D constant advection scheme, computations
lead to:
dt
ds
1 =⇒ s = t
dx
ds
U =⇒ x = x0 + Ut
giving therefore, T(x, t) = T0(x - Ut), linking the value of the solution at all time to its initial
condition. Therefore from a single observation at t0 , it suffices to estimate the original departure
points x0 - Ut to infer the prediction at t.
However, when U is not constant in time, the method remains doable, not along characteristic lines
defined by : (x0 + Ut), but along characteristic curves which are given by:
dt
ds
1 =⇒ s = t
dx
ds
U(x, t)
(39)
A great deal in the semi-Lagrangian literature involves solving correctly eq. (39). We use the con-
ventional mid-point integration rule and the semi-Lagrangian is implemented using Pytorch function
gridsample, following in (Jaderberg et al., 2015). Further developments can be found for exam-
ple in (Diamantakis, 2014).
E.3.2 NATL
This second dataset depicts the actual ocean circulation, i.e. we consider both eq. (32) and eq. (33).
In this case, no assumptions are made on U and S represents both diffusion terms D T and forcing
terms F T . We access daily data over a year of ocean surface temperature of the North Atlantic
observations model resulting from (Ajayi et al., 2019) 1. The dataset covers a 2300km × 2560km
zone at 1.5km resolution, in the North Atlantic Ocean.
In this real-life dataset, sea surface height (SSH) partial derivative with respect to x and y serves as
proxies to the (unobserved) velocity fields U. Indeed, recall that simplifying hypotheses led us to
eq. (35).
We divide the Natl zone into 270 patches of size 64 × 64. For each region, we extract sea surface
temperatures, velocity fields, source terms and height variables. We sample 200/20/50 sequences of
1 year, for respectively train, validation and test. In this case, ∆t = 86400s (1 day).
F Training Details
All experiments were conducted on NVIDIA TITAN X GPU using Pytorch (Paszke et al., 2019).
Hyper-Parameters Interpretation From eq. (4), two independent terms appear justifying an al-
ternate projections approach.
First, we highlight that strictly minimizing d(hk, f) biases our estimation of hk. However, it may
yield a good estimation of hk provided that fk contributes significantly to the prediction off. Hence,
we interpret this loss as an initialization loss. Thus, in most applications, we progressively decrease
its magnitude along training as detailed in appendices F.1 to F.3.
On the other hand, d(hu , 0) aims at constraining the free form function hu to make its action as
small as possible. We interpret this loss as a stability penalty.
1Details available at : https://meom-group.github.io/swot-natl60/access-data.html
22
Published as a conference paper at ICLR 2022
Finally, aiming to recover exact trajectories of observations, we proceed as suggested in (Yin et al.,
2021) progressively increasing the hyper-parameters associated to d(h, f).
The practical implementation is summarized in the following algorithm:
Algorithm 3 Alternate estimation: Practical Setting
Initialization: θu0 = 0, θk0 = minhk∈Hk d(hk, f), λh, λhk, λhu
for epoch = 1 : Nepochs do
for batch = 1 : Bk do
I	θn+1 = θn - evek [λhd(h,f)+λhk'(hk)]
end
for batch = Bk : Bu do
I	θn+1 = θn - τιVθu [λhd(h,f)+λhu d(hu, 0)]
end
λh = τ2λh; λhk = T2λhk; λhu = T2λhu
end
F.1 Damped Pendulum
Architecture Details The physical parameters to be learned is a scalar of dimension 1, and hu is
a 1-hidden layer MLP with 200-hidden neurons with leaky-relu activation.
Optimization For this dataset we use RMSProp optimizer with learning rate 0.0004 for 100
epochs with batch size 128. We supervise the trajectories up to t = ∆t × 50, i.e we enforce dφ
over (t0 + ∆t, .., t0 + 50∆t). Overall the number of optimization subsequences for training is
17000. We alternate projection on Sk and Su by descending the gradient 10-batches on hk then
10-batches on hu .
Hyperparameters We initialize λhk = 0.1 and decrease it geometrically down to λhk = 0.001.
We initialize λh = 0.1 and increase it geometrically up to λh = 100. λhu is fixed through training
at 0.1.
The hyper-parameters were chosen by randomly exploring the hyper-parameters space by sampling
them so that λ 〜U(1,0.1,..., 10-4). We select the ones with the lowest prediction errors, i.e with
lowest dφ(h, f).
For the ablation study of Table 1, we set to 0 the hyper-parameters associated to the non-considered
loss.
The training time for this dataset is 1 hour.
F.2 Lotka-Volterra
Architecture Details The physical parameters to be learned is a vector of dimension 2 accounting
for (α, β) in eq. (16), and hu is a 1-hidden layer MLP with 200-hidden neurons with leaky-relu
activation.
Optimization We use Adam optimizer with learning rate 0.0005 for 200 epochs with batch size
128. Overall the number of sequences for training is 15000. We supervise the trajectories up to
t = ∆t × 25, i.e we enforce dφ over (t0 + ∆t, .., t0 + 25∆t). We alternate projection on Sk and Su
by descending the gradient 10-batches on hk then 10-batches on hu.
Hyperparameters We initialize λhk = 0.1 and decrease it geometrically down to λhk = 0.001.
We initialize λh = 0.001 and increase it geometrically up to λh = 1. λhu is fixed through training
at 0.001.
23
Published as a conference paper at ICLR 2022
The hyper-parameters were chosen by randomly exploring the hyper-parameters space by sampling
them so that λ 〜U(1,0.1,..., 10-4). We select the ones with the lowest prediction errors (i.e
lowest d(h, f)).
For the ablation study of Table 1, we set to 0 the hyper-parameters associated to the non-considered
loss.
The training time for this dataset is 2 hours.
F.3 ADV+S
Architectures Details The physical parameters to be estimated are the velocity fields U, of dimen-
sion (2, 64, 64). As U varies over time, we follow data assimilation principles to map a sequence
of 4 consecutive measurements of the tracer field T to the associated velocity field (Gaultier et al.,
2013). To do so, we parameterize a recognition network Gψ by U-net with at most 512 latent chan-
nels also following the implementation of (Isola et al., 2017), taking as input a sequence of 4 time
steps of T: (Tt0, .., Tt0+3∆t). The residual dynamics hu is learned by a convolutional ResNet, with
1 residual block taking as entry the same sequence of T . We implement hk via a semi-lagrangian
scheme, taking as input Tt and the estimated Ut to predict Tt+1.
Optimization We use Adam optimizer with learning rate 0.0001 for 30 epochs with batch size 32.
We supervise the trajectories up to t = ∆t × 6, i.e we enforce dφ on (Tt0+∆t, ..., Tt0+6∆t). Overall
the number of sequences for training is 36800. We alternate projection on Sk and Su by descending
the gradient 4-batches on hk then 6-batches on hu .
Figure 3: Best viewed in color. Schematic view of our model in the context of section 5.2, applied on the
Adv+S dataset.
Hyperparameters, setting of eq. (4) We initialize λhk = 0.1 and decrease it geometrically down
to λhk = 0.00001. We initialize λh = 0.01 and increase it geometrically every epoch up to λh,f =
1000. λhu is fixed through training at 0.1. We select the hyperparameters with the lowest prediction
errors (i.e lowest d(h, f)). For the ablation study of Table 1, we set to 0 the hyper-parameters
associated to the non-considered loss.
The training time for this dataset is 8 hours.
F.4 Natl
Architecture Details The architectures in this setting are identical to the ones described in ap-
pendix F.3.
Optimization We use Adam optimizer with learning rate 0.00001 for 50 epochs with batch size
32. Overall the number of sequences for training is 67000. We enforce dφ over 6 time-steps, i.e we
supervise the predictions on timesteps: (t0 + ∆t, .., t0 + 6∆t). We use dropout in both Gψ and hu.
24
Published as a conference paper at ICLR 2022
Hyperparameters, setting of eq. (4) For this setting, λh geometrically increases from 0.01 up to
100. We initialize λhk = 0.1 and decrease it geometrically down to λhk = 0.00001. λhu is fixed
through training at 0.1. We alternate projection on Sk and Su by descending the gradient 10-batches
on both hk and hu .
The selected model is the one with lowest prediction errors on validation set (i.e lowest d(h, f)),
sampling uniformly the hyperparameters: λ 〜U(1,0.1,..., 10-4).
Hyperparameters, setting of eq. (5) Because the dynamics of Natl is highly non linear and
chaotic, we follow (Jia et al., 2019) and first warm-up the parameters recognition network Gψ on
the velocity fields proxies for 10 epochs. For this setting, λh geometrically increase from 0.01 up to
1. λhk is set equal to λh. λhu is fixed through training at 0.01.
After warm-up, we alternate projection on Sk and Su by descending the gradient 100-batches on
hk and 300 on hu. In this setting of eq. (5), the selected model is the one with lowest d(h, f) +
d(hk,fpr) error, sampling uniformly the hyperparameters: λ 〜U(1,0.1,..., 10-4).
The training time for this dataset is 12 hours.
Baselines We train NODE (Chen et al., 2018) and Aphynity (Yin et al., 2021) on both the Adv+S
and Natl dataset. For the training of Aphinity, we set the learning rate at 0.0001 and train on 30
epochs. We initialize λh = 0.01 and increase it geometrically every epoch up to λh = 100. λhu
is fixed through training at 0.1. For the training of NODE, we set the learning rate at 0.00004 and
train on 50 epochs. To perform prediction, we first encode the 4-consecutive measurements ofT (as
a 3 × 64 × 64 state) then learn to integrate this state in time thanks to a network h. h is a 3-layer
convolutional networks, with 64 hidden channels. It is integrated using RK4 scheme available from
https://github.com/rtqichen/torchdiffeq.
G Additional Results and S amples
G. 1 Results for Pendulum and Lotka-Volterra Datasets
We provide respectively in figs. 4 and 5 phase diagrams for the damped pendulum and Lotka-Volterra
experiments. Both graphs in the phase space indicate that the trajectories and their nature are well
handled by the learned decomposition, providing a periodic phase space for Lotka-Volterra (fig. 5),
and a converging spiral for the damped pendulum (fig. 4).
G.2 Results for Adv+S and Natl
In this section, we provide additionial results on both Adv+S and Natl datasets. A thorough abla-
tion study (table 4) gives results with constant hyperparameters λh and λhk (row Vanilla Optim),
which validates our hyper-parameters interpretation. Indeed, the results are better when respectively
increasing and decreasing λh and λhk. Besides, the row Ours eq. (5) refers to a training performed
as introduced in appendix C.4 with fpr = h? trained on eq. (4). Figure 7 shows predictions up to 4
days on the Adv+S data. Finally, figs. 9 and 11 provide results on Natl dataset associated to training
relying on both eq. (4) and eq. (5) and with NODE (Chen et al., 2018).
25
Published as a conference paper at ICLR 2022
Figure 4: Damped Pendulum Phase Diagram. The true phase diagram (blue) and learned (orange dashed) are
close, indicating consistency in the prediction
Figure 5: Lotka-Volterra Phase Diagram. The true phase diagram (blue) and learned (orange dashed) are close,
indicating consistency in the prediction
26
Published as a conference paper at ICLR 2022
Table 4: Ablation Study on Adv+S. We report the MSE (× 100) on the predicted observations T , the estimated
velocity fields U and the residual source term S over 6 and 20 time steps from an initial datum t0. Unlike
alternate training, i.e. Algorithm 1, “Joint” rows refer to the simultaneous optimization of hk and hu .
Training	Models	t0 + 6			t0 + 20		
		T	U	S	T	U	S
	Ours (U known)	0.52	n/a	0.19	2.0	n/a	0.32
	Ours eq. (4)	074	1.99	0.17	8.49	2.26	0.31
	only d(h, f)	1.02	4.08	0.19	10.59	4.19	0.32
Alternate	d(h, f) + d(hk, f)	1.02	3.66	0.19	11.42	3.84	0.34
	d(h, f) + d(h, hk)	0.77	2.38	0.19	9.5	2.45	0.34
	Ours eq. (5)	0.75	2.77	0.17	8.36	2.84	0.29
	Vanilla optim.	1.51	3.77	0.3	13.33	4.1	5.15
Joint	Ours eq. (4)	1.44	3.3	0.3	12.82	3.5	0.5
	only d(h, f)	1.38	6.96	0.39	11.9	7.09	0.54
Figure 6: Best viewed in color. Estimations, targets and differences between estimations and targets on T ,
U = (u, v) and S for Adv+S. Each column refers to a time step, ranging from 1 to 6 half-days. On the left,
true and estimated U = (u, v) over 6 time steps, and differences between targets and estimations. On the right,
prediction of T and S over 6 time steps, and differences between targets and estimations.
27
Published as a conference paper at ICLR 2022
Figure 7: Best viewed in color. Estimations and targets on T, U = (u, v) and S for Adv+S. Each column
refers to a time step, ranging from 1 to 8 half-days. On the left, sequence of T inputs (4 time steps). In the
middle, prediction of T , U = (u, v) and S over 8 time steps. On the right, true T , U and S over 8 time steps.
Figure 8: Best viewed in color. Estimations, targets and differences between estimations and targets on T ,
U = (u, v) and S for Adv+S. Each column refers to a time step, ranging from 1 to 5 half-days. On the left,
true T, U and S over 5 time steps.. In the middle, prediction of T, U = (u, v) and S over 8 time steps. On the
right, differences between targets and estimations.
u truth
□ estimated
(eq (4))
u estimated
㈣⑸)
v truth
v estimated
(Gq (4))
V estimated
(e<l ⑸)
Figure 9: Best viewed in color. Sequence of estimations on U = (u, v) for the Natl data. The second and third
row respectively refer to training according to eq. (4) and eq. (5). The loss term d(hk , fkpr) in eq. (5) enables
our model to learn more accurate velocity fields than when only trained following eq. (4).
28
Published as a conference paper at ICLR 2022
True SST
Ours eq.5
Neural-ODE
Figure 10: Best viewed in color. Sequence of prediction on T for the Natl data. Contrary to our model (row
eq. (5)), NODE (row Neural-ODE) struggles to predict any motion in T.
Figure 11: Best viewed in color. Sequence of prediction on T, u, v, S for the Natl data across 3 days trained
using proxy data according to eq. (5)
29