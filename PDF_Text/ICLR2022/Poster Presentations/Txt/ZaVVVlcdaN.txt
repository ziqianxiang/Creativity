Published as a conference paper at ICLR 2022
FedChain: Chained Algorithms for
Near-Optimal Communication Cost
in Federated Learning
Charlie Hou
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
charlieh@andrew.cmu.edu
Giulia Fanti
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, Pennsylvania, USA
gfanti@andrew.cmu.edu
Kiran K. Thekumparampil
Department of Electrical and Computer Engineering
University of Illinois at Urbana-Champaign
Champaign, Illinois, USA
thekump2@illinois.edu
Sewoong Oh
Allen School of Computer Science and Engineering
University of Washington
Seattle, Washington, USA
sewoong@cs.washington.edu
Ab stract
Federatedd learning (FL) aims to minimize the communication complexity of
training a model over heterogeneous data distributed across many clients. A
common approach is local update methods, where clients take multiple optimization
steps over local data before communicating with the server (e.g., FedAvg). Local
update methods can exploit similarity between clients’ data. However, in existing
analyses, this comes at the cost of slow convergence in terms of the dependence
on the number of communication rounds R. On the other hand, global update
methods, where clients simply return a gradient vector in each round (e.g., SGD),
converge faster in terms of R but fail to exploit the similarity between clients even
when clients are homogeneous. We propose FedChain, an algorithmic framework
that combines the strengths of local update methods and global update methods to
achieve fast convergence in terms of R while leveraging the similarity between
clients. Using FedChain, we instantiate algorithms that improve upon previously
known rates in the general convex and PL settings, and are near-optimal (via an
algorithm-independent lower bound that we show) for problems that satisfy strong
convexity. Empirical results support this theoretical gain over existing methods.
1 Introduction
In federated learning (FL) (McMahan et al., 2017; Kairouz et al., 2019; Li et al., 2020), distributed
clients interact with a central server to jointly train a single model without directly sharing their data
with the server. The training objective is to solve the following minimization problem:
minhF (x) = ɪ X Fi(X)i	(I)
xN
i=1
where variable x is the model parameter, i indexes the clients (or devices), N is the number of
clients, and Fi(x) is a loss that only depends on that client’s data. Typical FL deployments have two
properties that make optimizing Eq. (1) challenging: (i) Data heterogeneity: We want to minimize
the average of the expected losses Fi(x) = Ezi〜Di [f (x; zj] for some loss function f evaluated on
client data zi drawn from a client-specific distribution Di . The convergence rate of the optimization
depends on the heterogeneity of the local data distributions, {Di}iN=1, and this dependence is captured
by a popular notion of client heterogeneity, ζ2, defined as follows:
Z2 := max sup IIVF(x) — VFi(x)k2 .	(2)
i∈[N] x
1
Published as a conference paper at ICLR 2022
This captures the maximum difference between a local gradient and the global gradient, and is a
standard measure of heterogeneity used in the literature (Woodworth et al., 2020a; Gorbunov et al.,
2020; Deng et al., 2020; Woodworth et al., 2020a; Gorbunov et al., 2020; Yuan et al., 2020; Deng et al.,
2021; Deng & Mahdavi, 2021). (ii) Communication cost: In many FL deployments, communication
is costly because clients have limited bandwidths. Due to these two challenges, most federated
optimization algorithms alternate between local rounds of computation, where each client locally
processes only their own data to save communication, and global rounds, where clients synchronize
with the central server to resolve disagreements due to heterogeneity in the locally updated models.
Several federated algorithms navigate this trade-off between reducing communication and resolving
data heterogeneity by modifying the amount and nature of local and global computation (McMahan
et al., 2017; Li et al., 2018; Wang et al., 2019b;a; Li et al., 2019; Karimireddy et al., 2020b;a;
Al-Shedivat et al., 2020; Reddi et al., 2020; Charles & Konecny, 2020; Mitra et al., 2021; Woodworth
et al., 2020a). These first-order federated optimization algorithms largely fall into one of two camps:
(i) clients in local update methods send updated models after performing multiple steps of model
updates, and (ii) clients in global update methods send gradients and do not perform any model
updates locally. Examples of (i) include FedAvg (McMahan et al., 2017), SCAFFOLD (Karimireddy
et al., 2020b), and FedProx (Li et al., 2018). Examples of (ii) include SGD and Accelerated SGD
(ASG), where in each round r the server collects from the clients gradients evaluated on local data at
the current iterate x(r) and then performs a (possibly Nesterov-accelerated) model update.
As an illustrating example, consider the scenario when Fi 飞 are μ-strongly convex and β-smooth such
that the condition number is K = β∕μ as summarized in Table 1, and also assume for simplicity that
all clients participate in each communication round (i.e., full participation). Existing convergence
analyses show that local update methods have a favorable dependence on the heterogeneity ζ . For
example, Woodworth et al. (2020a) show that FedAvg achieves an error bound of O((KZ2∕μ)R-2)
after R rounds of communication. Hence FedAvg achieves theoretically faster convergence for
smaller heterogeneity levels ζ by using local updates. However, this favorable dependency on ζ
comes at the cost of slow convergence in R. On the other hand, global update methods converge
faster in R, with error rate decaying as O(∆ exp(-R/√κ)) (for the case of ASG), where ∆ is the
initial function value gap to the (unique) optimal solution. This is an exponentially faster rate in R,
but it does not take advantage of small heterogeneity even when client data is fully homogeneous.
Our main contribution is to design a novel family of algorithms that combines the strengths of local
and global methods to achieve a faster convergence while maintaining the favorable dependence on
the heterogeneity, thus achieving an error rate of O((Z2∕μ) exp(-R/√κ)) in the strongly convex
scenario. By adaptively switching between this novel algorithm and the existing best global method,
we can achieve an error rate of O(mm{∆, Z2∕μ} exp(-R∕√κ)). We further show that this is near-
optimal by providing a matching lower bound in Thm. 5.4. This lower bound tightens an existing
one from (Woodworth et al., 2020a) by considering a smaller class of algorithms that includes those
presented in this paper.
We propose FedChain, a unifying chaining framework for federated optimization that enjoys the
benefits of both local update methods and global update methods. For a given total number of rounds
R, FedChain first uses a local-update method for a constant fraction of rounds and then switches
to a global-update method for the remaining rounds. The first phase exploits client homogeneity
when possible, providing fast convergence when heterogeneity is small. The second phase uses the
unbiased stochastic gradients of global update methods to achieve a faster convergence rate in the
heterogeneous setting. The second phase inherits the benefits of the first phase through the iterate
output by the local-update method. An instantiation of FedChain consists of a combination of a
specific local-update method and global-update method (e.g., FedAvg → SGD).
Contributions. We propose the FedChain framework and analyze various instantiations under
strongly convex, general convex, and nonconvex objectives that satisfy the PL condition.1 Achievable
rates are summarized in Tables 1, 2 and 4. In the strongly convex setting, these rates nearly match the
algorithm-independent lower bounds we introduce in Theorem 5.4, which shows the near-optimality of
FedChain. For strongly convex functions, chaining is optimal up to a factor that decays exponentially
in the condition number K. For convex functions, it is optimal in the high-heterogeneity regime, when
1F satisfies the μ-PL condition if 2μ(F(x) — F(x*)) ≤ ∣∣VF(x)∣∣2.
2
Published as a conference paper at ICLR 2022
Algorithm 1 Federated Chaining (FedChain)
Input: Alocal local update algorithm, Aglobal centralized algorithm, initial point X°, K, rounds R
.Run AloCal for R/2 rounds
x1∕2 J AloCal(XO)
.Choose the better point between Xo and X1/2
Sample S clients S ⊆ [N]
DraW Zi,k 〜Di, i ∈S, k ∈{0,...,K — 1}
χι J arg minχ∈{X0,X1∕2} SK Pi∈s PK=o1 f(x; zi,k)
. Finish convergence with Aglobal for R/2 rounds
X2 J Aglobal(Xl)
Return X 2
Z > βDR1/2. For nonconvex PL functions, it is optimal for constant condition number κ. In all
three settings, FedChain instantiations improve upon previously knoWn Worst-case rates in certain
regimes of ζ . We further demonstrate the empirical gains of our chaining frameWork in the convex
case (logistic regression on MNIST) and in the nonconvex case (ConvNet classification on EMNIST
(Cohen et al., 2017) and ResNet-18 classification on CIFAR-100 (Krizhevsky, 2009)).
1.1	Related Work
The convergence of FedAvg in convex optimization has been the subject of much interest in the
machine learning community, particularly as federated learning (Kairouz et al., 2019) has increased in
popularity. The convergence of FedAvg for convex minimization Was first studied in the homogeneous
client setting (Stich, 2018; Wang & Joshi, 2018; WoodWorth et al., 2020b). These rates Were
later extended to the heterogeneous client setting (Khaled et al., 2020; Karimireddy et al., 2020b;
WoodWorth et al., 2020a; Koloskova et al., 2020), including a loWer bound (WoodWorth et al., 2020a).
The only current knoWn analysis for FedAvg that shoWs that FedAvg can improve on SGD/ASG
in the heterogeneous data case is that of WoodWorth et al. (2020a), Which has been used to prove
slightly tighter bounds in subsequent Work Gorbunov et al. (2020).
Many neW federated optimization algorithms have been proposed to improve on FedAvg and
SGD/ASG, such as SCAFFOLD (Karimireddy et al., 2020b), S-Local-SVRG (Gorbunov et al.,
2020), FedAdam (Reddi et al., 2020), FedLin (Mitra et al., 2021), FedProx (Li et al., 2018). HoWever,
under full participation (Where all clients participate in a communication round; otherWise the setting
is partial participation) existing analyses for these algorithms have not demonstrated any improve-
ment over SGD (under partial participation, SAGA (Defazio et al., 2014)). In the smooth nonconvex
full participation setting, MimeMVR (Karimireddy et al., 2020a) Was recently proposed, Which
improves over SGD in the loW-heterogeneity setting. Currently MimeMVR and SGD are the tWo best
algorithms for Worst-case smooth nonconvex optimization With respect to communication efficiency,
depending on client heterogeneity. In partial participation, MimeMVR and SAGA are the tWo best
algorithms for Worst-case smooth nonconvex optimization, depending on client heterogeneity.
Lin et al. (2018) proposed a scheme, post-local SGD, opposite ours by sWitching from a global-
update method to a local-update method (as opposed to our proposal of sWitching from local-update
method to global-update method). They evaluate the setting Where ζ = 0. The authors found that
While post-local SGD has a training accuracy Worse than SGD, the test accuracy can be better (an
improvement of 1% test accuracy on ResNet-20 CIFAR-10 classification), though theoretical analysis
Was not provided. Wang & Joshi (2019) decrease the number (K) of local updates to transition from
FedAvg to SGD in the homogeneous setting. This is conceptually similar to FedChain, but they do
not shoW order-Wise convergence rate improvements. Indeed, in the homogeneous setting, a simple
variant of ASG achieves the optimal Worst-case convergence rate (WoodWorth et al., 2021).
2	Setting
Federated optimization proceeds in rounds. We consider the partial participation setting, Where at the
beginning of each round, S ∈ Z+ out of total N clients are sampled uniformly at random Without
3
Published as a conference paper at ICLR 2022
replacement. Between each global communication round, the sampled clients each access either (i)
their own stochastic gradient oracle K times, update their local models, and return the updated model,
or (ii) their own stochastic function value oracle K times and return the average value to the server.
The server aggregates the received information and performs a model update. For each baseline
optimization algorithm, we analyze the sub-optimality error after R rounds of communication between
the clients and the server. SUboPtimality is measured in terms of the function value EF(X) - F(x*),
where X is the solution estimate after R rounds and x* = arg minx F(x) is a (possibly non-UniqUe)
oPtimum of F. We let the estimate for the initial suboPtimality gaP be ∆ (AssumPtion B.9), and the
initial distance to a (not necessarily unique) optimum be D (Assumption B.10). If applicable, is the
target expected function value suboptimality.
We study three settings: strongly convex Fi's, convex Fi,s and μ-PL F; for formal definitions, refer
to App. B. Throughout this paper, we assume that the Fi ’s are β-smooth (Assumption B.4). If Fi ’s
are μ-strongly convex (Assumption B.1) or F is μ-PL (Assumption B.3), we denote K = β∕μ as the
condition number. Di is the data distribution of client i. We define the heterogeneity of the problem
as Z2 := maxi∈[N] Supx ∣∣VF(x) — NFi(x)k2 in Assumption B.5. We assume unless otherwise
specified that ζ2 > 0, i.e., that the problem is heterogeneous. We assume that the client gradient
variance is upper bounded by σ2 (Assumption B.6). We also define the analogous quantities for
function value oracle queries: ZF Assumption B.8, σF Assumption B.7. We use the notation O, Ω to
hide polylogarithmic factors, and O, Ω if we are only hiding constant factors.
3	Federated Chaining (FedChain) Framework
We start with a toy example (Fig. 1) to illus-
trate FedChain. Consider two strongly con-
vex client objectives: F1 (x) = (1∕2)(x - 1)2
and F2 (x) = (x + 1)2. The global objective
F(x) = (F1 (x) + F2 (x))∕2 is their average.
Fig. 1 (top) plots the objectives, and Fig. 1 (bot-
tom) displays their gradients. Far from the op-
timum, due to strong convexity, all client gradi-
ents point towards the optimal solution; specifi-
cally, this occurs when x ∈ (-∞, -1] ∪ [1, ∞).
In this regime, clients can use local steps (e.g.,
FedAvg) to reduce communication without sacri-
ficing the consistency of per-client local updates.
On the other hand, close to the optimum, i.e.,
when x ∈ (-1, 1), some client gradients may
point away from the optimum. This suggests
that clients should not take local steps to avoid
driving the global estimate away from the op-
timum. Therefore, when we are close to the
optimum, we use an algorithm without local
steps (e.g. SGD), which is less affected by client
gradient disagreement.
Figure 1: A toy example illustrating FedChain. We
have two client objectives: F1(x) and F2(x). F(x)
is their average. The top plot displays the objec-
tives and the bottom plot displays the gradients. In
regions where client gradients agree in direction,
i.e. (-∞, -1] ∪ [1, ∞) it may be better to use an
algorithm with local steps (like FedAvg), and in
the region where the gradient disagree in direction,
i.e. (-1, 1) it may be better to use an algorithm
without local steps (like SGD).
This intuition seems to carry over to the non-
convex setting: Charles et al. (2021) show that
over the course of FedAvg execution on neural
network StackOverflow next word prediction,
client gradients become more orthogonal.
FedChain: To exploit the strengths of both local
and global update methods, we propose the fed-
erated chaining (FedChain) framework in Alg. 1.
There are three steps: (1) Run a local-update
method Alocal, like FedAvg. (2) Choose the bet-
ter point between the output of AloCal (which we denote X1/2), and the initial point X0 to initialize (3)
a global-update method Aglobal like SGD. Note that when heterogeneity is large, Alocal can actually
4
Published as a conference paper at ICLR 2022
Table 1: Rates for the strongly convex case. * Rate requires R ≥ N/S. ♦ Rate requires S = N.
Method/Analysis	,, ~ ,. EF(X) — F(x*) ≤O(∙)
Centralized Algorithms	
SGD ASG	△ exp(一κTR) + (1 — NS) μSR ∆eχp(一κ-2 R) + (1 — S) μSR	
Federated Algorithms	
FedAvg (Karimireddy et al., 2020b) FedAvg (Woodworth et al., 2020a) SCAFFOLD (Karimireddy et al., 2020b)	△ exp(—κ-1R) + κ( ζμ )R-2 K 仔)r-2* △ exp(— min{κ-1, S}R)*
This paper	
FedAvg → SGD (Thm. 4.1) FedAvg → SAGA (Thm. 4.3) FedAvg → ASG (Thm. 4.2) FedAvg → SSNM (Thm. 4.4) Algo.-independent LB (Thm. 5.4)	22 min{A ⅛} exp(一κ— R) +(I — N) μsR min{∆, ζμ} exp(— min{κ-1, S}R)* min{A ζμ} exp(-K-2 R) + (I — S) μsR K mm。, ζμ2} eχp(-min{ qNSκ, S }R)* 3 Z2IC min{∆,κ 2 (ζ-)} eχp(-κ 2 R)
output an iterate with higher suboptimality gap than X°. Hence, selecting the point (between Xo
and X1/2) with a smaller F allows us to adapt to the problem,s heterogeneity, and initialize Aglobal
appropriately to achieve good convergence rates. To compare X1/2 and the initial point Xo, We
approximate F(Xo) and F(X1/2) by averaging; We compute SK Pi∈S,k∈κ f (x; Zi,k) for X1∕2,X0,
where K is also the number of local steps per client per round in Alocal, and S is a sample of S
clients. In practice, one might consider adaptively selecting how many rounds of Alocal to run, which
can potentially improve the convergence by a constant factor. Our experiments in App. J.1 show
significant improvement when using only 1 round of Alocal with a large enough K for convex losses.
4	Convergence of FedChain
We first analyze FedChain (Algo. 1) when Alocal is FedAvg and Aglobal is (Nesterov accelerated) SGD.
The first theorem is without Nesterov acceleration and the second with Nesterov acceleration.
Theorem 4.1 (FedAvg → SGD). Suppose that client objectives Fi ’s and their gradient queries
satisfy Assumptions B.4, B.5, B.6, B.7 and B.8. Then running FedChain (Algo. 1) with Alocal as
FedAvg (Algo. 4) with the parameter choices of Thm. E.1, and Aglobal as SGD (Algo. 2) with the
parameter choices of Thm. D.1, we get the following rates 2:
•	Strongly convex: If Fi's satisfy Assumption B.1 for some μ > 0 then there exists a fi-
nite K above Which we get, EF(x2) 一 F(x*) ≤ O(mm{∆,Z2∕μ} exp(-R∕κ) + (1 —
S/N)Z2∕(μSR)).
•	General convex: If Fi，s satisfy Assumption B.2, then there exists a finite K
above which we get the rate EF(X2) — F(x*) ≤ O(min{βD2∕R, PβZD3∕√R} 十
(P 1- S/N)(PβζD3∕√SR)).
•	PL condition: If Fi ,s satisfy Assumption B.3for μ > 0, then there exists a finite K above
which we get the rate the rate EF(X2) — F(x*) ≤ O(min{∆,Z2∕μ} exp(-R∕κ) + (1 —
S∕N)(κZ2∕μSR)).
2We ignore variance terms and ζF2 = maxi∈[N] supx(Fi(x) - F (x))2 as the former can be made negligible
by increasing K and the latter can be made zero by running the better of FedAvg → SGD and SGD instead of
choosing the better of X1/2 and Xo as in Algo. 1. Furthermore, ZF terms are similar to the ζ2 terms. The rest of
the theorems in the main paper will also be stated this way. To see the formal statements, see App. F.
5
Published as a conference paper at ICLR 2022
Table 2: Rates for the general convex case. * Rate requires R ≥ S. ♦ Rate requires S = N. ♦ Analysis from Karimireddy et al. (2020b).		
Method/Analysis	EF(X) - F(x*) ≤ O(∙)	
Centralized Algorithms		
SGD	警+ /	-^S ZD 	 	 N √SR
ASG	βD~+q	-^S ZD 	 	 N √SR	
Federated Algorithms		
FedAvg	βD2 + 3∕βζ2D4 + ʌ ∕1 - S SD R + V R2 + V 1 N √SR	
FedAvg (Woodworth et al., 2020a)	√βζRD4	
SCAFFOLD	q喑*	
This paper		
FedAvg → SGD (Thm. 4.1)	min{ βD2,：	√βZD3 ɔ , 4 A~~S√βZD3 ~√R~}+ V1 - N ~^SR~
FedAvg → ASG (Thm. 4.2)	min{βD22, √	^ζD3} + q /1—ɪ √ζD3 + q∕1-ɪ 立 R } + V 1 N	√SR + V 1 N √SR
Algo.-independent LB (Thm. 5.4)	min{ βD2 ,二	ZD- } √R5 }	
For the formal statement, see Thm. F.1.
Theorem 4.2 (FedAvg → ASG). Under the hypotheses of Thm. 4.1 with a choice of Aglobal as ASG
(Algo. 3) and the parameter choices of Thm. D.3, we get the following guarantees:
•	Strongly convex: If Fi S satisfy Assumption B.1 for μ > 0 then there exists a finite K
above which we get the rate, EF(X2) 一 F(x*) ≤ O(min{∆,Z2∕μ} exp(-R∕√K) + (1 —
S/N )Z 2∕μSR).
•	General convex: If Fi ,s satisfy Assumption B.2 then there exists a finite K
above which we get the rate, EF(X2) — F(x*) ≤ O(min{βD2∕R2, pβZD3∕R} +
pr-s/N (ZD∕√SR) + P1- S/N (PβZD3∕ √SR)).
For the formal statement, see Thm. F.2. We show and discuss the near-optimality of FedAvg →
ASG under strongly convex and PL conditions (and under full participation) in Section 5, where
We introduce matching lower bounds. Under strong-convexity shown in Table 1, FedAvg → ASG,
when compared to ASG, converts the ∆exp(-R∕√κ) term into a min{∆, Z2∕μ} exp(-R∕√κ)
term, improving over ASG when heterogeneity moderately small: Z2∕μ < ∆. It also significantly
improves over FedAvg, as min{∆, Z2∕μ} exp(-R∕√κ) is exponentially faster than K(Z 2∕μ)R-2.
Under the PL condition (which is not necessarily convex) the story is similar (Table 4), except we use
FedAvg → SGD as our representative algorithm, as Nesterov acceleration is not known to improve
under non-convex settings.
In the general convex case (Table 2), let β = D = 1 for the purpose of comparisons. Then FedAvg →
ASG,s convergence rate is min{1∕R2, Z"∕R} + ,1 - S∕N(Z∕√SR) + P1 - S∕N√ζ∕√SR.
If Z < R, then Z"∕R < 1∕R2 and if Z < PS∕R7, then P1 - S∕N√Z∕√SR < 1∕R2, so the
FedAvg → ASG convergence rate is better than the convergence rate of ASG under the regime
Z < min{1∕R2, ,S∕R7}. The rate of Karimireddy et al. (2020b) for FedAvg (which does not
require S = N) is strictly worse than ASG, and so has a worse convergence rate than FedAvg
→ ASG if Z < min{1∕R2, pS∕R7}. Altogether, if Z < min{1∕R2, PS∕R7}, FedAvg → ASG
achieves the best known worst-case rate. Finally, in the S = N case, FedAvg → ASG does not have a
regime in Z where it improves over both ASG and FedAvg (the analysis of Woodworth et al. (2020a),
which requires S = N) at the same time. It is unclear if this is due to looseness in the analysis.
Next, we analyze variance reduced methods that improve convergence when a random subset of the
clients participate in each round (i.e., partial participation).
6
Published as a conference paper at ICLR 2022
■ ASG
-FedAvg
----SCAFFOLD
"b FedAvg->SGD
-+ SCAFFOLD->SGD
-H FedAvg-> ASG
"÷' SCAFFOLD->ASG
-Φ M-(FedAvg->SGD)
' M-(SCAFFOLD->SGD)
♦ M-(FedAvg->ASG)
-⅜ M-(SCAFFOLD->ASG)
Figure 2: Plot titles denote data homogeneity (§ 6). "X→Y'' denotes a FedChain instantiation with X
as Alocal and Y as Aglobal, circle markers denote stepsize decay events and plusses denote switching
from Alocal to Aglobal. Across all heterogeneity levels, the multistage algorithms perform the best.
Stepsize decayed baselines are left out to simplify the plots; we display them in App. J.2.
Theorem 4.3 (FedAvg → SAGA). Suppose that client objectives Fi ’s and their gradient queries
satisfy Assumptions B.4, B.5, B.6, B.7 and B.8. Then running FedChain (Algo. 1) with Alocal as
FedAvg (Algo. 4 ) with the parameter choices of Thm. E.1, and Aglobal as SAGA (Algo. 5) with the
parameter choices of Thm. D.4, we get the following guarantees as long as R ≥ Ω( N):
•	Strongly convex: If Fi s satisfy Assumption B.1 for μ > 0, then there exists a finite K above
which we get the rate EF(X2) - F(x*) ≤ O(min{∆,Z2/μ} exp(-min{S∕N, 1∕κ}R)).
•	PL condition: If Fi s satisfy Assumption B.3for μ > 0, then there exists a finite K above
2
which we get the rate EF(x2) — F(x ) ≤ O(mm{∆,Z2∕μ} exp(-(S∕N) 3R∕κ)).
For the formal statement, see Thm. F.3.
Theorem 4.4 (FedAvg → SSNM). Suppose that client objectives Fi ’s and their gradient queries
satisfy Assumptions B.4, B.5, B.6 and B.8. Then running FedChain (Algo. 1) with Alocal as FedAvg
(Algo. 4) with the parameter choices of Thm. E.1, and Aglobal as SSNM (Algo. 6) with the parameter
choices OfThm. D.5, we get the following guarantees as long as R ≥ Ω( N):
•	Strongly convex: If Fi s satisfy Assumption B.1 for μ > 0, then there exists a finite
K (same as in FedAvg → SGD) above which we get the rate EF(X2) — F(x*) ≤
O(min{∆, Z2∕μ} exp(- min{S∕N, PST(NK)}R)).
For the formal statement, see Thm. F.4. SSNM (Zhou et al., 2019) is the Nesterov accelerated version
of SAGA (Defazio et al., 2014).
The main contribution of using variance reduced methods in Aglobal is the removal of the sam-
pling error (the terms depending on the sampling heterogeneity error (1 - S∕N)(ζ2∕S)) from
the convergence rates, in exchange for requiring a round complexity of at least N∕S. To il-
lustrate this, observe that in the strongly convex case, FedAvg → SGD has convergence rate
min{∆, Z2∕μ} exp(-R∕κ) + (1-S∕N)(Z2∕SR) andFedAvg → SAGA (FedAVg → SGD's variance-
reduced counterpart) has convergence rate min{∆, Z2∕μ} exp(- min{1 ∕κ, S∕N}R), dropping the
(1 — S∕N)(Z2∕μSR) sampling heterogeneity error term in exchange for harming the rate of linear
convergence from 1∕κ to min{1∕κ, S∕N}.
This same tradeoff occurs in finite sum optimization (the problem minx (1∕n) in=1 ψi (x), where
ψi ’s (typically) represent losses on data points and the main concern is computation cost), which is
what variance reduction is designed for. In finite sum optimization, variance reduction methods such
as SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014; Reddi et al., 2016) achieve
linear rates of convergence (given strong convexity of ψi ’s) in exchange for requiring at least N∕S
updates. Because we can treat FL as an instance of finite-sum optimization (by viewing Eq. (1) as a
finite sum of objectives and Z2 as the variance between Fi ’s), these results from variance-reduced
finite sum optimization can be extended to federated learning. This is the idea behind SCAFFOLD
(Karimireddy et al., 2020b).
It is not always the case that variance reduction in Aglobal achieves better rates. In the strongly convex
case if (1 — S∕N)(Z2∕μS6) > N∕S, then variance reduction gets gain, otherwise not.
7
Published as a conference paper at ICLR 2022
5	Lower bounds
Our lower bound allows full participation. It assumes deterministic gradients and the following class
from (Woodworth et al., 2020a; Woodworth, 2021; Carmon et al., 2020):
Definition 5.1. For a v ∈ Rd, let supp(v) = {i ∈ [d] : vi 6= 0}. An algorithm is distributed
(r)
zero-respecting if for any i, k, r, the k-th iterate on the i-th client in the r-th round xi,k satisfy
SUPP(Xirk)) ⊆ U SUPP(VFi(X(rkO))	U	SUPP(VFiO(Xy,0Q	⑶
0≤k0<k	i0∈[N],0≤k0≤K-1,0≤r0<r
DiStribUted zero-reSPecting algorithmS’ iterateS have comPonentS in coordinateS that they have any
information on. AS diScUSSed in (Woodworth et al., 2020a), thiS meanS that algorithmS which are not
diStribUted zero-reSPecting are jUSt “wild gUeSSing”. AlgorithmS that are diStribUted zero-reSPecting
inclUde SGD, ASG, and FedAvg. We aSSUme the following claSS of algorithmS in order to boUnd the
heterogeneity of oUr conStrUction for the lower boUnd Proof.
Definition 5.2. We Say that an algorithm iS distributed distance-conserving if for any i, k, r, we have
for the k-th iterate on the i-th client in the r-th round x(r) satisfies ∣∣x(r) 一 x*∣∣2 ≤ (c∕2)[∣∣Xinit 一
x*k2 + PN=I IlXinit — x*∣2], where x* := argmi% Fj(x) and x* := argmi% F(x) and Xinit is the
initial iterate, and c is some scalar Parameter.
Algorithms which do not satisfy Definition 5.2 with c at most logarithmic in Problem Parameters (see
§ 2) are those that move sUbstantially far away from X*, even farther than the Xi*’s are from X*. With
this definition in mind, we slightly overload the UsUal definition of heterogeneity for the lower boUnd:
Definition 5.3. A distribUted oPtimization Problem is (ζ, c)-heterogeneous if
maxi∈[N ] supx∈A ∣VFi(X) 一 VF (X)∣2 ≤ ζ2, where we define A := {X : ∣X 一 X* ∣2 ≤
(c∕2)(∣Xinit 一 X* ∣2 + PiN=1 ∣Xinit 一 X* ∣2)} for some scalar Parameter c.
Those achievable convergence rates in FL that assUme Eq. (2) can be readily extended to accoUnt for
Definition 5.3 as long as the algorithm satisfies Definition 5.2. We show that the chaining algorithms
we ProPose satisfy Definition 5.3 in Thm. D.1, Thm. D.3, and Thm. E.1 for c at most Polylogarithmic
in the Problem Parameters defined in § 2. 3 *. Other FL algorithms also satisfy Definition 5.2, notably
FedAvg analyzed in Woodworth et al. (2020a) for c an absolUte constant, which is the cUrrent tightest
known analysis of FedAvg in the fUll ParticiPation setting.
Theorem 5.4. For any number of rounds R, number of local steps per-round K, and (ζ, c)-
heterogeneity (Definition 5.3), there exists a global objective F which is the average of two β-smooth
(Assumption B.4) and μ(≥ 0)-strongly convex (Assumption B.1) quadratic client objectives Fi and
F2 with an initial SUb-OPtimality gap of ∆, SUCh that the output X ofany distributed zero-respecting
(Definition 5.1) and distance-conserving algorithm (Definition 5.2) satisfies
•	Strongly convex: F(X) 一 F(x*) ≥ Ω(min{∆, 1∕(cκ3/2)(Z2∕β)} exp(-R∕√κ).) when
μ > 0, and
•	General Convex F(X) 一 F(x*) ≥ Ω(min{βD2∕R2,ZD∕(c"√R5)}) when μ = 0.
Corollary 5.5. Under the hypotheses of Theorem 5.4, there exists a global objective F which is μ-PL
and satisfies F(X) — F(x*) ≥ Ω(min{∆, 1∕(cκ3S) (Z2∕β)} exp(-R∕√K)).
A Proof of the lower boUnd is in APP. G, and the corollary follows immediately from the fact that
μ-strong convexity imPlies μ-PL. This result tightens the lower bound in Woodworth et al.(2020a),
which Proves a similar lower boUnd bUt in terms ofa mUch larger class of fUnctions with heterogeneity
bounded in ζ* = (1∕N) PiN=1 ∣VFi(X*)∣2. To the best of our knowledge, all achievable rates that
can take advantage of heterogeneity require (ζ, c)-heterogeneity (which is a smaller class than ζ*-
heterogeneity), which are incomParable with the existing lower bound from (Woodworth et al., 2020a)
that requires ζ* -heterogeneity. By introducing the class of distributed distance-conserving algorithms
3We do not formally show it for SAGA and SSNM, as the algorithms are functionally the same as SGD and
ASG under full ParticiPation.
8
Published as a conference paper at ICLR 2022
Table 3: Test accuracies (↑). “Constant” means the stepsize does not change during optimization,
while “w/ Decay” means the stepsize decays during optimization. Left: Comparison among algo-
rithms in the EMNIST task. Right: Comparison among algorithms in the CIFAR-100 task. “SCA.”
abbreviates SCAFFOLD.__________________________ _______________________________________________
Algorithm	Constant	w/ Decay	Algorithm	Constant	w/ Decay
Baselines			Baselines		
SGD	-^0.7842	0.7998	SGD	-^03987^^	0.1968
FedAvg	0.8314	0.8224	FedAvg	0.4944	0.5059
SCAFFOLD	0.8157	0.8174	SCAFFOLD	一	—
FedChain			FedChain		
FedAvg → SGD	^^0.8501^^	0.8355	FedAvg → SGD	-^0.5134	0.5167
SCA. → SGD	0.8508	0.8392	SCA. → SGD	一	—
(which includes most of the algorithms we are interested in), Thm. 5.4 allows us, for the first time, to
identify the optimality of the achievable rates as shown in Tables 1, 2, and 4.
Note that this lower bound allows full participation and should be compared to the achievable rates
with S = N ; comparisons with variance reduced methods like FedAvg → SAGA and FedAvg →
SSNM are unnecessary. Our lower bound proves that FedAvg → ASG is optimal UP to condition
number factors shrinking exponentially in √κ among algorithms satisfying Definition 5.2 and
Definition 5.3. Under the PL-condition, the situation is similar, except FedAvg → SGD loses √κ in
the exponential versus the lower bound. On the other hand, there remains a substantial gap between
FedAvg → ASG and the lower bound in the general convex case. Meaningful progress in closing the
gap in the general convex case is an important future direction.
6	Experiments
We evaluate the utility of our framework on strongly convex and nonconvex settings. We compare the
communication round complexities of four baselines: FedAvg, SGD, ASG, and SCAFFOLD, the
stepsize decaying variants of these algorithms (which are prefixed by M- in plots) and the various
instantiations of FedChain (Algo. 1).
Convex Optimization (Logistic Regression) We first study federated regularized logistic regres-
sion, which is strongly convex (objective function in App. I.1). In this experiment, we use the MNIST
dataset of handwritten digits (LeCun et al., 2010). We (roughly) control client heterogeneity by
creating “clients” through shuffling samples across different digit classes. The details of this shuffling
process are described in App. I.1; if a dataset is more shuffled (more homogeneous), it roughly
corresponds to a lower heterogeneity dataset. All clients participate in each round.
Fig. 2 compares the convergence of chained and non-chained algorithms in the stochastic gradient
setting (minibatches are 1% of a client’s data), over R = 100 rounds (tuning details in App. I.1).
We observe that in all heterogeneity levels, FedChain instantiations (whether two or more stages)
outperform other baselines. We also observe that SCAFFOLD→SGD outperforms all other curves,
including SCAFFOLD→ASG. This can be explained by the fact that acceleration increases the effect
of noise, which can contribute to error if one does not take K large enough (we set K = 20 in the
convex experiments). The effect of large K is elaborated upon in App. J.1.
Nonconvex Optimization (Neural Networks) We also evaluated nonconvex image classification
tasks with convolutional neural networks. In all experiments, we consider client sampling with
S = 10. We started with digit classification over EMNIST (Cohen et al., 2017) (tuning details in
App. I.2.1), where handwritten characters are partitioned by author. Table 3 (Left) displays test
accuracies on the task. Overall, FedChain instantiations perform better than baselines.
We also considered image classification with ResNet-18 on CIFAR-100 (Krizhevsky, 2009) (tuning
details in App. I.2.2). Table 3 (Right) displays the test accuracies on CIFAR-100; SCAFFOLD is not
included due to memory constraints. FedChain instantiations again perform the best.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is supported by Google faculty research award, JP Morgan Chase, Siemens, the Sloan
Foundation, Intel, NSF grants CNS-2002664, CA-2040675, IIS-1929955, DMS-2134012, CCF-
2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML), and CNS-2112471
as a part of NSF AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE).
Most of this work was done prior to the second author joining Amazon, and it does not relate to his
current position there.
References
Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated
learning via posterior averaging: A new perspective and practical algorithms. arXiv preprint
arXiv:2010.05273, 2020.
Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and
optimization. arXiv preprint arXiv:1506.01900, 2015.
Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. A universally
optimal multistage accelerated stochastic gradient method. arXiv preprint arXiv:1901.08022, 2019.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. Mathematical Programming,184(1):71-120, 2020.
Zachary Charles and Jakub Konecny. On the outsized importance of learning rates in local update
methods. arXiv preprint arXiv:2007.00878, 2020.
Zachary Charles, Zachary Garrett, Zhouyuan Huo, Sergei Shmulyian, and Virginia Smith. On
large-cohort training for federated learning. arXiv preprint arXiv:2106.07820, 2021.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. Advances in neural information
processing systems, 27, 2014.
Yuyang Deng and Mehrdad Mahdavi. Local stochastic gradient descent ascent: Convergence analysis
and communication efficiency. In International Conference on Artificial Intelligence and Statistics,
pp. 1387-1395. PMLR, 2021.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning. arXiv preprint arXiv:2003.13461, 2020.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally robust federated
averaging. arXiv preprint arXiv:2102.12660, 2021.
Alireza Fallah, Asuman Ozdaglar, and Sarath Pattathil. An optimal multistage stochastic gradient
method for minimax problems. In 2020 59th IEEE Conference on Decision and Control (CDC),
pp. 3573-3579. IEEE, 2020.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly
convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on
Optimization, 22(4):1469-1492, 2012.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal
on Optimization, 23(4):2061-2089, 2013.
EdUard Gorbunov, FiliP Hanzely, and Peter Richtarik. Local sgd: Unified theory and new efficient
methods. arXiv preprint arXiv:2011.02828, 2020.
10
Published as a conference paper at ICLR 2022
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. Advances in neural information processing Systems, 26:315-323, 2013.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U
Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in
federated learning. arXiv preprint arXiv:2008.03606, 2020a.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020b.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local sgd on identical
and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pp.
4519-4529. PMLR, 2020.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized sgd with changing topology and local updates. In International Conference
on Machine Learning, pp. 5381-5393. PMLR, 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, ., 2009.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smithy.
Feddane: A federated newton-type method. In 2019 53rd Asilomar Conference on Signals, Systems,
and Computers, pp. 1227-1231. IEEE, 2019.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches, use
local sgd. arXiv preprint arXiv:1808.07217, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Aritra Mitra, Rayana Jaafar, George J. Pappas, and Hamed Hassani. Linear Convergence in
Federated Learning: Tackling Client Heterogeneity and Sparse Gradients. arXiv e-prints, art.
arXiv:2102.07053, February 2021.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alex Smola. Fast incremental method for
nonconvex optimization. arXiv preprint arXiv:1603.06159, 2016.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018.
Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.
11
Published as a conference paper at ICLR 2022
Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-runtime
trade-off in local-update sgd. In SysML, 2019.
Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. Matcha: Speeding up
decentralized sgd via matching decomposition sampling. In 2019 Sixth Indian Control Conference
(ICC),pp. 299-300. IEEE, 2019a.
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving
communication-efficient distributed sgd with slow momentum. arXiv preprint arXiv:1910.00643,
2019b.
Blake Woodworth. The minimax complexity of distributed optimization. arXiv preprint
arXiv:2109.00534, 2021.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous
distributed learning. arXiv preprint arXiv:2006.04735, 2020a.
Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan,
Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International
Conference on Machine Learning, pp. 10334-10343. PMLR, 2020b.
Blake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro. The min-max complexity
of distributed stochastic convex optimization with intermittent communication. arXiv preprint
arXiv:2102.01583, 2021.
Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. arXiv preprint
arXiv:2006.08950, 2020.
Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. arXiv preprint
arXiv:2011.08474, 2020.
Kaiwen Zhou, Qinghua Ding, Fanhua Shang, James Cheng, Danli Li, and Zhi-Quan Luo. Direct
acceleration of saga using sampled negative momentum. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 1602-1610. PMLR, 2019.
12
Published as a conference paper at ICLR 2022
Appendices
A	Additional Related Work	14
B	Definitions	14
C	Omitted Algorithm Definitions	15
D	Proofs for global update methods	17
D.1 SGD .................................................................. 17
D.2 ASG .................................................................. 21
D.3	SAGA ................................................................ 24
D.4	SSNM ................................................................ 29
E Proofs for local update methods	34
E.1	FedAvg .............................................................. 34
F Proofs for FedChain	37
F.1	FedAvg → SGD ........................................................ 37
F.2	FedAvg → ASG ........................................................ 38
F.3	FedAvg → SAGA ....................................................... 39
F.4	FedAvg → SSNM ....................................................... 39
G Lower Bound	39
G.1	Strong convexity and	smoothness of F, F1, F2 ........................ 41
G.2 The solutions of F, F1, F2 ........................................... 41
G.3	Initial suboptimality gap ........................................... 41
G.4 Suboptimality gap after R rounds of communication .................... 41
G.5 Computation of ζ2 .................................................... 41
G.6 Theorem .............................................................. 42
H	Technical Lemmas	44
I	Experimental Setup Details	47
I.1	Convex Optimization ................................................. 47
I.2	Nonconvex Optimization .............................................. 47
J	Additional Convex Experiments	48
J.1 Verifying the effect of increased K and R = 1 ........................ 48
J.2 Including Stepsize Decay Baselines ................................... 49
13
Published as a conference paper at ICLR 2022
Table 4: Rates under the PL condition. * Rate requires R ≥ S.
Method/Analysis	EF(X) - F(x*) ≤ O(∙)
Centralized Algorithms	
SGD	△ exp(-KTR) + (1 - S)嵩	
Federated Algorithms	
FedAvg (Karimireddy et al., 2020a)	,	2 2	Jr2 κ∆exp(一κ R)+ μR2	
This paper	
FedAvg → SGD (Thm. 4.1)	min{A Zr} eχp(-κTR)+(1 -S) μ‰
FedAvg → SAGA (Thm. 4.3)	min{∆, ζμ} exp(-((S)3κ)-1R)*
Algo.-independent LB (Corollary 5.5)	.	3 3	— 3 /Z2、、	/	_1c、 min{∆,κ 2 (ζ-)} eχp(-κ 2 R)
A	Additional Related Work
Multistage algorithms. Our chaining framework has similarities in analysis to multistage algorithms
which have been employed in the classical convex optimization setting (Aybat et al., 2019; Fallah
et al., 2020; Ghadimi & Lan, 2012; Woodworth et al., 2020a). The idea behind multistage algorithms
is to manage stepsize decay to balance fast progress in “bias” error terms while controlling variance
error. However chaining differs from multistage algorithms in a few ways: (1) We do not necessarily
decay stepsize (2) In the heterogeneous FL setting, we have to accomodate error due to client drift
(Reddi et al., 2020; Karimireddy et al., 2020b), which is not analyzed in prior multistage literature.
(3) Our goal is to minimize commuincation rounds rather than iteration complexity.
B Definitions
Assumption B.1. Fi 's are μ-strongly convex for μ > 0, i.e.
EFi3, y - X ≤ -(Fi(X)- Fi(y) + μIlx - y∣l2)	(4)
for any i, x, y .
Assumption B.2. Fi ’s are general convex, i.e.
hVFi(x), y — xi ≤ -(Fi(x) - Fi(y))	(5)
for any i, x, y .
Assumption B.3. F is μ-PL for μ > 0, i.e.
2μ(F(x) - F(x*)) ≤∣∣VF(x)k2	(6)
for any x.
Assumption B.4. Fi ’s are β-smooth where
kVFi(x) -VFi(y)k ≤βkx-yk	(7)
for any i, x, y .
Assumption B.5. Fi’s are ζ2-heterogeneous, defined as
ζ2 := sup max kVF (x) - VFi(x)k2	(8)
x i∈[N ]
Assumption B.6. Gradient queries within a client have a uniform upper bound on variance and are
also unbiased.
Ezi 〜DJVf(x; Zi)-VFi(x)k2 ≤ σ2, Ezi 〜D」Vf(x; Zi)] = VFi (x)	(9)
Assumption B.7. Cost queries within a client have a uniform upper bound on variance and are also
unbiased.
Ezi〜Di(f (x; Zi)- Fi(x))2 ≤ σF, Ezi〜Dif (x; Zi) = Fi(x)	(10)
14
Published as a conference paper at ICLR 2022
Assumption B.8. Cost queries within a client have an absolute deviation.
ζF2 = sup max(F (x) - Fi (x))2
x i∈[N]
(11)
Assumption B.9. The initial suboptimality gap is upper bounded by ∆, where x0 is the initial
feasible iterate and x* is the global minimizer.
EF(xo) - F(x*) ≤ ∆
(12)
Assumption B.10. The expected initial distance from the optimum is upper bounded by D, where
x0 is the initial feasible iterate and x* is one of the global minimizers:
Ekx0 - x*k2 ≤ D2
(13)
C Omitted Algorithm Definitions
Algorithm 2 SGD
Input: initial point x(0), stepsize η, loss f
for r = 0, . . . , R - 1 do
.Run a step of SGD
Sample S clients Sr ⊆ [1,...,N]
Send x(r) to all clients in Sr
Receive {g(r)}i∈Sr = Grad(x(r), Sr,z(r)) (Algo. 7)
O(r) — 1 P 产
g = S 乙i∈Sr gi
x(r+1) = x(r) — η ∙ g(r)
end for
Algorithm 3 ASG
Input: initial point x(0) = xa(g0), {αr}1≤r≤R, {γr}1≤r≤R, loss f
for r = 1, . . . , R do
.Run a step of ASG
Sample S clients Sr ⊆ [1,...,N]
Send x(r) to all clients in Sr
T(T) _ (I-αrXμ+Yr) T(r-1) -L ar[(I-αQμ+Yr] _(r-1)
Xmd = Yr + (1-αr)μ Xag	+ Yr + (1-α2)μ X
Receive {g(r)}i∈Sr = Grad(Xmd), Sr,z(r)) (Algo. 7)
g") = S Pi∈Sr g(r)
X(T)= argminχ{αr [hg(r),x)+ μ kxm) — x∣∣2] + [(1 — a「) 2 + YrkX(I) — xk2]}
X(agr) = αr X(r) + (1 - αr )Xa(gr-1)
end for
15
Published as a conference paper at ICLR 2022
Algorithm 4 FedAvg
Input: Stepsize η, initial point x(0)
for r = 0, . . . , R - 1 do
Sample S clients Sr ⊆ [1, . . . , N]
Send x(r) to all clients in Sr, all clients set χ(r) = χ(r)
for client i ∈ Sr in parallel do
for k = 0,..., √K- 1 do
Client samples gi(,rk) =√1K P√K0 1 Nf(Xfk z(r)√K+k
x(r)	— x(r) - ∙ ∙ Jr)
xi,k + 1 = xi,k	η gi,k
end for
Receive g(r) = P√=0^1 g(r) from client
end for
g(r) = SS Pi∈Sr g(r)
x(r+1) - x(r) — η ∙ g(r)
) Where z(rk)√κ+k
〜Di
end for
Algorithm 5 SAGA
Input: stepsize η, loss f, initial point x(0)
Send φ(0) = χ(0) to all clients i
Receive c(0) = ɪ PK-1 Vf (φ(0); z(-1)), z(-1) ~ Di from all clients, c(0) = N PN=Ic(0)
for r = 0, . . . , R - 1 do
Sample S clients Sr ⊆ [1, . . . , N]
Send x(r) to all clients in Sr, Receive {gi ^^} i ∈Sr	Grad(x(r), Sr, z(r)) (Algo. 7)
0(r) = 1 P	0(r) _ 1 P c(r) + c(r)
g = S 乙i∈Sr gi S 乙i∈Sr ci + c
x(r+1) = x(r) — η ∙ g(r)
Option I: Set ci(r+1) = gi(r), φi(r+1) = x(r) for i ∈ Sr
Option II: New independent sample of clients Sr, Send x(r) to all clients in Sr, receive
{")}i∈sr = Grad(X(r), Sr,z(r)), set c(r+1) = g(r), φ(r+1) = x(r) for i ∈ Sr
ci(r+1) = ci(r) and φi(r+1) = φi(r) for any i not yet updated
c(r+1) = N pN=i c(r+1)
end for
16
Published as a conference paper at ICLR 2022
Algorithm 6 SSNM
Input: stepsize η, momentum T, client losses are Fi(X) = Ezi〜D」f (x; zi)] + h(x) = Fi(x) + h(x)
where Fi can be general convex and h(x) is μ-strongly convex, initial point x(0)
Send φ(0) = x(0) to all clients i
ReCeiVe c(0) = Kk PK=O1 Nf (φ(0);z(-I)), z(-1) 〜Difrom all clients c(0) = N PN=ι ci0)
for r = 0, . . . , R - 1 do
Sample S clients Sr ⊆[1, . . . , N]
Send x(r) to all clients in Sr, y(? = τx(r) + (1 — T)φ(? for i『∈ Sr
Receive CIm = -1 PKT Nf (?/(r【z(r) ) for i ∈ S z(r)〜D
ReceiVe gir = K λ=0=N n f (yir ； zir,k) for ir ∈ Sr, Zir ,k 〜Di
O(r) = 1 P L Jr) - 1 P L c(r) + c(r)
g	S 乙ir ∈Sr gir S 乙ir ∈Sr Cir + C
x(r+1) = arg mi□χ{h(χ) + hg(r),x)+ 2η ∣∣x(r) — x∣∣2}
Sample new independent sample of S clients Sr0 ⊆[1, . . . , N]
Send x(r) to all clients in Sr, φIr+1) = τx(r+1) + (1 — T)φ∣r) for Ir ∈ Sr
ReCeiVe gIr) = KK PK=O1 Nf(OIr+1)； ZIr)k) for Ir ∈ Sr, z(r)k 〜Di
Ci(r+1) = gI(rr) for Ir ∈ Sr0
Ci(r+1) = Ci(r) and φi(r+1) = φi(r) for any i not yet updated
c(r+1) = N PN=1 c(r+1)
end for
Algorithm 7 Grad(x, S, Z )
for client i ∈ S in parallel do
Client samples g(r) = Nf (x(r); z(r)) where z(r) 〜Di, k ∈{0,...,K — 1}
ʌ(r) _ ɪ PK-1，n(r)	'	'
gi = K 2k=0 gi,k
end for
return {gi(r)}i∈Sr
D Proofs for global update methods
D.1 SGD
Theorem D.1. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4
and Assumption B.6. Then running Algo. 2 gives the followingfor returned iterate X:
• Strongly convex: Fi ’s satisfy Assumption B.1for μ > 0. Ifwe return X =志 PR=O Wr x(r)
with Wr = (1 — ημ)1-(r+1) and WR = PR=O Wr, η = O(min{ 1, μR}), and R > κ,
R	σ2	S	ζ2
ef(X)- F(X* *) SOCexpJK)+ E + (I- N)岛)
E∣x(r) — X* ∣2 ≤ O(DI2) for any 0 ≤ r ≤ R
• General convex (grad norm): Fi ’s satisfy Assumption B.2. If we return the average iterate
X = R Pr=I X(r), and η = min{ɪ, (β∆R)1/2} where C is the update variance
EkNF (X)k2 ≤O(啜十 √βσDR+K √D)
E∣X(r) — X* ∣2 ≤ O (D2) for any 0 ≤ r ≤ R
17
Published as a conference paper at ICLR 2022
• PL condition: Fi S satisfy Assumption B.3. Ifwe return the final iterate X = X(R), set
η = O(min{ 1, μR}), then
EF(X(R))- F(x*) ≤ O(∆exp(-R) + JK^ + (1 - S)鼻)
K μSKR	N μSR
Lemma D.2 (Update variance). For SGD, we have that
Erkg叫2 ≤ kVF(x(r))k2 + 黑 + (1- S-1)ζ2	(14)
SK	N - 1 S
Proof. Observing that
Erkg(r)k2 = Erkgs)- VF(x(r))k2 + kVF(x(r))k2	(15)
and using Lemma H.1, we have
ErkgS)- VF(x(r))k2 ≤ 黑 + (1- S-1)ζ2	(16)
SK	N - 1 S
□
D.1.1 Convergence of SGD for Strongly Convex functions
Proof. Using the update of SGD,
∣∣x(r+1) — x* k2 ≤ ∣∣x(r) — x* k2 — 2ηhg(r), x(r) — x*i + η2 ∣∣g(r) ∣∣2	(17)
Taking expectation up to the r-th round
ErkX(r+1) - X*k2 ≤ kX(r) - X*k2 - 2ηhVF (X(r)), X(r) - X*i + η2Erkg(r)k2	(18)
By Assumption B.1,
-2ηhVF(x(r)),x(r) - x*i ≤ -2η(F(Xs)) - F(x*)) - ημkx(r) - x*∣∣2	(19)
Using Lemma D.2, Assumption B.4, and setting η ≤ 泰,
ErkX(r+1) - X*k2	(20)
≤ (1 - ημ)kx(r) - x*k2 - η(F(x(r)) - F(x*)) + η2(鼠 + (1 - 曰)之)	(21)
SK	N - 1 S
Rearranging and taking full expectation,
W (r八	*( *∖ V(I-ημ)Ekx(r)-x*k2 -Ekx(r+1) - x*k2	( σ z 1 S - 1 ζ2
ef(x())- F(x ) ≤-----------------------η-------------------+ η(sκ + (I - N-y)-S)
(22)
letting wr = (1 - μη)1-(r+1), WR = PR=o, X= WWR PR=O x(r), then
EF(X) - F(x*) ≤
EkX(O)-门户 + η( W + (1- S-I)U)
ηWR	+ 八 SK +( N - 1) S )
(23)
From (Karimireddy et al., 2020b) Lemma 1, if R > κ and η = min{器,log(max(jR RD ICc)} where
「一上 i 1 — .s-1 3
C = NK + (1 N-1 ) S ,
R	σ 2	S ζ2
ef (X)- F (x*) ≤ OgeXp(-K)+E+(1 - N) 篇)	(24)
Which finishes our work on the convergence rate. Next we consider the distance bound. Returning to
the recurrence
Erkx(r+1) -x*k2	(25)
≤ (1 - ημ)kx(r) - x*k2 - η(F(x(r)) - F(x*)) + η2(鼠 + (1 -曰)之)	(26)
SK	N - 1 S
18
Published as a conference paper at ICLR 2022
Taking full expectation and unrolling,
Ekxs) - x*k2	(27)
≤ Ekx⑼-x*k2 eχp(-ημr) + η(M + (I - S--1)=)	(28)
μ SK	N — 1 S
Note that Karimireddy et al. (2020b) chose η so that η(Sσ2 + (1 — N-I) W) ≤ O(μD2 exp(-μηR))
And so
Ekx(r) — x*k2 ≤ O(D2)	(29)
□
D.1.2 Convergence of SGD for General Convex functions
Proof. By smoothness (Assumption B.4), we have that
F(x(r+1)) — F(x(r)) ≤ —ηhVF(x(r)),g(r)i + βη2kg(r)k2	(30)
Taking expectation conditioned up to r and using Lemma D.2,
ErF(x(r+1)) — F(x(r)) ≤ —(η —半)kVF(x(r))k2 + 孚(σ- + (1 — S-4)ζ2)	(31)
2	2 SK	N — 1 S
If η ≤ 1,
Er F (x(r+1)) — F (X(T)) ≤ — 2 kVF (x(r))k2 + βη2 (SK + (1 — N-L) Z2)	(32)
Taking full expectation and rearranging,
2EkVF(x(r))k2 ≤ EF(x(r+1))n— EF(X(r)) + βη(SK + (1 - N--)ζ2)	(33)
Summing both sides over r and averaging,
1R X EkVF(x(r))k2 ≤ EF(X(O))ηREF(X(R)) + β(⅛ + (1 - N-)Z2)	(34)
r=0
Letting X be an average of all the x(r)'s, noting that EF(X(O)) — EF(X(R)) ≤ ∆, ∆ ≤ βD2, and
choosing η = min{ 1, (β∆R )1∕2} where C = S⅛ + (1 — Nv≡1) Zr,
EkVF (X)k2 ≤O( β∣ + √βσDR + V √SR)	(35)
So we have our convergence rate. Next, for the distance bound,
∣∣X(r+1) — X* k2 = ∣∣X(r) — X* k2 — 2ηhg(r), X(r) — X*i + η2 ||g(r) ∣∣2	(36)
Taking expectation up to r, we know from Lemma D.2,
ErkX(r+1) —X*k2	(37)
≤ kX(r) — X*k2 - 2nhg(r),X(r) — X*i + η2kVF(X(r))k2 + η2(嚏 + (1 — S)ζ2)	(38)
SK	N S
By Assumption B.2 and Assumption B.4,
ErkX(r+1) —X*k2	(39)
≤ kX(r) — X*k2 - 2η(1 - βη)(F(Xs))- F(X*)) + η2(嚏 + (1 — S)ζ2)	(40)
SK	N S
19
Published as a conference paper at ICLR 2022
And with η ≤ 1,
Erkxe-x*k2 ≤kχ6-x*k2 + η2(σL + (1- S)ζ2)	(41)
SK	N S
Unrolling and taking full expectation,
Ekx(T)- x*k2 ≤ Ekx(O)- x*k2 + η2R(展 + (1- S)ζ2)	(42)
SK	N S
We chose η so that βη(SK + (1 - NS) ζS2) ≤ 余.Therefore
Ekx(r) - x*k2 ≤ Ekx(O) - x*k2 + δ ≤ 2D2	(43)
□
D.1.3 Convergence of SGD under the PL-condition
Proof. Using Assumption B.4 we have that
F(x(r+1))- F(XS)) ≤-η<VF(x(r)),g(r)i + βη2kg(r)k2
Taking expectations of both sides conditioned on the r-th step,
ErF(x(r+1)) - F(x(r)) ≤ -ηkF(x(r))k2 + βη2Erkg(r)k2
Using Lemma D.2,
ErF(x(r+1)) - F(X(T)) ≤ -2kF(x(r))k2 + βη2(SK + (1- Nl-I)Z2)
Using Assumption B.3, we have that
ErF(x(r+1)) - F(X(T)) ≤ -ημ(F(x(r)) - F(x*)) + βη2(σ- + (1 - S-4)ζ2)
2 SK	N - 1 S
Rearranging,
ErF(x(r+1)) - F(X*) ≤ (1 - ημ)(F(x(r)) - F(x*)) + βη2(σ- + (1 - S-1)ζ2)
2 SK	N - 1 S
Letting C = SK + (1 - NN--I) ζS and subtracting 筌 from both sides,
Er F (x(r+1)) - F (x*) - βcη ≤ (1 - ημ)(F (x(r)) - F (x*)-等)
2μ	2μ
Which, upon unrolling the recursion, gives us
EF(X(R)) - F(x*) ≤ ∆exp(-ημR) + βcη
2μ
Now, if we choose stepsize
min{1,
β
log(maχ{e, μ^}) }
μR
η
Then the final convergence rate is
ef (X(R))- F (x*)Sexp(-R) + 总
(44)
(45)
(46)
(47)
(48)
(49)
(50)
(51)
(52)
+ (1 - S
□
20
Published as a conference paper at ICLR 2022
D.2 ASG
The precise form of accelerated stochastic gradient we run is AC-SA(Ghadimi & Lan, 2012; 2013).
The following specification is taken from Woodworth et al. (2020a):
We run Algo. 3 for Rs iterations using x(0) = ps-1, {αt}t≥1 and {γt}t≥1, with definitions
• Rs = dmax{4q4β, 3μ∆28cS + l) }e
…一工 Z —	4φs .
αr	r+1, Yr	r(r + 1),
• φs = max{2β, [3∆2-(S-I) Rs,Rs + 1)(Rs +2)]1/2}
Where C = SK + (1 - N-I) ζS2. Set Ps = XaRs) where XaRs) is the solution obtained in the previous
step.
Theorem D.3. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4
and Assumption B.6. Then running Algo. 3 gives the followingfor returned iterate X:
• Strongly convex: Fi s satisfy Assumption B.1 for μ > 0. Ifwe return X = xaRRS) after R
rounds of the multistage AC-SA specified above,
EF(X)- F(X*) ≤ OgeXp(-* + μSκR + (I-S) μsR)
EkX - x* k2 ≤ O(KEkX(O)-X*k2 eχp(-√κ)+μSκR+(I- N) μsR)
And given no randomness, for any 0 ≤ r ≤ R
kX(r) - X* k2 ≤ kX(0) - X*k2
kX(arg)-X*k2 ≤ kX(0) - X*k2
and for any 1 ≤ r ≤ R
kX(mrd)-X*k2 ≤ kX(0) - X*k2
• General convex (grad norm): Fi's satisfy Assumption B.2. Ifwe return X = XaRS) after
R rounds of the multistage AC-SA specified above on the regularized objective FU(X) =
F(x) + μ∣∣X⑼-x∣∣2 with μ = max{R log2(e2 + R2), J∆SKR，J}, we have
EkVF(X)k2 ≤O(R +
上 + (1 - S) α +
SKR + ( N) SR +
√SKR+∖F-N √SR)
EkX - x*k2 ≤ O(D2)
D.2.1 Convergence of ASG for Strongly Convex Functions
Proof. The convergence rate proof comes from Woodworth et al. (2020a) Lemma 5. What remains is
to show the distance bound.
From Proposition 5, (Eq. 3.25, 4.25) of Ghadimi & Lan (2012), we have that the generic (non-
multistage) AC-SA satisfies (together with Lemma H.1)
EF(x(r)) + μEkx(r) - x*k2 - F(x)
≤ Γr XX Yτ [EkX(TT)-X*∣∣2 - EkX(T) - X*k2]+Γr 4r ( σL + (1- S ) Z2 )
T=1 IT	μ SK	NS
(53)
(54)
21
Published as a conference paper at ICLR 2022
Where Γr = 1, r = 1	Notice that
(1 - αr)Γr-1 r ≥ 2
r
Γr X Γτ[Ekx(TT)- x*k2 - Ekx(T) - x*k2] =Γtγι[kx⑼-χ*k2 - kx⑺-x*k2]	(55)
τ=1 Γτ
So
(μ + Γtγι)EkxS) - x*k2 ≤ ΓtγιE∣∣x⑼-x*k2 + Γr4r(展 + (1 - S)IZ)	(56)
μ SK	N S
Which implies
Ekx(r) - x*k2 ≤ Ekx⑼-x*k2 + Γr42(嚏 + (1 - S)ζ2)	(57)
μ2 SK	NS
Furthermore, Γr = 丁；1) So
Ekx(r) - x*k2 ≤ Ekx⑼-x*k2 +	8	(展 + (1 - S)ζ2)	(58)
μ2(r + 1) SK	N S
If there is no gradient variance or sampling,
kx(r) — x*k2 ≤ kx(0) — x*∣∣2	(59)
Next, we show the above two conclusions hold for x(mrd) and xa(gr).
For xa(gr), we show by induction:
Base case: xa(gr) = x(0), so it is true
Inductive case: x(agr) is a convex combination of x(r) and xa(gr-1), so the above statements hold for
x(agr) as well.
For x(mrd), note it is a convex combination of xa(gr-1) and x(r-1), so the above statements on distance
also hold for x(mrd).
For the distance bound on the returned solution, use strong convexity on the convergence rate:
Ekx - x*k2 ≤ O(KEkx(O) - x*k2 exp(-√κ) +——CR)	(60)
□
D.2.2 Convergence of ASG for General Convex Functions
For the general convex case, we use Nesterov smoothing. Concretely, we will run Algo. 3 assuming
strong convexity by optimizing instead a modified objective
Fμ(x) = F(x) + μkx(0) - xk2	(61)
Define xμ = argmi% Fμ(x) and △* = EFμ(x(O)) - F(xμ). We will choose μ carefully to balance
the error introduced by the regularization term and the better convergence properties of having larger
μ.
Proof. Observe that
EkVF(x)k2 = EkVFμ(x) - μ(x - X(O))k2 ≤ 2EkVFμ(x)k2 + 2μ2E∣∣x - x(0)k2	(62)
We evaluate the second term first. We know that from the theorem statement on strongly convex
functions and letting K = β+μ and K = μ because F is now β + μ-smooth,
EFμ(x) - Fμ(xμ) ≤ O((EFμ(x(O))- F“(x2)) exp(-√R0) + μSKR + (1- S)篇)⑹)
22
Published as a conference paper at ICLR 2022
Which implies
σ2	S	ζ2
EFμ(x) ≤ O(EF“(X(O)) + μsKR + (I- W) 岛)	碎
σ2	S ζ 2
= O(EF (X(O)) + μsκR + (I-耳) 篇)	(65)
So it is true that
EFμ(X) = E[F(X) + 2IlX - x(0)k2]	(66)
σ2	S ζ2
= O(EF (X(O)) + E + (1 - N) μsR)	(67)
If we rearrange,
μEkX — x(0)k2 ≤ O(EF(X(O))- F(X) + -2^+ + (1 — S)三)	(68)
2	μSKR	N μSR
σ2	S ζ 2
≤ O(EF(X(O))- F(X*) + — + (1 - N)篇)	(69)
σ2	S ζ 2
≤O(△+μsκR + (1- N) μsR)	(70)
Next We evaluate 2EkVFμ(X)k2. Observe that the smoothness of Fμ is β + μ. Returning to the
convergence rate,
EFμ(X) - &(%) ≤ o(δ* exp(--√=j) +	qκR +(I - τf)-⅛B)	(71)
√κ0	μSKR	N μSR
We have that
Fμ(X(O))- Fμ(Xμ) = F(X(O))- F(Xμ) - 2kX(O) - Xμk2 ≤ F(X(O))- F(x*)	(72)
So ∆μ ≤ ∆. By Assumption B.4,
EkVFμ(X)k2 ≤ (β + M)O(Aexp(--『) +	GkR + (I - ^∕v)^^≡>)	(73)
7 K	μSKR	N μSR
R	βσ2 σ2	S ζ 2	S βζ 2
≤ O(βAexp(-花)	+ μ∆ +	μSKR +	SKR	+ (1 - N)SR +	(1 - N)μSR)
(74)
So altogether,
R	σ 2	S ζ 2
EkVF(X)k ≤ OMAexpJ-κo) + (1 + κ) SKR + (1 + κ)(1 - N)SR + μ∆)	(75)
Choose μ = max{ R log2(e2 + R2), Q∆SK2R，V (I-SSRf。}. By Theorem E.1's proof in Yuan &
Ma (2020),
EkVF(X)k2 ≤ O(R+SKR+(1 - N)SR+-SKR+J1 - N-R)	(76)
Next We evaluate the distance bound for the returned iterate. Observe that from the distance bound in
the strongly convex case,
EkX - X/2 ≤ O(EkX(O)- xMI2 eχp(-√κ) + μ2SKR +(1 - N)μ⅛R)	(77)
Given that μ = max{R log2(e2 + R2), J∆SKR,，(1-∆S肾2},
EkX - χμk2 ≤ O(EkX(O)- χμk2 + kχ - χμk2)	(78)
≤O(EkX(O)-Xμk2)	(79)
23
Published as a conference paper at ICLR 2022
Now we look at the actual distance we want to bound:
Ekx - x*k2 ≤ O(Ekx - xμk2 + Ekxμ - X(O)k2 + E∣∣x* - X⑼k2)	(80)
(81)
Observe that
Fμ(xμ) = F(xμ) + kx⑼-xμk2 ≤ F(x*) + kx⑼-x*k2 = Fμ(x*)	(82)
which implies that
kχ(0)-xμk2 ≤kx⑼-x*k2	(83)
So altogether
Ekx - x*k2 ≤ O(D2)	(84)
□
D.3 SAGA
Theorem D.4. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4
and Assumption B.6. Then running Algo. 5 gives the followingfor returned iterate x:
• Strongly convex: Fi s satisfy Assumption B.1for μ > 0. Ifwe return x =志 PR=O Wr x(r)
with Wr = (1 一 ημ)1-(r+1) and WR = PR=O Wr, η = O(min{ 1, μR }), R > κ, and we
use Option I,
〜	μ S	σ2
EF(x) - F(x*) ≤ O(∆exp(-min{，N}R) + μRKS)
• PL condition: Fi s satisfy Assumption B.3. Ifwe Set η = 3§(n/s)2/3 , use Option II in a
multistage manner (details specified in proof), and return a uniformly sampled iterate from
the final stage,
R	σ2
ef(X)- F(x) ≤ OgexM-κwSF3)+ 即)	(85)
D.3.1 Convergence of SAGA for S trongly Convex functions
The proof of this is similar to that of Karimireddy et al. (2020b, Theorem VII).
Proof. Following the standard analysis,
Erkx(r+1) - x*k2	(86)
=kx(r) - x*k2 - 2ηErhg(r),x(r) - x*i + Erkng(r)k2	(87)
(88)
We treat each of these terms separately.
Second term: Observe first that
1	1 K-1
Erg⑺=Er(V X [斤 X Vf (x(r); z(rk)) - c(r)] + Cs))	(89)
SK	,
i∈Sr	k=O
= VF (x(r))	(90)
And so the middle term has, by (strong) convexity,
-2nErhg(r),x(r) - x*〉= -2nhVF(x(r)),x(r) - x*〉	(91)
≤ -2n(F(x(r)) - F(x*) + μkx(r) - x*k2)	(92)
24
Published as a conference paper at ICLR 2022
Third term:
Erkηg(r)k2	(93)
1	1 K-1
=Erknc X [- X Vf (x(r); z(rk)) - c(r)] + c(r))k2	(94)
SK	,
i∈Sr	k=0
=n2Erk K1S X Vf(X⑺;Zfk))-c(r) + C叫2	(95)
k,i∈Sr
≤ 4n2ErkK1S X Vf(x(r); Zirk)-VF(x(r))k2 +4n2Erkc(r)k2	(96)
k,i∈Sr
+ 4n2ErkK1S X VFi(x*) - cir)k2 +4n2ErkK1S X VFi(x(r)) -VFi(x*)k2 (97)
k,i∈Sr	k,i∈Sr
(98)
Where the last inequality comes from the relaxed triangle inequality. Then, by using Assumption B.6,
Assumption B.4, and Assumption B.2,
Erkηg(r)k2	(99)
≤ 4n2Erkc(r)k2 + 4n2Erkɪ X VFi(x*) - c(r)k2 + 8βn2[F(x(r)) - F(x*)] + 4n2σL
KS	KS
k,i∈Sr
(100)
Taking full expectation and separating out the variance of the control variates,
Ekηg(r)k2	(101)
≤ 4n2kEc(r)k2 +4n2kɪ X VFi(x*) - Ecir)k2 +8βn2E[F(X*-F(x*)] +12窖
KS	KS
k,i∈Sr
(102)
Now observe that because c(r) = N P3c(r),
Ekng(r)k2 ≤ 8N2 X kVFi(x*) - Ecir)k2 +8βn2E[F(x(r)) - F(x*)] +12KS2	(103)
N i=1	K S
≤ 8n2Cr + 8βn2E[F(x(r)) - F(x*)] + 12n2σ2	(104)
KS
Bounding the control lag:
Recall that	c(r+1) = (c(r) w.p. 1 - NN i	一也 PKo1 Vf(x(r)； Zirk)) Wp S	(105)
Therefore	Ec(r+I) = (I- S)E[c(r)] + SEVFi(x(r))	(106)
Returning to the definition of Cr+1,		
Cr+1 =	1N nn XEkE[c(r+1)] -VFi(X*)k2 i=1	(107)
=	NN XEk(I- NN)(E[c(r)] - VFi(x*)) + NN(EVFi(X(r)) - VFi(X*))k2 i=1	(108)
≤	S	SN (1 - n)Cr + N EEkVFi(X(r)) - VFi(X*)k2 i=1	(109)
25
Published as a conference paper at ICLR 2022
where in the last inequality we applied Jensen’s inequality twice. By smoothness of Fi’s (Assump-
tion B.4),
Cr+1 ≤ (1 - N)Cr + 2NS[EF(x(r)) - F(x*)]
Putting it together:
So putting it all together, we have
EkX(r+1) - X*k2
12η2 σ2
≤ (1 - ημ)Ekx⑺-x*k2 - η(2 - 8βη)(EF(Xe)) - F(x*)) + 8η2Cr +/。
KS
From our bound on the control lag,
9η2NCr+1 ≤ 9η2N(1 - S)Cr + 18βη2[EF(”- F(X*)]
S	SN
=(1 - ημ)9η2NCr + 9η2(嘈 - 1)Cr + 18βη2[EF(x(r)) - F(x*)]
SS
Adding both inequalities, we have
EkX(TI)- x*k2 + 9SNCr+1
≤ (1 - ημ)[EkX(r) - X*k2 + 9η2NCr] - η(2 - 26βη)(EF(Xs)) - F(x*))
S
l 2(9ημN	]、〃 l 12η2σ2
+ η( - - I)Cr + ~K^~
Let η ≤ 26β, η ≤ 岛,then
EkX(TI)- X*k2 + 9SNCr+1
≤ (1 - ημ)[EkX(r) - X*k2 + 9SNCr] - η(EF(X(r)) - F(x*)) + ^KKf
Then by using Lemma 1 in (Karimireddy et al., 2020b), setting ηmaχ = min{磊,9^}, R ≥
(110)
(111)
(112)
(113)
(114)
(115)
(116)
(117)
(118)
and choosing η = min{
log(max(1,μ2 Rdo /c))
(119)
1
2ηmaxμ，
μR
, ηmax } where d0 = Ekx(0) -
C = IKKS2, We have that by outputting X =志 PR=o WrX(r) with WR
(1 - μη)1-r, we have
*
x'
P
k2 + TCo and
R
r=0 wr
and wr
___, . . 一 , , ~ , , 一、
EF(X)- F(X ) ≤ O(μdo exp(-μηmaχR) +
(120)
=O(μdo eχp(-μηmaχR) + μR)
=O(μ[E∣∣X⑼-X*k2 + Nη2Co]exp(-μηmaχR) +	)
S	μRKS
We use a warm-start strategy to initialize all control variates in the first N/S rounds such that
K-1
c(0) = K X Vf(X⑼;z(,-1))
k=0
By smoothness of Fi’s (Assumption B.4),
1N
Co = N EEkEc(O)-VFi(X*)k2
N i=1
1N
=N EEkVFi(X(O))-VFi(X*)k2
i=1
≤ βE(F(X(O)) - F(X*))
(121)
(122)
(123)
(124)
(125)
26
Published as a conference paper at ICLR 2022
And recalling that η ≤ min{嬴,^^ }, We know that
9Nη2Co ≤ 9Nη2βE(F(X(O))- F(x*)) ≤ ηβ(F(X(O))- F(x*)) ≤ 1∆	(126)
S	S	μ	μ
So altogether,
σ2
EF(X)- F(X ) ≤ O(∆exp(-μηmaχR) + μRKS)	(127)
□
D.3.2 Convergence of SAGA under the PL condition
This proof follows Reddi et al. (2016).
Proof. We start with Assumption B.4
EF(x(r+1)) ≤ E[F(Xs)) + BF(x(r)),x(r+1) - X㈤)+ β∣∣x(r+1) - x(r)k2]	(128)
Using the fact that g(r) is unbiased,
EF(X(r+1)) ≤ E[F(Xs)) - η∣VF(X(r))∣2 + βη2∣∣g(r)k2]	(129)
Now we consider the Lyapunov function
N
Lr= E[F(X(T)) + N X ∣X(r) - φ(r)∣∣2]	(130)
N i=1
We bound Lr+1 using
N
N XE∣X(r+1) - Φir+1)k2	(131)
i=1
=N X[NE∣X(r+1) - X(r)k2 + NNSE∣X(r+1) - φ(r)k2]	(132)
N i=1 N	N
Where the equality comes from how φi(r+1) = X(r) with probability S/N and is φi(r) otherwise.
Observe that we can bound
E∣X(r+1) - φi(r) ∣2	(133)
= E[∣X(r+1) -X(r)∣2 +	∣X(r)	-	φi(r)∣2 + 2hX(r+1) - X(r), X(r) - φi(r)i]	(134)
≤ E[∣X(r+1) - X(r)k2 +	∣X(r)	-	φ(r)k2] + 2ηE[21b∣VF(X(r))k2 + 1 b∣X(r)	- φ(r)∣2]	(135)
Where we used unbiasedness of g(r) and Fenchel-Young inequality. Plugging this into Lr+1,
Lr+1 ≤ E[F(X(r)) - η∣VF(Xs))∣2 + βη2∣g(r) k2]	(136)
N
+ E[Cr + 1∣X(r+1) - X(r)k2 + Cr + 1 -Nr X ∣X(r) - Φ(r)k2]	(137)
i=1
+ 2(N -N)Cr+1η XE[2b∣VF(X(r))k2 + 2b∣X(r) - φ(r)k2]	(138)
i=1
≤ E[F(X(r)) - (η - cr+1ηbN - S) )∣VF(X(r))k2 + (βη2 + Cr+ιη2)Ekg(T) k2]	(139)
+ (NNSCr+1 + cr+iηbS - S))N XE∣X(r) - φir)k2	(140)
i=1
27
Published as a conference paper at ICLR 2022
Now we must bound Ekg(r) k2.
1	1 K-1	1 N
Ell r ∖ ' ∣^	∖、v £∕%(r)”(r八	V £(小(T) <(r八 1_∣___\、v £(小(T) ♦	&(明32	门41、
Ek (S	[ K	Vf (X ； zi,k ) -vf (φi ； zi,k)] + NK 乙 Vf (φi ； zi,k ))k	(141)
i∈Sr	k=0	i=1
1	1 K-1
≤ 2Ek(S X [K X Vf (x(r)； z(r?) -Vf(φir);璟)]	(142)
i∈Sr	k=0
1N	1N
- NK XVf(x(r)； Zfk))- Vf (φ(r);选)))]『+ 2Ek NK X Vf(X⑺;Zfk))k2	(143)
i=1	i=1
≤ 2Ek 1 X VFi(X(T))- VFi(φ(r))k2 + 2E∣∣VF(x(r))k2 + 嚏	(144)
S	SK
i∈Sr
Where the second to last inequality is an application of Var(X) ≤ E[X2], and separating out the
variance (taking advantage of the fact that Z's and z's are independent), and V is some constant.
We use Assumption B.4 and take expectation over sampled clients to get
2N	2
Ekg(r)k2 ≤ JN XEkx(t) - Φ(r)k2 + 2EkVF(x(r))k2 + SK
N i=1	SK
Returning to our bound on LT+1,
Lr+1 ≤ EF(x(r)) - (η - cr+1ηbN - S) - η2β - 2cr+ιη2)EkVF(x(r))k2
+ (cr+ι(NNS + 吟-史 + 2η2β2) + η2β3)N XEkx(r) - φ(r) k2
i=1
νη2b2	β
+ ~KK~ (cr+1 + 2)
We set Cr = cr+ι(NN-S + ηb(N-S) + 2η2β2) + η2β3 which results in
LT+1
≤ Lr-(η - cr+1η(N - S) - η2β - 2cr+ιη2)EkVF(x(r))k2 + *&+1+ β)
bN	SK	2
Let Γr = η 一 cr+1bN-S) 一 η2β 一 2射+\『 Then by rearranging
ΓrEkVF(X(T))k2 ≤ Lr - Lr + 1 +	& + 1 + β)
SK	2
Letting γn = min0≤r≤R-1 Γr,
R-1
γn X EkVF(X(r))k2
r=0
R-1
X EkVF (X(r))k2
r=0
R-1
≤ X ΓrEkVF (X(r))k2
r=0
R-1 νη2σ2	β
≤ L0 - LR + £ SK (Cr+1 + 2)
r=0
Implying
∆	1 R-1 νη2σ2	β
≤ — + — Σ⅛^(cr+1 + ⅞)
γn γn r=0 SK	2
Therefore, if We take a uniform sample from all the χ(r), denoted XR,
R-1	2 2
EkVF (X(R))k2 ≤ γ∆R + 占 X 线~ (cr+1 + 2)
(145)
(146)
(147)
(148)
(149)
(150)
(151)
(152)
(153)
(154)
(155)
28
Published as a conference paper at ICLR 2022
We start by bounding Cr. Take η = 3β(NS)2/3 and b = β(NS)1/3. Let θ = S - Ilbb(NN ~S -
2η2β2. Observe that θ < 1 and θ > 9S. Then Cr = Cr+ι(1 - θ) + η2β3, which implies
Cr= η2β3TRzr ≤ 卑3 ≤ 4 (NS)1/3
So we can conclude that
Yn = min(η - cr+iη - η2β - 2cr+ιη2) ≥ ɪ(S)2/3	(156)
r	β	12β N
So altogether,
EkVF(XbR))k2 ≤ 等(N)2/3 +	(IW^)	(157)
=ɪ( N )2/3 + 尊 WNSS)2 双解')	(158)
12β∆ N	2νσ2
≤ -R-( s )2/3+后	(159)
Now we run Algo. 5 in a repeated fashion, as follows:
1.	Set xb0) = ps-1
2.	Run Algo. 5 for Rs iterations
3.	Set ps to the result of Algo. 5
Repeat for S stages. Let po be the initial point. Letting Rs = d24κ(S)2∕3] this implies that
2μ(EF(Ps)- F(x*)) ≤ EkVF(ps)k2 ≤ μ(EF(Ps-I)- F(x*)) + 尊 (16。)
2	S K
Which gives
EF(ps) - F(x*) ≤ O(∆exp(-s) + σ--)	(161)
μSK
If the total number of rounds is R, then
EF(X) - F(x*) ≤O(∆exp(-R) + &)	(162)
K μSK
□
D.4 SSNM
Note that our usual assumption Fi(X) is μ-strongly convex can be straightforwardly converted into
the assumption that our losses are Fi(X) = Fi(X) + h(X) where Fi(X) is merely convex and h(X) is
μ-strongly convex (see (Zhou et al., 2019) section 4.2).
Theorem D.5. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4
and Assumption B.6. Then running Algo. 6 gives the followingfor returned iterate X:
• Strongly convex: Fi，s satisfy Assumption B.1 for μ > 0. Ifwe return the final iterate
and set∏	= —1— T	= (N犷)砂	if	(NZS) > 3	and n	= q/ , 1 T	= (N犷)砂	if
and set η 2μ(N]Sy /	1+ιμ	if K > 4	and η V 3*(N/S)e, /	1+ιμ	if
(N/S) V 3
≤ 4，
S
EF(X(R)) — F(x ) ≤ O(κ∆exp(-min{—,
ɪ }R) +
Nκ
κσ2
μKS)
29
Published as a conference paper at ICLR 2022
D.4.1 Convergence of SSNM on S trongly Convex functions
Most of this proof follows that of (Zhou et al., 2019) Theorem 1.
First, we compute the variance of the update g(r) .
N
E[kg(r) - NN X VPi(y(r))k2]
N i=1
N2
≤ Ek 1 X VFir (y(r))-VFir (φ(r)) - N X(VFi(y(r))-VFi(φ(r)))k2 + KS
ir ∈Sr	i=1
≤ Ek 1 X VFir(y*-VFir(φ(r))k2 + KS
S	KS
ir ∈Sr
≤ 2βE[1 ^X	F∙ (φ(r))	- F∙ (y(r))	- hVF∙ (y(r)) φ(r)	- y(r)i]	+	IVQ
—2βE[ S / J	Fir (φir ) FIir (yir ) hV F ir(yir ), φir yi「i]	+	K S
ir ∈Sr
NN	2
=2β[1 XFi(φ(r)) - Fi(y(r)) - -1 XhVFi(y,吟,φ(r) - y(r))] + νQ-
N	N	KS
i=1	i=1
(163)
(164)
(165)
(166)
(167)
For some constant ν. In the first inequality we separated out the gradient variance, second inequality
we use the fact that Var(X) — E[X2], third inequality used Assumption B.4, and fourth we took
expectation with respect to the sampled clients. From convexity we have that
Fir (y?) - Fir (x*)	(168)
—O Fir (y(r)),y(r)-χ*i	(169)
=FhVFir(y(r)),φ(r) - y(r)i + hVFir (y*, X(T)-χ*i	(170)
=F hVFir (y(r)),φ(r) - y(T)i + hVFir (yirr) - g(r),x(r) - x*i	(171)
+ hg(r), x(r) - x(r+1)i + hg(r), x(r+1) -x*i	(172)
where the second to last inequality comes from the definition that yi(r) =τx(r) + (1 - τ)φi(r). Taking
expectation with respect to the sampled clients, we have
1N	1 N
N X Fi(y(r)) - F(X*) — W XhVFi(y(r)),φ(r) -y(r)i + Esrhg(r),x⑺-x(r+1)i (173)
i=1	τ	i=1
+ ESr hg(r), x(r+1) -x*i	(174)
For ESr hg(r), x(r) - x(r+1)i, we can employ smoothness at (φ(Ir+1), yI(r)), which holds for any
Ir ∈ Sr0 :	r r
Fir(Φir+1)) - Fir(yir)) — BFIr(y", Φir+ι) - yir)i+2kΦir+ι) - yir)k2	(g
using φ(Ir+1) =τx(r+1) + (1-τ)φ(Ir) and yI(r) =τx(r) + (1-τ)φ(Ir) (though the second is never
explicitly computed and only implicitly exists)
Fir(φlr+1)) - Fir(yIr)) — ThVFir^)),x(r+I)-Xs)i + βτ2kx(r+I)-X叫2	(176)
Taking expectation over Sr0 and using φ(ir+1) =τX(r+1) + (1 - τ)φ(ir) and yi(r) =τX(r) + (1 - τ)φ(ir)
we see that
NN
Esr [ 1 X Fir(Φir+1))] - NN XFi(y(r)) — ThNN XVFi(y(r)),x(r+1) - x(r)i	(177)
ir ∈Sr0	i=1	i=1
+ βτ2kx(r+1) - x(r)k2	(178)
30
Published as a conference paper at ICLR 2022
and rearranging,
hg(r), x(r) - x(r+1)i	(179)
N
≤ TN XFi(F)- τSESr[ X FIr(φir	)]	(180)
i=1	Ir ∈Sr0
N
+ hN X VFi(y(r)) - g(r),x(r+1) - x(r)i + β2-kx(r+1) - x(r)k2	(181)
i=1
Substituting this into Eq. (173) after taking expectation over Sr, and observing that from (Zhou et al.,
2019, Lemma 2) we have identity
hg⑺,x(r+1) - Ui ≤ -ɪ∣∣x(r+1) - x(r)k2 + -1 ||x(r) - uk2	(182)
2η	2η
—1 + η ∣∣χ(r+1) — u∣2 + h(u) — h(χ(r+1))	(183)
so with u = χ*,we get
N
N X Fi(yir) - F(x*)
i=1
NN
≤ W XhVFi(y(r)), φ(r) - y(r)i + -N XFi(y(r))
-	i=1	- i=1
--S ESr,Sr [ X FIr (OIr+I))]
Ir ∈Sr0
N
+ ESrhN X VFi(y(r)) - g(r),x(r+1) - x(r)i + β2-ESrkxe - x(r)k2
i=1
-2η ESrkx(r+1)- x(r)k2+2η kx(r) - x*k2 - 1+ημ ESrkxe-浊广
+ h(x*) - ESrh(x(r+1))
(184)
(185)
(186)
(187)
(188)
(189)
We use the constraint that β- ≤ - - -jβ^ plus Young’s inequality (a, b〉≤ ɪkak2 + Ckbk2 with
η	1-τ	2c	2
C = 1βττ on ESrh寺 PN=ι VFi(y(r)) - g(r),x(r+1) - x(r)i to get
N
N X Fi(y(r))- F(x*)
N i=1
N
≤ 1-τ XhVFi(y(r)),φir) - y(r)i
-N
i=1
N
+ -N X Fi(yi ) - -S ESr,Sr [ X FIr (φIr	)]
i=1	Ir ∈Sr0
N
+ 干ESrkNXVFi(y()-g( k2 + 2ηkx(r) -x*k2
—
1+ημESrkx(r+1) - x*k2 + h(x*) - ESrh(x(r+1))
(190)
(191)
(192)
(193)
(194)
31
Published as a conference paper at ICLR 2022
using the bound on variance, we get
N
NN X Fi(y(r))- F(x*)
N i=1
NN
≤ τN XFi(y(r)) - TSESr,sr[ X FIr(φlr+1))] + K XFi(φir)) - Fi(y(r))
i=1	Ir ∈Sr0	i=1
+2η kx(r)-x*k2-号 ESrkx[-x*k2
+ h(x*) - ESrh(x(r+1)) + (12-TKσ2
combining terms,
TSEsr,sr [ X FIr (φIr+1))] - F(x*)
τ	Ir∈Sr0
≤ l-T X Fi(φfr) + 1- kx⑺-x*k2 - 1+ημ ESrkxS+1) - x*k2
TN	i	2η	2η	r
- ESrh(x(r+1)) +
(1 — τ )νσ2
2βτKS
Using convexity of h and φ(Ir+1) = T x(r+1) + (1 - T)φ(Ir) for Ik ∈ Sr0,
h(φ(Irk+1)) ≤ τh(x(r+1)) + (1 - τ)h(φ(Irk))
After taking expectation with respect to Sr and Sr0 ,
1N	1	1
-ESr [h(x(r+1))] ≤ K X h(φ( )- TESr,sr [W X h(ΦIr ))]
i=1	Ir ∈Sr0
(195)
(196)
(197)
(198)
(199)
(200)
(201)
(202)
(203)
and plugging this back in, multiplying by S/N on both sides, and adding both sides by
TNESr [Pi∈Sr(Fi(φ(r)) - Fi(x*))]
1	1N
-ESr,Sr [NN XFi(Φ(r+1))- Fi(x*)]
τ	i=1
≤ (1-1TrS(ɪ X Fi(φir))- ")) + 吃ESr [X (Fi(Φ(r)) - Fi(x*))]
τN N	τN r
i=1	i∈Sr
+ S kx(r) - x*k2 - (1+ ημ)S E kx(r+1) - x*k2 + (I - T )νσ2
+ 2ηN kx x k 2ηN	ESrkx	x k+ 2βτKN
Observe that the probability of choosing any client index happens with probability S/N , so
t1nESr [X (Fi(Φ(r)) - Fi(x*))] = NTNS (N X Fi(φ(r)) - £(x*))
T	i∈/ Sr0	T	i=1
which implies
1	1N
一ESr,Sr [N ∑Fi(φ(r+1)) - £(x*)]
T N i=1
1	- TS 1 ɪ /、
≤ -Tɪ(N X Fi(φ(r)) - Fi(x*))
T	i=1
+ S kx(r) - x*k2 - (1+ ημ)SE kx(r+1) - x*k2 + (I - T)νσ2
+ 2ηNkx	x k 2ηN ESrkx	x k+ 2βτKN
(204)
(205)
(206)
(207)
(208)
(209)
(210)
32
Published as a conference paper at ICLR 2022
To complete our Lyapunov function so the potential is always positive, we need another term:
1N
-N XhVFi(x*),Φ(r+1) - X*i
i=1
=-N X hVFir(X*),Φir+1) - X*i- N X hVFj(X*),φjr) - X*i
ir ∈Sr	j∈Sr
=X - N EFIr (X*),X(r + 1) - X*i + N EFIr (X*),φlr) - X*i
Ir ∈Sr0
1N
-N XhVFi(X*),φ(r)-X*i
i=1
Taking expectation with respect to Sr, Sr0,
(211)
(212)
(213)
(214)
1N
ESr,Sr0 [-nn XhVFi(X*),φ(r+1)
i=1
S 1N
-x*i] = -(1 - N)(nn XhVFi(X*), φ(r) -
i=1
x*i)
(215)
Let Br = N PlN=I Fi(Of)) - F(X*) - N PN=IhVFi(X*),φ(r) - x*i and Pr = llx(r)
then we can write
1E γr ] . (1 + ημ)sE IP ] <1 - τS B , S P . (1 - T)νσ2
TESr,Sr [Br+1]+	2ηN ESr [Pr+1 ] ≤ ^-Br + 而Pr + 2βτKN
-x*k2,
(216)
Case 1: Suppose that (NKS) ≤ 3, then choosing η =J
we evaluate the parameter constraint βτ ≤ η1 - 1βτT:
1 T = (N∕S)ημ =	J (NS)
3"(N/S)β, T =	1+ημ = 1+∕∑(=/;
< 1,
1 βτ	1	1
βτ ≤ η- ι-τ =⇒ (1 + ι-τ H ≤ β =⇒
2-τ
(N/S)
1-τ1+
3κ
1
3(N∕S)κ
≤
3(N∕S)
κ
(217)
which shows our constraint is satisfied. We also know that
1	= 1 - TNS =	1
T (1 + ημ)	τ	(N∕S)ημ
(218)
So we can write
1
(N/s)ημ
ESr,sr [Br+ι] + 2η(N/S)ESr [Pr+ι]
(219)
≤ (1 + ημ) I((N/S)ημBr + 2η(N∕S)Pr) + ' 2∕tKn
Telescoping the contraction and taking expectation with respect to all randomness, we have
(N∕⅛ E[BR] + 2η(N∕S) E[PR]
≤ (1 + ημ)-R((, EBo +	晨、EPo) + (I- T)νσ2 (1+^μ)
(N∕S)ημ	2η(N∕S)	2βTK N ημ
(220)
(221)
(222)
Bo = F(X(O)) - F(x*) and EBR ≥ 0 based on convexity. Next, We calculate the coefficient of the
variance term.
1―τ 1+^μ = (1 - 1)(1 + ɪ) = (S(1 + ɪ) - 1)(1 + -1)
T ημ T1-	ημ 'N	ημ	ημ
=O((N(1 + √κ(∖∕Zr)) - I)(I + √κ(∖∕^^)))
NS	S
=O(( N+√κ( N )1/2 -I)(I+√κg)1/2))
NN	S
= O(κ)
(223)
(224)
(225)
(226)
33
Published as a conference paper at ICLR 2022
Substituting the parameter choices and using strong convexity,
Ekx(R)- x*k2 ≤ (I + S3(N⅛厂R(2δ + D2)+ O(μKN)	R
Case 2: On the other hand, We can have (NKS) > 4 and choose η = 2*(N⑸,T
■,尸/2) — < 1. One can check that the constraint βτ ≤ 1 - βτ- is satisfied.
1+ 2(N∕S)	2	η IT
After telescoping and taking expectation,
2E[BR] + 2η⅛) E[PR]
≤ (1+η”)-R(2EB0+2η⅛) EP0)+⅛KN2 (中)
Next, We calculate the coefficient of the variance term.
1-τ 1+ημ = (1 - 1)(1 + -) = (S(1 + ɪ) -1)(1 + ɪ)
τ ημ τ	ημ N ημ	ημ
=O((S(1 + N) -1)(1 + N))
NS	S
=O( N)
S
Which gives us
1	2	σ2
Ekx(R)-x*k2 ≤ (1 + E 厂R( r+D2 )+O( βμκs)
Altogether, supposing We choose our parameters as stated in the tWo cases, We have:
(N∕S)ημ
1+ημ
(228)
(229)
(230)
(231)
(232)
(233)
S
EF(X(R)) — F(x ) ≤O(κ∆exp(-min{—,
S- }R) +
Nκ
κσ2
μKS)
(234)
E	Proofs for local update methods
E.1 FedAvg
Theorem E.1. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4
and Assumption B.6. Then running Algo. 4 gives the followingfor returned iterate X:
•	Strongly convex: Fi ’s satisfy Assumption B.1 for μ > 0. If we return the final iterate, set
η = 1, and sample S clients per round arbitrarily,
EF(X(R)) - F(χ*) ≤ O(∆eχp(-R^K) + ζ2 +	)
κ μ μK
Further, if there is no gradient variance and only one client i is sampled the entire algorithm,
∣∣x(r) — x* k2 ≤ O(∣∣x(O) — x* k2 + ∣∣x(0) — x"∣2) for any 0 ≤ r ≤ R
•	General convex: Fi ’s satisfy Assumption B.2. If we return the last iterate after run-
ning the algorithm on Fμ(x) = F(x) + 2 ∣∣x(0) — x∣2 with μ = Θ(max{磊 log2(e2 +
R ), D, DK1/4 }), and η = β+μ,
EF(X(R))- F(x*) ≤ O(g + 黑 + ZD)
KR	K1/4
and for any 0 ≤ r ≤ R,
E∣∣χ(r) - x*∣∣2 ≤ O(D2)
34
Published as a conference paper at ICLR 2022
•	PL condition: Fi S satisfy Assumption B.3 for μ > 0. Ifwe return the final iterate, set
η = 1, and sample one client per round,
EF(X(R))- F (x*) ≤Oceχp(-IRKK ) + 2μ + 2μ√κ)
E.1.1 Convergence of FedAvg for Strongly Convex functions
Proof. By smoothness of F (Assumption B.4), We have for k ∈ {0,..., √K - 1}
FT(T) ʌ	F(T(T)IV riBF(T(r八 c(r)∖ I βη2∣∣∕T)Il2	035、
F (xi,k+ι) - F (xi,k) ≤ -ηhvF (xi,k),gi,ki + Fllgi,k Il	(235)
Using the fact that for any a, b We have -2ab = (a - b)2 - a2 - b2,
F(x(r，1) - F(x(?) ≤ -ηkVF(x(r2)∣∣2 + βη2 - η ∣∣g(r)H2 + ηkg(r) - VF(x(2)∣∣2	(236)
i,k+1	i,k	2	i,k	2 i,k 2 i,k	i,k
Letting η ≤ ɪ,
F(xk+ι) - F(x(rk) ≤ -2IVF(⅛))k2 + 2%)- VF(x⅛∣∣2	(237)
Conditioning on everything up to the k-th step of the r-th round,
% F /+i) - F (x(rk) ≤ - 2 ∣vF (xirk)∣2+2 % 赭? - VF(*?)『	q38)
≤- 2 ∣VF (xirk)∣2 + 与 + η√=	(239)
2	2	2K
Where the last step used the fact that E[X2] = Var(X) + E[X], the assumption on heterogeneity
(Assumption B.5), and the assumption on gradient variance (Assumption B.6) along With the fact
that g?k is an average over √K client gradient queries. Next, using μ-strong convexity of F
(Assumption B.1),
E . F(χ(r) ) - F(X(T)) ≤ -TlLff(F(X(T)) - F(x*)) + η^- + ησ
Er,kF (xi,k+ι) F (xi,k) ≤ ημ(F (xi,k) F (X )) + 2 + 2√K
Which after taking full expectation gives
ηζ2	ησ2
EF(X(Tk+1) - F(x*) ≤ (1 - ημ)(EF(X(Tk)- F(x*)) + η2- + 2√K
Unrolling the recursion over k We get
EF(Xi(,TK) )-F(X*)
≤ (1 - ημ)√K(EF(X(T0)) - F(X*)) + (ηζ2
2	√K-1
+ 2√K) X (I-ημ)k
Also note that X(T+1) = 1 Pi∈s, x(t√^. So by convexity of F,
EF(X(T+1)) - F(X*)
≤1X EF (Xt√K) - F (X*)
i∈Sr
≤ (1 - ημ)√K(EF(X(T))- F(X*)) + (E
2	√K-1
+ 2√K) X (I-ημ)k
Unrolling the recursion over R, We get
EF (X(T)) - F(X*)
广 2	2	τ-1 √K-1
≤ (1 - ημ)τ√K(F(X(O))- F(X*)) + (η2- + τ⅛) XX (1 - ημ)τ√K+k
2	2 K τ=0 k=0
(240)
(241)
(242)
(243)
(244)
(245)
(246)
(247)
(248)
35
Published as a conference paper at ICLR 2022
Which can be upper bounded as
Z2	σ2
EF(XS)) - F(x*) ≤ (1 - ημ)RVK(F(X⑼)-F(x*)) + Z- +	√=	(249)
The final statement comes from the fact that ∣∣x - ηVFi(χ) - x"| ≤ ||x - x"| because of η ≤ β
and applying triangle inequality.	□
E.1.2 Convergence of FedAvg for General Convex functions
For the general convex case, we use Nesterov smoothing. Concretely, we will run Algo. 3 assuming
strong convexity by optimizing instead a modified objective
Fμ(x) = F(x) + μkx⑼-xk2	(250)
Define xμ = argmi% Fμ(x) and △* = EFμ(x(0)) - F(xμ). We will choose μ carefully to balance
the error introduced by the regularization term and the better convergence properties of having larger
μ.
Proof. We know that running Algo. 4 with η = j11μ gives (from the previous proof)
EFμ(x(R)) - Fμ(xμ) ≤ △ eχp(- ^+^) +1— + -~~√==	(251)
节i	2μ	2μ√ K
We have that by (Yuan & Ma, 2020, Proposition E.7)
EF(X(R))- F(x*) ≤ EFμ(x(R))- F“(x%) + 2D2	(252)
So we have (because △* ≤ △ as shown in Thm. D.3),
EF(X(R))- F(x*) ≤ ∆exp(-R√K) + ζμ + 2μ√K + μD2	(253)
Then if we choose μ ≥ Θ( √KR log2(e2 + √KR)),μ ≥ Θ( -ζ), and μ ≥ Θ( DK),
EF(X(R))- F(x*) ≤ O(√⅛ + σDl + ZD)	(254)
KR	K 1/4
Now we show the distance bound. Recall that
EFμ(X(R)) - a(%)≤ △ eχp(- ^+^) +1- + --√=	(255)
〒i 2μ	2μ√K
By smoothness, strong convexity of Fμ, and the choice of μ (as we chose each term above divided by
μ to match D2 up to log factors), we have that
EkX(R) - Xμk2 ≤ O(D2)	(256)
So,
EkX(R) - X*k2 ≤ 3E∣x(R) - Xμ∣2 + 3E∣∣X(0) - x%||2 +3E∣∣x* - X(0)k2 ≤ O(D2)	(257)
Where the last inequality follows because
F(Xμ) + 2|x(0) - Xμk2 ≤ F(x*) + 2∣x⑼-X*k2	(258)
□
E.1.3 Convergence of FedAvg under the PL condition
Proof. The same proof follows as in the strongly convex case, except Eq. (244), where we avoid
having to use convexity by only sampling one client at a time. This follows previous work such as
Karimireddy et al. (2020a). Averaging when the functions are not convex can cause the error to blow
up, though in practice this is not seen (as mentioned in Karimireddy et al. (2020a)).	□
36
Published as a conference paper at ICLR 2022
F Proofs for FedChain
F.1 FEDAVG → SGD
F.1.1 CONVERGENCE OF FEDAVG → SGD ON STRONGLY CONVEX FUNCTIONS
Theorem F.1. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4,
Assumption B.6, Assumption B.7, Assumption B.5, Assumption B.8. Then running Algo. 1 where
Alocal is Algo. 4 in the setting of Thm. E.1 and Aglobal is Algo. 2 in the setting Thm. D.1:
•	Strongly convex: Fi S satisfy Assumption B.1 for μ > 0. Then there exists a lower bound of
K such that we have the rate
ζ2	R
O(mm{ —, ∆} exp(-) +
σ2
μSKR
+ (1- NS) ⅛R
μ	K
•	General convex: Fi ’s satisfy Assumption B.2. Then there exists a lower bound of K such
that we have the rate
入, .邛 12ζ1/2D3/2 βD∖	e1/2b1/2D3/2
(min{	R1∕2	, F} + (SKR)1/4
+ (1 - S )1/4 β 1/2zF/2D + (1 - S )1/4 e1/2z1/2D3/2)
+ ( N) S1/4R1/2 + ( N)	(SR)1/4	)
•	PL condition: Fi's satisfy Assumption B.3 for μ > 0. Then there exists a lower bound of K
such that we have the rate
ζ 2	R
O(mιn{ —, ∆} exp(-) +
2
κσ2
μNKR
+(1- N
μ	κ
Proof. From Thm. E.1, we know that with K > max{σ4, R log2(∆μ)}
ζ2
EF(X1/2) - F(x*) ≤O(Z)	(259)
2
From Lemma H.2, if K >--F /2
S min{∆, Z2 }
EF(X1) - F(x*) ≤ O(min{U, △} + J- S-I工)	(260)
μ V N - 1 √S
And so from Thm. D.1, we know that
EF(x2) - F(x*)	(261)
≤ O(min{ —, △} eχp(--) +-:KR + (1 -*~Zrp + \卜-^j^¾))	(262)
μ	κ μNKR	N μSR V N SS
□
F.1.2 CONVERGENCE OF FEDAVG → SGD ON GENERAL CONVEX FUNCTIONS
σ4 β2D2
Proof. From Thm. E.1, We know that With K > max{ σ4, β^D }
EF(X(R)) - F(x*) ≤ O(ZD)
2
From Lemma H.2, if K >-F /2
S min{∆, Z2 }
一，、—〜， 一… / S -1 ZF、
EF(X1) - F(Xi ≤ O(min{ZD, △} + √1 - M)	(263)
N-1 S
37
Published as a conference paper at ICLR 2022
And so from Thm. D.1, we know that
EkVF(x2)k2 ≤ O(βmin俨,2
R
βσD , S~Se
√SKR + V	N √
(264)
Next, using that E∣∣X2 - x*k ≤ O(D2) from Thm. E.1 and Thm. D.1 as well as convexity,
EF(x2) - F(x*)	(265)
≤ PEkVF(X2)k2PE∣∣X2 - x*k2	(266)
- β1/2 min{Z 1/2D3/2, ∆1∕2D} ≤O(	R/	(267)
+ βν2σ"D3/2	+	S)1/4 β"ZF∕2D	+	S)1/4 β"Z1/2D3/2	) + (SKR)1/4	+	( N) S1/4R1/2	+	( N)	(SR)1/4	)	(268)
One can pre-run SGD before all of this for a constant fraction of rounds so that (Woodworth et al.,
2020a, Section 7):
△ <O(以 + q +
_ R	√SKR
This likely not practically necessary § 6. Altogether,
EF(X2) - F(x*)
≤ PEkVF(X2)k2PE∣∣X2 - x*k2
S [β1/2Z1/2D3/2 βD2ι
≤O(min{ -R/— F }
+ β“σ"D3/2 + S )1/4 e1/2ZF/2D +	S )1/4 β"Z1/2D3/2 )
十 (SKR)1^4 + ( N) S1/4R1/2 + (	N)	(SR)1/4 )
(269)
(270)
(271)
(272)
(273)
□
F.1.3 CONVERGENCE OF FEDAVG → SGD UNDER THE PL-CONDITION
Proof. The proof is the same as in the strongly convex case.
F.2 FEDAVG → ASG
Theorem F.2. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4,
Assumption B.6, Assumption B.7, Assumption B.5, Assumption B.8. Then running Algo. 1 where
Alocal is Algo. 4 in the setting of Thm. E.1 and Aglobal is Algo. 3 in the setting Thm. D.1:
•	Strongly convex: Fi S satisfy Assumption B.1 for μ > 0. Then there exists a lower bound of
K (same as in FedAvg → SGD) such that we have the rate
O(min{ —, △} exp(--R) + ζ	+ (1 — S)ɪ; + J- - S今)
μ	ʌ/K	μSKR	N μSR V N SS
•	General convex: Fi ’s satisfy Assumption B.2. Then there exists a lower bound of K (same
as in FedAvg → SGD) such that we have the rate
β∖/Z1/2D3/2 βD2 } + β"σ"D3/2 + σD	S)1/2 ZD
(min{	R , R2 } + (SKR)1/4 + (SKR)1/2 +( N)	(SR)1/2
+ (1- S )1/4
β1∕2ζy2D
S1/4R1/2
+	S)1/4 β1∕2Z 1/2D3/2
+ ( N)	(SR)1/4
Proof follows those of FedAvg → SGD (Thm. F.1).
□
)
38
Published as a conference paper at ICLR 2022
F.3 FEDAVG → SAGA
Theorem F.3. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4,
Assumption B.6, Assumption B.7, Assumption B.5, Assumption B.8. Then running Algo. 1 where
Alocal is Algo. 4 in the setting of Thm. E.1 and Aglobal is Algo. 5 in the setting Thm. D.4:
•	Strongly convex: Fi S satisfy Assumption B.1 for μ > 0. Then there exists a lower bound of
K (same as in FedAvg → SGD) such that we have the rate
O(min{ —, ∆} eχp(-max{N,κ}-1R) + σ )
μ	S	μSKR
•	PL condition: Fi S satisfy Assumption B.3 for μ > 0. Then there exists a lower bound of K
such that we have the rate
O(min{ q, ∆} eχp~κ( N )2/3)TR)+σ-)
μ	S	μSK
Proof follows those of FedAvg → SGD (Thm. F.1) and using Lemma H.2 with S = N as the
algorithm already requires R > S.
F.4 FEDAVG → SSNM
Theorem F.4. Suppose that client objectives Fi ’s and their gradient queries satisfy Assumption B.4,
Assumption B.6, Assumption B.5, Assumption B.8. Then running Algo. 1 where Alocal is Algo. 4 in
the setting of Thm. E.1 and Aglobal is Algo. 6 in the setting Thm. D.5:
• Strongly convex: Fi's satisfy Assumption B.1 for μ > 0. Then there exists a lower bound of
K (same as in FedAvg → SGD) such that we have the rate
O,(min{ ζ2,2 eχp(-maχ{ N,yκ(N) }-1R)+
κσ2
μKS)
Proof follows those of FedAvg → SGD (Thm. F.1) and using Lemma H.2 with S = N as the
algorithm already requires R > S.
G Lower B ound
In this section we prove the lower bound. All gradients will be noiseless, and we will allow full
communication to all clients every round. There will be only two functions in this lower bound: F1
and F2. IfN > 2, then F1 is assigned to the first bN/2c clients and F2 to the next bN/2c clients. If
there is an odd number of machines We let the last machine be F3(x) = 2 ∣∣χ∣∣2. This only reduces
the lower bound by a factor of at most N-I. So We look at the case N = 2.
Let `2 , C, ζbe values to be chosen later. Let d be even. At a high level, `2 essentially controls the
smoothness of F1 and F2, C is basically a constant, andζis a quantity that affects the heterogeneity,
initial suboptimality gap, and initial distance. We give the instance:
ʌ	e`o C ',匕	C μ C
Fl(x) = -'2ζx1 +---2Xd + ^2- ^X (x2i+1 - x2i) + 2 Ilxll	(274)
i=1
` d/2
F2 (X) = W £(x2i - x2i-1)2 + μ llxl2	(275)
2 i=1	2
Where F = fi+f2 .
These functions are the same as those used in (WoodWorth et al., 2020a; WoodWorth, 2021) for
loWer bounds in distributed optimization. They are also similar to the instances used to prove convex
39
Published as a conference paper at ICLR 2022
optimization lower bounds (Nesterov, 2003) and distributed optimization lower bounds (Arjevani &
Shamir, 2015).
These two functions have the following property
「	1	∫VF1ι(x
even) ∈ span{e1 , e2 , . . . , e2i+1 }
xeven ∈ span{e1 , e2 , . . . , e2i} =⇒	(276)
VF2(xeven) ∈ span{e1,e2, . . . ,e2i}
VF1(xodd) ∈ span{e1, e2, . . . , e2i-1}
xodd ∈ span{e1, e2, . . . , e2i-1} =⇒	(277)
VF2(xodd) ∈ span{e1,e2,. . . ,e2i}
What the property roughly says is the following. Suppose we so far have managed to turn an
even number of coordinates nonzero. Then we can only use gradient queries of F1 to access the
next coordinate. After client 1 queries the gradient of F1, it now has an odd number of unlocked
coordinates, and cannot unlock any more coordinates until a communication occurs. Similar is true if
the number of unlocked coordinates is odd. And so each round of communication can only unlock a
single new coordinate.
We start by writing out the gradients of F1 and F2 . Assume that d is even. Then:
[VF1(x)]1 = -'2Z + μxι	(278)
[VF1(x)]d = C'2xd + μxd	(279)
∣[VF1(x)]i = '2(xi - xi-ι)+ μxi i odd, 2 ≤ i ≤ d 一 1	0)
I [VFι(x)]i = —'2(xi+1 — Xi) + μxi i even, 2 ≤ i ≤ d — 1
and
[VF2(x)]ι = —'2(x2 — xι) + μxι	(281)
[VF2(x)]d = '2(xd — Xd-ι) + μxd	(282)
([VF2(x)]i = —'2(xi+1 — Xi)+ μxi i odd, 2 ≤ i ≤ d — 1	3)
∖[VF2(x)]i = '2(xi — Xi-1) + μxi i even, 2 ≤ i ≤ d — 1
We define the class of functions that our lower bound will apply to. This definition follows (Wood-
worth et al., 2020a; Woodworth, 2021; Carmon et al., 2020):
Definition G.1 (Distributed zero-respecting algorithm). For a vector v, let supp(v) = {i ∈
{1, . . . , d} : vi 6= 0}. We say that an optimization algorithm is distributed zero-respecting if
(r)
for any i, k, r, the k-th iterate on the i-th client in the r-th round Xi,k satisfy
supp(Xi(,rk) ) ⊆ [ supp(VFi(Xi(,rk)0))	[	supp(VFi0 (Xi(0r,0k)0 ))	(284)
0≤k0<k	i0∈[N],0≤k0≤K-1,0≤r0<r
Broadly speaking, distributed zero-respecting algorithms are those whose iterates have components
in only dimensions that they can possibly have information on. As discussed in (Woodworth et al.,
2020a), this means that algorithms which are not distributed zero-respecting are just ”wild guessing”.
Algorithms that are distributed zero-respecting include SGD, ASG, FedAvg, SCAFFOLD, SAGA,
and SSNM.
Next, we define the next condition we require on algorithms for our lower bound.
Definition G.2. We say that an algorithm is distributed distance-conserving if for any i, k, r, we have
for the k-th iterate on the i-th client in the r-th round x(? satisfies kx(2 — x*∣∣2 ≤ (。/2)|||/也让—
x*k2 + PN=IIlXinit — x"∣2], where x* := argminχ Fj(x) and x* := argmi% F(x) and Xinit is the
initial iterate, for some scalar parameter c.
Algorithms which do not satisfy Definition 5.2 for polylogarithmic c in problem parameters (see § 2)
are those that move substantially far away from X*, even farther than the Xi*’s are from X*. With this
definition in mind, we slightly overload the usual definition of heterogeneity for the lower bound:
Definition G.3. A distributed optimization problem is	(ζ, c)-heterogeneous	if
maxi∈[N ]	supx∈A IVFi(X)	— VF (X)I2	≤	ζ2,	where we define A :=	{X	: IX —	X*	I2 ≤
(c/2)(IXinit — X* I2 + PiN=1 IXinit — X* I2)} for some scalar parameter c.
40
Published as a conference paper at ICLR 2022
While convergence rates in FL are usually proven under Assumption B.5, the proofs can be converted
to work under Definition G.3 as well, so long as one can prove all iterates stay within A as defined
in Definition G.3. We show that our algorithms satisfy Definition G.3 as well as a result (Thm. E.1,
Thm. D.3, Thm. D.1)4. Other proofs ofFL algorithms also satisfy this requirement, most notably the
proof of the convergence of FedAvg in Woodworth et al. (2020a).
Following (Woodworth et al., 2020a), we start by making the argument that, given the algorithm is
distributed zero-respecting, we can only unlock one coordinate at a time. Let Ei = span{e1, . . . , ei},
and E0 be the null span. Then
Lemma G.4. Let X be the output ofa distributed zero-respecting algorithm optimizing F = 2 (Fι +
F2) after R rounds of communication. Then we have
SUPP(X) ∈ Er	(285)
Proof. A proof is in Woodworth et al. (2020a, Lemma 9).	□
We now comPUte varioUs ProPerties of this distribUted oPtimization Problem.
G.1 STRONG CONVEXITY AND SMOOTHNESS OF F, F1, F2
From Woodworth (2021, Lemma 25), as long as '2 ≤ β-μ, we have that F, Fι, F2 are β-smooth and
μ-strongly convex.
G.2 THE SOLUTIONS OF F, F1, F2
First, observe that from the gradient of F2 computed in Eq. (281), x2 = arg mi% F2 (x) = ~. Next
^^
observe that from the gradient of Fi computed in Eq. (278), x； = argmmX Fι(x) = -2ζeι. Thus
22
kx； k2 = 0 and kχik2 =备.
From Woodworth (2021, Lemma 25), if we let α = ʌ/l + 2-2, q = a-1, C = 1 - q, then
V μ	a+1
k ；k k2 _ 心	Pd	2ii	_	Z q2(1-q2d)
kX k = (1-q)2	乙i=1	q	=	(1-q)2(1-q2)	.
G.3 Initial suboptimality gap
From Woodworth (2021, Lemma 25), F(0) - F(x；) ≤ 总--^).
G.4 SUBOPTIMALITY GAP AFTER R ROUNDS OF COMMUNICATION
22
From (Woodworth, 2021, Lemma 25), F(X) - F(x；) ≥ 以1-^/%*)q2R, as long as d ≥ R +
log2
2log(1∕q).
G.5 COMPUTATION OF ζ2
We start by writing out the difference between the gradient of F1 and F2, using Eq. (278) and
Eq. (281).
d-1
∣∣VFi(χ) - VF2(x)k2 = '2(-ζ + χ2 - XI)2 + '2(CXd - xd + Xd-I)2 + X '2(Xi+1 - Xi-I)2
i=2
(286)
We upper bound each of the terms:
(Xi+1 - Xi-1) = Xi+1 - 2Xi+1Xi-1 + Xi-1 ≤ 2Xi+1 + 2Xi-1	(287)
4We do not formally show it for SAGA and SSNM, as the algorithms are functionally the same as SGD and
ASG under full participation.
41
Published as a conference paper at ICLR 2022
(-Z + x2 — xl)2 = Z2 + x2 + X2 — 2Zx2 + 2Zxi — 2x2x1 ≤ 3Z2 + 3x2 + 3xi	(288) (289)
(Cxd - xd	+ xd-1)2	= C2x2d	+ x2d + x2d-1 -	2C x2d + 2Cxdxd-1	- 2xdxd-1	(290)
≤ C2x2d	+ 2x2d + 2x2d-1	+ 2Cx2d-1	(291)
So altogether, using the fact that C ≤ 1,
1 kVFi(x) -VF2(x)k2 `2	d-1	(292)
≤ 3Z2 + 3x2 + 3x2 + 3xd + 3xd-1 +): 2x2+ι + 2x2-1		(293)
	i=2	
≤ 3Z2 +7∣xk2		(294)
≤ 3Z2 + 14∣x - x*∣∣2 + 14∣∣x*k2		(295)
沁	Z2 q2	'	12 Z2	
≤ 5Z2 +28c (Li) +28c-	2 下 μ	(296)
NeXt We use Definition G.2 which Says that ∣∣x - x*k ≤ C[||电向一x*∣∣2 + PN=I Ilxinit - x"∣2]∙ For
ease of calculation, We assume C ≥ 1. Recall that We calculated ∣∣x*k2 ≤(一标,：*)
22
0 < q < I), kχ2k2 = 0, kxιk2 = ɪ.
妥kVFι(x) - VF2(x)k2 ≤ 3Z2 +24c∣x*k2 +7c∣M∣2 + 7c∣x^∣2
`2
≤ 3Z2 +
24cZ2q2	7c'2Z2
(1-q)2(1-q2) + ~7r~
(because
(297)
(298)
Altogether,
kvF1(x) - vF2(x)k2 ≤ 3'2ζ2 + (1 -4X-2q2) + 苧	(299)
G.6 Theorem
With all of the computations earlier, We are noW prepared to prove the theorem.
Theorem G.5. For any number of rounds R, number of local steps per-round K, and (ζ, c)-
heterogeneity (Definition 5.3), there exists a global objective F which is the average of two β-smooth
(Assumption B.4) and μ(≥ 0)-strongly convex (Assumption B.1) quadratic client objectives Fi and
F2 with an initial SUb-OPtimality gap of ∆, such that the output x ofany distributed zero-respecting
(Definition 5.1) and distance-conserving algorithm (Definition 5.2) satisfies
• Strongly convex: F(x) - F(x*) ≥ Ω(min{∆, 1∕(cκ3/2)(Z2∕β)} exp(-R∕√κ).) when
μ > 0, and
• General Convex F(x) - F(x*) ≥ Ω(min{βD2∕R2,ZD∕(c"√R5)}) when μ = 0.
Proof. The convex case: By the previous computations, We knoW that after R rounds,
F (X)- F S) ≥ d) q2R	(300)
kx*k2 ≤ W)	(301)
kVFι(x) -VF2(x)k2 ≤ 3'2Z2 +	24C'2 Z2q2 2 +7c4ζ2	(302)
(1 - q)2(i - q2)	μ2
42
Published as a conference paper at ICLR 2022
To maintain the property that kx(0) - x*k = ||x* k ≤ D and Definition G.3, Choose Z s.t.
2	ζ2 Z = V min{ '2 ,		(1 - q)2(1 - q2)	Z2 μ2Z2 (1 - q)2(1 - q2)D , c'2 ,	q2	2	(303)
		c'2q2			
For an absolute constant	V.				
This leads to (throughout we use		'μ = (1-q)2, (Woodworth, 2021, Eq 769))			
F(X) - F(x*) ≥	μZ2q2 ≥ 16(1 - q)2 (1 - q ≥vmin{μζ2 g	) q2R q)2(1	-q2)Z2 μ3Z2 μ(1-q)2(1-q2)D2 ʌ			q2	(304) (305) q2R -q2)q (306) (307) (308) (309) (310)
≥ V min{ '2 , 、.ʃ(1 - q)2ζ2 ≥ V min{ μ2(I - q)4 μ(1 - 4c'2q2	, 、∙ 「	Z2q ≥V min{ 32'20-^	c'2q2	, c'2 , μ(1 - q)2(1 - q2)Z2 ,	c'2q2	, -q)2(1 - q2)D2 }	 ~q2	} 16(1 - _ μz2	μζ2(1 - q) 2), 16c'2 , 64c'2(1 + q),		q2	j16(1 - q2-	q2R q)2(1 - q2)q μD2 }q2R 16 }q	W	
We choose μ = 6R, which ensures a ≥ 2. So noting that (1 - q2) = (1 - 1 + q = α2+αι,1 - q = α++ι:			q)(1 + q) ≤ (1 + q),
F(X) - F(x*)	(311) ,「Z2 , α — 1 α + 1、μZ2 μZ2 / 2 α + 1、μD2 . 2r	…八、 ≥ v min{ʌ- ( ɪ. ɔ ), ɪ, ⅛ ( ɪ1 9	), ⅜r}q2R	(312) 32'2 α + 1 2α	16c'22 64c'22 α + 1 2α	16 、	.ʃ z2 尸-1 α +1、 〃Z2	〃Z2 , 2 α + 1∖ μD2ι / oκ>1 尸 + 1\、 ≥ Vmin⅛τ(—-), 7FT2,	(—-), ~T7Γ} exp(-2Rlog(	T)) 32'2 α + 1 2α	16c'2 64c'2 α + 1 2α	16	α - 1 (313) 、∙ ʃ z2	〃Z2	〃Z2 ∕1∖ μD21.	/	4R ≥ V min{ TΓΓF, Ia 02 , R…2 (_), -I } } exp(	T)	(314) 64'2 16c'22 64c'22 α 16	α - 1			
So,	F(X) - F(x*) Z 2	μZ 2	μZ 2 ≥ Vmin{丽,吟,64c'∣'	金 UE } exP(-8√√	(315) =μ)	(316)
	Z2	Z2	Z2 ≥ °(3出a,c'2R2,C'2>, ≥ Ω(上处}) ≥ 2(c'2R3, R2 })	平})	(317) (318) (319)
Where We used c > 1. Setting '2 = Θ(min{β,以分根氏“ }) (which will ensure that '2 ≤ β-μ with
appropriate constants chosen), Which altogether gives us
F(X)- F(x*) ≥ Ω(min{ci/fD^, βD})	(320)
Strongly Convex Case:
43
Published as a conference paper at ICLR 2022
By the previous computations, we know that after R rounds,
∕μZ2q2	…
F(X)- F(x*) ≥	、2qi~~2Γq	(321)
16(1 - q)2(1 - q2)
q'2ζ2
F(X)- F(x*) ≤ 4q'-q)	(322)
∣∣VF1(x)-VF2(x)k2 ≤ 3'2Z2 +( J*32 2 +7c`4ζ2	(323)
(1 — q)2(i — q2)	μ2
To maintain the property that F(X) — F(x*) ≤ ∆ and that ∣∣VFι(χ) — VF2(χ)k2 ≤ Z2 for all X
encountered during the execution of the algorithm,
Choose ζ s.t.
V min{ '2 ,
(1 - q)2(i - q2)Z2 42Z2 (1 — q)∆
c'2q2	， c'4 ，	q'2
(324)
For some constant ν.
Again using 'μ = (1-q) ) and following the same calculations as in the convex case, We use the fact
that 9μ ≤ β, and that it is possible to choose '2 s.t. 2μ ≤ '2 ≤ β-μ, which ensures β ≥ 2.
F (X) — F (x*)
≥	〃Z2 q2	2R
― 16(1 — q)2(1 — q2) q
、	∙ ʃ ζ2	"2	"2 3 δ1.	4	4R、
≥V min{ 丽,≡2,64C'i(α), Z}exp(—o-i)
(325)
(326)
(327)
(328)
We use 9μ ≤ β and because it is possible to choose '2 such that 2μ ≤ '2 ≤ β-μ (Woodworth, 2021,
Eq. 800), which ensures α ≥ 2 and 1 ≥、&
F (X) — F (x*)
≥ °(min{ Z ,¾^ ,⅞τ('2)1/2, ∆} exp(—8√√μ))
≥ a(min{ ⅛2(μ )1/2,∆} exp(—8R√μ))
Next we note that '2 ≤ β and '2 ≥ β (Woodworth, 2021, Eq 801), which gives US
F(X) — F(x*) ≥ Ω(min{^/^, ∆} exp( — √R))
(329)
(330)
(331)
(332)
□
H Technical Lemmas
Lemma H.1. Let
K
g⑺:=SK XXg(,rk)	(333)
i∈Sr k=1
Given Assumption B.6, Assumption B.5, and uniformly sampled clients per round,
Ek 1 X X Jr) — VF(X(T))k2 ≤ σ + (1 — S — 1 )Zl	(334)
Ek SK 2^2^gi,k VF(X 川 ≤ SK + (1 N-I)S	(334)
i∈Sr k=1
44
Published as a conference paper at ICLR 2022
If instead the clients are arbitrarily (possibly randomly) sampled and there is no gradient variance,
K
kSK X Xg(rk) - VF(x(r))k2 ≤ Z2	(335)
i∈Sr k=1
Proof.
1K
EkSKE EVf (x； Zi)-VF(x)k2	(336)
i∈Sr k=1
1	1	K-1
=EkS E VFi(X)- VF(x)k2 + ESrkSKE E Vf (x； Zi)- VFi(X)k2	(337)
i∈Sr	i∈Sr k=0
≤ Ek 1 X VFi(X)-VF(x)k2 + ɪEkVf (x; Zi)-VFi(X)k2	(338)
S	SK
i∈Sr
(339)
Where the first inequality uses the fact that E[X2] = Var(X) + E[X]2, and the second equality
uses the fact that the variance is i.i.d across i and k. Note that EkVf (x； Zi) - VFi(x)k2 ≤ σ2 by
Assumption B.6. On the other hand by Assumption B.5
Ek1 X VFi(x) - VF(x)k2 ≤ (1 - H) EkVFi(X)-VF(X)k2 ≤ (1 - H) W
S	N-1	S	N-1 S
i∈Sr
(340)
So altogether
1 K	σ2	S 1 ζ2
EkSKE EVf (x; Zi)-VF(x)k2 ≤ SK + (1- N-I)S	(341)
i∈Sr k=1
The second conclusion follows by noting
kS X VFi(x) - VF(x)k2 ≤ 1 X kVFi(x) - VF(x)k2	(342)
i∈Sr	i∈Sr
□
Lemma H.2. Let u,v be arbitrary (possibly random) points. Define F(X)	=
SK Pi∈s PK-I f (x； zi,k) where zi,k 〜Di. Let
w = arg min F (x)
x∈{u,v}
Then
E[F(w) - F(x*)]	(343)
≤ min{F(U)- F(x*), F(v) - F(x*)} + 4焉 + 4^-S-L √	(344)
and
E[F (w) — F (x*)]	(345)
≤ min{EF(U)- F(x*), EF(v) - F(x*)} + 4氤 + 4ʌ/1-^—∙∣√F	(346)
Proof. Suppose that F(u) + 2a = F (v), where a ≥ 0. Then,
E[F(w) - F(x*)] = P(w = u)(F(u) - F(x*)) + P(w = v)(F(v) - F(x*))	(347)
45
Published as a conference paper at ICLR 2022
Substituting F (v) = F (u) + 2a,
E[F (W) — F (x*)] = P (W = U)(F(U) — F (x*)) + P(W = V)(F(U) + 2a — F (x*))	(348)
≤ F(U) — F(x*) + 2a	(349)
Observe that E(F(X) — F(x))2 ≤ 藐 + (1 — nN-I)ζF by Assumption B.7 and Assumption B.8.
Therefore by Chebyshev’s inequality,
P(IF(X)- F(X)l≥ a) ≤ J(Sl + (1- N-1)ζ2)
Observe that
P(w = v) ≤ P(F(u) > F(u) + a) + P(F(V) < F(v) — a)
≤与(除+ (1 —
a2 SK
H)ζ2)
And so it is also true that
E[F (w) — F (x*)] = F (u) — F (x*) + 2aP (w = v)
≤F (U)- F (x*)+2a( ⅛(轰+(1—N-4) ZF))
=F (U) - F (x*) + 4(条 + (1 - H) Zl)
a SK N— 1 S
Therefore altogether we have that
4 σ 2	S 1 ζ 2
E[F(w) — F(x*)] ≤ F(u) — F(x*) + maxmin{-(+ (1 —	)ZF), 2a}
a a S K	N— 1 S
So by maximizing for a
-(金
a(SK
+ (1 —
j2(轰+(1—N-⅛)ZF)
which gives us
E[F(w) — F(x*)] ≤ F(u) — F(x*) + 4√FK + 4J- N-I√
(350)
(351)
(352)
(353)
(354)
(355)
(356)
(357)
(358)
Then proof also holds if we switch the roles of U and v, and so altogether we have that: if F(U) <
F(v),
E[F(w) — F(x*)] ≤ F(u) — F(x*) + 4√FK + 4J- N-I√
if F(U) > F(v),
E[F(w) — F(x*)] ≤ F(v) — F(x*) + 4√FK + 4J- N-I宗
implying the first part of the theorem.
(359)
(360)
E[F(w) — F(x*)] ≤ min{F(U)- F(x*), F(v) — F(x*)} + 4√FK + 4^1 — N-I冬
(361)
For the second part,
E[min{F (U) — F (x*),F (v) — F (x*)}∣v] = /	F (U) — F (x*)
≤ min{EF(u) — F(x*), F(v) — F(x*)}
and then
E[min{EF (u) — F (x*),F (v) — F (x*)}] = /	F (u) — F (x*)
≤ min{EF(u) — F(x*),EF(v) — F(x*)}
(362)
(363)
(364)
(365)
□
46
Published as a conference paper at ICLR 2022
I Experimental S etup Details
I.1	Convex Optimization
We empirically evaluate FedChain on federated regularized logistic regression. Let (xi,j , yi,j) be the
jth datapoint of the ith client and ni is the number of datapoints for the i-th client. We minimize
Eq. (1) where
_ . 、	1 3	-	， 丁 、	，	、一， ɪ ... μ.. ..O
Fi(W) = —(E -yi,j log(w>xi,j) - (I - yi,j) IOg(I - W Xij))) + 判wk2.
ni j=1	2
Dataset. We use the MNIST dataset of handwritten digits (LeCun et al., 2010). We model a federated
setting with five clients by partitioning the data into groups. First, we take 500 images from each
digit class (total 5,000). Each client’s local data is a mixture of data drawn from two digit classes
(leading to heterogeneity), and data sampled uniformly from all classes.
We call a federated dataset X% homogeneous if the first X% of each class’s 500 images is shuffled
and evenly partitioned to each client. The remaining (100 - X)% is partitioned as follows: client
i ∈ {1,. . . , 5} receives the remaining non-shuffled data from classes 2i - 2 and 2i - 1. For example,
in a 50% homogeneous setup, client 3 has 250 samples from digit 4, 250 samples from digit 5, and
500 samples drawn uniformly from all classes. Note that 100% homogeneity is not the same thing as
setting heterogeneity ζ = 0 due to sampling randomness; we use this technique for lack of a better
control over ζ. To model binary classification, we let even classes represent 0’s and odd classes
represent 1’s, and set K = 20. All clients participate per round.
Hyperparameters. All experiments initialize iterates at 0 with regularization μ = 0.1. We fix the
total number of rounds R (differs across experiments). For algorithms without stepsize decay, the
tuning process is as follows.
We tune all stepsizes η in the range below:
{10-3, 10-2.5, 10-2, 10-1.5, 10-1}	(366)
We tune the percentage of rounds before switching from Alocal to Aglobal (if applicable) as
{10-2, 10-1.625, 10-1.25, 10-0.875, 10-0.5}
(367)
For algorithms with acceleration, we run experiments using the more easily implementable (but less
mathematically tractable for our analysis) version in Aybat et al. (2019) as opposed to Algo. 3.
For algorithms with stepsize decay, we instead use the range Eq. (367) to decide the percentage
of rounds to wait before decreasing the stepsize by half-denote this number of rounds as Rdecay.
Subsequently, every factor of 2 times Rdecay we decrease the stepsize by half.
We use the heuristic that when the stepsize decreases to n/K, we switch to Agiobai and begin the
stepsize decay process again. All algorithms are tuned to the best final gradient norm averaged over
1000 runs.
I.2	Nonconvex Optimization
I.2.1	EMNIST
In this section we detail how we use the EMNIST (Cohen et al., 2017) dataset for our nonconvex
experiments, where the handwritten characters are partitioned by their author. In this dataset, there
are 3400 clients, with 671,585 images in the train set and 77,483 in the test set. The task is to train a
convolutional network with two convolutional layers, max-pooling, dropout, and two dense layers as
in the Federated Learning task suite from (Reddi et al., 2020).
Hyperparameters. We set R = 500, and rather than fixing the number of local steps, we fix the
number of local client epochs (the number of times a client performs gradient descent over its own
dataset) to 20. The number of clients sampled per round is 10. These changes were made in light of
the imbalanced dataset sizes across clients, and is standard for this dataset-task (Reddi et al., 2020;
Charles & Konecny, 2020; Charles et al., 2021). For all algorithms aside from (M-SGD), we tune
47
Published as a conference paper at ICLR 2022
η in the range {0.05, 0.1, 0.15, 0.2, 0.25}. We tune the percentage of rounds before switching from
Alocal to Aglobal in {0.1, 0.3, 0.5, 0.7, 0.9}, where applicable.
We next detail the way algorithms with stepsize decay are run. We tune the stepsize η in
{0.1, 0.2, 0.3, 0.4, 0.5}. Let Rdecay be the number of rounds before the first decay event. We
tune Rdecay ∈ d{0.1R, 0.275R, 0.45R}e. Whenever the number of passed rounds is a power of
2 of Rdecay, we halve the stepsize/ After R0 rounds have passed, we enter Aglobal, where we tune
R0 ∈ d{0.5R, 0.7R, 0.9R}e. We restart the decay process upon entering Aglobal.
For M-SGD we tune
η ∈ {0.1, 0.2, 0.3, 0.4, 0.5}
and
Rdecay ∈ d{10-2R, 10-1.625R, 10-1.25R, 10-0.875R, 10-0.5R}e
For stepsize decaying SCAFFOLD and FedAvg, we tune η ∈ {0.1, 0.2, 0.3, 0.4, 0.5} and Rdecay ∈
d{0.1R, 0.275R, 0.45R}e. When evaluating a particular metric the algorithms are tuned according
to the mean of the last 5 evaluated iterates of that particular metric. Reported accuracies are also the
mean of the last 5 evaluated iterates, as the values mostly stabilized during training.
I.2.2	CIFAR-100
For the CIFAR-100 experiments, we use the CIFAR-100 (Krizhevsky, 2009) dataset, where the
handwritten characters are partitioned via the method proposed in (Reddi et al., 2020). In this dataset,
there are 500 train clients, with a total of 50,000 images in the train set. There are 100 test clients and
10,000 images in the test set. The task is to train a ResNet-18, replacing batch normalization layers
with group normalization as in the Federated Learning task suite from (Reddi et al., 2020). We leave
out SCAFFOLD in all experiments because of out of memory issues.
Hyperparameters. We set R = 5000, and rather than fixing the number of local steps, we fix
the number of local client epochs (the number of times a client performs gradient descent over its
own dataset) to 20. The number of clients sampled per round is 10. These changes were made
in light of the imbalanced dataset sizes across clients, and is standard for this dataset-task (Reddi
et al., 2020; Charles & Konecny, 2020; Charles et al., 2021). For all algorithms We tune η in
{0.1, 0.2, 0.3, 0.4, 0.5}. Algorithms without stepsize decay tune the percentage of rounds before
sWitching to Aglobal in {0.1, 0.3, 0.5, 0.7, 0.9}. Algorithms With stepsize decay (aside from M-SGD)
tune the number of rounds before the first decay event in {0.1, 0.275, 0.45} (and for every poWer of 2
of that percentage, the stepsize decays in half), and the number of rounds before sWapping to Aglobal
in {0.5, 0.7, 0.9} if applicable (stepsize decay process restarts after sWapping). M-SGD tunes the
number of rounds before the first decay event in {0.01, 0.0237, 0.0562, 0.1334, 0.3162}. We tune for
and return the test accuracy of the last iterate, as accuracy Was still improving as We trained.
J	Additional Convex Experiments
In this section, We give supplemental experimental results to verify the folloWing claims:
1.	Higher K alloWs (1) accelerated algorithms to perform better than non-accelerated algo-
rithms, and furthermore (2) alloWs us to run Alocal for one round to get satisfactory results.
2.	Our FedChain instantiations outperform stepsize decaying baselines
J.1	VERIFYING THE EFFECT OF INCREASED K AND R = 1
Our proofs of FedChain all also Work if We run Alocal for only one communication round, so long as
K is sufficiently large (exact number is in the formal theorem proofs). In this section We verify the
effect of large K in conjunction With running Alocal for only one round. The setup is the same as in
App. I.1, except We tune the stepsize η in a larger grid:
η ∈ {10-3, 10-2.5, 10-2, 10-1.5, 10-1, 10-0.5, 100}	(368)
The plots are in Fig. 3. For algorithms that are "1-X→Y”, We run algorithm X for one round and
algorithm Y for the rest of the rounds. These algorithms are tuned in the folloWing Way. Algorithm X
48
Published as a conference paper at ICLR 2022
Figure 3: We investigate the effect of high K (K = 100). "1-X→Y” denotes a chained algorithm
with X run for one round and Y run for the rest of the rounds. Chained algorithms, even with one
round allocated to Alocal, perform the best. Furthermore, accelerated algorithms outperform their
non-accelerated counterparts.
0% shuffled
50% shuffled
100% shuffled
Figure 4: The same as Fig. 2, except we include stepsize decay for baseline algorithms. Chained
algorithms still perform the best.
has its stepsize tuned in Eq. (368), and Algorithm Y independently has its stepsize tuned in Eq. (368).
We require these to be tuned separately because local update algorithms and centralized algorithms
have stepsizes that depend on K differently if K is large (see, for example, Thm. D.1 and Thm. E.1).
The baselines have a single stepsize tuned in Eq. (368). These are tuned for the lowest final gradient
norm averaged over 1000 runs.
Overall, we see that in the large K regime, algorithms that start with a local update algorithm and
then finish with an accelerated centralized algorithm have the best communication complexity. This
is because larger K means the error due to variance decreases, for example as seen in Thm. F.2.
Acceleration increases instability (if one does not perform a carefully calibrated stepsize decay,
as we do not in the experiments), so decreasing variance helps accelerated algorithms the most.
Furthermore, a single round for AlOCal suffices to See large improvement as seen in Fig. 3. This is
expected, because both the “bias” term (the ∆ exp(- RKK) term in Thm. E.1) and the variance term
become negligible, leaving just the heterogeneity term as desired.
J.2 Including Stepsize Decay Baselines
In this section, we compare the performance of the algorithms against learning rate decayed SGD,
FedAvg, ASG, and SCAFFOLD. We use the same setting as App. I.1 for the decay process and
the stepsize. In this case, we still see that the chained methods outperform their non-chained
counterparts.
49