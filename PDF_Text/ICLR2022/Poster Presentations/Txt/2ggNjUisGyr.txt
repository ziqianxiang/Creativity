Published as a conference paper at ICLR 2022
Partial Wasserstein Adversarial Network
for Non-rigid Point Set Registration
Zi-Ming Wang, Nan Xue, Ling Lei, GUi-Song Xia*
CAPTAIN
Wuhan University
Abstract
Given two point sets, the problem of registration is to recover a transformation
that matches one set to the other. This task is challenging due to the presence
of large number of outliers, the unknown non-rigid deformations and the large
sizes of point sets. To obtain strong robustness against outliers, we formulate the
registration problem as a partial distribution matching (PDM) problem, where the
goal is to partially match the distributions represented by point sets in a metric
space. To handle large point sets, we propose a scalable PDM algorithm by
utilizing the efficient partial Wasserstein-1 (PW) discrepancy. Specifically, we
derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show
its gradient can be explicitly computed. Based on these results, we propose a
partial Wasserstein adversarial network (PWAN), which is able to approximate
the PW discrepancy by a neural network, and minimize it by gradient descent.
In addition, it also incorporates an efficient coherence regularizer for non-rigid
transformations to avoid unrealistic deformations. We evaluate PWAN on practical
point set registration tasks, and show that the proposed PWAN is robust, scalable
and performs more favorably than the state-of-the-art methods.
1 Introduction
Point set registration is a fundamental task in many computer vision applications, such as object
tracking (Gao & Tedrake, 2018), shape retrieval (Berger et al., 2017) and contour matching (Avots
et al., 2019). As illustrated in Fig. 1, given two point sets representing two partially overlapped shapes,
i.e., a reference set and a source set, the goal of this task is to recover an appropriate transformation
that matches the source set to the reference one. It is challenging due to the following factors: 1)
The point sets may contain a fraction of outliers which do not have valid correspondences in the
other point set, such as the noise points and the points in the non-overlapped region of the shapes. 2)
The number of points consisted in the point sets may be huge. 3) The deformation of point sets is
generally unknown and can be non-rigid.
(b) Result of DM
(c) Result of PDM
(d) Ground truth
(a) Input sets
Figure 1: An example of non-rigid point set registration using the distribution matching (DM)
formulation (Hirose, 2021a) and the proposed partial distribution matching (PDM) formulation.
The registration problem is generally solved in the distribution matching (DM) framework, where
the point sets are regarded as probability distributions, and are aligned by minimizing a discrepancy
between them. To reduce the influence of outliers, practical registration methods utilize the robust
* Corresponding authors
1
Published as a conference paper at ICLR 2022
Figure 2: An overview of PWAN. The transformation Tθ and the network f are trained adversarially.
discrepancies, such as Kullback-Leibler divergence (Myronenko & Song, 2010; Hirose, 2021a) and
L2 distance (Jian & Vemuri, 2011). Thus they are able to greedily align the largest possible fraction of
points while being tolerant of a small number of outliers. However, for point sets that are dominated
by outliers, the greedy behaviors of these methods easily bias toward outliers, and lead to degraded
registration results. An example is shown in Fig. 1(b).
To obtain stronger robustness against outliers, we propose to formulate the registration problem in
a novel partial distribution matching (PDM) framework, where we only seek to partially match the
distributions. Comparing to the DM based methods, the PDM formulation is more robust against
outliers. For example, in Fig. 1(c), the PDM formulation find a more natural solution where only a
fraction of points are well matched.
However, existing solutions for PDM problems generally require to compute the correspondence
between distributions (Chizat et al., 2018; Chapel et al., 2020), thus they are intractable for registration
problems which involve large scale distributions. To handle this issue, we propose an efficient solver
for large scale PDM problem. Our method utilizes the partial Wasserstein-1 (PW) discrepancy (Figalli,
2010), which can be efficiently optimized without computing the correspondence. Specifically, we
derive the Kantorovich-Rubinstein (KR) duality (Villani, 2003) for the PW discrepancy, and show that
its gradient can be explicitly computed via its potential. Based on these results, we propose a partial
Wasserstein adversarial network (PWAN), which approximates the potential by a neural network,
and the unknown transformation can be trained in an adversarial way with the network. We also
incorporate a coherence regularizer for the non-rigid transformation to enforce its smoothness. We
note that PWAN generalizes the popular Wasserstein generative adversarial net (WGAN) (Arjovsky
et al., 2017) to the PDM problem. An illustration of the proposed PWAN is presented in Fig. 2.
The contribution of this work is summarized as follows.
-	Theoretically, we derive the KR duality for the partial mass PW discrepancy, present its differentia-
bility property, and show its gradient can be efficiently computed. We further provide a qualitative
description of the KR potentials. More details can be find in Appx. A.3.
-	Based on the KR formulation of mass-type (partial mass) PW discrepancy and the closely related
distance-type PW discrepancy, we propose a scalable method, called PWAN, for large scale PDM
problem. The well-known WGAN is a special case of our method in the DM setting.
-	We apply PWAN to point set registration task, where point sets are regarded as discrete distributions.
We experimentally show that PWAN exhibits stronger robustness against outliers than the existing
methods, and can register point sets accurately even when they are dominated by outliers, such as
when they contain a large fraction of noise points, or when they are only partially overlapped.
2 Related Work
There is a large body of literatures on point set registration problem (Besl & McKay, 1992; Zhang,
1994; Chui & Rangarajan, 2000; Myronenko & Song, 2010; Maiseli et al., 2017; Vongkulbhisal et al.,
2018; Hirose, 2021a). The existing methods can be roughly categorized into two classes, i.e., the
2
Published as a conference paper at ICLR 2022
correspondence-based methods and the probabilistic methods. This section only discusses the latter
class which is the most related to our method. More discussions can be found in Appx. F.1.
The probabilistic methods solve the registration problem as a DM problem. Specifically, most of these
methods smooth the point sets as Gaussian mixture models (GMMs), and then align the distributions
via minimizing the robust discrepancies between them. Coherent point drift (CPD) (Myronenko &
Song, 2010) and its variants (Hirose, 2021a) are well-known methods is this class, which minimize
Kullback-Leibler (KL) divergence between the distributions. Other robust discrepancies, including
L2 distance (Ma et al., 2013; Jian & Vemuri, 2011), kernel density estimator (Tsin & Kanade, 2004)
and scaled Geman-McClure estimator (Zhou et al., 2016) have also been studied.
The proposed PWAN is related to the existing probabilistic methods because it also regards point sets
as distributions. However, it has two major differences from them. First, PWAN directly processes
the point sets as un-normalized discrete distributions instead of smoothing them to GMMs, thus it is
more concise and natural. Second, PWAN solves a PDM problem instead of the DM problem, as it
only requires to match a fraction of distributions, thus it is more robust to outliers.
Some recent works have been devoted to the PDM problem using the Wasserstein-type discrepancy.
Bonneel & Coeurjolly (2019) proposed a fast algorithm based on the sliced Wasserstein metric for a
special PDM problem, where a small distribution is fully matched to the large one. Various entropy
regularized partial or unbalanced discrepancies (Chizat et al., 2018; SejoUrne et al., 2019; ChaPel
et al., 2020) have been utilized for the PDM problem. However, they are generally not scalable to
large distributions as they require to compute the correspondence between distributions, or rely on the
mini-batch sampling techniques (Fatras et al., 2021). Thus they are not suitable for the registration
problem considered in this paper. The distance-type PW discrepancy used in our paper has been
utilized for imaging problem (Lellmann et al., 2014; Schmitzer & Wirth, 2019), but these methods do
not directly apply to distributions in continuous space. More discussions of the related computational
approaches of Wasserstein type discrepancies can be found in Appx. B.
3 Preliminaries on Partial Wasserstein- 1 Discrepancies
This section introduces the major tools used in this work, i.e., two types ofPW discrepancies between
measures: the partial-mass Wasserstein-1 discrepancy (Figalli, 2010; Caffarelli & McCann, 2010)
and the bounded-distance Wasserstein-1 discrepancy (Lellmann et al., 2014; Bogachev, 2007). For
simplicity, we call them the mass-type and the distance-type PW discrepancy respectively.
Given two discrete distributions α = xi ∈X aiδxi and β = y ∈Y bj δyj on a compact metric
space Ω ⊆ Rn, where δ is the Dirac function, and a = [a/?=] ∈ R+ and b = [bj]r=ι ∈ R] are the
associated mass vectors, the mass-type PW discrepancy (Figalli, 2010) LM,m is defined as an optimal
transport problem which seeks the minimal cost of transporting at least m (m ≤ min(||a||1, ||b||1))
unit mass from α to β where the cost of the transportation equals the distance. Formally, LM,m is
defined as
LM,m(α, β)
min
π∈Γm(α,β)
πi,j d(xi, yj),
i,j
(1)
where d is the metric defined in Ω, ∏ ∈ R]×r is the transport plan, ∏i,j encodes the amount of mass
transported from Xi to y7-. The feasible set Γm(α, β) is given by Γm(α, β) = {π ∈ R]×r | πIr ≤
a, πTlq ≤ b, ITπIr ≥ m}.
The other PW discrepancy used in this work is the distance-type PW discrepancy (Lellmann et al.,
2014) LD,h, which is a Lagrangian of LM,m with the mass constraint softened:
LD,h(α, β) = min	πi,j (d(xi, yj) - h),
π∈Γ0 (α,β)
i,j
(2)
Note that LD,h bounds the maximal transport distance by h.
In the special complete transport problem where all mass is transported, i.e., when ||a||1 = ||b||1,
m = ∣∣a∣∣ι for ^M,m or h ≥ diam(Ω) for Ld,h, both ^M,m and LD,h become equivalent to the
well-known Wasserstein-1 distance W1 (also known as the earth move distance), which, according to
3
Published as a conference paper at ICLR 2022
the Kantorovich-Rubinstein duality (Kantorovich, 2006), can be equivalently expressed as
W1(α,β) = sup	aif(xi) -	bif(yj)
f CLiP(Q) Xi∈X	yj∈Y
(3)
where Lip(Ω) represents the Lipschitz-1 function defined on Ω. We called equation (3) the KRform
of W1, and f the potential. Equation (3) is exploit in WGAN (Arjovsky et al., 2017) to efficiently
compute W1 , where f is approximated by a neural network. More detailed preliminaries can be
found in Appx. A.
4 The Proposed Approach
In this section, we present the details of the proposed PWAN. We first formulate the registration
problem in Sec. 4.1. Then we present an efficient method to solve the problem in Sec. 4.2 and 4.3.
We finally summarize our algorithm in Sec. 4.4.
4.1	Problem Formulation
Let Y = {yj}r=1 ⊆ Ω and X = {xi}q=1 ⊆ Ω be the source and reference point sets, where Ω ⊆ R3
is a closed bounding box of the points. The corresponding reference and source distributions are
α = xi
∈X aiδxi and β = yj∈Y bjδyj respectively, where ai , bj ∈ R+ are the mass assigned to
each point and are fixed to be 1 in this work. Let Tθ : Ω → Ω denote a differential transformation
parametrized by θ and βθ = Py ∈Y bjδTθ(yj) denote the transformed β. Our goal is to align βθ to α
by solving
minL(α,βθ)+C(Tθ),	(4)
θ
where discrepancy L is LM,m (1) or LD,h (2), which measures the dissimilarity between βθ and α,
and C is the coherence energy that enforces the spatial smoothness of Tθ .
4.2	Registration with the PW Discrepancies
In practical registration problems, βθ and α may contain a large number of outliers or non-overlapping
points that should not be matched to the other set. Therefore, in order to avoid biased results, it is
important for a registration method to allow the matching of only a fraction of points.
The use of LM,m and LD,h in problem (4) naturally leads to desirable partial matchings. To see this,
we express them in their respective primal forms, and formulate problem (4) as
πi,j d(xi , Tθ (yj )) + C(Tθ ) + const,	(5)
i,j
where const is a constant not relevant to θ, and
π ∈ Rq×r represents the correspondence ma-
trix given by the solution of the primal form
ofLM,m(α,βθ) (1) or LD,h(α, βθ) (2). A toy
example of the correspondence π is shown in
Fig. 3. As can be seen, π only establishes the cor-
respondence between a fraction of points that are
(a) LM,6	(b) LD,0.9
Figure 3: The computed correspondence π be-
tween α (blue) and βθ (red).
close to the other set within the mass threshold
m or the distance threshold h, while omitting
all other points. Therefore, minimizing (5) is
simply to align these corresponding point pairs
subjecting to the coherent constraint C. In other words, LM,m and LD,h provide two ways to control
the ratio of matching based on mass or distance criteria.
Note that according to the Lagrange duality, for each (α, βθ, m), there exists a h*, such that the solu-
tion π of LD,h* (α, βθ) recovers to that of LM,m(α, βθ). However, the minimization of LM,m(α, βθ)
in problem (5) is generally not equivalent to that of LD,h (α, βθ) with any fixed h, because the
corresponding h* depends on βθ , which varies during the optimization process.
4
Published as a conference paper at ICLR 2022
Although formulation (5) can indeed handle the partial alignment problem, it is computationally
intractable for large scale point sets, because it requires to solve for a matrix π of shape q × r in a
linear programing in each iteration. Therefore, a natural question is whether it is possible to avoid the
computation of π by exploiting the KR duality of LM,m and LD,h and re-formulate them in a similar
way as W1 (3).
The answer is affirmative for both LM,m and LD,h. As for LD,h, its KR form is already known
in Bogachev (2007); Lellmann et al. (2014); Schmitzer & Wirth (2019), and we further show that it is
valid to compute its gradient under a mild assumption.
Theorem 1.	LD,h (α, β) can be equivalently expressed as
LD,h (α, β) =	sup
f ∈Lip(Ω)
-h≤f ≤0
xi∈X
aif(xi) -	bjf(yj) - hmβ.
yj∈Y
(6)
The optimizer of problem (6) exists. Let f denote an optimizer of problem (6). When Tθ is Lipschitz
w.r.t. θ, LD,h (α, βθ) is continuous w.r.t. θ, and is differentiable almost everywhere. Furthermore, we
have
VθLp,h(α,βθ) = -E biVθf(T(yi)).	⑺
yi∈Y
As for LM,m, we derive its KR form based on that of LD,h, and show that its gradient can also be
computed under a mild assumption.
Theorem 2.	LM,m (α, β) can be equivalently expressed as
LM,m(α, β) =	sup	aif(xi) -	bjf(yj) + h(m - mβ).	(8)
f ∈Lip(Ω),h∈R+ X ∈X	∈ ∈ ∈Y
-h≤f ≤0	xi∈X	yj∈Y
The optimizer of problem (8) exists. Let (f, h) denote an optimizer of problem (8). When Tθ
is Lipschitz w.r.t. θ, LM,m (α, βθ) is continuous w.r.t. θ, and is differentiable almost everywhere.
Furthermore, we have
VθLM,m(α, βθ) = - X biVθf(Tθ(yi)).	(9)
yi∈Y
The proofs of both theorems can be found in Appx. D.3.
With the aid of Theorem 1 and 2, we can optimize LD,h and LM,m efficiently. Specifically, we
represent the potentials of LD,h and LM,m using neural networks fw,h satisfying -h ≤ fw,h ≤ 0
where h ∈ R+. To compute LM,m(α, βθ), we update {w, h} to maximize
LossM,m =	ai fw,h (xi ) -	bj fw,h (Tθ (yj )) + h(m - mβ ) - GP (fw,h ),	(10)
xi ∈X	yj ∈Y
and to compute LD,h(α, βθ), we update {w} to maximize
LossD,h =	aifw,h(xi) -	bjfw,h(Tθ(yj)) - GP (fw,h),	(11)
xi∈X	yj ∈Y
where GP(f) = Kmax^∈χ∪Tθ(γ){∣∣V^f(x)||2,1} is the gradient penalty (Gulrajani et al.,
2017), and κ is the strength of the penalty. Then, with the trained network fw,h, the gradients
VθLM,m(α, βθ) and VθLD,h(α, βθ) can be computed via back-propagating their respectively losses
to θ, thus θ can be updated via gradient descent.
To show our neural approximations of the PW discrepancies are sufficiently precise, we present a
simple example numerically comparing the primal and KR form in Tab. 2 and Fig. 9 in the appendix.
4.3	Optimize the Coherence Energy
This section discusses the optimization of the coherence energy, which ensures that the whole βθ
remains spatially smooth in the matching process. First of all, we define the non-rigid transformation
Tθ parametrized by θ = (A, t, V ) as
Tθ(yj) = yjA + t+ Vj,	(12)
5
Published as a conference paper at ICLR 2022
where yj ∈ R1×3 represents the coordinate of point yj ∈ Y , A ∈ R3×3 is the linear transformation
matrix, t ∈ R1×3 is the translation vector, V ∈ Rr×3 is the offset vector of all points in Y , and Vj
represents the j-th row of V . Despite its simplicity, Tθ includes several useful transformations as
its special case. For example, when V = 0 and A ∈ SO(3), Tθ becomes the rigid transformation,
and when A = I and t = 0, Tθ becomes the “drift” transformation in Myronenko & Song (2010).
Note that Tθ is Lipschitz w.r.t. θ (Proposition 7 in the appendix), thus it satisfies the requirement of
Theorem 1 and 2, i.e., it is valid to be used in our registration method.
Now we define the coherence energy similar to Myronenko & Song (2010), i.e., we enforce that
V varies smoothly in space. Formally, let G ∈ Rr×r be a kernel matrix, e.g., the Gaussian kernel
Gρ(i, j) = e--lyi-yj|| /ρ, and σ bea positive number. The coherence energy is defined as
C(Tθ) = λTr(VT(σI+Gρ)-1V),	(13)
where λ ∈ R+ is the strength of constraint, I is the identity matrix, and Tr(∙) is the trace of a matrix.
Since the matrix inversion (σI + Gρ)-1 is computationally intractable for large scale point sets, we
approximate it via the Nystrom method (Williams & Seeger, 2000), and obtain the gradient
dC∂Vθ) ≈ (2λ)(σ-1V - σ-2Q(Λ-1 + σ-1QTQ)TQTV),	(14)
where Q ∈ Rr×k, Λ ∈ Rk×k is a diagonal matrix, and k r. Note the computational cost of (14) is
only O(r) if we regard k as a constant. The detailed derivation is presented in Appx. F.3.
4.4	The Algorithm and Analysis
With the methods detailed in Sec. 4.2 and Sec. 4.3, we can finally solve problem (4) efficiently.
Formally, we formulate problem (4) as the following mini-max problem
min max Loss(α, βθ; we) + C (Tθ),	(15)
θ	we
where Loss = LossM,m (10) and we = {w, h}, or Loss = LossD,h (11) and we = {w}, and we solve
it by alternatively updating fw,h and Tθ.
An illustration of our method is shown in Fig. 2. The detailed algorithm is presented in Alg. 1.
For clearness, we refer to PWAN based on LM,m and LD,h as mass-type PWAN (m-PWAN) or
distance-type PWAN (d-PWAN) respectively.
Algorithm 1 PWAN for point set registration
Input: reference set X, source set Y, transform T, potential network fw,h, network update fre-
quency u, training type (“mass” or “distance")，mass threshold m, distance threshold h,
training step T, coherence parameters (λ, ρ, σ), Nystrom parameter k.
Output: the learned θ.
if training type = “mass” then
W J (w, h);	m J m； L J LossMm defined in (10)
else if training type = "distance” then
W J (w);	h J h;	L J LossD,h defined in (11)
end if
for t = 1, . . . , T do
Obtain the transformed distribution βθ.
for = 1, . . . , u do
Compute ∂L∕∂W by back-propagating L through We.
Update W by ascending the gradient ∂L∕∂W.	. Potential learning
end for
Compute ∂C /∂θ using (λ, ρ, σ, k) according to (14).
Compute ∂L∕∂θ by back-propagating L through θ.
Update θ by descending the gradient ∂(L + C)∕∂θ.	. Registration
end for
To provide an intuitive explanation why solving problem (15) can lead to partial alignment, we first
visualize the learned potential on a toy example in Fig. 4. As can be seen, the potential of PWAN
6
Published as a conference paper at ICLR 2022
attains the upper or lower bound (0 or -h) in some regions, thus the gradient within these regions is
strictly 0, i.e., the potential is strictly “flat” in these regions. This observation is formally stated and
proved in Proposition 11 and 12 in the appendix.
Due to the existence of flat regions, PWAN can automatically
discard a fraction of points during the registration process. Specif-
ically, if we omit the coherent energy, in each iteration of Alg. 1,
PWAN moves βθ along the gradient of potential. Therefore,
PWAN only moves the fraction of points with non-zero gradi-
ent, while leaving the points with zero gradients (in flat regions)
fixed. For the case in Fig. 4, only the leftmost 3 points in βθ will
be moved leftward in current iteration, while others will stay fixed.
In other words, PWAN seeks to match a fraction of points instead
of the whole point sets.
More discussions can be found in Appx. F.4.
Figure 4: The learned potential
on toy 1-dimensional point sets.
5	Experiments and Analysis
In this section, we experimentally evaluate the proposed PWAN on point set registration tasks.
After describing the experiment settings in Sec. 5.1, we first present a toy example to highlight the
robustness of the PW discrepancies in Sec. 5.2. Then we compare PWAN against the state-of-the-art
methods in Sec. 5.3, and discuss its scalability in Sec. 5.4. We finally evaluate PWAN on two real
datasets in Sec. 5.5. More experimental results are given in Appx. F.
5.1	Experiment Settings
As shown in Fig. 5, we use three synthesized
datasets in our experiments: bunny, armadillo
and monkey. The bunny and armadillo datasets
are from the Stanford Repository (Standford),
and the monkey dataset is from the internet.
The number of points in these shape are 8, 171,
106, 289 and 7, 958 respectively. Following Hi-
rose (2021b), we artificially deform these sets,
and we evaluate the registration results via the
mean square error (MSE).
Figure 5: The synthesized datasets used in our
experiments. The source point sets (blue) are syn-
thesized by bending the reference point sets (red)
in a non-linear manner.
We use a 5-layer point-wise multi-layer perception as our network (Fig. 10 in the appendix), and the
parameters used in the experiments are given in Appx. F.5.
5.2	Comparison of Several Discrepancies
To provide an intuition of our PDM formulation for registration, we compare the PW discrepancy
with two representative robust discrepancies used in DM based registration methods, i.e., KL diver-
gence (Hirose, 2021a) and L2 distance (Jian & Vemuri, 2011), on a toy 1-dimensional example. For
now, we do not consider the coherence energy.
We construct the toy point sets X and Y shown in Fig. 6(a), where we first sample 10 equi-spaced
data points in interval [0, 3], then we define Y by translating the data points by a distance t, and define
X by adding N outliers in a narrow interval [7.8, 8.2] to the data points. For N = {1, 10, 103}, we
record four discrepancies: KL, L2, LM,10 and LD,2 between X and Y as a function of t, and present
the results from Fig. 6(b) to Fig. 6(e).
As can be seen, there exist two alignment modes in this experiment, i.e., t = 0 and t = 6.5, which
respectively correspond to the correct alignment and the degraded alignment where Y is matched
to outliers. When the number of outliers is small, e.g., N = 1, all discrepancies admit a deep local
minimum t = 0. However, as for KL and L2 divergence, the local minimum t = 0 gradually vanishes
and they gradually bias toward t = 6.5 as N increases. In contrast, LM,10 and LD,2 do not suffer
7
Published as a conference paper at ICLR 2022
from this issue, i.e., the local minimum t = 0 remains deep regardless of the value of N , and the
local minimum t = 6.5 is always shallower than the local minimum t = 0.
The results show a key advantage of PDM against DM for registration: When the number of remote
outliers is large, the DM formulations (KL and L2) always tend to converge to the biased result, while
the correct alignment of PDM formulation is not influenced by the remote outliers, and it is less likely
to converge to the biased result.
Figure 6: Comparison of different discrepancies on a toy point set.
5.3 Evaluation of the Registration Accuracy
We compare the performance of PWAN with four state-of-the-art methods: CPD (Myronenko &
Song, 2010), GMM-REG (Jian & Vemuri, 2011), BCPD (Hirose, 2021a) and TPS-RPM (Chui &
Rangarajan, 2000). We evaluate them on the following two artificial datasets.
Point sets with extra noise points. We first sample N = 500 random points from each of the original
and the deformed sets as the source and reference sets respectively. Then we add extra uniformly
distributed noise points to the reference set, and we normalize both sets to mean 0 and variance 1.
We vary the number of outliers from 100 to 600 at an interval of 100, i.e., the outlier/non-outlier ratio
varies from 0.2 to 1.2 at an interval of 0.2.
Partially overlapped point sets. We first sample N = 1000 random points from each of the original
and the deformed sets as the source and reference sets respectively. Then for each set, we intersect it
with a random plane, and we retain the points in one side of the plane and discard all points in the
other side. We vary the retain ratio s from 0.7 to 1.0 at an interval of 0.05 for both the source and the
reference sets, thus the minimal ratios of overlapping area are (2s - 1)/s =0.57, 0.67, 0.75, 0.82,
0.89, 0.94 and 1 accordingly, and the minimal corresponding mass is m = (2s - 1) * 1000.
We evaluate m-PWAN with m = 500 (equivalently d-PWAN with h = +∞) in the first experiment,
while We evaluate m-PWAN with m = m and d-PWAN with h = 0.05 in the second experiment. We
run all methods 100 times and report median and standard deviation of MSE in Fig. 7.
Fig. 7(a) presents the results of the first experiment. The median registration error of PWAN is
generally comparable with TPS-RPM, and is much lower than the other methods. In addition, the
standard deviations of the results of PWAN are much lower than that of TPS-RPM. This suggests that
PWAN are more robust against outliers than baseline methods. Fig. 7(b) presents the results of the
second experiment. Two types of PWANs perform comparably, and they outperform the baseline
methods by a large margin in terms of both median and standard deviations when the overlap ratio is
low, while perform comparably with them when the data is fully overlapped. This result suggests the
proposed PWAN can effectively register the partially overlapped sets.
5.4	Evaluation of the efficiency
To evaluate the efficiency of PWAN, we first need to investigate the influence of its parameters. In
particular, the most important parameter that affects the efficiency is the network update frequency
u, which controls the tradeoff between the efficiency and effectiveness. Specifically, larger u leads
to more accurate estimation of the potential and the gradient, while smaller u allows for faster
estimations. We quantify the influence of u using the bunny dataset and report the results in the left
8
Published as a conference paper at ICLR 2022
(a) Evaluation of robustness against noise points.
Outlier/Non-outlier ratio
Overlapping region ratio	Overlapping region ratio	Overlapping region ratio
(b) Evaluation of robustness against partial overlapping.
Figure 7: Registration accuracy of the bunny (left), monkey (middle) and armadillo (right) datasets.
The error bars represent the medians and standard deviations of MSE.
panel of Fig. 8. As can be seen, both MSE and the variance of MSE decrease as u increases, while
the computation time increases proportionally with u. This result suggests that more accurate and
stable results can be achieved at the expense of higher computational cost, i.e., larger u.
We benchmark the computation time of dif-
ferent methods on a computer with two
Nvidia GTX TITAN GPUs and an Intel i7
CPU. We fix u = 20 for PWAN. We sam-
ple q = r points from the bunny shape,
where q varies from 103 to 7 × 105. PWAN
is run on the GPU while the other methods
are run on the CPU. We also implement a
multi-GPU version of PWAN where the po-
tential network is updated in parallel. We
run each method 10 times and report the mean of the computation time in the right panel of Fig. 8.
As can be seen, BCPD is the fastest method when q is small, and PWAN is comparable with BCPD
when q is near 106 . In addition, the 2-GPU version PWAN is faster than the single GPU version, and
it is faster than BCPD when q is larger than 5 × 105.
5	10	15	20	25	30	lθɔ I(T 10j IO6
Network update frequency	Number of points
Figure 8: Scalability of our method.
5.5 Evaluation on Real Data
To demonstrate the capability of PWAN on handling point sets with non-artificial deformations, we
evaluate it on a human face dataset (Zhang et al., 2008) and a 3D human dataset (DataSet). The
details of this experiment are presented in Appx. F.9 and Appx. F.10.
6 Conclusion
In this paper, we formulate the point set registration task as a PDM problem, where point sets are
regarded as discrete distributions and are only required to be partially matched. In order to solve the
PDM problem efficiently, we derived the KR form and the gradient for the PW discrepancy. Based on
the theory, we proposed the PWAN method for PDM problem, and applied it to practical point sets
registration task. We experimentally show that PWAN can effectively handle the point sets dominated
by outliers, including those containing large fraction of noise or being partially overlapped.
There are several issues need further study. First, the computation time of PWAN is still relatively
high. A possible approach to accelerate PWAN is to incorporate a forward generator network as
in Sarode et al. (2019) or to use the discriminative training as in Vongkulbhisal et al. (2018). Second,
it is interesting to explore PWAN in other PDM problems, such as domain adaption (Hu et al., 2020),
pose estimation (Kuhnke & Ostermann, 2019) and multi-label learning (Yan & Guo, 2019).
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work was supported by the National Natural Science Foundation of China under Grant 61922065,
Grant 62101390, Grant 41820104006, the Fundamental Research Funds for the Central Universities
under Grant 2042021kf0038, and the National Post-Doctoral Program for Innovative Talents under
Grant BX20200248.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv: Machine Learning,
2017.
Egils Avots, Meysam Madadi, Sergio Escalera, Jordi Gonzalez, Xavier Baro, Paul Pallin, and
Gholamreza Anbarjafari. From 2d to 3d geodesic-based garment matching. Multimedia Tools and
Applications,78(18):25829-25853, 2019.
Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre. Itera-
tive bregman projections for regularized transportation problems. SIAM Journal on Scientific
Computing, 37(2):A1111-A1138, 2015.
Matthew Berger, Andrea Tagliasacchi, Lee M. Seversky, Pierre Alliez, Gael Guennebaud, Joshua A.
Levine, Andrei Sharf, and Claudio T. Silva. A survey of surface reconstruction from point clouds.
Computer Graphics Forum, 36(1):301-329, 2017.
P.J. Besl and H.D. McKay. A method for registration of 3-d shapes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 14(2):239-256, 1992.
Vladimir I Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.
Nicolas Bonneel and David Coeurjolly. Spot: sliced partial optimal transport. ACM Transactions on
Graphics, 38(4):89, 2019.
Nicolas Bonneel, Julien Rabin, Gabriel Peyre, and Hanspeter Pfister. Sliced and radon wasserstein
barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22-45, 2015.
Luis A. Caffarelli and Robert J. McCann. Free boundaries in optimal transport and monge-ampere
obstacle problems. Annals of Mathematics, 171(2):673-730, 2010.
Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal tranport with applications on
positive-unlabeled learning. Advances in Neural Information Processing Systems, 33:2903-2913,
2020.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Frangois-Xavier Vialard. Scaling algorithms
for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563-2609,
2018.
Haili Chui and A. Rangarajan. A new algorithm for non-rigid point matching. In Proceedings
IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662),
volume 2, pp. 2044-2051, 2000.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
DataSet. Matching humans with different connectivity. URL http://profs.scienze.univr.
it/~marin/shrec19/.
Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced
wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3483-3491, 2018.
Kilian Fatras, Younes Zine, Remi Flamary, Remi Gribonval, and Nicolas Courty. Learning with
minibatch wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International
Conference on Artificial Intelligence and Statistics, volume 108, pp. 1-20, 2020.
10
Published as a conference paper at ICLR 2022
Kilian Fatras, Thibault Sejourna R6mi Flamary, and Nicolas Courty. Unbalanced minibatch optimal
transport; applications to domain adaptation. In International Conference on Machine Learning,
pp. 3186-3197. PMLR, 2021.
Alessio Figalli. The optimal partial transport problem. Archive for Rational Mechanics and Analysis,
195(2):533-560, 2010.
Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas
Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Leo Gautheron,
Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet,
Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and
Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1-8,
2021. URL http://jmlr.org/papers/v22/20- 451.html.
Wei Gao and Russ Tedrake. Surfelwarp: Efficient non-volumetric single view dynamic reconstruction.
In Robotics: Science and Systems XIV, volume 14, 2018.
Song Ge, Guoliang Fan, and Meng Ding. Non-rigid point set registration with global-local topology
preservation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp. 245-251, 2014.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617.
PMLR, 2018.
Aude Genevay, Lenaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyre. Sample complexity
of sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1574-1583. PMLR, 2019.
Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative
adversarial networks: Algorithms, theory, and applications. CoRR, abs/2001.06937, 2020. URL
https://arxiv.org/abs/2001.06937.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. In NIPS’17 Proceedings of the 31st International Conference on
Neural Information Processing Systems, volume 30, pp. 5769-5779, 2017.
Eric Heitz, Kenneth Vanhoey, Thomas Chambon, and Laurent Belcour. A sliced wasserstein loss for
neural texture synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 9412-9420, June 2021.
Osamu Hirose. A bayesian formulation of coherent point drift. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 43(7):2269-2286, 2021a.
Osamu Hirose. Acceleration of non-rigid point set registration with downsampling and gaussian
process regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(8):
2858-2865, 2021b.
Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, Junchi Yan, Zhongliang Jing,
and Henry Leung. Discriminative partial domain adversarial network. In European Conference on
Computer Vision, pp. 632-648, 2020.
Bing Jian and B C Vemuri. Robust point set registration using gaussian mixture models. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 33(8):1633-1645, 2011.
L. V. Kantorovich. On the translocation of masses. Journal of Mathematical Sciences, 133(4):
1381-1382, 2006.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced wasserstein kernels for probability distribu-
tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
5258-5267, 2016.
11
Published as a conference paper at ICLR 2022
Felix Kuhnke and Joern Ostermann. Deep head pose estimation using synthetic images and par-
tial adversarial domain adaption for continuous label spaces. In 2019 IEEE/CVF International
Conference on Computer Vision (ICCV) ,pp.10164-10173, 2019.
Jan Lellmann, Dirk A. Lorenz, Carola-Bibiane Schonlieb, and TUomo Valkonen. Imaging with
kantorovich-rubinstein discrepancy. Siam Journal on Imaging Sciences, 7(4):2833-2859, 2014.
Jiayi Ma, Ji Zhao, Jinwen Tian, ZhUowen TU, and Alan L. YUille. RobUst estimation of nonrigid
transformation for point set registration. In 2013 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2147-2154, 2013.
Baraka Maiseli, Yanfeng GU, and HUijUn Gao. Recent developments and trends in point set registration
methods. Journal of Visual Communication and Image Representation, 46(46):95-106, 2017.
PaUl Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):
583-601, 2002.
Andriy Myronenko and XUbo Song. Point set registration: Coherent point drift. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 32(12):2262-2275, 2010.
Vinit Sarode, XUeqian Li, HUnter Goforth, YasUhiro Aoki, Rangaprasad ArUn Srivatsan, Simon LUcey,
and Howie Choset. Pcrnet: Point cloUd registration network Using pointnet encoding, 2019.
Bernhard Schmitzer. Stabilized sparse scaling algorithms for entropy regUlarized transport problems.
SIAM Journal on Scientific Computing, 41(3):A1443-A1481, 2019.
Bernhard Schmitzer and Benedikt Wirth. A framework for wasserstein-1-type metrics. Journal of
Convex Analysis, 26(2):353-396, 2019.
Standford. The stanford 3d scanning repository.	[Online]. Available:
http://graphics.stanford.edu/data/3Dscanrep/.
Thibault S6journ6, Jean Feydy, FrangOis-Xavier Vialard, Alain Trouv6, and Gabriel Peyr6. Sinkhorn
divergences for Unbalanced optimal transport. arXiv preprint arXiv:1910.12958, 2019.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Yanghai Tsin and Takeo Kanade. A correlation-based approach to robust point set registration. In
European Conference on Computer Vision, pp. 558-569, 2004.
CedriC Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.
CedriC Villani. Optimal transport: old and new, volume 338. Springer, 2009.
Jayakorn Vongkulbhisal, Benat Irastorza Ugalde, Fernando De la Torre, and Joao P. Costeira. In-
verse composition discriminative optimization for point cloud registration. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 2993-3001, 2018.
Christopher K. I. Williams and Matthias Seeger. Using the nystrom method to speed up kernel
machines. In Advances in Neural Information Processing Systems 13, volume 13, pp. 682-688,
2000.
Yan Yan and Yuhong Guo. Adversarial partial multi-label learning. arXiv preprint arXiv:1909.06717,
2019.
Li Zhang, Noah Snavely, Brian Curless, and Steven M Seitz. Spacetime faces: High-resolution
capture for modeling and animation. In Data-Driven 3D Facial Animation, pp. 248-276. Springer,
2008.
Zhengyou Zhang. Iterative point matching for registration of free-form curves and surfaces. Interna-
tional Journal of Computer Vision, 13(2):119-152, 1994.
Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global registration. In European Conference on
Computer Vision, pp. 766-782, 2016.
12
Published as a conference paper at ICLR 2022
Appendix
In this appendix, we provide theoretical justifications of our algorithm and more experimental results.
We first present a more general introduction of the optimal transport problem and fix the notations in
Sec. A. Then we introduce existing computational approaches of optimal transport problem in Sec. B.
We derive our formulation in Sec. C and the gradient in Sec. D. We further discuss the properties of
our formulations in Sec. E. Finally, we present more details for the main text in Sec F.
A Preliminaries
This section introduces the optimal transport (OT) problem and fix the notations for later sections.
A.1 Optimal Transport
OT is a classic problem dating back to Monge and Kantorovich. It requires to solve the following
transportation problem: let α be the distribution of warehouses storing some raw materials, and β be
the distribution of factories requiring these materials. Assuming the total mass of materials stored in
the warehouse is mα, and the total mass of materials required by the factories is mβ. How to transport
at least m (m ≤ min(mα, mβ)) mass materials from α to β so that the total cost is minimized?
Formally, let Ω be compact metric space and M+(Ω) be the set of non-negative measures defined
on Ω. Define the source and target measures α,β ∈ M+(Ω), and a continuous cost function
C : Ω X Ω → R+.OT seeks to solve the following optimization problem
LM,m(α, β)
inf
π∈Γm(α,β)
L
Ω×Ω
c(x, y)dπ(x, y),
(16)
where Γm(α, β) are the set of non-negative measures ∏ defined on Ω × Ω satisfying
π(A × Ω) ≤ α(A), π(Ω × A) ≤ β(A) and π(Ω × Ω) ≥ m
for all measurable set A ⊆ Ω. For ease of notations, we abbreviate m@ = α(Ω), mβ = β(Ω) and
m(π) = π(Ω × Ω).
In the complete OT problem, it is generally assumed that mα = mβ = m, i.e., the source and the
target distributions contain equal mass of materials, and all materials should be transported. However,
the more general partial optimal transport (POT) problem (Figalli, 2010; Caffarelli & McCann, 2010)
only requires m ≤ min(mα, mβ), i.e., the source and target distributions may contain different mass
of materials, and only a partial of mass is required to transported.
A.2 Partial Wasserstein- 1 Discrepancy
In this paper, we focus on a specific POT problem where the cost function is a distance in the metric
space Ω, i.e., c(x, y) = d(x, y), where d is the distance function defined in Ω. In this case, we called
LM,m the mass-type partial Wasserstein-1 (PW) discrepancy.
In the complete OT case, this type of OT problem defines the Wasserstein-1 metric, which is also
known as the earth move distance (EMD) between distributions. According to the Kantorovich-
Rubinstein duality (Kantorovich, 2006), the Wasserstein-1 metric can be equivalently expressed
as
L
Ω
W1 (α, β)
sup
f ∈Lip(Ω)
fdα -
fdβ.
(17)
Note that this formulation offers a more efficient implement of Wasserstein-1 metric than the primal
form (16), as it only requires to solve for a function with a local constraint in Ω instead of a transport
plan with global constraints in Ω × Ω.
Several methods have been proposed to generalize (17) to the unbalanced OT (Chizat et al., 2018;
Schmitzer, 2019), i.e., OT problems with extra regularizers. See Schmitzer & Wirth (2019) for an uni-
fied framework of this type of generalizations. Amongst these generalizations, KR metric (Lellmann
et al., 2014) is closely related to the POT problem (16) considered in this paper, as it can be regarded
13
Published as a conference paper at ICLR 2022
as a Lagrangian of problem (16) where the mass constraint is soften. Specifically, the primal form
KR metric with parameter h > 0 is defined as
KRh(α,β)
h
inf )J	d(x, y)dπ(x, y) — hm(π) +《(ma + mβ).
(18)
It is important to notice that the KR metric has a natural explanation that it requires to find a optimal
plan whose transport distance does not exceed h. This can be seen by re-writing problem (18) as
KRh(α, β) = inf (d(x, y) - h)dπ(x, y) + const, and noticing that if d(x, y) > h, then the solution
π* to problem (18) should satisfy π*(x, y) = 0.
Importantly, the KR metric is known to have an equivalent form (Lellmann et al., 2014; Schmitzer &
Wirth, 2019)
/
Ω
KRh(α, β)
sup
f ∈Lip(Ω)
If l≤ 2
fdα -
fdβ,
(19)
which is very similar to (17). We called formulations (17) and (19) KR forms, and the solution to KR
forms potentials.
A.3 Theoretical Contributions
The main theoretical contributions of this work are summarized as follows.
-	We present the KR form of LM,m in Proposition 3 in Sec. C.
-	We prove the differentiability of the KR form of LM,m and derive its gradient in Sec. D.
-	We characterize of the potential of the KR form of LM,m in Sec. E. The main result is Proposi-
tion 11.
A.4 Notations
-	(Ω,d): a metric d associated to a compact metric space Ω. For example, Ω can be a closed cubic
in R3 and d can be the Euclidean distance.
-	C(Ω): the set of continuous bounded function defined on Ω equipped with the supreme norm.
-	Lip(Ω) ⊆ C(Ω): the set of I-LiPSChitz function defined on Ω.
-	M(X): the space of Radon measures on space X.
-	n#: The marginal of ∏ on its first variable. Similarly, n# represents the marginal of ∏ on its
second variable.
-	Given a function F: X → R ∪ +∞, the Fenchel conjugate of F is denoted as F* and is given by:
F*(x*) = sup < x,x* > —F(x), ∀x* ∈ X*	(20)
x∈X
where X* is the dual space of X and < ∙ > is the dual pairing.
B	Existing Computational Approaches of OT Problem
The computation of OT problem is an active field in machine learning. In this section, we briefly
discuss three major classes of approaches and relate our method to the existing ones.
One class of the most well-developed OT solver is based on the entropic regularizer. This class of
approaches relax the primal problem by adding an entropy regularizer to the transport plan, then
solve the relaxed problem via the Sinkhorn algorithm (Cuturi, 2013). However, the direct application
of the Sinkhorn algorithm has two drawbacks. First, the entropic bias introduced by the regularizer
always leads to undesired behaviors. Second, the computational cost is high for large scale problems,
since the Sinkhorn algorithm iteratively updates the whole transport matrix. Some works have been
devoted to address these two issues. To get rid of the bias, Genevay et al. (2018; 2019) proposed the
Sinkhorn divergence as an unbiased version of the entropic OT. To improve the efficiency, Schmitzer
(2019) proposed some acceleration techniques such as muti-scale computing, and Fatras et al. (2020)
14
Published as a conference paper at ICLR 2022
proposed to consider the mini-batch OT problem to avoid the computation of the complete transport
plan. The generalization of this type of approaches to unbalanced or partial OT problem was studied
in Benamou et al. (2015); Chizat et al. (2018); SejoUme et al. (2019); Fatras et al. (2021).
The second class of approaches solve the sliced OT problem (Bonneel et al., 2015; Kolouri et al.,
2016). They avoid computing OT problems in high dimensional space by projecting the distributions
onto random 1-dimensional lines, and solving a 1-dimensional OT problem each time. Due to their
simplicity and efficiency, this class of approaches have been applied to several fields in computer
vision, such as generative modelling (Deshpande et al., 2018) and texture synthesis (Heitz et al., 2021).
Recently, Bonneel & Coeurjolly (2019) proposed an algorithm for a special case of partial sliced
OT problem, where a small distribution is completely matched to a fraction of a large distribution.
However, this method does not handle the general partial sliced OT problem.
Our method belongs to the third class of approaches which focus on a specific type ofOT problem: the
Wasserstein-1 type problem. The foundation of this type of approach is the Kantorovich-Rubinstein
duality (17), which allows to efficiently compute Wasserstein-1 distance by learning a Lipschitz
function. This property was directly exploited in the popular Wasserstein GAN model (Arjovsky
et al., 2017). Some works (Lellmann et al., 2014; Schmitzer & Wirth, 2019) generalized this duality
to the unbalanced Wasserstein-1 type problem and applied them to imaging problems, but the exact
partial Wasserstein-1 problem ((16) with c = d) has not been considered. Our method completes
these approaches in a sense that we solve the exact partial Wasserstein-1 problem. Besides, unlike the
existing works (Lellmann et al., 2014; Schmitzer & Wirth, 2019) which only handle the distributions
in discrete image space, our method handles distributions in the continuous space, thus it is more
suitable for applications in machine learning such as point sets registration.
C Our Formulations
In this section, we derive the KR formulation of LM,m. The main result is Theorem 3.
First of all, we derive the Fenchel-Rockafellar dual of LM,m (α, β).
Proposition 1 (Dual form of LM,m). Problem (16) can be equivalently expressed as
LM,m(α, β)
sup f dα +	gdβ + mh.
(f,g,h)∈R Jω	Ω
(21)
where the feasible set R is
R = n(f, g, h) ∈ C(Ω) X C(Ω) X R+∣f ≤ 0, g ≤ 0, c(x, y) - h - f (X)-g(y) ≥ 0, ∀x,y ∈ ω}
(22)
In addition, the infimum in Problem (16) is attained.
Proof. We prove this proposition via Fenchel-Rockafellar duality. We first define space E: C(Ω) X
C(Ω) x R, space F: C(Ω X Ω), and a linear operator A: E → F as
A(f, g, h):	(x, y, h) →	f (x) + g(y) +	h;	∀f, g	∈	C(Ω),	∀h	∈ R,	∀x, y ∈ Ω.
Then we introduce a convex function H: F → R ∪ +∞ as
H(u) = 0 ifu ≥ -c
+∞ else
and L: E → R ∪ +∞ as
R fdα + R gdβ + hm if f ≥ 0, g ≥ 0, h ≤ 0
L(f,g,h) =
+∞	else
(23)
(24)
(25)
We can check when f ≡ g ≡ 1 and h = -1, H is continuous at A(f, g, h). Thus by Fenchel-
Rockafellar duality, we have
inf	H(A(f, g,h)) + L(f, g,h) = sup	-H*(-∏) - L*(A*∏)	(26)
(f ,g,h)∈E	π∈M(Ω×Ω)
15
Published as a conference paper at ICLR 2022
(—u)dπ∣ u(x,y) ≥ —c(x,y), ∀(x,y) ∈ Ω × ω}
udπ∣ u(x,y) ≤ c(x,y), ∀(x,y) ∈ Ω × ω}
We first compute the Fenchel dual H* (-∏) and L* (A*∏) in the right-hand side of (26). For arbitrary
∏ ∈ M(Ω X Ω), We have
H*(-π)
= supn (-u)dπ - H(u)o
= sup
u∈F
= sup
u∈F
It is easy to see that if π is a non-negative measure, then this supremum is cdπ, otherWise it is +∞.
Thus
H*(-π)
c(x, y)dπ(x, y)
+∞
if π ∈M+(Ω × Ω)
else
(27)
Similarly, We have
L*(A*π)
sup
(f,g,h)∈E
sup
(f,g,h)∈E
sup
(f,g,h)∈E
< (f,g,h),A*π > -L(f, g, h)
n< A(f, g, h), π > -( fdα + gdβ + hm)|f, g ≥ 0, h ≤ 0o
n/ fd(n# - α)+ /gd(n# - β) + h(∏(Ω × Ω) - m)|f, g ≥ 0, h ≤ 0}
If (α - n#) and (β - n#) are non-negative measures, and π(Ω × Ω) - m ≥ 0, this supremum is 0,
otherWise it is +∞. Thus
L* (A*π)
0+∞
if (α — n#) ∈ M+(Ω), (β — n#) ∈ M+ (Ω), π(Ω × Ω) ≥ m
else
(28)
In addition, the left-hand side of (26) reads
(f,gin,hf)∈EH(A(f,g,h))+L(f,g,h)
inf
(f,g,h)∈E
sup
(f,g,h)∈E
fda + /gdβ + hm| f, g ≥ 0, h ≤ 0, f (x) + g(y) + h ≥ -c(x, y), ∀x,y ∈ ω}
/ fdα + /gdβ + hm| f, g ≤ 0, h ≥ 0, f (x) + g(y) + h ≤ c(x, y), ∀x,y ∈ ω}
Finally, by inserting these terms into (26), We have
—
/
/
sup
(f,g,h)∈E
f,g≤0,h≥0
f (χ)+g(y)+h≤c(χ,y)∀χ,y∈Ω
fdα+
gdβ+hm
inf
π∈M+(Ω×Ω)
(α-n# )∈M+ (Q),(β-n# )∈M+ (Q)
π(Ω× Ω)≥m
c(x, y)dπ(x, y),
Which proves (21).
In addition, we can also check right-hand side of (26) is finite, since we can always construct
independent coupling e = ∙√mɑ 0 β√m)β, such that π ∈ M+(Ω × Ω), e# = Om)α ≤ α,
e# = βmΩ) β ≤ β and e(Ω × Ω) = m. Thus the Fenchel-Rockafellar duality suggests the infimum
is attained.	□
Then we define the following Lagrangian POT problem:
LD,h(α, β) =
inf	c(x, y)de(x, y)
π∈ro(α,β) √ω×ω
- hm(e).
(29)
16
Published as a conference paper at ICLR 2022
where h > 0 is the Lagrange multiplier. When c(x, y) = d(x, y), we immediately obtain the KR
form of this problem by reformulating the KR metric (18):
/
Ω
LD,h(α, β)
sup
f ∈Lip(Ω)
-h≤f ≤0
f dα -
fdβ-
Jω
hmβ.
(30)
Similar to Proposition 1, we derive the Fenchel-Rockafellar dual form of LD,h.
Proposition 2 (Dual form of LD,h). Problem (29) can be equivalently expressed as
L
Ω
L
Ω
LD,h(α, β)
sup
(f,g)∈R(h)
fdα +
gdβ.
(31)
where the feasible set is
R(h) = n(f,g) ∈ C(Ω) XC(Ω)∣f ≤ 0, g ≤ 0, c(x,y)-h-f(X)-g(y) ≥ 0,∀x,y ∈ ω} (32)
In addition, the infimum in Problem (29) is attained.
Proof. This proposition can be proved in similarly to Proposition 1, so We omit the proof here. □
Now, by comparing Proposition 2 with Proposition 1, we can see that LD,h and LM,m are related by
LM,m
sup	fdα +	gdβ + mh
(f,g,h)∈R Jω	Ω
sup LD,h + mh.
h∈R+
(33)
Therefore, We obtain the KR form of LM,m by inserting (30) into (33).
Theorem 3 (KR form of LM,m). When c(x, y) = d(x, y), problem (16) can be reformulated as
LM,m(α,β)
sup
f ∈Lip(Ω),h∈R+
-h≤f ≤0
fdα -
Jω
fdβ + h(m - mβ)
Jω
(34)
or equivalently as
/
Ω
/
Ω
LM,m(α, β) =
sup
f ∈Lip(Ω),f ≤0
fdα -
fdβ - inf (f)(m -mβ).
(35)
Proof. We obtain equation (34) by inserting (30) into (33) and merging tWo supremums. As for
equation (35), note that given a fixed f ∈ C(Ω) in problem (34), the optimal h is simply 一 inf(f) <
+∞. So we can replace h by 一 inf(f) in problem (34) to obtain problem (35).	□
For clearness, We define some functionals associated to LM,m and LD,h.
Definition 1 (LM,m and LM,m). Definefunctional Lm,w, associated to problem (34) as
LαM,β,m(f,h)
Ω ∕ω f d(α ― β) + h(m ― mβ)
-∞
h ∈ R+ , f ∈ Lip(Ω) , ―h ≤ f ≤ 0
else,
(36)
and funCtional Lm,w, associated to problem (35) as
LMmf)
Ω Jω fd(α — β) ― inf(f )(m — mβ)
l -∞
f ∈ Lip(Ω) , f ≤ 0
else,
(37)
Definition 2 (LD,h). Define a functional LD,h associated to (30) as
Lα,β (f) = {∕ω fdα - ∕ω fdβ -hmβ
f ∈ Lip(Ω) , -h ≤ f ≤ 0,
else
17
Published as a conference paper at ICLR 2022
With this definition, (30) becomes
LD,h(α, β) = sup LαD,βh(f)
f ∈C(Ω)	,
Problem (34) and (35) become
LM,m (α, β) =	sup	LαM,βm (f , h) and LM,m(α, β) = sup LαM,βm(f)
f ∈C(Ω),h∈R	,	f ∈C(Ω)	,
respectively.
We remark that the maximizer of LαD,,βh exists.
Proposition 3 (Existence of optimizer Schmitzer & Wirth (2019)). For α,β ∈ M+(Ω) and h > 0,
there exists f ∈ C(Ω) SUCh that L∏,h(α, β) = LDe(f).
Finally, We summarize the formulations discussed in this section in Table 1. Note that We have two
equivalent KR forms of LM,m. Although Lm,% has a simpler form without the extra variable h,
it contains the infimum Which is hard to implemented in practice. Thus We mostly use LM,m in
practical implementation.
Table 1: Equivalent formulations of LM,m and LD,h
	LM,m	L∏,h
Primal	inf	c(x, y)dπ(x, y) s.t. π(A X Ω) ≤ α(A), ∀A, π(Ω × A) ≤ β(A), ∀A, π(Ω × Ω) ≥ m,	inf	c(x, y)dπ(x, y) - hm(π) s.t. π(A × Ω) ≤ α(A), ∀A, π(Ω × A) ≤ β(A), ∀A,
Dual	sup	fdα +	gdβ + mh (f ,g,h) Ωι	Jω s.t. (f,g,h) ∈ C(Ω) × C(Ω) × R, f ≤ 0, g ≤ 0, h ≥ 0 c(x, y) - h - f(x) - g(y) ≥ 0, ∀x, y	sup	fdα +	gdβ (f,g)J。	Jω s.t. (f, g) ∈ C(Ω)× C(Ω), f ≤ 0, g ≤ 0, c(x, y) - h - f(x) - g(y) ≥ 0,∀x,y
KR (c = d)	sup	fd(α - β) + h(m - mβ) f,h Jω s.t. f ∈ Lip(Ω),h ≥ 0 -h≤f≤0	sup	fd(α - β) - hmβ f Jω s.t. f ∈ Lip(Ω) —h ≤ f ≤ 0
	sup	fd(α - β) - inf (f)(m - mβ) f Jω s.t. f ∈ Lip(Ω) f≤0	
D	Differentiability
This section proves the differentiability of both LD,h and LM,m in the KR form in Proposition 6 and
Proposition 5. To this end, we first need to show the potential of LM,m exists in Sec.D.1,
18
Published as a conference paper at ICLR 2022
D.1 Existence of Optimizers
In this section, we prove that the maximizer of LαM,β,m exists.
Proposition 4 (Existence of optimizers). For a,β ∈ M+(Ω) and m > 0, there exists f ∈ C(Ω)
and h ∈ R such that LM,m (α, β) = LαM,β,m (f, h).
To prove this proposition, we need the following lemma.
Lemma 1 (Continuity). LαM,β,m is continuous on C(Ω) X R.
Proof. Let (fn, hn) → (f, h) in C(Ω) x R. Assume LMem(fn, %) > -∞ when n is sufficiently
large. We first check LαM,β,m (f, h) > -∞ as follows. For arbitrary > 0, there exists N > 0 such
that for n > N, hn < h + , thus fn > -hn > -h - . By taking n → ∞, we see for arbitrary
> 0, f > -h - , which suggests f ≥ -h. In addition, it is easy to see f ≤ 0 and h ≥ 0. It is
also easy to see Lip(f) ≤ 1 due to the closeness of Lip(Ω). Thus according to the definition 36,
we claim LαM,β,m (f, h) > -∞. Furthermore, since -h - < fn < 0, and fn → f, by dominated
convergence theorem, we have
/
Ω
lim
n→∞
fndα
lim fndα =	fdα,
Jω n→∞	Ω
and lim	fndβ =	lim fndβ =	fdβ.
n→∞ Jω	Ω n→∞	Ω
(38)
(39)
Note we have hn(m - mβ ) → h(m - mβ ). We conclude the proof by combining these three terms
and obtaining LMem(fn, %) → LMBm(f, h).	口
Proof of Proposition 4. If we can find a maximizing sequence (fn, hn) that converges to
(f,h) ∈ C(Ω) x R, then Lemma 1 suggests that LM,m(α, β) = supf,h LMem(f, h)=
limn→∞ LαM,e,m(fn, hn) = LαM,e,m(f, h), which proves this proposition. Therefore, we only need to
show that it is always possible to construct such a maximizing sequence.
Let (fn,hn) be a maximizing sequence. We abbreviate max(fn) = maxχ∈o(fn(x)) and
min(fn) = minχ∈o(fn(χ)). We first assume fn does not have any bounded subsequence,
then there exists N > 0, such that for all n > N, min(fn) < -diam(Ω) (otherwise We
can simply collect a subsequence of fn bounded by diam(Ω)). We can therefore construct
fn = fn — (min(fn) + diam(Ω)) and hn = hn + (min(fn) + diam(Ω)). Note that max(fn) ≤
min(fn) + diam(Ω) = —diam(Ω) + diam(Ω) = 0, fn + hn = fn + hn ≥ 0, and fn ∈ Lip(Ω),
so LαM,e,m(ffn, hfn) > -∞, and
/
Ω
L
fnd(α — β) + hn(m — me )
/ fnd(α — β) + hn(m - mβ) + (min(fn) - diam(Ω))(m — ma)
Ω
which suggests that (fn, hn) is a better maximizing sequence than (fn, hn). Note fn is uniformly
bounded by diam(Ω) because 0 ≥ fn ≥ mm(fn) = —diam(Ω). As a result, we can always assume
hn is also bounded by diam(Ω). Because otherwise we can construct hn = — mm(fn) ≤ diam(Ω),
and it is easy to show (fn, hn) is a better maximizing sequence than (fn, hn). In summary, we can
always find a maximizing sequence (fn, hn), such that both fn and hn are bounded by diam(Ω).
Finally, since fn is uniformly bounded and equicontinuous, fn converges uniformly (up to a sub-
sequence) to a continuous function f. In addition, hn has a convergent subsequence since it is
bounded. Therefore, we can always find a maximizing sequence (fn, hn) that converges to some
(f, h) ∈ C(Ω) x R, which finishes the proof.	口
19
Published as a conference paper at ICLR 2022
Remark The proof of Proposition 4 is an analogue of Proposition 2.11 in Schmitzer & Wirth
(2019). The difference is that in our Proposition 4, besides f, we need to handle another variable
h acting as the lower bound of f. In contrast, Proposition 2.11 in Schmitzer & Wirth (2019) only
handles the fixed h.
D.2 Computation of Gradient
As we have proved the existence of the potential of LM,m, we can now consider the differentiability
of LM,m in the KR form.
We consider a transformation T : Rd X Ω → Ω parametrized by θ ∈ Rd. Let T(∙) = T(θ, ∙)
and denote βθ = Tθ(β) the corresponding push-forward measure of β. We show in Proposition 6
and 5 that, if Tθ is Lipschitz w.r.t. θ, then the objective functions LD,h(α, βθ) and LM,m (α, βθ) is
differentiable w.r.t. θ, and the gradient can be computed explicitly.
Proposition 5 (Differentiability of LM,m). If Tθ : Ω → Ω is Lipschitz w.r.t. to θ, then LM,m(α, βθ)
is continuous w.r.t. θ, and is differentiable almost everywhere. Furthermore, we have
PeLM,m(α,βθ)
Ve f(T (χ))dβ,
(40)
-
Ω
where (f, h) is a maximizer of LM,m.
Proof. To begin with, for arbitrary θ, θ0 , we consider the following two-step procedure that transports
m unit mass from α to Be；. First, we transport m mass from a to βe according to π, which is a
solution to LM,m
defined as
(α, βe ). Second, we transport m mass received in βe to βe0 according to a plan
π0
π0(A X B)= (e(A X B)7#(A))
if βe (A) > 0
else,
where πe(Te(x), Te0 (x)) = β(x). That is to transport all mass received at Te(x) to the corresponding
point Te0 (x). Thus we have
d(x, z)dπ0(x, z) ≤	d(x, z)dπe(x, z) =
d(Te(x), Te0(x))dβ(x).
Since Te is Lipschitz w.r.t. θ, there exists a constant ∆ > 0, such that d(Tθ(x), Te；(x)) ≤ ∆∣∣θ 一 θ0∣∣.
Therefore, the cost of the second step can be bounded as
d(x, z)
Jω×ω
dπ0(x, Z) ≤ mβ∆∣∣θ — θ0∣∣.
Denote cost1 the overall cost of this two-step procedure. We have
costι ≤ LM,m(α,βe) + mβ∆∣∣θ — θ0∣∣.
By applying the gluing lemma (Villani (2009) Sec.1) to π and π0, We can construct a transport plan e
that transports m unit mass from n# to e#. Denote Cost2 = Rω×ω d(x,y)d∏(x,y) the cost of e. On
one hand, we have
LM,m(α,βeo) ≤ cost2
according to the definition of LM,m . On the other hand, we have
cost2 ≤ cost1.
Because for arbitrary x,z ∈ Ω, the two-step procedure and e transport the same amount of mass
between them. However, the cost of transporting a unit mass from x to z is d(x, y) + d(y, z) for
a y ∈ Ω for the two-step procedure, but is d(χ, Z) for e, which is cheaper according to triangle
inequality. By combining these inequalities together, we have
LM,m(α,βeo) ≤ Cost2 ≤ costι ≤ ^M,m(α,βe) + mβ∆∣∣θ — θ0∣∣,
i.e.,
LM,m(α, Be；) ≤ LM,m(α, βe) + mβ∆∣∣θ — θ0∣∣.
20
Published as a conference paper at ICLR 2022
By switching θ and θ0 and repeating the argument, we have
LM,m(α, βθ) ≤ LM,m(α, Bθ0) + mβδ“θ -"儿
thus we have
∣LM,m(α,βθ) - LM,m(α,βθ0)| ≤ mβ∆∣∣θ - θ0∣∣,
which suggests LM,m (α, βθ) is continuous w.r.t. θ, and Radamacher’s theorem states that it is
differentiable almost everywhere.
The sketch of the rest of the proof is as follows. According to Proposition 4, the maximizer to the KR
form of LMm(α, βθ) exists, thus We can write VθLMm(α, βθ) as -▽6 Ω f (Tθ(Xy)dβ according
to the envelope theorem (Milgrom & Segal, 2002). Then we prove
Vθ
f(Tθ(x))dβ=
Vθf(Tθ(x))dβ,
when the right hand side of the equation is well defined following Arjovsky et al. (2017), which
completes our proof.	□
Proposition 6 (Differentiability of L∏,h). If Tθ : Ω → Ω is Lipschitz w.r.t. to θ, then L∏,h(α, βθ) is
continuous w.r.t. θ, and is differentiable almost everywhere. Furthermore, we have
VθLD,h (α, βθ)
Vθf(Tθ(x))dβ,
(41)
-
Ω
where f is a maximizer of LD,h.
Proof. We first prove LD,h (α, βθ) is continuous w.r.t. θ. By definition, we have LD,h(α, β) =
KRh(α, β) - 2 (mα + me). Thus by triangle inequality of the KR metric, for arbitrary θ, θ0 ∈ Rd,
we have
|LD,h (α, βθ) - LD,h(α, βθ0 )|
=|(KRh(α,βθ) - 2(mα + mβθ)) 一 (KRh(α,βθ') - 2(mα + me。,)，
=|KRh(α,βθ) - KRh(α,βθ,)|
≤KRh(βθ0, βθ)
(42)
Note that the second equality holds because T maintains the total mass, i.e., meθ0 = meθ = me. In
addition, we have
KRh(βθ0, βθ) ≤ W1(βθ0,βθ) ≤
d(Tθ(y)
Jω
- Tθ0 (y))dβ(y).
By assumption, Tθ is Lipschitz w.r.t. θ, thus there exists a constant ∆ > 0, such that
d(T(y), Tθ,(y)) ≤ ∆∣∣θ - θ0∣∣. Thus KRh(βθ,,βθ) ≤ me∆∣∣θ - θ01|. Finally, we have
LD,h(α,βθ) - LD,h(α,βθo) ≤ KRh(βθ,,βθ) ≤ rnφ∆∣∣θ - θ0∣∣,
which proves LD,h (α, βθ) is continuous w.r.t. θ, and Radamacher’s theorem states that it is differen-
tiable almost everywhere.
The rest of the proof is similar to that of Proposition 5.	□
Remark The main idea used in the proofs in this section is based on that in Arjovsky et al. (2017).
However, a key difference is that unlike W1, neither LM,m nor LD,h is a metric, i.e., they do not
necessarily satisfy triangle inequality, thus the differences cannot be bounded directly. This gap
is bridged by using the KR metric in Proposition 6, and by constructing a transportation plan in
Proposition 5.
21
Published as a conference paper at ICLR 2022
D.3 Application to Point Registration
For point set registration task, we focus on the discrete distributions and re-state the previous results.
We first present the proof of the main theorems in the main text.
Proof of Theorem 1. The KR formulation of LD,h is given in (30), and the existence of the optimizer
is proved in Proposition 3. The gradient of LD,h (α, βθ) is derived in Proposition 6. Theorem 1 is
proved by applying these results to discrete distributions.	□
Proof of Theorem 2. The KR formulation of LM,m is given in Theorem 3, and the existence of
the optimizer is proved in Proposition 4. The gradient of LM,m (α, βθ) is derived in Proposition 5.
Theorem 2 is proved by applying these results to discrete distributions.	□
Then we verify that the parametrized transformation Tθ used in our paper satisfies the regularity
assumption, i.e., it is Lipschitz w.r.t. the parameter θ.
Lemma 2. If T :Rd X Ω → Ω is Lipschitz w.rt. to its first variable, thenfor arbitrary y ∈ Y, and θ,
θ0 ∈ Rd, there exists ∆ > 0, such that
l∣τθ (y) -Tθ0 (y)ll≤ δ∣∣θ - θ0∣∣.
Proposition 7. Given a point set Y = {yi}r=ι ⊆ Ω. Let θ = (A,t, V) ∈ R12+3r, where A ∈ R3×3
is a affinity matrix, t ∈ R1×3 is a translation vector, V ∈ Rr×3 is the offset vectors for all points in
Y, and Vi represent the i-th row of V. For each point yi ∈ Y, define Tθ(yi) = yiA + t + Vi, where
Vi represents the i -th row of V. T : Rd × Ω → Ω is Lipschitz w.r.t. to its first variable.
Proof. Note for arbitrary A, there exists δ > 0, such that ||A||2 ≤ δ∣∣A∣∣F, where || ∙ ||f is the
τ-< i ∙	τr∏ i ∙,	n / Λ » τ r∖ n ∕1Γ 丁 ^τ~7-∖ ι 一，z^ ι
Frobenius norm. For arbitrary θ = (A, t, V), θ = (A, t, V) and yi ∈ Y, we have
.. ... ... ~ 〜 ~ ...
IT(yi) - Te(yi川2 = ll(A - A)yi + (t - t) + (V - Vol∣2
.. ~........... ..	~ .
≤ ||A	- Ae||2||yi||2	+ ||t	- te||2	+ ||Vi -	Vei||2
... ~......... .. ~.. .. .
≤ δ∣∣A - A||f∣∣yi∣∣2 + ∣∣θ - θ∣∣2 + ∣∣θ-θ∣∣2
.. ........... .. ~.. .. .
≤ δllθ - θll2l∣yill2 + llθ - ell2 + llθ - θl∣2
〜
= (δllyill + 2)llθ - θell2,
which proves that T is Lipschitz w.r.t. to its first variable.	□
E	Properties
This section answers two questions: 1) What are the connections between KR forms of LM,m and
LD,h? 2) What does the potential of LM,m looks like? We briefly discuss the first question in
Sec. E.1. For the second question, the main result is Proposition 11 in Sec. E.2, where we show
that for each point in α and β , the potential f either has gradient norm 1 or attains its maximum or
minimum.
E.1 CONNECTIONS BETWEEN LM, LD AND W1
This subsection briefly discuss the relations between the KR forms of LM,m, LD,h and W1.
The following proposition presents a simple fact about the relation between LM and LD. We omit all
proves in this subsection as all results can be easily verified.
Proposition 8 (Relations between LM and LD). Let f be a maximizer of LαM,β,m. For the fixed
h = 一 inf(f), f is also a maximizer of LD，.
Now we turn to the special cases of LM,m and LD,h . To begin with, we define two useful functionals
as follows.
22
Published as a conference paper at ICLR 2022
(43)
(44)
Definition 3 (LPα,β). Define functional LPα,β as
Rω fdα - Rω fdβ f ∈ Lip(Ω) , f ≤ 0, m. ≥ m§
-∞	else,
and define LP (α, β) = SuPf ∈c(ω) LPe (f) ∙
Definition 4 (W1α,β). Define the functional associated to Wasserstein-1 metric W1 as
∕ω fdα - J。fdβ f ∈ Lip(Ω), mɑ ≥ mβ
-∞	else,
thus Wi can be expressed as Wι(α, β) = SuPf ∈c(ω) LWe(f)∙
The following proposition describes the relations between LM,m, LD,h, LP and W1.
Proposition 9 (Special cases of LM,m and LD,h). (1) When mα ≥ mβ = m, the maximizer of
LM,m is also a maximizer of LP.
(2)	When ma ≥ mβ and h ≥ diam(Ω), the maximizer of Ld” is also a maximizer of Lp.
(3)	When mα = mβ, the maximizer of LP is also a maximizer ofW1.
We note LP is for “semi-complete” Wasserstein problem, where all mass of β is transported to α.
Thus it may be interesting on its own, for example in the template matching problem where the data
points in an incomplete “data distribution” is matched to a complete “model distribution”.
Finally, we have the following straightforward corollary.
Corollary 1. (1) When mα = mβ = m, LM,m is equivalent to W1.
(2) When ma = mβ and h ≥ diam(Ω), Ld,h is equivalent to Wi — hmβ.
E.2 Properties of the Potentials
Consider Wi as a special case of Lm,m. Its potential has the following property.
Lemma 3 (Potential of Wi (Gulrajani et al., 2017)). Let f be a maximizer of Wiα,β. Then f has
gradient norm 1 (α + β)-almost surely.
The main result of this subsection is the extension of this property to Lm,w, in Proposition 11.
To begin with, we note that by definition, LM,m (α, β) only transports a a fraction of mass and
discards the other. We called the transported mass “active” and the discarded mass “inactive”. An
important observation is that, if we throw away some inactive mass, the solution to the problem will
not be affected; and if we only focus on the active mass, we immediately obtain a solution to Wi .
This is formally stated as follows.
Proposition 10. Let π be the solution to the primal form of LM,m(α, β). Define α0 and β0 be
measures satisfying
n#(A) ≤	α0(A)	≤	α(A)	and	n#(A)	≤ β0(A)	≤	β(A).	(45)
for an arbitrary measurable set A.
(1)	π is also the solution to the primalform of LM,m(α∖ β0) and Wi (n#, n#), thus
LMm(α, β) = LM,m(α0,β0) = Wi (n#,n#)	(46)
1111 ∏-2
(2)	Let f be a maximizer of LαM,β,m. Then f is also a maximizer of LαM,,βm and Wi#, #.
Proof. (1) Notice all admissible solutions to Wi (n#, n#) are also admissible solutions to
LMm(α,β). Then we can prove Wi(n#, n#) = LM,m(α,β) by contradiction. Similarly, we
can prove Wi(n#,n#) = LM,m(α, β).
23
Published as a conference paper at ICLR 2022
(2) Note we have
LMJ(f) = L fdα0 + 卜nf(f) - f )dβ0 - inf(f)m
≥ [fdα + 卜nf(f) - f )dβ - inf(f)m = LMm(f)
LM,m (α, β),
(47)
where the inequality holds because f ≤ 0, inf(f) - f ≤ 0, α0 ≤ α and β0 ≤ β. According to the
first part of this proof, we have LM,m(α0,β0) = LM,m (α,β), thus LMlm (f) ≤ LM,m(α0,β0)=
LM,m (α, β). By combining these two equalities, we conclude that LαM0,,βm0 (f) = LM,m (α0, β0) =
LM,m (α, β), i.e., f is a maximizer ofLαM0,,βm0.
In addition, we have
12
Wi#，#(f) = / fdn# - / fdn# ≥ / fdn# - / fdn# -inf(f)(m - m(n#))
Ω	Ω	Ω	Ω
=LMt,m # (f) ≥ LMom(f) = LM,m(α, β),
where the first inequality holds because - inf(f )(m - m(n#)) ≤ 0, and the second inequality
holds following equation (47). According to the first part of this proof, We have Wi (n# ,n#)=
C , c、 ，	-4#,n#,
LM,m(α,β), thus Wi# # (f) ≤ Wi(n#, n#) = LM,m(α,β). By combining these two equalities,
π1 π2	π1 π2
We conclude Wi # # (f) = Wi (n# ,n#) = LM,m (α, β), i.e., f is a maximizer of Wi # #.	□
Thanks to Proposition 10 and Lemma 3, we can now characterize the potential of LM,m on active
mass. This is formally stated in the following corollary.
Corollary 2. Let π be the solution to the primal form of LM,m (α, β), and f be a maximizer of
LMem. Then f has gradient norm 1 (n# + n#)-almost surely.
Now, in order to completely characterize the potential of LM,m , we only need to characterize the
potential on the inactive mass. In fact, we find that the behavior of potential is rather simple on the
inactive mass, i.e., it always attains its maximum or minimum. This is formally stated as follows.
Corollary 3 (Flatness on inactive mass). Let f be a maximizer of LαM,β,m , and π be the solution to the
primal form of LαM,β,m . Let αS and βS be non-negative measures satisfying
n# ≤ α — as ≤ α and n# ≤ β — βs ≤ β.
Then f = 0 αS -almost surely, and f = - inf(f) βS -almost surely.
Proof. According to Proposition 10, we have LMrm,β = LM∖, and f is a maximizer of LMmmS,β.
In other words, we have
fdα - fβ - inf (f)(m - mβ ) =	fd(α - αS) -	fβ - inf (f)(m - mβ ).
By cleaning this equation, we obtain ∕ω f dɑs = 0. Since f ≤ 0 and as is a non-negative measure,
we conclude that f = 0 as-almost surely. The statement for βs can be proved in a similar way. □
Finally, we can present a qualitative description for the potential of LM,m by decompose mass a and
β into active and inactive mass.
Proposition 11. Let f be a maximizer of LM,m(α, β). There exist non-negative measures μ, Va ≤ a
and Vβ ≤ β satisfying μ + Va + Ve = a + β, such that 1) f has gradient norm 1 μ-almost SUrely 2)
f attains the maximum να -almost surely, and 3) f attains the minimum νβ -almost surely.
We note that similar conclusion holds for LD,h .
Proposition 12. Let f be a maximizer of Lp,h(a, β). There exist non-negative measures μ, Va ≤ a
and Vβ ≤ β satisfying μ + Va + Vβ = a + β, such that 1) f has gradient norm 1 μ-almost surely 2)
f attains the maximum Va-almost surely, and 3) f attains the minimum Vβ -almost surely.
24
Published as a conference paper at ICLR 2022
Remark Given Proposition 11, an important question is where are the inactive mass. An direct
observation is that if a region is far away from one of the measures, then all mass within this region
is inactive. Therefore, we can easily identify some inactive regions where the potential attains its
maximum or minimum, i.e., flat. We present the following straightforward propositions without
proof. A practical example is shown in Fig. 9.
Corollary 4 (Flat region). Let f denote a maximizer of LαM,β,m. Denote regions
Fα = x|dist(x, supp(β)) > - inf(f) and Fβ = y|dist(y, supp(α)) > - inf(f) .
Then f = 0 α∣Fα-almost surely, and f = inf(f)e|产后-almost surely.
Corollary 5 (Flat region). Let f denote a maximizer of LαD,,βh. Denote regions
Fα = x|dist(x, supp(β)) > h and Fβ = y|dist(y, supp(α)) > h .
Then f = 0 α∣Fα-almost surely, and f = h β|产后-almost surely.
F	More Details of the Main Text
F.1 More Details of Sec. 2
Our method is related to Wasserstein generative adversarial network (WGAN) (Arjovsky et al., 2017),
which is a popular method for large scale DM problems. A recent survey of the applications of
WGAN can be found in Gui et al. (2020). WGAN efficiently optimizes the Wasserstein-1 metric by
approximating the KR potential using a neural network. This technique is also used in our method.
In this sense, our method directly generalizes WGAN to PDM problems.
F.2 More Details of Sec. 4.2
We present a simple example comparing the primal and the KR forms in Fig. 9. We show the
correspondence P obtained by the primal form in the 1-st row, where the black lines link the
corresponding points. To estimate the KR forms, we parametrize the potential function fw,h by a
neural network shown in Fig. 10, and learn fw,h by maximizing (10) or (11). The 2-nd row visualizes
the learned potential function fw,h. The corresponding gradient norms ∣Vfω,h∣ are shown in the 3-rd
row. ∣Vfw,h∣ are further visualized in the 4-th row, where the size of each point is approximately
proportional to the gradient norm.
We further evaluate the precision of the estimated KR forms. For each setting, we compute LM,m
or LD,h in the KR forms by inserting the learned potential fw,h back into (10) or (11) (without
the gradient penalty term). To obtain the true values of LM,m and LD,h, we compute their primal
forms (1) and (2) using the POT tool box (Flamary et al., 2021). The results are summarized in Tab. 2.
As can be seen, our estimated PW discrepancies are close to the true values with the averaged relative
error less than 0.2%, which we think is sufficiently precise for our application.
As for the efficiency, we note that the KR formulation is not advantageous in this data scale, i.e., tens
of points, where the computation of the exact primal form only takes less than 1 sec on a CPU, while
the computation of the KR form takes a few minutes on a mid-end GPU (due to the requirement of
learning the potential network). However, for large scale dataset, e.g.,〜106, which the primal form
can not handle because the transport map (of size 106 × 106) does not even fit into the memory, the
KR form can still apply (as validate in our experiments) using a GPU with 12G memory.
Finally, we stress that since the implement of the KR form depends on the underlying neural network,
both the efficiency and precision naturally depend on the structure of the neural network. This is
fundamentally different from the solvers of the primal form, such as the Sinkhorn algorithm or the
linear program, which are fixed pipelines. Therefore, it is generally not possible to compare the
efficiency and precision between the KR solvers and the primal solvers in a more rigorous way.
F.3 More Details of Sec. 4.3
To estimate the gradient of the coherence energy fast, we first decompose G as G QΛQT via the
Nystrom method (Williams & Seeger, 2000), where k《r, Q ∈ Rr×k, and A ∈ Rk×k is a diagonal
25
Published as a conference paper at ICLR 2022
P
f
If
(a) LM,25 or LD,0.64 (b) LM,50 or LD,1.09	(c) LM,78 or LD,5	(d) W1
Figure 9: Comparison between the primal and the KR forms of the PW discrepancies on α (blue)
and βθ (red). See text for details.
Table 2: Precision of the estimated PW discrepancies for the “fish” shape in Fig. 9
	LM,25	LM,50	LM,78	LD,0.648	LD,1.09	LD,5	W1
True value	0.1354	0.4191	0.8955	-0.0722	-0.2795	-4.1044	1.0835
KR (Ours)	0.1352	0.4202	0.8994	-0.0724	-0.2791	-4.1004	1.0893
matrix. Then we apply the Woodbury identity to (σI + QΛQT)-1 and obtain
(σI+QΛQT)-1 =σ-1I-σ-2Q(Λ-1+σ-1QTQ)-1QT.
As a result, the gradient of the coherence energy can be approximated as
dc(τθ) = 2λ(σI + G)TV ≈ (2λ)(σ-1 V - σ-2Q(AT + σ-1 QTQ)TQTV).
∂V
F.4 Connections between PWAN and WGAN
In practice, the only difference between PWAN and WGAN is the structures of the potential networks.
This is summarized in Tab. 3. It is easy to see that when mα = mβ = m for LM,m, or mα = mβ
and h > diam(Ω) for LD,h, both d-PWAN and m-PWAN become WGAN.
Table 3: Comparison between the potential networks OfWGAN and PWAN.
	input threshold	Lipschitz	negative & bounded	lower bound h
WGAN	—	✓	—	—
d-PWAN	distance	✓	✓	fixed
m-PWAN	mass	✓	J	learnable
We note that PWAN has the following adversarial explanation. Let us call the points in α real points,
and those in βθ fake points. In the registration process, fw,h is trying to discriminate real and fake
26
Published as a conference paper at ICLR 2022
points by assigning each of them a score in range [-h, 0], where real points have higher scores than
fake points. fw,h is so certain of a fraction of points, that it assigns the highest or lowest possible
score (0 or -h) to them and does not change these scores easily. Meanwhile, Tθ is trying to cheat
fw,h by moving the fraction of fake points of which fw,h is not certain to obtain higher scores. In
addition, Tθ is also trying to move all fake points to keep the coherence energy low. The process ends
when Tθ cannot make further improvement on the fake points.
F.5 Experimental Details
Data synthesis The data synthesis procedure mostly follows Hirose (2021b). Specifically, given
the original point set X ∈ Rr×3, the deformed point set Y ∈ Rr×3 is generated by:
Y = X + V + ,	(48)
where is the Gaussian noise following N(0, 0.02), and V ∈ Rr×3 is the random offset vectors. To
ensure V varies smoothly in the space, we sample column vector Vj from a distribution p:
p(v) = N(v; 0, λ-1G),	(49)
where G ∈ Rr×r is a GaUssian kernel defined as Gρ(i,j) = e-llyi-yj||2/p. To sample V from
ditribution p, we compute V = G 1 U, where U ∈ Rr ×3 is a random matrix, of which each element
follows N(0,1), and G 1 ≈B QA1 is computed via the Nystrom method.
We use λ = 10 or 50 and ρ = 2 in our experiments.
Network structure The network used in our experiment is a 5-layer point-wise multi-layer percep-
tron with a skip connection. The detailed structure is shown in Fig. 10.
i ~( ⅛ )^*∙ ; ~(⅛)~*∙»∙*( <°L A I -(- A S
Figure 10: The structure of the network used in our experiments. The input is a matrix of shape
(n, 3) representing the coordinates of all points in the set, and the output is a matrix of shape (n, 1)
representing the potential of the corresponding points. mlp(x) represents a multi-layer perceptron
(mlp) with the size x. For example, mlp(m, n) represents a mlp consisting of two layer, and the
size of each layer is m and n. We use ReLu activation function in all except the output layer. The
activation function l(x; h) = max{-|x|, -h} is added to the output to clip the output to interval
[-h, 0].
Parameters We train the network fw,h using the Adam optimizer (Kingma & Ba, 2014), and we
train the transformation Tθ using the RMSprop optimizer (Tieleman & Hinton, 2012). The learning
rates of both optimizers are set to 10-4.
For experiments in Sec. 5.3, the parameters as set as follows: For PWAN, we set (ρ, λ, σ, T ) =
(2, 0.01, 0.1, 2000). For TPS-RPM, we set T _f inalf ac = 500, frac = 1, and T _init = 1.5.
For GMM-REG, we set sigma = 0.5, 0.2, 0.02, Lambda = .1, .02, .01, max_f unction_evals =
50, 50, 100 and level = 3. For BCPD and CPD, we set (β, λ, w) = (2.0, 2.0, 0.1).
For experiments in Sec. 5.4, the parameters as set as follows: For PWAN, we set (ρ, λ, σ, T ) =
(1.0, 10-3, 1.0, 3000). For BCPD and CPD, we set (β, λ, w) = (3.0, 20.0, 0.1). We also test w = 0.4
for these methods, but the difference is not obvious.
For experiments in Sec. F.9, the parameters as set as follows: For PWAN, we set (ρ, λ, σ, T ) =
(0.5, 5 × 10-4, 1.0, 3000). For BCPD and CPD, we set (β, λ, w) = (3.0, 20.0, 0.1).
For experiments in Sec. F.10, the parameters as set as follows: We use m-PWAN. we set (ρ, λ, σ, T) =
(1.0, 1 × 10-4, 1.0, 3000). For BCPD and CPD, we set (β, λ, w) = (0.3, 2.0, 0.1). We also tried
w = 0.6 but the results are similar (not shown in our experiments).
27
Published as a conference paper at ICLR 2022
We note the parameters for CPD and BCPD are suggested in Hirose (2021a).
Parameter selection The parameters m in m-PWAN and h in d-PWAN control the degree of
alignment. Specifically, increasing m or h will lead to the registration of larger fraction of distributions.
When m = 0 or h = 0, PWAN does not align any point (the gradients are 0 for all points); When
m = min{mα, mβ} or h = ∞, PWAN seeks to align the largest fraction of points. Therefore,
choosing overly large m or h may lead to the biased registration (like a DM method), while choosing
overly small m or h may lead to the insufficient registration where too few points are aligned.
For uniform point sets, the parameters m and h can both be determined easily. For d-PWAN, if the
averaged nearest distance between points is s, then h can be set near s. Because for the aligned point
sets, the points that are farther from the overlapped region than distance s are likely to be outliers,
and the use of h = s can effectively discard those outliers. For m-PWAN, one needs to estimate the
overlap ratio ρ of one point set, e.g., α, then m can be set as ρmα . We note this is how we actually
determined m and h in our experiments.
Refinement In our experiments, we find that a nearest-point-based refinement sometimes slightly
improves the performance. Specifically, when the algorithm 1 is near convergence, we can optionally
run a few steps of the nearest-point-based refinement. Specifically, since algorithm 1 is near con-
vergence, we can safely assume that the point sets are sufficiently aligned. Therefore, each point is
highly likely to correspond to its nearest neighborhood in the other set within the mass or distance
threshold. In other words, the correspondence matrix P ∈ Rq×r can be written as
1 if yj = NN(xi) and d(yj, xi) ≤ h
0 else
for LD,h, and
—	11 if yj = NN(Xi) and Xi ∈ NeareSt(m)
i,j 0 else
(50)
(51)
for LM,m, where NN() is the nearest neighborhood function, and N eareSt(m) represent the nearest
m points in X to Y . Thus we can slightly refine the result by switch the divergence term L to
L(α,βθ) = XEijd(Xi, Tθ(yj))2,	(52)
i,j
and update a few steps of the transformation Tθ via gradient descent.
F.6 More Details of Sec. 5.2
The KL divergence and the L2 distance between point set X and Y are formally defined as
L2(X,Y) =	X	!Φ(0∣Xi	-	Xj, 2σ) + ：φ(0∣yi	- y, 2σ)	-	-2-φ(0∣Xi	-	yj-,	2σ),	(53)
xi,xj∈X q	r	qr
yi ,yj ∈Y
KL(X,Y) = -1 X log(ω1 + (1 - ω) X 1 φ(yj∖x%,σ)),	(54)
q yj∈Y	q	xi∈X r
where φ(∙∣u, σ) is the Gaussian distribution with mean U and variance σ. For simplicity, we set σ = 1
and ω = 0.2 for KL and L2.
F.7 More Details of Sec. 5.3
We present more results of the experiments with N = 2000 in Fig. 11 and Fig. 12, and the corre-
sponding quantitative results are shown in Tab. 4 and Tab. 5. We do not show the results of TPS-RPM
on the second experiment, as it generally fails to converge. As can be seen, PWAN successfully
registers the point sets in all cases, while all baseline methods bias toward to the noise points or
to the non-overlapped region when outlier ratio is high, except for TPS-RPM which shows strong
robustness against noise points comparable with PWAN in the first example.
28
Published as a conference paper at ICLR 2022
To obtain a sense of the learned potential network fw,h, we visualize the potential at the end of the
registration process. The results are shown in Fig. 13 and Fig. 14. We represent the value of potential
and the gradient norm at each point by colors, where brighter color indicates higher value. As can be
seen in Fig. 14(c) and 13(c), the network assigns higher values to the points in α while assigning
lower values to the points in βθ. In addition, as shown in Fig. 14(d) and 13(d), most of outliers,
including noise points and the points in non-overlapping region, have low gradients, i.e., they are
successfully discarded by the network.
(a) Initial sets
(b) BCPD
(d) GMM-REG
(c) CPD
(e) TPS-RPM
(f) PWAN
Figure 11: An example of registering noisy point sets. The outlier/non-outlier ratios of the point sets
shown here are 0.2 (1st row), 0.6 (2nd row), 1.2 (3rd row) and 2.0 (4th row).
Table 4: Quantitative comparison of the registration results shown in Fig. 11 using MSE.
Outlier/Non-outlier ratio	BCPD	CPD	GMM-REG	TPS-RPM	PWAN
0.2	0.011	0.0015	0.018	0.0032	0.0031
0.6	0.075	0.083	0.022	0.0026	0.0030
1.2	0.12	0.21	0.042	0.0033	0.0030
2.0	1.24	0.289	3.76	2.354	0.004
Table 5: Quantitative comparison of the registration results shown in Fig. 12 using MSE.
Overlap ratio	BCPD	CPD	GMM-REG	d-PWAN	m-PWAN
0.57	0.18	0.92	032	0.017	0.015
0.75	0.028	0.45	0.043	0.0044	0.0090
1	0.00072	0.0025	0.0078	0.0037	0.0038
F.8 More Details of Sec. 5.4
We provide more details regarding the computation time of our method. We first sample q = r
points from the bunny shape, where q = 105 or 7 × 105. We then run both the 1 GPU version
and the 2 GPU version of PWAN 100 steps, and report their computation time in Fig. 15, where
q-M GPU represents registering q points using M GPU. As can be seen, the majority of time is spent
29
Published as a conference paper at ICLR 2022
(a) Initial sets (b) BCPD
(c) CPD
(d) GMM-REG (e) d-PWAN
(f) m-PWAN
Figure 12: An example of registering partially overlapped point sets. The overlap ratio of the point
sets shown here are 0.57 (1st row), 0.75 (2nd row) and 1 (3rd row).
(d) |Vf|
(c) Potential f
(a) Initial sets
(b) Registered sets
Figure 13: Visualization of the learned potentials on point sets with extra noise points.
on updating the network, and the parallel training can effectively reduce the computation time for
updating the network. In addition, the gain of speed increase as the number of points, i.e., we get
1.3× speedup when q = 105, while the speedup is 1.9× when q = 7 × 105, which is close to the
theoretical speedup value 2.
Finally, we evaluate PWAN on large scale point sets. We generate noisy and partially overlapped
armadillo datasets as stated in Sec. 5.3. We compare PWAN with BCPD and CPD, because they are
the only baseline methods that are scalable in this experiment. We present some registration results in
30
Published as a conference paper at ICLR 2022
(d) |Vf|
(a) Initial sets
(b) Registered sets
(c) Potential f
Figure 14: Visualization of the learned potentials on partially overlapped point sets.
Figure 15: Computation time of our method.
Fig. 16. As can be seen, our method can handle both cases successfully, while both CPD and BCPD
bias toward outliers.
More training details of PWAN is shown in Fig. 17, Fig. 18 and Fig. 19. As can be seen in
Fig. 17(a), 18(a) and 19(a), the source sets are matched to the reference sets smoothly. Fig. 17(b), 18(b)
and 19(b) suggest that at the end of the registration process, most of outliers are correctly discarded.
Besides, Fig. 17(c), 18(c) and 19(c) implies that the norm of gradient of the network is indeed
controlled below 1, i.e., it is indeed Lipschitz. In addition, both the training loss and the MSE
decrease smoothly in the training process.
F.9 Experiment on the Human Face Dataset
We evaluate our method in the space-time faces dataset (Zhang et al., 2008), which consists of a time
series of point sets sampled from a real human face. Each face consists of 23, 728 points and the true
correspondence between faces are known. We use the faces at time i and i + 20 as the source and the
reference set, where i = 1, ..., 20. All point sets in this dataset are the same size and are completely
overlapped.
The registration results are shown in Tab. 6, where we can see PWAN outperforms both CPD and
BCPD. We present the examples of the registration results in Fig. 20. As can be seen, PWAN
successfully aligns the faces in different time points.
31
Published as a conference paper at ICLR 2022
Initial sets	BCPD	CPD	PWAN
(a) An example of registering noisy point sets. The source and refernece sets contain 8 × 104 and 1.76 × 105
points respectively.
Initial sets	BCPD	CPD	d-PWAN	m-PWAN
(b) An example of registering partially overlapped point sets. The source and refernece sets both contain 7 × 104
points.
Figure 16: Examples of registering large scale point sets.
(a) Registration trajectory. The registration process proceedes from left to right.
(b) Visualization of the potential f (left) and it gra-
dient |Vf | (right) at the final step.
(c) Statistics of the training process.
Figure 17: One example of registering the noisy “armadillo” datasets. The source and refernece sets
contain 8 × 104 and 1.76 × 105 points respectively.
Table 6: Registration results of the space-time faces dataset.
BCPD	CpD	PWAN
0.0017 ± 0.001	0.00049 ± 0.0002	0.00032 ± 0.000089
F.10 Experiment on the 3D Human Dataset
We evaluate our method on a challenging human shape dataset (DataSet), which is taken from a
SHREC’19 track called “matching humans with different connectivity”. This dataset consists of 44
shapes, and we manually select 3 pairs of shapes for our experiments. To generate a point set of each
shape, we first sample 50000 random points from the surface of the 3D mesh, and then apply voxel
grid filtering to down-sample the point set to less than 10000 points. The description for the selected
point sets is presented in Tab. 7
32
Published as a conference paper at ICLR 2022
(a) Registration trajectory. The registration process proceedes from left to right.
(c) Statistics of the training process.
(b) Visualization of the potential f (left) and it gra-
dient |Vf | (right) at the final step.
Figure 18: One example of registering the partially overlapped “armadillo” datasets using d-PWAN.
Each point set consists of 7 × 104 points.
(a) Registration trajectory. The registration process proceedes from left to right.
(b) Visualization of the potential f (left) and it gra-
dient |Vf | (right) at the final step.
Figure 19: One example of registering the partially overlapped “armadillo” dataset using m-PWAN.
Each point set consists of 7 × 104 points.
33
Published as a conference paper at ICLR 2022
(a) The aligned point sets (right) is obtained by matching the 1-st frame (left) to the
21-st frame (middle)
(b) The aligned point sets (right) is obtained by matching the 19-st frame (left) to the
39-st frame (middle)
Figure 20: Examples of our registration results on the human faces dataset. Zoom in to see the details.
Table 7: Point sets used for registration. no.m represents the m-th shape in the dataset (DataSet).
(no.1,no.42)	(no.18, no.19)	(no.30, no.31)
SIze	(5575,5793)	(6090,6175)	(6895,6792)
same pose	different pose	different pose
I )pQprmrιπn
different person same person different person
We conduct the following two experiments. In the first experiment, we evaluate our method on
registering the complete point sets. In the second experiment, we evaluate our method on the more
challenging partial matching problem, where we generate incomplete point sets by manually cropping
a fraction of the no.30 and no.31 point sets. We consider 3 types of partial matching problem, i.e.,
match incomplete set to complete set, match complete set to incomplete set and match incomplete set
to incomplete set. For both of these two experiments, we compare our method with CPD and BCPD,
and we only present qualitative registration results, because we do not know the true correspondence
between point sets.
The results of the first experiment is shown in Fig. 21. As can be seen, PWAN can handle both
the local deformations (1-st row) and the articulated deformations (2-nd and 3-rd rows) well, and it
produces good full-body registration results. In contrast, although CPD and BCPD can handle local
deformations relatively well (1-st row), they have difficulties aligning point sets with large articulated
deformations, as significant registration errors are observed near the limbs (2-nd and 3-rd rows).
The results of the second experiment is shown in Fig. 22. As can be seen, both CPD and BCPD fail
in this experiment, as the non-overlapping points are seriously biased. For example, in the 3-rd row,
they both wrongly match the left arm to the body, which causes highly unnatural artifacts. In contrast,
the proposed PWAN can handle the partial matching problem well, since it successfully maintains
the shape of non-overlapping regions, which contributions to the natural registration results.
Finally, we note that although the proposed PWAN generally produces reasonable full-body registra-
tion results, it has some difficulties handling the local details. For example, the hands in Fig. 21 and 22
are generally not well aligned. This drawback might be alleviated by considering local constraints
such as Ge et al. (2014) in the future. In addition, it is worth noticing that the aligned point sets in the
second experiments are natural and do not exist in the original dataset. These results suggests the
potential of our method in other practical tasks such as point set completion and point sets merging.
34
Published as a conference paper at ICLR 2022
(a) Source set
(b) Reference set (c) PWAN (Ours)
(d) BCPD
(e) CPD
Figure 21: The results of registering complete point sets no.1 to no.42 (1-st row), no.18 to no.19
(2-nd row), and no.30 to no.31 (3-rd row). Our results are compared against BCPD (Hirose, 2021a)
and CPD (Myronenko & Song, 2010). Zoom in to see the details.
(a) Source set
(d) BCPD
(e) CPD
(b) Reference set (c) PWAN (Ours)
Figure 22: Registering incomplete point sets no.30 to no.31. We present the results of the complete
to incomplete (1-st row), incomplete to complete (2-nd row) and incomplete to incomplete (3-rd row)
registration. Our results are compared against BCPD (Hirose, 2021a) and CPD (Myronenko & Song,
2010). Zoom in to see the details.
35