Published as a conference paper at ICLR 2022
Sequence Approximation using Feedforward
Spiking Neural Network for Spatiotemporal
Learning: Theory and Optimization Methods
Xueyuan She, Saurabh Dash & Saibal Mukhopadhyay
School of Electrical and Computer Engineering
Georgia Institute of Technology, Atlanta, GA 30332, USA
{xshe6,sdash38}@gatech.edu, saibal.mukhopadhyay@ece.gatech.edu
Ab stract
A dynamical system of spiking neurons with only feedforward connections can
classify spatiotemporal patterns without recurrent connections. However, the the-
oretical construct of a feedforward spiking neural network (SNN) for approximat-
ing a temporal sequence remains unclear, making it challenging to optimize SNN
architectures for learning complex spatiotemporal patterns. In this work, we estab-
lish a theoretical framework to understand and improve sequence approximation
using a feedforward SNN. Our framework shows that a feedforward SNN with
one neuron per layer and skip-layer connections can approximate the mapping
function between any arbitrary pairs of input and output spike train on a compact
domain. Moreover, we prove that heterogeneous neurons with varying dynam-
ics and skip-layer connections improve sequence approximation using feedfor-
ward SNN. Consequently, we propose SNN architectures incorporating the pre-
ceding constructs that are trained using supervised backpropagation-through-time
(BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms
for classification of spatiotemporal data. A dual-search-space Bayesian optimiza-
tion method is developed to optimize architecture and parameters of the proposed
SNN with heterogeneous neuron dynamics and skip-layer connections.
1	Introduction
Spiking neural network (SNN) (Ponulak & Kasinski, 2011) uses biologically inspired neurons
and synaptic connections trainable with either biological learning rules such as spike-timing-
dependent plasticity (STDP) (Gerstner & Kistler, 2002) or statistical training algorithms such as
backpropagation-through-time (BPTT) (Werbos, 1990). The SNNs with simple leaky integrate-and-
fire (LIF) neurons and supervised training have shown classification performance similar to deep
neural networks (DNN) while being energy efficient (Kim et al., 2020b; Wu et al., 2019; Srinivasan
& Roy, 2019). One of SNN’s main difference from DNN is that the neurons are dynamical sys-
tems with internal states evolving over time, making it possible for SNN to learn temporal patterns
without recurrent connections. Empirical results on feedforward-only SNN models show good per-
formance for spatiotemporal data classification, using either supervised training (Lee et al., 2016;
Kaiser et al., 2020; Khoei et al., 2020), or unsupervised learning (She et al., 2021). However, while
empirical results are promising, a lack of theoretical understanding of sequence approximation using
SNN makes it challenging to optimize performance on complex spatiotemporal datasets.
In this work, we develop a theoretical framework for analyzing and improving sequence approxi-
mation using feedforward SNN. We view a feedforward connection of spiking neurons as a spike
propagation path, hereafter referred to as a memory pathways (She et al., 2021), that maps an input
spike train with an arbitrary frequency to an output spike train with a target frequency. Conse-
quently, we argue that an SNN with many memory pathways can approximate a temporal sequence
of spike trains with time-varying unknown frequencies using a series of pre-defined output spike
trains with known frequencies. Our theoretical framework aims to first establish SNN’s ability to
map frequencies of input/output spike trains within arbitrarily small error; and next, derive the basic
principles for adapting neuron dynamics and SNN architecture to improve sequence approximation.
1
Published as a conference paper at ICLR 2022
The theoretical derivations are then investigated with experimental studies on feedforward SNN for
spatiotemporal classifications. We adopt the basic design principles for improving sequence approx-
imation to optimize SNN architectures and study whether these networks can be trained to improve
performance for spatiotemporal classification tasks. The key contributions of this work are:
•	We prove that any spike-sequence-to-spike-sequence mapping functions on a compact do-
main can be approximated by feedforward SNN with one neuron per layer using skip-layer
connections, which cannot be achieved if no skip-layer connection is used.
•	We prove that using heterogeneous neurons having different dynamics and skip-layer con-
nection increases the number of memory pathways a feedforward SNN can achieve and
hence, improves SNN’s capability to represent arbitrary sequences.
•	We develop complex SNN architectures using the preceding theoretical observations and
experimentally demonstrate that they can be trained with supervised BPTT and unsuper-
vised STDP for spatiotemporal data classification.
•	We design a dual-search-space option for Bayesian optimization process to sequentially op-
timize network architectures and neuron dynamics of a feedforward SNN considering het-
erogeneity and skip-layer connection to improve learning and classification of spatiotem-
poral patterns.
We experimentally demonstrate that our network design principles coupled with the dual-search-
space Bayesian optimization improve classification performance on DVS Gesture (Amir et al.,
2017), N-caltech (Orchard et al., 2015), and sequential MNIST. Results show that the design prin-
ciples derived using our theoretical framework for sequence approximation can improve spatiotem-
poral classification performance of SNN.
2	Related Work
Prior theoretical approaches to analyze SNN often focus on the storage and retrieval of precise spike
patterns (Amit & Huang, 2010; Brea et al., 2013). There are also works that consider SNN for
solving optimization problems (Chou et al., 2018; Binas et al., 2016) and works that analyze the
dynamics of SNN (Zhang et al., 2019; Barrett et al., 2013). Those are different topics from the
approximation of spike-sequence-to-spike-sequence mappings functions. SNN that incorporates ex-
citatory and inhibitory signal is shown for its ability to emulate sigmoidal networks (Maass, 1997)
and is theoretically capable of universal function approximation. Feedforward SNN with specially
designed spiking neuron models (Iannella & Back, 2001; Torikai et al., 2008) have been demon-
strated for function approximation, while for networks using LIF neurons, function approximation
has been shown with only empirical results (Farsa et al., 2015). On the other hand, existing works
that have developed efficient training process for SNN and demonstrated classification performance
comparable to deep learning models, have mostly used simpler and generic LIF neuron models (Lee
et al., 2016; Kaiser et al., 2020; Kim et al., 2020b; Wu et al., 2019; Sengupta et al., 2019; Safa et al.,
2021; Han et al., 2020). Therefore, this paper develops the theoretical basis for function approxima-
tion using feedforward SNN with LIF neurons, and studies applications of the developed theoretical
constructs in improving SNN-based spatiotemporal pattern classification.
The effectiveness of heterogeneous neurons (She et al., 2021) and skip-layer connections (Srinivasan
& Roy, 2019; Sengupta et al., 2019) in SNN has been empirically studied in the past. However, no
theoretical approach has been presented to understand why such methods improve learning of spike
sequences, and how to optimize SNN’s architecture and parameters to effectively exploit these de-
sign constructs. It is possible to search for the optimal SNN configurations through optimization
algorithms, but the large amount of hyper-parameters for spiking neurons and network structure cre-
ates a high-dimensional search space that is long and difficult to solve. Bayesian optimization (Snoek
et al., 2012) uses collected data points to make decision on the next test point that could provide im-
provement, thus accelerates the optimization process. Prior works (Parsa et al., 2019; Kim et al.,
2020a) have shown that SNN performance can be effectively improved with Bayesian optimization.
While those works consider a single or a few neuron parameters, the dual-search-space Bayesian
optimization proposed in this work optimizes both network architecture and neuron parameters effi-
ciently by separating the discrete search spaces from the continuous search spaces.
2
Published as a conference paper at ICLR 2022
Figure 1: (a) A time-varying input spike sequence received by two memory pathways: neuron
membrane potential plots show the different response from the neurons to the given input. (b) A
minimal multi-neuron-dynamic (mMND) network with m layers and n neuron dynamics.
3	Approximation Theory of Feedforward SNN
3.1	definitions and Notations
Definition 1 Neuron Response Rate γ For a spiking neuron n with membrane potential at vreset and
input spike sequence with period tin, γ is the number of input spike n needs to reach vth.
Definition 2 Memory Pathways For a feedforward sNN with m layers, a memory pathway is defined
as a spike propagation path from input to the output layer. Two memory pathways are considered
distinct if the set of neurons contained in them is different.
Definition 3 Minimal Multi-neuron-dynamic (mMND) Network A densely connected network in
which each layer has an arbitrary number of neurons that have different neuron parameters. All
synapses from one pre-synaptic neuron have the same synaptic conductance.
Notations Neuron Delay tnd is the time for a spike from pre-synaptic neuron to arrive at its post-
synaptic neurons, as shown in Figure 1(a). For a feedforward sNN with m layers, a skip-layer
connection can be defined with source layer and target layer pair (ls , lt). The output feature map
from source layer is concatenated to the original input feature map of the target layer. For the
analysis of spike sequence in temporal space, the notation of Tmax and Tmin are defined as positive
real numbers such that Tmax > Tmin. > 0 is the error of approximation.
Figure 1(a) shows two memory pathways receiving an input spike sequence with time-varying pe-
riods. As the neurons have different dynamics, the two memory pathways have different response
to the input spike sequence. An example of mMND network with m layers and n neuron dynam-
ics is shown in Figure 1(b). sNN with multilayer perceptron (MLp) structure can be considered a
scaled-up mMND network with multiple neurons for each dynamic. A network with convolutional
structure can be considered a scaled-up mMND network with duplicated connections in each layer.
We analyze the correlation of network capacity and structure based on mMND networks. The de-
sign of neuron heterogeneity can also be implemented in MLp-sNN and Conv-sNN as described in
section 4. The analysis for network capacity can be extended to those networks according to their
specific layer dimensions.
3.2	Modeling of Spiking Neuron
sNN consists of spiking neurons connected by synapses. The spiking neuron model studied in this
work is leaky integrate-and-fire (LiF) as defined by the following equations:
Tm ~~u = a + RmI — v; V = Vreset, if v > Vthreshold
dt
(1)
3
Published as a conference paper at ICLR 2022
Rm is membrane resistance, τm = RmCm is time constant and Cm is membrane capacitance. a
is the resting potential. I is the sum of current from all input synapses that connect to the neuron.
A spike is generated when membrane potential v cross threshold and the neuron enters refractory
period r, during which the neuron maintains its membrane potential at vreset . The time it takes for
a pre-synaptic neuron to send a spike to its post-synaptic neurons is tnd. Neuron response rate γ is
a property of a spiking neuron’s response to certain input spike sequence. We show how the value
of γ can be evaluated below.
Remark For any input spike sequence, each individual spike can be described with Dirac delta
function δ(t - ti) where ti is the time of the i-th input spike. Consider membrane potential of a
spiking neuron receiving the input before reaching spiking threshold, with initial state at t = 0 with
v = vreset, solving the differential equation (1) leads to (Gerstner, 1995):
v(t) = Vresete-Tm + a(1 — e-Tm) + me-e-Tm ^X G / δ(t - ti)eTm dt
τm	i 0
(2)
Here, G is the conductance of input synapse connected to the neuron, which is trainable. From (2),
there exists a value ofu such that vm(tu-1) < vthreshold andvm(tu) >= vthreshold. By evaluating
(2) for u given neuron parameters and input spike sequence, the neuron response rate γ can be found.
3.3	Approximation Theorem of Feedforward SNN
To develop the approximation theorem for feedforward SNN, we first aim to understand the range of
neuron response rate that can be achieved. We show with Lemma 1 that for any input spike sequence
with periods in a closed interval, it is possible to set the neuron response rate γ to any positive
integer. Based on this property, we show with Theorem 1 that by connecting a list of spiking neurons
with certain γ sequentially and inserting skip-layer connections, network with spike period mapping
function P (t) can be achieved to approximate the target spike sequence. To understand whether
this capability of feedforward SNN relies on skip-layer connections, we develop Lemma 2 to prove
that skip-layer connections are indeed necessary. In subsection 3.4 we investigate the correlation
between approximation capability and network structures by analyzing the cutoff property of spiking
neurons, which can change the network’s connectivity. In our analysis, we focus on two particular
designs: heterogeneous network (Lemma 4) and skip-layer connection (Lemma 5), and show their
impact on the number of distinct memory pathways in a network. All lemmas are formally proved
in the appendix.
Lemma 1 For any input spike sequence with period tin in range [Tmin , Tmax], there exists a spik-
ing neuron n with fixed parameters vth , vreset, a, Rm and τm, such that by changing synaptic
conductance G, it is possible to set the neuron response rate γn to be any positive integer.
Proof Sketch. (Formal proof in Appendix A) Given an input spike sequence, the highest possible
membrane potential decay ∆v for any input tin ∈ [Tmin , Tmax] can be derived as a function of
neuron parameters. We show it is possible to make ∆v tend to zero by setting the neuron parameters.
Since the decay of v can be negligible, γn can be set to any positive integer by changing G.
Theorem 1 For any input and target output spike sequence pair with periods (tin, tout) ∈
[Tmin, Tmax] × [Tmin, Tmax], there exists a minimal-layer-size network with skip-layer connections
that has memory pathway with output spike period function P(t) such that |P(tin) - tout| < .
Proof Sketch. (Formal proof in Appendix B) With skip-layer connections, there can be multiple
memory pathways in a minimal-layer-size network as neurons can be either included or skipped
through. Hence it is possible to create memory pathways with different delay times for each input
spike in a network. By connecting the output of those memory pathways to a common neuron n0 ,
spike sequence of any arbitrary period tint such that tint <= tin can be generated within . By
setting γn0 > 1, the output from n0 receiving input spike sequence with tint is %t = Y ∙ tirιt.. Hence
it is possible to achieve a network with output spike period P(t) such that |P(tin) - tout| < .
Lemma 2 With no skip-layer connection, there does not exist a minimal-layer-size network that has
output spike period function P(t) such that for any input and target output spike sequence pair with
periods (tin , tout ) ∈ [Tmin , Tmax] × [Tmin , Tmax], |P (tin ) - tout | < .
4
Published as a conference paper at ICLR 2022
Proof Sketch. (Formal proof in Appendix C) A minimal-layer-size network without skip-layer con-
nection has only one memory pathway. For a particular input spike sequence with period tin , dif-
ferent output period P(tin) can be achieved by changing γ of neurons in the memory pathway. We
show that there exists two output spike periods P (tin) and P0(tin), such that P(tin) - P0(tin) is a
constant value independent of network or neuron configurations, and there can be no P00(tin) such
that P(tin) < P00(tin) < P0(tin). Therefore, for any minimal-layer-size network, there exists tout
within the range of (P(tin), P0(tin)) such that |P(tin) - tout| < does not hold true.
3.4	Network Structure and Memory Pathways
Based on Theorem 1, it is possible to approximate an input/output spike sequence mapping func-
tion using a minimal-layer-size network with specific configuration, which can be considered as a
memory pathway. Since any bounded continuous function on a compact domain can be approxi-
mated to arbitrary accuracy using a piece-wise constant function, and it is possible to use a memory
pathway to approximate each of the constant function, with increasing number of distinct memory
pathways, a feedforward SNN can achieve approximation of continuous functions with less error.
In this subsection, we show that with two structural designs: heterogeneous network i.e. a network
having neurons with different dynamics and adding skip-layer connections, a feedforward SNN has
the capability to achieve more distinct memory pathways.
Cutoff Frequency of a Memory Path We first show the correlation of cutoff period and spiking
neuron parameters with Lemma 3, which is proved in Appendix D.
Lemma 3 A spiking neuron has cutoff period ωc = τm ln(
sequence cannot cause the spiking neuron to spike.
VreSet -a
VreSet -a+ RmG
τm
) above which input spike
Remark From Lemma 3, it can be observed that the cutoff period ωc of a neuron can be configured
to any positive real number by changing the neuron parameters and synaptic conductance G. Further,
with fixed G, ωc can be configured to any positive real number by changing the neuron parameters.
Neurons that are in cutoff change the spike propagation path in a network as they send no output
spikes. This creates different memory pathways without changing the connections in a network.
Heterogeneous Network If an mMND network has the same parameters for all neurons in each
layer, the majority of the neurons are included in the same memory pathway, leading to the upper
bound of number of distinct memory pathways to be limited. With Lemma 4, we show the rela-
tionship between the upper bound of the number of distinct memory pathways and the number of
different neuron dynamics in an mMND network.
Lemma 4 For an mMND network with m layers and {λ1, λ2, ...λm} number of different neuron
dynamics in each layer, the least upper bound of the number of distinct memory pathways is im=1 λi.
Proof Sketch. (Formal proof in Appendix D) For an mMND network, it is possible to have neurons
with different ωc in each layer, which creates λi number of different neuron activation states for
layer i. Across all network layers, the highest possible number of different neuron activation states
is therefore the product of λ of each layer. Since neurons in cutoff do not propagate spikes, they can
be removed from a memory pathway. This leads to Qim=1 λi as the least upper bound of the number
of memory pathways.
Compared to a network with homogeneous neuron parameters, in which the upper bound of number
of distinct memory pathways is λm, Lemma 4 indicates that heterogeneous network increases the
maximum achievable number of distinct memory pathways in a feedforward SNN.
Skip-layer Connection We show that adding skip-layer connection increases the upper bound of
the number of memory pathways in a network with Lemma 5.
Lemma 5 For an mMND network with m layers and {λ1, λ2, ...λm} different neuron dynamics in
each layer and a skip-layer connection made between layer la and lb, s.t. a, b ∈ {1, 2, ...m} and
(b 一 a) > 1, the least upper bound of the number of memory pathways is 口m=I λi + (∩a=ι λi ∙
Qim=bλi)
5
Published as a conference paper at ICLR 2022
Input
(a) BPTT Training
Source
layer
Target
layer
Multi-neuron-dynamic Layers
ʌ
。一UJeUAP uo」n。U
SnO9U96O」9工
Dynamic
m
Dynamic
m-1
Dynamic
1
Dynamic
m
Dynamic
m-1
Dynamic
1
τ Transferred
Synapse
Learned
Synapse
(b) STDP Training	Skip-Iayer
connection
Multi-neuron-dynamic Layers
τ Transferred
Synapse
Learned
Synapse
Memory
Module
dm
。一UJeUAP uo」n。U
Sn09u962。工
Memory
Module
dm
Memory
Module
di
Memory
Module
di
Learner
Module
Learner
Module
Figure 2: (a) The proposed network with BPTT training, each multi-neuron-dynamic layer contains
a set of neuron dynamics from d1 to dm. (b) The proposed network with STDP training.
Proof Sketch. (Formal proof in Appendix D) Compared to the network considered in Lemma 4,
by adding a skip-layer connection, there are additional possible neuron activation states in the net-
work that result from the cutoff of neurons in layers between la and lb . Without layers between
la and lb in the spike propagation path, the least upper bound of the number of memory pathways
is increased by the maximum number of distinct memory pathways in a network that has layers
{l1, l2, ..., la, lb, lb+1, ..., lm} connected sequentially.
4	SNN Architecture Design using Approximation Theory
In this section, we discuss design of SNN architectures as inspired by the developed approximation
theory for feedforward SNN, with more details in Appendix G.
Network Template for BPTT Training For BPTT training, the network template is shown in Fig-
ure 2(a). The heterogeneity in neuron dynamics is implemented by using the multi-neuron-dynamic
layers, which can either be convolutional or fully connected. The multi-neuron-dynamic layers use
different neuron parameters for spiking neurons in each neuron dynamic module, which contains a
certain number of feature maps for convolutional layers or a certain number of neurons for fully-
connected layers. There are two types of synapses between layers: transferred synapses marked as
black dashed arrows and learned synapses marked as red solid arrows. The conductance of learned
synapses is optimized by the BPTT algorithm during training, and the transferred synapses have
the same conductance as the learned synapses from the same pre-synaptic neuron. During forward
pass, neurons in each layer receive the same input features and respond differently based on their
neuron dynamics to generate different output features. During back-propagation, only conductance
of the learned synapses are updated. Skip-layer connection is implemented with the output spike
matrix from source layer concatenated to the original input spike matrix of the target layer. The
skip-layer connection has the same implementation as the regular connection between consecutive
layers, with both learned and transferred synapses (Figure 2(a)). The last layer of the network is a
fully-connected layer with homogeneous dynamic to generate prediction labels.
Network Template for STDP Learning For networks trained with STDP, the template is shown
in Figure 2(b). Each layer contains a learner module and a memory module. Learner modules use
homogeneous neuron dynamic that is suitable for STDP learning, and memory modules consist of
neurons with different dynamics. There are also two types of synapses: transferred synapses and
learned synapses. Between two layers, memory modules are connected with transferred synapses
and memory modules are connected to learner modules with learned synapses. Leaner modules be-
6
Published as a conference paper at ICLR 2022
tween layers are not directly connected. STDP training proceeds as a layer-by-layer process. During
training of the first layer, conductance of synapses connecting neurons in memory module to neu-
rons in learner module is learned with STDP using all training data without supervision. Then, the
learned conductance is transferred to the transferred synapses in the same layer. During training of
the second layer, layer 1 memory module perceives the same input features and generates different
output features with the heterogeneous neurons. This lay-by-layer process is repeated until the layer
before the final layer finishes learning. The final linear layer is then fine-tuned using stochastic
gradient descent (SGD) based on spike frequency array from the last multi-neuron-dynamic layer
generated based on the labeled data. Skip-layer connection is implemented by connecting the mem-
ory module of the source layer to the target layer. The connections are made with the two types of
synapses and follow the same training process as the consecutive layers.
Dual-search-space Bayesian Optimization Bayesian optimization uses Gaussian process to model
the distribution of an objective function, and an acquisition function to decide points to evaluate.
For data points in a target dataset x ∈ X and the corresponding label y ∈ Y , an SNN with network
structure V and neuron parameters W acts as a function fV,W (x) that maps input data x to predicted
label y. The optimization problem in this work is defined as
minV,WP where P =	L(y, fV,W(x))
x∈X,y∈Y
(3)
V contains the number of layers Nlayers, the number of memory dynamics Ndynmaic and skip-
layer connection configuration variables Nskip, Lstart and Lend, each controlling the number of
skip-layer connections, the first layer and last layer to implement skip-layer connections. All of the
values are discrete. W contains the values for a, τm and Rm in (1), which are continuous. We
separate the discrete and continuous search spaces by implementing a dual-search-space optimiza-
tion process, where V is first optimized with fixed, manually tuned neuron parameters. After an
optimal structure is found, W are optimized for the selected V . Details on the configurations of the
optimization process are listed in the appendix. To achieve Bayesian optimization with constraints,
we implement a modified expected improvement (EI) acquisition function similar to the one shown
by Gardner (Gardner et al., 2014), which uses a Gaussian process to model the feasibility indicator
due to its high evaluation cost. In this work, since the constraint function can be explicitly defined,
we use a feasibility indicator that is directly evaluated. The modified EI function is defined as:
Ic(W) = ∆(W) ∙ max{0, P (W) -P (W+)}
(4)
where W is the network configuration containing W and V . W+ is the test point that provided
the best result. ∆(W) is the explicitly defined indicator function that takes the value of 1 when all
constraints are satisfied and 0 otherwise.
5	Experiments
5.1	Experiment Settings
Datasets tested in the experiment include the DVS Gesture (Amir et al., 2017), which is a human
gesture dataset captured by DVS cameras, and the N-Caltech101 (Orchard et al., 2015), which is an
event-based version of the Caltech101 dataset. The proposed method is also tested for MLP-style
SNN on the sequential MNIST dataset presented row-by-row. We also vary the amount of labeled
data used during training ranging from using 100% labeled data for training down to 10% labeled
data (30% for N-Caltech101) during training. Note, during STDP training networks always use the
entire but un-labeled training dataset; however, only the fraction of the labeled data is used for su-
pervised fine-tuning of the last layer. Comparison is made for DVS Gesture and N-Caltech101 with
prior works including ConvLSNN, which is a combination of convolutional SNN and recurrent SNN
with long and short-term neurons trained with BPTT (Salaj et al., 2020), DECOLLE (Kaiser et al.,
2020), which uses surrogate gradient to train a convolutional feedforward SNN, HATS (Sironi et al.,
2018), which implements time surfaces and SVM for classification and H-SNN (She et al., 2021)
which uses STDP to train a convolutional SNN with two neuron dynamics. Additional function
approximation experiments are discussed in Appendix E.
7
Published as a conference paper at ICLR 2022
—J-Single-search-space
-J- Dual-search-space stage 1
-1 -Dual-search-space stage 2
0	10	20	30	40	50	60
Evaluations
N-CaItecMOI v√ STDP Training
Oooo
6 5 4 3
(%)」0击 Uo=EP=e>
Figure 3: Validation error over optimization iterations for the proposed dual-search-space Bayesian
optimization compared to the normal single-search-space Bayesian optimization.
0	10	20	30	40	50	60	70	80
Evaluations
5.2	Effect OF Dual-search-space BAYESIAN Optimization
We compare the proposed dual-search-space Bayesian optimization with regular Bayesian optimiza-
tion using a single search space for network validation error over 5 runs. The result from the N-
CaIteCh101 dataset is shown in Figure 3. It can be observed that the two optimization approaches
achieve similar minimum validation error after convergence. By separating the search spaces, the
proposed optimization process reaches convergence faster than regular single-search-space opti-
mization. it is also worth noting that, between the two stages in the optimization process for BpTT
training, the first stage accounts for more reduction in validation error than the second stage. This
indicates that optimizing network structure causes more impact to BpTT training than optimizing
neuron parameters, which is potentially due to the reason that network structure more heavily af-
fects the number of memory pathways in the network than neuron parameters. On the other hand, for
sTDp training where learning behavior is sensitive to the dynamic of spiking neurons, the reduction
of validation error is more equally shared between the two optimization stages. Over the 5 runs,
among all network configurations achieved after the dual-search-space optimization converges, we
compare the configuration with the lowest number of trainable parameters against baseline models.
The specific configurations for the optimized networks are listed in Table 1. it can be observed that
for BpTT algorithm, the optimized networks have more layers than the sTDp trained networks, and
the optimal values found for neuron parameters are highly distinct for the two training methods.
Table 1: Configuration of optimized network models
conv. Layer skip-layer Number of Different Neuron parameters
Network	Number	connection	Neuron Dynamics and a	τm	Rm
BpTT, Gesture	9	(27	4, (-24,-17,-12,-9)	120	340
BpTT, N-caltech	12	(2,5),(5,8),(8,11)	5, (-23,-16,-14,-11,-8)	70	300
sTDp, Gesture	6	(2,4),(4,6)~~	4, (-26,-24,-15,-9)	110	260
sTDp, N-caltech	8	(3,5),(5,7)	6, (-21,-19,-17,-13,-9,-7)	140	240
5.3	Ablation Studies
To investigate the effect of using multiple neuron dynamics, we apply the same dual-search-space
Bayesian optimization process for networks that have homogeneous neuron dynamic for the same
Table 2: Ablation studies of optimized networks
Model	Heterogeneity	skip-layer	DVs Gesture	N-caltech101	s-MNisT
Homogeneous-BpTT	N	Y	95.0	65.3	95.5
No-skip-layer-BpTT	Y	N	96.5	63.5	94.8
This Work-BPTT	Y	Y	98.0	71.2	97.3
Homogeneous-sTDp	N	Y	91.3	37.0	94.3
No-skip-layer-sTDp	Y	N	93.1	51.9	95.5
This Work-STDP	Y	Y	96.6	58.1	96.1
8
Published as a conference paper at ICLR 2022
Table 3: Accuracy (%) for DVS Gesture (top) and N-Caltech101 (bottom)
Model	Labeled Data % In Training				Parameter Number
	100%	50%	30%	10%	
ConvLSNN (Salaj et al., 2020)	97.1	95.3	92.0	84.3	2.9M
DECOLLE (Kaiser et al., 2020)	97.5	95.0	91.2	83.9	1.3M
(Fang et al., 2021)	97.8	-	-	-	-
HATS (Sironi et al., 2018)	95.2	94.1	91.6	83.7	-
H-SNN (She et al., 2021)	96.2	95.8	93.7	88.2	0.74M
This Work-STDP Training	96.6	96.0	94.1	91.2	0.81M
This Work-BPTT Training	98.0	95.3	91.1	82.4	1.1M
	Labeled Data % In Training				Parameter
Model	100%	70%	50%	30%	Number
ConvLSNN (Salaj et al., 2020)	63.1	58.7	51.3	45.4	3.0M
DECOLLE (Kaiser et al., 2020)	66.9	61.9	56.2	50.6	2.0M
HATS (Sironi et al., 2018)	64.2	61.0	54.3	48.8	-
H-SNN (She et al., 2021)	42.8	41.9	37.0	34.6	1.7M
This Work-STDP Training	58.1	57.8	57.2	54.6	1.4M
This Work-BPTT Training	71.2	65.4	56.0	52.5	1.7M
number of evaluations as the proposed design. Similarly, to study the contribution to performance
gain from skip-layer connections, the Bayesian optimization process is used for network templates
without skip-layer connections. The optimization process runs for the same number of evaluations
as the proposed design. From the results shown in Table 2, it can be observed that compared to
baselines, the proposed networks achieve the best accuracy for all datasets. Specifically, when ho-
mogeneous network is used, the performance of STDP trained network is noticeably lower than
the proposed method for DVS Gesture and N-Caltech101. For BPTT training, using heterogeneous
network and skip-layer connection show different level of benefit for each dataset. For sequential
MNIST which has less complexity, the improvement from using heterogeneous neurons and skip-
layer connections is not as significant.
5.4	Comparison with Prior Works
DVS Gesture As shown in Table 3 (top), with 100% labels, the proposed network trained with BPTT
demonstrates higher accuracy than all tested networks without using the most trainable parameters.
The proposed network trained with STDP has slightly lower accuracy than some baselines when
100% labels are used; for reduced-label training it outperforms all other networks.
N-Caltech101 As shown in Table 3 (bottom), the proposed network trained with BPTT outperforms
all baselines with both 70% and 100% training labels and also has less trainable parameters than
most baselines. The un-supervised learning models i.e., H-SNN and the proposed network with
STDP, show considerably lower performance (more than what was observed for DVS Gesture) than
supervised ones when 100% labels are available; However, the proposed network with STDP shows
better performance than H-SNN, and outperforms all networks when available labels are below 50%.
6	Conclusion
We develop a theoretical basis to understand and optimize the ability of a feedforward SNN to
approximate temporal sequence. We analytically show how heterogeneity and skip-layer connec-
tions can improve sequence approximation with SNN, and empirically demonstrate their impact on
spatiotemporal learning. It is well-known in neuroscience that, heterogeneity (De Kloet & Reul,
1987) and irregular connectivity (Eickhoff et al., 2018) are intrinsic properties of human brains. Our
analysis shows that incorporating such concepts within artificial SNN is beneficial for designing
high-performance SNN for classification of spatiotemporal data.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This material is based on work sponsored by the Army Research Office and was accomplished
under Grant Number W911NF-19-1-0447. The views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the official policies, either
expressed or implied, of the Army Research Office or the U.S. Government.
Authors would like to thank Drs. Mark Mclean, Chris Purdy, Fernando Camacho, and James Keiser
from Laboratory for Physical Sciences for many helpful discussions on this work.
References
Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo,
Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, et al. A low
power, fully event-based gesture recognition system. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 7243-7252, 2017.
Y. Amit and Yibi Huang. Precise capacity analysis in binary networks with multiple coding level
inputs. Neural Computation, 22:660-688, 2010.
David G Barrett, Sophie Deneve, and Christian K Machens. Firing rate predictions in optimal bal-
anced networks. In Advances in Neural Information Processing Systems, pp. 1538-1546. Citeseer,
2013.
Jonathan Binas, Giacomo Indiveri, and Michael Pfeiffer. Spiking analog vlsi neuron assemblies as
constraint satisfaction problem solvers. In 2016 IEEE International Symposium on Circuits and
Systems (ISCAS), pp. 2094-2097, 2016. doi: 10.1109/ISCAS.2016.7538992.
Johanni Brea, Walter Senn, and Jean-Pascal Pfister. Matching recall and storage in sequence learning
with spiking neural networks. Journal of neuroscience, 33(23):9565-9575, 2013.
Chi-Ning Chou, Kai-Min Chung, and Chi-Jen Lu. On the algorithmic power of spiking neural
networks. arXiv preprint arXiv:1803.10375, 2018.
ER De Kloet and JMHM Reul. Feedback action and tonic influence of corticosteroids on brain
function: a concept arising from the heterogeneity of brain receptor systems. Psychoneuroen-
docrinology, 12(2):83-105, 1987.
Simon B Eickhoff, BT Thomas Yeo, and Sarah Genon. Imaging-based parcellations of the human
brain. Nature Reviews Neuroscience, 19(11):672-686, 2018.
Wei Fang, Zhaofei Yu, Yanqi Chen, Timothee Masquelier, Tiejun Huang, and Yonghong Tian. In-
corporating learnable membrane time constant to enhance learning of spiking neural networks.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2661-2671,
2021.
Edris Zaman Farsa, Soheila Nazari, and Morteza Gholami. Function approximation by hardware
spiking neural network. Journal of Computational Electronics, 14(3):707-716, 2015.
Jacob R Gardner, Matt J Kusner, Zhixiang Eddie Xu, Kilian Q Weinberger, and John P Cunningham.
Bayesian optimization with inequality constraints. In ICML, volume 2014, pp. 937-945, 2014.
Wulfram Gerstner. Time structure of the activity in neural network models. Physical review E, 51
(1):738, 1995.
Wulfram Gerstner and Werner M Kistler. Mathematical formulations of hebbian learning. Biological
cybernetics, 87(5-6):404-415, 2002.
Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. Rmp-snn: Residual membrane potential
neuron for enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
10
Published as a conference paper at ICLR 2022
Nicolangelo Iannella and Andrew D. Back. A spiking neural network architecture for nonlin-
ear function approximation. Neural Networks, 14(6):933-939, 2001. ISSN 0893-6080. doi:
https://doi.org/10.1016/S0893-6080(01)00080-6. URL https://www.sciencedirect.
com/science/article/pii/S0893608001000806.
Jacques Kaiser, Hesham Mostafa, and Emre Neftci. Synaptic plasticity dynamics for deep con-
tinuous local learning (decolle). Frontiers in Neuroscience, 14:424, 2020. ISSN 1662-453X.
doi: 10.3389/fnins.2020.00424. URL https://www.frontiersin.org/article/10.
3389/fnins.2020.00424.
Mina A Khoei, Amirreza Yousefzadeh, Arash Pourtaherian, Orlando Moreira, and Jonathan Tapson.
Sparnet: Sparse asynchronous neural network execution for energy efficient inference. In 2020
2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS), pp.
256-260. IEEE, 2020.
Seijoon Kim, Seongsik Park, Byunggook Na, Jongwan Kim, and Sungroh Yoon. Towards fast and
accurate object detection in bio-inspired spiking neural networks through bayesian optimization.
IEEE Access, 9:2633-2643, 2020a.
Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon. Spiking-yolo: Spiking neural
network for energy-efficient object detection. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 11270-11277, 2020b.
Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks us-
ing backpropagation. Frontiers in Neuroscience, 10:508, 2016. ISSN 1662-453X. doi: 10.
3389/fnins.2016.00508. URL https://www.frontiersin.org/article/10.3389/
fnins.2016.00508.
Wolfgang Maass. Fast sigmoidal networks via spiking neurons. Neural computation, 9(2):279-304,
1997.
Garrick Orchard, Ajinkya Jayawant, Gregory K. Cohen, and Nitish Thakor. Converting static image
datasets to spiking neuromorphic datasets using saccades. Frontiers in Neuroscience, 9:437, 2015.
ISSN 1662-453X. doi: 10.3389/fnins.2015.00437. URL https://www.frontiersin.
org/article/10.3389/fnins.2015.00437.
Maryam Parsa, J Parker Mitchell, Catherine D Schuman, Robert M Patton, Thomas E Potok, and
Kaushik Roy. Bayesian-based hyperparameter optimization for spiking neuromorphic systems.
In 2019 IEEE International Conference on Big Data (Big Data), pp. 4472-4478. IEEE, 2019.
Filip Ponulak and Andrzej Kasinski. Introduction to spiking neural networks: Information process-
ing, learning and applications. Acta neurobiologiae experimentalis, 71(4):409-433, 2011.
Ali Safa, Federico Corradi, Lars Keuninckx, Ilja Ocket, Andre Bourdoux, Francky Catthoor, and
Georges GE Gielen. Improving the accuracy of spiking neural networks for radar gesture recog-
nition through preprocessing. IEEE Transactions on Neural Networks and Learning Systems,
2021.
Darjan Salaj, Anand Subramoney, Ceca Kraisnikovic, Guillaume Bellec, Robert Legenstein, and
Wolfgang Maass. Spike-frequency adaptation provides a long short-term memory to networks
of spiking neurons. bioRxiv, 2020. doi: 10.1101/2020.05.11.081513. URL https://www.
biorxiv.org/content/early/2020/05/12/2020.05.11.081513.
Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking
neural networks: Vgg and residual architectures. Frontiers in neuroscience, 13:95, 2019.
Xueyuan She, Saurabh Dash, Daehyun Kim, and Saibal Mukhopadhyay. A heterogeneous spiking
neural network for unsupervised learning of spatiotemporal patterns. Frontiers in Neuroscience,
14:1406, 2021.
A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman. Hats: Histograms of averaged
time surfaces for robust event-based object classification. In 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 1731-1740, 2018. doi: 10.1109/CVPR.2018.
00186.
11
Published as a conference paper at ICLR 2022
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. Advances in neural information processing systems, 25, 2012.
Gopalakrishnan Srinivasan and Kaushik Roy. Restocnet: Residual stochastic binary convolutional
spiking neural network for memory-efficient neuromorphic computing. Frontiers in neuroscience,
13:189, 2019.
Hiroyuki Torikai, Atsuo Funew, and Toshimichi Saito. Digital spiking neuron and its learning for
approximation of various spike-trains. Neural Networks, 21(2):140-149, 2008. ISSN 0893-6080.
doi: https://doi.org/10.1016/j.neunet.2007.12.045. URL https://www.sciencedirect.
com/science/article/pii/S0893608008000051. Advances in Neural Networks Re-
search: IJCNN ’07.
Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE, 78(10):1550-1560, 1990.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking neural
networks: Faster, larger, better. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 1311-1318, 2019.
Shao-Qun Zhang, Zhao-Yu Zhang, and Zhi-Hua Zhou. Bifurcation spiking neural network. arXiv
preprint arXiv:1909.08341, 2019.
12
Published as a conference paper at ICLR 2022
A SNN Dynamics
Remark For a sequentially connected neuron list with m neurons all with γ = 1 and neuron delay
tnd , an input spike at time t leads the neuron list to generate an output spike at time t + mtnd
Remark For any input sequence with period tin to a spiking neuron with response rate γ such that
γ > 1, if refractory period is set to r < tin, the neuron can exit refractory period before the next
spike arrives.
Lemma 1 For any input spike sequence with period tin in range [Tmin , Tmax], there exists a spiking
neuron n with fixed parameters vth , vreset, a, Rm and τm, such that by changing synaptic conduc-
tance G, it is possible to set the neuron response rate γn to be any positive integer.
Proof. For a given input spike sequence period tin , consider the maximum possible membrane
potential decay that can be reached within a period of tin. From (1), When I = 0,篝 < 0 and | d |
increases with higher v. Hence, the maximum decay ofv is reached when initial membrane potential
v(t = 0) → vt-h and the neuron decays for period tin = Tmax. The decayed membrane potential
v(t = Tmax) can be derived by solving the differential equation (1) for v(t) = vth att = 0:
v(t = Tmax)= Vthe- TmX - ae-TmX + a	(5)
It is possible to have a spiking neuron with Rm, a and τm such that ∆v, defined as
TmaX	TmaX
∆v = v(t = Tmax) - v(t = 0)= Vthe	Tm - ae Tm + a - Vth	(6)
tends to zero. With this configuration, since the the highest possible decay of membrane potential is
negligible, for any target γ, it is possible to set G such that
G = Vth - Vreset
(7)
γ
The proof is complete.
Note, input sequence that has no spike has tin tending to infinity, thus violating the bounded con-
straint mentioned in Lemma 1.
B	Proof of Theorem 1
Theorem 1 For any input and target output spike sequence pair with periods (tin, tout) ∈
[Tmin, Tmax] × [Tmin, Tmax], there exists a minimal-layer-size network with skip-layer connections
that has memory pathway with output spike period function P(tin) such that |P(tin) - tout| < .
Proof. For any given (tin, tout), first consider the condition where tin > tout. It is possible to
construct a minimal-layer-size network N connecting m spiking neurons with neuron response rate
γ = 1 sequentially, denoted as a m-tuple of neurons {n1, n2, ..., nm}. Since any configuration of
skip-layer connection with source layer and target layer pair (ls , lt), such that ls ∈ [1, m - 2], and
lt ∈ [ls + 2, m], can be added, it is possible to add a (m - 2)-tuple of skip-layer connections
Ssl = {(i, m) ∀ i∈ {1,2,3,...,m-2}}	(8)
Denote the synaptic conductance for all the skip-layer connections as a (m - 2)-tuple
sl sl sl	sl
SGsl = {G1 , G2 , G3 , ..., Gm-2}	(9)
For any tout < tin , it is possible to find a k-tuple of synaptic conductance
SGSl = {Gsl,G2i, G3i,…,Gki} s.t. i = btoutJ and k = [m-2J	(10)
tnd	i
13
Published as a conference paper at ICLR 2022
Set synaptic conductance in SGsl \SG0 sl to 0. Then set the conductance of synapse connecting nm-1
and nm to 0. In such way, The output spikes from network N have period
P (tin ) = b Tout C ∙ tnd
tnd
(11)
For given , it is possible to choose tnd such that tnd < 2, therefore satisfying |P(tin) - tout| < .
m can be chosen as
m
Tmax - Tmin
tnd
or equivalently:
m
Tmax - Tmin
(12)
(13)
2
Since Tmax-Tmin is finite, m is finite.
For tin < tout, using N as described above, it is possible to achieve output spike with period within
of any period in (0, tin]. For a given tout, assume the configuration in neuron list N has output
spike interval t0int such that kt0int = tout, where k is a positive integer. From Lemma 1, it is possible
to set G for a neuron nm+1 such that its neuron response delay satisfies γnm+1 = k for input spike
period t0int. A new network, denoted as N0, can be formed by connecting nm+1 to the output of
N. N0 has output spike with period P(tin) = kt0int = tout. Hence, to reach the given , it requires
neuron list N to have output spike interval tint such that
|tint - tint1 < k	(14)
Since k is finite, (14) can be achieved.
For tin >= tout, it is possible to configure network N0 such that tint satisfies |tint - tout| < , and
the value of γnm+1 set to 1, hence |P(tin) - tout| < can be achieved. The proof is complete.
C Proof of Lemma 2
Lemma 2 With no skip-layer connection, there does not exist a minimal-layer-size network that has
output spike period function P(tin) such that for any input and target output spike sequence pair
with periods (tin, tout) ∈ [Tmin, Tmax] × [Tmin , Tmax], |P (tin ) - tout | < .
Proof. A minimal-layer-size network N with m layers can be denoted as a m-tuple of neurons
{n1 , n2, ..., nm} connected sequentially. Since no skip-layer connection exists, there is only one
distinct memory pathway that contains all neurons {n1 , n2, ..., nm}.
Denote the set of neuron response rate corresponding to each neuron in N as
Γ = {γ1,γ2, ...,γm}	(15)
For a given input sequence with tin , denote the timing of the first spike as ti1n, consider the output
spike sequence for network with
γi = 1 ∀ γi ∈ Γ	(16)
The first output spike from N has timing tout = tin + mtnd, and the second output spike has timing
tout = tin + tin + mtnd. It can be easily derived that the period of the output spike sequence is
P(tin) = tin
(17)
14
Published as a conference paper at ICLR 2022
Also consider the output spike sequence for network with
γj =2 for any j ∈ {1,2,3,4,...,m} and γi = 1 ∀ i ∈ ({1,2,3,4,...,m}\{j})	(18)
Following the same process, the period of the output spike sequence is
P(tin)
(19)
Since the smallest increase to any γi is by 1, there is no set of values for γ such that the network
output spike sequence has period P (tin) satisfying tin < P (tin) < 2tin. Since within the range
(tin, 2tin), there exists values of tout such that |P (tin) - tout | < does not hold. The proof is
complete.
D Memory Pathways in SNN
In this section we analyze the increase to the least upper bound of the number of memory pathways
in a network by using heterogeneous networks and skip-layer connections.
____VreSet -a__)
Vreset-a+ Rm G
Lemma 3 A spiking neuron has cutoff period ωc = τm ln(
sequence cannot cause the spiking neuron to spike.
above which input spike
Proof. Consider (2), since membrane potential increases at time ti and decays otherwise, solving for
t = ti and the equation can be expanded:
,ti	,	ti 、	Rrrl	ti-t1	Rrrl	ti-t2	Rrrl
vm(ti) = VreseteTm + a(1 - eτm) + — Ge+ — Ge+ ... + —mG	(20)
τm	τm	τm
For input With frequency f, ti+1 - ti = ∆t = ɪ, subtracting membrane potential values at two
consecutive ti provides:
.	.	ti+ι	ti	ti+1	ti	Rm
∆vm = Vm(ti+1) - Vm(ti) = Vreset(e^m - eTm) - a(e^m - eTm) + -^Ge
ti+1-t1
τm
(21)
setting time of the first input spike t1 to zero leads to:
△vm = eTm ((eTm — 1)(Vreset — a) +------^GeTm )	(22)
τm
ti	∆t	R	∆t	.
AS eTm > 0, and the term ((eTm )-1)(Vreset-a) + RmGeTm) does not depend on ti, the polarityof
τm
∆Vm does not change with time. Vm is either strictly increasing, staying the same or decreasing with
higher ti . This indicates that, when ∆Vm ≤ 0 the post-synaptic neuron can never spike regardless
of how many pre-synaptic spikes it receives. ∆Vm ≤ 0 when input spike period tin satisfies
tin ≥ τm ln(
Vreset - a	)
Vreset - a + RmG
(23)
Therefore, the cutoff period of the neuron is
ωc = τm ln(
Vreset - a
Vreset - a + RmG
(24)
)
The proof is complete.
15
Published as a conference paper at ICLR 2022
In the following proof, We consider cutoff frequency, f = -1 of spiking neurons. From (24) it
ωc
can be easily derived that fc can be configured to any positive real number by changing the neuron
parameters.
Lemma 4 For an mMND network with m layers and {λ1, λ2, ...λm} number of different neuron
dynamics in each layer, the least upper bound of the number of memory pathways is Qim=1 λi .
Proof. Denote the set of neurons in layer l with distinct neuron dynamics as
Sn = {n1, n2 , n3, ..., nλl }	(25)
Since the network is mMND, |Snl | = λl. Denote the set of cutoff frequency corresponding to each
neuron in Snl as
Fcl = {fcnl1, fcnl2, fcnl3,..., fcnlλl}	(26)
Since neurons in Snl can have different neuron parameters, from Lemma 3, it is possible to set the
parameters such that all entries of Fcl are distinct. Hence, there exists a permutation π such that
fcnlπ(1) < fcnlπ(2) < fcnlπ(3) ... < fcnlπ(λl)	(27)
Denote the input spike frequency to layer l as filn, neuron nli is a part of a valid memory pathway in
the network if
fcnli ≤ filn	(28)
Therefore, for input spike frequency filn ≥ fcnlπ(λl)
, all neurons in Snl can be part of a valid memory
nl	nl
pathway of the network. For input spike frequency such that fc π(i) ≤ filn < fc π(i+1), i neurons can
be part of a valid memory pathway of the network.
For any input to the network fin ∈ [Fmin , Fmax], denote the number of ways different neurons in
Snl can be part of valid memory pathways as kl. The total number of distinct memory pathways K
in the network is the product of kl of all layers:
m
K=Ykl	(29)
l=1
Since for all layers, 0 ≤ kl ≤ λl,
m
K≤Yλl	(30)
l=1
Consider that, for any layer l ∈ {1, 2, 3, ..., m}, the input filn is bounded by [Fml in, Fml ax]. kl = λl
can be achieved by setting fcnπ(1) and fcnπ(λl) such that:
fcnlπ(1) = Fml in and fcnlπ(λl) = Fml ax	(31)
Hence the bound is tight for (30). The proof is complete.
Lemma 5 For an mMND network with m layers and {λ1, λ2, ...λm} different neuron dynamics in
each layer and a skip-layer connection made between layer la and lb, s.t. a, b ∈ {1, 2, ...m} and (b-
a) > 1, the least upper bound of the number of memory pathways is Qim=1 λi + (Qia=1λi ∙ Πm=b λi)
16
Published as a conference paper at ICLR 2022
Proof. Denote the mMND network with skip-layer connection between layer la and layer lb as P .
Denote the set of all neurons in P as
SP = {n11,n12,n31, ...,n21,n22,n32, ...}	(32)
where ni1 is a neuron in layer l1 , and ni2 is a neuron in layer l2 , etc. The activation state onj of a
neuron can be denoted with binary values 0 and 1 with onj = 1 representing that nij receives input
ni
frequency that is above its cutoff frequency fcni .
The set of all possible neuron activation states O in P that generates non-zero network output feature
vector can be partitioned into two subsets denoted as OA and OB .
Set OA contains all states where the input frequency fikn to any layer lk such that a < k < b satisfies
fikn < fcnik ∀i∈ {1, 2, 3,..., λk}	(33)
Set OB contains all the remaining neuron activation states in O, where all layers receive input fre-
quency higher than cutoff frequency of at least one neuron in that layer.
For all the states in OA, no spike signal is sent from layer b - 1 to layer b, since at least one layer
between la and lb generates no output. Hence, output from P is not affected if connections between
layer li and li+1, such that i ∈ {a, a + 1, ..., b - 1}, are removed. Network P is therefore equivalent
to network P0 that has layers {l1, l2, ..., la, lb, lb+1, ..., lm} connected sequentially.
According to Lemma 4, it can be derived that the least upper bound of the number of distinct memory
pathways in P0 is Qa=I λi ∙ QmLb λi.
Hence, for all states in set OA, the least upper bound of the number of distinct memory pathways in
P is also Qa=ι λ% ∙ Qm=b λi. For all states in set OB, since the activation of neurons in the source
layer of the skip-layer connection is already accounted for when considering layer la, the least upper
bound of the number of distinct memory pathways is the same as network P that has no skip-layer
connection, which is Qim=1 λi according to Lemma 4.
For the set of memory pathways from states in OA, denoted as MA, and the set of memory pathways
from states in OB, denoted as MB, the number of memory pathways of network P is |MA ∪ MB |.
From Lemma 5, the bound is tight for |Ma | ≤ Qa=I λi ∙ Qm=b λi and for |Mb | ≤ Qm=I λ%. It also
satisfies that MA ∩ MB = 0 since all elements in MA have (m - (b - a -1)) layers and 疝 elements
in MB have m layers. Hence the bound is tight for
m	am
|Ma ∪ MB | ≤ Y λi + (Y λi ∙ Y λi)	(34)
The proof is complete.
E	Time-varying Function Approximation
A time-varying function f(t) can be approximated using piece-wise constant function, such as illus-
trated in Figure 4. It is therefore possible to approximate the time-varying function with a feedfor-
ward SNN by approximating each of the constant function with a memory pathway. In this section,
we test the approximation capability of feedforward SNN for time-varying functions using this prin-
ciple. The target functions to approximate have the form of:
f(x)
xn
x-m
(35)
Here, x is variable; n and m are function parameters. For discrete-time simulation of the network,
we approximate the target function with
x = tin and f (x) = tout
(36)
17
Published as a conference paper at ICLR 2022
60
50
40
f(t) 3。
20
10
0
0	2
6	8	10	12	14	16	18	20
t
Figure 4: Using a set of memory pathways to map a piece-wise constant function for approximating
a time-varying function.
where tin is the input spike period and tout is the output spike period. We test approximation
performance of a small-scale feedforward SNN with 6 fully-connected layers, skip-layer connec-
tions {(2, 5), (3, 5)} and 4 neuron dynamics. The network is trained with BPTT to minimize mean
squared error (MSE) loss between the spike period of network output t0out and target spike period
tout.
To evaluate the network’s approximation performance, we construct 6 networks with the same struc-
ture as discussed above, and change each to have different trainable parameter numbers by scaling
the layer size. For baseline comparison, networks with 6 layers, no skip-layer connection and using
homogeneous neurons, configured to have the same trainable parameter numbers as the proposed
networks, are tested. All networks are trained with BPTT. The loss function measures the difference
between the network output spike trains and the target spike trains with MSE. Two sets of function
parameters: (m = 1, n = 3.3) and (m = 2, n = 2.1) are tested for the target functions f(x)
on domain [3,10]. The resulting MSE losses for different network scales are shown at the bottom
of Figure 5. It can be observed that the proposed networks can approximate target functions with
less error than the baseline networks at all network scales. The smallest tested networks have rela-
tively high losses while performance increases quickly with more trainable parameters. The rate of
performance improvement decreases when trainable parameter number is above 4000.
To understand the impact of the target function parameters to approximation performance of SNN,
we test baseline and the proposed network with 4167 trainable parameters for different pairs of
Figure 5: MSE loss vs. number of trainable parameters for the function approximation experiments.
18
Published as a conference paper at ICLR 2022
-—4.0
(a)
Baseline Network
-2.0
-1.6
-1.2
-0.8
-0.4
0.4
0.8
1.2
1.6
2.0
m 0
1.8e-1
0.7e-1
3.4e-3
1.8e-3
n
Figure 6: (a) MSE loss (log) of baseline network (top) and the proposed network (bottom) for
approximating functions with different parameters m and n. (b) Heat plots of MSE loss for approx-
imating functions with different parameters m and n.
(b)
function parameters m and n. The resulting MSE loss is shown with a heat map in Figure 6. It
can be observed that for all m and n value pairs, the proposed network achieves lower loss than
the baseline network. Another observation is that, there is no clear correlation between the value
of m and approximation error. On the other hand, for higher values of n, the approximation error
generally increases for both baseline and the proposed network.
F	Network Spiking Activity
To investigate neuron spiking activity in the proposed network, for the function approximation task
as described in Appendix E, with parameters (m = 1, n = 2.3) and a randomly selected approxi-
mation point f(x = 19), we plot the timing of neuron spikes at the last layer over training epochs in
Figure 7 (a). It can be observed that the network initially generates spikes at widely distributed tim-
ings. After the first 50 training epochs spike timing starts to converge and remains stable at around
200 epochs. The final output spike period is t0out = 48, which matches the target output period.
Next, we investigate spiking activity of the BPTT trained network as described in Section 5.4 for
N-Caltech101 classification task. Three different test data points are presented to the network, and
the spikes from neurons in the first depth of layer 8 and neurons in the final layer, are recorded and
shown in Figure 7 (b). Here, (i), (iii) and (v) are from layer 8; (ii), (iv) and (vi) are from the final
layer. (i) and (ii) are from the observation of a data point with class label “5”; (iii) and (iv) are from
the observation of another data point also with class label “5”; (iii) and (iv) are from the observation
of a data point with class label “1”.
It can be observed that neurons in layer 8 exhibit similar activity in (i) and (iii) as the network is
presented with data points from the same class. (ii) and (iv) show that most spiking activities are
19
Published as a conference paper at ICLR 2022
(a)
120
Oo
80
60t
40
20
120
Oo
80
60t
‹J
20
Figure 7: (a) Raster plot for function approximation experiment; t is the simulation time step. (b)
Raster plots for the proposed SNN trained for N-Caltech101 with BPTT; t is the simulation time
step; (i), (iii) and (V) are from layer 8; (ii), (iv) and (vi) are from the final layer.
20
Published as a conference paper at ICLR 2022
from the neuron with index 5, indicating correct prediction for the two data points. When a data
point from a different class is presented, spiking activity in (v) shows different patterns than those
in (i) and (iii). In the final layer, neuron with index 1 generates the most spikes, leading to a correct
prediction. However, for this data point, neuron with index 58 is also generating a considerable
number of spikes, indicating that the network is more likely to mis-classify this data point, compared
to the other two tested data points.
G Implementation of Heterogeneous Conv-SNN
Multi-neuron-dynamic (MND) networks with convolutional layers can be considered a scaled-up
version of the mMND network, where each neuron dynamic now contains a matrix of neurons that
receive input features from different spatial locations. To implement heterogeneous Conv-SNN, first
consider a regular Conv-SNN layer with no heterogeneity. The spiking neuron matrix has dimension
{W, H, D}, where D is the depth of the layer. The convolution filter has dimension {C, w, h, D}
where C is the number of input channels and D is the number of output channels.
Based on this, a heterogeneous layer l can be constructed, by concatenating heterogeneous neu-
ron matrices with the same W and H along layer depth. The resulting spiking neuron matrix
has dimension {W, H, λD}, where λ is the number of neuron dynamics. Hence, layer depth
i ∈ {k + 1, k + 2, k + 3, ..., k + D} have the same neuron parameters, where 0 ≤ k ≤ λ - 1 is an
integer representing the index of neuron dynamics. The convolution filter has the same dimension as
the regular Conv-SNN layer: {C, w, h, D}, which is shared by layer depth with different k values.
During forward pass of this layer, a convolution operation is applied to generate an input signal ma-
trix with dimension {W, H, D}. For each value of k, neurons in depth {k+ 1, k+2, k+3, ..., k+d}
are simulated based on the neuron parameters for such neuron dynamic index k using the signal ma-
trix as input. For the next convolutional layer l0 receiving input from layer l, the filter dimension is
{λD, w0 , h0 , D0 } with λD input channels.
H Details on the Bayesian Optimization Process
During the first stage of the dual-search-space optimization process, the parameters to optimize
include: Nlayer, Lstart, Lend, Nskip, Ndynamic, all of which are positive integers. Specifically,
Nlayer is the number of convolutional layers. For skip-layer connection, there are three configura-
tion parameters to optimize: starting layer Lstart , which is the source layer of the first skip-layer
connection; ending layer Lend, which is the target layer of the last skip-layer connection; skip-layer
connection number Nskip, which defines how many connections to implement. The source layer of
the Nskip skip-layer connections are placed evenly between Lstart and Lend, each with skip length
equal to
[Lend - Lstart
Nskip
c
In the case of
Lend -
Lstart
Nskip
6= b
Lend - Lstart
Nskip
c
the value of Lend is reduced to the maximum value that satisfies
Lend -
Lstart
Nskip
b Lend - Lstart
Nskip
c
For heterogeneity, the number of different dynamic Ndynamic in all layers is optimized jointly. The
constraints for the parameters are defined as:
Nlayer ∈ [4, 15]
21
Published as a conference paper at ICLR 2022
and
Figure 8: Visualization of the learned distribution from Bayesian optimization: N-Caltech101 vali-
dation accuracy vs. skip-layer configurations.
(兴)Aue-nu< UO-4->EP--Q>
2 ≤ Lstart < (Nlayer - 1)
(Lstart + 1) < Lend ≤ Nlayer
0 ≤ Nskip ≤ (Lend - Lstart)/2
Ndy
namic
∈ [1, 10]
The manually configured neuron parameters are, τm = 100 and Rm = 300 for all neuron dynamics,
and a ∈ [-30, -5] is distributed evenly for each neuron dynamic. Figure 8 shows visualization
of the learned distribution from the Bayesian optimization process. Here, mapping from the search
space of skip-layer connection configurations to the validation accuracy of N-Caltech101 is shown.
The highest validation accuracy is reached at Lstart = 2 and Lend = 11.
Due to the exponential increase of search space with the number of neuron dynamics in each layer,
it is highly inefficient to search for every neuron parameters of each dynamic. In the second stage
of the optimization process, we choose to apply Bayesian optimization for the parameter a of each
neuron dynamic separately, while τm and Rm are optimized jointly with the same values shared by
all neuron dynamics. a values are taken to the precision of 100, and τm and Rm values are taken to
the precision of 101. The constraints are a ∈ [-30, -5], τm ∈ [50, 200] and Rm ∈ [200, 400]. The
value of tnd for all networks is set to 1. The parameters of each optimized network are shown in
Table 1. Note, the skip-layer connections are listed as source and target layer pairs.
22