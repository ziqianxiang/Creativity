Published as a conference paper at ICLR 2022
Chaos is a Ladder: A New Theoretical Un-
derstanding of Contrastive Learning via Aug-
mentation Overlap
YifeiWang1* QiZhang2* YisenWang3,4* JianshengYangI	ZhouChenLin3,4,5
1	School of Mathematical Sciences, Peking University
2	School of Computer Science and Engineering, Sun Yat-sen University
3	Key Lab. of Machine Perception (MoE), School of Artificial Intelligence, Peking University
4	Institute for Artificial Intelligence, Peking University
5	Pazhou Lab, Guangzhou, 510330, China
Ab stract
Recently, contrastive learning has risen to be a promising approach for large-scale
self-supervised learning. However, theoretical understanding of how it works is
still unclear. In this paper, we propose a new guarantee on the downstream per-
formance without resorting to the conditional independence assumption that is
widely adopted in previous work but hardly holds in practice. Our new theory
hinges on the insight that the support of different intra-class samples will become
more overlapped under aggressive data augmentations, thus simply aligning the
positive samples (augmented views of the same sample) could make contrastive
learning cluster intra-class samples together. Based on this augmentation over-
lap perspective, theoretically, we obtain asymptotically closed bounds for down-
stream performance under weaker assumptions, and empirically, we propose an
unsupervised model selection metric ARC that aligns well with downstream ac-
curacy. Our theory suggests an alternative understanding of contrastive learning:
the role of aligning positive samples is more like a surrogate task than an ultimate
goal, and the overlapped augmented views (i.e., the chaos) create a ladder for con-
trastive learning to gradually learn class-separated representations. The code for
computing ARC is available at https://github.com/zhangq327/ARC.
1	Introduction
Contrastive Learning (CL) emerges to be a promising paradigm for learning data representations
without labeled data (Oord et al., 2018; Hjelm et al., 2019). Recently, it has achieved impressive re-
sults and gradually closed the gap between supervised and unsupervised learning, hopefully leading
to a new era that resolves the hunger for labeled data in the deep learning field (He et al., 2020; Chen
et al., 2020b; Wang et al., 2021). However, despite its intriguing empirical success, a theoretical
understanding of how contrastive learning actually works in practice is still under-explored.
The general methodology of contrastive learning is quite simple, that is to maximize the similarity
between augmented views of the same image (a.k.a. positive samples), and minimize the similarity
between that of two random images (a.k.a. negative samples). Intuitively, it is an instance discrim-
ination task (differing each image from others) instead of a classification task (clustering images
from the same class together and differing with other classes). Nevertheless, as shown in Figure
1(a), CL representations are also class-separated. Therefore, understanding how the pretraining
task (CL) and the downstream task (classification) interact plays a central role in both theoretical
understandings and practical designings of contrastive methods.
Previously, Saunshi et al. (2019) and Lee et al. (2020) have tried to establish guarantees on the clas-
sification performance for self-supervised representations. However, their analysis relies heavily on
the assumption that the two positive samples, as augmented views of the same image, are (nearly)
conditionally independent on the class y . However, this is hardly practical as the augmented views
are still strongly input-dependent (see Figure 1(b)). In fact, if the conditional independence is satis-
fied, the unsupervised task will become as informative as the supervised task, making this discussion
* Equal contribution. Qi Zhang's work was done during an internship at Peking University.
,Corresponding author: YiSenWang (yisen.wang@pku.edu.cn).
1
Published as a conference paper at ICLR 2022
(a) Contrastive learning learns clustered features.
Figure 1: (a) t-SNE visualization of representations before and after contrastive learning. Each point
denotes a sample and its color denotes its class. (b) Applying aggressive data augmentations (Chen
et al., 2020a) to four images from ImageNet (two are cars and two are pens). The 1st column shows
the raw (center-cropped) images and the 2-5th colums show the augmented ones.
(b) Intra-class samples are more
alike via augmented views.
almost unnecessary. This motivates us to find more practical and weaker assumptions to understand
how contrastive learning actually works (even without conditional independence). To achieve this,
we need to re-examine the contrastive learning process. Previously, Wang & Isola (2020) show
that CL objective involves two goals: alignment (for positive samples) and uniformity (for negative
samples). Nevertheless, we show that there exist bad cases where the features could still have poor
performance even with perfect alignment and uniformity. Thus, contradictory to the common belief
of contrastive learning as learning invariance, we note that invariance alone is inadequate to learning
useful representations for downstream tasks.
In this paper, we provide a novel understanding of contrastive learning that requires only practical
and minimal assumptions, while also guarantee class-separated representations. Our core insight
hinges on the observation that contrastive learning usually adopts much more aggressive data aug-
mentations than that in supervised learning (He et al., 2020; Chen et al., 2020a). As shown in Figure
1(b), we notice that aggressive random cropping of two images can generate views that are very
much alike that we could even hardly tell them apart, e.g., the wheels of two different cars. In
other words, there will be support overlap between different intra-class images through aggressively
augmented views of them, a phenomenon we call augmentation overlap. Thus, the alignment of
positive samples will also cluster all the intra-class samples together, and lead to class-separated
representations. From our perspective, the role of data augmentation is to create a certain degree
of “chaos” between intra-class samples, and the role of contrastive loss is to “climb the ladder of
chaos”, i.e., the process that we gradually cluster intra-class samples by aligning positive samples.
Following this intuition, we develop a new theory for understanding the effectiveness of contrastive
learning from the perspective of augmentation overlap. Specifically, we derive the upper and lower
bounds for its downstream performance and show how the two bounds will asymptotically converge
with our assumptions on augmentation overlap. Driven by this analysis, we further discuss how
varying augmentation will affect the performance of contrastive learning from both synthetic and
real-world datasets, and show that the results align well with our theory. In summary,
•	We characterize the failure of the previous analysis of contrastive learning, and develop a
new understanding through the augmentation overlap effect. Compared to existing theories
on contrastive learning, ours can provide guidance to the practical designing of contrastive
methods and evaluation metrics.
•	We establish general guarantees (both upper and lower bounds) for the downstream perfor-
mance without assumptions on conditional independence. And we further show how the
two bounds could asymptotically converge under our less restrictive assumptions.
•	We provide a quantitative discussion on the effect of augmentation strength, which verifies
our theory from both theoretical and empirical aspects. Motivated by our theory, we further
propose a new unsupervised evaluation metric for contrastive learning named ARC and
show that it aligns well with downstream performance on real-world datasets.
2	Related Work
Contrastive Learning in Practice. Contrastive self-supervised learning originates from a mutual
information perspective of representation learning (Oord et al., 2018; Hjelm et al., 2019), and soon
2
Published as a conference paper at ICLR 2022
becomes a general learning paradigm that contrasts between positive and negative pairs (He et al.,
2020; Chen et al., 2020a). It is rapidly closing the performance gap between unsupervised and
supervised learning on large-scale dataset like ImageNet (Chen et al., 2021), and outperforms su-
pervised learning when combined with a few (e.g., 10%) labels (Chen et al., 2020b). Several recent
works show that similar performance could be achieved without negative samples by adopting cer-
tain training techniques (Grill et al., 2020; Chen & He, 2020).
Understanding Contrastive Learning Objectives. Both the original InfoNCE loss (Oord et al.,
2018) and its InfoMax variants (Hjelm et al., 2019; Poole et al., 2019) are designed as variational
estimates of the mutual information between inputs and representations, but these estimators are
shown to have poor bias-variance trade-offs (Song & Ermon, 2020). Instead, Wang & Isola (2020)
simply understand contrastive learning through the two terms in the InfoNCE loss: alignment of
positive samples and uniformity of negative samples. However, as we show later, this perspective is
also insufficient to explain the effectiveness of contrastive learning, and we should take the interplay
between augmentation and alignment into consideration.
Understanding Downstream Generalization. Saunshi et al. (2019) propose the first theoretical
guarantees by bridging the contrastive and classification objectives. Lee et al. (2020) further link the
reconstruction-based objective to the downstream objective. However, both Saunshi et al. (2019)
and Lee et al. (2020) rely on the unrealistic assumption that the positive samples are (nearly) con-
ditionally independent. Huang et al. (2021) establish bounds by assuming a very small intra-class
support diameter, which is also not practical. Besides, some also explore the information-theoretical
perspectives for analyzing contrastive learning (Tian et al., 2020; Tsai et al., 2021; Tosh et al., 2020;
2021), though their mutual information assumptions are hard to verify. Recently, similar to our anal-
ysis, HaoChen et al. (2021) also study the augmentation graph and establish guarantees in terms of
graph connectivity. Our work differs to theirs mainly in three aspects: 1) our analysis is applicable
for the widely adopted InfoNCE and CE losses, while theirs is developed for their own spectral
loss; 2) ours starts from the alignment and uniformity perspective while theirs starts from the ma-
trix decomposition perspective; 3) our theory is empirically verified and inspires a useful evaluation
metric for data augmentation, while their analysis focusing on minimizing the decomposition error
is farther from the practical designing of positive and negative samples. In a nutshell, compared to
previous discussions, our theory has a closer connection to the actual contrastive learning process,
and we verify the feasibility of each assumption with empirical evidence.
3	Limitations of Previous Understandings
We begin by introducing the basic notations and common practice of contrastive learning in the
image classification task. In general, it has two stages, unsupervised pretraining, and supervised
finetuning. In the first stage, with N unlabeled samples Du = {xi}iN=1, we pretrain an encoder
mapping from the d-dimensonal input space to a unit hypersphere f ∈ F : Rd → Sm-1 in the m-
dimensional space. In the second stage, we evaluate the learned representations z with the labeled
data Dl = {(xi , yi)} where labels yi ∈ {1, . . . , K}. Specifically, we fix the encoder and learn a
linear classification head g : Rm → RK on top from Dl = {(z, y)|z = f (x) ∈ Rm}.
Contrastive Pretraining. Taking a training example x ∈ Du, we draw its positive sample x+ =
t(χ) by applying a random data augmentation t 〜T, and draw M randomly augmented samples
{xi- }iM=1 from Du as its negative samples. Then, we can learn the encoder f with the widely used
InfoNCE loss (Oord et al., 2018)
LNCE (f) =Ep(x,x+)E{p(xi-)}
l0U	eχpf(X)>f(X+))
g PM=ι exp(f(x)>f (x-))
(1)
Let p(x) be the data distribution, p(x, x+) be the joint distribution of positive pairs, and we simply
assume p(x, x+) = p(x+, x) and p(x) = p(x, x+)dx+, ∀ x ∈ Rd following Wang & Isola (2020).
Linear Evaluation. To evaluate the learned representations by contrastive learning, we usually
adopt the Cross Entropy (CE) loss (Chen et al., 2020a) for a labeled pair (x, y) ∈ Dl
LCE (f, g)
= Ep(x,y)
exp f (x)>wy)
-log ---iT-------------
PK=1 exp (f (x)>Wi)
(2)
with a linear classifier g(z) = Wz where W = [w1, w2, . . . , wK].
3
Published as a conference paper at ICLR 2022
3.1	Existing Theoretical Assumptions and Their Limitations
Figure 2: Contrastive learning may
learn class inseparable features
even with perfect aligned pos-
tive samples and uniform negative
samples. Colors denote classes.
f(xt) =/。。
∀修∈d
sider the case when features {f (x,)}iN=I are randomly dis-
As discussed above, there are some previous understandings on how contrastive learning yields good
performance, and they mainly differ by their theoretical assumptions.
First, Wang & Isola (2020) interpret the first and second terms
of the InfoNCE loss (Eq. 1) as they are aiming at the following
two properties: 1) alignment (the nominator): positive samples
x,x+ has similar features, i.e., f (x) ≈ f (x+); 2) uniformity
(the denominator): features are roughly uniformly distributed
in the unit hypersphere SmT. In particular, they show that
InfoNCE can be minimized with 1) perfect alignment and 2)
perfect uniformity. However, as we illustrate in Figure 2, the
features could still have very poor downstream performance in
the finite sample scenario. This issue can be described rigor-
ously by the following proposition.
Proposition 3.1 (Class-uniform Features Also Minimize the
InfoNCE Loss). For N training examples of K classes, con-
tributed in Sm-I with maximal uniformity (i.e., , minimiz-
ing the 2nd term of Eq. 1) while also satisfying ∀xi,x+ 〜
p(x,x+),f (Xi) = f (x+). Because we have these two properties, the InfoNCE loss achieves its
minimum. However, the downstream classification accuracy is at most 1/K + ε and ε is nearly zero
when N is large enough.
Proofs can be found in Appendix A. This proposition indicates that the instance discrimination task
(alignment + uniformity) alone cannot guarantee the learning of class-discriminative features as
desired in the downsteam classification. instead, saunshi et al. (2019) and Lee et al. (2020) both
establish the relationship between pretraining and classification objectives and provide guarantees
for the downstream performance. in fact, the two works both assume the conditional independence
of the two positive samples, i.e., p(x, x+ |y) = p(x|y)p(x+|y). However, this assumption is too
strong as it is hardly practical. As shown in Figure 1(b), augmented views from the same class are
not actually independent as views from the same sample are more alike than that from other samples.
4	New Augmentation Overlap Theory for Contrastive Learning
The analysis above motivates us to find a minimal and practical assumption: 1) it is enough to
guarantee good performance on downstream tasks; 2) it is less restrictive than the i.i.d. assumptions
as in saunshi et al. (2019) and Lee et al. (2020).
4.1	Gap Between Contrastive Learning and Downstream Classification
We start with an assumption on the label consistency between positive samples, that is, any pair of
positive samples (x, x+) should belong to the same class.
Assumption 4.1 (Label Consistency). ∀ x,x+ 〜p(x, x+), we assume the labels are deterministic
(one-hot) and consistent: p(y|x) = p(y |x+).
This is a natural and minimal assumption that is likely to hold in practice. As shown in Figure 1(b),
the widely adopted augmentations in contrastive learning (Chen et al., 2020a) like images cropping,
color distortion, and horizontal flipping will hardly alter the belonging image classes.
With this minimal assumption, we can characterize the generalization gap between unsuper-
vised and supervised learning risks. We first introduce the mean CE loss, LCE(f) =
πn	i	exp( f (X) μy)	i- - 一- -	. i_ _ -i_-一 ∙ , _	,
Ep(χ,y) 一 log PK~exp(f(x)>μ∙) , where we use the classwise mean representation μk =
Ep(x|y=k) [f (x)] as the weight wk of the classifier g. it is easy to see that the mean CE loss up-
per bounds the CE loss, i.e., LCE(f) ≥ mi□g LcEf,g) and Saunshi et al. (2019) showed that
the mean classifier could achieve comparable performance to learned weights. Then, we have the
following upper and lower bounds on the downstream risk (measured by mean CE loss).
4
Published as a conference paper at ICLR 2022
Augmentation Graph
(input space)
Learned Representations
(feature space)
Under Overlapping
Perfect Overlapping
Overly Overlapping
(a) Contrastive learning With an augmentation
graph satisfying intra-Class connectivity.
(b) Augmentation graph under increasing augmentation
strengthes (left to right).
Figure 3: Illustrative examples of augmentation graphs, where each dot denotes a sample X ∈ Du
and its color denotes its class. The lighter disks denote the support of the positive samples p(χ+∣χ).
We draw a solid edge for each T-connected pair.
Theorem 4.2 (Guarantees for General Encoders). If Assumption 4.1 holds, then, for any f ∈ F, its
downstream classification risk LCe (f) Can be bounded by the contrastive learning risk Lnce(f)
Lnce (f)-PVRfxrRy - 2var(f (x) | y) -O (M T/2)
≤ LCe(∕) + log(M∕K) ≤ Lnce(∕) + PVar(f (x)|y) + O (MT/2),
where log(M /K) isa constant*, Var(f (x)|y) = Ep(y) Ep(x|y) kf (x) - Ep(x|y)f (x)k2 denotes the
conditional (intra-class) feature variance, and O M -1/2 denotes the order of the approximation
error by using M negative samples.
Notably, our generalization bounds above improve over previous ones in the following aspects:
1)	we do not require the conditional independence assumption as in Saunshi et al. (2019);
2)	we directly analyze the widely adopted InfoNCE loss (for contrastive learning) and CE loss
(for supervised finetuning), while Saunshi et al. (2019) are restricted to hinge and logistic
objectives that have worse performance in practice (Chen et al., 2020a);
3)	the class collision error terms introduced in Saunshi et al. (2019) (due to the existence of
same-class samples in the negative samples) now disappear in our bounds by adopting the
InfoNCE loss, which also helps understand why InfoNCE performs better in practice; and
4)	the bounds in Saunshi et al. (2019) will become looser with more negative samples, which
is contradictory to the common practice (Chen et al., 2020a). While in our bounds, a larger
M indeed has a lower approximation error and helps close the generalization gap.
In fact, several recent works have also been devoted to resolve the last “large-M” problem (Ash et al.,
2021; Merad et al., 2020). Nevertheless, their analysis also requires the conditional independence
assumption as in Saunshi et al. (2019), while we show this problem can be resolved even without
conditional independence. Nozawa & Sato (2021) also establish bounds for the InfoNCE loss, but
their bounds have incompressible class collision terms while ours do not.
Nevertheless, an important message of the theorem above is that Assumption 4.1 alone is still insuf-
ficient to guarantee good downstream performance. As there are intra-class variance terms in the
upper and lower bounds, when they are large enough, contrastive learning might still have inferior
performance as shown in Proposition 3.1. Although the variance terms can be easily eliminated with
the canonical conditional independence assumption, discussions in Section 1 have already demon-
strated its impracticality. In the next part, we will present a new understanding of how contrastive
learning could control this variance term in practice.
4.2	Closing the Gap with Intra-class Connectivity
The theorem above motivates us to study how contrastive learning could effectively control its intra-
class variance and learn class-separated features. Here, we propose a new understanding of this
clustering ability through a dissection of the augmented views. In particular, we notice that although
samples are different from each other, applying aggressive augmentations like that in SimCLR (Chen
* log(M /K) could be absorbed in the loss functions by replacing sum with mean in InfoNCE and CE.
5
Published as a conference paper at ICLR 2022
et al., 2020a) can largely make them more alike. For example, in Figure 1(b), two different cars
become very similar when they are both cropped to the wheels. Then, with contrastive learning, the
two cars will have closer representations as they share a common view of the wheels. In other words,
two different intra-class samples could be aligned together if they have overlapped augmented views.
If all intra-class samples could be bridged by data augmentations, we can successfully cluster the
whole class together. Below, we formalize the intuition above with the language of graphs.
Notations. A graph G is represented by a tuple G = (V, E) where V = (v1, v2, . . . , vN) is a set of
vertices and E ⊆ V × V is a set of edges. A path is a sequence of edges that joins a sequence of
vertices, e.g., v” 一 vi? -∙∙∙- Vik. We say that two vertices V and U are connected if G contains a
path from v to u. A graph is said to be connected if every pair of vertices in the graph is connected.
Two graphs are said to be disjoint if any pair of inter-graph vertices are not connected.
To begin with, we define the concept of T -connectivity of sample pairs, which describes whether
two samples could be connected via the augmentation overlap of their augmented views.
Definition 4.3 (T -connectivity). Given a collection of augmentations T = {t | t : Rd → Rd},
we say that two different images xi , xj ∈ Rd are T -connected if they have overlapped views:
supp(p(x+ ∣xi)) ∩ supp(p(χ+ |xj)) = 0, or equivalently, ∃ ti,tj ∈ T such that ti(xi) = tj (Xj).
Then, we can define an augmentation graph of all training samples in terms of their T -connectivity.
Definition 4.4 (Augmentation Graph). Given a setofN samples D = {xi}iN=1 and an augmentation
set T = {t | t : Rd → Rd}, we can define an augmentation graph G(D, T) = (V, E) as
•	we take the N natural samples as the vertices of the graph, i.e., V = {xi}iN=1;
•	there exists an edge eij between two vertices xi and xj if they are T -connected.
Based on these concepts, we introduce the following assumption that with a proper choice of data
augmentations, all intra-class samples could form a connected graph, as depicted in Figure 3(a).
Assumption 4.5 (Intra-class Connectivity). Given a training set Du, there exists an appropriate
augmentation set T such that the augmentation graph G(Du, T) is class-wise connected, i.e., ∀ k ∈
{1, . . ., K}, the subgraph Gk (graph G restricted to vertices in class k) is connected.
Comparing to Saunshi et al. (2019) and Lee et al. (2020) that require (nearly) conditional indepen-
dence p(x, x+ |y) = p(x|y)p(x+ |y), ours only requires the connectivity of intra-class samples as in
Figure 1(b), and does not need them to be conditionally independent.
To make this analysis technically simpler, we make another assumption that we can align positive
samples perfectly by minimizing the InfoNCE loss. In practice, the alignment loss can typically be
minimized up to a small error ε, and we have appended a more involved discussion of this weak
alignment scenario in Appendix B. For now, we focus on the simplified perfect alignment scenario.
Assumption 4.6 (Perfect Alignment). At the minimizer f? of the InfoNCE loss, we can achieve
perfect alignment, i.e., ∀ x,x+ 〜p(x, x+), f?(x) = f ?(x+).
Proposition 4.7. Under Assumptions 4.5 & 4.6, by minimizing the InfoNCE loss we can conclude
that the conditional variance terms vanish at the minimizer f?, i.e.,
Var(f ? (x) | y) = 0.	(4)
Intuitively, for samples in each class k, if the corresponding subgraph Gk is connected, there exists
a path connecting every intra-class pairs (xi, xj ), as shown in Figure 3(a). Consequently, aligning
the positive pairs will also align all samples on the path, and eventually align xi and xj . In this way,
all intra-class samples can be clustered together and the intra-class variance shrinks to zero (under
Assumption 4.6). Besides, because proper data augmentation will not cause inter-class augmentation
overlap (Assumption 4.1), inter-class samples can be well separated with the uniformity term. As a
result, we can attain alignment of intra-class samples while maximizing the uniformity of inter-class
samples. According to Theorem 4.2, we will have an asymptotically closed generalization gap (with
more negative samples M → ∞) for the encoder that minimizes the contrastive loss.
Theorem 4.8 (Guarantees for the Optimal Encoder). If Assumption 4.1, 4.5 & 4.6 hold and f is
L-smooth, then, for the minimizer f? = arg min LNCE (f), its classification risk can be upper and
lower bounded by its contrastive risk as
Lnce(∕?) - O (MT/2) ≤ LCe(∕?) + log(M∕K) ≤ Lnce(∕?) + O (M-1/2) .	(5)
6
Published as a conference paper at ICLR 2022
acc = 0.50	acc = 0.78	acc = 1.00	acc = 0.50
r = 0	r = 0.001
r = 0.1	r = 1.5
Figure 4: t-SNE visualization of features learned with different augmentation strength r on the
random augmentation graph experiment. Each dot denotes a sample and its color denotes its class.
We note that different to previous bounds that hold for any f ∈ F as in Theorem 4.2, our results
here only stand for the minimizer of the contrastive loss f ?. This indicates that the InfoNCE loss
alone cannot simply guarantee good downstream performance, and the learning dynamics matters
for the contrastive learning to learn useful features.
4.3	Rethinking the Role of DATA Augmentations
Our analysis above suggests a new understanding of the role of data augmentations in contrastive
learning. Conventionally, the success of contrastive learning is usually attributed to learning in-
variance w.r.t. various data augmentations by matching positive examples. However, as shown in
Proposition 3.1, matching positive pairs alone is theoretically inadequate to learn useful features.
Indeed, assuming that an ideal encoder that possesses invariance a priori does exist, like invariance
to translation (CNNs), rotation (Cheng et al., 2016), and scaling (Xu et al., 2014), do we obtain
class-discriminative features simply by random initialization? Still No, since these low-level prop-
erties are independent of high-level class information that we want to learn. Thus, the reason why
contrastive learning works cannot simply be attributed to the invariance learning principle.
We instead believe that the role of data augmentation is to create a certain degree of “chaos” be-
tween different intra-class samples (Figure 1(b)) such that they become more alike (or formally,
T -connected). In this way, the chaos serves as a “ladder” for bridging intra-class samples together
when labels are absent, and the mission of the contrastive loss is to “climb this ladder”, that is,
aligning intra-class samples by aligning the overlapped positive samples, as shown in Figure 3(a).
Therefore, from our perspective, instance discrimination by contrastive learning is actually a surro-
gate for the classification task, and the surrogate can complete its misson when the ladder of chaos
is complete (or formally, when intra-class connectivity holds).
5	Quantifying the Influence of Augmentation Strength
We have shown that with appropriate augmentations, we can derive guarantees on downstream per-
formance. However, in practice, as illustrated in Figure 3(b), there could be cases where augmen-
tations are either too weak (intra-class features cannot be clustered together as in Figure 2) or too
strong (inter-class features will also collapse to the same point) and lead to sub-optimal results. In
this section, we further provide a quantitative analysis of how different strength of data augmentation
will affect the final performance, both theoretically and empirically.
5.1	Characterization on Random Augmentation Graph
In practice, there are various data augmentation types that are hard to be described precisely. For the
ease of analysis, we consider a simple case where for each class k, there are N samples uniformly
distributed around the cluster center ck on a hypersphere Sd . We then augment each sample xi with
random samples in a hyper-disk of radius r on the hypersphere.
In Appendix D, we provide theoretical analysis on how different augmentation strength (measured
by r) will affect the connectivity of the augmentation as a function of the number of samples N ,
the position of the cluster centers ck and input dimensions d. In particular, the minimal r for the
graph to be connected decreases as N increases, so large-scale datasets can bring better connectiv-
ity. Meanwhile, the required r also increases as d increases, so we need more samples or stronger
augmentations for large-size inputs. Here, we show our simulation results by applying contrastive
7
Published as a conference paper at ICLR 2022
Aug Strength
(a) ACR v.s. aug-strength r.
(b) ACR while training (r=0.01).
(c) ACR while training (r=0.92).
Figure 5: (a) Average Confusion Rate (ACR) and downstream accuracy v.s. different augmentation
strength (before training). (b,c): ACR and downstream accuracy while training.
learning to the problem above. From Figure 4, we can see that when r = 0 (no augmentation), the
features are mixed together and hardly (linearly) separable, which corresponds to the under-overlap
case in Figure 3(b). As we increase r from 0 to 0.1, the features become more and more discrim-
inative. And when r is too large (r = 1.5), the inter-class features become mixed and inseparable
again (over-overlap). In Appendix C.2, we provide visualization results of the augmentation graphs,
which also align well with our analysis. Overall, our theoretical and empirical discussions verify our
theory that intra-class augmentation overlap with a proper amount of data augmentation is crucial
for contrastive learning to work well.
5.2	New Surrogate Metrics for Augmentation Overlap
From our theory and analysis above, we see that the augmentation overlap between intra-class sam-
ples indeed matters from contrastive learning to generalize better. Inspired by this, we propose the
Confusion Ratio metric as a measure of the degree of augmentation overlap. Specifically, for an
unlabeled dataset Du with N samples, we randomly augment each raw sample xi ∈ Du for C
times, and get an augmented set Du = {xij , i ∈ [N], j ∈ [C]}. Then, for each xip ∈ Du that is
an augmented view of xi ∈ Du , denoting its k-nearest neighbors in Deu in the feature space of f
as Nk (xip, f) and other augmented views from the same image as C(xip) = {xij , j 6= p}, we can
define its Confusion Ratio (CR) as the ratio of augmented views from different raw samples in its
k-nearest neighbors,
#[Nk (Xip,f ) \ C(Xip)]
CR(Xij,f)=	#Nk(Xip,f)— ∈ [0,1].	⑹
We also define its average as Average Confusion Ratio (ACR):
ACR(f )= Exij 〜Du CR(Xij,f).	⑺
When augmentation overlap happens, the nearest neighbors could be augmented views from a dif-
ferent sample, leading to a higher ACR. Thus, ACR measures the degree of augmentation overlap,
and a higher ACR indicates a higher degree of augmentation overlap. Here we take k = 1 by default.
Here, to measure the augmentation strength in real-world datasets, following the common practice
(Chen et al., 2020a), we adopt the RandomResizedCrop operator with scale range [a, b] for data
augmentation, and we define its strength of augmentation as r = (1 - b) + (1 - a) (a comparison
with other kinds of augmentations, e.g., color jittering, can be found in Appendix C.1). As shown in
Figure 5(a), ACR (augmentation overlap) indeed increases with the strength of data augmentations,
and only a moderate ACR achieves the best accuracy, which is consistent with our theory discussed
above. Besides, we also plot the change of ACR along the training process in Figure 5(b) & 5(c).
We can notice that for weak augmentations, the initial ACR is low, and it rapidly decreases to zero
and seldom changes while training, which leads to poor test accuracy. Instead, with proper aug-
mentations, the initial ACR is higher, and it gradually decreases to zero and obtains good accuracy.
This is also consistent with our theory that we need a certain amount of augmentation overlap for
contrastive learning to work well. At the beginning, this will lead to a higher ACR, but as training
continues, better alignment (lower ACR) will help bring up the test accuracy.
Average Relative Confusion (ARC). In the discussion above, we notice that ACR itself does not
indicate the test accuracy, but the relative change of ACR before and after training can be used as
such an indicator. A large change of ACR means a large change of augmentation overlap, which in-
dicates that the contrastive loss can actually cluster intra-class samples together through overlapped
8
Published as a conference paper at ICLR 2022
Figure 6: Average Relative Confusion (ARC) and downstream accuracy v.s. different augmenta-
tion strength on different datasets (CIFAR-10, CIFAR-100, and STL-10) with different contrastive
learning methods: SimCLR (Chen et al., 2020a) and BYOL (Grill et al., 2020).
(a) k = 10	(b) k = 20	(c) k = 100
Figure 7: Average Relative Confusion (ARC) and downstream accuracy v.s. different augmentation
strength on CIFAR-10 (SimCLR) with different number of nearest neighbors k.
views. Based on this observation, we propose Average Relative Confusion (ARC) as
ARC_ 1 - ACR(ffinal)
=1 - ACR(finQ,
(8)
a ratio calculated with the initial ACR of the initialized model finit and the final ACR of the pre-
trained model ffinal . A higher ARC indicates that the contrastive learning process faces a hard task
(augmentation overlap) at the beginning (high initial ACR), while successfully clustering intra-class
samples with good alignment of positive samples at the end (lo final ACR). Therefore, a higher ARC
score should correspond to higher downstream accuracy.
As shown in Figure 6 & 7, as augmentations become stronger, ARC scores indeed align well with the
change of downstream accuracy across 1) different datasets, 2) different contrastive methods, and 3)
different choices of k . This justifies our understanding of contrastive learning through augmentation
overlap. Meanwhile, as the calculation of ARC only involves unsupervised data, it could serve as a
good surrogate metric for evaluating contrastive learning without using labeled data. Compared to
previous evaluation methods like linear classification (Eq. 2), our ARC metric is more preferable as
1) itis theoretically motivated; 2) it does not need labeled data; 3) it does not need to learn additional
modules like linear classifiers or rotation tasks (Reed et al., 2021). More experimental details can
be found in Appendix E.
6 Conclusion
In this paper, we have proposed a new understanding of contrastive learning through a revisiting of
the role of data augmentations. In particular, we notice the aggressive data augmentation applied
in contrastive learning can significantly increase the augmentation overlap between intra-class sam-
ples, and as a result, by aligning positive samples, we can also cluster inter-class samples together.
Based on this insight, we develop a new augmentation overlap theory that could guarantee good
downstream performance without relying on conditional independence and obtain asymptotically
closed gaps. With this perspective, we also characterize how different augmentation strength affects
downstream performance with both random graphs and real-world datasets. Last but not least, we
also develop a new surrogate metric for evaluating contrastive learning without labels and show that
it aligns well with downstream performance. Overall, we believe that we pave a new way for under-
standing contrastive learning with insights on the designing of contrastive methods and evaluation
metrics.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Yisen Wang is partially supported by the National Natural Science Foundation of China under
Grant 62006153, Project 2020BD006 supported by PKU-Baidu Fund, and Huawei Technologies
Inc. Jiansheng Yang is supported by the National Science Foundation of China under Grant No.
11961141007. Zhouchen Lin is supported by the NSF China (No. 61731018), NSFC Tianyuan
Fund for Mathematics (No. 12026606), Project 2020BD006 supported by PKU-Baidu Fund, and
Qualcomm.
References
Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra. Investigating the role of
negatives in contrastive representation learning. arXiv preprint arXiv:2106.09943, 2021.
Ivan Budimir, Sever S Dragomir, and Josep Pecaric. Further reverse results for jensen’s discrete
inequality and applications in information theory. RGMIA research report collection, 3(1), 2000.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. ICML, 2020a.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-
supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
arXiv:2011.10566, 2020.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. arXiv preprint arXiv:2104.02057, 2021.
Gong Cheng, Peicheng Zhou, and Junwei Han. RIFD-CNN: Rotation-invariant and fisher discrimi-
native convolutional neural networks for object detection. In CVPR, 2016.
Jean-Bastien Grill, Florian Strub, Florent Altche, C. Tallec, Pierre H. Richemond, Elena
Buchatskaya, C. Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, B. Piot, K. KavUkcUoglu, Remi Munos, and Michal Valko. Bootstrap your own latent: A
new approach to self-supervised learning. NeurIPS, 2020.
Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. NeurIPS, 2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. CVPR, 2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS,
2017.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. ICLR, 2019.
Weiran Huang, Mingyang Yi, and Xuyang Zhao. Towards the generalization of contrastive self-
supervised learning. arXiv preprint arXiv:2111.00743, 2021.
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.
Ibrahim Merad, Yiyang Yu, Emmanuel Bacry, and StePhane Gaiffas. About contrastive unsupervised
representation learning for classification and its convergence. arXiv preprint arXiv:2012.01064,
2020.
Kento Nozawa and Issei Sato. Understanding negative samples in instance discriminative self-
supervised representation learning. NeurIPS, 2021.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
10
Published as a conference paper at ICLR 2022
Mathew D. Penrose. A Strong Law for the Longest Edge of the Minimal Spanning Tree. The Annals
OfProbability, 27(1):246 - 260,1999.
Allon G Percus and Olivier C Martin. Scaling universalities of kth-nearest neighbor distances on
closed manifolds. Advances in Applied Mathematics, 21(3):424-436, 1998. ISSN 0196-8858.
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In ICML, 2019.
Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, and Kurt Keutzer. Selfaugment:
Automatic augmentation policies for self-supervised learning. In CVPR, 2021.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning. In ICML, 2019.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. In ICLR, 2020.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning. NeurIPS, 2020.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic
posterior information to linear models. arXiv preprint arXiv:2003.02234, 2020.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. In ALT, 2021.
Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-
supervised learning from a multi-view perspective. In ICLR, 2021.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere, 2020.
Yifei Wang, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, and
Zhouchen Lin. Residual relaxation for multi-view representation learning. In NeurIPS, 2021.
Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, and Zheng Zhang. Scale-invariant con-
volutional neural networks. arXiv preprint arXiv:1411.6369, 2014.
11
Published as a conference paper at ICLR 2022
A Omitted Proofs
A.1 Proof of Proposition 3.1
Proposition A.1 (Class-uniform features also minimize the InfoNCE loss). ForN training examples
of K classes, consider the case when features {f (xi)}iN=1 are randomly distributed in Sm-1 with
maximal uniformity while also satisfying Nxi,x+ 〜p(x,x+), f (Xi) = f (x+). Because we have
perfect alignment and perfect uniformity, the InfoNCE loss achieves its minimum. However, the
downstream classification accuracy is at most 1 /K + ε and ε is nearly zero when N is large enough.
Proof. We only need to give a counterexample that satisfy the desired classification accuracy. We
consider the case when there is no T -connectivity between any pair of samples from {xi}iN=1, which
is easily achieved if we adopt a small enough data augmentation. In this scenario, the perfect align-
ment of positive samples (xi, xi+) could have no effect on the other samples. Therefore, when the
features {f(xi)}iN=1 are uniformly distributed in Sm-1, according to the law of large number, for
any measurable set U ∈ Sm-1, when N is large enough, there will be almost equal size of features
from each class in U. Consequently, any classifier g that classifies U to class k will only have 1/K
accuracy asymptotically.	□
A.2 Proof of Theorem 4.2
We will prove the upper and lower bounds separately as follows.
A.2.1 The Upper Bound
We first provide the upper bound of the approximation error of the following Monte Carlo estimate.
Lemma A.2. For LSE := log Ep(z) exp(f (x)> g (z)), we denote its (biased) Monte Carlo estimate
with M random samples Zi 〜 p(z), i = 1,...,M as LSEM = log 吉 PMI exp(f (x)>g(zi)).
Then the approximation error A(M) can be upper bounded in expectation as
A(M) := Ep(x,zi)|LdSE(M) - LSE| ≤ O(M-1/2).	(9)
We can see that the approximation error converges to zero in the order of 1/M -1/2.
Proof. First, we have
1M
Ep(χ,Zi) log M EeXp(f(x)>g(zi)) - logEp(Zi) exp(f (x)>g(zi))
1M
≤eEp(χ,Zi) M 2exp(f(x)>g(zi)) — Ep(Zi) exp(f(x)>g(zi))=O(M-1/2),
where the first inequality follows the Intermediate Value Theorem and e (the natural number) is
the upper bound of the absolute derivative of log between two points when |f (x)>g(zi)| ≤ 1.
And the second inequality follows the Berry-Esseen Theorem given the bounded support of
exp(f(x)>g(zi)) as following: for i.i.d random variables Yi with bounded support supp(Y ) ⊂
12
Published as a conference paper at ICLR 2022
[-α, α], zero mean and bounded variance σγ < α2, We have:
「1 卫1 r Γl 1 卫〕
E ( M X 匕 L √⅛EI √MσyX 匕
0√M	M l
=√M/	P〕√MσγXK∣>xdx
α√ M
≤√⅛ /k P[N(0, 1)I>X + √Mdx
≤√≡ (于 + / P" I) l>x]dx)
≤√M + √M EuM0,I) l ]=O(M T/2)
where the constant Ca only depends on α. Here, we set 匕 = exp(f(X)Tg(&))-
Ep(Zi) exp(f (X)Tg(a)). As ∣ f (χ)>g(zi)I ≤ 1, ∣ Yi∣ ≤ 2e. Yi has zero mean and bounded vari-
ance (2e)2.	□
Theorem A.3. For each f ∈ F, the mean CE loss can be upper bounded by the InfoNCE loss:
LCe(x, y; f) ≤ LNCE(x; f) - log(M∕K) + √Var(f (x) ∣ y) + A(M),	(10)
where Var(f (x)∣ y) = Ep(y)[Ep(x∣y)kf (x) — Ep(x∣y)f (x)∣∣2] denotes the conditional variance.
Proof. Denote p(x,x+,y) as thejoint distribution of the positive pairs x,x+ and the label y. Denote
the M independently negative smaples as {x- }M=1. According to Assumption 4.1, x+ and X here
has the same label y. Denote μy as the center of features of class y, y = 1,...,K. Then we have
the following lower bounds of the InfoNCE loss,
M
Lnce(f) = -Ep(X,χ+)f(x)Tf(X+) + Ep(X)Ep(以)logEexp(f (x)τf (x-))
Z i=1
1M
-Ep(x,x+)f (X)Tf(X+) + Ep(X)Ep(X-) log M Σ^exPf (X)Tf(Xi )) + log M
⑴
≥ - Ep(X,X +
1M
)f (X)Tf(X+) + Ep(X) log MEp(X-) £ exρ(f (X)Tf(X-)) - A(M) + log M
Z i=1
=-Ep(X,X+)f(x)Tf(X+) + Ep(X) logEp(X-) exρ(f (X)Tf(X-)) - A(M) + log M
=-Ep(X,X+,y)f (x)tf (x+) + Ep(X) logEp(y-)Ep(X-∣y-) exρ(f(x)Tf(X-)) - A(M) + logM
(2)
≥ - Ep(X,X+,y)f (x)tf (x+) + Ep(X) logEp(y-) exρ(Ep(X-∣y-) [f (x)τf (x-)]) - A(M) + logM
=-Ep(X,X+,y)f (x)t(μy + f (x+ ) - 〃y) + Ep(X) logEp(y-) exρ(Ep(X-∣y-) [f (x)τf (x-)]) - A(M) + logM
=-Ep(X,X+,y)[f (x)τμy + f (x)τ(f (x+) - 〃y)] + Ep(X) logEp(y-) exρ(f (x)τμy-) - A(M) + log M
(3)
≥ - Ep(X,X+,y) [f (x)τμy + Il(f (x+) - 〃y)k] + Ep(X) logEp(y-) exρ(f (x)τμy-) - A(M) + logM
(4)	I-----------------
≥ - Ep(X,y)f (X)T〃y - ,Ep(X,y)kf(x) - μyIl2 + Ep(X) logEp(y-) exρ(f (x)τμy-) - A(M) + logM
_	―一	` , `	_	_	1 VK	——	.	____ 一 一
=-Ep(X,y)f (x)τμy - √Var(f (x) ∣ y) + Ep(X) log K UXP(f (x)τμfc) - A(M) + log M
K
=Ep(X,y) [ - f (x)T〃y + log E exρ(f (x)τμk)] - √Var(f (x) ∣ y) - A(M) + log(M∕K)
k=1
=LCE(f) - √Var(f (x) ∣ y) - A(M) + log(M∕K),
13
Published as a conference paper at ICLR 2022
which is equivalent to our desired results. In the proof above, (1) follows Lemma A.2; (2) follows
the Jensen's inequality for the convex function exp(∙); (3) follows from the fact that because f (x) ∈
Sm-1, we have
f (X)>(f (X+) - μy) ≤ ( kf(x+) - μyk )	(f (X+) - μy) = kf (X+) - μyk； (II)
and (4) follows the CaUChy-Schwarz inequality and the fact that because p(χ,χ+) = p(x+,x)
holds, x, χ+ have the same marginal distribution.	口
A.2.2 The Lower Bound
In this part, we further show a lower bound on the downstream performance.
Lemma A.4 (Budimir et al. (2000) Corollary 3.5 (restated)). Let g : Rm → R be a differentiable
convex mapping and z ∈ Rn. Suppose that g is L- smooth with the constant L > 0, i.e., ∀X, y ∈
Rm, kVg(x) 一 Vg(y)k ≤ LkX - yk. Then we have
n
0 ≤ Ep(z)g(z)- g (Ep(Z)Z) ≤ L [Ep(z)kzk2-kEp(z)zk2] = L X Var(Zj)),	(12)
j=1
where X(j) denotes the j-th dimension of X.
With the lemma above, we can derive the lower bound of the downstream performance.
Theorem A.5. For any f ∈ F, we have
LCe(f) ≥ LNCE(x; f) - PVar(f (x) | y) - 1 Var(f (x) | y) - A(M)- log M, (13)
where Var(u(X)|y) = Ep(y) Ep(x|y) ku(X) - Ep(x|y)u(X)k2 denotes the conditional variance.
Proof. Similar to the proof of Theorem A.3, we have
K
LCe(f) = -Ep(X,y)f(X)>μy + Ep(X) log£exp(f(X)>〃i)
i=1
1K
=-Ep(χ,y)f (X)>μy + Ep(X) log K E exp(f (x)>〃i) + log K
i=1
=-Ep(x,y)f (X)>μy + Ep(X) log Ep(y-) exρ(f (X)>μyi) + log K
(1)	1 M
≥ - Ep(X,y)[f (X)Tf(X+) + f (X)>(〃y - f (X+))] + Ep(X)Ep(y-) log M EeXpf(X)>〃yJ - A(M) + log K
i=1
(2)
≥
-Ep(X,X+)f (X)>f (X+) - Ep(X,y)kf (X)> - μy k
1M
+ Ep(X)Ep(y-) log M EeXp(Ep(X-|y-)f(X)>f (X-)) - A(M) + log K
M i=1
(3)
≥ - Ep(X,X+
)f(X)>f(X+) - √Var(f (X) | y)
1M
+ Ep(X)Ep(y-)Ep(X-Iy) log M ∑exp(f (X) f (x ))
M i=1
1m
-2 X Var(fj (x ) | y) - A(M)+ log K
M
= -Ep(X,X+) f(X)>f(X+) + Ep(X-) log X exp(f(X)>f(X-))
i=1
m
-pVar(f (x) | y) - r X Var(fj (x-) | y) - A(M) + log K - log M
j=1
≥,lnce(x; f) - PVar(f (x) | y) - 1 Var(f (x) | y) - A(M) - log M,
2K
14
Published as a conference paper at ICLR 2022
which is our desired result. In the proof, (1) we adopt a Monte Carlo estimate with M samples from
p(y) and bound the approximation error with Lemma A.2; (2) follows the same deduction in The-
orem A.3; (3) the first term is derived following the CaUchy-SchWarz inequality for the alignment
term. As for the second term, we first show that the convex function logsumexp is L-smooth as
a function of f(xj-) in our scenario. Because kf(X)k ≤ 1, we have ∀f (xj1), f(xj2) ∈ Rm, the
following bound on the difference of their gradients holds
∂log[exp(f(x)>f(xj-1) + Pi6=j exp(f(x)>f(xi-)))]	∂log[exp(f(x)>f(xj-2) + Pi6=j exp(f(x)>f(xi-)))]
------------------------------------------------------------------------------------------------------	
∂f(Xj1)----------------------------------------------------------------∂f(x-)
exp(f (x)> f (xj-1 ))	exp(f (x)> f (xj-2 ))
---------ɪ	 - ------ɪ-----------------------ɪ-------
exp(f * (X)Tf(Xjɔ + Pi=j exP(f (XC))-------------------------------------------exP(f (X)Tf(Xj2) + Pi=j exp(f (x)>f (XC))
(Pi=j exP(f (X)Tf(X- A eχp(f (XjI)) - Pi=j exP(f (X)Tf(X- A eχp(f (X)Tf(Xj2 A
(eχp(f (x)>f (XjI)) + Pi=j exp(f (X)Tf(X-)))(exp(f (X)Tf(Xj2)) + Pi=j exp(f (X)Tf(X-)))
∙kf(X)k
≤kf(X)k≤ 2∣∣f(Xj1) - f(Xj2)∣∣
So here the logsumexp is L-Smooth for L = 1. Then, we can apply the reversed Jensen's inequality
in Lemma A.4; (4) holds because
m
X Var(fj(X)|y)
j=1
m
=	Ep(y) Ep(x|y) (fj (X) - Ep(x0 |y) fj (X ))
j=1
m
=Ep(y)Ep(x|y)	(fj(X) -Ex0fj(X0))2
j=1
=Ep(y)Ep(x|y)kf(X)-Ex0f(X0)k2
=Var(f (X)|y).
(14)
□
A.3 Proof of Proposition 4.7
Proposition A.6. Under Assumptions 4.5, & 4.6, by minimizing the InfoNCE loss, we can conclude
that the conditional variance term vanishes, i.e.,
Var(f (X) | y) = 0.	(15)
Proof. Consider any T -connected sample Xi , Xj . Accoding to the definition of T -connectivity,
there exist ti, tj ∈ T such that ti(X) = tj (X). When perfect alignment holds as in Assumption
4.6, we will have f(Xi) = f(ti(Xi)) and f(Xj) = f(tj (Xj)). Combining with ti(Xi) = tj (Xj),
we have f(Xi) = f(Xj). That is, any T -connected pair has the same representation. Then, in the
augmentation subgraph Gk that is connected according to Assumption 4.5, there exists a path for
any pair of samples Xi, Xj ∈ Gk where any two adjacent samples are T-connected. As a result, Xi
and Xj will also have the same representation by applying T-connectivity recursively. At last, all
samples in Gk will have the same representation and the intra-class variance vanishes.	口
A.4 Proof of Theorem 4.8
Theorem A.7 (Guarantees for the optimal encoder). If Assumption 4.1, 4.5 & 4.6 hold and f is
L-smooth, then, for the minimizer f? = arg min LNCE (f), its classification risk can be upper and
lower bounded by its contrastive risk as
Lnce(f?) - O (MT/2) ≤ L^(f?) + log(M∕K) ≤ Lnce(∕*) + O (MT/2) .	(16)
Proof. A direct combination of Theorem 4.2 and 4.7 will give us the above two-sided bounds. 口
15
Published as a conference paper at ICLR 2022
B Generalized Guarantees under Weak Alignment
In Section 4.2, we have shown that with perfect alignment (Assumption 4.6), the variance terms in
the bounds of Theorem 4.2 can be minimized to zero, and consequently, the upper and lower bounds
can be asymptotically closed. Nevertheless, in practice, due to the constraint of hypothesis class F
and optimization algorithms, we typically cannot achieve the exact minimizer, i.e., a perfect degree
of alignment. This motivates us to consider a less restrictive setting, namely the ε-weak alignment
assumption, where the alignment error could be as large as ε.
Definition B.1 (Weak Alignment). A mapping f satisfies ε-Weak alignment if ∀ x,x+ 〜
p(x,x+), kf (x) - f (x+)k ≤ ε.
For any ε-weak alignment f ∈ F, we have the following bounds on its downstream risk.
Theorem B.2 (Guarantees under weak alignment). If Assumption 4.1, 4.5 hold, then ∀f ∈ F sat-
isfying ε-weak alignment, its classification risk can be upper and lower bounded by its contrastive
risk as
Lnce(f) - Dε - 1 D2ε2 - O (MT/2)
2	(17)
≤LCe(∕ )+ log(M∕K) ≤ Lnce (f) + Dε + O (M T/2),
where D denotes the maximal diameter of the intra-class augmentation graphs {Gk , k = 1, . . . , K}
and m denotes the output dimension of the encoder f.
In this way, we extend the guarantees developed for optimal encoders (Theorem 4.8) to even non-
minimizers f ∈ F as long as it could align the positive samples within error ε.
inf
J9ΦIUBQ
0.0	0.5	1.0	1.5	2.0
Aug Strength
(a) Diameter D v.s. augmentation strength r.
,IΦΦIUEQ
(b) Diameter D v.s. the number of samples.
Figure 8: Evaluation of the maximal diameter D as a function of different augmentation strength (a)
and different number of samples (b) on the synthetic data in Section 5.1.
Empirical Verification. Besides the alignment error, we could notice this relaxation also introduces
the dependence on an additional parameter D, the maximal diameter of the intra-class augmentation
graphs. As shown in Figure 8, when the augmentation is very weak, the intra-class graph is not
connected and the diameter is ∞. Then, by applying stronger augmentations, D will become smaller
and smaller, and finally converge to 1 (fully connected). Besides, increasing the number of samples,
ranging from 50 to 10, 000, does not have a large impact on D in practice. Given these facts, we
could reasonably assume that D is bounded and has a relatively small value with properly chosen
augmentations. As a result, with a bounded diameter D, a small alignment error ε will guarantee
a small generalization gap between the upstream and downstream tasks. This generalizes Theorem
4.8 by quantifying the generalization gap under weak alignment.
Proof. Consider any pair of samples (x, x0) from the same class y, and the positive sample of x as
x+. As intra-class connectivity holds, x and x0 are connected, and the maximal length of the path
from x to x0 is D. Therefore, under the ε-weak alignment that
∀x,x+ 〜p(x,x+), kf (x) - f (x+)k ≤ ε,	(18)
we can bound the representation distance between x and x0 by the triangular inequality
kf (x) - f (x0)k≤D	sup	kf (x) - f (x+)k≤Dε.	(19)
p(x,x+ ∣y~p(x,x+)
16
Published as a conference paper at ICLR 2022
With the inequality above, we can bound the variance terms in Theorem 4.2. In particular, the
conditional variance can be bounded as
Var(f (x) | y)
=Ep(y)Ep(x|y)kf(x)-Ex0f(x0)k2
=Ep(y)Ep(x|y)kEx0f(x)-f(x0)k2
≤Ep(y)Ep(x∣y)Ep(χ0∣y)kf(x) - f(x')k2	Qo)
≤Ep(y)	max	kf (x) - f(x0)k2
x,x0 ~p(x∣y)
(1)
≤Ep(y)D2ε2 = D2ε2
where (1) follows Eq. 19. At last, we can bound the variance items in Theorem 4.2 with Eq. 20,
arrive at the desired bounds
LNCE(f) - Dε - 2D2ε2 — O (M-1/2)
≤LCe(∕) + log(M∕K) ≤ Lnce(∕) + Dε + O (MT/2),
which conclude our proof.	□
C Additional Empirical Evidence
C.1 Further Evaluation of ARC Metric
In the main text, we study the effect of different strength of RandomResizedCrop on the downstream
accuracy as our proposed metrics (ACR and ARC), which help verify our theory. Nevertheless,
in practice, the augmentations adopted in contrastive learning is composed of a list of different
kinds of augmentations. Therefore, in this part, we further study the effect of other types of data
augmentations, and we show that our ARC metric is also effective for evaluating not only other
kinds of data augmentations, but also their composed ones.
ARC
Figure 9: Downstream accuracy (ACC) v.s. Average Relative Confusion (ARC) for different types
of augmentations in SimCLR on CIFAR-10.
Comparing different kinds of augmentations. We begin by comparing the four kinds of data aug-
mentations adopted in SimCLR (Chen et al., 2020a): RandomResizedCrop, ColorJitter, Grayscale,
etc. For a fair comparison, we apply each one alone for contrastive learning, and evaluate both the
downstream accuracy and ARC. From Figure 11, we can conclude that among the six kinds of aug-
mentations, RandomResizedCrop is the most important augmentation, and ColorJitter is the second.
The rest of them are less powerful, as they cannot even learn useful features by themselves. We
can also see that our ARC metric aligns well with the downstream accuracy for different kinds of
augmentations.
Comparing ColorJitter with different strength. Based on the observation above, as we have
discussed RandomResizedCrop in Section 5.2, we now choose ColorJitter, the second important
augmentation, as another kind of augmentation for the study of different augmentation strength.
Specifically, we study the four parameters of brightness, contrast, saturation, and hue, where a large
value corresponds a large degree of augmentation. Note that we also adopt the default augmentations
in SimCLR while only changing the parameters of ColorJitter (different to the setup in Figure 9).
As shown in Figure 10, there is also a reverse-U curve like that in RandomResizedCrop, and the
17
Published as a conference paper at ICLR 2022
8 6 4
8 8 8
AOB.IngV Isi
Aug Strength
(a) Brightness.	(b) Saturation.
^0ω⅛00‹ωφl
O 0.3	0.5	0.8	1
Aug Strength
(c) Contrast.
^02⊃00< IS ①一
ARC
5 0 5 0
7.5 NS
8 8 8 8
(d) Hue.
Figure 10: Average Relative Confusion (ARC) v.s. downstream accuracy with different augmenta-
tion strength on four different kinds of color jittering operations.
sweet spot is usually achieved with 0.8, which corresponds to the default of choice in SimCLR
(which is selected with exhausted hyperparameter search). Meanwhile, our ARC metric still aligns
well with the downstream accuracy for different strength of different kinds of color jittering, which
demonstrates its wide applicability.
Figure 11: Downstream accuracy (ACC) v.s. the logarithm of Average Relative Confusion (ARC)
on a composition of RandomResizedCrop and ColorJitter with different strength. Experiments are
conducted on CIFAR-10 with SimCLR.
Comparing composed augmentations. In the above discussion, we focus on the effect of a single
kind of augmentations. Here, we show that our ARC metric is still effective for evaluating the
composition of different augmentations. Notably, it is hard to define a metric of augmentation
strength in this case, as the effect of different augmentations could be nested. Nevertheless, we can
still draw a “ACC - log(ARC)” plot to show the correlation between the downstream accuracy (ACC)
and our ARC metric, where each point denotes a model trained with randomly selected parameters
of RandomResizedCrop and ColorJitter. As shown in Figure 11, we can see there is indeed a strong
correlation between the two metrics, with a Pearson correlation coefficient ρ = 0.80. Therefore, our
metric can be used for selecting different kinds of augmentations as well as their compositions in an
unsupervised fashion.
18
Published as a conference paper at ICLR 2022
C.2 Visualization of Augmentation Graph
For a more intuitive and practical understanding of our augmentation overlap theory developed in
Section 4, we visualize of the augmentation graphs on both synthetic data (Section 5.1) and real-
world data (Section 5.2).
(a) r = 0, acc = 0.50.	(b) r = e-3,acc = 0.78.	(c) r = 0.1, acc = 1.00.	(d) r = 1.5,acc= 0.50.
Figure 12: Visualization of the augmentation graph with different augmentation strength r on the
synthetic data described in Section 5.1. Each color denotes a connected component. The corre-
sponding t-SNE visualization and test accuracy (of contrastive learning) can be found in Figure 4.
Synthetic data. Following the setting of experiments in Section 5.1, we construct the adjacent
matrix of different samples, calculate its connected components, and visualize it in Figure 12 with
different colors. It shows that when there is no augmentation, i.e., r = 0, each sample is a con-
nected component alone, and the number of connected components is the same as the number of
samples N . As we increase the augmentation strength, samples will be connected together through
the augmented views. In particular, when r = 0.1, the whole intra-class samples are connected
while inter-class samples are separated, which exactly satisfy our assumptions on intra-class con-
nectivity and label consistency, respectively. Therefore, this is the perfect overlap as desired, and
indeed, as shown in Figure 5.1, contrastive learning on it obtains 100% test accuracy. When we
keep increasing the augmentation strength to be as large as 1.5, inter-class samples also become
connected and inseparable, leading to a random guess in test accuracy (50%). This shows that the
relationship between the augmentation graph and the downstream performance aligns well with our
augmentation overlap theory.
(a) Under-overlap augmentation (b) Proper overlap augmentation (c) Over-overlap augmentation
graph (r=0.01, acc=0.25).	graph (r=0.92, acc=0.75).	graph (r=1.96, acc=0.29).
Figure 13: The augmentation graph of CIFAR-10 with different strength r of RandomResizedCrop
as in Section 5.2. We choose a random subset of test images, randomly augment each one for 20
times. Then, we calculate the sample distance in the representation space as in prior work like FID
(Heusel et al., 2017), and draw edges for image pairs whose smallest view distance is below a small
threshold. Afterwards, we visualize the samples with t-SNE and color intra-class edges in black and
inter-class edges in red and report their frequencies.
Real-world data. For the ease of analysis, our augmentation overlap theory adopts a simplified
scenario by assuming label consistency (Assumption 4.1) and intra-class connectivity (Assumption
4.5), and we have verified their feasibility on the synthetic data. In comparison, these assumptions
cannot hold exactly on real-world data as the chosen augmentations could be sub-optimal. Never-
theless, as shown in the augmentation graphs of CIFAR-10 (Figure 13), our assumptions could still
approximately hold: with a properly chosen augmentation strength, the inter-class connections will
be much less frequent than intra-class connections: 96.4% edges are intra-class edges. Further con-
sidering the continuity property and extrapolation ability of deep neural networks, these approximate
conditions could still achieve close performance to the optimal performance guaranteed under the
19
Published as a conference paper at ICLR 2022
exact conditions. Besides, we also have similar conclusions for the under-overlap and over-overlap
scenarios: 1) the lack of enough augmentations produces only a few edges in the augmentation
graph, as a result, even though all edges are intra-class edges, the downstream performance is still
poor (25% test accuracy); 2) too strong augmentations instead produce too many inter-class edges
(87.6%), which laso leads to poor downstream accuracy (29%). This highlights that our assumptions
on label consistency and intra-class connectivity are indeed effective guidelines for the designing of
contrastive methods.
D Theoretical Characterization of Augmentation S trength
Following the setting in Section 5.1, we can take the radius r as a notation of augmentation strength,
and analyze its effect on the connectivity of the corresponding augmentation graph.
Theorem D.1. For N random samples taken from a class, while gradually increasing the augmen-
tation strength r, we have the following results.
(a)	Under-overlap. When 0 ≤ T ≤ ri = [(dg] d (d)!(N-1 )1 [1 - 缘:// + O( (N-1)2)],
where r1 is the minimal distance between N samples, all samples (vertices) in the aug-
mentation graph will be isolated. As a result, the learned features could be totally random
as in Proposition 3.1. Instead, if r ≥ r1, there are at least two intra-class samples are
T -connected and enjoy the same representation.
(b)	Perfect overlap. When T ≥ r =幽* (N-+仔(N-T)d [1-嗡罕 + O((N⅛)],
where r2 is the maximal distance between N samples, all samples in the augmentation
graph will be T -connected. As a result, the classwise connectivity in Assumption 4.5 will
be guaranteed.
(c)	Over-overlap. When 0 ≤ r < r3 = T mini,j ka — Cjk 一 1, where r3 is the (asymptotic)
minimal distance between samples from different classes, the label consistency is guaran-
teed. Otherwise, when the augmentation is too large, e.g., r > r3, there will be inter-class
augmentation overlap and Assumption 4.1 not longer holds.
In the theorem above, we show that the proper augmentation strength is a function of the number
of samples N and the input dimensions d. In particular, for each x, as N increases, there will be
more natural examples and we only need a smaller r to obtain an overlap sample. Instead, as d
increases, due to the curse of dimensionality, there will be less samples within the same distance,
thus it requires a larger r.
Nevertheless, we actually only need the augmentation sub-graph Gk to be connected, instead of
being fully connected as in Theorem D.1 (b). While the connectivity is hard to analyze in the finite
sample scenario (N < ∞), we have the following asymptotic property as N → ∞.
Theorem D.2. For N uniformly distributed samples defined in Rd as above, we denote the min-
imal augmentation strength needed for connectivity as a function of N: cN = inf {r > 0 :
G(kr) is connected},
and the minimal augmentation strength needed for avoiding isolated points as
a function of N: dN = inf {r > 0 : every vertex at least has a neighbour}. Vu is the volume of unit
hyperball. Then we have the following asymptotic result:
∀d≥2,
lim
N→∞
lim
N→∞
N2
logN
From the theorem we can see that cdN decreases in the order of Θ
= 2(1—V≡.	(21)
Vu
('1ONN) as N → ∞. First,
this result is aligned with the empirical finding that self-supervised learning can benefit more from
large scale dataset (Chen et al., 2020b). Second, it also indicate a curse of dimensionality that the
required augmentation strength is exponentially large.
N
D.1 Proof of Theorem D.1
Proof. From definition and notation in section 4. We can construct an augmentation Graph G(D, T)
given N random samples. We define Dk as the distance from a random point to its k-th nearest
20
Published as a conference paper at ICLR 2022
neighbour. Percus & Martin (1998) discuss Dk in random grpah and give the estimation of that:
[(d/2)!]d (k - 1 + 1/d)!	S ι	1/d +1∕d2	1	.	(22
Dk ≈ -√π---------(k - 1)!	(N-I)d [1 - 2(N - 1) + O((N-IP)]	(22)
where d is the dimension of hypersphere and N is the number of random points. When r < D1 there
is no edge in the graph. So the class is separated. When r > DN-ι, any pair of vertexes have an
edge between them,so the graph is full connected.	□
D.2 Proof of Theorem D.2
Proof. Denote
cN = inf {ri > 0 : GN (V, E, ri)is connected}.
(23)
With Theorem 1.1 from Penrose (1999) and features are uniformly distributed in the surface of unit
hypersphere, Vu denotes to the volume of unit hypershpere
lim (CNTN27) = 2(1 ~ d)S, d ≥ 2	(24)
N→∞ N log N	Vu
∃No when N > No, and augmentation strength is larger than (Md-NVlθgN)d + eι, the graph is
connected,i.e the class is overlapped.
Then we want specify the case the class will be depart begin with some concepts in graph theory.
The largest nearest-neighbor link: For a give edge distance x and for each i= 1,...,n,let
deg UN,j =	1{kUj - Ukk ≤ ri}
1≤j 6=k≤N
(25)
to be the degree of the vertex Uj in the random graph GN (V, E, ri), and let
δN(ri) = min{deg Um,1(x), ..., deg UN,N (x)}	(26)
be the minimum vertex degree.Define the largest nearest-neighbor link, the smallest edge distance
for which each vertex has at least one neighbor
dN = inf {ri : δN(ri) ≥ 1}	(27)
With Theorem 1.2 from Penrose (1999),
iimsup(dN-——) =2-一TZd) , d ≥2	(28)
N→∞ N log N	Vu
∃N00 when N > N00 , and augmentation strength is less than (2(d2N"f N)d - €2, there will be at
least 1 isolated point which is not connected to any other point,i.e the class is departed.
Thus ∃N1 = max(N0, N00),when N > N1, if augmentation strength is larger than
(Md-N2SlθgN)1 + €i, the graph is connected, if augmentation strength is less than
(2(d-NSlθdg N)1 - €2, there will be at least 1 isolated point.	□
E Additional Experimental Details
E.1 S imulation on Random Augmentation Graph
Following our setting in Section 5.1, we consider a binary classification task with InfoNCE loss. We
generate data from two uniform distribution on a unit ball S2 in the 3-dimensional space. One center
is (0, 0, 1) and another is (0, 0, -1). The area of both parts are 1. We take 5000 samples as train set
and 1000 samples as test set. For the encoder class F, we use a single-hidden-layer neural network
with softmax activation, and we use InfoNCE loss to optimize it.
E.2 Experiments on Real-world Datasets
To better understand and verify our theorem, we conduct experiments on real-world datasets, in-
cluding CIFAR-10, CIFAR-100 and STL-10. We use SimCLR (Chen et al., 2020a) and BYOL
Grill et al. (2020) as our training framework and use ResNet18 as our network. For CIFAR-10 and
CIFAR-100, we adopt C = 10 augmentations for each image, and search neural neighbors in the
entire augmented dataset. For STL-10, we adopt C = 6 due to its relatively large size.
21