Published as a conference paper at ICLR 2022
Trust Region Policy Optimisation in
Multi-Agent Reinforcement Learning
Jakub Grudzien Kuba1,2∖ Ruiqing Chen3,*, Muning Wen4, Ying Wen4,
Fanglei Sun3, Jun Wang5, Yaodong Yang6;
1University of Oxford, 2Huawei R&D UK, 3ShanghaiTech University,
4Shanghai Jiao Tong University 5University College London
6Institute for AI, Peking University & BIGAI
* Corresponding to: yaodong.yang@pku.edu.cn
Ab stract
Trust region methods rigorously enabled reinforcement learning (RL) agents to
learn monotonically improving policies, leading to superior performance on a va-
riety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning
(MARL), the property of monotonic improvement may not simply apply; this is
because agents, even in cooperative games, could have conflicting directions of
policy updates. As a result, achieving a guaranteed improvement on the joint
policy where each agent acts individually remains an open challenge. In this pa-
per, we extend the theory of trust region learning to cooperative MARL. Central
to our findings are the multi-agent advantage decomposition lemma and the se-
quential policy update scheme. Based on these, we develop Heterogeneous-Agent
Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proxi-
mal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL al-
gorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they
need any restrictive assumptions on decomposibility of the joint value function.
Most importantly, we justify in theory the monotonic improvement property of
HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent
MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO signifi-
cantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all
tested tasks, thereby establishing a new state of the art.
1	Introduction
Policy gradient (PG) methods have played a major role in recent developments of reinforcement
learning (RL) algorithms (Silver et al., 2014; Schulman et al., 2015a; Haarnoja et al., 2018). Among
the many PG variants, trust region learning (Kakade & Langford, 2002), with two typical embodi-
ments of Trust Region Policy Optimisation (TRPO) (Schulman et al., 2015a) and Proximal Policy
Optimisation (PPO) (Schulman et al., 2017) algorithms, offer supreme empirical performance in
both discrete and continuous RL problems (Duan et al., 2016; Mahmood et al., 2018). The ef-
fectiveness of trust region methods largely stems from their theoretically-justified policy iteration
procedure. By optimising the policy within a trustable neighbourhood of the current policy, thus
avoiding making aggressive updates towards risky directions, trust region learning enjoys the guar-
antee of monotonic performance improvement at every iteration.
In multi-agent reinforcement learning (MARL) settings (Yang & Wang, 2020), naively applying
policy gradient methods by considering other agents as a part of the environment can lose its ef-
fectiveness. This is intuitively clear: once a learning agent updates its policy, so do its opponents;
this however changes the loss landscape of the learning agent, thus harming the improvement effect
from the PG update. As a result, applying independent PG updates in MARL offers poor conver-
gence property (Claus & Boutilier, 1998). To address this, a learning paradigm named centralised
training with decentralised execution (CTDE) (Lowe et al., 2017b; Foerster et al., 2018; Zhou et al.,
2021) was developed. In CTDE, each agent is equipped with a joint value function which, during
* First two authors contribute equally. Code is available at https://github.com/PKU-MARL/
TRPO-PPO-in-MARL.
1
Published as a conference paper at ICLR 2022
training, has access to the global state and opponents’ actions. With the help of the centralised value
function that accounts for the non-stationarity caused by others, each agent adapts its policy param-
eters accordingly. As such, the CTDE paradigm allows a straightforward extension of single-agent
PG theorems (Sutton et al., 2000; Silver et al., 2014) to multi-agent scenarios (Lowe et al., 2017b;
Kuba et al., 2021; Mguni et al., 2021). Consequently, fruitful multi-agent policy gradient algorithms
have been developed (Foerster et al., 2018; Peng et al., 2017; Zhang et al., 2020; Wen et al., 2018;
2020; Yang et al., 2018).
Unfortunately, existing CTDE methods offer no solution of how to perform trust region learning in
MARL. Lack of such an extension impedes agents from learning monotonically improving policies
in a stable manner. Recent attempts such as IPPO (de Witt et al., 2020a) and MAPPO (Yu et al.,
2021) have been proposed to fill such a gap; however, these methods are designed for agents that are
homogeneous (i.e., sharing the same action space and policy parameters), which largely limits their
applicability and potentially harm the performance. As we show in Proposition 1 later, parameter
sharing could suffer from an exponentially-worse suboptimal outcome. On the other hand, although
IPPO/MAPPO can be practically applied in a non-parameter sharing way, it still lacks the essential
theoretical property of trust region learning, which is the monotonic improvement guarantee.
In this paper, we propose the first theoretically-justified trust region learning framework in MARL.
The key to our findings are the multi-agent advantage decomposition lemma and the sequential pol-
icy update scheme. With the advantage decomposition lemma, we introduce a multi-agent policy
iteration procedure that enjoys the monotonic improvement guarantee. To implement such a proce-
dure, we propose two practical algorithms: Heterogeneous-Agent Trust Region Policy Optimisation
(HATRPO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO). HATRPO/HAPPO
adopts the sequential update scheme, which saves the cost of maintaining a centralised critic for
each agent in CTDE. Importantly, HATRPO/HAPPO does not require homogeneity of agents, nor
any other restrictive assumptions on the decomposibility of the joint Q-function (Rashid et al., 2018).
We evaluate HATRPO and HAPPO on benchmarks of StarCraftII and Multi-Agent MuJoCo against
strong baselines such as MADDPG (Lowe et al., 2017a), IPPO (de Witt et al., 2020b) and MAPPO
(Yu et al., 2021); results clearly demonstrate its state-of-the-art performance across all tested tasks.
2	Preliminaries
In this section, we first introduce problem formulation and notations for MARL, and then briefly
review trust region learning in RL and discuss the difficulty of extending it to MARL. We end by
surveying existing MARL work that relates to trust region methods and show their limitations.
2.1	Cooperative MARL Problem Formulation and Notations
We consider a Markov game (Littman, 1994), which is defined by a tuple hN , S, A, P, r, γi. Here,
N = {1, . . . , n} denotes the set of agents, S is the finite state space, A = Qin=1 Ai is the product
of finite action spaces of all agents, known as the joint action space, P : S × A × S → [0, 1] is the
transition probability function, r : S × A → R is the reward function, and γ ∈ [0, 1) is the discount
factor. The agents interact with the environment according to the following protocol: at time step t ∈
N, the agents are at state st ∈ S; every agent i takes an action at ∈ Ai, drawn from its policy ∏i(∙∣St),
which together with other agents’ actions gives a joint action at = (at1, . . . , atn) ∈ A, drawn from the
joint policy ∏(∙∣st) = Qn=ι ∏i (∙ i | s t) ;the agents receive a joint reward rt = r(st, at) ∈ R, and move
to a state st+1 with probability P (st+1 |st, at). The joint policy π, the transition probabililty function
P, and the initial state distribution ρ0, induce a marginal state distribution at time t, denoted by ρtπ .
We define an (improper) marginal state distribution ρπ , Pt∞=0 γtρtπ. The state value function
and the state-action value function are defined: Vn(S)，Ea。-〜ʤ〜P [ P∞=o Ytrt ∣ s0 = s] and
Qn(s, a) , Esl∞〜p,aL∞〜π [P∞=0 Ytrt ∣ so = s, a0 = a]. The advantage function is written
as Aπ(s, a) , Qπ (s, a) - Vπ(s). In this paper, we consider a fully-cooperative setting where all
agents share the same reward function, aiming to maximise the expected total reward:
∞
J(n) , ES03~P∏,∞,a0ι∞~π	Y rt .
t=0
Throughout this paper, we pay close attention to the contribution to performance from different
subsets of agents. Before proceeding to our methods, we introduce following novel definitions.
2
Published as a conference paper at ICLR 2022
Definition 1. Let i1:m denote an ordered subset {i1, . . . , im} ofN, and let -i1:m refer to its com-
plement. We write ik when we refer to the kth agent in the ordered subset. Correspondingly, the
multi-agent state-action value function is defined as
QTm (s, ai1m) , Ea-ii：m〜n-ii:m ∣Q∏ (s, ai1：m, a-i1：m)]，
and for disjoint sets j1:k and i1:m, the multi-agent advantage function is
Ani:m (s，aj1:k，ail:m) , Q∏∙∙ki1m 6，aj1k，ail:m ) - Qj1:k (s，aj1:k ) .	(1)
Hereafter, the joint policies π = (∏1,..., ∏n) and ∏ = (∏1,..., ∏n) shall be thought of as the
“current”, and the “new” joint policy that agents update towards, respectively.
2.2	Trust Region Algorithms in Reinforcement Learning
Trust region methods such as TRPO (Schulman et al., 2015a) were proposed in single-agent RL
with an aim of achieving a monotonic improvement of J(π) at each iteration. Formally, it can be
described by the following theorem.
Theorem 1. (Schulman et al., 2015a, Theorem 1) Let π be the current policy and π be
the next candidate policy. We define L∏(∏) = J(π) + ES〜ρπ,a〜∏ [A∏(s, a)], DmLx(∏,∏)=
maxs DKL (π(∙∣s), π(∙∣s)). Then the inequality of
J(∏) ≥ Ln(∏) - CDKax(∏,∏)	(2)
holds, where C = 4γ maχ1-YAn(Sa)I.
The above theorem states that as the distance between the current policy ∏ and a candidate policy ∏
decreases, the surrogate Ln (∏), which involves only the current policy,s state distribution, becomes
an increasingly accurate estimate of the actual performance metric J(∏). Based on this theorem, an
iterative trust region algorithm is derived; at iteration k + 1, the agent updates its policy by
πk+1 = arg max Lnk (π) - CDKmLax(πk, π) .
n
Such an update guarantees a monotonic improvement of the policy, i.e., J(πk+1) ≥ J(πk). To im-
plement this procedure in practical settings with parameterised policies πθ, Schulman et al. (2015a)
approximated the KL-penalty with a KL-constraint, which gave rise to the TRPO update of
θk+ι = arg max L∏θiJ∏θ), subject to ES〜ρ∏^ [Dkl (∏θk ,∏θ)] ≤ δ.	(3)
θ	k	θk
At each iteration k + 1, TRPO constructs a KL-ball Bδ(πθk ) around the policy πθk, and optimises
πθk+i ∈ Bδ (πθk) to maximise Lnθ (πθ). By Theorem 1, we know that the surrogate objective
Lnθ (πθ) is close to the true reward J(πθ) within Bδ(πθk ); therefore, πθk leads to improvement.
Furthermore, to save the cost on ES〜ρπ	[DκL(∏θk,∏θ)] when computing Equation (3), Schulman
πθk	k
et al. (2017) proposed an approximation solution to TRPO that uses only first order derivatives,
known as PPO. PPO optimises the policy parameter θk+1 by maximising the PPO-clip objective of
LnPO (πθ)=Es~ρ∏θk ,a』k min( ∏θθ^a∣S) Aπθk(S，a)，clip(∏θ)a∣S)，1 ± B，0(S，a)).	(4)
The clip operator replaces the ratio 姬需)with 1 + e or 1 - e, depending on whether or not the ratio
is beyond the threshold interval. This effectively enables PPO to control the size of policy updates.
2.3	Limitations of Existing Trust Region Methods in MARL
Extending trust region methods to MARL is highly non-trivial. One naive approach is to equip all
agents with one shared set of parameters and use agents’ aggregated trajectories to conduct policy
optimisation at every iteration. This approach was adopted by MAPPO (Yu et al., 2021) in which
the policy parameter θ is optimised by maximising the objective of
LMAppO(nO)= X Es~ρ∏θk ,a~πθk min ( Π[ W) Aπθk (S，a)，clip ( ∏θ,(aijSI)，1± ) Aπθk (S，a)).
=	(5)
3
Published as a conference paper at ICLR 2022
Unfortunately, this simple approach has significant drawbacks. An obvious demerit is that parameter
sharing requires that all agents have identical action spaces, i.e., Ai = Aj , ∀i, j ∈ N, which limits
the class of MARL problems to solve. Importantly, enforcing parameter sharing is equivalent to
putting a constraint θi = θj , ∀i, j ∈ N on the joint policy space. In principle, this can lead to a
suboptimal solution. To elaborate, we demonstrate through an example in the following proposition.
Proposition 1. Let’s consider a fully-cooperative game with an even number of agents n, one state,
and the joint action space {0, 1}n, where the reward is given by r(0n/2, 1n/2) = r(1n/2, 0n/2) = 1,
and r(a1-n) = 0 for all other joint actions. Let J* be the optimal joint reward, and Jshare be the
optimal joint reward under the shared policy constraint. Then
J t	2
share
---=—.
J *	2n
For proof see Appendix B. In the above example, we show that parameter sharing can lead to a
suboptimal outcome that is exponentially worse with the increasing number of agents. We also
provide an empirical verification of this proposition in Appendix F.
Apart from parameter sharing, a more general approach to extend trust region methods in
MARL is to endow all agents with their own parameters, and at each iteration k + 1, agents
construct trust regions of {Bδ(∏ii )}i∈N, and optimise their objectives {L∏θ (∏ii∏--i)}i∈N.
Dk	θk' θ θk '"
Figure 1: Example of a two-
player differentiable game with
r(a1,a2) = a1a2. We initialise
two Gaussian policies with μ1 =
-0.25, μ2 = 0.25. The purple
intervals represent the KL-ball of
δ = 0.5. Individual trust region
updates (red) decrease the joint re-
turn, whereas our sequential up-
date (blue) leads to improvement.
Admittedly, this approach can still be supported by the current
MAPPO implementation (YU et al., 2021) if one turns off pa-
rameter sharing, thus distributing the summation in Equation
(5) to all agents. However, such an approach cannot offer a
rigorous guarantee of monotonic improvement during training.
In fact, agents, local improvements in performance canjointly
lead to a worse outcome. For example, in Figure 1, we design a
single-state differential game where two agents draw their ac-
tions from Gaussian distributions with learnable means μ1, μ2
and unit variance, and the reward function is r(a1 ,a2) = a1a2.
The failure of MAPPO-style approach comes from the fact
that, although the reward function increases in each of the
agents' (one-dimensional) update directions, it decreases in the
joint (two-dimensional) update direction.
Having seen the limitations of existing trust region methods in
MARL, in the following sections, we first introduce a multi-
agent policy iteration procedure that enjoys theoretically-
justified monotonic improvement guarantee. To implement
this procedure, we propose HATRPO and HAPPO algorithms,
which offer practical solutions to apply trust region learning in MARL without the necessity of as-
suming homogeneous agents while still maintaining the monotonic improvement property.
3	Multi-Agent Trust Region Learning
The purpose of this section is to develop a theoretically-justified trust region learning procedure in
the context of multi-agent learning. In Subsection 3.1, we present the policy iteration procedure with
monotonic improvement guarantee, and in Subsection 3.2, we analyse its properties during training
and at convergence. Throughout the work, we make the following regularity assumptions.
Assumption 1. There exists η ∈ R, such that 0 < η 1, and for every agent i ∈ N, the policy
space Πi is η-soft; thatmeans thatfor every πi ∈ Πi, S ∈ S, and ai ∈ Ai, we have πi (ai∣s) ≥ η.
3. 1 Trust Region Learning in MARL with Monotonic Improvement Guarantee
We start by introducing a pivotal lemma which shows that the joint advantage function can be de-
composed into a summation of each agent’s local advantages. Importantly, this lemma offers a
critical intuition behind the sequential policy-update scheme that our algorithms later apply.
Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a
joint policy π, for any state s, and any agent subset i1:m, the below equations holds.
m
An：m (s, ai1:m) = X An (s, ai1jτ ,aij).
j=1
4
Published as a conference paper at ICLR 2022
For proof see Appendix C.2. Notably, Lemma 1 holds in general for cooperative Markov games,
with no need for any assumptions on the decomposibility of the joint value function such as those in
VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018) or Q-DPP (Yang et al., 2020).
Lemma 1 indicates an effective approach to search for the direction of performance improvement
(i.e., joint actions with positive advantage values) in multi-agent learning. Specifically, let agents
take actions sequentially by following an arbitrary order ii：n, assuming agent iι takes an action ai1
such that Ai1 (s, ai1) > 0, and then, for the rest m = 2,..., n, the agent im takes an action aim
such that Aim (s, ai1:m-1, aim) > 0. For the induced joint action a, Lemma 1 assures that A∏θ (s, a)
is positive, thus the performance is guaranteed to improve. To formally extend the above process
into a policy iteration procedure with monotonic improvement guarantee, we need the following
definitions.
Definition 2. Let π be a joint policy, πi1:m-1 = Q31 ∏ij be Some other joint policy of agents
ii：m-i, and πim be some other policy of agent im. Then
Ln:m (πi1：m-1 ,πim) , ES〜ρπ,aii:m-i 〜∏ii:m-i ,aim 〜∏im [Am (s, ai1:mT, aim)].
Note that, for any ∏i1:m-1, We have
Ln:m (πi"mτ ,∏im) = ES〜ρ∏,aii:m--aim〜∏im [AT :小…,aim)]
=ES〜ρπ,aii:m-i〜nii:m-i [Eaim〜∏im [A∏m (s,ai1:mT,aim)]] = 0.	(6)
Building on Lemma 1 and Definition 2, we can finally generalise Theorem 1 of TRPO to MARL.
Lemma 2. Let π be a joint policy. Then, for any joint policy π, we have
n
j (∏) ≥ J (∏) + x [L∏" (∏ ii：m-1 ,∏im) - c D max (∏im ,∏ im)].
m=1
For proof see Appendix C.2. This lemma provides an idea about how a joint policy can be improved.
Namely, by Equation (6), we know that if any agents were to set the values of the above summands
Ln:m (∏i1:m-1, ∏im) 一 CDmLx(∏im, ∏im) by sequentially updating their policies, each of them can
always make its summand be zero by making no policy update (i.e., ∏im = ∏im). This implies
that any positive update will lead to an increment in summation. Moreover, as there are n agents
making policy updates, the compound increment can be large, leading to a substantial improvement.
Lastly, note that this property holds with no requirement on the specific order by which agents
make their updates; this allows for flexible scheduling on the update order at each iteration. To
summarise, we propose the following Algorithm 1. We want to highlight that the algorithm is
Algorithm 1 Multi-Agent Policy Iteration with Monotonic Improvement Guarantee
1:
2:
3:
4:
5:
6:
7:
8:
Initialise the joint policy π0 = (π01, . . . , π0n).
for k = 0, 1, . . . do
Compute the advantage function Aπk (s, a) for all state-(joint)action pairs (s, a).
Compute e = maxs,a ∣A∏k(s, a)| and C =(兰产.
Draw a permutaion i1:n of agents at random.
for m = 1 : n do
Make an update ∏k+ι = argmaxπim ILnkm (∏k+ι-1
end for
, πim - C DKmLax (πkim, πim)i.
9
end for
markedly different from naively applying the TRPO update, i.e., Equation (3), on the joint policy of
all agents. Firstly, our Algorithm 1 does not update the entire joint policy at once, but rather update
each agent’s individual policy sequentially. Secondly, during the sequential update, each agent has a
unique optimisation objective that takes into account all previous agents’ updates, which is also the
key for the monotonic improvement property to hold.
3.2 Theoretical Analysis
Now we justify by the following theorm that Algorithm 1 enjoys monotonic improvement property.
5
Published as a conference paper at ICLR 2022
Theorem 2.	A sequence (πk)k∞=0 of joint policies updated by Algorithm 1 has the monotonic im-
provement property, i.e., J (πk+1) ≥ J(πk) for all k ∈ N.
For proof see Appendix C.2. With the above theorem, we finally claim a successful introduction of
trust region learning to MARL, as this generalises the monotonic improvement property of TRPO.
Moreover, we take a step further to study the convergence property of Algorithm 1. Before stating
the result, we introduce the following solution concept.
Definition 3. In a fuUy-Cooperative game, a joint policy π* = (π1,...,∏n) is a Nash equilibrium
(NE) if for every i ∈ N, πi ∈ Πi implies J (π*) ≥ J (πi, ∏-i).
NE (Nash, 1951) is a well-established game-theoretic solution concept. Definition 3 characterises
the equilibrium point at convergence for cooperative MARL tasks. Based on this, we have the
following result that describes Algorithm 1’s asymptotic convergence behaviour towards NE.
Theorem 3.	Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to
begin the update, a sequence (πk)k∞=0 of joint policies generated by the algorithm, in a cooperative
Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium.
For proof see Appendix C.3. In deriving this result, the novel details introduced by Algorithm 1
played an important role. The monotonic improvement property (Theorem 2), achieved through the
multi-agent advantage and the sequential update scheme, provided us with a guarantee on the con-
vergence of the return. Furthermore, randomisation of the update order assured that, at convergence,
none of the agents is incentified to make an update. The proof is finalised by excluding a possibility
that the algorithm converges at non-equilibrium points.
4 Practical Algorithms
When implementing Algorithm 1 in practice, large state and action spaces could prevent agents from
designating policies ∏i(∙∣s) for each state S separately. To handle this, We parameterise each agent's
policy πθi i by θi, which, together with other agents’ policies, forms a joint policy πθ parametrised
by θ = (θ1, . . . , θn). In this section, We develop tWo deep MARL algorithms to optimise the θ.
4.1 HATRPO
Computing DKmLax πθimim , πθimim in Algorithm 1 is challenging; it requires evaluating the KL-
k
divergence for all states at each iteration. Similar to TRPO, one can ease this maximal KL-
divergence penalty DKmLax πimim , πθimim by replacing it With the expected KL-divergence constraint
ES〜Pnek [dkl(∏imm (∙∣s),∏θmm (∙∣s))] ≤ δ where δ is a threshold hyperparameter, and the expectation
can be easily approximated by stochastic sampling. With the above amendment, We propose practi-
cal HATRPO algorithm in which, at every iteration k + 1, given a permutation of agents i1:n, agent
im∈{1,...,n} sequentially optimises its policy parameter θkim+1 by maximising a constrained objective:
im
θk+1
arg maχ ES	：m-i —1：m-1	0
θim	ρπθk ,	,iLm-1 ,	θim
θk+1
Aiπmθk (s, ai1:m-1, aim ),
SUbjeCttOES〜ρ∏θjDKL(/mm“S),πθmmɑS))] ≤ &
k	θk
(7)
To compUte the above eqUation, similar to TRPO, one can apply a linear approximation to the
objective fUnction and a qUadratic approximation to the KL constraint; the optimisation problem in
EqUation (7) can be solved by a closed-form Update rUle as
θk+ι = θkm + αj. (H 2? .i... (Hkm )-1gkm,	(8)
where Hikm =	ES〜ρ∏θ% [Dkl(∏Θ∕ (∙∣s),∏θmm (∙∣s))]∣θim=瞰 is the Hessian of the expected KL-
divergence, gkim is the gradient of the objective in EqUation (7), αj < 1 is a positive coefficient that
is foUnd via backtracking line search, and the prodUct of (Hkim )-1gkim can be efficiently compUted
with conjUgate gradient algorithm.
The last missing piece for HATRPO is to estimate E aG∙m-ι "m-i aim Mm Anmm (s, ai1:m-1, aim),
〜θk + 1	,	〜θim
which poses new challenges becaUse each agent’s objective has to take into accoUnt all previoUs
6
Published as a conference paper at ICLR 2022
agents’ updates, and the size of input vaires. Fortunately, with the following proposition, we can
efficiently estimate this objective by employing a joint advantage estimator.
Proposition 2. Let π = jn=1 πij be a joint policy, and Aπ (s, a) be its joint advantage function.
Let π i1:m-1 = Qjm-11 ∏ij be some other joint policy OfagentS ii：m-i, and ∏ im be some other policy
of agent im . Then, for every state s,
EaiLm—1 〜∏ i1：m —1 ,aim 〜∏im Aiπm s, ai1:m-1, aim
hz πim(aiml S)
=a〜π	∏im(aim |s)
亓 il:m-l
nil:m- 1
*") An(s，a)]⑼
(ai1:m-1 |s)
For proof see Appendix D.1. One benefit of applying Equation (9) is that agents only need to main-
tain a joint advantage estimator A∏ (s, a) rather than one centralised critic for each individual agent
(e.g., unlike CTDE methods such as MADDPG). Another practical benefit one can draw is that,
given an estimator A(s, a) of the advantage function Aπθ (s, a), for example GAE (Schulman et al.,
2015b), we can estimate E i
a 1:
:m-1 〜∏%m-1 m 5m [^^ (：，£,-1 , /m )] Withanestimatorof
i1:m-1 ,	θim
θk+1
nθmfmjs) - H i1：m(% N,	where M …=Π ;l Ol：)才他孙
(10)
Notably, Equation (10) aligns nicely with the sequential update scheme in HATRPO. For agent im ,
since previous agents i1:m-1 have already made their updates, the compound policy ratio for M i1:m
in Equation (10) is easy to compute. Given a batch B of trajectories with length T , we can estimate
the gradient with respect to policy parameters (derived in Appendix D.2) as follows,
1T
gkm = JB £ EM i1：m (S t，at)Vθim log πθmm (at| St)I θim =θim ∙
lBl τ∈B t=0	k
The term -1 ∙ Mi1：m (s, a) of Equation (10) is not reflected in gkm, as it only introduces a constant
with zero gradient. Along with the Hessian of the expected KL-divergence, i.e., Hkim, we can update
θkim+1 by following Equation (8). The detailed pseudocode of HATRPO is listed in Appendix D.3.
4.2	HAPPO
To further alleviate the computation burden from Hkim in HATRPO, one can follow the idea of PPO
in Equation (4) by considering only using first order derivatives. This is achieved by making agent
im choose a policy parameter θkim+1 which maximises the clipping objective of
ES~ρ∏θk ,a~πθk
min
(πθ黑(ai|s) Mi1：m(s，a)，clip(πθ黑(ai|s)
πθm (ails)	'，八	∏m (ai∣s)
1 ±	M i1：m (s， a)
!#
(11)
The optimisation process can be performed by stochastic gradient methods such as Adam (Kingma
& Ba, 2014). We refer to the above procedure as HAPPO and Appendix D.4 for its full pseudocode.
4.3	Related Work
We are fully aware of previous attempts that tried to extend TRPO/PPO into MARL. Despite em-
pirical successes, none of them managed to propose a theoretically-justified trust region protocol
in multi-agent learning, or maintain the monotonic improvement property. Instead, they tend to
impose certain assumptions to enable direct implementations of TRPO/PPO in MARL problems.
For example, IPPO (de Witt et al., 2020a) assume homogeneity of action spaces for all agents and
enforce parameter sharing. Yu et al. (2021) proposed MAPPO which enhances IPPO by consider-
ing a joint critic function and finer implementation techniques for on-policy methods. Yet, it still
suffers similar drawbacks of IPPO due to the lack of monotonic improvement guarantee especially
when the parameter-sharing condition is switched off. Wen et al. (2021) adjusted PPO for MARL
by considering a game-theoretical approach at the meta-game level among agents. Unfortunately, it
can only deal with two-agent cases due to the intractability of Nash equilibrium. Recently, Li & He
(2020) tried to implement TRPO for MARL through distributed consensus optimisation; however,
they enforced the same ratio ∏i(ai∣s)∕πi(ai∣s) for all agents (see their Equation (7)), which, similar
7
Published as a conference paper at ICLR 2022
(a) 2c-vs-64zg (hard)
(b) 3s5z (hard)
(c) corridor (super hard)
Figure 2: Performance comparisons between HATRPO/HAPPO and MAPPO on three SMAC tasks.
Since all methods achieve 100% win rate, we believe SMAC is not sufficiently difficult to discrimi-
nate the capabilities of these algorithms, especially when non-parameter sharing is not required.
to parameter sharing, largely limits the policy space for optimisation. Moreover, their method comes
with a δ∕n KL-Constraint threshold that fails to consider scenarios with large agent number.
One of the key ideas behind our HATRPO/HAPPO is the sequential update scheme. A similar
idea of multi-agent sequential update was also discussed in the context of dynamic programming
(Bertsekas, 2019) where artificial “in-between” states have to be considered. On the contrary, our
sequential update sceheme is developed based on Lemma 1, which does not require any artificial as-
sumptions and hold for any cooperative games. Furthermore, Bertsekas (2019) requires to maintain
a fixed order of updates that is pre-defined for the task, whereas the order in HATRPO/MAPPO can
be randomised at each iteration, which also offers desirable convergence property, as stated in Propo-
sition 3 and also verified through ablation studies in Appendix F. The idea of sequential update also
appeared in principal component analysis; in EigenGame (Gemp et al., 2020) eigenvectors, repre-
sented as players, maximise their own utility functions one-by-one. Although EigenGame provably
solves the PCA problem, it is of little use in MARL, where a single iteration of sequential updates
is insufficient to learn complex policies. Furthermore, its design and analysis rely on closed-form
matrix calculus, which has no extension to MARL.
Lastly, we would like to highlight the importance of the decomposition result in Lemma 1. This
result could serve as an effective solution to value-based methods in MARL where tremendous
efforts have been made to decompose the joint Q-function into individual Q-functions when the
joint Q-function are decomposable (Rashid et al., 2018). Lemma 1, in contrast, is a general result
that holds for any cooperative MARL problems regardless of decomposibility. As such, we think of
it as an appealing contribution to future developments on value-based MARL methods.
5	Experiments and Results
We consider two most common benchmarks—StarCraftII Multi-Agent Challenge (SMAC)
(Samvelyan et al., 2019) and Multi-Agent MuJoCo (de Witt et al., 2020b)—for evaluating MARL
algorithms. All hyperparameter settings and implementations details can be found in Appendix E.
StarCraftII Multi-Agent Challenge (SMAC). SMAC contains a set of StarCraft maps in which
a team of ally units aims to defeat the opponent team. IPPO (de Witt et al., 2020a) and MAPPO
(Yu et al., 2021) are known to achieve supreme results on this benchmark. By adopting parameter
sharing, these methods achieve a winning rate of 100% on most maps, even including the maps that
have heterogeneous agents. Therefore, we hypothesise that non-parameter sharing is not necessarily
required and the trick of sharing policies is sufficient to solve SMAC tasks. We test our methods
on two hard maps and one super-hard; results on Figure 2 confirm that SMAC is not sufficiently
difficult to show off the capabilities of HATRPO/HAPPO when compared against existing methods.
Multi-Agent MuJoCo. In comparison to SMAC, we believe Mujoco enviornment provides a more
suitable testing case for our methods. MuJoCo tasks challenge a robot to learn an optimal way
of motion; Multi-Agent MuJoCo models each part of a robot as an independent agent, for exam-
ple, a leg for a spider or an arm for a swimmer. With the increasing variety of the body parts,
modelling heterogeneous policies becomes necessary. Figure 3 demonstrate that, in all scenarios,
HATRPO and HAPPO enjoy superior performance over those of parameter-sharing methods: IPPO
and MAPPO, and also outperform non-parameter sharing MADDPG (Lowe et al., 2017b) both in
terms of reward values and variance. It is also worth noting that the performance gap between HA-
TRPO and its rivals enlarges with the increasing number of agents. Meanwhile, we can observe that
HATRPO outperforms HAPPO in almost all tasks; we believe it is because the hard KL constraint
8
Published as a conference paper at ICLR 2022
48。∙
3000-
Ant 2x4	Ant 4x2	Ant 8xl
二2∞C
0.0	02	0.4	06	0.8	1.0
Environment steps u7
0.0	02	0.4	06	0.8	1.0
Environment steps lβ1
(c) 8x1-Agent Ant
HaIfCheetah 6xl
(a) 2x4-Agent Ant
HaIfCheetah 2×3
0Λ 0.2	0.4	0.6	0∙β	1.0
Environment steps lβ7
(d) 2x3-Agent HalfCheetah
0Λ 0.2	0.4	0.6	0∙β	1.0
Environment steps lβ7
(b) 4x2-Agent Ant
HaIfCheetah 3×2
0.0	02	0.4	06	0.8	1.0
Environment steps u7
0.0	02	0.4	06	0.8	1.0
Environment steps	lβ1
⑴ 6x1-Agent HalfCheetah
Walker 6xl
5000
P
rδ
M «oo-
-O 30∞
S
lS-2000
ω
6
S IOOO
ω
<
o
Walker 2x3
141
1«7
0.0	02	0.4 OS 0.8
Environment steps
(g) 2x3-Agent Walker
(e) 3x2-Agent HalfCheetah
Walker 3x2
1 J IR Il ɪ .
p,l",M3α3po-d 山
0Λ 0.2	0.4	0.6	0∙β	1.0
Environment steps	u7
(h) 3x2-Agent Walker
HATRPO
HAPPO
MAPPO
——IPPO
MAODPG
Humanoid 17xl
HumanoidStandup 17xl
0.0	02	0.4	0.8	1.0
Environment steps
0.0	02	0.4	06	0.8	1.0
Environment steps	u7
(i) 6x1-Agent Walker
0Λ 0.2	0.4	0.6	0∙β	1.0
Environment steps lβ7
(j) 17x1-Agent Humanoid
PnBtf 3po-d 山 ωrasω><
Environment steps
(l) 10x2-Agent Swimmer
(k) 17x1-Agent HumanoidStandup

3po-d 山 ωrasω><


Figure 3: Performance comparison on multiple Multi-Agent MuJoCo tasks. HAPPO and HATRPO
consistently outperform their rivals, thus establishing a new state-of-the-art algorithm for MARL.
The performance gap enlarges with increasing number of agents.
in HATRPO, compared to the clipping version in HAPPO, relates more closely to Algorithm 1 that
attains monotonic improvement guarantee.
6	Conclusion
In this paper, we successfully apply trust region learning to multi-agent settings by proposing the first
MARL algorithm that attains theoretically-justified monotonical improvement property. The key to
our development is the multi-agent advantage decomposition lemma that holds in general with no
need for any assumptions on agents sharing parameters or the joint value function being decompos-
able. Based on this, we introduced two practical deep MARL algorithms: HATRPO and HAPPO.
Experimental results on both discrete and continuous control tasks (i.e., SMAC and Multi-Agent
Mujoco) confirm their state-of-the-art performance. For future work, we will consider incorporating
the safety constraint into HATRPO/HAPPO and propose rigorous safety-aware MARL solutions.
9
Published as a conference paper at ICLR 2022
References
Dimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning. arXiv preprint
arXiv:1910.00120, 2019.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multia-
gent systems. AAAI/IAAI, 1998(746-752):2, 1998.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020a.
Christian Schroder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip H. S. Torr, Wendelin
Bohmer, and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized Con-
tinuous cooperative control. CoRR, abs/2003.06709, 2020b.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International conference on machine learning,
pp. 1329-1338. PMLR, 2016.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: Pca as a nash
equilibrium. arXiv preprint arXiv:2010.00554, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1861-1870. PMLR, 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang,
David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv
preprint arXiv:2108.08612, 2021.
Hepeng Li and Haibo He. Multi-agent trust region policy optimization. arXiv e-prints, pp. arXiv-
2010, 2020.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 6382-6393, 2017a.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 6382-6393, 2017b.
A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra.
Benchmarking reinforcement learning algorithms on real-world robots. In Conference on robot
learning, pp. 561-591. PMLR, 2018.
David H Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen, Joel Jen-
nings, and Jun Wang. Learning in nonzero-sum stochastic games with potentials. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 7688-7699. PMLR, 18-24 Jul
2021.
10
Published as a conference paper at ICLR 2022
John Nash. Non-cooperative games. Annals of mathematics, pp. 286-295, 1951.
P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, and J Wang. Multiagent bidirectionally-
coordinated nets for learning to play starcraft combat games. arxiv 2017. arXiv preprint
arXiv:1703.10069, 2017.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.
The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, F. Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv, abs/1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387-395. PMLR, 2014.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085-2087, 2018.
R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems 12,
volume 12, pp. 1057-1063. MIT Press, 2000.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. In International Conference on Learning Representations,
2018.
Ying Wen, Yaodong Yang, and Jun Wang. Modelling bounded rationality in multi-agent interactions
by generalized recursive reasoning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 414-421. International
Joint Conferences on Artificial Intelligence Organization, 7 2020. Main track.
Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A game-
theoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828,
2021.
Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Hang Su, and
Jun Zhu. Tianshou: a highly modularized deep reinforcement learning library. arXiv preprint
arXiv:2107.14171, 2021.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theo-
retical perspective. arXiv preprint arXiv:2011.00583, 2020.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580.
PMLR, 2018.
11
Published as a conference paper at ICLR 2022
Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp.
10757-10766. PMLR, 2020.
Chao Yu, A. Velu, Eugene Vinitsky, Yu Wang, A. Bayen, and Yi Wu. The surprising effectiveness
of mappo in cooperative, multi-agent games. ArXiv, abs/2103.01955, 2021.
Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun
Wang. Bi-level actor-critic for multi-agent coordination. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pp. 7325-7332, 2020.
Ming Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen, Yaodong Yang,
Weinan Zhang, and Jun Wang. Malib: A parallel framework for population-based multi-agent
reinforcement learning. arXiv preprint arXiv:2106.07551, 2021.
12
Published as a conference paper at ICLR 2022
Appendices
A	Preliminaries	14
A.1 Definitions and Assumptions .......................................... 14
A.2 Proofs of Preliminary Results ........................................ 14
B	Proof of Proposition 1	16
C	Derivation and Analysis of Algorithm 1	17
C.1 Recap of Existing Results ............................................ 17
C.2 Analysis of Training of Algorithm 1 .................................. 17
C.3 Analysis of Convergence of Algorithm 1 ............................... 19
D HATRPO and HAPPO	22
D.1 Proof of Proposition 2 ............................................... 22
D.2 Derivation of the gradient estimator for HATRPO ...................... 22
D.3 Pseudocode of HATRPO ................................................. 23
D.4 Pseudocode of HAPPO .................................................. 24
E	Hyper-parameter Settings for Experiments	24
F	Ablation Experiments	26
G	Ablation Study on Non-Parameter Sharing MAPPO/IPPO	26
H	Multi-Agent Particle Environment Experiments	27
13
Published as a conference paper at ICLR 2022
A	Preliminaries
A.1 Definitions and Assumptions
Assumption 1. There exists η ∈ R, such that 0 < η 1, and for every agent i ∈ N, the policy
space Πi is η-soft; that means that for every πi ∈ Πi, S ∈ S, and ai ∈ Ai, we have πi(ai∣s) ≥ η.
Definition 3. In a fully-cooperative game, a joint policy π* = (π1,...,∏n) is a Nash equilibrium
(NE) if for every i ∈ N, πi ∈ Πi implies J (π*) ≥ J (πi, ∏-i).
Definition 4. Let X be a finite set and p : X → R, q : X → R be two maps. Then, the notion of
distance between p and q that we adopt is given by ||p - q|| , maxx∈X |p(x) - q(x)|.
A.2 Proofs of Preliminary Results
Lemma 3. Every agent i’s policy space Πi is convex and compact under the maximum norm.
Proof. We start from proving convexity od the policy space: we prove that, for any two policies
πi, πi ∈ Πi, for every α ∈ [0,1], απi + (1 — α)πi ∈ Πi. Clearly, for all s ∈ S and ai ∈ Ai, We
have
απi(ai∣s) + (1 — α)πi(ai∣s) ≥ αη + (1 — α)η = η.
Also, for every state s,
^X 卜πi(ai∣s) + (1 — a)ni(ai|s)i = a^X πi(a7,∣s) + (1 — a) ^X π7,(a7,∣s) = α + 1 — α = 1,
ai	ai	ai
Which establishes convexity.
For compactness, We first prove that Πi is closed.
Let πki k∞=0 be a convergent sequence of policies of agent i. Let πi be the limit. We Will prove that
πi is a policy. First, by Assumption 1, for any k ∈ N, s ∈ S, and ai ∈ Ai, We have πki ai|s ≥ η.
Hence,
πi (ai∣s) = lim ∏k (ai∣s) ≥ lim η ≥ η.
k→∞	k→∞
Furtheremore, for any k and s, We have Pai πki ai|s = 1. Hence
^X πi (ai∣s) = ^X lim πi (ai∣s) = lim ^X πi (ai∣s) = lim 1 = 1.
ai	ai	ai
With these tWo conditions satisfied, πi is a policy, Which proves the closure.
Further, for all policies πi, states S and actions a, ∣πi(ai∣s)∣ ≤ 1. This means that ∣∣∏i∣∣max ≤ 1,
which proves boundedness. Hence, Πi is compact.	□
Lemma 4 (Continuity of ρπ ). The improper state distribution ρπ is continuous in π.
Proof. First, let us show that for any t ∈ N, the distribution ρtπ is continous in π. We will do it
by induction. This obviously holds when t = 0, as ρ0 does not depend on policy. Hence, we can
assume that for some t ∈ N, the distribution ρtπ is continuous in π. Let us now consider two policies
∏ and ∏. Let s0 ∈ S. We have
加 ∏+1(SO) - ρ∏+1(s0 )∣ = X ρ∏ (S) X π⑷S)P (SOBa) - X ρ∏ (S) X π⑷S)P(Sls,a)
= X X
[ρ∏(S)∏(a∣S) — ρ∏(S)π(a∣S)] P(SlS,a)|
≤ XX ∣P∏(S)∏(a∣S) — ρ^(S)∏(a∣S)∣ P(SlS,a)
s
a
14
Published as a conference paper at ICLR 2022
=ΣΣ∣P∏(s)∏(α∣s) - Ptn(s)∏(α∣s)+ Ptn(s)∏(α∣s) - Ptn(s)π(α∣s)∣ P(Sls,a)
≤ XX(Pn(S) ∣π⑷S)- π⑷S)I+ π⑷S)I Pn(S)-Pin(S) ∣ )
s a
≤ XX(Pn (S)IIn- π∣∣+π⑷S)IIPn -Ptit ∣∣)
sa
=XPn(S) X ∣∣π- π∣∣+ X I∣P∏ - PnIIXπ⑷S)
sa	s	a
= ∣A∣∙∣∣π - π∣∣ + ∣s∣∙∣∣pΠ - p∏ ∣∣.
Hence, We obtain
l∣ρ∏+1 - ρ∏+1ll ≤ ∣A∣∙∣I∏ - π Il + ∖s∖∙∖∖p∏ - p∏∣∣.	(K)
Using the base case, taking the limit as π → π, we get that the right-hand-side of Equation (12)
converges to 0, which proves that every ρ∏+1 is continuous in π, and finishes the inductive step.
We can now prove that the total marginal state distribution ρ∏ is continous in π. To do that, let's
log[ e(1-γ)]
take an arbitrary e > 0, and a natural T such that T > -Iog^-i. Equivalently, we choose T such
that 2γγ:Y < W. Let π and π be two policies. We have
∞
∣pn ⑹-Pn (S)I= X Yt (pn (S)-Pn (S))
t = 0
T — 1	∞
=X Yt (Pn(s) - Pn (S)) + Tr X Yt-T (Pn(s) - Pn(S))
t=0	t=T
T — 1	∞
≤ X Yt I Pn (s) - Pn(S) I + YT X Yt-TI Pn(S) -Ptn (s) I
t=0	t=T
T — 1	∞
≤ X Yt ∣ pn(S)-Ptn(S)I + Yt X 2Yt-T
t=0	t=T
X Yt i pn (s) - ρ^ (S)I+ ⅛;
t=0	- Y
T —1
< X Yt 1 Pn(s) - Pn(S)I + I
t=0	2
T —1	T — 1
≤ X i Pn (s)- ρ^ (S)I+ 2 ≤ X ∣∣p∏ - ρ^ ∣∣+ 2.	(13)
t=0	t=0
Now, by continuity of each of Pn, for t = 0,1,..., T - 1, we have that there exists a δ > 0 such that
∣∣π - π∣∣ < δ implies ∣∣ρ∏ - ρ∏∣∣ < 务.Taking such δ, by Equation (13), we get ∣∣ρ∏ - Pn∣∣ ≤ e,
which finishes the proof.
Lemma 5 (Continuity of Q∏). Let π be a policy. Then Q∏ (s, a) is Lipschitz-continuous in π.
Proof. Let π and π be two policies. Then we have
IQn(S, a) - Qn(s,a)|
=I I r(s,a) + YEEP(SiS,a)n(a|S)QnGa) I - I r(s,a) + YEEP(SiS, a)n(a|S)QnGa)
s0 a	s ∖	s0 a
=γ∣∑∑P(s0∣s, a) [π(a∣S)Qn(s, a) - π(a∣S)Qn(s, a)] I
∣ s0 a
≤ YΣΣP(s0∣s, a) ∣π(a∣s)Qn(s, a) - π(a∣s)Qn(s, a)∣
s0 a
=γ∑∑P(s0∣s, a) ∣π(a∣s)Qn(s, a) - π(a∣s)Qn(s, a) + π(a∣s)Qn(s, a) - π(a∣s)Qn(s, a)∣
s0 a
≤ Y XXP(s0∣s, a) (∣π(a∣s)Qn(s, a) - π(a∣s)Qn(s, a)∣ + ∣π(a∣s)Qn(s, a) - π(a∣s)Qn(s, a)∣)
s0 a
15
Published as a conference paper at ICLR 2022
=γ	P(s0∣s,a)∣∏(a∣s) - ∏(a∣s)∣ ∙ ∣Q∏(s,a)∣
s0 a
+γ XX
P(s0∣s, a)∏(a∣s) ∣Q∏(s,a) - Qn(s,a)∣
s0 a
≤ Y XX P(SlS,a)||n - n|| ∙ Qmax
s0 a
+γ XX
P (s0∣s,a)∏(a∣s)∣∣Q∏ - Qn ||
s0 a
=YQmax ∙∣A∣∙∣I∏ - ∏ll + YlIQn - Qn ||
Hence, we get
IIQn - QnIl ≤ YQmax ∙ ∣A∣∙ ∣∣∏ - ∏∣∣ + γ∣∣Q∏ - QnII,
which implies
IIQn - Q^II≤ YQmax ∖iAHIn - π”.	(14)
1-Y
Equation (14) establishes LiPschitz-ContinUity with a constant Y Ql-γlAl.	口
Corollary 1. From Lemma 5 we obtain that the following functions are Lipschitz-continuous in π:
the state value function Vn =	a π(aIS)Qn (S, a),
the advantage function An (S, a) = Qn (S, a) - Vn (S),
and the expected total reward J(π) = Es~p。[Vn (s)].
Lemma 6. Let π and π be policies. The quantity Es~ρπ ,a~n [An (s, a)] is continuous in π.
Proof. We have
Es~ρ∏ ,a~n [An (s, a)] = ΣΣPn(s)Π(a∣s)An(s, a).
Continuity follows from continuity of each Pn (s) (Lemma 4) and An (s, a) (Corollary 1). 口
Corollary 2 (Continuity in MARL). All the results about continuity in π extend to MARL. Policy π
can be replaced with joint policy π; as π is Lipschitz-continuous in agent i’s policy πi, the above
continuity results extend to conitnuity in πi. Thus, we will quote them in our proofs for MARL.
B Proof of Proposition 1
Proposition 1. Let’s consider a fully-cooperative game with an even number of agents n, one state,
and the joint action space {0, 1}n, where the reward is given by r(0n/2, 1n/2) = r(1n/2, 0n/2) = 1,
and r(a1:n) = 0 for all other joint actions. Let J * be the optimal joint reward, and Jshare be the
optimal joint reward under the shared policy constraint. Then
J 2
share
—T = .
J *	2n
Proof. Clearly J* = 1. An oPtimal joint Policy in this case is, for examPle, the deterministic Policy
with joint action (0n/2, 1n/2).
Now, let the shared Policy be (θ, 1 -θ), where θ determines the Probability that an agent takes action
0. Then, the exPected reward is
J(θ) = Pr (a1:n = (0n∕2,1n∕2)) ∙ 1 + Pr (a1:n = (1n/2, 0n/2)) ∙ 1 = 2 ∙ θn∕2(1 - θ)n/2.
16
Published as a conference paper at ICLR 2022
In order to maximise J(θ), We must maximise θ(1 - θ), or equivalently, ,θ(1 - θ). By the
artithmetic-geometric means inequality, we have
√θ(1 - θ) ≤
θ +(1 - θ)
2
1
2
where the equality holds if and only if θ = 1 - θ, that is θ = 1. In such case we have
Jshare = J
2 ∙ 2-n/2 ∙ 2-n/2 = _2_
2n,
which finishes the proof.
□
C Derivation and Analysis of Algorithm 1
C.1 Recap of Existing Results
Lemma 7 (Performance Difference). Let π and π be two policies. Then, the following identity
holds,
J(∏) - J(∏) = ES〜ρ∏,a〜∏ [A∏(s, a)].
Proof. See Kakade & Langford (2002) (Lemma 6.1) or Schulman et al.(2015a) (APPendiX A). □
Theorem 1. (Schulman et al., 2015a, Theorem 1) Let π be the current policy and π be
the next candidate policy. We define L∏(∏) = J(π) + ES〜ρπ,a〜∏ [A∏(s, a)], DmLx(∏,∏)=
maxs DKL (π(∙∣s),∏(∙∣s)). Then the inequality of
J(∏) ≥ Ln(∏) - CDKLx(∏,∏)	(2)
holds, where C = 4γ maχ1-YAπ (S，a)1.
Proof. See Schulman et al. (2015a) (Appendix A and Equation (9) of the paper).	□
C.2 Analysis of Training of Algorithm 1
Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a
joint policy π, for any state s, and any agent subset i1:m, the below equations holds.
m
A*m (s, ai1:m) = X An (s, ai1jτ,aij).
j=1
Proof. By the definition of multi-agent advantage function,
A∏θm(s, ai1:m) = Q∏θm(s, ai1:m) - V∏θ (S)
m
=X [Q∏θk (s, ai1:k) - Q∏θk-1 (s, ai1*τ)]
k=1
m
= X Aiπkθ (s, ai1:k-1, aik),
k=1
which finishes the proof.
Note that a similar finding has been shown in KUba et al. (2021).	□
Lemma 8. Let π = "21 ∏i and ∏ = IIn=I ∏i be joint policies. Then
n
DKax (∏, ∏) ≤ X DKL (∏i,∏i)
i=1
17
Published as a conference paper at ICLR 2022
Proof. For any state s, we have
DKL (∏(∙∣s), ∏(∙∣s)) = Ea〜∏ [logπ(a∣s) - logπ(a|s)]
Ea〜π
- log
n
Y ∏i(ai∣s)
i=1
))
nn
Ea〜∏ XlogΠi(ai∣s) - Xlog∏i(ai|s)
i=1	i=1
nn
XEai〜∏i,a-i〜∏-i [log∏i(ai∣s) - log∏i(ai∣s)] = XDKL (∏i(∙∣s),∏i(∙∣s)).
i=1	i=1
Now, taking maximum over s on both sides yields
n
DmLx (∏, ∏)≤ X Dmax (∏i,∏ i),
i=1
as required.
□
Lemma 2. Let π be a joint policy. Then, for any joint policy π, we have
n
J(∏) ≥ J(π) + X [L∏-m (∏i1:m-1 ,πim) - CDmLx(πim ,πim)].
m=1
Proof. By Theorem 1
J(∏) ≥ Ln (∏)- CDmLx(∏, ∏)
=J(∏) + Es〜ρ∏,a〜π [An(s, a)] - CDmLx(∏, ∏)
which by Lemma 1 equals
n
=J(∏) + ES〜ρ∏,a〜n X Anm (s, ai1:m-1, aim) - CDmLx(∏, ∏)
m=1
and by Lemma 8 this is at least
n
≥ J(∏) + ES〜ρ∏,a〜n	X Anm (s, ai1:m-1, aim)
m=1
n
-X CDmLx(∏im,Πim )
m=1
nn
J(∏) + X ES〜ρ∏,ai-1 〜n~m-1,aim〜∏im [An" (s,62,aim)] - X CDmaX(∏im"im)
m=1	m=1
n
J(∏) + X (Ln:m(亓i1：m-1 ,∏im) - CDmax(∏im ,∏im)).
m=1
□
Theorem 2. A sequence (πk)k∞=0 of joint policies updated by Algorithm 1 has the monotonic im-
provement property, i.e., J (πk+1) ≥ J(πk) for all k ∈ N.
Proof. Let π0 be any joint policy. For every k ≥ 0, the joint policy πk+1 is obtained from πk by
Algorithm 1 update; for m = 1, . . . , n,
∏k+ι = argmax [心/ (W+m-1 ,∏im) - CDmaX (∏km,∏im)].
πim
18
Published as a conference paper at ICLR 2022
By Theorem 1, we have
J (πk+1) ≥ Lπk (πk+1) - CDKmLax(πk, πk+1),
which by Lemma 8 is lower-bounded by
n
≥Lπk(πk+1)- XCDKmLax(πkim,πkim+1)
m=1
n
=J (∏k)+ x (Lnkm (∏k+m-ι,∏k+ι)- CDmax(∏km,∏k+ι)),	(15)
m=1
and as for every m, πkim+1 is the argmax, this is lower-bounded by
n
≥ J (∏k) + X (Lnkm (∏k+m-ι,∏km)- CDmLx(∏km ,∏km)),
m=1
which, as mentioned in Definition 2, equals
n
= J(πk) + X0=J(πk),
m=1
where the last inequality follows from Equation (6). This proves that Algorithm 1 achieves mono-
tonic improvement.	□
C.3 Analysis of Convergence of Algorithm 1
Theorem 3. Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to
begin the update, a sequence (πk)k∞=0 of joint policies generated by the algorithm, in a cooperative
Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium.
Proof. Step 1 (convergence). Firstly, it is clear that the sequence (J (πk))k∞=0 converges as, by
Theorem 2, it is non-decreasing and bounded above by R-mγx. Let Us denote the limit by J. For every
k, we denote the tuple of agents, according to whose order the agents perform the sequential updates,
by i1k:n, and we note that i1k:n k∈N is a random process. Furthermore, we know that the sequence
of policies (πk ) is bounded, so by Bolzano-Weierstrass Theorem, it has at least one convergent
subsequence. Let π be any limit point of the sequence (note that the set of limit points is a random
set), and ∏kkj ∞=0b be a subsequence converging to π (which is a random subsequence as well). By
continuity of J in π (Corollary 1), we have
J(∏) = J l lim ∏kj = lim J (∏kj) = J.	(16)
j→∞	j j→∞ j
For now, we introduce an auxiliary definition.
Definition 5 (TR-Stationarity). A joint policy ∏ is trust-region-stationary (TR-Stationary) if, for
every agent i,
∏i = argmax 瓜〜ρ∏[A∏(s, ai)] - CnD.(∏i,πi)],
πi
where C∏ =(3^2, and e = maxs,a ∣A∏(s, a)|.
We will now establish the TR-stationarity of any limit point joint policy ∏ (which, as stated above,
is a random variable). Let Ei,- [∙] denote the expected value operator under the random process
(i1∞). Let also ek = maxs,a ∣A∏fc (s, a)|, and Ck =港市.We have
0 = lim Ei?— [J(∏k+ι) — J(∏k)]
k→∞	1:n
≥ lim Ei?- [L∏k (∏k+ι) — CkDmLx(∏k, ∏k+ι)] by Theorem 1
k→∞	1:n
≥ kι→∞ %∞ [LI 卜 k+ι) - Ck Dmax (∏kk ,∏k+J]
by Equation (15) and the fact that each of its summands is non-negative
19
Published as a conference paper at ICLR 2022
Now, We consider an arbitrary limit point π from the (random) limit set, and a (random) subsequence
(∏kj )∞=o that converges to π. We get
0 ≥ jlim∞Eii- [L∏kζ (∏kk+ι) - CkjDmLX 卜kkj, ∏kk+ι).
As the expectation is taken of non-negative random variables, and for every i ∈ N and k ∈ N, with
some positive probability pi , we have i1kj = i (because every permutation has non-zero probability),
the above is bounded from below by
pi jl→im∞ maix hLiπkj(πi) - Ckj DmmLax πkij,πii ,
which, as π% converges to π, equals to
Pi max [L∏(πi) - CnDmax (∏i, ∏i)] ≥ 0, by Equation (6).
πi
For convergence of Liπk (πi ) we used Definition 2 combined with Lemma 6, for convergence
of Ckj we used Corollary 1, and the convergence of DmmLax follows from continuity of DmL and
max. This proves that, for any limit point π of the random process (∏k) induced by Algorithm 1,
max∏i [L∏(πi) - CnDmax (∏i, ∏i)] = 0, which is equivalent with Definition 5.
Step 2 (dropping the penalty term). Now, we have to prove that TR-stationary points are NEs of
cooperative Markov games. The main step is to prove the following statement: a TR-stationary joint
policy ∏, for every state S ∈ S, satisfies
πi = arg max Eai_∏i [A∏ (s, ai)].	(17)
πi
We will use the technique of the proof by contradiction. Suppose that there is a state s0 such that
there exists a policy ∏i with
Eai~∏i [A%(s0, ai)] > Eai~∏i [A∏π(s0, ai)] .	(18)
Let us parametrise the policies πi according to the template
di-1
π’([sO) = (xι,... , xdi-1, 1 - X xij
j=1
where the values of xj (j = 1,...,di - 1) are such that ∏i(∙∣so) is a valid probability distribution.
Then we can rewrite our quantity of interest (the objective of Equation (17) as
di-1	di-1
Eai~∏i [An(so, ai)] = X xj∙ A∏(s0,aj) + (1- X Xh)An (S0,adi)
j=1	h=1
di-1
X χj[A∏(s0,aj)- An(S0,adi)] + An(S0,adi),
j=1
which is an affine function of the policy parameterisation. It follows that its gradient (with respect to
xi) and directional derivatives are constant in the space of policies at state S0. The existance of policy
πi(∙∣so), for which Inequality (18) holds, implies that the directional derivative in the direction from
∏i(∙∣so) to ∏i(∙∣so) is strictly positive. We also have
∂ Dkl(∏' (∙∣so),∏i(∙∣so))
dxj
∂
∂Xi [(n (ISO))T(IOgπ (ISO) - logπ (ISO))]
∂xj Hn i)T log πi]
(omitting state SO for brevity)
-	∂¾ X πk logXk- ∂¾ πd i log (1 - X Xk)
j k=1	j	k=1
-	立 +	πd ,
Xj	1 - Pd=LIXk
πi	∏i	i _i
—	—i H——A = 0, when evaluated at π = ∏i,	(19)
πji	πdii
20
Published as a conference paper at ICLR 2022
which means that the KL-Penalty has zero gradient at ∏i (∙∣ s 0). Hence, when evaluated atπi(ISO)=
∏i(∙∣so), the objective
Pn(SO)Eai〜πi [A∏(SO, ai)] - CnDKL (πi(ISO), πi(ISO))
has a strictly positive directional derivative in the direction of ∏'(∙∣so). Thus, there exists a policy
ei(∙∣S0), sufficiently close to ∏'(∙∣so) on the path joining it with ∏'(∙∣so), for which
Pn(SO)Eai〜πi [Al(so, ai)] - CnDkl(πi(1SO),πi(1SO)) > 0.
Let n be a policy such that ∏i(∙∣so) = ei(∙∣SO), and π*(∙∣s) = ∏'(∙∣s) for states S = so. AS for
these states we have
Pn(S)Eaiy [Al(s, ai)] = Pn(S)Eai〜∏i [A∏(s, ai)] =0, and DκL(∏i(∙∣s), ∏(∙∣s)) =0,
it follows that
Ln(∏i) - CnDmaX(∏i,Π) = Pn(SO)Eai〜∏i [a∏(so, ai)] — CnDkl(∏(∣so"(∙∣so))
> 0 = Ln(∏i) - CnDmLX(∏i,∏i),
which is a contradiction with TR-stationarity of π. Hence, the claim of Equation (17) is proved.
Step 3 (optimality). Now, for a fixedjoint policy π-i of other agents, ∏i satisfies
∏i = arg max Eai〜∏i [a∏ (s, ai)] = arg max Eai〜∏i [Qn (s, ai)], ∀s ∈ S,
πi	πi
which is the Bellman optimality equation (Sutton & Barto, 2018). Hence, for a fiXed joint policy
π-i, the policy πi is optimal:
πi = arg max J(πi, ∏-i).
πi
AS agent i was chosen arbitrarily, π is a Nash equilibrium.	□
21
Published as a conference paper at ICLR 2022
D HATRPO AND HAPPO
D.1 Proof of Proposition 2
Proposition 2. Let π = H；=i πij be a joint policy, and Aπ (s, a) be its joint advantage function.
Let π i1:m-1 = n*1 πij be some other joint policy ofagents ii：m—ι, and π im be some other policy
of agent im. Then, for every state s,
Eai1:m— 1 〜∏ i1：m —1 ,aim 〜∏im [AT (s, ai1:m-1, aim)]
=E …[(；- O ；1：：—I：:—1|S) An 3 a)].⑼
Proof. We have
=E	[.1：m (a j⑸A (S a) _齐…⑷…⑸A (S a)
a~π [∏iLm (ai1：m |s) π ( , )	ni1：m—1(ai1：m—1 |s) π ( , )
ni1：m(ai1：m |s)	-	-
=E 八 入	------A _(S ai1：m a-i1：m )
JaiI：m 〜ni1：m ,a i1：m 〜π i1：m	^ɪ ( ^ɪ ∣ ) ʃɪn Is, a , a J
一亓 i1：m—1(ai1：m—11S)	-	-
_ K .	______∖	I ' ,4 (Q 口i1：m—1 ɑ-i1：m—1 ʌ
-	EaiI：m — 1 〜ni1：m—\a—i1：m —1 〜n—i1：m —1 ["m —1	1 ∣s) An(S, a	, a	)
=EaiI：m 〜πi1：m ,a—i1：m 〜n—i1：m [An (S, a”：m , a ”:m )]
-	EaiI：m — 1 〜ni1：m — 1 ,a— i1：m — 1 〜π—i1：m — 1 [An (S, a”：m 1, a j1:m 1 )]
=EaiI：m 〜ni1：m [Ea— i1：m 〜n—i1：m [An (S, a”：m , a ”:m )]]
—	EaiI：m - 1 〜ni1：m - 1 [Ea-i1：m —1 〜n — i1：m -1 [An, (S, a”：m 1, a，1:m 1 )]]
=Eai1：m〜ni1：m [An：m (s, ai1:m)]
- Eai1：m- 1 〜ni1：m -1 [An：m- 1 (s, ai1:m-1)],
which, by Lemma 1, equals
=Eai1：m〜ni1：m [An：m (s, ai1:m) - A*m-1(s, ai1:m-1)]
=Eai1：m〜ni1：m [Anm(s, ai1:m-1, aim)].
□
D.2 DERIVATION OF THE GRADIENT ESTIMATOR FOR HATRPO
▽ , E	πθmm (aim|s)
V θim ES〜ρ∏	,a〜∏θ	i
i k ∏m (aim ∣s)
- 1 M i1:m (s, a)
▽ θim ES〜P∏θχ ,a-n”
；Mi (s, a)
■ θk
- ▽ θim ES〜Pk”，a-n” Mi1：m (s, a)
ES〜Pn” ,a〜n”
eP∏θk ,a〜n”
▽ Qim ∏im (aim ∣s)	∙	,	、
θ . θim'__— Mi1：m (s, a)
πQmm (aim|s)
Izk
Iilml vθim ιog 碇m (aim|S)Mi1：m(s,a).
Izk
Evaluated at θim = θɪm, the above expression equals
,a 〜n°k[Mj(s, a)vθim log πiθmm (ai 叫 S)IQim =瞰]
which finishes the derivation.
22
Published as a conference paper at ICLR 2022
D.3 Pseudocode of HATRPO
Algorithm 2 HATRPO
1: Input: Stepsize α, batch size B, number of: agents n, episodes K, steps per episode T, possible
2:
3:
4:
5
6
7
8
9
10
11
12:
steps in line search L, line search acceptance threshold κ.
Initialize: Actor networks {θ0i, ∀i ∈ N}, Global V-value network {φ0}, Replay buffer B
for k = 0, 1, . . . , K - 1 do
Collect a set of trajectories by running the joint policy πθk = (πθ11 , . . . , πθnn).
Push transitions {(oit, ait, oti+1, rt), ∀i ∈ N, t ∈ T} into B.
Sample a random minibatch of B transitions from B.
Compute advantage function A(s, a) based on global V-value network with GAE.
Draw a random permutation of agents i1:n.
Set M i1 (s, a) = A(s, a).
for agent im = i1 , . . . , in do
Estimate the gradient of the agent’s maximisation objective
BT
gim = B P P Vθim logπθmm (atm | otm)Mii：m(st, at).
b=1 t=1	k	θk
Use the conjugate gradient algorithm to compute the update direction
Xilr ≈(Hkm)-1gkm,
i
where Hkim is the Hessian of the average KL-divergence
BT
Bt P P DKL (∏θmm (∙∣οim),∏θmm (∙∣οim)).
b=1 t=1	θk
Estimate the maximal step size allowing for meeting the KL-constraint
13:
βkim ≈ j
2δ
.
(Xkm)T Hkm Xkm
Update agent im ’s policy by
θk+ι = θkm + αj βkm Xkm,
where j ∈ {0, 1, . . . , L} is the smallest such j which improves the sample loss by at least
ii i
Kajβkm Xkm ∙ gim, found by the backtracking line search.
14:
Compute M i1:m+1 (s, a)
15:
16:
πimi (aim |oim )
θim
ik+1,「.、Mi1:m (st, at). //Unless m = n.
πimi (aim |oim )	t, t
θkm
17: end for
end for
Update V-value network by following formula:
Φk+ι = argminφ BT P P fv≠(st) - Rt)
b=1 t=0
23
Published as a conference paper at ICLR 2022
D.4 Pseudocode of HAPPO
Algorithm 3 HAPPO
1:	Input: Stepsize α, batch size B, number of: agents n, episodes K, steps per episode T .
2:	Initialize: Actor networks {θ0i , ∀i ∈ N}, Global V-value network {φ0}, Replay buffer B
3:	for k = 0, 1, . . . , K - 1 do
4:	Collect a set of trajectories by running the joint policy πθk = (πθ11 , . . . , πθnn).
5:	Push transitions {(oit, ait, oit+1 , rt), ∀i ∈ N , t ∈ T } into B.
6:	Sample a random minibatch of B transitions from B.
7:	Compute advantage function A(s, a) based on global V-value network with GAE.
8:	Draw a random permutation of agents i1:n.
9:	Set Mi1 (s, a) = A(s, a).
10:	for agent im = i1 , . . . , in do
11:	Update actor im with θkim+1, the argmax of the PPO-Clip objective
B T	im	im im	im	im im
ɪ P P min	iim 心	t~—πMi1:m (st, at),	clip( θim 心	t~—π,	1 ± e )Mi1:m (st, at)
BT	im im im	t , t ,	im im im ,	t , t
b=1 t=0	πθim (at |ot )	πθim (at |ot )
kk
πimi (aim |oim )
θim
12:	Compute Mi1:m+1 (s, a)= 冗与1日MiE)Mi1:m(s, a). //Unless m = n.
θkm
13:	end for
14:	Update V-value network by following formula:
Φk+1 = argminφ BT P P (%(st) - Rt)
b=1 t=0
15:	end for
E Hyper-parameter Settings for Experiments
hyperparameters	value	hyperparameters	value	hyperparameters	value
critic lr	5e-4	optimizer	Adam	stacked-frames	1
gamma	0.95	optim eps	1e-5	batch size	3200
gain	0.01	hidden layer	1	training threads	32
actor network	mlp	num mini-batch	1	rollout threads	20
hypernet embed	64	max grad norm	10	episode length	160
activation	ReLU	hidden layer dim	64	use huber loss	True
Table 1: Common hyperparameters used in the SMAC domain.
Algorithms	MAPPO	HAPPO	HATRPO
actor lr	5e-4	5e-4	/
ppo epoch	5	5	/
kl-threshold	/	/	0.06
ppo-clip	0.2	0.2	/
accept ratio	/	/	0.5
Table 2: Different hyperparameters used for MAPPO, HAPPO and HATRPO in the SMAC
The implementation of MADDPG is adopted from the Tianshou framework (Weng et al., 2021), all
hyperparameters left unchanged at the origin best-performing status.
24
Published as a conference paper at ICLR 2022
hyperparameters	value	hyperparameters	value	hyperparameters	value
critic lr	5e-3	optimizer	Adam	num mini-batch	40
gamma	0.99	optim eps	1e-5	batch size	4000
gain	0.01	hidden layer	1	training threads	8
std y coef	0.5	actor network	mlp	rollout threads	4
std x coef	1	max grad norm	10	episode length	1000
activation	ReLU	hidden layer dim	64	eval episode	32
Table 3: Common hyperparameters used for IPPO, MAPPO, HAPPO, HATRPO in the Multi-Agent
MuJoCo domain
Algorithms ∣ IPPO		MAPPO	HAPPO	HATRPO
actor lr	5e-6	5e-6	5e-6	/
ppo epoch	5	5	5	/
kl-threshold	/	/	/	[1e-4,1.5e-4,7e-4,1e-3]
ppo-clip	0.2	0.2	0.2	/
accept ratio	/	/	/	0.5
Table 4: Different hyperparameters used for IPPO, MAPPO, HAPPO and HATRPO in the Multi-
Agent MuJoCo domain.
hyperparameters	value	hyperparameters	value	hyperparameters	value
actor lr	1e-3	optimizer	Adam	buffer size	1e6
critic lr	1e-3	exploration noise	0.1	batch size	200
gamma	0.99	step-per-epoch	50000	training num	16
tau	5e-2	step-per-collector	2000	test num	10
start-timesteps	25000	update-per-step	0.05	n-step	1
epoch	200	hidden-sizes	[256,256]	episode length	1000
Table 5: Hyper-parameter used for MADDPG in the Multi-Agent MuJoCo domain
task	value	task	value	task	value
Ant(2x4)	1e-4	Ant(4x2)	1e-4	Ant(8x1)	1e-4
HalfCheetah(2x3)	1e-4	HalfCheetah(3x2)	1e-4	HalfCheetah(6x1)	1e-4
Walker(2x3)	1e-3	Walker(3x2)	1e-4	Walker(6x1)	1e-4
Humanoid(17x1)	7e-4	HUmanOid-StandUP(17x1)	1e-4	SWimmer(10x2)	1.5e-4
Table 6: Parameter kl-threshold used for HATRPO in the Multi-Agent MuJoCo domain
hyperparameters	value	hyperparameters	value	hyperparameters	value
lr	7e-4	optimizer	Adam	num mini-batch	1
gamma	0.99	OPtim eps	1e-5	eval episode	32
gain	0.01	hidden layer	1	training threads	1
max grad norm	10	actor network	rnn	rollout threads	128
hidden layer dim	64	activation	ReLU	episode length	25
Table 7: Common hyperparameters used for MAPPO, HAPPO in the Multi-Agent Particle Environ-
ment
25
Published as a conference paper at ICLR 2022
F Ablation Experiments
In this section, we conduct ablation study to investigate the importance of two key novelties that our
HATRPO introduced; they are heterogeneity of agents’ parameters and the randomisation of order
of agents in the sequential update scheme. We compare the performance of original HATRPO with
a version that shares parameters, and with a version where the order in sequential update scheme is
fixed throughout training. We run the experiments on two MAMuJoCo tasks (2-agent & 6-agent).
Walker 6xl
p-eMω°;ωpo-d 山ω6eωΛV
(a) 2-Agent Walker	(b) 6-Agent Walker
Figure 4:	Performance comparison between original HATRPO, and its modified versions: HATRPO
with parameter sharing, and HATRPO without randomisation of the sequential update scheme.
The experiments reveal that, although the modified versions of HATRPO still outperform baselines
(represented by MAPPO), their deviation from the theory harms performance. In particular, pa-
rameter sharing introduces extra variance to training, harms the monotonic improvement property
(Theorem 2 assumes heterogeneity), and causes HATRPO to converge to suboptimal policies. The
suboptimality is more severe in the task with more agents, as suggested by Proposition 1. Similarly,
fixed order in the sequential update scheme negatively affected the performance at convergence
(especially in the task with 6 agents), as suggested by Proposition 3. We conclude that the fine per-
formance of HATRPO relies strongly on the close connection between theory an implementation.
The connection becomes increasingly important with the growing number of agents.
G Ablation Study on Non-Parameter Sharing MAPPO/IPPO
We verify that heterogeneous-agent trust region algorithms (represented by HATRPO) achieve supe-
rior performance to, originally homogeneous, IPPO/MAPPO algorithms with the parameter-sharing
function switched off.
Halfcheetah 3x2
p-eMω°;ωpo-d 山ω6eωΛV
Il
0.0	0.2	0.4	0.6	0.8	1.0
Environment steps le7
(b) 3-Agent Walker
(a) 3-Agent HalfCheetah
Figure 5:	Performance comparison between HATRPO and MAPPO/IPPO without parameter shar-
ing. HATRPO significantly outperforms its counterparts.
B
26
Published as a conference paper at ICLR 2022
H Multi-Agent Particle Environment Experiments
We verify that heterogeneous-agent trust region algorithms (represented here by HAPPO) quickly
solve cooperative MPE tasks.
Spread
0.00 0.25 0.50 0.75 1.00 1.25 1.50
Environment steps
1.75 2.00
le7
(a) Simple Spread
Figure 6: Performance comparison between MAPPO and HAPPO on MPE.
peMω°;ωpo--d 山ω6eωΛV
Reference
0.5	1.0	1.5	2.0	2.5
Environment steps
3.0
le6
(b) Simple Reference
27