Published as a conference paper at ICLR 2022
Surreal-GAN: Semi-Supervised Representation
Learning via GAN for uncovering heteroge-
neous disease-related imaging patterns
Zhijian Yang1,2, Junhao Wen1 and Christos Davatzikos1
1 Center for Biomedical Image Computing and Analytics, University of Pennsylvania
2Graduate Group in Applied Mathematics and Computational Science, University of Pennsylvania
{zhijian.yang,junhao.wen,christos.davatzikos}@pennmedicine.upenn.edu
Ab stract
A plethora of machine learning methods have been applied to imaging data, en-
abling the construction of clinically relevant imaging signatures of neurologi-
cal and neuropsychiatric diseases. Oftentimes, such methods do not explicitly
model the heterogeneity of disease effects, or approach it via nonlinear models
that are not interpretable. Moreover, unsupervised methods may parse hetero-
geneity that is driven by nuisance confounding factors that affect global brain
structure or function, rather than heterogeneity relevant to a pathology of inter-
est. On the other hand, semi-supervised clustering methods seek to derive a
pathology-related dichotomous subtypes, ignoring the truth that disease hetero-
geneity spatially and temporally extends along a continuum. To address the afore-
mentioned limitations, herein, we propose a novel method, termed Surreal-GAN
(Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using cross-sectional
imaging data, Surreal-GAN dissects underlying disease-related heterogeneity un-
der the principle of semi-supervised clustering [cluster mappings from the normal
control (CN) to patient (PT) domain], proposes a continuously dimensional repre-
sentation, and infers the disease severity of patients at individual level along each
dimension. The model first learns a transformation function from the CN domain
to the PT domain with latent variables controlling transformation directions. An
inverse mapping function together with regularization on function continuity, pat-
tern orthogonality and monotonicity was also imposed to make sure that the trans-
formation function captures necessarily meaningful imaging patterns with clinical
significance. We first validated the model through semi-synthetic experiments,
and then demonstrated its potential in capturing biologically plausible imaging
patterns in Alzheimer’s disease (AD).
1	Introduction
Neuroimaging, conventional machine learning, and deep learning have offered unprecedented op-
portunities to understand the underlying mechanism of brain disorders over the past decades (Da-
vatzikos (2018)), and pave the road towards individualized precision medicine (Bzdok & Meyer-
Lindenberg (2018)). A large body of case-control studies leverage mass univariate group compar-
isons to derive structural or functional neuroimaging signatures (Habeck et al. (2008), Hampel et al.
(2008), Ewers et al. (2011)). However, these studies suffer from under-powered statistical infer-
ences since the heterogeneous nature of neurological diseases usually violate the assumption that
each group population are pathologically homogeneous.
A developing body of methodologies aim to disentangle this heterogeneity with various machine
learning methods. Zhang et al. (2016) used the Bayesian Latent Dirichlet Allocation (LDA) model
to identify latent atrophy patterns from voxel-wise gray matter (GM) density maps. However, due
to the nature of the LDA model, the density maps need to be first discretized, and the model hy-
pothesized to exclusively capture brain atrophy while ignoring the potential regional enlargement.
Young et al. (2018) proposed another method, Sustain, to uncover temporal and phenotypic hetero-
geneity by inferring both subtypes and stages. However, the model only handles around 10-20 large
1
Published as a conference paper at ICLR 2022
brain regions without more detailed information. Notably, both methods applied directly in the PT
domain, confront main limitations in avoiding potential disease-unrelated brain variations. In con-
trast, semi-supervised clustering methods (Varol et al. (2016), Dong et al. (2015), Wen et al. (2021),
Yang et al. (2021)) were proposed to cluster patients via the patterns or transformations between
the reference group (CN) and the target patient group (PT). A recent proposed deep learning-based
semi-supervised clustering method, termed Smile-GAN (Yang et al. (2021)), achieved better cluster-
ing performance by learning multiple mappings from CN to PT with an inverse clustering function
for both regularization and clustering. Albeit these methods demonstrated potential in clinical appli-
cations, they have a major limitation. In particular, these methods model disease heterogeneity as a
dichotomous process and derive a discrete output, i.e., subtype, ignoring the fact that brain disorders
progress along a continuum. Moreover, variations among subtypes were contributed by both spatial
and temporal differences in disease patterns, thus confounding further clinical analysis.(Fig. 1a)
To address the aforementioned limitations, we propose a novel method, Surreal-GAN (Semi-
Supervised Representation Learning via GAN), for deriving heterogeneity-refined imaging signa-
tures. Surreal-GAN is a significant extension of Smile-GAN under the same principle of semi-
supervised learning by transforming data from CN domain X to PT domain Y. Herein, the key
extension is to represent complex disease-related heterogeneity with low dimensional representa-
tions with each dimension indicating the severity of one relatively homogeneous imaging patterns
(Fig. 1b). We refer to these low dimensional representations as R-indices (ri , where i represent
the ith dimension) of disease pattern. The first key point of the method is modelling disease as a
continuous process and learning infinite transformation directions from CN to PT, with each direc-
tion capturing a specific combination of patterns and severity. The idea is realized by learning one
transformation function which takes both normal data and a continuous latent variable as inputs and
output synthesized-PT data whose distribution is indistinguishable from that of real PT data. The
second key point is controlling the monotonicity (from light to severe with increasing index value)
of disease patterns through one monotonicity loss and double sampling procedure. The third key
point is boosting the separation of captured disease patterns through one orthogonality regulariza-
tion. The fourth key point is introducing an inverse mapping function consisting of a decomposition
part and a reconstruction part which further ensure that patterns synthesized through transforma-
tion are exactly captured for inferring representations of disease patterns, the R-indices. Besides the
above mentioned regularization, function continuity, transformation sparsity and inverse consistency
further guide the model to capture meaningful imaging patterns.
To support our claims, we first validated Surreal-GAN on semi-synthetic data with known ground
truth of disease patterns and severity. We compared performance of Surreal-GAN to NMF (Lee
& Seung (1999)), opNMF (Sotiras et al. (2014)), Discriminant-NMF (Lee et al. (2012)), Factor
Analysis and LDA (Blei et al. (2001)) models, and subsequently studied the robustness of model
under various conditions. Finally, we applied the model to one real dataset for Alzheimer’s dis-
ease (AD), and demonstrated significant correlations between the proposed imaging signatures and
clinical variables.
2	Methods
The schematic diagram of Surreal-GAN is shown in Fig. 1d. The model is applied on regional
volume data (volumes of different graymatter (GM) and whitematter (WM) regions) derived from
structural MRL The essential idea is to learn one transformation function f : X * Z → Y
which transforms CN data x to different synthesized PT data y0 = f(x, z), with latent vari-
able z specifying distinct combinations of patterns and severity. Here, the Latent (LAT) domain,
Z = {z : 0 ≤ zi ≤ 1 ∀1 ≤ i ≤ M}, is a class of vectors with dimension M (predefined number
of patterns). We denote four different data distributions as X 〜Pcn(X), y 〜Ppt(y), y0 〜PsynQ)
and Z 〜piat(z), respectively, where Z 〜Plat(Z) is sampled from a multivariate uniform distribution
U [0, 1]M , rather than a categorical distribution as in Smile-GAN. A continuous z variable lay the
foundation for learning continuous representations. In addition to the transformation function, an
adversarial discriminator D is introduced to distinguish between real PT data y and synthesized PT
data y0, thereby ensuring that the synthesized PT data are indistinguishable from real PT data (i.e.
minimizing the distance between real PT data distribution and synthesized PT data distribution).
A continuous latent variable does not necessarily lead to desired properties of CN to PT transfor-
mation (e.g. separation/monotonicity of captured patterns). That is, there are a number of functions
potentially achieving equality in distributions, thus making it hard to guarantee that the transfor-
2
Published as a conference paper at ICLR 2022
Figure 1: Model Ideas and Schematic Diagram. (a) Results from Semi-supervised clustering for
AD disease: model clusters PT into discrete subtypes based on both spatial and temporal variations:
P1 and P4 show mild and severe stages and are mixture of two patterns; (b) Improvements from
Surreal-GAN: two patterns are captured by two indices with values indicating severity; (c) General
architecture of networks; (d) Schematic diagram of Surreal-GAN.
(d) Schematic Diagram
mation function learned by the model is closely related to the underlying pathology progression
process. Moreover, during the training procedure, the transformation function backboned by the
neural network tends to ignore the latent variable z. Even when the Latent variable z is emphasized
through regularization, different components of z variable do not naturally lead to separate disease
patterns and are not positively correlated with pattern severity. Therefore, with the assumption that
there is one true underlying function for real PT data y = h(x, z), Surreal-GAN aims to boost the
transformation function f to approximate the true underlying function h, while having the latent
variable, z, contributing to recognizable imaging patterns rather than being random ”activators” of
the GAN. For this purpose, we constrain the function class of f via different types of regularization
as follows: 1) encourage sparse transformations; 2) enforce Lipschitz continuity of functions; 3)
introduce one inverse function g : Y → Z serving to decompose and reconstruct; 4) boost orthog-
onality among synthesized/captured patterns; 5) enforce monotonicity of synthesized patterns and
their positive correlations with components of z. The training procedure of Surreal-GAN is thus a
adversarial learning process constrained by these regularization.
2.1	Adversarial Loss
The adversarial loss (Goodfellow et al. (2014)) is used for iterative training of discriminator D and
the transformation function f, which can be written as:
LGAN(D,f) = Ey~Ppt(y)[lθg(D(y))]+ Eχ~*(x),z~Piat(z)[lθg(1 - D(f (x, z)))]	⑴
=Ey~Ppt (y) [log(D(y))] + Ey0~psyn(y) [log(I- D(y))]	⑵
3
Published as a conference paper at ICLR 2022
The transformation function f attempts to transform CN data to synthesized PT data so that they
follow similar distributions as real PT data. The discriminator, providing a probability that y comes
from the real data rather than transformation function, tries to identify the synthesized PT data
and distinguish it from real PT data. Therefore, the discriminator D attempts to maximize the
adversarial loss function while the transformation function f attempts to minimize against it. The
training process can be denoted as:
mfinmDax LGAN(D,f ) = Ey〜ppt(y)[log(Oy))] + %〜Psyn(y0)[log(1 - D(yO))]	⑶
2.2	Regularization
2.2.1	Sparse Transformation
We assumed that disease process will not change brain anatomy dramatically and primarily only
affect certain regions throughout most of disease stages, which means that the true underlying trans-
formation only moderately changes some regions while keeping the rest mildly affected with most
of z variable values. Therefore, to control sparsity and distance of transformations, we defined one
change loss to be the l1 distance between the synthesized PT data and the original CN data:
Lchangef ) = Ex 〜Pcn(X),z 〜Plat(Z)[||f(x, z) - x||1]	⑷
2.2.2	Lipschitiz Continuity and Inverse Mapping
First, by enforcing the transformation function f to be K1-Lipschitz continuous, we can derive that,
for fixed latent variable z = a and ∀x1, x2 ∈ X, ||f (x1, a) - f(x2, a)||2 ≤ K1||x1 - x2||2. Thus,
by controlling the constant K1 , we constrained the transformation function to preserve the original
distances among CN data instead of scattering them into the PT domain if a same combination of
disease patterns is imposed to them.
Second, to prevent latent variable z from being ignored, we introduced an inverse mapping function
from PT domain to Latent domain, g : Y → Z. By Lemma1 (section A.1), with function g being
K2-Lipschitz continuous and d(∙, ∙) being any distance metric satisfying triangular inequality, We
can derive that, ∀zι, z 〜Plat(Z), zι = z2, and X 〜Pcn(X), d(f (x, zι),f(x, z2)) is lower
bounded by (d(Kz2) -卷(d(g(f (x, zι)), zι) + d(g(f(x, z2)), z2))). Therefore, by minimizing
the distance between sampled latent variable z and reconstructed latent variable g(f(x, z)), we can
control differences between synthesized patterns to be positively related to distances between latent
variables, and thus to be non-trivial (i.e., same CN data is transformed to significantly different PT
data with different z variables). Therefore, we define a reconstruction loss as the l2 distance between
g(f(x, z)) and z:
Lrecons(f,g) = Ex~pcn(x),z~plat(z)[||g(f (X,z)) - z||2]	(5)
2.2.3	Pattern Decomposition
Nevertheless, a simple function g directly reconstructing sampled z from synthesized PT y0 do
not reconstruct each component zi completely based on changes led by zi in transformation. We
are more interested in pattern representations of real PT data than generating synthesized PT, and
inverse function g will be used for inferring R-indices as introduced in section 2.4. Thus, it is
very important for the inverse function to accurately capture patterns synthesized by intensively
regularized transformation function f . For this purpose, we further separated the function g : Y →
Z into gι : Y → RM*S (where M is number of patterns and S is number of input ROIS) and
g2 : RS → R. Decomposer g1 serves to reconstruct changes synthesized by each component zi in
transformation process: qi = f(x, ai) - x, where ai is a vector with ith component aii = zi and
aj = 0, ∀i = j. Let Gf (x,z) = [qT,…,qM]T be concatenation of all synthesized changes qi, we
defined the decomposition loss as:
Ldecom(f, g1) = Ex~pcn(x),z~plat(z) [||g1(f (X,z)) - qf (x,z) ||2]	(6)
g2 serves to further reconstruct each component of sampled z variable from g1 (f(x, z)) indepen-
dently, so that the reconstruction loss can be rewritten as:
T ，里八—T / ʃ ʌ ʌ ʌ — IE?	Γl Il	~lll	-7、
Lrecons(f,g) = Lrecons(f, g1,g2) = Ex〜pcn(x),z〜Plat(Z) |||lg2(gi (f (x,z)) - z||2]	⑺
4
Published as a conference paper at ICLR 2022
Wherelg2(gi(f(x,z))= g(f(x,z)) = [g2(g1(f(x,z))o：S),…，g2(g1(f (x, z))s*(m-i)：s*m)]t.
Function g here can be also considered as the approximation of expectation of posterior distribution.
In this sense, minimization of l2 distances can also be interpreted as maximization of mutual infor-
mation betWeen latent variable z and synthesized PT data y0 = f(x, z) as explained in Remark1
in section A.2 (Chen et al. (2016)). From this perspective, the transformation function is forced
to best utilize information from latent variable z, While also keeping mutual information betWeen
synthesized PT data y0 and original CN data x by controlling transformation distances.
2.2.4	Orthogonality of Patterns
Inclusion of latent variable z in the transformation function does not guarantee that different com-
ponents of the latent variable, zi , contributes to relatively different patterns in synthesized PT data.
Instead, during training procedure, different components, zi , are more likely to synthesize patterns
in same regions and lead to accumulated severity. Therefore, We define another orthogonal loss to
encourage separation among patterns synthesized by different components. With changes led by
each component, qi defined in section 2.2.3, We constructed a matrix Af(x,z) With the ith column
Af (x,z) = |qi|/||qi||2. To encourage separation of changes led by different components, We
boosted the matrix Af(x,z) to be relatively orthogonal by minimizing the folloWing orthogonality
loss:
Lorthof)= EX~Pcn(X),Z~piat(Z)[H Af (χ,z) Af(X,Z)— 1"f ]	⑻
2.2.5	Monotonicity and Positive Correlation
Besides separation of synthesized patterns, a positive correlation betWeen values of each compo-
nent zi (from 0 to 1) and severity of synthesized patterns can not be acquired spontaneously either.
Regarding severity of the disease pattern, We assumed that, as a pattern is becoming more severe,
absolute values of changes in regional volumes can only increase monotonically or remain constant.
In another Word, there is no oscillation in regional volumes as disease pattern is becoming more
severe, While more regions can be included gradually With absolute changes sWitched from 0 to a
positive value. To satisfy this requirement, we sampled another latent variable z0 〜Psev(z0∣z),
conditioned on previous sampled z variable, such that z0i ≥ zi, ∀1 ≤ i ≤ M . With these double
sampled latent variables, we defined the monotonicity loss to be:
Lmonof ) = Ex 〜Pcn(x),Z 〜Plat(Z),Z0 〜Psev ①㈤川 maX(If(X,z) - XI-If(X,z') - x|, 0)||2]	(9)
Minimization of this monotonicity loss penalize the case that volume changes induced by z is larger
than that induced by z0, while the other direction is permitted. To further ensure a positive correlation
(small z lead to mild patterns; large z lead to severe patterns), we introduced a cn loss, which serves
to more strictly constrain the distance between synthesized PT data y0 and CN data x, when latent
variable z is close to 0 vector. By letting pcn(z) = U(0, 0.05)M to be a multivariate uniform
distribution, the cn loss is defined as:
Lcn(f )= Ex 〜pcn(χ),Zcn 〜Pcn(Z) [IIf(x, Zcn)- x||l]	(10)
2.3	Full Objective
With the adversarial loss and all other loss functions defined for regularization, we can write the full
objective as:
L(D, f, g1, g2) = LGAN(D, f) + γ Lchange (f) + κLdecom(f, g1) + ζ Lrecon (f, g1, g2)
+ λLortho(f) + μLmono(f) + ηLcn(f)	(II)
with γ, κ, Z, λ, μ and η be parameters controlling the relative importance of different loss functions.
Through training process, we want to derive parametrized functions f,g1 and g2 satisfying:
f,g1,g2 = arg min maX L(D, f, g1, g2)	(12)
f,g D
2.4	Representation-Indices Inference
With the assumption that psyn (f (x, z)) ≈ pPt (y) and f satisfying all constraints, we consider
learned transformation function f tobe a good approximation of the true underlying function h, such
5
Published as a conference paper at ICLR 2022
that f (x, Z) ≈ h(x, σ(z)), where σ ∈ Ω and Ω is a class of permutation functions which change
orders of elements in vector z. Since the order of indices in derived representation is not important
and we can always reorder them to find the best matching, we simply rewrite it as f(x, z) ≈ h(x, z)
without loss of generality. For any real PT data, y = h(x, r)〜Ppt(y), We can derive that the
ground truth R-indices r ≈ g(f (x, r)) ≈ g(h(x, r)) ≈ g(y). Therefore, reconstruction function g
can be directly applied to infer R-indices of disease patterns, r, of any unseen PT data.
3	Implementation Details
3.1	Model Architecture
The general architecture of networks can be understood from Fig. 1c. The transformation function f
utilizes a encoding-decoding structure. Latent variable z and CN data x are first mapped to two vec-
tors with dimension 34 and the element-wise multiplication of them are then decoded to construct
the synthesized PT data with dimension 139. The inverse function g first decodes real/synthesized
PT data y/y0 to larger vector with dimension 139*M and then maps them into a vector with di-
mension M. The discriminator D utilizes the encoding structure which maps y/y0 to a vector with
dimension 2. More details of model architectures can be found in Appendix Table 2 and 3.
3.2	Lipschitz Continuity
In implementation, we performed weight clipping to ensure Lipschitz continuity of transformation
function f and inverse function g (Arjovsky et al. (2017)). With Θf and Θg denoting the weight
spaces of function f and g, the compactness of them imply the Lispchitz continuity of two func-
tions, with Lipschitz constants K1 and K2 only depends on Θf and Θg . The compactness of Θf
and Θg was guaranteed by clapping weight spaces into two closed box, Θf = [-cf , cf]d and
Θg = [-cg , cg]d. In implementation, both cf and cg were set to be 0.5 and these bounds can be
further relaxed. Weight clipping was claimed not to be the most satisfactory method to ensure Lips-
chitz continuity because of difficulty in choosing the appropriate clipping bounds. However, in our
case, we performed weight clipping for function f and g rather than discriminator D for a different
purpose and weight clipping contributed to good performances for deriving representations.
3.3	Training Details
Regarding optimization procedure, ADAM optimizer(Kingma & Ba (2014)) was used with a learn-
ing rate (lr) 4 * 10-5 for Discriminator and 2 * 10-4 for transformation function f and clustering
function g. β1 and β2 are set to be 0.5 and 0.999 respectively. For hyper-parameters, we set γ = 6,
K = 80, Z = 80, μ = 500, η = 6. However, performance of model on different tasks are robust to
varying hyper-parameters as shown in section B.2. Parameter λ determines the relative importance
of orthogonality loss and controls the degree of overlapping among synthesized patterns. Without
prior knowledge on overlapping degree of underlying disease patterns, we set λ to different values
in the following experiments and used agreements among multiple trained models to determine the
optimal one. Moreover, for all experiments, the batch size was set to be 1/8 of the PT data sample
sizes. The model was trained for at least 100000 iterations and saved until the reconstruction loss
and the monotonicity loss are smaller than 0.003 and 6 * 10-4 respectively. Higher lr might improve
convergence speed in some cases. Detailed training procedure is revealed by Appendix Algorithm1.
4	Experiments
4.1	Data Preprocessing
For semi-synthetic and real data experiments, we used T1-weighted (T1w) MR imaging (MRI) and
cognition from the iSAGING (Imaging-based coordinate SysTem for AGIng and NeurodeGenerative
diseases) consortium for cognitive impairment and dementia. All baseline MRIs were corrected for
intensity inhomogeneities (Sled et al. (1998)). Then, 139 regions of interest (ROIs) of brain volumes
were extracted using a multi-atlas label fusion method (Doshi et al. (2015)), harmonized (Pomponio
et al. (2020)) to remove site effect, and then utilized as input features for Surreal-GAN. (Fig. 6)
6
Published as a conference paper at ICLR 2022
4.2	Semi-synthetic experiments
Semi-synthetic data construction To set the ground truth of disease patterns and severity a prior,
we generated semi-synthetic data by imposing simulated disease-related patterns to real CN data,
thereby retaining the variation from the real CN data. Specifically, we split 1392 cognitively normal
(age < 70 years) into one real CN group with 492 subjects and one Pseudo-PT group with 900 sub-
jects. For the ith Pseudo-PT subject, we sampled a three-dimensional vector as pattern severity from
amultivariate uniform distribution, Si 〜U [0,1]3. Three types of atrophy patterns were introduced to
900 Pseudo-PT subjects. Different regions were included in different patterns as shown in Appendix
Table 4. Five different datasets were constructed for testing the robustness of model under different
conditions: (1) Basic dataset: For the ith Pseudo-PT subject and the jth ROI included in pattern k,
we decreased the volume Vij depending on sampled severity: Vij = Vij - Vij * Sik * N(1,0.05) * 0.3.
Each pattern consists of changes in 14 ROIs and has four of them shared with other patterns; (2)
Large overlapping dataset: Same as the Basic dataset, except that each pattern shares 8 ROIs with
others; (3) Scarce regions dataset: Each pattern consists of changes in only 4 ROIs with other parts
the same as the Basic dataset; (4)Within-pattern noise dataset: Regional changes incorporated in
the same pattern have larger within-pattern variances with Vij = Vij - Vij * sik * N(1, 0.2) * 0.3;
(5) Mild atrophy dataset: Compared to the basic case, smaller changes were imposed by letting
Vij = Vij - Vij * sik * N(1, 0.05) * 0.2.
Evaluation and Agreement Metric In our experiments, Concordance Index (c-index) was used as
the evaluation metric. Specifically, for each pattern, we calculated a c-index between the inferred
R-index and the ground truth. The average of M derived C-indices, refereed as pattern-c-index, was
used for model evaluations. Moreover, to quantify the pair-wise agreement between two trained
models, we proposed another Pattern-agr-index which equals pattern-c-index between R-indices de-
rived by two different models.
Experiment Procedure We first validated the relationship between model agreements and their
accuracy. On each dataset, we ran the model 10 times with parameter λ = 0.1, 0.2, 0.4, 0.6, 0.8
respectively. With each λ, the average pair-wise pattern-agr-index among 10 models was com-
pared with their average pattern-c-indices. Further, the λ value leading to the best agreement was
considered as the best choice, and results from 10 corresponding models were reported as model
performances for each task. Moreover, we fixed the optimal λ value for each task and changed other
parameters to 0 to understand contributions from each regularization term (section B.3). Lastly, we
compared our model with five other models. NMF and Factor Analysis (FA) are well-known for
deriving lower-dimensional representation of complex dataset. The variant opNMF (Sotiras et al.
(2014)) has shown promise in parsing complex brain imaging data with extra orthogonal constraint.
Discriminant-NMF (DNMF) (Lee et al. (2012)) was proposed to learn representations which best
classify subgroups. LDA, though known for extracting topics from documents, have been recently
applied to uncover heterogeneity of AD disease from neuroimaging data (Zhang et al. (2016)). Data
preprocessing for these models were introduced in section B.4.
4.3	Real Data Experiments
Data Selection We applied Surreal-GAN to an AD dataset. For AD, we defined the CN group
(N=850) to be subjects with Mini-mental state examination (MMSE) scores above 29, and the PT
group (N=2204) as subjects diagnosed as mild cognitive impairment (MCI) or AD at baseline.
Experiments Procedure All CN and PT subjects were first residualized to rule out the covariate
effects estimated in the CN group using linear regression. Then, adjusted features were standardized
with respect to CN group. Without ground truth, we selected both the optimal number of patterns,
M , and λ parameter by measuring agreements among repetitively trained models. For each combi-
nation ofM = 2, 3, 4 and λ = 0.1, 0.2, 0.4, 0.6, 0.8, we ran the model 10 times respectively. The M
and λ values leading to the highest agreement was considered optimal. Among the 10 corresponding
models, the one having the highest mean pair-wise agreements with the other models was used to
derive R-indices for all PT subjects. To visualize patterns corresponding to the ith dimension of
R-indices, ri, we grouped subjects into three different subgroups: ri < 0.4, 0.4 < ri < 0.7 and
ri > 0.7, with rj < 0.4 for all j 6= i. Voxel-wise group comparisons were performed between
each subgroup and the CN group via AFNI-3dttest (R.W. (1996)) and GM-tissue map (Davatzikos
et al. (2002)) which encodes the volumetric changes in GM observed during the registration. Pattern
representations, R-indices, ofPT subjects were further compared with other clinical variables to test
their significance. Details of clinical variables and statistical tests can be found in section B.9.
7
Published as a conference paper at ICLR 2022
Figure 2: Results on Semi-synthetic data (results presented as mean value ± standard error). (a)
Representation accuracies on different tasks (measured by Pattern-c-index); (b) Agreements among
repetitively trained models on different tasks (measured by Pattern-agr-index).
Table 1: Model Comparison Results
	Surreal-GAN	NMF	LDA	FA	OpNMF	DNMF
Basic Case	0.838	0.680	0.605	0.631	0.648	0.562
	± 0.011	± 0.000	± 0.001	± 0.000	± 0.000	± 0.017
Large	0.837	0.692	0.591	0.612	0.670	0.566
Overlapping	± 0.008	± 0.000	± 0.000	± 0.000	± 0.000	± 0.022
Scarce Regions	0.783	0.625	0.582	0.526	0.608	0.525
	± 0.007	± 0.000	± 0.001	± 0.000	± 0.000	± 0.006
Within-pattern	0.818	0.675	0.626	0.619	0.651	0.550
Noise	± 0.013	± 0.000	± 0.000	± 0.000	± 0.000	± 0.013
Mild Changes	0.762	0.611	0.574	0.526	0.594	0.539
	± 0.023	± 0.000	± 0.004	± 0.000	± 0.000	± 0.013
5	Results
5.1	Results on Semi-synthetic Data
We first validated that agreements among multiple trained models (Fig. 2b) are good indicators of
model accuracy (Fig. 2a) for parameter selections. Besides, from Fig. 2a, we proved the model’s
ability in capturing ground truth of patterns and severity under different cases including pattern over-
lapping and large within-pattern noise. However, the model experienced declining performances
when each pattern includes only scarce disease-related regions, and showed even worse perfor-
mances when very mild atrophies are led by disease process. This is expected since, for the mild
atrophy case, the simulated atrophy rate, 0-20% of the regional volume, is even smaller than the
standard deviation of most regional volumes among the normal population. Finally, Surreal-GAN
significantly outperformed all five methods on five data sets we constructed (Table 1). However,
none of these methods optimally utilize the CN data as a reference distribution, which is one of
important limitations preventing them from precisely learning disease-specific pattern representa-
tions rather than capturing disease-unrelated confounding variations. The family of DNMF methods
capture discriminant representation by minimizing and maximizing within-class and among-class
variations respectively, thus did not capture heterogeneity within the PT class accurately either. De-
signed for parsing disease heterogeneity, semi-supervised methods focused on clustering patients
into hard categorical subtypes (Varol et al. (2016), Dong et al. (2015), Wen et al. (2021), Yang
et al. (2021)). With different end points, these methods (including Smile-GAN) can not be directly
compared with Surreal-GAN. To the best of our knowledge, our model is the first semi-supervised
approach for learning disease-related representations, using a healthy reference group as a means
for comparison. Therefore, to better understand the model performance, we leveraged the simulated
8
Published as a conference paper at ICLR 2022
Figure 3: Voxel-wise statistical comparison results between selected subgroups and CN. (FDR
method with p-value threshold of 0.05 was used for multiple comparison). In each direction guided
by arrows, we increased values of one R-index and keep the other relatively fixed. Therefore, we
chose out groups of subjects who have much stronger expression of one pattern for visualization.
ground truth information (not available in real experiments), derived ”practical upper bounds” for
compared unsupervised models, and also computed the gap between Surreal-GAN and supervised
methods. (section B.5 and B.6)
5.2	Results on Real Data
On the AD dataset, models show highest agreement when the number of patterns M = 2. From
voxel-based comparison results (Fig. 3), we can clearly visualize two patterns related to AD. Each
dimension of R-indices, ri , shows a clear positive correlation with the corresponding pattern sever-
ity. Pattern1, correlated with r1 , shows diffuse cortical atrophies (e.g., bilateral insula, orbitofrontal
and frontal poles), while pattern2 reveals focal atrophy in medial temporal lobes. Reproducibility of
these two patterns were validated in section B.7. Furthermore, the 2D R-indices were effective and
interpretable signatures for disease diagnosis and prognosis, achieving similar performance in AD
classification compared to original 139 ROIs (section B.8).
Continuity and monotonicity of inferred R-indices further enable us to easily test correlation among
them and other clinical variables (Table 7). Both r1 and r2 are highly correlated with CSF-Abeta
value and memory dysfunction measured by ADNI-MEM. However, r1 shows stronger correlations
with white matter lesion volumes, presence of hypertension, executive function and language func-
tioning, while r2 is more correlated with CSF-Tau, CSF-pTau and presence of APOE-E4 alleles.
6	Conclusion
In this study, we have proposed a novel method, Surreal-GAN, for learning representations of under-
lying disease-related imaging patterns. This model has overcome limitations in previous published
semi-supervised clustering methods and shown great performance on semi-synthetic data sets. On
AD dataset, the model parse heterogeneous PT data into two concise and clinically informative in-
dices with preserved discriminant power, further proving its huge potential in uncovering disease
heterogeneity from MRI data.
9
Published as a conference paper at ICLR 2022
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan, 2017.
David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. volume 3, pp. 601-608,
01 2001.
Danilo Bzdok and Andreas Meyer-Lindenberg. Machine learning for precision psychiatry: Oppor-
tunities and challenges. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 3, 02
2018. doi: 10.1016/j.bpsc.2017.11.007.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets.
Advances in Neural Information Processing Systems, 29, 06 2016.
Christos Davatzikos. Machine learning in neuroimaging: Progress and challenges. NeuroImage,
197, 10 2018. doi: 10.1016/j.neuroimage.2018.10.003.
Christos Davatzikos, Ahmet Genc, Dongrong Xu, and Susan Resnick. Voxel-based morphometry
using the ravens maps: Methods and validation using simulated longitudinal atrophy. NeuroIm-
age, 14:1361-1369, 01 2002. doi: 10.1006/nimg.2001.0937.
Aoyan Dong, Nicolas Honnorat, Bilwaj Gaonkar, and Christos Davatzikos. Chimera: Clustering of
heterogeneous disease effects via distribution matching of imaging patterns. IEEE transactions
on medical imaging, 35, 10 2015. doi: 10.1109/TMI.2015.2487423.
Jimit Doshi, Guray Erus, Yangming Ou, Susan Resnick, Ruben Gur, Raquel Gur, Theodore Sat-
terthwaite, Susan Furth, and Christos Davatzikos. Muse: Multi-atlas region segmentation uti-
lizing ensembles of registration algorithms and parameters, and locally optimal atlas selection.
NeuroImage, 127, 12 2015. doi: 10.1016/j.neuroimage.2015.11.073.
Michael Ewers, Reisa Sperling, William Klunk, Michael Weiner, and Harald Hampel. Neuroimag-
ing markers for the prediction and early diagnosis of alzheimer’s disease dementia. Trends in
neurosciences, 34:430-42, 06 2011. doi: 10.1016/j.tins.2011.05.005.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Y. Bengio. Generative adversarial networks. Advances in Neural Informa-
tion Processing Systems, 3, 06 2014.
Christian Habeck, Norman Foster, Robert Perneczky, Alexander Kurz, Panagiotis Alexopoulos,
Robert Koeppe, Alexander Drzezga, and Yaakov Stern. Multivariate and univariate neuroimag-
ing biomarkers of alzheimer’s disease. NeuroImage, 40:1503-15, 06 2008. doi: 10.1016/j.
neuroimage.2008.01.056.
Harald Hampel, Katharina Burger, Stefan Teipel, Arun Bokde, Henrik Zetterberg, and Kaj Blennow.
Core candidate neurochemical and imaging biomarkers of alzheimer’s disease. Alzheimer’s &
dementia : the journal of the Alzheimer’s Association, 4:38-48, 01 2008. doi: 10.1016/j.jalz.
2007.08.006.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 12 2014.
Daniel Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature,
401:788-91, 11 1999. doi: 10.1038/44565.
Soo-Young Lee, Hyun-Ah Song, and Shun-ichi Amari. A new discriminant nmf algorithm and its
application to the extraction of subtle emotional differences in speech. Cognitive Neurodynamics,
6, 12 2012. doi: 10.1007/s11571-012-9213-1.
Raymond Pomponio, Guray Erus, Mohamad Habes, Jimit Doshi, Dhivya Srinivasan, Elizabeth
Mamourian, Vishnu Bashyam, Ilya M. Nasrallah, Theodore D. Satterthwaite, Yong Fan, Lenore J.
Launer, Colin L. Masters, Paul Maruff, Chuanjun Zhuo, Henry Volzke, Sterling C. Johnson,
Jurgen Fripp, Nikolaos Koutsouleris, Daniel H. Wolf, Raquel Gur, Ruben Gur, John Morris,
10
Published as a conference paper at ICLR 2022
Marilyn S. Albert, Hans J. Grabe, Susan M. Resnick, R. Nick Bryan, David A. Wolk, Rus-
sell T. Shinohara, Haochang Shou, and Christos Davatzikos. Harmonization of large mri datasets
for the analysis of brain imaging patterns throughout the lifespan. NeuroImage, 208:116450,
2020. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2019.116450. URL http:
//www.sciencedirect.com/science/article/pii/S1053811919310419.
Cox R.W. Afni: software for analysis and visualization of functional magnetic resonance neuroim-
ages. ComPutBiomedRes,29:162-173,06 1996. doi:10.1006/cbmr.1996.0014.
J Sled, Alex Zijdenbos, and Alan Evans. Sled jg, zijdenbos ap, evans aca non-parametric method
for automatic correction of intensity nonuniformity in mri data. ieee trans med imaging 17:87-97.
IEEE transactions on medical imaging, 17:87-97, 03 1998. doi: 10.1109/42.668698.
Aristeidis Sotiras, Susan Resnick, and Christos Davatzikos. Finding imaging patterns of structural
covariance via non-negative matrix factorization. NeuroImage, 108, 12 2014. doi: 10.1016/j.
neuroimage.2014.11.045.
Erdem Varol, Aristeidis Sotiras, and Christos Davatzikos. Hydra: Revealing heterogeneity of imag-
ing and genetic patterns through a multiple max-margin discriminative analysis framework. Neu-
roImage, 145, 02 2016. doi: 10.1016/j.neuroimage.2016.02.041.
Junhao Wen, Erdem Varol, Aristeidis Sotiras, Zhijian Yang, Ganesh B. Chand, Guray Erus,
Haochang Shou, Ahmed Abdulkadir, Gyujoon Hwang, Dominic B. Dwyer, Alessandro Pigoni,
Paola Dazzan, Rene S. Kahn, Hugo G. Schnack, Marcus V. Zanetti, Eva Meisenzahl, Ger-
aldo F. Busatto, Benedicto Crespo-Facorro, Romero-Garcia Rafael, Christos Pantelis, Stephen J.
Wood, Chuanjun Zhuo, Russell T. Shinohara, Yong Fan, Ruben C. Gur, Raquel E. Gur,
Theodore D. Satterthwaite, Nikolaos Koutsouleris, Daniel H. Wolf, and Christos Davatzikos.
Multi-scale semi-supervised clustering of brain images: deriving disease subtypes. Medical
Image Analysis, pp. 102304, 2021. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.
2021.102304. URL https://www.sciencedirect.com/science/article/pii/
S1361841521003492.
Zhijian Yang, Ilya Nasrallah, Haochang Shou, Junhao Wen, Jimit Doshi, Mohamad Habes, Gu-
ray Erus, Ahmed Abdulkadir, Susan Resnick, Marilyn Albert, Paul Maruff, Jurgen Fripp, John
Morris, David Wolk, Christos Davatzikos, Yong Fan, Vishnu Bashyam, Elizabeth Mamouiran,
Randa Melhem, and Balebail Raj. A deep learning framework identifies dimensional represen-
tations of alzheimer’s disease from brain structure. Nature Communications, 12, 12 2021. doi:
10.1038/s41467-021-26703-z.
Alexandra Young, Razvan Marinescu, Neil Oxtoby, Martina Bocchetta, Keir Yong, Nicholas Firth,
David Cash, David Thomas, Katrina Moore, Manuel Jorge Cardoso, John Swieten, Barbara Bor-
roni, Daniela Galimberti, Mario Masellis, Maria Tartaglia, James Rowe, Caroline Graff, Fabrizio
Tagliavini, Giovanni Frisoni, and Daniel Alexander. Uncovering the heterogeneity and temporal
complexity of neurodegenerative diseases with subtype and stage inference. Nature Communica-
tions, 9, 10 2018. doi: 10.1038/s41467-018-05892-0.
Xiuming Zhang, Elizabeth Mormino, Nanbo Sun, Reisa Sperling, Mert Sabuncu, and B.T. Thomas
Yeo. Bayesian model reveals latent atrophy factors with dissociable cognitive trajectories in
alzheimer’s disease. Proceedings of the National Academy of Sciences, 113:E6535-E6544, 10
2016. doi: 10.1073/pnas.1611073113.
11
Published as a conference paper at ICLR 2022
A Appendix: Methods
A.1 Lemma 1
Let f : X * Z → Y and g : Y → X be the transformation and inverse mapping function re-
spectively. If g is K-LiPschitz continuous and d(∙, ∙) is any distance metric satisfying triangu-
lar inequality, then for ∀z1, z2 ∈ Z, and ∀x ∈ X, d(f (x, z1), f(x, z2)) is lower bounded by
d(ZKz2) - -K(d(g(f(x, Z1)), Z1) + d(g(f(x, z2)), z2))
Proof: By triangular inequality, we can derive that:
d(f(x, zι),f(x, z2)) ≥ ɪ d(g(f(x, zι)),g(f (x, z2)))	(13)
K
≥ ~1(d(zι,	z2)	—	d(g(f (x,	zι)),	zι)	—	d(g(f (x, z2)),	z2))	(14)
K
=d(Z1,ZG — ɪ(d(g(f (χ, z1)), z1) + d(g(f (x, z2)), z2))	(15)
KK
A.2 Remark 1
By minimizing the reconstruction loss Lrecons defined in section 2.2.2 and 2.2.3, we are maximizing
a lower bound of the mutual information between latent variable z and synthesized PT data y0 =
f(x, z). Considering Q(z|y0) to be an approximation of distribution P (z|f (x, z)) and assuming
that it follows a Gaussian distribution N(g (y0),Σ), mutual information denoted by I and entropy
denoted by H , we can derive that:
I(z, f(x, z)) = H(z) — H(z|f(x, z))	(16)
=Ey0 〜Psyn(y0)[Ez0 〜p(zg[lθg P(Zly')]]+ H (Z)	^)
=EyO 〜Psyn(y0)[DKL(P(IyO)||Q(IyO)) + Ez0 〜p(z|y0)[log Q(ZlyO)]] + H(Z)	(18)
≥ Ey0 ~Psyn(y)[Ez0〜p(z|y0)[log Q(Zly0)]]	(19)
=EZ~Plat(z),y0~Psyn(f (x,z)) [lOg Q(ZIy')]	QO)
=EZ 〜Plat (Z),X 〜Pcn(X) [[lθg Q(ZIf(X, z))]	QI)
≈ ɪ XX(-ln^ - 1(Zi - f(xj,Zi))T∑-1(Zi - f(xj,Zi)) + constant)
nm	2	2
i=1j=1
(22)
(21) and (22) follows the Lemma 5.1 in Info-GAN (Chen et al. (2016)) and law of the unconscious
statistician (Lotus) Theorem respectively. Therefore, we derive that the mutual information between
the latent variable Z and synthesized PT data y0 is bounded below by nm PZi Pm=ι(- ln^ -
2 (Zi - f (xj, Zi))TΣ-1(zi - f (xj, Zi)) + constant). With a further assumption that Σ is an identity
matrix, maximization of this lower bound is equivalent to minimizing the reconstruction loss Lrecons.
A.3 Network Architecture
Table 2 and Table 3 display network architectures of transformation function f, discriminator D and
reconstruction function g (consisting of gi and g2).
Table 2: Architecture of transformation function f
	Layer	Input Size	Bias Term	leaky relu α	Output Size
Encoder from x	Linear1+Leaky-Relu Linear2+Leaky-Relu	139*1 69*1	No No	0.2 0.2	69*1 34*1
Decoder from Z	Linear1+Sigmoid	M*1	Yes	NA	34*1
Decoder to yO	Linear1+Leaky-Relu	34*1	No	0.2	69*1
	Linear2+Leaky-Relu	69*1	No	0.2	139*1
12
Published as a conference paper at ICLR 2022
Table 3: Architecture of discriminator D, Decomposer g1 and Reconstruction g2
	Layer	Input Size	Bias Term	leaky relu α	Output Size
	Linear1+Leaky-Relu	139*1	Yes	0.2	69*1
Discriminator	Linear2+Leaky-Relu	69*1	Yes	0.2	34*1
	Linear3+Softmax	34*1	Yes	NA	2*1
g1	Leaky-Relu+Linear1	139*1	Yes	0.2	(139*M)*1
	Linear1+Leaky-Relu	139*1	Yes	0.2	69*1
g2	Linear2+Leaky-Relu	69*1	Yes	0.2	34*1
	Linear3+Softmax	34*1	Yes	NA	M*1
A.4 Algorithm
Detailed training procedure of Surreal-GAN is disclosed by Algorithm 1.
Algorithm 1: Surreal-GAN training procedure. lc represents cross entropy loss and ei represents
a one hot vector With 1 at ith component. q〃x,z), lg2(g1(f(x,z)) and Af(x,z)are defined m
section 2.2.3 and 2.2.4.
while not meeting stopping criteria or reaching max_epoch do
for all batches {xi}im=1,{yi}im=1 do
sample m latent vectors {zi}im=1 from multivariate uniform distribution With
Zi 〜U[0,1]M
Update weights of discriminator D: Use ADAM to update θD With gradient:
VθD ml PM[(lc(D(yi), eι) + lc(D(f (Xi,Zi), eo)))]
sample m latent vectors {zi0}im=1 conditioned on previously sampled {zi}im=1, With each
z0ij 〜 U(zij, 1]M
sample another m latent vectors {zicn}im=1 With each zicn 〜 U(0, 0.05)M
Update weights of transformation function f : Use ADAM to update θf With
gradient: Vθf m1 P=[lc(D(f (xi, Zi), eι))) + γ(∣∣f (xi, Zi)- Xi∣∣ι) +
K||g1(f (Xi, Zi)) - qf(xi,Zi) ||2 + Zl%2(g1(f(xi,Zi)) - zi||2 + λllAT(xi,Zi) Af(Xi,Zi)-
IIIf + μ∣lmaχ(∣f(χi, Zi)- Xi| - |f(χi,zi) - Xi|,0)∣∣2 + η∣∣f(χi,Zcn)- Xi||i]
Update weights of function g1 : Use ADAM to update θg1 With gradient:
Vθgι ml Pi=i[∣∣gι(f (Xi, Zi))- ^f (Xi,zi)l∣2]
Update weights of function g2 : Use ADAM to update θg2 With gradient:
vθg2 m1 ∑m=i[lllg2 (gι(f (Xi,zi)) - Zi||2
end
end
B	Appendix: Experiments
B.1	Regions with simulated atrophy in semi-synthetic datasets
Regions of interest (ROI) included in three patterns (P1-P3) in different semi-synthetic datasets are
revealed by Table 4. Small overlapping case indicates the included ROIs in three different dataset
as described in section 4.2: (1) Basic dataset; (4)Within-pattern noise dataset and (5)Mild atrophy
dataset. Each ROI is a combination of left and right parts of the region, so each check sign represents
inclusion of 2 out of 139 regions.
13
Published as a conference paper at ICLR 2022
Table 4: ROIs included in synthetic patterns (OP:opercular part; TP: triangular part)
ROI	Small overlapping			Large overlapping			Scarce Region		
	P1	P2	P3	P1	P2	P3	P1	P2	P3
Amygdala	X			/	/		/		
Hippocampus	X	X		/	/		/		
Angular gyrus						/			/
entorhinal area	X			/					
frontal operculum		X			/				
inferior temporal gyrus	X			/					
lateral orbital gyrus			^^/^^			/			
medial frontal cortex		X	^^/^^		/	/			
middle frontal gyrus		X			/	/		/	
middle occipital gyrus			^^/^^			/			
OP of the inferior frontal gyrus		X			/			/	
parahippocampal gyrus	X			/					
posterior insula		X			/				
parietal operculum	X		^^/^^	/		/			
supramarginal gyrus			^^/^^			/			
superior parietal lobule				/		/			/
temporal pole	X			/					
TP of the inferior frontal gyrus		X			/				
B.2	Hyper-parameter Selection
We trained the model on the basic case and the mild changes case (the easy and hard case) with
varying hyper-parameters to test the model robustness to hyper-parameter selections. Specifically,
we changed each parameter from 50% to 150% of the preset value (introduced in section 3.3) while
keeping other parameters fixed. With each set of parameters, we trained the model 10 times and
reported average pattern-c-index and average pattern-agr-index as described in section 4.2. From
Fig. 4a, we can see that model performances are mostly robust to different choices. For the mild
changes case, models, with different η values, do show higher variations in performances. However,
pattern-agr-indices still have good indication of the best choice, and this metric can always be used
for parameter selections on different datasets.
B.3	Contribution from Regularization
Similar to section B.2, we set different parameters to 0 to understand contributions from correspond-
ing regularization terms. Specifically, we set κ = 0 (no decomposition loss), λ = 0 (no orthogonal-
ity loss), μ = 0 (no monotonicity loss), Z = 0 (no Cn loss), μ∕Z = 0 (no constraint on monotonicity
and positive correlation), κ∕λ∕μ∕Z = 0 (no extra regularization compared to Smile-GAN frame-
work) respectively with other parameters fixed. With different sets of parameter values, we trained
the model 10 times and report average pattern-c-indices. From Fig.4c, we can clearly understand the
importance of different regularization terms and visualize how additional regularization terms lead
to improved representation performances compared to a pseudo comparable ”Smile-GAN” (Using
same regularization as Smile-GAN but with continuous latent variable).
14
Published as a conference paper at ICLR 2022
-n
no cn/mono
—SmiIe-GAN*
Figure 4: Regularization and hyper-parameters. (a) Representation accuracy on different tasks with
variations in hyper-parameters (measured by Pattern-c-index); (b) Agreements among repetitively
trained models on different tasks with variations in hyper-parameters (measured by Pattern-agr-
index) (results presented as mean value ± standard error for (a,b)); (c) Representation accuracy
with regularization terms removed. (*Smile-GAN here refers to a ”pseudo Smile-GAN” with the
same regularization terms included in Smile-GAN model. Latent variable z is sampled from same
continuous distribution as in Surreal-GAN.)
Table 5: ”Practical upper bounds” of compared models
Mild Changes
all regularizations
no ortho
■ no decom
	Surreal-GAN	NMF	LDA	FA	OpNMF	DNMF
Basic Case	0.838 ± 0.011	0.834 ± 0.000	0.769 ± 0.005	0.750 ± 0.000	0.751 ± 0.000	0.631 ± 0.017
Large	0.837	0.844	0.777	0.721	0.715	0.641
Overlapping	± 0.008	± 0.000	± 0.001	± 0.005	± 0.000	± 0.017
Scarce Regions	0.783	0.760	0.640	0.618	0.666	0.548
	± 0.007	± 0.000	± 0.015	± 0.001	± 0.000	± 0.006
Within-pattern	0.818	0.831	0.775	0.764	0.730	0.644
Noise	± 0.013	± 0.000	± 0.003	± 0.000	± 0.015	± 0.031
Mild Changes	0.762	0.756	0.657	0.613	0.641	0.571
	± 0.023	± 0.000	± 0.028	± 0.000	± 0.000	± 0.015
B.4	Data Preprocessing for Compared Models
Z-scores of ROIs with respect to the CN group were directly used as input features of FA model.
However, for NMF, OpNMF and LDA model, we need to borrow a prior knowledge that atrophy
(volume decreasing) is more common in dementia-related brain diseases, and thus, reversed the sign
15
Published as a conference paper at ICLR 2022
Table 6: Comparisons with supervised regression models
Task	Surreal-GAN		LR		SVR	
	Train	Test	Train	Test	Train	Test
Basic Case	0.827	0.830^^	0.896	0.871	0.926	0.863
	±0.005	±0.007	±0.001	±0.003	±0.001	±0.006
Large Overlapping	0.828	0.827	0.901	0.876	0.927	0.869
	±0.008	±0.007	±0.001	±0.003	±0.001	±0.003
Sparse Regions	0.764	0.761	0.853	0.816	0.932	0.804
	±0.010	±0.014	±0.001	±0.005	±0.002	±0.009
Within Pattern Noise	0.813	0.812	0.890	0.862	0.926	0.857
	±0.007	±0.009	±0.001	±0.003	±0.001	±0.009
Mild Changes	0.741	0.743	0.853	0.817	0.929	0.811
	±0.010	±0.017	±0.001	±0.003	±0.009	±0.005
of z-scores, kept positive z-scores, and set negative values to 0. For LDA model, we further dis-
cretized the value by first multiplying z-scores by 10 and then mapping them to the closest integer
as described in Zhang et al. (2016). For the DNMF model, both CN and PT data and their cor-
responding diagnosis labels are provided as input, with ROI data first standardized to lie in range
0-1.
B.5	”Practical Upper Bounds” of Unsupervised Methods
As discussed in section 5.1, unsupervised methods were not designed to best utilize the CN data
as a reference distribution. Thus, we derived ”practical upper bounds” for compared models on
different tasks, with the assumption that these methods have some components capturing the ground
truth while having other components representing non-disease related variations. On each semi-
synthetic test, we ran each method with number of components, M, ranging from 3-10. For M > 3,
we intensively searched for the combination of 3 components that best match the ground truth and
calculated the pattern c-index. The best result among M = 3 - 10 is considered as an ”practical
upper bound” for the model. As shown in Table 5, upper bounds of NMF methods are comparable
to results from Surreal-GAN, while other methods still show much worse performances. Note that
these are ”practical upper bounds” derived with known ground truth. In real cases, we are not
interested in a set patterns that includes the true disease effects but want to find disease-related
patterns only. However, it will be very hard to know what the best number of components is and
which components are truly disease-related with these compared models. In contrast, Surreal-GAN
can directly and accurately output disease-specific representations without indices capturing non-
disease related variations, which is one of the key advantages of the proposed method.
B.6	Comparisons with Supervised Methods
We further compared the model with linear regression (LR) and support vector regression (SVR)
with RBF kernel to understand the gap between our model and the most optimal model performances
in semi-synthetic tests. A five fold cross validation were run with three different models. Average
pattern-c-indices on both train and test sets are reported. We can first observe that Surreal-GAN, as
an semi-supervised methods, shows almost equal performances on train and test set, proving a good
ability in generalization. On five different tasks (table6), there is a 0.04-0.07 pattern-c-indices gap
between Surreal-GAN and supervised model performances. This gap is considered reasonable but
there is still space for model improvements.
B.7	Generalization and Reproducibility on Real Dataset
We randomly half split the real data (described in section 4.3) in train and test set, with both CN
and PT data equally distributed in two sets. Following same experiment procedure described in
section 4.3, we first trained the model on the training set, and then applied the model to derive 2-
dimensional R-indices for all CN and PT data in the test set. Voxel-wise group comparisons were
16
Published as a conference paper at ICLR 2022
Figure 5: Voxel-wise statistical comparison results on test set. (FDR method with p-value threshold
of 0.05 was used for multiple comparison).
conducted between subgroups and CN in the test set for visualization. From Fig. 5, we can observe
that R-indices are correlated with almost same patterns shown in Fig. 3, though much smaller sample
size led to relatively lower statistical significance.
B.8	Low Dimensional Representation for AD Diagnosis
We used R-indices of test data introduced in section B.7, and compared their ability to classify
CN and PT with that of 139 ROI volumes. Support vector machine (SVM) was selected as the
classification model. 20% leave-out experiments were performed 100 times with derived R-indices
and 139 ROI as input features respectively. In each run, we randomly sampled 80% of data as
training set and applied trained models on the rest 20% to derive the AUC metric. Grid search
were used for parameter selection. With 2D R-indices as features, SVM model was able to reach
an average AUC, 0.895 ± 0.011, which is only mildly lower than AUC derived with 139 ROIs as
features, 0.929 ± 0.015. Therefore, we can conclude that, 2D R-indices, kept almost all discriminant
information among PT data, while parsing the heterogeneity in a clear, concise and interpretable
way.
Figure 6: Procedure for deriving Surreal-GAN input features (ROI volumes): MRI images were
segmented using the MUSE method (Doshi et al. (2015)). Volumes of segmented regions (ROI
volumes) were then extracted as input features for Surreal-GAN model.
17
Published as a conference paper at ICLR 2022
B.9	Associations with Clinical Variables
Each dimension of R-indices, ri , is compared with different clinical variables for AD disease. For
each clinical variable, we only selected out samples who have it available for statistical analyses.
ADNI composite scores, including ADNI-EF, ADNI-MEM and ADNI-LAN, measure subjects’
functions in executive functions, memory and language respectively. They are only available among
subjects from Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. Digit Symbol Substitu-
tion Test (DSST) measures a range of cognition operations including motor speed, attention, and vi-
suoperceptual. For all mentioned clinical scores, a higher value indicates a better performance. CSF-
Abeta, CSF-Tau and CSF-PTau are three very important hallmarks of AD disease corresponding to
amyloid and tau hypotheses. Since measures of these three values have great variance among differ-
ent studies, only ADNI samples with CSF measures were included for analysis. For dichotomous
variables (Apoe-E4 Alleles Carrier & Hypertension), point biserial correlation with corresponding
p value was derived between the variable and each pattern dimension, ri . Pearson correlations were
derived for other continuous variables.
Table 7: Correlation between pattern severity and clinical variables for AD disease
r1	r2		
White matter lesion volumes	0.415 (p<0.001)	0.184 (p<0.001)
DSST	-0.055 (p=0.603)	-0.212 (p=0.042)
Apoe-E4 Alleles Carrier	0.003 (p=0.965)	0.212 (p<0.001)
Hypertension	0.181 (p<0.001)	-0.006 (p=0.890)
CSF-Abeta	-0.238 (p<0.001)	-0.302 (p<0.001)
CSF-Tau	0.111 ( p=0.002)	0.270 (p<0.001)
CSF-PTau	0.059 (p=0.103)	0.152 (p<0.001)
ADNI-EF	-0.409 (p<0.001)	-0.240 (p<0.001)
ADNI-MEM	-0.453 (p<0.001)	-0.466 (p<0.001)
ADNI-LAN	-0.422 (p<0.001)	-0.281 (p<0.001)
18