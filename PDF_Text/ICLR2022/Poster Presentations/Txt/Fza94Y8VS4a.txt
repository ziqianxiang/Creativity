Published as a conference paper at ICLR 2022
The evolution of uncertainty
OF LEARNING IN GAMES
Yun Kuen Cheung*
Royal Holloway
University of London
Georgios Piliouras*
Singapore University of
Technology and Design
Yixin Tao*
London School of
Economics
Ab stract
Learning-in-games has become an object of intense interest for ML due to its
connections to numerous AI architectures. We study standard online learning in
games but from a non-standard perspective. Instead of studying the behavior of a
single initial condition and whether it converges to equilibrium or not, we study the
behavior of a probability distribution/measure over a set of initial conditions. This
initial uncertainty is well motivated both from a standard game-theoretic perspec-
tive (e.g. a modeler’s uncertainty about the agents’ initial beliefs, random external
signals) as well as from a ML one (e.g. noisy measurements, system initialization
from a dataset distribution). Despite this, little is formally known about whether
and under what conditions uncertainty is amplified or reduced in these systems.
We use the popular measure of differential entropy to quantify the evolution of
uncertainty. We find that such analysis shares an intimate relationship with vol-
ume analysis, a technique which was recently used to demonstrate the occurrence
of Lyapunov chaos when using Multiplicative Weights Update (MWU) or Follow-
the-Regularized-Leader (FTRL) algorithms in zero-sum games. This allows us to
show that the differential entropy of these learning-in-game systems increases lin-
early with time, formalizing their increased unpredictability over time. We show-
case the power of the framework by applying it in the study of multiple related
systems, including different standard online optimization algorithms in numerous
games and dynamics of evolutionary game theory.
1	Introduction
A primary goal of ML research is to understand the behavior of learning algorithms in various
settings. One standard approach is to determine from each initial condition whether a learning algo-
rithm converges to a local optimum or stable state. Yet, in the context of online learning in games,
and more generally in distributed multi-agent learning, it is natural to consider the evolution of a
probability distribution over initial conditions instead. In these settings, each agent forms an initial
belief on her own, and she typically does not reveal her belief to other agents or external model-
ers/analysts. For a modeler to understand the possible behaviors of the system while being uncertain
of the agents’ beliefs, one natural approach is to infer how the initialization distributes by using data
of observations from the past. The modeler then uses this inference to predict the likelihoods of
different outcomes in the future, either by simulation or by analysis. Also, random initialization can
happen due to random external signals, e.g. weather, as well as noisy measurements. For readers
who wish to learn more mathematical aspects and intuition behind of such models, see Appendix A.
In such cases, it is critical to understand whether the initial probability distribution evolves to-
ward stability, or if its uncertainty gets amplified. Such analysis provides insight into the learning
system’s robustness against random initialization and environmental perturbations. If a system coor-
dinator desires stability but the analysis shows the uncertainty gets amplified, she ought to coordinate
with the agents to make changes, e.g. encourage them to use other online learning algorithms.
To analyze how uncertainty evolves, we need a measure of it. A natural choice is entropy.
In his seminal work in 1948, Claude Shannon formulated an axiomatic foundation of informa-
tion theory and introduced the seminal definition of Shannon entropy (SE): given a discrete ran-
dom variable with possible outcomes x1, . . . , xn which occur with probability p1, . . . ,pn, its SE is
* E-mails: yunkuen.cheung@rhul.ac.uk, georgios@sutd.edu.sg, Y.Tao16@lse.ac.uk
1
Published as a conference paper at ICLR 2022
- Pin=1 pi log pi = E [log(1/pi)]. Entropy is the canonical measure of uncertainty: when pi = 1
for some i, the distribution is considered certain and its entropy is zero; uniform distribution is
viewed as the most uncertain, and it attains the maximum possible entropy of log n. For continuous
random variables with probability density function g, Shannon proposed an extension of SE called
the differential entropy (DE), defined as E [log(1/g(x))] = - X g(x) log g(x) dx, where X is the
support set of the random variable. We will analyze how DE evolves in multi-agent learning.
Our Contributions. In our model, a learning-in-game system starts with a distribution over a
set of initial conditions in the cumulative payoff space, a coordinate system which is inherent in
the implementations of many online learning algorithms. The initial distribution evolves over time
depending on the combination of agents’ learning algorithms and the underlying game. We focus on
two popular learning algorithms, Multiplicative Weights Update (MWU) and its optimistic variant
(OMWU).1 The game settings include the standard two-player matrix games, and one-population
games which are fundamental in biological evolution models. We show that the DE of a broad range
of learning-in-game systems increases linearly with time (up to a certain limit), formalizing their
increased unpredictability. Such systems include MWU in zero-sum games, OMWU in coordination
games, and MWU in certain one-population games which reward more to intra-species play than to
inter-species play. Our results apply to any initial distribution of absolutely continuous random
vectors with finite differential entropy.
At this point one may naturally wonder: What level of uncertainty does a linear increase of DE
indicate? What are its implications? To answer these questions, it is best to compare ourselves
against other simple benchmarks. Consider the following simple learning system: in each round,
the payoffs for each action are generated from a uniform distribution over [-1, 1]. In the cumula-
tive payoff space, the distribution at time T converges to a multivariate Gaussian distribution with
variance Θ(T), and hence the entropy grows at a rate of O(log T), much slower than linear growth.
In Section 4.1, we present an implication of linear DE growth in learning-in-game systems. Briefly
speaking, the DE cannot increase substantially for an indefinite amount of time in such systems.
Thus, the distribution after a sufficiently long time must concentrate in a region that yields slower
DE growth or decline; such region does not cover any point in the cumulative payoff space that cor-
responds to an interior Nash equilibrium. We will refer to this phenomenon by “the grand escape”.
See Theorem 4.5 for the formal description of the grand escape phenomenon; an additional assump-
tion, namely the initial distribution has bounded support, is needed to establish this theorem. For the
implications in information theory, we refer readers to Chapter 8 of Cover & Thomas (2006).
The central tool in analyzing the changes of DE is the Jacobian matrix of our multi-agent dy-
namical system. This is also the key notion in volume analysis, which was used in a recent series
of papers to demonstrate Lyapunov chaos in learning-in-game systems. (Cheung & Piliouras (2019;
2020); Cheung & Tao (2020)) By showing that the determinant of the Jacobian matrix is strictly
above 1 in a large domain of the cumulative payoff space, they showed that the volume (Lebesgue
measure) of a small set of initial conditions will blow up exponentially to become a set of large
diameter, thus demonstrating a general Lyapunov chaos phenomenon in learning-in-game. Indeed,
the same property of Jacobian matrix guarantees linear increase of DE. Additionally, we present the
first Jacobian analysis of online learning in one-population games. Finally, our DE analysis is robust
against small perturbations. Consequently, our results extend to the setting where the game in each
round can be different, as long as the payoffs are perturbations of each other. Such settings capture
many games in the real world, where we know the “reference values” of the payoffs, but the accurate
payoffs in each round differ slightly due to unknown external (random or adversarial) factors.
Our model and analysis can be viewed as an a strengthening of volume analysis. Volume analysis
focuses on whether it is possible to reach a state or not, whereas in our model we are also concerned
with how probable/likely it is to reach a state. More explicitly, a state can be reached but only
with a tiny chance. When studying chaos and volume, such states matter, but less so when studying
uncertainty as their contributions to entropy will be low. To illuminate this, we simulate MWU (with
step-size 0.01) in Matching Pennies game, and present a few plots in Figure 2. The top-left plot
shows the initial set, which is a small square around the unique Nash equilibrium. The top-right plot
1As we shall point out in Section 3, all our results about MWU also extend easily to the broad family
of Follow-the-Regularized-Leader (FTRL) algorithms, which is a generalization of MWU. For clarity of our
presentation, we choose to focus on MWU in this paper.
2
Published as a conference paper at ICLR 2022
-1∙∞ H------1-----1------1-----1-----1-----1-----1-----
-1.00	-0.75	-0.50	-0.25	0 00	0.25	0.50	0.75	1.00
LcO
0.58
0.55
0.52
0.49
046
Figure 1: “Possible” vs. “Probable”: In the heat-map, red (warm) and blue (cool) colors represent
high and low densities respectively.
shows the range of possibility after 40000 steps2. In our model, we assume the initial distribution
is uniform over the small square, and plot the heat-map of probability densities after 40000 steps
(bottom-left). We observe that the states in the boundary of the vortex are more probable to occur,
while the densities around the Nash equilibrium decline. The bottom-right plot shows the densities
that generate the heat-map.
Further Related Work. Learning models that explicitly study the effects random initialization
have received relatively little attention in game theory where static equilibrium concepts are typically
the preferred solution. Lahkar & Seymour (2013) studies online learning under random initializa-
tion in population dynamics, where one population game is a common payoff model. In population
dynamics, there exists a large population of agents who have different initial choices of mixed strate-
gies, modeled as a distribution. The dynamics proceed by randomly pairing up agents in each round
to play a game. Lahkar & Seymour (2014) extends this framework to other learning dynamics to
describe the evolution of a distribution of mixed strategies in a population playing variants of Rock-
Paper-Scissors games establishing local convergence to the interior equilibrium. Recently, further
learning models inspired by Q-learning dynamics have been defined (Hu et al., 2019). Very little is
formally known about the behavior of such models particularly under instability.
For zero-sum games with an interior Nash equilibrium, “the grand escape” phenomenon follows
from Bailey & Piliouras (2018); Cheung (2018), who showed that MWU diverges away from Nash.
Our results are more general, since their analysis only works with those zero-sum game dynamics,
while our technique applies to many other game dynamics as well. Such instability results in zero-
sum games are undesirable for ML applications such as GANs thus a lot of effort has been invested
in developing algorithms that achieve convergence Daskalakis et al. (2017); Mertikopoulos et al.
(2019); Daskalakis & Panageas (2018); Gidel et al. (2019); Mokhtari et al. (2020); Perolat et al.
(2021). Our research direction here is in some sense orthogonal. We aim to characterize in a more
detailed fashion the behavior of learning dynamics despite their instability.
The instability of MWU, FTRL and other optimization-driven dynamics in games has attracted
a lot of attention in recent years. One formal technique to establish the unpredictability of MWU
2This phenomenon was first discussed in Cheung & Piliouras (2019) and called the “von Neumann vortex”.
3
Published as a conference paper at ICLR 2022
and other dynamics is based on proving Li-Yorke chaos, which has been established robustly in dif-
ferent families of routing/potential/Cournot games (Palaiopanos et al. (2017); Chotibut et al. (2020);
Bielawski et al. (2021); Cheung et al. (2021)). The techniques in those papers are independent from
ours as they require the identification of one dimensional invariant manifolds, and Li-Yorke chaos
implies the existence of periodic orbits, which is not possible in our systems. In a very recent series
of works, Flokas et al. (2020); Giannou et al. (2021) established that all (even partially) mixed equi-
libria in all games are not asymptotically stable for any choice for FTRL dynamics both in discrete
as well as in the idealized continuous-time limit. In fact, it is possible to construct simple matrix
games such that the orbits of the resulting dynamics can be arbitrarily complex Andrade et al. (2021).
If, inspired by ML applications, we allow for more complex differentiable games it becomes even
simpler to establish strong instability results for effectively any type of training algorithms (Letcher
(2020); Balduzzi et al. (2020)). The sweeping nature of these strong negative results showcases the
practical interest in uncovering more information about the nature of unstable dynamics of games.
Our proposed framework introduces a novel, quantitative methodology for their study.
The notion of differential entropy (DE) has been used as uncertainty measure in many works
across multiple disciplines. We note that while the extended definition of DE seems natural, it is
not considered as the perfect generalization of SE since it misses some favorable properties, and
therefore, as quoted from the popular information theory text of Cover & Thomas (2006), “there
is need for some care in using the concept”.3 Nevertheless, DE remains an effective measure of
uncertainty4, which is commonly used in physics, economics, control theory, engineering, biology,
medical research and beyond. For a general discussion, we refer readers to the text of Cover &
Thomas (2006). For background information of evolutionary game theory, we recommend Hofbauer
& Sigmund (1998); Sandholm (2010); Vega-Redondo (1996).
2	Model
We use bold small letters to denote vectors, and bold capital letters to denote matrices. Let ∆d denote
the probability simplex of dimension d: ∆d := {x ∈ Rd : for each j, xj ≥ 0, and Pjd=1 xj = 1}.
MWU and OMWU. When an agent uses MWU, she wants to choose among d ≥ 2 pure actions
based on the past cumulative payoffs to these actions. The process starts at discrete time t = 0,
where the agent chooses an initial cumulative payoff vector p0 ∈ Rd. At any time t ≥ 1, upon
receiving the payoffs to the each of the d actions at time t - 1, which are denoted by a vector
rt-1 ∈ Rd, the agent updates the cumulative payoff vector by the rule
∀j = 1,2,...,d,	Ptt 一 PjT + 〜rj-1,	(1)
where > 0 is the step-size of the update. At any time t ≥ 0, the agent chooses randomly among the
d pure actions depending on the value of pt . Precisely, the agent chooses a mixed strategy xt ∈ ∆d
which is a function of pt.For MWU, xt is determined by the rule
∀j = 1, 2,..., d,	Xt — exp(pj)/ (X exp(pk)).	⑵
For OMWU, we start with p0 = p1 , and for t ≥ 2,
∀j = 1, 2,..., d,	Pt - PtT + e ∙ (2rj-1 - rj-2),	⑶
and xt is determined by the same rule (2).
In general, the payoff vectors rt-1 can be arbitrary, and they may or may not depend on the
mixed strategy xt-1 . However, in the context of learning-in-game, the underlying game generates
the payoff vectors that depend on the mixed strategies of the agents involved. We discuss two popular
game models relevant to our work, namely two-player matrix games and one-population games.
3Two issues with DE are: (1) DE can be negative and indeed it has no finite lower bound, while SE is always
positive; (2) DE depends on the choice of coordinate system. The positivity of SE is favorable in information
theory where they want to measure information contents of different communications. But for the purpose of
measuring and comparing uncertainty, positivity does not seem relevant. (2) is not an issue for us, since we will
always stick to a fixed coordinate system which is natural in learning-in-games.
4DE captures uncertainty well, e.g. for all popular distributions (Gaussian, uniform, exponential, Rayleigh
etc), their DE increases with their variances. Also, among all distributions over a bounded support set, the
uniform distribution over the support set attains maximum DE.
4
Published as a conference paper at ICLR 2022
Two-Player Matrix Games. In a two-player matrix game, Player 1 has n actions and Player 2
has m actions. The game is specified by two matrices A, B ∈ [-1, 1]n×m. When Player 1 chooses
action j and Player 2 chooses action k, Ajk , Bjk are the payoffs to Players 1 and 2 respectively.
When the players choose mixed strategies, the payoffs are extended via expectation. Denote the
mixed strategies of Players 1 and 2 by x ∈ ∆n and y ∈ ∆m . The payoff to action j of Player 1 is
[Ay]j, while the payoff to action k of Player 2 is [BTx]k. When both players use MWU or OMWU
to play this game repeatedly, we have the following discrete-time dynamical system, where pt , qt
denote the cumulative payoff vectors at time t of Players 1 and 2 respectively. The two players
choose initial cumulative payoff vectors p0 , q0 .
∀t ≥ 1,	Pt — PtT + 〜Ayt-1;
qt ― qt-1 + 〜BTxt-1.	(4)
Note that xt-1 is a function of Pt-1 as specified in (2); similarly, yt-1 is a function of qt-1. Thus,
we can also view (4) as an iterated function that maps from (Pt-1, qt-1) to (Pt, qt).
In the above setting, the game (A, B) is the same at all times t. It is also interesting to consider
scenarios where the game varies in each round. Our results extend to settings where there is a
“reference game” (A, B), but at each round the actual game being played is a perturbation of the
reference game. Precisely, the game at time t is (At, Bt) = (A, B) + (∆tA, ∆tB), where ∆tA, ∆tB
are matrices with maximum absolute entry at most β, for some small β > 0.
One-Population Games. One-population games is a fundamental model in mathematical biology,
which is widely used to explain the evolution of a population of different of species over time. An
one-population game is similar to a two-player game, but now Player 1 is playing against herself;
there is only one player (or in biology term, one population) in this game, while a mixed strategy
xt represents the fraction of different species among the population. An one-population game is
specified by a matrix A ∈ Rn×n . When the mixed strategy of the player is x ∈ ∆n, the payoff to
action j is [Ax]j. The resultant discrete-time dynamical system with MWU is
∀t ≥ 1,	pt J PtT + 〜Axt-1,	(5)
where xt-1 is a function of Pt-1 as in (2).
Random Initial Cumulative Payoff Vectors. In Section 1, we explained why we consider uncer-
tainty of the initial cumulative payoff vectors. This can be captured by a model where these vectors
follow a distribution. We use two-player game for discussion below; for one-population game it is
similar. Assume that the initial vectors (P0, q0) follow a joint probability distribution in the domain
Rn × Rm . As the initial vectors are random and they are updated according to the discrete-time
dynamical system (4), for any t ≥ 0, (Pt, qt) also follows a distribution, which is the main ob-
ject we analyze in this work. We are interested in knowing certain statistics of uncertainty of this
distribution, say differential entropy, which is the subject of the next section.
3 Differential Entropy
In this section, we discuss how changes of differential entropy (DE) can be analyzed in general, and
provide some intuition of the relationship between DE and volume. To proceed, we need a crucial
notion called the Jacobian matrix. Let f : Rd → Rd be a smooth function. Its Jacobian matrix is
Γ∂fι	∂fι	∂fι-∣
-π- -π---	∙ ∙ ∙	---
∂x1	∂x2	∂xd
Jf =	:	.	"J .	.
∂fd ∂fd …∂fd
∂x1 ∂x2	∂xd
We also need multivariate integration by substitution; see Appendix B for a discussion of it.
Let X be an absolutely continuous random variable in Rd . Let g be its probability density func-
tion, whose support set is X. The DE of X is h(X) := - X g(x) log g(x) dx, which we assume
to be finite. For any function f : Rd → Rd, f (X) is another random variable; we denote its prob-
ability density function by g, and its support set by Y. If f is a diffeomorphism, using integration
5
Published as a conference paper at ICLR 2022
by substitution, We have g(y) = g(f-1(y)) ∙ Idet J f-1 (y)∣. Using integration by substitution again,
the DE of f(X) can be computed as follows:
h(f(X))
-Y
g(y) log g(y)dy
- Yy g(f-1(y)) "det JfT(y)∣
log (g(f-1(y)) ∙ ∣det JfT(y)∣) dy
=-g g(x) ∙ I det Jf-1 (f(x))∣ ∙ log (g(x) ∙ I det J厂 1 (f(x))∣) ∙ |det Jf (x)| dx.
X
By the Inverse Function Theorem, ∣det Jf-1 (f (x))∣ ∙∣det Jf(x)∣ = 1. Thus,
h(f (X)) - h(X) =	g(x) log |det Jf (x)| dx
X
(6)
(7)
When the integral on the RHS is finite. This is true if |det Jf (x)| is a differentiable and bounded
function of x, Which holds in all learning-in-game systems We study. Thus, the Jacobian’s deter-
minant directly affects the change of DE. If there exists α > 0 such that |det Jf (x)| ≥ 1 + α for
all x ∈ X, then h(f (X)) - h(X) ≥ log(1 + α) > 0. In order to use (7) to analyze the changes
of DE of learning in tWo-player matrix games, We vieW the dynamical system (4) as a function that
maps from (pt-1, qt-1) to (pt, qt) in each round t ≥ 1. If the change of DE per round is at least
log(1 + α) > 0, then the DE increases linearly With time.
The Relationship Between DE and Volume. Before discussing the relationship, We first recap
What is volume. Let X ⊂ Rd be a measurable set With positive Lebesgue measure (Which is some-
times called the volume of the set), and let f : Rd → Rd be a diffeomorphism. The volume of f(X)
is X |det Jf (x)| dx. When |det Jf (x)| ≥ 1 + α for all x ∈ X, the volume increases by a factor of
at least (1 + α). If this occurs in a dynamical system every round, the volume of the initial set groWs
exponentially With time, and hence also its diameter, establishing that Lyapunov chaos occurs.
The occurrence of the Jacobian in both the formulae for computing volume and the change of
DE in (7) is not a coincidence, but With a simple intuition behind. The Jacobian yields a linear
approximation of f locally: for any x ∈ Rd and any small perturbation vector ∆x ∈ Rd, f(x +
∆χ) ≈ f (x) + Jf (x) ∙ ∆x. We consider a tiny hypercube with volume V around x, and its image
under the function f . By the linear approximation, the image is approximately a parallelotope
(the high-dimensional analogue of parallelogram), whose volume is known to be |det Jf (x)| ∙ v. By
partitioning X into infinitesimal tiny hypercubes and summing up, this is intuitively how the integral
formula for computing volume is obtained. Next, suppose the probability density at x is g(x). The
probability of the tiny hypercube is approximately g(x) ∙ v. After the transformation f, this amount
of probability is spread approximately uniformly in the parallelotope, thus the new density at f(x)
is g(x)/ |det Jf (x)|. If the volume increases locally at x, i.e. |det Jf (x)| > 1, it intuitively means
the uncertainty level increases locally. This intuition is captured by DE: the contribution to DE by
the tiny hypercube is -g(x) ∙ V log(g(x) ∙ v), while the contribution to DE by the parallelotope is
-g(x) ∙ v log ∣dgJ)∙vcj. Thus, the local change of DE is g(x) ∙ V log |det Jf (x)|, which has the same
sign as (|det Jf (x)| - 1). To summarize, local volume increase is equivalent local DE increase.
This equivalence allows us to spot that for those learning systems in two-player matrix games
studied in the series of volume analysis papers of Cheung & Piliouras (2019; 2020); Cheung &
Tao (2020), their DE with our model increases linearly in a large domain of the cumulative payoff
space. We present this result formally Section 4.1, and present an interesting consequence of the DE
growth. Partly motivated by Lahkar & Seymour (2013), we also transfer the techniques to analyzing
one-population games in Section 4.2, and we present two sufficient conditions on these games that
leads to linear DE growth with MWU.
As first spotted by Cheung & Piliouras (2019), these results not only cover MWU and OMWU,
but also the broad family of FTRL algorithms, since the properties of the Jacobian for FTRL is very
similar to that of MWU. (This is not surprising since FTRL is a generalization of MWU.)
4 Applications
In this section, we consider two applications: MWU in two-player zero-sum games and one-
population games. In Appendix C, we also consider OMWU dynamics in two-player coordination
6
Published as a conference paper at ICLR 2022
games and one-population games, and the settings where the game in each round is perturbed. A
two-player game (A, B) is zero-sum if A = -B. We focus on non-trivial games, defined below.
Definition 4.1 (Cheung & Piliouras (2019)) A zero-sum game (A, -A) is trivial if there exists
real numbers aι, a2, ∙∙∙ , an and bi, b2,…，bm such that Ajk = a§ + bk. The same definition
applies for one-population game A.
Cheung & Piliouras pointed out that a trivial game is not interesting as the players have dominant
strategies. They provided a measure of distance of a zero-sum game (A, -A) from triviality by
c(A) =	min	[max(Ajk - aj - bk) - min(Ajk - aj - bk)]	(8)
a1,∙∙∙ ,an,b1,∙∙∙ ,bm
4.1	MWU in two-player zero-sum game
Recall from (4) that in zero-sum game (A, -A), MWU dynamic in the cumulative payoff space is
pt+1 = pt + e ∙ Ay(qt) and qt+1 = qt + e ∙ (-A)>x(pt)
where x(pt) and y(qt) are the mixed strategies of two players at time t: xj (pt) =
exp(ptj)/ Pj0 exp(ptj0) and yk(qt) = exp(qkt )/ Pk0 exp(qkt 0). Two different regions are consid-
ered: the interior region S1 := {(p, q) | x(p) ≥ δ and y(q) ≥ δ}, and the boundary region S2,
which is the complement to S1 . Note that if the game possesses an interior Nash equilibrium, then
the equilibrium lies in S1 for any sufficiently small δ.
Let gt (p, q) denote the probability density of (p, q) at round t. Recall from 7 that the change
of entropy is	gt(p, q) log |det Jf (p,	q)|	d(p,	q).	If	gt (p, q)	is fully supported by	S1,	then by
using the Jacobian analysis of Cheung & Piliouras (2019), who showed that the determinant of the
Jacobian matrix is strictly larger than 1 in S1, we show the differential entropy increases linearly.
Theorem 4.2 For any non-trivial two-player zero-sum game (A, -A), with MWU dynamics of
step-size ≤ min{1/(32n2m2), δ 2 c(A)2 /8}, if gt(p, q) > 0 only for (p, q) ∈ S1, then the differ-
ential entropy will increase by at least **^)"% 加 round t.
Proof: For MWU dynamics, f(p, q) = (p+ Ay(q), q - A>x(p)) by (2). Cheung & Piliouras
(2019) showed that if E ≤ min{1∕(32n2m2), δ2c(A)2∕8}, then det Jf(p, q) ≥ 1 + δ c8A) •心
for any (p, q) ∈ S1, where c(A) is the measure of the distance of a zero-sum game (A, -A)
from triviality (8). As log(1 + r) ≥ ɪ+^ for any r > -1, / gt(p, q) log |det Jf(p, q)| d(p, q) ≥
62c(A)2∙e2∕8	口
1+δ2c(A)2∙e2∕8 .	Ll
Actually, the assumption that gt (p, q) is fully supported in S1 can be weakened. We show that
ifa sufficiently large probability lies in S1, then the differential entropy still increases linearly. With
this result, we can show Theorem 4.5 below, which states that there exists infinitely many rounds,
such that in each of these rounds t, the probability of (pt, qt) ∈ S2 will be at least some constant.
Theorem 4.3 If 八】g(p, q) d(p, q) ≥ λ, then the entropy will increase by at least 击^^&氤∙
λ 一 1—三∙ (1 — λ) in round t, which is strictly positive whenever λ > δ2l(A)2 ∙
Proof: CheUng & PiIioUras (2019) showed that if E ≤ min{1∕(32n2m2), δ2c(A)2∕8}, then
det Jf (p, q) ≥ 1+ δ C(A) ∙ e2 for any (p, q) ∈ Si, and det Jf (p, q) ≥ 1 - e3 for any (p, q) ∈ S?.
Recall that the change of entropy is / gt(p, q) log |det Jf(p, q)| d(p, q). As log(1 + r) ≥ ɪ++^ for
any r > -1, the change of entropy is at least
ZS	gt(p,	q)	log (1 + δ	c8A)	∙ E)	d(p, q) +	ZS	gt(p,	q)	log(1 - e3) d(p, q)
δ2c(A)2 ∙E2/8 ft,、、,	E3	t,
≥ 1 + δ2c(A)2 ∙ e2/8 JSIg (p，q) d(p, q) - L Js2 g (p，q) d(p, q).	口
In Theorem 4.3, for any fixed δ and c(A), the lower boUnd of λ decreases with E. In particUlar,
if E = δ2c(A)2∕64, the lower bound is 1/4, i.e. it only requires a small probability in Si for the DE
7
Published as a conference paper at ICLR 2022
to strictly increase. Theorem 4.3 indicates that the DE growth is at least linear (Ω(T)) as long as the
distribution has a sufficient amount of probability lying in S1. Next, in Lemma 4.4 we show that the
DE growth is at most logarithmic (O(log T)) whenever the initial distribution has bounded support.
These two seemingly contradicting bounds can only be compatible for one reason: the condition
needed by Theorem 4.3 will eventually be violated, i.e. there is insufficient amount of probability
lying in S1 after a sufficiently long time. We formalize this in Theorem 4.5 below.
Lemma 4.4 Suppose the initial distribution has bounded support in [-b, b]n+m, for some b > 0.
Then the differential entropy at time T is at most (n + m) log(T + b).
Proof: Since every entry in A is bounded in the interval [-1, 1], we have |ptj+1 - ptj | ≤ and
|qkt+1 - qkt | ≤ . Thus, the distribution at time T has bounded support in [-T - b, T + b]n+m .
Within this box, the maximal differential entropy is attained by the uniform distribution over it,
which is (n + m) log(T + b).
Theorem 4.5 For any non-trivial two-player zero-sum games (A, -A), with MWU dynamics of
step-size E ≤ min{1∕(32n2m2), δ2c(A)2∕8}, for any initial distribution with bounded support, and
for any λ > δ21(A)2, there exists an infinite sequence OftimeS (Ti, T2, •…)such that the probability
in S2 at each round Ti is at least 1 - λ.
The above theorem gives a lower bound of 1 - λ on the probability in the boundary area for
infinitely many steps, which is the ”grand escape” phenomenon we mentioned in the introduction.
4.2	MWU in one population game
Recall from (5) that in one-population game A, MWU dynamics in the cumulative payoff space is
pt+i = Pt + E ∙ AX(Pt), where Xk (Pt) = P：望鼠).
Similar to the two-player zero-sum games, we also consider two different regions: the interior
region Si = {P | X(P) ≥ δ}, and the boundary region S2, which is the complement to Si. Let
gt (P) denote the probability density of P at time t. We will show that for some sub-classes of one-
population games A, MWU dynamics increase differential entropy linearly under some conditions.
Also we show the analogue of Theorem 4.5: there exists infinitely many rounds, such that at each
of these rounds t, the probability of Pt ∈ S2 will be at least a constant. First of all, we present the
Jacobian analysis of such systems.
Jacobian Analysis. The Jacobian of the dynamics is J = I + e ∙ R, where I is the n X n identity
matrix, and R is an n × n matrix, in which
exp(pk)	exp(pk0 ) exp(pk)
Rjk = Ajkρ~~pvn∕n^^ʌ - TAjkO2	?72 = Xk(P) I Ajk - TAjkOxk0(P)).
k00 exp(pk00 )	k0	( k00 exp(pk00 ))	k0
For simplicity, with a little abuse of notation, we use Xk as a shorthand ofXk(P).
Next, we calculate the determinant of the Jacobian. When we compute it using the Leibniz
formula, it can be expressed in the following form, which is a polynomial of E:
det (J(p)) = 1 + E ∙ (first order term) + e2 ∙ (second order term) + …,
where the first order term is
):Rkk = x x XkAkk -): xk xk0 AkkO = $〉: xk xk0 (Akk + AkOkO - AkkO - AkOk).
k	k	k,kO	k6=kO
Thus, the following lemma gives the condition with which the first order term is non-negative. Note
that in order to have a increasing differential entropy, non-negative first order term is a necessary
condition for a small enough step size.
Lemma 4.6 The first order term is non-negative for all possible X if and only if Akk + AkOkO ≥
AkkO + AkOk for all k, k0. Moreover, if Akk + AkOkO ≥ AkkO + AkOk for all k, k0 and the inequality
is strict for some k, k0, then the first order term is strictly positive for all X ≥ δ.
In the terminology of mathematical biology, Akk is the payoff to species k for the intra-species
play within the species, while AkkO is the payoff to species k for the inter-species play between
species k and k0 . Informally speaking, the condition in the above lemma is: the one-population
game rewards intra-species play more than to inter-species play.
8
Published as a conference paper at ICLR 2022
Two Sufficient Conditions for Linear DE Growth. Recall that to demonstrate linear DE growth,
we want the determinant of the Jacobian to be strictly larger than 1. By Lemma 4.6, a clear sufficient
condition is: Akk + Ak0 k0 ≥ Akk0 + Ak0 k for all k and k0, and there exist k, k0 such that the
inequality is strict. But when there is no k, k0 with strict inequality, it is still possible to have linear
DE growth if the second order term is strictly positive. We present precise statements regarding
these two cases below. To proceed, we define s(A), which is positive for the first case:s(A) =
2 ∙ maxk,k0 {Akk + Ak0k0 - Akk0 - AkOk}.
For the first case, we now bound the higher order terms. Note that since |Akk0 | ≤ 1 and hence
|Rkk0 | ≤ 2. When expanding the determinant using the Leibniz formula, we get a polynomial of .
For any i ≥ 2, each i term in the expansion must be of the format i times a product of i factors
of the form Rk1,k2, so each such term has absolute value bounded by 2ii. To count the number
of such terms, note that within each term, the collection of k1’s is same as the collection of k2’s,
but the order of k2 can be any permutation of the k1 ’s. A simple counting shows there are at most
(n) ∙ i! ≤ ni such terms, and hence the i-th order coefficient is bounded by ni ∙ 2i. Therefore, when
E ≤ 16*), det J(P) ≥ 1 + δ S(A) ∙ e for all P ∈ Si, and det J(P) ≥ 1 — 8n2e2 for all P ∈ S2.
With a similar argument as in two-player zero-sum game, we achieve the following theorems.
Theorem 4.7 If S gt (P) dP ≥ λ, then the entropy will increase by at least
i-⅞252 ∙ (1 一 λ) in round t, which is strictly positive whenever λ > 双；：).
δ2s(A)∙e∕2	、
1+δ2s(A)∙e∕2 ∙ λ 一
Theorem 4.8 In a population game A such that Akk + Ak0k0 ≥ Akk0 + Ak0 k for all k and k0 and
there exist k and k0 such that the inequality is strict, with MWU dynamics of step-size E ≤ (^A),
32n2
for any initial distribution with bounded support, and for any λ > 武(A), there exists an infinite
seriesoftime (Ti, T2, •…)such that,forany i, the probability in S2 at round Ti is no less than 1 一 λ.
In Appendix D, in the same spirit we did in Figure 2, we present a few plots of MWU in an
one-population game that satisfies the condition in the above theorem.
Next, we consider the case that Akk + Ak0k0 = Akk0 + Ak0k for all k and k0. In this case, the first
order term is 0. Therefore, we will focus on the second order term, which is Pk <k (Rk1k1Rk2k2 一
Rk1 k2 Rk2 k1 ), which upon expansion becomes
一£ X : xkιxk2 (Ak1k2 一 X : Akιk0xk0) (Ak2k1 一 X : Ak2,k0xk0) -
k1,k2	k0	k0
Let C(A,B) (x, y) = 一 Pk1 ,k2 xk1 yk2 (Ak1 k2 一 Pk0 Ak1 k0 yk0 ) (Bk1 k2 一 Pk0 Bk0,k2 xk0 ). Then
the second order term is 2C(α,α>) (x, x). We will show this term is non-negative.
Lemma 4.9 If	Akk	+ Ak；ko =	Akk0+	Ak『k	for all k and	k0,	then 1 C(α,α>)(x, x)=
2 C(A,-A)(x,x) ≥，C(A).
Proof: By Cheung & Tao (2020), C(A,B)(x, y) = C(A,B+T)(x, y) if Tkk0 = uk + vk0 for all k
and k0. As (A> — (一4))叮 = Aji + Aij = Aii + Ajj, the first equality holds. The inequality
follows from Cheung & Piliouras (2019).
Thus, for the determinant of the Jacobian, the first order term is 0 and the second order term is at
leastδ C(A) in Si. Moreover, the i-th order term is bounded by ni2iei. Therefore, when E ≤ ⅛(A),
det J(P) ≥ 1 + δ 16A) ∙ e2 for any P ∈ Si, and det J(P) ≥ 1 — 16n3E3 for X ∈ S2.
Theorem 4.10 If 八］gt(p) dP ≥ λ then the entropy will increase by at least ι+δ⅞AAje2∕l6 ∙ λ 一
i-6n6⅛3 ∙ (1 — λ) in round t, which is strictly positive whenever λ > 512nAj.
Theorem 4.11 In a population game A such that Akk + Ak0k0 = Akk0 + Ak0k for all k and k0
and A is non-trivial, with MWU dynamics of step-size E ≤ f25(A3), for any initial distribution with
bounded support, and for any λ > 512A), there exists an infinite series of time (Ti, T2, ∙∙∙) such
that, for any i, the probability in S2 at round Ti is at least 1 一 λ.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We thank several anonymous reviewers for their suggestions. This research/project is supported in
part by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award
No: AISG2-RP-2020-016), NRF 2018 Fellowship NRF-NRFF2018-07, NRF2019-NRF-ANR095
ALIAS grant, grant PIE-SGP-AI-2020-01, AME Programmatic Fund (Grant No. A20H6b0151)
from the Agency for Science, Technology and Research (A*STAR) and Provost’s Chair Professor-
ship grant RGEPPV2101. Yixin Tao acknowledges ERC Starting Grant ScaleOpt-757481.
Reproducibility Statement
We present complete proofs for all theoretical results in this work.
Ethics S tatement
We do not see any ethical or future societal consequences of this work.
References
Gabriel P Andrade, Rafael Frongillo, and Georgios Piliouras. Learning in matrix games can be
arbitrarily complex. COLT, 2021.
James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In EC,
pp. 321-338, 2018.
David Balduzzi, Wojciech M Czarnecki, Thomas W Anthony, Ian M Gemp, Edward Hughes, Joel Z
Leibo, Georgios Piliouras, and Thore Graepel. Smooth markets: A basic mechanism for organiz-
ing gradient-based learners. arXiv preprint arXiv:2001.04678, 2020.
Jakub Bielawski, Thiparat Chotibut, Fryderyk Falniowski, Grzegorz Kosiorowski, MichaI Misi-
urewicz, and Georgios Piliouras. Follow-the-regularized-leader routes to chaos in routing games.
ICML, 2021.
Yun Kuen Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum
games. In NeurIPS 2018, pp. 3532-3542, 2018.
Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:
Chaos and butterfly effects of online learning in zero-sum games. In Conference on Learning
Theory, pp. 807-834. PMLR, 2019.
Yun Kuen Cheung and Georgios Piliouras. Chaos, extremism and optimism: Volume analysis of
learning in games. In NeurIPS 2020, 2020.
Yun Kuen Cheung and Yixin Tao. Chaos of learning beyond zero-sum and coordination via game
decompositions. In International Conference on Learning Representations, 2020.
Yun Kuen Cheung, Stefanos Leonardos, and Georgios Piliouras. Learning in markets: Greed leads
to chaos but following the price is right. In Zhi-Hua Zhou (ed.), IJCAI, pp. 111-117. ijcai.org,
2021. doi: 10.24963/ijcai.2021/16.
Thiparat Chotibut, Fryderyk Falniowski, MichaI Misiurewicz, and Georgios Piliouras. The route to
chaos in routing games: When is price of anarchy too optimistic? NeurIPS, 2020.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory 2nd Edition (Wiley Series
in Telecommunications and Signal Processing). Wiley-Interscience, 2006. ISBN 0471241954.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. arXiv preprint arXiv:1807.03907, 2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. arXiv preprint arXiv:1711.00141, 2017.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Thanasis Lianeas, Panayotis Mer-
tikopoulos, and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not
mix. NeurIPS, 2020.
10
Published as a conference paper at ICLR 2022
David Heaver Fremlin. Measure theory, volume 4. Torres Fremlin, 2000.
Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos.
Survival of the strictest: Stable and unstable equilibria under regularized learning with partial
information. COLT, 2021.
GaUthier GideL Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le PrioL Gabriel Huang, Si-
mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1802-1811.
PMLR, 2019.
Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge
University Press, 1998. doi: 10.1017/CBO9781139173179.
Shuyue Hu, Chin-wing Leung, and Ho-fung Leung. Modelling the dynamics of multiagent q-
learning in repeated symmetric games: a mean field theoretic approach. Advances in Neural
Information Processing Systems, 32:12125-12135, 2019.
Ratul Lahkar and Robert M. Seymour. Reinforcement learning in population games. Games and
Economic Behavior, 80(C):10-38, 2013. doi: 10.1016/j.geb.2013.02.006.
Ratul Lahkar and Robert M Seymour. The dynamics of generalized reinforcement learning. Journal
of Economic Theory, 151:584-595, 2014.
Alistair Letcher. On the impossibility of global convergence in multi-loss optimization. arXiv
preprint arXiv:2005.12649, 2020.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. International Conference on Learning Representations (ICLR), 2019.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artificial Intelligence and Statistics, pp. 1497-1507. PMLR, 2020.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. NeurIPS,
2017.
Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro
Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From Poincare re-
currence to convergence in imperfect information games: Finding equilibrium via regularization.
In International Conference on Machine Learning, pp. 8525-8535. PMLR, 2021.
William H. Sandholm. Population Games and Evolutionary Dynamics. MIT Press, 2010.
Fernando Vega-Redondo. Evolution, games, and economic behaviour. Oxford University Press,
1996.
11
Published as a conference paper at ICLR 2022
A	Motivating Example
To explain our mathematical model intuitively, we present the following simple one-dimensional
example. Let f (x) = 2x, and suppose that X0 is a random variable uniform on the interval [0, 1].
Then X1 = f(X0) becomes another random variable, which is uniform on [0, 2]. Analogously,
X2 = f(X1) is a new random variable being uniform on [0, 4]. Generally speaking, the problem is
to understand how the initial random variable X0 evolves under a given deterministic iterated update
rule f, and to study how the uncertainty evolves using the measure of differential entropy. In our
paper, we focus on such systems which arise in the context of learning-in-games.
In a learning-in-game system, X0 = (p0, q0) represents a distribution over initial beliefs (initial
cumulative payoffs) of the two players, and Xt = (pt, qt) represents the distribution of cumulative
payoffs at time t. Such a probabilistic formulation is natural from the perspective of an external
analyst (who is not a player of the game), who cannot be certain about what the initial beliefs the
players have. For her to proceed with an analysis of the system, a natural approach is to infer the
initial distributions from either the past data, or her knowledge about how random external signals
influence the initializations. The iterated update rule f in this case is determined by the learning
algorithms used by the players, and by the underlying game. For instance, if both players are using
MWU in a bimatrix game (A, B), the update rule f is given by (4).
Unlike the motivating example, for learning-in-game systems the dimensions can be arbitrarily
high, and the update rule is non-linear. Thus, deriving the exact evolution of the distribution of Xt
is very hard in general. But we can still use the popular statistical measure, differential entropy
(DE), to understand the evolution of uncertainty. We show that the DE increases linearly with time
for various combinations of learning algorithms and games, including MWU in zero-sum games,
OMWU in coordination games, and MWU in certain one-population games. Section 2 has precise
descriptions of the learning algorithms and the games. These results can be extended to a broad
family of non-zero-sum and non-coordination games, via the Jacobian analysis by Cheung & Tao
(2020). The results also extend to gradient ascent, as it is known to be a special case of FTRL.
B Integration by Substitution
A general theorem of integration by substitution is stated below.
Theorem B.1 (Fremlin (2000), Theorem 263D) Let D ⊂ Rd be any measurable set, and φ : D →
Rd be an injective function differentiable relative to its domain at each point of D. For each x ∈ D,
let Jφ (x) be the Jacobian of φ relative to D at x. Then for any real-valued function h defined on
φ(D),
h(x) dx
D
h(φ(x)) ∙ | det Jφ(x)∣ dx
if either integral is well-defined, provided that h(φ(x)) ∙ | det Jφ(x)∣ is interpreted as zero when
det Jφ(x) = 0 and h(φ(x)) is undefined.
In Section 3, we use integration by substitution twice. First, in the above theorem, by setting
h = g, φ = f-1 and D = f(X0) for any measurable set X0 ⊂ X, we have
g(x) dx
X0
j
f(X0)
g(f-1(x)) ∙ | det Jf-ι (x)| dx .
Recall that g is a probability density function of the absolutely continuous random variable X , so
the LHS of the above equality is same as P [X ∈ X0]. Thus, the integrand of the integral in the RHS
is the probability density function of f (X),i.e. g(x) = g(f T(X)) ∙ | det Jf-1 (x)|.
To derive (6), in the above theorem, We set h(χ) = -^(χ)log ^(χ), φ = f, D = X and hence
φ(D) = Y.
C More applications
C.1 OMWU in two-player coordination games
A tWo-player game (A, B) is a coordination game ifA = B. Similar to the MWU dynamics in tWo-
player zero-sum games, here We also look at tWo different areas: S1 = {(p, q)|x(p) ≥ δ and y(q) ≥
12
Published as a conference paper at ICLR 2022
δ} and S2, which is the complement to Si, S2 = Si. For a coordination game (A, A), CheUng &
PilioUras (2020) showed that when (p, q) ∈ Si, then det Jf(X) ≥ 1 + δ c8A) ∙ / when E is small
enoUgh and, when (p, q) ∈ S2, then det Jf (x) ≥ 1 - O(3). Therefore, by a calcUlation which is
similar to MWU in two-player zero-sUm game, we obtain the following theorems.
Theorem C.1 If R^ gt(p, q) d(p, q) ≥ λ then the entropy increases by at least 1+台；?)]；/% ∙ λ 一
O(E3)，(I- λ)∙
Theorem C.2 For any non-trivial two-player coordination games (A, A), with OMWU dynamics
of a small enough step-size E, for any initial distribution with bounded support, and for any λ such
that E = o(λ), there exists an infinite series of time (Ti, T2,…),such that the probability in S2 at
round Ti is at least 1 - λ.
C.2 OMWU in one population games
As pointed oUt in CheUng & PilioUras (2020), the continUoUs analogUe of OMWU, in the context
of online learning, is as follows. There are n actions. Let p(t) ∈ Rn denote the cUmUlative payoff
vector Up to time t, and u(t) ∈ Rn denote the instantaneoUs payoff vector to the n actions at time t.
The continUoUs analogUe is
P = U + E ∙ U.	(9)
In one-population game, u(t) = A ∙ x(t), where for the logit conversion used in MWU and OMWU,
Xk = exp(pk)/ £' exp(p'). It is easy to compute that
=~ = Xk 一(Xk)2	and	=一 = -Xkx' when ' = k.
∂pk	∂X'
Then for any ` = 1, 2, . . . , n, we have
∂[Axj
dp`
X∂Xk
.Ajk ∙ K
Aj'X' - ɪ2 Ajk XkX' = x' (Aj' - [Ax]j ) .
k
Following the notation in MWU case,
d[∂pχj = Rj'. By the chain rule, We have
U j = dAxj
n
ERj` ∙p`.
'=i
Putting the above equality into (9) yields
n
Pj = [AXj + e ∙ ERj' ∙ p',
'=i
which is a recurrence of the variables pι,p2,... ,pn. Unwinding this recurrence yields
n
Pj = [AXj + e ∙ X Rj' ∙[AX]` + O(E2).
'=i
The above system, upon Euler discretization to a discrete-time dynamical system with time interval
E, becomes
n
∀j,	pj+i	=	Pj	+ E ∙	[Ax]j	+ E2 ∙ X Rj'	∙ [Ax]'	+ O(e3).	(10)
'=i
Observe that on the RHS of (10), the first two terms coincide with that in the MWU case. The
remaining terms can be viewed as a second-order perturbation. Consequently, the Jacobian of the
system equation 10 is a second-order perturbation of the Jacobian for the MWU case, and hence
the constant and first-order terms of the determinants of these two Jacobians are the same, which is
Pjn=i Rjj .
Therefore, for the case that Akk + Ak0k0 ≥ Akk0 + Ak0k for all k and k0 and there exist k and k0
such that the inequality is strict, we obtain the following theorems. Recall that Si = {p|X(p) ≥ δ}
and S2, which is the complement to Si, S2 = Si; and s(A) = maxk*，{Akk +Ak，k，一Akk，-Ak，k }.
13
Published as a conference paper at ICLR 2022
Theorem C.3 If R^ gt(p) dp ≥ λ, then the entropy increases by at least 1+δ⅜(A∕∕2 ∙ λ 一 O(e2) ∙
(1 - λ) for a small enough step size .
Theorem C.4 In an one-population game A such that Akk + Ak0 k0 ≥ Akk0 + Ak0k for all k and k0
and there exist k and k0 such that the inequality is strict, with OMWU dynamics with a small enough
step-size , for any initial distribution with bounded support, and for any λ such that = o(λ), there
exists an infinite series of time (Ti, T2, •…)such that, for any i, the probability in S2 at round Ti is
no less than 1 一 λ.
For the case that Akk + Ak0k0 = Akk0 +Ak0k for all k and k0, we consider the second-order term.
The second-order terms of the two determinants (MWU and OMWU) are not the same though. The
second-order term of the determinant of Jacobian for the OMWU case is
nn	nn
X Rjj Rkk- X Rjk Rkj + XX Rj'R'j + XX ~∂j ∙ [AX]'.
1≤j<k≤n	1≤j<k≤n	j=1'=1	j=1'=1 Pj
Observe that the first two summations are just the same as those in the MWU case. Next, a straight-
forward calculation gives
Rjj ∙ 1j=' - Xj Rj' 一 x'Rjj,
where 1j=' = 1 if j = ', and 0 otherwise. Hence
dRj'
dPj
nn	n
X X ^∂j' ∙ [AX]` = X Rjj ∙[AX]j -
j=1'=1	pj	j=1
n
=X Rjj ∙[Ax]j —
j=1
nn
EE(XjRj' + X'Rjj) ∙ [Ax]j
n	nn	n
Xj [AX]j	Rj' 一	Rjj [AX]j	X'.
Since Pn=I X = 1 and Pn=I Rj' = 0, we have Pn=I Pn=I dRj ∙ [Ax]' = 0. To conclude, the
second-order term is
Rjj Rkk 一
1≤j <k≤n
nn
Rjk Rkj + ΣΣRj'R'j
n
X(Rjj)2 + X RjjRkk + X RjkRkj
j=1	1≤j<k≤n	1≤j<k≤n
XRJ
一	RjjRkk +
1≤j<k≤n
RjkRkj
1≤j <k≤n
Note that when Akk + Ak0k0 = Akk0 +Ak0k for all k and k0,	j Rjj = 0. Therefore, the second
order term of OMWU is the negative of the second order term of MWU, which will be smaller than
δ2c(A)
--------
8	.
Theorem C.5 In an one-population game A such that Akk + Ak0k0 ≥ Akk0 +Ak0kfor all k and k0,
δ2c(A)22
with OMWU dynamics, if fp(sS1 g(p) dp ≥ λ, then the entropy decreases by at least +2’我尸^ ∙
λ 一 O(e3) ∙ (1 一 λ) for a small enough step size .
C.3 Perturbations to Payoffs
In the main body, we focus on two-player zero-sum games. In this appendix, we discuss the Jacobian
analysis for two-player general games based on Cheung & Tao (2020), and explain why it is robust
against perturbations.
14
Published as a conference paper at ICLR 2022
First, we define the following function on the cumulative payoff space of two-player games,
which depends on a matrix M ∈ Rn×m .5
Lm(p, q) ：= 4 X	Xj(P)Xjo(p)yk(q)yk，(q) ∙ (Mjk + Mj，k，- Mjk，- Mj，k)2,
1≤j,j0≤n
1≤k,k0≤m
where Xj, Xj0 , yk, yk0 is as defined in (2).
For any general game (A, B), we can decompose it into the sum of a zero-sum game and a
coordination game as (A, B) = (Z, -Z) + (C, C), where Z = (A - B)/2 and C = (A + B)/2.
Cheung & Tao (2020) showed that the determinant of the Jacobian matrix of MWU with step-size
in the game (A, B) is
1 + (LZ(P, q) - LC (P, q)) 2 + O(4).
We rewrite LZ(P, q) - LC(P, q) as
4 X :	Xj (P)XjO (P)yk(q)yk0 (q) ∙ (Ajk + AjOkO - Ajk0 - AjOk ) (Bjk0 + BjOk - Bjk - BjOkO).
1≤j,j0 ≤n
1≤k,kO≤m
Thus, if
max [(Ajk + AjOk — Ajk — Aj%)(Bjk + BjOk — Bjk — BjOkO)] ≥ L > 0	(11)
j,jO,k,kO
and
min	[(Ajk	+ AjOkO	- AjkO	-	AjOk)(BjkO	+	BjOk	- Bjk -	BjOkO)]	≥ 0,	(12)
j,jO,k,kO
then in the domain S1 = {(P, q) | ∀j, k, Xj (P) ≥ δ and yk(q) ≥ δ}, the determinant is at least
1 + δ4Le2∕8 for all sufficiently small e.
Since the product (Ajk + AjO kO - AjkO - AjOk)(Bjk + BjOkO - BjkO - BjOk) depends on A, B
continuously, it is natural to expect that when A, B are slightly perturbed, the value of the product
changes also slightly. To make this precise, we suppose that in each round t, the actual payoff
matrices are (At, Bt), and their entries are perturbations from (A, B) by at most an absolute value
of β ≤ 2.6 Recall that the entries in A, B are bounded in the interval [-1, 1], so the absolute
values of Ajk + AjOkO - AjkO - AjOk and BjkO + BjO k - Bjk - BjOkO are bounded by 4, while the
perturbations to their values are bounded by 4β. By the lemma below, for the game (At, Bt), the
bound in (11) still hold by replacing its lower bound with L - 34β, and the bound in (12) still hold
by replacing jts lower bound with -34β. Thus the determinant of the Jacobian matrix in Si is at
least 1 + (δ4L∕4 - 34β)e2∕2 ≥ 1 + δ4Le2∕16 if β ≤ δ4L∕272.
Lemma C.6 If β ≤ 2, |a|, |b| ≤ 4 and |a0 - a|, |b0 - b| ≤ 4β, then a0b0 ≥ ab - 34β.
Proof: Let a0 - a = βa, b0 - b = βb. Then a0b0 - ab = bβa + aβb + βaβb ≥ -16β - 16β - β2 ≥
-34β.
D Plots of Uncertainty Evolution of MWU in One-Population
Game
5This function was denoted by C(M,-M) (p, q) in Cheung & Tao (2020), but to avoid confusion and cluster
of notation, we change it to LM (p, q).
6We assume that the entries in both (A, B) and (At, Bt) are in the interval [-1, 1], so β is at most 2.
15
Published as a conference paper at ICLR 2022
2.0 -
1.5 -
1.0 -
0.5 -
0.0 -
-0.5 -
-1.0 -
-1.5 -
-2.0 -
1.0	1.5	2.0
-2.0	-1.5	-1.0	-0.5	0.0	0.5
2.0 -
1.5 -
1.0 -
0.5 -
0.0 -
-	0.5 -
-	1.0 -
-	1.5 -
-	2.0 -
-2.0	-1.5	-1.0	-0.5	0.0	0.5	1.0	1.5	2.0
Figure 2: We plot the evolution of uncertainty for MWU in an one-population game specified by
0.1	1.0	-1.0
the matrix A = -1.0	0.1	1.0 , with step-size = 0.01. As we did in Figure 2, in the top
1.0	-1.0	0.1
left plot is the initial distribution, which is uniform in the small square. The top right plot shows the
“possibility” plot after 6000 steps, and the bottom left plot shows the corresponding heat-map plot
of probability densities.
16