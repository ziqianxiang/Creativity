Published as a conference paper at ICLR 2022
Recursive Disentanglement Network
Yixuan Chen； Yubin Shi； Dongsheng Lit
{yixuanchen20, ybshi21}@fudan.edu.cn, dongsli@microsoft.com
Yujiang WangJ Mingzhi Dong；；
yujiang.wang14@imperial.ac.uk, mingzhidong@gmail.com
Yingying Zhao,； Robert Dick§
yingyingzhao@fudan.edu.cn, dickrp@umich.edu
Qin LvFFan YangkLi Shang；
qin.lv@colorado.edu, {yangfan, lishang}@fudan.edu.cn
Ab stract
Disentangled feature representation is essential for data-efficient learning. The
feature space of deep models is inherently compositional. Existing β-VAE-based
methods, which only apply disentanglement regularization to the resulting embed-
ding space of deep models, cannot effectively regularize such compositional fea-
ture space, resulting in unsatisfactory disentangled results. In this paper, we for-
mulate the compositional disentanglement learning problem from an information-
theoretic perspective and propose a recursive disentanglement network (RecurD)
that propagates regulatory inductive bias recursively across the compositional
feature space during disentangled representation learning. Experimental studies
demonstrate that RecurD outperforms β-VAE and several of its state-of-the-art
variants on disentangled representation learning and enables more data-efficient
downstream machine learning tasks.
1	Introduction
Recent progress in machine learning demonstrates the ability to learn disentangled representations
is essential for data-efficient learning, such as controllable image generation, image manipulation,
and domain adaptation (Suter et al., 2019; Zhu et al., 2018; Peng et al., 2019; Gabbay & Hoshen,
2021; 2019). β-VAE (Higgins et al., 2017) and its variants are the most investigated approaches
for disentangled representation learning. Recent works on β-VAE-based methods introduce various
inductive biases as regularization terms and directly apply them on the resulting embedding space of
deep models, such as the bottleneck capacity constraint (Higgins et al., 2017; Burgess et al., 2018),
total correlation among variables (Kim & Mnih, 2018; Chen et al., 2018), and the mismatch between
aggregated posterior and prior (Kumar et al., 2017), aiming to balance among representation capac-
ity, independence constraints, and reconstruction accuracy. Indeed, as demonstrated by Locatello
et al. (2020; 2019), unsupervised disentanglement is fundamentally impossible without explicit in-
ductive biases on models and data sets.
However, our study shows that existing β-VAE-based methods may not be able to learn satisfac-
tory disentangled representations even for fairly trivial cases. This is due to the fact that the feature
spaces of deep models have inherently compositional structures, i.e., each complex feature is a com-
position of primitive features. However, existing methods with regularization terms solely applied to
the resulting embedding space cannot effectively propagate disentangled regularization across such
compositional feature space. As shown in Figure 1, applying the standard β-VAE to the widely
* China and Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University,
Shanghai, China, , Microsoft Research Asia, Shanghai, China, * Department of Computing, Imperial College
London, London, United Kingdom, § Department of Electrical Engineering and Computer Science, University
of Michigan, Michigan, United States, T Department of Computer Science, University of Colorado Boulder,
Boulder, United States, || School of Microelectronics, Fudan University, Shanghai, China, ** The corresponding
author.
1
Published as a conference paper at ICLR 2022
X
Encoder Decoder
父
(a)	(b)	(c)
Figure 1: Illustration of the negative impact on ignoring the compositional structure of the repre-
sentation space using dSprites dataset: (a) Illustration of z (from embedding space) and m (from
intermediate layers). (b) z is not disentangled sufficiently when m is not disentangled sufficiently.
(c) The disentanglement of z improves, as that of m improves.
used dataset dSprites (Matthey et al., 2017), we visualize the resulting representation z, as well as
its compositional low-level representations m extracted from the previous layer (as shown in Fig-
Ure 1(a)), and evaluate the independence between each pair of m and each pair of z, respectively ’.
Figure 1(b) and Figure 1(c) show that the disentanglement quality of low-level features m may im-
pact the resulting representation z in terms of disentanglement quality. This study demonstrates the
potential benefit to regularize the compositional feature space of deep models during disentangled
representation learning.
This work aims to tackle the compositional disentanglement learning problem. First, we formu-
late disentangled representation learning from an information-theoretic perspective, and introduce
a new learning objective covering three essential properties for learning disentangled representa-
tions: sufficiency, minimal sufficiency, and disentanglement. Theoretical analysis shows that the
proposed learning objective is a general form of β-VAE and several of its state-of-the-art variants.
Next, we extend the proposed learning objective to cover the disentanglement representation learn-
ing problem in the compositional feature space. Governed by the proposed learning objective, we
present Recursive Disentanglement Network (RecurD), a compositional disentanglement learning
method, which directs the disentanglement learning process across the compositional feature space
by applying regulatory inductive bias recursively through the feed-forward network. We argue that
the recursive propagation of inductive bias through the feed-forward network imposes a sufficient
condition of disentangled representation learning. Empirical studies demonstrate that RecurD out-
performs β-VAE (Higgins et al., 2017) and several other variants of VAE (Burgess et al., 2018; Kim
& Mnih, 2018; Chen et al., 2018; Kumar et al., 2017) on disentangled representation learning and
achieves more data-efficient learning in downstream machine learning tasks.
2	Compositional Disentanglement Learning
In this section, we first formulate disentanglement learning from the information-theoretic perspec-
tive by introducing three key properties and show such formulation is a general form of the opti-
mization objectives of β-VAE and several of its variants. Next, we extend the principled objective
to the compositional feature space to tackle the compositional disentanglement learning problem.
2.1	Disentanglement Learning Objective
The challenge of representation learning can be formulated as finding a distribution p (z|x) that maps
original data x ∈ X into a representation z with fixed amount of variables z = {z1 , . . . , zn} (Bengio
et al., 2013). The key intuition of z is to capture minimal sufficient information in a disentangled
manner, given the reconstruction task X ≈ ^. We denote the representation learning process as a
Markov Chain for which X → X → z, which means Z depends on X only through x, i.e., P (z|x)=
P (z∣x, ^) (see also, (Cover, 1999; Achille & Soatto, 2018)). The principled properties of Z are
defined as follows:
Definition 1. Sufficiency: a representation Z of X for X is sufficient if I (x, ^) = I (z, ^).
tThe independence between two components Ci and Cj is measured by the normalized mutual informa-
tion (Chen et al., 2018). Whenever NMI(ci; cj) = I(ci; cj)/H (c) = 0, ci and cj are independent (disen-
tangled).
2
Published as a conference paper at ICLR 2022
For the reconstruction task, Z is sufficient if it can successfully reconstruct X by x. The difference
between I (x; X) and I (z; X) is computed as follows: I (x; X) - I (z; X) = I (x; X|z) = H (X|z)-
H (^∣x). Given the reconstruction task X ≈ X, H (X|x) is constant and independent to z, so the
sufficient property can be optimized by minimizing H (X|z) (Federici et al., 2020; Dubois et al.,
2020).
Definition 2. Minimal Sufficiency: a representation Z of X is minimal sufficient if I (x; Z) = I (z; X).
A minimal sufficient z encodes the minimum amount of information about X required to reconstruct
X (Cover, 1999; Achille & Soatto, 2018). Since I (z; X) equals to I (x; X) when Z is sufficient,
the difference is computed as I (x; z) - I (z; X) = I (x; z) - I (x; ^). Given the reconstruction
task x ≈ X, I (x; ^) is constant and independent to z, so the minimal sufficiency property can be
optimized by minimizing I (X; z).
Definition 3. Disentanglement: a representation denoted as z = {z1 , . . . , zn} is disentangled if
Pj6=iI(zi;zj)=0.
From the definition of mutual information, I (zi; zj) = H (zi) - H (zi|zj) denotes the reduction
of uncertainty in zi when zj is observed (Cover, 1999). If any two components zi and zj are
disentangled, changes to zi have no influence on zj, which means I (zi; zj) = 0.
A representation satisfying all these properties can be found by introducing two Lagrange multipliers
λ1 and λ2 for two constrained expected properties with respect to the fundamental sufficiency prop-
erty. The principled objective of disentanglement learning is to minimize the following objective:
L = H (X|z) + λιI (x; z) + λ2 XI (zi, Zj).	(1)
j6=i
The above objective can be interpreted as the reconstruction error, plus two regularizers that yield
an optimally disentangled representation. The principled objective also helps us analyze and un-
derstand the success of recently developed β-VAE-based methods. These methods operate with
an encoder with parameter φ and a decoder with parameter θ, to induce the joint distributions
q (X, z) = qφ (z|X) q (X) and p (X, z) = pθ (X|z) p (z), respectively, where p (z) is a fixed prior
distribution. The learning objective of β-VAE contains the reconstruction error and KL divergence
between the variational posterior and prior. To understand the relationship of learning objectives
between Equation 1 and β-VAE-based methods, we decompose I (X; z) (Kim & Mnih, 2018) and
estimate an upper bound for Pj6=i I (zi, zj) (Te Sun, 1980; 1975), then we assign different weights
as follows:
λ1I (x; z) + λ2	I (zi,zj)
j6=i
n	n	(2)
≤ λaEx [K L	(q (z|x)	kp	(z))]	+	λbKL	q(z)	k Yq(zj)	+λcXKL(q(zj)	kp (zj))	.
As shown in Table 1, the learning objectives of β-VAE and its four variants can be regarded as
specific cases of Equation 1, i.e., they assign different weights to our regularization terms, which
can balance among latent variables capacity, independence constraints, and reconstruction accuracy,
leading to successful disentangled representation learning (Zhao et al., 2017; Li et al., 2020). More
details can be found in Appendix B. However, in these works, the inductive bias toward disentan-
glement is only applied to the embedding space of z, ignoring the need of disentanglement during
feature composition in feed-forward networks.
Table 1: Deriving the learning objectives of β-VAE and its four variants as specific cases of Equa-
tion 1.
MethOd	β-VAE	FaCtOrVAE	β -TCVAE	DIP-VAE	InfoVAE
Weight Relation	λa = β∙	λa = 1, λb = Y.	λa = 1, λb = β∙	XO=T λb = λc = λ.	λa = 1 - α, Xb + λc = α + λ — 1.
2.2	Compositional Objective
Considering an encoder with L layers to encode original data X into disentangled representation z.
Let us denote ml as the input features of the l-layer, which are divided into groups of features, i.e.,
ml = ∪j mlj , where mlj is the j-th feature subset.
3
Published as a conference paper at ICLR 2022
We formulate the compositional relation between features from two consecutive layers as follows:
mlj+1 = Layer ml × wjl . Layer can be applicable to commonly used neural network layers,
e.g., the convolution layer in computer vision tasks. The compositional relation is achieved by a
composition matrix wl ∈ Rdl×dl+1, so that features in ml are divided into dl+1 groups after passing
through all compositional vectors (wjl s). Note that mlj+1 is only related to the subset of features
from ml selected by wjl .
Similar to Section 2.1, we assume that the learning process of ml+1 depends on ml through original
data x, denoted as the Markov chain: ml → x → ml+1. It can be written as ml+1 → x → ml
according to the conditional independence implied by the Markovity (Cover, 1999). Consider two
output features mli+1, mlj+1 ∈ ml+1 and their corresponding input feature subsets mli, mlj ∈ ml,
we define key notions as follows:
Definition 4. Compositional Disentanglement: mli and mlj are disentangled if I mli; mlj = 0 .
A disentangled representation of mli and mlj may improve the disentanglement quality between
mli+1 and mlj+1. Similar to Definition 3, we can achieve compositional disentanglement by mini-
mizing I mli ; mlj .
Definition 5. Compositional Minimal Sufficiency: Assume that the learning process of mlj+1 is
denoted by the Markov chain: mlj+1 → x → mli, mlj . Given the original data x, an input feature
set mlj for the output feature mlj+1 is minimal sufficient if I x; mlj+1 = I mlj ; mlj+1 .
For the output feature mlj+1, the input feature set mlj is sufficient and another input feature set mli
is superfluous when mlj is able to capture all information of mlj+1 as well as the original data x.
Furthermore, according to Data-Processing Inequality (DPI) (Cover, 1999; Achille & Soatto, 2018)
in the Markov chain, there exists an inequality that:
I (x; mj+1) ≥ I (mj+1; mj) + I (mj+1； mi) - I (mi； mj),	(3)
where the difference between I x; mlj+1 and I mlj+1; mlj is equivalent to the difference be-
tween I mlj+1; mli and I mli; mlj . Therefore, matching I mlj+1; mli to I mli; mlj can yield
a minimal sufficient representation mlj for mlj+1 . Based on the definition of compositional disen-
tanglement, we can optimize the minimal sufficiency by forcing I mlj+1; mli to be 0. More details
can be found in Appendix B.
To learn disentangled representation via effectively regularizing the compositional feature space, we
augment the principled learning objective (Equation 1) with compositional regularizers. Therefore,
the compositional learning objective for disentangled representation is defined as follows:
L dl+1
L+1 dl+1
L = H (^∣m^
`----------
{z"^^^^^
sufficient
L+1),+λι (E EI (mi； mj+1) I +λ2 (E ILm1 (mi, mj)
(4)
=2 j 6=i
'--------------------7----------
minimal sufficient
=2 j 6=i
'----------------7------
disentangled
}
}
where mL+1 denotes the final disentangled representation z. Our intuition is that disentangled
learning for compositional feature space could benefit the disentanglement learning for high-level
representations.
3	Recursive Disentanglement Network
We now describe a learning method with the goal of optimizing the compositional disentanglement
learning objective. This method, called Recursive Disentanglement Network (RecurD), propagates
inductive bias (disentanglement) recursively across the compositional feature space.
3.1	Model architecture
As shown in Figure 2, RecurD contains an encoder and a decoder to learn the disentangled repre-
Sentation Z of data X and to reconstruct ^, where the encoder contains multiple Recursive Modules
and the decoder is a Deconvolutional Neural Network (Zeiler & Fergus, 2014).
4
Published as a conference paper at ICLR 2022
Figure 2: Recursive Disentanglement Network
The first Recursive Module of the encoder is implemented by a multi-channel convolutional net-
work (Lawrence et al., 1997) to encode the original image x. Following the notation in Section 2.2,
the output of the 1-st Recursive Module is denoted as m2 , which is also the input of 2-nd Re-
cursive Module. As for the l-th (l ≥ 2) Recursive Module, it contains (1) a Router R to learn a
composition matrix wl from the input features ml to decompose ml into subsets; and (2) a Group-
of-Encoders (GoE) layer consisting of n encoders to induce the output feature ml+1.
Inspired by the Gate of Mixture-of-Experts (Shazeer et al., 2017; Fedus et al., 2021) on parameter
selection for each input, we present a Router with T opK to learn the compositional relation. In
detail, the Router takes ml as input and compute composition matrix wl via learning similarity:
Wl = SoftmaX (ToPK (R (ml) , k)) , R (ml) = SoftmaX ([ml] [ml]T) ◦ v, and
TopK (R (ml) k) = (R (ml %，ifR (ml )j is in the toP k elements of R (ml ).j	⑸
,	-∞,	otherwiSe.
Here, R (m) denotes the similarity matriX and v is a learning matriX learned by a linear layer. ◦
denotes Hadamard product. TopK is the compositional strategy to determine whether input feature
mlij belongs to the input feature set of mlj+1, and k is a hyperparameter. The GoE layer consists
of dl+1 parallel encoders Enc1, . . . , Encdl+1 to generate the output features ml+1, where each
encoder is implemented by a convolutional neural network with specific parameters:
mj+1 = Encj (ml X Wj) .	(6)
Then, ml+1 is obtained by concatenating the outputs from each encoder, which is converted directly
to the input of the l + 1-th Recursive Module. Note that the output of L-th Recursive Module is
the learned disentangled representation z. Finally, the Decoder Dec takes z as input to obtain the
reconstructed input ^: X = Dec (z).
3.2	Learning of RecurD
As mentioned in Equation 4, the learning objective of RecurD governs the recursive disentanglement
learning across the compositional feature space. As shown by related works, a precise estimation
of mutual information in the high dimensional space is important for accurately estimating the loss.
According to the recent progresses, the mutual information between two representations can be
maXimized by using any sample-based differentiable mutual information lower bound. Similar to
the work of Federici et al. (2020); Hjelm et al. (2019); Wen et al. (2020), we utilize MINE es-
timators (Belghazi et al., 2018) to estimate the mutual information. This approach introduces an
auXiliary parametric model Estimator Net which is jointly optimized during the training procedure
using re-parametrized samples from the posterior distribution.
4	Related Work
β-VAE-based methods introduce various regularization terms and directly apply them on the result-
ing embedding space. Higgins et al. (2017) proposed β-VAE method by introducing a constraint
β over the KL divergence between the inferred distribution qφ (z|x) and its prior-an isotropic unit
Gaussian. Burgess et al. (2018) found that the network specializes in the factor that contributes most
to a small reconstruction error, with a limited channel capacity. Thus, they proposed the Annealed-
VAE, an eXtension of β-VAE, which gradually adds more latent encoding capacity by enforcing the
KL divergence to be at a controllable value C. Kim & Mnih (2018) analyzed the disentanglement
5
Published as a conference paper at ICLR 2022
performance of β-VAE by breaking down the regularization term into two components. One is Total
Correlation (TC) (Watanabe, 1960), which encourages the marginal distribution of representations
to be factorial. The other is mutual information I(x; z), which reduces the amount of information
about x stored in z. Based on the decomposition, they proposed FactorVAE to relax the regular-
ization of I(x; z) while directly penalizing the TC term. Similarly, Chen et al. (2018) proposed
β-TCVAE by decomposing the TC penalty as the dependence among variables and the distance be-
tween each variable’s posterior and prior. Since TC is intractable, both FactorVAE and β-TCVAE
use approximation methods. Another TC-related approach is DIP-VAE (Kumar et al., 2017), whose
designers argued that the estimation of TC requires additional parameters and suffers from vanishing
gradients. Therefore, DIP-VAE optimizes the moments distance between the aggregated posterior
and a factorized prior instead of estimating TC. The above methods improved over β-VAE by ap-
plying inductive biases directly to the high-level latent variable space. However, the feature space
of deep models is compositional in nature. Existing variants of β-VAE cannot effectively apply
disentanglement regularization across such compositional feature space, and yield inferior disentan-
glement in representation learning. Our approach diverges from prior works by taking a principled
information-theoretic approach to formulate and analyze the compositional disentanglement feature
structure.
Recent works on hierarchical VAEs introduce layer-wise disentanglement regularization to learn
conditioning structures across multi-layer latent variables, such as VampPrior (Tomczak & Welling,
2018), Ladder VAEs (S0nderby et al., 2016), and NVAE (Vahdat & Kautz, 2020). In these hierar-
chical model structures, e.g., the cross-layer residual connection structure in NVAE and VampPrior,
inter-layer regularization is less of a focus. The latent variables of a preceding layer serve as shared
inputs to the next layer, which introduces information redundancy and hence impairs representa-
tion disentanglement. In contrast, the proposed compositional objective optimizes the statistical
independence of inter-layer and intra-layer latent variables simultaneously, thereby minimizing the
information redundancy of inter-layer information sharing and improving disentanglement quality.
5	Experiments
This section presents both quantitative and qualitative experiments to evaluate RecurD in terms of
disentanglement quality and data efficiency on downstream tasks.
5.1	Performance of Disentanglement Learning
We compare the disentanglement learning performance of RecurD with β-VAE and its state-of-the-
art variants, including β-VAE (Higgins et al., 2017), Annealed-VAE (Burgess et al., 2018), Factor-
VAE (Kim & Mnih, 2018), β-TCVAE (Chen et al., 2018), DIP-VAE (Kumar et al., 2017). More
experimental results can be found in Appendix E.
Datasets: We consider two datasets in which each image is obtained by a deterministic function
of ground-truth factors: dSprites (Matthey et al., 2017) and 3DShapes (Burgess & Kim, 2018).
dSprites contains 737, 280 binary 64 × 64 images of 2D shapes with 5 ground truth factors, i.e., 3
shapes, 6 scales, 40 orientations, 32 x-positions, 32 y-position. 3DShapes contains 480, 000 RGB
64 × 64 × 3 images of 3D shapes with 6 ground truth factors, i.e., 4 shapes, 8 scales, 15 orientations,
10 floor hues, 10 wall hues, 10 object hues. For all experiments, we use a 9:1 training to testing data
ratio, following earlier work (Kumar et al., 2017; Locatello et al., 2019). More details on network
architecture and hyperparameter settings are included in Appendix D.
Evaluation Metrics for Disentanglement: There is no standard metric for evaluating disentangle-
ment (Zhou et al., 2021; Ridgeway & Mozer, 2018), and most existing metrics involve the estimation
of a variable-factor matrix relating the factors of variation to the learned representations. In the ex-
periments, we consider three widely used metrics: Separated Attribute Predictability (SAP) (Kumar
et al., 2017), Mutual Information Gap (MIG) (Chen et al., 2018) and Disentanglement, Complete-
ness, and Informativeness (DCI) (Eastwood & Williams, 2018). DCI contains three metrics for
disentanglement (DCI-D), Completeness (DCI-C) and Informativeness (DCI-I), respectively. Due
to the space limitation, we present the results of DCI-D and include the results on DCI-C and DCI-
I in the Appendix E. Overall, these metrics can comprehensively evaluate RecurD from different
disentanglement measurements.
6
Published as a conference paper at ICLR 2022
5.1.1	Quantitative Results
For quantitative analysis, we conduct three sets of experiments: 1) evaluating the average perfor-
mance of disentanglement score via three evaluation metrics; 2) measuring the trade-off among
reconstruction, minimal sufficiency and disentanglement with mini-batch samples; and 3) analyzing
the influence of compositional objective for disentanglement learning via the three properties.
Table 2 compares the reconstruction error and three widely-used disentanglement metrics of RecurD
and five other methods on the dSprites and 3DShapes datasets. Compared with all the baselines,
RecurD achieves much lower reconstruction error as well as higher SAP, MIG, and DCI scores in
most of the cases. It is important to point out that the reconstruction error of β-VAE increases
as β increases (stronger disentanglement regularization), which indicates that β-VAE comprises
reconstruction error for disentanglement. However, RecurD achieves better in both reconstruction
and disentanglement, demonstrating the superiority of the proposed compositional learning objective
and the recursive disentanglement network.
Figure 3 shows scatter plots relating Minimal Sufficiency and Disentanglement to reconstruction er-
ror (Sufficiency), where each point represents a mini-batch of data. Here, we compute the minimal
sufficiency score using I (x; z) and present it by the area of each point. In all baseline methods, we
observe that smaller scatters are correlated with higher reconstruction errors and higher disentangle-
ment scores. The reason is that β-VAE and its variants only place disentanglement regularization on
the embedding space of z which imposes a limit on the capacity of the information channel (Higgins
et al., 2017), so that features which are important for reconstruction but harmful for disentanglement
may lose during training, leading to compromised reconstruction quality. However, RecurD reveals
the necessity of disentanglement during feature composition in feed-forward networks and induces
effective information compression. Therefore, compared to all the baselines, the representations
from RecurD obtain better disentanglement and minimal sufficiency without degrading reconstruc-
tion quality.
Figure 4 shows the impact of the number of Recursive Modules in disentanglement learning. We
evaluate three RecurD variants on the principled properties, including RecurD 0, RecurD 1 and
RecurD 2 with 1, 2 and 3 Recursive Modules, respectively. And we report the values correspond-
Table 2: Three widely used evaluation scores and reconstruction error on the test sets for dSprites and
3Dshapes. Boldface indicates the best results, i.e., reconstruction error or disentanglement scores.
dataset	dSprites				3DShapes			
	Reconst. Error	SAP	MIG	DCI-D	Reconst error	SAP	MIG	DCI-D
β-VAE(β = 4)	0.0066	0.0284	0.2617	0.1191	0.0216	0.1463	0.2519	0.4682
β-VAE(β = 16)	0.0094	0.0242	0.2241	0.1820	0.0544	0.1488	0.2629	0.4528
β-VAE(β = 60)	0.0127	0.0445	0.1432	0.1291	0.0624	0.1041	0.2402	0.4317
Annealed-VAE	0.0171	0.0311	0.1177	0.1449	0.0811	0.0730	0.2217	0.4279
Factor-VAE	0.0228	0.0436	0.2594	0.1955	0.0800	0.1331	0.2630	0.4491
β-TCVAE	0.0162	0.0352	0.1585	0.1774	0.0312	0.0364	0.2070	0.4487
DIP-VAE	0.0213	0.0261	0.0731	0.1038	0.0213	0.2013	0.3108	0.4853
RecurD	0.0047	0.0502	0.2707	0.3841	0.0083	0.1979	0.3105	0.5804
dSprites	dSprites	dSprites
1.0 C 1.5 L 2.0	2.5 le-2 0.5	1.0' C lʒ「	2.0	2.5 le-2 0.5	1.0 C 1.5 L 2.0
Reconst. Error	Reconst. Eιτor	Reconst. Error
3DShapes	3DShapes	3DShapes
Figure 3: Reconstruction error vs. disentanglement performance. Scatters located at the left top
indicate better performance. The area of each scatter represents the minimal sufficiency score esti-
mated by I(x; z), and smaller area indicates better performance.
7
Published as a conference paper at ICLR 2022
ing to the three terms in our compositional learning objective on the training sets. We observe that
RecurD 1 and RecurD 2 perform much better than RecurD 0 on the optimization of minimal
sufficiency and disentanglement. In addition, during the early stage of training, the performance of
RecurD 2 improves much faster than RecurD 1. This experiment demonstrates that the recursive
propagation of inductive bias through the feed-forWard netWork improves disentangled representa-
tion learning.
5.1.2	Qualitative Results
For qualitative analysis, we present
the latent traversals of RecurD on two
datasets in Figure 5, in which we vary
a single variable learned by an en-
coder in GoE while keeping all oth-
ers fixed. For images in 3DShapes,
the latent traversals show that Re-
curD is able to successfully capture
all the six factors of variation. For
images in dSprites, we observe that
RecurD is able to discover x-position,
y-position and scale (continuous vari-
ables). More importantly, RecurD
can, to some extent, discover shape
and orientation (discrete variables),
which have been proved to be strug-
gling for many other methods (Ku-
mar et al., 2017; Kim & Mnih, 2018;
Figure 4: The performance of RecurD With varying number
of recursive modules on the principled properties.
(WgUI ə-胃ɑəsɪɑ) (N(N)I 一
Locatello et al., 2019). The reason is that β-VAE and its variants encourage independence among
variables via controlling the information capacity of z and matching the posterior p(z|x) to an
isotropic unit Gaussian. HoWever, learning discrete variables Would require using a discrete prior
instead of Gaussian (Kim & Mnih, 2018). On the other hand, RecurD is able to model both dis-
crete and continuous factors by directly disentangling based on the three principled properties on
the entire compositional feature space, leading to stronger representation capability.
5.1.3	Ablation and Parameter Dependence Study
In this section, we evaluate the impacts of hyperparameters in both learning objective and model
architecture. First, we study the impact of compositional learning objective with varying regular-
ization coefficients (λ1 and λ2) of minimal sufficiency and disentanglement. Then, we evaluate the
influence of hyperparameter k — the group size in the GoE of Recursive Module. Figure 6 (Re-
curD with varying λ1, λ2 and k) shows the scatter plots of disentanglement score (SAP) along with
reconstruction error on the test sets of dSprites and 3DShapes.
As shown in Figure 6, larger penalties on both minimal sufficiency and disentanglement yield higher
disentanglement score and lower reconstruction error, demonstrating the importance of the minimal
sufficiency term and the disentanglement term in Equation 1. Note that RecurD with λ2 = 0 is
Figure 5: First roW: original images. Second Figure 6: Ablation study on λ1, λ2 and group
roW: reconstructions. Remaining roWs: recon- size k . Note that RecurD With λ2 = 0 reduces to
structions of latent traversals.	β-VAE With the compositional architecture.
8
:WE】
im!3 口
Published as a conference paper at ICLR 2022
reduced to a standard β-VAE with the compositional architecture (as I (x; z) is an lower bound of
KL(p(z|x)kp(z))), which underperforms RecurD, indicating that an explicit penalty on disentan-
glement is important for disentangled representation learning. As for the group size in the GoE —
k, it is not surprising that when k increases, RecurD has lower reconstruction errors, with a slightly
inferior disentanglement performance. The reason is that a dense composition of feature space can
help the model maintain sufficient information but make it difficult to disentangle latent variables.
RecurD with k = 1 results in poor performance on both reconstruction and disentanglement, in-
dicating that over-emphasizing decomposition in the feature space may fail to preserve sufficient
information. More results are presented in the Appendix E.
5.2	Performance of Downstream Tasks
In this section, We compare the performance
of ReCurD and five baseline methods by mea-
suring their data efficiency on two downstream
tasks: a standard classification task on the
MNIST dataset and a domain generalization
task on the MNIST-Rotation dataset (Ghifary
et al., 2015). MNIST-Rotation is a synthetic
dataset consisting of 6 domains, each contain-
ing 1,000 images of the 10 digits randomly se-
lected from the training set of MNIST, with 6
rotation degrees: 0°, 15°, 30°, 45°, 60° and
75°. For both tasks, we set the training sets of
different proportions: 20%, 40%, 60%, 80%,
and 100%, to evaluate the classification perfor-
Data Ratio of Training set	Data Ratio of Training set
Figure 7: Performance comparison of RecurD
and baselines on the standard classification task
(left) and the domain generalization task (right).
Each model is trained by varying ratios of training
data. The dotted line represents the performance
of a single-layer neural network.
mance of RecurD and the baselines with differ-
ent amounts of training data. For the domain
generalization problem, we follow the previous
works (Li et al., 2017; Balaji et al., 2018; Du et al., 2020) with the same train-test split strategy and
the leave-one-domain-out strategy, i.e., we take the samples from one domain as the target domain
for testing, and the samples from the remaining domains as the source domain for training.
Figure 7 reports the average accuracy of different methods on the same test set. We can observe that
all methods achieve decent data efficiency on both tasks, i.e., without much performance degradation
even with 20% of training data, suggesting that the learning process of disentangled representation
can more effectively capture information from the inputs. However, β-VAE and its variants do not
achieve satisfactory classification accuracy for either task compared to a single-layer neural network.
The reason is that β-VAE and its variants obtain the disentangled representation by limiting the ca-
pacity of information channels, thus they tend to only maintain features that contribute more to dis-
entanglement, which may lose informative features that contribute more to classification. Compared
to the baselines, RecurD achieves consistently better performance in both tasks, especially on the
harder domain generalization task. The reason is that RecurD can learn disentangled representations
without sacrificing the reconstruction performance, confirming the hypothesis that compositional
disentanglement learning yields better generalization and more data-efficient representations. We
believe that the informative disentangled representations emerge when the right balance is achieved
between sufficient information preservation and minimal sufficient information learned in a disen-
tangled manner.
6	Conclusion
This paper has described a solution to the compositional disentangled representation learning prob-
lem. We first presented a general information-theoretic formulation of disentanglement representa-
tion learning, and then extended it to the compositional feature space. We then described RecurD,
a recursive disentanglement network, which propagates regulatory inductive bias recursively across
the compositional feature space. RecurD outperforms β-VAE and its state-of-the-art variants on
disentangled representation learning and achieves more data-efficient learning in downstream tasks.
9
Published as a conference paper at ICLR 2022
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain gen-
eralization using meta-regularization. Advances in Neural Information Processing Systems, 31:
998-1008, 2018.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning, pp. 531-540. PMLR, 2018.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in beta-vae. arXiv preprint
arXiv:1804.03599, 2018.
Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Proceedings ofthe 30th International Conference on Neural Information Processing Systems, pp.
2180-2188, 2016.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees GM Snoek, and Ling Shao.
Learning to learn with variational information bottleneck for domain generalization. In European
Conference on Computer Vision, pp. 200-216. Springer, 2020.
Yann Dubois, Douwe Kiela, David J Schwab, and Ramakrishna Vedantam. Learning optimal repre-
sentations with the decodable information bottleneck. arXiv preprint arXiv:2009.12789, 2020.
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disen-
tangled representations. In International Conference on Learning Representations, 2018.
Marco Federici, Anjan Dutta, Patrick Forre, Nate Kushman, and Zeynep Akata. Learning robust
representations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017, 2020.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Sanja Fidler, Sven Dickinson, and Raquel Urtasun. 3d object detection and viewpoint estimation
with a deformable 3d cuboid model. Advances in neural information processing systems, 25:
611-619, 2012.
Aviv Gabbay and Yedid Hoshen. Demystifying inter-class disentanglement. arXiv preprint
arXiv:1906.11796, 2019.
Aviv Gabbay and Yedid Hoshen. Scaling-up disentanglement for image translation. arXiv preprint
arXiv:2103.14017, 2021.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generaliza-
tion for object recognition with multi-task autoencoders. In Proceedings of the IEEE international
conference on computer vision, pp. 2551-2559, 2015.
10
Published as a conference paper at ICLR 2022
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
ICLR 2017, 2017.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. ICLR, 2019.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning,pp. 2649-2658. PMLR, 2018.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentan-
gled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.
Steve Lawrence, C Lee Giles, Ah Chung Tsoi, and Andrew D Back. Face recognition: A convolu-
tional neural-network approach. IEEE transactions on neural networks, 8(1):98-113, 1997.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision, pp.
5542-5550, 2017.
Yanjun Li, Shujian Yu, Jose C Principe, Xiaolin Li, and Dapeng Wu. Pri-vae: principle-of-relevant-
information variational autoencoders. arXiv preprint arXiv:2007.06503, 2020.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In international conference on machine learning, pp. 4114-4124.
PMLR, 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. A sober look at the unsupervised learning of disentangled repre-
sentations and their evaluation. arXiv preprint arXiv:2010.14766, 2020.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
William McGill. Multivariate information transmission. Transactions of the IRE Professional Group
on Information Theory, 4(4):93-111, 1954.
Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with dis-
entangled representations. In International Conference on Machine Learning, pp. 5102-5112.
PMLR, 2019.
Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances
in neural information processing systems, 28:1252-1260, 2015.
Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic
loss. arXiv preprint arXiv:1802.05312, 2018.
Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang,
and Tarek Abdelzaher. Controlvae: Controllable variational autoencoder. In International Con-
ference on Machine Learning, pp. 8655-8664. PMLR, 2020.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. Advances in neural information processing systems, 29:3738-3746,
2016.
Raphael Suter, Djordje Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness. In Interna-
tional Conference on Machine Learning, pp. 6056-6065. PMLR, 2019.
11
Published as a conference paper at ICLR 2022
Han Te Sun. Linear dependence structure of the entropy space. InfControl, 29(4):337-68, 1975.
Han Te Sun. Multiple mutual informations and multiple interactions in frequency data. Information
and Control, 46:26-45, 1980.
Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial
Intelligence and Statistics, pp. 1214-1223. PMLR, 2018.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020.
Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of Re-
search and Development, 4(1):66-82, 1960.
Liangjian Wen, Yiji Zhou, Lirong He, Mingyuan Zhou, and Zenglin Xu. Mutual information gradi-
ent estimation for representation learning. arXiv preprint arXiv:2005.01123, 2020.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational
autoencoders. arXiv preprint arXiv:1706.02262, 2017.
Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar E. Carlsson, and Stefano Ermon.
Evaluating the disentanglement of deep generative models through manifold topology. In Inter-
national Conference on Learning Representations, 2021.
Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Joshua B Tenen-
baum, and William T Freeman. Visual object networks: Image generation with disentangled 3d
representation. arXiv preprint arXiv:1812.02725, 2018.
12
Published as a conference paper at ICLR 2022
A Property of Markov Chain and Mutual Information
This section lists the related properties of Markov chain and mutual information used in this work.
Markov Chain
Consider three random variables a, b and c coming from a joint distribution p(a, b, c), if the con-
ditional distribution of c depends only on a and is conditionally independent of b, the variables can
form a Markov chain in the order denoted as: b → a → c. Specially, the joint distribution can be
written as (Cover, 1999):
p(a, b, c) = p(a)p(b|a)p(c|a).	(7)
Markovity implies a conditional independence between c and b when a is observed, the reason is:
p(a, b, c)	p(c, a)p(b|a)
p(c, b|a) = —=----------rʒ——=P(c∣a)p(b∣a).	⑻
By rewriting the conditional independence, the Markov chain of b → a → c also implies c → a →
b as:
p(a, b, c)	p(b, a)p(c|a)
p(b, c|a) = —=----------L——=P(c∣a)p(b∣a).	⑼
Mutual Information
1.	Positivity: I(a; b) ≥ 0; I(a; b|c) ≥ 0.
2.	Chain Rule: I(a; b, c) = I(a; b) + I(a; c|b)
3.	I(a; b) = H(a) - H(a|b) = 0 if and only if a and b are independent.
I(a; b) = H(a) - H(a | b)
=E [log P(⅛ 卜 E [log pdw_
=E [log p(⅛p(b)]
- g p(a) p(b)J	(10)
p(a, b)
=EKg"
= D (p(a, b)kp(a)) × p(b))
≥0
4.	Data-Processing Inequality (DPI): If three random variables a, b and c coming from a joint
distribution p(a, b, c) can form a Markov chain in the order denoted as: b → a → c, then I(b; a) ≥
I(b; c). (Theorem 2.8.1 in Cover (1999))
B Learning Objectives Decomposition
In this section, we decompose the proposed learning objective to analyze the relationship of learning
objectives between Equation 1 and β-VAE-based methods. Let p(x) denote the true distribution
of the data, and pφ (z|x) and pθ(x|z) denote the unknown distributions that We need to estimate,
parametrized by an encoder with φ and a decoder with θ.
B.1	Decomposition of Minimal Sufficiency
Minimal Sufficiency is defined as z can encode the minimum amount information of x required to
reconstruct x, optimized by minimize I(x; z). Inspired from the WorkofChenetaL (2018), we can
13
Published as a conference paper at ICLR 2022
decompose the minimal sufficiency by assuming the prior p(z) as a factorized Gaussian as:
I (x; z) = Eq(x,z) [KL (q (x, z) kq (z) p (x))]
q(x,z)	q (z|x) p (x)
=Eq(x,z) [log q (Z) P (x) J = Eq(x,z) [log q (Z) P (x) J
= Ep(x) Eq(z|x) log q (z|x) - log q (z) + log p (z) - log p (z) + log q(zj) - log q(zj)
jj
=Eq(x,z) [log 喀 1 — Eq(ZJX log 答 1 — Eq(z) "log Qq^#
,	q (Z)	j	P (Zj)	jq (Zj)
=Ep(X) [KL(q (ZlX) kp (Z))] — X KL (q (Zj) kp (Zj))- KL (q (Z) k Y q (Zj j
j	j	(11)
The first term is KL divergence between inferred posterior and prior, usually as a penalty term of
β-VAEs for disentangling. The second term is the dimension-wise KL divergence, which represents
the distance between each latent dimension to the prior. The third term is the Total Correlation
referring to the independence among variables.
B.2	Estimation of Disentanglement
Disentanglement is defined as the independence between any two variables, optimized by minimiz-
ing Pi6=j I(Zi; Zj), which is an lower bound of Total Correlation.
The concept of Total Correlation (TC) was described by McGill (1954) and formally formulated by
Watanabe (1960) to evaluate the mutual independence of multi-variant variables. Assume a set of Z
of random variables Z1, . . . , Zn. The TC in Z is expressed as:
n
C(Z1,...,Zn) = H(Zi) —H(Z1,Z2,...,Zn),	(12)
i=1
where H (Z1 , Z2 , . . . , Zn) denotes the joint entropy. Furthermore, according to the work of Te Sun
(1980; 1975), the relation between TC and mutual information can be described as:
n
C(Z)= X H (Zi)- H (Z) = — X ∆H (Z)	(13)
i=1	n≥2
The general ∆H (Z) is defined as Fano’s multiple mutual information among variables. In the case
of n = 2, it reduces to Shannon’s mutual information and in the case of n = 3, it coincides with
McGill’s mutual information. The formulation of TC can be simplified as:
C (zi, ..., Zn) = EI IZi Zj) + E ∆H IZi Zj; Zk) + ... + ∆H (zi；…；Zn)	(14)
j6=i	k6=j 6=i
In VAEs, to measure dependence for multiple latent variables, TC is computed as
DKL q (Z) k Qj q (Zj) . Therefore, the proposed disentanglement Pi6=j I (Zi； Zj) is a lower
bound of TC.
14
Published as a conference paper at ICLR 2022
B.3 Relation with Other Works
After decomposing the minimal sufficiency and estimating the disentanglement by TC, we can com-
bine two regularizers as:
λιI (x; z) + λ EI (zi； Zj)
j6=i
≤ λιEp(χ) [KL(q(ZIX) kp(Z))] - λ12 XKL(q (Zj) kp (Zj)) - λ13 KL (q(Z) k Y q (Zj)
+λ2XKL(q(zj)kp(zj))
j
= λ1Ep(x) [KL (q (Z|x) kp (Z))] + (λ2 - λ12)XKL(q(Zj) kp (Zj)) - λ13 KL (q (Z) k Yq(Zj)
jj
=λaEp(χ) [KL (q (Z|x) kp (Z))] + λb X KL (q (Zj) kp (Zj)) + λc KL (q (z) k Y q (Zj))
j	j	(15)
Therefore, by assigning different weights to the decomposed term, we can establish the relationship
between the proposed information-theoretic objective and existing β-VAE variants, which will pro-
vide insights regarding the capabilities and limitations of existing methods, and further motivate the
proposed work.
Specially, the objective function of β-TCVAE is I (x, Z) + KL q (Z) k Qj q (Zj)	+
Pj KL (q (Zj) kp (Zj)). If we decompose the I (x； Z) of β-TCVAE as: KL (q (Z|x) kp (Z)) -
KL q (Z) k Qj q (Zj) - Pj KL (q (Zj) kp (Zj)), the regularizer term of β-TCVAE can be writ-
ten as: αKL (q (Z|x) kp(Z))+(β -α)KL q (Z) k Qjq(Zj)+(γ -α)PjKL(q(Zj) kp (Zj)).
Since α = γ = 1 in β-TCVAE, therefore when λc = 0, the last line of Equation 15 is equivalent to
β-TCVAE.
C Proof of Compositional Minimal Sufficiency
In this section, we prove the statements reported in the section 2.2 of the paper.
Property:
(P1 ): Data-processing inequality in the markov chain: considering the markov chain b → a → c,
then I (b, a) ≥ I (b； c). (Theorem 2.8.1 (Cover, 1999).)
(P2): Chain rule for mutual information: I (a； b, c) = I (a； b) +I (a； c|b). (Theorem 2.5.1 (Cover,
1999).)
(P3): Decompositon of conditional mutual information. (Proposition B.1. (Federici et al., 2020).)
Conditional independence assumptions:
The Markov Chain mlj+1 → x → mli , mlj , implies the conditional independence between mlj+1
and mli , mlj when x is observed.
Proof:
I(X； mj+1) (≥) I (mj+1; mi, mj)
(P2) I (mj+1; mj) + I (mj+1; mi ∣ mj)
(=) I (mj+1; mj) + I (mj+1; m；) - I (m；； mj).
(16)
15
Published as a conference paper at ICLR 2022
D Experimental Details
For all baselines, we use a Convolutional Neural Network for the encoder, and a Deconvolutional
Neural Network for the decoder. Specially, for Factor-VAE, we use a 6-layer Multi-Layer Perceptron
for the discriminator, with the leaky ReLU as the activation on per layer, and we set γ as 6.4. For
β-VAE, we use a set of β = {4, 16, 60}. Annealed-VAE uses γ = 1000 with a linearly increasing
C from 0.5 nats to a 25.0 nats). β-TCVAE uses α = γ = 1 and β = 4. DIP-VAE is implemented
by the parameters λod = 100 and λd = 10.
For RecurD, we implement RecurD 0 as the same encoder/decoder architecture with all baselines,
as shown in Table 3. As for RecurD 1 and RecurD 2, we implement the same decoder architec-
ture with all baselines, and the details about the encoder of RecurD 1 and RecurD 2 are shown
in Table 4 and Table 5. Following the previous work, we use negative cross-entropy to compute
reconstruction error, which represents sufficiency in our work. As for the computation of minimal
sufficiency and disentanglement, we implement the MINE estimator by 4-layer Multi-Layer Percep-
tron, similar with the work of Federici et al. (2020). As for the hyperparameter in our model, we vary
λ1 and λ2 in the set {0.1, 0.2, 0.5, 1, 2, 5, 10, 50} while fixing λ1 = 1 and λ1 = 2 for both datasets.
During training, we use Adam optimiser with learning rate 1e - 4, β1 = 0.9, β2 = 0.999 for pa-
rameter updates. Specially, we utilize RecurD 1 on dSprites, 3DCars and 3DShapes, and RecurD 2
on CelebA.
Table 3: Encoder and Decoder architecture for all baselines and RecurD 0.
Input	dSprites (3DShaPes): 64 X 64 × 1(3) Images
Encoder	Conv: k=4, s=2, p=1, Channel=32, ReLU
	Conv: k=4, s=2, p=1, Channel=32, ReLU
	Conv: k=4, s=2, p=1, Channel=64, ReLU
	Conv: k=4, s=2, p=1, Channel=64, ReLU
	FC: 128 (256)
	FC: 2 × 8
Decoder	Input: Z ∈ R8
	FC:128, ReLU
	FC: 4 × 4 × 64, ReLU
	Deconv: k=4, s=2,p=1, Channel=64, ReLU
	Deconv: k=4, s=2, p=1, channel=32, ReLU
	Deconv: k=4, s=2, p=1, channel=32, ReLU
	Deconv: k=4, s=2,p=1, channel=1(3), ReLU
Output:	64 × 64 × 1 (3) Reconst. Images
Table 4: Encoder architecture of RecurD 1.
Input		64 × 64 × 1 (3) Image		
0-th Recursive Module	Router(v)	-		
	GoE	4× [Conv: k=4, s=2,p=1, channel=32, ReLU]		
		Conv: k=4, s=2,p=1, channel=64, ReLU		
1-th Recursive Module	Router(v)	FC, 8, ReLU		
	GoE	8 ×	Conv : k=4, s=2, p=1, channel= 16, ReLU FC, 16; FC, 2 × 1	
16
Published as a conference paper at ICLR 2022
Table 5: Encoder architecture of RecurD 2.
Input		64 X 64 × 1 (3) Image		
0-th Recursive Module	Router(V)	-		
	GoE	2 × [Conv: k=4, s=2, p=1, Channel=32, ReLU ]		
1-th Recursive Module	Router(V)	FC, 4, ReLU		
	GoE	4 × [Conv: k=4, s=2, p=1, Channel=16, ReLU]		
2-th Recursive Module	Router(V)	FC, 16, ReLU		
	GoE	16 ×	Conv : k=4, s=2, p=1, channel=16, ReLU FC, 16; FC, 2 × 1	
E Additional Experiments
This section reports additional experiments, which draw similar conclusions as that of Section 5.
E.1 Quantitative Results
The first set of quantitative results is the supplementary results of disentanglement score DCI-C and
DCI-I on dSprites and 3DShapes. Table 6, Figure 8 and Figure 9 show the results of disentanglement
score with reconstruction error, which also show RecurD can achieve a better trade-off between
reconstruction and disentanglement.
Figure 8: Reconstruction error vs. disentanglement performance on dSprites. Scatters located at the
left top indicate better performance.
^-VAE(16)
β-VAE(60)
jβ-VAE(16)
∕3-VAE(6O)
Figure 9: Reconstruction error vs. disentanglement performance on 3DShapes. Scatters located at
the left top indicate better performance.
The second set is the performance comparison results with three additional baselines, i.e, Info-
GAN (Chen et al., 2016), Control-VAE and NVAE. InfoGAN (Chen et al., 2016) maximizes the
mutual information between the small subset of the latent variables and the observations to increase
the interpretability of the latent representation. Control-VAE (Shao et al., 2020) dynamically tunes
17
Published as a conference paper at ICLR 2022
Table 6: DCI-C, DCI-I scores and reconstruction error on the test sets for dSprites and 3DShapes.
Bold face indicates the best results, i.e., reconstruction error and disentanglement scores.
dataset	dSprites			3DShapes		
	ReConst error	DCI-C	DCI-I	ReConst error	DCI-C	DCI-I
β-VAE(β = 4)	0.0066	0.1148	0.2181	0.0216	0.4463	0.1982
β-VAE(β = 16)	0.0094	0.1575	0.2087	0.0544	0.4691	0.1828
β-VAE(β = 60)	0.0127	0.1261	0.1769	0.0624	0.4215	0.1641
Annealed-VAE	0.0171	0.1793	0.1663	0.0811	0.4419	0.1682
Factor-VAE	0.0228	0.1375	0.1699	0.0800	0.4419	0.1707
β-TCVAE	0.0162	0.1988	0.1660	0.0312	0.4516	0.1774
DIP-VAE	0.0213	0.0968	0.1496	0.0213	0.4621	0.1961
RecurD	0.0047	0.3835	0.2222	0.0083	0.5668	0.2014
the weight β on the KL term to achieve a good trade-off between disentanglement and reconstruc-
tion quality. NVAE (Vahdat & Kautz, 2020) optimizes high-quality image generation via global
correlation capturing across multi-layer latent variables.
We compare with Info-GAN Control-VAE and NVAE on three datasets, including dSprites,
3Dshapes and 3Dcars. The 3DCars (Reed et al., 2015) exhibits different car models from Fi-
dler et al. (2012) under different camera viewpoints. The evaluation method is the disentanglement
score MIG. Table 7 show that NVAE and VampPrior can achieve comparable reconstruction error
compared to RecurD. However, their disentanglement qualities are not as good as RecurD, because
RecurD additionally regularizes the inter-layer information sharing and alleviates the information re-
dundancy of multiple layers. This experiment demonstrates the superiority of the proposed RecurD
method in disentanglement compared to existing hierarchical VAEs.
Table 7: MIG score and reconstruction error on the test sets for dSprites, 3DShapes and 3DCars.
Bold face indicates the best results, i.e., reconstruction error and disentanglement scores.
dataset	dSprites		3DShapes		3DCars	
	ReConst. Error	MIG	Reconst. Error	MIG	Reconst. Error	MIG
β-VAE(β = 4)	0.0066	0.2617	0.0216	0.2519	0.0376	0.1015
InfOGAN	-	0.1598	-	0.1874	-	0.1083
Control-VAE	0.0102	0.2455	0.0357	0.2630	0.0257	0.1583
NVAE	0.0041	0.0043	0.0078	0.0081	0.0118	0.0034
RecurD	0.0047	0.2707	0.0083	0.3105	0.0132	0.1762
E.2 Ablation Study
In this section, we report the supplementary results of ablation study. The first is to study the impact
of compositional learning objective with varying regularization coefficients (λ1 and λ2) of minimal
sufficiency and disentanglement. Figure E.2 and Figure E.2 show the scatter plots of disentan-
glement scores along with reconstruction error on the test sets of dSprites, with varying λ1, λ2.
Figure 12 and Figure 13 show the results on the 3DShapes.
The second set is to evaluate the influence of hyperparameter k — the group size in the GoE of
Recursive Module. Figure E.2 and Figure E.2 show the influence of varying k for disentanglement
scores and reconstruction error on the test sets of dSprites. Figure E.2 and Figure E.2 show the
results on the 3DShapes.
The third set is to evaluate the disentanglement metrics on the preceding layers. We compute MIG
on mL-1 and mL-2 of RecurD on 3DShapes. As shown in Table 8, high-level representations have
higher MIG than that of low-level representations (z > mL-1 > mL-2). These results confirm that
18
Published as a conference paper at ICLR 2022
the recursive propagation of inductive bias through the feed-forward network improves disentangled
representation learning.
The last set is to evaluate the impact of architecture of Gate-of-Encoders (GoE), we conduct three
variant GoEs, including Linear: GoE is implemented as a linear layer with softmax; Fix: GoE is
implemented as fix assignment, i.e., we split ml in d + 1 equal slices; and Att: GoE is implemented
as a multi-head attention layer (the head is fixed as 8). As shown in Table 9, this study demonstrates
that GoE indeed fits well with the disentanglement process.
Figure 10: Ablation study on λ1 and λ2 on dSprites.
Figure 11: Ablation study on λ1 and λ2 on dSprites.
Table 8: Comparison of MIG on z, mL-1 and mL-2 .
I	I mL-2	mL-1	z(mL)	Reconst. Error
MIG	I 0.1081	0.2892	0.3105	0.0083
19
Published as a conference paper at ICLR 2022
Ai = 0.0,½2 = 8.0
λι = 8rΛ2 = 8
λι = 8.Λ2 = 0
Ai = 0.0,½2 = 8.0
λι = 8rΛ2 = 8
λι = 8.Λ2 = 0
Figure 12: Ablation study on λ1 and λ2 on 3DShapes.
3DShape
3DShape
Figure 13: Ablation study on λ1 and λ2 on 3DShapes.
Figure 14: Ablation study on k on dSprites.
20
Published as a conference paper at ICLR 2022
Figure 15: Ablation study on k on dSprites.
Reconst. Error
Reconst. Error
Figure 16: Ablation study on k on 3DShapes.
Figure 17: Ablation study on k on 3DShapes.
21
Published as a conference paper at ICLR 2022
Table 9: Ablation study on the GoE architecture.
	I	I Linear	FiX	Att	RecurD
MIG	I 0.2539	0.2617	0.2604	0.3105
Reconst. Error	I 0.0176	0.0116	0.0095	0.0083
E.3 Qualitative Results
In this section, we report the qualitative samples of traversal images on three datasets, including
dSprites(Figure 18), 3DShapes(Figure 19) and CelebA(from Figure 20 to Figure 25).
Specifically, on CelebA, we tentatively increase the dimensionality of latent variables of RecurD
(from 16 to 32) by doubling the output of the last layer of each encoder from a single latent variable
to a 2-dimensional variable.
Figure 18: Traversal samples on dSprites.
Figure 19: Traversal samples on 3DShapes.
Figure 20: Traversal samples on CelebA
(Azimuth).
Figure 21: Traversal samples on CelebA
(Background Color).
22
Published as a conference paper at ICLR 2022
Figure 22: Traversal samples on CelebA
Figure 23: Traversal samples on CelebA
(Face Width).
(Face Color).
Figure 24: Traversal samples on CelebA
(Gender).
Figure 25: Traversal samples on CelebA
(Smile).
E.4 Evaluation of Computational Costs
In this section, we evaluate the computational complexity of our model on 3DShapes and CelebA.
The evaluation metrics consist of multiply-accumulate operation (MACs), the model parameters
(Params), evaluation time (Eva. time) and converge time (all models converge to the same recon-
struction error as β-VAE, denoted as Con. time).
In terms of parameter efficiency, as shown in Table 10, the recursive disentanglement network itself
only contains 0.826 million parameters (RecurD 2 w/o MINEs), which are comparable to Beta-VAE.
Most of the parameters of RecurD 2 are contributed by using MINE estimator. Specifically, in the
initial implementation (RecurD 2/specific MINEs), each pair of outputs of encoders is equipped
with a specific MINE model, which contributes 7.214 million parameters. We further optimize the
design by using shared MINE within the same feature category, which can effectively reduce the
total number of parameters down to 1.325 million (RecurD 2/shared MINEs).
Table 10: Complexity comparison of three models on 3DShapes and CelebA.
Dataset	Method	MACs(G)	Params(M)	Eva. time(s)	Con. time(s)
	RecUrD 1	3.466	3.672	0.005124	27.1212
3DShapes	RecUrD 2	3.469	3.694	0.005354	23.3197
	beta-VAE	3.144	0.769	0.003283	32.5251
	Factor-VAE	3.401	4.779	0.003389	40.6231
	RecUrD 1	3.944	7.935	0.01065	29.7421
CelebA	RecurD 2	3.957	8.040	0.01087	36.6408
	RecurD 2 w/o MINEs	2.960	0.826	-	-
	RecurD 2/shared MINEs	3.654	1.325	0.01072	36.6402
	beta-VAE	3.145	0.769	0.01030	45.2067
	Factor-VAE	3.402	4.792	0.01005	48.9939
23