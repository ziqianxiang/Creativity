Published as a conference paper at ICLR 2022
When, Why, and Which Pretrained GANs
Are Useful?
Timofey Grigoryev *
Yandex
grigorev.ta@phystech.edu
Artem Babenko
Yandex
artem.babenko@phystech.edu
Andrey Voynov *
Yandex
an.voynov@yandex.ru
Ab stract
The literature has proposed several methods to finetune pretrained GANs on new
datasets, which typically results in higher performance compared to training from
scratch, especially in the limited-data regime. However, despite the apparent em-
pirical benefits of GAN pretraining, its inner mechanisms were not analyzed in-
depth, and understanding of its role is not entirely clear. Moreover, the essential
practical details, e.g., selecting a proper pretrained GAN checkpoint, currently do
not have rigorous grounding and are typically determined by trial and error.
This work aims to dissect the process of GAN finetuning. First, we show that
initializing the GAN training process by a pretrained checkpoint primarily affects
the model’s coverage rather than the fidelity of individual samples. Second, we
explicitly describe how pretrained generators and discriminators contribute to the
finetuning process and explain the previous evidence on the importance of pre-
training both of them. Finally, as an immediate practical benefit of our analysis,
we describe a simple recipe to choose an appropriate GAN checkpoint that is the
most suitable for finetuning to a particular target task. Importantly, for most of the
target tasks, Imagenet-pretrained GAN, despite having poor visual quality, appears
to be an excellent starting point for finetuning, resembling the typical pretraining
scenario of discriminative computer vision models.
1	Introduction
These days, generative adversarial networks (GANs) (Goodfellow et al., 2014) can successfully ap-
proximate the high-dimensional distributions of real images. The exceptional quality of the state-of-
the-art GANs (Karras et al., 2020b; Brock et al., 2019) makes them a key ingredient in applications,
including semantic editing (Isola et al., 2017; Zhu et al., 2018; Shen et al., 2020; Voynov & Babenko,
2020), image processing (Pan et al., 2020; Ledig et al., 2017; Menon et al., 2020), video generation
(Wang et al., 2018a), producing high-quality synthetics (Zhang et al., 2021; Voynov et al., 2020).
To extend the success of GANs to the limited-data regime, it is common to use pretraining, i.e., to
initialize the optimization process by the GAN checkpoint pretrained on some large dataset. A line
of works (Wang et al., 2018b; Noguchi & Harada, 2019; Zhao et al., 2020; Mo et al., 2020; Wang
et al., 2020; Li et al., 2020) investigate different methods to transfer GANs to new datasets and
report significant advantages compared to training from scratch both in terms of generative quality
and convergence speed. However, the empirical success of GAN pretraining was not investigated
in-depth, and its reasons are not entirely understood. From the practical standpoint, it is unclear how
to choose a proper pretrained checkpoint or if one should initialize both generator and discriminator
or only one of them. To the best of our knowledge, the only work that systematically studies the
benefits of pretraining is Wang et al. (2018b). However, the experiments in (Wang et al., 2018b)
were performed with currently outdated models, and we observed that some conclusions from Wang
* Indicates equal contribution
1
Published as a conference paper at ICLR 2022
et al. (2018b) are not confirmed for modern architectures like StyleGAN2 (Karras et al., 2020b). In
particular, unlike the prior results, it appears that for state-of-the-art GANs, it is beneficial to transfer
from sparse and diverse sources rather than dense and less diverse ones.
In this work, we thoroughly investigate the process of GAN finetuning. First, we demonstrate that
starting the GAN training from the pretrained checkpoint can significantly influence the diversity
of the finetuned model, while the fidelity of individual samples is less affected. Second, we dissect
the mechanisms of how pretrained generators and discriminators contribute to the higher cover-
age of finetuned GANs. In a nutshell, we show that a proper pretrained generator produces sam-
ples in the neighborhood of many modes of the target distribution, while a proper pretrained dis-
criminator serves as a gradient field that guides the samples to the closest mode, which together
result in a smaller risk of mode missing. This result explains the evidence from the literature
that it is beneficial to initialize both generator and discriminator when finetuning GANs. Finally,
we investigate different ways to choose a suitable pretrained GAN checkpoint for a given target
dataset. Interestingly, for most of the tasks, Imagenet-pretrained models appear to be the optimal
initializers, which mirrors the pretraining of discriminative models, where Imagenet-based initial-
ization is de-facto standard (Donahue et al., 2014; Long et al., 2015; He et al., 2020; Chen et al.,
2020a). Our conclusions are confirmed by experiments with the state-of-the-art StyleGAN2 (Karras
et al., 2020b), chosen due to its practical importance and a variety of open-sourced checkpoints,
which can be used as pretrained sources. The code and pretrained models are available online at
https://github.com/yandex- research/gan- transfer
The main contributions of our analysis are the following:
1.	We show that initializing the GAN training by the pretrained checkpoint can significantly
affect the coverage and has much less influence on the realism of individual samples.
2.	We explain why it is important to initialize both generator and discriminator by describing
their roles in the finetuning process.
3.	We describe a simple automatic approach to choose a pretrained checkpoint that is the most
suitable for a given generation task.
2	Analysis
This section aims to explain the success of the GAN finetuning process compared to training from
scratch. First, we formulate the understanding of this process speculatively and then confirm this
understanding by experiments on synthetic data and real images.
2.1	High-level intuition
Let us consider a pretrained generator G and discriminator D that are used to initialize the GAN
training on a new data from a distribution ptarget. Throughout the paper, we show that a discriminator
initialization is “responsible for” an initial gradient field, and a generator initialization is “responsi-
ble for” a target data modes coverage. Figure 1 illustrates the overall idea with different initialization
patterns. Intuitively, the proper discriminator initialization guarantees that generated samples will
move towards “correct” data regions. On the other hand, the proper pretrained generator guarantees
that the samples will be sufficiently diverse at the initialization, and once guided by this vector field,
they will cover all target distribution. Below, we confirm the validity of this intuition.
2.2	Synthetic Experiment
We start by considering the simplest synthetic data presented on Figure 2. Our goal is to train a GAN
on the target distribution, a mixture of ten Gaussians arranged in a circle. We explore three options
to initialize the GAN training process. First, we start from random initialization. The second and the
third options initialize training by GANs pretrained on the two different source distributions. The
first source distribution corresponds to a wide ring around the target points, having high coverage
and low precision w.r.t. target data. The second source distribution is formed by three Gaussians that
share their centers with three target ones but have a slightly higher variance. This source distribution
has high precision and relatively low coverage w.r.t. target data. Then we train two source GANs
from scratch to fit the first and the second source distributions and employ these checkpoints to
initialize GAN training on the target data. The results of GAN training for the three options are
presented on Figure 2, which shows the advantage of pretraining from the more diverse model,
2
Published as a conference paper at ICLR 2022
Figure 1: Different G/D initialization patterns: the red dots denote pretrained generator samples,
the arrows denote a pretrained discriminator gradient field, the blue distribution is the target. From
left to right: bad discriminators will lead good initial samples out of the target distribution; bad
generators will drop some of the modes even being guided by good discriminators; both proper
G/D serve as an optimal initialization for transfer to a new task.
• ∙ SoUrce-II
Generated
W
Figure 2: Impact of GAN pretraining for synthetic data. 1) source and target distributions. 2-
3) GANs pretrained on two source distributions. 4-6): GANs trained on the target distribution,
initialized by the two source checkpoints and randomly. Each plot also reports the Wasserstein-1
distance between the generated and the target distributions.
which results in a higher number of covered modes. The details of the generation of the synthetic
are provided in the appendix.
Dissecting the contributions from G and D. Here, we continue with the synthetic example from
above and take a closer look at the roles that the pretrained generator and discriminator play when
finetuning GANs. Our goal is to highlight the importance of (1) the initial coverage of the target
distribution by the pretrained generator and (2) the quality of the gradient field from the pretrained
discriminator. We quantify the former by the established recall measure (Kynkaanniemi et al., 2019)
computed in the two-dimensional dataspace with k=5 for 1000 randomly picked samples from the
target distribution and the same number of samples produced by the pretrained generator. To evaluate
the quality of the discriminator gradient field, we use a protocol described in (Sinha et al., 2020).
Namely, we assume that the “golden” ground truth gradients would guide each sample towards
the closest Gaussian center from the target distribution. Then we compute the similarity between
the vector field VxD provided by the pretrained discriminator and the vector field of “golden”
gradients. Specifically, we evaluate the cosine similarity between these vector fields, computed for
the generated samples.
Given these two measures, we consider a series of different starting generator/discriminator check-
points (Gi, Di), i = 1, . . . , N. The details on the choice of the starting checkpoints are provided
in the appendix. Then we use each pair (Gi, Di) as initialization of GAN training on the target
distribution of ten Gaussians described above. Additionally, for all starting Gi /Di , we evaluate the
recall and the discriminator gradients field similarity to the “golden” gradients. The overall quality
of GAN finetuning is measured as the Wasserstein-1 distance between the target distribution and
the distribution produced by the finetuned generator. The scatter plots of recall, the similarity of
gradient fields, and Wasserstein-1 distance are provided in Figure 3. As can be seen, both the re-
call and gradient similarity have significant negative correlations with the W1 -distance between the
ground-truth distribution and the distribution of the finetuned GAN. Furthermore, for the same level
of recall, the higher values of the gradient similarity correspond to lower Wasserstein distances. Al-
ternatively, for the same value of gradient similarity, higher recall of the source generator typically
corresponds to the lower Wasserstein distance. We also note that the role of the pretrained generator
is more important since, for high recall values, the influence from the discriminator is not significant
(see Figure 3, left).
3
Published as a conference paper at ICLR 2022
This synthetic experiment does not rigorously proVe the existence of a causal relationship between
the recall or gradient similarity and the quality of the finetuned GANs since it demonstrates only
correlations of them. HoweVer, in the experimental section, we show that these correlations can
be successfully exploited to choose the optimal pretraining checkpoint, eVen for the state-of-the-art
GAN architectures.
1.0
0.8
=IBJ①N
4 2 0 8.
1aup0pl w≥
VD similarity	VD similarity	Recall
Figure 3: Scatter plots of the pretrained generator quality (Recall) and the pretrained discriminator
quality (VD similarity) Vs the quality of finetuned GAN (W 1 -distance). Each point represents
a result of GAN finetuning, which started from a particular pair of pretrained discriminator and
generator. The color indicates the W1 -distance between the final generator distribution and the
target distribution. The Pearson correlation of the final W1-distance is equal -0.84 for the Recall,
and -0.73 for the gradient similarity.
1 1 1 8
3	Large-scale experiments
3.1	Exploring pretraining for StyleGAN2
In this section, we confirm the conclusions from the preVious sections experimentally with the state-
of-the-art StyleGAN2 architecture (Karras et al., 2020b). If not stated otherwise, we always work
with the image resolution 256 × 256.
Datasets. We work with six standard datasets established in the GAN literature. We also include
two datasets of satellite images to inVestigate the pretraining behaVior beyond the domain of natural
images. As potential pretrained sources, we use the StyleGAN2 models trained on these datasets.
Table 1 reports the list of datasets and the FID Values (Heusel et al., 2017) of the source checkpoints.
We also experimented with four smaller datasets to Verify our conclusions in the medium-shot and
few-shot regimes. The details on the datasets are proVided in the appendix.
Experimental setup. Here, we describe the details of our experimental protocol for both the pre-
training of the source checkpoints and the subsequent training on the target datasets. We always
use the official PyTorch implementation of StyleGAN2-ADA (Karras et al., 2020a) proVided by the
authors1. We use the “stylegan2” configuration in the ADA implementation with the default hy-
perparameters (same for all datasets). Training is performed on eight Tesla V100 GPUs and takes
approximately three hours per 1M real images shown to the discriminator.
Pretraining of source checkpoints. We pretrain one checkpoint on the Imagenet for 50M real
images shown to the discriminator and seVen checkpoints on other source datasets from Table 1
for 25M images. A larger number of optimization steps for the Imagenet is used since this dataset
is more challenging and requires more training epochs to conVerge. For the large LSUN datasets
(Cat, Dog, Bedroom), we use 106 first images to preserVe memory. For Satellite-Landscapes, we
use ADA due to its smaller size. Then, we always use checkpoints with the best FID for further
transferring to target datasets for each source dataset.
Training on target datasets. For each source checkpoint, we perform transfer learning to all
datasets from Table 1. We use the default transfer learning settings from the StyleGAN2-ADA
implementation (faster adaptiVe data augmentation (ADA) adjustment rate, if applicable, and no
Gema warmup). ADA is disabled for the datasets containing more than 50K images and enabled
for others with default hyperparameters. In these experiments, we train for 25M real images shown
1https://github.com/NVlabs/stylegan2-ada-pytorch
4
Published as a conference paper at ICLR 2022
to the discriminator. Each transfer experiment is performed with three independent runs, and the
metrics are reported for the run corresponding to the median best FID (Heusel et al., 2017).
Metrics. In the experiments, we evaluate the	Dataset	Number of images	FID
performance via the four following metrics. (1) Frechet Inception Distance (FID) (Heusel et al., 2017), which quantifies the discrepancy between the distributions of real and fake im-	Datasets for pretraining		
	Imagenet	1281137	49.8
	LSUN-Cat	1000000	7.8
ages, represented by deep embeddings. Both distributions are approximated by Gaussians, and the Wasserstein distance between them is	LSUN-Dog	1000000	15.0
	LSUN-Bedroom	1000000	3.3
computed. (2) Precision (Kynkaanniemi et al.,	LSUN-ChUrch	126227	3.2
2019), which measures the realism of fake im- ages, assuming that the visual quality of a par- ticular fake is high if it belongs to the neigh- borhood of some real images in the embedding	Satellite-Landscapes	2608	26.6
	Satellite-Buildings	280741	12.4
	FFHQ	70000	5.5
space. (3) Recall (Kynkaanniemi et al., 2019), which quantifies GAN diversity, measuring the rate of real images that belong to the neighbor- hood of some fake images in the embedding	Additional target datasets		
	CIFAR-10	50000	—
	Grumpy Cat	100	—
space. (4) Convergence rate equals a number of real images that were shown to the discrim- inator at the moment when the generator FID for the first time exceeded the optimal FID by at	Flowers	8189	—
	Simpsons	41866	—
	BreCaHAD	3253	—
most 5%. Intuitively, this metric quantifies how fast the learning process reaches a plateau. FID is computed based on the image embeddings extracted by the InceptionV3 model2. Precision and Recall use the embeddings provided by the VGG-16 model3. Precision and Recall are al-	Table 1: The datasets used in our experiments. All images are resized to 256 × 256 resolution. The last column reports the FID values of the source checkpoints trained with the random initialization.		
ways computed with k=5 neighbors. For FID calculation, we always use all real images and 50K
generated samples. For Precision/Recall calculation, we use the first 200K real images (or less, if
the real dataset is smaller) and 50K generated samples.
Results. The metric values for all datasets are reported in Table 2, where each cell corresponds to
a particular source-target pair. For the best (in terms of FID) checkpoint obtained for each source-
target transfer, we report the FID value (top row in each cell), Precision and Recall (the second and
the third rows in each cell), and the convergence rate measured in millions of images (bottom row in
each cell). We highlight the sources that provide the best FID for each target dataset or differ from
the best one by at most 5%. We additionally present the curves of FID, Precision, and Recall values
for several target datasets on Figure 9 and Figure 10 in the appendix.
We describe the key observations from Table 2 below:
•	In terms of FID, a pretraining based on a diverse source (e.g., Imagenet or LSUN Dog) is
superior to training from scratch on all datasets in our experiments.
•	The choice of the source checkpoint significantly influences the coverage of the finetuned
model, and the Recall values vary considerably for different sources, especially for smaller
target datasets. For instance, on the Flowers dataset, their variability exceeds ten percent.
In contrast, the Precision values are less affected by pretraining, and their typical variability
is about 2-3%. Figure 4 reports the standard deviations of Precision/Recall computed over
different sources and highlights that Recall has higher variability compared to Precision,
despite the latter having higher absolute values.
•	Pretraining considerably speeds up the optimization compared to the training from scratch.
2 https://nvlabs- fi- cdn.nvidia.com/stylegan2- ada- pytorch/pretrained/
metrics/inception- 2015-12-05.pt
3 https://nvlabs- fi- cdn.nvidia.com/stylegan2- ada- pytorch/pretrained/
metrics/vgg16.pt
5
Published as a conference paper at ICLR 2022
0	/ 7	S ς? 7	σ 7	7	✓ /	©	Jr S 型 6	W
F P RC		5.7 0.782 0.417 5	5.52 0.793 0.419 8	6.08 0.767 0.455 7	5.18 0.793 0.459 11	8.49 0.798 0.318 14	6.57 0.797 0.355 16	4.87 0.784 0.457 12	5.52 0.795 0.472 13
FP R C	2~7H∖- 0.664 0.469 24		3.01 0.651 0.447 25	-^2.8- 0.663 0.485 25	^63- 0.657 0.471 25	3.77 0.667 0.314 21	4.3 0.643 0.344 24	~^57~ 0.679 0.475 25	3.3 ±0.2 0.668 0.459 23
F P R C	7.62 0.68 0.402 22	~668- 0.68 0.381 25		^772- 0.669 0.402 25	-665- 0.687 0.409 18	9.09 0.703 0.273 25	9.95 0.67 0.303 25	^712- 0.688 0.39 21	7.85 0.684 0.368 20
FP R C	-3.09- 0.689 0.54 23	3.11 0.7 0.497 22	"3.28- 0.69 0.496 23		^.97- 0.677 0.543 21	3.99 0.692 0.414 22	6.79 0.63 0.322 5	3.0 0.699 0.528 23	3.16 0.682 0.554 25
F P R R C	-148- 0.74 0.363 22	IJ1Λ - 0.747 0.334 22	13.9 0.757 0.359 24	T56- 0.738 0.36 25		18.4 0.753 0.237 25	18.4 0.754 0.256 25	^T44- 0.743 0.365 24	15.02 0.758 0.349 24
F F P R RC	-ITI- 0.347 0.549 24	11.5 0.337 0.53 20	11.77 0.316 0.52 21	11.2 0.34 0.555 16	T21- 0.333 0.526 25		16.7 0.281 0.413 25	10.72 0.319 0.574 25	12.36 0.348 0.507 19
F P R C	25.3 0.756 0.249 23	26.1 0.762 0.191 21	24.3 0.762 0.2 18	25.3 0.73 0.291 5	23.8 0.759 0.282 8	28.2 0.769 0.136 14		21.0 0.719 0.393 2	26.6 0.737 0.214 25
F P RC	8.6 ±0.5 0.79 0.493 15	8.29 0.764 0.45 11	7.6 0.75 0.479 5	-862- 0.759 0.502 8	7.11 0.769 0.525 8	10.4 0.783 0.397 25	^22- 0.759 0.401 19	6.2 ±0.5 0.761 0.559 3	9.33 0.781 0.455 20
FP R C	9.47 0.786 0.251 22	9.79 0.776 0.215 16	9.4 0.795 0.194 15	^88- 0.773 0.226 25	8.88 0.773 0.269 6	11.8 0.772 0.14 20	9.07 0.809 0.153 21	-831- 0.773 0.282 9	10.73 0.78 0.271 21
F P R C	11.9- 0.999 0.06 24	12.3 0.996 0.02 25	14.0 0.997 0.03 24	16.1 0.995 0.0217 25	12.6 0.998 0.04 25	28.5 0.868 0.01 1	16.3 0.999 0.045 25	14.7 0.999 0.05 25	15.34 0.997 0.0175 25
F P R C	~787- 0.432 0.406 24	8.0 ±0.4 0.423 0.349 25	8.12 0.431 0.353 24	7.93 0.436 0.384 21	^767- 0.416 0.395 22	10.0 0.404 0.219 24	9.98 0.418 0.173 25	8.28 0.427 0.364 25	8.42 0.42 0.335 25
F P R C	ɪsi- 0.694 0.385 3	23.12 0.679 0.417 1	24.80 0.696 0.462 1	25.40 0.709 0.412 4	25.36 0.692 0.439 1	23.81 0.730 0.337 2	21.84 0.712 0.473 3	22.73 0.703 0.483 1	23.72 0.705 0.434 11
Table 2: Metrics computed for the best-FID checkpoint for different source and target datasets. Each
row corresponds to a particular target dataset, and each column corresponds to a particular source
model used to initialize the training. For each target dataset, we highlight (by orange) the sources
that provide the smallest FID or which FID differs from the best one by at most 5%. In each cell, we
report from to bottom: FID, Precision, Recall, and convergence rate measured in millions of images
(lower is better). In purpose to make the table easier to read, we report std only once it exceeds 5%
which happens rarely. The typical values vary around 0.1.
6
Published as a conference paper at ICLR 2022
PreCi Precision Std
Recall std
IIIhiJllLIi
0.06
0.04
0.02
0.00
√, 7
W"
Q ∖?
CX V
V
・通
Z / £ T
cr

/ Z
Figure 4: Standard deviations of Precision/Recall values for each target dataset computed over dif-
ferent sources. Due to the symmetric nature of the quantities, the table reports the standard deviation
for precision and recall computed over an equal number of real and generated samples (minimum
between a dataset size and 50K)
Overall, despite having poor quality (FID=49.8), the Imagenet-pretrained unconditional StyleGAN2
model appears to be a superior GAN initialization that typically leads to more efficient optimization
compared to alternatives. This result contradicts the observations in (Wang et al., 2018b) showing
that it is beneficial to transfer from dense and less diverse sources rather than sparse and diverse ones,
like Imagenet. We attribute this inconsistency to the fact that (Wang et al., 2018b) experimented with
the WGAN-GP models, which are significantly inferior to the current state-of-the-art ones.
3.2	Analysis
In this section, we perform several additional experiments that illustrate the benefits of pretraining.
Pretraining improves the mode coverage for real data. Here, we consider the Flowers dataset and
assume that each of its 102 labeled classes corresponds to different distribution modes. To assign the
generator samples to the closest mode, we train a 102-way flowers classifier via finetuning the linear
head of the Imagenet-pretrained ResNet-50 on real labeled images from Flowers. Then we apply this
classifier to generated images from eleven consecutive generator snapshots from the GAN training
process on the interval from 0 to 200 kimgs taken every 20 kimgs. This pipeline allows for tracking
the number of covered and missed modes during the training process. Figure 5 (left) demonstrates
how the number of covered modes changes when GAN is trained from scratch or the checkpoints
pretrained on FFHQ and Imagenet. In this experiment, we consider a mode being “covered” if it
contains at least ten samples from the generated dataset of size 10 000. One can see that Imagenet,
being the most diverse source, initially covers more modes of the target data and faster discovers the
others. FFHQ also provides coverage improvement but misses more modes compared to Imagenet
even after training for 200 kimgs. With random initialization, the training process covers only a
third of modes after training for 200 kimgs . On the right of Figure 5, we show samples drawn for
the mode, which is poorly covered by the GAN trained from FFHQ initialization and well-covered
by its Imagenet counterpart.
4unou S①Pon ^ω⅛ω>ou
0	20	40	60	80 100 120 140 160 180 200
1K images
Figure 5: Left: Number of modes covered by the generator snapshots during the training process
from three different initializations. Right: samples of the 65-th class of the Flowers dataset, which is
well-covered by the GAN trained from the Imagenet initialization and poorly covered by the GAN
trained from the FFHQ initialization. Top: real images; Middle: FFHQ; Bottom: Imagenet.
7
Published as a conference paper at ICLR 2022
Figure 6: Evolution of the generated samples with different source initializations. Left: average
LPIPS-distance between images generated by the consecutive generator snapshots for the same la-
tent code. Right: images generated with the same latent code evolving during training: top row:
start from FFHQ, middle row: start from Imagenet, bottom row: start from random initialization.
Figure 7: Left: distribution of samples trajectories lengths with Flowers as a target dataset. Right:
generated class change probability for individual latents during the training.
Pretraining provides more gradual image evolution. The observations above imply that it is ben-
eficial to initialize training by the checkpoint with a higher recall so that the target data is originally
better “covered” by the source model. We conjecture that transferring from a model with higher
recall makes it easier to cover separate modes in the target distribution since, in this case, generated
samples can slowly drift to the closest samples of the target domain without abrupt changes to cover
previously missing modes. To validate this intuition, we consider a fixed batch of 64 random la-
tent codes z and a sequence of the generator states G1, . . . , GN obtained during the training. Then
we quantify the difference between consecutive images computed as the perceptual LPIPS distance
Zhang et al. (2018) LPIPS(Gi(z), Gi+1 (z)). Figure 6 shows the dynamics of the distances for
Flowers as the target dataset and Imagenet, FFHQ, and random initializations. Since the Imagenet
source initially has higher coverage of the target data, its samples need to transform less, which
results in higher performance and faster convergence.
Figure 6 indicates more gradual sample evolution when GAN training starts from a pretraining
checkpoint. Here we additionally report the distributions of samples’ trajectories’ lengths quantified
by LPIPS. Namely, for a fixed z and a sequence of the generator snapshots G1, . . . , GN obtained
during training, we calculate the length of the trajectory as a sum Pi LPIPS(Gi(z), Gi+1(z)).
Figure 7 (left) presents the length distributions for three initializations and Flowers as the target
dataset.
Finally, to track the dynamics of mode coverage of the target dataset, we obtain the class assignments
of the generated samples G1 (z), . . . , GN (z) with a classifier pretrained on the Flowers dataset.
Then for the samples Gi(z), Gi+1(z) generated with the consequent checkpoints, we calculate the
probability that the sample changes its class assignment by averaging over 256 latent codes. That
is, we evaluate the probability that a flower class of a sample Gi(z) differs from a class ofa sample
Gi+1 (z). The probabilities of the class change for different source checkpoints are presented in
Figure 7, right. Importantly, training from pretrained sources demonstrates higher class persistence
8
Published as a conference paper at ICLR 2022
GAN Model (source, target)	Inverted Domain	BestFID	LPIPS-error	F-error
s: Random, t: FFHQ	CeIebA-HQ	^35	^022	^GΓT86^^
s: Imagenet, t: FFHQ	CeIebA-HQ	4.86	0.22	0.174
s: Random, t: FFHQ	FFHQ	^35	^025	-q：T80^^
s: Imagenet, t: FFHQ	FFHQ	4.86	0.25	0.168
s: Random, t: L.Bedroom	L.Bedroom	^97	^047	^GΓT23^^
s: Imagenet, t: L.Bedroom	L.Bedroom	2.56	0.44	0.115
Table 3: Reconstruction errors for GAN models with different source and target datasets.
of individual samples. This indicates that the Imagenet-pretrained generator initially covers the
target dataset well enough and requires fewer mode-changing sample hops during training.
Pretraining is beneficial for downstream tasks. Here, we focus on the task of inverting a real
image given a pretrained generator, which is necessary for semantic editing. We employ the recent
GAN inversion approach (Tov et al., 2021) and train the encoders that map real images into the
latent space of generators approximating FFHQ and Bedroom distributions. For both FFHQ and
Bedroom, we consider the best generators that were trained from scratch and the Imagenet-based
pretraining. Table 3 reports the reconstruction errors quantified by the LPIPS measure and F -error.
The details of the metrics computation are provided in the appendix. Overall, Table 3 confirms that
higher GAN recall provided by pretraining allows for the more accurate inversion of real images.
4 Choosing proper pretrained checkpoint
This section describes a simple recipe to select the most appropriate pretrained checkpoint to initial-
ize GAN training for a particular target dataset. To this end, we consider a set of natural proxy met-
rics that quantify the similarity between two distributions. Each metric is computed in two regimes.
In the first regime, we measure the distance between the source dataset, consisting of real images
used to pretrain the GAN checkpoint, and the target dataset of real images. In the second regime,
we use the generated images from pretrained checkpoints instead of the source dataset. The second
regime is more practical since it does not require the source dataset. As natural proxy metrics, we
consider FID, KID (BinkoWski et al., 2018), Precision, and Recall measures.
To estimate the reliability of each metric, we
calculate the number of target datasets for
Which this metric does not correctly predict
the optimal starting checkpoint. We consider
a starting checkpoint optimal if it provides the
loWest FID score or its FID score differs from
Regime/Metric	FID	KID	Precision	Recall
Real Source	^1-	5	-H-	-2-
Generated Source	^1-	3	7	-3-
the loWest by most 5%. The quality for all met- Table 4: The number of target datasets for Which
rics is presented in Table 4, Which shoWs that the metrics fail to identify the best source (With up
FID or Recall can be used as a rough guide to to 5% best FID deviation).
select a pretrained source in both regimes. On
the other hand, Precision is entirely unreliable.
This observation is consistent With our findings from Section 2 that imply that Recall can serve as a
predictive measure of finetuning quality.
5 Conclusion
Transferring pretrained models to neW datasets and tasks is a Workhorse of modern ML. In this paper,
We investigate its success in the context of GAN finetuning. First, We demonstrate that transfer
from pretrained checkpoints can improve the model coverage, Which is crucial for GANs exhibiting
mode-seeking behavior. Second, We explain that it is beneficial to use both pretrained generators
and discriminators for optimal finetuning performance. This implies that the GAN studies should
open-source discriminator checkpoints as Well rather than the generators only. Finally, We shoW that
the recall measure can guide the choice of a checkpoint for transfer and highlight the advantages
of Imagenet-based pretraining, Which is not currently common in the GAN community. We open-
source the StyleGAN2 checkpoints pretrained on the Imagenet of different resolutions for reuse in
future research.
9
Published as a conference paper at ICLR 2022
References
Mikolaj BinkoWski, Danica J. Sutherland, Michael ArbeL and Arthur Gretton. Demystifying
MMD GANs. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=r1lUOzWCW.
AndreW Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. In International Conference on Learning Representations, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameWork for
contrastive learning of visual representations. In International conference on machine learning,
pp.1597-1607. PMLR, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines With momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Inter-
national conference on machine learning, pp. 647-655. PMLR, 2014.
Ian GoodfelloW, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, 2014.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a tWo time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation With
conditional adversarial netWorks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, 2017.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial netWorks With limited data. NeurIPS, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8110-8119, 2020b.
TUomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. In Advances in Neural Information
Processing Systems, pp. 3929-3938, 2019.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, AndreW Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017.
Yijun Li, Richard Zhang, Jingwan Lu, and Eli Shechtman. Few-shot image generation with elastic
weight consolidation. arXiv preprint arXiv:2012.02780, 2020.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431-3440, 2015.
Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-
supervised photo upsampling via latent space exploration of generative models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2437-2445, 2020.
10
Published as a conference paper at ICLR 2022
Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze discriminator: A simple baseline for fine-tuning
gans. arXiv preprint arXiv:2002.10964, 2020.
Atsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics
adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
2750-2758, 2019.
Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting
deep generative prior for versatile image restoration and manipulation. In European Conference
on Computer Vision, pp. 262-277. Springer, 2020.
Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for
semantic face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9243-9252, 2020.
Samarth Sinha, Zhengli Zhao, Anirudh Goyal ALIAS PARTH GOYAL, Colin A Raffel, and Au-
gustus Odena. Top-k training of gans: Improving gan performance by throwing away bad
samples. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 14638-14649. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
a851bd0d418b13310dd1e5e3ac7318ab- Paper.pdf.
Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder
for stylegan image manipulation. arXiv preprint arXiv:2102.02766, 2021.
Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan
latent space. In International Conference on Machine Learning, pp. 9786-9796. PMLR, 2020.
Andrey Voynov, Stanislav Morozov, and Artem Babenko. Big gans are watching you: To-
wards unsupervised object segmentation with off-the-shelf generative models. arXiv preprint
arXiv:2006.04988, 2020.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. In Advances in Neural Information Processing Systems, 2018a.
Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bog-
dan Raducanu. Transferring gans: generating images from limited data. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 218-234, 2018b.
Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost
van de Weijer. Minegan: effective knowledge transfer from gans to target domains with few im-
ages. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 9332-9341, 2020.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018.
Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio
Torralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with minimal human effort.
arXiv preprint arXiv:2104.06490, 2021.
Miaoyun Zhao, Yulai Cong, and Lawrence Carin. On leveraging pretrained gans for limited-data
generation. ICML, 2020.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, 2018.
11
Published as a conference paper at ICLR 2022
6 Appendix
Datasets
Here we provide the details for the used datasets. Table 5 reports the size, original image resolution
(which was always resized to 256 × 256 in our experiments), number of samples used for training,
and URL for each of the datasets. In Tables 6, 7, 8, 9 we report pairwise distances between source
and target datasets for different metrics. Figure 8 illustrates samples from each dataset. As for
BreCaHAD, we generate a dataset of 256 × 256 crops of the original dataset images with the code
provided in StyleGAN-ADA repository.
Dataset	Size	Original Resolution	Samples Used
CIFAR-104	50000	32 X 32	50000
FFHQ5	70000	1024 X 1024	70000
Flowers6	-8l89	varies	8l89
Grumpy-Cat7	100-	256 X 256	100
Imagenet8	1281137	varies	-1281 137-
-LSUN Bedroom9	3 033 042	256 X 256	-1000000-
LSUNCat9	1657266	256 X 256	-1000000-
LSUN ChUrch9	126227	256 X 256	126227
LSUNDOg9	5054817	256 X 256	-1000000-
-Satellite-Buildings* 5 * 7 8 9 10- 11 12 13	280741	300 X 300	280741
Satellite-Landscapes11	2608	1800 X 1200	2608
Simpsons12	41866	varies	41866
BreCaHAD13	3 253	256 X 256	3 253 —
Table 5: Datasets information.
Learning curves
On Figure 9 and Figure 10 we present the learning curves from Table 2 in the main text. To make
the plots readable, for each target dataset, we report only the curves corresponding to training from
scratch, training from the Imagenet checkpoint, and from two checkpoints that perform best among
the rest as a representative subset of sources.
Synthetic data details
Here we provide the details for the experiment described in Section 2.2. The synthetic target data is
formed by 10 Gaussians with centers on the circle of radius 20 and σ = 0.25. Source-I (blue) is a
distribution formed as a sum of a uniform distribution on a zero-centered circle of a radius 20 and
the zero-centered Gaussians with σ = 4. Source-II (green) is formed by 3 Gaussians with centers
that coincide with the consequent centers of three Gaussians of the original data and σ = 0.5. We
use the standard GAN loss (Goodfellow et al., 2014) and perform 5000 generator training steps
with 4 discriminator steps for every generator step. We use batch size 64 and Adam optimizers
with learning rate 0.0002 and β1, β2 = 0.5, 0.999. The generator has a 64-dimensional latent space
4https://www.cs.toronto.edu/~kriz/cifar.html
5https://github.com/NVlabs/ffhq-dataset
6https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html
7https://hanlab.mit.edu/projects/data-efficient-gans/datasets/
8https://image-net.org/index.php
9https://www.yf.io/p/lsun
10https://www.aicrowd.com/challenges/mapping-challenge-old
11https://earthview.withgoogle.com
12https://www.kaggle.com/c/cmx-simpsons/data
13https://figshare.com/articles/dataset/BreCaHAD_A_Dataset_for_Breast_
Cancer_Histopathological_Annotation_and_Diagnosis/7379186
12
Published as a conference paper at ICLR 2022
BreCaHAD Simpsons Grumpy Cat Flowers CIFAR-10 S.Landscapes S.Buildings L.Dog L.Church L.Cat L.Bedroom FFHQ Imagenet
Figure 8: Samples for each of the target and source datasets.
13
Published as a conference paper at ICLR 2022
I F I L.B I L.Ca ∣ L.Ch ∣ L.Dog ∣ S.B ∣ S.L ∣ I ∣
F	0	244.0	194.8	240.5	178.8	256.1	233.3	150.8
L.B	244.0	-0-	165.0	182.8	162.4	233.3	236.7	143.4
L.Ca	194.8	165.0	-0-	200.8	97.6	206.9	185.9	104.1
L.Ch	240.5	182.8	200.8	-0-	167.0	199.8	232.5	140.4
L.Dog	178.8	162.4	97.6	167.0	0-	200.0	182.3	63.9
S.B	256.1	233.3	206.9	199.8	200.0	-0-	172.2	177.5
S.L	233.3	236.7	185.9	232.5	182.3	172.2	-0-	145.3
C	197.2	188.1	120.9	192.3	102.2	202.1	185.3	85.4
Fl	257.7	254.7	235.4	243.8	215.9	285.4	261.4	192.8
GC	293.1	260.8	188.4	259.2	259.3	341.4	334.5	264.4
S	252.5	225.2	199.4	218.8	195.9	217.7	244.3	167.6
BCH	347.8	345.7	319.7	356.0	303.8	351.2	245.4	280.4
Table 6: FID distances between source and target datasets. Underlined cell in a row corresponds
to a source domain that is closest to a fixed target. Datasets names are shortened as: L.Bdr
(LSUN Bedroom), L.Cat (LSUN Cat), L.Chr (LSUN Church), L.Dog (LSUN Dog), S.Bld (Satel-
lite Buildings), S.Lnd (Satellite Landscapes), Imgn (Imagenet), C-10 (CIFAR-10), Flw (Flowers),
GC (Grumpy Cat), S (Simpsons), BCH (BreCaHAD).
I F I L.B I L.Ca ∣ L.Ch ∣ L.Dog ∣ S.B ∣ S.L ∣ I ∣
F	0	0.237	0.169	0.213	0.116	0.230	0.165	0.116
L.B	0.237	-0-	0.161	0.193	0.124	0.249	0.200	0.126
L.Ca	0.168	0.161	0^^	0.185	0.080	0.189	0.129	0.105
L.Ch	0.213	0.193	0.185	-0-	0.114	0.202	0.185	0.096
L.Dog	0.116	0.125	0.079	0.113	0-	0.155	0.095	0.027
S.B	0.229	0.248	0.189	0.202	0.156	-0-	0.129	0.179
S.L	0.165	0.200	0.130	0.185	0.095	0.129	-0-	0.109
C	0.137	0.149	0.092	0.144	0.048	0.170	0.117	0.060
Fl	0.227	0.260	0.211	0.230	0.157	0.277	0.212	0.153
GC	0.260	0.283	0.113	0.276	0.196	0.332	0.249	0.195
S	0.265	0.276	0.215	0.244	0.178	0.247	0.227	0.179
BCH	0.335	0.374	0.316	0.375	0.267	0.349	0.205	0.273
Table 7: KID distances between source and target datasets computed. Highlighted cell in a row
corresponds to a source domain that is closest to a fixed target.
and consists of six consequent linear layers, all but the last followed by batch-norms and ReLU-
activations. The intermediate layers’ sizes are 64, 128, 128, 128, 64. The discriminator is formed by
a sequence of five linear layers, each but the last followed by the ReLU-activation. The intermediate
layers’ sizes are 64, 128, 128, 64.
The starting checkpoints for the Dissecting Contributions experiments are taken from the interme-
diate checkpoints of the GAN training for Source-I. We take every 50-th checkpoint, gathering 100
in total. We perform fine-tuning to the target distribution with the same parameters as above except
the number of steps equals 1000.
Longer training
In this series of experiments, we run GAN training for a source checkpoint being either Imagenet-
pretrained or randomly initialized for two times higher number of steps (50 million real images
shown to the discriminator). The results are presented in Table 10. Generally, Imagenet-pretraining
almost always either improves GAN quality or performs equally to the random initialization while
speeding up convergence by a large margin.
14
Published as a conference paper at ICLR 2022
I F		LB	L.Ca	L.Ch	L.Dog	S.B	S.L	I
F	1	0.000	0.014	0.000	0.057	0.001	0.000	0.005
L.B	0.333	1-	0.333	0.235	0.337	0.307	0.058	0.021
L.Ca	0.448	0.598	1 ^^	0.253	0.384	0.619	0.229	0.094
L.Ch	0.027	0.050	0.007	-1-	0.058	0.208	0.016	0.003
L.Dog	0.539	0.679	0.591	0.350	1-	0.726	0.265	0.144
S.B	0.000	0.000	0.000	0.000	0.000	1	0.000	0.000
S.L	0.007	0.014	0.007	0.042	0.002	0.705	-1-	0.016
C	0.000	0.000	0.000	0.000	0.000	0.001	0.000	0.000
Fl	0.006	0.000	0.001	0.000	0.000	0.002	0.012	0.003
GC	0.000	0.000	0.001	0.000	0.000	0.000	0.000	0.000
S	0.000	0.000	0.000	0.012	0.000	0.046	0.000	0.000
BCH	0.000	0.000	0.000	0.000	0.000	0.000	0.000	0.000
Table 8: The Precision values computed for the targets datasets w.r.t. the source datasets.
	F	L.B	L.Ca	L.Ch	L.Dog	S.B	S.L	I
F	1	0.333	0.448	0.027	0.539	0.000	0.007	0.737
L.B	0.000	-1-	0.598	0.050	0.679	0.000	0.014	0.124
L.Ca	0.014	0.333	1-	0.007	0.591	0.000	0.007	0.218
L.Ch	0.000	0.235	0.253	-1-	0.350	0.000	0.042	0.303
L.Dog	0.057	0.337	0.384	0.058	1-	0.000	0.002	0.325
S.B	0.001	0.307	0.619	0.208	0.726	-1-	0.705	0.533
S.L	0.000	0.058	0.229	0.016	0.265	0.000	-1-	0.378
C	0.001	0.053	0.240	0.006	0.340	0.000	0.003	0.718
Fl	0.001	0.183	0.249	0.010	0.410	0.000	0.017	0.708
GC	0.000	0.020	0.790	0.000	0.970	0.000	0.000	0.000
S	0.013	0.324	0.328	0.060	0.379	0.000	0.045	0.294
BCH	0.001	0.203	0.428	0.053	0.546	0.006	0.553	0.789
Table 9: The Recall values computed for the targets datasets w.r.t. the source datasets.
15
Published as a conference paper at ICLR 2022
Dataset	From Scratch				Imagenet pretraining			
	Step (M)	FID	Precision	Recall	Step (M)	FID	Precision	Recall
L.Bedroom	50	2.50	-0.663-	0.485	50	~T3~	-0.691-	0.483
L.Cat	一	42	6.87	-0.686-	0.394	48	^63^	-07Γ2-	0.385
L.Church	36	3.01	-0.705-	0.547	12	ʒɪ	-0.693-	0.523
L.Dog	40	12.7	-0751-	0.384	45	TΣ8^	-0753-	0.382
S.Buildings	35	11.9	-0.363-	0.498	14	10.9-	-0.304-	0.591
S.Landscapes	25	27.4	0.737	0.214	1	ɪr	0.721	0.393
Table 10: Number of real images shown to the discriminator (step) for the checkpoint with the best
FID value, this value and corresponding precison and recall values for long-term trainings with two
initialization options.
Source Model ∣ FID ∣ Precision ∣ Recall ∣ Steps to Convergance
Imagenet	8.31	0.77	0.28	9
Imagenet (half)	-854^	-0:81-	0.22	25
FFHQ	-9:47-	-079-	0.25	22
FFHQ (half)	^9^	-077-	0.27	25
Table 11: Finetuning to Flowers from a converged source checkpoint and from a checkpoint that
passes two times fewer steps.
Transfer from an earlier epoch
This experiment verifies if it is important to transfer from a well-converged nearly-optimal source
checkpoint, or it is sufficient to start from a roughly stabilized checkpoint from the intermediate step
of the optimization process. To address the question, we perform a series of additional experiments
with Imagenet and FFHQ as source domains, and Flowers as a target domain. As pretrained check-
points, we consider the best-FID checkpoint and a checkpoint that passed two times fewer steps. The
results for these runs are presented in Table 11. Overall, the choice between two options has only a
marginal impact on the transfer quality, and one can use the source checkpoint from the middle of
training to initialize the finetuning process.
Details of experiments on the GAN inversion
We take the e4e generator inversion approach proposed by (Tov et al., 2021) and train an encoder that
maps real data to the GAN latent space. This scheme is known to be capable of mapping real images
to the GAN latent space preserving all generator properties such as latent attributes manipulations.
We follow the original author’s implementation and train an independent encoder model for each
generator. For a generator G we receive an encoder E which is trained to satisfy G(E(x))=x
for each real data sample x. We evaluate the encoders with the average LPIPS-distance (Zhang
et al., 2018) between a test set real samples and their inversions equal Eχ~ptestLPIPS(x, G(E(X))).
We also report the average distance between an original image and its reconstruction features of a
pretrained features extractor F which is equal Eχ~ptest∣∣F(x) - F(G(E(X)))∣∣2. The lower these
quantities -, the better reconstruction quality is. Following (Tov et al., 2021), for FFHQ-target
generators, we train the encoder on the FFHQ dataset and evaluate it on the Celeba-HQ dataset and
on FFHQ itself. As for LSUN-Bedroom, we split the original data into a train and a test subset in
the proportion 9 : 1 and train e4e on the train set and evaluate on the test set. As the feature extractor
F, for FFHQ we use a Face-ID pretrained model, same as in (Tov et al., 2021), and MoCo-v2 (Chen
et al., 2020b) model for LSUN-Bedroom.
16
Published as a conference paper at ICLR 2022
FID	FID	an	FID	an	FID
5	10	15	20
1M images
FFHQ
.8: ∙614∙ :2:-5
I I I I I
Ooooo
UolSIEd
0.8：
7± 06
CC
υ -
OJ
N 0.4：
5	10	15	20
1M images
LSUN-Bedroom
25 0.00 ' ' ' ' 5 ' ' ' '10' ' ' '15' ' ' '20' ' ' 25
1M images
25
■0	5	10	15	20	25	0	5	10	15	20	25	0	5	10	15	20	25
1M images	1M images	1M images
LSUN-Cat
0	5	10	15	20	25	0	5	10	15	20	25	0	5	10	15	20	25
1M images	1M images	1M images
5	10	15	20
1M images
0	5	10	15	20	25	0
1M images
LSUN-ChUrCh
10	15	20
1M images
LSUN-Dog
=3①N
5
2
5	10	15	20	25
1M images
O :8:6 ∙.84∙∙2∙
Ioooo
5	10	15	20	25	0	5	10	15	20	25
1M images	1M images
Satellite-Buildings
0	5	10	15	20	25	0	5	10	15	20	25	0	5	10	15	20	25
1M images	1M images	1M images
----From Scratch
----Imagenet
----FFHQ
----LSUN-Church
----LSUN-Cat
---LSUN-Dog
Figure 9: Learning curves for different target and sources datasets, part 1.
17
Published as a conference paper at ICLR 2022
FID	FID	FID	FID	FID	FID
15-
6 2 8 4
3 3 2 2
200	5	10	15	20	25
1M images
8- - -6- - -4- - -2
I I I I
OOOO
UolSIEd
3 19 7
Il
50	5	10	15	20	25
1M images
8 4 2
Oo O
UolSP①Jd
30
16
Satenite-LandSCaPeS
0.00	5	10	15	20	25
1M images
CIFAR-10
1.0
OOOO
1.0
0.00 ' ' ' ' 5 ' ' ' '10' ' ' '15' ' ' '20' ' ' 25
1M images
∙8∙64 2
OOOO
=3①N
0.00	5	10	15	20	25
1M images
Flowers
0.00 ' ' ' ' 5 ' ' ' 10' ' ' '15' ' ' '20' ' ' 25
1M images
5	10	15	20
1M images
8- - -6- - -4- - -2.
I I I I
OOOO
UolSP①Jd
6 2 8 4
2 2 11
8 6 4 2
I I I I
OOOO
UolSIEd
Grumpy-Cat
5
10	15	20
1M images
1.0
∙8∙64 2
OOOO
=3①N
∙8∙64
OOO
=3①N
10	15	20	25
1M images
0.2-
-0.θ'，，一 户..........・
25	0	5	10	15	20	25
1M images
4 2 0 8
111
8 6 4 2
I I I I
OOOO
UolSP①Jd
60	5	10	15	20	25
1M images
40
8 6 4 2
I I I I
OOOO
UolSP①Jd
1.0
SimPSonS
0.00	5	10	15	20	25
1M images
BreCaHAD
∙8864
OOO
=3①N
0	5	10	15	20	25	0	5	10	15	20	25
1M images	1M images
----From Scratch
----Imagenet
----Satellite-Landscapes
----LSUN-Bedroom
1.0
∙8∙64 2
OOOO
0.00 ' ' ' ' 5 ' ' ' 10' ' ' '15' ' ' '20' ' ' 25
1M images
0	5	10	15	20	25
1M images
----FFHQ
----LSUN-Cat
----LSUN-Dog
Figure 10: Learning curves for different target and sources datasets, part 2.
18