Published as a conference paper at ICLR 2022
Learning curves for continual learning in
neural networks: Self-knowledge transfer
AND FORGETTING
Ryo Karakida & Shotaro Akaho
National Institute of Advanced Industrial Science and Technology (AIST), Japan
{karakida.ryo,s.akaho}@aist.go.jp
Ab stract
Sequential training from task to task is becoming one of the major objects in deep
learning applications such as continual learning and transfer learning. Nevertheless,
it remains unclear under what conditions the trained model’s performance improves
or deteriorates. To deepen our understanding of sequential training, this study
provides a theoretical analysis of generalization performance in a solvable case of
continual learning. We consider neural networks in the neural tangent kernel (NTK)
regime that continually learn target functions from task to task, and investigate
the generalization by using an established statistical mechanical analysis of kernel
ridge-less regression. We first show characteristic transitions from positive to
negative transfer. More similar targets above a specific critical value can achieve
positive knowledge transfer for the subsequent task while catastrophic forgetting
occurs even with very similar targets. Next, we investigate a variant of continual
learning which supposes the same target function in multiple tasks. Even for the
same target, the trained model shows some transfer and forgetting depending on the
sample size of each task. We can guarantee that the generalization error monotoni-
cally decreases from task to task for equal sample sizes while unbalanced sample
sizes deteriorate the generalization. We respectively refer to these improvement and
deterioration as self-knowledge transfer and forgetting, and empirically confirm
them in realistic training of deep neural networks as well.
1	Introduction
As deep learning develops for a single task, it enables us to work on more complicated learning frame-
works where the model is sequentially trained on multiple tasks, e.g., transfer learning, curriculum
learning, and continual learning. Continual learning deals with the situation in which the learning
machine cannot access previous data due to memory constraints or privacy reasons. It has attracted
much attention due to the demand on applications, and fundamental understanding and algorithms
are being explored (Hadsell et al., 2020). One well-known phenomenon is catastrophic forgetting;
when a network is trained between different tasks, naive training cannot maintain performance on the
previous task (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017).
It remains unclear in most cases under what conditions a trained model’s performance improves
or deteriorates in sequential training. Understanding its generalization performance is still limited
(Pentina & Lampert, 2014; Bennani et al., 2020; Lee et al., 2021). For single-task training, however,
many empirical and theoretical studies have succeeded in characterizing generalization performance
in over-parameterized neural networks and given quantitative evaluation on sample-size dependencies,
e.g., double descent (Nakkiran et al., 2020). For further development, it will be helpful to extend the
analyses on single-task training to sequential training on multiple tasks and give theoretical backing.
To deepen our understanding of sequential training, this study shows a theoretical analysis of its
generalization error in the neural tangent kernel (NTK) regime. In more details, we consider the
NTK formulation of continual learning proposed by Bennani et al. (2020); Doan et al. (2021). By
extending a statistical mechanical analysis of kernel ridge-less regression, we investigate learning
curves, i.e., the dependence of generalization error on sample size or number of tasks. The analysis
1
Published as a conference paper at ICLR 2022
focuses on the continual learning with explicit task boundaries. The model learns data generated
by similar teacher functions, which we call target functions, from one task to another. All input
samples are generated from the same distribution in an i.i.d. manner, and each task has output samples
(labels) generated by its own target function. Our main contributions are summarized as follows.
First, we revealed characteristic transitions from negative to positive transfer depending on the target
similarity. More similar targets above a specific critical value can achieve positive knowledge transfer
(i.e., better prediction on the subsequent task than training without the first task). Compared to
this, backward transfer (i.e., prediction on the previous task) has a large critical value, and subtle
dissimilarity between targets causes negative transfer. The error can rapidly increase, which clarifies
that catastrophic forgetting is literally catastrophic (Section 4.1).
Second, we considered a variant of continual learning, that is, learning of the same target function
in multiple tasks. Even for the same target function, the trained model’s performance improves
or deteriorates in a non-monotonic way. This depends on the sample size of each task; for equal
sample sizes, we can guarantee that the generalization error monotonically decreases from task to task
(Section 4.2 for two tasks & Section 5 for more tasks). Unbalanced sample sizes, however, deteriorates
generalization (Section 4.3). We refer to these improvement and deterioration of generalization as
self-knowledge transfer and forgetting, respectively. Finally, we empirically confirmed that self-
knowledge transfer and forgetting actually appear in the realistic training of multi-layer perceptron
(MLP) and ResNet-18 (Section 5.1). Thus, this study sheds light on fundamental understanding and
the universal behavior of sequential training in over-parameterized learning machines.
2	Related work
Method of analysis. As an analysis tool, we use the replica method originally developed for statistical
mechanics. Statistical mechanical analysis enables us typical-case evaluation, that is, the average
evaluation over data samples or parameter configurations. It sometimes provides us with novel insight
into what the worst-case evaluation has not captured (Abbaras et al., 2020; Spigler et al., 2020).
The replica method for kernel methods has been developed in Dietrich et al. (1999), and recently
in Bordelon et al. (2020); Canatar et al. (2021). These recent works showed excellent agreement
between theory and experiments on NTK regression, which enables us to quantitatively understand
sample-size dependencies including implicit spectral bias, double descent, and multiple descent.
Continual learning. Continual learning dynamics in the NTK regime was first formulated by Bennani
et al. (2020); Doan et al. (2021), though the evaluation of generalization remains unclear. They derived
an upper bound of generalization via the Rademacher complexity, but it includes naive summation
over single tasks and seems conservative. In contrast, our typical-case evaluation enables us to newly
find such rich behaviors as negative/positive transfer and self-knowledge transfer/forgetting. The
continual learning in the NTK regime belongs to so-called single-head setting (Farquhar & Gal, 2018),
and it allows the model to revisit the previous classes (target functions) in subsequent tasks. This is
complementary to earlier studies on incremental learning of new classes and its catastrophic forgetting
(Ramasesh et al., 2020; Lee et al., 2021), where each task includes different classes and does not allow
the revisit. Note that the basic concept of continual learning is not limited to incremental learning
but allows the revisit (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017). Under the limited data
acquisition or resources of memory, we often need to train the same model with the same targets (but
different samples) from task to task. Therefore, the setting allowing the revisit seems reasonable.
3	Preliminaries
3.1	Neural tangent kernel regime
We summarize conventional settings of the NTK regime (Jacot et al., 2018; Lee et al., 2019). Let us
consider a fully connected neural network f = uL given by
Ul = σwWιhi-i/ JM- + σbbι, hi = φ(uι) (l = 1,…，L),	(1)
where we define weight matrices Wl ∈ RMl ×Ml-1, bias terms bl ∈ RMl, and their variances σw2
and σb2. We set random Gaussian initialization Wl,j, bl,i 〜N (0,1) and focus on the mean squared
error loss: L(θ) = P∖ι kyμ - f (χμ)k2, where the training samples {χμ, yμ}jN=ι are composed of
2
Published as a conference paper at ICLR 2022
inputs χμ ∈ RD normalized by ∣∣χμ∣∣2 = 1 and labels yμ ∈ RC. The set of all parameters is denoted
as θ, and the number of training samples is N . Assume the infinite-width limit for hidden layers
(Ml → ∞), finite sample size and depth. The gradient descent dynamics with a certain learning rate
then converges to a global minimum sufficiently close to the random initialization. This is known as
the NTK regime, and the trained model is explicitly obtained as
f(c)(x0) = f0(c)(x0) + Θ(x0, X)Θ(X)-1(y(c) -f0(c)(X))	(c= 1,..., C).	(2)
We denote the NTK matrix as Θ, arbitrary input samples (including test samples) as x0, and the set
of training samples as X . The indices of f mean 0 for the model at initialization and c for the head
of the network. Entries of NTK Θ(x0,x) are defined by Vθ fo(χ0)Vθ fo(χ)>. We write Θ(X) as an
abbreviation for Θ(X, X). The trained network is equivalent to a linearized model around random
initialization (Lee et al., 2019), that is, f(c) = f0(c) + Vθf0(c)∆θ with
C
∆θ=θ-θ0=XVθf0(c)(X)>Θ(X)-1(y(c) -f0(c)(X)).	(3)
c=1
While over-parameterized models have many global minima, the NTK dynamics implicitly select
the above θ, which corresponds to the L2 min-norm solution. Usually, we ignore f0 by taking the
average over random initialization. The trained model (2) is then equivalent to the kernel ridge-less
regression (KRR).
NTK regime also holds in various architectures including ResNets and CNNs (Yang & Littwin,
2021), and the difference only appears in the NTK matrix. Although we focus on the fully connected
network in synthetic experiments, the following NTK formulation of sequential training and our
theoretical results hold in any architecture under the NTK regime.
NTK formulation of Continual learning. We denote the set of training samples in the n-th task as
(Xn, yn) (n = 1, 2, ..., K), a model trained in a sequential manner from task 1 to task n as fn and its
parameters as θn. That is, we train the network initialized at θn-1 for the n-th task and obtain fn .
Assume that the number of tasks K is finite. The trained model within the NTK regime is then given
as follows (Bennani et al., 2020; Doan et al., 2021):
fn(x0) =fn-1(x0)+Θ(x0,Xn)Θ(Xn)-1(yn-fn-1(Xn)),	(4)
θn - θn-1 = Vθf0(Xn)>Θ(Xn)-1 (yn - fn-1 (Xn)).	(5)
We omit the index c because each head is updated independently. The model fn completely fits
the n-th task, i.e., yn = fn (Xn). The main purpose of this study is to analyze the generalization
performance of the sequentially trained model (4). At each task, the model has an inductive bias of
KRR in the function space and L2 min-norm solution in the parameter space. The next task uses
this inductive bias as the initialization of training. The problem is whether this inductive bias helps
improve the generalization in the subsequent tasks.
Remark (independence between different heads). For a more accurate understanding of the
continual learning in the NTK regime, it may be helpful to remark on the heads’ independence, which
previous studies did not explicitly mention. As in Eq. (2), all heads share the same NTK, and f(c)
depends only on the label of its class y(c). While the parameter update (3) includes information of all
classes, the c-th head can access only the information of the c-th class1. For example, suppose that
the n-th task includes all classes except 1, i.e., {2, ..., C}. Then, the model update on the the class 1
at the n-th task, i.e., fn1 - fn1-1, becomes
Vθf0(1)(x0)(θn-θn-1) =Vθf0(1)(x0)XVθf0(c)(Xn)>Θ(Xn)-1(yn(c) - fn(c-)1) =0
c=2
because Vθf0(c)Vθf0(c0)> = 0 (c 6= c0) in the infinite-width limit (Jacot et al., 2018; Yang, 2019).
Thus, we can deal with each head independently and analyze the generalization by setting C = 1
without loss of generality. This indicates that in the NTK regime, interaction between different
heads do not cause knowledge transfer and forgetting. One may wonder if there are any non-trivial
knowledge transfer and forgetting in such a regime. Contrary to such intuition, we reveal that when
the subsequent task revisits previous classes (targets), the generalization shows interesting increase
and decrease.
1We use the term “class”, although the regression problem is assumed in NTK theory. Usually, NTK studies
solve the classification problem by regression with a target y(c) = {0, 1}.
3
Published as a conference paper at ICLR 2022
3.2 Learning curve on single-task training
To evaluate the generalization performance of (4), we extend the following theory to our sequential
training. Bordelon et al. (2020) obtained an analytical expression of the generalization for NTK
regression on a single task (2) as follows. Assume that training samples are generated in an i.i.d.
manner (xμ 〜p(χ)) and that labels are generated from a square integrable target function f:
∞
f(x) = X Wiψi(x),	yμ = f(xμ) + εμ	(μ =1,...,N),	(6)
i=0
where Wi are constant coefficients and ε represents Gaussian noise with (εμεν)= δμνσ2. We define
ψi(x) := √ηiφi(χ) with basis functions φi(χ) given by Mercer,s decomposition:
dx0p (x0) Θ (x, x0) φi (x0) = ηiφi(x)	(i = 0, 1, . . . , ∞).	(7)
Here, ηi denotes NTK’s eigenvalue and we assume the finite trance of NTK i ηi < ∞. We
set η0 = 0 in the main text to avoid complicated notations. We can numerically compute
eigenvalues by using the Gauss-Gegenbauer quadrature. Generalization error is expressed by
(Rdxp(X) f(x) - f* lx))2)。
E1
where f* is a trained model andh•••)D is the average
over training samples. Bordelon et al. (2020) derived a typical-case evaluation of the generalization
error by using the replica method: for a sufficiently large N, we have asymptotically
∞2
E1 = ⅛ XηiW2 (F) +⅛σ2.
(8)
Although the replica method takes a large sample size N, Bordelon et al. (2020); Canatar et al. (2021)
reported that the analytical expression (8) coincides well with empirical results even for small N.
The constants κand γ are defined as follows and characterize the increase and decrease of E1:
1
∞
X
i=0
ηi
K + Nηi，
(9)
Y=X ⅛.
The κ is a positive solution of the first equation and obtained by numerical computation. The γ
satisfies 0 < γ < 1 by definition. For N = αDl (α > 0, l ∈ N, D 1), we can analytically solve it
and obtain more detailed evaluation. For example, a positive κ decreases to zero as the sample size
increases and E1 also decreases for σ2 = 0. For σ2 > 0, the generalization error shows multiple
descent depending on the decay of eigenvalue spectra. Since multiple descent is not a main topic of
this paper, we briefly summarize it in Section A.5 of the Supplementary Materials.
4 Learning Curves between two tasks
In this section, we analyze the NTK formulation of continual learning (4) between two tasks (K = 2).
One can also regard this setting as transfer learning. We sequentially train the model from task A to
task B, and each one has a target function defined by
fA(x) = X WA,iΨi(x), fB (x) = X WB,iΨi(x), [WA,i,WB,i]〜N(0,ηi P P ).	(10)
ii
The target functions are dependent on each other and belong to the reproducing kernel Hilbert
space (RKHS). By denoting the RKHS by H, one can interpret the target similarity P as the inner
product hfA,fB iH∕(kfA∣∣H∣∣fB IlH) = ρ. These targets have dual representation such as f(x)=
Pi αiΘ(x0i, x) with i.i.d. Gaussian variables αi (Bordelon et al., 2020), as summarized in Section
E.1. We generate training samples by yA = fA(xA) + ε1 (μ = 1,…,NA) and yB = f(xB) +
εfB	(μ	=	1,...,	NB), although we focus on the noise-less	case (σ = 0) in this	section.	Input samples
xA	and xB	are i.i.d. and generated by the same distribution p(x). We can measure	the	generalization
error in two ways: generalization error on subsequent task B and that on previous task A:
Ea→B (p) =( / dxp(x)(fB (x)	- fA→B (x))2),	(11)
Ea→b (p) = ( d dxp(x)(fA(x)	- fA→B (x))2∖ ,	(12)
4
Published as a conference paper at ICLR 2022
where we write f2 as fA→B to emphasize the sequential training from A to B. The notation EAba→ckB (ρ)
is referred to as backward transfer (Lopez-Paz & Ranzato, 2017). Large negative backward transfer
is known as catastrophic forgetting. We take the average(…)over training samples of two tasks
{Da, DB }, and target coefficients w. In fact, we can set W as constants, and it is unnecessary to take
the average. To avoid complicated notation, we take the average in the main text. We then obtain the
following result.
Theorem 1. Using the replica method under sufficiently large NA and NB, for σ = 0, we have
q2
Ea→b(P) = E 2(1 - ρ)(1 - qA,i) +	EB,i,	(13)
i	1 - γA
q2
EA→B(P) = E 2(1 - ρ)(1 + FγB (qA,i,qB,i))η2 + ^A^Eb" ,	(14)
i	1 - γA
where we define qA,i = ka∕(ka + NAni), qB,i = KB/(kb + NB6)，Eb~ = qB M2∕(1 — YB) and
FγB (a, b) =b(a-2)+b2(1 - a)/(1 -γB).
The constants κA and γA (κB and γB, respectively) are given by setting N = NA(NB) in (9). The
detailed derivation is given in Section A. Technically speaking, we use the following lemma:
Lemma 2. Denote the trained model (2) on single task A as fa = Ei WA,%ψi, and define the
following cost function: E =(Ei φi(wA ,i 一 Ui))。 for arbitrary constants φi and Ui. Using the
replica method under a sufficiently large NA, we have
E=
i
卜WA,i — Ui)2 一 2WA,i(WA,i — Ui)qA,i +
{wA,i + "；2 A (EA + σ2)} qA,i
φi
where EA denotes generalization error E1 on single task A.
For example, we can see that EA is a special case of E with φi = n and U = Wa, and that Ea→b (ρ)
is reduced to φi = qB”2/(1 — YB) and U = WB after certain calculation.
Spectral Bias. The generalization errors obtained in Theorem 1 are given by the summation over
spectral modes like the single-task case. The EB,i corresponds to the i-th mode of generalization
error (8) on single task B. The study on the single task (Bordelon et al., 2020) revealed that as the
sample size increases, the modes of large eigenvalues decreases first. This is known as spectral
bias and clarifies the inductive bias of the NTK regression. Put the eigenvalues in descending order,
i.e., λi ≥ λi+1. When D is sufficiently large and NB = αDl (α > 0, l ∈ N), the error of each
mode is asymptotically given by EB,i = 0 (i < l) and ηi2 (i > l). We see that the spectral bias also
holds in EA→B because it is a weighted sum over EB,i. In contrast, EAba→ckB includes a constant term
2(1 — P)ηi2. This constant term causes catastrophic forgetting, as we show later.
4.1	Transition from negative to positive transfer
We now look at more detailed behaviors of generalization errors obtained in Theorem 1. We first
discuss the role of the target similarity P for improving generalization. Figure 1(a) shows the
comparison of the generalization between single-task training and sequential training. Solid lines
show theory, and markers show experimental results of trained neural networks in the NTK regime.
We trained the model (1) with ReLU activation, L = 3, and Ml = 4, 000 by using the gradient descent
over 50 trials. More detailed settings of our experiments are summarized in Section E. Because
we set NA = NB = 100, we have EA = EB . The point is that both EA→B (P) and EAba→ckB (P) are
lower than EA(= EB) for large P. This means that the sequential training degrades generalization if
the targets are dissimilar, that is, negative transfer. In particular, EAba→ckB (P) rapidly deteriorates for
the dissimilarity of targets. Note that both EA→B and EAba→ckB are linear functions of P. Figure 1(a)
indicates that the latter has a large slope. We can gain quantitative insight into the critical value of P
for the negative transfer as follows.
Knowledge transfer. The following asymptotic equation gives us the critical value for EA→B :
EA→B (P)/EB 〜2(I- P)	for NA》NB.	(15)
5
Published as a conference paper at ICLR 2022
Figure 1: (a) Transitions from positive to negative transfer caused by target similarity ρ. We set
NA = NB . (b) Learning curves show negative transfer (EA→B /EB) in a highly non-linear way
depending on unbalanced sample sizes. We changed NA and set NB = 103.
A straightforward algebra leads to this (Section A.3). In the context of transfer learning, it is
reasonable that the target domain has a limited sample size compared to the first task. For ρ > 1/2, we
have EA→B < EB , that is, previous task A contributes to improving the generalization performance
on subsequent task B (i.e., positive transfer). For ρ < 1/2, however, negative transfer appears.
The following sufficient condition for the negative transfer is also noteworthy. By evaluating EA→B >
EB, we can prove that for any NA and NB, the negative transfer always appears for
ρ < ρ* := √γAA1 + √γA).	(16)
This is just a sufficient condition and may be loose. For example, we asymptotically have the critical
target similarity P = 1/2 > ρ* for NA》NB. Nevertheless, this sufficient condition is attractive in
the sense that it clarifies the unavoidable negative transfer for the small target similarity.
Backward transfer. The EAba→ckB(0) includes the constant term Pi ηi2 independent of sample sizes.
Note that qA and qB decrease to zero for large sample sizes (Bordelon et al., 2020), and we have
EAba→ckB(P)
〜Pi η2(1 一 ρ). In contrast, EA converges to zero for a large NA. Therefore, the
intersection between EAba→ckB(P) and EA reach P = 1 as NA and NB increase. This means that when
we have a sufficient number of training samples, negative backward transfer (EAba→ckB(P) > EA) occurs
even for very similar targets. Figure 1(a) confirms this result, and Figure 6 in Section A.6 shows more
detailed learning curves of backward transfer. Catastrophic forgetting seems literally “catastrophic”
in the sense that the backward transfer rapidly deteriorates by the subtle target dissimilarity.
4.2	Self-knowledge transfer
We have shown that target similarity is a key factor for knowledge transfer. We reveal that the sample
size is another key factor. To clarify the role of sample sizes, we focus on the same target function
(P = 1) in the following analysis. We refer to the knowledge transfer in this case as self-knowledge
transfer to emphasize the network learning the same function by the same head. As is the same in
P < 1, the knowledge obtained in the previous task is transferred as the network’s initialization for
the subsequent training and determines the eventual performance.
Positive transfer by equal sample sizes. We find that positive transfer is guaranteed under equal
sample sizes, that is, NA = NB. To characterize the advantage of the sequential training, we compare
it with a model average: (fA + fB)/2, where fA (fB) means the model obtained by a single-task
training on A (B). Note that since the model is a linear function of the parameter in the NTK regime,
this average is equivalent to that of the trained parameters: (Θa + Θb )/2. After straightforward
calculation in Section D, the generalization error of the model average is given by
Eave = (1 一 γB /2) EB .	(17)
Sequential training and model average include information of both tasks A and B; thus, itis interesting
to compare it with EA→B. We find
Proposition 3. For P = 1 and any NA = NB ,
EA→B (1) < Eave < EA = EB .	(18)
6
Published as a conference paper at ICLR 2022
The derivation is given in Section D. This proposition clarifies the superiority of sequential training
over single-task training and even the average. The first task contributes to the improvement on the
second task; thus, we have positive transfer.
Negative transfer by unbalanced sample sizes. While equal sample size leads to positive transfer,
the following unbalanced sample sizes cause the negative transfer of self-knowledge:
Ea→b ⑴/Eb 〜1/(1 一 YA) for NB》Na.	(19)
The derivation is based on Jensen’s inequality (Section A.3). While EA→B and EB asymptotically
decrease to zero for the large NB, their ratio remains finite. Because 0 < γA < 1, EA→B(1) > EB.
It indicates that the small sample size of task A leads to a bad initialization of subsequent training
and makes the training on task B hard to find a better solution.
Figure 1(b) summarizes the learning curves which depend on sample sizes in a highly non-linear
way. Solid lines show theory, and markers show the results of NTK regression over 100 trials. The
figure shows excellent agreement between theory and experiments. Although we have complicated
transitions from positive to negative transfer, our theoretical analyses capture the basic characteristics
of the learning curves. For self-knowledge transfer (ρ = 1), we can achieve positive transfer at
NA/NB = 1, as shown in Proposition 3, and for large NA/NB, as shown in (15). In contrast,
negative transfer appears for small NA/NB, as shown in (19). For ρ < 1, large NA/NB produces
positive transfer for ρ < 1/2, as shown in (15).
If we set a relatively large σ > 0, the learning curve may become much more complicated due
to multiple descent. Figure 5 in Section A.5 confirms the case in which multiple descent appears
in EA→B . The curve shape is generally characterized by the interaction among target similarity,
self-knowledge transfer depending on sample size, and multiple descent caused by the noise.
4.3	Self-knowledge forgetting
We have shown in Section 4.1 that backward transfer is likely
to cause catastrophic forgetting for ρ < 1. We show that even
for ρ = 1, sequential training causes forgetting. That is, the
training on task B degrades generalization even though both
tasks A and B learn the same target.
We have EAba→ckB (1) = EA→B (1) by definition, and Proposi-
tion 3 tells us that EA→B < EA . Therefore, no forgetting
appears for equal sample sizes. In contrast, we have
Ea→b ⑴/Ea 〜1/(1 - YB) for NA》NB. (20)
One can obtain this asymptotic equation in the same manner
as (19) since EA→B (1) is a symmetric function in terms of
indices A and B. Combining (20) with (15), we have EA <
EA→B (1)	EB. Sequential training is better than using
only task B, but training only on the first task is the best. One
can say that the model forgets the target despite learning the
same one. We call this self-knowledge forgetting. In the
Figure 2: Self-knowledge forget-
ting: unbalanced sample sizes de-
grade generalization.
context of continual learning, many studies have investigated the catastrophic forgetting caused
by different heads (i.e., in the situation of incremental learning). Our results suggest that even the
sequential training on the same task and the same head shows such forgetting. Intuitively speaking,
the self-knowledge forgetting is caused by the limited sample size of task B. Note that we have
EA→B(1) = Pi qA2 ,iEB,i/(1 - YA). The generalization error of single-task training on task B
(EB,i) takes a large value for a small NB and this causes the deterioration of EA→B as well. Figure
2 confirms the self-knowledge forgetting in NTK regression. We set NA = NB as the red line and
NB = 100 as the yellow line. The dashed line shows the point where forgetting appears.
5 Learning curves of many tasks
We can generalize the sequential training between two tasks to that of more tasks. For simplicity, let us
focus on the self-knowledge transfer (ρ = 1) and equal sample sizes. Applying Lemma 2 recursively
from task to task, we can evaluate generalization defined by En =〈/ dxp(x)(f(x) 一 fn(χ))2)D.
7
Published as a conference paper at ICLR 2022
O

y1
(23)
O
yn
Θ (Xn )
Theorem 4. Assume that (i) (Xn, yn) (n = 1,…,K) are given by the same distribution Xn 〜P(X)
and target yn, = Ei Wiψ(Xn) + εn,. (ii) sample SizeS are equal: Nn = N. For n = 1,...,K,
En+1=	二 q>Qn-1q + Rn+1σ2,	Q = diag(q2) +	nt2 qq>	(21)
(1 - γ)2	(1 - γ)κ2
where qi = κ∕(κ + ^N),扇=币定 and diag(q2) denotes a diagonal matrix whose entries are q2.
The noise term Rn is a positive constant. In the noise-less case (σ = 0), the learning curve shows
monotonic decrease: En+1 ≤ En. If all eigenvalues are positive, we have
En+1 < En	(n= 1, 2,...).	(22)
See Section B for details of derivation. The learning curve (i.e., generalization error to the number
of tasks) monotonically decreases for the noise-less case. the monotonic decrease comes from
λmax(Q) < 1. This result means that the self-knowledge is transferred and accumulated from task to
task and contributes in improving generalization. It also ensures that no self-knowledge forgetting
appears. We can also show that Rn converges to a positive constant term for a large n and the
contribution of noise remains as a constant.
KRR-like expression. The main purpose of this work was to address the generalization of the
continually trained model fn . As a side remark, we show another expression of fn :
-Θ(X1)	O
[Θ(x0,X1)…Θ(x0,Xn)]	θ (X2,Xl) θ (X2)
.
.
.
一 Θ(Xn,Xl)…
This easily comes from comparing the update (4) with a formula for the inversion of triangular block
matrices (Section C). One can see this expression as an approximation of KRR, where the upper
triangular block of NTK is set to zero. Usual modification of KRR is diagonal, e.g., L2 regularization
and block approximation, and it seems that the generalization error of this type of model has never
been explored. Our result revealed that this model provides non-trivial sample size dependencies
such as self-knowledge transfer and forgetting.
5.1	Experiments
Although Theorem 4 guarantees the monotonic learning curve for equal sizes, unbalanced sample
sizes should cause a non-monotonic learning curve. We empirically confirmed this below.
Synthetic data. First, we empirically confirm En on synthetic data (10). Figure 3(a1) confirms
the claim of Theorem 4 that the generalization error monotonically decreases for σ = 0 as the
task number increases. Dashed lines are theoretical values calculated using the theorem, and points
with error bars were numerically obtained by fn (4) over 100 trials. For σ > 0, the decrease was
suppressed. We set σ2 = {0, 10-5, 10-4, 10-3} and Ni = 100. Figure 3(a2) shows self-knowledge
forgetting. When the first task has a large sample size, the generalization error by the second task can
increase for small subsequent sample sizes Ni . For smaller Ni , there was a tendency for the error to
keep increasing and taking higher errors than that of the first task during several tasks. In practice,
one may face a situation where the model is initialized by the first task training on many samples and
then trained in a continual learning manner under a memory constraint. The figure suggests that if
the number of subsequent tasks is limited, we need only the training on the first task. If we have a
sufficiently large number of tasks, generalization eventually improves.
MLP on MNIST / ResNet-18 on CIFAR-10. We mainly focus on the theoretical analysis in the
NTK regime, but it will be interesting to investigate whether our results also hold in more practical
settings of deep learning. We trained MLP (fully-connected neural networks with 4 hidden layers)
and ResNet-18 with stochastic gradient descent (SGD) and cross-entropy loss. We set the number of
epochs sufficient for the training error to converge to zero for each task. We confirmed that they show
qualitatively similar results as in the NTK regime. We randomly divided the dataset into tasks without
overlap of training samples. Figures 3(b1,c1) show the monotonic decrease for an equal sample
size and that the noise suppressed the decrease. We set Ni = 500 and generated the noise by the
label corruption with a corruption probability {0, 0.2, ..., 0.8} (Zhang et al., 2017). The vertical axis
means the error, i.e., 1 - (Test accuracy [%])/100. Figures 3(b2,c2) show that unbalanced sample
sizes caused the non-monotonic learning curve, similar to NTK regression.
8
Published as a conference paper at ICLR 2022
Figure 3: (a1)-(c1) Learning curves for equal sample sizes. For noise-less case, they monotonically
decreased (orange lines). For noisy case, decrease was suppressed (grey lines; we plotted learning
curves for several σ2 and those with larger test errors correspond to larger σ2). We trained MLP
on MNIST and ResNet-18 on CIFAR-10. (a2)-(c2) Learning curves for unbalanced sample sizes
were non-monotonic (N1 = 4, 000 for NTK regression, N1 = 104 for SGD training of MLP and
ResNet-18). Numbers in the legend mean Nn (n ≥ 2).
6	Discussion
We provided novel quantitative insight into knowledge transfer and forgetting in the sequential
training of neural networks. Even in the NTK regime, where the model is simple and linearized,
learning curves show rich and non-monotonic behaviors depending on both target similarity and
sample size. In particular, learning on the same target shows successful self-knowledge transfer
or undesirable forgetting depending on the balance of sample sizes. These results indicate that the
performance of the sequentially trained model is more complicated than we thought, but we can still
find some universal laws behind it.
There are other research directions to be explored. While we focused on reporting novel phenomena
on transfer and forgetting, it is also important to develop algorithms to achieve better performance. To
mitigate catastrophic forgetting, previous studies proposed several strategies such as regularization,
parameter isolation, and experience replay (Mirzadeh et al., 2020). Evaluating such strategies with
theoretical backing would be helpful for further development of continual learning. For example,
orthogonal projection methods modify gradient directions (Doan et al., 2021), and replay methods
allow the reuse of the previous samples. We conjecture that these could be analyzed by extending our
calculations in a relatively straightforward way. It would also be interesting to investigate richer but
complicated situations required in practice, such as streaming of non-i.i.d. data and distribution shift
(Aljundi et al., 2019). The current work and other theories in continual learning or transfer learning
basically assume the same input distribution between different tasks (Lee et al., 2021; Tripuraneni
et al., 2020). Extending these to the case with an input distribution shift will be essential for some
applications including domain incremental learning. Our analysis may also be extended to topics
different from sequential training. For example, self-distillation uses trained model’s outputs for the
subsequent training and plays an interesting role of regularization (Mobahi et al., 2020).
While our study provides universal results, which do not depend on specific eigenvalue spectra or
architectures, it is interesting to investigate individual cases. Studies on NTK eigenvalues have made
remarkable progress, covering shallow and deep ReLU neural networks (Geifman et al., 2020; Chen
& Xu, 2020), skip connections (Belfer et al., 2021), and CNNs (Favero et al., 2021). We expect that
our analysis and findings will serve as a foundation for further understanding and development on
theory and experiments of sequential training.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
The main contributions of this work are theoretical claims, and we clearly explained their assumptions
and settings in the main text. Complete proofs of the claims are given in the Supplementary Materials.
For experimental results of training deep neural networks, we used only already-known models and
algorithms implemented in PyTorch. All of the detailed settings, including learning procedures and
hyperparameters, are clearly explained in Section E.
Acknowledgments
This work was funded by JST ACT-X Grant Number JPMJAX190A and JSPS KAKENHI Grant
Number 19K20366.
References
Alia Abbaras, Benjamin Aubin, Florent Krzakala, and Lenka ZdeboroVa Rademacher complexity
and spin glasses: A link between the replica and statistical theories of learning. In Mathematical
and Scienfic Machine Learning (MSML), pp. 27-54. PMLR, 2020.
Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In IEEE/CVF
Conference on ComPUter ViSion and Pattern Recognition (CVPR), pp. 11254-11263, 2019.
YuVal Belfer, Amnon Geifman, MeiraV Galun, and Ronen Basri. Spectral analysis of the neural
tangent kernel for deep residual networks. arXiv PrePrint arXiv:2104.03093, 2021.
Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for continual
learning with orthogonal gradient descent. arXiv PrePrint arXiv:2006.11942, 2020.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves
in kernel regression and wide neural networks. In International Conference on Machine Learning
(ICML), pp. 1024-1034. PMLR, 2020.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. NatUee
communications, 12(1):1-12, 2021.
Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In
International Conference on Learning RePreSentationS (ICLR), 2020.
Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector
networks. PhySical review letters, 82(14):2975, 1999.
Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre Alquier.
A theoretical analysis of catastrophic forgetting through the NTK overlap matrix. In International
Conference on ArtificiaI Intelligence and StatiSticS (AISTATS), pp. 1072-1080. PMLR, 2021.—
Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv PrePrint
arXiv:1805.09733, 2018.
Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
Sionality in convolutional teacher-student scenarios. arXiv PrePrint arXiv:2106.08619, 2021.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the Laplace and neural tangent kernels. In AdVanceS in neural information
ProceSSing SyStemS (NeUrIPS), pp. 1451-1461, 2020.
Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual
learning in deep neural networks. TreedS in cognitive sciences, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In AdVanceS in neural information ProceSSing SyStemS (NeUrIPS),
pp. 8571-8580, 2018.
10
Published as a conference paper at ICLR 2022
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 1l4
(13):3521-3526, 2017.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In
AdvanCeS in neural information processing SyStemS (NeUrIPS), pp. 8572-8583, 2019.
Sebastian Lee, Sebastian Goldt, and Andrew Saxe. Continual learning in the teacher-student setup:
Impact of task similarity. In International COnferenCe on MaChine Learning (ICML), pp. 61θ9-
6119.PMLR, 2021.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
AdvanCeS in Neural InfOrmatiOn PrOCeSSing SyStemS (NIPS), pp. 6467-6476, 2017.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109-165.
Elsevier, 1989.
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh.
Linear mode connectivity in multitask and continual learning. In InternatiOnal COnferenCe on
Learning RePreSentatiOnS (ICLR), 2020.
Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation amplifies regularization in
Hilbert space. In AdvanCeS in NeUral InfOrmatiOn Processing SyStemS (NeUrIPS), pp. 3351-3361,
2020.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In InternatiOnal COnferenCe on Learning
RePreSentatiOnS (ICLR), 2020.
Anastasia Pentina and Christoph Lampert. A PAC-Bayesian bound for lifelong learning. In
InternatiOnal COnferenCe on MaChine Learning (ICML), pp. 991-999. PMLR, 2014.
Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hid-
den representations and task semantics. In InternatiOnal COnferenCe on Learning RePreSentatiOnS
(ICLR), 2020.
Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher-student paradigm. JOUrnaI of StatiStiCaI Mechanics: TheOry and
Experiment, 2020(12):124001, 2020.
Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of
task diversity. In AdvanCeS in NeUral InfOrmatiOn PrOCeSSing SyStemS (NeUrIPS), pp. 7852-7862,
2020.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv PrePrint arXiv:1902.04760,
2019.
Greg Yang and Etai Littwin. Tensor programs IIb: Architectural universality of neural tangent kernel
training dynamics. In InternatiOnal COnferenCe on MaChine Learning (ICML), pp. 11762-11772.
PMLR, 2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International COnferenCe on Learning
RePreSentatiOnS (ICLR), 2017.
11
Published as a conference paper at ICLR 2022
Supplementary Materials
A Sequential training between two tasks
A. 1 Notations
The kernel ridge(-less) regression is given by
1N	1
f =argmm2λ ∑(f (xμ) - yμ) + 2 hf,fiH
(S.1)
where N is the sample size and H is the reproducing kernel Hilbert space (RKHS) induced by the
neural tangent kernel Θ. We assume that the input samples xμ are generated from a probabilistic dis-
tribution p(x) and that the NTK has finite trace, i.e., dxp(x)Θ(x, x) < ∞. Mercer’s decomposition
of Θ is expressed by
dx0p (x0) Θ (x, x0) φi (x0) = ηiφi(x)	(i = 0, 1, . . . , ∞).	(S.2)
In other words, we have Θ(x0, x) = Pi∞=0 ηiφi(x0)φi(x). Note that a function belonging to RKHS
is given by
∞
f(x) = Xaiφi(x)	(S.3)
i=0
With kfkH = P∞=o a2/n < ∞. Then, using the orthonormal bases {φi }i∞=0, the solution of the
regression is given by
f*(X)= Xw*ψi(x), w* = (ΨΨ> + λI) — 1 Ψy	(S.4)
i
where ψi(χ) := √ηiφi(χ). Each column of Ψ is given by φ(χμ) (μ = 1,…,N). We focus on the
NTK regime and take the ridge-less limit (λ → 0).
The sequentially trained model (4) between tasks A and B is written as
fA→B (x) - fA(x) = Θ(x, XB)Θ(XB)-1 (yB - fA(XB)) ,	(S.5)
fA(x) = Θ(x, XA)Θ(XA)-1yA.	(S.6)
The model fA is trained on single task A and represented by
fA (x) = wA*>ψ(x)	(S.7)
with
wA* = lim argminwA HA(wA),	(S.8)
λ→0	A
HA(WA) := ɪ X (w>ψ(XA) - yA)2 + 1 kwAk2.	(S.9)
2λ	2
μ=1
Eq. (S.9 ) is the objective function equivalent to (S.1 ) because hfA, fAiH = kwAk22 * *. Similarly, we
can represent fA→B by the series expansion with the bases of RKHS. Note that the right-hand side of
(S.5 ) is equivalent to the kernel ridge-less regression on input samples XB and labels yB - fA(B).
We have
fA→B(X) = (wA* + wB* )>ψ(X)	(S.10)
with
wB* = lim argminwB H(wB, wA* ),	(S.11)
λ→0	B
1 NB	2	1
H(WB,wA) := χτ E(WBψ(XB)- (yB -WA>ψ(XB))) + xkwBk2.	(S.12)
2λ	2
μ=1
12
Published as a conference paper at ICLR 2022
A.2 Proof of Theorem 1.
A.2.1 KNOWLEDGE TRANSFER EA→B
First, we take the average over training samples of task B conditioned by task A, that is,
EA→B∣A :=(/ dxP(X)(ZB (X)-(WA + WB 了ψ(χ)))	(S.13)
=(/ dxp(x)((wB -(WB - WA))>ψ(x))2,	(S.14)
where the set of task B’s training samples is denoted by DB. We have
EA→B = hEA→B∣AiDA .	(S.15)
Note that the objective function H(WB, WA) is transformed to
1 NB	2	1
H (WB ,wA ) = 2λ X ((WB -(WB - WA ))>ψ(xB ))) + 2 IIwB I∣2.	(S.16)
μ=1
Comparing (S.14 ) and (S.16 ), one can see that the average over task B conditioned by task A is
equivalent to single-task training with the target function (WB - WA)>ψ(x). Therefore, by using (8),
we immediately obtain
EA→B∣A = τ~1— X η (WB,i - wAi)2 f —:KBT- ) + 1 YB σ2	(S.17)
1 - γB i	,i κB + NBηi	1 - γB
=：X Φi(WB,i - WA i)2 + 1 YB σ2.	(S.18)
i	A,i 1 - γB
Next, We take the average of (S.18 ) over task A. Only WA depends on the task A and is determined by
the single-task training (S.8 ). This corresponds to Lemma 2 with U = WB and φi =中岛,J(1 - YB).
We obtain
Ea→B = E (WA,i - WB,i)2 - 2WA,i(WA,i — ^wB,i)QA,i
i
1 1 1	1	1	ηiNA L -2	2 ʌ 2	ηiqB,i
+ (WA,i +『F ^nj WAj qA,J qA,i] T-^
+σ2 (士 ɪɪ NA X n2qA @,i+ɪ!.	(S∙19)
Although this is a general result that holds for any WA and WB, it is a bit complicated and seems not
easy to give an intuitive explanation. Let us take the average over
[wA,i,wB,i] ~N(0,ni P P ).
The generalization error is then simplified to
EA→B (P) = hEA→Biw
q2
2(1 - P)(I-qA，i)」
EB,i
+σ2 (± 1⅛ NA X n2qA ,iqB ,i+⅛
where we used hWA,i iw = hWB ,iiw = ηi and hWA,i , WB,i iw = P.
(S.20)
(S.21)
(S.22)
□
Σ
i
13
Published as a conference paper at ICLR 2022
A.2.2 Backward transfer
Backward transfer is measured by the prediction on the previous task A:
Eba→kB :=(/ dχp(χ)(f4(χ) - fA→B(x))2)	(S.23)
=(/ dxp(x)((wB — (WA — WA))>ψ(x))2) .	(S.24)
First, we take the average over task B,
EA→B∣a =〈/ dxp(x)((w* - (Wa - WA))>ψ(x))2∖ .	(S.25)
This corresponds to Lemma 2 with the replacement φi J η and U J WA — WA. Recall that the
objective function ofNTK regression was given by (S.16). The target WA in Lemma 2 is replaced as
WA J WB - WA. We then have
eA→B∖A = E (WB,i - WA,i)2 - 2(WB,i - wA,1(WB,i - WA,i)qB,i
i
+ ( (WB,i - wA,i)2 +
1	ηiN
1 - YB KB
(Eη(WBj- wA,j)2qB,j + σ2)	qB,i
j=0	
ηi
(S.26)
YB X ηi(WBL WAI + X I2-SB
ii
WA,i - (Wp,i - 1YB(Wp,i - WA,i))
qB,i
+ T^B- σ2.	(S.27)
1	- YB
Next, we take the average over DA. Since the first and third terms of (S.27 ) are independent of
DA, we need to evaluate only the second term. The second term corresponds to Lemma 2 with
φi = ηiq2,i∕(1 - YB) and Ui = Wb~ - (1 - YB)∕qB,i(WB,i - WA,i). We obtain
back
EA→B
〉
DA
Yb Eni(WB,i - WA,i)2 + E (W A,i - WB,i)2(qB,i - (1 - YB ))2
ii
-2WA,i(WA,i - WB,i)(qB,i - (1 - YB))qA,i7B,i
+「A ,i + ɪ nKNA (X n WA ,j qA ,j + σ2 J qB ,i qB ,i# 士
I	ɪ	YA κA j_| ɪ	yb
+ σ2 (1 1	1 1	NA	X ni2qA/B,i	+ Tɪ!	.	(S.28)
1 - YA	1 - YB	K2A	i i A,i B,i 1 - YB
14
Published as a conference paper at ICLR 2022
Finally, by taking the average over (S.20 ), we have
EAba→ckB(ρ) = hEAba→ckBiw
i
X "bη2(1 - ρ) + 2 (qB,i - (1 - YB))2 Tɪ(1 - P)
1 - YB
-2 η2(qB,i-1+YB) qB,iqA,i (1 -P)+η2qA ,iqB ,i
1 - YB	1 - YA 1 - YB	,	,
1
+ σ2
1	NA
1 - YA 1 - YB KA
XX 演A,iqB,i + ɪ
2(1-ρ)	ηi2
i
1 + qB,i (qA,i - 2) +
q2B,i(I - qA,i)
1 - γB
+ 1 - YA 1 - Yb
×	ηi2 qA2 ,iqB2 ,i + σ 2
i
1	NA
1 - YA 1 - YB KA
X ηiqA,iθB,i + τ-B^)	(S.29)
1
1
1
□
A.2.3 Lemma 2
Lemma 2. Suppose training on single task A, the target of which is given by f = Ei WA,iΨi, and
denote the trained model (2) as f * = Ei WA ,iψi ∙ Define the following costfunction:
E
φi(wA,i - Ui)2 )
(S.30)
DA
for arbitrary constants φi and ui. Using the replica method under a sufficiently large NA, we have
E
E (WA,i - Ui)2 - 2WA,i(WA,i - Ui)qA,i
i=0
1
+〈 WA ,i +
ηiNA
1 - YA KA
(E ηWA,jqA,j + σ2) q qA,i φi.
j=0	
(S.31)
Proof. The process of derivation is similar to Canatar et al. (2021), but we have additional constants
φi and ui . They cause several differences in the detailed form of equations.
Define
βN
Z[J] =	dwA exp(-βHA(WA) + J^^2-E(WA))
where E(wA) = Pi φi(wA,i - ui)2 D . We omit the index A in the following. We have
E
(S.32)
2∂
lim _____
β→∞ βN ∂J
hlogZ[J]iD
(S.33)
J=0
To evaluate hlog Zi, we use the replica method (a.k.a. replica trick):
hlogZiD = ni→0 1(hZniD -1)
(S.34)
The point of the replica method is that we first calculate Zn for n ∈ N then take the limit of n to zero
by treating it as a real number. In addition, we calculate the average hZniD under a replica symmetric
ansatz and a Gaussian approximation by following the calculation procedure of the previous works
(Dietrich et al., 1999; Bordelon et al., 2020; Canatar et al., 2021).
15
Published as a conference paper at ICLR 2022
We have
hZn〉D = Z dWn exp (-2 XX kwak2 + βJN XX (Wa- u)> Φ(wa - u)) hQiNχμ,εμ},
a=1	a=1	(S.35)
Q := exp
(S.36)
where we define dWn = Qan=1 dWa and Φ is a diagonal matrix whose diagonal entries given by φi .
We take the shift of Wa → Wa + W. Then,
hZniD = Z dWn exp(-n2βkwk2 + nβJN(W - u)>Φ(W - U))
S-------------------------------{z------------------}
=:Z(J)
• exp(X -2kWak2 + βJNWa>ΦWa - βk>Wa) hQiNχ”,εμ},
βn
Q = exp(-2λ E(Wa>ψ(x") - ε")2),
a=1
k := W — JNΦ(W — u).
(S.37)
(S.38)
(S.39)
First, let US calculate hQ}{χμ 三6}. This term is exactly the same as appeared in the previous works
(Bordelon et al., 2020; Canatar et al., 2021). To describe notations, we overview their derivation.
Define qa = Wa>ψ (x) + ε, which are called order parameters. We approximate the probability
distribution of qa by a multivariate Gaussian:
PHqaD = p(2∏)n1det(c) exp 11X (qa- μa)[C-1]ab (qb- "，	(S.40)
with
μa := hqai{χ,ε} = (w">ψ3>x + hεiε = Vn0W0,	(S.41)
Cab := qaqb{x,ε} = Wa>hψ(x)ψ(x)>ixWb + hεaεbiε = Wa>ΛWa + Σab,	(S.42)
where Σab = σδab,〈…〉x denotes the average over p(χ) and A is a diagonal matrix whose entries
are η. We have √0w0 in (S.41 ) since ηo corresponds to the constant shift φ0(x) = 1. Training
samples are i.i.d. and we omitted the index μ. We then have
hQi{x,ε} = exp (- 2 log det (I + λC) - 2λ" (I + λC)	”)，	(S.43)
where I denotes the identity matrix. Define conjugate variables {μa, Cab} by the following identity:
1
C
/(∏
a≥b
dμadμadC abdC ab
X exp	-NE μa(μa	- Wa,0√ηO)	- NECab	(Cab	- Wa>AWb -	Σab)	,	(S.44)
a	a≥b
where C denotes an uninteresting constant and we took the conjugate variables on imaginary axes.
Next, we perform the integral over Wn . Eq. (S.37 ) becomes
hZniD
dWn ∏ dΩab exp(-N(X μaμa + X Cab(Cab - Σab))) exp[-NG - GS]
a≥b	a	a≥b
CZ(J) /
(S.45)
16
Published as a conference paper at ICLR 2022
where dΩab = dμadμadCabdCab and define
G = ιo IOgdet	(I	+ CC)	+ Kμ> (I +	CC)	μ,
2	λ 2λ	λ
(S.46)
exp(-Gs) = exp(X(-2kwak2 + βJNwa>Φwa - βk>wa))
X exp N ɪ2 μαw°a,o√ηo + N	CabWaTAWb .	(S.47)
a	a≥b
We can represent dWn exp(-GS) by
一 2 x> Q iXi — βb>xj	(S.48)
dWn exp(-GS) = Y dxi exp
i
where xi ∈ Rn denotes a vector [wi1, ..., wia, ..., win]T and i is the index of kernel’s eigenvalue mode
(i = 0, 1, ...). We defined
Qi = (1 - ΦiJN)In - ηiN(C + diag(C)),	(S.49)
β
bi = kiln (i ≥ 1),	(S.50)
b0 = k01n-------^^~μ,	(S.51)
β
where In is an n × n identity matrix and 1n is an n-dimensional vector whose all entries are 1. The
GS term includes φ and c that are specific to our study. When φ = η and u = wk, it is reduced to
previous works (Bordelon et al., 2020; Canatar et al., 2021). Taking the integral over {xi}, we have
/ dWn exp(-Gs )= C Y exp( 2 b>Q-1bi)/ Jdet Q i.	(S.52)
That is,
Gs = 1 Xlogdet Qi — 2 X b>Q^-1bi.	(S.53)
i=0	i=0
Replica symmetry and saddle-point method
Next, we carry out the integral (S.47 ) by the saddle-point method. Assume the replica symmetry:
μ = μα, r = C aa, C = C a=b,
μ = μa, r = Caa, c = Ca=b.
The following three terms are the same as in the previous works:
det (I + βC) = [l + β (r-c)∏i + n-^),
λ	λ	λ+β(r -c)
(I + βC )-1 =	ɪ— (I___________ɪ— 1
λ	1 + λ (r — C)I λ + β (r - c) + nβc
μ>μ + X Cab(Cab - ∑ab) = n(^μ + 可『-σ2) - 1 q(q - σ2)),
a≥b	2
(S.54)
(S.55)
(S.56)
(S.57)
(S.58)
where 11T denotes a matrix whose all entries are 1. Regarding the leading term of order n (n → 0),
lθgdet (I + λC)	= n lOg	(1	+ λ(『-c))	+ nλ +	ββC-c),	(S.59)
μ> (I + βC) μ = -一nμ----------.	(S.60)
∖ λ J	1 + λ (r - C)
17
Published as a conference paper at ICLR 2022
Furthermore. we have
Qi = (I — φiJN —	(2r — C))In — ~~S~1n1> .	(S∙61)
ββ
After straightforward algebra, the leading terms become
一 一 ʌ 一	ηiNc
log det Qi = n ln gi - nɪɪ,	(S.62)
giβ
1>Q-11n = n,	(S.63)
gi
with
gi ：= 1 — ΦiJN — ~~Ξ~(2r — ^)∙	(S.64)
β
Substituting these leading terms into (S.45 ), we obtain hZniD = C dθ exp(—nN S(θ)) with
S (θ) = —ɪ- (W — u)> Φ(w — U)
=(μμ + r(r — σ2) — 1 C(C — σ2)) + | (logβ (r — c) + ；+_：)
+ 2N X(lngi - ("ip "+ β后"gi) — 2g^(—2⅛0μ√η0 + Nnoμ2/∕β),
(S.65)
where θ denotes a set of variables {r, r, c, C, μ, μ}. Here, We take N》1 and use the saddle-point
method. We calculate saddle-point equations ∂S(θ* )∕∂θ = 0 and obtain
μ
r—C
1 ( c + μ2	1
2	(r — C)2	r — C
c + μ2
(r — c)2 ,
Nηoμ 而	1—
..----vη0,
g0β g0
b/QC I	血2、ηi	I
S(丁+%)币+
北2β (—2⅛0μ√ηo + Nηoμ2β) + σ2,
c=X(ηNc+昵)& -1X ηi+⅛β(—2加 μ√η0	+σ2.
From (S.67 ) and (S.68),
r—c
2r — C
From (S.70 ) and (S.71 ), we also have
β(r — c) = ^X η =: κ.
i=0 gi
Substituting (S.64 ) and (S.72 ) into (S.73 ), We obtain the implicit function for κ:
1=
i=0
ηi
W(φi),
W(φi) ：= κ(1 — φiJN)+ ηiΝ.
(S.66)
(S.67)
(S.68)
(S.69)
(S.70)
(S.71)
(S.72)
(S.73)
(S.74)
μ
μ
r
1
—
18
Published as a conference paper at ICLR 2022
Using κ and W, the saddle-point equations become
μ
βμ
—
κ
1	C + μ2 a	β
2	V β - K
ʌ _ β2	2、
C= K(c + μ )，
_	√η0k0κ
μ Nηo + W(φo),
κ
T =C + B
Substituting back these quantities into (S.65 ), we obtain
S (θ*)
βJ	>	1
——^-(W — u)>Φ(W — U) + - ln K +
l Bηoκ	层	Ie — I
+ 2W(φ) Nηo + W(φ0) +2Kσ + Const
RecanhZIniD ≈ exp(-nNS(θ*)). Wehave
(log ZiD = —NS(θ*).
(S.75)
(S.76)
(S.77)
(S.78)
(S.79)
(S.80)
(S.81)
Generalization error
We evaluate
E = lim -ɪ-ɪ (log Z〉d	= — lim 与S-S)θ^']
β→∞ βN ∂J	j=0 β→∞ β ∂J
Note that W, g, K and k depend on J. At the point of J = 0,
∂W (φi)
∂J
∂K
-φiNκ+∂J,
∂gi
∂J
1 ∂K
-(φi + M ∂Jηi)N,
∂K
∂J
κ2N
T-Y M
ηiφi
(K + ηiN)2 ,
Substituting (S.83 -S.86 ) into (S.82 ), we obtain
∂kki = -Nφi(Wi- Ui).
∂J
(S.82)
(S.83)
(S.84)
(S.85)
(S.86)
E
(wki — Ui)2 — 2wki (wki — Ui)qi
i=0
+ J w2 + i——Y ηN X ηjw2q2+σ2) 卜2 φi+ξo,
(S.87)
where ξ0 is
_	_2	Nηo + 2Wo	1	、(NL 2八
ξ0 = η°ik0[ VW0≡(Nηo + Wo)2	W0(Nη0 + W°J (l-Y 工 ηiqi
2	Nη0 + 2W0	φ0KN wk0(wk0 — U0)
— φ0 K W2(Nηo + Wo)2 + η0 W0(Nη0 + Wo)	( . )
with W0 = K + Nη0. When η0 = 0, we have ξ0 = 0. Note that the additional term ξ0 is not specific
to our case but also appeared in the previous work (Canatar et al., 2021). We have ξo = O(1∕ηo) for
a large η0 and ξ0 is often negligibly small.
□
19
Published as a conference paper at ICLR 2022
A.3 Equations for understanding negative transfer
A.3.1 Derivation of equation 15
First, let Us evaluate the term including qA,i = ka/(ka + NAη). Note that KA is a monotonically
decreasing function of NA because
∂ka	X η
∂NA = - L (κA + NAmy2
X (KA +Naη) < 0,
(S.89)
which comes from the implicit function theorem. Let us write KA at a finite NA = c as KA(c). We
then have
qA,i ≤ κAcy,	(S.90)
NAηi
for NA > c. Next, we evaluate
q2
EA→B (P) = 2(1 - P)EB - 2(1 - P) ɪ2 qA,iEB,i +	ɪ--，-EB,i∙	(S.91)
i	i 1 - γA
The second term is negligibly small for a sufficiently large NA > c because
XKA (c)	ηiqB,i	KA(c)
i qA，iEB，i ≤ F 入 T^B = FKB.
Note that We have 1 一 Y = K Pi=0 ηi∕(κ + η%N)2. The third term is
(S.92)
X ⅛EB,i ≤
EA
1	YA	K2A
1 — YB	1 — YB 1 — YA NA ,
(S.93)
where we used qB,i ≤ 1. It decreases to zero as NA increases. Thus, we have Ea→b (ρ)〜
2(1 — P)EB.
A.3.2 Derivation of equation 19
Ea→b (1) =	1	Pi qA ,iq2 ,i*
EB	- 1 -YA Pi qB,iηi
≥	1 K________KA________!2
—1 -YA	KA + Ei qB ,i宿 NA J
where the second line comes from Jensen’s inequality. Using (S.90 ), we have
X qB黯 ≤ kBN≠ Xηi,
i	Bi
(S.94)
(S.95)
(S.96)
for NB ≥ c. Since we assumed the finite trance of NTK (i.e.,	i ηi < ∞), the left-hand side of
(S.96 ) is finite and converges to zero for a sufficiently large NB. Therefore, we have
Ea→b (1)
EB
1
1 - YA
(S.97)
In contrast, EA→B∕EB ≤ 1∕(1 - YA) since qA ≤ 1. Thus, the ratio is sandwiched by 1∕(1 - YA).
A.3.3 S ufficient condition for the negative transfer
Let us denote the i-th mode of EA→B as EA→B,i and that of EB as EB,i . From EA→B,i > EB,i,
we have
q2
f(qi )：= 2(1 - ρ)(1 - qi)+ qi- - 1 > 0,	(S.98)
1-Y
where f (q) is a quadratic function and takes the minimum at q* = (1 - ρ)(1 - y). The condition
f(q) > 0 holds if and only if
f(q*) = -(1 - Y)P2 - 2yp + Y> 0.	(S.99)
Solving this, we find P < √7∕(1 + √7).
20
Published as a conference paper at ICLR 2022
A.4 Proof of Proposition 3.
For NA = NB, we have qA,i = qB,i = qi. Then,
EA→b ⑴=(1 J Y)2 X q4η2
and the generalization error of single-task training is given by
e =占 X q2η2.
i
(S.100)
(S.101)
Model average is obtained in Section D and We have Eave = (1 - γ∕2)E < E because 0 < γ < 1.
Thus, we only need to evaluate the relation between Eave and EA→B :
Eave - EA→B	=	(I	-	Y) ； X q2 η2	- -j-. 72 X ηiq4	(S.102)
2 1 - γ i i i	(1 - γ)2 i i i
=	(1-Y」κ2 X a	-二 X a2q2	(S.103)
Where We defined
and used qi = 1 - aiN and Y
cubic function:
κ2
(1 - Y)2N
{XN2a3(2 - Nai)- 2Y2(3 - Y)}
(S.104)
{z^^
=:F
η
K + ηiN'
(S.105)
N Pi ai2. We can provide a loWer bound of F by using the folloWing
I
}
G(ai) := N2ai2 (2 - Nai).
We have 0 ≤ ai ≤ 1∕N by definition. Let us consider a loWer bound of G by using its tangent line at
ai = t∕N (0 ≤ t ≤ 1), that is, Nt(4 - 3t)ai - 2t2(1 - t). Define
H(ai) := G(ai) - (N t(4 - 3t)ai - 2t2(1 - t)).
We have H(0) = 2t2 (1 - t) ≥ 0. Since H(1∕N) = -2(t - 1)2(t - 1∕2), We need t ≤ 1∕2 to
guarantee H(ai) ≥ 0 for all 0 ≤ ai ≤ 1∕N. Here, note that H is a cubic function of ai and has tWo
fixed points ai = t∕N and (4 - 3t)∕(3N). We can see that for t ≤ 1∕2, H has the local minimum at
ai = t∕N and
H(t∕N) =t2(2-t) - (t2(4 - 3t) -2t2(1 -t))
= 0.	(S.106)
Therefore, We have H(ai) ≥ 0 for all 0 ≤ ai ≤ 1∕N When the fixed constant t satisfies t ≤ 1∕2.
Thus, We have the loWer bound of G:
G(ai) ≥ Nt(4- 3t)ai -2t2(1 -t).	(S.107)
Using this loWer bound, We have
F = X aiG(ai) - Yγ2 (3 - γ)
i
≥ Yt(4 - 3t) - 2t2(1 - t) - 1 γ2(3 - γ),	(S.108)
Where We used Y = N Pi ai2 and Pi ai = 1. By setting t = Y∕2, We obtain
F ≥ Y2 (4 - 3γ∕2) - 2Y2 (1 - γ∕2) - 1 γ2 (3 - Y)
= 0.	(S.109)
Therefore, we have Eave > Ea→b .	□
A.5 Multiple descent
21
Published as a conference paper at ICLR 2022
æ
×10^6
102	103
NA
Figure 5: Theoretical values of EA→B (1). (Left) noise-less case (σ2 = 0), (Right) noisy case
(σ2 = 10-5). We set D = 100 and other settings are the same as in Figure 1. In noise-less case,
generalization error monotonically decrease as NA or NB increases. In contrast, when noise is added,
it shows multiple descents.
We overview the multiple descent of E1 investigated in Canatar
et al. (2021). It appears for σ > 0. Let us set NB = αDl
(α > 0, l ∈ N, D 1), which is called the l-th learning
stage. We have EB,i = 0 (i < l), EB,l (α) and ηi2 (i > l).
EB,l (α) is a function of α, and becomes a one-peak curve
depending on the noise and the decay of kernel’s eigenvalue
spectrum. Because each learning stage has a one-peak curve,
the whole learning curve shows multiple descents as the sam-
ple size increases. Roughly speaking, the appearance of the
multiple descent is controlled by γ. The γ is determined by the
kernel’s spectrum and sample size N . For example, previous
studies showed that if we assume the l-th learning stage, γ Fiure 4: T ical behaviors of κ
gure : ypca eavors o κ
is a non-monotonic function of α, the maximum of which is and Y
Y = 1/(√λl + √λI + 1)2 for a constant λι = Pk>l 根/切,
where η is a normalized eigenvalue of the kernel (Canatar et al., 2021). This tells US that Y repeats
the increase and decrease depending on the increase of the learning stage as is shown in Figure 4.
Roughly speaking, this non-monotonic change of Y leads to the multiple descent.
In sequential training, EB,l(α) can similarly cause multiple descent as in single tasks because EA→B
is a weighted summation over EB,i . Figure 5 is an example in which noise causes multiple decreases
and increases depending on the sample sizes. We set ρ = 1 and obtained the theoretical values by
(S.22 ). While EA→B (1) is a symmetric function of indices A and B for σ = 0, it is not for σ > 0.
Depending on NA and NB, the learning “surface” becomes much more complicated than the learning
curve of a single task.
A.6 Additional experiment on backward transfer
Figure 6 shows the learning curves of backward transfer. Solid lines show theory, and markers show
the mean and interquartile range of NTK regression over 100 trials. The figure shows excellent
agreement between theory and experiments. As is shown in Section 4.1, subtle target dissimilarity
(ρ < 1) causes negative backward transfer (i.e., catastrophic forgetting). For a large NA, EAba→ckB (ρ <
1) approaches a non-zero constant while EA approaches zero. Thus, we have EAba→ckB > EA even for
the target similarity close to 1. When sample sizes are unbalanced (NA NB), the learning curve
shows negative transfer even for ρ = 1. This is the self-knowledge forgetting revealed in (20). The
ratio EAba→ckB (1)/EA takes a constant larger than 1, that is, 1/(1 - YB).
22
Published as a conference paper at ICLR 2022
108
106
102
10°
L√t 104
1O4
Figure 6: Negative backward transfer easily appears depending on target similarity and sample sizes.
We changed NA and set NB = 102. We set D = 20 and other experimental details are the same as
in Figure 1(b).
10^2
10^l	100	101	102	103	103
Na/Nb
B Proof of Theorem 4.
B.1 Learning curve of many tasks
Eq. (4) is written as
n
fn(x) =XΘ(x,Xk)Θ(Xk)-1(yk - fk-1(Xk)).
(S.110)
k=1
Each term of this summation is equivalent to the kernel ridge-less regression on input samples Xk
and labels yk - fk-1(Xk). Therefore, we can represent fn by
n
fn (X) =	Wkt>ψ(X)
k=1
with the minimization of the objective function H :
Wnt = lim argminwn H(Wn,W1t:(n-1)),
λ→0
(S.111)
(S.112)
1	Nn
H(Wn,
W1:(n-1)) = 2λ〉:
μ=1
w>ψ(xμ) - (yμ - X wfe>ψ(xμ))! + 2 kwnk2
k=1	2
(S.113)
1 Nn	n-1	2	1
2λ X I (Wn -(W - X Wk))>ψ(X") - εμ ) + 2 llWnk2.
(S.114)
μ=1
k=1
The generalization error is
En+1 :
dxp(x)(f(x) - fn+l(x))2 )
dxp(X)((Wn+ι-(W - X Wt))>ψ(X))2)
k=1	D
(S.115)
(S.116)
where D = {D1, ..., Dn}.
Since the data samples are independent of each other among different tasks, we can apply Lemma 2
sequentially from wn+1 to w1. First, we take the average over the (n + 1)-th task, that is,
En+1|1:
dxp(X)((Wn+ι -(W - X wo)>ψ(X))2)
k=1
(S.117)
Dn+1
23
Published as a conference paper at ICLR 2022
This corresponds to Lemma 2 with φ = η and U = WA = W - Pn=ι w〉We have
n
En+1|1:n = fφ∖,i(Wi- EWki)2 + Riσ2
(S.118)
i=1
k=1
with
ηiqn+1,i	Yn+1
φ1,i .一 ~,	, R1 .一 ~∖	.
1 - Yn+1	1 - Yn+1
Here, κn, Yn and qn,k are determined by Nn. Next, we take the average over the n-th task:
(S.119)
En+1|1:(n-1)
En+1|1:n Dn,εn
(S.120)
n-1
φ1,i(wn,i - (wi - X Wli))2)	+ R1σ2
k=1	Dn
This corresponds to Lemma 2 with φ = φι and U = WA = W - Pn-1 w[ We obtain
(S.121)
En+1|1:(n-1)
n-1
X φ2,i(w - X wk)2 + R2σ2
i=1
k=1
(S.122)
with
φ2,i = φ1,iqn2 ,i +
R2 = R1 +
2
κn
Nn
(N-Yn) ηiq2,i X η φι,j qn,j,
(S.123)
κ2n(1 - γn)
ηjφ1,jqn2,j.
j
(S.124)
Similarly, we can take the averages from the (n - 1)-th task to the first task, and obtain
En+1 =)： φn+1,iwi + Rn + 1σ ,
i=1
(S.125)
φm+1,i = φm,iqn-m+1,i + K (] - Y	^yη qn—m+1,i (^X "，φmj qn—m+1,j
for m = 1, ..., n, and
(S.126)
Rn+1 = X Kn-m+Nn-m+n-m+l) X ηj φm,j qn-m+1,j .
n-m
(S.127)
They are general results for any Nn. By setting Nn = N for all n, we can obtain a simpler formulation.
In a vector representation, φn+1 is explicitly written as
φn+1 = J-
1-Y
diag(q2 ) + (T-NYF 河 >
n
q.
(S.128)
Finally, by taking the average over Wi 〜N(0, η) which is the one-dimensional case of (S.20), We
have
En+1 = (1 - γ)2 q>
diag(q2) + (1-NYκ qq>
n-1
q + Rn+1σ2
(S.129)
with
Nn
Rn+1 = F——后 E q> diag(q2)
κ2 (1 - Y )2
m=1
工 N 一>
+ (1-γ)κ2 qq
m-1
γ
q + —
1-γ
(S.130)
24
Published as a conference paper at ICLR 2022
B.2 MONOTONIC DECREASE OF En
For σ2 = 0, we have
En+1 = L 1 、2 q>Qn-1q.	(S.131)
(1 - γ)2
Note that Q is a positive semi-definite matrix. If the maximum eigenvalue is no greater than 1, En
is monotonically non-increasing with n. We denote the infinite norm as kAk∞ = maxi Pj |Aij |.
Because the infinite norm bounds the eigenvalues, we have
λ1(Q) ≤ max	|Qij|
i
j
N
=max 1 +-ηi
iκ
、
2
K⅛N )
(S.132)
(S.133)
{z"^^^^^
：=Ggi)
}
where G(η) is monotonically decreasing for η ≥ 0 and G(0) = 1. Therefore, the largest eigenvalue
λ1(Q) is upper-bounded by 1, and En becomes monotonically decreasing. In particular. if ηi > 0
for all i, we have λ1(Q) < 1 and obtain En+1 < En.
For λ1(Q) < 1, it is also easy to check that the series (S.130 ) in Rn+1 converges to a constant for
large n.	□
C Derivation of KRR-like expression
KRR-like expression comes from the inverse formula of a block triangular matrix. Let us write (23)
as
fn(x0)= [Θ(x0,1 : (n - 1)) Θ(x0, n)]Zn-1 y1：(n-1) ,	(S.134)
yn
where Zn is a block triangular matrix recursively defined by
Zn= Θ(n, 1Z:n(-n1 - 1)) ΘO(n) .	(S.135)
Then, (S.134 ) is transformed to
[θ(X0，1：(n - I)) θ(x0, n)] [-Θ(n)-iθ(nZ--(n - 1))Z--I Θ(0)[ ^7"] (S[36)
= Θ(x0, 1 : (n - 1))Zn--11y1：(n-1) + Θ(x0,n)Θ(n)-1(yn - Θ(n, 1 : (n - 1))Zn--11y1：(n-1))
(S.137)
= fn-1(x0) + Θ(x0, n)Θ(n)-1(yn - fn-1(Xn)).	(S.138)
Thus, one can see that the KRR-like expression is equivalent to our continually trained model.
D Derivation of model average
For clarity, We set σ = 0, NA = NB, and P = 1 (that is, WA = WB). Generalization error of a model
average is expressed by
Eave
dxp(x)
—
WB
fA(x) + fB(x) )2)
dxp(x)
-WAJ )> ψ(χ)!2+
WA + WB
2
WB
WA + WB
2
)D
(S.139)
(S.140)
(S.141)
((WB -
>
Λ
—
25
Published as a conference paper at ICLR 2022
We can evaluate Eave in a similar way to EA→B. First, take the average over DB. We define
Eave∣A = :〈((WB - (2Wb - Wa))>Λ(wb - (2Wb - WA)))DB .	(S.142)
This average corresponds to Lemma 2 with replacement from the target task A to B, φ J η and
U J 2Wb — WA. Then, we have
Eave∣A = 4 X { (wA,i — WB,i)2 — 2WB,i (wA,i - WB,i)qB,i + (WB ,i + ^^ EB) ^B ,i }力
i=0	(S.143)
=4 ^X(WA,i — (1 + qB,i)wB,i)l2ηi + ^-E EB ∙	(S.144)
4 i=0	4
Next, we take the average over DA. The first term of (S.144 ) corresponds to Lemma 2 with
replacement φ J η and U J (1 + qB,i)WB. Then, we get
Eave = hEave|AiDA	(S.145)
=4 ^X((I — qA,i)WA,i — (1 + qB,i)WB,i)2ηi + ɪEA + ^~EEB ∙	(S.146)
i=0
For NA = NB , we have qA = qB , γA = γB , and EA = EB . In addition, since we focused on
WA = WB, we have
Eave = ^X niWB,iqB,i + ɪEB	(S.147)
i=0
=(1 — γ2B) EB	(S.148)
□
E Experimental settings
E.1 Dual representation
In dual representation, the targets (10) are given by
[aAi,αB^i] ^N(0, J P /P0),	(S.149)
P0	P0
fA(x) = EaAjΘ(xj,x), fB(x) = EaBjΘ(xj,x)	(S.150)
jj
for a sufficiently large P0. We sampled x0 from the same input distribution p(x) and set P0 = 104
in all experiments. It is easy to check that targets (10) and (S.150 ) are asymptotically equiv-
alent for a large P0. Because Θ(x0, x) = Pi ψi (x0)ψi (x), we have PjP aA,j Θ(x0j, x) =
Pi PP (aA,jψi(x0j))ψi(x) ~ Pi WiΨi(x), where Wi is sampled from N(0,ηi).
E.2 Summaries on experimental setup
Figure 1(a). We trained the deep neural network (1) with ReLU, L = 3, and Ml = 4, 000 by the
gradient descent. We set a learning rate 0.5 and trained the network during 10,000 steps over 50 trials
with different random seeds. The target is given by (S.150 ) with C = 1, D = 10, and Gaussian input
samples Xi 〜N(0,1). We initialized the network with (σW,σ2) = (2,0), set NA = NB = 100,
and calculated the generalization error over 4, 000 samples. Each marker shows the mean and
interquartile range over the trials. To calculate the theoretical curves, we need eigenvalues ηi of the
NTK. We numerically compute them by the Gauss-Gegenbauer quadrature following the instruction
of Bordelon et al. (2020). Its implementation is also given by Canatar et al. (2021). Since the input
26
Published as a conference paper at ICLR 2022
samples are defined on a hyper-sphere Sd-1 and the NTK is a dot product kernel (i.e., Θ(x0 , x) can
be represented by Θ(x0>x)), the NTK can be decomposed into Gegenbauer polynomials:
∞
Θ(z) = X ηiN (d, i)Qi(z),
i=0
(S.151)
where {Qi} are the Gegenbauer polynomials and N (d, i) are constants depending d and i. Since the
Gegenbauer polynomials are orthogonal polynomials, we have
ηi = c
Z Θ(z)Qi(z)dτ(z),
(S.152)
for a certain constant c and dτ (z) = (1 - z2)(d-3)/2dz. The Gauss-Gegenbaur quadrature approxi-
mates this integral by
r
ηi ≈ c	wj Θ(zj)Qi (zj),	(S.153)
j=1
where wj are constant weights and zj are the r roots of Qr(z). The definitions of constants (N (d, i),
c and wj) are given in Section 8 of Bordelon et al. (2020) and we set r = 1000 as is the same in this
previous work. We computed ηi (i = 1, .., 1000) and substituted them to the analytical expressions.
Figure 1(b). This figure shows the results of NTK regression; (2) for the single task and (4) for
sequential training. We set NB = 4, 000, D = 50 and other settings were the same as in Figure 1(a).
Figure 2.	This figure shows the results of NTK regression; we set D = 20, and other settings are the
same as Figure 1(a).
Figure 3.	(NTK regression) This figure shows the results of NTK regression; We set D = 10 and
show the means and error bars over 100 trials. Other settings were the same as in Figure 1(a).
(MLP) We trained the deep neural network (1) with ReLU, L = 5, and Ml = 512 by SGD with a
learning rate 0.001, momentum 0.9, and mini-batch size 32 over 10 trials with Pytorch seeds 1-10. We
set the number of epochs to 150 and scaled the learning rate × 1/10 at the 100-th epoch, confirming
that the training accuracy reached 1 for each task.
(ResNet-18) We trained ResNet-18 by SGD with a learning rate 0.01, momentum 0.9, and mini-batch
size 128 over 10 trials with Pytorch seeds 1-10. We set the number of epochs to 150 and scaled the
learning rate × 1/10 at the 100-th epoch, confirming that the training accuracy reached 1 for each
task.
Note that the purpose of these experiments was not to achieve performance comparable to state-of-
the-art but to confirm self-knowledge transfer and forgetting. Therefore, we did not apply any data
augmentation or regularization method such as weight decay and dropout.
27