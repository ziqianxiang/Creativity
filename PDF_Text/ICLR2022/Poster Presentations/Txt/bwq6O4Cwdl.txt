Published as a conference paper at ICLR 2022
How Does SimSiam Avoid Collapse Without
Negative S amples ? A Unified Understanding
with Self-supervised Contrastive Learning
Chaoning Zhang*& Kang Zhang* & Chenshuang Zhang & Trung X. Pham
Chang D. Yoo & In So Kweon
Korea Advanced Institute of Science and Technology (KAIST), South Korea
chaoningzhang1990@gmail.com & zhangkang@kaist.ac.kr
Ab stract
To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely
used but often requires a large number of negative samples. Without negative
samples yet achieving competitive performance, a recent work (Chen & He, 2021)
has attracted significant attention for providing a minimalist simple Siamese (Sim-
Siam) method to avoid collapse. However, the reason for how it avoids collapse
without negative samples remains not fully clear and our investigation starts by
revisiting the explanatory claims in the original SimSiam. After refuting their
claims, we introduce vector decomposition for analyzing the collapse based on
the gradient analysis of the l2-normalized representation vector. This yields a uni-
fied perspective on how negative samples and SimSiam alleviate collapse. Such a
unified perspective comes timely for understanding the recent progress in SSL.
1	Introduction
Beyond the success of NLP (Lan et al., 2020; Radford et al., 2019; Devlin et al., 2019; Su et al.,
2020; Nie et al., 2020), self-supervised learning (SSL) has also shown its potential in the field of
vision tasks (Li et al., 2021; Chen et al., 2021; El-Nouby et al., 2021). Without the ground-truth
label, the core of most SSL methods lies in learning an encoder with augmentation-invariant repre-
sentation (Bachman et al., 2019; He et al., 2020; Chen et al., 2020a; Caron et al., 2020; Grill et al.,
2020). Specifically, they often minimize the representation distance between two positive samples,
i.e. two augmented views of the same image, based on a Siamese network architecture (Bromley
et al., 1993). It is widely known that for such Siamese networks there exists a degenerate solution,
i.e. all outputs “collapsing” to an undesired constant (Chen et al., 2020a; Chen & He, 2021). Early
works have attributed the collapse to lacking a repulsive component in the optimization goal and
adopted contrastive learning (CL) with negative samples, i.e. views of different samples, to alleviate
this problem. Introducing momentum into the target encoder, BYOL shows that Siamese architec-
tures can be trained with only positive pairs. More recently, SimSiam (Chen & He, 2021) has caught
great attention by further simplifying BYOL by removing the momentum encoder, which has been
seen as a major milestone achievement in SSL for providing a minimalist method for achieving
competitive performance. However, more investigation is required for the following question:
How does SimSiam avoid collapse without negative samples?
Our investigation starts with revisiting the explanatory claims in the original SimSiam paper (Chen
& He, 2021). Notably, two components, i.e. stop gradient and predictor, are essential for the success
of SimSiam (Chen & He, 2021). The reason has been mainly attributed to the stop gradient (Chen
& He, 2021) by hypothesizing that it implicitly involves two sets of variables and SimSiam behaves
like alternating between optimizing each set. Chen & He argue that the predictor h is helpful in
SimSiam because h fills the gap to approximate expectation over augmentations (EOA).
Unfortunately, the above explanatory claims are found to be flawed due to reversing the two paths
with and without gradient (see Sec. 2.2). This motivates us to find an alternative explanation, for
which we introduce a simple yet intuitive framework for facilitating the analysis of collapse in SSL.
* equal contribution
1
Published as a conference paper at ICLR 2022
Specifically, we propose to decompose a representation vector into center and residual components.
This decomposition facilitates understanding which gradient component is beneficial for avoiding
collapse. Under this framework, we show that a basic Siamese architecture cannot prevent collapse,
for which an extra gradient component needs to be introduced. With SimSiam interpreted as pro-
cessing the optimization target with an inverse predictor, the analysis of its extra gradient shows that
(a) its center vector helps prevent collapse via the de-centering effect; (b) its residual vector achieves
dimensional de-correlation which also alleviates collapse.
Moreover, under the same gradient decomposition, we find that the extra gradient caused by neg-
ative samples in InfoNCE (He et al., 2019; Chen et al., 2020b;a; Tian et al., 2019; Khosla et al.,
2020) also achieves de-centering and de-correlation in the same manner. It contributes to a unified
understanding on various frameworks in SSL, which also inspires the investigation of hardness-
awareness Wang & Liu (2021) from the inter-anchor perspective Zhang et al. (2022) for further
bridging the gap between CL and non-CL frameworks in SSL. Finally, simplifying the predictor for
more explainable SimSiam, we show that a single bias layer is sufficient for preventing collapse.
The basic experimental settings for our analysis are detailed in Appendix A.1 with a more specific
setup discussed in the context. Overall, our work is the first attempt for performing a comprehensive
study on how SimSiam avoids collapse without negative samples. Several works, however, have
attempted to demystify the success of BYOL (Grill et al., 2020), a close variant of SimSiam. A
technical report (Fetterman & Albrecht, 2020) has suggested the importance of batch normalization
(BN) in BYOL for its success, however, a recent work (Richemond et al., 2020) refutes their claim
by showing BYOL works without BN, which is discussed in Appendix B.
2	Revisiting SimSiam and its explanatory claims
l2-normalized vector and optimization goal. SSL trains an encoder f for learning discriminative
representation and we denote such representation as a vector z, i.e. f (x) = z where x is a certain
input. For the augmentation-invariant representation, a straightforward goal is to minimize the dis-
tance between the representations of two positive samples, i.e. augmented views of the same image,
for which mean squared error (MSE) is a default choice. To avoid scale ambiguity, the vectors are
often l2-normalized, i.e. Z = z/||z|| (Chen & He, 2021), before calculating the MSE:
LMSE = (Za - Zb)2/2 - 1 = -Za ∙ Zb = Lcosine,	(I)
which shows the equivalence of a normalized MSE loss to the cosine loss (Grill et al., 2020).
Collapse in SSL and solution of SimSiam. Based on a Siamese architecture, the loss in Eq 1
causes the collapse, i.e. f always outputs a constant regardless of the input variance. We refer to
this Siamese architecture with loss Eq 1 as Naive Siamese in the remainder of paper. Contrastive
loss with negative samples is a widely used solution (Chen et al., 2020a). Without using negative
samples, SimSiam solves the collapse problem via predictor and stop gradient, based on which the
encoder is optimized with a symmetric loss:
LSimSiam = -(Pa ∙ Sg(Zb) + Pb ∙ Sg(Za)),	(2)
where sg(∙) is stop gradient and P is the output of predictor h, i.e. P = h(z) and P = p∕∣∣p∣∣.
2.1	Revising explanatory claims in SimSiam
Interpreting stop gradient as AO. Chen & He hypothesize that the stop gradient in Eq 2 is an
implementation of Alternating between the Optimization of two sub-problems, which is denoted
as AO. Specifically, with the loss considered as L(θ, η) = Ex,T Fθ(T(x)) - ηx 2 , the op-
timization objective minθ,η L(θ,η) can be solved by alternating ηt J argmi% L(θt,η) and
θt+1 J argminθ L(θ,ηt). It is acknowledged that this hypothesis does not fully explain why
the collapse is prevented (Chen & He, 2021). Nonetheless, they mainly attribute SimSiam success
to the stop gradient with the interpretation that AO might make it difficult to approach a constant ∀x.
Interpreting predictor as EOA. The AO problem (Chen & He, 2021) is formulated independent of
predictor h, for which they believe that the usage of predictor h is related to approximating EOA for
filling the gap of ignoring ET [∙] in a sub-problem of AO. The approximation of ET[∙] is summarized
2
Published as a conference paper at ICLR 2022
in Appendix A.2. Chen & He support their interpretation by proof-of-concept experiments. Specif-
ically, they show that updating & with a moving-average ηX J m * ηX + (1 - m) * Fjt (T0(x))
can help prevent collapse without predictor (see Fig. 1 (b)). Given that the training completely fails
when the predictor and moving average are both removed, at first sight, their reasoning seems valid.
2.2	DOES THE PREDICTOR FILL THE GAP TO APPROXIMATE EOA?
Reasoning flaw. Considering the stop gradient, We divide the framework into two sub-models with
different paths and term them Gradient Path (GP) and Stop Gradient Path (SGP). For SimSiam, only
the sub-model with GP includes the predictor (see Fig. 1 (a)). We point out that their reasoning flaw
of predictor analysis lies in the reverse of GP and SGP. By default, the moving-average sub-model,
as shown in Fig. 1 (b), is on the same side as SGP Note that Fig. 1 (b) is conceptually similar to
Fig. 1 (c) instead of Fig. 1 (a). It is worth mentioning that the Mirror SimSiam in Fig. 1 (c) is what
stop gradient in the original SimSiam avoids. Therefore, it is problematic to perceive h as EOA.
GP	SGP
GP
GP
I I---s similarity ■-
Wm
predictor h
Uza	Zb
encoder f	encoder f
t v )
Y
image X
-----► similarity
predictor h
Za
Zb
image X
(c) Mirror SimSiam
encoder f
(a) SimSiam
(b) Moving Average
SG SGP
encoder f
Figure 1:	Reasoning Flaw in SimSiam. (a) Standard SimSiam architecture. (b) Moving-Average
Model proposed in the proof-of-concept experiment (Chen & He, 2021). (c) Mirror SimSiam, which
has the same model architecture as SimSiam but with the reverse of GP and SGP.
GP	SGP
image X
(a) Naive Siamese
GP
SGP
image X
(b) SimSiam with Symmetric Predictor
GP
SGP
image X
(c) SimSiam with Inverse Predictor
Figure 2:	Different architectures of Siamese model. When it is trained experimentally, the inverse
predictor in (c) has the same architecture as predictor h.
MethOd	#aug	Collapse	Std	Top-1 (%)
Moving average	-^2^^	X	0.0108	-^4657-
Same batch	10	X	0	1
Same batch	25	X	0	1
Table 1: Influence of Explicit EOA. Detailed setup is reported in Appendix A.3
Explicit EOA does not prevent collapse. (Chen & He, 2021) points out that “in practice, it would
be unrealistic to actually compute the expectation ET[∙]. But it may be possible for a neural net-
work (e.g., the preditor h) to learn to predict the expectation, while the sampling of T is implicitly
distributed across multiple epochs.” If implicitly sampling across multiple epochs is beneficial, ex-
plicitly sampling sufficient large N augmentations in a batch with the latest model would be more
beneficial to approximate ET[∙]. However, Table 1 shows that the collapse still occurs and suggests
that the equivalence between predictor and EOA does not hold.
3
Published as a conference paper at ICLR 2022
2.3 Asymmetric interpretation of predictor with stop gradient in SimSiam
Symmetric Predictor does not prevent collapse. The difference between Naive Siamese and Sim-
siam lies in whether the gradient in backward propagation flows through a predictor, however, we
show that this propagation helps avoid collapse only when the predictor is not included in the SGP
path. With h being trained the same as Eq 2, we optimize the encoder f through replacing the Z
in Eq 2 with P . The results in Table. 2 show that it still leads to collapse. Actually, this is well
expected by perceiving h to be part of the new encoder F, i.e. p = F (x) = h(f (x)). In other
words, the symmetric architectures with and without predictor h both lead to collapse.
Predictor with stop gradient is asymmetric. Clearly, how SimSiam avoids collapse lies in its
asymmetric architecture, i.e. one path with h and the other without h. Under this asymmetric archi-
tecture, the role of stop gradient is to only allow the path with predictor to be optimized with the
encoder output as the target, not vice versa. In other words, the SimSiam avoids collapse by exclud-
ing Mirror SimSiam (Fig. 1 (c)) which has a loss (mirror-like Eq 2) as LMirrOr = -(Pa ∙Zb+Pb Z),
where stop gradient is put on the input of h, i.e. pa = h(sg[za]) and pb = h(sg[zb]).
Predictor vs. inverse predictor. We interpret h as a
function mapping from z to p, and introduce a con-
ceptual inverse mapping h-1, i.e. z = h-1(p). Here,
as shown in Table 2, SimSiam with symmetric predic-
tor (Fig. 2 (b)) leads to collapse, while SimSiam (Fig. 1
(a)) avoids collapse. With the conceptual h-1, we in-
terpret Fig. 1 (a) the same as Fig. 2 (c) which differs
from Fig. 2 (b) via changing the optimization target
from pb to zb, i.e. zb = h-1 (pb). This interpretation
Method	Collapse	Top-1 (%)
Simsiam	X	66.62
Mirror SimSiam	X	1
Naive Siamese	X	1
Symmetric Predictor	X	1
Table 2: Results of various Siamese archi-
tectures. Detailed trend and setup are re-
ported in Appendix A.4
suggests that the collapse can be avoided by processing the optimization target with h-1. By con-
trast, Fig. 1 (c) and Fig. 2 (a) both lead to collapse, suggesting that processing the optimization
target with h is not beneficial for preventing collapse. Overall, asymmetry alone does not guarantee
collapse avoidance, which requires the optimization target to be processed by h-1 not h.
Trainable inverse predictor and its implication on
EOA. In the above, we propose a conceptual inverse
predictor h-1 in Fig. 2 (c), however, it remains yet
unknown whether such an inverse predictor is experi-
mentally trainable. A detailed setup for this investiga-
tion is reported in Appendix A.5. The results in Fig. 3
show that a learnable h-1 leads to slightly inferior
performance, which is expected because h-1 cannot
make the trainable inverse predictor output Zb Com-
pletely the same as zb . Note that it would be equiva-
lent to SimSiam if zbb = zb . Despite a slight perfor-
mance drop, the results confirm that h-1 is trainable.
The fact that h-1 is trainable provides additional ev-
Epoch
Figure 3: Comparison of original SimSiam
and SimSiam with Inverse Predictor.
idence that the role h plays in SimSiam is not EOA
because theoretically h-1 cannot restore a random augmentation T0 from an expectation p, where
p = h(z) = ET hFθt(T(x))i.
3	Vector decomposition for understanding collapse
By default, InfoNCE (Chen et al., 2020a) and SimSiam (Chen & He, 2021) both adopt l2-
normalization in their loss for avoiding scale ambiguity. We treat the l2-normalized vector, i.e. Z,
as the encoder output, which significantly simplifies gradient derivation and the following analysis.
Vector decomposition. For the purpose of analysis, we propose to decompose Z into two parts,
Z = o + r, where o, r denote center vector and residual vector respectively. Specifically, the
center vector o is defined as an average of Z over the whole representation space oz = E[Z].
However, we approximate it with all vectors in current mini-batch, i.e. Oz =吉 PMM=I Zm, where
M is the mini-batch size. We define the residual vector r as the residual part of Z, i.e. r = Z - oz.
4
Published as a conference paper at ICLR 2022
3.1	Collapse from the vector perspective
Collapse: from result to cause. A Naive Siamese is well expected to collapse since the loss is
designed to minimize the distance between positive samples, for which a constant constitutes an
optimal solution to minimize such loss. When the collapse occurs, ∀i, Zi = 吉 PMM=I Zm = Oz,
where i denotes a random sample index, which shows the constant vector is oz in this case. This
interpretation only suggests a possibility that a dominant o can be one of the viable solutions, while
the optimization, such as SimSiam, might still lead to a non-collapse solution. This merely describes
o as the consequence of the collapse, and our work investigates the cause of such collapse through
analyzing the influence of individual gradient components, i.e. o and r during training.
Competition between o and r. Complementary to the Standard Deviation (Std) (Chen & He,
2021), for indicating collapse, We introduce the ratio of o in z, i.e. m° = ∣∣o∣∣∕∣∣z∣∣, where || * || is
the L2 norm. Similarly, the ratio of r in z is defined as mr = ||r||/||z||. When collapse happens,
i.e. all vectors Z are close to the center vector o, mo approaches 1 and mr approaches 0, which
is not desirable for SSL. A desirable case would be a relatively small mo and a relatively large
mr, suggesting a relatively small (large) contribution of o (r) in each Z. We interpret the cause of
collapse as a competition between o and r where o dominates over r, i.e. mo mr. For Eq 1, the
derived negative gradient on Za (ignoring Zb for simplicity due to symmetry) is shown as:
Gcosine
∂LM SE
∂Za
∂Lcosine
Zb - Za <	⇒	^^∖Γ7	— Zb,
∂Za
(3)
—
where the gradient component Za is a dummy term because the loss -Za ∙ Za = -1 is a constant
having zero gradient on the encoder f .
Conjecture1. With Za = oz + ra, we conjecture
that the gradient component of oz is expected to up-
date the encoder to boost the center vector thus in-
crease mo, while the gradient component of ra is ex-
pected to behave in the opposite direction to increase
mr . A random gradient component is expected to
have a relatively small influence.
To verify the above conjecture, we revisit the dummy
gradient term Z&. We design loss -Za ∙ sg(θz) and
-Za ∙ sg(Za 一 Oz) to show the influence of gradi-
ent component o and ra respectively. The results in
Fig. 4 show that the gradient component Oz has the
effect of increasing mo while decreasing mr. On the
contrary, ra helps increase mr while decreasing mo .
Figure 4: Influence of various gradient com-
ponents on mr and mo .
Overall, the results verify Conjecture1.
3.2	Extra gradient component for alleviating collapse
Revisit collapse in a symmetric architecture. Based on Conjecture1, here, we provide an intuitive
interpretation on why a symmetric Siamese architecture, such as Fig. 2 (a) and (b), cannot be trained
without collapse. Take Fig. 2 (a) as example, the gradient in Eq 3 can be interpreted as two equivalent
forms, from which we choose Zb - Za = (Oz +rb) - (Oz +ra) = rb -ra. Since rb comes from the
same positive sample as ra, it is expected that rb also increases mr, however, this effect is expected
to be smaller than that of ra, thus causing collapse.
Basic gradient and Extra gradient components. The negative gradient on Za in Fig. 2 (a) is
derived as Zb, while that on Pa in Fig. 2 (b) is derived as Pb . We perceive Zb and Pb in these
basic Siamese architectures as the Basic Gradient. Our above interpretation shows that such basic
components cannot prevent collapse, for which an Extra Gradient component, denoted as Ge, needs
to be introduced to break the symmetry. As the term suggests, Ge is defined as a gradient term that
is relative to the basic gradient in a basic Siamese architecture. For example, negative samples can
be introduced to Naive Siamese (Fig. 2 (a)) for preventing collapse, where the extra gradient caused
by negative samples can thus be perceived as Ge with Zb as the basic gradient. Similarly, we can
also disentangle the negative gradient on Pa in SimSiam (Fig. 1 (a)), i.e. Zb, into a basic gradient
(which is Pb) and Ge which is derived as Zb - Pb (note that Zb = Pb + Ge). We analyze how Ge
prevents collapse via studying the independent roles of its center vector Oe and residual vector re .
5
Published as a conference paper at ICLR 2022
3.3	A toy example experiment with negative sample
Which repulsive component helps avoid collapse? Existing works often attribute the collapse in
Naive Siamese to lacking a repulsive part during the optimization. This explanation has motivated
previous works to adopt contrastive learning, i.e. attracting the positive samples while repulsing the
negative samples. We experiment with a simple triplet loss1, Ltri = -Za∙sg(Zb - Zn), where Zn
indicates the representation of a Negative sample. The derived negative gradient on Za is Zb - Zn ,
where Zb is the basic gradient component and thus Ge = -Zn in this setup. For a sample represen-
tation, what determines it as a positive sample for attracting or a negative sample for repulsing is the
residual component, thus it might be tempting to interpret that re is the key component of repulsive
part that avoids the collapse. However, the results in Table 3 show that the component beneficial for
preventing collapse inside Ge is oe instead of re . Specifically, to explore the individual influence
of oe and re in the Ge, we design two experiments by removing one component while keeping the
other one. In the first experiment, we remove the re in Ge while keeping the oe . By contrast, the
oe is removed while keeping the re in the second experiment. In contrast to what existing explana-
tions may expect, we find that the residual component oe prevents collapses. With Conjecture1, a
gradient component alleviates collapse if it has negative center vector. In this setup, oe = -oz, thus
oe has the de-centering role for preventing collapse. On the contrary, re does not prevent collapse
and keeping re even decreases the performance (36.21% < 47.41%). Since the negative sample is
randomly chosen, re just behaves like a random noise on the optimization to decrease performance.
Method	Ltriplet	Std	mo	mr	Collapse	Top-1 (%)
Baseline	-Za ∙ sg(Zb + Ge)	0.020	0.026	0.99	×	36.21
Removing re	-Za ∙ sg(Zb + oe)	0.02005	0.026	0.99	×	47.41
Removing oe	-Za ∙ Sg(Zb + re)	0	1	0	X	1
Table 3: Gradient component analysis with a random negative sample.
3.4 Decomposed gradient analysis in SimSiam
It is challenging to derive the gradient on the encoder output in SimSiam due to a nonlinear MLP
module in h. The negative gradient on Pa for LSimSiam in Eq 2 can be derived as
GSimSiam = — dLSimSiam = Zb = Pb +(Zb- Pb)= Pb + Ge,	(4)
∂Pa
where Ge indicates the aforementioned extra gradient -------------------------------------
component. To investigate the influence of Oe and 屋 on	oe	re COllaPSe ToP-I (%)
the collapse, similar to the analysis with the toy exam-	X	X	X	66.62
ple experiment in Sec. 3.3, we design the experiment by	X	×	×	48.08
removing one component while keeping the other. The	×	X	×	66.15
results are reported in Table 4. As expected, the model	×	×	X	1
collapses when both components in Ge are removed and
the best performance is achieved when both components Table 4: Gradient component analysis
are kept. Interestingly, the model does not collapse when for SimSiam.
either oe or re is kept. To start, we analyze how oe affects the collapse based on Conjecture1.
How oe alleviates collapse in SimSiam. Here, op is used to denote the center vector of P to
differentiate from the above introduced oz for denoting that of Z. In this setup Ge = Zb - Pb,
thus the residual gradient component is derived to be oe = oz - op. With Conjecture1, it is well
expected that oe helps prevent collapse ifoe contains negative op since the analyzed vector is Pa. To
determine the amount of component of op existing in oe, we measure the cosine similarity between
oe - ηp op and op for a wide range of ηp . The results in Fig. 5 (a) show that their cosine similarity
is zero when ηp is around -0.5, suggesting oe has ≈ -0.5op. With Conjecture1, this negative ηp
explains why SimSiam avoids collapse from the perspective of de-centering.
How oe causes collapse in Mirror SimSiam. As mentioned above, the collapse occurs in Mirror
SimSiam, which can also be explained by analyzing its oe. Here, oe = op - oz, for which we
evaluate the amount of component oz existing in oe via reporting the similarity between oe - ηzoz
1 Note that the triplet loss here does not have clipping form as in Schroff et al. (2015) for simplicity.
6
Published as a conference paper at ICLR 2022
and oz . The results in Fig. 5 (a) show that their cosine similarity is zero when ηz is set to around
0.2. This positive ηz explains why Fig. 1(c) causes collapse from the perspective of de-centering.
Overall, we find that processing the optimization target with h-1, as in Fig. 2 (c), alleviates collapse
(ηp ≈ -0.5), while processing it with h, as in Fig. 1(c), actually strengthens the collapse (ηz ≈ 0.2).
In other words, via the analysis of oe , our results help explain how SimSiam avoids collapse as well
as how Mirror SimSiam causes collapse from a straightforward de-centering perspective.
n
(a)
Figure 5: (a) Investigating the amount of op existing in oz - op and the amount of oz existing in
op - oz . (b) Normally train the model as SimSiam for 5 epochs, then using collapsing loss for 1
epoch to reduce mr , followed by a correlation regularization loss. (c) Cosine similarity between re
(oe) and gradient on Za induced by a correlation regularization loss.
(b)	(c)
Relation to prior works. Motivated from preventing the collapse to a constant, multiple prior
works, such as W-MSE (Ermolov et al., 2021), Barlow-twins (Zbontar et al., 2021), DINO (Caron
et al., 2021), explicitly adopt de-centering to prevent collapse. Despite various motivations, we find
that they all implicitly introduce an oe that contains a negative center vector. The success of their
approaches aligns well with our Conjecture1 as well as our above empirical results. Based on our
findings, we argue that the effect of de-centering can be perceived as oe having a negative center
vector. With this interpretation, we are the first to demonstrate that how SimSiam with predictor and
stop gradient avoids collapse can be explained from the perspective of de-centering.
Beyond de-centering for avoiding collapse. In the toy example experiment in Sec. 3.3, re is
found to be not beneficial for preventing collapse and keeping re even decreases the performance.
Interestingly, as shown in Table 4, we find that re alone is sufficient for preventing collapse and
achieves comparable performance as Ge. This can be explained from the perspective of dimensional
de-correlation, which will be discussed in Sec. 3.5.
3.5	Dimensional de-correlation helps prevent collapse
Conjecture2 and motivation. We conjecture that dimensional de-correlation increases mr for pre-
venting collapse. The motivation is straightforward as follows. The dimensional correlation would
be minimum if only a single dimension has a very high value for every individual class and the
dimension changes for different classes. In another extreme case, when all the dimensions have the
same values, equivalent to having a single dimension, which already collapses by itself in the sense
of losing representation capacity. Conceptually, re has no direct influence on the center vector, thus
we interpret that re prevents collapse through increasing mr .
To verify the above conjecture, we train SimSiam normally with the loss in Eq 2 and train for several
epochs with the loss in Eq 1 for intentionally decreasing the mr to close to zero. Then, we train the
loss with only a correlation regularization term, which is detailed in Appendix A.6. The results in
Fig. 5 (b) show that this regularization term increases mr at a very fast rate.
Dimensional de-correlation in SimSiam. Assuming h only has a single FC layer to exclude the
influence of oe, the weights in FC are expected to learn the correlation between different dimensions
for the encoder output. This interpretation echos well with the finding that the eigenspace ofh weight
aligns well with that of correlation matrix (Tian et al., 2021). In essence, the h is trained to minimize
the cosine similarity between h(za) and I(zb), where I is identity mapping. Thus, h that learns the
correlation is optimized close to I, which is conceptually equivalent to optimizing with the goal of
de-correlation for Z. As shown in Table 4, for SimSiam, re alone also prevents collapse, which
7
Published as a conference paper at ICLR 2022
0.4
0.2
0.0
0	50	100	150	200
Epoch
Figure 6: Influence of various gradient components on mr and mo .
0.004
0.003
0.002
0.001
0.000
0	50	100	150	200
Epoch
is attributed to the de-correlation effect since re has no de-centering effect. We observe from Fig.
6 that except in the first few epochs, SimSiam decreases the covariance during the whole training.
Fig. 6 also reports the results for InfoNCE which will be discussed in Sec. 4.
4	Towards a unified understanding of recent progress in SSL
De-centering and de-correlation in InfoNCE. InfoNCE loss is a default choice in multiple seminal
contrastive learning frameworks (Sohn, 2016; Wu et al., 2018; Oord et al., 2018; Wang & Liu, 2021).
The derived negative gradient of InfoNCE on Za is proportional to Zb + PiN=0 -λiZi, where
λi = PNx)(ZdaZi：)	, and Zo = Zb for notation simplicity. See Appendix A.7 for the detailed
i=0=X exp(Zɑ∙ Zi/T )
derivation. The extra gradient component Ge = PiN=0 -λiZi = -oz - PiN=0 λiri, for which
oe = -oz and re = - PiN=0 λi ri . Clearly, oe contains negative oz as de-centering for avoiding
collapse, which is equivalent to the toy example in Sec. 3.3 when the re is removed. Regarding
re, the main difference between Ltri in the toy example and InfoNCE is that the latter exploits a
batch of negative samples instead of a random one. λ% is proportional to exp(ZaZ), indicating
that a large weight is put on the negative sample when it is more similar to the anchor Za, for which,
intuitively, its dimensional values tend to have a high correlation with Za . Thus, re containing
such negative representation with a high weight tends to decrease dimensional correlation. To verify
this intuition, we measure the cosine similarity between re and the gradient on Za induced by a
correlation regularization loss. The results in Fig. 5 (c) show that their gradient similarity is high for
a wide range of temperature values, especially when τ is around 0.1 or 0.2, suggesting re achieves
similar role as an explicit regularization loss for performing de-correlation. Replacing re with oe
leads to a low cosine similarity, which is expected because oe has no de-correlation effect.
The results of InfoNCE in Fig. 6 resembles that of SimSiam in terms of the overall trend. For ex-
ample, InfoNCE also decreases the covariance value during training. Moreover, we also report the
results of InfoNCE where re is removed for excluding the de-correlation effect. Removing re from
the InfoNCE loss leads to a high covariance value during the whole training. Removing re also
leads to a significant performance drop, which echos with the finding in (Bardes et al., 2021) that
dimensional de-correlation is essential for competitive performance. Regarding how re in InfoNCE
achieves de-correlation, formally, we hypothesize that the de-correlation effect in InfoNCE arises
from the biased weights (λi) on negative samples. This hypothesis is corroborated by the tempera-
ture analysis in Fig. 7. We find that a higher temperature makes the weight distribution of λi more
balanced indicated a higher entropy of λi , which echos with the finding in (Wang & Liu, 2021).
Moreover, we observe that a higher temperature also tends to increase the covariance value. Over-
all, with temperature as the control variable, we find that more balanced weights among negative
samples decrease the de-correlation effect, which constitutes an evidence for our hypothesis.
Unifying SimSiam and InfoNCE. At first sight, there is no conceptual similarity between SimSiam
and InfoNCE, and this is why the community is intrigued by the success of SimSiam without nega-
tive samples. Through decomposing the Ge into oe and re, we find that for both, their oe plays the
role of de-centering and their re behaves like de-correlation. In this sense, we bring two seemingly
irrelevant frameworks into a unified perspective with disentangled de-centering and de-correlation.
Beyond SimSiam and InfoNCE. In SSL, there is a trend of performing explicit manipulation of
de-centering and de-correlation, for which W-MSE (Ermolov et al., 2021), Barlow-twins (Zbontar
et al., 2021), DINO (Caron et al., 2021) are three representative works. They often achieve perfor-
mance comparable to those with InfoNCE or SimSiam. Towards a unified understanding of recent
progress in SSL, our work is most similar to a concurrent work (Bardes et al., 2021). Their work
is mainly inspired by Barlow-twins (Zbontar et al., 2021) but decomposes its loss into three explicit
components. By contrast, our work is motivated to answer the question of how SimSiam prevents
8
Published as a conference paper at ICLR 2022
(a)
Figure 7: Influence of temperature. (a) Entropy of λi with regard to temperature; (b) Top-1 accuracy
trend with various temperature; (c) Covariance trend with various temperature.
(b)
0	50	100	150	200
Epoch
Epoch
(c)
collapse without negative samples. Their work claims that variance component (equivalent to de-
centering) is an indispensable component for preventing collapse, while we find that de-correlation
itself alleviates collapse. Overall, our work helps understand various frameworks in SSL from an
unified perspective, which also inspires an investigation of inter-anchor hardness-awareness Zhang
et al. (2022) for further bridging the gap between CL and non-CL frameworks in SSL.
5	Towards simplifying the predictor in SimSiam
Based on our understanding of how SimSiam prevents collapse, we demonstrate that simple com-
ponents (instead of a non-linear MLP in SimSiam) in the predictor are sufficient for preventing
collapse. For example, to achieve dimensional de-correlation, a single FC layer might be sufficient
because a single FC layer can realize the interaction among various dimensions. On the other hand,
to achieve de-centering, a single bias layer might be sufficient because a bias vector can represent the
center vector. Attaching an l2-normalization layer at the end of the encoder, i.e. before the predictor,
is found to be critical for achieving the above goal.
Pridictor with FC layers. To learn the dimensional
correlation, an FC layer is sufficient theoretically but
can be difficult to train in practice. Inspired by the
property that Multiple FC layers make the training more
stable even though they can be mathematically equiva-
lent to a single FC layer (Bell-Kligler et al., 2019), we
adopt two consecutive FC layers which are equivalent
to removing the BN and ReLU in the original predictor.
Method	Predictor	Top-1 (%)
SimSiam	Non-linear MLP	66.9
Two FC	FC+FC+Bias	66.7
One FC	Tanh(FC)	64.82
One bias	Bias	49.82
Table 5: Linear evaluation on CIFAR100.
The training can be made more stable if a Tanh layer is applied on the adopted single FC after every
iteration. Table 5 shows that they achieve performance comparable to that with a non-linear MLP.
Predictor with a bias layer. A predictor with a single
bias layer can be utilized for preventing collapse (see
Table 5) and the trained bias vector is found to have
a cosine similarity of 0.99 with the center vector (see
Table 6). A bias in the MLP predictor also has a high
cosine similarity of 0.89, suggesting that it is not a co-
incidence. A theoretical derivation for justifying such a
Bias	(1) single bias	(2) bias in MLP
Similarity	0.99	0.89
Table 6: Similarity between center vector
and (1) single bias layer (bp), (2) the last
bias layer of MLP in the predictor.
high similarity as well as how this single bias layer prevents collapse are discussed in Appendix A.8.
6	Conclusion
We point out a hidden flaw in prior works for explaining the success of SimSiam and propose to
decompose the representation vector and analyze the decomposed components of extra gradient. We
find that its center vector gradient helps prevent collapse via the de-centering effect and its residual
gradient achieves de-correlation which also alleviates collapse. Our further analysis reveals that
InfoNCE achieve the two effects in a similar manner, which bridges the gap between SimSiam and
InfoNCE and contributes to a unified understanding of recent progress in SSL. Towards simplifying
the predictor we have also found that a single bias layer is sufficient for preventing collapse.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work was partly supported by Institute for Information & communications Technology Plan-
ning & Evaluation (IITP) grant funded by the Korea government (MSIT) under grant No.2019-0-
01396 (Development of framework for analyzing, detecting, mitigating of bias in AI model and
training data), No.2021-0-01381 (Development of Causal AI through Video Understanding and Re-
inforcement Learning, and Its Applications to Real Environments) and No.2021-0-02068 (Artificial
Intelligence Innovation Hub). During the rebuttal, multiple anonymous reviewers provide valuable
advice to significantly improve the quality of this work. Thank you all.
References
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019.
Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.
Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using
an internal-gan. NeurIPS, 2019.
Jane Bromley, James W Bentz, Leon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard
Sackinger, and Roopak Shah. Signature verification using a “Siamese” time delay neural network.
International Journal of Pattern Recognition and Artificial Intelligence, 1993.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML, 2020a.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. ICCV, 2021.
Victor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library
of self-supervised methods for visual representation learning, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 2019.
Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand
Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-
covariance image transformers. arXiv preprint arXiv:2106.09681, 2021.
Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-
supervised representation learning. In ICML. PMLR, 2021.
Abe Fetterman and Josh Albrecht. Understanding self-supervised and contrastive learning with
”bootstrap your own latent” (byol), 2020.
https://untitled-ai.github.io/
understanding-self-supervised-
contrastive-learning.html.
10
Published as a conference paper at ICLR 2022
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural
Information Processing Systems, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. In ICLR, 2020.
Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and
Jianfeng Gao. Efficient self-supervised vision transformers for representation learning. arXiv
preprint arXiv:2106.09785, 2021.
Ping Nie, Yuyu Zhang, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. Dc-bert: De-
coupling question and document for efficient contextual encoding. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information Retrieval,
2020.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 2019.
Pierre H Richemond, Jean-Bastien Grill, Florent Altche, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. Byol works even without
batch statistics. arXiv preprint arXiv:2010.10241, 2020.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2015.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS,
2016.
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. {VL}-{bert}:
Pre-training of generic visual-linguistic representations. In ICLR, 2020.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019.
Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021.
Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR, 2021.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In CVPR, 2018.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and StePhane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021.
Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Towards un-
derstanding and simplifying moco: Dual temperature helps contrastive learning without many
negative samples. In CVPR, 2022.
11
Published as a conference paper at ICLR 2022
A Appendix
A.1 Experimental settings
Self-supervised encoder training: Below are the settings for self-supervised encoder training. For
simplicity, we mainly use the default settings in a popular open library termed solo-learn (da Costa
et al., 2021).
Data augmentation and normalization: We use a series of transformations including Ran-
domResizedCrop with scale [0.2, 1.0], bicubic interpolation. ColorJitter (brightness (0.4), contrast
(0.4), saturation (0.4), hue (0.1)) is randomly applied with the probability of 0.8. Random gray
scale RandomGrayscale is applied with p = 0.2 Horizontal flip is applied with p = 0.5 The images
are normalized with the mean (0.4914, 0.4822, 0.4465) and Std (0.247, 0.243, 0.261).
Network architecture and initialization: The backbone architecture is ResNet-18. The projection
head contains three fully-connected (FC) layers followed by Batch Norm (BN) and ReLU, for which
ReLU in the final FC layer is removed, i.e. FC1 +BN+ReLU+FC2+BN +ReLU +F C3 +BN.
All projection FC layers have 2048 neurons for input, output as well as the hidden dimensions. The
predictor head includes two FC layers as follows: FC1 + BN + ReLU + F C2. Input and output
of the predictor both have the dimension of 2048, while the hidden dimension is 512. All layers of
the network are by default initialized in Pytorch.
Optimizer: SGD optimizer is used for the encoder training. The batch size M is 256 and
the learning rate is linearly scaled by the formula lr × M/256 with the base learning rate lr set
to 0.5. The schedule for learning rate adopts the cosine decay as SimSiam. Momentum 0.9 and
weight decay 1.0 × 10-5 are used for SGD. We use one GPU for each pre-training experiment.
Following the practice of SimSiam, the learning rate of the predictor is fixed during the training.
We use warmup training for the first 10 epochs. If not specified, by default we train the model for
1000 epochs.
Online linear evaluation: For the online linear revaluation, we also follow the practice in
the solo-learn library (da Costa et al., 2021). The frozen features (2048 dimensions) from the
training set are extracted (from the self-supervised pre-trained model) to feed into a linear classifier
(1 FC layer with the input 2048 and output of 100). The test is performed on the validation set. The
learning rate for the linear classifier is 0.1. Overall, we report Top-1 accuracy with the online linear
evaluation in this work.
A.2 Two sub-problems in AO of SimSiam
In the sub-problem ηt J argmi% L(θt, η), ηt indicating latent representation of images at step t
is actually obtained through ηX J ET	(T(x))], where they in practice ignore ET[∙] and sample
only one augmentation T0, i.e. ηxt J Fθt (T 0(x)). Conceptually, Chen & He equate the role of
predictor to EOA.
A.3 Experimental details for Explicit EOA in Table 1
In the Moving average experiment, we follow the setting in SimSiam (Chen & He, 2021) without
predictor. In the Same batch experiment, multiple augmentations, 10 augmentations for instance,
are applied on the same image. With multi augmentations, we get the corresponding encoded rep-
resentation, i.e. zi, i ∈ [1, 10]. We minimize the cosine distance between the first representation z1
and the average of the remaining vectors, i.e. W= 9 P1=2 z%. The gradient stop is put on the aver-
aged vector. We also experimented with letting the gradient backward through more augmentations,
however, they consistently led to collapse.
12
Published as a conference paper at ICLR 2022
A.4 Experimental setup and result trend for Table 2.
Mirror SimSiam. Here we provide the pseudocode for Mirror SimSiam. In the Mirror SimSiam ex-
periment which relates to Fig. 1 (c). Without taking symmetric loss into account, the pseudocode is
shown in Algorithm 1. Taking symmetric loss into account, the pseudocode is shown in Algorithm 2.
Algorithm 1 Pytorch-like Pseudocode: Mirror SimSiam
#	f: encoder (backbone + projector)
#	h: predictor
for X in loader: # load a minibatch X with n samples
x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections
p_b = h(z_b.detach()) # detach z_b but still allowing gradient p_b
L = D_cosine(z_a, p_b) # loss
L.backward() # back-propagate
update(f, h) # SGD update
def D_cosine(z, p): # negative cosine similarity
Z = normalize(z, dim=1) # l2-normalize
P = normalize(p, dim=1) # l2-normalize
return -(z*p).sum(dim=1).mean()
Algorithm 2 Pytorch-like Pseudocode: Mirror SimSiam
# f: encoder (backbone + projector)
# h: predictor
for x in loader: # load a minibatch X with n samples
x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections
p_b = h(z_b.detach()) # detach z_b but still allowing gradient p_b
p_a = h(z_a.detach()) # detach z_a but still allowing gradient p_a
L = D_cosine(z_a, p_b)/2 + D_cosine(z_b, p_a)/2 # loss
L.backward() # back-propagate
update(f, h) # SGD update
def D_cosine(z, p): # negative cosine similarity
z = normalize(z, dim=1) # l2-normalize
P = normalize(p, dim=1) # l2-normalize
return -(z*p).sum(dim=1).mean()
Symmetric Predictor. To implement the SimSiam with Symmetric Predictor as in Fig. 2 (b), we
can just perceive the predictor as part of the new encoder, for which the pseudocode is provided in
Algorithm 3. Alternatively, we can additionally train the predictor similarly as that in SimSiam, for
which the training involves two losses, one for training the predictor and another for training the
new encoder (the corresponding pseudocode is provided in Algorithm 4). Moreover, for the second
implementation, we also experiment with another variant that fixes the predictor while optimizing
the new encoder and then train the predictor alternatingly. All of them lead to collapse with a similar
trend as long as the symmetric predictor is used for training the encoder. For avoiding redundancy,
in Fig. 8 we only report the result of the second implementation.
Result trend. The result trend of SimSiam, Naive Siamese, Mirror SimSiam, Symmetric Predictor
are shown in Fig. 8. We observe that all architectures lead to collapse except for SimSiam. Mirroe
SimSiam was stopped in the middle because a NaN value was returned from the loss.
A.5 Experimental details for inverse predictor.
In the inverse predictor experiment which relates to Fig. 2 (c), we introduce a new predictor which
has the same structure as that of the original predictor. The training loss consists of 3 parts: predictor
training loss, inverse predictor training and new encoder (old encoder+predictor) training. The new
13
Published as a conference paper at ICLR 2022
Algorithm 3 Pytorch-like Pseudocode: Symmetric Predictor
#	f: encoder (backbone + projector)
#	h: predictor
for X in loader: # load a minibatch X with n samples
x_a,	x_b	=	aug(x),	aug(x)	#	augmentation
z_a,	z_b	=	f(x_a),	f(x_b)	#	projections
p_a,	p_b	=	h(z_a),	h(z_b)	#	predictions
L = D(P_a,	p_b)/2 + D(P_b, p_a)/2 # loss
L.backward() # back-propagate
update(f, h) # SGD update
def D(p, z): # negative cosine similarity
Z = z.detach() # stop gradient
P = normalize(p, dim=1) # l2-normalize
z = normalize(z, dim=1) # l2-normalize
return -(p*z).sum(dim=1).mean()
Algorithm 4 Pytorch-like Pseudocode: Symmetric Predictor (with additional training on predictor)
#	f: encoder (backbone + projector)
#	h: predictor
for x in loader: # load a minibatch X with n samples
x_a,	x_b	=	aug(x),	aug(x)	#	augmentation
z_a,	z_b	=	f(x_a),	f(x_b)	#	projections
p_a,	p_b	=	h(z_a),	h(z_b)	#	predictions
d_p_a, d_p_b = h(z_a.detach()), h(z_b.detach()) # detached predictor output
#	predictor training loss
L_pred = D(d_p_a, z_b)/2 + D(d_p_b, z_a)/2
#	encoder training loss
L_enc =D(P_a, d_p_b)/2 + D(P_b, d_p_a)/2
L = L_pred + L_enc
L.backward() # back-propagate
update(f, h) # SGD update
def D(p, z): # negative cosine similarity with detach on Z
z = z.detach() # stop gradient
p = normalize(p, dim=1) # l2-normalize
Z = normalize(z, dim=1) # l2-normalize
return -(p*z).sum(dim=1).mean()
Epoch
Figure 8: Result trend of Naive Siamese, Mirror SimSiam, Symmetric Predictor.
encoder F consists of the old encoder f + predictor h. The practice of gradient stop needs to be
considered in the implementation. We provide the pseudocode in Algorithm 5.
14
Published as a conference paper at ICLR 2022
Algorithm 5 Pytorch-like Pseudocode: Trainable Inverse Predictor
#	f: encoder (backbone + projector)
#	h: predictor
#	h_inv: inverse predictor
for X in loader: # load a minibatch X with n samples
x_a,	x_b	=	aug(x),	aug(x)	#	augmentation
z_a,	z_b	=	f(x_a),	f(x_b)	#	projections
p_a,	p_b	=	h(z_a),	h(z_b)	#	predictions
d_p_a, d_p_b = h(z_a.detach()), h(z_b.detach()) # detached predictor output
#	predictor training loss
L_pred = D(d_p_a, z_b)/2 + D(d_p_b, z_a)/2 # to train h
inv_p_a, inv_p_b = h_inv(p_a.detach()), h_inv(p_b.detach()) # to train h_inv
#	inverse predictor training loss
L_inv_pred = D(inv_p_a, z_a)/2 + D(inv_p_b, z_b)/2
#	encoder training loss
L_enc = D(P_a, h_inv(p_b))/2 + D(P_b, h_inv(p_a))
L = L_pred + L_inv_pred + L_enc
L.backward() # back-propagate
update(f, h, h_inv) # SGD update
def D(p, z): # negative cosine similarity with detach on Z
Z = z.detach() # stop gradient
P = normalize(p, dim=1) # l2-normalize
z = normalize(z, dim=1) # l2-normalize
return -(p*z).sum(dim=1).mean()
A.6 Regularization loss
Following Zbontar et al. (2021), we compute covariance regularization loss of encoder output along
the mini-batch. The pseudocode for de-correlation loss calculation is put in Algorithm 6.
Algorithm 6 Pytorch-like Pseudocode: De-correlation loss
#	Z_a: representation vector
#	N: batch size
#	D: the number of dimension for representation vector
Z_a = Z_a - Z_a.mean(dim=0)
cov = Z_a.T @ Z_a / (NT)
diag = torch.eye(D)
loss = cov["diag.bool()].pow_(2).sum() / D
A.7 Gradient derivation and temperature analysis for InfoNCE
With ∙ indicating the cosine similarity between vectors, the InfoNCE loss can be expressed as
exp(Za ∙ Zb/τ)
LInfoNCE = - log---- —— -------—TN----------——
exp(Za ∙ Zb∕τ) + Pi=I exp(Za ∙ Zi/τ)
=- log	exP(Za ∙ Zb/t)
Pi=0 exp(Za ∙ Zi∕τ),
(5)
where N indicates the number of negative samples and Z0 = Zb for simplifying the notation. By
treating Za ∙ Zi as the logit in a normal CE loss, we have the corresponding probability for each
negative sample as λi = PNxp(Za'Zi/；)/、, where i = 0,1,2,…,N and We have P= 0 λi = L
i=0=∣X exP(Zɑ' Zi/T )	=
15
Published as a conference paper at ICLR 2022
The negative gradient of the InfoNCE on the representation Za is shown as
-3LIfoNCE = 1(1 - λo)Zb-1 X λiZi
∂Za	τ	τ
i=1
1N
= TZb- ɪ2 λiZi)
τ
i=0
1N
=T (Zb -	λi(oz + ri))	⑹
1N
=，Zb + (-Oz- »ri)
τ
i=0
N
H Zb + (-oz - ɪ2 λiri)
i=0
where 1 can be adjusted through learning rate and is omitted for simple discussion. With Zb as the
basic gradient, Ge = -oz - PiN=0 λiri, for which oe = -oz and re = - PiN=0 λiri.
When the temperature is set to a large value, λi = PNXP(ZayT	, approaches N+ι, indicated
i i=O p( a i/	)
by a high entropy value (see Fig. 7). InfoNCE will degenerate to a simple contrastive loss, i.e.
Lsimple = -Za ∙ Zb + N+ι PN=o Za ∙ Zi , WhiCh repulses every negative sample with an equal
force. In contrast, a relative smaller temperature will give more relative weight, i.e. larger λ, to
negative samples that are more similar to the anchor (Za).
The influence of the temperature on the covariance and accuracy is shown in Fig. 7 (b) and (c).
We observe that a higher temperature tends to decrease the effect of de-correlation, indicated by
a higher covariance value, which also leads to a performance drop. This verifies our hypothesis
regarding on how re in InfoNCE achieves de-correlation because a large temperature causes more
balanced weights λi, which is found to alleviate the effect of de-correlation. For the setup, we note
that the encoder is trained for 200 epochs with the default setting in Solo-learn for the SimCLR
framework.
A.8 Theoretical derivation for a single bias layer
With the cosine similarity loss defined as Eq 7 Eq 8:
cossim(a, b) = ɑ ，,
√ α2 ∙ b2
for which the derived gradient on the vector α is shown as
法Cossim(a, b) = kb| -Cossim(a, b) ∙泮.
(7)
(8)
The above equation is used as a prior for our following derivations. As indicated in the main
manuscript, the encoder output za is l2-normalized before feeding into the predictor, thus pa =
Za + bp , bp denotes the bias layer in the predictor. The cosine similarity loss (ignoring the symme-
try for simplicity) is shown as
Lcosine
-Pa ∙ Zb
Pa
l∣Pa Il
Zb
两
(9)
16
Published as a conference paper at ICLR 2022
The gradient on pa is derived as
∂Lcosine
∂pa
zb
㈤卜Hpak
—Cossim(Za, Zb) ∙ ∣∣ppa∣∣2
1
kpak
1
kpak
1
kpak
1
kpak
1
kpak
Zbk — Cossim(Za, Zb) •
Zb — Cossim(Za, Zb) ∙ Zap+bp)
Cossim(Za, Zb)
(Oz + rb)-------H H------- ∙ (Oz + ra +
kpa k
((θz + rb) — m ∙ (θz + ra + bp))
((1 — m)θz — mbp + r — m ∙ ra),
Where m = CoSSimZ。&.
Given that pa = Za + bp , the negative gradient on bp is the same as that on pa as
∂ LcoSine
∂bp
∂ LcoSine
∂pa
1
Ilpak
((1 — m)oz — mbp + rb - m ∙ ra).
(10)
(11)
We assume that the training is stable and the bias layer converges to a certain value When
—dcossim(Za,Zb) = 0. Thus, the converged bp satisfies the following constraint:
k11 ((1 — m)oz — mbp + rb — mra)) = 0
1—m 1
bp = m oz + m rb — ra .
(12)
With a batch of samples, the average of mmrb and ra is expected to be close to 0 by the definition of
residual vector. Thus, the bias layer vector is expected to converge to:
1—m
bp =---oz
m
(13)
Rational behind the high similarity between bp and Oz. The above theoretical derivation shows
that the parameters in the bias layer are excepted to converge to a vector Imm oz. This theoretical
derivation justifies why the empirically observed cosine similarity between bp and Oz is as high as
0.99. Ideally, it should be 1, however, such a small deviation is expected with the training dynamics
taken into account.
Rational behind how a single bias layer prevents collapse. Given that pa = Za + bp , the negative
gradient on Za is shown as
∂ LcoSine	∂ LcoSine
—：-----=-------：----
∂ Za	∂ pa
— Cossim(Za, Zb)
Za + bp∖
)
(14)
1 Z	Cossim(Za, Zb) Z	Cossim(Za, Zb) b
时 Zb ^k- Za ^k- bp.
Here, we highlight that since the loss -Za ∙ Za = -1 is a constant having zero gradients on the en-
coder, —CoSSim(Za,Zb) Za can be seen as a dummy term. Considering Eq 13 and m
kpa k
CoSSim(Za ,Zb )
l∣Pak
17
Published as a conference paper at ICLR 2022
We have b = (CossimZ Zb)- 1)θz. The above equation is equivalent to
cosine
∂zar
kp10k Zb-
A Zb-
两Zb-
cossim(Za, Zb)
-kpθk2-
bp
cossim(Za, Zb)
-kPαk2-
(	kPa k
cossim(Za, Zb)
- 1)oz
H Zb - (1 -
-l-(i -
kPak(
cossim(Za, Zb)
IlPak
(15)
cossim(Za, Zb)
一两一)oz.
With Zb as the basic gradient, the extra gradient component Ge = -(1 — Cossim(Za,Zb) )θz. Given
that Pa = Za+ bp and IZaI = 1, thus IPaI < 1 only When Za is negatively correlated With bp . In
practice, hoWever, Za and bp are often positively correlated to some extent due to their shared center
vector component. In other Words, IPaI > 1. Moreover, cossim(Za, Zb) is smaller than 1, thus
-(1 - Cossim(Za,Zb)) < 0, suggesting Ge consists of negative Oz with the effect of de-centerization.
This above derivation justifies the rationale Why a single bias layer can help alleviate collapse.
B Discussion: does BN help avoid collapse
0	50	100	150	200
Epoch
Figure 9: BN with MSE helps prevent collapse without predictor or stop gradient. Its performance,
however, is inferior to the cosine loss-based SimSiam (with predictor and stop gradient).
To our knowledge, our work is the first to revisit and refute the explanatory claims in (Chen & He,
2021). Several works, however, have attempted to demystify the success of BYOL (Grill et al.,
2020), a close variant of SimSiam. The success has been ascribed to BN in (Fetterman & Albrecht,
2020), however, (Richemond et al., 2020) refutes their claim. Since the role of intermediate BNs is
ascribed to stabilize training (Richemond et al., 2020; Chen & He, 2021), we only discuss the final
BN in the SimSiam encoder. Note that with our Conjecture1, the final BN that removes the mean
of representation vector is supposed to have de-centering effect. BY default SimSiam has such a
BN at the end of its encoder, however, it still collapses with the predictor and stop gradient. Why
would such a BN not prevent collapse in this case? Interestingly, we observe that such BN can help
alleviate collapse with a simple MSE loss (see Fig. 9), however, its performance is is inferior to the
cosine loss-based SimSiam (with predictor and stop gradient) due to the lack of the de-correlation
effect in SimSiam. Note that the cosine loss is in essence equivalent to a MSE loss on the l2-
normalized vectors. This phenomenon can be interpreted as that the l2-normalization causes another
mean after the BN removes it. Thus, with such l2-normalization in the MSE loss, i.e. adopting the
default cosine loss, it is important to remove the oe from the optimization target. The results with
the loss of -Za ∙ sg(Zb + Oe) in Table 3 show that this indeed prevents collapse and verifies the
above interpretation.
18