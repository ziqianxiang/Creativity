Published as a conference paper at ICLR 2022
Neural Spectral Marked Point Processes
Shixiang Zhu*, Haoyun Wangt, Zheng Dongt, Xiuyuan Cheng?, Yao Xiet
f Georgia Institute of Technology
? Duke University
yao.xie@isye.gatech.edu
Ab stract
Self- and mutually-exciting point processes are popular models in machine learning
and statistics for dependent discrete event data. To date, most existing models
assume stationary kernels (including the classical Hawkes processes) and simple
parametric models. Modern applications with complex event data require more
general point process models that can incorporate contextual information of the
events, called marks, besides the temporal and location information. Moreover,
such applications often require non-stationary models to capture more complex
spatio-temporal dependence. To tackle these challenges, a key question is to devise
a versatile influence kernel in the point process model. In this paper, we introduce
a novel and general neural network-based non-stationary influence kernel with
high expressiveness for handling complex discrete events data while providing
theoretical performance guarantees. We demonstrate the superior performance of
our proposed method compared with the state-of-the-art on synthetic and real data.
1 Introduction
Event sequence data are ubiquitous in our daily life, ranging from traffic incidents, 911 calls, social
media posts, earthquake catalog data, and COVID-19 data (see, e.g., Bertozzi et al. (2020)). Such
data consist of a sequence of events indicating when and where each event occurred, with additional
descriptive information (called marks) about the event (such as category, volume, or free-text). The
distribution of events is of scientific and practical interest, both for prediction purposes and for
inferring events’ underlying generative mechanism.
A popular framework for modeling events is point processes (Daley & Vere-Jones, 2008), which
can be continuous over time and the space of marks. An important aspect of this model is capturing
the event’s triggering effect on its subsequent events. Since the distribution of point processes is
completely specified by the conditional intensity function (the occurrence rate of events conditioning
on the history), such triggering effect has been captured by an influence kernel function embedded in
the conditional intensity. In statistical literature, the kernel function usually assumes a parametric
form. For example, the original work by Hawkes (Hawkes, 1971) considers an exponential decaying
influence function over time, and the seminal work (Ogata, 1998) introduces epidemic-type aftershock
sequence (ETAS) model, which considers an influence function that exponentially decays over space
and time. With the increasing complexity of modern applications, there has been much recent effort in
developing recurrent neural network (RNN)-based point processes, leveraging the rich representation
power of RNNs (Du et al., 2016; Mei & Eisner, 2017; Xiao et al., 2017b).
However, there are several limitations of existing RNN-based models. First, such models typically
do not consider the kernel function (Du et al., 2016; Li et al., 2018; Mei & Eisner, 2017; Upadhyay
et al., 2018; Xiao et al., 2017a;b); thus, the RNN approach does not enjoy the interpretability of the
kernel function based models. Second, the popular RNN models such as Long Short-Term Memory
(LSTM) (Hochreiter & Schmidhuber, 1997) still implicitly discounts the influence of events over
time (due to their recursive structure) (Vaswani et al., 2017; Zhu et al., 2021d). Such assumptions
may not hold in many real-world applications. Take the earthquake catalog as an example, which
is a typical type of discrete event data; most aftershocks occur along the fault plane or other faults
within the volume affected by the mainshock’s strain (Zhu et al., 2021b). This means that different
regions may be correlated to their surrounding area differently according to their geological structure,
which creates a complex non-stationary spatial profile that we would like to capture through the
model. Third, a majority of the existing works mainly focus on one-dimensional temporal point
1
Published as a conference paper at ICLR 2022

200
400
600
800
1000
200	400	600	800	1000
0.06
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
-0.1
0
-0.05
-0.1
0
-0.05
-0.1
0
tf=150
0
-0.05
-0.1
500	1000 I
t'=250
500	1000
⅛,=350
0
-0.05
-0.1
500	1000	0	500	1000
Figure 1: An example of non-stationary influence kernel k(t0, t) of event time t0 and future time t > t0.
processes. Although there are works on marked point processes (Du et al., 2016; Mei & Eisner, 2017;
Reinhart, 2018), they are primarily based on simplifying assumptions that the marks are conditionally
independent of the event’s time and location, which is equivalent to assuming the kernel is separable;
these assumptions may fail to capture some complex non-stationary, time- and location-dependent
triggering effects for various types of events, as observed for many real-world applications (see, e.g.,
(Bertozzi et al., 2020)).
Contribution. In this paper, we present a novel general non-stationary point process model based
on neural networks, referred to as the neural spectral marked point process (NSMPP). The key
component is a new powerful representation of the kernel function using neural networks, which
enables us to go beyond stationarity (and thus go beyond standard Hawkes processes) and has the
capacity to model high-dimensional marks. Figure 1 gives an example of non-stationary influence
kernel that measures the influence of the past events to the future time t. The premise of the model
design is that the conditional intensity function uniquely specifies the distribution of the point process,
and the most important component in the intensity function is the influence kernel. In summary, the
novelty of our approach includes the following:
•	The kernel function is represented by a spectral decomposition of the influence kernel with a
finite-rank truncation in practice. Such a kernel representation will enable us to capture the most
general non-stationary process as well as high-dimensional marks. The model also allows the
distribution of marks to depend on time, which is drastically different from the separable kernels
considered in the existing literature (Reinhart, 2018).
•	The spectral decomposition of asymmetric influence kernel consists of a sum of the product
of feature maps, which can be parameterized by neural networks. This enable us to harvest
the powerful expressiveness and scalability to high-dimensional input of neural networks for
complicated tasks involving discrete events data.
•	We establish theoretical guarantees of the maximum likelihood estimate for the true kernel function
based on functional variational analysis and finite-dimensional asymptotic analysis, which shed
light on theoretical understanding of neural network-based kernel functions.
•	Using synthetic and real data (seismic and police data), we demonstrate the superior performance
of our proposed method in complex situations; the performance gain is particularly outstanding for
cases involving non-stationary point processes.
Related work. Seminal works in point processes modeling (Ogata, 1988; 1998) assume parametric
forms of the intensity functions. Such methods enjoy good interpretability and are efficient to estimate.
However, classical parametric models are not expressive enough to capture the events’ dynamics in
modern applications.
Recent research interests aim to improve the expressive power of point process models, where a
recurrent neural networks (RNNs)-based structure is introduced to represent the conditional intensity
function (Du et al., 2016; Mei & Eisner, 2017; Xiao et al., 2017b). However, most of these works
either explicitly or implicitly specify the inter-event dependence in a limited form with restrained
representative power. For example, Du et al. (2016) expresses the influence of two consecutive events
in a form of exp{w(ti+1 - ti)}, which is an exponential function with respect to the length of the
time interval ti+1 - ti with weights w ; Mei & Eisner (2017) enhances the expressiveness of the
model and represents the entire history using the hidden state of an LSTM, which still implicitly
assumes the influence of the history decays over time due to the recurrent structure of LSTM.
Another line of research uses neural networks to directly model dependence of sequential events
without specifying the conditional intensity function explicitly (Li et al., 2018; Xiao et al., 2017a).
Some studies consider non-stationary influence kernel using neural networks in spatio-temporal
2
Published as a conference paper at ICLR 2022
point processes (Zhu et al., 2021b;a). Recent work (Omi et al., 2019) also uses a neural network to
parameterize the hazard function, the derivative of which gives the conditional intensity function.
However, the above approaches either capture the temporal dependence or assume the temporal and
mark dependence are separable rather than jointly accounting for marked-temporal dependence.
Recently, attention models have become popular in computer vision and sequential data modeling
(Britz et al., 2017; Luong et al., 2015; Vaswani et al., 2017). This motivates works including
Zhang et al. (2019); Zhu et al. (2021c;d); Zuo et al. (2020) to model the conditional intensity of
point processes using the attention mechanism and characterize the inter-event dependence by a
score function. The attention mechanism has proven to be more flexible in capturing long-range
dependence regardless of how far apart two events are separated and greatly enhances the performance
in practice. However, the main limitation of Zhang et al. (2019); Zuo et al. (2020) is that they rely
on a conventional score function - dot-product between linear mappings of events, which is still
limited in representing non-linear dependence between events for some applications. Zhu et al. (2019;
2021c;d) used a more flexible and general Fourier kernel as a substitution for the dot-product score;
however, the expressive power of the proposed Fourier kernel is still limited, and the spectrum of the
Fourier basis is represented by a generative neural network, which is difficult to learn in some cases
(Arjovsky & Bottou, 2017).
There are also works considering point processes with non-stationary intensities. Chen & Hall (2013)
proposed time-varying background intensity for point process, while we focus on the non-stationary
triggering kernel depicting complex events dependency. Remes et al. (2017; 2018) studied non-
stationary kernels combined with Gaussian processes, assuming specific structures of the kernels in
the Fourier domain. Such kernels are more restricted than ours since the nature of Gaussian processes
requires that the kernel is positive semidefinite.
2	Method
2.1	Background: Marked temporal point process
Marked temporal point processes (MTPPs) (Reinhart, 2018) consist of a sequence of events over time.
Each event is associated with a (possibly multi-dimensional) mark that contains detailed information
of the event, such as location, nodal information (if the observations are over networks, such as sensor
or social networks) categorical data, and contextual information (such as token, image, and text
descriptions). Let T > 0 be a fixed time-horizon, and M ⊆ Rd be the space of marks. We denote the
space of observation as X = [0, T ) × M and a data point in the discrete event sequence as
x = (t, m),	t ∈ [0, T), m ∈ M,
(1)
where t is the event time and m represents the mark. Let Nt be the number of events up to time t < T
(which is random), and Ht := {x1 , x2, . . . , xNt } denote historical events. Let N be the counting
measure on X, i.e., for any measurable S ⊆ X, N(S) = |HT ∩ S|. For any function f : X → R, the
integral with respect to the counting measure is defined as
f (x)dN(x) =	X	f(xi).
S	xi∈HT∩S
The events’ distribution in MTPPs can be characterized via the conditional intensity function λ(x),
which is defined to be the conditional probability of observing an event in the marked temporal space
X given the events’ history Ht(x), that is,
E (dN(x)Ht(χ)) = λ(x)dx.
(2)
Above, t(x) extracts the occurrence time of event x, and we omit the dependence on Ht(x) in the
notation of λ(x) for simplicity.
As self- and mutual-exciting point processes, Hawkes processes (Hawkes, 1971) have been widely
used to capture the mutual excitation dynamics among temporal events. The model assumes that
influences from past events are linearly additive towards the current event. The conditional intensity
function for a self-exciting point process takes the form of
λ[k](x) = μ +k(x0,x),
x0∈Ht(x)
(3)
3
Published as a conference paper at ICLR 2022
where μ > 0 stands for the background intensity, and the so-called “influence kernel" k : X×X → R
is crucial in capturing the influence of past events on the likelihood of event occurrence at the current
time. Here we use the notation [k] to stress the dependence of the conditional intensity function on
the kernel function k(x0, x). Written in the form of the integral over counting measure, we have that
λ[k](x)
μ +/
x0 ∈Xt(x)
k(x0, x)dN(x0),
(4)
where Xt is the subset of X with the first component smaller than t.
The most commonly made assumption in the literature is that the process is stationary, where the
influence of the past events is shift-invariant, such that k(x0, x) = f(x - x0) for a influence function
f : Rd → R+; a common influence function in one-dimensional cases is f(t) = α exp{-βt},
where β controls the decay rate and α > 0 controls the magnitude of the influence of an event. The
current work aims at going beyond stationary point processes, which enables us to better capture the
heterogeneity in the events’ influence across the spatial-temporal space, which naturally arises in
many applications.
2.2	Neural spectral representation for influence kernel
We propose to represent more general non-stationary influence kernels in the conditional intensity
function λ(x) specified in (4).
Kernel representation. The main idea of the proposed model is to represent the influence kernel k
using a general finite-rank decomposition
R
k(x0, x) =	νr ψr (x0)φr (x),	νr ≥ 0,	(5)
r=1
where
ψr : X → R,	φr : X → R, r = 1,…，R,
are two sets of feature functions in some smooth functional space F ⊂ C0 (X), and νr is the
corresponding weight - or “spectrum”. This representation is motivated by the spectral decomposition
of a general kernel function. While functional spectral decomposition is usually infinitely dimensional,
for practical considerations, we truncate the “spectrum” and only consider a finite rank representation.
Note that while we view (5) as similar to a spectral decomposition, it can be better understood as
feature maps in kernel representation, and, particularly, we do not need the feature functions ψr and
φr to be orthogonal.
The decomposition (5) represents the kernel function using three parts: two sets of (normalized)
feature functions and the energy spectrum—the spectrum νr plays the role of weights to combine the
feature maps. In the learning process, we can train the feature functions (typically neural networks)
and the weights separately; since learning the normalized feature maps tend to be more numerically
stable. The proposed form of kernel is not necessarily positive semi-definite, and even not symmetric.
Data Point	Hidden
■
So ftplus
- SoftP-Us 一
R
So ftplus
Embedding
Feature
φ or W
(a) Kernel architecture	(b) Network structure
Figure 2: (a) The architecture of the proposed non-stationary neural spectral kernel. A hidden embedding will
first summarize the data point; then, the embedding will be mapped to different features via multi-branch neural
networks. The kernel value is calculated by summing up the products between two sets of learned features, {ψr }
and {φr}, weighted by the spectrum {νr}. The spectrum, feature functions, and hidden embeddings are jointly
learned from data. (b) The structure of the multi-branch neural network. There are two shared layers with n
nodes per layer that generate the hidden embedding; d denotes the dimension of the input data point; p denotes
the dimension of the middle layer that generates the feature. Further specifications of neural networks will be
provided in the experiments in Section 4.
4
Published as a conference paper at ICLR 2022
Note that the spectral representation can allow for a completely general kernel that can be used for
non-stationary processes (since our influence function does not impose a shift-invariant structure).
Moreover, the spectral representation allows us to go beyond monotone decay or parametric form as
commonly assumed in the prior literature.
Neural network feature function. As one of the most salient features of our method, feature
functions {φr, ψr} are represented using neural networks, leveraging their known universal approxi-
mation power. First, the input data point x ∈ X will be projected to a hidden embedding space via
a multi-layer shared network, aiming to extract key information of the input data. Here we have
adopted Softplus non-linear activation in this feature extraction sub-network, while other options may
be possible. Next, the hidden embedding will be mapped to the different features {φr (x)} through R
branched sub-networks. To ensure the output feature is constrained in a bounded space, we choose
the scaled sigmoidal function as the activation of the output layer, i.e., f(x) = s/(1 + exp(-x)),
where s is a constant to enable rescaling of the output to a proper range (in our setting, we set s to
be 100). The overall architecture of our kernel formulation and the structure of the multi-branch
neural network are summarized in Figure 2 (a) and (b), respectively. Further specifications of neural
networks will be provided in the experiments in Section 4.
2.3	Maximum likelihood model recovery
An essential task in learning the neural point process model is to estimate the influence kernel
function for the point process. The reason is two-fold: First, the kernel function is the most important
component in representing the point process. Second, in practice, the influence kernel offers clear
interpretations, such as “how events at a particular time and location will influence future events at a
given time and location.” Such interpretation is essential for predicting using event data- one of the
main applications for point process models.
To estimate the influence kernel function for point process models, we consider a popular approach
through maximum likelihood estimation (MLE). Formally, the optimal kernel can be found by solving
the following optimization problem given M sequences of training event sequences over the time
horizon [0, T]: {xi,j}, i = 1, ..., Nj, j = 1, . . . , M:
max
k∈K
`[k] :=
log λj [k](x)dNj (x) -
λj [k](x)dx ,
(6)
where λj and Nj denote the conditional intensity and counting measure associated with the j-th
trajectory, respectively, and K ⊂ C0 (X × X ) represents the family of regular kernel functions
induced by the feature function family F and the finite-rank decomposition (5).
Learning algorithm. To solve MLE (6) when the kernel function k is parameterized by neural
networks, we use a stochastic gradient as summarized in Algorithm 1 (Appendix A). Note that to
calculate the log-likelihood function ` in (6), we need to evaluate an integral (the second term), which
does not have a closed-form expression. We approximate the integral numerically by Monte Carlo
integration - drawing samples and take the average, as described in Algorithm 2 (Appendix A).
3	Theoretical guarantees of MLE
We first consider the MLE as a functional optimization problem, since when using neural networks
to approximate the kernel function, we are interested in such functional approximation results. We
show that the expected log-likelihood reaches its maximum at the true kernel with the second-order
derivative bounded away from zero, and thus the true kernel function is identifiable by solving the
MLE problem under some regularity conditions.
Consider the log-likelihood of point processes over a family of kernel functions K which contains K,
the family induced by feature functions in F and non-negative spectrum {νr }R=ι. Note that K may
go beyond the finite-rank decomposition in (5). Later throughout the theoretical analysis, we omit the
spectrum as they can be absorbed into the feature functions. The details are discussed in Remark 3.2.
Assumption 3.1. (A1) The kernelfunctionfamily K ⊂ C0(X × X) which is uniformly bounded, and
the true kernel k* ∈ K; (A2) There exist ci, c2 positive constants, such that for any k ∈ K, a.s. for
event data trajectory, c1 ≤ λ[k](x) ≤ c2, ∀x ∈ X.
5
Published as a conference paper at ICLR 2022
Note that, apart from (A2), we only need kernel functions to be measurable for theoretical analysis
which is guaranteed by (A1). In practice, the proposed neural network parametrization leads to a
continuous and smooth (low-rank) kernel function, which induces non-singular λ.
We have the following lemma, which shows that when the log-likelihood function has a local
perturbation around the true kernel function, there is going to be a decrease in the log-likelihood
function value in expectation. Note We assume that k lies in (or can be well-approximated by) the
family of function class K - thanks to the well-known universal approximation power of neural
networks.
Lemma 3.1 (Local perturbation of likelihood function around the true kernel function). Under
Assumption 3.1,forany k ∈ K and δk = k 一 k, we have
'[k*] -'间 ≥ M {-XZX δλj(X) (λj[kj*(](X) 一 dx) + 2C2 XZXQj(X))2dNj(x)}, ⑺
where
δλj (X) :=	δk(X0, X)dNj (X0).
(8)
The implication of Lemma 3.1 is the following. Note that in (7), we have nicely decomposed the
difference in the likelihood function caused by a small perturbation around the true kernel function,
as two terms: the first term in (7) is a martingale integral (since the conditional expectation of
(dNj (χ)∕λj [k*](χ) - dx) is zero, ∀j), and the second term is the integral of a quadratic term against
the counting measure. The expectation of the first term is zero due to the property of the martingale
process. For the second term, per j ,
/ (δλj(x))2dNj(x) ≈ / (δλj(x))2λj(x)dx ≥ c1∕ (δλj(x))2dx,	(9)
and thus perturbation of the intensity function which corresponds to the second (quadratic) term in
(7) will be reflected in the decrease of the log-likelihood. Furthermore, we have the identifiability of
the kernel itself, as stated in the following theorem.
Theorem 3.2 (Kernel identifiability using maximum likelihood). Under Assumption 3.1, the true
kernel function k* is locally identifiable in that k* is a local minimum solution ofmaximum likelihood
(6) in expectation.
Remark 3.1 (Function identification and neural network parametrization). Theorem 3.2 derives a
result which holds for variational perturbation of the kernel function k in the possibly parametric
family K induced by the feature function family F . When F is the class of functions that a neural
network can represent, the perturbation δk is induced by the change of network parameters. It is
common that in neural network models, parameter identification is difficult to search for (e.g., due
to the symmetry of permuting hidden neurons); however, the neural network function identification
may still hold, e.g., under the mean-field approximation (Mei et al., 2018). Thus the kernel function
identification results in Theorem 3.2 is important when learning kernel functions by neural networks.
Remark 3.2 (Finite rank kernel representation). When assuming the parameter representation (5),
we represent the true kernel as k* (X0, X) = PrR=1 νrψr*(X0)φr*(X). For theoretical analysis purposes,
without loss of generality, we can assume Vr = 1, r = 1,…，R, since they can absorbed into the
feature functions. Consider a perturbed kernel where the feature functions are ψr = ψr + δψr and
φr = φr + δφr, and remains to satisfy Assumption 3.1. The kernel function variation δk in (8) can
then be written as δk(X0, X) = PrR=1 (δψr (X0)φr (X) + ψr (X0)δφr (X) + δψr(X0)δφr(X)). With a
rank-R representation of the kernel and smooth ψr , φr represented by neural networks, we potentially
prevent overfitting that leads to singular kernels, which may be a problem for the over-complete
kernel family K as in (A1). Later we show experimentally that the learned kernel is smooth and close
to the true kernel in Figure 3 and Figure 4.
We also studied the MLE for parameterized kernel in Appendix D, when the target feature function
belongs to a certain parametric function class with a finite number of parameters. This includes, for
instance, spline functions, functions represented by Fourier basis, and neural networks, which proves
that our proposed method acts as a fundamental framework of finite-rank and neural network kernel
representation.
6
Published as a conference paper at ICLR 2022
4	Numerical experiments
This section presents experimental results on both synthetic and real data and compares them with
several state-of-the-arts, including (i) standard Hawkes process with an exponentially decaying kernel
function (Hawkes) (Hawkes, 1971); (ii) recurrent marked temporal point processes (RMTPP) (Du
et al., 2016); and (iii) Neural Hawkes process (NH) (Mei & Eisner, 2017). See Appendix B for
a detailed review of those existing methods. To perform the experiments, we use the following
procedure: Given training data, We first estimate the kernel function to obtain k(∙, ∙) by solving the
maximum likelihood problem (6) using stochastic gradient descent, as described in Section 2.3. Note
that according to (2), the conditional intensity function can be treated as a prediction of the chance
of having an event at a given time t after observing the past events. Thus, to evaluate the prediction
performance on test data, given a test trajectory, We perform online prediction by feeding the past
events (in the test trajectory) into evaluating the conditional intensity function according to (3), Which
gives the conditional probability of a future event given the past observations.
Performance metrics. We consider tWo performance metrics: (i) The performance for synthetic data
is evaluated by measuring the out-of-sample mean-average-error (MAE) for conditional intensity
function. For synthetic data, We know the true kernel, which is denoted as k*. Given a sequence of
test data, let λ[k*] be the “true” conditional intensity function defined by (3) using the true kernel
k*, and let λ[k] be an estimated conditional intensity function for the same sequence using the
estimated kernel k. Thus, λ[k] can be viewed as a probabilistic prediction of the events (since it
specifies the likelihood of an event happening given the test trajectory’s history) when the kernel
k is estimated separately from training data. The out-of-sample MAE for one test trajectory is
defined as JX ∣λ[k*](x) - λ[k](x)∣dx. Then we average this over all test trajectories to obtain our
performance measure. (ii) For real data, since we do not know the true intensity function, we report
the average out-of-sample predictive log-likelihood. Given a test trajectory (information contained
in counting measure N(x)), the predictive likelihood for a trajectory using an estimated kernel k is
given as JX log λ[∕](x)dN(x) - JX λ[k](x)dx. The average out-of-sample predictive log-likelihood
is obtained by average over test trajectories. This has been widely adopted as a metric for real-data
(Mei & Eisner, 2017; Omi et al., 2019; Zhang et al., 2019; Zhu et al., 2021a;b;d): the higher the
log-likelihood, the better the model predicts. See all experiment configurations in Appendix B.
Synthetic data. In the experiment, we assume the background rate is a constant (μ = 1) and focus
on recovering both stationary and non-stationary kernel structures. We generate four one-dimensional
and three two-dimensional synthetic data sets, which are simulated by a vanilla Hawkes process
and our model described in Section 2.2 with randomly initialized parameters. The simulated data is
generated by the thinning algorithm described in Algorithm 3 (Appendix A), which is an accept-reject
method for simulating a point process (Daley & Vere-Jones, 2008; Gabriel et al., 2013). For ease of
comparison, we normalize the time and mark spaces for all data sets to the range from 0 to 100. Each
data set is composed of 1,000 event sequences with an averaged length of 121. We fit the models
using 80% of the synthetic data set and the remaining 20% as the testing set.
Kernel recovery. One of the most important ability of our approach is to recover the true kernel.
We report the kernel recovery results for one- and two-dimensional synthetic data sets, shown in
Figure 3 and Figure 4, respectively. In Figure 3, we observe that our proposed model can accurately
recover the true kernel for one-dimensional data. In contrast, the vanilla Hawkes process with a
stationary kernel failed to capture such non-stationarity. We also present results of two-dimensional
data in Figure 4, where a continuous one-dimensional mark is introduced to the events. The first three
columns demonstrate two-dimensional “slices” of the four-dimensional kernel evaluation, indicating
our model is also capable of recovering complex high-dimensional kernel structure. We note that
event points are sparsely scattered in the two-dimensional marked-temporal space, where the events’
correlation is measured in a high-dimensional (in our example, four-dimensional) kernel space.
Intensity prediction. We evaluate the predictive accuracy for conditional intensity λ[k](x) (3) using
estimated kernel k from the training data. Note that λ[k](x) is a probabilistic prediction since it can
be viewed as the instantaneous probability of an event given the historical events. The last panel
in Figure 3 shows the predicted one-dimensional conditional intensity functions given a sequence
randomly selected from the testing set. We compare our predicted λ[k](x) with the one estimated
by a vanilla Hawkes model with stationary kernel. The result shows that the predicted conditional
7
Published as a conference paper at ICLR 2022
D.Qe+QQ
-2.0^02
-4.0e02
-6.0^02
Figure 3: Kernel recovery results on a one-dimensional synthetic data set. The first three panels show the true
kernel that generates the data, kernel learned by our model, and kernel learned by a vanilla Hawkes process,
respectively. The fourth panel shows the true and predicted conditional intensity functions of a test sequence.
Figure 4: Kernel recovery results on a two-dimensional synthetic data set. Two rows presents the results of
the true model and our learned model, respectively. The first three columns show different snapshots of kernel
evaluation for each model; the last column shows their corresponding conditional intensity over marked-temporal
space given a test sequence, where the black dots indicate the location of the events.
Figure 5: Kernel recovery results on a one-dimensional synthetic data set generated by a vanilla Hawkes process
with a stationary exponentially decaying kernel. This experiment acts as a sanity check.
intensity λ[k](x) suggested by our model well matches that using the true kernel λ[k*](x). In
contrast, the Hawkes model only provides limited flexibility for modeling the non-stationary process.
Two panels in the last column of Figure 4 give another comparison between the true and predicted
two-dimensional conditional intensities over marked-temporal space. This result confirms that our
model can accurately predict the two-dimensional conditional intensity function. To validate the
robustness of our proposed method, we also test our model using another one-dimensional data set
generated by a vanilla Hawkes process with a stationary parametric kernel, as shown in Figure 5.
Though our model is evidently overparametrized for this data, our method is still able to recover
the kernel structure, as well as predict the conditional intensity accurately, which demonstrate the
adaptiveness of our model and confirm that our model can approximate the stationary kernel without
incurring overfitting. More results of kernel recovery experiments can be found in Appendix C.
Additionally, we compare our method with three other baselines on the synthetic data sets. Figure 6
shows the conditional intensities based on the true kernel λ[k*](x) (solid black lines) and predicted
intensity using each method. We observe that our method obtained much better predictive perfor-
mances compared to other baseline approaches. Table 1 summarizes two performance metrics for
these methods: the predictive log-likelihood and the MAE. The result shows that our method greatly
outperforms other baseline approaches in both metrics. In particular, the MAE of our method is at
least 90% lower than the other methods for both one-dimensional and two-dimensional data sets.
8
Published as a conference paper at ICLR 2022
Figure 6: Predicted conditional intensity using our method and other baselines for a sequence selected from test
data. We aggregate the conditional intensity in mark space for ease of presentation and visualize the average
conditional intensity over time for two-dimensional synthetic data on the third panel.
Table 1: Performance comparison of our method and other existing methods: Hawkes, RMTPP, and NH.
` / MAE1	，1D Synthetic 1	1D Synthetic 2	1D Synthetic 3	，2D Synthetic 1	2D Synthetic 2	2D Synthetic 3	,Earthquake (2D)	Robbery (1D)	#Parameters	Training/Testing time2
NSMPP	1 -17.68/0.24	-14.17 / 0.02	-21.94 / 0.02	1 -65.92/10.16	-87.02 / 0.20	-91.12 / 0.31	~-56.50 /NA-	-74.47 / NA	171,555	0.766 / 0.84
RMTPP	1 -29.84/3.27	-36.10 / 0.33	-89.56 / 0.34	1 -236.32/98.32	-320.01 / 20.97	-456.92 / 29.18	1	-218.39/NA	-132.55 /NA	274,168	0.245 / 7.29
NH	；-48.34/3.42	-52.97 / 0.41	-60.10 / 0.44	；-289.10/49.21	-219.74 / 12.45	-420.00 / 28.99	；-189.39/NA	-96.10 / NA	282,755	0.204 / 6.09
Hawkes	I -24.12/2.65	-77.36 / 0.52	-61.46 / 0.25	I NA/NA	NA / NA	NA / NA	I	NA/NA	-197.84 / NA	2	0.021 / <0.01
1 Each table’s entry of the data set includes the average predictive log-likelihood (`) and the MAE.
2 Training time represents the time for training one batch. Testing (inference) time refers to the time for predicting the conditional intensity function for a test sequence. Time is measured in seconds.
Real-data results. Finally, we test our method on two large-scale real data sets: (1) Atlanta 911
calls-for-service data. The data set contains the 911 calls-for-service data in Atlanta from 2015 to
2017. We extract 7,831 reported robberies from the data set since robbers usually follow a particular
modus operandi (M.O.). Each robbery report is associated with a timestamp indicating when the
robbery occurred. We consider each series of robberies as a sequence. (2) Northern California
seismic data. The Northern California Earthquake Data Center (NCEDC) provides public time series
data Northern California Earthquake Data Center. UC Berkeley Seismological Laboratory. Dataset
(2014) that comes from broadband, short period, strong motion seismic sensors, GPS, and other
geophysical sensors. We extract 16,401 seismic records with a magnitude larger than 3.0 from 1978
to 2018 in Northern California and partition the data into multiple sequences every quarter. We fit the
models using 80% of the data set and the remaining 20% as the testing set.
In Table 1, we report the out-of-sample predictive log-likelihood of each method since the ground truth
is not available for real data. We can see that our model attains the highest predictive log-likelihood
for both synthetic and real data sets. In particular, the better performance on real data and significantly
higher out-of-sample predictive log-likelihood achieved by our approach than other existing methods
show that the non-stationary kernel seemingly captures the nature of the real data better in these cases.
This shows the merit of our approach for real data where we do not know the ground truth.
Training / testing time. In Table 1, we compare the running time of our model and other baselines on
one-dimensional synthetic data set. The size and performance of each model are presented in Table 1.
The running time refers to wall clock time. We can observe that the training time of our model is
similar with other neural point process models (RMTPP and NH). Hawkes process model has the
minimum training time because it has only two parameters to be estimated. The testing (inference)
time of our model is significantly lower than the other two recurrent neural point process models.
5	Discussions
We have presented a new non-stationary marked point process model with a general kernel represented
by neural network feature maps, motivated by spectral decomposition of the kernel. The flexible kernel
with expressive power enabled by neural networks can capture complex dependence across temporal,
spatial, and mark spaces, which can be valuable for real-data modeling in various applications. The
benefits of our approach are demonstrated by superior out-of-sample predictive log-likelihood on
real data. The model can be learned efficiently with stochastic gradient descent. We also develop
a theoretical guarantee for maximum likelihood identifiability. Generally, model architectures may
well depend on specific tasks. For example, neural networks can be based on CNN if the high
dimensional markers are in image-like shapes and can also be LSTM or even BERT if the markers are
text-based. Thus the choice of the deep model architecture and optimization algorithm can be more
systematically explored, especially the comparison of non-stationary neural kernels to stationary
ones. Understanding and characterizing what kernels can be learned through such an approach is left
for future study. Also, when extending to high-dimensional mark space, more efficient algorithm is
needed for the computation of the log-likelihood.
9
Published as a conference paper at ICLR 2022
Acknowledgement
The work is supported by National Science Foundation (NSF) DMS-2134037. The works of S.Z.,
H.W., and Y.X. are supported by NSF CAREER Award, CCF CCF-1650913, DMS-1830210, DMS-
1938106. The work of X.C. is partially supported by the Alfred P. Sloan Foundation.
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017.
Andrea L Bertozzi, Elisa Franco, George Mohler, Martin B Short, and Daniel Sledge. The challenges
of modeling and forecasting the spread of covid-19. Proceedings of the National Academy of
Sciences,117(29):16732-16738, 2020.
Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. Massive exploration of neural machine
translation architectures. arXiv preprint arXiv:1703.03906, 2017.
Feng Chen and Peter Hall. Inference for a nonstationary self-exciting point process with an application
in ultra-high frequency financial data modeling. Journal of Applied Probability, 50(4):1006-1024,
December 2013. doi: 10.1239/jap/1389370096. URL https://doi.org/10.1239/jap/
1389370096.
D. J. Daley and D. Vere-Jones. An introduction to the theory of point processes. Vol. II. Probability and
its Applications (New York). Springer, New York, second edition, 2008. ISBN 978-0-387-21337-8.
General theory and structure.
Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.
Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’16, pp. 1555-1564, New York, NY, USA, 2016. Association for Computing Machinery.
ISBN 9781450342322. doi: 10.1145/2939672.2939875.
Edith Gabriel, Barry Rowlingson, and Peter Diggle. stpp: An r package for plotting, simulating and
analyzing spatio-temporal point patterns. Journal of Statistical Software, 53:1-29, 04 2013. doi:
10.18637/jss.v053.i02.
Alan G. Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika,
58(1):83-90, 04 1971. ISSN 0006-3444. doi: 10.1093/biomet/58.1.83. URL https://doi.
org/10.1093/biomet/58.1.83.
SePP Hochreiter and Jurgen SChmidhuber. Long short-term memory. Neural Comput, 9(8):
1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL
https://doi.org/10.1162/neco.1997.9.8.1735.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. Learning temporal point
processes via reinforcement learning. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, NIPS ’18, pp. 10804-10814, Red Hook, NY, USA, 2018.
Curran Associates Inc.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based
neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating
multivariate point process. In Advances in Neural Information Processing Systems 30, pp. 6754-
6764. Curran Associates, Inc., 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
10
Published as a conference paper at ICLR 2022
Northern California Earthquake Data Center. UC Berkeley Seismological Laboratory. Dataset.
NCEDC, 2014. URL https://ncedc.org.
Yosihiko Ogata. Statistical models for earthquake occurrences and residual analysis for point
processes. Journal OftheAmerican Statistical association, 83(401):9-27, 1988.
Yosihiko Ogata. Space-time point-process models for earthquake occurrences. Annals of the Institute
of Statistical Mathematics, 50(2):379-402, 1998.
Takahiro Omi, naonori ueda, and Kazuyuki Aihara. Fully neural network based model for general
temporal point processes. In Advances in Neural Information Processing Systems 32, pp. 2120-
2129. Curran Associates, Inc., 2019.
Alex Reinhart. A review of self-exciting spatio-temporal point processes and their applications.
Statistical Science, 33(3):299-318, 2018.
Sami Remes, Markus Heinonen, and Samuel Kaski. Non-stationary spectral kernels. arXiv preprint
arXiv:1705.08736, 2017.
Sami Remes, Markus Heinonen, and Samuel Kaski. Neural non-stationary spectral kernel. arXiv
preprint arXiv:1811.10978, 2018.
Utkarsh Upadhyay, Abir De, and Manuel Gomez Rodriguez. Deep reinforcement learning of marked
temporal point processes. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 3168-3178.
Curran Associates, Inc., 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕 Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017.
Halbert White. Maximum likelihood estimation of misspecified models. Econometrica: Journal of
the econometric society, pp. 1-25, 1982.
Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, and Hongyuan Zha. Wasserstein
learning of deep generative point process models. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, NIPS ’17, pp. 3250-3259, Red Hook, NY,
USA, 2017a. Curran Associates Inc. ISBN 9781510860964.
Shuai Xiao, Junchi Yan, Xiaokang Yang, Hongyuan Zha, and Stephen M. Chu. Modeling the intensity
function of point process via recurrent neural networks. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence, AAAI ’17, pp. 1597-1603. AAAI Press, 2017b.
Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. Self-attentive hawkes processes, 2019.
Shixiang Zhu, Henry Shaowu Yuchi, Minghe Zhang, and Yao Xie. Sequential adversarial anomaly
detection for one-class event data. arXiv preprint arXiv:1910.09161, 2019.
Shixiang Zhu, Ruyi Ding, Minghe Zhang, Pascal Van Hentenryck, and Yao Xie. Spatio-temporal point
processes with attention for traffic congestion event modeling. IEEE Transactions on Intelligent
Transportation Systems, 2021a.
Shixiang Zhu, Shuang Li, Zhigang Peng, and Yao Xie. Imitation learning of neural spatio-temporal
point processes. IEEE Transactions on Knowledge and Data Engineering, pp. 1-1, 2021b. doi:
10.1109/TKDE.2021.3054787.
Shixiang Zhu, Henry Shaowu Yuchi, Minghe Zhang, and Yao Xie. Sequential adversarial anomaly
detection with deep fourier kernel. In ICASSP 2021-2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 3345-3349. IEEE, 2021c.
Shixiang Zhu, Minghe Zhang, Ruyi Ding, and Yao Xie. Deep fourier kernel for self-attentive point
processes. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International
Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine
Learning Research, pp. 856-864. PMLR, 13-15 Apr 2021d. URL http://proceedings.
mlr.press/v130/zhu21b.html.
11
Published as a conference paper at ICLR 2022
Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer hawkes process.
In International Conference on Machine Learning, pp. 11692-11702. PMLR, 2020.
12
Published as a conference paper at ICLR 2022
A	Algorithms
Algorithm 1: Stochastic gradient-based learning algorithm
Input: X = {xj}j=1,...,N is the training set with N sequences; η is the number of learning
iterations; γ is the learning rate; M is the size of a mini-batch;
Initialization: model parameters θ0 ;
l — 0;
while l < η do
Randomly draw M sequences from X denoted as Xbl = {xj }j=1,...,M ⊂ X;
Θ1+1 ― θι + γ∂'∕∂θι given Xbi；
l - l + 1;
end
Algorithm 2: Monte Carlo estimation for the integral of conditional intensity in (6)
Input: λ denotes the model that we need to evaluate; xj = {(ti , mi)}iN=j1 is an event sequence,
where Nj is the number of events in the sequence; N is the number of samples uniformly drawn
from X; Λ is the integral of the conditional intensity over the data space;
Λ - 0;
i'	ι	τΓr ι
for n = 1, . . . , N do
Draw Xn := (tn, mn)〜X；
Ht(Xn)《-{ (ti, mi ) ∈ xj : ti < tn };
Λ - Λ + λj (Xn) given Ht(Xn)
end
Λ 一 ∣X∣Λ∕N;
Algorithm 3: Efficient thinning algorithm for simulating point process
input θ,T, M;
output A set of events Ht ordered by time.;
Initialize Ht = 0, t = 0, m 〜 Uniform(M);
while t < T do
Sample U 〜 Uniform(0,1); m 〜 Uniform(M); D 〜 Uniform(0,1);
χ0 J (t, m0); X J λ(x0∣Ht);
t J— t ——ln u/λ;
X J (t, m); λ J λ(x∣Ht);
if DλX > eλ then
I Ht J Ht ∪ {(t, m)}; m0 J m;
end
end
B Experimental setting and baseline methods
Now we describe the experiment configurations: We set the rank R = 5 in our setting. We consider
the shared network that summarizes input data into a hidden embedding to be a fully connected
three-layer network and the sub-network to be a fully connected network with two hidden layers.
The width of the hidden layers in the shared network is n = 128, and the width of the input layers
in sub-networks (or the output layer in the shared network) is p = 10. We adopt the SoftPlus
f(X) = 1∕ log(1 + exp(X)) as the activation function of each layer in the network. To learn the
model’s parameters, we adopt the Adam optimizer Kingma & Ba (2014) with a constant learning
rate of 10-2 and the batch size is 32. All experiments are performed on Google Colaboratory (Pro
13
Published as a conference paper at ICLR 2022
version) with 12GB RAM and dual-core Intel processors, which speed up to 2.3 GHz (without GPU).
Codes to reproduce the experimental results are publicly available1.
This study considers three baseline methods: (1) Standard Hawkes process with an exponentially
decaying kernel function (Hawkes): it specifies the conditional intensity function as λ(t) = μ +
α Pt <t β exp{-β(t - tj)}, where parameters μ, α, β can be estimated via maximizing likelihood
(Hawkes, 1971); (2) Recurrent marked temporal point processes (RMTPP): it assumes the conditional
intensity function λ(t) = exp v>hj + ω(t - tj) + b , where the j-th hidden state hj in the RNN
represents the history influence up to the nearest happened event j, and w(t - tj) represents the
current influence; the v, ω, b are trainable parameters (Du et al., 2016); and (3) Neural Hawkes
process (NH): it specifies the conditional intensity function as λ*(t) = f (V>ht), where ht is the
hidden state of a continuous-time LSTM up to time t representing the history influence, and the f (∙)
is a SoftPlus function which ensures the positive output given any input (Mei & Eisner, 2017).
We note that, unlike the standard Hawkes process and our NSMPP, RMTPP and NH do not parameterize
the kernel function directly; instead, they aim to model the conditional intensity using an LSTM-based
structure. Particularly, these models pass the history information sequentially via a hidden state,
where the recent memory will override the long-term memory. This has led RMTPP and NH to
“overemphasize" the recent events and therefore assume the temporal correlation would monotonically
decrease over time. In addition, RMTPP and NH can only deal with one-dimensional categorical
marks, while our model can be extended to high-dimensional continuous mark space. To ensure
comparability, we only consider one- and two-dimensional event consisting of time and mark in our
experiment (d ∈ {1, 2}). For RMTPP and NH, the event’s mark will be discretized and treated as
categorical input.
C	Additional experimental results
In this section, we present additional experiments to demonstrate the robustness of our approach by
considering fitting to stationary exponentially decaying kernel and other non-stationary kernels and
by varying the neural network architecture and training sample sizes.
Figure 7 and Figure 8 show the results on one- and two-dimensional data sets with non-stationary
kernels generated by model described in Section 2.2. Note that compared to the Hawkes models
with a stationary kernel, our model can capture the non-stationarity of the kernel and predict the
conditional intensity function λ more accurately. Besides, our proposed method also recovers the
kernel structure and predicts λ well in high-dimensional space. Note that the event points are more
densely scattered in the areas with high intensity than those with much lower ones.
Figure 9 and Figure 10 study the effect of increased model size and decreased training sample sizes
to the learning of our model. The results of these two ablation studies show that our proposed model
works consistently well without overfitting after increasing the model size or decreasing the training
sample size in both the stationary and non-stationary cases.
D MLE guarantee under finite dimensional functional
REPRESENTATION
We notice that in practice, the kernel function is usually restricted to some function family space for
the problem to be well-defined (given a finite number of events ). In this section, we study such a
set-up, where the target feature function belongs to a certain parametric function class with a finite
number of parameters. This includes, for instance, spline functions, functions represented by Fourier
basis, and by neural networks - the main interest of this paper. We are particularly interested in neural
networks due to its strong expressive power of functional representation.
We start from a general framework of feature function basis representation. Assume that the feature
functions of the kernel can be well-approximated by a linear combination of basis functions: bi (x) :
1https://github.com/meowoodie/Neural-Spectral-Marked-Point-Processes
14
Published as a conference paper at ICLR 2022
Z e⅛Q.yls⅛UAS E43eα.ytjq⅛4s
1.5e-02
1.3e-02
∙1.0e-02
7.5e-03
■5.0^03
True Kernel k(t, W)
5.0e-03
0.0e+00
-5, Oe-03
-1.0e-02
-1.5e-02
1.5e-02
1.3e-02
1.0e-02
7.5e-03
5.0e-03
Proposed Kernel k(t, f,)
Figure 7:	Additional kernel recovery results for two other one-dimensional synthetic data sets. The first three
columns show the true kernel that generates the data, kernel learned by our model, and kernel learned by a
Hawkes process, respectively. The fourth column shows the true and predicted conditional intensity functions
for a test sequence.
Kernel k{t, t. ιn,m!') (t = 1)
飞WeW
飞WeW
Z 一 əponυml-
C-SPoH pvu-e”_J
Eupow ≡>ml-
∞ -SPoH pvu-e”_J
‰ UEP ‰。Er ‰ UEP
3 二三 ≡ 二二
(m
l'-∙
■SIH
Time t
Conditional intensity A
Time t
Conditional intensity λ
Time t
Conditional intensity A
-1.4e+00
-1.3e+00
-1.2e+00
-l.le+OO
-l.Oe+OO
1.4e+OO
-1.3e+00
1 2e+00
-l.le+OO
-l.Oe÷OO
1.3e+00
1.2e+00
l.le+OO
-l.Oe+OO
9.0e-01
-1.3e+OO
-1.2e+OO
-l.le+OO
-l.Oe+OO
-9.0e-01


Figure 8:	Additional kernel recovery results for two other two-dimensional synthetic data sets. The first and third
rows show the true models and the second and fourth rows show the learned models. The first three columns show
different snapshots of kernel evaluation for each model; the last column shows their corresponding conditional
intensity over marked-temporal space given a test sequence, where the black dots indicate the location of the
events.
X → R, i = 1, . . . , S:
SS
ψr (x) = Earibi(x),	φr (x) = Eeribi(x),	r =1,…，R.
i=1	i=1
15
Published as a conference paper at ICLR 2022
Figure 9: Ablation studies on one-dimensional synthetic data sets used in Figure 3. “Proposed kernel with
increased network size” refers to the model with one more hidden layer and doubled layer width in the sub-
networks (p = 20); “Proposed kernel with half training sample size” refers to the model with default architecture
but trained with only a half of the training samples.
Figure 10: Ablation studies on one-dimensional synthetic data set used in Figure 5.
X 9EH
ve⅛α.ytj石 UAS
4。El
寸 SeG.ytjfu4s
Then the kernel function in (5) can be written as kA(x0, x) = b(x0)TAb(x), where b(x)
(bι(x), .一，bs(X))T, and the (p, q)-th entry of the matrix A is given by
R
Apq =	νr αrpβrq .
r=1
Here we assume each matrix A corresponds to a unique model, i.e. for any A0 6= A, there exists
x0, x ∈ X with t(x0) < t(x), kA(x0, x) 6= kA0(x0, x). Under such a parametrization, we can write
the intensity function in (4) as
λA(x) := λ[kA](x)
μ + /
Xt(x)
b(x0)TAb(x)dN(x0) = μ + (η(x), A),
(10)
which is linear in the vectorized A. Here〈•，•〉is the Frobenius inner product of matrices, and
η(x) ∈ RS×S is conditioned on Ht(x) with the (p, q)-th entry
ηpq (x) =
Xt(x)
bp (x0)bq (x)dN(x0).
Now the influence kernel estimation problem has been reduced to the problem of estimating the
S -by-S matrix A. We will estimate the coefficient matrix A by the Maximum Likelihood Estimator
16
Published as a conference paper at ICLR 2022
(MLE) in the set of low-rank matrices A = {A ∈ RS×S : kA ∈ K} (recall that K is the family
of kernels which admits the finite-rank decomposition as in (5). By saying A consists of low-rank
matrices, we implicitly assume that R S). Also we denote the family of kernel functions that can
be represented by the chosen basis functions as
Kfinite = {kA : A ∈ A}.
The MLE is defined as
AMLE = arg max 'a,
A∈A
where the log-likelihood 'a is the corresponding variant of (6),
'a
1
M
log (μ + 5j(x),A)) dNj(x) —
M
Xj=1ZX
(μ + 5j(x),A)) dx
•	With orthonormal bases, the classical theory has that the recovery of A will ensure the recovery
of the original kernel function. With a set of over-complete bases, R is less than S (the number
of bases function), and then A is a low-rank matrix, for which case we provide a theory for the
recovery of A via MLE (Theorem D.2).
•	More generally, the above framework contains other constructions of bi(x)’s. For example, if bi(x)
are random features, then αri and βri can be viewed as weights to combine features. Random
feature model can be naturally viewed as a neural network with one hidden layer, that is, bi(x) =
σ(wiT x) is the activation on the i-th hidden nodes, and Pi αibi(x) gives the second layer output
function. Here, wi are weights in the first layer, and linear combination coefficients αi are weights
in the second layer. Thus, the random feature model corresponds to only training the second layer
weights, leaving the 1st layer as randomly initialized. In this case, S is the number of hidden
neurons, and R can be interpreted as the number of heads in an attention model.
A few consequences. We derive a few necessary basic results based on the model parametrization
for presenting the results. Recall that η(x) ∈ RS×S depends only on data and basis but not the
coefficient matrix A.
• For each trajectory j , the intensity under parameter A at x has partial derivative
∂λA,j (x)
∂Apq
ηpq,j (x).
•	The score function, i.e., the partial gradient of the log-likelihood function with respect to the
coefficient matrix A, is given by
∂AA=M XL “A，(X)Tnpqj (χ)(dNj(X)—“A，(X)dX).
•	The Hessian matrix of the log-likelihood function is given by
∂APq∂Ars = - -M X ZX λ-j (X)npqj (X)^sj (X)网(X).
Next, we provide some analysis for the MLE with possible model misspecification, as the true kernel
k may not fall into Kfinite.
Theorem D.1 (Distance between the true kernel and the optimal fit). Let Ae ∈ A be the one which
maximizes the expected log-likelihood function, i.e.
A = arg max E ('a) .	(11)
A∈A
Under Assumption 3.1, let the '2 -norm ofa kernel be
kkk22=	k(X0, X)2dX0dX.	(12)
X Xt(x)
Then we have
kk* — kek2 ≤ c5lMlT + c4 exp(2(c2 — cι)∣M∣T)D(k*,Kfinite)2,
c1
where D(k*, Kfinite) is the '2-distance between the true kernel and the set Kfinite,
D(k, Kfinite)= min kk* - k∣∣2∙
k∈Kfinite
17
Published as a conference paper at ICLR 2022
Remark D.L This theorem holds true for any k ∈ K without the rank-R assumption on kernels. In
this case, both feature function approximation by basis and low-rank approximation of the kernel
contribute to the estimation error associated with model misspecification.
Next, similar to the classic asymptotic normality of the MLE under model misspecification (White,
1982), we have the following result for the low-rank MLE.
Theorem D.2 (Asymptotic normality of low-rank MLE). Under Assumption 3.1, Assumption E.1,
let the singular value decomposition of A be A = UΛV T. Let I be the expected Hessian matrix of
the log-likelihood at Ae,
I = E (∂vec(A2L(A)T ) LU=-E UX λe2vecm(X))Vecm(X))TdN(X)),
J = CoV
where the matrices are vectorized by concatenating their columns, and let J be the covariance matrix
of a single trajectory’s score function at Ae,
∂'a	∂'a	) I
∂vec(A), ∂Vec(A)Tj ∣ a=/
Ge ∈ RS×S be the expected score at Ae,
--
A=Ae
Let F = (IS 0 U, V 0 IS) ∈ RS2 ×2SR where 0 is the Kronecker product, IS is the identity matrix
of size S,
C =(Nt 0 G)Qs,s + ((A 0 G)Qs,s)t,
where f represents pseudo-inverse and Qa,b ∈ Rab×ab is the permutation matrix such that Vec(PT)=
Qa,bVec(P) for any a-by-b matrix P. If FT(I+ C)F shares the same null-space with F, then the
low-rank estimator AbMLE, solved from the constrained maximum likelihood problem, satisfies
√M (Vec(AMLE) - Vec(N)) →N (0,F (FT (I + C)F )tFT JF(FT (I + C)F )tF),
when M → ∞.
Remark D.2 (Parameter recovery guarantee). The result shows that the maximum likelihood estimate
for the kernel matrix A converges to the optimal fit A, and the matrices I, J, C captures the residual
variance; F projects the variance caused by I, J, C onto the tangent space of the low-rank manifold
at A. In practice, the variance term can be estimated empirically; J can be estimated from the
empirical covariance matrix of the score function of each trajectory, while the other matrices can be
approximated by the properties of the log-likelihood function at AMLE. If the true kernel falls into the
kernel family Kfinite , both I, J will equal to the Fisher Information, G, C will vanish, which greatly
simplify the process of estimating the variance. But with the presence of model misspecification, this
is generally not the case.
E Proofs
E.1 Proof of Lemma 3.1
Proof of Lemma 3.1. For the j-th trajectory, define `j [k] as
`j [k] =	log λj [k](X)dNj (X) -	λj [k](X)dX,
XX
(13)
1
then '[k]= a Ej 'j[k]. Let λj = λj [k*], λj = λj[k], where the conditional intensity λ[k] is
defined as in (4), k and k are the true kernel and the perturbed one respectively. Then we have
r	r
'j网—'j[k*] = J (logλj(x) — logλ*(x))dNj(x) — J (λj(x) — λ*(x))dx	(14)
18
Published as a conference paper at ICLR 2022
By (4), λj [k] is linear with respect to perturbation in k, that is,
λj [k](x) = ν +	k(x0, x)dNj (x0).
Xt(x)
We then have that
~ . .	... . f ... . .	............. . ..	..
λj (x) — λ*(x) = J	(k(x , x) — k (x0, x))dNj (x0) = δλj(x),
where the last equality is by definition of δλj .
Back to (14), the second term
J (λj(x) — λj(x))dx = J δλj(x)dx;
(15)
(16)
The first term under Taylor expansion of log has that, for each x ∈ X,
11
logλj(X) - logλj(X) = λJΓ^δλj(X) - ^ΓΓ∖2(δλj(X))2,
λj (x)	2ξj (x)
where ξj(x) takes value between λj (x) and λj(x). By Assumption 3.1, both λj(x) and λj (x) are
strictly positive and upper-bounded by c2, and thus 0 < ξj (X) ≤ c2 . This means that
2Λ12(δλj(x))2 ≥ -12(δλj(x))2, ∀x ∈ X.
2ξj (X)	c2
Thus, the 1st term in (14) satisfies
LQogλ (X)- log λj(X)) dNj (X)=J」%(X) dNj(Xx)
"	≤ Zg (X)j)
X	λj (X)
-X 2ξj(X)2 Qj (X))2 dNj (x)
—2c2 / (δλj(X))2dNj(x).	(17)
Putting together (16) and (17),
'j闪—'j[k*] ≤ {δλj∙(x)dN(X)) — 2C2 4(δλj∙(x))2dNj(x) — {δλj∙(x)dx
=jχ δλj(x) (dNjxx) — dx) — 2C2 jχ(δλj(x))2dNj(x).
The above holds for each j , and taking the average over the M trajectories proves the lemma.
□
E.2 Proof of Theorem 3.2
Lemma E.1 (lower bound for KL-divergence). Under Assumption 3.1,
E ('[k*] — '[k])
≥ 212 eχp (TC2 -CI)IMIT) ∣∣δkk2,
1 ιι. r	7 一 ι^^	1	r 7	7	7 ⅛=	7 .1 λ	■	1 /` j	∕yc∖∖
holds true for any k ∈ K	and	δk	= k —	k*	(the '2 -norm	is defined as	(12).)
Proof. By Lemma 3.1, since all trajectories are i.i.d., and for each trajectory the conditional expecta-
tion
E (λR(x] JHt(X))=dx，
λ[k ](x)
there is
E (半*]-哂)≥ E (Jx δλ(X) (λf¾
—
(δλ(X))2dN(X)
2C2E ({(δλ(x))2dN(x)}
19
Published as a conference paper at ICLR 2022
Following Assumption 3.1
^12E (/ (δλ(x))2dN(x)
E(/ (δλ(x))2λ[k*](x)dx) ≥	e(/ (δλ(x))2dx).
Next we lower bound the term above by taking the integral over the event space E = Fi∞=0 Ei of
trajectories, where for each i,
Ei = {(xi,X2,…，Xi) ∈ Xi,t(xi) < …< t(xi)} ⊂ Xi
consists of all the trajectories with exactly i events. For each HT ∈ E, let N be the associated
counting measure, the probability density of HT
P(HT) = exp (/ logλ[k*]dN(x) — / λ[k*](x)dx
≥ exp(Hτ| log ci - ∣M∣Tc2) =： P(HT).
X (δλ(x))2 dx can be lower bounded by taking the integral over the lower bound of probability
density ρ,
蒲E IL(δλ(x))2d
i
((δλ(x))2dxj P(HT)dx1dx2 .一 dxi,
and by the equivalence over ordering of xi,…，Xi,
(δλ(x))2dx 1 P(HT)dx1dx2 …dxi
(δλ(x))2dx I P(HT)dx1dx2 .…dxi
(*)
Then we substitute δλ(x) with X	δk(x0, x)dN(x0) and change the order of integrals, (*) equals to
i
δk(x0, x)δk(x00, x)dN(x0)dN(x00)dx P(HT)dx1dx2 …dxi
12^ ^X ɪ JJ J J	δk(x0, x)δk(x00, x)p(Ht)dN(x0)dN(x00)dx1dx2 …dx%dx.
For x0	6=	x00,	dN(x0)dN(x00)	= 1 when two of	xi, …，xi	equal to	x0,x00.	For x0 =	x00,
dN(x0)dN(x00) = 1 when one of xi,…，xi equals to x0 = x00. Again by the equivalence over
ordering of xi,…，x^ we assume x0 = xi, x00 = x2 when x0 = x00, or x0 = x00 = xi, (*) equals to
12^ sχ, “" [ 1) / / J /	δk(x0, x)δk(x00, x)p(Ht)dx0dx00dx3 …dx%dx
c ∞i
+ 品上不U- L	δk(x0, x)δk(x0, X)P(HT)dx0dx2 …dxidx
2c2 i=ii! JXjXi-1 JXt(X)	一
For the first row, we take the integral over x3,…，xi and for the second row, we take the integral
over x2,…，xi, (*) equals to
费 X i(i - 1) Z Z /	δk(x0,x)δk(x00,x)∣M∣i-2Ti-2ci exp(-C2∣M∣T)dx0dx00dx
+
δk(x0, x)δk(x0, x)|M|i-iTi-icii exp(-c2 |M|T)dx0 dx
|M|i-2Ti-2cii exp(-c2 |M|T)	δk(x0, x)dx0	dx
Xt(x)
20
Published as a conference paper at ICLR 2022
∞
δk(X, x)δk(x0, x)	-----rτ∣M∣i-1Ti-1cι exp(-C2∣M∣T)dx0dx
i=1 (i-1)!	1
Note that the first term is non-negative.
(*)
∞
δk(x0, x)δk(x0, x)	----；；|M|i-1Ti-1c1 exp(-C2∣M∣T)dx0dx
i=1 (i-1)!	1
δk(x0, x)δk(x0, x) exp(-(c2 - c1)|M|T)dx0dx
exp(-(c2 - cι)∣M∣T)kδkk2.
□
Proofof Theorem 3.2. This follows immediately from Lemma E.1.	□
E.3 Proof of Theorem D.1
LemmaE.2. Under Assumption 3.1,forany k ∈ K,
E('[k*] — '[k]) ≤ c3lMlT + c2 exp(© - cι)∣M∣T)kk - k*k2,
Proof. Similar to the proof of Lemma 3.1 and Theorem 3.2, for a single trajectory and any k ∈ K let
δk(x0, x) = k(x0, x) - k* (x0, x), ∀x0, x ∈ X, t(x0) < t(x),
δλ(x) =	δk(x0, x)dN(x0).
Xt(x)
There is
`[k*] - `[k]
[δλ(x)d一 f lo, p[k](x) ʌ dN(X)
/X (X) -JX g [λ[k*](x) J (X)
/ δλ(x)dx -	log 1 +
δλ(x))
λ[k*](X)J
dN(x)
δλ(x)
X
—
dN(x)	1 δλ(x)2
λ⅛⅛) + 2	dN(x)，
for some λ(x) determined by Ht(X) such that λ(x) is in between λ[k*](x) and λ[k](x) for all X ∈ X.
Then since the expectation of dx — dN(x)∕λ[k*](x) is 0,
E (`[k*] - `[k])
—E (Zi 1 δλ(x)2 λ[k*](x)dx
=EUX 2 X(x)2 λ[k ](X)
≤ 22EE (/ δλ(x)2dx).
For each trajectory HT ∈ E , the probability density of HT
ρ(HT )
exp	log λ[k*](x)dN(x) -	λ[k*](x)dx
≤ exp(Hτ| logc2-∣M∣Tci) =： P(HT).
21
Published as a conference paper at ICLR 2022
Following similar arguments as the proof of Lemma E.1,
E ('[k*] - '[k]) ≤ ^2iE(/ δλ(x)idx)
≤ ^ii X Z lZ δλ(x)idx) P(HT)dxι …dxi
=M X - IN Z IMIi-iTi-ici exp(-cι∣M∣T) I Z	δk(x0, x)dx0 ∣ dx
2Ci1 i=i (i-2)! X	i	Xt(x)
∞
δk(x0, x)i ^X  -—∣M∣i-1Ti-1ci exp(-cι∣M∣T)dx0dx
i=1 (i-1)!
2c22 L Cieχp((C2 -CI)MIT)
δk(x0, x)dx0 dx
Xt(x)
δk(x0, x)2c2 exp((c2 - c1)|M|T)dx0dx.
By Cauchy-Schwarz inequality,
c22 exp((c2 - c1)|M|T)
X
δk(x0, x)dx0	dx
Xt(x)
c22 exp((c2 - c1)|M|T)|Xt(x) |
c22 exp((c2 - c1)|M|T)|M|T
|	δk(x0, x)2dx0dx
δk(x0, x)2dx0dx
Xt(x)
=ciMT exPQi - cι)∣M∣T)kδkki.
So together we have
E('[k*] - '[k]) ≤ c3lM2lT + Ci exp((ci - cι)∣M∣T)kk - k*ki,
□
Then we get back to the proof of Theorem D.1.
Proofof Theorem D.1. Let Ao ∈ AR be the one which minimizes ∣∣k* - ka0 ∣∣i, i.e.,
kk* - kAo ∣i = D(k*, Kfinite).
By Lemma E.1 and Lemma E.2, there is
cilMlT + Ci exp((ci - cι)∣M∣T)D(k*, Kfinite)i ≥ E('[k*]-平4。])
2C1
≥ E ('W - '[kχ]) ≥ ɪ exp(-(Ci -CI)MIT)∣k* - kχki.
□
E.4 Proof of Theorem D.2
Let A = {A ∈ RS×S, k[A] ∈ K}. We prove the theorem under the following assumption:
22
Published as a conference paper at ICLR 2022
Assumption E.1. Assume A is the unique minimizer in (11), is of rank exactly R, and vec(A) is on
the interior of Vec(A)= {vec(A) : A ∈ A} ⊆ RS2.
G	八厂	…JI	I	....	…	J	.-	1	1 C '	1	J	^	I
Proof. Consider the local parametrization of AR in the neighborhood of A based on the singular
value decomposition Ae = UΛV T, U, V ∈ RS×R, Λ is a R-by-R diagonal matrix,
A = (UU) (A +3PI P3(Λ :PI)TP2) (VT).
Here (U U), (V V) are fixed orthogonal matrices where the first R columns are U, V respectively.
P1 ∈ RR×R, P2, P3 ∈ R(S-R)×R are parameters in the neighborhood of0 such that Λ + P1 is
non-singular. By Theorem 3.2 of White (1982), let P1,MLE , P2,MLE, P3,MLE be the parameters
which correspond to AbMLE, i.e.
b — — (λ+∖ I λ + P1,MLE	-	PTMLE
Pb3,MLE	Pb3,MLE(Λ +Pb1,MLE)-1Pb2T,MLE
For simplicity, letp = (vecT(P1), vecT(P2), vecT(P3))T. Then
Mc(PI
√MbMLE = √M Vec(Pb2
vec(Pb3
,MLE) D
,mle) I -→N(O,I JI 1),
,MLE)
where
I=E
(∂⅛) L1=0,P2=0,P3=0
包∂'a
( ∂p ∂pT
P1=0,P2=0,P3=0
J = E
For simplicity, from now on, we write all partial derivatives taken at P1 =0, P2 =0, P3 =0, A = Ae
without specifying the location. Let
∂ vec(A)
Y = Vec(A - A)----T-fp-p,
∂pT
since kγk = O(kpk22), there is
√Mγ — 0.
√MVec(AMLE- A) = √M (Y + SPMLE) D N (0, SI-JI-I ^^^) .
Next we look at the covariance matrix of the multivariate Gaussian distribution,
∂'a	∂vecτ (A) ∂'a
∂p ∂p	∂ Vec(A)
Since A maximizes E('a) and is on the interior of A and hence the interior of the manifold
parametrized by p, we have
E (空)=dvecT1A2E () = dve户Vec(G) = 0
∂p	∂p	∂Vec(A)	∂p
J = E
∂ ∂'a ∂'a
I ∂p ∂pτ
∂VecT(A)E ( ∂'a	∂'a	) ∂Vec(A)
∂p	(∂Vec(A) ∂Vecτ(A))	∂pτ
∂VecT(A) (C	( ∂'a ∂'a ) + E ( ∂'a ) E ( ∂'a
∂p y °v IdVec(A), ∂VecT(A) J IdVec(A) J IdVec(A)
∂VecT(A) J ∂Vec(A)
∂p	∂pT .
(18)
∂ Vec(A)
∂pT
23
Published as a conference paper at ICLR 2022
For any i, j ∈ [(2S - R)R],
∂ 2'a	∂vecT(A)	∂2'a
∂pi∂pj
∂pi	∂vec(A)∂pj
∂vecT(A)	∂'a
十 ∂pj∂pj ∂vec(A)
∂veCT(A)	∂2'a ∂veC(A)	∂2veCT(A)	∂'a
∂pi ∂veC(A)∂veCT(A) ∂pj	+ ∂pj∂pj ∂veC(A),
∂2'a
the (i,j)-th component of I
Iij = E
∂ 2'a
∂pi∂pj
"T(A) I dveC(A) + d2：eCT(A) veC(G).
∂pi	∂pj	∂pj ∂pj
We observe that some blocks of (
∂2vecτ(A)
dPjdPj
/二λ∖	C
veC(G))ij are 0.
∂ 2vecT(A)VeC(G)
∂vec(P1) ∂vecT (Pi)
∂ 2vecT(UP1V T + UPT V T + UP3V T + UP3(Λ + PI)TPT V T )vec(G)
∂vec(P1)∂vecT (P1)
0,
as (1) the second order derivative of UPiVT,UPTVT,UP3VT over Pi is 0 since they are either
linear in or irrelevant with Pi, (2) the second order derivative of UPiVT + UP3(Λ + Pi)-IP2VT
over Pi is 0 since it is taken at P2 = P3 = 0. Also We have
∂ 2veCT(A)VeC(G)
∂ VeC(Pi)∂veCT (P2)	,
∂ 2veCT(A)VeC(G)
∂ VeC(P2)∂veCT (P2)	,
for similar reasons. The only non-zero block is
∂ 2vecT(A)VeC(G)
∂vec( Pi )∂veCT (P3)	,
∂ 2veCT(A)VeC(G)
∂veC( P3 )∂veCT (P3)	,
∂2vecT(A)VeC(G)
∂vec(P2) ∂veCT (P3)
∂2vecT(UP3Λ-1PT VT )vec(G)
∂veC(P2)∂veCT(P3)
∂	∂vecT(UP3Λ-1PT VT )vec(G)
∂veC(P2)	∂veCT (P3)
∂	∂veCT(P3)(Λ-iPTVT X UT)veC(G)
∂vec(P2)
∂
∂vec(P2)
∂
∂veCT (P3)
((Λ-1P2tVT X UT)vec(GG))T
∂vec(P2)
∂
∂vec(P2)
VeCT(P2)(Λ-1 X VTGTU)
Λ-1 X VT GT U.
So
∂vecT(A) 了 ∂vec(A)
∂p
∂pT
+(0
0
0
0
Λ-1 X UT GeV
0 ~八
Λ-1 X VTGeτU .
0
I
At last, We compute ∂vec(A)∕∂pT,
∂vec(A)
∂veC(A)	∂veC(A)	∂veC(A)
∂pT
∂veCT (Pi) ∂veCT (P2) ∂veCT(P3)
∂∂veC(UPiVT) ∂veC(UPTVT) ∂veC(UP3VT)
(∂veCT (Pi)	∂ VeCT(P2)	∂veCT (P3)
(V X U (V X U)Qs-r,r V X U).
24
Published as a conference paper at ICLR 2022
Let
/Ir2	∖ (VT ㊈ IR IR ㊈ UT
Z =( Qr,S-R	, VT 0 Ir
Z has full row-rank, and
∂vec(A)
∂pτ
(VT 0 Ir	Ir 0 Uτ'
(V 0 U V 0 U V 0 U)	VT 0IR
∖	Ir 0 UT
((VVT + VVT) 0 U V 0 (UUT + UUT))
F.
Then the covariance matrix of the asymptotic distribution of VeC(AMLE)
∂vec(A)	ι	ι ∂vecT(A)
∂pτ	∂p
dvec(A) Z (Z TIZ )tZ T JZ (Z T∕Z)tZ T d VeCT(A)
∂p1	∂p
F (ZTIZUFT JF (ZTIZKF t .
Next we check that
(ZTIZ) = FT (I + C)F.
By (18), We know
(vecτ(UTG) vecτ(GV)) = vecτ(G)F = vecτ(G)	CTʌ，Z = 0.
So
UUT G = (is - UUT )G = G = G(IS - VVT) = GeVVτ.
ZTIZ - FTIeF
0	0	∖
0	Λ-1 0 VTGTUl Z
Λ-10 UTGeV	0	)
(VT 0 Ir	Ir 0 UTʌ T /0
FT 0 Ir	I I 0
∖	Ir 0 UT	∖0
00
_ 0_	Qs-r,r(Λ-1 0VTGτU)
(Λ-1 0 UTGV)Qr,s-r	0
(VT 0 Ir Ir 0 Uτ∖
VT 0 Ir	I
∖	Ir 0 UT)
(	0	((Ir 0 U)(Λ-1 0 UTGV)Qr,s-r(VT 0 Ir))t
I(IR 0 U)(Λ-1 0 UTGV)Qr,s-r(Vτ 0 Ir)	0
_0 ~_	((IrΛ-1Ir 0 UUTGVVT)Qr,s)t
(IrΛ-1Ir 0 UUτ Gvv T )Qr,s	0
0~	((Λ-1 0 G)Qr,s)t、
(Λ-10 G)Qr,s	0	广
25
Published as a conference paper at ICLR 2022
Ft(VΛ-1UT 乳 Ge)Qs,sF =	(IST^UT)(VΛ-1UT 乳 G)Qs,s (IS 乳 U V 乳 IS) Vt T 0 Is J CXUTAO)Qs,s (IS 0 U V 0 Is) (qr,s G 0Λ-1U T)(IS 0 UV 0 Is) (~	1	0) QQr,s(G Λ Λ-1) Oj (	0~	0) V(Λ-1 0 G)Qr,s 0广
FT CF = (	1 °~ V(Λ-1 乳 G)QR,s	0) + J1 %n	0)T = ZTIZ - FtIF. 0J V(Λ-1 0 G)Qr,s 0J
□
26