Published as a conference paper at ICLR 2022
Back2Future: Leveraging Backfill Dynamics
for Improving Real-time Predictions in Future
Harshavardhan Kamarthi, Alexander Rodriguez, B. Aditya Prakash
College of Computing
Georgia Institute of Technology
{harsha.pk,arodriguezc,badityap}@gatech.edu
Ab stract
For real-time forecasting in domains like public health and macroeconomics, data
collection is a non-trivial and demanding task. Often after being initially released,
it undergoes several revisions later (maybe due to human or technical constraints)
- as a result, it may take weeks until the data reaches a stable value. This so-
called ‘backfill’ phenomenon and its effect on model performance have been barely
addressed in the prior literature. In this paper, we introduce the multi-variate
backfill problem using COVID-19 as the motivating example. We construct a
detailed dataset composed of relevant signals over the past year of the pandemic.
We then systematically characterize several patterns in backfill dynamics and
leverage our observations for formulating a novel problem and neural framework,
Back2Future, that aims to refines a given model’s predictions in real-time. Our
extensive experiments demonstrate that our method refines the performance of
diverse set of top models for COVID-19 forecasting and GDP growth forecasting.
Specifically, we show that Back2Future refined top COVID-19 models by 6.65%
to 11.24% and yield 18% improvement over non-trivial baselines. In addition,
we show that our model improves model evaluation too; hence policy-makers can
better understand the true accuracy of forecasting models in real-time.
1	Introduction
The current COVID-19 pandemic has challenged our response capabilities to large disruptive events,
affecting the health and economy of millions of people. A major tool in our response has been
forecasting epidemic trajectories which enabled policymakers to plan interventions (Holmdahl &
Buckee, 2020). Broadly two classes of approaches have been devised: traditional mechanistic
epidemiological models (Shaman & Karspeck, 2012; Zhang et al., 2017), and the fairly newer
statistical approaches (Brooks et al., 2018; Adhikari et al., 2019; Osthus et al., 2019b) including deep
learning models (Adhikari et al., 2019; PanagoPoUlos et al., 2021; RodrigUez et al., 2021a), which
have become among the top-performing ones for multiple forecasting tasks (Reich et al., 2019). These
models also Use newer digital indicators like search qUeries (Ginsberg et al., 2009; Yang et al., 2015)
and social media (CUlotta, 2010). EPidemic forecasting is still a challenging enterPrise (Metcalf &
Lessler, 2017; Biggerstaff et al., 2018) becaUse it is affected by weather, mobility, strains, and others.
However, real-time forecasting also brings new challenges. As noted in mUltiPle CDC real-time
forecasting initiatives for diseases like flU (OsthUs et al., 2019a) and COVID-19 (Cramer et al., 2021),
as well as in macroeconomics (Clements & Galvao, 2019; Aguiar, 2015) the initially released public
health data is revised many times after and is known as the ’backfill’ Phenomenon.The varioUs factors
that affect backfill are multiple and complex, ranging from surveillance resources to human factors
like coordination between health institutes and government organizations within and across regions
(Chakraborty et al., 2018; Reich et al., 2019; Altieri et al., 2021; Stierholz, 2017).
While previous works have addressed anomalies (Liu et al., 2017), missing data (Yin et al., 2020),
and data delays (Zliobaite, 2010) in general time-series problems, the backfill problem has not been
addressed. In contrast, the topic of revisions has not received as much attention, with few exceptions.
For example in epidemic forecasting, a few papers have either (a) mentioned about the ‘backfill
problem, and its effects on performance (Chakraborty et al., 2018; RodrigUez et al., 2021b; Altieri
1
Published as a conference paper at ICLR 2022
et al., 2021; Rangarajan et al., 2019) and evaluation (Reich et al., 2019); or (b) proposed to address the
problem via simple models like linear regression (Chakraborty et al., 2014) or ’backcasting’ (Brooks
et al., 2018) the observed targets. The related problem of nowcasting involves prediction of revised
stable value of current week’s target from sequence of right-truncated past values [Aditya: cite one
paper ]. Prior works have used data assimilation and sensor fusion from a readily available stable
set of features to refine unrevised features for accurate nowcasting (Farrow, 2016; Osthus et al.,
2019a). However, most methods focus only on revisions in the target and typically study in the
context of influenza forecasting, which is substantially less noisy and more regular than the novel
COVID-19 pandemic or assume access to stable values for some features which is not the case for
COVID-19. In economics, Clements & Galvao (2019) surveys several domain-specific (Carriero et al.,
2015) or essentially linear techniques for data revision/correction behavior of several macroeconomic
indicators (Croushore, 2011).
Motivated from above, we study the challenging problem of multi-variate backfill for both features
and targets. We go further beyond prior work and also show how to leverage our insights towards a
general neural framework to improve model predictions and performance evaluation (i.e. rectification
of current target from the evaluator’s perspective). Our specific contributions are the following:
•	Multi-variate backfill problem: We introduce the multi-variate backfill problem using real-time
epidemiological forecasting as the primary motivating example. In this challenging setting, which
generalizes (the limited) prior work, the forecast targets, as well as exogenous features, are subject to
retrospective revision. Using a carefully collected diverse dataset for COVID-19 forecasting for the
past year, we discover several patterns in backfill dynamics, show that there is a significant difference
in real-time and revised feature measurements, and highlight the negative effects of using unrevised
features for incidence forecasting in different models both for model performance and evaluation.
Building on our empirical observations, we formulate the problem Bfrp, which aims to ‘correct’
given model predictions to achieve better performance on eventual fully revised data.
•	Spatial and Feature level backfill modeling to refine model predictions: Motivated by the
patterns in revision and observations from our empirical study, we propose a deep-learning model
Back2Future (B 2F) to model backfill revision patterns and derive latent encodings for features.
B2F combines Graph Convolutional Networks that capture sparse, cross-feature, and cross-regional
backfill dynamics similarity and deep sequential models that capture temporal dynamics of each
features’ backfill dynamics across time. The latent representation of all features is used along with the
history of the model’s predictions to improve diverse classes of models trained on real-time targets,
to predict targets closer to revised ground truth values. Our technique can be used as a ‘wrapper’ to
improve model performance of any forecasting model (mechanistic/statistical).
•	Refined top models’ predictions and improved model evaluation: We perform an extensive
empirical evaluation to show that incorporating backfill dynamics through B2F consistently improves
the performance of diverse classes of top-performing COVID-19 forecasting models (from the
CDC COVID-19 Forecast Hub, including the top-performing official ensemble) significantly. B2F
also enables forecast evaluators and policy-makers better evaluate the ‘eventual’ true accuracy of
participating models (against revised ground truth). This allows the model evaluators to quickly
estimate models that perform better w.r.t revised stable targets instead of potentially misleading
current targets. Our methodology can also be further adapted for nowcasting and other general
time-series forecasting problems. We also show the generalizability of our framework and model
B2F to other domains by significantly improving predictions of non-trivial baselines for US National
GDP forecasting (Marcellino, 2008).
2	Nature of backfill dynamics
In this section, we study important properties of the revision dynamics of our signals. We introduce
some concepts and definitions to aid in the understanding of our empirical observations and method.
Real-time forecasting. We are given a set of signals F = Reg × Feat, where Reg is the set of all
regions (where we want to forecast) and set Feat contains our features and forecasting target(s) for
each region. At prediction week t, xi(,t1):t is a time series from 1 to t for feature i, and the set of all
signals results in the multi-variate time series X1(:tt)1. Similarly, Y1(t:t) is the forecasting target(s) time
1In practice, delays are possible too, i.e, at week t, we have data for some feature i only until t - δi . All our
results incorporate these situations. We defer the minor needed notational extensions to Appendix for clarity.
2
Published as a conference paper at ICLR 2022
series. Further, let's call all data available at time t, DItt = {X(tt, Y(t)} as real-time Sequence. For
clarity we refer to ‘signal’ i ∈ F as a sequence of either a feature or a target, and denote it as di(,t1):t .
Thus, at prediction week t, the real-time forecasting problem is: Given D1(t:t), predict next k values of
forecasting target(s), i.e. yt+1：t+k. Typically for CDC settings and this paper, our time unit is week,
k = 4 (up to 4 weeks ahead) and our target is COVID-19 mortality incidence (Deaths).
Revisions. Data revisions (‘backfill’) are common. At prediction week t + 1, the real-time sequence
D1(t:t++11) is available. In addition to the length of the sequences increasing by one (new data point),
values of D1(t:t++11) already in D1(t:t) may be revised i.e., D1(t:t) 6= D1(t:t+1). Note that previous work has
studied backfill limited to Y(t) , while we address it in both X(t) and Y(t) . Also, note that the data in
the backfill is the same used for real-time forecasting, but just seen from a different perspective.
Backfill sequences: Another useful way we propose to look at backfill is by focusing on revisions of
a single value. Let's focus on value of signal i at an observation Week t0. For this observation week,
the value of the signal can be revised at any t > t0, which induces a sequence of revisions. We refer
to revision Week r ≥ 0 as the relative amount of time that has passed since the observation week t0.
Defn. 1. (Backfill Sequence BSEQ) For signal i and observation week t0, its backfill sequence is
BSEQ(i, t0) = hd(tt0 ,d(tt+11,..., d(∞ )〉, where d(? is the initial ValUe of the signal and d(∞) is the
final/stable value ofthe signal.
Defn. 2. (Backfill Error BERR) For revision Week r of a backfill sequence, the backfill error is
BERR(r,i,t0) = |d(tto+r) - d(∞)| / ∣d(∞o)|.
Defn. 3. (Stability time STIME) of a backfill sequence BSeq is the revision week r* that is the
minimum r for which the backfill error BERR < E for all r > r*, i.e., the time When BSEQ stabilizes.
Note: We ensured that BSEQ length is at least 7, and found that in our dataset most signals stabilize
before r = 20. For di(,∞t0), we use di(,ttf0), at the final week tf in our revisions dataset. In case we do not
find BERR < E in any BSEQ, we set STIME to the length of that BSEQ. We use E = 0.05. Example:
For BSEQ {223, 236,236,404,..., 404}, BERR for third week is l234-4041 = 0.41 and STIME is 4.
2.1	Dataset description
We collected and pre-processed important publicly available signals
from a variety of trusted sources that are relevant to COVID- 1 9
forecasting to form the COVID- 1 9 Surveillance Dataset (CoVDS).
See Table 1 for the list of 20 features (|Feat| = 21, including
Deaths). We collected revised features every week from April 2020
to July 2021. Our analysis covers 30 observation weeks from June
2020 to December 2020 (to ensure all our backfill sequences are of
length at least 7) for all |Reg| = 50 US states. The rest of the unseen
data from Jan 2021 to July 2021 is used strictly for evaluation.
Patient line-list: traditional surveillance signals used in epidemiologi-
cal models (Chakraborty et al., 2014; Brooks et al., 2018) derived from
line-list records e.g. hospitalizations from CDC (CDC, 2020), posi-
tive cases, ICU admissions from COVID Tracking (COVID-Tracking,
2020). Testing: measure changes in testing from CDC and COVID-
Tracking used by ROdrigUez et al. (2021b). Mobility: quantify change
in people's movement to several point of interests (POIs); derived from
mobility reports released by Google (2020); Apple (2020). Exposure:
digital signal measuring closeness between people at POIs,(Chevalier
et al., 2021) Social Survey: used by (Wang et al., 2020; Rodriguez et al., 2021b) CMU/Facebook
Symptom Survey Data contains self-reported responses about COVID-19 symptoms.
2.2	Observations
We first study different facets of the significance of backfill in CoVDS. Using our definitions, we
generate a backfill sequence for every combination of signal, observation week, and region (not all
signals are available for all regions). In total, we generate more than 30, 000 backfill sequences.
Table 1: List of features in
our CoVDS
Type	Features
Patient Line-List	ERVisits, HospRate, +veInc, HospInc, Recovered, onVentilator, inICU
Testing	TestResultsInc, -veInc, Facilities
Mobility	RetailRec, Grocery, Parks, Transit, WorkSpace, Resident, AppleMob
Exposure	DexA
Social Sur- vey	FbCLI, FbWiLi
3
Published as a conference paper at ICLR 2022
Backfill error BERR is significant. We computed BERR for the initial values, i.e., BERR(r =
0, i, t0), for all signals i and observation weeks t0.
Obs. 1. (BERR across signals and regions) Compute the average BERR for each signal; the median
of all these averages is 32%, i.e. at least half of all signals are corrected by 32% of their initial value.
Similarly in at least half of the regions the signal corrections are 280% of their initial value.
We also found large variation of BERR. For features (Figure 1a), compare avg. BERR = 1743% of
five most corrected features with 1.6% of the five least corrected features. Also, in contrast to related
work that focuses on traditional surveillance data Yang et al. (2015), perhaps unexpectedly, we found
that digital indicators also have a significant BERR (average of 108%). For regions (see Figure 1b),
compare 1594% of the five most corrected regions with 38% of the five least corrected regions.
(a) BErr per feat. type (b) BErr per region
Figure 1: BERR and STIME across feature type and regions, heat maps are log scaled.
(d) STime per region
(c) STime per feat. type
Stability time STime is significant. A similar analysis for STIME found significant variation across
signals (from 1 weeks for to 21 weeks for COVID-19, see Figure 1c for STime across feature
types) and regions (from 1.55 weeks for GA to 3.83 weeks for TX, see Figure 1d). This also impacts
our target, thus, actual accuracy is not readily available which undermines real-time evaluation and
decision making.
Obs. 2. (STIME of features and target) Compute the average STIME for each signal; the average of
all these averages for features is around 4 weeks and for our target Deaths is around 3 weeks, i.e.
on average, it takes over 3 weeks to reach the stable values of features.
Backfill sequence BSeq patterns. There is significant similarity among BSEQs. We cluster BSEQs
via K-means using Dynamic Time Warping (DTW) as pair-wise distance (as DTW can handle
sequences of varying magnitude and length). We found five canonical categories of behaviors (see
Figure 2), each of size roughly 11.58% of all BSeqs. Also, each cluster is not defined only by signal
Early Decline Early Increase
Late rise	Mid Decrease
■ ■ ■ ■ ■
Ooooo
(PS I,。)
8n75A bsss
0	10	20	30	0	10	20	30	0	10	20	30	0	10	20	30	0	10	20	30
Revision week
Figure 2: Centroid BSEQ of each cluster, scaled between [0, 1], showing canonical backfill behaviors
nor region. Hence there is a non-trivial similarity across both signals and regions.
Obs. 3. (BSEQ similarity and variety) Five canonical behaviors were observed in our backfill
sequences (Figure 2). No cluster has over 21% of BSEQs from the same region, and no cluster has
over 14% of BSEQs from the same signal.
Model performance vs BErr. To study the rela-
tionship between model performance (via Mean Abso-
lute Error MAE of a prediction) and B Err, we use
RevDiffMAE: the difference between MAE com-
puted against real-time target value and one against the
stable target value. We analyze the top-performing real-
time forecasting models as per the comprehensive evalu-
ation of all models in COVID-19 Forecast Hub (Cramer
(a) GT-DC	(b) YYG
Figure 3: BERR vs model REVDIFFMAE.
et al., 2021). YYG and UMass-MB are mechanistic while CMU-TS and GT-DC are statistical
models. The top performing Ensemble is composed of all contributing models to the hub. We expect
a well-trained real-time model will have higher RevDiffMAE with larger BErr in its target (Reich
et al., 2019). However, we found that higher BErr does not necessarily mean worse performance.
See Figure 3—YYG has even better performance with more revisions. This may be due to the more
complex backfill activity/dependencies in COVID in comparison to the more regular seasonal flu.
Obs. 4. (Model performance and backfill) Relation between BERR and REVDIFFMAE can be
non-monotonous and positively or negatively correlated depending on model and signal.
4
Published as a conference paper at ICLR 2022
Real-time target values to measure model performance: Since targets undergo revisions (5%
BErr on average), we study how this BErr affects the real-time evaluation of models. From
Figure 4, we see that the scores are not similar with real-time scores over-estimating model accuracy.
The average difference in scores is positive which implies that evaluators would overestimate models’
forecasting ability.
Obs. 5. MAE evaluated at real-time overestimates model performance by 9.6 on average, with the
maximum for TX at 22.63.
3 Refining the future via B2F
Our observations naturally motivate improving training and evaluation
aspects of real-time forecasting by leveraging revision information. Thus,
we propose the following two problems. Let predictions of model M for
week t + k be y(M, k)t. Since models are trained on real-time targets,
y(M, k)t is the model’s estimate of target yt(+t+kk) .
Figure 4: Real-time vs sta-
ble MAE.
k-week ahead Backfill Refinement Problem, BFRPk(M): At predic-
tion week t, We are given a revision dataset {D(,0 }t'≤t, which includes
our target Y1(:tt) . For a model M trained on real-time targets, given history of model’s predictions till
last week hy(M, k)1, . . . y(M, k)t-1i and prediction for current week y(M, k)t, our goal is to refine
y(M, k)t to better estimate the stable target yt(+tfk), i.e. the ’future’ of our target value at t + k.
Leaderboad Refinement problem, Lbrp: At each week t, evaluators are given a current estimate
of our target yt(t) and forecasts of models submitted on week t - k. Our goal is to refine yt(t) to
yt, a better estimate of y(tf), so that using yt as a surrogate for yFf) to evaluate predictions of
models provides a better indicator of their actual performance (i.e., we obtain a refined leaderboard
of models). Since Lbrp aims to refine the current estimate of the target, it is closely related to the
nowcasting problem. LBRP is also a special case of Bfrp: Assume a hypothetical model Meval
whose predictions are real-time ground truth, i.e. y(Meval, 0)t= yt(t), ∀t. Then, refining Meval is
equivalent to refining yt(t) to better estimate yt(tf ) which leads to solving LBRP.
Overview: We leverage observations from Section 2 to derive Back2Future (B 2F), a deep-learning
model that uses revision information from BSeq to refine predictions. Obs. 1 and 2 show that real-
time values of signals are poor estimates of stable values. Therefore, we leverage patterns in BSeq of
past signals and exploit cross-signal similarities (Obs. 3) to extract information from BSeqs. We also
consider that the relation of models’ forecasts to BErr of targets is complex (Obs. 4 and 5) to refine
their predictions. B2F combines these ideas through its four modules: • GRAPHGEN: Generates
a signal graph (where each node maps to a signal in Reg × Feat) whose edges are based on BSEQ
similarities. • BSEQENC: Leverages the signal graph as well as temporal dynamics of BSEQs to learn
a latent representation of BSEQs using a Recurrent Graph Neural Network. • MODELPREDENC:
Encodes the history of the model’s predictions, the real-time value of the target, and past revisions of
the target through a recurrent neural network. • REFINER: Combines encodings from B SEQENC and
ModelPredEnc to predict the correction to model’s real-time prediction.
In contrast to previous works that studies target BErr (Reich et al., 2019), we simultaneously
model all BSEQ available till current week t using spatial and signal similarities in the temporal
dynamics of BSeq. Recent works that attempt to model spatial relations for COVID19 forecasting
need explicitly structural data (like cross-region mobility) (Panagopoulos et al., 2020) to generate
a graph or use attention over temporal patterns of regions’ death trends. B2F, in contrast, directly
models the structural information of signal graph (containing features from each region) using
BSeq similarities. Thus, we first generate useful latent representations for each signal based on
BSeq revision information of that feature as well as features that have shown similar revision
patterns in the past. Due to the large number of signals that cover all regions, we cannot model
the relations between every pair using fully connected modules or attention similar to (Jin et al.,
2020). Therefore, we first construct a sparse graph between signals based on past BSeq simi-
larities. Then we inject this similarity information using Graph Convolutional Networks (GCNs)
and combine it with deep sequential models to model temporal dynamics of BSeq of each signal
while combining information from BSeq s of signals in the neighborhood of the graph. Further,
5
Published as a conference paper at ICLR 2022
we use these latent representations and leverage the history of a model M’s predictions to re-
fine its prediction. Thus, B2F solves BFRPk (M) assuming M is a black box, accessing only its
past forecasts. Our training process, that involves pre-training on model-agnostic auxiliary task,
greatly improves training time for refining any given model M . The full pipeline of B2F is also
shown in Figure 5. Next, we describe each of the components of B2F in detail. For the rest of
this section, we will assume that we are forecasting k weeks ahead given data till current week t.
L IL Refined
Refiner —Prediction
~τ~
Current Week
Prediction
GraphGen generates an
undirected signal graph
Gt =	(V, Et) whose
edges represent similar-
ity in BSeqs between
signals, where vertices
V = F = Reg × Feat. We
measure similarity using
DTW distance due to rea-
sons described in Section 2.
GraphGen leverages the
similarities across BSeq
Figure 5: B2F pipeline with all components	patterns irrespective of the
exact nature of canonical behaviors which may vary across domains. We compute the sum of DTW
distances of BSEQs for each pair of nodes summed over t0 ∈ {1, 2, . . . , t - 5}. We threshold t0 till
t - 5 to make the BSEQs to be of reasonable length (at least 5) to capture temporal similarity without
discounting too many BSEQs. Top τ node pairs with lowest total DTW distance are assigned an edge.
BseqEnc. While we can model backfill sequences for each signal independently using a recurrent
neural network, this doesn’t capture the behavioral similarity of BSeq across signals. Using a
fully-connected recurrent neural network that considers all possible interactions between signals also
may not learn from the similarity information due to the sheer number of signals (50 × 21 = 1050)
while greatly increasing the parameters of the model. Thus, we utilize the structural prior of
graph Gt generated by GRAPHGEN and train an autoregressive model BSEQENC which consists
of graph recurrent neural network to encode a latent representation for each of backfill sequence in
Bt = {BSEQ(i, t) : i ∈ F}. At week t, B SEQENC is first pre-trained and then it is fine-tuned for a
specific model M (more details later in this section).
Our encoding process is in Figure 5. Let BSEQt0+r(i, t0) be first r + 1 values of BSEQ(i, t0) (till
0	0	(t0 )
week t0 + r). For a past week t0 and revision week r, we denote hi,tr0 ∈ Rm to be the latent encoding
of B SEQt0 (i, t0) where t0r = t0 + r and t0 ≤ t0r ≤ t. We initialize hi(,0t)0 for any observation week
t0 to be a learnable parameter hi(0) ∈ Rm specific to signal i. For each week t0r we combine latent
encoding hi(,ttr0-1) and signal value di(,ttr0) using a GRU (Gated Recurrent Unit) (Cho et al., 2014) cell to
(t0)
get the intermediate embedding vi,t . Then, we leverage the signal graph Gt and pass the embeddings
{vi(,ttr0) : i ∈ F} through a Graph Convolutional layer (Kipf & Welling, 2016) to get hi(,ttr0) :
vi(,tt0r0) = GRUBE(di(,tt0r0),hi(,tt0r0-1)),	{hi(,tt0r0)}i∈F = GConv(Gt, {vi(,tt0r0)}i∈F).	(1)
Thus, hi(,ttr0) contains information from B SEQt0 (i, t0) and structural priors from Gt. Using hi(,ttr0),
BSEQENC predicts the value di(,ttr0+1) by passing through a 2-layer feed-forward network FFNi :
(t0 +1)	(t0 )
di,tr0 = FFNi(hi,tr0 ). During inference, we only have access to real-time values of signals for
the current week. We autoregressively predict hi(,tt+l) for each signal by initially passing {di(,tt) }i∈F
through BSEQENC and using the output {ci,^+11}i∈F as input for BSEQENC Iterating this l times
We get {h^+ll}i∈F along with {d(t+l}i∈F where l is a hyperparameter.
ModelPredEnc. To learn from history of a model’s predictions and its relation to target revisions,
ModelPredEnc encodes the history of model’s predictions, previous real-time targets, and revised
(up to current week) targets using a Recurrent Neural Network. Given a model M, for each observa-
tion week t0 ∈ {1, 2, . . . , t-1-k}, we concatenate the model’s predictions y(M, k)t0, real-time target
6
Published as a conference paper at ICLR 2022
+	0	t+
y t o + k as seen on observation Week t and revised target y to + k as c to — y(im , k) t0 ⅛∏ y to + k y to+k,
where ㊉ is the concatenation operator. A GRU is used to encode the sequence {C(t),..., C(1)1-k}:
{z1(t),...,zt(-t)1-k}—GRUME({C1(t),...,Ct(-t)1-k})	(2)
Refiner. It leverages the information from above three modules of B2F to refine model M ’s predic-
tion for current week y(M, k)t. Specifically, it receives the latent encodings of signals {hi(,tt+l)}i∈F
from BSEQENC, zt(-t)k-1 from MODELPREDENC, and the model’s prediction y(M, k)t for week t.
BSeq encoding from different signals may have variable impact on refining the signal since a few
signals may not very useful for current week’s forecast (e.g., small revisions in mobility signals may
not be important in some weeks). Moreover, because different models use signals from CoVDS
differently, we may need to focus on some signals over others to refine its prediction. Therefore,
we first take attention over BSEQ encodings from all signals {hi(,tt+l)}i∈F w.r.t y(M, k)t. We use
multiplicative attention mechanism with parameter w ∈ Rm based on Vaswani et al. (2017):
αi = SoftmaXi(y(M,k)twTh；+l),	h(t) = X《此+) .	(3)
i∈F
Finally we combine h(t) and z(-)k-1 through a two layer feed-forward layer FNNRF which
outputs a 1-dim value followed by tanh activation to get the correction γt ∈ [-1, 1] i.e., γt —
tanh(FFNRF(h(t) ㊉ z(-)k-1)). Finally, the refined prediction is y*(M, K)t — (Y + 1)y(M, K)t.
Note that we limit the correction by B2F by at most the magnitude of model’s prediction because the
average BERR of targets is 4.9% and less than 0.6% of them have BERR over 1. Therefore, we limit
the refinement of prediction to this range.
Training: There are two steps of training involved for B2F: 1) model agnostic autoregressive BSEQ
prediction task to pre-train BseqEnc; 2) model-specific training for Bfrp.
Autoregressive BSEQ prediction: Pre-training on auxiliary tasks to improve the quality of latent
embedding is a well-known technique for deep learning methods (Devlin et al., 2019; Radford et al.,
2018). We pre-train B seqEnc to predict the next values of backfill sequences{x(tot,ori+1)}i∈F.
Note
that we only use BSEQ sequences {BSEQt(t0, i)}i∈F,to<t available till current week t for training
BseqEnc. The training procedure in itself is similar to Seq2Seq prediction problems (Sutskever
et al., 2014) where for initial epochs we use the ground truth inputs at each step (teacher forcing) and
then transition to using output predictions of previous time step by the recurrent module as input to
next time step. Once we pre-train B seqEnc, we can use it for Bfrp as well as Lbrp for current
week t for any model M. Fine-tuning usually takes less than half the epochs required for pre-training
enabling quick refinement of multiple models in parallel.
Model specific end-to-end training: Given the pre-trained B SEQENC, we train, end-to-end,
the parameters of all modules of B 2F. The training set consists of past model predictions
hy(M, k)1, y(M, k)2, . . . y(M, k)t-1i and backfill sequences {BSEQt(t0, i)}i∈F,to≤t. For datapoint
y(M, k)to of week t0 < t - k, we input backfill sequences of signals whose observation week is
t0 into BSEQENC to get latent encodings {ht(ot,)i}i∈F. We also derive zt(ot) from MODELPREDENC
and finally Refiner ingestszt(ot),{htto,i}i∈Fand y(M, k)to to get γto. Overall, we optimize the loss
function: L(t) — Pit=-1k-1γtoy(M, k)to - yt(ot+) k . Following real-time forecasting, we train B2F
each week from scratch (including pre-training). Throughout training and forecasting for week t, we
use Gt as input to B SEQENC since it captures average similarities in BSEQs till current week t.
4	Back2Future Experimental Results
In this section, we describe a detailed empirical study to evaluate the effectiveness of our framework
B2F. All experiments were run in an Intel i7 4.8 GHz CPU with Nvidia Tesla A4 GPU. The model
typically takes around 1 hour to train for all regions. The appendix contains additional details (all
hyperparameters and results for June-Dec 2020 and k — 1, 3 and GDP forecasting). We also release
the code and datasets at www.github.com/AdityaLab/Back2Future.
Setup: We perform real-time forecasting of COVID-19 related mortality (Deaths) for 50 US states.
We leveraged observations (Section 2) from BSeq for period June 2020 - Dec. 2020 to design B2F.
7
Published as a conference paper at ICLR 2022
We tuned the model hyperparameters using data from June 2020 to Aug. 2020 and tested it on the rest
of dataset including completely unseen data from Jan. 2021 to June 2021. For each week t, we train
the model using the CoVDS dataset available till the current week t (including BSEQs for all signals
revised till t) for training. As described in Section 3, for each week, we first pre-train B SEQENC on
BSeq data and then train all components of B2F for each model we aim to refine. Then, we predict
the forecasts Deaths y* (M, k)t for each model M. Similarly We also evaluated B2F for real-time
GDP forecasting task with detailed results in Appendix. We observed that setting hyperparameter
τ = c|F| Where c ∈ {2, 3, 4, 5} provided best results. Note that τ influences the sparsity of the graph
as Well as the efficiency of the model since sparser graphs lead to fast inference across GConv layers.
We also found setting l = 5 provided the best performance.
Evaluation: Refined prediction y*(M, K)t are evaluated against the most revised version of the
target yt(0t+f)k, Where tf is second Week of Feb 2021. For evaluation, We use standard metrics in this do-
main Reich et al. (2019); Adhikari et al. (2019). Let absolute error of prediction e(M, k)t = |yt(+tfk) -
y*(M, K)t| for a week t and model M. We use (a) Mean Absolute Error MAE(M) = T PT= 1 |
e(M, k)t | and (b) Mean Absolute Percentage Error MAPE =击 PT= 1 e(M, k)t / | y(f |.
Candidate models: We focus on refining/rectifying the top models from the COVID-19 Forecast
Hub described in Section 2; these represent different variety of statistical and mechanistic models.
Table 2: B2F consistently refines all models. % improvements in MAE and Baselines: Due to
MAPE scores	averaged over	all regions from Jan 202 k=2		1 to June 2021	the novel problem, k=4	Ithere are no standard	
Cand. Model	Refining Model	MAE	MAPE	MAE	MAPE	baselines. Therefore,
Ensemble	^FN	-0.35 ± 0.11	-0.12 ± 0.22	0.87 ± 0.64	0.77 ± 0.14	L [. 	Clir πacp Iiπpc ClrQ
	B2F-MB	-2.23 ± 0.82	-1.57 ± 0.65	-2.19 ± 0.35	2 85 ± 0 53 our	UaSe!!lies	are
	B2F-NoGraph	-1.45 ± 0.14	-2.73 ± 0.35	-5.72 ± 0.21	-6.72 ± 0.82 used to study the
	B2F-Bseq	1.42 ± 0.60	0.37 ± 0.75	0.74 ± 0.36	0∙44 ± 0.07 effect of using BSEQS
	Back2Future	5.25 ± 0.13	4.39 ± 0.62	4.41 ± 0.73	1 ς ɪ λ C7	一	一一一0	.	, 	.	.	QTif] TTICrleI hias/lVfR)
GT-DC	^FN	-2.42 ± 0.22	-1.51 ± 0.90	-1.54 ± 0.57	0 48 ± 0 43 anvi∙ ∙mιou∙e! u!as(，v，Jɔ)
	B2F-MB	-3.02 ± 0.40	-3.41 ± 0.16	-2.91 ± 0.29	-3.22 ± 0.74 on refinement. (a)
	B2F-NoGraph	2.24 ± 0.37	3.51 ± 0.21	1.93 ± 0.39	0.78 ± 0.37 ffn： train a simple
	B2F-Bseq	2.13 ± 0.12	3.84 ± 0.78	1.08 ± 0.23	2.33 ± 0.97 feed-forward neural
	Back2Future	10.33 ± 0.19	11.84 ± 0.18	9.92 ± 0.98	1127 ± 0 88 ɪeeu ɪoiw ar H neuιaι ..
YYG	^FN	-2.08 ± 0.39	-1.34 ± 0.12	-2.64 ± 0.13	-3.36 ± 0.18 network for regression
	B2F-MB	-3.84 ± 0.08	-6.99 ± 0.56	-8.84 ± 0.96	-5.61 ± 0.27^task that takes as
	B2F-NoGraph	-1.25 ± 0.70	--0.7 ± 0.90	-6.13 ± 0.08	-5.31 ± 0.06 inputs model,s ∏re-
	B2F-Bseq	-1.78 ± 0.74	-2.26 ± 0.83	-0.79 ± 0.21	0 62 ± 027 Lnpuis ∙i,i∙ov∣∙e∙i∙ S Pre
	Back2Future	8.93 ± 0.26	6.32 ± 0.44	7.32 ± 0.42	5.73 ± 0.66 diction and real-time
UMASS-MB	FFN	-3.25 ± 0.38-	-5.74 ± 0.75~	-1.01 ± 0.18~	-5.28 ± 0.07-target to predict the
	B2F-MB	--8.2 ± 0.61	-7.54 ± 0.26	-6.49 ± 0.29	-7.56 ± 0.30 stable target	(b)
	B2F-NoGraph	-2.16 ± 0.22	-1.88 ± 0.67	-2.15 ± 0.24	-2 87 ± 0 34 SlaUi∙e IaIgeι.	(U) ..
	B2F-Bseq	1.58 ± 0.49	0.86 ± 0.17	0.36 ± 0.06	0.96±0.82 B2F-MB： exploits
	Back2Future	5.43 ± 0.51	4.66 ± 0.63	3.32 ± 0.76	3.11 ± 0.29一 only model bias and
CMU-TS	^FN	-5.24 ± 0.57	-4.93 ± 0.39	-3.12 ± 0.71	-0.65 ± 0.81 uses the ModelPre-
	B2F-MB	-8.17 ± 0.34	-8.21 ± 0.24	-3.72 ± 0.32	-6.11 ± 0.84
	B2F-NoGraph	-0.67 ± 0.69	-0.57 ± 0.32	-0.46 ± 0.07	-1.77 ± 0.79 DEnc architecture and
	B2F-Bseq	1.46 ± 0.33	1.05 ± 0.16	2.38 ± 0.43	2.26 ± 0.02^append a linear layer
	Back2Future	-7.5 ± 0.60	8.04 ± 0.58	5.73 ± 0.19	6.22 ±0.58 that takes encodings
from ModelPredEnc and model’s prediction (c) B2F-Bseq: exploits only BSeq without
model bias and only uses B seqEnc architecture and append a linear layer that takes encodings
from B seqEnc and model’s prediction (d) B2F-NoGraph: does not use BSeq similarity from
GraphGen by removing graph convolutional layers and retain only RNNs.
Refining real-time model-predictions: We compare the mean percentage improvement (decrease)
in scores of B2F refined predictions of diverse set of top models w.r.t stable targets over 50 US
states.We observe that B2F is the only method, compared to baselines, that improves scores for all
candidate models consistently (Table 2). The poor scores of baselines also shows the necessity of
incorporating both backfill information (unlike FFN and B2F-MB) and model prediction history
(unlike B2F-BSEQ and B2F-NOGRAPH).
We achieve substantial avg. improvements of 6.93% and 6.79% in MAE and MAPE respectively
with low standard deviation across 29 test weeks which shows that the improvements were consistent
across time and not just over few weeks that experienced large revision anomalies. Candidate models
refined by B2F show improvement of over 10% in over 25 states and over 15% in 5 states (NJ, LA,
GA, CT, MD). The improved predictions of CMU-TS and GT-DC (ranked 3rd and 4th in COVID-19
8
Published as a conference paper at ICLR 2022
Forecast Hub) due to B2F, outperform all the models in the hub (except for Ensemble) with 7.17%
and 4.13% improvements in MAE respectively. UMass-MB, ranked 2nd, is improved by 11.24%.
B2F also improves Ensemble, the current best-performing model of the hub, by 3.6% - 5.18% with
over 5% improvement in 38 states, and with IL and TX experiencing over 15% improvement.
(a) Average %
decrease of MAE
Figure 6: (a) B2F refines ENSEMBLE predictions
significantly for most states. (b) efficacy of B2F
ramps up within 6 weeks of revision data.
(b) % improve. in
MAE for each week
Rectifying real-time model-evaluation: We
evaluate the efficacy of B2F in rectifying the
real-time evaluation scores for the Lbrp prob-
lem. We noted in Obs 5 that real-time MAE
was lower than stable MAE by 9.6 on average.
The difference between B2F rectified estimates
MAE stable MAE was reduced to 4.4, a 51.7%
decrease (Figure 7). This results in increased
MAE scores across most regions towards stable
estimates. Eg: We reduce the MAE difference
in large highly populated states such as GA by
26.1% (from 22.52 to 16.64) and TX by 90% (from 10.8 to 1.04) causing an increase in MAE scores
from real-time estimates by 5.88 and 9.4 respectively.
Refinement as a function of data availability: During the initial weeks of the pandemic, we have
access to very little revision data both in terms of length and number of BSeq. So we evaluate the
mean performance improvement for each week across all regions (Figure 6b). B2F’s performance
ramps up and quickly stabilize in just 6 weeks. Since signals need around 4 weeks (Obs 2) to stabilize,
this ramp-up time using small amount of revision data is impressive.
B2F adapts to anomalies During real-time forecasting, models need
to be also robust to rare events of large anomalous data revisions, like
during the initial stages of the pandemic when data collection was
not fully streamlined. Consider observation week 5 where there was
an abnormally large revision to deaths nationwide (Figure 8a) when
the BErr was 48%. B2F still provided significant improvements of
Figure 7: B2F rectified MAE up to 74.2% for most model predictions (Figure 8b).
are closer to stable MAE
B2F refines GDP forecasts To evaluate the extensibility of B2F to
other domains that encounter the problem of backfill, we tested on the task of forecasting US National
GDP using 25 macroeconomic indicators from past and their revision history for years 2000-2021. We
found that B2F improves predictions of candidate models by 6%-15% and significantly outperforms
baselines. The details of the dataset and results are found in Appendix Sections B, E.2.
5	Conclusion
We introduced the important and chal-
lenging multi-variate backfill problem
using COVID-19 and GDP forecast-
ing as examples. We compiled and
released the comprehensive CoVDS
dataset to study revision patterns in
features and targets as well as aid in
COVID-19 forecasting. We presented
(a) Revision of US deaths (b) % decrease in MAE due to B2F
on week 5	refinement on week 5 targets
Figure 8: B2F adapts to abnormally high revisions
Back2Future (B2F), the novel deep-learning method to model this phenomenon, which exploits our
observations of cross-signal similarities using Graph Recurrent Neural Networks to refine predictions
and rectify evaluations for a wide range of models. Our extensive experiments showed that leveraging
similarity among backfill patterns as well as model bias via our proposed method leads to significant
6 - 11% improvements in all the top models.
As future work, our work can potentially help improve data collection and alleviate systematic
differences in reporting capabilities across regions. For example, B2F provides significant gains
consistently across time including when there are large anomalies in data. Therefore, our revision
modelling approach can be helpful for anomaly detection (Homayouni et al., 2021). We can also study
how backfill can affect uncertainty calibration in time-series analysis (Yoon et al., 2020). Adapting to
situations where data revisions can occur at different frequencies is another research direction.
9
Published as a conference paper at ICLR 2022
6	Ethics Statement
The features used in the CoVDS dataset and for GDP forecasting are publicly available and
anonymized without any sensitive information. Our backfill refinement framework and B2F is
generalizable to any domain that deals with real-time prediction tasks with feature revisions. Due to
the relevance of our dataset to public health and macroeconomics, prospects for misuse should not be
discounted. The disparities in data collection across features and regions can also have implications
on equity of prediction performance and is an interesting direction of research.
7	Reproduciblity Statement
As described in Section 4, we evaluated our model over 5 runs with different random seeds to
show the statistical significance of our method. We also provide a more extensive description of
hyperparameters and data pre-processing in the Appendix. The code for B2F and the CoVDS dataset
is publicly available at https://github.com/AdityaLab/Back2Future.
8	Acknowledgements
This work was supported in part by the NSF (Expeditions CCF-1918770, CAREER IIS-2028586,
RAPID IIS-2027862, Medium IIS-1955883, Medium IIS-2106961, CCF-2115126), CDC MInD
program, ORNL, and faculty research awards from Facebook, funds/computing resources from
Georgia Tech.
10
Published as a conference paper at ICLR 2022
References
Bijaya Adhikari, Xinfeng Xu, Naren Ramakrishnan, and B Aditya Prakash. Epideep: Exploiting
embeddings for epidemic forecasting. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining,pp. 577-586, 2019.
Angel Aguiar. Macroeconomic data. 2015.
Nick Altieri, Rebecca L Barter, James Duncan, Raaz Dwivedi, Karl Kumbier, Xiao Li, Robert
Netzorg, Briton Park, Chandan Singh, Yan Shuo Tan, Tiffany Tang, Yu Wang, Chao Zhang, and
Bin Yu. Curating a covid-19 data repository and forecasting county-level death counts in the united
states. Harvard Data Science Review, 2 2021. doi: 10.1162/99608f92.1d4e0dae. URL https://
hdsr.mitpress.mit.edu/pub/p6isyf0g. https://hdsr.mitpress.mit.edu/pub/p6isyf0g.
Apple. Apple mobility trends reports., 2020. URL www.apple.com/covid19/mobility.
Alberto Baffigi, Roberto Golinelli, and Giuseppe Parigi. Bridge models to forecast the euro area gdp.
International Journal of forecasting, 20(3):447-460, 2004.
Matthew Biggerstaff, Michael Johansson, David Alper, Logan C Brooks, Prithwish Chakraborty,
David C Farrow, Sangwon Hyun, Sasikiran Kandula, Craig McGowan, Naren Ramakrishnan, et al.
Results from the second year of a collaborative effort to forecast influenza seasons in the united
states. Epidemics, 24:26-33, 2018.
Logan C. Brooks, David C. Farrow, Sangwon Hyun, Ryan J. Tibshirani, and Roni Rosenfeld.
Nonmechanistic forecasts of seasonal influenza with iterative one-week-ahead distributions. PLOS
Computational Biology, 14(6):e1006134, June 2018. ISSN 1553-7358. doi: 10.1371/journal.pcbi.
1006134. URL https://dx.plos.org/10.1371/journal.pcbi.1006134.
Andrea Carriero, Michael P Clements, and Ana Beatriz Galvao. Forecasting with bayesian multivari-
ate vintage-based vars. International Journal of Forecasting, 31(3):757-768, 2015.
CDC. Coronavirus Disease 2019 (COVID-19), 2020. URL https://www.cdc.gov/
coronavirus/2019-ncov/cases-updates/.
Prithwish Chakraborty, Pejman Khadivi, Bryan Lewis, Aravindan Mahendiran, Jiangzhuo Chen,
Patrick Butler, Elaine O Nsoesie, Sumiko R Mekaru, John S Brownstein, Madhav V Marathe, et al.
Forecasting a moving target: Ensemble models for ili case count predictions. In Proceedings of the
2014 SIAM international conference on data mining, pp. 262-270. SIAM, 2014.
Prithwish Chakraborty, Bryan Lewis, Stephen Eubank, John S. Brownstein, Madhav Marathe, and
Naren Ramakrishnan. What to know before forecasting the flu. PLoS Computational Biology,
14(10), October 2018. ISSN 1553-734X. doi: 10.1371/journal.pcbi.1005964. URL https:
//www.ncbi.nlm.nih.gov/pmc/articles/PMC6193572/.
Judith A Chevalier, Jason L Schwartz, Yihua Su, Kevin R Williams, et al. Measuring movement and
social contact with smartphone data: A real-time application to covid-19. Technical report, Cowles
Foundation for Research in Economics, Yale University, 2021.
Kyunghyun Cho, B. V. Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of
neural machine translation: Encoder-decoder approaches. ArXiv, abs/1409.1259, 2014.
Michael P Clements and Ana Beatriz Galvao. Data revisions and real-time forecasting. In Oxford
Research Encyclopedia of Economics and Finance. 2019.
COVID-Tracking. The covid tracking project., 2020. URL https://covidtracking.com.
Estee Y Cramer, Velma K Lopez, Jarad Niemi, Glover E George, Jeffrey C Cegan, Ian D Dettwiller,
William P England, Matthew W Farthing, Robert H Hunter, Brandon Lafferty, et al. Evaluation of
individual and ensemble probabilistic forecasts of covid-19 mortality in the us. medRxiv, 2021.
Dean Croushore. Forecasting with real-time data vintages. The Oxford handbook of economic
forecasting, pp. 247-267, 2011.
11
Published as a conference paper at ICLR 2022
Aron Culotta. Towards detecting influenza epidemics by analyzing twitter messages. In Proceedings
ofthefirst workshop on social media analytics, pp. 115-122, 2010.
Songgaojun Deng, Shusen Wang, Huzefa Rangwala, Lijing Wang, and Yue Ning. Cola-gnn: Cross-
location attention based graph neural networks for long-term ili prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management, pp. 245-254,
2020.
J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT, 2019.
David Farrow. Modeling the past, present, and future of influenza. Private Communication, Farrow’s
work will be part of the published Phd thesis at Carnegie Mellon University, 2016.
Jeremy Ginsberg, Matthew H Mohebbi, Rajan S Patel, Lynnette Brammer, Mark S Smolinski, and
Larry Brilliant. Detecting influenza epidemics using search engine query data. Nature, 457(7232):
1012-1014, 2009.
Google. Google covid-19 community mobility reports., 2020. URL https://www.google.
com/covid19/mobility/.
Iwona Hawryluk, Henrique Hoeltgebaum, Swapnil Mishra, Xenia Miscouridou, Ricardo P Schneken-
berg, Charles Whittaker, Michaela Vollmer, Seth Flaxman, Samir Bhatt, and Thomas A Mel-
lan. Gaussian process nowcasting: Application to covid-19 mortality reporting. arXiv preprint
arXiv:2102.11249, 2021.
Inga Holmdahl and Caroline Buckee. Wrong but Useful — What Covid-19 Epidemiologic Models
Can and Cannot Tell Us. NEJM, 383(4):303-305, 2020.
Hajar Homayouni, Indrakshi Ray, Sudipto Ghosh, Shlok Gondalia, and Michael G Kahn. Anomaly
detection in covid-19 time-series data. SN Computer Science, 2(4):1-17, 2021.
Xiaoyong Jin, Yu-Xiang Wang, and Xifeng Yan. Inter-series attention model for covid-19 forecasting.
ArXiv, abs/2010.13006, 2020.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Vasileios Lampos, Tijl De Bie, and Nello Cristianini. Flu detector-tracking epidemics on twitter.
In Joint European conference on machine learning and knowledge discovery in databases, pp.
599-602. Springer, 2010.
Vasileios Lampos, Andrew C Miller, Steve Crossan, and Christian Stefansen. Advances in nowcasting
influenza-like illness rates using search query logs. Scientific reports, 5(1):1-10, 2015.
JF Lawless. Adjustments for reporting delays and the prediction of occurred but not reported events.
Canadian Journal of Statistics, 22(1):15-31, 1994.
Shenghua Liu, Bryan Hooi, and Christos Faloutsos. Holoscope: Topology-and-spike aware fraud
detection. In Proceedings of the 2017 ACM on Conference on Information and Knowledge
Management, pp. 1539-1548, 2017.
Massimiliano Marcellino. A linear benchmark for forecasting gdp growth and inflation? Journal of
Forecasting, 27(4):305-340, 2008.
C Jessica E Metcalf and Justin Lessler. Opportunities and challenges in modeling emerging infectious
diseases. Science, 357(6347):149-152, 2017.
Baltazar Nunes, Isabel Natdrio, and M LUcilia Carvalho. NoWcasting influenza epidemics using
non-homogeneous hidden markov models. Statistics in Medicine, 32(15):2643-2660, 2013.
Dave Osthus, Ashlynn R Daughton, and Reid Priedhorsky. Even a good influenza forecasting model
can benefit from internet-based noWcasts, but those benefits are limited. PLoS computational
biology, 15(2):e1006599, 2019a.
12
Published as a conference paper at ICLR 2022
Dave Osthus, James Gattiker, Reid Priedhorsky, Sara Y Del Valle, et al. Dynamic bayesian influenza
forecasting in the united states with hierarchical discrepancy (with discussion). Bayesian Analysis,
14(1):261-312, 2019b.
G. Panagopoulos, Giannis Nikolentzos, and M. Vazirgiannis. United we stand: Transfer graph neural
networks for pandemic forecasting. ArXiv, abs/2009.08388, 2020.
George Panagopoulos, Giannis Nikolentzos, and Michalis Vazirgiannis. Transfer graph neural net-
works for pandemic forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence,
2021.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing with unsupervised learning. 2018.
Prashant Rangarajan, Sandeep K Mody, and Madhav Marathe. Forecasting dengue and influenza
incidences using a sparse representation of google trends, electronic health records, and time series
data. PLoS computational biology, 15(11):e1007518, 2019.
Nicholas G. Reich, Logan C. Brooks, Spencer J. Fox, Sasikiran Kandula, Craig J. McGowan, Evan
Moore, Dave Osthus, Evan L. Ray, Abhinav Tushar, Teresa K. Yamana, Matthew Biggerstaff,
Michael A. Johansson, Roni Rosenfeld, and Jeffrey Shaman. A collaborative multiyear, multimodel
assessment of seasonal influenza forecasting in the United States. Proceedings of the National
Academy of Sciences of the United States of America, 116(8):3146-3154, 2019. ISSN 1091-6490.
doi: 10.1073/pnas.1812594116.
John C Robertson and Ellis W Tallman. Vector autoregressions: forecasting and reality. Economic
Review-Federal Reserve Bank of Atlanta, 84(1):4, 1999.
Alexander Rodriguez, Nikhil Muralidhar, Bijaya Adhikari, Anika Tabassum, Naren Ramakrishnan,
and B Aditya Prakash. Steering a historical disease forecasting model under a pandemic: Case of
flu and COVID-19. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021a.
Alexander Rodriguez, Anika Tabassum, Jiaming Cui, Jiajia Xie, Javen Ho, Pulak Agarwal, Bijaya
Adhikari, and B. Aditya Prakash. DeepCOVID: An Operational Deep Learning-Driven Framework
for Explainable Real-Time COVID-19 Forecasting. In Proceedings of the AAAI Conference on
Artificial Intelligence, 2021b.
Gerhard Runstler and Franck S6dillot. Short-term estimates of euro area real gdp by means of
monthly data. Technical report, ECB working paper, 2003.
Jeffrey Shaman and Alicia Karspeck. Forecasting seasonal outbreaks of influenza. Proceedings of
the National Academy of Sciences, 109(50):20425-20430, 2012.
Katrina Stierholz. Economic data revisions: What they are and where to find them. DttP, 45:4, 2017.
Oliver Stoner and Theo Economou. Multivariate hierarchical frameworks for modeling delayed
reporting in count data. Biometrics, 76(3):789-798, 2020.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
Greg Tkacz and Sarah Hu. Forecasting gdp growth using artificial neural networks. Technical report,
Bank of Canada, 1999.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.
Peng-Wei Wang, Wei-Hsin Lu, Nai-Ying Ko, Yi-Lung Chen, Dian-Jeng Li, Yu-Ping Chang, and
Cheng-Fang Yen. Covid-19-related information sources and the relationship with confidence
in people coping with covid-19: Facebook survey study in taiwan. Journal of medical Internet
research, 22(6):e20021, 2020.
Shihao Yang, Mauricio Santillana, and Samuel C Kou. Accurate estimation of influenza epidemics
using google search data via argo. Proceedings of the National Academy of Sciences, 112(47):
14473-14478, 2015.
13
Published as a conference paper at ICLR 2022
Changchang Yin, Ruoqi Liu, Dongdong Zhang, and Ping Zhang. Identifying sepsis subphenotypes
via time-aware multi-modal auto-encoder. In Proceedings of the 26th ACM SIGKDD international
conference on knowledge discovery & data mining, pp. 862-872, 2020.
Jaesik Yoon, Gautam Singh, and Sungjin Ahn. Robustifying sequential neural processes. In
Hal Daume In and Aarti Singh (eds.), Proceedings ofthe 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research, pp. 10861-10870. PMLR,
13-18 Jul 2020. URL http://proceedings.mlr.press/v119/yoon20c.html.
Qian Zhang, Nicola Perra, Daniela Perrotta, Michele Tizzoni, Daniela Paolotti, and Alessandro
Vespignani. Forecasting seasonal influenza fusing digital indicators and a mechanistic disease
model. In Proceedings of the 26th International Conference on World Wide Web, pp. 311-319.
International World Wide Web Conferences Steering Committee, 2017.
Indre Zliobaite. Change with delayed labeling: When is it detectable? In 2010 IEEE International
Conference on Data Mining Workshops, pp. 843-850. IEEE, 2010.
14