Published as a conference paper at ICLR 2022
Feature Kernel Distillation
Bobby He1,2* & Mete Ozay2
1Department of Statistics, University of Oxford
2 Samsung Research UK
Ab stract
Trained Neural Networks (NNs) can be viewed as data-dependent kernel ma-
chines, with predictions determined by the inner product of last-layer represen-
tations across inputs, referred to as the feature kernel. We explore the relevance
of the feature kernel for Knowledge Distillation (KD), using a mechanistic un-
derstanding of an NN’s optimisation process. We extend the theoretical analysis
of Allen-Zhu & Li (2020) to show that a trained NN’s feature kernel is highly
dependent on its parameter initialisation, which biases different initialisations of
the same architecture to learn different data attributes in a multi-view data set-
ting. This enables us to prove that KD using only pairwise feature kernel compar-
isons can improve NN test accuracy in such settings, with both single & ensemble
teacher models, whereas standard training without KD fails to generalise. We
further use our theory to motivate practical considerations for improving student
generalisation when using distillation with feature kernels, which allows us to pro-
pose a novel approach: Feature Kernel Distillation (FKD). Finally, we experimen-
tally corroborate our theory in the image classification setting, showing that FKD
is amenable to ensemble distillation, can transfer knowledge across datasets, and
outperforms both vanilla KD & other feature kernel based KD baselines across a
range of standard architectures & datasets.
1 Introduction & Background
A prevailing belief in the Deep Learning community is that feature learning, where data-dependent
features are acquired during training, is crucial to explaining the empirical success of Neural Net-
works (NNs) (Fort et al., 2020; Baratin et al., 2021). A comparison in this regard is often made to
kernel methods (Jacot et al., 2018), which can be thought ofas feature selection methods from a fixed
data-independent set of features. This separation has been caricaturised as a distinction between fea-
ture learning and kernel learning regimes (Chizat et al., 2019; Yang & Hu, 2020; Woodworth et al.,
2020) of NN training. Though less amenable to theoretical analysis compared to kernel regimes,
feature learning regimes have the capability to capture more of the complex empirical phenomenon
that one can observe in NNs due to parameter-space non-convexity, such as: i) how ensembling
trained NNs differing solely in their independent parameter initialisations can lead to improvements
in predictive accuracy & uncertainty (Lakshminarayanan et al., 2017; Allen-Zhu & Li, 2020), or
ii) the effectiveness of knowledge distillation with both single & ensemble teacher models (BUcilUa
et al., 2006; Hinton et al., 2015). This implies that in order to understand ensembling & knowledge
distillation (KD) in NNs, we need to Understand the mechanisms of NN featUre learning regimes.
Ensembling can be loosely sUmmarised as aggregating predictions from mUltiple models, & is Used
widely across machine learning (ML) to improve performance (Dietterich, 2000; Breiman, 2001).
Conversely, knowledge distillation (KD), the idea of transferring knowledge from a teacher model
to a stUdent model, has garnered the most attention with NNs. Remarkably, via KD it is possible to
tResearched	during internship at Samsung Research UK. Correspondence to
bobby.he@stats.ox.ac.uk.
1
Published as a conference paper at ICLR 2022
significantly improve a single student’s generalisation with knowledge from a teacher model, or an
ensemble of teachers. This means that the single student’s model has enough flexibility to generalise
well (relative to the teacher), thus one must factor in the optimisation process (such as the parameter
initialisation) in order to explain the mechanisms of ensembling and KD in NNs.
To describe KD, suppose We have N input-target (x, y)∈Rd×RCS data pairs TD={(xi, yi)}N=ι
sampled i.i.d. (independent & identically distributed) from some distribution D, and a stu-
dent NN architecture fs(x) = WS∙hs(x, θs). Here hs(x, θs)∈Rm×1 is a student-specific
feature extractor model (e.g. MLP, CNN, ResNet, or Transformer) With parameters θS , and
WS ∈ RCS ×m is a parameter matrix for the last layer. Assume also that We have loss:
L(θs ,Ws )=N PLL(fS(xi), yi) Which We seek to minimise over θS, WS in the hope that fS
can generalise to unseen (x, y) pairs. L is typically cross-entropy in the classification setting.
Vanilla KD (Hinton et al., 2015) distils knoWledge from a trained teacher netWork
fτ(X)=WT∙hτ(x, θτ)∈Rct to a student by regularising student fs towards the teacher fτ:
L(θs,Ws) = L(θs,Ws) + λKDN XL(fS(x^,fT(x^),	(1)
i=1	τ	τ
for temperature τ>0 & regularisation λKD>0 hyperparameters. Note, this is only valid if CT=Cs.
Following Hinton et al. (2015), many methods have been proposed using different quirks of NNs to
distil knowledge from teacher to student. A relevant line of work involves encouraging the student to
match how similar/related the teacher views two inputs x, x0 to be (Passalis & Tefas, 2018; Tung &
Mori, 2019; Park et al., 2019). These approaches have the benefit of being agnostic to teacher/student
architectures & prediction spaces CT & Cs , but as of yet remain heuristically motivated. In this
work, we explore such approaches under the more general framework of NN feature kernel (the
kernel induced by the inner product of last-layer features h) learning, allowing us to provide the
missing theoretical justification. Moreover, we use our theoretical insights to introduce practical
improvements for FKD in Section 4, which we show outperform these previous works in Section 5.
Allen-Zhu & Li (2020) provide the first theoretical exposition of the mechanisms by which vanilla
KD and ensembling improve generalisation in NNs. To this end, the authors introduce the notion of
multi-view data, which is when a class in a multi-class classification problem has multiple identify-
ing features/attributes. For example, an image ofa car can be discerned by i) wheels, ii) windows, or
iii) headlights. The key idea is that the NN parameter initialisation, and its random correlations with
certain attributes, will bias the NN to learn only a subset of the entire set of attributes pertaining to
a given class. When presented with single-view data lacking the class-identifying attribute that the
NN has learnt, the NN will not generalise. For example, an NN that has learnt to classify cars based
on if they have headlights will not generalise to a side-on image of a car that occludes headlights.
The implication then is that ensembling NNs works in part because independent parameter initiali-
sations learn independent sets of attributes, so more data features will be learnt across the ensemble
model. Moreover, it is argued that vanilla KD in NNs works because the features learnt by the
teacher model (or models) are imparted to the student via soft teacher labels that capture ambiguity
in a given data input (such as an image of a car whose headlights look like the eyes of a cat). This is
fundamentally different to ensembling in strongly convex feature selection problems, such as linear
or random features (Rahimi & Recht, 2007) models with `2 regularisation. In such cases, different
initialisations reach the same unique optimum, and additional noise must be added to ensure predic-
tive diversity in the ensemble (Matthews et al., 2017). These analyses suggest that it is not possible
to fully explain KD or ensembling in NNs without feature learning, thus motivating our study of
Feature Kernel Distillation, where one performs KD on NN features directly.
Our contributions Feature learning can be thought of as when the feature kernel, induced by the
inner product of last-layer representations in a NN, changes during training (Yang & Hu, 2020), and
kernel learning in NNs can be thought of as when this kernel is constant. In this work, we take a
2
Published as a conference paper at ICLR 2022
feature learning perspective of knowledge distillation (KD). We first highlight the importance of the
feature kernel by viewing trained NNs as data-dependent kernel machines, & use this to motivate
Feature Kernel Distillation (FKD). In FKD, we aim to ensure that the student’s feature kernel is
well suited for improved generalisation, using both the teacher’s data-dependent feature kernel as
well as an understanding of the student NN’s optimisation process. In Section 3, we adapt the
framework of Allen-Zhu & Li (2020) to show that FKD offers the same generalisation benefits as
found in vanilla KD in a multi-view data setting, and is further amenable to ensemble distillation.
We then derive practical considerations from our insights in Section 4, to improve FKD through
an understanding of the NN’s feature learning optimisation process, compared to previous methods
which implicitly used the feature kernel for KD. Finally, in Section 5, we provide experimental
support that our theoretical claims extend to standard image classification settings, by: verifying
that FKD is amenable to ensemble distillation; can transfer knowledge across datasets with different
prediction spaces (unlike vanilla KD); and outperforms vanilla KD & previous feature kernel based
distillation methods over a range of architectures on CIFAR-100 and ImageNet-1K.
2 Motivation for feature kernel distillation
One obvious limitation of vanilla KD
is that student fS and teacher fT
need to share prediction spaces, i.e.
CS =CT . In many situations, we may
have a teacher network trained on a
dataset with a different number of
classes than the student’s dataset, and
it is not clear how one could apply
vanilla knowledge distillation. One
Figure 1: Feature Kernel Distillation (FKD) from the feature ex-
TS
tractor of a teacher hT to that of a student hS .
possibility could be to regularise directly in feature space by comparing hS and hT element-wise,
but again this either requires same teacher-student last-layer sizes or additional projection layers.
To eschew such unnecessary complications, we take the perspective of NNs as data-dependent kernel
machines. Define an NN’s feature kernel to be:
Definition 1 (Feature Kernel). Suppose we have parameters θ and last-layer NN feature extractor
h(∙, θ):Rd → Rm. For two inputs Xi, Xj ∈ Rm, the feature kernel k is the kernel induced by the
inner product ofh(xi, θ) and h(xj, θ), that is: k(xi, xj) d=ef hh(xi, θ), h(xj, θ)i.
At initialisation, it is well known that in the infinite NN-width limit, with appropriate scaling, the
feature kernel k converges almost surely to a deterministic kernel known as the Neural Network
Gaussian Process (NNGP) kernel (Neal, 2012; Lee et al., 2018; Matthews et al., 2018; Yang, 2019).
Yang & Hu (2020) show that there is a parameterisation-dependent dichotomy between kernel &
feature learning regimes for infinite-width NNs, where the feature kernel k is constant or changes
during training, respectively. It has been widely demonstrated that a crucial component of the suc-
cess of finite-width NNs is their ability to flexibly learn features, and indeed the feature kernel, from
data during training (Fort et al., 2020; Aitchison, 2020; Chen et al., 2020b; Maddox et al., 2021).
To see the importance of the feature kernel, note that for a fixed θ with many common loss functions
L, and some mild assumptions on strong convexity (which could be enforced e.g. with standard
`2 regularisation), the optimal W is uniquely determined and k determines the entire predictive
function f *(∙). For example, with squared error, L(f (x), y) = ∣∣f (x) - y∣∣2, and '2 regularisation
strength λ > 0, a trained NN is precisely kernel ridge regression with the data-dependent feature
kernel k, whose job is to measure how similar different inputs are. Thus, all teacher knowledge is
contained in its feature kernel, kT , so the feature kernel can act as our primary distillation target, as
depicted in Fig. 1. We show a corresponding result for cross-entropy loss in App. A.
Fig. 2 corroborates our claims. For a ResNet20v1 (He et al., 2016) ‘reference’ model trained on CI-
FAR10 with cross entropy, we plot test class prediction confusion matrices between said model and:
3
Published as a conference paper at ICLR 2022
i) a retrained version where all
but the last-layer parameters are
fixed (hence fully determined
by the reference model’s fea-
ture kernel as per App. A),
& ii) an independent model
trained from a different initial-
isation. As expected, there is
significantly more disagreement
across test predictions for mod-
els with different initialisations
than those which share feature
kernels. This suggests: a) differ-
ent initialisations bias the same
architecture to learn different
Figure 2: CIFAR10 test prediction confusion matrices between a fixed
reference model and a model with: (left) retrained last layer, and (right)
independent initialisation.
features, and b) the feature kernel (largely) determines a model’s test predictions. Experimental
details and a breakdown of the predictive disagreements can be found in App. F.
Having described the feature kernel as a central object in any NN, we use this to motivate our
proposed FKD, where we treat the teacher’s feature kernel, kT , as a key distillation target for the
student’s feature kernel, kS . Encouraging similarity across feature kernels shares useful features
that the teacher has learnt with the student, which we theoretically show in Section 3.
We define the FKD student loss function via an additive regularisation term between feature kernels:1
LλKD(θ, W) =f L(θ, W) + A。∙ D(kθ, kτ)	⑵
where λKD > 0 is the regularisation strength, D is some (pseudo-)distance over kernels, and the
student feature kernel kS = kθ is written to make explicit the dependence on student feature extrac-
tor parameters θ. We stress that Eq. (2) does not require matching prediction (nor feature) spaces
between teacher and student, allowing us to apply FKD across tasks, architectures, and datasets.
We consider Eq. (2) with D set to:
D(kθ,kT) = E	i.i.d.6[(kθ(xι, x2) - kT(xι, X2))p],	(3)
X1 ,X2 〜D
with expectation approximated by an average over a minibatch. In this work We choose P = 2, so
that √D gives the FrobeniUs norm of the difference in feature kernel gram matrices over a batch.
3 Theoretical analysis for FKD
We now adapt the theoretical framework of Allen-Zhu & Li (2020), which is restricted to vanilla
KD, to demonstrate the generalisation benefits of FKD over standard training. Note that FKD distils
knowledge by comparing different data points, whereas vanilla KD compares a single data point
across classes: this core difference is reflected throughout our analysis relative to Allen-Zhu &
Li (2020). We first describe the multi-view data setting & CNN architecture we consider, before
recalling that standard training without KD fails to generalise well. We then provide our main
theoretical result, Theorem 2, which shows that FKD improves student test performance. Though
our theoretical results are limited to a specific scenario, inspired by real-world data (Allen-Zhu & Li,
2020), & NN architecture, we believe the setup we consider is apt: it is simple enough tobe tractable,
yet rich enough to display the merits of FKD. Moreover, we find in Section 5 that our conclusions
generalise to standard architectures & image datasets. In the interest of space & readability, we
focus on PrOViding intuition in this SeCtion, and fill in remaining details/proofs in the appendix.
1We will sometimes drop the student S sub/superscript where obvious for clarity, like in Eq. (2). Any
teacher specific object, e.g. kT , will always have corresponding T sub/superscript.
4
Published as a conference paper at ICLR 2022
Multi-view data. We consider the data classification problem introduced by Allen-Zhu & Li
(2020), with C classes and inputs x with P patches each of dimension d, meaning x ∈ (Rd)P .
For each class c, we suppose that there exist two attributes vc,1, vc,2 ∈ Rd. For x belonging to class
c, the attributes found in patches of x will include vc,1 and vc,2, as well as a random selection of
out-of-class attributes {vc0,l}c06=c,l∈[2].2 This denotes the multi-view nature of the data distribution.
In the true data-generating distribution D, We suppose that a proportion μ of the data (x, y) is Single-
view, which means that only one of vc,1 or vc,2 is present in x when (x, y) is from class c. These
Will be the data for Which standard training fails to generalise. A precise definition of multi-vieW
data is presented in App. B.1.1. Allen-Zhu & Li (2020) argue that this multi-vieW setting provides a
compelling proxy for standard image datasets such as CIFAR-10/100 Krizhevsky (2009).3
Intuition: FKD on multi-view data
Suppose we have an image classification task, with cat & car just two out of many classes. For
the car class, Vc,ι could correspond to headlights, whilst vc,2 could correspond to wheels. We
would then expect Vc,ι to also appear in patches of an input image, Xcat, corresponding to a cat
with headlight-like eyes. Allen-Zhu & Li (2020) show that a single trained model is biased to
learn exactly one of Vc,ι or vc,2, depending on its parameter initialisation. W.L.O.G, suppose that
the student is biased to learn vc2 & not vc,ι. If the teacher model has learnt vc,ι, this means that
the teacher model knows there is a similarity between XCat & any car image, Xcar, that displays
headlights. Mathematically, we show that this corresponds to a large value for kτ(xcat, XCar).
Our FKD regularisation forces the student to also have a large value for ks(xcat, XCar), ensuring
that attribute Vc,ι is also learnt by the student network. Without distillation, a student NN which
has learnt vc2 & not Vc,ι will not generalise to front-on images of cars that hide wheels.
Convolutional NN & corresponding feature kernel. Like Allen-Zhu & Li (2020), for our theo-
retical analyis we consider a single hidden-layer convolutional NN (CNN) with sum-pooling.4 For
each class c ∈ [C], we suppose that the CNN has m channels, giving Cm channels in total. For
channel r and class c, we suppose that we have weights θc,r ∈ Rd. This gives output for class c by
mP
fc(X) d=efXXR^eLU(hθc,r,Xpi)	(4)
r=1 p=1
where for ease of analysis ReLU is ReLU-like but with continuous gradient, see App. B.2.
Before we consider FKD, we must first define the feature kernel for this CNN f. To do so, we recast
f(x)=W ∙h(x, θ), where h(x, θ)∈RCm and W ∈RC×Cm satisfying, for r ∈ [m],c ∈ [C],c0 ∈ [C]:
P
h(X, θ)r+(c-1)m =f X ^LU( hθc,r, Xpi), and Wc0 ,r + (c-1_)m =f l{c = c0}.	(5)
p=1
Given that the feature kernel is k(X, X0) d=ef hh(X, θ), h(X0, θ)i, we now have that:5
Cm P
k(X,X0)= XX X ReLU(hθc,r, Xpi) ∙ ReLU(hθc,r, Xpo i).	(6)
c=1 r=1 p,p0=1
We first recall that standard training of the model f with gradient descent and cross entropy loss
fails to generalise on half the μ proportion of data that is single-view.
2It is straightforward to extend to the case of more than two views per class if need be.
3https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-
learning-ensemble-knowledge-distillation-and-self-distillation/
4It is straightforward to extend our analysis for max-pooling.
5The feature kernel defined in Eq. (6) corresponds to the Global Average Pooling CNN-GP kernel in Novak
et al. (2018) in the infinite-channel limit, which captures intra-patch correlations unlike the vectorised CNN-GP,
which corresponds to vectorising the spatial dimensions to give CmP rather than Cm channels.
5
Published as a conference paper at ICLR 2022
Theorem 1 (Standard training fails, Theorem 1 of Allen-Zhu & Li (2020)). For sufficiently many
classes C and channels m ∈ [polylog(C),C], with learning rate η ≤ .。/⑺,training time
T* = poly(C), and multi-view data distribution (App. B.1.1), the trained model f(T*)satisfies with
probability at least 1 一 e-Q(log2(C)):
•	Training accuracy is perfect: Forall (x, y) ∈ DD, y = argmax。fCT )(x).
•	Test accuracy is bad but consistent: P(χ,y)~D [y = argmaXc fCT )(x)] ∈ [0.49μ, 0.51μ].
Now we are ready to show that regularising the student model towards the teacher model’s feature
kernel, as in FKD, improves test accuracy. We suppose our teacher model is an ensemble of E ≥ 1
models {fe}eE=1, each with corresponding feature kernel ke, trained as standard on the same data
with independent initialisations θ0e. We average ke over e ∈ [E] to obtain our teacher feature kernel:
1E
kτ(x, x0) = Elke(x, x0).	⑺
This is akin to concatenating all features in {he}eE=1 into a ECm-dimensional feature vector, albeit
without the additional computational baggage. We then have our main theoretical result:
Theorem 2 (FKD improves student generalisation and is better with larger ensemble). Given an ar-
bitrary > 0. For any ensemble size E of teacher NNs trained as in Theorem 1 and sufficiently many
classes C,for m = polylog(C), with learning rate η ≤ po],。), and training time T * = polζ(C), the
ensemble teacher knowledge can be distilled into a single student model f(T*)using only teacher
feature kernel kτ, Eq. (7), such that with probability at least 1 — e-Q(log2(C)):
•	Training accuracy is perfect: Forall (x, y) ∈ DD, y = argmaxc fcτ )(x).
•	Test accuracy is good: P(χ,y)~D [y = argmaxc fcT )(x)] ≤ (^+ + e)μ.
Proof outline for Theorem 2, we first show, in Lemma 1, that a single trained NN’s feature kernel
(which we defined in Eq. (6)) can detect if two inputs share a data attribute that the NN has learnt
due to its weight initialisation. We extend this result to an ensemble teacher in App. C.2, showing
that the ensemble teacher feature kernel detects the union of all data attributes learnt by individual
{ke}eE=1. This simplifies our calculations when showing that our distillation regulariser, Eq. (3), is
effective for improved student generalisation. The full proof can be found in App. C.
Intuition: FKD with ensemble of teachers
To parse Theorem 2, suppose We have a single teacher i.e. E =1. Then, the test error is essentially
0.25μ in Theorem 2. The explanation is that both the student & teacher networks independently
learn one of {vc,ι}p=1 for each class c. Either vanilla KD (Theorem 4 of Allen-Zhu & Li (2020))
or our feature kernel approach allow the student to learn the union of the independent attributes
learnt by the student and the teacher, so for only a quarter of the single-view test data Xs will
the student not have learnt the useful class attribute present in xs. For general ensemble size
E, the story is the same: the student & E teachers each independently learn one of the two
useful attributes {vc,1}1∈2 for all C ∈ [C]. Distilling allows the student to learn the union of these
attributes, which means that the student will fail on only 2⅛r of the single-view data.
4	FKD in practice
Next, we highlight practical considerations for implementing FKD derived from our theory. Pseudo-
code and PyTorch-style code for our FKD implementation are given in Algs. 1 and 2 respectively.
6
Published as a conference paper at ICLR 2022
Correlation kernel. We propose D(ρθ, ρT ) as a regulariser in FKD instead of D(kθ, kT ), where:
Pz(x, x0) def	'z(x, X )	=, and PT(x, χ0) def ɪ Xt Pe(x, x0)
∙√kz(x, x)kz(x0, x0)	E e=1
defines feature correlation kernel Pz, corresponding to feature kernel kz, for z∈[E]∪{θ}.
The reason we use correlation kernels is that they normalise data, so that P(x, x)=1 ∀x,
which zeros diagonal differences in Eq. (3): we hope FKD allows the student to learn
from the teacher features shared between different inputs. Non-zero diagonal differences,
like in Similarity-Preserving (SP) KD (Tung & Mori, 2019), encourage the student to learn
noise as we show in App. D.1, and we hypothesise that this contributes to the improved
performance we observe of FKD over SP in Section 5. Moreover, this normalisation
helps balance individual teacher’s influence in an ensemble teacher, and ensures that FKD
does not need a temperature hyperparameter τ , which produces soft labels in vanilla KD.
Feature regularisation. One downside to using the cor-
relation kernel is that our FKD regularisation, D(Pθ, PT),
becomes invariant to the scale of kθ. For example, re-
placing kθ(x, x0) with MM(x)M(x0)kθ(x, x0), for any
M(x)ιRd→R+, leaves the student correlation kernel Un-
changed. This may lead to degeneracies when training θ,
& large variations in k(x, x) over x may harm generalisa-
tion (as evidenced by the fact that input normalisation is com-
mon across ML, from linear models to NNs). Moreover, our
proof of Theorem 2 is not quantitative, in that we only show
kθ(x, x0) = Θ(1) in the number of classes C up to poly-
logarithmic factor in C, when x6=x0 share a data attribute
vc,l that has been learned by parameters θ. These insights
motivate us to regularise the student feature h during FKD
Normalised k{xi x} value
Figure 3: Histogram of normalised fea-
ture kernel values,——k(X,x) ,、, over
, maxx0 k(x0 ,x0) ,
the CIFAR-100 test set.
training to control the norm of k(x, x) across inputs x. We use an additive `2 regularisation
B1 PB=I Ilh(xb, θ)k2 = B PB=I k(xb, Xb) with regularisation strength Xfr>0, for each minibatch
{(xb, yb)}bB=1. Fig. 3 shows that using Feature Regularisation (FR) encourages a more even spread
of k(x, x) across inputs for a student VGG8 network trained with a VGG13 teacher model on
CIFAR-100. Corresponding plots for other architectures can be found in Fig. 7. Similar to Dauphin
& Cubuk (2021), we find that FR improves generalisation & provide ablations in Section 5.
Algorithm 1 Feature Kernel Distillation with SGD.
Require: Maximum number of iterations T*, batch size B, learning rate η, teacher correlation kernel PT,
FKD regularisation λKD > 0, Feature regularisation λFR > 0. Initialise student parameters θ0 , W0 .
for iteration t = 0,...,T * do
Sample minibatch (XB, yB)B=ι i吟 DD.
Compute loss L = B1 PB=I L(f(xB),yB)+BB-J Pgj (pθt (XB, XB)-pτ(XB, XB))2.
Add feature regularisation L = L + λB^ PB=IIlh(XB, θt)∣∣2, (optional).
Update parameters θt+ι — θt — ηVθL, Wt+ι — Wt — ηVwL.
end for
return {θτ*, WT* }
5	Experiments
Due to space concerns, App. F contains further experiments & any missing experimental details.
Ensemble distillation. We first verify that larger ensemble teacher size, E, further im-
proves FKD student performance as suggested by Theorem 2. This is confirmed in
Fig. 4, using VGG8 for all student & teacher networks on the CIFAR-100 dataset.
7
Published as a conference paper at ICLR 2022
We also plot the test accuracy of the ensemble teacher across
sizes E, whose predictive probabilities are averaged over indi-
vidual teachers, as well as the test accuracy of an undistilled stu-
dent model. We see that FKD consistently outperforms vanilla
KD, and both distillation methods outperform the teacher in the
‘self-distillation’ setting of E=1 (Furlanello et al., 2018; Zhang
et al., 2019). Moreover, FKD allows a single student to match
teacher performance when E=2, before positive but diminish-
ing returns with larger E relative to the teacher ensemble.
Figure 4: FKD as teacher ensemble
size changes. Error bars denote 95%
confidence for mean of 10 runs.
Dataset Transfer. We next
show FKD can transfer knowl-
edge across similar datasets.
From a fixed VGG13 teacher
network trained on CIFAR-
100, we distil to student
VGG8 NNs on CIFAR-10,
Table 1: Test accuracy (%) of FKD & baselines in a dataset transfer dis-
tillation setting. Error bars indicate 95% confidence for mean of 10 runs.
Dataset
Student RKD
SP
FKD w.o. FR FKD
C-100 →	C-10	91.56	91.74±0.09	92.21±0.07	92.33±0.07	92.61±0.07
C-100 →	STL-10	72.69	72.86±0.27	73.88±0.17	75.17±0.30	75.44±0.31
C-100 →	Tiny-I	48.53	48.74±0.17	48.73±0.14	48.85±0.09	50.67±0.12
STL-10 & Tiny-ImageNet. As no student dataset has 100 classes, unlike CIFAR-100, it is not
clear how one can use vanilla KD (Hinton et al., 2015) in this case. We thus compare FKD to other
feature kernel based KD methods: Relational KD (RKD) (Park et al., 2019) & Similarity-Preserving
(SP) KD (Tung & Mori, 2019). In Table 1, we see that FKD without feature regularisation outper-
forms both baselines across all datasets, and that feature regularisation (FR) further improves FKD
performance, highlighting the benefit of our practical considerations in Section 4. The improved
performance is particularly stark on STL-10 (which we downsize to 32x32 resolution), where FKD
improves student performance by 2.75%. STL-10 is well suited for FKD as it has only 5K labeled in-
puts but 100K unlabeled datapoints, which can be used in our feature kernel regulariser, D(ρθ, ρT ).
Table 2: CIFAR-100 and ImageNet-1K accuracies (%) comparing FKD with KD baselines. * denotes result
from Tian et al. (2020); FKD uses the same teacher checkpoints provided by the authors,6 with error bars
denoting 95% confidence for the mean over 5 students.
CIFAR-100	ImageNet-IK
Teacher Student	ResNet32x4 ResNet8x4	VGG13 VGG8	ResNet32x4 ShuffleNetV1	ResNet50 VGG8	ResNet34 ResNet18
Teacher*	79.42	74.64	79.42	79.34	73.26
Student*	72.50	70.36	70.50	70.36	69.97
KD* (Hinton et al., 2015)	73.33±0.22	72.98±o.i7	74.07±0.17	73.81±0.11	70.66
RKD* (Park et al., 2019)	71.90±0.10	71.48±0.04	72.28±0.34	71.50±0.06	N/A
SP* (Tung & Mori, 2019)	72.94±0.20	72.68±0.17	73.48±0.37	73.34±0.30	70.62
CRD* (Tian et al., 2020)	75.51±0.16	73.94±0.19	75.11±0.28	74.30±0.12	71.17
FKD w.o. FR	74.89±0.24	73.08±O.16	74.66±0.23	73.99±0.15	70.84
FKD	75.57±0.22	73.78±o.17	75.00±0.30	74.61±0.28	71.23
Comparison on CIFAR-100 and ImageNet. Finally, we compare FKD to various knowledge
distillation baselines on CIFAR-100 and ImageNet, across a selection of teacher/student architec-
tures. We see in Table 2 that FKD consistently outperforms: vanilla KD (Hinton et al., 2015), RKD
(Park et al., 2019), and SP (Tung & Mori, 2019). Moreover, FKD either matches or outperforms
the high-performing Contrastive Representational Distillation (Tian et al., 2020). We use the exact
same teacher checkpoints used by Tian et al. (2020) and Chen et al. (2021) for CIFAR-100 and Im-
ageNet respectively to ensure fair comparison. We find, like in Table 1, that feature regularisation
consistently improves FKD performance and that even without feature regularisation, FKD outper-
forms all feature kernel based KD methods. This implies that using the correlation kernel to zero
out diagonal differences, as described in Section 4, indeed helps improve student performance.
6Apart from ImageNet-1K which used the pretrained ResNet34 from torchvision, like Chen et al. (2021)
8
Published as a conference paper at ICLR 2022
6	Related work
NN Knowledge Distillation. Following Hinton et al. (2015), there has been much interest in ex-
panding KD in NNs (Romero et al., 2014; Zagoruyko & Komodakis, 2016; Passalis & Tefas, 2018;
Zhang et al., 2018; Yu et al., 2019; Chen et al., 2020a; Tian et al., 2020). Most similar to FKD are
Park et al. (2019); Tung & Mori (2019) who also use relations between inputs to distil knowledge
(albeit not from the feature kernel learning perspective and without our theoretical justification), as
well as Qian et al. (2020) who focus on reducing computational costs of full-batch kernel matrix
operations. App. D highlights in more detail the differences of FKD compared to previous pairwise
feature kernel based KD methods. Allen-Zhu & Li (2020) made the first theoretical connection us-
ing the mechanisms of ensembling in NNs to explain the success of vanilla KD in NNs, which we
extend for feature kernel based KD.
Ensembling NNs. Ensembling NNs has long been studied for improving predictive accuracy
(Hansen & Salamon, 1990; Krogh et al., 1995) with particular recent focus towards uncertainty
quantification & Bayesian inference (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Zaidi
et al., 2020; Pearce et al., 2020; He et al., 2020; Wilson & Izmailov, 2020; Wenzel et al., 2020;
D’Angelo & Fortuin, 2021; Schut et al., 2021) and predictive diversity (Fort et al., 2019; D’Amour
et al., 2020). On the topic of Bayesian inference, the feature kernel has also been studied under the
name of Neural Linear Model (Riquelme et al., 2018; Ober & Rasmussen, 2019), and extensions
treating features h as inputs to standard Gaussian Process kernels are known by the name of Deep
Kernel Learning (Wilson et al., 2016; Ober et al., 2021; van Amersfoort et al., 2021).
NN Feature learning. A recent flurry of work has focussed on characterising & understanding the
importance of feature learning in NNs (Chizat et al., 2019; Fort et al., 2020; Baratin et al., 2021;
Lee et al., 2020; Aitchison, 2020; Ghorbani et al., 2020), fuelled in part by the development that
wide NNs become (Neural Tangent) Kernel machines in certain regimes (Jacot et al., 2018; Lee
et al., 2019; Yang & Littwin, 2021), thus forgoing feature learning. The consensus in these works is
that there are gaps between NTK theory & practical NNs that cannot be explained without feature
learning. However, Yang & Hu (2020) proved that feature-learning is still possible with infinite-
width NNs, and also that feature learning is equivalent to feature kernel learning in infinite-width
NNs. This motivates our study of the feature kernel as a key object for distillation. Regularising the
feature kernel to the true target covariance kernel was suggested by Yoo et al. (2021).
7	Conclusion
We have theoretically shown that the feature kernel is a valid object for Knowledge Distillation (KD)
in Neural Networks (NNs) by extending the analysis of Allen-Zhu & Li (2020), which focused on
vanilla KD (Hinton et al., 2015). Further, we used our theoretical insights to motivate practical con-
siderations when using feature kernels for distillation, such as using the feature correlation kernel
& using feature regularisation, to improve on previous feature based KD methods. We term our ap-
proach Feature Kernel Distillation (FKD), and note that FKD is more widely applicable than vanilla
KD, as it benefits from being agnostic to teacher and student prediction spaces. Experimentally, we
have demonstrated that FKD is amenable to ensemble distillation as suggested by our theory, is able
to transfer knowledge across similar datasets and that FKD outperforms vanilla KD & previous fea-
ture kernel based KD methods across a variety of architectures on CIFAR-100, and ImageNet-1K.
Limitations & future work. Though feature learning is central to our results, we stress that there
are still gaps between our theory & practice to understanding NN ensembling & KD, demonstrated
by the divergence between ensemble teacher & FKD in Fig. 4 for larger ensemble size. This could be
due to: the multi-view data setting not being able to capture the full complexity of real-world data,
the role of hierarchical feature learning between layers in a deep NN, or the importance of mini-
batching in stochastic gradient descent. Other future work could apply the multi-view data setting
to analyse uncertainty quantification in NN ensembles, assess the impact of different FKD regulari-
sation metrics in Eq. (3), or improve FKD further to compete with state-of-the-art KD methods.
9
Published as a conference paper at ICLR 2022
Acknowledgements We thank Emilien Dupont, Yee Whye Teh, and Sheheryar Zaidi for helpful
feedback on this work.
References
Laurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. In
International Conference on Machine Learning, pp. 156-164. PMLR, 2020.
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.
Aristide Baratin, Thomas George, Cesar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vin-
cent, and Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 2269-2277. PMLR, 2021.
Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
Cristian Bucilua, Rich Caruana, and Alexandru NicUIeScU-MiziL Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 535-541, 2006.
Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online knowledge distillation
with diverse peers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 3430-3437, 2020a.
Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, and Chun Chen. Cross-
layer distillation with semantic calibration. In Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 7028-7036, 2021.
Shuxiao Chen, Hangfeng He, and Weijie Su. Label-aware neural tangent kernel: Toward better gen-
eralization and local elasticity. Advances in Neural Information Processing Systems, 33, 2020b.
LenaIC Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
Advances in Neural Information Processing Systems, 32:2937-2947, 2019.
Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beu-
tel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Under-
specification presents challenges for credibility in modern machine learning. arXiv preprint
arXiv:2011.03395, 2020.
Francesco D’Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. arXiv preprint
arXiv:2106.11642, 2021.
Yann Dauphin and Ekin Dogus Cubuk. Deconstructing the regularization of batchnorm. In Interna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=d- XzF81Wg1.
Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multi-
ple classifier systems, pp. 1-15. Springer, 2000.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape per-
spective. arXiv preprint arXiv:1912.02757, 2019.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. Advances in Neural Information
Processing Systems, 33, 2020.
10
Published as a conference paper at ICLR 2022
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Bom again neural networks. In International Conference on Machine Learning, pp. 1607-1616.
PMLR, 2018.
Yan Gao, Titouan Parcollet, and Nicholas D. Lane. Distilling knowledge from ensembles of acoustic
models for joint ctc-attention end-to-end speech recognition. volume abs/2005.09310, 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.
Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern
analysis and machine intelligence, 12(10):993-1001, 1990.
Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the neu-
ral tangent kernel. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 1010-1022. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
0b1ec366924b26fc98fa7b71a9c249cf- Paper.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and gen-
eralization in neural networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 8580-8589, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Anders Krogh, Jesper Vedelsby, et al. Neural network ensembles, cross validation, and active learn-
ing. Advances in neural information processing systems, 7:231-238, 1995.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. Advances in Neural Information Processing
Systems, 30, 2017.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep Neural Networks as Gaussian Processes. In International Conference
on Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572-8583, 2019.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances
in Neural Information Processing Systems, 33, 2020.
Wesley Maddox, Shuai Tang, Pablo Moreno, Andrew Gordon Wilson, and Andreas Damianou. Fast
adaptation with linearized neural networks. In International Conference on Artificial Intelligence
and Statistics, pp. 2737-2745. PMLR, 2021.
11
Published as a conference paper at ICLR 2022
Alexander G de G Matthews, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Sample-then-
optimize posterior sampling for bayesian linear models. 2017.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference on
Learning Representations, volume 4, 2018.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2018.
Sebastian W Ober and Carl Edward Rasmussen. Benchmarking the neural linear model for regres-
sion. arXiv preprint arXiv:1912.08416, 2019.
Sebastian W Ober, Carl E Rasmussen, and Mark van der Wilk. The promises and pitfalls of deep
kernel learning. arXiv preprint arXiv:2102.12108, 2021.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems,
32:13991-14002, 2019.
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3967-3976,
2019.
Nikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge
transfer. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 268-284,
2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019a. URL http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-learning-library.pdf.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32,
2019b.
Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approxi-
mately bayesian ensembling. In International conference on artificial intelligence and statistics,
pp. 234-244. PMLR, 2020.
Qi Qian, Hao Li, and Juhua Hu. Efficient kernel transfer in knowledge distillation. arXiv preprint
arXiv:2009.14416, 2020.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Proceedings
of the 20th International Conference on Neural Information Processing Systems, pp. 1177-1184,
2007.
12
Published as a conference paper at ICLR 2022
Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch,
Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-
Lin Yeh, SzU-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Francois Grondin, William Aris,
Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio. SpeechBrain: A general-purpose
speech toolkit. 2021.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An
empirical comparison of bayesian deep networks for thompson sampling. arXiv preprint
arXiv:1802.09127, 2018.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Lisa Schut, Edward Hu, Greg Yang, and Yarin Gal. Deep Ensemble Uncertainty Fails as Network
Width Increases: Why, and How to Fix It. In Workshop on Uncertainty & Robustness in Deep
Learning, 2021.
Xu Tan, Yi Ren, Di He, Tao Qin, and Tie-Yan Liu. Multilingual neural machine translation with
knowledge distillation. In International Conference on Learning Representations, 2019.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=SkgpBJrtvS.
Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 1365-1374, 2019.
Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. On feature collapse
and deep kernel learning for single forward pass uncertainty. arXiv preprint arXiv:2102.11409,
2021.
Kees Van Den Doel, Uri M Ascher, and Eldad Haber. The lost honor of '2-based regularization. In
Large scale inverse problems, pp. 181-203. De Gruyter, 2013.
Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for
robustness and uncertainty quantification. arXiv preprint arXiv:2006.13570, 2020.
Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective
of generalization. arXiv preprint arXiv:2002.08791, 2020.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial intelligence and statistics, pp. 370-378. PMLR, 2016.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
Conference on Learning Theory, pp. 3635-3673. PMLR, 2020.
Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architec-
ture are Gaussian Processes. arXiv preprint arXiv:1910.12478, 2019.
Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020.
Greg Yang and Etai Littwin. Tensor programs iib: Architectural universality of neural tangent kernel
training dynamics. arXiv preprint arXiv:2105.03703, 2021.
13
Published as a conference paper at ICLR 2022
Boseon Yoo, Jiwoo Lee, Janghoon Ju, Seijun Chung, Soyeon Kim, and Jaesik Choi. Condi-
tional temporal neural processes with covariance loss. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning, volume 139 of Pro-
Ceedings of Machine Learning Research, pp. 12051-12061. PMLR, 18-24 JUl 2021. URL
http://proceedings.mlr.press/v139/yoo21b.html.
LU YU, Vacit OgUz Yazici, Xialei LiU, Joost van de Weijer, Yongmei Cheng, and ArnaU Ramisa.
Learning metrics from teachers: Compact networks for image embedding. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2907-2916, 2019.
Sergey ZagorUyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-
mance of convolUtional neUral networks via attention transfer. arXiv preprint arXiv:1612.03928,
2016.
Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris Holmes, Frank HUtter, and Yee Whye Teh. NeU-
ral ensemble search for Uncertainty estimation and dataset shift. arXiv preprint arXiv:2006.08573,
2020.
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be yoUr
own teacher: Improve the performance of convolUtional neUral networks via self distillation. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3713-3722,
2019.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and HUchUan LU. Deep mUtUal learning. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4320-4328,
2018.
14
Published as a conference paper at ICLR 2022
APPENDIX: FEATURE KERNEL DISTILLATION
A	Feature kernel dependence with cross-entropy loss
Suppose, we have n data points with fixed feature extractor h = h(X) ∈ Rn×p, trainable last layer
weights W ∈ Rp×C and targets Y ∈ Rn×C .
In Section 2, we described how with Mean Squared Error loss the trained NN is precisely kernel
ridge regression using the feature kernel, when there is `2 regularisation on the last layer weights.
Hence, the feature kernel exactly determines the predictions. We now prove the same for cross
entropy loss. That is to say, we wish to show that given the feature kernel k, the predictive probabil-
ities/logits at the optimal W * are independent of both W * and h.
We define the loss, for some regularisation λ > 0 (purely to enforce strong convexity) by
nλ
L(W) = ECE(yi, hiW) + 2 kWk2 ,
i=1
where hi ∈ R1×p is the ith row of h and
efy
CE(y,f ) = -log 干------f,	(8)
c∈[C] efc
is cross entropy loss.
Proposition 1. Letfeature extractor h(∙), training inputs X, training targets Y, '2 regularisαtion
λ > 0 all be fixed. Suppose we are given a test point x0 with features h0, then the test predic-
tion logits h0W* at optimal W* can be expressed solely in terms of feature kernel evaluations
k(∙, ∙)=hh(∙),h(∙)i∙
Proof. Ifwe differentiate L(W) and set to zero we get:
n
λWj*ι = X hi,j [1 {yi = l} - Pi,ι]	⑼
i=1
where p ∈ Rn×C is the result of applying softmax to each row of the logits h0W *.
Let hi denote the extracted features for training input xi. Now recall that hhi, hji = k(xi, xj ) is
the feature kernel evaluated at xi, xj. We multiply Eq. (9) by the test-data feature vector h0 to give:
n
λ(h0w*)1 = Xk(x*, Xi)[ι{yi = 1} -Pi,ι]	(10)
i=1
but h0W* are precisely the logits for test point x*, and likewise pi,l is the vector of probability
predictions at training point i. Hence, we could solve Eq. (10) numerically for logits/predictive
probabilities given only the feature kernel (without h(∙) or W*), and we see that the feature kernel
once again determines logit/prediction probabilities at the optimal last layer parameters like for
squared error loss, albeit this time implicitly for cross entropy loss.	□
15
Published as a conference paper at ICLR 2022
B Setup for training on Multi-view data
Before we prove Theorem 2, which demonstrates the generalisation benefits of FKD theoretically,
we need to recall the data and training setup of Allen-Zhu & Li (2020) for completeness and self-
containedness. Unless otherwise stated, everything in this section (App. B) is a simplified version of
the setup in Allen-Zhu & Li (2020). Our results hold in the more general version too, but we present
the setup in a simplified setting here for readability and convenience, without sacrificing the key
messages and intuitions: our focus is for our theoretical and practical contributions to guide each
other and align as much as possible, as opposed to e.g. maximising the generality of our theory.
B.1	Multi-view data distribution
Recall that we consider a C-class classification problem over P -patch inputs, where each patch has
dimension d, so our inputs are described by x = (x1 , . . . , xP) ∈ (Rd)P. For simplicity, we take
7~»	√,~v9 Fi	1 / C∙	1	1	∙ 1 ɪ ∙ 1	All Γ^rl C ɪ ∙ /ACAC'	z^l Px /ʌ ,
P = C2 and d = poly (C) for a large polynomial. Like Allen-ZhU & Li (2020), We use O, Θ, Ω to
hide polylogarithmic factors in the number of classes, C, which we take to be sufficiently large.
We assume that each class c ∈ [C] has exactly tWo attributes vc,1, vc,2 ∈ Rd, Which are orthonormal
for simplicity,7 such that:
V = {vc,1, vc,2}c∈[C]
is the set of all attributes.
B.1.1	Data generating mechanism
Let our data distribution for a data pair (x, y)〜 D be defined as D = μDs + (1 - μ)Dm, for multi-
view & single-view distributions Dm & Ds respectively. (x, y)〜D are generated as follows:
1.	Sample y ∈ [C] uniformly at random.
2.	Sample a set V0(x) of attributes uniformly at random from {vc0,1, vc0,2}c06=y each with
probability C and denote V(X) = V0(x) ∪ {vy,ι, vy2} as the set of attribute vectors used
in data x. We take s = C0.2 .
Intuition:
V0(x) correspond to the ambiguous attributes present in x, such as the cat whose eyes
look like car headlights. These V0 are crucial in our proofs as the FKD regularisation
between ambiguous images ensures that the student learns attributes it would have oth-
erwise missed.
For each v ∈ V(x), pick Cp (where Cp is a global constant) many disjoint patches in [P]
uniformly at random and denote this set as Pv (x). Denote P(x) = ∪v∈V(x)Pv (x): all
other patches p 6∈ P(x) will contain noise only.
4.	If x is single view, pick a value l = l(x)∈{1, 2} uniformly at random. l corresponds to the
attribute Vy ^ that is present in x, with Vy 3-^ missing.
5.	For each p ∈ Pv(x) for some v ∈ V(x):
xp = zpV +	αp,v0 V + ξp
v0∈V
7As d = poly(C) for a large polynomial, this isn’t too far-fetched an assumption.
16
Published as a conference paper at ICLR 2022
where ap,v0 ∈ [0, c‰] represents feature noise and ξp 〜N(0, σpId) is
independent
random noise, With σp=√dpoι⅛o.
The coefficients zp ≥ 0 satisfy:
(a)	If x is multi view,
• When v ∈ {vy,1, vy,2},
Pp∈Pv(x) zp ∈ [1, 2)
p∈Pv (x) zp4 = 1
(11)
Intuition:
These conditions ensure that both attributes for class y are equally likely to be
learnt, averaged over different random initialisations of parameters.
• When v ∈ V0(x),
Pp∈Pv(x) zp = 0.4
Pp∈Pv (x) zp4 = Θ(1)
(12)
(b)	If x is single view,
•	When	v=vy^,	Pp∈Pv (x) Zp	=	1
•	When	v=vy,3-f,	Pp∈Pv(x) Zp	=	C	0.2
•	When	v ∈ V0(x),	Pp∈Pv(x)zp	=	Γ
where γ = poy⅛
Intuition:
This is where the single view name comes from, as C-0.2《1 we see that Vy 3-^
is barely present in x.
6. For each p ∈ [P]\Pv (x):
αp,v0v0 + ξp
v0∈V
for feature noise ap” ∈ [0, C⅛] and ξp 〜N(0, CdId) is independent random noise.
Intuition:
One can think of the zero feature noise setting ap,vo = 0 ∀p, V for simplicity. But the
general formulation above renders the problem unlearnable by linear classifier, as the
maximum permissible feature noise across patches dominates the minimum possible
signal: CPK = C0.5》1
Remark It is possible to allow more relaxed assumptions, e.g. on Zp, as in Allen-Zhu & Li (2020).
Training data Recall we have D = μDs + (1 — μ)Dm, so that a proportion (1 一 μ) of the data is
multi-view. Our training data, D, is N independent samples from D. Letting D = Dm ∪ Ds denote
the split into multi and single view training data. We let μ = Poyj(C) and we suppose |N| = ∙Cμ- so
that each label C appears at least Ω(1) in D§.
17
Published as a conference paper at ICLR 2022
Figure 5: Comparison between ReLU & ReLU, for % = 0.2.
B.2	Smoothed ReLU CNN
Our theoretical analysis considers a single hidden layer CNN with sum-pooling, f, such that for
class c:
mP
fc(x)=XXR^eLU(hθc,r,xpi),	∀c∈ [C].
r=1 p=1
For a threshold % =「。巾篙。),We define the smoothed ReLU function as:
{0 if Z ≤ 0
4%43	if Z ∈ [0,%]
Z ― 3 % if Z ≥ %
0
which has continuous monotonic gradient denoted as ReLU .
B.3	Hyperparameter values in setup
Hyperparameter values used in the theoretical setup are given in Table 3.
B.4	Standard training
We define our empirical loss by:
L = Nn X Lf(Xi), yi)
i∈[N]
where L is the cross entropy loss defined in Eq. (8). We randomly initialise parameters
θ0,r i-.N (0,σ2) for σ2 = C .
Standard training in the theoretical analysis comprises of full-batch gradient descent on the empirical
loss L with learning rate η ≤「。品)and for T = poly(c) iterations.
C Proof of Theorem 2
We restate Theorem 2, where recall E denotes the teacher ensemble size and C is the number of
classes:
18
Published as a conference paper at ICLR 2022
Table 3: Hyperparameter values used in our theoretical analysis, corresponding to the setup of Allen-Zhu &
Li (2020). (*) denotes undefined in our presentation, but appearing in Allen-Zhu & Li (2020), because the
hyperparameter takes only one value in this work. We note that m is restricted to be polylog(C) in the setting
of Theorem 2.
Hyperparameter	Description	Value(s)
N	Training set size	c1∙2 μ
μ	Proportion of single-view data	1 poly(C)
d	Input patch dimension	poly(C)
P	Number of patches per input	C2
m	Number of channels per class	[polylog(C), C]
s	Out-of-class attribute sparsity	C0.2
σ0	Parameter initialisation standard deviation (std)	C-0.5
	Input patch additive noise std	1
σp		Vdpolylog(C)
Γ	Out-of-class attribute strength in single-view data.	1
		polylog(C)
%	h>r: . “一 ReLU threshold	1
		Vdpolylog(C)
q(*)	ər; 「	.. ReLU mid-section exponent	4
ρ (*)	In-class weaker attribute strength in single-view data	C-0.2
Theorem 2 (FKD improves student generalisation and is better with larger ensemble). Given an ar-
bitrary > 0. For any ensemble size E of teacher NNs trained as in Theorem 1 and sufficiently many
classes C,for m = Polylog(C), with learning rate η ≤ 口。/。), and training time T * = polζ(C), the
ensemble teacher knowledge can be distilled into a single student model f (T*)using only teacher
feature kernel kτ, Eq. (7), such that with probability at least 1 一 e-Q(log2(C)):
•	Training accuracy is perfect: For all (x, y) ∈ DD, y = argmaxc fCT ) (x).
•	Test accuracy is good: P(χ,y)~D [y = argmaxc fCT )(x)] ≤ (^+ + e)μ.
Outline of proof of Theorem 2
1.	We first analyse the feature kernel k for a single trained model, in App. C.1.
2.	We then extend our results to an ensembled teacher model feature kernel in App. C.2.
3.	We next outline our kernel distillation training scheme and provide a key result concerning
how the student’s parameters become increasingly correlated with the teacher’s learnt views
in Apps. C.3 and C.4
4.	We combine all threads to prove the final result in App. C.5.
C.1 Feature kernel for a single trained model
To prove Theorem 2, we first calculate the feature kernel k* for a single trained model with trained
parameters θ* from initialisation θ0 . The point of this exercise is to show that the feature kernel
k* is able to detect whether two inputs x, x0 share a common attribute which is in the subset of
attributes that was learnt by the trained parameters θ*, due to random correlation with initialised
parameters θ0. This is formally shown in Lemma 1.
19
Published as a conference paper at ICLR 2022
Recall the definition of the feature kernel k for our CNN architecture:
Cm P
k(x, x0) = XX X
ReLU(hθc,r, Xpi) ∙ ReLU(hθc,r,
c=1 r=1 p,p0=1
Let us first make a few more useful definitions. For l = 1, 2, we have:
Definition 2.
ZcMx) def l{vc,ι ∈ V(x)}	X	zp.
p∈Pvc,l (x)
Intuition:
Zc,ι (x) is a scalar constant describing the strength, in x, of the presence of attribute vc,ι (where C
may either correspond to true class y or to a different class; the latter has the effect of producing
‘soft labels, when using vanilla knowledge distillation).
Definition 3.
Φc,l =f X [hθt,r,vc,li] + .
r∈[m]
Definition 4.
Υc,l =f X [hθΞ,r,Vc,li]+2 .
r∈[m]
Intuition:
Υc ι, Φc ι are both parameter-dependent, data-independent scalars that describe the amount that
attribute vc,ι has been 'learnt, by the network parameters. YWl appears in the feature kernel
whereas Φc l appears in the function predictions fc directly, for our particular architecture in
Eq.(4)(Allen-Zhu & Li, 2020).
Lemma 1. We have, for x, x0 〜Dm,
k*(x, x0) =	X	γc,lzc,l(x)zc,l3) ± O(SMlx,C ) + O(焉)
(c,l)∙.(c,3-l)∈M	pygl )
where
sM(x,x0) = |{(c, l) : vc,l ∈	V(x)∩V(x0)and(c,3-l)∈/ M}|
is the number of shared attributes of x and x0, that are also members of the set of attributes learnt
by the single trained model with parameters θW, and M is defined in Fact A.e below.
Proof. We have
Cm	P	P
k*(x, x0) = XX [X ^(h 町r, xpi) ∙ X ReLU(hθW,r, xpo i)].
c=1 r=1 p=1	p0=1
First, define the hidden-layer activations for class c and channel r to be:
Definition 5.
P
Ψc,r(x) d=efXR^eLU(hθcW,r,xpi).
p=1
20
Published as a conference paper at ICLR 2022
We know from Theorem C.2. of Allen-ZhU & Li (2020) that, for multi-view X ∈ Dm and some
c∈ [C]:
Fact A.a For every P ∈ Pvc,l (x) and for l = 1, 2, we have: (θ", Xpi = (θ", vc,"zp ± 0(σ°)
Fact A.b For every P ∈ P(x)∖(Pvc,ι (x) ∪ Pvc* (x)), we have |〈。", xp)| ≤ O(σ°)
Fact A.c For every P ∈ [P ]\P (x), we have ∣hθ*, xp)| ≤ O(岂)
Fact A.d For every r ∈ [m]∖M0, every l ∈ [2], it holds that (θ", VCl ≤ O(σo), where:
M0 def [r ∈ H ∃l ∈ [2] ： hθ0,r,vc,ιi ≥ (1 - θ(ɪʌ)) ∙ max[hθ0,r,Vc,ιi]+].
log(C)	r∈[m]
Note from Proposition B.1 of Allen-Zhu & Li (2020), that m0 d=ef |MC0| = O(log5(C))
with probability at least 1 - e-。(IOg5(C)).
Intuition:
M0 denotes the key channels in [m] which have ‘won the lottery, and are relevant for
class c, in that in the C → ∞ limit the predictions for f are the same as if one forgets
the other channels, as shown in Allen-Zhu & Li (2020).
Fact A.e For every P ∈Pvc,l (x) and r ∈ [m],if (c, 3 -1) ∈ M,thenwehave: |〈。鼠，xp)| ≤ O(σ0),
where:
M =f |(c,l) ∈ [C] X [2] max] [hθ0,r ,vc,li]+ ≥ (1+ log21(m) ) ∙ mxK% ,vc,3-li] +
Intuition:
M denotes the data attributes Vc,ι which are more likely to be learnt by the NN pa-
rameters (compared to their fellow class attributes Vc,3-ι) because of correlations of the
initial parameters with such attributes. So, Fact A.e is saying that if (c, 3 - l) ∈ M,
then the attribute (c, l) is not learnt at all during standard single model training.
Thus,
2
Ψc,r(x)= X l{Vc,l eV(X)}∙	X	ReLU(hθ1,r, Xpi)	(13)
ι=1	p∈Pvc,l (x)
+	X	R^(O(σo))	(14)
p∈P (x)\ Sl {Pvc,l (x)}
+ X	R^(O(√⅛)).	(15)
p∈[P ]\P(x)	VC
First, note that |Pvc0,l0 (X)| = Cp is constant ∀c0, l0. From Fact A.b, Eq. (14) can be easily seen
to be O(σ4s)=O(C-1.8) as σ0 = C1 and S = C0.2, and likewise Eq. (15) can be seen to be
O((√7⅛)4P) = O(C-2) as P = C* 2, by Fact A.c. We note here that summing these equations over
C
m and C will be bounded above by O(.。就⑹).
21
Published as a conference paper at ICLR 2022
Now, let’s consider Eq. (13). Let notation vc,1, vc,2 ∈ S denote either vc,1 or vc,2 ∈ S for some set
S, and vc,1, vc,2 ∈/ S denote neither vc,1 nor vc,2 ∈ S.
There are three cases to consider:
1.	If vc,1, vc,2 ∈/ V(x), then Eq. (13) is zero.
2.	Else if ∃l ∈ [2] such that vc,l ∈ V(x) and (c, 3 - l) ∈ M, then by Fact A.e, we have
Eq. (13) is O(σ4)=O(C-2).’
Intuition:
This setting is when only attribute (c,l) appears in x, but attribute (c, 3 - l) was domi-
nant at initialisation so (c,l) has not been learnt by θ*.
3. Else, we have that:
2
Eq. (13) = * X l{vc,ι ∈V(x)}∙	X	^(hθ>, xpi)∙
Putting this all together with Fact A.a, we see that
21
Ψc,r(X)=E l{(c,l) ∈ sm(x, x)}	E	ReLU(hθc,r,Vc,ιiZp + o(σo)) + O(ðɪ^).
=1
p∈Pvc,l (x)
Now, ifr ∈/ Mc0 (i.e. neuron r is not dominant at initialisation), and (c, 1), (c, 2)∈sM(x, x), the by
Fact A.d, We have that Ψc,r(X)=O(C-1∙8).
On the other hand, recall % =.35⑹ and m0 = O (log5 (C)). And also, recall that if z > %, then
， 、 一，、 _ . . . .... . . . ________________________________________________ _ . _ . 一
ReLU(z) = z + O(%). Moreover, by Claim C.11 of Allen-Zhu & Li (2020) we know that ∃r0, l0
such that hθ*,ro,vc,i'i = Ω(m).Hence, for any r ∈ M°°, we know that either:
1. Ψc,r(x) = P2=1 l{(c, l) ∈ SM(x, x)}(hθ", Vc,ιiZc,ι(x) ± O(%)) + O(Ck), or
2. Ψc,r (x) = l{(c, 1), (c, 2) ∈ SM(x, x)}O(%) + O(Ck).
Moreover, Ψc,r (x) = O(1) ∀r, by e.g. Lemma C.21 of Allen-Zhu & Li (2020). And so it can be
seen, for % small enough we have:
m
X Ψc,r(X)Ψc,r(X0)
r=1
2m
X l{(c, 3 - l) ∈M}zc,ι(x)zc,ι(χ0)[X [hθ:,r, Vc,li]+2 ± θ( polylog(C) )] + 0( C18 )
2	11
∑ l{(c, 3 - l) ∈M}Zc,ι(x)zc,ι(x0)[γ2ι ± o(polylog(C))] + o(C18)
22
Published as a conference paper at ICLR 2022
where We also use Fact A.e above, such that it is not possible for both (θ*,vc,ιi+ and
hθC,r,vc,3-1i+ to be large.8 Note also from e.g. the proof of Allen-Zhu & Li (2020) Theorem
1, that if (c, 3 - l) ∈ M, then maxr (θ",vc,ιi = Θ(1).
Thus, we see that the contribution to k from the m class-c channels/neurons is:
kc (x, x0) d=ef X Ψc,r(x)Ψc,r(x0)
r=1
(16)
(17)
if (c, 1), (c, 2) ∈ sM(x, x0)
else
Summing k over [C] completes the proof of the lemma.	□
We now use Lemma 1 to analyse k* and ρ*(x, x0) = / k*(X,X )	.
ʌ/k* (x,x)k* (x0,x0)
First we look at Υ". Recall that (θ", VCl = O⑴ for all C ∈ [C],l ∈ [2], r ∈ [m], and likewise
so is Mc0 .
More specifically,
•	If (c, l) ∈ M, then we have from the proof of Theorem 1 in Allen-Zhu & Li (2020) that
Pr hθC,r,Vc,ιi+ ≥ Ω(log(C)),andsomaxr(8",Vc,ιi+ = Θ(1).
•	If neither (c, 1) nor (c, 2) are in M, then Claim C.10 of Allen-Zhu & Li (2020) shows us
.1	. 1	.1	/八士	∖ -U	/八士	∖ -U	K ∕r∖
that both maxr (θ",Vc, 1 i+, maxr (θ", PCai+ are Θ(1).
Combining these facts, we have that:
~ , , ,
Yll =Θ(1)	if (c, 3 - l) ∈ M
and so from Lemma 1, we have:
k*(x, x0) =	X	YzZC,(x)Zc,l(x0) ± O(;Mlx,:C)) + O(C-0.8)
(c,l)∙,(c,3-l)∈M	PygIJ
=θ(1) x	zc,l(x)Zc,l(x0) ± O(pM⅛⅛)+O(C-0.8)
(c,l)<c,3-l)∈M	P'E' '
=θ(sM(x, x0))±。( Pm⅛⅛)+3.
(18)
(19)
(20)
Finally, we arrive at an expression for the correlation kernel ρ* of a trained single model by
ρ*(x, x0) = θ (PsMSsML) )(1+O( po⅛Cy))+。(焉)	QI)
where we define that sM(x) = sM(x, x) is the number of attributes in x that have been learnt by
the trained network θ*, and is s(1 ± o(1)) with high probability.
8Unless neither (c, 1) nor (c, 2) ∈ M for a given c, but that only occurs in o(C) classes, and does not
change the order of e.g. sM (x, x0) which is what we really care about.
23
Published as a conference paper at ICLR 2022
Intuition:
Compare ρ* in Eq. (21) to the ‘soft' probability labels pτ ∈ RC of Allen-ZhU & Li (2020) (Claim
F.4) with temperature T = 啥；。):
PT(X)
if Vc,ι or Vc,2 is in V(x)
else
where s(x) is the number of indices C ∈ [C] such that vc^ or Vc,2 is in V(x). Note, the setting
of Allen-Zhu & Li (2020) is with a large Ω(1) ensemble, so every attribute is learnt (akin to M
being empty for us). In the case that M = {} being empty, if X = x0 and they share at least
one feature, then from App. B.1.1 with high probability they will share exactly one feature, so
that sm(x, x0) = 1. Moreover, s(x) = sm(x)≈sm(x0), hence We see that Eq. (21) matches
roughly with P(x), but without the need for a temperature hyperparameter.
We see that vanilla KD learns new attributes by comparing a single data point X between classes,
and giving larger target labels to the classes where ambiguous attributes learnt by the teacher are
present in x. On the other hand, ρ* gives higher values to data pairs x, x0 that share attributes
that have been learnt by the trained model, and as we will see later this is how FKD learns new
attributes in the student.
C.2 Ensembled teacher
To summarise what we have done so far in App. C.1, we have seen in Eqs. (20) and (21) that it is
possible, for a single trained model θ*, to simplify both the feature kernel k*(x, x0) and correlation
kernel ρ*(x, x0) in terms of the number of shared attributes between x, x0 which are also learnt by
the trained model. The set of attributes learnt by the single trained model is captured by the set
M =卜c,	* I)	∈	[C]	X [2]	max1 [hθ0,r,vc,li]+ ≥	(1+ I	21、) ∙	max1 Kθ0,r,vc,3-li]+]
r∈[m]	,	log2 (m)	r∈[m]	,
where θ0 was the random parameter initialisation for θ*. From Fact A.e, we know that if
(c, 3-l)∈M, then the attribute vc,l has not been learnt by the network.
Consider now an ensemble of E = Θ(1) independently trained networks, {θ*}E=ι, with an averaged
feature kernel:
1E
kτ(X, XO) = EEke(X, XO).
e=1
Suppose {θe,0}eE=1 denotes the corresponding independent parameter initialisations. Then, for each
e ∈ [E], let us define:
Me =f {(c,l) ∈[C] X [2] rm⅛h 町0,vc,ιi]+ ≥(1+l⅛y) ∙ rm⅛h 町0,"τi]+
Note that these Me are completely independent sets due to the independent initialisations, and also
by Proposition B.2 of Allen-Zhu & Li (2020), we know that
Therefore,
P (c, 1) or (c, 2) ∈ Me ≥ 1 - o(1)	∀c ∈ [C], e ∈ [E].
C≥ |Me| ≥ C(1 - op(1))	∀e∈ [E].
24
Published as a conference paper at ICLR 2022
Moreover, Eq. (11) & Proposition B.2 of Allen-Zhu & Li (2020) tell us that each of the two attributes
vc,1, vc,2 are equally likely to be in Me (and so learnt in the multi-view setup), This means that:
E1
| ∩ MeI = 9e-i C(I-Op(I)).
e=1
Define MT = TeE=1 Me . From Eq. (19) and the definition of kT , we see that:
kτ(x, x0) = Θ(1) X l{(c, 3 - l) ∈ Mt}Zc,ι(χ)Zc,ι(χ0) ± O(SMlTI(X(C))) + O(C-0∙8)
(c,l)
(22)
=θ(SMT (x，x0)) ±。( SMly⅛Cy)+O(C "8)	(23)
where for the reader’s convenience, we redefine:
sMT (x, x0) = {(c, l) : vc,l ∈ V(x) ∩ V(x0) and (c, 3 - l) ∈/ MT}.
Intuition:
We see that only for those attributes (c,l) such that (c, 3 -1) ∈ MT does the ensembled teacher
kτ miss the fact that We should have a strong Θ(1) kernel value between x, x0. ThiS is When
∣V(x) ∩ V(x0)∣ = {vc,ι} is non-empty (or in other words, when SMT(x, x0)=∣V(x)∩V(x0)∣),
and hence there should be a large kernel value kτ(x, x0).
So we see that only |Mt∣ of the attributes are not learnt by the teacher, which is a fraction
IMC| = 2E (1 - o(1)) of all the attributes. These missed attributes are where the 2⅛1 test error
in Theorem 2 comes from (teacher ensemble of size E, and plus 1 for the attributes learnt from
the student,s initialisation too).
What’s more, we can decompose the teacher’s feature kernel kT = Pc kcT into contributions kcT
from each class c, like in Eq. (16). From Eq. (17), we see that the contribution to the teacher’s
feature kernel from class , for x 6= x0 ;
kcT (x, x0)
if (c, 1), (c, 2) ∈ SMT (x, x0)
else,
(24)
is able to decipher between whether or not x, x0 share an attribute from class c for all attributes apart
from those (c, l) such that (c, 3 - l) ∈ MT.
C.3 Training scheme for FKD
We note at this point that we are morally done in terms of proving Theorem 2, with Eqs. (23)
and (24), our key results telling us that the (ensemble) teacher kernel kT can identify when two in-
puts share common attributes that have been learnt by the (ensemble) teacher, and more specifically
that kcT can do so when said common attribute is from class c.
What remains is a repackaging of the proof techniques of Allen-Zhu & Li (2020) (particularly for
their Theorem 4 regarding self-distillation), that knowledge distillation (this time only using feature
kernels instead of temperature-scaled logits, and with explicit dependence on teacher ensemble size)
can improve generalisation performance of a student.
25
Published as a conference paper at ICLR 2022
For convenience, the theoretical analysis of Allen-Zhu & Li (2020) introduces some slight discrep-
ancies between the actual practical weight updates of vanilla KD Hinton et al. (2015), i.e. the
gradients of:
L = L + λ ɪ X L( f(x) ,2)
N	ττ
and the weight updates in their theoretical exposition. Namely,
1.	The authors assume that a temperature-dependent threshold caps the logits to give soft
labels:
emin{τ 2fc(x),1}∕τ
pc(X) = Pj∈[c] emin{τ2fj(x),1}/T .
2.	The authors truncate the negative part of the gradient of the KD regularisation to only
encourage logits to increase not decrease, with weight updates for θc,r on input x:
-∆θt,r =f θt,r - θt+1 (X Vθc,r L + ηNN X (pτ (X)- pT,τ (X))-Vθc,r fc(x)
i
where pτ,τ are the temperature-scaled teacher labels.
3.	The authors scale the output of both student and teacher models by a (polylogarithmic)
factor, in order to ensure that both reach the threshold to give soft labels in Item 1. above.
4.	Self-distillation (Furlanello et al., 2018; Zhang et al., 2019) distils a single teacher and
a single student of same architecture into the student, like an ensemble of size 2 (stu-
dent+teacher). Allen-Zhu & Li (2020) modify the training scheme for their theoretical
analysis of self-distillation so that the student is first trained on its own in order separate
learning its own attributes/features from those of the teacher. Our analysis covers a similar
scheme.
These modifications are justified in that they make the theoretical analysis more convenient, whilst
illustrating the main mechanisms by which KD works, which is to share ‘dark knowledge’ that is
held in the teacher (in the form of the multi-view attributes that have been acquired by the teacher
due to its parameter initialisation), with the student.
In the same vein, we now introduce some modifications to the practical implementation of FKD we
propose in Alg. 1 to aid our theoretical analysis, and describe the main mechanisms by which FKD
works, corroborating our initial analyses in Section 2 and App. A about how the feature kernel is a
crucial object in any NN and captures all the ‘dark knowledge’ that a teacher network could possess
in the multi-view data setting.
It is likely possible to extend our proof of Theorem 2 with different modifications/training schemes,
but given that the focus of this work is to introduce FKD as a principled alternative to vanilla KD
with certain advantages such as prediction-space independence, and that the multi-view setting we
consider is a plausible simplification of real world data (as demonstrated in Allen-Zhu & Li (2020)),
we leave this to future work. We stress that any simplifications to the update rule in Alg. 1 for this
section can be efficiently computed, only requiring access to pairwise evaluations of the student and
teacher feature kernels, if need be.
Modified training regime for FKD
1.	We first suppose that the student is trained as standard (as in App. B.4) for T1
Poly(C)
η
steps, and learns its own subset of attributes MS , dependent on its initialisation θs0 , before
being trained with the FKD objective:
26
Published as a conference paper at ICLR 2022
Intuition:
This mirrors the self-distillation setup of Allen-ZhU & Li (2020) Theorem 4. The idea
being that the student first learns MS before picking UP the other attributes that the
teacher has access to.
2.	For a given feature kernel k, we threshold the feature kernel k based on value, to define a
modification k such that
k(x, χ0) = F	if k(x,XO) ≥ m12
(x, x ) = 0	else
Intuition:
This condition delineates between the setting where x, x0 share common attributes
learnt by student parameters θτ1 in the initial phase of training (i.e. delineates between
whether SMs (x, x0) nonempty or empty).
To see this: note that if vc^ ∈ V(x) ∩ V(x0) and (c, 3 — l) / MS then We know from
Allen-Zhu & Li (2020) that φT ≥ Ω(log(C)).
Hencemaxr (θ" ,vc,ιi ≥ Ω(log-4 (C)) as the number of active neurons mo = |M0| =
O (log3 * 5 C), and so it,s easy to see that for large enough m We have k(x, x0) ≥mvia
Lemma 1.
On the other hand, if {vc,ι} = V(x) ∩ V(x0) and (c, 3 一 l)/Ms then from Lemma 1
we know that k(x, x0) = O(C-0.8)《m12.
3. Similar to Allen-Zhu & Li (2020), we also truncate our FKD regularisation to only en-
courage kernel values to increase, and not decrease. For any input pair x1, x2, we have
parameter update:
-∆θc,r (X1, X2) H (⅛c(xi, X2) — k (X1, X2)) [ E Ψc,r (Xj Xec,r Ψ c,r (X3-j )]
j∈{1,2}
(25)
where recall
def P	P	0
Ψc,r(x) = EReLU(hθc,r, Xpi) so that RkΨc,r(x) = EReLU (〈。纣,Xpi)Xp
p=1	p=1
and
kc(X,X0) d=ef X Ψc,r (X)Ψc,r (X0)
r=1
were defined in Definition 5 and Eq. (16).
Intuition:
If the loss was
(kc(X1, X2) - kT(xi, X2))2
then the gradient with respect to θcr would be:
(kc (xl, x2) - kT (x1, x2))[ E ψc,r (Xj )Vθc,r ψc,r (x3-j )]
j∈{1,2}
so the only differences with Eq. (25) are truncating (⅛c(x1, X2) — kT(xι, X2))	and
also the thresholding to obtain k.
27
Published as a conference paper at ICLR 2022
To summarise, after training the student on its own for T1 steps (such that we are in the setting of
Theorem 1) to reach parameters θT1, We update for T2 = POly(C) steps as (hiding S subscript):
∆θt,r= -ηEχ1,χ2y 2 [(⅛c(x1, x2)-kT (X1, x2力-E ψc,r (Xj Xec,r ψc,r (X3-j )]	(26)
j∈{1,2}
C.4 Feature correlation growths
We noW seek to analyse to What extent the attributes {vc,l}c,l are learnt during our T2 FKD training
steps. The central objects describing hoW much vc,l has been learnt by parameters θ are:
Φtc,l d=ef X [hθct,r,vc,li]+	and	Φtc d=ef X Φtc,l
r∈[m]	l∈[2]
as Well as Ψc,r(X) as defined above.
Intuition:
Φc,ι is a data-independent quantity that reflects the strength of correlation with feature vc∣ by
parameters θ. On the other hand, Ψcr (x) is a data-dependent quantity that reflects the activation
of channel r for class C with input x.
Also recall that
Zc,l(x) =f l{vc,l ∈ V(x)}	X	Zp
p∈Pvc,l (x)
and define Vc,r,l(X) (which is convenient for calculating the size of gradient updates for θc,r):
Definition 6.
Vc,r,ι(x) = l{vc,ι ∈ V(x)}	X	ReLU (@产,Xpi)Zp
p∈Pvc,l (x)
Like how Lemma 1 simplified the feature kernel in terms of data-dependent Zc,l (X) and data-
independent Υc,l, we have a result from Allen-Zhu & Li (2020) to simplify function predictions
fc in terms of Zc,l (X) and Φc,l:
Claim 1 (Claim F.7 from Allen-Zhu & Li (2020)). For every t ≤ T1 + T2, every c∈	[C], every
(x, y) ∈ DD (or every test sample (x, y) 〜 D with probability 1 一 e-Q(log2 (C)))：
fC(χ)=牙 φC,ι
X Zc,l(x)) ± O(polylog(C))
We also have the following facts from Allen-Zhu & Li (2020) regarding the correlation of gradient
Vθc,rΨC,r(x) with Vc,ι for (x, y) ∈ DD,l ∈ [2] and r ∈ [m]:
. . _ . . ... . .. .. ... „ „ , 、 ^
Claim 2 (c.f. Claim F.6 of Allen-Zhu & Li (2020)). For every t ≤ Ti + T2, for every (x, y) ∈ DD,
every c ∈ [C], r ∈ [m] and l ∈ [2]:
•	If vc,1,vc,2 ∈ V(x), then (Vθc,r，,(x),vc,ιi ≥ (VC,r,ι(x) - O(σpP))
•	hVθc,r ΨC,r ,Vc,l i ≤ (1 {(vc,l ∈ V (x)}匕,r,l (x) + O(C-2))
•	For every i = c, |h—Vec,rΨC,r(x),v∕i∣ ≤ O(C-1.5)
28
Published as a conference paper at ICLR 2022
Disclaimer Technically, Allen-Zhu & Li (2020) only show Claims 1 and 2 for t ≤ T1 and one
would need to use similar proof techniques (such as their inductive hypothesis F.1) to show the case
for T1 ≤ t ≤ T2 , which we skip for conciseness.
We now study the growth of the student’s Φc,l, for those (c, l) which have been learnt by the teacher
but not the student:
Lemma 2 (Correlation Growth for attributes learnt by teacher). For every c ∈ [C], l ∈ [2], T2 ≥
t ≥ Ti, such that (c, 3 — l) ∈ MT, suppose ΦC ? ≤	, then we have:
Φtc+l1 ≥ ΦC,l + Ω( %) ∙ ΦC,14 ∙ ^0(ΦC,l)
Proof. For any c ∈ [C ], r ∈ [m], l ∈ [2], we have from Claim 2‘
θθe,r ,vc,li=ηEx1,x2 〜D2 网(X1, x2) -加(X1, x2))+[ X ψc,r (Xj ) (VC,r,l (x3-j )-0(σpP ))]
j∈{1,2}
Note that as μ ≤ Poly(C), We can suppose that both xi, X2 are multi-view data. Using Claim 2, We
have that
a\r ,vc,li≥ ηEχ1,χ2 〜D2 R (X1, x2) -加(X1, x2))+[ X ψc,r (Xj )(VC,r,l (x3-j )-O(σpP ))]
j∈{i,2}
Let r = argmaXrθ∈[m]{hθC,rθ,vc,ιi}, such that definitelyh0".,Vc,ιi≥Ω(ΦC,ι) because m =
polylog(C ).
But if X is multi-vieW and vc,ι ∈ V(X) such that Pp∈P	zp4 = Θ(1), and also by Fact A.a We
have that:
00
匕,r,ι(X) ≥ Ω(1) ∙ ReLU"的「您讣 ≥ Ω(ReLU 画,川
Ψc,r(X) ≥ X ^(hθC,r心Zp- O(σo)) ≥ Ω(ΦC,ι4)
p∈Pvc,l
NoW, We have assumed that (c, 3 - l) ∈/ MT, such that the teacher model has learnt attribute (c, l)
and satisfies kT(x, x0) = 1 when vc,ι ∈ V(x) ∩ V(x0) (for large enough polylogarithmic m).
Also, it is simple to see that When vc,ι ∈ V(X) ∩ V(X0) & vc,3-ι ∈/ V(X) ∩ V(X0), for large enough
m, that ΦCι ≤ 2m implies that kc(X, x0) ≤ m^ by Lemma 1, and so kc(X, x0) = 0, i.e. the student
has not (yet) learnt vc,ι .
So we see there are two more conditions that must be satisfied in order for
(⅛T(Xi, X2)—kc(Xi, X2))+=1 > 0:
1.	vc,ι ∈ V(xi) ∩ V(X2) so that kT(xi, X2) = 1
2.	vc,3-ι ∈/ V(Xi) ∩ V(X2) so that kc(Xi, X2) = 0.
Going back to App. B.1.1, we know that these conditions occur with probability C (1 — o(1)) for
independently sampled Xi, X2 〜力.Finally, putting everything together we have that:
hθt,r1,Vc,li+ -hθtc,r ,Vc,li+ ≥ Ω( % )Φc,l4 ∙ ^0(Φc,l)
29
Published as a conference paper at ICLR 2022
summing over r0 ∈ [m] and noting(丛。力,vc,ι)≥ 0 ∀r0, UP to small error (as σpP = poy(cj for
a large polynomial), gives us our result.
□
Lemma 2 immediately gives us the following corollaries, because ΦT1 ≥ ΩΩ(σq) from Allen-Zhu &
0
Li (2020) Induction HyPothesis F.1.g and ReLU is increasing.
2	5.1
Corollary 1. Define iteration threshold T2 = Θ( ηSC^7) = Θ( C-), then for every (c,l) such that
(c, 3 - l) ∈/ MT we have:
φTll+T2 ≥
Cl — 4m
But likewise, we can also bound the growth of Φc,l
Lemma 3. If (c, 3 — V) ∈ MS\Mt, once ΦC ι ≥ JEgIm(C), it no longer gets updated (for large
C):	,
2
Proof. ReCanDefinitionS 3 and4that 箕,ι= Pr∈[m] [hθt,r, vc,li]+ and Yc,ι= Pr∈[m] [hθt,r ,vc,li] +
Hence by Cauchy-Schwarz we have:
Φtc,l2 ≤ mΥtc,l =⇒ Υtc,l ≥
loglog(C) ≥	2
m2	— 0.42m2
for large enough C. Thus, ifx1, x2 are both multi-view and vc,l ∈ V(x1) ∩ V(x2), by Lemma 1 we
must have
kc(xι, x2) ≥ m2
It’s also not difficult to check that any other Possible setting for x1, x2, and V(x1) ∩ V(x2) will
lead to (kT(xι, x2) 一 ⅛c(xι, x2))+ = 0, and hence
艮(xi, X2) — kc(xι, X2))+ = 0	∀xi, X2
□
C.5 Wrapping up proof of Theorem 2
Proof. We are now ready to wraP uP our Proof. Recall from the Proof of Theorem 1 in Allen-Zhu &
Li (2020), that after the initial Phase of T1 stePs of student training on its own:
ΦT1 ≥ Ω(log(C)) ∀c ∈ [C],
and more sPecifically:
•	If (c, 3 — l) ∈/ MS , then
ΦT1ι ≥ Ω(log(C))
This gives us Perfect test accuracy on the multi-view data, and 50% accuracy on the single-
view data, so 0.5μ test accuracy overall without distillation, as per Theorem 1.
30
Published as a conference paper at ICLR 2022
•	Moreover, we have that if (c, 3 - l) ∈/ MT and (c, 3 - l) ∈ MS, then by Corollary 1 and
Lemma 3:

loglog(C)
m
1
≥	-
4m
1
Mogiog(C) ≥ ΦT1+T2
≥	-
4m
We see that this change in ΦT1ι+τ2 after FKD training is much smaller than Ω(log(C)) and
so the student after FKD training still has perfect multi-view accuracy, as well as correct
predictions on any single-view data that possess the attributes learnt in the initial phase of
training.
On the other hand, for single-view data, we know that if we have data point x, y and
attribute vc,ι ∈ V(x), SUCh that C = y then Pp∈Pv°,jχ) Zp = γ = O(poy⅛pj)，as
defined in App. B.1.1.
Hence for small enough Γ (《^^) we have that if (c, 3 -1) / MT and the single view data
x is of class C with l(x) = l then we have correct prediction， as per Claim 1.
Combining these means that we have correct prediction for any single-view data x of class C， and
l(x) = l such that (C, 3 - l) ∈/ MT ∩ Ms.
By the independence of these sets we have that |MT ∩ Ms | = (2-E)k(1 - o(1)) this means we
have test error less than (2-ET + e)μ for any e > 0，for large enough C as required.
D Differences between FKD & other feature kernel based KD
METHODS
In this section， we highlight how our FKD approach overcomes some of the shortcomings of previ-
ous feature kernel based KD methods which only use pairwise evaluations of the feature kernel: SP
(Tung & Mori， 2019) and RKD (Park et al.， 2019). One advantage of FKD relative to these previous
works is that we have shown FKD is amenable to ensemble distillation. Moreover， it goes without
saying that Feature Regularisation， which arises naturally thanks to our feature kernel learning per-
spective in Section 4， is already a significant departure that improves FKD relative to SP & RKD.
However， even without FR we observe in Section 5 that FKD outperforms both RKD & SP across
different datasets and architectures， which warrants explanation.
D.1 Importance of zero diagonal differences: SP (Tung & Mori, 2019)
First， we consider diagonal kernel differences， kS (x, x) - kT (x, x) for fixed x， and motivate using
zero diagonal differences， which is not present in SP (Tung & Mori， 2019) but is in FKD thanks to
our use of the correlation kernel. Fig. 6 displays this comparison between FKD & SP graphically.
Intuition: Downside of non-zero diagonal differences
The key intuition, which We detail below using our theoretical setup, is that non-zero diagonal
differences k(x, x) 一 kτ(x, x) = 0 encourage the student to learn noise in input x, compared
to when We have zero diagonal differences k(x, x) 一 kτ(x, x) = 0. In the latter case, We only
havenon-zero differences for k(x, x0) — kτ(x, x0) where X = x0.
31
Published as a conference paper at ICLR 2022
Figure 6: Comparison of (normalised) squared differences in kx,x0 = k(x, x0) between student S & teacher
T, across a minibatch of size 64 of CIFAR-100 training data, for SP (left) and FKD (right). We see that whereas
FKD has zero diagonal differences, SP is largely dominated by non-zero diagonal differences. Note there is a
slight abuse of notation here, in that we plot squared differences in normalised kernels, so that FKD uses the
correlation kernel and SP uses row-normalisation (Tung & Mori, 2019).
Diagonal updates Consider our parameter update Eq. (25) when X1 = X2 = X:
If we didn,t have zero diagonal differences and instead kc(x, x) - kT(x, x) = -1, then:
P0
∆θc,r (x, x) = 2ηΨc,r (x)	ReLU (hθc,r, xpi)xp,
p=1
Now suppose v = vc,1 ∈ V(x). For each p ∈ Pv(x), recall (from App. B.1.1) that:
xp = zp v + ξp ,
where we assume zero feature noise for simplicity.
We then see that (c.f. Claim C.13 of Allen-Zhu & Li (2020)):
0
〜
I * ʌ	Z	X	< X	~	/	∖ -W	Z 、 —	__，，/	、、
h∆θc,r (x, x), ξpi = 2Θ(η)Ψc,r (X)ReLU (hθc,r , Xpi) +
as hv,ξpi = O(√) with high probability.
But at the same time (by e.g. Claim F.6 of Allen-Zhu & Li (2020)):
h∆θc,r(X, X), vi = 2ηΨc,r(X)Vc,r,1(X)(1 ± o(1))
and note that that Vc,r,1(X) ≤ Θ(1) by definition. Moreover, in order for the student network to
learn attribute vc,1, then eventually it must satisfy maxro hθc,ro ,v)= O(1) from Allen-ZhU & Li
0
(2020). Thus, for % small enough, if r = argmaxr0 hθc,r0, vi, we have ReLU (hθc,r, Xpi) = 1.
Non-diagonal updates On the other hand, if X1 6= X2 and we have v ∈ V(X1) ∩ V(x2):
P0
∆θc,r (X1 , X2) = η Ψc,r (Xj ) ReLU (hθc,r , X3-j,p i)X3-j,p,
j=1,2
p=1
and so if x1 = x and ξp denotes the random noise in x1,p , then:
h∆θc,r(X1, X2),ξpi = Θ(η)Ψc,r(X2)R^U'(〈。纣,X1,pi) + O(^),
d
32
Published as a conference paper at ICLR 2022
as hx2,p, ξpi = O(√1d). But this time We have
h∆θc,r (Xi, X2 ),vi = η(Ψc,r (x1 ) VC,r, 1(X2 ) + Ψc,r (*2)匕,r,l(xi ))(1 土 θ(1)),
We see that Whereas in diagonal updates ∆θc,r(x, x) the increments for h∆θc,r(x, x), ξpi and
h∆θc,r(x, x), vi are 1:1, for non-diagonal updates ∆θc,r(x1, x2) they are 1:2 respectively.
Thus, the parameter updates for θc,r With non-zero diagonal differences in feature kernels are more
likely to learn noise, ξp , compared to our zero diagonal updates Which rely only on non-diagonal
∆θc,r(x1, x2) for x1 6= x2. This is Why We zero out diagonal differences for FKD, using the feature
correlation matrix in practice.
D.2 Problem of homogeneous NNs in RKD (Park et al., 2019)
The distance-wise version of RKD (Park et al., 2019) is as folloWs: for x, x0, We calculate
ψT (x, x0) = khT (x, θT ) - hT (x0, θT )k2, Where recall hT is the last-layer teacher feature ex-
tractor. LikeWise, We also calculate ψS (x, x0) = khS (x, θS) - hS(x0, θS)k2. The RKD loss adds
λKDEx,x0 [(ψT (x, x0) - ψS (x, x0))2] to the student’s training loss.9 While RKD (Park et al., 2019)
does ensure zero diagonal differences, i.e. that ψT (x, x) - ψS (x, x) = 0, it suffers from a related
issue, due to the homogeneity of NNs that use ReLU nonlinearity, Which is ubiquitous in image
classification tasks.
Suppose We take x and define x0 = Mx for some M > 0. For example, think of taking a cat image
and multiplying all the pixel values by M . For ReLU (C)NNs Without bias parameters, We have
that h(x, θ) is 1-homogeneous: h(x0, θ) = M h(x, θ). This means that it is likely (depending on
the norms of the features hS and hT ) that We Will have ψT (x, Mx) - ψS (x, Mx) 6= 0. But a cat
image multiplied by some scalar M is still a cat image, hence RKD runs into the same problems as
in App. D.1 of learning noise in x.
On the other hand for FKD: correlation kernel ρS (x, M x)=ρT (x, M x)=1, hence
ρS (x, M x)-ρT (x, M x)=0 ∀x ∈ Rd, M > 0.
E	PyTorch-style pseudocode for FKD
In Alg. 2, We provide PyTorch-style Paszke et al. (2019b) pseudocode for the distillation and feature
regularisation losses in FKD. We note that FKD only requires pairWise computations of feature
(correlations) kernels. This alleviates the need for matrix multiplication/inversion operations With
batch-by-batch size matrices, Which is beneficial for scalability.
F Experimental details and further results
F.1 Fig. 2: predictive disagreement across independent initialisations vs
RETRAINED LAST LAYER
All models are ResNet20v1 trained With standard hyperparameters:
•	160 epochs training time With batch size 128 and learning rate 0.1 Which is decayed by a
factor of 10 after epochs 80 and 120.
•	SGD optimiser With momentum 0.9 and Weight decay of 0.0001.
9We do not consider the angle-Wise RKD loss here, but there Will be similar issues due to homogeneity.
33
Published as a conference paper at ICLR 2022
Algorithm 2 PyTorch-style pseudocode for Feature Kernel Distillation (FKD).
# B:
# L_FKD:
# L_FR:
# D_s:
# D_t:
# f_s:
# f_t:
Batch size.
FKD regularisation strength.
Feature regularisation strength.
Student feature dimension.
Teacher feature dimension.
Student features B x D_s
Teacher features B x D_t
#	mm: matrix-matrix multiplication
#	Compute student feature correlation kernel matrix s_c
s_k = mm(f_s, f_s.T)	# B x B
s_k_diag_inv_sqrt = torch.diag(s_k).pow(-1/2)
s_k_diag_inv_sqrt = s_k_diag_inv_sqrt.reshape(-1, 1)	# B x 1
s_c = s_k_diag_inv_sqrt * s_k * s_k_diag_inv_sqrt.T	# B x B
#	Compute teacher feature correlation kernel matrix t_c
with torch.no_grad():
t_k = mm(f_t, f_t.T)	# B x B
t_k_diag_inv_sqrt = torch.diag(t_k).pow(-1/2)
t_k_diag_inv_sqrt = t_k_diag_inv_sqrt.reshape(-1, 1) # B x 1
t_c = t_k_diag_inv_sqrt * t_k * t_k_diag_inv_sqrt.T # B x B
distil_loss = ((t_c - s_c).pow(2)).mean()
feat_reg_loss = (f_s.pow(2)).mean()
#	FKD loss to be added to supervised loss
loss_fkd = L_FKD * distil_loss + L_FR * feat_reg_loss
•	CIFAR10 data is normalised in each channel such that the training data is zero mean and
unit standard deviation. Random crops and horizontal flips used as data augmentation.
•	All models are initialised with Kaiming initialisation He et al. (2015).
Out of 10000 test points, the predictive disagreements between a ‘reference’ model and either: in-
dependent initialisations (top row) or retrained last layers (bottom row) are depicted in Table 4.
We see two clear trends. First, the retrained last layer has much fewer disagreements with the ref-
erence model than an independent initialisation model, highlighting the importance of the feature
kernel. Secondly, the vast majority of disagreements between independent initialisations are where
one of the models is correct. This reinforces our intuition/theoretical analysis that ensembling NN
works because different initialisations bias the models to capture different useful features, and hence
ensemble distillation (via feature kernels) can improve student performance.
Table 4: Breakdown of predictive disagreements between reference and alternate models over 10000 CIFAR10
test points, in terms of which model (if any) was correct. Mean ± standard deviations over 3 independent
initialisations for top row, and over 3 independent reference models for bottom row. All models achieved
between 8.0%-8.5% test error.
Alternate model	Reference correct	Alternate correct	Neither correct	Total disagreement
Independent Init	350 ± 14.6	383 ± 15.9	124 ± 3.7	857 ± 29.8
Retrained LL	30 ± 2.9	35 ± 1.7	15±5.4	80 ± 4.1
F.2 Fig. 3: Additional feature kernel histograms
In Fig. 7, we provide additional plots to Fig. 3 that depict the difference in distribution (over x) of
feature kernel values k(x, x), for FKD with and without Feature Regularisation (FR).
34
Published as a conference paper at ICLR 2022
O
-一 suα,α LUe,!60tt一 H
Normalised 吊方㈤ value
of normalised
ResNet32x4 → ResNet8×4	ResNet50 → VGG8
0.0	0.2	0.4	0.6	0.8	1.0
Normalised k(x,x) value
FKD with FR
FKD w.o. FR
0.0	0.2	0.4	0.6	1.0
Normalised k(x,x) value
Figure 7:
values between FKD with & without Feature Regularisation
(FR), across different Teacher→Student architectures, on CIFAR-100 test set. We see, like in Fig. 3 that FR
encourages a more even distribution of k(x, x) across x, for all architectures.
Negative hypothesis. We originally hypothesised that FR could benefit FKD, in addition to bal-
ancing the distribution of k(x, x), as it could reduce the sparsity in the NN last-layer representa-
tion activations, which is consistent with the proofs of Theorems 1 and 2. Indeed, the NN predic-
tions are dominated by a select few neurons who ‘have won the lottery’ (Allen-Zhu & Li, 2020)
on account of being most correlated with one of the attributes {vc;l }l∈[2],c∈[C] at random initial-
isation. In our analysis, as few as O(log5 (m) out of m neurons could be inactive. From our
feature kernel learning perspective, this seems like a highly undesirable phenomenon, because if
k(x, x0) = hh(x; θ), h(x0; θ)i = PrC=m1 hr(x, θ)hr(x0, θ) only has useful contributions from a
dominant minority of r ∈ [Cm], then we are not utilising the full capacity of the model. FR seemed
appropriate to reduce this sparsity as `2 regularisation is known to promote non-sparse solutions
(Van Den Doel et al., 2013). However, we experimentally found the opposite to our hypothesis: that
FR trained FKD student had more inactive neurons relative to FKD students trained without FR.
This again highlights that, while (we believe) our results in this work provide compelling evidence
to highlight the validity of feature kernel based distillation, there are still gaps between our theory
and practice, and further questions to be answered in future work.
F.3 Fig. 4: Ensemble distillation
All individual VGG8 networks that made up the teacher ensemble were trained using the default
training regime from Tian et al. (2020), with independent parameter initialisations. Indeed, all of
our experiments in Section 5 used Tian et al. (2020)’s excellent open-source PyTorch codebase
(Paszke et al., 2019a).10
For student networks, we used the training regime for vanilla KD from Tian et al. (2020) for all
ensemble sizes. For FKD, we used the hyperparameters from our ResNet50→VGG8 experiment in
Table 2 for all ensemble sizes.
F.4 Table 1: Dataset Transfer
The VGG13 teacher checkpoint is provided by Tian et al. (2020). For both CIFAR-10 and STL-
10, all student networks are trained for 160 epochs with batch size 64 using SGD with momentum,
with learning rate decays at epochs 80, 120, 150. The student trained without KD used default
hyperparameters from Tian et al. (2020), which are indeed strong hyperparameters for standard
training. For FKD, RKD (Park et al., 2019) and SP (Tung & Mori, 2019), we tuned the learning
10https://github.com/HobbitLong/RepDistiller
35
Published as a conference paper at ICLR 2022
rate, learning rate decay, and KD regularisation strength λKD on a labeled validation set of size
5000 for CIFAR-10 and 1000 for STL-10, before retraining using best hyperparameters on the full
training(+unlabeled) dataset. We also tuned the FR regularisation strength, λFR for FKD when FR
was used. All RKD and SP hyperparameters were tuned in a large window around their default
values from Tian et al. (2020), which were all author recommended. For FKD, we allowed λKD to
range in [1,1000], and λFR to range in [0,20]. All hyperparameters sweeps were conducted using
Bayes search.
For STL-10, we used a batch size of 512 for all KD methods’ regularisation terms, compared to
64 for the standard cross-entropy loss. This was due to the fact that STL-10 has only 5K labeled
datapoints, and we wanted to ensure that the student used as much of the unlabeled data as possible
for each feature-kernel based KD method’s additional regularisation term during 160 epochs of
training. 512 batch size was the maximum power of 2 before we ran into memory issues on a 11GB
VRAM GPU, which occured for the RKD method.
Both CIFAR-10 and STL-10 data are normalised in each channel such that the training data is zero
mean and unit standard deviation. Random crops and horizontal flips used as data augmentation.
STL-10 images are downsized from 96x96 to 32x32 resolution.
F.5 Table 2: CIFAR- 1 00 and ImageNet comparison
CIFAR-100. All networks were trained for 240 epochs with batch size 64, with learning rate decay
at epochs 150, 180, 210 using SGD with momentum. All teacher networks use the exact same
checkpoints as provided by Tian et al. (2020). Learning rate, learning rate decay, λKD, and λFR
(when used) were tuned as in App. F.4 on a validation of size 5000. All other hyperparameters
were set to the default values used by Tian et al. (2020). The CIFAR-100 data is normalised in each
channel such that the training data is zero mean and unit standard deviation, with random crops and
horizontal flips used for data augmentation. All results provided denote the test set accuracy at the
end of the 240 epochs of training.
ImageNet. The ImageNet dataset (ILSVRC-2012) consists of about 1.3 million training images
and 50,000 validation images from 1,000 classes. Each training image is extracted as a randomly
sampled 224x224 crop or its horizontal flip without any padding operation. All teacher networks
use the exact same checkpoints as provided by Chen et al. (2021). The initial learning rate is 0.1 and
divided by 10 at 30 and 60 of the total 90 training epochs. We set the mini-batch size to 256 and the
weight decay to 10-4. λKD, and λFR (when used) were tuned as in App. F.4 on a validation of size
5000 except using Bayes search. All results are reported in a single trial. All other hyperparameters
were set to the default values used by Chen et al. (2021). All results provided denote the Top-1 test
accuracy (%). Accuracy of baselines were reported in Tian et al. (2020).
F.6 SENSITIVITY TO λKD
In Fig. 8, we plot the sensitivity of FKD to the strength of the distillation regularisation λKD in Eq. (2)
for the VGG13→VGG8 experiment on CIFAR-100. We see that a well tuned λKD(≈ 300 here) is
important for best student generalisation. Feature regularisation λFR = 20 in Fig. 8.
F.7 Table 5: Analyses in Neural Machine Translation
In this section, We performed analyses for a neural machine translation (NMT) task proposed by
Tan et al. (2019). In the analyses, we could only obtain data for En-De (from English to German)
translation since links to the datasets for other languages are broken. Therefore, we employed a
self-distillation method on a pre-trained English model for En-De translation as follows:
36
Published as a conference paper at ICLR 2022
VGG13→VGG8 on CIFAR-100
IO1	IO2	IO3	IO4
Λfkd Regularisation Strength
Figure 8: Comparison of normalised k(x, x) values between FKD with & without Feature Regularisation
(FR), across different Teacher→Student architectures, on CIFAR-100 test set. We see, like in Fig. 3 that FR
encourages a more even distribution of k(x, x) across x, for all architectures.
•	We train a single teacher transformer model on the IWSLT dataset for English (Tan et al.,
2019).
•	We perform self-distillation on the teacher model for En-De translation (Tan et al., 2019).
•	We did not search for optimal hyperparameters, and used default parameters of the code
provided by the authors of Tan et al. (2019). The results are given in Table 5.
Table 5:	BLEU of the teacher model of (Tan et al., 2019) (Teacher), self-distillation of (Tan et al., 2019) (SD),
SD with KD of (Hinton et al., 2015), SD with FKD, and SD with FKD loss obtained by replacing distillation
loss (2) of (Tan et al., 2019) with FKD, in En - De neural machine translation tasks.
Teacher (Tan et al., 2019)	SD (Tan et al., 2019)	SD with	KD SD with FKD	SD with FKD loss
27.32	27.49	27.51	27.64	27.79
We first note that, we adapted vanilla KD and our FKD for sequential data in the KD loss (2) of
(Tan et al., 2019) in this task. More precisely, we first computed vanilla KD and FKD on token
probabilities, and added these loss functions to the KD loss (eq 2 of (Tan et al., 2019)) in KD and
FKD. In the results, aggregating vanilla KD with the KD loss (eq 2 of (Tan et al., 2019)) improved
accuracy from 27.49 to 27.51. However, FKD further boosted BLEU to 27.64. We then replaced KD
loss (Eq. 2 of (Tan et al., 2019)) with FKD for training. Remarkably, FKD further boosted the BLEU
to 27.79. These results suggest that the proposed FKD can be applied in NMT tasks, successfully.
We hope that these results will motivate researchers to employ FKD in various different NLP tasks
including but not limited to multilingual NMT, named entity recognition and question answering.
F.8 Table 6: Analyses in Automatic Speech Recognition
In this section, we used a CRDNN model (VGG + LSTM,GRU,LiGRU+ DNN) on the TIMIT
dataset. In this experiment, we used a distillation approach proposed by Gao et al. (2020) for ASR
tasks as follows:
•	We train a single teacher model on the TIMIT dataset Ravanelli et al. (2021).
37
Published as a conference paper at ICLR 2022
•	We perform self-distillation on the teacher Gao et al. (2020).
•	We did not search for optimal hyperparameters, and used default parameters of the Speech-
Brain Library.
•	In this task, replacing CTC/NLL distillation losses with KD (Hinton et al., 2015) did
not converge. Additional investigation with hyperparameter search is needed. We used
phoneme error rate (PER) to measure accuracy of models.
Table 6:	Phoneme error rate (PER) of methods in automatic speech recognition tasks.
Teacher	Distilled Teacher (Gao et al., 2020) KD (Hinton et al., 2015) FKD
13.26	1280	1286	12.59
The results are given in Table 6. Similar to the NMT task, we adapted vanilla KD and our FKD
for sequential data as follows: We first computed vanilla KD and FKD loss functions on token
probabilities, and then added to the total loss (eq 7 of Gao et al. (2020)). In the analyses, Vanilla KD
(Hinton et al., 2015) increased the PER from 12.80 to 12.86. However, FKD further improved the
PER from 12.80 to 12.59. In this task, training models by replacing CTC/NLL distillation losses (eq
4 or 5 of Gao et al. (2020)) with KD (Hinton et al., 2015) and FKD did not converge. In conclusion,
these results propound that FKD can be applied for different tasks, i.e., image classification, NMT
and ASR, boosting accuracy of baseline distillation methods. We hope that these initial results will
motivate researchers in different communities (computer vision, NLP, and ASR) to further expound
and apply FKD in additional sub-tasks.
38