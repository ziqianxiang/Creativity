Published as a conference paper at ICLR 2022
Neural Network Approximation based on
Hausdorff distance of Tropical Zonotopes
Panagiotis Misiakos1", Georgios Smyrnis2, Georgios Retsinas3, Petros Maragos3
1ETH Zurich, 2University of Texas at Austin, 3National Technical University of Athens
panagiotis.misiakos@inf.ethz.ch, gsmyrnis@utexas.edu,
gretsinas@central.ntua.gr, maragos@cs.ntua.gr
Ab stract
In this work we theoretically contribute to neural network approximation by pro-
viding a novel tropical geometrical viewpoint to structured neural network com-
pression. In particular, we show that the approximation error between two neural
networks with ReLU activations and one hidden layer depends on the Hausdorff
distance of the tropical zonotopes of the networks. This theorem comes as a first
step towards a purely geometrical interpretation of neural network approxima-
tion. Based on this theoretical contribution, we propose geometrical methods that
employ the K-means algorithm to compress the fully connected parts of ReLU
activated deep neural networks. We analyze the error bounds of our algorithms
theoretically based on our approximation theorem and evaluate them empirically on
neural network compression. Our experiments follow a proof-of-concept strategy
and indicate that our geometrical tools achieve improved performance over relevant
tropical geometry techniques and can be competitive against non-tropical methods.
1	Introduction
Tropical geometry (Maclagan & Sturmfels, 2015) is a mathematical field based on algebraic geometry
and strongly linked to polyhedral and combinatorial geometry. It is built upon the tropical semiring
which originally refers to the min-plus semiring (Rmin, ∧, +), but may also refer to the max-plus
semiring (Cuninghame-Green, 2012; Butkovic, 2010). In our work, We follow the convention of the
max-plus semiring (Rmax, ∨, +) which replaces the classical operations of addition and multiplication
by max and sum respectively. These operations turn polynomials into piecewise linear functions
making them directly applicable in neural networks.
Tropical mathematics cover a wide range of applications including dynamical systems on weighted
lattices (Maragos, 2017), finite state transducers (Theodosis & Maragos, 2018; 2019) and convex
regression (Maragos & Theodosis, 2020; Tsilivis et al., 2021). Recently, there has been remarkable
theoretical impact of tropical geometry in the study of neural networks and machine learning (Maragos
et al., 2021). Zhang et al. (2018) prove the equivalence of ReLU activated neural networks with
tropical rational mappings. Furthermore, they use zonotopes to compute a bound on the number
of the network,s linear regions, which has already been known in (Montufar et al., 2014). In a
similar context, Charisopoulos & Maragos (2018) compute an upper bound to the number of linear
regions of convolutional and maxout layers and propose a randomized algorithm for linear region
counting. Other works employ tropical geometry to examine the training and further properties of
morphological perceptron (Charisopoulos & Maragos, 2017) and morphological neural networks
(Dimitriadis & Maragos, 2021).
Pruning or, generally, compressing neural networks gained interest in recent years due to the surprising
capability of reducing the size of a neural network without compromising performance (Blalock et al.,
2020). As tropical geometry explains the mathematical structure of neural networks, pruning may
also be viewed under the perspective of tropical geometry. Indeed, Alfarra et al. (2020) propose an
unstructured compression algorithm based on sparsifying the zonotope matrices of the network. Also,
,Conducted research as a student in National Technical University of Athens.
1
Published as a conference paper at ICLR 2022
Smyrnis et al. (2020) construct a novel tropical division algorithm that applies to neural network
minimization. A generalization of this applies to multiclass networks (Smyrnis & Maragos, 2020).
Contributions In our work, we contribute to structured neural network approximation from the
mathematical viewpoint of tropical geometry:
•	We establish a novel bound on the approximation error between two neural networks with
ReLU activations and one hidden layer. To prove this we bound the difference of the
networks’ tropical polynomials via the Hausdorff distance of their respective zonotopes.
•	We construct two geometrical neural network compression methods that are based on
zonotope reduction and employ K-means algorithm for clustering. Our algorithms apply on
the fully connected layers of ReLU activated neural networks.
•	Our algorithms are analyzed both theoretically and experimentally. The theoretical eval-
uation is based on the theoretical bound of neural network approximation error. On the
experimental part, we examine the performance of our algorithms on retaining the accuracy
of convolutional neural networks when applying compression on their fully connected layers.
2	Background on Tropical Geometry
We study tropical geometry from the viewpoint of the max-plus semiring (Rmax, ∨, +) which is
defined as the set Rmax = R ∪ {-∞} equipped with two operations (∨, +). Operation ∨ stands for
max and + stands for sum. In max-plus algebra we define polynomials in the following way.
Tropical polynomials A tropical polynomial f in d variables x = (x1, x2, ..., xd)T is defined as
the function
f (x) = max{aiT x + bi }	(1)
where [n] = {1, ..., n}, ai are vectors in Rd and bi is the corresponding monomial coefficient in
Rmax = R ∪ {-∞}. The set of such polynomials constitutes the semiring Rmax [x] of tropical
polynomials. Note that each term aiTx + bi corresponds to a hyperplane in Rd. We thus call the
vectors {ai}i∈[n] the slopes of the tropical polynomial, and {bi}i∈[n] the respective biases. We allow
slopes to be vectors with real coefficients rather than integer ones, as it is normally the case for
polynomials in regular algebra. These polynomials are also referred to as signomials (Duffin &
Peterson, 1973) in the literature.
Polytopes Polytopes have been studied extensively (Ziegler, 2012; Grunbaum, 2013) and occur as
a geometric tool for fields such as linear programming and optimization. They also have an important
role in the analysis of neural networks. For instance, Zhang et al. (2018); Charisopoulos & Maragos
(2018) show that linear regions of neural networks correspond to vertices of polytopes. Thus, the
counting of linear regions reduces to a combinatorial geometry problem. In what follows, we explore
this connection of tropical geometry with polytopes.
Consider the tropical polynomial defined in (1). The Newton polytope associated to f (x) is defined
as the convex hull of the slopes of the polynomial
Newt (f) := conv{ai : i ∈ [n]}
Furthermore, the extended Newton polytope of f(x) is defined as the convex hull of the slopes of the
polynomial extended in the last dimension by the corresponding bias coefficient.
ENewt (f) := conv{(aiT , bi) : i ∈ [n]}
The following proposition computes the extended Newton polytope that occurs when a tropical
operation is applied between two tropical polynomials. It will allow us to compute the polytope
representation corresponding to a neural network’s hidden layer.
Proposition 1. (Zhang et al., 2018; Charisopoulos & Maragos, 2018) Let f, g ∈ Rmax[x] be two
tropical polynomials . Then for the extended Newton polytopes it is true that
ENewt (f ∨ g) = conv{ENewt (f) ∪ ENewt (g)}
ENewt (f + g) = ENewt (f)㊉ ENewt (g)
2
Published as a conference paper at ICLR 2022
(2,1,1)
(0,0,1)
(0,0,0)
(2,1,2)
ENeWt (f)	ENeWt (g)	ENeWt (f ∨ g)	ENeWt (f + g)
Figure 1: Illustration of tropical operations between polynomials. The polytope of the max (∨) of f
and g corresponds to the convex hull of the union of points of the tWo polytopes and the polytope of
sum (+) corresponds to their MinkoWski sum.
Here ㊉ denotes MinkoWski addition. In particular, for two sets A, B ⊆ Rd it is defined as
A ㊉ B := {a + b | a ∈ A, b ∈ B}
Corollary 1. This result can be extended to any finite set of polynomials using induction.
Example 1. Let f, g be two tropical polynomials in 2 variables, such that
f(x, y) = max(2x+y+ 1, 0), g(x, y) = max(x, y, 1)
The tropical operations applied to these polynomials give
f ∨ g = max(2x + y + 1, 0, x, y, 1)
f + g = max(3x + y + 1,x, 2x + 2y + 1,y, 2x + y + 2, 1)
Fig. 1 illustrates the extended Newton polytopes of the original and the computed polynomials.
The extended NeWton polytope provides a geometrical representation of a tropical polynomial. In
addition, it may be used to compute the values that the polynomial attains, as Proposition 2 indicates.
Proposition 2. (Charisopoulos & Maragos, 2018) Let f ∈ Rmax [x] be a tropical polynomial in d
variables. Let UF (ENewt (f)) be the points in the upper envelope of ENewt (f), where upward
direction is taken with respect to the last dimension of Rd+1. Then for each i ∈ [n] there exists a
linear region of f on which f(x) = aiTx + bi if and only if (aiT, bi) is a vertex of UF (ENewt (f)).
Example 2. Using the polynomials from Example 1 we compute a reduced representation for f ∨ g.
f ∨ g = max(2x + y + 1, 0, x, y, 1) = max(2x + y + 1, x, y, 1)
Indeed, the significant terms correspond to the vertices of U F (ENewt (f ∨ g)) shown in Fig. 1.
2.1	Tropical Geometry of Neural Networks
Tropical geometry has the capability of expressing the mathematical structure of ReLU activated
neural netWorks. We revieW some of the basic properties of neural netWorks and introduce notation
that Will be used in our analysis. For this purpose, We consider the ReLU activated neural netWork of
Fig. 2 With one hidden layer.
Network tropical equations The netWork of Fig. 2 consists of an input layer x = (x1, ..., xd), a
hidden layer f = (f1, ..., fn) With ReLU activations, an output layer v = (v1, ..., vm) and tWo linear
layers defined by the matrices A, C respectively. As illustrated in Fig. 2 We have Ai,: = aiT , bi for
the first linear layer and Cj,: = (cj1, cj2, ..., cjn) for the second linear layer, as We ignore its biases.
Furthermore, the output of the i-th component of the hidden layer f is computed as
fi(x) = max X aikxk + bi, 0 = max(aiT x + bi, 0)	(2)
We deduce that each fi is a tropical polynomial With tWo terms. It therefore folloWs that ENeWt (fi)
is a linear segment in Rd+1. The components of the output layer may be computed as
n
vj(x) =	cjifi(x) =	|cji|fi(x) -	|cji|fi(x) =pj(x) - qj(x)	(3)
3
Published as a conference paper at ICLR 2022
Tropical rational functions In Eq. (3), functions pj , qj
are both linear combinations of {fi } with positive coef-
ficients, which implies that they are tropical polynomials.
We conclude that every output node vi can be written as
a difference of two tropical polynomials, which is defined
as a tropical rational function. This indicates that the
output layer of the neural network of Fig. 2 is equivalent
to a tropical rational mapping. In fact, this result holds
for deeper networks, in general, as demonstrated by the
following theorem.
Theorem 1. (Zhang et al., 2018) A ReLU activated deep
neural network F : Rd → Rm is equivalent to a tropical
rational mapping.
It is not known whether a tropical rational function r(x)
admits an efficient geometric representation that deter-
mines its values {r(x)} for x ∈ Rd, as it holds for tropi-
cal polynomials with their polytopes in Proposition 2. For
this reason, we choose to work separately on the polytopes
of the tropical polynomials pj , qj .
x1
x2
xk
xd
f2
.b
fi
vj
.b
fn
bn
Figure 2: Neural network with one hid-
den ReLU layer. The first linear layer has
weights {aiT} with bias {bi} correspond-
ing to i-th node ∀i ∈ [n] and the second
has weights {cji }, ∀j ∈ [m], i ∈ [n].
Zonotopes Zonotopes are defined as the Minkowski
sum of a finite set of line segments. They are a special
case of polytopes that occur as a building block for our
network. These geometrical structures provide a representation of the polynomials pj , qj in (3)
that further allows us to build our compression algorithms. We use the notation Pj , Qj for the
extended Newton polytopes of tropical polynomials pj , qj , respectively. Notice from (3) that for each
component vj of the output pj , qj are written as linear combinations of tropical polynomials that
correspond to linear segments. Thus Pj and Qj are zonotopes. We call Pj the positive zonotope,
corresponding to the positive polynomial pj and Qj the negative one.
Zonotope Generators Each neuron of the hidden layer represents geometrically a line segment
contributing to the positive or negative zonotope. We thus call these line segments generators of the
zonotope. The generators further receive the characterization positive or negative depending on the
zonotope they contribute to. It is intuitive to expect that a zonotope gets more complex as its number
of generators increases. In fact, each vertex of the zonotope can be computed as the sum of vertices of
the generators, where we choose a vertex from each generating line segment, either 0 or cji aiT , bi .
We summarize the above with the following extension of (Charisopoulos & Maragos, 2018).
Proposition 3. Pj, Qj are zonotopes in Rd+1. For each vertex v of Pj there exists a subset of indices
I+ of {1, 2, ..., n} with cji > 0, ∀i ∈ I+ such that v = i∈I cji aiT, bi . Similarly, a vertex u of
Qj can be written as u = i∈I cji aiT, bi where I- corresponds to cji < 0, ∀i ∈ I-.
3	Approximation of Tropical Polynomials
In this section we present our central theorem that bounds the error between the original and
approximate neural network, when both have the architecture of Fig. 2. To achieve this we need
to derive a bound for the error of approximating a simpler functional structure, namely the tropical
polynomials that represent the neural network. The motivation behind the geometrical bounding of
the error of the polynomials is Proposition 2. It indicates that a polynomial’s values are determined at
each point of the input space by the vertices of the upper envelope of its extended Newton polytope.
Therefore, it is expected that two tropical polynomials with approximately equal extended Newton
polytopes should attain similar values. In fact, this serves as the intuition for our theorem. The metric
we use to define the distance between extended Newton polytopes is the Hausdorff distance.
Hausdorff distance The distance of a point u ∈ Rd from the finite set V ⊂ Rd is denoted by either
dist (u, V) or dist (V, u) and computed as minv∈V ku - vk which is the Euclidean distance of u
4
Published as a conference paper at ICLR 2022
from its closest point v ∈ V. The Hausdorff distance H(V, U) of two finite point sets V,U ⊂ Rd is
defined as
H (V, U) = max max dist (v, U) , max dist (V, u)
v∈V	u∈U
Let P, P be two polytopes With their vertex sets denoted by VP, VP respectively. We define the
Hausdorff distance H (P' P) of the two polytopes as the Hausdorff distance of their respective vertex
sets VP, Vp. Namely,
H (p,P) ：= H (Vp, Vp)	(4)
Clearly, the Hausdorff distance is a metric of how close two polytopes are to each other. Indeed, it
becomes zero when the two polytopes coincide. According to this metric, the following bound on the
error of tropical polynomial approximation is derived.
Proposition 4. Let p,p ∈ Rmax [x] be two tropical polynomials and let P = ENeWt(P), P =
ENewt (P). Then,
max |p(x) - P(X) | ≤ P ∙ H(P, P)
where B = {x ∈ Rd : kXk ≤ r} is the hypersphere ofradius r, and P = √r2 + l.
The above proposition enables us to handle the more general case of neural networks with one hidden
layer, that are equivalent with tropical rational mappings. By repeatedly applying Proposition 4 to
each tropical polynomial corresponding to the networks, we get the following bound.
Theorem 1. Let v,v ∈ Rmax[x] be two neural networks with architecture as in Fig. 2. With Pj, Qj
we denote the positive and negative zonotopes of v. Thefollowing bound holds.
(m	∖
X H (Pj ,P) + H (Qj, Qj) J
Remark 1. The reason we choose to compute the error of the approximation on a bounded hyper-
sphere B is twofold. Firstly, the unbounded error of linear terms always diverges to infinity and,
secondly, in practice the working subspace of our dataset is usually bounded, e.g. images.
4	Neural Network Compression Algorithms
Compression problem formulation The tropical geometry theorem on the approximation of
neural networks enables us to derive compression algorithms for ReLU activated neural networks.
Suppose that we want to compress the neural network of Fig. 2 by reducing the number of neurons in
the hidden layer, from n to K. Let us assume that the output of the compressed network is the tropical
rational map V = (Vι,…，Vm). ItS j-th component may be written as Vj(x) = Pj(x) - j(x) where
using Proposition 3 the zonotopes of Pj, j are generated by 弓i(aT, b), ∀i. The generators need
to be chosen in such a way that Vj (x) ≈ Vj (x) for all X ∈ B. Due to Theorem 1 it suffices to find
generators such that the resulting zonotopes have H Pj , Pjj , H Qj , Qjj as small as possible ∀j .
We thus formulated neural network compression as a geometrical zonotope approximation problem.
Our approaches Approximating a zonotope with fewer generators is a problem known as zonotope
order reduction (Kopetzki et al., 2017). In our case we approach this problem by manipulating the
zonotope generators cji aiT, bi , ∀i, j 1. Each of the algorithms presented will create a subset of
altered generators that approximate the original zonotopes. Ideally, we require the approximation to
hold simultaneously for all positive and negative zonotopes of each output component Vj . However,
this is not always possible, as in the case of multiclass neural networks, and it necessarily leads to
heuristic manipulation. Our first attempt to tackle this problem applies the K-means algorithm to the
1Dealing with the full generated zonotope would lead to exponential computational overhead.
5
Published as a conference paper at ICLR 2022
(a) Original network. (b) Original zonotopes. (c) Resulting zonotopes.
Figure 3: Illustration of Zonotope K-means execution. The original zonotope P is generated by
ci aiT, bi for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7.
The approximation P of P is colored in purple and generated by Ci (af, b) , i = 1,2 where the
first generator is the K-means center representing the generators 1, 2 of P and the second is the
representative center of 3, 4. Similarly, the approximation Q of Q is colored in green and defined by
the generators cCi aCiT , Cbi , i = 3, 4 that stand as representative centers for {5, 6} and 7 respectively.
(d) Compressed network.
positive and negative generators, separately. This method is restricted on applying to single output
neural networks. Our second approach further develops this technique to multiclass neural networks.
Specifically, it utilizes K-means on the vectors associated with the neural paths passing from a node
in the hidden layer, as we define later. The algorithms we present refer to the neural network of Fig.
2 with one hidden layer, but we may repeatedly apply them to compress deeper networks.
4.1	Zonotope Approximation
Zonotope K-means The first compression approach uses K-means to compress each zonotope of
the network, and covers only the case of a single output neural network, e.g as in Fig. 2 but with
m = 1. The algorithm reduces the hidden layer size from n to K neurons. We use the notation
ci, i = 1, ..., n for weights of the second linear layer, connecting the hidden layer with the output
node. Algorithm 1 is presented below and a demonstration of its execution can be found in Fig. 3.
Algorithm 1: Zonotope K-means Compression
1.	Split generators into positive ci aiT , bi : ci > 0 and negative ci aiT , bi : ci < 0 .
2.
Apply K-means for 与 centers, separately for both sets of generators and receive
ncCi aCiT	,	Cbi	:	cCi	> 0o ,	ncCi	aCiT	,	Cbi	:	cCi	< 0o as output.
3.	Construct the final weights. For the first linear layer, the weights and the bias which
correspond to the i-th neuron become the vector cCi aCiT , Cbi .
4.	The weights of the second linear layer are set to 1 for every hidden layer neuron
where cCi aCiT , Cbi occurs from positive generators and -1, elsewhere.
Proposition 5. Zonotope K-means produces a compressed neural network with output vC satisfying
n
P ∙ max |v(x) — V(X)l ≤ K ∙ δmax +
I-Nmax
iT , bi k
i=1
where K is the total number of centers used in both K-means, δmax is the largest distance from a point
to its corresponding cluster center and Nmax is the maximum cardinality of a cluster.
The above proposition provides an upper bound between the original neural network and the one that
is approximated with Zonotope K-means. In particular, if we use K = n centers the bound of the
6
Published as a conference paper at ICLR 2022
approximation error becomes 0, because then δmax = 0 and Nmax = 1. Also, if K ≈ 0 the bound gets
a fixed value depending on the magnitude of the weights of the linear layers.
4.2	Multiple Zonotope Approximation
The exact positive and negative zonotope approximation performed by Zonotope K-means algorithm
has a main disadvantage: it can only be used in single output neural networks. Indeed, suppose
that we want to employ the preceeding algorithm to approach the zonotopes of each output in a
multiclass neural network. That would require 2m separate executions of K-means which are not
necessarily consistent. For instance, it is possible to have cj1i > 0 and cj2i < 0 for some output
components vj1 , vj2 . That means that in the compression procedure of vj1 , the i-th neuron belongs
to the positive generators set, while for vj2 , it belongs to the negative one. This makes the two
compressions incompatible. Moreover, the drawback of restricting to single output only allow us to
compress the final ReLU layer and not any preceeding ones.
Neural Path K-means To overcome this obstacle we apply a simultaneous approximation of the
zonotopes. The method is called Neural Path K-means and directly applies K-means to the vectors of
the weights aiT, bi, c1i, ..., cmi associated to each neuron i of the hidden layer. The name of the
algorithm emanates from the fact that the vector associated to each neuron consists of the weights of
all the neural network paths passing from this neuron. The procedure is presented in Algorithm 2.
Algorithm 2: Neural Path K-means Compression
1.	Apply K-means for K centers to the vectors aiT, bi, C:T,i , i = 1, ..., n, and get
the centers (af, bi, CTi), i = 1,…，K.
2.	Construct the final weights. For the first linear layer matrix the i - th row becomes
(aT, bi), while for the second linear layer matrix, the i - th column becomes CJ.,i.
Null Generators Neural Path K-means does not apply compression directly to each zonotope of
the network, but is rather a heuristic approach for this task. More precisely, if we focus on the set
of generators of the zonotopes of output j , Neural Path K-means might mix positive and negative
generators together in the same cluster. For instance, suppose (a£, bk, CTj is the cluster center
corresponding to vectors aiT , bi , C:T,i for i ∈ I. Then regarding output j, it is not necessary
that ∀i ∈ I all cji have the same sign. Thus, the compressed positive zonotope Pj might contain
generators of the original negative zonotope Qj and vice versa. We will call generators cji aiT , bi
contributing to opposite zonotopes, null generators.
Proposition 6. Neural Path K-means produces a compressed neural network with output v satisfying
P ∙ mB MX) - V(X)k1 ≤√mκδmax+ √m
√mδmax
Nmin
n
X g,ik∣∣(aT ,bi)W +
i=1
m
E(∣∣(aT ,biH∣ + g,ik) + EE∣T (aT, bi 并
i=1
j=1 i∈Nj
where K is the number of K-means clusters, δmax the maximum distance from any point to its
corresponding cluster center, Nmax , Nmin the maximum and minimum cardinality respectively of a
cluster and Nj the set of null generators with respect to output j .
n
The performance of Neural Path K-means is evaluated with Proposition 6. The result we deduce
is analogous to Zonotope K-means. The bound of the approximation error becomes zero when K
approaches n. Indeed, for K = n we get δmax = 0, Nmax = 1 and Nj = 0, ∀j ∈ [m]. For lower
values of K, the upper bound reaches a value depending on the magnitude of the weights of the linear
layers together with weights corresponding to null generators.
7
Published as a conference paper at ICLR 2022
Table 1: Reporting accuracy of compressed networks for single output compression methods.
Percentage of remaining neurons	MNIST 3/5				MNIST 4/9	
	(Smyrnis et al., 2020)	Zonotope K-means	Neural Path K-means	(Smyrnis et al., 2020)	Zonotope K-means	Neural Path K-means
100% (Original)	99.18 ± 0.27	99.38 ± 0.09	99.38 ± 0.09	99.53 ± 0.09	99.53 ± 0.09	99.53 ± 0.09
5%	99.12 ± 0.37	99.42 ± 0.07	99.25 ± 0.04	98.99 ± 0.09	99.52 ± 0.09	99.48 ± 0.15
1%	99.11 ± 0.36	99.39 ± 0.05	99.32 ± 0.03	99.01 ± 0.09	99.46 ± 0.05	99.35 ± 0.17
0.5%	99.18 ± 0.36	99.41 ± 0.05	99.22 ± 0.11	98.81 ± 0.09	99.35 ± 0.24	98.84 ± 1.18
0.3%	99.18 ± 0.36	99.25 ± 0.37	99.19 ± 0.41	98.81 ± 0.09	98.22 ± 1.38	98.22 ± 1.33
Table 2: Reporting theoretical upper bounds of Propositions 5, 6.
Percentage of remaining neurons	MNIST 3/5		MNIST 4/9	
	Zonotope K-means Neural Path K-means		Zonotope K-means Neural Path K-means	
100%	0.00	0.00	0.00	0.00
10%	17.07	246.74	18.85	229.37
2.5%	15.35	59.42	17.02	63.37
1%	14.79	42.22	16.44	45.58
0.5%	14.57	36.47	16.20	39.71
5	Experiments
We conduct experiments on compressing the linear layers of convolutional neural networks. Our
experiments serve as proof-of-concept and indicate that our theoretical claims indeed hold in practice.
The heart of our contribution lies in presenting novel tropical geometrical background for neural
network approximation that will shed light for further research towards tropical mathematics.
Our methods compress the linear layers of the network layer by layer. They perform a functional
approximation of the original network and thus they are applicable for both classification and
regression tasks. To compare them with other techniques in the literature we choose methods with
similar structure, i.e. structured pruning techniques without re-training. For example, Alfarra et al.
(2020) proposed a compression algorithm based on the sparsification of the matrices representing the
zonotopes which served as an intuition for part of our work. However, their method is unstructured
and incompatible for comparison. The methods we choose to compare are two tropical methods for
single-output (Smyrnis et al., 2020) and multi-output (Smyrnis & Maragos, 2020) networks, Random
and L1 Structured, and a modification of ThiNet (Luo et al., 2017) adapted to linear layers. Smyrnis
et al. (2020); Smyrnis & Maragos (2020) proposed a novel tropical division framework that aimed on
the reduction of zonotope vertices. Random method prunes neurons according to uniform probability,
while L1 prunes those with the lowest value of L1 norm of their weights. Also, ThiNet uses a greedy
criterion for discarding the neurons that have the smallest contribution to the output of the network.
MNIST Dataset, Pairs 3-5 and 4-9 The first experiment is performed on the binary classification
tasks of pairs 3/5 and 4/9 of the MNIST dataset and so we can utilize both of our proposed methods.
In Table 1, we compare our methods with a tropical geometrical approach of Smyrnis et al. (2020).
Their method is based on a tropical division framework for network minimization. For fair comparison,
we use the same CNN with two fully connected layers and hidden layer of size 1000. According to
Table 1, our techniques seem to have similar performance. They retain the accuracy of the network
while reducing its size. Moreover, in Table 2 we include experimental computation of the theoretical
bounds provided by Proposition 5, 6. We notice that the bounds decrease as the remaining weights
get less. The behaviour of the bounds was expected to be incremental because the less weights we
use, the compression gets worse and the error becomes larger. However, the opposite holds which
means that the bounds are tighter for higher pruning rates. It is also important to mention that the
bounds become 0 when we keep all the weights, as expected.
MNIST and Fashion-MNIST Datasets For the second experiment we employ MNIST and
Fashion-MNIST datasets. The corresponding classification is multiclass and thus Neural Path
K-means may only be applied. In Table 3, we compare it with the multiclass tropical method of
Smyrnis & Maragos (2020) using the same CNN architecture they do. Furthermore, in plots 4a,
4b we compare Neural Path K-means with ThiNet and baseline pruning methods by compressing
8
Published as a conference paper at ICLR 2022
Table 3: Reporting accuracy of compressed networks for multiclass compression methods.
Percentage of remaining neurons	MNIST	FaShion-MNIST (Smyrnis & Maragos, 2020) Neural Path K-means (Smyrnis & Maragos, 2020) Neural Path K-means
100% (Original) 50% 25% 10% 5%	98.60 ±	0.03	98.61	± 0.11	88.66 ± 0.54	89.52	± 0.19 96.39 ±	1.18	98.13	± 0.28	83.30 ± 2.80	88.22	± 0.32 95.15 ±	2.36	98.42	± 0.42	82.22 ± 2.85	86.67	± 1.12 93.48 ±	2.57	96.89	± 0.55	80.43 ± 3.27	86.04	± 0.94 92.93 ± 2.59	96.31 ± 1.29	-	83.68 ± 1.06
LeNet5 (LeCun et al., 1998). To get a better idea of how our method performs in deeper architectures
we provide plots 4c,4d that illustrate the performance of compressing a deep neural network with
layers of size 28 * 28, 512, 256, 128 and 10, which We refer to as deepNN. The compression is
executed on all hidden layers beginning from the input and heading to the output. From Table 3, we
deduce that our method performs better than (Smyrnis & Maragos, 2020). Also, it achieves higher
accuracy scores and experience lower variance as shown in plots 4a-4d. Neural Path K-means, overall,
seems to have good performance, even competitive to ThiNet. Its worst performance occurs on low
percentages of remaining weights. An explanation for this is that K-means provides a high-quality
compression as long as the number of centers is not less than the number of "real" clusters.
(a) LeNet5, MNIST (b) LeNet5, F-MNIST (c) deepNN, MNIST
(d) deepNN, F-MNIST
(e) AlexNet, CIFAR10 (f) CIFAR-VGG, CIFAR10 (g) AlexNet, CIFAR100
Figure 4: Neural Path K-means compared with baseline pruning methods and ThiNet. Horizontal
axis shows the ratio of remaining neurons in each hidden layer of the fully connected part.
(h) CIFAR-VGG, CI-
FAR100
CIFAR Dataset We conduct our final experiment on CIFAR datasets using CIFAR-VGG (Blalock
et al., 2020) and an altered version of AlexNet adapted for CIFAR. The resulting plots are shown in
Fig. 4e-4h. We deduce that Neural Path K-means retains a good performance on larger datasets. In
particular, in most cases it has slightly better accuracy an lower deviation than the baselines, but has
worse behaviour when keeping almost zero weights.
6	Conclusions and Future Work
We presented a novel theorem on the bounding of the approximation error between two neural
networks. This theorem occurs from the bounding of the tropical polynomials representing the neural
networks via the Hausdorff distance of their extended Newton polytopes. We derived geometrical
compression algorithms for the fully connected parts of ReLU activated deep neural networks, while
application to convolutional layers is an ongoing work. Our algorithms seem to perform well in
practice and motivate further research towards the direction revealed by tropical geometry.
9
Published as a conference paper at ICLR 2022
References
Motasem Alfarra, Adel Bibi, Hasan Hammoud, Mohamed Gaafar, and Bernard Ghanem. On the
decision boundaries of deep neural networks: A tropical geometry perspective. arXiv preprint
arXiv:2002.08838, 2020.
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? arXiv preprint arXiv:2003.03033, 2020.
Peter Butkovic. Max-linear systems: theory and algorithms. Springer monographs in mathematics.
Springer, 2010. ISBN 978-1-84996-298-8.
Vasileios Charisopoulos and Petros Maragos. Morphological perceptrons: geometry and training
algorithms. In International Symposium on Mathematical Morphology and Its Applications to
Signal and Image Processing, pp. 3-15. Springer, 2017.
Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks with piecewise
linear activations. arXiv preprint arXiv:1805.08749, 2018.
Raymond A Cuninghame-Green. Minimax algebra, volume 166. Springer Science & Business
Media, 2012.
Nikolaos Dimitriadis and Petros Maragos. Advances in morphological neural networks: Training,
pruning and enforcing shape constraints. In Proc. 46th IEEE Int’l Conf. Acoustics, Speech and
Signal Processing (ICASSP-2021), Toronto, June 2021.
Richard James Duffin and Elmor L Peterson. Geometric programming with signomials. Journal of
Optimization Theory and Applications, 11(1):3-35, 1973.
Branko Grunbaum. Convex polytopes, volume 221. Springer Science & Business Media, 2013.
Anna-Kathrin Kopetzki, Bastian Schurmann, and Matthias Althoff. Methods for order reduction of
zonotopes. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pp. 5626-5633.
IEEE, 2017.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A Filter Level Pruning Method for Deep Neural
Network Compression. In 2017 IEEE International Conference on Computer Vision (ICCV), pp.
5068-5076, 2017. doi: 10.1109/ICCV.2017.541.
Diane Maclagan and Bernd Sturmfels. Introduction to tropical geometry, volume 161. American
Mathematical Soc., 2015.
Petros Maragos. Dynamical systems on weighted lattices: general theory. Mathematics of Control,
Signals, and Systems, 29(4):1-49, 2017.
Petros Maragos and Emmanouil Theodosis. Multivariate tropical regression and piecewise-linear
surface fitting. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 3822-3826. IEEE, 2020.
Petros Maragos, Vasileios Charisopoulos, and Emmanouil Theodosis. Tropical geometry and machine
learning. Proceedings of the IEEE, 109(5):728-755, 2021. doi: 10.1109/JPROC.2021.3065238.
Guido Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. arXiv preprint arXiv:1402.1869, 2014.
Georgios Smyrnis and Petros Maragos. Multiclass neural network minimization via tropical newton
polytope approximation. In Proc. Int’l Conf. on Machine Learning, PMLR, 2020.
Georgios Smyrnis, Petros Maragos, and George Retsinas. Maxpolynomial division with application to
neural network simplification. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 4192-4196. IEEE, 2020.
10
Published as a conference paper at ICLR 2022
Emmanouil Theodosis and Petros Maragos. Analysis of the viterbi algorithm using tropical algebra
and geometry. In 2018 IEEE 19th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC), pp.1-5.IEEE, 2018.
Emmanouil Theodosis and Petros Maragos. Tropical modeling of weighted transducer algorithms on
graphs. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 8653-8657, 2019. doi: 10.1109/ICASSP.2019.8683127.
Nikos Tsilivis, Anastasios Tsiamis, and Petros Maragos. Sparsity in max-plus algebra and applications
in multivariate convex regression. In Proc. 46th IEEE Int’l Conf. Acoustics, Speech and Signal
Processing (ICASSP-2021), Toronto, June 2021.
Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In
International Conference on Machine Learning, pp. 5824-5832. PMLR, 2018.
Gunter M Ziegler. Lectures on polytopes, volume 152. Springer Science & Business Media, 2012.
11
Published as a conference paper at ICLR 2022
A Proofs for the section "Background on Tropical Geometry "
A.1 Proof of Proposition 3
Proof. The first argument follows from the fact that both pj , qj are linear combinations of tropical
polynomials consisting of two terms. Indeed, we compute
pj (x) =	cji max(aiT x + bi, 0) =	max(cjiaiT x + cjibi, 0) =P=ro=p=. ⇒1
cji>0	cji>0
Pj =㊉ ENeWt (max(cjiaτX + Cjibi, 0))
cji>0
Each ENeWt max(cjiaiTX + cjibi, 0) is a line segment With endpoints 0 and cjiaiT, cjibi =
cji aiT , bi . Therefore Pj is Written as the MinkoWski sum of line segments, Which is a zonotope by
definition. Similarly Qj is a zonotope.
Furthermore, from the definition of the MinkoWski sum, each point v ∈ Pj may be Written as
Pc >0 vi, Where each vi is a point in the segment ENeWt max(cjiaiTX + cjibi, 0) . A vertex of
Pj can only occur if vi is an extreme point of ENeWt max(cjiaiTX + cjibi, 0) for every i Which is
equivalent to either vi = 0 or vi = cji aiT , bi . This means that every vertex of Pj corresponds to a
subset I+ ⊆	[n]	of indices i With	cji	> 0,	for Which We choose vi =	cji	aiT,	bi	and for the rest it
holds vi = 0. Thus,
V = X Cji (aT, bi)
i∈I+
In the same way We derive the analogous result for the negative ZonotoPe Qj.	□
Corollary 2. The geometric result concerning the structure of zonotopes can be extended to max-
pooling layers. For instance, a max-pooling layer of size 2 × 2 corresponds to a polytope that is
constructed as the Minkowski sum of pyramids which could stand as generalized case of zonotope.
B	Proofs for the section "Approximation of Tropical
Polynomials "
B.1	Proof of Proposition 4
Proof. Consider a point X ∈ B and assume that P(X) = aτX + b, P(X) = CTX + d. Then,
P(X) — P(X) = P(X) — max {aτX + b} ≤ aτX + b - (UT, v)(：)
(aT ,b)∈Vp	V/
where (UT, V) may be any vertex of P. Similarly, we derive
(rT, s) (X) — (cTX + d) ≤ p(x) — P(x)
for any vertex (rT, S) of P. Therefore, we may select the vertices (UT, V) ∈ P, (rT, S) ∈ P so that
their respective distances from aT , b and cT , d , respectively, are minimized. Choosing them in
such a way gives
p(x) — P(X) ≤ aTx + b ^(aT, b) In similar manner we deduce p(x) — P(x) ≥ (rT, s)( “3 ,s	-(uT, v) (X) = ((aT, b) - (uT, v)) (X) ≤ (5) -(ut, v) Il (X)	≤ d ((aT, b) , P) Pr + 1 l) - cTx + d = ((rT, s) — (cT, d)) (x) ≥ / ∖	(6) )— (cT, d)∣l (1) ≥ -d (P, (cT, d)) Pr2 + 1
12
Published as a conference paper at ICLR 2022
Notice, that for the relations (5) and (6) we used Cauchy-Schwartz inequality
|hx, yi| ≤ kxkkyk ⇔ -kxkkyk ≤ hx, yi ≤ kxkkyk
Inequality (5) holds at any point x ∈ B for some vertex aT , b ∈ P, therefore
P(X) - P(X) ≤ P ∙ max d ((aτ, b) , VP)	(7)
(aT,b)∈VP	P
for all x ∈ B. Similarly, we derive
P(X) — P(X) ≥ min —ρ ∙ d (Vp, (CT, d)) = — max ρ ∙ d (Vp, (CT, d))	(8)
(c,d)∈Vp	(cT,d)∈Vp
Combining (7) and (8) gives
一 max ρ ∙ d (Vp, (CT, d)) ≤ P(X) — P(X) ≤ max ρ ∙ d ((aτ, b) , VP) ⇔
(cτ,d)∈Vp	(aτ,b)∈Vp
IP(X) -P(X)| ≤ P ∙ max ∖ t Tmax, P ∙ d ((aT, b), VP) ,, ιnax , P ∙
[(a ,b)∈Vp	(C ,d)∈Vρ
Hence, from the definition of the Hausdorff distance of two polytopes we derive the desired upper
bound
|p(x) — P(x)∣ ≤ ρ∙H (P,P) , ∀x ∈B⇒
max ∣p(x) — P(X) ∣ ≤ ρ ∙ H (p,p )
□
d (VP, (CT,d))
Remark 2. Note that with similar proof one may replace the Hausdorff distance of the two polytopes
by the Hausdorff distance of their upper envelopes. This makes our theorem an exact generalization
of Proposition 2. However, this format is difficult to use in practice, because it is computationally
harder to determine the vertices of the upper envelope.
B.2	Proof of Theorem 1
Proof. Notice that we may write
mm
kv(X) — MX)kι = X |vj(X) — Vj(X)I = X I(Pj(X) — qj(X)) — (Pj(X) — qj(X))I
j=1	j=1
mm
=X I(Pj(x) — Pj(x)) — (qj(x) — qj(X))I ≤ X |Pj(x) — Pj(X)I + |qj(X) — q(X)I
j=1	j=1
Thus from from Proposition 4 we derive
(m	∖
X H (Pj ,Pj) + H (Qj, Qj) J
□
C Proofs and Figures for the section "Neural Network
Compression Algorithms "
C.1 Illustration of Zonotope K-means
Below we present a larger version of Fig. 5 demonstrating the execution of Zonotope K-means .
13
Published as a conference paper at ICLR 2022
x1
x2
-5
{aji}i∈(1,2),j∈(1,...,7)
匕7
Figure 5: Illustration of Zonotope K-means execution. The original zonotope P is generated by
ci aiT, bi for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7.
The approximation P of P is colored in purple and generated by Ci (af, b) , i = 1,2 where the
first generator is the K-means center representing the generators 1, 2 of P and the second is the
representative center of 3, 4. Similarly, the approximation Q of Q is colored in green and defined by
the generators cCi aCiT , Cbi , i = 3, 4 that stand as representative centers for {5, 6} and 7 respectively.
(d) Approximating zonotopes.
v
C.2 Proof of Proposition 5
Proof. We remind that for the output functions it holds
v(x) = p(x) - q(x) , vC(x) = pC(x) - qC(x)
From triangle inequality we deduce
|v(x) - vC(x)| = |p(x) - q(x) - (pC(x) - qC(x))| < |p(x) - pC(x)| + |q(x) - qC(x)|
Prop 4 bounds |p(x) - pC(x)| and |q(x) - qC(x)| are bounded by H P, PC and H Q, QC respec-
tively. Therefore, it suffices to get an upper bound for these Hausdorff distances. Let us consider any
14
Published as a conference paper at ICLR 2022
vertex U = Pi∈ι十 Ci (OT, b，of P. For the vertex U ∈ P We need to choose vertex V ∈ P as close
to u as possible, in order to provide an upper bound for dist (u, P). Vertex V is selected as follows.
For each i ∈ I+ we select k such that Ck (a£ bk) is the center of the cluster where Ci (af, b,
belongs to. We denote the set of such clusters by C+, where each cluster center k appears only once.
Then, vertex V is constructed as V = Pk∈C CCk aCkT Cbk ∈ PC . We have that:
X Ci (aT, bi) - X Ck 园,bk)
i∈I+	k∈C+
E E cCT ,bi) —
k∈C+ i∈Ik+
Ci (aT, bi) + εi
^+
where we denote by Ik+ the set of indices i ∈ I+ that belong to the center k ∈ C+ and εi =
Ck (aT, Ck) - Ci (a/, bi) is the vector of the difference of the i-th generator, with its corresponding
K-means cluster center.
The maximum value of the upper bound occurs when I+ contains all indices that correspond
to Ci > 0. This value gives us an upper bound for maxu∈P d(U, P). To compute an up-
per bound for maxv∈v户 d(P, V) we assume V = Pk∈c十 Ck (a/ IJk) and consider the vertex
Pi∈I Ci aiT , bi ∈ P where I+ is the set of indices of positive generators corresponding to the
union of all clusters corresponding to the centers of C+. Note that the occurring distance
X Ci (aT, bi) - X Ck 卜T, bi)
i∈I+	k∈C+
was taken into account when computing the upper bound for maxu∈VP d(U, P), and thus both values
obtain the same upper bound. Therefore,
H (p,P) ≤ K+ ∙ δmax +(l- N^) X M∣(aT, bi)∖∖
Nmax
i∈I+
1	T7 ∙ . 1	F	C 1 ,	.	1♦,六	1 T ,1 ♦	1 ∙
where K+ is the number of cluster centers corresponding to P and I+ the indices corresponding to
all positive generators of P. Similarly,
H (Q, Q) ≤ K- ∙ δmax + (1 - N-) X |Ci| ∖∖ (aT, bi) ∖∖
Nmax
i∈I-
where K- , I- are defined in analogous way for the negative zonotope. Combining the relations gives
the desired bound.
1 ∙ |v(x) - V(χ)∣ ≤ H
ρ
P,PC +H Q,QC
≤ K ∙ δmax +
□
15
Published as a conference paper at ICLR 2022
C.3 Illustration for Neural Path K-means algorithm
Below we illustrate the vectors on which K-means is applied for the multi-output case. The vectors
that are compressed are consist of all the edges associated to a hidden layer neuron. The corresponding
edges contain all the possible neural paths that begin from some node of the input, end in some node
of the output and pass through this hidden node.
bn
Figure 6: Neural Path K-means for multi-output neural network compression. In green color we
highlight the weights corresponding to the i-th vector used by Neural Path K-means.
In the main text we defined the null generators of zonotopes that occur by the execution of Neural
Path K-means. Below we provide an illustration for them.
Figure 7: Visualization of K-means in Rd+1+n, where d is the input dimension and n the hidden
layer size. We color points according to the j-th output component of the network. Black and white
points correspond to generators of Pj and Qj respectively. White vertices in positive (brown) clusters
and black vertices in negative (blue) clusters are null generators regarding j-th output.
C.4 Proof of Proposition 6
Proof. Let us first focus on a single output, say j-th output. As in the proof for Zonotope K-means,
We will bound H (pj,Pj) , H(Qj,Qj) for all j ∈ [m]. From triangle inequality We get
|vj(X) - Vj(X)I ≤ |Pj(X) - Pj(X) | + |qj(X) - qj(X)I
Any vertex of u ∈ Pj can be Written as u = Pi∈I cji aiT, bi Where the set of indices Ij+ satisfy
cji > 0, ∀i ∈ Ij + and thus correspond to positive generators. To choose a nearby vertex from Pj
We perform the following. For each i ∈ Ij+ We select the center (aT Bk C(k)T) of the cluster to
where (af	b	C(i)T)	belongs, only if Cjk	>	0.	Such a center only exists if	Cji	(af,	b，is not a
16
Published as a conference paper at ICLR 2022
null generator. Else, We choose as representation the vector 0. Each cluster center, or 0, is taken into
account once and the vertex Pk∈0,十 Cjk (^k, bk) ∈ Pj is formed. We derive:
max dist ( u,Pj I ≤
u∈Vpj	j	3J ~
X Cji (aT, M- X Cjk (aT, Ik)
i∈Ij+
≤ ∑	∑ Cji (aT, bi) - Cjk
≤ Σ Σ
k∈Cj+ i∈Ijk+
Cji (aT, bi) - j
+ E ∣Cji∣∣∣(aT,bi)∣∣
i∈Nj+
+ X ∣Cji∣∣∣(aT,bi)∣∣
i∈Nj+
≤ E E Cji (aT,bi)-
(Cji + εji) [(aj, bi) + ʌi]
k∈Ci∈Ijk+
≤ XX-
k∈Cj+ i∈Ijk+ L
IIjk+∣
∣εji∣kλik
∣1jk+ ∣
+ ∑ ICji∣∣∣(aT,bi)∣∣
i∈Nj+
+ XX Ihi ∣∣∣(aT,：i)∣∣ + ∣Cji∣kλik 1 + X ∣Cji∣∣∣(aT,bi)∣∣
k∈C一 i∈I~、	j jk+l	i∈N一
where for all i ∈ Ijk+ the i-th vector of K-means is represented by the k-th center k ∈ C7-+. We
also assumed C⑻=C：,i + ε(i) ⇒ Cji = Cji + εj and (C, bi) = (aT, bi) + λi.
The maximum value of the upper bound occurs when j contains all indices that correspond
to Cji > 0. To compute an upper bound for maxv∈v户 dist (Pj, v) we write the vertex V as
V = Pk∈Cj+ Cjk(C bk) ∈ Pj and choose the vertex U = £鹏十 Cji (aT, bi) of Pj where
Ij+ is the set of all indices corresponding to generators that belong to these clusters. As in the
proof of Proposition 5, their distance was taken into account when computing the upper bound for
maxu∈vp, dist (u, Pj. Hence, both obtain the same upper bound which leads to
H(Pj,j≤ X	X
k∈C j + i∈1jk +
+ X	X
k∈C j + i∈1jk +
以 ∣kλik + Λ _	1 ʌ
∣ Ijk+∣	1	∣ Ijk+ ∣√
归ji∣ ∣∣(aT, bi)∣∣+ ICjiIkλik
Ijk+
+
+ ∑ ∣CT∣(aT,bi)∣∣
i∈Nj+
where Ij+ contains all indices corresponding to positive Cji . Similarly, we deduce
⅛Bk+(ι - j-) ∣ Cji ∣∣∣(aT, bi)∣+
∣ji ∣∣∣(aT,,bi)∣∣∣T%k#+ X ∣Cji∣∣∣(aT,bi)∣∣
∣ ljk~∣	i∈N
where Ij- contains all i such that Cji < 0. In total these bounds together with Proposition 4 give
17
Published as a conference paper at ICLR 2022
1
—∙ max
P x∈B
∖vj (χ) — Vj (χ)∣ ≤ £ £
归 j∕kλik
k∈Cj 记Ijk l
∖Ijk ∖
mɪj ∖cji∖ll(aT, bi) l l +
+ Σ Σ
k∈Cj i∈Ijk
∖εji∖ ll(aT, bi) Il + ∖cji∖kλik
∖Ijk ∖
+ ∑ ∖Cji∖∣∣(aT,bi)ll
i∈Nj
Here We used the notation Cj = C7-+ U Cj- = {1,2,…，K} and Ijk is either equal to Ijk+ or Jjk—
depending on k ∈ Cj. Note that {i∖i ∈ Ijk, k ∈ Cj} = {1,2,…，n}\ Nj ⊆ {1,2,…，n}, since
every generator that is not null corresponds to some cluster center with the same sign. Also, using
Nmax ≥ ∖Ijk∖ ≥ Nmin, it follows that
1
P
n
-max ∖vj (X) — Vj (x)∖ ≤ 工
X	i=1
n
+X
i=1
归 ji∖kλik
Nmin
∖cji ∖ 11 (aT, bi) l l
∖εji∖ II(aT, bi)U + ∖cji∖kλik
Nmin
+
+ ∑ ∖cji∖∣I(aT,bi)ll
i∈Nj
We will compute the total cost that combines all outputs by applying the inequality
m
⇔ Σ› ∖ ≤ √m ∣∣(uι,…,Um) k
j=i
which is a direct application of Cauchy-Schwartz inequality. Together with the relations
llɛ(i)k < δmax,	∣∣λi∣ <δmax,weget
1 m	m n
p ∙ X m∈χ∖vj(x) - Vj (χ)∖ ≤ XX
mn
+X X
ɪ +(1-Nmx)∖T(aT ,bi)M +
归ji∖ Il (aT, bi) U + ∖cji∖ lλi∣
Nmin
m
+ XX M∖∣I(aT,bi)ll
j = 1 i∈Nj
n
≤X
i=1
√m llC：,ik 11 (aT, bi) l l
+
n
+X
i=1
√m l l ε") l l l l (aT, bi) 11 + √m kC：,il kλil
Nmin
m
+ XX 1T (aT, bi) Il
j =1 i∈Nj
≤ √mKδmax + √m
√mδmax
Nmin
as desired.
nm
X(MaT, bi) I I + ∣C,i∣)+ XX M∖∣∣(aT, bi) Il
i=1	j = 1 i∈Nj
□
18