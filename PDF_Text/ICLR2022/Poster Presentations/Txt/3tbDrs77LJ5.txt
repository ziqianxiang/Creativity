Published as a conference paper at ICLR 2022
Large Learning Rate Tames Homogeneity:
Convergence and Balancing Effect
Yuqing Wang, Minshuo Chen, Tuo Zhao, Molei Tao
Georgia Institute of Technology
{ywang3398,mchen393,tourzhao,mtao}@gatech.edu
Ab stract
Recent empirical advances show that training deep models with large learning rate
often improves generalization performance. However, theoretical justifications on
the benefits of large learning rate are highly limited, due to challenges in analysis. In
this paper, we consider using Gradient Descent (GD) with a large learning rate on a
homogeneous matrix factorization problem, i.e., minX,Y kA - XY > k2F. We prove
a convergence theory for constant large learning rates well beyond 2/L, where L
is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously
establish an implicit bias of GD induced by such a large learning rate, termed
‘balancing’, meaning that magnitudes of X and Y at the limit of GD iterations
will be close even if their initialization is significantly unbalanced. Numerical
experiments are provided to support our theory.
1	Introduction
Training machine learning models such as deep neural networks involves optimizing highly nonconvex
functions. Empirical results indicate an intimate connection between training algorithms and the
performance of trained models (Le et al., 2011; Bottou et al., 2018; Zhang et al., 2021; Soydaner,
2020; Zhou et al., 2020). Especially for widely used first-order training algorithms (e.g., GD and
SGD), the learning rate is of essential importance and has received extensive focus from researchers
(Smith, 2017; Jastrzebski et al., 2017; Smith, 2018; Gotmare et al., 2018; Liu et al., 2019; Li & Arora,
2019). A recent perspective is that large learning rates often lead to improved testing performance
compared to the counterpart trained with small learning rates (Smith & Topin, 2019; Yue et al., 2020).
Towards explaining the better performance, a common belief is that large learning rates encourage
the algorithm to search for flat minima, which often generalize better and are more robust than sharp
ones (Seong et al., 2018; Lewkowycz et al., 2020).
Despite abundant empirical observations, theoretical understandings of the benefits of large learning
rate are still limited for non-convex functions, partly due to challenges in analysis. For example, the
convergence (of GD or SGD) under large learning rate is not guaranteed. Even for globally smooth
functions, very few general results exist if the learning rate exceeds certain threshold (Kong & Tao,
2020). Besides, popular regularity assumptions such as global smoothness for simplified analyses are
often absent in homogeneous models, including commonly used ReLU neural networks.
This paper theoretically studies the benefits of large learning rate in a matrix factorization problem
12
min (IA - XY >∣∣ F , where A ∈ Rn×n, X,Y ∈ Rn×d
(1)
We consider Gradient Descent (GD) for solving (1): at the k-th iteration, we have
Xk+1 = Xk + h(A - XkYk>)Yk and Yk+1 =Yk+h(A> -YkXk>)Xk,
where h is the learning rate. Despite its simple formula, problem (1) serves as an important foundation
of a variety of problems, including matrix sensing (Chen & Wainwright, 2015; Bhojanapalli et al.,
2016; Tu et al., 2016), matrix completion (Keshavan et al., 2010; Hardt, 2014), and linear neural
networks (Ji & Telgarsky, 2018; Gunasekar et al., 2018).
1
Published as a conference paper at ICLR 2022
Problem (1) possesses several intriguing properties. Firstly, the objective function is non-convex,
and critical points are either global minima or saddles (see e.g., Baldi & Hornik (1989); Li et al.
(2019b); Valavi et al. (2020a); Chen et al. (2018)). Secondly, problem (1) is homogeneous in X
and Y , meaning that rescaling X, Y to aX, a-1Y for any a 6= 0 will not change the objective’s
value. This property is shared by commonly used ReLU neural networks. A direct consequence of
homogeneity is that global minima of (1) are non-isolated and can be unbounded. The curvatures at
these global minima are highly dependent on the magnitudes of X, Y . When X, Y have comparable
magnitudes, the largest eigenvalue of Hessian is small, and this corresponds to a flat minimum; on
the contrary, unbalanced X and Y give a sharp minimum. Last but not the least, the homogeneity
impairs smoothness conditions of (1), rendering the gradient being not Lipschitz continuous unless
X, Y are bounded. See a formal discussion in Section 2.
Existing approaches for solving (1) often uses explicit regularization (Ge et al., 2017; Tu et al., 2016;
Cabral et al., 2013; Li et al., 2019a), or infinitesimal (or diminishing) learning rates for controlling
the magnitudes of X, Y (Du et al., 2018; Ye & Du, 2021). In this paper, we go beyond the scope of
aforementioned works, and analyze GD with a large learning rate for solving (1). In particular, we
allow the learning rate h to be as large as approximately 4/L (see more explanation in Section 2),
where L denotes the largest eigenvalue of Hessian at GD initialization. In connection to empirical
observations, we provide positive answers to the following two questions:
Does GD with large learning rate converge at least for some cases of (1)?
Does larger learning rate biases toward flatter minima (i.e., X, Y with comparable magnitudes)?
We theoretically show the convergence of GD with large learning rate for the two situations n =
1,d ∈ N+ or d = 1, n ∈ N+ with isotropic A. We also observe a, perhaps surprising, “balancing
effect” for general matrix factorization (i.e., any d, n, and A), meaning that when h is sufficiently
large, the difference between X and Y shrinks significantly at the convergence of GD compared to
its initial, even if the initial point is close to an unbalanced global minimum. In fact, with a proper
large learning rate h, kXk - Ykk2F may decrease by an arbitrary factor at its limit. The following is a
simple example of our theory for n = 1 (i.e. scalar factorization), and more general results will be
presented later with a precise bound for h depending on the initial condition and A.
Theorem 1.1 (Informal version of Thm.3.1 & 3.2). Given scalar A and initial condition X0 , Y0 ∈
R1×d chosen almost everywhere, with learning rate h . 4/L, GD converges to a global minimum
(X, Y) satisfying ∣∣X∣∣F + ∣∣YkF ≤ 2. Consequently, its extent of balancing is quantified by
kX - Y kF ≤ 2 - 2A.
We remark that having a learning rate h ≈ 4/L is far beyond the commonly analyzed regime in
optimization. Even for globally L-smooth objective, traditional theory requires h < 2/L for GD
convergence and h = 1/L is optimal for convex functions (Boyd et al., 2004), not to mention that
our problem (1) is never globally L-smooth due to homogeneity. Modified equation provides a tool
for probing intermediate learning rates (see Hairer et al. (2006, Chapter 9) for a general review, and
Kong & Tao (2020, Appendix A) for the specific setup of GD), but the learning rate here is too large
for modified equation to work (see Appendix C). In fact, besides blowing up, GD with large learning
rate may have a zoology of limiting behaviors (see e.g., Appendix B for convergence to periodic
orbits under our setup, and Kong & Tao (2020) for convergence to chaotic attractors).
Our analyses (of convergence and balancing) leverage various mathematical tools, including a
proper partition of state space and its dynamical transition (specifically invented for this problem),
stability theory of discrete time dynamical systems (Alligood et al., 1996), and geometric measure
theory (Federer, 2014).
The rest of the paper is organized as: Section 2 provides the background of studying (1) and discusses
related works; Section 3 presents convergence and balancing results for scalar factorization problems;
Section 4 generalizes the theory to rank-1 matrix approximation; Section 5 studies problem (1) with
arbitrary A and its arbitrary-rank approximation; Section 6 summarizes the paper and discusses
broadly related topics and future directions.
2
Published as a conference paper at ICLR 2022
2	Background and Related Work
Notations. kvk is `2 norm of a column or row vector v. kMkF is the Frobenius norm of a matrix M.
Sharp and flat minima in (1) We discuss the curvatures at global minima of (1). To ease the
presentation, consider simplified versions of (1) with either n = 1 or d = 1. In this case, X and
Y become vectors and we denote them as x, y, respectively. We show the following proposition
characterizing the spectrum of Hessian at a global minimum.
Proposition 2.1. When n = 1, d ∈ N+ or d = 1, n ∈ N+ in (1), the largest eigenvalue of Hessian
at a global minimum (x, y) is kxk2 + kyk2 and the smallest eigenvalue is 0.
Homogenity implies global minimizers of (1) are not isolated, which is consistent with the 0 eigen-
value. On the other hand, if the largest eigenvalue kxk2 + kyk2 is large (or small), then the curvature
at such a global minimum is sharp (or flat), in the direction of the leading eigenvector. Meanwhile,
note this sharpness/flatness is an indication of the balancedness between magnitudes of x, y at a
global minimum. To see this, singular value decomposition (SVD) yields that at a global minimum,
(x, y) satisfies xy> 2F = σm2 ax(A). Therefore, large kxk2 + kyk2 is obtained when |kxk - kyk| is
large, i.e., x and y magnitudes are unbalanced, and small kxk2 + kyk2 is obtained when balanced.
Large learning rate We study smoothness properties of (1) and demonstrate that our learning rate
is well beyond conventional optimization theory. We first define the smoothness of a function.
Definition 2.2 (L-smooth). A function f ∈ C1 defined on RN is L-smooth if for all u1 , u2 ∈ RN,
kVf (uι) -Vf (u2)k ≤ LiIuI- U2∣∣.	(2)
If further f ∈ C2, then V2f W LI. Moreover, if We have (2) for uι, u2 ∈ X ⊆ RN, We call it locally
L-smooth.
In traditional optimization (Nesterov, 2003; Polyak, 1987; Nesterov, 1983; Polyak, 1964; Beck &
Teboulle, 2009), most analyzed objective functions often satisfy (i) (some relaxed form of) convexity
or strong convexity, and (ii) L-smoothness. Choosing a step size h < 2/L guarantees the convergence
of GD to a minimum by the existing theory (revieWed in Appendix F.3). Our choice of learning rate
h ≈ 4/L (more precisely, 4/L0; see beloW) goes beyond the classical analyses.
Besides, in our problem (1), the regularity is very different. Even simplified versions of (1), i.e., With
either n = 1 or d = 1, suffer from (i) non-convexity and (ii) unbounded eigenvalues of Hessian, i.e.,
no global smoothness (see Appendix F.2 for more details). As shoWn in Du et al. (2018); Ye & Du
(2021); Ma et al. (2021), decaying or infinitesimal learning rate ensures that the GD trajectory stays in
a locally smooth region. HoWever, the gap betWeen the magnitudes of X, Y can only be maintained
in that case. We shoW, hoWever, that larger learning rate can shrink this gap. More precisely, if initial
condition is Well balanced, there is no need to use large learning rate; otherWise, We can use learning
rate as large as approximately 4/L, and Within this range, larger h provides smaller gap betWeen X
and Y at the limit (i.e. infinitely many GD iterations).
Related work Matrix factorization problems in various forms have been extensively studied in
the literature. The version of (1) is commonly knoWn as the loW-rank factorization, although here d
can arbitrary and We consider both d ≤ rank(A) and d > rank(A) cases. Baldi & Hornik (1989);
Li et al. (2019b); Valavi et al. (2020b) provide landscape analysis of (1). Ge et al. (2017); Tu et al.
(2016) propose to penalize the Frobenius norm of X>X - Y >Y 2F to mitigate the homogeneity
and establish global convergence guarantees of GD for solving (1). Cabral et al. (2013); Li et al.
(2019a) instead penalize individual Frobenius norms of iXi2F + iY i2F. We remark that penalizing
iX i2F + iY i2F is closely related to nuclear norm regularization, since the variational formula for
nuclear norm ∣∣Z ∣∣2 = minz=χγ > IlX IlF + ∣∣Y kF，More recently, Liu et al. (2021) consider using
injected noise as regularization to GD and establish a global convergence (see also Zhou et al. (2019);
Liu et al. (2022). More specifically, by perturbing the GD iterate (Xk, Yk) With Gaussian noise, GD
Will converge to a flat global optimum. On the other hand, Du et al. (2018); Ye & Du (2021); Ma et al.
(2021) shoW that even Without explicit regularization, When the learning rate of GD is infinitesimal,
3
Published as a conference paper at ICLR 2022
i.e., GD approximating gradient flow, X and Y maintain the gap in their magnitudes. Such an effect
is more broadly recognized as implicit bias of learning algorithms (Neyshabur et al., 2014; Gunasekar
et al., 2018; Soudry et al., 2018; Li et al., 2020; 2021). Built upon this implicit bias, Du et al. (2018)
further prove that GD with diminishing learning rates converges to a bounded global minimum of (1),
and this conclusion is recently extended to the case of a constant small learning rate (Ye & Du, 2021).
Our work goes beyond the scopes of these milestones and considers matrix factorization with much
larger learning rates.
Additional results exist that demonstrate large learning rate can improve performance in various
learning problems. Most of them involve non-constant learn rates. Specifically, Li et al. (2019c)
consider a two-layer neural network setting, where using learning rate annealing (initially large,
followed by small ones) can improve classification accuracy compared to training with small learning
rates. Nakkiran (2020) shows that the observation in Li et al. (2019c) even exists in convex problems.
Lewkowycz et al. (2020) study constant large learning rates, and demonstrate distinct algorithmic
behaviors of large and small learning rates, as well as empirically illustrate large learning rate yields
better testing performance on neural networks. Their analysis is built upon the neural tangent kernel
perspective (Jacot et al., 2018), with a focus on the kernel spectrum evolution under large learning
rate. Worth noting is, Kong & Tao (2020) also study constant large learning rates, and show that large
learning rate provides a mechanism for GD to escape local minima, alternative to noisy escapes due
to stochastic gradients.
3 Overparameterized scalar factorization
In order to provide intuition before directly studying the most general problem, we begin with a
simple special case, namely factorizing a scalar by two vectors. It corresponds to (1) with n = 1 and
d ∈ N+, and this overparameterized problem is written as
min，1(μ - xy>)2,	(3)
x,y∈R1×d 2
where μ is assumed without loss of generality to be a positive scalar. Problem (3) can be viewed as
univariate regression using a linear two-layer neural network with the quadratic loss, which is studied
in Lewkowycz et al. (2020) with atomic data distribution. Yet our analysis in the sequel can be used
to study arbitrary univariate data distributions; see details in Section 6.
Although simplified, problem (3) is still nonconvex and exhibits the same homogeneity as (1). The
convergence of its large learning rate GD optimization was previously not understood, let alone
balancing. Many results that we will obtain for (3) will remain true for more general problems.
We first prove that GD converges despite of > 2/L learning rate and for almost all initial conditions:
Theorem 3.1 (Convergence). Given (x0>, y0>) ∈ (Rd × Rd)\B where B is some Lebesgue measure-0
set, when the learning rate h satisfies
h ≤ ∙ ʃ	4	ɪɪ
≤ U∣xok2 + kyok2 +4μ, 3μj,
GD converges to a global minimum.
Theorem 3.1 says that choosing a constant learning rate depending on GD initialization guarantees
the convergence for almost every starting point in the whole space. This result is even stronger
than the already nontrivial convergence under small learning rate with high probability over random
initialization (Ye & Du, 2021). Furthermore, the upper bound on h is sufficiently large: on the one
hand, suppose GD initialization (x0, y0) is close to an unbalanced global minimum. By Proposi-
tion 2.1, we can check that the largest eigenvalue L(χo, yo) of Hessian V2f (xo, yo) is approximately
∣x0 ∣2 + ∣y0 ∣2 . Consequently, our upper bound of h is almost 4/L, which is beyond 2/L (see
Section 2 for more details). On the other hand, we observe numerically that the 4/L upper bound
is actually very close to the stability limit of GD when initialized away from the origin (see more
details in Appendix E).
The convergence in Theorem 3.1 has an interesting searching-to-converging transition as depicted in
Figure 1, where we observe two phases. In Phase 1, large learning rate drives GD to search for flat
4
Published as a conference paper at ICLR 2022
Z < ll⅜ll2 + IIyUl2 < —
ll⅜ll2+ l∣Λ∣l2≤∙∣
large h
small h
Figure 1:	The dynamics of GD under different learning rate h
regions, escaping from the attraction of sharp minima. After some iterations, the algorithm enters the
vicinity of a global minimum with more balanced magnitudes in x, y. Then in Phase 2, GD converges
to the found balanced global minimum. We remark that the searching-to-converging transition also
appears in Lewkowycz et al. (2020). However, the algorithmic behaviors are not the same. In fact, in
our searching phase (phase 1), the objective function does not exhibit the blow-up phenomenon. In
our convergence phase (phase 2), the analysis relies on a detailed state space partition (see Line 192)
due to nonconvex nature of (3), while the analysis in Lewkowycz et al. (2020) is akin to monotone
convergence in a convex problem.
In comparison with the dynamics of small learning rate, we note that the searching phase (Phase 1) is
vital to the convergence analysis. Meanwhile, the searching phase induces a balancing effect of large
learning rate. The following theorem explicitly quantifies the extent of balancing.
Theorem 3.2 (Balancing). Under the same initial condition and learning rate h as Theorem 3.1, GD
for (3) converges to a global minimizer (x, y) satisfying
kχk2 + kyk2 ≤ 2.
Consequently, its extent of balancing is quantified by
2
kx - yk2 ≤ h - 2μ.
One special case of Theorem 3.2 is the following theorem, which states that no matter how close to a
global minimum does GD start, if this minimum does not correspond to well-balanced norms, a large
learning rate will take the iteration to a more balanced limit. We also demonstrate a sharp shrinkage
in the distance between x and y.
Corollary 3.3 (From 'unbalanced’ to 'balanced’). For any δ ∈ (0, μ), let the GD initialization satisfy
(XO,yo) ∈ {(U,v) : |uv> - μl < δ, kuk2 + kvk2 > 8μ}∖B,
where B is some Lebesgue measure-0 set. When the learning rate h satisfies h =
the extent of balancing at the limiting point (x, y) of GD obeys
4
kx0 k2+ky0 k2+4μ,
kx - yk2 < 2kχo - yok2 + 2μ.
Both Theorem 3.2 and Corollary 3.3 suggest that larger learning rate yields better balancing effect, as
kx - yk2 at the limit of GD may decrease a lot and is controlled by the learning rate. We remark that
the balancing effect is a consequence of large learning rate, as small learning rate can only maintain
the difference in magnitudes of x, y (Du et al., 2018).
In addition, the actual balancing effect can be quite strong with kxk - ykk2 decreasing to be almost 0
at its limit under a proper choice of large learning rate. Figure 2 illustrates an almost perfect balancing
case When h =	7 +4加0 +4 ≈ 0.0122 is chosen as the upper bound. The difference ∣∣xk — yk ∣∣
decreases from approximately 17.9986 to 0.0154 at its limit. Additional experiments with various
learning rates and initializations can be found in Appendix A.
Technical Overview We sketch the main ideas behind Theorem 3.1, Which lead to the balancing
effect in Theorem 3.2. Full proof is deferred to Appendix G.
5
Published as a conference paper at ICLR 2022
九=0.0122
number of iterations
number of iterations
Figure 2:	The objective function is (1 - xy>)2/2, where x> , y> ∈ R10. Highly unbalanced initial
condition is uniformly randomized, with the norms to be kx0k = 18, ky0k = 0.09.
The convergence is proved by handling Phase 1 and 2 separately (see Fig.1). In Phase 1, we prove that
kxk k2 + kykk2 has a decreasing trend as GD searches for flat minimum. We show that kxk k2 + kyk k2
may not be monotone, i.e., it either decreases every iteration or decreases every other iteration.
In Phase 2, we carefully partition the state space and show GD at each partition will eventually enter
a monotone convergence region. Note that the partition is based on detailed understanding of the
dynamics and is highly nontrivial. Attentive readers may refer to Appendix G for more details. The
combination of Phase 1 & 2 is briefly summarized as a proof flow chart in Figure 3.
PhaSe 1
PhaSe 2
Figure 3: Proof overview of Theorem 3.1. At the k-th iteration, we denote (xk, yk) as the iterate and
Sk is defined as Xk+ιy>+ι - μ = sk(xky> - μ)∙
4 RANK- 1 APPROX. OF ISOTROPIC A (AN UNDER-PARAMETERIZED CASE)
Given insights from scalar factorization, we consider rank-1 factorization of an isotropic matrix A,
i.e., A = μln×n with μ > 0, d = 1, and n ∈ N+. The corresponding optimization problem is
min 1 ∣∣μIn×n - Xyτ∣∣F .
x,y∈Rn×1 2	F
(4)
Although similar at an uncareful glance, Problems (4) and (3) are rather different unless n = d = 1.
First of all, Problem (4) is under-parameterized for n > 1, while (3) is overparameterized. More
importantly, we'll show that, When (x, y) is a global minimum of (4), x, y must be aligned, i.e., X = 'y
for some ` > 0. In the scalar factorization problem, however, no such alignment is required. As a
result, the set of global minima of (4) is an n-dimensional submanifold embedded in a 2n-dimensional
space, while in the scalar factorization problem the set of global minimum is a (2d - 1)-dimensional
6
Published as a conference paper at ICLR 2022
submanifold — one rank deficient — in a 2d-dimensional space. We expect the convergence in (4) is
more complicated than that in (3), since searching in (4) is demanding.
To prove the convergence of large learning rate GD for (4), our theory consists of two steps: (i) show
the convergence of the alignment between x and y (this is new); (ii) use that to prove the convergence
of the full iterates (i.e., x & y). Step (i) first:
Theorem 4.1 (Alignment). Given (x0 , y0) ∈ (Rn × Rn)\B, where B is some Lebesgue measure-0
set, when learning rate h ≤ min {此 口2+74[2+4，7*, 2√7μ }, the iterator (Xk ,yk) of GD at the
k-th iteration satisfies | cos(∠(xk, yk))| → 1 as k → ∞.
Proof sketch. A sufficient condition for the convergence of | cos(∠(xk, yk))| is kxk k2 kyk k2 -
(xk>yk)2 → 0. To ease the presentation, let Uk = xk>yk, Vk = xk>xk, and Wk = yk>yk. By the GD
update and some algebraic manipulation, we derive
Vk+1Wk+1 - U2+1 = r ∙ (VkWk - U2),
where r% = (1 - h(Vk + Wk) + h?(VkWk - μ2))2. When k is sufficiently large, We can show a
uniform upper bound on rk < 1 - c for some constant c > 0. In this way, we deduce that VkWk - Uk2
will exponentially decay and converge to 0. More details are provided in Appendix H.	□
Theorem 4.1 indicates that GD iterations will converge to the neighbourhood of {(x, y) : x =
'y, for some ' ∈ R∖{0}}. This helps establish the global convergence as stated in Step (ii).
Theorem 4.2	(Convergence). Under the same initial conditions and learning rate h as Theorem 4.1,
GD for (4) converges to a global minimum.
Similar to the over-parametrized scalar case, this convergence can also be split into two phases where
phase 1 motivates the balancing behavior with the decrease of kxkk2 + kyk k2, and phase 2 ensures
the convergence. The following balancing theorem is thus obtained.
Theorem 4.3	(Balancing). Under the same initial conditions and learning rate h as Theorem 4.1,
GD for (4) converges to a global minimizer that obeys
kxk2 + kyk2 ≤ 2,
h
and its extent of balancing is quantified by
2
kx - yk ≤ h - 2μ.
This conclusion is the same as the one in Section 3. A quantitatively similar corollary like Corol-
lary 3.3 can also be obtained from the above theorem, namely, if x0 and y0 start from an unbalanced
point near a minimum, the limit will be a more balanced one.
5 General matrix factorization
In this section, we consider problem (1) with an arbitrary matrix A ∈ Rn×n . We replicate the
problem formulation here for convenience,
1
min - IlA - XY>∣∣c .
X,Y ∈Rn×d 2	F
(5)
Note this is the most general case with n, d ∈ N+ and any square matrix A. Due to this generalization,
we no longer utilize the convergence analysis and instead, establish the balancing theory via stability
analysis of GD as a discrete time dynamical system.
Let μι ≥ μ2 ≥ ∙∙∙ ≥ μn ≥ 0 be the singular values of A. Assume for technical convenience
k AkF = Pn=ι μ2 being independent of d and n. We denote the singular value decomposition of A
as A = UDV >, where U, D, V ∈ Rn×n, U, V are orthogonal matrices and D is diagonal. Then we
establish the following balancing effect.
7
Published as a conference paper at ICLR 2022
Theorem 5.1. Given almost all the initial conditions, for any learning rate h such that GD for (5)
converges to a point (X, Y ), there exists c = c(n, d) > c0 with constant c0 > 0 independent of h, n,
and d, such that (X, Y ) satisfies
c(kχkF + kY kF) <2,
and the extent ofbalancing is quantified by ∣∣X 一 (UV>)Y∣∣F < Ch — 2 pm=i1{d,n} μ., WhiCh means
min{d,n}
X
i=1
IkX kF -kY kF I2 <ch ― 2t
In particular, when d = 1, i.e., rank-1 factorization of an arbitrary A, the constant c equals 1.
We observe that the extent of balancing can be quantified under some rotation of Y. This is necessary,
since for factorizing a general matrix A (which can be asymmetric), at a global minimum, X, Y may
only align after a rotation (which is however fixed by A, independent of initial or final conditions).
Figure 4 illustrates an example of the balancing effect under different learning rates. Evidently,
larger learning rate leads to a more balanced global minimizer. Additional experiments with various
dimensions, learning rates, and initializations can be found in Appendix A; a similar balancing effect
is also shown for additional problems including matrix sensing and matrix completion there.
斤=0.1839
number of iterations
number of iterations
number of iterations number of iterations
Figure 4: Balancing effect of general matrix factorization. We independently generate elements in
A ∈ R6×6 from a Gaussian distribution. We choose X, Y ∈ R6×100 and randomly pick a pair of
initial point (X0, Y0) with kX0kF = 1 and kY0kF = 9.
Different from previous sections, Theorem 5.1 builds on stability analysis by viewing GD as a
dynamical system in discrete time. More precisely, the proof of Theorem 5.1 (see Appendix I for
details) consists of two parts: (i) the establishment of an easier but equivalent problem via the rotation
ofX and Y, (ii) stability analysis of the equivalent problem.
For (i), by singular value decomposition (SVD), A = UDV>, where U, D, V ∈ Rn×n, U and V are
orthogonal matrices, and D is a non-negative diagonal matrix. Let Xk = URk , Yk = VSk . Then
Xk+1 = Xk + h(A - XkYk>)Yk
Yk+1 = Yk + h(A - XkYk>)>Xk
U Rk+1 = URk + h(UDV> - URk Sk> V> )VSk
⇔	V Sk+1 = VSk + h(UDV> - URk Sk> V> )> URk
⇔	Rk+1 = Rk + h(D - RkSk>)Sk
⇔	Sk+1 =Sk+h(D-RkSk>)>Rk .
Therefore, GD for problem (5) is equivalent to GD for the following problem
1
min - ∣∣D — RS>∣∣c
R,S∈Rn×d 2	F
and it thus suffices to work with diagonal non-negative A.
For (ii), here is a brief description of the idea of stability analysis: consider each iteration of GD as a
mapping ψ from Uk 〜(Xk ,Yk ) to uk+ι 〜(Xk+ι,Yk+ι), where matrices X and Y are flattened
8
Published as a conference paper at ICLR 2022
and concatenated into a vector so that ψ is a closed map on vector space R2dn . GD iteration is thus a
discrete time dynamical system on state space R2dn given by
uk+1 = ψ(Uk) = Uk ― hVf (uk),
where f is the objective function f (Uk) = 11 ∣∣A - XkY> 情，and gradient returns a vector that
collects all component-wise partial derivatives.
It's easy to see that any stationary point of f, denoted by u*,isa fixed point of ψ, i.e., U = ψ(u*).
What fixed point will the iterations of ψ converge to? For this, the following notions are helpful:
Proposition 5.2. Consider a fixed point U of ψ. Ifall the eigenvalues of Jacobian matrix Vψ(u*)
are of complex modulus less than 1, it is a stable fixed point.
Proposition 5.3. Consider a fixed point U of ψ. If at least one eigenvalue of Jacobian matrix
Vψ(u*) is of complex modulus greater than 1, it is an unstable fixed point.
Roughly put, the stable set of an unstable fixed point is of negligible size when compared to that
of a stable fixed point, and thus what GD converges to is a stable fixed point for almost all initial
conditions (Alligood et al., 1996). Thus, we investigate the stability of each global minimum of f
(each saddle of f is an unstable fixed point of ψ and thus is irrelevant). By a detailed evaluation of
Vψ,s eigenvalues (Appendix I), we see that a global minimum (X, Y) of f corresponds to a stable
fixed point of GD iteration if 1 - ch kX k1F + kY k1F < 1, i.e., it is balanced as in Thm. 5.1.
6 Conclusion and Discussion
In this paper, we demonstrate an implicit regularization effect of large learning rate on the homo-
geneous matrix factorization problem solved by GD. More precisely, a phenomenon termed as
“balancing” is theoretically illustrated, which says the difference between the two factors X and Y
may decrease significantly at the limit of GD, and the extent of balancing can increase as learning
rate increases. In addition, we provide theoretical analysis of the convergence of GD to the global
minimum, and this is with large learning rate that can exceed the typical limit of 2/L, where L is the
largest eigenvalue of Hessian at GD initialization.
For the matrix factorization problem analyzed here, large learning rate avoids bad regularities induced
by the homogeneity between X and Y . We feel it is possible that such balancing behavior can also be
seen in problems with similar homogeneous properties, for example, in tensor decomposition (Kolda
& Bader, 2009), matrix completion (Keshavan et al., 2010; Hardt, 2014), generalized phase retrieval
(Candes et al., 2015; Sun et al., 2018), and neural networks with homogeneous activation functions
(e.g., ReLU). Besides the balancing effect, the convergence analysis under large learning rate may be
transplanted to other non-convex problems and help discover more implicit regularization effects.
In addition, factorization problems studied here are closely related to two-layer linear neural networks.
For example, one-dimensional regression via a two-layer linear neural network can be formulated as
the scalar factorization problem (3): Suppose we have a collection of data points (xi, yi) ∈ R × R
for i = 1, . . . , n. We aim to train a linear neural network y = (U>v)x with U, v ∈ Rd for fitting the
data. We optimize U, v by minimizing the quadratic loss,
(u*, v*) ∈ arg min
u,v
nX (yi-(UTvE)I = argmin n (⅛xf
- UTv .	(6)
As can be seen, taking μ = *=1 Xiy recovers (3). In this regard, our theory indicates that training
i=1 xi
of U, v by GD with large learning rate automatically balances U, v, and the obtained minimum is flat.
This may provide some initial understanding of the improved performance brought by large learning
rates in practice. Note that (6) generalizes to arbitrary data distribution of training a two-layer linear
network with atomic data (i.e., x = 1 and y = 0) in Lewkowycz et al. (2020).
It is important to clarify, however, that there is a substantial gap between this demonstration and
extensions to general neural networks, including deep linear and nonlinear networks. Although we
suspect that large learning rate leads to similar balancing effect of weight matrices in the network,
rigorous theoretical analysis is left as a future direction.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We thank anonymous reviewers and area chair for suggestions that improved the quality of this
paper. The authors are grateful for partial supports from NSF DMS-1847802 (YW and MT) and
ECCS-1936776 (MT).
References
Kathleen T Alligood, Tim D Sauer, and James A Yorke. Chaos: an introduction to dynamical systems.
Springer, 1996.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster
semi-definite optimization. In Conference on Learning Theory, pp. 530-582. PMLR, 2016.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004.
Ricardo Cabral, Fernando De la Torre, Joao P Costeira, and Alexandre Bernardino. Unifying nuclear
norm and bilinear factorization approaches for low-rank matrix decomposition. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 2488-2495, 2013.
Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow:
Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015.
Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:
General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.
Zhehui Chen, Xingguo Li, Lin F Yang, Jarvis Haupt, and Tuo Zhao. On landscape of la-
grangian functions and stochastic search for constrained nonconvex optimization. arXiv preprint
arXiv:1806.05151, 2018.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018.
Herbert Federer. Geometric measure theory. Springer, 2014.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. In International Conference on Machine Learning, pp. 1233-1242.
PMLR, 2017.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look
at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint
arXiv:1810.13243, 2018.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical integration.
Oberwolfach Reports, 3(1):805-882, 2006.
10
Published as a conference paper at ICLR 2022
Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th
Annual Symposium on Foundations ofComputer Science, pp. 651-660. IEEE, 2014.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few
entries. IEEE transactions on information theory, 56(6):2980-2998, 2010.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Lingkai Kong and Molei Tao. Stochasticity of deterministic gradient descent: Large learning rate for
multiscale objective function. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2625-2638. Curran
Associates, Inc., 2020.
Quoc V Le, Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, and Andrew Y Ng. On
optimization methods for deep learning. In ICML, 2011.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. 2020.
Qiuwei Li, Zhihui Zhu, and Gongguo Tang. The non-convex geometry of low-rank matrix optimiza-
tion. Information and Inference: A Journal of the IMA, 8(1):51-96, 2019a.
Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran Wang, and Tuo Zhao.
Symmetry, saddle points, and global optimization landscape of nonconvex matrix factorization.
IEEE Transactions on Information Theory, 65(6):3489-3514, 2019b.
Yan Li, Ethan X.Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial
training on separable data. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HkgTTh4FDH.
Yan Li, Caleb Ju, Ethan X Fang, and Tuo Zhao. Implicit regularization of bregman proximal point
algorithm and mirror descent on separable data. arXiv preprint arXiv:2108.06808, 2021.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019c.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. arXiv
preprint arXiv:1910.07454, 2019.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Tianyi Liu, Yan Li, Song Wei, Enlu Zhou, and Tuo Zhao. Noisy gradient descent converges to flat
minima for nonconvex matrix factorization. 2021.
Tianyi Liu, Yan Li, Enlu Zhou, and Tuo Zhao. Noise regularizes over-parameterized rank one matrix
recovery, provably. arXiv preprint arXiv:2202.03535, 2022.
Cong Ma, Yuanxin Li, and Yuejie Chi. Beyond procrustes: Balancing-free gradient descent for
asymmetric low-rank matrix sensing. IEEE Transactions on Signal Processing, 69:867-877, 2021.
Preetum Nakkiran. Learning rate annealing can provably help generalization, even for convex
problems. 2020.
11
Published as a conference paper at ICLR 2022
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o
(1∕k^ 2). In DokL akad. nauk Sssr, volume 269,pp. 543-547,1983.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-
tional mathematics and mathematical physics, 4(5):1-17, 1964.
Boris T Polyak. Introduction to optimization. optimization software. Inc., Publications Division,
New York, 1, 1987.
Sihyeon Seong, Yegang Lee, Youngwook Kee, Dongyoon Han, and Junmo Kim. Towards flatter loss
surface via nonmonotonic learning rate scheduling. In UAI, pp. 1020-1030, 2018.
Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference
on applications of computer vision (WACV), pp. 464-472. IEEE, 2017.
Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1-learning rate,
batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018.
Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using
large learning rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations
Applications, volume 11006, pp. 1100612. International Society for Optics and Photonics, 2019.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Derya Soydaner. A comparison of optimization algorithms for deep learning. International Journal
of Pattern Recognition and Artificial Intelligence, 34(13):2052013, 2020.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of
Computational Mathematics, 18(5):1131-1198, 2018.
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht. Low-rank
solutions of linear matrix equations via procrustes flow. In International Conference on Machine
Learning, pp. 964-973. PMLR, 2016.
Hossein Valavi, Sulin Liu, and Peter Ramadge. Revisiting the landscape of matrix factorization.
In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine
Learning Research, pp. 1629-1638. PMLR, 26-28 Aug 2020a.
Hossein Valavi, Sulin Liu, and Peter J Ramadge. The landscape of matrix factorization revisited.
arXiv preprint arXiv:2002.12795, 2020b.
Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix
factorization. arXiv preprint arXiv:2106.14289, 2021.
Xubo Yue, Maher Nouiehed, and Raed Al Kontar. Salr: Sharpness-aware learning rates for improved
generalization. arXiv preprint arXiv:2011.05348, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-115,
2021.
Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the
importance of noise in training neural networks. In International Conference on Machine Learning,
pp. 7594-7602. PMLR, 2019.
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, et al. Towards theoretically under-
standing why sgd generalizes better than adam in deep learning. arXiv preprint arXiv:2010.05627,
2020.
12
Published as a conference paper at ICLR 2022
Supplementary Materials for “Large Learning Rate Tames
Homogeneity: Convergence and Balancing Effect”
A Additional Experiments
A.1 More results for matrix factorization
In this section, we present more experiments with different choices of n, d, various initializations
and scalings, and a broader range of learning rates. All these experiments verify our claim on the
balancing effect of large learning rate, i.e., the shrinkage of the gap between the magnitudes of X
and Y exists for general matrix factorization problems, i.e., for any choice of n, d ∈ N+.
We first provide examples of scalar factorization in Figure 5, 6, and 7 to numerically justify our
theory in Section 3. In the three figures, the initial conditions randomly generated, respectively with
(kx0k , ky0k) = (9, 1), (kx0k , ky0k) = (19, 1), and (kx0k , ky0k) = (99, 1); the learning rates are
chosen within the range of Theorem 3.1 from large to small as h0, 6h0, 7 h0, 7h0, 7 h0, 7h° for the
1st-6th columns respectively where h0 = 4/(kx0 k2 + ky0 k2 + 8). The learning rates of the left three
columns are larger than 2/L (L is the local Lipschitz constant of gradient near the initial condition),
where we can see the decrease in the gap between kxkk and kyk k and larger learning rate leads to
smaller gap; the right three correspond to h < 2/L where there are almost no changes in kxk k and
kyk k as k increases. Moreover, the loss does not decrease monotonically at the beginning of the
iterations for all the h > 2/L cases while in the later iterations GD shows monotone convergence; for
the right three columns (h < 2/L cases), we can see monotone decrease of the loss. This validates
our two-phase pattern of convergence (see e.g. Figure 1 and Section 3 for detailed explanation).
h =0.04444
10----∙-- 10------∙--- 10------'-- 10	10	，
8	8 1	8	'	：	8	8
6	6	`----------6	6	6
4	4	4	4	4
2卜	2,	2	2	2
0	，	0	，	0 —，	0	0 —
0	50 100	0	50 100	0	50 100	0	50 100	0	50 100
10
10-10
10-15
h =0.0381
h =0.03175
0	50 100	0	50 100	0	50 100
h =0.0254
0	50 100
h =0.01905
0	50 100
h =0.0127
105l----1---
100
10-5
10-10
10-15^_,_
0	50 100
Figure 5:	Scalar factorization with kx0 k = 9, ky0 k = 1. The x-axis represents the number of
iterations k; the y-axis represents the value for the norm of xk and yk in the first row, and the value
for loss in the second row; the learning rate h for each column is the same.
Then we experiment with the general matrix factorization as a supplement of our theory in Sec-
tion 5 where we only rigorously prove the balancing effect given the convergence of GD. In the
following examples, we show that there indeed exist large learning rates that trigger the shrinkage
of the gap between kXkkF and kYk kF and at the same time guarantee the convergence of GD. Fig-
ure 8, 9, and 10 correspond to the over-parameterized version of matrix factorization problem (5)
with A ∈ R6×6 (asymmetric, generated by i.i.d. Gaussian) and X, Y ∈ R6×100. The initial con-
ditions are randomly generated with (kX0kF , kY0kF) = (9, 1), (kX0kF , kY0kF) = (19, 1), and
13
Published as a conference paper at ICLR 2022
0	50 100
h =0.01081
h =0.00927
h =0.00772
h =0.00618
0	50 10010-15
105
10-10
10-5
100
h =0.00463
0	50 100
⅛ =0.00309
105l----,-
100
10-5
10-10
10-15
0	50 100
0	50 100	0	50 100	0	50 100
Figure 6:	Scalar factorization with kx0k = 19, ky0k = 1. The x-axis represents the number of
iterations k; the y-axis represents the value for the norm of xk and yk in the first row, and the value
for loss in the second row; the learning rate h for each column is the same.
100 pi——'----
80
60
40
20----------
0^---------
0	50 100
h =0.00041
10-15
h =0.00035
100
10-10
10-5
105
100
80
60
40
20
0
0	50 100
10-15
h =0.00029
100
10-10
10-5
105
0	50 100	0	50 100
100	
80	
60	
40	
20	
0	Λ	
0	50 100
10-15
h =0.00023
100
10-10
10-5
105
0	50	10010-15
100		
80	
60	
40	
20	
0	
0	50 100
h =0.00017
100
10-10
10-5
105
100	—
80	
60	
40	
20	
0	
0	50 100
h =0.00012
100
10-10
10-5
105
0	50	10010-150	50	100
Figure 7:	Scalar factorization with kx0 k = 99, ky0 k = 1. The x-axis represents the number of
iterations k; the y-axis represents the value for the norm of xk and yk in the first row, and the value
for loss in the second row; the learning rate h for each column is the same.
(kX0 kF , kY0kF) = (99, 1) respectively. Similarly, the learning rates are chosen from large to small
as ho, 7ho, 5ho, 7ho, 3ho, 7ho for the 1st-6th columns respectively where 7h0 (the 2nd column) is
picked near the stability limit. We also similarly provide two examples of the under-parameterized
version in Figure 11 and 12. The two examples correspond to problem (5) with A ∈ R1oo×1oo
(asymmetric, similarly generated as the previous one) and X, Y ∈ R1oo×3. Here we use a shifted
loss which is to subtract the global minimum error.
As is shown in Figure 8, 9, 10, 11, and 12, we can observe the similar phenomenon as the scalar
case, in the sense that (1) larger learning rate gives a smaller the gap between kXk kF and kYk kF in
the limit, except for the overly large h that causes GD to diverge; (2) when the learning rate becomes
sufficiently big, a two-phase pattern of convergence appears, which does not manifest in traditional
optimization analysis and thus indicates that the learning rate is already larger than that permitted by
traditional theory, and yet one still has convergence.
14
Published as a conference paper at ICLR 2022
More precisely, the 2nd-4th columns are the large learning rate cases, where one observes a shrinkage
of the unbalancedness and non-monotonicity of the loss at the beginning, while the right two columns
corresponds to the small learning rates for which kXkkF and kYk kF barely change as k increases,
and the decrease of loss is monotone. These are all evidence of the consistency between the most
general case of matrix factorization in Section 5 and the special cases in Section 3 and 4.
100
1O200
h =0.2093
10
10-10
10-15
0	5	10
h =0.1794
h =0.1495
h =0.1196
10-15
10-10
100
,九=0∙0897
105
10-15
10-10
100
,九=0∙0598
105
0	50	100	0	50	100	0	50	100	0	50	100	0	50	100
Figure 8:	General over-parameterized matrix factorization with kX0kF = 9 kY0kF = 1. The x-axis
represents the number of iterations k; the y-axis represents the value for the norm of Xk and Yk in
the first row, and the value for loss in the second row; the learning rate h for each column is the same.
x10153
10∣x∣0
10200
1000
—IlXMIF
一冈F
h =0.05464
---Loss
10100
01-----------IJ
0	5	10
5	10
20
16
12
8
4
0
0	50	100
20
16
12
8
4
0
0	50	100
20
16
12
8
0
0	50	100
20
16
12
8
0
0	50	100
20
16
12
8
0
0	50	100
h =0.03903
100
h =0.04684
105
10-10
10-10
10-150	50	100
10-150	50 100
4
4
4
Figure 9: General over-parameterized matrix factorization with kX0 kF = 19 kY0kF = 1. The
x-axis represents the number of iterations k; the y-axis represents the value for the norm of Xk
and Yk in the first row, and the value for loss in the second row; the learning rate h for each column
is the same.
5
A.2 Matrix sensing and matrix completion
As we demonstrated, the balancing effect is a nontrivial implicit bias created by large learning rate in
GD, and this was rigorously established for the problem of matrix factorization. Matrix factorization
already corresponds to a class of important problems as we consider arbitrary n and d; however, we
feel balancing is an effect even more general, and thus we now demonstrate it empirically on two
related additional problems, namely matrix sensing and matrix completion.
15
Published as a conference paper at ICLR 2022
x10244
II¾I∣f
[向IF
100
80
60
40
20
0
0	50	100
100
80
60
40
20
0
0	50	100
h =0.00204
0l----------
0 5 10
0	5	10
h =0.00175
h =0.00146
10-15
10-15
10-10
10-10
0	1000 2000	0	1000 2000
0	50	100
100
80
60
40
20
0
0	50 100
h =0.00087
10-15
10-10
h =0.00058
0	1000 2000	0 1000 2000
4
3
2
1
Figure 10: General over-parameterized matrix factorization with kX0 kF = 99 kY0 kF = 1. The
x-axis represents the number of iterations k; the y-axis represents the value for the norm of Xk
and Yk in the first row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x-axis range of the 1st row is shortened to better show the changes of kXk kF
and kYkkF at the beginning of the iterations.
x10138
—IlX疝 F
—恒|卜
15
10
5
0
0	5	10
10 [-----1-----
8
6
4 :
2
0 ------'-----
0	50	100
10 [----1-----
8
6
4
2^-----------
0∣-----1-----
0	50	100
10∣--■--
8\:
6
4
2^_____:
0「	」
0	50	100
10 [-----1----
8
6
4
2
0	， T
0	50	100
10r------'----
8
6
4
2
K__________
0、 ，二
0	50	100
h =0.05714
h =0.0381
h =0.13333
∕ι=0.11429
h =0.09524
h =0.07619
Figure 11: General under-parameterized matrix factorization with kX0 kF = 9 kY0 kF = 1. The
x-axis represents the number of iterations k; the y-axis represents the value for the norm of Xk
and Yk in the first row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x-axis range of the 1st row is shortened to better show the changes of kXk kF
and kYkkF at the beginning of the iterations.
In Figure 13, we consider matrix sensing corresponding to the problem
1m
min	X(bi-hAi,XY>i)2
X,Y ∈Rn×d 2m
Here Ai ∈ R100×100 are generated from element-wise i.i.d. Gaussian; bi ∈ R are generated
from uniform distribution [0, 1]; m = 10; X, Y ∈ R100×6; hU, V i = tr(V >U). The problem is
solved via GD. Experiments in the figure correspond to learning rates chosen from large to small
as ho, 4ho, 3ho, 5ho where ho (the 1st column) is picked near the stability limit. We can see from
the figure that matrix sensing exhibits a similar balancing effect with matrix factorization that larger
learning rate leads to more balanced norms between X and Y .
16
Published as a conference paper at ICLR 2022
x10176
—IIX疝 F
--［向山
20
16
12
8
4
0
0	50	100
20
16
12
8
4
0
0	50	100
20
16
12
8
4
0
20
16
12
8
4
0
0	50	100
20
16
12
8
4
0
0	50	100
h =0.03243
h =0.0278
0∣-----------IJ
0	5	10
h =0.02317
3
5
h =0.0139
h =0.00927
100
10-10
10-5
h =0.01853
105
A--------
0	50	100
x 105
Figure 12: General under-parameterized matrix factorization with kX0kF = 19 kY0kF = 1. The
x-axis represents the number of iterations k; the y-axis represents the value for the norm of Xk
and Yk in the first row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x-axis range of the 1st row is shortened to better show the changes of kXk kF
and kYkkF at the beginning of the iterations.
8
6
4
2
In Figure 14, we consider matrix completion corresponding to the problem
x,γminn×d 2 R(A -XYT 升F，
where A ∈ R10×10 is a low-rank matrix with rank 2; X, Y ∈ R10×2; Pω(U) = (UijIaj)∈ω +
(0)(i,j)∈Ω with sparsity-0.6 where sparsity=(number of non-zero elements)/(number of all elements).
The problem is solved via GD. In the above mentioned figure, the learning rates are similarly chosen
as the above matrix sensing example. Likewise, the decrease of learning rate results in the increase in
the gap between the norms of X and Y .
Both the matrix sensing and matrix completion above hold the same homogeneity property between
X and Y . The “balancing effect” that we proved for matrix factorization is also observed when
and only when the learning rate h is large, in which case the norms of X and Y become close at
the convergence of GD (and yes, GD still converges even though h is large enough such that the
convergence is not monotone.
We feel the techniques invented and employed in this paper can extend to these two cases, but that is
beyond the scope of this paper.
B GD converging to periodic orbit
Consider the objective (1 - xy)2/2. Take step size h = 1.9. Then GD can converge to periodic orbits
with period 2, 3 and 4 respectively in Figure 15.
C Modified equation fails for large learning rates
Consider the objective function f(x, y) = (1 - xy)2/2. Then the GD update is the following
xk+1 = xk + h(1 - xk yk)yk ⇒ xk+1
yk+1 = yk + h(1 - ykxk)xk	yk+1
。：+h(-Vf(Xk ,yk)).
(7)
17
Published as a conference paper at ICLR 2022
h =0.000518
h =0.000414
10-2θ0
10-10
100
100
50
Figure 13: Matrix sensing. The x-axis represents the number of iterations k; the y-axis represents
the value for the norm of Xk and Yk in the first row, and the value for loss in the second row; the
learning rate h for each column is the same.
h =0.01615
h =0.01292
h =0.00969
Figure 14:	Matrix completion. The x-axis represents the number of iterations k; the y-axis
represents the value for the norm of Xk and Yk in the first row, and the value for loss in the second
row; the learning rate h for each column is the same. Note the x-axis range of the 1st row is
shortened to better show the changes of kXk kF and kYk kF at the beginning of the iterations.
By backward error analysis (Hairer et al., 2006, Chapter 9), the modified equation can better
approximate GD than gradient flow and is defined as follows
h
-Vf (x,y) - 2V2f (x,y)Vf(x, y) + O(h2).
(8)
Figure 16 shows the trajectories of the 1st order modified equation of (8) and GD (7) with initial
condition x = 4, y = 10 and h = 0.026. The x-axis represents the time t and for GD, the time point
for kth step is kh. We compare the absolute values of x and y of both methods due to the symmetry
of the global minima xy = 1. As is shown in the figure, even if GD almost converges to the most
balanced minimizer, the solutions of modified equation are still far away from each other, x ≈ 0.1 and
y ≈ 9. Actually, large learning rate h fall outside the convergence domain of the modified equation
which thus is not an appropriate tool for the analysis of norm balancing.
18
Published as a conference paper at ICLR 2022
Figure 15:	Three orbits of period 2, 3, and 4. The blue line are the orbits; the red line is a reference
line of the global minima xy = 1.
10
A PUe X°sφn-e>2n-osq∖/
8	—.-------------
---modified equation X
---modified equation y
6	——GD x
—GD y
4
2
0	2	4	6	8	10	12
Time t
Figure 16: Trajectories: modified equation vs GD
D Proof of Proposition 2.1
Proofof Proposition 2.1. This proposition is a direct consequence of Theorem F.2 and I.3.	□
E Over-parametrized scalar decomposition: stability limit
Consider the objective (1 - xy)2/2. Choose the initial condition to be x0 = 20, y0 = 0.07 and use
GD update. The upper bound of h in Theorem 3.1 is /々+/+4口, where μ = 1. In Figure 17, When
h = 方2+，2+4 (the left one), GD converges; however, when h is slightly larger than this bound, it
blows up. Hence our restriction for h is very close to the stability limit.
F	Overparametrized objective: large learning rate
In this section, we use column vector instead of row vector for sake of better understanding, i.e., our
objective function is (μ - x>y)2∕2.
F.1 Eigenvalues of Hessian
Lemma F.1 (Matrix determinant lemma). Suppose A is an invertible n × n matrix and u, v ∈ Rn
are column vectors. Then
det(A + uv>) = (1 + v>A-1u) det A.
19
Published as a conference paper at ICLR 2022
Figure 17: The loss with slightly different h near its upper bound
Theorem F.2. The eigenvalues of the Hessian of (μ 一 x>y)2∕2 are ±(μ — XTy) repeated n 一 1
times and 2(kχ∣∣2 + ∣∣yk2 ± vz(kxk2 + kyk2)2 + 4(μ 一 XTy)2 — 8(μ 一 XTy)xτy). Especially, at
XTy = μ, the eigenvalues are ∣∣x∣2 + ∣∣y∣2 and 0 repeated 2n _ 1 times.
Proof. Consider the objective f (x, y) = (μ — XTy)2/2. Its Hessian is the following
H
Then to calculate the eigenvalues, we also need
λI2n - H =J	λln v (μ —
(μ — X1 y)In
By Lemma F.1, we have for invertible B
det(B— (Xy)(Xy)T
=B - (X)(X)T
1— (Xy)TB-1 (Xy))detB.
Since (μ — XTy)In and λIn commute, we have
det B = det((λ2 — (μ — XTy)2)In) = (λ2 — (μ — XTy)2)n
By the formula for inversion of block matrix, We have for λ2 — (μ — XTy)2 = 0,
_____λ_____τ _	μ-x>y	τ ∖
λ2-(μ-x>y)2/n	— λ2-(μ-x>y)2 'n ∖
_____μ-χ>y_ I __________λ_____I .
λ2-(μ-x>y)2 In	λ2-(μ-x>y)2 In )
Then combining all these and by the continuity of characteristic polynomial, we obtain the following
expression for all λ
det(λI2n — H) = (λ2 — (μ — XTy)2)n 1(λ2 — λ(xτ X + yTy) — (μ — XTy)2 + 2(μ — XTy)XTy).
When XTy = μ, it becomes
det(λI2n — H) = λ2n-1 (λ — (XTX + yT y)).
□
F.2 Local non-convexity near the global minimum
Consider the region D = {δ > XTy — μ > 0}, a small neighbourhood of the global minima μ = XTy
for some δ > 0. Consider two points [x, y], [x, y] ∈ D with X = cX and y = y/c for c > 0 a constant.
20
Published as a conference paper at ICLR 2022
Then
f(x,y) + hVf (x,y), [x - χ,y - y]i = (μ - X>y)2∕2 + (μ - χ>y)(y>(χ - x) + χ>(y - y))
=(μ — X>y)2∕2 + (2 — C — 1∕c)(μ — XTy)XTy
≥ (μ — XTy)2∕2 = f(χ,y).
This contradicts the definition of convexity. Therefore the objective is not locally convex.
F.3 A REVIEW OF TRADITIONAL CONVERGENCE ANALYSIS OF GD UNDER L-SMOOTHNESS
For general function f , GD is defined as follows
Xk+1 = Xk — hVf(Xk).	(9)
Theorem F.3. If f : RN → R is L—Smooth, then with h < L, GD converges to a stationary point.
Proof. Let min f = f*.
f(χk+ι) ≤ f(xk) + hVf(xk),χk+ι — Xki + 2∣∣χk+ι — Xk∣∣2
=f (Xk) — h(1 — L2 h)∣Vf(xk )k2.
Then
N
X∣Vf(Xk)∣2≤
k=1
≤
ho⅛) (f(x0)- f (XN))
W-Ih (f(x0)- f *).
Therefore limk→∞ ∣Vf(xk)∣2 = 0, i.e., GD converges to a stationary point.
□
G	Proof of Theorem 3.1, Theorem 3.2, and Corollary 3.3
In this section, we use column vectors for x and y instead of row vectors for better understanding.
Then the objective function is 2(μ — xτy)2.
The following Theorem G.1 and G.2 are the main theorems of the over-parametrized scalar case.
Next, for sake of convenience, let u2k = ∣xk ∣2 + ∣yk ∣2.
Theorem G.1 (Main 1). Let h =。,+4.口. Assume c ≥ 1 and U2 > 8μ. Then GD converges to a
point in {(x,y) ： ∣∣x∣∣2 + IlyIl2 ≤ h, xτy = μ} exceptfor a Lebesgue measure-0 set of (x0,y0).
Proof. By Lemma G.7 and Lemma G.11, the theorem holds.	□
Theorem G.2 (Main 2). Let h = 8.：4c. =(2+。)*. Assume C ≥ 1 and u2 ≤ 8μ. Then GD
converges to a point in {(x, y) ： ∣x∣2 + ∣∣y∣2 ≤ 2, xτy = μ} exceptfor a Lebesgue measure-0 set
of (x0, y0).
Proof. For the measure-0 set, by Lemma G.5, the set of points converging to {∣x∣2 + ∣∣y∣2 > ∙2 }
is measure-0; by the proof of Lemma G.11, the set of points converging to the origin is measure-0;
also {u02 = ∣x∣2 + ∣y∣2 = 0} is a hyperplane and thus is measure-0. Hence the set of all the initial
conditions not converging to {∣x∣2 + ∣∣y∣2 > 2,xτy = μ} is measure-0.
Since u2 ≤ 8μ, if u2 > ∙2, by Lemma G.7, it will decrease to Uk ≤ 2 for some k. Then by
Lemma G.11, we have the convergence.	□
With the above two theorems, we can prove all the theorems and corollary in Section 3.
21
Published as a conference paper at ICLR 2022
Proofof Theorem 3.1 and Theorem 3.2. From Theorem G.1, h = ^+4c. for C ≥ 1 which implies
h ≤ .2+4* for all u2 > 8μ. Similarly, for Theorem G.2, h ≤ /. When u2 = 8μ, u2++4μ =
8μ+4μ = 31μ; when u0 > 8〃，u⅛μ < 31μ; when u2 < 8〃，u0⅛ > 31μ ∙ Also, allthe Iimit
points are in {∣∣χ∣∣2 + ||y『 ≤ h}. Then we can get Theorem 3.1 and 3.2 where the second
inequality of Theorem 3.2 is because at the global minimum x>y = μ and also ∣∣x - y∣∣2 =
∣∣xk2 + ky∣2 - 2x>y.
ProofofCorollary 3.3. For ∣x>yo — μ∣ < δ, ∣xo∣2 + ∣∣yo∣∣2 > 8μ, we have
χ>yo < μ + δ ⇒ kχo - yo『=llχo∣l2 + ∣∣yok2 - 2x>yo > h - 4μ - 2(μ + δ) = h - 6μ - 2δ∙
Also, from Theorem 3.2, ∣x - y∣2 ≤ 2 - 2μ and h - 2μ < 4 - 6μ - 2δ < 4 - 8μ. We then obtain
l∣χ - y∣2 ≤ 2 l∣χo - yok2 + 2μ.	口
We will divide the proof of the two main theorems into two phases: (1) when 2 <xk+ yk < 4 ,we
would like to show that GD escapes to a smaller ball Xk + y2 ≤ 2 except for a measure-0 set; (2)
once GD enters Xk + y2 ≤ 2, it will converge to the global minimum inside this region except for a
measure-0 set.
G.1 Phase 1: h2 <uk < 4
We first deal with the situation where GD just converges in this region by showing that such points
form a null set. This is stated in Theorem G.5.
Theorem G.3 and Corollary G.4 are the preliminary of proving Theorem G.5. Also Corollary G.4 is a
direct result of Theorem G.3.
Theorem G.3. Let f : RN → RM and f ∈ C 1 . If the set of critical points of f is a null-set, i,e.,
L({x ∈ RN : Vf (x) is not invertible}) = 0,
then L(f -1 (B)) = 0 for any null-set B.
Proof. Let G = {|Vf | 6= 0} and G is an open set, where |Vf| denotes the determinant of Vf. By
implicit function theorem, we have G ∩ f-1 (z) is a (N - 1)-submanifold in C1 .
Consider G ∩ {f ∈ B} = Sz∈B G ∩ f-1 (z), where L(G ∩ f-1 (z)) = 0 from the above discussion.
Hence, if B is countable, L(G ∩ {f ∈ B}) = 0.
When B is uncountable, using co-area formula from geometric measure theory, we have for any
bounded open ball Br,
g |Vf |dL =	g (X)dLN -1 (X)dz =	g(X)dLN -1 (X)dz.
G∩Br	R f-1(z)∩G∩Br	B f -1 (z)∩G∩Br
Let g be the indicator of Sz∈B G ∩ Br ∩ f-1 (z). Then since B is a null-set, we have
g (X)dLN -1 (X)dz
(z)∩G∩Br
dLN -1 (X)dz = 0
(z)∩G∩Br
⇒	g |Vf |dL = 0.
G∩Br
Then g|Vf| = 0 a.e. in G ∩ Br. Also, |Vf| 6= 0 a.e.. Therefore, g = 0 a.e. in G ∩ Br, i.e.,
L [ G∩Br ∩f-1(z) = gdL = 0.
z∈B
22
Published as a conference paper at ICLR 2022
Since we can find a sequence of bounded open ball Br, s.t., RN = Sr∞=1 Br, and also L(Gc) = 0,
we have
L(f-1(B)) ≤L	[ G∩f-1(z)	+	L(Gc)	=L	[∞	[ G∩Br∩f-1(z)	+	L(Gc)	=0.
z∈B	r=1 z∈B
□
Corollary G.4. Let ψ : R2d → R2d be the GD iteration map, i.e., ψ(x, y) = [x + h(μ 一 x>y)y, y +
h(μ 一 x>y)x]>. Then if
L({det(Dψ) = 0}) = 0,
then L(ψ-1 (B)) = 0for any null-set B.
Lemma G.5. Given h = -'+4.. and Uk > 2, GD will not converge to the points in {kx∣∣2 + ∣∣yk2 >
h2, x>y = μ}, except for a measure-0 set.
Proof. For d = 1,
det(Dψ) = 1 — hx2 — hy2 — 3h2x2y2 + 4h2xyμ —后μ2 = pι(x, y)…Pm(x, y) = 0,
where pi (x, y) is irreducible polynomial and pi (x, y) = 0 is a co-dimensional-1 manifold. Then
{(x, y) : det(Dψ) = 0} = Sim=1 {pi (x, y) = 0} are measure zero, i.e., L({det(Dψ) = 0}) = 0.
Similarly for d > 1, we also have L({det(Dψ) = 0}) = 0.
From the GD iteration, we have
x>+1yk+1 — μ = (Xkyk 一 μ) ∙(I 一 h(IlXkk2 + kykIl2) 一 h2χ>yk(μ 一 Xkyk)).
Then let B = {(x, y) : 1 — h(∣x∣2 + ∣∣y∣2) — h2x>y(μ — x>y) = 0}. For any (x, y) ∈ B, let
[x+,y+]> = ψ(x,y). Then x+y+ = μ. Similarly, L(B) = 0. By Corollary G.4 L(ψ-1(B)) = 0
and then L(ψ-n(B)) = L(ψ-1 ◦…。ψ-1(B)) = 0. Let ψ-0(B) = B and G = S∞=0 ψ-n(B).
Hence
∞∞
[ ψ-n(B)	≤ X L(ψ-n(B)) = 0.
i=0	i=0
Moreover, for any 0 < e < μ, assume ∣x>yk — μ∣ < e. When Uk ≥ 2 + eh(μ + e),
lx>+1yk+1 一 μl = |x>yk 一 μH1 一 huk 一 后Xky(μ 一 XkIyk)|
=|x>yk — μl •(—1 + huk + BXkyk(μ — x>yQ)
≥ |x>yk — μ∣∙ ( — 1 + h(h + eh(μ + e)) — h2(μ + e)e)
> |x>yk - μL
Hence, {∣∣x∣∣2 + ∣∣y∣∣2 > ∙2, x>y = μ} is not the limit of this GD map ψ, except for the measure-0
set G.
Next we show that GD will be bounded inside {∣∣x∣∣2 + ∣∣y∣∣2 < 4}.
Lemma G.6. Given h = 2+4c., thenfor 0 ≤ k < min{k : Uk ≤ ∙2}, we have Uk ≤ 4 — 3μ < 4
for all k.
Proof. First u2 = 4 — 4cμ ≤ 4 — 4μ < 4. Then if x>yo > μ or x>yo < — 4hμu02, from
Lemma G.8, U < u0 < 4. If — 4-U02 ≤ x>yo < μ, from Lemma G.9 and its proof, U < u0 < 4
and uι ≤ u0 + c ≤ u2 + μ ≤ 4 — 3μ < 4. Therefore, iteratively We have Uk < 4.	□
23
Published as a conference paper at ICLR 2022
Therefore, without loss of generality, we can just assume u2k ≤ u20 for a fixed kth iteration that we
need to analyze because for every two step there exists an ith iteration such that ui2 ≤ u20 and we can
choose k = i to do the analysis.
Lemma G.8 and G.9 describe one-step or two-step decay of u2k , which lead to the primary result,
Lemma G.7, in phase one. Moreover, the proof of Lemma G.7 contains a finer characterization of u2k
making it possible to end phase one and enter phase two.
Lemma G.7. Given h =说+4：* for C ≥ 1, GD will enter {kx∣∣2 + ∣∣yk2 ≤ ∙2} except for a
measure-0 set of initial conditions.
Proof. By Lemma G.6 and its discussion, assume without loss of generality 2 < uk ≤ u2. From
Lemma G.8 and G.9, the region where the decrease of u2k may be small is when xk, yk are close to
>	>	hμu2
Xkyk = μ or Xkyk = - 4-huk⅛ ∙
k
Since
χ>+1yk+1 — μ =(Xkyk — μ)(1 — huk — h2x>yk (μ — Xiyk)),
consider Sk = 1 一 hu2k 一 h2x>yk (μ 一 x>yk). From the proof of Lemma G.9, We know When
x>yk = 一 4hμUk2, Sk < -1+2h2μ2 < 0. Therefore,thereexists δ > 0, s.t., for all Xk ,yk ∈ {∣x>yk +
- uk
τ~μhUk⅛ | <δ, h < uk ≤ u2}, sk < 0∙ Then X>+iyk+i > μ. Hence, when -δ ≤ x>yk + jμuk2 < 0,
- uk	- uk
we will skip this step and consider the decrease of the next step with X>+1yk+1 > μ. Also, there
exist βι = βι(δ) > 0, s.t., when Xkyk + 4-h⅛ < 一δ,
-	uk
uk + 1 一 Uk = h(μ 一 X>yk)((4 — huk)Xkyk + hukμ) < 一为，
From the proof of Lemma G.9, when 一 ?：k ≤ X>yk ≤ 0, for β2 = max{hμ2(4 — hu2)(1 —
-	uk
2h2μ2)2, hμ2(8 - h(2u2 + C))} > 0,
uk+2 一 Uk < maχ{-h(M - Xkyk)μ(4 一 huk+I)(I - 2h2μ2)2, 一h(M - Xkyk)2(8 - h(Uk + uk+1))}
≤ 一 β2∙
Fix an small e in 0 < e < μ. When x>yk ≥ μ + e, from the proof of Lemma G.8, we have for
β3 = 4hμe > 0,
uk+ι - Uk ≤ 4hμ(μ - XkIyk) ≤ -4hμe = -β3.
When 0 < X>yk ≤ μ - e with e > 0, from the proof of Lemma G.9, we have for β4 = he2 (8 -
h(2u2 + μμ)) > 0,
uk+2 - Uk ≤ -h(μ - Xkyk)2(8 - h(Uk + uk+l)) ≤ -β4.
When ∣X>yk - μ∣ < e, assume ∣X>yk - μ∣ = ek. In this case, we have Sk < 0 meaning GD oscillates
around x>y = μ. Also, if 0 < x>yk < μ and Uk > h, Sk = 1 - hukk - h2X>y(μ - X>y) < -1,
i.e., if GD is in 0 < X>y < μ, then we have ∣X>+1yk+1 - μ∣ > ∣X>yk - μ∣. Hence we only
need to focus on the other side which is X>yk > μ. From Lemma G.5, if Uk ≥ h + eh(μ + e),
∣X>+1yk+1 - μ∣ > ∣X>yk - μ∣. Then within finite steps (note all these steps satisfy either one-step or
two-step decrease of u2k ; we ignore these steps only because the decrease maybe small), we will have
either ∣X>yk - μ∣ keeps increasing or UK < h + eκh(μ + eκ) (here we also consider XKyκ > μ).
For the former one, the decrease of u2k for each step will be lower bounded away from 0; for the latter
one, from the discussion above, uK+1 - UK ≤ -4hμeκ ⇒ uK+1 ≤ Ih.
From Lemma G.5, we know GD will not terminate in finite steps except for measure-0 set. From all
the discussion above, we have that u2k decreases by a constant for either one-step or two-step except
and hence GD will enter ∣∣x∣2 + ∣∣y∣2 ≤ h except for a measure-0 set.	□
24
Published as a conference paper at ICLR 2022
Lemma G.8. Given h =说:平 and Uk < h, when Xkyk > μ or x>yk < 一 [Y；k, we have
ul+ι 一 uk < 0∙
Proof.
uk+ι 一uk = h(μ-χ>yk)((4一 hukIyk + hukμ) = h(μ-X^k)(4x>yk + huk(μ-X^k))
If x>yk > μ,by Uk < 4,
Uk+1 一 Uk = h(μ - Xkyk)(4x>yk + huk(μ 一 Xklyk)) ≤ 4hμ(μ - x>yk) < 0∙
If x>yk < 一 4μ⅛ ⇒ x>yk < μ, then,
uk
uk+ι 一uk = h(μ一χ>yk)((4 一 hukIyk+ hukμ) < 0∙
□
Lemma G.9. Given h = 说二乎,C > max{ ɪ, 2hμ}, c > 0, and Uk ≤ u0, when 一 £：k ≤
x>yk < μ, then uk+2 - Uk < 0.
Proof. For every Uk, there exist a constant Ck > 0, s.t. h =	，以*. Since h < 姆 ≤ U < h, we
have c ≤ Ck < 仓∙ Since
x>+1yk+1 一 μ = (X>yk - μ)(i - hUk 一 h2Xkyk(μ - Xkyky) = (X>yk - μ)sk,
then Sk = 1 - h〃k - h2X>yk (μ - X>yk) is bounded by the value at xkyk
i.e.,
hμu2 rιrktQI] — ll
-4-hU2 or Xk yk = μ,
K,
Sk = 1 - hnk - h2X>yk (μ — Xkyk) ≤ max(1 - h〃k +
Since Uk > h, we have 1 - h〃k < -1. For the other one, since Uk
X,ι - hUk
> 2 and Ck < 2⅛,
…Uk+:=(1 Ck 3ck )+⅛ μck μ <-ι+2M
where C > ":Yh-h卜.This is because either U > 8μ ⇒ hμ < 泰 or h ≤ 泰 and then
”[Yh-h2 ( < 1 ≤ c. Also note here this is C not Ck; this value -1 + 2h2μ2 is achieved when
Uk = 2/h and Ck = h¾√染 or 端.
Also, when 0 ≤ Xkyk < μ,
sk = 1 - hUk - h2x> yk (μ - Xkyk) ≤ 1 - hUk < -1∙
We then prove 4 - hUk+1 > 0. When Xkyk < μ,
Uk+i - Uk = h(μ - Xkyk)((4 - hUk)X【yk+ hUkμ)∙
The maximum is achieved at Xkyk = 4≡⅛μ, i∙e∙, Uk+i- Uk ≤ 食 ≤ C.HenCe 4 - hUk+i >0
when c > 1/2.
Since Xkkyk < μ and Sk ≤ 0,
一 2 一 2	一 2 一 2 ,一 2 一 2
Uk+2 - Uk = Uk+2 - Uk + 1 + Uk + 1 - Uk
=h(μ - Xk+iyk+i)((4 - hUk+i)Xk+iyk+i+ hUk+i〃)
+ h(μ - Xkyk)((4 - h"k)X[yk + h"kμ)
=h(μ - Xkyk)(((4 - hUk)X[yk + h"kμ)+ Sk(hUk+iμ + (4 - hUk+i)(sk(Xkyk - μ)+ μ)))
=h(μ -Xkyk)(X[yk(4 - hUk)+ sk(4 - hUk+J(Xkyk -μ)+ hUk〃 + Sk(hUk+i〃 + (4 - h"k+ι)μ))∙
25
Published as a conference paper at ICLR 2022
When 0 ≤ x>yk < μ, s < -1, then
uk+2- Uk ≤ -h(μ - x>yk)2(8- h(Uk + uk+1)) <0
When -4hμu⅛ ≤ x>yfc < 0, Sk ≤ 0, then for C ≥ 2hμ,
- uk
uk+2 - Uk < h(μ - x>yk)μ(-4 + huk + 8h2μ2 - (4 - huk+I)(I - 2h2μ2)2)
< -h(μ - x>yk)μ(4 - huk+I)(I- 2h2μ2)2 < 0
where this bound is achieved by taking Sk = -1 + 2h2μ2 and x>yk = 0.	□
G.2 Phase 2: Uk ≤ 2
In this part, we will show the convergence of GD in Lemma G.11.
There are two convergence patterns: (i) transversal convergence, i.e., oscillating around the valley;
(ii) unilateral convergence, i.e., converging from one side of the valley.
The key point of the proof of pattern (i) is to analyze the change of Sk = 1-huk - h2χ> yk (μ-χ>yk).
We know x>+1yk+1 - μ = Sk(Wyk - μ), i.e., Sk measures the change of the loss. If for some
constant K > 0 we have ∣Sk | < 1 ∀k ≥ K such that ∣χ>yk - μ∣ is guaranteed to decrease to 0, then
the convergence follows. Moreover, we will need to analyze S2k and S2k+1 separately because in this
case Sk < 0 and χ>yk - μ changes sign at each step.
For pattern (ii), we will show that the trajectory of GD is bounded in a subset of this region |Sk | < 1
that guarantees the decrease of the loss.
Before presenting our main result in phase 2, we first show a boundedness theorem when GD enters
this phase.
Lemma G.10. Once GD enters {||x『+ ||y『≤ 2}, it will stay bounded inside {||x『+ ∣∣yk2 ≤
h + 2μ} and re-enter {∣x∣2 + ∣∣y∣2 ≤ 2 } within finite steps where the numberof such steps do not
depend on the number of iteration.
Proof. If at step K, UK ≤ h, then from Lemma G.15, uK+ι ≤ h + μ and it returns to phase 1.
From the proof of Lemma G.9, UK+2 ≤ UK+r + μ ≤ Ih + 2μ and we know either UK+？ < uK+ι or
uK+3 < uK+ι and so on until it re-enters U ≤ h for some i ≥ K. Therefore, all the Uk ≤ h + 2μ
once GD enters {∣χ∣2 + ∣∣y∣2 ≤ h}.	□
Then by Lemma G.10, we can always pick a kth iteration such that Uk ≤ h. Also, when u2 > 8μ,
we have hμ < ɪ. Together with the choice of h =0+：)* when 0 < u0 ≤ 8μ, we have hμ ≤ 3.
Then we obtain the following convergence of GD inside phase 2.
Lemma G.11. Given hμ ≤ 3, ifGD enters {∣x∣2 + ∣∣y∣2 ≤ 2}, it converge to x>y = μ inside this
region except for a measure-0 set of initial conditions.
Proof. First, from Corollary G.4, we know the set of points converging to (0, 0) in finite steps is
measure 0. For all the other points, we have the following discussion.
If Sk = 0, then x>+1yk+1 = μ, i.e., it converges.
If x[yk < —μ and h + 2hμ2 < u2k ≤ h, then by Lemma G.14,
uk+ι - Uk = h(μ - Xkyk)((4 - huj.)x>yk + hu2μ) ≤ 2h(μ2 - (x>yk)2) < 0.
Hence for δι > 0, when -μ - δι < x>yk < -μ, we have x>+ιyk+1 > μ and we will skip this step
and directly consider the (k+1)th iteration; when x>yk ≤ -μ-δι,姆+厂姆 ≤ -2h((μ+δι)2-μ2).
Namely when x> yk < -μ, either uk+ι is Some constant away from Uk or we can ignore the decrease
in this step and directly look at the next one with x>+ιyk+ι > μ.
26
Published as a conference paper at ICLR 2022
If x>yk < -μ and Uk ≤ 1 + 2hμ2, then
uk+ι - uk = h(μ - x>yk)(4x>yk + huk(μ - Xlyk))
≤ h(μ - x>yk)(4x>yk + (1 + 2h2μ2)(μ - x>yk))
≤ -4hμ2(1 — 2h2μ2) < 0.
If x>yk > 3μ,then u>+i - Uk ≤ 2h(μ2 - Xky2) < -16hμ2. Also, when Uk ≤ 6μ, Xkyk ≤ 3μ.
Next, for the rest of the region, we first consider 1 + 2hμ2 < u> ≤ 2 (by Lemma G.10, we are
always able to find Uk ≤ 2). In this region, by Lemma G.18, we know Sk < 0, i.e., (χ>yk -
μ)(x>+1yk+1 - μ) < 0. We divide the whole region into two parts: Xkyk > μ and x>yk < μ.
Therefore all the Xk+2i should be on the same side for i such that 1 + 2hμ2 < uk+2i ≤ 2 ∙
If -μ ≤ Xkyk < μ and Sk ≤ -1, by Lemma G.16, we have uk+2 一 Uk ≤ -4h(1 - hμ)(μ -
Xk>yk)2 < 0. Hence when GD does not converge, i.e., {(Xk+2n, yk+2n)}n∞=0 does not converge , U2k
will keep decreasing with Ulk+2i ≤ 2 for all K+2i with i ≥ 0 such that Sk+2i ≤ -1∙ Moreover, there
exists N > 0 s.t. sk+2n > -1 because Sk = 1 - h〃k - h2X>yk(μ -X>yk) and if Xk+2i,yk+2i isthe
first iteration to leave this region, then it has to satisfy X>+2iyk+2i < -μ which implies sk+2% > -1,
and then it follows from Lemma G.13 and G.12 just as the following two paragraphs of discussion; if
it never leaves this region and GD is not converging, i.e., ∣μ - X>yk | has a lower bound, then Uk+?%
will keep decreasing until sk+2n > -1 (from the proof of Lemma G.12, if μ - X>yk is very small,
then sk+i < Sk meaning both sides are blowing up and therefore ∣μ - X>yk | has a lower bound if
Sk ≤ -1).
If -μ ≤ X>yk < μ and Sk > -1, from Lemma G.12 and Lemma G.13, similar to the previous
discussion, there exists N > 0, s.t., for all k > Ni, Sk > min{-1 + co, -1 + cι(μ - X>yk)2} for
some co, c1 > 0, namely ∣μ - x>yk| strictly decreases. Then it will either converge in 1 + 2hμ2 <
∣∣X∣∣2 + Ilyll2 ≤ 2 or enter ∣∣x∣∣2 + ∣∣y∣∣2 ≤ 1 + 2hμ2.
If μ < x>yk ≤ 3μ, from the definition of Sk, we have Sk > -1. Also by Lemma G.13, we know
Sm > min{-1 + co, -1 + c1(μ - X>yk)2} for all m > k. Hence similarly, it will either converge
or enter Uk ≤ 1 + 2hμ2.
Then we consider Uk ≤ 1 + 2hμ2. From Lemma G.17 and Lemma G.20, we know |sk| < 1
and will be away from 1 by a constant in this area. We will show that GD stays bounded in the
converging domain. If Sk ≤ 0, it follows from the previous discussion and the proof of Lemma G.17
and Lemma G.20. If Sk > 0, when the trajectory is in {∣X>y∣ > μ}, from Lemma G.14, we have
uk+1 < Uk; when it is in {∣X>y∣ < μ}, then ∣xk+1 - yk+1∣ < |xk - yk | from Lemma G.19. Hence
the trajectory will stay inside the monotone decreasing region. Next, since ∣μ - X>yk | monotonically
decreases and is lower bounded by 0, we have that ∣μ - X>yk | converges. If ∣μ - X>yk | converges to
C > 0, then min{-1 + c0, -1 + c1(μ - X>yk)2} < s> < 1 -。2 for some co, c1, c2 > 0 meaning
∣μ - X>yk | does not converges to C. Contradiction. Therefore, ∣μ - X>yk | converges to 0.	□
Lemma G.12. When -μ ≤ X>yk < μ and 1 + 2hμ2 < uk ≤ 2 + 2μ ,if Sk > -1, then
sk+2 > min{-3 - h 4 , - 5h2” , -1 + h2(μ - Xkyk)2} > -1 ,for 2h2μ2 < 1 ∙
Proof. From previous statement, Sk < 0 when Uk > 1 + 2hμ2. For Sk = 1 - h〃k - h2X>yk(μ -
X>yk), let e = μ - Xkyk. Then X>+1yk+1 - μ = -Ske.
sk+1 = 1 - hUk+1 - h2X>+1yk+1(M - X>+1yk+1) = sk - h2eμ(3 + Sk) + h2e2(3 + Sk - hUk)∙
Hence if e is very small, we have sk+1 < sk. Also, Xk+2yk+2 - μ = sk+1 (X>+1yk+1 - μ)=
-sk+1Ske. Moreover, u>+1 ≤ h + 2μ by Lemma G.6.
27
Published as a conference paper at ICLR 2022
sk+2 = 1 — huk+2 — h2xk+2Vk+2(μ — xk+2Vk+2)
=sk — h%μ(3 + 4sk + sk sk+ι) + h2c2(3 + Sk — hu k + Sk (3 + sk+ι — huk+ι)))
≥ Sk — h2eμ(3 + 4sk + Sksk+ι) + h2e2(1 + (1 + sk+ι)sk)
sfc+ι=μ∕(2sfc e)	h2μ2	、2 ，C , 、	,2 2/.	2、
≥ Sk---------4------h e(3 + 4sk) + he (1 + Sk)
If 3 + 4Sk ≥ 0, then
Sk+2 ≥ Sk----4--h2e(3 + 4Sk ) + h2e2(1 + Sk )
e=(3+4sk)μ∕(2(1+sk))	h2μ2	h2μ2(3 + 4Sk )2
≥	Sk - ^4	4(1 + Sk)
Sk=O or Sk=_3/4	3	h2μ2	5h2μ2、
≥	mιn{,} > -1.
-	1 4	4 7	2 ʃ
If 3 + 4Sk < 0, i.e., —1 < Sk < - 4 ,then
Sk+2 = Sk— h2eμ(3+ 4Sk+ SkSk+i)+ h2e2(3 + Sk— huk+ Sk(3 + Sk+i— hUk+Iy)
=Sk — h2eμ(3 + 4Sk + Sk(Sk — h2eμ(3 + Sk) + h2e2(3 + S2 — huk)))
+ h2e2(3 + Sk — huk + Sk (3 + Sk+1 — huk+1))
=Sk — h2μe(Sk(3 + Sk) + 3 + Sk) — h4μe3Sk (3 + Sk — huk)
+ h%2(3 + Sk — huk + Sk (3 + sk+i — huk+ι) + h^μ^Sk (3 + Sk)).
When —1 < Sk < \-累卬,Sk — h2μe(sk(3 + Sk) + 3 + Sk) > —1. Since e ≤ 2μ, We have
⅛⅛μ ≥ ⅛≠ ≥ - 4 for 9 h2μ2 < 1∙ Also,
3
一h μe Sk (3 + Sk — huQ > ~ h μe > 0,
h%2(3 + Sk — huk + sk(3 + sk+ι — huk+ι) + h^μ^Sk(3 + Sk)) > h%? > 0.
Hence
sk+2 > —1 — h4μe3sk(3 + Sk — huk)
+ h%2(3 + Sk — huk + Sk(3 + sk+ι — huk+ι) + h2μ2Sk(3 + Sk)) > —1 + h%2
□
Lemma G.13. When x>yk > μ and 1 + 2hμ2 < 嫄 ≤ 2 + 2μ, if Sk > —1, then Sk+ι > Sk and
sk+2 > min{-1 + h2(μ — x[yk)2( 1 — 2h2μ2), — 2 — 2h4(μ — x>yk)2μ2} > — 1 ,for 8h2μ2 < 1 ∙
Moreover, if ujk+1 ≤ 2, Sk ≤ —1, and Sk+ι > — 1, then Sk+2 > Sk + h2(μ — x>yk)2(1 - 8h2μ2).
Proof. From previous statement, Sk < 0 when Uk > ɪ + 2hμ2. Without loss of generality, we can
just assume Uk < 3 (otherwise 遍+？ < 3 and we can use this as our starting point). For χ>yk > μ,
Sk = 1 — huk — h2x>yk(μ — x>yk). Let x>yk - μ = e. Then
sk + 1 = 1 - huk + 1 - h2x>+ιyk+1(μ - x>+ιyk+1) = sk + h2eμ(3 + sk) + h2e2(3 + Sk - huk) > sk,
where Sk > —3 in this region. Also, x>+2yk+2 - μ = Sk+1Ske.
Then for Sk ≤ —1, Uk ≤ 2, and Sk+1 > —1,
sk+2 = sk + h2eμ(3 + 4sk + sksk+1) + h2e2(3 + Sk - huk + Sk(3 + Sk+1 - huQi))
=Sk + h2eμ(3 + 4sk+ι + sk+J + h2e2 [(3 + Sk — huk )(1 — 4h2eμ — h2eμSk+1)
+ Sk(2 - huk+1) + Sk(1 + sk+1) - h2μ2(4 + sk+1)(3 + sk)]
> Sk + h2e2(1 — 8h2μ2)
28
Published as a conference paper at ICLR 2022
where the inequality is achieved by s⅛+ι > -1,uk ≤ 4, uk+1 ≤ 2, 8h2μ2 < 1,e ≤ 2μ.
For Sk > -1,
Sk+2 = Sk + h2eμ(3 + 4sk + Sk) + h2e2 [(3 + Sk — hu2t)(1 + Skh2eμ)
+ Sk(3 + Sk+1 - huk+1) + hμSk(3 + Sk)]
> Sk	+ h2eμ(3 + 4Sk +	Sk) + h2e2，k(1 + 2s∕2μ2) + Sk + h2μ2Sk(3 + Sk)]
=Sk	+ h2eμ(3 + 4Sk +	Sk) + h2e2 [2Sk + h2μ2Sk(3 + 2^k + Sk)]
When	Sk	> -1,	We have Sk + h2eμ(3 + 4Sk + Sk) > -1 and h2eμ(3 + 4Sk + Sk) > 0.	Since
8h2μ2	<	1, 2Sk	+ h2μ2Sk (3 + 2Sk	+ Sk) ≥ 2 — 3h2μ2 > 0 for —1 < Sk ≤ — 2. For	— 2	<
Sk ≤ 0, 2Sk + h2μ2Sk(3 + 2Sk + Sk) ≥ h2μ2Sk(3 + 2Sk + Sk) ≥ — 2h2μ2 > — 2. Hence
Sk+2 > min{-1 + h2e2(1 — 2h2μ2), — 2 — 2h4e2μ2} > —1.
□
Lemma G.14. Given Uk ≤ 2, when ∣x>yk | > μ,姆十1 — Uk < 0.
Proof.
uk+ι - uk = h(μ - xIykK4xIyk+ huk(μ - Xkyk))
=h(μ — x>yk)4x>yk + h2uk(μ - XIyk)2
≤ h(μ — x>yk)4x>yk + 2h(μ — Xkyk)
=2h(μ2 — (x>yk)2) < 0
□
Lemma G.15. Given Uk ≤ 2, u，i — Uk ≤ μ.
Proof.
uk+ι - uk = h(μ - XIyk)(4χ>yk+ huk(μ - Xkyky) ≤ 44hhu2,
where the maximum is achieved at X>yk = Du μ. When h = -ɪ4-, from the proof of
k yk	4—huk	u2+4cμ,
Lemma G.9, uk+i — Uk ≤ μ. When h =(2+：)* and in this case Uk ≤ 2 = (1 + Cμ), uk+i — Uk ≤
7(2 + c) μ < μ.	口
Lemma G.16. When —μ ≤ X1yk < μ, if Sk = 1 - huk - h2X>yk(μ - X1yk) ≤ —1, uk+2 - Uk ≤
—h(4 — 4hμ)(μ — X>yk)2 < 0.
Proof.
uk+2 - uk = h(μ - X>yk)(((4 - huk)X>yk+ hukμ)+ Whuk+ιμ +(4 - huk+i)(S(X[yk - μ) + μ)))
=h(μ - X>yk ) (-(4 - huk + s2(4 - huk+1))(μ - X>yk) + 4(1 + s)-
≤ -h(8 - h(uk + uk+1 ))(μ - X>yk)2
≤ -h(4 - 4hμ)(μ - x>yk)2 < 0
where the first inequality is because when Sk = -1 it has the maximum value; the second is from
Lemma G.10.	口
Lemma G.17. When Uk ≤ 2, then Sk < 1 and will be awayfrom 1bya COnStant
29
Published as a conference paper at ICLR 2022
Proof.
2
Sk = 1 - huk - h2x>yk(μ - x>yk) ≤ 1 - huk + 后专(μ +
where the equality is achieved when Xlyk = - Uk.
Since 0 < uk ≤ 2,
Sk ≤ 1 - huk + h2 u2k
≤ max{1, hμ} < 1.
Actually this Sk will be away from 1 by a constant. This is because when u2k are close to 0, Sk will be
close to 1; however, Uk will be increasing because of the decrease of ∣μ - χ>yk | and thus Sk will
decrease accordingly.	□
Lemma G.18. When Uk > 1 + 2hμ2 and -μ ≤ XIyk ≤ 3μ, (x>+1yk+1 - μ)(x>yk - μ) < 0,
meaning it oscillates around x>y = μ.
Proof.
1 — huk — hxkyk(μ — Xkyk) < 1 — h( — + 2hμ2) + 2hμ = 0
h
□
Lemma G.19. When -μ ≤ x>yk < μ and hμ ≤ 1, we have ∣Xk+ι — yk+ι∣ < |xk — yk|.
Proof.
∣χk+ι - yk+ι∣ = ∣χk - yk| |1 - h(μ - Xkyk)|.
Since hμ < 1, then
—1 < 1 — 2hμ ≤ 1 — h(μ — Xkyk) < 1.
□
Lemma G.20. When Uk ≤ ɪ + 2hμ2 and hμ ≤ 1, Sk ≥ — 1.
Proof. By hμ ≤ 3,
1	- huk -彦武期认出-X>yk) ≥ -2h2μ2 —4μ~ ≥--------≥ - 4.
□
H Proof of Theorem 4.1, Theorem 4.2, and Theorem 4.3
The proof in this section mainly follows the strategy of the scalar case. More precisely, all the
expressions, e.g., ukk+ι and X>+1yk+1 - μ, in this rank-1 approximation case contains two parts: the
one that can be handled using the technique in scalar case, and the other one measuring the extent of
alignment between Xk and yk. Again, let U2k = kXk k2 + kykk2.
Theorem H.1 (Main 1). Let h = -ɪ4-. Assume C ≥ √7 and u0 > 4√7μ. Then GD converges to
'	/	u0+4cμ
a point in {(x, y) : ||x『+ ∣∣yk2 ≤ 2, X>y = μ} exceptfor a Lebesgue measure-0 set of (x0, yo).
Proof. The proof follows from Lemma H.3 and Lemma H.8.
□
30
Published as a conference paper at ICLR 2022
Theorem H.2 (Main 2). Let h
4
4√7μ+4cμ
, 3、. Assume C
(√7+c)μ
≥ √7 and u2 ≤ 4√7μ. Then
GD converges to a point in {(x, y) : ∣∣x∣∣2 + ∣∣y∣∣2 ≤ h, x>y = μ} exceptfor a Lebesgue measure-0
set of (x0,y0).
Proof. Since C ≥ √7, in this case We have u0 ≤ 4√7μ ≤ 2(√7 + c)μ = ∙2. Therefore, the
convergence follows from Lemma H.8.	□
Hence We can shoW the proof of the theorems in Section 4.
ProofofTheorem 4.1. By Lemma H.3 and H.7, the proof follows from Lemma H.9.	□
ProofofTheorem 4.2 and 4.3. Similar to scalar case, h = ^+4c. for C ≥ √7 implies h ≤ 说+[^'
and h = 5+.. implies h ≤ 2√7. Therefore, the convergence and balancing of GD follow from
Theorem H.1 and H.2. The second inequality of Theorem 4.3 follows from x> y = μ at the global
minimum because of diagonal and non-negative A and the best rank-1 approximation in SVD. □
H.1 Phase 1
The main theorem in phase 1 is the following.
Lemma H.3. Givenh = u2+44cμ for c ≥ √7,GD will enter {kxk2 + ]|y『≤ h2} exceptfor a
measure-0 set of initial conditions.
Proof. Since the expressions, u2k+2 -u2k and u2k+2 -u2k, of the rank-1 version have an additional term
upper bounded by C0((xk>yk)2 - xk>xkyk>yk) for some constant C0 > 0 which is non-positive, we
have u2k decreases by a constant for either one- or two-step if (xk>yk)2 - xk>xkyk>yk < -C1 for some
constant ci > 0 when 2 < uk < 4. Therefore, we can pick a K > 0 s.t. (χ[yk)2 - χ>χky>yk <
-ci for k ≤ K. If GD enters {∣∣χ∣∣2 + ∣∣y∣∣2 ≤ ∙2} within K steps, then we are done. Otherwise,
we can just assume that for all the k > K, -c1 ≤ (xk>yk)2 - xk>xkyk>yk ≤ 0. Then again, like
the scalar case, we only need to consider when Xk, yk are close to χ>yk = μ and χ>yk
hμuk
4-huk.
For the region near xk> yk
4-hhu2, we can always choose a ci s.t. there exists a δ > 0, when
-δ ≤ Xlyk + 4hμUk2 < 0, we have both x> 1yk+ι > μ and Sk < 0. Then all the discussion just
4-huk	+
follows the scalar case. For the region near Xiyk = μ, i.e., ∣x>yk - μ∣ < e for some 0 < e < μ,
consider
—
—
x>+ιyk+ι - μ = (XIyk - 〃)Sk + h2(2μ - Xkyk)((X>yk)2 - XkXky/yk).
Similar to scalar decomposition, we only need to consider the case when sk ≤ -1 because if sk > -1
which can only happen when XIyk > μ, we have Uk ≤ h + ek h(μ + e) and then Uk+、≤ 2. If
Sk ≥ -1 for all the following iterations, together with the bounded second term in X>+1yk+1 - μ,
i.e., -2h2μcι ≤ h2(2μ - X>yk)((X>yk)2 - X>Xky]yk) ≤ 0, we have that there exists K > 0 s.t.
|xKyκ - μ∣ ≥ ei for some ei > 0 unless it leaves this region ∣X>yk - μ∣ < e. Therefore, by the
same discussion in scalar case, u2k will decrease either in one step or two steps by at least a constant.
Moreover, by Lemma H.4, GD will not terminate inside {h < ||x『 + ||y『 < h} in finite step.
Thus, GD is guaranteed to enter {∣∣x∣∣2 + ∣∣y∣∣2 ≤ 2} in finite steps.	□
Lemma H.4. GD will not converge to any fixed points in {∣∣x∣∣2 + ∣∣y∣∣2 > h } exceptfor a measure-0
set.
Proof. Similar to the discussion in scalar case, the set of points converging to XTy = μ in finite step
is measure-0.
31
Published as a conference paper at ICLR 2022
Also, since μx>y = xτxyτy = μ2, the fixed points of GD map requires xτxyτy = (x>y)2. This
can be seen in the following expression
χk+ιVk+ι — μ = Tyk — μ)(i — huk — h2x>Vk (μ — x>yk))+ h2(2μ — χ> yk)((x>yk )2 — χ> Xk y> Vk),
where the convergence of Xlyk - μ requires the convergence of χ>Xky>Vk - (x>Vk)2.
Hence if x>xky>Vk - (x>Vk)2 does not converge, then GD will not converge. If x>xky>Vk -
(Xlyk )2 converges, then C ≥ XkXkykyk - (x>Vk)2 ≥ 0 for all the rest of the iterations with some
c > 0, and we further assume ∣X>yk - μ∣ < e for any 0 < e < μ. Asis shown in scalar case, when
Uk ≥ h + eh(μ + e), we have Sk = 1 - huk - h2X>yk (μ - Xkyk) ≥ -1. Then
Xk+iyk+i - μ = (Xkyk - μ)sk + h2(2μ - X>yk 1((Xkyk)2 - X>Xky>yk)
will never converge (because Sk ≤ -1 and the second term is bounded).	□
Lemma H.5. When X>yk > μ or X>yk < 一 ?；%, we have uk ∣1 — Uk < 0.
f-'"uk
Proof.
uk+1 - Uk = h(μ - x>yk)((4 - hu2k)X>yk + hu2μ) + h(4 - hu2k)((X>yk)2 - X>Xky>yk)
≤ h(μ - x>yk)((4 - huk)X>yk+ hukμ)∙
Hence the proof follows 1D case.	□
Lemma H.6. When 一 [丫器 ≤ X>yk < μ, we have uk+2 — Uk < 0.
Proof. Let Sk = 1 — hu2k — h?Xkyk(μ — Wyk), the same as 1D.
X>+iyk+i - μ = (X>yk - μ)(1 - huk - BXkyk(μ - X^k)) + h2(2μ - x>yk)((X>yk)2 - x>Xky>yk)
=(Xkyk - μ)sk + M.
,2	. 2	, 2	.2,.2	- 2
uk+2 - uk = uk + 1 - uk + uk+2 - uk+1
=h(μ — Xky )((4 — huk )X[yk + hukμ) + h(4 — huk)((XIyk)2 — XkXkyky)
+ h(μ - XZ+iyk+i)((4 - huk+i)XZ+iyk+i + huk+ιμ)
+ h(4 - huk+i)((XZ+iyk+i)2 - Xk+iXk+iyZ+iyk+i)
= h(μ - Xkyk)(((4 - huk)XIyk+ hukμ)+ Sk(huk+ιμ + (4 - huk+J(sk(Xkyk - μ) + μ)))
+ ]h(4 - huk)((Xkyk)2 - XkXky【yk) + hM2(-4 + huk+1)
+ hM(—huk+ιμ + (—4 + huk+ι)μ + 2(4 - huk+ι)(μ - Xkyk)sk)
δ
I + II.
I is the same as 1D uk+2 - Uk and hence (1) < 0. For II,
II = hM2(-4 + huk+ι) + h((X[yk)2 - XkXkKy)
× (4 - hu2 + h2(2μ - Xkyk)(—huk+ιμ + (—4 + huk+ι)μ + 2(4 - huk+ι)(μ - Xkyk)sk))∙
X--------------------------------------V-------------------------------------}
III
From 1D results, Sk < 0. Then, III achieves its lower bound at Xkyk = μ or Xkyk = - ]μ∖k . For
⅛-/&uk
Xkkyk = μ,
III = 4 — huk — 4h2μ2 ≥ 4hμ(c — hμ) > 0,
32
Published as a conference paper at ICLR 2022
where the inequality is from Uk ≤ u2 = 4 - 4cμ and C > hμ.
For x>yk
hμuk
4-huk,
III = 4 - huk
—
4h2(8 - huk)(4 - huk - 2sk(4 - huk+1))μ2
(4 - huk)2
h h h 2	4h2(8 - huk)(4-huk- 2sk(4 - huk))μ2
≥ 4 - huk	(4 - huk)2
—ZI , 2	4h2(8 - huk)(I- 2sk)μ2
=4 - huk	4-⅛ui
2 / 2
uk≤u0	9
≥	4 — huk —
4h2(4 + 4chμ)(1 — 2sk )μ2
4chμ
uk≤uo 4hμ ,	_	2	--c 、、
≥	---(-1 + c + 2sk + ch(-1 + 2sk )μ)
≥ c2(1 + 8h2μ2)+ c(hμ -与)-7 -宇
**
≥ 0,
where * follows from
sk = 1 - huk - h2χ1yk(μ - Xkyk)
X>yk=μ∕2	2	h2μ2
≥	1 - huk - -ɪ
uk≥u2 -3 + 4chμ - W,
and ** follows from C ≥ √7.
From 1D discussion, -4 + huk+1 < 0. Also, (Xkyk)2 - x>Xky>yk < 0. Hence we have (3) ≥ 0.
Overall, (2) ≤ 0 and therefore the whole discussion of (1) is the same as 1D.
□
H.2 PHASE 2
The proof in phase 2 is partly different from the scalar case due to the alignment. Before presenting
the main convergence lemma, we first state the boundedness of GD in the following lemma.
Lemma H.7. Once GD enters {∣∣x∣∣2 + ∣∣y∣∣2 ≤ 2}, it will stay bounded inside {∣∣x∣∣2 + ∣∣y∣∣2 ≤
2 + 2hμ2(1 + i」2*2 )} and re-enter {∣∣x∣∣2 + ∣∣y∣∣2 ≤ 2} Withinfinite steps where the number of
such steps do not depend on the number of iteration.
Proof. Assume Uk ≤ 2. Then
uk+ι - uk = h(μ - x>yk)((4 - huk)x>yk+ hukμ)+ h(4 - huk)((X>Iyk) - x>xky>yQ
≤ 2h(μ2 - x>Xky>yk) ≤ 2hμ2,
where the first inequality follows from Uk ≤ 2. If further uk+1 ≤ 2 + 2hμ2, then by xk+1yfc+1 ≥
-yx>+1xk+1y>+1yk+1 and -1 + h2μ2 < 0, we have
22
uk+2 - uk+1
=h(μ - xk+ι yk+i)((4 - huk+I)X[+1yk+1+ huk+ιμ)
+ h(4 - huk+J((X>+ιyk+ι)2 - x>+1xk+1y>+1yk+1)
≤ 2h(x>+ιxk+ιy>+ιyk+ι(-1 + h2μ2)+ μ2(1 - 2x>+ιyk+ιh2μ + h2μ2))
≤ 2h(x>+ιxk+1y>+1 yk+ι(-1 + h2μ2) + μ2(1 + 2 Jx>+ιxk+ιy>+ιyk+ιh2μ + h2μ2))
2hμ2
≤ 1 - h2 μ2 ∙
33
Published as a conference paper at ICLR 2022
The first inequality is from uk+ι ≤ 2 + 2hμ2; the second inequality follows from the two conditions
mentioned above; the third inequality follows from choosing
Jx>+1xk + 1y>+lyk+1 = i-hμμ2 .
From previous proof, we know when Uk > 2, by Lemma H.3, we have GD will enter {∣∣χ∣∣2 +
∣∣yk ≤ 2} in finite steps with the decrease of Uk in either one step or two steps. Then, U ≤
2 + 2hμ2(1 + ι-h12μ2) for all i ≥ k.	□
Let Uk = xk>yk, Vk = xk>xk, Wk = yk>yk. Then we can rewrite the iteration for the three variables
Uk+1 = h(1 — hVk)μWk + hμVk (1 - hWk) + Uk (1 + h2Vk Wk - h(Vk + Wk) + h2μ2)
Vk+1 = 2hμUk + Vk - 2hVkWk + h2(μ2Wk — 2μUkWk + VkW2)	.
Wk+1 = 2hμUk + Wk — 2hVk Wk + E(FVk — 2μU Vk + V2Wk)
(10)
When h =说:4：. or h = 2√7μ, we have hμ ≤ 2√√7. The following is the main lemma in phase 2.
Lemma H.8. Given hμ≤ 2√7, ifGD enters {∣x∣2 + ||y『≤ 2}, it converges to x>y = IIxIIIlyIl =
μ, i.e., the global minimum, inside this region except for a measure-0 Set of initial conditions.
Proof. This proof follows the same method as the scalar decomposition. We need to prove that all
the conclusions in scalar case also holds in this rank one decomposition.
Consider 姆+1—Uk when x>yk < —μ or x>yk > μ,
uk+ι — uk =	h(μ — x> yk)((4	—	huk)χ>yk +	huk μ) +	h(4	— huk)((x> yk)2 — χ>χk y>yk)
=h(μ — x> yk)((4	—	huk)x>yk +	huk μ) +	h(4	— huk)(U2 — Vk Wk)
≤ h(μ — x>yk)((4 — huk)x>yk + hukμ),
where the last expression is the same as scalar case and therefore under the conditions such that
U2k+1 — U2k < 0 in scalar case, we also have U2k+1 — U2k < 0 in rank-one approximation.
As in scalar case, we also denote Sk = 1 — huk — h2x>yk (μ — x>yk). First consider ɪ + 2hμ2 <
Uk ≤ h ∙ If —μ ≤ x>yk < μ, uk+1 ≥ Uk, and Sk ≤ —1, consider 姆+2 — Uk ∙ From Lemma H.6, the
extra terms of U2k+2 — U2k compared to scalar case is
hM2(—4 + hU2k+1) + h((xk>yk)2 — xk>xkyk>yk)
X (4 — hU2 + h2(2μ — x>yk)(—hUk+iM + (—4 + hUk+1)μ + 2(4 — hUk+1)(μ — x>yk)Sk))
`----------------------------------------{------------------------------------}
III
where M = h2(2μ 一 x>yk)((x>yk)2 一 x>xky>yk). Then we will prove that In is positive and
then all the extra terms will be less than 0. By rewriting III in terms of xk>yk, we get
III = 2(x>yk)2h2Sk(4 - h〃k+i) + x>yk(庐城十伊 - 6h2Sk(4 - hαk+ι)μ - h2(—4 + hαk+ι)μ)
- 2h^^Uk+ιμ^ + 4h%k(4 - hUk+ι)μ2 + 2h2(-4 + hUk+ι)μ2 + 4 - h〃k
≥ 4 — h〃k + 12h2(-1 + 4sk)μ2 — 12h3Sk城十/2
≥ 4 — h〃k + 12h2(-1 + 4sk)μ2 — 12h3Sk〃k.2
=4 — h〃k + 12h2(-1 + Sk(4 — h〃k))μ2 > 0,
where the first inequality is because it obtains the minimum at x>yk = -μ; the second inequality
follows from U2k+1 ≥ U2k; the last inequality is from the range of U2k and Sk. Therefore the extra
terms of U2k+2 — U2k is always negative and then we have the same conclusion as scalar case, i.e.,
姆+2 — Uk < -C(μ — x>yk)2 for some constant C > 0.
34
Published as a conference paper at ICLR 2022
Next, consider s、+i when -μ ≤ Xkyk ≤ 3μ and ɪ + 2hμ2 < U ≤ h which implies -1 一 h~4~ ≤
sk < 0. The extra terms of sk+1 compared to scalar case is
h2(U2 - VkWk) ( 一 (4 一 huk) + (-X>yk + 2μ)(2h?Sk(Xkyk 一 μ) + h2μ) + h4x(-x>yk + 2μ)2
|
IV
where Uk2 一 VkWk ≤ 0. Then consider the upper bound of IV
IV = 一4 一 2(x[yk)2h%k + Xkyk(一h^μ + 6h2sμ) + huk + 2h^μ^ 一 4后Skμ2
≤ —4 + huk + (一九2 — 4h2Sk)μ2 < 0,
where the first inequality follows from χ> yk ≤ 3μ. Therefore the extra terms is non-negative, namely
all the lower bound for Sk+1 in the scalar case also holds for rank-one approximation.
Also We need to consider sk+2 when Sk > 一1, 一仙 ≤ x>yk ≤ 3μ, and ɪ + 2hμ2 < uk ≤ Ih. For
scalar case, from Lemma G.13 with e = x>+ιyk+ι 一 μ, We have
sk+2 = sk+1 + (h2(3 + sk+ι) 一 h3uk)e2 + h2e(-3μ — sk+ιμ)
=3h2e2 + h2 sk+ιe2 — h3uk e2 — 3h2eμ + Sk+ι(1 — h2eμ).
Since the minimum point w.r.t. Sk+ι is -A-hF' < 一1 一 h4Γ~, We have if Sk+ι is larger, then Sk+2
is also larger. Given the same value for Sk for both scalar and rank-one cases, Sk+1 is larger from
the above discussion and thus Sk+2 is also larger, namely, the lower bound for Sk+2 also holds in
rank-one cases.
Next consider Uk ≤ h + 2hμ2 where ∣Sk | < 1 and We need the iteration to have certain restriction
such that it will be bounded in a region. If Sk < 0, then it follows the same from the above
discussion. If Sk > 0 and x>yk > μ or x>yk < — μ, then by uk+1 一 Uk < 0, it will stay in
{x>yk > μ, uk ≤ h + 2hμ2} until convergence. If Sk > 0 and —μ ≤ x>yk < μ, similar to scalar
case, we consider |Xk>Xk 一 yk>yk| = |Vk 一 Wk| and
lVk + 1 — Wk+1 | = IVk - Wk| ∙ ∣1 一 h2 VkWk + 2h2Ukμ 一 h2μ2 |,
where Uk = Xk>yk and Uk2 ≤ VkWk. Then
1 —	h2VkWk	+ 2h2Ukμ —	h2μ2	≤ 1 —	h2U2 +	2h2Ukμ 一 h2μ2	< 1,
1 —	h2VkWk	+ 2h2Ukμ —	h2μ2	≥ 1 —	h2Uk 一	2h2μ — h2μ2 ≥	1 — ∣(1 +	2h2μ2)2	— 3h2μ2	> 0.
Therefore if —μ ≤ x>yk < μ and Uk ≤ ɪ + 2hμ2, we have ∣Vk+ι — Wk+ι∣ < |Vk — Wk∣,
meaning together with 0 ≤ μ - x>+1yk+1 < μ — x>yk, Xk+1 and yk+ι is bounded in the monotone
convergence region.
So far we retrieve all the conditions for convergence. Therefore, from the proof of the convergence in
scalar case, we have that there exists a constant K > 0, s.t., ∣ Sk ∣ < 1 and 0 < 2μ — x>yk < 2μ for
all k > K. Then let Sk = h2 (2μ — χ>yk) and we have
x>+1yk+1 一 μ = (X>yk 一 μ)(I 一 huk 一 Bxkyk(μ 一 χ>yk))
+ h2(2μ — x>yk)((Xkyk )2 — χ>χk y>yk)
=(X>yk 一 M)Sk + Sk (U2 - VkWk)
In fact X>yk — μ → 0 as k → 0. First, since Sk(U2 — VkWk) → 0 as k → 0 by Lemma H.9 and
∣Sk∣ < 1, we have X>yk - μ is bounded. Also, from previous discussion, ∣Sk∣ ≤ max{C2,1 —
C3(X>yk — μ)2} for some constant C2, C3 > 0. For any e0 > 0, there exists Ki > 0 and eι《 e。
such that |Sk(U2 — VkWk)| < ei for all k > Ki and C2 ≤ 1 — C3e2. Therefore X>yk - μ will
35
Published as a conference paper at ICLR 2022
decreases to O(". This is because
∣x>+1yk+1 — μ∣ ≤ (1 — C3el)∣x>yk - μ∣ + 2μ∖uκ 1 - VK1 WKIIC2(k K1)
≤ I(XKI yKi — μ)∣(I- C3E2)k-K1
k
+ 2h2μ(VκιWKi — UKJ X C2(i-KI)(I —。3劭1
i=Ki
≤ I(XKI yKi — μ)K1 — c3€ι)k K1 + 2h2μ(VKI WKI- uK 1 )(k — KI)Ck KI
where C4 = max{C0,1 — C3el} < 1 and We have kCk → 0 as k → ∞. Therefore there exists
K ≥ K1 s.t. xK2yK2 — μ = O(eι). Then we will show for all k > K2,∣x>yk 一 μ∣ < ∈o. AS is
shown before there exists K3 > K2 and €2《 ∈ι s.t. ∣Sk(Uk — VkWk)∣ < €2. Since ∣sk∣ < 1, the
increase of ∣χ>yk — μ∣ is also O(∈ι) and thus ∣χ>yk — μ∣ = O(eι) < €0 for k > K2. Then we have
the convergence of ∣χ>yk 一 μ∣ → 0.
In general, we have χ>yk → μ and ∣ cos(∠(xk,yk))∣ → 0, which implies GD converges to the global
minimum XTy = IlXkkyk = μ.	□
For the proof of alignment, we only need to consider when ∣∣Xk k2 + Ilyk k2 ≤ 2 + 2hμ2(1+ 一工2).
Also, we have Vk Wk — Uk ≥ 0 and
Vk+1Wk + 1 — Uk+1 = (VkWk - Uk)(I- h(Vk + Wk) + h2 (Vk Wk - W))乙
Thus we denote lk = 1 — h(Vk + Wk) + h2(VkWk — μ2) which characterizes the change of
Vk Wk — U2.
Lemma H.9. If ∣∣Xk ∣∣2 + Ilyk k2 ≤ 2 + 2hμ2(1 + j—⅛μ∑) ,then Vk Wk — Uk converges to 0 and
also ∣ cos(∠(Xk, yk))∣ converges to 1.
Proof. By 0 ≤ VkWk ≤ (Vk+Wk )2 and the boundedness theorem, we have
Vk + Wk ≤ τ + 2hμ2(1 + Tj--又2 2 ),
h	1 — h2从2
and
Ik = 1 — h(Vk + Wk) + h2(VkWk — μ2) ≥ —1 — h2μ2(3 +
1-⅛),
(11)
Lk = 1 - h(Vk + Wk) + h2 (VkWk - M2) ≤ W(Vk + Wk)2 - h(Vk + Wk) + 1 - h2μ2
≤ 1 — h2μ2 < 1.
We will show that there exists K > 0 and a constant C > 0 only depending on hμ, s.t. when k > K,
∣lk∣ < 1 - C.
If Vk + Wk < 2 — hμ2, then
lk = 1 — h(Vk + Wk) + h2 (VkWk — μ2) > —1 + h2VkWk > —1.
(If Vk Wk = 0, then Vk Wk — Uk = 0 meaning it just converge.)
Therefore when Vk + Wk < 2 — hμ2, we have ∣lk∣ < 1. We only need to consider lk+1 when
2 — hμ2 ≤ Vk + Wk ≤ 2 + 2hμ2(1 +	). NeXt we replace Ik, Vk, Wk, Uk by l, V, W, U for
36
Published as a conference paper at ICLR 2022
simplicity.
lk+1	= 1 -	h(Vk + 1	+	Wk+1) +	h2(Vfe+1Wfe + 1 - 〃2)
=1 -	h(Vk + 1	+	Wk+1) +	h2(Vk+IWk+ 1 - Uki+1) + h2 (Uk + 1 - /)
=1 -	h(Vk + 1	+	wk+1) +	h2l2 (VW - U2 ) + h2 (U⅛+1 — μ2)
(=) l +	h2(3 +	l2	- h(1 + 4h2μ2)(V + W))(VW - U2)
+ h2U2(3 + (l + 2h2μ2)2 - h(1 + 4h2μ2)(V + W))
+ h2μ2(h2(W + V)2 - h(W + V) + 4h4V2W2)
+ 2h2Uμ((1 - (l + 2h2μ2))h(W + V) - 2 + 2(l + 2h2μ2)(1 - l - h2μ2))	(12)
If l ≥ -4h2μ2, We have
lk+1 = l + h2(3 + l2 - h(1 + 4h2μ2)(V + W))(VW - U2)
+ h2U2(3 + (l + 2h2μ2)2 - h(1 + 4h2μ2)(V + W))
+ h2μ2(h2(W + V)2 - h(W + V) + 4h4V2W2)
+ 2h2Uμ((1 - (l + 2h2μ2))h(W + V) - 2 + 2(l + 2h2μ2)(1 - l - h2μ2))
≥ l + 2h2Uμ((1 - (l + 2h2μ2))h(W + V) - 2 + 2(l + 2h2μ2)(1 - l - h2μ2))
≥ l - h2μ(V + W)(5h2μ2 + (9h4μ4)∕8)
≥ -4h2μ2 - hμ(2 + 2h2μ2(1 + --二y))(5后μ2 + (9h4μ4)∕8)
1	- h2 μ2
> -1,
where the first inequality is because all the removed terms are positive when 2 - hμ2 ≤ V + W ≤
2 + 2hμ2(1 + 1-^2.2); the second inequality follows from recollecting l and take l from quadratic
formula with the h(V + W) taken at the bound, and U > - V+W; the third inequality follows from
the bounds of h(V + W) and l; the last inequality is from hμ ≤ 2√7.
If l < -4h2μ2 ⇒ l + h2μ2 < 0. By rewriting lk+1 in the following way
lk+1 = l + h2(4h4V2W2 - h(V + W) + h2(V + W)2)μ2 + 4h4U2μ2(l + h2μ2)
+ 2h2Uμ(-2 - h(V + W)(-1 + l + 2h2μ2) - 2(-1 + l + h2μ2)(l + 2h2μ2))
+ h2VW(3 + l2 - h(V + W)(1+ 4h2μ2)),
we have that the minimum of lk+1 is achieved when U = ± √VW. Thus we will consider two cases:
U ≥ 0 and U < 0. Note we always have U2 < VW (otherwise VW - U2 = 0 just converges) and
hence lk+1 is larger than the corresponding value at U = ±√VW. Let q = h|U|,v = hμ, q = Cv
for c ≥ 0. Then we have
q2 = c2v2 ≤ h2VW = h(V + W) - 1 + l - v2 < 1,	i.e., c < 1.	(13)
V
Next, let
lk+1 = lk+1 — h2(3 + l2 — h(1 + 4h2μ2)(V + W))(VW — U2) < lk+1 — h2(2 + l2)(VW — U2).
First let U ≥ 0 and q = hU < h√VW. Choose q = hU = h√VW. Then by
h(V + W) = 1 - l + h2(VW - μ2) = 1 - l + C2V2 - v2,
we have the minimum value of lk+1 when U ≥ 0
lk+1 ≥ lk + 1 = l + ( -1 + C)( —(( -1 + l)l) + c(2 + l + ∕2))v2	(14)
-(-1 + c)2(1 + C2 - 2l + 2cl)v4 + (-1 + c)4v6.
1) When -1 < l < -4h2μ2 and C = 1, we have lk+1 = /—1 = l = lk.
37
Published as a conference paper at ICLR 2022
2)	When -1 < l < -4h2μ2 and c > 1, the derivative of (14) with respect to C is
-l,k+1 = (1 - l)lv2 + (-2 - l - l2)v2 + 2v4 - 6lv4 - 4v6 + 3c2(2v4 - 2lv4 - 4v6)
dc 丁
+ 4c3(-v4 + v6) + 2c((2 + l + l2)v2 - 2v4 + 6lv4 + 6v6),
which is a third order polynominal with negative coefficient of the highest degree term and has a
root in [0,1]. Also, when C = 1, l!k+1 = 2v2(1 + l) > 0. Therefore, when 1 < c < V, lk+1 either
increases or first increases then decreases, namely, the minimum of l0+1 is achieved at C = 1 or
C = 1. If C = 1, by the previous discussion, l 0+1 = l. If C = ɪ,
l.1	= 1 + l2(-1 + v)2 - v2 - 2v3 + 5v4 - 4v5 + v6 + l(2 - 2v + 5v2 - 6v3 + 2v4)
v2(-24 + 44v - 25v2 + 4v3)
≥	4(-1+ v)2	> -,
where the first inequality follows from the property of quadratic function by taking l
—
(2-2v+5v2-6v3 + 2v4)
2(-1+v)2
.Therefore when -1 < l < -4h2μ2 and c > 1, l-1 > -1.
3)	When -1 < l < -4h2μ2 and c < 1, rewrite l k+ι with respect to l and we have
l++ι = 2(-1 + c)cv2 - (-1 + c)2v4 - (-1 + c)2c2v4 + (-1 + c)4v6 + l2((1 - c)v2
+ (-1 + c)cv2) + l(1 + (-1 + c)v2 + (-1 + c)cv2 + 2(-1 + c)2v4 - 2(-1 + c)2cv4),
where the minimum point is
l = - 2(-1 + c)2v2 - 2 - (1 - C)v2 < - A - 1 < -1∙
Therefore the minimum of l k +ι is obtained at l = -1, i.e.,
l 0+1 ≥ -1 + (-1 + c)2v2(2 — (3 — 2c + c2)v2 + (-1 + c)2v4)
≥-1 + (-1 + c)2v2(2 - 3v2 + (-1 + c)2v4) > -1,
where the inequality is from 0 ≤ c < 1. Therefore when -1 < l < -4h2μ2 and C < 1, l k+1 > -1.
4)	When l ≤ -1 and C ≥ 4, consider l,k +1 - l (15) and take the derivative d(l k+1 -1) = dl k+1.
From previous discussion, consider 5 l k +1 at C = 4, i.e.,
-d-l++1 = 1∕16v2(48 + 8l2 - 23v2 + v4 + l(40 - 6v2)) > v2 - (161v4)∕16 + (325v6)∕16 > 0,
where the first inequality follows from l > -1 - 6v2 by (11). Therefore the minimum of l k +1 - l is
at C =5 orC =1.If C = 4,
22
lk . 1 - l =——(160 + 16l2 - 41v2 + v4 - 8l(-18 + v2)) > ——(32 - 705v2 + 625v4) > 0
+1	256	256
where the first inequality is from l > -1 - 6v2 by (11). If C = ɪ,
l k+1 — l = l2(-1 + v)2 + l(-1 + v)(-1 + V — 4v2 + 2v3) + (-1 + v)(-1 — V + 2v3 — 3v4 + v5)
≥ (1 - v)3(1 + 3v + v2 - v3) > 0
where the first inequality is from l ≤ -1. Therefore when l ≤ -1 and C ≥ ∣, lk+1 - lk >
min{熏(32 - 705v2 + 625v4), (1 - v)3(1 + 3v + v2 - v3)} > 0.
In the next two cases C ≤ 1 and 1 < c < ∣, we no longer assume q = h√VW but want the same
lower bound of l k+「Since q = hU < h√VW, we have h(V + W) > 1 - l + c2v2 - v2 > 0. By
rewriting l 0 +1 to be
l k+1 =l — 4cv2 + 3 c2 v2 + 4c4v6 + 4cv2(1 — l — v2 )(l + 2v2) + v2h2(V + W )2
+ c2v2(l + 2v2)2 + (—v2 + 2cv2(1 — l — 2v2) — c2v2(1 + 4v2))h(V + W),
38
Published as a conference paper at ICLR 2022
it can be seen from the minimum point w.r.t. h(V + W) that when h(V + W) > 0 and 0 ≤ c < 4,
Ik+ι decreases as h(V + W) decreases. Therefore by h(V + W) > 1 - l + c2v2 - v2, we have the
same expression for the lower bound of 10+1 but with U2 < VW, i.e.,
lk+1 - l0+1 = h2(3 + l2 - h(1 + 4h2μ2)(V + W))(VW - U2) > h2(∣ + l2)(VW - U2),
lk+ι > l + (-1 + c)(-((-1 + l)l) + c(2 + l + l2))v2
-(-1 + c)2(1 + c2 - 2l + 2cl)v4 + (-1 + c)4v6.
5)	When l ≤ -1 and C ≤ 1, consider lfk +ι - l
lk+ι — l > (-1 + c)(-((-1 + l)l) + c(2 + l + l2))v2	(15)
-(-1 + c)2(1 + C2 - 2l + 2cl)v4 + (-1 + c)4v6
=(-1 + c)2l2v2 + (-1 + c)lv2(1 + C — 2(1 — c)v2 + 2(1 — c)cv2)
+ (-1 + c)v2(2c + (1 — c)v2 + (1 — c)c2v2 + (-1 + c)3v4)
where the minimum point is l = -1 + I1-”二(13" > -1. By l ≤ -1 We have
lk+1 - l > v2(2(-1 + c)2 - (-1 + c)2(3 - 2c + c2)v2 + (-1 + c)4v4)
=v2((-1 + c)2(2 - (3 - 2c + c2)v2) + (-1 + c)4v4)
≥ v2((-1 + c)2(2 - 3v2) + (-1 + c)4v4) ≥ 0.
Therefore when l ≤ —1 and C ≤ 1, l-1 — lk > h2(ɪ + l2)(VW — U2) ≥ ∣h2(VW — U2).
6)	When l ≤ -1 and 1 < c < 4, consider Ufc+1. By h(V + W) - h2VW = 1 - l - v2, we have
Uk+1 = h(1 - hV)μW + hμV(1 - hW) + U(1 + h2VW - h(V + W) + h2μ2)
=μ((c - 1)(l + v2) + 1 + cv2 - h2VW).
Thus denote hUfc+1 = cfc+1hμ = cfc+1v and we have
ck+1 = (c - 1)(l + v2) + 1 + cv2 - h2VW
< (c - 1)(l + v2) + 1 + cv2 - c2v2 < 1.
Then if lk+1 - l ≥ 0, then lk+2 - l = lk+2 - lk + 1 + lk+1 - l > lk+2 - lk+1 > ∣h2 (VW - U2).
Otherwise, we have lk+1 < l and consider lfc+2∙ Also, the distance between 1 and cfc+1 is larger than
that of 1 and c, i.e.,
1 —。左十1 — (c — 1) > (1 — (c — 1)(l + v2) + 1 + cv2 — c2v2) — (c — 1)	(16)
=1 + l + v2 + c2v2 + c(-1 - l - 2v2) > 0.
From previous discussion, l/k+1 - l increases as l decreases. Also 5 lk+1 < 0 when 0 ≤ c < 1, so
lk+1 - l increases as C decreases. By (16), we have cfc+1 < 2 - c. Then
lk+2 - l = lk+2 - lk+1 + lk + 1 - l > lk+2 - lk+1 + lk + 1 - l
> (-1 + c)(-((-1 + l)l) + c(2 + l + l2))v2
-(-1 + c)2(1 + c2 - 2l + 2cl)v4 + (-1 + c)4v6
+ (1 - c)v2(-((-1 + l)l) - (-2 + c)(2 + l + l2)
+ (-1 + c)(5 + c2 +2l - 2c(2 + l))v2 - (-1 + c)3v4)
=2v2(-1 + c)2((2 + l + l2) + (-2 - (-1 + c)2)v2 + (-1 + c)2v4)
> 2v2(-1 + c)2((2 - 3v2) + (-1 + c)2v4) > 0,
where the	second	inequality follows from ck+1 < 2 - c and lk+1 < l; the last inequality is	by
l ≤	-1	and	1	<	c	< 5. Therefore when l ≤ -1 and 1 < c < 4, lk+2 - lk > ∣h2(VW	- U2)	or
2v2(-1 + c)2((2 - 3v2) + (-1 + c)2v4).
39
Published as a conference paper at ICLR 2022
Second, let U < 0 and q = -hU < h√VW. Choose q = — hU = h√VW. Then similarly We
have the minimum value of lk+1 when U < 0
lk0 +1 = l + (1 + c)((—1 + l)l + c(2 + l + l2))v2 — (1 + c)2 (1 + c2	(17)
— 2l — 2cl)v4 + (1 + c)4v6
= (1+c)2l2v2+l(1+(—1+c2)v2+2(1+c)3v4)
+ (1 + c)v2(v2(—1 + v2) + c3v2(—1 + v2) + c2v2(—1 + 3v2) + c(2 — v2 + 3v4))
≥ —1+ (—4+4c+3c2)v2+ (15+ 16c — 5c2 —8c3 — 2c4)v4	(18)
+(—5—4c+2c2+c3)2v6,
Where the inequality is because When l > —1 — 6v2 + c2v2 (from (11)), lk0 +1 increases as l increases.
1)	When c ≥ 1, since (—5 — 4c + 2c2 + c3)2v6 ≥ 0, We consider (—4 + 4c + 3c2)v2 + (15 + 16c —
5c2 — 8c3 — 2c4)v4 denoted to be lkpa+r1t
；lp++r； = (4 + 6c)v2 + (16 — 10c — 24c2 — 8c3)v4.
It has a root between -2 and 0. Also when C = 1,亮Vp^： > 0; when C = ɪ,房/葭；< 0. Thus, the
minimum of Ik+； is achieved at C = 1 or C = ɪ, i.e.,用/ ≥ v2(3 + 16v2). Then
lk0 +1 ≥ —1 +v2(3+ 16v2) + (—5 — 4C+2C2 + C3)2v6 > 0.
Therefore when C ≥ 1, lk+1 > —1.
2)	When 0 ≤ C < 1 and l > —1, from previous discussion of lk0 +1 we have
lk0 +1 ≥ —1 + 2(1 + C)2v2 — (1 + C)2 (3 + 2C + C2)v4 + (1 + C)4v6
≥ —1 + (1 + C)2v2 (2 — 6v2) + (1 + C)4 v6
≥ —1+v2(2—6v2)+v6 > —1.
Therefore when 0 ≤ C < 1 and l > —1, lk+1 > —1.
3)	When 0 ≤ C < 1 and l ≤ —1, consider lk0 +1 — l. By (17),
l0 +1 —l = (1 + C)2l2v2 + l((—1 + C2)v2 + 2(1 + C)3v4)
+ (1 + C)v2(v2(—1 +v2) + C3v2(—1 +v2) + C2v2(—1 + 3v2) +C(2 — v2 + 3v4))
≥ 2(1 + C)2v2 — (1 + C)2(3 + 2C + C2)v4 + (1 + C)4v6
≥ v2(2 — 6v2) + v6,
where the first inequality is because the minimum point of l is greater than -1. Therefore when
0 ≤ C < 1 and l ≤ —1, lk+1 — lk > v2(2 — 6v2) + v6.
Actually all the proofs of lk > —1 ⇒ lk+1 > —1 + C for some C > 0 are valid for all 0 <
Vk + Wk ≤ h2 + 2hμ2(1 + 1-j12μ ).
In general, if lk > —1, then there exists a constant C > 0 only depending on hμ s.t. li > —1 + C for
all i > k. If lk ≤ —1, then either (1) l-i increases at least by a fixed value only depending on hμ, or
(2) lk+2 or lk+i increases by 2h2 (VkWk — U2). Ifit keeps staying in case (2), then since lk ≤ —1,
VkWk — Uk2 will be larger and larger until li > —1 for some i. Therefore, there exists a K > 0, s.t.
when k > K, lk > —1 + C. Then because lk ≤ 1 — h2μ2,let 0 < Co = max{1 — C, 1 — h2μ2} < 1
and we have |lk | ≤ C0 < 1 for all k > K . Further,
Vk Wk — Uk2 ≤ C0	(VKWK — UK2 ) → 0 as k → ∞,
ie, 1 - VUWk → 0 ⇒ 1 cos(∠(xk, yk))∣ → 1.
Otherwise, there exists a K > 0, s.t., VKWK — UK2 = 0, then VkWk — Uk2 = 0 for all k ≥ K.
There are two cases. (i) xK and yK are already aligned. (ii) one of VK and WK is 0, WOLG assume
40
Published as a conference paper at ICLR 2022
VK = 0. Then from the GD update,
( χk+ι = Xk + h(μl - Xky> )yk
I yk+ι =期k + h(μl -nkχ> )χk ,
XK+1 and yK+1 will be aligned for both cases. Then for all k ≥ K + 1, Xk and yk is aligned, i.e.,
| cos(∠(χk,yk ))| = 1.	□
I Proof of Theorem 5.1
I.1 General rank 1 approximation for non-negative diagonal matrix
We first discuss an easy case where A is a non-negative diagonal matrix (later on we will see this is
actually the canonical case) and consider
min kA - Xy>k2F/2.	(19)
x,y∈Rn
(μιlnι	\
Assume A =	...	I, where Ini is an identity matrix of dimension n × ni,
PPi=1 ni = n, μi ≥ 0, μi = μj for i = j. Let Si = {1, 2, ∙∙∙ ,ni} + PPj=ι nj, i.e., an indeχ
set describing the positional indices of Ini. Let μmaχ = max{μι, •一，μm} and Smax be the
corresponding index set. Let [X∞, y∞] be a fixed point of the GD map. Let Xs,∞ and ys,∞ be the sth
element of X∞ and y∞.
The following theorem characterizes the fixed points of the GD map.
Theorem I.1. Thefixedpoints ofGD map satisfy 52s∈s- x2,∞ ∙∑s∈si y2,∞ = μ2, xs,∞ = ys,∞ = 0
for S ∈ Si, ∀i = 1, ∙ ∙ ∙ , n. Thus k χ∞ kk y∞ k = μi, ∀i = 1, ∙ ∙ ∙ , n.
Proof. To solve the fixed points, we have ∀i = 1, ∙∙∙ ,n and X ∈ Si
(A - X∞y∞> )y∞ = Ay∞ - (y∞> y∞)X∞ = 0
(A - X∞ y∞ ) X∞
A> X∞
- (X>∞X∞)y∞ = 0
i.e.,
μiXs,∞=((Xs5/；： ⇒ μ2xs,∞ys,∞=(χ∞χ∞)(y∞ y∞)χs∞ ys,∞
Obviously, x∞ = y∞ = 0 is a fixed point. If Xs,∞ = 0, then /s,∞ = 0 and we have μ2 =
(x∞x∞)(y∞y∞). Since μi = μj for i = j, there exists at most one i, s.t. μ2 = (x∞x∞)(y∞y∞)
and all the Xs,∞, /s,∞ s.t. S ∈ Sj, j = i is zero.	□
This theorem identifies that each of these fixed points is concentrated in one of the positions of
the eigenvalue blocks. Note the fixed points contain both stable and unstable ones, i.e., all the
global minima and saddles are of the above form. Moreover, the global minima are the points
obeying Ps∈smaχ x2,∞ ∙ Ps∈Smax y2,∞ = μmɑx, xs,∞ = ys,∞ = 0 for S ∈ Smax and also
kx∞kky∞k = μmax. The SaddleS are the ones with μi = μmax, ∀i = 1, ∙ ∙ ∙ , n.
Remark I.2 (The alignment of X∞ and /∞ at the global minimum). At the global minimum of the
objective (19) with diagonal and non-negative A defined above, X∞ /∞> is the rank-1 approximation
of A with the largest eigenvalue, i.e., we have μmax = tr(X∞ /∞> ) = X>∞ /∞. Also, from the above
theorem, ∣∣χ∞kky∞k = μm,ax. Therefore for each global minimum, there exists a scalar l > 0, s.t.,
χ∞ = ly∞, i.e., χ∞ and y∞ are aligned at the global minimum.
Based on the analytical form of eigenvalues of the Jacobian of GD map (19) in Theorem I.1, we can
establish the following relation between X∞ and /∞ at the global minimum via stability analysis.
41
Published as a conference paper at ICLR 2022
Theorem I.3. For almost all initial conditions, ifGD converges to a minimum, then x∞y∞ = μmaχ
and
l∣χ∞k2 + ky∞k2 < 2;	(20)
h
the extent of balancing is quantified by
kx∞ - y∞k2 <  ------2μmaχ.	(21)
h
Proof. Since each unstable fixed point has its stable set with negligible size when compared to the
stable set of stable fixed point, then for almost all initial conditions, if GD converges to a global
minimum, this minimum is a stable fixed point of the GD map.
From the above theorem, We know the fixed points are μ2 = (x∞x∞)(y∞y∞). Assume μ2 =
For the Jacobian J = I2n +
blocks commute, we have
-h(y∞ y∞ )In
hA> - 2hy∞x>∞
hA - 2hx∞y∞>
-h(x∞ x∞ )In
I2n + A, since the lower two
det(λI — A)= det(λ21n + λh(χ∞x∞ + y∞y∞)In + h2μ2In - h2A2
+ h2(2Ay∞x>∞ +2x∞y∞> A - 4x∞y∞>y∞x>∞))
=det(λ21n + λh(χ∞χ∞ + y∞ y∞)In + h2μ2 In —后A2
+ h (2(A — x∞ y∞ )y∞ x∞ + 2x∞ y∞ (A — y∞ x∞ )))
=det(λ2In + λh(x∞x∞ + y∞ y∞)In + h2μ2In — h2A2)
n
=Y(λ2 + h(x∞x∞ + y∞y∞)λ + h2(μ2 — μ2)).
i=1
Hence the two values, 1 and 1 — h(x>∞x∞ + y∞>y∞), are eigenvalues of J at each nonzero fixed
point.
If μ = max{μi}, then |1 — h(x∞x∞ + y∞y∞)∣ < 1 ⇒all the eigenvalues are bounded by 1. Hence
x∞x∞ + y∞y∞ < h.
If μ = max{μi}, then 1+1(—h(χ∞x∞+y∞ y∞)+√h2(x >∞ x∞ + y∞y∞)2 + 4h2((μ2 — μ2))) >
1 for μi ≥ μ. It has at least one unstable direction. It,s stable set is measure 0.
The second inequality of the theorem follows from χ∞y∞ = μ at the global minimum because of
diagonal and non-negative A and SVD.	□
I.2 General matrix factorization for non-negative diagonal matrix
In this section, we consider problem (5) via GD iteration but with A ∈ Rn×n to be an arbitrary
non-negative diagonal matrix.
Let μι ≥ μ2 ≥ …≥ μnj ≥ 0 be the diagonal elements of A. Let r = rank(A) which of course
satifies r ≤ n. Also assume that lAlF does not scale with n or d (without loss of generality we can
assume lAlF = O(1)). Then we will have our balancing result in the following.
Theorem I.4. For almost all initial conditions, if GD for (5) converges to a global minimum, then
there exists c = c(n, d) > c0 with constant c0 > 0 independent of n, d, and learning rate h, such
that, (X, Y ) satisfies
2
c(kχ∞kF + ∣γ∞kF) <h,
and the extent of balancing is quantified by
kX∞ — Y∞kF < Jh — 2 Xr μi.
c	i=1
42
Published as a conference paper at ICLR 2022
Proof. Let r =rank(A) ≤ n. Since each unstable fixed point has its stable set with size negligible
when compared to the stable set of stable fixed point, then for almost all initial conditions, if GD
converges to a global minimum, this minimum is a stable fixed point of the GD map.
For matrix X = (xi χ2 •… Xd) ∈ Rn×d, where Xi ∈ Rn is the ith column vector, We vectorize the
∕xι∖
X2
variable and get Vec(X) =	.	. Then we can rewrite the GD iteration,
.
.
Xd
Xk+1 = Xk + h(A - XkYk>)Yk
Yk+1 = Yk + h(A> - YkXk>)Xk
⇒ ʃ Vec(Xk+1) = Vec(Xk) + h(I % A - T(Vec(Yk)vec(Xk)>))vec(Yk)
V Vec(Yk+1) = Vec(Yk) + h(10 A - T(Vec(Xk)vec(Yk)>))vec(Xk)
where 0 is the Kronecker product, and T : Rnd×nd → Rnd×nd is a linear operator, s.t.
then
/	M11	…MId、	Λ	M1>1	• •	•	M1>d
T	. . .	.	Il =	. . .	...	I ,
∖	Md1	• ∙ ∙	Mdd /	∣)	Md>1	• •	•	Md>d
			X1 y1>	X2y1>	•	• •	Xdy1>
			X1 y2> . .	X2y2>	• . .	• •	Xdy2> I
T (Vec(Y)Vec		(X )>) =			. .
			. X1yd>	. X2yd>	•	. • •	Xdyd>
The Jacobian of the vectorized GD map is (replace Xk and Yk by X and Y) I2nd - hM , where
M	Y>Y0In	T(Vec(Y)Vec(X)>) +I0(XY> -A)
M = T(Vec(X)Vec(Y)>) +I0 (YX> - A)	X>X 0In	.
We would like to obtain an estimation of the eigenvalues of I - hM for stability analysis. Since M is
the Hessian of the objective at the global minimum defined, all the eigenvalues are non-negative. Also,
since kAkF is independent of h, n and d, we can just assume without loss of generality kAkF = O(1)
and then take kXkF and kYkF to be O(1) at each global minimum. Then
tr(M) = n(kXk2F + kYk2F) = O(n).
Next consider tr(M 2). We only need the diagonal blocks of M2. The two diagonal blocks are the
following
M1 = (Y>Y0In)2
+ [T(Vec(Y)Vec(X)>)+I0 (XY> - A)][T(Vec(X)Vec(Y)>) + I 0 (YX> - A)],
M2 = (X>X 0In)2
+ [T (Vec(X)Vec(Y)>) + I 0 (YX> - A)][T (Vec(Y)Vec(X)>) + I 0 (XY> - A)].
Since we evaluate this matrix at the minimum, we have the gradient equals 0, i.e.,
(A-XY>)Y= 0,(A-YX>)X =0.
By the mixed-product property of Kronecker product, we have
M1 = (Y>Y)2 0In + T(Vec(Y)Vec(X)>)T(Vec(X)Vec(Y)>)
+Id0 [(A-XY>)(A-YX>)],
M2 = (X>X)2 0In + T(Vec(X)Vec(Y)>)T(Vec(Y)Vec(X)>)
+Id0 [(A-YX>)(A-XY>)].
43
Published as a conference paper at ICLR 2022
Then
tr(M2) =2kXk2FkYk2F+n(X>X2F+	Y>Y2F)+dA-XY>2F.
Also, this is the global minimum, meaning
∣∣A - XY >∣∣2 = tr((A - XY >)(A - YX >)) = ʃ Ei=d+1 μ2,	d<r
F	0,	d ≥ r
Therefore, when d ≥ r, we have
tr(M2) = 2 kXk2F kY k2F + n(∣∣X>X∣∣2F + ∣∣Y >Y ∣∣2F) = O(n).
When d < r, we have
tr(M2)=2kXk2FkYk2F+n(∣∣X>X∣∣2F+∣∣Y>Y∣∣2F)+d∣∣A-XY>∣∣2F
<2kXk2FkYk2F+n(∣∣X>X∣∣2F+∣∣Y>Y∣∣2F)+n∣∣A-XY>∣∣2F=O(n).
Therefore, the number of non-zero eigenvalues is N = O(n). Let λmax be the largest eigenvalue of
M . Then there exist a constant c0 > 0
λmax ≥ tr(M)/N ≥ c0(kXk2F+ kYk2F).
From stability analysis, we need the absolute values of the eigenvalues of I - hM to be bounded by
1, i.e.,
2
1	- hc0(IlXllF + IlYIlF) ≥ 1 - hλmax ≥ -1 ⇒ CO(IlXIIF + IlYIlF) ≤ 工.
F	F	F	Fh
Obviously, if we pick a constant c = c(n, d) for each n and d instead of c0, the above inequality also
holds with c(n, d) > c0 .
For the derivation of the second inequality, since at the global minimum tr(X∞Y∞> )
pm=n{d,r} μi = pm=n{d,n} 出匕 because for non-negative diagonal A, its SVD of the non-zero part
satisfies Unon-
Vnon-zero , we have
zero
IX∞ - Y∞IF = IX∞IF + IY∞IF - 2tr(X∞Y>) ≤ Ch
min{d,n}
-2 E μ,.
i=n
□
I.3 From diagonal matrices to general matrices
Proof of Theorem 5.1. First all the minima of this problem are global minima and all the saddles are
unstable fixed points. Since each unstable fixed point has its stable set with size negligible when
compared to the stable set of stable fixed point, then for almost all initial conditions, if GD converges
to a point, for almost all situations, this point is a stable fixed point of the GD map.
By singular value decomposition (SVD), A = UDV >, where U, D, V ∈ Rn×n, U and V are
orthogonal matrices, and D is a non-negative diagonal matrix. Let Xk = URk, Yk = V Sk. Then
Xk+n = Xk + h(A - XkYk>)Yk
Yk+n = Yk + h(A - XkYk>)>Xk
URk+n = URk + h(UDV > -URkSk>V>)VSk
⇔	VSk+n=VSk+h(UDV> -URkSk>V>)>URk
⇔	Rk+n = Rk + h(D - RkSk> )Sk
⇔	Sk+n =Sk+h(D-RkSk>)>Rk
Therefore, problem (5) is equivalent to the following problem solved by GD
min
R,S∈Rn×d
∣∣D - RS>∣∣F /2.
(22)
44
Published as a conference paper at ICLR 2022
From Theorem I.3 and I.4, we obtain: if GD for (22) converges to a global minimum (R, S), then
there exists a constant c > 0 independent of n, r, d, and h, s.t., the limiting minimum satisfies
2
HkRkF + ks kF) ≤ h,
and the extent of balancing is quantified by
2	min{d,r}
kR - S kF ≤ ch - 2 X μi.
c	i=1
Especially, when d = 1, n ∈ N+, c = 1.
Since X = UR, Y = VS ⇒ R = U>X, S = V>Y, we have kRkF = kXkF, kSkF = kY kF,
and kR- SkF = U>X - V>YF = X - (UV>)YF. The we obtain the first and second
inequalities in Theorem 5.1. Also, by SVDjXY>∣∣F = Pm=I{d'n} μ ≤ ∣∣χ∣∣F ∣∣γ|尼,we obtain
the third inequality.
45