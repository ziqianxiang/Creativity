Published as a conference paper at ICLR 2022
Memory Augmented Optimizers for Deep
Learning
PaUl-Aymeric McRae *1, Prasanna Parthasarathi *1,2, Mahmoud Assran1,2, and Sarath Chandar1,3,4
1 Mila - Quebec AI Institute, Canada
2McGill University, Canada
3Ecole Polytechnique de Montreal, Canada
4Canada CIFAR AI Chair
Ab stract
Popular approaches for minimizing loss in data-driven learning often involve an
abstraction or an explicit retention of the history of gradients for efficient parameter
updates. The aggregated history of gradients nudges the parameter updates in
the right direction even when the gradients at any given step are not informative.
Although the history of gradients summarized in meta-parameters or explicitly
stored in memory has been shown effective in theory and practice, the question of
whether all or only a subset of the gradients in the history are sufficient in deciding
the parameter updates remains unanswered. In this paper, we propose a framework
of memory-augmented gradient descent optimizers that retain a limited view of
their gradient history in their internal memory. Such optimizers scale well to large
real-life datasets, and our experiments show that the memory augmented extensions
of standard optimizers enjoy accelerated convergence and improved performance on
a majority of computer vision and language tasks that we considered. Additionally,
we prove that the proposed class of optimizers with fixed-size memory converge
under assumptions of strong convexity, regardless of which gradients are selected
or how they are linearly combined to form the update step.
1 Introduction
Gradient-based learning involves minimizing a scalar-valued function F(θ) with respect to a parame-
ter vector θ ∈ Rd using an iterative procedure. When training over n examples of input-output pairs,
(xi, yi), the optimization problem boils down to solving minθ F(θ), where
1n
F (θ) = nfLi(θ) with Li(θ) = L(Mθ (xi),yi),
i=1
for some problem-dependent loss function L and a predictive model M parameterized by θ.
Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951) is one common method used to
tackle this problem, and is often preferred to full-batch Gradient Descent when the quantity of data
required to train θ is large, since it can be more efficient to measure a single component of the
gradient (or a mini-batch of gradients (Bottou, 1999)), and move in a noisy direction, than to compute
a full gradient at each time step. Several techniques have been proposed to further accelerate the
convergence of SGD (Ghadimi and Lan, 2016; Allen-Zhu and Yuan, 2016; Zhou et al., 2018). These
include approaches that maintain a knowledge of previous gradients implicitly by summarizing them
in a momentum buffer (Polyak, 1964b), and potentially adapting the learning rate based on the
gradient statistics (Duchi et al., 2011; Hinton et al., 2012; Zeiler, 2012; Kingma and Ba, 2015).
Optimization techniques such as SGD with Momentum (Polyak, 1964a), AdaGrad (Duchi et al.,
2011), RMSprop (Hinton et al., 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma and Ba, 2015)
maintain a set of buffers that track running moments of the gradients. Such light-weight techniques
tThe two authors contributed equally to this paper.
1
Published as a conference paper at ICLR 2022
have shown significant application advantage in practice, whereas in theory, algorithms that store all
of the gradients, like SAG (Roux et al., 2012) and SAGA (Defazio et al., 2014), can achieve better
convergence. A drawback of full-history methods is that the memory requirement linearly increases
with the size of the data.
Figure 1: Common optimization algorithms placed
over the spectrum of memory requirements. The
proposed algorithms the C variants have a fixed
sized memory to store gradients during training.
The spectrum of gradient-based approaches (Fig-
ure 1) that use a knowledge of the past gradients
has Adam, RMSprop and SGD with Momentum
on the one end and approaches like SAG and
SAGA on the other. An optimization algorithm
in the middle of this spectrum can use less mem-
ory than full-history methods while providing
richer updates than adaptive-gradient methods.
Limited-Memory BFGS (Nocedal, 1980) and its
online, stochastic form, oLBFGS (Schraudolph
et al., 2007) aim to exploit this trade-off, but may
not converge faster than SGD (Mokhtari et al.,
2015), and are thus disadvantaged compared to
accelerated SGD variants. As a stepping stone
towards reaping the advantages of both ends of
the spectrum, we propose using memory to augment conventional optimizers. This memory is used
to store a small set of critical gradients (e.g., gradients with a larger '2 -norm) that are occasionally
updated with newer gradients. Specifically, rather than storing gradients for all the examples in
the dataset, our proposed approach aims to find a smaller set of gradients to store that will aid
in the optimization process. Through extensive experiments we show that this marginal increase
in the memory requirement (Figure 1) provides accelerated convergence and even improved test
performances in majority of the experiments.
In this work, we:
•	Present a framework of memory-augmented optimizers compatible with popular gradient-
based algorithms.
•	Prove theoretically that such algorithms converge for smooth strongly convex objectives.
•	Show that the proposed memory augmented optimizers can lead to faster convergence and
better final performance through exhaustive empirical study on eight different architectures
among four different tasks (classification, language modeling, natural language inference,
and dialogue) and six different datasets.
•	Demonstrate the memory augmented optimizers’ robustness to gradient selection, replace-
ment , or the aggregation techniques to summarize information in the memory.
2	Memory-Augmented Optimizers
In this work, we propose augmenting standard optimizers with a fixed size memory of past gradients.
The gradients are stored in the limited memory only when they are deemed critical as defined by
their `2 -norm.
Gradient Selection to the Memory Letting gt := VθLi(θt) denote a component (or mini-batch)
gradient at time-step t, we use kgt k2 as a scalar indicator of the importance of the gradient, which
serves as a proxy for the priority of a gradient to remain in the memory buffer gc . In order to ensure
that the buffer eventually flushes out stale gradients, the proxy norms for gradients in the buffer are
scaled down by a hyperparameter decay factor, denoted decay ∈ [0, 1).
The proposed approach maintains a gradient buffer of fixed capacity C and stores the gradients
selected by a chosen heuristic. We refer our heuristic as the critical gradients, which stores the top C
gradients by using the '2-norm of gt to determine priority. Gradients in the buffer are stored as tuples
(kgt k2 , gt), akin to a key-value pair in a dictionary structure, where the key kgt k2 is referred to as
the proxy norm, and decayed at each time step t by a decay factor decay ∈ [0, 1). At any iteration
t during training, the gradient in a full capacity priority buffer with the smallest '2 proxy-norm is
2
Published as a conference paper at ICLR 2022
replaced by the current mini-batch gradient gt if kgt k2 is greater than the smallest `2 proxy-norm
in the buffer. The multiplicative decay factor ensures that the buffer is frequently refreshed with
more recent gradients. Note that only the proxy-norm used for the heuristic rule is decayed by the
decay factor decay ∈ [0, 1), while the gradient itself is not affected. This technique of storing the
critical gradients is general enough to be employed in any deep learning model, and can be easily be
combined with many existing adaptive gradient optimization algorithms, such as those described in
(Polyak, 1964b; Zeiler, 2012; Duchi et al., 2011; Hinton et al., 2012; Kingma and Ba, 2015).
Critical Gradient Stochastic Gradient Descent Critical Gradient Stochastic Gradient Descent
(SGDC) is the explicit integration of critical gradients into SGD. Specifically, the iteration comes as,
Θt+1 = θt - α ∙ aggr(gt, gc),	(1)
where gc is the set of critical gradients and aggr denotes an aggregation function which is used to
combine the critical gradients with the current-iteration gradient. We propose two possible functions
for aggr including mean, the average of gt and all critical gradients, and sum, the addition of gt to
the average of all critical gradients. Mathematically, for a buffer of size C these are defined as
mean(gt, gc) = c + 1(gt+ X gc)	Q)
gc ∈gc
sum(gt, gc) = gt + C X gc	(3)
gc∈gc
In general, we find optimization to be robust to the specific choice of aggregation function, but we
do observe that adaptive learning rate methods (Adam, RMSprop) perform best with mean, while
convential SGD (with or without Polyak momentum) performs best with sum.
Critical Gradients with Accelerated Methods Agarwal et al. (2019) show that curvature in
adaptive gradients can be estimated without loss of generality with a finite moving window of past
gradients. Following which we propose critical gradient extensions of SGD with Momentum (SGDM)
(Eq.4), RMSprop (Eq.5), and Adam (Eq.6). The retention of the critical gradients can be naturally
extended to more first-order optimization methods with the replacement of gt in those algorithms
with aggr (gt, gc). The update steps for the corresponding C versions are as follows:
mt = Y ∙ mt-1 + aggr(gt,gc)	(4)
θt+ι = θt - α ∙ mt
mt = β1mt-1 + (1 - βι) • aggr(gt, gc)
Vt = β2vt-ι + (1 - β2) • (aggr(gt, gc))2
E[g2]t =0.9 ∙ E[g2]t-1 + 0.1 ∙ aggr(gt, gC)2
θt + 1 = θt —
√E[g2]t + e
• aggr(gt, gC)
(5)
m t
Vt
mt
1 - β1
Vt
1 - β2
θt+ι = θt —U——m t
√Vt + €
(6)
η
The general algorithm to integrate critical gradients into any optimizer is provided in Algorithm 1 in
Appendix A. Replacing gt with aggr allows the optimization function to retain key gradients and
optimize the parameters. The acceleration provided by the topC gradients complements the adaptive
learning rate methods by not only computing the learning rate with respect to gt but also with respect
to the critical gradients from the past. We observe that all C -extended optimizers in our experiments
in § 5 lead to faster convergence than their vanilla, non memory-augmented counterparts.
3	Proof of Convergence
Consider the SGDC update in Equation (1), where aggr(gt, gc) outputs a linear combination of the
critical gradients in memory gk ∈ gc, and the stochastic gradient gt, which is computed at the current
parameters θt . When the staleness of gradients in the memory is bounded, we can prove convergence
of this method through the lens of linear multi-step methods. Formally, suppose there exists an integer
3
Published as a conference paper at ICLR 2022
K > 0 such that gk ∈ gc at iteration t, implies t - k < K, and suppose the objective F : Rd → R is
twice-continuously differentiable, L-Smooth, and μ-strongly convex, with 0 < μ ≤ L. Assume that
for all k, the stochastic gradient gk is a random vector satisfying
E[gk] = Vf(θk) and E [∣∣gk-Vf(θk)『]≤ σ2,
for some finite constant σ2. Letting Zk := gk 一 Vf (θk) denote the gradient noise at iteration k, we
take these gradient noise terms to be mutually independent. For examples of functions satisfying
these properties, see, e.g., (Nesterov, 2004; Bubeck, 2015). Examples of typical tasks satisfying these
assumptions are '2-regularized logistic regression and '2-regularized least-squares regression. Taken
together, these properties imply that the Hessian V2F (θ) exists, and for all θ ∈ Rd, the eigenvalues
of V2F(θ) lie in the interval [μ, L]. In this case, let θ? denote the unique minimizer of F.
To prove convergence, we view SGDC as a linear multi-step method, i.e., the parameters at the
current time step are updated with a linear combination of gradients from the past K time steps.
From this view, we can describe the evolution of the algorithm according to a discrete-time linear
dynamical system, the convergence of which is characterized by the spectral properties of the system
matrix. Our convergence theorem relies on the largest singular value of this system matrix, but can be
strengthened to instead rely on the spectral radius of the system matrix by constructing an appropriate
Lyapnuov function containing a time-invariant system matrix satisfying a particular Integral Quadratic
Constraint (IQC) (Lessard et al., 2016). However, one downside of the IQC framework is that one
must typically solve a semidefinite program to obtain explicit convergence rates (Hu and Lessard,
2017), whereas Theorem 1 (refer Appendix §B for proof) provides an analytical convergence rate.
Theorem 1 (Linear Convergence of SGDC). Let
q(α,K) :=	sup	ρ(Λ),
{wk∈[0,1]},{ηk∈[μ,L]}
where ρ(Λ) are the singular values of the numerical matrix
λ0 λ1	∙∙∙ λK-1
Λ :=	0	1	…0	∈ Rk×k,
0	0	…	1
with λ0 = 1 - αη0 and λk = -αwkηk for k > 0. If the step-size α > 0 is chosen sufficiently small,
such that q(α,K) < 1, then
E kθt+ι - θ*k2 ≤ q(α,κ)2t kθι - θ*k2 + ] * 2 σ2,
1 -q(α,K)2
where the expectation on the left hand side is with respect to the joint distribution over the random
noise variables ζk, . . . , ζ1.
Theorem 1 shows that SGDC can be made to converge exponentially fast to a neighbourhood of
the solution, the size of which is proportional to the variance of the stochastic gradients σ2 and
the step-size α2 . If K = 1 (i.e., the memory gc, at all times t, only contains the mostly recently
computed gradient), then SGDC reduces to regular stochastic gradient descent, and the rate q(α,K) in
Theorem 1 reduces to the well known convergence rate of SGD for smooth strongly convex functions
(maXη∈[μ,L] |1 一 αηL∣), and the variance bound reduces to the standard variance bound for SGD
with a constant step-size (α2σ2∕(1 — q2)), see, e.g., (Bottou et al., 2018; Assran and Rabbat, 2020).
For K > 1, it may be possible to choose the step-size α and the aggregation weights {wk} to obtain
accelerated convergence rates, faster than SGD. For example, accelerated gradient methods such
as Polyak and Nesterov momentum can be viewed as multi-step methods with K = 2. Note that
although K appears in the numerator of the coefficient of the variance term, it is also present in the
rate term in the denominator, where faster convergence rates, smaller q(α,K), directly lead to smaller
variance bounds. Note that expressing the convergence rate of a multi-step method as the roots of a
polynomial, as we do in Theorem 1, is not new; see, e.g., Polyak (1964a).
4	Related Work
Past-Gradients-Summarizing Optimizers Several popular optimization algorithms incorporate
the history of previous gradients through a summarizing variable. In practice, this is often done by
4
Published as a conference paper at ICLR 2022
means of a decaying average, which is updated with each iteration. SGD with Momentum (SGDM)
(Polyak, 1964b) uses this summary variable as a means of stabilizing descent along one path even if
the current gradient points are uninformative. A variant of SGDM uses Nesterov momentum (NAG)
(Nesterov, 1983; Sutskever et al., 2013) and applies the velocity to the current gradient as a correction
factor. SGDC shares similarity with NAG except in SGDC the “momentum" is computed only using
C selected gradients. In a different use of a summarizing variable, AdaGrad (Duchi et al., 2011) keeps
a running sum of squared gradients which is used to dynamically adjust the learning rate. AdaDelta
(Zeiler, 2012) and RMSprop (Hinton et al., 2012) replace AdaGrad’s running sum with a decaying
average of the squared gradients. Adam (and AdaMax) (Kingma and Ba, 2015) uses a combination
of momentum and a decaying-average-dependent learning rate. In general, the C variants (SGDC,
AdamC, RMSpropC, SGDMC) provide an additional “layer" to the base algorithms that ensures any
parameter computed within the algorithm incorporates the critical gradients. Although optimization
using a fixed size buffer has been in existence, the novelty of the C variants comes from adding a
heuristic to keep only the gradients that provide the largest displacements towards the minimum.
Memory-Enhanced Optimizers Our algorithm belongs to a class of optimization methods which
are aware of a limited history of gradients from previous iterations. One fellow algorithm from
this class is Limited-History BFGS (LBFGS) (Nocedal, 1980), an optimization technique which
approximates the Hessian of a function using a limited history of gradients, then uses the estimated
Hessian to compute the descent direction for optimization. An online version of LBFGS (oLBFGS),
well-suited for a machine learning context, was proposed by Schraudolph et al. (2007). These
LBFGS methods utilize a moving window of past gradients, rather than our idea to maintain “critical"
gradients based on a well-defined metric.
The storage of past gradients as a means to improve optimizer convergence is not a novel idea; both
Stochastic Accelerated Gradient (SAG) (Roux et al., 2012) and SAGA (Defazio et al., 2014) do so
in the context of accelerating stochastic gradient descent, while Stochastic Dual Coordinate Ascent
(Shalev-Shwartz and Zhang, 2013) uses previous gradients for coordinate ascent. Empirically and
theoretically, these techniques have been shown to yield good performance. The SAG and SAGA
algorithms differ in their weight update step, but both involve storage of up to n gradients, where
n is the number of training examples, which can be costly. Our method relies on the storage of a
fixed number of gradients and uses a heuristic to decide whether to store a gradient in a dataset-size-
independent memory. Our class of optimizers can natively run in a batch-based training loop unlike
originally presented SAG and SAGA. Gazagnadou et al. (2019) address this issue in their recent work.
We also extend our method beyond SGD to several other first-order optimization methods.
SAG and SAGA belong to the family of variance-reduced gradient-based algorithms (J. Reddi
et al., 2015). Such algorithms theoretically and empirically outperform SGD, though with the
trade-off of having a large cost in terms of memory or gradient evaluations. Some algorithms in
this framework attempt to bridge both costs, for instance a variant of Stochastic Variance Reduced
Gradient (SVRG) (Johnson and Zhang, 2013) stores intermediate gradients to avoid recomputing
them later, and StochAstic Recursive grAdient algoritHm (SARAH) (Nguyen et al., 2017) uses a
summarizing variable. Our method attempts to recapture the benefits of variance-reduced methods
without significant computation or memory overhead.
Optimizing Optimizers Our optimizer method also shares similarities with techniques which seek
to optimize optimizers; that is, techniques which automatically learn the best way to accomplish the
task of tuning the optimizer used in an outer-problem. The architecture proposed by Metz et al. (2020)
uses an LSTM per tensor of the network paramters, with each LSTM being passed the gradient norms
to influence the parameter update step, echoing our technique’s use of gradient norm as a critical
metric to parameter updates. In a similar vein, Andrychowicz et al. (2016) also use LSTMs, which
are fed the complete gradient, and use the LSTM’s recurrence as a means of implicitly accessing
past gradients. Li and Malik (2017) uses a reinforcement learning-based approach and defines a
state-space which includes a recent history of gradients. While our method equally maintains a
ledger of past gradients, unlike this latter approach We use '2-norm as a metric to pick out the critical
gradients instead of arbitrarily keeping a recent history.
Motivation for explicit memory Motivation for explicit memory-augmented optimizers over the
ones that maintain an implicit summarization of history comes from memory-augmented neural
5
Published as a conference paper at ICLR 2022
networks like Neural Turing Machines (NTMs) and their variants (Graves et al., 2014; 2016; Gulcehre
et al., 2018). While a simple recurrent architecture like LSTM (Hochreiter and Schmidhuber, 1997)
integrates information in its cell state which is a single vector, NTMs maintain a memory matrix
which stores a set of such cell state vectors. This richer parameterization of memory helps NTMs in
learning complex algorithmic tasks which are difficult to learn for an LSTM. Analogous to LSTMs
and NTMs, SGDM and our C -extended optimizers maintain a single vector and a set of vectors
respectively as their memory. While our current formulation for memory-augmented optimizers
uses heuristics to choose what information to store in the memory and how to use it, one could
automatically learn both criteria and hence learn to optimize.
Motivation for identifying critical gradients Likewise, recent work (Paul et al., 2021) has identi-
fied that samples with larger gradient norms can be used to identify a smaller set of training data that
is important for generalization, and thereby prune the data as a pre-processing step. Our proposed
approach of augmenting optimizers with memory buffer of critical gradients can be seen as a softer
version of data-pruning in that the buffer retains gradients through the heuristic, thereby performing
updates using only the important gradients throughout the entire training, without discarding data.
5	Experiments
SNLI-C RMSpropc
SNLI-BL RMSpropc
CIFARK
SNLI-C Adarnc
WIKITEXT-LS RMS
CIFARIO-CO SGDc
SNLI-I RMSpropc
MWoZ-RoB
WIKITEXT-LS
MWoZ-RoB A
WIKITEXT-LS A
PTB-LS
MW浸直翡
IRlO-CO AdamC
WIKITEXT-LS RMSf
SNLI-BL -
(a) Acceleration in Convergence
CIFARIOO-CO
PTB-LS A
WIKITEXT-LS &
CIFARIOO-CO SGDMc
PTB-LS RMSpropc
PTB-LS SGDMc
SNLI-C SGDMc

SNLI-BLAdamc
MWoZ-RoB RMS
MWoZ-RoB SGDMc
CIFARIO-CO RMSpropc
MWoZ-BL RMS
SNLI-C
MWoZ-BL S<
MWoZ-BLA
SNLI-I Adamc
SNLI-BL SGDMc
CIFARIOO-CO …
SNLI-BL RMSpropc
MWoZ-RoB SGDMc
MWoZ-BL SGDc
CIFARIO-CO Adamc
⅛Λ∕IKITEXT-LS SGDMc
JFARIOO-CO SGDMc
PTB-LS RMSpropc
PTB-LS SGDMc
SNLI-C SGDMc
MWoZ-BL SGDMc
MWoZ-BLAdamc
SNLI-I Adamc
SNLI-BLSGDMc
CIFARIOO-CO Adamc
SNLI-I SGDMc
CIFARIOO-CO RMSpropc
CIFARIO-CO SGDMc
CIFARIO-CO RMSpropc
MW0Z-B^RMS^gc
SNLI-C RMSpropc
SNLI-C Adarnc
CIFARIOO-CO SGDc
PTB-LS Adamc
SNLI-BLAdamc
MWoZ-RoB RMSpropc
CIFARIO-CO SGDc
SNLI-I RMSpropc
SNLI-ISGDc
MWoZ-RoBSGDc
WIKITEXT-LS SGDc
MWoZ-RoB Adamc
WIKITEXT-LS Adamc
PTB-LS SGDc
(b) Improvement over the Test performance
Figure 2: (a) Showcases the acceleration in convergence (the unit being the number of gradient
updates with batch-size fixed) using a validation set provided by the C variants, compared pairwise
(eg: Adam vs AdamC), on the different tasks.(b) denotes the difference in performance of the models
trained with C variants with respect to its corresponding base method on the test set. A positive
value indicates that C variant performed better than its base method in that task. The side-by-side
comparison allows us to observe that the proposed optimizers enabled accelerated convergence to
better test results. Of the 9 tasks considered, the C optimizers performed the best in 6 tasks. Further,
(b) shows that C version of the base optimizers enhanced the performance to as much as 23%.
We compare the proposed class of optimizers augmented with memory (C variants) with their vanilla
versions on a variety of deep learning architectures. To understand the performance on common
deep learning datasets, we experiment with shallow/deep convolutional neural network architectures
(CO) on CIFAR 10/100 (Krizhevsky et al., 2009) respectively; Bi-LSTM (BL), InferSent (I), and
a text-based convolutional architecture (C) on the Stanford Natural Language Inference (SNLI)
(Bowman et al., 2015) dataset; LSTM on word level language modeling (LS) with the PennTreeBank
(Marcus et al., 1993), and WikiText (Merity et al., 2016) datasets; RoBERTa-Base (RoB) and Bi-
LSTM (BL) on language generation in dialogue task with MultiWoZ 2.0 dataset (Budzianowski
et al., 2018). Additionally, we preform analysis experiments using logistic regression (LR) and
multi layer perceptrons (MLP) on the MNIST digit classification dataset (LeCun and Cortes, 2010).
Across the experiments, we compare all the 8 optimizers averaged over 5 different runs with different
6
Published as a conference paper at ICLR 2022
seed values— Adam, SGD, SGDM, RMSprop, AdamC, SGDC, SGDMC, and RMSpropC on9
tasks — CIFAR100, CIFAR10, SNLI-I, SNLI-C, SNLI-BL, MWoZ-RoB, MWoZ-BL, PTB-LS, and
WIKITEXT-LS*. For a fair comparison, we follow the standard experimental protocol for all the
experiments and tune all the hyperparameters of an optimizer individually for every task.
Our experimental results are aggregated from 5 independent runs, with the hyperparameters for
each optimizer extensively tuned. This involves tuning the learning rate in all optimizers, the topC
and decay parameters in all C algorithms, and all optimizer-specific hyperparameters in both the
vanilla versions and their C counterparts. A full description of the values used to tune the various
parameters, architecture and dataset details are in Appendix §E, §D, and §C respectively. The best
set of hyperparameters was selected based on which ones yielded the best validation performance (i.e.
highest accuracy, highest BLEU score, or lowest perplexity).
Figure 2(a) shows that the proposed C variants provided a consistent acceleration in convergence
over their vanilla versions. The accelerated convergence also resulted in improved test performance,
as highlighted in Figure 2(b) (Exhaustive list is provided in Appendix §G). In addition to the
aforementioned improvements in vanilla-vs-C comparisons, the C optimizers yielded the best test
performance across all 8 optimizers in 6/9 tasks. Of the three remaining tasks, the C versions
performed close to the vanilla versions. In the case of MWoZ - BL RMSpropC the C version
showed marginally accelerated convergence close to the optimal solution. Furthermore, Figure 3
compares the validation performance after each epoch when trained with the different optimizers
on the different tasks. One immediate observation was that in language experiments - SNLL PTB,
MWoZ-the C versions stayed above most if not all of the vanilla optimizers. While that separation is
not as clear in the vision tasks, we observed that the pairwise comparison mostly put the C optimizer
at an advantage in accelerated convergence.
(a) SNLI/InferSent (b) SNLI/BLSTM
(c) SNLI/ConvNet (d) MWoZ/BiLSTM
(e) CIFAR10/ConvNet (f) CIFAR100/ConvNet (g) PTB/LSTM (h) WikiText-2/LSTM
Figure 3: The validation performance comparison of the proposed optimizers and their vanilla
versions showed that, in general, the proposed optimizers showed accelerated convergence than
their counterparts. Also, in the 3 tasks — SNLI-I (a), SNLI-BL (b) and MWoZ-BL (d) where the
C methods are not the optimal — only falls short by a negligible margin. For ease of visualization,
confidence intervals across seeds are suppressed.
*Code to reproduce results: https://github.com/chandar-lab/CriticalGradientOptimization.
Optimizer package: https://github.com/chandar-lab/CGOptimizer
7
Published as a conference paper at ICLR 2022
6	Analysis
Buffer Staleness Bound A key assumption behind the theoretical convergence of SGDC is the
existence of an upper bound on the buffer gradients’ staleness (i.e the number of iterations a gradient
remains in the critical buffer). We analysed the distribution of average number of steps a gradient
stays in the memory for the different values of topC and decay in Figure 4. We observe that tuning
the parameters ensures that gradients in the buffer are refreshed frequently, allowing for a renewal
of information. Later, in analysing the effect of decay and topC on the performance in a task, we
observe that the higher performance of the C methods correlate with the findings of this analysis.
Use of Critical Gradients Our Critical Gradient method utilizes a buffer which is filled by using
a norm-based, “King of the Hill" sampling of incoming gradients, i.e. gradients always stored and
sorted in decreasing order by '2-norm, With the smallest norm always being the one removed. We
conduct ablation studies to probe the three assumptions of critical gradients using LSTM architecture
with PTB dataset.: (a) the use of norm as a controlling metric, (b) the removal of the smallest-norm
entry when adding to a full buffer, and (c) the update step depending on an ensemble of buffer
gradients via the mean rather than on select gradients.
Figure 4: Histograms showing ages of the buffer as recorded at the end of every epoch, across five
seeds for several decay values, for a MLP on MNIST. 0.99 decays at a much slower rate, keeping
stale gradients in the memory, while 0.7 replaces the gradients frequently. Also, the size of the buffer
(topC ), as expected, correlates directly with retention of stale gradients.
Addressing assumption (a) we test the use of maximal norm sampling (Figure 5(a)) by comparing
with Cosine Similarity Sampling (CSS), Cosine Diversity Sampling (CDS), Mean-Norm Diversity
Sampling (MNDS), and random sampling via "coin toss". Figure 5(b) depicts our algorithm alongside
different gradient replacement methods, including Random Replacement (RR) and Norm-Controlled
Probabilistic Replacement (NCPR), as well as comparing against keeping a running First-In-Fist-Out
(FIFO) queue and only using the smallest ("bottomC") gradients. Finally we compare the typical
optimizer aggregation method with the buffer gradients minimum, maximum, and median norm
(Figure 5(c)). A full description of these techniques can be found in Appendix §F.9.
FIFO
NCPR
RR
bottomC
Figure 5: (a) compares gradient selection techniques based on different metrics, (b) compares
different replacement strategies when flushing out a gradient from memory and (c) compares choice
of aggregating the gradients in the buffer. The algorithm converges in all the different choices.
8
Published as a conference paper at ICLR 2022
We observe in most cases, the sampling (a) and replacement (b) strategies for gradients in the
buffer and all ablations converge regardless of these considerations. While the Critical Gradients
formulation is not unique as a high-performing implementation of a memory-augmented optimizer, it
remains conceptually straightforward, easy to implement, and the buffer properties are deterministic
as compared to some of our ablation studies which were probabilistic. Although, a rigorous analysis
is necessary to verify the generality of this result.
Experiments comparing the performance of mean-based aggregation against aggregation based on
individual gradients show that aggregation based on the ensemble must be employed, and that the
optimizers benefit from the added information.
Effect of hyperparameters We study the performance’s sensitivity to decay and topC using
AdamC on different tasks.
(a) SNLI, decay
(b) SNLI, topC
(c) CIFAR-100, decay (d) CIFAR-100, topC
Figure 6: We observe that non-trivial choices of decay and topC provided best performance for
AdamC with different architectures on SNLI task. Also, the usefulness of explicit memory aligns
with the evidence on gradient staleness from Figure 4.
The decay ∈ [0, 1) parameter allows to tune the retention of critical gradient information from
the past. Setting decay = 0 reduces the priority of new gradients added to the buffer, making the
C variants behave like an average over a moving window of length topC . On the other extreme,
decay = 1 does not decrease the priorities of the past gradients and allows them to be in the buffer
for a much longer period hindering the convergence of the optimizers. Also, decay maintains
the staleness of the gradients by exponentially decaying their priority in the memory buffer. We
experimented with AdamC optimizer for the different values of decay with every other parameter
fixed (Figure 6). We observed that decay values closer to 1 did not have any advantage at best or
leads to inferior performance on an unseen set at worst in most of the tasks.
topC ∈ [0, ∞) defines the size of the buffer that holds the past gradients. When topC is set to 0,
C variants behave like the base algorithms and when topC is the size of the dataset, the optimizer
becomes SAG-like. Also, topC is directly proportional to the staleness of the gradients in the buffer.
A higher topC , though, guarantees better expressibility also becomes responsible for more stale
gradients in the buffer. Bengio et al. (2020) also observes a similar issue in momentum used in
TD-learning and proposes a correction factor to alleviate the issue. Here, the staleness is contained
with the parameters topC and decay . As an empirical evidence, we observe that the optimizers
perform well on the validation set with lower topC values. In most of the experiments, topC > 20
was either not useful or was hurting the performance (also in Appendix F).
7	Conclusion
We propose a general method to enhance the performance of an optimizer with an augmented
memory buffer to maintain a limited set of critical gradients from the history. The proposed memory
mechanism, when integrated with several state-of-the-art optimizers, accelerated the learning in all
cases and also improved their performance in several cases. The C family of optimizers proposed in
this paper are the first steps towards designing smart optimizers that can learn to optimize. Some of
the immediate future research directions include learning what to store in the memory and learning
how to integrate the information in memory to the gradient update steps.
Acknowledgements We would like to acknowledge Compute Canada and Calcul Quebec for
providing computing resources used in this work. The authors would also like to thank members of
Chandar Research Lab, Mila for helping with the code reviews and reviewing the manuscripts. SC is
supported by a Canada CIFAR AI Chair and an NSERC Discovery Grant.
9
Published as a conference paper at ICLR 2022
References
Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv, 2018.
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. In International Conference on Machine Learning,
pages 102-110. PMLR, 2019.
Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex
objectives. In International conference on machine learning, pages 1080-1089. PMLR, 2016.
Marcin Andrychowicz, Misha Denil, Sergio G6mez Colmenarejo, Matthew W Hoffman, David Pfau,
Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by
gradient descent. In NeurIPS, 2016.
Mahmoud Assran and Michael Rabbat. On the convergence of nesterov’s accelerated gradient method
in stochastic settings. arXiv preprint arXiv:2002.12414, 2020.
Emmanuel Bengio, Joelle Pineau, and Doina Precup. Correcting momentum in temporal difference
learning. Deep Learning Workshop, NeurIPS, 2020.
Leon Bottou. On-line learning and stochastic approximations. On-Line Learning in Neural Networks,
1999.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal, September
2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:
//www.aclweb.org/anthology/D15-1075.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning, 8(3-4):231-357, 2015.
PaWeI Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman
Ramadan, and Milica Gasic. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for
task-oriented dialogue modelling. In EMNLP, 2018.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In NeurIPS, 2014.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. JMLR, 2011.
Nidham Gazagnadou, Robert Gower, and Joseph Salmon. Optimal mini-batch and step sizes for
SAGA. In ICML, 2019.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv, 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio G6mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. Nature, 2016.
Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. Dynamic neural turing
machine with continuous and discrete addressing schemes. Neural computation, 2018.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent, 2012.
10
Published as a conference paper at ICLR 2022
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
Bin Hu and Laurent Lessard. Dissipativity theory for nesterov’s accelerated method. In International
Conference on Machine Learning, pages 1549-1557. PMLR, 2017.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. On variance
reduction in stochastic gradient descent and its asynchronous variants. In NeurIPS, 2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using Predictive variance
reduction. In NeurIPS, 2013.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In Yoshua Bengio
and Yann LeCun, editors, ICLR, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiPle layers of features from tiny images. In
Citeseer, 2009.
Yann LeCun and Corinna Cortes.	MNIST handwritten digit database.
httP://yann.lecun.com/exdb/mnist/, 2010. URL http://yann.lecun.com/exdb/mnist/.
Yann LeCun, Patrick Haffner, Leon Bottou, and YoShUa Bengio. Object recognition with gradient-
based learning. In Shape, contour and grouping in computer vision. SPringer, 1999.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of oPtimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. Rcv1: A new benchmark collection for
text categorization research. JMLR, 2004.
Ke Li and Jitendra Malik. Learning to oPtimize. In ICLR, 2017.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly oPtimized bert Pretraining
aPProach. arXiv, 2019.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corPus of english: The Penn treebank. Computational Linguistics, 1993.
StePhen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein.
Tasks, stability, architecture, and comPute: Training more effective learned oPtimizers, and using
them to train themselves. arXiv, 2020.
Aryan Mokhtari, Alej, and ro Ribeiro. Global convergence of online limited memory bfgs. Journal
of Machine Learning Research, 2015.
Y. E. Nesterov. A method for solving the convex Programming Problem with convergence rate
o(1/k2). Dokl. Akad. Nauk SSSR, 1983.
Yurii Nesterov. Introductory lectures on convex oPtimization: a basic course. Kluwer Academic
Publishers, Pages 71-81, 2004.
Lam M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for machine
learning Problems using stochastic recursive gradient. In ICML, 2017.
Jorge Nocedal. UPdating quasi-newton matrices with limited storage. Mathematics of Computation,
1980.
Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. DeeP learning on a data diet: Finding
imPortant examPles early in training. NeurIPS, 2021.
Boris T. Polyak. Some methods of sPeeding uP the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964a.
11
Published as a conference paper at ICLR 2022
B	.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 1964b.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics, 1951.
Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential
convergence _rate for finite training sets. In NeurIPS, 2012.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by
error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,
1985.
Nicol N. Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-newton method for online
convex optimization. In AISTATS, 2007.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss
minimization. JMLR, 2013.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. JMLR, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In ICML, 2013.
Oriol Vinyals and Quoc Le. A neural conversational model. arXiv, 2015.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv, 2012.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex
optimization. arXiv preprint arXiv:1806.07811, 2018.
12
Published as a conference paper at ICLR 2022
A	General Critical Gradient Algorithm
We present the pseudocode for the Critical Gradient algorithm below.
Algorithm 1 Critical Gradients Optimization
Define #epochs E, #gradient updates B, model θ, learning rate α
Define optimizer O(gt , θt , h), which produces a parameter update given gradient gt, parameter θt and
hyperparameters h
Define aggregation function aggr(gt, gc) which outputs a linear combination of gt and the entries in gc
Initialize empty critical gradient buffer {gc } with decay rate d
for s = 0 to E do
for t = 0 to B do
Sample an input-output pair (xi, yi)
gt — Vθ Li(θt)
θt+ι — O(aggr(gt, gc), θt, h)
if {gc } is not full then
Add gt to {gc }
else
if ||gt||2 > ||min({gc})||2 then
Remove min({gc}) from {gc}
Add gt to {gc }
end if
end if
for gc ∈ {gc} do
||gc ||2 - d ∙ ||gc ||2
end for
end for
end for
B Convergence Proof
Proof. Recall that if F : Rd → R is twice continuously differentiable, then for all x, y ∈ Rd
VF(y) = VF(x) + Z V2F(x + t(y — x))dt (y — x).
0
(7)
Letting rk := θk — θ? denote the suboptimality of the parameters at iteration k and applying (7) with
y = θk and x = θ?, we get that
gk = Hkrk + ζk, where Hk = Z V2f(θ? + trk)dt,
0
where ζk denotes the gradient noise at iteration k . Using this and bounded staleness assumption, we
have that
K-1	K-1
aggr(gt, Gt) =	wt,kgt-k =	wt,k(Ht-krt-k + ζt-k),	(8)
k=0	k=0
where {wt,k}kK=-01 are non-negative scalars in the closed interval [0, 1] used to linearly aggregate
the gradients in memory with the most recent gradient gt. For gradients gt-k that are not stored
in memory, the corresponding aggregation scalar wt,k is equal to 0. Without loss of generality, we
take wt,0 = 1, since we can always incorporate it into the step-size α and re-scale the other weights
wt,1, . . . , wt,K-1 accordingly. Substituting (8) into (1) and subtracting θ? from each side, we get that
K-1
rt+1 = rt — α	wt,k(Ht-krt-k + ζt-k).
k=0
Thus, we have that SGDC evolves according to the linear system
rt+1
rt
.
.
.
rt+1-K
rt
rt-1
At .	— α
.
.
rt-K
K-1
k=0 wt,k ζt-k
0
.
.
.
0
where At
Tt,0	Tt,1
0I
00
Tt,K-1
0
• ∙ ∙ ∙
I
13
Published as a conference paper at ICLR 2022
with Tt,0 = I - αHt and Tt,k = -αwt,kHt-k for k = 1, . . . , K - 1. Unrolling the recursion, we
have that
rt+1
rt
.
.
.
rt+1-K
^θι - θ?-
0
(At …Ai)	.	— α
.
.
0
'PkKK=O wt,kCt-k ]	ΓPK=01 wj,k ζj-k
0	t— 1	I	0
.	-a X (At …Aj + 1 ) I	.
.	j=i	I	.
00
(9)
from which it is clear that we may expect convergence properties to depend on the spectral properties
of the matrices {Ak}tk=1. The convergence rate of SGDC will therefore depend on the largest
singular value of the matrices {Ak}tk=1. From Polyak (1964a, Lemma 4), we have that
kAjk ≤	sup ρ(Λ),	(10)
{wfc∈[0,1]},{ηfc∈[μ,L]}
where ρ(Λ) are the singular values of the numerical matrix
∈ RK×K,
λ0	λ1	∙∙∙	λK-1
Λ	:=	01	…	0
...............
0	0	…	1
with λ0 = 1 - αη0 and λk	=	-αwkηk	for k	=	1, .	. . , K - 1. Note that Polyak (1964a, Lemma
4) originally refers to the eigenvalues of a block matrix, however, in the case where the blocks are
commutative, it is straightforward to extend the lemma to describe the singular values. To see this,
simply apply the original lemma to the eigenvalues of the matrix Λ>Λ, where commutativity of the
individual blocks (Tj,0 , . . . , Tj,K are all symmetric and therefore commutative) allows you to specify
the eigenvalues of the new block matrix in terms of the eigenvalues of the original blocks.
Letting q(α,K) denote the right hand side of (10), we have that kAj k ≤ q(α,K) for all j = 1, . . . , t,
and q(α,K) has an analytic definition as the roots of a polynomial. Taking 2-norms, applying submul-
tiplicativity of matrix norms, and using the bounded variance assumption along with the fact that the
noise terms {ζk}tk=0 are mutually independent and that the aggregation weights {wt,k}t≥0,k∈[1,K)
are in the closed interval [0, 1] for all t, k, we have that
t
E krt+ιk2 ≤ q(t,κ) kθι - θ*k2 +α2σ2K X *K)，	(η)
j=1
where the expectation on the left hand side is with respect to the joint distribution over the random
noise variables ζk, . . . , ζ1.
By assumption, since the step-size α is chosen such that q(α,K) < 1, equation (11) simplifies as
E krt+ιk2 ≤ q(α,κ)2t |仇 —。?『+ 1 ^K 2 σ2,
1 - q(α,K)
where we have implicitly used the upper bound on the limit of a geometric sequence.
C Data set distributions
Table 1 details the train-valid-test splits of all the data sets used in the experiments.
Dataset	#Train	#Valid	#Test
CoVtyPe	5000	N/A	-NA
rcv1	5000	N/A	N/A
MNIST	50K	10K	10K
CIFAR-10	40K	10K	10K
CIFAR-100	40K	10K	10K
SNLI	550K	10K	10K
WikiTeXt	2M	213K	241K
Penn TreeBank	890K	70K	78K
MultiWoZ		115K(1.5M)	20K (200K)	20K (200K)
Table 1: The splits of the data sets in number of samples. For WikiText, Penn TreeBank and MultiWoZ
as the models are trained on a language modeling objective, the splits are given in number of tokens.
For MultiWoZ, the number of utterances is provided with the number of tokens in parentheses.
14
Published as a conference paper at ICLR 2022
D	Experiment Details
We tested the proposed class of optimizers on different tasks that have potentially different loss
surfaces in large parameter spaces. To cast a wide net and ensure that we capture a plethora of neural
architectures, we vary the network designs by using fully-connected, recurrent (Rumelhart et al.,
1985), convolutional (LeCun et al., 1999), dropout (Srivastava et al., 2014), ReLU (Agarap, 2018)
layers and a large Transformer language model architecture - RoBERTa-Base (Liu et al., 20l9) across
different tasks. A brief description of the data and architectures used follows.
CIFAR-10 is an image classification task with 10 classes. We train a shallow Convolutional Neural
Network (ConvNet) with dropout and ReLU activation using the different optimizers and their C
variants.
CIFAR-100 is an image dataset similar in size to CIFAR-10 but with 100 classes of images. This task
was approached using a Convolutional Neural Network with three batch-normalized convolutional
blocks and one fully-connected block with dropout applied between blocks.
We experiment with the commonly-used language inference dataset, Stanford Natural Language
Inference (SNLI) dataset, to compare the performances of C variants training on three different
text encoder architectures: 4-layer convolutional network (ConvNetEncoder), 2 layer bi-directional
encoder with 2 linear projection layers (InferSent) and a unidirectional LSTM with 2 recurrent layers
(LSTMEncoder). The classifier on top of the representations learned by the encoder architectures is a
2-layer fully-connected Multi-Layer Perceptron with dropout connection.
The Penn Tree Bank (PTB) is a syntax-annotated text corpus sourced from stories from the Wall
Street Journal. We use this corpus for a word-level language modeling task using a 1-layer LSTM
with dropout. Gradient clipping is employed to avoid exploding gradients. We evaluate the model by
measuring perplexity (PPL) in the validation set; lower PPL scores are preferred.
MultiWoZ 2.0 is a popular dataset that has human-to-human goal-oriented conversations on different
topics. The objective is to generate the next utterance conditioned on the history of utterances
in the conversation. We experiment with a very large language model architecture - RoBERTa-
Base - and a BiLSTM SequenCe-to-Sequence architecture (Vinyals and Le, 2015) for the next
utterance prediction. RoBERTa was trained with CausalLM-Head while BiLSTM model was an
encoder-decoder architecture with a Bi-LSTM encoder and LSTM with Attention decoder.
We use models with varying size of trainable parameters as shown in Table 2.
Table 2: We experimented with models with different sizes for a comprehensive study of the proposed
C variants.
Model		#Params	Dataset(s)
Logistic Regression NeuralNetwork ConvNet LSTM LSTMEncoder InferSent ConvNetEncoder RoBERTa-Base Bi-LSTM		8K (55/64) 25K 600K (62K) 20K (600K) 600K 1.2M 3.1M 125M 574K		MNIST (rcvl/covtype) MNIST CIFAR-100 (CIFAR-10) PTB (WikiText) SNLI SNLI SNLI MultiWoZ MUItiWOZ	
We use the same hyperparameter initialization for comparisons with base optimization methods and
tune the learning rate hyperparameter using grid search. The primary objective of the experiments
is to verify the consistency of convergence to better solutions across tasks and architectures. All
reported results are averaged over 5 different runs with different random seed values.
D. 1 Model Hparams
All experiments are averaged for 5 different runs with different seeds. We use PyTorch 1.1 for the
experiments and use their implementation of the base optimizers available in torch.optim. The
details of hyperparameters used for the model are in Table 3.
15
Published as a conference paper at ICLR 2022
Table 3: Architecture details for models used in experiments.
Model		Dataset	#Layers	#Hidden	ReLU/DroPout
Log. Reg.	covtype	1	-N/a-	NoTNo
Log. Reg.	rc1	1	N/A	No/No
Log. Reg.	MNIST	1	N/A	No/No
Log. Reg.	synthetic	1	N/A	No/No
MLP	MNIST	2	32	Yes/No
CNN	CIFAR-10	5	120	Yes/Yes
CNN	CIFAR-100	9	4096	Yes/Yes
LSTM	PTB	1	128	No/No
LSTM	WikiText	1	128	No/No
ConvNetEnc.	SNLI	2	200	Yes/Yes
LSTMEncoder	SNLI	2	200	No/Yes
InferSent	SNLI	2	200	Yes/Yes
RoBERTa-Base	MultiWoZ	12	768	Yes/Yes
Bi-LSTM Attn	MUItiWOZ	4	200	No/No
D.2 Runtime Statistics
We logged the approximate time for each epoch for different values of topC across the different
models. Although the results are populated from the experiments with Adam and its C variants, the
results can be extended to the other optimizers and its variants. These times are reported in Table 4.
Table 4: Approximate time taken for one epoch of training of the models in the public repositories on
the different data sets. The time is clocked in minutes. The time per epoch increases linearly with
increase in topC over the (B)ase method. Although for lower values of topC the time per epoch
is smaller, there is still room for improvement to bring down the time with smart update rules as
discussed in §7.	_______________________________________________________________
Dataset	Model	B	C5	C10	C20	C20	C100
MNIST	Log.Reg	0.3	-01-	0.5	0.6	0.9	-.5-
	Neural-Net	0.3	0.5	0.6	0.8	1.4	2.3
PTB	LSTM	0.08	0.4	0.6	0.9	2	6
WikiText	LSTM	0.6	1	1.4	2.6	5	10
CIFAR-10	CNN	0.5	0.9	1	1.5	3	5
CIFAR-100	CNN	0.75	1.5	2	4	6	12
	LSTMEnc.	1	3	6	10	25	45
SNLI	InferSent	2	4	9	20	35	50
	ConvNet	1	4	8	20	40	65
MultiWoZ	RoBERTa	15	25	35	50	NA	NA
	BiLSTM	3	4	5	5	7	10
E Hyperparameters
E.1 Range of Hparams
We present the range of hyperparameters used in our experiments (Table 5). Optimizer-specific
parameters were used on both their vanilla and C versions.
Table 5: The hyperparameter configurations of the different optimizers in the experiments.
Hparam	Choices
Adam βι Adam β2 RMSProP ɑ SGDM momentum learning _rate decay topC	{0.9, 0.99,0.999} {0.99, 0.999, 0.9999} {θ.9, 0.99,0.999} {0.9, 0.99,0.999} {0.1,0.01,0.001,0.0001, 0.00001} {0.7, 0.9,0.99} {5,10, 20}
16
Published as a conference paper at ICLR 2022
E.2 HParams of the Best Configurations
The learning rates and other hyperparameters of the optimizers used in the results reported in the
paper are listed in Tables 6, 7, 8 and 9.
Table 6: Best hyperparameter configurations for CIFAR-10 and CIFAR-100 image classification
tasks.____________________________________________________________________________
Dataset	Model	Optimizer	Learning Rate	topC	decay	Momentum	βl	β2	ɑ
		Adam C	0.0001	5-	07-	N/A	0.99	0.99	N/A
		Adam	0.001	N/A	N/A	N/A	0.9	0.99	N/A
		RMSpropC	0.0001	5	0.7	N/A	N/A	N/A	0.9
CIFAR-10	Shallow	RMSprop	0.0001	N/A	0	N/A	N/A	N/A	0.9
	ConvNet	SGDMC	0.001	5	0.7	0.9	N/A	N/A	N/A
		SGDM	0.001	N/A	N/A	0.9	N/A	N/A	N/A
		SGDC	0.01	5	0.99	N/A	N/A	N/A	N/A
		SGD		0.01	N/A	N/A	N/A	N/A	N/A	N/A
		AdamC	0.00001	20-	07-	N/a-	-09-	0.9999	N/A
		Adam	0.0001	N/A	N/A	N/A	0.9	0.9999	N/A
		RMSProPC	0.00001	20	0.7	N/A	N/A	N/A	0.99
CIFAR-100	Deep	RMSprop	0.0001	N/A	N/A	N/A	N/A	N/A	0.99
	ConvNet	SGDMC	0.001	20	0.7	0.9	N/A	N/A	N/A
		SGDM	0.01	N/A	N/A	0.9	N/A	N/A	N/A
		SGDC	0.01	5	0.9	N/A	N/A	N/A	N/A
		SGD			0.1	N/A	N/A		N/A	N/A	N/A	N/A
Table 7: Best hyperparameter configurations for language modelling tasks on WikiText and PennTree-
Bank datasets.________________________________________________________________________
Dataset	Model	Optimizer	LR	topC	decay	Momentum	βl	β2	ɑ
		AdamC	0.0001	5-	0.9	N/a-	0.9	0.999	N/A
		Adam	0.0001	0	0	n/a	0.9	0.999	N/A
		RMSProPC	0.0001	20	0.9	N/A	N/A	N/A	0.9
PTB		RMSprop	0.0001	0	0	N/A	N/A	N/A	0.9
		SGDMC	0.1	10	0.9	0.9	N/A	N/A	N/A
		SGDM	0.1	0	0	0.9	N/A	N/A	N/A
		SGDMC	0.1	2	0.95	N/A	N/A	N/A	N/A
	LSTM	SGD		0.1	0	0	N/A	N/A	N/A	N/A
		AdamC	0.0001	20-	09-	NTa-	-09-	0.9999	N/A
		Adam	0.0001	0	0	N/A	0.9	0.9999	N/A
		RMSProPC	0.0001	20	0.9	N/A	N/A	N/A	0.9
Wikitext		RMSprop	0.0001	0	0	N/A	N/A	N/A	0.9
		SGDMC	0.1	10	0.9	0.9	N/A	N/A	N/A
		SGDM	0.1	0	0	0.9	N/A	N/A	N/A
		SGDMC	0.1	2	0.95	N/A	N/A	N/A	N/A
		SGD		0.1		O	0		N/A	N/A	N/A	N/A
Table 8: Best hyperparameter configurations for text generation for dialogue on MultiWoZ2.0 dataset.
Dataset	Model		Optimizer	LR	topC	decay	Momentum	βl	β2	ɑ
		AdamC	0.00001	5	07-	N/A	0.99	0.999	N/A
		Adam	0.00001	0	0	N/A	0.99	0.999	N/A
		RMSProPC	0.0001	5	0.7	N/A	N/A	N/A	0.99
	RoBERTa-Base	RMSprop	0.00001	0	0	N/A	N/A	N/A	0.99
		SGDMC	0.01	5	0.7	0.9	N/A	N/A	N/A
		SGDM	0.01	0	0	0.9	N/A	N/A	N/A
		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A
MultiWoZ2.0		SGD		0.1	0	0	N/A	N/A	N/A	N/A
		-AdamC	0.001	5-	07-	N/A-	0.99	0.999	N/A
		Adam	0.001	0	0	N/A	0.99	0.999	N/A
		RMSProPC	0.001	5	0.7	N/A	N/A	N/A	0.99
	Bi-LSTM	RMSprop	0.001	0	0	N/A	N/A	N/A	0.99
		SGDMc	0.1	5	0.7	0.9	N/A	N/A	N/A
		SGDM	0.1	0	0	0.9	N/A	N/A	N/A
		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A
		SGD		0.1		0		0		N/A	N/A	N/A	N/A
F Analysis
F.1 Note on aggregation
The aggregation function aggr lies at the core of integrating critical gradients into an algorithm. In
§2 we presented mean and sum function for aggregation. The notable difference in these methods
is the importance of the current gradient, as it is more heavily weighted with sum (which does not
scale it) than with mean (which scales it by a factor of ^?3+]. While our algorithms converged
17
Published as a conference paper at ICLR 2022
Table 9: Best hyperparameter configurations for text classification for language inference on SNLI
dataset._________________________________________________________________________
Dataset	Model		Optimizer	LR	topC	decay	Momentum	β1	β	ɑ
		AdamC	0.0001	5-	07-	N/a-	0.99	0.999	N/A
		Adam	0.001	0	0	N/A	0.99	0.999	N/A
		RMSProPC	0.0001	5	0.7	N/A	N/A	N/A	0.99
	InferSent	RMSprop	0.001	0	0	N/A	N/A	N/A	0.99
		SGDMC	0.01	5	0.7	0.9	N/A	N/A	N/A
		SGDM	0.1	0	0	0.9	N/A	N/A	N/A
		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A
		SGD		0.1	0	0	N/A	N/A	N/A	N/A
		-AdamC	0.0001	5-	07-	N/A-	0.99	0.999	N/A
		Adam	0.001	0	0	N/A	0.99	0.999	N/A
		RMSProPC	0.0001	5	0.7	N/A	N/A	N/A	0.99
SNLI	ConvEncoder	RMSprop SGDMC	0.001 0.01	0 5	0 0.7	N/A 0.9	N/A N/A	N/A N/A	0.99 N/A
		SGDM	0.1	0	0	0.9	N/A	N/A	N/A
		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A
		SGD		0.1	0	0	N/A	N/A	N/A	N/A
		AdamC	0.0001	5-	07-	N/A-	0.99	0.999	N/A
		Adam	0.001	0	0	N/A	0.99	0.999	N/A
		RMSProPC	0.0001	5	0.7	N/A	N/A	N/A	0.99
	BLSTMEncoder	RMSprop	0.001	0	0	N/A	N/A	N/A	0.99
		SGDMC	0.01	5	0.7	0.9	N/A	N/A	N/A
		SGDM	0.1	0	0	0.9	N/A	N/A	N/A
		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A
		SGD		0.1		0		0		N/A	N/A	N/A	N/A
using both aggregation methods, we found that SGDC and SGDMC demonstrated better validation
performance using sum whereas RMSpropC and AdamC performed better using mean. This is
likely due to both RMSprop and Adam having adaptive learning rates, allowing them to be more
robust to changes in the scaling of the gradients.
F.2 Convergence in Convex Tasks
一SGDc
-Adamc
SGDMc
——RMSpropc
0.05
O IO 20
epoch
(a) covtype
0	10	20
epoch
(b) MNIST
0	10	20
epoch
(c) rcv1
---Adamc
SGDc
SGDMc
RMSpropc
---Adamc
SGDMc
SG DC
---RMSpropc

Figure 7: The training on the three different convex datasets show that the C variants of the base
optimizers empirically converge.
To empirically validate our proof of convergence for the proposed methods on convex loss surfaces,
We train all C optimizer variants on three tasks with Logistic Regression: '2-regularized binary
classification with rcv1 (Lewis et al., 2004), '2-regularized multi-class classification on covtype (DUa
and Graff, 2017), and non-regularized multi-class classification on MNIST. Figure 7 shows training
losses for all optimizers converging towards a minimal training loss.
F.3 B uffer Staleness B ound
We present additional results demonstrating the boundedness of the staleness of stored gradients
(Figure 8).
F.4 Additional Ablation Experiments
We present additional side-by-side comparisons of optimizers in the vein of §6 (Figure 9).
18
Published as a conference paper at ICLR 2022
20	40
Age (Steps)
(a) topC = 5
0.7
0.9
0.99
Figure 8: Histograms shoWing ages of the buffer as recorded at the end of every epoch, across five
seeds, for Logistic Regression on MNIST.
Figure 9: Additional ablation experiments performed on CIFAR-10
(c)
F.5 Robustness to Noise
As C variants retain larger gradients during training, and noisy data points may have gradients with
higher values of '2-norm, We experiment With the C variants by training Logistic Regression on a
synthetic binary classification dataset where the labels are perturbed with probability Pnoise .
We sample 500 data points from a 7-dimensional hypercube With the class means separated by 1 unit.
The data is split by 80/20 for training and evaluation. We train the model With the optimizers for 20
epochs on a fixed learning rate of 1 × 10-3, topC = 5, decay = 0.5 and other parameters from
the base model set to their defaults.
We compare the performance of base method and its C variant to study anomalies When the C variants
are exposed to noise (Figure 10). We observe that the models do not behave any differently than the
base methods shoWcasing that the noise in the dataset does not affect the Workings of the C variants
even With a small value for topC . This could be attributed to the decay parameter of the optimizer
that linearly scales doWn the priority of the older gradients in the buffer leading to them getting
replaced.
To construct the synthetic data set for the experiments, We used sklearn’s make_classification
method. The specific hyperparameters used to construct the dataset:
•	n_samples = 500, n_features = 10,
•	n_informative=7, n_redundant=0, n_repeated=0,
•	n_classes=2, n_clusters_per_class=1,
•	Weights=class_imbalance, flip_y=noise, class_sep= 0.5,
•	hypercube=True, shift=0.4, scale=1.0, shuffle=True, random_state=1403
19
Published as a conference paper at ICLR 2022
Figure 10: Pnoise is increased from 0 to 1 in increments of 0.1 indicating 10% more injection of noise.
The proposed variants did not show any anomaly on a noise induced dataset. For the experiments on
the decay the topC was 5 and decay 0.5. The plots are averaged over 5 different runs with different
seeds.
F.6 decay
The optimizers’ sensitivity to decay (Figure 11) had similar trend in other experiments, where the
optimizers showed better performance when decay was away from 0.99.
2220
nLU1m∙-ra>
0.05	0.1	0.2	0.5	0.99
decay
(a)	MultiWoZ-BiLSTM
0.05	0.1	0.2	0.5	0.99
decay
(b)	CIFAR-10
0.05	0.1	0.2	0.5	0.99
decay
(c)	MNIST
(d) MultiWoZ-RoBERTa	(e) PTB	(f) WikiText
Figure 11: Experiments on varying decay that across tasks. Graphs in the top row measure BLEU
score or accuracy (where a higher value is desired) and those in the bottom row measure perplexity
(where a lower value is desired)
F.7 topC
The optimizers’ sensitivity to topC (Figure 12) had a similar trend to other experiments, where the
optimizers showed better performance for lower values of topC .
F.8 gt VS gc
As a follow-up experiment to validate the non-informative gradients with higher values of topC ,
we observe the trend in the difference between average(||gc||2) and ||gt||2 for different values of
topC . We see that the gradients at each step get lower as topC increases. This could be because
of the stochasticity in aggr when computed with fewer gradients, which incidentally allow models
to converge better. Although storing all of the past gradients in memory has theoretical advantages,
in practice we observe that lower topC provides better training signal through gc than gt (Figure
13(a),13(b)). This explains the slightly lower performance of topC =100 across models in SNLI task
and the nonexistent to marginal improvements in MultiWoZ dataset with RoBERTa-Base model.
20
Published as a conference paper at ICLR 2022
Madel
BiLSTM-Attn
10	20 SQ IOO
tope
(a) MultiWoZ
(b) CIFAR-10
210200190180
A4xB-d-13d u≡4EP=E>
(d) MNIST
Figure 12: Experiments on varying topC that across tasks. Top row: (a) MultiWoz - BiLSTM
(BLEU Score) (b) CIFAR-10 - Convnet (Accuracy). Bottom row: (c) WikiText - LSTM (Perplexity)
(d) MNIST - Log. Reg. and MLP (Accuracy).
Figure 13: The difference between average(||gc||2) and ||gt||2 shown in plots (a) and (b) indicate
that increasing topC could hurt the performance. These plots correlate to experiments in §6
21
Published as a conference paper at ICLR 2022
The plots of gt vs gc is crucial to the results in that they provide an explanations for the faster
convergence (as well as smaller improvements) of the C variants over the base methods (Figure
14). Since the parameter updates are reminded of large displacement of the gradients via the aggr
method, the model updates the parameters more frequently. In the cases where gc is higher than
gt , we observe better performance and no improvements in cases where the difference is not as
significant.
(d) CIFAR-10
(e) PTB	(f) Roberta-MWoZ
Figure 14: Additional results on gt vs gc, explaining rationale behind the better performances of
lower values of topC across all the experiments. As the variance in aggr get diminished with
increase in topC the model does not gain much with increase in topC .
Further, We observe the significance of the gradients as defined in '2-norm diminishes as topC
increases. This indicates that the optimizer needs only to store only a small set of critical gradients in
order to improve the performance Without incurring much time and memory overhead. Employing
lazy updates to cut doWn on the time is a useful future direction of research.
F.9 Details of Ablation Experiments
We detail the various ablation experiments Whose results are presented in §6.
•	King of the Hill sampling: The default method for Critical Gradients. Incoming gradients are
added if their norm is larger than the priority of the smallest gradient, Which is subsequently
removed.
•	Smallest gradients (or "bottomC"): Replaces the max-heap buffer With a min-heap
•	First-In-First-Out (FIFO): Replaces the max-heap (priority queue) buffer With a queue Which
begins to dequeue When it reaches a capacity of topC .
•	"Coin-Toss"/"random": an incoming gradient is added to the buffer or not With equal
probability.
•	Mean-Norm Diversity Sampling (MNDS): Incoming gradients are probabilistically added
to the buffer With likelyhood proportional to its difference from the mean of the norms of
gradients in the buffer.
•	Cosine Diversity Sampling (CDS): Incoming gradients are probabilistically added to the
buffer With likelyhood antiproportional to its cosine similarity (normalized dot-product of
flattened vectors).
•	Cosine Similarity Sampling (CSS): Incoming gradients are probabilistically added to the
buffer With likelyhood proportional to its cosine similarity.
•	Random Replacement (RR): When the buffer is full and a neW gradient is added, the gradient
Which gets removed gets selected at random.
22
Published as a conference paper at ICLR 2022
•	Norm-Controlled Probabilistic Replacement (NCPR): When the buffer is full and a new
gradient is added, the gradient gk which gets removed gets selected with probability
Ilgk l∣2
Pgi∈gc ∣∣gi∣∣2.
G	Test Performance
The complete test performance of the models is reported in Table 10.
H Reproducibility Checklist
As per the prescribed Reproducibility Checklist, we provide the information of the following:
•	A clear description of the mathematical setting, algorithm and/or model: We provide details
of models used in §D
•	Submission of source code: Source code for the proposed optimizers and its variants is
provided as a zip. The code used to train the models on the different data sets are open
source GitHub repositories. Other codes developed for the project are included in the zip.
•	Description of the computing infrastructure used: We used 50 NVIDIA V100 32GB GPUs
in parallel hyper parameter search over the grid using wandb and submitit packages. For the
final runs we used 1 NVIDIA V100 32 GB GPUs for every seed of every model.
•	Average runtime for each approach: The approximate training time for our use of AdamC
accross tasks is reported in §D.2.
•	Explanation of evaluation metrics used, with links to code: The metrics used for evaluation
of the models are the popular ones. For the ease of readers citations for the metrics are
included in the paper.
•	Relevant statistics of the datasets used: We provide the statistics of the datasets used in C.
•	Explanation of any data that were excluded, and all pre-processing steps: We train on a
fraction of the covtype and rcv1 datasets instead of using the entire data. We sampled 5000
datapoints at random with seed set to 100.
•	Link to downloadable version of data: The data sets used in the paper are from public
repositories. Links to the paper that proposes the data sets is included in the README.md
files in the submitted repository.
23
Published as a conference paper at ICLR 2022
Table 10: Complete test results on the different tasks.
Model	Dataset	Metric	Optimizer	Performance
RoBERTa-	MWoZ-	^"PPL	Adam	2.57 ± 0.0
RoBERTa	MWoZ	PPL	AdamC	2.41 ± 0.0
RoBERTa	MWoZ	PPL	RMSProP	2.56 ± 0.01
RoBERTa	MWoZ	PPL	RMSProPC	2.42 ± 0.0
RoBERTa	MWoZ	PPL	SGDM	2.46 ± 0.0
RoBERTa	MWoZ	PPL	SGDMC	2.4 ± 0.01
RoBERTa	MWoZ	PPL	SGD	2.62 ± 0.01
RoBERTa	MWoZ	PPL	SGDC	2.41 ± 0.0	
Model	Dataset	Metric	Optimizer	Performance
BiLSTM	MWoZ-	BLEU	Adam	20.64 ± 0.19
BiLSTM	MWoZ	BLEU	AdamC	20.94 ± 0.26
BiLSTM	MWoZ	BLEU	RMSProP	21.3 ± 0.41
BiLSTM	MWoZ	BLEU	RMSProPC	21.21 ± 0.22
BiLSTM	MWoZ	BLEU	SGDM	19.31 ± 0.27
BiLSTM	MWoZ	BLEU	SGDMC	19.58 ± 0.47
BiLSTM	MWoZ	BLEU	SGD	14.26 ± 0.61
BiLSTM	MWoZ	BLEU	SGDC	16.35 ± 0.24
Model	Dataset	Metric	Optimizer	Performance
LSTM	^"Ptb	^"PPL	Adam	153.54 ± 1.65
LSTM	PTB	PPL	AdamC	133.28 ± 0.61
LSTM	PTB	PPL	RMSProP	141.83 ± 0.57
LSTM	PTB	PPL	RMSProPC	130.63 ± 0.64
LSTM	PTB	PPL	SGDM	139.93 ± 0.64
LSTM	PTB	PPL	SGDMc	132.22 ± 1.1
LSTM	PTB	PPL	SGD	386.57 ± 18.61
LSTM	PTB	PPL	SGDC	295.33 ± 2.96
Model	Dataset	Metric	Optimizer	Performance
LSTM	WikiTeXt	^"PPL	Adam	170.3 ± 1.23
LSTM	WikiText	PPL	AdamC	169.23 ± 1.06
LSTM	WikiText	PPL	RMSProP	180.42 ± 1.64
LSTM	WikiText	PPL	RMSProPC	172.15 ± 1.63
LSTM	WikiText	PPL	SGDM	166.82 ± 2.24
LSTM	WikiText	PPL	SGDMc	156.75 ± 1.32
LSTM	WikiText	PPL	SGD	461.84 ± 12.68
LSTM	WikiTeXt	PPL	SGDC	356.88 ± 8.34
Model -	Dataset	Metric	Optimizer	Performance
InferSent	SNLI	Accuracy	Adam	78.42 ± 0.26
InferSent	SNLI	Accuracy	AdamC	79.03 ± 0.18
InferSent	SNLI	Accuracy	RMSProP	77.91 ± 0.35
InferSent	SNLI	Accuracy	RMSProPC	79.07 ± 0.28
InferSent	SNLI	Accuracy	SGDM	79.15 ± 0.3
InferSent	SNLI	Accuracy	SGDMc	78.52 ± 0.26
InferSent	SNLI	Accuracy	SGD	78.3 ± 0.16
InferSent	SNLI	Accuracy	SGDC	78.9 ± 0.57
Model -	Dataset	Metric	Optimizer	Performance
ConvNet	SNLI	Accuracy	Adam	75.52 ± 0.0
ConvNet	SNLI	Accuracy	AdamC	77.77 ± 0.28
ConvNet	SNLI	Accuracy	RMSProP	74.69 ± 0.36
ConvNet	SNLI	Accuracy	RMSProPC	77.49 ± 0.24
ConvNet	SNLI	Accuracy	SGDM	76.79 ± 0.68
ConvNet	SNLI	Accuracy	SGDMc	78.06 ± 0.45
ConvNet	SNLI	Accuracy	SGD	78.5 ± 0.1
ConvNet	SNLI	Accuracy	SGDC	78.59 ± 0.24
Model	Dataset	Metric	Optimizer	Performance
LSTMEnC-	SNLI	Accuracy	Adam	76.79 ± 0.38
LSTMEnc	SNLI	Accuracy	Adamc	77.3 ± 0.09
LSTMEnc	SNLI	Accuracy	RMSProP	76.37 ± 0.36
LSTMEnc	SNLI	Accuracy	RMSProPC	77.48 ± 0.11
LSTMEnc	SNLI	Accuracy	SGDM	77.6 ± 0.22
LSTMEnc	SNLI	Accuracy	SGDMc	76.69 ± 1.04
LSTMEnc	SNLI	Accuracy	SGD	74.92 ± 1.32
LSTMEnC	SNLI	Accuracy	SGDC	76.81 ± 0.59
Model -	Dataset	Metric	Optimizer	Performance
ConvNet	CIFAR100	Accuracy	Adam	54.71 ± 0.55
ConvNet	CIFAR100	Accuracy	Adamc	53.55 ± 0.42
ConvNet	CIFAR100	Accuracy	RMSProP	54.35 ± 0.5
ConvNet	CIFAR100	Accuracy	RMSProPC	52.93 ± 0.57
ConvNet	CIFAR100	Accuracy	SGDM	56.03 ± 0.49
ConvNet	CIFAR100	Accuracy	SGDMc	59.07 ± 0.2
ConvNet	CIFAR100	Accuracy	SGD	55.37 ± 0.39
ConvNet	CIFAR100	Accuracy	SGDC	58.0 ± 0.37
Model -	Dataset	Metric	Optimizer	Performance
ConvNet	CIFAR10	Accuracy	Adam	64.26 ± 0.47
ConvNet	CIFAR10	Accuracy	AdamC	63.72 ± 0.37
ConvNet	CIFAR10	Accuracy	RMSProP	63.9 ± 0.33
ConvNet	CIFAR10	Accuracy	RMSProPC	64.02 ± 0.78
ConvNet	CIFAR10	Accuracy	SGDM	64.7 ± 0.69
ConvNet	CIFAR10	Accuracy	SGDMc	63.68 ± 0.77
ConvNet	CIFAR10	Accuracy	SGD	63.9 ± 0.63
ConvNet	CIFAR10	Accuracy	SGDC	64.89 ± 0.66
24