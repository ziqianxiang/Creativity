Published as a conference paper at ICLR 2022
Discriminative Similarity for Data Clustering
Yingzhen Yang
School of Computing and Augmented Intelligence
Arizona State University
Tempe, AZ 85281, USA
yingzhen.yang@asu.edu
Ping Li
Cognitive Computing Lab
Baidu Research
Bellevue, WA 98004, USA
liping11@baidu.com
Ab stract
Similarity-based clustering methods separate data into clusters according to the
pairwise similarity between the data, and the pairwise similarity is crucial for their
performance. In this paper, we propose Clustering by Discriminative Similarity
(CDS), a novel method which learns discriminative similarity for data clustering.
CDS learns an unsupervised similarity-based classifier from each data partition,
and searches for the optimal partition of the data by minimizing the generalization
error of the learnt classifiers associated with the data partitions. By generaliza-
tion analysis via Rademacher complexity, the generalization error bound for the
unsupervised similarity-based classifier is expressed as the sum of discriminative
similarity between the data from different classes. Itis proved that the derived dis-
criminative similarity can also be induced by the integrated squared error bound
for kernel density classification. In order to evaluate the performance of the pro-
posed discriminative similarity, we propose a new clustering method using a ker-
nel as the similarity function, CDS via unsupervised kernel classification (CDSK),
with its effectiveness demonstrated by experimental results.
1	Introduction
Similarity-based clustering methods segment the data based on the similarity measure between the
data points, such as spectral clustering (Ng et al., 2001), pairwise clustering method (Shental et al.,
2003), K-means (Hartigan & Wong, 1979), and kernel K-means (Scholkopf et al., 1998). The SUc-
cess of similarity-based clustering highly depends on the underlying pairwise similarity over the
data, which in most cases are constrUcted empirically, e.g., by GaUssian kernel or the K-Nearest-
Neighbor (KNN) graph. In this paper, we model data clUstering as a mUlticlass classification prob-
lem and seek for the data partition where the associated classifier, trained on clUster labels, can have
low generalization error. Therefore, it is natUral to formUlate data clUstering problem as a problem
of training UnsUpervised classifiers: a classifier can be trained Upon each candidate partition of the
data, and the qUality of the data partition can be evalUated by the performance of the trained classi-
fier. SUch classifier trained on a hypothetical labeling associated with a data partition is termed an
UnsUpervised classifier.
We present Clustering by Discriminative Similarity (CDS), wherein discriminative similarity is de-
rived by the generalization error boUnd for an UnsUpervised similarity-based classifier. CDS is based
on a novel framework of discriminative clUstering by UnsUpervised classification wherein an Un-
sUpervised classifier is learnt from Unlabeled data and the preferred hypothetical labeling shoUld
minimize the generalization error boUnd for the learnt classifier. When the popUlar SUpport Vector
Machines (SVMs) is Used in this framework, UnsUpervised SVM (XU et al., 2004) can be dedUced. In
this paper, a similarity-based classifier motivated by similarity learning (Balcan et al., 2008; Cortes
et al., 2013), is Used as the UnsUpervised classifier. By generalization analysis via Rademacher com-
plexity, the generalization error boUnd for the UnsUpervised similarity-based classifier is expressed as
sUm of pairwise similarity between the data from different classes. SUch pairwise similarity, param-
eterized by the weights of the UnsUpervised similarity-based classifier, serves as the discriminative
similarity. The term “discriminative similarity” emphasizes the fact that the similarity is learnt so as
Yingzhen Yang’s work was condUcted as a consUlting researcher at BaidU Research - BellevUe, WA, USA.
1
Published as a conference paper at ICLR 2022
to improve the discriminative capability of a certain classifier such as the aforementioned unsuper-
vised similarity-based classifier.
1.1	Contributions and Main Results
Firstly, we present Clustering by Discriminative Similarity (CDS) where discriminative similarity
is induced by the generalization error bound for unsupervised similarity-based classifier on unla-
beled data. The generalization bound for such similarity-based classifier is of independent interest,
which is among the few results of generalization bounds for classification using general similar-
ity functions (Section B.1 of Appendix). When the general similarity function is set to a Positive
Semi-Definite (PSD) kernel, the derived discriminative similarity between two data points xi ,xj is
SiKj = 2(αi +αj -λαiαj)K(xi - xj), where K can be an arbitrary PSD kernel and αi is the kernel
weight associated with xi . With theoretical and empirical study, we argue that SiKj should be used
for data clustering instead of the conventional kernel similarity corresponding to uniform kernel
weights. In the case of binary classification, we prove that the derived discriminative similarity SiKj
has the same form as the similarity induced by the integrated squared error bound for kernel density
classification (Section A of the appendix). Such connection suggests that there exists information-
theoretic measure which is implicitly equivalent to our CDS framework for unsupervised learning,
and our CDS framework is well grounded for learning similarity from unlabeled data.
Secondly, based on our CDS model, we develop a clustering algorithm termed Clustering by Dis-
criminative Similarity via unsupervised Kernel classification (CDSK) in Section 5. CDSK uses a
PSD kernel as the similarity function, and outperforms competing clustering algorithms, including
nonparametric discriminative similarity based clustering methods and similarity graph based cluster-
ing methods, demonstrating the effectiveness of CDSK. When the kernel weights {αi } are uniform,
CDSK is equivalent to kernel K-Means (Scholkopf et al.,1998). CDSK is more flexible by learning
adaptive kernel weights associated with different data points.
1.2	Connection to Related Works
Our CDS model is related to a class of discriminative clustering methods which classify unlabeled
data by various measures on discriminative unsupervised classifiers, and the measures include gen-
eralization error (Xu et al., 2004) or the entropy of the posterior distribution of the label (Gomes
et al., 2010). Discriminative clustering methods (Xu et al., 2004) predict the labels of unlabeled
data by minimizing the generalization error bound for the unsupervised classifier with respect to
the hypothetical labeling. Unsupervised SVM is proposed in Xu et al. (2004) which learns a binary
classifier to partition unlabeled data with the maximum margin between different clusters. The the-
oretical properties of unsupervised SVM are further analyzed in Karnin et al. (2012). Kernel logistic
regression classifier is employed in Gomes et al. (2010), and it uses the entropy of the posterior dis-
tribution of the class label by the classifier to measure the quality of the hypothetical labeling. CDS
model performs discriminative clustering based on a novel unsupervised classification framework by
considering similarity-based or kernel classifiers which are important classification methods in the
supervised learning literature. In contrasts with kernel similarity with uniform weights, the induced
discriminative similarity with learnable weights enhances its capability to represent complex inter-
connection between data. The generalization analysis for CDS is primarily based on distribution free
Rademacher complexity. While Yang et al. (2014a) propose nonparametric discriminative similarity
for clustering, the nonparametric similarity requires probability density estimation which is difficult
for high-dimensional data, and the fixed nonparametric similarity is not adaptive to complicated data
distribution.
The paper is organized as follows. We introduce the problem setup of Clustering by Discrimina-
tive Similarity in Section 3. We then derive the generalization error bound for the unsupervised
similarity-based classifier for CDS in Section 4 where the proposed discriminative similarity is in-
duced by the error bound. The application of CDS to data clustering is shown in Section 5. Through-
out this paper, the term kernel stands for the PSD kernel ifno special notes are made.
2
Published as a conference paper at ICLR 2022
2	Significance of CDSK over Existing Discriminative and
Similarity-Based Clustering Methods
Effective data similarity highly depends on the underlying probabilistic distribution and geometric
structure of the data, and these two characteristics leads to “data-driven” similarity, such as Zhu
et al. (2014); Bicego et al. (2021); Ng et al. (2001); Shental et al. (2003); Hartigan & Wong (1979);
ScholkoPf et al. (1998) and similarity based on geometric structure of the data, such as the subspace
structure (Sparse Subspace Clustering, or SSC in Elhamifar & Vidal (2013)). Note that the sparse
graph method, `1 -Graph (Yan & Wang, 2009), has the same formulation as SSC. Most existing clus-
tering methods based on data-driven or geometric structure-driven similarity suffer from a common
deficiency, that is, the similarity is not explicitly optimized for the purpose of separating underlying
clusters. In particular, the Random Forest-based similarity (Zhu et al., 2014; Bicego et al., 2021) is
extracted from features in decision trees. Previous works about subspace-based similarity (Yan &
Wang, 2009; Elhamifar & Vidal, 2013) try to make sure that only data points lying on or close to
the same subspace have nonzero similarity, so that data points from the same subspace can form a
cluster. However, it is not guaranteed that features in the decision trees are discriminative enough to
separate clusters, because the candidate data partition (or candidate cluster labels) do not participate
in the feature or similarity extraction process. Note that synthetically generated negative class are
suggested in Zhu et al. (2014); Bicego et al. (2021) to train unsupervised random forest, however, the
synthetic labels are not for the original data. Moreover, it is well known that the existing subspace
learning methods only obtain reliable subspace-based similarity with restrictive geometric assump-
tions on the data and the underlying subspaces, such as large principal angle between intersecting
subspaces (Soltanolkotabi & Candes, 2012; Elhamifar & Vidal, 2013).
Therefore, it is particularly important to derive similarity for clustering which meets two require-
ments: (1) discriminative measure with information such as cluster partition is used to derive such
similarity so as to achieve compelling clustering performance; (2) it requires less restrictive assump-
tions on the geometric structure of the data than current geometric structure-based similarity learning
methods, such as subspace clustering (Yan & Wang, 2009; Elhamifar & Vidal, 2013).
Significance. The proposed discriminative similarity of this paper meets these two requirements.
First, the discriminative similarity is derived by the generalization error bound associated with candi-
date cluster labeling, and minimizing the objective function of our optimization problem for cluster-
ing renders a joint optimization of discriminative similarity and candidate cluster labeling in a way
such that the similarity-based classifier has small generalization error bound. Second, our framework
only assumes a mild classification model in Definition 3.1, which only requires an unknown joint
distribution over data and its labels. In this way, the restrictive geometric assumptions are avoided in
our method. Compared to the existing discriminative clustering methods, such as MMC (Xu et al.,
2004), BMMC (Chen et al., 2014), RIM (Gomes et al., 2010), and the other discriminative clustering
methods such as (Huang et al., 2015; Nguyen et al., 2017), the optimization problem of CDSK with
discriminative similarity-based formulation is much easier to solve and it enjoys convexity and effi-
ciency in each iteration of coordinate descent described in Algorithm 1. In particular, as mentioned
in Section D of the appendix, the first step (11) of each iteration can be solved by efficient SVD or
other randomized large-scale SVD methods, and the second step (12) of each iteration can be solved
by efficient SMO (Platt, 1998). Moreover, the optimization problems in these two steps are either
convex or having closed-form solution. In contrast, MMC requires expensive semidefinite program-
ming. RIM has to solve a nonconvex optimization problem and its formulation does not guarantee
that the trained multi-class kernelized logistic regression has low classification error on candidate
labeling, which explains why it has inferior performance compared to our method. The discrimina-
tive Extreme Learning Machine (Huang et al., 2015) trains ELM using labels produced by a simple
clustering method such as K-means, and the potentially poor cluster labels by the simple clustering
method can easily result in unsatisfactory performance of this method. The discriminative Bayesian
nonparametric clustering (Nguyen et al., 2017) and BMMC (Chen et al., 2014) require extra efforts
of sampling hidden variables and tuning hyperparameters to generate the desirable number of clus-
ters (or model selection), which could reduce the effect of discriminative measures used in these
Bayesian nonparametric methods.
3
Published as a conference paper at ICLR 2022
3	Problem Setup
We introduce the problem setup of the formulation of clustering by unsupervised classification.
Given unlabeled data {xl }ln=1 ⊂ Rd, clustering is equivalent to searching for the hypothetical la-
beling which is optimal in some sense. Each hypothetical labeling corresponds to a candidate data
partition. Figure 1 illustrates four binary hypothetical labelings which correspond to four partitions
of the data, and the data is divided into two clusters by each hypothetical labeling.
Figure 1: Illustration of binary hypothetical labelings
The discriminative clustering literature (Xu et al., 2004; Gomes et al., 2010) has demonstrated the
potential of multi-class classification for clustering problem. Inspired by the natural connection
between clustering and classification, we proposes the framework of Clustering by Unsupervised
Classification which models clustering problem as a multi-class classification problem. A classifier
is learnt from unlabeled data with a hypothetical labeling, which is associated with a candidate
partition of the unlabeled data. The optimal hypothetical labeling is supposed to be the one such that
its associated classifier has the minimum generalization error bound. To study the generalization
bound for the classifier learnt from hypothetical labeling, the concept of classification model is
needed. Given unlabeled data {xl}ln=1, a classification model MY is constructed for any hypothetical
labeling Y = {yl }ln=1 as follows.
Definition 3.1. The classification model corresponding to the hypothetical labeling Y = {yl }ln=1 is
defined as MY = (S, F). S = {xl , yl}ln=1 are the labeled data by the hypothetical labeling Y, and
S are assumed to be i.i.d. samples drawn from the some unknown joint distribution PXY , where
(X, Y ) is a random couple, X ∈ X ⊆ Rd represents the data in some compact domain X, and
Y ∈ {1, 2, ..., c} is the class label of X, c is the number of classes. F is a classifier trained on S.
The generalization error of the classification model MY is defined as the generalization error of the
classifier F in MY.
The basic assumption of CDS is that the optimal hypothetical labeling minimizes the gener-
alization error bound for the classification model. With f being different classifiers, different
discriminative clustering models can be derived. When SVMs is used as the classifier F in the
above discriminative model, unsupervised SVM (Xu et al., 2004) is obtained.
In Balcan et al. (2008), the authors proposes a classification method using general similarity func-
tions. The classification rule measures the similarity of the test data to each class, and then assigns
the test data to the class such that the weighed average of the similarity between the test data and the
training data belonging to that class is maximized over all the classes. Inspired by this classification
method, we now consider using a general symmetric and continuous function S : X × X → [0, 1]
as the similarity function in our CDS model. We propose the following hypothesis,
hS (x, y) =	αiS(x,xi).	(1)
i:	yi=y
In the next section, we derive generalization bound for the unsupervised similarity-based classifier
based on the above hypothesis, and such generalization bound leads to discriminative similarities
for data clustering. When S is a PSD kernel, minimizing the generalization error bound amounts to
minimization of a new form of kernel similarity between data from different clusters, which lays the
foundation of a new clustering algorithm presented in Section 5.
4	Generalization B ound for Similarity-based Classifier
In this section, the generalization error bound for the classification model in Definition 3.1 with the
unsupervised similarity-based classifier is derived as a sum of discriminative similarity between the
data from different classes.
4
Published as a conference paper at ICLR 2022
4.1	Generalization B ound
The following notations are introduced before our analysis. Let α = [α1, . . . , αn]> be the nonzero
weights that sum up to 1, α(y) be a n × 1 column vector representing the weights belonging to
class y such that αi(y) is αi if y = yi, and 0 otherwise. The margin of the labeled sample (x, y) is
defined as mhS (x, y) = hS (x, y) - argmaxy0 6=yhS (x, y0), the sample (x, y) is classified correctly
if mhS (x, y) ≥ 0.
The general similarity-based classifier fS predicts the label of the input x by fS (x) =
argmaxy∈{1,...,c}hS (x, y). We then begin to derive the generalization error bound for fS using the
Rademacher complexity of the function class comprised of all the possible margin functions mhS .
The Rademacher complexity (Bartlett & Mendelson, 2003; Koltchinskii, 2001) of a function class
is defined below:
Definition 4.1. Let {σi}n=ι be n i.i.d. random variables such that Pr[σi = 1] = Pr[σi = -1] = 11.
The Rademacher complexity of a function class A is defined as
R(A) = E{σi},{xi}
sup
h∈A
1n
-废 σih(xj
n
i=1
(2)
In order to analyze the generalization property of the classification rule using the general similarity
function, we first investigate the properties of general similarity function and its relationship to PSD
kernels in terms of eigenvalues and eigenfunctions of the associated integral operator. The integral
operator(LSf)(x) = S(x, t)f (t)dt is well defined. It can be verified that LS is a compact
operator since S is continuous. According to the spectral theorem in operator theory, there exists
an orthonormal basis {φ1, φ2 , . . .} of L2 which is comprised of the eigenfunctions of LS, where
L2 is the space of measurable functions which are defined over X and square Lebesgue integrable.
φk is the eigenfunction of LS with eigenvalue λk if LSφk = λkφk. The following lemma shows
that under certain assumption on the eigenvalues and eigenfunctions ofLS, a general symmetric and
continuous similarity can be decomposed into two PSD kernels.
Lemma 4.1. Suppose S : X × X → [0, 1] is a symmetric continuous function, and {λk} and {φk}
are the eigenvalues and eigenfunctions of LS respectively. Suppose P λk∣φk(x)∣2 < C for some
k≥1
constant C > 0. Then S(x, t) = P λkφk(x)φk(t) for any x, t ∈ X, and it can be decomposed as
k≥1
the difference between two positive semi-definite kernels: S(x, t) = S+(x, t) - S-(x, t), with
S+(x,t) = X λkφk(x)φk(t), S-(x,t) = X ∣λk∣φk(x)φk(t).	(3)
k: λk≥0	k>k<0
We now use a regularization term to bound the Rademacher complexity for the classification
c>
rule using the general similarity function. Let Ω+(α) = P α(y) S+α(y) and Ω-(α) =
y=1
c>
P α(y)	S-α(y)	with	[S+]ij	=	S+(xi, xj)	and	[S-]ij	=	S-(xi, xj).	The space	Hy	of all the
y=1
hypothesis hs(∙, y) associated with label y is defined as
Hs,y = {(x,y) → X αiS(x, Xi): α ≥ 0,1>α = 1, Ω+(α) ≤ B+1, Ω-(α) ≤ B-1}
i: yi=y
for 1 ≤ y ≤ c, with positive number B+ and B- which bound Ω+ and Ω- respec-
tively. Let the hypothesis space comprising all possible margin functions be HS = {(x, y) →
mhS (x, y) : hS (x, y) ∈ HS,y}. We then present the main result in this section about the generaliza-
tion error of unsupervised similarity-based classifier fS.
Theorem 4.2. Given the discriminative model MY = (S, fs), suppose Ω+(α) ≤ B+2, Ω-(α) ≤
B-2, supx∈X |S+(x, x)| ≤ R2, supx∈X |S-(x, x)| ≤ R2 for positive constants B+, B- and R.
Then with probability 1 - δ over the labeled data S with respect to any distribution in PXY , under
5
Published as a conference paper at ICLR 2022
the assumptions of Lemma 4.1, the generalization error of the general classifier fS satisfies
R(fs) =Pr [Y = fs (X)]
≤Rn(fs) + 8R(2c- 1" +B-) + (16c(2c- I)(B+ +B-)R2 +1)F,
γ√n	∖	Y	√ V 2n
(4)
where Rbn(fs) = 1 P φ(hS(xi,yi)-argmαxy0=yhS(Xiiy )) is the empirical loss of fs on the labeled
i=1
data, γ > 0 is a constant and Φ is defined as Φ(x) = min {1, max{0, 1 - x}}. Moreover, if γ ≥ 1,
the empirical loss Rn(fS) satisfies
1 n α+α	1 n
Rn (fS) ≤ 1---E 2	S(Xi, Xj ) +	E	2(αi + αj)S(Xi, Xj )1Iyi = yj .	⑸
nγ i,j=1	2	nγ 1≤i<j≤n
The indicator function 1IE in (5) is 1 if event E is true, and 0 otherwise.
Remark 4.3. Lemma E.3 in the Appendix shows that the Rademacher complexity ofHS is bounded
in terms ofB+ andB-, and that is why these two quantities appear on the RHS of (4). In addition,
when S is a Positive Semi-Definite (PSD) kernel K, it can be verified that S-≡ 0, S = S+.
Remark 4.4. When the decomposition S = S+ - S-exists and S+, S- are PSD kernels, S is the
kernel of some Reproducing Kernel Kreln Space (RKKS) (Mary, 2003). Ong et al. (2004) and Loosli
et al. (2016) analyzed the problem of learning SVM-style classifiers with indefinite kernels from the
Kreln space. However, their work does not show when and how an indefinite and general similarity
function can have PSD decomposition, as well as the generalization analysis for the similarity-based
classifier using such general indefinite function as similarity measure. Our analysis deals with these
problems in Lemma 4.1 and Theorem 4.2. It should be emphasized that our generalization bound is
of independent interest in supervised learning, because it is among the few results of generalization
bounds using general similarity-based classifier. Section B.1 shows that the our bound is a principled
result with strong connection to established generalization error bound for Support Vector Machines
(SVMs) or Kernel Machines.
4.2 Clustering by Discriminative Similarity
We let
Sisjim = 2(αi +αj)S(Xi, Xj) - 2λαiαjS+ (Xi, Xj) - 2λαiαjS-(Xi, Xj )	(6)
be the discriminative similarity between data from different classes, which is induced by the gener-
alization error bound (4) for the unsupervised general similarity-based classifier fS. Minimizing the
bound (4) motivates US to consider the optimization problem that minimizes Rbn(fs) + λ(Ω+ (α) +
Ω— (α)). Replacing Rn (fs) by its upper bound in (5), we consider the following problem,
nn
minn	X	Ssjm 1Iyi=yj - X % 2 % S(χi, Xj) + λ(α>S+α + α>S-α)
, 1≤i<j≤n	i,j=1
s.t. α ≥ 0,1>α= 1,Y= {yi}in=1,	(7)
where λ > 0 is the weighting parameter for the regularization term Ω+(α) + Ω-(α). Note that
16c(2c-1)R2+8√2R(2c-1)c
√2γ
we do not set λ to
exactly matching the RHS of (4), because λ controls
the weight of the regularization term which bounds the unknown complexity of the function class
Hs . Note that (7) encourages the discriminative similarity Sisjim between the data from different
classes small. The optimization problem (7) forms the formulation of Clustering by Discriminative
Similarity (CDS).
By Remark 4.3, when S is a PSD kernel K, S- ≡ 0, S = S+, Sisjim reduces to the following
discriminative similarity for PSD kernels:
ij = 2(αi +αj - λαiαj )K (Xi - Xj ), 1 ≤ i, j ≤ n,
(8)
6
Published as a conference paper at ICLR 2022
and SiKj is the similarity induced by the unsupervised kernel classifier by the kernel K .
Without loss of generality, We set K = KT (x) = exp(- ⅛X⅛) which is the isotropic Gaussian
kernel with kernel bandwidth τ > 0, and we omit the constant that makes integral of K unit.
When setting the general similarity function to kernel Kτ , CDS aims to minimize the error bound
for the corresponding unsupervised kernel classifier, which amounts to minimizing the following
objective function
n
min
α∈Λ,Y={yi }in=1
SiKj 1Iyi6=yj
1≤i<j≤n
n
-X ɑi 2 % KT(Xi - Xj) + λα>Kα,
i,j=1
(9)
where SiKj	is defined in (8) with K =	KT.	K ∈	Rn×n and	Kij	=	KT (Xi -	Xj).	λ is tuned
such that SiKj ≥ 0, e.g., λ ≤ 2. In Section A, it is shown that the discriminative similarity (8) can
also be induced from the perspective of kernel density classification by kernel density estimators
with nonuniform weights. It supports the theoretical justification for the induced discriminative
similarity in this section.
5 Application to Data Clustering
In this section, we propose a novel data clustering method termed Clustering by Discriminative
Similarity via unsupervised Kernel classification (CDSK) which is an empirical method inspired by
our CDS model when the similarity function is a PSD kernel K = KT . In accordance with the CDS
model in Section 4.2, CDSK aims to minimize (9). However, problem (9) involves minimization
with respect to discrete cluster labels Y = {yi } which is NP-hard. In addition, it potentially results
in a trivial solution which puts all the data in a single cluster due to the lack of constraints on
the cluster balance. When Y is a binary matrix where each column is a membership vector for a
n
1 Tr(Y>LKY). Therefore, (9) is relaxed in the proposed
particular cluster, P	SiKj 1Iyi 6=yj =
1≤i<j ≤n
optimization problem for CDSK below:
1n
min	Tr(Y>LKY) - V
α∈Λ,Y∈Rn×c 2
i,j=1
αi + α KT(Xi - Xj) + λα>Kα	s.t. Y>DkY = L,
(10)
where Λ = {α : α ≥ 0, 1>α = 1}, SiKj = SiKj , LK = DK - SK is the graph Laplacian computed
with SK, DK is a diagonal matrix with each diagonal element being the sum of the corresponding
n
row of SK: [DK]ii = P SiKj , Ic is a c × c identity matrix, c is the number of clusters. The
j=1
constraint in (10) is used to balance the cluster size. This is because minimizing (9) without any
constraint on the cluster size results in a trivial solution where all data points form a single cluster.
Inspired by spectral clustering (Ng et al., 2001), the constraint Y>DKY = Ic used in CDSK
prevents imbalanced data clusters.
Problem (10) is optimized by coordinate descent. In each iteration of coordinate descent, optimiza-
tion with respect to Y is performed with fixed α, which is exactly the same problem as that of
spectral clustering with a solution formed by the smallest c eigenvectors of the normalized graph
Laplacian (DK)-1/2 LK (DK)-1/2; then the optimization with respect to α is performed with fixed
Y, which is a standard constrained quadratic programming problem. The iteration of coordinate
descent proceeds until convergence or the maximum iteration number M is achieved. Each itera-
tion solves two subproblems, (11) and (12). In order to promote sparsity of α, α can be initialized
by solving Pin=1 Xi - Pj6=i Xjαj + τ kαk0 for a positive weighting parameter τ = 0.1. The
algorithm of CDSK is described in Algorithm 1.
Furthermore, Section C in the appendix explains the theoretical properties of the coordinate descent
algorithm for problem (10).
The baseline named SC-NS performs spectral clustering on the nonparametric similarity proposed
in Yang et al. (2014a). The baseline named SC-MS first constructs a similarity matrix between data
7
Published as a conference paper at ICLR 2022
Algorithm 1 Clustering by Discriminative Similarity via unsupervised Kernel classification (CDSK)
Input: Unlabeled dataset {xl}ln=1, parameter λ, maximum iteration number M.
for t - 1 to M do
With fixed α, solve
min Tr(Y>LK Y) s.t. Y>DKY = Ic,	(11)
Y∈Rn×c
With fixed Y, solve
n α+α
minTr(Y>LKY) - E α + j KT(Xi - Xj) + λα>Ka
α∈Λ,	i,j=1
s.t. Y>DKY = Ic,	(12)
end for
Perform K-Means Clustering on rows of Y to obtain the clustering result.
denoted by W, where Wij = Kτ (xi - xj), then optimize the kernel bandwidth h by minimizing
P IlXi- d1 P WijXjk2 where di = P Wij. SC-MS then performs spectral clustering on W with
i	* ij	j
the kernel bandwidth h obtained from the optimization.
To demonstrate the advantage of the proposed parametric discriminative similarity, we compare
CDSK to various baseline clustering methods. SC stands for Spectral Clustering, which is the best
performer among spectral clustering with similarity matrix set by Gaussian kernel (SCK), spectral
clustering with similarity matrix set by a manifold-based similarity learning method (SC-MS) (Kara-
suyama & Mamitsuka, 2013), and spectral clustering with similarity matrix set by the nonparametric
discriminative similarity (SC-NS) in Yang et al. (2014a). In SC-MS, Gaussian kernel is used as data
similarity, and the parameters of the diagonal covariance matrix is optimized so as to minimize
the data reconstruction error term. SC-NS minimizes nonparametric kernel similarity between data
across different clusters, which is the same objective as that of kernel K-Means (SchOlkopf et al.,
1998), so its performance is the same as kernel K-Means. Please refer to Section 2 for discussion
about other baselines.
Datasets. We conduct experiments on the Yale face dataset, UCI Ionosphere dataset, the MNIST
handwritten digits dataset and the Georgia Face dataset. The Yale face dataset has face images of 15
people with 11 images for each person. The Ionosphere data contains 351 points of dimensionality
34. The Georgia Face dataset contains images of 50 people, and each person is represented by 15
color images with cluttered background. COIL-20 dataset has 1440 images of size 32 × 32 for
20 objects with background removed in all images. The COIL-100 dataset contains 100 objects
with 72 images of size 32 × 32 for each object. CMU PIE face data contains 11554 cropped face
images of size 32 × 32 for 68 persons, and there are around 170 facial images for each person under
different illumination and expressions. The UMIST face dataset is comprised of 575 images of size
112 × 92 for 20 people. CMU Multi-PIE (MPIE) data (Gross et al., 2010) contains 8916 facial
images captured in four sessions. The MNIST handwritten digits database has a total number of
70000 samples of dimensionality 1024 for digits from 0 to 9. The digits are normalized and centered
in a fixed-size image. The Extended Yale Face Database B (Yale-B) dataset contains face images
for 38 subjects with 64 frontal face images taken under different illuminations for each subject.
CIFAR-10 dataset consists of 50000 training images and 10000 testing images in 10 classes, and
each image is a color one of size 32 × 32, and we perform data clustering using all the training and
testing images. We also use the miniImageNet dataset used in Vinyals et al. (2016) to evaluate the
potential of clustering methods. MiniImageNet consists of 60, 000 color images of size 84 × 84 with
100 classes, and each class has 600 images. MiniImageNet is known to be more complex than the
CIFAR-10 dataset, and we perform clustering on the 64 classes in miniImageNet which are used for
few-shot learning, so 38, 400 images are used for clustering. For every clustering method involving
randomness such as K-Means, we report the average performance of running it for 10 times.
Performance Measures and Tuning λ by Cross Validation. We use Accuracy (AC) and Normal-
ized Mutual Information (NMI) (Zheng et al., 2004) as the performance measures. The results of
different clustering methods are shown in Table 1 and Table 2 in the format of AC(NMI). Except
8
Published as a conference paper at ICLR 2022
for SC-MS, the kernel bandwidth in all methods is set as the variance of the pairwise Euclidean
distance between the data. λ is the weight for the regularization term in our derived generalization
bound. As explained in Section B.1 of the appendix, λ plays the same role as the weight in the reg-
ularization term of SVMs or Kernel Machines. Following the common practice in the literature of
SVM or Kernel Machines, λ can be tuned by Cross-Validation (CV). While this is an unsupervised
learning task and these is no labeled data for CV, we still developed a well-motivated CV proce-
dure. Following the practice in Mairal et al. (2012), we randomly sampled 10% of the given data
as the validation data, then perform CDSK on the validation data. The best λ is chosen among the
discrete values between [0.05, 05] with a step of 0.05 which minimizes the average entropy of the
obtained embedding matrix Y ∈ Rn×c by Algorithm 1, where the average entropy is compute as
n
n P entroPy(Softmax(Yi)). This is because We would like to choose λ which renders the most
i=1
confident clustering embedding. We perform CDSK on all the datasets using this tuning strategy and
observe improved performance as shown in the above two tables. For clustering methods involving
random operations, the average performance over 10 runs is reported.
Computational Complexity. Suppose the optimization of CDSK comprises M iterations of co-
ordinate descent. The first subproblem (11) in Algorithm 1 takes O(n2c) steps using truncated
Singular Value Decomposition (SVD) by Krylov subspace iterative method. We adopt Sequen-
tial Minimal Optimization (SMO) (Platt, 1998) to solve the second subproblem (12), which takes
roughly O(n2.1) steps as reported in Platt (1998). Therefore, the overall time complexity of CDSK
is O(M cn2 + M n2.1). M is set to 20 throughout all the experiments.
Table 1: Clustering results on Yale-B, Ionosphere, Georgia Face, COIL-20, COIL-100, CMU PIE
and UMIST Face.
^^^^^	Dataset MethOdS~^^^^^^^^^~^_			Yale-B	Ionosphere	Georgia Face	COIL-20	COIL-100	CMU PIE	UMIST Face
K-Means	0.09(0.13)	0.71(0.13)	0.50(0.69)-	0.65(0.76)	0.49(0.75)	0.08(0.19)	0.42(0.64)
-SC	0.11(0.15)	0.74(0.22)	0.52(0.70)-	0.43(0.62)	0.28(0.59)	0.07(0.18)	0.42(0.61)
'1-Graph (EIhamifar & Vidal, 2013)	0.79(0.78)	0.51(0.12)	0.54(0.70)-	0.79(0.91)	0.53(0.80)	0.23(0.34)	0.44(0.65)
SMCE (Elhamifar & Vidal, 2011)	0.34(0.39)	0.68(0.09)	0.60(0.74)-	0.88(0.88)	0.56(0.81)	0.16(0.34)	0.45(0.66)
Lap-'1-Graph (Yang et al., 2014b)	0.79(0.78)	0.50(0.09)	0.58(0.73)-	0.79(0.91)	0.56(0.81)	0.30(0.51)	0.50(0.69)
RAG(ZhU et al.,2014)	0.13(0.19)	0.70(0.11)	0.17(0.38)-	0.50(0.64)	0.58(0.81)	0.14(0.34)	0.26(0.28)
MMC (XU et al.,2004)	0.71(0.69)	0.75(0.21)	0.42(0.58)-	0.80(0.89)	0.61(0.63)	0.22(0.30)	0.51(0.56)
BMMC (Chen etal.,2014)	0.65(0.63)	0.70(0.15)	0.34(0.41)-	0.82(0.93)	0.64(0.69)	0.18(0.23)	0.55(0.61)
RIM (Gomes et al,,2010)	0.62(0.74)	0.59(0.08)	0.39(0.56)-	0.77(0.82)	0.71(0.79)	0.26(0.34)	0.40(0.53)
RatioRF (Bicego et al., 2021)	0.39(0.53)	0.62(0.05)	0.18(0.40)-	0.65(0.75)	0.36(0.64)	0.15(0.36)	0.29(0.34)
CDSK (OUrs)	0.83(0.86)	0.76(0.25)	0.60(0.74)一	0.93(0.97)	0.78(0.92)	0.32(0.50)	0.67(0.80)一
Table 2: Clustering results on CMU Multi-PIE which contains the facial images captured in four
sessions (S1 to S4), MNIST, CIFAR-10, and Mini-ImageNet
一—一一一_____	DataSet Methods一一一一一一一	MPIE S1	MPIE S2	MPIES3	MPIE S4	MNIST	CIFAR-10	Mini-ImageNet
KM	0.12(0.50)	0.13(0.48)	0.13(0.48)	0.13(0.49)	0.52(0.47)	0.19(0.06)	-0.27(0.33)-
^SC	0.13(0.53)	0.14(0.51)	0.14(0.52)	0.15(0.53)	0.38(0.36)	0.21(0.04)	-0.29(0.35)-
'1-Graph (EIhamifar & Vidal, 2013)	0.59(0.77)	0.70(0.81)	0.63(0.79)	0.68(0.81)	0.57(0.61)	0.28(0.24)	-0.28(0.37)-
SMCE (EIhamifar & Vidal, 2011)-	0.17(0.55)	0.19(0.53)	0.19(0.52)	0.18(0.53)	0.65(0.67)	0.31(0.30)	-0.29(0.37)-
Lap-'1-Graph (Yang etal., 2014b)	0.59(0.77)	0.70(0.81)	0.63(0.79)	0.68(0.81)	0.56(0.60)	0.29(0.30)	-0.29(0.37)-
RAG(ZhU et al.,2014)	0.34(0.75)	0.30(0.69)	0.31(0.68)	0.29(0.70)	0.59(0.51)	0.22(0.10)	-0.18(0.33)-
MMC (XU et al., 2004)	0.49(0.58)	0.51(0.60)	0.53(0.65)	0.50(0.61)	0.64(0.60)	0.31(0.28)	-0.19(0.34)-
BMMC (Chen et al.,2014)	0.40(0.51)	0.44(0.59)	0.45(0.61)	0.49(0.66)	0.66(0.69)	0.29(0.26)	-0.16(0.32)-
RIM (Gomes et al., 2010)	0.50(0.63)	0.52(0.68)	0.55(0.71)	0.51(0.67)	0.54(0.62)	0.20(0.25)	-0.17(0.38)-
RatioRF (BiCego et al.,2021)	0.54(0.85)	0.55(0.86)	0.64(0.86)	0.62(0.86)	0.48(0.39)	0.20(0.09)	-0.26(0.38)-
CDSK (OUrs)	0.66(0.85)	0.72(0.88)	0.68(0.87)	0.73(0.89)	0.76(0.75)	0.46(0.39)	0.31(0.41)一
6 Conclusion
We propose a new clustering framework termed Clustering by Discriminative Similarity (CDS),
which searches for the optimal partition of data where the associated unsupervised classifier has
minimum generalization error bound. Under this framework, discriminative similarity is induced
by the generalization error bound for unsupervised similarity-based classifier, and CDS minimizes
discriminative similarity between different clusters. It is also proved that the discriminative similar-
ity can be induced from kernel density classification. Based on CDS, we propose a new clustering
method named CDSK (CDS via unsupervised kernel classification), and demonstrate its effective-
ness in data clustering.
9
Published as a conference paper at ICLR 2022
References
Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learning with similarity func-
tions. Mach. Learn.,72(1-2):89-112, 2008.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, March 2003.
Manuele Bicego, Ferdinando Cicalese, and Antonella Mensi. Ratiorf: a novel measure for random
forest clustering based on the tversky’s ratio model. IEEE Transactions on Knowledge and Data
Engineering, pp. 1-1, 2021.
Changyou Chen, Jun Zhu, and Xinhua Zhang. Robust bayesian max-margin clustering. In Zoubin
Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 532-540,
2014.
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Multi-class classification with maxi-
mum margin multiple kernel. In Proceedings of the 30th International Conference on Machine
Learning (ICML), pp. 46-54, Atlanta, GA, 2013.
Ehsan Elhamifar and Rene Vidal. Sparse manifold clustering and embedding. In Advances in Neural
Information Processing Systems (NIPS), pp. 55-63, Granada, Spain, 2011.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2765-2781, 2013.
Evarist Gine and Joel Zinn. Some limit theorems for empirical processes. Ann. Probab., 12(4):
929-989, 11 1984.
Mark Girolami and Chao He. Probability density estimation from optimally condensed data samples.
IEEE Trans. Pattern Anal. Mach. Intell., 25(10):1253-1264, 2003.
Ryan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized informa-
tion maximization. In Advances in Neural Information Processing Systems (NIPS), pp. 775-783,
Vancouver, Canada, 2010.
Ralph Gross, Iain A. Matthews, Jeffrey F. Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image
Vis. Comput., 28(5):807-813, 2010.
J. A. Hartigan and M. A. Wong. A K-means clustering algorithm. Applied Statistics, 28:100-108,
1979.
Gao Huang, Tianchi Liu, Yan Yang, Zhiping Lin, Shiji Song, and Cheng Wu. Discriminative clus-
tering via extreme learning machine. Neural Networks, 70:1-8, 2015.
Masayuki Karasuyama and Hiroshi Mamitsuka. Manifold-based similarity adaptation for label prop-
agation. In Advances in Neural Information Processing Systems (NIPS), pp. 1547-1555, Lake
Tahoe, NV, 2013.
Zohar Shay Karnin, Edo Liberty, Shachar Lovett, Roy Schwartz, and Omri Weinstein. Unsupervised
SVMs: On the complexity of the furthest hyperplane problem. In Proceedings of the 25th Annual
Conference on Learning Theory (COLT), pp. 2.1-2.17, Edinburgh, Scotland, 2012.
JooSeUk Kim and Clayton D. Scott. Performance analysis for l_2 kernel classification. In Advances
in Neural Information Processing Systems (NIPS), pp. 833-840, Vancouver, Canada, 2008.
JooSeuk Kim and Clayton D. Scott. Robust kernel density estimation. J. Mach. Learn. Res., 13:
2529-2565, 2012.
V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization
error of combined classifiers. Ann. Statist., 30(1):1-50, 02 2002. doi: 10.1214/aos/1015362183.
10
Published as a conference paper at ICLR 2022
Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Trans. Inf.
Theory, 47(5):1902-1914, 2001.
James R. Lee, Shayan Oveis Gharan, and Luca Trevisan. Multi-way spectral partitioning and higher-
order cheeger inequalities. In Howard J. Karloff and Toniann Pitassi (eds.), Proceedings of the
44th Symposium on Theory of Computing Conference, STOC 2012, New York, NY, USA, May 19
-22, 2012, pp. 1117-1130. ACM, 2012.
Gaelle Loosli, StePhane Canu, and Cheng Soon Ong. Learning SVM in kreln spaces. IEEE Trans.
Pattern Anal. Mach. Intell., 38(6):1204-1216, 2016.
Ravi Sastry Ganti Mahapatruni and Alexander G. Gray. CAKE: convex adaptive kernel density
estimation. In Proceedings of the Fourteenth International Conference on Artificial Intelligence
and Statistics (AISTATS), pp. 498-506, Fort Lauderdale, FL, 2011.
Julien Mairal, Francis Bach, and Jean Ponce. Task-driven dictionary learning. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 34(4):791-804, 2012.
Xavier Mary. Hilbertian subspaces, subdualities and applications. Ph.D. Dissertation, Institut Na-
tional des Sciences Appliquees Rouen, 2003.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in Neural Information Processing Systems (NIPS), pp. 849-856, Vancouver, Canada],
2001.
Vu Nguyen, Dinh Q. Phung, Trung Le, and Hung Bui. Discriminative bayesian nonparametric
clustering. In Carles Sierra (ed.), Proceedings of the Twenty-Sixth International Joint Conference
on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pp. 2550-2556.
ijcai.org, 2017.
Cheng Soon Ong, Xavier Mary, Stephane Canu, and Alexander J. Smola. Learning with non-
positive kernels. In Proceedings of the Twenty-first International Conference on Machine Learn-
ing (ICML), Banff, Alberta, Canada, 2004.
John Platt. Sequential minimal optimization: A fast algorithm for training support vector machines.
Technical report, 1998.
Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural Comput., 10(5):1299-1319, July 1998.
Noam Shental, Assaf Zomet, Tomer Hertz, and Yair Weiss. Pairwise clustering and graphical mod-
els. In Advances in Neural Information Processing Systems (NIPS), pp. 185-192, Vancouver and
Whistler, Canada, 2003.
Mahdi Soltanolkotabi and Emmanuel J. Candes. A geometric analysis of subspace clustering with
outliers. Ann. Statist., 40(4):2195-2238, 08 2012.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pp. 3630-3638, 2016.
Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. In
Advances in Neural Information Processing Systems (NIPS), pp. 1537-1544, Vancouver, Canada],
2004.
Shuicheng Yan and Huan Wang. Semi-supervised learning by sparse representation. In Proceedings
of the SIAM International Conference on Data Mining (SDM), pp. 792-801, Sparks, NV, 2009.
Yingzhen Yang, Feng Liang, Shuicheng Yan, Zhangyang Wang, and Thomas S. Huang. On a theory
of nonparametric pairwise similarity for clustering: Connecting clustering to classification. In
Advances in Neural Information Processing Systems (NIPS), pp. 145-153, Montreal, Canada,
2014a.
11
Published as a conference paper at ICLR 2022
Yingzhen Yang, Zhangyang Wang, Jianchao Yang, Jiangping Wang, Shiyu Chang, and Thomas S.
Huang. Data clustering by laplacian regularized l1-graph. In Proceedings of the Twenty-Eighth
AAAI Conference OnArtficial Intelligence (AAAI), pp. 3l48-3149, Quebec City, Canada, 2014b.
Xin Zheng, Deng Cai, Xiaofei He, Wei-Ying Ma, and Xueyin Lin. Locality preserving clustering for
image database. In Proceedings of the 12th ACM International Conference on Multimedia (MM),
pp. 885-891, New York, NY, 2004.
Xiatian Zhu, Chen Change Loy, and Shaogang Gong. Constructing robust affinity graphs for spectral
clustering. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1450-
1457, 2014.
A Connection to Kernel Density Classification
In this section, we show that the discriminative similarity (8) can also be induced from kernel den-
sity classification with varying weights on the data, and binary classification is considered in this
section. For any classification model MY = (S, f) with hypothetical labeling Y and the labeled
data S = {xi, yi}in=1, suppose the joint distribution PXY over X × {1, 2} has probabilistic density
function p(x, y). Let PX be the induced marginal distribution over the data with probabilistic den-
sity function p(x). Robust kernel density estimation methods (Girolami & He, 2003; Kim & Scott,
2008; Mahapatruni & Gray, 2011; Kim & Scott, 2012) suggest the following kernel density esti-
mator where the kernel contributions of different data points are reflected by different nonnegative
weights that sum up to 1:
n
pb(x) = τ0 XαiKτ(x - xi), 1>α = 1,α ≥ 0,	(13)
i=1
where τo = (2∏)1∕2∕d. Based on (13), it is straightforward to obtain the following kernel density
estimator of the density function p(x, y):
pb(x, y) = τ0	αiKτ (x - xi).
i:yi=y
(14)
Kernel density classifier is learnt from the labeled data S and constructed by kernel density es-
timators (14). Kernel density classifier resembles the Bayes classifier, and it classifies the test
data x based on the conditional label distribution P(Y |X = x), or equivalently, x is assigned
to class 1 if pb(x, 1) - pb(x, 2) ≥ 0, otherwise it is assigned to class 2. Intuitively, it is preferred
that the decision function rb(x, α) = pb(x, 1) - pb(x, 2) is close to the true Bayes decision func-
tion r = p(x, 1) - p(x, 2). Girolami & He (2003); Kim & Scott (2008) propose to use Integrated
Squared Error (ISE) as the metric to measure the distance between the kernel density estimators and
their true counterparts, and the oracle inequality is obtained that relates the performance of the L2
classifier in Kim & Scott (2008) to the best possible performance of kernel density classifier in the
same category. ISE is adopted in our analysis of kernel density classification, and the ISE between
the decision function rb and the true Bayes decision function r is defined as
ISE(rb, r)
krb - rk2L =	(rb - r)2dx.
(15)
The upper bound for the ISE ISE(r, rb) also induces discriminative similarity between the data from
different classes, which is presented in the following theorem.
Theorem A.1. Let n1 = P 1Iyi=1 and n2 = P 1Iyi=2 . With probability at least 1 - 2n2 exp -
i=1	i=1
2(n - 1)ε2 - 2n exp - 2nε2 over the labeled data S, the ISE between the decision function
rb(x, α) and the true Bayes decision function r(x) satisfies
ISE(b, r) ≤ τ0d(b, r) + τ1K(α) + 2τ° (-ɪ- + ε),	(16)
n	n-1
where
n
IdSE(rb, r) = 4	(αi + αj)Kτ (xi - xj)1Iyi 6=yj -	(αi + αj)Kτ (xi - xj),	(17)
1≤i<j≤n
i,j=1
12
Published as a conference paper at ICLR 2022
K (α) = α' (K√2h)α ― 4	αiaj K√2h(Xi-Xj )1Iyi=yj,	(18)
1≤i<j≤n
and K√2h is the gram matrix evaluated on the data {xi}n=ι With the kernel K√2h∙
Let λ1 > 0 be a Weighting parameter, then the cost function ISE + λ1K(α), designed according to
the empirical term ISE(rb, r) and the regularization term K(α) in the ISE error bound (16), can be
expressed as
n
1SE + λιK(α) ≤ X sise11yi=yj - X (αi + αj)KT(Xi- Xj) + λια>K√2hα,
1≤i<j≤n	i,j=1
Where the first term is comprised of sum of similarity betWeen data from different classes With sim-
ilarity Siisje = 4(αi + αj - λ1αiαj)Kτ(Xi - Xj), and Siisje is the discriminative similarity induced
by the ISE bound for kernel density classification. Note that Siisje has the same form as the discrim-
inative similarity SiKj (8) induced by our CDS model, up to a scaling constant and the choice of the
balancing parameter λ. The proof of Theorem A.1 is deferred to Section E.3.
B More Explanation ab out the Theoretical Results in
Section 4.1
B.1 Theoretical Significance of the Bound (4)
To the best of our knoWledge, our generalization error bound (4) is the first principled result about
generalization error bound for general similarity-based classifier With strong connection to the es-
tablished generalization error bound for Support Vector Machines (SVMs) or Kernel Machines.
We noW explain this claim. When the similarity function S is a PSD kernel function, We have
S- ≡ 0, S = S+ as explained in Remark 4.3. As a reminder, S is the general similarity function
used in the similarity-based classification, and S+, S- are PSD kernel functions, and it is proved
that S can be decomposed by S = S+ - S- under the mild conditions of Lemma 3.1. It folloWs that
we can set Ω-(α) = 0 and B- = 0. Plugging B- = 0 in the derived generalization error bound
for the general similarity-based classification (4), We have
Prob [Y = fs(X)] ≤ Rbn(fs) + 8R(2c -JcB+ +
16c(2c - 1)B+R2
Y
(19)
c>
According to its definition, Ω+(α) = E α(y) Sα(y) because S = S+. We define B := B+.
y=1
2	c>
Because Ω+(α) ≤ B+ as mentioned in Theorem 3.2, B satisfies P a(y) Sα(y) ≤ B2. As a
y=1
result, when S is a PSD kernel function, inequality (19) becomes
Prob [Y = fs(X)] ≤ Rbn(fs) + 8R(2c -J)CB
16c(2c - 1)R2B	]
Y
ʌ/logl
V 2n
(20)
c>
with P α(y) Sα(y) ≤ B2.
y=1
Note that the bound (20) is in fact the generalization error bound for supervised learning when using
S as the similarity function in the similarity-based classification. At the end of this subsection, we
c>
provide a lemma proving that	α(y) Sα(y) ≤ B2 ⇒ α>Sα ≤ cB2, where c is the number of
y=1
classes.
Now we compare the generalization error bound (20) to the established generalization error bound
for Kernel Machines in Bartlett & Mendelson (2003, Theorem 21) for the case that c = 2 with
13
Published as a conference paper at ICLR 2022
notations adapted to our analysis. The bound in Bartlett & Mendelson (2003, Theorem 21) is for
binary classification, which is presented as follows:
Prob [Y = fs(X)] ≤ Rbn(fs) + 4γ√√B + (8 + l) FI/δ, α>Sα ≤ B2∙	(21)
Comparing our generalization error bound (2) with c = 2to the max-margin generalization error
bound (21), it can be easily seen that the two bounds are equivalent up to a constant scaling factor.
In fact, our bound (2) is more general which handles multi-class classification.
c>
Lemma B.1. When S is a PSD kernel, then P α(y) Sα(y) ≤B2 ⇒α> Sα	≤ cB2 .
y=1
Proof. Let H be the Reproducing Kernel Hilbert Space associated with the PSD kernel function S,
and H is also called the feature space associated with S. We use(,, ∙)h to denote the inner product
in the feature space H. Then we have S(xi, xj) = Sij = hΦ(xi), Φ(xj)iH where Φ is the feature
c
mapping associated with H. Because α	= Pαy , it can be verified that
y=1
nn	n	n	c
> Sα	=	αi αj Sij	= h	αi Φ(xi ),	αj Φ(xj )i	≤ c	hey ,	ey iH ,	where	ey	=
i=1 j=1	i=1	j=1	y=1
c>
P OyΦ(xi). It follows that a>Sa ≤ C P a(y) Sa(y) ≤ cB2.	□
i : yi=y	y=1
B.2 Tightness of the Bound
Note that the generalization error Pr [Y 6= f(X)]	is bounded by Rn (f)	=
n	hS (xi,yi)- P hS(xi,y)
* P Φ (------------y=yi--------) in Theorem 4.2. The underlying principle behind this bound and
i=1
all such bounds in the statistical machine learning literature such as Bartlett & Mendelson (2003) is
the following property about empirical process (adjusted using our notations):
R(H) ≤ Ex,y sup |Ex,yRbn(f) -Rn(f)| ≤ 2R(H),	(22)
h∈H
where Eχ,y indicates expectation with respect to random couple (x, y)〜PXY, and PXY is a joint
distribution in a discriminative model MY = (S, f). (22) is introduced in the classical properties
of empirical process in Gine & Zinn (1984). By Lemma E.2 of this paper, with probability at least
1 - δ over the data {xi}n=1 1^- PXY,
R(H) ≤ (2c -I)CB + √2Bc(2c - 1)
n
(23)
for some constant B. It follows from (22), (23), and concentration inequality (such as McDiarmid’s
inequality) that for each sufficiently large n, with large probability, suph∈H |Ex,y Rn (f) - Rn(f)|
is less than O( -B). Therefore, We can bound the expectation of the empirical loss, i.e., Eχ,yRbn (f),
tightly using the empirical loss Rn(f) uniformly over the function space H.
C Theoretical Properties of the Coordinate Descent Algorithm
in Section 5
In this subsection, we give a detailed explanation about the theoretical properties of the coordinate
descent algorithm presented in Section 5. We first explain how the objective function of CDSK (10)
is connected to the objective function (9) developed in our theoretical analysis. It should be empha-
sized that (9) cannot be directly used for data clustering since it cannot avoids the trivial solution
where all the data are in a single cluster. We adopt the broadly used formulation of normalized
14
Published as a conference paper at ICLR 2022
cut and use P CuvoAAA) to replace P SK 以="亍 in (9), leading to the following optimization
k=1	k	i<j
problem:
α∈Λ,m"XCUvoA(⅛)-X1 yKTX-Xj)+λα>Kα,QaY),	(9')
where { A}k=ι are C data clusters according to the cluster labels {yi}, Ak is the complement of Ak,
cut(A, B) = P	SiKj , vol(A) = P	SiKj . We have the following theorem, which
xi ∈A,xj ∈B	xi ∈A,1≤j ≤n
can be derived based on Theorem 4.1 in the work of multi-way Cheeger inequalities (Lee et al.,
2012).
c
Theorem C.1. minα∈Λ,Y
k=1
Cut(Ak,Ak)
Vol(Ak)
c
P σt (Lnor), where Lnor is the normalized graph
t=1
Laplacian Lnor = (DK)-1/2 LK (DK)-1/2, a . b indicates a < Cb for some constant C and
σt(∙) indicates the t-th smallest singular value of a matrix.
Based on Theorem C.1, we resort to solve the following more tractable problem, that is,
kn
min X σt(Lnor) - X αi+αjKT(Xi- Xj) + λα>Kα，Q(α),	(9”)
α∈Λ	2
t=1	i,j=1
because Q (9”) is an upper bound for Q (9,) UP to a constant scaling factor. It can be verified that
problem (10) is equivalent to (9”), and (9”) is the underlying optimization problem for data clustering
in Section 5. The following proposition shows that the iterative coordinate descent algorithm in
Section 5 reduces the value of Q at each iteration.
Proposition C.2. The coordinate descent algorithm for problem (10) reduces the value of the ob-
jective function Q(α) at each iteration.
n
Proof. Let Q0(α, Y) ，Tr(Y>LKY) - P 2 j KT(Xi - Xj) + λα>Kα, and We use su-
i,j=1
perscript to denote the iteration number of coordinate descent. At iteration m, after solving the
subproblems (11) and (12), we have Q0(α(m), Y(m)) = Q(α(m)). At iteration m + 1, by solving
the subproblems (4) and (3) in order again, we have Q0(α(m+1), Y(m+1)) = Q(α(m+1)). Be-
cause of the nature of coordinate descent, Q0(α(m+1), Y(m+1)) ≤ Q0(α(m), Y(m)), it follows that
Q(α(m+1)) ≤ Q(α(m)).	□
Based on Proposition C.2, the iterations of coordinate descent are similar to that of EM algorithms
and they reduce the value of Q, where Y plays the role of latent variable for EM algorithms.
D More Details ab out Optimization of CDSK
The optimization of CDSK comprises M iterations of coordinate descent, wherein each iteration
solves the following two subproblems.
1)	With constant α,
min Tr(Y>LKY) s.t. Y>DKY = Ic ,	(24)
Y∈Rn×c
2)	With constant Y,
n α+α
min Tr(Y>LKY) — ^X --------------------£KT(Xi — Xj) + λα>Ka
α∈Λ,	i,j=1	2
s.t. Y>DKY = Ic,	(25)
15
Published as a conference paper at ICLR 2022
The first subproblem (24) takes O(n2c) steps using truncated Singular Value Decomposition (SVD)
by Krylov subspace iterative method. We adopt Sequential Minimal Optimization (SMO) (Platt,
1998) to solve the second subproblem (25), which takes roughly O(n2.1) steps as reported in Platt
(1998). SMO is an iterative algorithm where each iteration of SMO solves the quadratic program-
ming (25) with respect to only two elements of the weights α, so that each iteration of SMO can be
performed efficiently. Therefore, the overall time complexity of CDSK is O(M cn2 + M n2.1).
E Proofs
E.1	Proof of Lemma 4.1
Before stating the proof of Lemma 4.1, we introduce the famous spectral theorem in operator theory
below.
Theorem E.1. (Spectral Theorem) Let L be a compact linear operator on a Hilbert space H. Then
there exists in H an orthonormal basis {φ1 , φ2 , . . .} consisting of eigenvectors of L. If λk is the
eigenvalue corresponding to φk, then the set {λk} is either finite or λk → 0 when k → ∞. In
addition, the eigenvalues are real if L is self-adjoint.
Recall that the integral operator by S is defined as
(LS f)(x) =	S(x, t)f (t)dt,
and we are ready to prove Lemma 4.1.
Proof of Lemma 4.1. It can be verified that LS is a compact operator. Therefore, according to
Theorem E.1, {φk} is an orthogonal basis of L2. Note that φk is the eigenfunction of LS with
eigenvalue λk if LSφk = λkφk.
With fixed x ∈ X , we then have
m+'
λkφk(x)φk(t)
k=m
m+'	m+'
≤(X ∣λk∣∣φk(x)∣2)1 ∙ (X ∣λk∣∣φk(t)∣2)1
k=m	k=m
m+'
≤√C (X ∣λk ∣∣Φk (x)|2)2.
k=m
It follows that the series P λkφk(x)φk (t) converges to a continuous function ex uniformly on t.
k≥1
This is because φk = Lλφk is continuous for nonzero λk.
On the other hand, for fixed x ∈ X , as a function in L2,
S(x, ∙) = XhS(x, ∙),φkiφk = X λkφk(x)φk(∙).
k≥1	k≥1
Therefore, for fixed X ∈ X, S(x, ∙) = P λkφk(x)φk(∙) = eχ(∙) almost surely w.r.t the LebeSgUe
k≥1
measure. Since both are continuous functions, we must have S(x, t) = P λkφk(x)φk(t) for any
k≥1
t ∈ X. It follows that S(x, t) = P λkφk(x)φk(t) for any x, t ∈ X.
k≥1
We now consider two series which correspond to the positive eigenvalues and negative eigenvalues
of LS, namely	P	λkφk(x)φk(∙) and P ∣λk∣φk(x)φk(∙). Using similar argument, for fixed
k : λk ≥0	kλ <0
16
Published as a conference paper at ICLR 2022
x, both series converge to a continuous function, and we let
	S+ (x, t) =	λkφk(x)φk(t), k: λk≥0 S-(x,t) = X ∣λk∣φk(x)φk(t). Mλk<0
S+ (x, t) and S- (x, t) are continuous function in x and t. All the eigenvalues of S+ and S- are
nonnegative, and it can be verified that both are PSD kernels since
nn
cicjS+(xi, xj) =	cicj	λkφk(xi)φk(xj)
i,j=1	i,j=1	k: λk≥0 n =	λk	ci cjφk (xi)φk (xj ) k : λk≥0	i,j =1 n = X λk(X ciφ(xi))2 ≥ 0. k : λk≥0	i=1
Similarly argument applies to S-. Therefore, S is decomposed as S(x, t) = S+(x, t) - S-(x, t).
□
E.2 Proof of Theorem 4.2
Lemma E.3 will be used in the Proof of Theorem 4.2. The following lemma is introduced for the
proof of Lemma E.3, whose proof appears in the end of this subsection.
Lemma E.2. The Rademacher complexity of the class HS satisfies
c
R(HS) ≤ (2c-1)XR(HS,y).	(26)
y=1
c>	c>
Lemma E.3. Define Ω+(α) = P α(y) S+α(y) and Ω-(α) = P α(y) S-α(y). When
y=1	y=1
Ω+ (α) ≤ B+2,Ω-(α) ≤ B-2 for positive constant B+ and B-, supχ∈χ∣S+(x, x)| ≤ R2,
supx∈X |S- (x, x)| ≤ R2 for some R > 0, then with probability at least 1 - δ over the data
{xi}in=1, the Rademacher complexity of the class HS satisfies
R(HS) ≤ R(2c- 1)c√B+ + B- + 2c(2c - 1)(B+ + B-)R2∖(P.	(27)
n	2n
Proof of Lemma E.3 . According to Lemma 4.1, S is decomposed into two PSD kernels as S =
S+ - S- . Therefore, the are two Reproducing Kernel Hilbert Spaces HS+ and HS- that are associated
with S+ and S- respectively, and the canonical feature mappings in HS+ and HS- are φ+ and φ-,
with S+(x, t) = hφ+(x), φ+(t)iH+ and S- (x, t) = hφ-(x), φ-(t)iH- . In the following text, we
will omit the subscripts HK+ and HK- without confusion.
For any 1 ≤ y ≤ c,
hS (x, y) = X αiS(x, xi) = hw+, φ+(x)i - hw-, φ-(x)i
i: yi=y
with kw+ k2 = α(y)>S+α(y) ≤ B+2 and kw- k2 = α(y)>S-α(y) ≤ B-2. Therefore,
Hs,y ⊆ HS,y = {(x,y) → hw+,Φ+(x)i-hw-,Φ-(x)i,
kw+k2 ≤ B+2, kw-k2 ≤B-2},1≤y≤ c,
17
Published as a conference paper at ICLR 2022
1 rw/C， ∖	,-	r∙w /C	∖	n ∙	1	F	Ic <-v∖ /zɔ t ∖	1 ∙	1	. 1	Λ	. 1
and R(Hs,y) ⊆ R(Hs,y). Since We are deriving upper bound for R(Hs,y), We slightly abuse the
notation and let Hs,y represent Hs,y in the remaining part of this proof.
For x, t ∈ Rd and any hs ∈ Hs,y, we have
|hs(x) - hs(t)| = ∣hw+,φ+(x)i - {w-,φ-(x)i - (w+ ,φ+(t)i + (w-,φ-(t))∣
=∣hw+,φ+(x) - φ+(t)i + (w-,φ-(t) - φ-(x)i∣
≤ B+kΦ+ (x) - Φ+(t)k + B-∣∣φ-(X)- φ-(t)∣∣)
≤ (B+ + B-)ʌ/S+(x, x) + S +(t,t) + 2√S+ (x,x)S +(t,t)
≤ 2R2(B+ + B-).
We now approximate the Rademacher complexity of the function class Hs,y with its empirical
version R(Hs,y) using the sample {xi}. For each 1 ≤ y ≤ c, Define E(X)} = R(Hs,y)=
E{%}
I n
SUPhS(∙,y)∈Hs,y In ∑ σihs(Xi,y)
1	i=1
,then PIR(HS,y) = E{Xi} [ ∑ EJχi}] , and
SUP
/
X1,...,Xn,Xt
-E(y)	0
>Xt-1,Xt!Xt + * 1!...!Xn Xι,...,Xt-i,Xt,Xt + ι,...,Xn
sup ' E{σi}
X1,...,Xn,Xt
≤ sup / E{%}
X1,...,Xn,Xt
≤ sup ' E{σi}
X1,...,Xn,Xt
≤ sup ' E{σi}
X1,...,Xn,Xt
SUP
hS G,y)∈HS,y
1 n	1
|—fσihs(xi,y) I -	sup	I σhΓ(xihs(xi,y) +
ln 匕	1 hs(∙,y)∈HSJn M
1 n	1
sup	I - V^σihs(xi,y) I - sup	I - V^σihs(xi,y) +
hs(∖y)∈Hs,Jn 慧	1	hs(∙,y)∈Hs,Jn M
sup
hS (Fy)∈HS,y
sup
hS (Fy)∈HS,y
hs(Xt,y)
n
hs (Xiy)
I I n Xσihs(xi,y)I-I n Xσihs(xi,y) + hs(Xt,y) I 口
1 i=1	i=t	1J
n X σihs (Xi,y) - (n X σihs (Xi ,y) + s[t，y)
i=1	i=t
n
sup E{σi}[ sup	hs	- hs (XZy)
Xt,X；	_hS (Iy)∈HS,y	n	n
2R2 (B+ + B-)
.
n
It follows that〔 P EXy) x, 1 X十 乂… X
I / / x1 ,...,xt — 1 ,χt ,xt+1 ,...,xn
1 y=i
cording to the McDiarmid,s Inequality,
-P E(y)	0	I ≤
石]X1,...,Xt-1,Xt,Xt + 1,...,Xn I
2R2(B + +B-)c Ac-
c	c	2
pr[ IX R (HS,y ) - X R(HS,y ) I≥ ε] ≤ 2exp (-	JB-)2R4c2 ).	(28)
y=i	y=i	1	1
18
Published as a conference paper at ICLR 2022
Now we derive the upper bound for the empirical Rademacher complexity:
cc
XRb(HS,y) = XE{σi}	sup
y=1	y=1	hS ∈HS,y
1c
≤ n ΣE{σi}
sup
kw+k≤B+,kw- k≤B-
nn
1XXσihs(Xi)I I
n
Xσi(<w+,φ+(xi)i - hw-,φ-(xi)i)
i=1
(29)
cn	n
≤ - XE{σi} B+k Xσiφ+(Χi)k + B-k Xσiφ-(xi)k
y=1	i=1	i=1
B+c	n
-^-E{σi}	k Eσiφ+ (xi)k
n	i=1
n
k X σiφ- (xi)k
i=1
B+c	n	B-c
≤ ~^~t E{σi} k X σiφ+(xi )k2 + ——t
nn
i=1
n
E{σi}	k Xσiφ-(xi)k2
i=1
B+c uu n	B-c uu n	Rc +
≤ n t ɪj s+ (Xi, xi) + n~ t ɪj s (Xi, xi) ≤ √n (B + B ).
By Lemma E.2, (28) and (29), with probability at least 1 - δ, we have
c
R(HS) ≤ (2c-1)XR(HS,y) ≤
y=1
R(2c - 1)c(B+ + B-)
√n
,	C ∕ln 2
+ 2c(2c - 1)(B+ + B-)R2	δ
2n
(30)
□
Proof of Theorem 4.2 . According to Theorem 2 in Koltchinskii & Panchenko (2002), with proba-
bility 1 - δ over the labeled data S with respect to any distribution in P, the generalization error of
the kernel classifier fS satisfies
RJS) ≤ Rn(fS )+8R(HS )+∕n2lδ
(31)
where Rbn(fs) = 1 P Φ( mhS(：iy) is empirical error of the classifier for γ > 0. Due to the facts
i=1	γ
that mhS (x, y) = hS(xi, yi) - argmaxy0 6=yhS (xi, y0), α is a positive vector and S is nonnegative,
We have mhs (x, y) ≥ hs(xi, y%) - P hs(xi, y). Note that Φ(∙) is a non-increasing function, it
y6=yi
follows that
hS(xi, yi) -	hS(xi, y)
Φ(mhs(xi,yi)) ≤ φ(____________y=y__________
(32)
Applying Lemma E.3, (4) holds with probability 1 - δ. When γ ≥ 1, it can be verified that
I	Ic	n
IIhS(xi,yi) -	hS (xi, y)II ≤	hS(xi, y) ≤	αi = 1 ≤ γ for all (xi,yi), so that
y6=yi	y=1	i=1
hS(xi, yi) - P hS(xi, y)
φ(_____________y=y__________
≤1-
h(xi,yi) -	h(xi, y)
y6=yi
(33)
γ
γ
19
Published as a conference paper at ICLR 2022
Note that when	hs(xi,yi)	- P	hs(xi,y) ≥	0, then Y	hs(xi,yi)	- P	hs(xi,y)	∈	[0,1] so
y6=yi	y6=yi
hS (xi,yi)- P hS (xi,y)	h(xi,yi)- P h(xi,y)
that Φ (----------y=yi--------) = 1--------------y=yi-------. If hs(xi, yi) - P hs(xi, y) < 0, We
γ	γ	y6=yi
hS (xi,yi)- P hS (xi,y)
have Φ (-------------y=yi----------
≤1≤1-
h(xi,yi)- P h(xi,y)
y=yi
Y
. Therefore, (33) always holds.
By the definition of Rbn (fs), (32), and (33), (5) is obtained.
□
Remark E.4. It can be verified that the image of the similarity function S in Lemma 4.1 and The-
orem 4.2 can be generalized from [0, 1] to [0, a] for any a ∈ R, a > 0 with the condition γ ≥ 1
replaced by γ ≥ a. This is because Ls is a compact operator for continuous similarity function
S: X × X → [0, a], and hs(xi, yi) - P hs (xi, y) ≤ P hs(xi, y) ≤ a. Furthermore, given a
y6=yi	y=1
symmetric and continuous function S : X × X → [c, d], c, d ∈ R, c < d, we can obtain a symmetric
and continuous function S0: XXX → [0,1] by setting S0 = S-, and then apply allthe theoretical
results of this paper to CDS with S0 being the similarity function for the similarity-based classifier.
Proof of Lemma E.2. Inspired by Koltchinskii & Panchenko (2002), we first prove that the
Rademacher complexity of the function class formed by the maximum of several hypotheses is
bounded by two times the sum of the Rademacher complexity of the function classes that these
hypothesis belong to. That is,
k
R(Hmax) ≤ 2 X R(Hs,y),
y=1
(34)
where Hmax = {max{h1, . . . , hk} : hy ∈ Hs,y, 1 ≤ y ≤ k} for 1 ≤ k ≤ c - 1.
Ifno confusion arises, the notations ({σi}, {xi, yi}) are omitted in the subscript of the expectation
operator in the following text, i.e., E{σi},{xi,yi} is abbreviated to E. According to Theorem 11
of Koltchinskii & Panchenko (2002), it can be verified that
E{σi},{xi,yi} I〔h”
1n
一废 σihs(Xi)
n
i=1
+
k
≤	E{σi},{xi,yi}
y=1
sup
h∈HS,y
1n
—σihs(Xi)
n
i=1
!+
Therefore,
R(Hmax) = E{σi},{xi,yi}
≤ E{σi},{xi,yi}
sup
h∈H
max
1n
-Vσihs(Xi)
n
i=1
1n
(sup — V"σihs (Xi))十
h∈Hmax n
+E{σi},{xi,yi}
2E{σi},{xi,yi}
sup
h∈Hmax
1n
-n^2σihs (xi)) 十
i=1
sup
h∈H
max
1n
n X σ hs (Xi))十
i=1
k
≤ 2	E{σi},{xi,yi}
y=1
1n	+
(SUP — Σ σihs(xi) +
h∈HS,y n i=1
20
Published as a conference paper at ICLR 2022
k
≤ 2∑ ⅝M,{xw}
y=ι
sup
h∈HS,y
1 n
ZOihS (Xi)
n y
i=1
k
2 X R(HS,y).
y=1
(35)
The equality in the third line of (35) is due to the fact that -Oi has the same distribution as σ%. Using
this fact again, (34), We have
R(HS) = E{σi},{χi,yi}
1 n
sup	-V" Oi mhs(xi ,yi)
mhs ∈Hs n i=1
E{σi},{χi,ya}
1 n c
sup — σ σ σi): mhs (Xi, y)1∙y=yi
mhs ∈Hs n i=1	y=1
c
≤ EE{σi},{Xi,yi}
y=1
sup
m⅛s ∈Hs
1n
nEσimhs (Xi,y)/=yi
n i=1
1c
≤ 2n £ E{σa},{xa,ya}
y=1
n
sup £Oimhs(xi,y)(21Iy=yi - 1)
mhs ∈Hs i=1
1c
+ 2n 乙 E{%},{xi}
y=1
sup
m⅛s ∈Hs
n
E Oimhs (Xi,y)
i=1
1c
=~ ɪs E{σi},{xi}
n y=1
Also, for any given 1 ≤ y ≤ c,
n
sup	>2 Oimhs (Xi,y)
mhs ∈Hs i=1
n E{%},{xi}
sup
m%s ∈Hs
n
£Oimhs (Xi,y)
i=1
(36)
n E{σi},{xi}
n
sup	E Oih S (Xi, y) - OiargmaXy,=y hs (x” y0)
hs (∙,y)∈Hs,y ,y=1---c i=1
≤ n E{σi},{xi}
sup
hs (-,y)∈Hs,y
n
OihS(Xi, y)
i=1
+ n E{σi},{xi}
sup
hs (-,y 0 )∈h s,y,y 0=y
n
EOiargmaXy,=yhs(Xi, y')
i=1
≤ nE{σi},{xi}
sup
hs (-,y)∈Hs,y
n
OihS(Xi, y)
i=1
Combining (36) and (37),
c1
R(HS ) ≤Σ n E{σi},{xi}
y=1
cc
+ XX 2 X
+ n X E{σi},{xi}
y 0=y
sup
hs (-,y)∈Hs,y
E{σi},{xi}
y=1 y=1 y0=y
c
(2c - 1) X Eg},、}
y=1
y=1
sup
hs (∙,yO)∈Hs,
n
OihS(Xi, y)
i=1
sup
hs (∙,y 0 )∈Hs,y
sup
hs (∙,y)∈Hs,y
1
n
n
EOihS (Xi,y0)
i=1
n
EOihS (Xi,y)
i=1
n
£ Oihs (Xi,y0)
i=1
(37)
(38)
□
21
Published as a conference paper at ICLR 2022
E.3 Proof of Theorem A.1
Proof. According to definition of ISE,
ISE(rb, r) =	(rb — r)2 dx =	rb(X, α)2dx — 2	rb(X, α)r(X)dx +	r(X)2dx.	(39)
For a given distribution, RRd r(X)2dx is a constant. By Gaussian convolution theorem,
2>
b(X, α)2dx = τ1 Eas)(K√2hHy)-TI	E	2αiaj K√2h(xi - Xj )Hyi=yj, (40)
R	y=1	1≤i<j≤n
1
where τ1
(2n)d/2(V2h)d .
Moreover,
rb(x, α)r(x)dx
=	pb(x,	1)p(x,	1)dx +	pb(x, 2)p(x,	2)dx -	pb(x, 1)p(x,	2)dx -	pb(x, 2)p(x,	1)dx.
R	R	R	R	(41)
Note that
ɪ P p(x, 1)p(x, 1)dx =	α αjKT(X — Xj)p(x, 1)dx,
τ0 Rd	j: yj=1 Rd
we then use the empirical term
P αjKτ (xi —xj )1Iyi
i : i6=j
xj)p(x, 1)dx. Since E{xi,yi}i6=j
P
bounded difference holds for i : i6=j
n— 1
P αjKτ (xi-xj )1Iyi
i: i=j_______
n—1
Pr
1
to approximate the integral Rd αjKτ(x -
Rd αjKτ (x - xj)p(x, 1)dx, and
αj Kτ (xi—xj )1Iyi=1
——口----------------，therefore,
n—
P αjKτ (Xi - Xj )1Iyi=1
i: i=j------------------------J αjKT (x — Xj)p(x, 1)dx ≥ αjε
≤ 2exp ( — 2(n — 1)ε2).
It follows that with probability at least 1 - 2n1 exp - 2(n - 1)ε2 , where ni is the number of data
points with label i,
αj KT (Xi — Xj)
…yi=yj=1 ]--------------- p(X, 1)p(X, 1)dx
n — 1	τ0 Rd
Similarly, with probability at least 1 — 2n2 exp — 2(n — 1)ε2 ,
P αj KT (Xi — Xj)
……=2- 1------------------------11dP(X, 2)p(X, 2)dx
≤	αjε.
j : yj=1
≤	αjε.
j : yj=2
It follows from (42) and (43) that with probability at least 1 - 2n exp - 2(n - 1)ε2 ,
P αj KT (Xi — Xj)
i,"j'yi	]-------------L	(p(X,1)p(X,
n — 1	τ0	Rd
1)	+ pb(x, 2)p(x, 2)dx ≤ ε.
In the same way, with probability at least 1 - 2n exp - 2nε2 ,
αj Kτ (xi - xj)
ijyi=yj--------------------L	(P(χ,i)p(χ,
n	τ0 Rd
2)	+ pb(x, 2)p(x, 1)dx ≤ ε.
(42)
(43)
(44)
(45)
1
22
Published as a conference paper at ICLR 2022
Based on (44) and (45), with probability at least 1 - 2n2 exp - 2(n - 1)ε2 - 2n exp - 2nε2 ,
P	αjKτ (xi - xj)	P	αjKτ (xi - xj)
i,j : yi 6=yj	i,j : i6=j,yi=yj
ISE(r, r) ≤ 2τo-----2τo--------------
n	n-1
2
+ τ1 X α(y)τ (K√2h)α(y) - τ1	X 2αiαj K√2h(xi - χj )1Iyi=yj +2τ0ε
y=1	1≤i<j≤n
n
P	(αi + αj )Kτ (xi - xj )1Iyi 6=yj	P (αi + αj )Kτ (xi - xj )
1≤i<j≤n	i,j=1
≤ 4τo-----------------------------τo-
nn
21
+ τ1 ^X a(y) (K√2h)α(y) - τ1	^X 2αiaj K√2h (Xi- Xj )1-yi = yj + 2τ0( n - 1 + ε).
y=1	1≤i<j≤n
(46)
The conclusion of this theorem can be obtained from (46).	□
23