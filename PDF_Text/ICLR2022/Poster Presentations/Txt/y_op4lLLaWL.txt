Published as a conference paper at ICLR 2022
Variational autoencoders in the presence of
low-dimensional data: landscape and implicit
BIAS
Frederic Koehler* 1, Viraj Mehta*2, ChenghUi Zhou*3, and Andrej Risteski3
1Department of Computer Science, Stanford University, fkoehler@stanford.edu
2Robotics Institute, Carnegie Mellon University, virajm@cs.cmu.edu
3Machine Learning Department, Carnegie Mellon University, {chenghuz,
aristesk}@andrew.cmu.edu
Ab stract
Variational Autoencoders (VAEs) are one of the most commonly used generative
models, particularly for image data. A prominent difficulty in training VAEs is
data that is supported on a lower dimensional manifold. Recent work by Dai
and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a
conjecture that in standard VAE training the generator will converge to a solution
with 0 variance which is correctly supported on the ground truth manifold. They
gave partial support for this conjecture by showing that some optima of the VAE
loss do satisfy this property, but did not analyze the training dynamics. In this
paper, we show that for linear encoders/decoders, the conjecture is true—that is
the VAE training does recover a generator with support equal to the ground truth
manifold—and does so due to an implicit bias of gradient descent rather than
merely the VAE loss itself. In the nonlinear case, we show that VAE training
frequently learns a higher-dimensional manifold which is a superset of the ground
truth manifold.
1	Introduction
Variational autoencoders (VAEs) have recently enjoyed a revived interest, both due to architectural
choices that have led to improvements in sample quality (Oord et al., 2017; Razavi et al., 2019b;
Vahdat & Kautz, 2020) and due to algorithmic insights (Dai et al., 2017; Dai & Wipf, 2019). Nev-
ertheless, fine-grained understanding of the behavior of VAEs is lacking, both on the theoretical and
empirical level.
In our paper, we study a common setting of interest where the data is supported on a low-dimensional
manifold — often argued to be the setting relevant to real-world image and text data due to the man-
ifold hypothesis (see e.g. Goodfellow et al. (2016)). In this setting, Dai and Wipf (2019) proposed
a two-stage training process for VAEs, based on a conjecture that for standard VAE training with
such data distributions: (1) the generator’s covariance will converge to 0, (2) the generator will learn
the correct manifold, but not the correct density on the manifold (3) the number of approximately
0 eigenvalues in the encoder covariance will equal the intrinsic dimensionality of the manifold (see
also Dai et al. (2017)). Formally, they showed that some optima of the VAE loss satisfy this conjec-
ture, but they did not attempt to analyze the training dynamics.
In this paper, we revisit this setting and explore the behaviour of both the VAE loss, and the training
dynamics. Through a combination of theory and experiments we show that:
•	In the case of the data manifold being linear (i.e. the data is Gaussian, supported on a linear
subspace—equivalently, it is produced as the pushforward of a Gaussian through a linear map),
and the encoder/decoder being parametrized as linear maps, we show that: a) the set of optima
includes parameters for which the generator’s support is a strict superset of the data manifold; b)
*These authors contributed equally to this work.
1
Published as a conference paper at ICLR 2022
the gradient descent dynamics are such that they converge to generators with support equal to the
support of the data manifold. This provides a full proof of the conjecture in Dai & Wipf (2019),
albeit we show the phenomenon is a combination of both the location of the minima of the loss as
well as an implicit bias of the training dynamics.
•	In the case of the data manifold being nonlinear (i.e. the data distribution is the pushforward of
the Gaussian through a nonlinear map f : Rr → Rd , r ≤ d), the gradient descent dynamics from
a random start often converges to generators G whose support strictly contains the support of the
underlying data distribution, while driving reconstruction error to 0 and driving the VAE loss to
-∞. This shows that the conjecture in Dai & Wipf (2019) does not hold for general nonlinear
data manifolds and architectures for the generator/encoder.
Organization: We will provide an informal overview of our findings in Section 3. The rigorous
discussion on the VAE landscape are in Section 4 and on the implicit bias of gradient descent in
Section 5.
2	Setup
We will study the behavior of VAE learning when data lies on a low-dimensional manifold—more
precisely, we study when the generator can recover the support of the underlying data distribution.
In order to have a well-defined “ground truth”, both for our theoretical and empirical results, we will
consider synthetic dataset that are generated by a “ground truth” generator as follows.
Data distribution: To generate a sample point X for the data distribution, We will sample Z 〜
N(0, Ir*), and output X = f (z), for a suitably chosen f. In the linear case, f (Z) = Az, for some
matrix A ∈ Rd×r . In the nonlinear case, f (Z) will be a nonlinear function f : Rr → Rd. We will
consider several choices for f .
Parameterization of the trained model: For the model we are training, the generator will sample
z 〜N(0, Ir) and output X 〜 N(f (z), e2I), for trainable f, e; the encoder given input X will output
z 〜N(g(χ), D), where D ∈ Rr×r is a diagonal matrix, and g, D are trainable. In the linear case,
f, g will be parameterized as matrices A, B; in the nonlinear case, several different parameterizations
will be considered. In either case the VAE Loss will be denoted by L(∙), see (3).
3	Our Results
Linear VAEs: the correct distribution is not recovered. Recall in the linear case, we train a
linear encoder and decoder to learn a Gaussian distribution consisting of data points X 〜 N(0, Σ)—
equivalently, the data distribution is the pushforward of a standard Gaussian Z 〜 N(0, Ir) through
a linear generator X = Az with AAT = Σ; see also Section 2 above. In Theorem 1 of Lucas
et al. (2019), the authors proved that in a certain probabilistic PCA setting where Σ is full-rank, the
landscape of the VAE loss has no spurious local minima: at any global minima of the loss, the VAE
decoder exactly matches the ground truth distribution, i.e. AAT + 2I = Σ.
We revisit this problem in the setting where Σ has rank less than d so that the data lies on the lower-
dimensional manifold/subspace spanned by the columns of A or equivalently Σ, denoted span(A).
We show empirically (i.e. via simulations) in Section 6 that when Σ is rank-degenerate the VAE
actually fails to recover the correct distribution. More precisely, the recovered A has the correct
column span but fails to recover the correct density — confirming predictions made in Dai & Wipf
(2019). We then explain theoretically why this happens, where it turns out we find some surprises.
Landscape Analysis: Linear and Nonlinear VAE. Dai & Wipf (2019) made their predictions on
the basis of the following observation about the loss landscape: there can exist sequences of VAE
solutions whose objective value approaches -∞ (i.e. are asymptotic global minima), for which the
generator has the correct column span, but does not recover the correct density on the subspace.
They also informally argued that these are all of the asymptotic global minima of loss landscape (Pg
7 and Appendix I in Dai & Wipf (2019)), but did not give a formal theorem or proof of this claim.
We settle the question by showing this is not the case:* namely, there exist many convergent se-
quences of VAE solutions which still go to objective value -∞ but also do not recover the correct
*They also argued this would hold in the nonlinear case, but our simulations show this is generally false in
that setting, even for the solutions chosen by gradient descent with a standard initialization — see Section 6.
2
Published as a conference paper at ICLR 2022
column span — instead, the span of such A is a strictly larger subspace. More precisely, We obtain
a tight characterization of all asymptotic global minima of the loss landscape:
Theorem 1 (Optima of Linear VAE Loss, Informal Version of Theorem 3). Suppose that A, B are
fixed matrices such that A = ABA and suppose that #{i : Ai = 0} > r - d, i.e. the number
of zero columns of A Is strictly larger than r 一 d. Then there exists j → 0 and positive diagonal
matrices Dt such that limt→∞ L(A, B,Dt,q) = 一∞. Also, these are all of the asymptotic global
minima: any convergent SequenCe of points (At, Bt, Dt, j) along which the loss L goes to -∞
,∙ C 7	7 Jl	Jl ∙ .1 A	7 π Λ	1.1	. π Γ ∙	7 Cl -	7
satisfies At → A, Bt → B with A = ABA such that #{i : Ai = 0} > r 一 d.
To interpret the constraint #{i : Ai = 0} > r - d, observe that if the data lies on a lower-
dimensional subspace of dimension r* < d (i.e. r* is the rank of Σ), then there exists a generator
which generates the distribution with r - r* > r - d zero columns by taking an arbitrary low-
rank factorization LLT = Σ with L : d X r* and defining A : d X r by A = [L 0d×r-r*]. The
larger the gap is between the manifold/intrinsic dimension r* and the ambient dimension d, the more
flexibility we have in constructing asymptotic global minima of the landscape. Also, we note there
is no constraint in the Theorem that r - d ≥ 0: the assumption is automatically satisfied if r < d.
-1 '	♦	.1	. . ∙	111 ∙ ∙	. ∙ Γ∙ Λ	ɪ 7-1 Λ	. 1	1	Γ∙ ^Λ	.
To summarize, the asymptotic global minima satisfy A = ABA, so the column span ofA contains
that of A, but in general it can be a higher dimensional space. For example, if d, r ≥ r* + 2 and
Ir* +1	0
00
Ir*	0	Ir* +1 0
and the ground truth generator is A =	0r	0 , then A =	r0+1 0 and B
is a perfectly valid asymptotic global optima of the landscape, even though decoder A generates a
Ir*+1 0
different higher-dimensional Gaussian distribution N 0, r0+1 0 than the ground truth. In
the above result we showed that there are asymptotic global minima with higher dimensional spans
even with the common restriction that the encoder variance is diagonal; if we considered the case
where the encoder variance is unconstrained, as done in Dai & Wipf (2019), and/or can depend on
its input x, this can only increase the number of ways to drive the objective value to -∞.
We also consider the analogous question in the nonlinear VAE setting where the data lies on a low-
dimensional manifold. We prove in Theorem 6 that even in a very simple example where we fit a
VAE to generate data produced by a 1-layer ground truth generator, there exists a bad solution with
strictly larger manifold dimension which drives the reconstruction error to zero (and VAE loss to
-∞). The proof of this result does not depend strongly on the details of this setup and it can be
adapted to construct bad solutions for other nonlinear VAE settings.
We note that the nature both of these result is asymptotic: that is, they consider sequences of solu-
tions whose loss converges to -∞ — but not the rate at which they do so. In the next section, we
will consider the trajectories the optimization algorithm takes, when the loss is minimized through
gradient descent.
Linear VAE: implicit regularization of gradient flow. The above theorem indicates that studying
the minima of the loss landscape alone cannot explain the empirical phenomenon of linear VAE
training recovering the support of the ground truth manifold in experiments; the only prediction that
can be made is that the VAE will recover a possibly larger manifold containing the data.
We resolve this tension by proving that gradient flow, the continuous time version of gradient de-
scent, has an implicit bias towards the low-rank global optima. Precisely, we measure the effective
rank quantitatively in terms of the singular values: namely, if σk (A) denotes the k-th largest singular
value of matrix A, we show that all but the largest dim(span A) singular values of A decay at an
exponential rate, as long as: (1) the gradient flow continues to exist* , and (2) the gradient flow does
not go off to infinity, i.e. neither A or e go to infinity (in simulations, the decoder A converges to
a bounded point and e → 0 so the latter assumption is true). To simplify the proof, we work with
a slightly modified loss which “eliminates” the encoder variance by setting it to its optimal value:
*We remind the reader that the gradient flow on loss L(X) is a differential equation dx/dt = -VL(x).
Unlike discrete-time gradient descent, gradient flow in some cases (e.g. dx/dt = x2) has solutions which exist
only for a finite time (e.g. x = 1/(1 - t)), which “blows up” at t = 1), so we need to explicitly assume the
solution exists up to time T .
3
Published as a conference paper at ICLR 2022
T / ^Λ T1I ~∖	♦	T / ^Λ 7-l~7-λ∖ .1∙ 1	1	♦	1	1	、 C	1	Λ 1 ∙	.1 .1
Lι(A, B, e) := mm/ L(A, B, e, D); this loss has a simpler closed form, and We believe the theo-
rems should hold for the standard loss as well. (Generally, gradient descent on the original loss L
Will try to optimize D in terms of the other parameters, and if it succeeds to keep D Well-tuned in
terms of A, B, e then L will behave like the simplified loss Li.)
Theorem 2 (Implicit Bias of Gradient FloW, Informal version of Theorem 5). Let A : d × r be ar-
bitrary and define W to be the Span ofthe rows of A, let Θ(0) = (A(0), B (0), <≡(0)) be an arbitrary
initialization and define the gradient flow Θ(t) = (A(t), B(t), <≡(t)) by the ordinary differential
equation (ODE)
-θΓ = -VLi(Θ(t))	(1)
dt
with initial condition Θ0. If the solution to this equation exists on the time interval [0, T] and satisfies
maxt∈[0,τ] maxj[∣∣(At)j∣∣2 + e2] ≤ K, thenforall t ∈ [0, T] we have
d
X	σ2(A(t)) ≤ C(A,A) e-t∕K	⑵
k=dim(W)+1
where C (A, A) := ∣∣Pw ⊥ AT (0)kF and PW ⊥ is the orthogonal projection onto the orthogonal
complement of W.
Together, our Theorem 1 and Theorem 2 show that if gradient descent converges to a point while
driving the loss to -∞, then it successfully recovers the ground truth subspace/manifold span A.
This shows that, in the linear case, the conjecture of Dai & Wipf (2019) can indeed be validated
provided we incorporate training dynamics into the picture. The prediction of theirs we do not prove
is that the number of zero entries of the encoder covariance D converges to the intrinsic dimension;
as shown in Table 1, in afew experimental runs this does not occur — in contrast, Theorem 2 implies
that A should have the right number of nonzero singular values and our experiments agree with this.
Nonlinear VAE: Dynamics and Simulations. In the linear case, we showed that the implicit bias
of gradient descent leads the VAE training to converge to a distribution with the correct support. In
the nonlinear case, we show that this does not happen—even in simple cases.
Precisely, in the setup of the one-layer ground truth generator, where we proved (Theorem 6) there
exist bad global optima of the landscape, we verify experimentally (see Figure 1) that gradient
descent from a random start does indeed converge to such bad asymptotic minima. In particular,
this shows that whether or not gradient descent has a favorable implicit bias strongly depends on the
data distribution and VAE architecture.
More generally, by performing experiments with synthetic data of known manifold dimension, we
make the following conclusions: (1) gradient descent training recovers manifolds approximately
containing the data, (2) these manifolds are generally not the same dimension as the ground truth
manifold, but larger (this is in contrast to the conjecture in Dai & Wipf (2019) that they should be
equal) even when the decoder and encoder are large enough to represent the ground truth and the
reconstruction error is driven to 0 (VAE loss is driven to -∞), and (3) of all manifolds containing
the data, gradient descent still seems to favor those with relatively low (but not always minimal)
dimension. Further investigating the precise role of VAE architecture and optimization algorithm,
as well as the interplay with the data distribution is an exciting direction for future work.
3.1	Related work
Implicit regularization. Interestingly, the implicit bias towards low-rank solutions in the VAE
which we discover is consistent with theoretical and experimental results in other settings, such as
deep linear networks/matrix factorization (e.g. Gunasekar et al. (2018); Li et al. (2018); Arora et al.
(2019); Li et al. (2020); Jacot et al. (2021)), although it seems to be for a different mathematical
reason — unlike those settings, initialization scale does not play a major role. Similar to the setting
of implicit margin maximization (see e.g. Ji & Telgarsky (2018); Schapire & Freund (2013); Soudry
et al. (2018)), in our VAE setting the optima are asymptotic (though approaching a finite point, not
off at infinity) and the loss goes to -∞. Kumar & Poole (2020); Tang & Yang (2021) also explore
some implicit regularization effects tied to the Jacobian of the generator and the covariance of the
Gaussian noise.
4
Published as a conference paper at ICLR 2022
Architectural and Algorithmic Advances for VAEs. There has been a recent surge in activity
with the goal of understanding VAE training and improving its performance in practice. Much of
the work has been motivated by improving posterior modeling to avoid problems such as “posterior
collapse”, see e.g. (Dai et al., 2020; Razavi et al., 2019a; Pervez & Gavves, 2021; Lucas et al., 2019;
He et al., 2019; Oord et al., 2017; Razavi et al., 2019b; Vahdat & Kautz, 2020). Most relevant to the
current work are probably the works Dai & Wipf (2019) and Lucas et al. (2019) discussed earlier. A
relevant previous work to these is Dai et al. (2017); one connection to the current work is that they
also performed experiments with a ground truth manifold, in their case given as the pushforward of
a Gaussian through a ReLU network. In their case, they found that for a certain decoder and encoder
architectures that they could recover the intrinsic dimension using a heuristic related to the encoder
covariance eigenvalues from Dai & Wipf (2019); our results are complementary in that they show
that this phenomena is not universal and does not hold for other natural datasets (e.g. manifold data
on a sphere fit with a standard VAE architecture).
4 VAE Landscape Analysis
In this section, we analyze the landscape of a VAE, both in the linear and non-linear case.
Preliminaries and notation. We use a VAE to model a datapoint x ∈ Rd as the pushforward of
Z 〜N(0, Ir). We have the following standard VAE architecture:
p(x|z) = N(f(z), 2I),	q(z|x) = N(g(x), D)
where 2 > 0 is the decoder variance, D is a diagonal matrix with nonnegative entries, and f, g, D,
are all trainable parameters. (For simplicity, our D does not depend on x; this is the most common
setup in the linear VAE case we will primarily focus on.) The VAE objective (see Appendix A for
explicit derivation) is to minimize:
L(f, g, D
,E)= Ex~p* EzO〜N(0,Ir)h2-2 kx-f(g(x) + D1/2z0)k2 + kg(x)k2∕2i
+ dlog(-) + Tr(D)/2 - 1 Xlog D®.	(3)
i
We also state a general fact about VAEs for the case that the objective value can be driven to -∞,
which was observed in (Dai & Wipf, 2019): they must satisfy - → 0 and achieve perfect limiting
reconstruction error. The first claim in this Lemma is established in the proof of Theorem 4 and the
second claim is Theorem 5 in Dai & Wipf (2019). For completeness, we include a self-contained
proof in Appendix B.1.
Lemma 1 (Theorems 4 and 5 of Dai & Wipf (2019)). Suppose ft , gt , Dt , -t for t ≥ 1 are
a sequence such that limt→∞ L(ft, gt, Dt, -t) = -∞. Then: 1) limt→∞ -t = 0 and 2)
limt→∞ Ex 〜p* EzO 〜N (0,Ir) kx - ft(gt(x) + DttZ，户=0 ∙
In fact, the reconstruction error and - are closely linked in a simple way:
Lemma 2. If f,g, D are fixed, then the optimal value of - to minimize L is given by - 二
{ d Ex 〜p* EzO 〜N (0,Ir) [kx - f(g(x)+ D1∕2ζ0)k2]∙
4.1	LINEAR VAE
Setup: In the linear VAE case, we assume the data is generated from the model x = AZ with
A ∈ Rd×r* and Z 〜N(0, Ir*). We will denote the training parameters by A ∈ Rd×r, B ∈ Rr×d,
D ∈ Rr×r, and - > 0, where r ≥ 1 is a fixed hyperparameter which corresponds to the latent
dimension in the trained generator, and we assume D is a diagonal matrix. With this notation in
place, the implied VAE has generator/decoder X 〜N(Az, e2Id) and encoder Z 〜N(Bx, D). The
VAE objective as a function of parameters Θ = (A, B, D, -) is (see Appendix A):
l(θ) = 2-2 kA - ABAkF + 2 k BAkF + d log -≡+ 2 X (Diik Aik 2∕N2 + Dii - log Dii) (4)
i
Our analysis makes use of a simplified objective L1 , which “eliminates” D out of the objective by
plugging in the optimal value of D for a choice of the other variables. We use this as a technical tool
when analyzing the landscape of the original loss L.
5
Published as a conference paper at ICLR 2022
ɪ	C /ɪʌ ♦♦	.1	♦	1 ∙ r∙ 11	T ∖ C	.1	7 vɔ -	l- 1 rrfl	. 1	1 ∙
Lemma 3 (Deriving the simplified loss Li). Suppose that A, B, e are fixed. Then the objective
L is minimized by choosing for all i that Dn = 口区 二十声 where Ai is column i of A, and for
Li(A, B, e) := mm/ L(A, B, D, e) it holds that
ι ι	ι	~~	ι~	1 + log (IlAik2 "2)
Lι(A, B, ≡) = 212∣A - ABAkF + q∣BAkF + (d - r)log≡ + ∑-------------------∖----------L. (5)
i
Taking advantage of this simplified formula, we can then identify (for the original loss L) simple suf-
ficient conditions on A, B which ensure they can be used to approach the population loss minimum
by picking suitable j, Dt and prove matching necessary conditions.
Theorem 3. First, suppose that A : d × r, B : r × d are fixed matrices such that A = ABA and
suppose that #{i : Ai = 0} > r - d, i.e. the number of zero columns of A is strictly larger than
r 一 d. Thenfor any Sequence ofpositive & → 0 there exist a sequence ofpositive diagonal matrices
Dt such that:
1.	For every i such that Ai 6= 0, i.e. column i ofA is nonzero, we have (Dt)ii → 0.
Cl+	7^ / 7 Jl ΓΛ ~∖
2.	limt→∞ L(A,B,Dt,3 = -∞.
Z ->	I	.1. .1. ɪ I l ΓΛ ~	1 ■ ,	1 .1.
Conversely, suppose that that At,Bt,Dt,j is an arbitrary sequence such that
limt→∞ L(At, Bt, Dt, jt) = -∞. Then as t → ∞, we must have that:
1.	jt → 0 and kA - AjtBjtAk2F → 0.
2.	maxi (Dt)ii k(At)i k2F → 0 where (At)i denotes the i-th column ofAt.
3.	For any δ > 0, lim inft→∞ #{i : k(Ajt)ik22 < δ} > r - d, i.e. asymptotically Ajt has
strictly more than r - d columns arbitrarily close to zero.
In particular, if (At, Bt, Dt, jt) converge to a point (A, B, D, j) then j= 0, A = ABA, Dii = 0
for every i such that Ai 6= 0, and #{i : Ai = 0} > r - d.
For the sufficiency direction, we observe that in (5) the first term is zero if A = ABA and the sum
of the last two terms goes to -∞ if j→ 0 and enough columns of A are zero. Based upon similar
reasoning combined with Lemma 1, we show necessity. The full proof is in the Appendix.
4.2 Nonlinear VAE
In this section, we give a simple example of a nonlinear VAE architecture which can represent the
ground truth distribution perfectly, but has another asymptotic global minimum where it outputs data
lying on a manifold of a larger dimension (r* + S instead of r* for any S ≥ 1). The ground truth
model is a one-layer network (“sigmoid dataset” in Section 6) and the bad decoder we construct
outputs a standard Gaussian in r* + S dimensions padded with zeros.
Theorem 4 (Theorem 6 in Appendix). Let S ≥ 1 be arbitrary and consider the sigmoid setup
from Section 6. There exists A1 , A2 , B s.t. for jt → 0 there exists Dt s.t. (1) the VAE loss
L(Ai, A2, B, Dt, j) → -∞; (2) The output of the decoder X = Aiz + σ(A2z), Z 〜N(0, Ir)
is a standard Gaussian in the first r* + S coordinates and zero in the remaining ones.
Thus, the generator constructed has as support a manifold of larger dimension (r* + S). Moreover,
in Section 6, we show that this is not merely a theoretical possibility: we show through simulations
that gradient descent from a random initialization often converges to similar minima.
5	Implicit bias of gradient descent in Linear VAE
In this section, we prove that even though the landscape of the VAE loss contains generators with
strictly larger support than the ground truth, the gradient flow is implicitly biased towards low-rank
solutions. We prove this for the simplified loss Li(A, B, e) = min/ L(A, B, e, D), which makes
6
Published as a conference paper at ICLR 2022
the calculations more tractable, though we believe our results should hold for the original loss L as
well. The main result we prove is as follows:
Theorem 5 (Implicit bias of gradient descent). LetA : d×r be arbitrary and define W to be the span
ofthe rows of A, let Θ(0) = (A(0), B(0), <≡(0)) be an arbitrary initialization and define the gradient
flow Θ(t) = (A(t), B(t), c(t)) by the differential equation (1). With initial condition Θ0. If the
solution to this equation exists on the time interval [0, T] and satisfies maxt∈[o,τ] maxj [k(At)jk2 +
€2] ≤ K, thenfor all t ∈ [0, T] we have
d
X	σ2(A(t)) ≤ kPw⊥AT(t)kF ≤ e-t/KkPw⊥AT(0)kF	(6)
k=dim(W)+1
where PW⊥ is the orthogonal projection onto the orthogonal complement ofW.
Towards showing the above result, we first show how to reduce to matrices where A has d -
dim(rowspan(A)) rows that are all-zero. To do this, we observe that the linear VAE objective is
invariant to arbitrary rotations in the output space (i.e. x-space), so the gradient descent/flow trajec-
tories transform naturally under rotations. Thus, we can “rotate” the ground truth parameters as well
as the training parameters.
This is formally captured as Lemma 6 in the Appendix. Recall that by the singular value decomposi-
tion A = USVT for some orthogonal matrices U, V and diagonal matrix S, and rotation invariance
in the x-space lets us reduce to analyzing the case where U = I, i.e. A = SV T. This matrix has a
zero row for every zero singular value.
Analysis when A has zero rows. Having reduced our analysis to the case where A has zero rows,
the following key lemma shows that for every i such that row i of A (denoted A(i)) is zero, the
gradient descent step -VL or -VLi will be negatively correlated with the corresponding row A(i).
Lemma 4 (Gradient correlation). If row i of A is zero, then
X Aij ∂L ≥ X Djj A2j 巴
j =1 ∂Aij j =1
X Aij ∂⅛ ≥ X J.
The way we use it is to notice that since the negative gradient points towards zero, gradient descent
will shrink the size of A(i). Since the size of the matrix A stays bounded, this should mean that for
small step sizes the norm of row i ofA shrinks by a constant factor at every step of gradient descent
on loss L1 . We formalize this in continuous time for the gradient flow, i.e. the limit of gradient
descent as step size goes to zero: for the special case of Theorem 2 in the zero row setting, the
corresponding rows of A decay exponentially fast.
ɪ	一 /I-,	. ∙	1 i	1'	.	∖ T . A 1	t ∙ .	11	. ʌ ∕r∖ ∖	∕7∕c∖ 六 ∕c∖~∕c∖∖
Lemma 5 (Exponential decay of extra rows). Let A be arbitrary, and let Θ(0) = (A(0), B(0), e(0))
be an arbitrary initialization and define the gradient flow Θ(t) = (A(t), B (t), e(t)) to be a solution
of the differential equation (1) with initial condition Θ(0). If the solution exists on the time interval
[0, T ] and satisfies maxt∈[o,τ ] maxj[k(A(t))j ∣∣2 + e(t)2] ≤ K for some K > 0, then for all i such
that row i of A is zero we have ∣∣√4(i)(t)k2 ≤ e-t/K k√4(i)(0)∣2 for all t ∈ [0, T ].
6	Simulations
In this section, we provide extensive empirical support for the questions we addressed theoretically.
In particular we investigate the kinds of minima VAEs converge to when optimized via gradient
descent over the course of training.
Linear VAEs: First, we investigate whether linear VAEs are able to find the correct support for
a distribution supported over a linear subspace. The setup is as follows. We choose a ground
truth linear transformation matrix A by concatenating an r* X r* matrix consisting of iid standard
Gaussian entries with a zero matrix of dimension (d 一 r*) × r*; the data is generated as Az, Z 〜
N(0, Ir*). Thus the data lies in a r*-dimensional subspace embedded in a d-dimensional space.
We ran the experiment with various choices for r* ,d as well as the latent dimension of the trained
decoder (Table 1). Every result is the mean over three experiments run with the same dimensionality
and setup but a different random seed.
7
Published as a conference paper at ICLR 2022
Intrinsic Dimension Ambient Dimension	3 12	3 20	6 12	6 20	9 12	9 20	12 20
Mean #0’s in Encoder Variance	3.3	3.7	6	6	9.3	9	12
Mean # Decoder Rows Nonzero	3	3	6	6	9	9	12
Mean Normalized Eigenvalue Error	0.44	0.71	0.49	0.47	0.30	0.45	0.42
Table 1: Optima found by training a linear VAE on data generated by a linear generator (i.e. a linearly trans-
formed standard multivariate gaussian embedded in a larger ambient dimension by padding with zeroes) via
gradient descent. The results reflect the predictions of Theorem 5: the number of nonzero rows of the decoder
always match the dimensionality of the input data distribution with no variance while the number of nonzero
dimensions of encoder variance is greater than or equal to the nonzero rows. All VAEs are trained with a
20-dimensional latent space. Clearly, the model fails to recover the correct eigenvalues and therefore has a
substantially wrong data density function.
Results: From Table 1 we can see that the optima found by gradient descent capture the support
of the manifold accurately across all choices of d, r, with the correct number of nonzero decoder
rows. We also almost always see the correct number of zero dimensions in the diagonal matrix
corresponding to the encoder variance.
However, gradient descent is unable to recover the density of the data on the learned manifold in
the linear setting — in sharp contrast to the full rank case (Lucas et al., 2019). We conclude this
by comparing the eigenvalues of the data covariance matrix and the learned generator covariance
matrix. In order to understand whether the distribution on the linear subspace has the right density,
We compute the eigenvalue error by forming matrices X, X with n rows, for which each row is
sampled from the ground truth and learned generator distribution respectively. We then compute
the vector of eigenvalues λ, λ for the ground truth covariance matrix AAT and empirical covariance
matrix (1∕n)XTX respectively and compute the normalized eigenvalue error ∣∣λ - λ∣∣∕∣∣λ∣∣. In no
case does the density of the learned distribution come close to the ground truth.
Nonlinear Dataset In this section, we investigate whether VAEs are able to find the correct support
in nonlinear settings. Unlike the linear setting, there is no “canonical” data distribution suited for a
nonlinear VAE, so we explore two setups:
•	Sphere dataset: The data are generated from the unit sphere concatenated with zero padding at the
end. This can be interpreted as a unit sphere embedded in a higher dimensional space. We used 3
layers of 200 hidden units to parameterize our encoder and decoder networks.
To measure how well the VAE has learnt the support of the distribution, we evaluate the average
of (kx：(r+i)k2 - 1)2, where X are generated by the learnt generator. We will call this quantity
manifold error. We have also evaluated the padding error, which is defined as ||Xr+2：k2.
•	Sigmoid Dataset: Let Z 〜 N(0,Ir), the sigmoid dataset concatenates Z with σ((a*, Zi) where
a ∈ Rr is generated according to N(0, Ir). We add additional zero paddings to embed the gen-
erated data in a higher dimensional ambient space. The decoder is parameterized by a nonlinear
function f(Z) = AZ + σ(CZ) and the encoder is parameterized by a linear function g(x) = Bx .
The intrinsic dimension of the dataset is r.
To measure how well the VAE has learnt the support of the distribution, we evaluate the average
of (σ(ha*,X=ri) - Xr+ι)2, where X are generated by the learnt decoder. We will call this quantity
manifold error. The padding error is defined as similarly as the sphere dataset.
Results: Due to space constraints, results for the sphere dataset (Table 3 and Figure 3) are in Ap-
pendix D. In both of the nonlinear dataset experiments, we see that the number of zero entries in
the diagonal encoder variance is less reflective of the intrinsic dimension of the manifold than the
linear dataset. It is, however, at least as large as the intrinsic dimension (Table 3, 2). We consider a
coordinate to be 0 if it’s less than 0.1. We found that the magnitude of each coordinate to be well
separated, i.e. the smaller coordinates tend to be smaller than 0.1 and the larger tend to be bigger
than 0.5. Thus the threshold selection is not crucial. We did not include padding error in the tables
because it reaches zero in all experiments
We show the progression of manifold error, decoder variance and VAE loss during training for the
sphere data in Figure 3 and for the sigmoid data in Figure 2, which are included in Appendix D
due to space constraints. Datasets of all configurations of dimensions reached close to zero decoder
8
Published as a conference paper at ICLR 2022
250-
200-
150-
100-
50-
o-
0.2	0.4	0.6	0.8	1.0
Figure 1: A demonstration that in the nonlinear setting (both types of data padded with zeroes to embed in
higher ambient dimension, see Setup in Section 6) VAE training does not always recover a distribution with
the correct support. Left figure: A histogram of the norms of samples generated from the VAE restricted to
the dimensions which are not zero, which shows many of the points have norm less than 1. (The ground-truth
distribution would output only samples of norm 1.) The particular example here is Column 2 in Table 3. Right
figure: Two-dimensional linear projection of data output by VAE generator trained on our sigmoid dataset.
The x-axis denotes〈a*, x：r〉and the y-axis is xr+ι, the blue points are from the trained VAE and the orange
points are from the ground truth. In contrast to the ground truth data, which satisfies the sigmoidal constraint
xr+1 = σ(ha* , x:r i), the trained VAE points do not and instead resemble a standard gaussian distribution. This
is a case that closely resembles the example provided in Theorem 6. Also similar to Theorem 6, the VAE model
plotted here (from Column 6 in Table 2) achieves nearly-perfect reconstruction error, approximately 0.001.
Intrinsic Dimensions	3	3	5	5	7	7
Ambient Dimensions	7	17	11	22	15	28
VAE Latent Dimensions	6	8	10	16	13	24
Mean Manifold Error	0.09	0.13	0.23	0.24	0.18	0.28
Mean #0’s in Encoder Variance	3	3.6	6	6.3	7.3	8
Table 2: Optima found by training a VAE on the sigmoid dataset. The VAE training consistently yields encoder
variances with number of 0 entries greater than or equal to the intrinsic dimension.
variances, meaning the VAE loss is approaching -∞. To demonstrate Theorem 6, we took examples
from both datasets to visualize their output.
For the sphere dataset, we visualize the data generated from the model, with 8 latent dimensions,
trained on unit sphere with 2 intrinsic dimensions and 16 ambient dimensions (Column 2 in Table
3). Its training progression is shown as the orange curve in Figure 3 . We create a histogram of the
norm of its first 3 dimensions (Figure 1 (a)) and found that more than half of the generated data falls
inside of the unit sphere. The generated data has one intrinsic dimension higher than its training
data, despite its decoder variance approaching zero, which is equivalent to its reconstruction error
approaching zero by Lemma 2.
In the sigmoid dataset, the featured model has 24 latent dimension, and is trained on a 7-dimensional
manifold embedded in a 28-dimensional ambient space. We produced a scatter plot given 1000
generated data points Xr+ι from the decoder. The x-axis in the Figure 1(b) is(a*, X：r〉and the y-axis
is Xr+ι. In contrast to the groundtruth data, whose scatter points roughly form a sigmoid function,
the scatter points of the generate data resemble a gaussian distribution. This closely resembles the
example provided in Theorem 6. Hence, despite its decoder variance and reconstruction error both
approaching zero and loss consistently decreasing, the generated data do not recover the training
data distribution and the data distribution recovered has higher intrinsic dimensions than the training
data. We also investigated the effect of lower bounding the decoder variance as a possible way to
improve the VAE performance (details are given in Appendix E). This enabled the VAE to recover
the correct manifold dimension in the sigmoid example, but not the sphere example; methods of
improvements to the VAE’s manifold recovery is an important direction for future work.
9
Published as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32:7413-7424, 2019.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019.
Bin Dai, Yu Wang, John Aston, Gang Hua, and David Wipf. Hidden talents of the variational
autoencoder. arXiv preprint arXiv:1706.05148, 2017.
Bin Dai, Ziyu Wang, and David Wipf. The usual suspects? reassessing blame for vae posterior
collapse. In International Conference on Machine Learning, pp. 2313-2322. PMLR, 2020.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534,
2019.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Arthur Jacot, Francois Ged, Franck Gabriel, Berfin Simsek, and Clement Hongler. Deep linear
networks dynamics: Low-rank biases induced by initialization scale and l2 regularization. arXiv
preprint arXiv:2106.15933, 2021.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Abhishek Kumar and Ben Poole. On implicit regularization in β-vaes. In International Conference
on Machine Learning, pp. 5480-5490. PMLR, 2020.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47. PMLR, 2018.
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent
for matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839, 2020.
James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don’t blame the elbo! a
linear vae perspective on posterior collapse. Advances in Neural Information Processing Systems,
32:9408-9418, 2019.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. arXiv preprint arXiv:1711.00937, 2017.
Adeel Pervez and Efstratios Gavves. Spectral smoothing unveils phase transitions in hierarchical
variational autoencoders. In International Conference on Machine Learning, pp. 8536-8545.
PMLR, 2021.
Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-vaes. arXiv preprint arXiv:1901.03416, 2019a.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. In Advances in neural information processing systems, pp. 14866-14876, 2019b.
Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. Kybernetes, 2013.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
10
Published as a conference paper at ICLR 2022
Rong Tang and Yun Yang. On empirical bayes variational autoencoder: An excess risk bound. In
Conference on Learning Theory, pp. 4068-4125. PMLR, 2021.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020.
A Derivations of VAE losses
We have (for some constants C1, C2, C3):
logp(x∣z) = - 12k ∣∣x - f(z)k2 - dlog(e) + Ci
22
log p(z) = -∣z∣2∕2 + C2
log q(z∣x) = — 2hz — g(x), DT(Z — g(x)i — log √det D + C3
where the firstline uses that log √dete2I = log √e2d = d log(e). The VAE objective is to maximize
the expectation of logp(x∣z) + logp(z) — log q(z∣x) for X from the data p* and Z 〜q(z∣x). This
means that explicitly the objective is to maximize
Ex〜p* Ez〜q(z|x) [logp(X∣Z)+logP(Z) — log q(Z∣X)] — C
Ez 〜q(z∣x) — 22 ∣∣x — f (Z)Il2 — d Iog(E) — IlZk2/2 + 2 hz — g(X), DT(Z — g(X)i + log
Ez0 〜N (0,Ir) h — 22 kx — f (g(X) + D1/2ZO)Il2 — d log(E)-Ilg(X) + D1/2/112/2
+ 2 (Z0, Z0i + log √det Di
which simplifies to (up to additive constant)
Ex 〜p* EzO 〜N (0,Ir) h — 2^2 ∣∣x — f (g(X) + D1/2 Z0)k2 — kg(X) k2/2] — d IOg(E)- Tr(D)/2 + 2 X log Dii.
i
and converting this to minimization form gives the VAE Loss (3).
Linear VAE derivation. Plugging in the linear VAE parameters into the loss function, we get
l(A,B,D,可：=ExeEzO〜N(0,Ir) h212∣X — A(BX + D 1∕2Z0)k2 + ∣B/『/斗	⑺
1
+ d log(e) + Tr(D)/2 — 2£log Dii	(8)
i
We can write out the expectation as:
Ez〜N(0,I)EzO〜N(0,Ir) h2E2 IIAz — A(BAZ + D 1/220)||2 + kBANk2/2]
=Ez 〜N (0,I)EzO 〜N (0,I^) h 2121(A — AB A)Z — AD 1∕2Z0∣∣2 + IB Az『/2]
=⅛2 IA - AB AkF + 与 IAD1/2IF + 1 IB AkF
2E2	2E2	2
where We used that z, z0 are independent and the identity Ez〜N(o,I) IlMZll2 =(MMT, Ii =
IM I2F . Next, we can observe that
IAD1/2IF = X D ii IAiI2
i
where Ai is the i-th column of the matrix A. Therefore we recover (4).
11
Published as a conference paper at ICLR 2022
B Deferred Proofs from Section 4
B.1	General Facts
Proof of Lemma 1. For completeness, we include the proof of these claims; they are similar to the
proofs of Theorems 4 and 5 in Dai & Wipf (2019).
First, consider the objective for fixed f, g, D, and omit the subscript t. We have
Ex 〜p* Ez0 〜N (0,ιr) hEkx - f (g(x) + D1/2z0)k2 + kg(x)k2/2i ≥ 0
Tr(D)/2 - 1 Xlog Dii = J X (Dii- log Dii) ≥ r/2
ii
from the inequality x - log x ≥ 1 for x ≥ 0. Since these terms are both bounded below, the only
way the objective goes to negative infinity is if d log → -∞ which means → 0.
Now that we know Et → 0, we claim that limt→∞ Ex〜p* Ez，〜N(o,ir)kx-ft(gt(x)+D1/2z0)k2 = 0.
Suppose otherwise: then this for infinitely many t this quantity is lower bounded by some constant
c > 0, hence the objective for those t is lower bounded by c/E2 + d log(E) + r/2 and this goes to
+∞ as E → 0, instead of -∞.	□
Proof of Lemma 2. Taking the partial derivative of (3) with respect to E and setting it to zero gives
0 = - E3 Ex 〜p* EzO 〜N (0,Ir)kx - f(g(x) + D1∕2z0)k2 + —
and solving for E gives the result.	□
B.2	Linear VAE
ProofofLemma 3. Taking the partial derivative with respect to D ii gives 0 = ∣∣Aik2∕V2 + 1 - 1/D 近
which means
〜	1	e2
D--=---------------=-----------
J__/ 7 7   〜	-- 〜
kAik2∕3 + 1	kAik2 + ?2
hence
，一，	—	—2
D iikAik /E + D ii - log D ii = 1 - log ,, 2 ,，2-Σ2 .
kAik2 + E—2
It follows that the objective value at the optimal D is
LI(A, B,—) = 2—2IIA - ABAlIF + 2IIBAlIF + dlog — + 2 X (1 - log k^— k^ + ―2)
=2—2kA - ABAkF + 2kBAkF + (d - r)log—+ 2 X (1 - log ∣a ∣2 + —2).
□
Proof of Theorem 3. First we prove the sufficiency direction, i.e. that ifA = ABA and there exists
i such that A—i = 0 then we show how to drive the loss to -∞. By Lemma 3, if we make the optimal
choice of D (which clearly satisfies the conditions on D described in the Lemma) the objective
simplifies to
LI(A, B, E) = 2—2kA - ABAkF + 2IlBAkF + (d - r)log—+ 2 X (1 + log (kA』2 +—2))
=2kBAkF + (d - r)log — + 1 X (1 + log (kAik2 + —2))
12
Published as a conference paper at ICLR 2022
1 ∙ .1	1 1 ∙	1 . 1	. ∙ 4 ɪ 7-1 Λ TL T . .1 . Γ∙	1	1	7 Γ∖
where in the second line we used the assumption A = ABA. Note that for each zero column Ai = 0
We have (1∕2)log(∣∣Aik2 + e2) = logH so the objective will go to -∞ provided (d - r + #{i :
Ai = 0}) log H → -∞. Since H → 0 this is equivalent to asking d - r + #{i : Ai = 0} > 0, which
is exactly the assumption of the Theorem.
Next we prove the converse direction, i.e. the necessary conditions. Note: we split the first item
in the lemma into two conclusions in the proof below (so there are four conclusions instead of
three). The first conclusion follows from the first conclusion of Lemma 1. The second conclusion
of Lemma 1 tells us that
0 = lim Ez〜N(0,I)Ez0〜N(0,I;OkAz-At(BtAz + ^1/220)112 = lim IlA-AtBtAllF +kAtD^'kF
t→∞	r	t→∞
which gives us the second and third conclusions above. For the fourth conclusion, since
L1(At, Bt, Dt) ≤ L(At, Bt, Dt, Ht) we know that limt→∞ L1(At, Bt, Dt) = -∞ and recalling
Lι(A, B, H) = 22kA - ABAIIF + 2IlBAIIF + (d - r)log H+ 2 X (1 +log (kA/l2 + H2))
i
we see that, because the first two terms are nonnegative, this is possible only if the sum of the last
two terms goes to -∞. Based on similar reasoning to the sufficiency case, this is only possible if
strictly more than r - d of the columns of (At) become arbitrarily close to zero; precisely, if there
exists δ such that at most r - d of the columns of At have norm less than δ, then
(d - r) log H + J X(1 + log (kAik2 + H2))
≥ 1 X (l+log (kAik2 + H2))
i"∣Aik≥δ
≥ 2 X (1 + log(δ2 +H2))
i"∣Aik≥δ
which does not go to -∞ as H → 0 (and the other terms of L1 are nonnegative).	□
B.3 Nonlinear VAE
We give the full details of the construction of the bad nonlinear VAE optimum and prove that it is
an asymptotic global minimum. (Note: in the notation of Section 6 we are considering a* with 0/1
entries, but the proof generalizes straightforwardly for arbitrary a* with the correct support.)
Setup: Suppose s ≥ 1 is arbitrary and the ground truth x ∈ Rd with d > r* + s is generated in the
following way: (xι,..., x『*)〜 N(0, I『*), x『* + i = σ(xι T-----+ x『*) foran arbitrary nonlinearity
σ, and x『*+2 = •…=Xd = 0. Furthermore, suppose the architecture for the decoder with latent
dimension r > r* + 1 is
fA1,A2(z) := AIz + σ (A2z)
where σ(∙) is applied as an entrywise nonlinearity, and the encoder is linear, g(x) := Bx.
Observe that the ground truth decoder can be expressed by taking AH2 to have a single nonzero row
in position r + 1 with entries (1, . . . , 1, 0, . . . , 0),
BH
Ir*
0
0
0
where B is a ground truth encoder which achieves perfect reconstruction.
On the other hand, the following VAE different from the ground truth achieves perfect reconstruc-
tion:
AH1=	Ir*0+s	00	,	AH2=0,	BH =	Ir*0+1	00	(9)
13
Published as a conference paper at ICLR 2022
The output of this decoder is a Gaussian N(0, Ir0+s 0 ), which means it is strictly higher-
dimensional than the ground truth dimension r*. (This also means that if we drew the corresponding
plot of to Figure 1 (b) for this model, we would get something that looks just like the experimentally
obtained result.) We prove in the Appendix that it this is an asymptotic global optima:
Theorem 6. Let s ≥ 1 be arbitrary and the ground truth and VAE architecture is as defined as
above. For any Sequence Nt → 0, there exist diagonal matrices D)t such that:
1.	the VAE loss L(A1, A2, B, Dt, Nt) → -∞ where A1, A2, B are defined by (9)
2.	The number ofcoordιnates of Dt which go to zero equals r* + S.
Proof. We show how to pick DNt as a function of Nt and that if Nt → 0, the loss goes to -∞. From
now on we drop the subscripts.
With these parameters, the VAE loss is
Ex〜p* Ez0〜N(0,Ir) h2∣2 kx - f (g(x) + D 1∕2z0)∣∣2 + Ilg(X)Il 2/2] + d log® + Tr(D)∕2 - 2 X log Dii
i
r*+1	1
=(1∕222) X Dii + Ex〜p* b∣χLr* + 1k2/2] + dlog® + Tr(D)/2 - 2 X log Dii∙
Taking the partial derivative with respect to DN ii for i ≤ r* + s and optimizing gives 0 = (1/N2) +
1 - 1/DN ii i.e.
1
DN ii
N2
1 + 1/N2	N2 + 1
and plugging this into the objective gives
r* + 1	ι
(1∕222) ^X Dii + Ex〜p* [kχLr* + ιk2∕2] + dlog(e) + Tr(D)/2 - $ ^Xlog Dii
i=1
r*+1	1
(1/2) X ^2- + Ex~p* [kxi：r* + ik2/2] + (d - r* - s) log®
i=1	+ 1
i
r
r* + 1	1 r
+ Tr(D)/2+	2 log(1 + e2) + 2 E logDii.
i=r* +2
Setting the remaining Dii to 1, we see that using d > r* + s that the loss goes to -∞ provided
€ → 0, proving the result.	□
C Deferred Proofs from Section 5
First, we formalize the rotation invariance of the objective.
Lemma 6 (Rotational Invariance of Gradient Descent on Linear VAE). Let LA (A, B, D, €N) denote
the VAE population loss objective (4). Then for an arbitrary orthogonal matrix U, we have
LA(AN,BN,DN,€N) =LUA(UAN,BNUT,DN,€N).
Furthermore,
U VALA(A, B, D, W) = VUALUA(UA, B U T, D,可
and
(VB La(A, B, D, N))U T = VUB LUA(UA, B U T, D, N).
As a consequence, iffor any η ≥ 0 we define (A1, B1, D1, €N1) = (A, B, D, €N) - ηVLA (A, B, D, €N)
then
(UAι, BiUT, Dι,Nι) = (UA, BUT, D, N)- N(ua,BuTDaLUA(UA, BUT, D, N),
i.e. gradient descent preserves rotations by U. The same result holds for the gradient flow (i.e.
continuous time gradient descent), or replacing everywhere the loss L by the simplified loss L1 .
14
Published as a conference paper at ICLR 2022
ProofofLemma 6. We give the proof for L as stated, but it is exactly the same for the simplified
loss L1.
From the objective function (4) and UT = U-1 observe that
LUA(UA 力 U T ,D ,e)
=2∣2 IIua - UABU-IUAllF + $IlBU-1 UAllF + diog2+ 2 ^X (D近||UAilI2/群 + DDn - logDD，
i
=2∣2 ∣∣a - ABAkF + $ IIBBAkF + dlog 三十 2 X (DDMlAiIl2/烂 + DDa — log DD近)
i
_ , ≈ ~ ~ 、
=LA(A)B ,d,e)∙
Then from the above and the multivariate chain rule have
VALA(A, B, D, W) = VaLua(UA, B U t , D, W = U t (VujaLua(U,4, B U-1,D, W
so multiplying both sides on the left by U and using UT = U-1 gives the second claim, and similarly
VBLA(A, B, D, W) = VLUA(U A, B U T, D, W = (VBU T LUA(UA, B U T, D, W))U
gives the third claim. Then the gradient descent claim follows immediately.	□
ProofofLemma 4. First We prove the conclusion for the original loss L. Since (ABA)i'
∑j,k AijBjkAk' we have that
训A - ABAkF
∂Aj
∂AP X(Aie - X AijO Bjlk Ake I
ij '	∖	j0,k	)
∑2 Aie-E A,/ B" Ake
-E Bjk Ake
k
and if we know the corresponding row i in A is zero then this simplifies to
训A -ABAkF = X 2 (X AijOBjkAkej (X BjkAke)
which means that
X AijdkA - ABAkF = X2(X AijBjkAke] = 2k(ABA)⑺k2
j	dAij	e "	)
where the notation A(i) denotes row i of matrix A. Thus, for this term the gradient with respect to
row A(i) has nonnegative dot product with row A(i).
Also,
^^^(1∕2) X Djjk Ajk2∕w2 = DjjAij/W2
dAj V
and so
X Aij -A-(1∕2) X Dj kAj k2∕w2 = X Djj Aj∕w2
j	ij	j	j
which gives the first result.
For the second result with the simplified loss L1, observe that
，-X log(kAkk2 + W2) = ~2 Aij
∂Aj± gUI k" JkAj k2 + W2
so
X Aij -A- X iog(kAkk2 + W2) = X
j	-Aij k	j
2Aj
kAj k2 + W2
and the other terms in the loss behave the same in the case of L. Including the factor of 1∕2 from
the loss function gives the result.	□
15
Published as a conference paper at ICLR 2022
Proof of Lemma 5. From Lemma 4 we have that for any such row i,
-d kA叫t)k2 = 2(A⑴(t)$A⑴(t)i
dt	dt
r
2hA⑴⑴,-VA(t)(i)Lι(Θt)i ≤- X
j=1
(A(t))2j
k(A(t))jk2 + M
≤-(i∕κ)kA叫t)k2
which by Gronwall,s inequality implies ∣∣√4(i)(t)k2 ≤ e-t/Kk√4(i)(0)k2 as desired.	□
Proof of Theorem 5. Before proceeding, we observe that the first inequality in (6) is a consequence
of the general min-max characterization of singular values, see e.g. Horn & Johnson (2012). We
now prove the rest of the statement.
As explained at the beginning of the section, we start by taking the Singular Value Decomposition
A = U SV T where S is the diagonal matrix of singular values and U, V are orthogonal. We assume
the diagonal matrix S is sorted so that its top-left entry is the largest singular value and its bottom-
right is the smallest. Note that this means the first dim(W) rows of U are an orthonormal basis
for W. Note that for any time t, ∣∣Pw⊥AT(t)∣F = Pd=dim(w)+i k(UA(t)T)i∣∣2 because the rows
(Udim(W)+1, . . . , Ud) are an orthonormal basis for W⊥. Therefore we have that
d
∣Pw ⊥ AT (t)kF =	X	k(uA(t)T )ik2
i=dim(W)+1
d
≤ e-t/K	X	k(UA(0)T)ik2 = e—t/K∣Pw⊥AT(0)kF,
i=dim(W)+1
proving the result, provided we justify the middle inequality. Define A* := UTA = SVT, which
has a zero row for every zero singular value ofA, and apply Lemma 5 (using that the definition ofK
is invariant to left-multiplication of A by an orthogonal matrix) and Lemma 6 to conclude that the
rows of UTA(t), i.e. the columns of uA(t)T, corresponding to zero rows of A* shrink by a factor
of eT/K. This directly gives the desired inequality, completing the proof.	□
D Deferred Figures and Plots from Section 6
Eigenvalues of Linear Data. As we’ve discussed, in our linear setting the VAE does not recover
the ground truth data density. Since our generative process for ground-truth data is x = Az for a
matrix A and z normally distributed, we can characterize the density function by the eigenvalues of
the true or estimated covariance matrix. We give figures for the normalized error of these eigenvalues
between the learned generator and the ground truth in Table 1. A concrete example of eigenvalue
mismatch for a problem with 6 nonzero dimensions is a ground-truth set of covariance eigenvalues
λ = [0.001 0.156 1.54 5.06 9.55 16.4]
while the trained linear VAE distribution has covariance eigenvalues
ʌ -
^ = [0.035 0.166 1.49 4.24 5.97 7.85].
Here, the VAE was easily able to learn the support of the data but clearly is very off when it comes
to the structure of the covariances.
E	Experiments with Decoder Variance Clipping
As was suggested by an anonymous ICLR 2022 reviewer, one potential way to evade the results
in our paper is to restrict the decoder variance from converging to 0. In this section, we examine
(empirically) the impact of clipping the decoder variance during training. We caveat though, that
our paper does not analyze the landscape of the resulting constrained optimization problem, so our
results don’t imply anything about this regime.
16
Published as a conference paper at ICLR 2022
3 intrinsic dim; 7 ambient dim; 6 latent dim
3 intrinsic dim; 17 ambient dim； 8 latent dim
5 intrinsic dim； 22 ambient dim; 16 latent dim
5 intrinsic dim； 11 ambient dim; 10 latent dim
7 intrinsic dim； 15 ambient dim; 13 latent dim
7 intrinsic dim； 28 ambient dim; 24 latent dim
Figure 2: VAE training on 6 datasets with different choices of dimensions for sigmoidal dataset (see Setup
in Section 6). The x-axis represents every 5000 gradient updates during training. The left-most figure is
the manifold error (see Setup in Section 6), The middle and right figure confirms that the decoder variance
approaches zero and the VAE loss is steadily decreasing during the finite training time.
2 intrinsic dim; 6 ambient dim; 6 latent dim
2 intrinsic dim； 16 ambient dim； 8 latent dim
4 intrinsic dim； 21 ambient dim； 16 Iateit dim
4 intrinsic dim； 10 ambient dim； 10 latent dim
6 intrinsic dim； 14 ambient dim； 13 latent dim
Figure 3: VAE training on 5 datasets generated by appending zeros to uniformly random samples from a unit
sphere to embed in a higher dimensional ambient space. The x-axis represents each iteration of every 5000
gradient updates. The left-most figure is the manifold error ( see Setup in Section 6), The middle and right
figure confirms that the decoder variance approaches zero and the VAE loss is steadily decreasing during the
finite training time.
Intrinsic Dimensions	2	2	4	4	6
Ambient Dimensions	6	16	10	21	14
VAE Latent Dimensions	6	8	10	16	13
Mean Manifold Error	0.02	0.14	0.04	0.06	0.03
Mean #0’s in Encoder Variance	3	5	5	6	7
Table 3: Optima found by training a VAE on data generated by padding uniformly random samples from a
unit r-sphere with zeroes, so that the sphere is embedded in a higher ambient dimension. We evaluated the
manifold error as described in the setup. The VAE training on this dataset has consistently yielded encoder
variances with number of 0 entries greater than the number of intrinsic dimension.
17
Published as a conference paper at ICLR 2022
We conduct the same nonlinear experiments described in Section 6 where we fit VAEs to data gen-
erated from spheres and linear sigmoid functions. The only change is to clip the decoder variance
when it goes below a certain threshold. In the figures below, the chosen threshold is e-4 ≈ 0.018,
though we tried also e-2 and e-3, with similar outcomes. We initialize the decoder variance with
e-3 for this set of experiments, so the optimization still can decrease it.
With this change, the optimization process on the sigmoid dataset does yield encoder variances
with their number of zeros reflective of their intrinsic dimensions as in Table 4. For the sphere
experiment, this still does not happen, as in Table 5. In fact, the model consistently recovers one
more dimension than the true intrinsic dimension of the manifold and the smaller encoder variances
can be as large as 0.1. We also provide a figure (Figure 4) in the same style as Figure 1. We see
that training with a clipped decoder variance allows the model to better capture the general shape
of the sigmoid function, though the variance of the generated points is high for both of the sphere
and sigmoid datasets. Other training details, such as the general trend of manifold error, encoder
variance and VAE loss, can be referred to in Figure 5 and 6.
Overall, the benefit of clipping the decoder variance during training is inconclusive as we see in-
consistent results in the sphere and sigmoid datasets. Designing more algorithms to improve the
ability of VAE’s to recover data supported on a low dimensional manifold is an important direction
for future work—both empirical and theoretical.
Figure 4: A demonstration of how the data points generated by the model trained with clipped decoder variance
is distributed. Left figure: A histogram of the norms of samples generated from the VAE restricted to the
dimensions which are not zero, which shows many of the points have norm less than 1. (The ground-truth
distribution would output only samples of norm 1.) The particular example here is Column 2 in Table 5. The
data points that do not fall on the sphere tend to lie on both sides of it whereas the those generated without
decoder variance clipping tend to lie inside the sphere as in Figure 1. Right figure: Two-dimensional linear
projection of data output by VAE generator trained on our sigmoid dataset. The x-axis denotes〈a*, X：r)and
the y-axis is Xr+ι, the blue points are from the trained VAE and the orange points are from the ground truth.
The generated data points roughly capture the shape of the sigmoid function.
3 intrinsic dim; 7 ambient dim; 6 latent dim
3 intrinsic dim; 17 ambient dim; 8 latent dim
5 intrinsic dim; 22 ambient dim; 16 latent dim
5 intrinsic dim; 11 ambient dim; 10 latent dim
7 intrinsic dim; 15 ambient dim; 13 latent dim
7 intrinsic dim; 28 ambient dim; 24 latent dim
Figure 5: VAE training on 6 datasets with different choices of dimensions for sigmoidal dataset (see Setup
in Section 6). The x-axis represents every 5000 gradient updates during training. The left-most figure is the
manifold error (see Setup in Section 6), The middle and right figure shows that as the decoder variance is
bounded below, the VAE loss stops decreasing further.
18
Published as a conference paper at ICLR 2022
2 intrinsic dim; 6 ambient dim; 6 latent dim
2 intrinsic dim; 16 ambient dim; 8 latent dim
4 intrinsic dim; 21 ambient dim; 16 latent dim
4 intrinsic dim; 10 ambient dim; 10 latent dim
6 intrinsic dim; 14 ambient dim; 13 latent dim
Figure 6: VAE training on 5 datasets generated by appending zeros to uniformly random samples from a unit
sphere to embed in a higher dimensional ambient space. The x-axis represents each iteration of every 5000
gradient updates. The left-most figure is the manifold error ( see Setup in Section 6), The middle and right
figure shows that as the decoder variance is bounded below, the VAE loss stops decreasing further.
Intrinsic Dimensions	3	3	5	5	7	7
Ambient Dimensions	7	17	11	22	15	28
VAE Latent Dimensions	6	8	10	16	13	24
Mean Manifold Error	0.15	0.15	0.23	0.23	0.24	0.24
Mean #0’s in Encoder Variance	3	3	5	5	7	7
Table 4: Optima found by training a VAE on the sigmoid dataset. The VAE training yields encoder variances
with number of 0 entries equal to the intrinsic dimension.
Intrinsic Dimensions	2	2	4	4	6
Ambient Dimensions	6	16	10	21	14
VAE Latent Dimensions	6	8	10	16	13
Mean Manifold Error	0.03	0.03	0.03	0.02	0.02
Mean #0.1’s in Encoder Variance	3	3	5	5	7
Table 5: Optima found by training a VAE on data generated by padding uniformly random samples from a
unit r-sphere with zeroes, so that the sphere is embedded in a higher ambient dimension. We evaluated the
manifold error as described in the setup. The VAE training on this dataset has consistently yielded encoder
variances with number of 0.1 entries greater than the number of intrinsic dimension.
19