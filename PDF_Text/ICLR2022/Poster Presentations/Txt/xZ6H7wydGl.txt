Robust and Scalable SDE Learning:
A Functional Perspective
Scott Cameron
Oxford University, InstaDeep Ltd.
United Kingdom
s.cameron@instadeep.com
Tyron Cameron
Discovery Insure
South Africa
Arnu Pretorius
InstaDeep Ltd.
South Africa
Stephen Roberts
Oxford University
United Kingdom
Ab stract
Stochastic differential equations provide a rich class of flexible generative models,
capable of describing a wide range of spatio-temporal processes. A host of re-
cent work looks to learn data-representing SDEs, using neural networks and other
flexible function approximators. Despite these advances, learning remains compu-
tationally expensive due to the sequential nature of SDE integrators. In this work,
we propose an importance-sampling estimator for probabilities of observations of
SDEs for the purposes of learning. Crucially, the approach we suggest does not
rely on such integrators. The proposed method produces lower-variance gradient
estimates compared to algorithms based on SDE integrators and has the added
advantage of being embarrassingly parallelizable. This facilitates the effective use
of large-scale parallel hardware for massive decreases in computation time.
1 Introduction
Stochastic differential equations (SDEs) are a natural extension to ordinary differential equations
which allows modelling of noisy and uncertain driving forces. These models are particularly appealing
due to their flexibility in expressing highly complex relationships with simple equations, while
retaining a high degree of interpretability. Much work has been done over the last century focussing
on understanding and modelling with SDEs, particularly in dynamical systems and quantitative
finance (Pavliotis, 2014; Malliavin & Thalmaier, 2006). Examples of such work are the Langevin
model of stochastic dynamics of particles in a fluid, stochastic turbulence (Kumar et al., 2020), and
the Black-Scholes model. More recently, SDE models have gathered interest in machine learning
and current work focuses on efficient and scalable methods of inferring SDEs from data.
1.1 The SDE learning problem
Consider an It6 SDE (Pavliotis, 2014) of the following form
dX = f(X,t)dt+g(X,t)dW,	(1)
and a set of observations xi：N = {(tn, Xn)}N=「In the simplest case, these observations would
be directly of a realisation of the process at discrete points in time. In many physical applications,
the observations themselves would have some uncertainty associated with them, described by an
observation noise model. In more complex models, the SDE would be driving some unobserved
parameters describing further complex processes (Li et al., 2020). For example, discrete events
may be modeled by an inhomogenious Poisson processes, where the rate parameter is evolving
stochastically over time. In any case, the generating process can be summarized by the graphical
model in the following diagram and the corresponding learning problem can be described as: “given
observations x1:N, infer f and g”.
1
For the most part, the model can be trained using standard techniques, such as variational inference,
Markov chain Monte Carlo or optimizing an appropriate loss function. Generally, this requires
estimating an expectation over sample-paths, either to estimate the marginal probability of the obser-
vations as a function of f and g, or to estimate the mismatch of sample-paths with the observations. To
estimate these expectations, most current work makes use of integration algorithms. These algorithms
generate trajectories of X for a given f and g, which can be used to estimate the required expectation.
For example, the EUler-MarUyama algorithm (Li et al., 2020; Jia & Benson, 2019; LiU et al., 2019)
generates trajectories with the following update rule
Xt+∆t - Xt + f (Xt, t) δ% + g(Xt,t)N(O, δ"∙
(2)
The expectation of some fUnctional F is estimated Using the sample mean of its valUe over many
independent trajectories X (k)
1K
F = K X F[X(k)].	(3)
k=1
UnfortUnately, integrators sUch as EqUation (2) introdUce a seqUential dependence between each point
and the next, which inhibits parallelization. FUrthermore, it introdUces difficUlties in probabilistic
modelling of the fUnctions f and g ; in order to properly estimate the expectation, the sample paths
shoUld be generated Under consistent realisations of f and g. Models which implicitly describe f as a
GaUssian process woUld need to sample f(Xt, t) conditioned on all the previoUsly sampled valUes of
f along the trajectory, which may be compUtationally prohibitive.
The integrator generates trajectories sampled from the prior distribUtion over paths. In the initial
stages of learning, the path-space prior distribUtion UsUally has minimal overlap with the path-space
posterior and typical trajectories do not pass anywhere near the observations, resUlting in high
variance gradients. This grows considerably worse as the dimension of the problem and the time
between observations increases.
The central idea of this paper is to instead generate trajectories from a distribUtion mUch closer to
the path-space posterior distribUtion when estimating these expectations. In the simplest case where
the observations are directly of the process X , this means sampling trajectories which pass directly
throUgh the observations. Alternatively, one can marginalize oUt the observation noise distribUtion —
or efficiently estimate some expected loss fUnction — by importance sampling
EX [p(xi:N | X)] = ExLN~q
-p(xi:N | Xi:N) L 、
[q(Xl:N) PX (XLN)
(4)
where q is an importance distribUtion which may depend on the observations themselves. For a large
class of observation distribUtions, sUch as additive GaUssian noise or mUltinomial observations, a
natUral choice for q woUld be the corresponding conjUgate distribUtion, in which case the importance
weight p/q becomes a constant. In complex graphical models, q might be parameterized and
optimized variationally, or Gibbs sampling may be Used with a collection of simpler conditional
distribUtions. Taking advantage of this, we shall, withoUt loss of generality, concentrate only on
Ex [Qn δ(xn- X(tn))].
estimating the qUantity pX (X1:N)
In this work, we introdUce a novel algorithm for efficient estimation of probabilities in SDE models
for the pUrposes of learning. OUr algorithm has a nUmber of favoUrable charateristics:
•	OUr algorithm exhibits lower gradient variance in estimated expectations than algorithms
based on SDE solvers, since it enforces that sampled trajectories pass throUgh the observed
data, prodUcing accUrate resUlts even in hUndreds of dimensions.
•	Since oUr algorithm does not rely on an SDE solver, it completely removes all seqUential
dependence between sampled points. OUr algorithm is therefore trivially parallelizable, and
can take fUll advantage of modern parallel hardware sUch as mUlti-GPU training.
•	Since oUr estimator can be calcUlated in a single forward pass of the fUnction f, proba-
bilistic approaches sUch as variational GaUssian processes are viable candidates for the
representation of f.
2
1.2 Related work
Li et al. (2020) propose a method for calculating gradients of functions of the integrated process with
respect to parameters of f and g via a backwards SDE called the adjoint method. This approach
requires reconstructing the Wiener process backwards in time when integrating the backward equation
and the authors propose a memory efficient algorithm to do so. Their approach uses a constant
amount of memory independent of the number of integration steps similar to the adjoint method for
NeuralODEs (Chen et al., 2018). Tzen & Raginsky (2019) discuss the computation of gradients by
simulation of a forward SDE in the derivatives with respect to parameters of the model using the
chain rule of stochastic calculus. This approach does not require back-propagating through the SDE
solver. Kidger et al. (2021) propose a method of learning SDEs by treating the SDE as the generator
in a generative adversarial network setup, using a second SDE for the discriminator. All of these
approaches use SDE integration algorithms to estimate path-space expectations.
Batz et al. (2018) consider a nonparametric approach to SDE learning, using Gaussian processes
for the drift and diffusion functions. They initially consider a gradient matching approximation, in
which the drift Gaussian process may be fit directly by conventional methods, and thereafter use an
expectation-maximization algorithm, and estimate the transition probability for sparse observations
using a linear approximation of the process. Yildiz et al. (2018) propose an alternative approach to
Gaussian process-based SDE learning using the Euler-Maruyama integrator, approximating f by the
predictive mean of a Gaussian process conditioned on a MAP estimate for a set of inducing points.
Unfortunately, this approach completely ignores any uncertainty in the posterior over f.
It should be noted that each of the above mentioned studies rely on SDE integrators to generate
trajectories and therefore face the same difficulties associated with them. Furthermore, almost all
of the experiments therein are on system of dimension between 1 and 3, with a small number of
experiments of dimension as high as 6. We are unaware of any work which trains neural SDEs in
higher dimensions, and we believe this is a testament to the inherent difficulty of learning SDEs in
high dimensions.
Much other work focuses on learning specific classes of SDE or ODE models, such as symplectic or
Hamiltonian systems (Zhong et al., 2020; Greydanus et al., 2019), graph-based models (Poli et al.,
2021; Sanchez-Gonzalez et al., 2019), and controlled differential equations (Morrill et al., 2021).
Many of these ideas are independent of the learning algorithm and can potentially make use of our
algorithm when applied to SDEs.
2	Path-space importance sampling
Itis well known that solutions of linear SDEs are Gaussian processes (see Pavliotis, 2014, Section 3.7).
For these processes, the joint probability density over some finite collection of points is available
in closed form. Furthermore, the posterior distribution of the process conditioned on some set of
observations can be sampled from exactly. Unfortunately, this is not the case in general for non-linear
SDEs, and one has to resort to approximation methods in order to calculate probabilities. Most
commonly, one uses an SDE integrator to estimate expectations; however, other methods, such as
Laplace approximations and perturbation theory are sometimes used (Karimi & McAuley, 2016;
Dass et al., 2017; Hutzenthaler & Jentzen, 2020).
Alternatively, if we are able to find a second process which follows a similar SDE to the one of
interest, it is possible to express expectations and probabilities as importance sampling estimators.
Linear SDEs are an attractive candidate for this due to their Gaussianity and ease of sampling.
2.1	State-independent diffusion
For now, consider the simpler scenario in which the diffusion coefficient does not depend on the state
variable; i.e. g(x, t) = σ(t) for some function σ. The process defined by
dY = σ(t) dW,	(5)
is Gaussian with mean zero and conditional variance
EYh(Y(tι) - Y(t0))[ = / σ(t)σT(t)dt.	(6)
3
Sample paths of Y can be generated efficiently by simulating Brownian motion. The process Y
conditioned on a set of observations of the process is a Brownian bridge. If we are able to express
quantities of interest as expectations under this process instead of our original SDE, then we can
efficiently estimate such quantities via Monte Carlo sampling. As it turns out, expectations and
probabilities for an SDE with a general drift term can in fact be expressed as such. This relation is
given in the following theorem.
Theorem 1 Let X and Y be the stochastic processes generated by the following SDEs
dX = f(X,t)dt+σ(t)dW,	(7)
dY = σ(t) dW.	(8)
Further assume that σ(t)σT (t) is Lebesgue-almost-everywhere invertible and f is sufficiently well-
behaved such that X(t) has a unique probability density for all t.
The probability density ofobservations {(tn, Xn)}N=y under the process X is given by the COndi-
tional expectation
pX (x1:N ) = pY (x1:N )EY heS[Y]	Y (tn) = xn}nN=1i ,	(9)
where
S[Y ] = / f T (匕,t)(σ(t)σT (t))-1 dYt - 2 / f T (匕,t)(σ(t)σT (t))-1f(匕,t)dt.	(10)
This result follows from Girsanov’s theorem (Girsanov, 1960; Malliavin & Thalmaier, 2006, Sec-
tion 1.5) and the definition of conditional expectation. Intuitively speaking, the first term in Equa-
tion 10 encourages f to line up with typical paths which pass through the observations, while the
second term regularizes the magnitude of f.
Theorem 1 allows us to develop an importance sampling algorithm for estimating probabilities under
the process X by simulating the process Y, where the importance weights are given by ωi = eS[Y (i)] .
In this approach, one can generate entire sample paths of Y before calculating f(Y, t), allowing the
forward pass of f to be performed in parallel. This approach is described in Algorithm 1.
Algorithm 1 Path Integral Importance Sampling
1	: for i = 1, . . . , K do
2	:	Sample (Yt)ttN=t1, s.t. for each n, Ytn = xn	. Y is a Brownian bridge
3	ft — f(Yt,t) for each t
4	ɑ J Pt fTσ-2(t)(匕+△-Yt)
5	β — Pt fT σ-2(t)ft δ
6	Si J α — 2 β
7	: end for
8	return PY(xi：N) K PieXp(Si)
An important property of this algorithm is that the sampled paths directly pass through observations.
This means that the probability estimates directly relate the drift function f to plausible trajectories
instead of allowing the particles to wander around randomly. See Figure 1 for an illustration. In the
initial stages of training, simply integrating the SDE with drift f will typically not lead to trajectories
which pass near the data, especially in high dimensions. This can have a significant impact on the
variance of the gradients and can inhibit the model’s ability to learn. Training on paths that pass
through the data can greatly mitigate this high variance. As with other algorithms that generate SDE
sample paths, the discretization of the integrals introduces a small bias. However, this bias vanishes
in the ∆t → 0 limit.
2.1.1	A note on parallelizability
With the exception of the sampling step in Line 2 of Algorithm 1, each of the other operations,
including the sums, can be performed in parallel over feature and batch dimensions, as well as over
4
(a) SDE integration
Figure 1: Independently sampled paths. a shows paths sampled from the SDE using an integrator,
while b shows samples from a Brownian bridge which pass exactly through the observations. In both
cases the drift function is given by a neural network which is randomly initialized with the same seed.
The drift vector field is shown in grey.
(b) Brownian bridge importance samples
the t and i indices. Sampling a Brownian bridge, or any conditional GaUss-MarkoV process, can
be performed by first sampling the unconditional process, and then linearly interpolating with the
obserVations. The linear interpolation can be performed independently, and therefore in parallel, oVer
the time dimension. The only operation that is not completely parallelizable is the sampling of the
Unconditional base process — in this case standard Brownian motion. Brownian paths can be sampled
by cUmUlatiVe sUmmation of independent normal random nUmbers. HoweVer, this cUmUlatiVe sUm is
extremely fast, and can be performed independently between each consecUtiVe pair of obserVations.
In oUr tests, sampling whole Brownian trajectories was aboUt 5 orders of magnitUde faster than the
forward pass of the neUral network f, and so does not create any noticeable performance impact.
Once the Brownian paths are sampled, the rest of Algorithm 1 may be implemented in parallel
withoUt difficUlty. Contrast this to standard integrator-base methods; eVen if the Brownian paths are
sampled Upfront, the integrator mUst still perform each of the forward passes of f in seqUence.
See Appendix B for more details on sampling GaUssian bridges.
2.2 State-dependent diffusion
The assUmption that g is independent of the state is not in fact reqUired for the Validity of Theorem 1;
the reason this assUmption is reqUired is to ensUre that the process Y is GaUssian, so as to enable
easy conditional sampling and calcUlation ofpY (x1:N). UnfortUnately, one cannot simply find an
analogUe of Theorem 1 to calcUlate expectations of a general process as expectations with respect to
a constant diffUsion process.
To address the qUestion of state-dependent diffUsion, it is enlightening to consider a simple example
of sUch a process. Perhaps the most common example is geometric Brownian motion
dX = μX dt + σX dW.	(11)
The typical way in which one woUld solVe this is to introdUce a transformation Z = log(X), and
apply It6's lemma (see Malliavin & Thalmaier, 2006, Section 1.5; Pavliotis, 2014, Section 3.5) to
obtain the SDE
dZ = (μ — 1 σ2) dt + σ dW.	(12)
Similarly, one might address a problem of a general state-dependent diffUsion coefficient by consider-
ing a transformation to a space in which the transformed equation has constant diffusion. Let T(∙, t)
be a map which is invertible for all t. That is, there exists a fUnction T-1 sUch that if y = T(x, t),
then x = T -1(y, t). Transforming a process with constant diffusion σ, by the map T-1 gives a
process with diffusion coefficient
∂Ti-1
gi,j (x,t) = ∂yk σk,j
t)-1σk,j,
i,k
(13)
5
where repeated indices imply summation. With such a transformation we can represent any diffusion
matrix that can be written as a Jacobian. While the restrictions on the diffusion matrix may be seen as
an inherent limitation of a path-space importance sampling approach, it may be argued that constant
or simple diffusion functions would cover the majority of cases of interest, and almost all real-world
problems require some form of data pre-processing and, potentially, link functions.
This transformation allows us to infer the SDE in a transformed space, with constant diffusion, and,
if desired, transform the equation back to the original space using It6's lemma. This approach is
described in Algorithm 2, where we have used the symbol f to represent the drift in the transformed
space as to avoid ambiguity with the drift in the original space. One important point to note is that one
should be careful to allow gradients to propagate through the sampling step on Line 2 in Algorithm 2,
so as to allow learning of the function T .
Algorithm 2 Transformed-State Path Integral Importance Sampling
1:	for i = 1, . . . , K do
2:	Sample (Yt)ttN=t1, s.t. for each n, Ytn = T(xn, tn)	. Y is a Brownian bridge
-τ -τ,___.
3:	ft J f(Yt) for each t
4:	a J PtfT σ-2 (Yt+∆t- Yt)
5:	β J PtfTσ-2ft ∆t
6:	Si J α — 2 β
7:	end for
8:	return Qn det (包嘉卢)PY (T(xi：N，ti：N))表 PieXp(Si)
One would typically apply this method to infer f rather than the original f for performance reasons,
and then only reconstruct the SDE in the original space if required.
2.	2.1 Reconstruction of the SDE in the untransformed space
Generation of new sample paths can be performed easily in the transformed space and then simply
transforming back by applying T-1. However, sometimes one needs more information for the
purposes of analysing the learned equation. In this case the SDE can be transformed back using Ito's
lemma. The drift and diffusion coefficients are
∂T-1	∂T-1	1	∂2T-1
fi(x,t) = —^7,---+	---fk (y,t) + Xσj,k	σl,k
∂t ∂yk	2	∂yj ∂yl
∂Ti-1
gi,j (x，t) = ∂yk σk,j ∙
(14)
(15)
All expression are evaluated aty = T(x, t). When T is parameterized by a neural network with
ReLU non-linearities, the second derivative term in Equation (14) is almost-everywhere zero. The
remaining terms are Jacobian vector products, and so can be performed efficiently even in high
dimensions.1
3	Experiments
In this section, we provide some experiments to illustrate and verify the performance and capabilities
of our proposed algorithm. Further details on the experiments are given in Appendix C.
3.1	The Lorenz system
To compare the effect of our algorithm on learning, we follow Li et al. (2020) and train a neural
network based SDE model on the Lorenz system. In this model the drift function is given by a
1However, this may require forward-mode instead of backward-mode differentiation.
6
multi-layer perceptron, and the diffusion term is a simple scalar. We generated 16 independent
paths, consisting of 200 observations each, uniformly sampled over a time interval of [0, 10]. We
trained the model on this data by two different methods. In the first method, we minimize the
mean-squared error, estimated by integrating out the SDE, and calculating gradients using the adjoint
method (Li et al., 2020).2 In the second method, we directly optimize the log-probability calculated
using Algorithm 1. In both cases we used precisely the same optimization parameters, the same
time-step size ∆t, and the same number of sampled trajectories to calculate averages. Both models
used the same seed for the neural network initialization, and the same number of gradient steps were
performed in both cases. These models were trained on an Nvidia RTX 3070, with 8Gb of memory.
To assess the performance of these approaches, we record two metrics. The first is the mean-square
difference between the learned drift vector field, and the Lorenz vector field, normalized by the mean-
square magnitude of the Lorenz vector field, measured at the observations. Written mathematically:
err = Pnf (Xrj- fLOr(Xn))2
Pn (/Lor(Xn))2
(16)
This metric is shown in Figure 2b. The second metric we record is the mean-squared error of sampled
trajectories compared to the observations; this is shown in Figure 2a. However, we note that this is
not a fair performance metric since the adjoint-based method is directly optimizing for it. On the
x-axis we show the training time in minutes.
(a)
(b)
Figure 2: Learning curves of a neural SDE on the Lorenz system. b shows the normalized mean-
squared deviation of the learned drift from the ground truth. a shows the mean-squared error of
sample paths from the observations.
In Figure 2 we have limited the range of the x-axis in order for the lines to be more clearly visible.
The total training time for the adjoint-based MSE optimization was a factor 52 longer than the time
taken optimizing the probability estimates. We note that, while our approach was significantly faster,
it did use approximately four times as much memory as the adjoint-based method.3
The poor performance of the integrator-based learning algorithm in Figure 2b — despite its low mean-
squared-error — suggests that directly optimizing for the discrepancy of typical SDE sample-paths
compared to observations is not a robust approach to SDE learning.
3.2	Gradient variance
We applied both our algorithm and an SDE integrator to a neural network-based SDE model on
a real-world dataset and recorded the gradients, sampled over many independent trajectories. We
thereafter computed the variance of the gradients individually per parameter, for various time lengths
between observations. We used a discretization step size of 0.01. These results are shown in Figure 3.
2We also attempted training a model without the adjoint method and instead directly backpropagating through
the SDE integrator. However, a single forward pass required more memory than available on the GPU.
3The adjoint algorithm was specifically designed to have constant memory complexity in the number of
integration steps. Our algorithm does not exhibit this feature.
7
Figure 3: Gradient variances as a function of the time between observations. a shows gradient
variances of the observation mean-squared-error computed with an SDE integrator. b shows the
gradient variance of the log-probability of observations computed with Algorithm 1.
In each case we used the same neural network, initialized with a fixed seed, and the exact same values
for all parameters. We used the same loss functions as in Section 3.1. The data set we used was the
Hungarian chickenpox cases dataset from the UCI machine learning repository;4 it has 20 features
per observation. We scaled the features by their standard deviations before performing the experiment
to improve numerical stability.
Our findings are as follows:
•	For very short times, we found that both algorithms produced comparable gradient variances:
typically less than 10-7.
•	For longer time spans (3 orders of magnitude larger than the discretization step size), the
gradient variance of our algorithm increased by a few orders of magnitude; the maximum
we observed was on the order of 10-4. However, the SDE integrator approach resulted in
huge increases in variances, with some as large as 1018.
•	Between the above two extremes, we found that the gradient variance of our approach did
increase with the time between observations, but stayed below 10-5, while the gradient
variance from the SDE integrator exhibited very poor scaling with variances of up to 108 for
times between observations of 1.0.
Considering that both approaches produced comparable variances on very small timescales, we feel
that this experiment represents a fair comparison.
3.3	Uncertainty quantification with variational Gaussian processes
We attempt to test the viability of using our method for an SDE model with a GP prior over the drift
function and fitting the posterior variationally. To assess this, we used data trajectories from a van der
Pol oscillator. For the GP prior, we used a zero mean function and a radial basis function kernel, with
independent GP priors over each output dimension of the drift vector field. These choices are not
necessarily optimal; however, the purpose of this experiment is merely to illustrate the feasibility of
using variational GP to learn the vector field of an SDE model.
We used a grid-based inducing point approach and the kernel interpolation method of Wilson &
Nickisch (2015). The combination of these techniques allows one to take advantage of the Kronecker
product structure of the covariance matrices and the Toeplitz structure of each factor for accelerated
sampling. The model was trained variationally, sampling the variational posterior jointly along entire
trajectories.
4This dataset can be found at https://archive.ics.uci.edu/ml/datasets/Hungarian+
Chickenpox+Cases
8
(c)
(b)
(a)
Figure 4: GP posterior compared to ground truth vector field. a shows the GP vector field coloured
by uncertainty. The arrows show the integral curves of the posterior mean and the coloured are scaled
linearly with the log predictive variance. b shows the integral curves of the ground truth van der
Pol oscillator. c shows the deviation of GP posterior from the ground truth vector field in standard
deviations. The red lines show the 10%, 25%, 75%, and 90% quantiles.
The approximate GP posterior is shown in Figure 4a compared to the ground truth vector field of the
van der Pol oscillator in Figure 4b. Comparing the GP to the van der Pol vector field, we found that
the GP model was fairly accurate in areas with many data points, while reverting to the prior far away
from observations. To assess the quality of the uncertainty quantification, we further calculated the
discrepancy between the variational posterior and the ground truth vector field. Over a dense grid,
covering an enlarged area around the observations, we calculated the discrepancy as
A	fvdp(X) - μ(X)
-σ(X)-
A histogram is shown in Figure 4c.
(17)
Under Gaussianity assumptions, one expects to see the quantiles line up well with some associated
number of standard deviations. However, this will only be the case near to the observations. In
regions where the GP reverts to the prior, which has a stationary kernel, the mismatch between the
GP and the polynomial growth of the van der Pol oscillator results in heavy tails. We expect that a
better choice of kernel, which accounts for this polynomial behaviour, to show residuals with lighter
tails. We found that the distribution of these δs was fatter-tailed than a Gaussian distribution; the
middle 50% (25% - 75% quantiles) slightly passed one standard deviation on either side, as opposed
to the 68% of a standard Gaussian distribution. The middle 80% (10% - 90% quantiles) were within
3 standard deviations. While the posterior uncertainty was somewhat overconfident, it was not wholly
unreasonable.
We further note that, while the complexities involved in variational GP fitting can result in longer
training times, we still found that our method applied to a GP was significantly faster than using an
SDE integrator with a neural network.
4	Discussion
In this work we introduced an algorithm to efficiently estimate probabilities of observations under
the assumption of an SDE model for the purposes of learning. The proposed approach produces
estimates which are accurate, even in high dimensions, and can fully utilize parallel hardware for
significant speedups.
The proposed algorithm is capable of providing robust probability estimates which can be used in
models both with or without an explicit observation noise component. This is in contrast to SDE
integrator-based learning approaches, which must incorporate and estimate an explicit error term due
to their inability to generate paths which are guaranteed to pass through the observations.
Our algorithm produces more stable estimates than alternative integrator-based approaches, particu-
larly when the observations are sparsely distributed over long time intervals and in high dimensions.
This in turn results in more stable and robust learning. With the rise in applicability of large-scale
(neural network) SDE modeling in science, simulation, and domains such as finance, we see our
approach as an important contribution to reliable scaling of these methods.
9
Acknowledgments
The authors would like to thank InstaDeep Ltd. for their financial support of this work.
Reproducibility
Efforts have been made to describe all parameters and details of the algorithms used in this paper,
either in the content or in the appendix. The code included in the supplementary material includes
a Dockerfile and instructions for running some of the experiments. Where relevant, fixed seeds for
random number generators can be found in the code during initialization.
References
Philipp Batz, Andreas Ruttor, and Manfred Opper. Approximate Bayes learning of stochastic
differential equations. Phys. Rev. E, 98:022109, Aug 2018. doi: 10.1103/PhysRevE.98.022109.
URL https://link.aps.org/doi/10.1103/PhysRevE.98.022109.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. 31, 2018. URL https://proceedings.neurips.cc/paper/
2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.
Sarat C. Dass, Jaeyong Lee, Kyoungjae Lee, and Jonghun Park. Laplace based approximate posterior
inference for differential equation models, differential equations. Statistics and Computing, 27:
679-698,2017. doi: 10.1007∕s11222-016-9647-0.
I. V. Girsanov. On transforming a certain class of stochastic processes by absolutely continuous
substitution of measures. Theory of Probability & Its Applications, 5(3):285-301, 1960. doi:
10.1137/1105027. URL https://doi.org/10.1137/1105027.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian Neural Networks.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf.
Martin Hutzenthaler and Arnulf Jentzen. On a perturbation theory and on strong convergence rates
for stochastic ordinary and partial differential equations with nonglobally monotone coefficients.
The Annals of Probability, 48(1), Jan 2020. ISSN 0091-1798. doi: 10.1214/19-aop1345. URL
http://dx.doi.org/10.1214/19-AOP1345.
Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
59b1deff341edb0b76ace57820cef237-Paper.pdf.
Hadiseh Karimi and Kimberley B. McAuley. Bayesian Estimation in Stochastic Differential
Equation Models via Laplace Approximation. IFAC-PapersOnLine, 49(7):1109-1114, 2016.
ISSN 2405-8963. doi: https://doi.org/10.1016/j.ifacol.2016.07.351. URL https://www.
sciencedirect.com/science/article/pii/S2405896316305584. 11th IFAC
Symposium on Dynamics and Control of Process SystemsIncluding Biosystems DYCOPS-CAB
2016.
Patrick Kidger, James Foster, Xuechen Li, and Terry J Lyons. Neural SDEs as Infinite-Dimensional
GANs. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 5453-5463.
PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/kidger21b.
html.
Prashant Kumar, Martin Schmelzer, and Richard P. Dwight. Stochastic turbulence modeling in RANS
simulations via multilevel Monte Carlo. Computers & Fluids, 201:104420, 2020. ISSN 0045-7930.
doi: https://doi.org/10.1016/j.compfluid.2019.104420. URL https://www.sciencedirect.
com/science/article/pii/S0045793019303780.
10
Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. International Conference on Artificial Intelligence and
Statistics, 2020.
Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, S. Kumar, and Cho-Jui Hsieh. Neural SDE: Stabilizing
Neural ODE Networks with Stochastic Noise. ArXiv, abs/1906.02355, 2019.
Paul Malliavin and Anton Thalmaier. Stochastic Calculus of Variations in Mathematical Finance.
Springer, 01 2006. ISBN 3-540-43431-3. doi: 10.1007/3-540-30799-0.
James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations
for long time series. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
7829-7838. PMLR, 18-24 JUl 2021. URL https://Proceedings.mlr.ρress∕v139∕
morrill21b.html.
GA Pavliotis. Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and
Langevin Equations. Springer, 2014.
Michael Poli, Stefano Massaroli, JUnyoUng Park, AtsUshi Yamashita, Hajime Asama, and Jinkyoo
Park. Graph neUral ordinary differential eqUations. 2021.
Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian Graph
Networks with ODE Integrators, 2019.
Belinda Tzen and M. Raginsky. NeUral stochastic differential eqUations: Deep latent GaUssian models
in the diffUsion limit. ArXiv, abs/1905.09883, 2019.
Andrew Wilson and Hannes Nickisch. Kernel Interpolation for Scalable StrUctUred GaUssian Pro-
cesses (KISS-GP). In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volUme 37 of Proceedings of Machine Learning Research,
pp. 1775-1784, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.
press/v37/wilson15.html.
C. Yildiz, M. Heinonen, J. Intosalmi, H. Mannerstrom, and H. Lahdesmaki. Learning stochastic
differential equations with gaussian processes without gradient matching. In IEEE International
Workshop on Machine Learning for Signal Processing. MLSP, 2018.
Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning
hamiltonian dynamics with control. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=ryxmb1rKDS.
A Theorem 1
We have that X and Y are processes obeying the SDEs
dX = f(X)dt+g(X)dW,
dY = g(Y)dW,
(18)
(19)
dropping the time argument for notational simplicity. Expectations with respect to SDEs can be
expressed as path integrals. In the It6 convention, the path integral representation of an SDE has the
following form
EX [F [刈=N→∞/F[χ] Y1 √2∏>exp {-IX (Wtn )2}
(20)
:	DxF [x]J exp
L(x(t),X(t)) dt > ,
(21)
11
where J is a Jacobian factor which depends on g but not f and the Lagrangian is given by
L(x, X)
X - f(χ)Y
g(X))
(22)
The derivative of X here may be thought of distributionally; however, the expression for the measure
Dx exp{-1 / L} need only be defined on the Cameron-Martin subspace5; guaranteeing its unique
extension to the whole Wiener space. In contrast to this, the expression inside the expectation
F [x] should be defined almost-everywhere, and hence care should be taken when X appears in an
expectation.
We may expand the quadratic to separate the Lagrangian into a drift-free term plus terms dependent
on the drift
-2fT(x)g-2(x)X
+ f T (X)g-2(X)f (X).
(23)
(24)
(25)
The first term, on line 23, is the Lagrangian for the process Y , which has no drift term. Shuffling the
rest of the terms out of the measure and into the expectation, we have
where
EX F [X]
DXF [X]eS [x] exp
S[X] = / f T (Xt)g- (Xt)X t dt
-2 / fT(Xt)g-2 (Xt)f(Xt) dt
=	fT(Xt)g-2(Xt)dXt
-2 / fT(Xt)g-2 (Xt)f(Xt) dt.
(26)
(27)
(28)
(29)
(30)
Note that on the third line we have used the distributional definition of X; i.e. Xt dt = dXt.
Lastly, using the definition of conditional expectation, we have that
EA hF[A] p(B = b|A)i = EA hp(B = b|A)iEA hF[A] B=bi.	(31)
By trivial substitution, we have
pX (X1:N) = pY (X1:N)EY heS[Y] {Y (tn) = Xn}nN=1i .	(32)
B S ampling Bridges
Let Y be a Gauss-Markov process with kernel k. Since the process is Markov, we need only sample
the process conditioned on two end-points. Let y = (y0, yT) be the known values of the process at
times τ = (t0, T). Let Y be the process defined by
Yt ：= Yt + K(t,τ)K(τ,τ)-1(y - YT), for to ≤ t ≤ T,	(33)
where K is the matrix whose entries are k(ti, tj). This distribution ofY is the same as the distribution
of Y conditioned on the values y at times τ . Note that once Yt has been sampled — which may
5The Hilbert space of functions whose derivatives are square integrable. The Wiener space is the space of
continuous functions. This is not a Hilbert space, instead it is a Banach space with the supremum norm. The
measure of the Cameron-Martin space is in fact zero; the sample paths of the SDE are almost-surely nowhere
differentiable.
12
require a cumulative sum — this transformation can be performed independently, and hence in
parallel, for each t. For each consecutive pair of observations, the bridge process can be sampled
independently.
In the simplest case, where Y is a Brownian motion starting from y0 at time t0 = 0, this reduces to
Yt = Yt + ⅛¾ (yτ - YT)
=Yt + Tτ(yτ - Yτ).
(34)
(35)
And similarly for any other processes which can be sampled with a fixed starting point.
In the more general case We can invert the two by two matrix to get an explicit formula for Yt
Yt = Yt +
k(t, t0)k(T, T) - k(t, T)k(t0, T)
k(t0,t0)k(T,T)- k(to,T)2	'
k(t,T)k(to,to) - k(t,to)k(to,T)
k(to,to)k(T, T)- k(to,T)2
(y0 - Y0)
(yτ - Yτ).
(36)
(37)
+
This may be extended straightforwardly to other cases, such as conditioning on the values of the
integral, or derivative of the process etc.
C Experiments
C.1 Probability estimates
As a sanity check, we compare the log-probability estimates of our algorithm to the actual log-
probabilities of an Ornstein-Uhlenbeck process. The probability density for this process is available
in closed form for any number of observations. We estimate the probability of a set of data points
evenly spaced over a time interval of [0,10], taken from a sine function Xn = sin(π∕10 X tn). For
these estimates we used 1000 time steps to estimate the integral S, and averaged over 100 trajectories,
independently sampled from a Brownian bridge passing through the observations. We repeated this
process 1000 times and report the mean and standard deviation of these estimates. Figure 5 shows
these estimates compared to the analytic probability given by the Ornstein-Uhlenbeck process for
various numbers of observations as a function of the dimension of the problem.
Log probability estimates vs ground truth
20
30
-IOOO
20	40	60	80	100
dimension
Figure 5: Log-probability estimates for an Ornstein-Uhlenbeck process. The solid lines indicate the
mean value of the estimator, and the shaded regions indicate three standard deviations from the mean.
The dashed lines are the analytic log-probability values.
An interesting observation is that the variance of the estimator seems to decrease with the number
of data points. This is atypical of probability estimators, since the magnitude of the log-probability
increases linearly with the number of observations (not to be confused with the number of samples
used to estimate the log-probability). In almost all cases, with the exception of 4 observations in
more than 40 dimensions, our algorithm gives accurate log-probability estimates with small standard
13
deviations. The error of this exception is approximately 30%, but is significantly reduced by adding
more observations, or using more samples. We believe this error is acceptable considering the small
number of observations and the large scale. For 10 observations in 100 dimensions the standard
deviation of our estimator is only 0.4% of the magnitude of the log-probability and the error of the
mean estimate is 2% of the analytic log-probability. This small bias can be further reduced by using
more samples or a smaller ∆t. A single estimate in the 100 dimensional case took less than half a
second to compute on a laptop CPU, and is therefore feasible to use in the inner loop of a learning
algorithm.
C.2 Lorenz
C.2.1 Data set
We generated data by simulating a Lorenz system with the exact same parameters as Li et al.
(2020). We generated 16 independent trajectories, with initial values sampled from a standard normal
distribution, and discretely sampled the values of the process over 200 equally spaced time points
ranging from 0 to 10. Unlike Li et al. (2020), we did not add any observation noise to the data.
We deliberately used a sparser data set than Li et al. (2020), due to personal experience of the difficulty
of fitting SDE models on sparse observations.
C.2.2 Model
The model we used to fit to the observations was
dXt = fθ(Xt)dt+σdW,	(38)
where σ is a scalar and fθ is a fully-connected neural network with ReLU nonlinearities and parame-
ters θ. We used layer sizes of 3 → 32 → 256 → 32 → 3.
C.2.3 Training parameters
We used precisely the same parameters for each model, the only difference in the training procedures
being the loss functions used. In the first case, the loss function per trajectory was
Lmse(θ; X1:N) = } X(X(? - Xn)[	(39)
KN n
k,n
where N = 200 is the number of observations per trajectory, and K is the number of sampled paths
used to estimate the MSE, and X(k) is sampled using an SDE integrator starting at Xt(1k) = x1. We
then averaged this loss function over the 16 independent observed trajectories.
In the second case, the loss function per trajectory was
Lpi(Θ; xi:N) = -NlogPX(xi:N | θ),	(40)
where PX(xi：N | θ) is calculated using Algorithm with K trajectory samples. Again this loss was
averaged over the 16 independent observed trajectories. In both cases we used K = 64 and a time
step size of ∆t = 10-2 .
For each model we used the Adam optimizer with a learning rate of 10-3 and ran the optimization
algorithm for 104 iterations. The model using our importance sampling approach completed the full
number of iterations in 34 minutes, while the integrator-based approach took 29 hours and 46 minutes
to complete the same number of iterations.
Our approach consistently maintained 100% GPU utilization, except during MSE evaluation which
used an integrator, while the integrator based approach was unable to surpass 25% GPU utilization in
this experiment.
C.3 Variational Gaussian processes
Our approach allows more straightforward implementation of SDE models using a Gaussian process
prior for the drift function. Some previous approaches often resort to approximations such as
14
using only the predictive mean or MAP estimates (Yildiz et al., 2018). For dense observations,
directly discretizing the SDE and fitting the GP to the observation deltas may be viable. For sparse
observations this is much more difficult. Our proposed method makes variational GP methods slightly
simpler, since the values of f (Yt) can be sampled jointly along the path. However, it is important to
note that various complexities still arise such as the difficulty of efficiently sampling high-dimensional
Gaussian distributions.
We used the kernel interpolation method of Wilson & Nickisch (2015) which stores a grid of
inducing locations per dimension. Storing inducing locations per dimension for a Kronecker product
kernel effectively gives a variational distribution over md inducing points with only d × m memory
requirements, where m is the number of inducing points per dimension and d is the dimension. When
these inducing locations are evenly spaces, the covariance factors have a Toeplitz structure which
allows sampling in linearithmic time complexity.
To fit the model we used gradient ascent on the ELBO which is calculated as follows:
ELBO=Ef〜q[logp(x∣f)] - KL [q(f )∣p(f)].	(41)
The KL divergence is between two Gaussian distributions calculated at the inducing locations in
the usual way. For the likelihood term, we generated trajectories (as in Algorithm 1), followed by
jointly sampling the variational GP posterior along these trajectories and proceed with the rest of the
computation in Algorithm 1 using these samples. Our optimization objective is calculated
L = -KK Xlogp(xi:N | f(k)) + KL[q(f)∣p(f)],	(42)
K
where each f(k) is an independent sample over the whole path of Y . In other words, f (k)(Yt1 ) and
f (k)(Yt2) are correlated, but f(k)(Yt1) and f (j)(Yt2) are not when k 6= j.
In our experiment we used 32 evenly spaced inducing points per dimension, and jointly sampled the
variational posterior over 64 trajectories of 1024 time steps. The model was trained with the Adam
optimizer with an initial learning rate of 0.1, and an exponential decay with factor 0.999 for 105
training steps. However, the square deviation of the predictive mean from the ground truth converged
well before 104 iterations. Training for 104 iterations took 34 minutes, 105 iterations took about 5
and a half hours.
15