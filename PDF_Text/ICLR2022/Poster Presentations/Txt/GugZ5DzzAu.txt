Published as a conference paper at ICLR 2022
Permutation Compressors for Provably Faster
Distributed Nonconvex Optimization
Rafal Szlendak*
KAUST
Saudi Arabia
Alexander Tyurin
KAUST
Saudi Arabia
Peter Richtarik
KAUST
Saudi Arabia
Ab stract
We study the MARINA method of Gorbunov et al. (2021)-the current State-of-the-
art distributed non-convex optimization method in terms of theoretical communi-
cation complexity. Theoretical superiority of this method can be largely attributed
to two sources: the use of a carefully engineered biased stochastic gradient esti-
mator, which leads to a reduction in the number of communication rounds, and the
reliance on independent stochastic communication compression operators, which
leads to a reduction in the number of transmitted bits within each communication
round. In this paper we i) extend the theory of MARINA to support a much wider
class of potentially correlated compressors, extending the reach of the method be-
yond the classical independent compressors setting, ii) show that a new quantity,
for which we coin the name Hessian variance, allows us to significantly refine the
original analysis of MARINA without any additional assumptions, and iii) identify
a special class of correlated compressors based on the idea of random permuta-
tions ,for which We coin the term PermK. The use of it leads to O(√n) (resp.
O(1 + d/√n)) improvement in the theoretical communication complexity of MA-
RINA in the low Hessian variance regime when d ≥ n (resp. d ≤ n), where n is
the number of workers and d is the number of parameters describing the model
we are learning. We corroborate our theoretical results with carefully engineered
synthetic experiments with minimizing the average of nonconvex quadratics, and
on autoencoder training with the MNIST dataset.
1	Introduction
The practice of modern supervised learning relies on highly sophisticated, high dimensional and
data hungry deep neural network models (Vaswani et al., 2017; Brown et al., 2020) which need
to be trained on specialized hardware providing fast distributed and parallel processing. Training
of such models is typically performed using elaborate systems relying on specialized distributed
stochastic gradient methods (Gorbunov et al., 2021). In distributed learning, communication among
the compute nodes is typically a key bottleneck of the training system, and for this reason it is
necessary to employ strategies alleviating the communication burden.
1.1	The problem and assumptions
Motivated by the need to design provably communication efficient distributed stochastic gradient
methods in the nonconvex regime, in this paper we consider the optimization problem
n
mRdf (X) = 1 iPfi(χ
(1)
where n is the number of workers/machines/nodes/devices working in parallel, andfi : Rd → R
is a (potentially nonconvex) function representing the loss of the model parameterized by weights
x ∈ Rd on training data stored on machine i.
*The work of Rafat Szlendak was performed during a Summer research internship in the Optimization and
Machine Learning Lab at KAUST led by Peter Richtdrik. Rafat Szlendak is an undergraduate student at the
University of Warwick, United Kingdom.
1
Published as a conference paper at ICLR 2022
While we do not assume the functions {fi } to be convex, we rely on their differentiability, and on
the well-posedness of problem (1):
Assumption 1. The functions f1, . . . , fn : Rd → R are differentiable. Moreover, f is lower
bounded, i.e., there exists finf ∈ R such that f(x) ≥ finf for all x ∈ Rd.
We are interested in finding an approximately stationary point of the nonconvex problem (1). That
is, We wish to identify a (random) vector X ∈ Rd such that
E [kVf(X)k2i ≤ ε	(2)
while ensuring that the volume of communication between the n workers and the server is as small
as possible. Without the lower boundedness assumption there might not be a point with a small
gradient (e.g., think off being linear), which would render problem (2) unsolvable. However, lower
boundedness ensures that the problem is well posed. Besides Assumption 1, we rely on the following
smoothness assumption:
Assumption 2. There exists a constant L+ > 0 such that 1 PZi ∣∣Vfi(x) -Vfi(y)k2 ≤
L2+ kx - y k2 for all x, y ∈ Rd . To avoid ambiguity, let L+ be the smallest such number.
While this is a somewhat stronger assumption than mere L--Lipschitz continuity of the gradient of
f (the latter follows from the former by Jensen’s inequality and we have L- ≤ L+), it is weaker
than Li -Lipschitz continuity of the gradient of the functions fi (the former follows from the latter
with Lj ≤ 1 Pi L2). So, this is still a reasonably weak assumption.
1.2	A brief overview of the state of the art
To the best of our knowledge, the state-of-the-art distributed method for finding a point X satisfying
(2) for the nonconvex problem (1) in terms of the theoretical communication complexity1 is the MA-
RINA method of Gorbunov et al. (2021) (see Algorithm 1). MARINA relies on worker-to-server com-
munication compression, and its power resides in the construction of a carefully designed sequence
of biased gradient estimators which help the method obtain its superior communication complexity.
Algorithm 1 MARINA
1:	Input: starting point X0, stepsize γ, probability p ∈ (0, 1], number of iterations T
2:	Initialize g0 = Vf(X0)
3:	for k = 0, 1, . . . , T - 1 do
4:	Sample θt 〜Be(p)
5:	Broadcast gt to all workers
6:	for i = 1, . . . , n in parallel do
7:	Xt+i = Xt - γgt
8:	Set gt+1 = Vfi(Xt+1) if θt = 1, and gi+1 = gt + Ci(Vfi(xt+1) - Vfi(Xt)) otherwise
9:	end for
10:	gt+1 = 1 Pn=ι gi+1
11:	end for
12:	Output: XT chosen uniformly at random from {xt}T-l1
The method uses randomized compression operators Ci : Rd → Rd to compress messages (gradient
differences) at the workers i ∈ {1, 2, . . . , n} before they are communicated to the server. It is
assumed that these operators are unbiased, i.e., E [Ci(a)] = a for all a ∈ Rd, and that their variance
is bounded as
E ∣Ci(a) - a∣2 ≤ω∣a∣2
for all a ∈ Rd and some ω ≥ 0. For convenience, let U(ω) be the class of such compressors. A key
assumption in the analysis of MARINA is the independence of the compressors {Ci}in=1.
1For the purposes of this paper, by communication complexity we mean the product of the number of com-
munication rounds sufficient to find X satisfying (2), and a suitably defined measure of the volume of commu-
nication performed in each round. As standard in the literature, we assume that the workers-to-server commu-
nication is the key bottleneck, and hence we do not count server-to-worker communication. For more details
about this highly adopted and studied setup, see Appendix F.
2
Published as a conference paper at ICLR 2022
In particular, MARINA solves the problem (1)-(2) in
T =等(L-+L+ q 1-pn)
communication rounds2, where ∆0 := f(x0) - finf, x0 ∈ Rd is the initial iterate, p ∈ (0, 1]
is a parameter defining the probability with which full gradients of the local functions {fi } are
communicated to the server, L- > 0 is the Lipschitz constant of the gradient of f, and L+ ≥ L- is
a certain smoothness constant associated with the functions {fi }.
In each iteration of MARINA, all workers send (at most) pd + (1 - p)ζ floats to the server in ex-
pectation, where ζ := maxi supv∈Rd size(Ci (v)), where size(Ci (v)) is the size of the message v
compressed by compressor Ci . For an uncompressed vector v we have size(v) = d in the worst
case, and if Ci is the RandK sparsifier, then size(Ci (v)) = K. Putting the above together, the
communication complexity of MARINA is T(pd+ (1 - p)ζ), i.e., the product of the number of com-
munication rounds and the communication cost of each round. See Section B for more details on
the method and its theoretical properties.
An alternative to the application of unbiased compressors is the practice of applying contractive com-
pressors, such as TopK (Alistarh et al., 2018), together with an error feedback mechanism (Seide
et al., 2014; Stich et al., 2018; Beznosikov et al., 2020). However, this approach is not competitive
in theoretical communication complexity with MARINA; see Appendix G for details.
1.3 Summary of contributions
(a)	Correlated and permutation compressors. We generalize the analysis of MARINA beyond
independence by supporting arbitrary unbiased compressors, including compressors that are corre-
lated. In particular, we construct new compressors based on the idea of a random permutation (we
called them PermK) which provably reduce the variance caused by compression beyond what inde-
pendent compressors can achieve. The properties of our compressors are captured by two quantities,
A ≥ B ≥ 0, through a new inequality (which we call “AB inequality") bounding the variance of the
aggregated (as opposed to individual) compressed message.
(b)	Refined analysis through the new notion of Hessian variance. We refine the analysis of
MARINA by identifying a new quantity, for which we coin the name Hessian variance, which plays
an important role in our sharper analysis. To the best of our knowledge, Hessian variance is a new
quantity proposed in this work and not used in optimization before. This quantity is well defined
under the same assumptions as those used in the analysis of MARINA by Gorbunov et al. (2021).
(c)	Improved communication complexity results. We prove iteration complexity and commu-
nication complexity results for MARINA, for smooth nonconvex (Theorem 4) and smooth Polyak-
Eojasiewicz3 (Theorem 5) functions. Our results hold for all unbiased compression operators, in-
cluding the standard independent but also all correlated compressors. Most importantly, we show
that in the low Hessian variance regime, and by using our PermK compressors, we can improve upon
the current state-of-the-art communication complexity of MARINA due to Gorbunov et al. (2021) by
UP to the factor √n in the d ≥ n case, and up to the factor 1 + d/√n in the d ≤ n case. The im-
provement factors degrade gracefully as Hessian variance grows, and in the worst case we recover
the same complexity as those established by Gorbunov et al. (2021).
(d)	Experiments agree with our theory. Our theoretical results lead to predictions which are cor-
roborated through computational experiments. In particular, we perform proof-of-concept testing
with carefully engineered synthetic experiments with minimizing the average of nonconvex quadrat-
ics, and also test on autoencoder training with the MNIST dataset.
2 Beyond Independence: The Power of Correlated Compressors
As mentioned in the introduction, MARINA was designed and analyzed to be used with compressors
Ci ∈ U(ω) that are sampled independently by the workers. For example, if the RandK sparsification
2Gorbunov et al. (2021) present their result with L- replaced by the larger quantity L+ . However, after
inspecting their proof, it is clear that they proved the improved rate we attribute to them here, and merely used
the bound L- ≤ L+ at the end for convenience of presentation only.
3The PE analysis is included in Appendix D.
3
Published as a conference paper at ICLR 2022
operator is used by all workers, then each worker chooses the K random coordinates to be commu-
nicated independently from the other workers. This independence assumption is crucial for MARINA
to achieve its superior theoretical properties. Indeed, without independence, the rate would depend
on ω instead4 of ω∕n, which would mean no improvement as the number n of workers grows, which
is problematic because ω is typically very large. For this reason, independence is assumed in the
analysis of virtually all distributed methods that use unbiased communication compression, includ-
ing methods designed for convex or strongly convex problems (Khirirat et al., 2018; Mishchenko
et al., 2019; Li et al., 2020; Philippenko & Dieuleveut, 2020).
In our work we first generalize the analysis of MARINA beyond independence, which provably ex-
tends its use to a much wider array of (still unbiased) compressors, some of which have interesting
theoretical properties and are useful in practice.
2.1	AB inequality: a tool for a more precise control of compression variance
We assume that all compressors {Ci}in=1 are unbiased, and that there exist constants A, B ≥ 0 for
which the compressors satisfy a certain inequality, which we call “AB inequality”, bounding the
variance of 1 Pi Ci (a∕ as a stochastic estimator of 1 Pi ai.
Assumption 3 (Unbiasedness). The random operators C1, . . . , Cn : Rd → Rd are unbiased, i.e.,
E [Ci(a)] = a for all i ∈ {1, 2, . . . , n} and all a ∈ Rd.
Assumption 4 (AB inequality). There exist constants A, B ≥ 0 such that the random operators
C1, . . . , Cn : Rd → Rd satisfy the inequality
n n 2 n n 2
E	1 P Ci(ai) - n P a』≤ A ɪ	P |回2	- B 1 P 电	⑶
i=1	i=1	i=1	i=1
for all a1, . . . , an ∈ Rd. If these conditions are satisfied, we will write {Ci}in=1 ∈ U(A, B).
It	is easy	to	observe that whenever the AB	inequality	holds, it must necessarily	be	the	case	that
A	≥ B .	Indeed,	if we fix nonzero a ∈ Rd and choose	ai = a for all i, then the right hand	side of
the AB inequality is equal to A - B while the left hand side is nonnegative.
Our next observation is that whenever Ci ∈ U(ωi) for all i ∈ {1, 2, . . . , n}, the AB inequality holds
without any assumption on the independence of the compressors. Furthermore, if independence is
assumed, the A constant is substantially improved.
Lemma 1. IfCi ∈ U(ωi) fori ∈ {1, 2, . . . , n}, then {Ci}in=1 ∈ U(maxi ωi, 0). If we further assume
that the compressors are independent, then {Ci}n=1 ∈ U( 1 maxi ωi, 0).
In Table 1 we provide a list of several compressors that belong to the class U(A, B), and give values
of the associated constants A and B. While in the two examples captured by Lemma 1 we had
B = 0, intuitively, we should want B to be as large as possible.
2.2 Input variance compressors
Due to the above considerations, compressors for which A = B are special, and their construction
and theoretical properties are a key contribution of our work. Moreover, as we shall see in Section 4,
such compressors have favorable communication complexity properties. This leads to the following
definition:
Definition 1 (Input variance compressors). We say that a collection {Ci}in=1 of unbiased operators
form an input variance ComPressor System ifthe variance of 1 Ei Ci (a∕ is Controlled by a multiple
of the variance of the input vectors {ai}in=1. That is, if there exists a constant C ≥ 0 such that
n n 2
E 1 PCi(ai) - n P aill	≤ CVar(a1,...,an)	(4)
for all a1, . . . , an ∈ Rd. If these conditions are satisfied, we will write {Ci}in=1 ∈ IV(C).
If {Ci}in=1 ∈ U(A, B) and A = B, then {Ci}in=1 ∈ IV(A).
4This is a consequence of the more general analysis from our paper; Gorbunov et al. (2021) do not consider
the case of unbiased compressors without the independence assumption.
4
Published as a conference paper at ICLR 2022
Table 1: Examples of compressors {Ci}in=1 ∈ U(A, B). See the appendix for many more.
Compressors	A	B	Calculation of A, B	Reference
Ci ∈ U(ωi )	maxi ωi	0	Lemma 1	standard
Ci ∈ U(ωi), independent	-maxi ωi	0	Lemma 1	standard
PermK (d ≥ n);Def2	n 1	1	Theorem 1	new
PermK (d ≤ n); Def 3	n-d 1 — n—ι	n -d 1 — n—ι	Theorem 2	new
2.3 PERMK: PERMUTATION BASED SPARSIFIERS
We now define two input variance compressors based on a random permutation construction.5 The
first compressor handles the d ≥ n case, and the second handles the d ≤ n case. For simplicity of
exposition, we assume that d is divisible by n in the first case, and that n is divisible by d in the
second case.6 Since both these new compressors are sparsification operators, in an analogy with the
established notation RandK and TopK for sparsification, we will write PermK for our permutation-
based sparsifiers. To keep the notation simple, we chose to include simple variants which do not
offer freedom in choosing K . Having said that, these simple compressors lead to state-of-the-art
communication complexity results for MARINA, and hence not much is lost by focusing on these
examples. Let ei be the ith standard unit basis vector in Rd. That is, for any x = (x1, . . . , xd) ∈ Rd
we have x = i xiei .
Definition 2 (PermK for d ≥ n). Assume that d ≥ n and d = qn, where q ≥ 1 is an integer.
Let π = (π1, . . . , πd) be a random permutation of {1, . . . , d}. Then for all x ∈ Rd and each
i ∈ {1, 2, . . . , n} we define
qi
Ci (x):= n ∙	∑	x∏j e∏j.	(5)
j=q(i-1)+1
Note that Ci is a sparsifier: we have (Ci (x))l = nxl if l ∈ {πj : q(i - 1) + 1 ≤ j ≤ qi} and
(Ci(x))l = 0 otherwise. So, kCi(x)k0 ≤ q := K, which means that Ci offers compression by the
factor n. Note that we do not have flexibility to choose K; we have K = q = d/n. See Appendix J
for implementation details.
Theorem 1.	The PermK compressors from Definition 2 are unbiased and belong to IV(1).
In contrast with the collection of independent RandK sparsifiers, which satisfy the AB inequal-
ity with A = d/KT and B = 0 (this follows from Lemma 1 since ωi = d/K — 1 for all i),
PermK satisfies the AB inequality with A = B = 1. While both are sparsifiers, the permutation
construction behind PermK introduces a favorable correlation among the compressors: we have
hCi(ai),Cj(aj)i =0foralli 6=j.
Definition 3 (PermK for n ≥ d). Assume that n ≥ d, n > 1 and n = qd, where q ≥ 1 is an integer.
Define the multiset S := {1, . . . , 1, 2, . . . , 2, . . . , d, . . . , d}, where each number occurs precisely
q times. Let π = (π1, . . . , πn) be a random permutation of S. Then for all x ∈ Rd and each
i ∈ {1, 2, . . . , n} we define
(6)
Ci (x) := dxπi eπi .
Note that for each i, Ci from Definition 3 is the Rand1 sparsifier, offering compression factor d.
However, the sparsifiers {Ci}in=1 are not mutually independent. Note that, again, we do not have a
choice of K in Definition 3: we have K = 1.
Theorem 2.	The PermK compressors from Definition 3 are unbiased and belong to IV(A) with
A = 1 - n-d.
n-1 .
Combining PermK with quantization. It is easy to show that if {Ci}in=1 ∈ U(A, B), and Qi ∈
U(ωi) are chosen independently of {Ci}in=1 (we do not require mutual independence of {Qi}), then
{Ci ◦ Qi}in=1 ∈ U((maxi ωi + 1)A, B) (see Lemma 9). This allows us to combine our compression
techniques with quantization (Alistarh et al., 2017; Horvgth et al., 2019).
5More examples of input variance compressors are given in the appendix.
6The general situation is handled in Appendix I.
5
Published as a conference paper at ICLR 2022
Table 2: Value of L2± in cases when fi(x) = φ(x) + φi(x), where φ : Rd → R is an arbitrary dif-
ferentiable function and φi : Rd → R is twice continuously differentiable. The matrices Ai ∈ Rd×d
are assumed (without loss of generality) to be symmetric. The matrix-valued function L± (x, y) is
defined in Theorem 3.
φ(x)	Φi(x)	Hessian variance L±
any	0	0
any	b>x + Ci	0
0	2 x> AiX + b> x + Ci	λmax (1 P乙 A2 -( 1 P乙 Aj)
0	smooth	ciitλ	(x一y)>L±(χ,y)(x-y) Sup	||一向|2
		x,y∈Rd,x6=y	y
3 Hessian Variance
Working under the same assumptions on the problem (1)-(2) as Gorbunov et al. (2021) (i.e., As-
sumptions 1 and 2), in this paper we study the complexity of MARINA under the influence of a new
quantity, which we call Hessian variance.
Definition 4 (Hessian variance). Let L± ≥ 0 be the smallest quantity such that
n
1	P kVfi(X)-Vfi(y)k2-kVf(x)-Vf(y)k2 ≤ L± ∣∣x -y∣∣2, ∀x,y ∈ Rd.	(7)
i=1
We refer to the quantity L2± by the name Hessian variance.
Recall that in this paper we have so far mentioned four “smoothness” constants: Li (Lipschitz
constant of Vfi), L- (Lipschitz constant of Vf), L+ (see Assumption 2) and L± (Definition 4). To
avoid ambiguity, let all be defined as the smallest constants for which the defining inequalities hold.
In case the defining inequality does not hold, the value is set to +∞. This convention allows us to
formulate the following result summarizing the relationships between these quantities.
nn
Lemma 2. L- ≤ L+, L- ≤ 1 P Li, L+ ≤ 1 P Lj, and Lj 一 L- ≤ L± ≤ Lj.
It follows that if Li is finite for all i, then L-, L+ and L± are all finite as well. Similarly, if L+ is
finite (i.e., if Assumption 2 holds), then L- and L± are finite, and L± ≤ Lj . We are not aware
of any prior use of this quantity in the analysis of any optimization methods. Importantly, there
are situations when L- is large, and yet the Hessian variance L2± is small, or even zero. This is
important as the improvements we obtain in our analysis of MARINA are most pronounced in the
regime when the Hessian variance is small. We also wish to stress that the finiteness of L± is not an
additional assumption - it follows directly from Assumption 2.
3.1 Hes sian variance can be zero
We now illustrate on a few examples that there are situations when the values of L- and Li are large
and the Hessian variance is zero. The simplest such example is the identical functions regime.
Example 1	(Identical functions). Assume that fι = f2 = •…=fn. Then L± = 0.
This follows by observing that the left hand side in (7) is zero. Note that while L± = 0, it is possible
for L- and Lj to be arbitrarily large! Note that methods based on the TopK compressor (including
all error feedback methods) suffer in this regime. Indeed, EF21 in this simple scenario is the same
method for any value of n, and hence can’t possibly improve as n grows. This is because when
ai = aj for all i,j, 1 Pi TopK(ai) = TopK(a，i). As the next example shows, Hessian variance is
zero even if we perturb the local functions via arbitrary linear functions.
Example 2	(Identical functions + arbitrary linear perturbation). Assume that fi(x) = φ(x) + bi>x+
ci, for some differentiable function φ : Rd → R and arbitrary bi ∈ Rd and ci ∈ R. Then L± = 0.
This follows by observing that the left hand side in (7) is zero in this case as well. Note that in
this example it is possible for the functions {fi } to have arbitrarily different minimizers. So, this
6
Published as a conference paper at ICLR 2022
example does not correspond to the overparameterized machine learning regime, and is in general
challenging for standard methods.
3.2 Second order characterization
To get an insight into when the Hessian variance may be small but not necessarily zero, we establish
a useful second order characterization.
Theorem 3.	Assume that for each i ∈ {1, 2, . . . , n}, the function fi is twice continuously differ-
entiable. Fix any x,y ∈	Rd and define	Hi(x,	y)	:=	R1	V2fi(X	+ t(y 一	x))	dt,	H(x, y):=
n
-1 E Hi(x,y). Then the matrices Li(x,y) := H2(x,y), L-(x,y) := H2(x,y), L+(x,y):=
i=1
-En=I H2(x, y) and L±(x, y) := L+(x, y) — L-(x, y) are symmetric and positive Semidefinite.
Moreover,
Li2	sup x,y∈Rd ,x6=y	(x-y)> k	Li(χ,y)(x-y) ∣χ-yk2	,	L2- =	sup x,y∈Rd ,x6=y	(x—y)>L-(χ,y)(x-y) kχ-yk2	,
L2+ =	sup x,y∈Rd ,x6=y	(x-y)> I	L+(χ,y)(x-y) ∣χ-yk2	,	L2±	sup x,y∈Rd ,x6=y	(x 一y)>L±(x,y)(x-y) llx-yk2
While L2± is obviously well defined through Definition 4 even when the functions {fi } are not
twice differentiable, the term “Hessian variance” comes from the interpretation of L2± in the case of
quadratic functions.
Example 3 (Quadratic functions). Let fi(x) = 11 x>AiX + b>X + Ci, where Ai ∈ Rd×d are
symmetric. Then L± = λmaχ(- Pn=I A2 — (∙1 Pn=I Ai)2), where λmaχ(∙) denotes the largest
eigenvalue.
Indeed, note that the matrix 1 Pn=I A2 — (ɪ Pn=I Ai )2 Can be interpreted as a matrix-valued
variance of the Hessians A1, . . . , An, and L2± measures the size of this matrix in terms of its largest
eigenvalue.
See Table 2 for a summary of the examples mentioned above. As we shall explain in Section 4, the
data/problem regime when the Hessian variance is small is of key importance to the improvements
we obtain in this paper.
4 Improved Iteration and Communication Complexity
The key contribution of our paper is a more general and more refined analysis of MARINA. In par-
ticular, we i) extend the reach of MARINA to the general class of unbiased and possibly correlated
compressors {Ci}in=1 ∈ U(A, B) while ii) providing a more refined analysis in that we take the
Hessian variance L2± into account. 7
Theorem 4.	Let Assumptions 1, 2, 3 and 4 be satisfied. Let the stepsize in MARINA be chosen as
0 < γ ≤ MM, where M = L- + JI-P ((A — B)Lj + BL±). Then after T iterations, MARINA
finds a random point XT ∈ Rd for which
E [I∣vf(XT)『i ≤ 竽
In particular, by choosing the maximum stepsize allowed by Theorem 4, MARINA converges in T
communication rounds, where T is shown in the first row Table 3. If in this result we replace L2±
by the coarse estimate L2± ≤ L2+, and further specialize to independent compressors satisfying
Ci ∈ U(ω) for all i ∈ {1, 2,..., n}, then since {Ci}n=ι ∈ U(ω∕n, 0) (recall Lemma 1), our general
rate specializes to the result of Gorbunov et al. (2021), which we show in the second row of Table 3.
7At this point, the authors wish to point out an independent work by Yun et al. (2022) which appeared online
shortly after ours. While the problem addressed by Yun et al. (2022) is different, the spirit remains the same 一
improving the performance of a method of interest by designing carefully correlated randomness.
7
Published as a conference paper at ICLR 2022
Table 3: The number of communication rounds for solving (1)-(2) by MARINA and EF21.
Method + Compressors	T = # Communication Rounds		
MARINA ∩ {Ci}i=ι ∈ U(A,B) (this paper, 2021)	O	(∆γ	(L-+qIp ((A - B)L++BL±)))
MARINA ∩ Ci ∈ U(ω) and independent (Gorbunov et al., 2021)	O ( δ0 (L-+千 l+))		
EF21 ∩ Ci are a-contractive 	(RiCh⅛rik et al., 2021)		O( ∆0 (L- + (1+√-α - 1)l+))		
Table 4: Optimized communication complexity of MARINA and EF21 with particular compres-
sors.
	Communication Complexity			
Method + Compressor	d ≥ n (Lemma 13)	d ≤ n (Lemma 14)		
MARINA ∩ PermK	O (浮min {dL-, dL- + √nL±oτ-	"ɪ	∆0 min -	dL-L-+√ L±o)
MARINA ∩ RandK	O (⅜- min{dL-, √n L+})	不	∆0 min -	dL-,L-+√ L+0)
EF21 ∩ TopK	O ( (∆dL-)	O(∆0 dL-)		
(a) Note, that L- ≤ L+ and L± ≤ L+ .
However, and this is a key finding of our work, in the regime when the Hessian variance L2± is
very small, the original result of Gorbunov et al. (2021) can be vastly suboptimal! To show this, in
Table 4 we compare the communication complexity, i.e., the # of communication rounds multiplied
by the maximum # of floats transmitted by a worker to the sever in a single communication round.
We compare the communication complexity of MARINA with the RandK and PermK compressors,
and the state-of-the-art error-feedback method EF21 of Richtðrik et al. (2021) with the ToPK com-
pressor. In all cases we do not consider the communication complexity of the initial step equal to
O(d). In each case we oPtimized over the Parameters of the methods (e.g., p for MARINA and K
in all cases; for details see APPendix L). Our results for MARINA with PermK are better than the
comPeting methods (recall Lemma 2).
4.1	Improvements in the ideal zero-Hessian-variance regime
To better understand the imProvements our analysis Provides, let us consider the ideal regime char-
acterized by zero Hessian variance: L2± = 0. If we now use comPressors {Ci}in=1 ∈ U(A, B) for
which A = B , which is the case for PermK, then the dePendence on the Potentially very large
quantity L2+ is eliminated comPletely.
Big model case (d ≥ n). In this case, and using the PermK comPressor, MARINA has communi-
cation complexity O(L-∆0ε-1d∕n), while using the RandK compressor, the communication com-
plexity of MARINA is no better than O(L-∆0ε-1 d∕√n). Hence, We get an improvement by at least
the factor √n. Moreover, note that this is an n× improvement over gradient descent (GD) (Khaled
& RiCht^rik, 2020) and EF21, both of which have communication complexity O(L-∆0ε-1d). In
Appendix M, we discuss how we can get the same theoretical improvement even if L2± > 0.
Big data case (d ≤ n). In this case, and using the PermK compressor, MARINA achieves commu-
nication complexity O(L-∆0ε-1), while using the RandK compressor, the communication com-
plexity of MARINA is no better than O(L-∆0ε-1(1 + d∕√n)). Hence, we get an improvement by
at least the factor 1 + d/√n. Moreover, note that this is a d× improvement over gradient descent
(GD) and EF21, both of which have communication complexity O(L-∆0ε-1d).
5 Experiments
We compare MARINA using RandK and PermK, and EF21 with TopK, in two experiments. In the
first experiment, we construct quadratic optimization tasks with different L± to capture the depen-
dencies that our theory predicts. In the second experiment, we consider practical machine learning
8
Published as a conference paper at ICLR 2022
task MNIST (LeCun et al., 2010) to support our assertions. Each plot represents the dependence
betWeen the norm of gradient (or function value) and the total number of transmitted bits by a node.
5.1	Testing theoretical predictions on a synthetic quadratic problem
To test the predictive poWer of our theory in a controlled environment, We first consider a synthetic
(strongly convex) quadratic function f = * P f composed of nonconvex quadratics fi(x):=
1 x>AiX 一 x>bi, where b ∈ Rd, Ai ∈ Rd×d, and Ai = A>. We enforced that f is λ-strongly
convex, i.e., 1 Pn=ι Ai < λI for λ > 0. We fix λ = 1e—6, and dimension d = 1000 (see Fig-
ure 1). We then generated optimization tasks with the number of nodes n ∈ {10, 1000, 10000} and
L± ∈ {0, 0.05, 0.1, 0.21, 0.91}. We take MARINA’s and EF21’s parameters prescribed by the theory
and performed a grid search for the step sizes for each compressor by multiplying the theoretical
ones with powers of two. For simplicity, we provide one plot for each compressor with the best
convergence rate. First, we see that PermK outperforms RandK, and their differences in the plots
reproduce dependencies from Table 4. Moreover, when n ∈ {1000, 10000} and L± ≤ 0.21, EF21
with TopK has worse performance than MARINA withPermK, while in heterogeneous regime, when
L± = 0.91, TopK is superior except when n = 10000. See Appendix A for detailed experiments.
L± = 0:1- =1.0	L± =0.05;2.- = 1.0	L± =0.1; L- =0.99	L± = 0.21: L_ = 1.0	L± = 0.91; L- =0.86
■ I I I I I I I I
--GXVAI-
Or “sapou J。-aqulnN
0.50 0.75 1.00
ahiκ I n
■ I I I
--GXVAI-
OOor “sagUJ。WEnN OOOOr “sapou jo-aqulnN
J=√φ∆=
- I I I
--GXVAI-
0.25 0.50 0.75 1.00 125
#bits t n le6
0X)0 0.25 0.50 0.75 1X)0 1.25 IJO 0.0	05	1.0	1.5
#bits / n ⅛6	#bits / n ⅛6
0.00 025 0.50 0.75 1.00 1.25
#bits / n ieβ
-a- PemK: x32
- Y- RandK: χ2
→-- TbPIG x512
Figure 1: Comparison of algorithms on synthetic quadratic optimization tasks with nonconvex {fi }.
0.00 025 0.50 0.75 1.00 1.25
#bits / n ie6
5.2	Training an autoencoder with MNIST
Now we compare compressors from Section 5.1 on the MNIST dataset (LeCun et al., 2010). Our
1N	2
current goal is to learn the linear autoencoder, f (D, E) := N E ∣∣DEai - aik , Where D ∈
i=1
Rdf ×de, E ∈ Rde×df, ai ∈ Rdf are MNIST images, df = 784 is the number of features, de = 16
is the size of encoding space. Thus the dimension of the problem d = 25088, and compressors send
at most 26 floats in each communication round since We take n = 1000. We use parameter pb to
control the homogeneity of MNIST split among n nodes: if pb = 1, then all nodes store the same
data, and ifpb= 0, then nodes store different splits (see Appendix A.5). In Figure 2, one plot for each
compressor With the best convergence rate is provided for pb ∈ {0, 0.5, 0.75, 0.9, 1.0}. We choose
parameters of algorithms prescribed by the theory except for the step sizes, Where We performed a
grid search as before. In all experiments, PermK outperforms RandK. Moreover, We see that in
the more homogeneous regimes, When pb ∈ {0.9, 1.0}, PermK converges faster than TopK. When
pb = 0.75, both compressors have almost the same performance. In the heterogenous regime, When
pb∈ {0, 0.5}, TopK is faster than PermK, but they both significantly outperform RandK.
Probability (p) ≡ 1.0
Probability (p) - 0.9
Probability (p) ≡ 0.75
Probability (p) ≡ 0.5
Probability (p) ≡ 0.0
2	4	6
#bits / n le6
2	4	6
#bits/n le6
2	4	6
#bits / n le6
2	4	6
#bits / n le6
2	4	6
#bits/n le6
Figure 2: Comparison of algorithms on the encoding learning task for the MNIST dataset.
9
Published as a conference paper at ICLR 2022
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural
Information Processing Systems (NIPS) ,pp.1709-1720, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and Cedric
Renggli. The convergence of sparsified gradient methods. In Advances in Neural Information
Processing Systems (NeurIPS), 2018.
Aleksandr Beznosikov, Samuel Horvdth, Peter Richtðrik, and Mher Safaryan. On biased compres-
sion for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a- Paper.pdf.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex
optimization via stochastic path integrated differential estimator. In NeurIPS Information Pro-
cessing Systems, 2018.
Ronald A Fisher and Frank Yates. Statistical tables for biological, agricultural aad medical research.
1938.
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtdrik. Linearly converging
error compensated SGD. In 34th Conference on Neural Information Processing Systems (NeurIPS
2020), 2020.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richtdrik. MARINA: Faster non-
convex distributed learning with compression. In 38th International Conference on Machine
Learning, 2021.
Samuel Horvdth, Chen-Yu Ho, LUdovit Horvdth, Atal Narayan Sahu, Marco Canini, and Peter
Richtdrik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988,
2019.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes
SignSGD and other gradient compression schemes. In 36th International Conference on Machine
Learning (ICML), 2019.
Ahmed Khaled and Peter Richtdrik. Better theory for SGD in the nonconvex world. arXiv preprint
arXiv:2002.03329, 2020.
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with com-
pressed gradients. arXiv preprint arXiv:1806.06573, 2018.
Donald E. Knuth. The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algo-
rithms. Addison-Wesley Longman Publishing Co., Inc., USA, 1997. ISBN 0201896842.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. In International Conference on Machine
Learning, 2019.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
10
Published as a conference paper at ICLR 2022
Zhize Li, Dmitry Kovalev, XUn Qian, and Peter Richtdrik. Acceleration for compressed gradi-
ent descent in distributed and federated optimization. In International Conference on Machine
Learning, 2020.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtdrik. Page: A simple and optimal prob-
abilistic gradient estimator for nonconvex optimization. In International Conference on Machine
Learning,pp. 6286-6295. PMLR, 2021.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takdc, and Peter Richtdrik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Lam Nguyen, Jie Liu, Katya Scheinberg, and Martin Takdc. SARAH: A novel method for machine
learning problems using stochastic recursive gradient. In The 34th International Conference on
Machine Learning, 2017.
Constantin Philippenko and Aymeric Dieuleveut. Bidirectional compression in heterogeneous set-
tings for distributed or federated learning with partial participation: tight convergence guarantees.
arXiv preprint arXiv:2006.14591, 2020.
Xun Qian, Hanze Dong, Peter Richtdrik, and Tong Zhang. Error compensated loopless SVRG
for distributed optimization. OPT2020: 12th Annual Workshop on Optimization for Machine
Learning (NeurIPS 2020 Workshop), 2020.
Peter Richtdrik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and
practically faster error feedback. arXiv preprint arXiv:2106.05203, 2021.
Mher Safaryan, Rustem Islamov, Xun Qian, and Peter Richtdrik. FedNL: Making Newton-type
methods applicable to federated learning. arXiv preprint arXiv:2106.02969, 2021.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference
of the International Speech Communication Association, 2014.
Sebastian Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for SGD
with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, 2019.
Sebastian U. Stich, J.-B. Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In Advances
in Neural Information Processing Systems (NeurIPS), 2018.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. Doublesqueeze: Parallel stochas-
tic gradient descent with double-pass error-compensated compression. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 6155-6165, Long Beach,
California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/
v97/tang19d.html.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, E ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa- Paper.pdf.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient
compression for distributed optimization. In Neural Information Processing Systems, 2019.
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized SGD
and its applications to large-scale distributed optimization. In Jennifer Dy and Andreas Krause
(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pp. 5325-5333, StockholmsmAQssan, Stockholm
Sweden, 10-15 Jul 2018. PMLR.
Chulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local sgd with shuffling: Tight conver-
gence bounds and beyond. In 10th International Conference on Learning Representations (ICLR),
2022.
11
Published as a conference paper at ICLR 2022
Appendix
Contents
1	Introduction	1
1.1	The problem and assumptions .............................................. 1
1.2	A brief overview of the state of the art ................................. 2
1.3	Summary of contributions ................................................. 3
2	Beyond Independence: The Power of Correlated Compressors	3
2.1	AB inequality: a tool for a more precise control of compression variance . 4
2.2	Input variance compressors ............................................... 4
2.3	PermK: permutation based sparsifiers ..................................... 5
3	Hessian Variance	6
3.1	Hessian variance can be zero ............................................. 6
3.2	Second order characterization ............................................ 7
4	Improved Iteration and Communication Complexity	7
4.1	Improvements in the ideal zero-Hessian-variance regime ................... 8
5	Experiments	8
5.1	Testing theoretical predictions on a synthetic quadratic problem ......... 9
5.2	Training an autoencoder with MNIST ....................................... 9
A Extra Experiments	14
A.1 Experiments setup ........................................................ 14
A.2 Experiment with quadratic optimization tasks: full description ........... 14
A.3 Comparison of MARINA with RandK and MARINA with PermK on quadratic opti-
mization problems ....................................................... 14
A.4 Comparison of EF21 with TopK and MARINA with PermK on quadratic optimiza-
tion problems ........................................................... 15
A.5	Experiment with MNIST: full description ................................ 18
A.6	Comparison of MARINA with RandK and MARINA with PermK on MNIST dataset	18
A.7	Comparison of EF21 with TopK and MARINA with PermK on MNIST	dataset	.	.	19
A.8	Behavior of the Hessian Variance in Experiments ........................ 20
A.8.1 Linear Regression with 2 nodes .................................... 20
A.8.2 Local Hessian Varaince in MNIST experiments ....................... 20
B MARINA Algorithm	21
C Missing Proofs
22
12
Published as a conference paper at ICLR 2022
C.1	Proof of Lemma 1 ....................................................... 22
C.2	Proof of Lemma 2 ....................................................... 22
C.3	Proof of Theorem 1 ..................................................... 23
C.4	Proof of Theorem 2 ..................................................... 24
C.5	Proof of Theorem 3 ..................................................... 24
C.6	Proof of Theorem 4 ..................................................... 26
D	Polyak-匕ojasiewicz Analysis	29
D.1	Proof of Lemma 6 ....................................................... 29
D.2	Proof of Theorem 5 ..................................................... 30
E	EF21 Analysis	31
E.1	EF21 rate in the non-convex regime ..................................... 31
E.2 EF21 in PL regime......................................................... 32
F	Communication Model	34
G	On Contractive Compressors and Error Feedback	34
G.1 On Contractive Compressors ............................................... 34
G.2 On Error Feedback ........................................................ 35
H	Composition of Compressors with AB Assumption and Unbiased Compressors	36
I	General Examples of PermK	37
I.1	Case d ≥ n ............................................................. 37
I.2	Case n ≥ d ............................................................. 38
J	Implementation Details of PermK	39
K	More Examples of Permutation-Based Compressors	39
K.1	Block permutation compressor ........................................... 39
K.2	Permutation of mappings ................................................ 40
L	Analysis of Complexity Bounds	43
L.1	Nonconvex optimization ................................................. 43
L.1.1	Case n ≤ d ...................................................... 43
L.1.2	Case n ≥ d ...................................................... 46
L.2	PL assumption .......................................................... 47
L.2.1	Case n ≤ d ...................................................... 47
L.2.2	Case n ≥ d ...................................................... 48
M Group Hessian Variance	50
13
Published as a conference paper at ICLR 2022
A Extra Experiments
In this section, we provide more detailed experiments and explanations.
A.1 Experiments setup
All methods are implemented in Python 3.6 and run on a machine with 24 Intel(R) Xeon(R) Gold
6146 CPU @ 3.20GHz cores with 32-bit precision. Communication between master and nodes is
emulated in one machine.
In all experiments, we compare MARINA algorithm with RandK compressor and PermK compres-
sor and EF21 with TopK. In RandK and TopK, we take K = dd/ne; we show in Lemma 13
that K = dd/ne is optimal for RandK. For TopK, the optimal rate predicted by the current state-
of-the-art theory is obtained when K = d (however, in practice, TopK works much better when
K	d). Lastly, we make the pessimistic assumption that L2± and L2+ are equal to their upper
bound 1 Pi=1 L2.
A.2 Experiment with quadratic optimization tasks: full description
First, we present Algorithm 2 which is used in the experiments of Section 5.1. The algorithm
is designed to generate sparse quadratic optimization tasks where we can control L± using the
noise scale. Furthermore, it can be seen that the procedure generates strongly convex quadratic
optimization tasks; thus, all assumptions from this paper are fulfilled to use theoretical results.
Algorithm 2 Quadratic optimization task generation
1:	Parameters: number nodes n, dimension d, regularizer λ, and noise scale s.
2:	for i = 1, . . . , n do
3:	Generate random noises Vs = 1 + sξS and Vb = sξb, i.i.d. ξS, ξ 〜N(0,1)
4:	Take vector b = νi-(-1 + νb, 0,…，0) ∈ Rd
5:	Take the initial tridiagonal matrix
(2	-1	0 ∖
Ai = Vs	-1	..	..	∈ Rd×d
4 1	...	...	-1
0	-1	2
6:	end for
7:	Take the mean of matrices A = 1 pη=1 Ai
8:	Find the minimum eigenvalue λmin(A)
9:	for i = 1, . . . , n do
10:	Update matrix Ai = Ai + (λ - λmin(A))I
11:	end for
12:	Take starting point x0 = (√d, 0,…，0)
13:	Output: matrices Ai, ∙ ∙ ∙ , An, vectors b1, •…,bn, starting point x0
Homogeneity of optimizations tasks is controlled by noise scale s; indeed, with noise scale equal to
zero, all matrices are equal, and, by increasing noise scale, functions become less “similar” and L2±
grows. In Section 5.1, we take noise scales s ∈ {0, 0.05, 0.1, 0.2, 0.8}.
In Figure 3, we provide the same experiments as in Section 5.1 but with λ = 0.0001 to capture
dependencies under PE condition. Here, We also see that PermK has better performance when the
number of nodes n ≥ 1000 and L± ≤ 0.21.
A.3 COMPARISON OF MARINA WITH RANDK AND MARINA WITH PERMK ON QUADRATIC
OPTIMIZATION PROBLEMS
In this section, we provide detailed experiments from Section 5.1 and comparisons between RandK
and PermK using different step sizes (see Figure 4 and Figure 5). We omitted plots where algorithms
14
Published as a conference paper at ICLR 2022
NS = 0.05： Lt - 0.05； L- H 1.0 NS = 0.1; Lt - 0.1; L- - 1.0 NS = 0.2; Lt H 0.21； L- H 1.0 NS = 08; Lt - 084; L. - 0.91
NS = 0：t± =Q； L- - 1.0
OI -SePOUEeqUInN
81 -SWXlWqEnN Oool -SePOUEeqEnN 080ri u⅞0c⅞⅛E32
Figure 3: Comparison of algorithms under PE condition on synthetic quadratic optimization tasks.
Each row corresponds to a fixed number of nodes; each column corresponds to a fixed noise scale. In
the legends, we provide compressor names and fine-tuned multiplicity factors of step sizes relative
to theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bits that every
node has sent. Dimension d = 1000.
diverged. We can see that in all experiments, PermK behaves better than RandK and tolerates larger
step sizes. The improvement becomes more significant when n increases.
A.4 COMPARISON OF EF21 WITH TOPK AND MARINA WITH PERMK ON QUADRATIC
OPTIMIZATION PROBLEMS
In this section, we provide detailed experiments from Section 5.1 and comparisons of EF21 with
TopK and MARINA with PermK with different step sizes (see Figure 6 and Figure 7). We omit-
ted plots where algorithms diverged. As we can see, when L± ≤ 0.21 and n ≥ 10000, PermK
converges faster than TopK . While in heterogeneous regimes, when L± is large, TopK has better
performance except when n = 10000. When n > d, we see that PermK converges faster in all
experiments.
15
Published as a conference paper at ICLR 2022
NS = O：t± =0:L- « 1.0
NS = 0.05: Lt = 0.05: L- « 1.0 NS = O.IzLt « 0.1: L- - 1.0
NS = 0.2; L± ≡ 0.21: L- - 1.0 NS = 0.8: L÷ = 0.84; L_ ≡ 0.91
1 1 1
~=f=
81-SWW 5-EnN
1 1 1 1 I
~=f=
081 ⅝1⅞1E 32
*btts∕n l*β
P∙ιmK
P∙ιmK
P∙ιmK
RandK
RandK
RandK
xl6
x32
)⅛4
x2
M
χ9
ιo-*
.1»T
10^,
三二：
P∙ιmκ
P∙ιmK
P∙ιmK
RandK
RandK
RandK
X4
x8
xl«
Xl
x2
xβ
O-W 0.25 αs> 9.TS 1.∞ 1.≡ 1.S0
4)bHs I n i«e
T- ZimK: Xg
-*- ZeK: x32
→- ZeK: Xed
+∙ Randfcxl
t，	RandK-x2
θ⅛	RandK∙x8
0-00 0.25 050 O-TS 1.∞ 125
*bttεIn ι∙
IOO 1.25
i«e
αβo azs as。
*b
2mK: x4
RetmK:x8
■eK: xlβ
RandK-Xl
RandK-x2
RandK: nl
0Λ0 025 0.50 O-TS 1.00 1.25
4>blts∕n U6
ιo-*∙
jβτ.
ιo-"
βι8 US 08
-T- FtrmK: x4
-Λ- FtrmK: xβ
y l>∙rιnK: Xie
-►- RandKX2
-■- RandKx4
-∙- RandK- xβ
1.25 1.SQ
leβ
Figure 4:	Comparison of RandK and PermKon synthetic quadratic optimization tasks. Each row
corresponds to a fixed number of nodes; each column corresponds to a fixed noise scale. In the
legends, we provide compressor names and fine-tuned multiplicity factors of step sizes relative to
theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bits that every
node has sent. Dimension d = 1000.
NS = Oltt = 0:t- - 1.0
NS = 0.05: tt u 0.05; L- - 1.0 NS=O.Iltt - 0.1; L- - 1.0
NS = 0.2: LEH 0.21: L-H 1.0 NS = 0.8： Lt - 0.84： L-H 0.91
。1 -SWUJo-Oompmo-£UInN 8。1 ⅞1E32
二
-⅛ls
M-T- P∙ιmK: Xl
-A- P«nnK:x2
→- P«nnK:x4
* RandK=Xl
RandK： x2
RandK： x4
LZ
US
-V- P∙ιmK -A- P∙ιmK T- PflimK -I RandK -«-■ RandK -*-■ RandK	X2 Wl XS Xl X2 Wl
Rβrmκ: xz
Permit： M
RermK vβ
RandK-Xl
RandK->α
RandKsl
M-V- P∙ιmK: Xl
-A- P«nnK:x2
→- P«nnK:x4
UndK: Xl
-* RandK: x2
--∙ RandK： x4
o-o 9.2 e«.
«.» 1.0 1.2
n US
二
-⅛ls
三三
P∙ιmK
P∙ιmK
P∙ιmK
RandK
RandK
RandK
二
-⅛ls
］卜	→- P*imK:xl«
I⅛.0.	* P«imK:x32
T⅛i,C' 一». →- mek： xm
RandK: x2
r,二:学F≈∖	RandK: X4
RandK: xβ
*bte∕n i«e
-ψ- P∙ιmK
-A- P∙ιmK
→- PflimK
RandK
-B- RandK
-* RandK
X4
x8
xl«
Xl
x2
xβ
«.V 0.2
M-V- l>∙rmK∙x4
-A- ⅛rn>K xβ
→- ZrmKi X16
♦- RandK-x2
RandK-x4
-♦- RandK-xβ
0-n VS 9.59 CR ι∙w us
4>blts∕n IeT
-V- FtrmK: x4
-A- ZnnK x8
→- l>∙rιnK: xie
RandKX2
RandK-x4
'-∙- RandK- xβ
az，
0.50 ar ɪoo 1.25
*blts∕n
PemKX4
RtimKxS
ZnnK xl«
Randbxl
RandK-x2
RandK-x4
d8 ""
d8 ONS UW
13
UG
1■W
l(β
«S 9.59 β∙7S 1.W l∙a
4>blts∕n U6
»5
*bftsZn
1.S 1.9V
UG
-ψ- P∙ιmK: x4
-A- P∙ιmK xβ
→- P∙ιmK: Xie
-►- Randfcxl
-■- RandK-x2
-♦- RandK-x4
⅛pj ^*' ∙.∙.. T- ZEK: x4
S ⅛	"-*- ZeKxS
V*	→- l>∙πnK: Xie
T‰ʌ2~ Randfcxl
∖ ~>~-y -■■•■■■ RandK- x2
、'y	•*- RandK: x4
⅜→- ZrmI6x4
-A- P∙πnK∙x8
→- ZrmKi X16
RandK-x2
RandK-x4
-♦- RandK∙x8

Figure 5:	Comparison of RandK and PermK under PE condition on synthetic quadratic OPtimiza-
tion tasks. Each row corresponds to a fixed number of nodes; each column corresponds to a fixed
noise scale. In the legends, we provide compressor names and fine-tuned multiplicity factors of step
sizes relative to theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of
bits that every node has sent. Dimension d = 1000.
16
Published as a conference paper at ICLR 2022
NS= 0.05； tt - 0.05； t- - 1.0 NS = O.Ijt1 - 0.1;	- 1.0 NS = 0.2jti « 0.21； t- - 1.0 NS =0.8： Lt - 0.84; L_ - 0.91
NS=O：t± =0：t- - 1.0
-T- P∙ιmK: Xl
-⅛- P«nnK:x2
→- P«nnK:x4
+ -R>pK∙xβ
TbPI6x16
MmK: Xl
P*imK:x2
PeimK:x4
%pK:x8
%pK:xl«
TbpK:x32
-V- l>∙πnK∙x4
-⅛- FtimK;XS
-*- ⅛πnK: Xie
+ TaPKi x64
-*- %pK: X128
♦ ∙Rpκ )αse
w∙,
.w-j
ιo-,∙
W
0-∞ 025 050 O-TS 1.00 1.25
*bttε)n ι∙
aoo o∙25 αs> ακ ιoo 1.25
4>blts∕n 1««
*btts∕n l*β
+ l>∙πnK: Xie
-⅛- l>∙πnK∙x32
-4- ZeKxW
+ Taat X128
-*- TapKi )αse
-∙- T>pK: XS12
-V- P∙ιmK: Xie
-A- P∙ιmK∙x32
→- P∙ιmK∙xβd
+ PpK: XM
-*- "R>pK: X128
→- T>pK1χ25β
+ ZimK: x8
-⅛- ZimK: x32
-4- ZimKxtd
- -RpK X128
--*-∙ TapKi 1^5«
TbpM >β12
W"
RtrmIC x4
-A- FtrmK: XS
y l>∙rιnK: Xie
-+- TbPK:x128
→- "R>pK:x25e
f_ "R>pK: x512
α∞ β∙τs ι∙w ι∙J5
4>blts∕n i«e
0-∞ CNS 050 CR 1.00 WS 1.∞
«bns I ∏ i«e
T- ZnnKxA
-*- FtimK;XS
→- l>∙πnK: X16
+ TaPKiX128
* τ>pκ∙>β5e
-∙- ¼pK∙xS12
-T- taimK:x4
-*- taimK:x8
--4- l>∙πnK: Xie
+ TbPKi X128
TbPK <αse
♦ TbpKi χsi2
α∞ ms
ι∙≈
le6
0-0	«.S	1.0	1,5	α∞ MS 9.59 O-TS 18 1.25
4>blts∕n UG	4)blts∕n l*β
αβo ms
-V- FtrmK: x4
-A- FtrmK: xβ
y l>∙rιnK: Xie
→- TbPK:x128
→- "R>pK:x25e
：f - T>pK:x512
aso απ ι.βo
*blts∕ n
us
1.∞
uβ


Figure 6:	Comparison of ToPK and PermKon synthetic quadratic optimization tasks. Each row
corresponds to a fixed number of nodes; each column corresponds to a fixed noise scale. In the
legends, we provide compressor names and fine-tuned multiplicity factors of step sizes relative to
theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bits that every
node has sent. Dimension d = 1000.
NS = 0.2: LEH 0.21: L-H 1.0 NS = 0.8： Lt - 0.84： L-H 0.91
∙*r∙∙∙
111
-⅛ls
OI ^u⅞βc⅞⅞E32
NS = Oltt = 0:t- - 1.0
NS = 0.05: tt u 0.05; L- - 1.0 NS=O.Iltt - 0.1; L- - 1.0
-V- P«nnK:x2
-*- P«nnK:x4
-4- P*imK:xS
+ T>pK:x8
--*-∙ "R>pK: Xie
♦ "t>pK: x32
OOO MS MQ β∙7S 1.W US
4>blts∕n lβ7
-ψ- P∙ιmK: Xie
-Λ- P«imK:x32
,--4- P∙ιmK: x6Λ
,→- PpK: x32
-*- "R>pK:xe4
-∙- T>pK: xl28
M 1∙0
♦Mts/n
Tj-
i«e
-ψ- P*imK:x4
-*- P*imK:x8
→- P∙ιmK: Xie
→- T>pK:x32
-*- %pK: X«4
T>pK: xl28
α∞ β∙⅛ ι∙w ι∙⅛
4>blts∕n i«e
-V- P∙ιmK: x4
-A- P∙ιmK xβ
→- P∙ιmK: Xie
+ -R>pK:xM
TbPK:x128
.→- T>pK:x25e
MS 8。 β∙7S 1.W 1.J5
4>blts∕n Iβ7
-V- P∙ιmK: Xie
-*- PaimK:x32
--4- P∙πnK: X«4
R- +fps
"R>pKιx512
-+ 'R>pK: X1O24
CNS αs> »ts ι.oo ι.≡
♦bits / n
-ψ- P∙ιmK: x4
-A- P∙ιmK xβ
→- P∙ιmK: Xie
+ TbPK:x128
'-*- "R>pK:x25e
∣→- T>pK:x512
M IQ
*blts∕n
⅛w-'
¥ FermK xs
-⅛- ZnnK x32
+ ZrmK x64
一→- Tbat χ2se
L "R>pK: x512
s⅛ f - TaPKi X1O24
«.w ms as。 β∙τs ι∙w ι∙a
♦bits I n
T- ZEK:x4
-*- ZEK:xS
-V- l>∙πnK: Xie
-R>pK∙xl28
0⅛ -R>pκ∙ >αse
-+ "R>pK: >βl2
ιo-*∙
rιo^,
I 1»-,■
ιo-,∙
▼ l⅛rmκ: X4
-*- ZnnK xβ
T- ZrmK X16
→- T>p⅛>⅛4
f- T>pK: X128
→- %pK:x25e
-T- P*imK:x4
-A- P∙ιτnK: xβ
,`.→- P∙ιmK:Xie
"R>pκ∙x25e
"R>pK: x512
'、；-6 'R>pK: X1O24
W,-
«.W MS «.W dR 18 1.J5
4>bltS∕n 1««
Il。T
-T- P«imK:x4
-A- P«imK:x8
→- P∙ιmK: Xie
→- "R>pK=x25e
"R>pK: x512
-6 "R>pK: X1O24
1.5	«.W MS
UG
9.59 β∙⅛ IS ι∙a
♦bits I n i«e
10^,. . _
«.W 02S
aso απ ι.βo
*blts∕ n
1.∞
uβ

an «s
Figure 7:	Comparison of TopK and PermK under PL condition on synthetic quadratic optimization
tasks. Each row corresponds to a fixed number of nodes; each column corresponds to a fixed noise
scale. In the legends, we provide compressor names and fine-tuned multiplicity factors of step sizes
relative to theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bits
that every node has sent. Dimension d = 1000.
17
Published as a conference paper at ICLR 2022
A.5 Experiment with MNIS T: full description
We introduce parameter p. Initially, we randomly split MNIST into n + 1 parts: D0,D1,…，Dn,
where n = 1000 is the number of nodes. Then, for all i ∈ {1, . . . , n}, the ith node takes split D0
with probability pb, or split Di with probability 1 - pb. We define the chosen split as Dci . Using
probability pb, we control the homogeneity of our distribution optimization task. Note that ifpb = 1,
all nodes store the same data D0, and ifpb= 0, nodes store different splits Di.
Let us consider the more general optimization problem than in Section 5.2. We optimize the follow-
ing non-convex loss with regularization:
N2	2
min	f(D, E) = n P IIDEai- aik + 2IIDE - IllF ,
D∈Rdf ×de,E∈Rde×df	i=1
where ai ∈ Rdf are MNIST images, df = 784 is the number of features, de = 16 is the size of
encoding space. regularizer λ ≥ 0.
Each node stores function
fi(D,E) =	χ kDEaj — ajk2 + 2 IlDE - IIlF, ∀i ∈{1,...,n}.
|Di| j∈Dci	2
In Figure 8, one plot for each compressor with the best convergence rate is provided for λ =
{0, 0.00001, 0.001} andpb= {0, 0.5, 0.75, 0.9, 1.0}.
We see that in homogeneous regimes, when pb ∈ {0.9, 1.0}, PermK outperforms other compressors
for any λ. And the larger the regularization parameter λ, the faster PermK convergences compared
to rivals.
Probability (fi)- 1.0
Probability (fi) -0.9
Probability Q5) - 0.75
Probability ⅛)-0.5
S
0.0,
Probability (fi) - 0.0
X X X
β 4 3
~⅛
ToooO6 I'
lτ
S
TOO6 I
Figure 8: Comparison of algorithms on the encoding learning task for the MNIST dataset. Each row
corresponds to a fixed regularization parameter λ; each column corresponds to a fixed probability
pb. In the legends, we provide compressor names and fine-tuned step sizes. Axis x represents the
number of bits that every node has sent.
A.6 COMPARISON OF MARINA WITH RANDK AND MARINA WITH PERMK ON MNIST
DATASET
In this section, we provide detailed experiments from Section 5.2 and comparisons of RandK and
PermK with different step sizes (see Figure 9). We omitted plots where algorithms diverged. We
see that in all experiments, PermK is better than RandK. Practical experiments on MNIST fully re-
produce dependencies from our theory and experiments with synthetic quadratic optimization tasks
from Section 5.1.
18
Published as a conference paper at ICLR 2022
Probability (p) = 0.75	Probability 电=0.5
ξ∙
Too。。:
Probability ¢)= 1.0
M-∙
»«10-,■
3κM-,∙
J «1»-,■
Probability (p) = 0.9
6"ir∙
4MM-∙
3KM∙,
Jκiq-∙
6"ir∙
4K10-∙
3KM∙,
6"ir∙
4MM-∙
3KM∙,
Probability (p) = 0.0
	■	RWnK Step Size = OJ
10-1	Γ⅛-*- EIKzStepsize=I
	L→- l*<nι KzStep size = S
	%-i RWdlt Step s⅛e=0ΛW5
	«ItwdK:Sttp Size = O.US
6"ir∙	K-∙- RwdK:Sttpsize = OjS Z	、一L
	
4K10-∙	
W	
2	3	4	5
le«
∙K10"λ
JκTO-∙
6"ir∙
4K10-∙
3κM-∙
6"ir∙
4MM-∙
3κM-∙
eɪɪir*
4K10-∙
6"ir∙∙
4MM-∙∙
I +	FemitStepsize = Oi
β *	RwniKStepsize= ι
	RwniKStepsize = S
I-	RVldK: step Size = 0.125 IMCdtStepsize = Oas
1	Rvιdκ: step size = 0J 上
1	2	3	， Se
i«e
3”：
sɪtiɪ-,
Sκlβ-∙
∙κM-∙
♦bits I ∏
1«
♦bits I ∏
1«
♦bits I ∏
Figure 9: Comparison of RandK and PermKon the encoding learning task for the MNIST dataset.
Each row corresponds to a fixed regularization parameter λ; each column corresponds to a fixed
probability pb. In the legends, we provide compressor names and fine-tuned step sizes. Axis x
represents the number of bits that every node has sent.
I →- RMmK: St8f> size = 2
I -A- RMmKStspsize = O
fʌ -+- RMmK St»f> SiZe = 8
曲-i MndK:StSpSize = βAβ25
F-≡∙ MndK:Stsp Size = O.US
pi	MndK:Stsosize = 85
	-T- Rwmt Steρs⅛e = OJ	
	i	RMmK Step size= 1 RMm K step SiZe= 2 WdK:Sttp size = Mβ25 RVldK: step Size = 0.125 RVIdK: step Size = OJS —
		
		
1«
SKM-"	I -*- RHmK: Step Size = 05 ɪ -*- RwWlK:SttpSize= 1 r∣∖ →- RMmK: step 8⅛e = 2 IL-*- WdK SttPSiZe=Me25 Ir	WdK step Size = 0.125 liʌ -∙- RVIdK: step size = 0j5 Γ⅛Ξ^
	
∙κM-∙	'0~
♦bits I n
A.7 COMPARISON OF EF21 WITH TOPK AND MARINA WITH PERMK ON MNIST DATASET
In this section, we provide detailed experiments from Section 5.2 and comparisons of RandK and
PermK with different step sizes (see Figure 10). We omitted plots where algorithms diverged. We
see that, when pb ∈ {0.9, 1.0}, PermK tolerates larger step sizes and convergences faster than TopK.
When pb ∈ {0, 0.5}, both compressors approximately tolerate the same step sizes, but TopK has a
better performance when λ ∈ {0, 0.00001}.
Probability ¢)= 1.0
Probability (p) = 0.9
-⅛
Q。：
SKM-*-
T- MnnK： Step size= 1
-A- PamK： Step size = 2
→- Pamκ: Step size = 4
f - τ>ρκ: Stspsize = ds
→- IiipK:Stepsize= ι
-∙- IiipK:Stspsize = S
* rennκ: Step stat = i
* rβwκ: Step si» = 4
Y- rβwκ: Step t<a> = 9
RPK： Step size = 85
* T⅛pt stβf>size = OS
-∙- IiipK:Stepsize= ι
M-∙
4K10-∙.
3κM-,∙
»«10-,■
4K10-∙'
3κM-,∙
→- RVmK: Stspsis= 2
-i-	RannK：Steps8 = 4
→- RannK：Steps8 = 3
→- -RpK:Stsp size = βAβ25
-*- -RpK:StSp Size=O-US
-∙- ⅜f>K: Stβf> size = 0j5
eɪɑo-"
4K10-∙
3κM-∙
JκM-∙
SKM-*-
RMmK:SSpsize = OS
-⅛- RMmK:Stepsize= 1
→- RMmK:StKpsize =2
f -	Step size = 0.25
-*- li>f>K: StKpSize = O-S
li>f>K:St»f> Size=I
T- RMmK:SSpSize=OS
-*- RMmK:Stepsize= 1
Y- RMmK:StKpsize =2
f - li>f>K: StKpSize = O-S
* li>f>K:St»f> Size=I
-∙- l<>f>K: Stspsize = 2
T- RMmK:SSpsize = OS
-⅛- RMmK:Stepsize= 1
→- RMmK:StKpsize =2
+ step Size = 0.25
→- li>f>K: StKpSize = O-S
-∙- τ⅛ρκ: SWPS⅛e = ι
Probability (p) = 0.75
	-*- ReimK:Stspsize = OJ -*- ReimK:Stspsize= 1 Y- RMmK:StKpsize=S -*- ”: S3ps⅛e = 0∙5 →- l⅝f>K: Stspsize=I -∙- l⅝f>K: Stspsize = 2 二
2	4	«	.
1««
T- RMmK:SSpSize=OS
-A- RMmK:Stepsize= 1
Y- RMmK:StKpsize =2
f - li>f>K: StKpSize = O-S
* li>f>K:St»f> Size=I
-∙- l<>f>K: Stspsize = 2
»«10-,■
4MM-∙∙
3κM-,∙
T- RMmK:SSpsize = OS
-⅛- RMmK:Stepsize= 1
→- RMmK:StKpsize =2
+ step Size = 0.25
→- li>f>K: StKpSize = O-S
-∙- τ⅛ρκ: SWPS⅛e = ι
• KM-"-
Probability (p) = 0.5
ɪr" 6"ir∙	I	-*- Rwmt S⅛p size = OJ -*- RMmκSts(>⅛e= 1 RMmK: St8f> size = 2 T⅛pK: St»f> size = 0.5 →- RpK:SSpsize=I * -∙- IilpK:Stspsize = 2
*κιq-∙		
3KM∙,		
2	4	«	.
1««
-*- RMmK: Stβf> Size = OS
-A- RMmKStβf>size= 1
RMmK: St8f> size = 2
IiipK:Stepsize = 0.5
* RPK=SSpSize=I
-∙- IiipK:Stspsize = S
»«10-,■
4MM-∙∙
-¥- RMmK: Stβf> Size = OS
-⅛- RwWlKSWPSize= 1
→- RMmK: St8f> size = 2
+ -R>f>K：st»f>size = 0.25
→- IiipK:Stepsize = 0.5
-∙- RPK:Stepsize=I
• KM-1-
Probability (p) = 0.0
10-' 6"ir∙	I	RwmKSwP s⅛e = 0s T -4- RWnKSts(>size= 1 ⅛	→- RMmK: St8f> size = 2 1	+ ι⅛f>K: SWPS⅛e = 0.5 I	-*- IiipK:Stspsize = I 1	-∙- τ⅛ptswp8⅛e = i ⅜∖
4K10-∙	
3κM-∙	
2	4	« .
l«e
6"ir∙∙
4K10-∙'
-*- RMmK: Stβf> Size = OS
-A- RMmKStβf>size= 1
RMmK: St8f> size = 2
IiipK:Stepsize = 0.5
* RPK=SSpSize=I
-∙- IiipK:Stspsize = S
-»- RMmK: Stβf> Size = OS
-⅛- RwWlKSWPSize= 1
→- RMmK: St8f> size = 2
+ -R>f>K：st»f>size = 0.25
→- IiipK:Stepsize = 0.5
-∙- RPK:Stepsize=I
S«1O-'



9«10->
Figure 10: Comparison of ToPK and PermKon the encoding learning task for the MNIST dataset.
Each row corresponds to a fixed regularization parameter λ; each column corresponds to a fixed
probability pb. In the legends, we provide compressor names and fine-tuned step sizes. Axis x
represents the number of bits that every node has sent.
19
Published as a conference paper at ICLR 2022
Table 5: The “adaptive” ratio (L1)2 / (L±)2 calculated at points Xt from experiments with MARINA
and PermK on MNIST (see Figure 2).
t := Iteration number
10	300^^600^^90δ-
(L+)2 / (L±)2~Homogeneous (p = 0.9)~104.73~9J6~569~3.61
①+产 /(L±，2 HeterogeneoUs(P = 0.0)	18.83	2.31	1.69 1.30
A.8 B ehavior of the Hessian Variance in Experiments
Now, we investigate the behavior of the Hessian Variance L± in practical machine learning tasks
and compare it with L+. Our goal is to show that L± can be much smaller than L+ in some tasks.
A.8. 1 Linear Regression with 2 nodes
Let us consider the linear regression problem distributed between 2 machines:
min f (w):= 2∣∣Xιw - yι∣∣1 2 + 1 ∣∣X2w - y2∣∣2],
w∈Rd
where (X1 , y1 ) and (X2, y2 ) refer to a random split of the "mg" dataset from LIBSVM (1, 385
samples and d = 6 features). Using the results from Example 3, we get L+ = 3609.6653 while
L± = 34.58. Clearly, L± two orders smaller than L+ .
A.8.2 Local Hessian Varaince in MNIST experiments
Now, we estimate L± and L+ in the following way. While MARINA running, we capture points xt
and calculate the “adaptive” Lt± and Lt+ , that satisfy
1n
-£ ∣∣Vfi(χt) - Vfi(Xt+1)∣∣2 -||Vf(Xt)- Vf (χt+1)∣∣2 = (L±)2 ||xt -xt+1∣∣2
i=1
and
n
-X IlVfi(Xt)-Vfi(Xt+1)∣∣2 = (L+)2∣∣xt-xt+1∣∣2.
n i=1
In experiments from Section 5.2 (see also Figure 2), We evaluate (L+)2 / (L±)2 in MARINA with
PermK. Results are presented in Table 5. We see that in the homogeneous case, (L±)2 is about
10 X times smaller than (L1) 2 (on average), and (L1) 2 is much larger at the start of the process. On
the other hand, in the heterogeneous case, the difference is smaller. The above experiments provide
further numerical justification that L2± can be effectively much smaller than L2+, and that this drives
the improvement of our new PermK compressor.
20
Published as a conference paper at ICLR 2022
B MARINA Algorithm
To the best of our knowledge, the state-of-the-art method for solving the nonconvex problem (1) in
terms of the theoretical communication efficiency is MARINA (Gorbunov et al., 2021) (see Algo-
rithm 1). In its simplest variant, MARINA performs iterations of the form
xk+1 = xk - γgk,
1n
gk=1 X gk,
(8)
where gf is a carefully designed biased estimator of the gradient Vfj(χk), and γ > 0 is a learning
rate. The gradient estimators used in MARINA are initialized to the full gradients, i.e., g0 = Vfi(χ0),
for i ∈ {1, . . . , n}, and subsequently updated as
k+1 =	Vfi(xk+1)	if	θk	=1
gi =	gk + Cik(Vfi(xk+1)	-Vfi(xk))	if	θk	=0 ,
where θk is a Bernoulli random variable sampled at iteration k (equal to 1 with probability p ∈ (0, 1],
and equal to 0 with probability 1 - p), and Ci : Rd → Rd is a randomized compression operator
sampled at iteration k on node i independently from other nodes. In particular, Gorbunov et al.
(2021) assume that the compression operators Ci are unbiased, and that their variance is proportional
to squared norm of the input vector:
E [Ci (x)] = x, E hkCi (x) - xk2i ≤ ωi kxk2 ,	∀x ∈ Rd.
In each iteration of MARINA, the gradient estimator is reset to the true gradient with (small) prob-
ability p. Otherwise, each worker i compresses the difference of the last two local gradients, and
communicates the compressed message
mik = Cik(Vfi(xk+1) - Vfi(xk))
to the server. These messages are then aggregated by the server to form the new gradient estimator
via
1n
gk+1= gk + - X mk.
Note that i) this preserves the second relation in (8), ii) the server can compute gk+1 since it has
access to gk, which is the case (via a recursive argument) ifg0 is known by the server at the start of
the iterative process8.
Further, note that the expected communication cost in each iteration of MARINA is equal to
Comm = pd + (- - p)ζ,	ζ = max ζi ,
i
where d is the cost of communicating a (possibly dense) vector in Rd, and ζi ≤ d is the expected
cost of communicating a vector compressed by Ci .
MARINA one of the very few examples in stochastic optimization where the use ofa biased estimator
leads to a better theoretical complexity than the use ofan unbiased estimator, with the other example
being optimal SGD methods for single-node problems SARAH (Nguyen et al., 2017), SPIDER (Fang
et al., 2018), PAGE (Li et al., 2021).
8This is done by each worker sending the full gradient g0 = Vfi(x0) to the server at initialization.
21
Published as a conference paper at ICLR 2022
C Missing Proofs
C.1 Proof of Lemma 1
Lemma 1. IfCi ∈ U(ωi) fori ∈ {1, 2, . . . , n}, then {Ci}in=1 ∈ U(maxi ωi, 0). If we further assume
that the compressors are independent, then {Ci}n=ι ∈ U( 1 maxi ωi, 0).
Proof. Let us first assume unbiasedness only. By Jensen’s inequality,
1n	1n
n X Ci(Oi)- n X ai
i=1	i=1
2	1n
≤ — kC( IlCi(Oi)- ai∣∣ ∙
n i=1
It remains to apply expectation on both sides and then use inequality
E hICi(Oi) - OiI2i ≤ ωi IOiI2 ,∀i ∈ {1, . . . ,n},
to conclude that {Ci}in=1 ∈ U(maxi ωi, 0).
Let us now add the assumption of independence.
2
E U n X °” nX ai I.
=Eu1 X (Ci(ai) - ai)|
n
=n X E [kCi(ai)-aik2]
n
≤ manF X kai∣2,
i=1
+ n X E KCi(Oi) - ai, Cj (aj ) - aji]
i6=j
by independence, thus, A = maxi ωi /n, B = 0.
□
C.2 Proof of Lemma 2
nn
Lemma 2. L- ≤ L+, L- ≤ ɪ P Li, L+ ≤ ɪ P Lj, and Lj 一 L- ≤ L± ≤ Lj.
Proof. Let us define
L-(x,y) := IIVf (x) - Vf (y)k2 ,
1n
L+(χ,y) := nɪj kvfi(X)-Vfi(y)k ,
L± (x, y) := Lj(x, y) - L-(x,y).
The inequalities are now established as follows:
1.	By Jensen’s inequality and the definition of Lj,
L-(x, y) ≤ Lj(x, y) ≤ L2j Ix - yI2 ,
thus, L- is at most Lj .
2.	By the triangle inequality, we have
nn
IVf (x) - Vf (y)k ≤ n X IVfi(X) - Vfi(y)k ≤ - X Li ∣x - y∣,
n i=1	n i=1
thus L- is at most ɪ Pn= ι Li.
22
Published as a conference paper at ICLR 2022
3.	From the definition of Li , we have
nn
L+ (x, y) = - X kVfi(x) - Vfi(y)k2 ≤ - X L2 kx - yk2,
and Lj is at most n P2ι L2.
4.	The right inequality follows from L- (x, y) ≥ 0 and
L±(x,y) ≤ L+(x, y) ≤ L2+ kx - yk2 .
Now, we prove the left inequality. From the definition of L± , we have
L± (x, y) ≤ L2± kx - yk2 ,
and
L±(x, y) = L+ (x, y) - L-(x,y),
hence,
L+(x,y) ≤ L2± kx - yk2 + L-(x, y) ≤ (L2- + L2±) kx - yk2 ,
thus L2+ ≤ L2- + L2±.
□
C.3 Proof of Theorem 1
Theorem 1. The PermK compressors from Definition 2 are unbiased and belong to IV().
Proof.
We fix any x ∈ Rd and prove unbiasedness:
qi
E [Ci(x)] = n X E xπj eπj
j =q(i-1)+1
=n ( X d X Xiei
j =q(i-1)+1	i=1
nq
—-X = X.
d
Next, we find the second moment:
qi	qi	d
E [kCi(χ)k2] = n2 X EIjXnj|2]= n2	X	dX |xi|2=n2d Iixk2 =nIixk2.
j=q(i-1)+1	j=q(i-1)+1 i=1
For all a1 , . . . , an ∈ Rd , the following inequality holds:
E - XCi(ai)∣ ]	=	-2 X E hkCi(ai)k2i + X E[hCi(ai),Cj@)i]
n i=1	n i=1	i6=j
-n
=覆 XE [kCi(ai)k2]
-n
=-X kaik2 .
i=1
Hence, Assumption 4 is fulfilled with A = B = .
□
23
Published as a conference paper at ICLR 2022
C.4 Proof of Theorem 2
Theorem 2.	The PermK compressors from Definition 3 are unbiased and belong to IV(A) with
A = 1 - n-d ∙
Proof.
We fix any x ∈ Rd and prove unbiasedness:
1d
E [Ci(x)] = dE [x∏ie∏i] = dd Σxiei = x.
i=1
Next, we find the second moment:
1d
E [kCi(x)k2] = d X d2∣Xi∣2 = d kxk2 .
i=1
For all i 6= j ∈ {1, 2, . . . , n}, x, y ∈ Rd, we have
E [hCi(x), Cj(y)i] = E [hCi(x),Cj(y)i| πi = πj] Prob (πi = πj)
(q - I)
(n — 1)d
(q - I)
(n — 1)d
(q - I)d
n-1
d
X E [ hCi (x), Cj (y)i| πi = q, πj = q]
q=1
d
d2xqyq
q=1
hx, yi .
For all a1, . . . , an ∈ Rd, the following inequality holds:
E U nX C"[
1n	1
n XE [kCi(ai)k2] + n XE[hCi(ai),Cj@)i]
i=1	i6=j
n X ιιaik2 + n2 X EKCi(ai), Cj (aj )i]
i=1	i6=j
⅛ X kaik2+n(2(n —)1) X hai, aj i
n i=1	n n	i6=j
(d-(P^)1X kaik2+(q-ιd
n	n(n — 1) n	n — 1
1n
n X ai
n-d
1 ----
n-1
)1 X kaik2 + n-d
n	n—1
i=1
1n
-X ai
n i=1
i=1
2
2
Hence, Assumption 4 is fulfilled with A = B = 1 -
□
n-d
n-1 .
C.5 Proof of Theorem 3
Theorem 3.	Assume that for each i ∈ {1, 2, . . . , n}, the function fi is twice continuously differ-
entiable. Fix any x,y ∈ Rd and define Hi(x, y) := R1 V2fi(x + t(y — x)) dt,	H(x, y):=
n
n E Hi(x,y). Then the matrices Li(x,y) := H2(x, y), L-(x,y) := H2(x,y), L+(x,y):=
i=1
24
Published as a conference paper at ICLR 2022
n1 ∑n=ι H2(x, y) and L±(x, y) ：= L+(x, y) 一 L-(x, y) are symmetric and positive Semidefinite.
Moreover,
Li2 =	sup
x,y∈Rd ,x6=y
(x-y^L+(X,y)(X-y)
kx-yk2
sup
x,y∈Rd ,x6=y
(X-y^L-(X,y)(X-y)
kχ-yk2
L2+
sup
X,y∈Rd ,X6=y
sup
X,y∈Rd ,X6=y
(X-y)>L±(X,y)(X-y)
kX-yk2
Proof. The fundamental theorem of calculus says that for any continuously differentiable function
ψ : R → R we have
ψ(1) 一 ψ(0) = Z ψ 0 (t)dt.
0
Choose i ∈ {1, 2, . . . , n}, j ∈ {1, 2, . . . , d}, distinct vectors x, y ∈ Rd, and let
ψij (t) := hPfi(x + t(y 一 x)), eji,
where ej ∈ Rd is the jth standard unit basis vector. Since fi is twice continuously differentiable,
ψij is continuously differentiable, and by the chain rule,
ψi0j (t) = hP2fi(x +t(y 一 x))(y 一 x), eji.
Applying the fundamental theorem of calculus, we get
ψij (1) 一 ψij (0) = Z hP2fi(x +t(y 一 x))(y 一 x), ej idt.
0
(9)
Let ψi : R → Rd be defined by ψi(t) := Pfi(X + t(y 一 x)) = (ψii(t),..., ψid(t)). Combining
equations 9 for j = 1, 2, . . . , d into a vector form using the fact that
Z hP2fi(x + t(y 一 x))(y 一 x), ej idt
0
P2fi(x + t(y 一 x))dt (y 一 x), ej
we arrive at the identity
Pfi(y) 一 Pfi(x)	= ψi(1) 一 ψi(0)
(=9)	P2fi(x +t(y 一 x))dt (y 一 x)
= Hi(x, y)(y 一 x).
(10)
Next, since P2fi(x + t(y 一 x)) is symmetric for all t, so is Hi(x, y), and hence Li(x, y) :=
Hi2(x, y) = Hi>(x, y)Hi(x, y), which also means that Li(x, y) is symmetric and positive semidef-
inite. Combining these observations, we obtain
kPfi(x) 一 Pfi(y)k2 (1=0) (x 一 y)>Li(x, y)(x 一 y).
(11)
Clearly,
r2	∣∣Vfi(x) - Vfi(y)k2 (ii)	(X — y)>Li(χ,y)(x — y)
Li = sup ---------------∏----------- = sup ----------------∏---------------
X,y∈Rd,X6=y	∣X 一 y ∣	X,y∈Rd ,X6=y	∣X 一 y ∣
Using the same reasoning, we have Vf(y) 一 Vf (X) = H(X, y)(y 一 X), and
[2	l∣Vf (x) — Vf (y)k2	(X — y)>L-(x,y)(x — y)
L- = sup -------------------2--------= sup ---------------------2-------
X,y∈Rd,X6=y	∣X — y ∣	X,y∈Rd ,X6=y	∣X — y ∣
where L- (X, y) := H2 (X, y) = H> (X, y)H (X, y) is symmetric and positive semidefinite, since
Hi(X, y) are symmetric and positive semidefinite. Finally,
L2+
n1 pi=1 kVfi(x) - Vfi(y)k2
sup -----------------2-----------
X,y∈Rd ,X6=y	∣X — y ∣2
(x -y)> (1 P=I H2(χ,y))(x -y)
sup ----------------------2------------
X,y∈Rd ,X6=y	∣X — y ∣
(X — y)>L+(x,y)(X — y)
sup	2	22	,
X,y∈Rd ,X6=y	∣X — y ∣
25
Published as a conference paper at ICLR 2022
and
L2±
sup	nPn=IWfi(X)-Wi⑷k2-Rf(X)-W⑻k2
x,y∈Rd ,x6=y	kx - y k
(x - y)> (n Pn=I Hi(x,y) - H2(χ,y))(X-y)
SUP --------------------------2----------------
x,y∈Rd ,x6=y	kX - yk
(x - y)>L±(χ,y)(χ - y)
SUP ----------------2-------.
x,y∈Rd ,x6=y	kX - y k
Note, that L+(X, y) inherits symmetry and positive semidefiniteness from Hi2(X, y). Symmetry of
L±(X, y) is trivial. To prove positive semidefiniteness of L±(X, y), note that
1n
L±(χ,y)	= n EHi2(χ,y)- H (X, y)
i=1
1n
=	-^2(Hi(χ,y) - H (χ,y) + H (χ,y)) - H 2(χ,y)
n
i=1
1n	1	n
=	-X (Hi(χ,y) - H(χ,y)) + -H(χ,y) X (Hi(X,y)- H(χ,y))
n i=1	n	i=1
1n
+ n 工(Hi(X,y)- H (x,y)) H (χ,y)
1n
=	—£ (Hi(X,y)- H(X,y)) ,
n
i=1
which is positive Semidefinite.	口
C.6 Proof of Theorem 4
Theorem 4.	Let Assumptions 1, 2, 3 and 4 be satisfied. Let the stepsize in MARINA be chosen as
0 < γ ≤ MM, where M = L- + J1-p((A^-B)L++BL±J. Then after T iterations, MARINA
finds a random point XT ∈ Rd for which
E [∣∣vf(XT)『]≤ 竽
Proof. In the proof, we follow closely the analysis of Gorbunov et al. (2021) and adapt it to utilize
the power of Hessian variance (Definition 4) and AB assumption (Assumption 4). We bound the
term E gt+1 - vf (Xt+1)2 in a similar fashion to Gorbunov et al. (2021), but make use of the
AB assumption. Other steps are essentially identical, but refine the existing analysis through Hessian
variance.
First, we recall the following lemmas.
Lemma 3 (Li et al. (2021)). Suppose that L- is finite and let Xt+1 = Xt - γgt. Then for any
gt ∈ Rd and γ > 0, we have
f(Xt+1) ≤ f(Xt) - 2 IIvf(Xt)II2 - (2γ - Lr) ∣∣Xt+1 -Xt∣∣2 + Y Ilgt- vf(Xt)∣∣2.	(12)
Lemma 4 (Richtdrik et al. (2021)). Let a,b > 0. If 0 ≤ Y ≤ √+b, then aγ2 + bγ ≤ 1. Moreover,
the bound is tight UP to thefactor of 2 since √+b ≤ min
{√ɑ，b} ≤ √a2+b
Next, we get an upper bound of E ∣∣gt+1 - vf(Xt+1)∣∣2 Xt+1 .
26
Published as a conference paper at ICLR 2022
Lemma 5. Let us consider gt+1 from Line 8 ofAlgorithm 1 and assume, that Assumptions 2, 3 and
4 hold, then
E [∣∣gt+1 -V/(xt+1)∣∣2∣xt+1] ≤ (1 - P)((A - B) L2+ + BL±)∖ ∖ xt+1 - xt ∣ ∣ 2
+(1- p)∣∣gt -Vf (Xt)∣∣2.	(13)
Proof. In the view of definition of gt+1, We get
E [∣∣gt+1 -Vf(Xt+1 )∣∣2∣χt+1]
=(1 -p)E [∣∣gt + 1 XXCi (Vfi(Xt+1)-Vfi(Xt)) -Vf(Xt+1)∣∣ | xt+1
I Il	i=ι	Il ∣ I
(1 -p)E ] ∣∣n ECi(Vfi(xt+1) - Vfi(Xt)) - Vf (xt+1) + Vf (xt)∣∣ ∣ xt+1
+ (1 - p)∣∣gt-Vf(χt )∣∣2.
In the last equality we used unbiasedness of Ci. Next, from AB inequality, we have
E [∣∣gt+1 -Vf(xt+1)∣∣2∣xt+1]
≤ (1 -p)E ]∣n XXCi (Vfi(xt+1) - Vfi(xt)) - Vf(xt+1) + Vf(xt)∣∣ | xt+1
+ (1 -p)∣∣gt -Vf(Xt)∣∣2.
≤ (1 -p) (A (1XX ∣∣Vfi(xt+1) - Vfi(Xt)∣∣2) - B ∣∣Vf(xt+1) - Vf(Xt)∣∣2
+ (1 -p)∣∣gt -Vf(Xt)∣∣2
=(1 - P)((A - B) (1E ∣∣Vfi(Xt+1) - Vfi(Xt)∣∣2)
+ B (1E ∣∣Vfi(Xt+1) -Vfi(Xt)∣∣2 -∣∣Vf(Xt+1) -Vf(Xt)Ij )
+ (1 -p)∣∣gt -Vf(Xt)∣∣2.
Using Assumption 2 and Definition 4, we obtain (13).
□
We are ready to prove Theorem 4. Defining
Φt := f (Xt) - finf + 2p∣∣gt -Vf (Xt)∣∣2,
L2 := (A - B) l+ + BL±,
27
Published as a conference paper at ICLR 2022
and using inequalities (12) and (13), we get
E Φt+1
≤ E f(χt) - finf - 2 IRf(Xt)『-(2Y - L-) ∣∣χt+1 - χt∣∣2 + 2 IIgt- Vf(χt)∣∣2
+ 2pE [(1 -p)b2 I∣xt+1 - XtII2 + (1-p) I∣gt - Vf(Xt)II2]
E [Φt] - 2e [IIVf(Xt)II2]
γ(i - P)L
2p
1	L-
-2 + ɪ
E IIXt+1 - Xt II2
≤ E [Φt] - 2e [IIVf(Xt)II2]
where in the last inequality we use
Y(I- p)L2 - _! + L ≤ 0
2p	2γ	2 -
following from the stepsize choice and Lemma 4.
Summing UP inequalities E [Φt+1] ≤ E [Φt] - 2E [∣∣Vf (Xt)k2] for t = 0,1,...,T - 1 and
rearranging the terms, we get
TXE [IIVf(Xt)II2i	≤ ɪX1 (E [φt] -E [φt+1]) = 2(E[φ0] TE[φT]) ≤ 2∆T0,
t=0	γ t=0	γ	γ
since g0 = Vf (x0) and ΦT ≥ 0. Finally, using the tower property and the definition of XT (See
Section B), We obtain the desired result.	□
28
Published as a conference paper at ICLR 2022
D POLYAK-匕OJASIEWICZ ANALYSIS
In this section, We analyze the algorithm under Polyak-Eojasiewicz (PE) condition. We show that
MARINA algorithm with Assumption 5 enjoys a linear convergence rate. Now, we state the assump-
tion and the convergence rate theorem.
Assumption 5 (PE condition). Function f satisfies PoIyak-Lojasiewicz (PL) condition, i.e.,
kVf(x)k2 ≥ 2μ(f(x)- f?), ∀x ∈ Rd,	(14)
where μ > 0 and f ? := infχ f (x).
Lemma 6. For L- > 0 and μ from Assumption 5 holds that L- ≥ μ.
Theorem 5. Let Assumptions 1, 2, 3, 4 and 5 be satisfied and
then for xT from MARINA algorithm the following inequality holds:
E [f(xT) - f ?] ≤ (1- γμ)T ∆0.
We provide the proof to Theorem 5 in Section D.2.
In Table 6, we provide communication complexity of MARINA with PermK and RandK, and EF21
with TopK, optimized w.r.t. parameters of the methods. As in Section 4, we see that MARINA with
PermK is not worse than MARINA with RandK (recall Lemma 2).
Let us consider zero Hessian variance regime: L± = 0. When d ≥ n, PermK compressor has
communication complexity O (max {dL-∕nμ, d}), while RandK compressor has communication
complexity O (max {dL-∕√nμ, d}). And the communication complexity of PermK is strictly better
when dL-∕√nμ > d. Moreover, if d ≤ n and (1 + d∕√n) L-∕μ > d, then we get the strict improve-
ment of the communication complexity from O (max {(1 + d∕√n) L-∕μ, d}) to O (max {L-∕μ, d})
over MARINA with RandK.
Table 6: Optimized communication complexity of MARINA and EF21 with particular compressors under
PE condition (up to a logarithmic factor).
	Communication complexity	
Method	d ≥ n (Lemma 15)	d ≤ n (Lemma 16)
MARINA T PermK	O (max n μmin ndL-, n L-+√nL± o ,dor-	O (max 11 min ndL-,L- + √nL±} ,d})
MARINA T RandK	O [max n ι min ndL-, √ L+o ,do)	O (max 11 min [dL-,L- + √L+ j ,dj)
-EF21 T TopK	o （牛）	O （贤）
D.1 Proof of Lemma 6
Lemma 6. For L- > 0 and μ from Assumption 5 holds that L- ≥ μ.
Proof. We can define L- using the following inequality:
f (x) ≤ f (y) + hVf (y), X - yi + L2- ||x - y『,∀χ, y, ∈ Rd.
Let us take x = y - 1∕L- Vf (y). Then,
f (y - l∑Vf(y)) ≤ f(y) - 2⅛ kVf(y)k2, ∀y ∈ Rd.
Rearranging the terms and using Definition 14, we have
2L7 kVf(y)k2 ≤ f(y) - f (y - L-Vf(y)) ≤ f(y) - finf ≤ 2μ |Vf(y)∣2 ,
thus, μ ≤ L-.
□
29
Published as a conference paper at ICLR 2022
D.2 Proof of Theorem 5
Theorem 5.	LetAssumptions 1, 2, 3, 4 and 5 be satisfied and
then for XT from MARINA algorithm the following inequality holds:
E f(xτ) - f *] ≤ (1 - M)T ∆0.
Proof. The analysis is almost the same as in Gorbunov et al. (2021), but We include it for complete-
ness. Let us define
Φt := f (xt) - finf + P Ia- Vf(Xt)『，L2 := ((A - B) L+ + BL± .
As in Appendix C.6, we use (12) and (13) to get that
E [Φt+1 ]
≤ Ef(Xt)- finf - 2 IIVf(Xt)∣∣2 - (ɪ - Lr) I∣xt+1 - XtII2 + 2 ∣∣gt - Vf(Xt)II2
+ PE [(1 -P)L2 IIXt+1 - XtII2 + (1 -p)IIgt - Vf(Xt)『]
(< E (1 - γμ)(f (Xt) - finf) - (ɪ - Lr) IIXt+1 - XtII2 + Y IIgt- Vf (Xt)II2
+ PE [(1 -P)L2 IIXt+1 - XtII2 + (1 -p)IIgt - Vf(Xt)『]
=E (1 - γμ)(f(Xt) - finf) + (2 + P(1 -p)) IIgt- Vf (Xt)II2
+ e [(P (1 - P)b2 - ɪ + Lr )IIXt+1-XtII2]
≤ (1-γ〃)E [Φt].
In the last inequality, we used P (1 - p)L2 -套 + L- ≤ 0 and 2 + P(1 一 p) ≤ (1 - γμ) P, that
follow from (15) and Lemma 4. Unrolling E [Φt+1] ≤ (1 — γμ)E [Φt] and using g0 = Vf (x0), we
have
E [f (xT) - finf] ≤ E [Φt] ≤ (1 - γμ)τΦ0 = (1 - γμ)τ (f (x0) - finf).
This concludes the proof.	□
30
Published as a conference paper at ICLR 2022
E EF21 Analysis
We provide convergence proofs of EF21 algorithm from Richtdrik et al. (2021) for non-convex and
PE regimes. They will be almost identical to the one by Richtdrik et al. (2021) (indeed, the only
change is the constant L+ instead of L), but we have decided to include it for the sake of clarity.
E.1 EF21 rate in the non-convex regime
We will be using the following lemmas, the proofs of which are in their corresponding papers.
Lemma 7 (Richtdrik et al. (2021)). Let C to be α-contractive for 0 < α ≤ 1. Define Git :=
∣∣gt 一 Vfi(xt)k2 and Wt := {gt ,...,gtn, xt, xt+1}. Forany s > 0 we have
E [Gt+1 | Wt] ≤ (1-θ(s))Gi + β(s) ∣∣Vfi(xt+1) -Vfi(xt)∣∣2,	(16)
where
θ(s) := 1 — (1 一 α)(1 + s), and β(s) := (1 一 α)(1 + s-1) .	(17)
Lemma 8 (Richtdrik et al. (2021)). Let 0 < α ≤ 1 and for s > 0 let θ(s) and β(s) be as in
equation 17. Then the solution of the optimization problem
mSin{W : 0<s<
is given by s* = √⅛α — 1. Furthermore, θ(s*)
(18)
β(s*)
θ(s*) — √Γ-
1-√-α-a and
=一 1 = "0≡一 1 ≤ 2 一 1.
α αα	α
(19)
1
We are now ready to conduct the proof.
Theorem 6.	Let Assumptions 1 and 2 hold, and let the stepsize be set as
0<γ≤
(20)
Fix T ≥ 1 and let XT be chosen from the iterates x0, x1,..., xτ-1 uniformly at random. Then
E h∣∣vf(χτ )∣∣2i≤ 2fJ + 誓.
(21)
Proof. STEP 1. Recall that Lemma 7 says that
E h∣∣gt+1 — Vfi(xt+1 )∣∣2 | Wti ≤ (1 — θ) ∣∣gt 一 Vfi(xt)∣∣2 + β ∣∣Vfi(xt+1) — Vfi(xt)∣∣2, (22)
where θ = θ(s*) and β = β(s*) are given by Lemma 8. Averaging inequalities equation 22 over
i ∈ {1, 2, . . . , n} gives
n
E[Gt+1∣ W t] = - X EbIgt+1-Vfi(Xt+1)∣2∣ W t]
i=1
1n	1n
≤ (1 一 θ) - X ∣∣gt - Vfi(Xt)∣∣2 + βnn X ∣∣Vfi(Xt+1) — Vfi(xt)∣∣2
n i=1	n i=1
1n
=(1 一 θ)Gt + β-]T ∣∣ Vfi(Xt+1) — Vfi(xt)∣∣2
n i=1
≤ (1 一 θ)Gt +βL+ ∣∣xt+1 一 xt∣∣2 .	(23)
Using Tower property and L-smoothness in equation 23, we proceed to
E [Gt+1] =E [E [Gt+1 | Wt]] ≤ (1 — θ)E [Gt] + /LjE [∣∣xt+1 — xt∣∣2] .	(24)
31
Published as a conference paper at ICLR 2022
STEP 2. Next, using Lemma 3 and Jensen’s inequality applied to the function x 7→ kxk2 , we obtain
the bound
f(χt+1) ≤ f(χt)- 2
≤ f(χt)- 2
Wf(Xt )『
Wf(Xt )『
—χtιι2+2 ；X(gt -Vfi(Xt))
n
i=1
— xt∣∣2 + 2 Gt.	(25)
Subtracting finf from both sides of equation 25 and taking expectation, we get
E [f(xt+1) — f叫	≤ E [f(xt) — f叫- YE h∣∣Vf(xt)∣∣2i
— (2γ — L-)E h∣∣xt+1 - xt∣∣2i + γE [Gt].	(26)
COMBINING STEP 1 AND STEP 2. Let δt := E f(Xt) — finf , st := E [Gt] and rt
E ∣∣χt+1 一 χt∣∣2 . Then by adding equation 26 with a 务 multiple of equation 24 We obtain
δt+1 + ɪst+1
2θ
≤
δt — 2 ∣∣ Vf(Xt)∣∣2 — fɪ - LL) rt + 2st + 2θ (βL+rt + (1 — θ)st)
2	2γ	2	2	2
≤
L
—
2
—
βL2
The last inequality follows from the bound γ2 —^+ + L-γ ≤ 1, which holds from our assumption
on the stepsize and Lemma 4. By summing up inequalities for t = 0, . . . , T — 1, we get
T-1
0 ≤ δT + 2θsT ≤ δ0 + 2θs0 - 2 X E h∣∣ Vf (Xt)∣∣2i.
t=0
Multiplying both sides by *, after rearranging we get
X- Te [∣∣vf(χt)∣∣2i ≤ 寺+θT.
t=0	γ
It remains to notice that the left hand side can be interpreted as E [ 11 Vf (XT) 112 ], where XT is chosen
from X0, X1,..., XT-1 uniformly at random.	口
E.2 EF21 IN P匕 REGIME
Theorem 7. Let Assumptions 1, 2 and 5 hold, and let the stepsize in EF21 be set as
Let Ψt := f (Xt) - finf + θGt. Thenfor any T ≥ 0, we have
E [ΨT] ≤ (1 - γμ)TE [Ψ0].
(27)
(28)
Proof. Again, this follows RiChtðrik et al. (2021) almost verbatim.
We proceed as in the previous proof, but use the PE inequality and subtract finf from both sides of
equation 25 to get
E [f(Xt+1) - finf]	≤ E [f(Xt) - finf] - 2 ∣∣Vf(Xt)∣∣2 - (2γ - L-) ∣∣Xt+1 -Xt∣∣2 + 2Gt
≤	(ι - γμ)E [f(Xt) - finf] - (2γ —2-) H/"1 - Xt『+2Gt.
32
Published as a conference paper at ICLR 2022
Let δt := E f(xt) - finf, st := E [Gt] and rt := E hxt+1 - xt 2i. Then by adding the above
inequality with a W multiple of equation 24, We obtain
δt+1 + Yst+1
θ
≤ (1 - γμ)δt -	- L-) Tr + qst + %((1 - θ)st + βL+rt)
=(i-γμ)δt+Y(1-2) St-G - L--牛)rt.
Note that our assumption on the stepsize implies that 1 - 2 ≤ 1 - γμ and 2γ - L- - BL+ ≥ 0.
2 2βL+ + γL- ≤ 1, which holds because of Lemma 4
The last inequality follows from the bound γ
and our assumption on the stepsize. Thus,
δt+1 + γst+1 ≤
θ
It remains to unroll the recurrence.
(1-γμ) (δt + θst
□
33
Published as a conference paper at ICLR 2022
F Communication Model
As mentioned in the introduction, we consider the regime where the worker-to-server communica-
tion is the bottleneck of the system so that the server-to-workers communication can be neglected.
While this is a standard model used in many prior works, we include a brief explanation of why and
when this regime is useful.
1.	Peer-to-peer communication. First, this regime makes sense when the server is merely an
abstraction, and does not exist physically. Indeed, from the point of view of each worker,
the server may merely represent “all other nodes” combined. In this model, “a worker
sending a message to the server” should be interpreted as this worker sending the message
to all other workers. Clearly, in this model there is no need for the “server” to communicate
the aggregated message back to the workers since aggregation is performed on all workers
independently, and the aggregated message is immediately available to all workers without
the need for any additional communication.
2.	Fast broadcast. Second, the above regime makes sense in situations where the server ex-
ists physically, but is able to broadcast to the workers at a much higher speed compared
to the worker-to-server communication. This happens in several distributed systems, e.g.,
on certain supercomputers (Mishchenko et al., 2019). Virtually all theoretical works on
communication efficient distributed algorithms assume that the server-to-worker commu-
nication is cheap, and in this work we follow in their footsteps.
Having said that, our work can be extended to the more difficult regime where the server-to-worker
communication is also costly (Horvgth et al., 2019; Tang et al., 2019; PhiliPPenko & Dieuleveut,
2020; Gorbunov et al., 2020). However, for simplicity, we do not explore this extension in this
work.
G On Contractive Compressors and Error Feedback
G. 1 On Contractive Compressors
The most successful algorithmic solutions to solving the nonconvex distributed oPtimization Prob-
lem (1) in a communication-efficient manner under the communication model described in AP-
Pendix F involve stochastic gradient descent (SGD) methods with communication compression.
There are two large classes of such methods, dePending on the tyPe of comPression oPerator in-
volved: (i) methods that work with contractive (and Possibly biased stochastic) comPression oPer-
ators, such as ToPK or RankK, and (ii) methods that work with unbiased and indePendent (across
the workers) stochastic comPression oPerators, such as RandK .
A (randomized) comPression oPerator C : Rd → Rd is α-contractive (we write C ∈ C(α)), where
0 < α ≤ 1, if
E hkC(x)-xk2i ≤ (1-α)kxk2, x∈Rd.	(29)
A canonical examPle is the (deterministic) ToPK comPressor, which outPuts the K largest (in ab-
solute value) entries of the inPut vector x, and zeroes out the rest. ToPK is α-contractive with
α = K/d. Another examPle is the RankK comPressor based on the best rank-K aPProximation of x
rePresented as an a×b = d matrix. It can be shown that RankK is α-contractive with α = K/min{a,b}
(Safaryan et al., 2021, Section A.3.2). We refer to the work of Vogels et al. (2019) for a Practical
communication-efficient method PowerSGD based on low-rank aPProximations.
Of sPecial imPortance are α-comPressors arising from unbiased comPressors via aPProPriate scal-
ing. Let Q : Rd → Rd be an unbiased oPerator with variance ProPortional to the square norm of the
inPut vector. That is, assume that E [Q(x)] = x for all x ∈ Rd and that there exists ω ≥ 0 such that
E hkQ(x) - xk2i ≤ ωkxk2,	∀x ∈ Rd.	(30)
We will write Q ∈ U(ω) for brevity. It is well known that the oPerator C = (ω + 1)-1 Q is
α-contractive with α = (ω + 1)-1. An examPle of a contractive comPressor arising this way is
34
Published as a conference paper at ICLR 2022
(ω + 1)-1RandK, which keeps a subset of K entries of the input vector x chosen uniformly at
random, and zeroes out the rest. As TopK, (ω + 1)-1RandK is α-contractive, with α = K/d.
Distributed SGD methods relying on general contractive compressors, i.e., on contractive which
do not arise from unbiased compressors from scaling, need to rely on the error-feedback / error-
compensation mechanism to avoid divergence.
G.2 On Error Feedback
An alternative approach to the one represented by MARINA is to seek more aggressive compression,
even at the cost of abandoning unbiasedness, in the hope that this will lead to better communication
complexity in practice. This is the idea behind the class of contractive compressors, defined in (29),
which have studied at least since the work of Seide et al. (2014). Example of such compressors are
the TopK (Alistarh et al., 2018) and RankK (Vogels et al., 2019; Safaryan et al., 2021) compressors.
While such compressors are indeed often very successful in practice, their theoretical impact on
the methods using them is dramatically less understood than is the case with unbiased compressors.
One of the key reasons for this that a naive use of biased compressors may lead to (exponential)
divergence, even in simple problems (Beznosikov et al., 2020). Because of this, Seide et al. (2014)
proposed the error feedback framework for controlling the error introduced by compression, and thus
taming the method to convergence. While it has been successfully used by practitioners for many
years, error feedback yielded the first convergence results only relatively recently (Stich et al., 2018;
Stich & Karimireddy, 2019; Wu et al., 2018; Koloskova et al., 2019; Tang et al., 2019; Karimireddy
et al., 2019; Qian et al., 2020; Beznosikov et al., 2020; Gorbunov et al., 2020).
The current best theoretical communication complexity results for error feedback belong to the EF21
method of Rich电ik et al. (2021) Who achieved their improvements by redesigning the original error
feedback mechanism using the construction of a Markov compressor. However, even EF21 currently
enjoys substantially Weaker iteration and communication complexity than MARINA. For instance, We
shoW in Appendix L that EF21 With TopK is only proved to have the communication complexity of
the gradient descent Without any compression.
35
Published as a conference paper at ICLR 2022
H Composition of Compressors with AB Assumption and Unbiased
Compressors
Lemma 9. If {Ci}in=1 ∈ U(A, B) and Qi ∈ U(ωi) for i ∈ {1, 2, . . . , n}, then {Ci ◦ Qi}in=1 ∈
U((maxi ωi + 1) A, B).
Proof. By the tower property, for all a1, . . . , an ∈ Rd, we have
E
I n X Ci(Qi(Iai))-
1n
n X Q(Oi)
i=1
1n
1VCi(Qi(ai))-
n
i=1
≤E
1n
An EkQi(ai)k2- B
n i=1
n X Qi(ai) I I Q1(a1), ∙ ∙∙ , Qn(an)
1n
n X Qi(Oi)
i=1
Since Qi ∈ U(ωi) for i ∈ {1, 2, . . . ,n}, we get
E
n	n	I2
n X Ci(Qi(Oi))- n X Qi(Oi)I
i=1
i=1
1n
≤ max ωi + 1)A- X kaik2 - BE
n i=1
n x Qi(Oi)I
Using Jensen’s inequality, we derive inequalities:
2
I1 n
E - EQi(Oi)
I n i=1
1n
-∑E[Qi(Oi)]
n i=1
-n
-X Oi
n i=1
≥
2
and
E I l n X Ci(Qi(Oiy)- n X Qi(Oi)
i=1
i=1
≤ max ωi + 1) A^ X kOik2 - B
i	ni=1
-n
-X Oi
n
i=1
2
The last inequality completes the proof.
□
36
Published as a conference paper at ICLR 2022
I GENERAL EXAMPLES OF PERMK
For the sake of clarity, in the main part of our paper, we assumed that n | d or d | n, and pro-
vided corresponding examples of PermK (see Definition 2 and Definition 3). Now, we provide two
examples of PermK that work with any n and d and generalize the previous examples.
I.1 CASEd≥n
The following example generalizes for the case when n does not divide d. Let us assume that
d = kn + r and 0 ≤ r < n. As in Definition 2, we permute coordinates and split them into the
blocks of sizes {k, . . . , k, r}. The first n block of size k we assign to nodes. Next, we take the last
block of size r and randomly assign each coordinate from this block to one node. As the size of the
last block of size r is less than n, some nodes will send one coordinate less.
Definition 5 (PermK (d ≥ n)). Let us assume that d ≥ n, d = kn + r,
0 ≤ r < n, πd = (πd,...,πd) is a random permutation of {1, ∙∙∙ ,d}, and πn =
(∏n,...,∏n) is a random permutation of {1,…,n}. We define the tuple of vectors S(x) =
(xπ
d
kn+1
eπkdn+1
. . , xπd	eπd	, 0, . . . , 0) of size n. Then,
kn+r kn+r
ki
xπjdeπjd + (S(x))πin
j=k(i-1)+1
Theorem 8. Compressors {Ci}in=1 from Definition 5 belong to IV(1).
Proof. We fix any x ∈ Rd and prove unbiasedness:
ki
X E xπjdeπjd +E (S(x))πin
j=k(i-1)+1
=n (kx +(1-r) 0+ r 1 x)
d	n nd
kn + r
nd
= x,
for all i ∈ {1, . . . , n}.
Next, we derive the second moment:
ki	2
X E xπjdeπjd	+E
j=k(i-1)+1
n2 (d	+ (1- r)阳『+ /J
nn
2 kn + r 2
n
nx2,
E kCi(x)k2	= n2
We fix x, y ∈ Rd. For all i 6= l ∈ {1, . . . , n}, we have
E[hCi(x),Cl(y)i]
ki	kl
= n2	xπjd eπjd + (S(x))πin ,
j=k(i-1)+1	j=k(l-1)+1
yπjd eπjd + (S (y ))πln	= 0,
due to orthogonality of vectors ep, for all p ∈ {1, . . . , d}, and the fact that i 6= l.
37
Published as a conference paper at ICLR 2022
Thus, for all a1 , . . . , an ∈ Rd , the following equality holds:
E [ n X Ci(aj] = n12 X E h∣Ci (a*2] +
Hence, Assumption 4 is fulfilled with A = B = 1.
n⅛ X E [hCi(ai), Cj (aj)i] = n X I∣ai k .
i6=j
i=1
□
I.2 CASE n ≥ d
The following definition generalizes Definition 3 for the case when d does not divide n. Let us
assume that n = qd + r and 0 ≤ r < d. As in Definition 3, we permute the multiset, where each
coordinate occures q times. Then, we randomly assign each element from the multiset of size qd to
one node. Note that r randomly chosen nodes are idle.
Definition 6 (PermK, (n ≥ d)). Let us assume that n ≥ d, n = qd + r, 0 ≤ r < d.
d
Let us fix point x ∈ Rd, that we want to compress. Define the tuple of vectors S(x) =
(x1e1, . . . , x1e1, x2e2, . . . , x2e2, . . . , xded, . . . , xded), where each vector occurs q times. Concat
r zero vectors to S(X): S(X) = S(X)㊉(0,..., 0). Let ∏ = (∏ι,..., ∏n) be a random permutation
of {1, . . . , n}. Define
n
Ci(X) = q(S(X))∏i.
Theorem 9. Compressors {Ci}n=ι from Definition 6 belong to IV (A) with A = 1 一 蓝--a.
Proof. We start with proving the unbiasedness:
d
d
E[Ci(X)] = nn∑^XjejProb (∏i = j) = q	X j e j	X,
for all i ∈ {1, . . . , n}, X ∈ Rd.
Next, we find the second moment:
n2 d	n
E UCi(X)『]=q2 X X2prob (πi =j) = q iiXk2,
for all i ∈ {1, . . . , n}, X ∈ Rd.
For all i 6= j ∈ {1, . . . , n} and X, y ∈ Rd, we have
n2 d
E KCi(X),Cj(y)i] = — EXkykProb (∏i = k,∏j = k)
q k=1
n2 G	q(q 一I)	n(q - 1)/	、
=可 ZXkyk 而F = (n-1q ""
Thus, for all a1 , . . . , an ∈ Rd, the following equality holds:
nX CiT2
n
n XE [kCi(ai)k2]
n i=1
E
+ n12 X EKCi(ai), Cj(aj)i]
i6=j
ɪ X kaik2 + n(q-1) j2 X hai,aji
nq i=1	(n 一 1)q n2 i6=j
1
q
—
(q - 1)、1 X Ii I∣2 n(q — 1)
(n - 1)q Jn " ik + (n — 1)q
2
1n
-X ai
n i=1
38
Published as a conference paper at ICLR 2022
Hence, Assumption 4 is fulfilled with A = B = 1 - n(-^.
□
J	IMPLEMENTATION DETAILS OF PERMK
Now, we discuss the implementation details of PermK from Definition 2. Unlike RandK and TopK
compressors, PermK compressors are statistically dependent. We provide a simple idea of how to
manage dependence between nodes. First of all, note that the samples of random permutation π are
the only source of randomness. By Definition 2, they are shared between nodes and generated in
each communication round. However, instead of sharing the samples, we can generate these samples
in each node regardless of other nodes. Almost all random generation libraries and frameworks are
deterministic (or pseudorandom) and only depend on the initial random seed. Thus, at the beginning
of the optimization procedure, all nodes should set the same initial random seed and then call the
same function that generates samples of a random permutation. The computation complexity of
generating a sample from a random permutation π = (π1, . . . , πd) is O (d) using the Fisher-Yates
shuffle algorithm (Fisher & Yates, 1938; Knuth, 1997). All other examples of compressors can be
implemented in the same fashion.
K More Examples of Permutation-Based Compressors
K. 1 Block permutation compressor
In block permutation compressor, we partition the set {1, . . . , d} into m ≤ n disjoint blocks. For
each block Pi,序Idevices SParsify their vectors to coordinates with indices in Pi only.
Definition 7. Let P to be a partition of the set {1, . . . , d} into m ≤ n non-empty subsets, and n =
mq + r, where 0 ≤ r < m. Define matrices A1, . . . , An as follows: put Ai := 0 if i > mq. Denote
the subsets in P as Pi, ∙∙∙ , Pm. Next, for any Pi ∈ P, we set A(i-i)q+ι, A(i-i)q+2, •…,Aiq to
n Diag(Pi). Here by Diag(S) we mean the diagonal matrix where each ith diagonal entry is equal
to 1 ifi ∈ S and 0 otherwise. Let π = (πi, . . . , πn) be a random permutation of set {1, . . . , n}. We
define Ci(x) := Aπi x. We call the set {Ci}in=i the block permutation compressor.
Lemma 10. Compressors {Ci}n=ι belong to IV(A) with A = 1 — n(--g.
Proof. We start with the proof of unbiasedness:
1n	1 m
E[Ci(x)] =—∑A∏i X=1 ∙ q∑q Diag(Pi) x = Ix = x,
i=i	q i=i
for all i ∈ {1, . . . , n}, x ∈ Rd .
Next, we establish the second moment:
m 2 d 2
2 r	q n	q n	n
E UCi(X)k ]=η∙ 0 + X n Xqxj= n Xqxj= q kxk
l=i	j∈Pl	j=i
for all i ∈ {1, . . . , n}, x ∈ Rd .
The following equality will be useful for the AB assumption:
E [hCi(x),Cj(y)i]
Prob (πi ∈ Pk, πj ∈ Pk)
q(q - I)
n(n - 1)
n(q — I)
q(n — 1)
hx, yi ,
39
Published as a conference paper at ICLR 2022
for all i 6= j ∈ {1, . . . , n}, x, y ∈ Rd. Thus,
Xn Ci(ai)2
-X ∣∣aik2+J(q ] J2 X hai, aj i
qn	(n- 1)qn2
1
qn
(nq-⅛)X kaik2 +
n(q - 1) I 1
n ʌf
(n-1)q
(n - 1) - q +1∖ 1 XX II I∣2 l
(n - 1)q In Σ kaik +
n(q - 1)
(n - 1)q
ai
n - q 1 Il I∣2 n(q - 1)
(n - 1)q ∙ n 々kaik + (n - 1)q
1 - n-1) n X kaik2+
1n
1X ai
n
i=1
i=1
1n
1X ai
n
i=1
2
n(q - 1) 1 1
n乙
(n-1)q
i=1
ai
E
1
n
—
n
n
2
2
2
for all aι,...,an ∈ Rd. Hence, Assumption 4 is fulfilled with A = B = 1 - n(-ɪ1).	口
K. 2 Permutation of mappings
Definition 8. Let Q1, . . . , Qn : Rd → Rd be a collection of deterministic mappings Rd → Rd.
Let π = (π1, . . . , πn) be a random permutation of set {1, . . . , n}, where n > 1. Define Ci := Qπi.
Assume that the following conditions hold:
1.
There exists ω ≥ 0 such that E kCi (x)k2	≤ (ω + 1) kxk2 for all i ∈ {1, . . . , n},
x, y ∈ Rd.
n
2.	There exists θ ∈ R such that	hQi(x), Qi(y)i = θ hx, yi for all x, y ∈ Rd.
i=1
n
3.	n P Qi(X) = X forall X ∈ Rd.
i=1
We call the collection {Ci}in=1 the permutation of mappings.
Lemma 11. Permutation of mappings belongs to U(A, B) with A = ω+1 — n-ι(1 —今)and
B = 1 - n-1 (1-S).
40
Published as a conference paper at ICLR 2022
Proof. Unbiasedness follows directly from Condition 3. Let us fix a1, . . . , an ∈ Rd. We shall now
establish the AB assumption.
E
≤
n12 (ω
n
kaik1 2 + E	Qπi(ai),Qπj(aj))	-	hai,aji
n12 (ω
i=1
n
i=1
kaik2 -
i6=j
n
i=1
ai
i6=j
+Xn kaik2+XE	Qπi(ai),Qπj(aj)
(ω+1) H kaik2 - 1XX
i=1
i=1
ai
i=1	i6=j
2
+ n X E KQni (ai), Qnj (aj))]
i6=j
n
n
n
Let us now compute n12 P E KQni @), Qnj (aj))].
i6=j
n12 X EKQni (ai),Qnj (aj))]	=	* X nɪ) X hQ"QvQ )i
i6=j	i6=j	u6=v
=n(n - 1) X n X hQMai),Qv(Oj )i.
i6=j	u6=v
Now,
n12 X hQu(x),Qv(y)i	=
u6=v
Condition 3
Condition 2
nn	n
-2 XX hQu(χ),Qv(y)i- -2 X hQu(χ),Qv(y)i
n u=1 v=1	n u=1
1n
hx, yi - n2 E hQu(X), Qu(V)
u=1
(1 - n2) hx, yi,	∀x, y ∈ Rd.
Hence,
n X E KQni (ai), Qnj (aj ))]
i6=j
1
n(n - 1)
41
Published as a conference paper at ICLR 2022
Finally,
Elil n X Ci(ai) - n X ai
2
≤
i=1
i=1
(ω+1)1
n
nn
n X kai k - n X
n
n
ai
i=1
ω+1	1
—
n n- 1
i=1
2
+ nn% (1- n2) X hai,aj i
n
(1-f)1 X kaik2-(
i=1
1———
n-1
n
(l-g)) 1 X ai
n2	n
i1
i=1
2
Hence, Assumption 4 is fulfilled with A = ω+1 - η⅛ι(1 - n), B = 1 - n-ι(1 - n).	□
42
Published as a conference paper at ICLR 2022
L Analysis of Complexity Bounds
In this section, we analyze the complexities bounds of optimization methods, and typically these
bounds have a structure of a function that we analyze in the following lemma.
Lemma 12. Let us consider function
f(x, y) = (x + (1 - x)y)
where x ∈ (0, 1], y ∈ [ymin, 1], ymin ∈ (0, 1/2], a ≥ 0, and b ≥ 0, then
f (x, y) ≥ 2 min{a, aymin + b},
∀x, y ∈ (0, 1].
Proof. First, let us assume that x ≥ 1/2. Then,
f (χ,y) ≥ a.
Second, let us assume that y ≥ 1/2. Then,
f(χ,y) ≥ a (χ + 2(1 - X)
a
≥ 2.
Finally, let us assume, that y ≤ 1/2 and x ≤ 1/2. Then,
f(x, y) =
≥
≥
≥
≥
≥
≥
a(1 - x)y + 2b (1 - x) (1 - y)
aymin
b
+ 2.
2
□
L.1 Nonconvex optimization
L.1.1 CASEn≤d
We analyze case, when n ≤ d. For simplicity, we assume that n | d, n > 1, and d > 1. For
PermK from Definition 2, constants A = B = 1 in AB inequality (see Lemma 1). We define the
upper bound for the communication complexity of MARINA with PermK as NPermK (p), where p
is a parameter of MARINA, and MARINA with RandK as NRandK (p, k), where k is a parameter of
RandK. From Theorem 4, we have that oracle complexity of MARINA with PermK is equal to
43
Published as a conference paper at ICLR 2022
During each iteration of MARINA, on average, each node sends the number of bits equal to
O (Pd +(1 - P)n),
thus, the communication complexity predicted by theory is
NPermK (p) :
d
+ (I- Pp n
+
(31)
up to a constant factor.
Analogously, for RandK, the communication complexity predicted by theory is
∆0
NRandK(p,k) := — (pd + (1 - p)k) I L- +
(32)
up to a constant factor. To the best of our knowledge, this is the state-of-the-art theoretical commu-
nication complexity bound for the RandK compressor in the non-convex regime.
Finally, for TopK , by Theorem 6, the theoretical communication complexity is
∆0
NTopK(k)	：= ~^Γkς I L- + L+
d — k + ʌ/d2 - dk
(33)
k
up to a constant factor. We consider the variant of EF21, where gi0 are initialized with gradients
Vfi (x0), for all i ∈ {1,..., n}, thus E [G0] = 0 in Theorem 6.
The following lemma will help us to choose the optimal parameters of NPermK (P), NRandK (P, k),
and NTopK(k). We wish to stress that in the following sections the term lower bound should not be
understood as the lower bound for the communication complexity. We merely provide lower bounds
for the upper bounds predicted by the current state-of-the-art theory by optimizing their parameters.
Lemma 13. For communication complexity NPermK (P) of MARINA with PermK, communication
complexity NRandK (P, k) of MARINA with RandK and communication complexity NTopK (k) of EF21
with TopK defined in (31), (32) and (33) the following inequalities hold:
1. Lower bounds:
2. Lower bounds:
Upper bounds:
∆0
≥	mm < dL-,
(34)
(35)
NRandK (p, k)
, ∀p ∈ (0, 1], ∀k ∈ {1,...,d},
Upper bounds: For all k ∈ {1,..., d∕√n}, P = k/d,
4∆0dL+
NRandK (P, k) ≤ -
εn
(36)
Moreover, for all k ∈ {1, . . . , d},P = 1,
∆0dL-
NRandK (1,k)=-------
ε
(37)
44
Published as a conference paper at ICLR 2022
3.
Plill Nτopκ(k) = NROKld)
k∈{l,…,d}
∆o⅛-
ε
(38)
Proof.
1. We start with the first inequality:
for all p ∈ (0,1]. We can obtain the bound 34 if we take p = l/n:
NPermK (J) =	+	~ (J +
2Δ0 ( dτ
≤ — -L- +
ε ∖n
We obtain the equality 35 by taking p = 1.
2.
for all p ∈ (0,1], k ∈ {1,...)d}. We can obtain the bound 36 if we take k ∈
{1,..., cZ∕ √n} and p = k/d:
The equality 37 is obtained by taking p= 1.
45
Published as a conference paper at ICLR 2022
3. This part is easily proved, using L- ≤ L+ from Lemma 2, and directly minimizing (38).
□
In Table 4, we summarize bounds (34), (35), (36), (37), and (38).
L.1.2 CASEn≥d
Now, we analyze case, when n ≥ d. For simplicity, without losing the generality, we assume that
d | n, n > 1, and d > 1. Then, PermK from Definition 3 satisfies the AB inequality with A = B =
d-1
n-1 .
In each iteration of MARINA, on average, PermK sends
O (pd + (1 - p))
bits, thus the theoretical communication complexity is
∆0
NPermK (P) = - (Pd +(1 - P))
(39)
up to a constant factor.
Lemma 14. For communication complexity NPermK (P) of MARINA with PermK, communication
complexity NRandK (P, k) of MARINA with RandK and communication complexity NTopK (k) of EF21
with TopK defined in (39), (32) and (33) the following inequalities hold:
1.	Lower bounds:
∆0
NPermK(p) ≥ 正 mm d dL-,L- +
∀P∈ (0, 1].
Upper bounds:
(40)
(41)
2.	Lower bounds:
NRandK (P, k)
∀P ∈ (0, 1], ∀k ∈ {1,...,d},
Upper bounds:
NRandK
Moreover, for all k ∈ {1, . . . , d},P = 1,
NRandK (1, k) =
∆0dL-
ε
(42)
(43)
3.
∆0dL
k∈min,d} NTopK(k) = NTopK(d) =
(44)
Proof.
46
Published as a conference paper at ICLR 2022
1.
NPermK (p) = < (Pd +(1 - P)) (L- + 卜曰
≥ 华(P + (I -P)I)
Using Lemma 12 with a = dL-, b = d√L±, and ymin = 1/d, We get
∆0
NPermK(p) ≥ 匹 mm d dL-,L- +
for allP ∈ (0, 1]. We can show the bound 40 if we take P = 1/d:
NPermK
The bound 41 is obtained by taking P
2.
3.
As in Lemma 13 we can get, that
NRandK (P, k) ≥
∆0
——min < dL-, L- +
2ε
for allP ∈ (0, 1], k ∈ {1, . . . , d}. Moreover, if we take P = 1/d and k = 1, we have
1	2∆0
NRandK (d, 1 1 ≤ ɛ	(L- +
The bound 43 is obtained by taking P = 1.
For TopK, the reasoning the same as in Lemma 13.
□
In Table 4, we summarize bounds (40), (41), (42), (43), and (44).
L.2 P匕ASSUMPTION
L.2. 1 CASE n ≤ d
Using the same reasoning as in Appendix L.1, Theorem 5 and Theorem 7, we can show that com-
munication complexities predicted by theory are equal to
NPermK (P) := log
d
+ (1 — P)- max
n
∆0	L-
NRandK(p, k) ：= log — (Pd +(1 — p)k) max < I — +
∆0	L-
NTopK (k)：= log —k max < ( — +
L+ d — k + ʌ/d2 — dk
μ
k
,P ,
Jr-I
LbP卜(46)
(45)
(47)
L
μ
up to a constant factor.
Lemma 15. For communication complexity NPermK (P) of MARINA with PermK, communication
complexity NRandK (P, k) of MARINA with RandK and communication complexity NTopK (k) of EF21
with TopK defined in (45), (46) and (47) the following inequalities hold9 :
9In the lemma, we use “Big Theta” notation, which means, that if f(x) = Θ(g(x)), then f is bounded both
above and below by g asymptotically up to a logarithmic factor.
47
Published as a conference paper at ICLR 2022
1.
inf NPermK (p) = Θ
p∈(0,1]
2.
p∈(0,1],ikn∈f{1,...,d}NRandK(p,k)=Θ
3.
min	NTopK (k)
k∈{1,...,d}
Proof. Rearranging (45), (46) and (47), we get
Note, that
------ -----≥ d,	∀k ∈{1,..., d},
1 - √1 - d
thus in all complexities, the second terms inside the max brackets are at least d.
Analysis of first terms inside the max brackets is the same as in Lemma 13.	口
In Table 6, we provide complexity bounds with optimal parameters of algorithms.
L.2.2 CASE n ≥ d
The only difference here is that the communication complexity of PermK predicted by our theory
is the following:
∆0
NPermK (P)=IOg -(Pd+(I-P))max
2(1 — P) d — 1 L±
p n — 1 μ
),p1.
(48)
Lemma 16. For communication complexity NPermK (P) of MARINA with PermK, communication
complexity NRandK (P, k) of MARINA with RandK and communication complexity NTopK (k) of EF21
with TopK defined in (48), (46) and (47) the following inequalities hold:
1.
inf NPermK (P) = Θ (max { m min { dL-, L- + √= L± } , d
2.
p∈(0,1]in{1,..,d} NRandK (P，k) = θ Im© { μ min dL--L-- + &+ }，d
48
Published as a conference paper at ICLR 2022
3.
min	NTopK (k)
k∈{1,...,d}
The proof of Lemma 16 is the same as in Lemma 15.
Using the same reasoning as before, we provide complexity bounds in Table 6.
49
Published as a conference paper at ICLR 2022
M Group Hessian Variance
We showed the communication complexity improvement of MARINA algorithm with PermK under
the assumption that L± L-. In general, L± can be large; however, we can still use the notion of
L± but in a different way, by splitting the functions into several groups where L± is small.
We split a Set {1,…，n} into nonempty sets {Gk}k=「Uk=ι Gk = {1,…，n}, Gi T Gj = 0, for
all i = j ∈ {1,…，g}, and |Gk| > 0, for all k ∈ {1,…，g}. Let Us fix some set Gk and define
functions
2
L-k(x,y) ：= J看 X (Vfi(X)-Vfi(y)),
L+k (x,y):=焉 X kVfi(x) -Vfi(y)k2,
1Gk1 i∈∣Gk∣
LG±k (x, y) := LG+k (x, y) - LG-k (x, y)
and the smallest constants LG-k, LG+k, LG±k for fUnctions LG-k (x, y), LG+k (x, y), and LG±k (x, y), sUch
that
LG-k(x,y) ≤	LG-k	kx	- yk2 ,LG+k(x,y) ≤	LG+k	kx -	yk2	, LG±k (x,	y) ≤	LG±k	kx - yk2	,
for all k ∈ {1,…，g} ,x,y ∈ Rd.
In this section, we have the following assUmption aboUt groUps.
Assumption 6. Compressors between groups are independent, i.e. Ci and Cj are independent, for
all i ∈ Gk , j ∈ Gp , k 6= p. And Assumption 4 is satisfied with constants AGk and BGk inside each
group Gk ,for k ∈ {1, ∙∙∙ ,g}.
Now, we prove groUp AB ineqUality.
Lemma 17 (GroUp AB ineqUality). Let us assume that Assumptions 3 and 6 hold, then
E
1n
-X
n i=1
2
Ci(ai) -
-X ail
BGkIGk |2 l _1_ X
辰 Nk i
(49)
Proof.
+^2	X EI / X Ci(ai)	- X ai, X	Ci(ai)	- X a)	I.
k6=p i∈Gk	i∈Gk i∈Gp	i∈Gp
50
Published as a conference paper at ICLR 2022
Due to independence and unbiasedness, the last term vanishes, and, using AB inequality, We get
2
ElI nX CiM-1X @
Il i=1
1 ɪ
-2 X E
n2乙J
i=1
g
=X
k = 1
g
≤ X
k = 1
k=1
IGkI2
n2
X Ci(ai) - X «i|2
i∈Gk
EII ∣⅛
里(AGk
n2
i∈Gk
E Ci(ai)-
i∈Gk
∖Gk I，
击 iS αil
χ*1- BT 击 X
From this We can get the result.
□
Next, we prove analogous lemma to Lemma 5.
Lemma 18. Let US consider gt+1 from Line 8 ofAlgorithm 1 and assume, thatAssumptions 3 and 6
hold. Moreover, if Assumption 2 holds for ^very group Gk, for k ∈ {1, ∙ ∙ ∙ , g}, then
E [∣gt+1 -V/(xt+1)∣2∣xt+1]
≤ (1 -P) (X (AGk-B2Gk"Gk12 (毋)2 + Xb⅛F (L±k)2) l∣χt+1 -χtl∣2
∖k=1	k=1	)
+ (1 - P)Igt-V/(xt) I2.	(50)
Proof. In the view of definition of gt+1, we get
E [∣∣gt+1 -Vf(Xt+1 )∣∣2∣χt+1]
=(1 -p)E ]∣gt + n XCi (Vfi(Xt+1)-Vfi(Xt)) -Vf (xt+1)l | xt+1
I Il	i=ι	Il ∣ I
(1 -p)E ]∣n XCi (Vfi(xt+1) -Vfi(xt)) -Vf(Xt+1) + V∕(xt)∣ ∣xt+1
+ (1 -p) I∣gt - Vf(Xt)∣∣2.
51
Published as a conference paper at ICLR 2022
In the last inequality we used unbiasedness of Ci . Using (49), we get
E hIIgt+1 -Vf(xt+1)II2xt+1i
≤ (1 - p)E
1 XXCi (Vfi(Xt+1)-Vfi(Xt)) -Vf(Xt+1) + Vf(Xt)I 卜+1
+ (1- P)Ilgt-Vf(χt)∣∣2.
≤ (1 - p)
AGk|Gk|2 1
n2	|Gk|
X IIVfi(xt+1) - Vfi(xt)II2
i∈Gk
-XX b⅛m2 ll |Gk| X Vfi(Xt+I)-Vfi(Xt)
k=1	n I k i∈Gk
+(1-P)IIgt-Vf(Xt)II2
2
(1 - p)
(AGk - BGk) |Gk|2
n2
g
LG+k(xt+1,xt)+X
k=1
BGk|Gk|2
n2
LG±k(xt+1,xt)
+(1-P)IIgt-Vf(xt)II2
Theorem 10. Let Assumptions 1, 2, 3 and 6 be satisfied. Let the stepsize in MARINA be chosen as
t II2
-----∖ -1
1-P 2
□
γ≤
P
then after T iterations, MARINA finds point XT for which E IjIVf(XT )『]≤ ^T.
Theorem 11. Let Assumptions 1, 2, 3, 5 and 6 be satisfied and
-ι	、
γ ≤ min
ILG，2μ 卜
then for xT from MARINA algorithm the following inequality holds:
E f(xτ) - f ?] ≤ (1 - γμ)T ∆0.
We omit proofs of this theorems as they repeat proofs from Appendix C.6 and D.2; the only differ-
ence is that we have to take Lb2 = Lb2G .
Let us assume that n ≤ d, all groups have equal sizes |Gk| = G and constants LG±k = L±G , for all k ∈
{1, . . . , g}, and in each group we use PermK compressor from Definition 2, thus communication
complexity predicted by our theory is the following:
NPermK (P) :
d
+ (I — P)不
G
⅛)GL±.
52
Published as a conference paper at ICLR 2022
Using the same reasoning as in Lemma 13, we can take p = 1 or p = 1/G to get that
inf
p∈(0,1]
NPermK (p)
(51)
For the case when we have one group, we restore the communication complexity from Lemma 13.
Comparing (34) with (51), We see that dL-∕n from (34) is always better than dL-∕G from (51);
however; if dL±∕√n is a bottleneck and LG is small, then communication complexity (51) can be
better.
Let us consider an example of a quadratic optimization task with two groups, wherein one group, all
matrices are equal to A, and in another one, all matrices are equal to B, A 6= B, A = A> < 0 and
B = B> < 0, then G = n∕2, LG± = 0, and L± > 0 (see Example 3). Hence, we get that
inf
p∈(0,1]
NPermK (p)
This bound is better than (34) by at least the factor 1 + √nL±∕L-.
53