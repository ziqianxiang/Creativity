Published as a conference paper at ICLR 2022
Towards Understanding Generalization via
Decomposing Excess Risk Dynamics
Jiaye Teng1,*, Jianhao Ma2,∖Yang Yuan1,3
1Institute for Interdisciplinary Information Sciences, Tsinghua University
2 Department of Industrial and Operational Engineering, University of Michigan, Ann Arbor
3Shanghai Qi Zhi Institute
tjy20@mails.tsinghua.edu, jianhao@umich.edu
yuanyang@tsinghua.edu
Ab stract
Generalization is one of the fundamental issues in machine learning. However,
traditional techniques like uniform convergence may be unable to explain gener-
alization under overparameterization (Nagarajan & Kolter, 2019). As alternative
approaches, techniques based on stability analyze the training dynamics and derive
algorithm-dependent generalization bounds. Unfortunately, the stability-based
bounds are still far from explaining the surprising generalization in deep learning
since neural networks usually suffer from unsatisfactory stability. This paper pro-
poses a novel decomposition framework to improve the stability-based bounds via
a more fine-grained analysis of the signal and noise, inspired by the observation
that neural networks converge relatively slowly when fitting noise (which indicates
better stability). Concretely, we decompose the excess risk dynamics and apply the
stability-based bound only on the noise component. The decomposition framework
performs well in both linear regimes (overparameterized linear regression) and
non-linear regimes (diagonal matrix recovery). Experiments on neural networks
verify the utility of the decomposition framework.
1	Introduction
Generalization is one of the essential mysteries uncovered in modern machine learning (Neyshabur
et al., 2014; Zhang et al., 2016; Kawaguchi et al., 2017), measuring how the trained model performs
on unseen data. One of the most popular approaches to generalization is uniform convergence (Mohri
et al., 2018), which takes the supremum over parameter space to decouple the dependency between
the training set and the trained model. However, Nagarajan & Kolter (2019) pointed out that uniform
convergence itself might not be powerful enough to explain generalization, because the uniform
bound can still be vacuous under overparameterized linear regimes.
One alternative solution beyond uniform convergence is to analyze the generalization dynamics,
which measures the generalization gap during the training dynamics. Stability-based bound is among
the most popular techniques in generalization dynamics analysis (Lei & Ying, 2020), which is derived
from algorithmic stability (Bousquet & Elisseeff, 2002). Fortunately, one can derive non-vacuous
bounds under general convex regimes using stability frameworks (Hardt et al., 2016). However,
stability is still far from explaining the remarkable generalization abilities of neural networks, mainly
due to two obstructions. Firstly, stability-based bound depends heavily on the gradient norm in
non-convex regimes (Li et al., 2019), which is typically large at the beginning phase in training neural
networks. Secondly, stability-based bound usually does not work well under general non-convex
regimes (Hardt et al., 2016; Charles & Papailiopoulos, 2018; Zhou et al., 2018b) but neural networks
are usually highly non-convex.
The aforementioned two obstructions mainly stem from the coarse-grained analysis of the signal and
noise. As Zhang et al. (2016) argued, neural networks converge fast when fitting signal but converge
* Equal contribution.
1
Published as a conference paper at ICLR 2022
Figure 1: (a) The gradient norm of standard training (training over noisy data) is larger than that
of variance training (training over pure noise) at initialization θ(0) = θv(0) ≈ 0. We denote the
minimizers of the training loss by θ, θv , respectively (where the y-axis is the training loss). (b) The
whole training loss landscape is highly non-convex while the trajectory of the variance training lies
in a convex region due to the slow convergence. We defer the details to Appendix B.
relatively slowly when fitting noise1, indicating that the training dynamics over signal and noise
are significantly different. Consequently, on the one hand, the fast convergence of signal-related
training contributes to a large gradient norm at the beginning phase (see Figure 1a), resulting in
poor stability. On the other hand, the training on signal forces the trained parameter away from the
initialization, making the whole training path highly non-convex (see Figure 1b). The above two
phenomena inspire us to decompose the training dynamics into noise and signal components and only
apply the stability-based analysis over the noise component. To demonstrate that such decomposition
generally holds in practice, we conduct several experiments of neural networks on both synthetic and
real-world datasets (see Figure 2 for more details).
Based on the above discussion, we improve the stability-based analysis by proposing a decomposition
framework on excess risk dynamics2, where we handle the noise and signal components separately
via a bias-variance decomposition. In detail, we decompose the excess risk into variance excess
risk (VER) and bias excess risk (BER), where VER measures how the model fits noise and BER
measures how the model fits signal. Under the decomposition, we apply the stability-based techniques
to VER and apply uniform convergence to BER inspired by Negrea et al. (2020). The decomposition
framework accords with the theoretical and experimental evidence surprisingly well, providing that it
outperforms stability-based bounds in both linear (overparameterized linear regression) and non-linear
(diagonal matrix recovery) regimes.
We summarize our contributions as follows:
•	We propose a new framework aiming at improving the traditional stability-based bounds,
which is a novel approach to generalization dynamics analysis focusing on decomposing
the excess risk dynamics into variance component and bias component. Starting from the
overparameterized linear regression regimes, we show how to deploy the decomposition
framework in practice, and the proposed framework outperforms the stability-based bounds.
•	We theoretically analyze the excess risk decomposition beyond linear regimes. As a case
study, we derive a generalization bound under diagonal matrix recovery regimes. To our
best knowledge, this is the first work to analyze the generalization performance of diagonal
matrix recovery.
•	We conduct several experiments on both synthetic datasets and real-world datasets (MINIST,
CIFAR-10) to validate the utility of the decomposition framework, indicating that the
framework provides interesting insights for the generalization community.
1In this paper, we refer the signal to the clean data without the output noise, and the noise to the output noise.
See Section 2 for the formal definitions.
2We decompose the excess risk, which is closely related to generalization, purely due to technical reasons.
The excess risk dynamics tracks the excess risk during the training process.
2
Published as a conference paper at ICLR 2022
Figure 2: Experiments of neural networks on synthetic (linear ground truth, 3-layer NN, SGD) and
real-world (MNIST, 3-layer CNN, Adam) datasets. The trend of excess risk dynamics (blue) meets its
bias component (BER) at the beginning phase and meets its variance component (VER) afterwards,
indicating that ER can indeed be decomposed into VER and BER. See Appendix A for more details.
1.1 Related Work
Stability-based Generalization. The stability-based researches can be roughly split into two
branches. One branch is about how algorithmic stability leads to generalization (Feldman & Vondrak,
2018; 2019; Bousquet et al., 2020). Another branch focus on how to calculate the stability parameter
for specific problems, e.g., Hardt et al. (2016) prove a generalization bound scales linearly with
time in convex regimes. Furthermore, researchers try to apply stability techniques into more general
settings, e.g., non-smooth loss (Lei & Ying, 2020; Bassily et al., 2020), noisy gradient descent in
non-convex regimes (Mou et al., 2018; Li et al., 2019) and stochastic gradient descent in non-convex
regimes (Zhou et al., 2018b; Charles & Papailiopoulos, 2018; Zhang et al., 2021). In this paper, we
mainly focus on applying the decomposition framework to improve the stability-based bound.
Uniform Convergence is widely used in generalization analysis. For bounded losses, the gener-
alization gap is tightly bounded by its Rademacher Complexity (Koltchinskii & Panchenko, 2000;
Koltchinskii, 2001; Koltchinskii et al., 2006). Furthermore, we reach faster rates under realizable
assumption (Srebro et al., 2010). A line of work focuses on uniform convergence under neural
network regimes, which is usually related to the parameter norm (Bartlett et al., 2017; Wei & Ma,
2019). However, as Nagarajan & Kolter (2019) pointed out, uniform convergence may be unable to
explain generalization. Therefore, more techniques are explored to go beyond uniform convergence.
Other Approaches to Generalization. There are some other approaches to generalization, including
PAC-Bayes (Neyshabur et al., 2017a; Dziugaite & Roy, 2017; Neyshabur et al., 2017b; Dziugaite &
Roy, 2018; Zhou et al., 2018a; Yang et al., 2019), information-based bound (Russo & Zou, 2016;
Xu & Raginsky, 2017; Banerjee & Montufar, 2021; Haghifam et al., 2020; Steinke & Zakynthinou,
2020), and compression-based bound (Arora et al., 2018; Allen-Zhu et al., 2018; Arora et al., 2019).
Bias-Variance Decomposition. Bias-variance decomposition plays an important role in statistical
analysis (Lehmann & Casella, 2006; Casella & Berger, 2021; Geman et al., 1992). Generally, high
bias indicates that the model has poor predicting ability on average and high variance indicates that the
model performs unstably. Bias-variance decomposition is widely used in machine learning analysis,
e.g., adversarial training (Yu et al., 2021), double descent (Adlam & Pennington, 2020), uncertainty
(Hu et al., 2020). Additionally, (Nakkiran et al., 2020) propose a decomposition framework from an
online perspective relating the different optimization speeds on empirical and population loss. Oymak
et al. (2019) applied bias-variance decomposition on the Jacobian of neural networks to explain their
different performances on clean and noisy data. This paper considers a slightly different bias-variance
decomposition following the analysis of SGD (Dieuleveut et al., 2016; Jain et al., 2018; Zou et al.,
2021), focusing on the decomposition of the noisy output.
3
Published as a conference paper at ICLR 2022
Matrix Recovery. Earlier methods for solving matrix recovery problems rely on the convex relaxation
techniques for minimum norm solutions (Recht et al., 2010; Chandrasekaran et al., 2011). Recently,
a branch of works focuses on the matrix factorization techniques with simple local search methods. It
has been shown that there is no spurious local minima in the exact regime (Ge et al., 2016; 2017;
Zhang et al., 2019). In the overparameterized regimes, it was first conjectured by Gunasekar et al.
(2018) and then answered by Li et al. (2018) that gradient descent methods converge to the low-rank
solution efficiently. Later, Zhuo et al. (2021) extends the conclusion to the noisy settings.
2 Preliminary
In this section, we introduce necessary definitions, assumptions, and formally introduce previous
techniques in dealing with generalization.
Data distribution. Let x ∈ X ⊂ Rp be the input and y ∈ Y ⊂ R be the output, where x, y are
generated from a joint distribution (x, y)〜P. Define Pχ, Py, and Py∣χ as the corresponding
marginal and conditional distributions, respectively. Given n training samples D , {xi, yi}i∈[n]
generated from distribution P, we denote its empirical distribution by Pn . To simplify the notations,
we define X ∈ Rn×p as the design matrix and Y ∈ Rn as the response vector.
Excess Risk. Given the loss function `(θ; x, y) with parameter θ and sample (x, y), we de-
fine the population loss as L(θ; P) ，E(x,y)〜P ['(θ; x, y)] and its corresponding training loss
as L(θ; Pn)，1 Pi '(θ; Xi, yi). Let At denote the optimization algorithm which takes the dataset
D as an input and returns the trained parameter θ(t at time t, namely, At(D) = θ(t). During
the analysis, we focus on the excess risk dynamics Ec(θ(t); P), which measures how the trained
parameter θ(t performs on the population loss:
ElW P)，L(θ(t) ； P) - min L(θ;P).
θ
Although the minimizer may not be unique, we define L(θ*; P)，mine L(θ; P) where θ* denotes
arbitrarily one of the minimizers. Additionally, bounding the generalization gap L(θ⑴;P)-
L(θ(t); Pn) suffices to bound the excess risk under Empirical Risk Minimization (ERM) framework
by Equation equation 1. Therefore, we mainly discuss the excess risk following Bartlett et al. (2020).
El(Θ ⑴;P )= ∣L(θ ⑴;P) -L(θ(t); Pn)] + ∣L(θ ㈤;P n)-L(θ*; Pn)] + ∣L(θ*; Pn) -L(θ*; P)].
×---------------------------V---------------------------} X------------------------------V----------------------------} X---------------------------V------------------------}
generalization gap
≤0 under ERM	≈0 by concentration inequalities
(1)
VER and BER. As discussed in Section 1, we aim to decompose the excess risk into the variance
excess risk (VER) and the bias excess risk (BER) defined in Definition 1. The decomposition focuses
on the noisy output regimes and we split the output y into the signal component E[y|x] and the noise
component y - E[y|x]. Informally, VER measures how the model performs on pure noise, and BER
measures how the model performs on clean data.
Definition 1 (VER and BER) Given (x, y)〜P ,let (x, E[y∣x])〜Pb denote the signal distri-
bution and (x, y — E[y∣x])〜Pv denote the noise distribution. The variance excess risk (VER)
ELv (θ; P) and bias excess risk (BER) ELb (θ; P) are defined as:
EL(θ; P) , El(θ; Pv),	EL(θ; P) , El(θ; Pb).
To better illustrate VER and BER, we consider three surrogate training dynamics: Standard Training,
Variance Training, and Bias Training, corresponding to ER, VER, and BER, respectively.
Standard Training: Training process over the noisy data (X, Y) from the initialization θ(0). We
denote the trained parameter at time t by θ(t.
Variance Training: Training process over the pure noise (X, Y - E[Y|X]) from the initialization
θv0). We denote the trained parameter at time t by θvt).
Bias Training: Training process over the clean data (X, E[Y|X]) from the initialization θ(0). We
4
Published as a conference paper at ICLR 2022
(t)
denote the trained parameter at time t by θb .
When the context is clear, although the trained parameters θ(e), θVt), θ(t) are related to the corre-
sponding initialization and the algorithms, We omit the dependency. Besides, We denote θ*, θV and
θb the optimal parameters which minimize the corresponding population loss L(θ; P), L(θ; Pv),
and L(θ; Pb), respectively.
Techniques in generalization. We next introduce tWo techniques in generalization analysis includ-
ing stability-based techniques in Proposition 1 and uniform convergence in Proposition 2. These
techniques Will be revisited in Section 3.
Proposition 1 (Stability Bound from Feldman & Vondrak (2019)) Assume that algorithm At is
-uniformly-stable at time t, meaning that for any two dataset D and D0 with only one different data
point, we have
sup EA ['(At(D); x, y) - '(At(D0); x, y)] ≤ e.
(x,y)
Then the following inequality holds3 with probability at least 1 - δ:
|L (At(D); P) - L (At(D); Pn)| =O
e log(n) log(n∕δ) + Jlog(I∕δ)
Proposition 2 (Uniform Convergence from Wainwright (2019)) Uniform convergence decouples
the dependency between the trained parameter and the training set by taking supremum over a
parameter space that is independent of training data, namely
L(At(D); P) - L(At(D); P n) ≤ sup [L(θ; P) - L(θ; Pn)].
θ∈B
where B is independent of dataset D and At(D) ∈ B for any time t.
3	Warm-up: Decomposition under Linear Regimes
In this section, We consider the overparameterized linear regression With linear assumptions, Where
the sample size is less than the data dimension, namely, n < p. We Will shoW hoW the decomposition
frameWork Works and the improvements compared to traditional stability-based bounds. Before diving
into the details, We emphasize the importance of studying overparameterized linear regression. As
the arguably simplest models, We expect a frameWork can at least Work under such regimes. Besides,
recent Works (e.g., Jacot et al. (2018)) shoW that neural netWorks converge to overparametrized
linear models (With respect to the parameters) as the Width goes to infinity under some regularity
conditions. Therefore, studying overparameterized linear regression provides enormous intuition to
neural netWorks.
Settings. When the context is clear, We reuse the notations in Section 2. Without loss of generality,
we assume that E[x] = 0. Besides, we assume a linear ground truth y = x>θ* + E where
E[|x] = 0. Let Σx , E[xx>] denote the covariance matrix. Given `2 loss `(θ; x, y) = (y -x>θ)2,
we apply gradient descent (GD) with constant step size λ and initialization θ(0) = 0, namely,
θ(t+D = θ(t + n Pi Xi(yri — χ>θ⑴).We provide the main results of overparameterized linear
regression in Theorem 1 and then introduce how to apply the decomposition framework.
Theorem 1 (Linear Regression Regimes) Under the overparameterized linear regression settings,
assume that W = √ θ*' X = is σW-Subgaussian. If ∣∣xk ≤ 1,同 ≤ V, supt∈[τ] ∣∣θvt'k ≤ B are all
θ θ*,>ςxθ*
bounded, the following inequality holds with probability at least 1 - δ:
EL铲);P) = O (max n1, θ*,>∑χθ*σW, [V + B^ ^0^+ ≡2 + Tλ[V±B2
w	n λT	n
where the probability is taken over the randomness of training data.
3In the rest of the paper, we use O or . to omit the constant dependency and use O to omit the log dependency
of n. For example, we write 2/n = O(1∕n) or 2/n . 1/n, log(n)/n = O(1∕n), and log2(n)∕n = O(1∕n).
5
Published as a conference paper at ICLR 2022
By setting T = θ(√n) in Theorem 1, the upper bound is approximately O(√n), indicating that
the estimator at time T is consistent4. Below we show the comparison with some existing bounds,
including benign overfitting bound and traditional stability-based bound5.
Comparison with benign overfitting. The bound in Bartlett et al. (2020) mainly focus on the min-
norm solution (as T → ∞,the parameter trained in GD converges to the min-norm solution). Instead,
the bound in Theorem 1 does not require the min-norm solution assumption.
Comparison with stability-based bound. The stability-based bound (without applying the decompo-
sition framework) is O (max{1, [V + B0]2},制2 + T[V + B0]2) , where B0 = SUptk0(t)k
denotes the bound of the trained parameter in standard training. As a comparison, the notation B in
Theorem 1 denotes the bound of the trained parameter in variance training. One major difference
between B and B0 is that: B is independent of ∣∣θ*k while B0 is closely related to ∣∣θ*k. Therefore,
with a large time T, the bound in Theorem 1 outperforms the stability-based bound when ∣∣θ*∣
is large compared to V (namely, large signal-to-noise ratio)6. We refer to Appendix F.1 for more
discussion.
Proof Sketch. We defer the proof details to Appendix C due to space limitations. Firstly, we provide
Lemma 1 to show that the excess risk can be decomposed into VER and BER.
Lemma 1 Under the overparameterized linear regression settings with initialization θ(0) = θb(0) =
θv(0) = 0, for any time T, the following decomposition inequality holds:
EL(θ(T)； P) ≤ 2EL(ΘVT)； P) + 2EL(θ(τ)； P).
We next bound VER and BER separately in Lemma 2 and Lemma 3. When applying the stability-
based bound on VER, we first convert VER into a generalization gap (see Equation 1) and then bound
three terms individually.
Lemma 2 Under the overparameterized linear regression settings, assume that ∣x∣ ≤ 1, || ≤ V,
SUptk θV" k ≤ B are all bounded. Consider the gradient descent algorithm with constant stepsize λ,
with probability at least 1 - δ:
ELevT) P) = O (Tλ [V + B]2 log(n) log(2n∕δ) + max {l, [V + B]2} /^^ . (2)
In Lemma 3, we apply uniform convergence to bound BER. Although we can bound it by directly
calculating the excess risk, we still apply uniform convergence, because exact calculating is limited to
a specific problem while uniform convergence is much more general. To link the uniform convergence
and excess risk, we split the excess risk into two pieces inspired by Negrea et al. (2020). For the first
piece, which stands for the generalization gap, we bound it by uniform convergence. For the second
piece, which stands for the (surrogate) training loss, we derive a bound decreasing with time T.
-	Z»* , 丁 —
Lemma 3 Under the overparameterized linear regression settings, assume that W = √联丁工 族 is
σw2 -subgaussian, then with probability at least 1 - δ:
EL WT) ；P) = o (θl⅛θl σw SlogB+λ⅛ kθ*k2).	⑶
Although we do not present it explicitly, the results in Lemma 3 are derived under uniform convergence
frameworks. Combining Lemma 1, Lemma 2 and Lemma 3 leads to Theorem 1.
4Consistency means the upper bound converges to zero as the number of training samples goes to infinity.
5We remark that the assumptions in Theorem 1 can be relaxed. Firstly, assuming x is subGaussian suffices to
show that w is subGaussian. Secondly, assuming is subGaussian suffices to bound || ≤ V with a log(n) cost.
6A large time t can guarantee that the dominating term in Theorem 1 is W [V + B0]2. We remark that with
a large signal-to-noise ratio, we have W [V + B0]2 ≥ W [V + B]2 ).
6
Published as a conference paper at ICLR 2022
4 Decomposition under General Regimes
This section mainly discusses how to decompose the excess risk into variance component (VER)
and bias component (BER) beyond linear regimes. Due to a fine-grained analysis on the signal and
noise, the decomposition framework improves the stability-based bounds under the general regimes.
We emphasize that the methods derived in this section are nevertheless the only way to reach the
decomposition goal and we leave more explorations for future work. To simplify the discussion, we
assume the minimizer θ* of the excess risk exists and is unique. Before diving into the main theorem,
it is necessary to introduce the definition of local sharpness for the excess risk function, measuring
the smoothness of the excess risk function around the minimizer θ*.
Definition 2 (Local Sharpness) For a given excess risk function El (θ; P) with El(Θ*; P) = 0, we
define s > 0 as the local sharpness factor when the following equation holds7 :
0<
lim	胃 θ;P)
θ“∣θ-θ*k→o ∣∣θ - θ*ks
< ∞.
Assumption 1 (Uniform Bound) Assume that for excess risk function EL (θ; P) with local sharp-
ness s, given Px, for any Py|x and θ, the uniform bounds 0 < mu(Px) ≤ Mu(Px) exist:
mu(Px) ≤
El(Θ; P)
∣∣θ - θ*ks
≤ Mu (Px).
When the context is clear, we omit the dependency of Px and denote the bounds by mu and Mu.
Roughly speaking, the uniform bounds act as the maximal and minimal eigenvalues of the excess risk
function EL(θ; P). For example, under the overparameterized linear regression regimes (Section 3),
the uniform bounds are the maximum and minimum eigenvalues of the covariance matrix Σx .
Assumption 2 (Dynamics Decomposition Condition, DDC) We assume the trained parameters
θ(T) are (a, C, C0)-bounded at time T, namely:
kθ(T) - θ*k ≤ a(kθVT) - θVk + kθ(T) - θ:k) + √T + √,	(4)
where n denotes the size Oftraining samples, T denotes the training time, and θ*, θV and θb denote
the minimizers of the corresponding population loss L(θ; P), L(θ; Pv), and L(θ; Pb), respectively.
We emphasize that although We use -Cp + √0 in Assumption 2, it can be replaced with any o(1)
term which converges to zero when T, n go to infinity. Empirically, several experiments demonstrate
the generality of DDC and its variety (see Figure 3). From the theoretical perspective, we derive
that overparameterized linear regression is (1, 0, 0)-bounded in DDC, and diagonal matrix recovery
regimes satisfy DDC (see Section 4.1). We next show how to transfer DDC to the Decomposition
Theorem in Theorem 2.
Theorem 2 (Decomposition Theorem) Assume that the excess risk has a unique minimizer with
local sharpness factor s. Under Assumption 1 and Assumption 2, the following decomposition
inequality holds:
El(Θ(T) ； P) ≤ [4a]s M [EL (ΘVt )； P)+ EL (θ(T )； P )i + Mu [ √T [+ MU
s
.	(5)
We defer the proofs to Appendix D due to limited space. In Theorem 2, Assumption 1 and As-
sumption 2 focus on the population loss and the training dynamics individually. We remark that our
proposed generalization bound is algorithm-dependent due to parameters (a, C, C0) in DDC.
By Theorem 2, we successfully decompose the excess risk dynamics into VER and BER. The
remaining question is how to bound VER and BER individually. As we will derive in Section 4.1
via a case study on diagonal matrix recovery, we bound VER by stability techniques and BER by
uniform convergence.
7Here ∣∣ ∙ k refers to the '2-norm.
7
Published as a conference paper at ICLR 2022
(a) linear regression, n = 300
(d) linear regression, n = 800
(e) matrix recovery, n = 600
(f) neural network, n = 1500
Figure 3: DDC holds in overparameterized linear regression, general matrix recovery, and neural networks.
For linear regression and neural networks, we set a = 1, C = C0 = 0 in DDC. For matrix recovery problems,
we set a = 1, C = 0, C0 = 8.5 in DDC. The upper bound (the right hand side in DDC, red line) is over the
standard norm (the left hand side in DDC, orange line), indicating that DDC holds in general. We defer the
experimental details to Appendix B.
4.1 Diagonal Matrix Recovery
From the previous discussion, we now conclude the basic procedure when applying the decomposition
framework under Theorem 2. Generally, there are three questions to be answered in a specific problem:
(a) Does the problem satisfy DDC? (b) How to bound VER using stability? (c) How to bound BER
using uniform convergence? To better illustrate how to solve the three questions individually, we
provide a case study on diagonal matrix recovery, a special and simplified case of a general low-rank
matrix recovery problem but still retains the key properties. The diagonal matrix recovery regime and
its variants are studied as a prototypical problem in Gissin et al. (2019); HaoChen et al. (2021).
Settings. Let X?=Diag{σ?, ∙ ∙ ∙ , σ?, 0, ∙ ∙ ∙ , 0} ∈ Rd×d denote a rank-r (low rank) diagonal
matrix, where σ? ≥ ∙ ∙ ∙ ≥ σ? > 0. We define A ∈ RdXd as the measurement matrix which is
diagonal and y ∈ Rd as the measurement vector. Assume the ground truth yi = Aiσi? + εi where
yi , Ai denotes their corresponding i-th element, i denotes the noise, and σi? = 0 if r < i ≤ d. Our
goal is to recover X? from dataset D = {A(i), y(i)}i∈[n] with n training samples.
In the training process, we use the diagonal matrix U = Diag{u1 , ∙ ∙ ∙ , ud} to predict X? via
X = UU >. Given the loss function8 Ln (U) = 1 Pn=IIlyj) — Aj) Θ UU >∣∣2,we train the model
with Gradient Flow9 Ut = —VLn(Ut). During the analysis, we assume that Ai is O(1)-UniformIy
bounded with unit variance, and εi is V2-subGaussian with uniform bounds ∣ε(j) | ≤ V. We emphasize
that the training procedure of diagonal matrix recovery is fundamentally different from linear regimes
since Ln(U) is non-convex with respect to U.
Theorem 3 (Decomposition Theorem in Diagonal Matrix Recovery) In the diagonal matrix re-
covery settings, if we set the initialization U0 = αI where α > 0 is the initialization scale, then for
8We use to represent the Hadamard product.
9We use gradient flow instead of gradient descent mainly due to technical simplicity.
8
Published as a conference paper at ICLR 2022
any 0 < t < ∞, with probability at least 1 - δ, we have:
EL(Ut； P).(kX ?kF+dV 2+dα4) Jlog*)+X σ+axe。。t)+αtd
j=1 j
dV2(t + 1) 1 , S1 22dn∖	2	2( 1 ∖ rν2 log (r∕δ)
+	log(n) log — + da2 + log2 02 ——
(6)
By setting a = Θ ((d2n)- 1) and t = Θ (log ((dnσr-)凡)in Theorem 3, the upper bound is
dV2+kX?k2
approximately O (----√n F ), indicating that the estimator at time t is consistent.
Comparison with stability-based bound. The stability-based bound (without applying the decomposi-
tion framework) is O (dV +√X kF + IdV +叱 k?)(t+I)). AS a comparison, the bound in Theorem 3
outperforms the stability-based bound in the sense that we deduce the coefficient of the blowing-up
term t+1 from dV2 + ∣∣X?k? to dV2, which is a significant improvement with a large signal-to-noise
ratio under large time t. Besides, the bound in Theorem 3 captures the bias-variance tradeoff, meaning
that the excess risk curve falls first and then rises.
Proof Sketch. We defer the proof details to Appendix E due to space limitations. Similar to Section 3,
the proof of Theorem 3 consists of the following three lemmas, corresponding to the DDC, BER, and
VER terms separately. We first prove that diagonal matrix recovery regimes satisfy DDCin Lemma 4.
Lemma 4 Under the assumptions in Theorem 3, for any 0 ≤ t < ∞, with probability at least 1 - δ,
we have
∣∣UtU>-X*∣∣F ≤ WbUtb,>-X*∣∣F + IIUvUviF+ O (log (0)产on亘!.⑺
Different from the overparameterized linear regression regimes, Lemma 4 suffers from a O (√n)
factor due to the non-linearity. This phenomenon can be verified empirically in Figure 3b and
Figure 3e. We next apply stability techniques and uniform convergence to tackle VER and BER in
Lemma 5 and Lemma 6, respectively.
Lemma 5 For VER, under the assumptions in Theorem 3, with probability at least 1 - δ, we have
EL(Ut； P) < dV2(t + 1) log(n) log (2dn) + dα2 + dV2 Jlθg").
nδ	n
(8)
Lemma 6 For BER, under the assumptions in Theorem 3, with probability at least 1 - δ, we have
EL(Ut； P) < (kX?kF + da4)《log,/^+ X σ2 + 04jeΩ(σjt) + 0td.	⑼
j=1 j
Combining the above lemmas leads to Theorem 3.
Remark: Directly applying Theorem 2 under overparameterized linear regression regimes costs a
loss on condition number. The cost comes from the DDC condition where we transform the function
space (excess risk function) to the parameter space (the distance between the trained parameter and
the minimizer). We emphasize that the field of decomposition framework is indeed much larger than
Theorem 2, and there are other different approaches beyond Theorem 2. For example, informally, by
introducing a pre-conditioner matrix A, changing DDC from the form ∣θ - θ? ∣ to ∣Aθ - Aθ? ∣ and
setting A = ∑X/2, one can avoid the cost of the condition number. We leave the technical discussion
for future work.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work has been partially supported by National Key R&D Program of China (2019AAA0105200)
and 2030 innovation megaprojects of china (programme on new generation artificial intelligence)
Grant No.2021AAA0150000. The authors would like to thank Ruiqi Gao for his insightful sugges-
tions. We also thank Tianle Cai, Haowei He, Kaixuan Huang, and, Jingzhao Zhang for their helpful
discussions.
References
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variance decomposition. arXiv preprint arXiv:2011.03321, 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep
nets via a compression approach. In International Conference on Machine Learning, pp. 254-263.
PMLR, 2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
PradeeP Kr. Banerjee and Guido Montufar. Information complexity and generalization bounds, 2021.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett, Philip M Long, Gdbor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Raef Bassily, Vitaly Feldman, Cristdbal Guzmdn, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. arXiv preprint arXiv:2006.06914, 2020.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. The Journal OfMachine Learning
Research, 2:499-526, 2002.
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610-626. PMLR, 2020.
George Casella and Roger L Berger. Statistical inference. Cengage Learning, 2021.
Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-sparsity
incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572-596, 2011.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745-754.
PMLR, 2018.
Aymeric Dieuleveut, Francis Bach, et al. Nonparametric stochastic approximation with large step-
sizes. Annals of Statistics, 44(4):1363-1399, 2016.
Gintare Karolina Dziugaite and Daniel Roy. Entropy-sgd optimizes the prior of a pac-bayes bound:
Generalization properties of entropy-sgd and data-dependent priors. In International Conference
on Machine Learning, pp. 1377-1386. PMLR, 2018.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. arXiv
preprint arXiv:1812.09859, 2018.
10
Published as a conference paper at ICLR 2022
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable
algorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270-1279. PMLR,
2019.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv
preprint arXiv:1605.07272, 2016.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. In International Conference on Machine Learning, pp. 1233-1242.
PMLR, 2017.
Stuart Geman, Elie Bienenstock, and Ren6 Doursat. Neural networks and the bias/variance dilemma.
Neural computation, 4(1):1-58, 1992.
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental
learning drives generalization. arXiv preprint arXiv:1909.12051, 2019.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. arXiv preprint arXiv:2004.12983, 2020.
Jeff Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. In Conference on Learning Theory, pp. 2315-2357. PMLR, 2021.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234. PMLR, 2016.
Shi Hu, Nicola Pezzotti, Dimitrios Mavroeidis, and Max Welling. Simple and accurate uncertainty
quantification from bias-variance decomposition. arXiv preprint arXiv:2002.05582, 2020.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Prateek Jain, Sham Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing
stochastic gradient descent for least squares regression: mini-batching, averaging, and model
misspecification. Journal of Machine Learning Research, 18, 2018.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv
preprint arXiv:1710.05468, 2017.
Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions
on Information Theory, 47(5):1902-1914, 2001.
Vladimir Koltchinskii and Dmitriy Panchenko. Rademacher processes and bounding the risk of
function learning. In High dimensional probability II, pp. 443-457. Springer, 2000.
Vladimir Koltchinskii et al. Local rademacher complexities and oracle inequalities in risk minimiza-
tion. Annals of Statistics, 34(6):2593-2656, 2006.
Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006.
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning, pp. 5809-5819. PMLR, 2020.
Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. arXiv preprint arXiv:1902.00621, 2019.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory,
pp. 2-47. PMLR, 2018.
11
Published as a conference paper at ICLR 2022
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-convex
learning: TWo theoretical viewpoints. In Conference on Learning Theory, pp. 605-638. PMLR,
2018.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization
in deep learning. arXiv preprint arXiv:1902.04742, 2019.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap: Good online learners
are good offline generalizers. arXiv e-prints, pp. arXiv-2010, 2020.
Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence:
Generalization via derandomization with an application to interpolating predictors. In International
Conference on Machine Learning, pp. 7263-7272. PMLR, 2020.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. arXiv preprint arXiv:1706.08947, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017b.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-
tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint
arXiv:1906.05392, 2019.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In
Artificial Intelligence and Statistics, pp. 1232-1240. PMLR, 2016.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth
loss. arXiv preprint arXiv:1009.3896, 2010.
Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual
information. In Conference on Learning Theory, pp. 3437-3452. PMLR, 2020.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classification
via an all-layer margin. arXiv preprint arXiv:1910.04284, 2019.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning
algorithms. arXiv preprint arXiv:1705.07809, 2017.
Jun Yang, Shengyang Sun, and Daniel M Roy. Fast-rate pac-bayes generalization bounds via shifted
rademacher processes. In NeurIPS, pp. 10802-10812, 2019.
Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, and Yi Ma. Understanding generaliza-
tion in adversarial training via the bias-variance decomposition. arXiv preprint arXiv:2103.09947,
2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Richard Y Zhang, Somayeh Sojoudi, and Javad Lavaei. Sharp restricted isometry bounds for the
inexistence of spurious local minima in nonconvex matrix recovery. Journal of Machine Learning
Research, 20(114):1-34, 2019.
12
Published as a conference paper at ICLR 2022
Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pingali, Chao Chen, and Mayank Goswami.
Stability of sgd: Tightness analysis and improved bounds. arXiv preprint arXiv:2102.05274, 2021.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the imagenet scale: a pac-bayesian compression approach. arXiv preprint
arXiv:1804.05862, 2018a.
Yi Zhou, Yingbin Liang, and Huishuai Zhang. Generalization error bounds with probabilistic
guarantee for sgd in nonconvex optimization. arXiv preprint arXiv:1802.06903, 2018b.
Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. On the computational and
statistical complexity of over-parameterized matrix sensing. arXiv preprint arXiv:2102.02756,
2021.
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign
overfitting of constant-stepsize sgd for linear regression. arXiv preprint arXiv:2103.12692, 2021.
13
Published as a conference paper at ICLR 2022
Supplementary Materials
In this section, we first provide additional numerical experiment in Section A and figure illustration in
Section B. We then give all the omitted proofs of lemmas and theorems in Section C, Section D, and
Section E. We finally provide some further discussions, including why we apply stability on VER,
the motivation behind DDC and a relaxed version of DDC.
A Numerical Experiment
In this section we provide additional numerical experiments on neural network to verify that our
theory is satisfied for general neural network architectures and optimization algorithms.
(a) Depth, L = 2
(b) Depth, L = 3
(c) Depth, L = 4
(d) Width, m = 64
(e) Width, m = 256
(f) Width, m = 512
(g) SGD
(h) Adam
Figure 4: Numerical results in Section A.1.
(i) Rprop
A.1 Synthetic Experiment
In this section, our goal is to recover a sparse linear function f?(x) = hθ?, xi (x ∈ R30) with fully
connected ReLU network. For each setting, we run 5 independent trials and report their average
performances (the solid line) and the corresponding standard deviations (the error bar).
Effects of depth: We first explore the effect of depth and width for the excess risk decomposition
property. In this experiment, we fix the width of each layer to be 64, and use SGD as the optimizer
with Gaussian initialization (N (0, σ2) where σ = 1 × 10-3) and stepsize η = 1 × 10-2. For the dash
line, we plot a(BER + VER) to verify the performance of our risk decomposition framework. Here a
can be regarded as a measure of the success of our theory. Ideally a should be a small constant so that
14
Published as a conference paper at ICLR 2022
our bound is non-vacuous. Specific to this experiment, for Figure 4a, 4b, 4c, we set a = 1.3, 2.0, 2.3,
respectively. Overall, these results are well-predicted by our theory. Moreover, it is remarkable to
notice that this constant becomes large once increasing the depth. This mainly contributes to the fact
that deeper neural network has higher non-linearity.
Effects of width: In this experiment, we explore the effect of width for 2-layer ReLU network with
SGD optimizer (the initialization and the stepsize are the same as above). For Figure 4d, 4e, 4e,
we set a = 1.3, 1.0, 1.0, respectively. As can be seen, the wider the neural network, the better our
risk decomposition works. We comment that this behavior also comes to the level of non-linearity. It
is now well-known that when the width tends to infinity, the neural network converges to a linear
function (e.g. (Jacot et al., 2018)).
Effects of optimizer: In this experiment, we try three different optimizers: SGD (the same
hyperparameters as before), Adam (stepsize η = 0.002, (β1, β2) = (0.9, 0.999), = 1e - 08, no
weight decay), Rprop (learning rate η = 5 × 10-4, (η1, η2) = (0.5, 1.2), stepsizes (1 × 10-6, 50)).
For Figure 4g, 4h, 4i, we set a = 1.3, 1.2, 1.1. It can be seen that our risk decomposition framework
is robust to different optimization algorithms, demonstrating its generality and potential to extend to
general optimization problems.
A.2 CIFAR- 1 0
In this section we deliver additional experiment in CIFAR-10 dataset. In this experiment, we use
ResNet-18 and choose Adam as the optimizer. We regard that optimizing over the original dataset
as the bias training (in the sense that all the label is noiseless, referring to the bias part). As for the
variance training, We set the ground truth to be the uniform logit V = [0.1,…，0.1]> (here We use
one-hot coding and choose the categorical cross entropy loss). As for the labels in the training dataset,
We generate it uniformly from {eι, ∙∙∙ , e10}, where ei is the standard basis in R10. Under such data
generating model, its Bayes risk is provided by -10 × 0.1 × log(0.1) ≈ 2.3026 for categorical cross
entropy loss. For the standard training, We generate the training label as folloWs: y = (1 -p)y?+pej.
Here p ∈ [0, 1] is the corruption probability, y? is the true label and j is uniformly sampled from
{1,…,10}. In this experiment, we set the corruption probability to be 0.4. The numerical result is
provided in Figure 5 . Different from the performance on MINIST, here the variance training actually
converges to the ground truth. We conjecture that this is due to the structure of ResNet, resulting to
some kind of implicit regularization toWards the training process.10
B	Figure Illustration
B.1 Figure 1a
Figure 1a conveys an important message that variance training has a much smaller gradient norm at the
beginning compared to standard training. This mainly comes from the difference in their landscapes.
Since there is no signal term in the variance learning, We can regard the landscape corresponding
to the variance training as just a translation transformation of the original landscape (Figure 1a, the
blue line is just a translation transformation of the green line). Based on this difference, they pursue
different optima. We denote θv , θ the empirical loss minimizer, respectively. Due to concentration,
we have θv ≈ 0, and θ ≈ θ?. Note that we use a near-zero initialization θ(0) = θV0) ≈ 0, we
immediately have that θV0) ≈ θv, indicating that the gradient norm is small. On the other hand, since
θ (0) is far away from the optimum, its initial gradient norm can be very large.
To better illustrate this principle, we use linear regression to provide a simple example. For the
standard training, the empirical loss is Ln(θ; P) = 1 PZi (h0, Xii - hθ?, Xii - εi)2. Hence, we
10We do observe that for certain learning rate regimes, the algorithm will overfit the training dataset for
variance training, leading to a large VER (〜10).
15
Published as a conference paper at ICLR 2022
have
*sμ -8μ-dE①
Figure 5: Three excess risk dynamics for CIFAR-10.
2n
▽Ln(θ(O); P) = n X ((θ(O) ,X)-hθ?,Xii -ε) Xi
i=1
2n
=-n ɪ2 (hθ , Xii + εi) Xi ≈ -2θ ,
(10)
if We assume Xi 〜N (0, IdXd). On the other hand, for the variance training, the empirical loss is
Ln(θ; Pv) = 1 Pn=1 (hθ,Xii — εi)2. Therefore, We have 2
2n	2n
▽Ln(θ(0) ； Pv) = η X ('F, XH Xi = - - X εiXi ≈ 0.
i=1	i=1
(11)
In conclusion, We have
(▽Ln(θ⑼；P)∣∣ ≈ 2 kθ?k,卜Ln(θv0);Pv)∣∣ ≈ 0.	(12)
Obviously, there is a huge initial gradient norm gap betWeen standard and variance training, shoWing
that our bound is significantly better than the direct stability-based bound under such regimes.
B.2 Figure 1b
Figure 1b provides an example shoWing that the trajectory experienced by the variance training is
likely to stay a convex region, though the global landscape can be highly non-smooth and non-convex.
The reason is similar to that of Figure 1a. Since θv0) ≈ θv, the gradient norm is small and the training
dynamics may stay close to the optimum, Which lies on a smooth and convex region. On the contrary,
the standard training needs to cross potentially many bad local minimum or saddle points, suffering
from large cumulative gradient norms.
B.3 Figure 2
The setting of Figure 2 is similar to that of Section A. For MINIST dataset, We use three layer
convoluted neural netWork With Adam optimizer.
16
Published as a conference paper at ICLR 2022
B.4 FIGURE 3
Figure 3 provides the numerical verification of Dynamics Decomposition Condition, and our upper
bounds for both linear regression and matrix recovery problems 11.
Linear Regression We set the dimension d = 500, the sample sizes are n = 300, 800, respectively.
Samples xι …，Xn are i.i.d. from standard Gaussian distribution N (0, Id×d), and the output
yi = hχi, θ*i + εi where εi 〜N(0, σ2) and σ = 2. We set the initialization θ(0) = θ(0) = θV0) = 0
and the learning rate λ = 0.01.
Matrix Recovery We set the dimension d = 20, rank r = 3, the ground truth X? = V ΣV >, where
Σ = Diag{5,3,1,0,…，0} and V is an orthonormal matrix. The numbers of the measurement
matrices are 200, 600, respectively. The measurement matrices Ai,…，An are i.i.d. standard
Gaussian matrices. The corresponding measurement yi =(Ai, X?)+ εi, where ε% 〜N(0,1). In this
numerical experiment, we use the initialization U(0) = Ub(0) = Uv(0) = αId×d where α = 0.01.
We solve this problem via gradient descent with constant stepsize η = 0.1.
For the upper bound, it is calculated via upper bound = ∣∣Xb 一 X?|工 + ∣∣XV ∣∣f + √n, where the
last term is corresponding to the additional term in Lemma 4.
Neural Network We the dimension d = 30, the ground truth function is f?(x) = hθ?, xi where
θ? is sparse and ∣θ*∣ = Θ(1). The covariate X is generated from Gaussian distribution. The label
for bias training is exactly yi = f?(xi). For the variance trainng, its label is generated from an
additional Gaussian distribution εi 〜N(0, σ2) where σ = 1.5. For the standard training, its label is
yi = f?(xi) + εi. For the upper bound, it is calculated via upper bound = ∣∣fθb 一 f?∣1 + ∣∣fθv ∣∣p.
Here ∣∣∙∣p is defined to be the L2(P) norm, i.e., ∣g∣p = Ex〜P [f(x)2]. For these settings, we
choose SGD (with stepsize η = 1 × 10-3) as the optimizer where the initialization is randomly
generated from a Gaussian distribution.
C Proof for Overparameterized Linear Regression
C.1 Proof of Theorem 1
In this section, we prove Theorem 1 stated as follows:
Theorem 1 (Linear Regression Regimes) Under the overparameterized linear regression settings,
assume that W = / θ*.>x = is σW-Subgaussian. If ∣∣x∣ ≤ 1,同 ≤ V, supt∈[τ] ∣∣θVt'k ≤ B are all
ʌ/ θ*,>Σχθ*	L」
bounded, the following inequality holds with probability at least 1 一 δ:
El®T)； P) = O (max n1, θ*,>∑xθ*σW, [V + B^o /^ + 整 + T≡^
w	n	λT	n
where the probability is taken over the randomness of training data.
Proof:
11Here, we extend the diagonal matrix recovery problem to the general matrix recovery problem.
17
Published as a conference paper at ICLR 2022
We first calculate the population loss under linear regression regime, namely, for any time t:
L(θ(t); P) = Eχ,y [y - X>θ(t)i2
=) E [x>θ* + e - x>(9(t)i2
=E [x>θ* - x>θ㈤]2 - 2E }x>θ㈤i + Ee2
=[θ* - θ⑴i	> Σx [θ* - θ㈤i - 2ExEe [ex>θ㈤ |x] + Ee2
=[θ* - θ⑴i	> Σx [θ* - θ㈤i - 2Ex [x>θ⑴Ee [e∣x]i + Ee2
=[θ* - θ㈤i	> Σx [θ* - θ(t)i + Ee2,
where (a) We use the condition that y	= x>θ* + e and (b) We use the assumption that E[e∣x]	= 0.
Therefore, the excess risk can be written as
E(θ㈤)=[θ* - θ⑴i > Σx [θ* - θ⑴i .
NoW plugging the Lemma 1, Lemma 2, Lemma 3, into Theorem 2, We derive that With probability at
least 1 - δ:
El®T)； P) . EL(ΘVT)； P) + EL(θ(T)； P)
.θ^ σW SE +-Λ- |叫2
n w δ λ[2T + 1]
+ Tλ [V + B]2 log(n) log(2n∕δ) + max(1, [V + B]20 ∖Jog：40 .
n	2n
=O (max n1, θ*,>∑χθ*σW, [V + B『o r0^ + 穿 + Tλ^Vn±B2),
Which completes the proof.
C.2 Proof of Lemma 1
We first recall Lemma 1 as folloWs:
Lemma 1 Under the overparameterized linear regression settings with initialization θ(0) = θb(0) =
θv(0) = 0, for any time T, the following decomposition inequality holds:
EL (θ(T) ； P) ≤ 2EL (ΘVT) ； P) + 2EL (θ(T) ； P).
Proof: Firstly, When We use gradient descent, We have that:
θ(t+1) = θ(t) + ； X xM-x>θ叫
i
θVt+1) = θVt) + λ X χi[q-χ[θVt))],
v	vn	v
i
θ(t+1) = θ(t) + ： Xxi[x>θ*-x>θ(t)]∙
i
We next prove the conclusion by induction. Since We choose θ(0) = θv(0) = θb(0) = 0, the conclusion
holds at the initialization
θ(0) = θv(0) + θb(0) .
18
Published as a conference paper at ICLR 2022
Now assume that θ(t) = θ(t) + θ(t) holds at time t, We have
θ(t+1) = θ⑴ + n Xg"-x>θ⑴]
i
=春 + θ(t) + n XXihei + x>θ* - x> (春 + 呼)]
i
即 + λ X Xi[ei-x>
vn
i
θ(Vt+1) + θ(t+1).
θVt)]	+	θ(t) + λ X Xi[x>θ*-x>θ(t)]
i
In summary, We have: θ(t) = θf) + θ(t) holds for any time t. Besides, We easily calculate that
θ* = θV + θ* since θ* = θb = θ* and θ^ = 0.
Therefore, we have θ(t) - θ? = θVt) - θ? + θ(t) - θ? holds for any t. By Cauchy-Schwarz inequality,
We derive that for a specific time T :
El(Θ(T); P)
=[θ*- θ(T)i> Σχ[θ*- θ(T)i
=m*-θ(T) b
=M - θv )+θb - θ(T )b
≤)2 归-就 )B∑x+2 归-θ(T )B∑x
≤2EL 阳); P ) + 2EL (θ(T); P).
where we denote the norm kuk2A = u>Au.

C.3 Proof of Lemma 2
Lemma 2 Under the overparameterized linear regression settings, assume that kxk ≤ 1, || ≤ V,
SuPtk θ(t) k ≤ B are all bounded. Consider the gradient descent algorithm with constant stepsize λ,
with probability at least 1 - δ:
EL 鸣); P) = O (T [V + B]2 log(n)log(2n∕δ)+max {l, [V + B]2} JT2") . (2)
Proof: When the context is clear, we omit v in this section and denote the trained parameter by 0(t).
For any time t, we first split the excess risk into three components following Equation 1.
El(Θ”; Pv) = [L(θ(t); Pv) -L(θ(t); Pn)] + [L(θ(t); Pn)-L(θ*; P：)] +[L(θ*; Pn) -L(θ*; PlI)].
L	」..L	」.X------------------}
^{^^^^^^^^~
(III)
(13)
'----------------------{-----------------------} '------------------------{----------------------}
(I)	(II)
Part (I). We bound the first component in Equation 13 using stability.
Fact A: the loss `(θ; x, y) is 2 [V + B]-Lipschitz.
kVθ'(θ; X, y)k = 2kx(e - x>θ)k ≤ 2[V + B].
Fact B: the stability of the parameter is 2λt [v + b]. Denote the parameter trained on dataset S and
the parameter trained on dataset S0 as θg) and。弋),respectively. We will show that, when S and S0
19
Published as a conference paper at ICLR 2022
have only one different sample (e.g., WLOG S = {xι, x2,…,Xn} and S0 = {x1, x2,…,Xn}),
θ(t) is close to θS).
θ*1) - θ(t+1)
≤MI-λX>x
陷 + λx>Y-
I - λX0,>X0
n
≤WI-λX>x
k + k ]λX0,>X0 - λX>xl θ(to)k
+ λkX>Y - X0,>Y0
n
—
n
n
≤ ∣∣hθSt)- θSt0)i Il + λ Il hχιχ> - x1x>,0i θSt0)∣∣ + λ ∣∣x>ει - xι>ε1
≤∣∣[θ(t)-θSo)i∣∣ + 2λ [V+B ].
Summation over time t provides the following result:
kθ(t) - θSo)k≤ W [V+B].
Therefore, the total stability with respect to the '2-loss is
i`(θ 给一'(θS))∣≤ W [V+B]2.
By Proposition 1, we have that
L(θ⑴;Pv) -L(θ⑴;Pn) . tλ [V + B]2 log(n)log(2n∕δ) +、/"以10 .
vn	n
Part II. We next show that the second component in Equation 13 is less than zero based on Lemma 7.
l(θ㈤;Pn) - l(θ* ； Pn)
=1 [ke- Xθ㈤k2-kd∣2i
=1 β>
n
-2X
I-
I - λX>X_I 1 Xt + Xt,>
I-
I -λX>x_ 1 X>x
I-
I -λX>X
n
t
Xt
.
The i-th eigenvalue of the matrix [-2X [I - [I - λX>X]t]Xt + Xt,>[I - [I - nnX>X]t]X>X [I -
[I - nX>X]t]Xt] can be calculated as follows
-2 1- (1-λσi)t + 1- (1-λσi)t2 = (1 - λσi)2t -1≤ 0,
where σi refers to the i-th eigenvalue of the matrix nnX>X. Therefore, We have
l(θ ⑴;Pn) -l(θ*; Pn) ≤ 0.
Part III. We bound the third component in Equation 13 using concentration.
Note that ∣'2(θ; x, y)| = |(y - x>θ)21 ≤ [V + B]2, so we can use Hoeffding,s inequality,
P[∣L(Θ*;Pn) -L(Θ*;Pv)| ≥ u] ≤ 2exp -
2nu2
(B + V)4_ .
By setting u = [V + B]2
io log(n∕6, then with probability at least 1 - δ, we have
lL(θ*;Pn)-L(θ*; Pv )l≤ [V+B]2 7ɪogp.
20
Published as a conference paper at ICLR 2022
Combining Part (I), (II), and (III) together, at time T , we have:
El(Θ(T); Pv) < Tλ [V + B]2 log(n) log(2n∕δ) + 卜⑴/δ + [V + B]2 卜:⑼
n	n	2n
Tλ
一 [V + B]2log(n) log(2n∕δ) 十
n
max{1,[V + B]2 } Jlogg∕δ).

C.4 Proof of Lemma 3
Lemma 3 Under the overparameterized linear regression settings, assume that w
σw2 -subgaussian, then with probability at least 1 - δ:
θ*,>x	is
√θ*,>Σχθ*
EL WT) ； P) = O (σw ZogB+λ[2T1+H kθ*k2).	⑶
Proof: Since We use the noiseless data (x, E(y∣x)) and E(y∣x) = xτθ*, by Lemma 7 We have
θ(t) = I - Ii - AXTXI	xtxθ* = θ*
bn
I-
I - λXτxIt
n
θ*,
where we use the fact that [l - [I - nXτX]t] Xtχ = I - [I - nXτX]t.
Therefore, BER can be expressed as:
EL(θ; Pn) = [θ* - θ(t)iT ∑x [θ* - θ(t)i
=θ*,τ	I-	λX τX n	t	Σx	I	-λXτX n		t θ*			
=θ*,τ	I-	λXτX n	t	Σx	-	1X τX n		I-	λXτX n		t θ*
+θ*,τ	I-	AX τX n	t	1 X T X n		XI-	λX T n		X	t θ*.	
We split the excess risk into two parts. Intuitively, it is just like the decomposition in Equation 1. The
first part is similar to the generalization gap to use uniform convergence to bound it. Note that here
we require that λ < supX
k ι Xo>χok.
Hence, we have
1
21
Published as a conference paper at ICLR 2022
t
t
<
<
<
<
θ*,τ
I--XτX
n
sup θ*,τ
X0
ςx - nχTX"∣	]i
i---X0TX0
n
suptr θ*,τ
X0	∖
sup tr
Xo
sup
X0
sup
Xo
sup
Xo
-X τX	θ*
-XτX I - -X0τX0	θ*
I---X0τX0
n
I - -X0τX0
n
I---X0τX0
n
I---X0τX0
n
Σx - -Xτxl [i - -X0τX0	θ*
Σx - -XτX I - -X0τX0	θ*θ*
Σx - -X>X I - -X0τX0	θ*θ*
,丁
,丁
-XτX I - -X0τX0	θ*θ*
Σx - -XτX I - -X0τX0	θ*θ*
,τ
,τ
sup tr ( θ*,τ
X0	∖
Σx - -Xτx] [i - -X0τX0	θ*
sup tr ( θ*,τ
X0	∖
Σx - -XτX θ*
n
n
t
t
t
t
t
—
n
n
t
t
1
n
—
n
n
t
n
n
n
n
n
λ
n
t
t
n
n
t
t
n
θ*,τ Σx - -XτX θ*
n
where We apply Lemma 8 repeatedly since rank [θ*,τθ*A] = 1 where A stands for arbitrary matrix.
We highlight the difference between X。and X. The notation X represents the contribution of the
training set to the training error. The notation X0 represents the contribution of the training set to the
training estimator θ. The uniform convergence is taken over the estimator θ. Therefore, we take sup
operator on X0.
We next bound θ*,τ [Σx - 1XτX] θ*. Assume that w = θ*,τx/pθ*,τΣxθ* is SubGaussian
with subGaussian norm σw. Obviously, E [w] = 0, E [w2] = L Therefore, we have that: w2 - - is
sub-Exponential distributions with sub-Exponential normM--晨 < σw.
While e/ [θτΣxθ] < σ22, we have the following tail bound
P ∣θ*,τ
∑x - -XτX
n
θ*∣≥e) = P (∣ - X [(xjθ)2- θτ∑χθ]∣≥E
=P (∣- X [w2 - 1] ∣ ≥ E/[θτ∑χθ]
[e∕[θτ∑xθ]]2-
< 2 exp
-c
nσW
where C is a universal constant.
Therefore, by setting E = [θτ∑xθ]
σW Iog[2∕δ]
√c√n
with probability at least - - δ, we have
θ*,τ
∑x - -Xτχ
n
θ* < [θτ∑χθ]
σW Vzlog[2∕δ]
√c√n
22
Published as a conference paper at ICLR 2022
With abuse to use c as a constant, we have with probability at least ≥ 1 - δ
θ*,> ∣Σx - 1X>X] θ* ≤ C [θ>Σxθ] σw Vzlog(20 .
n	n
For the second part, which is closely related to the empirical loss of θb(t), We will show
that: for a general case, the empirical loss converges with rate O(1/t). Denote the eigen-
values of §X>X as σ1, ∙∙∙ ,σn, 0,…,0. Then the corresponding eigenvalues of matrix
[I - λX>X]t [nχ>x] [I - nχ>x]t are (1 -也)2t σi . Therefore, we have:
θ*,> II - AX>xl [1 x>x] ∣I - λx>xl θ*
nn	n
≤ kθ* k2 max(1 - λσi)2t σi
i
≤ΛW1) kθ*k2.
The maximum is attained while σi
1
λ(2t+1).
Combining these terms leads to the conclusion.

C.5 Auxiliary Lemmas
Lemma 7 The dynamics of θ(t) using GD is:
θ(t)
I -λX>X
n
t
θ(0) - X*Y + X*Y.
where X * represents the pseudo inverse of matrix X.
Proof: The proof is based on induction. We notice that when t = 0, the equation directly follows.
Assume when t = t, the equation holds. Then at time t +1, we have
0(t+1) = θ⑴ + λX>(Y - Xθ⑴)
n
ι- AX>x
n
I -λX>X
n
+ x *y + AX >y
n
I -λX>X
n
I-λX>X
n
t+1
t+1
[θ(O)- Xtγ]
+ ι - AX>x x*y + AX>y
[θ⑼-Xtγ] + XtY,
t
—
n
λ
n
where we use the fact that X>XXt = X>.

Lemma 8 For rank-1 matrix A, we have that kAk = |tr [A]|.
D Proof of Theorem 2
In this section, we provide the proof of Theorem 2. We first recall the formal statement of Theorem 2.
Theorem 2	(Decomposition Theorem) Assume that the excess risk has a unique minimizer with
local sharpness factor s. Under Assumption 1 and Assumption 2, the following decomposition
inequality holds:
El(Θ(T) ； P) ≤ [4a]s M [el (ΘVt ); P)+ EL (θ(T); P)i + Mu [ √ ]+ MU
s
.	(5)
23
Published as a conference paper at ICLR 2022
Proof: Under Assumption 1 (upper bound), we have:
ElW; P) = EL®" ;P' kθ(t) — θ*ks
kθ(t)- θ*∣∣s
≤ [M∕kθ⑴-θ*k]s.
Combining Assumption 2 (DDC) and the lower bound in Assumption 1, we have
kθ(t) - θ*k
≤akθ(t)-依k+akθvt)- θvk + √t + √n
≤[ J1"SaEL/s(θ(t); Pb) + [J]1∕saEL∕s(θvt); Pv) + -C + √,
mu	mu	t n
where we remark that Assumption 1 holds uniformly on distribution Py|x and θ. Therefore, we have
that
Ec(θ(t)；P) ≤
M 1/SaEL/SM)； Pb) + M 1∕saEL∕s(θvt); Pv) + √ +
≤ 4s
"W Pb) + MasEL(θvt); Pv) +
C0 s
√nJ
≤ [4a]s M hEL(θ(t); Pb) + El* Pv)] + MU
where we use the fact that (a + b + c + d)p ≤ max{(4a)p, (4b)p, (4c)p, (4d)p} ≤ (4a)p + (4b)p +
(4c)p + (4d)p when a, b, c, d, p > 0.

E Proof for Diagonal Matrix Recovery
In this section, we prove Theorem 3 via proving the proceeding three lemmas separately.
E.1 Proof of Theorem 3
Theorem 3	(Decomposition Theorem in Diagonal Matrix Recovery) In the diagonal matrix re-
covery settings, if we set the initialization U0 = αI where α > 0 is the initialization scale, then for
any 0 < t < ∞, with probability at least 1 - δ, we have:
EL(Ut； P) < (kX?kF + dV2 + dα4)/萼) + XX 2+ ""st) + α2d
n	n "σ σ2 + a4e"(bjt)	t
j=1 j	(6)
+ dV2" log(n) log (亭)+ dɑ2 + log2 (3)TrV "⑷.
n	δ	α2	n
Note that although we do optimization based on U, we consider its square X = UU> as the parameter
since we require the solution is unique during the analysis. Note that we do not change the update
rule (where GD is still taken on U). One can directly check that under Diagonal Matrix Recovery
regimes, s = 2 and Mu = mu = 1. Therefore, combining Lemma 4, Lemma 5 and Lemma 6 leads
to Theorem 3.
E.2 Proof of Lemma 4
Lemma 4 Under the assumptions in Theorem 3, for any 0 ≤ t < ∞, with probability at least 1 - δ,
we have
uUtU>- X *∣∣f ≤ IlUbUtb,> - X *i∣f+W UV,>∣∣F+O flog (α) r rν2lon(r∕δ)).⑺
24
Published as a conference paper at ICLR 2022
Proof: Since all the measurement matrices A(I),…，A(n) are diagonal, We can analyze the dynamic
of each singular value separately.
For a given singular value σ, assume a% 〜 N(0,1), ε 〜 SubG(V2). Then we define σι =
n Pn=I a2σ, and σ2 = 1 Pn==1 qi£i, denote σ = σι + σ2, and ξ = 1 Pn==1 a2. Denote u2(t),
ub2(t), u2v(t) the dynamics of standard training, bias training, and variance training, respectively. Then
our goal is to show that ∣σ - u2 (t) | ≤ ∣σ - u2 (t) | + U (t) + small term holds with high probability.
Before diving into the details, we first give the dynamics of u2(t), ub2(t), uv2 (t). If σ > 0, we have
Furthermore, if σ2 > 0, we have
Otherwise, if σ2 < 0 we have
If σ? = 0, we have
u2 (t)
ub (t) =
u2v(t)
σα2 e2σt
σ - ξα2 + ξα2 e2σt
σια2e2σ1t
σι — ξɑ2 + ξα2e2σ1t
σ2α2e2σ2t
σ2 — ξα2 + ξα2e2σ2t
2 ⑺=__________M2|a2_________
V( ) = (∣σ2∣+ ξα2) e2S2∣t - ξα2 .
2
Ub (t) = ξα2t + 1.
Now we turn to the proof of Lemma 4. If σ = 0, then σ = σ2, which implies u2 (t) = u2v(t),
indicating that the conclusion holds.
Hence, we only consider the case that σ ≥ σr > 0. By triangle inequality, it suffices to show
∣u2(t)- u2(t)∣ = O (lσ2l).
Without loss of generality, we assume σ > σ1 > 0. Note that
∣u2 (t) - ub2(t)∣
σα2e2σt	σ1α2e2σ1t
-------Z---- - -------Z------
σ + ξα2 e2σt σ1 + ξα2e2σ1t
+ O(∣σ2∣)
α2σσι (e2σt - e2σ1t)
(σ + ξα2e2σt)(σι + ξα2e2σ1t)+002 .
Via integration-by-parts formula, we have
e2σt
e2σ1 t
2σ2e2σt
Zt
0
e-2σ2sds ≤ 2σ2te2σt.
Therefore, it suffices to show that
σσ1α2te2σt
(σ + ξα2e2σt) (σ1 + ξα2e2σ1t)
O(1).
We prove that there exists a universal constant C, such that
σσια2te2σt
(σ + ξα2e2σt)(σι + ξɑ2e2σ1t)—，
which is equivalent to show
φ(t) = C (σ + ξɑ2e2σt) (σι + ξɑ2e2σ1t) — σσια2te2σt ≥ 0, ∀0 ≤ t < ∞.
Note that φ(0) = C (σ + ξα2) (σι + ξα2) ≥ 0, and its gradient is
φ0(t) = 2Cξ2α4(σ + σι)e2(σ+σ1)t + 2Cσσιξα2 (e2σt + e2σ1t) - σσιa2e2σt - 2σ2σa2te2σt.
25
Published as a conference paper at ICLR 2022
By concentration of Chi-square distribution, We have ξ ≥ 1 - lolog(yδ) = Ω(1) with probability at
least 1 - δ, providing that n & log (∣). It suffices to show that φ0(t) ≥ 0, which is equivalent to
ψ(t) = Cα2e2σ1t + Cσ1 - σσ1t ≥ 0.
It attains its minimum 2 + Cσ1 - 2 log (2⅛2) at the point t = log (2⅛2) /2σ1. Therefore, it
suffices to set C = log (α⅛ )∙
Overall, we have
∣u2(t) - u2(t)∣ = O (log α12 ∣σ2∣)	if σ? > 0;
u2(t) - ub2 (t) ≤ uv2 (t) if σ? = 0.
Hence, with proper choice of probability, we have with probability at least 1 - δ, the following holds
kXt -X?kF ≤ "f -X*∣∣F+kXvkF+O (log(α12)尸on且).
To summarize, the case is (a = 1,C = 0,C0 = O (log (去)prσ2 log (r∕δ)))-bounded (See
DDC).
E.3 Proof of Lemma 5
Lemma 5 For VER, under the assumptions in Theorem 3, with probability at least 1 - δ, we have
EL(Ut; P) < dV2(t + 1) log(n) log (2dn) + dɑ2 + dV2 Jl*").
nδ	n
(8)
Proof:
Similar to the proof in overparameterized linear regression regimes, we first split the excess risk in
three componenets:
EL(Ut； Pv) = L (u2(t)) - Ln (u2(t)) + Ln (u2(t)) - Ln (0) + Ln (0) - L (0).	(14)
、---------{z--------------} 、---------{z-------} '---------{---------}
(I)	(I)	(III)
Part (I).	We bound the first component in Equation 13 using stability. We define σ2 = n Pin=1 aiεi,
σ2 = n Pn-IIaiεi+ an*. Similarly,we define ξ = n Pn=I a2, and ξ0 = n. Pn-IIa2 + a2.
In the first case, we assume both σ2, σ20 > 0, so that their difference can be bounded
|uv2(t)-u0v2(t)|<
∣∣α2σ2σ20 e2σ2t - e2σ20t∣∣ +α4 ∣∣ξ0σ2e2σ2t -ξσ20e2σ20t∣∣ +α4(σ2ξ0 - σ20ξ)e2(σ2+σ20)t
(σ2 — ξɑ2 + ξα2e2σ2t) (σ2 — ξ0α2 + ξ0α2e2σ2t
WLOG, we assume σ2 > σ20 . Then, similar to the proof in Lemma 4, we have
∣∣∣α2σ2σ20 e2σ2t -e2σ20t∣∣∣
(σ2 — ξɑ2 + ξα2e2σ2t) (σ2 — ξ0α2 + ξ0α2e2σ2t)	。,"2	QeI".
The second term is dominated by the first term. Hence, we only need to handle the third term. Note
that
lσ2ξ0 - σ2ξl =	lσ2	(ξ0	-	ξ)+ ξ(σ2	-	σ2)|	≤	∣an	- an	I + ~	(anεn - an^n)，
n n n ∣n	nn ∣
we have
α4(σ2ξ0- σ ξ)e2s2+σ2 *___________≤ ∣σ2ξ0 - σ ξ∣
(σ2 — ξɑ2 + ξα2e2σ2t) (σ2 — ξ0ɑ2 + ξ0α2e2σ2t) -	ξξ0
< ^-2^ Ian - an I + - (anεn - aMn)
n n n ∣n	nn
26
Published as a conference paper at ICLR 2022
Combined the above three parts, we finally have
|u2v (t) - u0v2 (t)| = O
In the second case, we assume both σ2 , σ20 < 0. Since both of them would decrease to 0, we have
∣u2(t) - ub2(t)l ≤ α2 where α = O(n14).
In the last case, without loss of generality, we assume σ2 < 0 < σ20 . Since uv2 (t) decreases to 0, and
u02 (t) increases to 必,we have ∣u2(t) — u02(t)∣ ≤ 必 ≤ ∣σ2 一 σ2∣.
Overall, we have
∣u2 (t)-uv2(t)∣ = o (V(tn+1)).
Besides, since the loss is O(V)-Lipschitz:
INu2'(U)II= 2 (au2 - ε) a . V.
where u2 is bounded upper bounded by O(V) during the training analysis due to a small initialization.
Then the total stability is
∣'(u2(t)) -'(uv2(t))∣≤ V2f+1)
n
Summation over the dimension d, the whole loss 'd(u2 (t)) has stability
∣'d(u*t))-'d(uV2(t))∣≤ dV2(t+1).
n
By Proposition 1, we have that
L(θVt); P) - L(θ化 Pn) < dV2(t +1) log(n) log(2dn∕δ) + 'logH0 .
vv	n	n
Part (II).	We next calculate the second term. Take the explicit formula into the equation, we have
1n
Ln (U ⑴)-Ln (O) = ~ ɪ2 (ai UV ⑴一2aiεiuv ⑴).
n i=1
If σ2 = n pn=1 a%εi > 0, We have
1n 24	2
—E (a2u4 (t) - 2aiεiU2 (t))
i=1
(σ2 — ξα2 )σ2 α2e2σ2t
(σ2 — ξα2 + ξα2e2σ2t)2
≤ 0.
If σ2 < 0, we have
1n
—JS (aiUV⑴-2aiεiuV⑴)
— i=1
ξ____________σ2α4_____________ + ________2匕2|2 α2_________
((∣σ2∣ + ξɑ2)e2lσ2lt -ξɑ2)2	(修2| + ξα2)e2lσ2lt - ξα2
≤ ξα4+2lσ2lα2 < α2 匕
In conclusion, We have Ln (u2(t)) - Ln (O) < α2√ V
Part (III). For the third term, we just need one single concentration inequality. Note that for the
variance training, the loss function is of the order O (V2). Hence, via the Hoeffding inequality, with
probability at least 1 - δ, we have
Ln (O)-L(O) < Vrog1M.
—
Combining the three terms leads to Lemma 5.

27
Published as a conference paper at ICLR 2022
E.4 Proof of Lemma 6
Lemma 6 For BER, under the assumptions in Theorem 3, with probability at least 1 - δ, we have
EL(Ut; P) . (kX?kF + dα4) J
log (d∕δ)	XX	σj4	a2d
n +j=1 σ2 + α4 eκ°j + T
(9)
Proof:
Similar to linear regression, we decompose the excess risk into two parts:
i=1
n
(u2 ⑴一σ)2 + n X ai (u2 ⑴一σ)2.
i=1
We use the uniform convergence to bound the first term. Assume that σ > 0, we have
σ - ub (t) = σ -
σ1α2e2σ1t
σι — ξɑ2 + ξα2e2σ1t
σσι — ξα2σ + α2e2σ1t (ξσ — σ)
σι — ξɑ2 + ξα2e2σ1t
σσ1 — ξα2σ
σι — ξɑ2 + ξα2e2σ1t
σ2 — α2σ
σ — α2 + α2e2ξσt
≤ σ(1 — α2 ) ≤ σ.
Now assume σ = 0, we have
2
u2(t) = ξOE ≤ α2.
The second term refers to the training loss, if σ > 0, we have
1 G 2( 2f ∖2	(σ2 — α2σ)2
n X ai (ub ( ) - σ) = ξ (σ — α2 + α2e2ξσt)2
σ4
σ2 + α4eQSt)
If σ = 0, we have
n X a(…丫=ξ (ξ02⅛.	.
Finally, by assigning probability properly, we have
EL(Ut； Pb).(\\X ?kF+d04) r log，/6+X + ：：&(j+αtd.
j=1 j
F Additional Discussion
In this section, we discuss more the details in the main text. We first show a case where our bound
outperforms previous stability-based bound under linear regimes. We then validate the rationality
of applying stability-based bound by showing that the generalization gap (under variance regime)
increases with time. We next provide some motivation behind Dynamics Decomposition Condition
by showing that it indeed holds as the beginning of the training phase. We finally introduce a related
version of Dynamics Decomposition Condition which does not require the information about θ*, ΘV,
and θ*.
28
Published as a conference paper at ICLR 2022
F.1 Comparison to previous stability-based bounds under linear regimes
One of the shortcomings of the stability-based bound is its failure with a large time T . Our decompo-
sition framework can aid in reducing the failure at a large time T . For example, when considering a
large time T = n3/4 with λ = 1, the newly-proposed decomposition bound is
O(n-1/4[V + B]2).
As a comparison, the original stability-based bound is
O(n-1/4[V + B0]2).
We next show that B < B0 holds with a large signal-to-noise ratio.
We write the above formula as
kθVt)k = B<B0= kθ(t)k.
From the formulation of the trained parameter, We have θVt) = [I - [I - λX>χ]t]χte and
θ(t) = [I - [I - nX>X]t]XtY = [I - [I - λX>X]t]θ* + θVt). Therefore, it suffices to show that
k[I-[I- λX>X]t]θ*k > 2∣∣θVt)k.
Note that the first part is only related to signal θ* and the second part is related to noise e (by applying
concentration, the second part is closely related to the noise level). Therefore, informally, with a large
signal-to-noise ratio, the last equation ∣∣[I - [I - λX>X]t]θ*k > 2∣∣θVt)k holds. To summary, it
holds that B < B0 and our bound outperforms the onriginal stability-based bounds.
F.2 The generalization gap under variance increases with time
In this part, we will prove that the (variance regime) generalization gap indeed increases with time,
which validates that applying stability-based bound (which also increases with time) is rational.
We provide the following Lemma 9 which validates the above statement.
Lemma 9 Let ∆v(t) = L(θVt); P) - L(θVt); Pn) be the generalization gap under variance regime.
Assume that Σx = I. Then under linear regression settings as in Section 3, we have
∂
∂t Z ⑴ ≥ 0.
Proof: Note that the population loss is calculated as follows by Equation C.1
L(θVt); P) = [θV[> Σχ [θV[ + Ee2.
And the training loss is calculated as follows:
L(θVt); Pn) = 1 ∣∣E - XθVt) k2
n
=1 ∣XθVt)k2 - 2E> [xθVt)i +1 ∣e∣2	.
nV n	V n
=[θVt)i > 1 X>X [θVt)i - nE>XθVt) + n∣Ek2
Note that by Lemma 7, we have
29
Published as a conference paper at ICLR 2022
Therefore, We have:
∆v ⑴=L(θ 化 P )-L@% Pn)
=W)J Σχ [θVt)] + Ee2 - [θVt)]τ 1XTX W)] + nETXM- 1 ||研2
=[θvt)]τ [∑x - 1Xτx][。叫 + 2EτXθVt) + Ee2 - 11EI2
L J |_ n _|L Jn	n
=Eτ IX"τ I - [ʃ - AXτX
n
+ 2ETX I - Il - λXτX 1 IXtE] + Ee2 - 1 ∣∣E∣2.
nn	n
By omitting the terms in ∆^(t) that is uncorrelated to t, we have (denote as ∆V(t)):
IXt]τ Ii- AXτX 1 ∣∑χ -1XτX 1 Ii- AXτX
n	n	n
-	AXτX [XtE]
-	1X τX 1 Ii - ax tx 1 [X tE]
nn
t
[XtE]
-	1 xτx - 2∑x Ii - AXTx [xtE].
-2Eτ [Xt]τ ∑χ - 1Xτx] ∖l
--EτX Ii - AXτX 1 [XtE]
nn
=Eτ [Xt]τ Ii- AXτX 1 ∣∑χ
n
-2Eτ [Xt]τ[∑χ] I - AXτX
n
=ET [x t]τ[Iι- AX TX j]∑
where we use the fact that [Xt]τ XτX = X.
We next focus on the matrix
Ma = [Xt]T	[1 - nxTX
I - 1Xτx
n
where we plug in ∑x = I by assumption. Denote the SVD of 1XτX as Uτ∑U where the i-th
eigenvalue is σi. Then when σi = 0, the i - th eigenvalue of Ma is also equal to zero (due to the
persudo inverse Xt). When σ% > 0, the i - th eigenvalue of Ma can be derived as
σ,(t; Ma) = J≡=H3
σi
Its derivation is then
第σi(t; Ma)=么1-®)'10©1-®)[(1 - %)(1 -也)'-1] ≥ 0,	(15)
∂t	σi
since 1og(1 - Aσi) ≤ 0 and (1 - σi)(1 - Aσi)t - 1 ≤ 0. Therefore, the derivation of each eigenvalue
is larger than zero.
We rewrite ∆V (t) = zτ∑MazT where ZT = UE is a vector independent of time t and ∑Ma is the
diagonal matrix with diagonal σi(Ma). As a result, we have
∂ -	Td	丁
∂t△V(t) = z (沅∑Ma)Z ≥0,
since 等 ∑Ma 占 0 according to Equation 15.
30
Published as a conference paper at ICLR 2022
Note that ∆V (t) and ∆v (t) only differ with a term independent of time, We have
∂∂
∂t δv ⑴=∂t δV (t) ≥ 0.
F.3 Motivation behind Dynamics Decomposition Condition
For a dataset S = {(Xi, yi)}n=ι, where yi = fθ? (Xi) + 已％, and θ? ∈ θ. We choose the '2-loss
to solve this problem L(θ) = 2n Pn=ι (fθ(Xi) - yi)2. Then we can show that the Dynamics
Decomposition Condition holds at the beginning phase if we initialize θV(0) = 0, and assume
f0(X) = 0, ∀X ∈ supp{P}. For sufficient small t > 0, the dynamics are well approximated by
1n
θ(t) = θ(O)- - fθ( (fθ(O)(Xi)- yi) ▽%(O)(Xi)t;
n i=1
n
θ(t) = θ(0) - - X (fθ(0) (Xi) - fθ? (Xi)) Vfθ(0) (Xi)t;	(16)
n i=1 b	b
n
θVt) = θV0)- n X (/eV。)(Xi)- εi) vfθV0) (XiK
n i=1
Suppose θ(0) = θb(0), then based on Taylor expansion, we have the approximation
n
θ㈤ ≈ θ(t) + θVt) + - X εiV2fθ(0) (Xi) (θV0) - θ⑼)t ≈ θ(t) + θVt).	(17)
n i=1
Therefore, the Dynamics Decomposition Condition holds at least at the beginning phase. Moreover,
we discuss in the main text that for a branch of learning tasks, such as linear regression and matrix
recovery, these Dynamics Decomposition Condition uniformly holds for arbitrary 0 ≤ t < ∞.
F.4 Relaxed Version of DDC
One may notice that there exist θ*, θV, θb in DDC (See Assumption 2), which is the optimal solution
to the corresponding population loss. They are sometimes unavailable in practice and even have no
explicit solution. For more convenient analysis, we propose a relaxed version of DDC in Lemma 10.
Lemma 10 (Relaxed DDC) Let the loss be '2 loss where '(y, y) = (y — y)2. We assume the
predicting model y = ff^(x) IS unique With respect to θ (when θi = θj, fθi = fθj). Besides, assume
fθ=o(x) = 0 for all x. For the ground truth, we assume an additive noise, namely, y = fe* (x) + E
where is independent of x. Then the following Equation 18 suffices to prove (max{a, -}, C, C0)-
boundedness for DDC (See Assumption 2).
kθ⑴-θ(t) k ≤ akθVt) k + √Ct + √Cn.	(18)
Based on Lemma 10, one can verify DDC without knowing the optimal solution parameter θ*, θV, θ^.
Besides, since we usually have the iteration equation (the relationship between ()(-(^+11 and 0(t)) given
an algorithm, it is possible to verify DDC based on the iteration forms. Note that we may not directly
verify the condition in practice based on the relaxed DDC if we do not have the explicit split on signal
E[y|x] and noise y - E[y|x].
31