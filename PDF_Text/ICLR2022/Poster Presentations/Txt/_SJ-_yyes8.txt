Under review as a conference paper at ICLR 2022
Mastering Visual Continuous Control:
Improved	Data-Augmented	Reinforcement
Learning
Anonymous authors
Paper Under doUble-blind reView
Abstract
We present DrQ-V2, a model-free reinforcement learning (RL) algorithm for VisUal
continUoUs control. DrQ-V2 bUilds on DrQ, an off-policy actor-critic approach
that Uses data aUgmentation to learn directly from pixels. We introdUce seVeral
improVements that yield state-of-the-art resUlts on the DeepMind Control SUite.
Notably, DrQ-V2 is able to solVe complex hUmanoid locomotion tasks directly from
pixel obserVations, preVioUsly Unattained by model-free RL. DrQ-V2 is conceptUally
simple, easy to implement, and proVides significantly better compUtational footprint
compared to prior work, with the majority of tasks taking jUst 8 hoUrs to train on a
single GPU. Finally, DrQ-V2’s implementation is pUblicly released to proVide RL
practitioners with a strong and compUtationally efficient baseline.
1 Introduction
Creating sample-efficient continUoUs control methods that obserVe high-dimensional images has
been a long standing challenge in reinforcement learning (RL) . OVer the last three years, the RL
commUnity has made significant headway on this problem, improVing sample-efficiency significantly.
The key insight to solVing VisUal control is the learning of better low-dimensional representations,
either throUgh aUtoencoders (Yarats et al., 2019; Finn et al., 2015), Variational inference (Hafner
et al., 2018; 2019; Lee et al., 2019), contrastiVe learning (SriniVas et al., 2020; Yarats et al., 2021a),
self-prediction (Schwarzer et al., 2020b), or data aUgmentations (Yarats et al., 2021b; Laskin et al.,
2020). HoweVer, cUrrent state-of-the-art model-free methods are still limited in three ways. First, they
are Unable to solVe the more challenging VisUal control problems sUch as qUadrUped and hUmanoid
locomotion. Second, they often reqUire significant compUtational resoUrces, i.e. lengthy training
times Using distribUted mUlti-GPU infrastrUctUre. Lastly, it is often Unclear how different design
choices affect oVerall system performance.
12 DMC Tasks (Samples)
UJn*jωcc3po-duj
Em3κ3po-d 山
12 DMC Tasks (lime)
Hours (for 3 × 10β Frames)
Em3κ3po-d 山
Em3e3po-d 山
Humanoid Walk (Time)
Hours (for 30 × IO8 Frames)
SAC CURL DrQ	DrQ-V2 (OUrs)
Figure 1: DrQ-v2 demonstrates significantly better sample efficiency and computational footprint
compared to state-of-the-art model-free methods for VisUal continUoUs control while being conceptU-
ally simple and easy to implement. (Left two) AVerage performance resUlts across 12 challenging
tasks from the DeepMind Control SUite (the set of tasks can be seen in FigUre 8). (Right two)
Performance on the Humanoid Walk task from VisUal inpUt, preVioUsly UnsolVed by model-free
methods. In both cases we report sample complexity and wall-clock time axes for eValUation, with
time being measUred on a single GPU machine and Using official implementations for each method.
1
Under review as a conference paper at ICLR 2022
In this paper we present DrQ-v2, a simple model-free algorithm that builds on the idea of using
data augmentations (Yarats et al., 2021b; Laskin et al., 2020) to solve hard visual control problems.
Most notably, it is the first model-free method that solves complex humanoid tasks directly from
pixels. Compared to previous state-of-the-art model-free methods, DrQ-v2 provides significant
improvements in sample efficiency across tasks from the DeepMind Control Suite (Tassa et al., 2018).
Conceptually simple, DrQ-v2 is also computationally efficient, which allows solving most tasks in
DeepMind Control Suite in just 8 hours on a single GPU (see Figure 1). Recently, a model-based
method, DreamerV2 (Hafner et al., 2020) was also shown to solve visual continuous control problems
and it was first to solve the humanoid locomotion problem from pixels. While our model-free
DrQ-v2 matches DreamerV2 in terms sample efficiency and performance, it does so 4× faster in
terms of wall-clock time to train. We believe this makes DrQ-v2 a more accessible approach to
support research in visual continuous control and it reinforces the question on whether model-free or
model-based is the more suitable approach to solve this type of tasks.
DrQ-v2, which is detailed in Section 3, improves upon DrQ (Yarats et al., 2021b) by making several
algorithmic changes: (i) switching the base RL algorithm from SAC (Haarnoja et al., 2018a) to
DDPG (Lillicrap et al., 2015a) with clipped double Q-learning from TD3 (Fujimoto et al., 2018), (ii)
this allows us straightforwardly incorporating multi-step return, (iii) adding bilinear interpolation to
the random shift image augmentation, (iv) introducing an exploration schedule, (v) selecting better
hyper-parameters including a larger capacity of the replay buffer. A careful ablation study of these
design choices is presented in Section 4.4. Furthermore, we re-examine the original implementation
of DrQ and identify several computational bottlenecks such as replay buffer management, data
augmentation processing, batch size, and frequency of learning updates (see Section 3.2). To remedy
these, we have developed a new implementation that both achieves better performance and trains
around 3.5 times faster with respect to wall-clock time than the previous implementation on the
same hardware with an increase in environment frame throughput (FPS) from 28 to 96 (i.e., it takes
106/96/3600 ≈ 2.9 hours to train for 1M environment steps). DrQ-v2’s implementation is available
at https://anonymous.4open.science/r/drqv2.
2	Background
2.1	Reinforcement Learning from Images
We formulate image-based control as an infinite-horizon Markov Decision Process (MDP) (Bellman,
1957). Generally, in such a setting, an image rendering of the system is not sufficient to perfectly
describe the system’s underlying state. To this end and per common practice (Mnih et al., 2013), we
approximate the current state of the system by stacking three consecutive prior observations. With
this in mind, such MDP can be described as a tuple (X , A,P,R,γ,d0), where X is the state space
(a three-stack of image observations), A is the action space, P : X×A→∆(X) is the transition
function1 that defines a probability distribution over the next state given the current state and action,
R : X×A→[0, 1] is the reward function, γ ∈ [0, 1) is a discount factor, and d0 ∈ ∆(X) is the
distribution of the initial state x0. The goal is to find a policy π : X→∆(A) that maximizes the
expected discounted sum of rewards En [P∞=o γtrt], where xo 〜do, and ∀t We have at 〜∏(∙∣xt),
Xt+1 〜P(∙∣xt, at), and r = R(xt, at).
2.2	Deep Deterministic Policy Gradient
Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015a) is an actor-critic algorithm for
continuous control that concurrently learns a Q-function Qθ and a deterministic policy πφ . For this,
DDPG uses Q-learning (Watkins and Dayan, 1992) to learn Qθ by minimizing the one-step Bellman
residual Jθ(D) = E(Xt,at,rt,χt+ι)〜d[(Qθ(xt, at) - r - YQo(Xt+ι, ∏φ(xt+1))2]. The policy ∏ is
learned by employing Deterministic Policy Gradient (DPG) (Silver et al., 2014) and maximizing
Jφ(D) = Ext〜d[Qθ(xt, ∏φ(xt))], so ∏φ(xt) approximates argmaxaQθ(xt, a), Here, D is a replay
buffer of environment transitions and G is an exponential moving average of the weights. DDPG is
amenable to incorporate n-step returns (Watkins, 1989; eng and Williams, 1996) when estimating
TD error beyond a single step (Barth-Maron et al., 2018). In practice, n-step returns allow for faster
1Here, ∆(X) denotes a distribution over the state space X.
2
Under review as a conference paper at ICLR 2022
Figure 2: (Left): DrQ-v2 is an off-policy actor-critic algorithm for image-based RL. It alleviates
encoder overfitting by applying random shift augmentation to pixel observations sampled from the
replay buffer. (Right): Examples of walking and standing behaviors learned by DrQ-v2 for a complex
humanoid agent from DMC (Tassa et al., 2018) with 21 and 54 dimensional action and state spaces,
respectively. DrQ-v2 does not have access to the internal state of the environment, only observing
three consecutive pixel frames at a time. Despite this imperfect observational channel, our agent still
manages to solve the tasks. To the best of our knowledge, this is the first successful demonstration by
a model-free method, using pixel-based inputs of these tasks.
reward propagation and has been previously used in policy gradient and Q-learning methods (Mnih
et al., 2016b; Barth-Maron et al., 2018; Hessel et al., 2017).
2.3	Data Augmentation in Reinforcement Learning
Recently, it has been shown that data augmentation techniques, commonplace in Computer Vision,
are also important for achieving the state-of-the-art performance in image-based RL (Yarats et al.,
2021b; Laskin et al., 2020). For example, the state-of-the-art algorithm for visual RL, DrQ (Yarats
et al., 2021b) builds on top of Soft Actor-Critic (Haarnoja et al., 2018a), a model-free actor-critic
algorithm, by adding a convolutional encoder and data augmentation in the form of random shifts.
The use of such data augmentations now forms an essential component of several recent visual RL
algorithms (Srinivas et al., 2020; Raileanu et al., 2020; Yarats et al., 2021a; Stooke et al., 2020;
Hansen and Wang, 2021; Schwarzer et al., 2020b).
3	DrQ-v2: Improved Data-Augmented Reinforcement Learning
In this section, we describe DrQ-v2, a simple model-free actor-critic RL algorithm for image-based
continuous control, that builds upon DrQ.
3.1	Algorithmic Details
Image Augmentation As in DrQ we apply random shifts image augmentation to pixel observations
of the environment. In the settings of visual continuous control by DMC, this augmentation can be
instantiated by first padding each side of 84 × 84 observation rendering by 4 pixels (by repeating
boundary pixels), and then selecting a random 84 × 84 crop, yielding the original image shifted by
±4 pixels. We also find it useful to apply bilinear interpolation on top of the shifted image (i.e, we
replace each pixel value with the average of the four nearest pixel values). In our experiments, this
modification provides an additional performance boost across the board.
Image Encoder The augmented image observation is then embedded into a low-dimensional latent
vector by applying a convolutional encoder. We use the same encoder architecture as in DrQ, which
first was introduced introduced in SAC-AE (Yarats et al., 2019). This process can be succinctly
summarized as h = fξ (aug(x)), where fξ is the encoder, aug is the random shifts augmentation,
and x is the original image observation.
Actor-Critic Algorithm We use DDPG (Lillicrap et al., 2015a) as a backbone actor-critic RL
algorithm and, similarly to Barth-Maron et al. (2018), augment it with n-step returns to estimate TD
error. This results into faster reward propagation and overall learning progress (Mnih et al., 2016a).
3
Under review as a conference paper at ICLR 2022
While some methods (Hafner et al., 2020) employ more sophisticated techniques such as TD(λ) or
Retrace(λ) (Munos et al., 2016), they are often computationally demanding when n is large. We find
that using simple n-step returns, without an importance sampling correction, strikes a good balance
between performance and efficiency. We also employ clipped double Q-learning (Fujimoto et al.,
2018) to reduce overestimation bias in the target value. Practically, this requires training two Q-
functions Qθ1 and Qθ2. For this, we sample a mini-batch of transitions τ = (xt, at,rt:t+n-1, xt+n)
from the replay buffer D and compute the following two losses:
Lθk,ξ(D) = EiD [(Qθk(ht, at) — y)2] ∀k ∈ {1,2},	(1)
with the TD target y defined as:
y
n-1
γirt Yirt+i + Ynmin Qθk (ht+n, at+n),
k=1,2
=0
where ht = fξ(aug(xt)), ht+n = fξ(aug(xt+n)), at+n = ∏φ(ht+n + e, & and & are the SloW-
moving weights for the Q target networks. We note, that in contrast to DrQ, we do not employ a
target network for the encoder fξ and always use the most recent weights ξ to embed xt and xt+n .
The exploration noise e is sampled from clip(N(0,σ2), -c, c) similar to TD3 (Fujimoto et al., 2018),
with the exception of decaying σ, which we describe below. Finally, we train the deterministic actor
πφ using DPG with the following loss:
Lφ(D) = -Ext〜D [ min Qθ% (%, *],
k=1,2
(2)
where ht = fξ(aug(xt)), at = ∏φ(ht) + e, and e 〜CliP(N(0, σ2), -c, c). Similar to DrQ, We do
not use actor’s gradients to update the encoder’s parameters ξ.
Scheduled Exploration Noise Empirically, we observe that it is helpful to have different levels of
exploration at different stages of learning. At the beginning of training we want the agent to be more
stochastic and explore the environment more effectively, while at the later stages of training, when
the agent has already identified promising behaviors, it is better to be more deterministic and master
those behaviors. Similar to Amos et al. (2020), we instantiate this idea by using linear decay σ(t) for
the variance σ2 of the exploration noise defined as:
σ(t) = σinit + (1 - min(g 1))(σfinai - /口上),	(3)
where σinit and σfinal are the initial and final values for standard deviation, and T is the decay horizon.
Key Hyper-Parameters We conduct an extensive hyper-parameter search and identify several
hyper-parameter changes compared to DrQ. The three most important hyper-parameters are: (i) the
size of the replay buffer, (ii) mini-batch size, and (iii) learning rate. Specifically, we use a 10 times
larger replay buffer than DrQ. We also use a smaller mini-batch size of 256 without any noticeable
performance degradation. This is in contrast to CURL (Srinivas et al., 2020) and DrQ (Yarats et al.,
2021b) that both use a larger batch size of 512 to attain more stable training in the expense of
computational efficiency. Finally, we find that using smaller learning rate of 1 × 10-4, rather than
DrQ’s learning rate of 1 × 10-3, results into more stable training without any loss in learning speed.
3.2	Implementation Details
Faster Image Augmentation We replace DrQ’s random shifts augmentation (i.e.,
kornia.augmentation.RandomCrop) by a custom implementation that uses flow-
field image sampling provided in PyTorch (i.e., grid_sample). This is done for two reasons. First,
we noticed that Kornia’s implementation does not fully utilize GPU pipelining since it has some
intermediate CPU to GPU data transferring which breaks the computational flow. Second, using
grid_sample allows straightforward addition of bilinear interpolation. Our custom random shifts
augmentation improves training throughput by a factor of 2.
4
Under review as a conference paper at ICLR 2022
Faster Replay Buffer Another computational bottleneck of DrQ was the replay buffer. The specific
implementation had poor memory management which resulted in slow CPU to GPU data transfer,
which also restricted the number of image-based transitions that could be stored. We reimplemented
the replay buffer to address these issues which led to a ten-fold increase in storage capacity and faster
data transfer. More details are available in our open-source release. We note that the improved training
speed of DrQ-v2 was key to solving humanoid tasks as it enabled much faster experimentation.
4	Experiments
In this section we provide empirical evaluation of DrQ-v2 on an extensive set of visual continuous
control tasks from DMC (Tassa et al., 2018). We first present comparison to prior methods, both
model-free and model-based, in terms of sample efficiency and wall-clock time. We then present a
large scale ablation study that guided the final version of DrQ-v2.
4.1	Setup
Environments We consider a set of MuJoCo tasks (Todorov et al., 2012) provided by DMC (Tassa
et al., 2018), a widely used benchmark for continous control. DMC offers environments of various
difficulty, ranging from the simple control problems such as the single degree of freedom (DOF)
pendulum and cartpool, to the control of complex multi-joint bodies such as the humanoid (21 DOF).
We consider learning from pixels. In this setting, environment observations are stacks of 3 consecutive
RGB images of size 84 × 84, stacked along the channel dimension to enable inference of dynamic
information like velocity and acceleration. In total, we consider 24 different tasks, which we group
into three buckets, easy, medium, and hard, according to the sample complexity to reach near-optimal
performance (see Appendix B). Our motivation for this is to encourage RL practitioners to focus on
the medium and hard tasks and stop using the easy tasks for evaluation, as they are mostly solved at
this point and may no longer provide any valuable signal in comparing different methods.
Training Details For all tasks in the suite an episode corresponds to 1000 steps, where a per-step
reward is in the unit interval [0, 1]. This upper bounds the episode return to 1000 making it easier to
compute aggregated performance measures across tasks. To facilitate fair wall-clock time comparison
all algorithms are trained on the same hardware (i.e., a single NVIDIA V100 GPU machine) and
evaluated with the same periodicity of 20000 environment steps. Each evaluation query averages
episode returns over 10 episodes. Per common practice (Hafner et al., 2019), we employ action repeat
of 2 and measure sample complexity in the environment steps, rather than the actor steps. In all the
figures we plot the mean performance over 10 seeds together with the shaded regions which represent
95% confidence intervals. A full list of hyper-parameters can be found in Appendix E.
Comparison Axes In many real-world applications, taking a step in the environment incurs sig-
nificant computational cost making sample efficiency a critical feature of an RL algorithm. It is
hence important to compare RL algorithms in terms of their sample efficiency. We facilitate this
comparison by computing an algorithm’s performance measured by episode return with respect to
environment steps. On the other end, striving low sample complexity often comes at the cost of a
poor computational efficiency. Unfortunately, recent deep RL literature has paid very little attention
to this important axis, which has led to skyrocketing hardware requirements. Such a trend has made
it virtually impossible for an RL practitioner with modest hardware capacity to contribute to advance-
ments in image-based RL, leaving research in this area to a few well-equipped labs. To democratize
research in visual RL, we additionally propose to compare the agents in terms of wall-clock training
time given the same single GPU hardware. We note that it is possible to adapt DrQ-v2 to a distributed
setup, as has been done for DDPG in prior work (Barth-Maron et al., 2018; Hoffman et al., 2020).
4.2	Comparison to Model-Free Methods
Baselines We compare our method to several state-of-the-art model-free algorithms for visual RL
including CURL (Srinivas et al., 2020), DrQ (Yarats et al., 2021b), and vanilla SAC (Haarnoja et al.,
2018a) augmented with the convolutional encoder from SAC-AE (Yarats et al., 2019). Vanilla SAC is
a weak baseline and only included as a ground point to showcase the recent progress in visual RL.
5
Under review as a conference paper at ICLR 2022
Enlφccα,po-d 山
Acrobot Swmgup
UJmQH 3po-d 山
Finger Turn Hard
Ooo
5 0 5
7 5 2
Enw3pod 山
EnW,υpo--fi
12	3
Frames (×10β) le6
Humanoid Walk
Acrobot Swmgup
1
J 8.6 hours
(a) Sample Efficiency.
Fmgerlurn Hard
u∙ln*j,uaa,posaw
Cheetah Run
Hours (for 3 × 10β Frames)
En∙*ja,Ha,po--d 山
8.6 hours
Reach Duplo
IOOO
750
500
250
° O 20	40
Hours (for 3 × 10β Frames)
©Poss
Hopper Hop
J	20	40
Hours (for 3 X 10β Frames)
O 20	40
u∙lnla,tta,posa山
ɔ 20	40
Hours (for 3 × 10β Frames)
Humanoid Walk
0	200	400
Hours (for 30× IO6 Frames)
u,Jn3ωHωposa山
40
Frames)
Ema,Ha,po--d 山
8.6 hours
Hours (for 3 × 10β
O 20	40
Reacher Hard
Hours (for 3 × 10β Frames)
u-nw ©po-d 山
(b) Wall-clock Time.
■ SAC ■ CURL ■ DrQ D DrQ-V2 (OUrs)
Figure 3: We compare DrQ-v2 on a subset of continuous control tasks that offer various challenges,
inclUding complex dynamics, sparse rewards, hard exploration, and more. (a) DrQ-V2 demonstrates
faVorable sample efficiency and comfortably oUtperforms leading model-free baselines, as well as
reqUiring less wall-clock training image (b).
Sample Efficiency Axis We present resUlts on seVeral medium and hard tasks in FigUre 3a. FUll
resUlts can be foUnd in Appendix (FigUre 6, FigUre 8, and FigUre 10). OUr empirical stUdy reVeals
that DrQ-V2 oUtperforms prior model-free methods in terms of sample efficiency across the three
benchmarks with different leVels of difficUlty. Importantly, DrQ-V2’s adVantage is more pronoUnced
on harder tasks (i.e., acrobot, qUadrUped, and hUmanoid), where exploration is especially challenging.
Finally, DrQ-V2 solVes the DMC hUmanoid locomotion tasks directly from pixels, which, to the best
of oUr knowledge, is the first sUccessfUl demonstration of sUch feat by a model-free method.
Compute Efficiency Axis To facilitate a fair comparison in terms of sheer wall-clock training time,
besides employee the identical training protocol (see Section 4.1), we also Use the same mini-batch
size of 256 for each agent. In FigUre 13, we eValUate DrQ-V2 on a sUbset of DMC tasks for the sake of
breVity only, and note that the demonstrated resUlts can be easily extrapolated to the other tasks giVen
the linear dependency between training time and sample complexity. In oUr benchmarks, DrQ-V2
is able to achieVe a throUghpUt of 96 FPS, which faVorably compares to DrQ’s 28 FPS (a 3.4×
increase), and CURL’s 16 FPS (a 6× increase) throUghpUts. Practically, DrQ-V2 solVes easy, mediUm,
and hard tasks within 2.9, 8.6, and 86 hoUrs respectiVely. FUll resUlts can be foUnd in Appendix
(FigUre 7, FigUre 9, and FigUre 11).
4.3 Comparison to Model-Based Methods
Baseline To see how DrQ-V2 stacks Up against model-based methods, which tend to achieVe better
sample complexity in expense of a larger compUtational footprint, we also compare to recent and
6
Under review as a conference paper at ICLR 2022
Acrobot Swmgup
Frames (×106)
107 5 2
UJm3a3po-d 山
Reacher Hard
Frames (×106)
Humanoid Walk
(a) Sample Efficiency.
Acrobot Swingup
Enlα,αa,po-d 山
Hours (for 3 × 10β Frames)
mωcc3po-d 山
Quadruped Run
IOOO
750
500
250
0 0 IO 20	30
Hours (for 3 X 10β Frames)
Oooo-
0 5 0 5
17 5 2
En3pod 山
Enw 3po-d 山
0	100	200	300
Hours (for 30 X 10β Frames)
(b) Wall-clock Time.
■ Dreamer-V2 ■ DrQ-V2 (OUrs)
Figure 4: Model-based Dreamer-v2 needs to train a world model and thus performs more computations
dUring training than model-free DrQ-V2. Still, (a) DrQ-V2 is able to match Dreamer-V2’s sample
efficiency, while (b) reqUiring mUch less wall-clock training time.
UnpUblished2 improVements to Dreamer-V2 (Hafner et al., 2020), a leading model-based approach
for VisUal continUoUs control. The recent Update shows that the model-based approach can solVe the
DMC hUmanoid tasks directly from pixel inpUts. The open-soUrce implementation of Dreamer-V2
(https://github.com/danijar/dreamerv2) only proVides learning cUrVes for Humanoid
Walk. For this reason we rUn their code to obtain resUlts on other DMC tasks. To limit hardware
reqUirements of compUte-expensiVe Dreamer-V2, we only rUn it on a sUbset of 12 oUt of 24 considered
tasks. This sUbset, howeVer, oVerlaps with all the three (i.e. easy, mediUm, and hard) benchmarks.
Sample Efficiency Axis OUr empirical stUdy in FigUre 4a reVeals that in many cases, DrQ-V2,
despite being a model-free method, can riVal sample efficiency of state-of-the-art model-based
Dreamer-V2. We note, howeVer, that on seVeral tasks (for example Acrobot Swingup) Dreamer-V2
oUtperforms DrQ-V2. We leaVe inVestigation of sUch discrepancy for fUtUre work. FUll resUlts are
proVided in Appendix D (FigUre 12).
Compute Efficiency Axis A different pictUre emerges if comparison is done with respect to wall-
clock training time. Dreamer-V2, being a model-based method, performs significantly more floating
point operations to reach its sample efficiency. In oUr benchmarks, Dreamer-V2 records a throUghpUt
of 24 FPS, which is 4× less than DrQ-V2’s throUghpUt of 96 FPS, measUred on the same hardware.
In FigUre 4b we plot learning cUrVes against wall-clock time and obserVe that DrQ-V2 takes less time
to solVe the tasks. FUll resUlts can be foUnd in Appendix (FigUre 13).
4.4 Ablation Study
In this section we present an extensiVe ablation stUdy that gUided Us to the final Version of DrQ-V2.
Here, for breVity we only discUss experiments that were most impactfUl and omit others that did not
pan oUt. For compUtational reasons, we only ablate on 3 different control tasks of VarioUs difficUlty
leVels. OUr findings are sUmmarized in FigUre 5 and detailed below.
Switching from SAC to DDPG DrQ (Yarats et al., 2021b) leVerages SAC (Haarnoja et al., 2018a)
as the backbone RL algorithm. While it has been demonstrated by many works, inclUding the
original manUscripts (Haarnoja et al., 2018a;b) that SAC is sUperior to DDPG (Lillicrap et al., 2015b),
oUr carefUl examination identifies two shortcomings that preclUde SAC (within DrQ) to solVe hard
exploration-wise image-based tasks. First, the aUtomatic entropy adjUstment strategy, introdUced
in Haarnoja et al. (2018b), is inadeqUate and in some cases leads to a prematUre entropy collapse.
2ArXiV V3 reVision from May 3, 2021 introdUces a new resUlt on the Humanoid Walk task in Appendix D.
7
Under review as a conference paper at ICLR 2022
Acrobot Swmgup
OOA
O O
2 1
UJαpod 山
Frames (×106)
Quadruped Walk
Vooo-
Uooo
0 6 4 2
Eruαpod 山
		
		
7 -——		
Frames (×106)
Reacher Hard
Ooo-
Ooo
6 4 2
Enwapod 山
DrQ[SAC] DrQ[DDPG]
(a)	DrQ (dotted silVer) relies on SAC as a base RL algorithm. Replacing SAC with DDPG results in a
significant performance gain (blue).
UJm,uαωpo-d 山
u-lru4,ααjpo-d 山
Quadruped Walk
Eruωαtupo-d 山
Frames (×106)
Frames (×106)

ι
2
3
DrQ[DDPG,n=1]	DrQ[DDPG,n=3]	DrQ[DDPG,n=5]
(b)	DDPG straightforwardly incorporates n-step returns, a critical tool for exploration. We obserVe that
the 3 (blue) and 5 (red) steps Variants proVide additional improVements to the preVious Version that uses
single step TD-targets (silVer). Going forward, we adopt 3-step returns (blue).
UJn3,uD=α,posa山
Quadruped Walk
UJmωɑωpo-d 山
■ DrQ[DDPG,n=3,B=105] ■ DrQ[DDPG,n=3,B=5 ∙ 105] ■ DrQ[DDPG,n=3,B=106]
(c)	Increasing the size of the replay buffer (B) improVes performance, oVer the original 105 used by DrQ
(silVer). Going forward, we use a buffer size of 106 (red).
Acrobot Swmgup
0	12	3
Frames (×106)
Quadruped Walk
0123
Frames (×106)
EnwωPOMd 山
■ DrQ[DDPG,n=3,B=106,noise=fixed] ■ DrQ[DDPG,n=3,B=106,noise=decay] (DrQ-V2)
(d)	Finally, a decaying schedule for the variance of the exploration noise (blue) helps on hard exploration
tasks, Versus the fixed Variance Variant (silVer).
Figure 5: An ablation study that led us to the final Version of DrQ-V2. We incrementally show each
of the four key improVements to DrQ that collectiVely form DrQ-V2. The silVer dotted curVes in the
first row show the original DrQ. In subsequent rows they show progressiVe improVements, using
the optimal choice from the preVious rows (i.e., the silVer curVe in the third row shows DrQ with a
DDPG base RL algorithm and 3-step returns). The red and blue curVes show the effect of indiVidual
modifications. In the last row the blue curVe corresponds to DrQ-V2.
This preVents the agent from finding more optimal behaViors due to the insufficient exploration.
In Figure 5a, we empirically Verify our intuition and, indeed, obserVe that DDPG demonstrates better
exploration properties than SAC. Here, DDPG uses constant σ =0.2 for the exploration noise.
8
Under review as a conference paper at ICLR 2022
N-step Returns The second issue concerns the inability of soft Q-learning to incorporate n-step
returns to estimate TD error in a straightforward manner. The reason for this is that computing a target
value for soft Q-function requires estimating per-step entropy of the policy, which is challenging
to do for large n in the off-policy regime. In contrast, DDPG does not require estimating per-step
entropy to compute targets and is more amenable for n-step returns. In Figure 5b we demonstrate that
estimating TD error with n-step returns improves sample efficiency over vanilla DDPG. We select
3-step returns as a sensible choice for our method.
Replay Buffer Size We hypothesize that a larger replay buffer plays an important role in circum-
venting the catastrophic forgetting problem (Fedus et al., 2020). This issue is especially prominent in
tasks with more diverse initial state distributions (i.e., reacher or humanoid tasks), where the vast
variety of possible behaviors requires significantly larger memory. We confirm this intuition by
ablating the size of the replay buffer in Figure 5c, where we observe that a buffer size of 1M helps to
improve performance on Reacher Hard considerably.
Scheduled Exploration Noise Finally, we demonstrate that it is useful to decay the variance of the
exploration noise over the course of training according to Equation (3). In Figure 5d, we compare two
versions of our algorithm, where the first variant uses a fixed standard deviation of σ = 0.2, while the
second variant employes the decaying schedule σ(t), with parameters σinit = 1.0, σfinal = 0.1, and
T = 500000. Having the exploration noise to decay linearly over time turns out to be helpful and
provide an additional performance boost, which was especially useful for solving humanoid tasks.
5	Related Work
Visual Reinforcement Learning Successes of visual representation learning in computer vi-
sion (Vincent et al., 2008; Doersch et al., 2015; Wang and Gupta, 2015; Noroozi and Favaro,
2016; Zhang et al., 2017; Gidaris et al., 2018) has inspired successes in visual RL, where coherent
representations are learned alongside RL. Works such as SAC-AE (Yarats et al., 2019), PlaNet (Hafner
et al., 2018), and SLAC (Lee et al., 2019), demonstrated how auto-encoders (Finn et al., 2015) could
improve visual RL. Following this, other self-supervised objectives such as contrastive learning
in CURL (Srinivas et al., 2020) and ATC (Stooke et al., 2020), self-prediction in SPR (Schwarzer
et al., 2020a), contrastive cluster assignment in Proto-RL (Yarats et al., 2021a), and augmented
data in DrQ (Yarats et al., 2021b) and RAD (Laskin et al., 2020), have significantly bridged the
gap between state-based and image-based RL. Future prediction objectives (Hafner et al., 2018;
2019; Yan et al., 2020; Finn et al., 2015; Pinto et al., 2016; Agrawal et al., 2016) and other auxiliary
objectives (Jaderberg et al., 2016; Zhan et al., 2020; Young et al., 2020; Chen et al., 2020) have shown
improvements on a variety of problems ranging from gameplay, continuous control, and robotics. In
the context of visual control settings, clever use of augmented data (Yarats et al., 2021b; Laskin et al.,
2020) currently produces state-of-the-art results on visual tasks from DMC (Tassa et al., 2018).
Humanoid Control The humanoid control problem first presented in Tassa et al. (2012), has been
studied as one of the hardest control problems due to its large state and action spaces. The earliest
solutions to this problem use ideas in model-based optimal control to generate policies given an
accurate model of the humanoid . Subsequent works in RL have shown that model-free policies can
solve the humanoid control problem given access to proprioceptive state observations. However,
solving such a problem from visual observations has been a challenging problem, with leading RL
algorithms making little progress to solve the task (Tassa et al., 2018). Recently, Hafner et al. (2020)
was able to solve this problem through a model-based technique in around 30M environment steps
and 340 hours of training on a single GPU machine. DrQ-v2, presented in this paper, marks the first
model-free RL method that can solve humanoid control from visual observations, taking also around
30M steps and 86 hours of training on the same hardware.
6	Conclusion
We have introduced a conceptually simple model-free actor-critic RL agent for image-based con-
tinuous control - DrQ-V2. Our method provides significantly better computational footprint and
masters tasks from DMC directly from pixels, most notably the humanoid locomotion tasks that
9
Under review as a conference paper at ICLR 2022
were previously unsolved by model-free approaches. To support our empirical results and in-
spire further research in visual RL we provide an efficient PyTorch implementation of DrQ-v2 at
https://anonymous.4open.science/r/drqv2.
10
Under review as a conference paper at ICLR 2022
References
Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke
by poking: experiential learning of intuitive physics. In Proceedings of the 30th International
Conference on Neural Information Processing Systems, pages 5092-5100, 2016.
Brandon Amos, Samuel Stanton, Denis Yarats, and Andrew Gordon Wilson. On the model-based
stochastic value gradient for continuous reinforcement learning. CoRR, 2020.
Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva
TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributional policy gradients. In
International Conference on Learning Representations, 2018.
Richard Bellman. A markovian decision process. Indiana Univ. Math. J., 1957.
Bryan Chen, Alexander Sax, Gene Lewis, Iro Armeni, Silvio Savarese, Amir Roshan Zamir, Jitendra
Malik, and Lerrel Pinto. Robust policies via mid-level visual representations: An experimental
study in manipulation and navigation. CoRR, 2020.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE International Conference on Computer Vision,
pages 1422-1430, 2015.
Jing eng and Ronald J. Williams. Incremental multi-step q-learning. Machine Learning, 1996.
William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark
Rowland, and Will Dabney. Revisiting fundamentals of experience replay. CoRR, 2020.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Learning
visual feature spaces for robotic manipulation with deep spatial autoencoders. CoRR, 2015.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, 2018.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta a nd Pieter Abbeel, and Sergey Levine. Soft actor-critic
algorithms and applications. CoRR, 2018b.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.
Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
discrete world models. CoRR, 2020.
Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmen-
tation. In International Conference on Robotics and Automation, 2021.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Daniel Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
11
Under review as a conference paper at ICLR 2022
Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara
Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex
Novikov, Sergio G6mez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew
Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed
reinforcement learning, 2020.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks, 2016.
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.
Reinforcement learning with augmented data, 2020.
A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement
learning with a latent variable model. arXiv e-prints, 2019.
Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR,
2015a.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. CoRR, abs/1509.02971, 2015b.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv e-prints,
2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. CoRR, 2016a.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning, 2016b.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient
off-policy reinforcement learning. CoRR, 2016.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pages 69-84. Springer, 2016.
Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, and Abhinav Gupta. The curious robot:
Learning visual representations via physical interactions. CoRR, 2016.
Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data
augmentation for generalization in deep reinforcement learning. CoRR, 2020.
Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman.
Data-efficient reinforcement learning with momentum predictive representations. arXiv preprint
arXiv:2007.05929, 2020a.
Max Schwarzer, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron C. Courville, and Philip
Bachman. Data-efficient reinforcement learning with momentum predictive representations. CoRR,
2020b.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
Machine Learning, 2014.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
12
Under review as a conference paper at ICLR 2022
Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning
from reinforcement learning. arXiv preprint arXiv, 2020.
Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors
through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent
Robots and Systems, 2012.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pages 1096-1103. ACM, 2008.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In ICCV, 2015.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 1992.
Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis, King’s
College, 1989.
Wilson Yan, Ashwin Vangipuram, Pieter Abbeel, and Lerrel Pinto. Learning predictive representations
for deformable objects using contrastive estimation. arXiv preprint arXiv:2003.05436, 2020.
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Im-
proving sample efficiency in model-free reinforcement learning from images. arXiv preprint
arXiv:1910.01741, 2019.
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with
prototypical representations. CoRR, 2021a.
Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. In 9th International Conference on Learning Representations,
ICLR 2021, 2021b.
Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav Gupta, Pieter Abbeel, and Lerrel Pinto.
Visual imitation made easy. CoRR, 2020.
Albert Zhan, Philip Zhao, Lerrel Pinto, Pieter Abbeel, and Michael Laskin. A framework for efficient
robotic manipulation. CoRR, 2020.
Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning
by cross-channel prediction. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1058-1067, 2017.
13