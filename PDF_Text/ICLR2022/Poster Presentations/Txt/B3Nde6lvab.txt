Published as a conference paper at ICLR 2022
Eliminating Sharp Minima from SGD with
Truncated Heavy-tailed Noise
Xingyu Wang1	Sewoong Oh2 Chang-Han Rhee1
1Northwestern University, 2University of Washington
xingyuwang2017@u.northwestern.edu
Ab stract
The empirical success of deep learning is often attributed to SGD’s mysterious
ability to avoid sharp local minima in the loss landscape, as sharp minima are
known to lead to poor generalization. Recently, evidence of heavy-tailed gradient
noise was reported in many deep learning tasks, and it was shown in (SimSekli
et al., 2019a;b) that SGD can escape sharp local minima under the presence of such
heavy-tailed gradient noise, providing a partial solution to the mystery. In this work,
we analyze a popular variant of SGD where gradients are truncated above a fixed
threshold. We show that it achieves a stronger notion of avoiding sharp minima: it
can effectively eliminate sharp local minima entirely from its training trajectory.
We characterize the dynamics of truncated SGD driven by heavy-tailed noises.
First, we show that the truncation threshold and width of the attraction field dictate
the order of the first exit time from the associated local minimum. Moreover, when
the objective function satisfies appropriate structural conditions, we prove that as
the learning rate decreases the dynamics of the heavy-tailed truncated SGD closely
resemble those of a continuous-time Markov chain that never visits any sharp
minima. Real data experiments on deep learning confirm our theoretical prediction
that heavy-tailed SGD with gradient clipping finds a "flatter" local minima and
achieves better generalization.
1 Introduction
Stochastic gradient descent (SGD) and its variants have seen unprecedented empirical successes in
training deep neural networks. The training of deep neural networks is typically posed as a non-
convex optimization problem, and even without explicit regularization the solutions obtained by SGD
often perform surprisingly well on test data. Such an unexpected generalization performance of SGD
in deep neural networks are often attributed to SGD’s ability to avoid sharp local minima1 in the loss
landscape, which tends to lead to poor generalization (Hochreiter & Schmidhuber, 1997; Keskar et al.,
2016; Li et al., 2018b; Jiang et al., 2019); see Appendix D for more details. Despite significant efforts
to explain such phenomena theoretically, understanding how SGD manages to avoid sharp local
minima and end up with flat local minima within a realistic training time still remains as a central
mystery of deep learning. 2 Recently, the heavy-tailed dynamics of SGD received significant attention,
and it was suggested that the heavy tails in the stochastic gradients may be a key ingredient that
facilitates SGD’s escape from sharp local minima: for example, SS imsSekli et al. (2019a;b) report the
empirical evidence of heavy-tails in stochastic gradient noise in popular deep learning architectures
(see also (Hodgkinson & Mahoney, 2020; Srinivasan et al., 2021; Garg et al., 2021)) and show that
SGD can escape sharp local minima in polynomial time under the presence of the heavy-tailed
gradient noise. More specifically, they view heavy-tailed SGDs as discrete approximations of Levy
driven Langevin equations and argue that the amount of time SGD trajectory spends in each local
minimum is proportional to the width of the associated minimum according to the metastability
theory (Pavlyukevich, 2007; Imkeller et al., 2010a;b) for such heavy-tailed processes.
1We use the terminology sharpness in a broad sense; we refer to Appendix C for a detailed discussion.
2To see a detailed discussion on existing results on selection of local minima from the stability perspective
and the novelty of our analysis, see Appendix E.
1
Published as a conference paper at ICLR 2022
Figure 1: (Left) Histograms of the locations visited by SGD. With truncated heavy-tailed noises,
SGD hardly ever visits the two sharp minima m1 and m3 . The objective function f is plotted at the
bottom, and dashed lines are added as references for the locations of local minima. (Middle) Typical
trajectories of SGD in different cases: (a) Heavy-tailed noises, no gradient clipping; (b) Heavy-tailed
noises, gradient clipping at b = 0.5; (c) Light-tailed noises, no gradient clipping; (d) Light-tailed
noises, gradient clipping at b = 0.5. The objective function f is plotted at the right of each figure,
and dashed lines are added as references for locations of the local minima. (Right) First Exit Time
from Ω2 = (-1.3,0.2). Each dot represents the average of 20 samples of first exit time. Each dahsed
line shows a polynomial function ci /ηβ where β is predicted by Theorem 1 and ci is chosen to fit the
dots. The non-solid green dot indicates that for some of the 20 samples of the termination threshold
5 × 107 was reached, and hence, it is an underestimation. Results in (Left) and (Middle) are obtained
under learning rate η = 0.001 and initial condition X0 = 0.3.
In this paper, we study the global dynamics and long-run behavior of heavy-tailed SGD and its
practical variant in depth. While in full generality the structure of gradient noises in SGD is state-
dependent, in this work we focus on the role of noise magnitude and analyze the setting where each
SGD update is perturbed by iid heavy-tailed noise. In particular, we consider an adaptive version of
SGD, where the stochastic gradient is truncated above a fixed threshold. Such truncation scheme is
often called gradient clipping and employed as default in various contexts (Engstrom et al., 2020;
Merity et al., 2018; Graves, 2013; Pascanu et al., 2013; Zhang et al., 2020; Gorbunov et al., 2020).
We uncover a rich mathematical structure in the global dynamics of SGD under this scheme and
prove that the asymptotic behavior of such SGD is fundamentally different from that of the pure form
of SGD: in particular, under a suitable structural condition on the geometry of the loss landscape,
gradient clipping completely eliminates sharp minima from the trajectory of SGDs. This provides a
critical insight into how heavy-tailed dynamics of SGD can be utilized to find a local minimum that
generalizes better.
Figure 1 (Left, Middle) clearly illustrates these points with the histograms of the sample trajectories of
SGDs. Note first that SGDs with light-tailed gradient noise—(c) and (d) of Figure 1 (Left, Middle)—
never manages to escape a (sharp) minimum regardless of gradient clipping. In contrast, SGDs
with heavy-tailed gradient noise—(a) and (b) of Figure 1 (Left, Middle)—easily escapes from local
minima. Moreover, there is a clear difference between SGDs with gradient clipping and without
gradient clipping. In (a) of Figure 1 (Left), SGD without gradient clipping spends a significant
amount of time at each of all four local minima ({m1, m2, m3, m4}), although it spends more time
around the wide ones ({m2, m4}) than the sharp ones ({m1, m3}). On the other hand, in (b) of
Figure 1 (Left), SGD with gradient clipping not only escapes from local minima but also avoids
sharp minima ({m1, m3}) almost completely. This means that after we run SGD for long enough
(more precisely, the required run length t∕ηβ is of polynomial order; see Theorem 2), it is almost
guaranteed that it won’t be at a sharp minimum, effectively eliminating sharp minima from its training
trajectories.
We also propose a novel computational strategy that takes advantage of our newly discovered global
dynamics of the heavy-tailed SGD. While the evidence of heavy tails were reported in many deep
learning tasks (SimSekIi et al., 2019b;a; Garg et al., 2021; Gurbuzbalaban et al., 2020; Hodgkinson
2
Published as a conference paper at ICLR 2022
& Mahoney, 2020; Nguyen et al., 2019; Mahoney & Martin, 2019; Srinivasan et al., 2021; Zhang
et al., 2020), there seem to be plenty of deep learning contexts where the stochastic gradient noises
are light-tailed (Panigrahi et al., 2019) as well. 3 Guided by our new theory, we propose an algorithm
that injects heavy-tails to SGD by inflating the tail distribution of the gradient noise and facilitating
the discovery of a local minimum that generalizes better. Our experiments with image classification
tasks, reported in Tables 1 and 2, illustrate that the tail-inflation strategy we propose here can indeed
improve the generalization performance of the SGD as predicted by our theory.
The rest of the paper is organized as follows. Section 2 formulates the problem setting and character-
izes the global dynamics of the SGD driven by heavy-tailed noises. Section 3 presents numerical
experiments that confirm our theory. Section 4 proposes a new algorithm that artificially injects heavy
tailed gradient noise in actual deep learning tasks and demonstrate the improved performance.
Technical Contributions: 1) We rigorously characterize the global behavior of the heavy-tailed
SGD with gradient clipping. We first focus on the case where the loss function is in R1 with
some simplifying assumptions on its geometry. Even with such assumptions, our theorem involves
substantial technical challenges since the traditional tools for analyzing SGD fail in our context due to
the adaptive nature of its dynamics and non-Gaussian distributional assumptions. For example, while
the unclipped pure SGD can be analyzed by partitioning its trajectory at arrival times of large noises
(as in Pavlyukevich (2005) and Imkeller et al. (2010a)), such an approach falls short in our context.
Instead, we developed a set of delicate arguments for dealing with SGD’s (near) regeneration structure
and the return times to the local minima, as well as controlling the probability of atypical scenarios
that would not arise in the unclipped case. Moreover, as evidenced by our Rd results in Appendix I,
the approach developed here is critical in extending the analysis to general loss landscapes.
2) We propose a novel computational strategy for improving the generalization performance of SGD
by carefully injecting heavy-tailed noise. We test the proposed algorithm with deep learning tasks
and demonstrate its superiority with an ablation study. This also suggests that the key phenomenon
we characterize in our theory— elimination of sharp local minima—manifests in real-world tasks.
2	Theoretical results
This section characterizes the global dynamics of SGD with gradient clipping when applied to a
non-convex objective function f. In Section 2.1 and 2.2, we make the following assumptions for
the sake of the simplicity of analysis. However, as our multidimensional result in Section 2.3 and
the experiments in Section 3 and 4 suggest, the gist of the phenomena we analyze—elimination of
sharp local minima—persists in general contexts where the domain of f is multi-dimensional, and
the stationary points are not necessarily strict local optima separated from one another.
Assumption 1. Let f : R → R be a C2 function. There exist a positive real L > 0, a positive
integer nmin and an ordered Sequence ofreal numbers mi, s1,m2, s2, •…,Snm^-ι,mnmin such that
(1) -L < mi < si < m2 < s2 < …< Snmin-1 < mnmm < L； (2) f 0(x) = 0 iff X ∈
{mi, si, .一,Snmin-1, mnmin}; (3) For any X ∈ {m1,m2,…,mnmin}, f00(x) > 0; (4) For any
X ∈ {si, s2,…，Snmin-i}, f"(x) < 0.
As illustrated in Figure 2 (Left), the assumption above requires that f has finitely many local minima
(to be specific, the count is nmin), all of which contained in some compact domain [-L, L]. Moreover,
the points si, •…，Snmin-I naturally partition the entire real line into different regions Ωi = (si-i, Si)
(here we adopt the convention that so = -∞, Snmin = +∞). We call each region Ωi the attraction
field of the local minimum mi, as the gradient flow in Ωi always points to m^
Throughout the optimization procedure, given any location X ∈ R we assume that we have access
to the noisy estimator f0(X) - Zn of the true gradient f0(X), and f0(X) itself is difficult to evaluate.
Specifically, in this work we are interested in the case where the iid sequence of noises (Zn)n≥i are
heavy-tailed. Typically, the heavy-tailed phenomena are captured by the concept of regular variation:
for a measurable function φ : R+ 7→ R+ , we say that φ is regularly varying at +∞ with index β
(denoted as φ ∈ RVβ) if limχ→∞ φ(tx)∕φ(x) = tβ for all t > 0. For details on the definition and
properties of regularly varying functions, see, for example, chapter 2 of Resnick (2007). In this paper,
3For a detailed comparison to existing works on heavy-tailed phenomena is SGD, see Appendix F.
3
Published as a conference paper at ICLR 2022
we work with the following distributional assumption on the gradient noise. Let
H+(x) , P(Z1 > x), H-(x) , P(Z1 < -x), H(x) , H+(x) + H-(x) = P(|Z1| > x).
Assumption 2. EZ1 = 0. Furthermore, there exists some α ∈ (1, ∞) such that function H(x) is
regularly varying (at +∞) with index -α. Besides, regarding the positive and negative tail for
distribution of the noises, we have
H+ (x)	H- (x)
lim ττ( = = p+, lim ττ( = = P- = 1 - p+
x→∞ H(x)	x→∞ H(x)
where p+ and p- are constants in interval (0, 1).
Roughly speaking, Assumption 2 means that the shape of the tail for the distribution of noises Zn
resembles a polynomial function x-α, which is much heavier than the exponential tail of Gaussian
distributions. Therefore, large values of Zn are much more likely to be observed under Assumption 2
compared to the typical Gaussian assumption. The index α of regular variation encodes the heaviness
of the tail—the smaller the heavier—and we are assuming that the left and right tails share the same
index α. The purpose of this simplifying assumption is clarity of presentation, but our Rd results in
Appendix I relax such a condition and allow different regular variation indices in different directions.
Our work concerns a popular variant of SGD where the stochastic gradient is truncated. Specifically,
when updating the SGD iterates with a learning rate η > 0, rather than using the original noisy gradient
descent step η(f0(Xn) - Zn), we will truncate it at a threshold b >0 and use ψb(η(f(Xn)- Zn))
instead. Here the truncation operator 夕∙(∙)is defined as
夕c(w)，W ∙ min{1, c∕∣w∣} ∀w ∈ R,c > 0.	(1)
Besides truncating the stochastic gradient, we also project the SGD into [-L, L] at each iteration;
recall that L is the constant in Assumption 1. That is, the main object of our study is the stochastic
process {Xjη}j≥0 driven by the following recursion
Xj =中L "j-1-中b"Xj-" Zj))).	(2)
The projection 夕L and truncation Wb here are common practices in many learning tasks for the
purpose of ensuring that the SGD does not explode and drift to infinity. Besides, the projection also
allows us to drop the sophisticated assumptions on the tail behaviors off that are commonly seen in
previous works (see, for instance, the dissipativity conditions in Nguyen et al. (2019)). For technical
reasons, we make the following assumption about the truncation threshold b > 0. Note that this
assumption is a very mild one, as it is obviously satisfied by (Lebesgue) almost every b > 0.
Assumption 3. For each i = 1, 2, .…,nmin, min{∣Si — mi|, ∣Si-ι — m∕}∕b is not an integer.
2.1	First exit times
Denote the SGD's first exit time from the attraction field Ωi with σi(η) = min{n ≥ 0 : Xn / Ωi}.
In this section, we prove that σi(η, x) converges to an exponential distribution when scaled properly.
To characterize such a scaling, we first introduce a few concepts. For each attraction field Ωi, define
(note that dxe = min{n / Z : n ≥ x}, bxc = max{n / Z : n ≤ x} )
ri = min{∣mi — Si-i∣, |si — m∕}, li = dri∕b]∙	(3)
Note that l*,s in fact depend on the the value of gradient clipping threshold b even though this
dependency is not highlighted by the notation. Here ri can be interpreted as the radius or the effective
width of the attraction field, and 偿 is the minimum number of jumps required to escape Ωi when
starting from mi . Indeed, the gradient clipping threshold b dictates that no single SGD step can travel
more than b, and to exit Ωi when starting from mi we can see that at least dri∕b] steps are required.
We can interpret li as the minimum effort required to exit Ω%. In this sense, li is an indicator of the
width of the attraction field Ωi. Theorem 1 states that li dictates the order of magnitude of the first
exit time as well as where the iterates Xn land on at the first exit time. For each Ωi, define a scaling
function λi(η) =∆ H(1∕η) ((1∕η)H(1∕η))li -1 . To stress the initial condition, we write Px for the
probability law when conditioning on X0j = x, or simply write Xnj(x).
4
Published as a conference paper at ICLR 2022
Figure 2: Typical transition graphs G under different gradient clipping thresholds b. (Left) The
function f illustrated here has 3 attraction fields. For the second one Ω2 = (si, s2), We have
s2 - m2 = 0.9, m2 - s1 = 0.6. (Middle) The typical transition graph induced by b = 0.5. The entire
graph G is irreducible since all nodes communicate With each other. (Right) The typical transition
graph induced by b = 0.4. When b = 0.4, since 0.6 < 2b and 0.9 > 2b, the SGD can only exit
Ω2 from the left with only 2 jumps if started from m2. Therefore, on the graph G there are two
communication classes: G1 = {m1, m2}, G2 = {m3}; G1 is absorbing While G2 is transient.
∣b = 0.4∣
Theorem 1.	Under Assumptions 1-3, there exist constants qi > 0 ∀i ∈ {1, 2,…,nmin} and
qi,j ≥ 0 ∀j ∈{1, 2,…，nmin}∖ {i} such that
(i)	Suppose that X ∈ Ωk for some k ∈ {1, 2, •一，n min}. Under Px, the scaled first exit time
qkλk(η)σk (η) converges in distribution to Exp(I) as η 1 0.
(ii)	For k,l ∈ {1, 2,…，n min} such that k = l, we have limn→o Px(Xσfc ⑺ ∈ Ωι) = qk,ι∕qk.
The proof and discussion are deferred to Appendix G. The constants qi, qi,j are explicitly identified
in terms of the gradient flows perturbed by Pareto jumps in Section C of Appendix. We note here that
Theorem 1 implies (i) for Xn to escape the current attraction field, say Ωi, it takes O(l∕λi(η)) time,
and (ii) the destination is most likely to be reachable within li jumps from m》
2.2 Elimination of Small Attraction Fields
Under proper structural assumptions on the geometry of f, the sharp minima of f can be effectively
eliminated from the trajectory of heavy-tailed SGD, facilitating the discovery of flat minima. This is
somewhat surprising given that gradient clipping mechanism makes the SGD iterates move slower.
The intuition behind this is that for narrow basins, applying gradient clipping has virtually no effect
on the order of exit time; whereas for a wide basin that requires multiple jumps to escape under the
clipping scheme, the clipping of gradients significantly slows down the escape and makes SGD stay
longer. In other words, gradient clipping only makes SGDs stay longer in the wider (better) basins.
Now, we introduce a few new concepts. Similar to the the minimum number OfjUmPS li defined in
(3), we define the following as the minimum number ofjumps to reach Ωj from mi for any j = i:
l = d(sj-1 - mi)∕be if j > i,
i,j d(mi - sj)∕be	if j < i.
(4)
Recall that Theorem 1 dictates that Xnη is most likely to move out of the current attraction field, say
Ωi, to somewhere else after O(l∕λi(η)) time steps, and the destination is most likely to be reachable
within l* jumps from m〃 Therefore, the transitions from Ωi to Ωj can be considered typical if Ωj
can be reached from mi with l* jumps—that is, li,j = l*. Now we define the following directed
graph that only includes these typical transitions.
Definition 1 (Typical Transition Graph). Given a function f satisfying Assumption 1 and gradient
clipping threshold b > 0 satisfying Assumption 3, a directed graph G = (V, E) is the corresponding
typical transition graph if(1) V = {mi, ∙∙∙ , mnjmin}; (2)An edge (mi → mj) is in E iff lij = l*.
Naturally, the typical transition graph G can be decomposed into different communication classes
Gi, ∙∙∙ , GK that are mutually exclusive by considering the equivalence relation associated with
the existence of the (two-way) paths between i and j . More specifically, for i 6= j , we say that
5
Published as a conference paper at ICLR 2022
i and j communicate if and only if there exists a path (mi,m^,…,mkn, mj) as well as a path
(mj, mk0, •…，mk> f,mi) in G; in other words, by travelling through edges on G, mi can be reached
from mj and mj can be reached from mi .
We say that a communication class G is absorbing if there does not exist any edge (mi → mj) ∈ E
such that mi ∈ G and mj ∈/ G. Otherwise, we say that G is transient. In the case that all mi ’s
communicate with each other on graph G, we say G is irreducible. See Figure 2 (Middle) for the
illustration of an irreducible case. When G is irreducible, we define the set of largest attraction fields
MIarge = {mi : i =1,2,…，nmin, 4 = llarge} where llarge = maxj lj; recall that li characterizes
the width of Ωi. Define the longest time scale λlarge(η) = H (1∕η)((H (1∕η)∕η))晔,-1. Note that this
corresponds exactly to the order of the first exit time of the largest attraction fields; see Theorem 1.
The following theorem is the main result of this paper.
Theorem 2.	Let Assumptions 1-3 hold and assume that the graph G is irreducible. For any t > 0,
β > 1 + (α - 1)llarge and x ∈ [-L, L],
1	八"ηβ C r	一】
bt∕ηβC I	1{xfuc (X) ∈ U jdu →0
L / η」J0	j:mj/MIarge
(5)
in probability as η → 0.
The proof is deferred to Appendix H. Here we briefly discuss the implication of the result. Suppose
that We terminate the training after a reasonably long time, say, ∖t∕ηβ C iterations. Then the random
variable that converges to zero in eq. (5) is exactly the proportion of time that Xnη spent in the
attraction fields that are not wide. Therefore, by truncating the gradient noise of the heavy-tailed
SGD, we can effectively eliminate small attraction fields from its training trajectory. In other words,
it is almost guaranteed that SGD is in one of the widest attraction fields after sufficiently long training.
Meanwhile, despite the asymptotic nature of Theorem 2, it has been confirmed in our simulation and
deep learning experiments that the elimination effect can be observed under typical choices of η.
Theorem 2 is merely a manifestation of the global dynamics of heavy-tailed SGD. The main messages
of the next theorem are: (a) under clipped heavy-tailed noises, the dynamics of Xnη for small η closely
resemble that of a continuous-time Markov chain; (b) this chain only visits local minima of the largest
attraction fields of f, thus minima in small attraction fields are completely avoided.
Theorem 3.	Let x ∈ Ω% for some i = 1,2, •…，nmin. If Assumptions 1-3 hold and G is irreducible,
then there exist a continuous-time Markov chain Y on M large and a random mapping π satisfying
•	π(m) ≡ m if m∈	M large;
•	π(m) is a random variable that only takes value in M large if m∈/ M large.
such that the scaled process {X}∕λ large (η)c (x) ： t ≥ 0} converges to process {Yt(π(m∕) : t ≥ 0}
in the sense of finite-dimensional distributions: for any positive integer k and any 0 < t1 < t2 <
∙…< tk, the random vector (XntI“large(哈、(x), •…,Xntk/λIarge(#」(x)) converges in distribution to
(Yti (π(mi)),…，Ytk (∏(mi))) as η J 0.
In section D of Appendix, we detail the proof, the exact parametrization of the generator matrix of
process Y, and the distribution of random mapping ∏(∙). Here We add some remarks. Intuitively
speaking, this result tells us that, regardless of where we initialize the SGD iterates, the dynamics
of the clipped heavy-tailed SGD converge to a continuous-time Markov chain avoiding any local
minima that is not in the largest attraction fields. Second, under small learning rate η > 0, ifXnn(x) is
initialized at X ∈ Ωi where Ωi is NOT a largest attraction field, then SGD will quickly escape Ωi and
arrive at some Ωj that is indeed a largest one——i.e., mj ∈ Mlarge; such a transition is so quick that,
under time scaling λlarge(η), it is almost instantaneous as if Xnn(x) is actually initialized randomly at
some of the largest attraction fields. This randomness is compressed in the random mapping π. In
Section B, we discuss how our characterization in Theorem 3 are more general and applicable in the
machine learning context than metastability results cited in (SimSekIi et al., 2019b); in Section H we
see that the regularization effect of truncated heavy-tailed noises described in Theorem 3 is of great
generality and can still be observed locally when the irreducibility condition is removed.
6
Published as a conference paper at ICLR 2022
2.3 Rd EXTENSIONS OF THE THEORETICAL RESULTS
We focused on the R1 case so far, for the clarity of the exposition. In this section, we informally
reiterate that the same effect under truncated heavy-tailed noises persists in high-dimensions. Rigorous
statements are provided in Appendix I. We consider a setting similar to those in Imkeller et al.
(2010a) and analyze the first exit time σ(η, x) = min{k ≥ 0 : Xkη (x) ∈/ G} from an open,
bounded domain G with smooth boundary. For some f : Rd 7→ R, the SGD iterates Xkη (x) =
Xk-I(X) — ψb(Vf (Xη-ι(χ)) + ηZk) are subject to the standard L2 norm clipping with threshold
b > 0, iid noises Zn with heavy tails that resemble 1∕xa with α > 1, and initial condition Xn (x) = x.
Assume that the origin 0 is the only attractor in G for the ODE X(t) = -Vf (x(t)). Also, let IG be
the minimum number of jumps required for the ODE x(t) to escape from G provided that x(0) = 0
and the L2 norm of all jumps are less than b. The following informal version of Theorem I.2 states
that, under the presence of heavy-tailed noises and gradient clipping, the first exit time from a region
in Rd is of order O(1∕η1+(α-1)lG). Therefore, the order of first exit times from different regions
in Rd are still dictated by the geometric characterization & i.e. the minimum number of jumps
required for escape, and those with largest lG may dominate the SGD trajectory as η J 0. Furthermore,
we extend the analysis to the generalized case where the distribution of noises Zn exhibits strong
preference of certain directions and has different heavy-tailed indices α along different directions.
We give the details of the Rd results in Section I and provide a proof in Section J.
Theorem 4 (Informal). Under certain regularity conditions, for Lebesgue almost every b > 0, there
exist q > 0 and λ(η) that is regularly varying w.r.t η with index 1 + (α — 1)12such that
λ(η)σ (η, x) ⇒ Exp(q) as η J 0 ∀x ∈ G .
3 Simulation Experiments
We empirically demonstrate that, (a) as indicated by Theorem 1, the minimum jump number defined
in (3) accurately characterizes the first exit times of the SGDs with clipped heavy-tailed gradient
noises; (b) sharp minima can be effectively eliminated from such SGD; and (c) these properties are
exclusive to heavy-tails. Under light-tailed noises, SGDs are trapped in sharp minima for a long time.
The test function f ∈ C2(R) is the same as in Fig. 1 (Left,e). m1 and m3 are sharp minima in narrow
attraction fields, while m2 and m4 are flatter and located in larger attraction fields. Heavy-tailed
noises have tail index α = 1.2, and light-tailed noises are N(0, 1). See Appendix A for details.
First, we compare the first exit time of heavy-tailed SGD (when initialized at -0.7) from Ω2 =
(—1.3, 0.2) under 3 different clipping mechanism: (1) b = 0.28, where the minimum jump number
required to escape is 1* = 3; (2) b = 0.5, where 1 = 2; (3) no gradient clipping, where 1* = 1
obviously. According to Theorem 1, the first exit times for the aforementioned 3 clipping mechanism
are of order (1∕η)1.6, (1∕η)1.4 and (1∕η)1.2 respectively. These theoretical predictions are accurate
as demonstrated in Figure 1 (Right). Next, we investigate the global dynamics of heavy-tailed SGD.
We compared the clipped case (with b = 0.5) against the case without clipping. Figure 1 (Left, a, b)
show the histograms of the empirical distributions of SGD, and Figure 1(Middle, a,b) plots the SGD
trajectories. Without gradient clipping, Xn still visits the two sharp minima m1 , m3 . Under gradient
clipping, the time spent at m1 , m3 is almost completely eliminated and is negligible compared to
the time Xn spent at m2, m4 in larger attraction fields. This matches the predictions of Theorems
2-3: the elimination of sharp minima with truncated heavy-tailed noises. We stress that the said
properties are exclusive to heavy-tailed SGD. As shown in Figure 1(Left,c,d) and Figure 1(Middle,
c,d), light-tailed SGD are easily trapped at sharp minima for extremely long time.
Figure 3 illustrates the same phenomena in R2, where f has several saddle points and infinitely many
local minima——the local minima of Ω2 form a line segment, which is an uncountably infinite set.
Under clipping threshold b, attraction fields Ωι and Ω2 are the larger ones since the escape from
them requires at least two jumps. This suggests that the theoretical results from Section 2 hold under
more general contexts than Assumptions 1-3. In the next section, we provide experimental evidence
that suggests that truncated heavy-tailed noise improves the generalization of SGD in deep learning.
7
Published as a conference paper at ICLR 2022
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
le7	(C)
(d)
0.0	0.5	1.0	1.5	2.0	2.5	3.0
training Iteration 比7
Figure 3: Experiment result of heavy-tailed SGD when optimizing the modified Himmelblau function.
(a) Contour plot of the test function f. (b) Different shades of gray are used to indicate the area of the
four different attraction fields Ωι, Ω2, Ω3, Ω4 of f. We say that a point belongs to an attraction field
Ωi if, when initializing at this point, the gradient descent iterates converge to the local minima in Ωi,
which are indicated by the colored dots. The circles are added to imply whether the SGD iterates can
escape from each Ωi with one large jump or not under clipping threshold b. (C) The time heavy-tailed
SGD spent at different region. An iterate Xk is considered “visiting” ΩΩ if its distance to the local
minimizer of Ωi is less than 0.5; otherwise we label Xk as “out”. (d) The transition trajectories of
heavy-tailed SGD. The dots represent the last “visited” attraction field at each iteration.
4	Heavy-tailed SGD in Deep Learning: An Ablation Study
In this section, we verify our theoretical results and demonstrate the effectiveness of clipped heavy-
tailed noise in training deep neural networks. Contrary to the report in (SimSekIi et al., 2019a), heavy-
tailed noise may not be ubiquitous in image classification tasks. For instance, the non-Gaussianity
assumption on SGD noise is disputed by experiments in (Panigrahi et al., 2019) for ResNet (see
(He et al., 2016)). For tasks considered in this section, the gradient noise is not heavy-tailed when
models are randomly initialized (see Appendix A). Motivated by the absence of heavy-tailed noises
in image classification, we make the SGD noise heavy-tailed. Let θ be the current model weight
during training, gSB (θ) be the typical small-batch gradient, and gGD (θ) be the true (deterministic)
gradient evaluated on the entire training dataset. Then by evaluating gSB (θ) - gGD (θ) we obtain
a sample of the gradient noise. Due to the prohibitive cost of evaluating gGD (θ), we instead use
gSB (θ) - gLB(θ) as its approximation where gLB denotes the gradient evaluated on a larger batch.
This is justified by the unbiasedness in ELB [gLB (θ)] = gGD (θ). For some heavy-tailed random
variable Z, by multiplying Z with SGD noise, we obtain the following perturbed gradient:
gheavy (θ) = gSB (θ) + Z (gSB*(θ) — gLB (θ))	(6)
where SB and SB* are two mini batches that may or may not be identical. We use the following
update recursion under gradient clipping threshold b: XR+ι = Xn — φb(ηgheavy (Xn)) where 夕b is
the truncation operator. We consider two different implementations: in our method 1 (labeled as “our
1” in Table 1), SB and SB* are chosen independently, while in our method 2 (labeled as “our 2” in
Table 1), we use the same batch for SB and SB*. In summary, by simply multiplying gradient noise
with heavy-tailed random variables, we inject heavy-tailed noise into the optimization procedure.
We conduct an ablation study and benchmark the proposed clipped heavy-tailed methods against
the following optimization methods. LB: large-batch SGD with Xkn+1 = Xkn - ηgLB (Xkn); SB:
small-batch SGD with Xkn+1 = Xkn - ηgSB (Xkn); SB + Clip: the update recursion is Xkn+1 =
Xn — 4b(ηgSB(Xn))； SB + Noise: Our method 2 WITHOUT the gradient clipping mechanism,
leading to the update recursion Xkn+1 = Xkn - ηgheavy (Xkn).
The experiment setting and choice of hyperparameters are adapted from (Zhu et al., 2019). We
consider three different tasks: (1) LeNet (LeCun et al., 1990) on corrupted FashionMNIST (Xiao
et al., 2017), (2) VGG11 (Simonyan & Zisserman, 2014) on SVHN (Netzer et al., 2011), (3) VGG11
on CIFAR10 (Krizhevsky et al., 2009) (see Appendix A for details). Here we highlight a few
points: First, within the same task, for all the 6 candidate methods will use the same η, batch size,
training iteration, and (when needed) the same clipping threshold b and heavy-tailed RV Z for a fair
8
Published as a conference paper at ICLR 2022
Table 1: Test accuracy and expected sharpness of different methods across different tasks. The
reported numbers are the averages over 5 replications. For 95% CL see Appendix A.
Test accuracy	LB	SB	SB + Clip	SB + Noise	Our1	Our2
FashionMNIST, LeNet	68.66%	69.20%	68.77%^^	64.43%	69.47%	70.06%
SVHN, VGG11	82.87%	85.92%	85.95%	38.85%	88.42%	88.37%
CIFAR10, VGG11	69.39%	74.42%	74.38%	40.50%	75.69%	75.87%
Expected Sharpness	LB	SB	SB + Clip	SB + Noise	Our1	Our2
FashionMNIST, LeNet	0.032	0.008	0.009	0.047	0.003	0.002
SVHN, VGG11	0.694	0.037	0.041	0.012	0.002	0.005
CIFAR10, VGG11	2.043	0.050	0.039	2.046	0.024	0.037
Table 2: Our method’s gain on test accuracy persists even when applied with techniques such as data
augmentation and SchedUled learning rates. For 95% CL See APPendix A.______________
CIFAR10-VGG11	Rep 1	Rep 2	Rep 3	Rep 4	Rep 5	Average
SB+Clip	89.40%	89.41%	89.89%	89.52%	89.47%	89.54%
Our1	90.76%	90.57%	90.49%	90.85%	90.79%	90.67%
Our2	90.67%	90.23%	90.52%	90.13%	90.70%	90.45%
CIFAR100-VGG16	Rep 1	Rep 2	Rep 3	Rep 4	Rep 5	Average
SB+Clip	55.76%	56.8%	56.38%	56.35%	56.32%	56.32%
Our1	67.43%	65.12%	65.14%	65.96%	63.57%	65.44%
Our2	67.19%	61.17%	60.97%	64.75%	60.90%	62.99%
comparison; the training duration is long enough so that LB and SB have attained 100% training
accuracy and close-to-0 training loss long before the end of training (the exception here is “SB +
Noise” method; see Appendix A for the details); Second, to facilitate convergence to local minima for
our methods 1 and 2, we remove heavy-tailed noise for last final 5,000 iterations and run LB instead4.
Table 1 shows that in all 3 tasks both our method 1 and our method 2 attain better test accuracy than
the other candidate methods. Meanwhile, both methods exhibit similar test performance, implying
that the implementation of the heavy-tailed method may not be a the deciding factor. We also report
the expected sharpness metric EV 〜N(0炉1)∣L(θ* + V) - L(θ*)∣ used in Zhu et al. (2019); Neyshabur
et al. (2017b) where N(0, δ2I) is a Gaussian distribution, θ* is the trained model weight and L is
training loss. In our experiment, we use δ = 0.01 and the expectation is evaluated by averaging
over 100 samples. We conduct 5 replications for each experiment scenario and report the averaged
performance in Table 1. Smaller sharpness of our methods 1 and 2 confirms that they encourage
minimizers with a “flatter” geometry, thus attaining better test performances.
The ablation study in Table 1 shows that both heavy-tailed noise and gradient clipping are necessary
to find a flat minima and hence achieve better generalization, which is predicted by our analyses.
SB and SB + Clip achieve similar inferior performances, confirming that clipping does not help
when noise is light-tailed. SB + Noise injects heavy-tailed noise without gradient clipping, which
achieves an inferior performance. This poor performance—even after extensive parameter tuning
and engineering (see Appendix A for more details)—demonstrates the difficulty on the optimization
front, especially when heavy-tailed noise is present yet little effort is put into controlling the highly
volatile gradient noises. This is aligned with the observations in Zhang et al. (2020); Gorbunov et al.
(2020) where adaptive gradient clipping methods are proposed to improve convergence of SGD in the
presence of heavy-tailed noises. This confirms that gradient clipping is crucial for heavy-tailed SGD.
Lastly, Table 2 shows that even in the more sophisticated settings with training techniques such
as data augmentations and scheduled learning rates, truncated heavy-tailed SGD still manages to
consistently find solutions with better test performance. For experiment details, see Appendix A. In
Table A.5 we also report the sharpness of the obtained solutions.
4The proposed method can be interpreted as a simplified version of GD + annealed heavy-tailed perturbation,
where a detailed annealing is substituted by a two-phase training schedule. In the first exploration phase the
clipped heavy-tailed noises drive the iterates to explore the loss landscape and identify “wide” attraction fields.
In the second exploitation phase, removing the artificial perturbation accelerates convergence to local minima.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work is partially supported by NSF awards DMS-2134012 and CCF-2019844 as a part of NSF
Institute for Foundations of Machine Learning (IFML).
References
Jeff Alstott, Ed Bullmore, and Dietmar Plenz. powerlaw: a python package for analysis of heavy-tailed
distributions. PloS one, 9(1):e85777, 2014.
Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.
Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical
data. SIAM review, 51(4):661-703, 2009.
Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on
neural networks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065, 2021.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. In International Conference on Machine Learning ,pp.1019-1028. PMLR, 2017.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph,
and Aleksander Madry. Implementation matters in deep rl: A case study on ppo and trpo. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=r1etN1rtPB.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
Saurabh Garg, Joshua Zhanson, Emilio Parisotto, Adarsh Prasad, J Zico Kolter, Zachary Chase
Lipton, Sivaraman Balakrishnan, Ruslan Salakhutdinov, and Pradeep Kumar Ravikumar. On
proximal policy optimization’s heavy-tailed gradients, 2021. URL https://openreview.
net/forum?id=cYek5NoXNiX.
Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-
tailed noise via accelerated gradient clipping. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
15042-15053. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/abd1c782880cc59759f4112fda0b8f98-Paper.pdf.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,
2013.
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. arXiv
preprint arXiv:2006.04740, 2020.
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In
International Conference on Machine Learning, pp. 3964-3975. PMLR, 2021.
Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima.
arXiv preprint arXiv:1902.00744, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural computation, 9(1):1T2, 1997.
Liam Hodgkinson and Michael W Mahoney. Multiplicative noise and heavy tails in stochastic
optimization. arXiv preprint arXiv:2006.06293, 2020.
Peter Imkeller, Ilya Pavlyukevich, and Michael Stauch. First exit times of non-linear dynamical
systems in Rd perturbed by multifractal Levy noise. Journal OfStatistiCaI Physics, 141(1):94-119,
2010a.
10
Published as a conference paper at ICLR 2022
Peter Imkeller, Ilya Pavlyukevich, and Torsten Wetzel. The hierarchy of exit times ofL6vy-driven
Langevin equations. The European Physical Journal Special Topics, 191(1):211-222, 2010b.
Fabian Immler and Christoph Traut. The flow of odes: Formalization of variational equation and
PoinCare map. Journal of Automated Reasoning, 62(2):215-236, 2019.
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural
networks. arXiv preprint arXiv:2002.09572, 2020.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-
aware minimization for scale-invariant learning of deep neural networks. arXiv preprint
arXiv:2102.11600, 2021.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E
Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network.
In Advances in neural information processing systems, pp. 396-404, 1990.
Dawei Li, Tian Ding, and Ruoyu Sun. Over-parameterized deep neural networks have no strict local
minima for any continuous activations. arXiv preprint arXiv:1812.11039, 2018a.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 6391-6401, 2018b.
Filip Lindskog, Sidney I Resnick, Joyjit Roy, et al. Regularly varying measures on metric spaces:
Hidden regular variation and hidden jumps. Probability Surveys, 11:270-314, 2014.
Michael Mahoney and Charles Martin. Traditional and heavy tailed self regularization in neural
network models. In International Conference on Machine Learning, pp. 4284-4293. PMLR, 2019.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM
language models. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=SyyGPP0TZ.
Takashi Mori, Liu Ziyin, Kangqiao Liu, and Masahito Ueda. Logarithmic landscape and power-law
escape rate of sgd. arXiv preprint arXiv:2105.09557, 2021.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. arXiv preprint arXiv:1706.08947, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring gen-
eralization in deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017b. URL https://proceedings.neurips.cc/
paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf.
Than HUy Nguyen, Umut Simsekli, and Gael Richard. Non-asymptotic analysis of fractional langevin
monte carlo for non-convex optimization. In International Conference on Machine Learning, pp.
4810-4819. PMLR, 2019.
Abhishek Panigrahi, Raghav Somani, Navin Goyal, and Praneeth Netrapalli. Non-gaussianity of
stochastic gradient noise. arXiv preprint arXiv:1910.09626, 2019.
11
Published as a conference paper at ICLR 2022
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310-1318. PMLR, 2013.
Ilya Pavlyukevich. Metastable behaviour of small noise l6vy-driven diffusion. arXiv preprint
math/0601771, 2005.
Ilya Pavlyukevich. Cooling down l6vy flights. Journal OfPhysicsA: Mathematical and Theoretical,
40(41):12299, 2007.
Philip E Protter. Stochastic integration and differential equations. Springer, 2005.
Sidney I Resnick. Heavy-tail phenomena: probabilistic and statistical modeling. Springer Science &
Business Media, 2007.
Chang-Han Rhee, Jose Blanchet, Bert Zwart, et al. Sample path large deviations for l6vy processes
and random walks with regularly varying increments. The Annals of Probability, 47(6):3551-3605,
2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Umut Simgekli, Mert Gurbuzbalaban, Thanh Huy Nguyen, Gael Richard, and Levent Sagun. On
the heavy-tailed theory of stochastic gradient descent for deep neural networks. arXiv preprint
arXiv:1912.00018, 2019a.
Umut Sg imsgekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient
noise in deep neural networks. In International Conference on Machine Learning, pp. 5827-5837.
PMLR, 2019b.
Vishwak Srinivasan, Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Kumar Ravikumar.
Efficient estimators for heavy-tailed machine learning, 2021. URL https://openreview.
net/forum?id=5K8ZG9twKY.
Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. part ii:
Continuous time analysis. arXiv preprint arXiv:2106.02588, 2021.
Lei Wu, Chao Ma, et al. How sgd selects the global minima in over-parameterized learning: A
dynamical stability perspective. Advances in Neural Information Processing Systems, 31:8279-
8288, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics:
Stochastic gradient descent exponentially favors flat minima. arXiv preprint arXiv:2002.03495,
2020.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=BJgnXpVYwS.
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, et al. Towards theoretically under-
standing why sgd generalizes better than adam in deep learning. arXiv preprint arXiv:2010.05627,
2020.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
pp. 7654-7663. PMLR, 09-15 Jun 2019. URL http://proceedings.mlr.press/v97/
zhu19e.html.
12
Published as a conference paper at ICLR 2022
A Details of Numerical Experiments
A. 1 DETAILS OF THE R1 SIMULATION EXPERIMENT
The function f used in the experiments is
f(x) = (X + 1.6)(x + 1.3)2(x - 0.2)2(x - 0.7)2(x - 1.6)(0.05∣1.65 - x|)0.6
• (1 + 0.01 + 4(X - 0.5)2 )(1 + 0.1+4(1+1.5)2 )(1 - 1 exp(-5(x + 0⑻(X + 0.8))).
(A.1)
Figure A.1: Illustration of the test function f used in the R1 experiment.
As shown in Figure A.1, the four isolated local minimizers of f are m1 = -1.51, s1 = -1.3, m2 =
-0.66, s2 = 0.2, m3 = 0.49, s3 = 0.7, m4 = 1.32, and in our experiment we restrict the iterates
on [-L, L] with L = 1.6. The heavy-tailed noises we used in the experiment were Zn = 0.1UnWn
where Wn were sampled from Pareto Type II distribution (aka Lomax distribution) with shape
parameter α = 1.2, and the signs Un were iid RVs such that P(Un = 1) = P(Un = -1) = 1/2.
In the first exit time experiment, We tested three different settings: (a) b = 0.28 (So that l* = 3);
(b) b = 0.5 (so that l* = 2); (c) no gradient clipping (so that 传 = 1). For the first case, we
tested learning rates {0.1, 0.05, 0.03, 0.02, 0.01, 0.005, 0.003, 0.001}, While for the other tWo cases,
we tested learning rates {0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001}. For each case, we ran the
simulation 20 times and plotted the average of the 20 exit times. Lastly, to prevent excessively long
running time of the experiment, the simulation was terminated when the iteration number reached
5 × 107. This threshold was reached only in the setting with η = 0.001, b = 0.28.
Next, we present extra sample paths of SGD when applied to function f in eq. (A.1) in Figure A.2
and A.3. The blue curve on the right side of each plot shows f rotated by 90 degrees, and the
dashed lines indicate the locations of local minima. For better readability of the figures, we plotted
Xn for every 5,000 iterations. To generate these plots, we initialized the SGD iterates at 0.3 (so
that it is in Ω3 = (0.2,0.7)) and fixed the learning rate as η = 0.001. Again, we tested both with
gradient clipping (with b = 0.5) and without gradient clipping. Moreover, we also tested light-tailed
noises where we use N(0, 1) as the distribution for noises Zn. For each sample path of Xn, we run
10, 000, 000 iterations. In the left plots of Figure A.2, one can see that with clipped heavy-tailed
stochastic gradients, the SGD iterates almost always stay around the wide attraction fields, and the
sharp minima are almost completely eliminated from the trajectories of SGD. In comparison, in the
right plots of Figure A.2 one can see that without gradient clipping, the heavy-tailed noises will drive
SGD to spend substantial amount of time in all the different local minima, including the sharp ones.
Lastly, from Figures A.3, one can see that under light-tailed noises and small learning rates, SGD
cannot escape a sharp minima once trapped there.
A.2 DETAILS OF THE Rd SIMULATION EXPERIMENT
As illustrated in the contour plot in Figure 3 (a), the function f in this experiment is a modified
version of Himmelblau function, a commonly used test function for optimization algorithm. The
13
Published as a conference paper at ICLR 2022
Figure A.2: Five sample paths of SGD under heavy-tailed noises with gradient clipping (left) and
without gradient clipping (right). Note that in each case, SGD sample paths exhibit similar patters:
SGD almost completely avoided sharp minima with gradient clipping, whereas SGD spent significant
amount of time at the sharp minima without gradient clipping.
Figure A.3: Five sample paths of SGD under light-tailed noises with gradient clipping (left) and
without gradient clipping (right). Note that regardless of the use of gradient clipping, SGD never
manages to escape the local minimum that it started from.
modifications serve two purposes. First, as shown in Figure 3 (b), for the modified function the four
attraction fields Ωι, Ω2, Ω3, Ω4 have different sizes; in particular, under gradient clipping threshold
b = 2.15, from the local minimizers of Ωι and Ω2 (indicated by red dots in the corresponding area)
at least two jumps are required to escape from the attraction field, while from the local minimizer in
Ω3 or Ω4 it is possible to escape with onejump. Therefore, for the minimum jump number required
14
Published as a conference paper at ICLR 2022
to escape, We have l↑ =传=2 > 13 = 14 = 1 in this case. Second, for the modified test function f,
the local minimizer in Ω2 is not a single point but a connected line segment, which is indicated by the
dark line in bottom-left region in Figure 3 (a) and the red line segment in in Figure 3 (b). Therefore,
the modification allows us to test the heavy-tailed SGD methods on a more general loss landscape.
Now we describe the construction of the test function f . Let h be the Himmelblau function with
expression h(x, y) = (x2 + y - 11)2 + (x + y2 - 7)2. Next, define the following transformation
for coordinates: φ(x, y) = x(exp(c0(x - cx) + 1)), y(exp(c0(x - cx) + 1) . Let the composition
be hφ(x, y) = h φ(x - ax, y) . To create the connected region of local minimizers, define the
following locally “cut” version of hφ :
i(x,y) = l{x ∈ [bι,br], |y - ay| < by},
h*(x,y) = (1 - i(χ,y))hφ(χ,y) + i(χ,y) min{hφ(χ,y), ci∣y - ay|1.1}.
In other words, by taking minimum of the original hφ and a polynomial function w.r.t. y around
the original local minimizer of Ω2,we obtain a function h that attains local minimum on an entire
line segment with y = ay. Lastly, the test function we use in the experiment is f = 0.1h*, with
ax = 1.5, ay = -2.9, bl = -5.5, br = -0.5, by = 2.0, c0 = 0.4, c1 = 12.
In the experiment, we initialize the SGD iterates Xk at X0 = (2.9, 1.0), which is very close to the
local minimizer in the small attraction field Ω3. For both the clipped and unclipped SGD, we perform
updates for 3 × 107 steps, under learning rate 5 × 10-4 and heavy-tailed noise Zk = 0.75Wk where
the iid samples Wk are isotropic and the law of kWkk, the size of the noise, is Pareto(1.2). For
clipped SGD, we use threshold b = 2.15. To prevent the iterates from drifting to infinity, after each
update Xk is projected back to the L2 ball centered at origin with radius 4.2 whenever Xk leaves this
ball.
-3	-2	-1	0	1	2	3
(a) with gradient clipping
-3	-2	-1	0	1	2	3
(b) no gradient clipping
Figure A.4: Heat map of SGD iterates when optimizing the modified Himmelblau function.
In Figure A.4, we use the 3 × 107 steps of SGD iterates (for both the clipped and unclipped case)
to create heat maps showing locations of SGD iterates. From this figure, two points can be made
clear: first, the heavy-tailed SGD does spend much less time at the two small attraction fields when
gradient clipping is applied; second, in Ω2 (the bottom-left attraction field) the SGD iterates frequent
the entire connected region of local minima instead of a certain point on this line segment.
A.3 Details of the ablation study
We first mention that the all experiments using neural networks are conducted on Nvidia GeForce
GTX 1080 Ti. For the ablation study, the experiments and scripts are adapted from the ones in Zhu
et al. (2019).5.
In Figure A.5, we display the gradient noise distribution in the three tasks of the ablation study after
the model is randomly initialized.
5https://github.com/UUUjf/SGDNoise
15
Published as a conference paper at ICLR 2022
80-F
0.4	0.6	0.8	1.0	1.2	1.4
(a) Corrupted FashionMNIST
Figure A.5: Distribution of gradient noise in different tasks of the ablation study.
(c) CIFARlOfVGGll
Figure A.6: Test accuracy of the proposed clipped heavy-tailed methods vs. test accuracy of vanilla
SGD in the ablation study.
The experiment setting and choice of hyperparameters are mostly adapted from the experiment in
Zhu et al. (2019). We consider three different tasks: (1) training LeNet on corrupted FashionMNIST
dataset; specifically, we use a 1200-sample subset of the original FashionMNIST training dataset,
and for 200 samples points in the training set we randomly assign a label instead of using the correct
ones; (2) VGG11 on SVHN dataset, where we use a 25000-sample subset of the training dataset; (3)
VGG11 on CIFAR10, where we use the entire training set. For all tasks we use the entire test dataset
when evaluating test accuracy.
Table A.1: Test accuracy (percentage) and expected sharpness of different methods across different
tasks. The reported numbers are the averages and 95%CI over 5 replications.
Test Accuracy	Corrupted FMNIST,LeNet	SVHN, VGG11	CIFAR10, VGG11
LB	68.7±0.4	82.9±0.4	69.4±0.5
SB	69.2±0.8	85.9±0.2	74.4±0.4
SB + Clip	68.8±0.6	85.9±0.2	74.4±0.8
SB + Noise	64.4±3.4	38.9±24.1	40.5±25.1
Our1	69.5±0.8	88.4±0.2	75.7±1.1
Our2	70.1±0.4	88.4±0.2	75.9±0.7
Expected Sharpness	Corrupted FMNIST, LeNet	SVHN, VGG11	CIFAR10, VGG11
LB	0.032±0.006	0.694±0.048	2.043±0.083
SB	0.008±0.001	0.037±0.007	0.050±0.013
SB + Clip	0.009±0.001	0.041±0.006	0.039±0.019
SB + Noise	0.047±0.02	0.012±0.009	2.046±2.4
Our1	0.003±0.0003	0.002±0.0007	0.024±0.005
Our2	0.002±0.0002	0.005±0.004	0.037±0.007
16
Published as a conference paper at ICLR 2022
Table A.2: Hyperparameters for training in the ablation study
Hyperparameters	FashionMNIST, LeNet	SVHN, VGG11	CIFAR10, VGG11
learning rate	0.05	0.05	0.05
batch size for gSB	100	100	100
training iterations	10,000	30,000	30,000
gradient clipping threshold	5	20	20
c	0.5	0.5	0.5
α	1.4	1.4	1.4
Table A.3: Sharpness of different methods across different tasks. The reported numbers are the
averages over 5 replications.____________________________________________________________________________
PAC-Bayes Sharpness	Corrupted FMNIST, LeNet	SVHN, VGG11	CIFAR10, VGG11
LB	5.9 X 103	2.97 × 104	4.87 × 104
SB	3 × 103	6.9 × 103	7.2 × 103
SB + Clip	3.3 × 103	7.3 × 103	6.8 × 103
SB + Noise	3.1 × 103	7.76 × 104	6.74 × 104
Our 1	1.9 × 103	2.1 × 103	4.8 × 103
Our 2	1.6 X 103	2.3 × 103	5.8 × 103
Maximal Sharpness	Corrupted FMNIST, LeNeT	SVHN, VGG11	CIFAR10, VGG11
LB	1.01 × 104	3.78 × 104	5.46 × 104
SB	4.9 × 103	9.1 × 103	8.5 × 103
SB + Clip	5.4 × 103	9.3 × 103	8 × 103
SB + Noise	5.4 × 103	1.19 × 105	1.18 × 105
Our 1	3.2 × 103	2.5 × 103	5.8 × 103
Our 2	2.5 × 103	2.8 × 103	6.5 × 103
The heavy-tailed multipliers Zn used in this experiment, whenever heavy-tailed noise is needed, are
Zn = cWn where Wn are iid Pareto(α) RVs. For each task, we first randomly initialize each model,
and then run the 6 candidate methods in parallel starting from the same randomly initialized model
weights for a fair comparison.
The hyperparameters in training for each task are listed in Table A.2. The same set of hyperparameters
is used for all methods in the same task. Whenever gradient clipping scheme is applied, we clip
the gradient if its L2 norm exceeds the threshold given in Table A.2. The exception here is the “SB
+ Noise” method: we use learning rate η = 0.005; for FashionMNIST task we train for 100,000
iterations and the heavy-tailed noise is removed for the final 50,000 iterations; for SVHN and
CIFAR10 tasks, we train for 150,000 iterations and heavy-tailed noise is removed for the last 70,000
iterations. Besides, for this method we always clip the model weights if its L∞ norm exceeds 1.
The reason for the extra tuning and extended training in “SB + Noise” method is that, without the
said modifications, in all three tasks we observed that the model weights quickly drift to infinity and
explodes; even with the weight clipping implemented, the model performance stays at random level
with no signs of improvements if we do not tune down learning rate.
In Table A.3, we also report the sharpness of solutions under different shaprness metrics. First, the
PAC-Bayes Sharpness metric (see equation (53) in Jiang et al. (2019)) is defined as 1∕σ2 where σ
is equal to the smallest δ that induces a 0.1 expected sharpness, and reflects the sharpness/flatness
parameter used in studies on generalization gaps under the PAC-Bayes framework (see Neyshabur et al.
(2017a)). Besides, the Maximal Sharpness metric (see equation (54) in Jiang et al. (2019)) is defined
as 1∕σ2 where σ is equal to the smallest radius δ that makes maxkνkco≤δ ∣L(θ* + V) - L(θ*)∣ ≥ 0.1,
and metrics of form max∣∣ ν∣∣≤δ ∣L(θ* + V) — L(θ*) | can be considered as a proxy for the spectral norm
of the Hessian at the solution (see Dinh et al. (2017)). It worth noticing that, for all three sharpness
metrics, the smaller the value is the "flatter" the loss landscape is around the solution. Lastly, for
evaluation of the PAC-Bayes Sharpness and Maximal Sharpness metrics, we conduct binary search
as in Algorithm 2 of Jiang et al. (2019) with d = 0.01, σ = 0, M1 = 10 and M2 = 100; in our
setting we always evaluate the training loss using one sweep of the entire training set, so M3 is a
17
Published as a conference paper at ICLR 2022
Table A.4: Results and 95% CI in the experiments with data augmentation.
Test Accuracy	SB + Clip	Our1	Our2
CIFAR10, VGG11	89.5±0.2^^	90.7±0.1	90.5±0.2
CIFAR100, VGG16	56.3±0.3	65.4±1.2	63.0±2.5
Expected Sharpness	SB + Clip^^	Our1	Our2
CIFAR10, VGG11	0.17±0.005	0.09±0.004	0.10±0.003
CIFAR100, VGG16	0.86±0.02	0.44±0.05	0.48±0.07
Table A.5: Sharpness of solutions obtained by different methods in CIFAR10/100 tasks with data
augmentation. Numbers reported here are the average of 5 replications.
CIFAR10-VGG11	SB + Clip	Our1	Our 2
Expected Sharpness	0.167	0.085	0.096
PAC-Bayes Sharpness	1.31 × 104	9 × 103	104
Maximal Sharpness	1.66 X 104	1.29 × 104	1.22 × 104
CIFAR100-VGG16	SB + Clip	Our1	Our 2
Expected Sharpness	0.857	0.441	0.479
PAC-Bayes Sharpness	2.49 × 104	1.9 × 104	1.98 × 104
Maximal Sharpness	2.75 × 104	2.12 × 104	2.16 × 104
case-specific and is equal to the number of batches of the training set under the batch size for the task
at hand.
In Figure A.6, we plot the test accuracy of our method against that of the SGD for all 5 replications
and 3 tasks.
A.4 Details of CIFAR10/100 experiments with data augmentation
For both methods, we train the model for 300 epochs and set the initial learning rate as 0.1. In our
method, the training can be partitioned into two phases. In the first phase (the first 200 epochs),
the learning rate is kept at a constant. In the second phase, for every 30 epoch we reduce the
learning rate by half. Also, an L2 weight decaying with coefficient 5 × 10-4 is enforced. As for
parameters for heavy-tailed noises in eq. (6), we use c = 0.5 and α = 1.4 in the first phase, and
in the second phase we remove heavy-tailed noise and use SB to update weights. In both methods
for the small-batch direction gSB the batch size is 128, while for gLB we evaluate the gradient on
a large sample batch of size 1,024. Under the epoch number 300 and batch size 128, the count of
total iterations performed during training is 1.17 × 105. To augment the dataset, random horizontal
flipping and cropping with padding size 4 is applied for each training batch. Lastly, gradient clipping
scheme is applied for both methods, and we fix b = 0.5. In other words, when the learning rate
is η (note that due to the scheduling of learning rates, η will be changing throughout the training),
the gradient is clipped if its L2 norm is larger than b∕η. The scripts are adapted from the ones in
https://github.com/chengyangfu/pytorch-vgg-cifar10.
These results are presented in Table 2. Furthermore, in Table A.5 we see that our truncated heavy-
tailed method also manages to find solutions with a flatter geometry.
A.5 Discussion on Gradient Noise Distributions in the Experiments
In this subsection, we (i) present the empirical evidence that supports our characterization of the
baseline model—i.e., the absence of heavy tails in the gradient noise—and (ii) clarify that the
emergence of heavy-tails in the *stationary distribution* of SGD (argued in Hodgkinson & Mahoney
(2020); Gurbuzbalaban et al. (2020)) does not contradict the observed absence of heavy tails in the
*gradient noise* of SGD. This allows us to study the impact of truncated heavy-tails in the gradient
noise separately from the choice of hyper-parameters.
We start with our observations on the noise distributions. In view of the well-established wisdom in
heavy-tail literature that there is no single perfect tail estimator, we analyzed the stochastic gradient
noise with four different methods: QQ plot, empirical mean residual life (EMRL), Hill plot, and
18
Published as a conference paper at ICLR 2022
PLFIT (Clauset et al., 2009). We applied these estimation/diagnostic tools (i) at the beginning of
the training, (ii) halfway through the training, (iii) at the end of the training. Throughout all our
experiments, we consistently observe strong evidence that the gradient noises are light-tailed, and
even if (against all odds) the noises were from a heavy-tailed distribution, the tail index should be far
greater (hence, the resulting tail is much lighter) than the heavy-tails we inject (or the popular alpha
stable assumption), and hence, the point we make with our tail-inflation experiment is still valid. We
summarize the results as follows.
First, QQ plots below (Fig. A.7-A.21) clearly show that the tails in noise distribution are always
much lighter than the Pareto distributions with α = 2 or even 10. Therefore, the typical power-law
assumption, especially the alpha-stable distributions in SimSekli et al. (2019b) (with α ∈ (0, 2)),
seems far from the distribution of the actual data we obtained in the image classification tasks. In fact,
the tail of the noise distributions seems to be between that of lognormal and normal distributions,
implying that it is lighter than any power-law distribution.
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.7: Ablation Study, Corrupted FMNIST & LeNet: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10
Lognormal
Normal
Figure A.8: Ablation Study, Corrupted FMNIST & LeNet: Half way through the training
Pareto, α = 2
Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.9: Ablation Study, Corrupted FMNIST & LeNet: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.10: Ablation Study, SVHN & VGG11: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.11: Ablation Study, SVHN & VGG11: Half way through the training
19
Published as a conference paper at ICLR 2022
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.12: Ablation Study, SVHN & VGG11: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.13: Ablation Study, CIFAR10 & VGG11: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.14: Ablation Study, CIFAR10 & VGG11: Half way through the training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.15: Ablation Study, CIFAR10 & VGG11: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.16: Data Augmentation, CIFAR10 & VGG11: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.17: Data Augmentation, CIFAR10 & VGG11: Half way through the training
Next, we plotted the empirical mean residual life (EMRL) of the gradient noise distributions in
Fig. A.22-A.26. It is well known that the mean residual life blows up to infinity if and only if the
distribution is heavy-tailed (more precisely, long-tailed). However, from the figures, one can see that
none of the EMRL exhibits such a pattern in any case tested in our experiments. Instead, we see clear
downward trends, which strongly suggest light tails, in all the tested cases.
20
Published as a conference paper at ICLR 2022
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure A.18: Data Augmentation, CIFAR10 & VGG11: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10
Lognormal
Normal
Figure A.19: Data Augmentation, CIFAR100 & VGG16: At the beginning
Pareto, α = 2
Pareto, α = 5 Pareto, α = 10 Lognormal
Normal
Figure A.20: Data Augmentation, CIFAR100 & VGG16: Halfway through the training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal
Normal
Figure A.21: Data Augmentation, CIFAR100 & VGG16: At the end of training
Figure A.22: Plots of empirical mean residual life for noises in FMNIST&LeNet Task throughout
training
Figure A.23: Plots of empirical mean residual life for noises in SVHN&VGG 11 Task throughout
training
21
Published as a conference paper at ICLR 2022
Figure A.24: Plots of empirical mean residual life for noises in CIFAR 10&VGG 11 Task throughout
training
Figure A.25: Plots of empirical mean residual life for noises in dataAug, CIFAR 10&VGG 11 Task
throughout training
Figure A.26: Plots of empirical mean residual life for noises in dataAug, CIFAR 100&VGG 11 Task
throughout training
If we assume a power-law tail, the Hill estimator is a classical tail index estimator with a long
history in extreme value theory literature. A critical algorithmic parameter of the Hill estimator is
the number of order statistics used in the estimation, and the Hill plot is a popular exploratory tool
for investigating the Hill estimators with different numbers of order statistics. Although it is well
known that Hill estimators and Hill plots are fallible if the power-law assumption is not satisfied, (and
hence, it is not suited for deciding whether a given set of samples are from light-tailed distribution or
heavy-tailed distribution; in particular, the method will return some power-law tail index α whether
or not the samples are from a heavy-tailed distribution or a light-tailed one) we present the Hill plot
to see what would be the estimated tail indices if the gradient noises hypothetically followed a power
law. In the Hill plots shown in Fig. A.27-A.31, we presented the rescaled version (altHill) of the Hill
plots (see Chapter 4.4 in (Resnick, 2007)) for the following reason. Hill estimator is a consistent
estimator of the power-law index when the proportion of the samples used approaches 0, and altHill
plots allow us to scrutinize the estimated indices under a small proportion of samples. In particular,
the points around the red dashed lines correspond to estimation using the top 1% of the samples. We
can see that most Hill plots stay well above 10 for the most part and almost never drop below 2. This
strongly suggests that even if the gradient noises are from a heavy-tailed distribution, it is likely to
have a very high power-law index (implying relatively lighter tails), and hence, we cannot expect to
observe a prominent heavy-tailed behavior from them.
A popular data-driven approach with statistical guarantees (again, under the assumption that the
samples are indeed from a heavy-tailed distribution) to select the number of order statistics in the
Hill plot is PLFIT (Clauset et al., 2009). We estimated the power-law indices using the python
implementation (Alstott et al., 2014) of PLFIT. The numbers are presented in Table A.6. All the
22
Published as a conference paper at ICLR 2022
Figure A.27: altHill Plots for noises in FMNIST&LeNet Task throughout training. Dashed Red Line:
Estimation based on the largest 1% data
Figure A.28: altHill Plots for noises in SVHN&VGG 11 Task throughout training. Dashed Red Line:
Estimation based on the largest 1% data
Figure A.29: altHill Plots for noises in CIFAR 10&VGG 11 Task throughout training. Dashed Red
Line: Estimation based on the largest 1% data
Figure A.30: altHill Plots for noises in Data Augmentation, CIFAR 10&VGG 11 Task throughout
training. Dashed Red Line: Estimation based on the largest 1% data
estimations are at least 5 for all cases tested in our experiments, and most of the time, the estimation
is above 10. Again, this means that even under the hypothetical assumption (against what the QQ
plots and EMRLs suggest) that the gradient noises were from a heavy-tailed distribution, the tail
indices of the gradient noises should be large, and hence, the gradient noises in our experiments
have much lighter tails than any α-stable distribution (which requires α < 2) or the heavy-tailed
noises we injected during tail inflation experiments (α = 1.4). In summary, extensive statistical
analyses above suggest the absence of heavy tails in the gradient noises in our experiments. Therefore,
the characterization of the vanilla SGD as the light-tailed (or at least lighter than the inflated tails)
benchmark in our experiments is valid, and our ablation study is well grounded.
23
Published as a conference paper at ICLR 2022
Figure A.31: altHill Plots for noises in Data Augmentation, CIFAR 100&VGG 16 Task throughout
training. Dashed Red Line: Estimation based on the largest 1% data
Table A.6: Power-law Indices Estimation throughout the Training, using PLFIT. All the estimations
are at least 5 for all cases tested in our experiments, and most of the times the estimation is above
10. This means that even under the assumption that the gradient noises were from a heavy-tailed
distribution, they should have much lighter tails than any α-stable distribution (which requires α < 2)
or the heavy-tailed noises We injected during tail inflation experiments (α = 1.4).
Task	Beginning	Middle	End
FMNIST,LeNet	14.3	14.2	16.5
SVHN, VGG11	5.0	5.2	12.5
CIFAR10, VGG11	9.2	6.6	7.0
dataAug, CIFAR10, VGG11	16.2	16.2	8.5
dataAug, CIFAR100, VGG16	35.1	14.4	5.35
Next, we compare our work to recent literature on heavy-tailed phenomena in stationary distribution
of SGD; see, for instance, Hodgkinson & Mahoney (2020); Gurbuzbalaban et al. (2020). To be
specific, Hodgkinson & Mahoney (2020); Gurbuzbalaban et al. (2020) shoW that heavy tails can
arise in the stationary distribution of SGD through multiplicative dynamics, and the tail index of the
resulting stationary distribution can be characterized by the learning rates and the magnitude of noises
(through batch size). HoWever, the results in Hodgkinson & Mahoney (2020); Gurbuzbalaban et al.
(2020) do not imply the existence of heavy tails in the gradient noises. In both papers (as Well as other
Works in the literature), the “heaviness” of the tail of the gradient noise (e.g., poWer-laW index α in
heavy-tailed cases) is fixed in the model (same as in our setting) and not entangled With the learning
rate or batch size. For example, Bk in (5) of Hodgkinson & Mahoney (2020) corresponds to the
gradient noise, and its tail index doesn’t depend on the learning rate or the batch size. In particular,
the change of learning rate does not induce heavy tails in Bk . On the other hand, We focus on the
impact of the heavy-tails *in the gradient noise* and the truncation scheme for (any) fixed batch size
and small learning rates on the global dynamics of SGD. In vieW of this, it should be clear that our
analysis can be decoupled from the choice of batch size or the impact of the learning rate on SGD’s
stationary distribution. Therefore, our observation and the design of the experiments are compatible
With the aforementioned references.
B Implications of the theoretical results
Systematic control of the exit times from attraction fields: In light of the Wide minima folklore,
one may Want to find techniques to modify the sojourn time of SGD at each attraction field. Theorem 1
suggests that the order of the first exit time (W.r.t. learning rate η) is directly controlled by the gradient
clipping threshold b. Recall that for an attraction field with minimum jump number l*, Theorem 1
tells us the exit time from this attraction field is roughly of order (1∕η)1+(α-1)l*. Given the width
of the attraction field, its minimum jump number l* is dictated by gradient clipping threshold b.
Therefore, gradient clipping provides us with a very systematic method to control the exit time of each
attraction field. For instance, given clipping threshold b, the exit time from an attraction field with
width less than b is of order (1∕η)α, while the exit time from one larger than b is at least (1∕η)2α-1,
which dominates the exit time from smaller ones.
24
Published as a conference paper at ICLR 2022
The role of structural properties of G and f : Recall that in order for Theorem 3 to apply, the
irreducibility of G is required. Along with the choice of b, the geometry of function f is a deciding
factor of the irreducibility. For instance, We say that G is symmetric if for any attraction field Ωi such
that i = 2, 3, ∙∙∙ , nmi∏ - 1 (so that Ωi is not the leftmost or rightmost one at the boundary), We have
qi,i-ι > 0, qi,i+ι > 0. One can see that G is symmetric if and only if, for any i = 2,3, ∙∙∙ , nmin - 1,
|si - mi| ∨ |mi - si-ι | < l*b, and symmetry is a sufficient condition for the irreducibility of G. The
graph illustrated in Figure 2(Middle) is symmetric, While the one in Figure 2(Right) is not. As the
name suggests, in the R1 case the symmetry of G is more likely to hold if the shape of attraction
fields in f is also nearly symmetric around its local minimum. If not, the symmetry (as Well as
irreducibility) ofG can be violated as illustrated in Figure 2, especially When a small gradient clipping
threshold b is used.
Generally speaking, our results imply that, even With the truncated heavy-tailed noises, the function
f needs to satisfy certain regularity conditions to ensure that SGD iterates avoid undesirable minima.
This is consistent With the observations in Li et al. (2018b): the deep neural nets that are more
trainable With SGD tend to have a much more regular structure in terms of the number and shape of
local minima.
Heavy-tailed SGD without gradient clipping: It is Worth mentioning that our results also charac-
terize the dynamics of heavy-tailed SGDs Without gradient clipping. For instance, since the reflection
operation at ±L restricts the iterates on the compact set [-L, L], if We use a truncation threshold b
that is large than 2L, then any SGD update that moves larger than b Will definitely be reflected at ±L.
Therefore, the dynamics are identical to that of the folloWing iterates Without gradient clipping:
χn,uncliPPed =夕L (Xn-IClipped - ηf 0(Xn-Tipped) +
(B.1)
The next result folloWs immediately from Theorem 1 and H.2.
Corollary B.1. There exist constants qi > 0 ∀i, qi,j > 0 ∀j 6= i such that the following claims hold
for any i and any X ∈ Ωi.
1)	Under Px, qiH(1∕η)σi(η) converges in distribution to an Exponential random variable
with rate 1 as η J 0;
2)	For any j = 1, 2, •…,nmin such that j = i,
lim px(-x2i (η) ∈ ω ) = qi,j /qi.
ηψ0
3)	Let Y be a continuous-time MarkOv chain on {mι, ∙∙∙ , mn,min} With generator matrix Q
parametrized by Qi,i = -qi, Qi,j = qi,j. Then
XMI eηd)c(x) → Yt(mi) as η J 0
in the sense of finite-dimensional distributions.
At first glance, Corollary B.1 may seem similar to the results in SimSekIi et al. (2019a) and Pavlyuke-
vich (2007). However, the object studied in SimSekIi et al. (2019a); Pavlyukevich (2007) is different:
they study the folloWing Langevin-type stochastic differential equation (SDE) driven by an α-stable
LeVy process Lt with scaling factor η > 0:
dYtη = -f0(Ytη-)dt+ηdLt.
In particular, Pavlyukevich (2007) studies the metastability of Ytη and concludes that as η J 0, the first
exit time and global dynamics of Ytη admit a similar characterization as described in our Theorem
B.1. Then Theorem 4 in SS imsSekli et al. (2019a) argues that when the learning rate η is sufficiently
small, the distribution of the first exit time of the SGD Xnη and that of the Levy-driven Langevin SDE
Ytη are similar. However, the analysis of SS imsSekli et al. (2019a) hinges critically on the assumption
that Ltα is symmetric and α-stable. While such an assumption is convenient for their analysis, it is a
strong assumption. It implies that the gradient noise distribution belongs to a very specific parametric
family and excludes all the other heavy-tailed distributions. In particular, the assumption precludes
analysis of any heavy tails with finite variance. On the contrary, our work directly analyzes the SGD
Xnη and reveals the heavy-tailed SGD dynamics at a much greater level of generality. Specifically,
we allow the noise to have general regularly varying distributions with arbitrary tail index—which
includes α-stable distributions as a (very) special case—and extend the characterization of global
dynamics of heavy-tailed SGD to the adaptive versions of SGD where gradient clipping is applied.
25
Published as a conference paper at ICLR 2022
C Geometric Characterization l* and Existing Sharpness Metrics
Throughout the paper, we have been using the term sharp minima when describing the elimination
effect of truncated heavy-tailed SGD, while appealing to l*, the minimum number of jumps requires
for escape defined in eq. (3), when rigorously presenting our theoretical results about dynamics
of truncated heavy-tailed sGD. Despite the possible ambiguity of its use in the main paper, the
terminology sharpness is meant to familiarize our rather technically involved notion of how heavy-
tailed sGD behave. To resolve the potential confusions, in this section we provide a detailed
discussion on the relationship between our geometric characterization l* and the existing sharpness
metrics.
in light of recent discovery that all local minima might be global minima for over-parametrized
deep neural nets (see Li et al. (2018a)), one popular explanation for the generalization gap between
different local minima (for instance, between solutions found by GD and sGD) is that the geometry
around the solution is closely related to its performance in the test setting. After the seminal work
by Keskar et al. (2016), a myriad of attempts have been made to provide empirical or theoretical
evidences for the link between the sharpness of a local minimum and its generalization performance;
see, for instance, Xie et al. (2020); Jiang et al. (2019); He et al. (2019); Zhou et al. (2020). in
summary, there exist at least four different classes of sharpness metrics among the current literature:
(a)	spectral norm of the Hessian at the local minimum, one surrogates of which is the maximal
sharpness type of metrics; a variant used in Xie et al. (2020) based on corresponding large
deviation theory is the eigenvalue of the Hessian along certain escape directions;
(b)	expected sharpness type of metrics that evaluate the general fluctuation of the loss function
within a Lp ball of centered at the local minimum (see Zhu et al. (2019));
(c)	complexity metrics based on PAc-Bayes theory (see, for example, Neyshabur et al. (2017a));
(d)	geometric property of a domain or the entire attraction field instead of the local minimum
itself; see the mass of a Radon measure m(W) over the domain W in Theorem 1 in Zhou
et al. (2020).
Under such taxonomy, the l* characterization proposed in this paper falls into category (d). indeed,
it describes the minimum effort required for escaping the domain rather than the geometry merely
around a certain neighborhood of the local minimum. Here we have two remarks on the definition
of l*. First, for a given loss landscape, this quantity l* is dictated by the gradient clipping threshold
b > 0. in fact, this concept is tailored for the gradient clipping case, as in the unclipped case with
heavy-tailed SGD, for any attraction field Ωi we always have l* = 1. Second, it is analogous to
the term m(W) in Zhou et al. (2020) in the sense that it reflects the volume of the attraction field
(when compared to a given threshold b). Therefore, we stress that a more precise interpretation of
l*, as well as other metrics in class (d), is that they characterize how wide or narrow each minimum
(attraction field) is, and a more a clear description of our theoretical results Theorem 1-3 is that
truncated heavy-tailed SGD effectively avoids all the narrow minima when learning rate is small
enough.
While one can intuitively see that wide minima are more likely to be flat ones, we acknowledge the
existence of counterexamples where a sharp minimum lies in a wide attraction field. Nevertheless,
as the variety of definitions for sharpness keeps growing in existing literature, it becomes rather
unlikely for one geometric characterization to always agree with (or be equivalent to) other existing
approaches. More importantly, the aim of this paper is not to establish l* as the orthodox geometric
property when studying generalization gap. instead, as demonstrated in Table 1, A.3 and A.5, in
modern deep learning tasks the truncated heavy-tailed method (which prefers solutions with high
l* as indicated by our theoretical results) obtains solutions that generalize better and exhibit flatter
geometry when evaluated under different sharpness metrics. This observation is well aligned with the
recent large-scale empirical study in Jiang et al. (2019), suggesting that in typical training setting it is
beneficial to find wider minima (characterized by l*) in order to achieve a flatter geometry and better
generalization performance.
26
Published as a conference paper at ICLR 2022
D	On the Relationship B etween Sharpness and Generalization
In principle, a sharp minimum does not necessarily lead to poor generalization in the sense that it
is possible to construct pathological counterexamples in theory or in specific experiment settings;
see, for example, Dinh et al. (2017); Neyshabur et al. (2017a). However, the loss geometry that
arises in practice seems to exhibit a strong correlation between the sharpness and the test error. For
instance, sharpness-aware optimization methods (Foret et al., 2020; Kwon et al., 2021) improve
generalization across various tasks and achieve state-of-the-art performance on the CIFAR dataset.
Also, one of the authors of the aforementioned paper Neyshabur et al. (2017a) continued investigating
different complexity measures and conducted much larger scale experiments in Jiang et al. (2019).
Among more than 40 complexity measures from theoretical and empirical studies in the literature
tested in Jiang et al. (2019), the sharpness metrics are the top ones for predicting the generalization
performance. In our own experiments, we also show that sharpness and test accuracy are highly
correlated.
E	Stability-driven Analyses on SGD
Wu et al. (2018) takes the perspective of linear stability and establishes conditions for SGD to be
attracted to or avoid certain solutions based on learning rate, batch size, and the concept of non-
uniformity of local minima. Inspired by (Wu et al., 2018), (Jastrzebski et al., 2020) analyzes the
trajectory-wise stability of SGD and found that a “break-even point” partitions the training procedure
into two phases: the implicit regularization effects in SGD due to a larger learning rate becomes
visible in the second phase after this “break-even point”. Similarly, (Cohen et al., 2021) reports that
typical GD trajectories in standard image classification tasks can be partitioned into two phases: in
the first “progressive sharpening” phase the sharpness of the Hessian monotonically increases, while
in the second phase we observe the “edge of stability” regime where the sharpness of Hessian hovers
slightly above the critical value 2/n and the training loss slowly decrease in an oscillating fashion.
Compared to the aforementioned stability analyses, one major difference of our heavy-tailed regime
is that, even under gradient clipping, the trajectory cannot be partitioned into such “phases” and the
traditional sense of stability around certain local minima is nullified by the constant basin hopping
and exploration behaviors under heavy-tailed noises. In particular, the polynomial order of the exit
time in heavy-tailed SGD dictates that, when compared to exponentially long exit time in vanilla
SGD (see Xie et al. (2020)), the iterates won’t become “stabilized” and keep staying around a
certain region in a typical training procedure. Instead, our work characterizes the metastability of
the truncated heavy-tailed SGD (i.e. constantly transitioning between different wide minima) that
cannot be observed in GD or light-tailed SGD. More importantly, compared to the aforementioned
stability related analyses, our work provides a very tight characterization of the global dynamics and
distributions of the entire sample path.
F Existing Analyses on Heavy-tailed Phenomena in SGD
We start with one clarification: what has been established in Hodgkinson & Mahoney (2020);
Gurbuzbalaban et al. (2021); Wojtowytsch (2021); Mori et al. (2021) is that the stationary distribution
of the SGD (or the corresponding continuous-time SDE) can be heavy-tailed, and the heavy tailes
therein can depend on various training hyperparameters. This, however, does not imply that the
gradient noise is heavy-tailed. In particular, the heavy-tail index (or heavy-tailedness itself) of the
gradient noise will not be affected by hyperparameters such as the learning rate.
To be specific, Hodgkinson & Mahoney (2020); Gurbuzbalaban et al. (2021) show that heavy tails
can arise in the stationary distribution of SGD through multiplicative dynamics, and the tail index of
the resulting stationary distribution can be characterized by the learning rates and the magnitude of
noises (through batch size). Similarly, the SDEs studied in Wojtowytsch (2021); Mori et al. (2021) are
both driven by light-tailed (Gaussian) perturbations, and the authors show that even under light-tailed
perturbations, heavy tails can arise in the stationary distribution through multiplicative dynamics, and
the tail index of the resulting stationary distribution can be characterized by the learning rates. In
comparison, we focus on the impact of the heavy-tails in the gradient noise and the truncation scheme
on the global dynamics of SGD. Our results show that, under heavy-tailed noises, the power-law
27
Published as a conference paper at ICLR 2022
index α for the tail in noises also characterizes the first exit time and global dynamics of SGD
with no dependence on other training hyperparameters (at least in the asymptotic scheme). In other
words, when the driving force of the dynamics is heavy-tailed, its “heavy-tailedness” (i.e., the same
index α) characterizes the “heavy-tailedness” of the entire SGD without dependency on the other
hyperparameters, and its effect on the entire SGD path (rather than just the stationary distributions)
admits a very clear and tight expression, as characterized in theoretical results in this work.
G Proof of Theorem 1
This section proves Theorem 1. We first start with providing the definitions of qi and qi,j that appear
in the statement of the theorem. Let Leb+ denote the Lebesgue measure restricted on [0, ∞), and
define a (Borel) measure να on R\{0} as follows:
Vamx) = ι{x> 0} Xαp+1+ ι{x< 0}∣αp+ι
where α, P-, and p+ are constants in Assumption 2. Define a Borel measure μi on Rl^ X
as the product measure
μi = (VaW × (Leb+卢-1
(G.1)
We also define mappings h as follows. For a real sequence W = (w1,w2, ∙∙∙ ,w琴)and a positive
l *
real number sequence t = (tjj=2, define tι = t； = 0 and tj = t； +12 +-+ tj for j = 2, ∙∙∙ ,4.
Now we define a path X : [0, ∞) → R as the solution to the following ODE with jumps:
X(0)=夕 L(mi + 夕 b(wι));	(G.2)
* = -f0(X(t)), ∀t ∈ [tj-1,tj), ∀j = 2,…，4;	(G.3)
X(tj) =2L(X(tj-) + Pb(Wj)), ∀j = 2,…，li.	(G.4)
l*-；
Now we define the mappings hi : Rli × R+	7→ R as
hi (w,t) = X(tl*).
It is easy to see that hi ’s are continuous mappings. With these mappings, we define the following
sets:
l*-；
Ei = {(w, t) ⊆ Rli ×(R+)	： hi(w, t) ∈ Ωi};	(G.5)
Rli*-；
:hi(w, t) ∈ Ωj}.	(G.6)
Lastly, the constant qi and qi,j are defined as follows:
qi = μi(Ei), qij = μi(Eij) ∀i = j.	(G.7)
Before we move on to the proof of Theorem 1, we add a few remarks regarding the intuition behind
it.
• Suppose that Xnη is started at the ith local minimum mi of f, and consider the behavior of
Xn over the time period H；，{1,..., dt∕η∖} for a sufficiently large t. The heavy-tailed
large deviations theory Rhee et al. (2019) and a heuristic application of the contraction
principle implies that the path of XInn over this period will converge to the gradient
flow of f, and the event that Xnn/m escapes Ωi within this period is a (heavy-tailed) rare
event. This means that the probability of such an event is of order (1∕η)(a-1)l*. Moreover,
whenever it happens, it is almost always because Xn is shaken by exactly l* large gradient
noises of size O(1∕η), which translates to l* jumps in Xnn/m 's path, while the rest of its
path closely resemble the deterministic gradient flow. Moreover, conditional on the event
28
Published as a conference paper at ICLR 2022
that Xn fails to escape from Ωi within this period, the endpoint of the path is most likely to
be close to the local minima, i.e., XItn ≈ mi. This suggests that over the next time period
H，{dt∕η] + 1, d~t∕η∖ +2,..., 2「t/n]} of length d't∕η∖, Xn will behave similarly to its
behavior over the first period H1. The same argument applies to the subsequent periods
H, H4,... as well. Therefore, over each time period of length dt∕η∖, there is (1∕η)(α-1)l^
probability of exit. In view of this, the exit time should be of order (1∕η)1+(α-1)li and
resemble an exponential distribution when scaled properly.
•	Part (i) of Theorem 1 builds on this intuition and rigorously prove that the first exit time is
indeed roughly of order 1∕λi(η) ≈ (1∕η)1+(α-D4 and resembles an exponential distribu-
tion.
•	Given this, one would expect that Xση (η), the location of SGD right at the time of exit, will
hardly ever be farther than lib away from m%: the length of each update is clipped by b, and
there will most likely be only li large SGD steps during this successful attempt. Indeed, from
the definition of q%,j,s above, one can see that q%,j > 0 if and only if infy∈Ωj |y — mi | < lib.
Summarizing the three bullet points here, we see that the minimum number of jumps lii dictates how
heavy-tailed SGD escapes an attraction field, where the SGD lands on upon its exit, and when the
exit occurs.
Now we are ready to start proving Theorem 1. First, note that Assumption 1 implies the following:
•	There exist co > 0, €0 ∈ (0,1) such thatforany X ∈ {mι, si,…，Snmin-1, mnmhl}, |y—x| <
0,
|f 0(y)| > c0|y —x|,	(G.8)
and for any y ∈ [—L, L] such that |y - x| ≥ eo for all X ∈ {mi, si,…，Snmin-1, mnm1},
we have
|f0(y)| > c0;	(G.9)
•	There exist constants L ∈ (0, ∞), M ∈ (0, ∞) such that |m0| < L, |mnmin | < L, and (for
any X ∈ [—L, L])
|f0(X)| ≤ M, |f00(X)| ≤ M.	(G.10)
Recall that for c > 0, the truncation operator was defined as
夕c(w)，夕(w, C) = (W ∧ C) ∨ (—c), ∀w ∈ R,	(G.11)
and the SGD iterates were defined as
Xn =中 L (Xn — ψb{η(f 0(χn)—。八+。)).	(G.12)
Here η > 0 is the learning rate (step length) and b > 0 is the gradient clipping threshold. Also, recall
that for any k ∈ [nmin], we let
σk(η) = min{n ≥ 0: Xn ∈ Ωk}
to be the time that Xn exists from the k—th attraction field Ωj. Meanwhile, given the gradient
clipping threshold b, recall that
ri =∆ min{mi — si-i, si — mi},	(G.13)
lii =∆ dri∕b∖.	(G.14)
Intuitively speaking, lii tells us the minimum number of jumps with size no larger than b required
in order to escape the attraction field if we start from the local minimum of this attraction field Ωi.
Lastly, recall the definition of H(∙) = P(|Zi | > ∙) and
x( 、△ “ 7H(1∕η八般-i
λi (η) = H(I∕η^ —η—)	.
29
Published as a conference paper at ICLR 2022
The proof of Theorem 1 hinges on the following two lemmas that characterize the behavior of Xnη
in two different phases respectively. Let k ∈ [nmin] and X ∈ Ωk. We consider the SGD iterates
initialized at X0η = x. In the first phase, the SGD iterates return to [mk - 2, mk + 2], a small
neighborhood of the local minimizer in attraction field Ωk; in other words, it ends at
Tr(ektu)rn(η, ) =∆ min{n ≥ 0 : Xnη ∈ [mk - 2, mk + 2]}.	(G.15)
During this phase, we show that for all learning rate η that is sufficiently small, it is almost always
the case that Xn would quickly return to [mk - 2e, mk + 2e], and it never leaves Ωk before Tekr「
Lemma G.1. There exists some c ∈ (0, ∞) such that for any k ∈ [nmin], the following claim holds
for all > 0 small enough:
iim r 1 inf	Py(Xn ∈ a®Yn∈ Hrκm,e)],τɪma ≤
自 y∈[-L,L]: y∈(sk-1+3sk-e)	、
C log(1∕e)) = 1
η.
During the second phase, Xnη starts from somewhere in [mk - 2, mk + 2] and tries to escape from
Ωk, meaning that the phase ends at o® (η). During this phase, we show that the distributions of the
first exit time σk(η) and the location Xση (η) do converge to the ones described in Theorem 1 as
learning rate η tends to 0.
Lemma G.2. There exist constants qi > 0 ∀i ∈ [nmin ] and qi,j ≥ 0 ∀j ∈ [nmin ] with i, j 6= i such
that the following claim holds: given any C > 0, u > 0 and any k, l ∈ [nmin ] with k 6= l, we have
limsup	sup	Px (q®λk(η)σk(η) > u) ≤ C + exp ( — (1 — C)U)	(G.16)
n，0	x∈[-L,L], x∈(mk-2e,mk + 2e)	'	/
liminf	inf	Px (q®λk(η)σk(η) > u) ≥ -C + exp ( - (1 + C)u) (G.17)
n，0	x∈[-L,L], x∈ (mk - 2e,mk + 2e)	∖	)
limsup	sup	Px(xσk(n)	∈	Ωι)	≤	qk'l + C	(G.18)
ηψ0 x∈[-L,L], x∈(mk - 2e,mk + 2e)	、	q	qk
liminf	inf	Px(Xη ( )	∈	Ωl)	≥	qk,l - C	(G.19)
n，0 x∈[-L,L], x∈(mk-2e,mk + 2e)	'	σk (η)	)	qk
for all > 0 that are sufficiently small.
Now we are ready to show Theorem 1.
ProofofTheorem 1. Fix some k ∈ [nmi∩] and X ∈ Ωk ∩ [-L, L]. Let qk and q®,i be the constants in
Lemma G.2.
We first prove the weak convergence claim in Theorem 1(i). Arbitrarily choose some u > 0 and
C ∈ (0, 1). It suffices to show that
limsupPx(qkλk(η)σk(η) > u) ≤ 2C + exp ( - (1 - C)u),
ηψo
liminfPx(q®λk(η)σk(η) > u) ≥ (1 - C)( - C + exp ( -(1 + C)u))∙
Recall the definition of the stopping time Tr(e®tu)rn in eq. (G.15). Define event
Ak(η,e) = {Xn ∈ Ωk ∀n ∈ WekU)rn(η, e)], Tek)ra(η,e) ≤ ciog^ }
where c < ∞ is the constant in Lemma G.1. First, since X ∈ Ωk = (sk-ι, Sk), it holds for all e > 0
small enough that X ∈ (sk-1 + , sk - ). Next, one can find some > 0 such that
• (Due to Lemma G.1)
Px((Ak(η, c))c) ≤ C ∀η sufficiently small;
• (Due to eq. (G.16)eq. (G.17) and strong Markov property) For all η sufficiently small,
Px(qkλk(η)(σ(η) - Tek)m(η,e)) > (I- C)U Ak(η, e)) ≤ C + exp ( - (1 - C)u),
Px(qkλk(η)(σ(η)- Tek)rn(η, e)) > u Ak(η, e)) ≥ -C + exp (-(1 + C)u).
30
Published as a conference paper at ICLR 2022
Fix such e. Lastly, for this fixed e, due to λk ∈ RV—1—成(α—i)(η) and α > 1, we have qkλk (η) ∙
C 弋1")< Cu for all η sufficiently small. In summary, for all η sufficiently small, we have
Pχ(qkʌk(η)σk(η) > u)
≤Pχ((Ak(η, e))c) + Pχ({qCk(η)σk(η) > u} ∩ Ak(η, e))
≤C + Px ({qkλk(η)σk(η) > u} ∩ Ak(η, E))
=C + px(qkλkm)Sm)- Tek)√η, E)) > u - qkλk(η)Teωrn(η, e) I Ak(η,e, ∙ Px(Ak(η, e))
≤c + px(qkλk(η)(σ(η) - Tek)4η, E)) > (1 - C)u I Ak(η, E))
≤2C + exp ( - (1 - C)u)
and
Px(qkλk(η)σk(η) > u)
≥Px ({qkλk(η)σk(η) > u} ∩ Ak(η, E))
=px(qkλk(η)(σ(η)-	TekUrn(η, E))	> u -qkʌk(η)Tek)rn(η,E)	Ak(η,	E))	∙ Px(Ak(η,E))
≥px(qkλk(η)(σ(η)-	TekUrn(η, E))	> u -qkλk(η)Tek)rn(η,E)	Ak(η,	E))	∙(1 - C)
≥Px(qkλk(η)(σ(η) - TekUrn(η, E)) > U ∣ Ak(η, E)) ∙(I- C)
≥(1 - C) ( - C + exp ( - (1 + C)u))
so this concludes the proof for Theorem 1(i).
In order to prove claims in Theorem 1(ii), we first observe that on event Ak (η, e) we must have
σ(η) > Tek)rn(η, e). Next, arbitrarily choose some C ∈ (0,1), and note that it suffices to show that
Px(Xn(n) ∈ Ωι) ∈ ((1 - C) qkq—C, C + qkq+C) holds for all η sufficiently small. Again, we can
find E > 0 such that
•	(Due to Lemma G.1)
Px((Ak(η, e))c) ≤ C ∀η sufficiently small;
•	(Due to eq. (G.18)eq. (G.19) and strong Markov property) For all η sufficiently small,
L ≤ Px(X") ∈ Ωι∣Ak (η,E)) ≤ ;.
In summary, for all η sufficiently small, we have
Px (xnk(n) ∈ Ω1) ≤ Px((Ak(η, e))c) + Px ({X£⑺ ∈ Ωι} ∩ Ak(η, e))
≤ C + Px (XnkS) ∈ Ωι ∣ Ak(η, E))Px(Ak(η, e))
≤ C + qk,l + C
qk
and
Px (x") ∈ ω') ≥ Px ({X") ∈ Ω} ∩ Ak(η,E))
=Px(x；U) ∈ Ωι ∣ Ak(η,E))Px(Ak(η,E))
≥ (1 - C) qkl-C
qk
and this concludes the proof.
□
31
Published as a conference paper at ICLR 2022
The rest of this section is devoted to the proofs of Lemma G.1 and Lemma G.2. Specifically, Lemma
G.1 is an immediate Corollary of Lemma G.13, the proof of which will be provided below. The proof
of Lemma G.2 can be found at the end of this section.
G.1 Proofs of Lemma G.1, G.2
The following three lemmas will be applied repeatedly throughout this section. The proofs are
straightforward but provided in Section K for the sake of completeness.
Lemma G.3. Given two real functions a : R+ → R+, b : R+ → R+ such that a(e) J 0, b(e) J 0 as
E J 0, and a family of geometric RVs {U (e) : e > 0} with success rate a(e) (namely, P(U (E) > k)=
(1 - a())k for k ∈ N), for any c > 1, there exists 0 > 0 such that for any ∈ (0, 0),
Lemma G.4. Given two real functions a : R+ 7→ R+, b : R+ 7→ R+ such that a(E) J 0, b(E) J 0
and
a(E)/b(E) → 0
as E J 0, and a family of geometric RVs {U (E) : E > 0} with success rate a(E) (namely, P(U (E) >
k) = (1 - a(E))k for k ∈ N), for any c > 1 there exists some E0 > 0 such that for any E ∈ (0, E0),
a(E)∕(c ∙ b(E)) ≤ P(U(E) ≤ 1∕b(E)) ≤ C ∙ a(E)∕b(E)
Lemma G.5. Suppose that a function g : E 7→ R (where E is an open set in Rd) is g ∈ C2 (E) and
∣∣V2g(∙)^ ≤ C on its domain E for Some constant C < ∞. For a finite integer n, a sequence of
vectors {zι,…,zn} in Rd, and vectors x, x ∈ E,η > 0, consider two sequences {xk}k=o,…,n and
{xek}k=0,...,n constructed as follows:
x0 = x
Xk = Xk-1 + ηVg(xk-i) + ηzk ∀k = 1, 2, ∙∙∙ , n
x0 = x
e = Xk-1 + ηVg(Xk-i) ∀k = 1, 2,…，n
Ifwe have that the line segmentfrom Xk to Xk is contained in E andη ∣∣zι + •… + Zkk + l∣x 一 Xk ≤ e
for all k = 1, 2, ∙∙∙ , n for some X > 0, then
∣∣xk 一 Xkk ≤ X∙ exp(ηCk) ∀k = 1, 2,…，n.
To facilitate the analysis below, we introduce some additional notations. First, we will group the
noises Zn based on a threshold level δ > 0: let us define
Z≤δ,η = ZnI{η∖Zn| ≤ δ},	(G.20)
Z>δ,η = ZnI{η∖Zn∖ >δ}.	(G.21)
The former are viewed as small noises while the latter will be referred to as large noises or large
jumps. Furthermore, for any j ≥ 1, define the jth arrival time and size of large jumps as
Tjη(δ) =∆ min{n > Tjη-1(δ) : η∖Zn∖ > δ},	T0η(δ) =0	(G.22)
Wjη(δ) =∆ ZTjη(δ) .	(G.23)
Next, for any e > 0, let Ωi(c) = [mi - e, m^ + e] be an e—neighborhood of the local minimum mi,
and Si(E) = [si - e, Si + e] be an e-neighborhood of the local maximum s》
For most part of this section, we will zoom in on one of the local minima mi and its attraction field
Ωi = (si-ι, Si). Without loss of generality, we assume mi = 0, and denote the attraction field as
Ω = (s-,s+). (If mi happens to be the local minimum at the left or right boundary, then the attraction
field is [-L, S+) or (S-, L] where the SGD iterates will be reflected at ±L.) Henceforth we will drop
the dependency on notation i when referring to this specific attraction field until the very end of this
section. Throughout the proof, the following (deterministic) dynamic systems will be used frequently
as benchmark processes to indicate the most likely location of the SGD iterates. Specifically, given
32
Published as a conference paper at ICLR 2022
any X ∈ Ω, we use Xn(X) to indicate that the starting point is x, namely Xo(x) = x. Similarly,
consider the following ODE xη (t; x) as
xη (0; x) =x;	(G.24)
dxXdtX)= -ηf 0 (xη(t; x)).	(G.25)
When we use update rate η = 1, we will drop the dependency of η and simply use x(t; x) to denote
the process.
Based on Assumption 3, We know the existence of some constant e ∈ (0, e0) (note that ∈o is the
constant in eq. (G.8)) such that
r =∆ min{-s-, s+},	(G.26)
甘=dr/b],	(G.27)
(l* - 1)b + 100l*e < r - 100l*e	(G.28)
r + 100l*e < l*b - 100l*e.	(G.29)
Here r can be understood as the effective radius of the said attraction field. Also, we fix such e small
enough so that (let cL- = -f 0 (-L), cL+ = -f0(-L)), we have
0.9C- ≤	-f 0(x)	≤	1.1C-	∀x	∈	[-L, -L + 100“，	(G.30)
0∙9c+ ≥	-f0(x)	≥	1.1c+	∀x	∈	[L - 1006, L].	(G.31)
Similar to the definition of ODE xη, let us consider the following construction of ODE Xη that
can be understood as xη perturbed by l* shocks. Specifically, consider a sequence or real numbers
0 = tι < t2 < t3 < •…< tι* and real numbers wι,…,wι* where |wj| ≤ b for each j. Let
t = (tι, ∙∙∙ ,tι*), W = (wι,…,wι*). Based on these two sequences and rate η > 0, define Xη(t; x)
as
xη (0,x; t, w)=2 L(X + 夕 b(wι));	(G.32)
dx (t'dX't, W) = -ηf0(xη(t,x;t, W)) ∀t ∈ {t1,t2,…，tι*}	(G.33)
xη(tj,x; t, w)=2L(Xn(tj-,x; t, w) + 夕b(wj)) ∀j = 2,…，l*	(G.34)
Again, when η = 1 we drop the notational dependency on η and use X to denote the process. Now
from eq. (G.28)eq. (G.29) one can easily see the following fact: there exist constants t, S > 0 such that
x(tι*, 0; t, w) ∈ Ω (note that the starting point is 0, the local minimum) only if (under the condition
that |wj | ≤ b ∀j)
tj - tj-ι ≤ S ∀j = 2, 3,…，l*	(G.35)
∣Wj| > s ∀j = 1, 2,…，l*.	(G.36)
The intuition is as follows: if the inter-arrival time between any of the l* jumps is too long, then the
path of xn (t; x) will drift back to the local minimum mi so that the remaining l* - 1 shocks (whose
sizes are bounded by b) cannot overcome the radius r which is strictly larger than (l* - 1)b; similarly,
if size of any of the shocks is too small, then since all other jumps have sizes bounded by b, the
shock created by the li* jumps will be smaller than (l* - 1)b + 1006S, which is strictly less than r. We
fix these constants tS, δS throughout the analysis, and stress again that their values are dictated by the
geometry of the function f, thus do not vary with the accuracy parameters 6 and δ mentioned earlier.
In particular, choose δS such that δS < 6S.
In our analysis below, 6 > 0 will be a variable representing the level of accuracy in our analysis.
For instance, for small 6, the chance that SGD iterates will visit somewhere that is 6-close to s- or
s+ (namely, the boundary of the attraction filed) should be small. Consider some 6 ∈ (0, 60) where
60 is the constant in Assumption 1. Due to eq. (G.8)eq. (G.9), one can see the existence of some
g0 > 0, C1 < ∞ such that
• ∣f0(x)∣ ≥ go for any X ∈ Ω such that |x — s-1 > €0, ||x — s+∣ > 60;
33
Published as a conference paper at ICLR 2022
• Let %de(x, η) = min{t ≥ 0 : xη (t, x) ∈ [-e, e]} be the time that the ODE returns to a
€—neighborhood of local minimum of Ω when starting from x. As proved in Lemma 3.5 of
Pavlyukevich (2005), for any X ∈ Ω such that |x - s-1 > e, |x - s+∣ > e, we have
log(1/)
tODE(x,η) ≤ ci--------- (G.37)
η
and we define the function
^(e) = ci log(1∕e).	(G.38)
In short, given any accuracy level , the results above give us an upper bound for how fast the ODE
would return to a neighborhood of the local minimum, if the starting point is not too close to the
boundary of this attraction field Ω.
For the first few technical results established below, we show that, without large jumps, the SGD
iterates Xnη (x) are unlikely to show significant deviation from the deterministic gradient descent
process yηn(x) defined as
y0η (x) = x,	(G.39)
yηn(x) = yηn-i(x) - ηf0 yηn-i(x).	(G.40)
We are ready to state the first lemma, where we bound the distance between the gradient descent
iterates yηn(y) and the ODE xη(t, x) when the initial conditions x, y are close enough.
Lemma G.6. The following claim holds for all η > 0: for any t > 0, we have
sup |xη(s,χ) -yηs∣(y)l ≤ (2ηM + ∣χ- y∣)eχp(ηMt)
s∈[0,t]
where M ∈ (0, ∞) is the constant in eq. (G.10).
Proof. Define a continuous-time process yη(s; y) =∆ ybηsc (y), and note that
xη (s, x) = xη (bsc, x) - η	f0(xη (u, x))du
xη (bsc, x) = x - η Z	f0(xη(u, x))du
0
ybηsc (y) = yη(bsc,y) = y - ηZ	f0(yη(u, y))du.
Therefore, if we define function
b(u) = xη(u, x) - yη(u, y),
from the fact |f0(∙)∣ ≤ M, one can see that |b(u)| ≤ ηM + |x - y| for any U ∈ [0,1) and
∣b(1)∣ ≤ 2ηM + |x - y|. In case that s > 1, from the display above and the fact ∣f00(∙)∣ ≤ M, we
now have
∣yηsc(χ) -χη(s,χ)l ≤ ∣b(bsC)l + ηM;
|b(bsc)| ≤ ηM
Zbsc|b(u)|
du.
From Gronwall’s inequality (see Theorem 68, Chapter V of Protter (2005), where we let function
α(u) be α(u) = |b(u + 1)|), we have
∣yηsj(x) - xη(s,x)∣ ≤ (2ηM + |x - y∣)exp(ηMt).
This concludes the proof.
□
34
Published as a conference paper at ICLR 2022
Now we consider an extension of the previous Lemma in the following sense: we add perturbations to
the gradient descent process and ODE, and show that, when both perturbed by l* similar perturbations,
the ODE and gradient descent process should still stay close enough. Analogous to the definition
of the perturbed ODE xeη in eq. (G.32)-eq. (G.34), we can construct a process Y η as a perturbed
gradient descent process as follows. For a sequence of integers 0 = tι < t2 < •… < tι* (let
t = (tj )j≥ι) and a sequence of real numbers Wei,…，Wι* (let W = (Wj )j≥ι) and y ∈ R, define (for
all n = 1,2, ∙∙∙ , tι*) the perturbed gradient descent iterates with gradient clipping at b and reflection
at ±L as
ι*
en(y;t, W)=2 L(en-i(y;t, W) + P b( -ηf 0(en-i(y;t, W)) + X ι{n = tj}Wj))	(G41)
j=2
with initial condition 赭(y; t, W)=夕L (y + 夕b(Wι)).
Corollary G.7. Given any > 0, the following claim holds for all sufficiently small η > 0:
*
for any x,y ∈ Ω, and sequence of integers t = (tj Yj=I and any two sequence of real numbers
w = (wj)ιj*=1, we = (wej)ιj*≥1 such that
•	|x -y| < ;
•	ti = 0, and tj 一 tj-ι ≤ ※/η for all j ≥ 1 where t is the constant in eq. (G.35);
•	|wj - wej| < for allj ≥ 1;
then we have
SUp ∣eη(t,x; t, w) - Wtc (y; t, w)| ≤ Pe
t∈[0,tl*]
where the constant P = (3exp(ηMt) + 3)l*.
Proof. Throughout this proof, fix some η ∈ (0, e/2M). We will show that for any η in the range the
claim would hold.
First, on interval [0, t2), from Lemma G.6, one can see that (since 2Mη < e)
SUp ∣xη (t, x; t, W)—汴 c (y; t, W)| ≤ 3 exp(ηMt) ∙ e.
t∈[0,t2)
The at t = t2, by considering the difference between w2 and we2, and the possible change due to one
more gradient descent step (which is bounded by ηM < e), we have
SUp ∣eη(t, x; t, w) — eηt c (y; t, W)| ≤ (3exp(ηMt) + 2) ∙ e.
t∈[0,t2]
Now We proceed inductively. For any j = 2,3,…，l* — 1, assume that
sup ∣eη(t,x; t, w) — ybηtc(y; t, W)| ≤ (3exp(ηMf) + 3)j-i ∙ e.
t∈[0,tj]
Then by focusing on interval [tj, tj+1] and using Lemma G.6 again, one can show that
sup ∣xη(t, x; t, w) — ybηtc (y; t, W)| ≤ 2e + ((3exp(ηMt) + 3)j-i + 1)exp(ηMt)e
t∈[tj,tj-1]
≤ (3 exp(ηMt) + 3)j ∙ e.
This concludes the proof.	□
In the next few results, we show that the same can be said for gradient descent iterates yyn and the
SGD iterates Xn . Specifically, our first goal is to show that before any large jump (see the definition
in G.21), it is unlikely that the gradient descent process yηn would deviate too far from Xnη. Define
the event
A(n, η, e, δ) = ( max	η∣Zι +------+ Zk| ≤ e1	(G.42)
lk=1,2,…,n∧("(δ)-1)	J
35
Published as a conference paper at ICLR 2022
and recall that arrival times Tjη(δ) are defined in eq. (G.22).
As a building block, we first study the case when the starting point x is close to the reflection boundary
-L. The takeaway from the next result is that the reflection operator hardly comes into play, since the
SGD iterates would most likely quickly move to somewhere far enough from ±L; besides, throughout
this procedure the SGD iterates would most likely stay pretty close to the corresponding deterministic
gradient descent process.
Lemma G.8. Given E ∈ (0, e/9) ,it holdsfor any sufficiently small e,δ,η > 0 that, if X ∈ [一L, -L +
“ and ρo(∣x — y| + 9e) < e, then on event A(n, η, e, δ) we have
Xn (X) 7 k ⑻1 ≤ PO ∙ (Ix -y∣ +9E) ∀k =1, 2, ... ,n ∧ (Tn⑷-I) ∧ TncaPe(X)
where TenCaPe(x) = min{n ≥ 0 : Xn (x) > —L + E} and ρo = exp(0Mb) is a constant that does
not vary with our choice of E, δ, η.
Proof. For any k < Tn(δ), we know that Zk = Z≤δ (thus η∣Zk ∣ < δ). Also,recall that ∣f 0(x)∣ ≤ M
for any X ∈ Ωi. Therefore, as long as η and δ are small enough, we will have that
Iη(-f0(xn(x)) + Z≤δ)∣≤ b	(G.43)
so the gradient clipping operator in eq. (G.12) has no effect when k < T1n(δ), and in fact the only
possible time for the gradient clipping trick to work is at Tjn(δ). Therefore, we can safely rewrite the
SGD update as
Xkn (X) = Xkn-1(X) - ηf 0(Xkn-1(X)) + ηZk + Rk ∀k < T1n(δ)
where each Rk ≥ 0 and it represents the push caused by reflection at -L.
First, choose E small enough so that 9e ‹ 邑 Next, based on eq. (G.31) we have the following lower
bound:
Xkn(X) ≥ X + 0.9cL-ηk - E ∀k < T1n(δ).
∆
Let t0(X, E) =∆ min{n ≥ 0 : Xnn (X) ≥ -L + 2E}. Due to the inequality above, we know that
3E
t0(x，E) ≤ E∙	(G.44)
One the other hand, given the current choice of E, if we choose η and δ small enough, then using the
same argument leading to eq. (G.43), we will have
Xten (x,) (X) ≤ -L + 2.1E ≤ X + 1.1cL-ηk + 2.1E
if t0(X, E) ≥ 1 (namely X < -L + 2E).
Let us inspect the two scenarios separately. First, assume t0(X, E) ≥ 1. For the deterministic gradient
descent process ynn (y), we have the following bounds:
y + 0.9cL-ηk ≤ ynk(y) ≤ y + 1.1cL-ηk ∀k ≤ te0(X, E) ∧ (T1n(δ) - 1).
This gives us
|Xkn(X) - ynk(y)| ≤ |X - y| + 0.2cL-ηk + 2.1E ∀k ≤ te0(X, E) ∧ (T1n(δ) - 1).
At time k = et0(X, E), due to previous bound on et0(X, E), we know that |Xen (X) - yne	(y)| ≤
t0 (x,)	t0 (x,)
|X - y| + 7E. If n∧ (T1n(δ) - 1) ≤ et0(X, E) then we have already shown the desired claim. Otherwise,
starting from time t0(X, E), due to the definition of event A(n, η, E, δ) in eq. (G.42), we know that
the SGD iterates Xnn (X) will not touch the boundary -L afterwards. Therefore, by directly applying
Lemma G.5, and notice that |f00(X)| ≤ M for any X ∈ [-L, L] and, we have
2ME
Xn(X) - yn(y)I ≤ (|x - y| +9e) ∙ exp (r) ∀k ≤ n ∧ (Tn(δ) - 1) ∧ TeSCape(x).
36
Published as a conference paper at ICLR 2022
Indeed, it suffices to use Lemma G.5 for the nextd2^/(0.93-)] steps to show that |X?(x) - yk(y)∣
should be smaller than E for the nextd2^/(0.9ηc-)[ steps, while yk(y) will reach some where in
(-L + 2e, -L + 3e) withind2^/(0.9ηc-)[ steps so we must have
n ∧ (Tn(δ) — 1) ∧ TXaPe(X) ∧ to(x, E) — to(x, E) ≤ 2e/(0.9ηc-)	(G.45)
Lastly, in the case t0(x, E) = 0 (which means x ≥ -L + E), we can use Lemma G.5 directly as we
did above and establish the same bound. This concludes the proof.	□
Obviously, a similar result can be shown if x is in the rightmost attraction field (snmin-1 , L] and the
approach is identical. We omit the details here. In the next Lemma, we consider the scenario where
the starting point x is far enough from the boundaries.
Lemma G.9. Given any e > 0,thefollowing holds for all sufficiently small η > 0: for any x,y ∈ Ω
and positive integer n such that |x - L| > 2E, |x + L| > 2E, |x - s- | > 2E, |x - s+| > 2E and
|X - U1 < 2exp(iMn)，0n ^vent
A(n,η, 2exp(ηMn) ,δ) ∩ {卜n(y)l ∈ Ω, ∣Xjn(x)∣ ∈ Ω ∀j = 1, 2,…，n ∧ (Tn(δ) - 1)}
we have
∣xm(y) - Xm(x)| ≤ E ∀m =1, 2,…，n ∧ (Tn(δ) - 1).
Proof. For sufficiently small η, we will have that the (deterministic) gradient descent iterates |ynn |
is monotonically decreasing in n, which ensures that ynn always stays in the range that are at least
E-away from ±L or s-, s+. We now show that the claim holds for any such η.
On event {|y?(y)| ∈ Ω, ∣Xjn(x)∣ ∈ Ω ∀j = 1,2,…，n ∧ (Tf(δ) - 1)}, we are able to apply
Lemma G.5 inductively for any m ∈ [n] and obtain that
|yn (y) - Xn(X)I ≤ (|x - y| + 2eχp(EηMn)) exp(ηMj) < E ∀j = 1, 2,…，m
and conclude the proof. The reason to apply the Lemma inductively for m = 1, 2,…，n, instead of
directly at step n, is to ensure that SGD iterates Xnn would not hit the boundary ±L (so the reflection
operator would not come into play on the time interval we are currently interested in), thus ensuring
that Lemma G.5 is applicable.	□
Similar to the extension from Lemma G.6 to Corollary G.7, we can extend Lemma G.9 to show that,
if we consider the a gradient descent process that is only perturbed by large noises, then it should stay
pretty close to the SGD iterates Xnn . To be specific, let
Y0n (X) = X	(G.46)
Yn (χ) = ^L(γnn-1(χ) - 3b( - ηf0(γn-1(χ))+X ɪs =界⑷那。“)).	(g.47)
j≥1
be a gradient descent process (with gradient clipping at threshold b) that is only shocked by large
noises in (Zn)n≥1. The next corollary can be shown by an approach that is identical to Corollary G.7
(namely, inductively repeating Lemma G.9 at each jump time) so we omit the details here.
Corollary G.10. Given any E > 0, the following holds for any sufficiently small η > 0: For any
|X| < 2E, on event A0(E, η, δ) ∩ B0(E, η, δ), we have
I γnn (χ) - χn (χ)∣ <% ∀n = 1,2,…，Tn ⑷
where
Ao(e, η, δ) = n∀i = 1,…，l*,	max	η∣Zτn (δ)+ι + …+ ZjI ≤ ----------E —JI 八 0；
1	, ，j=τin-1(δ)+ι,…,Tin(δ)-i " Ti-1 ⑷+1 丁	j - 2exp(2tM) ʃ'
Bo(e, η, δ) = {∀j = 2,…，l*, Tjn(δ) - Tn-i(δ) ≤ 2"η}
and ρe ∈ (0, ∞) is a constant that does not vary with η, δ, E.
37
Published as a conference paper at ICLR 2022
The next two results shows that the type of events A(n, η, e, δ) defined in eq.(G.42) is indeed very
likely to occur, especially for small e. For clarity of the presentation, we introduce the following
definitions that are slightly more general than the small and large jumps defined in eq. (G.20)eq. (G.21)
(for any c > 0)
Z≤c = Zn l{∣Zn∣ ≤ c},
Z> = Zn l{∣Zn∣ >c>.
Lemma G.11. Define functions u(η) = d/nɪ-ʌ, v(η) = eη* with e, δ > 0. If real numbers
∆, ∆, β, e, δ and positive integers j, N are such that thefoUOWing conditions hold:
△ ∈ [0, (1 - -∧ A ɪ),	(G.48)
ɑ 2
β ∈ (1, (2 - 2∆) A α(1 - ∆)),	(G.49)
∆
∆ ∈ [0, ɪ], ∆ <α(1- ∆) - β,	(G.50)
N < (α(1- ∆) - β)j,	(G.51)
v(η) — jηu(η) ≥ v(η)∕2 for all η > 0 sufficiently small,	(G.52)
then
p(k=12max1∕nβeη∣z≤u⑺十∙∙∙+ Zfu(n)l >3v(η)) = o(ηN)
as η J 0.
Proof. From the stated range of the parameters, we know that
α(1 - ∆) > β,
(α(1- ∆) - β)j > N,
so we are able to find Y ∈ (0,1) small enough such that
α(1 - ∆)(1 - 2γ) > β,	(G.53)
(α(1- ∆)(1 - 2γ)- β)j > N.	(G.54)
Fix such Y ∈ (0,1) for the rest of the proof, and let n(η)，∣^(1∕η)β^∣, I
∣Z≤u(n)∣ > u(η)1-γ}. Then
#{ i
∈
[n(η)]:
P(| Z≤u(n) + ∙∙∙ + Z≤(*) ∣>v(η))
j-1
X1P>(∣ z≤u(n)	7≤u(n)∣∣ y≤u(n)	v≤u(n) ∣
=2^P∖ ∣Z1	+ ∙∙∙ + Zn(η) ∣>v(η), 1 = Z) + P( ∣Z1	+ ∙∙∙ + Zn(n) ∣ > v(η), 1 ≥ j)
i=0 S---------V------------Z S-----------V------------•
,(I)	,(II)
Note that since ∣Z≤u(η)∣ < u(η),
(I) ≤
∙ P(∣ Z≤u(η) + ∙∙∙ + z≤* ∣ > "η) -ηiηu(η), ∣Z≤u(η)∣ ≤ u(η)1-γ Vi ∈ [n(η) - i
≤ n(η)i ∙ P(∣ Z≤u(η)1-γ + ∙∙∙ + Z≤^1-γ ∣ > …:⑺
≤ n(η)i∙ P(∣ 2心)1-，+ ∙∙∙ + Zn(UIg
where the last inequality follows from eq. (G.52). First, since EZi = 0, we have
(G.55)
∣EZ≤u⑺
1-γ ∣ = ∣ EZ>u(η)
I-Y
r∞
/	P(∣Z1∣ > x)dx ∈ RV(α-1)(1-γ)(1-∆)(η)
J u(η)1-^Y
38
Published as a conference paper at ICLR 2022
Therefore, for all η > 0 that are sufficiently small,
∣EZ≤u⑺1-γ + ∙∙∙ + EZ≤ζ)⅛-γ |
≤n(η) ∙ η(α-i)(i-∆)(i-2γ) ≤ 2η(α-i)(i-∆)(i-2γ)-β
41/n)(1f1-2Y)due to eq.(G.53)
≤v(η) due to ∆/2 ≤ ∆ in eq. (G.50) and 1 - γ < 1.
4η
If We let Yn = Z≤u(n) Y - EZ≤u(η) Y and plug the bound above back into eq. (G.55), then (for
all η > 0 that are sufficiently small)
(I)	≤ n(η)i ∙ P(IYI + ∙∙∙ + γn(η)-il > -η^-)
____________________16 ∙1加2-力_______________________
2(n(η) - i)E∣Y1∣2 + ∣δ1-Y ∙ (1加)(1^)(I-Y) ∙ 4/吟-人
≤ n(η)i exp
(G.56)
where the last inequality is obtained from Bernstein,s inequality. Note that from Karamata,s theorem,
E∣Y1∣2 = Var(Z≤u(η)i) ≤ E∣z≤u(η)1-γ ∣2
fu(η)1-γ
≤ /	2xP(∣Zι∣ > x)dx ∈
Jo
RV-(1-∆)(1-7)(2-a)(η).
Now note that
•	In case that α < 2, for all η > 0 that are sufficiently small, we have (using eq. (G.50))
2(n(η) - i)E∣Y∣2 ≤ (1加严(2-由(1-4) < (1∕η)2(I-△)
_ _ ~
⇒ ,	^η-∖_- > 1∕η∆;
2(n(η) - i)E∣Y1∣2 ~ f 1 ,
•	In case that α > 2, for all η > 0 that are sufficiently small,
2(n(η) - i)E∣Y1∣2 < 1∕ηβ+ 等
and we know that β + △ < 2 - 2∆ due to 2 - β > 2∆ and 2∆ ≤ ∆ (see eq. (G.48)-
eq. (G.50));
•	Since γ > 0 and 2∆ ≤ ∆, we know that
,	.., . ≈, ≈
(1 - ∆)(1 - γ) + (1 - ∆) < 2 - 2∆.
Therefore, it is easy to see that the R.H.S. of eq. (G.56) decays at a geometric rate as η tends to zero,
hence o(ηN). On the other hand,
(II)	≤ P(I > j) ≤ (n?) ∙ P(∣Z≤u(η)∣ > u(η)1-γ ∀i = 1,...,j)
≤ n(η)j ∙ p(∣Z≤u(η)∣ > u(η)1-γ)j,
which is regularly varying w.r.t. η with index (α(1 - Y)(1 - ∆) - β)j. Therefore, for all η > 0
sufficiently small,
(II)	≤ η(α(1-2Y)(I-ai)j < ηN due to eq. (G.54).
Collecting results above, we have established that
p(η∣Z≤u(η) + ∙ ∙ ∙ + 的犷| > v(η)) = o(ηN)∙
The conclusion of the lemma now follows from Etemadi,s theorem.	□
39
Published as a conference paper at ICLR 2022
Now consider the following setting. Let us fix some positive integer N and β ∈ (1, 2 ∧ α). Then we
can find some positive integer j such that (α - β)j > N. Meanwhile, given any > 0, we will have
- jδ ≥ /2 for all δ > 0 sufficiently small. Therefore, by applying Lemma G.11 with ∆ = ∆e = 0
(hence u(η) = δ∕η, v(η) = E) and β,j, N, e, δ as described here, We immediately get the following
result.
Lemma G.12. Given any β ∈ (1, α ∧ 2), E > 0, and N > 0, the following holds for any sufficiently
small δ > 0:
P( max	η∣Z≤“η +---------+ Z≤"η |> j = o(ηN)
∖j=ι,2,…，d(i∕η)β e	1	j	>
as η J 0.
Using results and arguments above, we are able to illustrate the typical behavior of the SGD iterates
Xnη in the following two scenarios. First, we show that, when starting from most parts in the attraction
field Ω, the SGD iterates Xn will most likely return to the neighborhood of the local minimum within
a short period of time without exiting Ω. Given that there are only finitely many attraction fields on f,
it is easy to see that the key technical tool Lemma G.1 follows immediately from the next result.
Lemma G.13. For sufficiently small E > 0, the following claim holds:
lim	sup	Px(Xn ∈ Ω ∀n ≤ Treturn 56, and Treturn (η, e) ≤ ρ(∈)∕η) =1
n，0 xEQ:|x—s-∣∧∣x-s+∣>e '	/
where the stopping time involved is defined as
Treturn(η, E) =∆ min{n ≥ 0 : Xnη(x)∈	[-2E, 2E]}
thefunCtion £(E) is defined in eq. (G.38), and thefunCtion ρ(∙) is defined as P(E)
0.9c-∧c+ + 2,(E)
Proof. Throughout, we only consider E small enough so that Lemma G.8 could hold. Also, fix some
N > 0, ∆α ∈ (0, α 一 1) and β ∈ (1, α). Let σ(x, η) = min{n ≥ 0 : Xn ∈ Ω}.
Without loss of generality, we assume Ω = [-L, s+) and x < 0 (so reflection at -L) is a possibility.
Any other case can be addressed similarly as shown below.
From Lemma G.4 and the regular varying nature of H(∙), we have, for any e, δ > 0,
p(τη(δ) ≤ Pg)Iη ≤ ηα-1-5	(G.57)
for any sufficiently small η.
η
Let Teηscape(x) be the stopping time defined in Lemma G.8. From eq. (G.44),eq. (G.45),eq. (G.57) and
Lemma G.12, we know that
SUp	P (TnCaPe(X) < σ(x, η), TnCaPe(X) ≤ ∩	and XMn (X) ∈ [-L + G-L + 24)
x∈[-L,-L + e] '	0.9C—〃	Tescape	)
≥1 - nN - ηα-1-5	(G.58)
for all sufficiently small η.
Next, we focus on X ∈ Ω such that |x - s-∣∧∣χ - s+∣ > e and X ≥ -L + 己 We start by considering
the time it took for the (deterministic) gradient descent process ynn(X) to return to [-1.5E, 1.5E]. From
the definition of £(E) in eq. (G.38) and Lemma G.6, we know that for n small enough such that
nexp(2Mf(E)) < 0.5e, we have
min{n ≥ 0 : ynn(X) ∈ [-1.5e, 1.5e]} ≤ 2£(七)加.
Now consider event A(d(1∕n)β],n, 4eχp(2M^9),δ) (see definition in eq. (G.42)). From Lemma
G.12, we know that for any sufficiently small δ, we have
p((A(d(1∕n)βe,n, 4exp(2EM^(E)),6)c)= o(nN)	(G.59)
40
Published as a conference paper at ICLR 2022
Combining this result with eq. (G.57)eq. (G.59) and Lemma G.9, we get
SUP	Px(TretUrn(η, E) < σ(x, η),‰turn(η, E) ≤ 2法)加)	(G.60)
x∈Ω: | x —s—∣∧∣ x —s+1 >e,x≥-L+忑	'	/
≥ 1 - ηN - ηα-1-∆α	(G.61)
for any sufficiently small η. To conclude the proof, we only to combine strong Markov property (at
Tncape) With bounds in eq.(G.58)eq. (G.61).	□
In the next result, we show that, once entering a E-small neighborhood of the local minimum, the
SGD iterates Will most likely stay there until the next large jump.
Lemma G.14. Given N0 > 0, the following claim holds for any E, δ > 0 that are sufficiently small:
SUp	p(∃n < Tn(δ) s.t. ∣xn(x)| > 3e) = o(ηN0)
x∈[-2e,2e]
as η J 0.
Proof. Fix E small enough such that 3E < E0 (see Assumption 1 for the constant E0). Also, fix some
∆α ∈ (0, 1), β ∈ (1, α), N > α + ∆α - β + N0. Due to Lemma G.12, for any δ sufficiently small,
We Will have
p( max	η∣Z≤'δ + …+ Z≤,δ| > —= = o(ηN).	(G.62)
lj=1,2,…，d(i∕n)β] " 1	j 1	exp(2M)J	'	`	'
Fix such δ > 0. We noW shoW that the desired claim is true for the chosen E, δ.
First of all, from Lemma G.3, We knoW the existence of some θ > 0 such that
P(Tn(δ) > 1/ηα+∆α) = o(exp(-1∕ηθ)).	(G.63)
Next, let us zoom in on the first d(1∕η)β] SGD iterates. For any η small enough, we will have
ynn(x) ∈ [-2E, 2E] for any n ≥ 1 and ydn(1∕n)β e (x) ∈ [-E, E] given x ∈ [-2E, 2E]. From noW on We
only consider such η. Due to Lemma G.9, we know that on event { maxj=1,2,…，「([加)] η∣z≤,δ +
…十 Z/,δ | > exp(2M) },wehave
|Xnn(x)| ≤3E ∀n≤ d1∕ηβe ∧ (T1n(δ) - 1)
and on event {maxj=1,2,…，d(i/n)e ηlZ≤,δ + …+ Z≤,δ | > eχp⅛M)} ∩{T1n(δ) > d(1∕η)βe}, we
have XTη ⑷⑺ ∈ [-2e, 2e]. Now by repeating the same argument inductively fordi/na2a—β]
times, we can show that on event
{∀i = 1, 2,…，dηα+Aα-βe, j=ι, max∕n)βe ηlZ≤,δ∕n)βe+1 + …+ Z≤,δ∕n)βe+j| > exp(2M) },
we have |Xn (x)| ≤ 3e ∀n ≤ 1/ηα+∆α ∧ (Tf(δ) — 1). To conclude the proof, we only need to
combine this fact with eq.(G.62).	□
We introduce a few concepts that will be crucial in the analysis below. Recall the definition of
perturbed ODE exn in eq. (G.32)-eq. (G.34) (note that we will drop the notational dependency on
learning rate η when we choose η = 1). Consider the definition of the following two mappings
from where W = (wι,…,wι*) is a sequence of real numbers and t = (t1,t2, ∙∙∙ ,tι*) with
0 = tι < t2 < t3 < •…as
h(W, t) = ex(tl* , 0; t, W).
Next, define sets (for any E ∈ (-E, E))
E(E) = {(W, t) ⊆ Rl* × (R+	: h(W, t) ∈∕ [s— - E, s+ + E]}.	(G.64)
We add a few remarks about the two types of sets defined above.
41
Published as a conference paper at ICLR 2022
•	Intuitively speaking, E() contains all the perturbations (with times and sizes) that can send
the ODE out of the current attraction field (allowing for some error with size );
•	From the definition of t, S in eq. (G.35)eq. (G.36) and Corollary G.7, one can easily see that
for a fixed E ∈ (-S, S),
(w, t) ∈ E() ⇒ |wj| > δS, tj - tj-1 ≤ tS ∀j;
•	Lastly, E(E) are open sets due to f ∈ C2.
Use Leb+ to denote the Lebesgue measure restricted on [0, ∞), and define (Borel) measure να with
density on R\{0}:
να(dx) = 1{x > 0} 1⅛ + 1{x < 0}∣χ;j⅛
where α > 1 is the regular variation index for the distribution of Z1 and p- , p+ ∈ (0, 1) are constants
in Assumption 2. Now We can define a Borel measure μ on Rl* X(R+) as product measure
μ = (να)l* × (Leb+)l*τ.	(G.65)
Due to remarks above, one can see that for E ∈ (-S, S), we have μ(E(e)) < ∞. We are now ready to
analyze a specific type of noise Zn .
Definition G.1. For any n ≥ 1 and any E ∈ (-ES, ES), δ ∈ (0, b ∧ δS), η > 0, we say that the jump Zn
has (E, δ, η)-overflow if
•	η∣Zn∣ > δ；
•	In the set {n + 1,…,n + 2」1"£加]}, there are at least (l* — 1) elements (ordered as
n <t2 < t3 < •…< t?*) such that η∣Zti ∣ > δ forany i = 2,…，l;
•	Let tι = n and ti = t — ti-ι for any i = 2, ∙∙∙ , l*, Wi = ηZi for any i = 1, ∙∙∙ , l* ,for
real sequence w = (w1,w2, ∙∙∙ , wι*) and a sequence ofpositive number t = (η(ti — n))i=2,
we have
(w, t) ∈ E(E).
Moreover, ifZn has (E, δ, η)-overflow, then we call h(w, t) as its (E, δ, η)-overflow endpoint.
Due to the iid nature of (Zj)j≥1, let us consider an iid sequence (Vj)j≥0 where the sequence
has the same law of Z1. Note that for any fixed n ≥ 1, the probability that Zn has (E, δ, η)-
overflow is equal to the probability that V0 has(E, δ, η)-overflow. More specifically, we know that
P(η∣V0∣ > δ) = H(δ∕η), and now we focus on conditional probability admitting the following form:
p(E, δ, η) = PV0 has (E, δ, η)-overflow η∣V0∣ > δ.	(G.66)
For any open interval A = (aι, a2) such that A ∩ [s- + S, s+ - S] = 0, we also define
p(E, δ, η; A) = P V0 has (E, δ, η)-overflow and the endpoint is in A η∣V0∣ > δ .	(G.67)
Lemma G.15. For any E ∈ (-ES, ES), δ ∈ (0, b ∧ δS), and any open interval A = (a1, a2) such that
∣a1∣ ∧ ∣a2 ∣ > r - ES and ∣a1∣ 6= L, ∣a2 ∣ 6= L, we have
lim
n10
p(E, δ, η; A)
l* 1
δα(H1⅛l)
μ(E(e) ∩ h-1 (A))
where μ is the measure defined in eq. (G.65), and p(∙, ∙, ∙; A) is the conditional probability defined in
eq. (G.67).
Proof. Let us start by fixing some notations. Let T1 = 0, and define stopping times Tj = min{n >
Tj-1 : η∣Vn∣ > δ} and inter-arrival times Tj0 = Tj - Tj-1 for any j ≥ 1, and large jump Wj = VTj
for any j ≥ 0. Note that: first of all, the pair (Ti0, Wi) is independent of (Tj0, Wj) whenever i 6= j;
besides, Wj and Tj0 are independent for all j ≥ 1.
42
Published as a conference paper at ICLR 2022
Define the following sequence (of random elements) W = (wι,… ,wι*) and t = (tι, ∙∙∙ , tι*) by
wj = ηWj , tj = ηTj .
If V0 has (, δ, η)-overflow, then the following two events must occur:
•	Tj ≤ 2t∕η for any j = 2, ∙∙∙ , l*;
•	η∣Wj | > S for any j = 1, 2, ∙∙∙ , l*;
•	(W, t) ∈ E()
Therefore, for sufficiently small η, we now have
p(, δ, η)
= (P(TO ≤ 2t∕η))l -1 ∙ / l{(w,t) ∈ E(e)}
. P(ηWι = dwι∣η∣Wι∣ >δ)…P(ηWι* = dwι*| η∣Wι*| >δ)
• P(ηT2 = dt2 ∣ηT2 ≤ 2t)…P(ηT0* = dtl*∖ηT[* ≤ 2t)
= (P(TO ≤ 2t∕η))l -1∙Qη,δ(E(E) ∩ h-1(A))	(G.68)
where Qη,δ is the Borel-measurable probability measure on Rl* × (R+	induced by a sequence
of independent random variables (W1↑(η, δ), • • • , Wl↑* (η, δ), T2↑(η, δ), • • • , Tl↑* (η, δ)) such that
•	For any i = 1,…，l*,the distribution of W↑(η, δ) follows from P (ηWι ∈ • I η∣Wι* | > δ);
•	For any i = 2, ••• , l*,the distribution of T↑(η, δ) follows from P (ηT ∈ • I ηT ≤
;
•	Qη,δ(•) = P((ηW↑(η,δ),…，ηWι↑(η,δ),ηT↑(η,δ),…，ηPj=2T↑(η,δ)) ∈ •)
Now we study the weak convergence of W1↑ , T1↑ :
•	Due to the regularly varying nature of distribution of Z1 (hence for W1), we know that: for
any x > δ,
limP(nWi > x ∣ η∣Wι* | > δ) = p+ ^α, lim P(nWi < -X I η∣Wι* | > δ) = P-二；
therefore, W1↑ (η, δ) weakly converges to a (randomly signed) Pareto RV that admits the
density
αδα	αδα
να,δ(dx) = l{x > 0}P+ χα+1 + 1{x < 0}P- ∣χ∣α+l
as η J 0;
•	For any X ∈ [0, 2t], since limηjo [x∕η]H(δ∕η) = 0, it is easy to show that
1 - (1 - H(δ∕η))bxm
lim —bx/nCH (δ∕η)一
therefore, we have (for any X ∈ (0, 2tS])
1;
P(nT1 ≤ x | nT1 ≤ 2t) = 1 - (I - H(δ∕n))bxJηc → X
⑺	1	1 - (1 - H(δ∕n))b2"ηc T 2S
as η J 0, which implies that T1↑ converges weakly to a uniform RV on [0, 2tS].
43
Published as a conference paper at ICLR 2022
Let us denote the weak limit of measure Q%δ as μ^,2t. In the discussion before the Lemma We have
shown that, for any (w, t) ∈ E(E) (with δ ∈ (0, S)), we have ∖wi∖ ≥ S and |t] ≤ 2t; since we require
δ < S,by definition of measures μ and μδ,2t We have
δ“
μs,2E(E(e) ∩ h-1 (A)) = (2s)l*-1 ∙ μ(E(O ∩ hT(A))∙
For simplicity of notations, we let E (e, A) = E (E) ∩ h-1(A). By definition of the set E(e), we have
(recall that A is an open interval (αι, α2) that does not overlap with [s- + e, s+ - SI)
E(e, A) = h-1 ((-∞, s— — E) U (s+ + E, ∞)) ∩ h-1((aι, a2))
=h-1 (((-∞, S- — E) U (s+ + E, ∞)) ∩ (α1, a2))
=h-1 (F(E,a1,a2))
where F(e, a1, a2) = ((-∞, S- - E) U (s+ + e, ∞)) ∩ (a1, a2). Meanwhile, it is easy to see that h
is a continuous mapping, hence
(w, t) ∈ ∂E(e, A) =⇒ h(w, t) ∈ {s- + e, s+ — e, a1, a2}.
FiX some S with S = ±L, ∖s∖ > (l* - 1)b + S. For any fixed real numbers t2, ∙∙∙ ,tι*-1, w1, ∙∙∙ , wι*,
if h(w1, ∙ ∙ ∙ , wι*,t2, ∙ ∙ ∙ ,tι*-1,t) = s, then since x(t∕* - 1,0; w1, ∙ ∙ ∙ , wι*,t2, ∙ ∙ ∙ ,tι*-1,t) ∈
[s - b, s + b], due to Assumption 1 (in particular, there is no point x on this interval with ∖f 0(x) ∖ ≤ c0
),there exists at most one possible t that makes h(w1, ∙ ∙ ∙ ,w∣j*,t2, ∙ ∙ ∙ ,tι*-1,t) = s. Therefore,
let W be iid RVs from law νqδ defined above, and (Tj,0)j≥2 be iid RVs from Unif[0,2t], T0 =
0, Tk = P：=2 Tj ,0. By conditioning on all Wj and all TM, ∙ ∙ ∙ , TP-「we must have
p(h(Wj,∙∙∙ ,%,T2, ∙∙∙ ,T*) = S I W； = dw1,∙∙∙ , W* = dwι*,
TJ = dt2, ∙ ∙ ∙ , Tι1,-1 = dtι*-1) = 0	(G.69)
which implies
P(h(W；, ∙∙∙ ,Wj,T2,∙∙.,窃)=S) =0
hence
μ(∂E(e,A)) =0.
By Portmanteau theorem (see Theorem 2.1 of Billingsley (2013)) we have
部5(E(E, A)) = 〃6川E(E, A)).
Collecting the results we have and using eq. (G.68), we can see that
lim sup
n，0
P(E, δ,η;A)
H(1∕η)
η
l*-1
δα
lim sup
n，0
(2f)l*τ ∙ p(e, δ, η; A)
δa* ∙ (p(t0 ≤ 2S∕η))1	1
(δα P(TI ≤ 2S∕η)γ*τ
I2S ∙ H(i∕η)∕η)
≤ lim sup
n，0
(2f)l*τ ∙ p(e, δ, η; A)
δαi* ∙ (p(t0 ≤ 2s∕η))1 1
∙ lim sup
n，0
P(T1 ≤ 2f∕η)∖ ι*-1
/e 八、1.	∕δɑ P(T ≤ 2t∕η)∖l*-1
≤“(E(E,A)) ∙lims0up(宣∙ H(")
Fix some κ > 1. From Lemma G.4 and the regularly varying nature of function H, we get
lim sup (δ: ∙ P(T1 ≤ 2*)l*-1≤ κi*-1 lim sup
M0l2t	H(i∕η)∕η )	-	ηψ0p
2tH (δ∕η)∕η)l*-1 = l*-1
H(i∕η)∕η J =
44
Published as a conference paper at ICLR 2022
Due to the arbitrariness of κ > 1, we have established that
limsup p(y*A)	≤ μ(E(e)).
ηψo	(H (i∕η))	1δα
The lower bound can be shown by an argument symmetric to the one for upper bound.	□
The following result is an immediate corollary of Lemma G.15.
Corollary G.16. For any E ∈ (—e, e), δ ∈ (0,b ∧ δ), we have
lim
ηψo
p(e,δ,η)
Z	、 l* 一1
δα<H(1∕η)A l	1
μ(E(E))
where μ is the measure defined in eq. (G.65), and p(∙, ∙, ∙) is the conditional probability defined in
eq. (G.66).
Define the following stopping times:
σ(η) = min{n ≥ 0 : Xn ∈ Ω};	(G.70)
R(E, δ, η) = min{n ≥ T1η(δ) : Xnη ∈ [-2E, 2E]}.	(G.71)
σ indicate the time that the iterates escape the current attraction field, while R denotes the time the
SGD iterates return to a small neighborhood of the local minimum after first exit from this small
neighborhood. In the next few results, we study the probability of several atypical scenarios when
SGD iterates make attempts to escape Ω or return to local minimum after the attempt fails. First, we
show that, when starting from the local minimum, it is very unlikely to escape with less than l* big
jumps.
Lemma G.17. GivenE ∈ (0, e), N > 0,the following claim holds for any sufficiently small δ > 0:
sup	Pxσ(η) < R(E, η), σ(η) < Tlη*(δ) = o(ηN)
x∈[-2,2]
as η J 0.
Proof. Based on the given e > 0, fix some e = 4exp(2Mke)). Recall the definition of £(E) in
eq. (G.38).
First, using Lemma G.14, we know that for sufficiently small δ, we have
sup PA1×(E, δ, η) =o(ηN)	(G.72)
x∈[-2,2]
where
A; (E,δ,η) = n∃n < Tn(δ) s.t. Xn(x)| > 3e}.
Define event
A× (e, δ, η) = {∃j = 2, ∙∙∙ ,l* St k=1,2,…,tma)xτn_ ι(δ)-1 ηlZ≤-n(δ)+ι+∙ ∙ ∙+Zjι(δ)+k |〉E}.
From Lemma G.12, we know that for sufficiently small δ > 0,
PA2×(eE,δ,η) =o(ηN).	(G.73)
From now on, we only consider such δ that eq. (G.72)eq. (G.73) hold.
On event	A1×	∪ A2×	∩ {σ(η)	< R(E, η)}	∩ {σ(η)	>	T1η(δ)},	we must have σ(η)	>	T1η(δ) and
T2n(δ)∧σ(η)- Tn(δ) < 2f(E)∕η.
45
Published as a conference paper at ICLR 2022
Otherwise, due to Lemma G.6 and G.9, We know that at step t = Tn(δ) + ∖i(e)∕ηC, we have
|X7 | < 2e, and |Xn | ≤ e ∀n ≤ W
for any sufficiently small η. By repeating this argument inductively, we obtain the following result:
define
J = min{j = 1, 2,…：σ(η) ∈ [Tjn(δ), Tj+1(δ))},
then on event (a× ∪ A×) ∩ {σ(η) < R(e,η), σ(η) < Tn (δ)}, we must have
Tj(δ) ∧ σ(η) - Tjn-i(δ) ∧ σ(η) < 2法)∕η ∀j = 2, 3,…，J.	(G.74)
Furthermore, using this bound and Lemma G.9, we know that on event A1× ∪ A2×	∩ {σ(η) <
Ran), σ(η) <τn (δ)},
•	|XTη (δ) | ≤ IXTη ](δ) | + b + C + W for allj = 2, 3, j - 1,
•	lXσ(n)1 ≤ lXτη-ι(δ)1 + C + W
However, this implies
Xn(n)∣≤ l*(W+ c) + (l*-1)b<r
and contradicts the definition ofσ(η). In summary,
sup	Pχ(σ(n)	<	R(c,n),	σ(n)	< Tn (δ))	≤ p(A×(c,δ,n)	∪	A×(e,δ,n))	= o(nN).
x∈[-2,2]
□
The following two results follow immediately from the proof above, especially the inductive argument
leading to bound eq. (G.74), and we state them without repeating the deatils of the proof.
Corollary G.18. Given C ∈ (0, CW), N > 0, the following claim holds for any sufficiently small δ > 0:
sup Px (Tn (δ) ≤ σ(n) ∧ R(c, n), and ∃j = 2, 3,…，l* s.t. Tj(δ) 一 Tjn-1(δ) > 2t(c)∕n)
=o(ηN)	as n J 0.
Corollary G.19. Given C ∈ (0, CW), N > 0, the following claim holds for any sufficiently small δ > 0:
sup	Px(R(c,	n)	<	Tn	(δ)	∧ σ(n),	R(c, n)	- Tn(δ)	>	21*t(c)∕n)	= o(nN)
x∈[-2,2]
as n J 0.
In the next result, we show that, if the inter-arrival time between some large jumps are too long, or
some large jumps are still not large enough, then it is very unlikely that the SGD iterates could escape
at the time of l*-th largejump (or even get close enough to the boundary of the attraction field).
Lemma G.20. Given C ∈ (0, CW) and any N > 0, the following holds for all δ > 0 that are sufficiently
small:
sup	Px(B2×(C, δ, n)) = o(nN).
x∈[-2,2]
where
B×(c,δ,n) = {Tn (δ) ≤ σ(n) ∧ R(C,n)} ∩ {∃j = 2,3,…，l* s∙t.τj(S) — Tj-Kδ) > t/n
or ∃j = 1, 2,…，l* s.t.n∣叼(δ)∣ ≤ wO ∩{∣XTη ∣ ≥ r - W}.
46
Published as a conference paper at ICLR 2022
Proof. Let A1× , A2× be the events defined in the proof of Lemma G.17. Based on the given > 0, fix
some e= 4exp(2M^(e)).
Let J = min{j = 2,3,…：Tj (δ)-Tη-1(δ) > t∕η}. On event (a× (e, δ, η)∪A× (己 δ, η))c ∩{J ≤
l*}, from Lemma G.6 and G.9 and the definition of constant t, We know that
•	|XTη (δ) | ≤ IXTη W) | + b + C + W for allj = 2, 3, J - 1;
•	|XTn (δ)1 ≤ 2w
•	IXnη I < s - CW ∀n ≤ TJη (δ)
Now starting from step TJη (δ), by using Lemma G.6 and G.9 again one can see that
•	|XTn(δ)| ≤ IXTn 1(δ) | + b + C + Wfor all j = J +1, ∙∙∙ , l*.
Combining these results, we have that IXTηη I < r - CW on event A1× (C, δ, η) ∪ A2× (eC, δ, η)	∩ {J ≤
l*}.	l*
Next, define J0	=	min{j = 1,2,…;η∣Wjl(δ)∣	≤ δ}. Similarly, on	event	(a× (c,δ,η) ∪
A× (e, δ,	η)) ∩{J > l*} ∩ {J 0 ≤ l*}, using Lemma G.6 and G.9 again one	can see that
•	|XTn (δ)1 ≤	IXTn 1(δ)1 + b + C + W for all j	= 1, 2,…，l*, j = J0;
•	IXTn(δ)I ≤	XTn_ 1(δ)∣ + w + C + Wforall j	= 1, 2,…，l*,j = J0.
Since δW ∈ (0, CW), we have IXTη n I < r - CW on this event.
Tl*
In summary, the following bound
sup Px(B2×) ≤ P(A1×(C, δ, η) ∪ A2×(eC, δ, η)) = o(ηN)
x∈[-2,2]
holds for any δ that is sufficiently small, which is established in Lemma G.17. This conclude the
proof.	□
In the next lemma, we show that, starting from the local minimum, it is unlikely that the SGD iterates
will be right at the boundary of the attraction field after l* large jumps. Recall that there are n面口
attraction fields on f, and excluding so = -∞, Snmin = ∞ the remaining points si,…，Snmin-ι are
the boundaries of the attraction fields.
Lemma G.21. There exists a function Ψ(∙) : R+ → R+ satisfying limeψο Ψ(c) = 0 such that the
following claim folds. Given C ∈ (O, w∕(3p + 3ρ + 9)), we have
lim sup
ηψο
≤ δαΨ(C)
for all δ sufficiently small, where ρW and ρe are the constants defined in Corollary G.7 and G.10, and
the event is defined as
B3× (C, δ, η)
Tlη*(δ) ≤ σ(η) ∧ R(C, η) ∩ ∃k ∈ [nmin - 1] such thatXTηn (δ) ∈ [sk - C, sk + C] .
47
Published as a conference paper at ICLR 2022
Proof. Let A1× , A2× be the events defined in the proof of Lemma G.17. Based on the given > 0, fix
some e = 4exp(2M^(e)) . Fix some N > αl*∙
Choose δ small enough so that claim in Lemma G.20 holds for the prescribed. Using the same
arguments in Lemma G.20, we have the following inclusion of events:
B3×(, δ, η) ∩ A1× (, δ, η) ∪ A2×(e, δ, η)c
⊆{∀j = 2,3,…，l*, Tj(δ)- Tj-i(δ) ≤ 〃n} ∩{∀j = 1,2,3,…，l*, η∣Wf(δ)∣ >”.
Therefore, on event B3× (, δ, η) ∩ A1× (, δ, η) ∪A2× (e, δ, η) , we can apply Corollary G.7 and G.10
and conclude that ZTη⑻ has (-e, δ, η)-overflow, and its (-e, δ, η)-overflow endpoint lies
(Sk — 3(p + P + 3)g Sk + 3(p+ P + 3)e)
for some k ∈ [nmin - 1]. Using Lemma G.15 and Corollary G.16, we have that (for any sufficiently
small η)
PB3×(, δ, η) ∩ A1×(, δ, η) ∪ A2×(e, δ, η)c
≤δα( H≡ )l*τ±1 μ(E(i) ∩ h-l((sk-工 Sk + ^)))
η	k=1
where ^ = 3(ρ + e + 3)e. Besides, as established in the proof of Lemma G.17, we have
PA1×(,δ,η)∪A2×(e,δ,η) =o(ηN)
for all sufficiently small δ. In conclusion, we only need to choose
nmin -1
ψ(E) = ^X μ(E(Y) ∩ h I((Sk - 3(P + P + 3)e, sk + 3(p+ P +3)E))).
k=1
To conclude the proof, just note that by combining the continuity of measure with the conditional
probability argument leading to eq.(G.69), We can show that limeψο Ψ(e) = 0.	□
Lastly, we establish the lower bound for the probability of the most likely way for SGD iterates to
exit the current attraction field: making l* large jumps in a relatively short period of time. Recall that
e is the fixed constant in eq. (G.26)-eq. (G.29).
Lemma G.22. Given E ∈ (0, e/3), it holdsfor any sufficiently small δ > 0 such that
lim inf
自
inf∣x∣≤2e Px(A°(e, δ,η))
l*	1
(H(i∕η)∕η)l 1
≥ c*δα
where the event is defined as
A°(e,δ,η) = {σ(η) <R(e,η), σ(η) = T* (δ), XT(Z ∈ [s- - e, s+ +E]}
∩ {τjη(δ) - Tj-I(δ) ≤ 2MM di∕η↑ ∀j = 2,3,…,ι*}
and the constant
c. = 1(-)l*α(三)l*-1
*	2(2b)	(4M)
is strictly positive and does not vary with E, δ.
Proof. Let A1× , A2× be the events defined in the proof of Lemma G.17. Fix some N such that
N > αl*. Based on the given e > 0, fix some e = 4eχp(2M^9). We only consider δ < M.
Furthermore, choose δ small enough so that eq. (G.72) and eq. (G.73) hold for the chosen N and E.
Also, We only consider η small enough so that ηM < b ∧ E.
48
Published as a conference paper at ICLR 2022
Due to eq. (G.26)-eq. (G.29), we can, without loss of generality, assume that r = s+, and in this case
we will have
l*b - 100l*e >s+ + 100l*e.
Under this assumption, we will now focus on providing a lower bound for the following event that
describes the exit from the right side of Ω (in other words, by crossing s+)
A→(e,δ, η) = {σ㈤ < Ran), σS) = Tn ⑻,XTη > s+ +，
∩{τjn(δ)- Tj-ι(δ) ≤ 2Md1∕n]∀j = 2,3,…,l*}.
First, define event
A3(δ,n) = {wjn(δ) ≥ 2b ∀j = ι, ∙∙∙,ι*, Tj (δ)- τn-ι(δ) ≤ S d1∕n]∀j = 2,…，l*},
and observe some facts on event A3(δ, η) ∩ (a; (e, δ, n) ∪ A× (e, δ, n)).
•	|X? | ≤ 3e∀n < Tn(δ); (due to A× not occurring)
•	XTη η (δ) ∈ [b - 3, b + 3]; (due to W1η ≥ 2b and the effect of gradient clipping at step T1η,
as well as the fact that XTnη-1 ∈ [-3, 3] from the previous bullet point)
•	Due to |f 0(∙) | ≤ M and δ < M, one can see that (for any n ≥ 1)
SUp	lηf0(χ)l + ∣ηZ≤δ,n| ≤ 2ηM；
x∈[-L,L]
this provides an upper bound for the change in SGD iterates at each step, and gives us
Xn ∈ [b - 3e - Gb + 3e + “ ∀T?(δ) ≤ n < Tn(δ)
where we also used T?(δ) - Tn(δ) ≤ 品 d1∕η]
•	Therefore, at the arrival time of the second large jump, we must have XTη ⑼ ≥ 2b — 3e - e;
•	By repeating the argument above inductively, we can show that (for all j = 1,2,…，l*)
Xn ∈ [(j - 1)b - 3e - (j - 1)e, (j - 1)b + 3e + (j - 1闰 ∀j- ≤ n < Tj
XTη ∈ [jb - 3e - (j - I)Gjb + 3e + (j - 1闰；
Tj
In particular, we know that Xn ∈ Ω for any n < T? (so the exit does not occur before Tn),
and at the arrival of the l*-th large jump, we have (using 3e ‹ e)
X2η zʌ、≥ l*b -1e > s+ + e.
Tl* ⑷ _	+
In summary, we have shown that
A3(δ, η) ∩ (A; (e, δ, η) ∪ A× & δ, η))c ⊆ A→(e, δ, η).
To conclude the proof, just notice that (for sufficiently small η)
P(A3 (δ, η) ∩ (A×(e,δ,η) ∪ A× (e, δ, η)) c)
≥ P(AW(δ, η)) - P(A× (e, δ, η)) - P(A× g δ, η))
≥ P(A3(δ, η)) - ηN	due to eq. (G.72) and eq.(G.73)
**
≥ (-H⅛7ηy) (4mh(δ∕η"η)	-ηN duetoLemmaG.4
≥ 2c*δα(H(1∕η)∕η)l*-1 - ηN for all η sufficiently small, due to H ∈ RV-α
≥c*δα(H (1∕η)∕η)l*τ.
□
49
Published as a conference paper at ICLR 2022
In order to present the main result of this section, we need to take into account the loss landscape
outside of the current attraction field Ω. Recall that there are n而n attraction fields on f. For all
the attraction fields different from Ω, We call them (Ωk)%=1-1 where, for each k ∈ [nmin - 1], the
+
attraction field Ωk = (Sk , s[) with the corresponding local minimum located at Tmk. Also, recall that
σ(η) is the first time Xn exits from Ω. Building upon these concepts, we can define a stopping time
nmin -1
τ (η,	)	=∆	min{n ≥ 0 :	Xnη	∈ [ [mek	- 2,	mek	+ 2]}	(G.75)
k=1
as the first time the SGD iterates visit a minimizer in an attraction field that is different from Ω.
Besides, let index Jσ(η) be such that
_	,	、	__g	~	.	.	r
Jσ(η) =	j ^⇒ Xσ(η)	∈ eej	Nj	∈ [nmin	- 1]∙	(G.76)
In other words, it is the label of the attraction field that Xnη escapes to. Lastly, define
λ(η) = H(i∕η)(H(i∕η)∕η)1-1,	(g.77)
V ω = μ(E(0)),	(G.78)
νΩ = μ(E(0) ∩ h-1(Ωk)) ∀k ∈ [nmin - 1].	(G.79)
For definitions of the measure μ, set E, and mapping h, see eq. (G.64) and eq. (G.65).
Now we are ready to state Proposition G.23, the most important technical tool in this section. In
eq. (G.80) and eq. (G.81), we provide upper and lower bounds for the joint distribution of first exit
time σ and the label Jσ indexing the attraction field we escape to; it is worth noticing that the claims
hold uniformly for all u > C. In eq. (G.82) and eq. (G.83), we provide upper and lower bounds for
the joint distribution of when we first visit a different local minimum (which is equal to τ ) and which
one we visit (indicated by Xτη). The similarity between eq. (G.80) eq. (G.81) and eq. (G.82)eq. (G.83)
suggests a strong correlation between the behavior of the SGD iterates at time σ(η) and that of time
τ(η, ), and this is corroborated by eq. (G.84): we show that it is almost always the case that τ is
very close to σ, and on the short time interval [σ(η), τ(η, )] the SGD iterates stay within the same
attraction field.
Proposition G.23. Given C > 0 and some k0 ∈ [nmin - 1], the following claims hold for all > 0
that is sufficiently small:
limsup sup sup Px (νQλ(η)σ(η) > u, Jσ (η) = k0
ηψ0 u∈(C,∞) x∈[—2e,2e] '	'
≤2C + exp(-(1 - C)3u) Vg + C,	(G.80)
V ω
liminf inf inf Px (νQλ(η)σ(η) > u, Jσ (η) = k]
ηψ0 u∈(C,∞) x∈[-2e,2e]	∖	)
≥ - 2C + eχp(-(1 + C)3u)V - C,	(G.81)
vω
limsup sup SuP Px(vD(n)T(η, e) > u, XSη e)∈ B(mk, 2e))
ηψ0 u∈(C,∞) x∈[—2e,2e]	'	，
≤4C + eχp(-(1 - C)3u) Vg + C,	(G.82)
V ω
liminf inf inf Px 俨1加)丁(n, e) > u, XS )∈ B(mk，, 2e))
ηψ0 u∈(C,∞) x∈[-2e,2e]	∖	T(η,e)	)
V V V V V Ωθ - C
≥ - 4C + eχp(-(1 + C)3u) k C ,	(G.83)
vω
liminf rinf 1px (λ(η)(τ(η, e) - σ(η)) < C, Xn ∈ ωj。⑺ ∀n ∈ [σ(η),τ(η, e
ηψ0 x∈[-2e,2e]	∖
≥1 - C.
(G.84)
50
Published as a conference paper at ICLR 2022
Before presenting the proof to Proposition G.23, we make some preparations. First, we introduce
stopping times (for all k ≥ 1)
Tk(e,δ,η) = min{n > τ⅛-i(e, δ,η) : η∣Zn∣ > δ}
e (e,δ,η) = min{n ≥ Tk (gδ,η): |X£ | ≤ 2e}
with the convention that τ0 (, δ, η) = τe0 (, δ, η) = 0. The intuitive interpretation is as follows. For
the fixed we treat [-2, 2] as a small neighborhood of the local minimum of the attraction field
Ω. All the e partitioned the entire timeline into different attempts of escaping Ω. The interval
[Tek-1, Tek] can be viewed as the k-th attempt. If for σ(η), the first exit time defined in eq. (G.70), we
have σ(η) > Tek, then we consider the k-th attempt of escape as a failure because the SGD iterates
returned to this small neighborhood of the local minimum again without exiting the attraction field.
On the other hand, the stopping times Tk-1 indicate the arrival time of the first large jump during
the k-th attempt. The proviso that Tek ≥ Tk-1 can be interpreted, intuitively, as that an attempt
is considered failed only if, after some significant efforts to exit (for instance, a large jump) has
been observed, the SGD iterates still returned to the small neighborhood [-2, 2]. Regarding the
notations, we add a remark that when there is no ambiguity we will drop the dependency on , δ, η
and simply write Tk, Tek.
To facilitate the characterization of events during each attempt, we introduce the following definitions.
First, for all k ≥ 1, let
jk = #{n = τk-ι(e,δ,η),τk-ι(e,δ,η) + 1,…，ek(e,δ,η) ∧ σ(η): n|Zn| > δ}
be the number of large jumps during the k-th attempt. Two implications of this definition:
•	First, for any k with σ(η) < Tek, we have jk = 0. Note that this proposition concerns the
dynamics of SGD UP until σ(η), the first time the SGD iterates escaped from Ω, so there
is no need to consider an attempt that is after σ(η), and we will not do so in the analysis
below;
•	Besides, the random variable jk is measurable w.r.t. Fτek∧σ(η), the stopped σ-algebra
generated by the stopping time Tek ∧ σ(η).
Furthermore, for each k = 1, 2,…，let
Tk,1 (, δ, η) = Tk-1(, δ, η) ∧ σ(η),
Tk,j (e,δ,η) = min{n > Tkj-1(3 δ,η) : η∣Zn∣ > δ} ∧ σ(η) ∧ ek ∀j ≥ 2,
Wk,j (, δ, η) = ZTk,j(,δ,η) ∀j ≥ 1
with the convention Tk,0(, δ, η) = Tek-1(, δ, η). Note that for any k ≥ 1, j ≥ 1, Tk,j is a stopping
time. Besides, from the definition ofjk, one can see that
Tek-1 + 1 ≤ Tk,j ≤ Tek ∧ σ(η) ∀j ∈ [jk],	(G.85)
and the sequences Tk,j jjk=1 and Wk,j jjk=1 are the arrival times and sizes of large jumps during the
k-th attempt, respectively. Again, when there is no ambiguity we will drop the dependency on , δ, η
and simply write Tk,j and Wk,j .
In order to prove Proposition G.23, We analyze the most likely scenario that the exit from Ω would
happen. Specifically, we will introduce a series of events with superscript × or ◦, where × indicates
that the event is atypical or unlikely to happen and ◦ means that it is a typical event and is likely to be
observed before the first exit from the attraction field Ω. Besides, the subscript k indicates that the
event in discussion concerns the dynamics of SGD during the k-th attempt. Our goal is to show that
for some event A× (, δ, η) its probability becomes sufficiently small as learning rate η tends to 0, so
the escape from Ω almost always occurs in the manner described by (A× (e, δ, η))c. In particular, the
definition of this atypical scenario A× involves the union of some atypical events Ak× , Bk× that occur
in the k-th attempt. In other words, the intuition ofA× is that something abnormal happened during
one of the attempts before the final exit.
Here is one more comment for the general naming convention of these events. Events with label A
often describe the “efforts” made in an attempt to get out of Ω (such as large noises), while those with
label B concern how the SGD iterates return to [-2, 2] (and how this attempt fails). For instance,
51
Published as a conference paper at ICLR 2022
A× discusses the unlikely scenario before Tk,ι*, the arrival of the l*-th Iargejump in this attempt,
while B× in general discusses the abnormal cases after Tk,ι* and before the return to [-2g 2e]. On
the other hand, Ak describes a successful escape during k-th attempt, while Bk means that during
this attempt the iterates return to without spending too much time.
Now we proceed and provide a formal definition and analysis of the aforementioned series of events.
As building blocks, we inspect the process (Xnη)n≥1 at a even finer granularity, and bound the
probability of some events (Ak×,i)i≥0, (Bk×,i)i≥1 detailing several cases that are unlikely to occur
during the escape from or return to local minimum in the k-th attempt. First, for each k ≥ 1, define
the event
A×,o(e,δ,η) = {∃i = 0,1,…，广 ∧ jk s.t.
max	η∣Z≤δ+ι + … + Z≤δ,η | > ———ITC——-O.
j = Tk,i+1,∙∙∙ ,(Tk,i+1-i)∧Tk∧σ(η)	Tk,i+1	j	3ρ + 3ρ + 3 ʃ
(G.86)
Intuitively speaking, the event characterizes the atypical scenario where, during the k-th attempt,
there is some large fluctuations (compared to e) between any of the first l* large jumps (or the first jk
large jumps in case that jk < l*). Similarly, consider event (for all k ≥ 1)
A×,ι(e,δ,η) = {σ(η) < 彳k, jk < l*}	(G.87)
that describes the atypical case where the exit occurs during the k-th attempt with less than l* large
jumps. Next, for all k ≥ 1 we have another atypical event (note that from eq. (G.85) we can see that,
for any j ≥ 1,jk ≥ j implies Tk,j ≤ σ(η) ∧ τek)
A×,2 = {jk ≥ l*, ∃j = 2, 3,…，l* s.t.Tkj- Tkj-I > 2f(e"η}.	(G.88)
representing the case where we have at least l* large noises during the k-th attempt, but for some of
the large noise (from the 2nd to the l*-th), the inter-arrival time is unusually long. Moving on, we
consider the following events (defined for all k ≥ 1)
A×,3 = {jk < l*,不k< σ(η), e⅛ - Tk,1 > 2l*f("η}	(G.89)
that describes the atypical case where the k-th attempt failed but the return to the small neighborhood
[-2I, 2I] took unusually long time.
The following event also concerns the scenario where there are at least l* large noises during the
k-th attempt:
A×,4 = {jk ≥ l*, XTk,l*∣≥ r - J ∃j = 1, 2,…，l* s^t.η∖Wk,j| ≤ ";	(G.90)
specifically, it describes the atypical case where, during this attempt, right after the l* - large noise
the SGD iterate is far enough from the local minimum yet some of the large noises are not that large.
Lastly, by defining events
Ak×,5 =∆ njk ≥ l*, Tk,ι* ≤ σ(η) ∧τek, XTηk,l* ∈	[	[sj-I, sj + I]o,	(G.91)
j∈[nmin-1]
we analyze an atypical case where the SGD iterates arrive at somewhere too close to the boundaries
of Ω at the arrival time of the l* large noise during this attempt. As an amalgamation of these atypical
scenarios, we let
5
Ak× (I, δ, η) =∆ [ Ak×,i(I, δ, η).	(G.92)
i=0
Also, we analyze the probability of some events (Bk× )k≥1 that concern the SGD dynamics after the
l*-th large noise during the k-th attempt. Let us define
B×,1(g δ, η) = {jk ≥ l*, XTk,ι* ∈ [s- + e, s+ - e], Tkj- TkjT ≤ 2 ^-^ ∀j = 2, 3,…，l*}
B×,2(e,δ,η) ={Tk- Tk,ι* > p(c)/n} ∪ {σ(η) < ek}
Bk×(I,δ,η)=∆Bk×,1 ∩ Bk×,2	(G.93)
52
Published as a conference paper at ICLR 2022
where ρ(∙) is the function in Lemma G.13. From the definition of B×, in particular the inclusion of
Bk×,2, one can see that the intuitive interpretation of event Bk× is that the SGD iterates did not return
to local minimum efficiently (or simply escaped from the attraction field) after the l*-th large noise
during the k-th attempt. In comparison, the following events will characterize what would typically
happen during each attempt:
Ak(e,δ,η) = {jk ≥ l*, σ(η) = Tk,ι*, Xk* / [s- - e,s+ + e],
Tkj- Tkj-I ≤ η ∀ ∀j = 2, 3,…，l*},	(G.94)
Bk(e,δ,η) = {σ(η) >T ,斤k— Tk,ι ≤ %m9+}∙	(G.95)
kη
Intuitively speaking, Ak tells us that the exit happened right at Tk,ι*, the arrival time of the l* -th
large noise during the k-th attempt, and Bk tells us that the first exit from Ω did not occur during the
k-th attempt, and the SGD iterates returned to local minimum rather efficiently. All the preparations
above allow use to define
k-1
A×(e,δ,η) = [	\ (A× ∪ B× ∪ A°)c ∩ (A× ∪ B×)∙	(G.96)
k≥1	i=1
We need the next lemma in the proof of Proposition G.23. As mentioned earlier, the takeaway is that
A× is indeed atypical in the sense that we will almost always observe (A× )c.
Lemma G.24. Given any C > 0, the following claim holds for all e > 0, δ > 0 sufficiently small:
lim sup sup Px(A× (e, δ, η)) < C∙
nψ0	∣x∣≤2e
Proof. We fix some parameters for the proof. First, with out loss of generality we only consider
C ∈ (0, 1), and we fix some N > αl*. Next we discuss the valid range of e for the claim to hold. We
only consider e > 0 such that
e < 6(p + e + 3) ∧ 3
where P and P are the constants in Corollary G.7 and Corollary G.10 respectively, and e° is the
constant in eq. (G.8). Moreover, recall function Ψ in Lemma G.21 and the constant c* > 0 in Lemma
G.22. Due to limeψo Ψ(e) = 0, it holds for all e small enough such that
业<C
c*
(G.97)
In our proof we only consider e small enough so the inequality above holds, and the claim in Lemma
G.13 holds. Now we specify the valid range of parameter δ that will be used below:
•	For all sufficiently small δ > 0, the claim in Lemma G.14 will hold for the prescribed e and
with N0 = N ;
•	For all sufficiently small δ > 0, the claims in Lemma G.17, Corollary G.18, Corollary G.19
and Lemma G.20 will hold with the prescribed e and N ;
• For all sufficiently small δ > 0, the inequalities in Lemma G.21 and G.22 will hold for the e
we fixed at the beginning.
We show that the claim holds for any e, δ small enough to satisfy the conditions above.
First, recall that
a×,o(e,δ,η) = n∃i = 0,1,…，l* ∧ jk s∙t∙
Fi+…max-* ∧σ(η) ηlZ≤kδi+ι+…+Z≤δ,ηι>
e
3ρ + 3p + 3 }∙
53
Published as a conference paper at ICLR 2022
Due to our choice of δ stated earlier and Lemma G.12, there exists some ηo > 0 such that for all
n ∈ (0,η0),
P(A×,0(e,δ,η)) ≤ ηN Vk ≥ 1.
(G.98)
Similarly, recall that A×1(e, δ, η) = {σ(η) < τk, jk < l*}. Let us temporarily focus on the first
attempt (namely the case k = 1). From Lemma G.17 and our choice of E and δ, We know the existence
of some η1 > 0 such that
sup Px(Aj1(3δ,η)) ≤ ηN Vη ∈ (0,ηι).
∣x∣≤2e	'
(G.99)
Next, for A，= {jk ≥ 产,∃j = 2,3,…，l* s.t. Tkj - Tkj-1 > 2t(e)∕η∣, from Corollary G.18
and our choice of δ at the beginning, we have the existence of some η2 > 0 such that
sup Px(A×2(3δ,η)) ≤ ηN Vη ∈ (0,η2)∙	(G.100)
∣x∣≤2e	'
Moving on, for A×3 = {jk < ∕*, τ⅛ < σ(η), τ⅛ - Tk,1 > 2∕*t(e)∕η∣, due to Corollary G.19 and
our choice of e, δ, we have the existence of some η3 > 0 such that
sup Px(Aj3(3 δ,η)) ≤ ηN Vη ∈ (0,η3)∙	(G.101)
∣x∣≤2e	'
Asfor A×,4 = {jk ≥ l*, ∣XTk,l* | ≥ r - e, ∃j = 1,2,…，l* s.t. η∣Wkj∣ ≤ S}, from Lemma G.20,
one can see the existence of η4 > 0 such that
sup Px(A×4(3δ,η)) ≤ ηN Vη ∈ (0,η4).	(G.102)
∣x∣≤2e	'
LaStIy, for a×,5 = {jk ≥ l*, Tk,ι* ≤ σ(η)∧不k, X兀* ∈ Uj∈[nImn-1]岛-e, sj∙ + e]}, fromLemma
G.21 we see the existence of η5 > 0 such that
sup Px(A×5(e,δ,η)) ≤ 2δ0Ψ(eMH(1∕η)∕η)' -1 Vη ∈ (0,η5).	(G.103)
∣x∣≤2e	'	/
Recall that A×(e,δ,η) = U5=0A×i(e,δ,η). Also, for definitions of B×,Ak,B◦, see
eq. (G.93),eq. (G.94),eq. (G.95) respectively. Our next goal is to establish bounds regarding the
probabilities of these events. First, if we consider the event ∩;=ι(A/ U Bj)c ∩ B；, then the inclusion
of the (B°)k=ι implies that during the first k attempts the SGD iterates have never left the attraction
field, so
kk
∩ (A× U B× )c ∩ B° = ( \ (A× U B×)c ∩ Bj) ∩ {σ(η) > τk}.
j=i	j=i
Next, note that
k-1
Px(B× ∣ ∩ (A× U B×)c ∩ Bj)
j=i
k-1	k-1
=Px(B×,1 ∣ ∩ (A× U B× )c ∩ Bj)Px (b×,2 I ( ∩ (A× U B×)c ∩ Bj) ∩ B×,J
j=i	j=1
.	2^(e)	k-1	——	—
≤Px(jk ≥	l*, Tkj-	Tkj-I	≤ -ʌ-)	Vj	= 2, 3,∙∙∙ ,l*	∣ ∩(A×	U B×)c	∩ Bj)
η	j=1
k-1
∙ Px (b×,2∣(∩ (A× U B×)c ∩ Bj) ∩ B×,1).
j=1
54
Published as a conference paper at ICLR 2022
From the definition of the events A×, B×, B°, one can see that ∩k-1(A× U B×)c ∩ B° ∈ Fτk-1∧σ(η),
and on this event ∩k-1(A× U B×)c ∩ B； We have σ(η) > τ⅛-1. So by applying strong Markov
property at stopping time τ⅛-1 八 σ(η), we have
k-1
Px(B× | ∩ (A× U B×)c ∩ B°)
j=i
k-1
≤P(Tj⑻-Tj-ι(δ) ≤ 2t(e)∕η Vj ∈ [l* - 1]) ∙ Px (b×,2 ∣ ( ∩ (A× U B×)c ∩ B；) ∩ B×,J
j=i
ι* -1	k-1
≤2 (H(δ∕η)t(e)∕η)	∙ Px (b×,2 ∣ ( ∩ (A× U B×)c ∩ B；) ∩ B×,j
j=i
(for all η sufficiently small due to Lemma G.4)
≤4( T )'*T( Hl/η2 )'*T ∙ Px(B×,2∣( ∩(a× U B×)c ∩ B；) ∩ B×,1)
/	j=i
for all sufficiently small η, due to H ∈ RV-α(η). Meanwhile, note that
•	(∩k-1(A× U B×)c ∩ B；) ∩ B×j ∈F%*;
•	on this event (∩k-1(A× U B×)c ∩ B；) ∩ Bj」 we have σ(η)八 Tk > Tk,ι* and XTk 产 ∈
[s- + €, s+ — e].
Therefore, using Lemma G.13 and strong Markov property again (at stopping time T*ι*), we know
the following inequality holds for all η sufficiently small:
sup sup Px
k≥1 ∣x∣≤2e
k-1
(B×,2∣(∩ (a× U b× )C ∩ B；)
j=i
∩ Bk× 1
k , 1
k-1
ek - TkM ≤ P(e)∕η}C| (∩ (A× U B×)c ∩ B；) ∩ B×j)
j=i
sup sup Px({σ(η) > Tk
k≥1 ∣x∣≤2e	'
≤Ψ(e)
Therefore, we know the existence of some η6 > 0 such that
sup Px(B× | ∩(A× U B×)c ∩ B；) ≤ Φ(e)δα(HI也)l -1 Vη ∈ (0,ηβ), Vk ≥ 1.
Ix∣≤2e	j=1	η η j
(G.104)
Similarly, we can bound conditional probabilities of the form Px (A× | ∩j-1 (A× U Bj)c ∩ B；). To
be specific, recall that A× = U5=0A×i. By combining eq. (G.98)-eq. (G.103) with Markov property,
we know the existence of some η7 > 0 such that
JUp Px(A× | ∩(A× U B×)c ∩ B；) ≤ 5ηN + 2Φ(e)δα(Hl/η) J -1 Vη ∈ (0,η7), Vk ≥ 1.
j	(G.105)
On the other hand, a lower bound can be established for conditional probability involving A； , the
event defined in eq. (G.94) describing the exit from Ω during an attempt with exactly l* large noises.
55
Published as a conference paper at ICLR 2022
Using Lemma G.22 and Markov property of (Xn)n>ι, one can see the existence of some ηs > 0
such that
jnf 尸“阂 | ∩(A× U B×)c ∩ B°) ≥ c*δα(H≡η))' T ∀η ∈ (0,ηβ).	(G.106)
囹—e	j=1	η
In order to apply the bounds eq. (G.104)-eq. (G.106), We make use of the following inclusion
relationship:
k-1
(∩ (A× U B×)c ∩ Bj) ∩ (A× U B×)c C Ak U B".	(G.107)
3=1
To see why this is true, let us consider a decomposition of the event on the L.H.S. of eq. (G.107). As
mentioned above, on event ∩3-1(A× U B× )c ∩ Bk we know that σ(η) > τ⅛-ι, so the k-th attempt
occurred and there are only three possibilities on this event:
, jk < l*;
•	jk ≥ 1*, XT七,” ∈ Ω;
,	jk ≥ 1*, Xk* ∈ Q
Let us partition the said event accordingly and analyze them one by one.
,	On (∩k-1(A× U B×)c ∩ Bk) ∩ (A× U B×)c ∩ {jk < l*}, due to the exclusion of A×
(especially A× 1 and A×3), we can see that if jk < l*, then we must have σ(η) > τk and
τk — Tk,1 ≤ 2∕*t(e)∕η. Therefore,
k-1
(∩ (A× U B×)c ∩ Bk) ∩ (A× U B×)c ∩ {jk < l*} C Bk.
3=1
, On (∩k-1(A× U B×)c ∩ Bk) ∩(A× U B×)c∩{jk ≥ l*, XTk * ∈ Ω}, then the exclusion
of A×2 implies that Tkj - Tkj-I ≤ 2t(e)∕η for all j = 2, ∙∙∙ , l*, and the exclusion of
A×5 tells us that if XTk l* ∈ Ω, then we have XTk l* ∈ [s- - 3 s+ + e]. In summary,
k-1
(∩ (A× U B×)c ∩ Bk) ∩ (A× U B×)c ∩{jk ≥ l*, XTW ∈ Ω}C Ak.
3 = 1
, On (∩k-1(A× U B×)c ∩ Bj) ∩ (A× U B×)c∩{jk ≥ l*, XTk * ∈ Ω}, ⅛e exclusion of
A×,2 again implies that Tkj - Tkj-I ≤ 2t(e)∕η for all j = 2, ∙ ∙ ∙ ,l*, hence Tkr - Tk,1 ≤
2l*t(e)∕η. Similarly, the exclusion of A×5 tells us that if XTk l* ∈ Ω, then we have
XTk l* ∈ [s- + e, s+ - e]. Now since B× did not occur (see the definition in eq. (G.93)), we
must have σ(η) > τ⅛ and τ⅛ - Tk,ι* ≤ ρ(e)∕η, hence τk - Tk,1 ≤ 2l 叱+。9. Therefore,
k-1
(∩ (A× U B×)c ∩ Bk) ∩ (A× U B× )c ∩{jk ≥ l*, XTW ∈ Ω}C Bk.
3 = 1
Collecting results above, we have eq. (G.107). Now we discuss some of its implications. First, from
eq. (G.107) we can immediately get that
k-1	k-1
(∩ (A× U B×)c ∩ Bk) ∩ (A× U B×)c = (∩ (A× U B×)c ∩ Bk) ∩ (A× U B×)c ∩ (Ak U BQ.
j=1	j=1
(G.108)
56
Published as a conference paper at ICLR 2022
Next, recall the definitions of Ak in eq. (G.94) and B◦ in eq. (G.95), and one can see that Ak and Bk
are mutually exclusive, since the former implies that the first exit occurs during the k-th attempt
while the latter implies that this attempt fails. This fact and eq. (G.108) allow us to conclude that
k	k	k-1
∩(A× UB× U A-)c = ∩(A× UB×)c ∩ B≡ = (∩ (A× U B×)c ∩ Bj ∩ (A× U B× U AQc.
i=1	i=1	i=1
(G.109)
Now we use the results obtained so far to bound the probability of
/k-1	、
A×(e,δ,η) = U ∩ (A× U B× U A°)。∩ (A× U B×).
k≥1 ' i=1	)
Using eq. (G.109), We can see that (for any x ∈ [-2e, 2e])
Px(A×(e,δ,η))
/ / k-1
X 叱 ∩ (A×
k≥1	∖'i=1
U B× U A°)c ∩ (A× U b×)
/ / k-1
X %	∩(A×
k≥1	∖'i=1
U B×)c ∩ B°	∩ (A× U B×)
k-1
X Px (a× U B× | ∩ (A× U B×)c ∩ Bj
k≥1	i=1
k-1 j	l j-1
. Y Px ( ∩ (A× U B×)c ∩ B◦ ∣ ∩ (A× U B×)c ∩ Bj
j=1	i=1	i=1
k-1
X Px (a× U B× | ∩ (A× U B×)c ∩ Bj
k≥1	i=1
k-1	j-1	j-1
- ∏ Px ((∩ (A× U B×)c ∩ B°) ∩ (A× U B× U Aj )c∣ ∩ (A× U B×)c ∩ Bj
j=1	i=1	i=1
k-1
=X Px (a× U B× | ∩ (A× U B×)c ∩ Bj
k≥1	i=1
k-1	j-1
. ∏ Px ((A× U B× U A-)c∣ ∩ (A× U B×)c ∩ B；)
j=1	i=1
k-1	k-1	j-1
≤ X Px	(a×U	B×	|	∩	(A×	U B×)c ∩ B)Y	1	-	Px (A；|	∩(A×U B×)c ∩ BD
k≥1	i=1	j=1	∖	i=1	)
57
Published as a conference paper at ICLR 2022
This allows us to apply eq. (G.104)-eq. (G.106) and conclude that (here we only consider η <
min{ηi : i ∈ [8]} ),
sup Px(A×(, δ, η))
∣x∣≤2e
≤
X(5ηN + 2ψMα( H(≡
k≥1	η
1-c*δα( H(≡ 广)1
/	. . .、l*-1
5ηN + 2Ψ(f)δα( Hn/η )
=------------；-------Γ7*≡1-----
c*δα( Hfn )
≤ 2W(') + 5η	for sufficiently small η, due to H ∈ RV-a(η) and our choice of N > αl*
c*
≤业<C for all η small enough such that 5ηα< Ψ().
c*
The last inequality follows from our choice of in eq. (G.97). This concludes the proof.
□
Having established Lemma G.24, we return to Proposition G.23 and give a proof. Recall that, aside
+
from the attraction field Ω = (s-, s+), there are n曲口 一 1 other attraction fields Ωk = (Sk , s[) (for
each k ∈ [nmin - 1]). Besides, the function λ(∙) and constants vω, Vk are defined in eq. (G.77)-
eq. (G.79).
Proof of Proposition G.23. We fix some parameters for the proof. First, with out loss of generality
we only need to consider C ∈ (0, 1). Next we discuss the valid range of for the claim to hold. We
only consider > 0 such that
€	€0
e < 6(p + e + 3)” 3
where P and ρe are the constants in Corollary G.7 and Corollary G.10 respectively, and €0 is the
constant in eq. (G.8). Due to continuity of measure μ, it holds for all € small enough such that (let
^ = 3(p + ρ + 3)e)
μ(E(0))
μ(E(9)
μ(E (0))
μ(E(- 9)
μ(h-1((s- - 2^, S- + 2^) ∪ (s+ - 2^, s+ + 2^)))
μ(E(-^))
μ(E(^) ∩ (S-,- ^, s+o + ^))
μ(E (^))
μ(E(-^) ∩ (s-o + 2^, s+o - 2^))
< 1/(1 - C),
> 1/(1 + C),
≤C
≤	V+C
(G.110)
(G.111)
(G.112)
(G.113)
(G.114)
μ(E (-f≡))
-C
≥	vω-
In our proof we only consider € small enough so the inequality above holds, and the claims in Lemma
G.13 hold. Moreover, we only consider € and δ small enough so that Lemma G.24 hold and we have
lim sup Pχ(A×(e,δ,η)) < C.	(G.115)
n，0 ∣x∣≤2e
We show that the desired claims hold for all €, δ sufficiently small that satisfy conditions above.
First, in order to show eq. (G.84), we define event
A×(e,δ,η) = (A×(e, δ,η))c∩
{λ(η)(τ (η, €) - σ(η)) ≥ C or ∃n = σ(η) + 1,…，τ (η,e) such that Xn ∈ Ω j ⑺}.
58
Published as a conference paper at ICLR 2022
Since λ ∈ RV-ι-ι*g-i) and α > 1, for the E We fixed at the beginning of this proof, P(E) is a fixed
constant as well (the function P is defined in Lemma G.13) and we have limηj0 λ(η)ρ(E)∕η = 0. Next,
the occurrence of A× (E, δ, η) c (in particular, the exclusion of all the Ak×,5 defined in eq. (G.91)),
we know that Xση(η) ∈/ [sJ- - E, sJ- + E] ∪ [sJ+ - E, sJ+ + E] (recall that for any k ∈ [nmin - 1], we
+
have Ωj = (s- , s+); for definition of Jσ see eq. (G.76)). Meanwhile, for all η sufficiently small, we
have E∕λ(η) > p(E)/n. Therefore, using Lemma G.13 we can see that (for all η sufficiently small)
sup Pχ(A× (E,δ,η) | (A×(e, δ,η))c) ≤ C.	(G.116)
∣x∣≤2e	、	/
Lastly, observe that
P({λ(η) (τ(η, E)- σ(η)) ≥ C or ∃n = σ(η) + 1,…，τ(η, E) such that Xn ∈ Ωj。⑺})
≤pχ ((A×)c ∩ {λS)(τ(η, E)- σ(η)) ≥ C
or ∃n = σ(η) + 1,…，τ (η, e) such that Xn ∈ Ω Jσ(η)}) + Pχ(A×)
so by combining eq. (G.115) with eq. (G.116), we can obtain eq. (G.84).
Moving on, we discuss the upper bounds eq. (G.80) and eq. (G.82). Recall that the fixed constant
k0 ∈ [nmin - 1] is prescribed in the description of this proposition. Let us observe some facts on event
(A× (E, η, δ))c ∩ {Jσ(η) = k0}: If we let J(E, δ, η) =∆ sup{k ≥ 0 : τek < σ(η)} be the number of
attempts it took to escape, and
J↑(e, δ, η) = min{k ≥ 1 : Tk,ι has(3(p+ e +3)e, δ, η)—overflow},
then for all η sufficiently small, we must have J ≤ J↑ on event (A× (E, η, δ))c ∩ {Jσ (η) = k0}. To
see this via a proof of contradiction, let us assume that, for some arbitrary positive integer j , there
exists some sample path on (A× )c ∩ {Jσ(η) = k0} such that J↑ = j < J. Then from the definition
of (A× )c, in particular the exclusion of event Aj×,0 (see the definition in eq. (G.86)), for all sufficiently
small η, we are able to apply Corollary G.10 and G.7 and conclude that XT 匕* ∈ Ω: indeed, using
Corollary G.10 and G.7 we can show that the distance between XTη * and the perturbed ODE
eeη (Tj,l* - Tj,1,0; (0, Tj,2 - Tj,1,…，Tj,l* - T∙,l), (ηWj,1,…，ηWj,l*))
is strictly less than 3(ρ + ρ + 3)e; on the other hand, the definition of(3(p + ρ + 3)e, δ, η) —overflow
implies that
e7 (T^,l* - Tj,1,0； (0,Tj,2 - Tj,1,…，Tj,l* - T∙,l), (ηWj,1,…，ηWj,l*))
∈ [s- — 3(P + P + 3)e,s+ + 3(/9 + ρ + 3)e].
XT 匕* ∈ Ω, which contradicts our assumption j = J↑ < J. In summary,
(A× )c ∩ {Jσ = k0}, we have J↑(E, δ, η) ≥ J(E, δ, η). Similarly, if we
Therefore, we must have
we have shown that, on
consider
J工(e, δ, η) = min{k ≥ 1 : Tk,ι has ( — 3(p + ρe + 3)e, δ, η)—overflow},
then by the same argument above we can show that J，(e, δ, η) ≤ J(e, δ, η). Now consider the
following decomposition of events.
•	On {J，< J↑}, we know that for the first k such that Tk,ι has ( — 3(ρ + P +
3)E, δ, η —overflow, it does not have 3(P9 + Pe + 3)E, δ, η —overflow. Now we analyze
the probability that Zo does not have (^, δ, n)—overflow conditioning on that it does have
(—E, δ, n)—overflow (recall that we let ^ = 3(ρ + p+ 3)). Using Lemma G.15 and the bound
eq. (G.112), we know that for all η sufficiently small,
sup Px((A×)c ∩{J，< J↑})
∣x∣≤2e	、	)
μ( h-1((s- - 2^, s- + 2^) ∪ (s+ — 2^, s+ +
≤ —--------------,—〜-------------
μ(E(-E))
≤ C.
(G.117)
59
Published as a conference paper at ICLR 2022
•	On (A×)c ∩ {Jσ = k0} ∩ {J↑ = J'}, due to J↑ = J，= J We know that TJ(e,δ,η),ι is
the first among all Tk,1 to have (^ δ, η)-overflow. Moreover, due to {Jσ = k0} and using
Corollary G.10 and G.7 again as we did above, we know that the overflow endpoint of
TJ(e,δ,η),ι is in (s- - ^, s+0 + ^ (recall that Ωk> = (s-, s+0)). In summary, for any n ≥ 0
(A×)c ∩ {Jσ = k0} ∩ {J↑ = JJ >n}
⊆ (A×)c ∩ {J↑ > n}∩ T Tj↑,i has overflow endpoint in (S-O - ^ s+, 十 ^
so using Lemma G.15, we obtain that (for all η sufficiently small)
sup Px (A× )c ∩ {Jσ = k0} ∩ {J↑ = JJ > n}
∣x∣≤2e	、	)
p(e,δ,η; (SkO -工 s+0 + N))
p(^, δ, η)
νΩo + C
(G.118)
V ω
uniformly for any n = 0,1,2, •…due to eq. (G.113).
• On the other hand, on (A× )c, if TJ工」has overflow endpoint in (S- + 2^, sf, - 2^), then
from Definition G.1 we know that TJ工」also has (^ δ, η)-overflow, hence Jj = J↑ = J.
Moreover, using Corollary G.10 and G.7 again, we know that XTη	∈ (Sk-0, Sk+0) so
Jσ = k0. In summary, for any n ≥ 0,	,
(A×)c ∩ {Jσ = k0} ∩{J↑ = JJ > n}
⊇ (A× )c ∩ {JJ > n}∩ {TJJ」has overflow endpoint in (s-z + 2^, s+, - 2^)}
so using Lemma G.15, we obtain that (for all η sufficiently small)
inf Px (A×)c ∩{Jσ = k0} ∩ {J↑ = JJ > n}
≥ inf Px((A×)c ∩{JJ >n})∙ PL Mnf +2^,s+0-功
∣x∣≤2e	∖	/	P(-(≡, δ,n)
≥∣xinf2e Px((A» ∩{JJ >n}) ∙ ⅛C .
(G.119)
uniformly for any n = 0, 1, 2, ∙ ∙ ∙ due to eq. (G.114).
Besides, the following claim holds on event (A× )c.
• From eq. (G.109), the definition of B◦ as well as the definition of event Ak (see eq. (G.94)),
one can see that for any j = 1, 2, ∙ ∙ ∙ , J, we have
τej ∧ σ(η) - Tj,1 ≤
2l*f(e) + ρ(e)
n
• Now ifwe turn to the interval (τej-1, Tj,1] (the time between the start of the j-th attempt
and the arrival of the first large noise during this attempt) for each j = 1, 2, ∙ ∙ ∙ , J, and the
following sequence constructed by concatenating these intervals
S(e, δ, n) = (1, 2,…，t1,1, e1 + 1, el + 2,…，T2,1,…，
τek + 1, τek + 2, ∙ ∙ ∙ , Tk+1,1, τek+1 + 1, τek+2 + 1, ∙ ∙ ∙ ,
then the discussion above have shown that, for
min{n ∈ S(gδ, n): Zn has(3(p + e + 3)e, δ,η∣ -overflow} ≥ Tj,i.
60
Published as a conference paper at ICLR 2022
Meanwhile, from the definition of overflow we know that the probability that Zi has
(3(p + e + 3)e, δ,η) -overflow is equal to
H(δ∕η)p(3(p + ρ + 3)e, δ, η).
• Therefore, if, within the duration of each attempt, we split the attempt into two parts at the
arrival time of the first largejump (Tk,1)k≥1 at each attempt, and define (here the subscript
before or after indicates that we are counting the steps before or after the first large jump in
an attempt)
SbefOre(E, δ, η) = {n ∈ S(E, δ, η) ： n ≤ σ(η)}, Ibefore(g δ, η) = #SbefOre(E, δ, η),
Safter亿 δ, η) = {n / S(e, δ, η) : n ≤ σ(η)}, Iafter(e, δ, η) = #Safter(e, δ, η),
then we have σ(η) = IbefOre + Iafter. Moreover, the discussion above implies that
Iafter ≤ J(21*+(e)+ P(E))∕η
Ibefore ≤ min{n	/	S(e,	δ, η) :	Zn	has(3(P +	e + 3)e,	δ,	η) -overflow}
and on event (A×)c.
Define geometric random variables with the following success rates
Ui(E,δ,η)〜Geom(p(3(p + , + 3)e, δ, η))
U2(E, δ, η)〜GeOm(H(δ∕η)p(3(p + , + 3)e, δ, η)
Using results above to bound IbefOre and Iafter separately on event (A× )c, we can show that (for all η
sufficiently small and any u > 0)
sup	Px (vQλ(η)σ(η) > u, Jσ(η)
x∈[-2e,2e]	'
≤ sup	Px(A×(e, δ, η)) +	sup	Pχ ({vQλ(η)σ(η)	> u,	Jσ(η)	=	k0}	∩	(A×(E,δ,η))c
∣x∣≤2e	x∈[-2e,2e]	'	∙
≤C +	sup	Px ({v°λ(η)σ(η)	> u, Jσ(η)	=	k0}	∩	(A× (e, δ,	η))c)	due to eq. (G.115)
x∈[-2e,2e]	、	)
≤C +	sup Pχ({v"(η)Ibefore(E,δ,η) > (1 - C )u, Jσ (η) = k0}∩ (A×(e,δ,η))c
x∈ [—2e,2e]	1	'
+ sup	Pχ({vQλ(η)Iafter(E, δ,η) > Cu} ∩ (A×(E,δ,η))c
x∈[-2e,2e]	1	■
≤C +	sup Pχ({v"(η)Ibefore(E,δ,η) > (1 - C )u, Jσ (η) = k0}∩ (A×(e,δ,η))c
x∈ [—2e,2e]	1	'
+P(vnλ(η)P(E) + 2l*t(E) ∙ Uι(E,δ,η) > Cu
η
≤C
+ sup	Pχ({vQλ(η)Ibefore(E,δ,η) > (1 — C)u, Jσ(η) = k0} ∩ (A×(e, δ, η))c ∩ {Jψ = J↑})
x∈[-2e,2e]	'	/
+ sup	Pχ((A×)c ∩{JJ < J↑}) + p(vωλ(η)P(E)+ 2l*t(E) ∙ Ui(e,δ,η) > Cu)
x∈[-2e,2e]	'	η	)
≤2C + p(v°λ(η)U2(E, δ,η) > (1 - C)u) Vky+C
+P(vnλ(η)P(E) + % '(e) . Ui(e, δ, η) > Cu)	(G.120)
η
61
Published as a conference paper at ICLR 2022
where the last inequality follows from eq. (G.117) and eq. (G.118). Now let us analyze the probability
terms on the last row of the display above. For the first term, let a(η) = H(δ∕η)p(3(p+ e+3)e,δ, η^.
Due to Lemma G.15, we have (recall that VQ = μ(E(0)))
lim________a(η___________
ηΨ0 λ(η)μ(E(3(p + P + 3)E))
1.
Combining this with eq. (G.110), one can see that for all η sufficiently small,
P卜Q》(n)U2(g δ, η) > (1 — C)U) ≤ p(a(η)Geom(a(η)) > (1 — C)2u) ∀u > 0.
Next, let b(η,u) = p(a(η)Geom(a(η)) > (1 一 C)2u) = P(GeOm(a(η)) > (1-C) u). For
g(y) = log(1 - y), we know the existence of some y0 > 0 such that for all y ∈ (0, y0), we have
log(1 一 y) ≤ 一(1 一 C)y. So one can see that for all η sufficiently small,
logb(u,η) ≤ (1 :C U log(1 — a(η)) ≤ 一(1 一 C)3u
a(η)
⇒ b(u, η) ≤ exp ( — (1 — C)3u)	(G.121)
uniformly for all u > 0.
For the second probability term, if we only consider u ≥ C, then
p(vQλ(η) p9 + ""(C) ∙ Uι(e,δ,η) > CU) ≤ p(vQλ(η) p(e) + 2*乱。∙ g(e,δ,η) >
ηη
Using H ∈ RV-α (η) with α > 1, we get
P(3(P + p + 3)e, δ, η) Ui(c, δ, η) → EXP(I) as η J 0
due to the nature of the Geometric random variable U1. Besides, due to H ∈ RV-α (η) with α > 1
and Lemma G.15, it is easy to show that
λ(η) ρ(e) + 2l*f(e)
lim —y----------n------= 0.
ηψ0 P(3(P + e + 3)e,δ,η)
Combining these results with Slutsky’s theorem, we now obtain
μ(E(0))λ(η)P(C)+ 2l*t(E) . Uι(e,δ,η) → 0 as η J 0.
η
Therefore,
Iimsupsup p(μ(E(0))λ(η)P(C) + 4 '(')∙ Uι(δ, η) > Cu) = 0.	(G.122)
nψo u≥c 、	η	)
Plugging eq. (G.121) and eq. (G.122) back into eq. (G.120), we can establish the upper bound in
eq. (G.80). To show eq. (G.82), note that for event
E(c, η) = {ν"(η)τ(η, C) > u, XT(n,e) ∈ B(mk, 2c)},
we have (for definitions of τ, see eq. (G.75))
E(c, η) ⊇{v"(η)σ(η) > u, J,(η) = k0} ∩ 旧 ∈ Ω j⑺ ∀n ∈ [σ(η),τ(η, c)]},
E(C, η)∩{vΩλ(η)σ(η) > u, J(η) = j} ∩ {Xn ∈ ω%(哈 ∀n ∈ [σ(η),τ(η, c)]} = 0 ∀j = k .
Therefore, for all η sufficiently small,
sup Px(E(C, η))
∣x∣≤2e
≤ SUP Pχ(A×) + SUP Pχ((A× )c ∩{Xn ∈ ΩJσ(η) for some n ∈ [σ(η),τ(η,C)]})
∣x∣≤2e	∣x∣≤2e	)
+ sup Pχ((A×)c∩{v"(η)σ(η) > u, J,(η) = k0} ∩ {Xn ∈ Ω j⑺ ∀n ∈ [σ(η),τ(η,C)]})
∣x∣≤2e	'	/
≤4C + exp ( — (1 — C)3u)
νΩo + C
V ω
62
Published as a conference paper at ICLR 2022
uniformly for all u ≥ C, due to eq. (G.115), eq. (G.84) and eq. (G.120).
The lower bound can be shown by an almost identical approach. In particular, analogous to eq. (G.120),
we can show that (for any u > 0)
inf	Px (vQλ(η)σ(η) > u, Jσ(η) = k0)
x∈[-2,2]
≥ 境eJnf 2e]Pχ({vQλ(η)σ(η) > u, Jσ(η) = k0} ∩ (A×(gδ,η))c)
≥ inf	Px ({vD(n)/before(g δ, η) > u, J(η) = k0} ∩ Z (g δ, η))c)
x∈[-2,2]
≥p(vωλ(η)U2(e, δ, η) > (1 - C)U) ^^ - 20
duetoP(E\F) ≥ P(E) - P(F) and eq. (G.115)eq. (G.117)eq. (G.119) where
U2(e, δ,η) 〜GeOm(H(δ∕ηM(- 3(p + e +3b,δ,η)).
Using the similar argument leading to eq. (G.121), we are able to show eq. (G.81), eq. (G.83) and
conclude the proof.	□
Recall that σi(η) = min{n ≥ 0 : Xn ∈ Ωi} and that value of constants qi, qi j are specified via
eq. (G.2)-eq. (G.7). Define
τimin(η, ) =∆ min{n ≥ σi (η) : Xnη ∈ [[mj - 2, mj + 2]},	(G.123)
Ji(η) = j ^⇒ X[(n) ∈ ω ∀j ∈ [nmin].	(G.124)
The following result is simply a restatement of Proposition G.23 under the new system of notations.
Despite the reiteration, we still state it here because this is the version that will be used to prove
Lemma G.2, which is the key tool for establishing Theorem 1, as well as many other results in Section
H.
Proposition G.25. Given C > 0 and i, j ∈ [nmin] such that i 6= j, the following claims hold for all
> 0 that are sufficiently small:
limsup sup	SuP	Pχ (qiλi(η)σi(η) > u, X"、∈ Ωj)
nψ0 u∈(C,∞) x∈(mi —2e,mi + 2e)	'
≤C + exp ( — (1 — C)u) qi,j + C,
qi
limιinf inf	inf	Px (qiλi(η)σi(η) > u, X")∈ Ω八
ηψ0 u∈(C,∞) x∈(mi-2e,mi + 2e)	∖	iSJ	J
≥- C + exp ( — (1 + C)u) qij------,
qi
limsup SuP	SuP	Pχ(qiλi(η)τmn(η, e) > u, XTmin(n e)∈ Ωj)
ηψ0 υ∈(C,∞) x∈(mi-2emi + 2e)	'	i ，
≤C + exp ( — (1 — C)u) qi,j + C,
limιinf inf	inf	Pχ(qiλi(η)τmn(η, e) > u, XTmin(G e)∈ Ωj)
ηψ0 u∈(C,∞) x∈(mi-2e,mi + 2e)	Ii (n, )
≥ — C + exp ( — (1 + C)u) qi,j . C,
limιinf	inf	Px (qiλi(η) (τmin(η,e) — σi(η)) < C,
nψ0 x∈ (mi-2e,mi + 2e)
Xn ∈ Ωji(η) ∀n ∈ [σi(η),τmin(η,e)]) ≥ 1 — C.
Concluding this section, we apply Proposition G.25 and prove Lemma G.2.
63
Published as a conference paper at ICLR 2022
Proof of Lemma G.2. Fix some C ∈ (0, 1), u > 0, and some k, l ∈ [nmin] with k 6= l. Let qi, qi,j be
the constants defined in eq. (G.7).
Fix some Co ∈(0, nC- ∧ 落C). Using Proposition G.25, We know that for all E sufficiently small,
we have
limsup sup	Pχ(qkλk(η)σk(η) > u, X;k(n)∈ Ωj)
ηψ0 x∈(m⅛-2e,m⅛ + 2e)	'	)
≤ CO + eχp ( - (1 - C) U) ''j +^~^ ∀j ∈ [nmin].
qk
Summing up the inequality above over all j ∈ [nmin], we can obtain eq. (G.16). The lower bound
eq. (G.17) can be established using an identical approach.
In order to show eq. (G.19), note that we can find C1 ∈ (0, u) sufficiently small so that
-Cl +exp ( - (1 + Ci) ∙ 2Cι) qk,l - CI ≥ qk,l - C.
qk	qk
Fix such C1 . From Proposition G.25, we also know that for all E small enough, we have
lim0nf x∈(mjnf mfc +2e)Px(qkλk ^k ㈤ > "，^⑺ ∈ M
η	x∈ mk - ,mk
≥ -Ci + exp ( - (1 + Ci) ∙ 2Cι) qk,l - C1.
qk
Then using Px (Xnk⑺ ∈ Ω, ≥ P(qkλk(η)σk (η) > u,X2k(η) ∈ Ω, we conclude the proof for
eq. (G.19).
Moving on, we show eq. (G.18) in the following way. Note that we can find C2 ∈ (0, u) small
enough so that
2C2 + qk，l + C2 < qkl+C.	(G.125)
qk	qk
Fix such C2. Since eq. (G.17) has been established already, we can find some u2 > 0 such that for all
E small enough,
lim sup	sup	Px	qkλk(η)σk(η)	≤	u2	<	C2	(G.126)
n，0 x∈ (m⅛ — 2e,m⅛ + 2e)	、	'
Fix such u2. Meanwhile, fix some C3 ∈ (0, C2 ∧ u2). From Proposition G.25 we know that for all E
sufficiently small,
limsup SUp	Px(qkλk(η)σk(η) > U2, Xn (n) ∈ Ωιj	(G.127)
n，0 x∈(m⅛-2e,m⅛ + 2e)	'	)
≤ C3 +exp ( - (1 - C3)u2) qk,l + C3
qk
≤ C2 + qk,l + C2.	(G.128)
qk
Lastly, observe the following decomposition of events (for any X ∈ Ωk)
Px(Xnk(n) ∈ Ωι) ≤ Px(qkλk(η)σk(η) ≤ U2) + Px (qkλk(η)σk(η) > U2, Xnfc(n) ∈ Ω,.
Combining this bound with eq. (G.125)-eq. (G.128), we complete the proof.	□
H Proofs for Section 2.2
In this section, we show that gradient clipping scheme effectively partitions the entire optimization
landscape of f into different regions based on the radius r and minimum jump number l* for each
attraction field Ωi. Furthermore, when staying in each region, the behavior of SGD iterates closely
64
Published as a conference paper at ICLR 2022
resembles a Markov chain that only visits wider attraction fields in this region. We exclude the trivial
case where nmin = 1 and there is only one attraction field.
This structure is as follows. First we present some key lemmas that can be used to prove the Theorem
2-3 in the main paper. Then we devote the rest of the section to establish those lemmas. In order to
prove Theorem 2, we will make use of the following lemma, where we show that the type of claim in
Theorem 2 is indeed valid if we look at a much shorter time interval. Then when we move onto the
proof of Theorem 2, it suffices to partition the entire horizon into pieces of these short time intervals,
on each of which we analyze the dynamics of SGD respectively.
Lemma H.1. Assume the graph G is irreducible, and let > 0, δ > 0 be any positive real numbers.
For the following random variables (indexed by η)
1	八 t∕λ λaarg (η)c	r	一、
V SmmU (nd) = Wg≡ /	1{XuC∈ U	j du,	(H.1)
0	j:mj ∈/ M large
the following claim holds for any sufficiently small t > 0:
lim sup sup Px V small(η, , t) >	≤ 5δ
n，0 x∈[-L,L]	'	)
Proof of Theorem 2. It suffices to show that for any t> 0, κ > 1 + (α - 1)llarge, > 0, δ ∈ (0, ),
we have
limsupPx(V*(n, t,κ) > 3e) < δ
n，o
for
1	八〃nK」r	一】
V*(n,t,K) = ^/nκr J	1{Xηuc(x) ∈ U	%}du∙
0	j:mj ∈/ M large
Let us fix some > 0, δ ∈ (0, ). First, let
N⑺=⅛⅛an⅛re.
The regularly varying nature of H implies that λlarge(η) ∈ RV1+llarge(α-1) (η). Since κ > 1 +
llarge(α - 1), We know that limao N(η) = ∞. Next, due to Lemma H.1, We can find to > 0 and
n > 0 such that for any η ∈ (0, η)
sup Py (Vsmall (η, , t0) > ) < δ.	(H.2)
y∈[-L,L]
For any k ≥ 1, define
1	/.kbto∕λlarge (η)C
Vk (η)= » /`laiJ	1{xfuce U	Ωj}du.	(H.3)
[tθ/λlarge(η)J J(k-1) bto∕λl"ge(η)C I buc	XM岫	J
It is clear from its definition that Vk stands for the proportion of time that the SGD iterates are outside
of large attraction fields on the interval [(k - 1)[工盛⑺ J, k[入唐⑺ J]. From eq. (H.2) and Markov
property, one can see that for any η ∈ (0, η)
SUp Px(Vfc(η) > e | Xn,…，xηk-i)bt0∕λ⅛≡e(η)C) ≤ δ
x∈[-L,L]
uniformly for all k ≥ 1. Now define K(η) = #{n = 1, 2,…，N(η) : Vk(η) > e}. By a simple
stochastic dominance argument, we have
SUp Px(K(η) ≥ j) ≤ P(Binomial(N(η), δ) ≥ j) ∀j = 1, 2,….
x∈[-L,L]
65
Published as a conference paper at ICLR 2022
Meanwhile, strong law of large numbers implies the existence of some ηι > 0 such that
P( BinOmN(N5),δ) > 2δ) < δ for all η ∈ (0, ηι), thus
sup Pχ(K(η)∕N(η) > 2δ) ≤ δ ∀η ∈ (0, ηι ∧ η).
∣x∣≤L
Lastly, from the definition of K(η) and N(η), we know that for all the N(η) intervals [(k -
1) bMatO®)C, kb入蓝⑺C] With k ∈ [N(η)], only on K(η) of them did the SGD iterates spent more
then proportion of time outside of the large attraction fields, hence
V *(η,t,κ) ≤e+NI.
In summary, we now have
Px(V*(η,t,κ) > 3e) < δ
for all η ∈ (0, ηι ∧ η). This concludes the proof.	□
When introducing Theorem 3 in the main paper, we stated that the results of eliminating sharp minima
can be extended to the more general reducible case. Here we present the corresponding theoretical
results in Theorem H.2 and H.3 below. The main message can be summarized as follows： (a) SGD
with truncated and heavy-tailed noise naturally partitions the entire training landscape into different
regions; (b) In each region, the dynamics of Xnη for small η closely resemble that of a continuous-time
Markov chain that only visits local minima; (3) In particular, any sharp minima within each region is
almost completely avoided by SGD.
When the typical transition graph (see Definition 1 in the main paper) is not irreducible, there will
be multiple communication classes on the graph. Suppose that there are K communication classes
Gι,…，GK. From now on, we zoom in on a specific communication class G ∈ {Gι,…，GK}.
For this communication class G, define IG = max{l* : i = 1,2, ∙∙∙ , nmi∏; mi ∈ G}. For each
local minimum mi ∈ G, we call its attraction field Ωi a large attraction field if l* = IG, and a small
attraction field if li < IG. We have thus classified all mi in G into two groups: the ones in large
attraction fields m1arge,…，miarge and the ones in small attraction fields msman,…，msmall∙ Also,
1	iG	1	iG
define a scaling function λg associated with G as λg(η) = H(1∕η) (H,")) G .
Theorem H.2. Under Assumptions 1-3, if G is absorbing, then there exists a continuous-time Markov
chain Y on {mlarge,…，mm管} such thatfor any X ∈ Ωi, |x| ≤ L (where i ∈ {1, 2,…，nmin}) with
mi ∈ G, and
Xnt∕λG(n)C(X) → Yt(πG(mi)) as η J0
in the sense of finite-dimensional distributions, where πG is a random mapping satisfying (1)
∏G(m) ≡ m if m ∈ {m 1arge, .…,m?：e}; (2) ∏G(m) is a random variable that only takes value in
{mage,…，mlarge} if m ∈ {mSmaal,…，mSmedl}.
GG
We stress that Theorem 3 in the main paper follows immediately from Theorem H.2 above.
Next, to state the corresponding result for a transient communication class G, we introduce a couple
of extra definitions. We consider a version of Xnη that is killed when Xnη leaves G. Define stopping
time
τG(η) = min{n ≥ 0 ： Xn ∈ U Ωi}
i:mi ∈G
(H.4)
as the first time the SGD iterates leave all attraction fields in G, and we use a cemetery state ↑ to
construct the following process χn,n as a version of Xn with killing at tg：
Xt,n = JXn	if n<τG(η),
n	It	if n ≥ τG(η).
(H.5)
66
Published as a conference paper at ICLR 2022
Theorem H.3. Under Assumptions 1-3, if G is transient, then there exists a continuous-time Markov
chain Y with killing that has state space {marge, ∙∙∙ , m，誓,↑} (we say the Markov chain Y is
killed when it enters the absorbing cemetery state ↑) such that for any X ∈ Ωi, |x| ≤ L (where
i ∈ {1, 2, .…,nmin}) with mi ∈ G, and
Xltt7λG(η)C(X) → Yt(πG(mi)) as η J 0
in the sense of finite-dimensional distributions, where πG is a random mapping satisfying (1)
∏G(m) ≡ m if m ∈ {mlarge, ∙∙∙ , mmWg}; (2) ∏G(m) is a random variable that only takes value in
{marge,…，mlGrge,竹 if m ∈ {mSraU,…，msmall}.
To show Theorem H.2 and H.3, we introduce the following concepts. First, we consider the case
where the SGD iterates Xnη is initialized on the communication class G and G is absorbing. For some
∆ > 0, η > 0, define (let B(u, v) =∆ [u - v, u + v])
σ0G (η, ∆) =∆ min{n ≥ 0 : Xnη ∈	[ B(mi, 2∆)}	(H.6)
i: mi ∈G
τ0G(η,∆) =∆ min{n ≥ σ0G(η, ∆) : Xnη ∈	[	B(mi, 2∆)}	(H.7)
i:	mi ∈Gsmall
IG(η, δX j ^⇒ XTG ∈ B(mj, 2δb IG(η, δX j ^⇒ XσηG ∈ B(mj,空)	(H⑻
σkG(η, ∆) =∆ min{n > τkG-1(η, ∆) : Xnη ∈	[	B(mi, 2∆)} ∀k ≥ 1	(H.9)
i	.mi∈G, i=IG-ι
τkG(η, ∆) =∆ min{n ≥ σkG-1(η, ∆) : Xnη ∈	[	B(mi, 2∆)} ∀k ≥ 1	(H.10)
i:mi/Gsmall
IG(η, δX j ^⇒ XTG ∈ B(mj, 2δb eG(η, δX j ^⇒ X：G ∈ B(mj, 2δ^ ≥ 1.
(H.11)
Intuitively speaking, at each τkG the SGD iterates visits a minimizer that is not in a small attraction
field on G, and we use IkG to mark the label of that large attraction field. Stopping time σkG is the first
time that SGD visits a minimizer that is different from the one visited at τkG, and τkG+1 is the first time
that a minimizer not in a small attraction field of G is visited again since σkG (and including σkG). It is
worth mentioning that, under this definition, we could have IkG = IkG+1 for any k ≥ 0. Meanwhile,
define the following process that only keeps track of the updates on the labels (IkG)k≥0 instead of the
information of the entire trajectory of (Xnη)n≥0:
Xη,∆ = ] m/G if ∃k ≥ 0 such that TG ≤ n < τ-G+1
n 0 otherwise
(H.12)
In other words, when n <τG We simply let XnA = 0, otherwise it is equal to the latest “marker” for
the last visited wide minimum up until step n. This marker process X jumps between the different
minimizers of the large attractions in G. In particular, if for some n we have Xnη ∈ B(mj, 2∆) for
some j with mj ∈ Glarge, then we must have XnA = mj, which implies that, in this case, XjT,δ
indeed indicates the location of Xnη .
Note that results in Theorem H.2 and H.3 concern a scaled version of Xη. Here we also define the
corresponding scaled version of the processes
γ*,η ∆ Yn
Xt = Xbt∕λG(n)C
v *,%△ A
Xt	=	G(η)c ,
(H.13)
(H.14)
a mapping T*(n, η) = nλg(η) that translates a step n to the corresponding timestamp for the scaled
processes, and the following series of scaled stopping times
,σ晨η,∆) = τ*(σG(η,δ),哈∙
TKη, ∆) = T(τG(η, 4),n)
(H.15)
67
Published as a conference paper at ICLR 2022
Before presenting the proof of Theorem H.2 and H.3, we make several preparations. First, our proof
is inspired by ideas in Pavlyukevich (2005) and here we provide a briefing. At any time t > 0, if we
can show that X” is almost always in set Ui：m稔 ∈ Giarge B (mi, 2∆) (So the SGD iterates is almost
always close to a minimizer in a large attraction field), then the marker process XJ%δ is a pretty
accurate indicator of the location of X)η, so it suffices to show that the marker process XJ明δ
converges to a continuous-time Markov chain Y .
Second, we construct the limiting process Y and the random mapping πG before utilizing them in
Theorem H.2 and H.3. As an important building block for this purpose, we start by considering
the following discrete time Markov chain (DTMC) on the entire graph G = (V, E). Let PDTMC
be a transition matrix with PDTMC(mi,mj) = μi(Ei,j)∕μi(Ei) for all j = i, and YDTMC =
(YjDT M C)j≥0 be the DTMC induced by the said transition matrix. Let
TGDTMC =∆ min{j ≥0: YjDTMC ∈∕ Gsmall}	(H.16)
be the first time this DTMC visits a large attraction field on the communication class G, or escapes
from G. Lastly, define (for any j such that mj ∈∕ Gsmall)
Pi,j = P(YTDTMC (mi) = mj	(H.17)
as the probability that the first large attraction field on G visited by YDTMC is mj when initialized
at mi.
We add a comment regarding the stopping times TGDTMC and probabilities pi,j defined above. In the
case that G is absorbing, we have YjDTMC(mi) ∈ G for all j ≥ 0 if mi ∈ G. Therefore, in this case,
given any i with mi ∈ G, we must have
TGDTMC = min{j ≥ 0 : YjDTMC(mi) ∈ Glarge},	X	pi,j = 1.
j:	mj ∈Glarge
On the contrary, when G is transient We may have Pjj.m,∈GlargePi,j < 1 and Pjj. m.∈GPi,j > 0.
Lastly, whether G is absorbing or transient, We always have Pij = l{i = j} if mi ∈ Glarge.
Next, consider the following definition of (continuous-time) jump processes.
Definition H.1. A continuous-time process Yt on R is a (Uj)j≥0, (Vj)j≥0 jump process if
Y = ( 0	ift<U0
t	IPj≥0 Vj l[U0 + U1 + --- + Uj, U0 + Uι + --- + Uj + ι)(t) otherwise，
where (Uj)j≥0 is a sequence of non-negative random variables such that Uj > 0 ∀j ≥ 1 almost
surely, and (Vj)j≥0 is a sequence of random variables in R.
Obviously, the definition above implies that Yt = Vj for any t ∈ [Uj, Uj+1).
Now we are ready to construct the limiting continuous-time Markov chain Y. To begin with, we
address the case where G is absorbing. For any m0 ∈ Glarge, let Y (m0) bea (Sk)k≥0, (Wk)k≥0 -
jump process where S0 = 0, W0 = m0 and (for all k ≥ 0 and i, j with mi ∈ Glarge , mj ∈∕ Gsmall)
PWk+1 =mj,	Sk+1 >t	Wk=mi, (Wl)lk=-01, (Sl)lk=0	(H.18)
=P(Wk+1 =	mj,	Sk+1 >t	Wk	= m) = exp(-qit)qj ∀t>	0	(H.19)
where
qi = μi (Ei)	(H.20)
qi,j = ι{i = j }μi(Ei,j) +	X	μi(Ei,k )Pk,j	(H∙21)
k:	mk ∈Gsmall
and pk,j is defined in eq. (H.17). In other words, conditioning on Wk = mi, the time until next jump
Sk+1 and the jump location Wk+1 are independent, where Sk+1 is Exp(qi) and Wk+1 = mj with
68
Published as a conference paper at ICLR 2022
probability qi,j /qi . First, it is easy to see that Y is a continuous-time Markov chain. Second, under
this definition Y is allowed to have some dummy jumps where Wk = Wk+1: in this case the process
Yt does not move to a different minimizer after the k + 1-th jump, and by inspecting the path of Y
we cannot tell that this dummy jump has occurred. As a result, that generator Q of this Markov chain
admits the form (for all i 6= j with mi , mj ∈ Glarge)
Qi,i = -	qi,k, Qi,j = qi,j.
k:k6=i, mk ∈Glarge
Moreover, define the following random function ∏g(∙) such that for any mi ∈ G,
mj with probability qi,j /qi if mi ∈ Gsmall
πG(mi)=	mi	if mi ∈ Glarge	,
(H.22)
By Y (πG(mi)) we refer to the version of the Markov chain Y where we randomly initialize W0 =
πG(mi). The following lemma is the key tool for proving Theorem H.2.
Lemma H.4. Assume that the communication class G is absorbing. Given any m^ ∈ G, X ∈ Ω%,
finitely many real numbers (tι)k= 1 such that 0 < tι < t2 < •…< ty, and a Sequence of strictly
positive real numbers (ηn)n≥1 with limn→0 ηn = 0, there exists a sequence of strictly positive real
numbers (∆n)n≥1 with limn ∆n = 0 such that
•	As n tends to ∞,
(XTn4 (x),…，Xkηn4(x)) ⇒ (跖(∏g (mi)), ∙∙∙ ,Ytk0 (∏G(mi)))	(H.23)
•	For all k ∈ [k0],
n→∞ Px (XIA ∈ U BW,&))
j : mj ∈Glarge
= 0.
(H.24)
JX；”a if t< τ*(τG(η),η)	(H25)
ɪ ↑ otherwise,	^	.
Now We address the case where G is transient, let ↑ be a real number such that ↑ ∈ [-L,L], and
We use f as the cemetery state since the processes Xn or Xjn are restricted on [-L, L]. Recall the
definition of TG defined in eq.(H.4). Analogous to the process X* in eq.(H.5), we can also define
Xt,*,n = (X；n if t < T(TG(η),η)	X币,n,∆
t ∖ ↑ otherwise	, t
Next, analogous to TG , consider the stopping time
TGY =∆ min{t > 0 : Yt ∈/ G}.
When G is transient, due to the construction of Y we know that TGY < ∞ almost surely. The
introduction of TGY allows us to define
Y * = Yt if t < TGY
t I ↑ otherwise.
(H.26)
The following Lemma will be used to prove Theorem H.3.
Lemma H.5. Assume that the communication class G is transient. Given any mi ∈ G, X ∈ Ω%,
finitely many real numbers (tι)k= 1 such that 0 < tι < t2 < •…< tk/, and a sequence of strictly
positive real numbers (ηn)n≥1 with limn→0 ηn = 0, there exists a sequence of strictly positive real
numbers (∆n)n≥1 with limn ∆n = 0 such that
•	As n tends to ∞,
(XtT，nn4(x),…，XJjnn4(x)) ⇒ (Yt1 (∏G(mi)),…，匕ko (∏G(mi)))	(H.27)
•	For all k ∈ [k0],
n	→∞ Px(Xtk*,nn ∈	U	B(mj, ∆n) and Xtk*,nn = ↑) = 0.	(H.28)
j: mj ∈Glarge
69
Published as a conference paper at ICLR 2022
Proof of Theorem H.2 and H.3. We first address the case where G is absorbing. Arbitrarily choose
some ∆ > 0, a sequence of strictly positive real numbers (ηn)n≥1 with limn ηn = 0, a positive
integer k0, a series of real numbers (tj)k= 1 with 0 <tι < .… < tko, and a sequence (Wj)j= 1 with
wj ∈ Glarge for all j ∈ [k0]. It suffices to show that
limPx (Xζ,ηn ∈ B(Wk, ∆)∀k ∈ [k0]) = P(Kk(∏ɑ(mi)) = Wk ∀k ∈ [k0])∙
Using Lemma H.4, we can find a sequence of strictly positive real numbers (∆n)n≥1 with limn ∆n =
0 such that eq. (H.23) and eq. (H.24) hold. From the weak convergence in eq. (H.23), we only need
to show
lim Pχ(x1kηn ∈ B (XtJnAn, △)) =0 ∀k ∈ [k]
For all n large enough, we have 2∆n < ∆. For such large n, observe that
Px (xtk,ηn ∈ B!XU~δ, △))
≤Px(Xttk,ηn ∈/	[	B(mj,2∆n)
j:mj ∈Glarge
(due to definition of the marker process X, see eq. (H.6)-eq.(H.11) and eq.(H.12)-eq. (H.14))
≤Px(Xttk,ηn ∈/	[	B(mj,∆n)
j:mj ∈Glarge
and by applying eq. (H.24) we conclude the proof for Theorem H.2.
The proof of Theorem H.3 is almost identical, with the only modification being that we apply Lemma
H.5 instead of Lemma H.4. In doing so, we are able to find a sequence of (∆n )n≥1 with limn ∆n = 0
such that eq. (H.27) and eq. (H.28) hold. Given the weak convergence claim in eq. (H.27), it suffices
to show that
lim Px(x"ηn ∈B(X[j EnAn, △)) =0 ∀k ∈ [k0].
↑, We must have Xj：nn,An = ∣ as well. Therefore, for all n large enough so that
Px(X"ηn ∈B芭” f △))
=Px(XFn ∈ B(Xtk*m∙nd, ∆), X⅛,ηn = t)
≤Px (Xrnn ∈	U	B(mj, 2∆η), Xrnn = t)
j: mj ∈Glarge
≤Px (X"ηn ∈	U	B(mj, ∆η), Xtt,ηn = t).
j: mj ∈Glarge
Apply eq. (H.28) and we conclude the proof.	□
H.1 Proof of Lemma H.1
First, we introduce another dichotomy for small and large noises. For any γe > 0 and any learning
rate η > 0, we say that a noise Zn is small if
.一 . ~
η∣Zn∣ > ηe
and we say Zn is large otherwise. For this new classification of small and large noises, we introduce
the following notations and definitions:
Z≤,e,n = Znl{r1∖Zη∖ ≤ ηe},	(H.29)
z>,e,n = Zni{η∖Zn∖ > ηe},	(h.30)
~'ʌ-. ,	- ~.
Te1n(γe) =∆ min{n ≥ 1 : η∖Zn∖ > ηγe}.	(H.31)
70
Published as a conference paper at ICLR 2022
Similar to Lemma G.12, the following result is a direct application of Lemma G.11, and shows that
it is rather unlikely to observe large perturbation that are caused only by small noises. Specifically,
since α > 1 we can always find
e ∈ (O, (I - - ∧ I)
α2
β ∈ (1,(2 - 2γ) ∧ α(1- e)).
Now in Lemma G.11, if We let ∆ = e, ∆ = ∆∕2 and E = δ = 1 (in other words, u(η) =
1∕η1-γ, v(η) = ηe/2), then for any positive integer j the condition eq.(G.52) is satisfied, allowing
us to draw the following conclusion immediately as a corollary from Lemma G.11.
Lemma H.6. Given N > 0 and
γ ∈ (0, (1 - -) ∧ 2), β ∈ (1, (2 - 2e) ∧ (α - αγ)),
we have (as η J 0)
p( max	η∣z≤,γ,η + ∙∙∙ + Z≤,γ,η| >ηe/2) = o(ηN).
∖j=1,2,…，d(i∕η)β e	1	j	)
The flavor of the next lemma is similar to that of Lemma G.13. Specifically, we show that, with
high probability, the SGD iterates would quickly return to the local minimum as long as they
start from somewhere that are not too close the boundary of an attraction field (namely, the points
si, s2,…，Snmin). To this end, we consider a refinement of function £(•) defined in eq.(G.38). For
any i = 1,2, ∙∙∙ , nmin, any X ∈ Ωi and any η > 0, γ ∈ (0, 1), we can define the return time to
ηγ -neighborhood for the ODE xη as
t!γi)(x, η) = min{t ≥ 0 : ∣xη(t, x) — m/ ≤ ηγ}.
Given the bound in eq. (G.37) (which is stated for a specific attraction field) and the fact that there
only exists finitely many attraction fields, we know the existence of some c2 < ∞ such that for any
i = 1, 2,…,nmin, any η > 0, any Y ∈ (0,1) and any X ∈ Ωi such that |x 一 Si| ∨ |x 一 Si-ι∣ > ηγ,
we have
4)(χ,η) ≤ c2Y iog(1∕η)∕η
and define function t↑ as
t↑(η, γ) =∆ c2γ log(1∕η).
Lastly, define the following stopping time for any i = 1,2, ∙∙∙ , nmi∩, any X ∈ Ωi and any ∆ > 0
TretUrn (η, ∆) = min{n ≥ 0: Xn (x) ∈ B(m” 2∆)}
where we adopt the notation B(u, v) =∆ [u - v, u + v] for the v-neighborhood around point u.
Lemma H.7. Given
1	1	γe	γe
e∈(0,(1-α)∧(2)), γ∈ (0,16MC2∧4),
and any i = 1, 2, •…,nmn any ∆ > 0, we have
lim,i∏f	I inf	Px(T'eL(η, △) ≤ 2c2Y log%), Xn ∈ Ωi ∀n ≤ TɪI (η, ∆)
ηψ0 x∈Ωi:|x —si-1 ∣∨∣x-Si∣≥2ηγ	∖	η
= 1.
Proof. Throughout this proof, we only consider η small enough such that
2c2Ylog(1∕η)∕η < d(1∕η)β], ηM ≤ η2γ, 2ηγ < ∆∕2, 2ηγ/4 < ηγ.	(H.32)
The condition above holds for all η > 0 sufficiently small because β > 1, 2γe < 1, and γ < γe∕4.
Also, fix some β ∈ (1, (2 一 2ee) ∧ (α 一 aee))
71
Published as a conference paper at ICLR 2022
Define the following events
A×(η) = n max	η∣Z≤,γ,η + …+ Z「| > 卡/\
1	I j=ι,2,…，d(i∕η)βe	1	j	J
A× (η) = {T1η (e) ≤ d(1∕η)β]} (See eq. (H.31) for definition of the stopping time involved)
and fix some N > 0. From Lemma H.6, we see that (for all sufficiently small η)
P(A1×(η)) ≤ηN.
Besides, using Lemma G.4 together with the fact that β < α(1 - γe), we know the existence of some
constant θ > 0 such that
P(A2×(η)) ≤ηθ
for all sufficiently small η.
Now we focus on the behavior of the SGD iterates on event A1× (η) ∩ A2× (η) . Let us arbitrarily
choose some X ∈ Ωi such that |x - Si∣∨∣x - Si-11 > 2ηγ. First, from Lemma G.6 and eq. (H.32),
we know that
M(X)- yft」(x)l ≤ 2ηM exp RMc2Y log(1∕η))
≤ 2η2γe-2Mc2γ ≤ 2ηγe ≤ ηγ ∀t ≤ 2c2γ log(1∕η)∕η	(H.33)
Next, from the definition of the function t↑(∙) and eq. (H.33), We know that for
T⅛,return(x; η, △)= min{n ≥ 0 : yn(x) ∈ B (mi,	+ ηγ)},	(H.34)
we have
TGD,return(X; η, △) ≤ 2c2γ log(1∕η)∕η	(H.35)
yηn > si-1 + ηγ, yηn < si - ηγ ∀n ≤ 2c2γ log(1∕η)∕η.	(H.36)
Furthermore, on event A1× (η) ∩ A2× (η) , due to Lemma G.5 and eq. (H.32), we have that
γe
∣χn(x) — yn (x)|	≤	ηγ/2	exp(2Mc2 log(1∕η)) =	η2-	c2γ	≤ ηγ/4	< ηγ	∀n ≤ 2c2γ log(1∕η)∕η.
Combining this with eq. (H.35), eq. (H.36), we can conclude that (recall that due to eq. (H.32) we
have 2ηγ ‹ ∆∕2)
Tr(eit)urn(η, △) ≤ 2c2γ log(1∕η)∕η
Xn ∈ Ωi ∀n ≤ 2c2γlog(1∕η)∕η
on event
A1×(η) ∩ A2×(η)
c
. Therefore,
lim0lf x∈ΩL∣x-sJn∨∣x-Si∣≥2nY Px (TreiUrn(η, △)≤
2c2Y log(1∕η)
η
Xn ∈ Ωi ∀n ≤ TeiUrn(η, △))
≥ lim inf PA1× (η) ∩ A2× (η)	≥ lim inf 1 - ηN - ηθ = 1.
nψ0	∖ ∖	η η	ηψ0
This concludes the proof.
□
The takeaway of the next lemma is that, almost always, the SGD iterates will quickly escape from the
neighborhood of any si, the boundaries of each attraction fields.
Lemma H.8. Given any γ ∈ (0, 1),t > 0, we have
lim inf inf Px ( min	{n ≥ 0	: Xη	∈ [ [ B(si,	2ηγ)} ≤	τr∕, ɔ	= 1.
ηψ0 x∈[-L,L] x∖	I — n	∈ γ v i, η jf -	H(1∕η)7
72
Published as a conference paper at ICLR 2022
Proof. We only consider η small enough so that
ι+γ
,_0 min	1 |si - si-1 | > 3ηF,
i = 2,3,∙∙∙ ,nmin -1
ηM < ηγ .
Also, the claim is trivial if x ∈/ ∪jB(sj, 2ηγ), so without loss of generality we only consider the case
where there is some j ∈ [nmin] and x ∈ [-L, L], x ∈ B(sj, 2ηγ). Let us define stopping times
TY = min{n ≥ 1: η∣Zn∣ > 5ηγ};	(H.37)
TeLpe = min{n ≥ 0： Xn ∈ ∪j B(sj, 2ηγ)},	(H.38)
and the following two events
A×(η) = {TY > 777∣75},
H(1∕η)
A×(η) = {η∖ZτY | > η 1+2γ}.
First, using Lemma G.3 and the regularly varying nature of H(∙) = P(∖Zι∖ > ∙), We know the
existence of some θ > 0 such that
P(A1×(η)) ≤exp(-1∕ηθ)
for all η > 0 sufficiently small. Next, by definition of TY, one can see that (for any η ∈ (0, 1))
P(AWn)) = Hw-Yy.
Again, due to H ∈ RV-α and 1 - γ > 0, we know the existence of some θ1 > 0 such that
P(A2×(η)) <ηθ1
for all η > 0 sufficiently small. To conclude the proof, we only need to note the following fact
on event A1× (η) ∪ A2× (η) c. There are only two possibilities on this event: TeYscape ≤ TY - 1, or
TeYscape ≥ TY . Now we analyze the two cases respectively.
•	On (A×(n) ∪ A×(n))c ∩ {Tγcape ≤ TY — 1}, we must have TYcape < TY ≤ t/H(1∕n).
•	On (A× (n) ∪ A× (n))c ∩ {Tscape ≥ TY}, we know that at n = TY — 1, there exists an
integer j ∈ {1, 2,…,n面口 — 1} such that Xn ∈ B(sj, 2nY). Now since nM < nY and
η ∖ZTγ ∖ > 5ηY , we must have
∖XTγ — XTγ-ι∖ > 4nY ⇒ XTγ ∈B(sj,2nY).
On the other hand, the exclusion of event A× (n) tells us that ∖XTTY — XTY-ι∖ < 2n 1++γ.
Due to eq. (H.37), we then have Xnη ∈∕ ∪iB(si, 2ηY).
In summary, (A×(n) ∪ A×(n))c ⊆ {TrYeturn ≤ t/H(1∕n)} and this conclude the proof.	□
In the next lemma, we analyze the number of transitions needed to visit a certain local minimizer
in the loss landscape. In general, we focus on a communication class G and, for now, assume it is
absorbing. Next, we introduce the following concepts to record the transitions between different local
minimum. To be specific, for any n > 0 and any ∆ > 0 small enough so that B(mj, ∆) ∩ Ωc = 0
for all j , define
T0 (n, ∆) = min{n ≥ 0 : Xnη ∈ ∪jB(mj , 2∆)};	(H.39)
I0(n, ∆) = j iff XTη0(η,∆) ∈B(mj,2∆);	(H.40)
Tk (n, ∆) = min{n > Tk-1(n, ∆) : Xnη ∈ ∪j6=Ik-1(η,∆)B(mj, 2∆)} ∀k ≥ 1	(H.41)
Ik(n,∆) = j iff XTηk(η,∆) ∈ B(mj,2∆) ∀k ≥ 1.	(H.42)
As mentioned earlier, the next goal is to analyze the transitions between attraction fields it takes to
visit mj when starting from mi when mi , mj ∈ G. Define
Ki (n, ∆) =∆ min{k ≥ 0 : Ik (n, ∆) = i}.
73
Published as a conference paper at ICLR 2022
Lemma H.9. Assume that G is an absorbing communication class on the graph G. Then there exists
some constant p > 0 such that for any i with mi ∈ G, any > 0, and any ∆ > 0,
sup	Px(Ki(η, ∆) > u ∙ nmin) ≤ P(Geom(P) ≥ u) + C ∀u =1, 2,…，
j: mj ∈G; x∈B (mj ,2∆)
sup	Px ∃k ∈ [Ki(η, ∆)] s.t. mIk (η,∆) ∈/ G ≤ C
j: mj ∈G; x∈B (mj ,2∆)
hold for all η > 0 sufficiently small.
Proof. The claim is trivial if, for the initial condition, we have x ∈ B(mi, 2∆). Next, let us observe
the following facts.
•	Define (recall the definitions of measure μ% and sets Ei, Eij in eq. (G.1)eq.(G.5)eq. (G.6))
J(j) = arg min |i -ej| ∀j 6= i
j：*i(Ej,e)>0
P* = min	μj(EjJj))
P	j∙.j=m,mj∈G μj (Ej).
•	From the definition of J(j) and the fact that there are only finitely many attraction fields we
can see that P* > 0. Moreover, G being a communication class implies that
|J(j) - i| < |j - i| ∀j 6= i, mj ∈ G.
Indeed, if i < j , then since G is a communication class and there are some mi ∈ G with
i < j, We will at least have μj∙(Ejj-I) > 0, so | J(j) - i| ≤ |i - j | - 1; the case that i > j
can be approached analogously.
•	Now from the definition of J(j) and Proposition G.25, together with the previous bullet
point, we know that for all η sufficiently small,
inf	Px(|Ik+1	- i|	≤	|Ik	- i| - 1, mIk+1	∈ G |	Ki(η, ∆) > k,	mIk	∈	G)	≥ P*/2
uniformly for all k ≥ 0.
•	Meanwhile, since P* > 0, we are able to fix some δ > 0 small enough such that
nmin δ
-----；--- < C.
(P* /2)nmin
•	On the other hand, for any ej with mej ∈/ G, by definition of the typical transition graph we
must have μj∙ (Ej e)=0 for any j with mj ∈ G. Then due to Proposition G.25 again, one
can see that for all η > 0 that is sufficiently small,
sup Px(mIk+1 ∈/ G | Ki(η, ∆) > k, mIk ∈ G) < δ
x∈[-L,L]
uniformly for all k ≥ 0.
•	Repeat this argument for nmin times, and we can see that for all η sufficiently small
P* nmin
inf Px(Ki(η, ∆) ≤ k + nmin | Ki(η, △) > k, m/七 ∈ G) ≥ (k)
x∈[-L,L]	2
sup	Px (∃l	∈	[nmin]	s.t. mIk+l	∈/	G |	Ki (η, △) > k,	mIk	∈	G)	≤	nminδ
x∈[-L,L]
uniformly for all k ≥ 1.
74
Published as a conference paper at ICLR 2022
•	Lastly, to apply the bounds established above, We will make use of the following expression
of several probabilities. For any j with Imj ∈ G and x ∈ B(m7∙, 2∆) and any U = 1,2, ∙∙∙,
Px(∃l ∈ IUnmin] s∙t∙ mIι ∈ G, Ki (η, △) > U , nmin)
u — 1
=X : Px (∃k ∈ [nmin] s∙t∙ mIk十Vnmin ∈ G J Ki(η, △) > V ∙ nmin, mIι ∈ G ∀l ≤ Vnmin)
v=0
v — 1
' Y Px (Ki(n，△) > (w + 1)nmin, mIk十Wnmin ∈ G ∀k ∈ [nmin] J
w = 0
Ki(η, △) > w ∙ nmin, mIι ∈ G Vl ≤ Wnmin)
Px(mIι ∈ G Vl ∈ [unmin], Ki (η, △) > u , nmin)
u — 1
:^ɪ P(Ki(n，△) > (V + 1)nmin, mIk十Vnmin S G Vk ∈ [nmin] J
v=0
Ki(η, △) > vnmin, m/k ∈ G Vk ∈ [vnmin]
In summary, now we can see that (for sufficiently small η)
sup	Px (∃k ∈ [Ki(η, △)] s.t. m/k(*∆) ∈ G)
j: mj ∈G; x∈B(mj,2∆)	`	，
∞
≤ E	sup	Px (∃v ∈ [nmin] SUCh that mIv+unmin ∈ G,
u=0 j： mj ∈G; x∈B(mj ,2∆)	`
Ki (η, △) > unmin, mIk ∈ G Vk ∈ [unmin]
∞
≤ E	SUP	Px (∃v ∈ [nmin] such that mIv+unmln ∈ G I
u=0 j： mj ∈G; x∈B(mj ,2∆)	、	1
Ki (η, △) > unmin, mIk ∈ G Vk ∈ [unmin]
u—1
• ∏	sup	Px(Ki(η, △) > (v + 1)nmin,馆几….∈ G Vk ∈ [nmin]
v = 0 j: mj ∈g; x∈B(mj ,力)	∖	™n
Ki(η, △) > vnmin, mIk ∈ G Vk ∈ [vnminf)
≤ E nmin δ(1-(号下)j =	≤ C
2^	2	27	(p*∕2)nmιn
u≥0	p / j
and
sup
j:mj ∈G, x∈B(mj ,2∆)
≤	sup
j:mj ∈G, x∈B(mj ,2∆)
+	sup
j:mj ∈G, x∈B(mj ,2∆)
Px(Ki(η, △) > U • nmin)
Px(∃l ∈ [Unmin] s∙t∙ mIι ∈ G, Ki(η, △) > U ∙ nmin)
Px(mIi ∈ G Vl ∈ [unmin], Ki(η, △) > U ∙ nmin)
u
≤e + (1 - (p2-)nm
__	*
uniformly for all U = 1,2, ∙ ∙ ∙ ∙ To conclude the proof, it suffices to set P =(勺)nmin∙
□
75
Published as a conference paper at ICLR 2022
The proof above can be easily adapted to the case when the communication class G is transient.
Define
KiG(η, ∆) =∆ min{k ≥ 0 : Ik(η, ∆) = i or mIk(η,∆) ∈/ G}.
Lemma H.10. Assume that G is a transient communication class on the graph G. Then there exists
some constant P > 0 such thatfor any i with m% ∈ G and any ∆ ∈ (0, e/3),
sup	Px(KG(η, ∆) > u ∙ nmin) ≤ P(GeOm(P) ≥ U) ∀u =1, 2,…	(H.43)
j : mj ∈G; x∈B(mj ,2∆)
hold for all η > 0 sufficiently small.
Proof. The structure of this proof is analogous to that of Lemma H.9. Again, the claim is trivial if,
for the initial condition, we have x ∈ B(mi, 2∆). Next, let us observe the following facts.
• Define (recall the definitions of measure μ% and sets Ei, Eij in eq. (G.1)eq.(G.5)eq. (G.6))
J(j) =∆ arg min	|i -ej| ∀j 6= i
j：“i(Ej,e)>0
* ∆
P =
μj (EjJ Cj))
min --------:二：
j: j=i, mj ∈G μj (Ej)
• From the definition of J(j) and the fact that there are only finitely many attraction fields we
can see that P* > 0. Moreover, G being a communication class implies that
|J(j) - i| < |j - i| ∀j 6= i, mj ∈ G.
Indeed, if i < j, then since G is a communication class and there are some mi ∈ G with
i < j, We will at least have μj (Ejj-ι) > 0, so | J(j) - i| ≤ |i - j | - 1; the case that i > j
can be approached analogously.
•	Now from the definition of J(j) and Proposition G.25, together with the previous bullet
point, we know that for all η sufficiently small,
inf	Px(|Ik+1	- i| ≤	|Ik	- i|	-	1,	mIk+1	∈ G | KiG(η,	∆) > k ≥ P*/2
uniformly for all k ≥ 0.
•	Repeat this argument for nmin times, and we can see that for all η sufficiently small
x∈[i-nLf,L] Px(KiG(η, ∆) ≤ k + nmin | KiG(η, ∆) > k) ≥
nmin
uniformly for all k ≥ 1.
•	Lastly, for any j = i with mj ∈ G and X ∈ B(mj, 2∆) and any U = 1,2, •…，
Px(KG (η, ∆) > U ∙ nmin)
u-1
=YPx (K尸 (η, δ) > (V + 1)nmin j Ki (η, δ) > V ∙ nmin)
v=0
u-1
=Y 1 - Px (Ki (η, ∆) ≤ (V + 1)nmin j Ki (η, ∆) > V ∙ nmin)
v=0
In summary, now we can see that (for suffciently small η)
sup	Px(KG(η, ∆) ≥ u ∙ nmin) ≤(1 - (p-)nmin)
j :mj ∈G, x∈BCmj ,2∆)	2
__	*	_
uniformly for all U = 1,2,….To conclude the proof, it suffices to set P = (P-)nmin.	□
76
Published as a conference paper at ICLR 2022
We are now ready to prove Lemma H.1, which, as demonstrated earlier, is the key tool in proof of
Theorem 2.
Proof of Lemma H.1. The claim is trivial if llarge = 1, so we focus on the case where llarge ≥ 2. Fix
some
1	1	γe	γe
γ ∈ (0,(1- -) ∧ (2)), β ∈ (1, (2 - 2e) ∧ (α - 0e)), Y ∈ (0,16M^ ∧ 4).
Let q* = maxj μj(Ej (0)). We show that for any t ∈ (0, 春)the claim is true.
Now we only consider ∆ ∈ (0, e∕3) and η small enough so that ηM ≤ ηγ and ηγ < ∆. Consider
the following stopping times
TeYscape =∆ min{n ≥ 0 : Xnη ∈∕ ∪jB(sj, 2ηY)};
TrYeturn =∆ min{n ≥ 0 : Xnη ∈ ∪jB(mj, 2ηY)}.
First, from Lemma H.8, we know that
SUP Px(TesCape > 1/HQ/哈)< δ∕2
x∈[-L,L]
for all η sufficiently small. Besides, by combining Lemma H.7 with Markov property (applied at
Tescape), we have
SUP Px ( TrYtUm- TesCape > * lθg(1∕η)∕η I TesCape ≤ 7F7TT5) < δ∕2
x∈[-L,L]	'	H	H (1∕η"
for all η suffiCiently small. Therefore, for all η suffiCiently small,
SUP P(TYtUrn > —1— + 2c^ IogM) < δ.
x∈[-L,L]	H(1∕η)	η
(H.44)
Let J be the unique index such that XTY ∈ Ωj. Our next goal is to show that, almost always, the
Treturn
SGD iterates will visit the local minimum at some large attraction fields. Therefore, without loss of
generality, we can assume that mJ ∈/ M large, and define
Tlγarge =∆ min{n ≥ Trγeturn: Xnη ∈	[ B(mi,2∆)}
i:mi ∈M large
and introduce the following definitions:
τ0 =∆ Trγeturn , J0 =∆ J
τk =∆ min{n > τk-1 : Xnη ∈	[ B(mj, 2∆)}
j6=Jk-1
Jk = j ⇔ XTk ∈ Ωj ∀k ≥ 1
K =∆ min{k ≥ 0 : mJk ∈ M large}.
In other words, the sequenCe of stopping times (τk)k≥1 is the time that, starting from TrYeturn, the SGD
iterates visited a loCal minimum that is different from the one visited at τk-1, and (Jk)k≥0 reCords
the label of the visited loCal minima. The random variable K is the number of transitions required
to visit a minimizer in a large attraCtion field. From Lemma H.9, we know the existenCe of some
p* > 0 such that (for all η sufficiently small)
sup Px(K ≥ U ∙ nmin) ≤ P(Geom(p*) ≥ U) + g ∀u = 1, 2, 3,….
where Geom(a) is a Geometric random variable with success rate a ∈ (0, 1). Therefore, one can find
integer N(δ) such that (for all sufficiently small η)
sup P(K ≥ N(δ)) ≤ δ.
x∈[-L,L]
(H.45)
77
Published as a conference paper at ICLR 2022
Next, given results in Proposition G.25 and the fact that there are only finitely many attraction fields,
one can find a real number u(δ) such that (for all sufficiently small η)
sup Px(Tk - Tk-I ≤	u(δ)	) ≤ δ∕N(δ)	(H.46)
x∈[-L,L]	λJk-1 (η
uniformly for all k = 1, 2,…，N(δ). From eq. (H.44), eq. (H.45),eq. (H.46), We now have
TJx(Xn j[M g Bm, 3∀n ≤ iH^+H⅛y+2c2 Y T)
≤ 3δ
(H.47)
for any sufficiently small η. To conclude the proof we just observe the following facts. First, due to
H/ RV-α and llarge ≥ 2, we have
λlarge(η)	log(1∕η)
Iim H(1∕η"η = 0, lim HH lim「λl簪㈤=0.
Therefore, for sufficiently small η, we will have (note that , δ are fixed constants in this proof, so
N(δ, u(δ are also fixed)
N(6u(6 HII2# + H(1∕lη) + 2c2γ
bt∕λlarge(η)c
Second, recall that We fixed some t / (0,今)where q*
small enough so that
iog(i∕η)
≤ e.
(H.48)
maxj μj(Ej). Also, choose some C > 0
C<δ∕2, 2(1 + Cy < 4.
From Proposition G.23 and the fact that there are only finitely many attraction fields, there exists
some no > 0 such that for any η ∈ (0, no) and any ∆ > 0 sufficiently small,
sup
sup
i:mi ∈M large x∈[mi-2∆,mi+2∆]
≤ sup
sup
i:mi ∈M large x∈[mi-2∆,mi+2∆]
≤C + 2(1 + C)2q*t ≤ 2δ.
Px(σi㈤ ≤ >⅛y)
Px(μi(Ei)λlarge(η)σi(η) ≤ q*t
Combine this bound with Markov property (applied at TK), and we obtain that
SUp Px(∃n ∈ [bt∕λlarge(η)C] s.t. X^k / U	Ωi) ≤ 2δ
x∈[-L,L]	i:mi ∈M large
for all η sufficiently small. Together with eq. (H.47)eq. (H.48), we have shown that
sup Px (V small(η, , t) >	≤ 5δ
x∈[-L,L]
holds for all η sufficiently small.
□
H.2 Proof of Lemma H.4, H.5
We shall return to the discussion about the dynamics of SGD iterates on a communication class G.
Recall that
Glarge = {ml1arge
miarge}, GSmaII = {m1malL …，mimall}.
If Xnη is initialized at some sharp minimum on G, then we are interested in the behavior of Xnη at the
first visit to some large attraction fields on G. Define
TG(η, ∆) = min{n ≥ 0 : Xn / U	B (mi, 2∆) or Xn / 5： mi∈GΩi}.	(H.49)
i: mi∈Glarge
Not only is this definition of TG analogous to the one for TGDTMC in eq. (H.16), but, as illustrated in
the next lemma, TG also behaves similarly as TG on a communication class G in the following sense:
the probabilities pi,j defined in eq. (H.17) govern the dynamics regarding which large attraction field
on G is the first one to be visited. Besides, TG is usually rather small, meaning that the SGD iterates
would efficiently arrive at a large attraction field on G or simply escape from G.
78
Published as a conference paper at ICLR 2022
Lemma H.11. Given any θ ∈ (0, (α - 1)/2), ∈ (0, 1), i,j ∈ [nmin] such that mi ∈ Gsmall, mj ∈/
Glarge, the following claims hold for all ∆ > 0 that is sufficiently small:
limsup sup Pχ 仇⑺ ∆) ≤	η ,	XTG	∈ B(mj,	2∆))	≤ Pi,j	+ 5e,
ηψ0 x∈B(mi,2∆)	λG(η) G
lim0nfx∈Bimf,2∆)Px(TG(n，A ≤ 焉,XTG ∈ B(mj，2M ≥Pij-5e,
limsup sup	Px(TG(n, ∆) > η ) ≤ 2e.
η'0 x∈B(mi,2∆)	×	λG(ηy
Proof. For GsmaU = 0 to hold (and the discussion to be meaningful), We must have IG ≥ 2.
Throughout the proof, We assume this is the case. Besides, We require that ∆ ∈ (0, e/3) so We have
B (mi, 3∆) ∩ ΩC = 0 ∀i ∈ [nmi∩]
and the 3∆-neighborhood of each local minimum Will not intersect With each other. In this proof We
Will only consider ∆ in this range.
From Lemma H.9 (if G is absorbing) or Lemma H.10 (if G is transient), We knoW the existence of
some integer N() such that for (see the definition of Ik in eq. (H.39)-eq. (H.42))
NG (η, ∆) =∆ min{k ≥ 0 : mIk (η,∆) ∈ Glarge or mIk (η,∆) ∈/ G},
We have
sup	Px (NG (η, ∆) > N ()) <
x∈B (mi,2∆)
for all η sufficiently small. Fix such N (). Next, from Proposition G.25, We can find u() ∈ (0, ∞)
and ∆ ∈ (0, e/3) such that for all ∆ ∈ (0, ∆), we have
SUP	Px(Tk(η, ∆) - Tk-i(η, ∆) > u(e"Λ(lk-i(η, ∆),η)) ≤ e∕N(e) ∀k ∈ [N(e)]
x∈B (mi,2∆)
for all η sufficiently small. Fix such U(E) and ∆. Now note that on the event
A = {ng ≤ n(e)} ∩ {Tk(η,∆) -Tk-ι(η,∆) ≤ U(E)NIk-ι(η,∆),η) ∀k ∈ [N(e)]},
due to the choice of θ ∈ (0, (α - 1)∕2) and H ∈ RV-α, we have (when η ∈ (0, 1))
η2θ
Tk(η,∆) - Tk-ι(η, ∆) ≤ -η-- ∀k < NG(η, ∆)
λG (η)
η2θ
⇒TG(η, δ> TNG(η,∆)(η, ∆) ≤ N (E)U(E) -ʌʒ.
λG (η )
For any η sufficiently small, we will have N(E)U(E)入?；；) < 人?@) ∙ In summary, we have established
that for all ∆ ∈ (0, ∆),
lim sup sup	Px TG >
ηψ0 x∈B(mi ,2∆)	'
≤ lim sup sup	Px(Ac) < 2.
ηψ0 x∈B(mi ,2∆)
(H.50)
Next, let
S(E) = {(m1,…，mN(e)) ∈ {m1,…，mnrain}N9 : ∃k ∈ [N(e)] s∙t∙ mk = mj}.
We can see that S(E) contains all the possible transition path for Y DTMC where the state mj is
visited within the first N(E) steps. Obviously, |S(e)| ‹ ∞.^Let ei = e∕∣S(e)∣. If we are able to
show the existence of some ∆ι > 0 such that for all ∆ ∈ (0, ∆ι), the following claim holds for any
(m0k)kN=(1) ∈ S(E):
lim sup sup	Px (mIk	= m0k ∀k	∈ [N (E)])	- P(YkDTMC (mi)	=	m0k	∀k ∈	[N (E)])	< E1,
ηψ0 x∈B(mi,2∆)
(H.51)
79
Published as a conference paper at ICLR 2022
then We must have (for all ∆ ∈ (0, ∆ ∧ ∆ 1))
lim suP suP	Px XTηG ∈ B(mj, 2∆) - pi,j
ηψ0 x∈B(mi,2∆) '	'	)
lim sup sup	Px	XTηG	∈ B(mj, 2∆),	TG	≤	N()	+ Px	XTηG	∈ B(mj, 2∆),	TG	> N()
ηψ0 x∈B(mi,2∆) I '	)	'
- PYTDDTTMMCC (mi) = mj, TGDTMC ≤ N() -PYTDDTTMMCC(mi)=mj, TGDTMC >N()
≤ lim sup sup	Px XTη ∈ B(mj, 2∆), TG ≤ N()
ηψ0 x∈B(mi,2∆) '	'
-PYTDDTTMMCC(mi)=mj, TGDTMC≤N()
+ lim sup sup	Px(TG > N()) + P(TGDTMC(mi) > N ())
ηψ0 x∈B(mi,2∆)
≤∣S(e)∣eι +limsup	SuP	Px(TG > N(e)) + P(Tdtmc(mi) > N(E))
ηψ0 x∈B(mi,2∆)
≤3.
To shoW that eq. (H.51) is true, We fix some (m0k)kN=(1) ∈ S(E) and let (k0(k))kN=(1) be the sequence
With mk0(k) = m0k for each k ∈ [N (E)]. From the definition of YDTMC We have (let k0(0) = i)
N ()-1
P YkDTMC(mi)=m0k∀k∈ [N(E)]	= Y
k=0
μk0(k)(Ek0(k),k0(k+1))
μk0(k)(Ek0(k))
On the other hand, using Proposition G.25, We knoW that for any arbitrarily chosen E0 ∈ (0, 1), We
have
N ()-1
lim suP suP	Px mIk (η,∆) = m0k ∀k ∈ [N (E)] ≤
ηψ0 x∈B(mi,2∆)	、	/	k=0
N()-1
limionfx∈Bimf 2∆) Px {mikg∆)=mi ∀k ∈ Ng]) ≥ γ ■
,∕4∙U x∈Q (Imi ,24)
k=0
μk0(k)(Ek0(k),k0(k+1)) .( J)
μko (k)(Ek0(k))	，
μk0(k)(Ek0(k),k0(k+1)) ∙ (1 - C
μko (k)(Ek0(k))	,
for all ∆ > 0 sufficiently small. The arbitrariness of E0 > 0, together With ∣S(E)∣ < ∞, alloWs
US to see the existence of some ∆ 1 > 0 such that with ∆ ∈ (0, ∆ι), eq. (H.51) holds for any
(m0k)kN=(1) ∈ S(E). To conclude the proof, observe that
liizpx∈BSmp,2∆J Px(XTG ∈	，H- Px (TG(n，δ) ≤ λ⅛，XTG ∈ B(mj，2δ
≤ lim suP suP Px TG >
ηψ0 x∈B(mi,2∆)	'
due to eq. (H.50).
<E
□
Recall that continuous-time process X*,η is the scaled version of Xη defined in eq. (H.13), and
the mapping T* (n, η) = nλg(η) returns the timestamp t for XtE corresponding to the unscaled
step n on the time horizon of Xn. As an inverse mapping of Tt, we define the mapping Nt (t, η)=
[t∕λG(η)C that maps the scaled timestamp t back to the step number n for the unscaled process Xη.
In the next lemma, we show that, provided that Xt,η stays on a communication class G before some
time t, the scaled process Xtt,η is almost always in the largest attraction fields of a communication
class G.
Lemma H.12. Let G be a communication class on the graph G. Given any E1 > 0, t > 0 and any
X ∈ Ωi with mi ∈ G,thefollowing claim holdsfor all ∆ > 0 small enough:
limsup Px({Xt,η ∈ U	B(mj, 3∆)} ∩ {x；w ∈ U Ωk ∀s ∈ [0,t]}) ≤ 2q.
η10	j:mj ∈GIarge	k: mk∈G
80
Published as a conference paper at ICLR 2022
Proof. Let ∆ ∈ (0, e/3) for the constant e in eq. (G.28)eq. (G.29), so We are certain that each
B (mi, 2∆) lies entirely in Ωi and would not intersect with each other since
B(mi, 3∆) ∩ ΩC = 0 ∀i ∈ [nmi∩].
Besides, with E = ∆∕3, we know the existence of some δ > 0 such that claims in Lemma G.14
would hold of the chosen , δ. Fix such δ for the entirety of this proof. Lastly, fix some
1	1	γe	γe
Y ∈ (0, (1 - -) ∧ (5)), β ∈ 3 (2 - 2γ) ∧ (α - αγ)), Y ∈ (0, -ɪ ∧ Y).
α 2	16M c2	4
The blueprint of this proof is as follows. We will define a sequence of stopping times (Nj)j6=1
such that the corresponding scaled timestamps T* = T*(N7- ,η) gradually approach t. By analyzing
the behavior of X*,η on a time interval [t 一 ∆t,t] that is very close to t (in particular, on the
aforementioned stopping times T*), we are able to establish the properties of a series of events
Ai ⊇ A2 ⊇ A3. Moreover, we will show that A3 ⊆ {X；,nA ∈ Ui: m. ∈Gg B (mi, 3∆)}, so the
properties about events A, A2 , A3 can be used to bound the probability of the target event.
Arbitrarily choose some ∆t ∈ (0, t). To proceed, let No = N*(t 一 ∆t,η) be the stopping time
corresponding to timestamp t - ∆t for the scaled process. Using Lemma H.8, we know that for
stopping time N1 =∆ min{n ≥ N0 : Xnη ∈/ ∪jB(sj, 2ηγ)}, we have
ηψo x∈[—L,L]Px(N1-N0<
WTny) = L
Next, let N2 =∆ min{n ≥ N1 : Xnη ∈ ∪jB(mj, 2∆)}. From Lemma H.7 and H ∈ RV-α (so that
log(1∕η)∕η = o(H(1∕η)), We have
,η10 x∈[—L,L]Px(N2-N1<
WTny) = L
Collecting results above, we have
ηψo x∈[—L,L] Px(N2 -N0 <
WTny)=-.
(H.52)
Now note the following fact on the event {N 一 No < 冼/； }. The definition of the mapping
T* implies that, for any pair of positive integers ni ≤ n2, we have T*(n2, η) 一 T*(nι,η) =
(n2 — nι)λG(η) ≤ (n2 - ni) ∙ H(1∕η). Therefore, on {N - No ≤ H(^} we have
T*(N2,η) - T*(No, η) < ∆t/2 ⇒ T*(N2, η) <t —寸.
Besides, let T2* = T*(N2, η). Now we can see that for event
Ao = {X;，n ∈ U	Ωk ∀s ∈ [0,t]} ∩ {N2 - No <
k: mk ∈G
∆t∕2
H(-∕η)卜
we have
Ao ⊆ Ai = {T2 <t- ∆ O ∩{x;,n ∈ U	Ωk ∀s ∈ [0,T*]}.
k: mk∈G
Meanwhile, from eq. (H.52) we obtain that
limsup sup Pχ(A： ∩{X;,η ∈ U Ωk ∀s ∈ [0,t]}) = 0.	(H.53)
η'0 x∈[-L,L]	k: mk∈G
Moving on, we consider the following stopping times
N3 = min{n ≥ N : Xn ∈ U	B(mj, 2∆) or Xn ∈ U Ωj},
j:	mj ∈Glarge	j: mj ∈G
T3* =∆ T*(N3, η).
81
Published as a conference paper at ICLR 2022
Using Lemma H.11, We have
limsup Pχ(N3 — N^2 >:(“)∣ Ai) ≤ ∈1 ∙	(H.54)
Meanwhile, on event Ai ∩ {% — N2 ≤ 就} ∩{x3,η ∈ Uki∈GΩk ∀s ∈ [0, t]}, we have
T3 — T3 ≤ ∆t∕4, hence T3 ∈ [t — ∆t,t — ∆t∕4]. In summary,
Ai ∩ {N3-n2 ≤ λ 7)} ∩ {χ3,η ∈ u ω ∀s ∈ [0, t]}
Gm	k: mk ∈G
⊆ {t3 ∈ [t — ∆t,t — ∆t∕4]} ∩ {x3,η ∈ U Ωk ∀s ∈ [0,T3]}
k:	m⅛∈G
Moreover, on event {xjη ∈ Uk∙ mk∈g Ω ∀s ∈ [0, t] },if we let J3 be label of the local minimum
visited at ɪɜ such that J3 = j ^⇒ XTF ∈ B(m7∙, 2∆), then we must have mj3 ∈ Glarge.
Meanwhile, consider the following stopping times
Tσ = min{s > T3 : X；，n ∈ Ωj3}.
From Proposition G.25, we know that
limjupPχ(τσ — T3 ≤ ∆t ∣ {T3 ∈ [t — ∆t,t — ∆J4]} ∩ {x；，n ∈
≤∈ι + 1 — exp ( — (1 + fi)qFt)
where q3 = maxj μj (Ej). Now we define the event
A2 = Ai ∩ {N3 — N2 ≤ ∆∕4)} ∩ {Tσ — T3 > ∆t} ∩ {x3,η ∈
U Ωk ∀s ∈ [0, T3]})
k: mk ∈G
(H.55)
U	Ωk ∀S ∈ [0,T3]}.
k: mk∈G
Using eq. (H.53)-eq. (H.55), we get
limsup sup Px(Ac ∩{X3,η ∈ U	Ωk ∀s ∈ [0, t]}) ≤ 2∈ι + 1 — exp ( — (1 + €ι)q*∆t).
η即 x∈[-L,L]	∖	k: mk∈G	7
(H.56)
Furthermore, on event A2, due to T3 ∈ [t — ∆t, t — ∆t∕4] as established above, we must have
X3,η ∈ Ωj3∀s ∈ [T3,t].
Now let US focus on a timestamp T] = t — Hm1G(n) and N4 = N*(T[, η) . Obviously, T] > T3
on event A2 . Next, define
N5 = min{n ≥ N4 : Xn ∈ U B(mj, 2∆)}
j
τ5 = t3(n5 ,η).
Using Lemma H.7 and H.8 again as we did above when obtaining eq. (H.52), we can show that
limsupPx(N5 — N4 >，/t/16) = 0.	(H.57)
ηψo '	H (1∕η),
On the other hand, on event A2 ∩ ∣N5 — N4 ≤ Hj焉} we must have
4 rp*	rp* / ∆t∕16 ʌ / X CC rp* _ rɪ	∆t∕8 ∖ / X ɪ	∆t∕16 ∖ /
•	T5 一 T4 ≤ H(1∕η) λG(η), so T5 ∈ [t — H(i∕η)入 G (n),t — H(：/# λG(η)];
• X3η ∈ Ωj3,duetoTσ — T3 > ∆t.
82
Published as a conference paper at ICLR 2022
This implies that for event
N = {t5 ∈ [t-H∆l∕8)λG(η),t-H∆τu6)λG(η)], xT5η ∈ Ωj3}∩{xrη ∈ U ω, ∀s ∈ [0,τ5]},
η	η	k: mk∈G
We have A2 ∩ ∣N5 - N ≤ 就/1：)} ⊆ A. Lastly, observe that
•	From Lemma G.4, we know that for %(6) = min{n > N5 : η∣Zn∣ > δ} we have
limsupP(Nβ(δ) - N ≤ ∆t∕H(1∕η)) ≤ ∆t∕δα;
ηψo
•	As stated at the beginning of the proof, our choice of δ allows us to apply Lemma G.14 and
show that
limsup sup Pχ(∃n = N5,…，N — 1 s.t. Xn ∈ B(mj3,3∆) | A) = 0;
nΨ0 x∈[—L,L]
•	Combining the two bullet points above, we get
limsupPχ(∃s ∈ [T5,t] such that XsE / B(mj3,3∆) I A) ≤ ∆t∕δα.	(H.58)
nψo	×	1 j
On the other hand,
Ae ∩ {Xss,η ∈B(mJ3,3∆)∀s∈ [T5s,t]} ⊆ {Xts,η ∈ U	B(mk, 3∆)}.
k: mk ∈Glarge
In summary, for event
A3 = A2 ∩ {N5 - N4 ≤ H1/6)O ∩ {xs，n / B(mj3,3∆) ∀s / [T5,t]},
we have A3 ⊆ {Xts,η ∈ Sk: m ∈Glarge B(mk, 3∆)}. Besides, due to eq. (H.56)eq. (H.57)eq. (H.58),
we get
limsup sup Px(Ac ∩{Xs,n /	U Ωk ∀s / [0,t]})
n10 x∈[-L,L]	、	k： mk∈G	7
≤2∈ι + 1 - eχp ( - (I + Ei)q*4t) + δɑ.
Remember that δ, 1 , qs are fixed constants while ∆t can be made arbitrarily small, so by driving ∆t
to 0 we can conclude the proof.	□
Recall the definition of jump processes in Definition H.1. Central to the proof of Lemma H.4, the
next result provides a set of sufficient conditions for the convergence of a sequence of such jump
processes in the sense of finite dimensional distributions.
Lemma H.13. For a sequence of processes (Yn)n≥1 that, for each n ≥ 1, Yn is a
(Ujn)j≥0, (Vjn)j≥0 jump process, and a (Uj)j≥0, (Vj)j≥0 jump process Y, if
•	U0 ≡ 0;
•	(Un Vn, Un, Vn, Un, Vn, ∙ ∙ ∙) converges in distribution to (0, V0, Ui, Vι, U2, V2, ∙∙∙) as
n → ∞;
•	For any x > 0 and any n ≥ 1,
P(U1 + ∙∙∙ + Un = x) = 0;
•	For any x > 0,
lim P(Ui + U + …Un >x) = 1,
n→∞
83
Published as a conference paper at ICLR 2022
then the finite dimensional distribution of Y n converges to that of Y in the following sense: for any
k ∈ N and any 0 < tι ‹ t2 < •…< tk < ∞ ,the random element (Yn, •…,Ytn) converges in
distribution to (Ytι, ∙∙∙ , Ytk) as n → ∞.
Proof. Fix some k ∈ N and 0 < tι < t2 < •…< tk < ∞. For notational simplicity, let t = tk.
Let (D, d) be the metric space where D = D[0,t], the space of all cħdlħg functions in R on the time
interval [0, t], and d is the Skorokhod metric defined as
d(Zι,G)= λnΛ kζι- ζ2 ◦ λk∨kλ -Ik
where Λ is the set of all nondecreasing homeomorphism from [0, t] onto itself, and I(s) = s is the
identity mapping. Also, we arbitrarily choose some ∈ (0, 1) and some open set A ⊆ Rk.
From the assumption, we can find integer J() such that P PjJ=(1) Uj ≤ t < , as well as an integer
N () such that, for all n ≥ N (), we have P PjJ=(1) Ujn ≤ t + P U0n ≥ t1 < . We fix such
J (), N() (we may abuse the notation slightly and simply write J, N when there is no ambiguity).
Using Skorokhod's representation theorem, We can construct a probability space (Ω, F, Q) that
supports random variables (Un, V0n,…,Un, Vnn)η≥ι and (Uo, Vo, ∙∙∙ , UJ, Vj) and satisfies the
following conditions:
•	L(Un, Vn, ∙∙∙ ,Un,Vn) = L(Un, V0n,…，UJ, Vn) for all n ≥ 1;
•	L(Uo, V0,…，Uj,VJ) = L(Uo, V0,…，Uj, VJ);
•	Ujn -a-.s→. Uj and Vjn -a-.s→. Vj asn → ∞ for all j ∈ [J].
Therefore, on (Ω, F, Q) we can define the following random elements (taking values in the space of
cadlag functions):
(
Vej0n	ifs<Ue0n,
∑j=o Vnl[Un+un+…+un, un+un+…+u+i)(S) otherwise ,
j
YJ = X Vj 1[U1+…+Uj, Uι+…+Uj+1)(S) ∀s ≥ 0.
j=0
Note that (1) for the first jump time of YJ we have Uo ≡ 0, hence YJ = V0; (2) when defining
Yn,J we set its value on [0, Un) to be Vn instead of 0.
Since Ujn -a-.s→. Uj and Vjn -a-.s→. Vj as n → ∞ for all j ∈ [J], we must have
lim d(Yn，J ,Yj ) = 0
n
almost surely, which further implies that YnJJ ⇒ YJ
as n → ∞ on (D, d). Now from our
assumption that, for thejump times Ui +-+ Uj, we have P(U1 +-+ Uj = x) = 0 ∀χ > 0, j ≥ 1,
as well as (13.3) in Billingsley (2013), we then obtain
(YnJJ,…，Yn ,j ) ⇒ (YJ,…，Yj)	(H.59)
as n → ∞. Recall that A is the open set we arbitrarily chose at the beginning of the proof, and > 0
is also chosen arbitrarily. Now we observe the following facts.
•	Using eq. (H.59), we can see that
linninfQ((Yn,j,…，Yn,j) ∈ A) ≥ Q(YJ,…，YJ) ∈ A)∙
84
Published as a conference paper at ICLR 2022
•	The choice of N () and J () above implies that
J()
I	Q((YT㈤,…，Y⅛J9) ∈ A) - P((YtI,…，Ytk) ∈ A)∣ ≤ P( X Uj ≤ t) < e,
j=1
IQ((YnJJQ …,Y, ,J©) ∈ A)—P((Yn ,…，Ytk)∈ ⑷ ∣
J()
≤	P XUjn ≤t +P U0n ≥t1 < ∀n≥N().
j=1
Collecting the two results above, we have established that
limninf P((Yn，…，Y,) ∈ A) ≥ P((YtI，…，匕上)∈ A) - 2e.
From Portmanteau theorem, together with arbitrariness of > 0 and open set A, we can now conclude
that (Yn,…，Yn) converges in distribution to (Ytι, ∙∙∙ ,Ytk).	□
The following lemma concerns the scaled version of the marker process X*,%δ defined in eq. (H.13)-
eq. (H.14). Obviously, it is a jump process that complies with Definition H.1. When there
is no ambiguity about the sequences (ηn)n≥ι and (∆n)n≥ι, let Xtn) = XtEnAn. From
eq. (H.6)-eq. (H.11) and eq. (H.15), We know that for any n ≥ 1, X(n) is a ((τt(ηn,∆n)-
Tk-ι(ηn, ∆n))k≥0, (mιk(ηn,∆n))k≥0) -jump process (with the convention that τ-1 = 0). Also, for
clarity of the exposition, we let (for all n ≥ 1, k ≥ 0)
Sek(n) = σkk(ηn,∆n) - τkk-1(ηn,∆n),
Sk(n) = τkk (ηn, ∆n) - τkk-1(ηn, ∆n),
Wfk(n) = mIekG(ηn,∆n),
Wk(n) = mIkG(ηn,∆n) .
Lastly, remember that Y is the continuous-time Markov chain defined in eq. (H.18)-eq. (H.21) and
∏g(∙) is the random mapping defined in eq. (H.22) that is used to initialize Y. Besides, Y is a
(Sk )k≥0, (Wk )k≥0 jump process under Definition H.1, with S0 = 0 and W0 = πG(mi) (here
X ∈ Ωi and Xn = x, so i is the index of the attraction field where the SGD iterate is initialized). The
following result states that, given a sequence of learning rates (ηn)n≥1 that tend to 0, we are able to
find a sequence of (∆n)n≥ι to parametrize X(n) = Xk,nn,An, X(n) = X*,nn,An so that they have
several useful properties, one of which is that the jump times and locations of X(n) converges in
distribuiton to those of Y(πG (mi)).
Lemma H.14. Assume the communication class G is absorbing. Given any m^ ∈ G, X ∈ Ω%, finitely
many real numbers (tι)k= 1 such that 0 < tι < t2 < •…< tko, and a sequence of strictly positive
real numbers (ηn)n≥1 with limn→0 ηn = 0, there exists a sequence of strictly positive real numbers
(∆n)n≥1 with limn ∆n = 0 such that
•	Under Px (so X0η = X), as n tends to ∞,
(S0n), W0n), S(n), W(n), S2n), W2n),…)⇒ (So, Wo, Si, Wi,S2, W2,…)(H.60)
•	(Recall the definition of Tk , Ik in eq. (H.39)-eq. (H.42)) Given any > 0, the following
claim holds for all n sufficiently large:
sup Pχ(∃j	∈	[Tk (ηn, ∆n), Tk (ηn, ∆n)]	St	Xj	∈ U	Ωι	|	mlk %△"	∈	G)< 前
k≥o	l: ml∈G
(H.61)
85
Published as a conference paper at ICLR 2022
•	Given any e > 0, thefollowing claim holdsfor all n sufficiently large,
suP Pk (mIk5n ,∆n)+v ∈ Glarge ∀v ∈ [un min ] I mIk(ηn,∆n) ∈ G)
k≥0	'	1	)
≤P(Geom(p*) ≥ u) + E Vu = 1, 2,…；	(H.62)
•	For any l ∈ [k0],
nlimo Px(XLn ∈	U	B (mj，∆n), X；"n ∈ U % VS ∈ [0,5]) =0
j: mj ∈GIarge	j： mj ∈G
(H.63)
where p* > 0 is a constant that does not vary with our choices of ηn or ∆n.
Proof. Let
Vj = qj = μj (Ej)
νj,k = μj (Ej,k)
so from the definition of ”k We have ”k = l{j = k}νj,k + Pi： mι∈^^ jιPι,k.
In order to specify our choice of (∆n)n≥ι, We consider a construction of sequences
(∆(j))j≥0, (η(j))j≥0 as follows. Fix some θ ∈ (0,α - 1)/2). Let ∆(0) = η(0) = 1. One
can see the existence of some (∆(j)j≥ι, (η(j)j≥ι such that
•	∆(j) ∈ (0, ∆(j - 1)/2], η(j) ∈ (0, η(j - 1)/2] for all j ≥ 1;
•	(Due to Lemma H.7) for any j ≥ 1, η ∈ (0, η(j)], (remember that x and i are the fixed
constants prescribed in the description of the lemma)
Px (σ0 (η, A(j)) < ηθ, er(η, A(j)) = i) > 1 - 2-.
For definitions of σ? ,τ? ,I? ,If, see eq. (H.6)-eq. (H.11).
•	(Due to Lemma H.11) for any j ≥ 1, η ∈ (0, ιη(j)],
Px(τk(η, A(j)) - σk(η, A(j)) < ηθ, 1G(η, A(j)) =R ∣ IG(η, A(j)) = iι)-加屈
< 1/2j
uniformly for all k ≥ 0 and all m汨 ∈ Gsmall, m逅 ∈ Glarge. Also, by definition of σ* and
τ*, we must have
Pxkki(η, A(j)) -σk(η, A(j)) = 0, IG (η, A(j)) = iι ∣ IG (η, A(j)) = i1) = 1
for all k ≥ 0 and mil ∈ Glarge.
• (Due to Proposition G.25) for any j ≥ 1, η ∈ (0, n(j)],
-2 + exp ( - (1 + 2)qiiu) i1].~~2l
≤px (σk+ι(η, A(j)) -τ晨η, A(j)) > u, e+ι = i21IG(η, A(j)) = iι)
≤ 2 + exp ( - (1 - 2 )qiiu) S N
qi1
uniformly for all k ≥ 1, all U > 1∕2j, and all mʤ ∈ Glarge, m^ ∈ G.
86
Published as a conference paper at ICLR 2022
•	(Due to G being absorbing and, again, Proposition G.25) for any j ≥ 1, for any j ≥ 1,
η ∈ (0, η(j)], (Recall the definition of Tk,Ik in eq. (H.39)-eq.(H.42))
Px (Ik+ι (η,	Aej)) =	i2	|	Ik	(η,	Aej)) =	iι)	<	21j	(H.64)
uniformly for all k ≥ 0, mi1 ∈ G, mi2 ∈/ G.
•	(Due to Lemma H.9) There exists some p* > 0 such that for any j ≥ 1, for any j ≥ 1,
η ∈ (O, η(j)],
Px(mv+Ik(ηAj)) ∈ Glarge ∀v ∈ [unmin] I Ikg Aej)) = il)
≤P(Geom(pt') ≥ u) + 1∕2j	(H.65)
uniformly for all k ≥ 0, u ≥ 1 and mi1 ∈ G.
•	(Due to Lemma H.12) for any j ≥ 1, for any j ≥ 1, η ∈ (0, η(j)],
Px (xik'ηn	∈ U	B(mj,	∆(j)),	Xsmn	∈ U	Ωj ∀s	∈ [0%，])	<	1∕2j
j: mj ∈Glarge	j: mj ∈G
(H.66)
uniformly for all k ∈ [k0].
FiX such (∆(j)j≥0, (η(j)j≥0. Define a function J(∙) : N → N as
J(n) = 0 ∨ max{j ≥ 0 : η(j) ≥ ηn}
with the convention that max 0 = -∞. Lastly, let
∆n = A(J(n)) ∀n ≥ 1.
Note that, due to limn ηn = 0, we have limn J(n) = ∞, hence limn ∆n = 0. Besides, the
definition of J(∙) tells us that in case that J(n) ≥ 1 (which will hold for all n sufficiently large), the
claims above holds with η = ηn and j = J(n). In particular, by combining limn J(n) = ∞ with
eq. (H.64)eq. (H.65)eq. (H.66) respectively, we have eq. (H.61)eq. (H.62)eq. (H.63).
Now it remains to prove eq. (H.60). To this end, it suffices to show that, for any positive integer
K, we have (S0n), W0(n), ∙ ∙ ∙ , S(n), W^)) converges in distribution (So, Wo,…，Sk, WK) as n
tends to infinity. In particular, note that S0 = 0, W0 = πG(mi), so W0 = mj with probability pi,j if
mi ∈ Gsmall, and Wo ≡ mi if mi ∈ Glarge.
For clarity of the exposition, we restate some important claims above under the new notational system
with Sek(n), Wfk(n), Sk(n), Wk(n) we introduced right above this lemma. Given any > 0, the following
claims hold for all n sufficiently large:
•	First of all,
PxSeo(n) <ηnθ, Wfo(n) =mi) > 1 -.	(H.67)
•	For all k ≥ 0 and all mi1 ∈ Gsmall , mi2 ∈ Glarge,
IIIPxSk(n)	-	Sek(n)	<	ηnθ,	Wk(n)	= mi2	III	Wfk(n)	= mi1)	-pi1,i2III	< .	(H.68)
•	For all k ≥ 0 and all mi1 ∈ Glarge,
PxSk(n) - Sek(n) =0, Wk(n) =mi1 III Wfk(n) =mi1) = 1.	(H.69)
87
Published as a conference paper at ICLR 2022
•	For all k ≥ 0, all mi1 ∈ Glarge, m^ ∈ G and all u > e,
—E + exp ( — (1 + e)qiιU) i1,i2---------
^ii
≤pχ (邓L - Skn) > u, fk+1 = mi2 I Wkn) = mil)
≤pχ (W - S(n) > u — ηt, f+ι = mi2 i w(n) = mil)
≤e + exp ( — (1 — e)qi1 u) "i1,i2 + '	(H.70)
•	Here is one implication of eq. (H.68). Since |G| ≤ nm⅛, We have
Px (SSn- S(n) ≥ ηn I W(n) = mi) < nmin ∙ E
for all k ≥ 0 and mi1 ∈ Gsmal'.
(H.71)
•	Note that for any m^, m^ ∈ Glarge and any k ≥ 0
mi2 I W(n)
=1 {i2= i1}Pχ (S(+)1 — S严 >u, f+1
=mij
mi2 I W(n) = mi1
+ X P Px (S(+1 - S(?1 ≥ (u - s) V 0, wk+)ι = mi2 1 w(n) = mi3
i3: mi3 ∈Gsmall J∙s>0
∙ Px (Skn)1 - Skn)= ds, wk+)i = mi3 1 w(n) = mi1).
Fix some i3 with mi3 ∈ Gsmall. On the one hand, due to eq. (H.71),
—
Skn)I ≥ U — s, Wk+)ι = mi2 1 W(n) = mi3
mi3
mi1
On the other hand, by considering the integral on (u — *, ∞), we get
S(+1 ≥ (u — s) V 0, f+1
mi2 1 fIkn) = mi3
≥ I	Pχ(f(2
S s∈ (u,∞)	`
mi2
ds, fk+1 =，
1 f(n) = mi3
mi3
mi1
—
∙ Px (Skn)1 — Skn)= ds, wk+)i = mi3 1 W(n) = mi1
≥(Pi3，i2—E)(Y + exp( —(1 + E)qi1 U) V^ )
due to eq. (H.68) and eq. (H.70). Meanwhile,
—
Skn)I ≥ (u - S)V 0, f ^+1 = mi2
mi3
mi3
≤(nmi∩e + pi3,i2 + E)(E + exp ( — (1 — E)qi1u)
I WF)=
νi1,i3 + E
mi1
qi1
due to eq.(H.68), eq. (H.70) and eq. (H.71).
88
Published as a conference paper at ICLR 2022
• Therefore, for any mi1, m^ ∈ GIarge and any k ≥ 0,
Px (Sk+)1 - S(T) > u, WkZ = mi2 I Wkn) = mil)
≤g(e) + exp ( - (1 - e)%U)
l{i2 = i1}νi1,i2 + Ei3:mi3 ∈Gsmall νi1,i3pi3,i2
qiι
≤g(e) + eχp (- (1 - e)qiιU)仇1"2
qiι
(H.72)
and
Px (Sfe+)1 - Skn) > u, Wk+1 = mi2 1 Wkn) = mi1)
/ ∖	/	{	∖	、l{i2 = i1}νi1,i2 + ∑i3: mi3 ∈Gsmall νi1,i3Pi3,i2
≥ - g(e) + eχp ( - (1 + e)qi1u)--------------------------------------------------------
qi1
≥ - g(E) + eχp ( - (1 + e)qi1 U) "1"2	(H.73)
qi1
where q* = maxi % and
g(e)=e+q*+nmin(I+e)e+e
1 + e
T nmin
+ nmin(e +---* ) + nmin(nmine + e + I)(IH--* )e∙
Note that lim40 g(e) = 0.
Now we apply the bounds in eq. (H.67)eq. (H.72)eq. (H.73) to establish the weak convergence claim
regarding (S(n), WHn),…,SP), Wy)). Fix some positive integer K, some strictly positive real
numbers (Sk)30, a sequence (Wk)k=0 ∈ (GIarge)	with Wk = mifc for each k, and some e > 0
such that e < mink=0,1,…，k{s( }. On the one hand, the definition of the CTMC Y implies that
Pb0 < t0, W0 = W0； Sk > Sk and Wk = Wk ∀k ∈ [K])
κ
=P(∏G(mi) = W0) Y (Sk > Sk and Wk = w(1 W(-i = w(-i)
k=1
κ
=(l(mi ∈ GIarge, i0 = i} + l(mi ∈ GSmall}pi°,i1) ∙ Y exp(-qik-1 Sk)qik-ʌ.
k=1	qik-1
On the other hand, using eq. (H.67)eq. (H.72)eq. (H.73), we know that for all n sufficiently large,
Px 同n) < S0, W0n) = W0； Skn) > Sk and Wkn)= Wk ∀k ∈ [K])
≥(1 - e) (1 {mi ∈ GIarge, % = i} + l(mi ∈ GSmall}(pic,,i1 - e))
K
∙ Y (- g(e)+ eχp(-(1 + e)qik-1 sk)仇k-1,”)
k = 1	qik-1
and
Px(S(n) < S0, Wf)= W0； Skn) > Sk and Wkn)= Wk ∀k ∈ [K])
≤(1 {mi ∈ Glarge, % = i} + l{mi ∈ Gsmall}(pi0,i1 + e))
K
∙ Y (g(e)+ eχp(-(1 - e)qik-1 sk) *-1"k).
k = 1 '	qik-1
89
Published as a conference paper at ICLR 2022
Since > 0 can be arbitrarily small, we now obtain
lim Px	S0(n)	< t0,	W0(n)	=w0;	Sk(n)	> sk and Wk(n)	=mk∀k	∈	[K]
=P	S0	<	s0 , W0	=	w0 ;	Sk	>	sk	and Wk	=	wk ∀k	∈	[K]	,
and the arbitrariness of the integer K, the strictly positive real numbers (sk)kK=0, and the sequence
(wk)K=0 ∈ (Glarge)K+1 allows Us to conclude the proof.	□
To extend the result above to the case where the communication class G is transient, we revisit the
definition of the Y * in eq.(H.26). Let G = Glarge ∪ {↑} and let mo = ↑ (remember that all the local
minimizers of f on [-L, L] are m1, ∙∙∙ , mm⅛)∙ Meanwhile, using qi and q%,j in eq. (H.20)eq. (H.21),
we can define
f qi,j	if i ≥ 1, j ≥ 1
qi,j = \ 1{j=。}	ifi = 0
[Pj∈[nmin],mj∈G qi,j if i ≥ 1，j = 0.
and q* = 1, qi = q% ∀i ≥ 1. Next, fix some i with mi ∈ G and X ∈ Ωi. Define a sequence
of random variables (Si)k≥o, (Wl)k≥o such that Si = 0 and Wo = ∏G(mi),Wti = ↑ 1{W° ∈
Glarge} + Wo l{Wo ∈ Glarge} (see the definition of random mapping ∏g in eq. (H.22)) and (for all
k ≥ 0 and i, j with mj, mi ∈ G)
PWki+1 =	ml,	Ski+1 >tWki=mj,	(Wli)lk=-o1, (Sli)lk=o	(H.74)
=P(Wk+1 =	mi,	Sk+1 > t j Wki	=	mj)	= exp(-qjt)j ∀t	> 0	(H.75)
qj
Then it is easy to see that Yi(∏G(mi)) defined in eq. (H.26) is a ((Sk)k≥o, (Wj)k≥o) jump process.
In particular, from at any state that is not ↑ (namely, any mj with mj ∈ Glarge), the probability that
Yk moves to ↑ in the next transition is equal to the chance that, starting from the same state, Y moves
to a state that is not in G. Once entering mo = ↑, the process Yk will only make dummy jumps (with
interarrival times being iid Exp(1)): indeed, we have qoi = qoi,o = 1 and qoi,j = 0 for any j ≥ 1,
implying that, given Wj = m0 = ↑, we must have Wj+1 = mo = ↑. These dummy jumps ensure
that Yk is stuck at the cemetery state ↑ after visiting it.
Similarly, we can characterize the jump times and locations of the jump process Xk,*,η,∆ (for the
definition, see eq. (H.25)). When there is no ambiguity about the sequences (ηn)n≥1, (∆n)n≥1, let
Xk,(n) = Xt/mnAn and Xk,(n) = X*,ηn,∆n. Also, recall that TG defined in eq. (H.4) is the step n
when Xnη exits the communication class G. Now let (Ek)k≥o be a sequence of iid Exp(1) random
variables that is also independent of the noises (Zk)k≥1 (so they are independent from the SGD
iterates Xnη). For all n ≥ 1, k ≥ 0, define (see eq. (H.6)-eq. (H.11) and eq. (H.15) for definitions of
the quantities involved)
；t,(n) _ ∫σfe(ηn, ʌn) ∧ T* (τG Sn ), 〃n ) - Tki-15n, ʌn)
k =0
，t,(n) _ ∫τk⅜n, ʌn) ∧ T(TG(M, M - Tki-1(即 ʌn)
k =	Ek
if Tki-15n, ʌn) < τ*(τGSn),1‰)
otherwise
ifTki-1(ηn, ʌn) < Ti(TG(ηn),ηn)
otherwise
f t,(n) = (mIG(ηn,∆n) if σk (ηn, ʌn ) < T* (TG(Inn), M
k ɪf	otherwise
W t,(n) =	mIkG (ηn,∆n)
k It
if Tki(nn, ʌn) < r(3(nn),nn)
otherwise
with the convention that T-i 1 = 0. Note that Ti (TG(ηn),ηn) is the scaled timestamp for X(n) = X i,η
corresponding to TG(%)，hence「(丁3(3), ηn = min{t ≥ 0 : X(n) ∈ Uj： mj∙∈g Ωj}. One can
90
Published as a conference paper at ICLR 2022
See that XUn) is a ((SFn))k≥o, (W产n))k≥0 jump process. The next lemma is similar to Lemma
H.14 and discusses the convergence of the jump times and locations of Xt,(n) on a communication
class G in the transient case.
Lemma H.15. Assume that the communication class G is transient. Given any mi ∈ G, X ∈ Ω%,
finitely many real numbers (tι)k= 1 such that 0 < tι < t2 < •…< tk/, and a sequence of strictly
positive real numbers (ηn)n≥1 with limn→0 ηn = 0, there exists a sequence of strictly positive real
numbers (∆n)n≥1 with limn ∆n = 0 such that
•	Under Px (so X0η = x), as n tends to ∞,
(S0,⑺,W0^,⑺M叫 WFn),s2,叫 WFn),…)⇒ (S0, W0^,s[,wJ,s2,wg,∙∙∙)
(H.76)
•	For any l ∈ [k0],
Jim Px (xtl,(n) ∈ U	B(mj, ∆n), Xyn) ∈ U Ωj ∀s ∈ [0,tι]) = 0
j : mj ∈Glarge	j: mj ∈G
(H.77)
Proof. Let
νj,k = μj(Ej,k) ∀j, k ≥ 1, j = k
Pj,t =	X Pjj ∀mj ∈ GSmall
j： me∈G
qj,t =∆	X νj,k + X	νj,kPk,t ∀mj ∈ Glarge.
k：mk ∈/ G	k： mk ∈Gsmall
In order to specify our choice of (∆n)n≥1, we consider a construction of sequences
(∆(j))j≥o, (η(j))j≥o as follows. Fix some θ ∈ (0,α - 1)/2). Let ∆(0) = η(0) = 1. One
can see the existence of some (∆(j))j≥ι, (η(j))j≥ι such that
•	∆(j) ∈ (0, ∆(j - 1)/2], η(j) ∈(0, η(j- 1)/2] for all j ≥ 1;
•	(Due to Lemma H.7) for any j ≥ 1, η ∈ (0, η(j)], (remember that X and i are the fixed
constants prescribed in the description of the lemma)
Px (σo (η, Acj)) < ηθ, eG(η, Aej)) = i) > 1 - 2j.
For definitions ofσkG, τkG, IkG, IkG, see eq. (H.6)-eq. (H.11).
•	(Due to Lemma H.11) for any j ≥ 1, η ∈ (0, η(j)],
P	x(琮(η, ACj)) - σk(η, ACj)) < ηθ, IG(η, A(j)) = i21 IG(η, ACj)) = iι) -pm
<	1/2j
uniformly for all k ≥ 0 and all m汨 ∈ Gsmall, m^ ∈ GsmalL Also, by definition of σ* and
T*, we must have
P	xkk(η, ACj)) -σk(η, ACj)) = 0, IG(η, ACj))= iι ∣ eG(η, ACj)) = iι) = 1
for all k ≥ 0 and mi1 ∈ Glarge.
•	(Due to Proposition G.25) for any j ≥ 1, η ∈ (0, η(j)],
-2 + exp ( - (1 + 2)qiιu) i1=~~2^
qi1
≤Px (σk+ι(η, ACj)) -K (η, ACj)) > u, e+ι = i21 IG(η, ACj)) = iι)
≤ I +exp—1- 1M u) VIJ
91
Published as a conference paper at ICLR 2022
uniformly for all k ≥ 1, all u > 1/2j, and all mi1 ∈ Glarge, mi2 ∈ G.
• (DUe to Lemma H.12) for any j ≥ 1, for any j ≥ 1, η ∈ (0, η(j)],
Pχ(x(n) ∈ U B(mj, ∆(j)), Xsn) ∈ U Ωj ∀s ∈ [0,tk]) < 1∕2j (H.78)
j: mj ∈Glarge	j: mj ∈G
Uniformly for all k ∈ [k0].
Fix such (∆(j))j≥o, (η(j))j≥o. Define a function J(∙) : N → N as
J(n) = 0 ∨ max{j ≥ 0 : η(j) ≥ ηn}
with the convention that max 0 = -∞. Lastly, let
∆n = ∆(J(n)) ∀n ≥ 1.
Note that, due to limn ηn = 0, we have limn J(n) = ∞, hence limn ∆n = 0. Besides, since
XJs) = χ(n) given Xsn) ∈ Uj. m,∈g Ωj for all s ∈ [0, t], by combining limn J(n) = ∞ with
eq. (H.78) we obtain eq. (H.77).
Now it remains to prove eq. (H.76). To this end, it suffices to show that, for any positive integer K,
we have (S0,(n), W0^,(n),…，sK(n), W∕n)) converges in distribution (S0, W0^,…，SK, WK) as
n tends to infinity. In particular, due to introduction of the dummy jumps, we know that for any k
with τk(ηn ∆n) ≥ τG(ηn) (in other words, Xt,(n) has reached state ↑ within the first k jumps) we
have Sk+n) 〜ExP(I) and WJ+；) ≡ ↑. Similarly, for any k with S0 + …+ Sk ≤ TY, we have
Sk+ι 〜Exp(1) and W1+ι ≡ ↑. Therefore, it suffices to show that, for any positive integer K, any
series of strictly positive real numbers (Sk)30, any sequence (Wk)K=0 ∈ (G)K+1 such that Wj = ↑
for any j < K, indices ik such that wk = mik for each k, we have
lim Px(S0,(n) < to, Wj,(n) = wo； San) > Sk and Wj，(n) = Wk ∀k ∈ [K])
n→∞
=p(sO < so, w0^ = wo； Sk > Sk and W』=Wk ∀k ∈ [K])	(H.79)
Fix some (Sk)K=o, (Wk)K=o ∈ (G)K+1,and indices (ik)3ι satisfying the conditions above. Besides,
arbitrarily choose some e > 0 so that E < mink=o,... ,k Sk. To proceed, we start by translating the
inequalities established above under the new system of notations.
• First of all, for all n sufficiently large, (remember that x and i are prescribed constants in
the description of this lemma)
Px (S0，(n) < ηn, f0^,(n) = m) > 1 - E.	(H.80)
• For all k ≥ 0 and all mi1 ∈ Gsmall , mi2 ∈ Glarge, it holds for all n sufficiently large that
Px(Sfc,(n) - S*) < ηn, Wk,(n) = mi2 I wl(n) = mil) -Piι,i2 < e∙ (H.81)
• On the other hand, for all k ≥ 0 and all mi1 ∈ Gsmall, it holds for all n sufficiently large that
Px 卜Fn)- SFn) < ηn, Wk,(n) = t I fJ，(n) = mil) - X	Piι,i2
i2 : mi2 ∈/ G
=Px 卜Fn)- S*) < ηn, Wk,(n) = t I fJ,(n) = mil) - Piι,t <E.	(H.82)
• For all k ≥ 0 and all mi1 ∈ Glarge, it holds for all n that
PxSkt,(n)	- Sekt,(n)	=0,	Wkt,(n)	=	mi1	III	Wfkt,(n)	= mi1)	= 1.	(H.83)
92
Published as a conference paper at ICLR 2022
• For all k ≥ 0, all m五 ∈ Glarge, m^ ∈ G and all u > e, the following claim holds for all n
sufficiently large:
—C + exp ( — (1 + e)qi1 U) i1,i2--
VP (e"n)—	S t(n)>	U	f t(n)— m-	I	W t(n)—	m∙)
≤Pχ<Sk+1	sk >	u,	wk+ι	— mi2	i	wk —	miι J
VP (*,(n) _ qt,(n) 、m _ θθ f t(n) — rrι ∣ tλ∕t,(n) — rrj、
≤Px (Sk+1 — Sk > u — ηn, Wk + 1 = mi2 ∣ Wk - miι J
≤C + exp ( — (1 — c)qi1 u) /1 ,i2 + C	(H.84)
qiι
• On the other hand, for all k ≥ 0, all m” ∈ Glarge, the following claim holds for all n
sufficiently large:
—C + exp ( — (1 + c)qi1 u)-""^mi2/G 忆'"
≤p (5⑺—St,(n) > u f t(n) = t I Wt,(n) — m∙)
≤px<sk+1 sk > u，wk+1 — * ∣ wk — mi1 J
VP (et,(n) qt,(n)、 θθ f^^,(n) — + I Wt,(n) — m、
≤Px (Sk+1 -Sk >U — ηn, Wk+1 = T ∣ Wk — mi1)
LI c ∕1	∖	∖ C + Ei2： mi2 /G ViIn2	CTOC
≤c + exp ( — (1 — c)qi1 U)-------------- (H.85)
qi1
•	Here is one implication of eq. (H.81)eq. (H.82). Since |G| ≤ nmin, We have (when n is
sufficiently large)
Px (Skn) — S(n) ≥ *θ ∣ Wkn) = mi1) < nmin ∙ C	(H.86)
for all k ≥ 0 and mi1 ∈ Gsmall.
•	Note that for any m”, m^ ∈ Glarge and any k ≥ 0
Ip) (qt,(n) qt,(n)、q. TVt，(n) 一 ™ ∣ TVt，(n) — rn ∖
Px (Sk+1 - Sk >u，Wk+1 = mi2 ∣ Wk = mi1)
=1 {i2 = iι}Px(St+? — SFn) > u, fk+? = mi2 I W产n) = mi1)
I	∖、	/	IP)	(qt,(n)	_	et,(n)	> (m _	cA	∖/	∩	f t,(n)	— Tn I Wt,(n)	— Tn )
+	ʌ,	/	Px	( Sk + 1	—	Sk + 1	≥ (u —	S)	∨	0,	wk+1	= mi2 ∣ Wk	= mi3 )
iɜ: mi3 ∈ Gsmall 's>0
JP) (et,(n) _ qt,(n) — J	f t(n) — m I Wt，(n) — Tn )
∙ Px (Sk+1 — Sk	= ds, wk+1 = mi3 ∣ Wk = mi1 J ∙
Fix some iɜ with mi3 ∈ Gsmall. Due to eq. (H.86)
P	P	(Stj(n)	— gt,(n)	≥ U	— s f t,(n) = m-	I f t(n)	= m-)
/	Px	I Sk + 1 Sk + 1	≥ U s, Wk+1 = mi2	I Wk	= mi3 J
Zs∈(0,u-ηn ]	∖	1	)
JP)	(et,(n)	_ qt,(n)	— J :f t(n)	— m I Wt，(n) —	Tn )
∙ Px	(Sk + 1	— Sk	= ds, wk+1	= mi3 ∣ Wk =	mi1 J
≤nminc∙
93
Published as a conference paper at ICLR 2022
Meanwhile, by considering the integral on (u - *, ∞), we get
P 叱卜船)
Js∈(u-ηn,∞)	'
邓乎 ≥ (u -S) V 0, f J" = mi2 I f 产n)= mi3
JP) (Qt,(n) _ qt,(n) — f ft,(n)——m
,px ( sk+1 - Sk = ds, W k + 1 = mi3
k，(n) = mii
≥ I Px(fk+?=mi2
S s∈(u,∞)	`
k,S)= mi3
P S ct,(n)	qt,(n) — dq f t，(n) — m
, Px(Sk+1 - S k = ds, W k + 1 = mi3
k,S) = mii
≥(Pi3,i2 - c) ( - C + exp ( - (1 + C)QiIu)
νii,i3 - C
Qii
due to eq.(H.81) and eq. (H.84). As for the upper bound,
P	Px(S 船)
J s∈(u-ηn,∞)	∖
SM) ≥ (u -S) V 0, fk,+n) = mi2 I f，(n)= mi3
P S et,(n)	qt,(n) — d q f t,(n) — m
, Px(Sk+1 - Sk	= dS，Wk + 1 = mi3
k,S) = mii
≤(nminc + Pi3,i2 + C)(C + exp ( - (1 - C)QiIu)
ViI ,i3 + C
Qii
—
—
due to eq. (H.81), eq. (H.84) and eq. (H.86).
• Therefore, for any mii, mi2 ∈ Glarge and any k ≥ 0,
JP) (Qt,(n) _ Qt,(n)、q. I"t,(n) — m I T4Zt,(n) — m
Px Sk+1 - Sk	> u, Wk+1 = mi2 Wk	= mii
≤g(c) +exp ( - (1 - c)%U)
l{i2 * i1}vii,i2 + Σi3: mi3 ∈Gsmall ViIn3pi3,i2
qii
≤g(c) +exp ( - (1 - c)qiιU)
qi1,i2
Qii
(H.87)
and
P S S t<n) - S t<n) > U Wt,(n) = m- I Wt,(n) = m-
Px I Sk+1 Sk > u, wk+1 = mi2 IWk = mii
≥ - g(C) + exp ( - (1 + C)Qii U)
l{i2 * iJνii,i2 + ∑i3: mi3 ∈Gsmall νii,i3 pi3,i2
Qii
≥ - g(C) + eχp ( - (1 + C)QiI u) "i'i2
Qii
(H.88)
where q* = maxi Qi and
g(C) = 2c + q* + nmin(1 + c)
C + C N nmm
q*
+ nmin(C + 亍 ) + nmin(nminC + C + I)(1 + 亍 )c∙
Note that lim^o g(c) = 0.
• On the other hand, for the case where the marker process Xt,(n) jumps to the cemetery state
↑ from some m汨 ∈ Glarge, note that
P (St,(n) _ St(n) > q. wt,(n)—十
PxISk+1 Sk	> u, wk+1 = T
=Px(Sk+?- Sk,⑺ > u,fk+n) = τ
+ X Z Px (Sk+n) - Sk+?
i3: mi3 ∈Gsmall 's>0
wk，(n) = mii)
Wk，(n) = mJ
≥ (u - s) v 0, wt+n) = T
k，(n) = mi3
JP) (Qt,(n) qt,(n)	— f	ft,(n)	— Tn I Wt,(n) —	Tn、
, Px (Sk + 1	- Sk	= dS,	Wk+1	= mi3 IWk =	mii)	.
94
Published as a conference paper at ICLR 2022
Arguing similarly as we did above by considering the integral on [0, u - ηnθ] and (u- ηnθ , ∞)
separately, and using eq. (H.82) and eq. (H.85), we then get (for all n sufficiently large)
JP) (qt,(n)	qt,(n)、q. I"t<n) — + I τ4Zt，(n)— ™ ʌ
Px (Sk + 1 - Sk	>u，Wk+1 = T ∣ Wk	— mil)
/ /、	/ /r 、	∖Ei2=mi2∈G νi1,i2 + Ei2：mi2 ∈ Gsmal1 νi1,i2 pi2,t
≤g(e) +exp ( - (1 - e)qiιU)-------2-----------------2---------------
qi1
—g(C) + exp ( - (I - e)qiιU)仇1，t	(H.89)
qi1
and
t,(n)	t,(n)	t,(n) II	t,(n)
Px Sk+1 - Sk	> U, Wk+1 — T I Wk	— mi1
、 i i	i i2 i	∖Σ2i2=mi2∈G ViIn2 + Ei2：mii2 ∈Gsm11 ViIn2pi2,t
≥ - g(c) + exp( - (1 + c)qiι U)-----2-----------------2---------------
qi1
——g(c)+exp ( — (1 + c)qiι U)仇 1'*	(H.90)
qi1
For simplicity of presentation, we also let qj,0 — qj,t and pj,0 — pj,t. First of all, remember that we
have fixed some series of strictly positive real numbers (Sk)K=0，some SeqUence (Wk)3o ∈ (G)K+1
such that wj 6— T for any j < K, and indices ik such that wk — mik for each k . The definition of the
continuous-time Markov chain Y t implies that
PS0 < t0, W0 — w0; Sk > sk and Wk — wk ∀k ∈ [K]
K
—P(∏G(mi) = W0)Y Zk > Sk and Wk — mk ∣ Wk-1 — Wk-1)
k=1
K
—(l{mi ∈ GIarge, io = i} + 1{mi ∈ Gsmall}Pi0,i1) ∙ Y exp(-qik-ι Sk)qk-1,ik∙
k=1	qik-1
On the other hand, using eq. (H.87)-eq. (H.90), we know that for all n sufficiently large,
Px 同n) < so, WOn)= wo; Skn) > Sk and Wkn)= Wk ∀k ∈ [K])
≥(1 - c) (1 {mi ∈ Glarge, io = i} + l{mi ∈ GSmall}(pi0,i1 - e))
K
• Y ( - g(e) + exp(-(1 + c)qik-ι Sk) ^k-I^)
k=1	qik-1
and
Px bOn) < SO, WAn)= wo; Skn) > Sk and Wkn)= mk ∀k ∈ [K])
≤(1 {mi ∈ Glarge, io = i} + l{mi ∈ Gsmall}(Pi0,i1 + E))
K
• Y (g(c) + exp(-(1 - C)qik-1 Sk)仇：\")∙
k=1	qik-1
The arbitrariness of c > 0 then allows us to establish eq. (H.79) and conclude the proof.	□
Now we are ready to prove Lemma H.4 and Lemma H.5.
Proof of Lemma H.4. From Lemma H.14, one can see the existence of some (∆n )n≥1 with
limn ∆n = 0 such that eq. (H.60)-eq. (H.63) hold. For simplicity of notations, we let X(n)=
X*,nnAn, X(n) = X*,ηn, and let I = tk0
95
Published as a conference paper at ICLR 2022
Combine eq. (H.60) with Lemma H.13, and we immediately get eq. (H.23). In order to prove
eq. (H.24), it suffices to show that for any > 0,
lim sup Px Xt(kn) ∈/	[	B(mj , ∆n)
n	j: mj ∈Glarge
≤ 4 ∀k ∈ [k0].
Fix > 0, and observe following bound by decomposing the events
PxXt(kn) ∈/	[	B(mj,∆n)
j: mj ∈Glarge
≤Pχ(x(n) ∈	U	B(mj, ∆n), X(n)∈ U	Ωj ∀t ∈ [0,a)
j: mj ∈Glarge	j: mj ∈G
+Px (∃t ∈ [0, t] SUCh that Xtn) / U Ωj)
j: mj ∈G
Therefore, given eq. (H.63), it sUffiCes to prove
limsup Px (∃t / [0, t] such that X(n) ∈ U Ωj) ≤ 3e.
n	j: mj∈G
(H.91)
Let
T0(n) =∆ min{t ≥ 0 : Xt(n) ∈ U B(mj,2∆n)}
j :mj ∈ G
IOn)=j ^⇒ XTnn)/ B(mj,"京
Tk(n) =∆ min{t > Tk(-n)1 : Xt(n) ∈ U B(mj,2∆n)}
j:mj ∈G, j 6=Ik(n-)1
Ikn)=j ^⇒ XTnn)/ B(mj, 2、n.
Building upon this definition, we define the following stopping times and marks that only records
the hitting time to minimizer in large attraction fields in G (with convention k(n),large(-1) =
-1, T-(n1),large = 0, T-(n1) = 0)
k(n),large(k) =∆ min{l > k(n),large(k- 1) : mI(n) ∈ Glarge}
T (n),large ∆ T(n)	I (n),large ∆ I(n)
Tk	= Tk(n),large (k) , Ik	= Ik(n),large(k) .
Now by defining
J(n)(t) =∆ #{k ≥ 0 : Tk(n) ≤ t},
Jl(anrg)e(t) =∆ #{k ≥ 0: Tk(n),large ≤ t},
J(n)(s,t) = #{k ≥ 0 : Tkn) / [s,t]},
we use J(n)(t) to count the numbers of visits to local minima on G, and Jl(anrg)e(t) for the number of
visits to minimizers in the large attraction fields on G. J(n)(s, t) counts the indices k such that at
Tk(n) a minimizer on G is visited and regarding the hitting time we have Tk(n) ∈ [s, t].
First of all, the weak convergence result in eq. (H.60) implies the existence of some positive integer
N () such that
limsupPx(Jlan)e⑴ > N(e)) < e∙
n
Fix such N (). Next, from eq. (H.62), we know the existence of some integer K() such that
IimsupsupPx(J(n) (Tk-)1-lαrge, Tk(n),large) > K(O∖ ≤ e∕N(e).
n k≥0
96
Published as a conference paper at ICLR 2022
Fix such K() as well. From the results above, we know that for event
Aι(n) = {J≤e(t) ≤ N(" """[-*/?),*) ≤ Kg ∀ ∈ [N(e)]},
we have limsupn Px ((A1(n))c) ≤ 2e. On the other hand, on event A1 (n), we must have
J (n) ⑴ ≤ NgKg.
Meanwhile, it follows immediately from eq. (H.61) that
Iimsupsupp(∃t ∈ ∖T(-)ι,T(b')] such that X(n) ∈	[ Ωj)
n	k≥0
j : mj ∈G

< NgKg,
hence for event
A2(n) = {x(n) ∈ U Ωj ∀t ∈ [0,τN⅛)κ(e)]},
j: mj∈G
we must have limSuPn Px ((A2 (n))c) ≤ e. To conclude the proof, note that
Aι(n) ∩ A2(n) ⊆ {J (n)(t) ≤ N (E)K (叫 ∩ {X(n) ∈ U Ωj ∀t ∈ [0,4(I")]}
j : mj ∈G
={TNN(e)K(e) ≥ t) ∩ {x'' ∈ U	Qj ∀t ∈ [0, TN(1)K(e)] }
j : mj ∈G
⊆ {x(n)∈ U Ωj ∀t ∈ [0刃}
j : mj ∈G
so we have established eq. (H.91).
□
Proof of Lemma H.5. From Lemma H.15, one can see the existence of some (∆n)n≥1 with
limn ∆n = 0 such that eq. (H.76) and eq. (H.77) hold. For simplicity of notations, we let
Xt，(n) = Xt，*，nnAn, Xt，(n) = Xt，*，nn, and lett= tko
Combine eq. (H.76) with Lemma H.13, and we immediately get eq. (H.27). In order to prove
eq. (H.28), note that
{xk(n) /	U	B(mj, ∆n) and XJjn) = t}
j: mj ∈Glarge
= {Xjk(n) /	U	B(mj，∆n) and XFn) / U Ωj ∀s / [0,tk]}
j: mj ∈Glarge	j:mj ∈G
so the conclusion of the proof follows directly from eq.(H.77).	□
I	FIRST EXIT TIME OF TRUNCATED HEAVY-TAILED SGD IN Rd
I.1 Main Result
The object of interests is the following truncated heavy-tailed SGD iterates
Xn+i(x) = Xn(X) + ψb{- ηVf(Xn(X)) + ηZk+ι) ∀k ≥ 0
where the initial condition is prescribed by X0 (x) = x, f is a real-valued function on Rd, η > 0
is the learning rate, (Zk)k≥1 is the sequence of heavy-tailed noises, the standard gradient clipping
operator is 夕b(v) = min{1,奇} ∙ V with ∣∣∙k being L2 norm. To ease notations, we also use Px to
denote the conditional law on {X0n = X}.
Specifically, we are interested in the first exit time of Xnn from a domain G, i.e. the stopping time
σ(η) = min{n ≥ 0 : Xnn ∈/ G}.
We work with following assumptions.
97
Published as a conference paper at ICLR 2022
Assumption I.1. The region G is connected, bounded and open, and 0 ∈ G.
Assumption I.2. The function f is smooth, i.e. f ∈ C2 (Rd).
Assumption I.3. The boundary set ∂G is a (n - 1)-dimensional manifold of class C2 such that the
vector field n(∙) of the outer normals on ∂ G exists with
▽f (V)Tn(v) ≥ co ∀v ∈ ∂G	(I.1)
for some constant c0 > 0.
Assumption I.4. For V2 f (0),the Hessian of f (∙) at point 0, all the eigenvalues are strictly positive.
For any X ∈ G,let xt(x) be the ODE flow with x°(x) = X solving
Xt(x) = -Vf (Xt(X)) ∀t ≥ 0,
Assumption I.5. W.L.O.G., the origin 0 is an attractor of the domain, i.e. Vf (0) = 0 and 0 is
asymptotically stable in G in the sense that
lim xt(x) = 0 ∀x ∈ G.
t→∞
We have the following assumption regarding the heavy-tailed noises (Zn)n≥1. For any X ∈ Rd, X 6= 0,
define the following polar transformation
T(X) = (kXk, X/kXk)	(I.2)
with Tr (X) = kXk , Tθ (X) = X/ kXk. Also, let O = {0}
Assumption I.6. EZ1 = 0. Besides, there exist a positive integer m ≥ 1, a sequence 1 < α1 <
a2 < •… < am < ∞, a sequence ofslowly-varyingfunctions (lι, ∙∙∙ ,lm),a sequence ofprobability
measures (Si,…，Sm) on the unit sphere SdT with support Fj = supp(Sj) as closed sets on SdT
such that
•	Fi ∩ Fj = 0 for any i = j;
•	Forany j ∈ [m], define the cone Ej = T-I(Fj) ∪ O in Rd and measure P(j)(∙) = P(Zi ∈
• ∩ Ej), we have
taj • lj (t) • P(j) ◦ T-1(t • dr X dθ) → Vaj (dr) X Sj (dθ) as t → ∞	(I.3)
in the sense of M(Ej \O). Here να is a Borel measure defined on (0, ∞) satisfying
να [t, ∞) = t-α ∀t > 0;
•	For measure P(0)(∙) = P(Zι ∈ ∙∖(∪m=IEj)) and any a > 0,
ta . P(O) ◦ T-1(t . dr X dθ) → 0 as t → ∞	(I.4)
in the sense ofM(Rd\O);
•	For any j ∈ [m], the measure Sj is absolutely continuous w.r.t. spherical measure on Sd-1.
A function l : R+ 7→ R+ is slowly varying (at ∞) if limt→∞ l(tX)/l(X) = 1 holds for any X > 0.
For details on M-convergence and regular variation in general metric spaces, see Lindskog et al.
(2014). Here we state one implication of the assumption. For any j = 0, 1, • • • , m, let
Hj(X)=∆Pj{y∈Rd : kyk ≥ X}.	(I.5)
The assumption above immediately implies that, for any j = 1, • • • , m, Hj (•) is regularly varying (at
∞) with index -aj (denoted as Hj ∈ RV-αj), namely limt→∞ Hj (tX)/Hj (t) = X-αj ∀X > 0.
Meanwhile, for any a > 0, Ho(x) = o(1∕xa) as X → ∞.
The asymptotically behavior of the first exit time hinges on the following geometric character-
ization of the domain G. For any integer k ≥ 1, a sequence of strictly positive real numbers
(t2, • • • , tk) with t1 = 0, a sequence of non-zero vectors (w1, • • • , wk) and some η > 0, let
98
Published as a conference paper at ICLR 2022
t(k) = (tι, •…，tk), w(k) = (wι, •…，Wk), and define the ODE path with k jumps (clipped at size b)
by w(k) = (wι, •…，Wk), we define perturbed ODE path eη as
eη(0； t(k), w(k)) = ψb(ηwι);
de©3：'w(k)) = -ηv"eη(t，x；t(k)，w(k))) ∀t ∈ {t1,t1 +12,…，XXtj}
j=1
j
eη(t, x；t(k),w(k)) = eη(t-,x；t(k),w(k)) + 4b(nwj) ift = ^Xti for some j
i=1
with the convention that g(t-) =∆ lims↑t g(s) for any function g. Also, when η = 1 we simply write
xe. Now we can assign a cost to each jump Wj based on the direction using the following function:
J(W)=αj-1 ifW6=0,W∈Ej,	(I.6)
∞ otherwise.
Given any set of perturbations described by t(k) ∈ {0} X R2-1, w(k) ∈ (Rd∖θ)k, we identify the
destination of the flow as
h(k；t(k),w(k))=xe(Xk tk； t(k), w(k)).
j=1
Besides, define function I(w(k)) =	(iι,…，im)	if #{i	∈	[k]	:	Wi	∈	Ej}	=	ij	for all j ∈	[m].
This allows us to define the following configuration sets
m
A(i1,…，im)= {w(k) ∈ (Rd∖θ)k : I(W(k)) = (iι,…，im), k = X ij }	(I.7)
j=1
for any (iι, ∙∙∙ , im) ∈ Nm, i.e., some set of jumps w(k) is said to have configuration (iι, •…，im)
or belong to the configuration set A(iι, ∙∙∙ , im) if the number of jumps in cone Ej is equal to ij.
We can also define the cost for each configuration as
m
J(ii,…，im) = X(αj- 1)ij.	(I.8)
j=1
Now we can characterize the minimum cost to exit G :
k
JG = min{X J(wj) : ∃k ∈ N, t(k) ∈ {0} × R2-1, w(k) ∈ (Rd∖θ)k
j=1
s.t. xe(Xk tk, 0； t(k), w(k)) ∈/ G}.	(I.9)
j=1
From the boundedness of G, one can see that 0 < JG < ∞ regardless of the actual value of the
clipping threshold b > 0. We need the following technical assumption regarding the configurations
of jumps that can trigger the exit with the minimum cost.
Assumption I.7. There exists only one array (iι,…，im) ∈ Nm such that Em=I j(αj _ 1) = JG
and (for k = Pjm=1 im)
∃t(k) ∈ {0} X r+-1, W(k) ∈ A(iι,…，im) s.t. e(X tk, 0； t(k), w(k)) ∈ G.
j=1
We use i* = (iɪ, •…，imm) to denote the unique configuration in Assumption I.7 and k* = Pm=I ij.
The implication is that, for a set of jumps t(k), w(k) with any other configuration, one of the following
must happen: (i) this set of jumps has a cost strictly higher than JG ; (ii) this set of jumps cannot send
the ODE flow out of G.
99
Published as a conference paper at ICLR 2022
Meanwhile, we introduce the following concept as the coverage of a certain configuration set.
G(i1,…，im)= {X(S, 0； t(k), W(k))
mk
k = Eij,	t(k) ∈	{0}	× r+-1, w(k)	∈ A(iι,…，im),	S ∈	[0,£tj]}.	(I.10)
It is easy to see that, for any configuration (iι, •…，im) with cost J(iι, ∙∙∙ , im) ≤ JG, the coverage
G(iι, ∙∙∙ , im) is a closed set, and the following technical assumption holds for (Lebesgue) almost
every b > 0. Here the distance d(A, B) = infχ∈A,y∈B ||x - yk for any A, B ⊆ Rd, and A° is the
interior of the set A.
Assumption I.8. For any (iι, ∙∙∙ , im) ∈ Nm, one ofthefollowing must occur:
•	d(G(iι,…，im), GC) > 0；
•	(G(iι,…，im) ∩Gc)0 = 0
Note that with i* and k defined above, We can define the following mapping h from r =
(ri,…，rk*), θ = (θι,…，θk*) ∈ (SdT)k*,t = (tι,…，tk*) ∈{0}× R** such that
k*
h(r, θ,t) = xe(X ti, 0； t, w)
*
where W = (wi)k=ι with Wi = riθi. Also, We introduce the concept of type for configuration i*.
Specifically, define
j(ii,…，im) = {(ji, •…,jk*) ∈{0,1, 2,…，m}k* : #{n : jn = k} = ik ∀k ∈ [m]}
and for any j ∈ j(i*), We say that W = (wi,…，Wk*) ∈ A(i*) has type j if
wi ∈ Eji ∀i ∈ [k*].
In other words, based on the direction of each jump in W ∈ A(i*) we group them into different types.
Note that |j(i*)| < ∞. Now for any type j ∈ j(i*), define a (Borel) measure on Rk+* × (Sd-1)k* ×
k*	k
μj = (∏Vaji) × (∏Sji) × mLJ
i=1	i=1
where, for any α > 0, the measure Va is the Borel measure on (0, ∞) with Vα(χ, ∞) = 1∕χ1+a.
Lastly, define measure μ as μ = Pj∈j(i*) μj. As will be established in Lemma K.2, the following
technical assumption is also a very moderate one since it holds for (Lebesgue) almost every b > 0
under the current setting.
Assumption I.9. The set h-i (∂G) has zero mass under the measure μ.
Having specified the problem setting, we are now ready to present Theorem I.1, the main result of
this section. The implication of the theorem is clear: under proper scaling, the first exit time σ(η)
converges in distribution to an Exponential random variable. Moreover, the scaling λ(η) is roughly
of order η1+JG, implying that the first exit time σ(η) is roughly of order 1∕η1+G as the learning rate
η approaches 0.
Theorem I.1. Let Assumptions I.1-I.9 hold. There exists a function λ(η) that is regularly varying (as
η J 0) with index (1 + JG) such that,for any X ∈ G and any t > 0,
lim Px(σ(η)λ(η) > t) = exp(-qt)
ηψo
where the constant q = μ(h-1(GC)).
The proof is provided in Section J. As a concluding remark, we stress that the order of the first exit
time is dictated by JG, the minimum cost for exit we introduced above. Bearing obvious similarity to
100
Published as a conference paper at ICLR 2022
the first exit time analysis of SDE driven by heavy-tailed Levy processes in Imkeller et al.(2010a),
our result can be viewed as a natural generalization when gradient clipping is applied and heavy-
tailed noises may not always align with a finite number of lines. In Imkeller et al. (2010a), the
unclipped setting implies that the escape from the domain G can always be achieved with one big
jump. However, a big perturbation (in noise) along different directions may correspond to different
heavy-tailed indices, i.e., induce different costs given our definition of cost function J(w) in eq. (I.6).
In Theorem 1 of Imkeller et al. (2010a), we see that the order of the first exit time is determined by α1 ,
the smallest heavy-tailed index. The underlying reason is that the exit is almost always trigger by a
single big jump with the smallest cost. Similarly, in our setting where multiple jumps are required for
escape due to the clipping mechanism, we see that the order of the first exit time is not dictated by the
number of jumps or the accumulated distances of the jumps, but the smallest possible accumulated
cost defined as the summation of J (wi) where wi ’s are the jumps the lead to escape from G. As
detailed in the proof, this is because such jumps with the smallest costs JG dictates the most likely
way for exiting G .
I.2	A Special Case: Uniform Heavy Tail Index Along All Directions
As stated above, Theorem I.1 deals with general case where the noise distribution is allowed to have
different heavy-tailed indices along different directions (see Assumption I.6). As a special case, it
is worth noticing that if a single heavy-tailed index α can be used to describe the tail behavior of
noises along any direction (this can be easily guaranteed if the heavy-tailed noise is manually injected
into a light-tailed setting), then the minimum cost JG will be equal to l*(α - 1) where l* is the
minimum number of jumps required for escape. The readers can see that this is a natural extension of
our R1 results, implying that the same strong preference for “wide” minima still hold in Rd under
truncated heavy-tailed SGD. To be specific, we work with the following assumption about the noise
distribution.
Assumption I.10. EZ1 = 0. Besides, there exists some α > 1, a slowly-varying functions l, a
probability measures S on the unit sphere Sd-1 with support supp(Sj) = Sd-1 such that
•	For the measure ν(∙) 二 P(Zι ∈ ∙ ∩ Ej), we have
tα ∙ l(t) ∙ ν ◦ T-1(t ∙ dr X dθ) → Vα(dr) X S(dθ) as t → ∞	(I.11)
in the sense of M(Rd\O). Here να is a Borel measure defined on (0, ∞) satisfying
να [t, ∞) = t-α ∀t > 0;
•	The measure S is absolutely continuous w.r.t. spherical measure on Sd-1.
Compared to Assumption I.6, one can easily see that Assumption I.10 is a stronger version with the
specific proviso that a single index α can describe the tail of the noise distribution along any direction
in Rd . The first exit time results now admit a simplified form. In particular, the cost of jumps will
degenerate to the count of jumps in the sense that Pik=1 J(wi) = k(α - 1) for any k ≥ 1 and any
(wι,…，wk) ∈ (Rd∖O)k. Moreover, we now have JG = IG ∙ (α - 1) where
IG = min{k ∈ N : ∃t(k) ∈ {0} X R2-1, w(k) ∈ (Rd∖θ)k s.t. e(Xtk, 0; t(k), w(k)) ∈ G}
j=1
/LτlW	/LτlW	l^ -1
and the measure μ can now be expressed as μ = (∏i=ι Va) X (Ili=I S) X mLGeb . Therefore, the
following theorem is merely a restatement of Theorem I.1 in this simplified setting. Still, we present
the result to highlight the role of the minimum jump number JG in the first exit time.
Theorem I.2. Let Assumptions I.1-I.5, I.10 and I.7-I.9 hold. There exists a function λ(η) that is
regularly varying (as η J 0) with index 1 + IG (ɑ — 1) such that, for any X ∈ G and any t > 0,
lim Px(σ(η)λ(η) > t) = exp(-qt)
ηψo
where the constant q = μ(h-1(GC)).
It is clear that first exit time since the first exit time is roughly of order 1∕η1+(a-1)lG. Therefore, even
in the general Rd case, the quantity l%, i.e. the minimum count of jumps to escape from a domain G,
101
Published as a conference paper at ICLR 2022
induces a hierarchy of first exit time as the first exit time from the domain with largest q(requiring
most number of jumps to escape) will dominate the first exit time from other regions.
Lastly, if we work under the standard setting such as the ones in Zhou et al. (2020) where local
strong convexity of f in the domain G is assumed, the results can be further simplified and the
minimum jump number l% will be directly tied to the width of the each domain. Specifically, the
quantity rG =∆ supy∈G kyk can be interpreted the effective width or radius of G, and we work with
the following assumption.
Assumption I.11. The function f is strongly convex on the closed ball {y ∈ Rd : kyk ≤ rG}.
Two consequences follow immediately from this assumption. First of all, there exists some constant
c > 0 such that
kxt(x)k	≤	kxk e-ct	∀x	∈	{y	:	kyk	<	rG}.
As a result, we must have l& ≥ rg/b. Next, as long as rg/b is not an integer (which holds for Lebesgue
almost every b > 0), for k = drg/be we can find t2 > 0,…，tk > 0 and wι = 0,…，wk = 0
where e(s, 0; (0, t2,…，tk), (wι,…，Wk)) ∈ G for some s > 0. In fact, it is worth noticing that
Assumption I.8 now degenerates to the condition that rG/b is not an integer, and we now know that
for any such b > 0, we have lg ≤ drg/be. In summary, we have established the following result
indicating that the strong preference for wider minima under truncated heavy-tailed SGD still persists
in Rd given proper convexity assumption on f .
Theorem I.3. Let Assumptions I.1-I.5, I.10-I.11 and I.7-I.9 hold. There exists a function λ(η) that is
regularly varying (as η J 0) with index 1 + l*(α - 1) such that, for any X ∈ G and any t > 0,
lim Px(σ(η)λ(η) > t) = exp(-qt)
ηψo
where the constant q = μ(h-1(GC)) and l* =drg/b^].
I.3	Relaxing the Technical Assumptions
In order to achieve the tightest characterization of the first exit time, some assumptions introduced
above are slightly stronger than the ones in Imkeller et al. (2010a). For instance, in Assumption I.3
we require ∂G to be of class C2 while the assumption (A3) in Imkeller et al. (2010a) only requires
it to be a C 1 manifold. Besides, Assumption I.6 requires that Sj , the limiting distribution of the
directions of each heavy-tailed component in noises, to be absolutely continuous w.r.t. the spherical
measure. As will be stressed in Section K.1, these conditions are only imposed to prove Lemma
K.2, thus ensuring that Assumption I.9 holds for almost every b > 0. Briefly speaking, all the efforts
to guarantee Assumption I.9 allows us to conclude that the law of scaled first exit time λ(η)σ(η)
converges exactly to that of Exp(q) with q = μ(h-1(Gc).
Fortunately, the discussion below will show that, even when we relax all the extra technical assump-
tions above (hence removing Assumption I.9), the order of the first exit time is still dictated by the
minimum cost for escape Jg, and a similar result about the first exit time can be obtained where the
distribution of the scaled first exit time λ(η)σ(η) will be asymptotically bounded by two Exponential
RVs.
Specifically, we reiterate that the first half of Assumption I.3, i.e. ∂G is a differential manifold, will
only be applied to prove Lemma K.2. The second half of the proof, namely the lower bound in
eq. (I.1), only serves the ensure that for any x ∈ G, we have xt(x) ∈ G ∀t ≥ 0 so that the gradient
flow starting in G will never leave this domain. Now let us focus on the open sets G and G for some
small > 0 where
G=∆	{y∈Rd:	d(y,	G) <},	G=∆	{y∈G:	d(y,	Gc) >}.
Note that ∩eGe = G, ∪eGe = G and for any positive reals ∈ι,⑦ such that ∈ι =⑦ and the two reals
are small enough for G1 and G2 to be non-empty,
∂Ge1 ∩ ∂Ge2 = 0,	∂Ge1 ∩ ∂Ge2 = 0.
Given eq. (I.1), as well as the smoothness of the vector field Vf (see Assumption I.2) and the fact
that G is connected, bounded and open and contains a unique attractor 0 (see Assumption I.1 and I.5),
102
Published as a conference paper at ICLR 2022
it is easy to see that for all > 0 sufficiently small, we will have xt(x) ∈ G ∀t ≥ 0 for any x ∈ G
and xt (x) ∈ G ∀t ≥ 0 for any x ∈ G.
Meanwhile, in our proof below we will establish eq. (J.17) (which, again, does not require the
C2 class assumption on ∂G or the absolute continuity assumption on the measures Sj ). Note that
eq.(J.17) implies the existence of ∆ > 0 such that μ(h-1(G∆)) < ∞. Therefore, for all but only
countably many E ∈ (0, ∆), We have μ(h-1(∂Ge)) = μ(h-1(∂Ge)) = 0. In other words, We can
find a sequence of n with limn n = 0 such that
μ(h-1(∂Gen)) = μ(h-1(∂Gen)) = 0 ∀n ≥ 1
. Also, for a fixed b > 0 that ensures Assumption I.7 and I.8, one can see that similar conditions Will
also hold for sets G and G as long as E > 0 is sufficiently small. In summary, if We consider first
exit times
σn(η) =∆ min{k ≥0 : Xkη ∈/ Gn}, σn(η) =∆ min{k ≥0 : Xkη ∈/ Gn}
With the obvious bounds that σn(η) ≤ σ(η) ≤ σn(η) for all n ≥ 1, then it suffices to apply
Theorem I.1 directly onto σn(η), σn(η). Note that We did not attempt to establish that ∂G or ∂G are
differential manifolds. HoWever, We reiterate that this is not needed since the C2 manifold condition
in Assumption I.3 only serves to ensure that Assumption I.9 holds for almost surely every b > 0,
Which We sidestep by picking a proper sequence En to explicitly satisfy the condition. More formally
speaking, consider the folloWing relax ted assumptions.
Assumption I.12. The boundary set ∂G is a (n - 1)-dimensional manifold of class C1 such that the
vector field n(∙) of the outer normals on ∂ G exists with
▽f (V)Tn(v) ≥ co ∀v ∈ ∂G
for some constant c0 > 0.
Assumption I.13. EZ1 = 0. Besides, there exist a positive integer m ≥ 1, a sequence 0 < α1 <
α? < •…< am < ∞, a sequence ofslowly-varyingfunctions (lι, ∙∙∙ , lm),α sequence OfProbabiUty
measures (Si,…，Sm) on the unit sphere SdT with support Fj = supp(Sj) as closed sets on SdT
such that
•	Fi ∩ Fj = 0 for any i = j;
•	Forany j ∈ [m], define the cone Ej = T-I(Fj) ∪ O in Rd and measure P(j)(∙) = P(Zi ∈
• ∩ Ej), we have
taj • lj (t) • P(j) ◦ T-1(t • dr X dθ) → Vaj (dr) X Sj (dθ) as t → ∞
in the sense of M(Ej \O). Here να is a Borel measure defined on (0, ∞) satisfying
να [t, ∞) = t-α ∀t > 0;
•	For measure P(0)(∙) = P(Zi ∈ ∙∖(∪m=IEj)) and any α > 0,
ta • P(O) ◦ T-1 (t • dr X dθ) → 0 as t → ∞
in the sense ofM(Rd\O).
We say Assumptions I.1-I.2, I.12, I.4-I.5, I.13, I.7-I.8 are the set of relaxed assumptions. The
discussion above implies that the folloWing result is an immediate consequence from Theorem I.1.
Theorem I.4. Let the relaxed assumptions hold. There exists a function λ(η) that is regularly varying
(as η J 0) with index (1 + JG) such that, for any X ∈ G and any t > 0,
exp(-qt) ≤ lim inf Px (σ(η)λ(η) > t) ≤ lim sup Px (σ(η)λ(η) > t) ≤ exp(-q↑ t)
ηψo	ηψo
with q = μ(h-1(Gc)), q↑ = μ(h-1((G)c))
J Proof of Theorem I.1
This section is devoted to establishing Theorem I.1. For clarity of the exposition, We break doWn the
proof into several major steps, each contained in a subsection beloW.
103
Published as a conference paper at ICLR 2022
J.1 Picking constants t, e, S
The goal of this subsection is to fix several important constants that characterize the typical sizes and
inter-arrival times of large perturbations in sGD trajectory that can cause the escape from domain
G . We will abuse the notations slightly when referencing certain constants, and quantities such as
c0, c1, c2 may not be equal to the ones used in assumptions above.
First, from Assumption i.6 and the definition of JG, we can define
l* = dJg∕(αι — 1)] + 1	(J.1)
and We must have ∞ > l* > k*. Meanwhile, from Assumption I.8, We can find S > 0 such that
d(G(iι,…，im), GC) > 100l*S	(J.2)
for any (i1 , . . . , im) ∈ Nm With Pjm=1 ij (αj — 1) < JG, as Well as
∃x ∈ G(i*) such that d(x, G) > 100l*S.
Furthermore, Assumption I.5 implies that Vf (v) = 0 for any V ∈ G∖{0}. From Assumptions I.3, I.4
and I.5 (and by picking a smaller S > 0 if needed), one can see the existence of some c0 > 0, S > 0
such that
Ilxt(X)k≤ e-c0t∣∣x∣∣ ∀t ≥ 0, x ∈ B(0, S),	(J.3)
IlVf(x)k ≥ co ∀x ∈G∖B(0,S),	(J.4)
d(xt(x), Gc) ≥ d(x, Gc) + cot if xs(x) ∈ G and d(xs(x), Gc) ≤ S∀s ∈ [0, t].	(J.5)
Here B(x, r) = {y ∈ Rd : Ix — yI < r} is the open ball centered at x With radius r > 0. One
immediate consequence from eq. (J.5) is that
d(xt(x), Gc) ≥ min{d(x, Gc), S} ∀x ∈ G, t ≥ 0.	(J.6)
Besides, the boundedness of domain G and smoothness of f (∙) imply the existence of some M > 0
such that
G⊆ B(0,M),	(J.7)
kVf(x)k≤ M ∀x ∈G,	(J.8)
∣∣V2f (x)∣∣ ≤ M ∀x ∈G	(J.9)
Where for the matrix norm We use spectral norm.
Recall the definitions of the configuration sets A(iι, •…，im), the unique configuration i* and count
of jumps k* to trigger the exit With minimum cost (see Assumption I.7 and the remark underneath).
The folloWing three results provide control on the sizes and inter-arrival times of typical jumps that
can trigger the escape from G. The proofs involve analyses of the deterministic dynamical system
xet and Will be provided in section K. In particular, We introduce another concept as type of jumps
(wι, ∙∙∙ , Wk) that is similar configuration (iι,…,im) and configuration sets A(iι,…，im). For
any k ∈ N and any (wι,…，Wk) ∈ (Rd∖O)k, we say that the (wι,…，Wk) is of type-j for some
j = (jι,…，jk) ∈{1, 2, ∙∙∙,m}k iff
Wi ∈ Eji ∀i = 1,…，k.	(J.10)
In other words, if w1 and w2 are of the same type j, then they have the same cardinality |w1 | =
|w2 | = |j| = k for some k ∈ N; moreover, for any i ∈ [k], the jumps Wi1 and Wi2 are both in the cone
Eji so both jumps point at directions in cone Eji and have the same cost J(Wi1) = J(Wi2) = αji — 1
for some ji ∈ [m]. Define the set
Atjype = {w ∈ (Rd\O)|j| : w is of type-j}.
The accumulated cost for any W ∈ Ajype is defined as Jype(j) = Pi=ι(αji 一 1). Lastly, define the
skip-one accumulated cost for any type j as
JtypeCj ) = max{	X	(αji - 1) : k = 1, 2, ∙∙∙ , |j|},
i=1,…，jl,i=k
i.e., Jtyype(j) is the highestpossible accumulated cost if we remove one element in W for W ∈ Ajpe.
104
Published as a conference paper at ICLR 2022
Lemma J.1. There exist some t ∈ (0, ∞), δ ∈ (0, ∞),s > 0 such that the following claim
holds. Let k0 ∈ N and j = (jι,…，jko) ∈ {1, 2,…，m}k0 be such that Jtype j < JG. For any
t(kO) = (tι,…，tko) ∈ {0} XR+-1, w(k0) = (wι,…，wk，) ∈ Atype, thefollowingsetofconditions
tj <i ∀j = 2, 3,…，k0	(J.11)
Ilwj k >i ∀j = 1,2,…，k0.	(J.12)
is the necessary condition for inf s≥0 d xe(s, 0; t(k), w(k)), Gc ≤ 0.
Lemma J.2. Let ti, δi, 0 be the positive constants prescribed in Lemma J.1. There exists some 1 > 0
such that the following claim holds.
Let k0 ∈ N and j = (jι,…，jk，)∈ {1, 2,…，m}k0 be such that Jtype(j) < JG. For any
X ∈ B(0, 6ι) ,any t(k0) = (tι,…，tk，)∈ {0} × R；'-1, w(k0) = (wι, ∙ ∙ ∙ ,wko) ∈ Ajpe, the
following set of conditions
tj < 2i ∀j = 2, 3,…，k0	(J.13)
Ilwjk >i∕2 ∀j = 1, 2,…，k0.	(J.14)
is the necessary condition for inf s≥0 d xe(s, x; t(k), w(k)), Gc ≤ 0/2.
Lemma J.3. There exist some 0 > 0, δ0 > 0 such that
sup d(e(s, x; t(k), w(k)), Gc) > €o
s≥0
for all x ∈ B(0, δ°), all (iι,…，im) ∈ Nm with J(iι,…，im) < JG and k = Em=I j, and all
w(k) ∈ A(ii,…，im), t(k) ∈ {0} × R；-1.
Similar to the ODE path x, we defined the following paths with rate η > 0:
(x?)(x) = -ηVf(x?(x)) ∀t ≥ 0,	(J.15)
xη0 (x) = x.	(J.16)
In other words, ODE flow xt is equivalent to xtη with rate η = 1. The next result gives us an upper
bound for the return time to a neighborhood of the local minimum as for the gradient flow.
LemmaJ.4. For TODE(x，e) = min{t ≥ 0 : x? (x) ∈ B(0, e)}, there exists ci ∈ (0, ∞) such that
for any ∈ (0, i),
TODE(X) ≤ c1 + c1 nog%) ∀x ∈G.
Proof. DUe to eq. (J.4), We know that To = Supx∈g f(x);infx∈G f(x) < ∞ and
c0
sup TO1 DE (x, i) ≤ T0 .
χ∈G
FUrthermore, from eq. (J.3) one can see that
1 ( ∖ 1 1	∕θg(1/E)Tog(1∕i)
Sup TODE (x, e) - TODE (x, e) ≤	.
x∈G	co
Now combining these two bounds with a 1/n time scaling, we have
SUD τ?	(X e) ≤ c1 + c1 log(1∕e)
Sup 1ODE (X, E) ≤
x∈G	n
for C1 = max{To + IogD,看}.	□
With Lemma J.1-J.4, we are able to pick some constants to facilitate the analysis below. By decreasing
Ei and increasing M if needed, we can assume the existence of some ti < ∞, δi < Ei such that
105
Published as a conference paper at ICLR 2022
• (Due to LemmaJ.1 andJ.2) For any k ∈ N and j ∈ {1, 2, ∙∙∙ , m}k such that J⅛e(j) < JG,
any X ∈ B(0,100l*e), any t(k) = (tι,…，tk) ∈ {0} X R：-1, w(k) = (wι,…，wk) ∈
Atjype with
k
d(e(X tj,x; t(k), w(k)), Gc) < 100l*e,
j=1
it holds that
tj <t ∀j = 2, 3,…，k and Ilwjk > δ ∀j = 1,2,…，k.	(J.17)
•	For any (i1, . . . , im) ∈ Nm with J(i1, . . . , im) < JG,
d(G(iι,…，im), Gc) > 100l*e;	(J.18)
•	There exists some X ∈ G(i*) such that
d(x, G) > 100l*e;	(J.19)
•	(Due to Lemma J.3) The constant e > 0 is sufficiently small such that
sup d(e(s,x; t(k), w(k)), Gc) > 100l*<≡	(J.20)
s≥0
for all x ∈ B(0,100l*e), all (iι,…，im) ∈ Nm with J(iι,…，im) < JG and k =
Pm=ι j, and all w(k)∈ A(iι,…，im), t(k) ∈ {0} × R：-1;
•	(Due to Lemma J.4) The constant t < ∞ is large enough so that
SUp TODE (χ, C) < t7n.
x∈G
J.2 Bounding the distance between ODE flow and SGD iterates
Moving on, we show that, without large jumps, the SGD iterates Xnη (X) are unlikely to show
significant deviation from the deterministic gradient descent process yηn(X) defined as
y0η(X) = X,	(J.21)
yn(x) = yN-ι(χ) - nW(yL(χ))∙	(J.22)
We are ready to state the first lemma, where we bound the distance between the gradient descent
iterates yηn (y) and the ODE xη (t, X) when the initial conditions X, y are close enough.
Lemma J.5. Given t > 0 and X, y ∈ G such that the line segment between xη (k, X) and yηk (y) lies
in G for any k = 1, 2, .…，[t],
sup xη(s,X) - ybηsc(y) ≤ (2nM + kX - yk) exp(nM t)
s∈[0,t]
where M ∈ (0, ∞) is the constant in eq. (J.8)eq. (J.9).
Proof. Define a continuous-time process yη (s; y) =∆ ybηsc (y), and note that
xη (s, X) = xη (bsc, X) - n	Vf (xη (u, X))du
bsc
Vf (xη (u, X))du
xη (bsc, X) = X - n
0
ybηsc(y) = yη(bsc,y) = y -nZ
bsc
Vf (yη (u,y))du.
Therefore, if we define function
b(u) = xη (u, X) - yη (u, y),
106
Published as a conference paper at ICLR 2022
from the fact ∣∣Vf (∙)k ≤ M, one can see that Ilb(U)Il ≤ ηM + ||x - yk for any U ∈ [0,1) and
∣∣b(1)k ≤ 2ηM + ||x - y∣. In case that s > 1, from the display above and the fact ∣∣V2f (∙)∣∣ ≤ M,
we now have
∣∣ybηsc(x) -xη(s,x)∣∣ ≤ ∣b(bsc)∣ +ηM;
∣b(bsc)∣ ≤ ηM
Z bsc ∣b(U)∣
dU.
From Gronwall’s inequality (see Theorem 68, Chapter V of Protter (2005), where we let function
α(U) be α(U) = ∣b(U + 1)∣), we have
∣∣∣ybηsc(x) - xη (s, x)∣∣∣ ≤ (2ηM + ∣x - y∣) exp(ηM t).
This concludes the proof.	□
Now we consider an extension of the previous Lemma in the following sense: when both perturbed
by at most l* similar perturbations, the ODE flow and gradient descent process should still stay close
enough. Analogous to the definition of the perturbed ODE xeη , we can construct a process yeη as a
perturbed gradient descent process as follows. For some integer 1 ≤ l ≤ l*, a sequence of strictly
positive integers 也，…，tι) (with convention tι = 0 and let t(l) = (tι,…，tι)), a sequence of
non-zero vectors W(l) = (Wι,…，Wι) and y ∈ Rd, define the perturbed gradient descent iterates
with gradient clipping at b as
yenη(y; t(l), we(l))
l
=en-ι(y; t(l), W(I)) + ^b( - ηVf (yn-ι(y; t(l), W(I))) + X l{n = tι + …+ tj}Wj) (J.23)
j=2
with initial condition 谭(y; t⑴,W(I)) = y + 夕b(泊ι).
Corollary J.6. For any fixed > 0, t > 0, the following claim holds for all η > 0 sufficiently small:
given any x,y ∈ G, some integer 1 ≤ l ≤ l*, a sequence ofstriCtly positive integers t(l) = (tj)j=2
(with convention t1 = 0), and any two sequences of vectors W(l) = (wj)lj=1, We(l) = (wej)lj≥1 such
that (let tend = Plj=1 tj)
•	|x -y| < ;
•	tj ≤ 2t∕ηforαll j ∈ [l];
•	|wj - wej| < for allj ∈ [l];
•	The line segment between xeη (k, x; t(l), W(l)) and yekη (y; t(l), We(l)) lies in Gfor any k ≤
tend - 1,
then we have
sup ] ∣∣eη (s,x; t(l), W(I))- eηsc (y; t(l), W (I))∣∣ ≤ ρ(t)e
一一	，，一-、、，*	,—,	一	一 一
where the constant ρ = (2exp(2Mt) + 2)l . In particular, P = ρ(t) is a constant where 力 is the
constant in eq. (J.11).
Proof. Throughout this proof, fix some η ∈ (0, ∕2M). We will show that for any η in the range the
claim would hold.
First, on interval [0, t2), from Lemma J.5, one can see that (since 2Mη < )
sup ∣∣eη(s, x; s(l), W(I))—冗」(y; t(l), W(I)) ∣∣ ≤ 2 exp(2Mt) ∙ e.
107
Published as a conference paper at ICLR 2022
The at t = t2 , by considering the difference between w2 and we2, and the possible change due to one
more gradient descent step (which is bounded by ηM < ), we have
sup	xeη (s, x; s(l)
s∈[0,t2]
w(l)) - esc (y; t(l), W(I))II ≤ (2exp(2Mt)+2) ∙ e.
Now We proceed inductively. For any j = 2,3,…，l - 1, assume that
sup ∣∣eη(s, x; S(I), W(I)) — esc (y; t⑴,W(l)) ∣∣ ≤ (2 exp(2Mt) + 2)j-1 ∙ e.
Then by focusing on interval [tj, tj+1] and using Lemma J.5 again, one can show that
sup	∣∣eη (s, x; S(I), W(I)) — esc (y; t(l), W(l)) ∣∣ ≤ 2e + ((2 exp(2Mt) + 2)j-1 + 1)exp(2Mt)e
≤ (2exp(2Mt) + 2)j ∙ e.
This concludes the proof.	□
Using an almost identical approach, one can show the following results that bounds the distance
between two ODE flows with similar initial values and jumps.
Corollary J.7. There exists some constant ρ* ∈ (0, ∞) such that the following claim holds: For
any fixed e > 0, any x,y ∈ G, any integer 1 ≤ l ≤ l*, a sequence of strictly positive integers
t(l) = (tj)lj=2 (with convention t1 = 0), and any two sequences of vectors W(l) = (wj)lj=1, We(l) =
(wej)lj≥1 such that (let tend = Plj=1tj)
•	|x -y| < ;
•	tj ≤ 2t∕ηforall j ∈ [l];
•	|wj - wej | < for all j ∈ [l];
•	The line segment between xeη(s, x; t(l), W(l)) and xeη(s, y; t(l), We(l)) lies in Gfor any s <
tend,
then we have
sup ∣∣xeη (s, x; t(l), W(l)) - xeη(s,y; t(l), We (l))∣∣ ≤ ρ*.
s∈[0,tend]
In the next few results, we show that a similar type of control can be obtained on the distance between
gradient descent iterates yenη and the SGD iterates Xnη. Specifically, our first goal is to show that before
any large jump, it is unlikely that the (deterministic) gradient descent process ynη would deviate too
far from Xnη . To facilitate the analysis below, we introduce some additional notations. First, we will
group the noises Zn based on a threshold level δ > 0: let us define
Z≤δ'η = Znl{η kZnk≤ δ},	(J.24)
Z>δ,η = Znl{η kZnk>δ}.	(J.25)
The former are viewed as small noises while the latter will be referred to as large noises or large
jumps. Furthermore, for any j ≥ 1, define the jth arrival time, size, and direction of large jumps as
Tjη(δ)=∆ min{n > Tjη-1(δ) : ηkZnk >δ},	T0η(δ) =0,	(J.26)
Wjη (δ) =∆ ZTjη(δ),	(J.27)
Θjη(δ) =∆ Wjη(δ)/ ∣∣Wjη(δ)∣∣ .	(J.28)
The following event
A(n,η, e,δ) = n max η ∣∣Zι +--------------------+ Zk k ≤ Eo	(J.29)
G=1j2j∙∙∙ ,n∧(τη (δ)-1)	J
describes a scenario where, before the first large jump, not much perturbation has been caused by
noises.
108
Published as a conference paper at ICLR 2022
Lemma J.8. For any > 0, η > 0, any x, y ∈ G and positive integer n such that kx - yk <
2exp(ηMn), on event
A(n, η, 2exp(ηMn), δ) \
the line segment between yjη (y) and Xjη (x) lies in G ∀j = 0, 1,…，n ∧ (Tn(δ)- 1)},
we have
kym(y) - Xm(x)k <e ∀m =1, 2,…，n ∧ (Tn(δ) - 1).
Proof. On the said event, we are able to apply Lemma G.5 and obtain that
∣∣yjn(y)-Xn (x)∣∣ ≤ (kx-yk + 2exp(；Mn))exP(nMj)<e W 2,…，n ∧ (Tn (δ) - 1)
and conclude the proof.	□
Similar to the extension from Lemma J.5 to Corollary J.6, we can extend Lemma J.8 to show that, if
we consider the a gradient descent process that is only perturbed by large noises, then it should stay
pretty close to the SGD iterates Xnn . To be specific, let
Y0n (x) = x
Yn (X) = Yn-I(X)- 3b( - ηVf (Y!ι(χ)) + X i{n = Tn (δ)}ηZn).
j≥1
be a gradient descent process (with gradient clipping at threshold b) that is only perturbed by large
noises. The next corollary can be shown by an approach identical to the one for Corollary J.6 (namely,
inductively repeating Lemma J.8 at arrival time of each jump) so we omit the details here.
Corollary J.9. There exists some function ρe : (0, ∞) 7→ (0, ∞) such that the following claim hold
for all > 0, t > 0 and all sufficiently small η > 0: For any X, y ∈ G ) with kX - yk < and any
l ∈ [l*], on event Ao(e, η, δ, l, t) ∩ Bo(e, η, δ, l,t) ∩ Co(e, η, δ, l,t), we have
kYn(y)- Xn(x)k <e(t)e ∀n =1, 2,…，Tn(δ)
where
A水,η, δ, l,t)=n∀i=1,…，l, j=TiL")max ,t/ "lη %乙⑷+1+∙∙∙+Zj Ii ≤ 2;χ⅛M) o;
Bo(e,η,δ,l,t) = n∀j = 2, ∙∙∙ ,l,Tj (δ) - T-ι(δ) ≤ 2t∕η},
Co(e, η, δ, l, t) = {the line segment between Yj (y) and Xn (x) lies in G ∀j = 0,1,…，Tn(δ) 一 1}.
In particular, ρ = e(t) is a constant
The next result shows that the type of events A(n, η, , δ) defined in eq. (J.29) is indeed very likely to
occur, especially for small . For clarity of the presentation, we introduce the following definitions
that are slightly more general than the small and large jumps defined in eq. (J.24)eq. (J.25) (for any
c>0)
Z≤ = ZnI{kZnk ≤ c},
Z> = Znl{kZnk >C}.
Lemma J.10. Given any t > 0,	> 0, and N > 0, the following holds for any sufficiently small
δ > 0:
p( ImaXdt/ne η iZ≤"n + S/ »= o(ηN)
as η J 0.
109
Published as a conference paper at ICLR 2022
Proof. For any C ∈ (0, ∞), δ > 0, β ∈ (0, 1), consider the following decomposition of the small
noise Z≤"η. Here We adopt the convention that E0 = Rd∖(∪m=ιEj). (For definitions and properties
of the closed cones Ei,…，Em, see Assumption I.6 and the remark underneath.)
Znj) = ZnI{Zn ∈ Ej},
z≤C,(j)=Znj) IqZn"≤ c},
ZCj(n) = Znj) 1{加』∈ (C,δ∕η]}
Based on this decomposition, We define the folloWing iid random variable sequences
ze(j)(n) =∆ Zn≤C,(j) -EZn≤C,(j),
Zj) (n)=卜 C,,(jη (n)∣∣,
Zej) (n)= |小铲,叫.
Meanwhile, define the projection operators Pi : Rd → R with Pi(vι, ∙∙∙ , vd) = vi. First, recall that
EZ1 = 0. So for all C sufficiently large, We have
sup∣∣EZ≤y∣∣ = sup∣∣EZ>y∣∣ < 三.
y≥C	y≥C	2t
Moreover, due to Hj ∈ RV-&+i) for all j = 1, 2,…，m and Ho(x) = o(1∕xα) for any a > 0
(for definition of functions Hj, see eq. (I.5)), we know that E ∣∣Znj) ∣∣ < ∞ for any j = 0,1, ∙∙∙ ,m,
implying that for all C large enough, we have (for any j = 0,1,…，m)
sup ∣EZ1≤y,(j) - EZ1≤z,(j)
y,z≥C
1
------, 二
2t 24 √m+Γ
(J.30)
Fix such C. Besides, recall that in Assumption I.6 we have 1 < αι < α2 < ∙ < αm. Therefore, we
are able fix some β ∈ (0, 1) and δ > 0 so that
βα1 > 1,	(J.31)
(N +1)δ< 19 Z-,	(J.32)
12 m + 1
(C N , + 1) δ < —:——.	(J.33)
βαι — 1	12√ m + 1
With the fixed C and δ, note that eventually δ∕η>C as η J 0. Therefore, for all η sufficiently small,
p(n=i,maxdt∕ηe η ∣∣Z≤"η + …+ Z≤δT∣ >e)
≤P max η
'n=1,2,，dt∕ηe
n
X ZB/ - EZ≤“η	>f)
kk
k=1	∣
∣m n
—P(	rnov ∣ IXyX	Z≤C,(j)	EZ≤C,(j)	1	ZL(j)	k,∖	∣ EZ≤C,(j)	EZ≤δ∕5(j)
=P	max η ∣	Zk	- EZk	+ ZC,δ,η(k) + EZk	- EZk
∖n=1,2,…，dt∕ηe	JC iλ	, ,
∣ j=0 k=1
j=0
max
=1,2,…，dt∕ηe
n
X λ 7≤C,(j) _ ≤ 7≤C,(j) 1 πr(j)3) (l,∖ 1 ≤ 7≤C,(j) _ ≤ 7≤δ∕η,(j)
Zk - EZk + ZC,δ,η (k) + EZk - EZk
k=1
〉2√m + 1
m	∣n	∣
≤ X P( max η ∣∣X z(j)(k) >—:	]
j=0 1n=1,2,…,dt∕ηe	∣∣k=1	6√m + 1)
m	dt∕ηe
+ XP(η X Z(j)(k) > τ√⅛).
j=0 ' 公	6vm + ι)
> 2)
(J.34)
(J.35)
m
≤
η
110
Published as a conference paper at ICLR 2022
We bound the two terms eq. (J.34) and eq. (J.35) respectively. First, observe that
eq.
md
(J.34)≤XXP
j=0 i=1
max
n=1,2,…，dt∕ηe
n
η∣X Pi(e(j)(k))∣
k=1
6p(m + 1)d
>
Let e(j)(k) = Pi(ej)(k)) and let e(j) be an iid copy. By definition, |e(j)| ≤ 2C. Then from
Hoeffding’s inequality,
eq.(J.34) ≤ 2(m+1)d ∙exp (- 2 ∙( 6pd(⅛+i))2 ∙ 2c1d∕ηe)=o(ηN)
As for term eq.(J.35), w.l.o.g. let Us fix some j = 0,1,…，m and bound
dt∕ηe 〜
p(η X ZMn) > 6√⅛)	(J.36)
n=1
k-1	dt∕ηe 〜	dt∕ηe 〜
XP(η X Zj)⑹ > 6√⅛，I =i) +P(η X Zj)S) > 6√⅛，I ≥ k)
i=0	n=1	n=1
'-------------V---------------}	'--------------V--------------}
,(I)	,(II)
where I = #{n = 1,2, ∙∙∙ , ∖t∕η∖ : Zj)(n) > 1∕ηβ} and k is some positive integer such that

kδ <
i2√mπ,
k(βα1 - 1) > N.
Note that we can find such k due to our choice of constants in eq. (J.31)-eq. (J.33).
For (I) and any i = 0,1, ∙∙∙ , k 一 1, observe that (the non-negatιve RVs Ze (n) and Z(n) are defined
at the beginning of the proof)
(I) ≤	(∖t∕ηe) ∙	P(η d X	Z⑺(n)	>「，一 iη ∙ δ,	Z(j)(n) ≤ 1∕ηβ	∀n	∈	[∖t∕η∖ —	i])
∖ i )	'	n=l	6 √m + 1	η	/
dt∕ηe-i 〜
≤ ∖t∕η]i ∙ P( X Z(j)(n) >
n=1
dt∕ηe
≤∖t∕η]i∙ P( X Zej)(n) >
n=1
Let Wf(j)(n) =∆ Zeβ(j) (n)
w⅛+r ∙ η,Zj) (n) ≤1/ηβ ∀n ∈ [∖t∕ηe-i
12 √m+r η 广
- EZeβ(j) (n) and let Wf(j) be an iid copy. Due to eq. (J.30), for all η
sufficiently small we will have that ∖t∕η∖∣EZβj)(1)∣ <
small,
24√m+1 ∙ η. Therefore, for all η sufficiently
dt∕ηe
(I) ≤ ∖t∕η↑i ∙ P(| X fj)(n)l >
n=1
24√m∏ η>'
(J.37)

1


1
Observe that |Wf(j) | ≤ 2∕ηβ and
∞	1∕ηβ
E(Wf(j))2 = varZeβ(j) ≤E(Zeβ(j))2 =	2yP(Zeβ(j) >y)dy=	2yHj(y)dy.
00
Note that there are only two possibilities:
•	If j 6= 0 and αj ≤ 2, then from Karamata’s theorem, we have E(Zeβ(j))2 ∈ RV-β(2-αj) (η)
and note that β(2 一 αj) < 1;
•	Otherwise, one can find some ∆ > 0 such that Hj (x) = o(1∕x2+∆), implying that
E(Zeβ(j))2 ≤ R0∞ 2yHj(y)dy < ∞ regardless of the actual value ofη > 0.
111
Published as a conference paper at ICLR 2022
In any case, using Bernstein’s inequality, we can conclude that
eq. (J.37) ≤ dt∕η]i ∙ 2exp ( —
dt∕ηeE(f ⑺)2 + 3 (12√m+1 )∕η1-β
o(ηN).
On the other hand,
(II) ≤ P(I ≥ k) ≤ (dtkηe) ∙P0)(n) > 1∕ηβ∀n = 1,...,k) ≤ dt∕η↑k ∙ (Hj(1∕ηβ))：
which is upper bounded by a function regularly varying w.r.t. η with index k βα1 — 1 > N .
Therefore, (II) = o(ηN) and We conclude the proof.	□
J.3 Analyzing different scenarios before the first exit or return
Using results and arguments above, We are able to illustrate the typical behavior of the SGD iterates
Xnη in the folloWing tWo scenarios. First, We shoW that, When starting from anyWhere in G, the SGD
iterates Xnη Will most likely return to the neighborhood of the 0 Within a short period of time Without
exiting Ω.
Lemma J.11. For any E ∈ (0, e), the following claim holds:
lim inf Px(Xnη ∈ G ∀n ≤ Treturn(η, )and Treturn (η, ) ≤ ρ()∕η = 1
ηψ0 x:d(x,Gc)〉e	∖	)
where the stopping time involved is defined as
Treturn(η, E) =∆ min{n ≥ 0 : kXnηk < 2E}
and the function ρ(E) = c1 + c1 log(2∕E) with c1 ∈ (0, ∞) being the constant specified in Lemma
J.4.
Proof. Let t = ρ(E) = c1 + c1 log(2∕E) and n = dt∕ηe and arbitrarily choose some x With
d(x, Gc) > E. Also, fix some N > 0. Due to Lemma J.10, We are able to pick some δ > 0 such that
(note that ηn ≤ 2t eventually as η J 0)
p( max η ∣∣Z≤δ" +------------+ Z≤δ"∣∣ > ɔ----7∣-7-τγ) = o(ηN)
∖n=ι,2,…，dt∕ηe U	Il	2exp(2ηnM )7
First, from Lemma J.5 We knoW that for any η small enough such that
2ηM exp(2tM) < E∕2,
We must have
IIIxη(s,x) — ybηsc(x)III ≤ E∕2 ∀s ∈ [0, n].
Next, using Lemma J.8, We see that on event
Amn 2^p⅛My,δ) ∖{Tη(δ) >「〃〃口，
We must have
kyη(x) — Xn(x)k <E ∀k = 0,1,∙∙∙, dt∕ne.
Combining these facts with the Lemma J.4, we have shown that, for all n ∈ (0,4meχp(2tM)),
on event A(n,n, 2exp(nnM) ,δ) T{Tn(δ) > dt∕ne} We must have Xn ∈ G ∀n ≤
Treturn(η,E) and Treturn(η, E) ≤ ρ(E)∕η. In other words,
lim sup	Px Xnη ∈∕ G for some n ≤ Treturn (η, E) OR Treturn(η, E) > ρ(E)∕η
n，0 x:d(x,Gc)>e	'	)
≤limP(AS,n,5—:	δ)c) +limP(Tn(δ) ≤ dt/ne).
nψo	2exp(nnM)	nψo
112
Published as a conference paper at ICLR 2022
However, our choice of δ at the beginning of the proof implies	that
limηψo P(A(n, η, 2eχp(ηnM), δ)c) = 0. Meanwhile, from Assumption I.6, We see that
「t/n]	m	m
P(T1n(δ) ≤ dt∕η]) = X (1 - XHj(δ∕η))k-1 ∙ (XHj(δ∕η))
k=1	j=0	j=0
dt/ne
≤(m +1)Hι(δ∕η) X(1 — Hι(δ∕η))k-1
k=1
for all η sufficiently small (due to regularly varying nature of Hj and 1 < α1 < •…< αm,). In
particular, since H1 ∈ RV-α1, for any fixed ∆ ∈ (0, α1 - 1), we have (for all η sufficiently small),
dt/ne	+ 1
P(Tn(δ) ≤ dt∕η]) ≤(m + 1)ηαT £(1^a1+^-1 = mɪP(U⑺ ≤「力加)
k=1	η
where U(η) =d Geom(ηα1+∆). Lemma G.4 then tells us that, for all η sufficiently small, P(U (η) ≤
dt∕η]) ≤ tηa1-1+δ, thus implying
limsupP(Tf(δ) ≤ d~t∕η~∖) ≤ limsup Mm + 0ηa1-1-∆ = 0
nψo	nψo	t
and concluding the proof.	□
In the next result, we show that, once entering a -small neighborhood of the local minimum, the
SGD iterates will most likely stay there until the next large jump.
Lemma J.12. Given N > 0, the following claim holds for all E ∈ (0, e∕3) and all δ > 0 that is
sufficiently small:
sup P∃n < T1n(δ) s.t. kXnn(x)k > 3E = o(ηN0)
x∈B(0,2)
as η J 0.
Proof. Let t = c1 + c1 log(1∕E) where c1 ∈ (0, ∞) is the constant specified in Lemma J.4, and
n = dt∕ηe. Fix some β > 1 + α1 and N > β - 1 + N0. Due to Lemma J.10, the following claim
holds for all δ that is sufficiently small:
P( max	η ∣∣Z≤δ" +------+ Z"δ"
∖k=1,2,…，dt/ne Il 1	k
e∕2 ʌ
2exp(2ηnM) J
= o(ηN)
>
We fix one of such δ. Therefore, for event
A=n∃j ≤d⅛τ⅞τe s∙t∙
k=ι,maxdt∕ne η Il Z1+(j-1) dt/ne + Z2+j-1)「t/ne + -+ Z≤ 'll > 2exp(2ηnM) 0,
the iid nature of noises (Zn)n≥1 implies that
P(A) ≤(1 + -1τ)P(	max	η∣∣Z≤"n + …+ Z≤"n∣∣ > ---------^2——7
tηβ-1	1 k=1,2,…，「t/ne 'll	k H 2exp(2ηnM)
=o(ηN-β+1) as η J 0.
Meanwhile, Note that	Tn(δ)	≤	Tf,(1)(δ)	= min{n	≥ 0 :	η∣Zn∣	> δ,	Zn	∈ E1}.	Therefore,
P(T1n(δ) > 1∕ηβ) ≤ P(T1n,(1) (δ) > 1∕ηβ). Specifically, note that T1n(δ) =d Geom(H1(δ∕η)) and
H1 ∈ RV-α1 . Lemma G.3 then tells us the existence of some c > 0 such that
P(Tn(δ) > 1∕ηβ) = o(exp(-c∕η))
113
Published as a conference paper at ICLR 2022
as η J 0. In summary, We have established that
P(N∪{τf(δ) > ι∕ηβ}) = o(ηN0).
η
NoW let us focus on the complementary event (A)c ∩ {T1η(δ) ≤ 1∕ηβ}. On this event, there must be
some j ≤ d 1/ne e SUCh that 1 + (j - 1)dt∕η∖ ≤ Tη (δ) ≤ j dt∕η∖.
Let us arbitrarily choose some x ∈ B(0, 2). If j = 1, then due to eq. (J.3), We knoW that
xη (s, x) ∈ B(0, 2) for all s ≥ 0. Also, from Lemma J.5 We knoW that for any η small enough suCh
that 2ηM exp(2tM) < ∕2, We must have
xη(s,x) - ybηsc(x) ≤ ∕2 ∀s ∈ [0, dt∕η∖].
η
Besides, on event (A)c ∪ {T1η(δ) ≤ 1∕ηβ} ∩ {j = 1}, due to Lemma J.8, We must have
kyη(x)-Xn(x)k<"2 ∀k = 0,1,…，T1 (δ)
With T1η(δ) < dt∕η∖. In ConClusion, in the Case that j = 1, We must have kXnη(x)k < 3 for all
n < T1η(δ).
NoW Consider the Case With j ≥ 2. Similarly, due to Lemma J.5 and J.8, We noW have that
xη(s,x) - ybηsc(x) ≤ ∕2 ∀s ∈ [0, dt∕η∖],
kyn(x)-χn (x)k <e∕2 ∀k = 0,1,…，「t/n],
kxη(s,x)k <2 ∀s∈ [0,dt∕η∖].
In partiCular, due to our ChoiCe of t at the beginning of the proof and Lemma J.4, one Can see that
kxη(dt∕η∖, x)k < . ColleCting all the results, We noW knoW that
kXn(x)k< 3e ∀k ≤[t∕n∖, ∣XPt∕n1(x)∣ < 2e.
NoW it suffiCes to apply the repeated apply the same arguments above. For instanCe, if j = 2, then
by letting x1 = Xdηt∕ηe(x) and bounding the gap betWeen trajeCtory of Xbηsc+dt∕ηe(x) and xη(s, x1 )
using Lemma J.5 and J.8, one Can shoW that kXnη(x)k < 3 for all n < T1η(δ) (sinCe in the Case of
j = 2, We have T1η(δ) < 2dt∕η∖). OtherWise, We have j ≥ 3 and the same arguments above Can be
applied to shoW that
kXkη(x)k < 3∀k ≤ 2dt∕η∖,	Xdηt∕ηe(x) < 2,,	X2ηdt∕ηe(x) < 2.
In summary, by proceeding inductively, we establish that for all n ∈ (0,4m eχktM)),
sup P∃n < T1η(δ) s.t. kXnη(x)k > 3	≤P(Ae∪{T1η(δ) > 1∕ηβ})
x∈B(0,2)
and this concludes the proof.	□
We introduce a few concepts that will be crucial to the analysis below. Recall the definition of ODE
with jumps xe (note that we will drop the notational dependency on learning rate η when we choose
n = 1). Recall the definitions of i*, k (see Assumption I.7 and I.8 and the remarks underneath).
We define the following mapping h from r = (r1,…，rk*), θ = (θι,…，θk*) ∈ (Sd-I)k* ,t =
(tι,…，tk*) ∈ {0} X R+* such that
k*
h(r, θ, t) = xe(X ti, 0; t, w)	(J.38)
i=1
where w = (wi)ik=*1 with wi = riθi. From the continuity of the ODE flow, we see that h is a
continuous mapping. Also, we introduce the concept of type for configuration i*. Specifically, define
j(iι,…，im) =	{(jι,…，jk*) ∈{0, 1, 2,…，m}k*	: #{n :	jn	= k} =	ik ∀k	∈	[m]}	(J.39)
114
Published as a conference paper at ICLR 2022
and for any j ∈ j(i*), we say that W = (wι,…，wk*) ∈ A(i*) has type j if
wi ∈ Eji ∀i ∈ [k*].
In other words, based on the direction of each jump in W ∈ A(i*) we group them into different types.
Note that |j(i*)| < ∞. Now for any type j ∈ j(i*), define a (Borel) measure on Rk+* × (Sd-1)k* ×
Rk+*-1 as
k*
k*
μj = (∏ Vaji) × (∏ Sji)× Lebk*-1
(J.40)
i=1
i=1
where, for any α > 0, the measure Va is the Borel measure on (0, ∞) with Vα(χ, ∞) = 1∕χα. We
use the concepts above to characterize behavior of large noises in (Zn)n≥1.
Definition J.1. For any η,δ > 0, j ∈ j(i*) and any Borel Set A ⊆ Rd, we say noise Zn is of
type-(A,δ,η,j) iff
•	η kZnk > δ;
I ` V
• For t1
∆
n and t = mm{k > ti-ι : η ∣∣Zk ∣∣ > δ} for all i = 2, 3,…，k, it holds that
ei 一 ei-ι < 2±∕η forall i = 2, 3, ∙∙∙ , k;
•	For ti =∆ eti - tei-1 (with t1 = 0), wi = ηZte , and
r = (IwIk ,kw2k , ∙∙∙ , kwk* k),
θ =(wl∕ kw1k ,…，wk* / kwk* k),
t = (t1, ∙∙∙ ,tk* ),
it holds that h(r, θ, t) ∈ A.
More generally, we say noise Zn is of type- (A, δ, η) iff there exists j ∈ j (i*) such that Zn is of
type-(A,δ,η,j).
Due to the iid nature of (Zj)j≥1, let us consider an iid sequence (Vj)j≥0 where the sequence has
the same law of Z1. Note that for any fixed n ≥ 1, the probability that Zn is of type-(A, δ, η, j) is
equal to the probability that V0 is of type-(A, δ, η, j). More specifically, let H(x) = Pjm=0 Hj (x) =
P(kZ1k > x). Due to Assumption I.6, H ∈ RV-a1 . Besides, P(η kV0k > δ) = H(δ∕η). Now we
focus on conditional probability admitting the following form:
p(A,δ,η,j) = PV0 is of type-(A, δ,η,j) η kV0k > δ.
Lemma J.13. For any δ > 0, any j ∈ j(i*), and any Borel set A ⊆ Rd such that
h(rι, ∙∙∙ ,rk* ,θι,…，θk* ,t2,…，tk*) ∈ A =⇒ t < 2t ∀i = 2,…，k* and r > δ ∀i ∈ [k*],
(J.41)
it holds that
μj (h-1 (A。))≤ liminf
p(A, δ, η, j)
δαι ∙
Qm=I (Hk(i∕η))ik
H(ι∕η)ηk*-1
≤ limηsup δaW ≤ μ (h-1(A)).
δ ∙	H(1∕η)ηk*-1
Proof. Let us start by fixing some notations. Let T1 = 0, and define stopping times Tj = min{n >
Tj-1 : η kVnk > δ} and inter-arrival times Tj0 = Tj 一 Tj-1 for any j ≥ 2 (with T10 = 0), and large
jump Wj = VTj for any j ≥ 0. Note that: first, the pair (Ti0, Wi) is independent of (Tj0, Wj) for any
i 6= j; besides, Wj and Tj0 are independent for all j ≥ 1.
Define the following sequence (of random elements) W = (wi,…,wι*) and t = (tι, ∙∙∙ ,tι*) by
wi = ηWi , ti = ηTi0 .
and r = (Tr(wi))ik=* 1, θ = (Tθ(wi))ik=* 1 where operators Tr(x) = kxk , Tθ(x) = x∕ kxk constitute
the polar coordinate transform. By definition of type-(A, δ, η, j), we know that
115
Published as a conference paper at ICLR 2022
•	Ti ≤ 2t∕η for any i = 2,…，k*;
•	η ∣∣Wik > δ for any i = 1, 2, ∙∙∙ ,k*;
•	Wi ∈ Eji (i.e. θi ∈ Fi) for all i ∈ [k*];
•	h(r, θ, t) ∈ A.
Therefore, (let Ri =Tr(Wi),Θi =Tθ(Wi))
p(A, δ, η, j)
= P(T20 ≤
∕l{(r,θ,t) ∈ h-1(A)}
k*	k*
• (Y P(Wi ∈ Ei) ∙ P(ηRi = dri, E = d%)) ∙ (Y P(ηTi, = dti | ηTi ≤ 2玲
i=1	i=2
= (P(T2 ≤ 2t∕η))k*τ . [YP(Wi ∈ Ei)] ∙Qn,δ,j(h-1(A))
i=1
(J.42)
where Qη,δ,j is the probability measure on Rk+* × (Sd-1)k* × Rk+*-1 induced by a sequence of
random elements (ηR1, • • • , ηRk* , Θ1, • • • , Θk* , ηT2↑(η), • • • , ηTk↑* (η)) such that
•	For any i = 1, .•• ,k*, we have Ri = Tr(Wi↑(η)), Θi = Tθ (Wi↑(η)) where the distribution
of Wi↑(η) follows the law P(V0 ∈ • η ∣V0∣ > δ, V0 ∈ Eji ; Besides, (Wi↑(η))ik=* 1 is an
independent sequence;
•	For any i = 2, ••• ,l*,the distribution of T↑(η) follows from P (ηTδ (η) ∈ • ∣ ηTδ (η) ≤ 2£)
(for the definition of stopping times Tkδ(η), see eq. (J.26)); Besides, (Ti↑(η))ik=* 1 is an
independent sequence, and it is independent of (Wi↑(η))ik=* 1;
•	Qη,δ,j (•) = P((ηR1, • • • ,ηRk*, Θ1, • • • , Θk*, ηT2↑(η), • • • , ηTk↑* (η)) ∈ •.
As for weak convergence of (ηR1, • • • , ηRk* , Θ1, • • • , Θk* , ηT2↑(η), • • • , ηTk↑* (η)), observe that
•	For any i ∈ [k*], from Assumption I.6 one can see the regularly varying nature of distribution
of V0 on the cone Eji (hence for W↑(η) as well), thus yielding that (ηRi, Θi) ⇒ (R*, Θ*)
as η J 0 where R* and Θ* are independent, the law of Θ* is Sji, and the law of R* is the
Pareto distribution with
δαji
P(Ri >X)=(X ∨ δ)j ;
•	For any X ∈ [0, 2t], since limnj0 [x∕η]H(δ∕η) = 0, it is easy to show that
1-(1- H(δ∕η)"c
lι^∩ ----------------- 1；
nm	bx∕ηCH (δ∕η)	;
therefore, we have (for any X ∈ (0,2t])
P(ηTδ (η) ≤ x | ηTδ (η) ≤ 2t)
1 - (1 - H(δ∕η))bx/nc
1 - (1 - H(δ∕η))b2j∕nc
X
→百
as η J 0, which implies that T↑ (η) converges weakly to a uniform RV on [0, 2t].
116
Published as a conference paper at ICLR 2022
Together with the assumption on set A in eq. (J.41), we can now see that, if we denote the weak limit
of measure Q%δ,j as μδ,j, then (the measure μj is defined in eq. (J.40))
From the continuity of mapping h, one can see that h-1(A) is a closed set and h-1(A°) is an open
set. Using Portmanteau theorem, we now have
Qk*. δαji ，一 一 `	，，一`	，，一`
(；=：*-1 μj (h 1 (A )) ≤ liminf Qη,δ,j (h 1(A)) ≤ limsuPQη,δ,j (h I(A))
(2t)	η	η
≤『μj (h-1(A)).
(2t)
(J.43)
(J.44)
Moving on, We analyze the limit of the other terms in eq. (J.42). For any i ∈ [kj], note that
P(Wi ∈ Ei) = Hii”.
H(δ∕η)
NoW due to the regularly varying nature of Hj and H,
lim
η
Qk=1P(Wi ∈ Ei)
δk*α1
Qk: ι Hji(i∕η)∕H(i∕η「 Qk=1 δα% .
(J.45)
On the other hand, from Lemma G.4 and the regularly varying nature of function H, for any fixed
κ > 1, We have
lim sup
ηψo
lim inf
ηψo
P(T2 ≤ 2〃n)、k
P(T2 ≤ 2t∕η) ʌk
-	1 ≥ (1∕κ)k
= (1∕κ)k
Due to arbitrariness of κ > 1, We yield that
-	1 lim inf
ηψo
-	1
≤ κk*-1 lim sup
ηψo
2 讴(δ∕η)∕η y*-1= k*-1
H(i∕η)∕η)	=
2tH (δ∕η)∕η ∖ k*-1
H(i∕η)∕η J
lim
η
(P(TI ≤ 2"η))k*-1
k*-1 = 1.
H(1/n)
(J.46)
*
*
*
*
η	δα1
Collecting all the limits in eq. (J.44)-eq. (J.46) and plugging them into eq. (J.42), We noW have
established that
μj (h-1(A。))≤ liminf
p(A, δ, η, j)
δαι ∙
Qk=I Hji(1∕η)
H (ι∕η)ηk*-1
≤ lim sup
η
p(A, δ, η, j)	≤
δαι Qk=I Hj人而 ≤
H(1∕η)ηk*-1
To conclude the proof, recall that for any type j ∈ j(i*), We have #{k = 1, 2, ∙∙∙ ,k* : jk = j}=
ij (i.e. for any j ∈ [m], the number of elements in (j1,…，jk*) that are equal to j is exactly ij),
**
thus Qk=1 Hji(I∕η) = Qk=1 (Hk (i∕η))ik.
□
More generally, for
p(A, δ, η) = PV0 is of type-(A, δ,η) η kV0k > δ,	(J.47)
we know that p(A,δ,η) = Pj∈j(i*) p(A,δ,η, j). Also, define measure μ as
μ = X μj.	(J.48)
j∈j(i*)
The next result follows immediately from Lemma J.13 and the fact that |j(ij)| < ∞. Recall that JG
is defined in eq. (I.9).
117
Published as a conference paper at ICLR 2022
Corollary J.14. There exists a function λ(η) ∈ RV1+JG -α1 (η) such that, given any δ > 0 and any
Borel set A ⊆ Rd with
h(rι,…，rk*, θι,…，θk*,t2,…，tk*) ∈ A =⇒ t < 2t ∀i = 2,…，k* and r > δ ∀i ∈ [k*],
(J.49)
it holds that
〃(h-1(A。))≤ liminf P(A，") ≤ limsup p(A，δ,η) ≤ μ(h-1(A))
、	η η η	δɑι ∙ λ(η) 一 η	δɑι ∙ λ(η)一、	,
In particular, the regularly varying function λ(∙) admits the form
eλ(η)
*
Qm=ι (Hj(i∕η))j
H (i∕η)ηk*-1
(J.50)
Consider the following stopping times
σ(η) = min{n ≥ 0 : Xnη ∈∕ G};
R(e,δ,η)=min{n ≥ Tn(δ): IlXnk ≤ 3e}.
Here σ indicate the first time that the iterates exit domain G, while R denotes the time the SGD
iterates return to a small neighborhood of the attractor 0 after the first large jump. In the next few
results, we study the probability of the different scenarios regarding the first exit time σ(η) and first
return time R(e, δ, η). To this end, let
Kn(δ) = max{k ≥ 0: Tk(δ) ≤ σ(η) ∧ R(e, δ,η)}
be the count of large jumps before the first exit or first return. Furthermore, we introduce the following
concept as the accumulated cost of large jumps before the first exit or return. Let J0η,δ = 0 and
Jkη,δ =∆ Jkη-,δ1+J(Wkη	(δ)) ∀k≤Kη	(δ)
where the cost function J(∙) is defined in eq. (I.6). In other words, Jy indicates the total cost of the
first k large jumps, if there are at least k large jumps, before the first exit or first return. Similarly, we
can also define a step-wise accumulated cost as
J,δ(n) = max{Jk,δ ： Tk(δ) ≤ n}∀n ≤ σ(η) ∧ R(gδ,n).
In other words, J^j δ (n) evaluates the total cost of large jumps up until step n. As a preparation
for our analyses below, We first discuss the following technical tools. For any e > 0, let i(e)=
c1 + c1 log(1∕) where c1 is the constant in Lemma J.4. Besides, define
e(e) = -----X— ∧「一 ∧ ——
4exp(2tM)	p(t(e))	ρ(t(e))
(J.51)
where functions ρ(∙),p(∙) are defined in Corollary J.6 and J.9 respectively. Furthermore, define an
event A× = A× (e, δ, η) ∪ A× (e, δ, η) where
A×(e,δ,η) = n∃n < Tn(δ) s.t. ∣∣Xn(x)k > 3，，	(J.52)
A× (e,δ,η)=n∃j=2，∙∙∙，l* s.t.
max	ʌ	η IIZTη ](δ)+1 + …+ ZTη 1(δ)+jl∣ > E(E) )
k = 1,2,…，(T,(δ)-Tj-ι(δ)-1)∧(2t(e)∕n) ∣∣	j-1l	j-1	∣∣	J
(J.53)
where the positive integer l is defined in eq. (J.1). By definition of l*, we must have Kη (δ) < l*.
Lastly, define events (for any K ∈ [l*])
B1×(K) =∆ (A×)c ∩ {JKk,δ < JG， K ≤Kk(δ)}
∩ {∀j ∈ [K]，[Tη+ι(δ) ∧ σ(η) ∧ R(e，6, η)] - Tjn(δ) ≤ 午}，
B2×(K) =∆ (A×)c ∩ {JKk,δ ≤ JG， K≤Kk(δ)}
∩ {∃j ∈ [K] s.t. [T+ι(δ) ∧ σ(η) ∧ R(e，6, η)] - Tj(δ) > 等}.
118
Published as a conference paper at ICLR 2022
Recall that the minimum cost for exit JG is defined in eq. (I.9). Note that, in the definition of B1× (K)
andB2×(K) above, Tjη+1(δ) ∧ σ(η) ∧R(,δ,η) < Tjη+1(δ) only if j = K = Kη(δ).
Lemma J.15. For any K ∈ [l*], any E ∈ (0, e/3) and any η > 0 sufficiently small, the following
claim holds on event B1× (K):
sup
s∈[0,TKη +1(δ)∧σ(η)∧R(,δ,η)-T1η(δ)]
IIXnsc+τη(δ)(χ)-eηGX⑴(χ); T,W)∣∣
< 2E
where T =	(。/n⑷-Tln(δ),T3n(δ) - Tg(δ),…，僚 +1(δ) - TK(δ)), W
(Wf(δ), W2n(δ),…，WK+ι(δ)), and X (I)(X) = XTη (δ)-1 (x).
Proof. We focus on the distances between the following three objects: Xnη (x),
■~■
Yenη(x)
Xnη(x)
1Y-1(χ) + 心 ηVf (Y-ι(χ)) + Pj≥ι i{n = Tn (δ)}ηZn)
if n < T1η(δ);
otherwise;
and
z(s) = en (s,X(I)(x); T, W) ∀s ≥ 0.
First of all, for X(1) = XTηη(δ)-1(x) = YeTηη(δ)-1(x), by definition of event (A× )c we have
∣∣X(I)II < 3e < e. Then due to eq. (J.20) and the definition of event B;(K) (in particular,
the fact that JKη,δ < JG), we know that
d(z(s), Gc) > 100l睦 ∀s ∈ [0,TK +ι(δ) ∧ σ(η) ∧ R(e, δ, η) — Tn(δ)).
In light of the definition of B1× (K) (i.e. the upper bound on Tjη+1(δ) - Tjη(δ)), by applying Corollary
J.6, we can show that
sup
s∈[0,TKη +1(δ)∧σ(n)∧R(,δ,n)-T1η(δ)]
IIz(s) - Ybnsc+T1η(δ) (x)II
< E.
In particular, by applying Corollary J.6 inductively for all n ≤ TKn +1(δ) ∧ σ(η) ∧ R(E, δ, η) - T1n(δ),
nn
we know that the line segment between z(s) and Ybnsc+T η(δ)(x) is in G for all s < TKn +1(δ) ∧ σ(η) ∧
R(E, δ, η) - T1n(δ) so Corollary J.6 can be further applied to n + 1 and establish the inequality above.
Similarly, due to our choice of ^(e) in eq. (J.51) and the upper bound on Tj+1(δ) 一 Tj (δ) in definition
of event B1× (K), by applying Corollary J.9 inductively, we know that for all η sufficiently small,
η sup	η IIIYbnsc+T1η(δ) (x) - Xbnsc+T1η(δ) (x)III < E
j=0,ι,2,∙∙∙ ,τK+ι(δ)∧σ(n)∧R(e,δ,n)-τ/(δ)
holds on event B× (K) and this concludes the proof.	□
Lemma J.16. For any K ∈ [l*], any e > 0 and any η > 0 sufficiently small,
B×(K ) = 0.
Proof. The definition of event B× (K) ensures that We can define some j* as the smallest integer in
[Kn(δ)] such that for T* = Tn (δ) + d2t(e)∕η∖, we have
Tn, (δ) <T * <Tn* + ι(δ) ∧ σ(η) ∧ R(e, δ,η).	(J.54)
Analogous to the proof of the previous lemma, we focus on the pair-wise distances between the
following three objects: Xnn (x),
■~■
Yenn(x)
Xnn(x)
1Y-1(x) + 心 ηVf (Y-1(x)) + Pj≥ι l{n = Tn(δ)}ηZn)
ifn < T1n(δ);
otherwise;
119
Published as a conference paper at ICLR 2022
and
Z(S) = eη (s,X⑴；T, W) ∀s ≥ 0.
with T =	(0,T2η(δ)	-	Tn(δ),T3(δ)	-	T2η(δ),…，Tj*+ι(δ)	-	Tj*(δ)),	W =
(Wn(δ), W2n(δ),…，Wjt+1(δ)), and X⑴ =XT〃⑷.Again, using Corollary J.6 and J.9,
one can see that for all η > 0 sufficiently small, we must have
sup
s∈[0,T *-Tη(δ)]
Xbnsc+T1η(δ) (x) - z(S)
< 2
on event B× (K). However, eq.(J.20)and ∣∣X⑴ ∣∣ < 3e < e implies that
sup	d(z(s), GC) > 100l*<≡.
s∈[0,Tjη* (δ)-T1η (δ)]
In the meantime, Lemma J.4 and our choice of e() in eq. (J.51) implies that
kz(T* - Tn(δ))k <e and IlXT*(χ)k < 3g
thus dictating that R(e, δ, η) < T* and contradicting eq.(J.54). This concludes the proof. □
Now we are ready to apply the tools above and analyze some atypical scenarios regrading the first
exit and first return time. In the next result we show that, when starting from the local minimum, it is
very unlikely to escape if the total cost of large jumps is less than JG .
Lemma J.17. For any fixed E ∈ (0, e/3), N > 0, thefollowing claim holdsfor any sufficiently small
δ > 0:
sup	Pχ(σ(η) < R(e,δ,η), Jnl δ(σ(η)) < JG) = o(ηN) as η J 0.
x: kxk≤2	,
Proof. Since the constant e > 0 is fixed, one can see that t = ci + ci log(1∕e) is also fixed where ci
is the constant in Lemma J.4. Besides, we can fix some eE > 0 such that
e< 4exp(2^M) H p(t)e<e
where functions ρ(∙), p(∙) are defined in Corollary J.6 and J.9 respectively.
Let us define an event A× = Ai× ∪ A2× where
Ai× =∆ n∃n<Tin(δ)S.t. IXnn(x)I > 3Eo,
A× = n∃j = 2, ∙∙∙ , l* s.t.
k=1,2,∙∙∙ ,(Tjn (δ)mTjxι(δ)-1)∧(2^∕n) η HZ%”1 + …+ ZTjL")+j ll > eθ
where the positive integer l* is defined in eq. (J.1). By definition of l*, we must have Kn(δ) < l* on
event {Jnl,δ (σ(η)) < JG}.
Now let us analyze the dynamics of Xnn on event (A× )c ∩ {Jnl,δ (σ(η)) < JG}. In particular, we
decompose it into two events
Bi=∆(A×)c∩{Jnl,δ(σ(η)) <JG}
∩{∀j = 1, 2,…，Kn (δ), [Tj+i(δ) ∧ σ(η)∧ R(e, δ,η)] - Tj (δ) ≤ 2t∕η},
B2=∆(A×)c∩{Jnl,δ(σ(η)) <JG}
∩{∃j = 1, 2,…，Kn (δ) s.t. [Tjn+ι(δ) ∧ σ(η) ∧ R(e,δ,η)] - Tj (δ) > 2t∕η}.
Using Lemma J.15 and the fact that B1 = (UK =1 B× (K) ∩ {Kn(δ) = K}) ∩ {北η⑹ < JG},
one can see that for any η > 0 sufficiently small, we must have
d(Xrn(n)∧R(e,δ,n)(x), Gc) > 100l*E - 2e> 100l*e - E > 0.
120
Published as a conference paper at ICLR 2022
on event B1 . In other words, on event B1 we must have R(, δ, η) < σ(η).
On the other hand, Lemma J.16 the fact that B? ⊆ UK=1 B× (K) ∩ {Kη(δ) = K} implies that
B2 = 0 whenever η > 0 is sufficiently small.
In summary, we have established that
SUp Pχ(σ(η) <R(e,η), J,δ(σ(η)) < Jg) ≤ SUp	P(A×).
x: kxk≤2	x: kxk≤2
Lastly, from Lemma J.10 and J.12, one can see that for any δ > 0 that is sufficiently small,
SUp	P(A× ) = o(ηN),
x: kxk≤2
and this concludes the proof.	□
Using an almost identical approach, we can establish the next two results and conclude that it is also
rather unlikely to have scenarios where
•	The accumulated cost of large jumps exceeds JG before the first return or exit,
•	Or the first return occurs before the first exit,
yet it takes rather long for the said event to occur.
Lemma J.18. Given E ∈ (0, e/3), N > 0, thefollowing claim holds for any sufficiently small δ > 0:
SUp	Pχ(∃K ∈ N s.t. JK ≥ JG and Tn (δ) — Tj-1(δ) > 2^(e)∕η for some j = 2,3,…，K
x∈B(0,2e)	、
= o(ηN)
as η J 0 where t(e) = ci + ci log(1∕e).
Proof. Given the fixed e > 0, define t = ci + ci log(1∕e). Also, fix e > 0 as the largest possible
value such that
e ≤------‰—, ρ(t)e ≤ e, ρe(t)e ≤ e
4 4exp(2tM) 八' 八'一
where functions ρ(∙), p(∙) are defined in Corollary J.6 and J.9 respectively.
Let us define an event A× = Ai× ∪ A2× where
Ai× =∆ n∃n<Tiη(δ)s.t. kXnη(x)k > 3Eo,
A× = n∃j = 2, ∙∙∙ , l* s.t.
max	ʌ η IIZTη ](δ)+i + …+ ZTη 1(δ)+jl∣ > e f
k=i,2,…,(T (δ)-τj-ι(δ)-i)∧(2t∕n) Il	j-1v	j-1	Il J
where the positive integer l* is defined in eq. (J.1). Furthermore, define event
2^
B = (A×)c ∩ {∃K ∈ N s.t. JKδ ≥ JG and Tj(δ) — Tn-ι(δ) > — for some j = 2,3,…，K)∙
Note that on event B, the index
K * = max{k ≥ 1: JF < Jg ∀j = 1, 2,…，k — 1}
is well defined with 1 ≤ K* < l* (due to the definition of l* ). As a consequence, we must have
TK* (S) ≤ σ(η) ∧ R(E,δ,η)∙
Now based on the exact value of K*, We can decompose the event B into B = ∪K=CIB(K) where
2^
B(K) = (A×)c ∩{K* - 1 = K} ∩ {JKK,δ < JG} ∩ {Tn+i(δ) - Tn(δ) > 万 for some j ∈ [K]}.
121
Published as a conference paper at ICLR 2022
By applying Lemma J.16, We know that for all η > 0 sufficiently small, B(K) = 0. Therefore, for
all sufficiently small η,
SUp Pχ(∃K ∈ N s.t. JK ≥ JG and Tj (δ) — Tj-1(δ) > 2^(e)∕η for some j = 2,3,…，K)
x∈B(0,2e)	、
≤ SUp Pχ(A×).
x∈B(0,2e)
Lastly, from Lemma J.10 and J.12, one can see that for any δ > 0 that is sufficiently small,
SUp P(A× ) = o(ηN),
x: kxk≤2
and this concludes the proof.	□
Lemma J.19. Given E ∈ (0, e/3), N > 0, thefollowing claim holds for any sufficiently small δ > 0:
SUp Px(R(e, δ,η) < σ(η), JKδ(S) < JG,
x∈B(0,2e)	、
Tj+ι(δ) ∧ R(e, δ, η) 一 Tj(δ) > "(')for some j ∈ [Kη(δ)]) = o(ηN) as η J 0
where t(e) = ci + ci log(1∕e).
Proof. Given the fixed e > 0, define t = ci + ci log(1∕e). Also, fix e > 0 as the largest possible
value such that
e ≤-------‰-----, ρ(t)e ≤ e, ρe(t)e ≤ e
4 4exp(2tM) 八' 八'一
where functions ρ(∙), ρe(∙) are defined in Corollary J.6 and J.9 respectively.
Let us define an event A× = Ai× ∪ A2× where
Ai× =∆ n∃n<Tiη(δ)s.t. kXnη(x)k > 3Eo,
A× = n∃j = 2, ∙∙∙ , l* s.t.
maχ	η IIZτn (δ)+i +	+ ZTn 1(δ)+jll > ef
k = i,2,…，(Tjn(δ)-Tj-ι(δ)-i)∧(2t∕η) Il j-1v	j-1v Il J
where the positive integer l* is defined in eq. (J.1). By definition of l*, We must have Kη (δ) < l* on
event {JKη,ηδ(δ) < JG }. Furthermore, define event
B =∆ (A×)c ∩ nR(E, δ, η) < σ(η), JKη,ηδ(δ) < JG,
Tη+i(δ) ∧ R(e, δ,η) - Tj(δ) > 午 for some j ∈ [Kη(δ)]}.
Now observe that B ⊆ UK=1 B (K) where
B(K) =∆(A×)c ∩ {Kη(δ) = K} ∩ {JKη,δ < JG}
口	口	2^
∩ {∃j ∈ [Kη(δ)] s.t. [Tj+ι(δ) ∧ σ(η) ∧ R(e, δ, η)] 一 Tj(δ) > -}.
By applying Lemma J.16, we know that for all η > 0 sufficiently small, B(K) = 0. Therefore, for
all sufficiently small η,
SUp Px (R(E,δ,η) < σ(η), JK,δ(δ) < JG,
x∈B(0,2e)	、
Tη+i(δ) ∧ R(E,δ,η)- Tj(δ) > 也 for some j ∈ [Kη(δ)]) ≤ sup	Px(A×).
η	x∈B(0,2e)
122
Published as a conference paper at ICLR 2022
Lastly, from Lemma J.10 and J.12, one can see that for any δ > 0 that is sufficiently small,
sup	P(A× ) = o(ηN),
x: kxk≤2
and this concludes the proof.	□
Recall that in Lemma J.17, we have shown that it is rather unlikely to have the first exit with
accumulated cost of large jumps less than JG . In the next result, we show that even with large jumps
of accumulated cost JG, if some jumps are still not large enough, or the inter-arrival times are too
long, then it is still very unlikely for the SGD iterates to even get close the the boundary. Define
KJηG,δ =∆ min{k ∈ [Kη (δ)] : Jkη,δ ≥ JG }
and let KJη,δ = ∞ if J η,ηδ δ < JG. Also, if KJη,δ < ∞, let
JG	Kη (δ)	JG
T≥JG(η,δ)=∆TKηη,δ(δ)
In other words, T≥JG (η, δ) is the first time that, prior to first exit or return, the accumulated cost of
large jumps has reached JG. Regarding the o(η1+JG-α1+∆) term in the next Lemma, we note that
the function λ defined in eq. (J.50) is regularly varying (as η J 0) with index 1 + JG - αι. Therefore,
η1+JG-α1+∆ = o(λ(η)) as η approaches 0.
Lemma J.20. There exists some ∆ > 0 such that thefollowing claim holds for all E ∈ (0, e/3) and
all δ > 0 that is sufficiently small:
Sup2 Px(B×,(I)(e, δ, η) ∪ b×,(id(e,δ,η)) = "η1+jG-αι+与
as η J 0 where £(E) = ci + ci log(1∕e) and
B2×,(I)(E,δ,η)={KJηG,δ<∞}∩{JKη,ηδ,δ >JG},
B2×,(II)(E, δ, η) = {KJηG,δ < ∞} ∩ {JKη,ηδ,δ =JG}
∩ n∃j = 2, 3,…，KJG StTj(δ) - Tj-i(δ) > 2t∕η or ∃j = 1, 2,…，KJG s.t. η ∣∣Wjη(δ)∣∣ ≤ "
∩ { min	d(Xj, Gc) ≤ e}.
j=0,1,…，t ≥JG (η,δ)
Proof. From Assumption I.8, we know the existence of some ∆ > 0 satisfying the following
condition: For any k ∈ N and any j ∈ {1, ∙∙∙ ,m}k such that Pk-IL(αji -1) < Jg < Pk=ι(αji-1)
(note that there are only finitely many possible choices for such j ), we have
k
JG + ∆ < X(αji - 1)	(J.55)
Fix such ∆ > 0. Meanwhile, given the fixed e > 0, we can define t = ci + ci log(1∕e) and choose
eE > 0 as the largest possible value such that
e ≤-------‰—, ρ(t)e ≤ e, e(+)e ≤ E
≤ 4exp(2tM), ρ() 一 , ρ( ) —
where functions ρ(∙), ρe(∙) are defined in Corollary J.6 and J.9 respectively. We stress that t is a fixed
constant while t depends on the value of E, and for any sufficiently small E we will have t > t.
Let us define an event A× = Ai× ∪ A2× where
Ai× =∆ n∃n<TiJ(δ)S.t. kXnJ(x)k > 3Eo,
A× = n∃j = 2, ∙∙∙ , l* s.t.
k = i,2,∙∙∙ ,(Tjn (δ)m淳 ι(δ)-i)∧(2^∕j) η llZ%”1 + …+ Zj-i")+j Il > eO
123
Published as a conference paper at ICLR 2022
where the positive integer l* is defined in eq. (J.1). By definition of l*, We must have KJ, < l on
event {KJη,δ < ∞}.
First, we analyze the following event
B = (A×)c ∩ {KjGδ < ∞}∩ {界(δ) - Tj-ι(δ) > 2^ for some j = 2,3,…，KJG}.
In particular, note that B = UK=0 B(K) where
B(K) =∆(A×)c∩{KJJG,δ =K+1}∩{JKJ,δ < JG}
∩ {τj+ι(δ)- Tn (δ) > 2^ for some j = 1, 2,3, ∙∙∙ ,K}.
When K = 0 < 1, the event B(0) = 0 by definition (due to event {T+ι(δ) - Tj(δ) >
2 for some j = 1,2,3,…，K}). For K = 1, ∙∙∙ ,l*, LemmaJ.16 implies that B(K) = 0 for all η
sufficiently small. In summary, B = 0 for allη sufficiently small.
Now we focus on the following two events
C = (A×)c ∩ {KJGδ	< ∞}∩	{τj(δ)	- Tj-ι(δ) ≤	2^	forall j = 2, 3,…，KJG} ∩ {J⅛5)	>	JG},
D = (A×)c ∩ {KJGδ	< ∞}∩	{Tj(δ)	- TL" ≤	2^	forall j = 2, 3,…，KJ} ∩ {趾⑹=JG}.
On the one hand, event C can be decomposed as follows. Let
k-1	k
J↑ = {j ∈{1, 2,…，m}k ： k = |j|, X(αji - 1) <J < £(。九一1)}
i=1	i=1
be the set that contains all the types j such that the accumulated cost reach JG if and only if the
last element is kept. One can see that there are only finitely many elements in J↑ . Now we have
C = Uj∈J↑ C(j) where (see the definition in eq. (J.10))
C(j) = (A×)c ∩ {KJGδ < ∞}∩ {Tj(δ) - Tj-ι(δ) ≤ 2^ forall j = 2, 3,…，KJ}
∩ {(wj3), Wn(S),…，WKη,δ (δ))is oftype-j).
For any j ∈ J↑ , observe that (let kj = |j |)
sup Px(C(j))
kxk≤2
≤p(Wn(δ) ∈ % ∀ ∈ [KjGδ] andTin(δ) - Tj-ι(δ) ≤ 2t forall i = 2,3,…，KJG)
=(Y hHSm), (P(TJ⑶ ≤ 2”η))S
The last equality is due to independence of WiJ(δ) and TiJ(δ) - TiJ-1(δ). The regularly varying
natures of functions Hj and H imply that
1.	Qk= 1 修	δk ɑι
li^m------------ -----------
施 Qk= i 需需	δPk=i。)
Meanwhile, from Lemma G.4 and the regularly varying nature of function Hj andH (in particular,
repeating the same calculations that leads to eq. (J.46)), we have
lim (P(TJ(δ) ≤ 2"η))kjτ =1
Jψ0	(H(I/J)	2 )kj T	.
∖ j	δɑι )
124
Published as a conference paper at ICLR 2022
Therefore, we have yielded that
lim sup
ηψo
supkxk≤2 Px(C(j))
eλj(η)
≤ -Jδj^ ∙(当 kr< ∞
一δPk= 1 αj δδaj
where
λej (η) =
Qk= ι Hjeln = o( 1+JG -α1 +∆)
ηkj-1H (1∕η) = (/	)
due to eq.(J.55). Therefore, one can see that as η J 0,
sup Px (C) = o(η1+JG-α1+∆).
kxk≤2
Furthermore, the discussion above have shown that for all η sufficiently small,
SUp Pχ(B×(1)(e,δ,η)) ≤ o(η1+JG-。1+多 + SUp Pχ(A×).
x: kxk≤2	x: kxk≤2
From Lemma J.10 and J.12, one can see that for all N > 1 + JG - α1 + ∆ and all δ > 0 that is
sufficiently small, SUpx: kxk≤2 P(A× ) = o(ηN). Fix such δ > 0. In conclusion, we now have that
supx: kxk≤2e Pχ(B×,(i)(e,δ,η)) = oR+JG-。1+“
On the other hand, on event D, Assumption I.7 shows that (see the definition in eq. (J.39))
{KJηG,δ < ∞} ∩ {JKη,ηδ(δ) = JG}
={KJG < ∞} ∩ {(wn (δ),W2 (δ),…，WK n,δ (δ)) is of type-j for some j ∈ j (i*)}.
Therefore,
B2×,(II)(, δ,η)∩D
=(A×)c ∩ {KJGδ < ∞}∩ {(Wln(δ), W2n (δ),…，WK n,δ (δ)) is of type-j for some j ∈ j(i*)}
∩ {∀j = 2, 3,…,KJ, Tj (δ) - Tji-I ⑹ ≤ 2"η} ∩ {…ɪ 叫~时^㈤,GC) ≤ 讣
∩ {∃j = 2, 3,…，KJ StTj (δ) - T∣-i(δ) > 2t∕η OR ∃j = 1, 2,…，KJ s.t.η∣∣Wji (δ)∣∣ ≤ ".
In particular, using Lemma J.15, one can see that when η is sufficiently small, on event B2×,(II)(, δ, η)∩
D we have kXki k < 3 for all k < T1i (δ) and
SUp	∣∣∣Xbisc - z i (S)∣∣∣ < 2.
s∈[0,T ≥JG (i,δ)-T1η (δ)]
Here
zn(s) = eη (s,X⑴；T, W) ∀s ≥ 0
with T =	(0,T2i(δ)	-	Ti(δ),T3i(δ)	-	T2i(δ),…，TK形+1(δ)	-	TKη,δ(δ)),	W =
(Wn(δ), W∣(δ),…，Wηη,δ (δ)), and X⑴=XTη⑷-广 Besides, eq.(J.17) dictates that, on event
B×(∏)(gδ, η) ∩ D, we must have infs≥o d(zn(s), GC) ≥ 100l*e, hence
min	d(X∣, GC) > 50l*e.
k∈[T ≥JG (i,δ)]	k
However, this clearly contradicts the definition of event B2×,(II)(, δ, η). In conclusion, we have
established that (for all η sufficiently small) B×(∏)(e, δ, η) ∩ D = 0 and
SUp Px B2×,(II)(, δ,η) ≤ SUp Px(A×).
x: kxk≤2	x: kxk≤2
However, as per the argument above, our choice of sufficiently small δ > 0 ensures that
supx： ∣∣χk≤2e P(A×) = o(η1+JG-oi+δ) and concludes the proof.	□
125
Published as a conference paper at ICLR 2022
Furthermore, starting from the local minimum 0, it is unlikely that the SGD iterates will be extremely
close to ∂G when the accumulated cost of large jumps reaches JG . The next lemma provides an upper
bound for the probability of the said scenario. For any set A ⊆ Rd, define its -enlargement as the
following open set A =∆ {x : d(x, A) < }.
Lemma J.21. For any E ∈(0, e/(3 + 3ρ*)) and any δ > 0 that is sufficiently small,
supx: kxk≤2 Px(B3×(, δ, η)) α
limsup-------	〜-------------- ≤ δα1 Ψ(e)
ηψ0	λ(η)
where B3× (E, δ, η)	=	{KJηG,δ <	∞} ∩ {d(XTη≥JG (η,δ), ∂G)	< E} and Ψ(E)	=
μ(h-1((∂G)(3+3ρ*)e)^ with h defined in eq. (J.38) and μ defined in eq. (J.48), and ρ* ∈ (0, ∞) is
the constant provided in Corollary J.7.
Proof. Again, given the fixed e > 0, we can define t = ci + ci log(1∕e) and choose e > 0 as the
largest possible value such that
e ≤-----‰—, ρ(t)e ≤ e, pe(t)e ≤ e
4 4exp(2tM) 八' 八'一
where functions ρ(∙),p(∙) are defined in Corollary J.6 and J.9 respectively. We stress that f is a fixed
constant while t depends on the value of e, and for any sufficiently small E we will have t > t.
Let us define an event A× = Ai× ∪ A2× where
Ai× =∆ n∃n < Tiη(δ) s.t. kXnη(x)k > 3Eo,
A× = n∃j = 2, ∙∙∙ , l* s.t.
max	ʌ η IIZTη 1(δ)+1 +	+ ZTη 1(δ)+jll > e(
k = 1,2,…,(Tjη (δ)-Tη-ι(δ)-1)∧(2t∕η) Il j-1v	j-1v	Il J
where the positive integer l* is defined in eq. (J.1). Consider the following decomposition of event
B3×(E, δ, η) ∩ (A×)c:
C(I)=∆B3×(E,δ,η)∩(A×)c∩{JKη,ηδ,δ > JG}.
C(II) = B× (e, δ, η) ∩ (A×)c ∩ {%δ = JG}∩ n∃j = 2,3,…，KnGδ s.t.
Tj(δ) - Tjn-i(δ) > 2t∕η or ∃j = 1, 2,…，KJG s.t. η ∣∣Wjn(δ)∣∣ ≤ "
C(In) = (B×(E,δ,η) ∩ (A×)c)∖(C(I) ∪ Cm).
Using Lemma J.20, we know that for all δ > 0 sufficiently small,
1.	SuPkxk≤2e Px(C(I) ∪ C(II)) C
limsup ——------------------------= 0.
nψo	λ(η)
As a result,
supkxk≤2 Px(B3×(E, δ, η))	supkxk≤2Px(A×)	supkxk≤2Px(C(III))
lim sup---------------------------- ≤ lim sup--------------------+ lim sup---	1----------.
nψo	λ(η)	nψo	λ(η)	nψo	λ(η)
From Lemma J.10 and J.12, one can see that for all N > 1 + JG - αi and all δ > 0 that is sufficiently
small, we have supx: kxk≤2 P(A×) = o(ηN) = o(λ(η)).
Moving on, we focus on bounding the probability of event C(III). In particular, note that
C(III) = (A×)c ∩ {KJηG,δ < ∞} ∩ {JKη,ηδ,δ = JG} ∩ {d(XTη≥JG(η,δ), ∂G)<E}
∩ n∀j = 2, 3,…，KnGδ, TJ(δ) - Tj-ι(δ) ≤ 2t∕η and ∀j = 1, 2,…，KnGδ, η ∣∣Wjn(δ)∣∣ ≥ ”.
126
Published as a conference paper at ICLR 2022
By applying Lemma J.15 on event C(III), one can see that when η is sufficiently small, on this event
we have kXkη k < 3 for all k < T1η(δ) and
sup	Xbηsc+T1η(δ)3 - zη(s) < 2.
s∈[0,T≥JG (η,δ)-T1η(δ)]	1
Here
Zrl(S) = eη (s,X⑴；T, W) ∀s ≥ 0
with T = (0,T2r(δ) - Tn (δ),T3r (δ)-以⑹,…XK η,δ (δ) - TK η,δjδ)), W =
(Wn(δ), W2r(δ),…，Wnη,δ (δ)), and X⑴=XTη⑻_了 Tobettercontrolthe location of zr(s),
we also construct
z0n(s) = xen s,0; T,W ∀s ≥ 0
where the only difference is that we substitute the initial value X(1) with 0. Note that on event C(III),
we have X(1) < 3. By applying Corollary J.7 (the condition about line segments contained in G
is verified due to eq. (J.18)), we have the following bound on event C(III):
sup
Kη,δ
s∈[0,Pi=J1G Ti]
kzr(S) - Z0(s)k< 3ρ*e.
Now recall Definition J.1. Combining all the bounds we have obtained so far, we see that, when η is
sufficiently small, on event C(III) we have
KJηG,δ
d(zr (X Ti), ∂ G) < (2 + 3ρ*)e
i=1
and W is of type-j for Some j ∈ j(i*). It then follows immediately from Corollary J.14 that
lim sup
rψο
SUpkxk≤2e Px(C(In))
λe(η)
≤ lim sUp
n
p((∂G)(2+3ρ*)e, δ, η)
λe(η)
≤ δα1 μ(h-1((∂GX2+3P*"))
≤ δα1 μ(h-1((∂G)6+M)e))
and this concludes the proof.
□
Lastly, we provide a lower bound for the probability of the most likely way of escape, i.e., due to k
large jumps with accumulated cost JG and relatively short inter-arrival time less (when compared to
t∕η). We stress that, in the next result, the value of constant c* > 0 would not vary with the choice of
parameters , δ .
Lemma J.22. There exists some c* > 0 such that thefollowing claim holds for any E ∈ (0, e/(4 +
3ρ*)) and any sufficiently small δ > 0:
.inf kxk≤2e Px(A° (e,δ,η))
lιm ini ——--------------------- ≥ c*δa1
r，0	λ(η)
where the event is defined as
A∙(e,δ,η) = {KJG < ∞}∩{σ(η)= T≥jg(η,δ)} ∩ {Tjr(δ) - Tr-ι(δ) ≤ 鼻 ∀j = 2, 3,…，k*}
and ρ* ∈ (0, ∞) is the COnStantprovided in Corollary J.7.
Proof. First of all, Assumption I.7 implies that
{KJG < ∞}∩ {σ(η) = T≥jg (η, δ)} = {κJGδ = k*}∩ {σ(η)=邛(δ)}.
127
Published as a conference paper at ICLR 2022
We also stress that KJ, = k means that σ(η) ∧ R(e, δ, η) ≥ T?* (δ) (i.e. the arrival time of the
k -th large jumps and that the accumulated cost of large jumps Wn (δ),…，WJ* (δ) is exactly JG.
Due to eq. (J.19) and eq. (J.17), there exists some j ∈ j(i*), some θ = (θι,…，θk*) With θi ∈ Fi ∀i,
some r = (ri,…，rk*) such that e ≥ δ ∀i, some t = (tι,…，tk*) such that tι = 0 and t ∈ (0, t)
for all i = 2,…，k* such that
d(h(e, θ, t), G) > 100l*e
∆
Where h is the mapping defined in eq. (J.38). Let ye =∆ h(re, θ, t). Moreover, the continuity of mapping
h implies the existence of some ∆ > 0 such that (e, θ, t) ∈ X ⊆ h-i (B(e, e)) where the open
domain X is defined as
X = {(r, θ, t) : ti =0 and M-θi∣∣ < ∆, ∣∏ - 7i∖ < △, |ti - ei| < ∆ ∀i ∈ [k*]}.
In particular, the parameter ∆ can be chosen small enough with
Λ	♦	丁 A 一	♦	~ ʌ 1
△ < min ti , △ < min rei ∧ 1
i=2,…，k*	i=1,…，k*
so that for any (r, θ, t) ∈ X, we have t > 0 for all i = 2,…，k and r > 0 for all i ∈ [k*]. Fix
such △ > 0. Meanwhile, from the definition of the measure μ(∙) in eq.(J.40)eq. (J.48) and the fact
that θi ∈ Fji where the closed set Fji is the support of the probability measure Sji on the unit sphere
SdT (see Assumption I.6), one can see that μ(X) > 0.
Let R = (Ri)k= i, Θ = (Θi)k=ι, T = (Ti)Ii=i with R = η ∣Wn(δ)k, 5 = Win(δ)/ ∣Wn(δ)k
and Ti = η(TiJ(δ) - TiJ-1(δ)) for all i ≥ 2 and T1 = 0. Now recall Definition J.1 and consider the
following event
B=∆{(R,Θ,T) ∈ X} = {ZT1η(δ) is of type-(h(X), η, δ)}.
Corollary J.14 then gives the bound
liminf ?(') ≥ δα1 μ(X) > 0.
n(0	λ(η)一
From now on, we fix some c* ∈ (0,μ(X)). Given the fixed E > 0, we can define £= ci + ci log(1∕e)
and choose e > 0 as the largest possible value such that
e ≤-----‰—, ρ(t)e ≤ e, pe(t)e ≤ E
4 4exp(2tM)，八' 八'一
where functions ρ(∙),p(∙) are defined in Corollary J.6 and J.9 respectively. We stress that f is a fixed
constant while t depends on the value of e, and for any sufficiently small E we will have t > t.
Let us define an event A× = Ai× ∪ A2× where
Ai× =∆ n∃n < Tin(δ) s.t. kXnn(x)k > 3Eo,
A× = n∃j = 2, ∙∙∙ , k* s.t.
k = i,2,∙∙∙ ,(Tjn(δ)m⅞ι(δ)-i)∧(2^∕n) η l|Z*")+ i + …+ ZTj")+j ll > eθ
where the positive integer l is defined in eq. (J.1).
Now we focus on the event B∖A×. On one hand, Lemma J.15 shows that, for all η sufficiently small,
on this event we have ∣∣XJ ∣∣ < 3e < e for all k < TJ(δ) and
sup
s∈[0,Tkη*(δ)-T1η(δ)]
XJ
Xbsc+T1η(δ
- zJ(s)ll
< 2e < €.
Here
zn(s) = en (s,X⑴；T, W) ∀s ≥ 0
128
Published as a conference paper at ICLR 2022
with τ = (o,τη(δ) - τη(δ),τ3η(δ)-耳⑷,…，τη*(δ)-以一⑷),W =
(Wn(δ), Wg(δ),…，W?* (δ)), and X⑴ = XTη⑷_了 To better control the location of
zη (s), we also construct
z0 (S) = X (s, 0; T, W) ∀s ≥ 0
where the only difference is that we substitute the initial value X(1) with 0.
First of all, from eq. (J.20), we see that
k*
d(zn(s), Gc) > 100l*e, d(zn(s), Gc) > 100l*e ∀s ∈ [0, X Ti).
i=1
Besides, due to X ⊆ h-1(B(e, e)), We have that d(z0 (Pk= 1 Ti), G) > 99l*e. Next, using Corol-
lary J.7, We have that sup∈[l3 P、T] ∣∣zn(S) - Zn(s)k < 3ρ*e < J which further implies that
k*
d(zn(XTi) G) > 99l屋-e ≥ 98l*e.
i=1
Now we have the following facts regarding the distance between SGD iterates Xjn and the domain G .
First, on event B∖A×, it holds that
d(Xj, Gc) > 100l屋-e ≥ 99l旌 ∀j < T2 (δ),
implying that σ(η) ≥ Tkn* (δ).
Next, if R(, δ, η) < Tkn* (δ), then there exists some T1n(δ ≤ j < Tkn* (δ) such that Xjn ≤ 2 hence
∣∣zn(j - Tn)∣ ≤ 2e + 2e < e < 100l* J However, in light of eq. (J.20) we know that on event
{R(, δ, η) < Tkn* (δ)} ∩ (B∖A×), we have
k*
d(zη(XTi),Gc) > 100l*J
i=1
and yield a contradiction. Therefore, on event B∖A× we must have R(, δ, η) ≥ Tkn* (δ).
Besides, bounding the gap between Xjn, zn(j) and z0n(j) at time j = Tkn* (δ) using results above, we
can show that
d(ΧTη	, G) > 98l*e - e ≥ 97l*e.
Tk* (δ)
Therefore, we must have σ(η) = Tkn* (δ). In summary, we have shown that, for all η sufficiently
small, B∖A× ⊆ A°(g δ, η). Combining all the bounds above, we have
liminf infkxk≤2e Px(A%,δ,η)) ≥ liminf 吗-^sup SuPkxk/「⑷)
ηi0	λ(η)	ηi0 λ(η)	ηψ0	λ(η)
≥ δα1 c*
- lim suP
ηψ0
SUPkxk≤2e P(A×)
eλ(η)
To conclude the proof, it suffices to invoke Lemma J.10 and J.12 with some N > 1 + JG - α1. □
J.4 Proof of the main result
Now we are ready to state the main result and provide upper and lower bounds to the distribution of
the scaled first exit time from domain G. In particular, we define a scaling function
λ(η) = λ(η)H (1/n)	(j.56)
where the function λ(∙) is defined in eq. (J.50). One can easily see that λ ∈ RV1+jg (η). We show
that the scaled first exit time λ(η)σ(η) converges in distribution to an exponential random variable as
η J 0, which implies that, in expectation, the first exit time is roughly a 1∕η1+JG term and its order is
dictated by the minimum cost for exit JG .
129
Published as a conference paper at ICLR 2022
Proposition J.23. Given any C ∈ (0, 1), u > 0, the following inequalities hold for all > 0
sufficiently small,
lim sup sup Px(σ(η)λ(η) > u) ≤ 2C + exp(-(1 - C)2qu),
ηΨ0	∣∣xk≤2e
lim inf inf Px(σ(η)λ(η) > u) ≥ -C + exp(-(1 + C)qu)
ηJ0	∣∣xk≤2e
where q = μ(h-1(Gc)).
Before presenting the proof to Proposition J.23, we make some preparations. First, we introduce
stopping times (for all k ≥ 1)
τk(, δ,η) = min{n > τek-1(, δ,η) : η kZnk > δ}
τek(, δ,η) = min{n ≥ τk(, δ,η) : kXnηk ≤ 2}
with the convention that τ0(, δ, η) = τe0(, δ, η) = 0. Intuitively, at each τek the SGD iterates have just
returned to a small neighborhood of the local minimum 0. Due to Markov property of Xnη , the SGD
iterates almost regenerate at each τek despite the previous trajectory, and the times (τek)k≥1 partitions
the entire timeline into different segments that can almost be interpreted as regeneration cycles, where
τek-1 can be understood as the starting point of the k-th cycle. In light of the embedded (informal)
regeneration process, one natural approach is to determine the dynamics and probability of making
an exit on one (hence every) cycle, and this will be carried out with the help of technical results in the
previous section. It is worth noticing that τek are defined under the proviso that τek ≥ τk where τk is
the first big jump during the k-th cycle. Considering results such as Lemma J.12, it is reasonable to
expect that the SGD iterates would be trapped at local minimum until a large jump strikes. Therefore,
we define the (informal) regeneration points τek in such a way that a cycle [τek-1, τek) ends only if we
have observed at least one large jump already. Regarding the notations, we add a remark that when
there is no ambiguity we will drop the dependency on , δ, η and simply write τk , τek .
Specifically, we are interested in the large jumps and the accumulated cost thereof during each cycle,
which definitely entails some systematic bookkeeping. For all k ≥ 1, the random variable
jk = #{n = τk-ι(e,δ,η),τk-ι(e,δ,η) + 1,∙∙∙ ,τ⅛(e,δ,η)∧σ(η):ηIIZnk >δ}
can be understood as the count of large jumps during the k-th cycle. Here are two remarks on this
definition.
•	First, for any k with σ(η) < τek, we have jk = 0. Recall that the object we study here is the
first exit time σ(η), so there is virtually no need to keep track of the dynamics of the SGD
after it leaves the domain G .
•	The random variable jk is measurable w.r.t. Fτek∧σ(η), the stopped σ-algebra generated
by the stopping time τek ∧ σ(η), which, intuitively, is stating that one should be able to
determine the number of large jumps during the k-th cycle when this cycle ends.
Furthermore, for each k = 1, 2,…，let
Tk,1(,δ,η) =∆ τk-1(, δ,η) ∧ σ(η),
Tkj (e,δ,η) = min {n > Tkj-i(e,δ,η) : η kZnk > δ} ∧ σ(η) ∧ Tk ∀j ≥ 2,
Wk,j (, δ, η) =∆ ZTk,j(,δ,η) ∀j ≥ 1
with the convention Tk,0(, δ, η) = τek-1(, δ, η). Note that for any k ≥ 1, j ≥ 1, Tk,j is a stopping
time. Besides, for any k with τek < σ(η)
τek-1 + 1 ≤ Tk,j ≤ τek ∧ σ(η) ∀j ∈ [jk],	(J.57)
and the sequences (Tkj) j=ι and (Wkj) j=ι are the arrival times and sizes of large jumps during the
k-th cycle, respectively. Again, when there is no ambiguity we will drop the dependency on , δ, η
and simply write Tk,j and Wk,j.
We are now able to keep track of the accumulated cost of jumps during each cycle. For any
k ≥ 1, i ≥ 1, we define
i
J∕cle(i; e,δ,η) = X J(Wk,i(e,δ,η)).
l=1
130
Published as a conference paper at ICLR 2022
j≥JG (k； 3, δ, η) =∆
Moreover, we define the following index
min{j = 1, 2,…，jk ： J∕cle(j； 3 δ, η) ≥ JG}	if e(e, δ, η) < σ(η)
∞	otherwise
with the convention that min0 = ∞. That is to say, if τ⅛ ≥ σ(η) or Jycle(jk) < JG, We define
j≥JG (k) as ∞; otherwise j≥JG (k) is the index for the first large jump during the k-th cycle that
drives the accumulated cost of large jumps to reach JG. For clarity of the presentation below, we also
define
T≥JG (k; 3 δ, η) = l{j≥JG (k) < ∞}Tk,j≥ Jg ⑻ + l{j≥JG (k) = ∞}0,
X≥JG (k； 3 δ, n) = l{j≥JG (k) < ∞}XT≥ JG	δ + l{j≥JG (k) = ∞}0.
T (k;, ,η)
In other words, when j≥JG (k) < ∞, the random variable X≥JG (k； 3, δη) is equal to XTη	, the
k,j≥JG (k)
location of the SGD iterates right when the accumulated cost on the k-th cycle reaches JG. Notation
wise, we will drop the dependency on 3, δ, η again and simply write j≥JG (k), X≥JG (k) or Jkcycle(j)
when there is no risk of ambiguity.
As have mentioned above, we want to zoom in on the each cycle and analyze the probability of each
possible case. To be specific, we want to introduce a series of scenarios, formally defined as several
events, that exhaust all the possibilities during a cycle. The events will be denoted in the form of Ak
or Bj× largely following the next few rules. First, we say an event/scenario is atypical if its probability
is rather small, and we assign it with superscript ×; otherwise we say the event is typical and add a
superscript ◦. Besides, the subscript k indicates the cycle that the event concerns. Lastly, events with
label (of type) A usually describe how the SGD iterates try to escape from G, whereas events with
label B focuses on how the SGD iterates return to the local minimum.
Now we proceed and formally define the said series of events. First, for each k ≥ 1, define the event
A×o(3,δ,η) = n∃i ∈ [l*] s.t.	maχ	.,、	η∣∣ZTk,i+ι + …+ Zjll >
j=τk,i-1 + 1,τk,i-1+2,∙∙∙ ,(Tk,i-1)∧(τk,i-1+ d 2=η I) ∧τk ∧σ(η)
∪ n ∣∣xjη∣∣ > 3e for some j = efc-1,e⅛-1 + 1,…，Tk,ι - l}	(J.58)
with £(e) = ci + ci l0g(l∕3) and function e(∙) defined in eq. (J.51). It is similar to the event A×
defined in eq. (J.52)eq. (J.53) in the previous section, and its probability will be controlled using a
similar approach.
Next, consider event (for all k ≥ 1)
Ak×,i(3, δ, η) =∆ nσ(η) <τek, Jkcycle(jk) <JGo	(J.59)
that describes an atypical case where the exit occurs during the k-th cycle yet the accumulated cost
of all large jumps is less than JG. Another atypical event is defined as
A×,2 = {j≥JG (k) < ∞}∩{∃j = 2, 3,…,j≥JG (k)s.t.Tk,j- Tk,j-i > 2f(e)∕η}.	(J.60)
This event describes the case where the accumulated cost of large jumps in a cycle has reached JG
but the inter-arrival time between some large jumps are unusually long. We stress that, by definition
ofj≥JG(k), we have {j≥JG (k) < ∞} = {τek-i < σ(η)} ∩ {Jkcycle(jk) ≥ JG}.
Moving on, we consider the following events (defined for all k ≥ 1)
A×,3 = {Jfcycle(jfc) < JG, ek < σ(η)} ∩ n∃j = 2, 3,…，j& s.t∙Tk,j- Tk,j-i > 2f(e)∕η}
(J.61)
that describes another similar case where the k-th cycle ends with return to the local minimum but
the inter-arrival time between some large jumps are unusually long.
The next event
A×,4 = {j≥JG (k) < ∞, JkyCle(j≥JG (k)) = JG} ∩ {d(X ≥JG (k), G C) ≤ 4∩ {∃j ∈ [j ≥JG (k)] s.t. η kWk,j k ≤ S
(J.62)
131
Published as a conference paper at ICLR 2022
describes the case where the accumulated cost on the k-th cycle has hit JG exactly at some point
with SGD iterates being rather close to Gc yet some large jumps are not large enough when compared
to the fixed constant δ.
Meanwhile, with events
Ak×,5 =∆ {j≥JG(k) <∞, Jkcycle(j≥JG(k))=JG}∩nd(X≥JG(k),∂G) <o,	(J.63)
we analyze the case where the SGD iterates reaches somewhere close the the boundary set ∂G with
large jumps of cost equal to JG . Lastly, define event
Ak×,6 =∆ {j≥JG (k) < ∞, Jkcycle(j≥JG (k)) > JG}	(J.64)
for the case where the accumulated cost of large jumps on the k-th cycle exceeds JG without hitting
it. As an amalgamation of these atypical scenarios, we let
6
Ak× (, δ, η) =∆ [ Ak×,i(, δ, η).	(J.65)
i=0
Next, we analyze the probability of some events (Bk× )k≥1 that concern the behavior of SGD iterates
during the k-th cycle after T≥JG (k). Let us define
Bk×,1(,δ,η)=∆ {j≥JG(k) <∞, Jkcycle(j≥JG(k))=JG}∩{d(X≥JG(k),Gc) ≥}
∩ {Tkj - Tkj-I ≤ 2tη) ∀j = 2, 3,…，j≥JG (k)0
B×,2(e,δ,η) = {e - Tk,j"G(k) > p("n} ∪ mη) < ek}
Bk×(,δ,η)=∆Bk×,1∩Bk×,2	(J.66)
where ρ(∙) is the function in Lemma J.11. From the definition of B×, in particular the inclusion of
Bk×,2 , one can see that the intuitive interpretation of event Bk× is that the SGD iterates did not return
to local minimum efficiently or even escaped from G after large jumps with cost equal to JG . In
comparison, the following events will characterize what would typically happen during each attempt:
Akk δ,η = {j≥JG (k) < ∞, Jfcycle (j≥JG (k)) = JG} ∩ {σ(η) = T ≥JG (k)}
∩ {Tkj - Tkj-I ≤ 2tηE) ∀j = 2, 3,…，j≥JG (k)},	(J.67)
Bk(e,δ,η) = {σ(η) >e ,入-Tk,ι ≤ "l t(：+ P(C) }∩{J*le(jk) ≤ JG }.	(J.68)
Intuitively speaking, Ak tells US that the exit happened right at T≥JG (k) with accumulated cost of
large jumps in the cycle being exactly JG, and Bk requires that that the first exit did not occur during
the k-th cycle, and the SGD iterates returned to local minimum rather efficiently. It is worth noticing
that, by definition, j≥JG (k) < l* if j≥JG (k) < ∞.
Our immediate next goal is to analyze the event
k-1
A×(e,δ,η) = [	\ (A× ∪ B× ∪ A°)c ∩ (A× ∪ B×).	(J.69)
k≥1 i=1
In particular, we show that its probability can be made arbitrarily small with proper E, δ, η, implying
that we will almost always observe the event (A× )c.
Lemma J.24. Given any C > 0, the following claim holds for all E > 0, δ > 0 sufficiently small:
lim sup sup Px(A× (E, δ, η)) ≤ C.
nψ0 x: ∣∣xk≤2e
Proof. We start by bounding for the probabilities of all the Ak×,i events. Fix some N > 1 + JG. For
A×,o(e,δ,η) = {∃i ∈ [l*] St	maχ	.,、	η∣∣ZTk,i+ι+ …+ Zjll >e(c)}
j=τk,i-ι + 1,τk,i-ι+2,…，(Tk,i-I)∧(Tk,i-ι + d2tη I)∧τk∧σ(n)
∪ n ∣∣χη∣∣ > 3c for some j = ek-1,ek-1 + 1,…，Tk,ι - l},
132
Published as a conference paper at ICLR 2022
using Lemma J.10 and J.12, one can show the existence of some 6o ∈ (0, e/3) such that for any
∈ (0, 0), there is some δ0() > 0 with
lim SUp
η
supkxk≤2 Px(A1×,0(, δ, η))
ηN
0 ∀δ ∈ (0, δ0()).
(J.70)
Next, for Ak×,1(, δ, η) = σ(η) < τek, Jkcycle(jk) < JG , it follows from Lemma J.17 that, for any
∈ (0, 0), there exists some 0 < δ1() ≤ δ0() such that
lim SUp
η
supkxk≤2Px(A1×,1(,δ,η))
ηN
0 ∀δ ∈ (0, δ1 ()).
(J.71)
For event A×,2 = {j≥JG (k) < ∞} ∩ {∃j = 2, 3,…，j≥JG (k) s∙t∙ Tk,j - Tkj-I > 2t.(e)∕η},
Lemma J.18 implies that, for any ∈ (0, 0), there exists some 0 < δ2() ≤ δ1() such that
lim SUp
η
supkxk≤2 Px(A1×,2(, δ, η))
ηN
0 ∀δ ∈ (0,δ2()).
(J.72)
For event A×,3 = {Jcycle(jk) < Jg, e⅛ < σ(η)}∩{∃j = 2, 3,…，j& s∙t∙Tk,j -Tk,j-1 > 2f(e)∕η},
thanks to Lemma J.19, one can see that, for any ∈ (0, 0), there exists some 0 < δ3() ≤ δ2()
such that
lim SUp
η
supkxk≤2 Px(A1×,3(, δ, η))
ηN
0 ∀δ ∈ (0,δ3()).
(J.73)
For event
A×,4 = {j≥JG (k) < ∞, JkyCle(j≥JG (k)) = JG} ∩ {d(X≥JG (k), GC) ≤ 4∩ {∃j ∈ [j≥JG (k)] s∙t∙η kWk,j k ≤ ",
using Lemma J.20, one can see the existence of some ∆ > 0 such that, for all ∈ (0, 0), there is
some 0 < δ4() ≤ δ3() such that
lim SUp
η
SUpkxk≤2e Px(A；,4(e,S, η))
η1+JG -αι+δ
0 ∀δ ∈ (0, δ4()).
(J.74)
As for event Ak×,5 = {j≥JG (k) < ∞, Jkcycle(j≥JG (k)) = JG} ∩ nd(X ≥JG (k), ∂G) < o, the plan
is to use Lemma J.21 to control its probability. But before that, we recall the definition of function
Ψ(e) = μ(h-1((∂G)(3+3ρ*)e)) in LemmaJ.21 and make several observations. First, note that ∂G is
a closed set, so ∩>0h-1 (∂G) = h-1(∂G) for the mapping h in eq. (J.38). Then it follows from
Assumption I.9 that limeψo μ (h-1 ((∂G)')) = 0, implying that for e > 0 sufficiently small, We must
have
- Cg
ψ(e) < 丁∙
where C > 0 is the fixed constant in the description of the Lemma, and c* > 0 is the constant in
Lemma J.22. We stress that both of them would not vary with , δ, η. Combining this with Lemma
J.21, we know the existence of some 5 ∈ (0, 0] such that, for all ∈ (0, 5), one can find some
0 < δ5 () ≤ δ4 () with
lim SUp
η
SUpkxk≤2e Px(A×,5亿 δ,η))
λe(η)
Cc
≤ δαΨ(e) <δα才 ∀δ ∈ (0,δ5(e))∙	(J.75)
Lastly, for event Ak×,6 = {j≥JG (k) < ∞, Jkcycle(j≥JG (k)) > JG}, from Lemma J.20 again we know
that (let 6 = 5) for all ∈ (0, 6), one can find some 0 < δ6() ≤ δ5() with
lim SUp
η
sUpkxk≤2Px(A1×,6(,δ,η))
η1+JG -αι+δ
0 ∀δ ∈ (0, δ6()).
(J.76)
133
Published as a conference paper at ICLR 2022
Recall that A×(e,δ,η) = U6=0A×-(e,δ,η). Also, the events B×,Ak,Bk are defined in
eq. (J.66)eq.(J.67)eq.(J.68) respectively. Before bounding the conditional probability of events B×,
We make several observations. First, if We consider the event ∩k=1(A× U B× )c ∩ B°, the inclusion
of the (Bj) j=1 implies that during the first k cycles the SGD iterates have never left G, so
k	k
∩ (A× U B× )c ∩ Bj = (∩ (A× U B×)c ∩ Bj) ∩ {σ(η) > e}.
j=i	j=i
Next, note that
k-1
Px(B× | ∩ (A× U B×)c ∩ Bj)
j=i
k-1
=Px(B×,1 | ∩(A× U B×)c ∩ Bj
j=i
'--------------V--------------
=(I)
k-1
(∩ (A× U B×)c ∩ Bj) ∩ BQ .
j=i
_ - /
{z
=(II)
We start by analyzing term (I). From Assumption I.7, one can see that on event B×,ι, it holds that
j≥JG (k) = k*, (Wk,i,…，Wk,k*) is of type-j for some j ∈ j(i*)
and Tkj- Tkj-I ≤ 2竽 ∀j = 2, 3,…，k*. Due to independence of Wk,i and Tk,i+1 — Tk,i, We
have
sup (I) ≤
∣∣x∣∣≤2
X	P((Wn(S),…，Wn*(S)) isoftype-j)
j： j∈j(i*)
k*
X π
j： j∈j(i*) i=1
Hji(δ∕η)
H(δ∕η).
Due to the strong Markov property at stopping time τ⅛-1, Lemma G.4, and the regularly varying
nature of functions Hi, H,
lim sup
n，0
SUPkxk≤2 (I)
(2法)∕δα1 )k*-1(H(i∕η)∕η)k*-1 ∙∣j(i*)∣∙ δ⅞*⅛
(∏Z1 (Hi(i∕η))iq) /(h (i∕η))k*
≤ i.
In other words, for any e,δ > 0, there exists some C(e, δ) < ∞ such that
lim sup
n，0
SUPkxk≤2 (I)
_ ~ ,、
δαι λ(η)
≤ ∣j(i*)∣C(e,δ).
On the other hand, due to Lemma J.11 and the strong Markov property at stopping time Tk,k*, for
any E ∈ (0, e) and any δ > 0,
lim sup sup (II) <
nΨ0	∣∣xk≤2e
Cc*∕4
∣j(i*)∣C(e,δ).
In summary, we have established that for any E ∈ (0, e) and any δ > 0,
lim sup SUPkxk≤2e Px(B× ^^(^ U B" ∩ Bj)
nψ0	δɑι ∙ λ(η)
< Cc* ∕4 ∀k ≥ 1.
(J.77)
Similarly, we can bound conditional probabilities of the form Px (A× ∣ T：-： (A× U B×)c ∩ Bj). To
be specific, recall that A× = U6=0A×i. Combining eq. (J.70)-eq. (J.76) with strong Markov property,
we know the for all E ∈ (0, e6) and k ≥ 1,
IimSUPSUPkxk≤2' Px(AkI ∩j=1 (Aj U Bj)C ∩ Bj) < Cc*∕4 ∀δ ∈ (0,S6(e)).	(J.78)
nψ0	δɑι ∙ λ(η)
134
Published as a conference paper at ICLR 2022
On the other hand, a lower bound can be established for conditional probability of event A, the event
defined in eq. (J.67). Due to Lemma J.22 and the strong Markov property at τek-1, one can see the
existence of some 7 ∈ (0, 6] such that, for any ∈ (0, 7), there is some 0 < δ7() ≤ δ6() such
that
lim inf
ηψo
infkχk≤2e Px(Ak | Tk-1(A× ∪ B×)c ∩ Bj)
δα1 eλ(η)
≥ c*
∀δ ∈ (0, δ7()), k ≥ 1.
(J.79)
In order to apply the bounds eq. (J.77)-eq. (J.79), we make use of the following inclusion relationship:
k-1
(\	(A× ∪	B×)c ∩	Bj)	∩	(A× ∪	B×)c ⊆	Ak	∪	Bk.	(J.80)
j=1
To see why this is true, let us consider a decomposition of the event on the L.H.S. of eq. (J.80). As
mentioned above, on event Tjk=-11(Aj× ∪ Bj× )c ∩ Bjj we know that σ(η) > τek-1, implying that for the
first k - 1 cycles the first exit does not occur and leaving us with only three possibilities on this event:
•	j≥JG (k) = ∞ (on {σ(η) > τek-1}, this would happen if and only if Jkcycle(jk) < JG);
•	j≥JG(k) < ∞,X≥JG(k) ∈/ G;
•	j≥JG(k) <∞,X≥JG(k) ∈G.
Let us partition the said event accordingly and analyze them one by one.
•	On (Tk-I(A× ∪ B×)c ∩ Bj) ∩ (A× ∪ B×)c ∩ {Jycle(jk) < JG}, due to the exclusion of
A× (especially A× 1 and A×3), We can see that if jk < l*, then We must have σ(η) > ek
and e - Tk,ι ≤ 2l*S(E)/η. In particular, note that on this event we must have l* > jk by
definition of l in eq. (j.1). Therefore,
k-1
(\ (A× ∪ B×)c ∩ Bj) ∩ (A× ∪ B×)c ∩ {j≥JG (k) = ∞}⊆ Bk.
j=1
•	On (Tk-1(A× ∪B×)c∩Bj)∩(A× ∪B×)c∩{j≥JG(k) < ∞, X≥JG (k) ∈/ G}, the exclusion
of A×2 implies that Tkj — Tkj-I ≤ 2t(e)∕η for all j = 2,…，j≥JG (k), and the exclusion
of Ak×,6 tells us that Jkcycle(j≥JG (k)) = JG. Moreover, the exclusion of Ak×,5 ∪ Ak×,6 tells us
that ifX≥JG (k) ∈/ G, we must have d(X≥JG (k), G) > E. In summary,
k-1
(\(A× ∪ B×)c ∩ Bj) ∩ (A× ∪ B×)c ∩ {j≥JG (k) < ∞, X ≥ JG (k) ∈ G}⊆ Ak.
j=1
•	On (Tk-1(A× ∪ B×)c∩ Bj) ∩ (A× ∪ B×)c ∩{j≥JG (k) < ∞,X≥JG(k) ∈ G}, the same
argument in the previous bullet point can be applied to show that Tkj — Tkj-I ≤ 2t(e)∕η
for all j = 2,…，j≥JG (k), JyCle(j≥JG (k)) = JG, and d(X≥JG(k), Gc) > e. Now
since B× did not occur, we must have σ(η) > ek and ek — T≥JG (k) ≤ ρ(e)∕η, hence
e — Tk, 1 ≤ 2l* 叱+ρ9. Therefore,
k-1
(\ (A× ∪ B×)c ∩ Bj) ∩ (A× ∪ B×)c ∩ {j≥JG(k) < ∞, X≥JG(k) ∈G}⊆ Bk.
j=1
With eq. (J.80) established, we can immediately get that
k-1	k-1
(\ (A× ∪ B×)c ∩ Bj) ∩ (A× ∪ B×)c = ( \ (A× ∪ B×)c ∩ Bj) ∩ (A× ∪ B×)c ∩ (Ak ∪ Bk).
j=1	j=1
(J.81)
135
Published as a conference paper at ICLR 2022
Next, recall the definitions of Ak in eq. (J.67) and Bk in eq. (J.68), and one can see that the two
events Ak and Bk are mutually exclusive, since the former implies that the first exit occurs during the
k-th cycle whereas the latter implies the first exit does not occur in the first k cycles. This fact and
eq. (J.81) allow us to conclude that
k	k	k-1
∩(A× UB× U A-)c = ∩(A× UB×)c ∩ B≡ = (∩ (A× U B×)c ∩ Bj ∩ (A× U B× U AQc.
i=1	i=1	i=1
(J.82)
The last step is to use all the results so far to bound the probability of event
/k-1	、
A×(e,δ,η) = U ∩ (A× U B× U A°)c ∩ (A× U B×).
k≥1 ∖i=1	)
Using eq. (J.82), we can see that (for any x with ∣∣x∣∣ ≤ 2e)
Pχ(A×(e,δ,η))
=X Px (( ∩ (A× U B× U A”) ∩ (A× U Bk))
k≥1	'' i=1	)	)
=X Pχ(( ∩ (A× U B×)c ∩ Bj ∩ (Ak U Bk))
k≥1	''i=1	'	/
k-1	k-1	j	j-1
=XPX (a×	U B× |	∩ (A×	U B×)c	∩ B/Y Px	( ∩	(A×	U B×)c ∩ B° ∣	∩	(A×	U B×)c ∩ BD
k≥1	i=1	j=1	i=1	i=1
k-1
=X PX (ak U B× | ∩ (A× U B×)c ∩ Bj
k≥1	i=1
k-1	j-1	j-1
- ∏ PX (( ∩ (A× U B×)c ∩ B°) ∩ (A× U B× U A°)c∣ ∩ (A× U B×)c ∩ Bj
j=1	i=1	i=1
k-1	k-1	j-1
=X PX (ak	U B× |	∩ (A×	U B×)c	∩ B) ∏ PX	((A×	U B× U	A°)c	∣	∩	(A×	U	B×)c ∩ Bj
k≥1	i=1	j=1	i=1
k-1	k-1	j-1
≤ X PX (a× U B× | ∩ (A× U B×)c ∩ B)Y 1 - PX(A° ∣ ∩ (A× U B×)c ∩ Bj .
k≥1	i=1	j=1 ∖	i=1	)
This allows us to apply eq. (J.77)-eq. (J.79) and conclude that for all e ∈ (0, €7), all δ ∈ (0, 67(c))
and all η > 0
sup PX(A×(e,δ,η)) ≤ X (δα1e(η) ∙ W) ∙ (1 - δα1 λ(η) ∙	)k-1
∣X∣≤2e	k≥l	2	2
.δαι e(η) ∙岑
---	7∑;	- C√ .
δαι e(η)∙号
when η is sufficiently small.	□
Having established Lemma J.24, we are ready to provide a proof for Proposition J.23.
Proofof Proposition J.23. Define sets
E+(z) = {y ∈	Rd	：	d(y,	G) > (3 +	3ρ*)z},	E-(z)	= {y ∈ Rd :	d(y,	GC)	< (3 +	3ρ*)z}
136
Published as a conference paper at ICLR 2022
where ρ* is the fixed constant in Corollary J.7. From the continuity of measure, We have
lim μ(h-1(E+ 9)) = μ(h-1((G)c)), lim μ(h-1(E-(e))) = μ(h-1(G c)).
eψ0	eψ0
From Assumption I.7 we know q = μ(h-1(GC)) > 0. From Assumption I.9 we also have
μ(h-1 (∂G)) = 0, hence μ(h-1((G)c)) = q as well. Therefore, together with LemmaJ.24, we know
the existence of some 6ι ∈ (0, 3+∣PT) such that, for any E ∈ (0, s), there is some 0 < δι(e) with
lim sup sup Px (A× (, δ, η)) < C.
ηψ0 kxk≤2e
and
(1 - C)q < μ(h-1(E +(e))) ≤ q ≤ μ(h-1(E-(e))) < (1 + C)q.
Henceforth, we fix some E and δ in this range and prove the inequalities in the Proposition.
To proceed, let J(E, δ, η) =∆ sup{k ≥ 1 : τek-1 < σ(η)}. We now show that on event (A× (E, δ, η))c,
we must have J ≤ J↑ where (recall Definition J.1)
J↑(e, δ, η) = min {k ≥ 1 : Zt^λ isoftype-(E +(c),η, δ)}
with E+(z) = {y ∈ Rd : d(y, G) > (3 + 3ρ*)z} where ρ* is the fixed constant in Corollary J.7. To
see this via a proof of contradiction, let us assume that J↑ = j < J for some integer j . Now we
make some observations.
•	First of all, due to J↑ = j < J and eq. (J.17), on event (A× (E, δ, η))c ∩ {J↑ = j < J} we
have
Tj,i - Tj,i-1 ≤ 2t∕η ∀i = 2, 3,…，k*；
•	The definition of event (A× )c, especially the exclusion of event Aj×,0 defined in eq. (J.58)
now allows us to apply Lemma J.15 and show that
s∈[0TsupT-T ] Xbηsc+Tj,1 -zη(s) <2E.
s∈[0,Tj,kT -Tj,1 ]
Here
zη(s) = eη (s,X⑴；T, W) ∀s ≥ 0
with T = (0,Tj,2 - Tj,ι ,Tj,3 - Tj,ι,…，Tj,k* - Tj,k*-ι) W = (Wj,1,…，Wj,k*), and
X(1) = X-η 1+T . Moreover, the previous bullet point allows us to apply Corollary J.7 and
conclude that
sup	kzη(s) - z0η(s)k ≤ 3ρ*E
s∈[0,Tj,kT -Tj,1]
where zη(s) = xeη s, 0; T, W ∀s ≥ 0 and ρ* is the fixed constant in Corollary J.7. As a
result, z0η (Tj,kT - Tj,1) - XTηj,kT < (2 + 3ρ*)E.
•	However, due to J↑ = j, we have d(z0η (Tj,kT - Tj,1), G) > (3 + 3ρ*)E. This immediately
implies that XTη T ∈/ G and yields the contradiction J ≤ j .
Similarly, one can show that on event (A×)c we must have J ≥ J' for
J%,δ,η) = min {k ≥ 1 : Zt^ isoftype-(E-(c), η, δ)}
with E- (z) =∆ {y ∈ Rd : d(y, Gc) < (3 + 3ρ*)z}.
Besides, the following claim holds on event (A× )c ∩ {J < ∞}.
137
Published as a conference paper at ICLR 2022
•	The definition of (A×)c implies that during the J-th cycle the event AJ occurs whereas for
any j < J we have B；. Therefore, for any k = 1, 2,…，J,we have
〜八(、2l	V 2l*'(E) + P(E)
Tk ∧ σ(η) - Tk,ι ≤ --------------.
η
In particular, the coefficient l* on the R.H.S. can be justified as follows: by definition of Ak
and Bk, we can see that JyCle(jQ ≤ JG for all k ≤ J (with jj = k due to the occurrence
of AJ); the definition of l* in eq.(J.1) then implies that l* > jk for all k ≤ J.
•	Now we consider the following set
S(E, δ,η) = ∪k≥1 {τk-1 + 1, li ek-1 ÷ 2,…，Tk,1 - 1, Tk,1}
the can be understood as the concatenation of all steps between any return time τek-1 and
the first large jump time Tk,1 during the k-th cycle. Our discussion above implies that, on
event (A× )c, we have then the discussion above have shown that, for
min{n ∈ S(E, δ,η) : Zn is of type-(E+(E), η, δ)} ≥ TJ,1.
It is worth noticing that the probability of Z1 being oftype-(E+(E), η, δ) is (see the definition
in eq.(J.47)) H(δ∕η)p(E+(E), δ, η).
Therefore, for the following partition of the timeline
Sbefore (E, δ, η) =	{n	∈	S(E, δ, η)	: n ≤ σ (η)},	Ibefore (E, δ, η) = #Sbefore (E, δ, η),
Safter(E, δ,η) =∆	{n	∈/	S(E, δ,η)	: n ≤ σ(η)},	Iafter(E, δ, η) =∆ #Safter(E, δ, η),
we have σ(η) = Ibefore ÷ Iafter. Moreover, on event (A× )c, we must have
Iafter ≤ J(21*+(e)÷ ρ(f))∕η ≤ J↑(21*+(e)÷ ρ(f))∕η
Ibefore ≤ min{n ∈ S(E, δ, η) : Zn is oftype-(E+(E), η, δ)}}.
Next, define geometric random variables with the following success rates
Ui(e, δ,η)〜GeOm(H(δ∕η)p(E +(E),δ,η)),
U2(E, δ,η)〜Geom(p(E +(E),δ,η)).
Given the results above for bounding Ibefore and Iafter on event (A× )c, we have
sup Px λ(η)σ(η) > u
kxk≤2
≤ SUP Pχ((A×)c) ÷ SUP Px((A×)c ∩ {λ(η)(Ibefore
kxk≤2	kxk≤2
÷ Iafter ) > u
≤ SUP Px (A×)c ÷ SUP Px
kxk≤2
÷ SUP Px
kxk≤2
kxk≤2
(A×)c ∩ {λ(η)Ibefore > (1 - C)u}
((A×)c ∩ {λ(η)Iafter > Cu}
≤ SUP Px((A×)c) ÷ P(λ(η) ∙ Ui > (1 - C)U) ÷P λ(η) ∙ U ∙
2l*£(E)÷ P(E) > Cu).
kxk≤2
{z^
,(I)
η
{z^^^^
,(II)
|
}
|
}
L .	∕τ∖	11 .1	. ʌ /	∖	TT / r I ∖V /	∖	11	-Γ 1 Λ 1 .1	1	1	♦	.	C TT / ∖
For term (I), recall that λ(η) = H(1∕η)λ(η). Corollary J.14 and the regularly varying nature of H(∙)
then imply that
li H(δ∕η)p(E+(e),δ,η) = H(δ∕η) ∙P(E +(e),δ,η)
舒	λ(η	— -HiOM ∙ δαιe(η)一
μ(h-1(E+(E))) > (1 - C)q.
138
Published as a conference paper at ICLR 2022
From Lemma G.3, We then have limsupηψo (I) ≤ exp( 一(1 — C)2u). For term (II), let C(E) =
2l*t(c) + P(E) and note that
(II) = P(U2 > ------∙ —^n-
()V 2 > C(E) λ(n)∕n
Moreover, since
e
^ ∈ ∈ ∈ RV-αι + 1(n),
λ(η)∕η	1
e
we know that for any M > 0, we have λ(λη}∕η > M eventually as n J 0. Therefore, given any
M > 0, we have (II) ≤ P U2 > M∕eλ(η) for any η sufficiently small. Now from Lemma G.3,
lim SuPnjO (II) ≤ exp(一M) for any M > 0, and here we fix some M > 0 such that exp(—M) < C.
In summary, we have shown
lim sup sup Px λ(n)σ(n) > u ≤ 2C + exp(—(1 — C)2qu)
ηj0 kxk≤2
and established the upper bound. The lower bound can be shown by an almost identical approach. In
particular, since
inf Px λ(n)σ(n) >u) ≥ inf Px(A×)c∩ {λ(n)(Ibefore+Iafter) >u})
kxk≤2	kxk≤2
≥ inf Px(A×)c∩{λ(n)Ibefore >u})
kxk≤2
≥P(λ(n)∙U1 >u) — SuP Px((A×)c)
kxk≤2
where U0 (e, δ, n)〜Geom (H(δ∕n)p(E- (e), δ, n)). The same calculation above for term (I) can be
used here to provide a lower bound for lim infn即 P(λ(η) ∙ U1 > u) and conclude the proof. □
Now we are ready to prove Theorem I.1, the main theorem of this section.
ProofofTheorem I.1. Recall that our choice of scaling function λ(∙) in eq. (J.56) is regularly varying
(w.r.t. η) with index 1 + JG. Fix some x ∈ G, t > 0 and C ∈ (0, 1). It suffices to show that
lim sup Px(σ(η)λ(η) > t) ≤ exp(-q(1 - C)3t) + 2C,
ηψo
liminfPx(σ(η)λ(η) > t) ≥ exp(-q(1 + C)t) - C.
ηψo
First, we are able to pick some E ∈ (0, 1) sufficiently small such that d(x, Gc) > E and Proposition
J.23 is applicable. Due to Lemma J.11, for event
A =∆ {Tre
turn(η, E) ≤ ρ(E)∕η, Xnη ∈ G ∀n ≤
Treturn(η, E)},
we have limηjo Px(A) = 1. Next, on event A, we have σ(n) — TretUrn(n, E) > 0 and IXTtUrlI® e)|| ≤
2E. Moreover, by combining Proposition J.23 with the strong Markov property at stopping time
Treturn(η, E), we have
limsupPx((σ(n) — TktUrn(n, E))λ(n) > (1 — C)t | a) ≤ 2C + eχp(-(1 — C)3qt),
ηψo 、	/
liminf Px((σ(n) — Tkturn(n, E))λ(n) > t | A)≥ —C + exp( —(1 + C)qt).
Observe that
Px(σ(η)λ(η) >t) ≤ Px({σ(η)λ(η) >t}∩A)+Px(Ac)
≤ Px
+ Px
((σ(η) - Treturn(η,e))λ(η) > (1 - C)t | A)Px(A)
({‰turn(η, e)λ(η) > Ct} ∩ A) + Px(Ac).
139
Published as a conference paper at ICLR 2022
Besides, on event A We have TretUrn(η, E) ≤ O(1∕η) as η J 0. Given that λ(η) ∈ RV(ι+jg)(n), We
have Treturn(η, )λ(η) ≤ Ct on event A for all η sufficiently small. Therefore, by applying the bounds
above, we establish that limsupn1。Pχ(σ(η)λ(η) > t) ≤ 2C + exp(-(1 — C)3qt). Similarly, in
order to shoW the loWer bound, observe that
Px(σ(η)λ(η) >t) ≥ Px({σ(η)λ(η) >t}∩A)
≥ Pχ((σ(η) - Tkturn(η,f))λ(η) > 11 a)Px(A).
Taking liminf on both sides yields liminfη Pχ(σ(η)λ(η) > t) ≥ -C + exp(-(1 + C)qt) and
concludes the proof.	□
K Proof of Technical Lemmas
Proof of Lemma G.3. For any E > 0,
b1/b()c
a(E)
By taking logarithm on both sides, We have
lnP
b1/b(E)c ln 1-a(E)
[1∕b(e)C ln (1 - a(e)) -a(e)
1∕b(e)	—a(e)	b(e)
Since limχ→o ln(1+x) = 1, we know that for E sufficiently small, we will have
a(E)
C而
≤ lnP U(E) >
≤-
a(E)
c ∙ b(e)
(K.1)
By taking exponential on both sides, we conclude the proof.
□
Proof of Lemma G.4. To begin with, for any E > 0 we have
Using bound eq. (K.1), we know that for E sufficiently small, P(U(E) > 1∕b(e)) ≥ exp(-c ∙
a(E)∕b(E)). The upper bound follows from the generic bound 1 - exp(-x) ≤ x, ∀x ∈ R with
X = c ∙ a(E)/b(E).
Now we move onto the lower bound. Again, from bound eq. (K.1), we know that for sufficiently
small E, we will have
P(U (E) ≤ b(E)) ≥ 1-exp(-√c ∙ a⅛).
Due to the assumption that limeψo a(c)∕b(E) = 0 and the fact that 1 — exp(-x) ≥ √ for x > 0
sufficiently close to 0, we will have (for E small enough) P (U(E) ≤ b(^) ≥ ɪ ∙需.
□
Proof of Lemma G.5. Let ak =∆ xk - xek . Using intermediate value theorem, one can easily see that
ak
k
η X (vg(≡j-ι) — Ng(Xj-1, + η(zι +-------+ Zk) + χ — e；
j=1
⇒ IlakIl ≤ ηc(l∣ao∣l + …IIak-IIl) + c.
The desired bound then follows immediately from Gronwall’s inequality.
□
140
Published as a conference paper at ICLR 2022
Proof of Lemma J.1. Due to the finiteness of all types j0 with Jtype(j0) < JG, it suffices to fix one of
such j0 and prove the existence of constants t, δ, 6o.
Let e be the constants in eq.(J.3). For any r ∈ (0, e), define the following stopping time
Tr(x) = min{t ≥ 0 : xt(x) ∈ B(0,r)}.
Due to Assumptions I.4 and I.5 as well as continuity in PoinCare Map (see Theorem12 in Immler &
Traut (2019)), for any fixed r > 0 we know that Tr (∙) is a continuous function on G. Note that, by
definition, we have Tr (x) = 0 for any X ∈ B(0,r). Due to G being compact and eq. (J.3), we know
the existence of some constant Tr ∈ (0, ∞) such that supχ∈g Tr (x) ≤ Tr and xt(x) ∈ B(0, r) for
any X ∈ G, t ≥ Tr.
Due to Assumption I.8 and the finiteness of all types j0 with Jtype(j0) < JG , there exists a 0 ∈
(0, e/2) such that
sup d(e(s, 0; t, w), Gc) > 2s	(K.2)
s≥0
for all j0 with Jtype(j0) < JG , t ∈ {0} × R|+j0|-1, w ∈ Atjy0pe. We fix such 0 > 0. Here is one
implication that is worth mentioning. Recall that j is fixed in the description of the Lemma and
k0=|j|.If
inf d(e(s, 0; t(k ), w(k )), Gc) ≤ s
s≥0
for some t(k0) ∈ {0} × Rk+0-1, w(t0) ∈ Atjype, then we must have
k0
d(e(Xti, 0;t(k0),w(k0)), Gc) ≤ eo.
i=1
To show the existence of some S > 0 we appeal to a proof by contradiction. (For a clean pre-
sentation, in t and w we omit the (k0) term in the superscript since the cardinality is fixed.) As-
sume that we can find a sequence (tn, wn)rb≥ι such that min { Ilwnk, ∙∙∙ , kw&∣∣ } ≤ 1/n and
d(e(Pk= ι tn, 0; tn, wn), Gc) ≤ €o for any n. Due to the truncation operator ψb(∙) inthe definition
of et, without loss of generality we can replace all jumps wn by 夕b (Wjn) to ensure that wr is always
in a compact set. Define
yjn = e(tn, 0; tn, wn) ∀j = ι, 2,…，k0.
By picking a subsequence when necessary, we can further assume that
•	For any i ≥ 1, Wjn) converges to some w* and yi converges to some y*. In particular, there
exists some I ∈ [k0] such that w* = 0.
•	Also, since Gc is a closed set, we have d(yk*0 , Gc) ≤ 0.
•	For any i = 2,3, ∙∙∙ , k0, t either converges to some finite t*, or limn t = ∞.
•	Note that
lim tin = ∞ ⇒ yi* = 0	(K.3)
n
for the following reason: for any r ∈ (0, S), our discussion about Tr(X) at the beginning of
the proof implies that lim supn kyin k ≤ r.
Let I be the largest index i ∈ [k0] such that yi* = 0. In case that we cannot find such index, let I = 0.
We first consider the case where I = 0. The bullet points above then imply that, for any i ≥ 1, we
must have limn tin = ti* < ∞. Now due to boundedness of ti* and the continuity of ODE flow, for
t* = (0,t*,t*,…，t*o),
w* = (wl,w2,w3,…，w*o),
141
Published as a conference paper at ICLR 2022
we have
k0
e(X£, 0; t*, w*) = y屋
i=1
Now recall that wI* = 0 for some I ∈ [k0]. By removing the vacuous (size-zero) jumps in
00
(t*, w*), we now know the existence of some ke0 ∈ N, we* ∈ (Rd\O)k , te* ∈ {0} × Rk+ such
that d(e(Pe= ι e*, 0;e*, W*), GC) ≤ 0 . Meanwhile, the condition Jtype (j ) < JG implies that, for
ek0
the total cost of jumps in we* after removal of size-zero jumps, Pi=1 J (wej*) < JG. However, this
contradicts eq. (K.2).
Similarly, in the case with I ≥ 1, we have that for any i > I, we must have limn tin = ti* < ∞. From
the boundedness of ti* and the continuity of ODE flow, for
t* = (0,te+1,te+2,…，tk，)，	(K.4)
w* = (we, wτ+1,we+2, ∙ ∙ ∙ ,w*0),	(K.5)
we have
k，
xe( X ti*, 0; t*, w*) =yk*，.	(K.6)
i=Ie+1
τ ..	1	..T	-. .1	.	.1	. 1	..1	一 J , r、 ：	,1 . .1
In particular, if I = 1, then for the index I with wI* = 0, we must have that I ≥ I, meaning that there
is at least one vacuous jump in w*. The same argument for the case I = 0 above can lead to the same
contradiction with eq. (K.2). Otherwise, with Ie ≥ 2, we already know that the accumulated cost of
all jumps in w* is strictly less than JG, yet We still have d(X(Pk=e+ι t*, 0； t*, w*), Gc) ≤ eo∙ This
contradicts eq. (K.2) again.
In summary, we have established the existence of the lower bound S > 0 onjump sizes. We fix such
S > 0. The existence of S < ∞ can be shown by an almost identical argument. In particular, if such
tS < ∞ does not exist, then by picking a subsequence if needed we are able to find a converging
sequence tn , wn n≥1 such that eq. (K.6) holds and Ie ≥ 1 due to inter-arrival time blowing up to
infinity. In particular, since y1* = w1* and kw1* k ≥ δS > 0, we must have I ≥ 2. By considering the
same t*, w* pair in equation K.4 and equation K.5, one can see that the accumulated cost of jumps
in W is strictly less than JG and yield a contradiction with eq. (K.2).	□
Proof of Lemma J.2. Due to the finiteness of all types j0 with Jtype(j0) < JG, it suffices to fix one of
such j0 and prove the existence of the positive constant 1.
To proceed with a proof by contradiction, the assume that such 1 > 0 does not exists. (For a clean
presentation, in t and w we omit the (k0) term in the superscript since the cardinality is fixed.) As a
result, we are able to pick a sequence (xn, tn, wn)n≥1 such that one of the following two cases must
occur:
•	limn IIxnIl = 0; For any n ≥ 1, infs≥o d(e(s, xn; tn, wn), Gc) ≤ e0∕2 and there is some
jn ∈ [k0] such that tjn ≥ 2tS;
•	limn IIxnll = 0; For any n ≥ 1, infs≥o d(e(s, xn; tn, wn), Gc) ≤ e0∕2 and there is some
jn ∈ [k0] such that wjn ≤ δS∕2.
We detail the analysis for the first case, as the second case can be addressed by an almost identical
argument. First of all, due to the truncation operator 夕b(∙) in the definition of et, without loss of
generality we can replace all jumps Wj by ψb (Wn) to ensure that Wn is always in a compact set.
Define
yn = e(tn,xn; tn, Wn) ∀j = ι, 2,…，k0.
By picking a subsequence if necessary, we can further assume that
142
Published as a conference paper at ICLR 2022
•	For any i ≥ 1, Wjn) converges to some Wi and yp converges to some y*.
•	Also, since Gc is a closed set, we have d(y", Gc) ≤ ∈o∕2.
•	For any i = 2, 3, ∙∙∙ , k0, t? either converges to some finite t*, or limn tn = ∞. In particular,
there is some I ∈ [k0] such that limn tn ≥ 2t.
•	Due to eq. (K.3), limn tin = ∞ would imply yii = 0.
Let Ii = max{i ∈ [k0] : limn tin ∈ [2t, ∞)} and I2 = max{i ∈ [k0] : limn tin = ∞}. If either
of the two sets above is empty, let the corresponding I1 or I2 be 0. The discussion above implies
that at least one of them must be non-zero, so there are only two possibilities: (i) 0 ≤ I1 < I2; (ii)
0 ≤ I2 < I1. We consider each scenario respectively.
First, if 0 ≤ I1 < I2, then limn yIn = yIi = 0, implying that I2 < k0. Moreover, for any
i = I2 + I,I2 + 2, ∙∙∙ , k0, limn £ = t* < ∞. Using the boundedness of t* and the continuity of
ODE flow, for
ti = (0,tl + l2 ,t2 + I2 ,…，tkO), wi = (Wi2 ,wl + I2 , w2 + I2 ,…，wi ),
we have xe(Pik=0 1+I tii, 0; ti, wi) = yki0 with d(yki0, Gc) ≤ 0/2. However, this contradicts equa-
tion J.2.
Next, in scenario (ii) with 0 ≤ I2 < Iι, we have limn tn = tjɪ ≥ 2t. Moreover, for any
i = I2 + 1, I2 + 2, ∙∙∙ , k0, limn t = t* < ∞. Using the boundedness of t* and the continuity of
ODE flow, for
t2 = (0,t*+i2,t*+i2, ∙∙∙ ,ttk0), w2 = (w*2,W*+I2,w2+i2,…，w*0),
we have xe(Pik=0 1+I ti*, 0; t*, w*) = yk*0 with d(yk*0, Gc) ≤ 0/2. However, bmW* is still of type j
with JyPej) < JG, and there is some i such that t* ≥ 2f. This would contradict Lemma J.1.
In summary, we have established the existence of ∈ι > 0 such that tj- < 2f is a necessary condition
for any X ∈ B(0, "and any t = (ti,…，tko) ∈ {0} X R+-i, W = (wi,…，Wk,)∈ AjyPe with
infs≥o d(e(s,χ; t(k), w(k)), Gc) ≤ ∈o∕2. As mentioned above, the necessity of ∣∣Wjk > S/2 can be
shown in an almost identical way. We omit the details here and conclude the proof.	□
ProofofLemma J.3. Since there are only finitely many (ii, •…，im) with J(ii, •…，im) < JG, it
suffices to fix one of such (ii, ∙∙∙ , im) and establish the existence of the required e0, δ0. (Henceforth,
let k = Pj=1 im.)
Assumption I.8, together with the bound in eq. (J.6), implies the existence of some 印 ∈ (0, e) such
that
sup d(e(s, 0; t(k), w(k)) > €1	(K.7)
t≥0
for any w(k) ∈ A(ii, •…，im), t(k) ∈ {0} × R：-1. For €0 = €1/2, we establish the existence of the
prescribed δ0 via a proof by contradiction. (Henceforth we drop the notational dependence on (k)
when referencing sequences t(k), w(k) since the cardinality k is fixed.)
Assume the existence ofa sequence (xn)n≥1 in Rd, a sequence of real positives (sn)n≥1, a sequence
(tn)n≥1 = (t1,n,…’,tk,n)n≥1 in {0} × R+ , and a sequence (Wn)n≥1 = (w1,n,…’,wk,n)n≥1 in
A(ii, ∙∙∙ , im) such that limn IlxnIl = 0 and
d(χ(sn, xn; tn, Wn) , G ) ≤ €0.
Due to existence of the clipping operator, all jumps Wj,n, can be replaced by Wb(Wj,n) without loss of
generality to ensure that all Wj,n are in a compact set. Also, without loss of generality, all sn can be
chosen as
sn = inf{s ≥ 0 : d xe(s, xn; tn, Wn), Gc ≤ €0}.
143
Published as a conference paper at ICLR 2022
From Assumption eq. (J.6), one can see that sn must be equal to Pij=n 1 ti,n for some jn ∈ [k], i.e., it
must be the arrival time of some jump. Moreover, one can easily see that for
yn
xe(sn , xn ; tn , wn ),
we always have d(yn , G) ≤ b so all yn are in a compact set as well due to the boundedness of G.
Therefore, by picking a subsequence when necessary, we can further assume that
El	..	「一 ….	Lj*	...	.
•	There exists some j* ∈ [k] such that Sn = Ei=I ti,n for all n ≥ 1, i.e. Sn is always the
arrival time of the j * -th jump;
•	For any j ∈ [j*], there exists some yj* such that for yn,j = xe(Pij=1ti,n,xn; tn,wn)
we have limn yn,j = yj*; In particular, for any j < j* we have d(yj* , Gc) > 1 and
d(yj**, Gc) ≤ 0;
•	For any j ∈ [k], there exists some wj* such that limn wj,n = wj*; Moreover, note that
y1* = w1*.
•	In particular, (w*,…，w*) ∈ A(i1,…，im);
•	For any j ∈ [k], either there exists some tj* < ∞ such that limn tj,n =tj*, or limn tj,n = ∞
(in this case we let tj* = ∞); in the latter case, due to the same argument in eq. (K.3), we
must have limn xe(Pij=1ti,n, xi ; tn, wn) = 0;
Obviously, y** = 0. Let jψ = max{j = 0,1, ∙∙∙ ,j* : y* = 0} with the convention that
y* = 0, y*,n = Xn.We must have 九 < j* and
•	limn 屹,n =屹=0
•	For any j =九 + 1, jψ + 2,…，j *, limn tjn =巧 < ∞.
Now using the continuity of the ODE flow, we must have that
j*
limχ(〉: tj,n, yjγ,n; (tjι + 1,n,~tjγ + 2,n,…，tj* ,n} (Wjj+ 1,n, w九+ 2,n,…，wj* ,n))
j=九十 1
j*
=e( X t*, 0; (t*j + 1,t*j+2, ∙ ∙∙ ,t** ), (wjj + 1,wjj+2, ∙∙∙ ,wj* )) = y** .
j=jj+1
However, due to d(yj** , Gc) ≤ 0 and recall our choice of 0 = 1 /2, for all n sufficiently large, we
must have
j*	5
d(x(	^X	tj,n,	y;Ln；	(tjj + 1,n, t 九+2,n,…，tj*,n), (WjJ+ 1,n,	wjj+2,n,…，wj* ,n )), Gc)	≤ g €1.
j=jj+1
Meanwhile, using Gronwall,s inequality repeatedly and SuPj=九十1,…,j*,n≥ι tjn < ∞, by substitut-
ing the initial condition yj*,n with 0, we have
j*
limllx( X ： tj,n, y九n； (tjj + 1,n, t 九+2,n, ∙∙∙ , tj*,n), (wjj + 1,n, wjj + 2,n,…，wj*,n))
j=jj+1
j*
-x ( X I tj,n, 0; (tjj + 1,n, t 九+2,n,…，tj*,n), (wjj + 1,n, w九+2,n,…，wj* ,n )) 口 = 0.
j=jj+1
This implies that, for all n sufficiently large,
j*
d(e( X tj,n, 0;(力九 + 1,
,n,t九+ 2,n,…，tj* ,n), (wjγ + 1,n, wjγ + 2,n,…，wj* ,n)) , GC) ≤ 4e1.
j=九十1
However, this contradicts eq. (K.7). This implies the existence of the required δ0 and concludes the
proof.
□
144
Published as a conference paper at ICLR 2022
K. 1 Sufficient conditions for Assumption I.9
In this section, we show that under a proper set of regularity conditions on the boundary set ∂G
and the distribution of noises Zn, Assumption I.9 will hold for (Lebesgue) almost every b > 0. In
particular, we stress that the C2 condition about manifold ∂G in Assumption I.3, as well as the
condition that measures Sj are absolutely continuous w.r.t. the spherical measure σ on Sd-1, are
only used to prove that μ(h-1(∂G)) = 0 and will only be applied in this section.
The key of our argument is the following geometric observation regarding the intersection of
C2-manifold ∂G and ∂B of some ball B. We stress that, as made evident by the proof, this
lemma is essentially based on two assumptions: (I) As the boundary set of the connected bounded
region G, ∂G is a closed set in Rd; (II) As a (d - 1)-dimensional manifold, ∂G is of class C2.
Lemma K.1. Let σx,b be the spherical measure on the sphere of the open ball B(x, b) for any
x ∈ Rd , b > 0. Under Assumptions I.1 and I.3, it holds for (Lebesgue) almost every b > 0 that
bχ,b(∂G ∩ ∂B(x,b)) = 0 ∀x ∈ Rd.	(K.8)
Proof. The fact that ∂G is a subset of a separable metric space implies the existence of a countable
atlas for this manifold. Therefore, we can find a sequence (Ui)i≥1 that are bounded open subsets
of Rd-1 containing 0, a sequence (Vi)i≥1 that are open sets in ∂G (in the metric space induced
by Euclidean distance), and a sequence of injective C2 mapping fi with fi : Rd-1 7→ Rd and
fi(Ui) = Vi such that∂G = ∪iVi.
Next, we zoom in on a specific chart (Ui, Vi, fi) and observe the following facts. For any x ∈
Ui, y ∈ Vi with fi(x) = y, there exist some orthogonal matrix Qy ∈ Rd×d such that Qyn(y) =
(0,0,…，0, I)T where the vector field n(∙) is the outer normal on ∂G. Besides, there is some vector
ay ∈ Rd such that Qyy + ay = 0. Moreover, there exist an open set on Uex in Rd-1 containing 0,
∆
an open set y ∈ Vy ⊂ Vi, an open set Vy =∆ {Qyw + ay : w ∈ Vy }, a C2 function gey : Ux 7→ R
satisfying gey (0) = 0 and
Ijy(W1,…，Wd-l) = Wd ∀W =(W1,…，Wd) ∈ Vy.
In other words, for any given y on this chart we simply rotate the chart to ensure that the tangent
space at y after rotation is {(χι,…,xd-i, 0) : Xi ∈ R ∀i ∈ [d - 1]}, and reparametrize the C2
diffeomorphism associated to this chart around y so that the coordinates are simply the projection
onto the said tangent space. In this sense, the (rotated) manifold is also the graph of the C2 mapping
gy. This allows us to define (for any y ∈ ∂G)
A(y)= (λl(V2gy (0)),λ2(V2gy (0)),…，λd-1 (V2gy (0)))
where, for any real symmetric (d - 1) X (d - 1) matrix A, λι(A) ≥ λ2(A) ≥ ∙∙∙ ≥ λd-ι(A) are
the ordered eigenvalues of the matrix, and V2gy (∙) is the Hessian of gy. It is worth noticing that A(∙)
is a continuous function (on ∂G) due to the manifold being of class C2 .
Restricting our discussion on some fixed chart (Ui, Vi, fi) for now, for any b > 0, let
Ai(b) = {x ∈ Ui ： for y = fi(x), ∃j ∈ [d - 1] such that ∣λj (V2gy(0)) | = 1/b}.
Note that for any b > 0, the set Ai(b) is a closed set (hence Borel measurable) since the continuity of
A(∙) implies that (Ai(b))c is an open set on U%. Furthermore, since mLeb(Ui) < ∞, there are at most
countably many b > 0 such that mLeb(Ai(b)) > 0. Given the countability of the atlas, we know that
B* = {b> 0 : ∃i ∈ N s.t. mLeb(Ai(b)) > 0}
contains at most countably many elements. In the rest of this proof, we show that eq. (K.8) holds for
any b > 0 such that b ∈/ B*.
Henceforth, we arbitrarily choose some b > 0 such that b ∈/ B*. We also arbitrarily choose some
x ∈ Rd and let B = B(x, b). To facilitate the discussion, we introduce a concept that is closely
related to the set Ai (b): let
AG(b) = {y ∈ Vi: f-1(y) ∈ Ai(b)} = {y ∈ Vi: A(y) = (b,b, ∙∙∙ ,b) or (-b, -b,…，-b)}
145
Published as a conference paper at ICLR 2022
and let AG (b) =∆ ∪i≥1AiG(b). Now consider the following decomposition of the sphere ∂B:
B2 =∆ ∂B ∩ AG (b),
Bi = ∂B∖B2.
First, note that for any y ∈ B1, one of the following four cases has to occur:
•	y ∈/ ∂G ;
•	y ∈ ∂G and the vector y - x lies in Ty∂G, the tangent space of the manifold ∂G at y;
•	y ∈ ∂G; the vector y - x is not in Ty∂G yet it is not orthogonal to Ty∂G either, i.e. y - x is
not equal to C ∙ n(y) for any C ∈ R where n(y) is the outer normal at y;
•	y ∈ ∂G; the vectors y - x and n(y) are linearly dependent.
Our next goal is to show that, in any of these four cases, we can always find a set y ∈ Oy that is
open on the sphere ∂B such that σx,b(Oy ∩ ∂G) = 0. Note that this is obviously true when y ∈/ ∂G,
since both ∂G and ∂B are closed sets in Rd . Now we consider the second case. If the vector y - x
lies in Ty∂G, then after applying the affine transformation with orthogonal matrix Qy (recall that
Qyn(y) = (0,…，0,1)), we have
Qy (y - x) ∈ Qy Ty ∂ G = {(wi,…，Wd-1, 0)： Wi ∈ R ∀i ∈ [d - 1]}.
Since Qyy + ay = 0, we now know that after the affine transformation, the center of the ball B
moves to
Qy X + ay = Qy 期 + &y + Qy (X - y) ∈ {(wi,…，Wd-1,0) ： Wi ∈ R ∀i ∈ [d - 1]}.
Without loss of generality, we can assume that QyX + ay = (b, 0,…，0,0). In other words, after the
affine transformation, the ball becomes By = QyB + ay = B((b, 0,0,…，0), b). Moreover, for any
W = (wi,…，Wd) ∈ ∂By, we must have wi ≥ 0 and
(wi - b)2 + (W2)2 +---+ (wd)2 = b2.
Meanwhile, from the definition of the mapping gey, one can see that there is an open set Uy around y
such that for any W ∈ Uy with W ∈ ∂G ∩ ∂B (hence We = QyW + ay ∈ Vey ∈ ∂Bey) such that
西(we)2 = b2 —(泊i — b)2 — W2 —…一泊 2-ι∙
∆2	2	2	2
Let f (wi,…，Wd-ι) = e(wi,…，Wd-ι) + w2 - 2bwi + w2 + …+ w2-ι. Now We now that
≈, . .	_	--r, —	_ _	~ Crr	一 一 一 . .	一 ________
f (0) =	0 and f (w)	= 0 for any	W ∈ Vy	∈ ∂By.	Moreover, by definition	we have Ngy (0) =	0 so
∂∂W?f (0) = -2b = 0. Due to implicit function theorem, we now know the existence of some open
set U * in Rd containing 0, some C2 function g* : Rd-2 → R such that for any W ∈ Vy ∩ ∂ By ∩ U *,
Wi = g*(W2,…，Wd-i), Wd = Gy(wi,…，Wd-i).
Therefore, within some open neighborhood Vy* of such y, the set Vy* ∩ ∂B ∩ ∂G is a submanifold
with dimension d - 2, so we must have
σx,b(Vy* ∩∂B∩∂G) =0.
Next, we consider the case where y ∈ ∂G ∩ Bi but y - X is neither in Ty∂G nor orthogonal to
Ty∂G. Similar to the construction of the affine transformation with Qy, ay above, one can find an
orthogonal matrix Qy and a vector Gy such that Qyy + ay = 0 and Qy (y - x) = (0,0,…，0, b).
Let B = B((0,0, ∙∙∙ , -b), b). In other words, this time the rotation we constructed ensures that,
after rotation, the vector between QGyy + ay = 0 (on the sphere ∂BG) and the center QGyX + Gay of the
ball B is aligned with the d-th axis. Moreover, there is an open set y ∈ Vyalt ⊂ Rd and a C2 function
gGyalt : Rd-i 7→ R with NgGyalt(0) 6= 0 such that for any yG ∈ VGyalt =∆ QGyVyalt + Gay, we have
yd = gylt(eι,…，Gd-i).
146
Published as a conference paper at ICLR 2022
By applying a change of coordinates if necessary (which can be achieved by multiplying another
orthogonal matrix), We can assume without loss of generality that Veylt(O) = (0,0, ∙∙∙ ,c) for some
c 6= 0.	Then for any we ∈ Vyalt	such that w ∈ ∂B ∩ ∂G, let w =	Qywe+ aey	and note that	we must have
w2 + w2	+ …+ w2-l + (b + eylt(w1, ∙ ∙	∙ , Wd-I)) = b2 .
In particular, eylt(wι, •…,Wd-1) = cwd-ι+r(w) for some C2 function r with r(0) = 0, Vr(O) = 0.
2
As a result, for function f (W) = w2 + w2 + …+ Wd-I + 仅 + eylt(wι,…，Wd-1)) . We have
8仅？ ] f (0) = 2bc = 0. Using implicit function theorem again, one can see the existence of some
open set 0 ∈ Vy ∈ Rd and some C2 function g* : Rd-2 → R such that for any W ∈ Vy; ∩ ∂G ∩ ∂B,
∆
we have that (for W =∆ QyWe + aey)
Wd-1 = e*(wi, ∙ ∙ ∙ ,Wd-2 ), Wd = elt(wi,…，Wd-1)∙
Again, we have established that within some open neighborhood Vy; of such y, the set Vy; ∩ ∂B ∩ ∂G
is a submanifold with dimension d - 2, so we must have
σx,b(Vy; ∩ ∂B ∩ ∂G) = 0.
Lastly, consider the case where y ∈ ∂G ∩ B2 and the vectors y - x and n(y) are linearly dependent.
In other words, the tangent space Ty∂G is also the tangent space Ty∂B. Since y ∈/ B1, we know
that ey (w) = 1WTAyW + ri (w) where Ay is a real symmetric matrix with no eigenvalue equal to
±1/b and r1 is a C2 function with |r1 (W)| = o(kWk2). On the other hand, for any We in the open set
Vy ⊂ ∂G, if we also have We ∈ ∂B ∩ ∂G, then for W =∆ QyWe + ay we have
Wd = ey(wi,…，Wd-1) = Jb2 — W2 - w2 — ••∙— W2-ι - b.	(K.9)
Also, note that b2 — w2 — Wd — ∙ ∙ ∙ — Wd-I — b = — 2(wi, ∙ ∙ ∙ , Wd-1) d-1 (wi, ∙ ∙ ∙ , Wd-1)7 +
r2(W1,… ,Wd-ι) where r2 is also a C2 function with ∣r2(W)∣ = o(∣∣w∣∣2). Therefore, for any
(wi,…，Wd-1) satisfying the equation eq. (K.9), we have
1(wι,…，Wd-i)(Ay — Id-I)(wi,…，Wd-i)T = —rι(wι,…，Wd-i) + r2(w1,…，Wd-i)∙
However, for the real symmetric matrix Ay — Id-I, note that none of its eigenvalue is equal to 0,
implying the existence of some > 0 such that
I 2(W1,…，wd-1)(Ay-------—- )(w1, ∙ ∙ ∙ , wd-1)T∣ ≥ E(W2 + …+ Wd-1).
For this fixed > 0, we can also find δ > 0 such that
| — rι(wι,…，Wd-i) + r2(w1,…，Wd-1)∣ ≤ 2(W2 +------+ Wd-I)
for any w2 T--+ w2-ι < δ. ASa result, the only solution to eq. (K.9) with w2 + + w2-ι < δ
is wi = W2 = •…，Wd-ι = 0. In summary, we have shown that there exists some set y ∈ Vy that is
open in Rd such that Vy; ∩ ∂B ∩ ∂G = {y}.
Collecting the results we have established so far, we now know that for any y ∈ B1 (recall that B1 is
an open set on ∂B), there is an open set Vy; containing y and satisfying σx,b(Vy; ∩ ∂B ∩ ∂G) = 0.
In particular, given the open cover ∪y∈B2 Vy= = Bi, Lindelof property then allows us to extract a
countable open ∪i≥1Vyi = Bi cover and conclude that
σχ,b(Bι ∩ ∂G) ≤ X σχ,b(Vy* ∩ ∂B ∩ ∂G) = 0.
i≥i
Moving on, we evaluate σx,b(B2 ∩ ∂G). For any y ∈ ∂B, one of the four cases has to occur:
•	y ∈/ AG (b);
147
Published as a conference paper at ICLR 2022
•	y ∈ AG (b) and the vector y - x lies in Ty∂G, the tangent space of the manifold ∂G at y;
•	y ∈ AG (b); the vector y - x is not in Ty∂G yet it is not orthogonal to Ty∂G either, i.e. y - x
is not equal to C ∙ n(y) for any C ∈ R where n(y) is the outer normal at y;
•	y ∈ AG (b); the vectors y - x and n(y) are linearly dependent.
Again, We show that in any of these four cases, there is some set y ∈ Vy= open in ∂B such that
σχ,b(AG(b) ∩ Vy=) = 0. In the first case, the fact that AG(b) is closed on ∂G immediately implies the
existence of some Vy= such that AG (b) ∩ Vy= = 0. For the second and third case, this can be shown
using exactly the same implicit function argument above. For the last case where y ∈ AG (b) ∩ ∂B
and the vectors y - x and n(y) are linearly dependent, from y ∈ AG (b) ∩ ∂B we know that y ∈ Vi
where (Ui, Vi, fi) is a chart of ∂G and Vi is open on ∂G. Moreover, recall the construction of open set
y ∈ Vy ⊂ Vi at the beginning of the proof. It is worth noticing that Vi∩AG(b) = Vi ∩ AiG (b). Besides,
due to the fact that the vectors y - x and n(y ) are linearly dependent, we know that the tangent
space Ty ∂G is also the tangent space Ty ∂B. Therefore, by definition of gey and Qy , ay , we know that
Qyy + ay = 0, and under the affine transformation, the ball becomes B((0,0,…,±b), b). Without
∆
loss of generality, we assume it is B = B((0,0,…，b),b). Moreover, since b ∈ B*, we have that
mLeb({w ∈ Uy : x ∈ Ai(b)}) = 0 where, as defined at the beginning of the proof, Uy is the domain
∆
of the C2 mapping gey , Vy is the image of the mapping, and Vy =∆ {Qy w + ay : w ∈ Vy } is the
image of Vy under the affine transformation. Therefore, for any we ∈ AiG (b) ∩ ∂B, let w = Qy we + ay
and we must have
Wd = Igy (wi, ∙∙∙
, wd-1
) = -b+	b2 -
2
w12
2
- w22
/	、一 r 一言
(wι,…，Wd-i) ∈{w ∈ Uy
2
一∙ ∙ • 一 Wd-1,
: x ∈ Ai(b)}.
Now let V=	=	QTy (v — ay)	:	V =	(wι, ∙ ∙ ∙ ,Wd-ι, -b +
Jb2 - w2 - W _...- w2-ι) for some W ∈ Uχ} and note that y ∈ Vz is an open set on
∂B. (Specifically, note that we simply identify an open set on the transformed sphere ∂By , and then
perform the inverse transformation to move the set back to the original sphere ∂B.) Then it follows
immediately from mLeb({w ∈ Uy : X ∈ Ai(b)}) = 0 that σχ,b(AG(b) ∩ VZ) = 0.
In summary, for any y ∈ ∂B, we can find a set y ∈ Vy= open on ∂B such that σx,b AG (b) ∩ Vy= = 0.
Lastly, by applying Lindelof property again, we extract a countable open cover ∪i≥ιVy* = B2 cover
and conclude that
σχ,b(∂B ∩AG (b)) ≤ ]Tσχ,b(AG (b) ∩ Vyi) =0
i≥1
and this concludes the proof.
□
As a result of Lemma K.1, the following Lemma is essentially built upon three assumptions/facts: (I)
As a boundary set, ∂G is a closed set in Rd; (II) As a (d - 1)-dimensional manifold, ∂G is of class
C2 ; (III) The measures Sj in Assumption I.6 are absolutely continuous w.r.t. the spherical measure
σ.
Lemma K.2. Under Assumptions I.1, I.3 and I.6, it holds for (Lebesgue) almost every b > 0 that
Proof. Fix some b > 0 satisfying the conditions in Lemma K.1. Recall that μ = 52j∈j(i*) μj (See
eq. (J.40)). It suffices to show that μj (h-1(∂G)) = 0 for some fixed j ∈ j(i*). In particular,
148
Published as a conference paper at ICLR 2022
observe that
μj (h-1(∂ G))
=Z
ti+1>0, θi∈Sd-1, ri>0 ∀i∈[k*-1]
•	( /	l{h*(r1,…，rk*-1,θ1,…，θk*-1, t2,…，tk* ) + gb(rk* θk* ) ∈ dG}
θk* ∈Sd-1,rk* >0
k*-1
•	Sjk* (dθk*)Vαjk* (drk*) • Y Vαji (dri) × Sji(dθi) × mLeb(dti+1)
i=1
where the function h* is defined as
h*(r1, • • • , rk*-1, θ1, • • • , θk*-1, t2, • • • , tk* )
k*-1
=x( ɪ2 ti, 0; (0, t2, ^^^ , tk*-1), (r1θ1,r2θ2, • • • ,rk*-1θk*-1))
i=2
(K.10)
Let Z = h*(rι, ∙∙∙ ,rk*-1,θ1, ∙∙∙ ,θk*-1,t2, ∙∙∙ ,tk*). Now by separating the two cases based on
whether the truncation operator takes effect or not, we have
θk
,*
l{h*(r1,…，rk*-i,θι,…，θk*-i, t2,…，tk* ) + ψb(rk* θk* ) ∈ dG}
∈Sd-1,rk* >0
• Sjk* (dθk* )Vαjk*(drk*)
= θk
+Zθk
,*
,*
∈Sd-1,rk* ∈(0,b)
l{z + bθk*
∈Sd-1
l{z + rk*
θk* ∈ ∂G Sjk* (dθk*)Vαjk* (drk*)
∈ ∂G}Sjk* (dθk*) •	Vαjk* (drk*).
(K.11)
(K.12)
For term eq. (K.11), note that it is equal to R l{T-1(rk* ,θk*) ∈ (-z+∂G)∩∂B(0, b)}Sjk* (dθk*) ×
ναj * (drk* ) where T-1(r, θ) = rθ is the inverse of the polar coordinate transform. Furthermore,
since the set (-z + ∂G) ∩ ∂B(0, b) is either empty or is a (d - 1)-dimensional C2 submanifold (w.r.t.
B(0, b) when viewed as a d-dimensional manifold). In other words, it has zero mass under mdLeb.
For the measure V* = TT ◦ (Sjk* X Vajk*), due to Sj being absolutely continuous w.r.t. σ, it is
easy to see that V* is absolutely continuous w.r.t. mLLeb. Therefore, we must have
V*((-z+∂G)∩∂B(0,b))=0,
implying that the integral in eq. (K.11) = 0. On the other hand, for term eq. (K.12), we know that
Rrk* >bναjk* (drk* ) = 1/b1+ajk* < ∞. Besides,
L - l{z + bθk* ∈ ∂G }Sjk*(dθk*) = L ɪ 1{z + bθ ∈ ∂G∩ ∂B(z, b)}Sjk* (dθ).
Then it follows immediately from Lemma K.1 and Sj being absolutely continuous w.r.t. the spherical
measure σ that the integral in term eq. (K.12) is equal to 0. In summary, we have shown that
{	l{h*(r1,…，rk*-1,θ1,…，θk*-1, t2,…，tk* ) + g b(rk* θk* ) ∈ dG }
θk* ∈Sd-1,rk* >0
• Sjk* (dθk*)Vαjk* (drk*) = 0
for any (r1, • • • , rk*-1, θ1, • • • , θk*-1, t2, • • • , tk* ). Plug this result back into eq. (K.10) and we
conclude the proof.	□
L Notations
Table L.1 lists the notations used in Section G.
149
Published as a conference paper at ICLR 2022
Table L.1: Summary of notations frequently used in Section G
[k]	{1,2,...,k}
η	Learning rate (gradient descent step size)
b	Truncation threshold of stochastic gradient
An accuracy parameter; typically used to denote an -neighborhood of si, mi
δ	A threshold parameter used to define large noises
e	A constant defined for eq.(G.28)-eq. (G.29). Since e < ∈o, in eq. (G.8) the claim holds
for |x - y| < e. Note that the value of the constant e does not vary with our choice of
η, , δ.
M	Upper bound of |f0 | and |f00 |		eq. (G.10)
L	Radius of training domain		eq. (G.10)
Ω	The open interval (s-, s+); a simplified notation for Ωi		
夕，夕C	夕c(w),2(w, C) = (w ∧ C) ∨ (-c)	truncation operator at level C > 0	
Z ≤δ,η n	Zni{η∣Zn∣ ≤ δ}		“small” noise eq. (G.20)
Zn>δ,η	Zn l{η∣Zn∣ > δ}		“large” noise eq. (G.21)
Tjη(δ)	min{n > Tj-ι(δ) : η∣Zn∣ > δ}	arrival time of j-th large noise eq. (G.22)	
Wjη(δ)	ZTjη(δ)	size of j-th large noise eq. (G.23)	
Xnη(x)	χn+ι(X) =中 L (Xn(X)- 3b(η(f0(Xn(X))	- Zn+1 )	,	X0η (X) = X	SGD
yηn(x)	yηn(x) = yηn-1(x) - ηf0(yηn-1(x)), y0η(x)	=X	GD
Ynη(X)	yηn(X) perturbed by large noises (Tη(δ), Wη(δ))	GD + large jump
eyηn(X; t, w)	yηn(X) perturbed by noise vector (t, w)	perturbed GD
xη (t, X)	dxη (t; X) = -ηf0 xη (t; X) dt, xη (0; X) = X	ODE
x(t, X)	x1 (t, X)	
exη (t, X; t, w)	xη(t, X) perturbed by noise vector (t, w)	perturbed ODE
A(n, η, C, δ)	m	max	η∣Zι +	+ Zk| ≤ c)∙ k∈[n∧(T1η(δ)-1)]	eq. (G.42)
r	r = min{-s-, s+}. Effective radius of the attraction field Ω.
l*	l* =dr/b]. The minimum number of jumps required to escape Ω when starting from
its local minimum m = 0.
h(w, t)	A mapping defined as h(w, t) = e(tι*, 0; t, w).
t, S	Necessary conditions for h(w, t) to be outside of Ω	eq. (G.35)-eq. (G.36)
t(e)	t(e) = ci log(1∕e). The quantity t(e)∕η provides an upper bound for the time it takes xη
to return to 2-neighborhood of local minimum m = 0 when starting from somewhere
-away from s-, s+. See eq. (G.37).
E(E)	{(w,t) ⊆ Rl* X R+-1 ： h(w,t) ∈ [(s- - C) ∨ (-L), (s+ + C) ∧ L]}
p(e,δ,rη)	The probability that, for t = (Tjn(δ) - 1)" and W = (ηWj(δ))", we have
(w, t) ∈ E(C) conditioning on {T1η(δ) = 1}. Intuitively speaking, it characterizes the
probability that the first l large noises alone can drive the ODE out of the attraction
field. Defined in eq. (G.66).
να
The Borel measure on R with density
να (dx)
i{χ> 0} χα+τ+i{χ< 0}*
xx
where p- , p+ are constants in Assumption 2 in the main paper.
μ	The product measure μ = (Va)l* × (Leb+)l*-i.
σ(η)	min{n ≥ 0 : Xn ∈ Ω}.
first exit time
150
Published as a conference paper at ICLR 2022
H(x)	P(|Z1| > x) = x-αL(x)
Treturn(, η)	min{n ≥ 0 : Xnη(x) ∈ [-2, 2]}
151
Published as a conference paper at ICLR 2022
M Results about tail distributions of noises in our numerical
EXPERIMENTS
M.1 QQ plots
QQ plots below clearly show that the tails in noise distribution are always much lighter than the
Pareto distributions with alpha = 2 or even 10. In fact, the tail of noise distributions seem to be
between that of lognormal and normal distributions, implying that it is lighter than any power-law
distribution.
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.1: Ablation Study, Corrupted FMNIST & LeNet: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.2: Ablation Study, Corrupted FMNIST & LeNet: Half way through the training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.3: Ablation Study, Corrupted FMNIST & LeNet: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10
Lognormal
Normal
Figure M.4: Ablation Study, SVHN & VGG11: At the beginning
Pareto, α = 2
Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.5: Ablation Study, SVHN & VGG11: Half way through the training
152
Published as a conference paper at ICLR 2022
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.6: Ablation Study, SVHN & VGG11: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.7: Ablation Study, CIFAR10 & VGG11: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.8: Ablation Study, CIFAR10 & VGG11: Half way through the training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.9: Ablation Study, CIFAR10 & VGG11: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.10: Data Augmentation, CIFAR10 & VGG11: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.11: Data Augmentation, CIFAR10 & VGG11: Half way through the training
153
Published as a conference paper at ICLR 2022
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.12: Data Augmentation, CIFAR10 & VGG11: At the end of training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.13: Data Augmentation, CIFAR100 & VGG16: At the beginning
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.14: Data Augmentation, CIFAR100 & VGG16: Halfway through the training
Pareto, α = 2 Pareto, α = 5 Pareto, α = 10 Lognormal Normal
Figure M.15: Data Augmentation, CIFAR100 & VGG16: At the end of training
154
Published as a conference paper at ICLR 2022
M.2 Empirical mean residual life (EMRL) plots
It is well known that the mean residual life blows up to infinity if and only if the distribution is
heavy-tailed (more precisely, long-tailed). However, from the figures below, one can see that none
of the EMRL exhibits such a pattern in any case tested in our experiments. Instead, we see clear
downward trends, which strongly suggests light tails in all cases tested.
Figure M.16: Plots of empirical mean residual life for noises in FMNIST&LeNet Task throughout
training
Beginning
Middle
End
Figure M.17: Plots of empirical mean residual life for noises in SVHN&VGG 11 Task throughout
training
Figure M.18: Plots of empirical mean residual life for noises in CIFAR 10&VGG 11 Task throughout
training
Figure M.19: Plots of empirical mean residual life for noises in dataAug, CIFAR 10&VGG 11 Task
throughout training
155
Published as a conference paper at ICLR 2022
Figure M.20: Plots of empirical mean residual life for noises in dataAug, CIFAR 100&VGG 11 Task
throughout training
M.3 Hill plots
In the hill plots below, the estimated power-law indices using only the top 1% of samples (on the left
hand side of the dashed lines) stay well above 10 for the most part and almost never drop below 2.
This strongly suggests that even if the gradient noises are from a heavy-tailed distribution, it is likely
to have a very high power law index (implying relatively lighter tails), and hence, we cannot expect
to observe a prominent heavy-tailed behavior from them.
Figure M.21: altHill Plots for noises in FMNIST&LeNet Task throughout training. Dashed Red Line:
Estimation based on the largest 1% data
Figure M.22: altHill Plots for noises in SVHN&VGG 11 Task throughout training. Dashed Red Line:
Estimation based on the largest 1% data
156
Published as a conference paper at ICLR 2022
Figure M.23: altHill Plots for noises in CIFAR 10&VGG 11 Task throughout training. Dashed Red
Line: Estimation based on the largest 1% data
Figure M.24: altHill Plots for noises in Data Augmentation, CIFAR 10&VGG 11 Task throughout
training. Dashed Red Line: Estimation based on the largest 1% data
Figure M.25: altHill Plots for noises in Data Augmentation, CIFAR 100&VGG 16 Task throughout
training. Dashed Red Line: Estimation based on the largest 1% data
Table M.1: Power-law Indices Estimation throughout the Training, using PLFIT. All the estimations
are at least 5 for all cases tested in our experiments, and most of the times the estimation is above
10. This means that even under the assumption that the gradient noises were from a heavy-tailed
distribution, they should have much lighter tails than any α-stable distribution (which requires α < 2)
or the heavy-tailed noises we injected during tail inflation experiments (α = 1.4).
Task	Beginning	Middle	End
FMNIST, LeNet	14.3	14.2	16.5
SVHN, VGG11	5.0	5.2	12.5
CIFAR10, VGG11	9.2	6.6	7.0
dataAug, CIFAR10, VGG11	16.2	16.2	8.5
dataAug, CIFAR100, VGG16	35.1	14.4	5.35
157