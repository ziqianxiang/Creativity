Published as a conference paper at ICLR 2022
Learning Guarantees for Graph Convolu-
tional Networks on the Stochastic Block
Model
Wei Lu
Department of Mathematics
Brandeis University
Waltham, MA 02453, USA
luwei@brandeis.edu
Ab stract
An abundance of neural network models and algorithms for diverse tasks on
graphs have been developed in the past five years. However, very few provable
guarantees have been available for the performance of graph neural network models.
This state of affairs is in contrast with the steady progress on the theoretical
underpinnings of traditional dense and convolutional neural networks. In this paper
we present the first provable guarantees for one of the best-studied families of
graph neural network models, Graph Convolutional Networks (GCNs), for semi-
supervised community detection tasks. We show that with high probability over the
initialization and training data, a GCN will efficiently learn to detect communities
on graphs drawn from a stochastic block model. Our proof relies on a fine-grained
analysis of the training dynamics in order to overcome the complexity of a non-
convex optimization landscape with many poorly-performing local minima.
1	Introduction
There is presently a large gap between what can be accomplished in practice using deep learning, and
what can be satisfactorily explained and predicted by the theory of deep learning. Nevertheless, the
past several years have seen substantial developments in the theory of deep learning (Ge et al., 2017;
Brutzkus & Globerson, 2017; Zhang et al., 2019a; Goel et al., 2020; Chen et al., 2020a).
One factor contributing to the gap between the theory and practice of traditional NNs is that real-
world data sets tend to have complex structure that is difficult to capture with formal definitions. For
example, popular image classification models are capable of memorizing arbitrary data (Zhang et al.,
2016), and yet they exhibit astonishing generalization performance on accurately-labeled natural
images. Hence, any rigorous proof of the observed generalization performance of deep learning
models on image classification tasks will necessarily require assumptions about the data that are
sharp enough to separate random inputs from natural images. Because of the difficulty of giving an
adequate characterization of real-world data, much of the recent progress in deep learning theory
has instead focused on proving results using very simple (e.g. Gaussian) input distributions or in
distribution-free settings (Ge et al., 2017; Brutzkus & Globerson, 2017; Zhang et al., 2019a; Vempala
& Wilmes, 2019).
Compared to traditional feed-forward (dense, convolutional, etc.) NNs, the theory of graph neural
networks (GNNs) is still in its infancy. On the other hand, it appears substantially easier to give
plausible descriptions of the combinatorial structure of real-world graph data sets than, e.g., to
characterize the distribution of natural images (Drobyshevskiy & Turdakov, 2019). We therefore
believe that GNNs offer a natural setting for developing provable guarantees that are able to capture
the power of deep learning on real-world datasets. In this paper, we contribute to that goal by giving
the first rigorous guarantees of efficient semi-supervised learning of stochastic block models via a
GNN.
1
Published as a conference paper at ICLR 2022
1.1	Graph neural networks
Many natural datasets for diverse machine learning problems have a graph structure, including social
networks, molecular structures, and transit networks. In order to efficiently exploit such combinatorial
structure, a variety of GNN models have been proposed, tuned for different kinds of tasks. A number
of taxonomies of GNN models have been proposed (Zhou et al., 2018; Wu et al., 2021); one of the
most essential differences between different GNN models is whether they are meant to label the
graph as a whole, or to label individual components of the graph, particularly vertices.
From a theoretical perspective, the best understood tasks for GNNs concern labeling the graph as
a whole, for example for the task of classifying a graph by its isomorphism type (Sato, 2020). In
particular, it has been established that many GNN models are of comparable power to various versions
of the Weisfeiler-Leman hierarchy1 (Xu et al., 2018; Morris et al., 2019).
Some progress has also been made on the theory of GNNs for vertex-labeling tasks. Recent works by
Sato et al. describe the representational power of certain GNN models for tasks such as computing
minimum vertex covers (Sato et al., 2019). Garg et al. also give bounds on the representational
power of GNN models, as well as using Rademacher bounds to estimate the generalization ability of
GNNs (Garg et al., 2020).
Our results concern the task of semi-supervised community detection. In this problem, each vertex
belongs to one community, and some subset of the vertices are labeled according to their community
membership. The task is to classify the community membership of the remaining vertices. This task
has been one of the most intensively studied problems in the GNN literature, but there have not yet
been any provable guarantees on the performance of proposed models.
We study (spatial-based) graph convolutional models similar to the GCN model proposed in Kipf
& Welling (2017). A single layer of such a model computes weights at each node by aggregating
the weights at neighboring nodes and applying an activation function with learned parameters, e.g.,
a linear map followed by a ReLU. Many variations on this theme, including various sophisticated
training regimes, have been proposed (Chen et al., 2017; Gao et al., 2018; Li et al., 2018; Zhang et al.,
2019b; Chen et al., 2018), but no provable guarantees have been available for the performance of
such models on natural data distributions, until the present work.
2	Main Results
One motivation for GNNs as a target for progress in deep learning theory is that there are well-studied
graph distributions that plausibly capture some of the structure of real-world data (Drobyshevskiy &
Turdakov, 2019). For example, even fairly simple preferential attachment models plausibly capture
some of the essential structure of the web (Kumar et al., 2000). Other graph models naturally capture
community structures, the simplest of which is the Stochastic Block Model (SBM) (Holland et al.,
1983). A graph is sampled from a SBM by first partitioning vertices into communities (with fixed or
random sizes). Two vertices are connected with probability p if they belong to the same community
and probability q if they belong to different communities.
In this paper, we consider the case of an SBM with two equal-sized communities in which vertices
have label 0 and 1 respectively. We denote the label of vertex X by '(χ) ∈ {0,1}. The graphs
are parameterized as SBM(n, p, q) where n is the number of vertices, p is the probability of an
intra-community connection, and q is the probability of a cross-community connection. We allow
n to vary (but will require it to be sufficiently large), while P and q are of the form P = a Ionn and
q = b log n for some fixed constants a > b. In the semi-supervised setting, the community labels of
some portion of the labels are revealed. We assume the label of each vertex is revealed independently
with probability λ. The input-layer features at a vertex x is (0, 0) if its label is not revealed, (1, 0) if
its label is revealed to be 0, and (0, 1) if its label is revealed to be 1.
Assumption 2.1 (Sparse Stochastic Block Model). The probabilities of intra and cross-community
connections are P = a IQnn and q = b 嵋 n, where a > b are constants.
1Weisfeiler-Leman hierarchy is a polynomial-time iterative algorithm which provides a necessary but
insufficient condition for graph isomorphism.
2
Published as a conference paper at ICLR 2022
We study the problem of recovering the communities from such graphs using GNN models. Of
course, recovering the communities of an SBM graph has been well-studied and its computational
complexity is fully understood in most cases (Abbe & Sandon, 2015; Kawamoto et al., 2019). SBM
models are therefore a natural test-case for understanding the power of GNN models for learning
community structure, and experimental studies have been done in this setting (Chen et al., 2020b;
Yadav et al., 2019). (Abbe et al., 2014) shows a sharp threshold in the task of community recovery:
lθnn > √2. This threshold clearly holds for our case (at sufficiently large values of
n), since P = a log n, q = b log n and a > b. The contribution here is not to learn the community
models. Rather it’s showing that (multi-layer) GCNs solve the classification problem, which is very
much not trivial (it is non-convex, and the training loss curve is empirically non-monotonic).
Our GNN models will be trained on a graph or several graphs generated by the SBM(n, p, q) model,
and seek to understand their accuracy on arbitrary SBM(n, p, q) graphs not necessarily in the training
set but with the same parameters a, b determining p and q (with n allowed to vary).
In particular, we study spatial-based graph convolutional models along the lines of the Graph
Convolutional Networks (GCN) introduced in (Kipf & Welling, 2017). Each layer of the model
computes a feature vector at every vertex of an input graph based on features of nearby vertices in the
previous layer. A typical layer-wise update rule is of the form
X (k+1) = φ(Ax (k)w (k)),
where
•	A is a suitably-normalized adjacency matrix of shape n X n where n is the number of
vertices. Usually A includes self-loops.
•	X(k) gives the feature vector in the k-th layer at each vertex as a matrix of shape n × mk,
where mk is the number of features in layer k.
•	φ is an activation function, such as the ReLU.
•	W(k) are the trainable weights in the k-th layer, a matrix of shape mk × mk+1.
In our version of this model, We define A = η(；+勺)A, where A = A +1, A is the adjacency matrix
of a given graph, and I is the identity matrix. For the given SBM(n, p, q), a randomly selected vertex
has 2 (p + q) neighbors in expectation, so A is obtained by normalizing each row of A + I with
the average size of a neighborhood. Since very deep GCN models seem to provide little empirical
benefit (Li et al., 2018), we use a single hidden layer with a softmax output layer. Furthermore, we
introduce a bias term B at the second layer. So the model has the following form:
f (X, A) = SOftmax(Aφ(Ax W (O))W ⑴ + B)
=SOftmax( -2-^--2 Aφ(AXW (O))W ⑴ + B
(1)
n2 (p + q)
where X is the input feature of the graph and W(O), W(1) and B are trainable parameters. Let h
denote the number of hidden features, which equals the number of columns of W(O) and the number
of rows of W(1).
We define the accuracy of the model as the probability of predicting correctly the label of a single
vertex in a randomly generated SBM(n, p, q) graph where the label of each vertex is revealed with
probability λ. We can now state our main result.
Theorem 2.2. For any e > 0 and δ > 0, given a GCN model with δ ≤ h ≤ n hidden features
and with parameters initialized independently from N(0, 1), if training graphs are sampled from
SBM(n,p, q) with n ≥ max(Ω(ɪ )2, Ω( 1)) and the label ofeach vertex revealed with probability λ,
and ifthe model is trained by coordinate descent for k = O (log log ɪ) epochs, then with probability
≥ 1 - δ, the model achieves accuracy ≥ 1 - 4.
Remark. We treat λ as constants, so it is omitted in the big O and Ω notation in the sampling and
training complexity.
We emphasize that the novelty of this theorem is not in learning two-class SBM models as such;
this is a long-solved problem. Instead, this is the first proof of efficient learning for a GCN on
semi-supervised community detection tasks using a natural family of random graph models.
3
Published as a conference paper at ICLR 2022
3 Preliminaries
In this section, we first introduce notations (a table of notations is also shown in the appendix for
readers’ convenience) and some interpretations. Then we introduce the structure of the paper. Given
yy y y
a vertex y, denote the row of AX corresponding to y as (t0y , t1y), so ty0 and ty1 give the numbers of
neighbors of y (including perhaps y itself) with revealed labels in class 0 and class 1 respectively. Let
W (0) =	α01
α01
α?…
α2…
Then αit0y + α0it1y , 1 ≤ i ≤ h gives h features of vertex y in the hidden layer. The inner product
of the y-th row of Φ(AXW(0)) and the columns of W⑴ gives weighted sums of features of
y : Pih=1 βiφ(αit0y + α0it1y) and Pih=1 βi0φ(αity0 + α0ity1), where φ represents the ReLU function.
Given a vertex x, the row of Aφ(AxW(O))W⑴ corresponding to X is denoted by (f0(x), f1(x))
and is of the form
(n2(p+ q)2 X 1[y 〜x] X βiφ(αity +αity ), n2 (p+ q)2 X 1[y 〜x] X β'iφ(αity + αit1)),
y∈G	i=1	y∈G	i=1	(2)
where l[y 〜x] is equal to 1 if y and X are connected, 0 otherwise. Denote
f0i (x) :
4βi
n2 (p + q)2
E l[y 〜x]φ(αity+ɑity)
y∈G
f1i (x) :
4βi
n2 (p + q)2
E l[y 〜χ]φ(αity+αity),
y∈G
sof0(x) = Pih=1f0i(x) and f1(x) = Pih=1 f1i(x).
Denote gj (x) := fj (x) + bj, j = 0, 1, where (g0(x), g1(x)) represents the logit of the model
corresponding to x. Denote ∆(x) := g0(x) - g1(x). In order to make correct predictions, we need
∆(x) > 0 when '(x) = 0 and ∆(x) < 0 when '(x) = 1.
The bias term B is useful in our analysis because its derivative controls how imbalanced the current
loss is between the classes. In training we consider the cross-entropy loss denoted as L, and have
∂L	∂L	1
E[诉 ] = -E[诉 ] = - 5(E[Z∣'(x) = 0] — E[Z∣'(x) = 1]),
∂b0	∂b1	2
where Z = eχp 蓝*)+，：；(XgI)(X)) ∙ Z can be regarded as a measure of wrong prediction: the numerator
is the exponential of the output corresponding to the wrong label and the denominator is a normalizer.
It is easy to see that Z > 11 if the prediction is wrong; Z < ɪ if prediction is correct. When
∣E[ ∂∂L ]∣ ≈ 0, the model,s loss is balanced in the sense of that ∣E[Z |'(x) = 0] 一 E[Z∣'(χ) = 1]∣ ≈ 0.
In order to have balanced performance in every epoch, we train the model through coordinate descent
instead of conventional gradient descent. Specifically, in each epoch we first update b0 and b1 until
∣E[∂∂L]∣ is smaller than some threshold. Then we update the other parameters.
In order to make a learning guarantee of the model, we need a high probability estimation of ∆(x).
In Section 4, we show that ∆(χ) is concentrated at one of two values, denoted by μo and μι, for
'(χ) = 0 and 1 respectively. The proof depends on different parameter regimes of hidden neurons.
Furthermore, to avoid the overlap between the concentration range of ∆(x), we also show the
separation between μo and μι. In Section 5, We analyze the dynamics of hidden neurons throughout
training to show that the concentration and separation improve at a controlled rate. Based on this
information, in Section 6 we prove the main theorem. Section 7 shows some experimental results to
verify our theory. The paper ends with future directions in Section 8.
4 Concentration and Separation of Output
In this section we show that ∆(χ) is concentrated at μo and μι and their separation. The difference
of the logits is
4
Published as a conference paper at ICLR 2022
h
∆(x) = g0(x)	- g1(x)	=	f0(x) -	f1(x) +	b0	- b1 =	∆i(x)	+	b0	- b1,
i=1
where
∆i (x) = f0i (x) - f1i (x)
4(βi- β0)
n2(p + q)2
E l[y 〜x]φ(αity + αity).
y∈G
For brevity, we write ∆(x) as ∆ and ∆i(x) as ∆i. In order to estimate ∆, we need to estimate each
∆i, 1 ≤ i ≤ h.
We denote the high probability estimate of ∆ as μo and μι for '(χ) = 0 and 1 respectively. Our
fine-grained analysis of the dynamics of coordinate descent on GCNs relies on a classification of
neurons into three families based on the sign and scale of the parameters: “good type”, “bad type”
and “harmless type”. The names also indicate whether the neuron has positive contribution to the
value of μo - μ1. We show that “good type” neuron makes positive contribution; the contribution
of “bad type” neuron is negative but lower bounded; “harmless type” neuron’s contribution is non
negative (see Corollary A.4 and the remark following it). We will specifically describe parameter
regime of each type in the following subsections. We analyze the dynamics of these types throughout
coordinate descent in the next section. First we give some definitions.
Definition 1. For 1 ≤ i ≤ h, we call (αi, α0i, βi, βi0) the i-th neuron of the model, where (αi, α0i)>
is the i-th column of W(0), (βi, βi0) is the i-th row of W(1).
Definition 2. We say that the i-th neuron is order-aligned if (αi - α0i)(βi - βi0) > 0, otherwise we
say it is order-misaligned.
4.1 Classification of neuron parameter regimes
We say the i-th neuron is of “good type” if it satisfies either (G1) or (G2) below. (There is also the
symmetric case obtained by switching αi with α0i and βi with βi0 . For brevity, we only consider the
cases that αi > α0i. This applies to the “bad” and “harmless” types below as well). Neurons in this
type are order-aligned and both αi and α0i are positive or the ratio between αi and α0i is large enough.
αi > α0i > 0 and βi > βi0	(G1)
αi
α% > 0 > α√, —0 > 1 and βi > βi	(G2)
i α0i	i
We say the i-th neuron is of “bad type” if it satisfies either (B1), (B2) or (B3). Neurons in this type
are order-misaligned and αi, α0i are either both positive or have the opposite signs.
αi > α0i > 0	and	βi	<	βi0	(B1)
a > 0 > αi, —0	> —(1 + log 3 n)	and	βi	<	β0	(B2)
i α0i	p	i
αi > 0 > αi, αi ≤ q(1 + log-3 n)	(B3)
i α0i	p
We say that the i-th neuron is of “harmless type” if it satisfies either (H1) or (H2):
a > 0 > αi, —0 ∈ (-(1+ log 3 n), 1] and βi > β0	(HI)
i αi0	p	i
αi ≤ 0 and α0i ≤ 0	(H2)
4.2 Concentration and Separation
Theorem 4.1. If the i-th neuron is of “good type” satisfying (G1) or of “bad type” satisfying (B1),
then for '(x) = 0:
P[∣∆i - λ(βi- β0) [(p2 + q2)ɑi + 2pqαi]∖ ≤
(p + q)2
(ai - ai)(βi - βO)O(Iog 2 n)|'(X)= 0] ≥ 1 - θ(n2),
5
Published as a conference paper at ICLR 2022
for '(x) = 1:
P[A —：2 [2pqai + (P2 + q2)αi]∣ ≤
(p + q)2
(ai- ai)(βi- βO)O(Iog-2 n)|'(X) = 1] ≥ 1- O(n)∙
Similar concentration hold for neurons satisfying (G2), (B2) and (B3), and for neurons of “harmless
type.”
We apply the method of bounded differences to show the concentration. The details are shown in the
appendix.
Given the concentration of ∆i for each type of neurons, we estimate the concentration of the output
∆ = Pih=1 ∆i + b0 - b1. For the i-th neuron, we denote the high-probability estimate of ∆i given in
the statement of Theorem 4.1 as m0 when '(x) = 0 and ml when '(x) = 1. By union bound, We
have the following corollary.
Corollary 4.2. Given a vertex x ∈ G with label unrevealed, we have
P[∣∆ - μj | ≤ δ∣'(x)=j] ≥ 1 - O(-),	⑶
n
where
hh
μj = (Xmj) + b0 - bι, j = 0,1 δ = X |ai - a'i∖∖βi - β0lO(log-2 n).
For any e > 0, we require the probability of concentration in (3) to be at least 1 一 e, where e = o(e).
If we choose e = e2, then we set 1 一 O(n) ≥ 1 一 e2, i.e,n ≥ Ω(ɪ)2.OUr following analysis will be
based on this condition.
From Theorem 4.1, we have the following result about the value of mi0 一 mi1
Corollary 4.3.
• If the i-th neuron is of “good type” and satisfies (G1), then
m0- m1=λ∖αi- αi∖∖βi-β0∖( p-q).
• If the i-th neuron is of “bad type” and satisfies (B1), then
m0 一 m1 = -λ∖αi - αi∖∣βi — βi∖ (P	q).
p+q
• If the i-th neuron is of “harmless type” and satisfies (H1), then
m0 — ml = λ∖βi — βi∖∖pai + qαi∖ P q2 ∙
(p + q)2
Similar results for neurons satisfying (G2),(B2),(B3) and (H1) are stated in the appendix, along with
the proof.
Remark. • As we can see from Corollary 4.3, the value of mi0 一 mi1 is positive for “good
type” neurons, non-negative for “harmless type” neurons and may be negative (but lower
bounded) for “bad type” neurons. Since positive values of mi0 一 mi1 decrease the loss of the
model, this explains the names for the types of neurons.
•	mi0 一 mi1 is proportional to ∖αi 一 α0i∖∖βi 一 βi0∖. In the next section, we analyze the dynamics
of the parameters αi , α0i , βi , βi0 . Using our understanding of these dynamics, in Theorem
6.2 we present a refined result about the separation of output which only depends on the
initialization of parameters.
•	Let C := μo — μι = Ph=1(m0 一 ml). By the two corollaries above, we have δ = o(∖c∖).
The balanced loss guaranteed by the bias term and the coordinate descent scheme ensure
that μ0 = Ω(c) and μ1 = Ω(c). It then follows that if the loss is sufficiently small, both
μ0 and μ1 have correct sign, i.e. μ0 > 0 > μι. (Otherwise, due to concentration of the
output, the model makes wrong prediction and the loss is large). So we will eventually have
δ = o(μ0) and δ = o(∖μι∖).
6
Published as a conference paper at ICLR 2022
5	Dynamics of Parameters
In this section, we describe the dynamics of each type of neurons through coordinate descent, which
can be visualized in the following figure in which the arrows indicate movement between types that
can happen with non-negligible probability.
Figure 1: Dynamics of hidden neurons
There are two noteworthy points from this figure. First, “good type” parameters are preserved under
coordinate descent. Second, there are no arrows coming into “bad type” except from itself.
These dynamics are proved by estimating the gradient with respect to the loss function for each type
of neuron. Because of the non-linearity of the activation, we rely heavily on the concentration result
proved above to get tight estimates. Without these concentration results, even estimating the sign of
the gradient seems difficult. The proof and experiments about the dynamics of hidden neurons are
deferred to the appendix.
6	Learning Guarantee
In this section, we prove our main result which states that with high probability a trained GCN can
detect communities in SBM with any desired accuracy. The proof is based on the following theorem
which shows that if μo and μι are separated enough, then the model achieves high accuracy.
Theorem 6.1.	∀ > 0, provided that the difference between μo and μι is large enough:
σ(-μ0-μι) < I, if ∣E[∂∂L]∣ < I, then P[∆ < 0∣'(x) = 0] < 4e,P[∆ > 0∣'(x) = 1] < 4e,
where σ(x) := 1/(1 + exp (-x)) represents the sigmoid function.
Next we show that the model can achieve such separation between μo and μι through coordinate
descent. In order to make constant update of parameters at every epoch, we set an adaptive learning
rate ηk =旧彘力 where Z(k) is the value of Z at the k-th epoch. We first refine Corollary 4.3 about
the separation of output for each type of neuron (mi0 - mi1 ) using the dynamics of parameters.
Theorem 6.2	(separation of output). Let mi0 and mi1 be defined as in Section 4, train the model for k
epochs by the defined coordinate descent with adaptive learning rate ηk = 旧[)(劭],
•	if the i-th neuron is of “good type”, then
m - mi ≥ a*⑼2 (p-q ι+√8λ (p-q)T
•	if the i-th neuron is of “bad type”, then
m0- mi ≥-k((Ai0))2 + (Bi。))2) ^+^,
•	if the i-th neuron is of “harmless type”, then
mi。 - mii ≥ 0,
7
Published as a conference paper at ICLR 2022
where Ai(0) = αi(0) - α0i(0) , Bi(0) = βi(0) - βi0(0).
Next we present a result about initialization, which shows that with high probability, there are enough
“good type” neurons and parameters have appropriate scale.
Lemma 6.3. Suppose all parameters in W (0) and W (1) are initialized independently following
standard normal distribution. Then the number hg of neurons initialized as “good type” satisfies
P[hg ≥ 8] ≥ 1 — exp(-6h4). Furthermore,
h	1	h1
P[X(αi-αi) +(Bi-Bi) ≤ 5h] ≥ 1-O(h) , P[ X	lai-αillβi-βil ≥ 80] ≥ 1-O( h).
i=1	the i-th neuron
initialized as
“good type”
Now we can prove the final result.
Proof of Theorem 2.2. First we show that if the loss E[Z] is small enough, the model achieves desired
accuracy. Indeed, if E[Z] < 2, since
E[Z] = E[Z∣pred is Wrong]P[pred is Wrong]+E[Z∣pred is correct]P[pred is correct] ≥ ∣P[pred is wrong],
we have P[pred is wrong] ≤ 4, i.e., P[pred is correct] > 1 - 4.
Otherwise, E[Z] ≥ 2e, since E[Z] = 1 (E[Z∣'(z) = 0] + E[Z|'(z) = 1]), we have E[Z|'(z)=
0]+ E[Z |'(z) = 1] ≥ 4e. On the other hand, ∣E[ IbL ]∣ < C implies that ∣E[Z |'(z) = 0] - E[Z |'(z)=
1] < 2.
By Theorem 6.2,
h
μo - μι = X(S)	- ml) = X (m0	- ml)	+ X (m0	- ml)	+ X (m0	- mI)
i=1	i∈"good"	i∈"bad"	i∈"harmless”
≥ λ (p-qY(l + √2λ (p-qYYk X A(0)B(0)- kλp(p-∣) X ((A(0))2 + (B(O))2).
-2 pp +	8 pp +q) )	i∈ξoj,	(P+ N i⅛.
By Lemma 6.3, with probability ≥ 1 -O(I),
X AiO)Bi(O) ≥ ⅛, X ((Ai0))2 + (Bi0))2) ≤ 5h.
80
i∈"good"	ie“bad”
Since h ≥ 1, then with probability ≥ 1 - δ,
λ h( - Pp-q∖2Λ , √2λPp-q∖2Vk	i,5λp(p-q)
“0-μi ≥ h( ≡ (币)C + k (币)- - k (P + q)2
≥ h(C1(1 + C2)2k -C3k),
(4)
where C1 , C2 and C3 are constants determined by p,q and λ.
By Theorem 6.1, if (4)≥ 2 log ∣ (then σ(-μ-μ ) ≤ 2), then the model achieves accuracy ≥ 1 - 4c.
It’s sufficient to have
C1(1 + C2)2k - C3k ≥ 2log∣,
i.e. k = O(loglogɪ).	□
7	Experiments
We show some experiments verifying Theorem 2.2. In particular, our experiments demonstrate that
accuracy increases with n, the probability of high-accuracy models increases with h, and coordinate
descent is able to recovery high-accuracy models in the sparse regime of Assumption 2.1. Additional
plots demonstrating the dynamics of hidden neurons with their ratios and differences can be seen in
the appendix.
8
Published as a conference paper at ICLR 2022
Experiment 1 In this experiment, we plot the
an estimate of the accuracy versus epoch for
varying n. The parameters p, q of SBM follow
Assumption 2.1, where we choose a = 1.0 and
b = 0.7. We set h = 20, λ = 0.3 and run
40 independent experiments for n = 250, 500
and 1000 respectively. In each experiment we
train the model for 100 epochs. The training
set has 40 randomly generated graphs from
SBM(n, p, q). We validate the performance by
the percentage of correct predictions on 200 ran-
dom vertices, each from a randomly generated
graph. The result is shown Figure 2. The shaded
region for each n is obtained from the max, min
and mean percentage of the 40 experiments. The
result verifies Theorem 2.2 which shows that the
accuracy of the model increases with n.
Experiment 2 In this experiment, we show the
effect of the number of hidden neurons h. The
parameters of SBM are the same as Experiment
1. We set h = 2, 5, 20. For each pair of (n, h)
we run 40 independent experiments and show
the distribution of validation in Figure 3. From
the top row to the bottom, n increases from 250
to 1000. From the left column to the right, h in-
creases from 2 to 20. In each plot, the x-axis rep-
resents the accuracy, while y-axis represents the
count of experiments. According to Theorem
2.2, the probability of achieving high accuracy
is 1 - O(1/h) and the accuracy increases with
n. We can see that in each row of Figure 3, as h
increases, we have lager probability to achieve
high accuracy; in each column, as n increases,
the model achieves higher accuracy. The results
verify our theory in the paper.
Figure 2: Display of accuracy versus epoch for
varying n. Accuracy is computed on vertices
drawn from random SBM graphs. The accuracy in-
creases with epoch, and we also see that the lower
bound on the accuracy increases with n, as ex-
pected from our theory.
Figure 3: Display of accuracy for varying n and h.
For each row, n is set to be 250 (top), 500 (middle),
1000 (bottom); for each column, h is set to be 2
(left), 5 (middle), 20 (right). Probability of failure
decreases as h increases, and accuracy increases
with n.
8	Future Directions
Graph neural networks offer a promising setting
for progress on the more general theory of deep
learning, because random graph models more
plausibly capture the structure of real-world data
compared to, e.g., the Gaussian inputs often used to prove deep learning guarantees for traditional
feed-forward neural networks. This paper has initiated the project of proving training guarantees for
semi-supervised learning using GCNs on SBM models, but much more work remains to be done.
Arguably the sparsest SBM models (expected constant degree) are the most compelling from the
perspective of modeling real-world communities, so it would be interesting to extend these results
to that setting. Models with more than two blocks, or overlapping communities (Petti & Vempala,
2018) would be even closer to real-world structure. We hope this initial step spurs further interest in
provable guarantees for training neural networks using plausible models of real-world data as the
input distribution.
9	Acknowledgement
This research was supported in part by NSF Grant 1849796. The author would like to thank Prof.
John Wilmes for guiding the research, and also thank Prof. Tyler Maunu and Prof. Pengyu Hong for
making the results more generalized and designing the experiments.
9
Published as a conference paper at ICLR 2022
References
Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic block model
without knowing the parameters. arXiv preprint arXiv:1506.03729, 2015.
Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block
model, 2014.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In International conference on machine learning, pp. 605-614. PMLR, 2017.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. arXiv preprint arXiv:1710.10568, 2017.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. arXiv preprint arXiv:1801.10247, 2018.
Sitan Chen, Adam R. Klivans, and Raghu Meka. Learning deep relu networks is fixed-parameter
tractable, 2020a.
Zhengdao Chen, Xiang Li, and Joan Bruna. Supervised community detection with line graph neural
networks, 2020b.
Mikhail Drobyshevskiy and Denis Turdakov. Random graph modeling: A survey of the concepts.
ACM Comput. Surv., 52(6):1-36, December 2019.
D.P. Dubhashi and A. Panconesi. Concentration of Measure for the Analysis of Randomized
Algorithms. Cambridge University Press, 2009. ISBN 9781139480994. URL https:
//books.google.com/books?id=UUohAwAAQBAJ.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 1416-1424, 2018.
Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. In International Conference on Machine Learning, pp. 3419-3430. PMLR,
2020.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. CoRR, abs/1711.00501, 2017.
Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans. Superpolynomial
lower bounds for learning one-layer neural networks using gradient descent. In International
Conference on Machine Learning, pp. 3587-3596. PMLR, 2020.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First
steps. Social networks, 5(2):109-137, 1983.
Tatsuro Kawamoto, Masashi Tsubaki, and Tomoyuki Obuchi. Mean-field theory of graph neural
networks in graph partitioning. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):
124007, dec 2019. doi: 10.1088/1742-5468/ab3456. URL https://doi.org/10.1088/
1742-5468/ab3456.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks,
2017.
R. Kumar, P. Raghavan, S. Rajagopalan, D. Sivakumar, A. Tomkins, and E. Upfal. Stochastic models
for the web graph. In Proceedings 41st Annual Symposium on Foundations of Computer Science,
pp. 57-65, 2000. doi: 10.1109/SFCS.2000.892065.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
10
Published as a conference paper at ICLR 2022
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In
Proceedings ofthe AAAI Conference on Artificial Intelligence, volume 33,pp. 4602-4609, 2019.
Samantha Petti and Santosh S. Vempala. Approximating sparse graphs: The random overlapping
communities model, 2018.
Ryoma Sato. A survey on the expressive power of graph neural networks, 2020.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation ratios of graph neural networks
for combinatorial problems. arXiv preprint arXiv:1905.10261, 2019.
Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks: Poly-
nomial convergence and sq lower bounds. In Conference on Learning Theory, pp. 3115-3117.
PMLR, 2019.
Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural
networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4-24, 2021.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Prateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth, Arun Rajkumar, and
Partha Talukdar. Lovasz convolutional networks. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 1978-1987. PMLR, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu
networks via gradient descent. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1524-1534. PMLR, 2019a.
Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. Bayesian graph convolutional
neural networks for semi-supervised classification. Proceedings of the AAAI Conference on
Artificial Intelligence, 33(01):5829-5836, 2019b.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
arXiv:1812.08434, 2018.
A Concentration and Separation of Output
Let N (x, 0), N(x, 1) denote the neighborhood of vertex x with label 0 and 1 respectively, i.e.
N(x, 0) = {y ∈ G,y 〜x, '(y) = 0}, N(x, 1) = {y ∈ G,y 〜x, '(y) = 1}. By the definition
of SBM, both |N(x, 0)| and |N(x, 1)| are binomial random variables. For '(x) = 0, |N(x, 0)| 〜
B(2,p),∣N(x, 1)| 〜B(2,q) and for '(x) = 1, |N(x, 0)| 〜B(n,q),∣N(x, 1)| 〜B(2,p). More-
over, ty and t1 are also binomial random variables, for '(y) = 0,t0 〜B(n2λ,p),t?〜B(n2λ,q),
similarly for '(y) = 1. Our following analysis is based on the condition that |N(x, 0)|, |N(x, 1)∣,tχ
and t1x are in their high probability range for all x ∈ G. Specifically we require the condition that for
all '(x) = 0, (similar conditions for '(x) = 1 are omitted):
∣n (χ, o)i- np
x nλp
tχ - F
≤ O(np)5, |N(x, 1)| — 等 ≤ O(nq)6；
≤ O(np)5, tχ — nλq ≤ O(nq)5.
(Cond)
By tail bound of binomial random variables and union bound, we have
P[(Cond)] ≥ 1 — ɪ.
n2
Under this condition, we show the concentration of ∆i for each type.
11
Published as a conference paper at ICLR 2022
A.1 "Good Type" neurons
For convenience, according to the activation pattern of φ(αity + α"?), we further divide (G2)
into subcases (G2,1), (G2,2) and (G2,3) by to the ratio of ∣箸∣. For example, in (G1) and (G2,1),
φ(αi尾 + αity) is active for both '(y) = 0 and '(y) = 1; in (G2,1), it is only active for '(y) = 0.
R >p(1+log-3 n)	(G2,1)
αi q
1 < —i Vp(I - log-3 n)	(G2,2)
αi q
P(1 — log-3 n) ≤ —0 ≤ P(1 + log-3 n)	(G2,3)
q	αi	q	,
We have the following estimation of ∆i in “good type”.
Theorem A.1 (concentration of output from “good type” neurons). Ifthe i-th neuron is of “good
type”, then
•	in both (Gi) and (G2,1):
P[∣∆i — λ(P+ q[ [(p2 + q2)αi + 2pqαi] ∣ ≤ Si — R)(βi — βi)O(log-2 n)∣'(x)= 0]
≥ 1 - O(~2 )
n2
P[A — :β+ qβi [2pqα + (p2 + q2 )αi] ∣ ≤ 3 — «i )(βi-βi)o(log-2 n)∣'(χ) = 1]
≥ 1 -。(3 i
n2
•	in( G2,2):
P[∣∆i — λ(βi;qβ2iP (Pai + qαii ∣ ≤ @ — αri)(βi — βi)O(log-2 n)∣'(x) = 0]
≥ 1 -。(3 i
n2
P[∣∆i — λ(βi] β2iq(Pai + qaii ∣ ≤ & —此⑼—βiiO(log-2 ni∣'(xi = 1]
(P + qi2
≥ 1 —。( ~2 i
n2
•	in( G2,3):
P[A —	")	βp(Pai+ qαi)∣	≤	(ai	-	ai)(βi	— βi)o(log-2 n)∣`(χ)	= 0]
(P + qi2
≥ 1 —。( ~2 i
n2
P[∣∆i — λ(βi[ β2iq(Pai + qaii ∣ ≤ & —此⑼—βiiO(log-2 ni∣'(xi = 1]
(P + qi2
≥ 1 — 0(~2 )∙
n2
12
Published as a conference paper at ICLR 2022
Proof. We have ∆i = (βi - βi) Py∈G l[y 〜x]"相机+晶々). We apply the method of averaged
bounded difference (Dubhashi & Panconesi, 2009) to estimate ∆i . In different parameter regimes,
φ(αit0y + αi0 ty1) has different activation patterns.
In (Gι) and (G2,1), φ(α%ty + α[tIy) is active with probability 1 - O(去)for both '(y) = 0 and
'(y) = 1. For '(x) = 0, at first we estimate E[∆i]. By condition (Cond):
e[δ∕------JΓ^-Y^^2JΓ [(p2 + q2)αi + 2pqαi] ≤ (αi - αi)(βi - AO)O(Iog 1 n).
(p + q)2	i	i	i
yj	0 yj
Let Yj =	"n2(p+q)21), then ∆% = (βi - βi) Pj Yj. Based on condition (Cond), Iyj -
2λ⅞Xqαi' I ≤ (αi - α)O(log-2 n) for '(yj) = 0. For any ak, ak,
7
ElYkIY1,…，Yk-1, Yk = ak] - ElYkIY1,…，Yk-1, Yk = ak] ≤ (αi - αi)O(log 2 n).
Moreover, when the number of vertices with revealed labels are fixed,
E[Yj∣Yι, ∙∙∙ ,Yk-i ,Yk = a®] - E[Yj∙ |Y1,…，K-i,K = ak] ≤ a - αi)O(log-6 n),
forj ≥ k. By condition (Cond), there are at most O(log3 n) non-zero terms for Yk, 1 ≤ k ≤ n. So
E[X Yj|YV-Yk-i,Yk	=	ak]	-	E[X YjIYV-YkfYk	=	ak ]	≤ (% -	αi)O(log--	n),
Ij	j	I
for 1 ≤ k ≤ n. By the method of averaged bounded difference, we have
P[∣∆i -λβ+-β⅛p2+q2)αi+2pqαi]∣ ≤ @-必)(民-βi)O(log n)-2 |'(x) = 0] ≥ 1-O(n2).
Other regimes can be proved similarly.	□
A.2 “Bad Type” Neurons
For convenience of our analysis, we further divide (B2) into subcases (B2,1), (B2,2) and (B2,3)
according to the ratio of ∣ 言 ∣
αi > p(1+log-3 n)	(B2,1)
∣ α0i ∣	q
αi
α0i
∈ (q(1 + log-3 n), P(1 - log-3 n)]
0i∈(P(I-log-1 n),p(I+log-3 n)]
(B2,2)
(B2,3)
We have the following estimation of ∆i in “bad type”.
Theorem A.2 (concentration of output from “bad type” neurons). If the i-th neuron is of “bad type”,
we have:
•	in (B1) and (B2,1):
P[A - λ(β] βi [(p2 + q2)αi + 2pqαi]∣ ≤ |ai - α'i∖∖βi - βilO(log-2 n)|'(X) = 0]
(p + q)2
≥ 1 - O(3)
n2
P[∣∆i - λ(β; β [2pqαi + (p2 + q2)αi]∣ ≤ ∖αi - αi∖∖βi - βi∖O(log-2 n)∖'(x) = 1]
(p + q)2
≥ 1 - O(~2 )
n2
13
Published as a conference paper at ICLR 2022
• in(B2,2 )：
P[∣∆i - ”二叱 P (Pai + qai)∣ ≤ ∣αi - α'i∖∖βi - β∣O(log-2 n)∣'(x) = 0]
1	(p + q)2	1
≥ 1 -。( -2 )
n2
P[∣∆i - lβi[ β2”(Pai + qai) ∣ ≤ ㈤-αi∣∣βi - βi∣O(log-2 n)∣'(x) = 1]
1	(p + q)2	1
≥ 1 -。( -2 )
n2
• in( B2,3 )：
P[∣∆i -甘；β) (Pai + qαi) ∣ ≤ 旧-ai∣∣βi - βi∣O(log-2 n)∣'(x) = 0]
(P + q)2
≥ 1 -。( -2 )
n2
P[∣∆i - λβ- β2”(Pai + qai) ∣ ≤ ㈤-ai∣∣βi - βi∣O(log-2 n)∣'(x) = 1]
(P + q)2
≥ 1 - O(~2)•
n2
• in( B3):
P[|A ∣ ≤ ∣ai - ai∣∣βi - βi∣o(log 2 n)W(X) = 0 or 1] ≥ 1 - 0(n).
Proof. The proof is similar as Theorem A.1	□
A.3 "Harmless Type" Neurons
We have the following estimation of ∆i in “harmless type”.
Theorem A.3 (concentration of output from “harmless type” neurons). If the i-th neuron is of
“harmless type”, we have:
• in (Hi):
P[∣∆i - λ∕βi[ β) (Pai + qai) ∣ ≤ @ - ai)(βi - βi)O(log-2 n)∣'(x) = 0]
(P + q)2
≥ 1 - O(~2 )
n2
P[∣∆i - λβ- β2”(Pai + qai) ∣ ≤ & - a'i)(βi - βi)O(log-2 n)∣'(x) = 1]
(P + q)2
≥ 1 - O(~2 )
n2
•	in (H2): ∆i = 0 for both '(x) = 0 and 1.
A.4 Separation of Output
Previous subsections have shown the concentration of ∆i for each type of neurons. For the i-th
neuron, we write the concentrated value as m0 if '(x) = 0 and mi if '(x) = 1. From Theorem
A.1, A.2 and A.3, we have the following result about the value of mjɔ - mɪ by straightforward
computation.
14
Published as a conference paper at ICLR 2022
Corollary A.4. We have thefollowing result about m0 — mj for 1 ≤ i ≤ h:
•	ifthe i-the neuron is of “good type”:
in (Gj) and (G2」)：m0 — mj
λ∖αi-αi∖∖βi—β;∖( p≡q)
in (G2,2) ： m0 — mj
λ∖βi 一 βi°∖∖pαi + qαil
> 2 ∖αi — αi∖∖βi — βi∖
in (G2,3) ： m0 — mj
λ∖βi 一 βi∖∖pai + qαi∖
P — q
(p + q)2
p — q )2
p + qj
P — q
(p + q)2
> λ∖αi—αi∖∖βi—β0∖	,
where『(Imi
•	ifthe i-the neuron is of “bad type”:
in (Bi) and (B2,1) : m0 — mj
in (B2,2) ： m0 — mj
in (B2,3) ： m0 — mj
αi - αi∖∖βi — βi∖ (P	q)
pp + qj
0	0 p—q
βi — βi∖∖pai + qai∖ (P + q)2
∕∣∣n	n∕∣ (p - qW
αi - ai∖∖βi — βi∖ (P + q)2
βi ― βi∖∖pai + qai∖ (PJ q2
(p + q)2
∕∣∣n	n∕∣ (p - qA
αi ― αi∖∖βi ― βi∖ (P + q)2
—λ
—λ
> —λ
—λ
> —λ
in (B3) : m0 — mj
0,
where Λj = (j+log-4n)p2-q2, Λ3 = (Iig-3「储-q2
α+lQg 3 n)p+q	(i-log 3 n)p+q
•	ifthe i-the neuron is of “harmless type”:
in (Hj) ： m0 — mj
=λ∖βi ― βi∖∖Pai + qαi∖ (PJ q2
(p + q)2
> λ∖αi - αi∖∖βi — βi∖ (P /Y
(p + q)2
where Λ5
in (H2) ： m0 — mj
Pq log 3
p+(j+log-3 )q
0,
B Dynamics of Parameters
We consider the cross-entropy loss in training. The loss on a particular vertex x is
L(X) = Tog Q(X)(X),
where O0 (x) and Oj (x) are the first and second component of the output respectively, i.e.
oo(χ) =	，exp"x∖、、, OI(X)=	，exp"(X):、、.
exp(go(x)) + exp(g(x))	exp(go(x)) + exp(g(x))
15
Published as a conference paper at ICLR 2022
For a given graph G generated by SBM, we set the objective function L(G) as the average loss over
all the vertices with revealed labels2, i.e.
L(G)
1
#{x ∈ G : '(x) is revealed}
L(x).
x:'(x) revealed
We first show the partial derivatives of parameters.
Theorem B.1 (derivatives of parameters). For 1 ≤ i ≤ h, let X be a vertex, '(x) its true label,
L(X) = - log O'(x)(x), then
∂L =	2(	)、2	(βi	- βi)z(-i)j(x Xi[y	〜X]i[α,ty +	αity	≥	o]ty
∂αi	n (p + q )	y
∂L = n2(p4+q)2 (βi - βi)Z (-1)I ⑺ X i[y ~ X]i[α,ty + αity ≥ 0]ty
IL = 2( )、2Z(-1) j(X) Xi[y 〜x]φ(αity + aity)
∂βi	n2 (p + q)2
∣L0 = - 2( )、2 Z(-1) j(X) X i[y 〜^]φ(aity + aity)
∂βi0	n2 (p + q)2
where Z
∂L
∂b0
∂L
∂b1
(-1)1-'(X)Z
(-1)'(X)Z,
exp(gι-'(χ)(x))
exp(g0(x))+exp(g1(x))
, t0y and t1y are the numbers of neighbors of y(including perhaps y
itself) with revealed labels in class 0 and class 1 respectively.
Proof. We compute ∂∂L, ∂∂L and IbL, others can be computed symmetrically. We have
L(X) = - log O'(χ)(x) = log(exp(g0(x)) +exp(gι(x))) - g'(χ)(x),
since Oj (X)
eχpSj(X))	j
eχp(go(χ))+eχp(gι(χ)), J
0, 1. So
dL_eg0(X)铲 + eg1(X) ⅛)	∂g'(X)(x)
-	-
∂αi	eg0(X) + eg1(X)	∂αi
(-1)1-'(X)Z (dg0(X)
∂αi
∂gι(x)
∂αi
Since gj (x) = fj (x) + bj, ⅜(X) = djΓ，j =。，L By (2)
∂f0(x)
∂αi
∂fl(x)
∂αi
n2(p+ q)2 X l[y 〜x]βil[aity + αity ≥ 0]ty
n2(p4+ q)2 X 1[y ~ x]β01 [αity + αity ≥ 0]ty.
Therefore
∣L = 2( L 、2 (-1)1-'(X)Z(βi - βi) X i[y 〜X]i[αity + at ≥ 0]ty.
∂αi	n (p + q)	y
2We abuse the notation L for L(x) and L(G), but the meaning is clear from the context.
16
Published as a conference paper at ICLR 2022
Next we compute 需.Similar as above,需=(-1)1-'(x)Z(d∂0(x) - d∂1(x)). By (2)
ɪ OPi	OPi	'	/	∖ OPi	OPi ' J
∂fθ(x)
~ββΓ
∂f1(x)
∂αi
4 L 「	…U / y、
n2(p + q)2〉」ɪ[^ 〜x]φ(ait0 + ait1)
0.
So
IL = 2( )、2(-1) j(X)ZXl[y 〜x]φ(αit0 +。筮).
∂βi	n2(p + q)2	Y
Lastly,
dL = (_I)I-'(χ)Z (彻。(X) — dgι(X) A
嬴=L)	∖b~∂b-	∂b^ J
=(-1)1-'(χ)Z,
since
Ogo(χ)
Obo
1,
Ogι(χ)
Obo
0.
□
In the following, we will use Theorem B.1 to analyze the dynamics of neurons of each type. As
we can see, all of 段,第,OL and 养 have the form YZ. In order to estimate these derivatives,
we show the concentration of Y and Z respectively. To estimate the concentration of Z, we need
the concentration of output obtained in Section 4. For any e > 0, we require the probability of
concentration in (3) to be at least 1 - e, where e = o(e). In particular, if we choose e = e2, then we
set 1 - O(~) ≥ 1 - E2, i.e.
n ≥ Ω
(5)
Our following analysis will be based on this condition.
Meanwhile in order to have balanced performance in each epoch of coordinate descent, we require
∣E[IL]∣ < I. SinceE[OL] = ɪ(-E[Z∣'(x) = 0] + E[Z∣'(x) = 1])),wehave
∣E[Z∣4(x) = 0] - E[Z∣4(x) = 1]∣ <2
(6)
We have the following relation between μ0 and μι. In the following, σ represents the sigmoid
function: σ(x)=可'.
Proposition B.2. If ∣ E[Z∣2(x) = 0] - E[Z∣`(x) = 1] ∣ < E, then ∣σ(-仰)一σ(μι)∣ ≤ σ0(μ0 — δ)δ +
σ0(μι + δ)δ + 3E, where δ is as shown in Corollary 4.2.
Proof. We have
Z= ∫σ(-Δ), '(x) = 0
=[σ(∆),	'(x) = 1
For '(x) = 0, by Lagrange mean value theorem, ∣σ(-μo) - Z∣ = ∣σ(一μ0) - σ(-∆)∣ = σ0(ξ)(∆ -
μo), where ξ is between -μ0 and -∆. By Corollary 4.2 and the condition of n, ∣∆ - μo ∣ ≤ δ with
probability ≥ 1 - E. From the remark following Corollary A.4, we have σ0(ξ) ≤ σ0(-μo + δ)=
σ0(μo - δ),3 with probability ≥ 1 - E. Then we have
P[∣σ(-μo) — Z∣ ≤ σ0(μo — δ)δ∣`(x) = 0] ≥ 1 — E.
Since
E[Z∣2(x) = 0]= E[Z∣∣σ(-仰)-Z∣ ≤ σ0(μ° - δ)δ]P[∣σ(-网)-Z∣ ≤ σ0(μ° - δ)δ]
+ E[Z∣∣σ(-μo) - Z∣ > σ0(μo - δ)δ]P[∣σ(-μ°) - Z∣ > σ0(μ° - δ)δ],
3σ0(x) is even.
17
Published as a conference paper at ICLR 2022
then (note that 0 < Z < 1)
E[Z∣2(x) = 0] ≤ σ(-μo) + σ0(μo — δ)δ + e
and
E[Z∣2(x) =0] ≥ (σ(-μo) - σ0(μo - δ)δ)(1 -已
i.e.
E[Z∣2(x) = 0] — σ(-μo) ≤ σ0(μo — δ)δ + Z
and
e[z∣2(x) = 0] - σ(-μo) ≥ -σ0(μo - δ)δ - e(σ(-μo) - σ((μo - δ)δ)
=-σ 0 (μo — δ)δ(1 — Z) — zσ(-μo)
≥ —σ0 (μo — δ)δ — Z.
So
∣E[Z∣2(x) = 0] — σ(-μo) ∣ ≤ σ0(μo — δ)δ + e.
Similarly
∣E[Z∣2(x) = 1] - σ(μι)∣ ≤ σ0(μι + δ)δ + Z.
By triangle inequality,
∣σ(-μo) - σ(〃ι) ∣ ≤ ∣σ(-〃o) - E[Z∣4(x) =0]∣ + ∣ E[Z∣4(x) = 0] - E[Z∣4(x) = 1] ∣
+ ∣E[Z∣4(x) = 1] - σ(μι) ∣
≤ σ0(μo — δ)δ + σ0(μι + δ)δ + 3Z.
□
From the proof above, We can directly obtain the following corollary about Z.
Corollary B.3.
P[ ∣ Z - E[Z∣`(x) = 0] ∣ ≤ 2σ0(μo - δ)δ + Z∣`(x) =0] ≥ 1 - e
P[ ∣ Z - E[Z∣`(x) = 1] ∣ ≤ 2σ0(μι + δ)δ + Z∣`(x) = 1] ≥ 1 - e.
In order to obtain the concentration of Z, we need to estimate σ0(μo 一 δ)δ and σ0(μ1 + δ)δ. The
following proposition is based on the condition that ∣μo + μι∣ ≥ 4δ. If ∣μo + μι∣ < 4δ, set
C := μo - μι, we have μo > C - 2δ and μι < -C + 2δ. Then the concentration of output shown in
Corollary 4.2 can guarantee 1 - E accuracy of the model for any e > 0. In fact, from ∣∆ - μo∣ < δ,
we have ∆ > μo - δ > C - 3δ > 0, due to δ = o(c). So
P[∆ > 0∣`(x) = 0] ≥ P[∣∆ - μo∣ < δ∣`(x) = 0] ≥ 1 - e.
Similarly,
P[∆ < 0∣`(x) = 1] ≥ P[∣∆ - μo∣ < δ∣`(x) = 1] ≥ 1 - e.
Since z = o(e), the model achieves overall accuracy ≥ 1 - e.
Proposition B.4. If ∣μo + μι∣ ≥ 4δ ,then σ0(μo — δ)δ = O(Z), σ0 (μι + δ)δ = O(Z).
Proof. First, we estimate the lower bound of ∣σ(-μo) - σ(μ1)∣ via the Fundamental Theorem of
Calculus. We have ∣σ(-μo) — σ(μι)∣ = ∣ ʃ-ʌθ σ0(t) dt∣.
If -μo < μι < 0, since μo + μι ≥ 4δ, we divide the interval [-μo,μι] into [-μo, -μo + 2δ] U
[-μo + 2δ, μ1 - 2δ] U [μ1 - 2δ, μ1] and estimate the lower bound of the integral. Since σ0(x) is
increasing on (-∞, 0], we have
”1
I	σ0(t) dt ≥ σ0(-μo) ∙ 2δ + Ii + σ0(μι — 2δ) ∙ 2δ,
J—μo
where Ii = R—；-2^ σ0(t) dt. If μι < -μo < 0, similarly we have
f	σ0(t) dt ≥ σ0(μι) ∙ 2δ + I2 + σ0(-μo — 2δ) ∙ 2δ,
J μι
⑺
(8)
18
Published as a conference paper at ICLR 2022
where I2 = f~1+0-δ2δ σ0(t) dt. We have a uniform lower bound from (7) and (8):
I /	σ0(t)	dt ≥	σ0(-μo	— 2δ)	∙	2δ +	σ0(μι	—	2δ)	∙	2δ + I,	(9)
I J-μ
where I = min{I1,I2}.
Furthermore, by Proposition B.2,
∣σ(-μo) — σ(μι)∣ ≤ σ0(μo — δ)δ + σ0(μι + δ)δ + 3e.	(10)
Combine (9) and (10):
2σ0(-μo — 2δ)δ + 2σ0(μι - 2δ)δ ≤ σ0(-μo + δ)δ + σ!(μι + δ)δ + 3e.	(11)
By Lagrange mean value theorem,
σ0(-μo — 2δ) = σ0(-μo + δ) — 3σ00(ξo)δ
σ0(μ1 — 2δ) = σ0(μ1 + δ) — 3σ00(ξι)δ,
where ξo ∈ (-μo 一 2δ, -μo + δ), ξ1 ∈ (μ1 - 2δ, μ1 + δ). Plug these into (11):
σ0(μo - δ)δ + σ0(μι + δ)δ - 6δ2(σ"(ξo) + σ00(ξι)) ≤ 3∈∙
Since δ2(σ00(ξo) + σ00(ξ1)) = o(σ0(μo - δ)δ) and o(σ0(μ1 + δ)δ), we have
σ0(μo - δ)δ = O(e)
σ0(μι + δ)δ = O(e).
□
Combine Proposition B.4 and Corollary B.3, we have the following concentration of Z.
Proposition B.5.
P[ I Z - E[Z∣`(x) = 0] I ≤ O(e) ∣'(x) =0] ≥ 1 - e
P[ i Z - E[Z∣`(x) = 1] i ≤ O(e)∣`(x) = 1] ≥ 1 - e.
Under the condition of balanced performance, we have the following corollary about the concentration
of Z independent of the label of x.
Corollary B.6. If ∣ E[IL] ∣ ≤ W, then P[∣Z - E[Z] ∣ ≤ O(<≡)] ≥ 1 - W.
Proof. Since E[IL] = ɪ(E[Z∣2(x) = 0] - E[Z∣`(x) = 1]), we have
∣E[Z∣2(x) =0] - E[Z∣`(x) = 1] i ≤ W.
On the other hand,
E[Z] = J (E[Z ∣2(x) = 0] + E[Z ∣`(x) = 1]).
So we have ∣ E[Z] - E[Z∣`(x) = 0] ∣ ≤ ∣. By Proposition B.5, P[∣Z - E[Z]∣ ≤ O(W)] ≥ 1 - W. □
Now we can derive the estimation of the derivatives.
Theorem B.7 (concentration of derivatives). For loss on the whole graph L = L(G), with probability
≥ 1 — O(ɪ), we have 4
1. If αi > ai > 0 or α >		0 > ai,	当∣ ≥ αi1 一	P (1 + log-		1 n), then			
	∂L +	+ (βi ∂ɑi	一的||	pp - q 、p + q	)E[Z]	≤	∣βi	-β'i ∣E[Z]O(log-	2 n)	(12)
	生一(β ∂αi (βi	-βi) ||	'p - q pp + q	)E[Z]	≤	∣βi	-β0∣E[Z]O(log-	2 n)	(13)
	∂L 而+(ai	-ai) |l	'p - q pp + q	)E[Z]	≤	∣αi	-ai∣E[Z]O(log-	-2 n).	(14)
4 ¾ir>dL dL	——	_ dL (Qpp ThfICrPm	R 1、WfI cnlv ŋpprl	tɑ PSHm∩fρ dL
口UiCe ∂β0	——	—∂β , (see ιueuιeιm	jj.ι), we onJy UeeU	Io esiumaie Qβ	.
19
(61)
T次。I)O I
-‰oε + N邕(试—笆I出 Q
⅛⅞< ⅞∕⅛.
b+a≡m—M£——I) b+a≡mlω>OI+I) b+a≡m lω>OI+I)
-\——Nd7-1-——MOTI) H mV WD MηOI bd H ZVe \——zd(eH"——M-+I) H IV a/-H
(81) V (eτu)⅛o I (Zlk⅜l.÷r)Nad— 3) —
■ (eRɑ)o +⅞l¾¾N邕(；0lW ⅛
E-s.(e3院。1 + I)s⅛lo∙一e3院。7 I)空出一强-dAoA 3 工.8
(匚)？τgoN直ð—宜Vlɪa(ðl:kll:wlrdw + 黑
(91)eτ次。DoN直过—3SVIgg匕十门”(⅞rls+ +
I (Dl a) DY ae
(W) 0心£。1)0目直送—这7|N邕 QbLrl⅜(⅜Γl¾o +幕
I LD — a)aγ Qe
Ua1I — (U——O 一
— I) Z(f⅛lo∙)eglu>o U W IaHM .-eg—MOI— 1)2不 + 1)2 出 d< O < 3⅛z
ZZOZ xuOIaJOdEd əɔuəjəjus E SE poqs=qnd

(X)-(eτ号)o+ Na⅛rlBl
一 (eTgO I Z (耍)UN邕(¾ΓI≤I出⅛⅛l⅛⅛
§ -(S小 ⅛)o +-l⅞Lrl⅜)N⅛⅜Γls
彳一 (Dl a)DYJ
■ (e-‰oε +」王)O STi 出 ⅛
S -(S加 ⅛)o+"b Lrl⅜)N⅛⅜ΓlS1
L L LΙJκ 一
. (eTU)ODO I」亚)O STi 出 M
⅛⅞Vl6
(IZ)V (eTgo— (I^)UNa¾rlSl
. (eTgO + (正)Y) I7≤±⅛l⅛
/ N/// bb
(国
(J‰OI)O +
」正)ON豆T笆
.(eTbB。DO +τl⅞¾STS 出
76
⅛
Published as a conference paper at ICLR 2022
4. If ai > 0 > ai, Iαi∣ ≤ P(1+log-3 η),βi < βi, then
∂L - ∂L ≥-ιβi - βim
αi	αi
(25)
∂L ≤ O®.
βi
(26)
Proof. We show the proof for item 1, other items can be proved similarly. Since L(G) is the average
of the losses over revealed vertices, we first show the concentration of dL(X), then we show the
,	∂αi ,
concentration of dL0G) using union bound. Since
dLx) = (-1) j(x)4(βi-βi)Z X
∂αi	i y
l[y 〜x]l[aity + aity ≥ 0]t0
n2(p + q)2
we first show the concentration of Y
(-1) 1-'⑺ Pyy
41[αity +αity≥0]ty
n2 (p+q)2
method of averaged bounded difference. Similar as the proof of Theorem A.1,
(-1)1-`(x)41[aitn2+P+q)2≥0]t0j. Based on Condition (Cond), for '(x) = 0, |Yj + -
,_7、/、
O (log 2 n) for '(yj) =0. Similar results hold for '(yj) = 1,'(x) = 1. So for any ak, ak,
using the
, let Yj =
2λP | ≤
n(p+q)2 | 一
E[X Yj∣Y1,…，Yk-i,Yk = ak] - E[X Yj∣Y1,…，Yk-i,Yk = a'k] ≤ Iai- αi)O(log-2 n).
j
j
By method of averaged bounded difference, for '(x) = 0,
P['(X=0Yj+λ( G )2
≤ O(log-2 n)] ≥ 1 — exp (—2 log3 n) ≥
1 - 3.
n2
Similarly
P[MJ+λ( G )2
≤ O(Iog-1 n)] ≥ 1 - W.
Hence
P[Y +
λ(p2 + q2)
(p+q)2
≤ O(log-1 n)] ≥ 1 - n12.
By Corollary B.6, P[|Z - E[Z]| ≤ O(<≡)]
≥ 1 - KsoWe have
2
P[
dLT + (βi - β0)λ
p2 + q
(p+q)
2E[Z]
≤lβi-βilE[Z]O(Iog-2 n)1'(X)=0]≥ 1 - O(n2).
For '(x) = 1, similarly we have
P[
dLT -—入总E[Z]
≤lβi-βilE[Z]O(Iog-2 n)1'(X) = 1]≥ 1 - O(n2).
By union bound, we have (12). (13) and (14) can be proved similarly.
□
Using Theorem B.7, we can analyze dynamics of neurons of each type. First, we introduce some
notations. Let ηk denote the learning rate at the k-th epoch, Z(k) be the value of Z at the k-th
epoch, αi(k) be the value of αi at the k-th epoch, similar for αi0(k), βi(k) and βi0(k). In particular,
αi(0), α0i(0), βi(0) and βi0(0) represent the values at initialization.
21
Published as a conference paper at ICLR 2022
B.1 “Good Type” Neurons
In this section, we show that “good type” neurons stay in the “good type” regime throughout
coordinate descent (Theorem B.8) using Theorem B.7.
Theorem B.8. “Good type” neurons are preserved in the “good type” throughout coordinate descent
with probability ≥ 1 一 O( n⅛) over the SBM randomness.
Proof. As shown in Section 4, “good type” regime is composed of (G1 ) and (G2), we show the
dynamics of neurons in (G1 ) and (G2) respectively.
Assume that neuron (αi(k), αi0(k), βi(k), βi0(k)) is in (G1), we show that it either stays in (G1) or moves
into (G2) throughout coordinate descent. In fact, by (14), with probability ≥ 1 一 O(*), -¾∙ <
0 < ¥ ,so β(k+1) >β(k),β0(k+1) <β0(k) andhence β(k+1) - β0(+ >β(k) — βi(k) >" 0. By
(12) and (13), -dLy < 0 < aLfe), so α(k+1) > α(k), αi(k+1) < αi(k). If αi(k+1) > 0, this neuron
∂αi	∂αi
stays in (G1 ). If α0i(k+1) < 0, since
α(k+1)
αi(k+1)
(k)	∂L
% 一 ηk∂αky
αi(k) - ηkτdLk)
∂αi
> 1,
the neuron moves into (G2).
Assume that neuron is in (G2), we also show that it either moves into (G1) or stays in (G2). As
shown in section 3.2, (G2) = (G2,1) ∪ (G2,2) ∪ (G2,3). If the neuron is in (G2,1), again by (12), (13)
and (14), α(k+1) > α(k) > 0 > Okk > a：(k+1), ∣4X∣ > 1,β(k+1) > β(k) > β") > βi(k+1),
αi
so the neuron stays in G2 2. If the neuron is in G2 2, by (15), (16) and (17), -dLk- < 0 < dLk ,
,	,	∂βi()	∂βi0()
WC ∕√k+1)、√(k+1) Ake ∂L V ∂L 0 WC c(k+1)、c0(k+1) I ɑik+1) I、I aik) I、1
so βi	>	βi	. Also,	∂-(k)	< ∂07(k)	< 0, so αi >	αi	, I a0(k+i) I >	I 07(k) I	> L
αi	αi	αi	αi
If α0i(k+1) < 0, the neuron stays in G2. If α0i(k+1) > 0, it moves into G1. If the neuron is in
G2,3, by (18) and (21), ^dL) < 0 < dLk), ^dLk - dLk) < 0, so β(k+1) > β(k) > β0(k) >
∂βi	∂βi ∂αi	∂αi	i	i	i
βi0(k+1), αi(k+1) 一 αi0(k+1) > αi(k) 一 α0i(k) > 0. By (19) and (20),
∂H ≤τβi-βi)E[Z](λ (p+-∣) 一。(Iog-2 n)),
能i ≤ (βi-βi)E[Z ](2 (p-q)+ O(Iog- 1n)).
Similar as in (G2,2), if ɑi(k+1) < 0, the neuron stays in (G2). If ai(k+1) > 0, it moves into (G1). □
B.2	“Bad Type” Neurons
As shown in Section 4, neurons of “bad type” consist of two cases: B1 and B2, where B2 = B2,1∪
B2,2 ∪ B2,3 ∪ B3. Since the output in B3 is concentrated at 0 (see Theorem A.2), we don’t need
to worry if neurons move into this region. Neurons in B1 ∪ B2,1 ∪ B2,2 ∪ B2,3 might exit “bad
type” regime and become “harmless” or “good” (if the neuron becomes order-aligned), which will do
no harm to the performance of the model. If they stay in B1 ∪ B2,1 ∪ B2,2 ∪ B2,3, the following
theorem shows that the separation mi0 一 mi1 can be upper bounded by initialization. In fact, Theorem
A.4 shows that m0 一 mɪ is proportional to ∣αi 一 αi∖∖βi 一 βi∣. The next theorem shows that both
∣αi 一 αi∖ and ∖βi 一 β0∖ shrink throughout coordinate descent. The worst situation is that the magnitude
of ∖αi 一 α0i∖ and ∖βi 一 βi0∖ of neurons in B3 increase and move into B1 or B2 at certain epoch. From
Theorem B.7 we see that the magnitude can only increase by a limited rate (we can see this more
explicitly in Theorem 6.2).
22
Published as a conference paper at ICLR 2022
Theorem B.9. If	, α0k1, β∖kk, β[kk) is in Bi U B2,1 U B2,2 U B2,3 then With probability
≥ 1 — O(n12) OvertheSBMrandomness, ∣ α-k+1) — α-(k+i) ∣ ≤ ∣ a(k) — α0(k) ∣ , ∣ β(k+1) — βi(k+i) ∣ ≤
*-β") ∣ .
Proof. In Bi and B2,1, by (12) and (13), ^⅛y > 0 > ^dLT, then α(k+I) < a(k),a，k+1) >
dai	dai
α”, so ∣α(k+1) — a：(k+I)I ≤ |a(k) — ɑ'ikk∖. Similarly, by (14),	= -^^ < 0, so
∣β(k+1)-β"+I)I ≤ Iek)-e0(k)| (Note that a(k) > α'(k),β(k) < β0(k)).	"
In B2,2, from (15) and (16), We have ~^^ > ∂fL⅛ > 0, so ∣α(k+1)- α'(k+1)∣ ≤ |a(k) — a'(k)|.
i	i
Ontheotherhand,系 < 0 < -f⅛,soβ'k+1 > 乩叫β"+1) < β'(k) and 展^1)- e'(k+1)| ≤
β i	β i
|e(k) — β'(k) |. In B2,3, by (24), ∕⅛y —六 > 0, so |a(k+1)- a'(k+1) | ≤ |a(k) — a'(k) |. By (18),
Oai	°αi
抵 < 0 <	o⅛) ,	so	|e(k+1) -	β'(k+I)I	≤	|e(k)	-	e'(k)|.	□
°Pi	°Pi
B.3	"HARMLESS TYPE" NEURONS
Section 4 shows that there are two cases of “harmless type”: H and H?. For neurons in H1, the
derivatives of parameters are estimated in (15), (16) and (17) (same as in G2,2)∙ We can have similar
analysis as in G2,2 and show that the inequality α > 0 > α', β' > β' can be preserved. Moreover
∣ Oi ∣ increases. So the neurons either stay in H1 or become “good type” if ∣ Oi ∣ > 1. In particular,
neurons in H1 do no harm to the performance of the model.
For neurons in H2, l[α/y + α%? ≥ 0] = 0, so the derivatives are all equal to 0. Therefore they are
never updated. Meanwhile they don,t affect the performance of the model since φ(αit* + α"?) = 0
and Z = 0.
C	Learning Guarantee
In this section, we prove Theorem 6.1, 6.2 and Lemma 6.3.
ProofofTheorem 6.1. We prove by contradiction. Suppose
P[∆ < 0|'(x) = 0] ≥ 4e,	(27)
then
E[Z|'(x) =0]= E[Z|'(x) =0, ∆ < 0]P[∆ < 0|'(x) = 0]
+ E[Z|'(x) = 0, ∆ ≥ 0]P[∆ ≥ 0|'(x) = 0]
≥ 1 ∙ 4e = 2e.	(28)
Furthermore, we claim that μo < δ. In fact, if μo ≥ δ, since P[|∆ —网| ≤ 6|'(x) = 0] ≥ 1 — E by
Corollary 4.2, and ∆ ≥ μo — δ ≥ 0, we have
P[∆ ≥ 0|'(x) = 0] ≥ P[|∆ — μo| ≤ δMx) = 0] ≥ 1 —E,
i.e. P[∆ < 0|'(x) = 0] ≤ E, which contradicts (27).
Let C	:=	μo	— μ1, then μ1 = μo — c < δ — c. Again, by Corollary 4.2, for '(x)	=	1, ∆ < μ1 + δ
with probability ≥ 1 — e,	we have Z = σ(∆) < σ(μ1 + δ) < σ(-c + 2δ). Then
E[Z|'(x) =	1]= E[Z|'(x) = 1, |∆ — μj < δ]P[|∆ — μj < δ]`(x)	=	1]
+ E[Z|'(x) = 1, |∆ — μj ≥ δ]P[|∆ — μj≥ δ1'(x)	=	1]
<	σ(-c + 2δ) ∙ 1 + 1 ∙ E
<	σ(- 2)+E.
23
3
(a
OAEq əMd∙2∙aSOdUIoɔəp ənɪBAUə--ə 百(OS) Aq uəa-ωq §0
ZV əjə^m
.s*μBpuηs
SAeq əM ∙‰l^⅜ H- I ⅝ A
zuɪsə əʌv IXəN ∙^⅛ UV C-^)Uμu əAEq əʌv .səuɪəjdp∞“IJO PUnoq jəʌv
Jμm E PUy əM √.< AJB=OJOU 日 OjJ 尸 OdAl POO2JO SIUOjnəu 303 Jl 7.9 m340IjOjOaId
□
-V-IUsA0 Λ vκ-W 宅=s-V-OUs7一O V vκOSU∙sl∙2PEbuoo E W q∙2qM
d V 一一I=s7n3iλ0=s7n亘
sa¾uη " V 一一戏亘.PuBqJaWOaWUO
.5A=I=s7n3i lo=s7N亘
.(W)WW əuæuiou•即V-I =s7n3i 4 V (ulΓ)b 80.SS .(。)。= 22SnP W ⅛⅛⅛w OqI
ZZOZ xuOIaJOdEd əɔuəjəjus E SE poqs=qnd
Published as a conference paper at ICLR 2022
Therefore we have a uniform lower bound of mi0 一 mi1 at the k-th epoch in “good type” regime:
“O-mi ≥ AC” 2 (M )l"争(M )T
Next we consider the “bad type” regime. By Corollary A.4, we have lower bound of mi0 - mi1 in B1,
B2 and B3 respectively. By Theorem B.9, in Bi and B2,呢 一 αi∣ and ⑸ 一 β0∣ shrink. Moreover,
since Λ1 > Λ35 6, we have a uniform lower bound of mi0 - mi1 in B1 and B2 :
m0 一 mi ≥ -W)B(0)1^!+^ ≥ - W)B**，
since Λι =(1+l°g- 1ιn)p2-q2 ≤ ⅛2 ≤ p.
(1+log-1 n)p+q -	2p+q ~1
Next We show that | αi — αi∣ and | βi — βi | can only increase by a limited rate in B3. From item 4 of
Theorem B.7, we have
∂L
∂αi
∂L
函
∂L
dai
∂L
≥-∣βi- β0∣O(e)
2 ∂L ≤ °®.
βi
—
—
Therefore (note that βi < βi0 )
A(k) ≤ A(k-i) + ηk∣B(k-D∣O(W)
|Bi(k)| ≤ |Bi(k-1)| + ηkO(W).
Since E[Z(k)] ≥ Ω(e)6, and W = o(e),e2 = O(ɪ), so ηkO(e) ≤ ηE[Z(k)]ɪ = n. Suppose
Ai(k) ≥ O(1), otherwise, Ai(k) and |Bi(k) | increase by an even smaller rate. So we have
A(k-1)
|Bi(k-1)|
≤ k((A(0))2+ (B(0))2)	(n → ∞).
We obtained the result for “harmless type” neurons directly from Corollary 4.3.
□
Proof of Lemma 6.3. Since all parameters are independent standard normal random variables, we
have
E[(α -
Var[(α -
α
α
0)2]=E[(β-β0)2]=2,
0)2] = Var[(β - β0)2] =8.
By Chebyshev’s inequality we have
h1
P[y?(αi	-	αi)	+	(βi	-	βi)	≤	5h]	≥ 1 一。( h).
i=1
522
pp-q is monotonically increasing.
6Otherwise, the model already achieves high accuracy, see the proof of Theorem 2.2
25
Published as a conference paper at ICLR 2022
For neurons initialized as “good type”, we have
E[a — a0∣a > α , α + a > 0]
Var[a — a0∣α > a0,α + a0 > 0]
2 - ɪ
4π
E[β — β0∣β > β0]
Var[β — β0∣β>β0] = 2 — 1.
π
Let ρ denote the probability that a neuron is initialized as “good type”. By G1, G2 and symmetry,
ρ = 2P[α > α0, α + α0 > 0, β > β0]. Since
P[α > α0,α + α0] = 4, P[β > β0] = 2
we have P = 4 .By Chernoffbound, P[hg ≥ P h] ≥ 1 一 exp ( — p2 h), so P[hg ≥ 8 ] ≥ 1 一 exp( — *).
Also by Chebyshev’s inequality,
P[	X	|ai	-	αillβi	— βo∖	≥	hg (2∏	— k)	lhg	≥ 8] ≥ 1 —
the i-th neuron
initialized as
“good type”
4 ― i
_____4π2
hg k2 .
Setk
1
10π ,
P[ X |ai - ai||ei — β0l≥ 80Ihg ≥ 8] ≥ 1 — O(h).
the i-th neuron
initialized as
“good type”
So we have
h
P[ X	|ai - αillβi — βi∖ ≥ 法]
80
the i-th neuron
initialized as
“good type”
≥ P[ X	∖αi - αi∖∖βi — βi∖ ≥ 80 Ihg ≥ 8]P[hg ≥ 8]
the i-th neuron
initialized as
“good type”
≥ (1 - O( 1 ))(1-exp (- 64))
≥ 1 - O(1).
h
□
D Experiments on Dynamics of Hidden Neurons
This experiment verifies our argument in Sections B.1, B.2, B.3 and Theorem 6.2 about the dynamics
of hidden neurons. We set h = 5, λ = 0.3 and train the model on graphs sampled from SBM with
n = 1000, a = 1.0, b = 0.7. The plot of accuracy and its distribution can be seen in Section 7. Here
we plot the dynamics of all the 5 hidden neurons in Figure 4, with each row corresponding to one
hidden neuron. In each plot, x-axis represents epoch and y-axis represents the value of neurons. The
first column depicts ai and ai, the second column ∖言∖, the third column ∖αi - αi∖, the fourth column
βi , βi0 and the last column ∖βi - βi0 ∖. As shown in the figure, the first, second and fourth neurons are
of “good" type satisfying (G2). Throughout training these neurons are preserved as “good" type:
they,re order-aligned, ∖ 孑 ∖ is lowered bounded by 1, and both ∖αi - α∣, ∖βi - βi∖ keeps increasing.
All of these verify our argument in B.1. The third neuron is “harmless" satisfying (H2). As shown
26
Published as a conference paper at ICLR 2022
in B.3, this neuron isn’t updated and doesn’t make contribution to the output. The fifth neuron is of
“bad” type satisfying (B2). Although ∣αi - αi∣ and ∣βi - β!0∖ increase, but by comparing with the first,
second and fourth row (“good" neurons), they increase at a much smaller rate. This verifies our result
in Theorem 6.2.
Figure 4: Dynamics of neurons of different types. The 1st, 2nd and 4th row correspond to neurons of
“good" type satisfying (G2). The 3rd row corresponds to “harmless" satisfying (H2). The 5th row
corresponds to “bad" type satisfying (B2). Their dynamics verify our results in Sections B.1, B.2,
B.3 and Theorem 6.2.
E	Table of Notations
We list the notations used in this paper for readers’ convenience.
27
Published as a conference paper at ICLR 2022
Notation
) 01
nPqabX 取Λ<ΛX m mβ%⅛l-¢∕0∕l5,0∈g4
Definition
number of vertices in a graph
probability of intra-community connection
probability of cross-community connection
parameter for P with P = a log n
parameter for q with q = b lof n
probability of revealing the label of a vertex
label of vertex x
adjacency matrix of a graph
normalized adjacency matrix With self loop A =做小)(A + I)
input feature of a graph
trainable weights in the first layer of GCN
trainable weights in the second layer of GCN
bias matrix of GCN with each row of B being [b0, b1]
bias in the first component
bias in the second component
number of hidden features
logit in the first component without bias
logit in the second component without bias
logit in the first component, g0 = f0 + b0
logit in the second component, g1 = f1 + b1
difference between logit, ∆ = g0 - g1 = f0 - f1 + b0 - b1
28