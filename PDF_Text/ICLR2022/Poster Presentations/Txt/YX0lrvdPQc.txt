Published as a conference paper at ICLR 2022
A JOHNSON-LINDENSTRAUSS FRAMEWORK FOR
Randomly Initialized CNNs
Ido Nachum, Jan HazIa, Michael Gastpar
School of Computer and Communication Sciences
Ecole PolytechniqUe Federale de Lausanne
1015 Lausanne, Switzerland
forename . surname) @epfl. Ch
Anatoly Khina
School of Electrical Engineering
Tel Aviv University
Tel Aviv 6997801, Israel
anatolyk@eng.tau.ac.il
Ab stract
How does the geometric representation of a dataset change after the application
of each randomly initialized layer of a neural network? The celebrated Johnson-
Lindenstrauss lemma answers this question for linear fully-connected neural net-
works (FNNs), stating that the geometry is essentially preserved. For FNNs with
the ReLU activation, the angle between two inputs contracts according to a known
mapping. The question for non-linear convolutional neural networks (CNNs) be-
comes much more intricate. To answer this question, we introduce a geometric
framework. For linear CNNs, we show that the Johnson-Lindenstrauss lemma
continues to hold, namely, that the angle between two inputs is preserved. For
CNNs with ReLU activation, on the other hand, the behavior is richer: The angle
between the outputs contracts, where the level of contraction depends on the na-
ture of the inputs. In particular, after one layer, the geometry of natural images is
essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the
same contracting behavior as FNNs with ReLU activation.
1 Introduction
Neural networks have become a standard tool in multiple scientific fields, due to their success in clas-
sification (and estimation) tasks. Conceptually, this success is achieved since better representation
is allowed by each subsequent layer until linear separability is achieved at the last (linear) layer. In-
deed, in many disciplines involving real-world tasks, such as computer vision and natural language
processing, the training process is biased toward these favorable representations. This bias is a prod-
uct of several factors, with the neural-network initialization playing a pivotal role (Sutskever et al.,
2013). Therefore, we concentrate in this work on studying the initialization of neural networks, with
the following question guiding this work.
How does the geometric representation of a dataset change after the application of each randomly
initialized layer of a neural network?
To answer this, we study how the following two geometrical quantities change after each layer.
(x, y)	The scalar inner product between vectors X and y.1
P := 高％	The Cosine similarity (or simply similarity) between vectors X and y.
The similarity ρ ∈ [-1, 1] between x and y equals ρ = cos(θ), where θ is the angle between them.
Consider first one layer ofa fully connected neural network (FNN) with an identity activation (linear
FNN) that is initialized by independent identically distributed (i.i.d.) Gaussian weights with mean
zero and variance 1/N, where N is the number of neurons in that layer. This random linear FNN
induces an isometric embedding of the dataset, namely, the similarity ρin between any two inputs,
1We mean here a vector in a wider sense: x and y may be matrices and tensors (of the same dimensions). In
this situation, the standard inner product is equal to the vectorization thereof:〈x, y)=(vec(x), vec(y)).
1
Published as a conference paper at ICLR 2022
xin and yin , is preserved together with their norm:
Pout ≈ Pout = Pin,
where ρout is the similarity between the resulting random (due to the multiplication by the random
weights) outputs, Xout and Kut (respectively), and Pout is the mean output similarity defined by
_ <xout,Yout)
Pout= HXoutHMuM,
and
K _	EKXout,Yout〉］
Pout :=	/
4见 UXoutH E ［伍ut『］
(1)
The proof of this isometric relation between the input and output similarities follows from the
celebrated Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984; Dasgupta and Gupta,
1999). This lemma states that a random linear map of dimension N preserves the distance be-
tween any two points up to an e > 0 contraction/expansion with probability at least 1 - δ for all
N > C log(1∕δ)∕e2 for an absolute constant c. In the context of randomly initialized linear FNNs,
this result means that, for a number of neurons N that satisfies N > c log(1∕δ)∕e2,
P (IPoUt- pin| < e) ≥ 1 - δ
So, conceptually, the Johnson-Lindenstrauss lemma studies how inner products (or geometry)
change, in expectation, after applying a random transformation and how well an average of these
random transformations is concentrated around the expectation. This is the exact setting of randomly
initialized neural networks. The random transformations consist of a random projection (multiplying
the dataset by a random matrix) which is followed by a non-linearity.
Naturally, adding a non-linearity complicates the picture. Let us focus on the case where the activa-
tion function is a rectified linear unit (ReLU). That is, consider a random fully-connected layer with
ReLU activation initialized with i.i.d. zero-mean Gaussian weights and two different inputs. For
this case, Cho and Saul (2009), Giryes et al. (2016), and Daniely et al. (2016) proved that2
Pout ≈ Pout
∖J∖ - PlI + (π - COST(Pin)) Pin
π
(2)
Following Daniely et al. (2016), We refer to the resulting function in (2) of Pout in Pin as the dual
activation of the ReLU; this function is represented in figure 1 by the yellow curve.
One may easily verify that the dual activation (2) of the ReLU satisfies Pout > Pin for Pin = 1,
meaning that it is a contraction. Consequently, for deep FNNs, which comprise multiple layers,
random initialization results in a collapse of all inputs with the same norm (a sphere) to a single
point at the output of the FNN (equivalently, the entire dataset collapses to a single straight line).
Intuitively, this collapse is an unfavorable starting point for optimization. To see why, consider the
(i)
gradient 胃僮⑺ of the weight Wj o of some neuron j in a deep layer i in a randomly initialized ReLU
FNN. By the chain rule (backpropagation), this gradient is proportional to the output of the previous
layer a(i-1) for the corresponding input, i.e., it holds that RWi α a(i-1). If the collapse is present
already at layer i, this output is essentially proportional to a fixed vector a*. But this implies that,
in the gradient update, the weights of the deep layer will move roughly along a straight line which
would impede, in turn, the process of achieving linear separability.
Indeed, it is considered that for a FNN to train well, its input-output Jacobian needs to exhibit dy-
namical isometry upon initialization (Saxe et al., 2014). Namely, the singular values of the Jacobian
∂χout∕∂xin must be concentrated around 1, where Xout and Xin denote the input and output of the
FNN, respectively. If the dataset collapses to a line, xout is essentially invariant to xin (up to a
change in its norm), suggesting that the singular values of ∂Xout ∕∂Xin are close to zero. Therefore,
randomly initialized FNNs exhibit the opposite behavior from dynamical isometry and hence do not
train well.
2The results in (Cho and Saul, 2009) and (Daniely et al., 2016) were derived assuming unit-norm vectors
IIxinll = Ilyinll = 1. The result here follows by the homogeneity of the ReLU activation function: R(ax)=
α R(x) for α ≥ 0, and ergodicity, assuming multiple filters are applied.
2
Published as a conference paper at ICLR 2022
(a) Gaussian images, filter size 3 × 3
(b) F-MNIST, filter size 3 × 3
(c) CIFAR-10, filter size 3 × 3 × 3
Figure 1: Input and output cosine similarities of a single randomly initialized convolutional layer
with 100 filters. Each red circle in the figures represents a random pair of images chosen from the
corresponding dataset. 200 pairs were sampled in each figure.
(d) ImageNet, filter size 11 × 11 × 3
1.1	Our Contribution
Our main interest lies in the following question.
Does the contraction observed in randomly initialized ReLU FNNs carry over to convolutional neu-
ral networks (CNNs)?
As we will show, qualitatively, the answer is yes. However, quantitatively, the answer is more subtle
as is illustrated in figure 1. In this figure, the similarity between pairs of inputs sampled at random
from standard natural image datasets—Fashion MNIST (F-MNIST), CIFAR-10, and ImageNet—are
displayed against the corresponding output of a randomly initialized CNN layer. For these datasets,
clearly ρout ≈ ρin meaning that the relation of ReLU FNNs (2)—represented in figure 1 by the
yellow curve—breaks down.
That said, for inputs consisting of i.i.d. zero-mean Gaussians (and filters comprising i.i.d. zero-mean
Gaussian weights as before) with a Pearson correlation coefficient ρ between corresponding entries
(and independent otherwise), the relation in (2) between PoUt and Pin of ReLU FNNs does hold for
ReLU CNNs as well, as illustrated in figure 1a.
This dataset-dependent behavior, observed in figure 1, suggests that, in contrast to randomly-
initialized FNNs which behave according to (2), randomly-initialized CNNs exhibit a richer be-
havior: PoUt does not depend only on Pin but on the inputs Xin and ya themselves. Therefore, in this
work, We characterize the behavior of PoUt after applying one layer in randomly initialized CNNs.
We start by considering randomly initialized CNNs with general activation functions. We show in
theorem 1 that the expected (over the filters) inner product E KXoUt, KuJ ] and the mean similar-
ity Pout depend on Xin and ya (and not just tXin,ya)) by extending the dual-activation notion of
Daniely et al. (2016). In theorem 2, we further prove that, by taking multiple independent filters,
(Xout, KuJ and Pout of (1) concentrate around E KXoUt, KuJ] and Pout, respectively.
3
Published as a conference paper at ICLR 2022
We then specialize these results to linear CNNs (with identity activation) and derive a convolution-
based variant of the Johnson-LindenstraUss lemma that shows that (X°ut,Y0ut)≈ 3n,yin) and
PoUt ≈ Pin for linear CNNs, both in expectation (PoUt = Pin for the latter) and with high probability.
For randomly initialized ReLU CNNs, we derive the following tight Upper and lower boUnds for
Pout in terms of Pin in theorem 3:
max{ρin, 0} ≤ Pout ≤ 1 ;'in .	(3)
These boUnds imply, in tUrn, that for Pin = 1 each ReLU CNN layer is contracting. In theorem 4 we
prove that PoUt for random Gaussian data satisfies the relation (2), in accordance with figure 1a.
To explain the (almost) isometric behavior of CNNs for natUral images (figUre 1), we note that many
natural images consist of large, relative to the filter size, approximately monochromatic patches.
This observation leads to a simple model of black and white (binary) images with “large patches”. To
describe this model mathematically, we define a notion of a shared boundary between two images in
definition 2, and model large patches by bodies whose area is large compared to the shared boundary.
We prove that PoUt ≈ Pin for this model, meaning that the lower bound in (3) is in fact tight.
1.2	Related Work
In this paper, we study how various inputs are embedded by randomly initialized convolutional
neural networks. Neural tangent kernel (NTK) (Jacot et al., 2018) is a related line of work. This
setting studies the infinite width limit (among other assumptions) in which one can consider neural
network training as regression over a fixed kernel; this kernel is the NTK. There are two factors
that affect the calculation of the NTK: The embedding of the input space at initialization and the
gradients at initialization. In this paper we study the first one.
Arora et al. (2019), and Bietti and Mairal (2019) give expressions for the NTK and the convolu-
tional NTK (CNTK). Theorem 1 may be implicitly deduced from those expressions. Arora et al.
(2019) provide concentration bounds for the NTK of fully connected networks with finite width.
Bietti and Mairal (2019) derive smoothness properties for NTKs, e.g., upper bounds on the defor-
mation induced by the NTK in terms of the initial Euclidean distance between the inputs. A related
approach to NTK is taken in (Bietti, 2021) where convolutional kernel networks (CKN) are used.
Standard initialization techniques use the Glorot initializtion (Glorot and Bengio, 2010) and He ini-
tialization (He et al., 2015). Both were introduced to prevent the gradients from exploding/vanishing.
On a similar note, Hanin and Rolnick (2018) discuss how to prevent exploding/vanishing mean acti-
vation length—which corresponds to the gradients to some degree—in FNN with and without skip
connections. For a comprehensive review on other techniques see (Narkhede et al., 2021-06-28).
Schoenholz et al. (2017); Poole et al. (2016); Yang and Schoenholz (2017) study the initialization
of FNNs using mean-field theory, in a setting where the width of the network is infinite. They
demonstrate that for some activation functions there exists an initialization variance such that the
network does not suffer from vanishing/exploding gradients. In this case, the network is said to be
initialized at the edge of chaos.
As mentioned earlier, Saxe et al. (2014) introduced a stronger requirement than that of moderate
gradients at initialization, in the form of dynamical isometry. for linear FNNs, they showed that
orthogonal initialization achieves dynamical isometry whereas Gaussian i.i.d. initialization does not.
For non-linear FNNs, Pennington et al. (2017) show that the hyperbolic tangent (tanh) activation
can achieve dynamical isometry while ReLU FNNs with Gaussian i.i.d. initialization or orthogonal
initialization cannot. In contrast, Burkholz and Dubatovka (2019) show that ReLU FNNs achieve
dynamical isometry by moving away from i.i.d. initialization. And lastly, Tarnowski et al. (2019)
show that residual networks achieve dynamical isometry over a broad spectrum of activations (in-
cluding ReLU) and initializations.
Xiao et al. (2018) trained tanh (and not ReLU) CNNs with 10000 layers that achieve dynamical
isometry using the delta-orthogonal initialization. Similarly, Zhang et al. (2019) trained residual
CNNs with 10000 layers without batch normalization using fixup-initialization that prevents explod-
ing/vanishing gradients.
4
Published as a conference paper at ICLR 2022
2 Setting and Definitions
The output z ∈ Rn×n of a single filter of a CNN is given by
Z = σ(F * x)
where x ∈ Rn×n×d is the input; F ∈ Rr×r×d is the filter (or kernel) of this layer; and σ : R → R
is the activation function of this layer and is understood to apply entrywise when applied to tensors.
The dimensions satisfy n, d, r ∈ N, r ≤ n. The first two dimensions of x and z are the image
dimensions (width and height), while the third dimension is the number of channels, and is most
commonly equal to either 1 for grayscale images and 3 for RGB color images; r is the dimension of
the filter; ‘*’ denotes the cyclic convolution operation:
(F * x)uv =	E	Fijk ∙ xu-i,v-j,k	(4)
i∈Zr,j∈Zr,k∈Zd
where the subtraction operation in the arguments is the subtraction operation modulo n. In this
work, we will concentrate on the ReLU activation function which is given by σ(x) = max{0, x}.
Typically, a single layer of a CNN consists of multiple filters F1, . . . , FN.
The following definition is an adaptation of the dual activation of Daniely et al. (2016) to CNNs.
Definition 1 (Dual activation). The dual activation function σ : Rn X Rn X R → R ofan activation
function σ is defined as
σ(x, y,ν) = E [σ(νX) ∙ σ(νY)]
for input vectors x, y ∈ Rn , and a parameter ν > 0, where
(X )-N ((0), (£ U)).	(5)
For tensors x,y ∈ Rn×n×d, the dual activation σ is defined as in (5) with the inner products and
norms replaced by the standard inner product between tensors:
(x, y =	E	Xijk ∙ yjk,	Ilxll = Vz〈x,x).
i∈Zn,j∈Zn,k∈Zd
In the special case of an activation that is homogeneous, i.e., satisfying σ(cx) = cσ(x) for c ≥ 0,
we use an additional notation for -1 ≤ρ≤ 1, with some abuse of notation:
V2 := E [σ(X)2],	3(ρ):= EB¾≡ ,	(X)〜N ((0) ,(1 P)) .	(6)
σ	νσ2	Y	0 ρ 1
This is the dual activation function of Daniely et al. (2016). Note that, by this definition, σ(1) = 1.
We will be mostly interested in the ReLU activation, denoted by R, for which the following holds
(see, e.g., (Daniely et al., 2016, Table 1 and Section C of supplement))
VR :=1/2 ,	R(P) = E[R(X)2 R(Y)] = " + (π -cos-1(P)) P
3 Main Results
In this section, we present the main results of this work. The proofs of all the results in this section
may be found in the appendix.
5
Published as a conference paper at ICLR 2022
Theorem 1. Let x, y ∈ Rn×n×d be inputs to a convolution filter F ∈ Rr×r×d with r ≤ n and
activation function σ such that the entries of F are i.i.d. Gaussian with variance ν2. Then,
EKσ(F * x),σ(F * y)}] = E σ ([x]rj, [y]rj, ν).	⑺
i,j∈Zn
In particular, for a homogeneous activation function we have
E[<σ(F * x),σ(F * y)〉] = ν2 鹏 E ∣∣ [x]j ∣∣∣∣ [y]j ∣∣ σ (Pij),	(8)
i,j∈Zn
E [σ(F * x)2] = ν2 ∙ V2 ∙ r2∣∣x∣∣2 ,	(9)
〈团石七期力）
WMyo
where ρij
and νσ2 has been defined in (6).
In particular, due to (9), for homogeneous activations We can choose V = 1/(Vσr) and get
E [σ(F * x)2] = ∣∣x∣∣2. That is, the norms are preserved in expectation for every input x.
Note that theorem 1 may be deduced as a special case from existing more general formulas; see, e.g.,
Arora et al. (2019); Bietti and Mairal (2019). Nevertheless, it is an important starting point for us.
While theorem 1 holds in the mean sense, it does not hold for a specific realization of a single
filter F, in general. The next theorem, Whose proofs relies on the notions of sub-Gaussian and sub-
exponential RVs and their concentration of measure, states that theorem 1 holds approximately for
a large enough number of applied filters.
We apply basic, general concentration bounds on the activation. There exist sharper bounds in
special cases, e.g., in the context of cosine similarity and ReLU, see (Buchanan et al., 2021, app. E).
Theorem 2. Let x, y ∈ Rn×n×d be inputs to N filters F1, . . . , FN ∈ Rr×r×d with r ≤ n such that
imax Il [x]rj∣l ≤ R,	imaχ Il [y]rj∣l ≤ R，	(IO)
i,j ∈Zn	i,j∈Zn
all the entries of all the filters are i.i.d. Gaussian with zero mean and variance ν2, and a Lipschitz
continuous activation function σ with a Lipschitz constant L and satisfying σ(0) = 0. Then, for
δ > 0, however small,
P (IN ES(FI * x),σ(Fι * y)〉- E σ([x]j,[y]j,ν) ≥) ≤ δ (11)
\|	l=1	i,j=1	)
for N > max (K, K2) log 2n2, where K = Dυ1L1R1n2/e and D > 0 is an absolute constant.
Remark 1. Clearly, ∣∣[χ]j∣∣ and ∣∣[y]j∣∣ in (10) may be bounded by Ilxll and ∣∣y∣∣, respectively.
HoWever, since r is typically much smaller than n, We prefer to state the result in terms of the norms
of [x]irj and [y]irj.
Theorem 2 states a concentration result in terms of the inner product. We noW give a parallel result
for homogeneous activations in terms of the cosine similarity ρout . To that end, consider a homoge-
neous activation σ and N random filters F1, . . . , FN; for these filters, the quantities of (1) for tWo
inputs x and y are equal to
∑N=1 W (Fl * x) ,σ (Fl * y))	Eij∈Zn I[x] jH®j忻(Pij)
Pout=T≡≡≡7≡≡zf，Pout=	r2w≡
(12)
Where Pij is defined in theorem 1. Applying theorem 2 to these quantities, yields the folloWing.
Corollary 1. Let x, y be inputs to N filters F1, . . . , FN such that
H[x]jn / κ,	H[y]jn “ κ)
max	≤- ≤ R , max ≤ R ,
i,j∈Zn IlXIl	i,j∈Zn Ilyll
all the entries of the filters are i.i.d. Gaussian with zero mean and variance ν2, and a Lipschitz
homogeneous activation σ with Lipschitz constant L. Then, for 0 < e ≤ 1/10 and δ > 0,
P( IPout(X,y) - Pout(X,y)| ≥ E) ≤ δ
for N > max (K, K 2)lοg 6n2, where K
DL2R2 n2
eνσr2 and D> 0 is an absolute COnSsan
6
Published as a conference paper at ICLR 2022
Remark 2. To make sense out of the constants in corollary 1, we point out that applying it to
inputs x,y ∈ {±1}n×n×d (So that ∣∣x∣∣2 = IlyIl2 = dn2 and ∣∣[x]j∣∣2 = ∣∣[y]j∣∣2 = dr2) results in
K = DL-, which does not depend on n or V2. There remains a log n2 factor that increases with n.
fcνσ
In the remainder of this section, We derive an analogous result to the Johnson-Lindenstrauss lemma
(Johnson and Lindenstrauss, 1984; Dasgupta and Gupta, 1999) in section 3.1 for random linear
CNNs, namely, CNNs with identity activation functions. We then move to treating random CNNs
with ReLU activation functions: We derive upper and lower bounds on the inner product between
the outputs in section 3.2, and prove that they are achievable for Gaussian inputs in section 3.3, and
for large convex-body inputs in section 3.4, respectively.
3.1	Linear CNN
Theorem 1 yields a variant of the Johnson-Lindenstrauss lemma where the random projection is
replaced with random filtering, that is, by applying a (properly normalized) random filter, the inner
product is preserved (equivalently, the angle).
Lemma 1. Consider the setting of theorem 1 with V = '/r. Then, E KF * x,F * y)] =(x, y.
Lemma 2. Consider the setting of theorem 2 with ν = 1/r (and σ(x) ≡ x). Then,
P
(k E
M * χ,F * y)-
〈x,y)
≥ ) ≤ δ
for N > max (K, K2) log 亭,where K = DRn and D > 0 is an absolute constant.
Related result appears in Krahmer et al. (2014) (theorem 1.1) which shows norm preservation of
sparse vectors for convolutional operators with a filter dimension that equals the input dimension.
3.2	CNN with ReLU Activation
The following theorem implies that every layer of a neural network is contracting in expectation
(over F). That is, the expected inner product between any two data points will get larger with each
random CNN layer. We also develop an upper bound on the new correlation.
Theorem 3.	Consider the setting of theorem 1 with a ReLU activation function and variance V =
2/r2, for inputs x and y with similarity ρ. Then,
max{(x,y), 0} ≤ EKR(F * x), R(F * y))] ≤ IlxIIlIyll 1+P,	(13)
Remark 3. All the bounds in theorem 3 are tight. For simplicity, we present examples where the unit
vectors x, y are flat, i.e., they are not tensors and r = 1. Similar examples can be drawn for tensor
inputs and r > 1. The upper bound is realized for x, y ∈ R2 of the form x
y
. Similarly, x, y ∈ R3 with
x
(√1 - ρ, √ρ, O), y = (O, -√ρ, √1 - P)
satisfy (x, y) = -P and E [σ(F * x)σ(F * y)] = 0 for r = 1 and 0≤P≤1.Finally, we can take
X = (√1 — P, √ρ, 0) and y = (0, √ρ, √1 — P) to obtain (x, y) = E [σ(F * x)σ(F * y)] = ρ.
These examples are illustrated in figure 3.
Remark 4. Although the lower bound is tight, in most typical scenarios it will be strict, so, in
expectation, the convolutional layers will contract the dataset after application of enough layers. To
see why, observe (23c); equality is achieved iff Pij = 1 ∨ [x]irj = 0 ∨ [y]irj = 0. For real data sets
this will never hold over all (i, j). This can be observed in figures 1b, 1c, and 1d where the linear fit
has a slope of 0.97.
3.3 Gaussian Inputs
The following result shows that the behavior reported by (Daniely et al., 2016) [and illustrated in
figure 1a by the orange curve] for FNNs holds for CNNs with Gaussian correlated inputs; this is
illustrated in figure 1a. For simplicity, we state the result for d = 1 (n × n inputs instead of
n × n × d); the extension to d > 1 is straightforward.
7
Published as a conference paper at ICLR 2022
Theorem 4.	Let X, Y ∈ Rn×n be zero-mean jointly Gaussian with the following auto- and cross-
correlation functions:
E [XijXki] = E [YijYki] = n12δikδji,	E [XijYki] = nδikδji,	(14)
i.e., X and Y have pairwise i.i.d. entries which are correlated with a Pearson correlation coeffi-
cient ρ. Assume further that the filter F ∈ Rr×r comprises zero-mean i.i.d. Gaussian entries with
variance 2/r2 and is independent of (X, Y ). Then, for a ReLU activation function σ,
E [(σ(F * X), σ(F * YM = VZr-卢 +(π - cos-⅛.
π
3.4 Simple Black and White Image Model
This section provides insight why we get roughly an isometry when embedding natural images via
a random convolution followed by ReLU.
As a conceptual model, we will work with binary pictures {0, 1}n×n or equivalently as subsets of
Zn × Zn. Also, when considering real-life pictures, an apparent characteristic is that they consist of
relatively large monochromatic patches, so for the next theorem, one should keep in mind pictures
that consist of large convex bodies.
To state our results, we define a notion of a shared r-boundary.
Definition 2 (Boundary).	Let A, B	⊂	Zn × Zn and let	r	∈ [n]. Then,	the r-boundary	of the
intersection between A and B, denoted	by ∂r(A, B), is	defined as the set of all pixels	(i, j) ∈
Zn × Zn such that
∃a1, b1, c1, d1	∈ {-r, . . .	, r}	: (i + a1, j + b1)	∈	A ∧ (i + c1, j	+ d1) ∈ B	(15a)
∃a2, b2, c2, d2	∈ {-r, . . .	, r}	: (i + a2,j + b2)	∈/	A ∨ (i + c2,j	+ d2) ∈/ B	(15b)
where the addition in (15a) and (15b) is over Zn.
In words, a pixel (i, j) belongs to the r-boundary ∂r(A, B) if, considering the square with edge size
2r + 1 centered at the pixel: (1) the square intersects both A and B, (2) the square is not contained
in A or is not contained in B .
Example 1. Let A and B be axis-aligned rectangles of sizes a × b and c × d, respectively, and an
intersection of size e X f where e,f ≥ 2r. Then, ∣∂r(A, B)| = 4r(e + f). This is illustrated in
figure 2.
The next theorem bounds the inner product between A and B after applying a ReLU convolution
layer in terms of the r-boundary of the intersection of A and B .
Theorem 5. Let A, B ⊂ [n] × [n] and a convolution filter F ∈ R(2r+1)×(2r+1) such that the entries
in F are i.i.d. Gaussians with variance 2/(2r + 1)2. Then,
〈A, B〉-∣∂r(A, B)| ≤ EKR(F * A), R(F * B)〉] ≤〈A, B) + ®(A, B)| ,	(16a)
IlA - Bll2 - 2∣∂r(A, B)| ≤ E [∣∣ R(F * A) - R(F * B)∣∣2] ≤ IlA - B∣∣2 + 2包(A, B)] . (16b)
Theorem 5 and corollary 1 imply
|pout - Pin | ≤
∂r (A,B)
MH 网+e
with high probability for a large enough number of filters N . This shows that a set of images con-
SiSting of “large patches"，meaning that ∂AA,B) is small (as in example 1 for small r), is embedded
almost-isometrically by a random ReLU convolutional layer. Moreover, any set of images may be
embedded almost-isometrically this way. To see how, fix r while increasing artificially (digitally)
the resolution of the images. The latter would yield ∂AAB) → 0 as the resolution tends to infinity.
8
Published as a conference paper at ICLR 2022
Figure 2: Illustration of the shared r-boundary from definition 2 in case of two axis-aligned rectan-
gles. The boundary is marked in green.
4 Discussion
4.1	Grayscale versus RGB Images
The curious reader might wonder how the analysis over black-and-white (B/W) images can be ex-
tended to grayscale and RGB images. While we concentrated on B/W images in this work, the
definition of a shared boundary readily extends to grayscale images as does the result of theorem 5.
This, in turn, explains the behavior of the grayscale-image dataset F-MNIST in figure 1b.
For RGB images, the framework developed in this work does not hold in general. To see why,
consider two extremely simple RGB images: Red—consisting of all-zero green and blue channels
and a constant unit-norm red channel, and Green—consisting of all-zero red and blue channels and
a constant unit-norm green channel. Clearly,〈Red, Green = 0∙ However, a simple computation,
.	.	..	.-.	-	-	-	-	。,一 ，一	一	八 一 ，一	、 、 r	,	^ z .,	-...
in the spirit of this work, shows that E KR(F * Red), R(F * Green))] = 1∕π = R(0) which is far
from 0. That is, for such monochromatic pairs, the network behaves as a fully connected network
(2) and not according to the almost isometric behavior of theorem 5 and figure 1d.
So why does an almost isometric relation appear in figures 1c and 1d? The answer is somewhat
surprising and of an independent interest. It appears that for the RGB datasets CIFAR-10 and Im-
ageNet, the RGB channels have high cosine similarity (which might not be surprising on its own),
and additionally all the three channels have roughly the same norm. Quantitatively—averaged over
104 images that were picked uniformly at random from the ImageNet dataset—the angle between
two channels is ~12.5° and the relative difference between the channel norms is ~9.2%.
This phenomenon implies that our analysis for black and white images (as well as its extension
to grayscale images) holds for RGB natural-images datasets. That said, there are images that are
roughly monochromatic (namely, predominantly single-color images) but their measure is small
and therefore these images can be treated as outliers.
4.2	Future Work
Figure 1 demonstrates that a randomly initialized convolutional layer induces an almost-isometric
yet contracting embedding of natural images (mind the linear fit with slope ~0.97 in the figure), and
therefore the contraction intensifies (“worsens”) with each additional layer.
In practice, the datasets are preprocessed by operations such as mean removal and normalization.
These operations induce a sharper contraction. Since a sharp collapse in the deeper layers implies
failure to achieve dynamical isometry, a natural goal for a practitioner would be to prevent such
a collapse. Saxe et al. (2014) propose to replace the i.i.d. Gaussian initialization with orthogonal
initialization that is chosen uniformly over the orthogonal group for linear FNNs. However, the
latter would fail to prevent the collapse phenomenon as well (see (Pennington et al., 2017)). This
can be shown by a similar calculation to that of the dual activation of ReLU with i.i.d. Gaussian
initialization. In fact, the dual activations of both initializations will coincide in the limit of large
input dimensions.
That said, the collapse can be prevented by moving further away from i.i.d. initialization. For ex-
ample, by drawing the weights of every filter from the Gaussian distribution under the condition
that the filter’s sum of weights is zero. Curiously, also including batch normalization between the
layers prevents such a collapse. In both cases, the opposite phenomenon to a collapse happens, an
expansion. In the deeper layers, the dataset pre-activation vectors become orthogonal to one an-
other. Understanding this expansion and why it holds for seemingly unrelated modifications is a
compelling line of research.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The work of I. Nachum and M. Gastpar in this paper was supported in part by the Swiss National
Science Foundation under Grant 200364.
The work of A. Khina was supported by the Israel Science Foundation (grants No. 2077/20)
and the WIN Consortium through the Israel Ministry of Economy and Industry.
References
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. In Proceedings of the International Conference on Machine
Learning (ICML), pages 1139-1147. PMLR, 2013.
William B Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Sanjoy Dasgupta and Anupam Gupta. An elementary proof of the Johnson-Lindenstrauss lemma.
International Computer Science Institute, Technical Report, 22(1):1-5, 1999.
Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. In Proceedings of the
International Conference on Neural Information Processing Systems (NeurIPS), 2009.
Raja Giryes, Guillermo Sapiro, and Alex M Bronstein. Deep neural networks with random Gaussian
weights: A universal classification strategy? IEEE Transactions on Signal Processing, 64(13):
3444-3457, 2016.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
the power of initialization and a dual view on expressivity. In Proceedings of the International
Conference on Neural Information Processing Systems (NeurIPS), pages 2261-2269, 2016.
Andrew M. Saxe, James L. Mcclelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural network. In In International Conference on Learning
Representations, 2014.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
alization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint
arXiv:1905.12173, 2019.
Alberto Bietti. Approximation and learning with deep convolutional models: a kernel perspective.
arXiv preprint arXiv:2102.10032, 2021.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neu-
ral networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Ma-
chine Learning Research, pages 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010.
PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classification. CoRR, abs/1502.01852, 2015. URL
http://arxiv.org/abs/1502.01852.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
Meenal V Narkhede, Prashant P Bartakke, and Mukul S Sutaone. A review on weight initialization
strategies for neural networks. Artificial intelligence review, 2021-06-28. ISSN 0269-2821.
10
Published as a conference paper at ICLR 2022
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep informa-
tion propagation. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL
https://openreview.net/forum?id=H1W1UN9gg.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. Advances in neural information
processing systems, 29:3360-3368, 2016.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: Theory and practice. In Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, pages 4788-4798, 2017.
Rebekka Burkholz and Alina Dubatovka. Initialization of relus for dynamical isometry. Advances
in Neural Information Processing Systems, 32:2385-2395, 2019.
Wojciech Tarnowski, Piotr WarchoL StanisIaW Jastrzebski, Jacek Tabor, and Maciej Nowak. Dy-
namical isometry is achieved in residual networks in a universal way for any activation function.
In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2221-2230.
PMLR, 2019.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convo-
lutional neural networks. In International Conference on Machine Learning, pages 5393-5402.
PMLR, 2018.
Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization: Residual learn-
ing without normalization. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL
https://openreview.net/forum?id=H1gsz30cKX.
Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold problem.
In International Conference on Learning Representations (ICLR), 2021.
Felix Krahmer, Shahar Mendelson, and Holger Rauhut. Suprema of chaos processes and the re-
stricted isometry property. Communications on Pure and Applied Mathematics, 67(11):1877-
1904, 2014.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,
2018.
11
Published as a conference paper at ICLR 2022
A Notation
R, N, Z
∨, ∧
Zn, [n]
Aij , Tijk
[T]irj
Ik
vec(∙)
(χ,y)
Hxn
ρ
P
σ, σ
R(x)
R(P)
N(μ, C)
The sets of real, natural, and integer numbers, respectively.
The logical ‘or’ and ‘and’ operators, respectively.
The sets {0, 1, . . . , n - 1} and {0, 1, . . . , n - 1}.
The (i, j ) entry of the matrix A and the (i, j, k) entry of the tensor T , respectively.
The sub-tensor of T formed from rows i to i + r - 1 and columns j to j + r - 1 (and
full third “layer” dimension).
The identity matrix of dimensions k × k .
The vectorization operation of a matrix or a tensor in some systemic order.
The standard inner product between vectors x and y . For matrix or tensor x and y (of
the same dimensions) the inner product is defined as(x, y) = (vec(x), vec(y)).
The standard Euclidean norm, induced by the standard inner product: ∏x∏ =，(x,x).
The (cosine) similarity. The similarity between x and y is defined as P
〈x,y〉
W≡.
The mean similarity. P between random X and Y is defined as P
EKX,Y〉]
v∕e[∣∣X ∣∣2]e[∣∣Y Il2]
An activation function and its dual, respectively, as defined in definition 1.
The ReLU applied to x: R(x) := max{0, x}; for a tensor x the R is applied entrywise.
The dual activation of the ReLU at P ∈ [-1,1]: R(ρ) := ^1~+ +(π∏^°s__(P)IP.
A Gaussian vector with mean vector μ and covariance matrix C.
B Proof of theorem 1
To prove theorem 1, we first prove the following simple result.
Lemma 3. Let u, v ∈ Rk be two (deterministic) vectors, and let G, H ∈ Rk be zero-mean jointly
Gaussian vectors with the following auto- and cross-correlation matrices:
E [GGt] = E [HHT] = V2Ik,	E [GHT] = PV2Ik.
Then,
Z
uTG
vTH
is a zero-mean Gaussian vector with a covariance matrix
IP [7 σ,T ] — 2 ( IlUIl	P(U,v)、
ElZZ J=V(P(U,v〉H2 〉
Proof of lemma 3. Z is a Gaussian vector as a linear combination of two jointly Gaussian vectors.
Furthermore,
E[Z] =E	vUTTHG	=	vUTTEE[[HG]]	=0,
E [ZZT] = E
UTGGTU UTGHTv	UTE
vTHGTU vTHHTv = vTE
U UTE GHT
u vT E [HHT
vv
V2
(Ilull2 P(U,明
IP(UM ∣∣vh2 )
□
We are now ready to prove theorem 1.
Proof of theorem 1. The equation (7) is proved as follows.
E [(σ(F * x), σ(F * y)〉] = E E [σ((F, [x]j〉)∙ σ ((F, [y]j〉)]	(17a)
i,j∈Zn
=E 3 ([x]rj, [y]rj, ν),	(17b)
i,j∈Zn
12
Published as a conference paper at ICLR 2022
where (17a) follows from the definition of the cyclic convolution (4) and the linearity of expectation,
and (17b) follows from definition 1 using lemma 3 with F taking the role of G and H (with ρ = 1),
and [x]irj and [y]irj taking the roles of u and v, respectively.
Now, we set out to prove (8) under the assumption that σ is homogeneous and the following notation.
Qx]rj, [y]rj>
j『
))
E[<σ(F * x),σ(F * y)〉] = E σ ([x]j, [y]j,ν)
i,j∈Zn
=E E [σ(νXij) ∙ σ(νYij)]	(18a)
i,j∈Zn
=V2 E MjM [yj∣ Ek(Xj/ IMrjIl)	/ 他引)(侬)
i,j ∈Zn
=v2 E l∣[χ]rj∣ll∣[y]jll Eb(Xj) ∙σ(Yj)]	(18C)
i,j∈Zn
=V2 ∙v2 E "xlrjU ll[y]jllσ (Pi),	(18d)
i,j∈Zn
where (18a) follows from definition 1, (18b) follows from the homogeneity of σ, (18C) follows from
standard random variable CalCulus, and (18d) follows from (6).
Finally, to obtain (9), We apply (8) for X = y and use that Pij = 1 and σ(1) = 1:
E [σ(F * x)2] = v2 ∙ν2 E Mrj『3(Pij)
i,j∈Zn
=v2 ∙v2 E MrjMi)
i,j∈Zn
=V2 ∙V2 E H[x]rjH2 = V2 ∙ V2 ∙ r2∣∣x∣∣2 .	□
i,j∈Zn
C Proof of theorem 2
To prove the ConCentration of measure results of this Work, We Will make use of the folloWing
definition.
Definition 3. The OrliCz norm of a random variable (RV) X With respeCt to (W.r.t.) a Convex funCtion
ψ : [0, ∞) → [0, ∞) suCh that ψ(0) = 0 and limx→∞ ψ(x) = ∞ is defined by
e [ψ (与)]≤}
∣∣χ∣∣ψ := Ef {t > 0
In particular, X is said to be Sub-Gaussian if ∣∣X口3？ < ∞, and sub-exponential if ∣∣X∣∣ψ] < ∞,
Where ψp (x) := exp {xp} - 1 forp ≥ 1.
The folloWing tWo results, Whose proofs are available in (Vershynin, 2018, Ch. 2 and 5.2), Will be
useful for the proof of theorem 2.
Lemma 4. Let X and Y be sub-Gaussian random variables (not necessarily independent). Then,
1.	Sum of independent sub-Gaussians. If X and Y are also independent, then their sum,
X + Y, is sub-Gaussian. Moreover, ∣∣X + Y
≤ C (∣χ∣ψ2 + ∣Y∣ψ2)for an abso-
lute constant C. The same holds (with the same constant C) also for sums of multiple
independent sub-Gaussian RVs.
13
Published as a conference paper at ICLR 2022
2.	Centering. X 一 E [X] is sub-Gaussian. Moreover, ∣∣X 一 E [X]|显 ≤ CIlX|展 for an
absolute constant C.
3.	Lipschitz functions of Gaussians. If X is a centered Gaussian and f is a function with
Lipschitz constant L, then ∣∣f (X) — E[f(X 力卜？ ≤ CllX ∣∣ψ2 for an absolute constant C.
4.	Product ofsub-Gaussians. XY is sub-exponential. Moreover IlXY∣∣ψι ≤ ∣∣X∣∣ψ2 ∣∣Y∣∣ψ2.
Theorem 6 (Bernstein’s inequality for sub-exponentials). Let X1, . . . , XN be independent zero-
mean sub-exponential RVs. Then,
眼t Xi
≥ t) ≤ 2eχp {-min {K 'K} ∙ C ∙ N},
∀t ≥ 0,
where K = maxi IlXillψι and c> 0 is an absolute ConStant.
We are now ready to prove the desired concentration-of-measure result.
Proofoftheorem 2. (Fl, [x] j〉and〈Fl, [y] j)are (jointly) Gaussian RVs—as linear combinations
of i.i.d. Gaussian RVs—with mean zero and variances ν2 ∣∣ [x]r7-∣∣2 and ν2 ∣∣ [y]j『，respectively, for
all I ∈ [N] and i,j ∈ Zn. Hence, (Fl, [x]j)and (Fl, [y]j) are also SUb-GaUSSian with
∣∣(Fι,[x]j)∣∣ψ2 ≤ CoνR,	∣∣(Fl,[y]j)∣∣ψ2 ≤ CoνR,	∀l ∈ [N],	∀i,j ∈ Zn
for some universal constant C0 > 0. Define now
XijI := σ ((Fl, [x]j)) ,	Yijl := σ ((FMy]j))，	∀l ∈ [N],	∀i,j ∈ Ln
Since σ is Lipschitz continuous with a Lipschitz constant L, and since the inner products (Fl, [x] j)
and (Fl, [y]j) are centered Gaussians, by property 3 in lemma 4 it follows that Xj and Yijl are
sub-Gaussian with
IlXijI — E [Xijl]∣ψ2 ≤	CoLνR,	IIYijI	— E	[Yijl]∣ψ2	≤	CoLνR,	∀l ∈	[N],	∀i,j ∈	Zn.(19)
Due to the assumption σ(0) = 0 and the fact that σ has Lipschitz constant L, inequality ∣σ(x)∣ ≤
L|x| holds for every x ∈ R. Accordingly, we can bound the expectation of E [Xijl] with
|E [Xijl]∣ ≤ E[∣Xijl∣] ≤ LE [∣(Fλ [x]j)∣] ≤ L，Var [(Fl, [x]j)] ≤ LVR ,	(20)
where We applied the Cauchy-Schwarz inequality. Similarly, it holds that
∣E[Yjl]∣≤ LνR.	(21)
Since by homogeneity of norm it also holds for any constant D ∈ R that ∣D∣ψ2 ≤ Co∣D∣, by
triangle inequality it then follows from (19), (20) and (21) that
IXijl∣ψ2 ≤ IIXije- E [Xijl]∣ψ2 + IE [Xijl]∣ψ2 ≤ 2CoLνR ,
IYjl∣ψ2 ≤ 2CoLνR.
Therefore, subsequent application of properties 4 and 2 of lemma 4 to XijlYijl , yields
IXijlYijl∣ψ1 ≤ 4C2ν2L2R2,
IlXijIYjI- E [XijlYjl]∣ψ1 ≤ 4C2Cν2L2R2,
(22)
14
Published as a conference paper at ICLR 2022
for all I ∈ [N] and all i, j ∈ Zn , where C > 0 is is the absolute constant from lemma 4. Now it
follows
P(I N f∖σlF * x),σ(Fl * y]) - £ 个([χ]rj, [y]ij,ν) ≥e
W	l=1	i,j=1
P ( N E	{χjYijg- E [XijlYijl] } ≥ E
∖	l∈[N ]∙,i,j∈Zn	,
≤ P ( N E E {XijlYijl - E [XijlYijl] }
∖	i,j三Zn l∈[N]
≤ Pl n— max
N i,j∈Zn
≤P
i,j∈Zn
E'j {XijlYijl — E [XijlYijl] }
l∈[N ]
^Ei {XijlYijl — E [XijlYijl] }
l∈[Ν ]
≤ 2n2 exp { - min (c4C2ν4L4R4n4 , C2Cν2L2R2n2 ) cN},
(23a)
(23b)
(23c)
(23d)
(23e)
where (23a) follows from theorem 1 and (19), (23b) follows from the triangle inequality, (23d)
follows from the union bound, and (23e) follows from (22) and theorem 6.
Finally, noting that (11) holds for N > max ( COC 仁2 R n ,。。。“上 R n ) log 亭 concludes the
proof.	□
D Proof of corollary 1
Let Us start by inspecting (12) to observe that, by homogeneity of σ, neither the value of PoUt nor the
distribution of ρout depend on ν or the norms of x and y. Therefore, let us assume without loss of
generality that X and y are unit vectors, as well as that V = ɪ. Furthermore, note that homogeneity
of σ implies σ(0) = 0, so theorem 2 can be applied with activation function σ.
Let A ：= N ENjσ(Fι *x),σ(Fe *y)〉, B :=寺 £3 ∣∣σ(墨 *X)KC := N £3 ∣∣σ(4 *y)∣∣2
and A' := r⅛ ∑i,j∈z H [x] j H * H [y]ijHσ(ρij). COmparing against (12), We see that PoUt √§C
and that PoUt = A'.
We now apply theorem 2 three times, for pairs of vectors (X, y), (X, X) and (y, y), respectively, with
parameters of e/5 and δ∕3. Using equations (7)-(9) from theorem 1, we check that indeed each of
the three following relations holds for N > max(K, K2) log 等,except with probability δ∕3:
IA-A'ι≤5,	IB- 1ι≤ 1，	1C- 1ι≤E.	(24)
By union bound, all three inequalities in (24) hold simultaneously except with probability δ. Finally,
by elementary inequalities we establish that if (24) holds for some A, A', B, C ∈ R such that ∣A∣ ≤ 1
and 0 < 5 ≤ 1/10, then it also holds that
PoUt - PoUtI = I -7== - A'
BC
≤5 .
Since this occurs except with probability δ, the proof is concluded.
□
15
Published as a conference paper at ICLR 2022
E	Proof of lemmata 1 and 2
Proof of lemma 1. Lemma 1 may be viewed as a special case of theorem 1 with an identity activation
function σ . Therefore,
E KF * x,F * y)] (= E σ ([χ]rj, [y]rj,ν) = E V2Qχ]rj, [y]rj) = (χ, y),
i,j∈Zn	i,j∈Zn
where (a) follows from theorem 1, (b) follows from definition 1, and (C) holds since V2 = 1/r2. □
Proof of lemma 2. Again, lemma 2 follows from theorem 2 for σ(x) ≡ x (and hence L = 1) and
from lemma 1.	□
F Proof of theorem 3
We start with the lower bound. Define Pj := IIJxjy]j> ∣∣ for all i,j ∈ Zn. Then, for all i,j ∈ Zn,
J	gjuwji
([χ]rj,[y]j) = MJMiyJI Pij
(25a)
i2J + (π - cos-1(ρiJ))ρiJ
(25b)
= r2E [σ "[x]J)) σ ((F, [y]J))]
where (25a) holds by the definition of ρiJ , (25b) holds since
(25c)
'∖ — a2 + (π — cos-1(a)) a
a ≤ ----------------------
∀∣a∣ ≤ 1,
and (25c) follows from (Daniely et al., 2016, Table 1 and Section C of supplement). Thus,
(x,y) = 1∕r2	E。叫,松达)二	E E [σ "	[x]J)) σ "	[y]J))] = E	[σ(F *	x)σ(F	*	y)],
,J∈Zn
,J∈Zn
(26)
where the inequality follows from (25), and the last step is due to (17). Now, since the ReLU activa-
tion is non-negative, 0 ≤ E [σ(F * x)σ(F * y)], which completes the proof of the left inequality in
(13).
For the upper bound, we use the following convexity argument. First, by homogeneity of ReLU, we
can assume without loss of generality that x and y have unit norm. Therefore, it remains to show
E [σ(F * x)σ(F * y)] ≤ 1++ρ for x, y such that ∣∣x∣∣ = Ilyll = 1 and (x, y) = ρ.
Recall that Pij = jxjj[yyir； . We expand the definitions in the same way as in (26) and (25c)
E[σ(F * x)σ(F * y)] = E E [σ ((F, [x]J)) σ ((F, [y]J))]
i,j∈Zn
Σ
i,J∈Zn
r2
R(ρiJ ) .
(27)
≤ MJM [y]JI
π
π
WxJUWJI
τ. ∙	∙ 1 1	11,1, U ∕r∖ TU ∕r∖	/-∖	1 .1 . τ⅛ / ∖ ∙	i`	T/	/ TA	1 ∙ 1
Itis easily checked that R(1) = 1, R(—1) = 0 and that R(x) is convex for —1 ≤ x ≤ 1. Accordingly,
using the decomposition P = 1++ρ ∙ 1 + 1-ρ ∙ ( —1),we have by Jensen,s inequality for every i,j ∈ Zn
R(Pij) ≤ 1+ρj
16
Published as a conference paper at ICLR 2022
Figure 3: An illustration of the examples from remark 3. Note that similar examples can be con-
structed in higher dimensions, e.g., by equally distributing the coefficients over a larger set of coor-
dinates.
Substituting into (27),
Il [x]rj H"y]rjl 1 + Pij
E [σ(F * x)σ(F * y)] ≤ T ——F-------------------L
i,j∈Zn
=击 E ∣[χ]H∣[y]jI + 2⅛	E。%j,[y]j)
i,j∈Zn	i,j∈Zn
≤壶JE MrjI2) ( E叱卅)+1《初
i,j∈Zn	i,j∈Zn
=1+£
=2	,
(28)
where in (28) We applied the CaUchy-SchWarz inequality and used the initial assumption ∣∣χ∣ =
∣y∣ = L
G Proof of theorem 4
The proof folloWs from the folloWing steps.
E [(σ(F * X ),σ(F * Y )〉] = E E £ σ(<F, [X ]Q)σ((F, [Y ]j〉)F
i,j∈Zn
=E [n2E[σ (XF) σ (YF)]]
=1 ^— +(「-3E UF『]
_ √1 - P2 + (∏ - CosT(P)) P
π
(29a)
(29b)
(29c)
(29d)
Where (29a) folloWs from the laW of total expectation; (29b) folloWs from (14), from the indepen-
dence of F in (X, Y), and from lemma 3 With F taking the roles of u and v, and [X]irj and [Y]irj
taking the roles ofG and H, resulting in XF and YF Which are zero-mean jointly Gaussian With vari-
ances ∣∣F∣∣2 /n2 and a Pearson correlation coefficient ρ; (29c) follows from the homogeneity of the
ReLU activation function and from (Cho and Saul, 2009, eq. (6)), (Giryes et al., 2016, Theorem 4),
(Daniely et al., 2016, Section 8); (29d) holds by the definition of F.	□
17
Published as a conference paper at ICLR 2022
H Proof of theorem 5
In this section, We will use the following additional notations. [A] j will denote the submatrix of A
formed from rows i - r to i + r and columns j - r to j + r, and 1k×k will denote a k × k matrix
with all entries equal 1.
The main effort in the proof is establishing (16a). We apply theorem 1, so we have
E [(σ(F * A),σ(F * B)〉] = Ei j σ([A] j, [B] j, 2∕(2r + 1)). And specifically, for the ReLU acti-
vation, it holds that
3([A]j,[B]j, 2∕(2r + 1))
加砥小但务花(Pj)
(2r + 1)2
(30)
Whereo∙= ([4%，归％)
where Pj=H阳』快叫『
A natural way to show that the convolutional layer induces isometry in expectation would be to show
3([A]j,[B]j, 2∕(2r +1)) = Aj ∙ Bj
for all pairs (i, j). Unfortunately, this does not hold for all pairs. However, we will show that the
only pairs for which it does not hold belong to the r-boundary ∂r (A, B). There are two cases to
consider A j ∙ Bij = 0 and A j ∙ Bj = 1 since A and B are binary.
Let Gij = G ([A] j, [B] j), 2∕(2r + 1)). Let E0 = {(i,j) ∈ Zn : Aj ∙ Bj = 0 ∧ 扇j = 0} and
Ei = {(i,j) ∈ Zn : Aij ∙ Bj = 1 ∧ Gj = 1}. Since clearly 0 ≤ Gj ≤ 1 and(A, B) = ∣{(i,j):
Aij ∙ Bj = 1}|, we have
(A,B)-∣Eι∣≤ EF [σ(F * A),σ(F * B)] ≤ (A, B) + |E0| .
In particular, if we show E0, E1 ⊆ ∂r(A, B), then (16a) will be established. This is what we show
in the rest of the proof.
For readability, we repeat here the two conditions for a pair (i, j ) to be included in the boundary, see
definition 2. Namely, (i, j) belongs to ∂r(A, B) if and only if both conditions below hold:
1.	∃a1, b1, c1, d1 ∈ {-r, . . . , r} : (i + a1,j + b1) ∈ A ∧ (i + c1,j + d1) ∈ B.
2.	∃a2, b2, c2, d2 ∈ {-r, . . . , r} : (i + a2,j + b2) ∈∕ A ∨ (i + c2,j + d2) ∈∕ B.
Let (i, j) be such that Aj ∙ Bj = 0. By equation (30), Gj = 0 if and only if [A]j = 0	or	[B]j	=	0.
So, when A j ∙ Bj = 0, for Gj = 0 to hold it is necessary that there are ai, bi,ci, di ∈	{-r,...,r}
such that Ai+a1，j+b1 = 1 and Ai+c1，j+d1 = 1 which corresponds to the first item in definition 2.
The second item holds with a? = b2 = c2 = d? = 0 since Aj ∙ Bj = 0 so either Aj = 0 or
Bj = 0. This shows that pairs of indices for which Aj∙ ∙ Bj = 0 and Gj = 0 are contained in
∂r(A, B), in other words that E0 ⊆ ∂r(A, B).
Now take (i,j) such that Aj∙ ∙ Bj = 1. By equation (30), Gj = 1 if and only if [A] j =
1(2r+i)×(2r+i) and [B]irj = 1(2r+i)×(2r+i) . So, when Aj ∙ Bj = 1 and Gj = 1, then there
are a2, b2, c2, d2 ∈ {-r, . . . , r} such that Ai+a2，j+b2 = 0 or Bi+c2，j+d2 = 0 which corresponds to
the second item in definition 2. The first item holds with ai = bi = ci = di = 0 since Aj∙ ∙ Bj = 1
so both Aj = 1 and Bj = 1. This shows that pairs of indices for which Aj ∙ Bj = 1 and Gj = 1
are contained in ∂r(A, B), or equivalently that Ei ⊆ ∂r(A, B).
This concludes the proof of (16a). As for (16b), it follows easily by substituting in the formula
IlA — Bll2 = IlAII2 + IlBII2 — 2(A,B). We then use the fact E [∣∣R(F * A)2∣∣] = IlAIl2, which
follows from (9), recalling that V2 = 2∕(2r + 1)2 and VR = 1/2.	□
18