Published as a conference paper at ICLR 2022
Generative Principal Component Analysis
Zhaoqiang Liu *
National University of Singapore
dcslizha@nus.edu.sg
Subhroshekhar Ghosh
National University of Singapore
subhrowork@gmail.com
Jonathan Scarlett
National University of Singapore
scarlett@comp.nus.edu.sg
Jiulong Liu *
Chinese Academy of Sciences
jiulongliu@lsec.cc.ac.cn
Jun Han
PCG, Tencent
junhanjh@tencent.com
Ab stract
In this paper, we study the problem of principal component analysis with genera-
tive modeling assumptions, adopting a general model for the observed matrix that
encompasses notable special cases, including spiked matrix recovery and phase
retrieval. The key assumption is that the first principal eigenvector lies near the
range of an L-Lipschitz continuous generative model with bounded k-dimensional
inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate
of order Jk lθg L, where m is the number of samples. Moreover, We provide a
variant of the classic power method, which projects the calculated data onto the
range of the generative model during each iteration. We show that under suit-
able conditions, this method converges exponentially fast to a point achieving the
above-mentioned statistical rate. This rate is conjectured in (Aubin et al., 2019;
Cocola et al., 2020) to be the best possible even when we only restrict to the spe-
cial case of spiked matrix models. We perform experiments on various image
datasets for spiked matrix and phase retrieval models, and illustrate performance
gains of our method to the classic power method and the truncated power method
devised for sparse principal component analysis.
1 Introduction
Principal component analysis (PCA) is one of the most popular techniques for data processing and
dimensionality reduction (Jolliffe, 1986), with an abundance of applications such as image recogni-
tion (Hancock et al., 1996), gene expression data analysis (Alter et al., 2000), and clustering (Ding
& He, 2004; Liu & Tan, 2019). PCA seeks to find the directions that capture maximal variances
in vector-valued data. In more detail, letting x1 , x2 , . . . , xm be m realizations of a random vector
X ∈ Rn with a population covariance matrix Σ ∈ Rn×n, PCA aims to reconstruct the top principal
eigenvectors of Σ. The first principal eigenvector can be computed as follows:
u1 = arg max wTΣw s.t. kwk2 = 1,	(1)
w∈Rn
where the empirical covariance matrix is defined as Σ := * Pm=I(Xi 一 C)(Xi — C)T, with C :=
ml Pm=I Xi. In addition, subsequent principal eigenvectors can be estimated by similar optimization
problems subject to being orthogonal to the previous vectors.
PCA is consistent in the conventional setting where the dimension of the data n is relatively small
compared to the sample size m (Anderson, 1962), but leads to rather poor estimates in the high-
dimensional setting where m n. In particular, it has been shown in various papers that the empiri-
cal principal eigenvectors are no longer consistent estimates of their population counterparts (Nadler,
* Corresponding authors.
1
Published as a conference paper at ICLR 2022
2008; Johnstone & Lu, 2009; Jung & Marron, 2009; Birnbaum et al., 2013). In order to tackle the
curse of dimensionality, a natural approach is to impose certain structural constraints on the princi-
pal eigenvectors. A common assumption is that the principal eigenvectors are sparse, and this gives
rise to the problem of sparse principal component analysis (SPCA) (Zou et al., 2006). In particular,
for recovering the top principal eigenvector, the optimization problem of SPCA is given by
u1 = arg max wTΣw s.t. kwk2 = 1, kwk0≤s,	(2)
w∈Rn
where kwk0 = |{i : wi 6= 0}| denotes the number of non-zero entries of w, and s ∈ N represents
the sparsity level. In addition to reducing the effective number of parameters, the sparsity assumption
also enhances the interpretability (Zou et al., 2006).
Departing momentarily from the PCA problem, recent years have seen tremendous advances in deep
generative models in a wide variety of real-world applications (Foster, 2019). This has motivated a
new perspective of the related problem of compressed sensing (CS), in which the standard sparsity
assumption is replaced by a generative modeling assumption. That is, the underlying signal is as-
sumed to lie near the range of a (deep) generative model (Bora et al., 2017). The authors of (Bora
et al., 2017) characterized the number of samples required to attain an accurate reconstruction, and
also presented numerical results on image datasets showing that compared to sparsity-based meth-
ods, generative priors can lead to large reductions (e.g., a factor of 5 to 10) in the number of mea-
surements needed to recover the signal up to a given accuracy. Additional numerical and theoretical
results concerning inverse problems using generative models have been provided in (Van Veen et al.,
2018; Dhar et al., 2018; Heckel & Hand, 2019; Jalal et al., 2020; Liu & Scarlett, 2020a; Ongie et al.,
2020; Whang et al., 2020; Jalal et al., 2021; Nguyen et al., 2021), among others.
In this paper, following the developments in both PCA/SPCA and inverse problems with generative
priors, we study the use of generative priors in principal component analysis (GPCA), which gives
a generative counterpart of SPCA in (2), formulated as follows:
u1 = arg max wTΣw s.t. w ∈ Range(G),	(3)
w∈Rn
where G is a (pre-trained) generative model, which we assume has a range contained in the unit
sphere of Rn .1 Similarly to SPCA, the motivation for this problem is to incorporate prior knowl-
edge on the vector being recovered (or alternatively, a prior preference), and to permit meaningful
recovery and theoretical bounds even in the high-dimensional regime m n.
1.1	Related Work
In this subsection, we summarize some relevant works, which can roughly be divided into (i) the
SPCA problem, and (ii) signal recovery with generative models.
SPCA: It has been proved that the solution of the SPCA problem in (2) attains the optimal statistical
rate ʌ/s log n/m (VU & Lei, 2012), where m is the number of samples, n is the ambient dimension,
and s is the sparsity level of the first principal eigenvector. However, due to the combinatorial
constraint, the computation of (2) is intractable. To address this computational issue, in recent
years, an extensive body of practical approaches for estimating sparse principal eigenvectors have
been proposed in the literature, including (d’Aspremont et al., 2007; Vu et al., 2013; Chang et al.,
2016; Moghaddam et al., 2006; d’Aspremont et al., 2008; Jolliffe et al., 2003; Zou et al., 2006; Shen
& Huang, 2008; JoUrnee et al., 2010; Hein & Buhler, 2010; Kuleshov, 2013; Yuan & Zhang, 2013;
Asteris et al., 2011; Papailiopoulos et al., 2013), just to name a few.
Notably, statistical guarantees for several approaches have been provided. The authors of (Yuan
& Zhang, 2013) propose the truncated power method (TPower), which adds a truncation operation
to the power method to ensure the desired level of sparsity. It is shown that this approach attains
the optimal statistical rate under appropriate initialization. Most approaches for SPCA only focus
on estimating the first principal eigenvector, with a certain deflation method (Mackey, 2008) being
leveraged to reconstruct the rest. However, there are some exceptions; for instance, an iterative
thresholding approach is proposed in (Ma, 2013), and is shown to attain a near-optimal statistical rate
1Similarly to (Liu et al., 2020; 2021a), we assume that the range of G is contained in the unit sphere
for convenience. Our results readily transfer to general (unnormalized) generative models by considering its
normalized version. See Remark 1 for a detailed discussion.
2
Published as a conference paper at ICLR 2022
when estimating multiple individual principal eigenvectors. In addition, the authors of (Cai et al.,
2013) propose a regression-type method that gives an optimal principal subspace estimator. Both
works (Ma, 2013; Cai et al., 2013) rely on the assumption of a spiked covariance model to ensure a
good initial vector. To avoid the spiked covariance model assumption, the work (Wang et al., 2014)
proposes a two-stage procedure that attains the optimal subspace estimator in polynomial time.
Signal recovery with generative models: Since the seminal work (Bora et al., 2017), there has been
a substantial volume of papers studying various inverse problems with generative priors. One of the
problems more closely related to PCA is spectral initialization in phase retrieval, which amounts
to solving an eigenvalue problem. Phase retrieval with generative priors has been studied in (Hand
et al., 2018; Hyder et al., 2019; Jagatap & Hegde, 2019; Wei et al., 2019; Shamshad & Ahmed,
2020; Aubin et al., 2020; Liu et al., 2021b). In particular, the work (Hand et al., 2018) models
the underlying signal as being in the range of a fully-connected ReLU neural network with no off-
sets, and all the weight matrices of the ReLU neural network are assumed to have i.i.d. zero-mean
Gaussian entries. In addition, the neural network needs to be sufficiently expansive in the sense
that n ≥ Ω(ni-ι logni-ι), where n is the width of the i-th layer. Under these assumptions, the
authors establish favorable global optimization landscapes for the corresponding objective, and de-
rive a near-optimal sample complexity upper bound. They minimize the objective function directly
over the latent space in Rk using gradient descent, which may suffer from local minima in general
optimization landscapes (Hyder et al., 2019; Shah & Hegde, 2018).
In (Aubin et al., 2020), the assumptions on the neural network are similar to those in (Hand et al.,
2018), relaxing to general activation functions (beyond ReLU) and n ≥ Ω(ni-ι). The authors focus
on the high dimensional regime where n, m, k → ∞ with the ratio m/n being fixed, and assume that
the input vector in Rk is drawn from a separable distribution. They derive sharp asymptotics for the
information-theoretically optimal performance and for the associated approximate message passing
(AMP) algorithm. Both works (Hand et al., 2018; Aubin et al., 2020) focus on noiseless phase
retrieval. When only making the much milder assumption that the generative model is Lipschitz
continuous, with no assumption on expansiveness, Gaussianity, and offsets, a spectral initialization
step (similar to that of sparse phase retrieval) is typically required in order to accurately reconstruct
the signal (Netrapalli et al., 2015; CandeS et al., 2015). The authors of (LiU et al., 2021b) propose an
optimization problem similar to (3) for the spectral initialization for phase retrieval with generative
models. It was left open in (Liu et al., 2021b) how to solve (or approximate sufficiently accurately)
the optimization problem in practice.
Understanding the eigenvalues of spiked random matrix models has been a central problem of ran-
dom matrix theory, and spiked matrices have been widely used in the statistical analysis of SPCA.
Recently, theoretical guarantees concerning spiked matrix models with generative priors have been
provided in (Aubin et al., 2019; Cocola et al., 2020). In particular, in (Aubin et al., 2019), the as-
sumptions are similar to those in (Aubin et al., 2020), except that the neural network is assumed to
have exactly one hidden layer. The Bayes-optimal performance is analyzed, and it is shown that the
AMP algorithm can attain this optimal performance. In addition, the authors of (Aubin et al., 2019)
propose the linearized approximate message passing (LAMP) algorithm, which is a spectral algo-
rithm specifically designed for single-layer feedforward neural networks with no bias terms. The
authors show its superiority to classical PCA via numerical results on the Fashion-MNIST dataset.
In (Cocola et al., 2020), the same assumptions are made as those in (Hand et al., 2018) on the neural
network, and the authors demonstrate the benign global geometry for a nonlinear least squares ob-
jective. Similarly to (Hand et al., 2018), the objective is minimized over Rk using a gradient descent
algorithm, which can get stuck in local minima for general global geometries.
1.2	Contributions
The main contributions of this paper are as follows:
•	We study eigenvalue problems with generative priors (including GPCA), and characterize
the statistical rate of a quadratic estimator similar to (3) under suitable assumptions.
•	We propose a variant of the classic power method, which uses an additional projection
operation to ensure that the output of each iteration lies in the range of a generative model.
We refer to our method as projected power method (PPower). We further show that under
appropriate conditions (most notably, assuming exact projections are possible), PPower
3
Published as a conference paper at ICLR 2022
obtains a solution achieving a statistical rate that is conjectured to be optimal in (Aubin
et al., 2019; Cocola et al., 2020) for spiked matrix models.
•	For the spiked matrix and phase retrieval models, we perform numerical experiments on
image datasets, and demonstrate that when the number of samples is relatively small com-
pared to the ambient dimension, PPower leads to significantly better performance compared
to the classic power method and TPower.
Compared to the above-mentioned works that use generative models, we make no assumption on
expansiveness, Gaussianity, and offsets for the generative model, and we consider a data model that
simultaneously encompasses both spiked matrix and phase retrieval models, among others.
1.3	Notation
We use upper and lower case boldface letters to denote matrices and vectors respectively. We write
[N ] = {1,2,…，N} for a positive integer N, and we use IN to denote the identity matrix in RN ×n .
A generative model is a function G : D → Rn , with latent dimension k, ambient dimension n, and
input domain D ⊆ Rk . We focus on the setting where k n. For a set S ⊆ Rk and a generative
model G : Rk → Rn, we write G(S) = {G(z) : z ∈ S}. We use kXk2→2 to denote the spectral
norm of a matrix X. We define the 'q-ball Bk(r) := {z ∈ Rk : kz∣∣q ≤ r} for q ∈ [0, +∞].
Sn-1 := {x ∈ Rn : kxk2 = 1} represents the unit sphere in Rn. The symbols C, C0, C00 are
absolute constants whose values may differ from line to line.
2	Problem Setup
In this section, we formally introduce the problem, and overview some important assumptions that
we adopt. Except where stated otherwise, we will focus on the following setting:
•	We have a matrix V ∈ Rn×n satisfying
V = V + E,	(4)
where E is a perturbation matrix, and V is assumed to be positive semidefinite (PSD). For
PCA and its constrained variants, V and V can be thought of as the empirical and population
covariance matrices, respectively.
•	We have an L-Lipschitz continuous generative model G : B2k (r) → Rn. For convenience,
similarly to that in (Liu et al., 2020), we assume that Range(G) ⊆ Sn-1.
Remark 1. Fora general (unnormalized) L-Lipschitz continuous generative model G, we can
instead consider a Corresponding normalized generative model G : D → Sn-1 as in (Liu
et al., 2021b), where D := {z ∈ B2k (r) : kG(z)k2 > Rmin} for some Rmin > 0, and
G(Z) = kGZ)k2 ∙ Then，the Lipschitz constant of G becomes L/Rmin. For a d-layer neural
network, we typically have L = nΘ(d) (Bora et al., 2017). Thus, we can set Rmin to be as
small as 1∕nθ(d) without changing the scaling laws, which makes the dependence on Rmin
very mild.
•	We aim to solve the following eigenvalue problem with a generative prior:2
V := max WTVW s.t. W ∈ Range(G).	(5)
w∈Rn
Note that since Range(G) ⊆ Sn-1, we do not need to impose the constraint kWk2 = 1. Since
V is not restricted to being an empirical covariance matrix, (5) is more general than GPCA
in (3). However, we slightly abuse terminology and also refer to (5) as GPCA.
•	To approximately solve (5), we use a projected power method (PPower), which is described
by the following iterative procedure:3
w(t+1) = PG(VW(t)),	(6)
2To find the top r rather than top one principal eigenvectors that are in the range of a generative model,
we may follow the common approach to use the iterative deflation method for PCA/SPCA: Subsequent prin-
cipal eigenvectors are derived by recursively removing the contribution of the principal eigenvectors that are
calculated already under the generative model constraint. See for example (Mackey, 2008).
3In similar iterative procedures, some works have proposed to replace V by V + ρIn for some ρ ∈ R to
improve convergence, e.g., see Deshpande et al. (2014).
4
Published as a conference paper at ICLR 2022
Algorithm 1 A projected power method for GPCA (PPower)
Input: V, number of iterations T, pre-trained generative model G, initial vector w(0)
Procedure: Iterate w(t+1) = PG (Vw(t)) for t ∈ {0,1,...,T - 1}, and return W(T)
where Pg(∙) is the projection function onto G(Bk(r)),4 and the initialization vector w(0)
may be chosen either manually or randomly, e.g., uniform over Sn-1. Often the initialization
vector w(0) plays an important role and we may need a careful design for it. For example, for
phase retrieval with generative models, as mentioned in (Liu et al., 2021b, Section V), we may
choose the column corresponding to the largest diagonal entry of V as the starting point. See
also (Yuan & Zhang, 2013, Section 3) fora discussion on the initialization strategy for TPower
devised for SPCA. We present the algorithm corresponding to (6) in Algorithm 1.
Remark 2. To tackle generalized eigenvalue problems encountered in some specific appli-
cations, there are variants of the projected power method, which combine a certain power
iteration with additional operations to ensure sparsity or enforce other constraints. These ap-
plications include but not limited to sparse PCA (Journee et al., 2010; Yuan & Zhang, 20l3),
phase synchronization (Boumal, 2016; Liu et al., 2017), the hidden clique problem (Desh-
Pande & Montanari, 2015), the joint alignment problem (Chen & Candes, 2018), and Cone-
constrained PCA (Deshpande et al., 2014; Yi & Neykov, 2020). For example, under the simple
spiked Wigner model (Perry et al., 2018) for the observed data matrix V with the underlying
signal being assumed to lie in a convex cone, the authors of (Deshpande et al., 2014) show that
cone-constrained PCA can be computed efficiently via a generalized projected power method.
In general, the range ofa Lipschitz-continuous generative model is non-convex and not a cone.
In addition, we consider a matrix model that is more general than the spiked Wigner model.
Although it is not needed for our main results, we first state a lemma (proved in Appendix A) that
establishes a monotonicity property with minimal assumptions, only requiring that V is PSD; see
also Proposition 3 of Yuan & Zhang (2013) for an analog in sparse PCA. By comparison, our main
results in Section 4 will make more assumptions, but will also provide stronger guarantees. Note
that the PSD assumption holds, for example, when E = 0, or when V is a sample covariance matrix.
Lemma 1. For any x ∈ Rn, let Q(x) = xTVx. Then, if V is PSD, the sequence {Q(w(t))}t>0 for
w(t) in (6) is monotonically non-decreasing.
3	Specialized Data Models and Examples
In this section, We make more specific assumptions on V = V + E, starting with the following.
Assumption 1 (Assumption on V). Assume that V is PSD with eigenvalues λι > % ≥ … ≥
λn ≥ 0. We use x (a unit vector) to represent the eigenvector of V that corresponds to λι.
In the following, it is useful to think of x is being close to the range of the generative model G. In the
special case of (3), letting m be the number of samples, it is natural to derive that the upper bound of
IlEIl 2→2 grows linearly in (n/m)b for some positive constant b such as 2 or 1 (with high probability;
see, e.g., (Vershynin, 2010, Corollary 5.35)). In the following, we consider general scenarios with
V depending on m samples (see below for specific examples). Similarly to (Yuan & Zhang, 2013),
we may consider a restricted version of IEI2→2, leading to the following.
Assumption 2 (Assumption on E). Let S1, S2 be two (arbitrary) finite sets in Rn satisfying m =
Ω(log(∣Sι∣∙ ∣S2∣)). Then, we have for all si ∈ Si and s2 ∈ S2 that
∣sT Es2∣ ≤ C r Iog(IS小⑸I) ∙ksι∣2 ∙ks2k2,	⑺
m
where C is an absolute constant. In addition, we have IEI2→2 = O(n/m).4 5
4That is, for any x ∈ Rn, PG(x) := arg minw∈Range(G) kw - xk2. We will implicitly assume that the
projection step can be performed accurately, e.g., (Deshpande et al., 2014; Shah & Hegde, 2018; Peng et al.,
2020), though in practice approximate methods might be needed, e.g., via gradient descent (Shah & Hegde,
2018) or GAN-based projection methods (Raj et al., 2019).
5For the spectral norm of E, one often expects an even tighter bound O(ʌ/n/m), but we use O(n∕m)
to simplify the analysis of our examples. Moreover, at least under the typical scaling where L is polynomial
5
Published as a conference paper at ICLR 2022
The following examples show that when the number of measurements is sufficiently large, the data
matrices corresponding to certain spiked matrix and phase retrieval models satisfy the above as-
sumptions with high probability. Short proofs are given in Appendix B for completeness.
Example 1 (Spiked covariance model). In the spiked covariance model (Johnstone & Lu, 2009;
Deshpande & Montanari, 2016), the observed vectors x1, x2, . . . , xm ∈ Rn are of the form
r
Xi =): Pβquq,isq + zi,	(8)
q=1
where si,..., Sr ∈ Rn are orthonormal vectors that we want to estimate, while Zi 〜 N(0, In)
and Uq,i 〜N (0,1) are independent and identically distributed. In addition, βι,... ,βr are positive
constants that dictate the signal-to-noise ratio (SNR). To simplify the exposition, we focus on the
rank-one case and drop the subscript q ∈ [r]. Let
1m
V = m X(XiXT - In),	(9)
i=1
and V = E[V] = βssτ.6 Then, V satisfies Assumption 1 with ʌi = β > 0, λ = ... = Xn = 0,
and X = s. In addition, letting E = V 一 V, the Bernstein-type inequality (Vershynin, 2010,
Proposition 5.10) for the sum of sub-exponential random variables yields that for any finite sets
Si, S2 ⊂ Rn, when m = Ω( log( | Si | ∙ ∣S2∣)), with probability 1 一 e-Q(log(|Sl|TS2|)), E satisfies (7)
in Assumption 2. Moreover, standard concentration arguments give kEk2→2 = O(n/m) with prob-
ability 1 一 e-Q(n).
Remark 3. We can also consider the simpler spiked Wigner model (Perry et al., 2018; Chung &
Lee, 2019) where V = βssτ + √n H, with the signal s being a unit vector, β > 0 being an SNR
parameter, and H ∈ Rn×n being a symmetric matrix with entries drawn i.i.d. (up to symmetry) from
N(0, 1). In this case, when m = n is sufficiently large, with high probability, VX := E[V] = βssτ
and E := V 一 VX similarly satisfy Assumptions 1 and2 respectively.
Example 2 (Phase retrieval). Let A ∈ Rm×n be a matrix having i.i.d. N(0, 1) entries, and let aiτ
be the i-th row of A. For some unit vector s, suppose that the observed vector is y = |As|, where
the absolute value is applied element-wise.7 We construct the weighted empirical covariance matrix
as follows (Zhang et al., 2017; Liu et al., 2021b):
1m
V = 一 E yiaiaT 1{i<yi<u} - YIn),	(10)
m i=i
where u > l > 1 are positive constants, and for g 〜 N(0, 1), γ := E |g|1{l<|g|<u} . Let VX =
E[V] = βssτ, where β := E |g|3 一 |g| 1{l<|g|<u} . Then, VX satisfies Assumption 1 with λXi =
β > 0, λX2 = . . . = λXn = 0, and XX = s. In addition, letting E = V 一 VX, we have similarly to
Example 1 that E satisfies Assumption 2 with high probability.
4	Main Results
The following theorem concerns globally optimal solutions of (5). The proof is given in Appendix D.
Theorem 1. LetV = VX +E with Assumptions 1 and2 being satisfied by VX and E respectively, and
let XG := PG(X) = argmiηw∈Raηge(G) ∣∣w — X∣∣2. Suppose that V is a globally optimal solution
to (5). Then, for any δ ∈ (0, 1), we have
∣v vτ - XXT kF=⅛!P+° (s≡!+o g+、k2), (id
where en = Oqk 空IL).
in n (Bora et al., 2017), the upper bound for kEk2→2 can be easily relaxed to O (n/m)b for any positive
constant b, without affecting the scaling of our derived statistical rate.
6To avoid non-essential complications, β is typically assumed to be known (Johnstone & Lu, 2009).
7Without loss of generality, we assume that s is a unit vector. For a general signal s, we may instead focus
on estimating S = s∕ks∣∣2, and simply use ml pm=ι yi to approximate ∣∣sk2.
6
Published as a conference paper at ICLR 2022
We have stated this result as an upper bound on ∣∣vVT - XxTkf, which intuitively measures the
distance between the 1D subspaces spanned by V and x. Note, however, that for any two unit
vectors w1, w2 with w1T w2 ≥ 0, the distances kw1 - w2k2 and kw1w1T - w2w2TkF are equivalent
up to constant factors, whereas if w1T w2 ≤ 0, then a similar statement holds for kw1 + w2 k2 . See
Appendix C for the precise statements.
In Theorem 1, the final term quantifies the effect of representation error. When there is no such error
(i.e., x ∈ Range(G)), under the scaling λι - λ2 = Θ(1), L = nQ(1), and δ = O(1∕n), Theorem 1
simplifies to ∣∣VVT — xxτ∣∣F = O( Jk lθg L). This provides a natural counterpart to the statistical
rate of order JS * n for SPCA mentioned in Section 1.1.
Before providing our main theorem for PPower described in (6), we present the following important
lemma, whose proof is presented in Appendix E. To simplify the statement of the lemma, we fix δ
to be O(1/n), though a more general form analogous to Theorem 1 is also possible.
Lemma 2. Let V = V + E with Assumptions 1 and 2 being satisfied by V and E respectively, and
further assume that x ∈ Range(G). Let Y = X2/X1 with λι = Θ(1). Then, for all S ∈ Range(G)
satisfying STx > 0, we have
∣∣Pg(Vs)- x∣∣2 ≤
2Yks - x∣2
ST x
k log(nLr)
m
(12)
where C is an absolute constant.
Remark 4. The assumption STx > 0 will be particularly satisfied when the range of G only Con-
tains nonnegative vectors. As mentioned in various works studying nonnegative SPCA (Zass &
Shashua, 2007; Sigg & Buhmann, 2008; Asteris et al., 2014), for several practical fields such as
economics, bioinformatics, and computer vision, it is natural to assume that the underlying signal
has no negative entries. More generally, the assumption STx > 0 can be removed ifwe additionally
have that -x is also contained in the range of G. For this case, when STx < 0, we can instead
derive an upper boundfor ∣∣S + x∣2.
Based on Lemma 2, we have the following theorem, whose proof is given in Appendix F.
Theorem 2. Suppose that the assumptions on the data model V
Lemma 2, and assume that there exists to ∈ N such that xTw(t0) :
——
V + E are the same as those in
Y = λ2∕λ1 ∈ [0,1), and ν, T are
2γ+ν with 2工+ν
both positive and scale as Θ(1). Let μo = 又T;匕)
≤ 1-τ, where
=2γ < 1
27+ν < 1,
and in addition, suppose that m ≥ Cν,τ ∙ k log(nLr) with Cν,τ > 0 being large enough. Then, we
have after ∆o = O( log (kiogmLr))) iterations ofPPower (beyond to) that
kw(t)-xk2 ≤	JkJog(nLr),	(13)
(1 - μo)ν V m
i.e., this equation holds for all t ≥ To := to + ∆o. Moreover, if Y = 0 then ∆o ≤ 1, whereas if
Y = Θ(1), we have exponentially fast convergence via the following contraction property: There
exists a constant ξ ∈ (0, 1) such that for t ∈ [to, To), it holds that
∣∣w(t+1) - x∣2 ≤ (1 - ξ)∣w(t) - x∣2.	(14)
Regarding the assumption xTw(t0) ≥ 27 + V, we note that when to = 0, this condition can be
viewed as having a good initialization. For both Examples 1 and 2, we have Y = 0. Thus, for
the spiked covariance and phase retrieval models corresponding to these examples, the assumption
x7Tw(t0) ≥ 2Y7 + ν reduces to x7Tw(t0) ≥ ν fora sufficiently small positive constant ν, which results
in a mild assumption. Such an assumption is also required for the projected power method devised
for cone-constrained PCA under the simple spiked Wigner model, with the underlying signal being
assumed to lie in a convex cone; see (Deshpande et al., 2014, Theorem 3). Despite using a similar
assumption on the initialization, our proof techniques are significantly different from Deshpande
et al. (2014); see Appendix G for discussion.
When L is polynomial in n, Theorem 2 reveals that we have established conditions under which
PPower in (6) converges exponentially fast to a point achieving the statistical rate of order ʌ k lθg l .
7
Published as a conference paper at ICLR 2022
Based on the minimax rates for SPCA (Vu & Lei, 2012; Birnbaum et al., 2013) and the information-
theoretic lower bounds for CS with generative models (LiU & Scarlett, 2020b; Kamath et al., 2020),
the optimal rate for GPCA is naturally conjectured to be of the same order Jk 臂 L. We highlight
that Theorem 2 partially addresses the computational-to-statistical gap (e.g., see (Wang et al., 2016;
Hand et al., 2018; Aubin et al., 2019; Cocola et al., 2020)) for spiked matrix recovery and phase
retrieval under a generative prior, though closing it completely would require efficiently finding a
good initialization and addressing the assumption of exact projections.
Perhaps the main caveat to Theorem 2 is that it assumes the projection step can be performed exactly.
However, this is a standard assumption in analyses of projected gradient methods, e.g., see (Shah &
Hegde, 2018), and both gradient-based projection and GAN-based projection have been shown to
be highly effective in practice (Shah & Hegde, 2018; Raj et al., 2019).
5	Experiments
In this section, we experimentally study the performance of Algorithm 1 (PPower). We note that
these experiments are intended as a simple proof of concept rather than seeking to be comprehen-
sive, as our contributions are primarily theoretical. We compare with the truncated power method
(TPower) devised for SPCA proposed in (Yuan & Zhang, 2013, Algorithm 1) and the vanilla power
method (Power) that performs the iterative procedure w(t+1) = (Vw(t))/kVw(t) k2. For a fair
comparison, for PPower, TPower, and Power, we use the same initial vector. Specifically, as
mentioned in (Liu et al., 2021b, Section V), we choose the initialization vector w(0) as the column
of V that corresponds to its largest diagonal entry. For all three algorithms, the total number of iter-
ations T is set to be 30. To compare the performance across algorithms, we use the scale-invariant
hx∖w	hx .W(T )i
Cosine Similarity metric defined as Cossim (x, W(T)) := kχ*kw(τ,where X is the ground-truth
signal to estimate, and w(T) denotes the output vector of the algorithm.
The experiments are performed on the MNIST (LeCun et al., 1998), Fashion-MNIST (Xiao et al.,
2017) and CelebA (Liu et al., 2015) datasets, with the numerical results for the Fashion-MNIST
and CelebA datasets being presented in Appendix H and I. The MNIST dataset consists of 60, 000
images of handwritten digits. The size of each image is 28 × 28, and thus n = 784. To reduce the
impact of local minima, we perform 10 random restarts, and choose the best among these. The cosine
similarity is averaged over the test images, and also over these 10 random restarts. The generative
model G is set to be a pre-trained variational autoencoder (VAE) model with latent dimension k =
20. We use the VAE model trained by the authors of (Bora et al., 2017) directly, for which the
encoder and decoder are both fully connected neural networks with two hidden layers, with the
architecture being 20 - 500 - 500 - 784. The VAE is trained by the Adam optimizer with a mini-
batch size of 100 and a learning rate of 0.001. The projection step Pg(∙) is solved by the Adam
optimizer with a learning rate of 0.03 and 200 steps. In each iteration of TPower, the calculated
entries are truncated to zero except for the largest q entries, where q ∈ N is a tuning parameter.
Since for TPower, q is usually selected as an integer larger than the true sparsity level, and since it
is unlikely that the image of the MNIST dataset can be well approximated by a k-sparse vector with
k = 20, we choose a relatively large q, namely q = 150. Similarly to (Bora et al., 2017) and other
related works, we only report the results on a test set that is unseen by the pre-trained VAE model,
i.e., the training of G and the PPower computations do not use common data.8
1.	Spiked covariance model (Example 1): The numerical results are shown in Figures 1 and 2.
We observe from Figure 1 that Power and TPower attain poor reconstructions, and the
generative prior based method PPower attains significantly better reconstructions. To il-
lustrate the effect of the sample size m, we fix the SNR parameter β = 1 and vary m in
{100, 200, 300, 400, 500}. In addition, to illustrate the effect of the SNR parameter β, we fix
m = 300, and vary β in {0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4}. From Figure 2, we observe that for
these settings ofm and β, PPower always leads to a much higher cosine similarity compared
to Power and TPower, which is natural given the more precise modeling assumptions used.
8All experiments are run using Python 3.6 and Tensorflow 1.5.0, with a NVIDIA GeForce GTX
1080 Ti 11GB GPU. The corresponding code is available at https://github.com/liuzq09/
GenerativePCA.
8
Published as a conference paper at ICLR 2022
I万？⑺aq⑺旧因固同I万？⑺©回⑺旧回回回
(b) β = 2 and m = 100
(a) β = 1 and m = 200
100 150 200 250 300 350 400 450 500
m
Figure 1: Examples of reconstructed images of the MNIST dataset for the spiked covariance model.
—t— Power
—⅛- TPower
—f— PPower
—f— Power
—f- TPower
-4— PPower
n 0.6
!θ,4
c.
ω
8 0.2
0.0
ro 0.6
in 0.4
0.4
U 0.2
0.0
0.0
200 400 600 800 10001200 1400 1600
m
J 0.2
(a) Fixing β = 1 and varying m (b) Fixing m = 300 and varying β
(c) Analog of (a) for phase ret.
Figure 2: Quantitative comparisons of the performance of Power, TPower and PPower ac-
cording to the Cosine Similarity for the MNIST dataset, under both the spiked covariance model
(Left/Middle) and phase retrieval model (Right).
2.	Phase retrieval (Example 2): The results are shown in Figure 2 (Right) and Figure 3. Again,
we can observe that PPower significantly outperforms Power and TPower. In particular, for
sparse phase retrieval, when performing experiments on image datasets, even for the noiseless
setting, solving an eigenvalue problem similar to (5) can typically only serve as a spectral
initialization step, with a subsequent iterative algorithm being required to refine the initial
guess. In view of this, it is notable that for phase retrieval with generative priors, PPower can
return meaningful reconstructed images for m = 200, which is small compared to n = 784.
6	Conclusion
We have proposed a quadratic estimator for eigenvalue problems with generative models, and We
showed that this estimator attains a statistical rate of order Jk 臂 L. We provided a projected power
method to efficiently solve (modulo the complexity of the projection step) the corresponding opti-
mization problem, and showed that our method converges exponentially fast to a point achieving a
statistical rate of order ʌ∕k 呼 L under suitable conditions.
Acknowledgment. J.S. was supported by the Singapore National Research Foundation (NRF) under
grant R-252-000-A74-281, and S.G. was supported in part by the MOE grants R-146-000-250-133,
R-146-000-312-114, and MOE-T2EP20121-0013.
"--6=0 JaMOd ^SS£H MOdd
(a) m = 200
(b) m = 400
Figure 3: Examples of reconstructed images of the MNIST dataset for phase retrieval.
9
Published as a conference paper at ICLR 2022
References
Orly Alter, Patrick O Brown, and David Botstein. Singular value decomposition for genome-wide
expression data processing and modeling. PNAS, 97(18):10101-10106, 2000.
Theodore Wilbur Anderson. An introduction to multivariate statistical analysis. Wiley New York,
1962.
Megasthenis Asteris, Dimitris S Papailiopoulos, and George N Karystinos. Sparse principal compo-
nent of a rank-deficient matrix. In Int. Symp. Inf. Theory (ISIT), pp. 673-677. IEEE, 2011.
Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse PCA
with provable guarantees. In Int. Conf. Mach. Learn. (ICML), pp. 1728-1736. PMLR, 2014.
Benjamin Aubin, Bruno Loureiro, Antoine Maillard, et al. The spiked matrix model with generative
priors. Conf. Neur. Inf. Proc. Sys. (NeurIPS), 32:8366-8377, 2019.
Benjamin Aubin, Bruno Loureiro, Antoine Baker, et al. Exact asymptotics for phase retrieval and
compressed sensing with random generative priors. In Math. Sci. Mach. Learn. (MSML), pp.
55-73. PMLR, 2020.
Aharon Birnbaum, Iain M Johnstone, Boaz Nadler, and Debashis Paul. Minimax bounds for sparse
PCA with noisy high-dimensional data. Ann. Stat., 41(3):1055, 2013.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In Int. Conf. Mach. Learn. (ICML), pp. 537-546, 2017.
Nicolas Boumal. Nonconvex phase synchronization. SIAM J. Optim., 26(4):2355-2377, 2016.
T Tony Cai, Zongming Ma, and Yihong Wu. Sparse PCA: Optimal rates and adaptive estimation.
Ann. Stat., 41(6):3074-3110, 2013.
Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger flow:
Theory and algorithms. IEEE Trans. Inf. Theory, 61(4):1985-2007, 2015.
Xiaojun Chang, Feiping Nie, Yi Yang, et al. Convex sparse PCA for unsupervised feature learning.
ACM T. Knowl. Discov. D., 11(1):1-16, 2016.
Yuxin Chen and Emmanuel J Candes. The projected power method: An efficient algorithm forjoint
alignment from pairwise differences. Comm. Pure Appl. Math., 71(8):1648-1714, 2018.
Hye Won Chung and Ji Oon Lee. Weak detection of signal in the spiked Wigner model. In Int. Conf.
Mach. Learn. (ICML), pp. 1233-1241. PMLR, 2019.
Jorio Cocola, Paul Hand, and Vlad Voroninski. Nonasymptotic guarantees for spiked matrix recov-
ery with generative priors. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), volume 33, 2020.
Alexandre d’Aspremont, Laurent El Ghaoui, Michael I Jordan, and Gert RG Lanckriet. A direct
formulation for sparse PCA using semidefinite programming. SIAM Rev., 49(3):434-448, 2007.
Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse princi-
pal component analysis. J. Mach. Learn. Res., 9(7), 2008.
Yash Deshpande and Andrea Montanari. Finding hidden cliques of size，N/e in nearly linear time.
Found. Comput. Math., 15(4):1069-1128, 2015.
Yash Deshpande and Andrea Montanari. Sparse PCA via covariance thresholding. J. Mach. Learn.
Res., 17(1):4913-4953, 2016.
Yash Deshpande, Andrea Montanari, and Emile Richard. Cone-constrained principal component
analysis. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), pp. 2717-2725, 2014.
Manik Dhar, Aditya Grover, and Stefano Ermon. Modeling sparse deviations for compressed sensing
using generative models. In Int. Conf. Mach. Learn. (ICML), pp. 1214-1223. PMLR, 2018.
10
Published as a conference paper at ICLR 2022
Chris Ding and Xiaofeng He. K-means clustering via principal component analysis. In Int. Conf.
Mach. Learn. (ICML), pp. 29, 2004.
David Foster. Generative Deep Learning : Teaching Machines to Paint, Write, Compose and Play.
O’Reilly Media, Inc, USA, 2019.
Peter JB Hancock, A Mike Burton, and Vicki Bruce. Face processing: Human perception and
principal components analysis. Mem. Cogn., 24(1):26-40, l996.
Paul Hand, Oscar Leong, and Vladislav Voroninski. Phase retrieval under a generative prior. In
Conf. Neur. Inf. Proc. Sys. (NeurIPS), pp. 9154-9164, 2018.
Reinhard Heckel and Paul Hand. Deep decoder: Concise image representations from untrained
non-convolutional networks. In Int. Conf. Learn. Repr. (ICLR), 2019.
Matthias Hein and Thomas Buhler. An inverse power method for nonlinear eigenProbIems with
applications in 1-spectral clustering and sparse PCA. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), pp.
847-855, 2010.
Rakib Hyder, Viraj Shah, Chinmay Hegde, andM Salman Asif. Alternating phase projected gradient
descent with generative priors for solving compressive phase retrieval. In IEEE Int. Conf. Acoust.
Sp. Sig. Proc. (ICASSP), pp. 7705-7709, 2019.
Gauri Jagatap and Chinmay Hegde. Algorithmic guarantees for inverse imaging with untrained
network priors. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), volume 32, 2019.
A Jalal, S Karmalkar, A Dimakis, and E Price. Instance-optimal compressed sensing via posterior
sampling. In Int. Conf. Mach. Learn. (ICML), 2021.
Ajil Jalal, Liu Liu, Alexandros G Dimakis, and Constantine Caramanis. Robust compressed sensing
using generative models. Conf. Neur. Inf. Proc. Sys. (NeurIPS), 2020.
Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis
in high dimensions. J. Am. Stat. Assoc., 104(486):682-693, 2009.
Ian T. Jolliffe. Principal component analysis. Springer-Verlag, 1986.
Ian T Jolliffe, Nickolay T Trendafilov, and Mudassir Uddin. A modified principal component tech-
nique based on the Lasso. J. Comput. Graph. Stat., 12(3):531-547, 2003.
Michel Journee, Yurii Nesterov, Peter Richtarik, and Rodolphe Sepulchre. Generalized power
method for sparse principal component analysis. J. Mach. Learn. Res., 11(2), 2010.
Sungkyu Jung and J Stephen Marron. PCA consistency in high dimension, low sample size context.
Ann. Stat., 37(6B):4104-4130, 2009.
Akshay Kamath, Sushrut Karmalkar, and Eric Price. On the power of compressed sensing with
generative models. In Int. Conf. Mach. Learn. (ICML), pp. 5101-5109, 2020.
Volodymyr Kuleshov. Fast algorithms for sparse principal component analysis based on Rayleigh
quotient iteration. In Int. Conf. Mach. Learn. (ICML), pp. 1418-1425. PMLR, 2013.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE, 86(11):2278-2324, 1998.
Huikang Liu, Man-Chung Yue, and Anthony Man-Cho So. On the estimation performance and
convergence rate of the generalized power method for phase synchronization. SIAM J. Optim., 27
(4):2426-2446, 2017.
Zhaoqiang Liu and Jonathan Scarlett. The generalized Lasso with nonlinear observations and gen-
erative priors. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), volume 33, 2020a.
Zhaoqiang Liu and Jonathan Scarlett. Information-theoretic lower bounds for compressive sensing
with generative models. IEEE J. Sel. Areas Inf. Theory, 1(1):292-303, 2020b.
11
Published as a conference paper at ICLR 2022
Zhaoqiang Liu and Vincent YF Tan. The informativeness of k-means for learning mixture models.
IEEE Trans. Inf. Theory, 65(11):7460-7479, 2019.
Zhaoqiang Liu, Selwyn Gomes, Avtansh Tiwari, and Jonathan Scarlett. Sample complexity bounds
for 1-bit compressive sensing and binary stable embeddings with generative priors. In Int. Conf.
Mach. Learn. (ICML), 2020.
Zhaoqiang Liu, Subhroshekhar Ghosh, Jun Han, and Jonathan Scarlett. Robust 1-
bit compressive sensing with partial Gaussian circulant matrices and generative priors.
https://arxiv.org/2108.03570, 2021a.
Zhaoqiang Liu, Subhroshekhar Ghosh, and Jonathan Scarlett. Towards sample-optimal compressive
phase retrieval with sparse and generative priors. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), 2021b.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Int. Conf. Comput. Vis. (ICCV), pp. 3730-3738, 2015.
Zongming Ma. Sparse principal component analysis and iterative thresholding. Ann. Stat., 41(2):
772-801, 2013.
Lester W Mackey. Deflation methods for sparse PCA. In Conf. Neur. Inf. Proc. Sys. (NeurIPS),
volume 21, pp. 1017-1024, 2008.
Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral bounds for sparse PCA: Exact and
greedy algorithms. Conf. Neur. Inf. Proc. Sys. (NeurIPS), 18:915, 2006.
Boaz Nadler. Finite sample approximation results for principal component analysis: A matrix per-
turbation approach. Ann. Stat., 36(6):2791-2817, 2008.
Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimiza-
tion. IEEE Trans. Sig. Proc., 63(18):4814-4826, 2015.
Thanh V Nguyen, Gauri Jagatap, and Chinmay Hegde. Provable compressed sensing with generative
priors via Langevin dynamics. https://arxiv.org/2102.12643, 2021.
Gregory Ongie, Ajil Jalal, Christopher A Metzler, et al. Deep learning techniques for inverse prob-
lems in imaging. IEEE J. Sel. Areas Inf. Theory, 1(1):39-56, 2020.
Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis. Sparse PCA through low-
rank approximations. In Int. Conf. Mach. Learn. (ICML), pp. 747-755. PMLR, 2013.
Pei Peng, Shirin Jalali, and Xin Yuan. Solving inverse problems via auto-encoders. IEEE J. Sel.
Areas Inf. Theory, 1(1):312-323, 2020.
Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and sub-
optimality of PCA I: Spiked random matrix models. Ann. Stat., 46(5):2416-2451, 2018.
Ankit Raj, Yuqi Li, and Yoram Bresler. GAN-based projector for faster recovery with convergence
guarantees in linear inverse problems. In IEEE/CVF Int. Conf. Comp. Vision (ICCV), 2019.
Viraj Shah and Chinmay Hegde. Solving linear inverse problems using GAN priors: An algorithm
with provable guarantees. In Int. Conf. Acoust. Sp. Sig. Proc. (ICASSP), pp. 4609-4613. IEEE,
2018.
Fahad Shamshad and Ali Ahmed. Compressed sensing-based robust phase retrieval via deep gener-
ative priors. IEEE Sens. J., 21(2):2286-2298, 2020.
Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank
matrix approximation. J. Multivar. Anal., 99(6):1015-1034, 2008.
Christian D Sigg and Joachim M Buhmann. Expectation-maximization for sparse and non-negative
PCA. In Int. Conf. Mach. Learn. (ICML), pp. 960-967, 2008.
Dave Van Veen, Ajil Jalal, Mahdi Soltanolkotabi, et al. Compressed sensing with deep image prior
and learned regularization. https://arxiv.org/1806.06438, 2018.
12
Published as a conference paper at ICLR 2022
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices.
https://arxiv.org/abs/1011.3027, 2010.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Vincent Vu and Jing Lei. Minimax rates of estimation for sparse PCA in high dimensions. In Int.
Conf. Artif. Intell. Stat. (AISTATS) ,pp.1278-1286. PMLR, 2012.
Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe. Fantope projection and selection: A near-optimal
convex relaxation of sparse PCA. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), volume 26, 2013.
Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs
in estimation of sparse principal components. Ann. Stat., 44(5):1896-1930, 2016.
Zhaoran Wang, Huanran Lu, and Han Liu. Tighten after relax: Minimax-optimal sparse PCA in
polynomial time. In Conf. Neur. Inf. Proc. Sys. (NeurIPS), 2014.
Xiaohan Wei, Zhuoran Yang, and Zhaoran Wang. On the statistical rate of nonlinear recovery in
generative models with heavy-tailed data. In Int. Conf. Mach. Learn. (ICML), pp. 6697-6706,
2019.
Jay Whang, Qi Lei, and Alexandros G Dimakis. Compressed sensing with invertible generative
models and dependent noise. https://arxiv.org/2003.08089, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for bench-
marking machine learning algorithms. https://arxiv.org/1708.07747, 2017.
Yufei Yi and Matey Neykov. Non-sparse PCA in high dimensions via cone projected power iteration.
https://arxiv.org/2005.07587, 2020.
Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. J.
Mach. Learn. Res., 14(4), 2013.
Ron Zass and Amnon Shashua. Nonnegative sparse PCA. In Conf. Neur. Inf. Proc. Sys. (NeurIPS),
pp. 1561-1568, 2007.
Huishuai Zhang, Yi Zhou, Yingbin Liang, and Yuejie Chi. A nonconvex approach for phase retrieval:
Reshaped Wirtinger flow and incremental algorithms. J. Mach. Learn. Res., 18, 2017.
Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. J. Comput.
Graph. Stat., 15(2):265-286, 2006.
13
Published as a conference paper at ICLR 2022
Appendix (Generative Principal Component Analysis,
Liu/Liu/Ghosh/Han/Scarlett, ICLR 2022)
A PROOF OF LEMMA 1 (NON-DECREASING PROPERTY OF Q)
Since w(t+1) = PG (VW㈤)and W⑴ ∈ Range(G), we have
kVw(t) - w(t+1)k2 ≤ kVw(t) -w(t)k2,	(15)
and since kW(t+1) k2 = kW(t) k2 = 1, expanding the square gives
DVW(t), W(t+1)E ≥ Q(W(t)).	(16)
Then, we obtain
Q(W(t+1)) = DVW(t+1), W(t+1)E	(17)
= Q(W(t+1) - W(t)) + 2 DV(W(t+1) - W(t)), W(t)E + Q(W(t))	(18)
≥ 2 DV(W(t+1) - W(t)), W(t)E + Q(W(t))	(19)
≥ Q(W(t)),	(20)
where (18) follows by writing W(t+1) = W(t) + (W(t+1) - W(t)) and expanding, (19) follows from
the assumption that V is PSD, and (20) follows from (16).
B Proofs for S piked Matrix and Phase Retrieval Examples
Before proceeding, we present the following standard definitions.
Definition 1. A random variable X is said to be sub-Gaussian if there exists a positive constant C
such that (E [|X |p])1/p ≤ C √p for all P ≥ 1. The Sub-Gaussian norm of a Sub-Gaussian random
variable X is defined as kX kψ2 := supp≥1 p-1/2 (E [|X |p])1/p.
Definition 2. A random variable X is said to be sub-exponential if there exists a positive constant C
1
such that (E[∣X∣p])p ≤ CP for allP ≥ 1. The sub-exponential norm of X is defined as ∣∣X∣∣ψ1 :=
suPp≥ι PT(E [|X|p]) 1.
The following lemma states that the product of two sub-Gaussian random variables is sub-
exponential, regardless of the dependence between them.
Lemma 3. (Vershynin, 2018, Lemma 2.7.7) Let X and Y be sub-Gaussian random variables (not
necessarily independent). Then XY is sub-exponential, and satisfies
∣XY∣ψ1≤∣X∣ψ2∣Y∣ψ2.
(21)
The following lemma provides a useful concentration inequality for the sum of independent sub-
exponential random variables.
Lemma 4. (Vershynin, 2010, Proposition 5.16) Let X1, . . . , XN be independent zero-mean sub-
exponential random variables, and K = maxi ∣Xi ∣ψ1. Then for every α = [α1, . . . , αN]T ∈ RN
and ≥ 0, it holds that
2

≤ 2exP	-c ∙ min 7211	112 , 7∣∣ II
K2∣α∣22 K ∣α∣∞
(22)
where c > 0 is an absolute constant. In particular, with α = [ N,..., N]T, we have
(23)
14
Published as a conference paper at ICLR 2022
The widely-used notion of an -net is introduced as follows.
Definition 3. Let (X, d) be a metric space, and fix > 0. A subset S ⊆ X is said be an -net of X
if, for all x ∈ X, there exists some s ∈ S such that d(s, x) ≤ . The minimal cardinality of an -net
of X, if finite, is denoted C(X, ) and is called the covering number of X (at scale ).
The following lemma provides a useful upper bound for the covering number of the unit sphere.
Lemma 5. (Vershynin, 2010, Lemma 5.2) The unit Euclidean sphere SN-1 equipped with the
Euclidean metric satisfies for every > 0 that
C(SN-1,e) ≤ fl + ∣)	.	(24)
The following lemma provides an upper bound for the spectral norm of a symmetric matrix.
Lemma 6. (Vershynin, 2010, Lemma 5.4) Let X be a symmetric N × N matrix, and let C be an
-net of SN-1 for some ∈ [0, 1/2). Then,
kXk2→2 = sup |hXr, ri| ≤ (1 - 2)-1 sup |hXr, ri|.	(25)
r∈SN-1	r∈C
With the above auxiliary results in place, we provide the proofs of Assumption 2 holding for the two
examples described in Section 3.
B.1 Spiked Covariance Model (Example 1)
As per Assumption 2, fix two finitesignal sets Si and S2. For r = 1,we have Xi = √βuis + Zi and
a direct calculation gives E[V] = V = βssT. Recall also that ∣∣sk2 = 1, Ui 〜N(0,1), and Zi 〜
N(0, In). It follows that for any si ∈ Si, we have that STXi = √βuiSTsi + ZTsi is SUb-GaUSsian,
with the sub-Gaussian norm being upper bounded by C(√β +l)∣∣si∣∣2. Similarly, we have for any
s2 ∈ S2 that ∣∣sτXikψ2 ≤ C(√β + I)∣∣s2∣∣2. Applying Lemma 3, we deduce that (STXi)(sτXi) is
sub-exponential, with the sub-exponential norm being upper bounded by C2 (√β + I)2∣∣sik2∣∣s2k2.
In addition, from (9) and V = βssτ, we have
ST Es2 = ST (V — V)S2	(26)
m
=m X((XTSI)(XTS2)- ((STs2) + β(STSI)(STs2))) ,	(27)
m i=i
and we observe thatE[(XiTSi)(XiTS2)] = (SiTS2) +β(STSi)(STS2). Then, from Lemma 4, we obtain
that for any t > 0 satisfying m = Ω(t), the following holds with probability 1 — e-Q(t) (recall that
C may vary from line to line):
1 dp	E	「	EE -	LC	ʌ/t
m X((Xi SI)(XiS2) - ((SI S2) + β(s SI)(S Sz))) ≤ C(pβ + I) IlSIll2∣∣s2∣∣2 ∙ √m, (28)
i=i
where we note that the assumption m = Ω(t) ensures that the first term is dominant in the minimum
in (23). Taking a union bound over all Si ∈ Si and S2 ∈ S2, and setting t = log(∣Si∣ ∙ ∣S21), we
obtain with probability 1 — e-Q(log(|SlHS2|)) that (7) holds (with β being a fixed positive constant).
Next, we bound |rT Er| for fixed r ∈ Sn-i, but this time consider t > 0 (different from the above
t) satisfying t = Ω(m). In this case, we can follow the above analysis (with Si and s both replaced
by r), but the assumption t = Ω(m) means that when applying Lemma 4, the second term in the
minimum in (23) is now the dominant one. As a result, for any t > 0 satisfying t = Ω(m), and any
r ∈ Sn-i, we have with probability 1 — e-Q(t) that
m
IrTErI = — X((XTr)2 - (1+ β(STr)2))
m i=i
≤ C(Pe+1) ∙ m.
(29)
(30)
15
Published as a conference paper at ICLR 2022
From Lemma 5, there exists an (1/4)-net C1 of Sn-1 satisfying log ∣C 11 ≤ n log9. Taking a union
bound over all r ∈ C1, and setting t = Cn, We obtain with probability 1 - e-Q(n) that
sup ∣rτEr∣ = O (-n).	(31)
4
Then, from Lemma 6, we have
kEk2→2
≤ 2 sup ∣∣rT Er∣∣ = O
r∈C ι
(32)
B.2 Phase Retrieval (Example 2)
Let W = m1 Pm=I yiaiaT 1{i<yi<u} and W = βssT + γIn. It is shown in (Liu et al., 2021b,
Lemma 8) that
E [W] = W,	(33)
which implies
E[V] = β SST = V.	(34)
Then, for any s1 ∈ S1 and s2 ∈ S2, we have
STES2 = ST(V - V)S2 = ST(W - W)S2	(35)
1m
=m〉： ^yi(ai S1)(ai S2)1{l<yi<u}- (β(STSI)(STS2)+ Y(STS2))) .	(36)
m i=1
Since each ai has i.i.d. N(0, 1) entries, we observe that yi (aiTS1)(aiTS2)1{l<yi<u} is sub-
exponential with the sub-exponential norm being upper bounded by CukS1 k2 kS2 k2. In addition,
from (33), we have E[yi (aiTS1)(aiTS2)1{l<yi<u}] = β(STS1)(STS2) + γ(S1TS2). Then, from
Lemma 4, we obtain that for any t > 0 satisfying m = Ω(t), with probability 1 - e-Q(t),
≤ CukSIk2 ks2k2 ∙ -7=
∣1 m
m £(y，(aTSI)(aTS2)1{l<yi<u} - Ge(STSI)(STS2) + Y(STS2)))
∣ m i=1
(37)
Taking a union bound over all si ∈ Si and s ∈ S2, and setting t = log(∣Sι∣∙ ∣S2∣), we obtain that
with probability 1 - e-Q(log(|S1HS2|)), (7) holds as desired (with U being a fixed positive constant).
In addition, similarly to (32), we have with probability 1 - e-Q(n) that kEk 2→2 = o( m).
C Equivalence of Distances
The following lemma gives a useful equivalence between two distances.
Lemma 7. For any pair of unit vectors w1 , w2 with w1T w2 ≥ 0, we have
kw1 - w2k22 ≤ kw1w1T - w2w2Tk2F ≤ 2kw1 - w2k22.	(38)
Moreover, if w1T w2 < 0, then the same holds with kw1 - w2k2 replaced by kw1 + w2k2.
Proof. When w1T w2 ≥ 0, we have
T	T 2	T	TT	T	T
kw1w1T - w2w2Tk2F = tr((w1 w1T - w2w2T)T(w1w1T - w2w2T))	(39)
=2 (1 - (WTW2)2)	(40)
≥ 2(1 — wTw2)	(41)
= kW1 - W2k22,	(42)
where (40) follows by expanding the product and writing tr(w1w1Tw1w1T) = tr(w1Tw1w1Tw1) =
(w1T w1)2 = 1 and handling the other terms similarly, and (41) follows since w1T w2 ∈ (0, 1). In
16
Published as a conference paper at ICLR 2022
addition, We have
∣∣WiWT — W2WT IlF	=2 (1 — (WTw2)2)	(43)
	= 2 1 — WiT W2	1 + WiT W2	(44)
	≤ 4 (1 — WTW2)	(45)
	= 2∣Wi — W2 ∣22 ,	(46)
which gives the desired inequality. The case	wtW2 < 0 is handled similarly	□
D Proof of Theorem 1 (Guarantee on the Global Optimum)
Let the singular value decomposition (SVD) of V be
V = U D UT,
(47)
where D = Diag([λi,..., λn]), and U ∈ Rn is an orthonormal matrix with the first column being
X. For i > 1, let the i-th column of U be ui. Then, we have
VtVV = λi (XTV)2 + Xλi (UTV)2
i>i
≤ ʌi (XTv)2 + 入2 X (UTV)2
i>i
=ʌi (XTV)2 + λ2 (1 — (XTV)2)
where we use (xtV)2 + Pi>i (UTV) = 1 in (50).
In addition, for any A ∈ Rn×n and any si, s2 ∈ Rn, we have
(48)
(49)
(50)
ST Asi — ST As2
Si + S2 Si - S2 )T a (Si + S2 Si - S2
―2 — + —2 —)	(—2 — + —2—
—
Si + S2 Si — S2
2
—
T
A
Si + S2 Si — S2
2
T A (宁
2
—
2
T A (P
(51)
2 y
+2宁
(52)
In particular, when A is symmetric, we obtain
ST Asi — ST As2 =(S1 + S2 )T A(S1 — S2).
(53)
Let M be a (δ∕L)-net of Bk(r); from (Vershynin, 2010, Lemma 5.2), we know that there exists
such a net with
一 ，… … 4Lr
log |M| ≤ k log —.
(54)
Since G is L-Lipschitz continuous, we have that G(M) is a δ-net of Range(G) = G(Bk(r)). We
Write
ʌ / ʌ ~ ∖ , ~
v = (v - x) + x,
(55)
where X ∈ G(M) satisfies ∣∣V — X∣∣2 ≤ δ. Suppose that X V ≥ 0, if this is not the case, ^^e can use
analogous steps to obtain an upper bound for ∣∣X + V∣∣2 instead of ∣∣X — V∣2. We have
λi - λ2	m
—2——I∣x — VII2
=(ʌi — λ2) (1 — XTV)
≤ (ʌi — λ2) (1 — (xtV)2)
=ʌi — (ʌi (xTV)2 + λ2 (1 — (xtV)2))
(56)
(57)
(58)
(59)
17
Published as a conference paper at ICLR 2022
=XTVx -(X1 (XTv)2 + λ (1 - (Xtv)2))	(60)
≤ Xt VX - vτ VV	(61)
=XGVXG + (X + XG)TV(X - XG)- VτVV	(62)
≤ XGVXG + 2λ1 ∣∣X - xg∣∣2 - vτVV	(63)
=XG(V - E)xg + 2X1IlX - XG∣∣2 - VT(V - E)V	(64)
≤ vτEV - XGEXG + 2χι∣X - xg∣2	(65)
=XτEX + 2 (冷)T E (?)+ 2 (?)T E (*) -XGEXG + 2λι∣X -XG∣2
(66)
≤ XτEX + 2δ∣E∣2→2 - XGEXG + 2λι∣X - xg∣2	(67)
=2 (x-+2xg)T e (X-XG) + 2 (X-XG)T E (x+xg) + 2δ∣E∣2→2 + 2λι∣X - xg∣2
(68)
I k log 4Lr ..	..	....	—..	..
≤ 2C∖ —邑工∙ ∣X - xg∣2 + 2δ∣E∣2→2 + 2%∣∣X - xg∣2	(69)
V m
k k log 4⅛r^ …	..	..	..	..	..・	....	—..
≤ 2C∖ ---- ∙ (I∣x - V∣∣2 + ∣∣v - x∣2 + ∣∣x - XG∣∣2) + 2δ∣E∣2→2 +2λι∣X - xg∣2 (70)
m
≤ 2C∖ -g δ ∙∣V - X∣2 + O (也)+ O((χι + En)IlX - xgk2),	(71)
mm
where:
•	(57)-(58) follow from ∣∣X∣∣2 = ∣∣V∣∣2 = 1 and hence ∣XτV| ≤ 1;
•	(60) follows since (λι, x) are an eigenvalue-eigenvector Pair for V with ∣∣x∣2 = 1；
•	(61) follows from (50);
•	(62) follows from (53) with V being symmetric and setting si = X, s2 = xg；
•	(63) follows from ∣∣X + xg∣2 ≤ 2 and ∣V∣2→2 = Xi；
•	(64) follows since V = V — E;
•	(65) follows since V is a globally optimal solution to (5) and XG ∈ Range(G);
•	(66) follows from (52) with si = V and s2 = X;
•	(67) follows from (55) along with ∣∣V - X∣2 ≤ δ and ∣∣V + X∣2 ≤ 2;
•	(68) follows from (52);
•	(69) follows from Assumption 2 (with Si = S2 being G(M) shifted by xg) and (54);
•	(70) follows from the triangle inequality;
•	(71) follows by substituting ∣∣V - Xk2 ≤ δ, along with the assumptions ∣∣E∣∣2→2 =
O(n∕m), m = Ω(klog 手),and En = O( Jk%丁 ).
From (71), we have the following when VτX ≥ 0:
∣V-x∣2 = ⅛5P + O (二)+ Og + ；"-XGk2) .(72)
18
Published as a conference paper at ICLR 2022
As mentioned earlier, if V T X < 0, We have the same upper bound as in (72) for ∣∣v + x∣∣2. Therefore,
we obtain
(73)
(74)
(75)
IlvVT - XxTkF = ,2 (1 -(XTV)2)
=,2(1 — XTV) (1 + XTV)
≤ √2min{∣v — X∣2, ∣V + X∣2}
,ri+ ](λ1 + €n )kX — XG k2
+ O (V -Xr-G—
, (76)
where (73) follows from (40), (75) follows from ∣∣V ± X∣2 = 2(1 ± XT V), and (76) follows from
(72).
E Proof of Lemma 2 (Auxiliary Result for PPower Analysis)
By the assumption Range(G) ⊆ Sn-1 , for any X ∈ Rn and a > 0, we have
PG (X) = PG (aX),	(77)
which is seen by noting that when comparing ∣∣x — a12 with ∣∣x — b∣2 (in accordance with projection
mapping to the closest point), as long as ∣a∣2 = ∣b∣2, the comparison reduces to comparing hX, ai
with hX, bi, so is invariant to positive scaling ofX.
Let η = 1∕χι > 0 and S = Pg(Vs). Then, we have S = PG(Vs) = Pg(xVs). Since X ∈
Range(G), we have
kηvs — s∣2 ≤ kηvs — χ∣∣2.	(78)
This is equivalent to
k(ηVs — χ) + (χ — s)k2 ≤ llηVs — χk2,	(79)
and expanding the square gives
l∣X - S∣2 ≤ 2hηVs — X,S — X).	(80)
Note also that from VXXX =λX1 XX, we obtain XX = ηXVX XX, which we will use throughout the proof.
For δ > 0, let M be a (δ∕L)-net of B2k(r); from Lemma 5, there exists such a net with
log |M| ≤ k log 4Lr.	(81)
δ
By the L-Lipschitz continuity of G, we have that G(M) is a δ-net of Range(G) = G(B2k(r)). We
write
S = (s — S0) + S0, S = (S — S) +S,	(82)
where S ∈ G(M) satisfies ∣∣S — S∣2 ≤ δ, and So ∈ G(M) satisfies ∣∣s — So∣2 ≤ δ. Then, we have
hηVS — χ, s — χ)= hη^V(s — χ), s — Xi + hηEs, S — χ),	(83)
which follows from V = VX + E and XX = ηXVX XX. In the following, we control the two terms in (83)
separately.
Λ EI .	/ --⅛^^Γ/	— ∖ ʌ — ∖ -J-VT 1	—	. /ɔ ɪ	1 ʌ	ʌ —	. A	1	Ilill
1. The term hηV(s — χ), S — χ): We decompose S = αχ + βt and S = αχ + βt, where ∣∣t∣2 =
|闺|2 = 1 and tTX = tTX = 0. Since ∣∣s∣2 = ∣∣S∣2 = 1, we have α2 + β2 = α2 + β2 = 1.
In addition, we have α = stX and α = STX. Recall that in (47), we write the SVD of V
as VX = UX DXUXT. Since tTXX = 0, we can write t as t = Pi>1 hiuXi . In addition, since
19
Published as a conference paper at ICLR 2022
ktk2 = 1, We have P› h = 1. Hence, by the Cauchy-Schwarz inequality, We have
KV t, ti∣≤kV tk2
(84)
i>1
(85)
2
Therefore, we obtain
KnV(S - x), s - x)|
=
≤s
=12
f*h
i>1
(86)
λ2 Eh
i>1
(87)
(88)
∣h(α - 1)x + nβ V t, (α - 1)x + βt>∣
.. .. . ʌ. 一 ʌ..
= ∣(α-1)(α-1)+ nββ(v t,t)|
≤ (1 - α)(1 - a) + n∣ββ∣λ2
=(1 — α)(1 - a) + γ√ 1 — α2√1 - α2,
where (89) uses ηVX = x, and (90) uses kx∣∣2 = 1 and〈x, t)= 0.
2. The term(nEs, S - x): We have
K nEs, s - x)∣ = (nE((S - S0) + S0), s - X
=(nE(s - S0), S - x) + (nEs0, (S - S) + (S - x))
(89)
(90)
(91)
(92)
(93)
(94)
≤ 圳Ek2-2δks - xk2 + 圳Ek2-2
m
¥ ∙kS-xk2
(95)
≤ O (δ∣∣E∣∣2→2)
m
¥ ∙kS-xk2,
(96)
where (95) follows from Assumption 2 and (81), and (96) follows from ∙η = 1∕λ1, along
with the fact that We assumed λι = Θ(1).
Note that kx - sk2 = 2(1 - STX) = 2(1 - α). Hence, and using (80), (83), (92), and (96), we obtain
2(1 - α) ≤ 2 ((1 - α)(1 - α) + γ√1 — α2√1 - α2)
+ O (δ∣∣E∣∣2→2)
m
X	∙ √2I-^).
(97)
Using 2(1 - α) - 2(1 - α)(1 - α) = 2α(1 - α), √1 - α2 = √T-α√Γψα ≤ √2(1 - α), and
similarly √1 - α2 ≤ √2(1 - α), We obtain from (97) that
2α(1 - α) ≤ 2γ√2(1 - α) √2(1 - a)
k-l°g-^ ) ∙ √2(1 - a) + O (δ∣∣E∣∣2→2) . (98)
m
Since kS - xk2 = 2(1 - α) and ks - xk2 = 2(1 - α), this is equivalent to
αks - xk2 ≤	2Yks - Xk
k log L
m
S - Xk2 + O (δkEk2→2).	(99)
20
Published as a conference paper at ICLR 2022
This equation is of the form az2 ≤ bz + C (where Z = ∣∣s 一 X∣∣2 and a = α == STX > 0), and
using a simple application of the quadratic formula,9 we obtain
∣s 一 X∣2 ≤ 2γkSTxxk2 + O
≤ 2γ⅛=-⅛ + O
Sl X
k log L
m
k log(nLr)
m
+ JSTX ∙ δkE∣2-2
(100)
(101)
where we use the assumption ∣E∣2→2 = O(n/m) and set δ = 1/n in (101).
F Proof of Theorem 2 (Main Theorem for PPower)
Suppose for the time being that (13) holds for at least one index t ≥ t0 (we will later verify that this
is the case), and let T0 ≥ t0 be the smallest such index. Thus, we have

IlW(TO) - x∣∣2 ≤
C
(1 ― μo)ν
k log(nLr)
m
(102)
Note that according to the theorem statement, 1 一μ0 is bounded away from zero. Using ∣w(T0) ∣2 =
∣∣X∣2 = 1 and the assumption that m ≥ Cν,τ ∙ klog(nLr) with Cν,τ > 0 being large enough, We
deduce from (102) that ∣W(TO) — X∣2 is sufficiently small such that
XTW(TO) ≥ 1 — τ.
Next, using the assumption 27 + V ≤ 1 — T, we write
2γ,	1	= 27 + (1 - μo)ν ≤	27 + V ≤	1
(1 — μo)ν(1 — τ)	1 — τ	(1 — μo)ν(1 — T) - (1 — μo)ν(1 — T) - (1 — μo)ν
Then, from Lemma 2, we obtain
(103)
(104)
∣W(TO+1) — X7 ∣2 ≤
27
Xt W(TO)
• kw(To) - Xk2 + C	Jo lθg(nLrI
kw	Xk2 + Xt W(TO)V	m
27
1 — τ
C
(1 — μo )ν
k log(nLr)
m
k log(nLr)
m
C
(1 — μo)ν
k log(nLr)
m
(105)
(106)
(107)
≤
≤


+
C
1 — τ

where (106) follows from (102)-(103), and (107) follows from (104). Thus, we have transferred
(102) from T0 to T0 + 1, and proceeding by induction, we obtain
kW(t) - Xk2 ≤	B■
(108)
for al t ≥ T0 .
Next, we consider t ∈ [t0, T0). Again using Lemma 2 (with S = W(tO+1) = PG(VW(tO))), we have
l|W(tO+1) — X∣2 ≤ 〃okW(tO)— X∣2 + -C- ∙ Jklog(nLr),	(109)
2γ7 + V	m
where we recall that μo = XTWtO) = ^+ < 1, and note that the denominator in the second term
of (109) follows since X7T W(tO ) = 2γ7 + V. Supposing that t0 < T0 (otherwise, the above analysis
for t ≥ T0 alone is sufficient), we have that (13) is reversed at t = t0 :
∣W(tO)-x∣2 >-ɪ- r k⅛nLr).	(110)
(1 — μo)ν V m
9Since the leading coefficient a = α of the quadratic is positive, z must lie in between the two associated
roots. This yields Z ≤ b+"：：+4。。, from which the inequality √a + b ≤ √ + √ gives (100).
21
Published as a conference paper at ICLR 2022
This means that We can upper bound the second term in (109) by V ∙ J k log^Lr) < (1 - μo) k w(t0) -
Xk2, which gives
kw(t0 + 1) - xk2 < kw(t0) - xk2.
Squaring both sides, expanding, and canceling the norms (which all equal one), we obtain
XTw(t0+1) > XTw(t0),
and by induction, We obtain that {XTw(t)}t∈[to,To) is monotonically increasing.
(111)
(112)
Recall that we assume that λ1 = Θ(1), and that T0 = to + ∆o is the smallest integer such that (13)
holds. To verify that T0 is finite and upper bound ∆0, We consider the folloWing three cases:
•	μo = 0 (or equivalently, Y = ∖2 = 0): In this case, (109) gives To = to + 1 (or T0 = to, which
We already addressed above). Thus, We have ∆o ≤ 1, as stated in the theorem.
•	μo = o(1) (or equivalently, Y = o⑴ and λ2 = o(1)): Since {xtw(t)}t∈[to,To) is monotoni-
cally increasing, for any positive integer ∆ with to + △ ≤ T0, by applying Lemma 2 (or (109))
multiple times, we obtain10
kw(t02)- xk2 ≤ μ∆kw(t0) - Xk2 + 1-^ ∙ FC- ∙ Jklθg(nLr)
1 — μo 2γ + V V m
≤ μ∆kw(to) - X k2 +	1	∙1∙《k°^.
1 — μo 2γ + v V m
Then, if we choose △o ∈ N such that
C
≤ —
- 2v
k log(nLr)
m
we obtain from (114) that
|廿0心) - Xk2 ≤ μ}0 kw(t0) - Xk2 + ɪɪ ∙ 2γC+7 ∙ √kj0gmLr2
≤ 2μ∆0 + ɪ ∙ 2⅛ ∙ r^Lr
Cμo
V ν(1 - μo)
k log(nLr)
m
+ 1 - μo 2γ + V
k log(nLr)
m
C
(1 - μo )ν
k log(nLr)
m
(113)
(114)
(115)
(116)
(117)
(118)
(119)


1
C
where (117) follows from ∣∣w(t0) - X∣∣2 ≤ 2, (118) follows from (115) and 1 < ɪɪ^,
and (119) follows from μo =春，which implies μ0 + 2γ+ν = "0V22Y+V+"=言》=1.
Observe that (119) coincides with (13), and since μo = o(1), we obtain from (115) that
△o=ο(ι°g( k logmnLr)))as desired.
•	μo = Θ(1) (or equivalently, γ = Θ(1) and Y2 = Θ(1)): Recall that we only need to focus on
the case To > t0. This means that (110) holds, implying that we can upper bound the second
term in (109) by (；μo)ν ∙ ∣∣w(t0) - X∣∣2, yielding
l∣w(t0+1) - x∣2 < μo∣w(t0) - x∣2 + (1- μo)ν ∙ ∣w(t0) - x∣2	(120)
2γY + V
= 2γ+(1 [ μo)ν ∙kw(t0) - X∣2	(121)
2γY + V
= (1 - ξ)∣w(t0) -XY∣2,	(122)
10In simpler notation, if zt+1 ≤ azt +b, then we get zt+2 ≤ a2zt+(1+a)b, then zt+3 ≤ a3zt+(1+a+a2)b,
and so on, and then we can apply 1 + a + ... + ai-1 = 1-ai for a = 1.
22
Published as a conference paper at ICLR 2022
where ξ = 2+ = μo(1 - μo) = Θ(1). With the distance to X shrinking by a constant factor
in each iteration according to (122), and the initial distance ∣∣w(t0) - X∣∣2 being at most 2 due
to the vectors having unit norm, We deduce that ∆0 = O (log (kɪogmLry)) iterations suffice
to ensure that (13) holds for t = t0 + ∆0 .
G	Comparison of Analysis to Deshpande et al. (2014)
As mentioned in Section 4, our analysis is significantly different from that of Deshpande et al. (2014)
despite using a similar assumption on the initialization. We highlight the differences as follows:
1.	Perhaps the most significant difference is that the proof of (Deshpande et al., 2014, Theorem 3)
is highly dependent on the Moreau decomposition, which is only valid for a closed convex
cone (see (Deshpande et al., 2014, Definition 1.2)). In particular, the Moreau decomposition
needs to be used at the beginning of the proof of (Deshpande et al., 2014, Theorem 3), such as
Eqs. (18) and (19) in the supplementary material therein. We do not see a way for the proof to
proceed without the Moreau decomposition, and our Range(G) may be very different from a
convex cone.
2.	We highlight that one key observation in our proof of Lemma 2 (and thus Theorem 2) is
that for a generative model G with Range(G) ⊆ Sn-1, and any x ∈ Rn and a > 0, we have
PG (ax) = PG (x) (Eq. (77)). This enables US to derive the important equation S = PG(Vs)=
Pg(ηVs). We are not aware of a similar idea being used in the proof of (Deshpande et al.,
2014, Theorem 3).
3.	In the PPower method in (Deshpande et al., 2014), the authors need to add ρIn with ρ > 0
to the observed data matrix V to improve the convergence. In particular, they mention in the
paragraph before the statement of Theorem 3 that “the memory term ρvt is necessary for our
proof technique to go through”. In contrast, our proof of Theorem 2 does not require adding
such terms, even when our data model is restricted to the spiked Wigner model considered
in (Deshpande et al., 2014).
4.	We consider a matrix model that is significantly more general than the spiked Wigner model
studied in (Deshpande et al., 2014).
H Numerical Results for the Fashion-MNIST Dataset
The Fashion-MNIST dataset consists of Zalando’s article images with a training set of 60, 000 ex-
amples and a test set of 10, 000 examples. The size of each image in the Fashion-MNIST dataset is
also 28 × 28, and thus n = 784.
The generative model G is set to be a boundary-seeking generative adversarial network (BEGAN).
The BEGAN architecture is summarized as follows.11 The generator has latent dimension k = 62
and four layers. The first two are fully connected layers with the architecture 62 - 1024 - 6272,
and with ReLU activation functions. The output of the second layer, reshaped to 128 × 7 × 7,
is forwarded to a deconvolution layer with kernel size 4 and stride 2. The third layer uses ReLU
activations and has output size 64 × 14 × 14, where 64 is the number of channels. The fourth layer
is a deconvolution layer with kernel size 4 and strides 2, and it uses ReLU activations and has output
size 1 × 28 × 28, where the number of channels is 1.
The BEGAN is trained with a mini-batch size of 256, a learning rate of 0.0002, and 100 epochs. The
other parameters are the same as those for the MNIST dataset. We perform two sets of experiments,
considering the spiked covariance and phase retrieval models separately. The corresponding results
are reported in Figures 4, 5, 6, and 7. From these figures, again, we can observe clear superiority
of PPower to Power and TPower. We note that for the Fashion-MNIST dataset, some of the
images are not sparse in the natural basis, but we observe from Figures 4 and 6 that even for the
sparsest images (sandals), PPower also significantly outperforms TPower.
11Further details of the architecture can be found at https://github.com/hwalsuklee/
tensorflow-generative-model-collections.
23
Published as a conference paper at ICLR 2022
1
(a) β = 1 and m = 200


(b) β = 2 and m = 100
Figure 4: Examples of reconstructed Fashion-MNIST images for the spiked covariance model.
(a) Fixing β = 1 and varying m
Figure 5: Quantitative comparisons of the performance of Power, TPower and PPower according
to the Cosine Similarity for the Fashion-MNIST dataset and the spiked covariance model.
0.8
⅛0∙6
I
in
g 0.4
8
0.2
0.0
0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
β
(b) Fixing m = 300 and varying β
Figure 6: Examples of reconstructed images of the Fashion-MNIST dataset for phase retrieval.
Figure 7: Quantitative comparisons of the performance of Power, TPower and PPower according
to the Cosine Similarity for the Fashion-MNIST dataset and the phase retrieval model.
24
Published as a conference paper at ICLR 2022
JBMOd JBMod,-MJBMOd,-
lβn
0.8
0.6
0.4
0.2
0.0
A4jLlro=IU∞。 u5i0□
2000	4000	6000	8000	10000
m
(a) Fixing β = 1 and varying m
Figure 9: Quantitative comparisons of the performance of Power, TPower, TPowerW
and PPower according to the Cosine Similarity for the CelebA dataset and spiked covariance model.
Figure 8: Examples of reconstructed CelebA images for the spiked covariance model.
0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
β
(b) Fixing m = 3000 and varying β
I Numerical Results for the CelebA Dataset
The CelebA dataset consists of more than 200, 000 face images of celebrities, where each input
image is cropped to a 64 × 64 RGB image with n = 64 × 64 × 3 = 12288. The generative model
G is set to be a pre-trained Deep Convolutional Generative Adversarial Networks (DCGAN) model
with latent dimension k = 100. We use the DCGAN model trained by the authors of (Bora et al.,
2017) directly. We select the best estimate among 2 random restarts. The Adam optimizer with 100
steps and a learning rate of 0.1 is used for the projection operator.
For the images of the CelebA dataset, the corresponding vectors are clearly not sparse in the natural
basis. To make a fairer comparison to the sparsity-based method TPower, we convert the original
images to the wavelet basis, and perform TPower on these converted images. The obtained results
of TPower are then converted back to the vectors in the natural basis. The corresponding method
is denoted by TPowerW, with “W” referring to the conversion to images in the wavelet basis. In
each iteration of TPower and TPowerW, the calculated entries are truncated to zero except for
the largest q entries. For CelebA, q is set to be 2000. Other parameters are the same as those for
the MNIST dataset. We also perform two sets of experiments, considering the spiked covariance
and phase retrieval models separately. The corresponding results are reported in Figures 8 and 9.
From these figures, we can observe the superiority of PPower to Power, TPower and TPowerW,
whereas TPowerW only marginally improves over TPower.
25