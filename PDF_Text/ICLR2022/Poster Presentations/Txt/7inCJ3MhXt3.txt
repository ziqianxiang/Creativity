Published as a conference paper at ICLR 2022
Learning	Neural	Contextual	Bandits
through Perturbed Rewards
Yiling Jia1, Weitong Zhang2, Dongruo Zhou2, Quanquan Gu2, Hongning Wang1
1 Department of Computer Science, University of Virginia
2Department of Computer Science, University of California, Los Angeles.
yj9xs@virginia.edu, wt.zhang@ucla.edu, drzhou@cs.ucla.edu,
qgu@cs.ucla.edu, hw5x@virginia.edu
Ab stract
Thanks to the power of representation learning, neural contextual bandit algo-
rithms demonstrate remarkable performance improvement against their classical
counterparts. But because their exploration has to be performed in the entire neu-
ral network parameter space to obtain nearly optimal regret, the resulting com-
putational cost is prohibitively high. We perturb the rewards when updating the
neural network to eliminate the need of explicit exploration and the correspond-
ing computational overhead. We prove that a
O (d√T) regret upper bound is still
achievable under standard regularity conditions, where T is the number of rounds
of interactions and d is the effective dimension of a neural tangent kernel matrix.
Extensive comparisons with several benchmark contextual bandit algorithms, in-
cluding two recent neural contextual bandit models, demonstrate the effectiveness
and computational efficiency of our proposed neural bandit algorithm.
1	Introduction
Contextual bandit is a well-formulated abstraction of many important real-world problems, includ-
ing content recommendation (Li et al., 2010; Wu et al., 2016), online advertising (Schwartz et al.,
2017; Nuara et al., 2018), and mobile health (Lei et al., 2017; Tewari & Murphy, 2017). In such
problems, an agent iteratively interacts with an environment to maximize its accumulated rewards
over time. Its essence is sequential decision-making under uncertainty. Because the reward from
the environment for a chosen action (also referred to as an arm in literature) under each context is
stochastic, a no-regret learning algorithm needs to explore the problem space for improved reward
estimation, i.e., learning the mapping from an arm and its context1 to the expected reward.
Linear contextual bandit algorithms (Abbasi-Yadkori et al., 2011; Li et al., 2010), which assume
the reward mapping is a linear function of the context vector, dominate the community’s attention
in the study of contextual bandits. Though theoretically sound and practically effective, their lin-
ear reward mapping assumption is incompetent to capture possible complex non-linear relations
between the context vector and reward. This motivated the extended studies in parametric bandits,
such as generalized linear bandits (Filippi et al., 2010; Faury et al., 2020) and kernelized bandits
(Chowdhury & Gopalan, 2017; Krause & Ong, 2011). Recently, to unleash the power of repre-
sentation learning, deep neural networks (DNN) have also been introduced to learn the underlying
reward mapping directly. In (Zahavy & Mannor, 2019; Riquelme et al., 2018; Xu et al., 2020), a
deep neural network is applied to provide a feature mapping, and exploration is performed at the
last layer. NeuralUCB (Zhou et al., 2020) and NeuralTS (ZHANG et al., 2020) explore the entire
neural network parameter space to obtain nearly optimal regret using the neural tangent kernel tech-
nique (Jacot et al., 2018). These neural contextual bandit algorithms significantly boosted empirical
performance compared to their classical counterparts.
Nevertheless, a major practical concern of existing neural contextual bandit algorithms is their added
computational cost when performing exploration. Take the recently developed NeuralUCB and Neu-
ralTS for example. Their construction of the high-probability confidence set for model exploration
1 When no ambiguity is invoked, we refer to the feature vector for an arm and its context as a context vector.
1
Published as a conference paper at ICLR 2022
depends on the dimensionality of the network parameters and the learned context vectors’ represen-
tations, which is often very large for DNNs. For instance, a performing neural bandit solution often
has the number of parameters in the order of 100 thousands (if not less). It is prohibitively expen-
sive to compute inverse of the induced covariance matrix on such a huge number of parameters, as
required by their construction of confidence set. As a result, approximations, e.g., only using the
diagonal of the covariance matrix (Zhou et al., 2020; ZHANG et al., 2020), are employed to make
such algorithms operational in practice. But there is no theoretical guarantee for such diagonal ap-
proximations, which directly leads to the gap between the theoretical and empirical performance of
the neural bandit algorithms.
In this work, to alleviate the computational overhead caused by the expensive exploration, we pro-
pose to eliminate explicit model exploration by learning a neural bandit model with perturbed re-
wards. At each round of model update, we inject pseudo noise generated from a zero-mean Gaussian
distribution to the observed reward history. With the induced randomization, sufficient exploration
is achieved when simply pulling the arm with the highest estimated reward. This brings in consider-
able advantage over existing neural bandit algorithms: no additional computational cost is needed to
obtain no regret. We rigorously prove that with a high probability the algorithm obtains a O(d√T)
regret, where d is the effective dimension of a neural tangent kernel matrix and T is the number of
rounds of interactions. This result recovers existing regret bounds for the linear setting where the
effective dimension equals to the input feature dimension. Besides, our extensive empirical evalua-
tions demonstrate the strong advantage in efficiency and effectiveness of our solution against a rich
set of state-of-the-art contextual bandit solutions over both synthetic and real-world datasets.
2	Related Work
Most recently, attempts have been made to incorporate DNNs with contextual bandit algorithms.
Several existing work study neural-linear bandits (Riquelme et al., 2018; Zahavy & Mannor, 2019),
where exploration is performed on the last layer of the DNN. Under the neural tangent kernel
(NTK) framework (Jacot et al., 2018), NeuralUCB (Zhou et al., 2020) constructs confidence sets
with DNN-based random feature mappings to perform upper confidence bound based exploration.
NeuralTS (ZHANG et al., 2020) samples from the posterior distribution constructed with a simi-
lar technique. However, as the exploration is performed in the induced random feature space, the
added computational overhead is prohibitively high, which makes such solutions impractical. The
authors suggested diagonal approximations of the resulting covariance matrix, which however leave
the promised theoretical guarantees of those algorithms up in the air.
Reward perturbation based exploration has been studied in a number of classical bandit models
(Kveton et al., 2019a; 2020; 2019b). In a context-free k-armed bandit setting, Kveton et al. (2019b)
proposed to estimate each arm’s reward over a perturbed history and select the arm with the highest
estimated reward at each round. Such an arm selection strategy is proved to be optimistic with a
sufficiently high probability. Later this strategy has been extended to linear and generalized linear
bandits (Kveton et al., 2019a; 2020). In (Kveton et al., 2020), the authors suggested its application
to neural network models; but only some simple empirical evaluations were provided, without any
theoretical justifications. Our work for the first time provides a rigorous regret analysis of neural
contextual bandits with the perturbation-based exploration: a sublinear regret bound is still achiev-
able in terms of the number of interactions between the agent and environment.
3	Neural Bandit Learning with Perturbed Reward s
We study the problem of contextual bandit with finite K arms, where each arm is associated with a
d-dimensional context vector: xi ∈ Rd for i ∈ [K]. At each round t ∈ [T], the agent needs to select
one of the arms, denoted as at, and receives its reward rat,t, which is generated as rat,t = h(xat ) +
ηt. In particular, h(x) represents the unknown underlying reward mapping function satisfying 0 ≤
h(x) ≤ 1 for any x, and η is an R-SUb-GaUssian random variable that satisfies E[exp(μηt)] ≤
exp[μ2R2] for all μ ≥ 0. The goal is to minimize pseudo regret over T rounds:
RT = E [Xt=ι(ra* - rt,at) ,	3D
2
Published as a conference paper at ICLR 2022
Algorithm 1 Neural bandit with perturbed reward (NPR)
1:	Input: Number of rounds T, regularization coefficient λ, perturbation parameter V, network
width m, network depth L.
2:	Initialization: θ0 = (vec(W1), . . . , vec(WL)] ∈ Rp with Gaussian distribution: for 1 ≤ l ≤
L - 1, Wl = (W, 0; 0, W) with each entry of W sampled independently from N (0, 4/m);
WL = (w>, -w>) with each entry ofw sampled independently from N(0, 2/m).
3:	for t = 1, . . . , T do
4:	if t > K then
5:	Pull arm at and receive reward rt,at, where at = argmaxi∈[K] f(xi, θt-1).
6:	Generate {γS}s∈[t]~N(0, ν2).
7:	Set θt by the output of gradient descent for solving Eq (3.2).
8:	else
9:	Pull arm ak .
10:	end if
11:	end for
where a* is the optimal arm with the maximum expected reward.
To deal with the potential non-linearity of h(x) and unleash the representation learning power
of DNNs, we adopt a fully connected neural network f (x; θ) to approximate h(x): f(x; θ)
√mWlΦ(Wl-iΦ(…φ(Wιx))} where φ(x) = ReLU(x), θ = [vec(Wι),..., Vec(WL)] ∈
Rp with p = m + md + m2 (L - 1), and depth L ≥ 2. Each hidden layer is assumed to have the
same width (i.e., m) for convenience in later analysis; but this does not affect the conclusion of our
theoretical analysis.
Existing neural bandit solutions perform explicit exploration in the entire model space (Zhou et al.,
2020; ZHANG et al., 2020; Zahavy & Mannor, 2019; Riquelme et al., 2018; Xu et al., 2020), which
introduces prohibitive computational cost. And oftentimes the overhead is so high that approxima-
tion has to be employed (Zhou et al., 2020; ZHANG et al., 2020), which unfortunately breaks the
theoretical promise of these algorithms. In our proposed model, to eliminate such explicit model
exploration in neural bandit, a randomization strategy is introduced in the neural network update.
We name the resulting solution as Neural bandit with Perturbed Rewards, or NPR in short. In NPR,
at round t, the neural model is learned with the t rewards perturbed with designed perturbations:
mθin L(θ) = Xs=1"(xas ； θ) - Sa + Ys))2/2 + mλ∣∣θ - θo∣∣2∕2	(3.2)
where {γs}s=ι 〜N(0, σ2) are Gaussian random variables that are independently sampled in each
round t, and σ is a hyper-parameter that controls the strength of perturbation (and thus the explo-
ration) in NPR. We use an l2-regularized square loss for model estimation, where the regularization
centers at the randomly initialization θ0 with the trade-off parameter λ.
The detailed procedure of NPR is given in Algorithm 1. The algorithm starts by pulling all candidate
arms once. This guarantees that for any arm, NPR is sufficiently optimistic compared to the true
reward with respect to its approximation error (Lemma 4.4). When all the K arms have been pulled
once, the algorithm pulls the arm with the highest estimated reward, at = argmaxi f(xi; θt-1).
Once received the feedback rat ,t, the model perturbs the entire reward history so far via a freshly
sampled noise sequence {γst}ts=1, and updates the neural network by {(xas , ras,s + γst)}ts=1 using
gradient descent. In the regret analysis in Section 4, we prove that the variance from the added
{γst}s∈[t] will lead to the necessary optimism for exploration (Abbasi-Yadkori et al., 2011). We
adopt gradient descent for analysis convenience, while stochastic gradient descent can also be used
to solve the optimization problem with a similar theoretical guarantee based on recent works (Allen-
Zhu et al., 2019; Zou et al., 2019).
Compared with existing neural contextual bandit algorithms (Zhou et al., 2020; ZHANG et al., 2020;
Zahavy & Mannor, 2019; Riquelme et al., 2018; Xu et al., 2020), NPR does not need any added com-
putation for model exploration, besides the regular neural network update. This greatly alleviates the
overhead for the computation resources (both space and time) and makes NPR sufficiently general
to be applied for practical problems. More importantly, our theoretical analysis directly corresponds
to its actual behavior when applied, as no approximation is needed.
3
Published as a conference paper at ICLR 2022
4 Regret Analysis
In this section, we provide finite time regret analysis of NPR, where the time horizon T is set
beforehand. The theoretical analysis is built on the recent studies about the generalization of DNN
models (Cao & Gu, 2020; 2019; Chen et al., 2020; Daniely, 2017; Arora et al., 2019), which illustrate
that with (stochastic) gradient descent, the learned parameters of a DNN locate in a particular regime
with the generalization error being characterized by the best function in the corresponding neural
tangent kernel (NTK) space (Jacot et al., 2018). We leave the background of NTK in the appendix
and focus on the key steps of our proof in this section.
The analysis starts with the following lemma for the set of context vectors {xi}iK=1.
Lemma 4.1. Thereexists a positive constant C such that for any δ ∈ (0,1), with probability at least
1 一 δ, when m ≥ CK4L6 log(K2L∕δ)∕λ0, there exists a θ* ∈ Rp, for all i ∈ [K],
h(xi) = hg(xi; θ0), θ* - θ0i,	√m∣∣θ* - Θ0k2 ≤ √2h>H-1h,
(4.1)
where H is the NTK matrix defined on the context set and h = (h(x1), . . . , h(xK)). Details of H
can be found in the appendix.
This lemma suggests that with a satisfied neural network width m, with high probability, the under-
lying reward mapping function can be approximated by a linear function over g(xi; θ0), parameter-
ized by θ* 一 θo, where g(xi; θo) = Vθf (x; θo) ∈ Rd is the gradient of the initial neural network.
We also define the covariance matrix At at round t as,
At = Es=I g(xs,as ； θ0)g(Xs,as ； θ0)>∕m + λI
(4.2)
where λ > 0 is the l2 regularization parameter in Eq (3.2). Discussed before, there exist two kinds
of noises in the training samples: the observation noise and the perturbation noise. To analyze the
effect of each noise in model estimation, two auxiliary least square solutions are introduced, A-Ibt
and At-1bt with
bt = Xs=ICrs,as + E[γsτ])g(Xs,as ； θ0)∕√m,	bt = Xs=ICrs,as + YsT)g(Xs处；θ0)∕√m∙
These two auxiliary solutions isolate the induced deviations: the first least square solution only
contains the deviation caused by observation noise {ηs}ts-=11; and the second least square solution
has an extra deviation caused by {γst-1}ts-=11.
To analysis the estimation error of the neural network, we first define the following events. At round
t, we have event Et,1 defined as:
Et, 1 = n∀i ∈ [K], lhg(xt,i； θ0), A-1bt∕√mi - h(xt,i)∣ ≤ αt∣∣g(xt,i; θ0)∣∣A-1 }.
According to Lemma 4.1 and the definition of bt, under event Et,1, the deviation caused by the
observation noise {ηs}ts-=11 can be controlled by αt. We also need event Et,2 at round t defined as
Et,2 = {∀i ∈ [K] : |f(xt,i； θt-i) - hg(xt,i； θ0), A-1bt∕√m∖ ≤ e(m) + βtkg(x5 Θ0)∣∣A-1},
where (m) is defined as
e(m) =CeJmT/6T2∕3λ-2∕3L3VZlogm + Ce,2(1 一 ηmλ)jdTL∕λ
+ Ce,3m-1∕6T 5∕3λ-5∕3L4 Piogm (1 + pτ∕λ),
with constants {Ce,i}i3=1. Event Et,2 considers the situation that the deviation caused by the added
noise {γst-1}ts-=11 is under control with parameter βt at round t, and the approximation error (m) is
carefully tracked by properly setting the neural network width m.
The following lemmas show that given the history Ft-1 = σ{x1, r1, x2, r2, ..., xt-1, rt-1, xt},
which is σ-algebra generated by the previously pulled arms and the observed rewards, with proper
setting, events Et,1 and Et,2 happen with a high probability.
4
Published as a conference paper at ICLR 2022
Lemma 4.2. For any t ∈ [T], λ > 0 and δ > 0, with αt
√λS, we have P(Et,1∣Ft-1) ≥ 1 - δ.
RR log (det(At)∕(δ2 det(λI))) +
In particular, S denotes the upper bound of √2h>H-1h, which is a constant if the reward function
h(∙) belongs to the RKHS space induced by the neural tangent kernel.
Lemma 4.3. There exist positive constants {Ci}i3=1, such that with step size and the neural net-
work satisfy that η = Cι(mλ+mLT)-1, m ≥ C2√⅛L-3S[log(TKL2∕δ)]3S,and m[log m]-3 ≥
C3 max{TL12λ-1,T7 λ-8L18(λ + LT )6, L21T 7λ-7(1 + √T∕λ)6},given history Ft-1,by choos-
ing βt = σ√4logt + 2log K, we have P(Et,2∣Ft-1) ≥ 1 - t-2.
We leave the detailed proof in the appendix. According to Lemma 4.2 and Lemma 4.3, with more
observations become available, the neural network based reward estimator f(x; θt) will gradually
converge to the true expected reward h(x). In the meantime, the variance of the added perturba-
tion leads to reward overestimation, which will compensate the potential underestimation caused
by the observation noise and the approximation error. The following lemma describes this anti-
concentration behavior of the added noise {γst-1}ts-=11.
Lemma 4.4. For any t ∈ [T], with σ = at(1 - λλK1(Aκ))-1/2, at defined in Lemma 4.2,
(m) defined in Lemma 4.3, given history Ft-1, we have P Et,3) = P(f(xt,i; θt-1) ≥ h(xt,i) -
e(m)∣Ft-1,Et,1) ≥ (4e√π)-1, where AK is the covariance matrix constructed after the initial
K-round arm pulling (line 8 to 10 in Algorithm 1) by Eq (4.2), and λK(AK) represents the K-th
largest eigenvalue of matrix AK .
With at defined in Lemma 4.2, and (m) and βt defined in Lemma 4.3, we divide the arms into two
groups. More specifically, we define the set of sufficiently sampled arms in round t as:
Ωt = {∀i ∈ [K] : 2e(m) + (at + βt)∣∣g(x5 θ0)∣∣A-1 ≤ h(xt,a*) - h(xt,i)}∙
Accordingly, the set of undersampled arms is defined as Ωt = [K] \ Ωt. Note that by this definition,
the best arm a* belongs to Ωt. In the following lemma, we show that at any time step t > K, the
expected one-step regret is upper bounded in terms of the regret due to playing an undersampled
arm at ∈ Ωt.
Lemma 4.5. With m, η, σ satisfying the conditions in Lemma 4.2, 4.3, 4.4, and (m) defined in
Lemma 4.3, with probability at least 1 - δ the one step regret of NPR is upper bounded,
E[h(xt,a* ) - h(xt,at )] ≤ P(EtE + 4e(m) + (βt + αt) (1 + 2∕P(at ∈。t)) Ilg(Xt,at; θ0)/√mkA-1 ∙
Furthermore, P(at ∈ Ωt) ≥ Pt(Et,3) - Pt(Et,2).
To bound the cumulative regret of NPR, we also need the following lemma.
Lemma 4.6. With d as the effective dimension of the NTK matrix H, we have
XT=1 min k kg(xt,at ； θ0)∕√mkA二，1} ≤ 2(elog(1 + TK∕λ) + 1)
The effective dimension is first proposed in (Valko et al., 2013) to analyze the kernelized contextual
bandit, which roughly measures the number of directions in the kernel space where the data mostly
lies. The detailed definiton of d can be found in Definition B.3 in the appendix. Finally, with proper
neural network setup, we provide an m-independent regret upper bound.
Theorem 4.7. Let d be the effective dimension of the NTK kernel matrix H. There exist positive
constants C1 and Cr, such that for any δ ∈ (0, 1), when network structure and the step size for
model update satisfy m ≥ poly(T, L, K, λ-1, S-1, log(1∕δ)), η = C1(mTL + mλ)-1 and λ ≥
max{1, S-2}, with probability at least 1 - δ, the cumulative regret of NPR satisfies
Rt ≤ r(RJelog(1+ TK∕λ) + 2 - 2logδ + √λs) ∙ J2dTlog(1 + TK∕λ) + 2 + 7 + K
(4.3)
where Γ = 44e√π(1 + J(4log T + 2log K ”(1 — λλκ1(Aκ))).
5
Published as a conference paper at ICLR 2022
Proof. By Lemma 4.2, Et,1 holds for all t ∈ [T] with probability at least 1 - δ. Therefore, with
probability at least 1 - δ, we have
TT
RT = XE[(h(xtq) - h(xt,at))1{Et,ι}] ≤ X E[(h(xtq) - h(xt,aJ)1{Et,ι}] + K
t=1	t=K+1
T
≤4Te(m) + X Pt(Et,2) + (βt + αt)(1 +
t=1
2	、…	…LU
Pt(Et,3)- Pt(Et,2))kg(xt,at; θ0)∕√mkA-I + K
The second inequality is due to the analysis of one-step regret in Lemma 4.5. According to Lemma
4.3, by choosing δ = σct, with Ct = √4 logt + 2log K, Pt(Et,2) ≤ t-2. Hence, PT=I Pt(Et,2) ≤
π2∕6. BasedonLemma 4.4, with σ = αt(1 - λλK1(Aκ))-1/2, Pt(Et,3) ≥ 4e√∏. Therefore,
1 + 2/(Pt(Et,3) - Pt(Et,2)) ≤ 44e√π. By chaining all the inequalities, we have the cumulative
regret of NPR upper bounded as,
π2	LT	2
RT ≤y + 4Te(m) + ∑t=ι(βt + αt)(I + Pt(Et,3) - Pt(Et,2))kg(Xt,at; θ0)/√mkA-1 + K
≤Γ (Rqelog(1 + TK∕λ) +2 - 2log δ + √λs) ∙ q2>eτ log(1 + TK∕λ) + 2 + π2 + K
+ C3lm-1∕6T5∕3λ-2∕3L3plog m + Ce,2(1 - ηmλ)JTPTL∕λ
+ C33m-1∕6T 8∕3λ-5∕3L4Plog m (1 + pτ∕λ)
where Γ = 44e√π(1 + J(4log T + 2log K)∕(1 - λλK1(Aκ))). The second inequality is due
—	j
to Lemma 4.6, and the bounds of events Et,2 and Et,3. By setting η = c(mλ + mLT)-1, and
J = (1 + LT∕λ)(log(Ce,2)+logTpTL∕λ)∕c, Ce,2(1 -ηmλ)JTpTL∕λ ≤ 1.Thenby choosing
the satisfied m, Ce,1m-1/6T5∕3λ-2∕3L3√I⅛m + CeRm-V6T8∕3λ-5∕3L4√I0gm(1 + pT∕λ) ≤
2∕3. RT can be further bounded by RT ≤「(Rjdlog(1 + TK∕λ) + 2 - 2logδ + √λs) ∙
ET log(1+ TK∕λ) + 2 + 7 + K
This completes the proof.
□
5 Experiments
In this section, we empirically evaluate the proposed neural bandit algorithm NPR against several
state-of-the-art baselines, including: linear and neural UCB (LinUCB (Abbasi-Yadkori et al., 2011)
and NeuralUCB (Zhou et al., 2020)), linear and neural Thompson Sampling (LinTS (Agrawal &
Goyal, 2013) and NeuralTS (ZHANG et al., 2020)), perturbed reward for linear bandits (LinFPL)
(Kveton et al., 2019a). To demonstrate the real impact of using diagonal approximation in confi-
dence set construction, we also include NeuralUCB and NeuralTS with diagonal approximation as
suggested in their original papers for comparisons.
We evaluated all the algorithms on 1) synthetic dataset via simulation, 2) six K-class classification
datasets from UCI machine learning repository (Beygelzimer et al., 2011), and 3) two real-world
datasets extracted from the social bookmarking web service Delicious and music streaming service
LastFM (Wu et al., 2016). We implemented all the algorithms in PyTorch and performed all the
experiments on a server equipped with Intel Xeon Gold 6230 2.10GHz CPU, 128G RAM, four
NVIDIA GeForce RTX 2080Ti graphical cards.
5.1	Experiment on synthetic dataset
In our simulation, we first generate a size-K arm pool, in which each arm i is associated with a
d-dimensional feature vector xi . The contextual vectors are sampled uniformly at random from a
unit ball. We generate the rewards via the following two nonlinear functions:
h1(x) = 10-2(x>ΣΣ>x),	h2 (x) = exp(-10(x>θ)2) ,
6
Published as a conference paper at ICLR 2022
(a) h1(x) = 10-2(x>ΣΣ>x)
(b) h2(x) = exp(-10(x>θ)2)
N∖R	■ Hmefor Model Update 25∞-	.	' NeuraIUCB
20∞ ɔXlIjg Hme for Arm Selection	_ . . ` NeUrag
-O20∞ ■ ■ ■ ■ GS - NeuralUCB(Dlagonal)
K ∖∖× ∖∖s	K	S ： ■ Neurarre (Diagonal)
i:Il i I I I I II
lIL≡J ⅛LLLU iLjlLJ
NeuralUCB NeuraITS NeuraIUCB NeuraIUCB NPR	1* 2<> 50 1∞	【1616】 【3232] [6464] [12β 128]
NeuraIUCB NeuratTS
iIUCB
jonal)
Size of Ann Pool
Neura I Network Structure
(c)	Elapsed time
(d)	Effect of pool size
(e)	Effect of network complexity
Figure 1: Empirical results of regret and time consumption on synthetic dataset.
where each element of Σ ∈ Rd×d is randomly sampled from N(0, 1), and θ is randomly sampled
from a unit ball. At each round t, only a subset of k arms out of the total K arms are sampled without
replacement and disclosed to all algorithms for selection. The ground-truth reward ra is corrupted
by Gaussian noise η = N(0, ξ2) before feeding back to the bandit algorithms. We fixed the feature
dimension d = 50 and the pool size K = 100 with k = 20. ξ was set to 0.1 for both h1 and h2.
Cumulative regret is used to compare the performance of different algorithms. Here the best arm
is defined as the one with the highest expected reward in the presented k candidate arms. All the
algorithms were executed up to 10000 rounds in simulation, and the averaged results over 10 runs
are reported in Figure 1(a) to 1(e). For the neural bandit algorithms, we adopted a 3-layer neural
network with m = 64 units in each hidden layer. We did a grid search on the first 1000 rounds
for regularization parameter λ over {10-i}i4=0, and step size η over {10-i}i3=0. We also searched
for the concentration parameter δ so that the exploration parameter ν is equivalently searched over
{10-i}i4=0. The best set of hyper-parameters for each algorithm are applied for the full simulation
over 10000 rounds, i.e., only that trial will be continued for the rest of 9000 rounds.
In Figure 1(a) and 1(b), we can observe that linear bandit algorithms clearly failed to learn non-
linear mapping and suffered much higher regret compared to the neural algorithms. NPR achieved
similar performance as NeuralUCB and NeuralTS, which shows the effectiveness of our randomized
exploration strategy in neural bandit learning.
Figure 1(c) shows the average running time for all the neural bandit models in this experiment. With
the same network structure, the time spent for model update was similar across different neural
bandit models. But we can clearly observe that NeuralUCB and NeuralTS took much more time in
arm selection. When choosing an arm, NeuralUCB and NeuralTS, and their corresponding versions
with diagonal approximation, need to repeatedly compute: 1) the inverse of the covariance matrix
based on the pulled arms in history; and 2) the confidence set of their reward estimation on each
candidate arm. The size of the covariance matrix depends on the number of parameters in the neural
network (e.g., 7460-by-7460 in this experiment). With diagonal approximation, the computational
cost is significantly reduced. But, as mentioned before, there is no theoretical guarantee about the
approximation’s impact on the model’s performance. Intuitively, the impact depends on how the
full covariance matrix concentrates on its diagonal, e.g., whether the random feature mappings of
the network on the pulled arms are correlated among their dimensions. Unfortunately, this cannot
be determined in advance. Figure 1(a) and 1(b) show that the diagonal approximation leads to a
significant performance drop. This confirms our concern of such an approximation. In contrast, as
shown in Figure 1(c), there is no extra computation overhead in NPR; in the meantime, it provides
comparable performance to NeuralUCB and NeuralTS with a full covariance matrix.
7
Published as a conference paper at ICLR 2022
25« - T- LinFPL
→- LinTS
20« - -i— LinUCB
-→- NPR
i5W----»- NeuraFTS
-NeuraITS (Diagonal)
low----»- NeuraIUCB
:lwɪ
0	2000	4000	6000	8000	10000
Rounds
(a) Adult
(b) Mushroom
Figure 2: Comparison of NPR and the baselines on the UCI datasets.
(c) Shuttle
To further investigate the efficiency advantage of NPR, we reported the total running time for the
neural bandit algorithms under different sizes of the arm pool and different structures of the neural
network under the same simulation setting. With the same neural network structure, models with
different sizes of candidate arms should share the same running time for model update. Hence,
Figure 1(d) reports how the size of arm pool affects the time spent on arm selection. As we can find,
with more candidate arms, NeuralUCB, NeuralTS, and their corresponding diagonal approximations
spent more time on arm selection, or more specifically on constructing the confidence set of the
reward estimation for each arm. With a fixed size arm pool, the complexity of the neural network
will strongly affect the time for computing the inverse of the covariance matrix. In Figure 1(e), the
x-axis shows the structure for the hidden layers. The running time for NeuralUCB and NeuralTS
significantly increased with enlarged neural networks. The running time of NPR stayed stable,
across varying sizes of arm pools and the network complexity. This shows the strong advantage of
NPR in efficiency and effectiveness, especially for large-scale problems.
5.2	Experiment on classification datasets
Following the setting in (Zhou et al., 2020), we evaluated the bandit algorithms on K-class classifi-
cation problem with six public benchmark datasets, including adult, mushroom, and shuttle
from UCI Machine Learning Repository (Dua & Graff, 2017). Due to space limit, we report the
results on these three datasets in this section, and leave the other three, letter, covertype,
and MagicTelescope in the appendix. We adopted the disjoint model (Li et al., 2010) to
build the context vectors for candidate arms: given an input instance with feature x ∈ Rd of
a k-class classification problem, we constructed the context features for k candidate arms as:
x1 = (x, 0, . . . , 0), . . . , xk = (0, . . . , 0, x) ∈ Rd×k. The model receives reward 1 if it selects
the correct class by pulling the corresponding arm, otherwise 0. The cumulative regret measures the
total mistakes made by the model over T rounds. Similar to the experiment in synthetic datasets,
we adopted a 3-layer neural network with m = 64 units in each hidden layer. We used the same
grid search setting as in our experiments on synthetic datasets. Figure 2 shows the averaged results
across 10 runs for 10000 rounds, except for mushroom which only contains 8124 data instances.
There are several key observations in Figure 2. First, the improvement of applying a neural bandit
model depends on the nonlinearity of the dataset. For example, the performance on mushoom and
shuttle was highly boosted by the neural models, while the improvement on adult was limited.
Second, the impact of the diagonal approximation for NeuralUCB and NeuralTS also depends on
the dataset. It did not hurt their performance too much on shuttle, while using the full covariance
matrix led to much better performance on Mushroom. Overall, NPR showed consistently better or
at least comparable performance to the best neural bandit baselines in all six datasets. Such robust
performance and high efficiency make it more advantageous in practice.
5.3	Experiment on LastFM & Delicious
The LastFM dataset was extracted from the music streaming service website Last.fm, and the Deli-
cious data set was extracted from the social bookmark sharing service website Delicious (Wu et al.,
2016). In particular, the LastFM dataset contains 1,892 users and 17,632 items (artists). We treat the
“listened artists” in each user as positive feedback. The Delicious dataset contains 1,861 users and
69,226 items (URLs). We treat the bookmarked URLs for each user as positive feedback. Following
the standard setting (Cesa-Bianchi et al., 2013), we pre-processed these two datasets. For the item
8
Published as a conference paper at ICLR 2022
(a) LastFM dataset
Figure 3: Comparisons of normalized rewards on LastFM & Delicious datasets.
(b) Delicious dataset
features, we used all the tags associated with each item to create its TF-IDF feature vector. Then
PCA was applied to reduce the dimensionality of the features. In both datasets, we used the first 25
principle components to construct the item feature vectors. On the user side, we created user fea-
tures by running node2vec (Grover & Leskovec, 2016) on the user relation graph extracted from the
social network provided by the datasets. Each of the users was represented with a 50-dimensional
feature vector. We generated the candidate pool as follows: we fixed the size of candidate arm pool
to k = 25 for each round; for each user, we picked one item from those non-zero payoff items
according to the complete observations in the dataset, and randomly picked the other 24 from those
zero-payoff items. We constructed the context vectors by concatenating the user and item feature
vectors. In our experiment, a 3-layer neural network with m = 128 units in each hidden layer was
applied to learn the bandit model. And the same gird search was performed to find the best set of
hyper-parameters. We performed the experiment with 10000 rounds. Because of the high demand
of GPU memory for the matrix inverse computation, we only compared NPR to NeuralUCB and
NeuralTS with the diagonal approximation.
We compared the cumulative reward from each algorithm in Figure 3. Besides the neural contextual
bandit algorithms, we also include the best reported model on these two datasets, hLinUCB (Wang
et al., 2016) for comparison. To improve visibility, we normalized the cumulative reward by a
random strategy’s cumulative reward (Wu et al., 2016), where the arm was randomly sampled from
the candidate pool. All the neural models showed the power to learn the potential non-linear reward
mapping on these two real-world datasets. NPR showed its advantage over the baselines (especially
on LastFM). As NPR has no additional requirement on the computation resource (e.g., no need for
the expensive matrix operation as in the baselines), we also experienced much shorter running in
NPR than the other two neural bandit models.
6 Conclusion
Existing neural contextual bandit algorithms sacrifice computation efficiency for effective explo-
ration in the entire neural network parameter space. To eliminate the computational overhead caused
by such an expensive operation, we appealed to randomization-based exploration and obtained the
same level of learning outcome. No extra computational cost or resources are needed besides the
regular neural network update. We proved that the proposed solution obtains O(d√T) regret for T
rounds of interactions between the system and user. Extensive empirical results on both synthetic
and real-world datasets support our theoretical analysis, and demonstrate the strong advantage of
our model in efficiency and effectiveness, which suggest the potential of deploying powerful neural
bandit model online in practice.
Our empirical efficiency analysis shows that the key bottleneck of learning a neural model online is
at the model update step. Our current regret analysis depends on (stochastic) gradient descent over
the entire training set for model update in each round, which is prohibitively expensive. We would
like to investigate the possibility of more efficient model update, e.g., online stochastic gradient de-
scent or continual learning, and the corresponding effect on model convergence and regret analysis.
Currently, our analysis focuses on the K-armed setting, and it is important to extend our solution
and analysis to infinitely many arms in our future work.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank the anonymous reviewers for their helpful comments. YJ and HW are partially supported
by the National Science Foundation under grant IIS-2128019 and IIS-1553568. WZ, DZ and QG are
partially supported by the National Science Foundation CAREER Award 1906169 and IIS-1904183.
The views and conclusions contained in this paper are those of the authors and should not be inter-
preted as representing any funding agencies.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.
In International Conference on Machine Learning, pp. 127-135, 2013.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On ex-
act computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, 2019.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual
bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 19-26, 2011.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, 2019.
Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. In the Thirty-Fourth AAAI Conference on Artificial Intelli-
gence, 2020.
Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. A gang of bandits. arXiv preprint
arXiv:1306.0811, 2013.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suf-
ficient to learn deep relu networks? In International Conference on Learning Representations,
2020.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, pp. 844-853. PMLR, 2017.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422-2430, 2017.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Louis Faury, Marc Abeille, Clement Calauzenes, and Olivier Fercoq. Improved optimistic algo-
rithms for logistic bandits. In International Conference on Machine Learning, pp. 3052-3060.
PMLR, 2020.
Sarah Filippi, Olivier Cappe, AUreIien Garivier, and Csaba Szepesvari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, pp. 586-594,
2010.
10
Published as a conference paper at ICLR 2022
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances
in neural information processing systems, pp. 2447-2455, 2011.
Branislav Kveton, Csaba Szepesvari, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbed-
history exploration in stochastic linear bandits. In Proceedings of the 35th Conference on Uncer-
tainty in Artificial Intelligence (UAI), pp. 176, 2019a.
Branislav Kveton, Csaba Szepesvari, Sharan Vaswani, Zheng Wen, Tor Lattimore, and Mohammad
Ghavamzadeh. Garbage in, reward out: Bootstrapping exploration in multi-armed bandits. In
International Conference on Machine Learning, pp. 3601-3610. PMLR, 2019b.
Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and
Craig Boutilier. Randomized exploration in generalized linear bandits. In Proceedings of the
22nd International Conference on Artificial Intelligence and Statistics, 2020.
Huitian Lei, Ambuj Tewari, and Susan A Murphy. An actor-critic contextual bandit algorithm for
personalized mobile health interventions. arXiv preprint arXiv:1706.09090, 2017.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pp. 661-670. ACM, 2010.
Alessandro Nuara, Francesco Trovo, Nicola Gatti, and Marcello Restelli. A combinatorial-bandit
algorithm for the online joint bid/budget optimization of pay-per-click advertising campaigns. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown. In Interna-
tional Conference on Learning Representations, 2018.
Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising
using multi-armed bandit experiments. Marketing Science, 36(4):500-522, 2017.
Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health.
In Mobile Health, pp. 495-517. Springer, 2017.
Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
Huazheng Wang, Qingyun Wu, and Hongning Wang. Learning hidden features for contextual ban-
dits. In Proceedings of the 25th ACM International on Conference on Information and Knowledge
Management, pp. 1633-1642, 2016.
Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. Contextual bandits in a collabo-
rative environment. In Proceedings of the 39th International ACM SIGIR conference on Research
and Development in Information Retrieval, pp. 529-538, 2016.
Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep
representation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020.
Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.
Weitong ZHANG, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. In
International Conference on Learning Representations, 2020.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, pp. 11492-11502. PMLR, 2020.
11
Published as a conference paper at ICLR 2022
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. Machine Learning, 2019.
A Additional Experiments
A. 1 Experiments on Synthetic Datasets
A.1.1	Reward prediction with more complicated neural networks
In the main paper, to compare NPR with the existing neural bandit baselines, we chose a relatively
small neural network, e.g., m = 64, so that NeuralUCB/NeuralTS can be executed with a full co-
variance matrix. To better capture the non-linearity in reward generation, we increased the width of
the neural network to m = 128, and report the results in Figure 4. All other experiment settings
(e.g., underlying reward generation function) are the same as reported in the main paper. And to pro-
vide a more comprehensive picture, we also reported the variance of each algorithm’s performance
in this figure.
With the increased network width and limited by the GPU memory, NeuralUCB and NeuralTS can
only be executed with the diagonal matrix of the covariance matrix when computing their required
“exploration bonus term”. In Figure 4, we can observe 1) all neural bandit algorithms obtained the
expected sublinear regret; and 2) NPR performed better than NeuralUCB and NeuralTS, because of
the diagonal approximation that we have to use on these two baselines. This again strongly suggested
the advantage of NPR: the approximation due to the computational complexity in NeuralUCB and
NeuralTS limits their practical effectiveness, though they have the same theoretical regret bound as
NPR’s. 3) FALCON+ shows better performance in the initial rounds, while with more iterations, its
regret is higher than the neural bandit baselines’ and our NPR model’. Compared to the neural bandit
models, FALCON+ has much slower convergence, e.g, the slope of its regret curve did not decrease
as fast as other models’ over the course of interactions. After a careful investigation, we found
this is caused by its scheduled update: because less and less updates can be performed in the later
rounds, FALCON+ still selected the suboptimal arms quite often. Another important observation is
that FALCON+ has a very high variance in its performance: in some epochs, it could keep sampling
the suboptimal arms. This is again caused by its lazy update: in some epochs of some trials, its
estimation could be biased by large reward noise, which will directly cost it miss the right arm in
almost the entire next epoch.
(a) h1(x) = 10-2(x>ΣΣ>x)
UnUCB
□∏T5
□∏FPL
NPR
NeuraIUCBsDiag
NeuralTS-□iaς
R⅛LCON4-
NeuralTS-Diag
R⅛LCON+
(b) h2 (x) = exp(-10(x>θ)2)
T二三
Figure 4: Comparison of NPR and the baselines on the synthetic dataset with m = 128
A.1.2 COMPARISONS UNDER DIFFERENT VALUES OF k
For the experiment in the main paper, at each round only k = 20 arms are sampled from the K = 100
arm pool and disclosed to all the algorithms for selection. It is to test a more general setting in
practice, e.g., different recommendation candidates appear across rounds. In the meantime, such
setting also introduces more practical difficulties as only a subset of arms are revealed to the agent
to learn from each time. In this experiment, we report the results of the neural bandit models with
12
Published as a conference paper at ICLR 2022
(a) h1(x) = 10-2(x>ΣΣ>x)
Figure 5: Comparison of NPR and the baselines on the synthetic dataset with different k .
(b) h2 (x) = exp(-10(x>θ)2)
different value k, e.g., the number of arms disclosed each round. We followed the setting in Section
A.1.1 to set the network width m = 128.
Figure 5 demonstrates that for both datasets, revealing all the candidate arms, e.g., k = 100, will
considerably reduce the regret compared to only revealing a subset of the arms. But in both settings,
all bandit algorithms, including NPR, can obtain desired performance (i.e., sublinear regret). And
again, NPR still obtained more satisfactory performance as it is not subject to the added compu-
tational complexity in NeuralUCB and NeuralTS. The FALCON+ baseline performed much better
when k = 20 than k = 100, as it has less chance to select the suboptimal arms in each epoch, which
is expected based on its polynomial dependence on the number of arms K. On the contrary, NPR
only has a logarithmic dependence on K, whose advantage was validated in this experiment.
A.2 Experiments on Classification Datasets
Rounds
Rounds
(c) Covertype
(a) MagicTelescope	(b) Letter
Figure 6: Comparison of NPR and the baselines on the UCI datasets.
Table 1: DataSet StatiSticS
Dataset	Adult	Mushroom	Shuttle	Magic	Letter	Covertype
Input Dimension	2×15	2 ×23	7 X 9	2×12	16 × 26	54 × 7
We present the experiment results of the classification problem on UCI datasets,
MagicTelescope, letter and covertype. The experiment Setting and grid Search
Setting for parameter tuning are the Same aS deScribed in Section 5. For dataSetS letter and
covertype, only the reSultS of NeuralUCB and NeuralTS with diagonal approximation are
reported aS the required GPU memory for the calculation of matrix inverSe, together with the
neural model update, exceedS the Supported memory of the NVIDIA GEFORCE RTX 2080
graphical card. Similar obServationS are obtained for theSe three dataSetS. Compared to the linear
contextual modelS, the neural contextual bandit algorithmS Significantly improve the performance
with much lower regret (miStakeS on claSSification). NPR Showed conSiStently better performance
to the beSt neural bandit baSelineS. And due to itS perturbation-baSed exploration Strategy, no
extra computational reSourceS are needed, which makeS it more practical in real-world problemS,
eSpecially large-Scale taSkS.
13
Published as a conference paper at ICLR 2022
B Background for Theoretical Analysis
We denote the set of all the possible K arms by their context vectors as {xi}iK=1. Our analysis is
built upon the existing work in neural tangent kernel techniques.
Definition B.1 (Jacot et al. (2018); Cao & Gu (2019)). Let {xi}iK=1 be a set of contexts. Define
Hei(,1j)=Σi(,1j)=hxi,xji,	Bi(,lj) =	ΣΣi(,li)
理+1) = 2E(u,v)〜N(0,B(lj) [φ(U)φ(V)]，
H(l+1) = 2H(jE(u,v)〜N(0,B(lj) [φ0(u)φ0(v)] + ∑(l+1).
Then, H = (He (L) + Σ(L))/2 is called the neural tangent kernel (NTK) matrix on the context set.
With Definition B.1, we also have the following assumption on the contexts: {xi}iK=1.
Assumption B.2. H	λ0I; and moreover, for any 1 ≤ i ≤ K, kxik2 = 1 and [xi]j = [xi]j+d/2.
By assuming H λ0I, the neural tangent kernel matrix is non-singular (Du et al., 2019; Arora et al.,
2019; Cao & Gu, 2019). It can be easily satisfied when no two context vectors in the context set are
in parallel. The second assumption is just for convenience in analysis and can be easily satisfied by:
for any context vector x, ∣∣x∣∣2 = 1, we can construct a new context x0 = [x>, x>]>/√2. It can be
verified that if θ0 is initialized as in Algorithm 1, then f(xi; θ0) = 0 for any i ∈ [K].
The NTK technique builds a connection between the analysis of DNN and kernel methods. With
the following definition of effective dimension, we are able to analyze the neural network by some
complexity measures previously used for kernel methods.
K
Definition B.3. The effective dimension d of the neural tangent kernel matrix on contexts {xi}iK=1
is defined as
e_ logdet(I + TH∕λ)
=log(1 + TK∕λ)
The notion of effective dimension is first introduced in Valko et al. (2013) for analyzing kernel
contextual bandits, which describes the actual underlying dimension in the set of observed contexts
{xi}iK=1.
And we also need the following concentration bound on Gaussian distribution.
Lemma B.4. Consider a normally distributed random variable X 〜N(μ, σ2) and β ≥ 0. The
probability that X is within a radius of βσ from its mean can be written as,
P(IX - μl ≤ βσ) ≥ 1 - exp(-e2/2).
C Proof of Theorems and Lemmas in Section 4
C.1 Proof of Lemma 4.2
Proof of Lemma 4.2. By Lemma 4.1, with a satisfied m, with probability at least 1 - δ, we have for
any i ∈ [K],
h(xi) = hg(xi; θo)∕√m, √m(θ* - θo)i, and √m∣∣θ* - Θ0k2 ≤ √2h>H-1h ≤ S.
Hence, based on the definition of At and bt, we know that A-Ibt is the least square solution on top
of the observation noise. Therefore, by Theorem 1 in Abbasi-Yadkori et al. (2011), with probability
at least 1 一 δ, for any 1 ≤ t ≤ T, θ* satisfies that
k√m(θ*- θo) - A-1b11" ≤
Jr2 iog-⅞t(A⅛ + √λs,
δ2 det(λI)
(C.1)
14
Published as a conference paper at ICLR 2022
where R is the sub-Gaussian variable for the observation noise η. Then we have,
Kg(Xt,i； θo), A-1bt∕√mi- h(xt,i)∣
= Kg(Xt,i； θo), A-1bt∕√mi - hg(xt,i; θo), θ* - θoi∣
≤k√m(θ* — θo) - A-1btkAtkg(xt,i； θ0)∕√m∣∣A-1 ≤ αt∣∣g(xt,i; θ0)∕√m∣∣A-1.
with at = JR log δdRA；1) + √λS. This completes the proof.	口
C.2 Proof of Lemma 4.3
The proof starts with three lemmas that bound the error terms of the function value and gradient of
a neural network.
Lemma C.1 (Lemma B.2, Zhou et al. (2020)). There exist constants {Ci}5=ι > 0 such that for
any δ > 0, with J as the number of steps for gradient descent in neural network learning, if for all
t ∈ [T], η and m satisfy
2√t∕(mλ) ≥ CIm-3∕2L-3∕2[log(KL2∕δ)]3/2,
2pt∕(mλ) ≤ G min {L-6[logm]-3/2, (m(λη)2L-6t-1(logm)-1)3/8},
η ≤ C3(mλ + tmL)-1,
m1/6 ≥ C4√ogmL7∕2t7∕6λ-7∕6(l + √t∕λ),
then with probability at least 1 - δ, we have that kθt-ι - Θ0k2 ≤ 2,t∕(mλ) and
I∣θt-1 - θo - A-1bt∕√mk2 ≤ (1 - ηmλ)J∕2√t∕(mλ) + C¾m-2∕3√logmL7∕2t5∕3λ-5∕3(1 + √t∕λ).
Lemma C.2 (Lemma B.3 Zhou et al. (2020)). There exist constants {CU}3=ι > 0 such that for any
δ > 0, ifτ satisfies
Cum-3/2L-3∕2[log(KL2∕δ)]3/2 ≤ T ≤ CuL-6[logm]-3/2,
then with probability at least 1 - δ, for any θ and θ satisfying Iθ - θ0 I2 ≤ τ, Iθ - θ0 I2 ≤ τ and
j ∈ [K] we have
If(Xj；e) - f(xj； b) - hg(Xj； θ), e - bi∣ ≤ CUT4/3L3√mlog m.
LemmaC.3 (LemmaB.4, Zhou et al. (2020)). There exist constants {CV }3=ι > 0 such that for any
δ > 0, if T satisfies that
Cvm-3∕2L-3∕2[log(KL2∕δ)]3/2 ≤ T ≤ CvL-6[logm]-3∕2,
then with probability at least 1 - δ, for any Iθ - θ0 I2 ≤ T and j ∈ [K] we have Ig(Xj; θ)IF ≤
Cv √ mL.
With above lemmas, we prove Lemma 4.3 as follows.
ProofofLemma 4.3. According to Lemma C.1, with satisfied neural network and the learning rate
η, we have ∣∣θt-ι - θ0∣∣2 ≤ 2 vzt∕mλ. Further, by the choice of m, Lemma C.2 and C.3 hold.
Therefore, |f (xt,i； θt-ι) - hg(xt,i; θo), A-1bt∕√mi∣ can be first upper bounded by:
|f(xt,i； θt-ι) - hg(x5 θo), A-1bt∕√mi∣
≤lf (xt,i; θt-1)- f (Xt,i; θ0) - hg(xt,i; θO), θt-1 - θ0il + Kg(Xt,i; θO), θt-1 - θ0 - AtIbJVmi|
≤C3im-1∕6T2∕3λ-2∕3L3√Iogm + Kg(Xt,i； θo), θt-ι- θo - A-1bt∕√mi∣	(C.2)
where the first inequality holds due to triangle inequality, f(Xt,i; θO) = 0 by the random initial-
ization of θO, and the second inequality holds due to Lemma C.2 with the fact Iθt-1 - θO I2 ≤
2√t∕mλ.
15
Published as a conference paper at ICLR 2022
Next, we bound the second term of Eq (C.2),
Kg(Xt,i； θo), Θt-1 - θo - A-1bt∕√mi∣
≤lhg(xt,i; θo), θt-i — θo — A-Ibt/√m)∣ + Kg(Xt,i； θo), A-Ibt/√m - A-1bt∕√mi∣
≤kg(xt,i; Θ0)k2kθt-1 — θo — A-Ibt/√m∣∣2 + Kg(Xt,i； θo), A-Ibt/√m — A-1bt∕√mi∣
≤Ce,2(1 — ηmλ)J /TL" + Ce,3m-1∕6T 5∕3λ-5∕3L4Plog m (1 + PT/A)
+ Kg(Xt,i； θo), A-1bJ√m — A-1bt/^)|	(C.3)
where the first inequality holds due to the triangle inequality, the second inequality holds according
to CaUchy-SchWarz inequality, and the third inequality holds due to Lemma C.1, C.3, with the fact
that kθt-ι — θo∣∣2 ≤ 2pt/mA and t ≤ T.
Now, we provide the upper bound of the last term in Eq (C.3). Based on the definition of bt and bt,
we have
Kg(Xt,i； θo), A-1bt/^ — A-1b J√mi∣ = Kg(Xt,i； θo"√m, A-1 Xs=IYsTg(Xsg ； θo)i∣
According to the definition of the added random noise Ys 〜N (0, σ2), andLemmaB.4, with βt ≥ 0,
we have the following inequality,
Pt (Kg(Xt,i； θθ"√m, A-I Xs=I YsTg(Xs,as ； θO)∕√mil ≥ etkg(Xt,i； θo"√mkA-i
≤2 exp —
β2 kg(Xt,i； θθ"√mkA-ι
≤2 exp
2σ2g(Xt,i; θo)> A-1(ps=1 g(Xs,as; θ0)g(Xs,as; θo)[/m)A-1g(Xt,i; θo)/m
德),
where the last inequality stands as,
g(Xt,i； θO)TA-I(Xs=1 g(Xs,as ； θO)g(Xs,as ； θO)>/m) A-Ig(Xt,i； θθ"m
≤g(Xt,i; θo)>A-1(Xs=1 g(Xs,as ； θo)g(Xs,as ； Bo)>/m + λI)A-1g(Xt,i; θo)/m
= kg(Xt,i； θθ)∕√mkA-ι.
Therefore, in round t, for the given arm i,
Pt (Kg(Xt,i； θo), A-1bJ√m — A-1bt/√mi∣ ≤ βtkg(Xt,i; θo"√m∣∣A-ι) ≥ 1 —exp(—e2/(2b2)).
Taking the union bound over K arms, we have that for any t:
Pt (∀i ∈ [K], |hg(Xt,i； θo), A-1bt/^ — A-1bJ√mi∣ ≤ βt∣g(Xt,i; θo"√m∣∣A-i) ≥ 1 — Kexp(—e2/(2b2)).
By choosing βt = σ√4 log t + 2log K, we have:
Pt (∀i ∈ [K], ∣hg(Xt,i; θo), A-1bt/^ — A-1bJ√mi∣ ≤ βt∣g(Xt,i; θo"√m∣∣A-ι) ≥ 1 — t2.
This completes the proof.	□
C.3 Proof of Lemma 4.4
The proof requires the anti-concentration bound of Gaussian distribution.
Lemma C.4. For a Gaussian random variable X 〜 N(μ, σ2), for any β > 0,
σ
P( 口 >β)≥ exΡ√∏r.
16
Published as a conference paper at ICLR 2022
Proof of Lemma 4.4. Under event Et,2 , according to Lemma 4.2, (C.2) and (C.3), we have, for all
i∈ [K],
Pt(f (xt,i； θt) > h(xt川)-e(m))
≥Pt(hg(xt,i; θo), A-1bt∕√mi > hg(xt,i θo), A-1bt∕√mi + αtkg(x5 θo)∕√mkA-ι)
According to the definition of At, bt and bt, We have,
-	、t、	1
hg(xt,i; θo), A-1bt∕√m - A-1b t∕√mi = EYt ∙ —g(xt,i; θ0)>A-1g(xs,as; θo) = Ut
m
s=1
Which folloWs a Gaussian distribution With mean E[Ut] = 0 and variance Var[Ut] as:
Var[Ut] =σ2 ∙ (Xs = IW g(Xt,i； θθ)>A-Ig(Xs,as ； θ0 ))2)
=σ2kg(xt,i; θ0)∕√mkA-1 - λσ2kg(xt,i； θ0)∕√mkA-2
≥σ2(I- λλm1n(At))kg(xt,i； θ0)∕√mkA-l
≥σ2(1 - λλκ1(Aκ))kg(xt,i； θ0)∕√mkA-1,
Where the second equality holds according to the definition of At, and the first inequality holds as
for any positive semi-definite matrix M ∈ Rd×d,
x>M2x = λ2max(M)x>(λ-m2ax(M)M2)x ≤ λmax(M)kxk2M,
and λmin(At)	≥ λK (AK).	Therefore, based on Lemma C.4, by choosing σ =
αt∕	1 - λλ-K1(AK), the target probability could be loWer bounded by,
Pt(f(xt,j; θt) > h(xt,at) - e(m)) ≥ Pt(Ut > αtkg(x5 θ0)∕√mkA-1)
=P(_JU_ > αtkg(xt,i; θ0"√mkA-1) ≥ P(_JU_ > 1) ≥	1
'Var[Ut]	Var[Ut]	'Var[Ut]	4e√π
This completes the proof.	□
C.4 Proof of Lemma 4.5
ProofofLemma 4.5. With the defined sufficiently sampled arms in round t, Ωt, we have the set of
undersampled arms Ωt = [K] \ Ω> We also have the least uncertain and undersampled arm et in
round t defined as,
et = argmin kg(xj,t； θo)∕√mkA-ι
j∈Ω t	t
In round t, it is easy to verify that E[h(xt,a*) - h(xt,at)] ≤ E[(h(xt,a*) - h(xt,at))l{Et,2}] +
Pt(Et,2).
Under event Et,1 and Et,2, we have,
h(xt,at ) - h(xt,at )
=h(Xt,at ) - h(xt,et ) + h(xt,et ) - h(xt,at )
≤∆et + f (xt,et, θt) - f (xt,at, θt) + 24m) + (Bt + αt)(kg(xt,et ； θ0)∕√mkA-i + ∣∣g(xt,at ； θ0)∕√mkA-i )
≤4e(m) + (Bt + αt )(2kg(xt,et ； θ0 )∕√mkA-i + kg(xt,at ； θ0)∕√mkA-i )
Next, we will bound E[∣g(xt,et ； θo)∕√m∣A-ι ]. It is easy to see that,
E[∣g(xt,at ； θθ)∕√m∣A-ι ]
=E[∣g(xt,at ； θo)∕√m∣A-ι 1{at ∈ Ωt}] + E[∣g(xt,at ； θo)∕√m∣A-ι 1{at ∈ Ωt}]
≥∣g(xt,et ； θ0)∕√m∣A-i Pt (at ∈ Ω t).
17
Published as a conference paper at ICLR 2022
It can be arranged as kg(xt,e,； θo)∕√m∣∣A-I ≤ E[kg(xt,at； θo)∕√m∣∣A-ι]∕Pt(αt ∈ Ωt). Hence,
the one step regret can be bounded by the following inequality with probability at least 1 - δ,
E[h(Xt川)-h(Xt,aj]
≤P(Et,2) + 4e(m) + (βt + αt)(l + 2∕P(at ∈ Ω力 |厄区皿;θo)∕√m∣∣a-i .
For the probability Pt (at ∈ Ωt), We have:
Pt (at ∈ Ωt) ≥Pt(∃ai ∈ Ωt : f(xta; θt) > max f(xtg； θt))
aj ∈Ωt
≥Pt(f(xt川；θt) > max f(xt,aj； θt))
aj ∈Ωt
≥Pt(f (xt,a*; θt) > max f(xt,aj; θt),Et,2 occurs)
't	aj ∈Ωt ' J	'
≥Pt(f(xt,* θt) > h(xp) i(m)) — Pt(Et,2)
≥Pt(Et,3) - Pt(Et,2)
where the fourth inequality holds as for arm aj ∈ Ωt, under event Et,1 and Et2:
f(xt,j； θt) ≤ h(xt,j) + e(m) + (βt + αt)∣∣g(xt,j； θ0)∕√m∣∣A-1 ≤ h(xt川)-4m)∙
This completes the proof.	口
C.5 Proof of Lemma 4.6
We first need the following lemma from Abbasi-Yadkori et al. (2011).
Lemma C.5 (Lemma 11, Abbasi-Yadkori et al. (2011)). We have the following inequality:
XT=Imin {kg(xt,at ； θθ)∕√mkA 二，1} ≤ 2log detAT .
Lemma C.6 (Lemma B.1, Zhou etal. (2020)). Let G = [g(x1; θo),..., g(xK; θo)]∕√m ∈ Rp×K.
Let H be the NTK matrix as defined in Definition B.1. For any δ ∈ (0, 1), if
m = Ω( "(一)
then with probability at least 1 - δ, we have
kG>G - HkF ≤K.
ProofofLemma 4.6. Denote G = [g(x1; θo)∕√m,..., g(xK; θo)∕√m] ∈ Rp×K, then we have
log detAΓ = logdet (I + X： 1 g(xt,at ； θθ)g(xt,at ； θθ)>∕(mλ))
det λI	t=1
TK
≤ logdet (I + Et=I Ei=I g(xi； θο)g(xi; θο)>∕(mλ))
= log det I+TGG>∕λ = log det I+TG>G∕λ ,	(C.4)
where the inequality holds naively, the third equality holds since for any matrix A ∈ Rp×K, we
have det(I + AA>) = det(I + A>A). We can further bound Eq (C.4) as follows:
log det I+TG>G∕λ = log det I+TH∕λ+T(G>G - H)∕λ
≤ log det I+TH∕λ + h(I + TH∕λ)-1, T(G>G - H)∕λi
≤ logdet I + TH∕λ + k(I + TH∕λ)-1kFkG>G - HkF ∙ T∕λ
≤ logdet (I + TH∕λ) + T√K∣∣G>G - HkF
≤ log det I+TH∕λ + 1 = delog(1 + TK∕λ) + 1,
18
Published as a conference paper at ICLR 2022
where the first inequality holds due to the concavity of log det(∙), the second inequality holds due
to the fact that(A, Bi ≤ IlAkFIIBIlf, the third inequality holds due to the facts that I + H∕λ 占 I,
λ ≥ 1 and kA∣∣f ≤ √K∣∣A∣∣2 for any A ∈ Rk×k, the fourth inequality holds by Lemma C.6
with the required choice of m, the fifth inequality holds by the definition of effective dimension in
Definition B.3. This completes the proof.	□
19