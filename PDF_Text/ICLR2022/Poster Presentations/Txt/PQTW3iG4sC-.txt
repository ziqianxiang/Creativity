On Feature Learning in Neural Networks with
Global Convergence Guarantees
Zhengdao Chen	Eric Vanden-Eijnden
Courant Institute of Mathematical Sciences	Courant Institute of Mathematical Sciences
New York University	New York University
New York, NY 10012, USA	New York, NY 10012, USA
zc1216@nyu.edu	eve2@cims.nyu.edu
Joan Bruna
Courant Institute of Mathematical Sciences and Center for Data Science
New York University
New York, NY 10012, USA
bruna@cims.nyu.edu
Ab stract
We study the optimization of wide neural networks (NNs) via gradient flow (GF)
in setups that allow feature learning while admitting non-asymptotic global con-
vergence guarantees. First, for wide shallow NNs under the mean-field scaling
and with a general class of activation functions, we prove that when the input
dimension is no less than the size of the training set, the training loss converges to
zero at a linear rate under GF. Building upon this analysis, we study a model of
wide multi-layer NNs whose second-to-last layer is trained via GF, for which we
also prove a linear-rate convergence of the training loss to zero, but regardless of
the input dimension. We also show empirically that, unlike in the Neural Tangent
Kernel (NTK) regime, our multi-layer model exhibits feature learning and can
achieve better generalization performance than its NTK counterpart.
1	Introduction
The training of neural networks (NNs) is typically a non-convex optimization problem, but remarkably,
simple algorithms like gradient descent (GD) or its variants can usually succeed in finding solutions
with low training losses. To understand this phenomenon, a promising idea is to focus on NNs with
large widths (a.k.a. under over-parameterization), for which We can derive infinite-width limits under
suitable ways to scale the parameters by the widths. For example, under a "1 / √width" scaling of
the weights, the GD dynamics of wide NNs can be approximated by the linearized dynamics around
initialization, and as the widths tend to infinity, we obtain the Neural Tangent Kernel (NTK) limit of
NNs, where the solution obtained by GD coincides with a kernel method [37]. Importantly, theoretical
guarantees for optimization and generalization can be obtained for wide NNs under this scaling
[19, 5]. Nonetheless, it was pointed out that this NTK analysis replies on a form of lazy training
that excludes the learning of features or representations [16, 72], which is a crucial ingredient to the
success of deep learning, and is therefore not adequate for explaining the success of NNs [27, 41].
Meanwhile, for shallow (i.e., one-hidden-layer) NNs, if we choose a “1 / width” scaling, we can
derive an alternative mean-field (MF) limit as the widths tend to infinity. Under this scaling, feature
learning occurs even in the infinite-width limit, and the training dynamics can be described by the
Wasserstein gradient flow of a probability measure on the space of the parameters, which converges
to a global minimizer of the loss function under certain conditions [59, 50, 63, 14]. Generalization
guarantees have also been proved for learning with shallow NNs under the MF scaling by identifying
a corresponding function space [6, 48]. However, currently there are three limitations to this model of
over-parameterized NNs. First, the global convergence guarantees for shallow NNs only hold in the
infinite-width limit (i.e. they are asymptotic). While [12] studies the deviation between finite-width
NNs and their infinite-width limits during training, the analysis is done only asymptotically to the
1
next order in width. Second, a convergence rate has yet to be established except under special
assumptions or with modifications to the GD algorithm [13, 34, 43, 38]. Third, while several works
have proposed to extend the MF formulation to deep (i.e., multi-layer) NNs [4, 62, 52, 23, 57], there
is less concensus on what the right model should be than for the shallow case. In summary, we still
lack a model for the GD optimization of shallow and multi-layer NNs that goes beyond lazy training
while admitting fast global convergence.
In this work, we study the optimization of both shallow NNs under the MF scaling and a type of
partially-trained multi-layer NNs, and obtain theoretical guarantees of linear-rate global convergence.
1.1	Summary of main contributions
We consider the scenario of training NN models to fit a training set of n data points in dimension d,
where the model parameters are optimized by gradient flow (GF, which is the continuous-time limit
of GD) with respect to the squared loss. Allowing most choices of the activation function, we prove
that:
1.	For a shallow NN, if the hidden layer is sufficiently wide and the input data are linearly
independent (requiring n ≤ d), then with high probability, the training loss converges to
zero at a linear rate.
2.	For a multi-layer NN where we only train the second-to-last layer, if the hidden layers are
both sufficiently wide, then with high probability, the training loss converges to zero at
a linear rate. Unlike for shallow NNs, here we no longer need the requirement on input
dimension, demonstrating a benefit of jointly having depth and width.
We also run numerical experiments to demonstrate that our model exhibits feature learning and can
achieve better generalization performance than its NTK counterpart.
1.2	Related works
Over-parameterized NNs, NTK and lazy training. Many recent works have studied the opti-
mization landscape of NNs and the benefits of over-parameterization [24, 67, 60, 66, 64, 54, 65, 42,
3, 40, 76, 15, 75, 17, 39, 9, 32, 20, 31, 61, 10]. One influential idea is the Neural Tangent Kernel
(NTK) [37], which characterizes the behavior of GD on the infinite-width limit of NNs under a
particular scaling of the parameters (e.g. for shallow NNs, replacing 1/m with 1/√m in (1)). In
particular, when the network width is polynomially large in the size of the training set, the training
loss converges to a global minimum at a linear rate under GD [19, 5, 55]. Nonetheless, in the NTK
limit, due to a relatively large scaling of the parameters at initialization, the hidden-layer features do
not move significantly [37, 19, 16]. For this reason, the NTK scaling has been called the lazy-training
regime, as opposed to a feature-learning or rich regime [16, 72, 26]. Several works have investigated
the differences between the two regimes both in theory [28, 29, 69, 47] and in practice [27, 41].
In addition, several works have generalized the NTK analysis by considering higher-order Taylor
approximations of the GD dynamics or finite-width corrections to the NTK [2, 35, 7, 33].
Mean-field theory of NNs. An alternative path has been taken to study shallow NNs in the
mean-field scaling (as in (1)), where the infinite-width limit is analogous to the thermodynamic
or hydrodynamic limit of interacting particle systems [59, 50, 63, 14, 49, 70]. Thanks to the
interchangeability of the parameters, the neural network is equivalently characterized by a probability
measure on the space of its parameters, and the training can then be described by a Wasserstein
gradient flow followed by this probability measure, which, in the infinite-width limit, converges to
global mimima under mild conditions. Regarding convergence rate, ref. [71] proves that if we train
a shallow NN to fit a Lipschitz target function under population loss, the convergence rate cannot
beat the curse of dimensionality. In contrast, we will study the setting of empirical risk minimization,
where there are finitely many training data. Ref. [34] shows that mean field Langevin dynamics on
shallow NNs can converge exponentially to global minimizers in over-regularized scenarios, but we
focus on GF without entropic regularization. Besides the question of optimization, shallow NNs
under this scaling represent functions in the Barron space [48] or variation-norm function space [6],
which provide theoretical guarantees on generalization as well as fluctuation in training [12]. Several
works have proposed different mean-field limits of wide multi-layer NNs and proved convergence
2
guarantees [4, 62, 51, 52, 23, 57, 22], but questions remain. First, due to the presence of different
symmetries in a multi-layer network compared to a shallow network [57], the limiting object at the
infinite-width limit is often quite complicated. Second, it has been pointed out that under the MF
scaling of a multi-layer network, an i.i.d. initialization of the weights would lead to a collapse of
the diversity of neurons in the middle layers, diminishing the effect of having large widths [23]. In
addition, while another line of work develops MF models of residual models [8, 46, 21, 36], we are
interested in multi-layer NN models with a large width in every layer.
Feature learning in deep NNs. Ref. [1] demonstrates the importance of hierarchical learning by
proving the existence of concept classes that can be learned efficiently by a deep NN with quadratic
activations but not by non-hierarchical models. Ref. [11] studies the optimization landscape and
generalization properties of a hierarchical model that is similar to ours in spirit, where an untrained
embedding of the input is passed into a trainable shallow model, and prove an improvement in sample
complexity in learning polynomials by having neural network outputs as the embedding. However,
the trainable models they consider are not shallow NNs but their linearized and quadratic-Taylor
approximations, and furthermore the convergence rate of the training is not known. Ref. [73] proposes
a novel parameterization under which there exists an infinite-width limit of deep NNs that exhibits
feature learning, but properties of its training dynamics is not well-understood. Our multi-layer NN
models adopt an equivalent scaling (see Appendix C), and our focus is on proving non-asymptotic
convergence guarantees for its partial training under GF.
2	Problem setup
2.1	Model
We summarize our notations in Appendix A. Let Ω ⊆ Rd denote the input space, and let X =
[xi,…,xd]| ∈ Ω denote a generic input data vector. A shallow NN model under the MF scaling can
be written as:
md
f (X) = m XX ciσ (√d XX Wij Xj),	⑴
where m is the width, W ∈ Rm×d and c = [c1, ..., cm] ∈ Rm are the first- and second-layer weight
parameters of the model, and σ : R → R is the activation function. For simplicity of presentation, we
neglect the bias terms. In this paper, we study a more general type of models with the following form:
∀i ∈ [m]
m
f (x) = m X ciσ(hi(x)),
i=1
hi(x) = -‰ X WijΦj(x),
D j=1
(2)
(3)
where W ∈ Rm×D and c = [c1, ..., cm] ∈ Rm are parameters of the model, and φ1, ..., φD are a set
of functions from Ω to R that we call the embedding. Each of hi,…,hm is a function from Ω to R,
and we will refer to them as the (hidden-layer) feature map or activations. For simplicity, we write
Φ(x) = [φi (x),…Φd (x)]| ∈ RD. We consider two types of the embedding, Φ, as described below:
Fixed embedding D is fixed and Φ is deterministic. In the simplest example, we set D = d and
φj (X) = xj, ∀j ∈ [D], and recover the shallow NN model in (1). More generally, our definition
includes cases where Φ is a deterministic transformation of an input vector in Ω into an embedding
vector in RD. This can be understood as input pre-processing or feature engineering.
High-dimensional random embedding D is large and Φ is random. For instance, we can sample
each Zj i.i.d. in Rd and set φj∙ (x) = σ(√zjx), equivalent to setting φι,...,φmtl as the hidden-layer
3
activations of a shallow NN with randomly-initialized first-layer weights. Then, the model becomes
∀i ∈ [m]
m
f(x) = m X Ciσ(hi(x)),
i=1
1D	1
hi(X)= √d X Wijσ √dz Zjx)
(4)
(5)
Thus, we obtain a 3-layer feed-forward NN whose first-layer weights are random and fixed, and
We call it a partially-trained 3-layer (P-3L) NN. Note that the scaling in this model is different
from both the NTK scaling (1/√m instead of 1/m in (4)) and the MF scaling for multi-layer NNs
adopted in [4, 62, 51, 57, 23] (1/D instead of 1/√D in (5)). We show in Appendix B that when σ is
homogeneous, this scaling is consistent With the Xavier initialization of neural netWork parameters
up to a reparameterization [30, 56]. We also show in Appendix C that in certain cases this scaling is
equivalent to the maximum-update parameterization proposed in [73]. Numerical experiments that
compare different scalings are described in Section 4.
2.2	Training with gradient flow
Consider the scenario of supervised least-squares regression, where we are given a set of n training
data points together with their target values, (xι,yι),..., (xn, yn) ∈ Ω X R. We fit our models by
minimizing the empirical squared loss:
1n
L[f] = 2Σ (f(xa)-ya)2 .	(6)
a=1
To do so, we first initialize the parameters randomly by sampling each ci and Wij i.i.d. from
probability measures πc ∈ P(R) and πw ∈ P(RD), respectively, and then perform GD onW . For
simplicity of analysis, we leave c untrained, and further assume that
Assumption 1. ∏ = ɪδ^(dc) + 11 δ-^(dc) for some C > 0 Independentfrom m, which is the law of
a scaled Rademacher random variable.
If σ is Lipschitz, it is differentiable almost everywhere, and we write σ0(x) to denote the derivative
of σ when it is differentiable at x and 0 otherwise. When σ is differentiable at hi (x), there is
∂f	1
∂wi^(X) = m√Dciσ (hi(x))φj(X),	(7)
and the gradient of the loss function with respect to Wij is given by
=iτ√ =	Ci X (f (Xa) - ya) σ0(hi(Xa))φj (Xa) ∙	⑻
∂Wij m D a=1
Thus, we can perform GD updates on according to the following rule: ∀i ∈ [m] and ∀j ∈ [D],
∂L[f]	δ	n
Wij	J	Wij - mδ Wm = Wij---------7== ci ^X (f (Xa) - ya) σ (hi(Xa))0j (Xa),	(9)
∂Wij	D a=1
where δ > 0 is the step size. As we discuss in Appendix B, this is consistent with the standard GD
update rule for Xavier-initialzed NNs. In the limit of infinitesimal step size (δ → 0), the evolution of
the parameters during training is described by the GF equation: if we use the superscript t ≥ 0 to
denote time elaposed during training, the time-derivative of the parameters is given by
n
--√= X (ft(Xa) - ya) σ'(hi (Xa))φj (Xa),
a=1
Wtj
(10)
where ft denotes the output function and ht1, ..., htm denote the hidden-layer feature maps determined
by the parameters at time t. Then, induced by the evolution of Wt , each hit evolves according to
Dn
∀X ∈ Ω :	ht(X) = √= X WtjΦj (x) = -Ci X G(x, Xa) (ft(Xa) - ya) σ0(ht(Xa)) , (11)
D j=1	a=1
4
where We define a kernel function G : Ω X Ω → R as
1D	1
G(X, X) = D ɪl φj(χ)φj(X ) = D(φ(χ))lφ(x ).
Accordingly, the output function ft satisfies
1m
∀x ∈ Ω :	ft(x) =— Xciσ0(h.(x))ht(x)
m i=1
=-m X σ'(ht(X))X G(X, xa) (f'(Xa) - ya) σ0(hi (Xa)),
i=1	a=1
Thus, the loss function Lt := L[ft] evolves according to
n
L t = Xf t(Xa ) — ya) ft (Xa)
a=1
2m n
=-m XXGab Ut(Xa)- ya) (ft(Xb) — W σ0 (hi (Xa )) σ0 (hi (Xb))
i=1 a,b=1
≤- c2λm≡ X X U(XaiY (σ0(hi(Xa)))2 ,
i=1 a=1
(12)
(13)
(14)
where we define the (symmetric) Gram matrix G ∈ Rn×n with entries Gab = Gba = G(Xa, Xb),
and use λmin(G) to denote its least eigenvalue. We will also use Gmin = mina∈[n] Gaa and
Gmax = maxa∈[n] Gaa to denote the minimum and maximum diagonal entries of G, respectively.
Since G is positive semi-definite, We see that Lt ≤ 0, which means that the loss value is indeed
non-increasing along the GF trajectory.
Feature learning Compared to the NTK scaling of neural networks, the crucial difference is the
1/m factor in (2), instead of 1∕√m. It is known that under the NTK scaling, due to the 1/√m
factor, the movement of the feature maps, hi,…，hm,is only of order O(1 /√m) while the function
value changes by an amount of order Ω(1). While this greatly simplifies the convergence analysis,
it also implies that the hidden-layer representations are not being learned. In contrast, with the
1/m factor in (2), if σ is Lipschitz with Lipschitz constant Lσ, there is |ft2 (X) - ft1 (X)| ≤
Lσ^ Pm=I ∣ht2(x) 一 hi1 (x)|, ∀t1,t2 ≥ 0. Therefore, regardless of m and D,
m
m X Ihi1 (X) - hi2(x)∣≥ (^)-1(Lσ)-1∣ft1 (X) - ft2(X)| ,	(15)
m i=1
which implies that the average movement of the feature maps is on the same order as the change in
function value, and thus the hidden-layer representations as well as the NTK undergoes nontrivial
movement during training. In Appendix C, we further justify the occurrence of feature learning using
the framework developed in [73].
3 Convergence analysis
3.1	Models with a fixed embedding
To prove that the training loss converges to zero, we need a lower bound on the absolute value of
Lt . Indeed, if G is positive definite, which depends on Φ and the training data, we can establish one
in the following way. First, as a simple case, if we use an activation function whose derivative’s
absolute value is uniformly bounded from below by a constant Kσ0 > 0, such as linear, cubic or
(smoothed) Leaky ReLU activations, we can derive a Polyak-Lojasiewicz (PL) condition [58, 45]
from (14) directly,
Lt ≤- 2^2λmin(G) (Kσ0 )2 Lt ,
(16)
5
which implies Lt ≤ Loe-2c2λmin(G)(Kσ0 )2t, indicating that the training loss decays to 0 at a linear
rate.
For more general choices of the activation function, a challenge is to guarantee that, heuristically
speaking, for each a ∈ [n], σ0 hi(xa) does not become near zero for too many i ∈ [m] before the
loss vanishes. To facilitate a finer-grained analysis, we need the following mild assumption on σ:
Assumption 2. σ is Lipschitz with Lipschitz constant Lσ, and there exists an open interval I =
(Il, Ir) ⊆ R on which σ is differentiable and ∣σ0∣ is lower-bounded by some K。，> 0.
Intuitively, I is an active region of σ, within which the derivative has a magnitude bounded away
from zero. This assumption is satisfied by the majority of activation functions in practice, including
smooth ones such as tanh and sigmoid as well as non-smooth ones such as ReLU. Then, under the
following initialization scheme, we prove a general result for models with a fixed embedding.
Assumption 3. πw is the D-dimensional standard Gaussian distribution, i.e., each Wij is sampled
independently from a standard Gaussian distribution.
Theorem 1 (Fixed embedding). Suppose that Assumptions 1, 2 and3 are satisfied, and λmin(G) > 0.
Then ∃^o, r and C > 0 such that ∀δ > 0, if C ≥ ^oλmaχ(G)∕λmin(G) and m ≥ C(1 + ^2)log(n∕δ),
then with probability at least 1 - δ, it holds that ∀t ≥ 0,
Lt ≤ L0e-rc2λmin(G)t .	(17)
Here, ^o, r and C depend on I, Gmin, Gmax, kyk,Lσ and K。，(but not on m, n, d,D, δ, or λmin(G)).
The result is proved in Appendix E, and below we briefly describe the intuition. A key to the proof is
to guarantee that enough neurons remain in the active region throughout training. Specifically, with
respect to each training data point (i.e. for each a ∈ [n]), we can keep track of the proportion of
neurons (among all i ∈ [m]) for which hit(xa) ∈ I. We show that if the proportion is large enough
at initialization (shown by Lemma 3 in Appendix E.2 under Assumption 3), then it cannot drop
dramatically without a simultaneous decrease of the loss value, as long as the ci ’s are not too small in
absolute value. This property of the dynamics is formalized in the following lemma:
Lemma 1. Consider the dynamics ofLt and hit(xa) i∈[m],a∈[n] governed by (11) and (14). Assume
that λmin(G) > 0, and∀i ∈ [m],陵| = C > 0. UnderAssumption 2, define
ηt=minimaXXx 1ht(χa)∈ι},∀t ≥ 0 and
Then ∀t ≥ 0, there is
η0 = min]	XXX 1h0(xa)∈(2⅛Ir，e)}
(18)
W) 3 ≥ S0) 2 -κ(λmm (G) ,λmaχ(G) )((L0) 2 - (Lt) 1 )/C ,	(19)
where κ(λ1, λ2)=/丈忆Il).
3.1.1	EXAMPLE: SHALLOW NEURAL NETWORKS WHEN n ≤ d
In the case of shallow NNs under the MF scaling, G = G(0) ∈ Rn×n , where
GaO)=d (Xa)Txb	(20)
Thus, G(0) is positive definite if and only the training data set x1, ..., xn} ⊆ Rd consists of linearly-
independent vectors, which is possible (and expected if the training data are sampled independently
from some non-degenerate distribution) when n ≤ d. In that case, Theorem 1 implies
Corollary 1 (Shallow NN with n ≤ d). Suppose that Assumptions 1, 2 and 3 are satisfied. If the
training data are linearly-independent vectors, then under GF (10) on the first-layer weights of the
shallow NN, the training loss converges to zero at a linear rate.
While the assumption that n ≤ d is restrictive, we note that existing convergence rate guarantees
for the GD-type training of shallow NNs in the MF scaling need strong additional assumptions [38],
modifications to the GD algorithm [34, 13, 53], or restrictions to certain special tasks [43].
6
3.2	Models with a high-dimensional random embedding
A clear limitation of Corollary 1 is that it is only applicable when n ≤ d, since otherwise the Gram
matrix G(0) cannot be positive definite. This motivates us to consider the use of a high-dimensional
embedding Φ to lift the effective input dimension. In particular, we focus on the scenario where D is
large and Φ is random. While the Gram matrix G in this case is also random, we only need that it
concentrates around a deterministic and positive definite limit as D tends to infinity:
Condition 1 (Concentration of G around a positive definite matrix). There exists a (determinis-
tic) positive definite matrix G ∈ Rn×n with least eigenvalue λmin(G) > 0 such that ∀δ,u > 0,
∃Dmin (δ, U) > 0 such that if D ≥ Dmin (δ, U) ,then P(∣∣G — G ∣∣2 > u) < δ.
Condition 1 is sufficient for us to apply Lemma 1 and obtain the following global convergence
guarantee, which extends Theorem 1 to models with a high-dimensional random embedding. The
proof is given in Appendix F.
Theorem 2 (High-dimensional random embedding). Under Assumptions 1, 2, 3 and Condition 1,
∃^o, r and C > 0 such that ∀δ > 0, if C ≥ ^oλmaχ(G)∕λmin(G), m ≥ C(1 + ^2)log(n∕δ) and
D ≥ Dmin (1 δ, 2 λmin(G)) ,then with probability at least 1 — δ, it holds that ∀t ≥ 0,
Lt ≤ L0efc2λmin(G)t .	(21)
Here, ^o, r and C depend on I, Gmin, GGmax, ∣y∣,Lσ and 不，(but not m, n, d, D, δ, or λmin(G)).
3.2.1	EXAMPLE: PARTIALLY-TRAINED THREE-LAYER NEURAL NETWORKS (P-3L NNS)
Consider the P-3L NN model defined in (4). In this case, the Gram matrix is G(1), defined by
1D 1	1
Gab= D X U√dZjxa) U√dZjxb)	(22)
j=1
If Z1, ..., Zm are sampled i.i.d. from a probability measure πz on Rd and fixed during training, then
the limiting Gram matrix, denoted by G(I) ∈ Rn×n,is given by
Gab)= Ez~∏z ]σ(√dzTxa)σ(√dzTxb)]	(23)
Thus, for the convergence result, the assumption we need on the limiting Gram matrix is
Assumption 4. ∏z is SUb-GaUSSian and the matrix G(I), which depends on the choice of σ and the
training set, is positive definite with λmin (G(I)) > 0 and (G ⑴)maχ < ∞.
This assumption also plays an important role in the NTK analysis, and it is satisfied if, for example,
πz is the d-dimensional standard Gaussian distribution, no two data points are parallel, and σ is either
the ReLU function [19] or analytic and not a polynomial [18]. When Assumption 4 is satisfied, as
long as σ is Lipschitz, we can use standard concentration techniques to verify Condition 1. Thus,
Theorem 2 implies that
Theorem 3 (P-3L NN). UnderAssumptions 1, 2, 3 and 4, ∃^o, r, Ci and C2 > 0 such that ∀δ > 0, if
C ≥ ^oλmaχ(G(I))/λmin(G(1)), m ≥ Ci(1 + c2) log (n∕δ) and D ≥ C2n2 log(n∕δ)∕λmm(G(1))2,
then with probability at least 1 — δ, it holds that ∀t ≥ 0,
Lt ≤ Loefc2λmin(G⑴)t .	(24)
Here, ^o, r, Ci and C? depend on I, Gm)口, GmUx, ∣yk,Kσ0 as well as the sub-Gaussian norm of μz
(but not on m, n, d, D, δ or λmin(G(I))).
The proof is given in Appendix G. Compared to Corollary 1 for shallow NNs, a highlight of Theorem 3
is that the requirement of n ≤ d is no longer needed. This demonstrates an advantage of the high-
dimensional random embedding realized by the first hidden layer in the P-3L NN, thus illustrating a
benefit of having both depth and width in NNs from the viewpoint of optimization. Compared to the
NTK result [18], our analysis assumes the same level of over-parameterization, but crucially allows
feature training to occur, which we discuss in Section 2.2 and support empirically in Section 4.3.
Furthermore, by using a multi-layer NN with random and fixed weights as the high-dimensional
random embedding, we extend the P-3L NN to a partially-trained L-layer NN model in Appendix H,
for which similar convergence results can be proved for training its second-to-last layer via GF.
7
Table 1: Three different scalings of the partially-trained 3L NN model considered in Experiment 3.
Model	Ours	NTK	MF [4, 52, 62, 23]
f(X)	ml Pi=I Ciσ(hi(x))	√m P乙 Ciσ(hi(项一	ml Pi=ι ciσ(hi(x))
hi(x)	√m P=I Wijσ(√ZTxr	√m PT=I Wjσ(√dZk	表 Pj= Wjσ(√dZk
W k+1 ij	Wij- mδ ∂Wk ij	Wkj - δ ∂Wk 	ij		Wij- m2 δ ∂Wk ij
4	Numerical experiments
Additional results and details of the experiments are provided in Appendix I.
4.1	Experiment 1: Convergence of training when fitting random data
We train shallow NNs to fit a randomly labeled data set {(x1, y1), ..., (xn, yn)} with d = 20.
Specifically, we sample each xa i.i.d. with every entry sampled independently from a standard
Gaussian distribution, and each y& i.i.d. uniformly on [-2, 1 ] and independently from the Xa's. We
see from Figure 1 that the convergence happens at a nearly linear rate when n = 20 and 40, and the
rate decreases as n becomes larger. This is coherent with our theoretical result (Corollary 1), and
interestingly also echoes a prior result that the convergence rate of optimizing a shallow NN using
population loss can suffer from the curse of dimensionality [71], which implies a worsening of the
convergence rate as the number of data points increases.
4.2	Experiment 2: Benefit of input embedding
We consider a model defined by (2) and (3) with d = 30 and Φ(x) = vec(xx|) ∈ Rd2, which we
call a shallow NN augmented with quadratic embedding. We compare this model against a plain
shallow NN (without the extra embedding), both with m = 8192, to fit a series of training sets with
various sizes where the target y is given by another shallow NN augmented with quadratic embedding
with m = 5. We see from Figure 2 that the augmented shallow NN achieves lower test error given
the same number of training samples, demonstrating the benefit of a good embedding.
4.3	Experiment 3: Feature learning v.s. lazy training
We consider the P-3L NN model defined in (4) and (5) with D = m (i.e. both hidden layers having
the same width), and compare it with 3-layer NN models under NTK and MF scalings, as we define
in Table 1 based on prior literature [37, 4, 52, 62, 23], which undergo partial training in the same
fashion. We adopt the data set used in [69] (more details in Appendix I.3), and train the models by
minimizing the unregularized squared loss for varying n’s and m’s.
First, we see from the top-left plot in Figure 4 that, consistently across different m, the training loss
converges at a linear rate for the model under our scaling, which is coherent with Theorem 3. Second,
we see from the second row that feature learning occurs in the model under our scaling but negligibly
in the model under the NTK scaling, as expected [16]. Note also that under the MF scaling, the
feature maps h1(x), ..., hm(x) concentrate near 0 at initialization due to the small scaling, but gains
diversity during training. Third, we see from Figure 3 that our model yields the smallest test errors
out of all three, and in addition, as n grows the test error decreases faster under the MF scaling than
under the NTK scaling, both indicating an advantage of feature learning compared to lazy training.
5	Conclusions and limitations
We consider a general type of models that includes shallow and partially-trained multi-layer NNs,
which exhibits feature learning when trained via GF, and prove non-asymptotic global convergence
guarantees that accommodates a general class of activation functions. For a randomly-initialized
shallow NN in the MF scaling that is wide enough, we prove that by performing GF on the input-layer
weights, the training loss converges to zero at a linear rate if the number of training data does not
exceed the input dimension. For a randomly-initialized multi-layer NN with large widths, we prove
8
Figure 1: Training loss v.s.
number of GD steps for dif-
ferent n in Experiment 1.
Figure 2: Test error v.s. n in
Experiment 2 by the two models
with m = 8192.
Figure 3: Test error v.s. n in Ex-
periment 3 by the three models
with m = 8192.
hi(x1)
Figure 4: Results of Experiment 3 when n = 600. Each column corresponds to a different scaling
of the P-3L NN model, as described in Table 1. Row 1: Evolution of training loss (solid curve)
and test error (dashed curve) during training. Row 2: Distribution of the hidden-layer feature map
(pre-activation) associated with two particular input data points. Each dot represents a different i,
(i.e., neuron in the second hidden layer,) and the x- and y-coordinates equal hi(x1) and hi(x2),
respectively, where x1 is an input from the training set and x2 is an input from the test set.
that by performing GF on the weights in the second-to-last layer, the same result holds except there is
no requirement on the input dimension. We also perform numerical experiments to demonstrate the
advantage of feature learning in our partially-trained multi-layer NNs relative to their counterparts
under the NTK scaling.
Our work focuses on the optimization rather than the approximation or generalization properties of
NNs, which are also crucial to understand. In addition, as our current theoretical results on global
convergence neglect the bias terms and assume that the last-layer weights are untrained, a more
general version is left for future work.
Acknowledgments
The authors acknowledge support from the Henry MacCracken Fellowship, NSF RI-1816753, NSF
CAREER CIF 1845360, NSF CHS-1901091 and NSF DMS-MoDL 2134216.
9
References
[1]	Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs
deep learning. arXiv preprint arXiv:2001.04413, 2020.
[2]	Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparame-
terized neural networks, going beyond two layers. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019.
[3]	Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning, pages 242-252,
2019.
[4]	Dyego Araujo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep
neural networks. arXiv preprint arXiv:1906.00193, 2019.
[5]	Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584, 2019.
[6]	Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal
of Machine Learning Research, 18(1):629-681, 2017.
[7]	Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation
of wide neural networks. In International Conference on Learning Representations, 2020.
[8]	Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initializa-
tion efficiently learns positive definite linear transformations by deep residual networks. In
International conference on machine learning, pages 521-530. PMLR, 2018.
[9]	Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers
in the overparameterized regime. arXiv preprint arXiv:2004.12019, 2020.
[10]	Niladri S Chatterji, Philip M Long, and Peter L Bartlett. When does gradient descent with
logistic loss interpolate using deep networks with smoothed relu activations? arXiv preprint
arXiv:2102.04998, 2021.
[11]	Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard
Socher. Towards understanding hierarchical learning: Benefits of neural representations. arXiv
preprint arXiv:2006.13436, 2020.
[12]	Zhengdao Chen, Grant Rotskoff, Joan Bruna, and Eric Vanden-Eijnden. A dynamical central
limit theorem for two-layer neural networks. Advances in Neural Information Processing
Systems, 33, 2020.
[13]	Lenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent.
arXiv preprint arXiv:1907.10300, 2019.
[14]	Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Processing
Systems, pages 3036-3046, 2018.
[15]	Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural
networks trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
[16]	Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. In Advances in Neural Information Processing Systems, pages 2937-2947, 2019.
[17]	Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
In International Conference on Machine Learning, pages 1655-1664. PMLR, 2019.
10
[18]	Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of
Proceedings ofMachine Learning Research, pages 1675-1685. PMLR, 09-15 JUn 2019.
[19]	Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neUral networks. arXiv preprint arXiv:1811.03804, 2018.
[20]	Ethan Dyer and GUy GUr-Ari. Asymptotics of wide networks from feynman diagrams. In
International Conference on Learning Representations, 2020.
[21]	Weinan E, Chao Ma, and Lei WU. Machine learning from a continUoUs viewpoint, i. Science
China Mathematics, 63(11):2233-2266, 2020.
[22]	Weinan E and Stephan Wojtowytsch. On the banach spaces associated with mUlti-layer relU net-
works: FUnction representation, approximation theory and gradient descent dynamics. CSIAM
Transactions on Applied Mathematics, 1(3):387-440, 2020.
[23]	Cong Fang, Jason D Lee, PengkUn Yang, and Tong Zhang. Modeling from featUres: a mean-field
framework for over-parameterized deep neUral networks. arXiv preprint arXiv:2007.01452,
2020.
[24]	C Daniel Freeman and Joan BrUna. Topology and geometry of half-rectified network optimiza-
tion. arXiv preprint arXiv:1611.01540, 2016.
[25]	Spencer Frei and QUanqUan GU. Proxy convexity: A Unified framework for the analysis of
neUral networks trained by gradient descent. arXiv preprint arXiv:2106.13792, 2021.
[26]	Mario Geiger, Leonardo Petrini, and MatthieU Wyart. Landscape and training regimes in deep
learning. Physics Reports, 2021.
[27]	Mario Geiger, Stefano Spigler, ArthUr Jacot, and MatthieU Wyart. Disentangling featUre and
lazy learning in deep neUral networks: an empirical stUdy. arXiv preprint arXiv:1906.08034,
2019.
[28]	Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of
lazy training of two-layers neUral network. In Advances in Neural Information Processing
Systems, pages 9111-9121, 2019.
[29]	Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neUral
networks oUtperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.
[30]	Xavier Glorot and YoshUa Bengio. Understanding the difficUlty of training deep feedforward
neUral networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volUme 9 of Proceedings of
Machine Learning Research, pages 249-256, Chia LagUna Resort, Sardinia, Italy, 13-15 May
2010. PMLR.
[31]	Sebastian Goldt, MadhU Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova.
Dynamics of stochastic gradient descent for two-layer neUral networks in the teacher-stUdent
setUp. In Advances in Neural Information Processing Systems, pages 6979-6989, 2019.
[32]	Anna GolUbeva, GUy GUr-Ari, and Behnam NeyshabUr. Are wider nets better given the same
nUmber of parameters? In International Conference on Learning Representations, 2021.
[33]	Boris Hanin and Mihai Nica. Finite depth and width corrections to the neUral tangent kernel. In
International Conference on Learning Representations, 2020.
[34]	Kaitong HU, Zhenjie Ren, David Siska, and LUkasz SzprUch. Mean-field langevin dynamics
and energy landscape of neUral networks. arXiv preprint arXiv:1905.07769, 2019.
[35]	Jiaoyang HUang and Horng-Tzer YaU. Dynamics of deep neUral networks and neUral tangent
hierarchy. arXiv preprint arXiv:1909.08156, 2019.
11
[36]	Jean-Francois Jabir, David Siska, and Eukasz Szpruch. Mean-field neural odes Via relaxed
optimal control. arXiv preprint arXiv:1912.05475, 2019.
[37]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571-8580, 2018.
[38]	Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural
network via displacement convexity. The Annals of Statistics, 48(6):3619-3642, 2020.
[39]	Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110,
2016.
[40]	Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165, 2017.
[41]	Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman
Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
2020.
[42]	Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
[43]	Yuanzhi Li, Tengyu Ma, and Hongyang R. Zhang. Learning over-parametrized two-layer
neural networks beyond ntk. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of
Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning
Research, pages 2613-2682. PMLR, 09-12 Jul 2020.
[44]	Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-
parameterized non-linear systems and neural networks. arXiv preprint arXiv:2003.00307,
2020.
[45]	Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les
equations aux deriveespartielles, 117(87-89):2, 1963.
[46]	Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean-field analysis of deep
resnet and beyond: Towards provable optimization via overparameterization from depth. arXiv
preprint arXiv:2003.05508, 2020.
[47]	Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu
neural networks at infinite-width limit. Journal of Machine Learning Research, 22(71):1-47,
2021.
[48]	Chao Ma, Lei Wu, et al. The barron space and the flow-induced function spaces for neural
network models. Constructive Approximation, 2021.
[49]	Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers
neural networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015,
2019.
[50]	Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-
E7671, 2018.
[51]	Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks.
arXiv preprint arXiv:1902.02880, 2019.
[52]	Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of
multilayer neural networks. arXiv preprint arXiv:2001.11443, 2020.
12
[53]	Atsushi Nitanda, Denny Wu, and Taiji Suzuki. Particle dual averaging: Optimization of mean
field neural networks with global convergence rate analysis. arXiv preprint arXiv:2012.15477,
2020.
[54]	Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient
descent takes the shortest path? In International Conference on Machine Learning, pages
4951-4960. PMLR, 2019.
[55]	Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global
convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas
in Information Theory, 1(1):84-105, 2020.
[56]	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
[57]	Huy Tuan Pham and Phan-Minh Nguyen. Global convergence of three-layer neural networks in
the mean field regime. ICLR, 2021.
[58]	Boris T. Polyak. Gradient methods for the minimisation of functionals. Ussr Computational
Mathematics and Mathematical Physics, 3:864-878, 1963.
[59]	Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time
convergence and asymptotic error scaling of neural networks. In Advances in Neural Information
Processing Systems, pages 7146-7155, 2018.
[60]	Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural
networks. In International Conference on Machine Learning, pages 4433-4441, 2018.
[61]	Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborova. Optimization and gen-
eralization of shallow neural networks with quadratic activation functions. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 13445-13455. Curran Associates, Inc., 2020.
[62]	Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks.
arXiv preprint arXiv:1903.04440, 2019.
[63]	Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law
of large numbers. SIAM Journal on Applied Mathematics, 80(2):725-752, 2020.
[64]	Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the op-
timization landscape of over-parameterized shallow neural networks. IEEE Transactions on
Information Theory, 65(2):742-769, 2018.
[65]	Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error
guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
[66]	Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network
and its applications in convergence and critical point analysis. In Doina Precup and Yee Whye
Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70
of Proceedings of Machine Learning Research, pages 3404-3413. PMLR, 06-11 Aug 2017.
[67]	Luca Venturi, Afonso S. Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural
network optimization landscapes. Journal of Machine Learning Research, 20(133):1-34, 2019.
[68]	Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press, 2018.
[69]	Colin Wei, Jason D. Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization
and optimization of neural nets vs their induced kernel. In Advances in Neural Information
Processing Systems, pages 9709-9721, 2019.
13
[70]	Stephan Wojtowytsch. On the convergence of gradient descent training for two-layer relu-
networks in the mean field regime. arXiv preprint arXiv:2005.13530, 2020.
[71]	Stephan Wojtowytsch and E Weinan. Can shallow neural networks beat the curse of dimen-
sionality? a mean field training perspective. IEEE Transactions on Artificial Intelligence,
1(2):121-129, 2020.
[72]	Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.
arXiv preprint arXiv:2002.09277, 2020.
[73]	Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural
networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pages 11727-11737. PMLR, 18-24 Jul 2021.
[74]	Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized
two-layer neural network. arXiv preprint arXiv:2102.02410, 2021.
[75]	Zhihui Zhu, Daniel Soudry, Yonina C Eldar, and Michael B Wakin. The global optimization
geometry of shallow linear neural networks. Journal of Mathematical Imaging and Vision,
62(3):279-292, 2020.
[76]	Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.
14
A Additional notations
•	For a positive integer n, we let [n] denote the set {1, ..., n}.
•	We use i, j (as subscripts) to index the neurons in the hidden layers, a, b (as subscripts or
superscripts) to index different training data points, t (as a superscript) to denote the training
time / time parameter in gradient flow.
•	WeWrite Pa for 1 Pa=ι.
•	We use bold letters (e.g. x, z, c, y) to denote vectors.
•	We use W and {Wij}i∈[m],j∈[D] interchangeably to refer to the same set of parameters.
B Consistency of the scaling and GD update rule with Xavier
INITIALIZATION
Consider a three-layer netWork defined by
m
f (x) = X θ(3)σ(hi(x))	(25)
i=1
md
∀i ∈ H :	hi(X)= X θ(2)σ (√= X θjk)Xk)	(26)
j=1	d k=1
with weight parameters {θjk)}7-,k∈[m], {θ(2)h,j∈[m] and {θ(3) h∈[m] are initialized according to
Xavier initialization, which means that we sample each θj? i.i.d. from N(0, m+d), each θj i.i.d.
from N(0, 2m), and each θ(3) i.i.d. from N(0, m+ι). If m》d, both N(0, m+d) and N(0, m+ι)
can be approximated by N(0, m1). Then, UP to this approximation, by redefining Ci = √mθ(3),
Wij = √mθ(2) and Zjk = √mθjk), we can write
∀i ∈ [m]
1m
f(x) = √m∑Sciσ(hi (X)),
i=1
1D
hi(X) = √m EWijσ
j=1
(27)
(28)
and note that ci, Wij and zjk are all initialized i.i.d. of order O(1). In addition, if σ is homogeneoUs,
this is then eqUivalent to (4) and (5) when D = m.
Moreover, there is ∂Wf-(x) = √m-df (x). Then, since performing GD on θj with step size δ
j	∂ θij
means Updating θi(j2) according to
一j δ∂f
(29)
this is eqUivalent to Updating Wij according to
WijWm 陪-δ^
=Wij- mδ∂Lf(X),
∂Wij
which jUstifies the m factor on the right-hand-side of (9).
(30)
15
C Relationship to the maximum-update parameterization and
FEATURE LEARNING
Consider the partially-trained L-layer NN model defined in Section H in the case where D = m	d.
In the framework of abc-parameterization introduced in [73], our model corresponds to setting
a1 = 0, a2
aL = 2, aL+1 = 1
bi =0, ∀l ∈ [L+ 1]
Furthermore, as we explain in Appendix B, the appropriate learning rate scales linearly with m (as in
(9)), which corresponds to having
C= -1	(31)
Meanwhile, the maximum-update (μP) parameterization [73] is characterized by setting	
a1 = -2, a2 =…=aL = 0, aL+1 = 2	(32)
bi =1, ∀l ∈ [L +1]	(33)
C =0	(34)
Recall the symmetry of abc-parameterization derived in [73], which states that one gets a different
but equivalent abc-parameterization by setting
a，i《—a，i + θ, bi《—bi — θ, C《—C — 2θ
(35)
Since our parameterization can be obtained from the maximum-update parameterization by applying
the transformation above with θ = 1, they are equivalent in the function space. In particular, for our
parameterization, the r parameter defined in [73] can be computed as
r = min{bL+1, aL+1 + C} + aL+1 + C+ min {2ai — 1i6=1}
i=1,...,L
=min{0,1 + ( — 1)} + 1 + (―1) + min{2 ∙ 0 — 0, 2 • 2 — 1}
(36)
=0
Hence, according to [73], our parameterization exhibits feature learning.
D Proof of Lemma 1
Since We assume that G is positive definite and |ci| = C > 0, ∀i ∈ [m], we can derive from (14) that
Lt
2m n
=-m χχ (ft (Xa)- ya) (ft(Xb) - yb) σ0(ht(Xa))σ0(ht(Xa))Gab
i=1 a,b=1
mn
≤ - ^2λmin(G)m XX (ft(Xa) - ya)2 (σ0(hi(Xa)))2
i=1 a=1
nm
=-^2λmin(G) X (ft(Xa) - ya)2 m X (σ0 (hi (Xa)))2
a=1	i=1
nm
≤ - ^2λmin(G) X (ft(Xa) - ya)2 m X 以出金(Kσ"2
(37)
a=1
i=1
nm
≤ - "min(G)(KσO)2 X (f t (Xa)- ya) m再 ∖ — X 1ht(xb)∈I (
b∈[n]	m
a=1	i=1
≤ - 222λmin(G)(Kσ0 )2 Lt m∈in	m X 1ht (χa)∈I^
16
Since I is an open interval, ∃ξ > 0 such that we can find a subinterval Io ⊆ I such that the distance
between Io and the boundaries of I (if I is bounded on either side) is no less than ξ, i.e.,
inf |u - u0∣ ≥ ξ	(38)
u∈Io ,u0 ∈R∖I
In particular, we can choose ξ = ɪ(Ir - Iι) and Io = (Il + ξ,Ir - ξ). Then there is
1%i(Xα)∈I ≥1h0(Xα)∈l0 , ht(Xα)∈I
≥1h0(xα)∈l0 , ∖hti(xa)-h0(xa)∖<ξ	(39)
≥1h0(Xα)∈I0 - 1∖ht(Xα)-h0(Xα )∖≥ξ
and so
fι m [	1 1 m ]	1ι m
minjm 工 Iht(Xa)∈i] ≥ m[n∕m 工 1那(Xa)∈I° j- m^j m 工 1期(Xa)-h0(Xa)∖≥ξ
)
Thus, we have
(40)
Lt ≤ -2c2λmin(G)(Kσ,)2 L
Meanwhile, since
(min[^ X 1h0(Xa)∈J- mn {m X1 ∖ ht(Xa)-h0(Xa) ∖ ≥J)
-	-	(41)
n
就(Xa) = -Ci E (f'(Xb)- yb) σ'[hi(xb)')Gab ,	(42)
b=1
there is
Im	Imn
mXIht(Xa)I ≤cmX X(ft(XbIyb)Mht(Xb))Gab
i=1	i=1 b=1
≤C
2	1
X (ft(Xb)-yb) σ'(hi(Xb))GabI )
Cimn	\ 2
m XXi (产(Xb)- yb) σ(ht(Xb))Gab 1 2
i=1 b=1	)
1
Clmn	\ 2
m XXft(Xb)-yb『(σ'(ht(Xb)))2
i=1 b=1	)
(43)
Therefore,
Since ∀ξ ∈ R, there is
ʌ	(	I Lt I	ʌ2
≤Cλmax(G)	/^2 λ	TGT
I (C) λmin (G) /
≤λmax(G) (λmin (G))-2 | LT
tm
m X|hS(Xa)IdS
i=1
≤λmax(G) (λmin (G))-2	Q ds
(44)
ξ ∙ 1∖ht(Xa)-h0(Xa)∖≥ξ ≤ 1 ht(Xa)- h0(Xa)
(45)
17
we derive that, ∀a ∈ [n],
1 m	ι m
m X 1∣hi(xa)-h0(xa)∣≥ξ ≤ξ-1 - XIht(Xa) - h0(Xa)I
i=1	i=1
≤C1∕] LSF ds,
where we set Ci = λmax(G) (λmin (G))- 2 ξ-1 > 0 for simplicity. As a consequence,
1 1 A	I ft 2 ;c I 2 T
* I- Σ 1∣ht(…0(xa )∣≥ξj ≤ C1 J0 l LIdS
(46)
(47)
Define
"miʤXW…。}-CIJ'o l lsl2ds	(48)
Note that at t = 0, there is 计=mi∏a∈[n] {+ PzI 1h0(xa)∈ι0 }∙ Then, on one hand, we know
from (40) and (47) that ∀t ≥ 0,
1 ∣ hi (xa)-h0(xa) ∣ ≥ξ} ≥ nt ,	(49)
1 1 m	]	1 1 m
ηt ≥ min ∣m 工 Jemq- s I -二
Hence, (41) implies that
Lt ≤-2^2λmin(G) (Kσ )2 Ltnt
≤-2^2λmin(G) (Kσ )2 Ltnt
(50)
On the other hand, by the definition of nt,
nt=-C1∣ l t ∣ 2
1
2
(51)
-1
≥C1Lt (2c2λmin(G)(Kσ,)2 Ltnt) 2
≥C2(^)-1Lt (Lt)-2 (nt)-2,
≥-C11 L t∣∙∣Lt ∣
where we set C2 = 2-2 C1 Cmin(G))-2 (Kσ0)T = √¾KTλR(G) ≤ √⅞K⅛ιl(G) for SimPIiC-
ity. Therefore, when ηt > 0,
d fl (ηt) 3) = (ηt)2 ηt ≥ C2(^)TLt (Lt)-2 ≥ C2(c)-1 d (2Lt)1 ,
dt ∖ 3	I	dt
which implies that
2 (ηt)2 - 3 (η0)3 ≥ C2(c)-1 ((2Lt)1 - (2L0)1) ≥ -C2(c)-1 (2L0)1
and so ∀t ≥ 0,
2	3	2	3	2	3	.ci
2 (ηt)2 ≥ 3 (ηt)2 ≥ 2 (η0)2 - C2(^)-1(2L0)2
(52)
(53)
(54)
E Proof of Theorem 1
To apply Lemma 1, we need two additional lemmas, which we will prove in Appendix E.1 and
E.2. The first one guarantees that the loss value at initialization, L0, is upper-bounded with high
probability:
18
Lemma2. ∀δ > 0,if m ≥ Ω (^2 log (nδ-1) Gmaχ∕ky∣∣2), then with probability at least 1 一 δ, there
is
L0 ≤ kyk2
(55)
The second one proves that η0 is lower-bounded with high probability, which heuristically Says that
there is indeed a nontrivial proportion of neurons in the central part of the active region of σ, for
every a ∈ [n]:
Lemma 3. ∀δ > 0, if m ≥	Jθg(nδ )-, then with probability at least 1 一 δ, there is
2(K(I,Gmi
n,Gmax))
η0 > K(I, Gmin, Gmax) ,
(56)
where K (I, λ1,λ2) = 6√1∏λ (Ir — Iι)exp {一 max{λl)2llrl} } is a positive number that depends on
I, λ1 and λ2.
With these two lemmas, we deduce that ∀δ > 0, if m ≥ Ω ((1 + ^2∕∣∣y∣∣2) log (nδ-1)), then with
probability at least δ, there is ∀t ≥ 0,
2 W) 3 ≥ 2 (K(I,Gmin,Gmax)) 3 - √2C? (C)-1 k y k ,	(57)
where K (I, Gmin, GmaX) is defined as in Lemma 3. Therefore, if our choice of C satisfies
3√2C2kyk
(K(I, Gmin, GmaX))
then there is ∀t ≥ 0,
in which case (50) gives
ηt ≥ 2-3 K (I, Gmin, Gmax) > 0 ,
Lt ≤ —21 ^2λmin(G) (Kσ0 )2 LK(I, Gmin, Gmax),
which will allow us to finally conclude that
Lt ≤ L0 exp {-21 λmin(G) (Kσ0 )2 K (I, Gmin, Gmax)^2t}
(58)
(59)
(60)
(61)
Note that (60) establishes a PL condition. Several other convergence analyses of NNs have also relied
on variants of the PL condition [25, 44, 74].
E.1 Proof of Lemma 2
Proof. Since at initialization, Ci i∈[m] and Wi0j i∈[m],j∈[D] are both sampled i.i.d. and Ci i∈[m]
has mean zero, we know that ∀a ∈ [n], f 0(xa) = ml Pm=1 ασ(ht(xa)) is the sample mean of i.i.d.
random variables with zero-mean. Moreover, since Wi0j i∈[m],j∈[D] is sampled from N(0, 1), we
know that ∀i ∈ [m], the random variable Ciσ hit(xa) is sub-Gaussian [68], with sub-Gaussian norm
kciσ(ht(Xa)) kψ2 ≤c∣∣σ(ht(Xa))I∣ψ2
≤^Lσ (Gaa) 1 MSG	(62)
≤^Lσ (GmaX) 2 MSG ,
where MSG > 0 is some absolute constant. Thus, by Hoeffding’s inequality [68], ∀a ∈ [n], ∀r > 0,
P (IfO(Xa)∣≥ u) =P(Im∙ X Ciσ(hi (Xa))
≥u
≤2 exp 一
Ku2 m
kciσ(ht(Xa)) llψ2
(63)
ʌ 、
c ≥
)
≤2 exp
Ku2 m
t-^2(Lσ )2GmaX(MSG)2
19
where K is some absolute constant. Hence, by union bound,
P Xn f0(xa)2 ≥ kyk2
a=1
n
≤ X P (f0(χa)∣≥kyk)
a=1
J______K kyk2m
I ^2(Lσ )2Gmax(MSG)2
(64)
Thus, ∀δ > 0, if
m ≥ ^2(Lσ)2GmaxKT(MSG)2|加「2 log (2^
(65)
then with probability at least 1 - δ, there is
n
L =2 X f 0 (Xa)-ya|
a=1
n
≤ 2 Xf0 (Xa)I2 + 1 kyk2
a=1
≤kyk2
(66)
E.2 Proof of Lemma 3
Since each Wi0j are sampled i.i.d. from N (0, 1), we know that ∀a ∈ [n], independently for each
i ∈ [m], hi0(Xa) follows a Gaussian distribution with mean 0 and variance Gaa. Therefore,
m
Elh0(χa)∈io 〜Binomial (m, 1 - ∏ (I0； Gaa)),
i=1
Hence, by Hoeffding’s inequality, ∀a ∈ [n], ∀r > 0,
P ∖~ X 1h0(xa)∈l0 ≥ 1-π(I0; Gaa )+ r ) ≤ exP {-2mr2}
i=1
∀a ∈ [n], choosing r = ɪπ (I0; Gaa), We then get
(67)
(68)
1m	1	1m	1
P ( m E 1h0(xa)∈l0 ≤ 2π (IO； Gaa)J =P (m E 1h0(xa)∕l0 ≥ 1 - 2π (I0； Gaa)
≤ exp {- 1 m (∏ (IO； Gaa))2 }
≤ exp {- 1 m (min{∏ (IO； Gbb)}) }
(69)
and so by union bound
P (η0	≤ 1 min {π	(I0； Gbb)})	=P (minι	]—X 1h0(χa)∈io	∖ < 1 min {π (I0； Gbb)}
2 b∈[n]	a∈[n] m i=1 i a 0	2 b∈[n]
nm
≤ XPX 1h0
a=1
i=1
(Xa)∈l0 < lmin {n (I0； Gbb)}
2 b∈[n]
n 1m	1
≤ X P I m X 1h0(xa)∈Io < 2π (I0; Gaaa)
(70)
a=1
≤n exp -
i=1
1	m ( min {∏ (I0； Gbb)})
2	b∈[n]
□
20
Since ∀b ∈ [n], there is Gmin ≤ Gbb ≤ G
max,
1	Ir--ξ _ U
:	e Gbb du
2πGbb Il+ξ
≥ , ( - (Ir — Il — 2ξ)	inf	exp — - u—
√2∏Gbb	rl+ξ≤u≤rr-ξ	G Gbb
≥	1	(I _ i _ 2ξ)exn ʃ_max{lIll,lIr|}
≥√2∏Gmax(r	l ξ) pι	(Gmm)2
1	max{|Il|, |Ir|}
≥ 3√2∏Gmaχ(r- l)exp[	(Gmm)2	J
Letting K(I,λ1,λ2) = 6√=^(Ir - Il) exp {一 maxjjIl)/1}} > 0,we Can then write
P (η0 ≤ K(I, Gmin, GmaX)) ≤P (% ≤ 1 min {∏ (I0； Gbb)})
2 b∈[n]
∏ (Io; Gbb)
(71)
≤n exp -2m (K(I, Gmin, Gmax))2
(72)

	
	
Thus, ∀δ > 0, if m ≥ “力/^⑺ /—e, then with probability at least 1 - δ, it holds that
2(K(I,Gmin,G
max))
η0 > K(I,Gmin, GmaX) > 0.
F	Proof of Theorem 2
By Condition 1, we know that ∀δ > 0, if D ≥ Dmin( 11 δ, ɪλmin(G)), then with probability at
least 1 - 1 δ, there is ∣∣G - G∣∣1 ≤ 2λmin(GG), and hence λmin(G) ≥2λmin(G), Gmin ≥ 1 Gmin,
λmax(G) ≤ λmax(G) + 2λmin(G) ≤ 2λmax(G), and Gmax ≤ 2Gmax∙ ^We then Perform the
following analysis conditioned on the event that ∣G - G∣1 ≤ 2λmin(G).
Since the sampling of ci i∈[m] and Wi0j i∈[m],j∈[D] is independent from the realization of G,
we know from Lemma 3 that if m ≥ -7----- log(4nδ__)--≥	lθg(4nδ/	2, then with
一 2(K(I, 2 λmin(G),2λmaχ(G)))2 — 2(K (I,Gmin,Gmax)) 2 ,
probability at least 1 - 1 δ, there is
η0 > K(I,Gmin, GmaX) ≥ K(I) 2Gmin, 2Gmax)	(73)
From Lemma 2, we also know that if m ≥ Ω (^2 log (nδ-1) λmax(G)∕ky∣2) ≥
Ω (^2 log (nδ-1) λmax(G)∕ky∣2),then with probability at least 1 - 1 δ, there is L0 ≤ ∣∣yk2. There-
fore, in total, we know that with probability at least 1 - δ, the following conditions all hold:
∣G - G∣2 ≤ 1 λmin(G) ,	(74)
η0 ≥K(I, 2Gmin, 2Gmax) ,	(75)
L0 ≤∣y∣2 ,	(76)
in which case, by applying Lemma 1 with G = G(1), we get
(ηt) 2 ≥ (η0) 2 - Kl(λmin (G) ,λmax(G))(C)T ((L。)1 - (Lt)1) ,	(77)
where K1(λ1, λ1) = 9λ-1λ2KJ01(Ir - Il) > 0. Thus, by the definition of Kι(∙, ∙), we know that
K1(λmin (G) , λmax (G)) ≤ 4K1(λmin (G) , λmax (G)) ,	(78)
and so ∀t ≥ 0,
(ηt) 3 ≥(η0) 3 - 4Kl(λmin (G) ,λmax(G))(^)T ((L0) 2 - (Lt) 1 )
≥ (K(I, 1 Gmin, 2Gmax)) 3 - 4Kι(λmin (G) , λmax (G) )(^)-1 ∣y ∣
21
Therefore, if our choice of C satisfies
ʌ 、
c ≥
8Kl(λmin (G) ,λmaχ (G))∣∣y∣∣
I —	—	、、3
(K(I 1 Gmin, 2Gmax))2
then there is ∀t ≥ 0,
21
η ≥ 2 3 K(I) $Gmin, 2GmaX)
Hence, (50) implies that ∀t ≥ 0,
L t ≤ - 2^2λmin(G)(Kσ0 )2 Lt 2-3 K (I, | Gmm, 2Gmaχ )
≤- 2-3 ^2λmin(G)(Kσ0 )2 LtK (I, | Gmm, 2Gmaχ)
and therefore
Lt ≤ L0 exp {-2-3 ^2λmin(G)(Kσθ)2K (I, 2 Gmin, 2G max)
(80)
(81)
(82)
(83)
G Proof of Theorem 3
In view of Theorem 2, it is sufficient to verify that Condition 1 holds for Dmin(δ, u) =
Ω (n2u-2 log(nδ-1)), which is given by the following lemma:
Lemma 4. ∀δ ≥ 0, if D ≥ Ω (n2u-2 log(nδ-1)) ,then with probability at least 1 一 δ,
kG⑴一G(I)∣∣2 ≤ u	(84)
G.1 Proof of Lemma 4
Let Z be a random vector on Rd with law given by ∏z, and then we can write G)? =
E [σ(x∣Z)σ(x∣Z)] for a, b ∈ [n]. By assumption, Z is sub-gaussian with sub-gaussian norm
∣∣Zkψ2 ：= sup ∣∣xlZkψ2 < ∞	(85)
x∈Sd-1
Thus, ∀a ∈ [n], we have
kσ(x∣Z)∣∣ψ2 ≤ LσIlxTZ∣ψ2 ≤ Lσ∣Zkψ2	(86)
Hence, by Lemma 2.7.7 in [68], we know that ∀a, b ∈ [n], σ(x∣Z)σ(x∣Z) is a sub-exponential
random variable with sub-exponential norm
ι∣σ(xιZ )σ(xlZ )kψι ≤ ι∣σ(xaZ)辰 ι∣σ(xlZ )kψ2 ≤ (Lσ )2kZ kψ2	(87)
Then, by Bernstein’s inequality (Theorem 2.8.1 in [68]), since each zj is sampled i.i.d. from πz, we
have that ∀a, b ∈ [n] and ∀u > 0,
1
D X σ(x∣zj)σ(x∣zj) - E [σ(x∣Z)σ(x∣Z)]
≥u
≤2 exp
ʃ κ .
-K min
u2 D	uD
l∣σ(xlZ)σ(xlZ)kψ1, kσ(xlZ)σ(xlZ)l∣ψι
))
(
u2D	uD
≤ P -K	j(Lσ )4kZkψ2 , (Lσ )2kZkψ2 ∫ 卜
(88)
where K > 0 is some absolute constant. In other words, for any δ0 > 0, if
D ≥ max ( (LL)4kZ*g(M)T), (""EgO)T)),勒
22
then We have IGab) - G!? ∣ ≥ U With probability at least 1 - δ. If We choose U = U and δ0 =今,
then we get, if
}
then
D ≥ max
^ I
n2(Lσ)4kZk4ψ2 log(2n2δ-1) n(Lσ)2kZk2ψ2 log(2n2δ-1)
K(U0)2
KU0
(90)
P(kG⑴-G⑴ kF ≥ (u0)2)
n
≤XP
a,b=1
	
U0
≥ 一
n
≤n2
≤δ
δ
n2
(91)
Hence, With probability at least 1 - δ, We have
kG(I)- G(I)k2 ≤ kG⑴
-G⑴kF ≤ (u0)2 ,
(92)
H Generalization to deeper models
By setting Φ to be the activations of the second-to-last hidden-layer of a multi-layer NN, We can
obtain generalizations of the P-3L NN to deeper architectures. For example, in the feed-forWard case,
We can obtain the folloWing partially-trained L-layer NN :
1m
f (X) = m X ciσ(hi	(X)),
i=1
∀i ∈ [m]	:	hi(L-1)(X)	1D =TD X Wijσ(hjL-2)(x)) D j=1
∀l ∈ [L - 3], ∀i ∈ [D]	:	hi(l+1)(X)	D =√D X Wi*σ(hj')(X))， j=1
∀j ∈ [D]	:	h(j1)(X)	1 | =√d ZjX，
where WW(I),…，WW(L-3) ∈ RD×D and zι,…，ZD ∈ Rd are sampled randomly and fixed. This model
can be written in the form of (4) and (5) with φj(x) = σ(hjL-2)(x)). The corresponding Gram
matrix is recursively defined and also appears in the NTK analysis [18]. In particular, the results
in [18] imply that if σ is analytic and not a polynomial, then Condition 1 holds, and hence similar
global convergence results can be obtained as corollaries of Theorem 2.
I	Further details of the numerical experiments
In our models, {ci}i∈[m] is sampled i.i.d. from the Rademacher distribution μc = ɪδι + 2δ-ι,
zj j∈[D] is sampled i.i.d. from N (0, Id), and	Wij i∈[m],j∈[D] is initialized by sampling i.i.d.
from N (0, 1). In the model under NTK scaling, we additionally symmetrize the model at initialization
according to the strategy used in [16] to ensure that the function value at initialization does not blow
up when the width is large. We choose to train the models using 50000 steps of (full-batch) GD with
step size δ = 1. When the test error is computed, we use a test set of size 500 generated by sampling
i.i.d. from the same distribution as the training set.
The experiments are run with NVIDIA GPUs (1080ti and Titan RTX).
23
Figure 5: Training loss v.s. number of GD steps for different n in Experiment 1 with m = 4096.
I.1	EXPERIMENT 1
We choose σ to be tanh. For each choice of n, we run the experiment with 5 different random
seeds, and Figure 1 plots the evolution of the training loss during GD averaged over the 5 runs with
m = 8192.
Figure 5 is the same as Figure 1 except for having m = 4096. We see that the two two plots agree
well.
I.2	EXPERIMENT 2
We choose σ to be ReLU. For each choice of n and each of the two models, we experiment with 5
different random seeds, and Figure 2 plots the test error at the 50000 GD step averaged over the 5
runs ± its standard deviation.
In Figure 6, we plot the evolution of the training loss and test error during GD for the two different
models, with m = 2048 or 8192 and different choices of n, averaged over 5 runs with different
random seeds. We see in particular that the difference between the two choices of m is negligible,
suggesting that it is unlikely to obtain performance improvements with further over-parameterization.
I.3	EXPERIMENT 3
We choose σ to be ReLU and input dimension d = 50. We use a training set of size n = 600 for the
results reported in Figure 4. The data set is inspired by [69]: We sample both the training and the test
set i.i.d. from the distribution (x, y)〜D on Rd+1, under which thejoint distribution of (x1,x2, y)
is
P(x	1 = 1, x2 = 0, y	=1)	1 =— 4	(93)
P(x1	= -1, x2 = 0, y	=1)	1 =— 4	(94)
P(x1	= 0, x2 = 1, y =	-1)	1 =— 4	(95)
P(x1 =	0, x2 = -1, y =	-1)	1 =— 4	(96) (97)
and x3, ..., xd each follow the uniform distribution in [-1, 1], independently from each other as well
as x1 , x2 and y.
Figures 7 and 8 are the same as Figure 4 except for having n = 400 and 800, respectively. We see
that as n increases, test error improves for all three models, while our P-3L NN model remains the
one achieving the lowest test error.
24
n=50; Shallow acc
1.2
1.0
0.8
0.6
0.4
0.2
0.0
-0.2
0	2500	5000	7500	10000
1.2
1.0
0.8
0.6
0.4
0.2
0.0
-0.2
0	2500	5000	7500	10000

m=2048
m=8192
n=100; Shallow acc

m=2048
m=8192
n=50; Shallow+Quad		
1.2 1.0 ■ 0.8 ■ 0.6 ■ 0.4 ■ 0.2 ■ 0.0 ■		m=2048	
		m=8192
		
		
—0.2 I	I	I	I	I- 0	2500	5000	7500	10000 n=100; Shallow+Quad		
1.2 1.0 ■ 0.8 ■ 0.6 ■ 0.4 ■ 0.2 ■ 0.0 ■			m=2048 	m=8192
		k	
		
-0.2  1	1	1	1	1- 0	2500	5000	7500	10000		
		n=150; Shallow+Quad
1.2 1.0 ■			m=2048 	m=8192
0.8 ■		∖
0.6 ■		
0.4 ■		
0.2 ■ 0.0 ■		I	
		
		
-0.2」		2500	5000	7500	10000
n=200; Shallow+Quad	
1.2	
1.0 ■		m=2048 	m=8192
0.8 ■	
	∖ 、.	
0.6 ■	
0.4 ■	
0.2 ■	
	k	
0.0 ■ -0.2 -	
0	2500	5000	7500	10000
1.2
1.0
0.8
0.6
0.4
0.2
0.0
-0.2
n=250; Shallow+Quad
m=2048
m=8192
0	2500	5000	7500	10000
Figure 6: Test error v.s. GD steps in Experiment 2 for the two models and difference choices of n.
25
P-3L NN
P-3L NN (NTK)
P-3L NN (MF)
-6 -4 -2 0	2	4	6
hi(x1)
-6 -4 -2 0	2	4	6
hi(x1)
P-3L NN
Figure 7: Same as Figure 4 except for setting n = 400.
-6 -4 -2 0	2	4	6
hi(x1)
Figure 8: Same as Figure 4 except for setting n = 800.
26