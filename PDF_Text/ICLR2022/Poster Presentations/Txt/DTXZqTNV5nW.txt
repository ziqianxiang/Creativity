Published as a conference paper at ICLR 2022
Actor-Critic Policy Optimization in a Large-
Scale Imperfect-Information Game
Haobo Fu,1*Weiming Liu,2*'shuang Wu,1 Yijia Wang,3' Tao Yang,1 Kai Li,45 JUnliang Xing,45 Bin
Li,2 Bo Ma,1 Qiang Fu,1 and Wei Yang1
1	Tencent AI Lab, shenzhen, China
2	University of science and Technology of China, Hefei, China
3	Peking University, Beijing, China
4	Institute of Automation, Chinese Academy of sciences, Beijing, China
5	school of Artificial Intelligence, University of Chinese Academy of sciences, Beijing, China
Ab stract
The deep policy gradient method has demonstrated promising results in many large-
scale games, where the agent learns purely from its own experience. Yet, policy
gradient methods with self-play suffer convergence problems to a Nash Equilibrium
(NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a
convergence guarantee to a NE in 2-player zero-sum games, but it usually needs
domain-specific abstractions to deal with large-scale games. Inheriting merits from
both methods, in this paper we extend the actor-critic algorithm framework in
deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-
information game, 1-on-1 Mahjong, whose information set size and game length
are much larger than poker. The proposed algorithm, named Actor-Critic Hedge
(ACH), modifies the policy optimization objective from originally maximizing the
discounted returns to minimizing a type of weighted cumulative counterfactual
regret. This modification is achieved by approximating the regret via a deep neural
network and minimizing the regret via generating self-play policies using Hedge.
ACH is theoretically justified as it is derived from a neural-based weighted CFR, for
which we prove the convergence to a NE under certain conditions. Experimental
results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the
literature demonstrate that ACH outperforms related state-of-the-art methods. Also,
the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong.
1 Introduction
Policy gradient methods using deep neural networks as policy and value approximators have been
successfully applied to many large-scale games (Berner et al., 2019; Vinyals et al., 2019; Ye et al.,
2020). Usually, a score function representing the discounted returns is maximized by the policy, i.e.,
the actor. In the meantime, a value function, known as the critic, is learned to guide the directions
and magnitudes of the policy gradients. This type of actor-critic methods are efficiently scalable
with regard to the game size and the amount of computational resources. However, as pointed out
in srinivasan et al. (2018) and Hennes et al. (2020), policy gradient methods with self-play have no
convergence guarantee to optimal solutions in competitive Imperfect-Information Games (IIGs). The
main reason is that the policy gradient theorem (sutton et al., 1999) is established within the single
agent situation, where the environment is Markovian. However, learning becomes non-stationary and
non-Markovian when multiple agents learn simultaneously in a competitive environment.
An optimal solution to a 2-player zero-sum IIG usually refers to a Nash Equilibrium (NE), where
no player could improve by unilaterally deviating to a different policy. Tremendous progress in
computing NE solutions has been made by a family of tabular methods: Counterfactual regret
* Equal contribution. Correspondence to: Haobo Fu (haobofu@tencent.com).
^ Work done while internship at Tencent AI Lab, Shenzhen, China.
1
Published as a conference paper at ICLR 2022
minimization (CFR) (Zinkevich et al., 2008). CFR is a type of iterative self-play algorithm based
on regret minimization, and it guarantees to converge to a NE with regard to the average policy in
2-player zero-sum IIGs. A perfect game model is required in CFR to sample many if not all actions
from a state. To handle large-scale IIGs with CFR, abstractions (applied to either the action space or
the state space) are usually employed to reduce the game to a manageable size (Moravcik et al.,2017;
Brown & Sandholm, 2018; 2019)1. However, abstractions are domain specific (Waugh et al., 2009;
Johanson et al., 2013; Ganzfried & Sandholm, 2014). More importantly, some large-scale IIGs are
inherently difficult to be abstracted, such as the game of Mahjong (Li et al., 2020b).
In this paper, we investigate a large-scale IIG, i.e., 2-player (1-on-1) zero-sum Mahjong, whose
information set size and game length are much larger than poker2. Li et al. (2020b) has recently de-
veloped a strong 4-player Mahjong agent based on supervised learning and traditional Reinforcement
Learning (RL). In comparison, we study 1-on-1 Mahjong from a game-theoretic perspective, i.e.,
aiming for a NE. We are interested in methods using only trajectory samples to learn, as it is infeasible
to consistently sample multiple actions for each state in large-scale IIGs with long episodes. We
employ deep neural networks to generalize across states, since the state abstraction in 1-on-1 Mahjong
is inherently difficult, as explained in the Appendix A.2. We make the following contributions.
•	Inheriting the scalability of deep RL methods and the convergence property of CFR, we develop a
new actor-critic algorithm, named Actor-Critic Hedge (ACH), for approaching a NE in large-scale
2-player zero-sum IIGs. ACH employs a deep neural network to approximate a type of weighted
cumulative counterfactual regret. In the meantime, ACH minimizes the regret via generating
self-play policies using Hedge (Freund & Schapire, 1997).
•	We introduce a Neural-based Weighted CFR (NW-CFR), of which ACH is a practical implementa-
tion. We prove that the exploitability of the average policy in NW-CFR decreases at the rate of
O(T -1/2) under certain conditions, where T denotes the number of iterations in NW-CFR.
•	To facilitate research on large-scale 2-player zero-sum IIGs, we propose a 1-on-1 Mahjong bench-
mark. The corresponding game enjoys a large population in online games.
•	We build a 1-on-1 Mahjong agent, named JueJong, based on ACH. In an initial evaluation against
human players including a Mahjong champion3, JueJong demonstrates superior performance.
2	Notations and Background
2.1	Imperfect-Information Games and Nash Equilibrium
An IIG is usually described in an extensive-form game tree. A node (history) h ∈ H in the tree
represents all information of the current situation. For each history h, there is a player p ∈ P or a
chance player c that should act at h. Define P : H → P ∪ {c}. When P(h) ∈ P, the player P(h) has
to take an action a ∈ A(h), and A(h) is the set of legal actions in h. The chance player is responsible
for taking actions for random events. The set of terminal nodes is denoted by Z . For each player
p ∈ P, there is a payoff function defined on the set of terminal nodes, up : Z → R. In this paper, we
focus on 2-player zero-sum games, where P = {0, 1} and u0(z) + u1 (z) = 0 for each z ∈ Z.
For either player p, the set of histories H is partitioned into information sets (infosets). We denote the
set of infosets for player p by Ip and an infoset in Ip by Ip . Two histories h, h0 ∈ H are in the same
infoset if and only if h and h0 are indistinguishable from the perspective of player p. Hence, a player
p’s policy πp is defined as a function that maps an infoset to a probability distribution over legal
actions. We further define A(Ip) = A(h) and P(Ip) = P(h) for any h ∈ Ip. A policy profile π is a
tuple of policies (πp, π-p), where π-p represents the player p’s opponent policy. The expected payoff
for player p under π is denoted by up(πp, π-p). We use ∆(I) to denote the range of payoffs reachable
from a history h in infoset I. Let ∆ = maxI∈Ip,p∈P ∆(I). fπ(h) denotes the joint probability of
reaching h under π. fpπ(h) is the contribution of player p to fπ(h), and f-πp(h) is the contribution of
the opponent and chance: fπ(h) = fpπ(h)f-πp(h). We focus on the perfect-recall setting, where each
1DeepStack (MOravcfk et al., 2017) employs sparse lookahead trees, much like the action abstraction.
2Without specification, we mean Heads-up No-Limit Texas Hold’em in this paper.
3Haihua Cheng is the Competition Mahjong champion of 2014 World Mahjong Master Tournament, 2018
Tencent Mahjong Tournament, and 2019 Tencent Mahjong Tournament.
2
Published as a conference paper at ICLR 2022
player recalls the sequence of their own infosets reached. We define fpπ (Ip) = fpπ (h), ∀h ∈ Ip and
f-πp(Ip) = Ph∈Ip f-πp(h). Hence, fπ(Ip) = Ph∈Ip fπ(h) = fpπ(Ip)f-πp(Ip).
A best response BR(π-p) to π-p is a player p’s policy that satisfies up(BR(π-p), π-p) =
max∏0 Up(∏P,∏-p). A NE is a policy profile ∏*, where each player plays a best response to
the other: Up(∏p,∏-P) = max∏0 Up(∏P,∏-p), Vp ∈ P. The exploitability of a player’s pol-
icy, denoted by e(∏p), measures the performance gap between ∏p and a NE policy ∏p: e(∏p)=
Up(∏p, ∏-p) - Up(∏p, BR(∏p)). The exploitability of π is e(π) = ∣pp∣ Pp∈p e(∏p).
2.2	Reinforcement Learning and Policy Gradient Methods
RL usually assumes a Markov decision process, where the agent selects an action ai from the
legal action set A(si) in state si ∈ S4 at each time step i. The agent then receives a reward ri
from the environment and transitions to a new state si+1. The objective in RL is to learn a policy
that maximizes the expected discounted returns, i.e., the state value, starting from any state s:
Vπ(s) = Eπ[P∞=i γj-irj∣si = s] = Eπ[G], with the discount factor Y ∈ [0,1).
Many methods in RL belong to policy gradient methods, in which the parameters θ of policy π(a∣s; θ)
are updated by performing gradient ascent directly on Eπθ [G]. One early example is the standard
ReiNfORCE algorithm (Williams, 1992) that updates θ in the direction Vθ logπ(a∣s; θ)G, which
is an unbiased estimate of V Eπθ [G]. To further reduce the variance of the gradient, an action-
independent baseline is often subtracted from G: V log∏(a∣s; θ)[G 一 B(s)]. A recent policy
gradient method, named Advantage Actor-Critic (A2C) (Mnih et al., 2016), learns a parameterized
value function as the baseline: B(s) = V(s; w), where w often shares some parameters with θ.
Moreover, the value G - B(s) is replaced in A2C with the estimated advantage of action a in state
s: A(s, a) = Q(s, a) - V (s). The Q value Q(s, a) is usually estimated using sampled rewards and
predicted values of future states. A2C updates the parameters θ and w in a synchronous manner.
In an asynchronous training environment, the behavioral policy is usually different from the learning
policy. Proximal Policy Optimization (PPO) (Schulman et al., 2017) takes this discrepancy into
account by multiplying A2C gradient with an importance ratio r(a∣s; θ) = π(a∣s; θ)∕π(a∣s; θoid),
which results in r(a∣s; θ)Vθ logπ(a∣s; θ)A(s,a) = Vθr(a∣s; θ)A(s, a). Furthermore, PPO con-
strains the KL divergence between the learning policy πθ and the old behavioral policy πθold by
clipping the ratio to a small interval around 1.0.
2.3	Counterfactual Regret Minimization
CFR is an iterative algorithm that minimizes the total regret of policy by minimizing the cumulative
counterfactual regret in every state (infoset). The cumulative counterfactual regret of an action
a in state s is defined as Rtc(s, a) = Ptk=1 rkc (s, a), where rkc (s, a) denotes the instantaneous
counterfactual regret of action a in state s at iteration k. rtc(s, a) is equal to f-πtp(s)Aπt (s, a) (as
proven in Srinivasan et al. (2018)), where Aπt (s, a) is the advantage function of player p = P(s) at
state s as defined in traditional RL. Intuitively, rtc(s, a) represents how regretful the player p is that
he does not select the action a in state s at iteration t. The term f-πtp(s) = Ph∈s f-πtp(h) is needed
here to reflect the fact that reaching state s is also controlled by the opponent -p and chance. To
minimize the regret, a regret minimizer is utilized in each state at each iteration, generating a series of
local policies π1(s), . . . , πt(s). Once πt is obtained, rtc(s, a) is generated by a game tree traversing
using πt, and Rtc(s, a) is updated accordingly: Rtc(s, a) = Rtc-1(s, a) + rtc(s, a).
For either player p, define the total regret as RT = max∏0 PT=I (up(∏P,∏-p,t) 一 Up(∏p,t, ∏-p,t))∙
It is proven in Zinkevich et al. (2008) that RT ≤ PssES RT(s)] + , where [∙]+ = max{∙, 0}
and RTc (s) = maxa∈A(s) RTc (s, a). As a result, the total regret can be minimized by minimiz-
ing the cumulative counterfactual regret at each state. Moreover, the average policy ∏t(a|s)=
Pt∈τ fp∏t(s)∏t(a∖s)/Pt∈τ fpπ>t(S) for both players converges to aNE if RT for both players grows
sub-linearly with T (Zinkevich et al., 2008). There are two commonly used regret minimizers: Regret
Matching (RM) (Hart & Mas-Colell, 2000) and Hedge (Cesa-Bianchi & Lugosi, 2006). In RM, a
4We use state s and infoset Ip interchangeably in this paper.
3
Published as a conference paper at ICLR 2022
player selects an action with probability in proportion to its positive cumulative counterfactual regret.
In Hedge, the policy ∏t+ι(a∣s) is decided according to:
eη(s)Rtc(s,a)
πt+1 (Hs) = P / eη(S)Rt(s,a0).	⑴
If the player plays according to Hedge in state s and η(s) = ,8 log( A(s)∣)∕(∆2(s)T), RT(s) ≤
△(s)，log(|A(s)|)T/2 (CeSa-BianChi & Lugosi, 2006). In other words, the total regret grows
sub-linearly with T, and therefore the average policy ∏t(a|s) for both players converges to a NE.
3	The Motivation of Actor-Critic Hedge
In this section, we motivate ACH by introducing a new neural-based CFR algorithm, NW-CFR, which
employs a neural network to generalize across states and relies on only trajectory samples for training.
The key idea in NW-CFR is that a neural network (called the policy net) is used to approximate the
t
expectation of the sum of sampled advantages Rta(s, a) := E[ k=1 Aπk (s, a)]. Aπk (s, a) is set to
Aπk (s, a) if s is visited at iteration k (given that only one trajectory is sampled at each iteration) and
0 otherwise. As a result, the expectation E[Aπk (s, a)] depends on both the advantage Aπk (s, a) and
the sampling policy μk at iteration k.
At the beginning of iteration t, suppose Rta-1(s, a) is well approximated by the output y(a|s; θt-1)
of the policy net, parameterized as θt-1 in NW-CFR. The policy πt for iteration t is first obtained
via Hedge, i.e., softmaxing on η(s)y(a∣s; θt-ι). The reason we use Hedge instead of RM is that
softmaxing is shift-invariant, which may be more robust to the function approximation error compared
to the threshold operation in RM. M trajectories are then sampled into a buffer Bv via self-play using
the policy profile πt = (πp,t, π-p,t). Those samples in Bv are used to train the action value net,
parameterized as ω, by minimizing the squared loss: 2[Q(s, a; ω) - G]2, where G is the sampled
return. Afterwards, another M trajectories are sampled into a buffer Bπ via self-play using another
sampling policy profile μt = (μp,t, ∏-p,t). We use an additional behavioral policy μp,t for the player
p for more flexibility in the convergence (more details are provided in the next section). The policy θ
is then optimized according to the loss function Lπ = Ps∈S Lπ(s), where
Lπ(s)
fPa{y(a|s； θ) - [y(a|s; θt-ι) + MM PM=I is∈nAnt(S,a)]}2 if S ∈ Bn,
Pa{y(a|s; θ) - [y(a|s; θt-1) + 0]}2	otherwise,
(2)
where τi is the ith sampled trajectory in Bπ . The next iteration of NW-CFR begins after the policy
training finishes. The pseudocode of NW-CFR is given in Algorithm 1. Note that we present NW-CFR
from the perspective of player p, and the same procedure applies to the player -p. NW-CFR runs
concurrently for both players and synchronizes at the beginning of each iteration.
A potential benefit of training on the sampled advantage over the sampled instantaneous coun-
terfactual regret (as done in Brown et al. (2019), Li et al. (2020a), and Steinberger et al.
(2020)) is that the variance of the sampled advantage could be much lower. As rkc (S, a) =
f-p(s)AKk (s, a) (Srinivasan et al., 2018), the sampled instantaneous counterfactual regret 残(s, a)
is [fμk (s)]-1f-p(s)Aπk (s, a) = [fμk (s)]-1Aπk (s, a), where fμk (s) is the probability of reaching
s considering only the contribution of μp,k. The larger variance (due to [fμk(s)]-1) of rcc(s, a)
may have a negative influence on the performance when function approximation is used with only
trajectory samples. This influence is magnified in large-scale games, as [fμk (s)]-1 becomes large. 4 * *
4 Theoretical Properties of NW-CFR
In this section, we prove the convergence of NW-CFR to an approximate NE. To this end, we
first deduce that the policy target in NW-CFR isRta(s, a), which is a type of weighted cumulative
counterfactual regret. We then define a new family of CFR algorithms: weighted CFR. Finally, we
show that NW-CFR is equivalent to a type of weighted CFR with Hedge under certain conditions. The
convergence properties of weighted CFR with Hedge and hence NW-CFR are proven accordingly.
We can rewrite the NW-CFR policy loss (in Equation 2) at iteration t as Lπ = Ps∈S a{y(a|s; θ) -
[y(a∣s; θt-ι) + -M PMI ls∈τiAπt(s, a)]}2. In other words, the target of y(a∣s; θ) at iteration t is
4
Published as a conference paper at ICLR 2022
Algorithm 1: Neural-based Weighted CFR
Function Neural-based Weighted CFR(p, T, M, θ, ω):
ωo J ω, θo J θ.
for t J 1 to T do
# Obtain the policy: ∏t J Softmax(η(s)y(a∣s; θt-ι)).
#	Train the action value net:
Reset Bv J 0.
for i J 1 to M do
LTi ~ SelfPlay(∏p,t, ∏-p,t), Bv JBv ∪ Ti
Train ω on the loss E(s,a,G)~Bν {2 [Q(s, a; ω) — G]2}
ωt J ω
#	Train the policy net:
Reset Bπ J 0.
for i J 1 to M do
LTi ~ SelfPlay(μp,t, ∏-p,t), Bn JBn ∪ Ti
Estimate Ant (s, a),∀a ∈ A(s), ∀s ∈ Bn as Q(s, a; ωt) 一 Pb ∏t(b∣s)Q(s, b; ωj.
Sum aggregate advantage Ant (s, a) in Bn by state s.
Train θ on the loss E§~s{L∏(s)}, where Ln(S) is defined in Equation 2.
θt J θ. # Save θt to a policy buffer.
y(a∣s; θt-ι) + 吉 PM=ι ls∈τi Ant (s,a), whose expectation is
τk"ik,irμ (μp,k ,n-p,k )
tM
XX Mls∈τk,iAnk(S, a)
k=1 i=1
t
E「k ~(μp,k ,n-p,k )	X Ank (s,a)	= Ra(S,a).
k=1
Also, Eτk~(μp,k,n-p,k) ls∈τk Ank (s,a) = fμk (s)f-p(S)Ank (s,a),where fμ (s)f-p(s) is the reach-
ing probability of state S at iteration k. Therefore, Ra(s, a) = Pk=ι fμk (s)f-p(S)Ank (s, a). Since
rk(s, a) = f—P(S)Ank (s, a) (Srinivasan et al., 2018), we have Ra(s, a) = Pk=ι fμk (s)/C(s, a).
As a result, Rta (S, a) can be viewed as a type of weighted cumulative counterfactual regret that
multiples each instantaneous counterfactual regret with fμk (s). Without loss of generality, we define
a new family of CFR algorithms, weighted CFR, below. Afterwards, we present Theorem 1.
Definition 1. Weighted CFR follows the same procedure as the original CFR (Zinkevich et al.,
2008), except that the instantaneous counterfactual regret rtc (S, a) is weighted by some weight wt(S),
wt(S) > 0 and t∞=0 wt(S) = ∞. The original CFR is a type of weighted CFR with wt(S) = 1.0.
Theorem 1. NW-CFR is equivalent to a type ofweighted CFR with Hedge when Wt(S) = fμt (s) > 0,
given that enough trajectories are sampled and y(a|S; θt) is sufficiently close to Rta(S, a). Further,
if η(s) = P ln ∣A(s)∣∕{[wh(s)]2∆2 (s)T} and Wt(S) = fjμt (s) ∈ [wι (s),w%(s)] ⊂ (0,1],t =
1,..., T, the average policy5 ∏ of the corresponding weighted CFR with Hedge and equivalently
NW-CFR with ∏p(a∣s) = PT=ι [f[ (s)∏t (a k)]/PT=Ifnt(S),∀P ∈ P ,has e exploitability after T
iterations, where
e ≤∣S∆rim +∆ X Wh(S) -Wl(S).	(3)
2T	s∈S	Wh(S)
In Theorem 1, we proved that the exploitability e of NW-CFR is bounded by Equation 3 when
y(a|S; θt) is sufficiently close to Rta(S, a). There are two terms in the bound. The first term converges
to zero at the rate of O(T -1/2), and the second term is O(1), which is weighted by the sum of the
normalized range of the weights Ps∈S (Wh(S) - Wl(S))/Wh(S). However, it is possible to reduce the
second term to an arbitrarily small value via tightening the range of fμt (s), which is experimentally
demonstrated in the Appendix D.
5Given ∏p,t at each iteration, we could obtain 不P using the techniques introduced in Steinberger (2019).
5
Published as a conference paper at ICLR 2022
Corollary 1. Ifthe behavioral policy μp,t for each player P ∈ P is constant across iterations, and
fμt (S) > 0, ∀s ∈ S, t > 0, NW-CFR is equivalent to CFR with Hedge when y(a∣s; θt) is SUffiCientIy
close to Rta(s, a).
As shown in Corollary 1, When the behavioral policy μp,t for each player P ∈ P is time-invariant,
i.e., Wh(S) = fμt (s) = wι(s), ∀s ∈ S ,t > 0, the second term of e in Equation 3 vanishes, and CFR
with Hedge is recovered. All the proofs are given in the Appendix C.
5	ACH: A Practical Implementation of NW- CFR
When applying NW-CFR to large-scale problems, two practical issues need to be addressed:
The average policy. Theorem 1 and Corollary 1 state the convergence property of the average
policy in NW-CFR. Yet, as pointed out in Srinivasan et al. (2018), Hennes et al. (2020), and Perolat
et al. (2021), obtaining the average policy with deep neural nets in large-scale games is inherently
difficult, due to either the computation or the memory demand. Alternatively, we could employ some
additional technique to hopefully induce the current policy convergence towards a NE. Srinivasan
et al. (2018) and Hennes et al. (2020) handled this by adding an entropy regularization to the current
policy training, which is, to some extent, theoretically justified later in Perolat et al. (2021).
Training on states not sampled. Theoretically, in order to optimize Equation 2, we need to collect
both sampled and non-sampled states. Optimizing with only sampled states makes y(a|S; θt) a biased
estimation of Rta(S, a). Yet, collecting non-sampled states may be intractable in large-scale games
(Li et al., 2020a) or in situations where a perfect environment model is not available.
To strike a balance between theoretical soundness and practical efficiency, we provide a practical
implementation of NW-CFR, which is ACH. ACH adapts NW-CFR by training the current policy
with an entropy regularization on only sampled states, without the calculation of the average policy.
In order to utilize distributed clusters, ACH employs a framework of decoupled acting and learning
(similar to IMPALA (Espeholt et al., 2018)), trains the network with mini-batches, and handles
asynchronous training with the importance ratio clipping of PPO. The behavior policy μp,t is set to
πp,t6 in ACH. More details of ACH are presented in the Appendix E.
6	Related Work
To obviate the need of abstractions, various neural forms of CFR methods have been developed. An
early work of this direction is regression CFR (Waugh et al., 2015), which calculates weights for a
number of hand-crafted features to approximate the regret. Deep CFR (Brown et al., 2019) is similar
to regression CFR but employs a neural network to approximate the regret. Also, deep CFR traverses
a part of the game tree using external sampling, in comparison to the full traversal of the game tree
in regression CFR. Double neural CFR (Li et al., 2020a) is another method that approximates the
regret and the average policy using deep neural networks, where a novel robust sampling technique is
developed. Both deep CFR and double neural CFR build on the tabular Monte Carlo CFR (MCCFR)
(Lanctot et al., 2009), where either outcome sampling (sampling one action in a state) or external
sampling (sampling all actions in a state) could be employed.
When dealing with games with long episodes, a necessity may be that only trajectory samples are
allowed. To improve the learning performance with only trajectory samples, DREAM (Steinberger
et al., 2020) adapts deep CFR by using a learned Q-baseline, which is inspired by the variance
reduction techniques in tabular MCCFR (Schmid et al., 2019; Davis et al., 2020). Another recent
work using only trajectory samples is ARMAC (Gruslys et al., 2020). By replaying through past
policies and using a history-based critic, ARMAC predicts conditional advantages, based on which
the policy for each iteration is generated. Other popular neural network based methods, which learn
from trajectory samples and are inspired by game-theoretic approaches other than CFR, include neural
fictitious self-play (Heinrich & Silver, 2016), policy space response oracles (Lanctot et al., 2017),
and exploitability descent (Lockhart et al., 2019), all of which require to compute an approximate
best response at each iteration. Such computation may be prohibitive in large-scale games.
6In a preliminary experiment presented in the Appendix F, we find that the performance of the current policy
in ACH (trained with an entropy regularization) is not sensitive to the choice of μp,t.
6
Published as a conference paper at ICLR 2022
The most related methods to ACH are Regret Policy Gradient (RPG) (Srinivasan et al., 2018)
and Neural Replicator Dynamics (NeuRD) (Hennes et al., 2020), both of which employ the
actor-critic framework and thus have similar computation and memory complexities as ACH.
RPG minimizes a loss that is an upper bound on the regret after threshold, and the correspond-
ing policy gradient is VRPG(s) = - Pa Vθ[Q(s, a; W) - Pb π(b∣s; θ)Q(s,b; w)] + . However,
RPG requires an l2 projection after every gradient step for the convergence to a NE, while such
projection is not required in ACH. NeuRD is inspired by the replicator dynamics, a well stud-
ied model in evolutionary game theory (Gatti et al., 2013). The policy gradient in NeuRD is
VNeuRD(S) = pa[Vθy(a∣s; θ)][Q(s,a; W)- Pb π(b∣s)Q(s,b; w)], where y(a∣s; θ) is the output
of the policy net. There are important differences between ACH and NeuRD in how the algorithm is
motivated and how the policy net at each iteration is optimized. Also, the convergence analysis is
given only for the single-state all-actions tabular NeuRD (Hennes et al., 2020). Yet, we prove the
convergence of NW-CFR, of which ACH is a practical implementation, in full extensive-form games.
7	Experimental Studies
We firstly introduce a 1-on-1 Mahjong benchmark, on which we compare ACH with related state-
of-the-art methods of similar computation complexity: PPO, RPG, and NeuRD. Since our goal is
to approximate a NE, the standard and default performance metric, exploitability, is employed. We
approximate a lower bound on the exploitability of an agent by training a best response against it as
suggested in Timbers et al. (2020) and Steinberger et al. (2020), because traversing the full game tree
to compute the exact exploitability is intractable in such a large-scale game as 1-on-1 Mahjong. As a
complement, head-to-head performance of different methods on 1-on-1 Mahjong is also presented.
Moreover, the agent obtained by ACH is evaluated against practised Mahjong human players. To
further validate the performance of ACH in IIGs other than 1-on-1 Mahjong, experimental results on
a non-trivial poker game, i.e., heads-up Flop Hold’em Poker (FHP) (Brown et al., 2019) are presented.
Deep CFR with outcome sampling (OS-DCFR) and DREAM are added to enable a more thorough
comparison. Additional results on smaller benchmarks from OpenSpiel (Lanctot et al., 2019) are
given in the Appendix G. Note that results are reported for the current policy (of ACH, PPO, RPG,
and NeuRD) and the average policy (of OS-DCFR and DREAM) respectively.
7.1	A 2-Player Zero-Sum Mahjong Benchmark
Mahjong is a tile-based game that is played world wide with many regional variations, such as
Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full of
strategy, chance, and calculation. To facilitate Mahjong research from a game-theoretic perspective,
we propose a 2-player zero-sum Mahjong benchmark, whose game rules are similar to Competition
Mahjong. The corresponding game, “2-player Mahjong Master”, is played by humans in Tencent
mobile games. A full description of the game rules is in the Appendix A.1. Apart from being the first
benchmark for the 1-on-1 Mahjong game, our benchmark has a larger infoset size and a longer game
length (the effects are explained in the Appendix A.3), compared with existing poker benchmarks
(Lanctot et al., 2019). The infoset size (i.e., the number of distinct histories in an infoset) in 1-on-1
Mahjong is around 1011, compared to 103 in poker. This is due to the fact that only two private cards
are invisible in poker, while there are 13 invisible tiles in 1-on-1 Mahjong. In addition, players can
decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most 1-on-1 poker games end
within 10 steps. More details about the 1-on-1 Mahjong benchmark are given in the Appendix A.
7.2	Results on Our 1-on-1 Mahjong Benchmark
All methods run in an asynchronous training platform with overall 800 CPUs, 3200 GB memory,
and 8 M40 GPUs in the Ubuntu 16.04 operating system. Each method shares the same neural
network architecture, a full description of which is given in the Appendix B. We performed a mild
hyper-parameter search on PPO and shared the best setting for all methods. The advantage value is
estimated by the Generalized Advantage Estimator (GAE(λ)) (Schulman et al., 2016) for all methods.
An overview of the hyper-parameters is listed in the Appendix H.1.
Approximate Lower Bound Exploitability. To approximate a lower bound on the exploitability
of the agents obtained by each method, we train a best response against each agent. The agent of
7
Published as a conference paper at ICLR 2022
0.0
4 2 0 T-4
ISo-ISOJ Ous ΦCT2Φ><
0.2	0.4	0.6	0.8
Training Steps
ι.o
le6
(a) Approximate Lower Bound Exploitability
5 4 3 2 1
UOM sajoɔs ΦCTSΦ><
0.4	0.6
Training Steps
0.8
1.0
le6
(b) Head-to-Head performance
Figure 1: (a): The training curves of the best response against each agent. Lower is better. (b): The
training curves of each agent. The performance of an agent is evaluated by the average scores the
agent wins against a common rule-based agent. Higher is better. We report the mean as solid curves
and the range of the average scores across 5 independent runs as shaded regions.
each method is selected at the 1e6th training step. Note that the agent is fixed as a part of the 1-on-1
Mahjong environment when training the best response. We train the best response using PPO with the
same hyper-parameters that were used to train the PPO agent with self-play. During the training of
the best response, we evaluate the best response every 500 training steps using 10, 000 head-to-head
plays against each agent. According to the average scores each agent loses to its best response in
Figure 1(a), we may conclude that ACH is significantly more difficult to exploit than other methods
in the large-scale 1-on-1 Mahjong environment.
Head-to-Head Evaluation. We then compare the head-to-head performance of PPO, RPG, NeuRD,
and ACH in the 1-on-1 Mahjong environment. First, we compare the training process of each method
by evaluating each agent every 500 training steps against a common rule-based agent7 using 10, 000
head-to-head plays, the results of which are shown in Figure 1(b). As we can see, all methods beat
the common rule-based agent significantly, while ACH has a clear advantage over other methods
in terms of stability and final performance. The relatively slow convergence of RPG may due to
the threshold operation on the advantage, which could reduce sample efficiency in large-scale IIGs.
Second, the superior performance of ACH is further validated in head-to-head evaluations with other
agents in Table 1, where all the agents are selected at the 1e6th training step. The agent of ACH wins
all other agents by a significant margin.
	PpO	RPG	NeURD
RPG	-0.21 ± 0.05	-	-
NeuRD	-0.03 ± 0.02	0.04 ± 0.10	-
ACH	0.39 ± 0.02	0.66 ± 0.05	0.41 ± 0.07
Table 1: Mean (± standard deviation) of the average winning scores of the row agents against the
column agents. The statistics are estimated by 5 independent runs (resulting 5 different agents for
each method). In each run, the average winning scores are obtained via 10, 000 head-to-head plays.
Human Evaluation. We evaluate the agent, selected at the 1e6th training step, of ACH against
human players. First, the agent, named JueJong, is roughly evaluated by playing over 7, 700 games
against 157 practiced Mahjong players, where JueJong won an average of 4.56 scores per game.
Second, we select the top 4 out of 157 players according to their performances against JueJong and
play JueJong against the four players for 200 games each. As shown in Figure 2(a), the average
winning scores of JueJong oscillate in the first 120 games but all plateau above 0 afterwards. More
importantly, we evaluate JueJong against the Mahjong champion Haihua Cheng for 1, 000 games,
as shown in Figure 2(b). After playing 1, 000 games, JueJong won the champion by a score of
0.82 ± 0.96 (mean ± standard deviation), with a p-value of 0.19 under one-tailed t-test. Hence, we
may conclude that Haihua Cheng failed to exploit JueJong effectively within 1, 000 games.
7The rule-based agent is implemented such that it selects the action Hu, Ting, Kong, Chow, and Pong in
descending priority whenever available and discards the tile that has the fewest neighbours.
8
Published as a conference paper at ICLR 2022
S①」OUS
(a) Against 4 practiced Mahjong players.
S0」OuS
(b) Against the Mahjong champion.
——OS-DCFR
——DREAM
——ACH(Ours)
2 1
O O
1 1
D=一 qe±!odxw
0.2	04	0.6	0.8	1.0
Episodes le8
Figure 2: Performance of JueJong against human players.
.——DREAM
:——ACH(Ours)
10s	10β IO7 108 IO9 IOno 10n
Samples consumed
A4三q£o_dX 山
A2C
RPG
NeURD
ACH(Ours)
0.2	0.4	0.6	0.8	1.0
Episodes le8
Figure 3: The exploitability on FHP, with the x-axis being the number of episodes generated (left and
right) and the number of samples consumed (middle). We report the mean as solid curves and the
range as shaded regions across 3 independent runs. OS-DCFR and DREAM were run once as their
performances are relatively stable, according to Brown et al. (2019) and Steinberger et al. (2020).
7.3	Results on the FHP Benchmark
We further evaluate ACH and compare it with OS-DCFR, DREAM, A2C8, RPG, and NeuRD on FHP.
FHP is a simplified Heads-up Limit Texas Hold’em (HULH), which includes only the first two of the
four bettings in HULH. It is a medium-sized game with over 1012 nodes and 109 infosets. All the
methods share the same neural network architecture proposed in Brown et al. (2019). We perform a
mild hyper-parameter search for ACH, A2C, RPG, and NeuRD. For OS-DCFR and DREAM, we
follow the hyper-parameters presented in Steinberger et al. (2020). The exploitability is measured
in the number of chips per game, where a big blind is 100 chips. All the hyper-parameters and the
running environment are described in the Appendix H.2.
As shown in Figure 3, ACH performs competitively with OS-DCFR and slightly worse than DREAM
on FHP in terms of exploitability per episodes generated. However, ACH is much more training
efficient: ACH achieves an exploitability of 10 almost 100 times faster than DREAM and 1,000 times
faster than OS-DCFR. Also, in comparison with methods of similar training complexity (A2C, RPG,
and NeuRD), ACH converges significantly faster and achieves a lower exploitability.
8	Conclusions
In this paper, we investigated the problem of adapting policy gradient methods in deep RL to tackle
a large-scale IIG, i.e., 1-on-1 Mahjong. To this end, we developed a new model-free actor-critic
algorithm, i.e., ACH, for approximating a NE in large-scale IIGs. ACH is memory and computation
efficient, as it uses only trajectory samples at the current iteration and requires no computation of best
response. ACH is theoretically justified as it is derived from a new neural-based CFR, i.e., NW-CFR,
of which we proved the convergence to an approximate NE in 2-player zero-sum IIGs under certain
conditions. The superior performance of ACH was validated on both our 1-on-1 Mahjong benchmark
and other common benchmarks. Secondly, to facilitate research on large-scale IIGs, we proposed the
first 1-on-1 zero-sum Mahjong benchmark, whose infoset size and game length are much larger than
poker. Finally, using ACH we obtained the 1-on-1 Mahjong agent JueJong, which has demonstrated
stronger performance against the Mahjong champion Haihua Cheng.
8PPO is replaced with A2C in a synchronous training environment.
9
Published as a conference paper at ICLR 2022
Acknowledgement
We thank the Mahjong champion Haihua Cheng for his efforts in this work. We appreciate the
support from Tencent Mahjong (https://majiang.qq.com). We are grateful to Tencent AI
Arena (https://aiarena.tencent.com) for providing the powerful computing capability to
the experiments on 1-on-1 Mahjong.
Reproducibility S tatement
The experiments on 1-on-1 Mahjong were run in a large cluster of thousands of machines, on which
we have developed an efficient actor-learner training platform, similar to IMPALA (Espeholt et al.,
2018). The code of the platform is not released currently but is planned to be open sourced in the
near future. The code of the 1-on-1 Mahjong benchmark is available at https://github.com/
yata0/Mahjong. The code of ACH is available at https://github.com/Liuweiming/
ACH_poker. All the hyper-parameters for all the experiments are listed in the Appendix H. All the
theoretical results are presented in the main text, with all the proofs given in the Appendix C.
References
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, and et al. Dota 2 with
large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019.
Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418-424, 2018.
Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):
885-890, 2019.
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. In International Conference on Machine Learning (ICML), pp. 793-802, 2019.
Nicolo Cesa-Bianchi and Gdbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Trevor Davis, Martin Schmid, and Michael Bowling. Low-variance and zero-variance baselines for
extensive-form games. In International Conference on Machine Learning, pp. 2392-2401. PMLR,
2020.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
scalable distributed deep-rl with importance weighted actor-learner architectures. In International
Conference on Machine Learning (ICML), volume 80, pp. 1406-1415, 2018.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.
Sam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth
mover’s distance in imperfect-information games. In Twenty-Eighth AAAI Conference on Artificial
Intelligence, 2014.
Nicola Gatti, Fabio Panozzo, and Marcello Restelli. Efficient evolutionary dynamics with extensive-
form games. In AAAI Conference on Artificial Intelligence, 2013.
Audrunas Gruslys, Marc Lanctot, Remi Munos, Finbarr Timbers, Martin Schmid, Julien Perolat,
Dustin Morrill, Vin^cius Flores Zambaldi, Jean-Baptiste Lespiau, John Schultz, Mohammad Ghesh-
laghi Azar, Michael Bowling, and Karl Tuyls. The advantage regret-matching actor-critic. CoRR,
abs/2008.12234, 2020.
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127-1150, 2000.
10
Published as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR),pp. 770-778, 2016.
Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
information games. CoRR, abs/1603.01121, 2016.
Daniel Hennes, Dustin Morrill, Shayegan Omidshafiei, Remi Munos, Julien PerolaL Marc Lanctot,
AUdrUnas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas, Edgar Du6nez-Guzmdn, et al. Neural
replicator dynamics: Multiagent learning via hedging policy gradients. In International Joint
Conference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 492-501, 2020.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp.
448-456, 2015.
Michael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space
abstractions in extensive-form games. In Proceedings of the 2013 international conference on
Autonomous agents and multi-agent systems, pp. 271-278, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H. Bowling. Monte carlo sampling for
regret minimization in extensive games. In Yoshua Bengio, Dale Schuurmans, John D. Lafferty,
Christopher K. I. Williams, and Aron Culotta (eds.), Advances in Neural Information Processing
Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings
of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada, pp. 1078-1086.
Curran Associates, Inc., 2009.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat,
David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement
learning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4190-4203, 2017.
Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, ViniCiUS Flores Zambaldi, Satyaki Upadhyay,
Julien Perolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, and et al.
Openspiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019.
Hui Li, Kailiang Hu, Shaohua Zhang, Yuan Qi, and Le Song. Double neural counterfactual regret
minimization. In International Conference on Learning Representations (ICLR), 2020a.
Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin,
Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement learning.
CoRR, abs/2003.13590, 2020b.
Edward Lockhart, Marc Lanctot, Julien Perolat, Jean-Baptiste Lespiau, Dustin Morrill, Finbarr
Timbers, and Karl Tuyls. Computing approximate equilibria in sequential adversarial games by
exploitability descent. In International Joint Conference on Artificial Intelligence (IJCAI), pp.
464-470, 2019.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning (ICML), pp. 1928-1937, 2016.
Matej Moravcfk, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.
Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro
Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From poincare
recurrence to convergence in imperfect information games: Finding equilibrium via regularization.
In International Conference on Machine Learning, pp. 8525-8535. PMLR, 2021.
11
Published as a conference paper at ICLR 2022
Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling.
Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive
form games using baselines. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33,pp. 2157-2164, 2019.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In International Confer-
ence on Learning Representations, (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P6rolat, Karl Tuyls, Remi Munos, and
Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 3422-3435, 2018.
Eric Steinberger. Single deep counterfactual regret minimization. arXiv preprint arXiv:1901.07621,
2019.
Eric Steinberger, Adam Lerer, and Noam Brown. DREAM: deep regret minimization with advantage
baselines and model-free learning. CoRR, abs/2006.10410, 2020.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradi-
ent methods for reinforcement learning with function approximation. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 1057-1063, 1999.
Finbarr Timbers, Edward Lockhart, Martin Schmid, Marc Lanctot, and Michael Bowling. Approxi-
mate exploitability: Learning a best response in large games. CoRR, abs/2004.09677, 2020.
Oriol Vinyals, Igor Babuschkin, WojciechM. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, and et al. John P. Agapiou and.
Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):
350-354, 2019.
Kevin Waugh, Martin Zinkevich, Michael Johanson, Morgan Kan, David Schnizlein, and Michael
Bowling. A practical use of imperfect recall. In Eighth symposium on abstraction, reformulation,
and approximation, 2009.
Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael H. Bowling. Solving games with
functional regret estimation. In AAAI Conference on Artificial Intelligence, pp. 2138-2145, 2015.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256, 1992.
Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu,
Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao
Huang, and Wei Liu. Towards playing full MOBA games with deep reinforcement learning. In
Advances in Neural Information Processing Systems (NeurIPS), 2020.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1729-1736, 2008.
12
Published as a conference paper at ICLR 2022
A Introduction of the 1-on-1 Mahjong Benchmark
Mahjong is a tile-based game that is played world wide with many regional variations, such as
Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full
of strategy, chance, and calculation. In this paper, we investigate a 1-on-1 Mahjong game, whose
information set size is much larger than poker, as shown in Figure 4. The game rules of 1-on-1
Mahjong are similar to Competition Mahjong. The corresponding game, “2-player Mahjong Master",
is played by humans in Tencent mobile games (https://majiang.qq.com).
tυz∞*jtυs UOqeEJOM-u_
IO48 -
4-Player Mahjong
IO11-----
,HULH
IO3+一千一
IO14
Ivl Mahjong
IO74
io121
Heads-Up
No-Limit Hold'em
-------1►
IO162
Information Set Count
Figure 4:	The game complexity of Heads-up Limit Texas Hold’em (HULH), Heads-up No-Limit
Texas Hold’em, 1-on-1 Mahjong, and 4-Player Mahjong.
A.1 THE GAME RULES
Category	Name	Type	Copy	Count
SimPIes	Characters	—∙1	幺3 χjθ⅛ 魂重毒魂 f r r 金' f	4	36
Honors	Winds	菜蒿耕龙	4	16
Honors	Dragons	"B	4	12
Bonus	Flowers	¾⅞⅜ *	1	4
Bonus	Seasons		1	4
Figure 5:	A list of all tiles in the 1-on-1 Mahjong game.
There are 24 unique tiles and 72 tiles in total in the 1-on-1 Mahjong game, as shown in Figure 5. At
the beginning of the 1-on-1 Mahjong game, each player is dealt with 13 tiles, the content of which is
invisible to the other. Afterwards, each player takes actions in turn. Typically, the first player draws a
tile from the deck wall and then discards a tile, and the next player takes the same types of actions
in sequence. There are exceptional cases where the player does not draw a tile from the deck wall
but Chow, Pong, or Kong the tile the opponent just discarded. Afterwards, the player discards a
tile, and the game proceeds. Also, after drawing a tile from the deck wall, there are cases where the
player could Kong, after which the player draws and discards a tile in sequence. There are 10 types
of actions with 105 different actions in total, a full description of which is listed in Table 2.
13
Published as a conference paper at ICLR 2022
Type	Description	#Action
Discard	Discard one of the tiles in the hand.	16
Pong	Make a meld of 3 identical tiles by seizing the opponent's latest discarded tile.	16
Concealed-Kong	Make a meld of 4 identical tiles when they are in the hand, and the Concealed-Kong is not revealed to the opponent.	16
Kong	Make a meld of 4 identical tiles by seizing the opponent,s latest discarded tile.	16
Add-Kong	Make a meld of 4 identical tiles by adding a tile to his own exposed Pong.	16
Chow	Make a meld of 3 character tiles in a row by seizing the opponent,s latest discarded tile.	21
Draw	Give up the action of Pong, Chow, or Kong, and draw a tile from the deck wall.	1
Ting	When there is only one tile away from a legal hand, the player can declare Ting.	1
Hu	Form a legal hand by drawing a tile or seizing the opponent,s latest discarded tile. The HU action ends the game.	1
Pass-Hu	Give up the action of Hu. Afterwards, the player should select another legal action.	1
Table 2: Different actions in the 1-on-1 Mahjong game.
The goal of each player is to complete a legal hand prior to the opponent, by drawing a tile or using
the tile the opponent just discarded. A legal hand is generally in the form of four melds and a pair,
with an exception of 7 pairs. Different categories of legal hands, 64 in total, come with different
points, which is fully described in Table 3. Besides, a legal hand can belong to multiple categories,
and the score of a legal hand is the sum of points of corresponding categories. Legal hands with
higher points are generally more difficult to form in terms of either luck or strategy, and it is critical
for a player to trade off the winning probability and the corresponding winning points. If no player
completes a legal hand before the tiles are exhausted, the game is tied. A flow chart of the game is
illustrated in Figure 6.
	Category name	Points	Pattern descriptions	Conflicts
1	Pure Double Chow	1	Two identical sequences of 3 character tiles.	
2	Short Straight	-1-	A hand with two successive sequences.	
3	Two Terminal Chows	1	A hand with two sequences 123 and 789.	
4	Melded Kong	-1-	A hand with exposed quads.	
5	Edge Wait	1	A hand completion with the situation there is only one tile name to complete because you have the incomplete sequence in edge (12or 89).	
6	Closed Wait	1	A hand completion with the situation there is only one tile name to complete because you have the incomplete sequence lacking center (like 24 and 79).	
7	Single Wait	1	A hand completion solely waiting on a tile to form a pair.	
8	Self Draw	-1-	A hand completion by draw.	
9	Flower Tile	1	Whenever a player draws a flower or season tile, this is counted as 1 point.	
10	Ready Hand	-2	A hand that is one tile away from winning.	
14
Published as a conference paper at ICLR 2022
11	Dragon Pung	2	A hand with a triplet or quad of dragon tile.	
12	Concealed Hand	2	A hand without exPosed melds and comPle- ted by seizing the discarded tile.	
13	All ChoWs	2	A hand with four sequences and a pair in characters.	
14	Tile Hog	2	A hand with four same tiles, other than a melded quad.	
15	TWo Concealed Pungs	2	A hand with two concealed triplets or quads.	
16	Concealed Kong	-2-	A hand with a concealed quad.	
17	All SimPles	2	A hand consisting of no character 1, 9 and honor tile.	
18	Outside Hand	4	A hand that includes terminals and honors in each meld, including the pair.	
19	Fully Concealed Hand	4	A hand without exposed melds and comple- ted by drawing a tile.	Self Draw
20	Two Melded Kongs	4	A hand with two exposed quads.	Melded Kong
21	Last Tile	4	Winning on a tile that is the last of its kind. Three of the other tiles on the table already revealed to all players.	
22	Little Three Winds	6	A hand with two wind triplets or quads and a wind pair.	
23	All Pungs	6	A hand with four triplets or quads.	
24	Half Flush	6	A hand consisting of character and honor tiles.	
25	Two Dragon Pungs	6	A hand containing two dragon triplets or quads.	Dragon Pung
26	Two Concealed Kongs	6	A hand containing two concealed quads.	Concealed Kong
27	Melded Hand	6	Every set in the hand must be completed with tiles discarded by other players.	Single Wait
28	Out with RePlacement Tile	8	Hand completion with supplemental tile when you melding quad.	Self Draw
29	Rob Kong	8	Winning off the tile that opponent adds to a melded triplet(to create a Kong).	Last Tile
30	Last Tile Claim	8	Winning off another player on the last tile (of the game).	
31	Pure Straight	16	A hand with three sequences 123, 456 and 789 in characters.	Short Straight, Two Terminal Chows
32	Pure Shifted Chows	16	Three sequences in characters each shifted either one or two numbers up from the last, but not a combination of both.	
33	All Flowers	16	A player has or draws all flower tiles.	Flower Tiles
34	Full Flush	16	A hand consisting of only characters.	
35	Three Concealed Pungs	16	A hand with three concealed quads or triplets.	Two Concealed Pungs
36	Four Honour Pungs	24	A hand with four honor triplets or quads.	All Pungs
37	Big Three Winds	24	A hand with three winds be triplets or quads.	Little Three Winds
15
Published as a conference paper at ICLR 2022
38	Seven Pairs	24	A hand with seven pairs.	Concealed Hand, Single Wait, Fully Concealed Hand.
39	Pure Triple Chow	24	A hand with three identical sequences.	Pure Double Chow
40	Pure Shifted Pungs	24	A hand with three number triplets or quads with successive numbers.	Pure Triple Chow
41	Four Pure Shifted Chows	32	Four shifted number sequences, each shifted either one or two numbers up from the last, but not a combination of both.	Pure Shifted Chows, Short Straight, Two Terminal Chows, Pure Double Chow
42	Three Kongs	32	A hand with three quad melds.	Two Melded Kongs, Melded Kong, Two Concealed Kongs, Concealed Kong
43	All Terminals and Honours	32	A hand consisting of only terminal and honor tiles.	All Pungs, Outside Hand
44	Heavenly Ready Hand	32	A hand that is one tile away from winning at the beginning of the game.	Ready Hand
45	Quadruple Chow	48	A hand with four identical sequences.	Pure Triple Chow, Tile Hog, Pure Double Chow, Two Terminal Chow, Pure Shifted Pungs
46	Four Pure Shifted Pungs	48	A hand with four character triplets or quads with successive numbers.	Pure Shifted Pungs, Pure Triple Chow, All Pungs
47	Four Winds Seven Pairs	48	Seven pairs hand with four different wind pairs.	Seven Pairs, Concealed Hand, Single Wait, Fully Concealed Hand
48	Three Dragons Seven Pairs	48	Seven pairs hand with three different dragon pairs.	Seven Pairs, Concealed Hand, Single Wait, Fully Concealed Hand
49	Little Four Winds	64	A hand with three winds be triplets or quads, and the last wind be pair.	Big Three Winds, Little Three Winds
16
Published as a conference paper at ICLR 2022
50	Little Three Dragons	64	A hand with two dragons be triplets or quads, and the last dragon be pair.	Two Dragon Pungs, Dragon Pung
51	All Honours	64	A hand consisting of only honor tiles.	All Terminals And Honours, All Pungs, Four Honour Pungs, Outside Hand
52	Four Concealed Pungs	64	A hand with four concealed triplets or quads.	Concealed Hand, All Pungs, Three Concealed Pungs, Two Concealed Pungs, Fully Concealed Hand
53	Pure Terminal Chows	64	A hand with two Two Terminal Chows and a pair of number 5 in character.	All Chows, Seven Pairs, Full Flush, Pure Double Chow, Two Terminal Chows
54	Big Four Winds	88	A hand with triplets or quads of all four winds and an pair.	All Pungs, Little Three Winds, Big Three Winds, Four Honor Pungs
55	Big Three Dragons	88	A hand with triplets or quads of all three dragons.	Dragon Pung, Two Dragon Pungs
56	Nine Gates	88	Collecting number tiles 1112345678999 without melding, and completing with any tile of characters.	Full Flush, Concealed Hand, Fully Concealed Hand
57	Four Kongs	88	A hand with four quad melds.	Three Kongs, Two Melded Kongs, Melded Kong, Single Wait, Concealed Kong, Two Concealed Kongs, All Pungs
58	Seven Shifted Pairs	88	Seven pairs hand with successive seven numbers in characters.	Seven Pairs, Single Wait, Concealed Hand, Full Flush, Fully Concealed Hand
17
Published as a conference paper at ICLR 2022
59	Upper Four	88	A hand consisting of character tiles of 6, 7,8or9	Seven Pairs
60	Lower Four	88	A hand consisting of character tiles of 1, 2,3or4	Seven Pairs
61	Big Seven Honours	88	Seven pairs hand with four different wind pairs and three different dragon pairs.	Seven Pairs, Four Winds Seven Pairs, Three Dragons Seven Pairs, Outside Hand, Single Wait, Concealed Hand, Fully Concealed Hand, All Honours
62	Heavenly Hand	88	The dealer draws a winning hand at the beginning of the game.	Self Draw, Concealed Hand
63	Earthly Hand	88	A player completes a winning hand with the dealer’s first discard and in most variants, provided the dealer does not draw a quad.	Self Draw, Concealed Hand
64	Humanly Hand	88	A player completes a winning hand with the opponent player’s first discard. And before that any action of Chow, Pong or Kong is not available.	
Table 3: The categories of legal hands in ascending order of corresponding points.
The winning points of a legal hand are obtained by summing the points of the
matched categories in reverse order while excluding the conflict categories.
A.2 The State and Action Space
The state space size of 1-on-1 Mahjong, shown as the infoset count in Figure 4, is approximately 1074.
Yet, the state space of 1-on-1 Mahjong is not as easily abstracted as in poker. The primary reason is
that a single tile difference in the state could significantly impact the policy, e.g., making a legal hand
illegal and vice versa. In contrast, states that have similar strength in poker could share a common
policy. For instance, the optimal preflop policy could be very similar for “Ace-Four" and “Ace-Three"
in poker. Another reason is that a state in 1-on-1 Mahjong is divided into different information groups,
as demonstrated in Figure 7(a). Different information groups have significantly different meanings.
For instance, one group denotes the player’s hand, which is invisible to the opponent, while another
one denotes the player’s discarded tiles, which are visible to both players.
There are 105 different actions in total, as demonstrated in Table 2. The number of legal actions in a
state is relatively small compared to poker. Yet, the game length in 1-on-1 Mahjong is larger than
that in poker. Players can decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most
1-on-1 poker games end within 10 steps. As a result, the reaching probability of states in 1-on-1
Mahjong may vary more significantly than that in poker.
A.3 The Effects of a Larger Infoset Size and a Longer Game Length
As shown in Figure 4, 1-on-1 Mahjong has a larger infoset size than poker. The infoset size does not
seem to have an influence on the convergence of a tabular CFR (Zinkevich et al., 2008). However,
when trajectory sampling and function approximation are used together, the situation may be different.
To be more specific, in a trajectory sampling algorithm, the variance of the sampled instantaneous
counterfactual value (regret) of a larger infoset may tend to be higher, which may have a large
18
Published as a conference paper at ICLR 2022
Figure 6: A flow chart of the 1-on-1 Mahjong game.
influence on performance when neural network function approximation is used. In other words,
1-on-1 Mahjong may be complementary to poker in evaluating algorithms using deep neural networks
and only trajectory samples.
The game length has a direct impact on the sampling methods used. For poker, which has a relatively
short game length, methods that sample multiple actions in a state are very common in the literature.
Yet, sampling multiple actions consistently in game trees with long episodes is certainly prohibitive,
as the number of samples goes exponentially with the game length. 1-on-1 Mahjong has a maximal
game length about 40, which may be daunting for methods that try to sample multiple actions in
a state. In other words, 1-on-1 Mahjong may be complementary to poker in evaluating algorithms
when only trajectory samples are allowed.
B The Model Design of Our 1-on-1 Mahjong agent JueJong
The model of JueJong is an end-to-end neural network that takes all relevant information as input and
outputs both the probabilities of all actions and the state value. This is different from five separated
neural networks in Suphx (Li et al., 2020b), representing five different types of actions. Also, we
train JueJong from zero by pure self-play using ACH, while the five neural networks in Suphx were
trained by supervised learning on human data, with only the “Discard” network further enhanced by
RL. Figure 7 gives an overview of the model design of JueJong.
19
Published as a conference paper at ICLR 2022
(a)
(b)
Value net
❾	is_ready [2,2]	—
Bonus	[2,8]
3 × Stage
Feature D
Position [2,1]	，
Built-in	Last action [2,105]
Policy net
FC FC
(c)
IIl
Figure 7: (a) The graphical user interface of 1-on-1 Mahjong with marked regions representing
different groups of information, which are encoded in either image-like features or one-hot features.
(b) The image-like feature encoding scheme. (c) The model architecture design.
The marked regions in Figure 7(a) summarize the information an agent can observe. Regions 1 to
7 are encoded in image-like features, which represent the player’s hand, the player’s Chow, Pong,
and Kong, the player’s concealed-Kong, the player’s Discard, the opponent’s Chow, Pong, and Kong,
the opponent’s concealed-Kong, and the opponent’s Discard respectively. A feature map, as shown
in Figure 7(b), of height 4 and width 17 is employed, where a 0 or 1 in the ith row and jth column
means whether there are i tiles of the jth type in the input set of tiles. Therefore, a single or two
successive convolutional layers with 3 × 3 kernels efficiently derive row and column combinations.
Note that, in Suphx (Li et al., 2020b), the feature maps are one-dimensional vectors, where 1 × 3
kernels are used. The black tile in Figure 7(b) is designed exclusively for Region 6 to indicate how
many concealed-Kongs the opponent has. For discarded tiles in Region 4 or 7, 24 such feature
maps are used to indicate the latest 24 discarded tiles in order, with one feature map encoding one
discard tile. All the feature maps are concatenated in the channel dimension, and therefore the final
image-like features are in the shape of 4 × 17 × 53 (h × w × c).
Regions 8 to 11 are encoded in one-hot features, which represent the player’s position (0 or 1), the
is_ready state of both players, and the bonus tiles of both players. Additionally, the last action (not
shown as a region in Figure 7(a)) for each player is encoded in a one-hot feature vector as well.
We apply residual blocks (He et al., 2016) to transform the image-like features. There are totally
3 stages with 3 residual blocks and 1 transition layer in each stage, as shown in Figure 7(c). Each
block contains two convolutional layers with kernel size 3 × 3. The transition layers are point-
wise convolutions that scale the number of output channels to 64, 128, and 32, respectively for
each stage. Subsequently, the transformed image-like features are reshaped to a vector, which is
concatenated with the one-hot feature vectors. A fully-connected layer (dimension 1024) is then
used to transform the concatenated feature vector, and two branches with two fully-connected layers
(dimension 512 × 512) each output the action probabilities and the state value respectively. Besides,
we apply batch normalization (Ioffe & Szegedy, 2015) and ReLU non-linearity after all convolutional
layers and fully-connected layers.
C	Theoretical Properties of NW-CFR
C.1 Proof for Theorem 1
In order to prove Theorem 1, we first prove the following Lemma.
20
Published as a conference paper at ICLR 2022
Lemma 1. For a weighted CFR, if wt(s) ∈ [wl(s), wh(s)], 0 < wl(s) ≤ wh(s) ≤ 1, then
maxα∈A(s) Rw(S,a)	(Wh(S)- Wl(S))IA(S)Iδ%
max r rk(s, a) ≤ ------------；~；-------+--------------；~；---------.
a∈A(s) k=1	Wh(S)	Wh(S)
Proof. First, for any state S ∈ S and action a ∈ A(S), we have
t
Rtw(S,a) :=XWt(S)rkc(S, a)
k=1
=	Wk (S)Irkc (S, a)I -	Wk0 (S)Irkc 0 (S, a)I
k:rkc (s,a)≥0	k0 :rkc0 (s,a)<0
≥	Wl(S)Irkc(S,a)I -	Wh(S)Irkc0(S,a)I.
k:rkc (s,a)≥0	k0 :rkc0 (s,a)<0
So,
t
Rtw(S, a) ≥ Wh(S) rkc (S, a) - (Wh(S) - Wl(S)) Irkc 0 (S, a)I.
k=1	k0 :rkc0 (s,a)≥0
In other words,
t
Xrkc(S,a) ≤
k=1
Rw(s,a)
Wh(S)
+ Wh(S) - Wl(S)
Wh(S)
Irkc0(S,a)I
k0:rkc0 (s,a)≥0
So,
max
a∈A(s) k=1
t
≤
maXa∈A(s) Rw (s, a)
Wh(S)
(Wh(S) - Wl(S))∆t
Wh(s)
(4)
(5)
(6)
(7)
(8)
□
Then we can prove the Theorem:
Proof. For NW-CFR, the policy at iteration t is generated according to Hedge:
eη(s)Rta-1(s,a)
πt(s,a) = P , eη(s)Ra-ι(s,αo),	⑼
where
t-1
Ra-I(S,a) = X fpμk (s)rc (s,a).	(10)
k=1
Meanwhile, weighted CFR with Hedge generates policy at iteration t according to
eη(s)Rtw-1 (s,a)
πt Ga) = Pa, eη(s)RW-ι(s,a0) ,	(II)
where
t-1
Rtw-1(S, a) = XWk(S)rkc(S,a).	(12)
k=1
So, When Wk(s) = fμk(s) ≥ Wl(s) > 0 for any S ∈ S and k > 0, NW-CFR and weighted CFR with
Hedge generate the same policy at each iteration, and therefore NW-CFR is equivalent to weighted
CFR with Hedge.
Stw (S) =
Then, we can prove the convergence property of NW-CFR. Let
eη(s)Rtw (s,a)
a∈A(s)
(13)
21
Published as a conference paper at ICLR 2022
We have	Sw
ln Sw =ln X eη(S)Rw(Sa- ln |A(s)|
0	a∈A(s)
≥ln max eη(S)Rtw(S,a) - ln IA(S)I
a∈A(S)
=η(S) max Rtw (S, a) - ln IA(S)I.
a∈A(S)
Meanwhile, for each k = 1, . . . , t,
Sw	P	eη(S)Rkw-1(S,a)eη(S)wk(S)rkc(S,a
ln 京=ln [ 一 Pa∈A(s) eη(S)Rw-I(S⑷
ln	πk (S, a)eη(S)wk (S)rkc (S,a
a∈A(S)
Since
ln E[esX] ≤ SEX + 及 - a)
8
Sw
ln sw— ≤ η(s) E πk(s,a)wk(S)rk(S,6 十
k-1	a∈A(S)
η2(S)δ2(S)碑(S)
Notethatrkc(S, a) = vkc (S, a) -	a∈A(S)πk(S,a)vkc(S,a). So
πk(S, a)Wk (S)rkc (S, a) = 0.
a∈A(S)
Therefore,
SW	η2(S)△)(S) Pk=I wk (S)
in F- ≤ -----------------------
So,
η(S) maχ RW(S,a) - in ∣A(s)∣ ≤ η)(S)△)(S) )P= IW(S)
a∈A(S)	8
i.e.,
max Rtw (S, a) ≤
a∈A(S)
According to Lemma 1,
in IA(S) + η(s)∆2(s) P
η(S)
tk=1 Wk2 (S)
max Rtc(S, a) ≤
a∈A(S)
ln IA(S)I + η(S)δ2(S) Pk=IW(S) + (Wh(S) - WlISy)δ
η(S)wh(S)
8wh(S)
wh(S)
(14)
(15)
(16)
(17)
(18)
(19)
(20)
(21)
(22)
8
8
8
When η(S)
we have
√8ln ∣A(s)∣∕{[wh(s)]2∆2(s)T} and Wt(S) ∈ [wι(s),w%(s)] ⊂ (0,1],t = 1,...,T,
max RT(s,α) <Jln|A(S)△：(S)W22(S)T 十(Wh(S)-W：(S))δt
a∈ A(S) IT	V	2Wh (s)	Wh(S)	(23)
≤∆J T in A(S)I +(Wh(S)-Wl(S))δt.
2	Wh(S)
According to Theorem 2 in Zinkevich et al. (2008), the total regret
RT ≤	max [RcT (S, a)]+
S∈S a∈A(S)
≤ Xbr "I + (Wh(S)WhWl)(S))”
≤∣s∣δ …+∆T X w≈^ .
22
Published as a conference paper at ICLR 2022
As a result, according to the folk theorem in Zinkevich et al. (2008), the average policy has
exploitability, where

卷 X RTτT ≤ ∣s∣∆∕21τ ιn IAi+δ X
p∈P	s∈S
Wh(S) — Wl(S)
Wh(S)
(25)
□
C.2 Proof for Corollary 1
Proof. When the behavioral policy μp,k of each player p ∈ P is constant across iterations ∀k > 0,
the reaching probability fμ (S) of any state S ∈ S is also constant. Assume fμk (S) = w(s), then,
t-1	t-1
Ra-I(S,a) = X fμk (S)rc(s, a) = W(S) X rcGa) = W(S)Rtc-1 (S, a).	(26)
k=1	k=1
In other words, Rta-1(S, a) is equal to the cumulative counterfactual regret scaled by a time-invariant
weight W(S). Hence, the policy at iteration t is
πt(a∣S)
eη(s)w(s)Rtc(s,a)
eη0 (s)Rtc (s,a)
e/(S)W(S)Rc-I(S,a0)
en0(S)Rc-I(S,aO)
(27)
a
a
where the new η0(S) is set to ,8ln ∣A(s)∣∕[Δ2(S)T]. As a result, for constant μp,k, NW-CFR is
equivalent to CFR with Hedge when y(a∣S; θt) is sufficiently close to Rta(S, a). Furthermore, since
Wl(s) = fμk (s) = Wh(s), the second term in Equation 25 vanishes, i.e.,
e ≤ ∣s∣δ γ2T ln ∣A∣.
(28)
As a result, the exploitability bound of CFR with Hedge is recovered.
□
D Experimental Results of the Weighted CFR
As stated in Section 5, ACH is a practical implementation of NW-CFR, which is a straightforward
neural extension to the weighted CFR defined in Definition 1, together with Wt(S) = fμt (s) and
Hedge. In order to investigate the behavior of weighted CFR, in this section, we instantiate multiple
weighted CFR algorithms with different settings of the weight Wt(S) by varying μp,t, since fμ (s)
depends only on μp,t . Note that the weighted CFR traverses the full game tree at every iteration and
that μp,t is only used to calculate the state reaching probability fμt (s). We test these algorithms on
three small IIGs in OpenSpiel: Kuhn poker, Leduc poker, and Liar’s Dice.
m=qfoaxw
250 500
—CFR(Hedge)
----Uniform
----Current(0.5)
----Current(O-I)
----Current
750 1000 1250 1500 1750 2000
Iterations
m=qfoaxw
250 500 750 1000 1250 1500 1750 2000
Iterations
A4三qfoaxw
——CFR(Hedge)
,---Uniform
----Current(0.5)
----Current(O-I)
----Current
250 500 750 1000 1250 1500 1750 2000
Iterations
(a) Kuhn poker	(b) Leduc poker	(c) Liar’s Dice
Figure 8: Exploitability of the weighted CFR with Wt(S) = fμ (S) and CFR (i.e., the weighted CFR
with Wt(S) = 1.0). The probability fμ (S) is determined by the behavior policy μp,t. The setting of
μp,t for each line is given in the legend, in which “Uniform” means μp,t(s) = the uniform policy;
“Current” means μp,t(s) = ∏p,t(s); “Current(x)” means μp,t(s) = xUniform +(1 -x)∏p,t(s). Note
that the exploitability is reported with regard to the average policy.
As shown in Figure 8, the weighted CFR with Wt(S) induced by a stationary μp,t (i.e., the uniform
policy) converges at the same pace with CFR(Hedge). As a result, Corollary 1 is verified on the three
23
Published as a conference paper at ICLR 2022
small benchmarks. Also, as Theorem 1 states, the exploitability of the weighted CFR is influenced by
the range of wt(s) = fμ (s). This is experimentally demonstrated in Figure 8 by setting μp,t to a
mixed policy between the current policy πp,t and the uniform policy. We can see that the weighted
CFR still performs competitively with CFR(Hedge), when μp,t(s) = 0.5Uniform + 0.5∏p,t(s).
E ACH: A Practical Implementation of NW-CFR
To address the practical issues mentioned in Section 5, we provide a practical and parallel implemen-
tation of NW-CFR, i.e., ACH, which employs a framework of decoupled acting and learning, similar
to IMPALA (Espeholt et al., 2018). ACH maintains a policy net y(a|s; θ) and a value net V (s; ω),
where θ and ω share a large portion of parameters (see Figure 7). Both players use the same θ and
ω. We do not use an additional time-invariant behavioral policy for sampling actions. Instead, we
use the current policy ∏t, i.e., μp,t = ∏p,t, ∀p ∈ P. As a result, we can use the same samples to train
both the value net and the policy net. Also, η(s) is incorporated into the learned target value in the
policy net, so the policy π(a∣s) is obtained by directly softmaxing on y(a∣s; θ).
The advantage A(s, a) is estimated by GAE(λ) (Schulman et al., 2016), using sampled rewards
and V (s; ω), for only sampled states and actions. The value and policy nets are updated as soon
as a mini-batch of samples is available. In other words, we update θ and ω once using a single
mini-batch at each iteration. As a result, the policy loss reduces to Ln(s) = η(s) "„)A(a, s),
where ∏ M(α∣s) accounts for the fact that the action a was sampled using ∏oid(a∣s). ACH handles
asynchronous training with the importance ratio clipping [1 - ε, 1 + ε] of PPO (Schulman et al.,
2017). To avoid numerical issues, the mean y(∙ |s; θ) is subtracted from the policy output, which is
then clipped within a range [-lth, lth]. The pseudocode of ACH is given in Algorithm 2.
Algorithm 2: ACH
Initialize the policy and critic parameters: θ and ω.
Start multiple actor and learner threads in parallel.
Actors:
while true do
Fetch the latest model from the learners.
Generate samples via self-play in the form: [a, s, A(s, a), G, πoιd(a|s)].
_ Send the samples to the replay buffer.
Learners:
fort ∈ 1,2,3, ... do
Fetch a mini-batch of samples from the replay buffer.
Lsum = 0.
for each sample [a,s,A(s,a), G, ∏oid(a∣s)] ∈ the mini-batch do
=(1{⅛⅛¾ < 1 + ε}l{y(a∣s; θ)	-	y(∙ |s; θ) < lth}	if A(s, a)	≥	0,
C = I1{⅛⅛⅞ > 1-ε}l{y(a∣s; θ)	-	y(∙∣s; θ) > -lth}	if A(s,a)	<	0.
_ Lsum += -Cn(S) ∏o(Id(Saθ)) A(a, s) + 2[V(s; ω) - G)]2 + β PO π(a∣s; θ) log π(a∣s; θ).
_ Update θ and ω once using gradient on Lsum.
We employ an entropy loss to encourage exploration during training and hopefully the convergence
of current policy to a NE (Srinivasan et al., 2018). ACH updates θ and ω simultaneously, and the
overall loss is:
LACH = - cn(s) ∏(a(a∣; A(a, s) + 2[V(s; ω) - G)]2 + β X π(a∣s; θ) logπ(a∣s; θ).	(29)
Theoretically, y(a|s0; θ) for non-sampled states s0 should also be trained with the target y(a|s0; θoιd).
Yet, in ACH, we only update θ once using a single mini-batch at each iteration. Therefore, θ is equal
to θoιd before the update. In other words, the policy loss for non-sampled states is 0 in ACH.
24
Published as a conference paper at ICLR 2022
Figure 9: The exploitability of the current policy in ACH with different behavior policies on FHP. We
report the mean as solid curves and the range as shaded regions across 3 independent runs.
F The Effect of the Behavior Policy on ACH in FHP
As We noted in the paper, the behavior policy μp,t in ACH could be set to either the current policy
πp,t or simply a uniform sampling policy. As Corollary 1 states, a tighter bound on the exploitability
of the average policy OfNW-CFR can be obtained, if μp,t is stationary over iterations. However, since
We have decided to use the current policy (trained With an entropy regularization) for evaluation in
ACH, the effect of the behavior policy is unclear from a theoretical perspective. Hence, we conduct
an experiment to investigate this using FHP, which is a non-trivial poker benchmark but still has the
property that the exact exploitability of an agent can be efficiently computed.
In Figure 9, we compare the exploitability of the current policy of ACH on FHP, by setting the behavior
policy in ACH to either the current policy or a uniform sampling policy. From the comparison, it
seems that the performance of the current policy of ACH is not sensitive to the choice of behavior
policy. One reason might be that the additional entropy loss forces the current policy to be stable and
prone to a uniform random policy. Another reason might be that the current policy instead of the
average policy is evaluated. We will investigate this in more depth in the future.
G Additional Results on Small IIG Benchmarks in OpenSpiel
We further evaluate ACH and compare it with A2C, RPG, and NeuRD on three benchmarks from
OpenSpiel: Kuhn poker, Leduc poker, and Liar’s Dice. All the experiments were run single-threaded
on a 2.24GHz CPU. We use the network architecture provided in OpenSpiel, which has a 128-
neurons fully-connected layer followed by ReLU and two separate linear layers for the policy and the
state/action value. For A2C, RPG, and NeuRD, we use the default hyper-parameters in OpenSpiel.
ACH shares most of the hyper-parameters with A2C. All the hyper-parameters are listed in the
Appendix H.3.
The exploitability of an agent is exactly calculated using tools in OpenSpiel. For each method, we
compute the exploitability of each agent every 1e5 training steps, the results of which are plotted
in Figure 10. Clearly, ACH converges significantly faster and achieves a lower exploitability than
other methods across the three benchmarks. There is still some gap between 0 and the exploitability
ACH converges to. This may due to the neural network approximation error and the fact that we
use the current policy instead of the average policy for the evaluation. As expected, A2C has the
worst performance, since it is designed for single-agent environments. Moreover, the superiority of
ACH is most significant on the Liar’s Dice benchmark, which is the most complex one of the three
benchmarks.
As a complement, we also present the head-to-head performance of A2C, RPG, NeuRD, and ACH on
the three benchmarks in OpenSpiel. As demonstrated in Table 4, the agent of ACH won all other
agents across the three benchmarks.
25
Published as a conference paper at ICLR 2022
2.01.51.0
"三 qelo_dxa
---A2C
---RPC
---NetlRD
—ACHiOurs)
0.0	0.2	0.4	0.6	0.8	1.0
Training Steps le7
8 6 4 2
Al 三 q£o_dxa
0.0-.	.	.	.	.
0.0	0.2	0.4	0.6	0.β	1.0
Training Steps le7
(a) Kuhn poker	(b) Leduc poker	(c) Liar’s Dice
Figure 10: The training curves of each agent on the three benchmarks from OpenSpiel. We report
the mean as solid curves and the range of the exploitability across 8 independent runs as shaded
regions. We notice that there exists some discrepancy between the NeuRD results on Kuhn poker and
Leduc poker reported here and those reported in Hennes et al. (2020) and Lanctot et al. (2019). The
reason might be due to NeuRD’s sensitivity to running environments including hyper-parameters and
random seeds.
	Kuhn poker			Leduc poker			Liars Dice		
	A2C	RPG	NeuRD	A2C	RPG	NeuRD	A2C	RPG	NeuRD
RPG	-0.024±0.20	-	-	0.521±0.23	-	-	0.192±0.14	-	-
NeuRD	-0.096±0.16	-0.087±0.16	-	0.569±0.16	-0.108±0.17	-	-0.226+0.05	-0.355+0.12	-
ACH	0.104±0.05	0.115±0.05	0.118±0.05	0.495±0.22	0.050±0.14	0.117±0.15	0.275+0.05	0.122+0.05	0.407+0.06
Table 4: Mean (± standard deviation) of the average winning scores of the row agents against the
column agents. The mean and the standard deviation are estimated by 8 independent runs. In each
run, the average winning scores are obtained via 10, 000 head-to-head plays. All the agents are
selected at the 1e7th training step.
H Hyper-parameters
H.1 Hyper-parameters for the 1-on-1 Mahjong Experiment
We used the Adam optimizer (Kingma & Ba, 2014) for the experiments on the 1-on-1 Mahjong
benchmark. We performed a mild hyper-parameter search on PPO and used the best setting for the
shared hyper-parameters of all methods (PPO, RPG, NeuRD, and ACH). Table 5 gives an overview
of hyper-parameters for each method. Also, we report the performance of the current policy, instead
of the average policy, for each method.
Parameter	Range	Best
Shared		
Ratio clip (ε)	-	0.5
GAE (λ)	-	0.95
Learning rate	{2.5e-3, 2.5e-4}	2.5e-4
Discount factor (γ)	-	0.995
Value loss coefficient (α)	-	0.5
Entropy coefficient (β)	{1e-1, 1e-2}	1e-2
Batch size	{4096, 8192}	8192
NeuRD		
Logit threshold (Ith)	-	6.0
ACH		
Logit threshold (lth)	-	6.0
Hedge coefficient (η(s))	{1.0, 1e-1}	1.0
Table 5: The hyper-parameters used for the 1-on-1 Mahjong experiment.
26
Published as a conference paper at ICLR 2022
H.2 Hyper-parameters for the FHP Experiment
We used the Adam optimizer (Kingma & Ba, 2014) for the experiments on FHP. All the experiments
on FHP were run multi-threaded and synchronously using ten 2.24GHz CPUs. We update the neural
networks of ACH and other methods (A2C, RPG, and NeuRD) every 1000 episodes, with a batch
consisting all the samples collected within the latest 1000 episodes. Since the average game length of
FHP is around 2, the batch size is roughly 2000. We performed a mild hyper-parameter search for
ACH, A2C, RPG, and NeuRD, as shown in Table 6. Note that we do not need to clip the advantages
in a synchronous method, so the ratio clip hyper-parameter ε is not needed. We multiply the rewards
in FHP with a reward normalizer, as the rewards in FHP are in the range [-700, 700]. Besides, we
found that A2C, RPG, and NeuRD are very sensitive to the entropy coefficient, and we had to use a
larger entropy coefficient in these algorithms than ACH. For OS-DCFR and DREAM, we use the
same hyper-parameters presented in Steinberger et al. (2020). An overview of the hyper-parameters
on FHP is given in Table 6. Note that we report the performance of the current policy for ACH, A2C,
RPG, and NeuRD, while the average policy is used for evaluation in OS-DCFR and DREAM.
Parameter	Range	Best
Shared		
GAE (λ)	-	0.95
Learning rate	{1e-3, 1e-4}	1e-4
Discount factor (γ)	-	0.995
Value loss coefficient (α)	-	2.0
Batch size	-	≈ 2000
Entropy coefficient (β)	{1e-2, 3e-2, 5e-2}	5e-2
Reward normalizer	-	0.002
NeuRD		
Logit threshold (lth)	{2.0, 4.0}	2.0
ACH		
Logit threshold (lth)	{2.0, 4.0}	2.0
Entropy coefficient (β)	{1e-2, 3e-2, 5e-2}	3e-2
Hedge coefficient (η(s))	{1.0, 0.1}	1.0
Table 6: The hyper-parameters used for the FHP experiment.
H.3 Hyper-parameters for the Experiment on OpenSpiel
We used stochastic gradient descent with a constant learning rate for all the experiments on bench-
marks from OpenSpiel. For A2C and RPG, we used the default implementations and hyper-parameters
in OpenSpiel. NeuRD was originally implemented using the counterfactual regret in OpenSpiel. We
re-implemented NeuRD using predicted advantages and set the shared hyper-parameters of NeuRD
identical to those in RPG. All methods employ an entropy loss to encourage exploration during
training and hopefully the convergence of the current policy to a NE. Also, we report the performance
of the current policy, instead of the average policy, for each method.
Parameter	Range Best
Learning rate	{1e-3, 5e-3}	1e-3
Value loss coefficient (α)	{1.0, 2.0}	2.0
Hedge coefficient (η(s))	{1.0, 1e-1,1e-2}	1.0
Table 7: The hyper-parameter search ranges and best settings of ACH for the OpenSpiel experiment.
In the OpenSpiel implementations, the value parameters are updated separately and more frequently
compared to the policy parameters for A2C, RPG, and NeuRD. Yet, the value loss and the policy loss
are combined in ACH, and all parameters are updated simultaneously, as illustrated in Algorithm 2.
27
Published as a conference paper at ICLR 2022
Parameter	A2C	RPG	NeuRD	ACH
Batch size	4~	64	64	64
Critic learning rate	1e-4	1e-2	1e-2	-
Policy learning rate	1e-4	1e-2	1e-2	-
# Critic updates per policy update	32	32	32	-
Entropy coefficient (β)	1e-2	1e-2	1e-2	1e-2
Logit threshold (lth)	-	-	2.0	2.0
Table 8: The hyper-parameters used for the OpenSpiel experiment.
Also note that, in a single-threaded training environment, the actor and the learner run in sequence.
As a result, π(a∣s; θ) is always identical to ∏oid(a∣s) in ACH in Algorithm 2.
We performed a mild hyper-parameter search for ACH, which is illustrated in Table 7. The final
hyper-parameters used for each method are listed in Table 8, with 3 additional hyper-parameters of
ACH listed in the “Best” column in Table 7.
I	The Relationship Between ACH and Suphx
Recently, Suphx (Li et al., 2020b) has achieved stunning performance on Japanese Riichi Mahjong.
The development of Suphx is enabled by a novel integration of existing supervised learning and RL
methods in addition to some newly developed techniques. Three new techniques were introduced
in Suphx: global reward prediction, oracle guiding, and run-time policy adaptation. The global
reward prediction technique is to handle the multi-round game situation, which is irrelevant to our
1-on-1 Mahjong setting (only one round per game in our test setting). The oracle guiding technique
decays the invisible feature during training, which is of independent interest in dealing with imperfect-
information. The run-time policy adaptation technique adapts the trained policy at test time, and this
may be combined with ACH, which is a training algorithm. In summary, Suphx is more of a novel
system than a new algorithm. For this reason, we did not implement and compare Suphx with ACH in
our 1-on-1 Mahjong experiment. Nonetheless, the oracle guiding technique may be complementary
to ACH in handling imperfect-information, and we will investigate this in future work.
28