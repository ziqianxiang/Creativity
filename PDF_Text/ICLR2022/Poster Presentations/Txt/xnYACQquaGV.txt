Published as a conference paper at ICLR 2022
Neural Contextual Bandits with Deep Repre-
sentation and Shallow Exploration
Pan Xu
California Institute of Technology
panxu@caltech.edu
Zheng Wen	Handong Zhao
DeepMind	Adobe Research
zhengwen@google.com hazhao@adobe.com
Quanquan Gu
University of California, Los Angeles
qgu@cs.ucla.edu
Ab stract
We study neural contextual bandits, a general class of contextual bandits, where
each context-action pair is associated with a raw feature vector, but the specific re-
ward generating function is unknown. We propose a novel learning algorithm that
transforms the raw feature vector using the last hidden layer of a deep ReLU neu-
ral network (deep representation learning), and uses an upper confidence bound
(UCB) approach to explore in the last linear layer (shallow exploration). We prove
that under standard assumptions, our proposed algorithm achieves O( √T) finite-
time regret, where T is the learning time horizon. Compared with existing neural
contextual bandit algorithms, our approach is computationally much more effi-
cient since it only needs to explore in the last layer of the deep neural network.
1 Introduction
MUIti-armed bandits (MAB) (Auer et al., 2002; AUdibert et al., 2009; Lattimore & Szepesvari, 2020)
are a class of online decision-making problems where an agent needs to learn to maximize its ex-
pected cumulative reward by repeatedly interacting with a partially known environment. Following
a bandit algorithm (also called a strategy or policy), in each round, the agent adaptively chooses
an arm, and then receives a reward associated with that arm. Since only the reward of the chosen
arm will be observed (bandit information feedback), a good bandit algorithm has to deal with the
exploration-exploitation dilemma: trade-off between pulling the best arm based on existing knowl-
edge/history data (exploitation) and trying the arms that have not been fully explored (exploration).
In many real-world applications, the agent will also be able to access detailed contexts associated
with the arms. For example, when a company wants to choose an advertisement to present to a user,
the recommendation will be much more accurate if the company takes into consideration the con-
tents, specifications, and other features of the advertisements in the arm set as well as the profile of
the user. To encode the contextual information, contextual bandit models and algorithms have been
developed, and widely studied both in theory and in practice (Dani et al., 2008; Rusmevichientong &
Tsitsiklis, 2010; Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011). Most existing contex-
tual bandit algorithms assume that the expected reward of an arm at a context is a linear function in
a known context-action feature vector, which leads to many useful algorithms such as LinUCB (Chu
et al., 2011), OFUL (Abbasi-Yadkori et al., 2011), etc. The representation power of the linear model
can be limited in applications such as marketing, social networking, clinical studies, etc., where the
rewards are usually counts or binary variables. The linear contextual bandit problem has also been
extended to richer classes of parametric bandits such as the generalized linear bandits (Filippi et al.,
2010; Li et al., 2017) and kernelised bandits (Valko et al., 2013; Chowdhury & Gopalan, 2017).
With the prevalence of deep neural networks (DNNs) and their phenomenal performances in many
machine learning tasks (LeCun et al., 2015; Goodfellow et al., 2016), there has emerged a line
of work that employs DNNs to increase the representation power of contextual bandit algorithms
(Allesiardo et al., 2014; Riquelme et al., 2018; Collier & Llorens, 2018; Zahavy & Mannor, 2019;
1
Published as a conference paper at ICLR 2022
Zhou et al., 2020; Deshmukh et al., 2020; Zhang et al., 2020). The problems they solve are usually
referred to as neural contextual bandits. For example, Zhou et al. (2020) developed the NeuralUCB
algorithm, which can be viewed as a natural extension of LinUCB (Chu et al., 2011; Abbasi-Yadkori
et al., 2011), where they use the output of a deep neural network with the feature vector as input
to approximate the reward. Zhang et al. (2020) adapted neural networks in Thompson Sampling
(Thompson, 1933; Chapelle & Li, 2011; Russo et al., 2018) for both exploration and exploitation
and proposed NeuralTS . For a fixed time horizon T , it has been proved that both NeuralUCB
and NeuralTS achieve a O(d T ) regret bound, where d is the effective dimension of a neural
tangent kernel matrix which can potentially scale with O(TK) for K-armed bandits. This high
complexity is mainly due to that the exploration is performed over the entire huge neural network
parameter space, which is inefficient and even infeasible when the number of neurons is large. A
more realistic and efficient way of learning neural contextual bandits may be to just explore different
arms using the last layer as the exploration parameter. More specifically, Riquelme et al. (2018)
provided an extensive empirical study of benchmark algorithms for contextual-bandits through the
lens of Thompson Sampling, which suggests decoupling representation learning and uncertainty
estimation improves performance.
In this paper, we show that the decoupling of representation learning and the exploration can be
theoretically validated. We study a new neural contextual bandit algorithm, which learns a map-
ping to transform the raw features associated with each context-action pair using a deep neural
network (deep representation), and then performs an upper confidence bound (UCB)-type explo-
ration over the linear output layer of the network (shallow exploration). We prove a sublinear regret
of the proposed algorithm by exploiting the UCB exploration techniques in linear contextual ban-
dits (Abbasi-Yadkori et al., 2011) and the analysis of deep overparameterized neural networks using
neural tangent kernels (Jacot et al., 2018). Our theory confirms the empirically observed effective-
ness of decoupling the deep representation learning and the UCB exploration in contextual bandits
(Riquelme et al., 2018; Zahavy & Mannor, 2019).
Contributions we summarize the main contributions of this paper as follows.
•	We propose a contextual bandit algorithm, Neural-LinUCB, for solving a general class of con-
textual bandit problems without knowing the specific reward generating function. The proposed
algorithm learns a deep representation to transform the raw feature vectors and performs UCB-
type exploration in the last layer of the neural network, which we refer to as deep representation
and shallow exploration. Compared with LinUCB (Li et al., 2010; Chu et al., 2011) and neural
bandits such as NeuralUCB (Zhou et al., 2020) and NeuralTS (Zhang et al., 2020), our algo-
rithm enjoys the best of two worlds: strong expressiveness due to the deep representation and
computational efficiency due to the shallow exploration.
•	Despite the usage of a DNN as the feature mapping, We prove a O(√T) regret for the proposed
Neural-LinUCB algorithm, which matches the regret bound of linear contextual bandits (Chu
et al., 2011; Abbasi-Yadkori et al., 2011). To the best of our knoWledge, this is the first Work
that theoretically shoWs the convergence of bandits algorithms under the scheme of deep repre-
sentation and shalloW exploration. It is notable that a similar scheme called Neural-Linear Was
proposed by Riquelme et al. (2018) for Thompson sampling algorithms, and they empirically
shoWed that decoupling representation learning and uncertainty estimation improves the perfor-
mance. Our Work confirms this observation from a theoretical perspective.
•	We conduct experiments on contextual bandit problems based on real-World datasets, demon-
strating a better performance and computational efficiency of Neural-LinUCB over LinUCB and
existing neural bandits algorithms such as NeuralUCB, Which Well aligns With our theory.
1.1	Additional related work
There is a line of related Work to ours on the recent advance in the optimization and generalization
analysis of deep neural netWorks. In particular, Jacot et al. (2018) first introduced the neural tangent
kernel (NTK) to characterize the training dynamics of netWork outputs in the infinite Width limit.
From the notion of NTK, a fruitful line of research emerged and shoWed that loss functions of deep
neural netWorks trained by (stochastic) gradient descent can converge to the global minimum (Du
et al., 2019b; Allen-Zhu et al., 2019b; Du et al., 2019a; Zou et al., 2018; Zou & Gu, 2019). The
generalization bounds for overparameterized deep neural netWorks are also established in Arora
et al. (2019a;b); Allen-Zhu et al. (2019a); Cao & Gu (2019a;b). Recently, the NTK based analysis
2
Published as a conference paper at ICLR 2022
is also extended to the study of sequential decision problems including bandits (Zhou et al., 2020;
Zhang et al., 2020), and reinforcement learning algorithms (Cai et al., 2019; Liu et al., 2019; Wang
et al., 2020; Xu & Gu, 2020).
Our algorithm is also different from Langford & Zhang (2008); Agarwal et al. (2014) which reduce
the bandit problem to supervised learning. Moreover, their algorithms need to access an oracle that
returns the optimal policy in a policy class given a sequence of context and reward vectors, whose
regret depends on the VC-dimension of the policy class.
Notation We use [k] to denote a set {1,...,k}, k ∈ N+. kx∣∣2 = √x>x is the Euclidean norm
of a vector x ∈ Rd. For a matrix W ∈ Rm×n, we denote by kWk2 and kWkF its operator norm
and Frobenius norm respectively. For a semi-definite matrix A ∈ Rd×d and a vector x ∈ Rd, we
denote the Mahalanobis norm as ∣∣x∣∣ a = Vx>Ax. Throughout this paper, we reserve the notations
{Ci}i=0,1,... to represent absolute positive constants that are independent of problem parameters
such as dimension, sample size, iteration number, step size, network length and so on. The specific
values of {Ci}i=0,1,... can be different in different context. For a parameter of interest T and a
function f (T), We use notations such as O(f (T)) and Ω(f (T)) to hide constant factors and O(f (T))
to hide constant and logarithmic dependence ofT.
2	Preliminaries
In this section, we provide the background of contextual bandits and deep neural networks.
2.1	Linear contextual bandits
A contextual bandit is characterized by a tuple (S, A, r), where S is the context (state) space, A is
the arm (action) space, and r encodes the unknown reward generating function at all context-arm
pairs. A learning agent, who knows S and A but does not know the true reward r (values bounded
in (0, 1) for simplicity), needs to interact with the contextual bandit for T rounds. At each round
t = 1, . . . , T, the agent first observes a context st ∈ S chosen by the environment; then it needs to
adaptively select an arm at ∈ A based on its past observations; finally it receives a reward
rbt(xs,at) = r(xs,at) + ξt,	(2.1)
where xs,a ∈ Rd is a known feature vector for context-arm pair (s, a) ∈ S × A, and ξt is a random
noise with zero mean. The agent’s objective is to maximize its expected total reward over these T
rounds, which is equivalent to minimizing the pseudo regret (Audibert et al., 2009):
T
RT = E X(b(xstq) - b(xst,at)) ,	(2.2)
t=1
where a↑ ∈ argmaxa∈∕{r(xst,。) = E[b(x§t,a)]}. To simplify the exposition, we use xt,a to denote
xst ,a since it only depends on the round index t in most bandit problems, and we assume A = [K].
In linear contextual bandits, the reward function in (2.1) is assumed to have a linear structure
r(xs,a) = x>aθ* for some unknown weight vector θ* ∈ Rd. One provably sample efficient al-
gorithm for linear contextual bandits is Linear Upper Confidence Bound (LinUCB) (Chu et al.,
2011) or Optimism in the Face of Uncertainty Linear bandit algorithm (OFUL) (Abbasi-Yadkori
et al., 2011). Specifically, at each round t, LinUCB chooses the action at = argmaxa∈[K] {xt>,aθt +
αtkxt,a∣∣A-ι}, where θt is a point estimate of θ*, At = λI + Pt=1 xi,aix>,a. with some λ > 0
is a matrix defined based on the historical context-arm pairs, and αt > 0 is a tuning parameter that
controls the exploration rate in LinUCB.
2.2	Deep neural networks
In this paper, we use f(x) to denote a neural network with input data x ∈ Rd. Let L be the number
of hidden layers and Wl ∈ Rml ×ml-1 be the weight matrices in the l-th layer, where l = 1, . . . , L,
m1 = . . . = mL-1 = m and m0 = mL = d. Then a L-hidden layer neural network is defined as
f (x) = √mθ*>σL(WLσL-1 (WL-I …σ1 (WIX)…))，	(2.3)
3
Published as a conference paper at ICLR 2022
where σι is an activation function and θ* ∈ Rd is the weight of the output layer. To simplify the
presentation, we will assume σ1 = σ2 = . . . = σL = σ is the ReLU activation function, i.e.,
σ(x) = max{0, x} for x ∈ R. We denote w = (vec(W1)>, . . . , vec(WL)>)>, which is the
concatenation of the vectorized weight parameters of all hidden layers of the neural network. We
also write f (x; θ*, W) = f (x) in order to explicitly specify the weight parameters of neural network
f. It is easy to show that the dimension p of vector w satisfies p = (L - 2)m2 + 2md. To simplify
the notation, we define φ(x; W) as the output of the L-th hidden layer of neural network f.
φ(x; w) = √mσ(WLσ(WL-ι ∙∙∙ σ(Wιx) ∙∙∙)).	(2.4)
Note that φ(x; W) itself can also be viewed as a neural network with vector-valued outputs.
3	Deep Representation and S hallow Exploration
3.1	High-level Idea of the Proposed Algorithm
The linear parametric form in linear contextual bandits might produce biased estimates of the reward
due to the lack of representation power (Snoek et al., 2015; Riquelme et al., 2018). In contrast, it
is well known that deep neural networks are powerful enough to approximate an arbitrary function
(Cybenko, 1989). Therefore, a natural extension of linear contextual bandits is to use a deep neural
network to approximate the reward generating function r(∙). Nonetheless, DNNS usually have a pro-
hibitively large dimension for weight parameters, which makes the exploration in neural networks
based UCB algorithm inefficient (Kveton et al., 2020; Zhou et al., 2020).
In this work, we study a neural contextual bandit algorithm, where the hidden layers ofa deep neural
network are used to represent the features and the exploration is only performed in the last layer of
the neural network. In particular, for any arm feature vector x, we use (θ*, φ(x; w)i to approximate
the unknown reward function r(x), where φ(x; w) defined as in (2.4) is a neural network with
weight w, and θ* is a unknown weight parameter. Note that we can also view (θ*, φ(x; Wyi as a
neural network with φ(x; w) being the output of the last hidden layer and θ* the weight parameter
of the last (linear) layer. Different from existing neural bandit algorithms, we only add a UCB bonus
term involving the last layer instead of all the weight parameter of this large neural network.
This decoupling of the representation and the exploration achieves the best of both worlds: efficient
exploration of shallow (linear) models and high expressive power of deep models. In what follows,
we will describe a neural contextual bandit algorithm that uses the output of the last hidden layer of
a neural network to transform the raw feature vectors (deep representation) and performs UCB-type
exploration in the last layer of the neural network (shallow exploration). Since the exploration is
performed only in the last linear layer, we call this procedure Neural-LinUCB, which is displayed
in Algorithm 1.
3.2	Detailed Implementation of Deep Representation and Shallow Exploration
Now we describe the details of Algorithm 1. In round t, the agent receives an action set with raw
features Xt = {xt,1, . . . , xt,K}. Then the agent chooses an arm at that maximizes the following
upper confidence bound:
at = argmax { hφ(xt,k； wt-i), θt-1 + αt∣∣φ(xt,k; wt-i)∣∣A-ι 卜,
k∈[K]	t-1
(3.1)
where θt-1 is a point estimate of the unknown weight in the last layer, φ(x; w) is defined as in (2.4),
wt-1 is an estimate of all the weight parameters in the hidden layers of the neural network, αt > 0
is the algorithmic parameter controlling the exploration, and At defined as follows.
t
At = λI+	φ(xi,ai; wi-1)φ(xi,ai; wi-1)>,	(3.2)
i=1
and λ > 0. After pulling arm at, the agent will observe a noisy reward rbt := rb(xt,at) = r(xt,k)+ξt,
where ξt is an independent ν-subGaussian random noise for some ν > 0 and r(∙) is an unknown
reward function. In this paper, we will interchangeably use notation rbt to denote the reward received
at the t-th step and an equivalent notation rb(x) to express its dependence on the feature vector x.
4
Published as a conference paper at ICLR 2022
Upon receiving the reward rbt, the agent updates its estimate θt of the output layer weight by using
the same '2-regularized least-squares estimate in linear contextual bandits (AbbaSi-YadkOri et al.,
2011). In particular, we have θt = A-1bt, where bt = Pt=ι bφ(xig； Wi-ι).
To save the computation, the neural network φ(∙; Wt) will be updated once every H steps. Therefore,
we have W(q-1)H+1
wqH for q = 1, 2, ..... We call the time steps {(q - 1)H + 1, . . . , qH}
an epoch with length H . At time step t = Hq, we will retrain the neural network based on all the
historical data via Algorithm 2, which minimizes the following empirical loss function:
qH
Lq(W) = X (θ>φ(xi,ai ； W)- bi/
(3.3)
i=1
In practice, one can further save computational cost by only feeding data {xi,ai , rbi , θi }iq=(q-1)H+1
from the q-th epoch into Algorithm 2 to update the parameter Wt , which does not hurt the perfor-
mance since the historical information has been encoded into the estimate ofθi. In this paper, we will
perform the following gradient descent step Wqs) = Wqs-I) - ηqVwLq(w(s-1)), for S = 1,...,n,
where Wq(0) = W(0) is chosen as the same random initialization point. We will discuss more about
the initial point W(0) in the next paragraph. Then Algorithm 2 outputs W(qn) and we set it as the
updated weight parameter WHq+1 in Algorithm 1. In the next round, the agent will receive another
action set Xt+1 with raw feature vectors and repeat the above steps to choose the sub-optimal arm
and update estimation for contextual parameters.
Initialization: Recall that W is the collection of all hidden layer weight parameters of the neural net-
work. We will follow the same initialization scheme as used in Zhou et al. (2020), where each entry
of the weight matrices follows some Gaussian distribution. Specifically, for any l ∈ {1, . . . , L - 1},
we set Wl
W0
0W
, where each entry ofW follows distribution N(0, 4/m) independently; for
WL, we set it as [V -V], where each entry of V follows distribution N(0, 2/m) independently.
Comparison with LinUCB and NeuralUCB: Compared with linear contextual bandits in Sec-
tion 2.1, Algorithm 1 has a distinct feature that it learns a deep neural network to obtain a deep
representation of the raw data vectors and then performs UCB exploration. This deep representa-
tion allows our algorithm to characterize more intrinsic and latent information about the raw data
{xt,k}t∈[τ],k∈[κ] ⊂ Rd. However, the increased complexity of the feature mapping φ(∙; w) also
introduces great hardness in training. For instance, a recent work by Zhou et al. (2020) also stud-
ied the neural contextual bandit problem, but different from (3.1), their algorithm (NeuralUCB)
performs the UCB exploration on the entire network parameter space, which is Rpe+d, where
pe = m + md + (L - 1)m2 . Note that in Zhou et al. (2020), they need to compute the inverse
of a matrix Zt ∈ R(pe+d)×(pe+d), which is defined in a similar way to the matrix At in our paper
except that Zt is defined based on the gradient of the network instead of the output of the last hidden
layer as in (3.2). In sharp contrast, At in our paper is only of size d × d and thus is much more
efficient and practical in implementation, which will be seen from our experiments in later sections.
We note that there is also a similar algorithm to our Neural-LinUCB presented in Deshmukh et al.
(2020), where they studied the self-supervised learning loss in contextual bandits with neural net-
work representation for computer vision problems. However, no regret analysis has been provided.
When the feature mapping φ(∙; w) is an identity function, the problem reduces to linear contextual
bandits where we directly use xt as the feature vector. In this case, it is easy to see that Algorithm 1
reduces to LinUCB (Chu et al., 2011) since we do not need to learn the representation parameter W.
Comparison with Neural-Linear: The high-level idea of decoupling the representation and explo-
ration in our algorithm is also similar to that of the Neural-Linear algorithm (Riquelme et al., 2018;
Zahavy & Mannor, 2019), which trains a deep neural network to learn a representation of the raw
feature vectors, and then uses a Bayesian linear regression to estimate the uncertainty in the bandit
problem. However, these two algorithms are significantly different since Neural-Linear (Riquelme
et al., 2018) is a Thompson sampling based algorithm that uses posterior sampling to estimate the
weight parameter θ* via Bayesian linear regression, whereas Neural-LinUCB adopts upper confi-
dence bound based techniques to estimate the weight θ*. Nevertheless, both algorithms share the
same idea of deep representation and shallow exploration, and we view our Neural-LinUCB algo-
rithm as one instantiation of the Neural-Linear scheme.
5
Published as a conference paper at ICLR 2022
Algorithm 1 Deep Representation and Shallow Exploration (Neural-LinUCB)
1:	Input: regularization parameter λ > 0, number of total steps T, episode length H, exploration
parameters {αt > 0}t∈[T]
2:	Initialization: A0 = λI, b0 = 0; entries of θ0 follow N(0, 1/d), and w(0) is initialized as
described in Section 3; q = 1; w0 = w(0)
3:	for t = 1, . . . , T do
4:	receive feature vectors {xt,1, . . . , xt,K}
5:	choose arm at = argmaxk∈[κ] θ[-∖φ(xt,k； wt-ι) +αtkφ(xt,k; Wt-I)Ila-1j and obtain
reward rbt
6:	update At and bt as follows:
At = At-1 + φ(xt,at; wt-1)φ(xt,at; wt-1)>, bt = bt-1 + rbtφ(xt,at; wt-1),
7:	update θt = At-1bt
8:	if mod(t, H) = 0 then
9:	Wt J output of Algorithm 2
10:	q =	q + 1
11:	else
12:	wt = wt-1
13:	end	if
14:	end for
15:	Output wT
Algorithm 2 Update Weight Parameters with Gradient Descent
1:	Input: initial point wq(0) = w(0), maximum iteration number n, step size ηq, and loss function
defined in (3.3).
2:	for s = 1, . . . , n do
3:	Wqs) = Wqs-I) - ηq RwLq (WqsT)).
4:	end for
5:	Output W(qn)
4 Main Results
To analyze the regret bound of Algorithm 1, we first lay down some important assumptions on the
neural contextual bandit model.
Assumption 4.1. For all i ≥ 1 and k ∈ [K], we assume that Ixi,k I2 = 1 and its entries satisfy
[xi,k]j = [xi,k]j +d/2.
The assumption that Ixi,kI2 = 1 is not essential and is only imposed for simplicity, which is also
used in Zou & Gu (2019); Zhou et al. (2020). The condition on the entries of xi,k is also mild
since otherwise We could always construct χi,k = [x>k, x>,k]>/√2 to replace it. An implication of
Assumption 4.1 is that the initialization scheme in Algorithm 1 results in φ(xi,k; W(0)) = 0 for all
i ∈ [T] andk ∈ [K].
We assume the following stability condition on the spectral norm of the neural network gradient:
Assumption 4.2. There is a constant 'Lip > 0 such that it holds ∣∣∂dφ(x; wo) 一 ∂dφ(x0; w。)' ≤
'Lip∣x — x0∣∣2 for all x, x0 ∈ {xi,k}i∈[τ],k∈[κ].
The inequality in Assumption 4.2 resembles the Lipschitz condition on the gradient of the neural
network. However, it is essentially different from the smoothness condition since here the gradient
is taken with respect to the neural network weights while the Lipschitz condition is imposed on the
feature parameter x. Similar conditions are widely made in nonconvex optimization (Wang et al.,
2014; Balakrishnan et al., 2017; Xu et al., 2017), in the name of first-order stability, which is essential
to derive the convergence of alternating optimization algorithms. Furthermore, Assumption 4.2 is
only required on the TK training data points and a specific weight parameter W0 . Therefore, the
condition will hold if the raw feature data lie in a certain subspace of Rd. We provided some further
discussions in the supplementary material about this assumption for interested readers.
6
Published as a conference paper at ICLR 2022
In order to analyze the regret bound of Algorithm 1, we need to characterize the properties of the
deep neural network in (2.3) that is used to represent the feature vectors. Following a recent line of
research (Jacot et al., 2018; Cao & Gu, 2019a; Arora et al., 2019b; Zhou et al., 2020), we define the
covariance between two data point x, y ∈ Rd as follows.
Σe (0)(x, y) = Σ(0)(x, y) = x>y,
Σl-1(x, x) Σl-1(x, y)
Σl-1(y, x) Σl-1(y, y) ,
ς,)(X, y) = 2E(u,v)~N(0,Λ(l-1)(x,y)) [σ(U)σ(V)],
∑ (l)(χ, y) = 2∑ (IT)(χ, y)Eu,v [σ(u)σ(v)] + ς(I)(x, y),	(4.1)
where (u, V)〜 N(0, A(IT) (x, y)), and σ(∙) is the derivative of activation function. We denote the
neural tangent kernel (NTK) matrix H ∈ RTK×TK based on all feature vectors {xt,k}t∈[T],k∈[K] .
Renumbering {xt,k}t∈[T],k∈[K] as {xi}i=1,...,T K, then each entry Hij is defined as
Hij = 2 (∑(L)(Xi, Xj) + ∑(L)(Xi, Xj)),	(4.2)
for all i, j ∈ [T K]. Based on the above definition, we impose the following assumption on H.
Assumption 4.3. The neural tangent kernel defined in (4.2) is positive definite, i.e., λmin(H) ≥ λ0
for some constant λ0 > 0.
Assumption 4.3 essentially requires the neural tangent kernel matrix H to be non-singular, which is
a mild condition and also imposed in other related work (Du et al., 2019a; Arora et al., 2019b; Cao
& Gu, 2019a; Zhou et al., 2020). Moreover, it is shown that Assumption 4.3 can be easily derived
from Assumption 4.1 for two-layer ReLU networks (Oymak & Soltanolkotabi, 2020; Zou & Gu,
2019). Therefore, Assumption 4.3 is mild or even negligible given the non-degeneration assumption
on the feature vectors. Also note that matrix H is only defined based on layers l = 1, . . . , L of the
neural network, and does not depend on the output layer θ. It is easy to extend the definition of H
to the NTK matrix defined on all layers including the output layer θ, which would also be positive
definite by Assumption 4.3 and the recursion in (4.2).
Before we present the regret analysis of the neural contextual bandit, we need to modify the regret
defined in (2.2) to account for the randomness of the neural network initialization. For a fixed time
horizon T, we define the regret of Algorithm 1 as follows.
T
RT = E X(b(χtq) - b(xt,at))∣w⑼,	(4.3)
t=1
where the expectation is taken over the randomness of the reward noise. Note that RT defined
in (4.3) is still a random variable since the initialization of Algorithm 2 is randomly generated.
Now we are going to present the regret bound of the proposed algorithm.
Theorem 4.4. Suppose Assumptions 4.1, 4.2 and 4.3 hold. Assume that ∣∣θ*k2 ≤ M for some
positive constant M > 0. For any δ ∈ (0, 1), let us choose αt in Neural-LinUCB as
at = V^2(dlog(1 + t log(HK)∕λ) +log(1∕δ)) + 火1/M.
We choose the step size ηq of Algorithm 2 as ηq ≤ Co (d2mnT5∙5L6 log(TK∕δ)) 1 and the width
of the neural network satisfies m = POly(L, d, 1∕δ, H, log(TK∕δ)). With probability at least 1 - δ
over the randomness of the initialization of the neural network, it holds that
Rt ≤ Cιατ Jτdlog(1 + Td2) + 2Lip
L3d5r2T Jlog mlog( 1 )log(TK) Ilr - e∣∣H-ι
m1/6
where constants {Ci}i=0,1,2 are independent of the problem, r = (r(X1), r(X2), . . . , r(XTK))> ∈
RTK ande = (f(x1;θo, wo), ...,f(XTK; θτ -1, WT-1))> ∈ RTK, and ∣∣r∣∣A = √r>Ar.
7
Published as a conference paper at ICLR 2022
Remark 4.5. Theorem 4.4 shows that the regret of Algorithm 1 can be bounded by two parts: the
first part is of order O( √T), which resembles the regret bound of linear contextual bandits (Abbasi-
Yadkori et al., 2011); the second part is of order O(m-1/6TP(r - e)>H-1(r - e)), which de-
pends on the estimation error of the neural network f for the reward generating function r and the
neural tangent kernel H.
It is worth noting that our theoretical analysis depends on the reward structure assumption that
r(∙) = (θ*, ψ(∙)i. However, the linear structure between θ* and ψ(∙) is not essential. As long as
the deep representation of the feature vector and the uncertainty weight parameter can be decoupled,
Algorithm 1 can be easily extended to settings with milder assumptions on the reward structure such
as generalized linear models (Sarkar, 1991; Filippi et al., 2010; Li et al., 2017; Kveton et al., 2020).
For more general bandit models where no assumption is imposed to the reward generating function,
it is still unclear whether the decoupled deep representation and shallow exploration would work
especially in cases a thorough exploration may be needed.
Based on the result in Theorem 4.4, we can easily verify the following conclusion:
Corollary 4.6. Under the same conditions of Theorem 4.4, if we choose a sufficiently overpa-
rameterized neural network mapping φ(∙) such that m ≥ T3, then the regret of Algorithm 1 is
RT = Oe(√TP(r - C)>H-1(r - e)).
Remark 4.7. For the ease of presentation, let us denote E := kr - erkH-1 . Ifwe have E = O (1), the
total regret in Theorem 4.4 becomes O(√T) which matches the regret of linear contextual bandits
(Abbasi-Yadkori et al., 2011). We remark that there is a similar assumption in Zhou et al. (2020)
where they assume that r>H-1r can be upper bounded by a constant. They show that this term
can be bounded by the RKHS norm of r if it belongs to the RKHS induced by the neural tangent
kernel (Arora et al., 2019a;b; Lee et al., 2019). In addition, E here is the difference between the true
reward function and the neural network function, which can also be small if the deep neural network
function well approximates the reward generating function r(∙).
5	Experiments
In this section, we provide empirical evaluations of Neural-LinUCB on real-world datasets. As we
have discussed in Section 3, Neural-LinUCB could be viewed as an instantiation of the Neural-
Linear scheme studied in Riquelme et al. (2018) except that we use the UCB exploration instead
of the posterior sampling exploration therein. Note that there has been an extensive comparison
(Riquelme et al., 2018) of the Neural-Linear methods with many other baselines such as greedy
algorithms, Variational Inference, Expectation-Propagation, Bayesian Non-parametrics and so on.
Therefore, we do not seek a thorough empirical comparison of Neural-LinUCB with all existing
bandits algorithms. In this experiment, we only aim to validate the advantages of our algorithm over
the following baselines: (1) Neural-Linear (Riquelme et al., 2018); (2) LinUCB (Chu et al., 2011),
which does not have a deep representation of the feature vectors; (3) NeuralUCB (Zhou et al., 2020),
and (4) NeuralTS (Zhang et al., 2020) which perform UCB/TS exploration on all the parameters of
the neural network. All numerical experiments were run on a workstation with Intel(R) Xeon(R)
CPU E5-2637 v4 @ 3.50GHz.
Datasets: we evaluate the performances of all algorithms on bandit problems created from real-
world data. Specifically, following the experimental setting in Zhou et al. (2020),we use datasets
(Shuttle) Statlog, Magic and Covertype from UCI machine learning repository (Dua & Graff, 2017),
and the MINST dataset from LeCun et al. (1998). The details of these datasets are presented in Table
1. In Table 1, each instance represents a feature vector x ∈ Rd that is associated with one of the K
arms, and dimension d is the number of attributes in each instance.
Implementations: for LinUCB, we follow the setting in Li et al. (2010) to use disjoint models
for different arms. For neural network based algorithms, we use a ReLU neural network defined
as in (2.3) with L = 2 and m = 100 for the UCI datasets (Statlog, Magic, Covertype). Thus the
neural network weights are W1 ∈ Rm×d, W2 ∈ Rk×m, and θ ∈ Rk respectively, where k = 100,
m = 100, and d is the dimension of features in the corresponding task. Since the problem size of
the MNIST dataset is larger, inspired by Hinton & Salakhutdinov (2006), we use a deeper NN and
set L = 3, k = 100 and m = 100, with weights W1 ∈ Rm×d, W2 ∈ Rm×m, W3 ∈ Rk×m,
8
Published as a conference paper at ICLR 2022
Table 1: Specifications of datasets from the UCI machine learning repository and the MNIST dataset
used in this paper.
	Statlog	Magic	Covertype	MNIST
Number of attributes	9	11	54	784
Number of arms	7	2	7	10
Number of instances	58,000	19,020	581,012	60,000
Figure 1: The cumulative regrets of LinUCB, NeuralUCB, Neural-Linear and Neural-LinUCB over
15, 000 rounds. Experiments are averaged over 10 repetitions.
and θ ∈ Rk. We set the time horizon T = 15, 000, which is the total number of rounds for each
algorithm on each dataset. We use stochastic gradient decent to optimize the network weights, with
a step size ηq =1e-5 and maximum iteration number n = 1, 000. To speed up the training process,
the network parameter w is updated every H = 100 rounds starting from round 2000. We also apply
early stopping when the loss difference of two consecutive iterations is smaller than a threshold of
1e-6. We set λ = 1 and αt = 0.02 for all algorithms, t ∈ [T]. For NeuralUCB and NeuralTS, since
it is computationally unaffordable to perform the original UCB exploration as displayed in Zhou
et al. (2020), we follow their experimental setting to replace the matrix Zt ∈ R(d+pe)×(d+pe) in their
papers with its diagonal matrix.
Results: we plot the cumulative regret of all algorithms versus round in Figures 1(a), 1(b) and 1(c)
for UCI datasets and in Figure 1(d) for MNIST. The results are reported based on the average of 10
repetitions over different random shuffles of the datasets. It can be seen that algorithms based on
neural network representations (NeuralUCB, NeuralTS, Neural-Linear and Neural-LinUCB) consis-
tently outperform the linear contextual bandit method LinUCB, which shows that linear models may
lack representation power and find biased estimates for the underlying reward generating function.
Furthermore, our proposed Neural-LinUCB achieves a comparable regret with NeuralUCB in all ex-
periments despite the fact that our algorithm only explores in the output layer of the neural network,
which is more computationally efficient as we will show in the sequel.The results in our experiment
are well aligned with our theory that deep representation and shallow exploration are sufficient to
guarantee a good performance of neural contextual bandit algorithms, which is also consistent with
the findings in existing literature (Riquelme et al., 2018) that decoupling the representation learning
and uncertainty estimation improves the performance.
We also conducted experiments to study the effects of different widths of deep neural networks on
the regret performance and to show the computational efficiency of Neural-LinUCB compared with
existing neural bandit algorithms. Due to the space limit, we defer the results to Appendix A.
6 Conclusions
In this paper, we propose a new neural contextual bandit algorithm called Neural-LinUCB, which
uses the hidden layers of a ReLU neural network as a deep representation of the raw feature vectors
and performs UCB type exploration on the last layer of the neural network. By incorporating tech-
niques in liner contextual bandits and neural tangent kernels, we prove that the proposed algorithm
achieves a sublinear regret when the width of the network is sufficiently large. This is the first regret
analysis of neural contextual bandit algorithms with deep representation and shallow exploration,
which have been observed in practice to work well on many benchmark bandit problems (Riquelme
et al., 2018). We also conducted experiments on real-world datasets to demonstrate the advantage
of the proposed algorithm over LinUCB and existing neural contextual bandit algorithms.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank the anonymous reviewers for their helpful comments. PX and QG are partially supported
by the National Science Foundation CAREER Award 1906169 and IIS-1904183. The views and
conclusions contained in this paper are those of the authors and should not be interpreted as repre-
senting any funding agencies.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, pp. 1638-1646, 2014.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, pp. 6155-6166, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019b.
Robin Allesiardo, Raphael Feraud, and Djallel Bouneffouf. A neural networks committee for the
contextual bandit problem. In International Conference on Neural Information Processing, pp.
374-381. Springer, 2014.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8139-8148, 2019b.
Jean-Yves Audibert, Remi Munos, and Csaba Szepesvari. Exploration-exploitation tradeoff using
variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876-1902,
2009.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235-256, 2002.
Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em
algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120,
2017.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning con-
verges to global optima. In Advances in Neural Information Processing Systems, 2019.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10835-10845,
2019b.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
neural information processing systems, pp. 2249-2257, 2011.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, pp. 844-853, 2017.
10
Published as a conference paper at ICLR 2022
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff func-
tions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics,pp. 208-214, 2011.
Mark Collier and Hector Urdiales Llorens. Deep contextual multi-armed bandits. arXiv preprint
arXiv:1807.09809, 2018.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. In Conference on Learning Theory, 2008.
Aniket Anand Deshmukh, Abhimanu Kumar, Levi Boyles, Denis Charles, Eren Manavoglu,
and Urun Dogan. Self-supervised contextual bandits in computer vision. arXiv preprint
arXiv:2003.08485, 2020.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Sarah Filippi, Olivier Cappe, AUrelien Garivier, and Csaba Szepesvari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, pp. 586-594,
2010.
Ian Goodfellow, YoshUa Bengio, and Aaron CoUrville. Deep learning. MIT press, 2016.
Geoffrey E Hinton and RUslan R SalakhUtdinov. RedUcing the dimensionality of data with neUral
networks. science, 313(5786):504-507, 2006.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neUral networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and
Craig Boutilier. Randomized exploration in generalized linear bandits. In International Confer-
ence on Artificial Intelligence and Statistics, pp. 2066-2076, 2020.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in neural information processing systems, pp. 817-824, 2008.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570-8581,
2019.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pp. 661-670, 2010.
11
Published as a conference paper at ICLR 2022
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-
tual bandits. In International Conference on Machine Learning, pp. 2071-2080, 2017.
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimiza-
tion attains globally optimal policy. In Advances in Neural Information Processing Systems, pp.
10564-10575, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 2020.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An em-
pirical comparison of bayesian deep networks for thompson sampling. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
SyYe6k-CW.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395-411, 2010.
Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. Foundations and TrendsR in Machine Learning, 11(1):1-96, 2018. ISSN
1935-8237.
Jyotirmoy Sarkar. One-armed bandit problems with covariates. The Annals of Statistics, pp. 1978-
2002, 1991.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep
neural networks. In International conference on machine learning, pp. 2171-2180, 2015.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Michal Valko, Nathan Korda, Remi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time anal-
ysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth Conference on Uncer-
tainty in Artificial Intelligence, pp. 654-663, 2013.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=BJgQfkSYDS.
Zhaoran Wang, Han Liu, and Tong Zhang. Optimal computational and statistical rates of conver-
gence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164, 2014.
Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function ap-
proximation. In International Conference on Machine Learning, 2020.
Pan Xu, Jian Ma, and Quanquan Gu. Speeding up latent variable gaussian graphical model esti-
mation via nonconvex optimization. In Advances in Neural Information Processing Systems, pp.
1933-1944, 2017.
Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. arXiv
preprint arXiv:2010.00827, 2020.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, 2020.
12
Published as a conference paper at ICLR 2022
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2053-2062, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
13
Published as a conference paper at ICLR 2022
A Additional Experimental Results
In this section, we provide more experimental results that are omitted in Section 5 due to space limit.
A. 1 Computational Efficiency of Neural-LinUCB
Throughout the experiments, our Neural-LinUCB algorithm is much more computationally efficient
than NeuralUCB since we only perform the UCB exploration on the last layer of the neural network,
where the dimension is much lower. In specific, on the Statlog dataset, it takes on average 1.11
seconds for NeuralUCB to finish 100 rounds (one epoch in Algorithm 1) and achieve the regret in
Figure 1(a), while it only takes 0.58 seconds for Neural-LinUCB to finish 100 rounds and achieve
the comparable or even better regret in Figure 1(a). On the Magic dataset, the average runtimes
for 100 rounds of NeuralUCB and Neural-LinUCB are 1.32 seconds and 0.81 seconds respectively.
On the Covertype dataset, the runtimes of NeuralUCB and Neural-LinUCB are 1.02 seconds and
0.66 seconds respectively. And on the MNIST dataset, the average runtimes for 100 rounds of
NeuralUCB and Neural-LinUCB are 4.67 seconds and 1.29 seconds respectively. For practical
applications in the real-world with larger problem sizes, we believe that the improvement of our
algorithm in terms of the computational efficiency will be more pronounced.
As we discussed in Section 5 and in the above paragraph as well, the computational efficiency of
Neural-LinUCB mainly stems from the design of shallow exploration. This is because in UCB based
bandit algorithms we need to compute the inverse of matrix A at every time step for arm selection
(Line 5 of Algorithm 1). Due to the large width of the neural network used in practice, the arm
selection operation could be rather time consuming. However, the neural network weight can be
updated periodically (i.e., in our paper it is only updated every H steps). To validate our analysis
on computational efficiency, we further studied the time profiling of the experiments conducted on
MNIST to compared our proposed algorithm with NeuralUCB in more details.
Table 2: Profiling experiment on MNIST for running 100 rounds: runtime (seconds) for different
algorithms on arm selection and network weight update.
Operations	NeuralUCB	NeUral-LinUCB
Arm selection (Line 5 in Algorithm 1)	3.60	0.28
Network weight update (Line 9 in Algorithm 1)	0.96	0.92
In particular, the setting is the same as that in Section 5 for MNIST experiments. We record the
time cost of the most expensive two subroutines: (1) the operation of arm selection (Line 5 in
Algorithm 1); and (2) the operation of updating the neural network weights (Line 9 in Algorithm
1), for H = 100 rounds. The time cost is presented in Table 2. For Neural-LinUCB, the arm
selection operation takes about 0.28 seconds (this is 21.71% of the total time cost by the algorithm
in these H = 100 rounds), among which the matrix inverse step only takes 0.17 seconds. For
NeuralUCB, the arm selection operation takes 3.60 seconds (this is 77.19% of the total time time
cost by NeuralUCB for H = 100 rounds). Therefore, the operation of arm selection in NeuralUCB
is much (almost 13 times) more time consuming than that in Neural-LinUCB. Moreover, since
the UCB matrix Zt in NeuralUCB is defined as Vf(x; w)Vf(x; w)>, it needs to compute the
gradients via back-propagation (0.93 seconds) and compute the matrix inverse (1.54 seconds), while
our Neural-LinUCB algorithm only needs to compute the matrix inverse of a small matrix (0.17
seconds). To summarize, our method is much more computationally efficient.
A.2 Impact of Large Widths
Note that the requirement of width m in our Theorem 4.4 is extremely high. On one hand, our the-
ory may be too conservative since the current understanding of deep learning is still very limited in
the field. We believe our work is a good starting point towards understanding the behavior of deep
bandits algorithms. On the other hand, we would also like to investigate the impact of mild overpa-
rameterization on the regret performance of Neural-LinUCB in practice. Therefore, we conducted
additional experiments on the Statlog dataset with wider neural networks. In particular, the neural
14
Published as a conference paper at ICLR 2022
10」69」Φ>⅛-DE3
O
O 2000 4000 6000 8000 10000 12000 14000
round
Figure 2: Performance of Neural-LinUCB with different widths on Statlog dataset.
network parameters are listed as follows
W1 ∈ Rm×d, W2 ∈ Rm×m,...,WL ∈ Rk×m, θ ∈ Rk,
where L is the depth, k = 100, d is the feature dimensions, and m is the width. We conducted
experiments for the following settings: (1) L = 2, m = 100, and thus the hidden layer width is
(100); (2) L = 2, m = 1000, and thus the hidden layer width is (1000); (3) L = 2, m = 10000, and
thus the hidden layer width is (10000); (4) L = 2, and the hidden layer width is (50000); (5) L = 3,
m = 10000, and thus the hidden layer width is (10000, 10000); and (6) L = 4, m = 10000, and thus
the hidden layer width is (10000, 10000, 10000). The results are plotted in Figure A.2. We observe
that the performance of our Neural-LinUCB algorithm is not negatively impacted by the width of
the neural network. In fact, Figure A.2 shows improved performance of Neural-LinUCB when the
total number of hidden nodes increases. This is consistent to the observations in Zhang et al. (2017)
that an overparameterized neural network trained by gradient descent does not necessarily lead to
overfitting and also aligns with our Theorem 4.4 that the regret bound of Neural-LinUCB decreases
as the width m increases.
B More Discussions on Assumption 4.2
In this section, we are going to show that Assumption 4.2 could be satisfied as long as the feature
vectors {x} lie in a begin subspace of Rd. Let us start with the case that φ : Rd → Rm is a
two-layer ReLU neural network with vector output. In particular, we define φ(x; w) as follows
φ(x; w) = σ(W2σ(W1x)), where w = (vec(W1),vec(W2))>,W1 ∈ Rm×d, W2 ∈ Rd×m,and
σ is the ReLU activation function applied elementwise. We use ui> to denote the i-th row ofW1 and
thus W1 = (u1 , . . . , um)>, where ui ∈ Rd, ∀i ∈ [m]. Similarly, we have W2 = (v1, . . . , vd)>,
where vj ∈ Rm is the j-th row of W2, ∀j ∈ [d]. Let us denote h as the vector σ(W1x). We thus
obtain
1{u1>x ≥ 0}u1>x
h =	.
.
1{u>mx ≥ 0}u>mx
1{v>h ≥ 0}v>h
Φ(x; W)=	.
1{vd>h ≥ 0}vd>h
We use φl(x; W) to denote the l-th entry of vector φ(x; W), for any l ∈ [d]. Then it holds that
∂φ I(W)=ι{v>h ≥ 0} IvI ι{u>χ ≥ 0}χ>,...,vm 1{UmX ≥ 0}χ>],
∂ vec(W1 )
for all l ∈ [d], where v1i is the i-th element in v1, i ∈ [m]. This further implies that
r1{v>h ≥ 0}(v1 ι{u>χ ≥ 0}χ>,...,vm 1{UmX ≥ 0}χ>)"
dφ(X； W) =	.	∈ Rd×md
∂νec(Wι)	丁 / , 丁 . 丁	丁	丁、
.ι{v>h ≥ 0}(v11{u>χ ≥ 0}χ>,...,vm 1{UmX ≥ 0}χ>).
15
Published as a conference paper at ICLR 2022
Similarly, we can compute the gradient of φ(x; w) with respect to W2. In particular, we have
dφ1(x; Ww = ι{v>h ≥ 0}[ 1{u>χ ≥ 0}u>χ,..., 1{umx ≥ 0}umχ],
∂ vec(v1)
∂φι(χ; W)
∂vec(vj)
[0,...,0], j 6= 1.
Therefore, the gradient of φ(χ; W) with respect to W2 is
∂ φ(χ; W)
∂vec(W2)
一∂φι(x;W)
∂vec(vι)
0>
∂φd(x;W)
∂vec(vd),
∈ Rd×md .
Lastly, we have
dφ(x; w) — h ∂φ(x;W)	∂φ(xγw) i d×(md+md)
一∂W — = [∂vec(Wι)	∂vec(W2)	∈ R
Therefore, for any two feature vectors χ and χ0 from {χi,k}i∈[T],k∈[K], if many nodes in the initial
neural network φ(χ; W0) are activated or deactivated at the same time for both χ and χ0, then the
spectral norm of the matrix dφ∂WW0) - dφ(XWW0) would satisfy the condition in Assumption 4.2. A
more thorough study of this stability condition is out of the scope of this paper, though it would be
an interesting open direction in the theory of deep neural networks.
C Proof of the Main Results
In this section, we provide the proof of the regret bound for Neural-LinUCB. Recall that in neural
contextual bandits, we do not assume a specific formulation of the underlying reward generating
function r(∙). Instead, We use deep neural networks defined in Section 2.2 to approximate r(∙). We
will first show that the reward generating function r(∙) can be approximated by the local linearization
of the overparameterized neural network near the initialization weight W(0). In particular, we denote
the gradient of φ(χ; W) with respect to W by g(χ; W), namely,
g(x; w) = Vwφ(x; w),	(C.1)
which is a matrix in Rd×p. We define φj (χ; W) to be the j-th entry of vector φ(χ; W), for any
j ∈ [d]. Then, we can prove the following lemma.
Lemma C.1. Suppose Assumptions 4.3 hold. Then there exists w* ∈ Rp such that ∣∣w* -W(O) ∣∣2 ≤
1 /√mp(r - e)>H-1(r - e) and it holds that
r(xt,k) = θ*>φ(xt,k； wt-1) + θ>g(xt,k; W(O)) (w* — W(O)),
for all k ∈ [K] andt = 1, .. .,T.
Lemma C.1 implies that the reward generating function r(∙) at points {xi,k}i∈[τ],k∈[κ] can be ap-
proximated by a linear function around the initial point w(O). Note that a similar lemma is also
proved in Zhou et al. (2020) for NeuralUCB.
The next lemma shows the upper bounds of the output of the neural network φ and its gradient.
Lemma C.2. Suppose Assumptions 4.1 and 4.3 hold. For any round index t ∈ [T], suppose it is
in the q-th epoch of Algorithm 2, i.e., t = (q - 1)H + i for some i ∈ [H]. If the step size ηq in
Algorithm 2 satisfies
≤	C__________
η 一 d2mnT5.5L6 log(TK∕δ)，
and the width of the neural network satisfies
m ≥ max{Llog(TK∕δ),dL2 log(m∕δ),δ-6H 18L16 log3(TK)},	(C.2)
16
Published as a conference paper at ICLR 2022
then, with probability at least 1 - δ we have
kwt -w(0)k2
kg(xt,k; w(0))kF
kφ(X； wt)k2
δ3/2
≤ C1 dLm,
≤ √dlog(n) log(TK∕δ),
for all t ∈ [T], k ∈ [K], where the neural network φ is defined in (2.4) and its gradient is defined
in (C.1).
The next lemma shows that the neural network φ(X; w) is close to a linear function in terms of the
weight w parameter around a small neighborhood of the initialization point w(0) .
Lemma C.3 (Theorems 5 in Cao & Gu (2019b)). Let w, w0 be in the neighborhood of w0, i.e.,
w, w0 ∈ B(w0, ω) for some ω > 0. Consider the neural network defined in (2.4), if the width m
and the radius ω of the neighborhood satisfy
m ≥ C0 max{dL2 log(m∕δ), “-4/3L-8/3 log(TK) log(m∕(ωδ))},
ω ≤ C1 L-5 (log m)-3/2,
then for all X ∈ {Xt,k}t∈[T],k∈[K], with probability at least 1 - δ it holds that
∣φj(x; W) — φj(x; w)| ≤ C2ω4/3L3d-1 /2pmlogm,
where φj (x; W) is the linearization of φj (x; W) at W0 defined as follow:
φj(x; w) = φj(x; W0) + (Vwφj(x; w0), w - w0).	(C.3)
Similar results on the local linearization of an overparameterized neural network are also presented
in Allen-Zhu et al. (2019b); Cao & Gu (2019b).
For the output layer θ*, We perform a UCB type exploration and thus We need to characterize the
uncertainty of the estimation. The next lemma shows the confidence bound of the estimate θt in
Algorithm 1.
Lemma C.4. Suppose Assumption and 4.3 hold. For any δ ∈ (0, 1), With probability at least 1 - δ,
the distance between the estimated weight vector θt by Algorithm 1 and θ* can be bounded as
folloWs:
t
θt- θ* - A-I X φ(Xs,as ； Ws-I)θ>g(xs,as ； W(O))(W*-w(0)”
s=1	At
≤ V，2(dlog(1 + t(logHK)∕λ) + log1∕δ) + λ1∕2M,
for any t ∈ [T].
Note that the confidence bound in Lemma C.4 is different from the standard result for linear con-
textual bandits in Abbasi-Yadkori et al. (2011). The additional term on the left hand side of the
confidence bound is due to the bias caused by the representation learning using a deep neural net-
work. To deal with this extra term, we need the following technical lemma.
Lemma C.5. Assume that At = λI + Pts=1 φsφs>, where φt ∈ Rd and kφtk2 ≤ G for all t ≥ 1
and some constants λ,G > 0. Let {Zt}t=ι,…be a real-value sequence such that |Zt| ≤ U for some
constant U > 0. Then we have
t
At-1Xφsζs	≤ 2Ud,	∀t= 1,2,...
s=1	2
The next lemma provides some standard bounds on the feature matrix At, which is a combination
of Lemma 10 and Lemma 11 in Abbasi-Yadkori et al. (2011).
17
Published as a conference paper at ICLR 2022
Lemma C.6. Let {xt}t∞=1 be a sequence in Rd and λ > 0. Suppose kxtk2 ≤ G and λ ≥
max{1, G2} for some G > 0. Let At = λI + Pts=1 xtxt>. Then we have
T
det(At) ≤ (λ + tG2∕d)d, and X |氏 口；- ≤ 2logf∖T) ≤ 2d log(1 + TG2∕(λd)).
At-1	det(λI)
Now we are ready to prove the regret bound of Algorithm 1.
Proof of Theorem 4.4. For a time horizon T, without loss of generality, we assume T = QH for
some epoch number Q. By the definition of regret in (4.3), we have
T	QH
RT = E X(bxt,at ) - b(xt,at)) = E XX(b(XqH+i,aqH+i)-亍制〃+，2+2))∙
t=1	q=1 i=1
Note that for the simplicity of presentation, we omit the conditional expectation notation of w(0) in
the rest of the proof when the context is clear. In the second equation, we rewrite the time index
t = qH + i as the i-th iteration in the q-th epoch.
By the definition in (2.1), we have E[rb(xt,k)|xt,k] = r(xt,k) for all t ∈ [T] and k ∈ K. Based on the
linearization of reward generating function, we can decompose the instaneous regret into different
parts and upper bound them individually. In particular, by Lemma C.1, there exists a vector W ∈ Rp
such that we can write the expectation of the reward generating function as a linear function. Then
it holds that
r(xt,at ) - r(xt,at ) = θ> [g(xt,a3 W⑼)-g(xt,at ； W⑼)](w* - W⑼)
+ θ*> [CM,* Wt-l) — Φ(xt,at ； Wt-l)]
=θ> [g(xt,a3 W⑼)— g(xt,at ； W⑼)](w* - W⑼)
+ θ>-l[φ(xt,a: ； Wt-1)- φ(xt,at ； Wt-1)]
— (θt-i — θ*)>[Φ(xt川；Wt-l) — ΦM,at ； Wt-i)].	(C.4)
The first term in (C.4) can be easily bounded using the first order stability in Assumption 4.2 and the
distance between W and w(0) in Lemma C.1. The second term in (C.4) is related to the optimistic
rule of choosing arms in Line 5 of Algorithm 1, which can be bounded using the same technique for
LinUCB (Abbasi-Yadkori et al., 2011). For the last term in (C.4), we need to prove that the estimate
of weight parameter θt-1 lies in a confidence ball centered at θ*. For the ease of notation, We define
t
Mt = A-1 X φ(Xs,as ； Ws-1)θ>g(xs,as ； W⑼)(W* — W ⑼).	(C.5)
s=1
Then the second term in (C.4) can be bounded in the following way:
一 (θt-1 — θ* )T[C(xt,a* ； Wt-l) — Φ(xt,at ； Wt-1)]
=Tθt-1 — θ* — Mt-I)Tφ(xt川；Wt-l) + (θt-1 — θ* — Mt-I)Tφ(xt,at ； Wt-l)
— MT-l[φ(xt,a3 wt-l) — φ(xt,at ； wt-l)]
≤ kθt-1 — θ* — Mt-IkAt-1 ∙ kφ(Xt,。3 Wt-I)kA-11
+ kθt-1 — θ* — Mt-IkAt-1 ∙ kφ(Xt,at; WtT)kA-11
+ l∣M3[φ(xt,说；Wt-I) — φ(xt,at ； Wt-l)]l∣2
≤ αtkφ(xt,a3 Wt-I)kA-11 + αtkφ(Xt,at ； Wt-I)kA 二
+ kMt-1k2 ∙ kφ(xt川；Wt-1) — φ(xt,at ； Wt-1)k2.	(C.6)
18
Published as a conference paper at ICLR 2022
where the last inequality is due to Lemma C.4 and the choice of αt. Plugging (C.6) back into (C.4)
yields
r(Xt川 ) - r(xt,at ) ≤ αtkφ(xt,at ； Wt-I)kA-11 - αtkφ(xt,aJ ； Wt-I)kA-11
+ αtkφ(xt,a^ ； w"kA-i, + αtkφ(xt,at ； wt-1)kA-i
t-1	t-1
+ l∣Mt-lk2 ∙ kΦ(xt,a3 Wt-1)— Φ(xt,at ； Wt-l)∣∣2
+ l∣θθ∣2 ∙ l|g(xt,a： ； W(O))- g(xt,at ； W(O))∣F ∙ ∣∣W* — W ⑼ ∣2
≤ 2αtkφ(Xt,at ； Wt-1)kA-11 + kMt-Ik2 ∙ kφ(xt,a3 Wt-1)— φ(xt,at ； Wt-I)k2
+ 'Lipkθθ∣2 ∙ kxtq - xt,at∣2 ∙ ∣∣W* — W ⑼ ∣2,	(C.7)
where in the first inequality we used the definition of upper confidence bound in Algorithm 1 and
the second inequality is due to Assumption 4.2. Recall the linearization of φj in Lemma C.3, we
have
φ(x; Wt-1) = φ(x; Wo ) + g(x; WO)(Wt-1 — W0).
Note that by the initialization, we have φ(x; WO) = 0 for any x ∈ Rd. Thus, it holds that
φ(xt,a3 Wt-1)— φ(xt,at ； Wt-1)
=φ(xt,a3 Wt-1)— φ(xtq ； WO) + φ(xt,at ； Wθ) — Φgat ； Wt-1)
=ΦM川；Wt-1)— <b(xt,a： ； Wt-1)+ g(xta；WO)(Wt-1 — Wo)
+ Φ(xt,at ； Wt-1)— <b(xt,at ； Wt-1)— g(xt,at ； WO)(Wt-1 — Wo),	(C.8)
which immediately implies that
Hφ(xt,a: ； Wt-I) — φ(xt,at ； Wt-I) ∣∣2
≤ Ilφ(xt,a3 Wt-I) — B(Xtq ； Wt-1)||2 + ∣∣φ(xt,at ； Wt-I) — B(Xt,at ； Wt-1)||2
+ U(g(xt,a3 Wo) — g(xt,at ； W0))(Wt-1 — Wo)∣2
≤ Co"4/3L3d1/2 Pmlogm + 'LipkxtM — xt,at ∣2∣Wt-1 — W(0)∣2,	(C.9)
where the second inequality is due to Lemma C.3 and Assumption 4.2. Therefore, the instaneous
regret can be further upper bounded as follows.
r(Xt,a" — r(Xt,at)
≤ 2αtkφ(xt,at; Wt-1 )kA-1 + 'Lip∣∣θ0∣∣2 ∙ ∣∣xt,a* — xt,atk2 ∙ ∣∣w* — W(O)Il2
t-1
+ ∣∣Mt-1k2 ∙ (Coω4∕3L3d1∕2Pmlogm + 'Lip∣Xt川- Xt,at∣2∣Wt-1 — W(O)∣∣2). (C.10)
By Assumption 4.1 We have ∣∣xt,a* — Xt,at ∣2 ≤ 2.By Lemma C.1 and Lemma C.2, We have
∣∣w* — w(o) ∣∣2 ≤ q'l∕m(r — e)>H-1(r — e),
,、	δ3∕2	(C.11)
∣∣wt — w(o) ∣∣2 ≤.
"	11 — m1/2Tn9/2L6 log3(m)
In addition, since the entries of θo are i.i.d. generated from N(0, 1∕d), We have ∣θo ∣2 ≤
2(2 +，dTlog(I∕δ)) with probability at least 1 一 δ for any δ > 0. By Lemma C.2, We have
∣∣g(xt,at； W(O))∣F ≤ C1√dm. Therefore,
∣θ>g(xs,as； W(O))(w* - W(O))I ≤ C2dql0g(l∕δ)(r -e)>H-1(r -e).
Then, by the definition of Mt in (C.5) and Lemma C.5, we have
∣Mt-1∣2 ≤ C3d2qlog(l∕δ)(r -e)>H-1(r -e).	(C.12)
19
Published as a conference paper at ICLR 2022
Substituting (C.12) and the above results on |必用-xt,a* k2, kθ0k2, kw* - W(O)Il2 and ∣∣wt-ι -
w(0)k2 back into (C.10) further yields
r(Xt,a" - r(xt,at )
≤ 2αtkφ(xt,at; Wt-1)∣∣A-I1 + C4'Lipm-1/2qiog(1∕δ)(r - e)>H-1(r - e)
+ (CwV3L3W2pmogm + mι∕2T n'LipL：og3(m) )c3d2qiog(1∕δ)(r- e)>HT(r- e).
Note that we have ω = O(m-1/2Ir - erIH-1 ) by Lemma C.1. Therefore, the regret of the
Neural-LinUCB is
RT ≤ t
Q H	_____
QH maxαt2XXk φ(xi,ai ; w qH+i ) k A - 1 + C4∣Lipm Tpl plog(1∕δ) kr - ekH-1
t∈[T]	Ai
q=1 i=1
CoTL3d1∕2√log m∣∣r - ekH/31
m1/6
+
C3d2√log(1∕δ)∣r -e∣∣H-ι
≤ C5 VZTdlog(1 + TG2∕(λd))(ν√dlog(1 + T(logTK)∕λ) + log 1∕δ + λ1/M)
+ C6'LipL3d5∕2mτ∕6TPlogmlog(1∕δ)log(TK∕δ)∣∣r -F∣∣h-i,
where the first inequality is due to Cauchy’s inequality, the second inequality comes from the upper
bound of αt in Lemma C.4 and Lemma C.6. {Cj}j=0,...,6 are absolute constants that are independent
of problem parameters.	□
Proof of Corollary 4.6. It directly follows the result in Theorem 4.4.
□
D Proof of Technical Lemmas
In this section, we provide the proof of technical lemmas used in the regret analysis of Algorithm 1.
D.1 Proof of Lemma C.1
Before we prove the lemma, we first present some notations and a supporting lemma for simplifica-
tion. Let β = (θ>, w>)> ∈ Rd+p be the concatenation of the exploration parameter and the hidden
layer parameter of the neural network f(X; β) = θ>φ(X; w). Note that for any input data vector
X ∈ Rd, we have
∂	∂>	>
∂βf(x; β) = (φ(x; w)>, θ>∂Wφ(x; W)) = (φ(x; w)>, θ>g(x; W)) ,	(D.1)
where g(x; w) is the partial gradient of φ(x; w) with respect to w defined in (C.1), which is a
matrix in Rd×p. Similar to (4.2), we define HL+1 to be the neural tangent kernel matrix based on all
L + 1 layers of the neural network f(x; β). Note that by the definition of H in (4.2), we must have
HL+1 = H + B for some positive definite matrix B ∈ RTK×TK. The following lemma shows that
the NTK matrix is close to the matrix defined based on the gradients of the neural network on TK
data points.
Lemma D.1 (Theorem 3.1 in Arora et al. (2019b)). Let > 0 and δ ∈ (0, 1). Suppose the activation
function in (2.3) is ReLU, i.e., σl(x) = max(0, x), and the width of the neural network satisfies
m ≥ ω( ¥ log (L
(D.2)
Then for any x, x0 ∈ Rd with ∣x∣2 = ∣x0∣2 = 1, with probability at least 1 -δ over the randomness
of the initialization of the network weight w it holds that
I/ ɪ ∂f(β, x)	1 ∂f(β, x0) ∖
I ∖ √m ∂β ，√m ∂β /
-HL+1(x,x0)
≤ .
20
Published as a conference paper at ICLR 2022
Note that in the above lemma, there is a factor 1∕√m before the gradient. This is due to the ad-
ditional √m factor in the definition of the neural network in (2.3), which ensures the value of the
neural network function evaluated at the initialization is of the order O(1).
Proof of Lemma C.1. Recall that we renumbered the feature vectors {xt,k}t∈[T],k∈[K] for all arms
from round 1 to round T as {xi}i=1,...,T K. By concatenating the gradients at different inputs and
the gradient in (D.1), we define Ψ ∈ RTK×(d+p) as follows.
Γ 磊θ>φ(X1；W)’
ψ =	θ>Φ(Xτκ ； W).
φ(xι; w(0))>
.
.
.
φ(xi; W(0))>
.
.
.
Φ(xτκ； W(O))T
θ>g(xι;W(O))’
.
.
.
θ>g(xi; W(O))
.
.
.
θ>g(xτκ； W(O))
By Applying Lemma D.1, we know with probability at least 1 - δ it holds that
∣hΨj*, Ψι*i- HL+ι(xj, xι)| ≤ e
for any > 0 as long as the width m satisfies the condition in (D.2). By applying union bound over
all data points {x1, . . . , xt, . . . , xTK}, we further have
kΨΨ> - HL+1kF ≤ TKe.
Note that H is the neural tangent kernel (NTK) matrix defined in (4.2) and HL+1 is the NTK matrix
defined based on all L + 1 layers. By Assumption 4.3, H has a minimum eigenvalue λO > 0,
which is defined based on the first L layers of f . Furthermore, by the definition of NTK matrix
in (4.2), we know that HL+1 = H + B for some semi-positive definite matrix B. Therefore, the
NTK matrix HL+1 defined based on all L + 1 layers is also positive definite and its minimum
eigenvalue is lower bounded by λ0. Let e = λo∕(2τκ). By triangle equality We have ΨΨ> 上
Hl+i — kΨΨ> — Hl+ik2I A Hl+i - kΨΨ> - Hl+i∣∣fI A Hl+i - λo∕2I A 1∕2Hl+i,
which means that Ψ is semi-definite positive and thus rank(Ψ) = TK since m > TK.
We assume that Ψ can be decomposed as Ψ = PDQ>, where P ∈ RTK×TK is the eigenvectors
of ΨΨ> and thus PP> = ITK, D ∈ RTK×TK is a diagonal matrix with the square root of
eigenvalues of ΨΨ>, and Q> ∈ RTK ×(d+p) is the eigenvectors of Ψ>Ψ and thus Q>Q = ITK.
We use Q1 ∈ Rd×T K and Q2 ∈ Rp×T K to denote the two blocks of Q such that Q> = [Q1>, Q2>].
By definition, we have
Q>Q= [Q1>,Q2>] QQ1 = Q1>Q1 + Q2>Q2 = ITK.
Note that the minimum singular value of Q1 ∈ Rd×TK is zero since d is a fixed number and
TK > d. Therefore, it must hold that rank(Q2) = TK and thus Q2> Q2 is positive definite. Let
r = (r(x1), . . . , r(xi), . . . , r(xT K))> ∈ RTK denote the vector of all possible rewards. We further
define G ∈ RTKd×p and Φ ∈ RTKd as follows
Γ g(x1； W(O))
.
.
1.
G = √m	g(xi； W(O))
.
.
.
g(xTK； W(O))
and Θ, ΘO ∈ RTK ×TKd as follows
-θ*>	一
-φ(x1,1; wo)-
.
.
.
Φ =	φ(xt,k ； Wt-1)
.
.
.
.φ(xτ,κ ； WT-1).
(D.3)
Θ*
θ*>
ΘO
θO>
(D.4)
θ*>
θO>
21
Published as a conference paper at ICLR 2022
It can be verified that Ψ = PD[Q1> , Q2>] and PDQ2> = Θ0G. Note that we have Q2>Q2 is
positive definite by Assumption 4.3, which corresponds to the neural tangent kernel matrix defined
on the first L layers. Then We can define W as follows
W* = W(O) + 1∕√mQ2(Q>Q2)-1D-1P>(r - Θ*Φ).	(D.5)
We can verify that
Θ*Φ + √mPDQ>(w* - W(O)) = r.
On the other hand, we have
kW* - W(O)k22 ≤ 1/m(r - Θ*Φ)>PD-1(Q2>Q2)-1D-1P>(r - Θ*Φ)
≤ 1/m(r - Θ*Φ)>H-1(r - Θ*Φ),
which completes the proof.	□
D.2 Proof of Lemma C.2
Note that we can view the output of the last hidden layer φ(x; W) defined in (2.4) as a vector-output
neural network with weight parameter W. The following lemma shows that the output of the neural
network φ is bounded at the initialization.
Lemma D.2 (Lemma 4.4 in Cao & Gu (2019b)). Let δ ∈ (0, 1), and the width of the neural
network satisfy m ≥ CoLlog(TKL∕δ). Then for all t ∈ [T], k ∈ [K] and j ∈ [d], we have
∣φj(xt,k; W(O))I ≤ Cipiog(TK∕δ) with probability at least 1 - δ, where W(O) is the initialization
of the neural network.
In addition, in a smaller neighborhood of the initialization, the gradient of the neural network φ is
uniformly bounded.
Lemma D.3 (Lemma B.3 in Cao & Gu (2019b)). Let ω ≤ COL-6(log m)-3 and W ∈ B(WO, ω).
Then for all t ∈ [T], k ∈ [K] and j ∈ [d], the gradient of the neural network φ defined in (2.4)
satisfies ∣∣Vwφj(xt,k; w)∣∣2 ≤ Ci√Lm with probability at least 1 - TKL2 exp(-C2mω2∕3L).
The next lemma provides an upper bound on the gradient of the squared loss function defined in
(3.3). Note that our definition of the loss function is slightly different from that in Allen-Zhu et al.
(2019b) due to the output layer θi and thus there is an additional term on the upper bound of ∣θi ∣2
for all i ∈ [T].
Lemma D.4 (Theorem 3 in Allen-Zhu et al. (2019b)). Let ω ≤ Coδ3/2/(T9/2L6 log3 m). For all
w ∈ B(W(O),ω), with probability at least 1 - exp(-Cimω2/3L) over the randomness of w(o), it
holds that
2 ≤ C2TmL(W) SuPi=i,...,H kθik2 .
2d
Proof of Lemma C.2. Fix the epoch number q and we omit it in the subscripts in the rest of the
proof when no confusion arises. Recall that W(s) is the s-th iterate in Algorithm 2. Let δ > 0 be any
constant. Let ω be defined as follows.
ω = S3/2m-i/2 T-9/2L-6 log-3(m).	(D.6)
We will prove by induction that with probability at least 1 - δ the following statement holds for all
s = 0, 1, . . . , n
φj(x； W(S)) ≤ Co X Pbh(TK/δ)，	for ∀j ∈ [d]; and kw[s) - W(O)k ≤ ω. (D.7)
h=O	+
First note that (D.7) holds trivially when s = 0 due to Lemma D.2. Now we assume that (D.7) holds
for all j = 0, . . . , s. The loss function in (3.3) can be bounded as follows.
qH	qH
L(Wcj)) = X(θ>φ(χi; Wej))- bi)2 ≤ X 2(kθik2 ∙ kφ(χi; Wcj))k2 + 1).
i=i	i=i
22
Published as a conference paper at ICLR 2022
By the update rule of θt , we have
kθtk2 =	λI + Xt φ(xi; Wi-i)φ(xi Wi-i)>)	X φ(xi; Wi-i)bll ≤ 2d, (D.8)
i=1	i=1	2
where the inequality is due to Lemma C.5, which combined with (D.7) immediately implies
L(Wj)) ≤ CITd Iog(TK/δ)
≤ CiTd3 Iog(TK/δ)log2 n.
(D.9)
Substituting (D.8) and (D.9) into the inequality in Lemma D.4, we also have
∣∣VL(wj))∣∣2 ≤ C2 JdTmL(Wj)) ≤ C3d2Tlog(n),mIog(TK/δ).
Now we consider W(s+1). By triangle inequality we have
s
∣∣W(s+1) - W(0)∣∣2 ≤ X ∣∣W(j+1) - W(j)∣∣2
j=0
s
=X η∣∣VL(Wj) )∣∣2
j=0
s
≤ ^X ηd2T log(n) pm Iog(TK/δ),
(D.10)
(D.11)
where the last inequality is due to (D.10). If we choose the step size ηq in the q-th epoch such that
η≤
ω
d2Tn log(n) Pm Iog(TK/δ)
(D.12)
then we have kWq(s+1) - W(0)k2 ≤ ω. Note that the choice of m, ω satisfies the condition in
Lemma C.3. Thus we know φj (x; W) is almost linear in W, which leads to
∣φj(x; w(s+1))∣ ≤ ∣φj(x; W(S)) + hVφj(x; W(S)), w(s+1) — W(S)i∣ + C5ω4^3L3d-1 /2pmlog m
≤ SX 0\/l：g(；K/6) + η√dm∣∣vL(W(S))k2 + 2C5ω4∕3L3d-l∕2pmlogm
h+1
h=0
≤ ^X °0',g(；K+ C3η√dmpCT2d4m Iog(TK/δ) log n
h=0	+
+ 2C5ω4∕3L3dT∕2 Pm log m
=X CoPIog(TK0 + ω√dm + 2C5ω4∕3L3d-1∕2pmiogm,	(D.13)
h+1	n
h=0
where in the second inequality we used the induction hypothesis (D.7), Cauchy-Schwarz inequality
and Lemma D.3, and the third inequality is due to (D.10). Note that the definition of ω in (D.6)
ensures that ω√dm < 1/2 and ω4/3L3d-1 /2√m log m ≤ m-1/6T-6L-5d-i/2√log m ≤ 1/n as
long as m ≥ n6. Plugging these two upper bounds back into (D.13) finishes the proof of (D.7).
Note that for any t ∈ [T], we have Wt = Wq(n) for some q = 1, 2,   Since we have Wt ∈ B(W, ω),
the gradient g(x; W(0)) can be directly bounded by Lemma D.3, which implies kg(x; W(0))kF ≤
C6√dLm. Applying (D.7) with S = n, we have the following bound of the neural network function
φ(x; Wq(n)) = φ(x; Wt) for all t in the q-th epoch
kφ(x; Wt)k2 ≤ Copdlog(n) log(TK/6),
which completes the proof. In this proof, {Cj > 0}j=0,...,6 are constants independent of problem
parameters.	□
23
Published as a conference paper at ICLR 2022
D.3 Proof of Lemma C.4
The following lemma characterizes the concentration property of self-normalized martingales.
Lemma D.5 (Theorem 1 in Abbasi-Yadkori et al. (2011)). Let {ξ}t∞=1 be a real-valued stochastic
process and {xt}t∞=1 be a stochastic process in Rd. Let Ft = σ(x1 , . . . , xt+1 , ξ - 1, . . . , ξt) be a
σ-algebra such that xt and ξt are Ft-1-measurable. Let At = λI + Pts=1 xsxs> for some constant
λ > 0 and St = Pts=1 ξsxi. If we assume ξt is ν-subGaussian conditional on Ft-1, then for any
η ∈ (0, 1), with probability at least 1 - δ, we have
kStkA-1 ≤ 2ν2 log (det(At)1/2det(λI)T2 ).
Proof of Lemma C.4. Let Φt = [φ(x1,a1 ; w0), . . . , φ(xt,at; wt-1)] ∈ Rd×t be the collection of
feature vectors of the chosen arms up to time t and brt = (rb1, . . . , rbt)> be the concatenation of all
received rewards. According to Algorithm 1, we have At = λI + Φt Φt> and thus
θt = At-1bt = (λI + ΦtΦt>)-1Φtbrt.
By Lemma C.1, the underlying reward generating function rt = r(xt,at) = E[rb(xt,at)|xt,at] can be
rewritten as
Tt = <θ*, ΦM,at ； WLI)) + θ> g(Xt,at ； W(O))(W* - W ⑼).
By the definition of the reward in (2.1) we have rbt = rt + ξt. Therefore, it holds that
t
θt = At-1ΦtΦt>θ* + At-1 X φ(Xs,as; Ws-1)(θ0>g(Xs,as; W(0))(W* -W(0)) +ξs)
s=1
t
= θ* - λAt-1θ* + At-1 X φ(Xs,as; Ws-1)(θ0>g(Xs,as; W(0))(W* - W(0)) +ξs).
s=1
Note that At is positive definite as long as λ > 0. Therefore ∣∣ ∙ ∣∣At and ∣∣ ∙ ∣∣At are Well defined
norms. Then for any δ ∈ (0, 1) by triangle inequality we have
∣θt - θ* - At-1ΦtΘtGt(W* - W(0))∣At ≤ λ∣θ*∣At-1 +∣Φtξt∣At-1
≤ ν"og (det(At)""正门 + wm
holds With probability at least 1 - δ , Where in the last inequality We used Lemma D.5 and the fact
that ∣Θ*∣a-i ≤ λ-1"kθ*k2 ≤ λ-"M by LemmaC.1. Plugging the definition of Φt, Θt and Gt
and apply Lemma C.6, We further have
t
θt - θ* - At-1 X φ(Xs,as; Ws-1)θ0>g(Xs,as; W(0))(W* - W(0))
s=1	At
≤ V，2(dlog(1 + t(logHK)∕λ) + log1∕δ) + λ1∕2M,
where we used the fact that ∣φ(x; w) ∣∣2 ≤ C√dlog HK by Lemma C.2.	□
D.4 Proof of Lemma C.5
We now prove the technical lemma that upper bounds ∣At-1 Pts=1 φsζs ∣2 .
Proof of Lemma C.5. We first construct auxiliary vectors φet ∈ Rd+1 and matrices Bt ∈
R(d+1)×(d+1) for all t = 1, . . . in the following way:
φ = G lφt	B = Atl 0d	(D14)
φt = [pi-G-2kΦtk2], Bt = [0> 0],	()
24
Published as a conference paper at ICLR 2022
where 0d ∈ Rd is an all-zero vector. Then by definition we immediately have
t	t
At-1Xφsζs	= BtXφesζs
s=1	2	s=1	2
(D.15)
For all s = 1, 2 . . ., let {βs,j}jd=+11 be the coefficients of the decomposition of U-1ζsφes on the natural
basis. Specifically, let {e1, . . . , ed+1} be the natural basis of Rd+1 such that the entries of ej are all
zero except the j-th entry which equals 1. Then we have
d
U-1ζsφes = Xβs,jej, ∀s= 1,2,...	(D.16)
j=1
5T	1 F ,1	, IC I ， r ♦	I Λ I ^TT t II V H ，τ ɪ r	♦	.	∙ r∙ .1
We can conclude that ∣βs,j | ≤ 1 since |Zs| ≤ U and ∣∣φs∣∣2 ≤ 1. Moreover, it is easy to verify that
，，二，， ■ ~ 一一 . 一 ~ ・
kφtk2 = 1 for all t ≥ 1. Therefore, we have
t
Bt Xφesζs
s=1
t
= BtXφesφes>φesζs
2	s=1	2
t	d
BtXφesφes>UXβs,jej
s=1	j=1	2
d	t
= UXBt Xφs φs βs,j ej
j=1	s=1	2
d	t
≤ UX Bt Xφesφes>βs,j
j=1	s=1	2
d	t
UXAt-1Xφsφs>βs,j
j=1	s=1	2
(D.17)
where the inequality is due to triangle inequality and the last equation is due to the definition of φt
and Bt in (D.14). For each j = 1, . . . , d + 1, we have
t
At-1 Xφsφs>βs,j
s=1	2
At-1	φsφs>βs,j + At-1	φsφs>βs,j
S∈[t]'βs,j ≥0	S∈[t];βs,j<0
≤ At-1	φsφs>βs,j
S∈[t]βs,j ≥0
Since We have ∣βs,j | ≤ 1, it immediately implies
t
At=λI+Xφsφs>	X
+ At-1	φsφs> (-βs,j) .
s∈Mβs,j<0	2
(D.18)
φsφs>βs,j ,
s=1	S∈[t];βs,j ≥0
t
At = λI + X φsφs>	X	φsφs>(-βs,j).
s=1	S∈[t];βs,j<0
Further by the fact that kA-1Bk2 ≤ 1 for any A B	0, combining the above results With (D.18)
yields
t
At-1 Xφsφs>βs,j
s=1
≤ 2.
2
Finally, substituting the above results into (D.17) and (D.15) We have
t
At-1Xφsζs	≤ 2Ud,
s=1	2
Which completes the proof.
□
25