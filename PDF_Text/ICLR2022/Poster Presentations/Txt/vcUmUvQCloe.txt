Published as a conference paper at ICLR 2022
Joint Shapley values: a measure of joint fea
TURE IMPORTANCE
Chris Harris
Tokyo, Japan
Raptor Financial Technologies
chrisharriscjh@gmail.com
Colin Rowat
Economics
University of Birmingham, UK
c.rowat@bham.ac.uk
Richard Pymar
Economics, Mathematics & Statistics
Birkbeck College University of London, UK
r.pymar@bbk.ac.uk
Ab stract
The Shapley value is one of the most widely used measures of feature importance
partly as it measures a feature’s average effect on a model’s prediction. We intro-
duce joint Shapley values, which directly extend Shapley’s axioms and intuitions:
joint Shapley values measure a set of features’ average contribution to a model’s
prediction. We prove the uniqueness of joint Shapley values, for any order of
explanation. Results for games show that joint Shapley values present different
insights from existing interaction indices, which assess the effect of a feature within
a set of features. The joint Shapley values provide intuitive results inML attribution
problems. With binary features, we present a presence-adjusted global value that is
more consistent with local intuitions than the usual approach.
1	Introduction
Game theory’s Shapley value partitions the value arising from joint efforts among individual agents
(Shapley, 1953). Specifically, denote by N = {1, . . . , n} a set of agents, and by GN the set of games
on N, where a game is a set function V from 2N to R with v(0) = 0. Then V(S) is the worth created
by coalition S ⊆ N. When there is no risk of confusion, we omit braces to indicate singletons (e.g. i
rather than {i}) and denote a set’s cardinality by the corresponding lower case letter (e.g. s = |S|).
For any agent i, Shapley’s value is then
ψi (v) ≡ X	s!(n■-s_1)! [v (S ∪ i) - V (S)].	(1)
/ /	γ^) I
S⊆N ∖{i}
This is the average worth that i adds to possible coalitions S, weighted as follows: if the set of agents,
S, has already ‘arrived’, draw the next agent, i, to arrive uniformly over the remaining N\S agents.
Shapley’s value is widely used in explainable AI’s attribution problem, partitioning model predictions
among individual features (q.v. Strumbelj & Kononenko (2014); Lundberg & Lee (2017)) after the
model has been trained. Evaluating the prediction function at a specific feature value corresponds to
an agent’s presence; evaluating it at a reference (or baseline) feature value corresponds to the agent’s
absence. A feature’s Shapley value is its average marginal contribution to the model’s predictions.
When features are correlated, individual measures of importance may mislead (Bhatt et al., 2020;
Patel et al., 2021), as measures of individual significance such as the t-test do. Thus, Shapley’s value
has been extended to sets of features (Grabisch & Roubens, 1999a; Marichal et al., 2007; Alshebli
et al., 2019; Dhamdhere et al., 2020). As these extensions introduce axioms not present in Shapley,
they do not preserve the Shapley value’s intuition.
1
Published as a conference paper at ICLR 2022
We extend Shapley’s axioms to sets of features, randomizing over sets rather than individual features.
The resulting joint Shapley value thus directly extends Shapley’s value to a measure of sets of feature
importance: the average marginal contribution of a set of features to a model’s predictions.
Our approach’s novelty is seen in our extension of the null axiom: in Shapley (1953), a null agent
contributes nothing to any set of agents to which it may belong; here, a null set contributes nothing
to any set to which it may belong. By contrast, interaction indices (Grabisch & Roubens, 1999a;
Alshebli et al., 2019; Dhamdhere et al., 2020) recursively decompose sets into individual elements,
retaining the original Shapley null axiom. Thus, these indices measure sets’ contributions relative to
their constituent elements — so are complementary to the joint Shapley value.
The generalised Shapley value (Marichal et al., 2007) is closer to our work, but differs in a number of
respects: our probabilities are independent of the size of the set of features under consideration1; our
efficiency axiom is fully joint, while theirs are based on singletons and pairs; our symmetry axiom
provides uniqueness without reliance on recursion or partnership axioms.
To illustrate, consider a null coalition, T, whose individual members contribute positively to coalitions
that they join. Interaction indices assign a negative value to T , as the individual members act
discordantly together. However, from a joint feature importance viewpoint, T should be assigned
value 0. The joint Shapley value matches this intuition.
In the movie review application presented below, the joint Shapley values reveal contributions of
collections of words, including grammatical features such as negation ({disappointed} versus {won’t,
disappointed}), and adjectives ({effort} versus {terrific, effort}).
Like the Shapley-Taylor interaction index (Dhamdhere et al., 2020), our efficiency axiom depends
on a positive integer k, the order of explanation, which limits the number of joint Shapley values to
those for subsets up to cardinality k. As there are 2N - 1 non-empty subsets of N, the full set of
joint Shapley values rapidly becomes unmanageable otherwise. In practice, k should be set to trade
off insight (favouring higher k) against computational cost (favouring lower k).
For each k, the extended axioms yield a unique joint Shapley value. Unlike in Shapley (1953), the
joint anonymity and symmetry axioms are not interchangeable: each imposes distinct restrictions.
Section 2 presents and extends the original Shapley axioms. Section 3 introduces joint Shapley values,
deriving them as the unique solution to the extended axioms. Section 4 illustrates joint Shapley
values in the game theoretical environment and applies them to the Boston housing (Harrison &
Rubinfeld, 1978) and Rotten Tomatoes movie review (Pang & Lee, 2005) datasets, comparing them to
interaction indices and presenting a sampling technique to facilitate calculation. Section 5 concludes.
The Appendix collects proofs and other supplemental material.
2	Extending Shapley’s axioms
For a game v ∈ GN , and a permutation σ on N, denote a permuted game by σv ∈ GN such that
σv(σ(S)) = v(S), ∀S ⊆ N, where σ(S) = {σ(i) : i ∈ S}. An index φ(v) of the game v ∈ GN is
any real-valued function on 2N .
The original axioms uniquely satisfied by the (standard) Shapley value ψ, are:
LI linearity : ψ is a linear function on GN, i.e. ψ(v + w) = ψ(v) + ψ(w) and ψ (av) = aψ(v) for
any v, w ∈ GN and a ∈ R.
NU null : An agent that adds no worth to any coalition has no value, i.e. if v(S ∪ {i}) = v(S) for
all S ⊆ N \ {i}, then ψi(v) = 0. This axiom is sometimes called dummy.
EF efficiency : The sum of the values of all agents is equal to the worth of the entire set, i.e. for all
v ∈ GN, Pin=1 ψi(v) = v(N).
AN anonymity : For any σ on N and any v ∈ GN, ψi(v) = ψσ(i) (σv), for all i ∈ N.
1Measures of joint significance (e.g. F -tests) and model selection (e.g. BIC or AIC) penalize larger feature
sets to avoid overfitting. As joint Shapley values are calculated after model training, this problem does not arise.
2
Published as a conference paper at ICLR 2022
SY symmetry : If two agents add equal worth to all coalitions that they can both join then they
receive equal value: if v(S ∪ {i}) = v(S ∪ {j})∀S ⊆ N \ {i, j} then ψi (v) = ψj (v). This
is strictly weaker than anonymity.
Now extend each of these axioms in natural ways to conditions on sets rather than singletons. Below,
φS (v) denotes an index for coalition S on game v.
JLI joint linearity : φ is a linear function on GN, i.e. φ(v + w) = φ(v) + φ(w) and φ(av) = aφ(v)
for any v, w ∈ GN and a ∈ R. (This axiom has not been modified.)
JNU joint null : A coalition that adds no worth to any coalition has no value, i.e. if v(S ∪ T) = v(S)
for all S ⊆ N \ T, then φT (v) = 0.
JEF joint efficiency : The sum of the values of all coalitions up to cardinality k is equal to the worth
of the entire set, i.e. for all v ∈ GN,
X φT (v) = v(N).
0=T⊆N:
Tl≤k
JAN joint anonymity : For any σ on N and any v ∈ GN, φT(v) = φσ(T)(σv), for all T ⊆ N.
JSY joint symmetry : If two coalitions perform equally when joining coalitions that they can both
join and for other coalitions they add no worth then they receive an equal value, i.e. if
1.	v(S∪T) = v(S ∪ T0) for all S ⊆ N \ (T ∪ T0),
2.	V(S ∪ T) = V(S) forall S ⊆ N \ T such that S ∩ T0 = 0,
3.	V(S ∪ T0) = V(S) for all S ⊆ N \ T0 such that S ∩ T= 0,
then φT(V) = φT0 (V).
Axiom JSY only equates the joint Shapley values for coalitions T and T0 if they contribute identically
to coalitions that they may both join, and contribute nothing to the other coalitions. Axioms JLI, JEF
and JAN are all also used in Dhamdhere et al. (2020). Our joint null and joint symmetry notions
appear to be new: they reflect our interest in a set of features’ contribution to a model’s predictions,
so that the set’s cardinality should not play a role in determining its value.
3 Joint S hapley values
Our main result is that there is a unique solution to axioms JLI, JNU, JEF, JAN and JSY, the joint
Shapley value. The uniqueness is up to the kth order of explanation; we say nothing about |T| > k.
Theorem 1. For each order of explanation k ∈ {1, . . . , n}, there is a unique (up to the kth order of
explanation) index φJ which satisfies axioms JLI, JNU, JEF, JAN and JSY. It has the form
φJT (V) = X q|S| [V(S ∪ T) -V(S)]
S⊆N\T
for each 0 6= T ⊆ N with |T| ≤ k, where (q0, . . . , qn-1) uniquely solves the recursive system
q0
k
i=1
Cn),
qr
Prs=-(1r-k)∨0
Pk∧(n-r)
s=1
rs qs
n-sr
(2)
Σ
1
for all r ∈ 1, . . . , n - 1.
When k = 1, the joint Shapley values coincide with Shapley’s values.2 When k = n, we have
(-2)r-j
2n-j - 1
∀r ∈ {0, . . . ,n - 1}.
For each k, the constants q『are non-negative and satisfy Pn-1_k (：)q§ = 1. Further, as thejoint
Shapley value is similar in form to equation (1)’s standard Shapley value, the value of coalition T
2The Shapley-Taylor interaction index, φST, also has this property (Dhamdhere et al., 2020).
3
Published as a conference paper at ICLR 2022
depends on its marginal contribution to other coalitions; unlike interaction indices, it does not depend
on the worth of its constituent agents.
As with the Shapley value, the joint Shapley value can be seen as the worth brought by ‘arriving’
agents but, rather than arriving one at a time, they can now also arrive in coalitions. We develop this
interpretation in Appendix A.
We show here the implication of each of the joint axioms introduced above and prove Theorem 1.
It is already known that joint linearity restricts measures to be linear combinations of worths:
Lemma 1 (Grabisch & Roubens (1999a), Proposition 1). If φ satisfies JLI, thenfor every 0 = T ⊆ N
there exists a family of real constants {aTS}S⊆N such that for every v ∈ GN,
φT (v) = X aTS v(S).
S⊆N
Axiom JNU then constrains the values of the constants {aTS}:
Lemma 2. Suppose φ satisfies JLI and JNU and let {aT}s⊆n, e=τ⊆n be the constants from
Lemma 1. Then for every 0 6= T ⊆ N and 0 6= S ⊆ N \ T, aTS = -aTS∪T. Further, for every
0 6= T ⊆ N, S ⊆ N \ T, and 0 6= H ( T, aST∪H = 0.
SCombining these two lemmas yields:
Proposition 1. Suppose φ satisfies JLI and JNU. Then there exist constants {pT (S)} that depend on
T and S such that for every 0 6= T ⊆ N and v ∈ GN
φT (v) = X pT(S)[v(S ∪ T) - v(S)].	(3)
S⊆N\T
Now establish a condition on the {pT(S)} values which must be satisfied under JEF:
Proposition 2. For each order of explanation k, φ satisfies axioms JLI, JNU and JEF if and only if
for every 0 = T ⊆ N with |T| ≤ k and V ∈ GN, φτ(V) = Ps⊆n\tPT(S) [v(S ∪ T) - v(S)] with
pT (S ) satisfying
δN(S) =	X pT(S\T) - X	pT(S),	(4)
0=T ⊆S:	0=T ⊆N\S:
∣T∣≤k	∣T∣≤k
for all 0 6= S ⊆ N, where δN(S) equals 1 if S = N and 0 otherwise.
Recall that symmetry axiom SY is strictly weaker than anonymity axiom AN (Malawski, 2020).
This is not the case for their joint counterparts: each imposes a different constraint on the constants
{pT(S)}, as we shall see here. First, consider the effect of imposing JAN.
Proposition 3. For each order of explanation k, φ satisfies axioms JLI, JNU, JEF and JAN if and only
iffor every 0 = T ⊆ N and V ∈ GN, φτ(v) = Ps⊆n\tPT(S)[v(S ∪ T) — v(S)] with {pτ (S)}
satisfying (4) and
PT (S) =PT0	(S0)∀0	6=	T,T0	⊆ N, S ⊆ N\T,	S0	⊆ N\T0 s.t. s =	s0,t =	t0.	(5)
The analogous result for JSY is:
Proposition 4. For each order of explanation k, φ satisfies axioms JLI, JNU, JEF and JSY if and only
iffor every 0 = T ⊆ N and V ∈ GN, φτ(v) = Ps⊆n\tPT(S)[v(S ∪ T) — v(S)] with {pτ (S)}
satisfying (4) and
PT (S) =PT0 (S)∀0 6= T,T0 ⊆ N, S ⊆ N\ (T∪T0).	(6)
Combining Propositions 2-4 completes the proof of Theorem 1.
The computational complexity of deriving the (q0, . . . , qn-1) is O(nk2). Once these have been
determined, the joint Shapley values can be calculated; the complexity of doing so is O(3n ∧ (2nnk)).
4
Published as a conference paper at ICLR 2022
4 Experiments
4.1 Game theoretical
We present two game theoretical models from Dhamdhere et al. (2020) with known ‘ground truths’.
For each, we compare the joint Shapley value, the Shapley interaction index (Grabisch & Roubens,
1999b), the generalised Shapley value (Marichal et al., 2007), the added-value index (Alshebli et al.,
2019), and the Shapley-Taylor interaction index (Dhamdhere et al., 2020), respectively:
ΦT1 (V) ≡ X n-t+1 (n-t) 1 X (T)I V (S ∪ L), ∀T ⊆ N ；
S⊆N\T	L L	L⊆T
φTS (V) ≡ X(n -S - t”! [v (S ∪ T) - V (S)];
s⊆N∖ T (n- t+1)!	)	( )]；
ΦAV(v) ≡ v (T) - X 2n⅛	X X c!(s -Sc- 1)! [v (C ∪ i)-v (C)]；
i∈T	S⊆N "∈SC⊆S∖i
ST k	PL⊆T (-1)t-l V (L)	if t< k
φT (V； ) ≡ Ik Ps⊆Ν∖T (n-1)-1 Pl⊆t (-1)t-l V (S ∪ L) if t = k.
Table 1: Joint and interaction measures in n = 3 game theory examples
T	V	Shapley (k=1)	φSI	φGS	φAV	φST (Vm； k)		φJ (Vm； k)	
						k=2	k=3	k=2	k=3
		The majority game: Vm (T ) = 1 when t ≥ 2, and is equal to 0 otherwise							
i	0	1/3	1/3	1/3	-1/3	0	0	1/9	2/21
i, j	1		0	1/2	1/3	1/3	1	2/9	4/21
N	1		-2	1	0		-2		3/21
		A linear model with crosses: Vc (T) =			Pi∈T 1	+ c max {0, t - 2} for c ∈ R			
i	1	1/3(3+c)	1/3(3+c)	1/3(3+c)	-1/12c	1	1	5/18(2+c)	5/21(2+c)
i, j	2		1/3c	1/2 (4 + c)	-1/6c	1/3c	0	1/18(8+c)	1/21(8+c)
N	3+c		c	3+c	3/4c		c		3/21(3+c)
In the n= 3 majority game, Table 1 shows that, while Shapley-Taylor assigns 0 to singletons, the
joint Shapley value recognises that singletons contribute positively when joining existing coalitions.
The Shapley-Taylor and Shapley Interaction indices assign negative values to N (for discordant
interaction between members); the joint Shapley rewards N for adding worth in its own right.
In the linear model with crosses, joint Shapley values’ signs can again differ from those of interaction
indices: let c = -2, which sets φ1J = 0: if i = 1 joins the empty coalition or either singleton, it adds
unit worth; however, when i = 1 joins {2, 3}, it subtracts unit worth. Averaging (according to the
arrival order), gives a net contribution of 0: coalition 1 does not contribute any worth in expectation.
By contrast, the Shapley-Taylor value always (for k> 1) assigns value to singletons equal to their
worth, as it is not (by design) capturing information about expected contributions of features.
4.2 The AI/ML attribution problem
Following Strumbelj & Kononenko (2010), let f bea prediction function, and X = (x1,..., Xn) ∈ A
an instance from the feature space. For a set of features S ⊆ N, define the prediction difference
Vx (S) when only features in S are known, as
Vx(S) ≡ |Ai X f(T (X, z,S))- f(Z)],
|A| z∈A
(7)
where τ(X, z, S) ∈ A is defined as τ(X, z, S)i equal to xi for i ∈ S and zi otherwise. Thus Vx(S)
is the difference between the expected prediction when only the feature values of X in S are known,
5
Published as a conference paper at ICLR 2022
and the expected prediction when no feature values are known. Then for each T ⊆ N, φJT (vx) is the
contribution of features T to the prediction f (x). To estimate φJT(vx), let X be a random variable
whose law coincides with the law of the number of agents already present when T arrives in the
arrival interpretation (so that the law of X depends on t, the size of T):
P(X = i)=(n »j0 (n -A
-1
for each i ∈ {0, . . . , n} and S be a random variable uniform on the set of subsets of N \ T of size
X . Once X is sampled, we can generate the set of agents already arrived, called S, by choosing it
uniformly from all subsets of N \T of size precisely X. Then
n-t	n t
φJT (vx) =	j qjE[vx(T ∪ S) - vx (S)].
Thus, by the law of large numbers, we can estimate this expectation by taking an average of
vi(T ∪ Si) - vi(Si) where Si has the same distribution as S and vi(S) = f(τ(x, zi, S)) - f(zi),
where zi is chosen uniformly from the feature space. Refer to φJT (vx) as the local joint Shapley
value of features T at instance x. We can combine local values to obtain, for each feature, a
global joint Shapley value. We do so in two ways. The first is the standard methodology (q.v.
Lundberg & Lee (2017)), which averages the absolute values of locals. We introduce the second for
models with exclusively binary variables by considering presence/absence of features T in x. This
presence-adjusted global joint Shapley value is defined as
φT (f) = iAi X (2 ∙1
x(T)=1
(8)
where 1x(T)=1 is one if all features T are present in x, and 0 otherwise. Covert et al. (2020)
introduced SAGE (Shapley Additive Global Importance), a more sophisticated treatment of global
influence measures that maintains efficiency at the global level.
All experiments are run on a single Intel(R) Core(TM) i7-6820HQ CPU. Training details and tuning
parameters are provided in the accompanying code.
4.2.1	S imulated data
Consider three features, x1 , x2 and x3, each uniformly drawn from (0, 1) and a dataset of 50
observations. We first consider independently drawn features; we then investigate correlation by
fixing x2 = 1 - x1 . We use simplified ‘ML models’, f (x), to obtain exact global joint Shapley
values, averaging the absolute values of local joint Shapley values derived from equation (7). Table 2
displays the results.
Table 2: Uniform random variables; k = 3
	x1	x2	x3	x1 , x2	x1 , x3	x2, x3	x1, x2, x3
		independent features					
f1 (x) = x1	0.122	0	0	0.049	0.049	0	0.037
f2 (x) = x1 + x2	0.122	0.114	0	0.061	0.049	0.045	0.046
f3 (x) = x1 - x2	0.122	0.114	0	0.062	0.049	0.045	0.047
	correlated features:			x2 = 1	- x1		
f2 (x)	0.122	0.122	0	0	0.049	0.049	0
f3 (x)	0.122	0.122	0	0.098	0.049	0.049	0.073
Independent variables As only x1 influences f1 , only coalitions including it receive value; the
more diluted its role, the lower the value; x2 and x3 are symmetrically irrelevant. For f2 , x1 and
x2 play equal roles, and receive equal values (up to variance due to the sample size). For f2 and f3,
{x1, x2} receives similar value, and more than in f1, where only x1 influenced model predictions.
Comparing f2 and f3 shows that values only differ for coalitions containing both x1 and x2 .
6
Published as a conference paper at ICLR 2022
Correlated features f2 now exhibits what we term a cancellation effect between x1 and x2,
assigning a value of zero to coalitions containing both. Similarly, f3 reveals an enhancement effect:
values assigned to coalitions with both x1 and x2 are larger than in the independent case.
Enhancement and cancellation effects do not uniquely identify underlying phenomena. To illustrate,
let x1 , x2 and x3 be independent Bernoulli(0.5) random variables. Setting k = 3 and letting n → ∞,
we obtain the exact presence-adjusted global joint Shapleys in Table 3.
Table 3: Bernoulli(0.5) random variables; k = 3
f (x)	x1	x2	x3	x1, x2	x1, x3	x2, x3	x1, x2, x3
f1 (x) = x1	5/21	0	0	1/21	1/21	0	1/56
f2 (x) = x1 + x2	5/21	5/21	0	2/21	1/21	1/21	1/28
f4 (x) = x1x2	5/42	5/42	0	1/14	1/42	1/42	3/112
The joint Shapley values containing x1 and x2 are larger for models f2 and f4 than for model f1,
a different sort of enhancement effect from that above. However, the joint Shapley value for x1 is
smaller in f4 than it is in f1 or f2, again a different type of cancellation effect.
Finally, joint Shapley values can provide insight into the black-box model’s structure. If,
for example, f can be decomposed as f (x) = g (x1) + h (x2, . . . , xn) with independent
Bernoulli(0.5) random variables, then the presence-adjusted global joint Shapley value of x1 is
1/2 (g (1) - g (0)) Pn=-o1 (n- 1)qs = 5/21, as shown in the table. If the joint Shapley value deviates
from this, we reject the decomposition, as in the case of f4 .
4.2.2	Boston housing data
For comparability, we follow Dhamdhere et al. (2020) by training a random forest on the Boston hous-
ing dataset (Harrison & Rubinfeld, 1978), computing global values using the first notion discussed
above. Table 4 presents the largest and smallest global joint Shapley and Shapley-Taylor values.
Table 4: Joint Shapley values for the Boston dataset
Shapley (k=1)	k=2	ΦST (f ； k) k=3	k=2	φJ (f; k) k=3
RM: 2.57	RM: 3.12	RM: 3.12	LSTAT: 0.63	LSTAT: 0.42
LSTAT: 2.47	LSTAT: 2.04	LSTAT: 2.04	RM: 0.56	RM: 0.34
AGE: 0.81	DIS: 1.55	DIS: 1.55	LSTAT, RM: 0.30	AGE: 0.11
DIS: 0.63	CRIM: 1.37	CRIM: 1.37	AGE, LSTAT: 0.21	LSTAT, RM: 0.11
CRIM: 0.46	B: 1.31	DIS, LSTAT: 1.33	AGE, RM: 0.20	NOX: 0.10
NOX: 0.44	NOX: 1.15	B: 1.31	DIS, RM: 0.19	DIS: 0.08
PTRATIO: 0.27 B: 0.24	. . . CHAS, RM: 0.15	. . . AGE, CRIM, RM: 0.21	. . . CHAS, TAX: 0.02	. . . RAD, TAX, ZN: 0.00
TAX: 0.22	LSTAT, TAX: 0.15	AGE, DIS, PTRATIO: 0.21	RAD, ZN: 0.01	CHAS: 0.00
INDUS: 0.19	INDUS, RAD: 0.15	DIS, LSTAT, PTRATIO: 0.21	CHAS, RAD: 0.01	CHAS, RAD, TAX: 0.00
RAD: 0.13	NOX, TAX: 0.14	AGE, LSTAT, PTRATIO: 0.20	ZN: 0.01	CHAS, RAD, ZN: 0.00
ZN: 0.07	DIS, INDUS: 0.13	AGE, NOX, RM: 0.20	CHAS: 0.01	CHAS, TAX, ZN: 0.00
CHAS: 0.07	DIS, PTRATIO: 0.12	B, CRIM, LSTAT: 0.19	CHAS, ZN: 0.01	CHAS, ZN: 0.00
For k = 1, the joint Shapley values are the classical Shapley values. The unimportance of CHAS
seems to reflect its low variance (only 35 of 506 units back onto the Charles).
For k = 2, LSTAT and RM still have the largest joint Shapley values, with {LSTAT, RM} having
third largest. The top pairs all involve LSTAT or RM, indicating that these variables also contribute
to explaining house prices jointly with other variables. By contrast, {LSTAT, RM} is the 16th
largest Shapley-Taylor interaction value. The k = 2 singleton joint Shapley values are fairly evenly
distributed throughout the pairs; by contrast, the singleton k = 2 Shapley-Taylor values outrank all
the pairs. This is consistent with our extension of axiom NU to not favour singletons. The largest joint
Shapley value involving NOX is {NOX, LSTAT}, which is about five times as large as {NOX, DIS}
7
Published as a conference paper at ICLR 2022
and {NOX, RAD}. This is consistent with Harrison and Rubinfeld’s observation that NOX offset
RAD and DIS, but reinforced LSTAT: knowing the values of NOX and LSTAT adds a lot of predictive
power; knowing the values of NOX and RAD or DIS tends to wash out additional predictive power.
For k = 3, singletons again dominate: the largest triple is {AGE, LSTAT, RM}, the three individually
most important features. Continuing the above analysis, the joint Shapley value of {DIS, RAD} is
about 0.015, while that of {DIS, NOX, RAD} is about 0.008, and that of {DIS, LSTAT, RAD} is
about 0.022. We understand this to mean {DIS, NOX, RAD} contains the same sort of information as
{DIS, RAD}, but with NOX offsetting the other variables, while LSTAT brings novel socio-economic
information to the distance variables DIS and RAD, which it continues to partially offset. Similarly,
the least important joint features include the individually unimportant CHAS and its variants —-
echoing the role played by RM and LSTAT at the top of the list. While {DIS, LSTAT} has a Shapley-
Taylor value of 1.33 (indicating a large interaction between the pair but not its overall importance),
its joint Shapley is 0.06 (ranked 18th) — directly assigning a value of importance.
Table 4’s values are exact. Figure 1 demonstrates the estimation procedure above, showing con-
vergence of the sampled values as the iterations increase. As with other sampling procedures (e.g.
Strumbelj & Kononenko (2014)), this cuts the complexity to order n to a linear power of k; as k → n,
the complexity approaches 2n times a polynomial in n (compared to 3n for the exact φJ).
Figure 1: Sampled φJ converges to exact φJ
with L2 norm ≈ 1.45 ; k = 2 Boston
Figure 2: Difference between consecutive φJ
samples averages converges to zero; k = 2
movie review #2
4.2.3	Movie reviews
We train a fully connected neural network (two hidden layers, 16 units per layer, ReLU activations)
on the binary movie review classifications in Pang & Lee (2005). From the full set of reviews, we
remove a test block of 100 (picked to include those in Table 1 of Dhamdhere et al. (2020)) for analysis.
We encode reviews as the 1000 most common words in the corpus, augmented by key words in
Table 1 of Dhamdhere et al. (2020) to aid comparison of measures, for a total of 1004. (The DSA
features are drawn from positive reviews: {won’t}, {disappointed}, {both}, {inspiring}, {a}, {crisp},
{excellent}, {youthful}, {John}, {terrific}.) Binary accuracy is typically c. 76% after four epochs.
Table 5’s local joint Shapley values are tiny as each of the 21004 - 1 joint features has a tiny effect on
the probability of a positive review. In the vein of Dhamdhere et al., we identify intuitive effects:
1.	negation: the negative local joint Shapley values for {disappointed} and {be, disappointed}
become positive when negated by adding {won’t} to the coalition.
2.	enhancement: the positive local joint Shapley values for {both} and {and} are enhanced by
the positive {and, both}.
3.	context: the sign of local joint Shapley values involving {well} depend on the context; while
{you, well} is positive, {left, well} is negative.
4.	lost potential: the positive local joint Shapley values involving {fascinating} become
negative when {been} is added.
5.	adjectival: while {director} and {effort} are individually negative, they become positive
when qualified with the adjectives {winning} and {terrific}, respectively.
8
Published as a conference paper at ICLR 2022
Table 5: Examples of local joint Shapley values in the Pang & Lee (2005) movie reviews
Review	joint Shapleys
1: negation: aficionados of the whodunit won’t be disappointed	{disappointed}: -2 × 10-5 {won’t}: 6 × 10-5 {be, disappointed}: -9 × 10-8 {won’t, disappointed}: 6 × 10-8 {won’t, be, disappointed}: 5 × 10-9
2: enhancement: both inspiring and pure joy	{both}: 2 × 10-4 {and}: 6 × 10-5 {and, both}: 1 × 10-6
3: context: you wish Jacquot had left well enough alone	{you, well}: 9 × 10-7 {left, well}: -3 × 10-7
4: lost potential: fascinating little thriller that would have been perfect	{would}: -1 × 10-4 {fascinating}: 2 × 10-4 {would, fascinating}: 3 × 10-7 {would, been, fascinating}: -1 × 10-8
5: adjectival: director . . . award-winning . . . make a terrific effort	{effort}: -1 × 10-5 {director}: -9 × 10-6 {terrific, effort}: 8 × 10-7 {winning, director}: 5 × 10-7
For global values, as the features are binary, we compute the presence-adjusted global joint Shapleys,
φJ. Comparing the DSA features' global φJ across the test reviews for k = 1 and k = 2 shows
{terrific} to be the largest, followed by {both} then {excellent}. This reflects prevalence in the training
set, where {terrific} appears four times as often in positive reviews. Similarly, on the negative φJ
side, {John} is followed by {disappointed}: {John} appears thrice as often in negative reviews.
The largest pairs are {both, inspiring} followed by {excellent, youthful}. All of the pairs, except
{John, terrific}, have a positive sign: these pairs typically do not appear outside the positive reviews
from which they were drawn. As our encoding discards sequential information, the pairs are smaller
than the singletons: a coalition merely indicates co-occurrence in a review, rather than a bigram.
As the 10 DSA features are a very small subset of all 1004, we cannot asses whether JEF holds.
However, as a health check note: the average presence-adjusted local joint Shapleys over the positive
reviews is about 20% larger than that over the negative reviews. They are both negative: as these
features are drawn from positive reviews, but largely absent in any given review, their effect is
negative. Further, Table 6 indicates that the ranking of single features is largely preserved as k
increases from 1 to 2: the exception is {a, crisp}: in the corpus, {crisp} is typically preceded by {a}.
Figure 2 shows the estimated values’ convergence.
Table 6: Largest presence-adjusted global joint Shapley values on DSA features
k = 1	terrific	both	excellent	won’t	crisp	youthful	a	inspiring	disappointed	John
k = 2	terrific	both	excellent	won’t	a	youthful	inspiring	crisp	disappointed	John
5	Conclusions
The joint Shapley value directly extends Shapley’s value to measure the effect of a set of features on a
model’s predictions. Further work is needed to maintain properties like global efficiency (Covert et al.,
2020) and to make sampling more efficient (Williamson & Feng, 2020; Mitchell et al., 2021). We
believe that understanding complex models is labor intensive. Nevertheless, we envisage a common
workflow that computes k = 1 Shapley values, followed by k = 2 to identify strong pairwise effects;
analyses for k ≥ 3 then respond to the analyst’s evolving questions about the model’s functioning.
9
Published as a conference paper at ICLR 2022
6	Ethics statement
This paper contributes to the literature on explainable AI, an important component of the Fairness,
Accountability and Transparency research agenda for ethical AI. It does not use human subjects; it
only draws on publicly available datasets; the work has not been sponsored, and does not seek to
promote any third organisations; none of the authors face any conflict of interest.
7	Reproducibility statement
Our proofs and source code are available in the accompanying supplemental material; all data are
taken from the public domain.
References
Bedoor K Alshebli, Tomasz P Michalak, Oskar Skibski, Michael Wooldridge, and Talal Rahwan.
A measure of added value in groups. ACM Transactions on Autonomous and Adaptive Systems
(TAAS),13(4):1-46, 2019.
Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep
Ghosh, RUchir Puri, Jose M. F. Moura, and Peter Eckersley. Explainable machine learning in
deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,
pp. 648-657, 2020.
Ian Covert, Scott Lundberg, and Su-In Lee. Understanding global feature contributions with additive
importance measures. arXiv preprint arXiv:2004.00668, 2020.
Kedar Dhamdhere, Ashish Agarwal, and Mukund Sundararajan. The Shapley Taylor interaction
index. In International Conference on Machine Learning, pp. 9259-9268. PMLR, 2020.
Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among
players in cooperative games. International Journal of Game Theory, 28(4):547-565, 1999a.
Michel Grabisch and Marc Roubens. Probabilistic interactions among players of a cooperative game.
In Beliefs, Interactions and Preferences in Decision Making, pp. 205-216. Springer, 1999b.
David Harrison and Daniel L Rubinfeld. Hedonic housing prices and the demand for clean air.
Journal of Environmental Economics and Management, 5(1):81-102, 1978.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances
in Neural Information Processing Systems, pp. 4765-4774, 2017.
Marcin Malawski. A note on equal treatment and symmetry of values. In Transactions on Computa-
tional Collective Intelligence XXXV, pp. 76-84. Springer, 2020.
Jean-Luc Marichal, Ivan Kojadinovic, and Katsushige Fujimoto. Axiomatic characterizations of
generalized values. Discrete Applied Mathematics, 155(1):26-43, 2007.
Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for Shapley
value estimation. arXiv preprint arXiv:2104.12199, 2021.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the ACL, 2005.
Neel Patel, Martin Strobel, and Yair Zick. High dimensional model explanations: an axiomatic
approach. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Trans-
parency, pp. 401-411, 2021.
Lloyd S. Shapley. A value for n-person games. In Harold William Kuhn and Albert William
Tucker (eds.), Contributions to the theory of games, volume II of Annals of Mathematical Studies,
chapter 17, pp. 307-317. Princeton University Press, Princeton, 1953.
10
Published as a conference paper at ICLR 2022
Brian Williamson and Jean Feng. Efficient nonparametric statistical inference on population feature
importance using Shapley values. In International Conference on Machine Learning, pp. 10282-
10291. PMLR, 2020.
Erik Strumbelj and Igor Kononenko. An efficient explanation of individual classifications using game
theory. The Journal of Machine Learning Research, 11:1-18, 2010.
Erik Strumbelj and Igor Kononenko. Explaining prediction models and individual predictions with
feature contributions. Knowledge and Information Systems, 41(3):647-665, 2014.
In this supplementary material we discuss the arrival order interpretation, present the proofs of all
results, and discuss a notion of joint symmetry obtained by removing conditions 2 and 3 from JSY.
A Arrival order interpretation
As discussed in the paper, the joint Shapley value can be viewed in terms of the worth brought by
‘arriving’ agents but, rather than arriving one at time, they can now also arrive in coalitions. To
be precise, consider this procedure: at time 0, no agents have arrived; at each t ∈ {1, 2, . . .}, the
next set of agents to arrive is chosen uniformly from the set of non-empty subsets of size at most
k of the remaining (yet to arrive) agents. Then φJT is the expected worth brought by coalition T
when it arrives (a coalition is assigned zero worth if it does not arrive at any time). To see this,
denote by Ai the coalition to arrive at time i, by Bi the union of all coalitions that have arrived
up to time i: Bi = Sj≤i Aj, and by pT the probability that at some time coalition T arrives,
pT = P(∃ i : Bi = T). We have the recursive relationship:
n
pT = X X	P(Bi-1 = S)P(Ai =T\S | Bi-1 =S)
i=1	S(T:
∣s∣≥∣τ l-k
n
X X	P(Bi-1 = S)
i=1 S(T:
∣s∣≥∣τ l-k
(n-| S∣ )∧k
Σ
r=1
lTl-1
X
s=(lT l-k)∨0
(n-s)∧k
rX=1	n-r s
-1
where in the last line S is any set of size s. Thus we see that pT only depends on T through its
cardinality, and defining
p^t := PT
(n-t)∧k
rX=1	n-rt
-1
for any T with |T | = t, the expected worth brought by coalition T under this procedure is
n
X X P(Bi-I = S, Ai = T)[v(S ∪ T)-v(S)]= X P∣s∣[v(S ∪ T)-v(S)].
i=1 S⊆N\T	S⊆N\T
Further, we have the relationship
Pt
∑(n-t)∧k /n-t`
r=1	r
and since we know that P0 = 1 (we start with no agents having arrived) and Pn = 1 (we finish with
all agents having arrived) we also have the identities:
1
ps,
k
1 = po X
r=1
By comparing with the identities defining (qo,..., qn-ι), We deduce that Pt = qt for all t ∈
{0, . . . , n - 1}, which verifies this arrival interpretation.
11
Published as a conference paper at ICLR 2022
B Proofs
ProofofLemma 2. For the first statement, for each 0 = S ⊆ N \ T consider the game
vS (R) =	10
if R = S or R = S ∪ T,
otherwise.
Then for such S, by JNU, φT (vS) = 0. By this equality, Lemma 1, and the definition ofvS,
0 = φT(vS) =	aTRvS (R) = aTS + aTS∪T.
R⊆N
For the second statement, for each 0 6= H ( T let αH be a constant and for S ⊆ N \ T, consider
the game
α	αH if R = S ∪ H for some 0 6= H ( T,
xS	0 otherwise.
By JNU, φT (xSα) = 0 for every S ⊆ N \ T. Thus by Lemma 1
0 = φT (xSα ) = X aTRxSα = X aTS∪H xSα (S ∪ H) = X aTS∪H αH .
R⊆N	0=H (T	0=H(T
Since this holds for every choice of constants αH, it follows that aTS∪H = 0 for all S ⊆ N \ T and
0 = H ( T, as required.	口
Proofof Proposition 1. By Lemma 1 there exist constants {aT}s⊆n O=T⊆n such that for every V
and 0 6= T ⊆ N,
φT(V)=	EaTV(S)	= E	I aSV(S)	+ E aS∪HV(S ∪ H) +	aS∪TV(S	∪ T)
S⊆N	S⊆N\T	∖	0=H(T
= X (aTS V(S) +aTS∪TV(S∪T)) = X aST∪T[V(S∪T) - V(S)];
S⊆N\T	S⊆N\T
where the last two equalities owe to Lemma 2. The proof is complete by setting PT(S) = aT∪τ. 口
Proof of Proposition 2. Suppose φ satisfies axioms JLI, JNU and JEF. Then by Proposition 1 the
constants {pT(S)} exist, such that for every V and 0 6= T ⊆ N,
φT(V) = X pT(S)[V(S∪ T) - V(S)].
S⊆N\T
Now for each 0 6= R ⊆ N consider the identity game
wR(S) = 1 ifS=R,
0 otherwise.
Then for every 0 6= T ⊆ N with |T | ≤ k,
φT(wR) = X pT(S)[wR(S ∪ T) - wR(S)].
S⊆N\T
Note that the term wR(S ∪ T) - wR(S) in the above sum is equal to 1 only when S ( R and
T = R \ S, i.e. only when S = R \ T and 0 6= T ⊆ R. Further note that this term is equal to -1
only when S = R and T 6= 0, i.e. when S = R and 0 6= T ⊆ N \ R (as must have S ∩ T = 0). In
all other cases, this term is 0. Hence we deduce from JEF that
δN (R) = wR(N) = X pT(R \ T) - X	pT (R).
06=T ⊆R:	06=T ⊆N \R:
∣T∣≤k	∣T∣≤k
12
Published as a conference paper at ICLR 2022
Now we show the implication in the other direction. If φT (v) = PS⊆N \T pT (S)[v(S ∪ T) - v(S)]
then it is immediate that JLI and JNU are satisfied. For JEF, we wish to show that for every v,
X X pT(S)[v(S∪T)-v(S)] =v(N).
0=T⊆N: S⊆N\T
∣T∣≤k
Note that for each 0 = R ⊆ N, the coefficient of V(R) on the left-hand side in the above equation is
X pT(R\T) - X	pT(R).
0=T ⊆R:	0=T ⊆N\R:
|T ∣≤k	∣T∣≤k
But by equation (4), this is equal to 5n(R).	□
Proof of Proposition 3. In light of Proposition 2 we just have to consider JAN.
Only if: Suppose φ satisfies JLI, JNU, JEF and JAN. First, we shall establish that
pT (S) = pT (S0) ∀0 6= T ⊆ N, S,S0 ⊆ N\T s.t. s = s0.	(9)
Fix such a T, S and S0. Consider again the identity game, wS and let σ be a self-inverse permutation
such that S 7→ S0, S0 7→ S, and σ ({i}) = {i} for all i 6∈ S ∪ S0. As T ⊆ N \ (S∪S0) and
σ (T ) = T , we have by JAN
φT (wS) = φσ-1(T) (wS) = φT (σwS)
where
σwS (R) = WS (σ-1 (R)) = { 0 otherwiσe(S) = So } = WSO(R).
Hence we obtain φT(wS) = φT (σwS) = φT (wS0). Next, from Proposition 1 we have
φT (WS) =	pT (Q) [WS (Q ∪ T) - WS (Q)] = -pT (S) ,
Q⊆N \T
and similarly φT (WS0) = -pT (S0). Hence we obtain pT (S) = pT (S0), showing (9).
Using induction on s, we now establish that (5) holds. Fix T and T0 of the same size. For the base
case, suppose s = s0 = n - t. For S ⊆ N \ T and S0 ⊆ N \ T 0, this forces S = N \ T and
S0 = N \ T0. Now consider the game
xn (R) =	10
if r = n
otherwise,
so that xn (R) = 1 if and only if R = N. Define a self-inverse permutation σ so that σ (T ) = T0,
σ (T0) = T and σ ({i}) = {i} for all i 6∈ (T ∪ T0). Then by JAN and as σxn = xn,
φT(xn) = φσ-1(T 0)(xn) = φT0 (σxn) = φT0(xn).
Next, from Proposition 1,
φT (xn) = X pT(Q)[xn(T∪Q)-xn(Q)] =pT(N\T),
Q⊆N \T
and similarly φT0 (Wn) = pT0 (N \ T0). Hence we obtain pT (N \ T) = pT0 (N \ T0) which
establishes the base case.
We now suppose that pT (S) = pT0 (S0) for all s = s0 ≥ n - c where S ⊆ N \ T, S0 ⊆ N \ T0 and
c is a positive integer. We shall show that pT (S) = pT0 (S0) for all s = s0 ≥ n - c - 1 where where
S ⊆ N \ T and S0 ⊆ N \ T0. To this end, consider the game
x (R) = 10
if r ≥ n - c - 1 + t,
otherwise
13
Published as a conference paper at ICLR 2022
Thus, as before, we may write
φT (x) =	X	pT	(Q)	[x (Q	∪T) - x (Q)]	= X	pT	(Q)	.
Q⊆N \T	Q⊆N \T
n-c-1≤q<n-c-1+t
Similarly,
φT0 (x) =	X	pT (Q) .
Q0⊆N\T0
n-c-1≤q0<n-c-1+t
Again, by JAN, we prove that φT (x) = φT0 (x). Define a self-inverse permutation σ so that σ (T) =
T0, σ (T0) = T and σ ({i}) = {i} for all i 6∈ (T ∪ T0). As worth in game x depends only on a
coalition’s cardinality, we have σx = x. Thus, by JAN, φT (x) = φσ(T) (σx) = φσ(T) (x) = φT0 (x).
However,
φT (x)
pT (Q)+	pT (Q)
Q⊆N \T	Q⊆N \T
q=n-c-1	n-c-1<q<n-c-1+t
pT (Q)+	pT0 (Q0);
Q⊆N\T	Q0⊆N\T0
q=n-c-1	n-c-1<q0<n-c-1+t
and
φT0 (x) = X	pT0 (Q0) +	X	pT0(Q0)
Q0⊆N\T0	Q0⊆N\T0
q0=n-c-1	n-c-1<q0 <n-c-1+t
which gives, by the inductive hypothesis and φT (x) = φT0 (x),
pT(Q)=	pT0 (Q0).
Q⊆N\T	Q0⊆N\T0
q=n-c-1	q0=n-c-1
But by (9), pT (Q) = pT (Q0) if q = q0. Thus the above equation becomes
1 pT (Q)
n-t
n-c- 1
pT0 (Q0)
for any Q ⊆ N \T and Q0 ⊆ N \T0 with q = q0 = n - c - 1. Thus, pT (Q) = pT0 (Q0), completing
the inductive step, and the ‘only if’ statement.
If: Suppose (5) is satisfied. Fix a permutation σ on N and game V ∈ GN. Then for any 0 = T ⊆ N,
Φt (σv)	= E	PT(S)[σv (S ∪	T)	- σv (S)] = E	pT (S) [v (σ-1 (S ∪	T))	- v	(σ-1	(S))]
S⊆N\T	S⊆N\T
=X PT(S)[v(σ-1(S) ∪ σ-1(T)) - v (σ-1 (S))].
S⊆N\T
Defining the set S0 = σ-1 (S) allows us to rewrite the above as
φτ (σv) =…= X Pt (σ (SO)) [v (s'∪ σ-1(T)) -V (S0)]
S0⊆N ∖σ-1(T)
= X	pσ-1(T) (SO) [v(S0∪ σ-1 (T)) -V (S0)]= φσ-i(T)(v),
S0⊆N ∖σ-1(T)
with the penultimate step due to condition (5).	□
Proof of Proposition 4. In light of Proposition 2 we just have to consider JSY.
Only if: Suppose φ satisfies JLI, JNU, JEF and JSY, fix 0 6= T, TO ⊆ N, and consider again the
identity game wR. Then for any 0 6= R ⊆ N \ (T ∪ TO),
•	wR(S ∪ T) = 0 = wR(S∪TO) for all S ⊆ N\ (T∪TO),
14
Published as a conference paper at ICLR 2022
•	WR(S ∪ T) = 0 = WR(S) for all S ⊆ N \ T such that S ∩ T0 = 0,
•	WR(S ∪ T0) = 0 = wr(S) for all S ⊆ N \ T0 such that S ∩ T = 0.
Hence by JSY φT (WR) = φT0 (WR). But φT (WR) = pT (R) and φT0 (WR) = pT (R). This shows
that pT (R) = pT0(R) for all 0 6= R ⊆ N \ (T ∪ T0). To show that pT (0) = pT0 (0) we consider
the game
W* (S) = [1 if S= 0,
0 otherwise.
Then
•	W*(S∪T) = 1 = W*(S∪T0) for all S ⊆ N\ (T∪T0),
•	W*(S ∪ T) = 1 = W* (S) for all S ⊆ N \ T such that S ∩ T0 6= 0 (since then S 6= 0),
•	W*(S ∪ T0) = 1 = W* (S) for all S ⊆ N \ T0 such that S ∩ T 6= 0 (since then S 6= 0).
It thus follows by JSY that φT (W*) = φT0 (W*). However, φT (W*) = pT (0) and φT0 (W*) =
pT 0 (0), which gives the required identity and shows that (6) holds.
If: Now we show the implication in the other direction. Suppose (6) holds and v ∈ GN satisfies the
three conditions in JSY. Then
φT (v) = X pT(S)[v(S∪T)-v(S)] = X	pT(S)[v(S∪T)-v(S)]
S⊆N\T	S⊆N \(T ∪T0)
= X	pT0 (S)[v(S ∪ T0) - v(S)] = X pT0(S)[v(S∪ T0) - v(S)] = φT0(v).
S⊆N\(T ∪T 0)	S⊆N\T 0
Hence JSY is satisfied.	□
Proof of Theorem 1. We have to show that there exists exactly one choice of constants {pT(S)}
which satisfy equations (4)-(6). Notice that satisfying (5) and (6) is equivalent to satisfying
pT(S) =pT0(S)∀S ⊆ N\T, S0 ⊆ N\T0 s.t. s = s0.
Thus pT(S) does not depend on T at all, and only depends on the cardinality of S. Let qs denote
pT(S) for any S ⊆ N \ T. Then we can re-write equation (4) in terms of qs as
qs
n-1
X niqi,
i=n-k
Ps-(Is-k)∨0 (s)%
(10)
∑k∧(n-s) ∕n-s)
i=1	i
∀ s ∈ {1, . . . , n - 1}.
(11)
1
Note that for any q0, equation (11) fully determines all other qi, for i ∈ {1, . . . , n - 1} and q0 is
then determined by (10). Thus there is at most one solution. However, we have already identified
(see the arrival-order discussion in Appendix A) that a solution to this recurrence is given by
(qo,...,qn-ι) = (p0,...,pn-1),forwhichp0 = (Pk=I (n))
□
C S trong joint symmetry
We examine the effect of removing conditions 2 and 3 from JSY. As it turns out, this leads to the
non-existence of an index. To be precise, we consider replacing axioms JAN and JSY with:
SJS strong joint symmetry : fix 0 6= T, T0 ⊆ N. Then
V (S ∪ T) = V (S ∪ T0) ∀S ⊆ N\ (T ∪ T0)
⇒ φT (v) = φT0 (v) .
15
Published as a conference paper at ICLR 2022
Proposition 5. There is no index φ satisfying axioms JLI, JNU, JEF, and SJS that is guaranteed to
exist for all games.
Proof. Since φ satisfies JLI, JNU, and JEF, by Proposition 2,
φT(v) = X pT(S)[v(S ∪ T) - v(S)]
S⊆N\T
with {pT (S)} satisfying (4), for any game v ∈ GN. We consider two games v1, v2 ∈ G{1,2}. As
N = {1,2}, (4) gives P{1}(0) = P{2}({1}), P{2}(0) = P{1}({2}), and p{1,2}(0) + P{1}(0) +
P{2} (0) = 1.
Suppose v1({1}) = v1({1, 2}) = 1, v1({2}) = 0. SJS thus gives that φ{1}(v1) = φ{1,2}(v1), i.e.
P{1,2}(0) = P{1}(0) + P{2}(0) which implies P{1,2} (0) = 1/2.
Suppose also that v2({1}) = v2({1, 2}) = v2 ({2}) = 1. SJS gives that φ{1} (v2) = φ{2} (v2) =
φ{1,2}(v2), i.e. P{1,2}(0) = P{1}(0) = P{2}(0) which implies P{1,2}(0) = 1/3, giving a contra-
diction.	□
Thus, SJS is too strong a notion of symmetry, imposing linear restrictions on sets of unequal sizes.
16