Published as a conference paper at ICLR 2022
Structure-Aware Transformer	Policy	for
Inhomogeneous	Multi-Task	Reinforcement
Learning
Sunghoon Hong1,3, Deunsol Yoon1,3, Kee-Eung Kim1,2
1 Kim Jaechul Graduate School of AI, KAIST, Daejeon, Republic of Korea
2School of Computing, KAIST, Daejeon, Republic of Korea
3 LG AI Research, Seoul, Republic of Korea
{shhong01, solpino, kekim}@kaist.ac.kr
Ab stract
Modular Reinforcement Learning, where the agent is assumed to be morpholog-
ically structured as a graph, for example composed of limbs and joints, aims to
learn a policy that is transferable to a structurally similar but different agent. Com-
pared to traditional Multi-Task Reinforcement Learning, this promising approach
allows us to cope with inhomogeneous tasks where the state and action space
dimensions differ across tasks. Graph Neural Networks are a natural model for
representing the pertinent policies, but a recent work has shown that their multi-
hop message passing mechanism is not ideal for conveying important information
to other modules and thus a transformer model without morphological informa-
tion was proposed. In this work, we argue that the morphological information is
still very useful and propose a transformer policy model that effectively encodes
such information. Specifically, we encode the morphological information in terms
of the traversal-based positional embedding and the graph-based relational em-
bedding. We empirically show that the morphological information is crucial for
modular reinforcement learning, substantially outperforming prior state-of-the-art
methods on multi-task learning as well as transfer learning settings with different
state and action space dimensions.
1	Introduction
Deep reinforcement learning (RL) has made remarkable successes over the last several years, achiev-
ing human-level performance in various tasks (Silver et al., 2016; Mnih et al., 2015). However, these
are limited by individually training policies for each task, often requiring a large amount of interac-
tion data. Multi-task learning, an approach to training a model jointly from diverse tasks in order
to improve learning efficiency and prediction accuracy of task-specific models by exploiting com-
monalities and differences across tasks, has become prevalent in computer vision (CV) (Dosovitskiy
et al., 2021) and natural language processing (NLP) (Devlin et al., 2019; Radford et al., 2019).
In this regard, Multi-Task Reinforcement Learning (MTRL) is a promising approach, but training
a policy for multiple tasks in the traditional manner is not straightforward in many cases, e.g., a
policy designed for a specific robot cannot be reused for another one with a different embodiment.
Therefore, most MTRL methods assume same state and action dimensions across tasks (Rusu et al.,
2016; Parisotto et al., 2016; Pinto & Gupta, 2017; Yang et al., 2020; Kalashnikov et al., 2021) and
define each task by its own reward function, e.g., grab a cup or move it with a robot arm. Instead,
we are interested in a more general setting where the tasks are inhomogeneous, i.e., of different state
and action space dimensions, also known as incompatible control (Kurin et al., 2021).
One of the popular approaches to inhomogeneous MTRL is to assume a graph structure for the agent,
depicted in Figure 1 as an example, where limbs and joints are represented as nodes and edges. In
this modular setting, Graph Neural Networks (GNNs) provide a natural choice for the model of the
policy, since (1) they can process graph-structured input of arbitrary sizes and connections, which
allows us to obtain a single policy model that can control any agent with various morphology leading
1
Published as a conference paper at ICLR 2022
Figure 1: Illustration of modular robot locomotion task. Each node is corresponded to a limb, and
each non-torso limb has an actuator to execute an action, which controls the torque of the joint
connecting two limbs.
to different state and action space dimensions, and (2) they natively exploit the inductive bias that
arises from the morphological structure through their message-passing (MP) scheme, i.e., messages
are allowed to be delivered only to nodes connected by edges. Hence, GNN-based approaches
have shown to yield significantly better results in inhomogeneous MTRL, compared to the vanilla
approaches that don’t take into account the morphological structure of the agent.
Recently, Kurin et al. (2021) posed a concern that the MP scheme in GNN is prone to over-
smoothing (Li et al., 2018), where the crucial information is washed out during the multi-hop
communication. They argued that the advantage of leveraging morphological information for in-
homogeneous MTRL was overshadowed by this problem. They therefore advocated adopting the
self-attention mechanism, which allows direct communications among nodes, while sacrificing the
morphological information. Yet, it is well known that injecting the structural inductive bias into
the self-attention mechanism, i.e., positional embedding, can help significantly improve the per-
formance (Vaswani et al., 2017). In the same context, the states of neighbor nodes may be more
important than those of non-neighbor nodes for determining actions in modular RL.
In this paper, we introduce Structure-aWAre Transformer (SWAT), a new modular method for in-
homogeneous MTRL that effectively incorporates the agent morphology into the transformer-based
policy. We propose two forms of structural embeddings of the morphology to be incorporated into
the transformer model: (1) traversal-based positional embedding (PE) which represents absolute po-
sitions of nodes via tree-traversal algorithms, and (2) graph-based relational embedding (RE) which
represents relative distances of node pairs reflecting their connectivity. These allow direct commu-
nication of messages among nodes while taking into account the agent morphology.
The experimental results on benchmark tasks for inhomogeneous MTRL strongly support the ef-
fectiveness of our structural embedding approach, outperforming prior state-of-the-art methods in
terms of sample efficiency and final performance. Through transfer learning experiments, we also
demonstrate that the structural information conveyed by our approach plays an important role for
downstream tasks.
2	Background
2.1	Inhomogeneous MTRL
We assume a Markov Decision Process (MDP) defined by tuple M = (S, A, T, ρ0 , r, γ) to represent
an RL task, where S is the state space, A is the action space, T (st+1 |st, at) is the state transition
probability, ρ0 is the initial state distribution, rt = r(st , at) ∈ R is the reward associated with
state and action, and γ ∈ (0, 1) is the discount factor. The objective of RL is to learn a probability
distribution over actions conditioned on states, i.e., policy π(at ∣st), which maximizes the expected
discounted return R = E[ t≥0 γtrt].
2
Published as a conference paper at ICLR 2022
In the inhomogeneous MTRL, the agent is tasked with a set of inhomogeneous MDPs M =
{M1,M2,…,MK}. We say MDPs are inhomogeneous when any pair of MDPs have different
dimensionalities in their state or action spaces, i.e., dim(Si) 6= dim(Sj) or dim(Ai) 6= dim(Aj),
where Mi, Mj ∈ M andSi, Ai is the state and action space ofMi. In the context of MTRL, the goal
is to find a policy that maximizes the average expected discounted return over all the environments,
KK PK=ι E[Ri] where Ri denotes the expected discounted return in MDP Mi.
We now describe modular RL, where the agent can be represented as an undirected graph G =
(V, E), e.g., the robotic agent with n limbs and l joints as in Figure 1. Each node (vertices) vi ∈
V for i ∈ 1...n represents a limb, and an undirected edge ei,j ∈ E represents connectivity by
a joint between a vi and vj . For the ease of notation, we assume an ordered set of nodes, i.e.,
a node vi is i-th element in the ordered set. E can be represented as an adjacency matrix A ∈
{0, 1}n×n that describes the connectivity among nodes, i.e., if vi, vj are connected, Ai,j = 1 and
Pi,j Ai,j = 2l. Thus at each time step t, the agent observes the state st, which consists of local
sensory information of the limb such as limb type, coordinates, and angular velocity. Denoting the
local sensory information of the limb vi at time step t as vector vti , the state st can be represented as
{vt1, vt2, . . . , vtn} with morphology E. Given st, the policy outputs the action at = {at1, at2, . . . , atn}
where ait is the torque value for the corresponding actuator controlled by the limb vi .
2.2	Policy models for inhomogeneous MTRL
Traditional policy model for RL, e.g., multi-layer perceptron (MLP) policy, is not suitable for inho-
mogeneous MTRL since it can only handle the state and action spaces with the same dimension. In
this regard, GNNs (Hamilton et al., 2017; Wu et al., 2020) have been a natural choice for modeling
policies (Wang et al., 2018; Huang et al., 2020), since they can scale to different state and action
dimensions as well as incorporate the agent morphology in their message-passing (MP) scheme.
Let us assume a graph G = (V, E) where each node vi ∈ V has the corresponding information, i.e.,
node representation or message vi . The messages in GNNs are delivered as follows:
1.	Message aggregation： mi - σ({vj : j ∈ NiY), ∀vi ∈ V
2.	Node update:VuPdate 一 fθ(Vi, mi), ∀Vi∈V,
(1)
where Ni is the indices of the neighborhood of vi, σ is a message aggregation function, e.g., average
or concatenation, fθ is a parameterized node update function that computes a new message given the
original message vi and aggregated message mi. GNNs consist of the stack of multiple GNN-layers
for multi-hop communication, where a single layer operates a single iteration. GNNs are suitable for
inhomogeneous MTRL with various agent morphologies, since each node is processed in a modular
manner, allowing a single policy to be learned for diverse agents. In particular, Shared Modular
Policies (SMP) (Huang et al., 2020) assumes a tree morphology and adopts both-way MP scheme
where messages are delivered back and forth between a root and leaves, showing significantly better
performance than a standard monolithic policy in inhomogeneous MTRL.
The self-attention mechanism, introduced in the transformer (Vaswani et al., 2017), is another way
of passing messages. Unlike GNNs, it assumes that all nodes in a graph are connected, even enabling
direct communication among distant nodes. specifically, each node feature Vi ∈ Rdv is projected to
three vectors, qi, ki ∈ Rdk, Vi ∈ Rdv, where qi = ViWq, ki = ViWk, Vi = ViWv are a query,
key, value vectors respectively, and Wq, Wk ∈ Rdv ×dk , Wv ∈ Rdv ×dv are learnable projection
mappings. Messages are then delivered in the self-attention mechanism as:
αi,j = qikT	wi,j =	eχp(αi,j)
=√dk ,	= Pg，exp(αi,j0),
VUPdate — Ewij Vj,
j
(2)
where αi,j is the unnormalized attention score, and wi,j is the attention weight.
When the agent morphology is a sparse graph, which is quite usual for many agents in nature,
the crucial information tends to be washed out during multi-hop communication, an undesirable
phenomenon known as over-smoothing. In order to avoid the issue, AMORPHEUs (Kurin et al.,
2021) adopts the self-attention mechanism and computes the attention weight over all nodes in the
graph, yielding better results than the previous GNN-based approaches. However, it discards the
morphological information although it is potentially very useful.
3
Published as a conference paper at ICLR 2022
2.3	Positional information in Transformers
In the transformer, injecting the positional information is essential since the self-attention mecha-
nism is permutation invariant (Vaswani et al., 2017). In the case of NLP, a permutation of tokens
can change the whole context and meaning of a sentence. Thus, the structural bias, representing the
positions of tokens, is explicitly given as a form of positional embedding (PE), which is found to
be significantly useful at learning the contextual representations of words in different positions (Ke
et al., 2021; Wang & Chen, 2020). To be specific, the absolute positional information for the i-th
token vi in the sequence is given by the absolute PE vector pi and it is added to the corresponding
token embedding vi . On the other hand, the relative positional information between two tokens,
e.g., relative distance (i - j), is given by relational embedding (RE), ri,j ∈ R, which is added to the
attention score as ai,j = qikjT/√dk + ri,j (Raffel et al., 2020; ShaW et al., 2018).
3	Motivation for S tructural Bias
Figure 2: Training curve
on Animal-Walkers
AMORPHEUS (Kurin et al., 2021) proposes a plain transformer
model without morphological information due to the aforementioned
over-smoothing problem. However, we can still incorporate the mor-
phological information into the transformer, i.e., the structural embed-
ding widely utilized in NLP to add the structural bias.
In order to appreciate its effect in MTRL, we apply 2 common methods
used in NLP models, the learnable embedding (LEARN) and the sinu-
soidal encoding (SINU). LEARN uses a learnable vector to represent
the absolute position of each token in a sequence, while SINU uses
a sinusoidal function that can represent the relative position between
tokens as well. First, we converted the morphology into a sequence
by labeling the index of each node along with the pre-order traver-
sal of the tree with a torso labeled as a root. We then aggregated the
sinusoidal encoding or the learnable embedding with the node repre-
sentations to inject the structural bias into AMORPHEUS. We run AMORPHEUS, LEARN, and
SINU on Animal-Walkers, which are trained jointly for 5 morphologically different agents.
The results in Figure 2 show how each of the embeddings affects the MTRL performance. LEARN
improves the performance the most, because it can roughly capture the knowledge about where
each limb is placed. On the other hand, SINU suffers learning instability and the performance
rather worsens. The reason for the deterioration, we conjecture, is that SINU encodes morphology
incorrectly. SINU, devised to capture a relative distance in sequential data, can give misguided
relational information when it is applied to graph-structured data. For example, the distances from
a torso to both arms are the same in a graph, but SINU regards them differently. Although this is a
preliminary observation, it manifests the necessity of a pertinent embedding scheme for representing
both the positional and the relational information of morphology for inhomogeneous MTRL.
4	Method
Motivated by the result in the previous section, we present the structural embedding scheme that can
be effectively incorporated into the transformer-based policy. It allows our policy to leverage the
morphological information while free from the over-smoothing. We introduce two components for
injecting the structural bias into the transformer: traversal-based PE and graph-based RE.
4.1	Traversal-based Positional Embedding
Unlike language sequences in NLP, it is not straightforward to capture the positional information of
each node in a graph. Yet, if we assume tree-structured robot morphology defining a torso as the
root node, as done in SMP (Huang et al., 2020), we can represent it as a combination of multiple
sequences in terms of tree traversals. We can traverse any tree by several consistent orderings, e.g.,
pre-order or post-order traversal. Although a single traversal sequence, as in section 3, is ambiguous
4
Published as a conference paper at ICLR 2022
to reconstruct the given tree, we can uniquely identify a binary tree with in-order traversal along
with another traversal (Burgdorff et al., 1987).
Inspired by this property, we introduce traversal-based PE for representing the position of each node
in the given tree. Since we assume an agent morphology as a general tree with an arbitrary number
of children, we apply left-child-right-sibling representation (LCRS) to represent a general tree as
a binary tree. Then, we traverse the binarized tree by pre-order, in-order, and post-order, which is
sufficient to reconstruct the original tree. A node position can be represented as a tuple that consists
i	ii
of the position in each traversal. Concretely, pvpre , pivn , ppvost ∈ N denote the indices of a node vi
in each traversal ordering respectively, and we assign the PE pi ∈ Rdv to the node vi using the
i	ii
learnable embedding vectors pvpre , pivn , ppvost as follows:
i	ii
pi = AGGREGATE([pvpre, pivn, ppvost]),	3
()
Vi J Vi + pi
We simply concatenate the embedding vectors in this work in aggregate. By combining multiple
traversals, the PE assigned to each node can uniquely represent the node position in the whole
morphology.
4.2	Graph-based Relational Embedding
In a graph, the relation among nodes also provides structural information from another perspective
than the position. Compared to the node positional information above, the relational information
contains more explicit knowledge between a pair of nodes. We introduce graph-based RE by utiliz-
ing three meaningful features extracted from the graph, which are incorporated into the transformer.
Normalized Graph Laplacian The Graph Laplacian (Merris, 1994) is a matrix that represents
connectivity in terms of both adjacency and node degrees in a graph, defined as L = D - A where
A is an adjacency matrix and D is a diagonal degree matrix, i.e., Di,i = |Ni|. We use the normalized
graph Laplacian to bound its norm, which is defined as Rlap = D- 1 LD-2. Then, Rlap ∈ Rn×n
represents the relation among neighbors, capturing local information.
Shortest Path Distance Shortest path distance (SPD) measures the distance between two nodes in
a graph, which can be easily computed by several algorithms such as breadth-first search (BFS). Itis
similar to the relative positional encoding approach in NLP domains, but we use real-value distance
Rspd ∈ Rn×n divided by the number of nodes n to bound in [0, 1], i.e., Ris,pjd = SPD(i, j)/n
where SP D(i, j) denotes the shortest path distance between vi , vj . SPD considers the direct path
among all nodes, so we can say it captures the global information.
Personalized PageRank Personalized PageRank (PPR), a variation of PageRank (Page et al.,
1999), represents the proximity between two nodes in a graph based on a random walk model (Park
et al., 2019), which is widely utilized in the graph domain (Klicpera et al., 2019; Bojchevski et al.,
2020). PPR is a node visitation probability distribution of a -discounted random walk model with
a restarting node. To be concrete, the random walker travels across the Markov chain formed via
the graph with its transition probability matrix P ∈ Rn×n, where Pi,j = 1/|Ni| for ∀vj ∈ Ni
otherwise 0. The random walker warps to the restarting node with probability and follows the P
with 1 - . Then, the node visitation probability distribution conditioned on the restarting node vi ,
P P R(i), can be obtained by solving a recursive form:
x J (1 - )xP + 1i,
PPR(i) = 1i(I - (1 - )P)-1,
where xj denotes the probability that the walker resides on vj, and 1j denotes the one-hot encoded
restarting node vi . Then, the proximity between vi and vj , Rip,pjr, can be denoted by the j-th entry
of P P R(i). In contrast to SPD, PPR considers every possible path between two nodes, whereby it
captures the relation among nodes, without the overall graph dispensed with.
5
Published as a conference paper at ICLR 2022
Embedding
Figure 3: The overview of SWAT.
Although the aforementioned graph features all reflect the agent morphology, they are mutually
complementary; for instance, while Graph Laplacian mainly focuses on neighbor information, other
features provide knowledge about distant nodes, generating local and global views respectively. We
thus represent the relational information among nodes in various perspectives by combining those
features. In detail, we aggregate them and learn the graph-based RE R ∈ Rn×n×nhead from them:
Ri,j = gφ([Rlia,jp,Ris,pjd,Rip,pjr]),
(4)
where gφ is a function parameterized by φ that maps the graph features to a RE vector Ri,j and
nhead denotes the number of heads. Note that we represent the relational information between two
nodes as a vector Ri,j ∈ Rnhead instead of a scalar to take full advantage of multi-head attention.
Now we modify the attention score formula with the RE in multi-head setting as follows:
α
i,j
(h)
dk dk/nhead
+ Ri(,hj) ,
(5)
where the subscript (h) denotes the h-th head and Ri(,hj) is a h-th entry of Ri,j .
4.3	Structure-aware Transformer Policy
We now present how SWAT works in inhomogeneous control tasks, illustrated in Figure 3. SWAT is
based on a 3-stage framework similar to the existing methods: 1) Encode local sensory information
vi for each node with the traversal-based PE into the hidden representation by a single MLP layer
shared across all nodes. We use the same notation vi for the encoded node representation for clear
notation. 2) Update each node representation by MP. While SMP uses the multi-hop communica-
tion MP scheme, we use the direct communication through self-attention with graph-based RE. 3)
Finally, decide the action for each non-torso node by pooling the final node representations. To learn
the policy via an RL algorithm, we use TD3 (Fujimoto et al., 2018), a deterministic policy gradient
algorithm in the actor-critic framework, following both SMP and AMORPHEUS.
5	Experiment
5.1	Experiment setting
We run experiments on modular MTRL benchmarks (Huang et al., 2020; Wang et al.,
2018), which are created based on Gym MuJoCo locomotion tasks. 9 environments can
be categorized into 2 settings, in-domain and cross-domain. For in-domain, there are 4 en-
vironments: (1) Hopper++, (2) Walker++, (3) Cheetah++, and (4) Humanoid++;
they all contain both the intact morphology and its variants, e.g., Humanoid only with one
leg or arm, for Humanoid++. The cross-domain environments are combinations of in-
domain environments: (1) Walker-Humanoid++, (2) Walker-Humanoid-Hopper++,
6
Published as a conference paper at ICLR 2022
Cheetah-Walker-Humanoid-Hopper+ +
Figure 4: Training curves on 9 environments. We evaluate on 5 different seeds and plot the mean of
average returns over all morphologies. The shaded area represents the standard error.
(3)	Cheetah-Walker-Humanoid++, (4) Cheetah-Walker-Humanoid-Hopper++
(CWHH++), and (5) Animal-Walkers1. See Appendix A.1 for more details.
We compare our method, SWAT, with 2 baselines, the GNN-based method SMP (Huang et al.,
2020) and the morphology-free self-attention method AMORPHEUS (Kurin et al., 2021). We use
TD3 (Fujimoto et al., 2018) for training the policy over both baselines and ours for fairness. The
policy is trained jointly over all morphologies in various environments, and we run every experiment
for 5 random seeds to report the mean and the standard error.
5.2	MTRL RESULT
Our results are summarized in Figure 4. As Kurin et al. (2021) pointed out, SMP, which exploits
the morphological information but suffers the over-smoothing problem, shows the worst perfor-
mance among all the methods. On the other hand, AMORPHEUS that discards the morphological
information shows better performance than SMP owing to direct communication via self-attention
mechanism.
SWAT clearly outperforms the previous state-of-the-art, AMORPHEUS, especially in the cross-
domain environments, where different types of morphologies are mixed. Furthermore, the perfor-
mance gap between SWAT and AMORPHEUS is notably larger in Animal-Walkers, which
has the greatest morphological diversity. Another noteworthy observation is that SWAT achieves
the best mean performance as well as converges faster than other baselines in both in-domain and
cross-domain environments. These results consistently demonstrate that the effectiveness of our
embedding method increases as more various types of robots are jointly trained. We speculate the
reason for the effectiveness is that the PE and RE encourage our model to transfer commonalities
1Unlike other environments whose variants are in fact subsets of the intact morphology,
Animal-Walkers consists of fundamentally different morphologies, such as a Walker, Horse, and Ostrich.
7
Published as a conference paper at ICLR 2022
5000
4000
3000
2000
1000
0
Figure 5: Performance in transfer learning.
CWHH++ (l⅛St Set)
SWAT
—∙- SMP
—∙- Amorpheus
— SWAT w/o transfer
—l-----1----,-----,---,----r≡
0.0	0.2	0.4	0.6	0.8	1.0
Total environment steps (Ie6)
Ot
Animal-Walkers
6000
^2000
4000
------.----,----Γ≡
4	6	8 10
T
CWHH++
3000
2000
1000
0
0
10
Total environment steps (leβj
Figure 6: Ablation study on PE and RE.
Total environment steps (Ie6)


among resemblant partial morphologies (e.g., the gait patterns of the lower bodies of the Walker and
Humanoid), while discriminating against unrelated ones (the legs of Hopper from those of Walker).
By utilizing these structural embeddings across all environments, our model can learn faster and
achieve higher final returns than those without the structural embeddings.
5.3	Transfer Learning result
In this section, we benchmark SWAT in a transfer learning setting where the policy is trained in var-
ious tasks and then transferred to another downstream task, which is a common learning strategy in
CV and NLP. We compare SWAT with SMP, AMORPHEUS, and SWAT w/o transfer, i.e., SWAT
without pretraining. All models except for SWAT w/o transfer are pretrained in the train environ-
ments and transferred to the test environment with morphology unseen to them. We evaluate them
on Humanoid++, which has the largest number of limbs among the in-domain, and on CWHH++,
which contains all in-domain environments. Note that we only train them for 1 million time steps
during transfer learning.
As shown in Figure 5, with all pretrained models outperforming SWAT w/o transfer, SWAT learns
considerably faster than other baselines, showing much higher sample efficiency, and again outper-
forms them in average returns. Similar to multi-task learning settings, the useful knowledge obtained
from the training tasks can be effectively transferred to the unseen tasks by means of the positional
and relational information, enabling quick adaptation with fewer samples. Although our main focus
is MTRL and transfer learning, we also conduct experiments on zero-shot setting in Appendix A.4.
5.4	Ablation study
In this section, we conduct additional experiments to analyze how the positional and relational in-
formation affects the performance of our model respectively. We ablate the PE and the RE, and
both of them from SWAT, rendering our model identical to AMORPHEUS. We evaluate these on
Animal-Walkers and CWHH++, where we expect for their morphological diversity to incite each
embedding to play a more crucial role than in other environments.
As shown in Figure 6, the performance of SWAT degrades when the embeddings are removed one
by one. This result supports that our proposed embeddings indeed play important roles in different
manners. In other words, we need both positional and relational information to take full advantage
of the given morphology. Meanwhile, as it can be seen from SWAT\{RE}, i.e., SWAT with the
positional embedding only, versus SWAT\{PE}, i.e., SWAT with the relational embedding only, the
performance gains from the positional information are greater than ones from relational information.
We conjecture that the self-attention, originally devised to learn the implicit relations among nodes,
might capture some relational information but cannot the absolute positional information. Further
ablation studies about each component of PE and RE are provided in Appendix A.5.
5.5	Behavior Analysis
In this section, we investigate how the agent that is jointly trained in the various environments acts
in a single environment. To examine this, we visualize trajectories of the agent trained in CWHH++.
Figure 7(a) compares the mean performance of AMORPHEUS and SWAT in the single environment
of intact Humanoid with the largest number of 9 limbs.
8
Published as a conference paper at ICLR 2022
Total environment steps (le6)
(a)
(b)
Figure 7: (a) AMORPHEUS and SWAT performance. (b) We visualize trajectories from AMOR-
PHEUS and SWAT on Humanoid with 9 limbs in CWHH++.
As shown in Figure 7(b), AMORPHEUS does not learn to walk but instead jumps to move forward
like Hopper. This result shows the unexpected effect of multi-task learning. In multi-task learning,
the knowledge from other tasks is not always beneficial to the task of interest; therefore, careless
exploitation of such knowledge can hamper learning rather than help it. That is, the knowledge
obtained in Hopper environments hinders learning to walk in the case of AMORPHEUS. On the
other hand, SWAT successfully learns to run since the structural embeddings allow it to transfer the
appropriate knowledge exclusively where necessary. We speculate this is possible as the embeddings
which have learned similar limbs across different agents are mapped closely to one another. We
visualize the structural embeddings in Appendix A.3.
6	Related Work
There have been lots of studies about MTRL with the homogeneous setting where the state and
action spaces are the same across tasks but each task has a different objective (Rusu et al., 2016;
Parisotto et al., 2016; Teh et al., 2017; Kalashnikov et al., 2021). The inhomogeneous setting has
emerged in recent years and explored by several works. Devin et al. (2017) decompose the policy
model into task-specific modules and morphology-specific ones, then reuse the combination of them
for various settings. D’Eramo et al. (2020) use a shared module over all tasks that is combined with
a task-specific encoder and decoder to learn a policy per task. Those approaches based on the
separated module have a limitation in that they require a larger number of model parameters as the
number of tasks increases. Chen et al. (2018) encodes agent morphology in a feature vector and
learns the policy for different morphologies conditioned on the state and feature vector.
Another line of work that is closely related to ours is the modular approach. Wang et al. (2018)
and Pathak et al. (2019) represent the agent morphology as a graph and utilize GNNs as the policy
network in order to tackle the inhomogeneous setting. Both of their studies show that the GNN-
based policy has large benefits over a monolithic policy in inhomogeneous MTRL. Recently, Huang
et al. (2020) present SMP based on GNNs for the inhomogeneous setting. They assume each agent
as a tree and propose a both-way MP scheme where the messages are propagated from leaves to
root (bottom-up) and from root to leaves (top-down). Kurin et al. (2021) pose a concern about
the MP in GNNs due to the over-smoothing problem, and propose AMORPHEUS, which adopts
self-attention mechanism rather than GNNs for direct communication in exchange for leveraging
the morphological information. In contrast, we utilize the morphological information through the
structural embeddings, allowing direct communication while taking advantage of the structural bias.
7	Conclusion
In this paper, we argue that the knowledge of agent morphology plays an important role in modular
MTRL and present SWAT that leverages structural biases arising from the morphology. Instead of
the MP scheme, we encode the structural information through the medium of the traversal-based PE
and the graph-based RE, which can be easily incorporated into the transformer-based policy. We em-
pirically demonstrate that our method achieves the state-of-the-art performance in inhomogeneous
robot locomotion MTRL benchmarks as well as in transfer learning settings.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors thank Hyolim Kang for contributing to the improvement of SWAT. This work was
supported by the National Research Foundation (NRF) of Korea (NRF-2019R1A2C1087634, NRF-
2021M3I1A1097938) and Institute of Information & communications Technology Planning & Eval-
uation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, No.2020-0-00940,
No.2021-0-02068)
10
Published as a conference paper at ICLR 2022
References
Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek
ROzemberczki, Michal Lukasik, and StePhan Gunnemann. Scaling graph neural networks with
approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 2464-2473, 2020.
HA Burgdorff, Sushil Jajodia, Frederick N. Springsteel, and Yechezkel Zalcstein. Alternative meth-
ods for the reconstruction of trees from their traversals. BIT Numerical Mathematics, 27(2):
133-140, 1987.
Tao Chen, Adithyavairavan Murali, and Abhinav Gupta. Hardware conditioned policies for multi-
robot transfer learning. In Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, pp. 9355-9366, 2018.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowl-
edge in multi-task deep reinforcement learning. In 8th International Conference on Learning
Representations, ICLR 2020. OpenReview.net, 2020.
Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning mod-
ular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE Interna-
tional Conference on Robotics and Automation, ICRA 2017, pp. 2169-2176. IEEE, 2017. doi:
10.1109/ICRA.2017.7989250.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, pp. 4171-4186, 2019.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. In 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net,
2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 1025-1035, 2017.
Wenlong Huang, Igor Mordatch, and Deepak Pathak. One policy to control them all: Shared modular
policies for agent-agnostic control. In International Conference on Machine Learning, pp. 4455-
4464. PMLR, 2020.
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,
Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic re-
inforcement learning at scale. CoRR, abs/2104.08212, 2021.
Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In 9th
International Conference on Learning Representations, ICLR 2021, 2021.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Combining neural networks
with personalized pagerank for classification on graphs. In 7th International Conference on
Learning Representations, ICLR 2019, 2019.
Vitaly Kurin, Maximilian Igl, Tim Rocktaschel, Wendelin Boehmer, and Shimon Whiteson. My
body is a cage: the role of morphology in graph-based incompatible control. In 9th International
Conference on Learning Representations, ICLR 2021, 2021.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.
11
Published as a conference paper at ICLR 2022
Russell Merris. Laplacian matrices of graphs: a survey. Linear Algebra and its Applications, 197-
198:143-176,1994. ISSN0024-3795. doi: https:〃doi.org/10.1016/0024-3795(94)90486-3.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford InfoLab, 1999.
Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. In 4th International Conference on Learning Representations,
ICLR 2016,, 2016.
Sungchan Park, Wonseok Lee, Byeongseo Choe, and Sang-Goo Lee. A survey on personal-
ized pagerank computation algorithms. IEEE Access, 7:163049-163062, 2019. doi: 10.1109/
ACCESS.2019.2952653.
Deepak Pathak, Christopher Lu, Trevor Darrell, Phillip Isola, and Alexei A. Efros. Learning to
control self-assembling morphologies: A study of generalization via modularity. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, pp. 2292-2302, 2019.
Lerrel Pinto and Abhinav Gupta. Learning to push by grasping: Using multiple tasks for effective
learning. In 2017 IEEE International Conference on Robotics and Automation, ICRA 2017, pp.
2161-2168. IEEE, 2017. doi: 10.1109/ICRA.2017.7989249.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 21:1-67, 2020.
Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, GUillaUme Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation. In 4th International Conference on Learning Representations, ICLR 2016, 2016.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-
tions. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.
464-468, 2018.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game ofGo with
deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Ad-
vances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, pp. 4496-4506, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, pp. 5998-6008, 2017.
Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy with
graph neural networks. In 6th International Conference on Learning Representations, ICLR 2018,
2018.
12
Published as a conference paper at ICLR 2022
Yu-An Wang and Yun-Nung Chen. What do position embeddings learn? an empirical study of pre-
trained language model positional encoding. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 6840-6849, 2020.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 32(1):4-24, 2020.
Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft
modularization. In Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.
13
Published as a conference paper at ICLR 2022
A Appendix
A.1 Environment description
Following Kurin et al. (2021), we conduct experiments on benchmarks as in Table 2. Huang et al.
(2020) proposed Hopper++, Walker++, Cheetah++, and Humanoid++, removing some parts
from the intact morphology, while every morphology is able to hop, walk, or run, i.e., they do not
contain the morphology that is unable to go forward such as the robot with only arms or the one
lacking its torso.
To explain the name of environment in more detail, the first part means its intact morphology and
the middle part means the number of limbs belongs to its morphology, and the last part means
the missing limbs. For example, humanoid_2d_7_right_leg originates from HUmanoid and
consists of 7 limbs, lacking right legs, both right thigh and right shin. Note that we dub Walker
in Wang et al. (2018) as Animal-Walkers, becaUse it is qUite confUsing in that Walker++
also contains the variants of Walker. Since Animal-Walkers consists of varioUs animal-shaped
morphology, we distingUish it from Walker.
All the environments provides the morphology of the robot as a graph-strUctUred data that is ex-
tracted from the corresponding MUJoCo XML file. The reward is given by the distance the agent
move forward, penalized by the norm of action, i.e., the sUm of torqUe valUes in all actUators. And
it terminates when the agent slips and cannot proceed anymore or it sUcceeds to sUrvive 1,000 time
steps.
A.2 Implementation detail
We implement SWAT based on AMORPHEUS which is bUilt on TransformerEncoder from Py-
Torch, sharing the codebase of SMP. Additionally, we simply modify TransformerEncoder to incor-
porate with PE and RE, enabling relational embedding to be added per head. We Use 3 independent
embedding layers for each traversal to learn PE, where each embedding size has the eqUal size, i.e.,
sUmmation of all PEs per traversal is eqUal to the embedding size of final PE. And we Use a simple
fUlly-connected layer for gφ in EqUation 4 to learn the graph-based RE. SWAT shares hyperparam-
eter setting Used in KUrin et al. (2021) as shown in Table 1, except the replay bUffer size dUe to the
memory efficiency. Also, we Use the residUal connection as reported in KUrin et al. (2021).
Hyperparemeter	Value
Learning rate	0.0001
Gradient clipping	0.1
Normalization	LayerNorm
Attention layers	3
Attention heads	2
Attention hidden size	256
Encoder oUtpUt size	128
Mini-batch size	100
Replay bUffer size	500K
Embedding size	128
Table 1: Hyperparameter setting in SWAT
14
Published as a conference paper at ICLR 2022
Environment	Train set	Test set
Hopper++		
hopper_3 hopper_4 hopper_5		
Walker++		
	walker _2_main walker _4_main walker _5_main walker _7_main	walker _3_main walker _6_main
Cheetah++		
	Cheetah_2_back cheetah_2 .front Cheetah_3_back Cheetah_3_front Cheetah_4_allback Cheetah_4_allfront Cheetah_4_back cheetah_4 .front Cheetah_5_balanced Cheetah_5_front Cheetah_6_back cheetah_7 _full	cheetah-3-balanced cheetah_5 _back Cheetah_6_front
Humanoid++		
	humanoid_2d_7_left_arm humanoid_2d_7_lower _arms humanoid_2d_7 .right -arm humanoid_2d_7_right_leg humanoid_2d_8_left_knee humanoid_2d_9_full	humanoid_2d-7-left-leg humanoid_2d_8_right-knee
Walker-Humanoid++ (WH++)
Union of Walker++ and Humanoid++.
Walker-Humanoid-Hopper++ (WHH++)
Union of Walker++, Humanoid++, and Hopper+ + .
Cheetah-Walker-Humanoid++ (CWH++)
Union of Cheetah++, Walker++, and Humanoid+ + .
Cheetah-Walker-Humanoid-Hopper++ (CWHH++)
Union of Cheetah++, Walker++, Humanoid++, and Hopper+ + .
Animal-Walkers
FullCheetah (Wolf)
Hopper
HalfCheetah (Horse)
HalfHumanoid
Ostrich
Table 2: Full list of environments.
15
Published as a conference paper at ICLR 2022
A.3 Structural Embedding
We visualize both PE and RE in SWAT learned in cross-domain environments, CWHH++. Figure 8
shows that the PEs of limbs placed at similar position in morphology are mapped closely. Figure 9,
10, and 11 shows SWAT learns relational information among nodes through Rlap , Rspd and Rppr ,
which considers not only local but global information of graph.
Figure 8: t-SNE visualization of positional embeddings learned from multiple robots with various
morphology. The points are corresponded to the reduced positional embedding of each node in
different morphologies. The shape denotes the morphology it belongs to and the color denotes its
limb type. Walker(Cheetah)\{foot} are the variants lacking of feet.
16
Published as a conference paper at ICLR 2022
Figure 9: Illustration of the relational embeddings from two heads on Hopper (5 limbs)
Figure 10: Illustration of the relational embeddings from two heads on Walker (7 limbs)
lower arm2
upper arm2
lower arml
upper arml
shin2
thigh2
shinl
thighl
torso
Figure 11: Illustration of the relational embeddings from two heads on Humanoid (9 limbs)
A.4 Zero-shot Evaluation
Table 3 and 4 show zero-shot evaluation results on in-domain setting and cross-domain set-
ting respectively. For in-domain setting, each policy is trained on Walker++, Humanoid++,
Cheetah++ and evaluated on their test set respectively. For example, SWAT trained on the train
set of Walker++ is tested on Walker_3加ain and Walker_64ain. Note that We omit the
lacking part in the name of morphologies in the Table 3 and 4. For cross-domain setting, each pol-
icy is trained on CWHH++ and evaluated on its test set, i.e., the union of test sets of Walker++,
Humanoid++, Cheetah++. We evaluate by the average performance and the standard error over
3 seeds, Where each seed is evaluated on 100 rollouts. As in Kurin et al. (2021), the high variance
due to instability in generalization can be observed in SWAT as Well as in SMP and AMORPHEUS.
Therefore, it Would be hard to say Which one is statistically advantageous to the others in zero-shot
generalization task.
17
Published as a conference paper at ICLR 2022
	SWAT	AMORPHEUS	SMP
Walker_3 Walker_6	220.49 ± 66.62 396.10 ± 136.98	264.04 ± 37.39 745.28 ± 635.61	218.07 ± 49.27 900.64 ± 172.95
humanoid_2d_7 humanoid_2d_8	2428.38 ± 1252.61 4117.4 ± 171.54	659.54 ± 519.64~~ 3624.95 ± 1035.22	3042.23 ± 1154.78 3798.49 ± 137.23
Cheetah_3 cheetah_5 cheetah_6	10.94 ± 224.54 1011.95 ± 945.82 2610.97 ± 747.48	171.07 ± 230.04~~ 3027.78 ± 735.40 4989.3 ± 257.68	139.09 ± 268.49 2768.24 ± 229.80 5861.86 ± 307.39
Table 3: Comparison in zero-shot evaluation on in-domain test set.
	SWAT	AMORPHEUS	SMP
Walker_3	256.62 ± 133.57	216.73 ± 40.24	52.30 ± 24.39
Walker_6	1564.45 ± 1229.78	3209.67 ± 1425.86	852.41 ± 363.50
humanoid_2d_7	1916.45 ± 1358.3	2590.07 ± 905.40	532.02 ± 314.94
humanoid_2d_8	4040.86 ± 296.85	1201.67 ± 565.64	1205.34 ± 683.62
Cheetah_3	-89.63 ± 41.44	50.13 ± 199.91	133.56 ± 68.88
cheetah_5	855.30 ± 243.15	1842.01 ± 40.70	290.19 ± 52.70
cheetah_6	2625.68 ± 291.62	3113.09 ± 262.48	2778.24 ± 391.44
Table 4: Comparison in zero-shot evaluation on cross-domain test set.
A.5 Further Ablation Study on Embedding Components
In this section, we further conduct experiments to analyze how each component in the positional
and relational embedding affects the performance of our model. We firstly verify the usefulness of
components in the PE, which are pre-order (PRE), in-order (IN) and post-order (POST) traversal.
As shown in Figure 12, SWAT\{POST,IN} shows the worst performance due to ambiguity, i.e., a
binary tree cannot be reconstructed only with pre-order traversal. Using in-order traversal along
with pre-order traversal is theoretically sufficient to recover a binary tree, so giving an additional
post-order seems to be redundant. However, as it can be seen from SWAT versus SWAT\{POST},
the empirical result shows that the additional information actually helps learning more efficiently in
some cases. This is because although redundant, they can provide positional information in different
perspectives, making the model easier to figure out node positions. We thus decided to use all three
traversals in the main experiments. Similar to the PE, we also check the effect of each component
in the RE, which are PPR, SPD and LAP, shown in Figure 13. While the performance gaps among
3 variants are not large, SWAT consistently outperformed others. We thus used all three features for
RE in the main experiments although they provide overlapping information.
18
Published as a conference paper at ICLR 2022
Total environment steps (Ie6)
Total environment steps (Ie6)
Total environment steps (Ie6)
Total environment steps (Ie6)
Figure 12: Ablation study on traversal-based positional embedding.
Total environment steps (Ie6)
Total environment steps (Ie6)
WHH++	CWH++
Total environment steps (Ie6)
Total environment steps (Ie6)
Total environment steps (Ie6)
Figure 13: Ablation study on graph-based relational embedding.
19