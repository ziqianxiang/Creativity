Published as a conference paper at ICLR 2022
Proving the Strong Lottery Ticket Hypothesis
for Convolutional Neural Networks
Arthur C. W. da Cunha & Emanuele Natale
Inria Sophia Antipolis
Sophia Antipolis, France
{arthur.carvalho-walraven-da-cunha,emanuele.natale}@inria.fr
Laurent Viennot
Inria Paris, IRIF
Paris, France
laurent.viennot@inria.fr
Ab stract
The lottery ticket hypothesis states that a randomly-initialized neural network con-
tains a small subnetwork which, when trained in isolation, can compete with the
performance of the original network. Recent theoretical works proved an even
stronger version: every sufficiently overparameterized (dense) neural network
contains a subnetwork that, even without training, achieves accuracy comparable
to that of the trained large network. These works left as an open problem to extend
the result to convolutional neural networks (CNNs). In this work we provide such
generalization by showing that, with high probability, it is possible to approximate
any CNN by pruning a random CNN whose size is larger by a logarithmic factor.
1	Introduction
Many impressive successes in machine learning are reached through neural network architectures
with a huge number of trainable parameters. Consequently, substantial research in the field aims
at reducing the size of such networks while maintaining good accuracy; e.g., for deployment in
resource constrained devices (Yang et al., 2017).
A major empirical fact of such endeavour is the contrast between the initial model overparametriza-
tion, which appears necessary for effective training, and the extent to which the size of the resulting
model can be reduced through compression techniques. Among the latter, pruning methods appear
as a mature and efficient way of achieving significant compression, often without incurring any ac-
curacy loss (Blalock et al., 2020). Recently, the aforementioned contrast between the initial and final
number of parameters has been addressed by the lottery ticket hypothesis (Frankle & Carbin, 2019),
or LTH for short, which states that any randomly initialized network contains lottery tickets; that
is, sparse subnetworks that can be trained just once and reach the performance of the fully-trained
original network. This hypothesis was first verified experimentally, leveraging pruning methods to
identify the lottery tickets (Frankle & Carbin, 2019; Lee et al., 2019).
Ramanujan et al. (2020) then proposed a stronger version of the hypothesis, named strong lottery
ticket hypothesis (SLTH) by Pensia et al. (2020): it stipulates that a network with random weights
contains, with high probability, sub-networks that can approximate any given sufficiently-smaller
neural network. In other words, a sufficiently large and randomly initialized network that can be
successfully trained for a task, could instead be suitably pruned to obtain a network that, even with-
out training, achieves good accuracy. Experimental support for this stronger version were reported
by Ramanujan et al. (2020); Zhou et al. (2019); Wang et al. (2020), which find lottery tickets in a
range of architectures, including convolutional neural networks (CNNs). A first rigorous proof of
the SLTH was given by Malach et al. (2020) for the case of dense networks (i.e., consisting of fully
connected layers). Pensia et al. (2020) and Orseau et al. (2020) successively improved this result by
showing that logarithmic over-parametrization is sufficient. Their results are also restricted to dense
networks and they leave as an open problem to extend it to CNNs.
1
Published as a conference paper at ICLR 2022
Our contributions. We extend and complete the proof of the SLTH (and thus, also, of the LTH),
for classical network architectures which can combine convolutional and fully connected layers.
More precisely, we prove that any CNN with given weights can be approximated by pruning a CNN
with random weights (random CNN for short), with the latter being larger than the former by a
logarithmic factor. We also provide basic experiments showing that starting from a random CNN
which is roughly 30 times larger than LeNet5, it is possible to compute in few hours a pruning
mask that allows to approximate the trained convolutional layers of LeNet5 with relative error 10-3,
even when ignoring some hypothesis of our theoretical result. Our theoretical analysis follows the
approach of Malach et al. (2020) and make use of two layers to approximate one. We borrow from
Pensia et al. (2020) the use of random subset sum (RSS) (Lueker, 1998) to approximate a given
weight via the sum of a subset of a sample of random weights, and carefully design instances of
RSS via a combination of two convolutional layers. By controlling the error accumulated by each
layer with Young’s convolution inequality, we establish the following result.
Informal version of Theorem 1. Given ε, δ > 0, any CNN with k parameters and
0 layers, and kernels with 01 norm at most 1, can be approximated within error ε
by pruning a random CNN with O (k log m®% 方}) parameters and 20 layers with
probability at least 1 - δ.
This result generalizes those by Pensia et al. (2020), Orseau et al. (2020), and Malach et al. (2020)
as dense layers can be regarded as convolutional layers where kernel and input sizes match.
Roadmap. After discussing related work in the next section, we state our theoretical results along-
side a high-level idea of the proofs. Successively, we report our experimental results. Finally, in
Section 4, we provide detailed proofs of our statements.
1.1	Related Work
Pruning methods are classical neural network compression strategies that date back to the 80s (Le-
Cun et al., 1989b; Mozer & Smolensky, 1988). We recommend the recent survey Blalock et al.
(2020) for an overview of the current state of research on these techniques.
As for the lottery ticket hypothesis, Lange (2020) summarizes the progress on the topic until the
results by Malach et al. (2020). In the following we briefly mention works which are not discussed
in Lange (2020). Cosentino et al. (2019) shows that lottery tickets can be adversarially trained,
yielding sparse and robust neural networks. Soelen & Sheppard (2019) shows that lottery tickets are
transferable, in the sense of showing remarkable accuracy for tasks other than the original one for
which they have been found. Sabatelli et al. (2021) further shows that minimal retraining on a new
task allows lottery tickets to often achieve better generalization than models trained ad-hoc for the
task. Yu et al. (2020) empirically supports that the LTH holds also in the context of reinforcement
learning and natural language processing. Fischer & Burkholz (2021) extends works on the SLTH
to accommodate biases in practical settings. Diffenderfer & Kailkhura (2021) shows that lottery
tickets are robust to extreme quantization of the weights. Aladago & Torresani (2021) provides a
method to train networks where each initial weight is restricted to few possible random values. An
extreme case of the latter is to share only a single (random) value among all weights, and focus the
training solely on finding the best architecture (Gaier & Ha, 2019).
Our work also relates to recent papers investigating properties of random CNNs, such as Ulyanov
et al. (2020) which observes that random CNNs already seem to capture some natural image statistics
required for tasks such as de-noising and inpainting.
2	Theoretical Results
We start by introducing some of our notation. The rest of it follows Goodfellow et al. (2016) with
minor modifications, so we defer a full description to Section 4.1.
Given n ∈ N, we denote the set {1 ,...,n} by [n]. The symbol * represents the convolution oper-
ation, Θ represents the element-wise (Hadamard) product, and σ represents ReLU activation func-
tion. Finally, the notation ∣∣∙∣∣ ι refers to the sum of the absolute values of each entry in a tensor while
2
Published as a conference paper at ICLR 2022
IMlmaX denotes the maximum norm: the maximum among the absolute value of each entry. Those
are akin to vector norms and should not be confused with operator norms.
We restrict our setting to convolutional neural networks f: [0, 1] D × D ×c0 → RD × D ×(C of the form
f (X) = K K * 仪 K ”1 *∙∙∙仪 K1 * X)),
where Ki ∈ Rdi×di×(i-1 ×(i, and the convolutions have no bias and are suitably padded with zeros.
The restrictions on tensor sizes and the exclusion of bias terms1 aim only to ease presentation.
Our initial goal is to approximate a convolution with a single kernel, as depicted in Figure 1, using
convolutions with (pruned) random kernels. We achieve this by the means of the structure presented
in Figure 2, using two convolutions with random tensors.
X
K
D
K * X
Figure 1: Schematics of the convolution between an input X ∈ RD×D×( and a kernel K ∈ Rd×d×(
resulting in a D × D matrix.
Lemma 1 asserts that, with high probability, we can prune this structure to approximate the output
of a convolution with any given kernel as long as the amount of random kernels is large enough.
X
d
d
U(1),...,U(n)
U * X
V
V* (U*X)
Figure 2: Schematics of the use of two convolutions to approximate the operation depicted in Fig-
ure 1. The elements of the set U = {U(1),..., U(n)} and V are random tensors. Notice that the
intermediate tensor U * X has size D × D × n and yet the final output is a D × D matrix.
Lemma 1 (Single kernel). Let D,d,c,n ∈ N, and ε,C ∈ R>o, where n ≥ C log d2c,
U ∈ Rd×d×(×n, V ∈ R1×1×n×1, and S ∈ {0, 1}size(U), where the entries of U and V are i.i.d.
Unif ([-1, 1]) random variables. Moreover, define the random CNN g : [0, 1]D×D×( → RD×D×1
and its pruned version gS by
g(X) = V * 仪U * X) and	gS(X) = V * 仪(U Θ S) * X).
Then, we can choose constant C independently from other parameters so that, with probability at
least 1 一 ε,forall K ∈ [—1, 1]d×d×C×1 with IlKIII ≤ 1, there exists a pruning mask S such that
SUp	IlK * X-g S(X)IlmaX <ε.
X∈[0,1]D×D×c
Proof idea. We leverage the absence of negative entries in the input and an initial pruning of U to
bypass the ReLU non-linearity. This allows us to virtually replace the operations in g by a single
1If biases are present, the structures used in the proofs also puts them in a RSS configuration. Thus the
results can be readily adapted by replacing the di2 terms by di2 + 1.
3
Published as a conference paper at ICLR 2022
convolution with a random kernel obtained by combining U and V. Each entry of this resulting
kernel is the sum of n random variables, where we can choose to include/exclude each term in the
sum by choosing to keep/prune the relevant weights. We finish the proof by applying Theorem 2 to
conclude that n variables suffice to approximate all entries, simultaneously, with enough precision
to ensure the thesis.	口
We now extend Lemma 1 to an entire layer. As before, a detailed proof is provided in Section 4.3.
Lemma 2 (Convolutional Layer). Let D, d, c0, c1, n ∈ N, and ε, C ∈ R>0, where n ≥
Cc 1 log d2c0cι, U ∈ Rd×d×C0×n, V ∈ R1×1×n×Cι, S ∈ {0,1}Size(U) and T ∈ {0,1}Size(V), where
the entries of U and V are i.i.d. Unif ([-1, 1]) random variables. Finally, define the random CNN
g: [0, 1]D×D×C0 → RD×D×C1 and its pruned version gT,S(X) by
g(X) = V * 仪U * X) and	gt,s(X) = (V Θ T) * 仪(U Θ S) * X).
Then, we can choose constant C independently from other parameters so that, with probability at
least 1 一 ε,forall K ∈ [—1, 1]d×d×C0×c 1 with IIKIII ≤ 1, there exist masks S and T such that
sup IlK * X 一 gT,S(X)Imax <ε.
X∈[0,1]D×D×c0
Proof Idea. The lemma follows by applying Lemma 1 to each kernel independently, so that all
of them are approximated by a factor of at most ε∕c 1. Such approximation allows to apply the
union bound so that the desired approximation holds simultaneously for all c1 output kernels with
probability at least 1 一 ε.	口
Next, we extend Lemma 2 from a single layer to the entire network, thus proving our main result. A
detailed proof is given in Appendix B.
Theorem 1 (Convolutional Network). Let D, d, co,£ ∈ N, and ε,C ∈ R>o. For each i ∈
[4, let Ci,n ∈ N, where n ≥ Cci log 靠浮：,and L2i-1 ∈ Rd×d×"-1××n, L2i ∈
R1×1×ni ×C, S2i-1 ∈ {0,1}Size(L2i 1), S22 ∈ {0,1}Size(L2i), where the entries of L1,..., L2? are
i.i.d. Unif([-1,1]) random variables and define the random 20-layer CNN g: [0, 1]D×D×C0 →
RD × D × C and its pruned version gsi s?2 (X) by
g(X) =	L22 * σ(∙∙∙	σ(L1	* X))	and	gsi,…？g(X)	=	(L2£	Θ S22) * σ[∙∙∙ σ[(L1	Θ S1)	* X]].
Finally, let F be the class OffunctionSfrom [0, 1] D × D ×C0 to RD × D × Cg such that, for each f ∈ F
f (X) = K 2 * σ (K 2-1 *∙∙∙ σ (K1 * X)),
where,foreach i ∈ [£], Ki ∈ [—1, 1]di×di×C-1 ×C and ∣∣Ki∣∣ι ≤ 1.
Then, we can choose constant C independently from other parameters so that, with probability at
least 1 一 δ, the following holds for every f ∈ F:
inf	SUp	Ilf(X) -gSi,…,S2g (X)Lax <ε
∀ i ∈[2 2 ] , Si ∈{0,1}siζe(L i ) X∈[0,1]D × D × c 0
Proof Idea. The proof leverages Lemma 2 in an analogous way to how the latter relied on Lemma
1; namely, we apply Lemma 2 by requiring an approximation factor that guarantees, with sufficient
probability, that a suitable approximation is reached across all layers simultaneously. The latter
requirement is responsible for the £ factor which appears in the logarithms of the dimensions of each
random tensor L2.	口
2.1 Discussion on Theorem 1
Size analysis. For each layer, we emulate a4-D kernel K with size di × di × ci-1 × ci with two 4-D
kernels U and V with size d2 × d2 × Ci -1 × n and 1 × 1 × n × Ci respectively with n ≥ Cci log m-n^d； ∙
Under the technical assumption Ci = O (d2 Ci-1) for i ∈ [£], the size of V is within a constant factor
4
Published as a conference paper at ICLR 2022
of that of U, and the whole random network We prune has size O(k log m®：； δ} )，where k is the
size of the network we want to approximate. This technical assumption is met for all classical
convolutional networks used for image processing with a reasonably small constant in the big O
notation. We come back on this assumption below.
Limitations. The properties of convolutional layers require stronger hypotheses in Theorem 1
when compared with the results for dense layers Malach et al. (2020) or Pensia et al. (2020). First,
we require non-negative inputs for all layers, however, since the output of the ReLU function is
never negative, this restriction is only relevant for the input of the first layer. The mentioned works
avoid this restriction by exploiting the identity a = σ(a) - σ(-a) to deal separately with the
positive and negative entries. The fact that each entry of the output of a convolution is affected by
potentially multiple input entries prevents us from employing a similar strategy. Nonetheless, we
remark that, while this is a relevant theoretical indication of the challenges imposed by the operation
of convolution, in practice the inclusion of biases suffices to easily convert any CNN with a domain
including negative values into an equivalent CNN that takes only non-negative inputs. Finally, the
possibly multidimensional entries of convolutions also motivates the restriction on the norm of the
target weight tensors in terms of the 1-norm.
Generalizations. For the sake of simplicity, we state and prove Theorem 1 in a restricted setting.
It is worth remarking a series of generalizations that can be obtained at the mere cost of making
the proofs more technically involved. We defer a more complete treatment of such extensions to a
journal version of this work. First, the proof could consider also other parameters, such as stride,
padding, average pooling and other operations that can be seen as convolutions. Moreover, we could
consider more general convolutions, not necessarily 2-D, operating on tensors of any sufficiently
large dimension. In particular, itis not necessary to assume that V has size 1 × 1 × n × ci in the above
analysis. Using a 5-D tensor with size d × d × ci-1 × (n/ci) × c1 for U, an appropriate convolution
U * X would result in a D × D × (n/c) × Ci tensor, and we could usea 1 × 1 × (n/c) × Ci tensor for V
without the need for the mask T by performing in parallel ci appropriate convolutions. Note that the
size ofV is then smaller than the size of U. The technical assumption used in the above size analysis
is thus not necessary to guarantee that logarithmic over-parametrization is sufficient. Finally, observe
that that our results generalize to any probability distribution for the weights that contains a b-scaled
Unif ([-a, a]) for some constant a > 0 (in the sense of Definition 2 in Appendix C), where the
parameters a and b only impact the constants in the theorem.
3	Experiments
As networks with higher parameter count tend to be more robust to noise, we stick to the small
CNN architecture used by Pensia et al. (2020), namely, LeNet5 (LeCun et al., 1989a) with ReLU
activations. We conduct our experiments by first training a the network to 98.99% test accuracy
on MNIST dataset (Lecun et al., 1998). To avoid well-known limitations of the MNIST dataset (in
particular its large number of zero entries), we also trained it on the Fashion-MNIST dataset (Xiao
et al., 2017) to 89.12% test accuracy. We adopted Kaiming Uniform (He et al., 2015) for weight
initialization, a batch size of 64 and trained for 50 epochs using ADAM optimizer (Kingma & Ba,
2015) with learning rate of 0.001, exponential decay of 0.9 and momentum estimate of 0.999, the
default values in Flux.jl (Innes et al., 2018) machine learning library.
Once the network is trained we change its weights for a random subset sum approximation of them.
More precisely, for each weight w we sample x from Unif([-1, 1]n) and use Gurobi optimization
software (Gurobi Optimization, LLC, 2021) to solve the mixed-integer program
n
min IW 一	ai ∙ Xi I s.t. ai ∈ {0, 1} ∀i ∈ [n],
a1 ,...,an
i=1
where n is the sample size. Solving this subset sum problem with n = 30 for the 2572 parameters
in the convolutional layers of LetNet takes around 1 hour on 32 cores ofa Intel® Xeon® Gold 6240
CPU @ 2.60GHz.
Figure 3 shows the accuracy of the approximation for different sample sizes. We start to obtain good
approximations (error smaller than 10-2) from sample sizes around 15-20. Also, when comparing
5
Published as a conference paper at ICLR 2022
5	10	15	20	25	30
sample size
Figure 3: Relative error of random subset sum approximation of the convolution weights of a LeNet5
trained on MNIST (left) and on Fashion-MNIST (right). The error is given in logarithmic scale as
the maximum distance between a weight and its approximation for different sample sizes.
to the weights obtained for MNIST and for Fashion-MNIST, we have better approximations for the
smaller sample sizes for MNIST. We believe this is due to the fact that the training on Fashion-
MNIST resulted in filters with larger weights (up to a factor 2, roughly), since a larger sample size
is necessary to approximate a larger interval of values (see Theorem 2).
The high precision in the approximation of most weights leads to negligible change in the accuracy
of the network. For this reason, we focus on studying the error at the output of the convolutional
section of LeNet5, right before the flattening. Also, at this point the activation tensor has dimension
7 × 7 × 16 as opposed to the vector of size 10 at the end of LeNet5.
」aj」"Indan。">Q-9J UJnUJ-XeUJ
」aj」"Indan。">Q-9J UJnUJ-XeUJ
Figure 4: Maximum relative output error for the convolutional portion of LeNet5 trained on MNIST
(left) and Fashion-MNIST (right) for different sample sizes. The maximum is computed over all
images in the dataset.
Figure 4 shows the maximum relative error for all approximated outputs compared to original ones.
The relative error of the output for an input image is computed as the maximum activation error
divided by the maximum original activation (both maxima are taken over all 7 × 7 × 16 activations).
Once again, MNIST leads to better precision for the smaller sample sizes. This can be explained
by the fact that weights are better approximated in that range with MNIST as seen in Figure 3. In
both cases, we get a relative error close to 10-3 with sample size 20, and even better with larger
sample sizes. Within the settings of Theorem 1, this corresponds to expanding the convolutional
portion of the network by a factor of, roughly, 30 if we take into account kernel sizes and number of
channels. This high precision is achieved even though the trained weights do not satisfy the norm
restrictions of Theorem 1. Indeed, as we do not use any explicit regularization, the 1-norms of the
kernels obtained are quite high (from 50 to 15000 roughly for both datasets).
4	Technical Analysis
4.1	Notation and Conventions
We employ the notation from Goodfellow et al. (2016) with minor adjustments. For self-
containment, we detail below not only our adaptations but all the relevant notation inherited from
the textbook.
6
Published as a conference paper at ICLR 2022
We identify the type of the objects with the font used for their symbols. This applies to scalars
(e.g. x), to vectors (e.g. x) and its entries (e.g. xi), to tensors (e.g. X) and its entries (e.g. Xi,j,k),
and to sets (e.g. X). We denote slices of tensors by indexing it with colons. For example, the
expression X:,:,i represents a 2-D slice of a 3-D tensor. Finally, we refer to the axis of a 4-D tensors
as rows, columns, channels, and kernels (a.k.a. filters), in this order.2
This work considers explicitly only 2-dimensional convolutions with multiple channels, multiple
kernels and enough zero-padding to preserve the output size. However, as we discuss in Section 2.1,
our results can be generalized in many ways.
Definition 1 (Convolution). Given tensors K ∈ Rd×d×c and X ∈ RD×D×c, the 2-dimensional
discrete convolution between K and X is the D × D matrix with entries given by
(K * X)i,j =	):	Ki'j,k ∙ Xi—尸+1 ,j - jf+1 ,k for i,j ∈ [D].
i ∖j'∈[ d ] ,k ∈[ c ]
where X is suitably zero-padded. Similarly, if K ∈ Rd×d×c0 ×c1 is a 4-dimensional tensor, we define
K*Xas the D × D × c1 tensor with entries
(K * X) j =	∑	Kii j ,k,2 ∙ Xi-iz + 1 ,j - jz + 1 ,k for i,j ∈ [D],g ∈ [c 1].
i i ,j i∈[ d ] ,k ∈[ c ]
The output of a convolution with a 4-D kernel can also be can be expressed in terms of the 3-D case
using tensor slices. For instance, the equality above can be rephrased as
(K * X):,:£ = K:,:,:£ * X	for £ ∈ [C1].
4.2	Single Kernel Approximation (Proof of Lemma 1)
Our first goal is to bypass the non-linearity so we can combine the two convolutions in g(X) =
V * σ(U * X) into a single one. Given that the activation function under consideration is the ReLU,
it suffices to ensure that its input has no negative entry. Hence, we prune all negative entries of U,
obtaining the tensor U+ = max{0, U}, where the maximum is applied entry-wise. Since, by hy-
pothesis, the entries of the input X are non-negative, it follows that the entries of the tensor U+ * X
are also non-negative. Therefore,
V*σ(U+*X) =V*(U+*X).
(1)
We now look at the first convolution on the right side of Equation 1. By Definition (1), we have
n
[V * (U+ * X)]r,s,1 =工 V1,1 ,t, 1 ∙ (U+ * X)r,s,t
t=1
n
工 Y1,1,t,1
t=1
(工
i,j∈[d],k∈[c]
• Xr—i +1 ,s — j + 1 ,k
)
n
∑	∑	(V 1,1 ,t,1∙
t=1 i,j∈[d],k∈[c]
∑ (it V1,1 ,t, 1
i,j∈[d],k∈[c]	t=1
Ui+,j,k,t • Xr-i+1,s-j+1,k
• Xr-i+1,s-j+1,k .
The equation above shows that performing V * (U+ * X) is equivalent to performing a single convo-
lution between X and a tensor L ∈ Rd×d×c×1 whose coordinates are given by
n
Li,j,k,1 =	V1,1,t,1 • Ui+,j,k,t.	(2)
t=1
2Goodfellow et al. (2016) uses a different ordering.
7
Published as a conference paper at ICLR 2022
This reveals aRSS configuration where we can choose to include/exclude each value V1,1 .t, 1 ∙ Ui+j,k,t
in the sum by choosing to keep/prune Ui+,j,k,t . Since Equation 1 continues to hold after further
pruning U+ , we finish our proof by doing exactly that: we leverage Theorem 2 to ensure that, with
high probability, we can solve this RSS problem for each entry of L to approximate the respective
entry of K.
To see that We can apply Theorem 2 in this setting, for ε/ > 0, i,j ∈ [d], and k ∈ [C], denote
by Ei,j,k,ε，the event
{∀Z ∈ [-1,1], ∃S ⊆ [n]: Iz — 工 Yι, 1 ,t, 1 ∙ U+j,k,t∣ <ε}
t∈S
We noW use the RSS result, Lueker (1998, Corollary 3.3) (Theorem 2 in Appendix C), to shoW that
there exists constants a, b such that
E( G max ,321 min z-工 V1,1 ,t, 1 ∙ U+j,k,t∖) ≤ ae-bn.
∖z∈[-n/32,n/32] S⊆[(f) * n]
t∈S
It is not hard to shoW that, since (V1,1,t,1)1≤t≤n and (Ui+,j,k,t)1≤t≤n are i.i.d. Unif([-1, 1]) random
variables, then the value of the density of (V1,1 ,t, 1 ∙ U：j 卜 ∕1≤t≤n is at least lo2g2 on [—1 /2, 1 /2],
and, thus, it contains a lo2g2-scaled Unif([-1 /2, 1 /2]) (see Lemma 3 in Appendix C for details).
In particular, setting X = V1,1 ,t,1 ∙ U+j 卜七,we have that μ- = E(1 X≤oX) ≤ — log2 < —1 /16
and μ + = E(1 χ>oX) ≥ log2 > 1 / 16. Therefore, we can apply Theorem 2 with ξ = 1 /32:
there exist constants a, b > 0 such that the expected value of the [-n/32, n/32]-subset-sum gap for
(V1,1 ^t, 1 ∙ U+j,k,t)1≤t≤n is at most ae-bn. That is,
E( max min ∣Z - W V1,1 ,t, 1 ∙ U+jkt[) ≤ ae-bn.
z∈∈[-n/32,n/32] S⊆[n] 1 乙	i,j,k,t∖J
t∈S
Assuming n ≥ 32, Markov,s inequality yields P(Ei,j,k,εf) ≤ a-bn∙ Setting ε/ = 忌 and C =
2 + ɪola, and supposing without loss of generality that ε/ < e-1, the condition n ≥ C log 1 implies
bn ≥ 2 log ε/ + log a and a-bn < ε!, and we get
Now define the simultaneous event Eε/ = ij, j k Ei,j,k,ε，. By a union bound over the inequality
above for i, j ∈ [d], k ∈ [C], we have
P(E翁≥ ≥ 1 - ε.
Finally, conditioning on E ≡ , it holds that
d2 c
sup	inf	sup IlK * X - V * σ[(U Θ S) * X]∣∣
K∈[0,1]d×d×1×1 S∈{0,1}size(U) X∈[0,1]D×D×1	max
(=) SuP inf SuP ∣∣K * X — V * (U+ Θ S) * X∣∣maχ
=1 SuPinf SuP ∣∣[k - V * (U+ θ S)] * XLax
(c)
≤ sup inf sup (∣∣K - V * (U+ Θ S)∣∣1∙∣∣X∣Imax)
KSX	1
(d)
≤ SuP inf ∣∣K — V * (U+ Θ S)∣∣1
(e)
≤ d2 C∙SuP i
(f) 2 ε
≤ d c~∩~ =
d2 C
ε,
8
Published as a conference paper at ICLR 2022
where (a) follows from Equation 1, (b) from the distributivity of the convolution operation, (c) from
Proposition 1, (d) from the fact that X ∈ [0, 1]D×D×1, (e) from the inequality ∣∣x∣∣ι ≤ m∣∣xIlmaX
for x ∈ Rm, and (f) from the definition of E ɪ.
cd2
4.3	Convolutional Layer Approximation (Proof of Lemma 2)
The general goal of this argument is to choose binary masks T and S so that (V Θ T) * σ [(U Θ S) * X]
is a sufficiently close approximation of K * X.
For 0 ∈ [cι] let K(() be K's 0-th kernel. That is,
K( () = K::: (.
,,,
Notice that K * X is the concatenation along the third dimension of each K(() * X, i.e., for 0 ∈ [Cι],
We have (K * X)：,：,( = K(() * X.
We fix T a priori to be the block diagonal matrix B with entries given by Bι,ι,t,( = 1((-ι)n<≤(n，
for t ∈ [n],0 ∈ [cι], where n/ = n/cι. In the rest of the proof, we show how to choose S, based on
U and V, in order to approximate the kernels K(().
We perform the approximation of each K(() using different sections of the tensors. To this end, for
0 ∈ [cι], let
U = U:,:,:,(( —1)n1 <t≤(nn,	S = S:,:,:,(( —1)n1 <t≤(n1,	and V = V:,:,(( —1)n1 <t≤(n1,:.
As we did in the proof of Lemma 1, we perform an initial pruning on U by restricting S to the space
of masks that prune all of its negative entries. This allows us to ignore the ReLU activation and
conclude that
((V Θ B) * σ[(U Θ S) * X])r,s,(
=	£	Y1,1 ,t,(	£	(U θ S)i,j,k,t ∙ Xr-i +1 ,s-j + 1 ,k
(( —1)nn<t≤(nn	i,j∈[d] ,k∈[c]
=(V( * [(U( Θ S() * X])
=(V( * σ[(U( Θ S() * X]).
For 0 ∈ [c 1 ] and ε/ > 0, denote by E(,ε，the event
ʃ sup	inf	sup	∣∣K(() * X — V( * σ[(U( Θ S() * X]∣∣	< ε'].
1K( U )∈[-1,1] d × d × c 0×1 SU ∈{0,1}siZe(U U) X∈[0,1]d × D × c 0	max J
Consider the event Eε∕c 1 = ∩( E(,ε∕c 1 ∙ Since n/ = n/c 1 = C log d2c0, for each 0 ∈ [C1], Lemma 1
ensures that P(E(,ε∕c 1) ≥ 1 — ε∕c 1, which implies P(Eε∕c J ≥ 1 一 ε.
Finally, conditioning on Eε∕c 1 and using the fact that the output channels of a convolutional layer
are calculated independently, we conclude
sup
K∈[-1,1]d×d×c0×
c
inf	SUp IlK * X — (V Θ T) * σ[(U Θ S) * X]H
1 S∈{0,1}size(U) X∈[0,1]D×D×c0	maX
T∈{0,1}size(V)
≤ sup
K∈[-1,1]d×d×c0×c1
inf	SUp IlK * X — (V Θ B) * σ[(U Θ S) * X]∣∣
S∈{0,1}size(U) X∈[0,1]D×D×c0	maX
=max sup	inf	sup l∣K(() * X — V( * σ[(U( Θ S() * X]
( ∈[c 1] K( g )∈[-1,1] d × d × C 0 S g ∈{0,1}siZe(U U) X∈[0,1]D × D × C 0 U
< ε.
max
9
Published as a conference paper at ICLR 2022
5	Reproducibility Statement
We made our best effort to describe in detail all the relevant parameters for Section 3. We also
took care to initialize the random number generator not only for Julia but also for Gurobi, so that
the experiments can be reproduced using the source code available at https://github.com/
ArthurWalraven/cnnslth. The latter also include a BSON serialized version of our trained
models and solutions to RSS instances, that can readily be imported with Julia.
Acknowledgments
This work has been supported by the AID INRIA-DGA agreement n°2019650072. The authors
are grateful to the OPAL infrastructure from Universite Cote d,Azur for providing resources and
support.
References
Maxwell Mbabilla Aladago and Lorenzo Torresani. Slot machines: Discovering winning combina-
tions of random weights in neural networks. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pp. 163-174. PMLR, 2021.
URL http://proceedings.mlr.press/v139/aladago21a.html.
Davis W. Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John V. Guttag. What is the
state of neural network pruning? In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne
Sze (eds.), Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA,
March 2-4, 2020. mlsys.org, 2020. URL https://proceedings.mlsys.org/book/
296.pdf.
Justin Cosentino, Federico Zaiter, Dan Pei, and Jun Zhu. The search for sparse, robust neural
networks. CoRR, abs/1912.02386, 2019. URL http://arxiv.org/abs/1912.02386.
James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding accurate
binary neural networks by pruning A randomly weighted network. In 9th International Confer-
ence on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-
view.net, 2021. URL https://openreview.net/forum?id=U_mat0b9iv.
Jonas Fischer and Rebekka Burkholz. Towards strong pruning for lottery tickets with non-zero
biases. CoRR, abs/2110.11150, 2021. URL https://arxiv.org/abs/2110.11150.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neu-
ral networks. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.
net/forum?id=rJl-b3RcF7.
Adam Gaier and David Ha. Weight agnostic neural networks. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d,Alche—Buc, Emily B. Fox, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
5365-5379, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
e98741479a7b998f88b8f8c9f0b6b6f1- Abstract.html.
Ian J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. Deep Learning. Adaptive com-
putation and machine learning. MIT Press, 2016. ISBN 978-0-262-03561-3. URL http:
//www.deeplearningbook.org/.
Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.
gurobi.com.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classification. In 2015 IEEE International Confer-
ence on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026-1034.
10
Published as a conference paper at ICLR 2022
IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.123. URL https://doi.org/10.
1109/ICCV.2015.123.
Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso,
Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. Fashionable modelling with flux.
CoRR, abs/1811.01457, 2018. URL https://arxiv.org/abs/1811.01457.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Robert Tjarko Lange. The lottery ticket hypothesis: A survey. https://roberttlange.github.io/year-
archive/posts/2020/06/lottery-ticket-hypothesis/, 2020. URL https://roberttlange.
github.io/posts/2020/06/lottery-ticket-hypothesis/.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. ProceedingsoftheIEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791. URL
https://ieeexplore.ieee.org/document/726791/.
Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E.
Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition.
Neural Comput., 1(4):541-551, 1989a. doi: 10.1162/neco.1989.1.4.541. URL https://doi.
org/10.1162/neco.1989.1.4.541.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touret-
zky (ed.), Advances in Neural Information Processing Systems 2, [NIPS Conference, Den-
ver, Colorado, USA, November 27-30, 1989], pp. 598-605. Morgan Kaufmann, 1989b. URL
http://papers.nips.cc/paper/250-optimal-brain-damage.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: single-shot network pruning
based on connection sensitivity. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https:
//openreview.net/forum?id=B1VZqjAcYX.
George S. Lueker. Exponentially small bounds on the expected optimum of the partition and
subset sum problems. Random Struct. Algorithms, 12(1):51-62, 1998. doi: 10.1002/(SICI)
1098-2418(199801)12:1%3C51::AID-RSA3%3E3.0.CO;2-S. URL https://doi.org/10.
1002/(SICI)1098-2418(199801)12:1%3C51::AID-RSA3%3E3.0.CO;2-S.
Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. Proving the lottery ticket
hypothesis: Pruning is all you need. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of
Machine Learning Research, pp. 6682-6691. PMLR, 2020. URL http://proceedings.
mlr.press/v119/malach20a.html.
Michael Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In David S. Touretzky (ed.), Advances in Neural Informa-
tion Processing Systems 1, [NIPS Conference, Denver, Colorado, USA, 1988], pp. 107-115.
Morgan Kaufmann, 1988. URL https://proceedings.neurips.cc/paper/1988/
file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf.
Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic pruning is all you need.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1e9491470749d5b0e361ce4f0b24d037- Abstract.html.
Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris S. Papailiopou-
los. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is suffi-
cient. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
11
Published as a conference paper at ICLR 2022
Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1b742ae215adf18b75449c6e272fd92d- Abstract.html.
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad
Rastegari. What’s hidden in a randomly weighted neural network? In 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
USA, June 13-19, 2020, pp. 11890-11899. Computer Vision Foundation / IEEE, 2020.
doi: 10.1109/CVPR42600.2020.01191. URL https://openaccess.thecvf.com/
content_CVPR_2020/html/Ramanujan_Whats_Hidden_in_a_Randomly_
Weighted_Neural_Network_CVPR_2020_paper.html.
Matthia Sabatelli, Mike Kestemont, and Pierre Geurts. On the transferability of winning tick-
ets in non-natural image datasets. In Giovanni Maria Farinella, Petia Radeva, Jose Braz, and
Kadi Bouatouch (eds.), Proceedings of the 16th International Joint Conference on Computer
Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2021, Volume
5: VISAPP, Online Streaming, February 8-10, 2021, pp. 59-69. SCITEPRESS, 2021. doi:
10.5220/0010196300590069. URL https://doi.org/10.5220/0010196300590069.
Ryan Van Soelen and John W. Sheppard. Using winning lottery tickets in transfer learning for
convolutional neural networks. In International Joint Conference on Neural Networks, IJCNN
2019 Budapest, Hungary, July 14-19, 2019, pp. 1-8. IEEE, 2019. doi: 10.1109/IJCNN.2019.
8852405. URL https://doi.org/10.1109/IJCNN.2019.8852405.
Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. Int. J. Comput. Vis.,
128(7):1867-1888, 2020. doi: 10.1007/s11263-020-01303-4. URL https://doi.org/10.
1007/s11263-020-01303-4.
Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, and Xiaolin Hu. Pruning
from scratch. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth
AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY,
USA, February 7-12, 2020, pp. 12273-12280. AAAI Press, 2020. URL https://aaai.org/
ojs/index.php/AAAI/article/view/6910.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/
abs/1708.07747.
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural
networks using energy-aware pruning. In 2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6071-6079. IEEE Computer
Society, 2017. doi: 10.1109/CVPR.2017.643. URL https://doi.org/10.1109/CVPR.
2017.643.
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari S. Morcos. Playing the lottery with rewards and
multiple languages: lottery tickets in RL and NLP. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=S1xnXRVFwH.
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Ze-
ros, signs, and the supermask. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
3592-3602, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
1113d7a76ffceca1bb350bfe145467c6- Abstract.html.
12
Published as a conference paper at ICLR 2022
A B ound on the Norm of a Convolution
The following proposition can be seen as a special case of Young’s Convolution Inequality for func-
tions over Zn where the norms in the inequality are the 01 and 0∞ norms.
Proposition 1 (Tensor Convolution Inequality). Given K ∈ Rd×d×c0 ×c1 and X ∈ RD×D×c0, we
have
IlK * Xllmax ≤∣∣K∣∣1∙∣∣X∣Imax∙
Proof.	We have llK * XIlmax ≤ ∙. max	£	I K2，,j∖k,Kx2-2，+1 -1 ,k | 2,j ∈[ ] ] ∈[ c 1] 2, ,j ^∈[ d ] ,k ∈[ c ] ≤ max ∑ IK22J,k,Kl∣X∣max 2,j,K Z~~d 2 2,j 2 ,k ≤ max (工 |K2 2 ,j 2 ,k,X I)IlXllmax ® ∖ Mk	) ≤ max IlKMXllmax= IlKIl1∙∣∣X∣∣max∙ 2,j,X
□
B CNN Approximation (Proof of Theorem 1)
Since Lemma 2 provide bounds in terms of the output ofa layer, the study of the propagation of this
error through the network is mostly independent of the layer type. Hence, the next proof follows the
structure of Pensia et al. (2020, Theorem 1), where our Lemma 2 assumes the role of their Lemma 3,
and where we leverage our Proposition 1 in order to address the problem of bounding the maximum
norm of a convolution.
Proof (of Theorem 1). For the sake of brevity, in the proof We denote the max-norm simply by ∣∣∙∣.
Let Xi be the input of the i-th layer of the network f . Thus,
1.	X1 = X,
2.	XX+1 = σ(Ki * Xi) for 1 ≤ i ≤ 0 - 1 and
3.	f (X) = KK * XX.
By applying Lemma 2 to each layer, we choose masks S2i and S2i-1 so that
sup∣∣Ki * X -(L2i Θ S22) * σ[(L2i-1 Θ S2i-1) * X]∣∣ < W	⑶
with probability at least 1 — 2K.
Since the ReLU function is 1-Lipschitz with respect to the max norm, the above event implies the
following for all i ∈ [0 — 1]:
sup∣∣σ(K2 * X) — σ[(L22 Θ S22) * σ[(L22-1 Θ S22-1) * X]]∣∣ < £	(4)
By a union bound, with probability 1 - ε, Equation 3 and Equation 4 hold for all layers simultane-
ously. In the rest of the proof, we implicitly condition on the latter event.
For any fixed function f, let g be the pruned network constructed layer-wise, by pruning with binary
22
masks which satisfy Equation 3 and Equation 4. Let these pruned tensors be L , and let X be the
input to the (2i — 1)-th layer ofg.
2
We note that X satisfies the the recurrence relation
13
Published as a conference paper at ICLR 2022
~ 1
1.	X = X,
二i+1	-2 i	-2 i-1 _
2.	X = σ(L * σ(L	* X )) for 1 ≤ i ≤ £ — 1,
i	i-1
Because IIXll ≤ 1, Equation 4 implies that ∣∣X ∣∣ ≤ (1 + 务).To see this, note that Equation 4
implies, for 1 ≤ i ≤ £ — 1,
Il σ(Ki * Xi) — Xi+1 N ≤ ε
UlIXill	肉 M 2£
thus
Iσ(Ki *Xi) — Xi+1∣≤ 2£∣Xi∣∙
By the reverse triangle inequality, the last inequality implies
~ i +1	ε ~i	:	~i
∣X+ l≤ R∣X I +1σ(Ki *X)∣
£	~ 5	.-	~ i
≤ R ∣X I +1Ki * X I
ε ~ i	:	~ i
≤ 而 ∣X I +1Ki ∣1.∣X∣
ε ~ i ~ i
≤ RIX I + IX I
≤ (1+⅛ )ili∙
Applying this inequality recursively, we get that ∣Xi ∣ ≤ (1 + 京)	for 1 ≤ i ≤ £ — 1. This allows
ii
us to bound the error between X and X . For 1 ≤ i ≤ £ — 1, we have
IXi+1 — Xi +1∣ = Iσ(Ki * Xi) — σ(l22 * σ(L2i-1 * Xi))I
i	i	2i	2i 1 i
≤ Iσ(Ki	*	Xi)	—	σ(Ki	* X )∣ + ∣σ(Ki	* X ) —	σ(L	*	σ(L	* X ))∣
:	:	i i	: i i	2 2 i	2 2 i— 1 ~i
≤ IKi ∣1∣Xi — X I + I σ (Ki * X) — σ (L * σ (L	* X ))∣
:	ii	: ii	22i	22i—1	~i
≤ IXi — X I + Iσ(Ki * X ) — σ(L * σ(L * X ))∣
i	i-1
≤ix i - xi + (1 + 2£)女	⑸
where for the last inequality we use Equation 3. Unrolling Equation 5 we get
i-1	∙ 1
∣Xi -X i∣≤ ∑(1+2⅛)i -
i=1
ε
2£.
14
Published as a conference paper at ICLR 2022
Finally, this last inequality leads, with probability at least 1 - ε, to
H f (X) - g(X)H =
≤
≤
≤
≤
≤
∣K。* XJ LL晨 σ(L2"1 * XbH
IK0 * Xe - Ke * XeH + HK£ * Xe - L2' * σ(LL”1 * Xj∣∣
∣keH1HXX - XeH + HKK * Xe - LLe * σ(LLe-1 * Xe)H
IX。-XeH + HKe * XX-L2e * σ(LLe-1 * Xe)H
IXe- X XH + (1 + 2∣)	W
(X (1+京)卷)+(1+京)卷
e ,	.一
≤ X(1+京)京
i=1
=(1 + εil) - 1
< eε/L-1
< ε,
where the last inequality holds because ε < 1.
Replacing ε in this proof with min{ε, δ} concludes the proof of the theorem.
□
C Random Subset-Sum Theorem
For the sake of completeness, in this section we recall a result by Lueker (1998) together with the
necessary definitions.
Definition 2. Given two positive constants a and b, we say that a distribution with density f contains
a b-scaled Unif ([-a, a]) distribution if for each x ∈ [-a, a] it holds f(x) ≥ b. We simply say that
a distribution F contains a uniform distribution if there exist positive constants a and b such that F
contains a b-scaled Unif ([-a, a]) distribution.
The following is a weaker version of Corollary 1 in the Appendix of Pensia et al. (2020).
Lemma 3. Let X1 and XL be two independent random variables following a Unif ([-1, 1]) distri-
bution. Then X 1 ∙ Xl contains a log^-scaled Unif([-ɪ, 11 ]) distribution.
We say that z is 2η-subsetsum-approximated with S = {X1, . . ., Xn} if there exists a subset Iz ⊆
[n] s.t. | Σi∈ιz Xi-H ≤ 2η.
Definition 3. The [a, b]-subset-sum gap of S = {X1, . . ., Xn} is the smallest value of η such that
each z ∈ [a, b], can be 2η-subsetsum-approximated with S.
Theorem 2 (Corollary 3.3 in Lueker (1998)). Let S = {X1, . . ., Xn} be n i.i.d. bounded random
variables and ξ > 0 any constant. Suppose that the distribution of X1 contains a uniform distribu-
tion. Let μ- = E[1 X≤oX], μ + = E[1 X≥oX], μabs = E[|X|] = μ + - μ-. The expected value of
the [(μ- + ξ)n, (μ + - ξ)n]-subset-sum gap of S is exponentially small with respect to n.
15