Published as a conference paper at ICLR 2022
COPA: Certifying Robust Policies for Of-
fline Reinforcement Learning against Poison-
ing Attacks
Fan Wu1* Linyi Li1* Chejian Xu1 Huan Zhang2 Bhavya Kailkhura3
Krishnaram Kenthapadi4 Ding Zhao2 Bo Li1
1University of Illinois at Urbana-Champaign 2Carnegie Mellon University
3 Lawrence Livermore National Laboratory 4Amazon AWS AI
{fanw6,linyi2,chejian2,lbo}@illinois.edu	huan@huan-zhang.com
kailkhura1@llnl.gov kkenthapadi@gmail.com dingzhao@andrew.cmu.edu
* Equal contribution.
Ab stract
As reinforcement learning (RL) has achieved near human-level performance in a
variety of tasks, its robustness has raised great attention. While a vast body of
research has explored test-time (evasion) attacks in RL and corresponding de-
fenses, its robustness against training-time (poisoning) attacks remains largely
unanswered. In this work, we focus on certifying the robustness of offline RL
in the presence of poisoning attacks, where a subset of training trajectories could
be arbitrarily manipulated. We propose the first certification framework, COPA,
to certify the number of poisoning trajectories that can be tolerated regarding dif-
ferent certification criteria. Given the complex structure of RL, we propose two
certification criteria: per-state action stability and cumulative reward bound. To
further improve the certification, we propose new partition and aggregation pro-
tocols to train robust policies. We further prove that some of the proposed certifi-
cation methods are theoretically tight and some are NP-Complete problems. We
leverage COPA to certify three RL environments trained with different algorithms
and conclude: (1) The proposed robust aggregation protocols such as temporal
aggregation can significantly improve the certifications; (2) Our certifications for
both per-state action stability and cumulative reward bound are efficient and tight;
(3) The certification for different training algorithms and environments are dif-
ferent, implying their intrinsic robustness properties. All experimental results are
available at https://copa-leaderboard.github.io.
1 Introduction
Reinforcement learning (RL) has been widely applied to a range of applications, including
robotics (Kober et al., 2013; Deisenroth et al., 2013; Polydoros & Nalpantidis, 2017) and au-
tonomous vehicles (Shalev-Shwartz et al., 2016; Sallab et al., 2017). In particular, offline RL (Levine
et al., 2020) is proposed to leverage previously observed data to train the policy without requiring
expensive interaction with environments or online data collection, and enables the reuse of training
data (Agarwal et al., 2020). However, offline RL also raises a great safety concern on poisoning
attacks (Kiourti et al., 2020; Wang et al., 2020; 2021). A recent survey of industry reports that data
poisoning is significantly more concerning than other threats (Kumar et al., 2020). For offline RL,
the situation is even worse, and a recent theoretical study shows that the robust offline RL against
poisoning attacks is a strictly harder problem than online RL (Zhang et al., 2021).
Although there are several empirical and certified defenses for classification tasks against poisoning
attacks (Peri et al., 2020; Levine & Feizi, 2021; Weber et al., 2020), it is challenging and shown
ineffective to directly apply them to RL given its complex structure (Kiourti et al., 2020). Thus, ro-
bust offline RL against poisoning attacks remains largely unexplored with no mention of robustness
certification. In addition, though some theoretical analyses provide general robustness bounds for
RL, they either assume a bounded distance between the learned and Bellman optimal policies or are
limited to linear MDPs (Zhang et al., 2021). To the best of our knowledge, there is no robust RL
1
Published as a conference paper at ICLR 2022
method that is able to provide practically computable certified robustness against poisoning attacks.
In this paper, We tackle this problem by proposing the first framework of Certifying robust policies
for general offline RL against poisoning attacks (COPA).
Certification Criteria. One critical challenge in certifying robustness for offline RL is the certifi-
cation criteria, since the prediction consistency is no longer the only goal as in classification. We
propose two criteria based on the properties of RL: per-state action stability and CUmUlative reward
bound. The former guarantees that at a specific time, the policy learned with COPA will predict the
same action before and after attacks Under certain conditions. This is important for gUaranteeing the
safety of the policy at critical states, e.g., braking when seeing pedestrians. For cumulative reward
bound, a lower bound of the cumulative reward for the policy learned with COPA is guaranteed
under certain poisoning conditions. This directly guarantees the worst-case overall performance.
COPA Framework. COPA is composed of two components: policy partition and aggregation
protocol and robustness certification method. We propose three policy partition aggregation pro-
tocols: PARL (Per-State Partition Aggregation), TPARL (Temporal Partition Aggregation), and
DPARL (Dynamic Temporal Partition Aggregation), and propose certification methods for each of
them corresponding to both proposed certification criteria. In addition, for per-state action stability,
we prove that our certifications for PARL and TPARL are theoretically tight. For cumulative reward
bound, we propose an adaptive search algorithm, where we compute the possible action set for each
state under certain poisoning conditions. Concretely, we propose a novel method to compute the
precise action set for PARL and efficient algorithms to compute a superset of the possible action
set which leads to sound certification for TPARL and DPARL. We further prove that for PARL our
certification is theoretically tight, for TPARL the theoretically tight certification is NP-complete,
and for DPARL it is open whether theoretically tight certification exists.
Technical Contributions. We take the first step towards certifying the robustness of offline RL
against poisoning attacks, and we make contributions on both theoretical and practical fronts.
•	We abstract and formulate the robustness certification for offline RL against poisoning attacks,
and we propose two certification criteria: per-state action stability and cumulative reward bound.
•	We propose the first framework COPA for certifying robustness of offline RL against poisoning
attacks. COPA includes novel policy aggregation protocols and certification methods.
•	We prove the tightness of the proposed certification methods for the aggregation protocol PARL.
We also prove the computational hardness of the certification for TPARL.
•	We conduct thorough experimental evaluation for COPA on different RL environments with
three offline RL algorithms, demonstrating the effectiveness of COPA, together with several
interesting findings.
2	Related Work
Poisoning attacks (Nelson et al., 2008; Diakonikolas et al., 2016) are critical threats in machine
learning, which are claimed to be more concerning than other threats (Kumar et al., 2020).
Poisoning attacks widely exist in classification (Schwarzschild et al., 2021), and both empirical
defenses (Liu et al., 2018; Chacon et al., 2019; Peri et al., 2020; Steinhardt et al., 2017) and certified
defenses (Weber et al., 2020; Jia et al., 2020; Levine & Feizi, 2021) have been proposed.
After Kiourti et al. (2020) show the existence of effective backdoor poisoning attacks in RL, a
recent work theoretically and empirically validates the existence of reward poisoning in online
RL (Zhang et al., 2020b). Furthermore, Zhang et al. (2021) theoretically prove that the offline RL
is more difficult to be robustified against poisoning than online RL considering linear MDP. From
the defense side, Zhang et al. (2021) propose robust variants of the Least-Square Value Iteration
algorithm that provides probabilistic robustness guarantees under linear MDP assumption. In
addition, Robust RL against reward poisoning is studied in Banihashem et al. (2021), but robust
RL against general poisoning is less explored. In this background, we aim to provide the certified
robustness for general offline RL algorithms against poisoning attacks, which is the first work that
achieves the goal. We discuss broader related work in Appendix I.
3	Certification Criteria of COPA
In this section, we propose two robustness certification criteria for offline RL against general poi-
soning attacks: per-state action stability and cumulative reward bound.
2
Published as a conference paper at ICLR 2022
Offline RL. We model the RL environment by an episodic finite-horizon Markov decision pro-
cess (MDP) E = (S, A, R, P, H, d0), where S is the set of states, A is the set of discrete actions,
R : S × A → R is the reward function, P : S × A → P(S) is the stochastic transition function
with P(∙) defining the set of probability measures, H is the time horizon, and do ∈ P(S) is the
distribution of the initial state. At time step t, the RL agent is at state st ∈ S. After choosing action
at ∈ A, the agent transitions to the next state st+ι 〜P(st,at) and receives reward r = R(st, at).
After H time steps, the cumulative reward J = PtH=-01 rt. We denote a consecutive sequence of all
states between time step l and r as sl:r := [sl, sl+1, . . . , sr].
Here we focus on offline RL, for which the threat of poisoning attacks is practical and more chal-
lenging to deal with (Zhang et al., 2021). Concretely, in offline RL, a training dataset D = {τi}iN=1
consists of logged trajectories, where each trajectory τ = {(sj, rj, aj, s0j)}lj=1 ∈ (S × A × R × S)l
consists of multiple tuples denoting the transitions (i.e., starting from state sj, taking the action aj,
receiving reward rj , and transitioning to the next state s0j ).
Poisoning Attacks. Training dataset D can be poisoned in the following manner. For each tra-
jectory τ ∈ D, the adversary is allowed to replace it with an arbitrary trajectory τe, generating a
manipulated dataset D. We denote D ㊀ D = (D'D) ∣J(D∖D) as the symmetric difference between
two datasets D and D. For instance, adding or removing one trajectory causes a symmetric differ-
ence of magnitude 1, while replacing one trajectory with a new one leads to a symmetric difference
of magnitude 2. We refer to the size of the symmetric difference as the poisoning size.
Certification Goal. To provide the robustness certification against poisoning attacks introduced
above, we aim to certify the test-time performance of the trained policy in a clean environment.
Specifically, in the training phase, the RL training algorithm and our aggregation protocol can be
jointly modeled by M : D → (S? → A) which provides an aggregated policy, where S? denotes
the set of all consecutive state sequences. Our goal is to provide robustness certification for the
poisoned aggregated policy ∏ = M(D), given bounded poisoning Size (i.e., |D ㊀ D| ≤ K).
Robustness Certification Criteria: Per-State Action Stability. We first aim to certify the ro-
bustness of the poisoned policy in terms of the stability of per-state action during test time.
Definition 1 (Robustness Certification for Per-State Action Stability). Given a clean dataset D, we
define the robustness certification forPer-State action stability as that for any D satisfying | D ㊀ D | ≤
K, the action predictions of the poisoned and clean policies for the state (or state sequence) S are the
same, i.e., ∏ = M(D),∏ = M(D),∏(s) = ∏(s), under the the tolerable poisoning threshold K.
In an episode, We denote the tolerable poisoning threshold for the state at step t by Kt.
The definition encodes the requirement that, for a particular state, any poisoned policy will always
give the same action prediction as the clean one, as long as the poisoning size is within K (K
computed in Section 4). In this definition, s could be either a state or a state sequence, since our ag-
gregated policy (defined in Section 3) may aggregate multiple recent states to make an action choice.
Robustness Certification Criteria: Lower Bound of Cumulative Reward. We also aim to cer-
tify poisoned policy’s overall performance in addition to the prediction at a particular state. Here we
measure the overall performance by the cumulative reward J(π) (formally defined in Appendix A).
Now we are ready to define the robustness certification for cumulative reward bound.
Definition 2 (Robustness Certification for Cumulative Reward Bound). Robustness certification for
cumulative reward bound is the lower bound of cumulative reward JK such that JK ≤ J(π) for any
π = M(D) where |D ㊀ D| ≤ K, i.e., π is trained on poisoned dataset D within poisoning Size K.
4	Certification Process of COPA
In this section, we introduce our framework COPA, which is composed of training Protocols, ag-
gregation Protocols, and certification methods. The training protocol combined with an offline RL
training algorithm provides subpolicies. The aggregation protocol aggregates the subpolicies as an
aggregated policy. The certification method certifies the robustness of the aggregated policy against
poisoning attacks corresponding to different certification criteria provided in Section 3.
3
Published as a conference paper at ICLR 2022
Table 1: Overview of theoretical results in Section 4. “Certification” columns entail our certification theorems.
“Analysis” columns entail the analyses of our certification bounds, where “tight” means our certification is
theoretically tight, “NP-complete” means the tight certification problem is NP-complete, and “open” means
the tight certification problem is still open. Theorems 8 and 13 and proposition 9 are in Appendices C and F.6.2.
CartififtiCn	ProPoSed Aggregation ProtoCol___________________________________
Certification
Criteria
PARL (πP, Definition 7)
CertifiCation
AnalySiS
CertifiCation
TPARL (πT, Definition 8)
AnalySiS
DPARL (πD, Definition 3)
CertifiCation AnalySiS
Per-State Action Theorem 8 tight (Proposition 9) Theorem 1 tight (Proposition 2) Theorem 3 open
Cumulative Reward Theorem 4 tight (Theorem 13) Theorem 5 NP-complete (Theorem 6) Theorem 7 open
Overview of Theoretical Results. In Table 1, we present an overview of our theoretical results:
For each proposed aggregation protocol and certification criteria, we provide the corresponding cer-
tification method and core theorems, and we also provide the tightness analysis for each certification.
4.1	Partition-Based Training Protocol
COPA’s training protocol contains two stages: partitioning and training. We denote D as the entire
offline RL training dataset. We abstract an offline RL training algorithm (e.g., DQN) by M0 : 2D →
Π, where 2D is the power set ofD, and Π = {π : S → A} is the set of trained policies. Each trained
policy in Π is a function mapping a given state to the predicted action.
Partitioning Stage. In this stage, we separate the training dataset D into u partitions {Di }iu=-01
that satisfy UU=-ο1 Di = D and ∀i = j, Di ∩ Dj = 0. Concretely, when performing partitioning,
for each trajectory τ ∈ D, we deterministically assign it to one unique partition. The assignment
is only dependent on the trajectory τ itself, and not impacted by any modification to other parts of
the training set. One design choice of such a deterministic assignment is using a deterministic hash
function h to compute the assignment, i.e., Di = {τ ∈ D | h(τ) ≡ i (mod u)}, ∀i ∈ [u].
Training Stage. In this stage, for each training data partition Di, we independently apply an
RL algorithm M0 to train a policy πi = M0 (Di). Hereinafter, we call these trained polices as
subpolicies to distinguish from the aggregated policies. Concretely, let [u] := {0, 1, . . . , u - 1}. For
these subpolicies, the policy indicator 1i,a : S → {0, 1} is defined by 1i,a(s) := 1[πi(s) = a],
indicating whether subpolicy πi chooses action a at state s. The aggregated action count na : S →
N≥0 is the number of votes across all the subpolicies for action a given state s: n0(s) := ∣{i∣∏i(s)=
a, i ∈ [u]}| = Piu=-01 1i,a(s). Specifically, we denote na(sl:r) for Pjr=l na(sj), i.e., the sum of votes
for states between time step l and r. A detailed algorithm of the training protocol is in Appendix E.1.
Now we are ready to introduce the proposed aggregation protocols in COPA (PARL, TPARL,
DPARL) that generate aggregated policies based on subpolicies, and corresponding certification.
4.2	Aggregation Protocols: PARL, TPARL, DPARL
With u learned subpolicies {πi}iu=-01, we propose three different aggregation protocols in COPA to
form three types of aggregated policies for each certification criteria: PARL, TPARL, and DPARL.
Per-State Partition Aggregation (PARL). Inspired by aggregation in classification (Levine &
Feizi, 2021), PARL aggregates subpolicies by choosing actions with the highest votes. We denote
the PARL aggregated policy by πP : S → A. When there are multiple highest voting actions, we
break ties deterministically by returning the “smaller” (<) action, which can be defined by numeri-
cal order, lexicographical order, etc. Throughout the paper, we assume arg max over A always uses
< operator to break ties. The formal definition of the protocol is in Appendix A.
The intuition behind PARL is that the poisoning attack within size K can change at most K sub-
policies. Therefore, as long as the margin between the votes for top and runner-up actions is larger
than 2K for the given state, after poisoning, we can guarantee that the aggregated PARL policy will
not change its action choice. We will formally state the robustness guarantee in Section 4.3.
Temporal Partition Aggregation (TPARL). In the sequential decision making process of RL,
it is likely that certain important states are much more vulnerable to poisoning attacks, which we
refer to as bottleneck states. Therefore, the attacker may just change the action predictions for these
bottleneck states to deteriorate the overall performance, say, the cumulative reward. For example,
in Pong game, we may lose the round when choosing an immediate bad action when the ball is
closely approaching the paddle. Thus, to improve the overall certified robustness, we need to focus
4
Published as a conference paper at ICLR 2022
on improving the tolerable poisoning threshold for these bottleneck states. Given such intuition and
goal, we propose Temporal Partition Aggregation (TPARL) and the aggregated policy is denoted as
πT , which is formally defined in Definition 8 in Appendix A.
TPARL is based on two insights: (1) Bottleneck states have lower tolerable poisoning threshold,
which is because the vote margin between the top and runner-up actions is smaller at such state;
(2) Some RL tasks satisfy temporal continuity (Legenstein et al., 2010; Veerapaneni et al., 2020),
indicating that good action choices are usually similar across states of adjacent time steps, i.e.,
adjacent states. Hence, we leverage the subpolicies’ votes from adjacent states to enlarge the vote
margin, and thus increase the tolerable poisoning threshold. To this end, in TPARL, we predetermine
a window size W, and choose the action with the highest votes across recent W states.
Dynamic Temporal Partition Aggregation (DPARL). The TPARL uses a fixed window size W
across all states. Since the specification of the window size W requires certain prior knowledge,
plus that the same fixed window size W may not be suitable for all states, it is preferable to perform
dynamic temporal aggregation by using a flexible window size. Therefore, we propose Dynamic
Temporal Partition Aggregation (DPARL), which dynamically selects the window size W towards
maximizing the tolerable poisoning threshold per step. Intuitively, DPARL selects the window size
W such that the average vote margin over selected states is maximized. To guarantee that only
recent states are chosen, we further constrain the maximum window size (Wmax).
Definition 3 (Dynamic Temporal Partition Aggregation). Given subpolicies {πi}iu=-01 and maximum
window size Wmax , at time step t, the Dynamic Temporal Partition Aggregation (DPARL) defines
an aggregated policy πD : Smin{t+1,Wmax} → A such that
nD(Smax{t-Wmax+i,0}：t) ：= arg maxna(st-w，+i：t), where W0 = arg max	∆W.	(1)
a∈A	1≤W ≤min{Wmax ,t+1}
In the above equation, na is defined in Section 3 and ∆tW is given by
δt ：= TTV (JnaI (St — W +1:t) - na，2 (St — W + Lt)),
W	(2)
where a1 = arg maxna (St—W+1:t), a2 = arg max na (St—W +1:t).
a∈A	a∈A,a6=a1
In the above definition, ∆tW encodes the average vote margin between top action a1 and runner-up
action a2 if choosing window size W. Thus, W0 locates the window size with maximum average
vote margin, and its corresponding action is selected. Again, we use the mechanism described in
PARL to break ties. Robustness certification methods for DPARL are in Sections 4.3 and 4.4.
In Appendix B, we present a concrete example to demonstrate how different aggregation protocols
induce different tolerable poisoning thresholds, and illustrate bottleneck and non-bottleneck states.
4.3	Certification of Per- S tate Action Stability
In this section, we present our robustness certification theorems and methods for per-state action.
For each of the aggregation protocols (PARL, TPARL, and DPARL), at each time step t, We will
compute a valid tolerable poisoning threshold K as defined in Definition 1, such that the chosen
action at step t does not change as long as the poisoning size K ≤ K.
Certification for PARL. Due to the space limit, we defer the robustness certification method for
PARL to Appendix C. The certification method is based on Theorem 8. We further show the theo-
retical tightness of the certification in Proposition 9. All the theorem statements are in Appendix C.
Certification for TPARL. We certify the robustness of TPARL following Theorem 1.
Theorem 1. Let D be the clean training dataset; let πi = M0 (Di), 0 ≤ i ≤ u - 1 be the learned
subpolicies according to Section 4.1 from which we define na (Section 3); and let πT be the Temporal
Partition Aggregation policy: πT = M(D) where M abstracts the whole training-aggregation
process. D is a poisoned dataset and πfT is the poisoned policy: πfT = M(D).
For a given state st encountered at time step t during test time, let a :
at time step t the tolerable poisoning threshold (see Definition 1)
Kt =	min max 4 P I 5^h(i)， ≤ 3a®
a0 6=a,a0∈A	a,a a,a
πT (smax{t-W +1,0}:t), then
(3)
)
5
Published as a conference paper at ICLR 2022
where {h(ai,)a0}iu=1 is a nonincreasing permutation of
min{W -1,t}	min{W -1,t}
X	1i,a (st-j) + min{W, t + 1} -	X	1i,a0 (st-j)
j=0	j=0
and δa,a0 := na (smax{t-W +1,0}:t) - (na0 (smax{t-W +1,0}:t) + 1[a < a]).
{hi,a,a0}iu=-01,
(4)
1[πi (s) = a] (Section 3), and W is the window size.
Here, 1i,a (s)
Remark. We defer the detailed proof to Appendix F.2. The theorem provides a per-state action
certification for TPARL. The detailed algorithm is in Algorithm 3 (Appendix E.2). The certification
time complexity per state is O(|A|u(W +log u)) and can be further optimized to O(|A|u log u) with
proper prefix sum caching across time steps. We prove the certification for TPARL is theoretically
tight in Proposition 2 (proof in Appendix F.3). We also prove that directly extending Theorem 8 for
TPARL (Corollary 10) is loose in Appendix F.4.
Proposition 2. Under the same condition as Theorem 1, for any time step t, there exists an
RL learning algorithm Mo, and a poisoned dataset D, such that |D ㊀ D| = Kt + 1, and
πfT (smax{t-W +1,0}:t) 6= πT (smax{t-W +1,0}:t).
Certification for DPARL. Theorem 3 provides certification for DPARL.
Theorem 3. Let D be the clean training dataset; let πi = M0 (Di), 0 ≤ i ≤ u - 1 be the learned
subpolicies according to Section 4.1 from which we define na (see Section 3); and let πD be the
Dynamic Temporal Partition Aggregation: πD = M(D) where M abstracts the whole training-
aggregation process. D is a poisoned dataset and πfD is the poisoned policy: πfD = M(D).
For a given state st encountered at time step t during test time, let a := πD (smax{t-Wmax+1,0}:t)
and W0 be the chosen time window (according to Equation (1)), then tolerable poisoning threshold
KD = min
~.	WV *,W0
min	L 0 ,00
1≤W * ≤min{Wmax ,t+1},W * 6=W0 ,a0 6=a,a00 6=a	,
(5)
W r/ - w w 7 7 L , ■	∕c∖	■, w TTT- τττ-/ W τ Wr * .^W^ 0 IC ι ι , ι IT Ce ", ■	/
where Kt is defined by Equation (3) with W as W0 and La,a0,0 defined by the below Definition 4.
Definition 4 (L in Theorem 3). Under the same condition as Theorem 3, for given W*, W0, a, α0, a00,
we let a# := arg maxa0 6=a0,a0∈A na0 (st- *+1:t), then
LW[,W' ：= max (P | X g(i) + W 0(nW* - nW) — W *(nW 0 - nW0 00 ) - 1 [a0 >a] < θ)	(6)
where naw is a shorthand of na(st-w+1:t) and {g(i)}iu=1 is a nonincreasing permutation of {gi}iu=-01.
Each gi is defined by
max{W * ,W0}
gi :=	max σw (a0) - σw (πi (st-w)), where
a0∈A
w=0
(7)
σw(ao) := W01[ao = a0,w ≤ W*] — W01[ao = a#, W ≤ W*] — W*1[a0 = a,w ≤ W0] + W*1[a0 = a00,w ≤ W0].
Proof sketch. A successful poisoning attack should change the chosen action from a to an another
d_. If after attack, the chosen window size is still W0, the poisoning size should be at least larger than
Kt according to Theorem 1. If the chosen window size is not W0, We find out that the poisoning size
rW * .W 0
is at least mina00=a La/ a0 0 +1 from a greedy-based analysis. Formal proof is in Appendix F.5. □
Remark. The theorem provides a valid per-state action certification for DPARL policy. The de-
tailed algorithm is in Algorithm 4 (Appendix E.2). The certification time complexity per state is
O(Wm2 ax|A|2u+Wmax|A|2u log u), which in practice adds similar overhead compared with TPARL
certification (see Appendix H.3). Unlike certification for PARL and TPARL, the certification given
by Theorem 3 is not theoretically tight. An interesting future work would be providing a tighter
per-state action certification for DPARL.
4.4	Certification of Cumulative Reward B ound
In this section, we present our robustness certification for cumulative reward bound. We assume the
deterministic RL environment throughout the cumulative reward certification for convenience, i.e.,
the transition function is P : S × A → S and the initial state is a fixed s0 ∈ S. The certification goal,
as listed in Definition 6, is to obtain a lower bound of cumulative reward under poisoning attacks,
given bounded poisoning size K . The cumulative reward certification is based on a novel adaptive
search algorithm COPA-Search inspired from Wu et al. (2022); we tailor the algorithm to certify
against poisoning attacks. We defer detailed discussions and complexity analysis to Appendix E.3.
6
Published as a conference paper at ICLR 2022
COPA-Search Algorithm Description. The pseudocode is in Algorithm 5 (Appendix E.3).
The method starts from the base case: when the poisoning threshold Kcur = 0, the lower bound
of cumulative reward JKcur is exactly the reward without poisoning. The method then gradually
increases the poisoning threshold Kcur, by finding the immediate larger K0 > Kcur that can expand
the possible action set along the trajectory. With the increase of Kcur J K0, the attack may cause
the poisoned policy ∏ to take different actions at some states, thus resulting in new trajectories. We
need to figure out a set of all possible actions to exhaustively traverse all possible trajectories. We
will introduce theorems to compute this set of possible actions. With this set, the method effectively
explores these new trajectories by formulating them as expanded branches ofa trajectory tree. Once
all new trajectories are explored, the method examines all leaf nodes of the tree and figures out the
minimum reward among them, which is the new lower bound of cumulative reward JK under new
poisoning size K0. We then repeat this process of increasing poisoning size from K0 and expanding
with new trajectories until we reach a predefined threshold for poisoning size K.
Definition 5 (Possible Action Set). Given previous states s0:t, the subpolicies {πi}iu=-01, the aggre-
gation protocol (PARL, TPARL, or DPARL), and the poisoning size K, the possible action set A
at step t is a subset of action space: A ⊆ A, such that for any poisoned policy πe, as long as the
poisoning size is within K, the chosen action at step t will always be in A, i.e., at = e(so：t) ∈ A.
Possible Action Set for PARL. The following theorem gives the possible action set for PARL.
Theorem 4 (Tight PARL Action Set). Under the condition of Definition 5, suppose the aggregation
protocol is PARL as defined in Definition 7, then the possible action set at step t
)
AT (K) = a ∈ A X max{na0 (st) - na(st) - K + 1[a0 < a], 0} ≤ K
a0∈A
(8)
We defer the proof to Appendix F.6.1. Furthermore, in Appendix F.6.2 we show that: 1) The theo-
rem gives theoretically tight possible action set; 2) In contrast, directly extending PARL’s per-state
certification gives loose certification.
Possible Action Set for TPARL. The following theorem gives the possible action set for TPARL.
Theorem 5. Under the condition of Definition 5, suppose the aggregation protocol is TPARL as
defined in Definition 8, then the possible action set at step t
A(K)
where h(ai0),a
K
a∈AXh(ai0),
i=1
> δa0,a, ∀a0 6= a ,
(9)
and δa0,a follow the definition in Theorem 1.
We defer the proof to Appendix F.7. The possible action set here is no longer theoretically tight.
Indeed, the problem of computing a possible action set with minimum cardinality for TPARL is
NP-complete as we shown in the following theorem (proved in Appendix F.8), where we reduce
computing theoretically tight possible action set to the set cover problem (Karp, 1972). This result
can be viewed as the hardness of targeted attack. In other words, the optimal untargeted attack
on TPARL can be found in polynomial time, while the optimal targeted attack on TPARL is NP-
complete, which indicates the robustness property of proposed TPARL.
Theorem 6. Under the condition of Definition 5, suppose we use TPARL (Definition 8) as the ag-
gregation protocol, then computing a possible action set A(K) such that any possible action set S
satisfies |A(K)| ≤ |S| is NP-complete.
Possible Action Set for DPARL. The following theorem gives the possible action set for DPARL.
Theorem 7. Under the condition of Definition 5, suppose the aggregation protocol is TPARL as
defined in Definition 3, then the possible action set at step t
A(K)= {at} ∪ 卜 ∈A∣ 1≤W .VWmaχ,t+ι} LXWO ≤ K∖ ∪ (a ∈AI xx % > δa0,a, ∀a0 = a)	(Io)
(	W *=W 0,a00 = at，	i=1	J
where at = πD (smax{t-Wmax+1,0}:t) is the clean policy’s chosen action, W0 is defined by Equa-
tion (1), LaW0,.a,0W0 0 is defined by Definition 4 with a being replaced by at, and h(ai0),a, δa0,a is defined in
Theorem 1 with W replaced by W0.
We prove the theorem in Appendix F.8. As summarized in Table 1, we further prove the tightness or
hardness of certification for PARL and TPARL, while for DPARL it is an interesting open problem
on whether theoretical tight certification is possible in polynomial time.
7
Published as a conference paper at ICLR 2022
oitar ytilibats oitar ytilibats oitar ytilibat
NQD NQD-RQ 15C
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
PARL	TPARL (LV= 4)	DPARL(WmaX = 5)
≥ K	≥ K	≥ K
Figure 1: Robustness certification for per-state action stability. We plot the cumulative histogram of the
tolerable poisoning size K for all time steps. We provide the certification for different aggregation protocols
(PARL, TPARL, DPARL) on three environments and #partitions u = 50. The results are averaged over 20 runs
with the vertical bar on top denoting the standard deviation.
5 Experiments
In this section, we present the evaluation for our COPA framework, specifically, the aggregation
protocols (Section 4.2) and the certification methods under different certification criteria (Sec-
tions 4.3 and 4.4). We defer the description of the offline RL algorithms (DQN (Mnih et al., 2013),
QR-DQN (Dabney et al., 2018), and C51 (Bellemare et al., 2017)) used for training the subpolicies
to Appendix G.1, and the concrete experimental procedures to Appendix G.2. As a summary, we
obtain similar conclusions from per-state action certification and reward certification: 1) QR-DQN
and C51 are oftentimes more certifiably robust than DQN; 2) temporal aggregation (TPARL
and DPARL) achieves higher certification for environments satisfying temporal continuity, e.g.,
Freeway; 3) larger partition number improves the certified robustness; 4) Freeway is the most stable
and robust environment among the three. More interesting discussions are deferred to Appendix H.
5.1	Evaluation of Robustness Certification for Per-State Action Stability
We provide the robustness certification for per-state action stability based on Section 4.3.
Experimental Setup and Metrics. We evaluate the aggregated policies πP, πT, and πD following
Section 4.2. Basically, in each run, we run one trajectory (of maximum length H) using the derived
policy, and compute Kt at each time step t. Given {Kt}H=-1, we obtain a cumulative histogram—
for each threshold K, we count the time steps that achieve a threshold no smaller than it and then
normalize, i.e., PH=-11[Kt≥K]∕H. We call this quantity stability ratio since it reflects the per-state
action stability w.r.t. given poisoning thresholds. We also compute an average tolerable poisoning
thresholds for a trajectory, defined as PH=-I k±∕h. More details are deferred to Appendix G.2.
Evaluation Results. We present the comparison of per-state action certification for different RL
methods and certification methods in Figure 1. We plot partial poisoning thresholds on the x-axes
here, and omit full results in Appendix H.6, where we also report the average tolerable poisoning
thresholds. We additionally report benign empirical reward and the comparisons with standard
training in Appendices H.1 and H.2, as well as more analytical statistics in Appendices H.3 and H.4.
The cumulative histograms in Figure 1 can be compared in different levels. Basically, we compare
the stability ratio at each tolerable poisoning thresholds K——higher ratio at larger poisoning size
indicates stronger certified robustness. On the RL algorithm level, QR-DQN and C51 consistently
outperform the baseline DQN, and C51 has a substantial advantage particularly in Highway. On the
aggregation protocol level, we observe different behaviors in different environments. On Freeway,
methods with temporal aggregation (TPARL and DPARL) achieve higher robustness, and DPARL
achieves the highest certified robustness in most cases; while on Breakout and Highway, the single-
step aggregation PARL is oftentimes better. This difference is due to the different properties of
environments. Our temporal aggregation is developed based on the assumption of consistent action
8
Published as a conference paper at ICLR 2022
------⅛------ U = 30, PARL ..............<....... U	= 30, TPARL (IV = 4)	u = 30, DPARL (IVrnax = 5)
+	U = 50, PARL ..............*....... U	= 50, TPARL (IV = 4)	---*---------u = 50, DPARL (IVrnax = 5)
Breakout, H = 50
3
3
Freeway, H = 200
3
Freeway, H = 400
5
4
3
2
2
0
2
0
LO 15 20 25
2
10 15 20 25
5
4
0
LO 15 20 25
Poisoning size K
Poisoning size K
Poisoning size K
Figure 2: Robustness certification for cumulative reward. We plot the lower bound of cumulative reward
bound Jκ w.r.t. poisoning size K under three aggregation protocols (PARL, TPARL (W = 4), DPARL
(Wmax = 5)) with two #partitions u, evaluated on three environments with different horizon lengths H .
Breakout, H = 75
0
Poisoning size K
Highway, H = 30
25
2Q
15
IQ
5
25
2Q
15
IQ
5
25
20
15
IQ
5
Poisoning size K
selection in adjacent time steps. This assumption is true in Freeway while violated in Breakout and
Highway. A more detailed explanation of environment properties is omitted to Appendix H.5. On
the partition number level, a larger partition number generally allows larger tolerable poisoning
thresholds as shown in Appendix H.6. Finally, on the RL environment level, Freeway achieves
much higher certified robustness for per-state action stability than Highway, followed by Breakout,
implying that Freeway is an environment that accommodates more stable and robust policies.
5.2	Evaluation of Robustness Certification for Cumulative Reward B ound
We provide the robustness certification for cumulative reward bound according to Section 4.4.
Experimental Setup and Metrics. We evaluate the aggregated policies πP , πT, and πD follow-
ing Theorem 4, Theorem 5 and Theorem 7. We compute the lower bounds of the cumulative reward
Jκ w.r.t. the poisoning size K using the COPA-SEARCH algorithm introduced in Section 4.4. We
provide details of the evaluated trajectory length along with the rationales in Appendix G.2.
Evaluation Results. We present the comparison of reward certification for different RL algorithms
and certification methods in Figure 2. Essentially, at each poisoning size K, we compare the lower
bound of cumulative reward achieved by different RL algorithms and certification methods—higher
value of the lower bound implies stronger certified robustness. On the RL algorithm level, QR-
DQN and C51 almost invariably outperform the baseline DQN algorithm. On the aggregation
protocol level, methods with temporal aggregation consistently surpass the single-step aggregation
PARL on Freeway but not the other two, as analyzed in Section 5.1. In addition, we note that DPARL
is sometimes not as robust as TPARL. We hypothesize two reasons: 1) the dynamic mechanism is
more susceptible to the attack, e.g., the selected optimal window size is prone to be manipulated;
2) the lower bound is looser for DPARL given the difficulty of computing the possible action set
in DPARL (discussed in Theorem 7). On the partition number level, a larger partition number
(u = 50) demonstrates higher robustness. On the horizon length level, the robustness ranking of
different policies is similar under different horizon lengths with slight differences, corresponding
to the property of finite-horizon RL. On the RL environment level, Freeway can tolerate a larger
poisoning size than Breakout and Highway. More results and discussions are in Appendix H.7.
6 Conclusions
In this paper, we proposed COPA, the first framework for certifying robust policies for offline RL
against poisoning attacks. COPA includes three policy aggregation protocols. For each aggregation
protocol, COPA provides a sound certification for both per-state action stability and cumulative
reward bound. Experimental evaluations on different environments and different offline RL training
algorithms show the effectiveness of our robustness certification in a wide range of scenarios.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR, Alfred P.
Sloan Fellowship, the U.S. Department of Energy by the Lawrence Livermore National Laboratory
under Contract No. DE-AC52-07NA27344 and LLNL LDRD Program Project No. 20-ER-014, and
Amazon Research Award.
Ethics Statement. In this paper, we prove the first framework for certifying robust policies for
offline RL against poisoning attacks. On the one hand, such framework provides rigorous guaran-
tees in practice RL tasks and thus significantly alleviates the security vulnerabilities of offline RL
algorithms against training-phase poisoning attacks. The evaluation of different RL algorithms also
provides a better understanding about the different degrees of security vulnerabilities across differ-
ent RL algorithms. On the other hand, the robustness guarantee provided by our framework only
holds under specific conditions of the attack. Specifically, we require the attack to change only a
bounded number of training trajectories. Therefore, users should be aware of such limitations of
our framework, and should not blindly trust the robustness guarantee when the attack can change a
large number of training instances or modify the training algorithm itself. As a result, we encourage
researchers to understand the potential risks, and evaluate whether our constraints on the attack align
with their usage scenarios when applying our COPA to real-world applications. We do not expect
any ethics issues raised by our work.
Reproducibility Statement. All theorem statements are substantiated with rigorous proofs in Ap-
pendix F. In Appendix E, we list the pseudocode for all key algorithms. Our experimental eval-
uation is conducted with publicly available OpenAI Gym toolkit (Brockman et al., 2016). We
introduce all experimental details in both Section 5 and Appendix G. Specifically, we build the
code upon the open-source code base of Agarwal et al. (2020), and we upload the source code at
https://github.com/AI-secure/COPA for reproducibility purpose.
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, 2020.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019.
Kiarash Banihashem, Adish Singla, and Goran Radanovic. Defense against reward poisoning attacks
in reinforcement learning. arXiv preprint arXiv:2102.05776, 2021.
Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it
stronger. arXiv preprint arXiv:1712.09344, 2017.
Vahid Behzadan and Arslan Munir. Mitigation of policy manipulation attacks on deep q-networks
with parameter-space noise. In International Conference on Computer Safety, Reliability, and
Security,pp. 406-417. Springer, 2018.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449-458. PMLR, 2017.
Richard Bellman. Dynamic programming. Science, 153(3731):34-37, 1966.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Henry Chacon, Samuel Silva, and Paul Rad. Deep learning poison data attack detection. In 2019
IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI), pp. 971-978.
IEEE, 2019.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
10
Published as a conference paper at ICLR 2022
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and trends in Robotics, 2(1-2):388-403, 2013.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high dimensions without the computational intractability. In 57th Annual
Symposium on Foundations of Computer Science, pp. 655-664. Institute of Electrical and Elec-
tronics Engineers (IEEE), 2016.
Michael Everett, Bjorn Lutjens, and Jonathan P How. Certifiable robustness to adversarial state
uncertainty in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning
Systems, 2021.
Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
IEEE Computer Society. Standards Committee. Working group of the Microprocessor Stan-
dards Subcommittee and American National Standards Institute. IEEE standard for binary
floating-point arithmetic, volume 754. IEEE, 1985.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-
sarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15262-15271, 2021.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipula-
tions on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217-237. Springer, 2019.
Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of nearest neighbors
against data poisoning attacks. arXiv preprint arXiv:2012.03765, 2020.
Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer compu-
tations, pp. 85-103. Springer, 1972.
Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor
attacks on deep reinforcement learning. In 2020 57th ACM/IEEE Design Automation Conference
(DAC), pp. 1-6. IEEE, 2020.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint
arXiv:1705.06452, 2017.
Ram Shankar Siva Kumar, Magnus Nystrom, John Lambert, Andrew Marshall, Mario Goertzel,
Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry per-
spectives. In 2020 IEEE Security and Privacy Workshops (SPW), pp. 69-75. IEEE, 2020.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
11
Published as a conference paper at ICLR 2022
Robert Legenstein, Niko Wilbert, and Laurenz Wiskott. Reinforcement learning on slow features of
high-dimensional input streams. PLoS computational biology, 6(8):e1000894, 2010.
Edouard Leurent. An environment for autonomous driving decision-making. https://github.
com/eleurent/highway-env, 2018.
Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against general
poisoning attacks. In International Conference on Learning Representations, 2020.
Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against gen-
eral poisoning attacks. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=YUGG2tFuPM.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses,pp. 273-294. Springer, 2018.
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Xiaojin Zhu. Policy poisoning in batch reinforcement
learning and control. Advances in Neural Information Processing Systems, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3578-3586.
PMLR, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein,
Udam Saini, Charles Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert
your spam filter. LEET, 8:1-9, 2008.
Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Ro-
bust deep reinforcement learning with adversarial attacks. In 17th International Conference on
Autonomous Agents and Multiagent Systems, AAMAS 2018, pp. 2040-2042. International Foun-
dation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2018.
Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, and
John P Dickerson. Deep k-nn defense against clean-label data poisoning attacks. In European
Conference on Computer Vision, pp. 55-70. Springer, 2020.
Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning:
Applications on robotics. Journal of Intelligent & Robotic Systems, 86(2):153-173, 2017.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching
via environment poisoning: Training-time adversarial attacks against reinforcement learning. In
International Conference on Machine Learning, pp. 7974-7984. PMLR, 2020.
Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement
learning framework for autonomous driving. Electronic Imaging, 2017(19):70-76, 2017.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. In Pro-
ceedings of the 33rd International Conference on Neural Information Processing Systems, pp.
11292-11303, 2019.
12
Published as a conference paper at ICLR 2022
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just
how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In
International Conference on Machine Learning, pp. 9389-9398. PMLR, 2021.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
3520-3532, 2017.
Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online
{rl} with unknown dynamics. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=9r30XCjf5Dt.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23-30.
IEEE, 2017.
Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
learning. In Conference on Robot Learning, pp. 1439-1456. PMLR, 2020.
Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Back-
door attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021.
Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, and Saif Eddin Jabari. Stop-and-go: Ex-
ploring backdoor attacks on deep reinforcement learning-based traffic congestion control systems.
arXiv preprint arXiv:2003.07859, 2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In
International Conference on Machine Learning, pp. 5276-5285. PMLR, 2018.
Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, and Bo Li. Crop: Certify-
ing robust policies for reinforcement learning through functional smoothing. In International
Conference on Learning Representations, 2022.
Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games
with limited data. Advances in Neural Information Processing Systems, 34, 2021.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh. Robust deep reinforcement learning against adversarial perturbations on state obser-
vations. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21024-21037. Curran Asso-
ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
f0eb6568ea114ba6e293f903c34d7488- Paper.pdf.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, pp. 11225-
11234. PMLR, 2020b.
Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust offline reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021.
13
Published as a conference paper at ICLR 2022
A Omitted Definitions
A. 1 Cumulative Reward
Definition 6 (Cumulative Reward). Let P : S × A → P(S) be the transition function with P(∙)
defining the set of probability measures. Let R, d0 , H be the reward function, initial state distribu-
tion, and time horizon. We denote J(π) as the cumulative reward of the given policy π:
H-1
J(∏) := EE R(st,at), where at = ∏(st),st+ι 〜P(st,at),so 〜do,	(11)
t=0
It is natural to adapt this definition when there is a discount factor. If policy π considers recent nt
states instead of only St to make action predictions, we can change at = ∏(st) to at = n(st-nt+i：t)
in Equation (11).
A.2 PARL (Per-State Partition Aggregation) Protocol
Definition 7 (Per-State Partition Aggregation). Given subpolicies {πi}iu=-01, the Per-State Partition
Aggregation (PARL) protocol defines an aggregated policy πP : S → A such that
πP(s) := argmaxna(s),	(12)
a∈A
where na(s) is defined in Section 3.
As mentioned in Section 4, when the arg max in Equation (12) returns multiple elements, we break
ties deterministically by returning the “smaller” (<) action, which can be defined by numerical
order, lexicographical order, etc.
A.3 TPARL (Temporal Partition Aggregation) Protocol
Definition 8 (Temporal Partition Aggregation). Given subpolicies {πi}iu=-01 and window size W,
at time step t, the Temporal Partition Aggregation (TPARL) defines an aggregated policy πT :
S min{t+1,W} → A such that
πT (smax{t-W +1,0}:t) = arg max na (smax{t-W +1,0}:t),	(13)
a∈A
where sl:r and na are defined in Section 3.
In the above definition, at time step t, the input of policy πT contains all states between time step
max{t - W + 1, 0} and t within window size W; and the output is the policy prediction with the
highest votes across these states. Recall that in Section 3, we define na(sl:r) = Pjr=l na (sj ). We
break ties in the same way as in PARL.
B	Illustration of COPA Aggregation Protocols
We present a concrete example to demonstrate how different aggregation protocols induce different
tolerable poisoning thresholds, illustrate bottleneck and non-bottleneck states, and provide addi-
tional discussions.
Suppose the action space contains two actions A = {a1, a2}, and there are six subpolicies {πi}iu=-01
where u = 6. We are at time step t = 7 now. When there is no poisoning, the subpolicies’ action
predictions for state s0 to s7 are shown by Table 2. After aggregation, all aggregated policies will
choose action a1 at t = 7.
PARL (Definition 7). The PARL aggregation protocol only uses the current state s7 to aggregate
the votes. On s7, three subpolicies choose action aι and three others choose action a2. By breaking
the tie with aι < a2, the PARL policy ∏p(s7) = aι. The tolerable poisoning threshold Kt = 0,
because one action flipping from a1 to a2 by poisoning only one subpolicy can change the aggregated
policy to a2 .
TPARL (Definition 8). The TPARL aggregation protocol uses window size W = 7. This implies
that, we aggregate all states s1:7 to count the votes and decide the action. Since na1 (s1:7) = 36 and
14
Published as a conference paper at ICLR 2022
Table 2:	A concrete example of action predictions, where “1” means action a1 and “2” means action a2 . When
there is no poisoning attack, the corresponding time window spans by PARL, TPARL, and DPARL are shown
by green ,[blue , and pink respectively. All aggregated policies choose action aι, but have different tolerable
poisoning thresholds as shown in the last column.
Action Prediction for
s0 s1	s2	s3	s4	s5 s6	s7
K at s7
π0
π1
π2
π3
π4
π5
PARL (πP, Definition 7)
TPARL with W = 7 (πT , Definition 8)
DPARL with Wmax = 8 (πD , Definition 3)
1111
1111
1111
1111
1111
1112
1111
1111
1111
1112
1112
1222
1
1
1
0
2
1
na2 (s1:7) = 6, after poisoning two subpolicies, we still nea1 (s1:7) ≥ nea2 (s1:7). Thus, we achieve an
tolerable poisoning threshold Kt = 2.
Compared with PARL, the temporal aggregation in TPARL increases the vote margin between a1
and a2 and thus improve the tolerable poisoning threshold at current state.
DPARL (Definition 3). The DPARL aggregation protocol chooses the window size W0 = 8 since
this window size gives the largest average vote margin ∆tW0 (defined in Equation (1)). We can easily
find out that with poisoning size K = 1, we will still choose W0 = 8 as the window size and the
resulting votes for a1 can be kept higher than votes for a2. However, when it comes to K = 2, ifwe
totally flip subpolicies π0 and π1 to let them always predict action a2, then the DPARL aggregated
policy will turn to choose W0 = 1 as the window size and then choose action a? as the action output.
Thus, We achieve a tolerable poisoning threshold Kt = 1 this time.
Illustrations of Bottleneck and Non-Bottleneck States. According to our illustration in Sec-
tion 4.2, bottleneck states are those where subpolicies vote for diverse actions but the best action
is the same as previous states, and non-bottleneck states are those where subpolicies mostly vote
for the same action. As we can see, s0 to s6 are non-bottleneck states since the subpolicies almost
unanimously vote for one action. In contrast, s7 is a bottleneck state.
From the perspectives of different aggregation protocols, we observe that for the bottleneck state
s7, both TPARL and DPARL exploit temporal aggregation to boost the certified robust poisoning
threshold (from 0 to 2 and from 0 to 1 respectively), thus demonstrating the effectiveness of TPARL
and DPARL on improving certified robustness. On the other hand, for non-bottleneck states s0 to
s6, we can easily see that different aggregation protocols do not differ much in terms of provided
certified robustness levels.
Explanations for Bottleneck and Non-Bottleneck States. We provide additional discussions
regarding why bottleneck states are vulnerable and why our temporal aggregation strategy can ef-
fectively alleviate the problem. We first explain the existence of the bottleneck states. Given the
property of the bottleneck states that there is high disagreement among different subpolicies on such
states, they may lie close to the decision boundary. This may be a result of poisoning, or simply due
to the intrinsic difficulty of the state. On the other hand, such states naturally exist in the rollouts
(like natural adversarial examples (Hendrycks et al., 2021)), and may induce high instability of the
rollouts during testing. We next discuss additional intuitions for our temporal aggregation. Essen-
tially, temporal aggregation effectively leverages the adjacent non-bottleneck states to rectify the
decisions at the bottleneck states, based on the assumption of temporal continuity which has been
revealed and utilized in several previous works (Legenstein et al., 2010; Veerapaneni et al., 2020; Ye
et al., 2021).
15
Published as a conference paper at ICLR 2022
C Certification of Per-State Action Stability for PARL
We certify the robustness of PARL following Theorem 8.
Theorem 8. Let D be the clean training dataset; let πi = M0 (Di), 0 ≤ i ≤ u - 1 be the learned
subpolicies according to Section 4.1 from which we define na (see Section 3); and let πP be the Per-
State Partition Aggregation policy: πP = M(D) where M abstracts the whole training-aggregation
process. D is a poisoned dataset and πfP is the poisoned policy: πfP = M(D).
For a given state st encountered at time step t during test time, let a := πP(st), then
K t
na(st) - maxa0=a (naθ(st) + 1[a0 < a])
2
(14)
Remark. The theorem provides a valid per-state action certification for PARL, since according to
Definition 1, as long as the poisoning Size is smaller or equal to Kt, i.e., |D ㊀ D| ≤ Kt, the action
of the poisoned policy is the same as the clean policy: f (st) = ∏p(St) = a. To compute Kt,
according to Equation (14), we rely on the aggregated action count na for state st from subpolicies.
The a0 < a is the “smaller-than” operator introduced following Definition 7.
Proof of Theorem 8. Suppose the poisoning size is within K, then after poisoning, the aggregated
action count nea(st) ∈ [na(st) - K, na(st) +K] where na(st) is such count before poisoning. Thus,
to ensure the chosen action does not change, a necessary condition is that nea(st) ≥ nea0 (st) + 1[a0 <
a] for any other a0 < a. Solving K yields Equation (14).	□
We use the theorem to compute the per-state action certification for each time step t along the tra-
jectory, and the detailed algorithm is in Algorithm 2 (Appendix E). Furthermore, we prove that this
certification is theoretically tight as the following proposition shows. The proof is in Appendix F.1.
Proposition 9. Under the same condition as Theorem 8, for any time step t, there exists anRL learn-
ing algorithm Mo, and a poisoned dataset D, such that ∖ D ㊀ D| = K t + 1, and ∏p(st) = ∏p (St).
D A Table of Possible Action Set
Table 3:	Expressions of possible action set A(K) given poisoning threshold K for different aggregation
protocols. Full theorem statements are in Theorems 4, 5 and 7 (in Section 4.4).
Protocol	A(K)
PARL	n« ∈ A I Pa0∈A max{nao(st) - na(st) - K + 1[a0 < a], 0} ≤ k0
TPARL	{a ∈A∣PK=1 闺,a >δa0,a,∀a0 = a}
DPARL
{at} ∪ ∕α0 ∈ A ∣	min	LW：0WO ≤ K) ∪ Ia ∈ A ∣ X h(i?a > 6.0&,∀a0 = a]
I	∣ 1≤W*≤min{Wmaχ,t+1},W* = W0,a00=at a ,a	J ∣	∣ W a ,a	,	J
For all three aggregation protocols, we summarize the expressions used for computing the possible
action set A(K) given tolerable poisoning threshold K in Table 3.
E	Algorithm Pseudocode and Discussion
E.1 COPA Training Protocol
Algorithm 1: COPA training protocol.
Input: training dataset D, number of partitions k, deterministic hash function h
Output: COPA subpolicies {πi }ik=-0* 1 2 3 4 5
1 for i ∈ [k] do
2 LDi 一 {τ ∈ D | h(τ) ≡ i (mod k)}	. Separate the training data D into k partitions
3 for each partition Di do
4 Lni -Mo(Di)	. Subpolicy trained on partition Di with offline RL algorithm M0
5 return {πi}ik=-01
16
Published as a conference paper at ICLR 2022
E.2 COPA Per-State Action Certification
Per-State Partition Aggregation (PARL).
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Algorithm 2: COPA per-state certification algorithm for Per-State Partition Aggrega-
tion (PARL).
Input: environment E = (S, A, R, P, H, d0), subpolicies {πi }ik=-0* 1
Output: COPA robust size at each time step {Kt}H=-o1
S0 〜do	. sample initial state
for t from 0 to H - 1 do
for each a ∈ A do
L Compute na(st) from subpolicies, {∏i}k-o1 decisions	. na is defined in Section 3
Determine the chosen action at — ∏p(st) according to PARL (Definition 7)
Compute Kt according to Equation (14) in Theorem 8
_ st+ι 〜P(st,at)
return {Kt}H=-1
Temporal Partition Aggregation (TPARL).
Algorithm 3: COPA per-state certification algorithm for Temporal Partition Aggrega-
tion (TPARL).
Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}ik=-01, window size W
Output: COPA robust size at each time step {Kt}H=-
S0 〜do	. sample initial state
for t from 0 to H - 1 do
for each a ∈ A do
L Compute na(st) from subpolicies, {∏i}k-o1 decisions	. na is defined in Section 3
Determine the chosen action at J ∏τ(st) according to TPARL (Definition 8)
Pmin J ∞
for each a0 ∈ A, a0 6= at do
for i from 0 to k - 1 do
L Compute 肌用七⑹ according to Equation (4)
{h(ait),a0}ik=1 J sorted({hi,at,a0}ik=-01,reverse = True)
Compute δat ,a0
sum J 0, p J 0
for j from 1 to k do
ifsum+ h(ajt),a0 > δat,a0 then
P J j - 1
break
p J j, sum J sum + hjL
Pmin J min{Pmin,p}
Kt J Pmin
_ st+1 〜P(st,at)
return {Kt}H=0
17
Published as a conference paper at ICLR 2022
Dynamic Temporal Partition Aggregation (DPARL).
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
Algorithm 4: COPA per-state certification algorithm for Dynamic Temporal Partition Aggrega-
tion (DPARL).
Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}ik=-01, maximum window size Wmax
Output: COPA robust size at each time step {Kt}H=-o1
S0 〜do	. sample initial state
for t from 0 to H - 1 do
for each a ∈ A do
[ Compute na(st) from subpolicies, {∏i}k-01 decisions given st . na is defined in Section 3
Determine the chosen action at — ∏d (St) and chosen window size W0 according to
DPARL (Definition 3)
Pmin 一 ∞
K t — Algorithm 3	.use Algorithm 3 with W replaced by W0 to compute Kt
K D — K t
for each a0 ∈ A, a0 6= at do
for each a00 ∈ A, a00 6= at do
for W * from 1 to min{Wmaχ,t +1} do
a# J argmaχao=a0,a0∈A na° (st-W*+1M)
for w from 1 to max{W 0 , W * } do
L Compute maXa°∈A σw (ao) according to Equation (7)
for i from 0 to k - 1 do
for w from 1 to maχ{W 0 , W * } do
L Compute σw (∏i(st-w)) according to Equation (7)
Compute gi according to Equation (7)
{g(i)}ik=1
J sorted({gi}ik=-01, reverse = True)
Compute naW0 *, naW#*, naWt 0, naW000
tmp J W 0 (naW0 - naW# ) - W* (naWt - naW00 ) - 1[a0 > at]
sum J 0, p J 0
for j from 1 to k do
if sum + tmp ≥ 0 then
I p — j -1
break
p J j, sum J sum + g(j )
WV*,W0 ,	DD ,	.「斤D WV*,W0]
La0,a00	— p,K t — min{Kt ,La0,a00 }
_ St + 1 〜P(st,at)
return {KD }H=-1
E.3 COPA Cumulative Reward Certification
COPA-SEARCH alternately executes the procedure of trajectory exploration and expansion and
poisoning threshold growth. In trajectory exploration and expansion, COPA-SEARCH organizes all
possible trajectories in the form of a search tree and progressively grows it. For each node (represent-
ing a state), we leverage Theorems 4, 5 and 7 to compute the Possible Action Set. We then expand
the tree branches corresponding to the actions in the derived set. In poisoning threshold growth,
when all trajectories for the current poisoning threshold are explored, we increase K0 to seek certi-
fication under larger poisoning sizes, via maintaining a priority queue of all poisoning sizes during
the expansion of the tree. The iterative procedures end when the priority queue becomes empty.
18
Published as a conference paper at ICLR 2022
The highlighted line in the following algorithm needs to inject different algorithms based on the
aggregation protocol: for PARL (πP), use Theorem 4; for TPARL (πT), use Theorem 5; for
DPARL (πD), use Theorem 7.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
Algorithm 5: COPA-SEARCH: adaptive tree search for cumulative reward certification.
Input: environment E = (S, A, R, P, H, d0), subpolicies {πi}ik=-01, aggregated policy πP or πT (with
W) orπT (with Wmax)
Output: a map M that maps poisoning size K to corresponding certified lower bound of cumulative
reward J K
. Initialize global variables
p_que 4- 0	. initialize an empty priority queue containing tuples of
(state historys0:t , action a, poisoning size K, reward J) sorted by increasing K
Jglobal 4	∞	. initialize global minimum reward
Function GE TAC T I ONS (s0:t, Klim, Jcur):
A — PossibIeActionS(So：t, {∏i}k=11,∏* ,Klim)	. compute the possible action set given
state history, subpolicies, aggregation policy π*, and poisoning size according to
theorems in Section 4.4
a_list — 0
for each action a ∈ A do
if P (s, a) = ⊥ then
continue . game terminate here, no possible larger or lower cumulative reward
to search
a_list — a_list ∪ {a}	. record possible actions to expand
if A = A then
K J mnpossibleActions(S0：t ,{∏i }k=-01 ,π* ,K) = A K
AneW J PossibIeActionS(So：t, {∏i}k=-∣1 ,∏≠ ,K0) \ A . compute the immediate actions that
will possibly be chosen if enlarging the poisoning size
for each action a ∈ Anew do
p_que.pUsh((S0：t,a,K0, Jcur))
return a_list
Procedure E X P AN D(s0：t , Klim , Jcur ):
if Jcur ≥ Jglobal ∧ (step reward is non-negative) then
L return
a-list J GETACTIONS (S0：t, {∏i}k =C1,∏* ,Klim)
Section 4.4
if a-list = 0 then
Jglobal j- min{ Jglobal , Jcur }
return
for a ∈ a_list do
st+1 J P(st, a)
if st+1 = ⊥ then
I	Jglobal j- min{ Jglobal , Jcur }
else
L Expand (so：t+i,Klim,Jcur + R(st,a))
. pruning
. compute according to theorems in
MJ0
sC J dC
E X P A N D (sC , Klim = 0, Jcur = 0)
while True do
if p_que = 0 then
L break
(so：t,a, K, J) J p_que.pop()
(-,-,K0, -) J p-que.top()
M(K) J Jglobal
st+1 J P(st, a)
_ Expand (so：t+i,K0, J + R(st,a))
. initial state is s0
. expand initial trajectory
. no state to expand
. pop out the first element
. examine the next element
obtain one new point of certification result
. expand the tree from the new node
return M
indeed the algorithm can terminate at any time within the while loop
19
Published as a conference paper at ICLR 2022
Time Complexity. The complexity of COPA-SEARCH is O(H|Sexplored|(log |Sexplored| +
|A|T)), where |Sexplored| is the number of explored states throughout the search procedure, which is
no larger than cardinality of state set S , H is the horizon length, |A| is the cardinality of action set,
and T is the time complexity of per-state action certification. The main bottleneck of the algorithm
is the large number of possible states, which is in the worse case exponential to state dimension.
However, to provide a deterministic worst-case certification agnostic to environment properties, ex-
ploring all possible states may be inevitable.
Relation with Wu et al. (2022). The COPA-SEARCH algorithm is inspired from Wu et al.
(2022, Algorithm 3), which also leverages tree search to explore all possible states and thus derive
the robustness guarantee. However, the major distinction is that their algorithm tries to derive a
probabilistic guarantee of RL robustness against state perturbations, while COPA-Search derives
deterministic guarantee ofRL robustness against poisoning attacks. We think this general tree search
methodology can be further extended to provide certification beyond evasion attack (Wu et al., 2022)
and poisoning attack (our work) and we leave it as future work.
Extension to stochastic MDPs. In the current version of COPA-SEARCH, the exhaustive search
is enabled by the deterministic MDP assumption. However, we foresee the potential of conveniently
extending COPA-Search to stochastic MDPs and will discuss one concrete method below. In con-
trast to interacting with the environment for one time to obtain the deterministic next state transition
in the deterministic MDP case, in the case of stochastic MDPs, we can leverage sampling (i.e., by
repeatedly taking the same action at the same state) to obtain the set of high probability next state
transitions with high confidence. In this way, our COPA-SEARCH will be able to yield a proba-
bilistic bound, in comparison with the deterministic bound achieved in this paper enabled by the
deterministic MDP assumption.
F	Proofs
F.1 Tightness of Per- S tate Action Certification in PARL
Proof of Proposition 9. We prove by construction. Given the state st, we first locate the subpolicies
whose chosen action is a, and denote the set of them as
B = {i * * * ∈ [u] | πi(st) = a = πP(st)}.
Wealso denote a0 = argmax。,=。n。，(st)+1[a0 < a]. According to Kt,s definition (Equation (14)),
|B| = na(st) > na(st)∕2 ≥ Kt.
We now pick an arbitrary subset B0 ⊆ B such that |B0| = Kt + 1. For each i ∈ B0, We locate its
corresponding parititioned dataset Di for training subpolicy πi . We insert one point pi to Di , such
that our chosen learning algorithm Mo can train a subpolicy ∏ = Mo(Di ∪ {pi}) that satisfies
∏i(st) = a0. For example, the point could be Pi = {(st, a0, s0, ∞)} and Mo learns the action with
maximum reward for the memorized nearest state, where s0 can be adjusted to make sure the point
is hashed to parition i.1 Then, we construct the poisoned dataset
De =	[ Di ∪ {pi} ∪	[	Di	.
∖i∈Β0	i	∖i∈[u]∖B0	)
Therefore, we have D ㊀ D = ∪i∈B,Pi = ∣B0∣ = Kt + 1.
On this poisoned dataset, we train subpolicies {∏i}U-1 and get the aggregated policy πf. To study
πfP (st), we compute the aggregated action count ne。 on these poisoned subpolicies. We found that
1Strictly speaking, we need to choose a determinisitc hash function h for dataset partitioning such that
adjustment on s0 and reward (currently ∞, but can be an arbitrary large enough number) can make the point
being partitioned to i, i.e., h(pi) ≡ i (mod u). Since our adjustment space is infinite due to infinite number of
large enough reward, such assumption can be easily achieved. Same applies to other attack constructions in the
following proofs.
20
Published as a conference paper at ICLR 2022
ea(st) = na(st) - |B01 and 兀，(St)="『(St) + |B0|. Therefore,
ea(st) - ea0 (st) = na(st) - n (st) - 2(Kt + 1)
=na(st) -na0 (st) - 2 (jna(St)- na0(2t)- 1[a0 <a] k +1)
< na(st) - na0 (st) - (na(st) - na0(st) - 1[a0 < a]) = 1[a0 < a].
Therefore, if a0 < a, nea (st) ≤ nea0 (st), and a0 has higher priority to be chosen than a; if a0 > a,
nea(st) ≤ nea0 (st) - 1, and a0 still has higher priority to be chosen than a. Hence, πfP(st) 6= a =
∏p(st). To this point, We conclude the proof with a feasible construction.	□
F.2 Per-State Action Certification in TPARL
Proof of Theorem 1. For ease of notation, we let w = min{W, t+ 1} so w is the actual window size
used at step t. We let t0 = t - w + 1, i.e., t0 is the actual start time step for TPARL aggregation at
step t. Now we can write the chosen action at step t without poisoning as a = πT (st0:t).
We prove the theoremby contradiction: We assume that there is a poisoning attack whose poisoning
size K ≤ Kt where Kt is defined by Equation (3) in the theorem, and after poisoning, the chosen
action is ατ = a. We denote {∏i}U-11 to the poisoned subpolicies and ∏T to the poisoned TPARL
aggregated policy. From the definition of Kt, we have
K	Kt
X h(ai,)aT ≤ X h(ai,)aT ≤ δa,aT,	(15)
i=1	i=1
since K ≤ Kt, and each h,aT is an element of hi，2福 for some i0 where
w-1	w-1
hi0,a,aT =	1i0,a(st-j) + w -	1i0,aT(st-j) ≥ 0
j=0	j=0
by Equation (4) in the theorem. Since the poisoning attack within threshold K can at most affect K
subpolicies, we let B be the set of affected policies and assume |B| = K without loss of generality.
Formally, B = {i ∈ [u] | ∃t0 ∈ [t0,t],∏i(st0) = ∏i(sto)}. Therefore, according to the monotonicity
of h(ai)aT, from Equation (15),
,K
Xhi,a,aT ≤ X h(ai,)aT ≤ δa,aT.	(16)
i∈B	i=1
According to the assumption of successfully poisoning attack, after attack, the sum of aggregated
vote for aτ plus 1[aτ < a] should be larger then that of a. Formally, after the poisoning,
neaT (st0:t) + 1[aτ < a] > nea(st0:t)
or equivalently
nea(st0:t) - (neaT (st0:t) + 1[aτ < a]) < 0.
From the statement of Theorem 1,
δa,aT = na (st0 :t ) - (naT (st0:t) + 1[a < a]).
Take the difference of the above two equations, we know that the attack satisfies
na(st0:t) - nea(st0:t) - (naT (st0:t) - neaT (st0:t)) > δa,aT .	(17)
Since the attack only changes the subpolicies in B, we have
(W-I	w—1	∖
X 1[πi(st-j ) = a] - X 1[πi (St-j) = a] I
j=0	j =0
w-1
≤	1i,a (st-j ),
i∈B j=0
(W-I	w—1	∖
E 1[πi(st-j) = aτ] - E 1[πi(st-j) = aτ]
j=0	j =0
≥ X X 1i,aT (stj) -w .
i∈B	j=0
21
Published as a conference paper at ICLR 2022
Inject them into Equation (17) yields
(W-I	w—1	∖
ErUst-j )+w -汇 1i,aτ(st-j ) I > δa,aT
j=0	j=0
hi,a,aT
which contradicts Equation (16) and thus concludes the proof.	口
F.3 Tightness of Per- S tate Action Certification in TPARL
Proof of Proposition 2. For ease of notation, we let w = min{W, t + 1} so w is the actual window
size used at step t. We let t0 = t - w + 1, i.e., t0 is the actual start time step for TPARL aggregation
at step t. Now we can write the chosen action at step t without poisoning as a = πT (st0:t).
We prove by construction, i.e., We construct an poisoning attack with poisoning size Kt + 1 that
deviates the prediction of the poisoned policy. Specifically, we aim to craft a poisoned dataset D
with poisoning Size | D ㊀ D | = Kt + 1, such that for certain learning algorithm M o, after partitioning
and learning on the poisoned dataset, the poisoned subpolicies ∏ = Mo(Di) can be aggregated to
produce different action prediction: f(sto：t) = a.
Before construction, we first show that Kt given by Equation (3) satisfies Kt < u. If Kt = u, it
means that for arbitrary a0 6= a,
u	u-1	u-1	w-1	w-1
Ehai,)a0 = Ehi,a,a0 =	£	I	E	1i,a(St-j) +	W -	E 1i,a0 (St-j)	I =	na(sto:t)	+	Uw	- na0 (sto:t)
i=1	i=0	i=0	j=0	j=0
≤ δa,a0 = na(sto:t) - na0 (sto：t) - 1 [a < a],
which implies Uw ≤ 0 contracting uw > 0. Now, we know Kt < u, so Kt + 1, our poisoning size,
is smaller or equal to u.
We start our construction by choosing an action aT:
aT = arg min arg max p.
_	a0=a,a0∈A PP=I 嗯0 ≤δa,a0
According to the definition of Kt in Equation (3), for ατ we have
Kt+1
X h(ai,)aT > δa,aT .	(18)
i=1
We locate the subpolicies to poison as
B = {i ∈ [u] | Iha aT is hjaT in the nonincreasing permutation in Equation (4),j ≤ Kt + 1}
_ ,	,	(19)
Therefore, |B| = Kt + 1 and
X hi,a,aT > δa,aT	(20)
i∈B
by Equation (18). For each of these subpolicies i ∈ B, we locate its corresponding partitioned
dataset Di for training subpolicy πi, and insert one trajectory pi to Di, such that our chosen learning
algorithm Mo can train a subpolicy ∏ = M(Di ∪ {pi}) satisfying ∏i(sto) = ατ for any t0 ∈
[t0, t]. For example, the trajectory could be pi = {(St0, aτ, S0, ∞)}tt0=t and M0 learns the action
with maximum reward for the memorized nearest state, where S0 can be adjusted to make sure the
trajectory is hashed to partition i. Then, we construct the poisoned dataset
De =	[ Di ∪ {pi} ∪ I [ Di I .
∖i∈B0	i	∖i∈[u]∖B0	)
EIC	I	六一.C,,	Ic"亍7, T 、T	,l	. 1	.
Therefore, we have D ㊀ D = ∪i∈B0Pi = |B0| = Kt + 1. Now, we compare the aggregated votes
for a and aτ before and after poisoning:
(W-I	w-1	∖
1[πei (St-j ) = a] -	1[πi (St-j ) = a]
j=o	j=o
22
Published as a conference paper at ICLR 2022
w-1
= -	1[πi (st-j) = a],
i∈B j=0
(W-I	w—1	∖
1[πei(st-j) = aT] -	1[πi (st-j) = aT]
j=0	j=0
(W-I	∖
w - X 1[πi(st-j) = aT]	.
j=0
Now we compare the margin between aggregated votes for a and aT after poisoning:
neaT (st0:t) - nea(st0:t) + 1[aT < a]
=naT (st0:t) - na(st0:t) + 1[aT < a] + neaT (st0:t) - naT (st0:t) - (nea(st0:t) - na(st0:t))
(W-I	w —1	∖
1[πi(st-j) = a] +w -	1[πi(st-j) = aT]
j=0	j=0
= - δa,aT +	hi,a,aT > 0.
i∈B
As a result, after poisoning, aT has higher priority to be chosen than a, i.e., πfT (st0 :t) 6= a =
∏τ(stoit). To this point, We conclude the proof with a feasible attack construction.	□
F.4 Loose Per- S tate Action Certification in TPARL and Comparision with Tight
One
The following corollary of Theorem 8 states a loose per-state action certification.
Corollary 10. Under the same condition as Theorem 1,
K	na(smax{t-W +1,0}:t) - maxa0=a (na0 (SmaX{t —W +1,0}:t) + 1 2 [a < a])
t	2min{ W,t + 1}
is a tolerable poisoning threshold at time step t in Definition 1, where W is the window size.
(21)
Proof of Corollary 10. We let w = min{t + 1, W} to be the actual aggregation window size at time
step t. After poisoning with size K, at most K subpolicies are affected and each affected policy can
only make ±w changes to the aggregated action count. This implies that, after poisoning, for any
action a0 ∈ A, the aggregated Vote_CoUnt 兀0 (St-W+i：t) ∈ [n。'(St-W+i：t) - uw,na0(St-W+i：t) +
uw]. Thus, when K ≤ Kt where Kt is defined in Theorem 1, for any a0 = a, we have
na(st-w+Lt) - na0 (St-w+Lt) - 1[a < a]
=na(St-W+1:t) - na0 (St-W+1:t) - 1[a < a] - 2Kw
≥na(st-W+Lt) - na0 (St-w+Lt)	-	1 [a0	<	a]	- 2w ∙ Kt
≥na(st-W+Lt) - na0 (St-w+Lt)	-	1 [a0	<	a]	- (na (St-W+Lt)	- m01ax(na00 (St-w+Lt)	+ 1 [a"	< a])
= max(na00 (St-W+1:t) + 1[a < a]) - (na0 (St-W+1:t) + 1[a < a]) ≥ 0.
a00 6=a
From the definition of TPARL protocol, the poisoned policy still chooses action a, which implies
Kt is a tolerable poisoning threshold.	□
In the main text, we mention that the certification from Corollary 10 is looser than that from Theo-
rem 1. This assertion is based on the following two facts:
1. According to Proposition 2, the certification given by Theorem 1 is theoretically tight, which
means that any other certification can only be as tight as Theorem 1 or looser than it.
2. There exists examples where the computed Kt from Theorem 1 is larger than that from Corol-
lary 10.
For instance, suppose W = 5, action set A = {a1, a2}, and there are three subpolicies. At time
step t = 4, πi for S0 to S4 are [a1, a1, a1, a1, a2] for all subpolicies (i.e., i ∈ [3]). Thus, the
23
Published as a conference paper at ICLR 2022
benign policy ∏t(s。j)=a`. By computation, the Kt from Theorem 1 is 1; while the Kt from
Corollary 10 is 0.
Indeed, Corollary 10 can be viewed as using 2w to upper bound hi,a,a0 = w + Pjw=-01 1i,a(st-j) -
Pjw=-01 1i,a0 (st-j). Intuitively, Corollary 10 assumes every subpolicy can provide 2w vote margin
shrinkage, and Theorem 1 uses hi,a,a0 to capture the precise worse-case margin shrinkage and thus
provides a tighter certification.
F.5 Per-State Action Certification in DPARL
Proof of Theorem 3. Without loss of generality, we assume Wmax ≤ t + 1 and otherwise we let
WmaX — min{Wmax,t + 1}. We let t。= max{t - WmaX + 1,0} be the start time step of the
maximum possible window. To prove the theorem, our general methodology is to enumerate all
possible cases of a successful attack, and derive the tolerable poisoning threshold for each case
respectively. Taking a minimum over these tolerable poisoning thresholds gives the required result.
Specifically, we denote P to the predicate of robustness under poisoning attack: P = [πfD (st0:t) =
a], and denote K to the poisoning attack size. Therefore, we can decompose P as such:
P = P(W0) ∧	^	-Q(W*,a0).	(22)
1≤W *≤Wmax,W *=W 0
a0 6=a
Recall that W0 is the chosen window size by the protocol DPARL with unattacked subpolicies πD
(Equation (1)). In Equation (22), the predicate P(W0) means that after poisoning attack, whether
the prediction under window size W0 is still a; the predicate Q(W* ,a0) means that after poisoning
attack, whether the chosen action is a0 at window size W * and average vote margin is larger (or equal
ifa0 < a) at window size W* compared to W0. Formally, let nea be the aggregated action count after
poisoning and ∆e tW be the average vote margin after poisoning at window W (see Equation (2)),
P(W0) = arg max nea(st-W 0+1:t) = a ,
a∈A
Q(W*, a0) = arg max nea(st-W *+1:t) = a0 ∧	(23)
a∈A
∆etW*	≥∆etW0∧(a0<a)∨∆etW*>∆etW0∧(a0>a).
According to Theorem 1,	_
_	K ≤ K t =⇒ P (W 0)
where Kt is defined by Equation (3) with W replaced by W0. The following Lemma 11 shows a
sufficient condition for Q(W*, a0). We then aggregate these conditions together with minimum to
obtain a sufficient condition for P:
W / w^w W τ>7	ɪVr W*,W01
K ≤ min Kt,	min	La0 a,00
1≤W * ≤min{Wmax,t+1},W * 6=W0,a06=a,a00 6=a a ,a
and thus conclude the proof.	□
Lemma 11. Let Q(W*, a0), K, W0 be the same as defined in proof of Theorem 3, then
K ≤ min LWa,0W0 =⇒ -Q(W*, a0),	(24)
a00 6=a a ,a
where LaW0 ,*a,0W0 is defined in Definition 4.
Proof. We prove the equivalent form:
Q(W*,a0) =⇒ K > min LW0*,0W0
a00 6=a a ,a
(25)
Suppose a poisoning attack can successfully achieve Q(W*, a0), we now induce the requirement on
its poisoning size K. First, we notice that
∆e tW *	≥∆etW0∧(a0<a)∨∆etW*	>∆etW0∧(a0>a)
tt	tt
0 W *W 0∆ W * ≥ W *W 0∆ W 0 + 1[a0 > a]
(26)
24
迎
1=?
+ W V jrθ∖l — (。:l+∕MTs)/产葭 _ (Ml+∕ΛlTs)D 葭)* 加—((?：[+ *AITS)#% —(工l + *Λ!Ts)产葭)/小心
'Y	⑼
?6
^3Qv	0 = m	HR
(ITSyM)(°Srnq 猊U：	R R +
{∕Λl'*Λl}x 它 B
[υ < /例[—((?：1+/AITS)/产葭—(工1+/AITS)D葭)*山—((工l + *Λ!Ts) #。葭—(工l + *Λ!Ts)产葭)ijW>
O = m	@三?
(ITS)MffrO-((吁⑹辿设。 R	R +
{∕Λl'*Λl}x 它 B
[υ < W[ — ((?：1+/AITS)/尸葭—(工1+/AITS)D葭)*/4 — ((?：[+ *AITS) #。葭—(工1+ *AITS)尸葭)∕√11=
(O=m	O=m
[„» = (^+m-⅛)⅛]T R *½l + m=。+笳TS)啊[M *幽—
τ-z√ll	T-z√ll
O=m	O=m \ @三？
[#» = (τ+m-⅛)⅛]l R /幽一[y=0+i"Ts)啊[M /幽 R_
1一*AI	l — *Al /
(O=m	O=m
[„» = (τ+m-⅛)⅛]l R *½l + m= 0+i71Ts)啊[M *幽—
τ-z√ll	T-z√ll
O=m	O=m \ @三？
[铲=0+吁?s)*][ M ,Til - Lp = (τ+m^s)⅛]τ R /Μ R +
1一 *711	1— *711	/
[υ < W[ — ((?：1+/AITS)/产葭—(工1+/AITS)D葭)*山—((工l + *Λ!Ts) #。葭—(工l + *Λ!Ts)产葭)/M=
[υ < /例[—((?：1+/AITS) "& —(工1+/AlTs)&) *山—((工l + *Λ!Ts) #% —(工l + *Λ!Ts)/&)
：(ZZ)UOTjBnba uτ əʌpɔəfgo Qqj jɔədsuɪ əM jʃ > Im JBqj
OloN ^[n] ɔ S jəs Ut səɪɔnodgns Qqj səguBqɔ X əzts Suτuosτod uτqjτM χoRnB Suτuosτod B əsoddn8
(ZZ)
∙[υ < 刈[+ ((工l + ∕Λ!Ts)/产” _ 0：1+ /AlTs)&) *山 > ((Ml+*ΛlTs) #% —(工l + *Λ!Ts)产")/小^=—
/"产 q ʌʃ
oj juəɪnʌmbə st uoμτsodojd sτqι uoμτpuoo KJBSSəɔəu
B st	< X JBqj Moqs PUB ζuoμτpuoo Qqj sb UOTjnnbQ JSBj Qqj əsn əM mou iυ * jjυ qo∏g JoH
,[υ < 刈[+ ((工l + ∕Λ!Ts) /产”—(工【+ /AlTs)&) *山 < ((?：[+*/HTS)#%—(工l + *Λ!Ts)内”),小
1O ≠ //0EU
(	曾壬//管	\
∖υ < /例[+ I (?：1+/AITS) "& XaUl —(工l + ∕Λ!Ts)& I *山 < ((MI+*Λ1Ts) #%—(工l + *Λ!Ts)产")^=—
(∕*Mfel
jə^ əm tSUOTJBXBJQJ OMI
QAoqe Qqj PUB (93) UODBnbH xπojj tsnqι punoq jəmoj b səpɪʌojd ssŋɪɔ JQqjo Aub qjτM Suμndmoo
PUB tSuτuosτod jəjjb ssŋɪɔ doj Qqj əsn pjnoqs	uτ ut⅛jbxπ Qqj IBqI iɔBJ Qqj UIay səmoɔ 4t<,, QjQqM
(	rO 半”r0	∖
，((?〕+MITS) ”&XT3UI _(?〕+，小TS) &)*幽 < Jy∕½l*½l
tpιreq JQqjo Qqj Uo utSjbxπ jb∏pb PUB punoq Qqj uəəMmq dπS Qqj 5[uuqs AjjBOTiidmQ oj tSuτuosτod
əjojəg ssŋɪɔ dn-jəuumQqj t(17 uoμτu^αəəs)(工1+*μ—，S)C)D葭^三0°'产壬C)DXaUlXJa= 砰D Qsooqo əm
əjəh punoq jəddn UB səpuojd ssŋɪɔ JQqjo Aub oj jɔədsəj qjτM Suμndmoo PUB tSuτuosτod jəjjb ssŋɪɔ
dn-jəuunj Qqj oj jɔədsəj qjτM əg pjnoqs uτ ut^ibxπ Qqj jπqj jəbj Qqj UIaIJ səmoɔ 4t>,, QjQqM
1 ((2〕+*AITs)#% _*(?R*A!Ts)，2) /M > JJP幽*小
∖d = (^t+*ai-⅛)ou y,3oxBuιSjb JBqj UOTjduinssB
S(D '*½1)3 PUB uotjtu^əp əgi oj gmpjoɔɔv uoμτu^p Aq jəgəjuf ire st +λi∕, JV∕½1≠½1 əɔɑɪs
CCoC mɔl JB JodBd əɔuəjəjuoɔ B sb P叫SUqnd
Published as a conference paper at ICLR 2022
(b)	0	*	0
≤ W (na0 (St-W* +Lt) - na# (St-W* +Lt)) - W* (na(st-W0+1:t) - na00 (St-W0+Lt)) - 1[a > a] +
L W*,W0
La0,a00
X	g(i)
i=1
(c)
< 0.
W* W0
Thus, Equation (27) is proved. Therefore, K > LaW0,a,0W0 is a necessary condition for Q(W* , a0),
i.e., Equation (25).
In the above deriviation, the definitions of gi, gi, and σw are from Equation (7). (a) comes from
the facts that {g(i)}iu=1 is a nondecreasing permutation of {gi}iu=-01, gi ≥ 0, and |B| ≤ K. (b)
W* W0
comes from the assumption K ≤ LaW,a0,0W and also g(i) ≥ 0. (c) comes from the definition in
Equation (6).	□
F.6 Possible Action Set in PARL and Comparison
F.6.1 Certification
Proof of Theorem 4. According to the definition of possible action set, we only need to prove the
contrary: for any a ∈ A \ AT (K), within poisoning size K, the poisoned policy cannot choose a:
πfP (St) 6= a.
According to Equation (8), any a ∈ A \ AT (K) satisfies
X max{na0 (St) - na(St) - K + 1[a0 < a], 0} > K.	(28)
a0∈A
Given poisoning size K, since each poisoning size can affect only one subpolicy, we know
nea(St) ≤ na(St) + K
where nea denotes to the poisoned aggregated action count. We suppose the attack could be success-
ful, then for a0 < a, nea0 (St) ≤ nea(St) - 1, and thus na0(St) - nea0(St) ≥ na0(St) -na(St) - K+ 1.
Similarly, for a0 > a, a0(St) ≤ nea(St), and thus na0(St) -nea0(St) ≥ na0 (St) -na(St) -K. Also, for
any a0 6= a after poisoning na0 (St) - nea0 (St) ≥ 0; otherwise deviating the difference subpolicies’
decisions’ from a0 to a is strictly no-worse. Given these facts, the amount of votes that need to be
reduced is the LHS of Equation (28) which is larger than K. However, we only have K poisoning
size, i.e., K votes that can be reduced. As a result, our assumption that the attack could be successful
is falsified and the poisoned policy cannot choose a.	□
Corollary 12 (Loose PARL Action Set). Under the condition of Definition 5, suppose the aggrega-
tion protocol is PARL as defined in Definition 7, then the possible action set at step t
AL (K) = a ∈ A max na0 (St) - na(St) ≤ 2K - 1[a > arg max na0 (St)] .	(29)
a0∈A	a0∈A
Proof of Corollary 12. Again, we prove the contrary, for any a ∈ A\AL (K), within poisoning size
K, the poisoned policy cannot choose a: πfP (St) 6= a.
According to Equation (29), let am = arg maxa0∈A na0(St), then any a ∈ A \ AL (K) satisfies
nam(St) - na(St) > 2K - 1[a > am].
After poisoning, we thus have
nam (St) - na(St) > -1[a > am] =⇒ nam (St) - na(St) ≥ 1 - 1[a > am]
From the definition, a ∈/ A (K ) so a 6= am . If am < a, nam (St ) ≥ na (St ); if am > a, nam (St) >
na (St). In both cases, am has higher priority to be chosen than a, and thus the poisoned policy
cannot choose a.	□
F.6.2 Comparison
Theorem 13. Under the condition of Definition 5, suppose the aggregation protocol is PARL as de-
fined in Definition 7, AT (K), AL (K) are defined according to Equations (8) and (29) accordingly,
then
1. AT (K) ⊆ AL (K); and there are subpolicies {πi}iu=-01 and state St such that AT (K) ( AL (K).
26
Published as a conference paper at ICLR 2022
2. Given subpolicies {πi}iu=-01 and state st, for any a ∈ AT (K), there exists a poisoned training set
D whose poisoning Size |D ㊀ D| ≤ K and some RL training mechanism, such that ∏p(st) = a
where πfP is the poisoned PARL policy trained on D.
Proof of Theorem 13. We prove the two arguments separately.
1.	We first prove AT(K) ⊆ AL(K). For any a ∈ AT (K), let am = arg maxa0∈A na0 (st), then
max{na0 (st) - na(st) - K + 1[a0 < a], 0} ≤ K
a0∈A
=⇒nam (st) - na (st) - K + 1[am < a] ≤ K
=⇒nam (st) - na (st) ≤ 2K - 1[a > am]
where the last proposition is exactly the set selector of AL (K) so a ∈ AL (K).
We then prove that AT (K) ( AL (K) can happen by construction. Suppose that there are three
actions in the action space: A = {a1, a2, a3}. We construct subpolicies for current state st such
that the aggregated action counts are
na1 (st) = 10, na2 (st) = 9, na3 (st) = 1.
Given poisoning size K = 5, we find that
na1 (st ) - na3 (st ) = 9 ≤ 10 - 1[a3 ≥ a1 ] = 9	=⇒ a3 ∈ a (K),
(na1 (st) - na3 (st) - K + 1[a1 < a3]) +
(na2 (st) - na3(st) - K + 1[a2 < a3]) = 9 > K = 5 =⇒ a3 ∈/ aT(K).
Therefore, AT (K) ( AL(K) for these subpolicies and state st.
2.	We prove by construction. For any a ∈ AT (K), we construct the set of subpolicies to poison
Ba ⊆ [u] such that |Ba | ≤ K, then describe the corresponding poisoned dataset Da, and finally
prove that the poisoned policy πfPa(st) = a.
For a ∈ AT (K), by definition (Equation (8)), we know
X max{na0(st) - na(st) - K + 1[a0 < a], 0} ≤ K.	(30)
δ y 、	'	一 'Z	' J
a ∈A
:=ta,a0
We now define a set of actions Ca⊆ A such that
Ca={a0 ∈ A | ta,a0 >0}.
According to this definition, a ∈/ Casince ta,a ≤ 0.
Fact F.1. For a ∈ AT (K) and any a0 ∈ A, na(st) + K - 1[a0 < a] ≥ 0.
Proof of Fact F.1. Suppose na(st) + K - 1[a0 < a] ≤ 0, then na(st) = K = 0 and a0 < a.
Then
max{na0(st)-na(st)-K+1[a0 < a], 0} ≥	na0(st)-na(st)-K =	na0(st) = u > 0
a0三A	a0三A	a0三A
which contradicts the requirement that the LHS of the above inequality should be ≤ K = 0. □
Give Fact F.1, for any a0 ∈ Ca, na0(st) ≥ ta,a0. Notice that na0(st) is the number of subpolicies
that vote for action a0 at state st. Therefore, we can pick an arbitrary subset of those subpolicies
whose cardinality is ta,a0. We denote Baa0 to such subset:
Baa0 ⊆{i∈ [u] | πi(st) = a0}, |Baa0|=ta,a0.
Now define Bαa: Bαa= a0∈Ca Baa0. We construct Bβa to be an arbitrary subset of those subpoli-
Cies whose prediction is not a and who are not in Ba, and limit the Ba's cardinality:
Be ⊆{i ∈ [u] I ∏i(st) = a}\ Ba, ∣Bβl = min{K - |B£|,u - n“(st)-圆|}.
Such Bβa can be selected, because:
• From definition, Bαa ⊆ {i ∈ [u] | πi(st) 6= a}, where the cardinality of{i ∈ [u] | πi(st) 6=
a} is U - na(st). So 0 ≤ U 一 na,(st) 一 |B；|.
• Since
__	__ _______________ (*)
∣Bα I ≤ E BI = E ta,a，= E 2 ≤ k,
a0∈Ca	a0∈Ca	a0∈A,ta,a0 >0
27
Published as a conference paper at ICLR 2022
K - |Ba | ≥ 0, and thus |Ba| ≥ 0. Here (*) is due to Equation (30).
• The superset {i ∈	[u]	|	∏i(st)	=	a} \	Ba	has cardinality U 一 nα,(st)	- |B；|	and |Ba| ≤
U - nα(St)-IBa |.
To this point, we can define the set of subpolicies to poison:
Ba := Bαa ∪Bβa,
and IBaI = IBαaI + IBβaI ≤ K.
In a similar fashion as the attack construction in proof of Proposition 9, for each i ∈ Ba, we
locate its corresponding partitioned dataset Di for training subpolicy πia . We inset one trajectory
Pa to Di such that our chosen learning algorithm M o can train a subpolicy ∏ = M 0 (Di ∪ {pf })
such that ∏ia(st) = a. For example, the trajectory could be Pa = {(st, a, s0, ∞)} where s0 is an
arbitrary state that guarantees pia is hashed to partition i; and M0 learns the action with maximum
reward for the memorized nearest state. Then, the poisoned dataset
Dα = ( [ Di ∪ {pal}) ∪ I U Di).
∖i∈Ba	i	∖i∈[u]∖B0	)
Thus, Da is ∣D ㊀ Da∣ = ∣Ba∣ ≤ K, i.e., the constructed attack,s poisoning size is within K.
We now analyze the action prediction of the poisoned policy πfP. For action a, after poisoning,
nea(st) = na(st)+IBaI = na(st)+min{K, U-na(st)} = min{na(st) + K, U}. If nea(st) = U,
then all subpolicies vote for a, apparently πfPa(st) = a; otherwise, nea(st) = na(st) + K. In
this case, for any action a0 ∈ Ca, since we at least choose subpolicies in Baa0 and change their
action prediction to a, the aggregated action count after poisoning is nea0 (st) ≤ na0(st) - IBaa0 I =
na(st)-ta,a0 = na(St)-"0(st)+na(st)+K-1[a0 <a]= na(st)+K-1[a0 <a]= na(st)-
1[a0 < a]. Thus, a0 has lower priority to be chosen than a. For any action a0 ∈/ Ca and a0 6= a,
by the definition of Ca, ta,a0 = na0(St) - na(St) - K+ 1[a0 < a] ≤ 0. After poisoning, the vote
of a0 does not increase, i.e., ena0 (St) ≤ na0 (St) ≤ na(St) + K - 1[a0 < a] = nea(St) - 1[a0 < a].
Thus, a0 also has lower priority to be chosen than a. In conclusion, we have πfPa (St) = a.
To this point, for any a ∈ AT (K), we successfully construct the corresponding poisoned dataset
a
Da within poisoning size K such that πfP (St) = a, thus concludes the proof.
□
F.7 Possible Action Set in TPARL
Proof of Theorem 5. For ease of notation, we let w = min{W, t + 1}, so w is the actual window
size used at step t. We let t0 = t - w + 1, i.e., t0 is the actual start time step for TPARL aggregation
at our current step t. Now We can write chosen action at step t without poisoning as ∏τ(sto：t).
We only need to prove the contrary: for any a ∈ A \ A(K), within poisoning size K, the poisoned
policy cannot choose a: πfT (St0:t) 6= a.
According to Equation (9), for such a, there exists a0 6= a such that
K
h(ai0),a ≤ δa0,a = na0(St0:t) - na(St0:t) - 1[a < a0].	(31)
i=1
Suppose there exists such poisoning attack that lets πfT (St0:t) = a. This implies that for a0, after
poisoning, we have
nea0 (St0:t) - nea0 (St0:t) - 1[a0 < a] < 0,	(32)
where nea is the aggregated action count after poisoning. Since the poisoning size is within K, it
can affect at most K subpolicies. We let B ⊆ [U], IBI ≤ K to represent the affected subpolicy set.
Therefore,
nea0 (St0:t) - nea(St0:t) - 1[a0 < a]
=na0(St0:t) - na(St0:t) - 1[a0 < a] +(nea0 (St0:t) - na0(St0:t)) - (nea(St0:t) - na(St0:t))
、----------------V----------------}
δa0,a
28
Published as a conference paper at ICLR 2022
w-1	w-1
Σ	1[ττi(st-j) = a0,∏i(st-j) = a0] - E 1[∏i(st-j) = a',∏i(st-j) = a0]
j=0	j=0
w-1	w-1
Σ 1[πri(st-j) = a, ∏i(st-j) = a] - E 1[∏i(st-j) = a,∏(st-j) = a]
j=0	j=0
w-1	w-1
Σ 1[∏i(st-j) = a0,∏i (st-j) = a0] + £ 1[∏i (st-j) = a,∏i(st-j) = a]
j=0	j=0
(W-I	w-1	∖
X 1[πi(st-j) =a0]+X1[πi (st-j ) 6= a]
j=0	j=0
(W-I	w-1	∖
1i,a0(st-j)+w-	1i,a(st-j)
j=0	j=0
=δa0,a
(a)	|B|	(i)	(b)	K (i) (c)
-	hi,a0,a ≥ δa0,a -	ha0,a ≥ δa0,a - ha0,a ≥ 0.
i∈B	i=1	i=1
This contradicts with Equation (32), and thus the assumption is falsified, i.e., there is no such poi-
Soning attack that let 而⑶。：/ = a. In the above equations, (a) is due to the fact that hai0)α is a
nonincreasing permutation of {hi,a0,a}iu=-01. (b) is due to the facts that |B| ≤ K and h(ai0),a ≥ 0. (c)
comes from Equation (31).	□
F.8 Hardness for Computing Tight Possible Action Set in TPARL
Proof of Theorem 6. By Definition 5, the possible action set with minimum cardinality (called min-
imal possible action set hereinafter) is unique. Otherwise, suppose A and B are both minimal
possible action set, but A 6= B, then A ∩ B is a smaller and valid possible action set. Therefore,
the oracle that returns the minimal possible action set, denoted by MINSET, can tell whether any
action a ∈ MINSET and thus whether any action a can be chosen by some poisoned policy πe whose
poisoning size is within K . In other words, the problem of determining whether an action a can
be chosen by some poisoned policy ∏ whose poisoning size is within K, denoted by ATTKACT,
is polynomially equivalent to MINSET: MINSET ≡P ATTKACT. Now, we show a polynomial
reduction from the set cover problem to ATTKACT, which implies that our MINSET problem is an
NP-complete problem.
The decision version of the set cover problem (Karp, 1972), denoted by SETCOVER, is a well-
known NP-complete problem and is defined as follows. The inputs are
1.	a universal set of n elements: U = {u1,u2, •…,Un};
2.	a set of subsets of U: V = {V1,…，Vm}, Vi ⊆ U, 1 ≤ i ≤ m, Um=I Vi = U;
3.	a positive number K ∈ R+ .
The output is a boolean variable b, indicating that whether there exists a subset W ⊆ V, |W | ≤ K,
such that ∀ui ∈ U, ∃V ∈ W, ui ∈ V . Given an oracle to ATTKACT, we need to show SETCOVER
can be solved in polynomial time, i.e., SETCOVER ≤P ATTKACT.
•	If K ≥ n:
We scan all sets V ∈ V .To being with, We have a record set S — 0, and an answer set W - 0.
Whenever We encounter a set that contains a new element Ui / S, We put W J W ∪ {Vj},
and record this element S — S ∪ {ui }. After one scan pass, W covers all elements of U (since
Ujm=1 Vj = U), and |W | ≤ n ≤ K. Therefore, W is a valid set cover. Since we can always find
such W, we can directly answer true.
29
Published as a conference paper at ICLR 2022
•	If K ≥ m:
We can directly return V as a valid set cover, since Sjm=1 Vj = U and |V | ≤ m ≤ K. Thus, we
can answer true.
•	If K < min{n, m}:
This is the general case which we need to handle. Now we construct the ATTKACT(K) problem
so that we can trigger its oracle to solve SETCOVER.
1.	The poisoning size is K.
2.	The	action space A = U ∪ {b} ∪ Γ where Γ	:= {#1, . .	. , #m2n}. (|A| = n + 1 + m2n.)
The	sorting of actions is uι <u < •…< un,	<b < #1	< •…< #m2n.
3.	The	subpolicies are {πja}jm=1 ∪ {πib, πic,j |	1 ≤ i ≤	n, 1 ≤ j ≤ K - 1}, where πja
corresponds to Vj ∈ V, and πib, πic,j correspond to ui ∈ U. (Number of subpolicies u =
m + Kn ≤ m + n2.)
4.	The current time step is t = nm, and the window size W = nm.
5.	The input action is b, i.e., asking whether b can be chosen by some poisoned TPARL policy,
i.e., ffT(si：t) = b, if the poisoning size is within K.
Now, we construct the states at each step t (1 ≤ t ≤ nm) so that the subpolicies’ action predictions
at these steps are as follows.
1.	Count the appearing time of each ui in V, and denote it by ci: ci = Pjm=1 1[ui ∈ Vj]. For
each ui, select a Vj0 that contains ui, and in the corresponding πja , assign m - ci + 1 steps
to predict ui; for all other Vj that contains ui, in the corresponding πja, assign one step to
predict ui .
2.	After this process, each πja at least has one time step whose action prediction is ui for each
ui ∈ Vj. Among all {πja}jm=1 and all time steps 1 ≤ t ≤ nm, mn step-action cells are filled,
and the remaining (m2 n - mn) cells are filled by #l ∈ Γ sequentially.
3.	For each πib, arbitrarily select W - m time steps to assign action prediction as ui; and fill in
other m time steps by remaining #l ∈ Γ sequentially.
4.	For each πic,j, for all time steps, let the action prediction be ui.
As we can observe, the number of actions, the number of subpolicies, and the window size are all
bounded by a polynomial of n and m. Therefore, such construction can be done in polynomial
time.
We then show SETCOVER = true q⇒ ∃K: b ∈ ATTKACT(K0), 1 ≤ K0 ≤ K.
Suppose the covering set is W ⊆ V, we denote K0 to |W |, and construct the ATTKACT
problem with poisoning size K0 as described above.
We can construct a poisoning strategy to let ffT(si：nm) = b, The poisoning strategy is to
find out πja for each Vj ∈ W, and to let them predict action b throughout all time steps:
πeja (s0t ) = b, 1 ≤ t0 ≤ nm.
After poisoning, the aggregated action count neb(s1:nm) = |W| × nm = K0nm. Since W
covers every ui ∈ U, for each ui ∈ U there exists a set Vj 3 ui , whose corresponding πeja is
poisoned to predict b. Thus, neui (s1:nm) < nui(s1:nm) = m+ (W -m) + W × (K0 - 1) =
WK0 = K0nm. For any #l ∈ Γ, ne#l (s1:nm) ≤ n#l (s1:nm) = 1. In summary,
neui (s1:nm) < K0nm, neb(s1:nm) = K0nm, ne#l (s1:nm) = 1.
Thus, after TPARL aggregation, the poisoned policy πfT (s1:nm) = b, and therefore b ∈
ATTKACT(K0).
Suppose it is K0 that let b ∈ ATTKACT(K0), which implies that there exists such a poisoning
attack within size K0 that misleads the poisoned policy to b: f(si：nm) = b. Since the
poisoning size is K0, after poisoning the aggregated action count
neb(s1:nm) ≤ K0W = K0nm.
For each ui ∈ U, since nui (s1:nm) = m + (W - m) + W × (K0 - 1) = K0nm, we always
have
〜	(平)〜
neui (s1:nm) < neb(s1
:nm ) ≤ nui (
s1:nm),	(33)
30
Published as a conference paper at ICLR 2022
where (*) is due to the condition of successful attack. We denote the set of poisoned subpoli-
cies by Π (∣Π∣ ≤ K0). Therefore, Equation (33) implies that for each Ui ∈ U, there exists at
least one subpolicy
πu0i ∈Π,πu0i	∈	{πja	|	ui	∈Vj}∪{πib}∪{πic,j	| 1 ≤j ≤K-1}	(34)
that is poisoned by the attack, otherwise the aggregated vote neui cannot change.
We partition Γ by Γa and Γbc, where
Γa = Γ ∩ {πja}jm=1, Γb = Γ ∩ {πib, πic,j | 1 ≤i≤n,1 ≤j ≤K-1}.
We construct additional poisoning set Γ+ following this process: In the beginning, Γ+ J 0.
For each ui	∈	U,	if Γa	∩	{πja	|	ui	∈	Vj }	is not empty, skip. Otherwise, according to
Equation (34), Γb ∩ {πib, πic,j | 1 ≤ j ≤ K - 1} is not empty. In	this	case,	we find
an arbitrary covering set of ui, namely Vj0 3 ui, and put πja	into Γa+ .	When the	process
terminates, we find that for each ui ∈ U,
(Γa ∪ Γa+) ∩ {πja | ui ∈ Vj} =6 0.	(35)
Following the mapping {πja}jm=1 J→ {Vj | 1 ≤ j ≤ m} =	V, the subset	(Γa ∪	Γa+) ⊆
{πja}jm=1 can be mapped to (Wa ∪W+a ) ⊆ V. From Equation (35), (Wa ∪W+a ) is a valid set
cover for U. We now study the cardinality of this set cover. From the process, we know that
every Vj° ∈ W? corresponds to a different set in Γb. Thus, |W? | ≤ ∣Γb∣, which implies that
|Wa ∪ W+1 ≤ |Wa∣ + |W+1 ≤ ∣Γa∣ + ∣Γb∣ = ∣Γ∣ ≤ K0.
To this point, we successfully construct a set cover within cardinally K0 ≤ K that covers U,
so SETCOVER = true.
Therefore, we can check whether SETCOVER = true by iterating K0 from 1 to K (<
min{n, m} iterations), constructing the ATTKACT(K0) problem, and querying the oracle.
The whole process can be done in polynomial time assumping O(1) computation time of the
ATTKACT(K0) oracle.
To this point, we have shown SETCOVER ≤P ATTKACT. On the other hand, an undeterminisitic
Turing machine can try different poisoning strategies by branching on whether to poison current
subpolicy and what actions to be assigned to each poisoned subpolicy. The decision of whether the
poisoning is successful can be done in polynomial time and b ∈ ATTKACT(K) corresponds to the
existence of successfully attacked branches. Thus, ATTKACT ∈ NP. Given that SETCOVER is
an NP-complete problem, so does ATTKACT and MINSET (since MINSET ≡p ATTKACT). □
F.9 Possible Action Set in DPARL
Proof of Theorem 7. For ease of notation, we assume that Wmax ≤ t + 1, and otherwise we let
Wmax J t + 1. We let t0 = max{t - Wmax + 1, 0} be the start time step of the maximum possible
window.
We only need to prove the contrary: for any a ∈ A \ A(K), within poisoning size K, the poisoned
policy cannot choose a: πfD (st0:t) 6= a. We prove by contradiction: we assume that there exists
such a poisoning attack within poisoning size K that lets fD(sto：t) = a. From the expression of
A(K) (Equation (10)), a = at = nŋ(sto:t). Suppose the selected time window before the attack is
W 0 (selected according to Equation (1) based on ∆tW and na), and the selected time window after
W
the attack is W0 (selected according to Equation (1) based on ∆tW and nea).
• If Wf0 = W0:
Suppose we use the TPARL aggregation policy with window size W = W0 instead of current
DPARL aggregation policy, then we will have πτ(st-w，+上t)= at and f (St-W，+i：t) = a.
Thus, according to the definition of possible action set (Definition 5), a ∈ A(K) where A(K) is
defined by Equation (9) in Theorem 5. This implies that a ∈ A(K) where A(K) is defined by
Equation (10), which contradicts the assumption that a ∈ A \ A(K).
If Wf0 6= W0:
According to the definition in Equation (10),
min
1≤W *≤Wmax ,W *=W 0,a00 = at
r W*,W0
La,a00
> K.
31

sɪrejəp uoτjBju5x∏5jdmτ Qqj Aq pəMOUoJ ζs∏5pτ uιqjμoSjB Qqj əɔnpojjuɪ
KUə!Jq ISJU əM S5[SBj TH əuɪɪjjo uτ sjɪnsəj γiOS MoqS qoτqM suιqjμoSjB qχ jπuoμnqτjjsτp əjb omj
jəjjŋɪ Qqj 5jτqM 'əu[[əSBq PjBPUBJS Qqj st əuo ISJU Qqi "(ʌɪo^ ζ jB jə əjBulə[[əg) ɪςɔ PUB i(SXOZ P
心UqBCl) Nθσ-Hθ t(ET03 'P lə q^W) Nθσ :smqjno§tB qχ əuɪujo əəJqI qjτM pəiuəuɪjədXə əM
SNoilVlNHHHldHI QNV SHH1IHO91V TH HNlTHo HHl JO S1IV1HQ
SqIVIHCI IVlNHWIXHdXH qVNOIlIQCIV 9
□
∙υ ≠ (^0⅛)Qji ：p OSooqɔ IoUUBɔ Ao∏odpəuosɪod W 'χ əzts Suτuosτod
uτqjτM ζ(jf)y ∖y ə υ Aub joj jπqj əpnɪɔuoɔ UBɔ əM MON SUOHɔ!PBJJUOɔ PUU OM 'səseɔ qjoq uτ əɔuɪs
（加
"(ʌɛ) UOTjBnba qjτM spτp∏jjuoo sτqι
・因 < V]τ + (% -,辿 < (⅛ -
jə^ əm<6£)UOHBnbHJO səpɪs OMI oj SUODBajəSqO OMI əsəgi SuτSSnja
'O > SHXPuBON SHl
'0	/MV	M
^mv ≠ tv JI ("SHX OJ sɪenbə SHl =切 JI(T osnegq -晶 <	- 17⅛ Z
jsəɪɪŋms Qqj əg PlnOqS ut^ibxπ
IiQqj pun uotjəb dn-jəuum Qqj st	pun uotjəb doj Qqj st v əɔmst (^u -	< 或B- & ɪ
(6E)
:suo□BAαsqo OMj Sutmo∏oj Qqj 5Λ∏q əM
/41	*	*
[?〃<〃][ + (%" - )幻辿 < ((熟—,装),小
oj (g£)UOTjBnba əjuməj əM •/m MθpuτM jb uotjəb dn-jəuum Qqj st z,yl^υ PUB '/小
∖<->)
MOpUTM JB UOTJOB Qqj S] 幽⑵，/山 MOpUTM JB UODOB dn-J5UUΠJ Qqj ST	tSuTUOSTθd jəjjb tAjQATJTnjUI
ye 00’ /寓管壬OV
°二U XBiu 既Te
/Λl~
y^0p	y3°OiO≠°O
夕1仞，叱〃 XPlU 既Te= m,o i 0二U XPuI 既Te = 小管
(ζ)	√H~	辿〜	(乙)
tUOTjnnbQ əʌogŋ Qqj Ul •切 > υ jτ 4t<,, st QjQqM
(8E)
1Jy
/Ai
z>
/41
JBqj S5∏dmτ
Tr)= S)Qir PUB υ =(""s)% ' a ? tuoμdmnssB 5[0BjjB ɪnjssəɔɔns əgi tpuπq JQqjo əgi uq
(∆E)
O > 因 < »]l -(⅛-z⅛)z½ι -(z⅛-z^)∕½ι
t(9E) UOTjBnba qjτM pəmgmoɔ
^ ^ '1=?
•因 < 小T北匕晶)辿一(蕾匕总)/小< [⅛ <小T意〃一,部)辿幽+⑶。M
X
5Λ∏q əM i(v) oj 〃)UOTjBnba UKHJ uoτjBΛ∏5p Qqj Sutmojjoj
(j τ+m-⅛)ou jo puπqjjoqs B st pun ('」+辿TS)ClD葭^三的，产壬ODXmllWJa
1=?
#k QjQqM
(9E)
1O > [⅛<°]τ-(⅛ -蓝初生-^u~ ,MuIm + ⑶。R
X _
t((∆) UOTJBnbH) uopra^əp Xqo <(?W ∞≡S
【=?
O > 阳 < p]τ-(z⅛-/初生 ―(蕾〃 -,MuIm + ⑶。3
∕½√∕ 幽Zɪ
~ snqj PUB
X < /@7
JBqj səɪɪdun UoDBnbə əAOqB Qqj tu5qι ∙(^τ+∕Ai-⅛) 0°u ^≠0⅛ra 击R 二 城D əugəp əM
CCoC mɔl JB JodBd əɔuəjəjuoɔ B sb P叫SUqnd
Published as a conference paper at ICLR 2022
Algorithm Ideas. The core of Q-learning (Watkins & Dayan, 1992) is the Bellman optimality
equation (Bellman, 1966)
Q?(s, a) = ER(s, a) + γEs0〜P max Q? (s0, a0),	(41)
a0∈A
where a parameterized Qθ is adopted to approximate the optimal Q? and iteratively improved. In
DQN (Mnih et al., 2013) specifically, the parameterization is achieved by using a convolutional
neural network (LeCun et al., 1998). In contrast to estimating the mean action value Qπ(s, a) in
DQN, distributional RL algorithms estimate a density over the values of the state-action pairs. The
distributional Bellman optimality can be expressed as follows:
Z?(s, a)	= r + YZ?	(s0,	argmaXaθ∈A Q?	(s0,	a0))	where r 〜R(s, a), s0	〜P(∙	| s,a).	(42)
Concretely, QR-DQN (Dabney et al., 2018) approximates the density D? with a uniform mixture
of K Dirac delta functions, while C51 (Bellemare et al., 2017) approximates the density using a
categorical distribution over a set of anchor points.
Implementation Details. For training the subpolicies using offline RL training algorithms, we use
the code base of Agarwal et al. (2020). The configuration files containing the detailed hyperparam-
eters can be found at their public repository https://github.com/google-research/
batch_rl/tree/master/batch_rl/fixed_replay/configs. For the three methods
DQN, QR-DQN, and C51, the names of the configuration files are dqn.gin, quantile.gin,
and c51.gin, respectively. On each partition, we train the subpolicy for 50 epochs.
G.2 Concrete Experimental Procedures
We conduct experiments on two Atari 2600 games, Freeway and Breakout from OpenAI
Gym (Brockman et al., 2016), and one autonomous driving environment Highway (Leurent, 2018),
following Section 4.
Concretely, in the training stage, we first partition the training dataset into u partitions (u = 30, 50,
or 100) by using the hash function h(τ) pre-defined in each environment. Let si be the i-th value in
the state representation and d be the dimensionality of the state. In Atari games where the state is
the game frame, h(τ) is defined as the sum of all pixel values of all frames in the trajectory τ, i.e.,
h(τ) = Ps∈τ Pi∈[d] si. In Highway where the state is a floating point number scalar containing
the positions and velocities of all vehicles, we define h(τ) = Ps∈τ Pi∈[d] f (si) where f : R → Z
is a deterministic function that maps the given float value to integer space. Concretely, we take f(x)
as the the sum of the higher 16 bits and the lower 16 bits of its 32-bit representation under IEEE
754 standard (group of the Microprocessor Standards Subcommittee & Institute, 1985). We then
train the subpolicies on the partitions with offline RL training algorithm M0 ∈ {DQN (Mnih et al.,
2013), QR-DQN (Dabney et al., 2018), C51 (Bellemare et al., 2017)}. The detailed description of
the algorithms can be found in Appendix G.1.
In the aggregation stage, we apply the three proposed aggregation protocols (PARL (Theorem 8),
TPARL (Theorem 1), and DPARL (Theorem 3)) on the trained u subpolicies and derive the aggre-
gated policies πP, πT, and πD accordingly.
Finally, in the certification stage, for each aggregated policy, we provide the per-state action and
cumulative reward certification following our theorems. When certifying the per-state action, we
constrain the maximum trajectory length H = 1000 for Atari games and H = 30 for Highway
(which is the full length in Highway) and report results averaged over 20 runs; for the cumulative
reward certification, we adopt the trajectory length H = 400 for evaluating Freeway, H = 75 for
Breakout, and H = 30 for Highway.
Configuration of Trajectory Length H. For Atari games, we do not evaluate the full episode
length (up to tens of thousands steps), since our goal is to compare the relative certified robustness
of different RL algorithms, and the evaluation on relatively short trajectory is sufficient under af-
fordable computation cost. Moreover, different episodes in Atari games are oftentimes of different
lengths; thus it is necessary that we restrict the episode length to enable a fair comparison. For
Highway, we evaluate on the full episode (H = 30) where we can efficiently achieve effective
comparisons.
33
Published as a conference paper at ICLR 2022
Table 4: Benign empirical performance of three aggregation protocols (PARL, TPARL, and DPARL) applied
on subpolicies trained using three offline RL algorithms (DQN, QR-DQN, and C51), with the number of sub-
policies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of varying randomness
in the environment.
Freeway	PARL	U = 30	U = 50														
		TPARL		TPARL		TPARL		DPARL (Wmax = 5)	PARL	TPARL		TPARL		TPARL		DPARL (Wmax = 5)
		(W	= 2)	(W	= 3)	(W	= 4)			(W	= 2)	(W	= 3)	(W	= 4)	
DQN	10.90 ± 0.77	11.65	± 0.57	11.25	± 0.54	12.00	± 0.45	11.45 ± 0.74	10.95 ± 0.97	11.60	± 1.02	12.40	± 0.58	11.65	± 0.65	11.90 ± 0.89
QR-DQN	11.60 ± 0.66	11.85	± 1.15	11.25	± 0.62	11.80	± 0.87	12.10 ± 0.99	11.50 ± 0.92	11.60	± 1.02	12.15	± 0.57	12.80	± 0.51	11.90 ± 0.77
C51	11.20 ± 0.60	12.45	± 0.50	12.55	± 0.50	11.40	± 0.66	12.40 ± 0.49	11.70 ± 1.27	11.80	± 0.75	12.70	± 0.46	11.50	± 0.87	11.95 ± 0.92
Breakout	u = 30							U= 50						
	PARL	TPARL		TPARL		TPARL (W = 4)	DPARL (Wmax = 5)	PARL	TPARL		TPARL (W = 3)	TPARL		DPARL (Wmax = 5)
		(W	= 2)	(W	= 3)				(W	= 2)		(W	= 4)	
DQN	58.65 ± 40.83	38.05	± 10.22	26.00	± 12.79	13.50 ± 11.41	36.00 ± 13.39	60.25 ± 31.99	37.90	± 11.55	25.90 ± 9.55	14.95	± 7.24	45.00 ± 13.44
QR-DQN	76.50 ± 79.76	45.25	± 42.70	22.05	± 9.13	19.05 ± 6.91	42.05 ± 15.60	62.80 ± 28.71	32.10	± 9.27	40.25 ± 52.97	17.30	± 7.89	41.00 ± 13.87
C51	51.80 ± 10.74	37.60	± 11.38	24.75	± 12.77	13.55 ± 7.37	34.75 ± 10.91	60.55 ± 20.44	34.85	± 11.92	26.15 ± 9.98	19.65	± 7.30	39.90 ± 14.69
Highway	PARL	TPARL		u = 30	u = 50											
				TPARL		TPARL (W = 4)	DPARL (Wmax = 5)	PARL	TPARL		TPARL		TPARL		DPARL (Wmax = 5)
		(W	= 2)	(W	= 3)				(W	= 2)	(W	3)	(W	= 4)	
DQN	28.07 ± 3.86	19.16	± 10.26	16.83	± 8.35	12.69 ± 6.17	23.55 ± 9.oι	29.05 ± 0.62	16.63	± 9.93	15.85	± 8.52	13.43	± 8.29	22.51 ± 8.01
QR-DQN	28.52 ± 1.22	18.63	± 8.07	15.53	± 7.46	12.77 ± 6.39	23.11 ± 7.94	27.64 ± 3.01	20.59	± 7.70	14.94	± 7.76	14.13	± 7.54	23.49 ± 7.95
C51	28.86 ± 1.08	20.20	± 9.21	13.30	± 8.65	9.36 ± 5.96	21.51 ± 10.04	27.44 ± 4.32	15.12	± 9.36	16.13	± 8.46	10.61	± 6.52	19.94 ± 11.14
H Additional Evaluation Results and Discussions
H.1 Benign Empirical Performance
We present the benign empirical cumulative rewards of the three aggregation protocols (PARL,
TPARL, and DPARL) applied on subpolicies trained using three offline RL algorithms (DQN, QR-
DQN, and C51) in Table 4. The cumulative reward is obtained by running a trajectory of maximum
length H and accumulating the reward achieved at each time step.
We discuss the conclusions on multiple levels. We choose H = 1000 for Atari games and H = 30
for Highway environment and report results averaged over 20 runs. On the RL algorithm level,
we see that QR-DQN achieves the highest score in most cases. On the aggregation protocol level,
temporal aggregation enhances the performance on Freeway while dampens the performance on
Breakout and Highway. We particularly note that the results obtained by using temporal aggregation
(TPARL and DPARL) gives significantly smaller variance compared to the single-step aggregation
PARL, especially in Breakout. This implies that temporal aggregation can help stabilize the policy.
However, the conclusion does not hold in Highway, which is an interesting phenomenon that is
worth investigating. On the partition number level, a larger partition number can give slightly
better results. On the environment level, Freeway is simpler and more stable than Breakout and
Highway.
H.2 Comparison of COPA with Standard Training
We provide additional experimental results on the comparison between the empirical performance
of our COPA and the standard training in terms of the convergence speed (Figure 3) and policy
quality (Table 5).
In Figure 3, we aim to show the comparison of the convergence speed. For our proposed training,
we plot all training curves of the sampled 5 subpolicies in blue, where each subpolicy is trained on
one partition of the dataset. (We do not plot all 50 curves for visual clarity.) For standard training,
we plot the training curve of the standard policy in red, where the single policy is trained on the
entire dataset. We see that on Freeway, most subpolicies converge slower than the standard policy,
but will reach similar convergence value as the standard training one; while there also exist a few
subpolicies that fail to be trained. On Breakout, within 50 epochs, we observe substantial fluctuation
for the training curves of all the policies, as well as large variance for the achieved reward at the last
34
Published as a conference paper at ICLR 2022
30
25
20
15
10
5
0
0	10	20	30	40	50
# epoch
# epoch
# epoch
Figure 3:	The convergence speed of the proposed partition-based training compared with the standard
training. For our proposed training, we plot all training curves of the sampled 5 subpolicies in blue, where
each subpolicy is trained on one partition of the dataset. (We do not plot all 50 curves for visual clarity). For
standard training, we plot the training curve of the standard policy in red, where the single policy is trained on
the entire dataset. In Atari games, we train each policy (or subpolicy) for a fixed number of 50 epochs, where
each epoch consumes 1M randomly sampled training data. In Highway environment, we train each policy (or
subpolicy) for a fixed number of 3000 epochs, where each epoch consumes 1K randomly sampled training
data. The offline RL algorithm used is DQN.
Table 5: The policy quality measured by empirical cumulative reward of the proposed aggregation proto-
cols (PARL, TPARL, and DPARL) compared with the standard training. In our aggregation, we aggregate
over u subpolicies with u equal to 30 or 50 . We report results averaged over 20 runs of varying randomness
in the environment, where each run is an episode of length at most 1000 for Atari games and 30 for Highway
environment. The offline RL algorithm used is DQN.
standard trained policy	u = 30	u = 50 TPAR	TPARL	DPARL	TPARL	DPARL PAKL	(W = 4) (Wmax = 5) PAKL	(W =4) (Wmax = 5)
Freeway 12.00 ± 0.89 Breakout 97.60 ± 117.28 Highway 28.90 ± 0.87	10.90 ± 0.77	12.00 ± 0.45	11.45	±	o.74	10.95 ± 0.97	11.65	± o.65	11.90 ± o.89 58.65 ± 40.83	13.50 ± 11.41	36.00	± 13.39	60.25 ± 31.99	14.95	± 7.24	45.00 ± 13.44 28.07 ± 3.86	12.69 ± 6.17	23.55	±	9.01	29.05 ± 0.62	13.43	± 8.29	22.51 ± 8.01
epoch. Thus, on these two Atari games, we cannot draw conclusions w.r.t. the convergence. Given
more computational resources to run more epochs, we would be able to draw more informative
conclusions. In contrast, on Highway, all subpolicies display similarly good convergence properties,
showing comparable convergence speed and value with the standard training on the entire dataset.
In Table 5, we aim to compare the policy quality of the aggregated policy derived in our COPA
framework (i.e., PARL, TPARL, and DPARL) with the policy obtained from standard training. We
see that on Freeway, our three protocols achieve comparable results with the policy obtained by
standard training on the entire dataset; on Breakout, although the quality of our obtained policies is
lower, our policies are much more stable with significantly lower variance than the standard training;
on Highway, only PARL obtains comparable results to the standard training policy, indicating the
lack of temporal continuity in this environment.
Table 6: Average window size (i.e., PtT=1 Wt /T) for the aggregation protocol DPARL (Wmax = 5) ap-
plied on subpolicies trained using three offline RL algorithms (DQN, QR-DQN, and C51), with the number of
subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over all time steps in 20 runs.
	U = 30			u=50		
	DQN	QR-DQN	C51	DQN	QR-DQN	C51
Freeway	2.69 ± 1.73	2.59 ± 1.71	2.64 ± 1.74	2.79 ± 1.73	2.70 ± 1.73	2.72 ± 1.73
Breakout	2.28 ± 1.46	2.44 ± 1.55	2.32 ± 1.48	2.33 ± 1.48	2.40 ± 1.52	2.39 ± 1.52
Highway	1.97 ± 0.44	2.21 ± 0.27	2.16 ± 0.37	2.23 ± 0.32	2.26 ± 0.24	2.18 ± 0.37
35
Published as a conference paper at ICLR 2022
Table 7: Runtime (unit: seconds) of the aggregation protocol DPARL (Wmax = 5) applied on subpolicies
trained using offline RL algorithm DQN, with the number of subpolicies (i.e., partition number) u equal to 30 or
50. We compare with the standard testing which tests the runtime of a single trained DQN policy without using
our framework. We report results averaged over 20 runs of varying randomness in the environment, where each
run is an episode of length at most 1000.
	standard testing	DPARL (u = 30)	DPARL (u = 50)
Freeway	2.53 ± 0.50	56.37 ± 1.75	79.05 ± 4.27
Breakout	2.00 ± 0.56	72.05 ± 13.09	108.05 ± 8.71
H.3 More Analytical Statistics for DPARL
Selected Window Size. We provide the mean and variance for the selected window sizes in
DPARL in Table 6. In our experiments, the maximum window size Wmax = 5, while the aver-
age selected window sizes are all around half of the maximum value. Thus, the average certification
time is expected to be much smaller compared with the worst-case time complexity.
Running Time. We provide the running time in Table 7. Specifically, we compare the running
time of DPARL (Wmax = 5) applied on u = 30 or 50 subpolicies trained using offline RL algorithm
DQN with the normal testing which tests the runtime of a single trained DQN policy without using
our framework. As we have shown in the remark of Theorem 3 in Section 4.3, the time complexity of
DPARL is O (WImIaχ∣A∣2U + WmaX|A|2u log u). In Table 7, We also see that the runtime ofDPARL
scales roughly quasilinearly with the number of subpolicies u, and quadratically with the action set
size |A|, Which is 3 for FreeWay and 4 for Breakout. We omit the runtime for HighWay, since the
horizon length, the environment type, and the neural netWork size are all different for HighWay and
Atari games.
H.4 Maximum Tolerable Poisoning Threshold vs. Total Trajectory Number
We provide the total number of trajectories in the offline dataset used for training the environments
FreeWay, Breakout and HighWay beloW, as Well as the corresponding ratio of the maximum tolerable
poisoning threshold w.r.t. the total number of trajectories. For Freeway, the total number of trajec-
tories in the entire offline dataset is 121,892, and the ratio (max Kt / # total trajectories) is 0.0002;
for Breakout, the number is 209,049, and the ratio is 0.0001. We emphasize that, as shown in previ-
ous work on probable defense against poisoning in supervised learning (Levine & Feizi, 2020), the
number of instances they can certify is also not high especially for challenging tasks (e.g., certifying
20 instances on the dataset GTSRB in Levine & Feizi (2020), attaining the ratio 0.0005), which
may imply the intrinsic difficulty of certifying against poisoning attacks. Given such difficulty, we
already achieve reasonable certification as the first work on certified robust RL against poisoning,
and we hope future works can further improve upon our results.
H.5 Comparison of Freeway, Breakout, and Highway
Freeway and Breakout are two typical types of games with distinctive game properties. Freeway is a
simple game where the agent aims to cross the road and avoid the traffic. In most of the time steps,
the agent would take the “forward” action; only when there is a need to avoid the traffic will the agent
stop and wait. In comparison, the game Breakout involves more complicated interactions between
the paddle and the environment. The paddle position and velocity both play important roles in order
to catch and bounce the ball at an appropriate angle, which requires a frequent switch of the paddle
moving direction. Similar to Breakout, as an autonomous driving environment, Highway requires
the agent to accurately analyze the rapidly changing environment around it and react quickly, leading
to frequent changes in vehicle’s actions.
Given the properties of the environments, we note that in Freeway, adjacent time steps may share
consistent action selections, which is not necessarily true in the Breakout and Highway. Thus, it
would be expected to be beneficial to consider the past history for making the current decision in
Freeway, while the past history may interfere with the action selection in Breakout and Highway.
36
Published as a conference paper at ICLR 2022
Table 8: Action change ratio (in percentage) of three aggregation protocols (PARL, TPARL, and DPARL)
applied on subpolicies trained in Highway environment using three offline RL algorithms (DQN, QR-DQN, and
C51), with the number of subpolicies (i.e., #partitions) u equal to 30, 50, or 100. We report results averaged
over 20 runs of varying randomness in the environment.
u	RL Algorithm	Aggregation Protocol		
		PARL	TPARL (W = 4)	DPARL (Wmax = 5)
	DQN	59.79 ± 8.99	24.89 ± 10.47	33.82 ± 8.75
30	QR-DQN	61.00 ± 12.74	31.14 ± 6.43	37.80 ± 7.16
	C51	59.00 ± 10.39	18.17 ± 11.52	35.44 ± 8.65
	DQN	55.50 ± 8.84	26.05 ± 11.69	40.07 ± 13.74
50	QR-DQN	60.37 ± 10.31	28.48 ± 6.81	37.78 ± 12.15
	C51	59.39 ± 12.34	28.04 ± 10.71	38.21 ± 11.23
	DQN	48.67 ± 5.9i	22.17 ± 7.65	33.45 ± 9.70
100	QR-DQN	61.17 ± 8.38	24.12 ± 9.85	36.76 ± 7.87
	C51	61.83 ± 10.41	22.91 ± 13.34	34.06 ± 9.19
Comparisons of Bottleneck states in Atari Games. In Breakout, the game goal is to control
the paddle so that the ball is bounced to hit the brick. There are clearly very different stages in
the Breakout game, e.g., when the ball is flying towards the paddle and when it is bounced back.
An example of the bottleneck state is when the ball is approaching the paddle. The poisoning
behavior may lead to the disastrous effect that the policy learns to control the paddle to slide in the
opposite direction of the ball at such bottleneck states, while other states may not be as vulnerable.
In comparison, in Freeway, the game goal is to control the agent to cross the road while avoiding
the traffic. The agent is faced with similar traffic conditions everywhere on the road, and can make
stable choices regardless of the complex situation. Thus there are very few bottleneck states.
A Study on the Frequency of Action Change in Highway. In Table 8, we present the action
change ratio (# action changes / trajectory length) in Highway environment. Comparing among
the aggregation protocols, we first note that the single-step aggregation policy πP frequently alters
actions as the reaction to the rapidly changing driving environment, while the temporal aggregation
protocol πT changes actions less frequently. This is because in TPARL, the agent is explicitly forced
to take stable actions based on a fixed window size. Second, comparing the action change ratio with
the benign empirical performance in Table 4, we observe that πP and πT achieves the highest and
lowest empirical performance respectively, which indicates the correlation between action change
ratio with the achieved empirical reward. Specifically, in TPARL, the action change is limited as a
result of the enforced window size, leading to the limited benign performance.
H.6 Full Results of Robustness Certification for Per-State Action Stability
As a complete set of results for per-state action certification apart from Figure 1 in Section 5.1, we
present the entire cumulative histogram of tolerable poisoning thresholds in Figure 4 and Figure 5,
together with the average tolerable poisoning thresholds in Table 9. The definitions of the two
metrcis can be found in Section 5.1.
Basically, the conclusions in terms of the comparisons on the RL algorithm level, the aggregation
protocol level, the partition number level, and the game level are all similar to that derived in Sec-
tion 5.1.
H.7 Full Results of Robustness Certification for Cumulative Reward Bound
In addition to the evaluation results provided in Figure 2 in section 5.2, we provide a more compre-
hensive set of evaluation results with more settings of trajectory length H here in Figure 6.
We draw similar conclusions as discussed in Section 5.2. Specifically, in Highway, C51 achieves
higher certified lower bounds than other two when the poisoning size is large, which can be explained
by the larger portion of states that can tolerate large poisoning sizes as shown in Figure 5.
37
Published as a conference paper at ICLR 2022
PARL	TPARL(W=4)	DPARL(WmaX = 5)
(a) Freeway
ι.o
0.8
0.6
0.4
0.2
0.0
⅛ *・* *⅛x *** 4上.
0123456789 1011121314
1.0
0.8
0.6
0.4
0.2
0.0
≥ K
(b) Breakout
Figure 4:	Robustness certification for per-state action stability on Atari games (full results). We plot the
cumulative histogram of the tolerable poisoning size K for all time steps in one trajectory. We provide results
on two games (Freeway and Breakout), two partition numbers (u = 30 and u = 50), and a comparison of
three certification methods (PARL, TPARL, and DPARL). The results are averaged over 20 runs considering
the randomness in the game environment, and the short vertical bar on top of each bar represents the standard
deviation.
38
Published as a conference paper at ICLR 2022
PARL	TPARL(W=4)	DPARL(WmaX = 5)
oitar ytilibat
NQD-RQ
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
Highway, u
01234567891011121314
01234567891011121314
≥ K
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
Highway, u = 50
0123456789 101112131415161718192021222324
0123456789 101112131415161718192021222324
0.8
0.6
0.4
0.2
0.0
0123456789 101112131415161718192021222324
≥ K
Highway, u = 100
0123456789 10111213141516171819202122232425262728293031323334353637383940414243444546474849
oitar ytilibat
NQD-RQ
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
iiιh Jl J*Illkh'LkLk⅛u.kiu，■+&*+/U *“ U., i..
0123456789 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0123456789 10111213141516171819202122232425262728293031323334353637383940414243444546474849
≥ K
Figure 5:	Robustness certification for per-state action stability on Highway environment (full results).
We plot the cumulative histogram of the tolerable poisoning size K for all time steps in one trajectory. We
provide results on Highway environment, three partition numbers (u = 30, u = 50 and u = 100), and a
comparison of three certification methods (PARL, TPARL, and DPARL). The results are averaged over 20 runs
considering the randomness in the environment, and the short vertical bar on top of each bar represents the
standard deviation.
---⅛---- U = 30, PARL …∙γ...... U = 30, TPARL (IV = 4)	--U = 30, DPARL(lVmax = 5)
* U = 50, PARL ........∙.... U	= 50, TPARL (W = 4)	-*----U = 50, DPARL (WmaX = 5)
Freeway, H = 100 Freeway, H = 200
Freeway, H = 400
O 5 IO 15 20 25
Breakout, H = 50
ZOQ.ao
0 O 3 6 9 12 15 18 21
Poisoning size K Poisoning size K Poisoning size K Poisoning size K Poisoning size K Poisoning size K
Figure 6: Robustness certification for cumulative reward (full results). We plot the lower bound of cumu-
lative reward JK w.r.t. poisoning size K under three different certification methods (PARL, TPARL (W = 4),
DPARL (Wmax = 5)) with two partition numbers (u ∈ {30, 50}). Each row corresponds to one RL algorithm,
and each column corresponds to one setting of trajectory length H .
39
Published as a conference paper at ICLR 2022
Table 9: Average tolerable poisoning thresholds of three aggregation protocols (PARL, TPARL, and
DPARL) applied on subpolicies trained using three offline RL algorithms (DQN, QR-DQN, and C51), with
the number of subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of
varying randomness in the environment.
Freeway	U = 30				u=50			
					PARL	TPARL		DPARL (WmaX = 5)
	PARL	TPARL		DPARL (WmaX = 5)				
		(W	= 4)			(W	= 4)	
DQN	9.90 ± 0.14	10.21	± 0.09	10.30 ± 0.09	16.78 ± 0.42	17.16	± 0.29	16.88 ± 0.45
QR-DQN	9.87 ± 0.20	10.11	± 0.30	10.16 ± 0.34	16.79 ± 0.52	17.31	± 0.29	17.03 ± 0.40
C51	9.57 ± 0.24	9.83	± 0.15	10.11 ± 0.19	16.82 ± 0.40	17.08	± 0.34	17.61 ± 0.15
Breakout	u = 30			u=50		
	PARL	TPARL (W = 4)	DPARL (WmaX = 5)	PARL	TPARL (W = 4)	DPARL (WmaX = 5)
DQN	1.08 ± 0.09	1.28 ± 0.15	0.85 ± 0.09	2.07 ± 0.22	1.92 ± 0.30	1.55 ± 0.28
QR-DQN	1.38 ± 0.09	1.33 ± 0.18	1.18 ± 0.10	2.12 ± 0.27	1.93 ± 0.14	1.71 ± 0.19
C51	1.10 ± 0.08	1.42 ± 0.19	0.93 ± 0.13	2.43 ± 0.21	2.37 ± 0.19	1.90 ± 0.19
						
Highway		u=30			u=50	
	PARL	TPARL	DPARL	PARL	TPARL	DPARL
						
		(W = 4)	(WmaX = 5)		(W = 4)	(WmaX = 5)
DQN	4.38 ± 0.78	2.99 ± 1.08	3.75 ± 1.85	7.16 ± 1.58	6.27 ± 1.61	6.32 ± 0.98
QR-DQN	4.18 ± 0.91	3.35 ± 1.44	2.84 ± 0.55	6.52 ± 1.17	5.02 ± 1.60	4.61 ± 1.20
C51	4.97 ± 1.09	4.31 ± 1.45	4.12 ± 0.95	7.95 ± 1.46	6.55 ± 2.20	6.69 ± 1.47
We additionally point out that the value JK achieved at poisoning size K = 0 (e.g., JK = 4
for Freeway, 2 for Breakout, and 28.31 for Highway under πP over u = 50 DQN subpolicies)
corresponds to the case where there is no poisoning at all on the training set. Our successive bounds
under larger K are non-vacuous compared to this value.
I A Broader Discussion on Related Work
I.1	Poisoning Attacks in RL
Below, we provide a brief discussion on the related works on policy poisoning and reward poison-
ing (Ma et al., 2019; Sun et al., 2021; Huang & Zhu, 2019).
Ma et al. (2019) and Huang & Zhu (2019) study reward poisoning where the attacker can modify
the rewards in an offline dataset or the reward signals during the online interaction. The attacker’s
goal is to force learning a particular target policy, or minimize the agent’s reward in the original
task. Under this framework, Ma et al. (2019) considers two victim learners with specific assump-
tions on the learner structure and develops attacks for them, while Huang & Zhu (2019) provides a
theoretical analysis on the conditions for successful attacks against a Q-learning agent. In compari-
son to only reward poisoning in Ma et al. (2019) and Huang et al. (2017), Sun et al. (2021) focuses
on general policy poisoning attacks for policy gradient learners in the online RL setting by using
the vulnerability-awareness metric to decide when to poison, and the adversarial critic to guide the
poisoning. Sun et al. (2021) achieve successful poisoning against policy-based agents in various
complex environments in online RL, but it remains an interesting open problem as to how to poison
the offline RL dataset which is agnostic to the learning algorithm.
I.2	Empirically Robust RL
Robust RL against Evasion Attacks. We briefly review several categories of RL methods that
demonstrate empirical robustness against evasion attacks.
40
Published as a conference paper at ICLR 2022
Randomization methods (Tobin et al., 2017; Akkaya et al., 2019) were first proposed to encourage
exploration. This type of method was later systematically studied for its potential to improve model
robustness. NoisyNet (Fortunato et al., 2017) adds parametric noise to the network’s weight during
training, providing better resilience to both training-time and test-time attacks (Behzadan & Munir,
2017; 2018), also reducing the transferability of adversarial examples, and enabling quicker recovery
with fewer number of transitions during phase transition.
Under the adversarial training framework, Kos & Song (2017) and Behzadan & Munir (2017)
show that re-training with random noise and FGSM perturbations increases the resilience against
adversarial examples. Pattanaik et al. (2018) leverage attacks using an engineered loss function
specifically designed for RL to significant increase the robustness to parameter variations. RS-
DQN (Fischer et al., 2019) is an imitation learning based approach that trains a robust student-DQN
in parallel with a standard DQN in order to incorporate the constrains such as SOTA adversarial
defenses (Madry et al., 2017; Mirman et al., 2018).
SA-DQN (Zhang et al., 2020a) is a regularization based method that adds regularizers to the training
loss function to encourage the top-1 action to stay unchanged under perturbation.
Built on top of the neural network verification algorithms (Gowal et al., 2018; Weng et al., 2018),
Radial-RL (Oikarinen et al., 2020) proposes to minimize an adversarial loss function that incor-
porates the upper bound of the perturbed loss, computed using certified bounds from verification
algorithms. CARRL (Everett et al., 2021) aims to compute the lower bounds of action-values under
potential perturbation and select actions according to the worst-case bound, but it relies on linear
bounds (Weng et al., 2018) and is only suitable for low-dimensional environments.
These robust RL methods only provide empirical robustness against perturbed state inputs during
test time, but cannot provide theoretical guarantees for the performance of the trained models under
any bounded perturbations.
Robust RL against Poisoning Attacks. Banihashem et al. (2021) considers the threat model of
reward poisoning attacks proposed by Ma et al. (2019); Rakhsha et al. (2020); Zhang et al. (2020b),
where the attacker aims to force learning a target policy while minimizing the cost of reward manip-
ulation. As shown in Ma et al. (2019); Rakhsha et al. (2020); Zhang et al. (2020b), this optimization
problem is feasible and always has a unique optimal solution. Leveraging this property, Banihashem
et al. (2021) formulates the defense task as another optimization problem which aims to optimize the
agent’s worst case performance among the set of plausible candidates of the true reward function.
They specifically consider two settings regarding the agent’s knowledge of the attack parameter, and
provide lower bounds on the performance of the defense policy. In contrast to Banihashem et al.
(2021) which focuses on a specific type of attack, our paper considers the threat model of general
poisoning attacks, where the attacker has the power to manipulate the training trajectories arbitrarily.
Given limited knowledge regarding the attack, our proposed COPA framework is general and appli-
cable to any potential attack. Our robustness is derived from the aggregation over both sub-policy
level and temporal level. One similarity between Banihashem et al. (2021) and our work is that we
both aim to provide lower bounds of the cumulative reward for our proposed method as the provable
guarantee / certification criteria.
I.3	Robustness Certification for RL
Wu et al. (2022) provide the first robustness certification for RL against test-time evasion attacks
following the line of work on randomized smoothing (Cohen et al., 2019; Salman et al., 2019). Con-
cretely, they apply per-state smoothing to achieve the certification for per-state action stability, as
well as trajectory smoothing to obtain the certification for cumulative reward. Notably, Wu et al.
(2022) propose an adaptive tree search algorithm to explore all possible trajectories and thus de-
rive the robustness guarantee for cumulative reward. The relationship between our COPA-Search
and their Algorithm 3 is discussed in detail in Appendix E.3. In comparison, robustness in their
work is derived from smoothing, while our robustness comes from aggregation. We propose aggre-
gation protocols and certification methods that leverage the temporal information by dynamically
aggregating over the past time steps, which is not covered in Wu et al. (2022).
41