Published as a conference paper at ICLR 2022
The Convex Geometry of Backpropagation:
Neural Network Gradient Flows Converge to
Extreme Points of the Dual Convex Program
Yifei Wang
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
wangyf18@stanford.edu
Mert Pilanci
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
pilanci@stanford.edu
Ab stract
We study non-convex subgradient flows for training two-layer ReLU neural net-
works from a convex geometry and duality perspective. We characterize the implicit
bias of unregularized non-convex gradient flow as convex regularization of an equiv-
alent convex model. We then show that the limit points of non-convex subgradient
flows can be identified via primal-dual correspondence in this convex optimization
problem. Moreover, we derive a sufficient condition on the dual variables which
ensures that the stationary points of the non-convex objective are the KKT points
of the convex objective, thus proving convergence of non-convex gradient flows
to the global optimum. For a class of regular training data distributions such as
orthogonal separable data, we show that this sufficient condition holds. Therefore,
non-convex gradient flows converge to optimal solutions of a convex optimization
problem. We present numerical results verifying the predictions of our theory for
non-convex subgradient descent.
1	Introduction
Neural networks (NNs) exhibit remarkable empirical performance in various machine learning tasks.
However, a full characterization of the optimization and generalization properties of NNs is far from
complete. Non-linear operations inherent to the structure of NNs, over-parameterization and the
associated highly nonconvex training problem makes their theoretical analysis quite challenging.
In over-parameterized models such as NNs, one natural question arises: Which particular solution does
gradient descent/gradient flow find in unregularized NN training problems? Suppose that X ∈ RN ×d
is the training data matrix and y ∈ {1, -1}N is the label vector. For linear classification problems
such as logistic regression, it is known that gradient descent (GD) exhibits implicit regularization
properties, see, e.g., (Soudry et al., 2018; Gunasekar et al., 2018). To be precise, under certain
assumptions, GD converges to the following solution which maximizes the margin:
argmin 1 ∣∣wk2, s.t. ynwτXn ≥ 1,n ∈ [N].	(1)
w∈Rd 2
Here we denote [N] = {1, . . . , N}. Recently, there are several results on the implicit regularization
of the (stochastic) gradient descent method for NNs. In (Lyu & Li, 2019), for the multi-layer
homogeneous network with exponential or cross-entropy loss, with separable training data, it is
shown that the gradient flow (GF) and GD finds a stationary point of the following non-convex
max-margin problem:
arg min 1 ∣∣θ∣2, s.t. ynf (θ; Xn) ≥ 1,n ∈ [N],	(2)
θ2
where f(θ; X) represents the output of the neural network with parameter θ given input X. In (Phuong
& Lampert, 2021), by further assuming the orthogonal separability of the training data, it is shown
that all neurons converge to one of the two max-margin classifiers. One corresponds to the data with
positive labels, while the other corresponds to the data with negative labels.
1
Published as a conference paper at ICLR 2022
However, as the max-margin problem of the neural network (2) is a non-convex optimization problem,
the existing results only guarantee that it is a stationary point of (2), which can be a local minimizer
or even a saddle point. In other words, the global optimality is not guaranteed.
In a different line of work (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020; 2021b), exact convex
optimization formulations of two and three-layer ReLU NNs are developed, which have global
optimality guarantees in polynomial-time when the data has a polynomial number of hyperplane
arrangements, e.g., in any fixed dimension or with convolutional networks of fixed filter size. The
convex optimization framework was extended to vector output networks (Sahiner et al., 2021b),
quantized networks (Bartan & Pilanci, 2021b), autoencoders (Sahiner et al., 2021c; Gupta et al.,
2021), networks with polynomial activation functions (Bartan & Pilanci, 2021a), networks with batch
normalization (Ergen et al., 2021), univariate deep ReLU networks, deep linear networks (Ergen &
Pilanci, 2021c) and Generative Adversarial Networks (Sahiner et al., 2021a).
In this work, we first derive an equivalent convex program corresponding to the maximal margin
problem (2). We then consider non-convex subgradient flow for unregularized logistic loss. We show
that the limit points of non-convex subgradient flow can be identified via primal-dual correspondence
in the convex optimization problem. We then present a sufficient condition on the dual variable to
ensure that all stationary points of the non-convex max-margin problem are KKT points of the convex
max-margin problem. For certain regular datasets including orthogonal separable data, we show that
this sufficient condition on the dual variable holds, thus implies the convergence of gradient flow
on the unregularized problem to the global optimum of the non-convex maximalo margin problem
(2). Consequently, this enables us to fully characterize the implicit regularization of unregularized
gradient flow or gradient descent as convex regularization applied to a convex model.
1.1	Related Work
There are several works studying the property of two-layer ReLU networks trained by gradient
descent/gradient flow dynamics. The following papers study the gradient descent like dynamics in
training two-layer ReLU networks for regression problems. Ma et al. (2020) show that for two-layer
ReLU networks, only a group of a few activated neurons dominate the dynamics of gradient descent.
In (Mei et al., 2018), the limiting dynamics of stochastic gradient descent (SGD) is captured by
the distributional dynamics from a mean-field perspective and they utlize this to prove a general
convergence result for noisy SGD. Li et al. (2020) focus on the case where the weights of the second
layer are non-negative and they show that the over-parameterized neural network can learn the
ground-truth network in polynomial time with polynomial samples. In (Zhou et al., 2021), it is shown
that mildly over-parameterized student network can learn the teacher network and all student neurons
converge to one of the teacher neurons.
Beyond (Lyu & Li, 2019) and (Phuong & Lampert, 2021), the following papers study the classification
problems. In (Chizat & Bach, 2018), under certain assumptions on the training problem, with over-
parameterized model, the gradient flow can converge to the global optimum of the training problem.
For linear separable data, utilizing the hinge loss for classification, Wang et al. (2019) introduce a
perturbed stochastic gradient method and show that it can attain the global optimum of the training
problem. Similarly, for linear separable data, Yang et al. (2021) introduce a modified loss based on
the hinge loss to enable (stochastic) gradient descent find the global minimum of the training problem,
which is also globally optimal for the training problem with the hinge loss.
1.2	Problem setting
We focus on two-layer neural networks with ReLU activation, i.e., f(θ, X) = (XW1)+w2, where
W1 ∈ Rd×m, w2 ∈ Rm and θ = (W1, w2) represents the parameter. Due to the ReLU activation,
this neural network is homogeneous, i.e., for any scalar c > 0, we have f (cθ; X) = c2f (θ; X). The
training problem is given by
N
mmin X '(ynf(θ; Xn)),	⑶
n=1
where '(q) : R → R+ is the loss function. We focus on the logistic, i.e, cross-entropy loss, i.e.,
'(q) = log(1+exp(-q)).
2
Published as a conference paper at ICLR 2022
Then, we briefly review gradient descent and gradient flow. The gradient descent takes the update rule
θ(t+ 1) = θ(t) - η(t)g(t),
where g(t) ∈ ∂°L(θ(t)) and ∂° represents the Clarke’s subdifferential.
The gradient flow can be viewed as the gradient descent with infinitesimal step size. The trajectory
of the parameter θ during training is an arc θ : [0, +∞) → Θ, where Θ = {θ = (W1, w2)|W1 ∈
Rd×m , W2 ∈ Rm}. More precisely, the gradient flow is given by the differential inclusion
-dθ(t) ∈ -∂°L(θ(t)),	for t ≥ 0, almost everywhere.
dt
2 Main Results
In this section, we present our main results and defer the detailed analysis to the following sections.
Consider the more general multi-class version of the problem with K classes. Suppose that y ∈ [K ]N
is the label vector. Let Y = (yn,k)n∈[N],k∈[K] ∈ RN×K be the encoded label matrix such that
_ ∫1,	if yn = k,
yn,k	- 1,	otherwise.
Similarly, we consider the following two-layer vector-output neural networks with ReLU activation:
-f1(θ1, X) ] J(XW(I))+w21) 一
F(Θ, X)=	.	=	.	,
fK(θK,X)	(XW1(K))+w(2K)
where we write Θ = (θ1, . . . , θK). For k = 1, . . . , K, we have θk = (W1(k), w2(k)) where
W1(k) ∈ RN×m and w2(k) ∈ Rm. One can view each of the K outputs of F(Θ, X) as the output of a
two-layer scalar-output neural network. Consider the following training problem:
KN
minɪ^ E'(yn,k fk(θk , Xn)).
(4)
k=1 n=1
According to (Lyu & Li, 2019), the gradient flow and the gradient descent finds a stationary point of
the following non-convex max-margin problem:
K1
argmin	2l∣θkk2, s.t. yn,k f(θk； Xn) ≥ 1,n ∈ [N], k ∈ [K].
Denote the set of all possible hyperplane arrangement as
P = {diag(I(Xw ≥ 0))|w ∈ Rd},
and let p = |P|. We can also write P = {D1, . . . , Dp}. From (Cover, 1965), we have an upper
boundP ≤ 2r (e(NT)) where r = rank(X). We first reformulate (5) as convex optimization.
Proposition 1 The non-convex problem (5) is equivalent to the following convex program
Kp
min X X(luj,k l2 + lu0j,k l2),
k=1 j=1
p
s.t. diag(yk)	DjX(uj,k - u0j,k) ≥ 1,
j=1
(2Dj - I)Xuj,k ≥ 0, (2Dj -I)Xu0j,k ≥0,j∈ [p],k∈ [K].
where yk is the k-th column of Y. The dual problem of (7) is given by
max tr(ΛT Y),
St diag(yk)λk 占 0, max ∣λT(XTw)+∣ ≤ 1,k ∈ [K].
kwk2≤1
(5)
(6)
(7)
(8)
where λk is the k-th column of Λ.
3
Published as a conference paper at ICLR 2022
We present the detailed derivation of the convex formulation (7) and its dual problem (8) in the
appendix. Given u ∈ Rd, we define D(u) = diag(I(Xu > 0)). For two vectors u, v ∈ Rd, we
define the cosine angle between u and v by cos ∠(u, v)
UT V
lluk2kvk2
2.1	Our contributions
The following theorem illustrate that for neurons satisfying sign(ykT (Xw1(k,i))+) = sign(w2(k,i) ) at
initialization, w1(k,i) align to the direction of ±XT D(w(1k,i))yk at a certain time T, depending on
sign(w2(k,i) ) at initialization. In Section 2.3, we show that these are dual extreme points of (7).
Theorem 1 Consider the K -class classification training problem (4) for any dataset. Suppose that
the neural network is scaled at initialization such that kw1(k,i) k2 = |w2(k,i) | for i ∈ [m] and k ∈ [K].
Assume that at initialization, for k ∈ [K], there exists neurons (w(1k,i) , w2(k,i) )such that
sign(ykT (Xw(1k,i)k)+) = sign(w2(k,i)k) = s,	(9)
where s ∈ {1, -1}. Consider the subgradient flow applied to the non-convex problem (4). Let
δ ∈ (0, 1). Suppose that the initialization is sufficiently close to the origin. Then, for k ∈ [K], there
exist T = T (δ, k) such that
cos∠ w(1k,i)k (T), sXT D(w(1k,i)k (T))yk ≥ 1 - δ.
Next, we impose conditions on the dataset to prove a stronger global convergence results on the flow.
We say that the dataset (X, y) is orthogonal separable among multiple classes if for all n, n0 ∈ [N],
Xn xn0 > 0, if yn = yn0,
XTXn ≤ 0, if yn = yn0.
For orthogonal separable dataset among multiple classes, the subgradient flow for the non-convex
problem (4) can find the global optimum of (5) up to a scaling constant.
Theorem 2 Suppose that (X, y) ∈ RN×d X [K]N is orthogonal separable among multiple classes.
Consider the non-convex subgradient flow applied to the non-convex problem (4). Suppose that the
initialization is sufficiently close to the origin and scaled as in Theorem 1. Then, the non-convex
subgradient flow converges to the global optimum of the convex program (7) and hence the non-convex
objective (5) up to scaling.
Therefore, the above result characterizes the implicit regularization of unregularized gradient flow as
convex regularization, i.e., group `1 norm, in the convex formulation (7). It is remarkable that group
sparsity is enforced by small initialization magnitude with no explicit form of regularization.
2.2	Convex Geometry of Neural Gradient Flow
Suppose that λ ∈ RN . Here we provide an interesting geometric interpretation behind the formula
cos ∠(u, XT D(u)λ) > 1 - δ.
which describes a dual extreme point to which hidden neurons approach to as predicted by
Theorem 1. We now explain the geometric intuition behind this result. Consider an ellipsoid
{Xu : kuk2 ≤ 1}. A positive extreme point of this ellipsoid along the direction λ is defined by
argmaxu： ∣∣u∣∣2≤1 λτXu, which is given by the formula k^^. Next, We consider the rectified
ellipsoid set Q := {(Xu)+ : kuk2 ≤ 1} introduced in (Ergen & Pilanci, 2021a) and shown in
Figure 1. The constraint maXuRuk2≤1 ∣λτ(Xu)+1 ≤ 1 on λ is equivalent to λ ∈ Q*. Here Q* is
the absolute polar set of Q, which appears as a constraint in the convex program (8) and is defined as
the following convex set
Q* = {λ : max ∣λτz| ≤ 1}.	(10)
z∈Q
4
Published as a conference paper at ICLR 2022
An extreme point of this non-convex body along the direction λ is given by the solution of the
problem
max λT (Xu)+ = max	max	λT Dj Xu.	(11)
u : kuk2≤1	Dj∈P u : kuk2 ≤1,(2Dj -I)Xu≥0
Here, (λ, u) are primal-dual pairs as they appear in the convex dual program (8). First, note that a
stationary point of gradient flow on the objective in (11) is given by the identity CU ∈ ∂Uλτ (Xu)+
where c is a constant. In particular, by picking the zero as the subgradient of (xTn u)+ when xTnu = 0,
U = XTD(u)λ = Pin=I λnXnI(uTXn > 0)
U = kχTD(U)λk2 = k PN=I λnXnI(uTXn > 0)∣∣2，
(12)
Note that the formula cos ∠(U, XT D(U)λ) > 1 - δ appearing in Theorem 1 shows that gradient flow
reaches the extreme points of projected ellipsoids {Dj XU : kUk2 ≤ 1} in the direction of λ = yk,
where Dj ∈ P corresponds to a valid hyperplane arrangement. This interesting phenomenon is
depicted in Figures 3 and 4. The one-dimensional spikes in Figures 1 and 3 are projected ellipsoids.
Detailed setup for Figure 1 to 4 and additional experiments can be found in Appendix F.
Figure 1: Rectified Ellipsoid Q := {(XU)+ :
kUk2 ≤ 1} and its extreme points (spikes).
Figure 2: Convex absolute polar set Q* of
the Rectified Ellipsoid (purple) and other dual
Figure 3: Trajectories of (Xw^ι,i)+ along the
training dynamics of gradient descent.
Figure 4: Trajectories of w^ι,i
wi,i
kw1,ik2
along the training dynamics of gradient de-
scent.
Figure 5: Two-layer ReLU network gradient descent dynamics on an orthogonal separable dataset.
w^ι,i = kw1；］ is the normalized vector of the i-th hidden neuron in the first layer.
5
Published as a conference paper at ICLR 2022
(14)
(15)
3	Convex max-margin problem
In this section, we consider the equivalent convex model of the max-margin problem and its optimality
conditions. We primarily focus on the binary classification problem for simplicity, which are later
extended to the multi-class case. We can reformulate the nonconvex max-margin problem (2) as
min1(kWιkF + I∣w2k2), s.t. Y(XW1)+w2 ≥ 1,	(13)
where Y = diag(y). This is a nonconvex optimization problem due to the ReLU activation and
the two-layer structure of neural network. Analogous to the convex formulation introduced in
(Pilanci & Ergen, 2020) for regularized training problem of neural network, we can provide a convex
optimization formulation of (13) and derive the dual problem.
Proposition 2 The problem (13) is equivalent to
p
Pc*vx = min X(Iluj k2 + Iluj k2),
j=1
p
s.t. Y X Dj X(u0j - uj) ≥ 1,
j=1
(2Dj -I)Xuj ≥ 0, (2Dj -I)Xu0j ≥0,∀j ∈ [p]
The dual problem of (14) is given by
D* = maxyτλ s.t. Yλ 占 0, max ∣λτ(XTu)+∣ ≤ 1.
λ	u"∣u∣∣2≤1
The following proposition gives a characterization of the KKT point of the non-convex max-margin
problem (2). The definition of B-subdifferential can be found in Appendix A.
Proposition 3 Let (W1, w2, λ) be a KKT point of the non-convex max-margin problem (2) (in terms
of B-subdifferential). Suppose that w2,i 6= 0 for certain i ∈ [m]. Then, there exists a diagonal matrix
DDi ∈ RN ×N satisfying
T
(Di)n = 1, for xTn w1,i > 0,
(Di)n ∈ {0,1}, for XTWl,i = 0,
(Di)n = 0, for XTWl,i < 0.
such that
Wii = XT Diλ,∣xτ D iλ∣2 = 1.
w2,i
Based on the characterization of the KKT point of the non-convex max-margin problem (2), we
provide an equivalent condition to ensure that it is also the KKT point of the convex max-margin
problem (14).
Theorem 3 The KKT point of the non-convex max-margin problem (13) (in terms of B-subdifferential)
corresponds to a KKT point of the convex max-margin problem (14) ifλ is dual feasible, i.e.,
max ∣λτ(Xu)+1 ≤ 1.	(16)
Bkuk2≤1
This condition is equivalent to for all Dj ∈ P, the dual variable λ satisfies that
max	∣λτDjXu| ≤ 1.	(17)
kuk2≤1,(2Dj-I)Xu≥0
3.1 Dual feasibility of the dual variable
A natural question arises: is it possible to examine whether λ is feasible in the dual problem? We say
the dataset (X, y) is orthogonal separable if for all n, n0 ∈ [N],
Xn Xn0 > 0, if yn = yn0 ,
XTn Xn0 ≤ 0, ifyn 6= yn0.
6
Published as a conference paper at ICLR 2022
For orthogonal separable data, as long as the induced diagonal matrices in Proposition 3 cover the
positive part and the negative part of the labels, the KKT point of the non-convex max-margin problem
(2) is the KKT point of the convex max-margin problem (14).
Proposition 4 Suppose that (X, y) is orthogonal separable. Suppose that the KKT point of the non-
convex problem include two neurons (w1,i+ , w2,i+) and (w1,i- , w2,i-) such that the corresponding
diagonal matrices Di+ and Di- defined in Proposition 3 satisfy that
ʌ _ . ,— , 一 ʌ 一 ,一, 一
Di+ ≥ diag(I(y = 1)),	Di- ≥ diag(I(y = -1)).
Then, the dual variable λ is dual feasible, i.e., satisfying (16).
The spike-free matrices discussed in (Ergen & Pilanci, 2021a) also makes examining the dual
feasibility of λ easier. The definition of spike-free matrices can be found in Appendix A
Proposition 5 Suppose that X is spike-free. Suppose that the KKT point of the non-convex problem
include two neurons (w1,i+ , w2,i+ ) and (w1,i- , w2,i- ) such that the corresponding diagonal matrices
τ∖	1 τ∖	14	1 ■ JΛ	■ , ■ C . ■	.1
Di+ and Di- defined in Proposition 3 satisfy that
ʌ _ . ,— , 一 ʌ 一 ,一, 一
Di+ ≥ diag(I(y = 1)),	Di- ≥ diag(I(y = -1)).
Then, the dual variable λ is dual feasible, i.e., satisfying (16).
Remark 1 For the spike-free data, the constraint on the dual problem is equivalent to
max	∣λτXu| ≤ 1,	or equivalently
Xu≥0,kuk2≤1
max	λTY+Xu ≤ 1, min λTY-Xu ≥ -1.
Xu≥0,kuk2≤1	Xu≥0
4 Sub-gradient flow dynamics of logistic loss
In this section, we consider the following sub-gradient flow of the logistic loss (3)
∂ W1,i⑴=w2,i⑴( X	黑⑴Xn⑴1
n:(w1,i(t))Txn>0
∂N
∂tw2,i⑴=E 晨⑴((W1,i⑴)TXn⑴)+.
n=1
where the n-th entry of λe(t) ∈ RN is defined
λn = -yn' (qn),	qn = yn(Xn ^Wl) + w2∙
(18)
(19)
For simplicity, we omit the term (t). For instance, we write W1,i = W1,i(t). To be specific,
when w1T,iXn = 0, we select 0 as the subgradient of w2,i (w1T,iXn)+ with respect to w1,i. Denote
σi = sign(Xui). For σ ∈ {1, -1, 0}N, we define
g(σ,λ)= T AnXn.	(20)
n：bn>0
For simplicity, we also write
g(u, λe) := g(sign(Xu), λ) = X	λAnXn.	(21)
n：w1T,ixn>0
Then, we can rewrite sub-gradient flow of the logistic loss (3) as follows:
∂∂
∂tWi, 1 = W2,ig(u, λ),	∂tWi,2 = WTIg(u, λ).	(22)
Assume that the neural network is scaled at initialization, i.e., kW1,i(0)k22 = w22,i(0) for i ∈ [m].
Then, the neural network is scaled for t ≥ 0.
7
Published as a conference paper at ICLR 2022
Lemma 1 Suppose that kw1,i (0)k2 = |w2,i(0)| > 0 for i ∈ [m]. Then, for any t > 0, we have
kw1,i(t)k2 = |w2,i(t)| > 0.
According to Lemma 1, for all t ≥ 0, sign(w2,i (t)) = sign(w2,i (0)). Therefore, we can simply
write si = si(t) = sign(w2,i(t)). As the neural network is scaled for t ≥ 0, it is interesting to study
the dynamics of w1,i in the polar coordinate. We write w1,i(t) = eri(t)ui(t), where kui(t)k2 = 1.
The gradient flow in terms of polar coordinate writes
∂tr = SiuTg(ui, λ),	∂tUi = Si (g(ui, λ) - (UTg(Ui, λ)) Ui)
(23)
Let
xmax = maxi∈[n] kxik2. Define gmin tobe
gmin = min kg(σ, y/4)k2, s.t. g(σ, y/4) 6= 0, where we denote	(24)
σ∈Q
Q = {σ ∈ {1, 0, -1}N∣σ = Sign(Xw), W ∈ Rd}.	(25)
As the set Q ⊆ {1, -1, 0}N is finite, we note that gmin > 0. We note that when maxn∈[N] |qn| ≈ 0,
y
We have λ ≈ 4. The following lemma shows that With initializations sufficiently close to 0,
kg(u(t), λ(t)) - g(u(t), y∕4)k2 and ∣∣ ddtg(u(t), λ(t))( CanbeVerySmall.
Lemma 2 Suppose that T > 0 and δ > 0. Suppose that (U(t), r(t)) follows the gradient flow (23)
with S = 1 and the initialization U(0) = U0 and r(0) = r0. Suppose that r0 is sufficiently small.
Then, the following two statements hold.
•	Forallt ≤ T, we have ∣∣g(u(t), λ(t)) - g(u(t), y∕4)∣∣2 ≤ gm^.
•	For t ≤ T such that sign(XU(S)) is constant in a small neighbor of t, we have
∣∣ it g(u(t),λ ⑴)∣∣2 ≤ g⅛δ.
Based on the above lemma on the property of g(U(t), λ(t)), we introduce the following lemma
to upper-bound the time such that cos ∠(U(t), g(U(t), λ(t))) approaches 1 - δ or sign(XU(t))
changes.
Lemma 3 Let δ ∈ (0, 1).Suppose that U0 satisfies that ∣U0∣2 = 1 and λ(0)T(XU0)+ > 0. Suppose
that (U(t), r(t)) follows the gradient flow (23) with S = 1 and the initialization U(0) = U0 and
e
r(0) = r0. LetV⑴=kgU(t1λ(Y))k2∙ We write v0 = V(O), σ0 = σ(0) αnd go = ∣∣g(σ0, y/4)k2.
Denote
T* =_______1_____llog 1- - δ/8 + 1	-j.	- log .1	-	δ∕8	+ VTuo )
2go √1 -	δ∕8 ∖	√1 - δ∕8 - 1	+ δ √1	-	δ∕8	- VTuo )
For c ∈ (0, 1 - δ], define
TShift(C) =	—71——llog	P1- δ∕8 + C -	log p1- δ∕8	+ VTU0
2go √1-^∕8	∖	√1-^∕8 - c √Γ-^∕8	- VTuo
(26)
(27)
Suppose that ro is sufficiently small such that the statements in Lemma 2 holds for T = T*. Then, at
least one of the following event happens
• There exists a time T such that we have sign(XU(t)) = sign(XUo) for t ∈ [0, T) and
sign(XU(t)) 6= sign(XUo). Let U1 = U(T) and V1 = limt→T -o V(t). If U1T V1 ≤
1 - δ, then the time T satisfies that T ≤ Tshift(V1TU1). Otherwise, there exists a time T0
satisfying T0 ≤ T*, such that we have sign(XU(t)) = sign(XUo) for t ∈ [0, T0] and
U(T 0)T V(T 0) ≥ 1 - δ.
• There exists a time T ≤ T*, such that we have sign(XU(t)) = sign(XUo) for t ∈ [0, T]
and U(T)TV(T) ≥ 1 - δ.
8
Published as a conference paper at ICLR 2022
Corollary 1 Suppose that there exists a time T such that we have sign(Xu(t))
sign(Xu0) for t ∈ [0, T) and sign(Xu(t)) 6= sign(Xu0). If T > T shift (v1T u1)
√1-δ∕8+vT uι
go√1-δ∕8 Iog √1-δ∕8-vTuι
1
一 log V1-"8+v07uo Y then, we have UTvι > 1 一 δ.
ɛ √Γ-δ∕8-vTuo ),	'	1	1
Proposition 6 Consider the sub-gradient flow (23) with s = 1 and the initialization u(0) = u0 and
r(0) = r0. Here at initilization the neuron u0 satisfies that ku0 k2 = 1 and yT(Xu0)+ > 0. Let
e
g(i~∖ —— __gI_(),__())_ FCY ∕7∏v ʌ ' 0 fey QH^ffiriout]、) Cm-1 -1 -ipvp -ci - 1	-	-1p	T	——	—)C∖cto,(--1、、
V(L) — U ( (t) e(t))u . o or any > > u, JUr SuJJlclently Smlalr r 0, tιιere exists	a	tιmιe	.l	—	VZ(iog((√	))
such that u(T)Tv(T) ≥ 1 一 δ and cos ∠(u(T), g(u(T), y)) ≥ 1 一 δ.
Remark 2 The statement of proposition is similar to Lemma 4 in (Maennel et al., 2018). However,
their proof contains a problem because they did not consider the change of sign(Xw) along the
gradient flow. Our proof in Appendix D.4 corrects this error.
We next study the properties of orthogonal separable datasets. Denote B — {w ∈ Rd : kwk2 ≤ 1}.
The following lemma give a sufficient condition on w to satisfy the condition in Proposition 4.
Lemma 4 Assume that (X, y) is orthogonal separable. Suppose that w ∈ B is a local maximizer oJ
yT (Xw)+ in B and (Xw)+ 6— 0. Then, hw, xni > 0 Jor n ∈ [N] such that yn — 1. Suppose that
w ∈ B is a local minimizer oJ yT (Xw)+ in B and (Xw)+ 6— 0. Then, hw, xni > 0 Jor n ∈ [N]
such that yn — 一1.
We show an equivalent condition of u ∈ B being the local maximizer/minimizer of yT (Xu)+ in B.
Proposition 7 Assume that (X, y) is orthogonal separable. Then, u ∈ B is a local maximizer oJ
yT (Xu)+ in B is equivalent to cos ∠(u, g(u, y)) — 1. Similarly, u ∈ B is a local minimizer oJ
yT (Xu)+ in B is equivalent to cos ∠(u, g(u, y)) — 一1.
Based on Proposition 4 and 7, we present the main theorem.
Theorem 4 Suppose that the dataset is orthogonal separable and θ(t) Jollows the gradient flow.
Suppose that the neural network is scaled at initialization, i.e., kw1,i(0)k2 — |w2,i(0)| Jor all i ∈ [m].
For almost all initializations which are sufficiently close to zero, the limiting point of 口露、is 修屏，
where θ* is a global minimizer ofthe max-margin problem (2).
We present a sketch of the proof. According to Proposition 6, for initialization sufficiently close to zero,
there exist two neurons and time T+, T- > 0 such that cos ∠(w1,i+ (T+), g(w1,i+ (T+),y)) ≥ ι - δ
and cos ∠(w1,i- (T-), g(w1,i- (T-), y)) ≤ 一(1 一 δ). This implies that w1,i+ (T+) and w1,i- (T+)
are sufficiently close to certain stationary points of gradient flow maximizing/minimizing yT(Xu+)
over B, i.e., {u ∈ B| cos(u, g(u, y)) — ±1}. As the dataset is orthogonal separable, from Proposition
7 and Lemma 4, the induced masking matrices Di+ (T+) and Di- (T-) by w1,i+ (T+)/w1,i- (T-)
in Proposition 3 satisfy that D% (T+) ≥ diag(I(y — 1)) and DDi- (T-) ≥ diag(I(y — -1)).
According to Lemma 3 in (Phuong & Lampert, 2021), for t ≥ max{T+, T-}, we also have Di+ (t) ≥
diag(I(y — 1)) and DDi- (t) ≥ diag(I(y — -1)). According to Theorem 3 and Proposition 4, the
KKT point of the non-convex problem (2) that gradient flow converges to corresponds to the KKT
point of the convex problem (14).
5	Conclusion
We provide a convex formulation of the non-convex max-margin problem for two-layer ReLU neural
networks and uncover a primal-dual extreme point relation between non-convex subgradient flow.
Under the assumptions on the training data, we show that flows converge to KKT points of the convex
max-margin problem, hence a global optimum of the non-convex objective.
6	Acknowledgements
This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, and US Army Research Office.
9
Published as a conference paper at ICLR 2022
References
Burak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex op-
timization of polynomial activation neural networks in fully polynomial-time. arXiv preprint
arXiv:2101.02429, 2021a.
Burak Bartan and Mert Pilanci. Training quantized neural networks to global optimality via semidefi-
nite programming. International Conference on Machine Learning (ICML), 2021, 2021b.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. Advances in Neural Information Processing Systems, 31:3036—
3046, 2018.
Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326-334, 1965.
Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization
of two-and three-layer networks in polynomial time. International Conference on Learning
Representations (ICLR), 2021, 2020.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
Journal of Machine Learning Research, 22(212):1-63, 2021a.
Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks
via convex programs. In International Conference on Machine Learning, pp. 2993-3003. PMLR,
2021b.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In
International Conference on Machine Learning, pp. 3004-3014. PMLR, 2021c.
Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demys-
tifying batch normalization in relu networks: Equivalent convex optimization models and implicit
regularization. arXiv preprint arXiv:2103.01499, 2021.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832-1841.
PMLR, 2018.
Vikul Gupta, Burak Bartan, Tolga Ergen, and Mert Pilanci. Exact and relaxed convex formulations
for shallow neural autoregressive models. In International Conference on Acoustics, Speech, and
Signal Processing, 2021.
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, pp. 2613-2682. PMLR, 2020.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Chao Ma, Lei Wu, and Weinan E. The quenching-activation behavior of the gradient descent dynamics
for two-layer neural network models. arXiv preprint arXiv:2006.14450, 2020.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network
features. arXiv preprint arXiv:1803.08367, 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally
separable data. 2021.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. pp. 7695-7705, 2020.
10
Published as a conference paper at ICLR 2022
Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, and Mert
Pilanci. Hidden convexity of wasserstein gans: Interpretable generative models with closed-form
solutions. arXiv preprint arXiv:2107.05680, 2021a.
Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network problems
are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms.
International Conference on Learning Representations (ICLR), 2021b.
Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regularization
behind neural reconstruction. International Conference on Learning Representations (ICLR),
2021c.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357-2370, 2019.
Qiuling Yang, Alireza Sadeghi, Gang Wang, and Jian Sun. Learning two-layer relu networks is nearly
as easy as learning linear classifiers on separable data. IEEE Transactions on Signal Processing,
69:4416-4427, 2021.
Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer
neural network. arXiv preprint arXiv:2102.02410, 2021.
11
Published as a conference paper at ICLR 2022
A Definitions and notions
We introduce several useful definitions and notions which will be utilized in the proof.
A.1 Definitions
Definition 1 Let O ⊂ Rn be an open set and let F : O → R be locally Lipschitz continuous at
x ∈ O. Let DF be the differentiable points of F in O. The B-subdifferential of F at x is defined by
∂BF (x) :=	lim F0(xk)|xk ∈ DF, xk → x
k→∞
(28)
The set ∂oF(x) = co(∂bF(x)) is called Clarke,s subdifferential, where co denotes the convex hull.
Definition 2 A matrix A is spike-free if and only if the following conditions hold: for all kuk2 ≤ 1,
there exists kzk2 ≤ 1 such that
(Au)+ = Az.	(29)
This is equivalent to say that
max 、kχt(Xu)+k2 ≤ 1.
u"∣uk2≤1,(I-XXT )(Xu)+=0
A.2 Notions
We use the following letters for indexing.
•	The index n is for the n-th data sample xn .
•	We use the index i to represent the i-th neuron-pair (w1,i, w2,i).
•	The index j is for the j-th masking matrix Di ∈ P.
B Proofs in Section 3
B.1 Proof for Proposition 2
Consider the following loss function I : RN X RN → R ∪ {+∞}
加 ʌ	∫0,	Iynzn ≥ 1,∀n ∈ [N],
`(z, y) =
+ ∞,	otherwise.
(30)
(31)
+ 2 (kwιkF + I∣w2k2),
For a given y ∈ {1, -1}N, 2(z, y) is a convex loss function of z. The non-convex max-margin is
equivalent to
min i((XWι)+W2,y) + 2 (∣∣WιkF + I∣w2k2) ∙	(32)
According to Appendix A.13 in (Pilanci & Ergen, 2020), the problem (32) is equivalent to
min l (X DiX(Ui — u), y
s.t. (2Di -I)Xui ≥ 0, (2Di -I)Xu0i ≥ 0,∀i ∈ [p].
This is equivalent to (14). For fixed y ∈ {1, -1}N, the Fenchel conjugate function of M(z, y) with
respect to z can be computed by
(33)
l* (λ, y) = max ZTX — i(z, y)
T
= max zTλ, s.t. diag(y)z ≥ 1,
z∈RN
=∫yTλ,	diag(y)λ ≤ 0
+ ∞, otherwise.
(34)
12
Published as a conference paper at ICLR 2022
According to Theorem 6 in (Pilanci & Ergen, 2020), the dual problem of (14) writes
max -l (λ, y), s.t. max ∣λτ(Xu)+∣ ≤ 1,	(35)
BkUk2≤1
which is equivalent to
max -YTλ, s.t. diag(y)λ ≤ 0, max ∣λτ(Xu)+1 ≤ 1.	(36)
Bkuk2≤1
By taking λ = 一λ, We derive (15). This completes the proof.
B.2 Proof for Proposition 3
For the non-convex max-margin problem (13), consider the Lagrange function
L(Wι, W2, λ) = 1(kWιkF + kw2k2) -(Yλ)τ(Y(XW1)+w2 - 1)
Where Yλ	0. The KKT point of the non-convex max-margin problem (13) (in terms of B-
subdifferential) satisfies
0 ∈ ∂WB1L(W1,w2,λ),
w2 - (XW1)T+λ = 0,	(37)
λn(yn(xTnW1)+w2 - 1) = 0.
The KKT condition on the i-th column of W1 is equivalent to
w1,i
N
w2,i λn xn gn,i,
n=1
(38)
Where gn,i ∈ ∂B(z)+ |z=xTw1,i. In other Words, We have
= I(xTn w1,i ≥ 0),	ifxTnw1,i 6= 0,
∈ {0, 1},	ifxTnw1,i = 0.
(39)
Let Di = diag([g1,i,..., gN,i]). Then, we Can write that
w1,i
N
λngn,ixnw2,i
n=1
w2,iXτ Diλ.
From the definition of gn,i, we have
gn,ixTn w1,i = 0.
Therefore, we can compute that
w2,i =(Xw1,i)T+λ
N
=	I(xTn w1,i ≥ 0)xTnw1,iλn
n=1
N
=	gn,ixTn w1,iλn
n=1
r-r∣_E ʌ _
=WTiXT DK
In summary, we have
Wl,i = W2,iXτ Diλ, W2,i = WfiXT D iλ.
Suppose that w2,i 6= 0. This implies that
Wii = Xt D iλ,	kXτ D i λk2 = 1.
W2,i
(40)
(41)
(42)
(43)
(44)
This completes the proof.
13
Published as a conference paper at ICLR 2022
B.3 Proof for Theorem 3
Proof We can write the Lagrange function for the convex max-margin problem (14) as
L({uj}jp=1,{u0j}jp=1,λ,{zj}jp=1,{z0i}jp=1)
pp
=	(kujk2 + ku0jk2) + λT diag(y) 1 - diag(y)	DjX(uj - u0j)
j=1	j=1
p
-	(zjT(2Dj-I)Xuj+(z0j)T(2Dj -I)Xu0j)
(45)
pp
=λTy+	(kujk2+ ku0jk2)+	(u0j)T(XTDjλ-XT(2Dj -I)z0j)
j=1	j=1
p
+ X(uj)T (-XT Dj λ - XT (2Dj - I)zj).
where zj, z0j ∈ RN satisfies that zj ≥ 0, z0j ≥ 0 forj ∈ [p] and λ ∈ RN satisfies that diag(y)λ ≥ 0.
The KKT point shall satisfy the following KKT conditions:
-XTDjλ+XT(2Dj -I)z0j ∈ ∂u0jku0jk2,
XTDjλ+XT(2Dj -I)zj ∈∂ujkujk2,
λn (X(Dj)nXT (Uj- Uj)- gn) =。，	(46)
zj,n (2(Dj)n,n - 1)xn uj = 0,
zj,n (2(Dj)n,n - 1)xn Uj = 0.
Let (Wι, w2, λ) be the KKT point of the non-convex problem (2) and λ satisfies (17). Let Di be
the diagonal matrix defined in Proposition 3 with respect to w1,i and denote P = {Di|i ∈ [m]}.
Without the loss of generality, we may assume that {DDi}m=I are different. (Otherwise, we can merge
two neurons wi,” and w1,i2 with D五 = Di? together.)
Suppose that Dj ∈ P, i.e., Dj = Di for certain i ∈ [m]. By letting U0j = w1,iw2,i, z0j = 0,
Uj = -w1,iw2,i and zj = 0, the following identities hold.
XTDjλ + XT(2Di - I)zj = XTDiλ = wiɪ = τru⅛.	(47)
j	w2,i	kU0ik
-XTDjλ + χT(2Di- I)Zj = -χTDiλ = U = kUik.	(48)
Therefore, for index j satisfying Di ∈ P, the first two KKT conditions in (46) hold.
For Dj ∈ P, we can let Uj = Uj = 0. As λ satisfies (17), we have
max	∣λTDjXUI ≤ 1.
kuk2≤1,(2Dj-I)Xu≥0
(49)
According to Lemma 4 in (Pilanci & Ergen, 2020), this implies that there exist zj , z0j ≥ 0 such that
k-XTDjλ+ZT(2Dj - I)z0jk ≤ 1,kXTDjλ+ZT(2Dj - I)zjk ≤ 1.	(50)
Therefore, the first two KKT conditions in (46) hold.
From our choice of Uj , zj , U0j , zj , the last two KKT conditions in (46) hold. We also note that
pm
X Dj X(U0j - Uj) = X(Xw1,i)+ w2,i.	(51)
j =1	i=1
As (W1, w2, λ) is the KKT point of the non-convex problem, the third KKT condition in (46) holds.
This completes the proof.
14
Published as a conference paper at ICLR 2022
C Proofs in Section 3.1
In this section, we present several proofs for propositions in Section 3.1.
C.1 Proof for Proposition 4
We start with two lemmas.
Lemma 5 Suppose that uo = XTJDoλ and ∣∣u0k2 ≤ L For any masking matrix Dj ∈ P such that
(Dj- Do)I(λ > 0) = 0,we have
max	λTDjXu ≤ 1.	(52)
(2Dj-I)Xu≥0,kuk2≤1
Proof According to Lemma 4 in (Pilanci & Ergen, 2020), the constraint (52) is equivalent to that
there exist zj ∈ RN such that zj ≥ 0 and
kXTDjλ+XT(2Dj -I)zjk ≤ 1.	(53)
. 1 .1 ∙ 1	Γ -Jt Τ-Τ	1 1	/Τ∙x	"ʌ ∖	Ir∙. 、 /Τ∙x	"ʌ ∖ ΤΤ / X	C'	C	1
Consider the index n ∈ [N] such that (Dj - D0)nn 6= 0. As (Dj - D0)I(λ > 0) = 0, we have
λn ≤ 0. We let (Zj)n = -λn. If (Do)nn = 0, then We have (Dj)nn = 1 and
(Dj- D 0)nnλnXn = λnXn = -XT (2(Dj )nn - 1)(Zj )n .	(54)
If(D0)nn = 1, then we have (Dj)nn = 0 and
(Dj- D 0)nnλnXn = -λnX = -XT (2(Dj )nn - 1)(Zj )n.	(55)
For other index n ∈ [N], we simply let (Zj)n = 0. Then, we have
(Dj - Do)nnλnXn =0 = -XT (2(Dj )nn - 1)(Zj )n.	(56)
Based on our choice ofZj, we have Zj ≥ 0 and for n ∈ [N]
(Dj - D0)nnλnXn = -XT(2(Dj)nn - 1)(Zj )n .	(57)
This implies that
XT(Dj - D0)λ = -XT(2Dj - I)Zj.	(58)
Hence, we have
XTDjλ + XT(2Dj - I)Zj = XTDoλ = U0.	(59)
Therefore, kXTDjλ + XT (2Dj - I)Zjk2 ≤ 1.
Lemma 6 Suppose that the data is orthogonal separable and Yλ ≥ 0. Suppose that uo = XTDoλ
and ku0k2 ≤ 1. For any masking matrix Dj such that Do - Dj ≥ 0, we have ∣∣XTDj∙λ∣∣2 ≤
ku0 k2 ≤ 1. Therefore, (52) holds.
Proof We note that uo = XT(D0 - Dj)λ + XDjλ. Denote a = XT(D0 - Dj)λ and b =
XTDj λ. We note that
aTb=	λnXn	λn0Xn0 .	(60)
∖n:(D O)nn=1,(Dj )n,n = 0	/	\n0：(DO)»» =0,(Dj ∖ n； n； =0	/
As diag(y)λ ≥ 0, λn has the same signature with yn. Therefore, from the orthogonal separability
of the data, we have
λnλn0XTXTo ≥ 0.	(61)
This immediately implies that aTb ≥ 0. Therefore,
1≥ ∣u0∣22=∣a+b∣22=∣a∣22+∣b∣22+2aTb≥ ∣a∣2.	(62)
This completes the proof.
Based on Lemma 5 and Lemma 6, we present the proof for Proposition 3. Let u+ = w1,i+ . From the
w2,i+
r∙ r∙ 1 ʌ	∙ . ∙ C	. .1 . 11 Il	1 1 -	t ∙	. ∙ ɪʌ _ ∕τ> 1 . T-x "ʌ ɪʌ
proof of Proposition 3, we note that ∣u+ ∣2 = 1. For any masking matrix Dj ∈ P, let D = Di+Dj.
As Di+ ≥ D, according to Lemma 6, we have
∣XTDλ∣2 ≤ ∣XTDi+λ∣2 = ∣u+∣2 ≤ 1.	(63)
As Yλ ≥ 0 and Di+ ≥ diag(I(y = 1)), we have (Dj-D)I(λ > 0) = Dj(I- Di+)I(λ > 0) = 0.
From Lemma 5, we note that λ satisfies (52). Similarly, we can show that -λ also satisfies (52).
This completes the proof.
15
Published as a conference paper at ICLR 2022
C.2 Proof for Proposition 5
PROOF Note that Yλ ≥ 0. Let Y+ = diag(I(y = 1)) and Y- = diag(I(y = -1)). We claim
that
max λT (XU)+ = max λTY+(XU)+. kuk≤1	+ kuk≤1	+	+	(64)
Firstly, We note that NN λT (XU)+ = X λn (xTn U)+ ≤ X(λn)+(xTn U)+ = λTY+ (XU)+. n=1	n=1	(65)
This implies that maxkuk≤1 λT (Xu)+ ≤ maxkuk≤1 λT Y+(Xu)+.
On the other hand, suppose that u ∈ arg maxkuk≤1 λTY+(Xu)+. As X is spike-free, there exists z
such that kzk2 ≤ 1 and Xz = (Xu)+ . Therefore, we have
λTY+(Xu)+ = λTY+Xz = λTXz = λT (Xz)+.	(66)
This implies that maxkuk≤1 λT (Xu)+ ≥ maxkuk≤1 λTY+(Xu)+.
For any Dj ∈ P with Dj ≥ Y+ . We note that
λT (Xu)+ ≤ λT Dj (Xu)+ ≤ λTY+(Xu)+.	(67)
Combining with (65), this implies that maxkuk≤1 λT (Xu)+ = maxkuk≤1 λTD(Xu)+.
Let Us go back to the original problem. Let u+ = w1,i+ . We note that (Xw+) = DDi+ Xw+ =
w2,i+	+
Di+ XXTDi+ λ. Therefore, We have
λτ (Xw+) = λτ Di+ XXT Di+ λ =IlXT Di+ λk2 = ku+k2 = 1.	(68)
ThUs, for any kuk2 ≤ 1, sUppose that (Xu)+ = Xz, Where kzk2 ≤ 1. Then, We have
λTDi+ (Xu)+ = λTDi+ Xz ≤ ∣∣z∣2 ≤ 1.	(69)
Therefore, maxkuk≤1 λT (Xu)+ = maxkuk≤1 λTD+ (Xu)+ ≤ 1. Similarly, We have
min λT (Xu)+ = min λTD- (Xu)+ ≥ -1.
kuk≤1	+ kuk≤1	-	+
This completes the proof.
D Proofs in Section 4
D.1 Proof for Lemma 1
Proof According to the sUb-gradient floW (22), We can compUte that
∂t (kw1,i∣2 — w2,i) = 2wT,i (w2,ig(U, λ)) — 2w2,iwTig(U, λ) = 0.	(7O)
Let To = sup{T∣∣∣wι,i(t)∣∣2 = ∣w2,i(t)∣ > 0,∀i ∈ [n],t ∈ [0,T)}. For t ∈ [0, To), as the neural
netWork is scaled, it is sUfficient stUdy the dynamics of w1,i in the polar coordinate. Let Us Write
w1,i(t) = eri(t)Ui(t), Where ∣Ui(t)∣2 = 1. Then, in terms of polar coordinate, the projected gradient
floW folloWs
∂
∂tr =Sign(W2,i)uTg(ui, λ),
ItUi =Sign(W2,i) (g(ui, λ) - (UTg(ui, λ)) Ui).
Without the loss of generality, We may assume that w2,i(0) 6= 0 for i ∈ [m]. Denote
xmax =max ∣xi∣2.
i∈[n]
(71)
(72)
16
Published as a conference paper at ICLR 2022


From the definition of λ, we have kλk∞ ≤ 1/4. Therefore, we have
∂
∂tri ≤ kg(ui, λ)k2 ≤
E	λjXj
j:xjT u>0
≤ nxmax
≤	2
2
(73)
Therefore, for finite t > 0, we have
r(t) ≥ri(0)- nxmaxt
which implies that |w2,i(t)| > 0. This implies that T0 = ∞.
(74)
D.2 Proof of Lemma 2
PROOF As we have kw1,ik2 = |w2,i|, for n ∈ [N], we can compute that
|qn| =I(XT WI)+ w2|
m
≤	|(XTnw1,i)+w2,i|
i=1
m
=X kw1,ik2|(XTnw1,i)+|
i=1
m
≤ X kw1,ik2|XTnw1,i|
i=1
m
≤kXnk2X kw1,ik22.
i=1
(75)
y
Note that λn = -yn'(qn) and yn = -yn'0(0). As ' is 4-LiPschitz continuous, We have
lʌn - yn/4| ≤ 4 |qn| ≤ W2 X kw1,ik2.
4	4	i=1
(76)
For any Cr ∈ Q, as λn ∈ [0,1/4] for n ∈ [N], We have
一	■~√	.,..
kg(σ,λ) - g(σ, y∕4)∣∣2
E (λn - yn∕4)Xn
n<σ)n>0
E	∣λk-黑∣kχkk2
k:(6)k>0
(77)
n	2m
≤ X 乎 X kwι,jk2
k=1	j=1
m
=c1 X kw1,ik22,
i=1
1
where ci = 11 IlXkF > 0 is a constant. Therefore, we can bound ∣∣g(^σ, λ(t))k by
mm
kg(σ, λ(t))∣2 ≤ ∣∣g(σ, y∕4)∣2 + ci X ∣∣wι,i(t)∣2 ≤ dmax + ci X e2ri(t),
i=1	i=1
(78)
where we let
gmax = mσ∈aQx kg(σ, y/4)k2.
(79)
≤
≤

2
17
Published as a conference paper at ICLR 2022
Let r(t) = maxi∈[m] ri(t). We note that
∂
dtr(t) ≤ dmax + cιne2rw ≤ c2(l + e2r㈤)，	(80)
where c2 = max{nc1, dmax} > 0 is a constant. If we start with r(0) 0, then, r(t) cannot grow
much faster than c2t. Let r(t) satisfy the following ODE:
∂
友r(t) = C2(1 + e2r㈤).	(81)
The solution is given by
ra(t) = c2(t - a) — J log(1 - e2c2Ca)),	(82)
where a > 0 is a parameter depending on the initialization. For any initial r(0), we have a unique a
satisfying r0(0) = r(0). Therefore, We have r(t) ≤ ro(t) and
kg(σ, λ(t)) - g(σ, y∕4)∣∣2 ≤ cιne2ra”	(83)
According to the bound (83), by choosing a sufficiently small r0, (which leads to a sufficiently small
a), such that
e2ra(τ) ≤ min d dminδ	l⅛nδ
e
—I 16c1 4n2xmaχ
(84)
Therefore, for t ≤ T , we have
Hence, we have
kg(σ, λ(t)) - g(σ, y∕4)k2 ≤ cine2ra㈤ ≤ cιne2ra(T) ≤ dmn^
8
kg(σ(t), λ(t)) - g(σ(t), y∕4)∣∣2 ≤ cιne2fa(t) ≤ cιne2ra(τ) ≤ dminδ
8
We can compute that
λi= = -yil(2) (qi) 万 qi∙
dt	dt
As l(2) (q) ∈ (0, 1∕4], we can compute that
d
dtqi
m
≤ X lw2,j lkxik2	dt wi,j
j=1	2
m
+ X llwi,j I∣2kxik2 dtw2,j
j=1
mm
n	22 n 2	2
≤ 4 ZkWIj k2xmax + 4 ʌ/w2,j XmaX
2
≤ nxmax g2r(t)
一 2	.
Therefore, we have
l'00(qi)1 ddtqi
≤ 1 -qi ≤ nxmXe2r(t)
4 dt i 8
(85)
(86)
(87)
(88)
(89)
Suppose that sign(Xu(s)) = σ(t) holds for s in a small neighbor of t. Then, we have
dd
—g(u(t), λ(t))	= —g(σ(t), λ(t))
dt	2 dt	2
n	d	n2x3
≤ X kxik2 -dʌi ≤ n4maxe (t)	(90)
dt 8
i=1
≤ n2xmax e2ra(T) ≤ dminδ
-8	-	16 .
This completes the proof.
18
Published as a conference paper at ICLR 2022
D.3 Proof of Lemma 3
PROOF Let T0 = sup{T |sign(Xu(t)) = sign(Xu0), ∀t ∈ [0, T)}. We analyze the dynamics of
u(t) in the interval [0, min{T0, T*}]. For t ≤ min{T0, T*}, as the statements in Lemma 2 hold, We
can compute that
齐⑴Tu⑴
T
g(S ⑼	u(t)
kg(σo, λ(t))k2
g(σ0,3(t))	)	d-u(t) + u(t)T------1-------d-g(σo, λ(t))
kg(σo, λ(t))k2 J	dt	kg(σo, λ(t))k2 dt
- u(t)T g(σ0, λe(t))
g(σo, λ(t))τddtg(σo, λ(t))
kg(σo, λ(t))k3
≥g(σ0, e(U)T ^ut-------------	-⅛g(σ0, e(t))
dt	gmin dt
≥kg(σo, e(t))k2 (1 - (v(t)Tu(t))2)- gm8nδ
≥g0 (I- S/8) (1 - (v(t)Tu(t))2) - gmin'
≥go (1 - δ∕4 - (v(t)Tu(t))2).
Here We utilize that g0 ≥ gmin, Where gmin is defined in (24). Let z(t) satisfies the ODE
dzP = (1 - δ/4 - z(t)2)g0,
dt
With initialization z(0) = v0Tu0. Then, We note that
z(t) = √1 - δ∕8 -
2√1 - δ∕8
1 + c3 exp(2got∕(P1 — δ∕8))，
where c3 = (√1-广 - 1)	. We Can compute that
z(T3) = 1 - δ.
According to the comparison theorem, for t ≤ min{T0, T3}, we have
v(t)T u(t) ≥ z(t).
(91)
(92)
(93)
(94)
(95)
We first consider the case where T0 = ∞. As T0 = ∞, we have
v(T*)tu(T*) ≥ Z(T*) = 1 - δ.	(96)
Therefore, the second event holds for T ≤ T*.
Otherwise, we have T0 < ∞. Recall that u1 = u(T0) and v1 = limt↑T0 v(t). Let T1 =
sup{T |v(t)T u(t) < v1Tu1, ∀t ∈ [0, T)} and T2 = sup{T |v(t)T u(t) < 1 - δ, ∀t ∈ [0, T)}.
If T2 ≤ T0, for t ∈ [0, T2], we have
-dv(t)Tu(t) ≥ (1 - δ∕4 - (1 - δ)2) go > 0.	(97)
dt
Therefore, v(t)T u(t) monotonically increases in [0, T2]. As v(t)T u(t) ≥ z(t) for t ∈ [0, T0], we
have thatz(T2) ≤ v(T2)Tu(T2) = 1 - δ = z(T3). Hence, we have T2 ≤ T*. Therefore, the second
condition of the first event holds at T = T2 .
Then, we consider the case where T2 ≥ T0. Fort ≤ T0, we have v(t)T u(t) ≤ 1 -δ. This implies that
v1T u1 ≤ 1 - δ. Apparently, we have T1 ≤ T0. If T1 < T0, as T0 ≤ T2, for t ∈ [0, T0], the inequality
19
Published as a conference paper at ICLR 2022
t→T0-0 V(t)Tu(T0),
(97) holds. This implies that limt→T0-0 v(t)T u(T0) > v(T1 )T u(T1 ) = lim
which leads to a contradiction. Therefore, we have T0 = T1 . We note that
z(Tshift(u1Tv1)) = u1Tv1.
(98)
As u(t)T g(u(t), λe(t)) ≥ z(t) for t ∈ [0, T0], we have that z(T1) ≤ u1T V1 ≤ z(T4). Hence, we have
T0 = T1 ≤ Tshift (u1TV1). This completes the proof.
D.4 Proof of Proposition 6
We first introduce a lemma.
Lemma 7 Let a, b ∈ Rd and 0 < δ < c. Suppose that ka - bk2 ≤ δ and kak2 ≥ c. Then, we have
a
02
2δ
2 ≤ ^.
(99)
b
而
—
PROOF As δ < c, we have kbk2 > kak2 - ka - bk2 ≥ c - δ > 0. We first note that
kak2-1 - kbk2-1 =
l⅛→⅛≤
kak2kbk2
δ
CW.
(100)
Therefore, we can compute that
a
瓦
a
瓦
<δ δ
cc
This completes the proof.
b
而
b
⅛
2δ
.
c
1
kak2
kbk2
(101)
≤
—
—
2
+
2
—
1
Then we present the proof of Proposition 6.
PROOF As yT (Xu0)+ > 0, with sufficiently small initialization and sufficiently small δ > 0, we


also have λ(0)T (Xu0)+ ≥ yT (Xu0)+/4 - kXk2kλ(0) - y/4k2 > 0. We prove that there exists a
time T such that U(T)TV(T) ≥ 1 一 4δ by contradiction. Denote v° = v(0). For all possible values
of ∣∣g(u, y/4) k 2, we can arrange them from the smallest to the largest by g(i)< g(2)< …< g(p).
]	√1-6/8+1-6/2 ]	√1-δ/8+g(i) vo u0 ʌ i T	V^p T c
log L一一log L-g-1vTυo Jand T = Pp=ι Ti. SUPPOSe
1
Let Ti
that r0 is sufficiently small such that statements in Lemma 2 holds for T . According to Lemma 3,
we can find 0 = t0 < t1 < . . . such that for i = 1, . . . , sign(Xu(t)) is constant on [ti-1 , ti) and
sign(Xu(ti-1)) 6= sign(Xu(ti)). We write ui = u(ti), gi = kg(u(ti), y/4)k2,


gi-
lim g(u(t), λ(t)),	gi = g(u(ti), λ(t)),
t↑ti
(102)
V- = kgi 口 and Vi = ɪg^. We note that g- = g，. According to Lemma 3, we have
ti ——ti 一 1 ≤--,	=----
2p1 - δ/8gi-1
1
log
≤—,	——
2 ʌ/1 - δ/8gmin
log
11 - δ/8+ (vi )Tui
P1 - δ∕8 - (V- )TUi
P1 - δ/8 + (v-)Tui
P1 - δ∕8 一(V- )Tui
- log
- log
yj∖ 一 δ∕8 + VT-Iui-i
P1 - δ/8 一 VT-IUi — 1
p∖ 一 δ∕8 + VT-Iui-1
p1 一 δ∕8 - Vτ-ιui-ι
Here we utilize that gi-1 ≥ gmin, where gmin is defined in (24). This implies that
P 一 ^8+(v-)T ui ≥ e2√1-δ∕8gmin(ti-ti-i) P - δ^ + VT-Iui-I
p1 - δ∕8 — (v-)tui —	p1 - δ∕8 — VT-iui-i
(103)
(104)
1
20
Published as a conference paper at ICLR 2022
We can show that for t satisfying t ≥ -/ 1 —— (log V1-"8+1zδ/2 - log 1+gm1nv: u0 j and
J 3 - 2√1-δ∕8gmin ∖ 1 √1-δ∕8-1 + δ∕2	& 1-g-nLvT U0 J
t ≤ T, we have kg(u(t), λ)k2 > gmin. According to Lemma 3, as gi ≥ gmin, we have
p1 -	δ/8	+ g-1n(g-)TUi	≥ e2√I≡δ∕8gmin(ti-ti-ι)	√1 -	J/8 +	g-1ngT-1ui-1
P1 -	δ∕8	- g-1n(g- )TUi	P1 -	δ/8 -	gm1ngT-IuiT
(105)
This implies that
Vz1	- 6/ +	gm1n(g-)Tui	≥ e2√1-δ∕8gminti V1	- δ/8 +	g-nvlT u0
P1	- δ/8 -	gm1n(gi )Tui	P1	- 6/8 -	gm1nvTu0
or equivalently, for any t > 0, we have
I--Σ~； 	1 , , , , , C~> , , , m , ,	I----Σ~； 	1 m
P1 - 6/8 + gmin(g(uW) X⑻ u(t) ≥ e2V^/8gmint P1 - 6/8 + g-invTu0
P1	- 6/8 - gmi1ng(u⑴,λe(t))Tu⑴-	P1	- 6/8 - gmi1nvTu0
Here we	utilize that g(u(t),	λ(t))T u(t)	is	continuous w.r.t. t. Therefore,
2⅛ (l°g ¥ - log ⅛⅛TU-)，We have
-I .	.	.	■	.	.	. . r-i-1	>	.
1+ gmi1n(g(u(t),λ(t)))Tu(t)	P^/8 +1- 6/2
1 - gmi1ng(u(t),λ(t))Tu(t)	PT-I/8 - 1 + 6/2
This implies that
gm-i1n(g(u(t), λe(t)))T u(t) ≥ 1 - 6/2.
If kg(u(t), λ)k2 = gmin， as the statements in Lemma 2 hold， we can compute that
kg(u(t),λ) - g(u(t), λ(t))k2 ≤ gmin- = 4 kg(u(t), λ)k2,
which implies that
Therefore， we have
■~■
kg(u(t),λe(t))k2 ≤ (1 + 6/4)kg(u(t), λ)k2.
v(t”(1g≡2
≥	1	(g(u(t),e (t)))τ u(t)
—1 + 6/4	gmin
≥ 1if ≥
1 - 46.
(106)
(107)
for t ≥
(108)
(109)
(110)
(111)
(112)
This leads to a contradiction.
Analogously， we can show that for t ≥ Pij=1 Ti， we have kg(u(t), y/4)k2 > g(i). Thus， by taking
t ≥ Pip=1 Ti， we have kg(u(t), y/4)k2 > g(p) = gmax. However， from the definition of gmax，
we have kg(u(t), y/4)k2 ≤ gmax. This leads to a contradiction. Therefore， there exists a time
T = PP=1 Ti = O (log 6-1) such that v(T )t u(T) ≥ 1 - 4 6.
We note that kg(u(T), y/4)k2 ≥ gmin. As the statements in Lemma (2) hold， we have
kg(u(τ),y/4) - g(u(τ),λ(τ))k2 ≤ ⅛n
8
(113)
According to Lemma 7， we have
..	~'
g(u(τ), y/4)	g(u(T), λ(τ))
- : -------：~~~ - -----------------
kg(u(T), y/4)k2	kg(u(T), λe(T))k2
2∣∣g(u(T), y/4)- g(u(T), λ(T 川2
gmin
(114)
≤
2
≤
6
4
21
Published as a conference paper at ICLR 2022
This implies that
u(T)T
g(u(τ), y/4)
kg(u(T), y∕4)k2
≥u(T)T v(T) -
. . ~ , ..
g(u(T ),λ)	g(u(T ),λ (T))
-
kg(u(T), λ)k2	kg(u(T), λe (T))k2
(115)
≥1 - δ.
Hence, we have
T g(u(T), y)	T g(u(T), y/4)
Cos∠(U(T), g(u(T), y)) = U(T) kg(u(T), y)k2 = U(T) kg(u(T), y∕4)k2 ≥ 1 - δ∙
This completes the proof.
D.5 Proof of Lemma 4
Proof This is proved in Lemma 2 in (Phuong & Lampert, 2021). Here we provide an alternative
proof. It is sufficient to prove for the case of local maximizer. Suppose that w is a local maximizer of
yT (Xw)+ in B. We first consider the case where yT (Xw)+ > 0.
If there exists n ∈ [N] such that hw, xni ≤ 0 and yn = 1. Consider v = xn/kxnk2 and let
We = kWw⅛, Where > 0. For index n0 ∈ [N] such that yn0 = 1, as the dataset is orthogonal
separable, We have xTn0 xn > 0 and
xT0 (w + Ev) = xT0W + I，e∣∣ XToXn > xT0W.	(116)
n	n	kxn k2 n	n
This implies that (XTn0 We)+ ≥ (XTn0 W)+. For yn0 = -1, as the data is orthogonal separable, We note
that XTn0 Xn ≤ 0 and
XTo (w + Ev) = XTo w + η__π-XToXn ≤ XTo W.	(117)
kXn k2
This implies that (XjTWe)+ ≤ (XjT W)+. In summary, We have
NN
yT (X(W + Ev))+ = Xyn(XjT(W + Ev))+ ≥ Xyn(XjTW)+ = yT (XW)+ > 0	(118)
n=1	n=1
If hW, Xn i < 0, then WTv < 0. This implies that With sufficiently small E, We have kW + Evk2 <
kWk2 = 1. Therefore,
Yy (XWe))+ = η—1—π- Yy (X(w + Ev))+ > Yy (X(w + Ev))+ ≥ Yy (Xw) + ,	(119)
kW+ Evk2
Which leads to a contradiction. If hw, Xni = 0, We note that
(XTn (w + Ev))+ = E > (XTn w)+.	(120)
This implies that
YT(X(w + Ev))+ ≥ YT(Xw)+ + E.	(121)
We also note that ∣∣w + Evk2 = √1 + e2 = 1 + O(e2). Therefore, with sufficiently small e, we have
YT(Xwe)+ ≥ Y (XW)+2+E > YT(Xw)+.	(122)
1 + E2
We then consider the case where YT (Xw)+ < 0. Apparently, we can make YT (Xw)+ larger by
replacing w by (1 - E)w, where E ∈ (0, 1), which leads to a contradiction.
Finally, we consider the case where YT (Xw)+ = 0. This implies that
X (XjT w)+ = X (XjT w)+.	(123)
n:yn=1	n:yn=-1
As (Xw)+ 6= 0, this implies that there exists at least for one index n ∈ [N] such that yn = 1 and
XTw > 0. Let v = Xn∕∣∣Xn∣∣2. We note that 岫二心 YT(X(W + Ev))+ > 0 for e > 0. This leads
to a contradiction.
22
Published as a conference paper at ICLR 2022
D.6 Proof of Proposition 7
It is sufficient to consider the case of the local maximizer. Denote Q = {σ ∈ {-1,0,1}N ∣diag(σ) ∈
P}. For σ, σ0 ∈ Q, we say σ ⊆ σ0 if for all index n ∈ [N] with σn 6= 0, σn0 = σn. We say σ ∈ Q
is open if σn 6= 0 for n ∈ [N]. Define
Sσ = {u|sign(Xu) = σ}.
(124)
We start with the two lemmas.
Lemma 8 Let λ ∈ RN. Suppose that uo satisfies that uo = kg^^k ∙ Let σ = Sign(U0). Then,
v ∈ B2 is a local maximizer of λT (Xu)+ in B2 if for any open σ0 satisfying σ ⊆ σ0, we have
kg(σ, y)k2 = kg(σ0, y)k2.
PROOF Suppose that σ is open. Then, Sσ is an open set. In a small neighbor around u0
g(UOA)
kg(U0,λ)∣2
U g(σ,λ) , λT(Xu)+ = uTg(σ, λ) is a linear function of u. The Riemannian
kg(σ,λ)k2
gradient of uT g(σ, λ) at v is zero. This implies that v locally maximizes λT(Xu)+.
Suppose that there exists at least one zero in σ. Consider any v ∈ B satisfying u0Tv = 0. Let > 0
be a small constant such that for any s ∈ (0, ], u0 + sv ∈ Sσ0 where σ ⊆ σ0. Let us
u+sv
√+2.
Suppose that kg(σ00, λ)k2 ≤ kg(σ, λ)k2 for all open σ00 satisfying σ ⊆ σ00. For any σ0 with
σ ⊆ σ0, we construct σ00 by σ0i0 = -1 for n ∈ [N] such that σ0n = 0 and σ0n0 = σ0n for
n ∈ [N] such that σn0 = 0. We note that kg(σ00, λ)k2 ≥ kg(σ0, λ)k2. Thus, kg(σ0, λ)k2 ≤
kg(σ00,λ)k2	≤	kg(σ,λ)k2.	AS	∣λτ(Xus)+∣	=	∣g(σ0,λ)TUs|	≤	kg(σ0,	λ)∣∣2, we have
∣λτ(Xus)+∣ ≤ kg(σ0,λ)k2 ≤ ∣∣g(σ, λ)k2. Therefore, u is a local maximizer of λτ(Xu)+.
Lemma 9 Suppose that the dataset is orthogonal separable. Let λ ∈ RN satisfy that diag(y)λ ≥ 0.
Suppose that uo satisfies that uo = kg(U0λ))k2 . Then，for any σ0 satisfying σ ⊆ σ0, we have
∣g(σ0,λ)∣2=∣g(σ,λ)∣2.
PROOF If there exists n ∈ [N] such that σn = 1 and yn = -1, as the data is orthogonal separable,
we note that
xTn g(σ, λ) = xTn	λn0 xn0	= yn(ynxn)T	(λn0 yn0)yn0 xn0	≤ 0,	(125)
∖n>σn0>O	)	∖nLσn0>O	)
which contradicts with sign(xTn g(σ, λ)) = sign(xTn uo) = σn = 1.
Suppose that there exists n ∈ [N] such that σn and yn = 1. Then, as the dataset is orthogonal
separable, then, for index n1 ∈ [N] such that σn1 = 0, we note that yn1 6= 1. Otherwise,
XTI g(σ, λ) = xT1 I E λn2xn2 ) = xT1 I E (λn yn2 )yn2 xn2 I > 0,	(126)
∖n2θn2 >0	)	∖n2Un2 >0	)
which contradicts with sign(xTn g(σ, λ)) = sign(xTn uo) = σn1 = 0. This also implies that the
index set {n ∈ [N]∣σn > 0} include all data with yn, = 1.
If there exists σ0 such that σ ⊆ σ0 and ∣g(σ0, λ)∣2 > ∣g(σ, λ)∣2. Then, there exists at least one
index n ∈ [N] such that σn ≤ 0 and σn0 = 1. However, from the previous derivation, we note that
yn = -1 and
xn g(σ , λ) = xn I X
λn1 xn1 I = xn I X (λn1 yn1 )yn1 xn1 I < 0,	(127)
\力％1 >0	)	41:%1 >0	)
which contradicts with σn0 = 1.
By combining Lemma 8 and 9, we complete the proof.
23
Published as a conference paper at ICLR 2022
D.7 Proof of Theorem 4
PROOF For almost all initialization, we can find two neurons such that sign(w2,i+) =
sign(yT (Xw1,i+)+) = 1 and sign(w2,i-) = sign(yT (Xw1,i-)+) = -1 at initializa-
tion. By choosing a sufficiently small δ > 0 in Proposition 6, there exist two neurons
w1,i+, w1,i- and times T+,T- > 0 such that cos ∠(w1,i+ (T+), g(w1,i+ (T+), y)) > 1 - δ and
cos ∠(w1,i+ (T+), g(w1,i+ (T+), y)) < -(1 - δ). This implies that w1,i+ (T+) andw1,i-(T+) are
sufficiently close to certain stationary points of gradient flow maximizing/minimizing yT (Xu+)
over B, i.e., {u ∈ B| cos(u, g(u, y)) = ±1}. As the dataset is orthogonal separable, according to
Lemma 4 and Proposition 7, the corresponding diagonal matrices Di+ (T+) and Di- (T-) satisfy
that Di+ (T+) ≥ diag(I(y = 1)) and Di- (T-) ≥ diag(I(y = -1)). According to Lemma 3 in
(Phuong & LamPert, 2021), We have Di+ (t) ≥ diag(I(y = 1)) and Di- (t) ≥ diag(I(y = -1))
hold for t ≥ max{T+, T-}.
With t → ∞, according to ProPosition 4, the dual variable λ in the KKT Point of the non-convex
max-margin problem (13) is dual feasible, i.e., λ satisfies (16). Suppose that θ* is a limiting point
CfJ θ(t)]
of IWH2 Λ≥0
and λ* is the corresponding dual variable. From Theorem 1, We note that the pair
(θ*, λ*) corresponds to the KKT point of the convex max-margin problem (14).
E	Proofs of main results on multi-class classification
E.1 Proof of Proposition 1
The neural netWork training problem (4) can be separated into K subproblems. Each of these
subproblems corresponds to the neural netWork training problem (3) for binary classification. For
each subproblem, by applying Proposition 2, We complete the proof.
E.2 Proof of Theorem 1
We note that the neural netWork training problem (4) can be separated into K subproblems. Each of
these subproblems corresponds to the neural netWork training problem (3) for binary classification.
By applying Proposition 6 With to each subproblem With y = yk, We complete the proof.
E.3 Proof of Theorem 2
Similarly, the corresponding non-convex max-margin problem (5) and the convex max-margin
problem (7) can be separated into K subproblems. Each of these subproblems corresponds to the non-
convex max-margin problem (2) and the convex max-margin problem (14) for binary classification.
By applying Theorem 4 to each subproblem With y = yk , We complete the proof.
F Numerical experiment
F.1 Details on Figure 5
We provide the experiment setting in Figure 1 and 5 as folloWs. The dataset is given by X =
-10.6.457 -10.3.457 ∈ R2×2 and y = -11 ∈ R2. Here We have N = 2 and d = 2. We note that this
dataset is orthogonal separable but not spike-free. We plot the ellipsoid set and the rectified ellipsoid
set in Figure 6.
24
Published as a conference paper at ICLR 2022
Figure 6: The ellipsoid set and the rectified ellipsoid set. Orthogonal separable dataset.
We enumerate all possible hyperplane arrangements in the set P and solve the convex max-margin
problem (14) via CVXPY to obtain the following non-zero neurons
0.58
u1,3 = -0.16
0	-0.23
w1,2 =	0.66
(128)
We note that the dual problem (15) is equivalent to
max λTy,
s.t. kXTDjλ-XT(2Dj-I)zj,+k2 ≤ 1,∀j ∈ [p],
k-XTDjλ-XT(2Dj-I)zj,-k2 ≤ 1,∀j∈ [p],
zj,+ ≥ 0, zj,- ≥ 0, ∀j ∈ [p], diag(y)λ ≥ 0.
(129)
The above problem is a second-order cone program (SOCP) and can be solved via standard convex
optimization frameworks such as CVX and CVXPY. We solve (129) to obtain the optimal dual
variable λ. For the geometry of the dual problem, as the dataset is orthogonal separable, the set
{λ : maxkuk2≤1 ∣λτ(Xu)+1 ≤ 1} reduces to {λ : max∣∣uk2≤ι ∣λτ(Xu；)+| ≤ 1,λT(Xu2)+∣ ≤
1}, where u；,呜 correspond to two vectors at the spikes of the rectified ellipsoid set. We draw the
sets {λ : max∣∣uk2≤ι ∣λτ(Xu)+1 ≤ 1}, {λ :, the optimal dual variable λ and the direction of y in
Figure 2.
For each Dj ∈ P, we solve for the vector uj which maximize/minimize λTDj Xuj with the
constraints kuj k2 ≤ 1 and (2Dj - I)Xuj ≥ 0. We plot the rectified ellipsoid set {(Xu)+ |kuk2 ≤
1}, vectors Uj, neurons in the optimal solution to (14) scaled to unit '2-norm and the direction of λ
in Figure 1. We note that each neuron u* in the optimal solution from (14) (scaled to unit '2-norm)
maximize/minimize the corresponding λτDjXUj given (2Dj - I)Xu* ≥ 0.
Then, we consider a two-layer ReLU network with m = 10 neurons and apply the gradient descent
method to train on the logistic loss (3). Let W；,i = 口411? for i ∈ [m]. We plot W；,i and (Xw；,i)+
at iteration {10l |l = 0, . . . , 4} along with neurons in the optimal solution to (14) scaled to unit
'2 -norm in Figure 5. Certain neurons do not move, while the activated neurons trained by gradient
descent tend to converge to the direction of the neurons in the optimal solution to (14).
We repeat the training on the logistic loss (3) with the gradient descent method several times and we
plot the trajectories in Figure 7.
25
Published as a conference paper at ICLR 2022
1.0-
0.5
0.0
-0.5
-1.0-
-1.0	-0.5	0.0	0.5
1.0-
0.5
0.0
-0.5
-1.0-
-1.0	-0.5	0.0	0.5
1.0-
0.5
0.0
-0.5
-1.0-
-1.0	-0.5	0.0	0.5
1.0-
0.5
0.0
-0.5
-1.0-
-1.0	-0.5	0.0	0.5
Figure 7: Multiple independent random initializations of gradient descent trajectories on the same
orthogonal separable dataset.
26
Published as a conference paper at ICLR 2022
F.2 Experiment on spike-free dataset
We repeat the previous numerical experiment on a non-spike-free dataset: X
1.65
0.47
0.47
1.35
R2×2 and y = 11 ∈ R2. Similarly, we plot the ellipsoid set and the rectified set in Figure 8.
Figure 8: The ellipsoid set and the rectified ellipsoid set for a non-spike-free dataset.
We enumerate all possible hyperplane arrangements in the set P and solve the convex max-margin
problem (14) via CVXPY to obtain the following non-zero neuron
0.43
u1,4 = 0.59
(130)
We plot the rectified ellipsoid set {(Xu)+|kuk2 ≤ 1}, vectors uj, neurons in the optimal solution to
(14) scaled to unit '2-norm and the direction of λ in Figure 9. We also plot Wι,i and (Xwι,i)+ at
iteration {10l |l = 0,..., 4} along with neurons in the optimal solution to (14) scaled to unit '2-norm
in Figure 10.
Figure 9: Recitified Ellipsoidal set and correspond-
ing extreme points for a non-spike-free dataset.
27
Published as a conference paper at ICLR 2022
neuron
neuron
7-th neuron
neuron
neuron
5-th
6-th
8-th
9-th
cone boundary
1-th neuron
2-th neuron
3-th neuron
4-th neuron
10-th neuron
★ optimal
cone boundary
1-th neuron
2-th neuron
3-th neuron
4-th neuron
5-th
6-th
neuron
neuron
7-th neuron
8-th
9-th
neuron
neuron
10-th neuron
★ optimal neuron
Figure 10: Multiple independent random initializations of gradient descent trajectories on the same
non-spike-free dataset. Note that the optimal extreme point (star), which is the uniquely optimal single
neuron is on the boundary of the main two-dimensional ellipsoid and not on the one-dimensional
spikes (projected ellipsoids). Also note that some neurons are stuck at spurious stationary points.
28