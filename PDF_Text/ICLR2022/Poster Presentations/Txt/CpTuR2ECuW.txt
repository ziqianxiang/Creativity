Published as a conference paper at ICLR 2022
LIGS: Learnable Intrinsic-Reward Generation
Selection for Multi-Agent Learning
David Mguni* 1；Taher Jafferjee1, Jianhong Wang2, Oliver Slumbers1,5, Nicolas Perez-Nieves2,
Feifei Tong1 , Li Yang3, Jiangcheng Zhu1 , Yaodong Yang4, Jun Wang5
1Huawei Technologies, 2Imperial College London, 3Shanghaitech University,
4Institute for AI, Peking University & BIGAI, 5University College London
Ab stract
Efficient exploration is important for reinforcement learners to achieve high re-
wards. In multi-agent systems, coordinated exploration and behaviour is critical
for agents to jointly achieve optimal outcomes. In this paper, we introduce a new
general framework for improving coordination and performance of multi-agent re-
inforcement learners (MARL). Our framework, named Learnable Intrinsic-Reward
Generation Selection algorithm (LIGS) introduces an adaptive learner, Generator
that observes the agents and learns to construct intrinsic rewards online that coordi-
nate the agents’ joint exploration and joint behaviour. Using a novel combination
of MARL and switching controls, LIGS determines the best states to learn to
add intrinsic rewards which leads to a highly efficient learning process. LIGS
can subdivide complex tasks making them easier to solve and enables systems
of MARL agents to quickly solve environments with sparse rewards. LIGS can
seamlessly adopt existing MARL algorithms and, our theory shows that it ensures
convergence to policies that deliver higher system performance. We demonstrate
its superior performance in challenging tasks in Foraging and StarCraft II.
1 Introduction
Cooperative multi-agent reinforcement learning (MARL) has emerged as a powerful tool to enable
autonomous agents to solve various tasks such as autonomous driving (Zhou et al., 2020b), ride-
sharing (Li et al., 2019), gaming AIs (Peng et al., 2017a), power networks (Wang et al., 2021a; Qiu
et al., 2021) and swarm intelligence (Mguni et al., 2018; Yang et al., 2017). In multi-agent systems
(MAS), maximising system performance often requires agents to coordinate during exploration
and learn coordinated joint actions. However, in many MAS, the reward signal provided by the
environment is not sufficient to guide the agents towards coordinated behaviour (Matignon et al.,
2012). Consequently, relying on solely the individual rewards received by the agents may not lead to
optimal outcomes (Mguni et al., 2019). This problem is exacerbated by the fact that MAS can have
many stable points some of which lead to arbitrarily bad outcomes (Roughgarden & Tardos, 2007).
As in single agent RL, in MARL inefficient exploration can dramatically decrease sample efficiency.
In MAS, a major challenge is how to overcome sample inefficiency from poorly coordinated explo-
ration. Unlike single agent RL, in MARL, the collective of agents is typically required to coordinate
its exploration to find their optimal joint policies1. A second issue is that in many MAS settings
of interest, such as video games and physical tasks, rich informative signals of the agents’ joint
performance are not readily available (Hosu & Rebedea, 2016). For example, in StarCraft Microman-
agement (Samvelyan et al., 2019), the sparse reward alone (win, lose) gives insufficient information
to guide agents toward their optimal joint policy. Consequently, MARL requires large numbers of
samples producing a great need for MARL methods that can solve such problems efficiently.
To aid coordinated learning, algorithms such as QMIX (Rashid et al., 2018), MF-Q (Yang et al., 2018),
Q-DPP (Yang et al., 2020), COMA (Foerster et al., 2018) and SQDDPG (Wang et al., 2020c), so-
called centralised critic and decentralised execution (CT-DE) methods use a centralised critic whose
* Correspondence to davidmguni@hotmail.com.
1Unlike single agent RL, MARL exploration issues cannot be mitigated by adjusting exploration rates or
policy variances (Mahajan et al., 2019).
1
Published as a conference paper at ICLR 2022
role is to estimate the agents’ expected returns. The critic makes use of all available information
generated by the system, specifically the global state and the joint action (Peng et al., 2017b). To
enable effective CT-DE, it is critical that the joint greedy action should be equivalent to the collection
of individual greedy actions of agents, which is called the IGM (Individual-Global-Max) principle
(Son et al., 2019). CT-DE methods are however, prone to convergence to suboptimal joint policies
(Wang et al., 2020a) and suffer from variance issues for gradient estimation (Kuba et al., 2021).
Existing value factorisations, e.g. QMIX and VDN (Sunehag et al., 2017) cannot ensure an exact
guarantee of IGM consistency (Wang et al., 2020b). Moreover, CT-DE methods such as QMIX
require a monotonicity condition which is violated in scenarios where multiple agents must coordinate
but are penalised if only a subset of them do so (see Exp. 2, Sec. 6.1).
To tackle these issues, in this paper we introduce a new MARL framework, LIGS that constructs
intrinsic rewards online which guide MARL learners towards their optimal joint policy. LIGS involves
an adaptive intrinsic reward agent, the Generator that selects intrinsic rewards to add according to
the history of visited states and the agents’ joint actions. The Generator adaptively guides the agents’
exploration and behaviour towards coordination and maximal joint performance. A pivotal feature
of LIGS is the novel combination of RL and switching controls (Mguni, 2018) which enables it to
determine the best set of states to learn to add intrinsic rewards while disregarding less useful states.
This enables the Generator to quickly learn how to set intrinsic rewards that guide the agents during
their learning process. Moreover, the intrinsic rewards added by the Generator can significantly
deviate from the environment rewards. This enables LIGS to both promote complex joint exploration
patterns and decompose difficult tasks. Despite this flexibility, special features within LIGS ensure
the underlying optimal policies are preserved so that the agents learn to solve the task at hand.
Overall, LIGS has several advantages:
•	LIGS has the freedom to introduce rewards that vastly deviate from the environment rewards. With
this, LIGS promotes coordinated exploration (i.e. visiting unplayed state-joint actions) among the
agents enabling them to find joint policies that maximise the system rewards and generates intrinsic
rewards to aid solving sparse reward MAS (see Experiment 1 in Sec. 6.1).
•	LIGS selects which best states to add intrinsic rewards adaptively in response to the agents’ be-
haviour while the agents learn leading to an efficient learning process (see Investigations in Sec. 6.1).
•	LIGS’s intrinsic rewards preserve the agents’ optimal joint policy and ensure that the total environ-
ment return is (weakly) increased (see Sec. 5).
To enable the framework to perform successfully, we overcome several challenges: i) Firstly, con-
structing an intrinsic reward can change the underlying problem leading to the agents solving
irrelevant tasks (Mannion et al., 2017). We resolve this by endowing the intrinsic reward function
with special form which both allows a rich spread of intrinsic rewards while preserving the optimal
policy. ii) Secondly, introducing intrinsic reward functions can worsen the agents’ performance
(Devlin & Kudenko, 2011) and doing so while training can lead to convergence issues. We prove
LIGS leads to better performing policies and that LIGS’s learning process converges and preserves
the MARL learners’ convergence properties. iii) Lastly, adding an agent Generator with its own goal
leads to a Markov game (MG) with N + 1 agents (Fudenberg & Tirole, 1991). Tractable methods
for solving MGs are extremely rare with convergence only in special cases (Yang & Wang, 2020).
Nevertheless, using a special set of features in LIGS’s design, we prove LIGS converges to a solution
in which it learns an intrinsic reward function that improves the agents’ performance.
2	Related Work
Reward shaping (Harutyunyan et al., 2015; Mguni et al., 2021) is a technique which aims to alleviate
the problem of sparse and uninformative rewards by supplementing the agent’s reward with a prefixed
term F. In Ng et al. (1999) it was established that adding a shaping reward function of the form
F (st+1, st) = γφ(st+1) - φ(st) preserves the optimal policy and in some cases can aid learning.
RS has been extended to MAS (Devlin et al., 2011; Mannion et al., 2018; Devlin & Kudenko, 2011;
2012; 2016; Sadeghlou et al., 2014) in which it is used to promote convergence to efficient social
welfare outcomes. Poor choices of F in a MAS can slow the learning process and can induce
convergence to poor system performance (Devlin & Kudenko, 2011). In MARL, the question of
which shaping function to use remains unaddressed. Typically, RS algorithms rely on hand-crafted
shaping reward functions that are constructed using domain knowledge, contrary to the goal of
2
Published as a conference paper at ICLR 2022
autonomous learning (Devlin & Kudenko, 2011). As we later describe LIGS, which successfully
learns an instrinsic reward function F, uses a similar form as PBRS however, F is now augmented to
include the actions of another RL agent to learn the intrinsic rewards online. In Du et al. (2019) an
approach towards learning intrinsic rewards is proposed in which a parameterised intrinsic reward is
learned using a bilevel approach through a centralised critic. In Wang et al. (2021b), a parameterised
intrinsic reward is learned by a corpus, then the trained intrinsic reward is frozen on parameters and
used to assist the training of a single-agent policy for generating the dialogues. Loosely related are
single-agent methods (Zheng et al., 2018; Dilokthanakul et al., 2019; Kulkarni et al., 2016; Pathak
et al., 2017) which, in general, introduce heuristic terms to generate intrinsic rewards.
Multi-agent exploration methods seek to promote coordinated exploration among MARL learners.
Mahajan et al. (2019) proposed a hybridisation of value and policy-based methods that uses mutual
information to learn a diverse set of behaviours between agents. Though this approach promotes
coordinated exploration, it does not encourage exploration of novel states. Other approaches to
promote exploration in MARL while assuming aspects of the environment are known in advance and
agents can perform perfect communication between themselves (Viseras et al., 2016). Similarly, to
promote coordinated exploration in partially observable settings, Pesce & Montana (2020) proposed
end-to-end learning of a communication protocol through a memory device. In general, exploration-
based methods provide no performance guarantees nor do they ensure the optimal policy. Moreover,
many employ heuristics that naively reward exploration to unvisited states without consideration of
the environment reward. This can lead to spurious objectives being maximised.
Within these categories, closest to our work is the intrinsic reward approach in Du et al. (2019). There,
the agents’ policies and intrinsic rewards are learned with a bilevel approach. In contrast, LIGS
performs these operations concurrently leading to a fast and efficient procedure. A crucial point of
distinction is that in LIGS, the intrinsic rewards are constructed by an RL agent (Generator) with
its own reward function. Consequently, LIGS can generate complex patterns of intrinsic rewards,
encourage joint exploration. Additionally, LIGS learns intrinsic rewards only at relevant states, this
confers high computational efficiency. Lastly, unlike exploration-based methods e.g., Mahajan et al.
(2019), LIGS ensures preservation of the agents’ joint optimal policy for the task.
3	Preliminaries
A fully cooperative MAS is modelled by a decentralised-Markov decision process (Dec-MDP) (Deng
et al., 2021). A Dec-MDP is an augmented MDP involving a set of N ≥ 2 agents denoted byN that
independently decide actions to take which they do so simultaneously over many rounds. Formally, a
dec-MDP is a tuple M = hN, S, (Ai)i∈N , P, R, γi where S is the finite set of states, Ai is an action
set for agent i ∈ N and R : S × A → P(D) is the reward function that all agents jointly seek to
maximise where D is a compact subset of R and lastly, P : S × A × S → [0, 1] is the probability
function describing the system dynamics where A := ×iN=1Ai. Each agent i ∈ N uses a Markov
policy πi : S × Ai → [0, 1] to select its actions. At each time t ∈ 0, 1, . . . , the system is in state
st ∈ S and each agent i ∈ N takes an action ait ∈ Ai. The joint action at = (at1 , . . . , atN) ∈ A
produces an immediate reward r 〜R(st, at) for agent i ∈ N and influences the next-state transition
which is chosen according to P . The goal of each agent i is to maximise its expected returns
measured by its value function vπi,π-i (s) = Eπi,π-i [Pt∞=0 γtR(st, at)], where Πi is a compact
Markov policy space and -i denotes the tuple of agents excluding agent i.
Intrinsic rewards can strongly induce more efficient learning (and can promote convergence to higher
performing policies) (Devlin & Kudenko, 2011). We tackle the problem of how to learn intrinsic
rewards produced by a function F that leads to the agents learning policies that jointly maximise
the system performance (through coordinated learning). Determining this function is a significant
challenge since poor choices can hinder learning and the concurrency of multiple learning processes
presents potential convergence issues in a system already populated by multiple learners (Zinkevich
et al., 2006). Additionally, we require that the method preserves the optimal joint policy.
4	The LIGS Framework
To tackle the challenges described above, we introduce Generator an adaptive agent with its own
objective that determines the best intrinsic rewards to give to the agents at each state. Using
3
Published as a conference paper at ICLR 2022
observations of the joint actions played by the N agents, the goal of the Generator is to construct
intrinsic rewards to coordinate exploration and guide the agents towards learning joint policies that
maximise their shared rewards. To do this, the Generator learns how to choose the values of an
intrinsic reward function F θ at each state. Simultaneously, the N agents perform actions to maximise
their rewards using their individual policies. The objective for each agent i ∈ {1, . . . , N} is given by:
∞
vπ ,π	,g (s) = E X γt R + Fθ s0 = s
where θ is determined by the Generator using the policy g : S × Θ → [0, 1] and Θ ⊂ Rp is the
Generator,s action set. The intrinsic reward function is given by Fθ(∙) ≡ θ(C - γ-1θC-ι where θC 〜g
is the action chosen by the Generator and θtc ≡ 0, ∀t < 0. Θ can be a set of integers {1, . . . , K}).
Therefore, the Generator determines the output of Fθ (which it does through its choice of θc). With
this, the Generator constructs intrinsic rewards that are tailored for the specific setting.
LIGS freely adopts any MARL algorithm for the N agents (see Sec. 10 in the Supp. Material). The
transition probability P : S × A × S → [0, 1] takes the state and only the actions of the N agents as
inputs. Note that unlike reward-shaping methods e.g. (Ng et al., 1999), the function F now contains
action terms θc which are chosen by the Generator which enables the intrinsic reward function to be
learned online. The presence of the action θc term may spoil the policy invariance result in Ng et al.
(1999). We however prove a policy invariance result (Prop. 1) analogous to that in Ng et al. (1999)
which shows LIGS preserves the optimal policy of M. The Generator is an RL agent whose objective
takes into account the history of states and N agents’ joint actions. The Generator’s objective is:
∞
vcπ,g(s) = Eπ,g	Xγt	Rθ(st, at) +	L(st,	at)	s0	= s ,	∀s ∈	S.	(1)
t=0
where Rθ(s, a) := R(s, a) + Fθ. The objective encodes Generator’s agenda, namely to maximise
the agents’ expected return. Therefore, using its intrinsic rewards, the Generator seeks to guide the
set of agents toward optimal joint trajectories (potentially away from suboptimal trajectories, c.f.
Experiment 2) and enables the agents to learn faster (c.f. StarCraft experiments in Sec. 6). Lastly,
L : S × A → R rewards Generator when the agents jointly visit novel state-joint-action tuples and
tends to 0 as the tuples are revisited. We later prove that with this objective, the Generator’s optimal
policy (for constructing the intrinsic rewards) maximises the expected (extrinsic) return (Prop. 1).
Since the Generator has its own (distinct) objective, the resulting setup is an MG, G = hN ×
{c}, S, (Ai)i∈N , Θ, P, Rθ, Rc, γi where the new elements are {c}, the Generator agent, Rθ :=
R+Fθ, the new team reward function which contains the intrinsic reward Fθ, Rc : S × A × Θ → R,
the one-step reward for the Generator (we give the details of this later).
Switching Control Mechanism
So far the Generator’s problem involves learning to construct intrinsic rewards at every state which
can be computationally expensive. We now introduce an important feature which allows LIGS to
learn the best intrinsic reward only in a subset of states in which intrinsic rewards are most useful.
This is in contrast to the problem tackled by the N agents who must compute their optimal actions
at all states. To achieve this, we now replace the Generator’s policy space with a form of policies
known as switching controls. These policies enable Generator to decide at which states to learn the
value of intrinsic rewards. This enables the Generator to learn quickly both where to add intrinsic
rewards and the magnitudes that improve performance since the Generator’s magnitude optimisations
are performed only at a subset of states. Crucially, with this the Generator can learn its policy rapidly
enabling it to guide the agents toward coordination and higher performing policies while they train.
At each state, the Generator first makes a binary decision to decide to switch on its F for agent i ∈ N
using a switch I( which takes values in {0, 1}. Crucially, now the Generator is tasked with learning
how to construct the N agents’ intrinsic rewards only at states that are important for guiding the
agents to their joint optimal policy. Both the decision to activate the function F and its magnitudes is
determined by the Generator. With this, the agent i ∈ N objective becomes:
∞
vπ,g (s0, I0) = E
X Yt {R + Fθ ∙ It} , ∀(s0,I0) ∈S×{0,1},
(2)
t=0
4
Published as a conference paper at ICLR 2022
where Iτk+1 = 1 - Iτk, which is the switch for F which is 0 or 1 and {τk}k>0 are times that a switch
takes place2 so for example if the switch is first turned on at the state s5 then turned off at s7 , then
τ1 = 5 and τ2 = 7 (we will shortly describe these in more detail). At any state, the decision to turn on
I is decided by a (categorical) policy gc : S → {0, 1} which acts according to Generator’s objective.
In particular, first, the Generator makes an observation of the state sk ∈ S and the joint action ak
and using gc, the Generator decides whether or not to activate the policy g to provide an intrinsic
reward whose value is determined by θck 〜g. With this it can be seen the sequence of times {τk} is
τk = inf{t > τk-1 |st ∈ S, gc(st) = 1} so the switching times. {τk} are rules that depend on the
state. Therefore, by learning an optimal gc, the Generator learns the useful states to switch on F.
To induce the Generator to selectively choose when to switch on the additional rewards, each switch
activation incurs a fixed cost for the Generator. In this case, the objective for the Generator is:
Vng (s0,I0) = E∏,g X Yt (r" (st, at)- X δT2k-1 + L(st, a")］，	⑶
where the Kronecker-delta function δτt	which is 1 whenever t = τ2k-1 and 0 otherwise imposes a
cost for each switch activation. The cost has two effects: first, it reduces the computational complexity
of the Generator’s problem since the Generator now determines subregions of S it should learn the
values of F . Second, it ensures the information-gain from encouraging the agents to explore state-
action tuples is sufficiently high to merit activating a stream of intrinsic rewards. We set τ0 ≡ 0,
θτk ≡ 0, ∀k ∈ N (θτk+ι,..., θτk+1-1 remain non-zero), θC ≡ 0 ∀k ≤ 0 and denote by I(t) ≡ It.
Discussion on Computational Aspect
The switching controls mechanism results in a frame-
work in which the problem facing the Generator has
a markedly reduced decision space in comparison
to the agent,s problem (though the agents share the
same experiences). Crucially, the Generator must
compute optimal intrinsic rewards at only a subset of
states which are chosen by gc. Moreover, the decision
space for the switching policy gc is SX {0,1} i.e at
each state it makes a binary decision. Consequently,
the learning process for gc is much quicker than the
agents, policies which must optimise over the deci-
sion space |S||A| (choosing an action at every state).
This results in the Generator rapidly learning its op-
timal policies (relative to the agent) in turn, enabling Figure 1: Schematic of the LIGS framework.
the Generator to guide the agents towards its optimal
policy during its learning phase. Also, in our experiments, we chose the size of the action set for
the Generator, Θ to be a singleton resulting in a decision space of size |S| × {0,1} for the entire
problem facing the Generator. We later show that this choice leads to improved performance while
removing the free parameter of the dimensionality of the Generator,s action set.
Summary of Events
At a time t ∈ 0, 1, . . .
1.
2.
3.
4.
5.
6.
The N agents makes an observation of the state st ∈ S.
The N agents perform a joint action at = (at1, . . . , atN) sampled from π = (π1, . . . , πN).
The Generator makes an observation of st and at and draws samples from its polices
(gc, g).
If gc(st) = 0:
◦	Each agent i ∈ N receives a reward ri 〜R(st, at) and the system transitions to the
next state st+1 and steps 1 - 3 are repeated.
If gc(st) = 1:
◦	Fθ is computed using St and the Generator action θc 〜g.
◦	Each agent i ∈ N receives a reward ri + Fθ and the system transitions to st+ι.
At time t + 1 if the intrinsic reward terminates then steps 1 - 3 are repeated or if the intrinsic
reward has not terminated then step 5 is repeated.
2More precisely, {τk }k≥0 are stopping times (0ksendal, 2003).
5
Published as a conference paper at ICLR 2022
4.1 The Learning Procedure
In Sec. 5, we provide the convergence properties of the algorithm, and give the full code of the
algorithm in Sec. 9 of the Appendix. The algorithm consists of the following procedures: the Gener-
ator updates its policy that determines the values θ at each state and the states to perform a switch
while the agents {1, . . . , N} learn their individual policies {π1, . . . , πN}. In our implementation,
we used proximal policy optimization (PPO) (Schulman et al., 2017) as the learning algorithm for
both the Generator’s intervention policy gc and Generator’s policy g. For the N agents we used
MAPPO (Yu et al., 2021). for the Generator L term we use3 L(st, at) := kh - hk22 where h is a
random initialised network which is the target network which is fixed and h is the prediction function
that is consecutively updated during training. We constructed Fθ using a fixed neural network
f : Rd 7→ Rm and a one-hot encoding of the action of the Generator. Specifically, i(θtc) is a one-hot
encoding of the action θtc picked by the Generator. Thus, F(θtc,θtc-1) = i(θtc) - γ-1i(θtc-1). The
action set of the Generator is Θ ≡ {1} where g is an MLP g : Rd 7→ Rm. Extra details are in Sec. 9.
5 Convergence and Optimality of LIGS
We now show that LIGS converges and that the solution ensures a higher performing agent policies.
The addition of the Generator’s RL process which modifies N agents’ rewards during learning can
produce convergence issues (Zinkevich et al., 2006). Also to ensure the framework is useful, we must
verify that the solution of G corresponds to solving the MDP, M. To resolve these issues, we first
study the stable point solutions of G . Unlike MDPs, the existence of a solution in Markov policies is
not guaranteed for MGs (Blackwell & Ferguson, 1968) and is rarely computable (except for special
cases such as team and zero-sum MGs (Shoham & Leyton-Brown, 2008)). MGs also often have
multiple stable points that can be inefficient (Mguni et al., 2019); in G such stable points would lead
to a poor performing agent joint policy. We resolve these challenges with the following scheme:
[I]	LIGS preserves the optimal solution of M.
[II]	The MG induced by LIGS has a stable point which is the convergence point of MARL.
[III] LIGS yields a team payoff that is (weakly) greater than that from solving M directly.
[IV] LIGS converges to the solution with a linear function approximators.
In what follows, we denote by Π := ×i∈NΠi. The results are built under Assumptions 1 - 7 (Sec.
15 of the Appendix) which are standard in RL and stochastic approximation theory. We now prove
the result [I] which shows the solution to M is preserved under the influence of LIGS:
Proposition 1 The following statements hold:
i)	max vπ,g (s, ∙) = max vπ (s), Vs ∈ S, ∀i ∈ N ,Vg where vπ (S) = E∏ [P∞ 0 γtR].
π∈Π	π∈Π	t=0
ii)	The Generator’s optimal policy maximises vπ (s) = E [ t∞=0 γtR(st, at)] for any s ∈ S.
Result (i) says that the agents’ problem is preserved under the Generator’s influence. Moreover the
agents’ (expected) total return is that from the environment (extrinsic rewards). Result (ii) establishes
that the Generator’s optimal policy induces it to maximise the agents’ joint (extrinsic) total return.
The result is proven by a careful adaptation of the policy invariance result in Ng et al. (1999) to our
MARL switching control setting where the intrinsic-reward is not added at all states. Building on
Prop. 1, we deduce the following result:
Corollary 1 LIGS preserves the dec-MDP played by the agents. In particular, let (π^,^) be a stable
point policy profile4 of the MG induced by LIGS, G then π is a solution to the dec-MDP, M.
Therefore, the introduction of the Generator does not alter the fundamentals of the problem. Our next
task is to prove the existence of a stable point of the MG induced by LIGS and show it is a limit point
of a sequence of Bellman operations. To do this we prove that a stable solution of G exists and that
G has a special property that permits its stable point to be found using dynamic programming. The
following result establishes that the solution of the MG G, can be computed using RL methods:
3This is similar to random network distillation (Burda et al., 2018) however the input is over the space A × S.
4By stable point profile we mean a Markov perfect equilibrium (MPE) (Fudenberg & Tirole, 1991).
6
Published as a conference paper at ICLR 2022
Theorem 1 Given a function V : S × A → R, G has a stable point given by lim TkV π,g =
k→∞
sup Vπ,g = Vπ ,g where (π?, g) is a stable solution of G and T is the Bellman Operator(c.f. (5)).
π ∈Π
Theorem 1 proves that the MG G (which is the game that is induced when Generator plays with the
N agents) has a stable point which is the limit of a dynamic programming method. In particular, it
proves the that the stable point of G is the limit point of the sequence T1 V, T2V, . . . ,. Crucially, (by
Corollary 1) the limit point corresponds to the solution of the dec-MDP M. Theorem 1 is proven
by firstly proving that G has a dual representation as an MDP whose solution corresponds to the
stable point of the MG. Theorem 1 enables us to tackle the problem of finding the solution to G
using distributed learning methods i.e. MARL to solve G. Moreover, Prop. 1 indicates by computing
the stable point of G leads to a solution of M. These results combined prove [II]. Our next result
characterises the Generator policy gc and the optimal times to activate F . The result yields a key
aspect of our algorithm for executing the Generator activations of intrinsic rewards:
Proposition 2 The policy gc is given by: gc(st, It) = H(Mπ,gVπ,g - Vπ,g )(st, It), ∀(st, It) ∈
S × {0, 1}, where Vπ,g is the solution in Theorem 1, M is the Generator’s intervention operator
(c.f. (4)) and H is the Heaviside function, moreover Tk = inf{τ > τ^k-ι∣Mπ,g Vπ,g = Vπ,g}.
In general, introducing intrinsic rewards or shaping rewards may undermine learning and worsen
overall performance. We now prove that the LIGS framework introduces an intrinsic reward which
yields better performance for the N agents as compared to solving M directly ([III]).
Theorem 2 Each agent’s expected return vπ,g whilst playing G is (weakly) higher than the expected
return for M (without the Generator) i.e. vπ,g (s, ∙) ≥ vπ (S), ∀s ∈ S, ∀i ∈ N.
Theorem 2 shows that the Generator’s influence leads to an improvement in the system performance.
Note that by Prop. 1, Theorem 2 compares the environment (extrinsic) rewards accrued by the agents
so that the presence of the Generator increases the total expected environment rewards. We complete
our analysis by extending Theorem 1 to capture (linear) function approximators which proves [IV].
We first define a projection Π by: ΠΛ := arg min ∣∣TV 一 A，for any function Λ.
Λ ∈{Φr∣r∈Rp}
Theorem 3 LIGS converges to the stable point of G, moreover, given a set of linearly independent
basis functions Φ = {φ1, . . . , φp} with φk ∈ L2, ∀k. LIGS converges to a limit point r? ∈ Rp which
is the unique solution to ∏F(Φr?) = Φr? where FA := RR + YP max{MA, A} . Moreover, r?
satisfies: kΦr? — Q?k ≤ (1 — γ2)-1/2 k∏Q? — Q*∣∣.
The theorem establishes the convergence of LIGS to a stable point (of G) with the use of linear
function approximators. The second statement bounds the proximity of the convergence point by the
smallest approximation error that can be achieved given the choice of basis functions.
6 Experiments
We performed a series of experiments on the Level-based Foraging environment (Papoudakis et al.,
2020) to test if LIGS: 1. Efficiently promotes joint exploration 2. Optimises convergence points
by inducing coordination. 3. Handles sparse reward environments. In all tasks, we compared
the performance of LIGS against MAPPO (Yu et al., 2021), QMIX (Rashid et al., 2018); intrinsic
reward MARL algorithms LIIR (Du et al., 2019), LICA (Zhou et al., 2020a), and a leading MARL
exploration algorithm MAVEN (Mahajan et al., 2019). We then compared LIGS against these
baselines in StarCraft Micromanagement II (SMAC) (Samvelyan et al., 2019). Lastly, we ran a
detailed suite of ablation studies (see Appendix) in which we demonstrated LIGS’ flexibility to
accommodate i) different MARL learners, ii) different L bonus terms for the Generator objective.
We also demonstrated the necessity of the switching control component in LIGS and LIGS’ improved
use of exploration bonuses.
7
Published as a conference paper at ICLR 2022
6.1	Cooperative Foraging Tasks
Experiment 1: Coordinated exploration. We tested our first claim that LIGS promotes coordi-
nated exploration among agents. To investigate this, we used a version of the level-based foraging
environment (Papoudakis et al., 2020) as follows: there are n agents each with level ai . Moreover,
there are 3 apples with level K such that PiN=1 ai = K . The only way to collect the reward is if
all agents collectively enact the collect action when they are beside an apple. This is a challenging
joint-exploration problem since to obtain the reward, the agents must collectively explore joint
actions across the state space (rapidly) to discover that simultaneously executing collect near an apple
produces rewards. To increase the difficulty, we added a penalty for the agents failing to coordinate
in collecting the apples. For example, if only one agent uses the collect action near an apple, it gets a
negative reward. This results in a non-monotonic reward structure. The performance curves are given
in Fig. 2 which shows LIGS demonstrates superior performance over the baselines.
Experiment 2: Optimal joint policies. We next tested our second claim that LIGS can promote
convergence to joint policies that achieve higher system rewards. To do this, we constructed a
challenging experiment in which the agents must avoid converging to suboptimal policies that deliver
positive but low rewards. In this experiment, the grid is divided horizontally in three sections; top,
middle and bottom. All grid locations in the top section give a small reward r/n to the agent visiting
them where n is the number of tiles in the each section. The middle section does not give any rewards.
The bottom section rewards the agents depending on their relative positions. If one agent is at the top
and the other at the bottom, the agent at the bottom receives a reward -r/n each time the other agent
receives a reward. If both agents are at the bottom, then one of the tiles in this section will give a
reward R, r/2 < R < r to both agents. The bottom section gives no reward otherwise. The agents
start in the middle section and as soon as they cross to one section they cannot return to the middle.
As is shown in Fig. 2, LIGS learns to acquire rewards rapidly in comparison to the baselines with
MAPPO requiring around 400k episodes to match the rewards produced by LIGS.
Experiment 3: Sparse rewards. We tested our claim that LIGS can promote learning in MAS with
sparse rewards. We simulate a sparse reward setting using a competitive game between two teams
of agents. One team is controlled by LIGS while the other actions of the agents belonging to the
other team are determined by a fixed policy. The goal is to collect the apple faster than the opposing
team. Collecting the apple results in a reward of 1, and rewards are 0 otherwise. This is a challenging
sparse reward since informative reward signals occur only apple when the apple is collected. As is
shown in Fig. 1 both LIGS and MAPPO perform well on the sparse rewards environment, whilst the
other baselines are all unable to learn any behaviour on this environment.
Investigations. We investigated the workings of the LIGS framework. We studied the locations
where the Generator added intrinsic rewards in Experiments 1 and 2. As shown in the heatmap
visualisation in Fig. 3, for Experiment 2, we observe that the Generator learns to add intrinsic rewards
that guide the agents towards the optimal reward (bottom right) and away from the suboptimal rewards
at the top (where some other baselines converge). This supports our claim that LIGS learns to guide
the agents towards jointly optimal policies. Also, as Fig. 3 shows, LIGS’s switching mechanism
Figure 2: Left. Coordinated Exploration. Centre. Optimal joint policies. Right. Sparse rewards.
---Oura
---MAPPO
---LΠR
---UCA
---MAVEN
---QMK
8
Published as a conference paper at ICLR 2022
Coordinated Exploration
OlZm 寸
SIeU-PJOOU AP
FleldXQjorxilnate
Coordinated Exploration
iŋŋ
S
° 80
ro
,∣ 60
g 40
d
ro
r
S 20
I
1	2	3	4	5	6	7	8	9 W
Distance to Other Plaver
LBF SubOptImaI
6∞
500
4∞ 2
S
-30Of
I
200«
FleIdXCoordInate


Figure 3:	Left. Heatmap of Exp. 1 showing where Generator adds rewards. Centre. Plot of distance
to other agent when Generator activates rewards in Exp 1. Right. Corresponding heatmap for Exp. 2.
___ ___
etaR niW
5m vs. 6m
0.5	1.0	1.5	2.0	2.5
Steps	1e6
0.8
4 0.6
Z
目0.4
S
0.2
6h vs. 8z
1.0 -∣
OUrS
MAPPO
LIIR
LICA
MAVEN
2	3
Steps
MMM2
1.0 r
0	1
3s5z vs. 3s6z
1.0 π
SeH UIM
。三二WM
Figure 4:	Median win rate over the course of learning on SMAC. LIGS outperforms the baselines on
all maps. LIIR, LICA, and MAVEN are generally not visible as their win rate is negligible.
means that the Generator only adds intrinsic rewards at the most useful locations for guiding the
agents towards their target. For Experiment 1, Fig. 3 shows that the Generator learns to guide the
agents towards the apple which delivers the high rewards. Fig. 3 (Right) demonstrates a striking
behaviour of the LIGS framework - it only activates the intrinsic rewards around the apple when both
agents are at most 2 cells away from the apple. Since the agents receive positive rewards only when
they arrive at the apple simultaneously, this ensures the agents are encouraged to coordinate their
arrival and receive the maximal rewards and avoids encouraging arrivals that lead to penalties.
6.2 Learning Performance in StarCraft Multi-Agent Challenge
To ascertain if LIGS is effective even in complex environments, we ran it on on the following SMAC
maps 5m vs. 6m (hard), 6h vs. 8z, Corridor, 3s5z vs 3s6z and MMM2 (super hard). These maps
vary in a range of MARL attributes such as number of units to control, environment reward density,
unit action sets, and (partial)-observability. In Fig. 4, we report our results showing ‘Win Rate’ vs
‘Steps’. These curves are generated by computing the median win rate (vs the opponent) of the agent
at regular intervals during learning. We ran 3 seeds of each algorithm (further setup details are in
the Supp. material Sec 14). LIGS outperforms the baselines in all maps. In 5m vs. 6m and 6h vs.
8z, the baselines do not approach the performance of LIGS. In Corridor MAPPO requires over an
extra million steps to match LIGS. In 3s5z vs. 3s6z and MMM2, LIGS still outperforms the baselines.
In summary, LIGS shows performance gains over all baselines in SMAC maps which encompass
diverse MAS attributes.
7	Conclusion
We introduced LIGS, a novel framework for generating intrinsic rewards which significantly boosts
performance of MARL algorithms. Central to LIGS is a powerful adaptive learning mechanism
that generates intrinsic rewards according to the task and the MARL learners’ joint behaviour. Our
experiments show LIGS induces superior performance in MARL algorithms in a range of tasks.
9
Published as a conference paper at ICLR 2022
8	Acknowledgements
We would like to thank Matthew Taylor and Aivar Sootla for their helpful comments.
References
Albert Benveniste, Michel Metivier, and Pierre Priouret. Adaptive algorithms and stochastic approxi-
mations, volume 22. Springer Science & Business Media, 2012.
David Blackwell and Tom S Ferguson. The big match. The Annals of Mathematical Statistics, 39(1):
159-163,1968.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018.
Xiaotie Deng, Yuhao Li, David Henry Mguni, Jun Wang, and Yaodong Yang. On the complex-
ity of computing markov perfect equilibrium in general-sum stochastic games. arXiv preprint
arXiv:2109.01795, 2021.
Sam Devlin and Daniel Kudenko. Theoretical considerations of potential-based reward shaping for
multi-agent systems. In The 10th International Conference on Autonomous Agents and Multiagent
Systems, pp. 225-232. ACM, 2011.
Sam Devlin and Daniel Kudenko. Plan-based reward shaping for multi-agent reinforcement learning.
The Knowledge Engineering Review, 31(1):44-58, 2016.
Sam Devlin, Daniel Kudenko, and Marek Grze´. An empirical study of potential-based reward
shaping and advice in complex, multi-agent systems. Advances in Complex Systems, 14(02):
251-278, 2011.
Sam Michael Devlin and Daniel Kudenko. Dynamic potential-based reward shaping. In Proceedings
of the 11th International Conference on Autonomous Agents and Multiagent Systems, pp. 433-440.
IFAAMAS, 2012.
Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, and Murray Shanahan. Feature control as
intrinsic motivation for hierarchical reinforcement learning. IEEE transactions on neural networks
and learning systems, 30(11):3409-3418, 2019.
Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. Liir: Learning individual
intrinsic reward in multi-agent reinforcement learning. 32, 2019.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Thirty-second AAAI conference on artificial
intelligence, 2018.
Drew Fudenberg and Jean Tirole. Tirole: Game theory. MIT Press, 726:764, 1991.
Anna Harutyunyan, Sam Devlin, Peter Vrancx, and Ann Nowe. Expressing arbitrary reward functions
as potential-based advice. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 29, 2015.
Ionel-Alexandru Hosu and Traian Rebedea. Playing atari games with deep reinforcement learning
and human checkpoint replay. arXiv preprint arXiv:1607.05077, 2016.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative dynamic
programming algorithms. In Advances in neural information processing systems, pp. 703-710,
1994.
Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang,
David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv
preprint arXiv:2108.08612, 2021.
10
Published as a conference paper at ICLR 2022
Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in
neural information processing Systems, 29:3675-3683, 2016.
Minne Li, Zhiwei Qin, Yan Jiao, Yaodong Yang, Jun Wang, Chenxi Wang, Guobin Wu, and Jieping
Ye. Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning. In
The World Wide Web Conference, pp. 983-994, 2019.
Sergio Valcarcel Macua, Javier Zazo, and Santiago Zazo. Learning parametric closed-loop policies
for markov potential games. arXiv preprint arXiv:1802.00899, 2018.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. arXiv preprint arXiv:1910.07483, 2019.
Patrick Mannion, Sam Devlin, Karl Mason, Jim Duggan, and Enda Howley. Policy invariance under
reward transformations for multi-objective reinforcement learning. Neurocomputing, 263:60-73,
2017.
Patrick Mannion, Sam Devlin, Jim Duggan, and Enda Howley. Reward shaping for knowledge-based
multi-objective multi-agent reinforcement learning. The Knowledge Engineering Review, 33, 2018.
Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement
learners in cooperative markov games: a survey regarding coordination problems. The Knowledge
Engineering Review, 27(1):1-31, 2012.
David Mguni. A viscosity approach to stochastic differential games of control and stopping involving
impulsive control. arXiv preprint arXiv:1803.11432, 2018.
David Mguni. Cutting your losses: Learning fault-tolerant control and optimal stopping under adverse
risk. arXiv preprint arXiv:1902.05045, 2019.
David Mguni, Joel Jennings, and Enrique Munoz de Cote. Decentralised learning in systems with
many, many strategic agents. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
David Mguni, Joel Jennings, Sergio Valcarcel Macua, Emilio Sison, Sofia Ceppi, and Enrique Munoz
de Cote. Coordinating the crowd: Inducing desirable equilibria in non-cooperative systems. arXiv
preprint arXiv:1901.10923, 2019.
David Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen, Joel Jennings,
and Jun Wang. Learning in nonzero-sum stochastic games with potentials. arXiv preprint
arXiv:2103.09284, 2021.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In ICML, volume 99, pp. 278-287, 1999.
Bemt 0ksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65-84.
Springer, 2003.
Georgios Papoudakis, Filippos Christianos, LUkas Schafer, and Stefano V Albrecht. Comparative
evaluation of multi-agent deep reinforcement learning algorithms. arXiv preprint arXiv:2006.07869,
2020.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning (ICML), pp.
2778-2787, 2017.
P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, and J Wang. Multiagent bidirectionally-coordinated
nets for learning to play starcraft combat games. arxiv 2017. arXiv preprint arXiv:1703.10069,
2017a.
Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang.
Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning
to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017b.
11
Published as a conference paper at ICLR 2022
Emanuele Pesce and Giovanni Montana. Improving coordination in small-scale multi-agent deep
reinforcement learning through memory-driven communication. Machine Learning, pp. 1-21,
2020.
Dawei Qiu, Jianhong Wang, Junkai Wang, and Goran Strbac. Multi-agent reinforcement learning
for automated peer-to-peer energy trading in double-side auction market. In Proceedings of the
Thirtieth International Joint Conference on Artificial Intelligence, IJCAI, pp. 2913-2920, 2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Tim Roughgarden and Eva Tardos. Introduction to the inefficiency of equilibria. Algorithmic Game
Theory, 17:443-459, 2007.
Maryam Sadeghlou, Mohammad Reza Akbarzadeh-T, and Mohammad Bagher Naghibi-S. Dynamic
agent-based reward shaping for multi-agent systems. In 2014 Iranian Conference on Intelligent
Systems (ICIS), pp. 1-6. IEEE, 2014.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and
logical foundations. Cambridge University Press, 2008.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887-5896. PMLR, 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
John N Tsitsiklis and Benjamin Van Roy. Optimal stopping of markov processes: Hilbert space theory,
approximation algorithms, and an application to pricing high-dimensional financial derivatives.
IEEE Transactions on Automatic Control, 44(10):1840-1851, 1999.
Alberto Viseras, Thomas Wiedemann, Christoph Manss, Lukas Magel, Joachim Mueller, Dmitriy
Shutin, and Luis Merino. Decentralized multi-agent exploration with online-learning of gaussian
processes. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pp.
4222-4229. IEEE, 2016.
Jianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, and Chongjie Zhang. Towards understanding
linear value decomposition in cooperative multi-agent q-learning. arXiv preprint arXiv:2006.00587,
2020a.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020b.
Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Shapley q-value: A local reward
approach to solve global reward games. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 7285-7292, 2020c.
Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim Green. Multi-agent reinforcement
learning for active voltage control on power distribution networks. Advances in Neural Information
Processing Systems, 34, 2021a.
Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Modelling hierarchical structure
between dialogue policy and natural language generator with option framework for task-oriented
dialogue system. In 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021. OpenReview.net, 2021b.
12
Published as a conference paper at ICLR 2022
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game
theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.
Yaodong Yang, Lantao Yu, Yiwei Bai, Jun Wang, Weinan Zhang, Ying Wen, and Yong Yu. A
study of ai population dynamics with million-agent reinforcement learning. arXiv preprint
arXiv:1709.04511, 2017.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580.
PMLR, 2018.
Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp.
10757-10766. PMLR, 2020.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient
methods. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit
assignment for multi-agent actor-critic. arXiv e-prints, pp. arXiv-2007, 2020a.
Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang,
Montgomery Alban, Iman Fadakar, Zheng Chen, et al. Smarts: Scalable multi-agent reinforcement
learning training school for autonomous driving. arXiv preprint arXiv:2010.09776, 2020b.
Martin Zinkevich, Amy Greenwald, and Michael Littman. Cyclic equilibria in markov games.
Advances in Neural Information Processing Systems, 18:1641, 2006.
13