Published as a conference paper at ICLR 2022
When Can We Learn General-Sum Markov
Games with a Large Number of Players
Sample-Efficiently?
Ziang Song
Song Mei
School of Mathematical Sciences, Peking University Department of Statistics, UC Berkeley
songziang@pku.edu.cn
songmei@berkeley.edu
Yu Bai
Salesforce Research
yu.bai@salesforce.com
Ab stract
Multi-agent reinforcement learning has made substantial empirical progresses in
solving games with a large number of players. However, theoretically, the best
known sample complexity for finding a Nash equilibrium in general-sum games
scales exponentially in the number of players due to the size of the joint action
space, and there is a matching exponential lower bound. This paper investigates
what learning goals admit better sample complexities in the setting of m-player
general-sum Markov games with H steps, S states, and Ai actions per player.
First, we design algorithms for learning an ε-Coarse Correlated Equilibrium (CCE)
in O(H5Smaxi≤m Ai∕ε2) episodes, and an ε-Correlated Equilibrium (CE) m
O(H6Smaxi≤m Ai2∕ε2) episodes. This is the first line of results for learning CCE
and CE with sample complexities polynomial in maxi≤m Ai . Our algorithm for
learning CE integrates an adversarial bandit subroutine which minimizes a weighted
swap regret, along with several novel designs in the outer loop. Second, we consider
the important special case of Markov Potential Games, and design an algorithm
that learns an ε-approximate Nash equilibrium within O(S i≤m Ai∕ε3) episodes
(when only highlighting the dependence on S, Ai , and ε), which only depends
linearly in Pi≤m Ai and significantly improves over existing efficient algorithms
in the ε dependence. Overall, our results shed light on what equilibria or structural
assumptions on the game may enable sample-efficient learning with many players.
1	Introduction
Multi-agent reinforcement learning (RL) has achieved substantial recent successes in solving artificial
intelligence challenges such as GO (Silver et al., 2016; 2018), multi-player games with team play
such as Starcraft (Vinyals et al., 2019) and Dota2 (Berner et al., 2019), behavior learning in social
interactions (Baker et al., 2019), and economic simulation (Zheng et al., 2020; Trott et al., 2021). In
many applications, multi-agent RL is able to yield high quality policies for multi-player games with a
large number of players (Wang et al., 2016; Yang et al., 2018).
Despite these empirical progresses, theoretical understanding of when we can sample-efficiently
solve multi-player games with a large number of players remains elusive, especially in the setting of
multi-player Markov games. A main bottleneck here is the exponential blow-up of the joint action
space—The total number of joint actions in a generic game with simultaneous plays is equal to
the product of the number of actions for each player, which scales exponentially in the number of
players. Such an exponential dependence is indeed known to be unavoidable in the worst-case for
certain standard problems. For example, for learning an approximate Nash equilibrium from payoff
queries in an one-step multi-player general-sum game, the query complexity lower bound of Chen
et al. (2015) and Rubinstein (2016) shows that at least exponentially many queries (samples) is
1
Published as a conference paper at ICLR 2022
required, even when each player only has two possible actions and the query is noiseless. Moreover,
for learning Nash equilibrium in Markov games, the best existing sample complexity upper bound
also scales with the size of the joint action space (Liu et al., 2021).
Nevertheless, these exponential lower bounds do not completely rule out interesting theoretical
inquiries—there may well be other notions of equilibria or additional structures within the game that
allow us to learn with a better sample complexity. This motivates us to ask the following
Question: When can we solve general-sum Markov games with sample complexity
milder than exponential in the number of players?
This paper makes steps towards answering the above question by considering multi-player general-
sum Markov games (MGs) with m players, H steps, S states, and Ai actions per player. We make two
lines of investigations: (1) Can we learn alternative notions of equilibria with better sample complexity
than learning Nash; (2) Can the Nash equilibrium be learned with better sample complexity under
additional structural assumptions on the game. This paper makes contributions on both ends, which
we summarize as follows.
•	We first design an algorithm that learns the ε-approximate Coarse Correlated Equilibrium
(CCE) with O(H5Smaxi∈[m] Ai∕ε2) episodes of play (Section 3). Our algorithm CCE-V-
Learning is a multi-player adaptation of the Nash V-Learning algorithm of Bai et al. (2020).
•	We design an algorithm CE-V-LEARNING which learns the stricter notion of ε-approximate
Correlated Equilibrium (CE) with O(H6Smaxi∈[m] Ai2∕ε2) episodes of play (Section 4). For
Markov games, these are the first line of sample complexity results for learning CE and CCE that
only scales polynomially with maxi∈[m] Ai, and improves significantly in the Ai dependency
over the current best algorithm which scales with Qi∈[m] Ai.
•	Technically, our algorithm CE-V-Learning makes several major modifications over CCE-
V-Learning in order to learn the CE (Section 4.2). Notably, inspired by the connection
between CE and low swap-regret learning, we use a mixed-expert Follow-The-Regularized
Leader algorithm within its inner loop to achieve low swap-regret for a particular adversarial
bandit problem. Our analysis also contains new results for adversarial bandits on weighted swap
regret and weighted regret with predicable weights, which may be of independent interest.
•	Finally, we consider learning Nash equilibrium in Markov Potential Games (MPGs), an important
subclass of general-sum Markov games. By a reduction to single-agent RL, we design an
algorithm NASH-CA that achieves O(ΦmaxH 3S	i∈[m] Ai∕ε3) sample complexity, where
Φmax ≤ Hm is the bound on the potential function (Section 5). Compared with the recent result
of Leonardos et al. (2021), we significantly improves the ε dependence from their 1∕ε6.
1.1 Related work
Learning equilibria in general-sum games The sample (query) complexity of learning Nash,
CE, and CCE from samples in one-step (i.e. normal form) general-sum games with m players
and Ai actions per player has been studied extensively in literature (Hart & Mas-Colell, 2000;
Hart, 2005; Stoltz, 2005; Cesa-Bianchi & Lugosi, 2006; Blum & Mansour, 2007; Fearnley et al.,
2015; Babichenko & Barman, 2015; Chen et al., 2015; Fearnley & Savani, 2016; Goldberg & Roth,
2016; Babichenko, 2016; Rubinstein, 2016; Hart & Nisan, 2018). It is known that learning Nash
equilibrium requires exponential in m samples in the worst case (Rubinstein, 2016), whereas CE and
CCE admit efficient poly(m, maxi≤m Ai)-sample complexity algorithms by independent no-regret
learning (Hart & Mas-Colell, 2000; Hart, 2005; Syrgkanis et al., 2015; Goldberg & Roth, 2016;
Chen & Peng, 2020; Daskalakis et al., 2021). Our results for learning CE and CCE can be seen as
extension of these works into Markov games. We remark that even when the game is fully known, the
computational complexity for finding Nash in general-sum games is PPAD-hard (Daskalakis, 2013).
Markov games Markov games (Shapley, 1953; Littman, 1994) is a widely used framework for
game playing with sequential decision making, e.g. in multi-agent reinforcement learning. Algorithms
with asymptotic convergence have been proposed in the early works of Hu & Wellman (2003); Littman
(2001); Hansen et al. (2013). A recent line of work studies the non-asymptotic sample complexity for
learning Nash in two-player zero-sum Markov games (Bai & Jin, 2020; Xie et al., 2020; Bai et al.,
2
Published as a conference paper at ICLR 2022
2020; Zhang et al., 2020; Liu et al., 2021; Chen et al., 2021; Jin et al., 2021; Huang et al., 2021)
and learning various equilibria in general-sum Markov games (Liu et al., 2021; Bai et al., 2021),
building on techniques for learning single-agent Markov Decision Processes sample-efficiently (Azar
et al., 2017; Jin et al., 2018). Learning the Nash equilibrium in general-sum Markov games are much
harder than that in zero-sum Markov games. Liu et al. (2021) present the first line of results for
learning Nash, CE, and CCE in general-sum Markov games; however their sample complexity scales
with Qi≤m Ai due to the model-based nature of their algorithm. Algorithms for computing CE in
extensive-form games has been widely studied (Von Stengel & Forges, 2008; Celli et al., 2020; Farina
et al., 2021; Morrill et al., 2021), though we remark Markov games and extensive-form games are
different frameworks and our results do not imply each other.
Markov potential games Lastly, a recent line of works considers Markov potential games (Macua
et al., 2018; Leonardos et al., 2021; Zhang et al., 2021), a subset of general-sum Markov games
in which the Nash equilibrium admits more efficient algorithms. Leonardos et al. (2021) gives a
sample-efficient algorithm based on the policy gradient method (Agarwal et al., 2021). The special
case of Markov cooperative games is studied empirically in e.g. Lowe et al. (2017); Yu et al. (2021).
For one step potential games, Kleinberg et al. (2009); Palaiopanos et al. (2017); Cohen et al. (2017a)
show the convergence to Nash equilibria of no-regret dynamics.
2	Preliminaries
We present preliminaries for multi-player general-sum Markov games as well as the solution concept
of (approximate) Nash equilibrium. Alternative solution concepts and other concrete subclasses of
Markov games considered in this paper will be defined in the later sections.
Markov games A multi-player general sum Markov game (MG; Shapley (1953); Littman (1994))
with m players can be described by a tuple MG(H, S, {Ai}im=1 , P, {ri}im=1), where H is the episode
length, S is the state space with |S | = S, Ai is the action space for the ith player with |Ai | = Ai .
Without loss of generality, We assume Ai = [AJ. We let a := (aι,…，am) denote the vector
of joint actions taken by all the players and A = Ai X … X Am denote the joint action space.
Throughout this paper We assume that S and Ai are finite. The transition probability P = {Ph}h∈[H]
is the collection of transition matrices, where Ph(∙∣s, a) ∈ ∆s denotes the distribution of the next
state When actions a are taken at state s at step h. The reWards ri = {rh,i}h∈[H],i∈[m] is the collection
of reward functions for the ith player, where rh,i(s, a) ∈ [0, 1] gives the deterministic1 reward of ith
player if actions a are taken at state s at step h. Without loss of generality, we assume the initial state
s1 is deterministic. A key feature of general-sum games is that the rewards ri are in general different
for each player i, and the goal of each player is to maximize her own cumulative reward.
Markov product policy, value function A product policy is a collection of m policies π :=
{πi }i∈[m] where πi is the general (potentially history-dependent) policy for the i-th player. We
first focus on the case of Markov product policies, in which πi = {πh,i : S → ∆Ai }h∈[H], and
∏h,i(ai∣s) is the probability for the ith player to take action ai at state S at step h. For a policy ∏
and i ∈ [m], we use π-i := {πj}j∈[m],j6=i to denote the policy of all but the ith player. The value
function Vhπ,i(s) : S → R is defined as the expected cumulative reward for the ith player when policy
π is taken starting from state s and step h:
H
Vhπ,i(s) := Eπ	rh0,i (sh0 , ah0 )sh = s .	(1)
h0=h
Best response & Nash equilibrium For any product policy π = {πi }i∈[m] , the best response for
the ith player against n-i is defined as any policy ∏t such that V1τl,π-i(si) = sup∏0 Viπi,π-i(si).
For any Markov product policy, this best response is guaranteed to exist (and be Markov) as the above
maximization problem is equivalent to solving a Markov Decision Process (MDP) for the ith player.
We will also use the notation V[iπ-i(si) to denote the above value function V[「(si).
1Our results can be straightforwardly generalized to Markov games with stochastic rewards.
3
Published as a conference paper at ICLR 2022
We say ∏ is a Nash equilibrium (e.g. Nash (1951); Pero山t et al. (2017)) if all players play the best
response against other players, i.e., for all i ∈ [m],
V∏i(sι)= V『(si).
Note that in general-sum MGs, there may exist multiple Nash equilibrium policies with different
value functions, unlike in two-player zero-sum MGs (Shapley, 1953). To measure the suboptimality
of any policy π, we define the NE-gap as
NE-gaP(π) := max SUpVμi,π-i(SI)- Vni(SI).
i∈[m] L μi	_
For any ε ≥ 0, we say π is ε-approximate Nash equilibrium (ε-Nash) if NE-gap(π) ≤ ε.
General correlated policy & Its best response A general correlated policy π is a set of H maps
∏ := {∏h : Ω X (SX A)h-i × S → ∆∕}h∈[H]. The first argument of ∏h is a random variable
ω ∈ Ω sampled from some underlying distribution, and the other arguments contain all the history
information and the current state information (unlike Markov policies in which the policies only
depend on the current state information). The output of πh is a general distribution of actions in
A = Ai X …X Am (unlike product policies in which the action distribution is a product distribution).
For any correlated policy π = {πh }h∈[H] and any player i, we can define a marginal policy π-i
as a set of H maps ∏-i := {∏h,-i : Ω × (SX A)h-i × S → ∆∕-Jh∈[H] where A-i :=
Ai x∙∙∙x Ai-ι X Ai+ι x∙∙∙x Am, and the output of ∏h,-i is defined as the marginal distribution
of the output of πh restricted to the space A-i . For any general correlated policy π, we can define its
initial state value function Viπ,i(Si) similar as (1). The best response value of the ith player against π
is V*iπ-i(sι) = supμi V1μi,π-i (si), where VK-i(si) is the value function of the policy (μ%, π-i)
(the ith player plays according to general policy μi, and all other players play according to ∏-i), and
the supremum is taken over all general policy μ% of the ith player.
Learning setting Throughout this paper we consider the interactive learning (i.e. exploration)
setting where algorithms are able to play episodes within the MG and observe the realized transitions
and rewards. Our focus is on the PAC sample complexity (i.e. number of episodes of play) for any
learning algorithm to output an approximate equilibrium.
2.1	Exponential lower bound for learning approximate Nash equilibrium
The focus of this paper is the setting where the number of players m is large. Intuitively, as the
joint action space has size |A| = Qim=i Ai which scales exponentially in m (if each Ai ≥ 2), naive
algorithms for learning Nash equilibrium may learn all ri(a) by enumeratively querying all a ∈ A,
and this costs exponential in m samples. Unfortunately, recent work shows that such exponential in
m dependence is unavoidable in the worst-case for any algorithm—there is an exp(Ω(m)) sample
complexity lower bound for learning approximate Nash, even in one-step general-sum games (Chen
et al., 2015; Rubinstein, 2016) (see Proposition A.2 for formal statement).
This suggests that the Nash equilibrium as a solution concept may be too hard to learn efficiently
for MGs with a large number of players, and calls for alternative solution concepts or additional
structural assumptions on the game in order to achieve an improved m dependence.
3	Efficient Learning of Coarse Correlated Equilibria (CCE)
Given the difficulty of learning Nash when the number of players m is large , we consider learning
other relaxed notions of equilibria for general-sum MGs. Two standard notions of equilibria for
games are the Correlated Equilibrium (CE) and Coarse Correlated Equilibrium (CCE), and they
satisfy {Nash} ⊂ {CE} ⊂ {CCE} for general-sum MGs (Nisan et al., 2007).
We begin by considering learning CCE (most relaxed notion above) for Markov games.
Definition 1 (ε-approximate CCE for general-sum MGs). We say a (general) correlated policy π is
an ε-approximate Coarse Correlated Equilibrium (ε-CCE) if
maX(Vit,iπ-i (Si)- I(Si)) ≤ ε
4
Published as a conference paper at ICLR 2022
K≥O
We say π is an (exact) CCE if the above is satisfied with ε = 0.
The following result shows that there exists an algorithm that can learn an ε-approximate CCE in
general-sum Markov games within O(H 5S maxi∈[m] Ai∕ε2) episodes of play.
Theorem 2 (Learning ε-approximate CCE for general-sum MGs). Suppose we run the CCE-V-
LEARNING algorithm (Algorithm 4) for all m players and
H5S maxi∈[m] Aiι	H4Si3
ε2	+ ε
episodes (ι = log(m maxi∈[m] AiHSK∕(pε)) is a log factor). Then with probability at least 1 - p,
the certified policy b defined in Algorithm 2 is an ε-CCE,i.e. maXi∈[m](V1^iπ-i (si) 一 VIbi(Sι)) ≤ ε.
Mild dependence on action space For small enough ε, the sample complexity featured in Theo-
rem 2 scales as Oe(H5S maxi∈[m] Ai∕ε2). Most notably, this is the first algorithm that scales with
maxi∈[m] Ai, and exhibits a sharp difference in learning Nash and learning CCE in view of the
exp(Ω(m)) lower bound for learning Nash in Proposition A.2. Indeed, existing algorithms such as
Multi-Nash-VI Algorithm with CCE subroutine (Liu et al., 2021) does require Oe(H4S2 Qim=i Ai∕ε2 )
episodes of play, which scales with Qi∈[m] Ai due to its model-based nature. We achieve significantly
better dependence on Ai and also S, though slightly worse H dependence.
Overview of algorithm and techniques Our CCE-V-LEARNING algorithm (deferred to Ap-
pendix B.1 due to space limit) is a multi-player adaptation of the Nash V-Learning algorithm
of Bai et al. (2020); Tian et al. (2021) for learning Nash equilibria in two-player zero-sum MGs.
Similar as Bai et al. (2020), we show that this algorithm enjoys a “no-regret” like guarantee for each
player at each (h, s) (Lemma B.3). We also adopted the choice of hyperparameters in Tian et al.
(2021) so that the sample complexity has a slightly better dependence in H. When combined with the
“certified correlated policy” procedure (Algorithm 2), our algorithm outputs a policy that is ε-CCE.
Our certified policy procedure is adapted from the certified policy of Bai et al. (2020), and differs in
that ours output a correlated policy for all the players whereas Bai et al. (2020) outputs a product
policy. The key feature enabling this maxi∈[m] Ai dependence is that this algorithm uses decentral-
ized learning for each player to learn the value function (V ), instead of learning the Q function (as
in Liu et al. (2021)) that requires sample size scales as Qi∈[m] Ai. The proof of Theorem 2 is in
Appendix B.
4	Efficient Learning of Correlated Equilibria (CE)
In this section, we move on to considering the harder problem of learning Correlated Equilibria (CE).
We first present the definition of CE in Markov games.
Definition 3 (Strategy modification for ith player). A strategy modification φ := {φh,s}(h,s)∈[H]×S
for player i is a set of H × S functions φh,s : (S × A)h-i × Ai → Ai. A strategy modification φ
can be composed with any policy π to give a modified policy φ π defined as follows: At any step
h and State S with the history information τh-ι = (si, ai, ∙ ∙ ∙ , sh-1, ah—i), if ∏ chooses to play
a = (ai, . . . , am), the modified policy φ π will play (ai, . . . , ai-i, φh,s(τh-i, ai), ai+i, . . . , am).
We use Φi denote the set of all possible strategy modifications for player i.
Definition 4 (ε-approximate CE for general-sum MGs). We say a (general) correlated policy π is an
ε-approximate CE (ε-CE) if
max sup Viφ,iπ(si) 一 Viπ,i(si) ≤ ε.
i∈[m] φ∈Φi
We say π is an (exact) CE if the above is satisfied with ε = 0.
Our definition of CE follows (Liu et al., 2021) and is a natural generalization of the CE for the
well-studied special case of one-step (i.e. normal form) games (Nisan et al., 2007).
5
Published as a conference paper at ICLR 2022
4.1	Algorithm description
Our algorithm CE-V-Learning (Algorithm 1) builds further on top of CCE-V-Learning and
Nash V-Learning, and makes several novel modifications in order to learn the CE. The key feature of
CE-V-Learning is that it uses a weighted swap regret algorithm (mixed-expert FTRL) for every
(s, h, i). At a high-level, CE-V-LEARNING consists of the following steps:
•	Line 6-11 (Sample action using mixed-expert FTRL): For each (h, s) we maintain Ai “sub-
experts” indexed by b0 ∈ [Ai] (Each sub-expert represents an independent “expert” that runs
her own FTRL algorithm). Sub-expert b0 first computes an action distribution qb0(∙) ∈ ∆∕i via
Follow-the-Regularized-Leader (FTRL; Line 8). Then we employ a two-step sampling procedure
to obtain the action: First sample a sub-expert b from a suitable distribution μ computed from
{qb0}b0∈[Ai], then sample the actual action ah,i from qb.
•	Line 13-17 (Take action and record observations): Player i takes action ah,i and observes other
player’s actions, the reward, and the next state. Sub-expert b then computes a loss estimator and
weight according to the observations, which will be used in future FTRL updates.
•	Line 19 (Optimistic value update): Updates the optimistic estimate of the value Vh,i using
step-size αt and bonus βt.
Finally, after executing Algorithm 1 for K episodes, we use the certified correlated policy procedure
(Algorithm 2) to obtain our final output policy πb. This procedure is a direct modification of the
certified policy procedure of (Bai et al., 2020) and outputs a correlated policy (because the randomly
sampled k and l in line 1 and line 4 of Algorithm 2 are used by all the players) instead of product
policy. The same procedure is also used for learning CCEs earlier in Section 3.
Here we specify the hyperparameters used in Algorithm 1:
at = (H + 1)∕(H + t), ηt = P ∣∕(Ait),	βt = CH 2Aipι∕t + 2cH 2ι∕t.	(2)
The constants αtj used in Algorithm 2 is defined as
αt0 := Qtk=1 (1 - αk) , αtj := αj Qtk=j+1 (1 - αk) .	(3)
Note that for any t ≥ 1, {αtj}1≤j≤t sums to one and defines a distribution over [t].
4.2	Overview of techniques
Here we briefly overview the techniques used in Algorithm 1.
Minimizing swap regret via mixed-expert FTRL The key technical advance in our Algorithm 1
over CCE-V-Learning and Nash V-Learning is the use of mixed-expert FTRL (Line 6-11). The
purpose of this is to allow the algorithm to achieve low swap regret at each (h, s) in a suitable
sense—For one-step (normal form) games, it is known that combining low-swap-regret learning for
each player leads to an approximate CE (Stoltz, 2005; Cesa-Bianchi & Lugosi, 2006). To integrate
this into Markov games, we utilize a celebrated reduction from low-swap-regret learning to usual
low-regret learning (Blum & Mansour, 2007), which for any bandit problem with Ai actions maintains
Ai sub-experts each running its own FTRL algorithm. Our particular application builds upon the
two-step randomization scheme of Ito (2020) which first samples a sub-expert b and the action from
this sub-expert. The distribution μ(∙) for sampling the sub-expert is carefully chosen by solving a
linear system (Line 10) so that μ also coincides with the (marginal) distribution of the sampled action,
from which the reduction follows.
FTRL with predictable weights Applied naively, the above reduction does not directly work for
our purpose, as our analysis requires minimizing the weighted swap regret with weights αit, whereas
the reduction of Ito (2020) relies crucially on the vanilla (average) regret. We address this challenge
by using a slightly modified FTRL algorithm for each sub-expert that takes in random but predictable
weights (i.e. depending fully on prior information and “external” randomness). We present the
analysis for such FTRL algorithm in Appendix F.4, and the consequent analysis for the weighted
swap regret in Appendix F.1-F.3, both of which may be of independent interest.
Proposal distributions Finally, a nuanced but important new design in CE-V-LEARNING is that all
sub-experts compute a proposal action distribution to sample the sub-expert and the associated action.
6
Published as a conference paper at ICLR 2022
Algorithm 1 CE-V-LEARNING for general-sum MGs (i-th player’s version)
Require: Hyperparameters: {αj}ι≤j≤t≤κ, {αt}ι≤t≤κ, {ηt}ι≤t≤κ, {βt}ι≤t≤κ.
1:	Initialize: For any (s,a,h), set Vh,i(s) - H, Nh(S) - 0. Set μh(a∣s) - 1/Ai, qb (a|s)一
1/Ai, 'h0t(a∣s) - 0, Nh(S) - 0 for all (b0, a, h, s,t) ∈ [Ai] × [Ai] × [H] ×S × [K].
2:	for episode k = 1, . . . , K do
3:	Receive S1.
4:	for step h = 1, . . . , H do
5:	// Compute proposal action distributions by FTRL
6:	Update accumulator t := Nh(Sh) J Nh(Sh) + 1. Set Ut J Ot/α1.
7:	Let tbo J Nb(Sh) for all b0 ∈ [Ai] for shorthand.
8:	Compute the action distribution for all sub-experts b0 ∈ [Ai]:
qh ⑷Sh) 8。exP ( -((Itb0 /ut) PT= 1 wh,τ (b0|Sh)'h0,T (a|Sh)).
9:	// Sample sub-expert b and action from qb(∙)
10:	Compute μh(∙∣Sh) ∈ ∆[Ai] by solving μh(∙∣Sh) = PA=I μh(b,Sh)qh(∙∣Sh).
11:	Sample sub-expert b 〜 μ%(∙∣Sh), and then action ah,i 〜 qh (N).
12:	// Take action and feed the observations to sub-expert b
13:	Take action ah,i, observe the actions ah,-i from all other players.
14:	Observe reward rh,i = rh,i(Sh, ah,i, ah,-i) and the next state Sh+1 from the environment.
15:	Update accumulator for the sampled sub-expert:tb:= Nhb(Sh) J Nhb(Sh) + 1.
16:	Compute and update loss estimator
[H - h + 1 - (rh,i + min{Vh+1,i(Sh+1), H - h})] /H ∙ 1 {αh,i = a}
qh (a|Sh) + ηtb	.
17:	Set wh,tb(b|Sh) J ut.
18:	// Optimistic value update
19:	Vh,i (Sh)	J (1 -	αt)	Vh,i	(Sh)	+ αt	(rh,i (Sh, ah,i,	ah,-i)	+ Vh+1,i (Sh+1) + βt)∙
'h,tb (a|Sh) J
Algorithm 2 Certified correlated policy πb for general-sum MGs
1:	Sample k J Uniform([K]).
2:	for step h = 1, . . . , H do
3:	Observe Sh, and set t J Nhk(Sh) (the value of Nh(Sh) at the beginning of the k’th episode).
4:	Sample l ∈ [t] with P(l = j) = αtj (c.f. Eq. (3)).
5:	Update k J khl (Sh ) (the episode at the end of which the state Sh is observed exactly l times).
6:	Jointly take action (ah,,ι, ah2,…，ah,m)〜Qm=I μh,i(∙∣Sh), where μh,i(∙∣Sh) is the policy
μh,i(∙∣Sh) at the beginning of the k,th episode.
Then, only the sampled sub-expert takes this action, and all other proposal distributions are discarded.
This is different from the original algorithms of (Blum & Mansour, 2007; Ito, 2020) in which the
FTRL update come after the sub-expert sampling and only happens for the sampled sub-expert. Our
design is required here as otherwise the sub-experts are required to predict the next time when it is
sampled in order to compute the weighted FTRL update, which is impossible.
4.3 Theoretical guarantee
We are now ready to present the theoretical guarantee for our CE-V-Learning algorithm.
Theorem 5 (Learning ε-approximate CE for general-sum MGs). Suppose we run the CE-V-
LEARNING algorithm (Algorithm 1) for all m players for
K ≥ O( H 6S maxi∈[m] A2l +
H4S maxi∈m] Ai ι3
ε
7
Published as a conference paper at ICLR 2022
episodes (ι = log(mmaxi∈[m] AiHSK/(pε) is a log factor). Then with probability
at least 1 - p, the certified correlated policy πb defined in Algorithm 2 is an ε-CE, i.e.
maxi∈[m] supφ∈Φi (V1φ,iπ(s1) - V1πb,i(s1)) ≤ ε.
Discussions To the best of our knowledge, Theorem 5 presents the first result for learning CE that
scales polynomially with maxi∈[m] Ai, which is significantly better than the best known existing
algorithm of Multi-Nash-VI with CE subroutine (Liu et al., 2021) whose sample complexity scales
with i∈[m] Ai . Similar as in Theorem 2, this follows as our CE-V-LEARNING uses decentralized
learning for each player to learn the value function (V ) . We also observe that our sample complexity
for learning CE is higher than for learning CCE by a factor of O(H maxi∈[m] Ai); the additional
maxi∈[m] Ai factor is expected as CE is a strictly harder notion of equilibrium. The proof of
Theorem 5 can be found in Appendix C.
5	Learning Nash Equilibria in Markov Potential Games
In this section, we consider learning Nash equilibria in Markov Potential Games (MPGs), an important
subclass of general-sum MGs. Despite the curse of number of players of learning Nash in general-sum
MGs, recent work shows that learning Nash in MPGs does not require sample size exponential in m,
by using stochastic policy gradient based algorithms (Leonardos et al., 2021; Zhang et al., 2021). In
this section, we provide an alternative algorithm Nash-CA that also achieves a mild dependence on
m and an improved dependence on ε by a simple reduction to single-agent learning.
5.1	Markov potential games
We first present the definition of MPGs. Our definition is the finite-horizon variant2 of the definitions
of Macua et al. (2018); Leonardos et al. (2021); Zhang et al. (2021) and is slightly more general as
we only require (4) on the total return. Throughout this section, π denotes a Markov product policy.
Definition 6. (Markov potential games) A general-sum Markov game is a Markov potential game if
there exists a potential function Φ mapping any product policy to a real number in [0, Φmax], such
that for any i ∈ [m], any two policies πi, πi0 of the ith player, and any policy π-i of other players, the
difference of the value functions of the ith player with policies (πi, π-i) and (πi0, π-i) is equals the
difference of the potential function on the same policies, i.e.,
V1π,ii,π-i(s1)-V1π,ii0,π-i(s1)=Φ(πi,π-i)-Φ(πi0,π-i).	(4)
Note that the range of the potential function Φmax admits a trivial upper bound Φmax ≤ mH (this
can be seen by varying πi for one i at a time). An important example of MPGs is Markov Cooperative
Games (MCGs) where all players share the same reward ri ≡ r.
5.2	Algorithm and theoretical guarantee
We present a simple algorithm NASH-CA (Nash Coordinate Ascent) for learning an ε-Nash in MPGs.
As its name suggests, the algorithm operates by solving single-agent Markov Decision Processes
(MDPs) one player at a time, and intrinsically performing coordinate ascent on the potential function
of the Markov game. Due to the potential structure of MPGs and the boundedness of the potential
function, the local improvements of players across the steps can have an accumulative effect on the
potential function, and the algorithm is guaranteed to stop after a bounded number of steps. We
give the full description of the Nash-CA in Algorithm 3. We remark that Nash-CA is additionally
guaranteed to output a pure-strategy Nash equilibrium (cf. Appendix D for definition).
Theorem 7 (Sample complexity for NASH-CA). For Markov potential games, with probability
at least 1 一 P, Algorithm 3 terminates within 4Φmax/ε steps of the while loop, and outputs an
ε-approximate (pure-strategy) Nash equilibrium. The total episodes of play is at most
ΦmaχH3S Pm=i 4- ΦmaχH3S2 PZl 4∣2
ε3	+	ε2
Where ι = log(mHSK maχ1≤i≤m Ai)is a log factor.
2Our results can easily adapted to the discounted infinite time horizon setup.
8
Published as a conference paper at ICLR 2022
Algorithm 3 NASH-CA for Markov potential games
Require: Error tolerance ε
1:	Initialize: π = {πi}i∈[m], where πi = {πh,i}(h,i)∈[H]×[m] for some deterministic policy πh,i.
2:	while true do
3:	Execute policy ∏ for N = Θ(HI) episodes and obtain V1,i(∏) which is the empirical average
of the total return under policy π.
4:	for player i = 1, . . . , m do
5:	FiX π-i, and let the ith player run UCBVI-UPLOW (Algorithm 7) for Ki = Θ( HSAI +
HSjAi1) episodes and get a new deterministic policy ∏i.
6:	Execute policy (bi, ∏-i) for N = Θ(Hj2ι) episodes and obtain Vbι,i(∏i, ∏-i) which is the
empirical average of the total return under policy (πbi, π-i).
7:	Set ∆i - bι,i(∏i, ∏-i) - bι,i(∏).
8:	if maxi∈[m] ∆i > ε∕2 then
9:	Update ∏j J bj where j = argmaxi∈[m] ∆i.
10:	else
11:	return π
Discussions For small enough ε, the sample complexity for the NASH-CA algorithm in the above
theorem is Oe(ΦmaχH3S Pi≤m Ai∕ε3). As Φmaχ ≤ mH, this at most scales with the number of
players as m Pi≤m Ai, which is much better than the exponential in m sample complexity for
general-sum MGs without additional structures. Compared with recent results on learning Nash via
policy gradients (Leonardos et al., 2021; Zhang et al., 2021), the Nash-CA algorithm also achieves
poly(m, maxi≤m Ai) dependence, and significantly improves on the ε dependence from their ε-6
to ε-3. In addition, our algorithm does not require assumptions on bounded distribution mismatch
coefficient as they do, due to the exploration nature of our single-agent MDP subroutine.
Also, compared with the sample complexity bound O(H4S2 Qm=I Ai∕ε2) of the Nash-VI algo-
rithm (Liu et al., 2021) for general-sum MGs (not restricted to MPGs), our Nash-CA algo-
rithm doesn’t suffer from the exponential dependence on m thanks to the MPG structure. We
do achieve a looser in the dependence on ε, yet overall our sample complexity is still better un-
less ε < (Pim=1 Ai)∕(Qim=1 Ai) is exponentially small. The proof of Theorem 7 can be found in
Appendix D.
A lower bound To accompany Theorem 7, we establish a sample complexity lower bound of
Ω(H2 Pm=I Ai∕ε2) for learning pure-strategy Nash in MCGs and hence MPGS (Theorem E.1 in
Appendix E). This lower bound improves in the Ai dependence over the naive reduction to single-
player MDPs (Domingues et al., 2021), which gives Ω(H3Smaxi∈[m] Ai∕ε2), though is loose on
the S, H dependence. The improved Ai dependence is achieved by constructing a novel class of hard
instances of on one-step games (Lemma E.2), which may be of further technical interest. However,
there is still a large gap between these lower bounds and the best current upper bound of either our
Oe(Pi=1 Ai∕ε3) or the Oe(Qi=1 Ai∕ε2) of Liu et al. (2021), which we leave as future work.
6 Conclusion
This paper investigates the question of when can we solve general-sum Markov games (MGs) sample-
efficiently with a mild dependence on the number of players. Our results show that this is possible
for learning approximate (Coarse) Correlated Equilibria in general-sum MGs, as well as learning
approximate Nash equilibrium in Markov potential games. In both cases, our sample complexity
bounds improve over existing results in many aspects. Our work opens up many interesting directions
for future work, such as sharper algorithms for both problems, sample complexity lower bounds,
or how to perform sample-efficient learning in general-sum MGs with function approximations. In
addition to Markov potential games, it would also be interesting to explore alternative structural
assumptions that permit sample-efficient learning.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Ziang Song is partially supported by the elite undergraduate training program of School of Mathe-
matical Sciences in Peking University.
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1-76, 2021.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for rein-
forcement learning. In International Conference on Machine Learning, pp. 263-272. PMLR,
2017.
Yakov Babichenko. Query complexity of approximate nash equilibria. Journal of the ACM (JACM),
63(4):1-24, 2016.
Yakov Babichenko and Siddharth Barman. Query complexity of correlated equilibrium. ACM
Transactions on Economics and Computation (TEAC), 3(4):1-9, 2015.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In
International Conference on Machine Learning, pp. 551-560. PMLR, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances in
Neural Information Processing Systems, 33, 2020.
Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efficient learning of stackelberg equilibria
in general-sum games. arXiv preprint arXiv:2102.11494, 2021.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,
2019.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemySIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning
Research, 8(6), 2007.
Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for
extensive-form correlated equilibrium. arXiv preprint arXiv:2004.00603, 2020.
Nicolo Cesa-Bianchi and Ggbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Xi Chen and Binghui Peng. Hedging in games: Faster convergence of external and sWap regrets.
arXiv preprint arXiv:2006.04953, 2020.
Xi Chen, Yu Cheng, and Bo Tang. Well-supported versus approximate nash equilibria: Query
complexity of large games. arXiv preprint arXiv:1511.00785, 2015.
Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for tWo-player markov
games With linear function approximation. arXiv preprint arXiv:2102.07404, 2021.
Johanne Cohen, Amelie Heliou, and Panayotis Mertikopoulos. Learning With bandit feedback in
potential games. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 6372-6381, 2017a.
Michael B Cohen, Jonathan Kelner, John Peebles, Richard Peng, Anup B Rao, Aaron Sidford, and
Adrian Vladu. Almost-linear-time algorithms for markov chains and neW spectral primitives
for directed graphs. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, pp. 410-419, 2017b.
10
Published as a conference paper at ICLR 2022
Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement
learning. arXiv preprint arXiv:1510.08906, 2015.
Constantinos Daskalakis. On the complexity of approximating a nash equilibrium. ACM Transactions
OnAlgorithms (TALG), 9(3):1-35, 2013.
Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning
in general games. arXiv preprint arXiv:2108.06924, 2021.
Omar DarWiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic rein-
forcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning
Theory, pp. 578-598. PMLR, 2021.
Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret
learning dynamics for extensive-form correlated equilibrium. arXiv preprint arXiv:2104.01520,
2021.
John Fearnley and Rahul Savani. Finding approximate nash equilibria of bimatrix games via payoff
queries. ACM Transactions on Economics and Computation (TEAC), 4(4):1-19, 2016.
John Fearnley, Martin Gairing, Paul W Goldberg, and Rahul Savani. Learning equilibria of games
via payoff queries. J. Mach. Learn. Res., 16:1305-1344, 2015.
Paul W Goldberg and Aaron Roth. Bounds for the query complexity of approximate equilibria. ACM
Transactions on Economics and Computation (TEAC), 4(4):1-25, 2016.
Richard W Hamming. Error detecting and error correcting codes. The Bell system technical journal,
29(2):147-160, 1950.
Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri ZWick. Strategy iteration is strongly
polynomial for 2-player turn-based stochastic games With a constant discount factor. Journal of
the ACM (JACM), 60(1):1-16, 2013.
Sergiu Hart. Adaptive heuristics. Econometrica, 73(5):1401-1430, 2005.
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127-1150, 2000.
Sergiu Hart and Noam Nisan. The query complexity of correlated equilibria. Games and Economic
Behavior, 108:401-410, 2018.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039-1069, 2003.
Baihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. ToWards general function approxima-
tion in zero-sum markov games. arXiv preprint arXiv:2107.14702, 2021.
Shinji Ito. A tight loWer bound and efficient reduction for sWap regret. Advances in Neural Information
Processing Systems, 33, 2020.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient?
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 4868-4878, 2018.
Chi Jin, Qinghua Liu, and Tiancheng Yu. The poWer of exploiter: Provable multi-agent rl in large
state spaces. arXiv preprint arXiv:2106.03352, 2021.
Robert Kleinberg, Georgios Piliouras, and Eva Tardos. Multiplicative updates outperform generic
no-regret learning in congestion games. In Proceedings of the forty-first annual ACM symposium
on Theory of computing, pp. 533-542, 2009.
Tor Lattimore and Csaba Szepesvdri. Bandit algorithms. Cambridge University Press, 2020.
Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence
of multi-agent policy gradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021.
11
Published as a conference paper at ICLR 2022
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier,1994.
Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322-328,
2001.
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning, pp. 7001-7010. PMLR,
2021.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Sergio Valcarcel Macua, Javier Zazo, and Santiago Zazo. Learning parametric closed-loop policies
for markov potential games. In International Conference on Learning Representations, 2018.
Dustin Morrill, Ryan D’Orazio, Marc Lanctot, James R Wright, Michael Bowling, and Amy R
Greenwald. Efficient deviation types and learning for hindsight rationality in extensive-form games.
In International Conference on Machine Learning, pp. 7818-7828. PMLR, 2021.
John Nash. Non-cooperative games. Annals of mathematics, pp. 286-295, 1951.
Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.
arXiv preprint arXiv:1506.03271, 2015.
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. Algorithmic Game Theory.
Cambridge University Press, 2007.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. arXiv preprint
arXiv:1703.01138, 2017.
Julien P6rolat, Florian Strub, Bilal Piot, and Olivier Pietquin. Learning nash equilibrium for general-
sum markov games from batch data. In Artificial Intelligence and Statistics, pp. 232-241. PMLR,
2017.
Aviad Rubinstein. Settling the complexity of computing approximate two-player nash equilibria. In
2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 258-265.
IEEE, 2016.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140-1144, 2018.
Gilles Stoltz. Incomplete information and internal regret in prediction of individual sequences. PhD
thesis, Universite Paris Sud-Paris XL 2005.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of
regularized learning in games. arXiv preprint arXiv:1507.00407, 2015.
Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown markov games.
In International Conference on Machine Learning, pp. 10279-10288. PMLR, 2021.
Alexander Trott, Sunil Srinivasa, Douwe van der Wal, Sebastien Haneuse, and Stephan Zheng.
Building a foundation for data-driven, interpretable, and robust policy design using the ai economist.
arXiv preprint arXiv:2108.02904, 2021.
12
Published as a conference paper at ICLR 2022
Oriol Vinyals,Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Bernhard Von Stengel and Frangoise Forges. Extensive-form correlated equilibrium: Definition and
computational complexity. Mathematics of Operations Research, 33(4):1002-1022, 2008.
Jun Wang, Weinan Zhang, and Shuai Yuan. Display advertising with real-time bidding (rtb) and
behavioural targeting. arXiv preprint arXiv:1610.03013, 2016.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory, pp. 3674-3682. PMLR, 2020.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging
sample-efficient offline and online reinforcement learning. arXiv preprint arXiv:2106.04895, 2021.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580.
PMLR, 2018.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
Kaiqing Zhang, Sham M Kakade, Tamer BaSar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020.
Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in multi-agent markov stochastic games:
Stationary points and convergence. arXiv preprint arXiv:2106.00198, 2021.
Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes,
and Richard Socher. The ai economist: Improving equality and productivity with ai-driven tax
policies. arXiv preprint arXiv:2004.13332, 2020.
A	EXPONENTIAL IN m LOWER B OUND FOR LEARNING NASH IN
General- sum MGs
In this section, we give a sample complexity lower bound for computing approximate Nash equi-
librium in one-step binary-action general-sum MGs (H = 1, S = 1 and Ai = 2) which has an
exponential dependence in m, the number of players. The result is built on the lower bound of query
complexity in Rubinstein (2016).
We use G to denote the one-step Markov game (H = 1 and S = 1), in which there are m players and
A = 2 actions for each player. We index the players by [m] = {1, . . . , m} and denote the actions
space of each player by [A] = {1, 2}. Since we restricted attention to binary-action games (i.e.
A = 2), the total number of joint actions is 2m.
We define a (exact) query as the procedure where the algorithm queries a joint action a ∈ [A]m and
observes the (deterministic) reward ri(a) ∈ [0, 1]. We define the query complexity (Chen et al., 2015)
for learning ε-approximate Nash equilibrium (ANE) as the following.
Definition A.1 (Query complexity). The query complexity QCp(ANE(m, ε)) for learning ε-ANE is
defined as the smallest n such that there exists a randomized oracle algorithm A satisfying the
following: for any binary-action, m-player game G, the algorithm A can use no more than n
sequential queries of the reward to output an ε-ANE with probability at least 1 - p.
In one-step MGs with deterministic reward, the query complexity is equivalent to the sample com-
plexity, since each query obtains a reward entry. The following result in Rubinstein (2016) gives a
2Q(m) query complexity lower bound for learning ε0-ANE in m-player binary action games.
13
Published as a conference paper at ICLR 2022
Algorithm 4 CCE-V-Learning for General-sum MGs (i-th player’s version)
Require: Hyperparameters: {αt}ι≤t≤κ, {ηt}ι≤t≤κ, {βt}ι≤t≤κ. _
1:	Initialize: For any (s,a,h), set Vh,i(s) - H, Vh,i(s) - 0, Lh,i — 0, μh(a∣s) - 1/Ai,
Nh(S) — 0.
2:	for episode k = 1, . . . , K do
3:	Receive s1.
4:	for step h = 1, . . . , H do
5:	Take action ah,〜 μh(∙∣sh), observe the action ah,,-i from the other players.
6:	Observe reward rh,i = rh,i(sh, ah, ah,-i) and next state sh+1 from the environment.
7:	Update accumulators: t := Nh(Sh) J Nh(Sh) + 1.
8:	Vh,i	(Sh)	- (I -	at) Vh,i	(Sh) + αt	(rh,i	(Sh,ah, ah,-i) + Vh+1,i (Sh+1)	+	凡)
9:	Vh,i	(Sh)	J (1 -	αt) Vh,i	(Sh) + αt	(rh,i	(Sh, ah, ah,-i) + Vh+1,i (Sh+1)-	βt)
10:	for all a ∈ Ai do	_
11:	'h,i(sh,a) — H1 [H-h + 1 -rh,i-min{Vh+1,i(sh+1), H-h}]1 {ah = a} /[μh(ah际)+ ηt].
12:	Lh,i(Sh , a) <- (1 - at) L h,i(s h , a) + αtLh,i (sh, a)
13:	Set μh(∙∣Sh) H exp[-(ηt∕αt)Lh,i(Sh, •)].
Proposition A.2 (Corollary 4.5, (Rubinstein, 2016)). There exists absolute constants ε0 > 0 and
c > 0, such that for all m,
QCP(ANE(m, εο)) = 2ω-, where P = 2-cm
This result shows that it is impossible for any algorithm to learn an ε0-ANE for every binary action
game with probability at least (1 - p) using poly(m, log(1/p)) samples: such an algorithm with
p = 2-cm would only use poly(m, log(2cm)) = poly(m) samples, yet the sample complexity lower
bound in Proposition A.2 requires at least 2。(m)= exp(Ω(m)) samples. Since Proposition A.2
allows εo = Θ(1), this also rules out the possibility of learning ε-ANE with poly(m, log(1∕p), 1∕ε)
samples for all small ε.
B Proofs for Section 3
B.1	Algorithm for learning CCE in general-sum Markov games
Our algorithm used to learn CCE in general-sum MGs is a combination of Algorithm 4 and Algorithm
2. In particular, Algorithm 4 computes a set of policies and plays these policies in each episode.
Algorithm 2 used the full history in Algorithm 4 to produce a certified, general correlated policy
which we will show to be a CCE (we will also use the same Algorithm 2 to produce the certified
policy in the algorithm of learning CE). During the execution of Algorithm 2, if the index t is 0 at
some step h, the certified policy can choose any action at and after step h.
In Algorithm 4, we choose the hyper-parameters as follows:
H+1
αt = H+Γ，
ηt
VAiP
瓦
H2ι
+ 2c----
t
where c > 0 is some absolute constant, and ∣ = log( m max-[m] AiHSK) is a log factor. The choice
pε
of ηt follows the V-OL algorithm in Tian et al. (2021) which helps to shave off an H factor in the
sample complexity compared with the original Nash V-Learning algorithm in Bai et al. (2020).
Here, we have a short comment on the log factor ∣. In fact, we need ι to be C log( m maxi∈[m] AiHSK)
pε
for some absolute constant C. For the cleanness of the results, in this paper, we ignore this difference
since this would not harm the correctness of all the results we present.
B.2	Proof of Theorem 2
We begin with an auxiliary lemma on αtj (its definition is in (3)).
14
Published as a conference paper at ICLR 2022
Lemma B.1 (Lemma 4.1 in Jin et al. (2018)). The following properties hold for αtj:
1.	√ ≤ Pj=I √ ≤ √forevery t ≥ 1.
2.	maxj∈[t] αj ≤ 平 and Pj=I (αj) ≤ 平 for every t ≥ 1 .
3.	P∞=j αj = 1 + H for every j ≥ 1.
4.	Pj=I 苧 ≥ 21t for every t ≥ 1.
Property 4 above does not appear in (Jin et al., 2018), for which we provide a quick proof here:
X α	≥ X	at	≥ 1/t X αj≥) 1∕(2t)	X ɑj	= 1∕(2t).
j=1 j	j=[t/2]	j	j=[t/2]	j=1
Here, (i) uses αj is increasing in j for fixed t.	□
Some notations The following notations will be used repeatedly (throughout this section and the
next section). At the beginning of any episode k for a particular state sh, We denote kh(Sh) < …<
khNh(sh)(sh) < k to be all the episodes that the state sh was visited, where Nhk(sh) is the times the
state sh has been visited before the start of k-th episode. When there is no confusion, We sometimes
will write kj = kj(Sh) in short. For player i, we let Vh,i, V；,, μk denote the values and policies
maintained by Algorithm 4 at the beginning of k-th episode, and akh denote taken action at step h and
episode k. For any joint policy μj (over all players), reward function r and value function V, we
define the operators Ph and D*. as
[PhV ](s, a)：= Es,〜Ph(∙∣s,a)V (s0),	(5)
Dμh [r + Ph V ](s) := Eah 〜μh[r(s, ah) + Es，+I〜P, (∙∣ s,a.)V (sh+1)].	⑹
Towards proving Theorem 2, we begin with a simple consequence of the update rule in Algorithm 4,
which will be used several times later.
Lemma B.2 (Update rule). Fix a state S in time step h and fix an episode k, let t = Nhk (S) and
suppose S was previously visited at episodes k1 < •一 < kt < k at the h-th step. The update rules in
Algorithm 4 gives the following equations:
tj
V	h,i(s)=α0H + X aj rh,i 卜,ahj,碍―)+ V h+ι,i 卜 h+ι) + 也，
j=1
t
V	h,i(s) = X αj [rh,i Gaj ah-i) + Vh+ι,i 卜 h+ι) - j .
j=1
We next present and prove the following lemma which helps to explain why our choice of the bonus
term is βt. The constant C in βt is actually the same with the constant C in this lemma.
Lemma B.3 (Per-state guarantee). Fix a state S in time step h and fix an episode k, let t = Nhk (S)
and suppose S was previously visited at episodes k1 < •一 < kt < k at the h-th step. With probability
at least 1 一 p2 ,for any (i, s, h, t) ∈ [m] ×S × [H ] X [K ], there exist a constant C s.t.
t
max
μ∈∆Ai
αtjD	kj
t μ×μkh,-
rh,i + Ph min{Vh+1,i, H 一 h} (S)
t
- X αtj
j=1
rh,i (s,αhj, ah：-，+ min{Vh+ι,i(sh+ι),H 一 h}	≤ CP3Ai〃t+CH2ι∣t.
15
Published as a conference paper at ICLR 2022
Proof of Lemma B.3 First, we decompose
maxX ajDμ×μkj] (rh,i + Ph min{Vh+1,i, H - M) (S)
μ j = 1	h,-i ∖	)
j
-	X aj rh,i ssa ah , ah,-i) + min{Vh+ι,i(Sh+1), H -M
j=1
into R?(i, S, h, t) + U (i, S, h, t) where
R?(i,s,h,t) :=maxX。渺““铲_ (rh,i + Phmin{Vh+ι,i,H - h}) (S)
j=ι	,-、	/
-	X ajD kj ×*	(rh,i + Ph min{Vh+ι,i, H -M) (s),
μμ	μh,i×μh,-i	,
j=1
and
U (i,s,h,t) := X αjDμj×μkL (rh,i + Ph min{V h+l,i,H - h}) (s)
j=1	,	,
tj
-	X aj rh,i (s,ah, ah,-i) + min{Vh+ι,i(Sk∖ι),H - h} .
j=1
We first bound U(i, S, h, t). Define Fl as the σ-algebra generated by all the random variables up
to the time when Sh is observed at the l-th episode. Recall that for j ≥ 1, kj = khj (S) = inf {l >
kj-1 : S is visited at step h in episode l} (with convention k0 = 0). Then {kj }j≥1 is a sequence of
increasing stopping times w.r.t. {Fl}l≥1. Define Gj = Fkj+1 . So {Gj}j≥0 is also a filtration. Under
Gj-1 (= Fkj), by the definition of operator D and P, we have
E rh,i(s,ahj,ahj-i)+ min{vh+ι,i(sh+ι),H-M Gj-I
=Dμj×μkL 卜h,i+ Phmin{Vh+ι,i,H -M) (s).
So we can apply Azuma-Hoeffding inequality. Note that Ptj=1 (αtj)2 ≤ 2H/t by Lemma B.1. Using
AzUma-Hoeffding inequality, We have with probability at least 1 - 4mΗsκ
U (i,s,h,t) = X αjDμj×μkL (rh,i + Ph min{V h+l,i ,H - M) (S)
j=1	,	,
tj
-	Xaj rh,i (S,ah, ah,-i)+ min{Vh+ι,i(Slh+ι),H -M
j=1
t
≤ t 2H2 log(4mHSK∕p) X(αj)2 ≤ 2，H3ι∕t.
After taking a union bound, we have the following statement is true with probability at least 1 - p/4,
U(i,S,h,t) ≤ 2PH3ι∕t forall (i,S,h,t) ∈ [m] ×S× [H] X [K].
Then we bound R?(i, S, h, t). For fixed (i, S, h), ifwe define the loss function
'j (a) = H Eai = a,a-i 〜μjJ H - h +1 - rh,i(S, a) - Ph min{V h+l,i,H - h}(S)] ∈ [0, 1],
16
Published as a conference paper at ICLR 2022
Algorithm 5 Correlated policy πbhk for general-sum Markov games
1:	for step h0 = h, . . . , H do
2:	Observe sh，, and set t — Nhk (sh,o).
3:	Sample l ∈ [t] with P(l = j) = αtj .
4:	Update k — kl (sho).
5：	Jointly take action (ah，，i,ah，2,∙∙∙, ahh,m)〜Qm=I μ%,il∖sh∕).
then R?(i, s, h,t) = H max* Ptj=I αj <μ - μhji, G
becomes the weighted regret with weight
αj. Note that the update rule for μhji(∙∣s) is essentially performing FolloW-the-Regularized-Leader
(FTRL) algorithm with changing step size for each state s and each step h to solve an adversarial
bandit problem. Lemma 17 in (Bai et al., 2020)3 bounds the Weight regret With high probability.
Using that lemma, we have with probability at least 1 - 4mHs
R?(i, s, h, t) ≤
Hatlog Ai + 3HAi X ηjαj + Hʌ 2| Xmj )2 + H max αtι + H max αj Unt
ηt	2 j =1	j =1	2 j≤t	j≤t
simultaneously for all t ∈ [K] . By Lemma B.1 and n = ʌ H, we have with probability at least
Ait
1_____P-
ɪ	4mΗS
R?(i, s, h,t) ≤ 2PH3Ai∣∕t + 3PH3Ai∣∕t + 2PH3ι∕t + H2ι∕t + 2，H34∣∕t
≤ 10PH3Ai∣∕t + 10H2ι∕t for all t ∈ [K].
Again, taking a union bound in all (i, s, h) ∈ [m] × S × [H], we have with probability at least
1 - p∕4,
R?(i, s, h, t) ≤ 10pH3Ai∣∕t + 10H2ι∕t for all (i, s, h, t) ∈ [m] ×S × [H] X [K].
Finally, we concluded that with probability at least 1 - p∕2, we have
U(i, s, h, t) + R?(i, s, h, t) ≤ CPH3A-ι∕t + CH2ι∕t for all (i, s, h, t) ∈ [m] ×S × [H] × [K]
for some absolute constant c.
□
Recall that the certified policy πb as in Algorithm 2 is a nested mixture of policies. We further define
policies {πbhk}h∈[H],k∈[K] in Algorithm 5. By construction, the relationship between πb and πbhk is
that when players jointly play policy the πb, they first sample k from Uniform([K]), then they play
together the policy πb1k (Algorithm 5 for h = 1) with the same sampled k. As a result, we have the
following relationship:
1K
Vfi(SI) = K X Vb (si).	⑺
k=1
Definition B.4 (Policy starting from the h-th step). We define the policy starting from the h-th step
for player i as π≥h,,i := {πh∕,i : Ω × (S × A)h0-h ×S → ∆∕JH=h. At each SteP h0 ≥ h, π≥h,,i
samples action based on current state, the history starting from the h-th step and a random number
ω ∈ Ω. We use Π≥h,i to denote allpoliciesforplayer i Startingfrom the h-th step. Similar to Section
2, we can define general correlated policy starting from the h-th step (where the random numbers
may be correlated for different players), and we use Π≥h to denote all such general correlated policy
starting from the h-th step.
3A very similar result is Lemma F.3 in our paper. However, here we need Lemma 17 in (Bai et al., 2020) to
get the optimal H dependency.
17
Published as a conference paper at ICLR 2022
For π ∈ Π≥h, we can define the value function starting from the h-th step as:
H
Vhπ,i(s) := Eπ X rh0,i|sh = s .	(8)
h0=h
We also define the value function of the best response as:
⅛πh,-i(s)：= max V(s).
μi∈∏≥h,i
One example of a policy starting from the h-th step is πbhk defined in Algorithm 5, so that we can
define Vnh (s) and VilXhLi (S).
Lemma B.5 (Valid upper and lower bounds). We have
Vh,i(s) ≥ VhFL (s), Vhh,i(s) ≤ Vn (S)
for all (i, s, h, k) ∈ [m] × S × [H] × [K] with probability at least 1 - p.
Proof of Lemma B.5 We prove this lemma by backward induction over h ∈ [H + 1]. The base
case of h = H + 1 is true as all the value functions equal 0 by definition. Suppose the claim is true
for h + 1. We begin with upper bounding Vhi,,iπh,-i (s). Let t = Nhk (s) and kj = khj (s) for 1 ≤ j ≤ t
to be the j’th time that s is previously visited. By the definition of certified policies πbhk,i and by the
value iteration formula of MGs, We have for any policy μi ∈ Π≥h,i,
啜 i,πk,-i (s)= XX αjEaihi×μhj Jrh,i(s, α) + Es0 〜Ph(,|s,aM+，H，i1S，a)，nk+l,-i (S0)
j=1	,
t
≤ X	αtjE	kj
t	a〜μh,i×μhh —
j=1	,
i πbkj
rh,i(s, α) + EsO〜Ph(∙∣s,a)Vh+1,i，	(s ) .
Here, μh,i is the policy μi atthe h-th step, and (μh+LH,i∣s, a) is the policy μi from time h + 1 to H
With history information at h-th step to be sh = s and ah =a. By the definition of Π≥h,i, We have
(μ%+LH,i∣s, a) ∈ Π≥h+ι,i which implies the inequality in the equation above. By taking supremum
over μi ∈ Π≥h,i and using the definition of the operator D in (6), We have
k	t	kj
⅛i h,-i(s) ≤ sup XαjD	kj [rh,i + PhV^-i](s).
∙∈δAi j=1	L-
Conditional on the high probability event in Lemma B.3, we use the inductive hypothesis to obtain
Vhi,,iπbhk,-i(s) ≤supXt αtjD
μi j = 1
t
≤ X	αtj
j=1
(i)	t j
≤ αt
j=1
t
≤ X	αtj
j=1
=V h,i(s)
kj
Z× 常」rh,i + Ph min{V …H-h}](s)
rh,i (s,ahj, ahj-J +min{V h+ι,i(sh+ι),H - h} + CpH 3Aiι∕t+CH 2/it
rh,i (s,ahj, ahj-J + mW。h+ι,i(sh+ι),H - h}+国
rh,i (s, ah, ah,-J + V h+ι,i (Sh+J + β
Here, (i) uses our choice of βj = CqHA + 2c j and √⅛ ≤ Pj=I √, 1 ≤ Pj=I 等.
18
Published as a conference paper at ICLR 2022
Meanwhile, for Vh,i(s), by the definition of certified policy and inductive hypothesis,
kt	j
Vnh(S)= X αj Dμkj [rh,i + PhVn+l,i](S)
j=1
t
≥ X aj Dμkj [rh,i + Ph max{Vh+l,i, 0H(S).
j=1
j	kj	kj	kj	kj
ThenWenOtethat∣Dμkj[rh,i + Ph max{V h+1,i, 0}](s)- [rh,i (s,ahh , ahlh-i) + max{V h+l,i(sh+l), 0}
is a martingale difference w.r.t. the filtration {Gj}j≥0, which is defined in the proof of Lemma B.3.
So by Azuma-Hoeffding inequality, with probability at least 1 - 2mSκH
j≥1
t
≥ X αtj
j=1
t
X aj Dμkj [rh,i + Ph max{V h+l,i, 0}](s)
j=1
jj	j	j
rh,i s,ahh, ah, + maχ{Vh+ι,i(sh+ι),0}
H3ι
-2V -T
(9)
On this event, we have
t
X aj Dμkj [rh,i + Ph max {V h+l,i, 0}](S)
j=1
≥) XCj r, . (q ɑkh nkh ʌ + rn口x /Vkh (ςkh ) nl _ β
≥ 乙 αt rh,i ls, ah , ah,-i ) + max] Vh+ι,i(sh+ι), 0 j - Pj
=V h,i(s).
Here,⑴ uses Pj=I αjβj ≥ 2 Pj=I αj √-3ι∕j ≥ 2√-3ι∕t.
As a result, the backward induction would work well for all h as long as the inequalities in
Lemma B.3 and (9) hold for all (i, S, h, k) ∈ [m] × S × [-] × [K]. Taking a union bound in all
(i, S, h, k) ∈ [m] × S × [-] × [K], we have with probability at least 1 - p∕2, the inequality in (9) is
true simultaneously for all (i, S, h, k) ∈ [m] × S × [-] × [K]. Therefore the inequalities in Lemma
B.3 and (9) hold simultaneously for all (i, S, h, k) ∈ [m] × S × [-] × [K] with probability at least
1 - p. This finishes the proof of this lemma.	□
Equipped with these lemmas, we are ready to prove Theorem 2.
Proof of Theorem 2 Conditional on the high probability event in Lemma B.5 (this happens with
probability at least 1 - p), we have
Vh,i(s) ≥ VhFL(s), Vh,i(s) ≤ Vbh(s)
for all (i, S, h, k) ∈ [m] × S × [-] × [K]. Then, choosing h = 1 and S = S1, we have
VrkLi(si) - Vnk(SI) ≤ V 1,i(sι) - V 1,i(sι).
Moreover, by (7), value function of certified policy can be decomposed as
K	Kk
Vbi(S) = KK X Vb (s), V"1" (si) = K X Vii1,b1,-i (si)
k=1	k=1
where the decomposition is due to the first line in the Algorithm 2: sample k —Uniform([K]).
19
Published as a conference paper at ICLR 2022
So we have
Vιt,ib-i (si) - Vfi(SI) ≤KK XL (VrkLi (si) - Vb (sι))
K k=1
K
≤ K X (Vi,i(SI)-V k,i(SI)).
k=i
To prove b is an approximate CCE, We only need to bound PK=I (vk,i(sι) - Vk,i(si)). Letting
δh,i := Vh,i(Sh)- Vh,i(Sh) and t = Nh(Sh). Suppose ShWas previously visited at episodes
ki , . . . , kt at the h-th step. By the update rule,
δk,i=V Ii(Sh) - V h,i(Sh)
t
=OtH + X αj[Vh+ι,i(Sh+i) - Vh+ι,i(Sh+i) + 2βj]
j=i
tt
=α0H + X αjδh+ i,i + X 2αj也
j=i	j=i
=α0H + Xαjδk+i,i + 2cXαj∖∕AiH ι + 4cXαj j.
j=i	j=i	j	j=i	j
We can use Lemma B.1 which gives Pj=I αj/√j ≤ 2/t and maxj≤t αj ≤ 2H/t to get
t
δh,i ≤ O0H + X αjδk+i,i + 4p"t + 8cH31(1 + logt)/t,
j=i
where we also uses Ptj=i 1/j ≤ 1 + log t.
Taking the summation w.r.t. k, we begin by the first two terms;
KK	K
X α0H = X H1 {t = 0} = X H1 {Nh(Sh) = 0} ≤ SH.
k=i	k=i	k=i
K Nhk(skh)	j k (i) K
X X j	δkhj (skh) (≤i)	X δk0
αNhk(skh)δh+i,i ≤	δh+
k=i j=i
k0=i
∞
i,i X
j=Nhk0 (skh0)+i
αNk0(S (≤) (1 + S) X δh+ι,i,
k0=i
where (i) is by changing the order of summation and (ii) is by Lemma B.1.
So
K
K
X δ3 ≤ SH +(1 + H)
k=i
fδ1h+ι,i + 4c√H3Ai7]Γ
k=i
k=i
+ 8cH 3∣ X 1 + lθg N(S)
⅛	Nk(S)
1
TNhw
By pigeonhole argument,
K
X
k=i
1
TNkw
Nh (S)	1
XX √n≤O(1)√SK.
s∈S n=i
Similarly,
X 1+ Ng(Nk(S) ≤ O(1)(1 + log(Κ)S(1 + log(Κ/S))) ≤ O(1)S(∣ +12) ≤ O(1)S∣2,
20
Published as a conference paper at ICLR 2022
where we assume ι ≥ 1. So we have
K	1	1 ∖ K	I________
∑ δh,i ≤ SH +(1 + H) δk+ι δk+ι,i + O⑴ √H3AiSKi + O(I)SH3∣3∙
Recursing this argument for h ∈ [H] gives
K
X δ1,i ≤ eH2S + O(I)Ph5AiSKi + O(I)SH4ι3.
=
To conclude,
公	ʌ	1 ʌ f—k	1 K ,
Vt,i	i(si) - V∏i(sι) ≤K X (Vι,i(sι) - Vfi(sι)) = K X 6>
k=	k=
≤O(1)SH 4ι3∕K + O
H15 5S max Ai∣∕K
Therefore, K ≥ O(H Smaχi∈m] AiI + SH：|3) guarantees that We have Vtiπ-i(si) - Vni(SI) ≤ ε
for all i ∈ [m]. This complete the proof of Theorem 2.	□
C	Proofs for Section 4
In this section we prove Theorem 5. We first define a set of lower value estimates Vh,i(s) (along
With the upper estimates used in Algorithm 1) via the folloWing update rule:
Vh,i Ish) J (1 - αt) Vh,i (Sh) + αt (rh,i (Sh ah, ah,-i) + Vh+1,i (sh+1) - Bt) ∙ (IO)
We emphasize that Vh,i(s) are analyses quantities only for simplifying the proof, and are not used by
the algorithm.
We restate and use several notations we introduced in the last section. At the beginning of any episode
k for a particular state Sh, we denote kh(Sh) < … < kNh (Sh)(Sh) < k to be all the episodes that the
state Sh was visited, where Nhk (Sh) is the times the state Sh has been visited before the start of k-th
episode. When there is no confusion, we sometimes will write kj = khj (Sh) in short. For player i,
we let Vh i, Vh,i, μk denote the values and policies maintained by Algorithm 4 at the beginning of
k-th episode, and ah denote taken action at step h and episode k. For any joint policy μh (over all
players), reward function r and value function V, we define the operators Ph and D*. as
[PhV](s, a):= Es,〜Ph(∙∣s,a)V(s0),
Dμh [r + PhV](s) := Eah〜.h[r(S, ah) + Esh+ι〜Ph(∙∣s,ah)V(Sh+1)].
The following lemma is the same as Lemma B.2 in the CCE case (except that for a different algorithm).
Lemma C.1 (Update rule). Fix a state S in time step h and fix an episode k, let t = Nhk (S) and
suppose S was previously visited at episodes k1 < •一 < kt < k at the h-th step. The update rulefor
V h,i (s) and V h,i(s) in Algorithm 1 and (10) gives the following equations:
tj
Vh,i(S) = α0H+Xaj rh,i 卜,ah, ah,-) + Vh+ι,i+βj ,
j=i
t
V h,i(S) = X αj [rh,i 9,ah,,喊八+ V h+i,i 卜 h+i) - βj ∙
j=i
21
Published as a conference paper at ICLR 2022
We next prove the following lemma which helps explain our choice of the bonus term e片 The constant
C in βt is the same with the constant C in this lemma. For any policy modification 夕i : Ai → Ai
for the ith player and one-step policy ∏h : S → △/ for any h, the modified policy 夕i ◊ ∏h is
defined as follows: if ∏h chooses to play a = (aι,...,am), the modified policy Si ◊ ∏h will play
(aι,...,ai-ι,夕 i(ai),ai+ι,...,am). Moreover, for ∏h,i : S → ∆∕i, policy Si ◊ ∏h,i chooses 夕 i(a)
when πh,i chooses a.
Lemma C.2 (Per-state guarantee). Fix a state s in time step h and fix an episode k, let t = Nhk (s)
and suppose S was previously visited at episodes k1 < •一 < kt < k at the h-th step. With probability
1 一 P ,for any (i, s, h, t) ∈ [m] × S × [H ] X [K ], there exist a constant C s.t.
%AU→4 XX αjDihj kh,i+Ph min{V h+1,i, H - h})(s)
t
一 X αtj
j=1
rh,i (s,ahj, ahj-i) +min{Vh+ι,i(sh+ι
),H — h} ≤ cH2Aip2ι∕t + cHIIAiI/t.
Proof of Lemma C.2 First, like the prove in Lemma B.3, we decompose
SUp	X ajDp0μkj ( rh,i + Ph min{Vh+1,i, H - h} ) (S)
"Ai→Ai j=1	h h
tj
一 X aj rh,i 卜,ah , ah,-i) + min{Vh+ι,i(sh+ι), H - h}
j=1
into R?(i, S, h, t) + U(i, S, h, t) where
R?(i,s,h,t) :=	SUp	X αj 叱 i。* (rh,i + Ph min{V h+l,i,H — h}) (S)
%Ai→Ai j=ι	2 h
t
一 X αjE	kj
乙 t a-i〜μh,-i
j=1
and
rh,i 卜，ah: a-i) + Esh+ι〜Ph(∙∣s,ahj,a-i) min{Vh+l,i(sh+l), H — h} ,
t
U(i,s,h,t) := X αjE,
j1
t
一 X αtj
j=1
kj
a-i 〜μh,-i
rh,i (S,点,a-i) + Esh + 1 〜Ph(∙∣s,ahj,a-i)min{vh+1,i(sh+1), H - h}
rh,i (s,ahj, ah=，+ min{Vh+ι,i(sh+αH - h} .
We first bound U(i, S, h, t). By the same reason in proof of Lemma B.3, we can apply Azuma-
Hoeffding inequality. Note that Ptj=1(αtj)2 ≤ 2H∕t by Lemma B.1. Using Azuma-Hoeffding
inequality, We have with probability at least 1 — 4mHsκ,
U(i, S, h, t) ≤ t2H2 log(4mHSK∕p)
t
X(αj)2≤ 2PH3l∕t.
j=1
After taking a union bound, the following statement is true with probability at least 1 — p∕4,
U(i, s, h, t) ≤ 2pH3ι∕t for all (i, s, h, t) ∈ [m] × S × [H] × [K].
Then we bound R?(i, S, h, t). For fixed (i, S, h), we define loss function
〃 / 、	1 口	「一 , r	/	、 E	. Ekj	/	、/r
'j (a) = H Eai = a,a-i 〜*kj_ JH 一 h + 1 一 rh,i(s, a) 一 Es.+ i 〜P. (∙∣ s,a,a-i) min{V h+1,i(sh+l), H 一 h}]∙
22
Published as a conference paper at ICLR 2022
Algorithm 6 Correlated policy πbhk for general-sum Markov games
1:	for step h0 = h, . . . , H do
2:	Observe sh，, and set t — Nhk (sh,o).
3:	Sample l ∈ [t] with P(l = j) = αtj .
4:	Update k — kl (sho).
5：	Jointly take action (ah，，i,ah，2,∙∙∙, ahh,m)〜Qm=I μ%,il∖sh∕).
Then we have `j(a) ∈ [0, 1] for all a and the realized loss function:
Oj (Oh)= H [H - h +1 - rh,i(s, ah , a-i) - min{Vh+1,i(sh+1), H - h}] ∈ [0, 1]
is an unbiased estimator of `j (akhj ). Then R?(i, s, h, t) can be written as
t
R?(i,s,h,t) = H	SUp	X aj ['j (aKi)-Si ◊ μhji, 'j i].
"Ai→Ai j=1
Now, for any fixed step h and state s, the distributions {qh(∙∣s)}fe and visitation counts (Nh(S)}fe
are only updated at episodes kh* 1 (s), . . . , kht (s). Further, these updates are exactly equivalent to the
mixed-expert FTRL update algorithm which we describe in Algorithm 8. Therefore, the R? (i, s, h, t)
above can be bounded by the weighted swap regret bound of Lemma F.1 (choosing the log term as
ι = 4 log 10mSpAHK) to yield that
t
H Sup	X aj['j(ahji) -〈0 ◊ μhi,'ji] ≤ 40H2Ai√∣∕t + 40H2Ai∣∕t forall t ∈ [K]
小：Ai→Ai j=	'	'
with probability at least 1 - p∕(4mSH). Taking a union bound over all (i, s, h) ∈ [m] × S × [H],
we have with probability at least 1 - p∕4,
R?(i, s, h, t) ≤ 40H2Ai√∕t + 40H2A.ι∕t for all (i, s, h, t) ∈ [m] × S × [H] X [K].
Finally, we conclude that with probability at least 1 - p∕2, we have
U(i, s, h, t) + R?(i, s, h, t) ≤ CH2Ai √√ι∕t + CH2Ai∣∕t for all (i, s, h, t) ∈ [m] × S × [H] × [K]
for some absolute constant c.	□
We define the auxiliary certified policies πbhk in Algorithm 6 (same as Algorithm 5 for the CCE case
but repeated here for clarity). Again, we have the following relationship:
1K
Vfi(S) = K X Vb (s).	(11)
k=1
Definition C.3 (Policy modification starting from the h-th step). A strategy modification starting
from the h-th SteP φ≥h := {Φh0,s}(h0,s)∈{h,h+ι,...,Η}×s for player i is a set of S × (H - h + 1)
functions φh√,s	:	(S	×	A)h -h	×	Ai	→	Ai.	This strategy modification φ≥h	Can be composed
with any policy π ∈ Π≥h (as in Definition B.4) to give a modified policy φ≥h ◊ π defined as
follows: At any step h0 ≥ h and state s with the history information starting from the h-th step
Th：h，— i = (sh, ah, … ,sh∕-ι, aho-ι), if π chooses to play a = (aι,..., am,), the modified policy
φ ◊ π will play (aι,..., αi-ι,φht ,s(τh↑ht-ι,αi), ai+1,..., amɔ. We USe Φ≥h,i denote the set of all
such possible strategy modifications for player i.
For any φ ∈ Φ≥h,i, φ ◊ πbhk also doesn’t depend on the history before the h-th step, so φ ◊ πbhk ∈ Π≥h,
which implies that Vhφ,iπh (s) is well-defined in (8).
23
Published as a conference paper at ICLR 2022
Lemma C.4 (Valid upper and lower bounds). We have
Vli(s) ≥φfsup VJnk(S), Vhh,i(s) ≤ VJh(S)
for all (i, k, h, s) ∈ [m] × [K] × [H] × S with probability at least 1 - p/2.
Proof of Lemma C.4 We prove this lemma by backward induction over h ∈ [H + 1]. The base
case of h = H + 1 is true as all the value functions equal 0. Suppose the claim is true for h + 1. We
begin with upper bounding supφ∈Φ Vhφ,ibπh (s). Let t = Nhk(s) and kj = khj (s) for 1 ≤ j ≤ t to
be the j’th time that s is previously visited. By the definition of certified policies πbhk,i and by the
value iteration formula of MGs, we have for any φ ∈ Φ≥h,i,
t
Vhfbo = X αjEa〜φh.加
j=1
t
Jr" ɑ)+Es，〜Ph(.|s,aM+l『1s，a)°bk+l
(s0)
≤ EajEa〜φh*
j=1
φπbhkj+1	0
rh,i(S, a)+ Es0 〜Ph(∙∣s,a)	SUp	V41i+ (s ).
φ∈Φ≥h+1,i
h
Here, φh,i is the strategy modification function φ at the h-th step, and (φ≥h+ι |s, a) is the modification
φ from the h + 1-th step with history information to be Sh = S and ah =a. By the definition of
Φ≥h,i, We have (φ≥h+ι∣s, a) ∈ Φ≥h+ι,i which implies the above inequality. By taking supremum
over φ ∈ Φ≥h,i and using the definition of the operator D in (6), we have
k	t	kj
SUp Vhr (S) ≤ SUp X αj%°μhj[rh,i + Ph SUp Vφ+π,h + 1] (S).
φ∈φ≥h,i	⅛5i:Ai→Ai j=ι	φ∈φ≥h+ι,i
Condition on the high probability event (with probability at least 1 - p/2) in Lemma C.2, we can use
the inductive hypothesis to obtain
t
sup	αtj D
t t ψi^μ-
%Ai→Ai j=ι	'
t
khj[rh,i +Phφ∈ΦS≥Uhp+1,iVhφ+1πb,hkij+1](S)
≤	SUp	XαjDW Wkj [rh,i+Ph min{Vhh.
心Ai→Ai j=1	…μh
j
+1,i, H - h}](s)
t
≤ X αtj
j=1
(i)t	j
≤ αt
j=1
t
≤ X αtj
j=1
=V M(S)
rh,i (S,ahj, ahj-J + min{V h+ι,i(Sh+ι),H - h} + CH 2 Aip/t+CH H2 AiUt
rh,i (s,αf,碟-j + mW。hh+ι,i(sh+aH -M+也
rh,i (s,a3 破二)+ Vh+ι,i + βj
Here, (i) uses our choice of βj = CH3q + 2cH2Aiι and √ ≤ Pj=I √√j,1 ≤ Pj=I 苧,so
that CH2Aipι∕t + CH2Aii∕t ≤ P αjβj.
Meanwhile, for V/(s), by the definition of certified policy and inductive hypothesis,
Vhbh (s) = X ajD“kj [『h,i + Ph⅛](s)
j=1
t
≥ X aj Dμ"h,i + Ph max{V h+1,i, 0H(S).
j=1 h
24
Published as a conference paper at ICLR 2022
kj	khj kjh	khj	khj
Note that < Dμkj [rh,i + Ph max (Vh+1,i, 0j](s) - rh,i I s,ah , ah,-i ) + max{Vh+1,i(sh+1), 0}
is a martingale difference sequence w.r.t. the filtration {Gj}j≥0, which is defined in the proof of
Lemma B.3. So by AzUma-HOeffding inequality, with probability at least 1 - 2%Skh,
j≥i
t
X aj Dμkj [rh,i + Ph max{Vh+l,i, 0H(S)
j=1
t
≥ X αtj
j=1
jj	j	j
rh,i	s,ahh, ahh-i + max{Vh+ι,i(Shb), 0}
H3ι
-2V -T
(12)
On this event, we have
t
E aj Dμkj [rh,i + Ph max
j=1	h
(i)	t j
≥ αt
j=1
t
≥ X αtj
j=1
=V h,i(s)
kj	kj
rh,i	S, ahh, ahh,-i +max
ι,i(sh+ι),0	- βj
/	kj	kj	kj	kj	—
rh,i	s,ahh, ahh-i	+ Vh+ι,i(sh+ι) - βj
Here, (i) uses Pj=I αjβj ≥ 2 Pj=I αj √-3ι∕j ≥ 2√-3ι∕t.
As a result, the backward induction would work well for all h as long as the inequalities in
Lemma C.2 and (12) hold for all (i, S, h, k) ∈ [m] × S × [-] × [K]. Taking a union bound in all
(i, S, h, k) ∈ [m] × S × [-] × [K], we have with probability at least 1 - p∕2, the inequality in (12) is
true simultaneously for all (i, S, h, k) ∈ [m] × S × [-] × [K]. Therefore the inequalities in Lemma
B.3 and (12) hold simultaneously for all (i, S, h, k) ∈ [m] × S × [-] × [K] with probability at least
1 - p. This finishes the proof of this lemma.	□
Equipped with these lemmas, we are ready to prove Theorem 5.
Proof of Theorem 5 Conditional on the high probability event in Lemma C.4 (this happens with
probability at least 1 - p), we have
vh,i(s) ≥ ©蒙	Vhfh(),
Vh,i(s) ≤ vb (S)
for all (i, S, h, k) ∈ [m] × S × [-] × [K]. Then, choosing h = 1 and S = S1, we have
(SupVφfk (Si) - Vnk(SI) ≤ V k,i (Si) - V 1,i(sι).
Moreover, by (11), value function of certified policy can be decomposed as
KK
Vfi(SI) = KK X Vnik (si)，ViT(Si) = KK X VTk (si)，
k=i	k=i
where the decomposition is due to the first line in the Algorithm 2: sample k —Uniform([K]).
Therefore we have the following bound on supφ∈Φ Viφ,inb(Si) - Vinb,i(Si):
1K
ΦuPV7(Si)-Vni(Si)=hum N X VrI (Si)-Vni(Si)
25
Published as a conference paper at ICLR 2022
KK
≤ KK X sup V"b1 (si) - Vbi1 (si)	≤ K X(Vk,i(sι) - Vk,i(sι)).
k=1 φ∈Φi	k=1
ByLemmaC.4 Letting δ3 := V[(Sh) - Vh,i(sh) and t = Nh(SJh). By the update rule, we have
δhi=V h,i(sh)- V h,i(sh)
t
=OtH + X αj[Vh+ι,i(sh+ι) - Vh+1,i(shj+1) + 阚]
j=i
tt
=α0H + X αjδk+ι,i + X 2αjβj
j=i	j=i
=α0H + X W+" + 2cAiH2 X αj r/ɪ +4c X αj HA.
j=i	j=i j	j=i j
Taking the summation w.r.t. k, by the same argument in the proof of Theorem 2, we can get
1 K	1 K
max V1丁(si) - Vbi(SI) = IK X (Vk,i(si) - Vk,i(si)) = IK X δk,i
k=i	k=i
≤ O(I)SH4 max Ail3/K + O ^H6S max Ai2ι∕K).
Therefore, if K ≥ O( HS弋之" A21 + HSm*m] AiJ), We have maxφ∈φ Vf；b(s1)-
Viπb,i(si) ≤ ε holds for all i ∈ [m], which means πb is an ε-approximate CE. This completes the proof
ofTheorem2.
D	Proofs for Section 5
Here, We first define pure-strategy Nash equilibrium. We say policy π is a pure strategy (deterministic
policy) if and only if for any (h, i, s) ∈ [H] X [m] X S, ∏h,i(a∕s) = Iai=aj .⑶ for some a!h i(s'). We
say π a pure-strategy Nash equilibrium if π is a pure-strategy and is a Nash equilibrium. Similarly, We
say π is a pure strategy ε-approximate Nash equilibrium if π is a pure strategy and NE-gap(π) ≤ ε.
Pure-strategy Nash equilibrium does not alWays exist in general-sum MGs, but is guaranteed to exist
(as We Will see) in Markov Potential Games.
D.1 Existence of pure-strategy Nash equilibria in Markov potential games
A particular property of MPGs is that, there alWays exists a pure-strategy Nash equilibrium. Such
a property does not hold for every general-sum MG. Pure-strategy Nash equilibria are preferred in
many scenarios since each player can take deterministic actions.
Proposition D.1. For any Markov potential games, there exists a pure-strategy Nash equilibrium.
See Theorem 3.1 in Leonardos et al. (2021) or Proposition 1 in Zhang et al. (2021) for a proof of
Proposition D.1.
D.2 THE UCBVI-UPLOW SUB-ROUTINE
In this subsection We consider the problem of learning a near optimal policy in the fixed horizon
stochastic reWard RL problem MDP(H, S, A, P, r). The setting is standard (c.f. Jin et al. (2018))
and is a special case of Markov games (c.f. Section 2) by setting the number of agents m = 1. We
Will use the same notations including policies and value functions as that of the Markov games as
26
Published as a conference paper at ICLR 2022
Algorithm 7 UCB-VI with Upper and Lower Confidence Bounds (UCBVI-UPLOW)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
Initialize: For any (s,a,h,s0): Qh(s,a) - H, Q∕s,a) - 0, Nh(S) = Nh(s, a)
Nh(s, a, s0) - 0.
for episode k = 1, . . . , K do
for step h = H, . . . , 1 do
for (s, a) ∈ S × A do
Set t — Nh(s, a).
if t > 0 then
„	，	r ，- -	--	、，-r，	、、	.	...
β J BONUS(t, Vh[(Vh+1 + Vh+1)/2](S, a)) (C.f. Eq. (13))
Y J IC/H) ∙ Ph(Vh+1 - Vh+1 )(s,a).
Qh(S, a) J min {(『h + bhVh+ι)(s, a) + Y + β,H}.
Qh(s,a) J max {(『h + bbhV h+J(s, a) - Y — β, θ}
for s ∈ S do
∏h(s) J argmaXa∈A Qh(s,a).
V h (S) J Qh(S, πh (S)); V h(S) J Qh(S, πh (S)).
Receive the initial state S1 from the MDP.
for step h = 1, . . . , H do
Take action ah = πh(Sh), observe reward 『h and next state Sh+1.
IncrementNh(Sh), Nh(Sh, ah), and Nh(Sh, ah, Sh+1) by 1.
Ph(∙∣Sh, ah) J Nh(Sh, ah, ∙)∕Nh($h, ah).
Let (Vh, Vh, ∏k) denote the value estimates and policy at the beginning of episode k.
return Policy πk? where k? = arg mink∈[K] (V 1(si) - Vk(Sι)).
introduced in Section 2, except that we will omit the sub-scripts i’s since here we just have a single
agent.
We consider the UCBVI-UPLOW algorithm (Algorithm 7), which is adapted from (Xie et al., 2021;
Liu et al., 2021), for learning approximate optimal policy in reinforcement learning problems. Such an
algorithm is used as a sub-routine in Algorithm 3 to learn approximate pure-strategy Nash equilibria
in MPGs. We remark that although in Algorithm 3 we propose to use the UCBVI-UPLOW algorithm
to search for a near optimal policy, many alternative algorithms can be used to find the near optimal
policy (e.g., UCBVI (Azar et al., 2017) or Q-learning (Jin et al., 2018)). Here we choose the UCBVI-
UPLOW algorithm because 1) it has a tight sample complexity bound; 2) it outputs a deterministic
policy which can be used to find a pure-strategy approximate Nash equilibrium.
In the description of Algorithm 7, the Pbh quantity appeared in lines 8,9,10, and 18 can be viewed
either as a set of empirical probability distributions or as an operator: for any fixed (Sh, ah), we can
view Ph(∙∣Sh,ah) as a probability distribution over S; for any given function V : S → R, we can
view Ph as an operator by defining (PhV)(s, a) := E.0〜b.(ʤ a)[V(s0)]∙ The Vh operator In line 7 IS
the empirical variance operator defined as VbhV = PbhV2 - (PbhV)2. The BONUS function in the
algorithm is chosen to be the Bernstein type bonus function
BONUS(t, bb2) = c(∖∕σ2ι∕t + H2Sι∕t).
(13)
We have the following sample complexity guarantee for the UCBVI-UPLOW algorithm returning
an ε-approximate optimal policy.
Lemma D.2. The UCBVI-UPLOW algorithm always returns a deterministic policy πk?. Moreover,
for any p ∈ (0, 1], letting ι = log(S AH K/p) and taking the number of episodes
K ≥ O(H3SAι∕ε2 + H3S2Aι2∕ε),
then with probability at least 1 - p, the returned policy πk? is ε-approximate optimal, i.e.,
k
suPμ VT(SI)- Vn (SI) ≤ ε∙
Proof of Lemma D.2 First, πk? is obviously deterministic from line 12 of Algorithm 7.
27
Published as a conference paper at ICLR 2022
The sample complexity guarantee of the UCBVI-UPLOW algorithm is a consequence of the sample
complexity guarantee of the Nash-VI algorithm for learning Nash in zero-sum Markov games as
proved in Liu et al. (2021).
More specifically, we denote the rewards and transition matrices by {rh(s, a)}h∈[H] and P =
{Ph(sh+1 |sh, ah)}h∈[H] for the MDP(H, S, A, P, r). Such a MDP can be viewed as a zero-sum
Markov game MG(H, S, A, B, P, r), in which (H, S, A) are the same as that of the MDP, the action
space for the min-player is a singleton B = 1 so that the rewards fh(s, a, b) ≡ rh(s, a) do not depend
on the action of the min-player and the transition matrices Ph(∙∣s, a, b) = Ph(∙∣s, aι) do not depend
on the action of the min-player either. Then, for any ε-approximate Nash equilibrium (μ, V) of the
associated zero-sum Markov game, the max-player,s policy μ must be ε-approximate optimal for the
original MDP.
By this correspondence, the UCBVI-UPLOW algorithm is actually a specific version of the Nash-VI
algorithm in Liu et al. (2021), and Line 12 in UCBVI-UPLOW is actually a specific version of
line 12 in Nash-VI in Liu et al. (2021): this is because in this specific Markov game, Q(s, a, b) and
Q(s, a, b) only depend on S and a, so that ∏h(s) = arg max。*/ Qh(s, a) is actually in the CCE set
CCE(Qh(S, a, b),Qh(s, a, b)).
By this reduction and by Theorem 4 in Liu et al. (2021), this lemma is proved.	□
D.3 Proof of Theorem 7
We use superscript t to represent variables at the t-th step (before π is updated) of the while loop.
Because We can choose the log factor as ∣ = 4log(mHSK maxi∈m] Ai) (this doesn,t affect the
εp
correctness of the theorem), for each execution of UCBVI-UPLOW, by Lemma D.2, it return a
ε∕4-optimal deterministic policy with probability at least 1 - 8PFh . Taking a union bound, we have
max Vι,i(μi,π-i) - Vι,i(bt,∏-i) ≤ ε∕4	(14)
μi
simultaneously for all i ∈ [m] and t ≤ 4mH∕ε with probability at least 1 - p∕2. For the empirical
estimator Vb1t,i, it,s bounded in [0, H]. Thus by Hoeffding,s inequality, for fixed i ∈ [m] and t
Nε2
P(IVι,i - Vi,i| ≥ ε/8) ≤ 2eχp (-32H2).
Choosing N = CH2ι∕ε2 for some large constant C, we have
P(V,i- Gl≥ε/8) ≤ ɪ.
Apply this inequality to Vb1t,i(πbit, π-t i) and Vb1,i(πt) and taking a union bound, we have
府i(bt,∏-i)- Vι,i(bt,∏-i)I ≤ ε∕8,	V,i(∏t) - Vι,i(∏t)∣ ≤ ε∕8	(15)
simultaneously for all i ∈ [m] and t ≤ 4mεH with probability at least 1 - p∕2. As a result, by (14)
and (15), we have
max Vι,i(μi,∏-i) - Vι,i(bt,∏-i) ≤ ε∕4
μi
V,i(bt,∏-i)- %(bt,∏-i)I≤ ε∕8	(16)
V,i(∏t)- Vι,i(∏t)∣≤ ε∕8
simultaneously for all i ∈ [m] and t ≤ 4mH∕ε with probability at least 1 - p. On this event,
∆ti = Vb1t,i(πbit, π-t i) -Vb1t,i(πt)
≤V1,i(πbit, π-t i) -V1,i(πt) + ε∕4.
28
Published as a conference paper at ICLR 2022
If the while loop doesn't end after the t-th iteration and t ≤ 4mH∕ε, there exists jt s.t. ∆jt ≥ ε∕2,
so we have
Φ(πt+1) -Φ(πt) = Φ(πbjtt,π-tjt) - Φ(πt)
(=i) V1,jt(πbjtt,π-tjt) -V1,jt(πt)
≥ ∆jt - ε∕4 = ε∕4.
Here, (i) follows the definition of potential function. Because Φ is bounded, the while loop ends
within 4Φmax∕ε ≤ 4mH∕ε steps. Therefore, (16) holds simultaneously for all i ∈ [m] and t before
the end of while loop with probability at least 1 - p. Again, on this event, if the while loop stops at
the end of t-th step, we have maxi∈[m] ∆ti ≤ ε∕2, then
max V1,i(μi,∏-i) - V1,i(∏t)= max V1,i(μi,∏-i) — V1,i(bt,∏-i) + V1,i(bt,∏-i) — V1,i(∏t)
μi	μi
≤ε∕4+Vb1t,i(πbit,π-ti) - Vb1t,i(πt) + 2ε∕8
= ε∕2 + ∆ti
≤ ε.
So the returned policy πt is a ε-approximate Nash equilibrium. Moreover, since UCBVI-UPLOW
outputs a pure-strategy policy and our initial policy is also a pure-strategy policy, we can conclude
that with probability at least 1 - p, within 4Φmax∕ε steps of the while loop, Algorithm 3 outputs an
ε-approximate (pure-strategy) Nash equilibrium.
Finally, the number of episodes within each step of the while loop is
m
N+X(Ki+N) =O
i=1
H3S P乙 Ai∣ H3S2 P乙 Ai∣2 H2m∣ A
ε2	+	ε	+	)
H3S Pm=1 Ai∣	H3S2 Pm=1 AiI2
+	+
ε2	ε
So the total sample complexity (episodes) is at most
ΦmaxH3SPim=1Aiι	ΦmaxH3S2 Pim=1 Aiι2
K = O (------ε3^=— +------------——
This concludes the proof.
□
E Lower Bound of Finding Approximate Pure-Strategy Nash
Equilibrium
In this section, we present an result on the sample complexity lower bound for learning a pure-strategy
Nash equilibrium in MPGs (a harder task than learning Nash as pure-strategy Nash is a stricter notion).
We remark that our lower bounds are actually constructed on Markov Cooperative Games (MCGs)
which is a subset of MPGs. Note that for MCGs, the potential function Φ is bounded in [0, H], so by
m
Theorem 7, the sample complexity of Nash-CA (Algorithm 3) is O( i=1 Ai∕ε3) highlighting the
dependency on ε, m and Ai, i = 1, . . . , m. We would show this Pim=1 Ai dependency is inevitable
for learning ε-approximate pure-strategy Nash equilibrium in MCGS by proving an Ω(Pm=1 Ai∕ε2)
lower bound.
We first present our main theorem:
Theorem E.1 (Lower bound for learning pure-strategy Nash in MCGs). Suppose Ai = 2k, i =
1, . . . , m , H ≥ 2, S ≥ 3 and m ≥ 4. Then, there exists an absolute constant c0 such that for any
ε ≤ 0.4 and any online finetuning algorithm M that outputs a pure-strategy policy πb = (πb1, . . . , πbm ),
if the number of episodes
K ≤ co HP=A = co 乎,
29
Published as a conference paper at ICLR 2022
then there exists general-sum Markov cooperative game MG on which the algorithm M suffers from
ε/4-suboptimality, i.e.
EMNE-gap(b) ≥ ε∕4,
where the expectation EM is w.r.t. the randomness during the algorithm’s execution within Markov
game MG.
This theorem can be viewed as a corollary of the following lemma by a simple reduction. We would
prove this theorem in the next subsection. One-step (general-sum) game is a game with only one state
and one step. In a one-step game, each player chooses an action simultaneously and then receive it’s
own reward. The Nash equilibrium and NE-gap can be defined similarly in one-step games.
Lemma E.2 (Lower bound for one-step game). Suppose Ai = 2k, i = 1, . . . , m and m ≥ 4. Then,
there exists an absolute constant c0 such that for any ε ≤ 0.4 and any online finetuning algorithm
that outputs a pure strategy πb = (πb1, . . . , πbm), if the number of samples
n ≤ c0
Pim=1 Ai
ε2
2km
co-2~,
ε2
then there exists a one-step game M with stochastic reward, on which the algorithm suffers from
ε∕4-suboptimality, i.e.
EM NE-Gap(πb) = EM max max ui (πi, πb-i) - ui (πb) ≥ ε∕4,
i	πi
where the expectation EM is w.r.t. the randomness during the algorithm’s execution within game M.
ui(π) is the expected reward of the ith player when strategy π are taken for each player.
The proof of this lemma is also in the next subsection. In the proof, we first construct a class of
one-step games which reward is Bernoulli( 1) or Bernoulli( 2 + ε) depending on the taken joint-action.
The proportion ofjoint-actions with reward Bernoulli( 2 + ε) is relatively small. Most importantly,
every pure-strategy ε-approximate Nash equilibrium has reward Bernoulli( 11 + ε). So in order to
find an ε-approximate pure-strategy Nash equilibrium, we must explore sufficient joint-actions. The
number of thejoint-actions with reward Bernoulli( 1 + ε) can be bounded by the covering number of
[2k]m under Hamming distance. Then we use KL divergence decomposition (Lemma E.5) to argue
rigorously that we need to explore sufficient joint-actions to get an ε-approximate pure-strategy Nash
equilibrium.
The rest of this section is organized as follows: We first prove Lemma E.2 in Section E.1, and then
prove the main Theorem E.1 in Section E.2.
Discussions of Theorem E.1 There’s Pim=1 Ai dependency4 in the lower bound of sample com-
plexity for finding a pure-strategy ε-approximate Nash equilibrium in MCGs. This bound is novel
and improves the existing result. The existing proof in sample complexity’s lower bound of Markov
games (Bai & Jin (2020)) relies on an reduction from Markov games to single-agent MDPs, so the
existing lower bound’s dependency on Ai (i = 1, . . . , m) is maxi∈[m] Ai .
Here, we don’t include S factor in our lower bound. The difficulty is that the NE-gap only depends on
the player with the most suboptimality. For a single-agent MDP, if the player can change the policy
at each state to improve the expected cumulative reward by ε, then the player can change policy at all
state to improve the expected cumulative reward to the utmost extent. In general-sum Markov games,
at different state, maybe different players can change the policy for this state to improve his expected
cumulative reward by ε. However, the definition NE-gap only allows one player to change the policy.
This difference in nature makes S and Pim=1 Ai incompatible in the lower bound.
If we consider another notion of suboptimality, i.e., changing maximum to summation:
NE-gap0(π) := X 卜Up V*π-i(sι)- Vni(SI).
i∈ [m] L μi	-
This definition of NE-gap is different from the previous definition. With NE-gap0, if each player
can change his policy to improve his expected cumulative reward by ε, the NE-gap0 would be at
4We only prove this lower bound when Ai all equal to 2k. This case is representative.
30
Published as a conference paper at ICLR 2022
least mε. Then we can similarly define ε-approximate Nash equilibrium as the policy π such that
NE-gap0(π) ≤ ε. We simply point out that with this new definition of NE-gap0 and ε-approximate
Nash equilibrium, mimicking the proof of Theorem 2 in Dann & Brunskill (2015), we can prove the
sample complexity’s lower bound for learning a pure-strategy ε-approximate Nash equilibrium in
Markov (cooperative) games is Ω(H2SPm=I Ai∕ε2).
E.1 Proof of Lemma E.2
For convenience, we call the joint-action (in one-step game) that is a Nash equilibrium a Nash strategy.
We begin with a special case of Lemma E.2, i.e. the case when Ai = 2 for all i ∈ [m].
Lemma E.3. Suppose Ai = 2, i = 1, . . . , m and m ≥ 4. Then, there exists an absolute constant c0
such that for any ε≤ 0.4 and any algorithm that outputs a pure strategy πb = (πb1, . . . , πbm ), if the
number of samples
n ≤ co 2mm,
ε2
then there exists a one step game M with stochastic reward on which the algorithm suffers from
ε∕4-suboptimality, i.e.,
EM NE-gap(πb) = EM max max ui(πi, πb-i) - ui(πb) ≥ ε∕4,
i	πi
where the expectation EM is w.r.t. the randomness during the algorithm’s execution within game M.
ui (π) is the expected reward of the ith player when strategy π are taken for each player.
The proof of this lemma further relies on the following lemma.
Lemma E.4. There exists a one-step game for m players where each player has two actions. The
deterministic reward is 0 or 1 and the number ofjoint actions that have 1 is at most 2-m+-. Moreover,
the only pure-strategy Nash equilibria are these joint actions which have reward 1.
Proof of Lemma E.4 We use r(a) to denote the reward of (joint) actions a ∈ {1, 2}m and define
hamming distance d(a, a0) = #{i : ai 6= a0i}. To ensure that pure-strategy Nash equilibria must
have reward 1, we only need to ensure that for a a ∈ {1, 2}m , there exists one a0 ∈ {1, 2}m such that
r(a0) = 1,	d(a, a0) ≤ 1.
In other words, the set {a : r(a) = 1} is a 1-net of {1,2}m under the distance d(∙, ∙). By the
definition of covering number, we only need to prove
m+1
N ({1,2}m,d,1) ≤ 丁
Define K(m, 1) = N ({1, 2}m, d, 1). By hamming code (Hamming (1950)), we know that for any
integer k ≥ 1,
K(2k - 1, 1)
22k-k-1
Moreover, we also have K(n, 1) ≤ 2K(n - 1, 1) by adding 0 and 1 behind the 1-net of {0, 1}n-1.
Taking largest k such that 2k - 1 ≤ n and iterating this construction on the Hamming code we get
K(m, 1) ≤ 2m-[log2(m+1)] ≤ 2m+-. This ends the proof.	□
The next lemma is KL divergence decomposition (Lemma 15.1 of (Lattimore & Szepesvdri, 2020)]),
we restate it in one-step games.
Lemma E.5 (KL divergence decomposition in one-step games). For any one-step games with
stochastic reward and any algorithm. Let X = (a1, r1, a2, r2, . . ., an, rn), where ak is the action
(adaptively) chosen by the algorithm at the kth round and rk is the reward received at the kth round
after ak is taken. P and Q are two probability measure for the stochastic reward. Let N(a) be the
total number of actions a in the first n rounds. Then
KL(X∣p∣∣X∣q) = X Ep[N(a)]KL(P(∙∣a)kQ(∙∣a)).
a
31
Published as a conference paper at ICLR 2022
Then we return to the proof of Lemma E.3. Suppose a game M satisfies the condition in Lemma
E.4, by permuting the actions of M, we get 2m games. They all satisfy the condition in Lemma E.4.
Suppose the reward of the i-th game is r(i)(∙) (i = 1, ∙∙∙ , 2m).
We consider the following family of one-step games with stochastic reward: Let a =
(a1,a2,...,am).
M(ε) = {M⑴ ∈ R{1,2}m with M⑴(a) = ∣ + r⑴(a)ε, i = 1, 2,..., 2m},
where in one-step game M(i), the reward is sampled from Bernoulli(M(i)(a)) if the joint action
is a = (a1 , a2, . . . , am). Moreover, we define M(0) as a game whose reward is sampled from
BemoUlli( 2) independent of the action.
We further let ν denote the uniform distribution on {1, 2, . . . , 2m}.
Proof of Lemma E.3 Fix a one-step game M(i) ∈ M(ε), by Lemma E.4, it’s clear that pure-
strategy Nash equilibria form a set D(i) := a ∈ {a : r(i) (a) = 1}. We have #D(i) ≤ 2m+1/m.
For any online finetuning algorithm A that outputs a pure strategy πb. Suppose πb takes joint-action
a = (bi, &2,…，bm). From the structure of the M(ε), We know
NE-Gap(πb) = εPi(ab 6∈ D(i)),
where Pi is w.r.t. the randomness of the algorithm and game M(i). So we only need to use n, the
number of samples to give a lower bound of Pi(ab 6∈ D(i)).
Since X = {ai, ri,…，an, rn} is a sufficient statistics for the posterior distribution D(i), we have
Ei〜νPi(b ∈ Ds) ≥ inf Ei〜νPi(g(X) ∈ Diii)
g
(i)
≥ inf Ei 〜vP0(g(X ) ∈ D⑺)一Ei 〜V TV(XIP0 ,X IPi)
g
(ii)	1
≥ 2 - Ei〜VTV(X|Po ,X|Pi)
(iii)	ι	_	∕ι…，…T
≥ 2 - Ei〜Vy 2KL(X|Po kX|Pi)
(iv)
2 - Ei 〜V
∖Z2XX
Epo[N (a)]KL(Po(∙∣a)kPi(∙∣a)).
Above, (i) follows from the definition of total variation distance; (ii) uses the fact that under P0, we
can,t get any information about a?, so Ei〜VPo(g(X) ∈ D(i)) = 1 - IDm)I ≥ 2; (iii) uses Pinsker's
inequality; (iv) uses KL divergence decomposition (Lemma E.5).
For a ∈ D(i), we have
KL(Po(∙∣a)kPi(∙∣a)) = KL((Ik (2 - ε, ∣+ ε))
11	1
= 2logE2
≤ 4ε2,
if ε ∈ (0,0.4]. For a ∈ D(i), KL(Po(∙∣a)kPi(∙∣a)) = 0. So
Ei〜“Pi(b ∈ D(ii) ≥ 1 - Eij /2 X Epo [N(a)]ε2
a∈D(i)
≥ 2 - J2Ei〜V~^X~EPO [N(a)]ε2
a∈D(i)
(=)1 - r2n|D(i)|ε2
-2 V	2m
32
Published as a conference paper at ICLR 2022
≥1 -ʌ H.
2m
Above, (i) uses Jensen’s inequality, (ii) uses the fact that EP0 [N (a)] is independent of i which gives
Ei〜V X Epo [N(a)] = 2m X Epo[N(α)] X 1
a∈D(i)	a	i:aED(i)
=ɪ n|D ⑴ |,
2m	,
where Pi：。三0口)1 = |D(1) | is because of the permutation. Finally, we choose co = 击,if n ≤ 击等,
Ei”％ (α∈ D⑴)≥ 4.
So there,s a game instance M(i) on which the algorithm suffer from ε∕4 sub-optimality.	□
The difference between Lemma E.3 and E.2 is the size of action space. To prove the Ai = 2k case,
we need to generalized E.4 to the case each player have 2k actions.
Lemma E.6. For all positive integers m and k. There exists a one-step game for m players where
each player has 2k actions. The deterministic reward is 0 or 1 and the number of joint actions that
2∙(2k)m
have 1 Is at most Ikm . Moreover, the only PUre-Strategy Nash equilibria are these joint actions
which has reward 1.
Proof of Lemma E.6 We would prove this lemma by induction. Without loss of generality, we
suppose the action for each player is 1,2, . . . ,2k. First, we define the hamming distance between
two vectors.
d(x, y) := #{i : x(i) 6= y(i)}.
The joint action space is [2k]m = {1, 2, . . . , 2k}m. Suppose we have an 1-net of [2k]m, D, which
means for every y ∈ [2k]m, we can find a x ∈ D, s.t. d(x, y) ≤ 1. If the reward ofa game satisfy:
r(a) = 1 for all a ∈ D, and r(a) = 0 for all a 6∈ D.
Then for every pure strategy a which has reward 0, we can find another pure strategy a0 s.t. r(a0) = 1
and d(a, a0) = 1. This means that one player can change to obtain higher reward. So a is not a
pure-strategy Nash equilibrium.
As a result, we can construct a game based on the 1-net. The only thing left to be verified is that the
number ofjoint actions that have reward 1 is at most 4既 .Note that the number of joint actions
that have reward 1 is actually |D|, so we need to prove
N([2k]m,d,1) ≤ 2 ∙ (2k)m ,	(17)
km
where N(∙, d, ε) denotes the covering number.
For k = 1, from the proof of Lemma E.4, (17) is true. For k > 1, we first decompose [2k]m into km
smaller blocks, i.e.
ωHi,12, ...,Im) = {2l1 - 1, 2l1} X {2l2 - 1, 2l2} X …，×{2lm - 1, 2lm},
where {l1, l2, . . . , lm} ∈ [k]m. Among these km blocks, we choose the blocks which satisfy
k|l1 + l2 + ∙ ∙ ∙ + lm. Apparently, there are km-1 blocks satisfying m|l1 + l2 + ∙ ∙ ∙ + lm. Because (17)
is true for k = 1, we can pick a 1-net of Ω(1,1,..., 1) with at most 2m elements. By a translation,
(all elements add a constant vector), we can pick a 1-net of each block with at most 2m- elements.
Totally there are at most 3学…=2(2mm dements. These elements form a Set P. We wou^
prove P is a 1-net of [2k]m.
In fact, for any X ∈ [2k]m, suppose X is in Ω(l1,l2,...,lm). After translating this block to
Ω(1,1,..., 1), x is coincided with xo.
33
Published as a conference paper at ICLR 2022
Figure 1: A class of hard-to-learn MDPs where s+ and s- are absorbing state.
If x0 ∈ P, change l1 to l1 such that k |l1 + l2 + ∙ ∙ ∙ + Im. Suppose x0 is the vector in block
Ω(l1, l2,... ,lm) that corresponds to x0 in Ω(1,1,..., 1). This gives x0 ∈ P. Moreover, d(x, x0) ≤
1 because the translation from Ω(l1, l2,..., Im) to Ω(l1, l2,..., Im) only changes the first component.
If x0 ∈ P, We can find x0 in P ∩ Ω(1,1,..., 1) such that d(x0, x0) = 1 because P∣ω(1,1,…，1)
is a 1-net. Suppose x0 and x00 differs at the j-th component. We can change lj to lj0 s.t. k|l1 +
l2 + ∙ ∙ ∙ + lj-1 + Ij + lj+1 + ∙ ∙ ∙ + Im. Suppose x0 corresponds to x0 in Ω(l1,..., Im) and x00 in
Ω(l1,..., lj-1, lj, lj+1,..., Im). By the definition of P, We know x00 ∈ P. Moreover, X and x0 only
differ at the j -th component because of translation and the fact that x0 and x00 only differ at the j -th
component. x0 and x00 also only differ at the j -th component because of translation. So We have x
and x00 may only differ at the j-th component, i.e. d(x, x00) ≤ 1.
Recall that x00 ∈ P. To conclude, We can alWays find another vector y ∈ P such that d(x, y) ≤ 1,
this means P is a 1-net of [2k]m. So (17) is proved.	□
Using this fundamental lemma and suppose a game M satisfies the condition in Lemma E.6, by
permuting the actions of M, We get (2k)m games. They all satisfy the condition in Lemma E.6.
Suppose the reWard of the i-th game is r(i) (∙) (i = 1, ∙ ∙ ∙ , (2k)m) .
We consider the folloWing family of one-step games With stochastic reWard: Let a =
(a1,a2,...,am).
M(ε) = {M⑶ ∈ R[2k]m with M⑴(a) = ； + r⑴(a)ε, i = 1, 2,..., (2k)m},	(18)
Where in one-step game M(i), the reWard is sampled from Bernoulli(M(i)(a)) if the joint action
is a = (a1, a2, . . . , am). Moreover, we define M(0) as a game whose reward is sampled from
Bernoulli(2) independent of the action.
Then by the same argument in proof of Lemma E.3, we can easily prove Lemma E.2.
E.2 Proof of Theorem E.1
We are now ready to prove Theorem E.1 based on Lemma E.2.
Because S ≥ 3, we construct a class of general-sum Markov games as follows (see figure 1): the set
of all joint-actions A can be divided into two sets A+ an A-.
Transition Starting from s1, for a ∈ A+, P1 (s+ ∣s1, a) = 2 + 2(H—1)and P1 (S- |s1, a) =
2 - 2(H-1). For a ∈ A-, P1 (s+ ∣s1, a) = P1 (S- ∣s1, a) = ∣. For all h ≥ 1 and a ∈ A,
Ph(s+|s+,a) = 1 andPh(s-|s-,a) = 1.
Rewards For any a and i ∈ [m], r1,i(S, a) = 0 rh,i(S+, a) = 1 and rh,i(S-, a) = 0, where h > 1.
This also implies the Markov game is cooperative.
If we choose A+ as the joint action with reward Bernoulli(2 + ε) in M(i) defined in (18). By the
construction, for any pure-strategy policy π, if the joint actions taken at step 1 is a 6∈ A+ , we have
34
Published as a conference paper at ICLR 2022
Algorithm 8 Mixed-expert FTRL for weighted adversarial bandits
Require: Hyper-parameters {ηt}1≤t≤T, {γt}1≤t≤T. Sequence {αit]≤≤t≤ττ..
1:	Initialize: Accumulators t〃 J 0 for all b0 ∈ [A].
2:	for t = 1, 2, . . . , T do
3:	Compute weight Ui J at∕a1.
4:	Compute action distributions for all sub-experts b0 ∈ [A] by FTRL:
q'(a) Ha exp (―⑺汕 ∕ut) ∙ X Wτ(b)bT (a)).
5:	Computepi ∈ △[* by solving the linear systempi(∙) = PA=Ipi(b0)qb0(∙).
6:	Sample sub-expert b 〜pi.
7:	Sample action ai 〜qb from sub-expert b.
8:	Play the action ai, and observe the (bandit-feedback) loss 'i(ai).
9:	Update accumulator for sub-expert b: tb J tb + 1.
10:	Compute the loss estimate Ibbb (∙) ∈ R≥0 as
`bibb (a) J
'ib (a)1 {a = a}
qb(a) + Yib
11:	Set wib (b) J ui.
NE-gap(π) ≥ ε. The only useful information in each episode is the transition at step 1. Transition
from s1 to s+ can be viewed as getting a 1 reward and transition from s1 to s- can be viewed as
getting a 0 reward. So learning pure-strategy Nash equilibrium of this class of Markov games is
equivalent to learning pure-strategy Nash equilibrium of a game for m players with stochastic reward.
As a result, by Lemma E.2, if the number of episodes
K ≤c0 ⅛⅛ ≤4c0
H2Pm=1At
ε2
then there exists general-sum Markov cooperative game MG on which the algorithm suffers from
ε∕4-suboptimality, i.e.
EM NE-gap(πb) ≥ ε∕4.
This finish the proof of Theorem E.1.
□
F	Adversarial Bandit with Low Weighted Swap Regret
In this section, we describe and analyze our main algorithm for adversarial bandits with low weighted
swap regret. Our Algorithm, Mixed-expert Follow-The-Regularized-Leader (FTRL) for weighted
adversarial bandits, is described in Algorithm 8.
Problem setting Throughout this section, we consider a standard (adversarial) bandit problem
with action space is [A] = {1, . . . , A} for some A ≥ 1. We assume that the realized loss values
'i ∈ [0, 1]a, and let 'i(a) := Ei['i(a)] denote the expected loss conditioned on (as usual) all
information before the sampling of the action ai in Line 6 & 7. The hyperparameters in Algorithm 8
are chosen as
ηi = γi = PU (At),
where the log factor
ι:= 4 log(8H AT ∕p).	(19)
35
Published as a conference paper at ICLR 2022
F.1 Main result for adversarial bandit with low weighted swap regret
We first define a strategy modification. A strategy modification is a function F : [A] → [A] which
can also be applied to any action distribution μ ∈ Δa, such that F ◊ μ gives the SWaP distribution
which takes action F (a) with probability μ(a).
The sWap regret (Blum & Mansour, 2007; Ito, 2020) measures the difference betWeen the cumulative
realized loss for the algorithm and that for swapped action sequences generated by an arbitrary strategy
modification F. Here, we consider a weighted version of the swap regret with some non-negative
weights αti 1≤i≤t, defined as
t
Rswap(t) :=	max X αi['i(ai) - 'i(F(ai))].	(20)
FJA]→[A] Y
i=1
We will also consider a slightly modified version of the swap regret used in our analyses for learning
CE, defined as
t
RswaP(t)：= max Xαt['i(ai) -〈F◊ pi,'i)],	(21)
F ιA]→[A] i=ι
where pt is t-th action distribution (from which the action at is sampled from) played by the algorithm.
We now state our main result of this section.
Lemma F.1 (Bound on weighted swap regret). If we execute Algorithm 8 for T rounds and the
weights αit are chosen according to (2), then with probability at least 1 - p/2, we have the
following bounds on the swap regret simultaneously for all t ∈ [T]:
t
Rswap(t) = max X αt['i(ai) - 'i(F(ai))] ≤ C(HAPPt + HAι∕t),
FJA]→[A] i=1
t
RswaP(t) = max Xαi['i(ai) -〈F◊ pi,'，] ≤ CHApP + HAι∕t),
FJA]→[A] Y
i=1
where C > 0 is some absolute constant.
The rest of this section is devoted to proving Lemma F.1, organized as follows. We first present some
important properties of Algorithm 8 in Section F.2. We then prove Lemma F.1 in Section F.3 using
new auxiliary results on weighted adversarial bandits with predictable weights. Lastly, these auxiliary
results are stated and proved in Appendix F.4.
F.2 Properties of Algorithm 8
Solution of the linear system In Line 5, we need to compute pt ∈ ∆[A] by solving the linear
system pt(∙) = PA=I pt(b0)qb (∙). Such pt(∙) can be interpreted as the stationary distribution of a
Markov chain over [A] with transition probability P(b|a) = qa(b). This guarantees the existence of
Pt ∈ ∆[A] such that pt(∙) = PA=Ipt(b0)qb(∙). Moreover, pt(∙) can be computed efficiently (see
e.g. Cohen et al. (2017b)).
FTRL update for each sub-expert Here we show that, the updates for any particular sub-expert
b ∈ [A] in Algorithm 8 is exactly equivalent to an FTRL update (with loss sequence being the ones
only over the episodes in which b is sampled) which we summarize in Algorithm 9. This is important
as our proof relies on reducing the weighted swap regret to a linear combination of weighted regrets
for each sub-expert b ∈ [A].
To see this, fix any b ∈ [A]. When b is sampled in Line 6 at episode t, we have accumulator tb and
weight wtb (b) = ut = αtt∕αt1 at the end of episode t. Action at is chosen from distribution
qb(a) Ha exp {<ntb∕wtb) ∙ X WTw))b(a))
36
Published as a conference paper at ICLR 2022
after Wtb (b) = Ut = αtt∕α^1 is observed. The loss estimate '九 is
bb (a. K (a)1 {a = at}.
~	qb(a) + Ytb
So from the sub-expert b’s perspective, she is performing FTRL (follow the regularized leader) with
changing step size and random weight (We summarize this in Algorithm 9). Suppose the sub-expert b
is chosen at episode t = k1, k2, . . . , ktb, then the weighted regret for sub-expert b becomes
Rt (b) := sup
θ*∈∆A
tb
X Wτ(b)('kτ(akτ )-hθ*,'kτi).
τ=1
(22)
Bounded weight wi(b) Recall αit is chosen as in (2). We have the following bound:
at	1	1	(H + 2)(H + 3) •…(H +1)
—≤ —=------：----：--：------=----------：---：----
aq	%	αι(1 — 02)• • • (1 — at)	1 • 2 ∙ ∙ ∙ (t — 1)
So for any t ≤ T , we have
att/at1 ≤ (H+T)2H ≡ W.
The weight wnb (b) = att/at1 have a (non-random) upper bound W.
≤ (H + t)H+1
(23)
F.3 Proof of Lemma F.1
We use bi to denote the sampled sub-expert b at the i-th episode. Define Gi as the σ-algebra generated
by all the random variables observed up to the end of the i-th episode.
First observe that for Rswap(t) we have the bound
	t RswaP⑴=max X αi['i(ai) - 'i(F(Qi))] F[A]→[A] M tt ≤ F r AJX ɑt['i(ai) - 'i(F (bi))]+	max X ɑi['i(F (bi)) - 'i(F (ai))], i=1	i=1 '{} '{} I	II
also, for Rswap (t) we have the bound
	swaP t RswaP (t) = max X ɑt['i(ai) - (F。pi,'i)] F ∙[A]→[A] . 1 i=1 tt ≤ f max X ɑi['i(ɑi) -'i(F(bi))]+F mαχ X at['i(F(bi)) -(F。Pi,'i〉], i=1	i=1 '{} '{} I	IeI
Term II and II can be bounded by concentration in a similar fashion; here we first focus on term II.
Observe that at the i-th episode, as pi obtained in Line 5 solves the equation
A
pi (a) = X pi(b0)qb (a),
b0=1
We have bi 〜pi(∙) and ai 〜qb(∙) has the same marginal distribution conditioned on Gi-ι, where
Gi-1 denote the σ-algebra generated by `i and all the random variables in the first i-1 episodes. There-
fore, fixing any strategy modification F : [A] → [A], we have ait [`i (F (bi)) - `i (F (ai))] 1≤i≤t is
a bounded martingale difference sequence w.r.t. the filtration {Gi}, so by Azuma-Hoeffding inequality
andPit=1(atj)2 ≤ 2H/t,
P
(X αi['i(F(bi)) - 'i(F(ai))] ≥ 2{Hlog1/p
≤ p.
37
Published as a conference paper at ICLR 2022
As there is at most AA strategy modifications, we can substitute p with p/(4AAT ), and take a union
bound to get
t
II = max Xai['i(F(bi)) — 'i(F(ai))] ≤ 2，HAlog(AT∕p)∕t ≤ C/HAι∕t	(24)
FJA]→[A] M
simultaneously for all t ∈ [T] with probability at least 1 - p∕4. We also note that, by a similar
argument (as bi is also distributed according to pi conditioned on the past), we have that
t
IeI = max X ai['i(F(bi)) -(Fopi,'i)] ≤ CPHAι∕t.
F ιA]→[A] i=ι
(25)
Therefore for bounding both Rswap(t) and Rswap(t), it suffices to bound term I.
We next bound term I. Define Ub := i ∈ [t] : bi = b and let ntb be the value of tb at the end of
the t-th episode, i.e. ntb = # i : bi = b . We also suppose the sub-expert b was chosen at episode
t = k1b, k2b, . . ., knbt up to episode t. Then we have
nb
t
Xαi['i(Oi)- 'i(F(bi))] = X X ɑi['i(Oi)- 'i(F(b))]
i=1
b∈A i∈Ub
ntb	kτb
XX a1 αtr ['kb (akτ) - 'kb (F (b))]
b∈A τ=1	αt
ntb
XXɑ1 Wτ(b)['kT(akb) - 'kT(F(b))]∙
b∈A τ=1
Here the last equation is because our choice of wτ (b)
corollary from the definition of αti in Eq. (3).
Then because
∕α1kb = αtkτ ∕αt1 which is a simple
ntb	ntb
X Wτ(b)['kT (akτ) - 'kT (F(b))] ≤ maxX Wτ(b)['kT(akτ) -〈'峙，。*〉] = Rt(b),
τ=1	τ=1
where Rt(b) defined in (22) is the weighted regret Rt(b) for sub-expert b , we can use our result on
weighted adversarial bandits with predictable weights (Lemma F.2) to bound this term (The upper
bound W of the weight wτ (b) can be taken as (H + T)2H by the calculation of (23). Moreover,
wτ (b) = αtkτ ∕αt1 and {αtj}tj=1 is increasing, so {wτ (b)}τ≥1 is non-decreasing.). Recall that our
choice of log term is ι = 4 log 4HpAT = log AT dlpg2 We where p0 ≤ p∕(4A). Thus by Lemma F.2,
with probability at least 1 - p∕(4A),
Rt(b) ≤ 15 max wτ (b)	Antbι + ι for all t ∈ [T].
τ ≤ntb
Taking a union bound, we get
Rt(b)	≤ 15 max wτ (b)	Antbι	+ ι for all	(t,	b) ∈	[T]	×	[A]	(26)
τ ≤ntb
with probability at least 1 - p∕4.
On this event, we have
t
F[m]→[A]X ai['i(ai)- 'i(F(bi))]
i=1
ntb
F maxA X X α1wτ(b)['kb (akτ) - 'kb (F(b))]
:[ ]→[A] b∈AT=1
38
Published as a conference paper at ICLR 2022
ntb
≤ XF .max . ,X α1wτ (b)['kT (akτ ) - 'kT (F (b))]
b∈A F :[ ]→[ ] T=1
≤ X αt1Rt(b)
b∈A
≤) X α1 max WKa)(15qAnbι+15ι)
b∈A	τ≤nb
…、	CkIn	Z——
=X α1 -tr-(15 JAnbI+151)
b∈A αt
(iii)	_ 2 H !-----
≤ E 7(15JAnbI+i5|).
a∈A
Here, (i) uses (26), (ii) uses the at is increasing w.r.t. t0 and (iii) uses mαxo≤to≤t at ≤ 2H/t.
Finally, because Pb∈∕ nb = t by the concavity of x → √x, we have with probability at least
1 - p/4,
I
t
F m→[A] X at['i(ai) - 'i(F(bi))]
i=1
30H
≤ 丁 ∙
30 HA pb/t + 30HAι∕t.
Combining this with (24) and (25), we finish the proof.
□
F.4 Auxiliary lemmas for weighted adversarial bandit with predictable
WEIGHTS
In this subsection, we consider the Follow the Regularized Leader (FTRL) algorithm (Lattimore &
SzePeSv^ri, 2020) with
1.	changing step size,
2.	weighted regret with Ft-1-measurable weights and loss distributions,
3.	high probability regret bound.
We present these results because the predictable weights we would use are potentially unbounded
from above; if weights are predictable and also bounded, then there may be an easier analysis.
Interaction protocol We first describe the interaction protocol between the environment and the
player for this problem. At each episode t, the environment adversarially choose a weight wt which
takes values in R>0, and distributions (Lat)a∈[A], where each Lat is a distribution that takes values
in P ([0, 1]) (the space of distributions supported on [0, 1]). Then the player receives the weight
wt, chooses an action at, and observes a loss 't(at)〜 Lat,t∙ The action chosen at can be based
on all the information in Dt ≡ {(ai,Wi, 'i(ai))i≤t-ι, wt}. Then, the environment can observe the
player’s action at and the incurred loss realization `t(at), and use these information and some external
randomness zt to choose the weight and distributions (wt+1, (La,t+1)a∈[A]) of the next episode. We
will consider the following variant of FTRL algorithm for the player.
We denote't ≡ (E'^^at['])α∈[A] ∈ [0, 1]a which is a random vector but is σ((Lαt)α∈[A])-
measurable. For some θ* ∈ RA, the regret of the player up to episode t is defined as
t
Rt ≡ X Wi('i(αi)-hθ*,'ii).
i=1
39
Published as a conference paper at ICLR 2022
Algorithm 9 FTRL for Weighted Regret with Changing Step Size and Predictable Weights
1:	for episode t = 1,...,T do
2:	Observe wt > 0.
3：	θt(a) Ha exp(-(ηt∕wt) pt-1 Wibi(a)).
4:	Take action at 〜θt(∙), and observe loss 't(at).
5:	't(a){-'t(a)1	{at	=	a} /(θt(a) +	Yt)	for all	a.
Let (zt)t≥1 be a sequence of external random variables which are identically and independently
distributed as Unif([0, 1]) and are independent of all other random variables. We define Ft =
σ({ai, 'i, zi}i≤t) to be the sigma algebra generated by the random chosen action ai, the random loss
`i(ai), and the external random variables zi by episode t. We assume that the random weights and
the random loss distributions (wt, (Lat)a∈[A] )t≥1 are a (Ft)t≥1-predictable sequence, in the sense
that (wt, (Lat)a∈[A]) is Ft-1-measurable. Then Ft contains all information (random chosen action,
random observed loss, random weight, and random loss distributions) before action at+1 is taken.
We assume the predictable sequence (wt)1≤t≤T have a global (non-random) upper bound W. Then
we define the log term ι = log(AT dlog2 W e/p). We set
ηt=Yt = ∖⅛
F.4.1 Regret bound
In the following, we consider to give a high probability weighted regret bound for Algorithm 9.
Lemma F.2. Let (wt, (Lat)a∈[A])t≥1 be any Ft-predictable sequence satisfying 1 ≤ mini≤t wi ≤
maxi≤t wi ≤ W for some constant (non-random) W > 0 almost surely. Moreover, suppose wi is
non-decreasing. Then, following Algorithm 9, with probability at least 1 — 4p, for any θ* ∈ ∆A and
t ≤ T we have
t
^X Wi('i(ai) — hθ*,'ii) ≤ 15max Wi ∙ [√Atι + ι],
i=1	i≤t
where ι = log(AT dlog2 W e/p).
Proof. This lemma follows from the bound in Lemma F.3 and a concentration step that we establish
below.
Define M = dlog2 We, and wi(k) = wi1 wi ≤ 2k . Then wi (k) is also Fi-1 measurable. We
Consider sequence wi (k)(`i (ai) — hθi, `ii) i≥1 . Since wi is Fi-1-measurable, E[wi (`i (ai) —
hθi, 'i)')∖Fi-ι] = 0, which means {wi(k)('i(ai) —〈0%,'))}i≥1 is a martingale difference sequence
w.r.t. filtration (Fi). So by Azuma-Hoeffding inequality,
P
wi(k)(`i(ai) — hθi, `ii) ≤
，2 ∙∣t(2k)2
≥ 1 - TM
Taking a union bound, we have
P 卜k ∈ [M], X Wi(k)('i(ai) — hθi,'ii) ≤ 2k√2tl) ≥ 1 — p∕T.
(27)
Denote k0 = dlog2 maxi≤t wie, then we have
t
{∀k ∈ [M], X Wi(k)('i(ai) — hθi,'ii) ≤ 2k√2t∑}
i=1
t
t
Wi(k0)('i(ai) — hθi, 'ii) ≤ 2k√2tl} ⊆ { X Wi('i(ai) - (θi,'ii) ≤ 2maχ Wi√2tl}.
i=1	i≤t
40
Published as a conference paper at ICLR 2022
Then probability bound gives
≥ 1 - p/T.
P
(X Wi('i(ai) — hθi,'ii) ≤ 2maχWi
Taking a union bound, we get
t
'X' Wi('i(ai) — (θi,'ii) ≤ 2 max Wi√2tι for all t ∈ [T]
i≤t
i=1
with probability at least 1 — p. Summing the above and the regret bound shown in Lemma F.3, we
finish the proof.	□
Lemma F.3. Let (wt, (Lat)a∈[A])t≥1 be any Ft-predictable sequence satisfying 1 ≤ mini≤t wi ≤
maxi≤t wi ≤ W for some constant (non-random) W > 0 almost surely. Moreover, suppose wi is
non-decreasing. Then, following Algorithm 9, with probability at least 1 — 3p, for any θ* ∈ ∆A and
t ≤ T we have
t
Wi hθi — θ*, 'ii ≤ 10 max Wi ∙
i=1	i≤t
where ι = log(AT dlog2 W e/p).
Proof. The regret Rt(θ*) can be decomposed into three terms
t
Rt (θ*)= X Wi hθi — θ*,'ii
i=1
t
XWi<为 —θ*,'i
i=1
X-----------
(A)
`bi — `i E
{z^
(C)
^
and we bound (A) in Lemma F.5, (B) in Lemma F.6 and (C) in Lemma F.7.
Setting ηt = Yt = Pit, the conditions in Lemma F.5 and Lemma F.7 are satisfied. Putting them
together and take union bound, we have with probability 1 — 3p
Rt W ≤ Wr
+ A X ηiWi +
2 i=1
t
max Wi ι + A	Yi Wi + 2 max Wi √2ti + 2 max Wi∣∕γt
i≤t	i≤t	i≤t
i=1
≤ max Wi
i≤t
+ ι + 2√ 2tι +
≤ 10 max Wi [√Atι + ι]
for all t ∈ [T]
This proves the lemma.
□
The rest of this section is devoted to the proofs of the Lemmas used in the proofs of Lemma F.3.
We begin the following useful lemma adapted from Lemma 1 in Neu (2015), which is crucial in
constructing high probability guarantees.
Lemma F.4. For any predictable sequence of coefficients c1, c2, . . . , ct s.t. ci ∈ [0, 2Yi]A w.r.t.
(Fi)i≥1 and fixing t, we have with probability at least 1 — p/(AT),
t
X Wi ci , `bi — `i
i=1
≤ 2 max Wiι
i≤t
41
Published as a conference paper at ICLR 2022
Proof. Define M =「log? We, and wi(k) = wj {wi ≤ 2k }. By definition,
/ 7 ʌ ∕? (	∖
wi(k)`i (a)
(a ∖7s ( ∖ -t r	i
wi (k)`i (a) 1 {ai = a}
θi (a) + Yi
wi(k)`i (a) 1 {ai = a}
θi (a) +
∕T∖Z)∕∖TU	-l
wi(k)'i(a)1{ai = a}〜
2k	Yi
≤
C	/ 7 ∖ 7 /	∖-i C	-l
C 7“	2γiwi(k)'i(a)1{ai — a}
2k	2kθi(a)
rʌ	∕τ∖7∕∖τr	、
2γi i _|_ YiWi(k)'i(a)1{ai =a}
+	2k θi(a)
(i) 2k ∣	1	2γiWi(k)'i (a) 1 {ai = a}
≤ 2Yiog [ +	久而
where (i) is because ι+z^2 ≤ log (1 + Z) for all Z ≥ 0.
Defining the sum
Si = *DcizE, Si =*hci,'ii,
2k	2k
Then Si is Fi-ι-measurable since w%, ci,' are Fi-ι-measurable. Using EiH to denote the condi-
tional expectation E[∙∣Fi], We have
Ei-Ihexp (Si)] ≤ Ei-1 "exP (X Cf log (1 +2γ⅛∣^))
≤ E i Y (1 + ci(a) wi(k)'i (a) 1 {ai = a}]]
a	2kθi (a)
m - Lci (a) Wi(k)'i (a) 1 {ai = a}
=Ei-1[1 + ⅛--------------E----------------J
= 1 + Si ≤ exp(Si)
where (i) is because Z1 log (1 + Z2) ≤ log (1 + Z1Z2) for any 0 ≤ Z1 ≤ 1 and Z2 ≥ -1. Here we
are using the condition ci (a) ≤ 2Yi to guarantee the condition is satisfied.
Equipped with the above bound, we can now prove the concentration result.
≤ P exp
ATM Et-ι
p
ATM Et-2
ATM Et-2
exp
exp
exp
Si)] ≥ 叫
Xt (Sbi - Si)##
t-1
X(Sbi -Si) Et-1 hexp (Sbt -St)i
i=1
Xt-1 (Sbi - Si)##
≤
≤
≤
—
≤…≤ ɪ.
_	_ ATM
So we have
t
P(X wi(k) cci,i- - Q ≤ 2kι) ≥ 1 -p∕(ATM).
t=1
Taking a union bound,
t
P(∀k ∈ [M], Xwi(k) ccibi- - Q ≤ 2kι) ≥ 1 - p∕(AT).
t=1
(28)
42
Published as a conference paper at ICLR 2022
Denote k0 = dlog2 maxi≤t wie, and note that
t
{∀k ∈ [M], X wi(k){i, Z -&〉≤ 2kι}
t=1
tt
⊆ X X Wi(k0)(Ci,工-Q ≤ 2kι} ⊆ X X Wi <Ci,bi - Q ≤ 2max Wi∣}
t=1	t=1	i≤t
Therefore, we have
P
Wi Ci, bi - Q ≤ 2 max Wi∣) ≥ 1 — p∕(AT).
This proves the lemma.
□
Using Lemma F.4, we can bound the (A)(B)(C) separately as below.
Lemma F.5. If ηi ≤ 2γi for all i ≤ t and {ηi /Wi}i≥1 is non-increasing, with probability 1 - p, for
any t ∈ [T] and θ* ∈ ∆A,
X Wi (θi - θ*,K> ≤ Wt log A + A XηiWi + 1 maxWi∣.
i=1	ηt	2 i=1	2 i≤t
Proof. We use the standard analysis of FTRL with changing step size, see for example Exercise
28.12(b) in Lattimore & SzePesVdri (2020). Notice the essential step size is nt/Wt which is non-
increasing and the essential loss vector is WtQt, We have
X IA	n*	八右	Wt logA _UX	∣^∕q K / ∖	κL(θi+1kθi)
), Wi ξθi	- θ , 'i/	≤------+>/ Wi	hθi	- θi+1, Qii----------∙
i=1	ηt	i=1	ηi
We claim that
Dθi - θi+ι,biE - KL(ZIkθi) ≤ ni Dθi,b2E .	(29)
In fact, applying Theorem 26.13 in Lattimore & Szepesvdri (2020) gives that
Dθi - θi+ι,QiE - KL(ZIkθi) ≤ * "θi + (1 -Ui)θi+ι,Q2E ,
for some ui ∈ [0, 1]. Note that our Qi only have at most one non-zero entry, i.e. Qi(a) > 0 if and only
if a = ai. If θi(ai) ≥ θi+1(ai), we have
Ti Duiθi + (1 - Ui)θi+1, b2 E = Ti [uiθi(ai) + (1 - ui)θi+1 (ai)]Qi(ai)2
≤ nθi(ai)bi(ai)2 = n(。常).
Otherwise, θi(ai)	<	θi+1(ai),	so we have	θi	- θi+1,Qbi	=	[θi(ai)	- θi+1(ai)]Qbi(ai)	≤ 0 and
-KL(θi+ιkθi)∕ni ≤ 0. In both cases, (29) is correct. As a result,
XWi Dθi - θ*,QiE ≤ 3A +1XnWi Dθi,EE
i=1	ηt	2 i=1
≤ T + 2X XηiWiQbi (a)
ηt	2 i=1 a∈A
(i) Wt log a , 1
≤-----------+ ʒ	niW niWi'i (a) + max Wi∣
ηt	2	i≤t
t	i=1 a∈A
43
Published as a conference paper at ICLR 2022
≤
Wt log A
ηt
+ A X ηwi +
i=1
max wiι
i≤t
where (i) is by using Lemma F.4 with ci(a) = ηi. The any-time guarantee is justified by taking union
bound.	□
Lemma F.6. With probability 1 - p, for any t ∈ [T],
tt
X wi θi,`i-`bi ≤AX
Yiwi + 2 max Wi √2tι.
i≤t
Proof. We further decompose the left side into
tt	t
X wi (θi,'i - G = X wi (θi,'i - Ei-i[bi]) + X wi Wi, Ei-1 [bi] - Q.
i=1	i=1	i=1
The first term is bounded by
tt
X wi (θi,'i - Ei-ι[g]) = X wi <θi,'i -
i=1	i=1
三,)
wi	θi,
i=1
γi
θi + γi
`i	≤ A Xt
i=1
γiwi.
To bound the second term, we use similar argument in the proof of Lemma F.4 , we define w
maxi≤t wi, M = dlog2 We. and wi (k) = wi1 wi ≤ 2k , notice
Dθi,'iE ≤ X θi (a) θX)+Y} ≤ X1 {ai=a} = 1，
a∈A	a∈A
thus {wi(k) (θi, Ei-ι['i] - 'J}i=ι is a bounded martingale difference sequence w.r.t. the filtration
{Fi}it=1. By Azuma-Hoeffding inequality,
t
X wi(k) (θi, Ei-1 ['i] - Q ≤
i=1
t	__________
2ι^X wi(k)2 ≤ 22ι ∙ t(2k)2
i=1
∖
with probability at least 1 - p/T M. Taking a union bound, we get
t
Xwi(k) (θi, Ei-1 [bi] - bi) ≤ √2ι ∙ t22k , for all (t, k) ∈ [T] X [M]
i=1
with probability at least 1 - p. On this event, choosing k0 = dlog2 we, we have
tt
X wi Wi, Ei-ι['i] - bi) ≤ X wi(k0) (θi, Ei-ι['i] - bi)
i=1	i=1
≤2k √2ι ∙ t ≤ 2max wi√2ι ∙ t.
i
This ends the proof.	□
Lemma F.7. With probability 1 — P, for any t ∈ [T] and any θ* ∈ ∆A, if Yi is non-increasing in i,
t
Xwi ^θ*,'i - 'i〉≤ 2maxwi∣∕γt.
i=1	i≤t
44
Published as a conference paper at ICLR 2022
Proof. Define a basis {ej}jA=1 of RA by
ej (a)
1 if a = j,
0 otherwise.
Then for all the j ∈ [A], apply Lemma F.4 with ci = γtej. Since now ci(a) ≤ γt ≤ γi, the condition
in Lemma F.4 is satisfied. As a result, for any t ∈ [T] and j ∈ [A], we have with probability at least
1 - p/(T A) that
t
X : Wi (ej,'i - 'i ) ≤ 2 max wiι/Yt.
i=1	i≤t
Taking a union bound, we have with probability at least 1 - p,
t
ej , `i -
`i
≤ 2 max Wi∣∕γt
i≤t
for all (j, t) ∈ [m] × [T].
Since any θ* is a convex combination of {ej}jA=ι, on this event, We also have
t
X Wi (θ*,g-'J
i=1
≤ 2 max Wi ι∕γt
i≤t
for all t ∈ [T].
This finishes the proof.
□
45