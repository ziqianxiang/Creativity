Published as a conference paper at ICLR 2022
Minimax Optimality (Probably) Doesn’t Imply
Distribution Learning for GANs
Sitan Chen
UC Berkeley
Jerry Li
Microsoft Research
Yuanzhi Li
CMU
Raghu Meka
UCLA
Ab stract
Arguably the most fundamental question in the theory of generative adversarial
networks (GANs) is to understand when GANs can actually learn the underlying
distribution. Theoretical and empirical evidence (see e.g. (Arora et al., 2018)) sug-
gests local optimality of the empirical training objective is insufficient. Yet, it does
not rule out the possibility that achieving a true population minimax optimal solu-
tion might imply distribution learning. In this paper, we show that standard cryp-
tographic assumptions imply this stronger condition is still insufficient. Namely,
we show that if local pseudorandom generators (PRGs) exist, then for a large fam-
ily of natural target distributions, there are ReLU network generators of constant
depth and poly size which take Gaussian random seeds so that (i) the output is
far in Wasserstein distance from the target distribution, but (ii) no polynomially
large Lipschitz discriminator ReLU network can detect this. This implies that even
achieving a population minimax optimal solution to the Wasserstein GAN objec-
tive is likely insufficient for distribution learning in the usual statistical sense. Our
techniques reveal a deep connection between GANs and PRGs, which we believe
will lead to further insights into the computational landscape of GANs.
1 Introduction
When will a generative adversarial network (GAN) trained with samples from a distribution D actu-
ally output samples from a distribution that is close to D? This question is one of the most founda-
tional questions in GAN theory—indeed, it was raised since the original paper introducing GANs.
However, despite significant interest, this question still remains to be fully understood for general
classes of generators and discriminators.
A significant literature has developed discussing the role of the training dynamics (Liu et al., 2017;
Li et al., 2018; Arora et al., 2018; Berard et al., 2019; Wiatrak et al., 2019; Thanh-Tung & Tran,
2020; Allen-Zhu & Li, 2021), as well as the generalization error of the GAN objective (Zhang et al.,
2017; Arora et al., 2017; Thanh-Tung et al., 2019). In most cases, researchers have demonstrated
that given sufficient training data, GANs are able to learn some specific form of distributions after
successful training. Underlying these works appears to be a tacit belief that if we are able to achieve
the minimax optimal solution to the population-level GAN objective, then the GAN should be able
to learn the target distribution. In this work, we take a closer look at this assumption.
What does it mean to learn the target distribution? As a starting point, we must first formally
define what we mean by learning a distribution; more concretely, what do we mean when we say that
two distributions are close? The original paper of (Goodfellow et al., 2020) proposed to measure
closeness with KL divergence. However, learning the target distribution in KL divergence is quite
unlikely to be satisfied for real-world distributions. This is because learning distributions in KL
divergence also requires us to exactly recover the support of the target distribution, which we cannot
really hope to do if the distribution lies in an unknown (complicated) low-dimensional manifold. To
rectify this, one may instead consider learning in Wasserstein distance, as introduced in the context
of GANs by (Arjovsky et al., 2017), which has no such “trivial” barriers. Recall that the Wasserstein
distance between two distributions D1 , D2 over Rd is given by
W1(D1,D2)=	sup ED1[f]-ED2[f] ,
Lip(f)≤1
1
Published as a conference paper at ICLR 2022
where for any f : Rd → R, we let Lip(f ) denote the Lipschitz constant of f . That is, two densities
are close in Wasserstein distance if no Lipschitz function can distinguish between them. In this
work we will focus on Wasserstein distance as it is the most standard notion of distance between
probability distributions considered in the context of GANs.
Note that if the class of discriminators contains sufficiently large neural networks, then minimax
optimality of the GAN objective does imply learning in Wasserstein distance. This is because we
can approximate any Lipschitz function arbitrarily well, with an exponentially large network with
one hidden layer (see e.g. (Poggio et al., 2017)). Thus, in this case, minimizing the population GAN
objective is actually equivalent to learning in Wasserstein distance. Of course in practice, however,
we are limited to polynomially large networks for both the generator and the discriminator. This
raises the natural question:
Does achieving small error against all poly-size neural network discriminators imply that the
poly-size generator has learned the distribution in Wasserstein distance?
One might conjecture that this claim is true, since the generator is only of poly-size. Thus, using
a (larger) poly-size discriminator (as opposite to the class of all 1-Lipschitz functions) might still
be sufficient to minimize the actual Wasserstein distance. In this paper, however, we provide strong
evidence to the contrary. We demonstrate that widely accepted cryptographic assumptions imply
that this is false, even if the generator is of constant depth:
Theorem 1.1 (Informal, see Theorem 3.1). For any n ∈ N, let γn be the standard Gaussian mea-
sure over Rn. Assuming local pseudorandom generators exist, the following holds for any suffi-
Ciently large m ∈ Z, d,r ≤ poly(m), and any diverse1 target distribution D* over [0,1]d given
by the pushforward of the uniform distribution Ir on [0, 1]r by a constant depth ReLU network of
polynomial size/Lipschitzness:2
There exist generators G : Rm → Rd computed by (deterministic) ReLU networks of constant
depth and polynomial size for which no ReLU network of polynomial depth/size/Lipschitzness can
tell apart the distributions G(Ym) and D*, yet G(Ym) and D* are Ω(1)-far in Wasserstein distance.
While Theorem 1.1 pertains to the practically relevant setting of continuous seed and output dis-
tributions, we also give guarantees for the discrete setting. In fact, if we replace D* and Ym by
the uniform distributions over {±1}d and {±1}m, we show this holds for generators whose output
coordinates are given by constant-size networks (see Theorem 3.2).
We defer the formal definition of local pseudorandom generators (PRGs) to Section 2.2. We pause
to make a number of remarks about this theorem.
First, our theorem talks about the population loss of the GAN objective; namely, it says that the
true population GAN objective is small for this generator G, meaning that for every ReLU network
discriminator f of polynomial depth/size/Lipschitzness, we have that
|E[f (D*)] - E[f(G(Ym))]∣ ≤ dω11).
In other words, our theorem states that even optimizing the true population minimax objective is
insufficient for distribution learning. In fact, we show this even when the target distribution can be
represented perfectly by some other generative model.
Second, notice that our generator is extremely simple: notably, itis only constant depth. On the other
hand, the discriminator is allowed to be much more complex, namely any ReLU network of poly-
nomial complexity. This discriminator class thus constitutes the most powerful family of functions
we could hope to use in practice. Despite this, we show that the discriminators are still not powerful
enough to distinguish the output of the (much simpler) generator from the target distribution.
Third, our conclusions hold both for d ≥ m and d ≤ m, so long as the input and output dimensions
are related by polynomial factors.
1See Definition 6. In the discussion proceeding this definition, we give a number of examples making clear
that this is a mild and practically relevant assumption to make.
2When we say “polynomial,” we are implicitly referring to the dependence on the parameter m, though
because d, r are bounded by poly(m), “polynomial” could equivalently refer to the dependence on those pa-
rameters if they exceeded m.
2
Published as a conference paper at ICLR 2022
Finally, we formally define the class of “diverse” target distributions for which our conclusions hold
in Section 2.3. We note that this class is quite general: for instance, it includes pushforwards of the
uniform distribution under random leaky ReLU networks (see Lemma 2.3).
Empirical results. To complement these theoretical results, we also perform some empirical vali-
dations of our findings (see Section 4). Our theorem is constructive; that is, given a local PRG, we
give an explicit generator which satisfies the theorem. We instantiate this construction with Goldre-
ich’s PRG with the “Tri-Sum-And” (TSA) predicate (Goldreich, 2011), which is an explicit function
which is believed to satisfy the local PRG property. We then demonstrate that a neural network dis-
criminator trained via standard methods empirically cannot distinguish between the output of this
generator and the uniform distribution. While of course we cannot guarantee that we achieve the
truly optimal discriminator using these methods, this still demonstrates that our construction leads
to a function which does appear to be hard to distinguish in practice.
GANs, PRGs, and circuit lower bounds. At a high level, our results and techniques demonstrate
surprising and deep connections between GANs and more “classical” problems in cryptography
and complexity theory. Theorem 1.1 already shows that cryptographic assumptions may pose a
fundamental barrier to the most basic question in GAN theory. In addition to this, we also show
a connection between this question and circuit lower bounds. In the supplementary material, we
show that if we are able to unconditionally exhibit generators which can fool polynomially large
ReLU network discriminators, then we would obtain breakthrough circuit lower bounds against
TC0 (see Theorem C.2 and Remark C.3). This complements Theorem 1.1, as it says that if we can
unconditionally construct generators which fool realistic discriminators, then we make progress on
long-standing questions in circuit complexity. We believe that exploring these connections may be
crucial to achieving a deeper understanding of what GANs can and cannot learn.
1.1	Related Work
GANs and Distribution Learning The literature on GAN theory is vast and we cannot hope to
do it full justice here. For a more extensive review, see e.g. (Gui et al., 2020). Besides the previously
mentioned work on understanding GAN dynamics and generalization, we only mention the most
relevant papers here. One closely related line of work derives concrete bounds on when minimax
optimality of the GAN objective implies distribution learning (Bai et al., 2018; Liang, 2018; Singh
et al., 2018; Uppal et al., 2019; Chen et al., 2020a; Schreuder et al., 2021). However, the rates they
achieve scale poorly with the dimensionality of the data, and/or require strong assumptions on the
class of generators and discriminators, such as invertability. Another line of work has demonstrated
that first order methods can learn very simple GAN architectures in polynomial time (Feizi et al.,
2017; Daskalakis et al., 2017; Gidel et al., 2019; Lei et al., 2020). However, these results do not
cover many of the generators used in practice, such as ReLU networks with > 1 hidden layers.
Local PRGs and Learning PRGs have had a rich history of study in cryptography and complex-
ity theory (see e.g. (Vadhan, 2012)). From this literature, the object most relevant to the present
work is the notion of a local PRG. These are part of a broader research program of building constant
parallel-time cryptography (Applebaum et al., 2006). One popular local PRG candidate was sug-
gested in (Goldreich, 2011). By now there is compelling evidence that this candidate is a valid PRG,
as a rich family of algorithms including sum-of-squares (ODonnell & Witmer, 2014) and statistical
query algorithms (Feldman et al., 2018) provably cannot break it.
Finally, we remark that Goldreich’s PRG and, more generally, hardness of refuting random CSPs
have been used in a number of works showing hardness for various supervised learning problems
(Daniely & Vardi, 2021; Daniely et al., 2014; Daniely, 2016; Daniely & Shalev-Shwartz, 2016;
Applebaum et al., 2006; Applebaum & Raykov, 2016). We consider a very different setting, and our
techniques are very different from the aforementioned papers.
2	Technical Preliminaries
Notation Denote by Un the uniform distribution over {±1}n, by γn the standard n-dimensional
Gaussian measure, and by In the uniform measure on [0, 1]n. Given distribution D over Rm and
measurable function G : Rm → Rd, let G(D) denote the distribution over Rd given by the push-
3
Published as a conference paper at ICLR 2022
forward of D under G—to sample from the pushforward G(D),
Given σ : R → R and vector v ∈ Rd, denote by σ(v) ∈ Rd the
sample x from D and output G(x).
result of applying σ entrywise to v .
To avoid dealing with issues of real-valued computation, let Rτ ⊂ R be the set of multiples of 2-τ
bounded in magnitude by 2τ .
2.1	GANs and Pseudorandom Generators
In this section we review basic notions about generative models and PRGs.
Definition 1 (ReLU Networks). Let CL,S,d denote the family of ReLU networks F : Rd → R of
depth L and size S. Formally, F ∈ CL,S,d if there exist weight matrices W1 ∈ Rk1×d,W2 ∈
Rk2×k1 , . . . , WL ∈ R1×kL-1 and biases b1 ∈ Rk1 , b2 ∈ Rk2 . . . , bL ∈ R such that
F(x)，WLφ (Wl-iΦ (…Φ(Wιx + bi)…)+ bL-ι) + 2
and PiL=-11 ki = S, where φ(z) , max(0, z) is the ReLU activation. Additionally, we let CLτ,,ΛS,d
be the subset of such networks which are additionally Λ-Lipschitz and whose weight matrices and
biases have entries in Rτ一 we will refer to T as the bit complexity ofthe network.
The following allows us to control the complexity of compositions of ReLU networks:
Lemma 2.1. Let J : Rs → Rr be a function each of whose output coordinates is computed by some
network in CLj'S； S, and let f ∈ CL2^ 广 Then f ◦ J ∈ ClL七 S for τ = max(τι, τ2), Λ = Λ1Λ2√r,
L = L1 + L2, and S = (S1 + 1)r + S2. Furthermore, for the network in CLτ,,ΛS,s realizing f ◦ J, the
bias and weight vector entries in the output layer lie in Rτ2 .
Next, we formalize the probability metric we will work with.
Definition 2 (IPM). Given a family F of functions, define the F -integral probability metric between
two distributions p, q by Wf (p, q) = SuPf ∈f |Ey 〜p[f (y)] — Ey 〜q [f (y)]|. When F consists ofthe
family of 1-Lipschitz functions, this is the standard Wasserstein-1 metric, which we denote by W1.
In the context of GANs, we will focus on discriminators given by ReLU networks of polynomial
size, depth, Lipschitzness, and bit complexity:
Definition 3 (Discriminators). F* denotes the set OfaU SequenceS of discriminators fd : Rd → R,
indexed by d ∈ N, whose size, depth, Lipschitzness, bit complexity grow at most polynomially in d.
We now formalize the definition of GANs, which closely parallels the definition of PRGs.
Definition 4 (GANs/PRGs). Let : N → [0, 1] be an arbitrary function, and let {d(m)}m∈N be
some sequence of positive integers. Given a sequence of seed distributions {Dm}m over Rm, a
sequence of target distributions {Dd*(m)} over Rd(m), a family F of discriminators f : Rd(m) → R,
anda sequence of generators Gm : Rm → Rd(m), we say that {Gm} -fools F relative to {Dd*(m)}
with seed {Dm} iffor all sufficiently large m,
∣E[f (Gm(Dm))] — E[f (D*(m))] ∣ ≤ ^(m) ∀f ∈ F, f : Rd(m) → R.	(1)
Note that in the notation of Definition 2, (1) is equivalent to WF(Gm(Dm), Dd*(m)) ≤ (m).3 *
In this definition, if the discriminators and generators were instead Boolean functions, we would
refer to {Gm} as pseudorandom generators.
Remark 2.2. It will often be cumbersome to refer to sequences of target/seed distributions and
discriminators/generators as in Definitions 3 and 4, so occasionally we will refer to a single choice
of m and d even though we implicitly mean that m and d are parameters that increase towards
infinity. In this vein, we will often say that a single network f is in F*, though we really mean that
f belongs to a sequence of networks which lies in F* . And for distributions p, q which implicitly
belong to sequences {pd}, {qd}, When We refer to bounds on WF*(p, q) We really mean that for any
sequence of discriminators fd ∈ F*, |E[fd(pd)] — E[fd(qd)]| is bounded.
3Here We are slightly abusing notation as F contains functions Which have domain not equal to Rd(m), and
We ignore these functions in the supremum.
4
Published as a conference paper at ICLR 2022
2.2	Local Pseudorandom Generators
In complexity theory, the role of neural network discriminators is played by Boolean circuits, which
one can roughly think of as networks which take in Boolean strings as input and whose activations
are logical operations (e.g. AND/OR/NOT) rather than ReLUs (see Section A.2 in the supplement).
It is widely believed that there exist so-called local PRGs capable of fooling all polynomial-sized
Boolean circuits and all of whose output coordinates are functions of a constant number of input
coordinates (Applebaum et al., 2006). One prominent candidate is Goldreich’s PRG:
Definition 5 ((Goldreich, 2011)). Let H be a collection of d subsets S1, . . . , Sd of {1, . . . , m}, each
of size k and each sampled independently from the uniform distribution over subsets of {1, . . . , m}
of size k. Let P : {±1}k → {±1} be a Boolean function; P is often referred to as a predicate.
Let GP,H : {±1}m → {±1}d denote the Boolean function whose `-th output coordinate is com-
Puted by evaluating P on the coordinates of the input indexed by subset s` in H.
The following is a standard assumption in cryptography (see (Applebaum, 2016)), namely that the
incredibly simple functions in Definition 5 fool all Boolean circuits of polynomial size:
Assumption 1. For any c > 1, there exists k ∈ N and P : {±1}k → {±1} such that for m
sufficiently large, with probability 1 - om(1) over the randomness ofH in Definition 5, the function
G = GP,H negl(m)-fools all polynomial-size Boolean circuits relative to Umc with seed Um for
some negligible function negl : N → [0, 1].4
2.3	Diverse Distributions
Recall that the main result of this paper is to construct generators that look indistinguishable from
natural target distributions D* according to any poly-sized neural network, but which are far from
D* in Wasserstein. In this section We describe in greater detail the properties that these D* satisfy.
Definition 6. A distribution μ over Rd is (N,β )-diverse if for any discrete distribution V on Rd
supported on at most N points, Wι(μ, V) ≥ β.
The main fact we use about (N, β)-diverse distributions is that they cannot be approximated by
pushforwards of Ulog2 N (see Lemma A.12 in the supplement for a formal statement).
Note that Definition 6 is a very mild assumption that simply requires that the distribution not be
tightly concentrated around a few points. Distributions that satisfy Definition 6 are both practically
relevant and highly expressive. For starters, any reasonable real-world image distribution will be
diverse as it will not be concentrated around a few unique images. One can also show that distri-
butions like uniform distributions over well-separated discrete point sets and over [0, 1]d are diverse
(Lemma A.13 and Lemma A.15 in supplement). Definition 6 also captures random expansive neural
networks with leaky ReLU activations:
Lemma 2.3 (Random expansive leaky ReLU networks). For k0 , . . . , kL ∈ N satisfying ki ≥
1.1ki-1 for all i ∈ [L], let W1 ∈ Rk1 ×k0 , W2 ∈ Rk2 ×k1 , . . . , WL ∈ RkL ×kL-1 be ran-
dom weight matrices, where every entry of Wi is an independent draw from N (0, 1/ki). For
the function F : Rk0 → RkL given by F(X) ，WLψλ (Wl-ιψλ (…Ψλ(Wιx)…)),where
ψλ(z) = ψλ(z) = z/2 + (1/2 — λ)∣z∣ is the leaky ReLU activation, F(Ik°) is (2m, β)-diverse for
m = (ko∕2)log(ko∕2) — 瓦/1.1 一 1 and β = Θ(λ)L with probability at least 1 一 exp(-Ω(d)).
For example, if λ, L = Θ(1), then F(Ik°) is (23丽log k0), Ω(1))-diverse for ko sufficiently large.
It is easy to see that the networks in Lemma 2.3 can be implemented as ReLU networks, so our main
theorem applies to target distributions given by pushing the uniform distribution over the continuous
cube through a random expansive leaky ReLU network of constant depth.
3	Fooling ReLU Network Dis criminators Does Not S uffice
In this section we will show that even though a generative model looks indistinguishable from some
target distribution D* according to any ReLU network in F*, it can be quite far from D* in Wasser-
4g : N → R≥0 is negligible if for every polynomial p, g(n) < |1/p(n)| for all sufficiently large n.
5
Published as a conference paper at ICLR 2022
stein. We begin by describing a simple version of this result over discrete domains in Section 3.1.
In Section 3.2 we extend this to target distributions over continuous domains, but where the gen-
erator still takes in a discrete-valued seed. Finally, in Section 3.3 we give a simple reduction that
extends these results to give generators that take in a continuous-valued random (Gaussian) seed,
culminating in the following main result:
Theorem 3.1. Let {Hm}m be a sequence of generators Hm : Rr(m) → Rd(m) for
r(m), d(m) ≤ poly(m) whose output coordinates are computable by networks in CLτ 0((mm)),,ΛS0((mm)),r(m)
for τ0(m), Λ0(m) ≤ poly(m). Suppose that Hm(Irg) is (2m, Ω(1))-diverse.
Fix any : N → [0, 1] satisfying (m) ≥ max(negl(m), exp(-O(m)). Under Assumption 1, there
is a sequence of generators Gm : Rm → Rd(m) such that for all m sufficiently large:
1	. Every output coordinate ofGm is computable by a network in CLτ,,ΛS,m for
T = max(O(log(Λ0(m) ∙ m ∙ d(m)/e(m))), τ0(m), O(1)),	Λ = O(Λ0(m)2poly(m)∕e(m)),
L = L0(m) + O(1), S = O(r(m) log(1/(m))) + 3m + S0(m).
2	Wf * (Gm (Ym),Hm(Ir(m) )) ≤ f(m)∙pθly(m),	3. Wl(Gm(Ym),Hm(Ir(m))) ≥ Ω(1).
Note that a natural choice of parameters for Hm would be
τ0 (m) ≤ O(log m), Λ0(m), S0(m), 1/(m) ≤ poly(m), L0(m) ≤ O(1).
(In fact, the poly(m) factor in bullet point 2 simply comes from the Lipschitzness of Hm, so (m)
only needs to scale inversely in this quantity for WF* to be small.) Altogether, we conclude that
Gm’s output coordinates are computable by constant-depth ReLU networks with polynomial size
and Lipschitzness and logarithmic bit complexity τ .
3.1	Stretching Bits to Bits
As a warmup, in this subsection we prove the following special case of Theorem 3.1 when the target
distribution and seed distribution are discrete.
Theorem 3.2. Under Assumption 1, for any constant c > 1, there is a sequence of generators
Gm : Rm → Rd(m) for d(m) ≥ mc such that for all m,
1	. Every output coordinate ofGm is computable by a network in CLτ,,ΛS,m for τ, Λ, L, S = Oc(1).
2	Wf * (Gm (Um ),Ud(m)) ≤ negl(m),	3. Wl(Gm(Um),Ud^(m)) ≥ Ω(1).
We emphasize that in this discrete setting, our quantitative guarantees are even stronger: all parame-
ters τ, Λ, L, S of the generator are constant, and no polynomial-sized ReLU network can distinguish
between Gm (Um) and Ud(m) with even non-negligible advantage.
As discussed in the introduction, a basic but important building block in the proof of Theorem 3.2
is the connection between Goldreich’s PRG and generative models computed by neural networks of
constant depth/size/Lipschitzness. We begin by elaborating on this connection and showing that any
predicate {±1}k → {±1} can be implemented as a network in CLτ,,ΛS,d where τ, Λ, L, S = Ok(1).
As a consequence of known constructions for implementing Boolean functions with ReLU networks
(see e.g. Lemma A.2 of (Chen et al., 2020b)), we can show:
Corollary 3.3. For any k ∈ N and any H and P as in Definition 5, the coordinates of the output of
GP,H are computed by networks in CLτ,,ΛS,m for τ = O(k), Λ = exp(O(k)), L = k, S = O(2kk).
Before we use this to prove Theorem 3.2, we need an extra technical ingredient to formalize the
fact that a discriminator given by a ReLU network of polynomially bounded complexity yields a
discriminator computable by a polynomial-sized Boolean circuit. The idea is that if WF* is large
so that there exists some ReLU network discriminator, then because the input to the discriminator is
sufficiently well concentrated, some affine threshold of the ReLU network can distinguish between
the two distributions:
6
Published as a conference paper at ICLR 2022
Lemma 3.4. Given independent X and Y such that E[Y] -E[X] = α and for which X -E[X] and
Y — E[Y] are σ2-sub-Gaussian, there exists a threshold t ∈ [E[X] — O(σ√log(σ∕∣0∣)), E[Y] +
O(σpiog(σ∕∣ɑ∣))] for which ∣ PX > t] - P[Y > t]∣ ≥ min(1/2, Ω(∣α∣∕σ)).
We can now sketch the proof of Theorem 3.2, deferring a formal argument to the supplement.
Proof sketch of Theorem 3.2. The parameter m will be clear from context in the following discus-
sion, so for convenience We will refer to d(m) and Gm as d and G. Let k, P, G, negl(∙) be such that
the outcome of Assumption 1 holds. By Corollary 3.3, every output coordinate of G is computable
by a network in CLτ,,ΛS,m for τ = O(k), Λ = exp(O(k)), L = k, S = O(2kk).
The fact that W1(G(Um), Ud) > 1/3 follows from the fact that G(Um) is uniform over 2m points
on the hypercube (with multiplicity), and Ud is (2m, 1/3)-diverse.
It remains to check that G fools F * relative to Ud. Suppose to the contrary that there exists some
f : Rd → R in F * for which ∣E[f (G(Um))] - E[f (Ud)]∣ > d-a for some constant a > 0. Note
that for any threshold t ∈ Rτ, there is a Turing machine Mτ : {±1}d → {±1} that computes y 7→
sgn(f (y) - t) using τ bits of advice. By applying Lemma 3.4 to random variables X = f (G(Um))
and Y = f(Ud), we get that there exists a threshold t ∈ Rpoly(d) for which ∣E[Mτ(G(Um))] -
E[Mτ(Ud)]∣ > 1∕poly(d), contradicting Assumption 1.	□
3.2	From Binary Outputs to Continuous Outputs
In this section we show how to extend Theorem 3.2 to the setting where the target distribution D* is a
pushforward of the uniform distribution on [0, 1]r. At a high level, the idea will be to post-process the
output of the generator constructed in Theorem 3.2. Roughly speaking, we take weighted averages
of clusters of output coordinates from the generator in Theorem 3.2 and pass these averages through
the pushforward map defining D*. Formally, we show:
Theorem 3.5. Let {Hm}m be a sequence of generators Hm : Rr(m) → Rd(m) for
r(m), d(m) ≤ poly(m) whose output coordinates are computable by networks in CLτ 0((mm)),,ΛS0((mm)),r(m)
for τ0(m), Λ0(m) ≤ poly(m). Suppose that Hm(Ir(m)) is (2m, Ω(1))-diverse.
Fix any : N → [0, 1] satisfying (m) ≥ max(negl(m), exp(-O(m)). Under Assumption 1, there
is a sequence of generators Gm : Rm → Rd(m) such that for all m sufficiently large:
1.	Every output coordinate ofGm is computable by a network in CLτ,,ΛS,m for
T = max(O(log(1∕e(m))), τ0(m), O(1)),	Λ = O(Λ0(m)) ∙ poly(m),
L = L(Im) + O(1),	S = O(r(m) ∙ log(1∕e(m))) + S0(m).
2.	Wf * (Gm (Um ),Hm(Ir(m))) ≤ t(m) ∙ poly(m),	3. WI(Gm(Um),Hm(Ir(m))) ≥ Ω(1).
Theorem 3.5 retains many of the nice properties of Theorem 3.2, e.g. it can tolerate distinguishing
advantage (m) which is negligible, at the mild cost ofan extra logarithmic dependence on 1∕(m)
in the bit complexity τ. And as with Theorem 3.1, if the networks Hm are of constant depth, the
resulting generators Gm are also of constant depth.
To prove Theorem 3.5, we begin by showing that for any pair of distributions which are close under
the WF metric for some family of neural networks F, their pushforwards under a simple generative
model will still be close under the WF0 metric for some slightly weaker family of networks F0 .
Lemma 3.6. Fix parameters Λ, Λ0 > 1. Let F = CLτ,,ΛS,s. IfWF(p, q) ≤ for some distributions
p, q on Rs, then for any J : Rs → Rr each of whose output coordinates is computed by a function
in CLOASOs for some L0 < L and S0 ≤ S-r-1, we have Wfo(J(P), J(q)) ≤ 2eΛ0√r for F0 =
CT-LO Soo r where T0 = T —「log? Λ√r~∖ and S00 = S — r(S0 + 1).
Now recall that in Theorem 3.2, we exhibited a GAN which is close in WF* to Us . Using
Lemma 3.6, we can show that a certain simple pushforward of this GAN will be close in WF* to the
uniform distribution over [0, 1]r for r slightly smaller than s. The starting point is the following:
7
Published as a conference paper at ICLR 2022
Fact 3.7. For any 0 <	< 1 and n ≥ log2(1/), let h : Rn → R be given by h(x) = hw, x + 1i
for W =(1/4,1/8,..., (1∕2)n+1) and 1 the all-1,s vector. Then Wi (h(Un),Iι) ≤ e
By leveraging Lemma 3.6 and Fact 3.7, we can get an approximation to the uniform distribution
over [0, 1]r out of the uniform distribution over {±1}s:
Lemma 3.8. Suppose > 0 satisfies log(1/) ≤ poly(s). If a distribution D over Rs satisfies
Wf*(D,Us) ≤ e,thenforr，s∕dlog(1∕c)] ,5 there is afunction J: Rs → Rr each ofwhose output
Coordinates is computed by afunction in cOO^oga/"。⑴ Such that Wf * (J (D), Ir) ≤ e ∙ poly(r).
By further combining Lemma 3.6 and Lemma 3.8, we can thus extend the latter from the uniform
distribution on [0, 1]r to simple pushforwards thereof.
Lemma 3.9. Under the hypotheses of Lemma 3.8, for any d ≤ poly(s) and any function H :
Rr → Rd each of whose output coordinates is computed by a function in CLτ 0,,ΛS0,r for τ0 ≤ poly(s),
there is a function J0 : Rs → Rd each of whose output coordinates is computed by a function in
cm+?+黑∕e"τ0)'O(AO√d) such that Wf* (J0(D), H(Ir)) ≤ eΛ0 ∙ Poly(S).
So for any pushforward H(Ir) of the uniform distribution on [0, 1]r, Lemma 3.9 lets us take the
GAN given by Theorem 3.2 and slightly post-process its output so that it is close in WF* to H(Ir).
We can now sketch Theorem 3.5’s proof, deferring the full argument to the supplement.
Proof of Theorem 3.5. Condition 3 follows because G(Um) is a uniform distribution on 2m points
(w/ multiplicity) and H (Ir) is (2m, Ω(1))-diverse. Next, let S = r ∙ dlog(1∕e)e. As We are as-
suming E ≥ exp(-O(m)), s ≤ r ∙ m = mc for some constant c > 1. Take G0 to be the gen-
erator G : Rm → Rs constructed in Theorem 3.2. By applying Lemma 3.9 to D = G0(Um),
We get a function J0 : Rs → Rd each of whose output coordinates is computed by a function in
¢£0+(0+^1/,)"0),O(AO√d) such that WFd (J0(G0(Um)), H(Ir)) ≤ W ∙ poly(m) ≤ 〜poly(m),
so we get condition 2 of the theorem for G , J0 ◦ G0 . Finally, by Lemma 2.1, every output
coordinate of G can be realized by a network in cLτ,,AS,m for τ = max(O(log(1∕E)), τ0, O(1)),
Λ = O(Λ0√dS) = O(Λ0) ∙ poly(m), L = L0 + O(1), and S = O(S) + r + S0 = O(S) + S0 (where
we used the fact that r = s∕dlog(1∕e)e < s). This establishes condition 1 of the theorem. □
3.3	From Binary Inputs to Continuous Inputs
In Theorem 3.5, we have shown how to go from Um to any simple pushforward Hm(Ir(m)) of the
uniform distribution over [0, 1]r(m). Here we complete the proof of our main result, Theorem 3.1,
by giving a simple reduction showing how to use Gaussian seed γm instead of Um . At a high level,
the idea will be to pre-process the inputs to the generator constructed in Theorem 3.5 by appending
appropriate activations at the input layer. We will need the following elementary construction.
Lemma 3.10. For any ξfor which 1∕ξ ∈ Rτ, the piecewise linear function hξ which outputs -1 for
inputs X ≤ —ξ, 1 for X ≥ ξ, and x∕ξ otherwise, lies in。£1《.
The function hξ will let us approximately convert from γm to Um . Specifically, the following
says that if we want to approximate the output distribution of the generator in Theorem 3.5 using
Gaussians instead of bits as seed, it suffices to attach entrywise applications of hξ at the input layer:
Lemma 3.11. Let G0 : Rm → Rd have output coordinates computable by networks in cLτ 0000,,AS0000,m00.
Forany e > 0, let ξ0，e∕ (Λ00P(2∕π)m3d) and let ξ bethe multiplicative inverse of d1∕ξ0].
The function G : Rm → Rd given by G(X) = G0 (hξ (X)), where hξ (X) denotes entrywise applica-
tion ofhξ defined in Lemma 3.10, satisfies that 1) each of the output coordinates ofG is computable
by a network in CLSmL for the parameters τ = max(τ”, O(log(Λ00md∕E))), Λ = O(Λ002 m2√d∕e),
L = L00 + 2, and S = 3m + S00, and 2) W1(G0(Um), G(γm)) ≤ E.
5We will assume for simplicity that this is an integer, though it is not hard to handle the case where
dlog(1/)e does not divide s.
8
Published as a conference paper at ICLR 2022
Remark 3.12. Note the only fact we use about γm in the proof of Lemma 3.11 is that outside of an
event with probability O(mξ), hξ(γm) is uniform over {±1}m. In particular, the same would hold
for any product measure each of whose coordinates is symmetric and anticoncentrated around zero.
For instance, up to a constant factor in ξ0, Lemma 3.11 also holds with γm replaced by Im.
Proof of Theorem 3.1. Substitute the generator constructed in Theorem 3.5, call it G0, into
Lemma 3.11; let G be the generator resulting from the lemma. Recall from Theorem 3.5 that
each output coordinate of G0 lies in CLτ00,,ΛS00,m for τ00 = max(O(log(1/)), τ0, O(1)), Λ00 =
O(Λ0poly(m)), L00 = L0 + O(1), S00 = O(r log(1/)) + S0. So by Lemma 3.11, every out-
put coordinate of G is computable by a network in CLτ,,ΛS,m for τ = max(τ 00, O(log(Λ0md/))) =
max(O(log(Λmd∕e)), τ0, O(1)), A = O(Λ002 m2√d∕e) = O(Λ02poly(m)∕e), L = L0 + 2 =
L0 + O(1), and S = 3m + S00 = O(r log(1∕e)) + 3m + SJ	□
4 Experimental Results
To empirically demonstrate the existence of a constant depth generator that can fool polynomially-
bounded discriminators, we evaluated the generator G given by Goldreich’s PRG (Goldreich, 2011)
with input dimension m = 50, output dimension d = 200, and predicate P : {±1}5 → {±1} given
by the popular TSA predicate, namely P(xi,..., χ5) = xi ∙ χ2 ∙ χ3 ∙ (χ4 ∧ χ5). This is the smallest
predicate under which Goldreich’s candidate construction is believed to be secure.
The target distribution D* is the uniform distribution U200 over {±1}200. As we prove in
Lemma A.13, Ud is sufficiently diverse that Wι(G(Um), Ud) ≥ Ω(1). We trained four differ-
ent discriminators given respectively by 1, 2, 3, 4 hidden-layer ReLU networks, where each hid-
den layer is fully connected with dimensions 200 × 200, to discriminate the output of the gener-
ator G(Um) from the target distribution Ud. We used the Adam optimizer with step size 0.001
over the DCGAN training objective, with batch-size 128. As we can see in Figure 1, the test loss
E[- log(D(X))] +E[- log(1 - D(G(z)))] - 2 log(2) stays consistently above zero, indicating that
the discriminator can not discriminate the true distribution from the generator output, even though
the Wasserstein distance between these two distributions is provably large.
Figure 1: Test loss (E[- log(D(X))] +E[- log(1 - D(G(z)))] - 2 log(2)) over course of training. Discrim-
inators cannot distinguish generator output from true distribution, though Wasserstein provably large.
5 Conclusions
In light of the obstructions presented in this paper, what are natural next steps for the theory of
GANs? Here, we offer a couple of thoughts and possible future directions.
One limitation of our lower bound is that it holds for a specific generator. Of course, it is quite
unlikely that we will ever encounter such a generator through natural GAN training. One way to
circumvent our lower bound is to argue that the training dynamics of the generator may have some
regularization effect which allows us to avoid these troublesome generators, and which allows GANs
to learn distributions in polynomial time.
Another orthogonal perspective is that our results suggest that perhaps statistical learning is too
strong ofa goal. If our GAN is indeed indistinguishable from the target distribution to all polynomial
time algorithms, then not only should the output of the GAN be sufficient for humans, but it should
also be sufficient for all downstream applications, which presumably run in polynomial time. This
raises the intriguing possibility that the correct metric for measuring closeness between distances in
the context of GANs should inherently involve some computational component (e.g. in the sense of
Dwork et al. (2021)) as opposed to the purely statistical metrics generally considered in the literature.
9
Published as a conference paper at ICLR 2022
Acknowledgments The authors would like to thank Boaz Barak, Adam Klivans, and Alex Lom-
bardi for enlightening discussions about local PRGs. SC was supported by NSF Award 2103300.
This work was done while the first, second, and fourth authors were visiting the Simons Institute for
the Theory of Computing.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical
generative models for real-world distributions. arXiv preprint arXiv:2106.02619, 2021.
Benny Applebaum. Cryptographic hardness of random local functions. Computational complexity,
25(3):667-722, 2016.
Benny Applebaum and Pavel Raykov. Fast pseudorandom functions based on expander graphs. In
Theory of Cryptography Conference, pp. 27-56. Springer, 2016.
Benny Applebaum, Yuval Ishai, and Eyal Kushilevitz. Cryptography in NC0 . SIAM Journal on
Computing, 36(4):845-888, 2006.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Uni-
versity Press, 2009.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224-
232. PMLR, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and
empirics. In International Conference on Learning Representations, 2018.
Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in
gans. In International Conference on Learning Representations, 2018.
Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A
closer look at the optimization landscapes of generative adversarial networks. arXiv preprint
arXiv:1906.04848, 2019.
Lijie Chen and Roei Tell. Bootstrapping results for threshold circuits “just beyond” known lower
bounds. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
pp. 34-41, 2019.
Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Statistical guarantees of generative
adversarial networks for distribution estimation. arXiv preprint arXiv:2002.03938, 2020a.
Ruiwen Chen, Rahul Santhanam, and Srikanth Srinivasan. Average-Case Lower Bounds and
Satisfiability Algorithms for Small Threshold Circuits. In Ran Raz (ed.), 31st Conference
on Computational Complexity (CCC 2016), volume 50 of Leibniz International Proceedings
in Informatics (LIPIcs), pp. 1:1-1:35, Dagstuhl, Germany, 2016. Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik. ISBN 978-3-95977-008-8. doi: 10.4230/LIPIcs.CCC.2016.1. URL
http://drops.dagstuhl.de/opus/volltexte/2016/5844.
Sitan Chen, Adam R Klivans, and Raghu Meka. Learning deep relu networks is fixed-parameter
tractable. arXiv preprint arXiv:2009.13512, 2020b.
Amit Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings of the forty-
eighth annual ACM symposium on Theory of Computing, pp. 105-117, 2016.
Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf’s. In
Conference on Learning Theory, pp. 815-830. PMLR, 2016.
Amit Daniely and Gal Vardi. From local pseudorandom generators to hardness of learning. arXiv
preprint arXiv:2101.08303, 2021.
10
Published as a conference paper at ICLR 2022
Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper
learning complexity. In Proceedings of the forty-sixth annual ACM symposium on Theory of
computing, pp. 441-448, 2014.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. arXiv preprint arXiv:1711.00141, 2017.
Cynthia Dwork, Michael P Kim, Omer Reingold, Guy N Rothblum, and Gal Yona. Outcome in-
distinguishability. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of
Computing, pp. 1095-1108, 2021.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans: the lqg setting. arXiv
preprint arXiv:1710.10793, 2017.
Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability
problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018.
GaUthier GideL Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le PrioL Gabriel Huang, Si-
mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1802-1811.
PMLR, 2019.
Mikael Goldmann and Marek Karpinski. Simulating threshold circuits by majority circuits. SIAM
Journal on Computing, 27(1):230-246, 1998.
Mikael Goldmann, Johan Hastad, and Alexander Razborov. Majority gates vs. general weighted
threshold gates. Computational Complexity, 2(4):277-300, 1992.
Oded Goldreich. Candidate one-way functions based on expander graphs. In Studies in Complexity
and Cryptography. Miscellanea on the Interplay between Randomness and Computation, pp. 76-
87. Springer, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM, 63(11):139-144, 2020.
Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative adver-
sarial networks: Algorithms, theory, and applications. arXiv preprint arXiv:2001.06937, 2020.
Russell Impagliazzo, Ramamohan Paturi, and Michael E Saks. Size-depth tradeoffs for threshold
circuits. SIAM Journal on Computing, 26(3):693-707, 1997.
Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in
wgans. In International Conference on Machine Learning, pp. 5799-5808. PMLR, 2020.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of first-order
approximation in gan dynamics. In International Conference on Machine Learning, pp. 3005-
3013. PMLR, 2018.
Tengyuan Liang. How well generative adversarial networks learn distributions. arXiv preprint
arXiv:1811.03179, 2018.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence proper-
ties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017.
Ester Mariucci and Markus Reiβ. Wasserstein and total variation distance between marginals of levy
processes. Electronic Journal of Statistics, 12(2):2482-2514, 2018.
Ryan ODonnell and David Witmer. Goldreich’s prg: evidence for near-optimal polynomial stretch.
In 2014 IEEE 29th Conference on Computational Complexity (CCC), pp. 1-12. IEEE, 2014.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why
and when can deep-but not shallow-networks avoid the curse of dimensionality: a review. Inter-
national Journal of Automation and Computing, 14(5):503-519, 2017.
11
Published as a conference paper at ICLR 2022
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
MathematicalSciences, 62(12):1707-1739, 2009.
Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak Dalalyan. Statistical guarantees for gen-
erative models without domination. In Algorithmic Learning Theory, pp. 1051-1071. PMLR,
2021.
Shashank Singh, Ananya UPPaL BoyUe Li, Chun-Liang Li, Manzil Zaheer, and Barnabas Poczos.
Nonparametric density estimation under adversarial losses. In NeurIPS, 2018.
Michael SiPser. Introduction to the theory of comPutation. ACM Sigact News, 27(1):27-29, 1996.
Hoang Thanh-Tung and Truyen Tran. CatastroPhic forgetting and mode collaPse in gans. In 2020
International Joint Conference on Neural Networks (IJCNN), PP. 1-10. IEEE, 2020.
Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. ImProving generalization and stability of
generative adversarial networks. arXiv preprint arXiv:1902.03984, 2019.
Ananya UPPal, Shashank Singh, and Barnabas Poczos. NonParametric density estimation & conver-
gence rates for gans under besov iPm losses. Advances in Neural Information Processing Systems,
32:9089-9100, 2019.
Salil P Vadhan. Pseudorandomness, volume 7. Now Delft, 2012.
Emanuele Viola. The sum of d small-bias generators fools Polynomials of degree d. Computational
Complexity, 18(2):209-217, 2009.
Ingo Wegener. The complexity of Boolean functions. John Wiley & Sons, Inc., 1987.
Maciej Wiatrak, Stefano V Albrecht, and Andrew Nystrom. Stabilizing generative adversarial net-
works: A survey. arXiv preprint arXiv:1910.00927, 2019.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017.
12
Published as a conference paper at ICLR 2022
Roadmap for Supplement In Section A we provide some additional technical preliminaries, in-
cluding background on Boolean circuits. In Section B we give the deferred proofs from Section 3 for
our main result, Theorem 3.1. In Section C.1 we give background on circuit lower bounds for TC0,
and in Section C we prove Theorem C.2 which effectively says that if one could exhibit genera-
tors with even logarithmic stretch which can fool constant-depth, sufficiently superlinear-size ReLU
networks, then this would imply breakthrough circuit lower bounds.
A Additional Technical Preliminaries
More Notation and Miscellaneous Tools Let 1n denote the all-ones vector in n dimensions;
when n is clear from context, we denote this by 1.
Given a vector v ∈ Rd, we let kv k denote its Euclidean norm. Given r > 0, let B(v, r) ⊂ Rd denote
the Euclidean ball of radius r with center v. Given a matrix W, we let kWk denote its operator
norm. Let σmin (W) denote its minimum singular value.
Given a distribution p, let p0n denote the product measure given by drawing n independent samples
from p.
1	x≥0
Define the function sgn(x) ,	.
-1 x < 0
Let φ : R → R denote the ReLU activation φ(z) , max(0, z). Let ψλ : R → R denote the leaky
ReLU activation ψλ(z) = z∕2+(1∕2 - λ)∣z∣. Note that
ψλ(z) = (1 - λ)φ(z) - λφ(-z).
We will need the following well-known result:
Theorem A.1 (Kirszbraun extension). Given an arbitrary subset S ⊂ Rd and f : S → R which is
L-Lipschitz, there exists an L-Lipschitz extension f : Rd → R for which f(y) = f(y) for all y ∈ S.
We will need the following basic fact about composing Lipschitz functions:
Fact A.2. Ifg1,. . ., gr : Rd → R are Λ-Lipschitz and h : Rr → R is Λ0 -Lipschitz, then the function
x 7→ h(g1(x), . . . ,gr(x))
is ΛΛ√r-Lipschitz.
1/2
Proof. Foranyx,χ0, we have |gi(x) - gi(χ0)∣ ≤ Λ∣∣x - χ0∣∣, so (Ei=ι(gi(χ) - gi(χ0))2)	≤
Λ√rkx — x0k. This implies that ∣h(gι(x),..., gr(x)) — h(gι(x0),..., gr (x0))∣ ≤ ΛΛ0√rkx — x0k
as desired.
Fact A.3. The volume of a d -dimensional Euclidean ball of radius 1 is at most (18∕d)d/2.
ICd (n/2)bd/2C
Proof. It is a standard fact that the volume of the ball can be expressed as 2d ∙ ( / 京-.If d is
even, then d!! = 2d/2 ∙ (d∕2)! ≥ 2d/2 ∙ e (∙d)d/2 ≥ (d∕e)d/2. If d is odd, then d!!=[d处!；/% ≥
e(d/2d)力2∙2d∕2 = (d∕e)d/2. We conclude that the volume is at most (2πe∕d)d/2 ≤ (18∕d)d/2. □
A. 1 Concentration of Measure
We will use the following consequence of McDiarmid’s inequality:
Lemma A.4 (McDiarmid’s Inequality). Suppose F : {±1}n → {±1} is such that for any x, x0 ∈
{±1}n differing on exactly one coordinate, |F (x) - F(x0)| ≤ c. Then
P	[|f (x) - E[f (x)]| > s] ≤ exp
X 〜{±1}n
13
Published as a conference paper at ICLR 2022
Corollary A.5. Given F : {±1}n → {±1} which is Λ-Lipschitz, define the random variable
X，F(Un). Then X 一 E[X] is Λ√2n-Sub-Gaussian.
Proof. Because F is Lipschitz, it satisfies the hypothesis of Lemma A.4 with c = Λ, so the corollary
follows by the definition of SUb-GaUssianity.	□
Theorem A.6 (Theorem 1.1, (Rudelson & Vershynin, 2009)). For n, d ∈ N with n ≥ d, let W ∈
Rn×d be a random matrix whose entries are independent draws from N (0, 1). Then for every > 0,
p[σmin(W) ≤ 人√ 一 √d-1)i ≤ (Ce)n-d+1 + e-cn
for absolute constants C, c > 0.
A.2 B o olean Circuits
In the context of pseUdorandom generators, the set of all polynomial-sized Boolean circUits is the
canonical family of discriminator fUnctions to consider when formalizing what it means for a gen-
erator to fool all polynomial-time algorithms.
Here we review some basics aboUt Boolean circUits; for a more thoroUgh introdUction to these
concepts, we refer the reader to any of the standard textbooks on complexity theory, e.g. (Arora &
Barak, 2009; Sipser, 1996).
Definition 7 (Boolean circUits). Fix a set G of logical gates, e.g. ∧, ∨, . A Boolean circuit C
is a Boolean function {±1}n → {±1} given by a directed acyclic graph with n input nodes with
in-degree zero and an output node with out-degree zero, where each node that isn’t an input node is
labeled by some logical gate in G. Unless otherwise specified, we will take G to be {∧, ∨, }.
The size S of the circuit is the number of nodes in the graph, and the depth D is given by the length
of the longest directed path in the graph. The value of C on input x ∈ {±1}n is defined in an
inductive fashion: the value at a node v in the graph is defined to be the evaluation of the gate at
v on the in-neighbors of v (as the graph is acyclic, this is well-defined), and the value of C on x is
then the value of the output node.
We will occasionally also be interested in the number W of wires in the circuit, i.e. the number of
edges in the graph. Note that trivially
S ≤ W +1.	(2)
Definition 8 (P/poly). Given T : N → N, let SIZE(T (n)) denote the family of sequences of Boolean
functions {fn : {±1}n → {±1}} for which there exist Boolean circuits {Cn} with sizes {Sn} that
compute {fn} and such that Sn ≤ T (n).
Let P/poly , c>1 SIZE(nc). We refer to (sequences of) functions in P/poly as fUnctions com-
pUtable by polynomial-sized circUits.
The following standard fact aboUt boUnded-depth Boolean circUits will make it convenient to trans-
late between them and neUral networks.
Lemma A.7 (See Theorem 1.1 in Section 12.1 of (Wegener, 1987)). For any Boolean circuit C of
size S and depth D with gate set G, there is another circuit C0 of size D ∙ S and depth D with gate
set G which computes the same function as C but with the additional property that for any gate in
C0, all paths from an input to the gate are of the same length.
The Upshot of Lemma A.7 is that for any length `, we can think of the gates of C0 at distance ` from
the inpUts as comprising a “layer” in the circUit.
A less combinatorial way of formUlating the complexity class captUred by polynomial-sized circUits
is in terms of TUring machines with advice strings.
Fact A.8 (See e.g. Theorem 6.11 in (Arora & Barak, 2009)). A sequence of Boolean functions
{fn : {±1}n → {±1}} is in P/poly if and only if there exists a sequence of advice strings {αn },
where αn ∈ {±1}n for an ≤ poly(n), and a Turing machine M which runs for at most poly(n)
steps and, for any n ∈ N, takes as input any x ∈ {±1}n and the advice string αn and outputs
M (x, αn) = fn (x).
14
Published as a conference paper at ICLR 2022
This fact will be useful for translating discriminators computed by neural networks into discrimina-
tors given by polynomial-sized Boolean circuits.
A.3 More on GANs and PRGs
In this section we fill in some additional details regarding the contents of Section 2.1. We begin with
a simple remark about Definition 1.
Remark A.9. In Definition 1, if L = 1, then S = 0 and the definition specializes to linear functions.
That is, C1τ,,0Λ,d is simply the class of affine linear functions F (x) = hw, xi + b for w ∈ Rτd and
b ∈ Rτ satisfying kwk ≤ Λ.
Next, we fill in the proof of Lemma 2.1, which we restate below for the reader’s convenience.
Lemma A.10. Let J : Rs → Rr be a function each of whose output coordinates is computed
by some network in CLτ1,,ΛS1 ,s, and let f ∈ CLτ2,,ΛS2 ,r. Then f ◦ J ∈ CLτ,,ΛS,s for τ = max(τ1 , τ2),
Λ = Λ1Λ2√r, L = Li + L2, and S = (Si + 1)r + S?. Furthermore, for the network in CfS S
realizing f ◦ J, the bias and weight vector entries in the output layer lie in Rτ2.
Proof. Suppose that the i-th output coordinate of J is computed by a neural network with weight
matrices Wi(i) ∈ Rk1(i)×s,. . . , W(Li) ∈ Ri×k(Li1)-1 and biases b(ii) ∈ Rk1(i),. . . , b(Li) ∈ R.
Define the (Pir=i ki(i)) × s weight matrix Wi by vertically concatenating the weight matrices
Wi(i) , . . . , Wi(r). For every 1 < j < Li define the (Pir=i kj(i)) × (Pir=i kj(i-) i) weight ma-
trix Wj by diagonally concatenating the weight matrices Wj(i) , . . . , Wj(r). Similarly, define the
r × (Pir=i kL(i) ) matrix WL1 by diagonally concatening the column vectors WL(i) , . . . , WL(r) . For
the bias vectors in these layers, for every 1 ≤ j ≤ Li define bj to be the vector given by concate-
nating b(ji), . . . , b(jr).
Now suppose that f is computed by a neural network with weight matrices WL1+i ∈ RkL1+1×r,
. . . , WL1+L2 ∈ Ri×kL1+L2-1 and biases bL1+i ∈ RkL1+1 , . . . , bL1+L2 ∈ R. Then by design, for
any y ∈ Rs we have
f (J (J))= W L1 + L2 φ(W Lι+L2-1φ(∙ ∙ ∙ φ(WIy + bI)…) + bL1 + L2-I) + bL1+L2 .
This network has depth Li + L2 and size
(LI-I r . ∖	L1+L2
X X kji) + r + X kj = r ∙ Si + r + S? = S.
j=i i=i	j=L1+i
The bit complexity of the entries of the weight matrices and biases are obviously bounded by
max(τι, τ2), and the LiPSchitzneSS of the network is bounded by Λ1Λ2√r by Fact A.2.	□
Finally, we will also use the following standard tensorization property of Wasserstein distance later:
Fact A.11 (See e.g. Lemma 3 in (MariUcci & Reiβ, 2018)). If p,q satisfy Wi(p, q) ≤ G then
Wi(p0n,q0n) ≤ e√n.
A.4 Diverse Distributions: Missing Proofs
Here we fill in some missing details from Section 2.3. We first show that diverse distributions cannot
be approximated by pushforwards of Um ifm is insufficiently large. This follows immediately from
the definition of diversity:
Lemma A.12. For any 0 < β < 1, if D* is a (2m, β)-diverse distribution over Rd, then for any
function G : {±1}m → Rd, Wι(G(Um), D*) ≥ β.
Proof. G(Um) is a uniform distribution on 2m points, with multiplicity if there are multiple points in
{±1}m that map to the same point in Rd under G, so the claim follows by definition of diversity. □
15
Published as a conference paper at ICLR 2022
Below we give some simple examples of diverse distributions.
Lemma A.13 (Discrete, well-separated distributions). For any α > 0 and any N, N0 ∈ N satisfying
N ≤ N0. Let Ω ⊆ Rd be a Setofpointssuch thatfor any z,z0 ∈ Ω, ∣∣z — z0k ≥ α. Then the uniform
distribution μ on any N 0 points from Ω is (N, β)-diverse for β = α(1 — N/N0).
Proof. Take any discrete distribution ν supported on at most N points y1, . . . , yN in Rd. Consider
the function f : Rd → R: for any y in the support of ν, let f(y) = 0, and for any y not in the
support of V, let f (y) = 1. As a function from Ω to R, where Ω inherits the Euclidean metric, f is
d
clearly 1 /α-Lipschitz over Ω. By Theorem A.1, there exists a 1 /α-Lipschitz extension f : Rd → R
of f, and we have
∣E[f(μ)] — E[f (V )]| = ∣E[f(μ)]∣≥ 1 — N/N0,
so Wι(μ, v) ≥ 1 — N/N0 as desired.	□
We now turn to examples of continuous distributions which are diverse. We first observe that a
distribution is (N, β)-diverse ifit satisfies certain small-ball probability bounds.
Definition 9. For a distribution D over Rd, define the Levy concentration function QD(r)，
supχ0∈Rd Px〜D[kx - x0k ≤ r].
Lemma A.14. Ifa distribution D over Rd satisfies QD (r) ≤ α, then D is (N, r(1 — N α))-diverse.
Proof. Take any N points z1 , . . . , zN ∈ Rd. By the bound on QD (r), the union S of the
balls of radius r around these points has Lebesgue measure at most Nα. Define the function
f : {z1, . . . , zN} ∪ (Rd\S) → {0, 1} to be zero on {z1, . . . , zN} and one on Rd\S. This func-
tion is 1/r-Lipschitz on its domain, so by Theorem A.1 there is an extension f0 : Rd → R of f
which remains 1/r-Lipschitz on its domain. Define the function f *(x)，|f (x)|. Note that for μ
the uniform distribution on {z1, . . . , zN},
|E[f(μ)] — E[f(D)]| = |E[f(D)]| ≥ 1 — Nα,
so we conclude that Wι(μ, D) ≥ r(1 — Na).	□
LemmaA.15 (Uniform distribution on box). Id is (N, 1/2)-diversefor N ≤ ɪ (d/18)d/2.
Proof. By Fact A.3, QId (r) ≤ (18r2 /d)d/2 . Taking r = 1 and applying Lemma A.14 allows us to
conclude that Wι(μ, Id) ≥ 1 — N(18/d)d/2, from which the lemma follows.	□
As we stated in Lemma 2.3, a large family of pushforwards of the uniform distribution over [0, 1]d
are similarly diverse. Below we restate this lemma and provide a complete proof.
Lemma A.16 (Random expansive leaky ReLU networks). Letγ > 0. For k0, . . . , kL ∈ N satisfying
ki ≥ (1 + γ)ki-1 for all i ∈ [L], let W1 ∈ Rk1×k0,W2 ∈ Rk2×k1, . . . ,WL ∈ RkL×kL-1 be
random weight matrices, where every entry ofWi is an independent draw from N (0, 1/ki). For the
function F : Rk0 → RkL given by
F(x)，WlΨi (Wl-iΨi (…ψλ(Wιx)…)),
we have that F (Ik0 ) is (2m, β)-diversefor
m = (ko/2)log(ko/2) — kL/Y — 1
λ
2e1/Y
Sofor example, if γ, λ,L are constants, then F(Ik°) is (2ςι(k0 Iog k0), Ω(1))-diversefor k° sufficiently
large.
We will prove this inductively by first arguing that pushing anticoncentrated distributions through
leaky ReLU (Lemma A.17) or through mildly “expansive” random linear functions (Lemma A.18)
preserves anticoncentration to some extent:
16
Published as a conference paper at ICLR 2022
Lemma A.17. Let 0 < λ ≤ 1/2. If a distribution D over Rd satisfies QD (r) ≤ α, then the
pushforward D0 ，Ψλ(D) also satisfies Qd0(λr) ≤ 2da, where here ψλ(∙∙∙) denotes entrywise
application of the leaky ReLU activation.
Proof. Consider any ball B(ν, λr) in Rd. Take any orthant KS of Rd, given by points whose i-th
coordinates are nonnegative for i ∈ S and negative for i 6∈ S. Let BS be the intersection of B with
this orthant. Then ψλ-1(BS) consists of points z ∈ KS for which
X(λzi - νi)2 + X((1 - λ)zi - νi)2 ≤ λ2r2.	(3)
We can rewrite the left-hand side of (3) as
λ2 X(zi - νiλ)2 + (1 - λ)2 X(zi - νi /(1 - λ))2 ≥ λ2 kz - ν(S)k2,
i∈S	i6∈S
where in the last step we used λ ≤ 1/2 and define the vector νS ∈ Rd by
VS = ∫νS∕λ	i ∈ S
i = IVS/(1-λ) i∈ S.
In other words, ψλ-1(BS) is contained in KS ∩ B(ν(S), r). In particular,
ψλ-1(B) ⊂ [ KS ∩ B(V(S), r),
S
so Px〜do [x ∈ B] ≤ 2d ∙ a by a union bound.	□
Lemma A.18. Suppose n, d ∈ N satisfy n ≥ (1 + γ)dfor some γ > 0. Let W ∈ Rn×d be a matrix
whose entries are independent draws from N (0, 1/n). Ifa distribution D over Rd satisfies QD (r) ≤
α, then for the linear map f : x → Wx, the pushforward D0，f (D) satisfies Qdo Q(Y+γ)) ≤ α
with probability at least 1 一 exp(-Ω(γd)).
Proof. By Theorem A.6, for any > 0 we have that σmin (W) ≥e ∙ (1- q∕d-1) with probability
at least 1 - (CC)n-d+1 - e-cn. Taking E = 1∕2C and noting that 1 - d-d-1 ≥ ɪ^ɪ^, We conclude
that
P σmin(W) ≥ 2(1 +γ) ≥ 1 - exp(-Ω(γd)).
Condition on this event. Now for any V ∈ Rn, if We write V as Wμ + μ⊥ where μ⊥ is orthogonal to
the column span of W, then ∣∣Wx - ν∣∣2 = ∣∣W(x - μ)k2 + ∣∣μ⊥∣∣2. So ∣∣Wx-νk ≤ 2(i++γ) implies
that kW(x - μ)∣ ≤ ?(二).But because σmin(W) ≥ ^γ+γ), WecOnclUdethatkx - μ∣ ≤ r, from
which the lemma follows.	□
We are now ready to prove Lemma 2.3:
ProofofLemma 2.3. By Lemma A.14 it suffices to bound the Levy concentration function. We will
induct on the layers of F. For i ∈ [L], let F(i) denote the sub-network
WiΨλ(WL-ιΨλ(…Ψλ(Wιx)…)),
and let Di denote the pushforward F(i) (Ik0 ), which is a distribution over Rki . We would like to
apply Lemma A.18 to each of the weight matrices Wi, . . . , WL, so condition on the event that the
lemma holds for these matrices, which happens with probability at least 1 - L exp(-Ω(γd)).
Recalling from Fact A.3 that QIk (r) ≤ (18r2/k0)k0/2 for any r > 0, we get from Lemma A.18
applied to Wi that QD1 (2(i+7)) ≤ (18r2∕ko)k0/2.
17
Published as a conference paper at ICLR 2022
Suppose inductively that we have shown that QDi (ri) ≤ αi for some ri, α > 0. Then by
Lemma A.17 and Lemma A.18 applied to weight matrix Wi+1, we conclude that
QDi+1 (ri+1) ≤ αi+1
for ri+1 = 2(1]iγ) ,αi+1 = 2kiαi.
(4)
Unrolling the recursion (4), we conclude that QDL (rL) ≤ αL for
V rλj(二 Y ≥ r ∙ ( 2⅛ V 2 (^ 了
aL = 2kl+…+kLτ (18r2/k0)k0/2 ≤ 2kL∕γ(18r2/k0)k0/2,	(5)
where the inequality in (5) follows from the fact that kι T-+ kL-ι ≤ kL-ι(1 + 1∕γ) ≤ kL∕γ. By
Lemma A.14, F(Ik0) = DL is (N, rL(1 - NαL))-diverse. The lemma follows by taking r = 1/3
and 2m = N = 1∕2αL.	□
B	Deferred Proofs from Section 3
B.1	Proof of Corollary 3.3
Corollary 3.3 is an immediate consequence of the following which appeared in (Chen et al., 2020b):
Lemma B.1. For any function P : {±1}k → {±1}, there is a collection of k weight matrices
W1 , . . . , Wk with entries in RO(k) for which
P(x) =Wkφ(∙∙∙φ(W1x)∙∙∙)	(6)
for all x ∈ {±1}k, and for which kWi k ≤ O(1). Furthermore, the size of the network on the
right-hand side of (6) is at most O(2k ∙ k).
We give a proof for completeness to make explicit the dependence of the parameters on k.
Proof. Consider the Fourier expansion F (x) = S⊆[k] F [S] i∈S xi. We show how to represent
each Fourier basis function Qi∈S xi as a ReLU network with at most k layers. Observe that for any
x1, x2 ∈ {±1},
x1 ∙ x2 = φ(x1 + x2) + φ(-x1 - x2) - φ(x2) - φ(-x2),	(7)
which is a two-layer neural network of size 4 whose two weight matrices have operator norm at
most 3. Suppose inductively that for some 1 ≤ m < n, there exist weight matrices W10 , . . . , Wm0
for which Qim=1 xi = W0mφ(∙ ∙ ∙ φ(W01x) ∙ ∙ ∙ ) for all x ∈ {±1}k, that this network has size 4m,
and that Qim=1 kWi0 k ≤ 6m.
We now show how to compute Qim=+11 xi. Define W010 by adding the m-th standard basis vector as a
new row at the bottom of W10 . For every 1 < i ≤ m, define Wi00 to be the matrix given by appending
a column of zeros to the right of Wi00 and then a new row at the bottom consisting of zeros except
in the rightmost entry. Note that kW00k1 = max(1, kW0ik). Define the network Fm : Rk → R2 by
Fm(x) = Wm00 φ(∙ ∙ ∙ φ(W100x) ∙ ∙ ∙ ).
Letting v, e ∈ R2 be the vectors (1, 1) and (0, 1), we can use (7) to conclude that
m+1	m	m
xi = φ xi + xm+1 + φ -	xi - xm+1 - φ(xm+1) - φ(-xm+1)
i=1	i=1	i=1
= φ(v>Fm(x)) + φ(-v>Fm(x)) - φ(e>Fm(x)) - φ(-e>Fm(x)).
We can thus write Qim=+11 xi as the ReLU network
m+1
Y xi = Wm000+1φ(∙ ∙ ∙ φ(W1000x) ∙ ∙ ∙)	(8)
i=1
18
Published as a conference paper at ICLR 2022
where
/ v>wm ∖
v> W00
Wm+1 = (1,1,-1,-1), Wm =	：vWOTm , W000 = Wi0 ∀1 ≤ i<m.
e Wm
-e>Wm00
Note that the entries of any Wi000 are in {0, ±1} and thus have bit complexity at most 2. Additionally,
kW000+ιk ≤ 2, Wm ≤ 3kWmmk = 3 ∙ max(1,kWmk), and kW/ = max(1, ∣Wik) for all
1 ≤ i < m, so Qim=+11kW0i00k
≤ 6m+1 . Furthermore, the size of the network in (8) is 4m + 4.
This completes the inductive step and we conclude that any Fourier basis function Qi∈S xi can
be implemented by an |S |-layer ReLU network with size 4|S | and the product of whose weight
matrices’ operator norms is at most 6|S| .
In particular, as the biases in the network are zero, we can rescale the weight matrices so they have
equal operator norm, in which case they each have operator norm at most O(1) and entries in RO(k)
Finally note that because the Fourier coefficients are given by E[F (x) Qi∈S xi], they are all multi-
ples of 1/2k and thus have bit complexity O(k). The proof follows from applying Lemma B.2 to
these Fourier basis functions and λ given by the Fourier coefficients of P, as ∣∣λk = ∣∣P∣∣ = 1.	□
The above proof required the following basic fact:
Lemma B.2. Let τ, τ0 ∈ N, and let λ ∈ Rrτ. Given neural networks F1, . . . , Fr : Rd → R each
with L layers and whose weight matrices {Wi(1)}, . . . , {Wi(r) } have operator norm bounded by
some R > 0 and entries in Rτ0, their linear combination i λiFi is a neural network with L layers,
size given by the sum of the sizes of Fι, ...,Fr, and weight matrices Wi,..., WL with entries in
RO(T+τ0)and satisfying ∣∣Wi ∣∣ ≤ R√r,, IlWL∣∣ ≤ R∣λ∣, and ∣Wik ≤ Rfor all 1 < i < L. Here
λ ∈ Rr is the vector with entries λi.
Proof. Denote the i-th weight matrix of Fj by Wi(j). Define Wi to be the vertical concatenation
(i)	(r)
ofWi , . . . , Wi , and for every 1 < i < L, define Wito be the block diagonal concatenation of
Wi(i), . . . , Wi(r). Finally, define WL to be the row vector given by the product
(WL	0	…	0 ∖
丁 0	W弋2 …	0
λ>	L
.	0	...	0
∖ 0	0	…W(Lr)j
For all 1 < i < L, ∣Wi∣ ≤ maxj∈[r] ∣W(i) ∣, and additionally ∣Wi ∣2 ≤ Pjr=i∣W(ij) ∣2 and
∣Wl∣ ≤ ∣λ∣ max臼rUIW”.	□
B.2	Proof of Lemma 3.4
We will need the following helper lemma about means of truncations of sub-Gaussian random vari-
ables:
Lemma B.3. If Z is σ2 -sub-Gaussian and mean zero, then for any interval I = [a, b] with a ≤ 0 ≤
b, we have ∣E[Z ∙ 1[Z ∈ I]]| ≤ O(b 一 a + σ) ∙ exp(- min(-a, b)2/2σ2).
19
Published as a conference paper at ICLR 2022
Proof. Define the random variable Z0 = Z ∙ 1[Z ∈ I]. Then by integration by parts,
E[Z0] ≤ E[Z ∙l[Z>b]]
∞
=	P[Z0 > t]dt
0
∞
= b P[Z > b] + P[Z > t]dt
b
≤ bexp(-b2∕2σ2) + O(σ ∙ exp(-b2∕2σ2))
≤ O(b + σ) ∙ exp(-b2∕2σ2).
and similarly, E[Z0] ≥ E[Z ∙ 1[Z < -a]] ≥ O (a - σ) ∙ exp(-b2∕2σ2), completing the proof. □
We now complete the proof of Lemma 3.4.
Proof of Lemma 3.4. Without loss of generality we can assume that E[X] = 0 and E[Y] = α. If
α ≥ cσ for some sufficiently large absolute constant, then we can simply take t = α∕2 and get that
| PX >t]-P[Y > t]| ≥ 1/2. Now suppose α < cσ, and let I = [-r,r+α] for r = σ/log(Cσ∕α)
for some large constant C > 0. Note that by this choice of r,
r exp(-r2∕2σ2 ) ≤ O(α),
where the constant factor can be made arbitrarily small by picking C sufficiently lage. Define the
random variables X 0，X ∙ 1 [X ∈ I] and Y0，Y ∙ 1 [Y ∈ I]. Then
α = E[Y] - E[X] = E[Y0] - E[X0]+ E[Y ∙ 1[Y∈ I]] - EX ∙ 1[X ∈ I]].	(9)
By Lemma B.3,
E[X ∙ 1 [X ∈ I]] ≤ O(2r + α + σ) ∙ exp(-(r + a)2∕2σ2) ≤ O(r) ∙ exp(-r2∕2σ2) ≤ O(ɑ) (10)
and similarly
E[Y ∙1[Y∈ I ]] ≤ E[Y] ∙ P[Y∈ I ]+ O(2r + α + σ) ∙ exp(-(r + α)2∕2σ2).
≤ 2αexp(-r2∕2σ2) + O(r) ∙ exp(-(r + a)2∕2σ2) ≤ O(α).
Additionally, we have
E[X0] - E[Y0]
Z α+r(ΦY0(z) -
0
ΦX0(z))dz-Z0
-α
(ΦX0 (z) - ΦY0 (z))dz
(11)
(12)
where ΦZ(z) denotes the cdf at z of random variable Z. Putting (9), (10), (11), (12) together, we
conclude that
min
(ΦY 0 (z) - ΦX0
(z))dz, 0
-α
(ΦX0 (z) - ΦY0
≥ Ω(α),
where the constant factor can be made arbitrarily close to 1/2 by making C sufficiently small. By
averaging, we conclude that there exists t ∈ [-α, α + r] for which
| P[X0 >t] - P[Y0 > t]| ≥ Ω(α∕r).
But P[X 6∈ I], P[Y 6∈ I] ≤ O(exp(-r2∕2σ2)) ≤ O(α∕r), where the absolute constant can be made
arbitrarily small by making C sufficiently small. The claim follows by a union bound, recalling the
definition of X0, Y0.	□
B.3	Thresholds of Networks as Circuits
In the proof of Theorem 3.2, we also need the following basic fact that signs of ReLU networks can
be computed in P∕poly.
Lemma B.4. For any f ∈ F *, there is a Turing machine that, given any input y, outputs sgn(f (y))
after poly(d) steps.
20
Published as a conference paper at ICLR 2022
Proof. Recall that the weight matrices W1, . . . , WL of f have entries in Rτ for τ = poly(d). So
for any 1 ≤ ' ≤ L, diagonal matrices Di ∈ {0, i}k1×k1,..., D'-ι ∈ {0, l}k'-1×k'-1, and vector
y ∈ {±1}d, every entry of the vector
W'D'-i(W'-iD'-2(…(Wiy + bi)…)+ b`-i) + b`
has bit complexity bounded by
log2 (' ∙ 2O('τ) Y kJ = O('τ + S) = poly(d),
where in the second step we used that log(ki) ≤ ki for all i ∈ [` - 1]. So for any input to f, every
intermediate activation has poly(d) bit complexity.
The Turing machine we exhibit for computing sgn(f (y)) will compute the activations in the network
layer by layer. The entries of Wiy + bi can readily be computed in poly(d) time. Now given the
vector of activations
V = W' φ(…φ(W1y + bi)∙∙∙) + b`
for some ` ≥ 1 (where v is represented on a tape of the Turing machine as a bitstring of length
poly(d)), We need to compute W'+ιφ(v) + b'+i. The ReLU activation can be readily computed
in poly(d) time, so in poly(d) additional steps we can form this new vector of activations at the
(' + 1)-layer. So within S ∙ poly(d) = poly(d) steps the Turing machine will have written down
f(y) (represented as a bitstring of length poly(d)) on one of its tapes, after Which it Will return the
sign of this quantity.	□
B.4	Proof of Theorem 3.2
We now give a complete proof of Theorem 3.2:
Proof. The parameter m will be clear from context in the following discussion, so for convenience
we will refer to d(m) and Gm as d and G. Let k, P, G be such that the outcome of Assumption 1
holds, and negl(∙) denote the function indicating the extent to which G fools poly-sized circuits.
By Corollary 3.3, every output coordinate of G is computable by a network in CLτ,,ΛS,m for τ =
O(k), Λ = exp(O(k)), L = k,S= O(2kk).
We first check that Wi (G(Um), Ud) > 1/3. Note that G(Um) has support of size 2m. In
Lemma A.13 we can take μ = Ud and conclude that μ is (2m, 2(1 - 2m-d))-diverse, so
Wi(G(Um),Ud) ≥ 2(1 - 2m-d) = 2(1 - 2m-mc) ≥ 1.
It remains to check that G fools F * relative to Ud. Suppose to the contrary that there exists some
f ∈ F * and absolute constant a > 0 for which |E[f (G(Um))] - E[f (Ud)]∣ > 1/da. We will argue
that this implies there is a poly-sized circuit C : {±1}d → {±1} distinguishing G(Um) from Ud.
First note that for any threshold t ∈ Rτ, by Lemma B.4 there is a Turing machine Mτ : {±1}d →
{±1} that computes y 7→ sgn(f (y) - t) with τ bits of advice. So if there existed a threshold t ∈ Rτ
for which
∣E[Mτ(G(Um))] - E[Mτ(Ud)]∣ > 1/da',	(13)
for some constant a0 > 0, then by Fact A.8, there would exist a Boolean circuit C distinguishing
G(Um) from Ud with non-negligible advantage, contradicting Assumption 1 and concluding the
proof.
We will apply Lemma 3.4 to show the existence of such a threshold t. Specifically, define random
variables X = f (G(Um)) and Y = f(Ud). By Corollary A.5 applied to the poly(d)-Lipschitz
function f : {±1}d → {±1}, Y - E[Y] is poly(d)-sub-Gaussian. And recalling that G ∈ CLτ,,ΛS,m
for Λ = exp(O(k)), we can apply Corollary A.5 to the poly(d) ∙Ok(1)-Lipschitz function f ◦ G :
{±1}m → {±1} to conclude that X - E[X] is σ2-sub-Gaussian for σ，poly(m) ∙ exp(O(k))=
poly(d). By Lemma 3.4, there exists a threshold t for which the left-hand side of (13) exceeds
min(1∕2, Ω(n a∕σ)), which is not negligible.
It remains to verify that t has bit complexity at most poly(d). As the entries in the weight matrices
and biases in f all have bit complexity poly(d) and f has size and depth poly(d), f(y) has bit
21
Published as a conference paper at ICLR 2022
complexity poly(d) for any y ∈ {±1}d. Similarly, the entries in the weight matrices and biases
in G all have bit complexity O(k) = O(1), so f (G(x)) has bit complexity poly(d) for any x ∈
{±1}m. By the bound on t in Lemma 3.4 and our bound on σ above, t therefore also has poly(d)
bit complexity.	□
B.5 Proof of Lemma 3.6
Proof. Suppose to the contrary that there existed some function f ∈ F0 for which
∣E[f(J(p))] - E[f (J(q))]∣> 2e∙ Λ.
By Lemma 2.1 and our choice of S00, the composition f ◦ J : Rs → R can be computed by a
network in CLR r^ whose bias and weight vector entries in the output layer lie in RT，.
We first show why this would lead to a contradiction. Consider the function h，C ∙ f ◦ J for
C
2dlog2 λ'3 ∈ [Λ0, 2Λ0),
which can be computed by taking the network computing f ◦J and scaling the bias and weight vector
in the output layer by C. Note that this scaling results in bias and weight vector entries in the output
layer for h with bit complexity T0 +「log2 Λ0√F∣ = T. Furthermore, h is ΛΛ0√r∕C ≤ Λ-Lipschitz,
so h ∈ CLτ,,ΛS,s. On the other hand, we would have
∣E[h(p)] - E[h(q)]∣> 2eΛ0√r∕C ≥ e,
yielding the desired contradiction of the assumption that WF (p, q) ≤ .
□
B.6	Proof of Fact 3.7
Proof. Note that h(Un) is the uniform distribution over multiples of 1∕2n in the interval [0, 1).
Given any such multiple z, let pz denote the uniform distribution over [z, z + 1∕2n). One way of
sampling from I1 is thus to sample z from h(Un) and then sample from pz.
Now consider any 1-Lipschitz function f : R → R. Note that for any z0 in the support of pz,
|f(z) - f(z0)| ≤ 1∕2n ≤ . We have
|E[f(h(Un))] -E[f(I1)]|
E E	[f(z) - f(z0)]	≤
Z^h(Un) zZ^ppz	_|
as desired.
□
B.7	Proof of Lemma 3.8
Proof. Let n，dlog(1∕c)^∣. For every i ∈ [r], define Si，{(i 一 1) ∙ n +1,..., i ∙ n}. Take J to
be the linear function where for every i ∈ [r], the i-th output coordinate of J is the linear function
which maps y ∈ Rs to hwi, y + 1i where wi is zero outside of Si and, over coordinates indexed by
Si, equal to the vector (1∕4, . . . , 1∕2n+1). Note that each output coordinate of J is computed by a
function in C1n,+0,1s,O(1).
By Fact 3.7 and Fact A.11,
Wf* (J(Us),Ir) ≤ poly(r) ∙ W1(J(Us),Ir) ≤ poly(r) ∙ e.
On the other hand, by Lemma 3.6 and the fact that the union of CL-IlSg-：* ,λ over τ, Λ, L, S
Poly(S) is still F*, we conclude that
Wf* (J(De), J(Us)) ≤ O(e√T),
from which the lemma follows by triangle inequality.
□
22
Published as a conference paper at ICLR 2022
B.8	Proof of Lemma 3.9
_______ ________ 一 . _ 一 一 __________________________ ，一， 一 、 ,.
Proof. Let J : Rs → Rr be given by Lemma 3.8. We know that Wf* (J(De), Ir) ≤ 〜poly(r).
By Lemma 3.6 applied to these two distributions and the generator function H, together with the
fact that the union of CL-TZdloS)^Sd｝：)d over Λ,L,S = Poly(S) is still F*, we thus have that
WF* (H(J(De)), H(Ir)) ≤ O(eΛ0√d) ∙ poly(r) = eΛ0 ∙ poly(s).
We will thus take J0 in the lemma to be H ◦ J. By Lemma 2.1, each output coordinate of J0 is
computed by a function in Cmax(O(ISgQ/",T，'。(八 √d) as claimed.	□
B.9	Proof of Theorem 3.5
Proof of Theorem 3.5. The parameter m will be clear from context in the following discussion, so
for convenience we will refer to r(m), d(m), (m), Hm, Gm as r,d, , H, G, and similarly for the
network parameters τ0 , Λ0 , L0, S0.
It is easy to verify condition 3 before we even define G: because G(Um) is a uniform distribution on
2m points (with multiplicity) and H (Ir) is (2m, Ω(1))-diverse, Wι(G(Um),Id) ≥ Ω(1) as claimed.
Let s = r ∙ dlog(1∕e)e. As we are assuming E ≥ exp(-O(m)), S ≤ r ∙ m = mc for some
constant c > 1. If s ≤ m, then define G0 : Rm → Rs to be the map given by projecting to
the first S coordinates so that G0(Um) and Us are identical as distributions. Otherwise, take G0 to
be the generator G : Rm → Rs constructed in Theorem 3.2, recalling that WF* (G0(Um), Us ) ≤
negl(m) ≤ E.
Next, by applying Lemma 3.9 to De = G0(Um), we get a function J0 : Rs → Rd each of whose
output coordinates is computed by a function in Cmax(O+Sgg/'))'T，'。(八 √d) SUCh that
WFd (J 0(G0(Um)), H (Ir)) ≤ eΛ ∙ poly(m) ≤ 〜poly(m),	(14)
where the second step follows by our assumption on Λ0 .
We will take G , J0 ◦G0. (14) establishes condition 2 of the theorem. Finally, by Lemma 2.1, every
output coordinate of G can be realized by a network in CLT,,ΛS,m for τ = max(O(log(1/E)), τ0, O(1)),
Λ = O(Λ0√dS) = O(Λ0) ∙ poly(m), L = L0 + O(1), and S = O(s) + r + S0 = O(s) + S0 (where
we used the fact that r = s∕dlog(1∕e)e < s). This establishes condition 1 of the theorem. □
B.10	Proof of Lemma 3.10
Proof. Note that
hξ(X) = φ(X∕ξ + I)- φ(X∕ξ - I)- 1,	(15)
so we can take weight matrices
W1 = 11∕∕ξξ	W2 = (1 -1)
and biases	b1	= (1,	-1)	and b2 = -1.	Note that	hξ	is 1∕ξ-Lipschitz. We conclude that	hξ	∈
CT,24ξ.	□
B.11	Proof of Lemma 3.11
Proof. We first verify that W1 (G0(Um), G(γm)) ≤ E. Take any 1-Lipschitz function f. Note that
we can sample from Um by sampling a vector g from γm, applying hξ entrywise to g, and replacing
each resulting entry of hξ(g) by its sign; importantly, the last step only affects entries i ∈ [m] for
which |gi | < ξ.
We will define E to be the event that |gi| ≥ ξ for all i ∈ [m], noting that
P[E] ≥ 1 - m ∙ P	[|g| < ξ] ≥ 1 - mξp2∕∏.
g 〜N (0,1)
23
Published as a conference paper at ICLR 2022
We can thus write
∣E[f(Gθ(Um))] - E[f(G(Ym))]∣
=E [(f(Go(hξ(g))) - f(G(g))) ∙ 1[E] + (f(Go(sgn(hξ(g)))) - f (G(g))) ∙ 1[Ec]]
g〜Ym
=E [(f(Go(sgn(hξ(g)))) - f(Go(hξ(g)))) ∙ 1[Ec]] .	(16)
g〜Ym
By Fact A.2, f ◦ Go is Λ00√d-Lipschitz. Furthermore, because hξ(g) ∈ [-1,1]m, ∣∣sgn(hξ(g))-
hξ(g)k ≤ √m. We can thus upper bound (16) by
≤ Λ00√md ∙ P[Ec] ≤ Λ00ξp(2∕π)m3d ≤ e
so W1 (G0 (Um), G(γm)) ≤ as desired.
It remains to bound the complexity of G. For any i ∈ [d], we can apply Lemma 2.1 with f given by
the i-th output coordinate of G0 and J given by the map which applies hξ to every entry of the input.
WethUs conclude that G ∈ CfSm for T = max(τ00, l0g2(l∕ξ)) = max(τ00, O(log(Λ00md/e))),
Λ = Λ00√m∕ξ = O(Λ002 m2√d∕e), L = L0 + 2, S = 3m + S00 as claimed.	□
C Fooling ReLU Networks Would Imply New Circuit Lower
Bounds
In this section we show that even exhibiting generators with logarithmic stretch that can fool all
ReLU network discriminators of constant depth and slightly superlinear size would yield break-
through circuit lower bounds.
First, in Section C.1 we review basics about average-case hardness and recall the state-of-the-art for
lower bounds against TC0. Then in Section C.2 we present and prove the main result of this section,
Theorem C.2.
C.1 AVERAGE-CASE HARDNESS AND TC0
One of the most common notions of hardness fora class of functions F is worst-case hardness, that
is, the existence of functions which cannot be computed by functions in F .
Definition 10 (Worst-case hardness). Given a class of Boolean functions F, a sequence of functions
fn : {±1}n → {±1} is worst-case-hard for F if for every f : {±1}n → {±1} in F, there is some
input x ∈ {±1}n for which f(x) 6= fn (x).
A more robust notion of hardness is that of average-case hardness, which implies worst-case hard-
ness. For any fn ∈ F , rather than simply require that there is some input on which f and fn
disagree, we would like that over some fixed distribution over possible inputs, the probability that f
and fn output the same value is small. Typically, this fixed distribution is the uniform distribution
over {±1}n, but in many situations even showing average-case hardness with respect to less natural
distributions is open.
Definition 11 (Average-case hardness). Given a class of Boolean functions F, a function : N →
[0, 1/2), anda sequence of distributions {Dn}n over {±1}n, a sequence of functions fn : {±1}n →
{±1} is (1/2+(n))-average-case-hardforF with respect to {Dn} if for every f : {±1}n → {±1}
in F,
P [f(x) = fn(x)] ≤ 1 + e(n)∙
X 〜Dn	2
By a counting argument, for any reasonably constrained class F there must exist functions which
are worst/average-case hard for F . A central challenge in complexity theory has been to exhibit
explicit hard functions for natural complexity classes. In the context of this work, by explicit we
simply mean that there is a polynomial-time algorithm for evaluating the function.
The complexity class we will focus on in this section is TC0, the class of constant-depth linear
threshold circuits of polynomial size:
24
Published as a conference paper at ICLR 2022
Definition 12 (Linear threshold circuits). A linear threshold circuit of size S and depth D is any
Boolean circuit of size S and depth D whose gates come from the set G of all linear threshold
functions mapping x ∈ {±1}n to sgn(hw, xi - b) for some arity n ∈ N, vector w ∈ Rn, and bias
b ∈ R. TC0 is the set of all linear threshold circuits of size poly(n) and depth O(1).6
The best-known worst-case hardness result for TC0 is that of (Impagliazzo et al., 1997) who showed:
Theorem C.1 ((Impagliazzo et al., 1997)). Let fn : {±1}n → {±1} be the parity function on n
bits. For any depth D ≥ 1, any linear threshold circuit of depth D must have at least n1+cθ-D
wires, where c > 0 and θ > 1 are absolute constants.
In (Chen et al., 2016) this worst-case hardness result was upgraded to an average-case hardness result
with respect to the uniform distribution over the hypercube. Remarkably, this slightly superlinear
lower bound from (Impagliazzo et al., 1997) has not been improved upon in over two decades!
We remark that the discussion about lower bounds for threshold circuits is a very limited snapshot
of a rich line of work over many decades. We refer to the introduction in (Chen & Tell, 2019) for a
more detailed overview of this literature.
C.2 Hardness Versus Randomness for GANs
For convenience, given sequences of parameters D(m), S(m) ∈ N (these will eventually correspond
to the depth and size of the linear threshold circuits against which we wish to show lower bounds)
let
Cd(D,S) ,C
poly(d),poly(d)
Θ(D),3d+Θ(S),d.
This will comprise the family of ReLU network discriminators that we will focus on. We now
show that if one could exhibit generators that can provably fool discriminators in Cd(D, S), then
this would translate to average-case hardness against linear threshold circuits of depth D and size
D. Formally, we show the following:
Theorem C.2. There is an absolute constant c > 0 for which the following holds. Fix se-
quences of parameters D(m), S(m) ∈ N. Suppose there is an explicit7 sequence of generators
Gm : Rm → Rd(m) for d(m) ≥ cmlogm such that WCd(m)(D(m),S(m))(Gm(Im), Id(m)) ≤ (m)
for some (m) ≥ 1/poly(m) and such that each output coordinate of Gm is computable by a net-
work in F *, then there exists a sequence of functions hd(m): {±1}d(m) → {±1} in NP which are
(1/2+E(m)/2+m-Q(m) )-average-case-hard with respect to some sequence ofexplicit distributions
{Dm}for linear threshold circuits of depth D(m) and size S(m).
Remark C.3. In particular, this shows that if we could exhibit explicit generators fooling all discrimi-
nators given by neural networks of polynomial Lipschitzness/bit complexity of depth D(m) and size
O(d(m)1+exp(-D(m).99)), then by (2) we would get new average-case circuit lower bounds for TC0.
In fact it was shown by (Chen & Tell, 2019) that such a result would imply TC0 6= NC1, which would
be a major breakthrough in complexity. This can be interpreted in one of two ways: 1) it would be
extraordinarily difficult to show that a particular generative model truly fools all constant-depth,
barely-superlinear-size ReLU network discriminators, or 2) gives a learning-theoretic motivation
for trying to prove circuit lower bounds.
Regarding the proof of Theorem C.2, note that the statement is closely related to existing well-
studied connections between hardness and randomness in the study of pseudorandom generators. In
fact, readers familiar with this literature will observe that Theorem C.2 is the GAN analogue of the
“easy” direction of the equivalence between hardness and randomness: an explicit pseudorandom
generator that fools some class of functions implies average-case-hardness for that class.
In order to leverage this connection however, we need to formalize the link between GANs (over
continuous domains) and pseudorandom generators (over discrete domains) in the next lemma. It
6Sometimes TC0 is defined with the gate set taken to consist of {∧, ∨, } and majority gates, though these
two classes are equivalent up to polynomial overheads (Goldmann et al., 1992; Goldmann & Karpinski, 1998).
Moreover, because a circuit of size S and depth D using the latter gate set is clearly implementable by a circuit
of size S and depth D using the former gate set, so our lower bounds against the former gate set immediately
translate to ones against the latter.
7By explicit, we mean that we are provided a way to evaluate these functions in polynomial time.
25
Published as a conference paper at ICLR 2022
turns out that in the preceding sections we already developed most of the ingredients for establishing
this connection.
Lemma C.4. Suppose there is an explicit sequence of generators Gm : Rm → Rd(m) such that
WCd(m)(D(m),S(m))(Gm(Im), Id(m)) ≤ (m) for some (m) = 1/poly(m) and such that each
output coordinate of Gm is computable by a network in F *. Then there is an explicit Sequence of
pseudorandom generators G0m : {±1}n(m) → {±1}d(m) for n(m) = Θ(m log m) that 2(m)-fool
linear threshold circuits of depth D(m) and size S (m).
Proof. As in the proofs of the theorems from Section 3, the parameter m will be clear from context,
so we will drop m from subscripts and parenthetical references.
Recall the function hξ from Lemma 3.10; we will take ξ = /poly(m). Also define n ,
Θ(log(m/)) and recall from the proof of Lemma 3.8 the definition of the linear function J :
Rmn → Rm : for every i ∈ [m], the i-th output coordinate of J is the linear function which maps
X ∈ Rmn to hwi, X + 1)，where Wi is zero outside of indices {(i - 1) ∙ n +1,... ,i ∙ n} and equal
to the vector (1/4, 1/8, . . . , 1/2n+1) on those indices.
Given generator G fooling Cd, we will show that the Boolean function G0 : {±1}mn → {±1}d
given by
G0 = hξ ◦ G ◦ J
is a pseudorandom generator that fools TC0 circuits. To that end, suppose there was a TC0 circuit
f : {±1}d → {±1} for which |E[f(G0(Umn))] - E[f (Ud)]| > 2. We will show that this implies
the existence ofa ReLU network f0 ∈ Cd(D, S) for which |E[f 0 (G(Im))] - E[f0(Id)]| > .
Our proof proceeds in three steps: argue that
1.	f ◦ hξ ∈Cd(D,S)
2.	E[f(Ud)] ≈ E[f(hξ(Id))]
3.	E[G0(Umn)] ≈ E[f(hξ(G(Im)))]
Note that 2 and 3, together with the fact that f is a discriminator for G0, imply that f0 , f ◦ hξ
is a discriminator for G. 1 then ensures that this discriminator is a ReLU network with the right
complexity bounds, yielding the desired contradiction.
To show step 1, we will show that f can be computed by a network in COpo(l1y)(,dp)o,lpyo(ldy)(,dd) and then apply
Lemma 2.1 and Lemma 3.10. Suppose the threshold circuit computing f has depth D, where D is
some constant. Recall from Lemma A.7 that we may assume, up to an additional blowup in size by
D, that the constant-depth threshold circuit C computing f is comprised of layers S1, . . . , SD such
that Si consists of all gates in C for which any path from the inputs to the gate is of length i.
Let ki denote the number of gates in Si (where kD = 1), and for each j ∈ [ki], suppose the linear
threshold function computed by the j-th gate in Si is given by sgn( hWi,j, ∙) - bi,j) for w%,j ∈ Rki-I.
As each linear threshold takes at most poly(d) bits as input, we can assume without loss of generality
that bi,j and the entries of wi,j lie in Rτ for τ = poly(d). For this τ, note that for any w ∈ Rτk, b ∈
Rτ, X ∈ {±1}k,
sgn(hw, Xi - b) = hξ0 (hw, Xi - b) ,
for some ξ0 = 1∕poly(d), where hι∕poiy(d)(∙) is the function defined in Lemma 3.10, and recall
from the proof of Lemma 3.10 that it can be represented as a two-layer ReLU network via (15). For
every i ∈ [D], we can thus define two weight matrices Wi(1) ∈ R2ki×ki-1 and Wi(2) ∈ Rki×2ki by
	w-	wi,1	一∖		/1 -1 0	0	…0	0 \
1) — 1 —— 			一	Wi,1	一 ... ...	Wy) =	0	0	1	-1	…0	0
ξ0	...	i		 .	.. 	 ... .	.	.	.	..	.
	——	wi,ki	——		\0	0	0	0	…1 -1/
	\—	wi,ki	—/		
26
Published as a conference paper at ICLR 2022
and biases bi(1) ∈ R2ki and bi(2) ∈ Rki by
bi() = (1, -1, 1, -1, . . . , 1, -1)	bi() = (-1, . . . , -1)
so that for all x ∈ {±1}d,
f (x) = WD φ ED φ (…φ (w(2)φ (w(1)x + bf)) + b12))…)+ bD)) + bD)	(17)
The entries of the weight matrices and bias vectors are clearly in Rpoly(d) , and because each hξ0
is poly(d)-Lipschitz and there are D = O(1) layers in the circuit, the function in (17) is poly(d)-
Lipschitz as a function over Rd. The size and depth of the network are within a constant factor of
the size S and depth D of the circuit. Lemma 2.1 and Lemma 3.10 then imply that f ◦ hξ has depth
Θ(D) and size 3d + Θ(S), as well as Lipshitzness and bit complexity polynomial in m because
≥ 1/poly(m) so that ξ ≥ 1/poly(m). Therefore, f ◦ hξ ∈ Cd(D, S).
To show step 2, recall from Lemma 3.11 and Remark 3.12 that W1 (Um, hξ(Im)) ≤
/poly(m). Recalling that f is poly(d) = poly(m)-Lipschitz, we obtain the desired inequality
|E[f (Ud)] - E[f (hξ (Id))]| ≤ /2. Here the factor of 1/2 is an arbitrary small constant coming
from taking the poly(m) in the definition of ξ sufficiently large.
Finally, to show step 3, recall by Fact 3.7 that W1 (J(Umn), Im) ≤ 2 /poly(m) by our choice of
n = Θ(log(m/)) (the 2 comes from taking the constant factor in the definition of n sufficiently
large). By applying Fact A.2 to f ◦ hξ and G, We know that the composition f ◦ hξ ◦ G is poly(m) /e-
Lipschitz. It follows that |E[G0(Umn)] - E[f (hξ (G(Im)))]| ≤ /2. The factor of 1/2 is an arbitrary
small constant coming from taking the constant factor in the definition of n sufficiently large.
Putting everything together, we conclude by triangle inequality that
∣E[f(G(Im))] - E[f(Id)]∣ >e,
a contradiction.	□
The following lemma gives the standard transformation from pseudorandom generators to average-
case hardness. We include a proof for completeness.
Lemma C.5 (Prop. 5 of (Viola, 2009)). Suppose the sequence of functions Gm : {±1}m →
{±1}d(m) (m)-fools a class of Boolean functions F. Define the function hd(m) : {±1}d(m) →
{±1} by
1 exists y ∈ {±1}m such that G(y) = x
hd(m) (x) = -1 otherwise	.
Let Dd(m) be the distribution over {±1}d(m) given by the uniform mixture between Ud(m) and
G(Um).
Then the sequence of functions {hd(m)} is (1/2 + 0(m))-average-case-hard for F with respect to
{Dd(m)}for0(m) = (m)/4 + 2m-d(m)-1.
Proof. As usual, we will omit most subscripts/parentheses referring to the parameter m. Let f :
{±1}d → {±1} be any function in F. Then
P[f(D) = hd(D)] =
≤
≤
≤
1 P[f (Ud) = hd(Ud)] + 1 P[f (G(Um)) = hd(G(Um))]
2 (P[f (Ud) = 0]+ P[hd(Ud) = 1]) + 1 P[f (G(Um)) = 1]
1
2
1
2
1
2
(P[f(Ud)=0]+2m-d)
(P[f(Ud)=0]+2m-d)
+ 2 P[f(G(Um)) = 1]
+ 2 (P[f (Ud) = 1] + ∣∕2)
+ I + 2m-d-1
where in the second step we used a union bound and the fact that h(G(Um)) is deterministically 1
by construction, in the third step we used the fact that P[hd(Ud)] ≤ 2m-d because there are at most
2m elements in the range of G, and in the fourth step we used the fact that G I-fools functions in
F.
27
Published as a conference paper at ICLR 2022
We are now ready to prove Theorem C.2.
Proof of Theorem C.2. By Lemma C.4, we can construct out of the generators Gm an explicit se-
quence of pseudorandom generators that stretch Θ(m log m) bits to d(m) ≥ C ∙ m log m bits and
2(m)-fool linear threshold circuits of size S(m) and depth D(m). The theorem follows upon
substituting this into Lemma C.5, which implies (1/2 + 0(m))-average-case-hardness for such
circuits with respect to the explicit distributions Dd(m) defined in Lemma C.5, where 0(m) =
e(m)∕2 + 2θ(m logm)-Wm)T = e(m)∕2 + m-Ω(m), provided the absolute constant C is sufficiently
large.
Finally, note that the average-case-hard functions hd(m) we get from Lemma C.5 are in NP because
given an input X and a certificate y, one can easily verify whether G(y) = x.	□
28