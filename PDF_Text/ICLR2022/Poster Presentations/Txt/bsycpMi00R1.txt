Published as a conference paper at ICLR 2022
Generalized Natural Gradient Flows in
Hidden Convex-Concave Games and GANs
Andjela Mladenovic *	Iosif Sakos	Gauthier Gidelt	Georgios Piliouras
Univ. of Montreal & Mila	SUTD	Univ. of Montreal & Mila	SUTD
Ab stract
Game-theoretic formulations in machine learning have recently risen in promi-
nence, whereby entire modeling paradigms are best captured as zero-sum games.
Despite their popularity, however, their dynamics are still poorly understood. This
lack of theory is often substantiated with painful empirical observations of volatile
training dynamics and even divergence. Such results highlight the need to develop
an appropriate theory with convergence guarantees that are powerful enough to in-
form practice. This paper studies the generalized Gradient Descent-Ascent (GDA)
flow in a large class of non-convex non-concave Zero-Sum games dubbed Hid-
den Convex-Concave games, a class of games that includes GANs. We focus on
two specific geometries: a novel geometry induced by the hidden convex-concave
structure that we call the hidden mapping geometry and the Fisher information
geometry. For the hidden mapping geometry, we prove global convergence under
mild assumptions. In the case of Fisher information geometry, we provide a com-
plete picture of the dynamics in an interesting special setting of team competition
via invariant function analysis.
1	Introduction
Min-max optimization has found extensive applications in modern Machine Learning (ML) and
Deep Learning. Popular application settings include Generative Adversarial Networks (GANs)
(Goodfellow et al., 2014), adversarial training (Madry et al., 2018), and multi-agent reinforcement
learning (Silver et al., 2017). In all of these cases, a pair of networks is typically trained towards
finding an approximate equilibrium of a highly non-convex non-concave problem. However, such
settings go beyond classical and well-known results in game theory for which equilibria only exist
in more restrictive settings, i.e., convex-concave games (Sion et al., 1958). Unfortunately, there is
no parallel guarantee if the payoff is not convex-concave, and all these applications are indeed based
upon non-convex non-concave games. Even worse, many negative results occur when dealing with
such payoffs: global or local minimax may not exist (Jin et al., 2020), and even if they exist, there is,
in general, no “reasonable algorithm”* 1 that can globally converge to any meaningful notion of local
optimum (Letcher, 2021; Hsieh et al., 2020). To overcome these negative results, we analyze a spe-
cific class of non-convex non-concave games called Hidden Convex-Concave (HCC) games (Flokas
et al., 2020; Gidel et al., 2021; Flokas et al., 2021) that include many Machine Learning applications
such as GANs (Goodfellow et al., 2014), Adversarial Example Games (AEG) (Bose et al., 2020), or
Minimax Estimation of Conditional Moment (Dikkala et al., 2020).
Our contributions. In the setting of HCC games, our analysis tackles unique challenges not occur-
ring in the standard convex-concave games setting. We propose a new type of dynamics, dubbed
Natural Hidden Gradient dynamics (NHG), and we prove its convergence to stationary points of the
HCC game. Critically, our convergence results are global, i.e., we do not make any assumptions
about initial conditions, e.g., that the initial points belong to some local neighborhood. This novel
algorithm is inspired by the generalization of gradient flows in different Banach spaces than the stan-
dard Euclidean one. Arguably, the most well-known non-Euclidean gradient is the natural gradient
* Emails of contact: {andjela.mladenovic,gidelgau}@mila.quebec,
iosif _sakos@mymail.sutd.edu.sg, georgios@sutd.edu.sg
t Canada CIFAR AI Chair
1see (Letcher, 2021, Definition 5) for a definition of “reasonable algorithm”
1
Published as a conference paper at ICLR 2022
flow induced by the Fisher information matrix, which enjoys deep connections to the Replicator
Dynamics (RD). Thus, a natural second question emerges: Can we have similar convergence guar-
antees for RD? In that regard, we provide a complete picture of the resulting dynamics in a setting of
team competition via invariant function analysis. Specifically, we reduce the flow of the dynamics to
a competition between two generalized gradients, one for each team. We show that the behavior of
the dynamics in this setting is constrained by a maximal number of invariant functions, which, com-
bined with the actual geometry of the available strategies, shows that the system can either converge
or cycle. These two results showcase, both, the importance of adapting the “algorithmic geometry”
to the application and the data at hand, as well as the unexplored effects of feasibility constraints on
the complexity of well-known game dynamics.
2	Related Work
Despite the popularity of deep learning applications involving non-convex non-concave games, our
understanding of their optimization dynamics and the nature of their solution concepts are still pre-
liminary. However, this space has already witnessed a few important early works that focus on
identifying new solution concepts. These solution concepts—which are also broadly applicable in
general min-max games—include (local/differential) Nash equilibria (Adolphs et al., 2019; Mazum-
dar & Ratliff, 2019), (local/differential) Stackelberg equilibria (Fiez et al., 2020; Wang et al., 2020),
local minmax (Daskalakis & Panageas, 2018), local robust points (Zhang et al., 2020), and approxi-
mate minimax theorems (Jin et al., 2020; Gidel et al., 2021). Numerous solutions concepts such as
cycles (Vlatakis-Gkaragkounis et al., 2019), chaotic behavior (Cheung & Piliouras, 2019; Cheung &
Tao, 2021), and computational issues (Daskalakis et al., 2021) indicate that solving min-max games,
in general, might involve challenging and complex behavior.
Many algorithms have been proposed to solve restricted classes of non-convex non-concave games
such as Polyak-Eojasiewicz games (NoUiehed et al., 2019; Yang et al., 2020), nonconvex-concave
games (Lin et al., 2020; Ostrovskii et al., 2021; Yang et al., 2020; Kong & Monteiro, 2021), as well as
classes of games inspired by variational inequalities (Mertikopoulos et al., 2019; Diakonikolas et al.,
2021; Lee & Kim, 2021). However, even though they are significant advances in the understanding
of non-convex non-concave game dynamics, such classes of games may not encompass games where
the players are parameterized neural networks such as GANs.
While convergence in GANs has been a topic of exploration (Kodali et al., 2017; Heusel et al.,
2017; Mescheder et al., 2018; Gemp & Mahadevan, 2018; Li et al., 2018; Hsieh et al., 2019; Cao &
Guo, 2020), its hidden convex-concave structure has not been exploited before, and there were no
theoretical global convergence guarantees in a general setting up to this date. In particular, Hsieh
et al. (2019) use a lifting trick and proceed in solving a relaxation of the problem in the distribution
space, while in our work we work entirely in the parameter space. With this respect, we consider
Flokas et al. (2021), and Gemp & Mahadevan (2018) as the closest related works. On the one hand,
in Gemp & Mahadevan (2018), the authors propose crossing-the-curl, a second-order technique, and
provide global convergence guarantees for the Wasserstein Linear Quadratic GAN (W-LQGAN).
While the W-LQGANs is a class of non-convex non-concave GANs it remains far from the GAN
formulation used in practice where the discriminator and the generator are neural networks. On
the other hand, the idea of Hidden Convex-Concave (HCC) games was first proposed by Flokas
et al. (2021) and Gidel et al. (2021). While Flokas et al. (2021) study the Gradient Descent-Ascent
(GDA) dynamics in the HCC setting, their work relies on hard-to-verify safety conditions. Our
global convergence results without hard-to-test assumptions on initialization are first of their kind,
to the best of knowledge.
3	Preliminaries
Many game-theoretic applications in machine learning often involve a specific structure where the
models’ payoff is a convex-concave function (e.g., minimizing Jensen-Shannon-Divergence when
training GANs). To model this specific structure, we propose the following definition of Hidden
Convex-Concave (HCC) games where intuitively, the game’s payoff is a convex-concave function
whose actions are parametrized by non-convex mappings.
2
Published as a conference paper at ICLR 2022
Definition 1 (Hidden Convex-Concave game). A Hidden Convex-Concave game comprises a collec-
tion of payoff (Lx,x0)x,x0 ∈Rd, a distribution p, and two parametrized mappings, F : RM × Rd → R,
and G : RN × Rd0 → R, such that the minimax game of interest is
min max Ψ(θ, φ) where Ψ(θ, φ) = L(Fe,Gφ)= E(xx，)〜p[Lχ,χo (Fe(x),Gφ(x0))] . (1)
θ∈RM φ∈RN
In this setting, while we do not expect Ψ(θ, φ) to be convex-concave, we assume the function L :
F × G → R to be convex-concave where F and G are, respectively, convex subsets of {F : Rd → R}
and {G : Rp → R}.2 We extended Flokas et al. (2021)’s definition to now be able to include most
minimax machine learning applications such as GANs or AEG.
Example 1 (Hidden Matching Pennies (HMP) games). Let us consider p, q ∈ [0, 1] the probabilities
of picking HEADS for the first and the second player, respectively, in a Matching Pennies game with
payoff matrix A ∈ R2×2, where Ai,j = 1 if i = j; -1, otherwise. Then the payoff of this game is
defined via
Lx,x0 (p, q) := (1 - 2p)(1 - 2q) if (p, q) ∈ [0, 1]2 and 0 otherwise .	(2)
Now, let us consider any mappings F : RM × Rd → [0, 1] andG : RN × Rd → [0, 1], such that their
output does not depend on the d-dimensional input, i.e., Fθ (x) = f (θ), and Gφ(x) = g(φ), ∀x ∈
Rd. The payoff Ψ(θ, φ) = L(Fθ, Gφ) := E[Lx,x0 (Fθ(x), Gφ(x0))] = (1 - 2f (θ))(1 - 2g(φ))
defines a HCC game.
In this example, the two agents play the typical bilinear game of Matching Pennies. However, they
do not act on it directly (i.e., choose randomized actions to apply, e.g., the probability of playing
HEADS). Instead, they choose the input parameters θ, and φ, which are fed into functions f , and g,
respectively, whose outputs define the probability of playing HEADS for each agent.
Example 2 (GANs). A Generative Adversarial Network (GAN) is a minimax game where the first
player, i.e., the generator, aims at learning a distribution pθ similar to a reference data distribution
pdata. In practice, the reference data distribution is taken to be the empirical data distribution.
Conversely, the second player, usually called the discriminator or critic, Dφ, tries to distinguish the
distributions ofpθ and pdata. The payoff, Ψ, of this game is defined as:
ψ(θ, Φ)= Ex〜Pdata[log Dφ(x)]+ Ex〜Pθ[log(1 - Dφ(x))].	(3)
Assuming that pdata and pθ have a density with respect to Lebesgue measure,3 and that the
support of pθ is included in the support of pdata, we can consider the distribution p such
that p(x, x0) = pdata(x) if x = x0, and 0 otherwise, and set Lx,x0 (p0, D) := log D +
Pd ：(xo)log(1 一 D) ∙ Thus Lx,xo is convex-concave for any x, x0 ∈ Rd. We have that Ψ (θ, φ)=
E(x,χ0)〜p[Lx,χ0(pe(x),Dφ(x0))], which is a HCCgame.
A GAN formulates the generative modeling task as finding a Nash equilibrium of a minimax game.
The generator of a GAN is defined as a function that aims to produce realistic data samples by
transforming samples drawn from a fixed noise distribution, e.g., N(0, Id). Here, we notice that the
GAN payoff is convex (actually linear) as a function of the density of the generated distribution. An
alternative formulation of a GAN is a Wasserstein GAN (WGAN) (Arjovsky et al., 2017). It turns
out that this GAN is also a HCC game.
Example 3 (WGANs). A Wasserstein GANisa constrained minimax game, where the second player
Dφ is a 1-Lipchitz function and where the payoff is
Ψ(θ, φ)
=Ex~Pdata [DΦ(X)] 一 Ex 〜Pθ [Dφ(X)] .	(4)
Similarly, as in Example 2, the WGAN payoff can be shown to be a HCC game.
As the last class of examples of HCC games, we present the Adversarial Example Games
(AEG) (Bose et al., 2020).
2One can always assume F and G to be convex sets by considering their convex hulls.
3We make this assumption for simplicity. We can consider RadOn-NikOdym derivatives for the general case.
3
Published as a conference paper at ICLR 2022
Example 4 (AEG). An Adversarial Example Game is a minimax game between a generator Gθ and
a classifier f. Given samples (x, y)〜Pdata, the generator Ge aims at finding adversarial examples
x0 such that kx - x0 k∞ ≤ and that x0 is not classified as y by f, thus generating a distribution
pθ. Overall, the payoff of this game is
ψ(θ, φ) = -E(χ0,y)〜Pθ['(fφ(x0),y)] for (x0,y)〜Pe ^⇒ x0 = Gθ(x), (x,y)〜Pdata, (5)
and where ` is the cross-entropy loss and Gθ is such that kx - Gθ (x)k ≤ , ∀x. By using a similar
construction as in Example 2, we can show that (5) is a HCC game with respect to Pe and fφ.
In this work, we make the assumption that the minimax problem induced by L admits a solution:
Assumption 1. The HCC game defined by (1) admits a Nash equilibrium, (F*,G*), i.e.,
minmaxL(F,G) = maxminL(F,G) = L(F*,G*).	(6)
F∈F G∈G	G∈G F∈F
Such an assumption is relatively mild since it holds when the set F is compact (by definition, it is
convex) (Sion et al., 1958). Note that this solution may not be achievable, i.e., we do not assume
that there exists θ* and φ* such that (Fe*, Gφ*) = (F*, G*). Such sufficient conditions for the
existence of a Nash equilibrium ofL : F × G → R are discussed in detail in Gidel et al. (2021, Prop.
1), e.g., Assumption 1 holds if the parameters θ and φ are bounded. From a high-level perspective,
this assumption is analogous to the existence of a global solution non-convex optimization. We use
(F*, G*) as a target to build a Lyapunov function of the natural hidden gradient flow.
3.1	Natural gradient flow
In this section, we present the notion ofa natural gradient flow (Amari, 1985; 1998). Let us consider
a function f : S → R and a class of symmetric positive definite matrices (Pe)e∈S 0 that we
will refer to as metric tensors. The natural gradient flow is the flow given by the steepest descent
direction (Ollivier et al., 2017) with respect to the geometry induced by the matrices Pe,
θ = -PeNf (θ).	(7)
When Pe = I , we consider the canonical Euclidean geometry and recover the standard gradient
flow. One celebrated example of a natural gradient in machine learning is the natural gradient
induced by the Fisher information matrix (Amari, 1998; Martens, 2020).
Example 5 (The natural gradient flow of the Fisher information matrix). Let us consider P (X) the
space of probability distributions on a set X ⊆ R with the metric induced by the Kullback-Leibler
(KL) divergence. Then the natural gradient flow of the Fisher information matrix is given by
θ = -Fe1^f (θ) where Fe := -Ex〜p0[Vθ logPe(x)], Pe ∈ P(X).	⑻
Moreover, ifn := |X| = dimS is finite, and ifPe = θ, the flow (8) is the natural gradient flow of
the Shahshahani metric (Shahshahani, 1979) induced by the metric tensors
Se := diag( θ1,..., θn).	⑼
Recently, alternative natural gradient formulations have been developed using the Wasserstein dis-
tance (Li & Montufar, 2018; Arbel et al., 2020).
3.2	Connections between Replicator Dynamics and gradient flows
The Replicator Dynamics (RD) are standard dynamics used in Evolutionary Game Theory and learn-
ing in games. It is, arguably, the most widely used model of evolutionary selection with multiple
applications in economics, biology, and other fields. Interestingly, RD enjoys a close connection
to gradient flows (see Sigmund (1984); Hofbauer et al. (1998); Harper (2009); Mertikopoulos &
Sandholm (2018)). Specifically, in the case of a symmetric and linear fitness landscape, the gradi-
ent induced by the Shahshahani metric of the mean fitness is a special case of RD. Such a land-
scape can be formally represented by a Potential game. A Potential game is a n-player game
G = (n, S := [m1] × . . . × [mn], u : S → Rn), with payoff function u, characterized by the
existence of a potential function Φ : S → R that satisfies
Φ(si, s-i) - Φ(s0i, s-i) = ui(si, s-i) - ui(s0i, s-i) ∀i ∈ [n] .	(10)
4
Published as a conference paper at ICLR 2022
The connection between potential games and the gradient flows induced by the Shahshahani metric
can be formalized with a generalization of Hofbauer et al. (1998)’s lemma.
Proposition 1. The Replicator Dynamics of a potential game G with potential function Φ is an
(extended) Shahshahani gradient in int(∆m1 × . . . × ∆mn) having potential Ψ(θ) := Esi 〜4[Φ(s)]-
i∈[n]
It follows that, for potential games, RD is a gradient flow with respect to a very specific geometry.
In the next section, we consider a geometry induced by the HCC structure, and which we leverage
to obtain a new natural gradient flow that we call Natural Hidden Gradient flow (NGH).
3.3	The Natural Hidden Gradient flow for HCC Games
In HCC games, we assume Ψ(θ, φ) = L(Fθ, Gφ) (see Definition 1), and, therefore, since the
entities that characterize a solution to the minimax problem are the mappings Fθ and Gφ instead of
their parametrizations, θ and φ, respectively, a natural geometry for consideration is the one defined
by the L2 distance in the space of F × G. Formally, let θ and θ0 be two parameterizations for F
(similarly, for G). The L2 distance between the mappings are:
kFθo — Fθk2 :=	Ex〜Px(Fθo(x)	- Fθ(x))2	and	∣∣Gφ	-	Gφ,k2 :=	Ex〜px,	(Gφo(x0)	-	Gφ(x0))2
where px and px0 are the marginal ofpx,x0. We can then derive the metric tensors of this geometry.
Proposition 2 (Metric tensors of the model space). Under mild regularity assumptions, we have
that, for any θ ∈ RM,
kFθ+δθ — Fθk2 = hδθ, Aθδθi + o(kδθk2) where Aθ := Ex〜Px”Fθ(x)VθFθ(x)|]. (11)
Consequently, Aθ defines a metric on the parameter space in which the “distance” between two
values, θ, and θ0, corresponds to the L2 distance between Fθ and Fθ0. We can, thus, construct the
corresponding natural gradient flow.
Proposition 3 (Natural Hidden Gradient dynamics). The flow induced by the geometry (11) is
θ = -Aθ E(x,χ0)〜p[Vθ Lx,xo(Fθ (x),Gφ(x0))]
φ = Bφ E(x,x0)〜p[vφLx,x0(Fθ (X), Gφ(XO))]
(D1)
where Aθ ：= Ex〜px[VθFθ(x)VθFθ(x)|] and Bφ := Ex，〜px, [VφGφ(x)VφGφ(x)|] and Ct
denotes the pseudo-inverse of a matrix C-
4 Convergence of the Natural Hidden Gradient flow
In this section, we propose a new type of dynamics, dubbed Natural Hidden Gradient dynamics
(NHG), and in the following two theorems, we prove their convergence in HCC games and GANs.
At the heart of our analysis lies the construction of a proper Lyapunov function that measures the
distance from the game’s equilibrium point. By proving that the Lyapunov function is proper, i.e.,
monotonic, we will prove our proposed dynamics are approaching a Nash Equilibrium.
4.1	Warm-up: A single datapoint
In this section, as a warm-up, we will consider the single datapoint case. In this case, (1) is,
Ψ(θ,φ)=L(Fθ(X),Gφ(X0))	(12)
where L : R × R → R is convex-concave (see Example 1). In this case, since the mapping Fθ and
Gφ are evaluated at a single point, one can simplify the notation and consider Fθ (X) = f (θ) and
Gφ(X0) = g(φ) where f : RM → R and g : RN → R are real-valued mappings that do not depend
on an input X or X0. This situation is already non-trivial since, as illustrated in Example 1, it can
correspond to a non-convex non-concave parametrization of the Matching Pennies game. In order
to solve this game we consider the Natural gradient of the metric defined in Proposition 2.
Proposition 4. In the uni-dimensional case, the Natural Hidden Gradient flow D1 takes the form of
δ —	▽： L(f (θ),g(φy)	L — ▽ ΦL(f (θ),g(φ))
θ =	Ef(θ)k2	and φ =	gφg(Φ)k2	.	(D2)
5
Published as a conference paper at ICLR 2022
Using Proposition 4, it is relatively straightforward to show that the distance to the optimum is a
Lyapunov function for the Natural Hidden Gradient flow.
Theorem 1. Let Ψ be the payoff of an HCC game (12) and consider the dynamics (D2). Then,
V(θ, φ) = 2(f(θ)- f *)2 + 2(g(φ) - g*)2	(13)
is a Lyapunov function, i.e., it is positive, non-increasing, and null if and only if it evaluated at a
game solution. Moreover, if L is strictly convex-concave, we have that V is decreasing and that any
limit point (θ, φ) satisfies Vθ L(f (θ), g(φ)) = VφL(f(θ),g(φ) = 0.
In order to see this, one can compute the time derivative of V. After some elementary computations,
it follows V(θ, Φ) = -(f(θ)-f*) dLf≡∣	,、+(g(Φ) — g*) aLfg)Q) I z and, thus,
f=f (θ)	g=g(φ)
by the convex-concavity of L, we have that V ≤ 0. However, generalizing this theorem to HCC
games with non-constant mappings (with respect to x) is non-trivial since we drastically used the
simplicity of the mappings (Proposition 4) to simplify the expression of the flow. In the next section,
we propose to extend our convergence analysis to finite sum HCC games.
4.2	The general finite sum case
In this section, we consider a finite-sum version of the HCC games as they appear in Definition 1.
In this case, the payoff is defined as
ψ (θ, φ) := nm X Li,j (Fe (xi),GΦ(Xj)) =: L((Fe (Xi))i∈[m], (Gφ(Xj))j∈[n]) ,	(14)
(i,j)∈[m]×[b]
where the function L : Rm × Rn → R is assumed to be convex-concave. We note Li,j := Lxi ,x0
for compactness. Let us recall that we assumed the existence ofa Nash equilibrium (F*, G*) for the
minimax problem (Assumption 1). In this situation, the Natural gradient defined in Proposition 3
has the following form:
θ= - Am X Ve Lij (Fe (g), Gφ(xj)), φ= Bφ X VφLij (Fe (g),Gφ(xj)),	(D3)
(i,j)∈[m]×[n]	(i,j)∈[m]×[n]
where Ae := mm Pm=ι VeFe(Xi)VeFe(Xi)| and Bφ := ɪ Pn=I VφGφ(xj)VφGφ(xj)|.
We will generalize the Lyapunov function considered in the uni-dimensional case (13). The idea is
to consider the L2 distance between (Fe(Xi), Gφ(X0j))i,j and (F*(Xi), G*(X0j))i,j as our Lyapunov
function V. However, in order to prove our result, we will need the following technical assumption.
Assumption 2. For any θ ∈ RM and φ ∈ RN, we have that the families of vectors
(Ve Fe (Xi))i∈[m] and (Vφ Gφ (X0j))j ∈[n] are linearly independent.
When the models are overparametrized, e.g., M > m and N > n, this assumption is relatively mild
since it can be insured by a small perturbation of the considered vectors. In practice, it suggests
regularizing the matrices Ae and Bφ by adding e ∙ Id which is a standard way to stabilize methods
requiring matrix inversions.
Theorem 2. Let Ψ be the payoff of a finite-sum HCC game given by (14) and consider the game
dynamics in (D3). Under Assumption 1, Assumption 2, we have that the quantity
nm
V(θ, Φ) = 2n X(Fθ(Xi)- F*(xi))2 + 2m X(Gφ(xj) — G*(xj))2
(15)
i=1
j=1
is a Lyapunov function, i.e., is positive, non-increasing and null if and only if evaluated at a game
solution. Moreover, if L is strictly convex-concave, V is decreasing as long as (θ, φ) 6= (θ*, φ*)
and if L is a μ-StrongIy Convex-ConcaVe function we have that V is decreasing exponentially as
V(θ, Φ) = V(θo, φo)exp(-μt).
We showed that, in the overparametrized regime, ifwe assume not to encounter any singular matrices
Aθ and Bφ along the trajectory, then, preconditioning low dimensional gradient of θ and φ can
behave like doing gradient update on F and G to leverage the hidden-convex-cave structure of
6
Published as a conference paper at ICLR 2022
the the payoff. We do not know how to recover the gradients updates on F and G in the non-
overparametrized regime. It is a great open question that we consider outside of the scope of this
paper as we focus on understanding convergence in minimax games for deep learning models (that
are over-parametrized). The proofs of Theorem 1 and Theorem 2 are in §A.3.
Regarding the practicability of the method described in (D3), efficient approximations of precondi-
tioning, such as the K-FAC algorithm, were proposed (Martens, 2020; Li & Montufar, 2018) and
used to train large models on Imagenet and CIFAR (Martens et al., 2021; Arbel et al., 2020).
5	Characterizations of Replicator Dynamics in HCC games
In this section, we consider a specific instance of HMP games (cf. Example 1), dubbed 2-Team HMP
games or the XOR-XOR games. Although the possibility of cycling orbits for RD in such games was
established before (Piliouras & Schulman, 2018), in this section, we show stronger results. Specifi-
cally, as we prove in subsection 5.2, by enforcing restrictions to the game’s parameters, it is possible
to affect the game’s outcome, e.g., we may deviate from the well-known cyclic behavior in the un-
restricted setting, and moreover, enforce divergence away from the game’s original equilibrium and
convergence to novel fixed points. We completely characterize the geometry of possible limit cycles
in such a restricted setting by exploiting intuitions developed via the connection between RD and the
Shahshahani information geometry. Specifically, we show the dynamics are controlled by invariant
functions, which correspond to weighted sums of the cross-entropy of the current mixed strategies
of opposing members relative to the uniform, equilibrium strategies (19). For the complete proofs
of this section, we refer the interested reader to §A.4.
5.1	2-Team Hidden Matching Pennies games
We introduce the 2-Team HMP game, G = (n := n1 + n2 , S := {0, 1}n, u : S → Rn), between
n1 + n2 members divided into two teams. The first team, team 1, consists of n1 members, while
the second team, team 2, consists of n2 members. The payoff function u for a strategy profile
s := (s1, s2) ∈ S is given by
uk,i(S) = (-1k— (I - 2 ∙ 1XOR(SI)=XOR(S2。k ∈ [2], i ∈ [nk] ∙	(16)
where XOR(sk) = 1 if |{sk,i | sk,i = 1}| is odd, and 0, otherwise. Given a mixed-strategy profile,
(θ, 1 - θ), θ := (θk)k∈[2] ∈ [0, 1]n, the expected payoff of the i-th member of team k is given by
k-1
ψk,i(θ) := ESk,i〜Ber(θk,i)[uk,i(s)] = ^k	(I- 2f(θ1 ))(1 - 2g(θ2))	(17)
k∈[2], i∈[nk ]
where f(θι) = Es1 〜Ber(θ1)[XOR(s1)] and g(θ2) = Es2^Ber(θ2)[XOR(s2)]. Notice that, since
each member of a given team aims at maximizing the same payoff, G is a Hidden Matching Pennies
game (Example 1) with hidden mappings f(θ1), and g(θ2),
n1
minmaxΨ(θ1,θ2) := X Ψ1,i(θ) = (1 - 2f(θ1))(1 -2g(θ2)).	(18)
θ2 θ1
i=1
It is not difficult to prove that the RD exhibit cyclic behavior in this setting. The following theorem
provides a fine-grained characterization of the dynamics of the HCC game (18) where we show that
the trajectories lie on the intersection of level sets of invariant functions.
Theorem 3. Consider the Replicator Dynamics of G. Given any interior initial condition, the
resulting orbit is a cycle that satisfies the following n1 + n2 - 1 independent invariant functions:
2
Vi1,i2(θ) = X nk[log(θk,ik) + log(1 - θk,ik)], ik ∈ [nk],k∈ [2].	(19)
k=1
5.2	Replicator Dynamics of Hidden Matching Pennies in a restricted setting
Next, we introduce restrictions in the range of each member’s strategies in G such that θk,i ∈ Sk,i :=
[ak,i, βk,i] ⊆ [0,1], ∀k ∈ [2], i ∈ [nk]. These restrictions reflect in RD as halts, i.e., θk,i = 0, every
7
Published as a conference paper at ICLR 2022
time the strategy of the i-th member of the k-th team exceeds those bounds. To ease our notation, we
let Sk := {i ∈ [nk] | 2 ∈ Sk,i}, k ∈ [2] denote all the members of the k-th team for which (2, 2)
is an allowed mixed strategy, and, to simplify this part of the analysis, we also make the following
mild assumption on the initialization of the dynamics:
Assumption 3. For all k ∈ [2] and i ∈ [nk], θk,i(0) ∈ Sk,i ⊂ (0, 1).
A significant observation is that for any mixed-strategy profile (θ, 1 - θ), θ := (θ1, θ2) ∈ [0, 1]n,
if there exists some ik ∈ [n《],∀k ∈ [2] such that。%卜=1, then θ is an equilibrium point; in fact,
it is not difficult to see that these are the only interior equilibrium points of G, which implies that
an interior equilibrium point is reachable if and only if Sk = 0, ∀k ∈ [2]. In our first result, We
prove that if the Nash Equilibrium of the unrestricted case is not reachable by both teams due to the
constraints, then the dynamics converges to a point which only depends on those restrictions.
Theorem 4. Under Assumption 3, if Sk = 0, ∀k ∈ [2], the restricted RD of G converge to
θ1,i = θ2,i =	fαι,i, if βι,i < 2 β1,i , otherwise Γ β2,i, if β2,i < 2 α2,i , otherwise	i ∈ [n1] i ∈ [n2]	g,i = or θ2,i =	Γ β1,i, if β1,i < 2 α1,i , otherwise a a2,i, if β2,i < 2 β2,i , otherwise	i ∈ [n1] i ∈ [n2] /
	{^^^^^^^^^^^^ if |S| is even			~^^^^^^{^^^^^^^^^β if |S| is odd	
where S ：= {(k, i) | k ∈ [2], i ∈ [nk], a, > 1}.
(20)
The key idea behind this result is that at any given time t ≥ 0, the direction ofa strategy θk,i(t) only
depends on whether |S|, the total number of members who can access the uniform strategy, is odd or
even. Thus, we can decouple the evolution of the dynamics of each of the members and analyze its
behavior separately. If an equilibrium point of the unrestricted setting is reachable by both teams,
i.e., Sk 6= 0, ∀k ∈ [2], then the dynamics converge to an invariant set whose degrees of freedom
depend on the number of members who have access to the uniform strategy, (1, 1).
Theorem 5. Under Assumption 3, if Sk 6= 0, ∀k ∈ [2], then the restricted RD of G converge to an
invariant set defined by the |S1| + |S2| - 1 independent invariant functions, Vi1,i2 (θ), ik ∈ Sk, ∀k ∈
[2], where Vi1,i2 (θ) is given as in (19).
One can prove this result by partitioning the time based on the set of members that have halted. The
analysis of Vi1,i2 (θ) in each time-partition is almost trivial, and the result follows by a continuity
argument on Vi1,i2 (θ). These results depict how the behavior ofRD depends on the parameter space
of the game. Notably, we show that if no equilibrium point is feasible, the RD converge to a point
described entirely by the strategy space restrictions. On the other hand, if an equilibrium point is
feasible, we prove the existence of a maximal number of invariant functions, with close connections
to the KL divergence. The latter does not merely show that the RD cycle in this setting, but that they
actually converge to an invariant set with specific degrees of freedom, and which we characterize.
For parameterizations that visualize the behavior ofRD in such restricted settings, see §B.1.
6	Experimental results
Toy multi-dimensional case. As a first experiment, we consider the HCC objective
ψ (θ, φ) = L(Fθ, GΦ) ：= fθm gφ +λ (IlFθ- 3 k2 - kGΦ - 3 k2),
where M is the payoff matrix of the Rock-Paper-Scissors game (see e.g. Gidel et al. (2021)). We
note Fθ(Xi) = [Fθ]i, i ∈ [3], and 3, i.e., the uniform distribution, is the game,s equilibrium. The
mappings F and G are 2-layer MLP with 130 parameters and GELU non-linearities (Hendrycks &
Gimpel, 2016). In Figure 1, we depict a comparison between the performance of GDA and NHG
dynamics on the task of solving minθ maxφ Ψ(θ, φ). We remark, the NHG dynamics converges
smoothly (Figure 1 (Right)), compared to GDA (Figure 1 (Center)), which fail to converge. The
value of the Lyapunov function of the game described is depicted in (Figure 1 (Left)).
GANs. For our second experiment, we implement the NHG dynamics to train a GAN. We consider
a synthetic experiment to learn a sine wave sampled uniformly from 0 to π with 1024 observations.
8
Published as a conference paper at ICLR 2022
12 3
- - -
Ooo
111
0n> >oundeA1
500	1000	1500	2000
Number of Iterations
Figure 1:	A comparison of GDA (Center) and NHG dynamics (Right) in the task of solving
minθ maxφ Ψ (θ, φ) where p = f (θ), and q = g(φ). In NHG, the Lyapunov function (Left) is
monotonically decreasing, as opposed to GDA.
Regarding the GAN, we consider a Flow-GAN architecture where our generator, G, is a Real NVP
consisting of 8 coupling layers (Dinh et al., 2017; 2014), and the discriminator is a 4-layer MLP
with 256-128-64-1 output features. We adapt K-FAC (Martens & Grosse, 2015) as an approximator
to compute NHG, with a learning rate of 10-4, selected using the standard parameter optimizer
package Optuna (Akiba et al., 2019). We used gradient clipping with gradient clip to a maximum
norm of 1 on, both the discriminator and the generator, to stabilize the optimization.
As observed in (Figure 2 (Center)), by iteration 200, the GAN optimized using K-FAC learns the
true sinusoid almost perfectly and converges to a better Wasserstein-1 score than the conventional
GDA (Figure 2 (Right)). The experiments reveal that our approach provides good performance and
convergence guarantees. However, we observe instabilities during training, and lack of convergence
in certain instances, which stay in line with the empirical observations regarding the difficulty of
GAN training, and the instability of matrix inversions close to singularities. We remark that this ex-
periment goes slightly beyond the theoretical results: while in theory, we assume a natural gradient
flow on a “full-batch”, our experiments are based on discrete stochastic updates, and an approxima-
tion of the pseudo-inverses of the matrices (see (D1)) using K-FAC. Thus, this experiment acts as a
proof-of-concept rather than a large-scale comparison between NHG and GDA. The details of both
experiments can be found within the source-code files included with this work.
1.00
0.75
050
025
0.00
-035
-050
-0.75
-1.00
0	1	2	3	4	5	6
0	1	2	3	4	5	6
0.5
I-U 一s,5φ",",eM
0.0 ∙...............................................
Q 25	50	75 ICO 125 150 175 200
Number of Iterations
Figure 2:	A plot of the real sinusoid sampled on the interval from 0 to 2π (Left). GAN generated
samples for NHG dynamics (Center). The performance of GDA vs. NHG dynamics on GANs as
measured by Wasserstein-1 distance (Right).
7 Discussion
We proposed a novel version of Gradient Descent Ascent dynamics in Hidden Convex-Concave
games, a subset of non-convex non-concave games. In this class of games, which includes GANs, the
utility is convex-concave in the function space. Still, training happens in the parameter space, where
the mappings between the input and output are non-convex non-concave functions. We explored
the dynamics of gradient flows induced by different Banach spaces (e.g., the Fischer information
geometry) that led to the discovery of Lyapunov functions suited to these geometries. Our analysis
of the convergence of our proposed type of dynamics, Natural Hidden Gradient (NHG) dynamics,
uses ideas from Game Theory and Dynamical Systems. We proved global convergence guarantees
for NHG in HCC games to local stationary points via Lyapunov function analysis in the finite-sum
case. To the best of our knowledge, such a non-local convergence result in HCC games and GAN-
like settings, is one of the first of its kind. We also show promising experimental results on practical
GANs using NHG and standard gradient approximation techniques such as KFAC. We are aware that
the current formulation of NHG may be challenging to scale up to large neural networks because
of the tensor pseudo-inversion step that is part of the dynamics. Investigating experimentally novel
versions of Gradient Descent Ascent dynamics with a richer set of experiments and developing
techniques to scale our results for larger neural networks is a natural direction for our future work.
9
Published as a conference paper at ICLR 2022
Statement of Reproducibility
We made sure to provide sufficient details to ensure the reproducibility of our results. The complete
proofs of the theoretical results can be found in §A, and all the assumptions have been stated and
are referenced in each statement. We provide details regarding the experimental results, such as
code language, required libraries, and parametrization to execute and reproduce the experiments in
section 6 and §B. We also include the source code files and the necessary input files in the supple-
mentary material that accompanies this work.
Acknowledgements
The authors would like to acknowledge Joey Bose for his help on the GANs experiments.
This research/project is supported in part by the National Research Foundation, Singapore under
its AI Singapore Program (AISG Award No: AISG2-RP-2020-016), NRF 2018 Fellowship NRF-
NRFF2018-07, NRF2019-NRF-ANR095 ALIAS grant, grant PIE-SGP-AI-2020-01, AME Pro-
grammatic Fund (Grant No. A20H6b0151) from the Agency for Science, Technology and Research
(A*STAR) and Provost’s Chair Professorship grant RGEPPV2101. This work is supported by the
Canada CIFAR AI Chair Program and an IVADO grant.
References
Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In The 22nd International Conference on Arti-
ficial Intelligence and Statistics. PMLR, 2019.
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019.
Shun-ichi Amari. Differential-geometrical methods in statistics. Lecture Notes on Statistics, 28,
1985.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2), 1998.
M Arbel, A Gretton, W Li, and G Montufar. Kernelized wasserstein natural gradient. In Interna-
tional Conference on Learning Representations, 2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning. PMLR, 2017.
Avishek Joey Bose, Gauthier Gidel, Hugo Berrard, Andre Cianflone, Pascal Vincent, Simon
Lacoste-Julien, and William L Hamilton. Adversarial example games. In NeurIPS, 2020.
Haoyang Cao and Xin Guo. Approximation and convergence of GANs training: an SDE approach.
arXiv preprint arXiv:2006.02047, 2020.
Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:
Chaos and butterfly effects of online learning in zero-sum games. In Conference on Learning
Theory. PMLR, 2019.
Yun Kuen Cheung and Yixin Tao. Chaos of learning beyond zero-sum and coordination via game
decompositions. In ICLR, 2021.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In NeurIPS, 2018.
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained
min-max optimization. STOC, 2021.
Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. Efficient methods for structured
nonconvex-nonconcave min-max optimization. In ICML, 2021.
10
Published as a conference paper at ICLR 2022
Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis. Minimax estimation of con-
ditional moment models. In NeurIPS, 2020.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
ICLR, 2017.
Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in stackelberg
games: Equilibria characterization, convergence analysis, and empirical study. In International
Conference on Machine Learning. PMLR, 2020.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Poincare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In NeurIPS, 2020.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Solving
min-max optimization with hidden structure via gradient descent ascent. In NeurIPS, 2021.
Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of GANs using variational
inequalities. arXiv preprint arXiv:1808.01531, 2018.
Gauthier Gidel, David Balduzzi, Wojciech Marian Czarnecki, Marta Garnelo, and Yoram Bachrach.
Minimax theorem for latent games or: How i learned to stop worrying about mixed-nash and love
neural nets. AISTATS, 2021.
I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, and Y Ben-
gio. Generative adversarial nets. In NeurIPS, 2014.
Marc Harper. Information geometry and evolutionary game theory. CoRR, abs/0911.1383, 2009.
URL http://arxiv.org/abs/0911.1383.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint
arXiv:1606.08415, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30,
2017.
Josef Hofbauer, Karl Sigmund, et al. Evolutionary games and population dynamics. Cambridge
university press, 1998.
Ya-Ping Hsieh, Chen Liu, and Volkan Cevher. Finding mixed Nash equilibria of generative adver-
sarial networks. In ICML, 2019.
Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization
algorithms: convergence to spurious non-critical sets. CoRR, 2020.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In International Conference on Machine Learning. PMLR, 2020.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of
GANs. arXiv preprint arXiv:1705.07215, 2017.
Weiwei Kong and Renato DC Monteiro. An accelerated inexact proximal point method for solving
nonconvex-concave min-max problems. SIAM Journal on Optimization, 2021.
Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvex-
nonconcave minimax problems. In NeurIPS, 2021.
Alistair Letcher. On the impossibility of global convergence in multi-loss optimization. In ICLR,
2021.
11
Published as a conference paper at ICLR 2022
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of first-order
approximation in gan dynamics. In International Conference on Machine Learning. PMLR, 2018.
WUchen Li and GUido Montufar. Natural gradient via optimal transport. Information Geometry, 1
(2):181-214, 2018.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave mini-
max problems. In ICML, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research, 2020.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, 2015.
James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha
Sohl-Dickstein, and Samuel S Schoenholz. Rapid training of deep neural networks without skip
connections or normalization layers using deep kernel shaping. arXiv preprint arXiv:2110.01765,
2021.
Eric Mazumdar and Lillian J Ratliff. Local nash equilibria are isolated, strict local nash equilibria in
‘almost all’zero-sum continuous games. In 2019 IEEE 58th Conference on Decision and Control
(CDC). IEEE, 2019.
Panayotis Mertikopoulos and William H Sandholm. Riemannian game dynamics. Journal of Eco-
nomic Theory, 2018.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra(-gradient) mile. In ICLR, 2019.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? In International conference on machine learning. PMLR, 2018.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving
a class of non-convex min-max games using iterative first order methods. NeurIPS, 2019.
Yann Ollivier, Ludovic Arnold, Anne Auger, and Nikolaus Hansen. Information-geometric opti-
mization algorithms: A unifying picture via invariance principles. Journal of Machine Learning
Research, 18(18), 2017.
Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efficient search of first-order nash
equilibria in nonconvex-concave smooth min-max problems. SIAM Journal on Optimization,
2021.
Georgios Piliouras and Leonard J Schulman. Learning dynamics and the co-evolution of competing
sexual species. In 9th Innovations in Theoretical Computer Science Conference (ITCS 2018).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.
Siavash Shahshahani. A new mathematical framework for the study of linkage and selection. Amer-
ican Mathematical Soc., 1979.
Karl Sigmund. The maximum principle for replicator equations. Iiasa working paper, IIASA,
Laxenburg, Austria, 1984.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676), 2017.
Maurice Sion et al. On general minimax theorems. Pacific Journal of mathematics, 8(1), 1958.
12
Published as a conference paper at ICLR 2022
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In NeurIPS 32: Annual Conference on Neural Information Processing Systems
2019, 2019.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In International Conference on Learning Representations, 2020.
Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class
of nonconvex-nonconcave minimax problems. In NeurIPS, 2020.
Guojun Zhang, Pascal Poupart, and Yaoliang Yu. Optimality and stability in non-convex smooth
games. arXiv preprint arXiv:2002.11875, 2020.
A Omitted proofs
A.1 Omitted proofs of Section 3.1
Proposition A.1. Let us consider P(X) the space of probability distributions on a set X ⊆ R with
the metric induced by the KuUback-Leibler (KL) divergence. If n := |X| = dim S, and pθ :二 θ ∈
P (X), then the natural gradient flow of the Fisher information matrix in (8) is the natural gradient
flow of the Shahshahani metric induced by the metric tensors in (9).
Proof. All we need to show is that Fθ = Sθ , ∀θ ∈ S. Simply, note that ∀i, j ∈ [n]:
(Fθ )i,j = -Ex 〜pθ
一 ∂2 log pθ (x) 一
_ ∂θi∂θj _
d 2 lθg θX Q _
-X∈X ^Wx 二 X∈X
∂2 log pθ (x)
-T	∂θi∂θj	pθ(x)
x∈X	i j
δx,iδx,j	δi,j	/u ʌ
—Z— 二 丁 = (Sθ)i,j .
θx	θi
□
A.2 Omitted proofs of section 3.2
Let G = (n, S := [m1] × . . . × [mn], u : S → Rn) be a n-player (exact) potential game with payoff
function u and potential Φ : S → R, and let M := ∆m1 × . . . × ∆mn be the mixed-strategy space
of G. We are going to consider the RD of G in int M = int ∆m1 × . . . × int ∆mn as given in the
following proposition.
Proposition A.2. The Replicator Dynamics of G in int M are given by the following dynamical
system of equations:
A . 八	Γ /	∖ 1	TTɔ	r / -.∖ι∖ 八
θi,j := θi,j (ESk ~θk[ui(sj, s-i)] - Esk~θk[ui (s)]) = θi,j
k∈[n]
k∈[n]
d∂Ψθ - Ψ(θ)
∂θi,j
(D4)
where Ψ(θ) := ESi 〜θi [Φ(s)] is the the expected potential function of the game.
i∈[n]
Proof. First note that
∂ψθ) = XY θk,Sk φ(s) ∂∂θfi = XY θk,Sk φ(s)δj,Si
,j s k6=i	,j s k6=i
=XY
θk,sk Φ(j, S-i)= ESi 〜θi[Φ(j, s-i)].
s-i k6=i	i∈[n]
13
Published as a conference paper at ICLR 2022
From which follows that
λ	八 ∕τπ	Γ / ∙	∖	/	∖ T \	八 ∕τπ	Γ τ / .	∖	τ /	∖ T \
θi,j = θi,j (Esk ~θk[uij, ST)- Ui(S)D = θi,j (Esk ~θk [φj, ST)- φ(S)D
k∈[n]
k∈[n]
∂Ψ Ψ(θ)
k^θi7
□
Next, let us consider a point θ ∈ int M, and note that the following proposition holds:
Proposition A.3. For any θ ∈ int M = int ∆m1 × . . . × int ∆mn, the tangent plane of int M at θ
is
Tθ(intM) = {μ ∈ Rm1 X ... X Rmn | £3,j = 0}.
j
(21)
Proof. We first note that int M is an open-subset of the surface D := {μ ∈ Rm1 X ... X Rmn |
Ui(μ) = 1}, where Ui(μ) = Pj μ%,j = 1; hence, Tθ(intM) = Tθ D, UP to isomorphism. Let
r(t) be a smooth curve in D with r(0) = θ. Then, by definition, Ui(r(t)) = 1 for all i, and, by
differentiating with respect to t, we have
VUi(r(t)) ∙ d r(t) = ddtUi(r(t)) = ' 1=0 .
dt dt	dt
However, £r(t) is tangent to D for all t; hence, £r(t) ∣	∈ Tθ D. It follows that VUi(r(0))
VUi(θ) is normal to Tθ D for all i, and ,thus, we have
Tθ D ⊆{μ ∈ Rm1 X ... X Rmn ∣hVUi (θ), μ =0} = {μ ∈ Rm1 X ... X Rmn | fμij =0}.
j
Finally, notice that dim{μ ∈ Rm1 X ... X Rmn | Pj μ%j = 0} = dimD, which implies the
proposition.
□
Proposition 1. The Replicator Dynamics of a potential game G with potential function Φ is an
(extended) Shahshahani gradient in int(∆m1 X . . . X ∆mn) having potential Ψ(θ) :=Esi 〜4[Φ(s)].
i∈[n]
Proof. Let θ ∈ intM := int ∆m1 X . . . X int ∆mn. Then for every ξ ∈ Tθ (int M) we have:
hθ ξ∖θ = X ɪθ. .ξ. .	= X f dψ(θ)	- ψ(θ)∖ ξ. .	= X	dψ(θ) ξ. .	- ψ(θ) X ξ..
hθ,ξiθ =匚 θi j θi,jξi,j	=，I ∂θi,j ψ(θ)尸i,j	=匚	∂θi,j ξij "⑼乙ξij
=X dψθ) ξi,j = vψ(θ) ∙ξ.
∂θi,j
T T	1 IC ∙ . ∙	1	. 1 . X—7 ,ʃ, / Λ ∖	A t	. 1 1 ʌ 1	∙ .t	..
Hence, by definition, we have that VΨ(θ) = θ, where the Del operator is defined with respect to
the Shahshahani metric, which implies the lemma.
Now, let us consider a different class of games, which we define as 2-Team Zero-Sum games.
14
Published as a conference paper at ICLR 2022
Definition 2. A 2-Team Zero-Sum game, G = (m := m1+m2, S := [n1] × . . . × [nm], u : S → Rm),
is a game between m players with strategy-space S whose utility function, u, satisfies the following:
.()=m m1ιφ(S), ifi ∈ [mi]
i	I- m^- Φ(s), otherwise
∀s ∈ S
(22)
for some function Φ : S → R.
We are going to, collectively, refer to the first mi players of a 2-Team Zero-Sum game, G = (m :=
mi + m2, S := [ni] × . . . × [nm ], u : S → Rm ), as team 1 and to the rest of them as team 2.
Notice that if we consider each team as a single player, then the two teams are playing a 2-Player
Zero-Sum game, G0 =	(2, S0 := Si ×	S2,u0 :	S0	→ R2) where Si	=	[ni]	× ...	× [nm1], S2	=
[n1+i × . . . × nm ], and the utility function u0k is the total utility of the corresponding team k, i.e.
∀s ∈ S we have
m1 1	m	1
u0ι(s) = X	Φ(s) = Φ(s) and u2(s) = - X	Φ(s) = -Φ(s).
mi	m2
i=i i i=m1+i 2
(23)
Note that, by definition, the 2-Team Hidden Matching Pennies game (see section 5) is a a 2-Team
Zero-Sum game. Let us consider the Replicator Dynamics of G given by the following dynamical
system of equations:
A . 八 /irɔ	Γ / ∙ -. ∖ 1 TTɔ	Γ / -. ∖1∖
θi,j := θi,j (Esk ~θk[uij, s-i)] - ESk~θk [Ui(S)])
f -----θi,jEsk 〜θk [φj, s-i) - φ(S)],	if i ∈ [m1]
=	1i
I------θi,jEsk〜θk [Φ(j, s-i) - Φ(s)],	otherwise
m2
where, as before, We can rewrite (D5) in terms of Ψ(x) := ESk〜)忆 [Φ(S)] as
1	∂Ψ(θ)
---θi,j (	- ψ(θ)),	if i ∈ [mi]
mi	∂θi,j
1	∂Ψ(θ)
- θi,j (	- ψ(θ)),	otherwise .
m2	∂θi,j
(D5)
(D6)
Notice that the Replicator equations of each team are similar to the Replicator equations ofa Poten-
tial game. In fact, it’s not difficult to prove the following lemma.
Lemma 1. Let G = (m := mi + m2, S := [ni] × . . . × [nm ], u : S → Rm ) be a 2-Team
Zero-Sum game. If we assume the strategies of team 2 to be time-invariant, then the Replicator
Dynamics ofG, given by (D5) (or (D6), equivalently), is a (mi-scaled) Shahshahani gradient in
int M := int∆nι X ... X int∆nm with potential Ψ1(θ) = Ψ ◦ ∏ι(θ) where Ψ(θ) := Esi〜eJΦ(s)],
and Πi : int M → int M is the natural projection
θij , ifi in team k
Πk,i,j(θ)=	i,0j,, otherwise.
(24)
Likewise, if we assume the strategies of team 1 to be time-invariant, then the Replicator Dynamics
ofG is a (m2-scaled) Shahshahani gradient in int M with potential Ψ02 = -Ψ ◦ Π2(θ).
Proof. This proof is similar to the proof of Proposition 1, but we need to perform the correct pro-
jection before applying the definition of a gradient flow. Let θ ∈ int M. Without any loss of the
generality, let us assume that the strategies of team 2 are time-invariant, i.e., θi,j = 0 for all i ≥ mi.
Then, for every ξ ∈ Tθ(int M) we have
15
Published as a conference paper at ICLR 2022
m1
hθ, ξiθ = X Uθij ξij = XX
i,j	i=1 j
m1 1
eθi,j m1
Y - Ψ(θ)) ξij
X X ∂ Ψ(θ)g	ψθ X Xf —X X ∂ Ψ(θ)g
之 ∑ Fξij - ψ⑹ g jξij =之 j Fξij
Σ
∂Ψ(Π1(θ))
∂θi,j
ξij = vψ1(θ) ∙ ξ.
Hence, by definition, We have that dΨ1(θ) = θ[, which implies the theorem.
□
Proposition 2 (Metric tensors of the model space). Under mild regularity assumptions, we have
that, for any θ ∈ RM,
kFθ+δθ - Fθk2 = hδθ, Aθδθi + o(kδθk2) where Aθ := Ex〜Px [VθFθ(x)VθFθ(x)|]. (11)
Proof. The assumption on F we need to prove this results are the following:
•	θ 7→ Fθ(x) is almost surely differentiable, i.e., almost surely (in x),
Fθ+δθ(x)=Fθ(x)+ hVFθ(x),δθi +kδθkf(x,θ), ∀θ ∈RM,	(25)
where f(x, δθ) →δθ→0 0 almost surely in x.
•	For small enough δθ ∈ RM, The remainder of the Taylor expansion of Fθ has a finite
variance, i.e.,
Ex〜Px [f (x, δθ)2] < +∞ .	(26)
A consequence of these two assumption is that (by dominated convergence theorem and Jensen’s
inequality)
Ex〜Px[∣f(x,δθ)∣] →δθ→o 0 and Ex〜p,[f(x,δθ月 →δθ→o 0.	(27)
Let us now prove the desired property. We start by doing a Taylor expansion of θ 7→ Fθ(x),
Fθ+δθ(x)=Fθ(x)+ hVFθ(x), δθi +kδθkf(x,θ).
Then we have that
kFθ+δθ - Fθk2 = Ex〜Px [(hVFθ(χ),δθi + f(x, θ)kδθk)2]
=Ex 〜px [(VFθ (χ)lδθ)2] + o(δθ)2
=Ex 〜px[δθl VFθ (X)VFθ (χ)lδθ] + o(δθ)2
=δθlEx 〜px [VFθ (X)V Fθ (χ)l]δθ + o(δθ)2;
which concludes the proof.
□
Proposition 3 (Natural Hidden Gradient dynamics). The flow induced by the geometry (11) is
θ= -Aθ E(x,xo)〜p[Vθ Lx,xo (Fθ (χ),Gφ(χ0))]
φ = BφE(x,x0)〜p[vΦLx,x0(Fe(X) gφ(XO))
(D7)
where Ae ：= Ex〜pxVeFe(x)VθFe(x)|] and Bφ := Ex，〜px, [VφGφ(x)VφGφ(x)|] and Ct
denotes the pseudo-inverse of a matrix C.
16
Published as a conference paper at ICLR 2022
Proof. The proposition directly follows from Proposition 2 applied to the definition of (7). However,
in order to convey more intuition we could use the definition that the natural gradient flow of the
objective function f induced by the distance d if
11
θ = arg min lim f (θ + λδθ) + —2 d2(Fθ+λδθ, Fθ).
δθ	λ→0 λ	2λ
By using the L2 distance and noting that when λ → 0 We have f (θ + λδθ) = f(θ) + λVf (θ)lδθ 十
o(λ) and 2λ2 ∣∣Fθ+λδθ 一 Fθ∣∣2 = 2 (δθ, Aeδθ) + o(1) we have that the RHS of the equation above
is
arg min Vf (θ)lδθ + 1 (δθ, Aeδθ);
δθ	2
which is minimized for δθ = -AVf (θ).
□
A.3 Omitted Proofs of section 4
Proposition 4. In the uni-dimensional case, the Natural Hidden Gradient flow D1 takes the form of
θ _ _VθL(f(θ)2g(φ))	d φ "(f(θ),g(φ))	(D8)
θ =	kVθf (θ)k2	and φ = kVφg(φ)k2 .	(D8)
Proof. For the uni-dimensional case, Ae = Vf(θ)Vf (θ)1 is a rank-1 matrix that projects any
vector in the direction of Vf (θ) and scales it by ∣Vf (θ)∣2. Thus, by definition of the pseudo-
inverse, we have that for any vector u ∈ Rd :
Vf(θ)hu, Vf(θ)i
∣Vf(θ)k4
Finally, notice that
VeL(f(θ),g(φ))=Vf(θ)
∂ (f,g(φ))
-∂f-
f=f(e)
which leads to the stated proposition.
□
Theorem 1. Let Ψ be the payoff of an HCC game (12) and consider the dynamics (D2). Then,
V(θ, φ) := 2(f (θ) - f *)2 + 1 (g(φ) - g*)2	(13)
is a Lyapunov function, i.e., it is positive, non-increasing, and null if and only if it evaluated at a
game solution. Moreover, if L is strictly convex-concave, we have that V is decreasing and that any
limit point (θ, φ) satisfies Ve L(f (θ), g(φ)) = VφL(f (θ), g(φ)) = 0.
Proof. By taking the time derivative of V(θ, φ), we get
, , ,
V(θ, φ)
-(Vf (θ)(f (θ) - f *))1
Vf(θ) ∂L(f,g(φ))∖
∣Vf(θ)k2	∂f	f=f(e)
+ (Vg(φ)(g(φ) - g*))1
Vg(φ) ∂L(f(θ),g)∣
kVg®k2	dg	∣g=g(Φ)
-(f(θ) - f*)
∂L(f,g(φ)
∂f
f=f(e)
+ (g(φ) -
∂L(f(θ)叫
dg	∣g=g(φ)
≤0
17
Published as a conference paper at ICLR 2022
where the last inequality holds because L is convex-concave. Specifically, let L : RM × RN → R
be any differentiable convex-concave function with saddle point (f *,g*), then We have, respectively
-hf — f*, VfL(f,g)i	≤ L(f*,g)- L(f,g)	and(g	- g*,	NgL(f,g)i	≤	L(f,g)-	L(f,g*).
By adding the two inequalities, we get
-hf — f *, Vf L(f, g)i + (g - g*, VgL(f, g)i ≤ L(f *,g) - L(f, g*) ≤ 0	(28)
where the last inequality holds because (f*,g*) is a saddle point. Hence, by definition, L is a
Lyapunov function.
Finally, when L is strictly convex-concave, if there exists a limit point of (θ, φ) that is not a point
where Vθ L(f (θ), g(φ)) = 0 and VφL(f (θ), g(φ)) = 0 then by strict convex-concavity we get
that V should decrease by a “significant enough amount” to create a contradiction with the fact that
V does converge.
□
Next, before proving the general case of Theorem 1, we’ll have to prove the following lemma:
Lemma 2. Let (ui)i∈[n] be a linearly independent family of vectors on Rd. Then, the matrix
n
A :=	uiui| .
i=1
(29)
is the Gram matrix OfthefamiIy (uj∈[n] and and we have that (ui, Atuji = δi,j, ∀ij ∈ [n].
Proof. Let us introduce the matrix P := [u1, . . . , un]|; we can easily verify that A = P|A. Let
P = UDV| be the SVD decomposition of P and, thus, At = V (D|D)tV | where D|D is a
diagonal matrix. Then, we have
hui,Atuki = hP|e(i), AtP|e(j)i = hV D|U|e(i), V (D|D)tV |V D|U|e(j)i
= he(i),UD(D|D)tD|U|e(j)i .
To conclude this lemma, we just need to notice that the matrix D is a matrix with non-zero entries
on the diagonal and, since we assumed that the vectors in (ui)i∈[n] are linearly independent, we
have that Di i > 0, ∀i ∈ [n]. Thus, by a direct computation, we get that
D(D|D)tD| = In
which leads to hui, Atuii = δi,j.
□
Theorem 2. Let Ψ be the payoff of a finite-sum HCC game given by (14) and consider the game
dynamics in (D3). Under Assumption 1, Assumption 2, we have that the quantity
1n	1m
V(θ, φ) := 2- X(Fθ(Xi)- F*(xi))2 + 2m X(Gφ(xj) — G*(xj))2	(15)
2n i=1	2m j=1
is a Lyapunov function, i.e., is positive, non-increasing and null if and only if evaluated at a game
solution. Moreover, if L is strictly convex-concave, V is decreasing as long as (θ, φ) = (θ*, φ*)
and if L is a μ-strongly convex-concave function we have that V is decreasing exponentially as
V(θ, Φ) = V(θo, φo)exp(-μt).
18
Published as a conference paper at ICLR 2022
Proof. Similarly as the proof of Theorem 1, we consider the time derivative of V (θ, φ). For com-
pactness, we are going to simplify the notation slightly by setting Fi := Fθ(xi), Gj := Gφ(x0j),
Fi := F*(xi), Gj = G*(xj), and Lij := Lχiχ0..
nm
V(θ, φ) = - X(Fi- Fi*)hvθFi, θi + mm X(Gj- Gj)EφGj, φi
1n
=-η2~ £(Fi- Fi*)hvθFi, AθE vθ Lkj (Fk ,Gj )i
i=1
(j,k)∈[m]×[n]
m
+ nm X(Gj - Gj)hvΦGj, Bφ X vΦLi,k (Fi,Gk )i
一R X (Fi- FOF aθ vθ Fk i dLjF≡ IF
i=1	(j,k)∈[m]×[n]
+J X (Gj- Gj) XhvΦGj, bφ vΦGk i dLi,∂Gi,G)
j=1	(k,i)∈[m]×[n]
From Lemma 2 We have that EeFi, A∖NgFki = n ∙ δi,k, ∀i ∈ [n], k ∈ [m], and that
hvφGj, BφvφFki = m ∙ δj,k. Hence, it follows
∂Li,j (Fi,G)∣
∂G IG=Gj
V(θ, φ) = - ɪ X (Fi- Fi) dLi,j^F,Gj) I +ɪ X (Gj- Gj)
nm	∂F	F =Fi	nm	j
i∈[n]	i∈[n]
j∈[m]	j∈[m]
= -h∂FL(F,G),F-Fji + h∂GL(F, G), G - Gji
where [∂FL(F, G)]ci,j) := dLij尸)∣	, ∂gL(F, G)]ci,j) := %Gi,G) ∣ ,and F, G, F *,
F =Fi	G=Gj
and Gj are indexed by (i, j), as well (while repeating vector elements as necessarily). Thus, using
the same reasoning as in Theorem 1, it follows that if L is convex-concave we have that V is non-
increasing, and, if L is strictly convex-concave, V (θ, φ) < 0 whenever (θ, φ) 6= (θj, φj). Finally,
if L is μ-strongly convex-concave, we have by definition that
-h∂FL(F,G),F-Fji +h∂GL(F,G),G-Gji
≥ -μ(kF - F*k2 + kG - G*k2) = -μV(θ, φ)
Thus we conclude that V(θ, φ) ≤ V(θο, φο) exp(-μt).
□
A.4 Omitted proofs of section 5
In order to prove (17), and (D9) we are, first, going to prove the following useful lemma:
Lemma 3. Let hg (i) := Ex-Ber© )[XOR(xι,..., xj] where θ ∈ [0, l]n ,and i ≤ n. Then the
following equality holds:
l - 2hθ(i) = Yi (l -2θj) .
j=1
(30)
Proof. The easiest way to prove this relationship is by induction on n ∈ N. For n = l, we only
need to verify that (30) holds for i = l. Indeed, we have,
19
Published as a conference paper at ICLR 2022
1 - 2hθ(I) = I- 2Ex〜Ber(θι)[XOR(XI)] = 1 - 2Eχ^Ber(θι) [χ1] = 1 - 2θ1 .
Next, let us assume that Lemma 3 holds for some n = n0 ∈ N. We are going to prove that Lemma 3
also holds for n = n0 + 1, and this comes down in proving that (30) holds for i = n0 + 1:
1 -2hθ(n0+1) = 1 - 2E[XOR(x1,..., xn0+1)] = 1 -2E[E[XOR(x1,...,xn0+1) | xn0+1]]
= 1 -2(θn0+1E[XOR(x1,...,xn0,1)]+(1 -θn0+1)E[XOR(x1,...,xn0,0)])
= 1 - 2(θn0+1E[1 -XOR(x1,...,xn0)]+(1 - θn0+1)E[XOR(x1,..., xn0)])
= 1 - 2(θn0+1(1 - hθ (n0)]) + (1 - θn0 +1)hθ (n0)) = (1 - 2θn0+1)(1 - 2hθ (n0))
n0	n0+1
=(1-2θn0+1)Y(1-2θj)= Y(1-2θj).
j=1	j=1
And we that, the proof by induction is complete.
□
Proposition A.4. Let G = (n := n1+n2, S := {0, 1}n, u : S → Rn) be a 2-Team Hidden Matching
Pennies game with payoff function given by (16). Then, the expected payoff of the i-th member of
team k is given by (17).
Proof. The first equality follows, trivially, from the definition of Φ(s). All is left to prove is that
-ESk,i〜Ber(θk,i)[Φ(s)] = (1 - 2Es1.〜Berg) [XOR(sι )])(1 - ZEs?.〜Ber。/[XOR(s2 )]) for all
θ ∈ [0,1]n. We define, Xk = XOR(Sk), k ∈ [2]. Note that, by definition, Xk 〜Ber(Pk), where
Pk := Esk,i〜Ber(θk,i)[XOR(sk)]; hence,
ESk,i〜Ber(θk,i)[φ(s)] = ESk,i〜Ber(θk,i)[1 - 2 ∙ 1XOR(sι )=XOR(s?)] = EXk〜Ber(Pk)[1 - 2 ∙ lX1=X2]
=EXk〜Ber(Pk)[(1 - 2 XOR(x))] = 1 - 2hp(2) = (I- 2pl)(1 - 2p2)
=(1 -叫"Ber(θι,i)[XOR(si)])(1 -叫 2遥〜Ber3) [XOR® )]).
□
Proposition A.5. Let G = (n := n1+n2, S := {0, 1}n, u : S → Rn) be a 2-Team Hidden Matching
Pennies game with payoff function given by (16). Then, the RD ofG is given by the dynamical system
of equations:
2
θk,i = (-1)	nθk,i(I- θk,i) ɪɪ ɪɪ (1 - 2θk,i) ∙	(D9)
nk	k0∈[2] i0∈[nk0]
(k,i)6=(k0 ,i0 )
Proof. From (17) we have that
ESk,i~Ber(θk,i) [uk,i(s)]
二(-1)k-1 — (1 -叫 ι,i 〜Ber(θι,i)[XθR(sl)])(1 -叫?* 〜BerQQ [XOR(s2 )])
(-1)k-1 — (I-	2hθι (nι)(1	- 2hθ2 (n2)	= (-1)k-1—	Y Y (I- 2θk,i)	.
nk	1	2	nk	k∈[2]i∈[nk]
Then, by the definition of RD of G, we get
20
Published as a conference paper at ICLR 2022
θk,i : = θk,iESk,i〜Ber(θk,i) [uk,i (O, s-k,i) ― uk,i (S)]
/	\
= (-1)k-1:θk,i	Y Y	(1 - 2θk,i)- Y Y (1	- 2θk,i)
k	k0∈[2]	i0∈[nk八	k0∈[2] i0∈[nk0]
(k,i)6=(k0,i0)
2
=(-I)	黑θk,i(I - θk,i) ɪɪ ɪɪ (I - 2θk,i) .
nk	k0∈[2] i0∈[nk0]
(k,i)6=(k0,i0)
□
Theorem 3. Consider the Replicator Dynamics of G. Given any interior initial condition, the
resulting orbit is a cycle that satisfies the following n1 + n2 - 1 independent invariant functions:
2
Vi1,i2(θ) = X nk[log(θk,ik) + log(1 - θk,ik)], ik ∈ [nk],k∈ [2].	(19)
k=1
Proof. For all k ∈ [2] and i ∈ [nk], we have
∂Vi1i2(θ)
∂θk,i
(nk (I - 2θk,J
θk,i(1 - θk,i)，
[	0,
if (k, i) ∈{(k0,ik0) | k0∈ [2]}
otherwise .
Subsequently, we have
V⅛(θ) = hVθ ¼1i2 (θ), θ i =2 ɪɪ ɪɪ (1 - 2θk,i) - 2 ɪɪ ɪɪ (1 - 2θk,i)=0 .
k∈[2] i∈[nk]
k∈[2] i∈[nk]
Observe that the above imply the existence of n1 + n2 - 1 independent invariant functions. Hence,
the dynamics converge to a limit set of a single degree of freedom, i.e., a cycle.
□
Proposition A.6. Let G = (n := n1 + n2, S := {0, 1}n, u : S → Rn) be a 2-Team Hidden
Matching Pennies game with payoff function given by (16). Given a mixed-strategy profile (θ, 1 -
θ), θ := (θ1, θ2) ∈ (0, 1)n, (θ, 1 - θ) is an equilibrium of G if and only if ∃ik, ∀k ∈ [2] such that
θk,ik = 2 ,Vk ∈ [2].
Proof. We know that G is equivalent to a Hidden Matching Pennies game (Example 1) with
payoff Ψ(θ) = (1 — 2f(θι))(1 — 2g(θ2)), where f(θι) := Es⑺〜Ber(Jy)[XOR(sι)], and
g(θ2) := Es2 i〜Ber(θ2 i)[XOR(s2)] are its hidden mappings. The only fully mixed-Nash equilib-
rium of this Hidden Matching Pennies game is (f (θι),g(θ2)) = (11, 2); hence, a mixed-strategy
profile (θ, 1 一 θ) is a fully mixed-Nash equilibrium of G, if and only if f (θι) = g(θ1) = 1. From
f(θι) = 1 we get
Esι,i〜Ber(θι,i)[XOR(SI)] = 2 ^⇒ 1 - 2hθ1 (HI)= 0
^⇒ Y(I - 2θ1,i) = 0
i=1
< ⇒ ∃i1 ∈ [n1] : θ1,i = 2
□
21
Published as a conference paper at ICLR 2022
We remark that, for any ik ∈ [nk], k ∈ [2],
22
Viι,i2 (θ) := X nk [log(θk,ik) + log(1 - θk,ik)] = X2 ∙ nk [ 2 log(θk,ik) + 1 Iog(I- θk,ik)]
k=1	k=1
2
=X 2 ∙ nkH(Ber( 1), Ber(θk,ik))
k=1
where H(p, q) is the cross-entropy of a distribution q relative to a distribution p. In other words,
every one of the invariant functions in (19) is the weighted sum of the cross entropy of two mixed-
strategies (of the i1-th member of team 1, and the i2-th member of team 2) relative to the uniform
strategy. That is, each invariant function measures (UP to a constant) the KUllback-Leibler diver-
gence of two opposing members to an actual equilibrium of the game. Notice that, for any equi-
librium Point of the Hidden Matching Pennies game, there exists at least on such Pair of oPPosing
members i1 ∈ [n1], and i2 ∈ [n2] such that
DKL(Ber(1) ∣∣ Ber(θ1,i1)) = DKL(Ber(1) ∣∣ Ber(θ2,i2)) =0.
For the rest of this section we are going to consider the RD of a 2-Team Hidden Matching Pennies
game G = (n := n1 + n2, S := {0, 1}n, u : S → Rn) in a restricted setting, given by the following
dynamical system of equations:
θk,i =
0,
0,
0,
2
(—1)	---θk,i(I - θk,i)Dk,i(θ),
nk
if θk,i ∈/ Sk,i
if θk,i =αk,iand(-1)kDk,i(θ) <0
if θk,i = βk,i and (-1)kDk,i(θ) > 0
otherwise
(D10)
where Dk,i (θ) :=	k0∈[2]	j∈[n 0] (1 - 2θk0,j), k ∈ [2] i ∈ [nk], and where we restrict each
(k0,j)6=(k,i)
mixed-strategy Profile (θ, 1 - θ) such that θk,i ∈ Sk,i := [αk,i, βk,i], ∀k ∈ [2], i ∈ [nk]. We let
Ω be an orbit defined by this RD whose initial conditions satisfy Assumption 3. This assumption
serves a dual PurPose. To begin with, it ensures that any orbit is initialized inside the restricted
parameter space that is defined by Sk,i. Furthermore, it makes sure the initial strategy profile has
full support, i.e., (θk,i(0), 1 - θk,i(0)) is an interior point of the simplex for all k ∈ [2], and i ∈ [nk].
It is easy to see that any dimension of the strategy space initially without support is impossible to be
updated by the RD in (D10); hence, it would be irrelevant for the analysis. Before we proceed, we
are going to introduce a couple of useful lemmas.
Lemma 4. ∀k ∈ [2], and i ∈ [nk] if θk,i(0) ∈ Sk,i, then θk,i(t) ∈ Sk,i, ∀t ≥ 0.
Proof. Let k ∈ [2] andi ∈ [nk] such that θk,i(0) ∈ Sk,i =⇒ αk,i ≤ θk,i(0) ≤ βk,i. We are going
to prove our case by abduction.
Suppose ∃t0 > 0 such that θk,i (t0) ∈/ Sk,i and, without any loss of the generality, let us assume that
θk,i (t0) > βk,i. We let T = {t ∈ (0, t0) | θk,i (t) = βk,i and we note that, since θk,i (0) ≤ βk,i, it
is implied by the continuity of θk,i (t) (Equation D10) and by the Intermediate Value Theorem that
T = 0. Finally We define tmaχ = max(T).
Let us now consider the value of θk,i(t) for some t ∈ (tmax, t0) and observe that if θk,i (t) = βk,i
we have
t ∈ T =⇒ t ≤ max(T)
= tmax =⇒ tmax < t ≤ tmax .
22
Published as a conference paper at ICLR 2022
This contradiction implies that θk,i (t) 6= βk,i. However, if θk,i (t) < βk,i, the Intermediate Value
Theorem, once again, implies ∃t0 ∈ (t, t0) such that θk,i (t0) = βk,i, which as before implies tmax <
t0 ≤ tmax. Hence, it must be the case that, for all t ∈ (tmax, t0):
θk,i(t) > βk,i (=⇒) θk,i(t) = 0.
However, by applying the Mean Value Theorem, if follows ∃t0 ∈ (tmax, t0) such that
；	Z 、
θk,i(t)
θk,i (0) ― θk,i(tmaX)
t0 - tmax
θk,i(0) - βk,i
t0 - tmaX
>0,
which is once again a contradiction.
□
Lemma 5. Under Assumption 3, if Sk = 0, ∀k ∈ [2] then thefollowing hold:
(a)	Ψ1,i1 (θ(t))Ψ2,i2 (θ(t)) < 0for all ik ∈ [nk], k ∈ [2], andt ≥ 0.
(b)	Ψk,ik(θ(t)) preserves sign for all ik ∈ [nk], k ∈ [2].
Proof. Since Sk = 0 for all k ∈ [2], it follows, by definition, that
2 ∈ Sk,i, ∀k ∈ [2], i ∈ [nk]
Furthermore, by Assumption 3, we have that θk,i(0) ∈ Sk,i for all k ∈ [2], i ∈ [nk]. Hence, it
follows, by Lemma 4, that ∀k ∈ [2], i ∈ [nk], and t ≥ 0:
θk,i(t) ∈ Sk,i =⇒ θk,i ⑴=2 =⇒ 1 - 2θk,i⑴=0.
Then, by Equation 17, we have
(-1)k-1
Ψk,i(θ(t)) = ——(1 — 2Esι j 〜Ber(θι , ) [XOR(si )])(1 - 2Es, jcBer@ , )[XOR(s2)])
nk	,	,	,	,
(-1)k-1
；(I- 2hθl)(1- 2hθ2 )
(-1)k-1
nk
n1
Y(1 -
j=1
2θ1,j ) Yn2 (1 - 2θ2,j )
j=1
(-i)k-1
nk
2 nk0
Y Y(1 - 2θk0,j(t)) 6= 0 .
k0=1 j=1
That implies (a). We can now condition on Ψk,i(θ(0)). Since Ψk,i(θ(t)) 6= 0 for all t ≥ 0, it must
be the case that either Ψk,i (θ(0)) > 0 or Ψk,i (θ(0)) < 0; let us, first, assume the former case. We
are going to prove, by abduction, that Ψk,i (θ(t)) > 0, ∀t ≥ 0.
Suppose ∃t0 > 0 such that Ψk,i(θ(t0)) ≤ 0. Since Ψk,i(θ(t)) 6= 0 for all t ≥ 0 it follows that
Ψk,i(θ(t0)) < 0 must be the case. However, by the continuity of Ψk,i(θ(t)) and the Intermediate
Value Theorem, it follows that ∃t00 ∈ (0, t0) such that Ψk,i (θ(t00)) = 0, and that is, indeed, a
contradiction. It follows that it must be the case Ψk,i (θ(t)) > 0, ∀t ≥ 0.
In a similar manner we may prove that Ψk,i (θ(0)) < 0 =⇒ Ψk,i (θ(t)) < 0, ∀t ≥ 0; hence, (b)
holds.
□
23
Published as a conference paper at ICLR 2022
Theorem 4. Under Assumption 3, if Sk = 0, ∀k ∈ [2] ,the restricted RD of G converge to
θ1*,i = θ2*,i =	Γα1,i, ifβ1,i < 2 β1,i , otherwise Γ β2,i, if β2,i < 2 α2,i , otherwise	i ∈ [n1] i ∈ [n2]	θ1*,i = or θ2*,i =	β β1,i,	if β1,i < 2 α1,i ,	otherwise a a2,i,	if β2,i < 2 β2,i ,	otherwise	i ∈ [n1]	
					i ∈ [n2]	(20)
	{^^^^^^^^^^^^ if |S| is even			~^^^^^^{^^^^^^^^^™ if |S| is odd		
where S := {(k, i) | k ∈ [2], i ∈ [nk], a, > 1}.
Proof. We begin by conditioning on the value of |S| and, since the proof is similar in both cases,
and without any loss of the generality, we are going to assume that |S| is even. Since Sk = 0 for all
k ∈ [2],i.e., 1 ∈ Sk,i for all k ∈ [2], i ∈ [nk], and, by Assumption 3, θk,i(0) ∈ Sk,i := [αk,i,βk,i]
for k ∈ [2], i ∈ [nk] it follows that θk,i(0) > 2, if ak,i > 1; θk,i(0) < 2; otherwise. Then, for all
i ∈ [n1], we have
2 nk
Ψ1,i(θ(0)) = - YY(1 - 2θk,j(0))
nk
k=1 j=1
=n- I Y (1- 2θk,j(0)) I ∙ I Y (1- 2θk,j(0))1 > 0,
k ∖(k,j)∈S	J ∖(k,j)∈s	)
since |S| is even. Subsequently, by Lemma 5, Ψ1,i(θ(t)) > 0, ∀t ≥ 0. That implies that
Ψ1,i(θ(t)) > 0, ∀i ∈ [n1], t ≥ 0, and, hence, by Lemma 5, we also have that Ψ2,i(θ(t)) <
0, ∀i ∈ [n2], t ≥ 0. Take any k ∈ [2], and i ∈ [nk]. In order to complete the proof, we’ll have
to condition on the value of k, and on whether (k, i) ∈ S. There are four cases in total that we”ll
have to consider, but, since in all of them we follow a similar reasoning, we’ll just go ahead and
demonstrate the single case of k = 1, and (k, i) ∈ S.
We are going to prove that there exists f ≥ 0 such that θι,i(t) = β1, i, ∀t ≥ f. By ASSUmP-
tion 3, and Lemma 4, we have α1,i ≥ θ1,i(t) ≥ β1,i for all t ≥ 0. Furthermore, since (1, i) ∈ S,
and, since Sk = 0, We have that a1,i ≥ β1,i > 1. That is, θ1,i(t) > 2 for all t ≥ 0, and, hence,
D1,i(θ(t)) := Y Y (1 - 2θk,j(t)) =]或 W ∙ Y Y(1 - 2θk,j(t))
k∈[2] j∈[nk]	1 - 2θ1,i(t)	k=1 j=1
(k,j)6=(1,i)
=n1 ∙ 1- 21(t) ∙ %,i(t) < 0.
Then, by Equation D10 it follows that ∀t ≥ 0:
…/ 2	0,
,	I 一θ1,i(1 - θ1,i)D1,i(θ),
n1
if θ1,i = β1,i
otherwise.
(D11)
Hence, θ1,i = β1,i is the single attracting point of the dynamical system described by D11 and,
hence, by definition, ∃t\ ≥ 0 such that θ1,i(t) = θ, = β1,i, ∀t ≥ t%. Similarly, we can prove
that ∃tk,i ≥ 0 such that θk,i(t) = θ^, ∀t ≥ 小
and by letting t* = max (tk,i)
Theorem 4
follows.
i[nk]
□
24
Published as a conference paper at ICLR 2022
Theorem 5. Under Assumption 3, if Sk = 0, ∀k ∈ [2] ,then the restricted RD of G converge to an
invariant set defined by the |S1| + |S2| - 1 independent invariant functions, Vi1,i2 (θ), ik ∈ Sk, ∀k ∈
[2], where Vi1,i2 (θ) is given as in (19).
Proof. We are going to begin by defining the following time-dependent set:
C⑴ := {{(k,i) | k ∈ [2], i ∈ [nk], θk,i(t) ∈ {αk,i,βk,i}},
if t ≥ 0
otherwise .
We perform a time partitioning based on the values of C(t), t ≥ 0. Specifically, we let P
{(t1, t2) | t1, t2 ∈ T : t ∈/ T, ∀t ∈ (t1, t2)}, where
T := {t ≥ 0 | ∃0 > 0 : C(t - ) 6= C(t), ∀ ∈ (0, 0)} .
It is not difficult to see that the continuity of θ(t) implies that T consists entirely of isolated points
and, due to this fact, P is well-defined. Let I ∈ P be one of these partitions. We are going to prove
that Viι,i2 (θ(t)) ≥ 0, ∀ik ∈ Sk, ∀k ∈ [2], t ∈ I.
First of all, observe that, by definition, the following two properties have to hold:
(a)	C(t1) = C(t2), ∀t1,t2 ∈I
(b)	By Equation D10, we have that ∀t ∈ I,
J	0,
C I (-1)k-1	θk,i(1 - θk,i)Dk,i(θ),
nk
if (k, i) ∈ C(t)
otherwise.
(D12)
For any ik ∈ Sk, k ∈ [2], we proceed by conditioning on the value of C(t), t ∈ I and, since
C(t1) = C(t2), ∀t1, t2 ∈ I, we distinct only three cases:
(a)	(k,ik)	∈	C(t),	∀k	∈ [2], t ∈	I.
(b)	(k, ik)	∈/	C(t),	∀k	∈ [2], t ∈	I.
(c)	∃k, k0 ∈ [2] suchthat(k,ik) ∈ C(t) and (k0,ik0) ∈/ C(t) for all t ∈ I.
The analysis of the first two cases is relatively straightforward. Let t ∈ I; then, if (k, ik) ∈
C(t), ∀k ∈ [2] is the case, then by Equation D12, we have that θk,ik (t) = 0, ∀k ∈ [2] and,
hence,
2
Vii-(θ(t)) = ʤ,m = X (⅛⅛⅛ ∙ 叱 ⑴)=0.
On the other hand, if (k, ik) ∈/ C(t), ∀k ∈ [2] is the case, then, once again, by Equation D12, we
have that θk,i% (t) = (-1)k-1	θk,i(1 — θk,i)Dk,i(θ), Vk ∈ [2]. ThatimPlies,
2
K1,i2 (θ(t)) = X
k=1
nk (1 - 2θk,i(t))
θk,i(t)(1 - θk,i(t))
∙ (T)k-1 口k,i⑴(I- 9kH力
2 ∙ YY YY(I- 2θk,j (t)) ∙X (-1)k-1 =0 .
k=1 j=1	k=1
In order to Proceed with the final case we’ll first need to Prove a small technical lemma:
25
Published as a conference paper at ICLR 2022
Lemma 6. If (k, i) ∈ C(t) for some t ∈ I, I ∈ P = {(t1, t2) | t1, t2 ∈ T : t ∈/ T, ∀t ∈ (t1, t2)}
then one of the following holds:
a)	θk,i (t) = αk,i , ∀t ∈ I.
b)	θk,i(t) = βk,i, ∀t∈I.
To see that, let I ∈ P and (k, i) ∈ C(t) for some t ∈ I. Then it, holds, by definition, that
C(t1) = C(t2), ∀t1,t2 ∈ I =⇒ C(t0) = C(t), ∀t0 ∈ I
And, since (k, i) ∈ C(t), we also have that
(k, i) ∈ C(t0), ∀t0 ∈I =⇒ θk,i(t0) ∈ {αk,i, βk,i}
Let us assume that θk,i(t) = αk,i and suppose ∃t0 ∈ I such that θk,i (t0) 6= αk,i =⇒ θk,i (t0) = βk,i
and αk,i 6= βk,i. By the Intermediate Value Theorem, it follows ∃t00 ∈ (min(t, t0), max(t, t0)) such
that
θk,i(t00) ∈ (αk,i, βk,i) =⇒ (k, i) ∈/ C(t00)
That is a contradiction, and, hence, a) holds. a) follows by a similar argument, assuming θk,i (t) =
βk, i is the case, instead.
Having established Lemma 6, we continue with the proof of Theorem 5. Let us assume that ∃k, k0 ∈
[2] such that (k, ik) ∈ C(t) and (k0, ik0) ∈/ C(t). Then Lemma 6 implies that either θk,ik (t0) =
αk,ik , ∀t0 ∈ I or θk,ik (t0) = βk,ik , ∀t0 ∈ I. Let us assume, without any loss of the generality,
the former case and let Us consider some t ∈ I. Since §左公 ∈ Sk, i.e., αk,ik ≤ 1, that implies
1 - 2θk,ik (t) ≥ 0. Next, Let us consider the value of (-1)k-1Dk,i(θ). We are going to prove by
abdUction that (-1)k-1Dk,i(θ) ≤ 0.
If (-1)k-1Dk,i(θ) > 0 then by Equation D10 We have thatθk,ik(t) = (-1)k-1 n2θk,i(1 -
θk,i)Dk,i(θ) 6= 0, where the last ineqUality follows by AssUmption 3. By the continUity θ(t), it
folloWs that ∃0 > 0 such that ∀ ∈ (0, 0), θk,ik (t + ) 6= θk,ik (t) = αk,ik =⇒ (k,ik) ∈/ C(t);
that is, indeed, a contradiction. It must then be the case that (-1)k-1Dk,i(θ) ≤ 0 and, thus, We
have that
忆 1,i2 (θ(t)) = 3：-当,*)) ∙ (-I)"-1 2- %,i(t)(1 - θk0,i(t))Dk0,i(θ(t))
θk0,i(t)(1 - θk0,i(t))	nk0
=2 ∙ (-1)k -1 ∙ (1 - 2θk0,i(t))Dk0,i(θ(t)) = 2 ∙ (-1) ∙ (-1)k-1 • (1 - 2θk,i(t))Dk,i(θ(t)) ≥ 0 .
Thus, we showed that in every case K1,i2 (θ(t)) ≥ 0, ∀t ∈ I. However, since θ(t) is continuous
(since it is differentiable), we can extend this property for any t ≥ 0, i.e., Vi1,i2 (θ(t)) ≥ 0, ∀t ≥ 0.
Finally, notice that Vi1,i2 (θ(t)) is bounded, and, hence, it follows that -Vi1,i2 (θ(t)) is a Lyapunov
function (up to a constant) of the dynamical system described by Equation D10, and, Theorem 5
follows by the definition of a Lyapunov function.
□
B	S upplementary experimental results
B.1	Experimental results for 2-Team Hidden Matching Pennies games
In this section we present exemplary settings for the dynamics presented in section 5. For an
overview of the conditions that characterize the behaviors presented in these examples, we refer-
26
Published as a conference paper at ICLR 2022
Figure 3: An example of the RD of a 2-Team Hidden Matching Pennies (Center) with no
restrictions applied in the parameters space (θ, 1 - θ) in (Left), and (Right). In this case, the
RD cycles for any initial point (θ(0), 1 - θ(0)). The evolution of θ(t), as a function of time,
t ∈ [0, 100] is depicted in (Bottom) along with the Lyapunov function given in (31).
ence the interested reader to section 5; for formal definitions and the proofs of these concepts, see
§A.4.
Let G = (n := n1 + n2, S := {0, 1}n , u : S → Rn) be a 2-Team Hidden Matching Pennies game
with n1 = n2 = 3 and payoff function given by (16). Figure 3 depicts the behavior of the RD in
an unrestricted instance of G, i.e., Sk,i = [0, 1] for all i ∈ [3], k ∈ [2]. The restrictions applied to
team 1 and team 2 are visualized by the feasible range of each member’s strategies (Figure 3 (Left),
and (Right), respectively). The black dot in each range indicates the initial strategy of each member,
i.e., θk,i(0). Note that, by Assumption 3, this strategy profile, (θ(0), 1 - θ(0)), is assumed to lie
inside the feasible region enforced by the constraints Sk,i .
We solve the initial value problem of the ODE that corresponds to this RD using the RADAU in-
tegration method implemented by the scipy package in Python 3, and we perform 500 evaluations
over the time interval t ∈ [0, 100]. The orbit of the RD with initial parametrization θ(0) is depicted
as a curve on the F × G space defined by the hidden mappings in (18) (Figure 3 (Center)). The red
dot indicates the the point (f(θ1(t)), g(θ2 (t)) at the end of the simulation, i.e., at time t = 100. The
corresponding strategies of each member are indicated by blue (team 1) and orange (team 2) dots
inside the corresponding feasible regions (Figure 3 (Left), and (Right)). The individual trajectory of
each θk,i , i ∈ [3], k ∈ [2] is depicted in Figure 3 (Bottom), where the curve labeled TkPi corre-
sponds to the trajectory of the i-th member of team k if k = 1, or the i - n1-th member of team k if
k = 2. In the case that the RD cycle, e.g., in the unrestricted setting, we depict one of the Lyapunov
functions of the RD (Figure 3 (Bottom)). Specifically, the Lyapunov function of our choice is
21
V (t)=X 同
E nk ∙ [log(θk,i(t)) + log(1 - θk,i(t))],
i∈Sk
(31)
the average value of all the linearly independent Lyapunov functions defined in (19). We selected
this specific Lyapunov function as the average is, in general, more numerically stable and for com-
pactness.
27
Published as a conference paper at ICLR 2022
In Figure 3, we verify that the RD of G, indeed, cycle in an unrestricted setting (Theorem 3).
The choice of initial points does not matter in this case; for completeness, we note that θ1 =
(0.1, 0.9, 0.2), and θ2 = (0.1, 0.9, 0.8) in all of the examples in this section.
Figure 4: An example of the RD of a 2-Team Hidden Matching Pennies (Center) in a re-
Stricted setting where 2 ∈ Sk,i for all i ∈ [3], k ∈ [2] ((Left), and (Right)). In this case,
the RD converges for any initial interior point (θ(0), 1 - θ(0)). The evolution of θ(t), as a
function of time, t ∈ [0, 100] is depicted in (Bottom).
As the first example in a restricted setting (Figure 4), we are going to enforce S1,1 = S1,3 =
S2,1 = [0.05, 0.4], and S1,2 = S2,2 = S2,3 = [0.6, 0.95]. First, observe that Sk,i ⊂ (0, 1) for all
i ∈ [3], k ∈ [2]; hence, Assumption 3 is satisfied. Next, notice that 1 ∈ Sk,i for all i ∈ [3], k ∈ [2].
It follows, by Theorem 4, that the RD should converge in this case (Figure 4 (Center), and (Bottom)),
and the point of convergence is, indeed, θɪ = (0.4, 0.6, 0.4), and θ2 = (0.05, 0.95, 0.95), as given
in Theorem 4.
Figure 5: An example of the RD of a 2-Team Hidden Matching Pennies (Center) in a re-
stricted setting where 2 ∈ Sι,i, and 1 ∈ S2,i for all i ∈ [3] ((Left), and (Right)). In this case
the RD converges for any initial interior point (θ(0), 1 - θ(0)), but the point of convergence
depends on the value of θ(0). The evolution of θ(t), as a function of time, t ∈ [0, 100] is
depicted in (Bottom).
28
Published as a conference paper at ICLR 2022
There exists a similar setting (Figure 5), where the RD of G converge, as well. Note, in this case,
while 2 ∈ S2,i for all i ∈ [3], We let 1 ∈ Sι,i for some i ∈ [3] (specifically, 1 ∈ Sι,i for all
i ∈ [3] in this particular example). Although we did not provide a convergence analysis for this
case, it is not difficult to see that the convergence guarantees from Theorem 4 can be generalized
to include this case. As is visualized in Figure 5 (Center), the exhibited behavior is a hybrid of the
two behaviors described by Theorem 4, and Theorem 5. In particular, the behavior of the dynamics
depends on the quadrant of the F × G space that the θ(t) lies. Ultimately though, the dynamics can
be shown to converge to a point θ*, which depends not only on the initialization θ(0) but also on
the restrictions in the space parameters, Sk,i , i ∈ [3], k ∈ [2]. The particular point of convergence
for this example is θɪ = (0.05,0.95,0.95), and θ2 = (0.4, 0.6, 0.6), where the space restrictions
are given as S2,1 = [0.05, 0.4], S2,2 = S2,3 = [0.6, 0.95], and S1,i = [0.05, 0.95] for all i ∈ [3].
Figure 6: An example of the RD of a 2-Team Hidden Matching Pennies (Center) in a re-
stricted setting where 1 ∈ Sι,i, and 1 ∈ S2,j for some i,j ∈ [3] ((Left), and (Right)). In
this case the RD converge to a cycle defined by, at least, |S1 | + |S2 | - 1 linearly independent
invariant functions for any initial interior point (θ(0), 1 - θ(0)) The evolution of θ(t), as a
function of time, t ∈ [0, 100] is depicted in (Bottom).
Finally, we present an example of cycling behavior in the restricted setting (Figure 6) where we
applied the restrictions S1,1 = S1,3 = [0.05, 0.8], S1,2 = S2,2 = [0.6, 0.95], S2,1 = [0.05, 0.4], and
S2,3 = [0.2, 0.95]. Note that 2 ∈ S1,1, S1,3, S2,3; hence, ∣sj + |S2| = 3, and there exists ik ∈ [nk]
for all k ∈ [2] such that 2 ∈ §k力.By Theorem 5, it follows that the restricted RD cycle in this case;
specifically, they converge to an invariant set defined by V1,3 (θ) = 0, and V3,3 (θ) = 0 as given
by (19). This behavior is depicted in Figure 6 (Center). Notice how the Lyapunov function, V (θ)
(Figure 6 (Bottom)) is monotonically increasing, and stabilizes at approximately t = 25.
29