Published as a conference paper at ICLR 2022
From Intervention to Domain Transportation:
A Novel Perspective to Optimize Recommenda-
TION
Da Xu
Walmart Labs
Sunnyvale, CA 94086, USA
DaXu5180@gmail.com
Yuting Ye
Division of Biostatistics
University of California, Berkeley
Berkeley, CA 94720, USA
yeyt@berkeley.edu
Chuanwei Ruan
Instacart
San Francisco, CA 94107, USA
Ruanchuanwei@gmail.com
Ab stract
The interventional nature of recommendation has attracted increasing attention
in recent years. It particularly motivates researchers to formulate learning and
evaluating recommendation as causal inference and data missing-not-at-random
problems. However, few take seriously the consequence of violating the critical
assumption of overlapping, which we prove can significantly threaten the validity
and interpretation of the outcome. We find a critical piece missing in the current
understanding of information retrieval (IR) systems: as interventions, recommen-
dation not only affects the already observed data, but it also interferes with the
target domain (distribution) of interest. We then rephrase optimizing recommen-
dation as finding an intervention that best transports the patterns it learns from
the observed domain to its intervention domain. Towards this end, we use do-
main transportation to characterize the learning-intervention mechanism of rec-
ommendation. We design a principled transportation-constraint risk minimization
objective and convert it to a two-player minimax game. We prove the consistency,
generalization, and excessive risk bounds for the proposed objective, and elaborate
how they compare to the current results. Finally, we carry out extensive real-data
and semi-synthetic experiments to demonstrate the advantage of our approach,
and launch online testing with a real-world IR system.
1 Introduction
For information retrieval (IR) systems, the users’ willingness to interact is often intervened by the
content we show. Recommendations not only impact the potential response of the users, but also
change the nature of the data collected for training machine learning models (Bottou et al., 2013).
The interventional nature of recommendation, as well as the fact that many questions in IR are
counterfactual, e.g., ”what the response would have been if we recommended something else”,
makes it natural to rephrase recommendation in the context of causal inference or missing data
problems (Rubin, 2005; Little et al., 2002). Among the two disciplines, technical tools of impor-
tance weighting (IW) and domain adaptation (DA) have been commonly applied. Recently, there
has been widespread interest in adapting those tools for learning the best recommendation (Schn-
1
Published as a conference paper at ICLR 2022
abel et al., 2016; Bonner and Vasile, 2018), but we find a critical discussion missing from most
existing literature in this direction. In many scientific disciplines, the effectiveness of IW and DA
relies heavily on overlapping between source and target domain (Austin and Stuart, 2015; David
et al., 2010). The underlying reason also happens to be the motivation of exploration-exploitation
techniques including bandits and reinforcement learning (Auer et al., 2002; Sutton and Barto, 1998):
in the less-explored regions where few observations are collected, the confidence of selecting any
hypothesis is discounted.
Unfortunately, modern IR systems are often cautious about exposing users to less relevant content
for the sake of immediate satisfaction and revenue, even with the help of bandits or RL (Bonner and
Vasile, 2018). In particular, many real-world recommenders are deterministic, e.g., an item is shown
with probability either zero or one, which further limits the exploration and thus the coverage of
collected data. In Section 3, we rigorously prove for both IW- and DA-based learning the hardness
or impossibility results caused by insufficient overlap. Intuitively, when a less-explored instance
is passed to a candidate hypothesis, even if DA or IW can improve the accuracy of the prediction,
they inevitably increase the uncertainty of that prediction as a consequence of the bias-variance
tradeoff. To a certain point, the increased uncertainty from having too many instances in the weakly
overlapped regions will fundamentally limit what any learning procedure can achieve. Further, if the
instance lies in the non-overlapped region, we can barely exceed regular extrapolation1 (Xu et al.,
2021b).
Figure 1: A realistic working mechanism of IR systems that emphasize domain transportation.
We resolve the fundamental limitation of insufficient overlapping in IR with the key insight that the
interventional impact of recommendation is not exclusive to observed data, but also interferes with
the target domain of interest — the instances that will be affected by the recommendations we are
about to make. For instance, the placement of a recommendation on the webpage proposed to the
user directly influences the occurrence of clicks. Therefore, we alternatively consider optimizing
recommendation as searching for an intervention that best transports the patterns it learns from the
source domain to its intervention domain. The procedure is best described by transport instead of
transfer because it will interfere with how the learnt patterns are carried to the target domain. This
change of view describes a complex but realistic working mechanism that involves learning, inter-
vention, and transportation, as depicted in Figure 1. On the one hand, we wish to learn patterns that
can be transported to the target domain of interest. On the other hand, the patterns we identify lead
to the intervention (in the form of recommendation) that generates the target domain. Following this
working mechanism, we impose constraints on the domain transportation between the (reweighted)
source domain and the future intervention domain. We point out that reweighting is essential for
learning from interventional feedback data (Rosenbaum and Rubin, 1983), especially for the work-
ing mechanism we identify here because the relationship between the source and target domain is
more involved. Towards this end, we propose a novel solution via transportation-constraint risk
minimization. We rigorously investigate for our objective:
•	the consistency, generalization, and excessive risk bounds that provide theoretical insights and
guarantees;
•	the equivalence to a two-player minimax game that can be efficiently solved by gradient descent-
ascent algorithm;
•	convergence to the standard IW-based solution for the ideal fully-overlapped bandit feedback;
We demonstrate the empirical performance of our method via comprehensive experiments via sim-
ulation, real-data analysis, as well as online experimentation and testing in a real-world IR system.
1The presumption for this argument is that the underlying causal mechanism is unknown, which applies to
the settings we discuss in the paper.
2
Published as a conference paper at ICLR 2022
2	Related Work
Bottou et al. (2013) first reveals the importance of characterizing the working mechanism of IR
systems. However, they study specifically the advertising systems where the underlying causal
structures are assumed given. Our work significantly extends their discussion by moving beyond
causal mechanisms. Issues caused by ignoring the interventional nature of recommendation have
been pointed out for numerous scenarios (Chen et al., 2020). Steck (2010) and Liang et al. (2016)
address optimizing recommendation in the context of missing data and causal inference, and the
majority of recent follow-up work investigate particularly inverse propensity weighting (IPW) and
learning domain invariant representation methods (Chen et al., 2020; Saito, 2020; Yang et al., 2018;
Joachims et al., 2017; Bonner and Vasile, 2018; Agarwal et al., 2019; Xu et al., 2020). As mentioned
in Section 1, they largely ignore the overlapping issue even when the recommendation is determin-
istic. Our theoretical results on the interplay between overlapping and learning performance are
novel and add to the current venue of revealing the tradeoff for IW- and DA-based learning (Byrd
and Lipton, 2019; Johansson et al., 2019; Ben-David and Urner, 2012). See Appendix D for further
discussions on related work.
Existing solutions for insufficient overlapping focus primarily on reducing or calibrating the vari-
ance, e.g., as discussed in Swaminathan and Joachims (2015b); Wang et al. (2020); Johansson et al.
(2020). They do not account for the unique working mechanism of IR where the domain discrepancy
can be actively controlled. We devise the domain-transportation constraint using the Wasserstein dis-
tance originated from the optimal transport theory (Courty et al., 2016; Redko et al., 2017). The idea
of converting a Wasserstein-constraint objective to two-player minimax game is similar to that of
the generative adversarial network (Gulrajani et al., 2017), however, we do not make any generative
assumption on the observed data. Finally, learning from bandit feedback and the associated coun-
terfactual risk minimization can be viewed as a special case of our framework (Swaminathan and
Joachims, 2015a). We will use it as a test bed to examine the guarantees of the proposed approach.
3	Preliminaries
We use upper-case letters to denote random variables and measures, bold-font letters for vectors and
matrices. We denote by ∣∣ ∙ k2, ∣∣∙∣∣ ∞, ∣∣∙∣∣ l the '2, '∞ norm and the LiPschitz constant of a function.
For clarity, we borrow the item recommendation setting to illustrate the IR system we discuss next.
The set of all users and items are given by U and I. To be concise, we denote the features (or
Pre-trained embeddings) for users and items by xu for u ∈ U and xi for i ∈ I. We study Primarily
the Practical imPlicit-feedback setting with Yui ∈ {0, 1}, e.g., users exPress Preferences imPlicitly
via clicks. Let D be the PoPulation of all Possible (xu, xi) or (u, i) dePending on the context. The
embeddings can also be free Parameters under the rePresentation maPPing f from X orU ∪I to Rd.
We let P be any base Probability measure suPPorted on the subsets ofD, and let Pn be the emPirical
versions according to the collected feedback of Dn where n is the samPle size. We assume the
weight of a user-item is Positive and bounded by a constant M . EquiPPed with either individual
weights wui ∈ [0, M] or weighting function w : U × I (or X × X) → [0, M], the reweighted
measures are given by such as: Pw (u, i) H w(u, i) ∙ P(u, i).
The candidate recommendation algorithm f ∈ F outPuts a score or Probability for each user-item
Pair. ToP-k recommender often ranks the outPut and then renders the toP-k items to the user.
We denote this Procedure by the function: reco(u, i; f) ∈ {0, 1}. Define the new measure on
D induced by deploying f on D as: Pf(u, i) h reco(u, i; f) ∙ P(u, i). If We choose the base
measure P as uniform, it is clear that Pf is a uniform measure on the (u, i) Pairs that will be
exposed: {(u, i) ∈ D : reco(u, i; f) = 1}. Finally, given a loss function ` for f, We define
the corresponding risk under P: EP R(f) = E(u,i)〜P 'f (u, i), yu，, the weighted risk under Pw:
EPwR(f) = E(u,i)〜P'(f (u, i), yui) ∙ w(u, i).
Our setting covers a Wide range of recommendation problems from classical collaborative filtering
to sequential recommendation (When user embedding is a function of the item sequence), including
those using bandits for exploration Where f(u, i) becomes the exploration probability, and the base
measure is given by the inverse propensities. We defer the detail of that setting to Appendix A.1.
Consequence of Insufficient Overlapping.
3
Published as a conference paper at ICLR 2022
We now rigorously justify our previous intuition that insufficient overlapping causes uncertainty that
fundamentally limits what IW- and DA-based learning can achieve. While the results we show for
IW are universal, i.e., it applies to any hypothesis class, we discuss particularly for DA the skip-
gram negative sampling (SGNS) algorithm (Mikolov et al., 2013). The reason is that SGNS is a
widespread technique for learning user or item embedding in IR systems (Xu et al., 2021a), and its
more straightforward formulation will allow us to present concise theoretical results to the readers.
As a gentle introduction, IW-based learning uses the ratio Q(u, i)/P(u, i) - where P and Q rep-
resent the source and target domain - to reweigh the loss of each instance in order to correct the
distribution shift. On the other hand, DA-based learning assumes there exists some hypothesis that
makes the optimal joint risk of inff∈F[EQR(f) + EP R(f)] small. For IW, we show the hardness
of learning, i.e., the best target risk inff EQR(f) can be uncontrolled for any F due to insufficient
overlapping. For DA, we show the impossibility of learning, i.e., there does not exist a solution of
SGNS that can make the optimal joint risk small when P and Q are not aligned.
Proposition 1. Let P and Q be the training and target distribution supported on SP, SQ ⊆ D, and
w(u, i) = Q(U；) ,for (u, i) ∈ Sp ∩ SQ. Dn consists oftraining instances sampled i.i.d from P.
Hardness of learning with IW: ifSP = SQ, for any f ∈ F, with probability at least 1 - δ, it holds:
i
EQR(f) .Epn,wR(f)+
M(log 1 + log N∞( 1, F)
n
Mdι(P kQ)(log 1 +log N∞( 1, F)
n
+
and P(IEQR(f) 一 Epn,wR(f )∣ & P(dι(PkQ) - 1)/n) > 0 under mild condition, where M :=
max{w(u, i)}, N∞(1, F):= N (1, F, '∞) is the 1 -covering number for F in ∣∣ ∙ ∣∣∞ based on
2n i.i.d samples from P, and d1 (P kQ) = S (dP /dQ)dP is a divergence measure. Further, if
Sp 6= SQ, for any prior and posterior distributions π and τ on F,for risks associated with the 0-1
loss, it holds with probability at least 1 - δ:
Ef 5,qR(∕ ) . Ef fPn,w R(f) + dK (πkτ );lθg I 上 + Ef f Sq∖Sp R(f) + O(Dn(T)),
where dKL is the Kullback-Leibler divergence, d1 (P ∣Q) is the same as above but defined only on
the overlapped part, and Dn (τ) consists of disagreement terms detailed in Appendix A.2.
Impossibility of learning for DA with SGNS: let R(f) be the associated risk for the SGNS algo-
rithm, and Cu,i (P), Cu,i(Q) be the co-occurrence statistics under P, Q, as mentioned in Xu et al.
(2021a). Even ifSp = SQ, it holds that:
in f[EpRJ)+EQR(f )]≥∣SP∣
E	d，KL(Cu,i(P )∣Cu,i(Q)) + c,
(u,i)∈SP
where c takes some positive value and equals to inff EpR(f) + inff EQR(f) when P = Q a.s.
We defer all the proofs to Appendix A.2. For IW, we reveal the generalization bounds for both the
frequentist setting (the first set of bounds) and the PAC-Bayesian setting (the second bound). While
they emphasize conceptually on different learning guarantees (Germain et al., 2009), and while the
latter can handle the existence of non-overlap (Sp 6= SQ), we find them both showing insufficient
overlapping (characterized by the discrepancy terms d1 and dKL) can cause arbitrarily large gaps
leading to poor generalizations to the target domain. For DA with SGNS, our lower bound reveals
the impact of insufficient overlapping indirectly via a notion of co-occurrence statistics defined in
Xu et al. (2021a). In particular, if the source and target domains are well-aligned in terms of the
co-occurrence statistics, the increased divergence term can make it impossible to find a hypothesis
that simultaneously performs well in both domains. Although this result holds only for SGNS, it
nevertheless suggests the limitations of DA caused by insufficient overlapping in IR.
4	Model and Theory
As much as we would like to continue investigating the consequence of insufficient overlapping,
for the purpose of this paper, we have shown the drawbacks of directly applying such as IW and
4
Published as a conference paper at ICLR 2022
DA to optimizing recommendation without accounting for the working mechanism of IR systems.
In what follows, we describe a principled solution motivated by our domain transportation view,
which requires actively identifying the suitable training domain via reweighting. Notably, we no
longer perceive reweighting as a change-of-measure tool commonly implied in the causal inference
and missing data solutions. Instead, we use reweighing as a mechanism to help identify a particular
training domain to ensure certain transportation properties (which we explain next) to the deploy-
ment domain2 , which may concern only a subpopulation of all the potential user-item instances.
4.1	Transportation-constrained risk minimization
Throughout our discussion, we emphasize the transportability of patterns across domains, which can
be difficult to define precisely. However, if we make a a mild assumption that all patterns can be
recovered by Lipschitz-bound functions, it immediately becomes clear that the Wasserstein distance
can characterize our notion of transportability. To show this point, we first describe an imbalanced
binary classification problem with two domains P and Q. Let `1 (u)
P and '-ι(u) = ⅛
be the positive and negative loss under prediction score u, where p1 is the prior probability that a
sample comes from P. Interestingly, the L-Wasserstein distance between P and Q admits3:
-dW(P,Q)
"配 ≤l} p1 L '1(g)dP+(1 - P1)〃 T (g)dQ,
(1)
which is exactly the optimal risk of the binary classification problem described in the first place!
Therefore, Wasserstein distance can be perceived as how well the classification patterns on P trans-
ports to Q by any means necessary, as long as they are Lipschitz-bounded. Since the constant of L
is simply a scaling factor, we simply continue with the 1-Wasserstein distance. Using dW to device
the domain transportation contraint between Pf and the particular training domain Pw we seek to
construct with our weighting mechanism, the learning objective is directly given by:
minimize EP R(f) s.t. D(w, f) ≤ ρ,	(2)
f∈F,w∈H	w
where D(w, f) is a shorthand for dW (Pw, Pf), and ρ can be treated as a hyper-parameter. As a san-
ity check, when ρ = 0, the feasible set is given by: f(u, i) = w(u, i) for all (u, i), so we are directly
optimizing the associated risk of the deployment domain. However, this is a suboptimal practice for
deterministic recommenders because in that case, only a small proportion of the feedback data will
be considered and the result can be heavily biased toward the historical recommendation. Therefore,
a suitable ρ will give us the opportunity to better explore the whole feedback data. Nevertheless, if
ρ becomes too large, we are likely to overfit the feedback data due to the high capacity of f and w
combined while the transportation constraint is too loose.
The proposed objective is tailored specifically to the working mechanism of IR systems, and it is
natural to question the validity and effectiveness. In what follows, we rigorously prove a series of
guarantees associated with our novel learning objective, and discuss how it connect and compare to
the existing results.
To begin with, we show that jointly optimizing w leads to the sample-consistency guarantee. For
clarity, we start with the feature-based learning where f(u, i) := f(xu , xi), and defer the extension
with the representation mapping to the next section. Let Z := D × {0, 1}, and we denote by
`f : Z → R+ the composition with the loss function such that for zui = (xu , xi, yui), we have
`f (u, i) := ` f(u, i), yui . We use wui as a shorthand for the individual weight.
Theorem 1.	Denote by Dn(w, f) = dw(Pn,w,Pn,f). Suppose that 'f is L-LiPschitz w.r.t ∣∣ ∙ ∣∣2.
Define SG = suPg"∣g∣∣L≤1 p(u,i)∈Dn g(u, i) and MG = SUpkgkL≤1, (u,i)∈D g(u, i). Assume Z ∈
RdZ and w ∈ H. Given any f ∈ F and ρ > 0, it holds with probability at least 1 - δ that:
min、	EPwR(f) -	min	Ep2R(f) ≤ 9M(Iog 2δ) + Bn(H,δ)L + Cn(w,W,f,δ).
w: D(w,f)≤ρ	w: Dn (w,f)≤ρ	,	n
2Suppose a candidate recommender f is deployed to D. According to Section 3, the domain of users and
items who are affected by the recommendation is given by Pf defined on {(u, i) ∈ D : reco(u, i; f) = 1}. We
refer to it as the deployment domain to differentiate from the notion of target domain which is conventionally
intervention-free.
3In Appendix A.3, we provide a more thorough introduction to Wasserstein distance for interested readers.
5
Published as a conference paper at ICLR 2022
Here, W = argminDn(w,f)≤δ-Bn(H,δ) EPw Rif), W = argminDn(w,f)≤δ + Bn(H,δ) EPw Rf),
Cn(W,W,f, δ)	=	n-1J18M max{Rw(f)2, Rw(f)2} log 表 and Rw(f)2	=
n PDn w2i'f (u, i)2∙	Also, Bn(Η,δ) is given by O^(SGn1/(dZ+1))	+
MGnT/ 2 q log 表+N∞( n, H)) ∙
We relegate the proof of this theorem to Appendix A.4. Crucially, other than showing that the
sample-consistency result can still be achieved with the jointly-optimized weights, we point out that
this bound is completely independent of distribution discrepancy terms that troubled IW and DA.
Notice that the bound depends on the weights only through Rw(f)2 and Rw(f)2 presented by the
Cn term. Given any f ∈ F, our result shows that both W and W are the minimizers of a weighted
risk term, i.e. EPwR(f) α PD Wui'f (u, i), under some constraint involving ρ. Therefore, thanks
to the constraint of D(w, f) ≤ P and the joint optimization of w, Rw (f)2 and Rw (f)2 will have
more controlled behaviors compared with the domain discrepancy terms in Proposition 1.
We now investigate the learning-theoretical guarantees of the proposed objective, particularly the
generalization and excessive risk bounds. Suppose the underlying labeling function of yui with
respect to (xu, Xi) is deterministic. With a little abuse of notation, We use Z 〜Pn,f to denote
sampling (xu, Xi, yui) Via (xu, Xi)〜Pn,f and the deterministic labelling function.
Theorem 2.	Suppose infz∈z{'f (Z) + λ∣∣z — Z∣∣2} ≤ MF for all Z ∈ Z and λ ≥ 0, f ∈ F. Let
Rn(F) be the empiricalRademachercomplexity: Rn(F) = Eσ SuPf∈f ∖ n P(U i)∈Dn σu,if(u, i)∣,
andσ takes {—1, 1} with equal probability. We consider individual weights Wui ∈ (0, 1) for brevity.
Generalization Error: for any f ∈ F, it holds with probability at least 1 — δ that:
Pw R(f) ≤ min n - λρ + EZ 〜Pn,f h inZ 'f (z) + λkz - zk2i + MFj
min E
w: D(w,f)≤ρ
llog(λ +1)	C R∞ PlogN∞(e, F)d
+v —n—+一~诉-----------
log1∕(2δ)
n
n..   ∙ _ . ∙ ɪ Ir P	♦	πn	-γλ∕ c∖	. ι	∙ ∙ ι . ∙ ι rr*
Excessive risk: define f = arg minf,Dn(w,f)≤ρ Epn,wR(f) as the empirical optimal and f
arg minf,D(w,f)≤ρ EPw R(f) as the theoretical optimal. With probability at least 1 — δ:
min	EPwR(f)—	m皿、EPwR(f*) ≤ 2&('。尸)十。(L + MP2og1”∖
w： D(w,f)≤ρ	w: D(w,f*)≤p	∖	n1/2	)
We defer both proofs to Appendix A.5. Theorem 2 establishes two critical learning guarantees for
the proposed transportation-constraint risk minimization. In particular, the generalization error re-
veals how we resolve the previous drawback mentioned for using DA in IR system. Recall from
Section 3 that the joint optimal risk might be uncontrolled for DA since it can depend on how dif-
ferent the training and target domains are. In this regard, with D(W, f) ≤ ρ explicitly constrains
the discrepancy between the training and deployment domain, We have the additional —λρ term in
the generalization error bound where the second term on the RHS can be think of as the empirical
”transported” risk. In other words, the constraint ensures that the deployment risk will not deviate
far from the training risk, and thus creates the possibility of learning transportable recommenda-
tion. Further, the excessive risk bound suggests that the transportation constraint does not cost the
convergence rate comparing with the standard empirical risk minimization guarantee (Mohri et al.,
2018).
4.2	Towards transportation-regularized risk minimization
To render the proposed objective computationally feasible, we apply a strong duality argument to
show that there exists a regularized counterpart for (2), where we defer the proof to Appendix A.6.
Claim 1. Suppose that ` 。 F is convex. It holds for any f ∈ F that:
min
Pw: dw(Pw,Pf)≤ρ
EPwR(f)
max — - λρ + min {EpwR(f) + λ ∙ dw(Pw,Pf )}∣.
λ≥0	Pw	w
6
Published as a conference paper at ICLR 2022
The above duality result suggests we alternatively consider the regularized learning objective:
minimizeEPwR(f) + λ ∙ dw(Pw,Pf),	(3)
f∈F,w∈H	w
where the relaxation on maxλ≥0 has been made. Here, λ is the tuning parameter that regularizes
the domain transportation between the reweighted training domain and the deployment domain. To
give some insight for the transportation-regularized objective, we use learning from bandit feedback
data as a test bed and examine how the optimal solution of (3) compares to the solution of the
standard counterfactual risk minimization with complete overlapping (Swaminathan and Joachims,
2015a). Let the logging policy by given by π. It holds in the bandit off-policy learning setting
that: Pf(u, i) 8 ∏(ui), where f (u, i) ∈ (0,1) now serves as the policy We try to optimize. The
counterfactual risk is given by: EPf R(f ):= P(U i)三D 1(：；)'(f (u, i),yui), and we prove that the
solution of (3) is consistent with that of the counterfactual risk minimization.
Theorem 3. Let f * = arg minf ∈f EPf R(f) be the solution ofthe counterfactual risk minimization,
and f, W be the solution ofthe proposed objective in ⑶.For suitably large λ s.t. λ ≥ O(L) ≥ 1, it
holds with probability at least 1 - δ that:
∣Epf. R(f *) — (EPw R(f) + λdw (Pw ,Pf) I . -7∕λd+ιy + log1"* 2') + λMG Jlog1∕(2δ)
f	w	f	n1/(d+1)	n	n

+
Mmax{Rf*∕∏(f *)2, Rw(f)2} log 21δ
n
where MG, Rw(f)2 are defined as in Theorem 1, and π is the logging policy.
Intuitively, since both f and π are probability distributions with the same support in the idea bandit
feedback setting, a suitably large λ will drive Pw ultimately toward Pf and lead to the consistency
result. We defer the proof of Theorem 3 to Appendix A.7. This example reveals the effectiveness of
the proposed transportation-regularized risk minimization, which we show is a strict extension of the
counterfactual risk minimization. For IR systems, the transportation-regularized risk minimization
applies to a much broader setting including deterministic recommendation. Nonetheless, directly
solving (3) is still challenging due to the complications in computing the Wasserstein distance. In-
spired by how Wasserstein GAN transforms its objective to a two-player adversarial game (Gulrajani
et al., 2017), we can also convert our objective to a minimax optimization problem discussed below.
4.3	Minimax Optimization
Algorithm 1: Batch-wise GDA for Minimax Optimization
Input: D(t): a batch of training sample of size m at step t. η: the learning rate for ascent step.
γ: shrinking parameter for the learning rate of descent step. λ: the regularization
parameter.
for each step t = 0, 1, . . . do
Compute reco(u, i; fθ1) for (u, i) ∈ D(t) (see Section 5 for the computation detail);
θ(t+1) 一 θ(t) - Y Vθι Lλ([θ(t), θ2t)], θ3t)); θ2t+1) 一 θ2t) - Y Vθ2 l7 ([θ(t), θ2t)], θ3t));
θ3t+1) 一 θ3t) + ηVθ3 Lλ([θ(t), θ2t)], θ3t)).
end
The key to our implementation is again perceiving Wasserstein distance as a binary classification
problem with Lipschitz-bounded classifier as we discussed in Section 4.1. Towards this end, we
denote by g ∈ G any suitable classifier for computing dW according to (1). Without loss of gener-
ality, we let w, f and g be parameterized by θ1, θ2 and θ3 . It is straightforward to verify that the
regularized objective in (3) transforms exactly into: min[θ1 ,θ2] maxθ3 Lλ([θ1, θ2], θ3), where Lλ
given by:
X	WUi(Θ2)'ui(fθ1) + λ( X	Wui(θ2)	∙	gui(θ3)	— X	reco(u,	i;	fej	∙	gui(θ3))∙	(4)
(u,i)∈D	(u,i)∈D	(u,i)∈D
7
Published as a conference paper at ICLR 2022
In view of (4) as a two-player game, the first player optimizes f and w to minimize both the training
risk and domain transportation, while the second player tries to increase the gap between the two
domains. The global equilibrium is reached when neither player can change the objective without
knowing each other’s strategy, which means f and w cannot simultaneously improve the empirical
risk and the domain transportation between Pw and Pf. It suggests we have found the recommenda-
tion hypothesis that best transports the learnt patterns to its intervention domain. We propose using
the two-time-scale gradient descent-ascent algorithm (GDA) to solve the minimax optimization.
GDA is very efficient for solving objectives like (4) with both empirical and theoretical evidence
(Lin et al., 2020), and it has been widely applied to optimize generative adversarial networks. Since
our solution will be deployed to real-world systems, we pay more attention to the convergence and
stability of GDA. In appendix A.8, we rigorously prove that for the bounded Lipschitz functions we
consider, GDA will converge to the global Nash equilibrium with a proper learning-rate schedule.
Further, it is unlikely to bouncing around near the optimum. Now that we have the performance
guarantee, we apply the batch-wise GDA described in Algorithm 1 to solve our objective.
5 Experiment and Result
We present in this section the experiment settings, implementations, empirical results, as well as
the analysis. The complete experimental results and the real-world online testing performance are
deferred to Appendix C. All the implementation codes are provided in the supplement material.
Computing the reco(u, i; f) function. For the deterministic top-K recommendation, reco(u, i; f)
requires computing and sorting the scores of {f (u, i1), . . . , f(u, i|I|)} for the user u. This is expen-
sive during training, so we refer to the widespread practice of downsampling. In particular, for each
(u0 , i0 ), we randomly sample m < |I| irrelevant items and see if i0 is among the top-K (Krichene
and Rendle, 2020). While the sampled outcome may have certain bias, it significantly speeds up the
training process while maintaining a decent accuracy under moderate m, e.g. m = 100.
Benchmark datasets. We refer to the MovieLens-1M, LastFM, and GoodReads datasets that are
extensively employed by the IR literature. We treat them as implicit feedback data, and the detailed
description and processing are deferred to Appendix C.1.
Simulation studies.. For implicit feedback, the response (click) is generated from the exposure and
relevance status via: P(YUi = 1) = P(OUi = 1) ∙ P(RUi = 1), where Oui indicates the exposure and
Rui is the relevance. The relevance score is critical for unbiased offline evaluation, but it is unknown
for real-world datasets. We thus conduct simulation so we know what the ground truth relevance is.
To make sure the distribution of the simulated data conforms to the real-world feedback data, we
first extract the exposure and relevance signals from public datasets (e.g. the three datasets described
above) via models such as matrix factorization, and then generate the new feedback data by adding
noise to the extracted relevance signal. The detailed mechanisms are provided in Appendix C.1.1.
mɪŋ
Learnt Weights	Learnt Weights	Learnt Weights	lambda
Figure 2: (a). Weight analysis on MovieLens and GoodReads. Upper: distribution of wui on the positive
and negative feedback data. Lower: relation between wui and fui under Pf (the recommended (u, i) under f)
and others; (b). Weight analysis on MovieLens simulation: the relations between wui and P(Oui = 1) on the
positive and negative synthetic feedback data for MCF (upper) and NCF (lower); (c). Ablation study for the
transportation regularization and sensitivity analysis on λ for MovieLens and its synthetic data.
8
Published as a conference paper at ICLR 2022
Table 1: Real-data and semi-synthetic experiment testing testing results for the MovieLens-1M, LaftFM and
GoodReads dataset. All the results are computed via ten runs and the Hit and NDCG metrics are multiplied
by 100 for presentation. The results for the LastFM and GoodReads datasets, and sequential recommendation
(Attn) are deferred to Appendix D.3. The metric of Rel@10, Hit@10 and NDCG@10 are short for the top-
10 relevance score, hitting rate and normalized discounted cumulative gain. The metrics on synthetic data
are computed by using the true relevance model. We highlight the best and SeCond best results. Attn is not
experimented on simulation because the ordering of the generated data is random.
Pop IPW-MF EXPOCF AC-MF ∣ MCF DT-MCF ∣ NCF DT-NCF ∣ Attn DT-Attn [
MovieLenS-IM
Rel@10	9.05	(.02)	14.00 (.05)	14.04	(.08)	14.05	(.07) Hit@10	43.67 (.10)	60.48 (.12)	60.21	(.14)	61.19	(.11) NDCG@10	22.73 (.03)	31.17 (.09)	30.98	(.10)	31.72	(.11)	14.02(.03)	14.65 (.07) 60.27 (.15)	61.95 (.12) 30.91 (.11)	32.83 (.08)	13.77 (.04)	13.98	(.06) 59.95 (.13)	61.17	(.15) 31.07 (.10)	32.27	(.07)	14.26 (.10)	16.32	(.09) 72.23 (.19)	74.82	(.24) 42.48 (.10)	43.09	(.14)
MovieL Rel@10*	2.85	(.02)	3.77	(.02)	3.73 (.03)	3.79 (.03) Hit@10	56.12 (.07)	75.02	(.11)	74.87 (.09)	74.98 (.13) NDCG@10	29.71 (.05)	37.79	(.06)	37.86 (.07)	37.77	(.07)	ens-1M Simulation 3.75 (.02)	398 (.02) 75.64(.09)	76.66 (.12) 37.70 (.05)	38.57 (.08)	3.96 (.02)	4.11 (.03) 75.69 (.09)	76.87 (.11) 38.43 (.05)	38.72 (.07)	-- -- --
LastFM
Rel@10	1.63	(.01)	6.41 (.04)	6.44	(.03)	6.48 (.03) Hit@10	25.22 (.01)	79.21 (.31)	79.62	(.27)	79.64 (.22) NDCG@10	15.35 (.01)	50.63 (.19)	50.76 (.22)	51.05 (.19)	6.38 (.03)	6.59 (.02) 79.02 (.22) 81.81 (20) 50.06 (.13) 52.96 (.14)	6.20 (.05)	6.56 (.04) 77.20 (.35) 80.83 (.33) 50.55 (.18) 52.03 (.26)	5.71 (.04)	5.82 (.05) 67.48 (.35) 68.74 (.37) 56.12 (.22) 56.60 (.19)
Rel@10	4.98	(.01)	5.94	(.08)	5.93	(.10)	5.97 (.07) Hit@10	43.37 (.06)	59.04 (.10)	59.28 (.14)	59.33 (.12) NDCG@10	26.16 (.03)	35.19	(.07)	35.22	(.08)	35.45	(.11)	GoodReads 5.90 (.08)	6.14 (.07) 58.32 (.12) 60.95 (.11) 34.69(.10) 36.45(.09)	5.84(.02)	6.07 (.09) 58.07 (.04)	59.73 (.34) 34.90 (.02)	36.27 (.15)	4.57 (.04)	4.68 (.10) 48.97 (.26)	49.30 (.25) 25.76 (.17)	26.04 (.13)
Models and baselines methods. The functions of f , w and g in the minimax objective (4) can
be any recommendation algorithm that suits the conteXt, such as matriX-factorization collaborative
filtering (MCF), neural collaborative filtering (NCF) (He et al., 2017), and sequential recommen-
dation model with the prevalent attention mechanism (Attn) as in Kang and McAuley (2018). We
indeX our domain domain transportation approach by such as DT-X, with X stands for the models
of f, w and g. We will eXperiment with a broad range of combinations for f, w and g as we present
in AppendiX C.3. As for the baseline methods, other than the standard popularity-based recommen-
dation (Pop) and MCF, we select particularly the approaches that adapts causal inference or missing
data techniques, including the IPW-debaised MF method (IPW-MF) from Saito et al. (2020), the
user-eXposure aware MF (ExpoMF) from Liang et al. (2016), and the adversarial-counterfactual MF
method (AC-MF) from Xu et al. (2020). The model configurations and training details are deferred
to AppendiX C.2.
Training & evaluation. We use the two-timescale GDA as described in Algorithm 1 with a pre-
selected shrinkage parameter, and we will conduct its sensitivity analysis in AppendiX C.4. The
configurations for both our model and the baselines are described in AppendiX C.2. We adopt the
convention where the second-to-last interaction is used for validation, and the last interaction is used
for testing. For evaluation, in addition to the regular Hit@10 and NDCG@10 as in Rendle et al.
(2020), we further consider the relevance score of the top-10 recommendation, i.e. Ptop-10 Rui,
since it also reveals the quality of the recommendation and more closely resembles the deployment
performance of EPf R(f). We have access to Rui during simulation, and on real-data analysis, we
settle to the Yui in testing data as an approXimation. The evaluation results are provided in Table 1.
Result analysis. From Table 1, we first observe that the proposed methods achieves the best perfor-
mances in both the benchmark and simulated datasets on all metrics. In particular, the DT-X method
significantly improves the performance of the original model X. Also, our solution outperforms all
the baselines, including those adapted from causal inference or missing data methods. The rest
simulation studies are provided in AppendiX C.3, and their results admit similar patterns. Finally,
we conduct ablation studies on the transportation regularizer and the sensitivity analysis on λ (see
Figure 2c). It is clear that without the transportation regularization, the performances degenerate sig-
nificantly in both the real data and simulation studies. Also, we find that the performance variation
of the proposed approach is relatively steady under different λ.
Analysis on w. We specifically analyze the learnt weights since they play a critical role in our
approach. On both MovieLens and GoodReads (Figure 2a), we find that: 1). the learnt weights are
larger on the positive training instances; 2). for the (u, i) that are among Pf (the recommended),
the weights wui are higher and relates positively with f (u, i), while there is no such pattern for
the non-recommended. They both suggest the weights are indeed learnt to balance the domains of
the positive feedback data and the instances recommended under f. In Figure 2b, we observe from
the Movielens simulation study that the learnt weights correlate positively to the eXposure on the
9
Published as a conference paper at ICLR 2022
negative training data, and are distributed more evenly on the positive data. It means that wui learns
to emphasize the instances that were more likely to be exposed but not clicked, which usually have
strong negative signals, and the positive instances. It is valuable for training under implicit feedback
because the algorithm learns to locate the strong negative instances. The remaining weight analysis,
ablation studies, and sensitivty analysis are provided in Appendix C.3.
Real-world IR system. We deploy our solution to a real-world IR system held by a major e-
commerce platform in the U.S. We deploy our transportation-regularized solution, and the online
A/B/C testing against the standard and IPW-based solutions for an item recommendation task. Due
to the space limitation, we defer the details and analysis to Appendix C.6.
6 Conclusion and Discussion
In this paper, we study from the critical intervention perspective of recommendation and summarize
a domain transportation view supported by the realistic working mechanism of IR systems. We
propose a principled transportation-constraint risk minimization for optimizing recommendation,
and illustrate by both theories and experiments how we find it more favorable than directly importing
the causal inference and missing data solutions. However, we point out that our work does not aim
to replace those solutions since we focus exclusively on optimizing recommendation. For instance,
we do not readily incorporate the known causal mechanisms (which might be available for certain
applications) to answer questions that are relevant to the decision making in IR. It will be interesting
to explore how the presumed or discovered causal structures can interact with domain transportation
and improve our solution. Towards this end, our transportation-constraint risk minimization can be
viewed as a modular toolkit that allows researchers to apply further insights to solve more complex
IR problems. Finally, we hope our work raises the awareness of developing IR-oriented solutions
with a proper understanding of the system’s mechanism. The novel perspectives, solutions, and
theoretical results in our paper may also open the door to future research.
References
A. Agarwal, K. Takatsu, I. Zaitsev, and T. Joachims. A general framework for counterfactual
learning-to-rank. In Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 5-14, 2019.
M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM journal on computing, 32(1):48-77, 2002.
P. C. Austin and E. A. Stuart. Moving towards best practice when using inverse probability of
treatment weighting (iptw) using the propensity score to estimate causal treatment effects in ob-
servational studies. Statistics in medicine, 34(28):3661-3679, 2015.
S. Ben-David and R. Urner. On the hardness of domain adaptation and the utility of unlabeled target
samples. In International Conference on Algorithmic Learning Theory, pages 139-153. Springer,
2012.
S. Bonner and F. Vasile. Causal embeddings for recommendation. In Proceedings of the 12th ACM
conference on recommender systems, pages 104-112, 2018.
L. Bottou, J. Peters, J. Quinonero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray,
P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of
computational advertising. Journal of Machine Learning Research, 14(11), 2013.
J. Byrd and Z. Lipton. What is the effect of importance weighting in deep learning? In International
Conference on Machine Learning, pages 872-881. PMLR, 2019.
J. Chen, H. Dong, X. Wang, F. Feng, M. Wang, and X. He. Bias and debias in recommender system:
A survey and future directions. arXiv preprint arXiv:2010.03240, 2020.
10
Published as a conference paper at ICLR 2022
H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado,
W. Chai, M. Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the
1st workshop on deep learning for recommender SyStemS, pages 7-10, 2016.
C. Cortes, Y. Mansour, and M. Mohri. Learning bounds for importance weighting. In Nips, vol-
ume 10, pages 442-450. Citeseer, 2010.
N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation.
IEEE tranSactionS on pattern analySiS and machine intelligence, 39(9):1853-1865, 2016.
C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max opti-
mization. In ProceedingS of the 32nd International Conference on Neural Information ProceSSing
SyStemS, page 9256-9266, 2018.
S. B. David, T. Lu, T. Luu, and D. Pal. Impossibility theorems for domain adaptation. In Proceedings
of the Thirteenth International Conference on Artificial Intelligence and StatiSticS, pages 129-
136. JMLR Workshop and Conference Proceedings, 2010.
R. M. Dudley. Real analysis and probability. CRC Press, 2018.
L.	Faury, U. Tanielian, E. Dohmatob, E. Smirnova, and F. Vasile. Distributionally robust coun-
terfactual risk minimization. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pages 3850-3857, 2020.
M.	Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas. Efficient and accurate estimation of
lipschitz constants for deep neural networks. Advances in Neural Information Processing Systems,
32:11427-11438, 2019.
R. Gao and A. J. Kleywegt. Distributionally robust stochastic optimization with wasserstein dis-
tance. arXiv preprint arXiv:1604.02199, 2016.
P. Germain, A. Lacasse, F. Laviolette, and M. Marchand. Pac-bayesian learning of linear classifiers.
In Proceedings of the 26th Annual International Conference on Machine Learning, pages 353-
360, 2009.
P. Germain, A. Lacasse, F. Laviolette, M. March, and J.-F. Roy. Risk bounds for the majority vote:
From a pac-bayesian analysis to a learning algorithm. Journal of Machine Learning Research, 16
(26):787-860, 2015.
A. Globerson and N. Tishby. Sufficient dimensionality reduction. Journal of Machine Learning
Research, 3(Mar):1307-1331, 2003.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of wasser-
stein gans. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pages 5769-5779, 2017.
X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua. Neural collaborative filtering. In Proceed-
ings of the 26th international conference on world wide web, pages 173-182, 2017.
C. Jin, P. Netrapalli, and M. Jordan. What is local optimality in nonconvex-nonconcave minimax
optimization? In International Conference on Machine Learning, pages 4880-4889. PMLR,
2020.
T. Joachims, A. Swaminathan, and T. Schnabel. Unbiased learning-to-rank with biased feedback. In
Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages
781-789, 2017.
F. D. Johansson, D. Sontag, and R. Ranganath. Support and invertibility in domain-invariant repre-
sentations. In The 22nd International Conference on Artificial Intelligence and Statistics, pages
527-536. PMLR, 2019.
F. D. Johansson, U. Shalit, N. Kallus, and D. Sontag. Generalization bounds and repre-
sentation learning for estimation of potential outcomes and causal effects. arXiv preprint
arXiv:2001.07426, 2020.
11
Published as a conference paper at ICLR 2022
W.-C. Kang and J. McAuley. Self-attentive sequential recommendation. In 2018 IEEE International
Conference on Data Mining (ICDM), pages 197-206. IEEE, 2018.
V.	Koltchinskii, D. Panchenko, et al. Empirical margin distributions and bounding the generalization
error of combined classifiers. Annals of statistics, 30(1):1-50, 2002.
W.	Krichene and S. Rendle. On sampled metrics for item recommendation. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
1748-1757, 2020.
D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh. Wasserstein distributionally
robust optimization: Theory and applications in machine learning. In Operations Research &
Management Science in the Age of Analytics, pages 130-166. INFORMS, 2019.
D. Liang, L. Charlin, and D. M. Blei. Causal inference for recommendation. In Causation: Foun-
dation to Application, Workshop at UAI. AUAI, 2016.
T. Lin, C. Jin, and M. Jordan. On gradient descent ascent for nonconvex-concave minimax problems.
In International Conference on Machine Learning, pages 6083-6093. PMLR, 2020.
R. J. Little, D. B. Rubin, and D. B. Rubin. Statistical analysis with missing data: Wiley series in
probability and statistics. 2002.
D. Liu, P. Cheng, Z. Dong, X. He, W. Pan, and Z. Ming. A general knowledge distillation framework
for counterfactual recommendation via uniform data. In Proceedings of the 43rd International
ACM SIGIR Conference on Research and Development in Information Retrieval, pages 831-840,
2020.
D. G. Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997.
A. Maurer and M. Pontil. Empirical bernstein bounds and sample variance penalization. arXiv
preprint arXiv:0907.3740, 2009.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781, 2013.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2018.
S. T. Rachev. The monge-kantorovich mass transference problem and its stochastic applications.
Theory of Probability & Its Applications, 29(4):647-676, 1985.
I. Redko, A. Habrard, and M. Sebban. Theoretical analysis of domain adaptation with optimal
transport. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pages 737-753. Springer, 2017.
S.	Rendle, W. Krichene, L. Zhang, and J. Anderson. Neural collaborative filtering vs. matrix fac-
torization revisited. In Fourteenth ACM Conference on Recommender Systems, pages 240-248,
2020.
R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business
Media, 2009.
P. R. Rosenbaum and D. B. Rubin. The central role of the propensity score in observational studies
for causal effects. Biometrika, 70(1):41-55, 1983.
D. B. Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal of
the American Statistical Association, 100(469):322-331, 2005.
N. Sachdeva, Y. Su, and T. Joachims. Off-policy bandits with deficient support. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
pages 965-975, 2020.
Y. Saito. Asymmetric tri-training for debiasing missing-not-at-random explicit feedback. In Pro-
ceedings of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 309-318, 2020.
12
Published as a conference paper at ICLR 2022
Y. Saito, S. Yaginuma, Y. Nishino, H. Sakata, and K. Nakata. Unbiased recommender learning from
missing-not-at-random implicit feedback. In Proceedings of the 13th International Conference
on Web Search and Data Mining, pages 501-509, 2020.
T.	Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations as treat-
ments: Debiasing learning and evaluation. In international conference on machine learning,
pages 1670-1679. PMLR, 2016.
U.	Shalit, F. D. Johansson, and D. Sontag. Estimating individual treatment effect: generalization
bounds and algorithms. In International Conference on Machine Learning, pages 3076-3085.
PMLR, 2017.
N.	Si, F. Zhang, Z. Zhou, and J. Blanchet. Distributionally robust policy evaluation and learning in
offline contextual bandits. In International Conference on Machine Learning, pages 8884-8894.
PMLR, 2020.
B.	K. SriPerUmbUdur, K. FUkUmizu, A. Gretton, B. SchOlkopf, and G. R. Lanckriet. On integral
probability metrics,\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698,
2009.
H. Steck. Training and testing of recommender systems on data missing not at random. In Pro-
ceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 713-722, 2010.
R. S. SUtton and A. G. Barto. Reinforcement learning: An introdUction. 1998.
A. Swaminathan and T. Joachims. CoUnterfactUal risk minimization: Learning from logged bandit
feedback. In International Conference on Machine Learning, pages 814-823. PMLR, 2015a.
A. Swaminathan and T. Joachims. The self-normalized estimator for coUnterfactUal learning. ad-
vances in neural information processing systems, 28, 2015b.
X. Wang, R. Zhang, Y. SUn, and J. Qi. DoUbly robUst joint learning for recommendation on data
missing not at random. In International Conference on Machine Learning, pages 6638-6647.
PMLR, 2019.
X. Wang, M. Long, J. Wang, and M. I. Jordan. Transferable calibration with lower bias and vari-
ance in domain adaptation. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
T. Xiao and D. Wang. A general offline reinforcement learning framework for interactive recom-
mendation. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, 2021.
D. XU, C. RUan, E. KorpeoglU, S. KUmar, and K. Achan. Adversarial coUnterfactUal learning and
evalUation for recommender system. Advances in Neural Information Processing Systems, 33,
2020.
D. XU, C. RUan, E. KorpeoglU, S. KUmar, and K. Achan. Theoretical Understandings of prodUct
embedding for e-commerce machine learning. In Proceedings of the 14th ACM International
Conference on Web Search and Data Mining, pages 256-264, 2021a.
K.	XU, M. Zhang, J. Li, S. S. DU, K. Kawarabayashi, and S. Jegelka. How neUral networks extrap-
olate: From feedforward to graph neUral networks. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b.
L.	Yang, Y. CUi, Y. XUan, C. Wang, S. Belongie, and D. Estrin. Unbiased offline recommender eval-
Uation for missing-not-at-random implicit feedback. In Proceedings of the 12th ACM Conference
on Recommender Systems, pages 279-287, 2018.
13
Published as a conference paper at ICLR 2022
A Appendix on Background and Main Theoretical Results
We provide the supplementary background introduction, proofs for the claim, proposition and the-
orem, as well as the detailed experiment settings and complete numerical results in this part of the
paper.
A. 1 Recommender with exploration policy, and learning from bandit
FEEDBACK DATA
Using multi-armed bandits for exploration has attracted increasing attention from the recommen-
dation literature in recent years. The major advantage is that the interactive process between users
and recommender is explicitly characterized in an online fashion. Exploration also helps alleviate
the exposure bias, popularity bias, position bias, etc, via more efficient exploitation of the feedback
data. A critical difference between the bandit recommender and the deterministic recommender we
referred to in our paper is that an item is exposed to the user with randomness, and the exploration
policy controls the randomness in the exposure.
Mathematically, we continue with X as the context (feature) space, I = {1, . . . , k} be the action
(item) space. The exploration-exploitation strategy can be viewed as a sequential game between the
user and recommender. For notation simplicity, we introduce the stationary bandit setting where
the exploration policy does not depend on the history. In each round, an item is sampled according
to the exploration policy π(u, i) := π(i | xu), which is a mapping between the item space and
the probability space, and expose it to the user. Then the user, whose feature is xu , reveal the
regret (or reward) action i ∈ I, which corresponds to the Yu,i in our setting. In the context of
delivering recommendation, clearly we have: P(reco(u, i; f) = 1) = π(u, i) during exploration,
and in the context of learning from the logged feedback data, we have P(Oui = 1) = π(u, i) since
it characterizes the probability of exposure.
Similar to the deterministic recommender, the challenge in bandit offline learning is largely due to its
partial-observation nature as we do not observe the complete reward for all items. However, unlike
the deterministic recommender, the random exposure mechanism allows efficient estimation of the
counterfactual regret, i.e. what the regret would have become if an alternative exploration policy
was used. The IPW estimator fits particularly well to this purpose, since it provides the unbiased
estimation of the counterfactual regret:
E
f(u,i
Pn ΠR7)
EPf Yui .
Swaminathan and Joachims (2015a) refers to the IPW-reweighted risk, which essentially replace the
regret Yui by the corresponding risk under f, as the counterfactual risk for learning from the logged
bandit feedback data. It exactly corresponds to setting the base measure P with PIPW, so it holds:
EpfR(f ) = P(u,i)∈D ∏g⅛ '(f(u,i),yui).
A.2 Proof and Background for the IW and DA bounds in Proposition 1
We first prove the upper bound for importance weighting under weak overlap .
Proof. Our proof leverages the classical ”double sampling” technique from Anthony and Bartlett
(2009). We use ~z = [z1, . . . , zn] to denote the observed samples, and ~z0 = [z01, . . . , z0n] to denote an
i.i.d copy of ~z. We first define by:
1	G A / 3	3Mt	∕2d(PkQ)t
UBι(f,~,t) = nɪswi`f(Zi) + ~n~ + V n ,
i=1
and
…、	1 G A 9 9	9Mt	∕l8d(PkQ)t
uB2(f,~,t) = nɪswi`f(Zi) + ~n~ + y n .
i=1
14
Published as a conference paper at ICLR 2022
Given f ∈ F ,let A := EqR(∕ ) + 6Mt + /8d(BQ)t it holds：
P(UB2(f,~0,t) ≤ UB1(f,~,t) ≤ P(UB2(f,~0,t) ≤ A) + P(UB1(f,~,t) ≥ A)
≤ 2P(X(f) - 1 XWi"l≥ 3Mt + r2d(P≡)
≤ 4e-t,
where the last line follows from Lemma A.2. Next, We define C(g ' ◦ F, 'ι(Pn,w)) be the E-Cover
of ` ◦ F with the empirical `1 norm under Pn,w such that for any f ∈ ` ◦ F, there exists f in
C(e,' ◦F,`n): In P Wif (Zi)- n P Wif(zi)∖ ≤ gfor (zι,... ,Zn) sampled i.i.d from P. It then
holds:
P(∃f ∈ F : EqR(∕) ≥ UB2 (f,~,t)+。
= E~z sup I[EQR(f) ≥UB2 (f,~Z,t)+E]
f∈F
(a)
≤ E~ sup I[EqR(∕) ≥ UB2(f, Z,t) + e] ∙ 2E~oI[UB1(f, ~0,t) ≥ EqR(∕)]
f∈F
≤ 2E~z,~z0 sup I[U B1(f, ~Z0 ,t) ≥U B2(f, ~Z, t) + e]
f∈F
≤) 2Pσ(z,~0)(∃f ∈C(e,' ◦F ,'ι(Pn,w ))： UBι(f,σ(~,~0),t) ≥ UB2(f, σ(~, ~0), t))
≤ 8N(e,'◦F,'ι(Pn,w)) ∙ e-t,
where (a) follows from the fact that E~oI[UBι(f, ~, t) ≥ EQR(f)] ≥ ɪ as suggested by Lemma
A.2, and in step (b) we let σ(~Z, ~Z0 )i takes the value ofZi, Z0i with equal probability, and the inequality
follows from the definition of the e cover. Notice that N(e,' ◦F, 'ι(Pn,w)) ≤ N(e∕M,' ◦F, '∞).
We take e = ɪ, which solves for t = C log 1 + log N(e/M,' ◦F, '∞) for some constant C and leads
to the desired result after arranging terms.
The corresponding lower bound follows directly from Lemma A.4, where we make a mild assump-
tion that there exists a hypothesis f ∈ F such that EPn R(f) = 1, so it holds that EPn,w R(f) =
d(P∣∣Q) - 1 suggested by Lemma 1 of Cortes et al. (2010).
We then show the PAC-Bayesian result that stated for the partial-overlap case. Our proof relies on
the existing results in the PAC-Bayesian literature as in Germain et al. (2015).
Proof. We use ` to denote the 0 - 1 loss for brevity. The PAC-Bayesian approach concerns with the
GibbS risk of Ef 〜TE(u,i)〜q'(∕(u, i), yu) where T is some posterior distribution on the hypothesis
space F. Clearly, when there are mismatch in the supports of SP and SQ, the Gibbs risk for the
mismatched part is given by Ef〜TE(u,i)∈SQ/SpR(f). We now focus on the overlapped part. Using
the classical decomposition result from Germain et al. (2015), which states that:
Ef 〜TEQR(f ) = 1 dQ(τ) + eQ (τ),
where d。(T) = Effo〜TEq'(∕(u, i), f0(u, i)) is a discrepancy term, and cq(t) =
2Ef,fo〜TEq'(∕(u, i), yui)'(f(u, i), yui) is the Gibbs risk for the joint hypothesis. Therefore, we
have:
Ef 〜T EQR(f ) - Ef 〜TEPw R(f) = (2 dQ (T) - 2 dPw (T)) + (eQ(τ ) - eP (T))
≤ 2 IdQ(T) - dPw (T)∣ + |eQ(T) - ePw (T)l,
2 '---------------{z----------------}
D(T)
where D(T) essentially characterizes the disagreement ofQand Pw in terms of the posterior T on
F. Therefore, for any posterior distribution T, We have: Ef〜TEQR(f) ≤ Ef〜TEPWR(f) + D(T),
15
Published as a conference paper at ICLR 2022
so the next step is to bound the Gibbs risk of Ef〜TEPwR(f).OUr strategy follows that of Germain
et al. (2015), that we employ the convex function ∆ : [0, M] × [0, M] → R. It then holds that:
n ∙ ∆(Ef〜TEpn,wR(f), Ef〜TEPwR(f))
≤ nEf〜T∆(Epn,wR(f), EPwR(f))
≤ dκL(τ k∏)+ln Ef 5 exp g(Ep“,w R(f), EPw R(f)))
by Lemma A.7
w.p. 1-δ
≤
dκL(τIIn) + ln δEf〜∏Ez1,…诙〜Pexp (n∆(1 XWEk)'f (zk),EZ〜Pw(z)'f (z))),
k
where the last line comes from the Markov inequality, and w(zk) is a short-
hand for w(u, i) for zk	=	(u, i, yui).	We then focus on bounding:
Ef〜∏EZk〜P exp (n∆(1 Pk w(zk)'f (zk), EZ〜Pw(z)'f(z))). Since 'f is the 0 — 1 loss,
the term inside ∆ (setting aside w(zi)) are essentially binomial variable of n trials with a success
rate of EP R(f). Since ∆ is convex in both arguments and w(u, i) is given by Q(u, i)/P (u, i), it is
easy to verify that:
Ezk〜Pexp (n∆(1 XWlZk)'f (zk),EZ〜Pw(z)'f (z)))
k
≤ d(P,Q) ∙ Ezk〜Pexp (n∆(1 X'f(zk),EPR(f))),
where d(P, Q) = S (dP /dQ)dP as we mentioned in the statement. It is then obvious that:
1	nk
EZk 〜P exp (n∆ n~ X 'f(z), EP R(f))) ≤ sup {£ Bin(k;n; r) exp (3-,r))},
n k	r∈[0,1] k=0	n
where we use r ∈ [0, 1] to replace the role of EPR(f), since the prior π is arbitrary, and Bin(-; n; r)
is probability of having - success in n rounds of Bernoulli trail where the success rate is r. It is
shown by Germain et al. (2009) that the RHS is O(1) by choose a particular ∆, so we obtain the
desired result by putting together the above arguments.
Now we briefly introduce the skip-gram negative sampling algorithm and prove its associated
rersults in Proposition 1. It have been shown by the recent work of Xu et al. (2021a) that the
user/item embeddings optimized via the skip-gram negative sampling (SGNS) algorithm are the
sufficient dimension reduction of a specific co-occurrence statistics. In particular, the SGNS objec-
tive function is given by:
P(Oui = 1"i) = σ(φ(i)lφ(U)) = ι + exp( -1φ(i)∣φ(u)),
P(Oui = O"" = σ( - φ(i)lφ(U))= "exp(；”◎(U)),
'u,i = — log P(Oui = 1∣U,i) + k ∙ Ek 〜Neg(I) [log P(Oui = 0∣u,i)],
where Φ(U), Φ(i) are the embeddings for the user U and item i, -is the number of negative samples,
and Neg(I) is the uniform negative-sampling distribution on the item set. The risk objective is thus
give by: EP R(Φ) = P(u,i)∈D `u,i. The SGNS algorithm has been widely applied to train user and
item embeddings for various recommendation tasks.
The sufficient dimension reduction (SDR), on the other hand, is inspired by the sufficient statistics
of exponential family. Broadly speaking, when predicting Y with X, if all the information in X
about Y can be compressed into a dimension reduction f(X), i.e. Y ⊥ X | f(X) or p(Y |X)
and p Y |f(X) are the same, then f(X) is a sufficient dimension reduction (SDR). According
16
Published as a conference paper at ICLR 2022
to Globerson and Tishby (2003), finding the SDR f (X) is equivalent to solving a optimization
problem:
min dKL(pYX) k q(Y,f(X))),	(A.2)
q(y,f (x))
where dKL is the Kullback-Leibler divergence.
Proof. Let Rui := NN be the relatedness measure defined in XU et al. (2021a), where Nui is the
NuNi
co-count of (u, i), and Nu, Ni are the individual counts of u and i. The co-occurrence statistics is
given by Cui := Oui | Rui , which is exposure Oui condition on the relatedness measure. It is shown
in the Claim 1 of Xu et al. (2021a) that:
min	EP R(Φ)
Φ(u),Φ(i)∈Rd
φ(u)mφ⅛∈Rd |D| X dKL(P(Oui I φ(U), φ⑶)∣l P(Oui IRui)),
,	(u,i)∈D
(A.3)
so Φ gives the SDR of Cui in Rd, according to the definition of (A.2). Note that the terms in the
RHS of (A.3) are all binary KL divergence, and it easy to verify that binary KL divergence satisfies:
dκL(a∣∣c) + dκL(ckb) ≥ dκL(a∣∣b) + (c - a) log c(--b)). Recall that Cui(P) and Cui(Q) are the
co-occurrence statistics under P and Q, respectively. Therefore, if we let Φ(P ) and Φ(Q) be the
solution to (A.3) under P and Q, by assuming SP = SQ, it holds that:
inf (EpR(Φ) + EqR(Φ)) ≥ inf EPR(Φ) + inf EPR(Φ)
≥ iS7	X dκL(p(Oui I φ(P)(U), φ(P)(i))∣∣ Cui(P))) +
P (u,i)∈SP
dκL(p(Oui I Φ(Q)(U), Φ(Q)(i)) Il Cui(Q))
=Γξ-1	^X dκL(Cui(P) IlCui(Q)) + C,
P (u,i)∈SP
and C = infφ (EPR(Φ) + EqR(Φ)) when P = Q a.s.
□
A.3 Background on the Wasserstein Distance
Perhaps the two most common discrepancy measures on probabilities are φ-divergence and integral
probability metric (IPM). Obviously, φ-divergence does not satisfy our need of characterizing do-
main transportation because it requires SP ⊆ SQ (weak overlap) to hold in practice. IPM, on the
other hand, provides the flexibility and is defined as
dG(P,Q) = sup	gdP - gdQ,
where G is a class of real-valued bounded measurable function. In this direction, we find the Wasser-
stein distance from the optimal transportation (OT) theory Rachev (1985) a good fit to our goal.
Although the maximum mean discrepancy distance is also valid, its computation and related theo-
retical results rely on the specification of a reproducing kernel Hilbert space, whose usage is less
common in the IR literature.
The origin of the Wasserstein distance is to give the optimal cost of transporting mass from one
marginal distribution to another:
dw,c(P, Q)
arg min
π∈Π(P,Q) D×D
c(z, z0)dπ(P, Q),
where Π(P, Q) is the set of all joint probability measure whose marginals are P and Q, and c(∙, ∙) is
the cost function. Clearly, dw,c(P, Q) relies on the geometry ofP and Q if the cost function is well-
chosen. For recommender system, z is often given by (xu, xi) or (Φ(u), Φ(i)), so it is natural to use
the k ∙ ∣2 distance: c(z, z0) = ∣∣z - z0∣∣2. We denote by dw for this particular cost function, which
17
Published as a conference paper at ICLR 2022
is also referred to as the 1-Wasserstein distance. The classical Kantorovich-Rubinstein theorem
Dudley (2018) connects the dW to IPM by proving the duality result of:
dW
sup gdP - gdQ.
g：{kgkL≤1}JD	JD
(A.4)
In the above formulation, the constant of 1 under sup is less critical since we can always rescale
g. The class of Lipschitz-bounded function covers a wide selection of models including neural
networks as discussed in Fazlyab et al. (2019).
A.4 Proof for Theorem 1
We first state a proposition for the following proofs.
Proposition A.1. Suppose `f is semi-continuous for all f ∈ F, P is any distribution supported on
Z, and c(z, z0) = kz - z0k2. It holds:
min
Q:dW (Q,P)≤ρ
EqR(∕) = sup { - λρ + EZ〜P [ inf {'f (Z) + λ ∙ c(z, z)}]}.
λ≥0	z∈Z
Proof. It is shown by Gao and Kleywegt (2016) that:
max	EqR(∕) = inf {λρ + EZ〜P [ sup{'f (Z) — λ ∙ c(z, z)}]}.
Q：dw (Q,P )≤ρ	λ≥0	z∈Z
Following the above result, we immediately have:
min	EQR(f) = - max	-EQ R(f)
Q:dW (Q,P)≤ρ	Q:dW (Q,P)≤ρ
= -inf {λρ + EZ〜P [ sup{-'f (Z) - λ ∙ c(z, z)}]}
λ≥0 L	lz∈z	」J
=sup { - λρ - EZ〜P [ sup{-'f (Z) - λ ∙ c(z, z)}]}
λ≥0	Z∈Z
=sup { - λρ + EZ〜P [ inf {'f (z) + λ ∙ c(z, z)}]}.
λ≥0 L	lZ∈Zt
□
Now we prove the result in Theorem 1.
Proof. We first define Hi = {w ∈ H : dw(Pw,Pf) ≤ ρ}, and Hi = {w ∈ H : dw(Pw,Pf) ≤
P + ni/Cw+1)+ 8Mʌ/lonɪ}, where Cw is the constant stated in Lemma A.5.
We then omit the w ∈ H in the following proofs for brevity. By Lemma A.5, for any δ > 0, it holds
with probability at least 1 - δ that Hi ⊆ H2, which suggests that for any given f ∈ F:
w：Dmin)≤ρ EPwRf) ≥
min
w:Dn(w,f )≤δ+B
EPwR(f)
with probability at least 1 - δ, where B = 於渊+1)+ 8Mʌ/ɪonɪ.
Now We define W = argminw：Dn(wf)<δ+B EPwR(f). Then using the result in Lemma A.1, for
any given f ∈ F, it holds with probability at least 1 - 4 that:
min	EPw R(f )= EPw R(f)
w:Dn(w,f )≤δ+B
-
≥ EPn,w R(f)
7M log 1
3n
2Rw (f )2 log 1
≥ min
w:Dn(w,f )≤δ+B
EPn,wR(f)
2Rw (f )2 log 1
—
—
n
7M log 1 _ 1
-3n	V `
n
where the last line follows from the definition of W.
18
Published as a conference paper at ICLR 2022
Using Proposition A.1, we have:
C mn…口 EPm,w R(f) = SUP { - λ(P + B) + EZ〜Pn,w [ Sf {'f (Z) + λ ∙ C(Z, Z)H }
w:Dn(w,f )≤δ+B	λ≥0	z∈Z
≥ Sup { - λP + EZ〜Pn,w [ !nf {'f (Z) + λ ∙ C(z, Z)}]} - L ∙ B
λ≥0	z∈z
= min EP	R(f) - L ∙ B,
W：Dn(W,f )≤δ	n,w '
where the second line uses the result in Lemma A.6 which bounds the λ* that achieves the supremum
with the Lipscitize constant L. Combining the above results, it holds with probability at least 1 一 δ
that:
-
1
δ
min
w:D(w,f )≤ρ
EPwR(f) ≥
min
w:Dn(w,f )≤δ
EPn,w R(f) -
7M log 1
3n
2Rw (f )2 log
- L ∙ B.
n
The upper bound can be shown in the same manner, by using W = arg minw：Dn(w,f )≤δ-B EPw R(f)
to replace W. Combining the upper and lower bounds, We obtain the result stated in Theorem 1.
□
A.5 Proof for Theorem 2
Suppose ' is some bounded loss function, then MF stated in Theorem 2 must be finite for λ ∈ [0, P],
otherwise it can be shown from Proposition A.1 and Lemma A.6 that:
min	EqR(∕) = sup { - λρ + EZ〜P[inf 'f (Z) + λ ∙ c(z, Z)]}
dw (Q,P )≤ρ y	λ≥0	≡∈z
is unbounded for finite ρ, which contradicts that ` is bounded.
We first prove the generalization error bound.
Proof. We use the shorthand φf,λ(Z) := EZ〜Pf [infZ 'f (Z) + λc(Z, z)]. Since we do not make any
parametric assumption on the weighting function W here (as the consequence of using W ∈ H has
been stated in Theorem 1), we have:
min	EqR(∕) = sup { - λρ + EZ〜Pf φf,λ(Z)} By Proposition A.1
dW (Pw,Pf )≤ρ	λ≥0
=sup { - λρ + EZ〜Pnf Φf,λ(z) + EZ〜Pf Φf,λ(z) - EZ〜Pnf Φf,λ(z) }
λ≥0	,	,
≤ suP { - λP + EZ 〜Pn,f + suP {EPf φf,λ(z) - EPn,f φf,λ(z)} }
λ≥0	f∈F
、---------------V----------------}
ψλ
Using the standard symmetrization argument, we can upper-bound Eψλ	=
n supf∈f {Ejf φf,λ(z) - φf,λ(zj} with the empirical Rademarcher complexity:
1n
Eψλ ≤ 2E sup - V"ciφf,λ(zi),
f∈F n i=1
where the expectation on LHS is taken with respect to the random variables of z1 , . . . , zn . Clearly,
ψ(λ) is bounded by MF that we illustrated in the beginning, so from McDiarmid’s inequality we
have:
P(ψ(λ) ≥ Eψ(λ) + √t) ≤ exp(-2t2).	(A.5)
19
Published as a conference paper at ICLR 2022
Also, note that y ɪEiφf,λ(zi) is zero-mean and 1-subgaussian with respect to the ∣∣ ∙ ∣∣∞ norm on
F, so for any f1, f2 ∈ F, it holds that:
tn
E[eχp 匕 Eei(φfι,λ(Zi)- φf2,λ(Zi)))]
i=1
≤ E[exp (tj=∙ ∙ ( inf {fι(Z) + λc(zι, Z)} + sup{-f2(Z) - λc(zι, Z)}))]n
√η	'z∈Z	z∈z
≤ E[exp (tj=s sup{fι(z) - f2(z)})]n
n z∈Z
(A.6)
≤ exp
t2 kf1 - f2 k∞
2
Using the entropy integral bound on the empirical Rademarcher complexity term, we have:
Eψ(λ) ≤ F Γ PlogNgF,'∞(Pn))de,
n0
since ' is Lipscitz (and we absorb the Lipscitz constant into C). Combining the above results, for any
given λ ≥ 0, it holds with probability at least 1 - δ that:
p(∃f ∈ F :	min	EqR(∕) ≥ -λρ + Ez~p f [ inf 'f (Z) + λc(z, Z)] +
V	dw(Pw,Pf)≤p	一	n,f ⅛∈z
C∕l∞ SogN(JF,'∞)de + MFt) ≤ 2eχp(-2t2).
n
n
Then we apply the same union-bound argument from Theorem 1 Koltchinskii et al. (2002) to extend
the above result to any λ > 0. In particular, we pick the sequence of λk = k and tk = t +
√logk, k = 1, 2,.... By simple calculations and the sum of infinite series arguments, we reach the
conclusino that for any f ∈ F, it holds with probability at least 1 - δ that:
H ,minv EQR(f) ≥ min n - λP + Ez~Pn,f[ inf 'f(z) + λc(z, z)] +
dW (Pw ,Pf )≤ρ	λ≥0	z∈Z
C R∞ PlogNk F,'∞)d + MF/ogɪ +

log(λ + 1)
)
□
n
n
n
Now we prove the excessive risk bound.
Proof. We start by defining: f
arg minf∈F minDn(w,f)≤ρ EPn,wR(f),
argminf∈F minD(w,f)≤ρ EPn,w R(f). Using the definition of f, we have:
—	_ , O,	_	_ ,..
min EPwRf) -	min	EPwR(f )
D(w,f)≤ρ	D(w,f*)≤P
—	_ , O,	_	_ , O,	_	_ ,.,.
≤ min EPw R(f) — min EPw R(f)+	min EPw R(f ) — min
D(Wf)≤ρ	W	Dn(w,f)≤ρ	W	Dn(w,f *)≤ρ	w	D(w,f*)≤ρ
and f *
EPwR(f*).
- C
Let λ
一 O
arg maxλ≥o { - λρ + Ez~p^[inf⅛∈z '/(Z) + λ ∙ c(z, Z)]}, and from Lemma A.6 We know
that λ ≤ L. It holds that:
—	— ， A

__ 一 , ',, ____________ 一 ,',,
min EPw R(f) - min	EPw R(f)
D(Wf)≤P	Dn(Wf)≤P
=(-λρ + Ez^Pf Linf.'/(Z) + * ∙ c(z, Z)]) - maχ { - λρ + Ez^Pf [inf.'/(Z) + λ ∙ c(z, Z)]}
≤ L (inf '/(Z) + *c(z,Z))d(P∕ - Pf
≤ SUp / (inf '^(Z)+ λc(z, Z))d(P^ - Pnf).
λ∈[0,L],ψ∈'◦FJZ z
20
Published as a conference paper at ICLR 2022
Again applying the standard symmetrization argument and McDiarmid’s inequality to the last line
(similar to what we have shown in proving the generalization bound), the following bound holds
with probability at least 1 - δ∕2:
min EPw R(f) - min	EPw R(f) ≤ 2Radn([0, L] 0 ' ◦F) + MF
D(Wf)≤P	Dn(Wf)≤P
Similarly, We let λ* = arg maxλ≥o { - λρ + Ez~Pf* [inf z∈z f* (Z) + λ ∙ c(z, Z)]}, and using the
same arguments, we also have the following bound that holds with probability at least 1 - δ∕2:
min	EPw R(f *) - min	EPw R(f *) ≤ 2Radn([0,L] 0 ' ◦F) + MF
D(W,f*)≤ρ	Dn(W,f *)≤ρ
Combining the above two bounds, we obtain the desired result.	口
V-
2log 2
n

A.6 Proof for Claim 1
Proof. Given f ∈ F, the mapping of PW to dW (PW , Pf) is convex in the space of probability
measures. Since dW (PW, Pf) = 0 can be achieved by taking PW = Pf a.e, using the generalized
Lagrange duality result in Lemma A.3, it holds that:
dw (PinPf JZ 'f (z)dpw=λ≥p dw (PinPf )≤ρ{-WZ Q ⑺皿+λ ∙ dw (PW ,Pf)}.
Notice that for any π ∈ Π(PW, Pf), where Π(PW, Pf) is the set of all joint distributions whose
marginals are given by PW and Pf, we have: Z `f (z)dPW = Z Z `f (z1)dπ(z1, z2). Therefore,
we have:
L `f (z)dPw+∏∈∏inw,Pf)!×Zz c(z1, z2)dπ(z1, z2)=∏∈∏inw,Pf){LJf (ZI)+λdz1, Eh
using the definition of the Wasserstein distance introduced in Appendix A.3. As a consequence, it
holds that:
inf	`f (z)dPW
dw(Pw,Pf)≤ρ Z
sup inf
λ≥0 W,π∈Π(Pw,Pf)
{ - λP + L Jf (ZI)
+ λ ∙ c(zι, Z2)}.
We first point out that:
inf
W,π∈Π(Pw,Pf)
{
Z×Z
'f(zι) + λ ∙ c(zι, Z2)} ≥ / inf {'f (zι) + λ ∙ c(zι, Z2)}dPf (z2),
(A.7)
and in the following proof we show that the other direction also holds, so the above inequality
becomes equality. Also, if we consider M as the space of all the regular conditional probabilities
from z2 to z1 , it is evident that:
(A.8)
/ 'f(zι) + λ ∙ c(zi, Z2)dM(Z1∣Z2)dPf (Z2)}.
1inf C {/ 'f (Z1)+ λ ∙ C(Z1,z2)dPf(Z2)}
W,π∈Π(Pw ,Pf)	Z
≤ inf
M∈M
Furthermore, using Lemma A.8, if we consider X as the space of all measurable mappings from Z
to Z, then it holds:
Jnf { / 'f (x(z)2)) + λc(x(z2), Z2)dPf(z2)} = J inf {'f(zι) + λ ∙ c(z1,Z2)}dPf(z2)∙
Consequently, if we let X ∈ X be the mapping that achieves e-close to the above infimum (which
requires certain continuity condition for 'f) and M(∙∣Z2) be defined by X(z2):
' 'f (zι) + λc(zι, z2)dM(z1∣z2)dPf(z2) = ' 'f (X(z2)) + λc(z2,Z2)dPf(z2)
ZZ
≤ i inf {'f (zι) + λ ∙ c(zι, z2)}dPf (z2) + e
Z z1
≤	Iinf 0、{ / 'f (z1)+ λ ∙ c(z1,z2 )dPf (z2)} + e
W,π∈Π(Pw,Pf)	Z
(A.9)
21
Published as a conference paper at ICLR 2022
Since > 0 is arbitrary, combining (A.7), (A.8) and (A.9), we have:
UITr∈Mf F ){∕ 'f (Z1) + λ ∙ c(z1,Z2)dPf(Z2)} = ∕ inf {'f (Z1) + λ ∙ c(z1,Z2)}dPf (Z2),
and using the result in Proposition A.1, we obtain the final that for any f ∈ F:
min	EPw R(f) = max{ - λρ+min{EPwR(f) + λ ∙ dw(Pw,Pf)}}.
dW (Pw ,Pf )≤ρ	λ≥0	Pw
□
A.7 Proof for Theorem 3
Proof. We first let f* = arg minf∈F EPf R(f), and define w* = f* (suppose F ⊂ H) such that
Pn,w* (u, i) = Pn,f* (u, i) and Pw* (u, i) = Pf* (u, i). Since F is the class of valid exploration
policies in the bandit setting, we have f(u, i) ∈ (0, 1) for all f ∈ F. Note that by the definition of
f* and w* :
min EPn,wR(f)+λ∙dw(Pn,w,Pn,f) ≤EPnw*R(f*)+λ∙dw(Pn,w*,Pn,f*).
f ∈F,h∈H	,	,
Combining the results in Lemma A.1 and Lemma A.5, it holds with probability at least 1 - δ∕2 for
any f ∈ F and w ∈ H that:
EPn,w*R(f*) - Epw* R(f*) ≤ 7⅛δ + J
2Rw*(f*)2 log 表
n
(dW (Pn,w* , Pn,f* ) ≤ nɪ/(w+ι) + 8Mg√ lonɪ
Hence, the following inequality holds with probability at least 1 - δ:
min EPn,wR(f)+λ∙dw(Pn,w,Pn,f)
f ∈F,h∈H
≤ miF EPfR(f )+7⅛1+J
2Rw* (f*)2 log 表
+n
1/(d+1)
+ 8Mg 百.(A'0)
n
n
On the other hand, with λ & L ≥ 1 where L is the LiPschitz constant for ' ◦F, for any given f ∈ F
and w ∈ H, it holds that:
EPf Wf ) = EPw Wf KL `f(Z)
Pw孚+ R卜
π(Z)	π(Z)
—
. EPw R(f) + sup EPw ψ(z) - EPfψ(z)
ψ∈'oF
≤ EPwR(f) + L ∙ dw(Pw,Pf),
where in the first line we use the background of learning from bandit feedback data where π is the
logging Policy (e.g. π(z) = π(u, i) for z = (xu, xi, yui)); and the last line we use the definition of
Wasserstein distance under the Kantorovich-Rubinstein theorem (APPendix A.3). Again, by comb-
ing Lemma A.1 and Lemma A.5, We can further bound the last line with probability at least 1 - δ∕2
with:
Epn,w R(f )+L∙dw (Pn,w ,Pn,f )+λ( ^on4δ+JRw (f ；log S+n
、--------------------V--
slack terms
+8MGv lonɪ)
1/(d+1)
〜
Now we let f,w = argminf∈f,w∈hEp””R(f) + λ ∙ dw(Pn,w,Pn,f) for λ & L ≥ LThen it
holds that:
— _ , .. _ _ , ~.
mm EPf R(f) ≤ EP/(f)
f∈F	f
≤ Epn,w R(f) + λ ∙ dw(Pn,w,Pn,f) + slack terms,
(A.11)
where the slack terms are stated above. Combining (A.10) and (A.11), we obtain the desired result.
□
22
Published as a conference paper at ICLR 2022
A.8 Analysis on the GDA algorithm.
By using bounded Lipschitz functions for wθ1, fθ2, we will show below that the gradient of Lλ is
L-Lipschitz. It is indicated by Daskalakis and Panageas (2018) that GDA with η ≤ 1/L avoids
unstable critical points for almost all initialization, which means that (θ(t, θ2t), θ3t)) is unlikely to
cycle as t grows. Furthermore, any stationary point yielded by GDA with large γ is basically a
local minimax point except degenerated case (Jin et al., 2020), and thus the convergence guarantee
of Algorithm 1. Here, a solution (θɪ, θ2, θ3) is local minimax if there exits δo > 0 and a function
h satisfying limδ→0 h(δ) → 0 such that ∀δ ∈ (0, δ0], and any (θ1 , θ2, θ3) satisfying [θ1 , θ2] -
[θɪ, θ2]∣∣2 ≤ δ and ∣∣θ3 - θ3k2 ≤ h(δ), it holds that:
Lλ(阴, θ^, θ3) ≤Lλ(®, θ2], θ3) ≤ “0 W maχ	Lλ([θι, θ2], θ3).
θ3 ”∣θ3-θ3k2 ≤h^
In other words, by using the two-time-scale GDA algorithm with a small η and a large γ, our re-
sulting solution has a similar role as a local minimum/maximum in a minimization/maximization
problem — the global Nash equilibrium, if any, must be one of the solutions yielded by the GDA in
Algorithm 1.
Proof. Here, we show that if wθ1 , fθ2 are bounded Lipschitz functions and gθ3 is a Lipschitz func-
tion, the gradient of the minimax objective is also Lipschitz.
Recall the objective function in (6), i.e.,
Lλ([θι, θ2], θ3):= X wui(θ2)'ui(fθ1)
(u,i)∈D
+λ E wui(θ2) ∙ gui(θ3)- E reco(u, i; fej ∙ gui(θ3).
(u,i)∈D	(u,i)∈D
It is straightforward to show that:
∂Lλ
沟
∂Lλ
∂θ2
∂Lλ
∂θ3
L g ∖d'ui(fθι )	∖ V g ∖ d reco(U,i; fθI)
T wui(θ2 ) ~lθ1-----------λ T gui(θ3)-------------go；------
X 'ui(fθι) dwu^ + λ X gui(θ3) ∙
∂θ2
(u,i)∈D	(u,i)∈D
∂Wui(θ2)
∂θ2
λ I X Wui(θ2) ∙ dgufθ3) — X reco(u, i; fej
∂θ3
(u,i)∈D	(u,i)∈D
∂gui(θ3)
∂θ3
We impose a few assumptions on fθ1, wui(θ2) and gui(θ3). First, we assume they are bounded and
their gradients are Lipschitz functions, which are standard assumptions for neural or factorization
models. Then, with a careful selection of the loss function ` (e.g., logistic loss, exp-loss, probit loss)
and the reco function (e.g., top-K), we can ensure `ui(fθ1 ) and reco(u, i; fθ1 ) to have Lipschitz
gradients. Finally, by employing clipping tricks on the gradients to ensure they will not explode,
which is also a common practice, we have universally bounded and Lipschitz functions that include:
fθι, Wui(θ2), gui(θ3), ⅛fθ2, reco(Uθlfθ1)
∂wui(θ2) gui(θ3)
-∂θ2-,	∂θ3
Therefore, the products and summations of these function is a L-Lipschitz function for some positive
constant L.	口
B Lemmas and Proofs
In this part of the appendix, we state the important lemmas that we used in our proofs.
Lemma A.1 (Empirical Bernstein bound). Suppose that `f : Z → [0, 1] without loss of generality.
With w(u, i) = Q(U；) and zι,..., Zn SamPled i.i.dfrom P ,for any δ > 0, it holds with probability
23
Published as a conference paper at ICLR 2022
at least 1 - δ that:
EQR(f) ≤ EPn,w + 7M3Iogi + Sif 星,
3n	n
where RW(f )2 = n P w2'f (zi)2∙
The result is modified from Corollary 5 of Maurer and Pontil (2009).
Lemma A.2. Let P and Q be the training and target distribution supported on SP, SQ ⊆ D, and
w(u, i) = Q(U；), for (u, i) ∈ Sp ∩ SQ. Dn consists of training instances sampled i.i.d from P.
Given a single hypothesis f ∈ F, suppose wui ∈ (0, M), for any δ > 0, it holds with probability at
least 1 - δ that:
EQR(f) ≤ Epn,w R(f )+2Mogi+SldiQ^,
3n	n
where d(PkQ) = RS (dP/dQ)dP.
The above result for importance weighting ofa single hypothesis is stated in the Theorem 1 of Cortes
et al. (2010).
Lemma A.3. Letfbe a real-valued Convexfunctional defined on a convex subset Ω ofa vector space
X, and let G be a convex mapping of X into a normed space Z. Suppose there exists x1 such that
G(xι) < P and that inf {f (x) : G(X) ≤ ρ, X ∈ Ω} is finite, then it holds that:
inf = max inf {f (x) + (G(x), z*i}.
G(x)≤ρ,x∈Ω	z*≥ρ,z∈Z x∈Ωl
The above Lagrange duality result is stated in Theorem 8.1 of Luenberger (1997).
Lemma A.4 (Lower bound based on maximum variance). Suppose f ∈ F takes values in [0, 1]
without loss of generality, and the variance off is denoted by σ2 (f). Let σ(F) = supf ∈F σ(f),
and assume 1 ≤ σ2(g) < +∞. It holds that:
P sup
f∈F
EP f - EPn f
-σ(F)-
≥ c > 0.
The result is stated in Theorem 9 of Cortes et al. (2010)
Lemma A.5 (Finite sample bound for 1-Wasserstein distance). Suppose P and Qare supported
on (perhaps non-overlapping subsets of) Z where Z ∈ Rd and is bounded. Let MG :=
suPg：kgkL〈i supz∈z g(z). For all δ > 0, it holds with probability at least 1 一 δ that:
dW (P, Q) - dW (Pn, Qn) ≤
cw
n1/(d+1)
+ 8MG
Proof. The first part of the proof follows from the result on integral probability metric stated in
Theorem 11 of Sriperumbudur et al. (2009), that for G = {g : kgkL ≤ 1}, we have:
sup
g∈G
gdP - gdQ
- sup	gdPn
g∈G Z
一 ZZ gdQn] I ≤ 6FMGo 4 + 4Rn(G),
with probability at least 1 一 δ, and Rn(G) is the empirical Rademarcher complexity of G. Using the
entropy integral bound on the empirical Rademarcher complexity, we have:
∞
Rn(G) ≤2e+4√n2 Z
/4
√lθgN(u, G,'2 (Pn))du.
Note that Z ∈ Rd implies: N(u, Z, ∣∣∙ ∣∣∞) = O(u-d). Using the fact that log(x + 1) ≤ X + 1, we
have: logN(u, G,'2(Pn)) ≤ logN(u, G, ∣∙ ∣∣∞) ≤ ciu-(d+1) + c2u-d. Since Z is bounded, the
entropy integral bound becomes:
4√2 ∕∙4diam(Z)---------------------
Rn(G) ≤ inf 2e +----—	v cιu-(d+1) + c2u-ddu,
>0	n /4
which directly leads to the desired bound where CW is the corresponding constant.	□
24
Published as a conference paper at ICLR 2022
LemmaA.6. Let c(z, Z) := ∣∣z 一 Z∣∣2. Define:
f * = argmin min	EqR(∕), and λ* = argmax λ λρ + EZ〜P [ inf 'f* (z) + λ ∙ c(z, Z)]:.
f ∈F Q：dw (Q,P)≤ρ	λ≥0	I	z∈z	J
IfaU f ∈ F is L-Lipschitz, i.e. suPz,z，f (Z)-f： )| ≤ L ,then it holds： λ* ≤ L.
Proof. Using the Lipschitz property, we have:
λ*ρ ≥ λ*ρ + EZ〜P [ inf 'f * (Z) + λ* ∙ c(Z, Z) — 'f * (z)]
X----------------------------------------}
≥
≥
≥
≥
'∙^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^
always ≤0,because = 0 when z=Z
-	λρ + EZ〜P [ inf 'f* (Z) + λ* ∙ c(z, Z) — 'f* (z)]
lZ∈Z Jj j
一λρ + EZ〜P [ 一 L ∙ c(z, Z) + λ ∙ c(z, Z)]
—	λρ + inf {—Lt + λt}
-	LP ( by choosing λ = L),
( by the definition of λ* )
which gives λ* ≤ L.	□
Lemma A.7 (Change of measure inequality). For any P and Q on X, and for any measurable
function f : X → R, it holds that:
Ex〜Q ≤ dκL(PkQ)+ln (Ex〜Pef(X))∙
This is a standard result that has been included in many textbooks for probability.
Lemma A.8 (Interchange of integration and minimization). Let X be a space of measurable
functions from Z to Rn with respect to u, a σ-finite measure on the underlying space. Let
f : Z × Rn → R be a normal integrand, then it holds:
inf
x∈X
f(z, x(z))u(dz) = inf f(t, x)u(dz)
The result is simplied from Theorem 14.60 of Rockafellar and Wets (2009).
C Complete Experiment Settings, Numerical Results, and
Real-world Deployment and Testing Analysis
We provide the complete experiment settings (real-data and simulation experiments), including the
data description and preprocessing, experiment setups, model configurations and training details.
The complete numerical results and analysis are also presented in this part of the appendix. Finally,
we discuss the real-world online experiments by deploying our solution to a e-commerce platform.
The implementation code for the real-data and simulation experiments are provided in the supple-
ment material. All the reported results are averaged over ten independent runs.
C.1 Dataset description, preprocessing and experiment setup
The detailed descriptions for the three real-world benchmark datasets are given as below.
•	Movielens-1M 4. The dataset provides the users’ ratings for movies that they watched,
which consists of around 1 million ratings collected from 60,40 users on 3,952 movies.
The movie ratings are ranged from 1 to 5, where a higher rating indicates more positive
feedback. Hence, the original Movielens-1M dataset possesses explicit feedback. The
traditional way of converting the dataset to implicit feedback is to set the rating cut-off
at 3, as suggested by He et al. (2017). We use the explicit feedback for semi-synthetic
experiments, and use the converted explicit feedback for the real-data experiments.
4http://files.grouplens.org/datasets/movielens/ml-1m.zip
25
Published as a conference paper at ICLR 2022
•	LastFM 5. The context of the LastFM dataset is music recommendation: each of the 1,892
listeners (recorded in the dataset) tags the artists they may find fond of overtime. Since the
tag is a binary indicator, the LastFM is an implicit feedback dataset. There is a total of
186,479 tagging events, where 12,523 artists have been tagged.
•	GoodReads 6. The book recommendation dataset is obtained from the users’ public shelves
on Goodread.com. We use the user review data on the history and biography sections for
their richness. There are in total 238,450 users, 302,346 unique books, and 2,066,193
ratings in these sections. The rating range is also from 1 to 5, and a higher rating indicates
more positive feedback. We concert the dataset to implicit feedback in the same way as the
Movielens-1M dataset for the real-data experiments.
Data preprocessing. As for preprocessing, we point out that the Movielens-1M dataset has been
filtered by the publisher, where each user has rated at least 20 movies. For the LastFM and Goodread
datasets, we first filter the infrequent items and users who have less than 20 total interactions as well.
C.1.1 S imulation setting
We generate the implicit feedback (clicks) according to the click model of: P(Yui = 1) = P(Rui =
1) ∙ P(Oui = 1), where Rui and Oui indicates the relevance and exposure. We refer to P(RUi = 1)
as the relevance score. To carry over the inductive bias of the benchmark dataset to the generated
data, we first learn P(Rui = 1) and P(Oui = 1) from the explicit feedback data of Movielens-1M
and GoodReads. Our simulation approach does not apply to the LastFM data because it lacks the
explicit feedback.
Since the ratings are explicit indicators for relevance, we directly conduct the matrix factorization
collaborative filtering (MCF) using the ratings as the response. We then estimate Psim(Rui = 1)
Via σ(Rui - u)p, where σ(∙) is the Sigmoid function, and U andP are the two hyperparameters that
help us control the overall relevance distribution between the original and generated data. For both
Movielens-1M and GoodReads, we find u = 3 andp = 2 gives a decent approximation. In this way,
we inherit the inductive bias for the relevance to the simulated data.
As for the exposure, we first assume all the exposed movies/books are rated by the user, so we
have Oui = 1 if and only if Yui > 0. We then conduct another MCF using the converted Oui as
response, and obtain the user and item embedding: Φ(u), Φ(i). We further design the exposure
model by PSim(Oui = 1)= σ(MLP([Φ(u), Φ(i)])), where MLP(∙) is a three-layer feed-forward
neural network with ReLU activation and no bias terms, whose parameters are given by the random
normal initialization.
In the sequel, the simulated click probability is given by: PSim(YUi = 1) = PSim(Rui = 1) ∙
Psim(Oui = 1), from which we generated the click data by picking the cutoff probability that ensures
the click v.s. non-click proportion of the generated data matches that of the original data.
C.1.2 Train-validation-test split
We adopt the widely-acknowledged approach for splitting the dataset to training, validation and test-
ing as in He et al. (2017) and Rendle et al. (2020). In particular, we leverage the ordering information
and use the last interaction of each user for testing, the second-to-last interaction for validation, and
the previous interactions for training. All three datasets are provided with the ordering information,
so we use the same splitting mechanism. For the semi-synthetic dataset, since the simulation does
not account for the ordering information, we treat the ordering as random and conduct the splitting in
the same fashion. We point out here that using sequential recommendation models is not reasonable
in this case, so we do not experiment with the attention models (Attn) in the simulation experiments.
C.1.3 Evaluation
As we discussed in Section 6, the three metrics that we use for both the real-data and semi-synthetic
experiments are Rel@K, NDCG@K and Recall@K. We also adopt the widely-acknowledged for-
mulation of NDCG@K and Recall@K as from He et al. (2017) and Rendle et al. (2020): for
5http://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip
6https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home
26
Published as a conference paper at ICLR 2022
each positive instance (clicked data) of (u0, i0, Yu0i0 = 1), we randomly sample 100 negative items
{i1, . . . , i100} and compute NDCG@K and Recall@K according to where i0 is ranked among the
total of 101 items. We point out that the recent work by Krichene and Rendle (2020) points out
the potential bias issue of using the sampled metrics; however, it is still the mainstream approach
that is used in the majority of existing literature. We apply them here so our results can be directly
compared with the results in the relevant literature. To make up for the bias issue of using sam-
pled metric, we further consider the Rel@K metric, which ranks all the items {i1, . . . , i|I|} and see
where i0 is placed for each u0 . In this way, we eliminate the bias of using sampled metric. Essen-
tially, Rel@K equals Recall@K with no sampling when each user interacts with only one item in
the testing data (which happens in our experiments by the way we conduct the train-validation-test
split).
The only difference in computing the three metrics for the real-data and semi-synthetic experiments
is that we replace Yu0i0 by the underlying relevance score of Psim(Ru0i0 = 1). In this way, we obtain
a more accurate characterization of the actual testing performance. Finally, for the MovieLens and
Goodreads datasets, we adopt the tradition that the previously-interacted items are not considered
during the evaluation for each user. This is because in practice, the same movie or book should not
be exposed to the user repeatedly.
C.2 Model configuration and training
We provide the model configurations, training and computation details in this part of the appendix.
C.2. 1 Model Configurations
To achieve a fair comparison, we fix all the user and item embedding dimensions to d = 32 for the
CF models and sequential recommendation model. For all the baseline models, we select the initial
learning rate from {0.001, 0.005, 0.01, 0.05, 0.1}, and the `2 regularization parameter from {0, 0.01,
0.05, 0.1, 0.2, 0.3}. The tuning parameters are selected separately to avoid excessive computations.
The other hyperparameters are identified according to the original papers as for the model-specific
configurations, other than the embedding dimension, learning rate, and `2 regularization. For in-
stance, NCF considers the hidden dimensions of the feed-forward neural network as the hyperpa-
rameters, IPW-CF considers the IPW threshold, Attn considers the hidden dimension and number of
attention blocks, AC-MF considers the adversarial strength of the IPW component. We use the same
range as provided in their published implementations, which we discuss later in Appendix C.2.2.
When implemented with our transportation-regularized risk minimization, we focus primarily on
using the same ”base model” for f, w and g, e.g. with MCF, NCF or Attn. In fact, we can pick
any combinations for f, w and g, but we find the performances comparable to using the same ”base
model”, so we randomly pick some of the configurations to report, instead of exhausting all the
possibilities. Finally, the hyperparameters of the ”base models” persist to our setting, in addition to
the penalization parameter λ (in eq.(4)), which we select from {0.005, 0.1, 0.3, 0.5}.
C.2.2 Training details
For both the proposed approach and the baseline methods, we adopt the widely-applied negative-
sampling-based training schema according to He et al. (2017) and Rendle et al. (2020), for both
the real-data and simulation experiments. In particular, for each positive instance (Yui = 1), we
randomly sample three pairs of user-item tuple and treat them as the negative instances. We point
out that in many existing literature, the number of negative samples per positive instance is treated
as a tuning parameter. Here, we treat it as a fixed quantity and focus on tuning the model-specific
configurations.
For all the experiments, we use the binary cross-entropy loss function for classification, and apply
early stopping during training when the Rel@K metric does not improve for more than five epochs.
We record the model parameters after each epoch, and select the one that achieves the best validation
performance for testing.
Transportation-regularized risk minimization. When training with the proposed approach, the
discount of learning rate (in Algorithm 1) is critical for the two-time-scale GDA. In a sense it re-
sembles the number of gradient updates under the same learning rate, and we focus on their relative
27
Published as a conference paper at ICLR 2022
ratios. In practice, we allow the model of w in the minimization step to have a learning rate different
from that of f. We denote the ratio for f, w and g as stepf, stepw and stepg. In practice, we select
the ratio among {1 : 1 : 10, 1 : 5 : 10, 1 : 10 : 10, 1 : 10 : 5} for our experiments, and the sensitivity
analysis is provided in Appendix C.3.1.
Baseline models. The popularity (Pop) baseline is implemented with off-the-shelf code, the MCF
and NCF models are implemented according to their original implementation published on Github7,
the IPW-debaised MF (IPW-MF) and user-exposure aware MF (ExpoMF) models are implemented
using their published code8 with necessary modifications. The adversarial-counterfactual MF (AC-
MF) method is also implemented using the published code9, which we apply directly to our datasets.
The sequential recommendation model with attention mechanism is also implemented according to
the original code10 11. We point out that most of the implementation provided by the authors include the
set of hyper parameters they found optimal in their experiments. We re-select those hyper parameters
in each of our settings.
As for the doubly robust joint learning (DR-MF) approach (Wang et al., 2019), the method was
originally proposed for the bandit feedback setting or when the feedback data is explicit and the
propensity scores can be effectively estimated. Since we focus on the implicit feedback setting, we
use the clicks to estimate the propensities, and plug it to the DR approach. The causal embedding
approach (CE-MF), on the other hand, assumes the access to a uniform exposure dataset. The
authors propose a workaround to construct such data from non-uniform-exposure feedback data
(Bonner and Vasile, 2018), however, they take advantage of the explicit feedback (rating). Since we
only have implicit feedback, we random subsample the click data and treat them as if they are from
the uniform exposure.
Computation. All the models are implemented with the auto-differentiation framework of Py-
Torch11. The computations are conducted on a Linux cluster with 2 Nvidia V100 GPU machines
(each with 32 Gb memory) and 32 CPU with a total memory of 100 Gb. We use the sparse Adam12
optimizer to update the hidden factors (user and item embeddings), and the usual Adam optimizer
to update the remaining parameters. This is because both the user and item embeddings are rela-
tively sparse in the datasets. Therefore, the Adam algorithm, which leverages the momentum of the
gradients from the previous batch, may not be ideal for updating the item and user embedding in the
current batch. The sparse Adam optimizer address the above issue.
C.3 Complete Numerical Results
Table A.1: Additional testing results on MovieLens-1M. The settings of this table follow from Table
1. DT-(M/M/N), for instance, indicates using the proposed method with f and MCF, w as NCF and
g as NCF. We highlight the best results.
I DT-(M/N/N) DT-(M/M/N) DT-(N/M/M) ∣ DR-MF CE-MF
MovieLens-IM
Rel@10 Hit@10 NDCG@10	14.55 (.11) 62.34 (.20) 32.75 (.11)	14.63 (.10) 63.02 (.21) 33.21 (.23)	14.52 (.08) 62.18 (.19) 32.90 (.16)	14.28 (.12) 61.03 (.16) 31.97 (.08)	14.17 (.08) 61.22 (.18) 32.04 (.07)
Rel@10*	4.62 (.03)	MovieLens Simulation 4.67 (.02)	4.84 (.04)		3.89 (.03)	3.91 (.02)
Hit@10	79.89 (.17)	79.80 (.15)	79.83 (.20)	75.99 (.10)	75.69 (.12)
NDCG@10	39.98 (.10)	39.94(.11)	40.57 (.11)	38.38 (.07)	38.21 (.06)
We present the complete numerical results in this part of the appendix. We start with the testing
performance with respect to the methods for MovieLens-1M that are not shown in the main paper
due to the space limitation.
7https://github.com/hexiangnan/neural_collaborative_filtering
8https://github.com/usaito/unbiased-implicit-rec-real for IPW-MF and https:
//github.com/dawenl/expo-mf for ExpoMF.
9https://github.com/StatsDLMathsRecomSys/Adversarial-Counterfactual-Learning-and-Evaluation
10https://github.com/kang205/SASRec
11https://pytorch.org/
12 https://agi.io/2019/02/28/optimization- using- adam- on- sparse- tensors/
28
Published as a conference paper at ICLR 2022
Table A.2: Simulation experiment results for the Goodreads simulation. The settings and interpre-
tations are the same as Table 1. We highlight the best results.
Pop IPW-MF ExpoCF AC-MF ∣ MCF DT-MCF ∣ NCF	DT-NCF |
GoodReads Synthetic
Rel@10	4.34 (.02)	4.62 (.09)	4.57 (.04)	4.68 (.10)	4.56 (.06)	4.78 (.06)	4.99 (.05)	5.13 (.07)
Hit@10	42.75 (.04)	49.22 (.34)	48.97 (.26)	49.30 (.25)	48.06 (.25)	49.94 (.21)	52.47 (.18)	52.96 (.21)
NDCG@10	23.20 (.02)	25.91 (.15)	25.76 (.17)	26.04 (.13)	25.25 (.15)	26.72 (.12)	28.17 (.11)	29.78 (.13)
Table A.3: Additional numeric results for the variations of the proposed domain transportation meth-
ods with different combinations of the f , g and w models).
DT-(M/N/N)^^DT-(N/M/M) ∣ DT-(M/N/N)^^DT-(N/M/M) ∣ DT-(M/N/N)^^DT-(N/M/M)
Rel@10
Hit@10
NDCG@10
GoodReads
5.92 (.04)	6.28	(.05)
58.48 (.22)	61.64	(.30)
30.32 (.17)	36.32 (.22)
GoodReads Simulation
4.65 (.04)	4.95 (.06)
48.97 (.26)	49.30	(.25)
25.88 (.14)	28.61	(.13)
LastFM
6.53 (.06)	6.38 (.07)
81.09 (.33)	78.96	(.31)
52.05 (.18)	51.37	(.20)
Additional results for MovieLens-1M and its synthetic data. In Table A.1, we show the addi-
tional results for the MovieLens experiments, including using the proposed domain-transportation
approach with different configurations of f, w and g (denoted by DT-(f/w/g)). Note that Attn does
not apply to the synthetic data, because the ordering in the synthetic data is randomly generated as
we mentioned previously. For notation, we use the short hand DT-(M/N/N), where in the parenthesis
are the assignments for f, w and g, and M is short for MCF, N is short for NCF. Also, we exper-
iment with the doubly-robust joint learning approach on MCF (denoted by DR-MF) as proposed
by Wang et al. (2019), and the causal embedding approach on MCF (denoted by CE-MF) proposed
in Bonner and Vasile (2018). The two methods were proposed for the explicit feedback data, and
we observe that while they are still able to improve the performance of MCF in our implict feed-
back setting, their improvements are much less significant compared with the proposed approach.
Also, it appears that the causal embedding approach suffers from high variances, which is due to the
workaround when constructing the ”uniform exposure” dataset from implicit feedback data.
Complete results for the LastFM data. In Table A.3, we show the complete results for the LastFM
experiments. We do not conduct the synthetic experiments on the LastFM data, because unlike the
other two datasets, the original dataset consists only of implicit feedback. Therefore, neither the
relevance nor the exposure can be efficiently estimated from the data. The settings, interpretations
and notations for our approach are the same as above.
Complete results for the Goodreads simulation experiment. In Table A.2 and Table A.3, we
show the complete results for the Goodreads experiments. Similarly, Attn does not apply to the
synthetic data for the same reason discussed above.
We then present the complete results in correspondence to Figure 2. In Figure A.1, we the analysis
of the learnt weights on the Goodreads data and its simulation. In Figure A.2, we show the same
set of analysis for the LastFM dataset. Finally, in Figure A.3, we provide the ablation study and
sensitivity analysis on λ, for both Goodreads and LastFM dataset.
C.3. 1	Sensitivity Analysis on the optimization setting and overlapping
To ensure the convergence of the minimax optimization without being trapped in a cycle, it is a
standard practice to use the two-time-scale GDA algorithm. Our situation is even more involved
since we not only have a minimax game, but there are two minimization players.
Intuitively, f is moved only when Pw has approximated Pf sufficiently well. It means that
w has converged in the sense that dW (Pf, Pw) is sufficiently small. Since we use the Monge
and Kantorovich formulation to represent the 1-Wasserstein distance dW, i.e., dW (P, Q) =
suPg：kgkL〈i JD gdP - JD gdQ, We need to ensure g to converge every time W moves so that the
integral difference is approximately equal to dW (Pf, Pw). In other words, this is a three-layer loops:
When f moves one step, We need w to move stepw steps until convergence; When w moves one step,
We need g to move stepg steps until convergence.
29
Published as a conference paper at ICLR 2022
DT-MCF
AUU3nbθ.lLL.
Type
czι Pos
I I Neg
出
O
DT-MCF
DT-MCF
-1.00 -0.95 -0.90 -0.85 -0.80 -0.75
Learnt Weights
DT-NCF
-0.95	-0.90	-0.85	-0.80	-0.75
Learnt Weights
DT-NCF
Type
---Recommended
---Others
-0.75 -0.50 -0.25 0.00 0.25 0.50 0.75
Learnt Weights
DT-NCF
Figure A.1: Analysis on the learnt weights for the Goodreads dataset. The setting and interpretation
of the figure follows from Fig. 1a and 1b in Section 5.
75
AUUφnbaJu-
Learnt Weights
DT-NCF
Type
I-I Neg
匚二J POs
0.70 0.75 0.80 0.85 0.90	0.95
Learnt Weights
Type
---Recommended
---Others
Learnt Weights
Figure A.2: Analysis on the learnt weights for the LastFM dataset. Note that the semi-synthetic
experiments are not conducted on the LastFM dataset. The setting and interpretation of the figure
follows from Fig. 1a and 1b in Section 5.
Figure A.4a shows that as the stepw increases when stepg is fixed, the overall performance of our
method is boosted, which aligns with the above claim that w should converge before f moves. We
observe similar phenomenon when stepg increases as stepw is fixed, which corroborates the claim
that g should converge before w moves. These results suggest that we might want to use a large
stepw and a large stepg . However, it can be computationally expensive. From our experience, we
can use a relatively large stepg and a slightly smaller stepw to achieve a good performance. In
addition, appropriate choices of learning rates lrf, lrw and lrg can expedite the convergence of w
and g so that we can use less steps.
C.4 Analysis for the complete results
We first observe from Table A.1 that the sequential recommendation methods achieve overall better
performances than the others. It is expected because the sequential signal in the MovieLens-1M
dataset is usually substantial, as point out by Kang and McAuley (2018). Similar to the previous
experiments, DT-Attn improves the performance of the regular Attn model. As for the variations of
our methods, they all improve the performance of the corresponding f model, and even outperforms
30
Published as a conference paper at ICLR 2022
0Ioac≤0I©wcc
Goodreads
0I@acc
MCF	NCF
Method
0.1	0.2	0.3	0.4	0.5
lambda
0I@acc
Figure A.3: The ablation study on the regularization component, and the sensitivity analysis on λ,
for both the Goodreads and LastFM datset. The setting and interpretation of the figure follows
from Fig. 1d in Section 5.
Re ⑥ F 工 tl(g)10 NDCG© I (
Figure A.4: (a).Performance of minimax optimization with different number of optimization steps
for updating w and g. The analysis is conducted on the MovieLens-1M dataset. (b). Sensitivity
analysis on the threshold of propensity score for IPW-MF. In the orignial paper (Saito et al., 2020),
the authors apply a threshold on the smallest possible propensity score as if there was sufficient
overlap. Here, we vary the threshold to hypothetically generate the different overlapping scenarios.
the DT methods where f , w and g use the same ”base model”. This is a phenomenon that worth
future investigation, since the results might indicate the DT-(f/w/g) methods combine the strengths
of the base models.
The results for the Goodreads and its simulation experiments in Table A.2 and Table A.3 show
the similar patterns. Firstly, using the proposed transportation-regularized approach improves the
performances of the baseline methods, and the best results are achieved by our DT-X methods.
Secondly, while the other enhanced MF/CF methods also improve upon the baseline MCF model,
but their improvements are much less significant than the DT approach. Thirdly, the DT-(f/w/g)
methods also achieve more significant improvements compared with the DT-f methods. We point
out that the sequential recommendation method (Attn) does not particularly suit the dataset, since
it is outperformed by the non-sequential methods. Still, DT-Attn achieves better performances than
Attn. Finally, the additional results for the LastFM dataset in Table A.3 have the similar patterns.
As for the analysis on the learnt weights, we observe from Figure A.1 and Figure A.2 that the
patterns we discussed in Section 6 for the MovieLens dataset and its simulation holds almost the
same for the Goodreads and LastFM datasets. Firstly, the learnt weights for the instances among
the feedback data can be distinguished by whether the instance is positive or negative. We observe
that the positive instance tends to have larger weights than the negative instances, which means the
weighting model is learnt to emphasise the ”higher-quality” positive implicit feedback. Secondly,
31
Published as a conference paper at ICLR 2022
the learnt weights also get larger for the instances that will be recommended under f, and the larger
the score of f, the higher the weights. It means the weighting function works harder on balancing
Pf with Pw , which conforms to our design. Lastly, for the simulation, the learnt weights tend to
be larger for the negative instances with higher exposure probability, which can be considered the
strong negative instances. This is because they are more likely to be exposed but are not clicked.
C.5	Efficiency analysis
Figure A.5: Analysis on the efficiency of the proposed approach. We report the training time (on
the x-axis) and the per-epoch evaluation results (on the y-axis), using the Movielens-1M data as
examples. We compare between the original MCF and the DT-MF approach, and when training
both models, all the model and data configurations are kept exactly the same.
In terms of the space complexity, it is evident that our approach need to store the parameters and
gradients of the f, w, and g models (if no parameter sharing is considered). For the time complexity,
however, the proposed approach is highly efficient due to the following reasons.
1.	As it is shown in Figure A.5, although the DT-MF approach has a larger per-epoch training
time, it makes training progress much faster than the original MCF thanks to the rewieghting and
DT regularization. For the Movielens dataset, DT-X catches up and outperforms X in a couple of
training epochs even before the X model converges.
2.	The update of f, w, and g can be efficiently parallelized.
3.	Our approach does not change the inference time since only the f component will be used for
evaluation and testing, just like when using f alone.
C.6 Deployment to real-world IR system and online testing
An important motivation of our work is to solve the practical challenges of industrial IR systems,
and the working mechanism we described in Section 1 applies to most settings. In particular, many
real-world recommenders are deterministic, and we can only expose a tiny proportion of the catalog
to the customers due to the limited slots and business requirements. For many industrial tasks, the
revenue is directly related to the performance of recommender since unqualified recommendations
may hurt customer satisfaction severely. Therefore, the feedback data often covers extremely limited
support of the product space. It causes a realistic dilemma:
•	if the candidate recommender f recommends products very differently from the previous
exposure, then the feedback data may not provide enough evidence to assert the perfor-
mance of f;
•	if the candidate recommender f recommends products highly similar to the previous expo-
sure, then f will inevitably inherit the various data bias.
From this realistic perspective, our approach aims to find the right balance between the two extremes:
the weighting model helps to reshape the feedback data domain (to better fit the objective risk), and
the regularizer enforces f to be sufficiently close to the reshaped feedback data domain. To test
the competence of our approach for industrial tasks, we apply the proposed domain transportation
approach to an industrial recommendation task, and conduct online A/B/C testing to examine the
real-world performance.
The context of our task is to recommend items that are similar to the current webpage’s product for
a major e-commerce platform in the U.S. The platform hosts more than a hundred million products
32
Published as a conference paper at ICLR 2022
	Recall@20	NDCG@20	^ CT R
D&W (control)	+00	+0.0	+0.0
IPW-D&W (variation1)	+0.23%	+0.06%	+0.05%
DT-D&W (variation2)	+0.56%	+0.09%	+0.07%
Table A.4: Offline evaluation of the three models used in the online A/B/C testing. Due to pri-
vacy reasons, we are only able to report the relative lift percentage with respect to the baseline. We
use D&W to represent the original deep&wide control model, IW-D&W to represent the approxi-
mate propensity-weighted version, and DT-D&W to represent the proposed domain transportation
approach when coupled With the deep&wide model. *: the CTR is computed using the history ex-
posure log and the past estimated CTR, similar to how we computed the relevance score Rel in the
benchmark and simulation experiments.
Figure A.6: Online A/B/C testing results for the similar-item model under direct training (control),
IPW-adjusted training (variation 1) and the proposed transportation-constraint risk minimization
(variation2). The first plot shows the daily gross merchandise value (GMV) lift, and the second
plot shows the overall lift of GMV, number of units per checkout (UNITS), number of orders per
visit (ORDERS), and the total number of converted visitors (CONVERTED VISITOR).
ranging from high-end electronics to daily grocery. Each item has its contextual features, including
rating, price, taxonomy, etc. The pre-trained embeddings are also available for the items obtained
from a modified version of the SGNS algorithm. When applying the item embedding to the model,
we use the pre-trained embeddings as initialization and update the parameters with a lower learning
rate than the other model parameters. We employ an architecture that is similar to the classical Deep-
and-wide model, initially proposed by Cheng et al. (2016) for recommending YouTube videos, with
the difference that we do not use the real-time features here.
We trained three versions of the deep-and-wide model offline:
•	control: directly training f using the implicit feedback data;
•	variation1: first training an exposure model using the past exposure log, apply its normal-
ized version (for controlling the variance) as the weighting function w , and then train f in
a way similar to the baseline IPW-MF mentioned in Section 5;
•	variation2: train f using the proposed transportation-regularized risk minimization, where
w and f are neural CF models that take the item embeddings as input.
All three versions are trained under the same setting. We deploy online A/B/C testing to examine the
real-world performances, and the monitored metrics and testing results are provided in Figure A.6.
We see that our transportation-regularized risk minimization (variation2) consistently outperforms
the original model (control) and the IW-adjusted model (variation1) in term of the gross merchan-
dise value (GMV), which is the most critical metric for industrial recommender system. Also, the
proposed approach improves the other monitored metrics (lower panel of Figure A.6) more signif-
33
Published as a conference paper at ICLR 2022
icantly than the IW-adjusted model. Our deployment results further suggest the capability of the
proposed approach for improving the performance of industrial IR systems.
D Extended Literature Review
When viewing our solution from the causal inference perspective, our objective resembles the coun-
terfactual loss proposed by Shalit et al. (2017) focusing on individual treatment effect, which was
also discussed in the following work (Johansson et al., 2019; 2020). Further, Johansson et al. (2019)
studies from the domain adaptation perspective the issues of insufficient overlapping. Unlike these
works where the target domain of interest is fixed in advance, IR systems get to choose the counter-
factual world in which they want to act. It leads to the main difference between our work and that
line of research: we can control the domain in which the counterfactual risk is computed.
Recently, there has been fruitful research progress in off-policy learning and data missing-not-at-
random. Again, most existing methods treat the uniform-exposed domain as the target, while we
aim at the deployment domain formed by the IR system. Further, most of them require either a suffi-
ciently randomized logging policy or explicit ratings in the feedback, while our approach is primar-
ily designed for implicit feedback under a deterministic policy. For instance, Sachdeva et al. (2020)
studied the overlapping issue for learning from bandit feedback. Similarly, the causal embedding
approach (Bonner and Vasile, 2018) requires access to a subset of randomly exposed feedback, and
many others rely on the explicit ratings to estimate the historical propensities (Wang et al., 2019;
Saito, 2020; Liu et al., 2020). On the other hand, the interventional nature of IR systems is also
discussed in Xiao and Wang (2021), but their solution is devised from the reinforcement learning
setting.
The Wasserstein distance has also been employed by distribution-robust optimization (DRO) (Kuhn
et al., 2019). For instance, Si et al. (2020) uses Wasserstein distance to assert certain robustness to
the contextual feature distribution during bandit off-policy learning, and Faury et al. (2020) proposes
a robust counterfactual learning framework to handle the various uncertainties in bandit feedback
data. Our work differs fundamentally from that research venue as we use Wasserstein distance to
regularize learnt policy (via constraining its deployment domain) rather than assert its robustness.
Further, DRO methods often have a min-max formulation, while our initial objective in (2) and (3)
tackle a min-min problem.
34